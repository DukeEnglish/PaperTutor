TeleMoMa: A Modular and Versatile Teleoperation
System for Mobile Manipulation
Shivin Dass1, Wensi Ai2, Yuqian Jiang1, Samik Singh1, Jiaheng Hu1,
Ruohan Zhang2, Peter Stone1,3, Ben Abbatematteo1, Roberto Mart´ın-Mart´ın1
1The University of Texas at Austin 2Stanford University 3Sony AI
Abstract—A critical bottleneck limiting imitation learning in
robotics is the lack of data. This problem is more severe in
mobile manipulation, where collecting demonstrations is harder
than in stationary manipulation due to the lack of available and
easy-to-useteleoperationinterfaces.Inthiswork,wedemonstrate Vision
TeleMoMa, a general and modular interface for whole-body
teleoperationofmobilemanipulators.TeleMoMaunifiesmultiple
human interfaces including RGB and depth cameras, virtual
HSR
realitycontrollers,keyboard,joysticks,etc.,andanycombination
thereof. In its more accessible version, TeleMoMa works using
simply vision (e.g., an RGB-D camera), lowering the entry bar VR
for humans to provide mobile manipulation demonstrations. We
demonstratetheversatilityofTeleMoMabyteleoperatingseveral
existingmobilemanipulators—PALTiago++,ToyotaHSR,and
Fetch — in simulation and the real world. We demonstrate
the quality of the demonstrations collected with TeleMoMa by Tiago
trainingimitationlearningpoliciesformobilemanipulationtasks
VR+Vision Fetch
involvingsynchronizedwhole-bodymotion.Finally,wealsoshow
that TeleMoMa’s teleoperation channel enables teleoperation on
site, looking at the robot, or remote, sending commands and Fig.1:TeleMoMa:amodularandversatilemobilemanipulationteleoperation
observations through a computer network, and perform user system.LeftandMiddleDemonstratorsperformingabimanualsweepingtask
studiestoevaluatehoweasyitisfornoviceuserstolearntocollect with the vision-only, virtual reality (VR), and a combination of vision+VR
demonstrations with different combinations of human interfaces interfaces.TeleMoMaenablesmultiplehumaninterfacesandtheircombina-
enabled by our system. We hope TeleMoMa becomes a helpful tion.MiddleandRightTiago(real),HSR(real),andFetch(simulation),three
toolforthecommunityenablingresearcherstocollectwhole-body oftherobotplatformsthatwedemonstrateteleoperatedfordifferentmobile
manipulationtaskswithTeleMoMa,demonstratingitsversatility.
mobile manipulation demonstrations. For more information and
video results, https://robin-lab.cs.utexas.edu/telemoma-web/.
not, is due to the availability of large datasets of human-
I. INTRODUCTION
collected demonstrations [9, 54]. They were obtained due to
A core goal of robotics is to build generalist robots capable
themultipleexistingandeasy-to-useteleoperationframeworks
of operating alongside humans in their environment. To this
for stationary manipulators [30, 64, 59, 39, 10]. For mobile
end, learning from human-collected robot demonstrations has
manipulation, however, the existing stationary manipulation
shown promise in endowing robots with the capabilities to
teleoperation systems are not sufficient, due to the additional
solvecomplextasks[4,42,32],boostedrecentlybytheadvent
degrees of freedom that the user has to control including
of foundation models capable of learning from large amounts
mobility and possibly multiple arms.
ofdata[5,14].Whilethesemodelsdemonstrateanimpressive
Several teleoperation frameworks for mobile manipulation
semantic understanding of the tasks [51, 6, 9, 12, 8], these
have been proposed in the past, with different capabilities
successeshavebeenlargelylimitedtostationarymanipulation.
and limitations. They either enable accurate control with
However,alargefractionofthetasksthatwewouldlikegener-
specific (and often expensive) hardware like motion capture
alist robots to perform require a combination of manipulation
systems [3, 50, 47] or puppeteering interfaces [16, 38], or
and mobility: e.g., sweeping the floor requires moving the
achievescalabilitybyoverloadingsimpleandavailabledevices
broom with both hands and walking around to reach the dirty
that work for stationary manipulators such as gamepads [10],
spots; covering a table with a tablecloth requires holding the
virtual reality controllers [46], or mobile phones [52, 58],
tablecloth and pulling it over the table while simultaneously
limiting the expressiveness of the demonstrations. Teleopera-
moving to reach all edges.
tion based solely on vision [63, 39, 21] promises an available
Oneofthereasonswhystationarymanipulationhasenjoyed
and accessible interface at the cost of accuracy and dexterity.
the benefits of large models, while mobile manipulation has
Each device alone presents a tradeoff between accuracy and
Correspondance:sdass@utexas.edu availability, versatility and expressiveness, and as a result, no
4202
raM
21
]OR.sc[
1v96870.3042:viXrasingle device enables scalable, expressive teleoperation for all methods [4, 42], as the ability to quickly collect large scale
mobile manipulators. roboticdatahasbecomeparamountfortraininglargecapacity
Inspired by the complementary capabilities of several of behavior models [31, 22, 10]. Many teleoperation modalities
the human interfaces for teleoperation, we introduce Tele- have been proposed to address these challenges, including
MoMa (Teleoperation for Mobile Manipulation), a novel kinesthetic teaching, joysticks, virtual reality, mobile phones,
teleoperation framework focusing on modularity and versa- RGB cameras, exoskeletons, and motion capture.
tility. TeleMoMa enables users to teleoperate different mobile Each modality has its benefits and shortcomings. Joysticks
manipulators with a variety of human interfaces or combi- (e.g. the SpaceMouse) offer intuitive control of a robot’s end-
nations thereof, in simulation or the real world, providing effector(s), but fail to enable joint control or navigation [43].
users the means to select the combination that best fits their Virtual reality enables users to perform tasks from the robot’s
teleoperation needs. TeleMoMa offers the lowest entry point perspective, but is limited by individual tolerance to motion
for researchers: it enables whole-body teleoperation with just sickness and does not naturally enable simultaneous loco-
a depth camera. But its modularity enables us to use other motion and manipulation [62, 56, 40, 11, 19, 28]. Mobile
interfaces such as virtual reality (VR) controllers, a keyboard, phones offer scalable data collection, but provide a very
a 3D mouse, a mobile phone, or their combination with limited interface, failing to naturally support joint control
vision, overcoming the limitations of each individual input or base motion [30, 31]. RGB cameras have been explored
modality.Suchasystemenablesresearcherstocollectdemon- as an accessible, scalable medium with limited mobility and
strations of whole-body mobile manipulation tasks at scale range of motion [21, 39, 49]. Exoskeletons and master-slave
for virtually any robot and hardware interface available. We devices enable dexterous control but are typically platform-
demonstrate that TeleMoMa allows researchers to teleoperate specific and costly [13, 64, 16, 38], and do not naturally
different mobile manipulation platforms out-of-box such as provide a way to coordinate base and arm motion. Motion
PAL Tiago [36], Toyota HSR [60] and Zebra Fetch [57]. capture similarly enables high-quality data collection, but is
TeleMoMa extends also to simulation, which we demonstrate costly and difficult to scale [3, 47, 50]. Kinesthetic teaching
by integrating it with OmniGibson [27] and the BEHAVIOR- was the predominant teleoperation paradigm for imitation
1K benchmark. learning for many years [4, 42, 24], but fails to enable more
In our experiments, we evaluate both TeleMoMa’s usability complicated bimanual or mobile manipulation tasks. Some
anditssuitabilityfordatacollectionforimitationlearning.We worksexplorethecombinationofdifferentmodalities[55,15]
conductedauserstudytoevaluatethebenefitsofmodularityin butfailtobesufficientlygeneralandextensible.Thus,despite
TeleMoMa and accessibility of TeleMoMa-enabled interfaces the plethora of available options, there remains a need for
for novice users. Our results indicate that a hybrid vision- a teleoperation system capable of adapting to the needs of
VRinterface isan efficientand naturalmode ofteleoperation, mobile manipulation in a scalable, accessible way.
and that novice users are quickly able to learn to use it. We TeleoperationforMobileManipulation.Recentsuccesses
also successfully trained several imitation learning policies inlearningfromlargecollectionsofhumandemonstrationshas
on the data collected using TeleMoMa and explored relevant been limited to stationary manipulators [9, 51, 6] or simple
questions for IL for mobile manipulation such as (a) What mobile manipulation tasks like pick and place that do not
inputs matter in imitation learning for mobile manipulators?, require coordination between base and arm motion [7, 8].
and (b) How do the policies scale as we increase the size This is in part due to the lack of accessible and intuitive
of the training data? We measured the role that different ways to collect demonstrations for mobile robots. Recently,
embodiments with different capabilities and the sim-real gap some methods have tried to address this using specialized
have in teleoperation performance, and demonstrate remote hardware, such as motion capture systems [3, 50, 25, 47],
teleoperation of real robots with an analysis of robustness to exoskeletons [16, 61, 35, 13] and more sophisticated human-
the latency and delays in the communication. computerinterfaces[45,26].Ontheotherhand,severalworks
In summary, TeleMoMa is a novel, modular, and versatile borrow from successful teleoperation interfaces in stationary
teleoperation framework for whole-body mobile manipulation manipulation, using interfaces such as VR [17, 46, 37, 20,
that facilitates the integration of different human interfaces, 18, 34, 23], kinesthetic teaching [61], visual motion track-
robotplatforms,andsimulators.Wehopethatourcontribution ing [63], keyboard and mouse [41] and mobile phones [58],
lowers the barrier of entry for researchers to collect demon- by modifying them to enable the control of mobile manipu-
strations for imitation learning for mobile manipulation. lators. Although these interfaces are accessible, they lack the
granularity necessary to coordinate all degrees of freedom of
II. RELATEDWORK
a mobile robot for a true mobile manipulation task.
Teleoperation for General Robotics. Teleoperation is WesummarizethemainfeaturesofTeleMoMaandcontrast
almost as old as the field of robotics itself [53], with itwithrelatedsystemsinTableI.Thecriteriaforeachcategory
early manipulators being controlled in kinematically identical is described further in Appendix A. We compare across two
master-slave systems [48] similar to the very recent Mobile primary dimensions: the teleoperation modalities provided,
ALOHA [16]. More recently, teleoperation has emerged as and the robot capabilities enabled. TeleMoMa is the only
a critical means of data collection for imitation learning teleoperation system to provide modularity and enable theTABLEI:ComparisonofExistingMobileManipulationTeleoperationSystems
TeleoperationSupport RobotSupport
Cost/ Height Whole-Body Robot Action
Modular Modality Bimanual Domain
Accessibility Control Teleop Agnostic Space
✓ ✓ ✓
Arduengoetal.[3] ff Mocap EEPose/BaseVel. Real
✓
MoMaRT[58] f Phone EEPose/BaseVel. Sim
✓
MOMA-Force[61] fff Kinesthetic EEPoseandWrench Real
✓ ✓
SATYRR [38] fff Puppeteer JointPos./BaseVel. Real
✓ ✓ ✓
TRILL[46] f VR EEPoses/Gait Sim&Real
✓ ✓
MobileALOHA[16] fff Puppeteer JointPos./BaseVel. Real
✓ ✓ ✓ ✓ ✓
TeleMoMa f * EEPoses/BaseVel./JointPos. Sim&Real
flexible combination of multiple input modalities. Moreover, their combination) extensively in our experiments because
it is the only system capable of full whole-body motion their combined capabilities strike a good balance between
(including torso control) that remains accessible and robot availability, generality, dexterity and accuracy. We defer the
agnostic. implementation details of other human interfaces such as
spacemouse, keyboard, and mobile phones to Appendix B.
III. TELEMOMASYSTEM
1) Vision-Based Human Interface: TeleMoMa offers a
TeleMoMa is a teleoperation system for mobile manipula- unique vision-based pipeline for the whole-body teleoperation
tors—versatilefordifferentrobotmorphologiesandmodular ofamobilemanipulatorusingasingleRGB-Dcamera.Weuse
inthehumaninputdevices.ItisgenerallycomposedofaTele- MediaPipe[29],alightweightRGB-basedmodelthatexecutes
operation Channel that defines the communication between a in real-time for body pose and hand keypoint detection. Our
Human Interface and a Robot Interface (Fig. 2). The Human proposed human interface uses the position and rotation of
Interface acquires human inputs across different teleoperation the hips to control the movement of the base of the mobile
modessuchasvision,VR,spacemouse,keyboard,andmobile manipulator. Since the model only provides the relative depth
phones, or their combinations, and maps them to a general of the keypoints to the center of the hip and not the absolute
mobile manipulation action command structure provided by depth,weusethedepthchannelofanRGB-Dcameratoobtain
the Teleoperation Channel that includes fields such as base, theabsolutevalues.Thehandkeypointsaremappedtotheend-
arm, gripper, and torso motion. Multiple input devices can effector of the robot based on the position and orientation of
be combined through our Human Interface to acquire the the palm with respect to the hip. We compute the per-frame
action commands in the best suited manner for a task. The relative pose displacement in Cartesian space of the hands
TeleoperationChannelhandsovertheactioncommandstothe and send them in the teleoperation channel’s action command
RobotInterface,arobot-specificmodulethatmapstheactions as arm delta commands. Additionally, we use the distance
to robot motor commands. While the specific implementation between the center of the hips and ankles to command the
oftherobotinterfacereliesonplatform-dependentcontrollers, robot height for robots with an actuated torso.
our requirements (controllers for the motion of the end- 2) Virtual Reality Controllers as Human Interface: Tele-
effectors, base, joints, ...) are general enough to enable the MoMa supports Oculus Quest and HTC Vive virtual reality
teleoperationofmostexistingplatforms,intherealworldand hardware devices as inputs to the VR human interface. The
simulation.Inthefollowing,weprovideadditionalinformation controllers are tracked with respect to the headset for Oculus
about the three components of TeleMoMa. and with respect to the lighthouse for HTC Vive. Similar to
Seo et al. [46], the tracked hand poses in Cartesian space are
A. Human Interface
used to command the end-effector in the task-space. As in
The Human Interface is responsible for processing the cap- the vision-based interface, we compute the per-frame relative
tured data from various teleoperation input devices and map- pose displacement of hands and use them in the teleoperation
ping them to a common action command structure. For each channel’sactioncommand.ThejoysticksintegratedintheVR
input device, the data is processed independently by a device- areusedtocommandthevelocitiesofthemobilebaseandalso
specific parser that maps the signals from the input modality control the torso extension.
(keyboard strokes, motion of a VR controller, location of
B. Teleoperation Channel
human skeleton keypoints on an image, ...) into elements of
the teleoperation channel’s action command. TeleMoMa sup- The Teleoperation Channel defines how the Human Inter-
ports input modalities such as vision, keyboard, spacemouse, face communicates with the Robot Interface, and is the key
VR (Oculus Quest and HTC Vive) and mobile phones. In the to TeleMoMa’s generality and modularity. Specifically, the
following, we explain the vision and VR human interfaces Teleoperation Channel defines an action command structure
fromTeleMoMaindetail.Wetestthesehumaninterfaces(and that serves as a bridge between the human and the robot andAction Commands
• left_arm_delta
• right_arm_delta
• left_gripper
• right_gripper
• torso_extension
• base_velocity
Human Robot
Interface Interface
Teleoperation Channel
On-Site Remote
Human observes the robot
Fig.2:TeleMoMaSystem.TeleMoMaconsistsofthreecomponents:theHumanInterfaceacquirescommandsfromthehumanusingdifferentinputdevices;
theTeleoperationChanneldefinestheactioncommandstructurebetweenthehumanandtherobotinterfaces,and,possibly,closestheloopwithobservations
fromtherobot;andtheRobotInterfaceimplementsarobot-specificmappingofactionstolow-levelrobotcommands.Thisarchitectureenablesmodularity
andversatility–combiningmultipledevicestoachieveintuitivewhole-bodyteleoperationformultipletasksandrobots.
the way the active human interfaces populate the entries of fromtheTeleoperationChannelintolow-levelcommands.We
this structure. donotdeemourrequirementsfortherobotplatformstoohigh:
During deployment, users can specify what input modality the robot should provide some controllers to move either the
theywanttousetocontroleachpartoftherobot’sembodiment end-effector(s) and the base in Cartesian space, the joints, or
including left and right arms and hands, torso, and base. combinations of both.
The Teleoperation Channel automatically manages the action The action command structure in TeleMoMa relayed to the
assignment based on the user specification, and consolidates Robot Interface can either contain values in task-space (end-
the possible missing elements of the action commands due to effector Cartesian relative motion), joint space (e.g., torso
differences in hardware frequency or network delays. commands or motion to other joints) and/or velocities (e.g.,
Finally, the Teleoperation Channel also defines the mech- base commands), or different combinations of those, as spec-
anism by which humans close the loop with the robot and ified by the user during deployment. The Robot Interface
observe the execution of the action commands, adapting processes these commands based on the particular robot em-
those to achieve the mobile manipulation tasks. We consider bodiment, filters out the unusable action components (such
two methods of observation: on-site and remote. When on- as left hand commands for a single-armed robot like Fetch),
site, the human directly observes the robot executing the and maps the rest to the robot using the preferred choices
action commands. When remote, the Teleoperation Channel of controllers such as operational space control [1] to control
communicates the images from the onboard sensors of the onetaskframe,orwhole-bodycontrol[2,33]tocommandthe
robot to the human interface to be displayed for the human, entire robot jointly, or having separate controls for each part
enabling teleoperation from a different location. We evaluate of the robot.
both modes in our experiments (Sec. IV).
C. Robot Interface
IV. EXPERIMENTS
TheRobotInterfaceisarobot-specificmodulethatmapsthe In our experiments we seek to answer the following ques-
commands obtained from the Human Interface to the motor tions: (1) What are the benefits of TeleMoMa’s modular-
commands to the robot. In most of our experiments, those are ity? (Sec. IV-A) (2) Can TeleMoMa collect high-quality data
torquesatthejointsoftherobot.Thespecificcontrollersused for imitation learning? (Sec. IV-B), (3) How does TeleMoMa
to compute the torques are not part of the TeleMoMa system perform in remote teleoperation of the robot with possible
but they are necessary to map the action commands obtained network delays? (Sec. IV-C), and (4) What is the effect of
...(a)Cover table(Tiago,realworld) (b)Cover table(Fetch,simulation)
Graspthetableclothanddrapeitoverthetable
(c)Slide chair(Tiago,realworld) (d)Slide chair(Tiago,simulation)
Orienttowardsthechairandpushitunderthetable
(e)Serve bread(Tiago,realworld) (f)Pick pot(Tiago,simulation)
Pickupapacketofbreadanddelivertothebreakfasttable Pickupapotandtransfertoanothertable
(g)Open fridge(HSR,realworld) (h)Re-shelve chips(HSR,realworld)
Openthedoorofafridge Movethemisplacedchipstothelowershelf
(i)Pick up(Tiago,realworld) (j)Dusting(Tiago,realworld)
Graspatowelfromthefloorandplaceitonthetable Dustatablewithbooksrestingontop
Fig.3:TasksinourevaluationofTeleMoMa.Shownaboveistheinitialandgoalstateofeachtask.
different robot embodiments and the gap between simulation in which the user’s pose is tracked with an RGB-D camera
and real in the usability of TeleMoMa? (Sec. IV-D). to control the arms, torso and base motion; and VR+Vision
combining both modalities, in which the robot’s arms are
A. User Study controlled using the Oculus controllers and the base and torso
To assess the performance of different teleoperation modal- motion is controlled via human pose tracking from RGB-D
ities in the TeleMoMa framework, we performed two user data.
studies with the PAL Tiago++ robot. We compared three In the first user study, we compared the three modalities
teleoperation modalities described in Sec. III-A: VR, in which (VR, Vision, VR + Vision) to assess the completion time in
the user controls the robot’s arms with the Oculus controllers twotasks:cover table(Fig.3(a)),inwhichtherobotmust
and the base and torso with the controller joysticks; Vision, grasp a tablecloth with both hands and drape it over a table,and dusting (Fig. 3(j)), in which the robot must dust a 100
table with books resting on top. Both tasks, but especially the VR
dusting task, benefit from the simultaneous motion of base VR+Vision
80
andarm(s),i.e.,whole-bodymotion,asenabledbyTeleMoMa Vision
since the robot is required to navigate around the desk while
60
periodically moving the hands to clear out any dust.
40
We recruited 12 participants with varying levels of tele-
operation experience. Each user was given the same instruc-
20
tions and a brief practice period with each modality. The
order in which users received the devices was randomized.
0
The completion times for successful trials are provided in
cover table dusting
Fig. 4. The only failures observed occurred with the Vision
Fig.4:UserStudy1:CompletionTime.Visionmodalitiesoutperformonly-
modality (3 fails out of 12 dusting trials) due to noise
VRforthemorechallengingdustingtask.Errorbarsdenotethestandard
and inaccuracies in the pose tracking. We observe that in errorofthemean.
the cover table task, performance is comparable across
teleoperationmodalities.However,inthedustingtask,pure
VR VR+Vision
VRisgenerallyslowerthanVR+VisionorVisionalonedueto 200 200
Avg User
thelackofintuitivewhole-bodyteleoperation:becausemoving
180 180 Expert
the base requires using the joysticks on the controllers, users
160 160
tended to only move the arm or the base one at a given time.
The results indicate that on their own, both VR and Vision 140 140
presentdrawbackspertainingtotheirindividualmodalities,but 120 120
when combined in the form of VR + Vision, TeleMoMa can
100 100
overcome their individual drawbacks to enable an improved
80 80
teleoperationexperience.Theseresultssupportempiricallythe
importance of enabling multiple input modalities and their 60 60
combination for teleoperation of mobile manipulators, and 40 40
TeleMoMa’s potential for enabling data collection in more
1 2 3 1 2 3
complex mobile manipulation tasks beyond pick and place. Trial Trial
Fig. 5: User Study 2: User Improvement, Learning Curve. New users
generally improve at completing the pick up task with TeleMoMa across
Inthesecondstudy,wesoughttoassesswhetherTeleMoMa teleoperationmodalities.Transparentlinesshowindividuallearningcurves.
users improve over time by measuring their learning curve.
B. Imitation Learning with TeleMoMa’s Data
Werecruited6participantswithvaryinglevelsofteleoperation
To empirically evaluate the quality of the data collected
experienceandcomparedtwomodalities(VRandVR+Vision,
with TeleMoMa, we train several visuomotor policies with
order randomized) on the pick up task (Fig. 3(i)). In this
behavioral cloning [44] using the data collected on a Tiago++
task, the robot must lower its torso in order to grasp a towel
robot (real). We consider three diverse mobile manipulation
from the floor, hand the towel from one hand to the other,
tasks:
navigate to a table, and place the towel on the table. Users
completed three consecutive trials with each modality and • cover table: Similar to the one described in
completion times were recorded. The results are visualized Sec. IV-A, the tasks involves bimanual grasping of a
in Fig. 5. We observe that new users generally improve at tablecloth and draping it over a table (Fig. 3(a)).
completing tasks with the system, with an average decrease • slide chair:Abimanualtask,thatrequirestherobot
of 29% and 26% in task completion time over three trials for to navigate and align itself behind a chair, grasp it, and
the VR and VR + Vision respectively. The completion times push the chair under a table (Fig. 3(c)).
were generally similar between VR and VR + Vision, with • serve bread: In a real kitchen setting, the robot is
some slower times in the VR + Vision modality owing to required to navigate to the kitchen counter, pick a bag of
the increased difficulty of controlling additional degrees of bread, and deliver it to the breakfast table (Fig. 3(e)).
freedom simultaneously and the additional noise introduced We collected 50 demonstrations each for slide chair
by the vision-based human interface. Despite slowing perfor- andserve breadtasksand100demonstrationsforcover
mance in some trials of this task, the additional capabilities table task using the combined VR + Vision interface of
from VR + Vision have the potential to unlock new whole- TeleMoMa. Additional demonstrations in the cover table
bodycontrolapplicationsoncemastered.Takentogether,these were necessary to allow the policies to learn the necessary
two user studies demonstrate the benefits of TeleMoMa as a accurate grasps on the cloth.
modular teleoperation system. Policy Architecture, Observations and Actions. We used
)s(
emiT
)s(
emiTTABLE II: Performance between IL policies trained with RGB vs. RGBD TABLEIII:ILPolicyperformancescalewithdata.Successesmeasuredover
imagesasinputs.Successesmeasuredover10rollouts. 10rollouts.
CoverTable SlideChair ServeBread CoverTable SlideChair ServeBread
Modality RGB RGB-D RGB RGB-D RGB RGB-D Fractionofdata 50% 100% 50% 100% 50% 100%
BC 60 60 40 60 20 40 BC 60 60 40 60 30 40
BC-RNN 70 90 50 80 30 70 BC-RNN 60 90 70 80 40 70
a feed-forward MLP (BC) and a recurrent LSTM based internet. Instead of watching the robot on-site, the demon-
network (BC-RNN) [32] with a sequence length of 10. The strator is provided with camera streams transmitted by the
inputs to all policies included RGB-D images obtained from teleoperation channel from the robot’s onboard sensors. To
two realsense cameras attached on each shoulder of the robot, minimize communication delays, TeleMoMa 1) sends com-
end-effector poses of the hands, gripper state, and the change pressed sensor images from the robot and decompresses them
in the mobile base pose obtained from the odometry of the on the client, and 2) in the case of a vision-based human
robot. The policies output a 17-dimensional action space: 6D interface, TeleMoMa processes the RGB-D images from the
Cartesiandeltasandagrippercommandforeachofthehands, vision interface on the client side and only sends the action
and linear and angular velocities for the base. commandsovertheteleoperationchannel.Forotherinterfaces,
Comparing Input Modalities. To analyze the importance the demonstrated action commands are directly sent to the
of depth sensing in learning mobile manipulation tasks, we TeleMoMa’s robot interface.
train two sets of policies: the first set was trained exclusively WedemonstratetheremoteteleoperationcapabilityofTele-
on RGB observations, while the second combined RGB and MoMa on several combinations of robot hardware and user
Depth. The performance of the two sets of policies for each interfaces. To evaluate the effects of communication delays,
of the tasks is summarized in Table II. Our analysis reveals a we compare the task completion time between on-site and
consistenttrend:irrespectiveofthepolicyarchitecture,thein- remote demonstrations using Tiago++ and Toyota HSR each
clusion of depth information markedly enhances performance on two different tasks. The cover table and the slide
across all tasks. Qualitatively, we observe that policies trained chair tasks are completed using Tiago++ with the on-site
using depth can position the base better, significantly improv- VR+Visioninterfaceandthreeremoteinterfaces(VR,Vision,
ing the efficacy of subsequent arm actions. These findings VR + Vision). The re-shelve chips task, in which the
suggest that depth information is a crucial component for the robot must move the misplaced chips to the lower shelf (Fig.
development of effective mobile manipulation policies, and 3(h)), and the open fridge task, in which the robot must
that the strong dependency between base and arm actions is open a fridge (Fig. 3(g)), are completed using HSR with
one of the main challenges in IL for mobile manipulation. the Vision interface. The demonstrations are provided by an
Performance with Different Amounts of Data. To in- expertuserofeachrobot.TheWi-Fispeedisabout100Mbps
vestigate how data volume influences policy performance, as measured on the HSR. Fig. 6(a) shows the completion
we experimented with two distinct policy groups: the first time in each modality averaged over 3 runs. We observe that
group was trained using the complete dataset we gathered remote human demonstrators have slower reaction times due
for each task, while the second group utilized only 50% of to delays and limited resolutions of the camera streams, but
these collected demonstrations. The results are summarized TeleMoMaprovidesthecapabilitytosuccessfullycompletethe
in Table III; we observe that policies trained with the full tasks under regular network conditions. We expect to be able
dataset consistently outperform those trained with half the todemonstratetheseremoteteleoperationcapabilitiesfromthe
data,demonstratingtheimportanceofdatasetsizeinimitation RSS venue to our lab.
learning, especially in this low-data regime. We additionally
notice that BC-RNN strictly outperforms regular BC in all D. Comparing Different Embodiments and Sim vs. Real
tasks,demonstratingthesignificanceoftemporaldependencies In the final set of experiments, we seek to study how the
for learning mobile manipulation tasks. domain(simvs.real)andthetypeofrobot(Tiagovs.HSRand
In general, the above experiments provide compelling ev- Tiago-sim vs. Fetch-sim) influence the teleoperation behavior
idence that IL policies trained with data collected using for the same tasks.
TeleMoMacanreliablyperformcomplexmobilemanipulation 1) Sim vs. Real: Fig. 6(b) depicts the results of comparing
tasks, thus indicating that TeleMoMa can facilitate high- completion time for cover table and slide chair
quality data collection for imitation learning. We demonstrate tasks in simulation and real environment using a Tiago robot.
more imitation results in the sim environment in Appendix C. We use sim time for simulation evaluation because of Om-
niGibson’s sub-realtime soft-body simulation. By maintaining
C. Remote Teleoperation
consistency across the robot, the task, and the teleoperation
TeleMoMa’s architecture allows a remote demonstrator to interface, we find that for both tasks the completion time in
control the robot from a client computer connected over the simulationandrealareclose,demonstratingthatthesimulation60 60 60
Tiago VR+Vision Remote Tiago Vision Tiago VR+Vision Sim Tiago VR+Vision Tiago Vision
Remote Tiago VR+Vision HSR Vision Sim Tiago VR+Vision Sim Fetch VR+Vision HSR Vision
50 Remote Tiago VR Remote HSR Vision 50 50
40 40 40
30 30 30
20 20 20
10 10 10
0 0 0
Cover Table Slide Chair Re-shelve ChipsOpen Fridge Cover Table Slide Chair Cover Table Slide Chair Pick Pot Re-shelve ChipsOpen Fridge
(a)On-Sitevs.RemoteTeleoperation (b)Realworldvs.Simulation (c)Teleoperatingdifferentrobotembodiments
Fig.6:CompletiontimesindifferentexperimentswithTeleMoMa.Thebarsindicatethemeanandstandarddeviationofseveraltrials(seetext).Fromleftto
right:Comparingcompletiontimesfortasksperformedon-siteandremote,withHSRandTiago;Completiontimesforrealvs.simulatedtaskswithTiago;
Completiontimesfordifferentrobotembodimentsonthesametasksintherealworldandsimulation.TeleMoMaallowsformultipletasksinsimulationand
therealworld,withseveralembodiments
environment in OmniGibson is a good proxy for mobile with a more accurate interface like VR enables accurate arm
manipulation in the real world, and that teleoperating with control and synchronization of base and arm movement, but
TeleMoMa provides a natural mechanism to collect demon- would still benefit from better visual pose-tracking models.
strations in sim. Second, occlusion presents a challenge for the vision-based
2) Comparing Embodiments: We additionally compare modalities, as the camera placement has an impact on the
how the completion time varies as we change the robot being operator’s visibility of the robot’s workspace. This can be
teleoperated by maintaining the task, teleoperation interface mitigated by carefully choosing a camera placement, using
and reality to be consistent. We compare Tiago and HSR on multiplecameras,orrenderingrobotobservationsonascreen.
re-shelve chips and open fridge tasks and depict Extending TeleMoMa to incorporate a puppeteering human
the results in Fig. 6(c, right). We observe that the higher interfacewouldenableevenmoreaccuratetasksatthecostof
number of degrees of freedom offered by Tiago compared mobility.
to HSR allows more fluid motion during teleoperation and In closing, we have demonstrated TeleMoMa, a general,
enables a more efficient (faster) completion of the task. modular, accessible teleoperation system that enables collec-
In simulation, we compare Tiago and Fetch on cover tion of high-quality expert demonstration data for a variety of
table,slide chair,andpick pottasksanddepictthe complex and novel mobile manipulation tasks. We showed
results in Fig. 6(c, left). For the pick pot task, we enabled TeleMoMa’s generality by teleoperating multiple different
sticky grasping (creating a controllable constraint between robots in simulation and reality, and conducted user studies to
hand and object) since the task would be infeasible otherwise verifytheusabilityofthesystem’svariousmodalities.Wehope
for a single-armed robot like Fetch. We observe that Fetch is that our system lowers the barrier of entry for researchers to
faster than Tiago on tasks requiring table-top manipulations, collect high-quality demonstrations for mobile manipulation,
possibly due to Fetch’s larger size and longer arms, making and helps unlock new mobile manipulation capabilities.
manipulation easier for users.
REFERENCES
V. CONCLUSIONS
[1] Aunifiedapproachformotionandforcecontrolofrobot
We presented TeleMoMa, a novel teleoperation system for manipulators: The operational space formulation. IEEE
mobile manipulators that enables versatility through modular- Journal on Robotics and Automation, 3(1):43–53, 1987.
ity.Whilenosingleteleoperationinterfaceprovidesallbenefits [2] Whole-bodydynamicbehaviorandcontrolofhuman-like
ofenablingdexterous,whole-bodymobileteleoperationwhile robots. International Journal of Humanoid Robotics, 1
remaining low cost and scalable, our general, modular tele- (01):29–43, 2004.
operation interface provides the ability to combine multiple [3] Miguel Arduengo, Ana Arduengo, Adria` Colome´, Joan
existing modalities combining also some of the benefits of Lobo-Prat, and Carme Torras. Human to robot whole-
them. This results in a performant data collection system body motion transfer. In 2020 IEEE-RAS 20th Inter-
that scales to many different robots and tasks, as indicated national Conference on Humanoid Robots (Humanoids),
by our user studies, imitation learning, remote teleoperation, pages 299–305. IEEE, 2021.
and comparisons between embodiments and sim vs. real. We [4] Brenna D Argall, Sonia Chernova, Manuela Veloso,
note some limitations of TeleMoMa. First, when tracking and Brett Browning. A survey of robot learning from
human pose from RGB data, noise and inaccuracies can demonstration. Robotics and autonomous systems, 57
impact a user’s ability to accomplish tasks. Combining vision (5):469–483, 2009.
)s(
emiT
)s(
emiT
)s(
emiT[5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Rafailov, Ran Tian, Ria Doshi, Roberto Mart´ın-Mart´ın,
Altman,SimranArora,SydneyvonArx,MichaelSBern- Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Ju-
stein, Jeannette Bohg, Antoine Bosselut, Emma Brun- lian, Samuel Bustamante, Sean Kirmani, Sergey Levine,
skill, et al. On the opportunities and risks of foundation Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song,
models. arXiv preprint arXiv:2108.07258, 2021. Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon
[6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker,
Chebotar,XiChen,KrzysztofChoromanski,TianliDing, StephenTian,SudeepDasari,SuneelBelkhale,Takayuki
DannyDriess,AvinavaDubey,ChelseaFinn,etal. Rt-2: Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao,
Vision-language-action models transfer web knowledge Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao,
to robotic control. arXiv preprint arXiv:2307.15818, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent
2023. Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Bur-
[7] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol gard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin
Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu,
AlexIrpan,EricJang,RyanJulian,etal. Doasican,not Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho,
as i say: Grounding language in robotic affordances. In YoungwoonLee,YuchenCui,YuehhuaWu,YujinTang,
Conference on Robot Learning, pages 287–318. PMLR, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo,
2023. Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment:
[8] Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Robotic learning datasets and RT-X models. https:
Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, //arxiv.org/abs/2310.08864, 2023.
Alexander Herzog, Karl Pertsch, et al. Q-transformer: [10] ShivinDass,KarlPertsch,HejiaZhang,YoungwoonLee,
Scalableofflinereinforcementlearningviaautoregressive Joseph J Lim, and Stefanos Nikolaidis. Pato: Policy
q-functions. In Conference on Robot Learning, pages assisted teleoperation for scalable robot data collection.
3909–3928. PMLR, 2023. arXiv preprint arXiv:2212.04708, 2022.
[9] Open X-Embodiment Collaboration, Abhishek Padalkar, [11] Joseph DelPreto, Jeffrey I Lipton, Lindsay Sanneman,
Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Her- Aidan J Fay, Christopher Fourie, Changhyun Choi, and
zog, Alex Irpan, Alexander Khazatsky, Anant Rai, Daniela Rus. Helping robots learn: a human-robot
AnikaitSingh,AnthonyBrohan,AntoninRaffin,Ayzaan master-apprentice model using demonstrations via vir-
Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bern- tual reality teleoperation. In 2020 IEEE International
hard Scho¨lkopf, Brian Ichter, Cewu Lu, Charles Xu, Conference on Robotics and Automation (ICRA), pages
Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang 10226–10233. IEEE, 2020.
Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline [12] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,
Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Di- Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
eterBu¨chler,DmitryKalashnikov,DorsaSadigh,Edward JonathanTompson,QuanVuong,TianheYu,etal. Palm-
Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue e: An embodied multimodal language model. arXiv
Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, preprint arXiv:2303.03378, 2023.
Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, [13] Hongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren,
Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Jingjing Chen, Ruo Zhang, Weiming Wang, and Cewu
Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Ra- Lu. Low-cost exoskeletons for learning whole-arm ma-
dosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, nipulation in the wild. arXiv preprint arXiv:2309.14975,
Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey 2023.
Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan [14] RoyaFiroozi,JohnathanTucker,StephenTian,Anirudha
Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tomp- Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran
son, Jonathan Yang, Joseph J. Lim, Joa˜o Silve´rio, Jun- Song, Ashish Kapoor, Karol Hausman, et al. Foundation
hyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, models in robotics: Applications, challenges, and the
Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, future. arXiv preprint arXiv:2312.07843, 2023.
Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, [15] Lars Fritsche, Felix Unverzag, Jan Peters, and Roberto
Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan Calandra. First-person tele-operation of a humanoid
Srinivasan,LawrenceYunliangChen,LerrelPinto,Liam robot. In 2015 IEEE-RAS 15th International Conference
Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max- on Humanoid Robots (Humanoids), pages 997–1002.
imilian Du, Michael Ahn, Mingtong Zhang, Mingyu IEEE, 2015.
Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin [16] Zipeng Fu, Tony Z Zhao, and Chelsea Finn. Mo-
Kim,NaoakiKanazawa,NicklasHansen,NicolasHeess, bile aloha: Learning bimanual mobile manipulation with
Nikhil J Joshi, Niko Suenderhauf, Norman Di Palo, low-cost whole-body teleoperation. arXiv preprint
Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver arXiv:2401.02117, 2024.
Kroemer, Pannag R Sanketi, Paul Wohlhart, Peng Xu, [17] Bryan R Galarza, Paulina Ayala, Santiago Manzano, and
Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Marcelo V Garcia. Virtual reality teleoperation systemfor mobile robot manipulation. Robotics, 12(6):163, 1,000 everyday activities and realistic simulation. In
2023. Conference on Robot Learning, pages 80–93. PMLR,
[18] C Gan, J Schwartz, S Alter, M Schrimpf, J Traer, 2023.
J De Freitas, J Kubilius, A Bhandwaldar, N Haber, [28] Jeffrey I Lipton, Aidan J Fay, and Daniela Rus. Baxter’s
M Sano, et al. Threedworld: A platform for interactive homunculus: Virtual reality spaces for teleoperation in
multi-modal physical simulation. Advances in Neural manufacturing. IEEE Robotics and Automation Letters,
Information Processing Systems (NeurIPS), 2021. 3(1):179–186, 2017.
[19] Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu [29] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris
Wang, and Song-Chun Zhu. Vrkitchen: an interactive McClanahan, Esha Uboweja, Michael Hays, Fan Zhang,
3d virtual environment for task-oriented learning. arXiv Chuo-LingChang,MingYong,JuhyunLee,etal. Medi-
preprint arXiv:1903.05757, 2019. apipe:Aframeworkforperceivingandprocessingreality.
[20] Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu InThirdworkshoponcomputervisionforAR/VRatIEEE
Oprea, John Alejandro Castro-Vargas, Sergio Orts- computervisionandpatternrecognition(CVPR),volume
Escolano, Jose Garcia-Rodriguez, and Alvaro Jover- 2019, 2019.
Alvarez. The robotrix: An extremely photorealistic and [30] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan
very-large-scale indoor dataset of sequences with robot Booher, Max Spero, Albert Tung, Julian Gao, John
trajectories and interactions. In 2018 IEEE/RSJ Inter- Emmons,AnchitGupta,EmreOrbay,etal. Roboturk:A
national Conference on Intelligent Robots and Systems crowdsourcingplatformforroboticskilllearningthrough
(IROS), pages 6790–6797. IEEE, 2018. imitation. InConferenceonRobotLearning,pages879–
[21] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, 893. PMLR, 2018.
Yu-WeiChao,QianWan,StanBirchfield,NathanRatliff, [31] Ajay Mandlekar, Jonathan Booher, Max Spero, Albert
and Dieter Fox. Dexpilot: Vision-based teleoperation Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio
of dexterous robotic hand-arm system. In 2020 IEEE Savarese, and Li Fei-Fei. Scaling robot supervision to
International Conference on Robotics and Automation hundreds of hours with roboturk: Robotic manipulation
(ICRA), pages 9164–9170. IEEE, 2020. dataset through human reasoning and dexterity. In 2019
[22] Ryan Hoque, Lawrence Yunliang Chen, Satvik Sharma, IEEE/RSJInternationalConferenceonIntelligentRobots
Karthik Dharmarajan, Brijen Thananjeyan, Pieter and Systems (IROS), pages 1048–1055. IEEE, 2019.
Abbeel, and Ken Goldberg. Fleet-dagger: Interactive [32] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush
robot fleet learning with scalable human supervision. In Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,
Conference on Robot Learning, pages 368–380. PMLR, Silvio Savarese, Yuke Zhu, and Roberto Mart´ın-Mart´ın.
2023. What matters in learning from offline human demon-
[23] GayaneKazhoyan,AlinaHawkin,SebastianKoralewski, strations for robot manipulation. In arXiv preprint
Andrei Haidu, and Michael Beetz. Learning motion arXiv:2108.03298, 2021.
parameterizations of mobile pick and place actions from [33] Nicolas Mansard, Olivier Stasse, Paul Evrard, and Ab-
observing humans in virtual environments. In 2020 derrahmane Kheddar. A versatile generalized inverted
IEEE/RSJInternationalConferenceonIntelligentRobots kinematicsimplementationforcollaborativeworkinghu-
and Systems (IROS), pages 9736–9743. IEEE, 2020. manoidrobots:Thestackoftasks. In2009International
[24] Jens Kober, J. Andrew Bagnell, and Jan Peters. Re- conferenceonadvancedrobotics,pages1–6.IEEE,2009.
inforcement learning in robotics: A survey. The In- [34] PabloMartinez-Gonzalez,SergiuOprea,AlbertoGarcia-
ternational Journal of Robotics Research, 32(11):1238– Garcia, Alvaro Jover-Alvarez, Sergio Orts-Escolano, and
1274, September 2013. ISSN 0278-3649, 1741-3176. JoseGarcia-Rodriguez.Unrealrox:anextremelyphotore-
doi: 10.1177/0278364913495721. URL http://journals. alisticvirtualrealityenvironmentforroboticssimulations
sagepub.com/doi/10.1177/0278364913495721. and synthetic data generation. Virtual Reality, 24:271–
[25] Franziska Krebs, Andre Meixner, Isabel Patzer, and 288, 2020.
Tamim Asfour. The kit bimanual manipulation dataset. [35] Yutaro Matsuura, Kento Kawaharazuka, Naoki Hiraoka,
In 2020 IEEE-RAS 20th International Conference on Kunio Kojima, Kei Okada, and Masayuki Inaba. De-
Humanoid Robots (Humanoids), pages 499–506. IEEE, velopment of a whole-body work imitation learning
2021. system by a biped and bi-armed humanoid. In 2023
[26] Christian Lenz and Sven Behnke. Bimanual telemanip- IEEE/RSJInternationalConferenceonIntelligentRobots
ulation with force and haptic feedback through an an- and Systems (IROS), pages 10374–10381. IEEE, 2023.
thropomorphic avatar system. Robotics and Autonomous [36] Jordi Pages, Luca Marchionni, and Francesco Ferro.
Systems, 161:104338, 2023. Tiago:themodularrobotthatadaptstodifferentresearch
[27] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gok- needs. In International workshop on robot modularity,
men, Sanjana Srivastava, Roberto Mart´ın-Mart´ın, Chen IROS, volume 290, 2016.
Wang,GabraelLevine,MichaelLingelbach,JiankaiSun, [37] LuigiPenco,KazuhikoMomose,StephenMcCrory,Dex-
et al. Behavior-1k: A benchmark for embodied ai with ton Anderson, Nicholas Kitchel, Duncan Calvert, andRobert J Griffin. Mixed reality teleoperation assistance arXiv:2202.10448, 2022.
for direct control of humanoids. IEEE Robotics and [50] Christopher Stanton, Anton Bogdanovych, and Edward
Automation Letters, 2024. Ratanasena. Teleoperation of a humanoid robot using
[38] Amartya Purushottam, Christopher Xu, Yeongtae Jung, full-body motion capture, example movements, and ma-
and Joao Ramos. Dynamic mobile manipulation via chine learning. In Proc. Australasian Conference on
whole-body bilateral teleoperation of a wheeled hu- Robotics and Automation, volume 8, page 51, 2012.
manoid. IEEE Robotics and Automation Letters, 2023. [51] Octo Model Team, Dibya Ghosh, Homer Walke, Karl
[39] Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey
Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dieter Hejna, Charles Xu, Jianlan Luo, et al. Octo: An open-
Fox. Anyteleop: A general vision-based dexterous source generalist robot policy, 2023.
robot arm-hand teleoperation system. arXiv preprint [52] Albert Tung, Josiah Wong, Ajay Mandlekar, Roberto
arXiv:2307.04577, 2023. Mart´ın-Mart´ın, Yuke Zhu, Li Fei-Fei, and Silvio
[40] Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Savarese. Learning multi-arm manipulation through
Bo¨lo¨ni, and Sergey Levine. Vision-based multi-task collaborative teleoperation. In 2021 IEEE International
manipulation for inexpensive robots using end-to-end Conference on Robotics and Automation (ICRA), pages
learningfromdemonstration. In2018IEEEinternational 9212–9219. IEEE, 2021.
conference on robotics and automation (ICRA), pages [53] Jean Vertut and Philippe Coiffet. Teleoperations and
3758–3765. IEEE, 2018. robotics: evolution and development. Prentice-Hall, Inc.,
[41] EllisRatner,BenjaminCohen,MikePhillips,andMaxim 1986.
Likhachev. A web-based infrastructure for recording [54] HomerWalke,KevinBlack,AbrahamLee,MooJinKim,
user demonstrations of mobile manipulation tasks. In Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-
2015 IEEE International Conference on Robotics and Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan
Automation (ICRA), pages 5523–5530. IEEE, 2015. Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2:
[42] Harish Ravichandar, Athanasios S Polydoros, Sonia A dataset for robot learning at scale. In Conference on
Chernova, and Aude Billard. Recent advances in robot Robot Learning (CoRL), 2023.
learning from demonstration. Annual review of control, [55] Baocheng Wang, Zhijun Li, Wenjun Ye, and Qing Xie.
robotics, and autonomous systems, 3:297–330, 2020. Development of human-machine interface for teleopera-
[43] Dongseok Ryu, Jae-Bok Song, Changhyun Cho, tion of a mobile manipulator. International Journal of
Sungchul Kang, and Munsang Kim. Development of Control, Automation and Systems, 10:1225–1231, 2012.
a six dof haptic master for teleoperation of a mobile [56] David Whitney, Eric Rosen, Daniel Ullman, Elizabeth
manipulator. Mechatronics, 20(2):181–191, 2010. Phillips, and Stefanie Tellex. Ros reality: A virtual
[44] StefanSchaal.Isimitationlearningtheroutetohumanoid reality framework using consumer-grade hardware for
robots?Trendsincognitivesciences,3(6):233–242,1999. ros-enabled robots. In 2018 IEEE/RSJ International
[45] Max Schwarz, Christian Lenz, Raphael Memmesheimer, Conference on Intelligent Robots and Systems (IROS),
Bastian Pa¨tzold, Andre Rochow, Michael Schreiber, and pages 1–9. IEEE, 2018.
SvenBehnke. Robustimmersivetelepresenceandmobile [57] Melonee Wise, Michael Ferguson, Derek King, Eric
telemanipulation: Nimbro wins ana avatar xprize finals. Diehr, and David Dymesich. Fetch and freight: Standard
arXiv preprint arXiv:2303.03297, 2023. platforms for service robot applications. In Workshop on
[46] Mingyo Seo, Steve Han, Kyutae Sim, Seung Hyeon autonomous mobile service robots, pages 1–6, 2016.
Bang, Carlos Gonzalez, Luis Sentis, and Yuke Zhu. [58] JosiahWong,AlbertTung,AndreyKurenkov,AjayMan-
Deep imitation learning for humanoid loco-manipulation dlekar, Li Fei-Fei, Silvio Savarese, and Roberto Mart´ın-
through human teleoperation. In 2023 IEEE-RAS 22nd Mart´ın.Error-awareimitationlearningfromteleoperation
International Conference on Humanoid Robots (Hu- data for mobile manipulation. In Conference on Robot
manoids), pages 1–8. IEEE, 2023. Learning, pages 1367–1378. PMLR, 2022.
[47] Adam Setapen, Michael Quinlan, and Peter Stone. Mar- [59] Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and
ionet: Motion acquisition for robots through iterative Pieter Abbeel. Gello: A general, low-cost, and intuitive
online evaluative training. In Ninth International Con- teleoperation framework for robot manipulators. arXiv
ference on Autonomous Agents and Multiagent Systems preprint arXiv:2309.13037, 2023.
- Agents Learning Interactively from Human Teachers [60] TakashiYamamoto,KojiTerada,AkiyoshiOchiai,Fumi-
Workshop (AAMAS - ALIHT), May 2010. noriSaito,YoshiakiAsahara,andKazutoMurase. Devel-
[48] Bruno Siciliano, Oussama Khatib, and Torsten Kro¨ger. opment of human support robot as the research platform
Springer handbook of robotics, volume 200. Springer, ofadomesticmobilemanipulator. ROBOMECHjournal,
2008. 6(1):1–15, 2019.
[49] Aravind Sivakumar, Kenneth Shaw, and Deepak Pathak. [61] Taozheng Yang, Ya Jing, Hongtao Wu, Jiafeng Xu,
Robotic telekinesis: Learning a robotic hand imita- Kuankuan Sima, Guangzeng Chen, Qie Sima, and Tao
tor by watching humans on youtube. arXiv preprint Kong. Moma-force:Visual-forceimitationforreal-worldmobile manipulation. In 2023 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS),
pages 6847–6852. IEEE, 2023.
[62] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee,
Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep
imitation learning for complex manipulation tasks from
virtual reality teleoperation. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pages
5628–5635. IEEE, 2018.
[63] Zhijun Zhang, Yaru Niu, Ziyi Yan, and Shuyang Lin.
Real-timewhole-bodyimitationbyhumanoidrobotsand
task-oriented teleoperation using an analytical mapping
method and quantitative evaluation. Applied Sciences, 8
(10):2005, 2018.
[64] TonyZZhao,VikashKumar,SergeyLevine,andChelsea
Finn. Learning fine-grained bimanual manipulation with
low-cost hardware. arXiv preprint arXiv:2304.13705,
2023.APPENDIX
100
A. Criterion for Table 1 Ground Truth
RGBD
We provide a detailed explanation for each column of
90
Table 1, including the criteria used to categorize methods.
• Teleoperation Support
80
1) Cost / Accessibility: We identified three tiers of
price based on commercially available systems or
70
disclosed cost.
f: $0 – 1,000 (VR, Vision, Phone)
ff: $1,000 – 10,000 (Mocap Systems) Low Randomness High Randomness
fff: $10,000+ (Custom Hardware)
Fig.7:ILResultsinSimulation.PolicywithRGBDinputyieldscomparable
performancetopolicywithgroundtruthchairpositionsasinput.
2) Modular: True if the method is modular in the
sense that it supports multiple input modalities or
combinationsthereof.TeleMoMaistheonlymethod
1) Mobile Phone: We created an app using the ARKit
that meets this criteria.
development kit to track the position and orientation of the
3) Modality: Modality describes the human interface
mobile phone, which sends commands over the network.
used for teleoperation (e.g. virtual reality (VR),
Similar to Virtual Reality Controller (Sec. IV-A2), the end
puppeteering with a kinematically similar device,
effector is commanded in the task space and the relative pose
motion capture systems (Mocap), etc.).
displacement per frame of the mobile phone is calculated and
• Robot Support mapped to the robot end effector. The gripper is controlled
1) Bimanual: True if the paper demonstrates bimanual by dedicated buttons in the mobile app. Additionally, simul-
teleoperation. taneous control of left and right arms can be facilitated if two
2) Height Control: True if the paper demonstrates mobilephonesarerunningtheapp,eachphonecontrollingone
control of the robot’s torso joint. ofthearms.Mobilephonescurrentlydon’tsupportnavigation
3) Whole-Body Teleoperation: True if simultaneous capabilities, but can be combined with other modalities such
arm and base motion is enabled by the method. astheVision-basedHumanInterface(Sec.IV-A1)tofacilitate
4) RobotAgnostic:Trueifthemethodworksformany mobile base movements.
different robots; false if it is specific to a particular 2) Spacemouse: Spacemouse has only 6-degrees of free-
platform. dom, which is why we use mode switching, and control
5) Action Space: “EE Pose(s)” denotes control of the each part of the robot independently. The users can switch
robot’s end-effector(s) in Cartesian space, whereas modes by pressing one of the side buttons of the spacemouse
“Joint Pos.” indicates joint-space control for the and switch between controlling left arm, right arm, base and
arms and/or torso. Base Vel. indicates control of torso. Two spacemouse’ can also be used simultaneously
the base velocity; TRILL [46] allows users to for controlling each of the arms and minimizing the mode
select among predefined gaits with a VR con- switching. The displacement of the spacemouse in each of
troller, denoted “Gait”. MOMA-Force enables tele- the 6 degrees of freedom is tracked and sent as the delta
operation of end-effector Cartesian pose through commands to control the arms. For the base and torso, only
kinesthetic teaching and additionally records de- the required displacements are used to send commands, while
sired end-effector wrenches, denoted “EE Pose and the remaining ones are discarded. The gripper can be toggled
Wrench”. TeleMoMa allows users to control end- by pressing the remaining side button when the spacemouse
effectorCartesianpose,basevelocity,andtorsojoint modeiscontrollingthecorrespondingarm.Spacemousegains
position;it isalsoreadilyextensible tojointcontrol significantly from modularity offered by TeleMoMa, by mini-
whentrackinghumanpose,butthisisleftforfuture mizing mode switching thus gaining more fluid control of the
work. robot.
3) Keyboard: Keyboardpressesareasynchronouslyreadby
thedevicelistenersandeachkeyismappedtoasingleDoFof
B. Method Details
themobilemanipulator.Eachkeyincreases/decreasesoneof
Following we describe how TeleMoMa facilitates the use the DoFs in the Cartesian space by some preset amount. This
of mobile phones, spacemouse, and keyboards as part of its results in a large number of keys that the teleoperator has to
Human Interface (Sec. IV-A). We are also open-sourcing the remember for controlling the robot. Instead, using a smaller
codetothecommunitytofacilitateplug-and-playteleoperation set of keys for controlling for instance, just the base, while
for mobile manipulators to improve data collection efficiency. controlling arms with something more intuitive such as the
)%(
egaR
sseccuSHyperparameters Value
and base motions. The results and their analysis are presented
BehaviorCloning(BC)
in Sec. V-B. We used RoboMimic [32] for training the
trainsteps(x500) 500
batchsize 32 policies. Comprehensive details of the policy architecture and
optimizer Adam hyperparameters used for training are provided in Table IV.
learningrate 1e-4
Notethatthesamehyperparameterswereusedacrossalltasks,
image&depthencoder resnet-18
policy(wxd) 512x2 and across simulation and real environments.
actionparameterization GMM Furthermore,inanefforttofacilitateandencourageongoing
RecurrentBC(BC-RNN)
research in mobile manipulation, the dataset collected on all
trainsteps(x500) 500
batchsize 16 the tasks will be made available along with the code.
optimizer Adam
learningrate 1e-4
image&depthencoder resnet-18
LSTMhiddendim 1000
LSTMnum.layers 2
skillhorizon 10
actionparameterization GMM
TABLE IV: Hyperparameters for the imitation policies (the hyperparameter
valueswerekeptconsistentacrosstasks)
spacemouse can drastically improve the teleoperation experi-
ence on both the interfaces, minimizing the mode switching
in case of spacemouse, and reducing the number of keys to
keep track of on the keyboard.
C. Imitation Results in Simulation
Weshowtheimitationresultsoftheslide chairtaskin
simulationhere.Wecollected100demosinOmniGibson,and
trained 2 policies using BC with different input observations:
one with RGB-D image from the head camera, and the other
withoraclechairpositionsinbothworldframeandrobotbase
framefromthesimulationenvironment.Robotproprioception,
including end effector poses for two arms in base frame,
and the base position and velocity in world frame, are also
providedasobservationinput.Weevaluatedthepolicyontwo
taskconfigurations:firstwithlowrandomness,wherethechair
positionisuniformlysampledwithin0.2metersparalleltothe
robot, and second with high randomness, where the sampling
interval is 1 meters. Each policy is evaluated with 25 rollouts
under these conditions.
The results are shown in Fig. 7. We observed that, the
performance of policies under high randomness is worse than
under low randomness, which is expected because of the
increased difficulty. We additionally observe that in both low
and high randomness settings, policy trained with RGB-D
input performs comparable to the one trained with ground
truth chair positions, indicating that the policies are able to
extract meaningful environment specific details from images
and depth. Qualitatively, we observe that the causes of failure
includemisalignmentbetweentherobotandthechair,slippage
of robot grippers, and knocking over the chair due to the
application of excessive force.
D. Imitation Learning Policy Hyperparameters
We performed imitation learning on one simulated (slide
chair – Appendix Sec. C) and three real world tasks –
cover table (Fig. 3(a)), slide chair (Fig. 3(b)) and
serve bread (Fig. 3(c)), that require synchronized hand