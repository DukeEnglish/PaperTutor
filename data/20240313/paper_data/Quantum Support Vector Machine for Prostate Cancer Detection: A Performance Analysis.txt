Quantum Support Vector Machine for Prostate Cancer
Detection: A Performance Analysis
W. El Maouaki‚àó1, T. SAID1, and M. BENNAI1,2
1Quantum Physics and Magnetism Team, LPMC, Faculty of Sciences Ben M‚ÄôSik,
Hassan II University of Casablanca, Morocco
2Lab of High Energy Physics, Modeling and Simulations, Faculty of Sciences,
University Mohammed V-Agdal, Rabat, Morocco
Abstract
This study addresses the urgent need for improved prostate cancer detection methods by har-
nessing the power of advanced technological solutions. We introduce the application of Quantum
SupportVectorMachine(QSVM)tothiscriticalhealthcarechallenge,showcasinganenhancement
indiagnosticperformanceovertheclassicalSupportVectorMachine(SVM)approach. Ourstudy
notonlyoutlinestheremarkableimprovementsindiagnosticperformancemadebyQSVMoverthe
classicSVMtechnique,butitdelvesintotheadvancementsbroughtaboutbythequantumfeature
map architecture, which has been carefully identified and evaluated, ensuring it aligns seamlessly
with the unique characteristics of our prostate cancer dataset. This architecture succeded in cre-
ating a distinct feature space, enabling the detection of complex, non-linear patterns in the data.
The findings reveal not only a comparable accuracy with classical SVM (92%) but also a 7.14%
increaseinsensitivityandanotablyhighF1-Score(93.33%). Thisstudy‚Äôsimportantcombination
of quantum computing in medical diagnostics marks a pivotal step forward in cancer detection,
offering promising implications for the future of healthcare technology.
Keywords: quantumcomputing,machinelearning,quantummachinelearning,quantumsupport
vector machine, Prostate cancer
1 Introduction
Prostate cancer, characterized by its high incidence rate among men worldwide [1], presents a critical
healthcare challenge [2]. The prognosis of this disease is heavily dependent on the timeliness and
accuracy of its detection. Early-stage identification of this cancer not only offers a greater array of
therapeuticoptionsbutalsosignificantlyenhancesthechancesofsuccessfultreatmentoutcomes[3,4].
This underscores the urgent need for advanced diagnostic methodologies that can effectively intercept
the disease in its nascent stages, thereby altering its path toward a more positive prognosis.
Machine learning, particularly through the implementation of support vector machines (SVMs),
has marked a significant step forward in the interpretation of intricate biomedical data, which could
be pivotal in addressing the early detection challenges of prostate cancer [5]. While traditional SVMs
havedemonstratedproficiencyindataclassification[6‚Äì9],theyfacesignificantconstraintsinscalability
and computational efficiency when applied to the expansive and high-dimensional datasets typical in
prostate cancer research. This limitation calls for innovative approaches that can effectively harness
the full potential of machine learning in this critical field of healthcare.
InthequesttoovercomethelimitationsimposedbytraditionalSVMsinthelandscapeofoncolog-
ical diagnostics, Quantum Support Vector Machines (QSVMs) have emerged as a frontier technology
[10, 11]. This algorithm leverages the principles of quantum computing, such as superposition and
entanglement,toprocessinformationinafundamentallydifferentmanner. Unliketheirclassicalcoun-
terparts, QSVMs are uniquely equipped to handle multidimensional data. This is due to their ability
‚àóCorrespondingauthor. Email: walid.elmaouaki-etu@etu.univh2c.ma
1
4202
raM
21
]GL.sc[
1v65870.3042:viXrato perform complex linear algebra computations, which are the core of SVMs, exponentially faster on
quantum processors. QSVMs offer a promising avenue to not only match but potentially surpass the
accuracy of classical SVMs, with the added benefit of reduced computational times [12, 13].
This study is based on the systematic evaluation of Quantum Support Vector Machines (QSVMs)
to determine their effectiveness in the intricate field of prostate cancer detection. A pivotal aspect
of this work is the identification and evaluation of an optimal feature map architecture, specifically
tailored to the dataset at hand, with its mathematical framework meticulously detailed. We aim to
rigorously benchmark the QSVM‚Äôs diagnostic performance against traditional SVMs across all perfor-
mance metrics, encompassing accuracy, specificity, sensitivity, and the F1 score, thereby quantifying
thepracticaladvantagesofquantum-enhancedmachinelearning. Additionally,theresearchisdesigned
torigorouslyassesstheQSVM‚Äôsoperationalfeasibilityonactualclinicaldata,exploringitsrobustness
anddiagnosticability. ThecomparativestudyhighlightsthatourQSVMmatchestheaccuracyoftra-
ditional SVMs and surpasses them in sensitivity and F1 score due to the high dimensional advantage
of quantum Hilbert space. This underlines QSVM‚Äôs improved detection of true positives, crucial for
clinical reliability and reducing false negatives in prostate cancer screening.
2 Methodology
To evaluate the potential benefits of QSVM in prostate cancer detection, a comparative analysis was
conducted using a dataset derived from prostate imaging results and patient attributes. The data
were preprocessed and transformed into a quantum-ready format suitable for QSVM classification.
The performance of QSVM was compared to classical SVM using performance metrics (accuracy,
precision, sensitivity, specificity, and F1-Score).
2.1 Dataset and Processing Steps
Quantum information processing systems are characterized by features like quantum superposition
andquantumentanglement,givingthemthepotentialtoenhanceimagerecognitionspeedsinmachine
learning. Certain quantum image processing techniques have demonstrated the potential to outpace
their classical equivalents dramatically [14‚Äì16]. Quantum classifiers have been applied to diverse
image datasets in the literature [17‚Äì21]. In this study, our attention is on the Prostate cancer dataset,
which is widely perceived as a significant challenge. The dataset used in this study is the Kaggle
ProstateCancerDataset[22]. Thedatasetconsistsof100observationsand9variables(outofwhich8
numericvariablesandonecategoricalvariable)whichareasfollows: Radius,Texture,Perimeter,Area,
Smoothness, Compactness, Symmetry, Fractal dimension, diagnosis result (labels). This problem is a
two-category classification problem: Cancerous and non-Cancerous Prostate.
In the data preprocessing phase, our initial dataset exhibited an imbalance, with a predominance
of label 1 over label 0 data points. This can potentially cause machine learning models to favor the
dominant label, leading to the misclassification of the minority label. To counter this, we utilized the
RandomOverSampler class, strategically duplicating examples from the minority class to balance the
training dataset. Subsequently, we standardized the dataset using the StandardScaler class from the
sklearn.preprocessing module. This process scales the features to have a mean of 0 and a standard
deviation of 1, enhancing feature comparability and often being a prerequisite for optimal machine
learningperformance. Theseoperationsextendourdatato124datasamples. ForQSVM,weaddeda
normalization step using the MinMaxScaler class. This scaled the features to range between 0 and 1,
aidingmodelinterpretabilityandfacilitatethesubsequentquantumdataencodingprocess,asdetailed
in Section 2.3. In this encoding process, features are mapped onto rotation gates, operations that are
optimized for values (or angles) in the 0 to 1 range. The conversion function of the MinMax method
[23] is given by equation 1. Finally, the processed data was divided into training (80%) and testing
(20%) subsets.
X‚àíX¬∑min( axis =0)
X = (max‚àímin)+min (1)
S X¬∑max(axis=0)‚àíX¬∑min(axis=0)
SVM and QSVM can be sensitive to the scale and distribution of the input data. Hence, prepro-
cessing is crucial for achieving meaningful and accurate results. In the following section, we‚Äôll discuss
how SVM and QSVM work and how they were employed to classify our prepared data.
22.2 Support Vector Machines (SVMs)
SVMs are a pivotal technique in machine learning, primarily designed for binary classification tasks.
Attheircore, SVMsworkbyintegratinglinearalgebraandoptimization. Givendatavectors, denoted
as x ‚àà Rr with r ‚â• 1, each associated with a class label -1 or 1, the primary objective of SVMs is to
identifytheoptimalhyperplanethatseparatestheseclasseswiththebroadestmargin. SVMconstructs
ahyperplanedescribedbytheequationw‚Éó¬∑‚Éóx+b=0. Foragiventraininginstance‚Éóx belongingtothe
i
positive category, the condition w‚Éó ¬∑‚Éóx+b ‚â• 1 must hold. Conversely, for ‚Éóx in the negative category,
i
the condition w‚Éó ¬∑‚Éóx+b‚â§‚àí1 is expected, see Fig. 1.
ùúî.ùë•‘¶+ùëè = ‚àí1
ùúî.ùë•‘¶+ùëè = 0
ùúî.ùë•‘¶+ùëè = +1
margin
Fig.1: Illustrationofthemaximum-marginhyperplaneforSVMclassification. Themarginhyperplanes
for support vectors are depicted by the dashed lines.
More formally, we extract a set of training samples, X ={x ,...,x }, from an underlying proba-
1 M
bility distribution P(x,y), and each vector has an associated label y ={y ,...,y }. Using this data,
1 M
SVMs develop a function, f : Rr ‚Üí {‚àí1,1}. This function is tasked with two primary goals: achiev-
ing high accuracy in predicting the actual labels and maximizing the margin, which is the distance
betweenthetwoclassesonthehyperplane. OncetheSVMistrainedwiththisfunction, itisequipped
to classify new data points that are sampled from the same probability distribution.
The decision function for a given new input x is:
f(x)=sign(cid:0) wTx+b(cid:1)
(2)
Where w represents the weight vector, x denotes the feature vectors, and b is the bias term. To
achieve this optimal hyperplane, SVM employs a convex optimization strategy that minimizes the
following primal optimization problem:
n
min1 ‚à•w‚à•2+C(cid:88) max(cid:0) 0,1‚àíy (cid:0) wTx +b(cid:1)(cid:1) (3)
w 2 i i
i=1
Here, the term max(cid:0) 0,1‚àíy (cid:0) wTx +b(cid:1)(cid:1) represents the hinge loss, which measures the misclassi-
i i
fication or violation of the margin. C is a regularization parameter, balancing the trade-off between
maximizing the margin and minimizing the contribution of the hinge loss.
We begin by presenting the standard kernelized support vector machines from classical machine
learning. We then describe how these techniques can be extended to the quantum setting by using
quantumcircuitstodefinethefeaturemaps. Towrapupthissection,weshowthevariationalquantum
circuits that compute the kernel for our application case.
2.2.1 Kernelized support vector machines
A key advantage of SVMs is their ability to efficiently perform nonlinear classification using kernel
functions, which implicitly map the input data into high-dimensional feature spaces [24]. Formally,
let‚Äôs represent this mapping as œï(‚Éóx), where œï denotes the feature map that transforms input vectors
‚Éóx into a higher dimensionality.
3The effectiveness of SVMs in this context lies in the ‚Äôkernel trick‚Äô, a technique that circumvents
the need for explicit computation of these high-dimensional transformations. Instead, SVMs utilize
the dot product between vectors in the transformed space. This is achieved by introducing a kernel
function K(‚Éóx ,‚Éóx ), which effectively represents the dot product in this elevated feature space:
i j
K(‚Éóx ,‚Éóx )=œï(‚Éóx )¬∑œï(‚Éóx ) (4)
i j i j
One of the popular kernel functions is the Radial Basis Function (RBF)
(cid:32) (cid:33)
‚à•‚Éóx ‚àí‚Éóx ‚à•2
K(‚Éóx ,‚Éóx )=exp ‚àí i j (5)
i j 2œÉ2
where œÉ is the width parameter. The RBF kernel allows SVMs to effectively delineate non-
linearboundarieswithouttheneedtodirectlyengagewithcomputationallyintensivehigh-dimensional
spaces.
Rather than addressing the primal optimization problem, contemporary methods [25] typically
focus on the dual form of equation 3:
n n
(cid:88) 1 (cid:88)
max Œ± ‚àí Œ± Œ± y y K(x ,x )
Œ± i 2 i j i j i j
i=1 i,j=1
subject to 0‚â§Œ± ‚â§C (6)
i
n
(cid:88)
Œ± y =0
i i
i=1
Once the Lagrange multipliers Œ± are computed, the decision function in the transformed space be-
i
comes:
(cid:32) n (cid:33)
(cid:88)
f(x)=sign Œ± y K(x ,‚Éóx)+b (7)
i i i
i=1
Here, it‚Äôs important to emphasize that only the support vectors (data points for which Œ± > 0)
i
actually affect this decision. This aspect highlights the efficiency of SVM, as it uses only a subset of
the training data to make predictions, rather than the entire dataset. Additionally, we can represent
the weight vector w in terms of these multipliers and the training data as w
=(cid:80)n
Œ± y x .
i=1 i i i
2.3 Quantum Support Vector Machine (QSVM)
QSVM is an innovative fusion of quantum computing and SVM, initially introduced by Rebentrost et
al. [10], and has since evolved, with further variants and enhancements being developed [26‚Äì28]. The
QSVM approach presents a unique method for data processing and analysis. In traditional SVM, the
timecomplexityscalesasO(poly(NM)),withN representingthefeaturespacedimensionsandM the
number of training samples. In contrast, QSVMs demonstrate a performance of O(log(NM)) in both
trainingandclassificationphases[29]. Thissignificantaccelerationisattributedtotheuseofquantum
circuits, which enable parallel computation of vector inner products.
In the classical case, we have used the tractable kernel corresponding to radial basis functions.
However, we suggest using a quantum feature map to compute this kernel, as in equation 4, which
should be assessed on a quantum computer. In the quantum case, we encode our data vector x into
a quantum state |œà(x)‚ü©, where |œà(.)‚ü© is a feature map. The feature map could be expressed as an
application of a 2n√ó2n unitary operator U to the quantum state |0‚ü©n as |œï(x)‚ü©=U |0n‚ü© (the full
œï(x)
expression is in equation 9), n is the dimension of the data.
Toadaptthefunctioninequation7foraquantummachine,it‚Äôsimportanttoselectanappropriate
quantum feature map. This decision subsequently determines the design of our quantum circuit.
Several methods are available for picking an apt feature map, as discussed in [30], such feature
maps we can name ZFeatureMap, PauliFeatureMap, and ZZFeatureMap who play a critical role in
encoding classical data into quantum states. These feature maps employ quantum gates like Pauli
rotations (X, Y, Z) to convert data features into rotations in the quantum state space. They crucially
introduce entanglement between qubits using gates like controlled-Z, allowing the quantum system to
represent complex data correlations in high-dimensional Hilbert spaces. These entanglements differ
4in topology patterns, for instance, some employ nearest-neighbor entanglements, creating connections
betweenadjacentqubits,whereasothersuseanall-to-all(fullyconnected)entanglementscheme,linking
every qubit with every other. This diversity in entanglement topologies allows for varying degrees of
complexity in representing data relationships.
In our case, we explored and determined the best feature map architecture for our dataset, and
subsequently the ZZFeaturemap fully entanglement architecture succeeded in capturing the richness
of our Prostate cancer dataset. For example, Fig. 3 depicts a 4 qubits quantum circuit of the ZZFea-
turemap fully entanglement feature map designed to process a dataset comprising 4 dimensions. Our
Prostatefeaturedatais8dimensional,sothefeaturemapwillapply8qubits(28√ó28)unitaryoperator
U to obtain estimates of kernel K. The kernel function can be computed using the Hilbert-Schmidt
inner product of the quantum states obtained from the two data points x and x .
i j
K(x ,x )=|‚ü®0n|U‚Ä† U |0n‚ü©|2 =|‚ü®œà(x )|œà(x )‚ü©|2 (8)
i j œï(xj) œï(xi) i j
Where ‚ü®œà(x ) | œà(x )‚ü© is the inner products (or the overlap) between feature representations (or
i j
quantum states) of two data points and can be estimated using a quantum circuit, as depicted in the
Fig. 2.
The ZZ feature mapping unitary operation is expressed in equation 9, and implemented in Fig. 3:
|œï(x)‚ü©=U œï(x)(cid:12) (cid:12)0‚äón(cid:11) =U 2fu nll‚àíent(cid:0) ‚äón q=1(R z(x q)H)(cid:1)(cid:12) (cid:12)0‚äón(cid:11) (9)
and
n‚àí1 n
(cid:89) (cid:89)
Ufull-ent := E (10)
q,k
q=1k=q+1
where
E
q,k
=e‚àíiŒ¶(xq,xk)ZqZk, Œ¶(x q,x k)=(œÄ‚àíx q)(œÄ‚àíx k) (11)
Z is the is the Pauli operator. Next, we explain how to compute the complete kernel matrix for n
data points from the measurement of the quantum circuit.
2.3.1 Derivation of the Kernel Matrix from Quantum Measurements
To determine the kernel value in equation 8, we create a quantum state by applying the quantum
featuremapofx andthentheadjointofthequantumfeaturemapforx sequentiallyinthequantum
i j
circuit:
|Œ®‚ü©=|œï(x )‚ü©‚äó|œï‚àó(x )‚ü© (12)
i j
Here, |œï‚àó(x )‚ü© represents the adjoint operation of the quantum feature map for x . After that, we
j j
measure the state |Œ®‚ü©. The probability P of observing the outcome |0‚ü©‚äón on all qubits is:
0
P =|‚ü®n‚äó0|Œ®‚ü©|2 (13)
0
This probability, when properly normalized, equates to the kernel value K(x ,x ), see Fig. 2. By
i j
iterating over all pairs of data points and extracting the corresponding P values, one can construct
0
the kernel matrix K. Each element K represents the kernel value between the data points x and
ij i
x . Subsequently, this quantum kernel matrix can be applied to various kernel-based algorithms,
j
specifically, in our instance, to support vector machines as indicated in equation 7. Utilizing quantum
measurements in the QSVM process allows us to benefit from quantum parallelism and interference,
making the computation of the kernel matrix more efficient than its classical counterpart.
Inthisstudy,weemployedthequantumsimulatorframeworkofferedbyQiskit,particularlyutilizing
the qasm simulator backend.
2.4 Advantages of QSVM
QSVMs offer several benefits over classical SVMs:
51. Speed and Scalability: One of the most prominent advantages of QSVMs is their ability to per-
form computations much faster than classical SVMs. Quantum computing leverages quantum
superposition and entanglement, enabling parallel processing of vast datasets. This is particu-
larly crucial in cancer detection, where the analysis of large-scale genetic and molecular data is
required. QSVMs can process these extensive datasets more efficiently, potentially reducing the
time for pattern recognition and classification.
2. Enhanced Feature Space Mapping: QSVMs have a natural advantage in mapping data into a
high-dimensional feature space. Unlike classical SVMs, which require explicit kernel functions
to map input data into higher-dimensional spaces, QSVMs can implicitly perform this mapping
using quantum feature maps. This implicit mapping is more efficient and can handle more
complex data structures, vital for identifying intricate patterns in cancer datasets.
3. QuantumKernelEstimation: QSVMsutilizequantumkernelestimation,whichcanleadtomore
accurate and nuanced separations in data classification. This quantum kernel trick can explore
correlations and patterns in the data that classical kernels might miss or oversimplify. In cancer
detection,thismeanspotentiallymoreaccurateidentificationofmalignantversusbenigncellsor
more precise staging of cancer.
4. Handling Noisy Data: Quantum algorithms, including QSVMs, are believed to be more robust
against noise in data. This robustness is particularly valuable in medical datasets, which often
contain uncertainties or incomplete information. QSVMs can provide more reliable analysis
under these conditions.
5. QuantumDataLoading: QSVMscanpotentiallybenefitfromquantumdataloadingtechniques,
which allow for the efficient input of large amounts of data into a quantum system [31]. This
capability is essential for dealing with high-dimensional data in cancer detection, where each
sample can include thousands of genetic markers.
|ùúì‚ü©
‚Ä†
|0‚äóùëõ‚ü© ùëà ùëà ùëÉ ‚Üí ùëò(ùë• ,ùë• )
ùúô ùë• ùëñ ùúô ùë• ùëó 0 ùëñ ùëó
‚Ä¶ ‚Ä¶ ‚Ä¶
Fig. 2: Quantum circuit to compute the kernel function, which can be approximated by assessing the
occurrence frequency of |0‚ü©‚äón in the output.
ùëÖ
ùêª ùëß
2ùë•
0
ùëÖ ùëÖ
ùêª ùëß ùëß
2ùë• 1 2Œ¶(ùë•0,ùë•1)
ùëÖ ùëÖ ùëÖ
ùêª ùëß ùëß ùëß
2ùë• 2 2Œ¶(ùë•0,ùë•2) 2Œ¶(ùë•1,ùë•2)
ùëÖ ùëÖ ùëÖ ùëÖ
ùêª ùëß ùëß ùëß ùëß
2ùë• 3 2Œ¶(ùë•0,ùë•3) 2Œ¶(ùë•1,ùë•3) 2Œ¶(ùë•2,ùë•3)
Fig. 3: ZZ-Feature Map quantum circuit
62.5 Performance metrics
Inassessingtheefficacyofthetwoclassificationmethodsinourstudy,weemployedasuiteofstandard
performancemetrics. Thesemetricsenableacomprehensiveunderstandingofeachmethod‚Äôsstrengths
and weaknesses in various aspects of classification [32]. Below, we provide definitions and equations
for each metric.
Accuracy is a metric that measures the overall correctness of the classification model. It is the
proportion of true results (both true positives and true negatives) in the total number of instances.
True Positives (TP) + True Negatives (TN)
Accuracy =
Total Number of Instances
Precision evaluates the model‚Äôs exactness, indicating the proportion of positive identifications that
were actually correct. It is particularly useful in scenarios where the cost of a false positive is high.
True Positives (TP)
Precision =
True Positives (TP) + False Positives (FP)
Sensitivity (also known as Recall) assesses the model‚Äôs ability to correctly identify all relevant
instances (true positives). It is crucial in contexts where missing any positive instance (such as a
disease in medical diagnostics) is undesirable.
True Positives (TP)
Sensitivity =
True Positives (TP) + False Negatives (FN)
Specificity is a metric that measures the proportion of true negatives correctly identified. It is
especially important in situations where avoiding false alarms is crucial.
True Negatives (TN)
Specificity =
True Negatives (TN)+False Positives (FP)
The F1-Score is a harmonic mean of precision and sensitivity. It provides a balance between
precision and sensitivity, offering a single metric to assess a model‚Äôs accuracy when a class imbalance
is present.
Precision √ó Sensitivity
F1-Score =2√ó
Precision + Sensitivity
3 Experimental results
Inthissection,wepresentourexperimentalfindingsasobtainedfromtheSVMandQSVMalgorithms
on the Prostate cancer dataset. The following figures elucidate the differences in how each model
processes the dataset in its feature space and the subsequent impact on classification performance.
Upon examining the kernel matrices for both the SVM and QSVM models, distinct patterns
emerged, providing insights into the data‚Äôs behavior in the corresponding feature spaces. In Fig.
4, which presents the kernel matrix for SVM, we observe a diagonal with values of 1, as expected,
since this diagonal represents the similarity of data points with themselves. More notably, there are
numerous high values in the off-diagonal regions, suggesting that many data points appear similar to
each other in the feature space defined by the RBF kernel. On the other hand, Fig. 5, displaying
the kernel matrix for QSVM, also showcases the anticipated diagonal of 1s but with fewer high val-
ues in the off-diagonal. This hints at a distinctive feature space created by the quantum feature map
(ZZFeatureMapwithfullentanglement)wheremostofthedatapointsarenotassimilartoeachother.
In order to fully understand the differences between SVM and QSVM models, a face-to-face ac-
curacy comparison is necessary. Fig. 6 captures this comparative landscape, shedding light on their
respective performances across training and testing datasets. When examined, it‚Äôs apparent that
QSVM stands out with perfect accuracy of 100% in the training phase, while the SVM falls behind
with an accuracy of 87.89%. However, when we transition to the testing dataset, both models exhibit
converging performance metrics, showing a closely matched ability to handle unseen data. This near
equivalence in test accuracy underscores the robustness inherent to both models, yet the enhanced
training performance of QSVM indicates a slight upper hand in grasping and adapting to the specific
details and complexities of the dataset.
7Kernel Matrix for Training Data
1.0
0.8
0.6
0.4
0.2
0 10 20 30 40 50 60 70 80 90
Training Data Index
Fig. 4: Kernel matrix for SVM training data using RBF kernel
In our exploration of the SVM and QSVM algorithms, the performance metrics in table 1 offer a
comprehensive view of how each model performs across multiple evaluation criteria. For the training
data,QSVMstandsoutwithperfectscoresacrossallmetrics(100%),reflectingitsunparalleledability
to fully capture the nuances of the training set. While SVM produces good results throughout the
trainingphase,doesn‚ÄôtquitematchtheperfectionofQSVMacrossmetricssuchasaccuracy,precision,
sensitivity, specificity, and F1-Score. Interestingly, when evaluating against the test dataset, both
modelsconvergetowardssimilaraccuracylevels(92%), demonstratingtheirrobustnessingeneralizing
tounseendata. Theobservationofperfecttrainaccuracyfollowedbyasmalldiminishedtestaccuracy
suggests that, while the QSVM may have overfit to the training data, it still performs well on unseen
data. Notably, QSVM maintains a competitive advantage in terms of sensitivity (100%) and F1-Score
(93.33%)fortestdata, whicharecrucialmetricsinthecontextofmedicaldiagnostics. Thesefindings,
combined, highlight the potential of QSVM in providing a complementary, if not better, approach to
traditional SVM, especially when sensitivity in classification is paramount.
In the cross-validation analysis detailed in Table 2, the robustness of the QSVM and the classical
SVM is examined through a 10-fold cross-validation method. The QSVM demonstrated a dynamic
range in cross-validation performance with a standard deviation of 0.13, and a mean score of 0.83. In
contrast, the SVM exhibited commendable stability in cross-validation with a standard deviation of
0.11 and a slightly higher mean score of 0.84. These findings suggest distinct behaviors of the two
models when subjected to varying subsets of data, which will be further investigated in the discussion
section.
Table 1: Performance Metrics (%)
Classifier Class Accuracy Precision Sensitivity Specificity F1-Score
QSVM Train 100 100 100 100 100
Test 92 87.5 100 81.81 93.33
Train 87.89 89.13 85.42 90.20 87.23
SVM
Test 92 92.85 92.86 90.91 92.86
Fig. 7 presents the confusion matrices derived from the test data for both QSVM and SVM. Such
matricesarepivotalforunderstandingthepredictivebehaviorofclassifiers, providinginsightsintothe
correctandincorrectpredictionsmadebythemodels. FortheQSVMinFig. 7a,atotalof9instances
were accurately predicted as cancerous, denoted as True Positives (TP). Additionally, the model also
8
xednI
ataD
gniniarT
0
01
02
03
04
05
06
07
08
09Kernel Matrix for Training Data
1.0
0.8
0.6
0.4
0.2
0 10 20 30 40 50 60 70 80 90
Training Data Index
Fig. 5: Kernel matrix for QSVM training data using the ZZFeatureMap with full entanglement
Table 2: Cross-validation results for QSVM and SVM
Model QSVM SVM
Fold 1 0.84615385 0.69230769
Fold 2 0.76923077 0.76923077
Fold 3 0.53846154 0.61538462
Fold 4 0.92307692 0.84615385
Fold 5 1.00000000 0.91666667
Fold 6 0.91666667 0.91666667
Fold 7 0.83333333 1.00000000
Fold 8 0.75000000 0.91666667
Fold 9 1.00000000 0.83333333
Fold 10 0.75000000 0.91666667
Mean Score 0.83 0.84
Std. Deviation 0.13 0.11
recorded 14 True Negatives (TN), correctly identifying non-cancerous instances. However, there were
2 instances, termed False Positives (FP), where the QSVM misclassified non-cancerous instances as
cancerous. Notably, the QSVM model demonstrated a robust performance by registering no False
Negatives (FN), meaning no cancerous instance was misinterpreted as non-cancerous.
Turning our attention to the SVM results presented in Fig. 7b, this model too showcased a com-
mendableclassificationcapability. Itaccuratelyclassified10instancesascancerous(TP)andcorrectly
identified 13 instances as non-cancerous (TN). Nevertheless, there were minor misclassifications with
1 instance each being classified as a False Positive (FP) and a False Negative (FN).
Comparatively, both models exhibit strong performance, with SVM having a slightly better true
positive rate and fewer false positives. However, QSVM achieves a perfect score in identifying non-
cancerous cases without any false negatives. These matrices emphasize the strengths and subtle dif-
ferences in performance between the quantum and classical approaches to SVM, informing potential
areas of model improvement and the particular strengths of each method in differentiating between
cancerous and non-cancerous cases.
The visual representations and performance metrics consistently suggest that while both SVM
and QSVM are robust classifiers, QSVM‚Äôs unique feature space grants it an advantage in certain
key metrics. Additionally, The kernel matrices further substantiated these numerical observations,
9
xednI
ataD
gniniarT
0
01
02
03
04
05
06
07
08
09Average accuracies for QSVM and SVM
1.0 QSVM
SVM
0.8
0.6
0.4
0.2
0.0
Train Test
Model
Fig. 6: Average accuracies for QSVM and SVM
Confusion Matrix Confusion Matrix
14
12
TP FP TP FP
12
Cancerous 9 2 Cancerous 10 1 10
10
8 8
6 6
FN TN FN TN
4
NonCancerous 0 14 NonCancerous 1 13 4
2
2
0
Cancerous NonCancerous Cancerous NonCancerous
Predicted label Predicted label
(a) (b)
Fig. 7: Confusion matrices obtained on test data for (a) QSVM and (b) SVM.
revealing how each model interacts with the feature space. The implications and potential reasons
behind these findings will be further explored in the subsequent discussion section.
4 Discussion
We used two different computational paradigms to identify detailed patterns in our dataset: the
conventional support vector machine and its quantum version, the quantum support vector machine.
These models were chosen based on their ability to capture nonlinear relationships in data using
their respective kernel approaches. While the SVM employs the well-known radial basis function to
compute the kernel, the QSVM employs a quantum feature map. This quantum feature map is adept
at efficiently managing inner products in a higher-dimensional space.
Based on both SVM and QSVM kernel matrices, as illustrated in Figs. 4 and 5 respectively, we
get invaluable insights into how each model processes the dataset in its feature space. In Fig. 4, the
SVM‚Äôskernelmatrixrevealsextensivehighsimilarityvaluesbetweendatapoints,suggestingastrongly
connected feature space under the RBF kernel. This contrasts with Fig. 5, where the QSVM‚Äôs kernel
matrixportraysamorespread-outfeaturespace,characterizedbyasparserpatternofhighvalues. This
advantage stems from the fact that the ZZ Feature map in QSVM leverages the unique properties of
10
lebal
eurT
ycaruccA
lebal
eurTquantumphysics,suchassuperpositionandentanglement. Specifically,thefullentanglementtopology
ensures that our quantum states are intrinsically interconnected. This connection allows for a richer
explorationofthefeaturespace,akafeatureHilbertspace[33],enhancingourunderstandingofcomplex
correlations within the data. This entanglement topology, combined with the inherent superposition
of the quantum system, enables the QSVM to encode data in ways that unveil complex patterns and
relationships which is computationally challenging for classical systems.
Relating these observations to the performance metrics presented in Table 1, we find the QSVM
exhibits perfect scores across all metrics for the training data. The outstanding performance indicates
that the quantum feature map‚Äôs distinctive, spread-out feature space is effective at distinguishing be-
tween classes without any overlap. Meanwhile, the SVM, with its high inter-point similarities kernel
matrix, still performs well, though not to the same extent as QSVM. The high similarity in its ker-
nel matrix could account for the slightly reduced performance, with some points potentially being
challenging to classify due to their close resemblance in the transformed space.
Transitioning our focus to the test data classification, while QSVM algorithm competes with the
classicalSVMintermsofaccuracy,precision,andspecificity,theQSVMmodeloutperformedtheSVM
modelintermsofsensitivity(ortruepositiverate). TheQSVMmodelachievedperfectsensitivitiesof
100%forboththetrainingandtestsets,whiletheSVMmodelhadlowervalueswithatrainsensitivity
of 85.42% and a test sensitivity of 92.86%, indicating the QSVM‚Äôs stronger confidence in correctly
identifying all positive instances and made no false negative. The QSVM model‚Äôs superior sensitivity
makes it advantageous in scenarios where correctly identifying positive instances is crucial, such as in
medical diagnostics, in our specific case study, this applies to the diagnosis of Prostate cancer disease.
The quantum feature mapping and kernel used in the QSVM algorithm contribute to its enhanced
sensitivitybyallowingformorecomplexrepresentationsandbetterseparationbetweenclasses, dueto
its potential to represent data in higher-dimensional quantum spaces. Also, it is predicted that this
efficient distinction of data in the quantum feature space promotes an extended performance for large
datasets where classical algorithms start to struggle.
ThehighsensitivityoftheQSVMmodelisparticularlybeneficialinsituationswhereavoidingfalse
negatives is critical. For instance, in a medical setting, high sensitivity is important to ensure that
no patients with a disease are missed. False negatives can have severe consequences in such cases,
as patients may go undiagnosed and untreated. In contrast, false positives can lead to unnecessary
treatmentorharm,butthecostofafalsenegativeisgenerallyconsideredtobemuchhigher. Therefore,
prioritizingsensitivityoverotherperformancemetrics(likeaccuracyorprecision)iscrucialinmedical
diagnoses and other applications where the consequences of missing positive instances are severe.
Additionally, theQSVMmodelslightlyoutperformedtheSVMmodelregardingtheF1score. The
QSVM achieved an F1 score of 93.33% on the test data, while the SVM model scored 92.86 %. The
F1-score provides a combined measure of precision and sensitivity and can be used to evaluate overall
performance, especially in medical settings. Therefore, the QSVM model demonstrates promising
performance in the detection of Prostate cancer.
The cross-validation performance of the QSVM, while showing variability indicative of potential
overfitting(asflaggedbytheperfecttrainingperformanceshowninTable1),alsoindicatesthemodel‚Äôs
sharp responsiveness to the complexities of the Prostate data set‚Äîa quality that could be addressed
in future work to enhance model performance. On the other side, the SVM‚Äôs stable performance
across cross-validation folds suggests resilience and not overfitting to the same extent as QSVM. Both
models present strengths; the SVM offers reliability across datasets while the QSVM demonstrates
a high sensitivity in detection. These characteristics suggest that each model could be beneficial in
different scenarios within the domain of cancer detection, depending on the specific requirements for
sensitivityandstability. Nevertheless,itisimportanttorecognizethatthesefindingsarederivedfrom
the simulations carried out, and additional research might be necessary to confirm these conclusions
in a more general context.
Asfarasweareaware,ourresearchisthefirsttoapplyQSVMalgorithmforcategorizingProstate
cancer datasets. This study aims to establish a basis for future Prostate cancer research. Beyond
contributing to the growing quantum machine learning applications literature, this research contrasts
theQSVMwiththeconventionalSVM‚ÄîacomparisonpreviouslyunexploredforProstatecancerdata
classification. Our findings underscore that the QSVM algorithm presents an advantage in perfor-
mance‚Äîyet not an alternative in terms of performance‚Äîto the conventional SVM technique in terms
of sensitivity and overall performance which are crucial for diagnosing Prostate diseases.
115 Conclusion
In conclusion, this article has highlighted the potential advantages of employing QSVM in Prostate
cancerdetectionwithinthefieldofquantumcomputing. OurfindingsrevealthatQSVMdemonstrates
superior performance in predicting prostate cancer, exhibiting a high degree of accuracy that is com-
parable to the classical SVM method. Notably, QSVM showed an increase of 7.14% in sensitivity,
and a high F1-Score compared to the classical SVM approach. These improvements are particularly
meaningful in the medical field, as they indicate a high ability of QSVM to correctly identify posi-
tive cases, thereby significantly reducing the risk of false negatives. This enhancement combined with
the potential speed of QSVM is crucial in early detection and effective treatment of prostate cancer,
potentially leading to better patient outcomes and survival rates.
ByincreasingPrecision, QSVMandSVMreducethelikelihoodoffalsepositives, therebylessening
patient anxiety and the need for unnecessary follow-up procedures. The improvement in QSVM‚Äôs
sensitivityisparticularlysignificantasitsuggestsalowerchanceofmisseddiagnoses,anessentialfactor
in strategies that provide early interventions to save lives. Furthermore, higher F1-Score, underscores
QSVM‚Äôs overall effectiveness and reliability as a diagnostic tool.
Moving ahead, this research presents both exciting opportunities and significant challenges for
future exploration. A paramount challenge is that we intend to thoroughly investigate the overfitting
tendencies observed, particularly within the QSVM model. To tackle this, our future work will be
twofold: firstly, we will expand our dataset to include a broader spectrum of data, enhancing the
diversity and volume necessary for improved model generalization‚Äîa challenge in itself. Secondly, we
willrefineourmodeltobettercapturethenuancesofthisexpandeddataset. Inparallel,wewillpursue
the integration of QSVM in real-time clinical settings, an effort that will not only test the model‚Äôs
robustness but also its translational value in healthcare. Moreover, we see significant opportunities
in adapting the QSVM framework for the detection and diagnosis of a wider range of cancers and
diseases, potentially revolutionizing multiple areas of medical diagnostics.
The promising results of QSVM in prostate cancer detection represent a significant potential in
theintersectionofquantumcomputingandhealthcare. Weareoptimisticthatcontinuedresearchand
development in this field will lead to more robust, efficient, and accessible diagnostic tools, ultimately
improving patient care and treatment outcomes in oncology and beyond.
Declarations
Ethical Approval
Not Applicable
Availability of Supporting Data
The data supporting the findings of this study are available from Kaggle‚Äôs Prostate Cancer dataset,
which is publicly accessible.
Competing Interests
The authors declare that they have no competing interests.
Funding
Not Applicable
Authors‚Äô Contributions
This work was carried out in collaboration between all authors. The author conducted the quantum
machinelearningmodelexperimentsandanalyzedthedata. Theco-supervisorandsupervisorprovided
guidance and oversight for the project. All authors read and approved the final manuscript.
12Acknowledgments
Not Applicable
References
[1] Prashanth Rawla. Epidemiology of prostate cancer. World journal of oncology, 10(2):63, 2019.
[2] MaryBeth B Culp, Isabelle Soerjomataram, Jason A Efstathiou, Freddie Bray, and Ahmedin
Jemal. Recentglobalpatternsinprostatecancerincidenceandmortalityrates. European urology,
77(1):38‚Äì52, 2020.
[3] Olusola Olabanjo, Ashiribo Wusu, Mauton Asokere, Oseni Afisi, Basheerat Okugbesan, Olufemi
Olabanjo, Olusegun Folorunso, and Manuel Mazzara. Application of machine learning and deep
learningmodelsinprostatecancerdiagnosisusingmedicalimages: Asystematicreview.Analytics,
2(3):708‚Äì744, 2023.
[4] Renato Cuocolo, Maria Brunella Cipullo, Arnaldo Stanzione, Lorenzo Ugga, Valeria Romeo,
Leonardo Radice, Arturo Brunetti, and Massimo Imbriaco. Machine learning applications in
prostate cancer magnetic resonance imaging. European radiology experimental, 3(1):1‚Äì8, 2019.
[5] Boluwaji A Akinnuwesi, Kehinde A Olayanju, Benjamin S Aribisala, Stephen G Fashoto, Elliot
Mbunge, Moses Okpeku, and Patrick Owate. Application of support vector machine algorithm
forearlydifferentialdiagnosisofprostatecancer. Data Science and Management,6(1):1‚Äì12,2023.
[6] Nasser H Sweilam, AA Tharwat, and NK Abdel Moniem. Support vector machine for diagnosis
cancer disease: A comparative study. Egyptian Informatics Journal, 11(2):81‚Äì92, 2010.
[7] Babacar Gaye, Dezheng Zhang, and Aziguli Wulamu. Improvement of support vector machine
algorithm in big data background. Mathematical Problems in Engineering, 2021:1‚Äì9, 2021.
[8] Priya Dubey and Surendra Kumar. Advancing prostate cancer detection: a comparative analysis
ofpclda-svmandpclda-knnclassifiersforenhanceddiagnosticaccuracy. Scientific Reports,13(1):
13745, 2023.
[9] Jiance Li, Zhiliang Weng, Huazhi Xu, Zhao Zhang, Haiwei Miao, Wei Chen, Zheng Liu, Xiaoqin
Zhang, Meihao Wang, Xiao Xu, et al. Support vector machines (svm) classification of prostate
cancer gleason score in central gland using multiparametric magnetic resonance images: A cross-
validated study. European journal of radiology, 98:61‚Äì67, 2018.
[10] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big
data classification. Physical review letters, 113(13):130503, 2014.
[11] ZhaokaiLi, XiaomeiLiu, NanyangXu, andJiangfengDu. Experimentalrealizationofaquantum
support vector machine. Physical review letters, 114(14):140504, 2015.
[12] Kyriaki A Tychola, Theofanis Kalampokas, and George A Papakostas. Quantum machine learn-
ing‚Äîan overview. Electronics, 12(11):2379, 2023.
[13] GianGentinetta, ArneThomsen, DavidSutter, andStefanWoerner. Thecomplexityofquantum
support vector machines. arXiv preprint arXiv:2203.00031, 2022.
[14] Arti Ranjan, Ashish K. S. Arya, and M. Ravinder. Quantum techniques for image processing.
In 2020 2nd International Conference on Advances in Computing, Communication Control and
Networking (ICACCCN), pages 1035‚Äì1039, 2020. doi: 10.1109/ICACCCN51052.2020.9362910.
[15] Xi-Wei Yao, Hengyan Wang, Zeyang Liao, Ming-Cheng Chen, Jian Pan, Jun Li, Kechao Zhang,
Xingcheng Lin, Zhehui Wang, Zhihuang Luo, Wenqiang Zheng, Jianzhong Li, Meisheng Zhao,
XinhuaPeng, andDieterSuter. Quantumimageprocessinganditsapplicationtoedgedetection:
Theory and experiment. Phys. Rev. X, 7:031041, Sep 2017. doi: 10.1103/PhysRevX.7.031041.
URL https://link.aps.org/doi/10.1103/PhysRevX.7.031041.
13[16] Jie Su, Xuchao Guo, Chengqi Liu, Shuhan Lu, and Lin Li. An improved novel quantum image
representation and its experimental test on ibm quantum experience. Scientific Reports, 11(1):
13879, 2021.
[17] Roopa Golchha and Gyanendra K Verma. Quantum-enhanced support vector classifier for image
classification. In2023 IEEE 8th International Conference for Convergence in Technology (I2CT),
pages 1‚Äì6, 2023. doi: 10.1109/I2CT57861.2023.10126421.
[18] DanyalMaheshwari,DanielSierra-Sosa,andBegonyaGarcia-Zapirain. Variationalquantumclas-
sifier for binary classification: Real vs synthetic dataset. IEEE Access, 10:3705‚Äì3715, 2021.
[19] AbhishekJadhav, AkhtarRasool, andManasiGyanchandani. Quantummachinelearning: Scope
for real-world problems. Procedia Computer Science, 218:2612‚Äì2625, 2023. ISSN 1877-0509. doi:
https://doi.org/10.1016/j.procs.2023.01.235. URL https://www.sciencedirect.com/science/
article/pii/S1877050923002351. InternationalConferenceonMachineLearningandDataEn-
gineering.
[20] Yusen Wu, Bujiao Wu, Jingbo Wang, and Xiao Yuan. Quantum phase recognition via quantum
kernel methods. Quantum, 7:981, 2023.
[21] LinWei, HaowenLiu, JingXu, LeiShi, ZhengShan, BoZhao, andYufeiGao. Quantummachine
learning in medical image analysis: A survey. Neurocomputing, 525:42‚Äì53, 2023.
[22] Sajid Saifi. Prostate cancer dataset, 2023. URL https://www.kaggle.com/datasets/
sajidsaifi/prostate-cancer. Accessed: October 11, 2023.
[23] Jian Jin, Ming Li, Long Jin, et al. Data normalization to accelerate training for linear neural net
to predict tropical cyclone tracks. Mathematical Problems in Engineering, 2015, 2015.
[24] Arti Patle and Deepak Singh Chouhan. Svm kernel functions for classification. In 2013 Interna-
tional Conference on Advances in Technology and Engineering (ICATE), pages 1‚Äì9, 2013. doi:
10.1109/ICAdTE.2013.6524743.
[25] John Platt. Sequential minimal optimization: A fast algorithm for training support vector ma-
chines. 1998.
[26] AbhishekJadhav, AkhtarRasool, andManasiGyanchandani. Quantummachinelearning: Scope
for real-world problems. Procedia Computer Science, 218:2612‚Äì2625, 2023.
[27] JieLin, Dan-BoZhang, ShuoZhang, TanLi, XiangWang, andWan-SuBao. Quantum-enhanced
least-squaresupportvectormachine: Simplifiedquantumalgorithmandsparsesolutions. Physics
Letters A, 384(25):126590, 2020.
[28] NouhailaInnan,MuhammedAl-ZafarKhan,BiswaranjanPanda,andMohamedBennai. Enhanc-
ing quantum support vector machines through variational kernel training. Quantum Inf Process,
374(22), 2023. doi: https://doi.org/10.1007/s11128-023-04138-3.
[29] Li Zhaokai, Liu Xiaomei, Xu Nanyang, et al. Experimental realization of quantum artificial
intelligence. arXiv preprint arXiv:1410.1054, 2014.
[30] Maria Schuld and Nathan Killoran. Quantum machine learning in feature hilbert spaces. Phys.
Rev. Lett., 122:040504, Feb 2019. doi: 10.1103/PhysRevLett.122.040504. URL https://link.
aps.org/doi/10.1103/PhysRevLett.122.040504.
[31] John A Cortese and Timothy M Braje. Loading classical data into a quantum computer. arXiv
preprint arXiv:1803.01958, 2018.
[32] Isaac Mart¬¥ƒ±n De Diego, Ana R Redondo, Rub¬¥en R Fern¬¥andez, Jorge Navarro, and Javier M
Moguerza. General performance score for classification problems. Applied Intelligence, 52(10):
12049‚Äì12063, 2022.
[33] MariaSchuldandNathanKilloran. Quantummachinelearninginfeaturehilbertspaces. Physical
review letters, 122(4):040504, 2019.
14