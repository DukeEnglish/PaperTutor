The Minimax Rate of HSIC Estimation for
Translation-Invariant Kernels
FlorianKalinke ZoltánSzabó
InstituteforProgramStructuresandDataOrganization DepartmentofStatistics
KarlsruheInstituteofTechnology LondonSchoolofEconomics
Karlsruhe,Germany London,UK
florian.kalinke@kit.edu z.szabo@lse.ac.uk
Abstract
Kerneltechniquesareamongthemostinfluentialapproachesindatascienceand
statistics. Undermildconditions,thereproducingkernelHilbertspaceassociated
toakerneliscapableofencodingtheindependenceofM 2randomvariables.
≥
Probablythemostwidespreadindependencemeasurerelyingonkernelsistheso-
calledHilbert-Schmidtindependencecriterion(HSIC;alsoreferredtoasdistance
covariancein the statistics literature). Despite various existing HSIC estimators
designedsinceitsintroductionclosetotwodecadesago,thefundamentalquestion
oftherateatwhichHSICcanbeestimatedisstillopen.Inthiswork,weprovethat
theminimaxoptimalrateofHSICestimationonRdforBorelmeasurescontaining
theGaussianswithcontinuousboundedtranslation-invariantcharacteristickernels
is n 1/2 . Specifically,ourresultimpliestheoptimalityintheminimaxsense
−
O
of manyofthe most-frequentlyusedestimators(includingthe U-statistic, the V-
statis(cid:0)tic,and(cid:1)theNyström-basedone)onRd.
1 Introduction
Kernel methods [SteinwartandChristmann, 2008, BerlinetandThomas-Agnan, 2004,
SaitohandSawano, 2016] allow embedding probability measures into reproducing kernel
Hilbertspaces(RKHS;[Aronszajn,1950])byuseofapositivedefinitefunction,thekernelfunction.
This approach has gained considerable attention over the last 20 years. Such embeddingsinduce
theso-calledmaximummeandiscrepancy(MMD;[Smolaetal.,2007,Grettonetal.,2012]),which
quantifies the discrepancy of two probability measures by considering the RKHS norm of the
distanceoftheirrespectiveembeddings. MMDisametriconthespaceofprobabilitydistributions
ifthekernelischaracteristic[Fukumizuetal.,2008,Sriperumbuduretal.,2010]. MMDisalsoan
integralprobabilitymetric [Zolotarev, 1983, Müller, 1997] where the underlyingfunctionclass is
chosentobetheunitballinthecorrespondingRKHS.
MMDallowsforthequantificationofdependencebyconsideringthedistancebetweentheembed-
dingofajointdistributionandthatoftheproductofitsmarginals.Thisconstructiongivesrisetothe
so-calledHilbert-Schmidtindependencecriterion(HSIC;[Grettonetal.,2005]),whichisalsoequal
totheRKHSnormofthecenteredcross-covarianceoperator. Infact,oneofthemostwidely-used
independence measures in statistics, distance covariance [Székelyetal., 2007, SzékelyandRizzo,
2009, Lyons, 2013], was shown to be equivalent to HSIC [Sejdinovicetal., 2013b] when the lat-
ter is specialized to M = 2 components; ShengandSriperumbudur [2023] proved a similar re-
sult for the conditional case. For M > 2 components [Quadriantoetal., 2009, Sejdinovicetal.,
2013a,Pfisteretal.,2018],universality[Steinwart,2001,Micchellietal.,2006,Carmelietal.,2010,
Sriperumbuduretal., 2011]ofthekernels(k )M (ontherespectivedomains)underlyingHSIC
m m=1
guaranteesthatthismeasurecapturesindependence[SzabóandSriperumbudur,2018]. Inthecase
ofM =2,characteristic(k )2 suffice[Lyons,2013].
m m=1
4202
raM
21
]TS.htam[
1v53770.3042:viXraHSIC has been deployed successfully in numerous contexts, including independence testing in
batch [Grettonetal., 2008, WehbeandRamdas, 2015, BilodeauandNangue, 2017, Góreckietal.,
2018,Pfisteretal.,2018,Albertetal.,2022,Shekharetal.,2023]andstreaming[Podkopaevetal.,
2023] settings, feature selection [Camps-Vallsetal., 2010, Songetal., 2012, Yamadaetal., 2014,
Wangetal., 2022] with applications in biomarker detection [Climente-Gonzálezetal., 2019] and
windpowerprediction[Boucheetal.,2023],clustering[Songetal.,2007,Climente-Gonzálezetal.,
2019], and causal discovery [Mooijetal., 2016, Pfisteretal., 2018, ChakrabortyandZhang,
2019, Schölkopfetal., 2021, KalinkeandSzabó, 2023]. In addition, HSIC has recently
found successful applications in sensitivity analysis [Veiga, 2015, FreitasGustavoetal., 2023,
Fellmannetal., 2023, Herrando-PérezandSaltré, 2024], in the context of uncertainty quantifica-
tion[Stengeretal.,2020],fortheanalysisofdataaugmentationmethodsforbraintumordetection
[Anaya-IsazaandMera-Jiménez, 2022], and that of multimodal neural networks trained on neu-
roimagingdata[Fedorovetal.,2024].
ManyestimatorsforHSICexist.TheclassicalonesrelyonU-statisticsorV-statistics[Grettonetal.,
2005, Quadriantoetal., 2009, Pfisteretal., 2018] and are known to converge at a rate of
n 1/2 . Infact,theV-statistic-basedestimatorsareobtainedbyreplacingthepopulationkernel
P −
O
meanembeddingwithitsempiricalcounterpart;estimatingthemeanembeddingcanbecarriedout
ata(cid:0) speed (cid:1) n 1/2 [Smolaetal.,2007,Theorem2],whichimpliesthatHSICcanbeestimated
P −
O
atthesamerate. ExistingapproximationssuchasNyströmHSIC[KalinkeandSzabó, 2023],also
achieve this ra(cid:0)te unde(cid:1)r the assumption of an appropriate rate of decay of the effective dimension.
While all of these upper boundsmatch asymptotically, it is notknown whether HSIC can be esti-
mated at a faster rate, thatis, whether the upperboundof n 1/2 is optimalin the minimax
P −
O
sense, or if designing estimators achieving better rates is possible. Lower bounds for the related
MMDareknown[Tolstikhinetal.,2016],buttheexistinganaly(cid:0)siscon(cid:1)sidersradialkernelsandre-
lies on independentGaussian distributions. Radial kernels are a special case of the more general
classoftranslation-invariantkernelsthatweconsider.1 TherelianceonindependentGaussiandistri-
butionsrenderstheanalysisofTolstikhinetal.[2016]inapplicableforHSICestimation. Wetackle
bothofthesesevererestrictionsinthepresentarticle.
Wemakethefollowingcontributions.
• Weestablishtheminimaxlowerbound n 1/2 ofHSICestimationwithM 2components
−
O ≥
onRdwithcontinuousboundedtranslation-invariantcharacteristickernels.Asthislowerbound
(cid:0) (cid:1)
matchestheknownupperboundsoftheexisting“classical”U-statisticandV-statistic-basedesti-
mators,andthatoftheNyströmHSICestimator,ourresultsettlestheirminimaxoptimality.
• Specifically, our result also implies the minimaxlower boundof n 1/2 for the estimation
−
O
ofthecross-covarianceoperator,whichcanbefurtherspecializedtogetbacktheminimaxresult
[Zhouetal.,2019,Theorem5]ontheestimationofthecovarianceop(cid:0)erator.(cid:1)
Thepaperisstructuredasfollows. NotationsareintroducedinSection2. Section3isdedicatedto
ourmainresultontheminimaxrateofHSICestimationonRd, with proofpresentedinSection4.
AnauxiliaryresultontheKullback-LeiblerdivergenceisshowninAppendixA.
2 Notations
In this section, we introduce a few notations N , [M], I , 0 , 1 , AT, v,w , v ,
>0 n n n h i k kRd
bdiag(M ,...,M ), A, + Rd ,ψ ,KL(P Q),L2 Rd,Λ , f ,supp(Λ), ,φ ,
1 N | | M1 P || k kL2(Rd,Λ) Hk k
k,µ ,MMD , M , M k ,P , M P ,Pn, (r ), (a ),a b ,HSIC ,and
k k ⊗m=1Hkm ⊗m=(cid:0)1 m(cid:1) m ⊗m=1 m (cid:0)OP n(cid:1) O n n ≍ n k
C . Throughoutthepaperweconsiderrandomvariables,probabilitymeasures,andkernelsonRd.
X
ForM N := 1,2,... ,let[M]:= 1,...,M . DenotebyI then n-sizedidentitymatrix
>0 n
andby0∈ =(0,..{ .,0)T } Rn(resp.1 { =(1,...,1} )T Rn)acolumnve× ctorofzeros(resp.ones).
n n
The transpose of a
matrix∈
A Rd1×d2 is written as
A∈T
Rd2×d1. For v,w Rd, v,w =
vTwstandsfortheirEuclidean∈
innerproduct; v =
∈
v,v
istheassociated∈ Euclidh eannoi
rm.
k kRd h i
1Thefamilyofradialkernelsencompasses, forexample,Gpaussians,mixturesofGaussians,inversemulti-
quadratics, andMatérnkernels; theLaplacekernel istranslation-invariant butnot radial (withrespect tothe
traditionally-chosenEuclideannormk·k ).
Rd
2bdiag(M 1,...,M N)formsablock-diagonalmatrixfromitsarguments(M n)N n=1(M
n
∈Rdn×dn,
n [N])and A denotesthedeterminantofamatrixA Rd d.
×
∈ | | ∈
The set of Borel probability measures on Rd is denoted by + Rd . For a random variable
M1
X P + Rd , we denote its characteristic function by ψ (ω) = E ei ω,X with
ω ∼ Rd a∈ ndM i=1 √ 1. LetP,Q + Rd ,assumethatPisabsoP(cid:0) lute(cid:1) lycontX in∼ uP ouswh .r.t.i Q,and
∈ (cid:0)−(cid:1) ∈M1 (cid:2) (cid:3)
dP
let denote the correspondingRadon(cid:0)-Nik(cid:1)odymderivative (of P w.r.t. Q). Then, the Kullback-
dQ
dP
Leibler divergence of P and Q is defined as KL(P Q) := log (x) dP(x). Given a
|| Rd dQ
(cid:18) (cid:19)
measure space Rd, Rd ,Λ , we denote by L2(Rd,Λ) :R= L2 Rd, Rd ,Λ the Hilbert
B B
spaceof(equivalenceclasses of)measurablefunctionsf : Rd, Rd (R, (R))forwhich
(cid:0) (cid:0) (cid:1) (cid:1) B (cid:0) →(cid:0) (cid:1)B (cid:1)
f 2 := f(x)2dΛ(x) < . The support of a probability measure Λ + Rd
k kL2(Rd,Λ) Rd| | ∞ (cid:0) (cid:0) (cid:1)(cid:1) ∈ M1
denotedbysupp(Λ)isthesubsetofRd forwhicheveryopenneighborhoodofx Rd haspositive
R ∈ (cid:0) (cid:1)
measure[Cohn,2013,p.207].
A function k : Rd Rd R is called a kernel if there exists a Hilbert space and a feature
map φ : Rd × such th→ at k(x,x) = φ(x),φ(x ) for all x,x Rd. A HH ilbert space of
′ ′ ′
functionsh
:→ RdH
R isan RKHS
ash
sociatedto
aiHkernelk
: Rd
∈
Rd R ifk(,x)
k k
→ H × → · ∈ H
and h,k(,x) = h(x) for all x Rd and h .2 In this work, we assume all kernels
k
to
beh meas· urabi lHek
andbounded.3
Thef∈
unctionφ
(x)∈ :=H
k(,x) is the canonicalfeaturemap, and
k
·
k(x,x) = k(,x),k(,x) = φ (x),φ (x) forallx,x Rd. Afunctionκ : Rd R
′ ′ k k ′ ′
is called poh sitiv· e defin· ite ifi Hk h c c κ(x xi H)k 0 for all n∈ N , c = (c )n → Rn,
i,j [n] i j i − j ≥ ∈ >0 i i=1 ∈
and x n Rd. A kernelk :∈ Rd Rd R is said to be translation-invariantif there exists
{ i }i=1 ⊂ P × →
a positive definite function κ : Rd R such that k(x,x) = κ(x x) for all x,x Rd. By
′ ′ ′
→ − ∈
Bochner’s theorem [Wendland, 2005, Theorem 6.6] (recalled in Theorem B.1) for a continuous
bounded translation-invariant kernel k : Rd Rd R there exists a finite non-negative Borel
× →
measureΛ suchthat
k
k(x,y)= e−i hx −y,ω idΛ k(ω) (1)
ZRd
forallx,y Rd. The(kernel)meanembeddingofaprobabilitymeasureP + Rd is
∈ ∈M1
µ (P)= φ (x)dP(x) , (cid:0) (cid:1)
k k k
∈H
ZRd
wheretheintegralismeantinBochner’ssense[DiestelandUhl,1977, ChapterII.2];thebounded-
ness of k ensures that it is well-defined. For P,Q + Rd one can define the (semi-)metric
∈ M1
calledmaximummeandiscrepancy[Smolaetal.,2007,Grettonetal.,2012]as
(cid:0) (cid:1)
MMD (P,Q)= µ (P) µ (Q) .
k k k
k − k Hk
If the mean embedding µ is injective, MMD is a metric and the kernel k is called characteris-
k
tic[Fukumizuetal.,2008,Sriperumbuduretal.,2010,SzabóandSriperumbudur,2018].
LetRd = ×M m=1Rdm (d= M m=1d m)andassumethateachdomainRdm isequippedwithakernel
k
m
: Rdm
×
Rdm
→
R w Pith associated RKHS
Hkm
(m
∈
[M]). The tensor product Hilbert
spaceof( )M isdenotedby M ; itisanRKHS[BerlinetandThomas-Agnan, 2004,
Hkm m=1 ⊗m=1Hkm
Theorem13]withthetensorproductkernelk = M k :Rd Rd Rdefinedby
⊗m=1 m × →
k (x m)M m=1,(x ′m)M m=1 = k m(x m,x ′m) forall x m,x ′m ∈Rdm, m ∈[M].
(cid:0) (cid:1)
mY∈[M]
Thekernelkhasthecanonicalfeaturemapφ (x )M = M φ (x ) M =:
k m m=1 ⊗m=1 km m ∈⊗m=1Hkm Hk
(x
m
∈Rdm,m ∈[M]).LetX =(X m)M m=1b (cid:0)earandom (cid:1)variabletakingvaluesinRdwithjointdis-
tributionP
∈
M+
1
Rd andmarginaldistributionsP
m ∈
M+
1
Rdm (m
∈
[M];d = M m=1d m).
2Forfixedx∈Rd(cid:0),the(cid:1)functionk(·,x):Rd →Rmeansx′ 7→k(x(cid:0)′,x).(cid:1) P
3Boundedness ofthekernel, thatis, sup x,x′∈Rdk(x,x′) < ∞, impliesboundedness ofthefeaturemap,
thatis,sup kφ (x)k <∞(andviceversa);itisalsoequivalenttosup k(x,x)<∞.
x∈Rd k Hk x∈Rd
3We write M P + Rd for the product of measures P (m [M]). Specifically,
Pn := n⊗m P=1 m +∈ M Rd1 n denotes the n-fold product of P. m For a se∈ quence of real-valued
⊗i=1 ∈ M1 (cid:0) (cid:1)
randomvariables(X n)∞ n(cid:0)=(cid:0)1an (cid:1)d (cid:1)asequence(r n)∞ n=1(r
n
>0foralln),X
n
=
OP
(r n)denotesthat
Xn isboundedinprobability.Forpositivesequences(a ) and(b ) ,b = (a )ifthereex-
rn n ∞n=1 n ∞n=1 n O n
istconstantsC >0andn N suchthatb Ca foralln n ;a b ifa = (b )and
0 >0 n n 0 n n n n
∈ ≤ ≥ ≍ O
b = (a ). Onecandefineourquantityofinterest, theHilbert-Schmidtindependencecriterion
n n
O
(HSIC;[Grettonetal.,2005,Quadriantoetal.,2009,Pfisteretal.,2018,SzabóandSriperumbudur,
2018]),as
HSIC (P)=MMD P, M P = C ,
k k ⊗m=1 m k X k Hk
C X =µ k(P) −(cid:0)µ k ⊗M m=1P m(cid:1) ∈Hk, (2)
andC denotesthecenteredcross-covarianceoperator.
X (cid:0) (cid:1)
3 Results
Thissectionisdedicatedtoourresults: TheminimaxlowerboundfortheestimationofHSIC (P),
k
where k is a productof continuousboundedtranslation-invariantcharacteristic kernelsis givenin
Theorem1(ii).ForthespecificcasewherekisaproductofGaussiankernels(statedinTheorem1(i)),
theconstantinthelowerboundismadeexplicit.Theorem1(ii)alsohelpstoestablishalowerbound
ontheestimationofthecross-covarianceoperator(Corollary1).
Before presenting our results, we recall the framework of minimax estimation [Tsybakov, 2009]
adaptedtooursetting. LetFˆ denoteanyestimatorofHSIC (P)basedonni.i.d.samplesfromP.
n k
Asequence(ξ ) (ξ >0foralln)issaidtobealowerboundofHSICestimationw.r.t.aclass
n ∞n=1 n
ofBorelprobabilitymeasuresonRdifthereexistsaconstantc>0suchthat
P
infsupPn ξ 1 HSIC (P) Fˆ c >0. (3)
Fˆ n P ∈P n
n−
(cid:12)
k
−
n
(cid:12)≥ o
IfaspecificestimatorofHSICF˜ nhasanup(cid:12) (cid:12)perboundthatm(cid:12) (cid:12)atches(ξ n)∞ n=1uptoconstants,thatis,
HSIC (P) F˜ = (ξ ), (4)
k n P n
− O
thenF˜ niscalledminimaxoptimal(cid:12) (cid:12). (cid:12) (cid:12)
(cid:12) (cid:12)
We use Le Cam’s method [LeCam, 1973, Tsybakov, 2009] (recalled in Theorem B.5) to obtain
boundsasin(3);estimatorsofHSICachievingtheboundsin(4)withξ =n 1/2arequotedinthe
n −
introduction. Thekeytotheapplicationofthemethodistofindanadversarialpairofdistributions
(P ,P ) forwhich
θ0 θ1
∈P ×P
1. there exist positive constants α, and n N such that for all n n , KL Pn Pn
0 ∈ >0 ≥ 0 θ1|| θ0 ≤
α, in other words, the correspondingn-fold productmeasuresmust be similar in the sense of
Kullback-Leiblerdivergence,but (cid:0) (cid:1)
2. HSIC (P ) HSIC (P ) 2s > 0, that is, their correspondingvaluesof HSIC must be
|
k θ1
−
k θ0
| ≥
dissimilar. In particular, to establish the minimax optimality of existing estimators w.r.t. their
knownupperbounds,wemustfindanadversarialpairthatsatisfiess n 1/2.
−
≍
The proof of the first part of our statement relies on the following Lemma 1 which yields the an-
alytical value of HSIC ( (µ,Σ)), where k = M k is the productof Gaussian kernels k
k N ⊗m=1 m m
(m [M]) and (µ,Σ) denotes the multivariate normal distribution with mean µ Rd and
∈ N ∈
covariancematrixΣ Rd d.
×
∈
Lemma 1 (Analytical value of HSIC for the Gaussian setting). Let us consider the Gaussian
kernel k(x,y) = e−γ 2kx −y k2 Rd (γ > 0, x,y Rd) and Gaussian random variable X =
∈
(X m)M
m=1 ∼
N(m,Σ) =: P, where X
m ∈
Rdm (m
∈
[M]), m = (m m)M
m=1 ∈
Rd,
Σ and= Σ[Σ =i,j b] i d,j i∈ a[ gM (Σ] ∈ R ,.d .× .d ,, ΣΣ i,j )∈ ,wR ed hi× ad vj e, and d = m ∈[M]d m. In this case, with Σ 1 = Σ
2 1,1 M,M P
1 1 2
HSIC2(P)= + .
k 2γΣ 1+I d 1 2 2γΣ 2+I d 1 2 − γΣ 1+γΣ 2+I d 1 2
| | | | | |
4Inthiswork,wefocusoncontinuousboundedtranslation-invariantkernels,whicharefullycharac-
terizedbyBochner’stheorem[Wendland,2005,Theorem6.6];thetheoremstatesthatafunctionon
Rd ispositivedefiniteifandonlyifitistheFouriertransformofafinitenonnegativemeasure.4 We
usethisdescriptiontoobtainourmainresult,whichisasfollows.
Theorem1(LowerboundforHSICestimationonRd). Let beaclassofBorelprobabilitymea-
suresoverRd containingthed-dimensionalGaussiandistribP utions. Letd = d andFˆ
m [M] m n
denote any estimator of HSIC (P) with n 2 i.i.d. samples from P . Assu∈me further that
k = M k whereeither,fork m [M], ≥ ∈ P P
⊗m=1 m ∈
(i) the kernels k
m
: Rdm Rdm R are Gaussian with common bandwidthparameterγ > 0
definedby(x m,x ′m)
7→× e−γ 2kxm→ −x′ mk2
Rdm (x m,x
′m
∈Rdm),or
(ii) thekernelsk
m
:Rdm Rdm Rarecontinuousboundedtranslation-invariantcharacteristic
× →
kernels.
Thenitholdsthat
1 5
infsupPn HSIC (P) Fˆ c − 8 ,
Fˆ n P ∈P (cid:26)(cid:12) k − n (cid:12)≥ √n (cid:27)≥ 2q
with(i)theconstantc= 2(2γ+γ
1)d
4+1(cid:12)
(cid:12)
>0(depending(cid:12) (cid:12)onγ anddonly)inthefirstcase,or(ii)some
constantc>0inthesecondcase.
We notethatwhileTheorem1(ii)appliestothemoregeneralclassoftranslation-invariantkernels,
weincludeTheorem1(i)asitmakestheconstantcexplicit.
Thefollowingcorollaryallowstorecovertherecentlowerboundontheestimationofthecovariance
operatorbyZhouetal.[2019,Theorem5]asaspecialcasethatwedetailinRemark1(e).
Corollary1(Lowerboundoncross-covarianceoperatorestimation). InthesettingofTheorem1(ii),
letFˆ denoteanyestimatorofthecenteredcross-covarianceoperatorC definedin(2)with
n X k
∈H
n 2i.i.d.samplesfromP . Thenitholdsthat
≥ ∈P
1 5
infsupPn C Fˆ c − 8 ,
Fˆ n P ∈P (cid:26)(cid:13) X − n (cid:13)Hk ≥ √n (cid:27)≥ 2q
forsomeconstantc>0. (cid:13) (cid:13)
(cid:13) (cid:13)
Remark1.
(a) ValidnessofHSIC. Thoughgenerallythe characteristicproperty of(k )M -sisnotenough
m m=1
[SzabóandSriperumbudur, 2018, Example2]forM > 2to ensurethe -characteristicprop-
ertyofk = M k (inotherwords,thatHSIC (P)=0iff.P = M I P ),onRd underthe
⊗m=1 m k ⊗m=1 m
imposedcontinuousboundedtranslation-invariantassumption(i)k beingcharacteristic,(ii)k
being -characteristic,and(iii)(k )M -sbeingcharacteristicareequivalent(TheoremB.4).
I m m=1
(b) Minimax optimality of existing HSIC estimators. The lower bounds in Theorem 1 asymp-
totically match the known upper bounds of the U-statistic and V-statistic-based estimators of
ξ = n 1/2. TheNyström-basedHSICestimatorachievesthesamerateunderanappropriate
n −
decayoftheeigenspectrumoftherespectivecovarianceoperator.Hence,Theorem1impliesthe
optimalityoftheseestimatorsonRdwithcontinuousboundedtranslation-invariantcharacteris-
tickernelsintheminimaxsense.
(c) Differencecomparedto Tolstikhinetal. [2016] (minimaxMMD estimation). We note thata
lowerboundfortherelatedMMD exists. However,theadversarialdistributionpair(P ,P )
k θ1 θ0
constructedbyTolstikhinetal.[2016, Theorem1]toobtainthelowerboundonMMDestima-
tionhasaproductstructurewhichimpliesthat HSIC (P ) HSIC (P ) = 0andhenceit
|
k θ1
−
k θ0
|
is notapplicablein our case of HSIC; Tolstikhinetal. [2016, Theorem 2] with radialkernels
hasthesamerestriction.
4We note that for many translation-invariant kernels, the corresponding spectral measures are known
[Sriperumbuduretal.,2010,Table2].
5(d) DifferencecomparedtoTolstikhinetal.[2017](minimaxmeanembeddingestimation). The
estimationofthemeanembeddingµ (P)isknowntohaveaminimaxrateof n 1/2 . But,
k −
O
thisratedoesnotimplyanoptimallowerboundfortheestimationofMMD asisevidentfrom
thetwoworks[Tolstikhinetal.,2016,2017]. ThesameconclusionholdsforHSIC(cid:0)estima(cid:1)tion.
(e) DifferencecomparedtoZhouetal.[2019](minimaxcovarianceoperatorestimation).Forthe
relatedproblemofestimatingthecenteredcovarianceoperator
CXX = (φ k(x) µ k(P)) (φ k(x) µ k(P))dP(x)
k
k,
− ⊗ − ∈H ⊗H
ZRd
Zhouetal.[2019,Theorem5]givethelowerbound
c
infsupPn CXX Fˆ
n
1/8
Fˆ n P ∈P (cid:26)(cid:13) − (cid:13)Hk⊗Hk ≥ √n (cid:27)≥
(cid:13) (cid:13)
in the same setting as in Theorem 1(cid:13)(ii), where F(cid:13)ˆ is anyestimatorof the centeredcovariance
n
CXX,andcisapositiveconstant. Bynotingthatthecenteredcovarianceisthecenteredcross-
covarianceofarandomvariablewithitself,Corollary1recoverstheirresult.
Thenextsectioncontainsourproofs.
4 Proofs
This section is dedicated to our proofs. We present the proof of Lemma 1 in Section 4.1, that of
Theorem1inSection4.2,andthatofCorollary1inSection4.3.
4.1 ProofofLemma1
As
HSIC2(P)=MMD2(P,Q)= µ (P) µ (Q) 2
k k k k − k k Hk
= µ (P),µ (P) + µ (Q),µ (Q) 2 µ (P),µ (Q)
h k k iHk h k k iHk − h k k iHk
with Q = M P = (m,bdiag(Σ ,...,Σ )), P = (m ,Σ ), it is suf-
⊗m=1 m N 1,1 M,M m N m m,m
ficient to be able to compute µ (P),µ (Q) -type quantities with P = (m ,Σ ) and
Q = (m ,Σ ). One can
sh hok
w
[Muk andeti eHtk
al., 2011, Table 1] that µ
(PN
),µ
(1 Q)1
=
e−1
2(m1N −m2)T(2 Σ1+2 Σ2+γ−1Id)−1
(m1−m2)
. Usingthisfactandthatm=m
=mh ,k theresuk ltfoi llH owk
s.
1 1 2
|γΣ1+γΣ2+Id|2
4.2 ProofofTheorem1
ThesetupandtheupperboundonKL(Pn Pn )agreefor(i)and(ii)butthemethodsthatweuse
θ1|| θ0
tolowerbound HSIC (P ) HSIC (P ) differ.Westructuretheproofaccordinglyandpresent
|
k θ1
−
k θ0
|
the overlappingpartbeforewe branchoutinto (i) and(ii). Both partsof the statementrely onLe
Cam’smethod,whichwestateasTheoremB.5forself-completeness.
Toconstructtheadversarialpair,weconsideraclass ofGaussiandistributionsoverRd suchthat
G
everyelement µ,Σ ,with
N ∈G
(cid:0) (cid:1) 1 0 0 0
.
. .
· .· .·
.
.
. .
.
. .
··· .
. .
 
0 1 ρ 0
Σ=Σ(i,j,ρ)= 
 
0
.
.
.
·· ·· ·· ρ
.
.
.
1
.
.
.
·
·
.·
·
.·
·
.
0
.
.
.
 
∈Rd ×d, (5)
 
0 0 0 1
 ··· ··· 
 
and (fixed)i = d , j = d +1, ρ ( 1,1). In otherwords, Σ is essentially the d-dimensional
1 1
∈ −
matrix I except for the (i,j) and (j,i) entry; both entries are identical to ρ, and they specify
d
the correlation of the respective coordinates. This family of distributions is indexed by a tuple
6(µ,ρ) Rd ( 1,1) =: and, for a , we write P for the associated distribution. To
a
∈ × − A ∈ A
bringourselvesintothesettingofTheoremB.5,wefixn N ,choose =Rd,setΘ = θ :=
>0 a
HSIC (P ) : a , = Pn : a = Pn : θ ∈ Θ ,andusetX hemetric(x,y) { x y
k a ∈A} PΘ { a ∈A} { a a ∈ } 7→| − |
forx,y R. Hence,thedataD P . Forbrevity,letF : Rstandfora HSIC (P ),
θ Θ k a
∈ ∼ ∈P A→ 7→
andletFˆ standforthecorrespondingestimatorbasedonnsamples.
n
As ,itholdsforeverypositivesthat
G ⊆P
supPn HSIC (P) Fˆ s supPn HSIC (P) Fˆ s .
k n k n
− ≥ ≥ − ≥
P P
∈P n(cid:12) (cid:12) o ∈G n(cid:12) (cid:12) o
(cid:12) (cid:12) (cid:12) (cid:12)
LetP = (µ ,Σ )(cid:12) andP = (µ(cid:12) ,Σ )with (cid:12) (cid:12)
θ0
N
0 0 θ1
N
1 1
µ =0 Rd, Σ =Σ(d ,d +1,0)=I Rd d,
0 d 0 1 1 d ×
∈ ∈
1
µ = 1 Rd, Σ =Σ(d ,d +1,ρ ) Rd d,
1 d 1 1 1 n ×
√dn ∈ ∈
where ρ ( 1,1) will be chosen appropriately later. We now proceed to upper bound
n
∈ −
KL Pn Pn andlowerbound F(θ ) F(θ ).
θ1|| θ0 | 1 − 0 |
(cid:0) (cid:1)
Upper bound for KL divergence Lemma A.1 implies that with ρ2 = 1, one has the bound
n n
KL Pn Pn α:= 5 forn 2.
θ1|| θ0 ≤ 4 ≥
(cid:0) (cid:1)
Lowerbound(i): Gaussiankernels. Recallthattheconsideredkernelisk(x,y)=e−γ 2kx −y k2 Rd
(γ >0). Theideaoftheproofisasfollows.
1. Weexpress F(θ ) F(θ ) inclosedformasafunctionofγ,ρ ,andd.
1 0 n
| − |
2. Usingtheanalyticalformobtainedinthe1ststep,weconstructthelowerbound.
Thisiswhatwedetailnext.
• Analyticalformof F(θ ) F(θ ): UsingthefactthatHSIC (P )=0,wehavethat
|
1
−
0
|
k θ0
F (θ ) F (θ ) 2 =F2(θ )=HSIC2(P )=MMD2( (µ ,Σ ), (µ ,I ))
1 − 0 1 k θ1 k N 1 1 N 1 d
(cid:12) =0 (cid:12)
(cid:12) = µ ( (µ ,Σ(cid:12) )) µ ( (µ ,I )) 2
k k N| {z1 } 1 − k N 1 d k Hk
= µ ( (µ ,Σ )),µ ( (µ ,Σ )) + µ ( (µ ,I )),µ ( (µ ,I ))
k 1 1 k 1 1 k 1 d k 1 d
h N N i Hk h N N i Hk
(i) (ii)
2 µ ( (µ ,Σ )),µ ( (µ ,I )) ,
| k 1 1{z k 1 d } | {z }
− h N N i Hk
(iii)
whichweco|mputeterm-by-ter{mzwithLemma1,an}dobtain
1/2
(i)= 2γΣ 1+I
d
−1/2 = (2γ+1)d −2 (2γ+1)2 (2γρ n)2 − ,
| | −
h 1/(cid:16)2 (cid:17)i
(ii)= 2γI d+I
d
−1/2 = (2γ+1)d − ,
| |
h i 1/2
(iii)= γΣ 1+γI d+I
d
−1/2 = (2γ+1)d −2 (2γ+1)2 (γρ n)2 − .
| | −
h (cid:16) (cid:17)i
Combining(i),(ii),and(iii)yieldsthat
HSIC2(P )=(i)+(ii) 2(iii)
k θ1 −
1/2 1/2
= (2γ+1)d −2 (2γ+1)2 (2γρ n)2 − + (2γ+1)d −
−
h (cid:16) (cid:17)i 1/2h i
2 (2γ+1)d −2 (2γ+1)2 (γρ n)2 − .
− −
h (cid:16) (cid:17)i
7• Lower bound on F(θ ) F(θ ): Next, we show that there exists c > 0 such that for any
1 0
| − |
n N itholdsthatHSIC2(P ) c.
∈ >0 k θ1 ≥ n
2
For0<x< 1+ 1 ,letusconsiderthefunction
2γ
(cid:16) (cid:17)
1/2 1/2
f c(x)= (2γ+1)d −2 (2γ+1)2 4γ2x − + (2γ+1)d −
−
h (cid:16) (cid:17)i 1/2h i
2 (2γ+1)d −2 (2γ+1)2 γ2x − cx
− − −
= zd −h 2 z2 4γ2x(cid:16) −1/2 + zd −1/2(cid:17)i 2 zd −2 z2 γ2x −1/2 cx,
− − − −
withtheshorthan(cid:2)dz :=(cid:0)2γ+1.5 W(cid:1)(cid:3)iththisn(cid:0)ota(cid:1)tion,f (1/(cid:2)n)=H(cid:0)SIC2(P (cid:1)(cid:3)) c/n;ouraimis
c k θ1 −
todeterminec>0suchthatf (1/n) 0foranypositiveintegern. Toachievethisgoal,notice
c
≥
thatf (0)=0,and
c
2γ2zd 2 γ2zd 2
− −
f (x)= c
c′
[zd 2(z2 4xγ2)]3/2 − [zd 2(z2 xγ2)]3/2 −
− − − −
2γ2zd 2 γ2zd 2 γ2zd 2
− − −
> c= c
[zd 2(z2 xγ2)]3/2 − [zd 2(z2 xγ2)]3/2 − [zd 2(z2 xγ2)]3/2 −
− − − − − −
γ2zd 2 γ2 γ2
−
> c= c= c.
(zd −2z2)3/2 − z2√zd − (2γ+1)2 (2γ+1)d −
q
Choosingnowc =
γ2
> 0,wehavef (x) 0,sof isanondecreasingfunction.
(2γ+1)2√(2γ+1)d c′ ≥
2
Notethatf (1/n) = HSIC2(P ) c/n 0,withx = 1/nand 1+ 1 − < 1 n < .
c k θ1 − ≥ 2γ ≤ ∞
Bytakingthepositivesquareroot,thismeansthat (cid:16) (cid:17)
γ
HSIC (P ) =:2s
k θ1 ≥ 1/4
(2γ+1) (2γ+1)d √n
(cid:16) (cid:17)
holdsforn 1,implyingthat F(θ ) F(θ ) 2s>0.
1 0
≥ | − |≥
WeconcludetheproofbyTheoremB.5usingthatα= 5 andmax e−5 4,1 −√5 8 = 1 −√5 8.
4 4 2 2
(cid:18) (cid:19)
Lowerbound(ii):translation-invariantkernels. LetΛ denotethespectralmeasureassociated
k
tothekernelkaccordingto(1). UsingthefactthatHSIC (P )=0,wehavefor F(θ ) F(θ )
k θ0
|
1
−
0
|
that
F(θ ) F(θ ) 2 =F2(θ )=HSIC2(P )=MMD2( (µ ,Σ ), (µ ,Σ ))
1 − 0 1 k θ1 k N 1 1 N 1 0
(cid:12) =0 (cid:12)
(cid:12) (cid:12)
(i) 2
= ψ| N{(µz 1,}Σ1) −ψ N(µ1,Σ0) L2(Rd,Λk)
( =ii)(cid:13) (cid:13) ei hµ1,ω i−1 2hω,Σ1ω i (cid:13) (cid:13)ei hµ1,ω i−1 2hω,Σ0ω i 2 dΛ k(ω)
−
ZRd
(cid:12) (cid:12)
= (cid:12) (cid:12)ei hµ1,ω i 2 e−21 hω,Σ1ω i e−1 2hω,Σ0ω i 2 d(cid:12) (cid:12)Λ k(ω)
−
ZRd
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) =1 (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(i ≥ii) |e−1 2{ hzω,Σ1}ω i −e−21 hω,Σ0ω i 2 dΛ k(ω)( ≥iv) ρ2 n [h′ω(0)]2 dΛ k(ω)( =v) (2 nc)2 ,
ZA
(cid:12) (cid:12)
ZA
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=:(2c)2 =:(2s)2>0
5Noticethat(2γ+1)2 −γ2x>(2γ+1)2 −4γ2x,and(2γ+1| )2 −4γ2x{z >0⇔x}
<
1+|{ 1z}2
fora
(cid:16) 2γ(cid:17)
positivex;hencetheimposedassumptiononxensuresthatthefunctionf iswell-defined.
c
8where(i)holdsbySriperumbuduretal. [2010, Corollary4(i)](recalledinTheoremB.2). (ii)fol-
lows from the analytical form ψ (µ,Σ)(t) = ei hµ,t i−1 2ht,Σt
i
of the characteristic function of a
multivariatenormaldistribution N (µ,Σ). For(iii),wedefinethenon-emptyopenset
N
A= ω =(ω ,...,ω )T Rd : ω ω <0 Rd,
1 d
∈
d1 d1+1
⊂
andusethattheintegratio(cid:8)nofanon-negativefunctionoverasubsety(cid:9)ieldsalowerbound. In(iv),
fixω Aandlet
∈
h
ω
:ρ [0,1] e −1 2hω,Σ(d1,d1+1,ρ)ω
i
(0,1].
∈ 7→ ∈
Note that h ω(ρ) = e−1 2(ωTω+2ρωd1ωd1+1) ; h
ω
is continuouson [0,1] and differentiableon (0,1).
Henceforanyρ (0,1),bythemeanvaluetheorem,thereexistsρ˜ (0,1)suchthat
∈ ∈
h (ρ) h (0)=ρh (ρ˜) ρ min h (c).
ω
−
ω ′ω
≥ c [0,1]
′ω
∈
Wehavethefirstandsecondderivatives
h′ω(c)= −ω d1ω d1+1e−21(ωTω+2cωd1ωd1+1) , h′ω′(c)=ω d2 1ω d2 1+1e−21(ωTω+2cωd1ωd1+1) >0,
whichimpliesthatc h (c)isastrictlyincreasingfunctionofcandthatitattainsitsminimumat
7→
′ω
c=0,thatis,
h (ρ) h (0) ρh (0)>0,
ω
−
ω
≥
′ω
wherethe2ndinequalityholdsbyρ>0andω A. Thisshowsthat
∈
[h ω(ρ) −h ω(0)]2 ≥[ρh′ω(0)]2 ,
and the monotonicity of integration gives (iv). For (v), we note that the kernel k = M k
⊗m=1 m
is characteristic [SzabóandSriperumbudur, 2018, Theorem 4] (recalled in Theorem B.4) as the
(k )M -sarecharacteristic. Thus,supp(Λ )=Rd (seeSriperumbuduretal.[2010,Theorem9];
m m=1 k
recalledinTheoremB.3),implyingthatΛ (A) > 0. (v)followsfromthepositivityofh (0)(for
k ′ω
anyω A), fromthefactthattheintegralofapositivefunctiononasetwithpositivemeasureis
∈
positive,andfromourchoiceofρ =n 1/2.
n −
Now,bytakingthepositivesquareroot,wehave
2c
F (θ ) F (θ ) =:2s. (11)
1 0
| − |≥ √n
We conclude by the application of Theorem B.5 using that α = 5 and max e−5 4,1 −√5 8 =
4 4 2
(cid:18) (cid:19)
1 √5
− 8.
2
4.3 ProofofCorollary1
WeusethesameargumentasinthebeginningoftheproofofTheorem1inSection4.2butadjustthe
settinginwhichweapplyTheoremB.5.Specifically,wenowletΘ= θ :=C : X P , a
{
a Xa a
∼
a
∈
withC definedasin(2)bethesetofcovarianceoperators,usethemetric(x,y) x y
X
fA o} rx,y ,andkeeptheremainingpartofthesetupthesame. Hence,itremainst7→ olok we− rbok uHndk
k
∈H
C C . By using that HSIC is the RKHS normof the cross-covarianceoperator, we
obtX aiθ n1 t−
hat
Xθ0 Hk
(cid:13) (cid:13)
(cid:13) (cid:13)
(i) (ii) 2c
C C C C = F(θ ) F(θ ) 2s= ,
Xθ1
−
Xθ0
Hk ≥
Xθ1
Hk −
Xθ0
Hk |
1
−
0
| ≥ √n
(cid:13) (cid:13) (cid:13) (cid:13) (cid:12) (cid:12) (cid:12)=(cid:13) (cid:13)HSICk((cid:13) (cid:13)Pθ1) =(cid:13) (cid:13)HSICk((cid:13) (cid:13)Pθ0)(cid:12) (cid:12)
(cid:12)
| {z } | {z }
where(i)holdsbythereversetriangleinequality,F isdefinedasinSection4.2,and(ii)isguaran-
teedby(11)forc>0. WeconcludeasintheproofofTheorem1(ii)toobtainthestatedresult.
9Acknowledgments and DisclosureofFunding
This work was supported by the German Research Foundation (DFG) Research Training Group
GRK 2153: EnergyStatusData —InformaticsMethodsforitsCollection, AnalysisandExploita-
tion,andbythepilotprogramCore-InformaticsoftheHelmholtzAssociation(HGF).
References
Mélisande Albert, Béatrice Laurent, Amandine Marrel, and Anouar Meynaoui. Adaptive test of
independencebasedonHSICmeasures. TheAnnalsofStatistics,50(2):858–879,2022.
AndrésAnaya-IsazaandLeonelMera-Jiménez. Dataaugmentationandtransferlearningforbrain
tumordetectioninmagneticresonanceimaging. IEEEAccess,10:23217–23233,2022.
NachmanAronszajn. Theoryofreproducingkernels. TransactionsoftheAmericanMathematical
Society,68:337–404,1950.
AlainBerlinetandChristineThomas-Agnan.ReproducingKernelHilbertSpacesinProbabilityand
Statistics. Kluwer,2004.
MartinBilodeauandAurélienGuetsopNangue. Testsofmutualorserialindependenceofrandom
vectorswithapplications. JournalofMachineLearningResearch,18:1–40,2017.
DimitriBouche,RémiFlamary,Florenced’AlchéBuc,RiwalPlougonven,MarianneClausel,Jordi
Badosa, and Philippe Drobinski. Wind powerpredictionsfromnowcaststo 4-hourforecasts: a
learningapproachwithvariableselection. RenewableEnergy,211:938–947,2023.
GustavoCamps-Valls,JorisM.Mooij,andBernhardSchölkopf. Remotesensingfeatureselection
by kernel dependence measures. IEEE Geoscience and Remote Sensing Letters, 7(3):587–591,
2010.
ClaudioCarmeli,ErnestoDeVito,AlessandroToigo,andVeronicaUmanitá. Vectorvaluedrepro-
ducingkernelHilbertspacesanduniversality. AnalysisandApplications,8:19–61,2010.
ShubhadeepChakrabortyand XianyangZhang. Distance metrics for measuringjointdependence
with applicationto causalinference. JournaloftheAmerican StatisticalAssociation, 114(528):
1638–1650,2019.
Héctor Climente-González, Chloé-Agathe Azencott, Samuel Kaski, and Makoto Yamada. Block
HSICLasso: model-freebiomarkerdetectionforultra-highdimensionaldata. Bioinformatics,35
(14):i427–i435,2019.
DonaldL.Cohn. MeasureTheory. Birkhäuser/Springer,secondedition,2013.
JosephDiestelandJohnJerryUhl. VectorMeasures. AmericanMathematicalSociety.Providence,
1977.
JohnDuchi. Derivationsforlinearalgebraandoptimization. Berkeley,California,3(1):2325–5870,
2007.
AlexFedorov,EloyGeenjaar,LeiWu,TristanSylvain,ThomasPDeRamus,MargauxLuck,Maria
Misiura, Girish Mittapalle, R Devon Hjelm, Sergey M Plis, et al. Self-supervised multimodal
learning for group inferences from MRI data: Discovering disorder-relevant brain regions and
multimodallinks. NeuroImage,285:120485,2024.
Noé Fellmann, Christophette Blanchet-Scalliet, Céline Helbert, Adrien Spagnol, and Delphine
Sinoquet. Kernel-based sensitivity analysis for (excursion) sets. Technical report, 2023.
(https://arxiv.org/abs/2305.09268).
MichaelFreitas Gustavo, Matti Hellström, and Toon Verstraelen. Sensitivity analysis for ReaxFF
reparametrizationusingtheHilbert–Schmidtindependencecriterion.JournalofChemicalTheory
andComputation,19(9):2557–2573,2023.
10KenjiFukumizu,ArthurGretton, XiaohaiSun, andBernhardSchölkopf. Kernelmeasuresofcon-
ditionaldependence. InAdvancesinNeuralInformationProcessingSystems(NIPS),pages498–
496,2008.
Tomasz Górecki, Miroslaw Krzys´ko, and Waldemar Wolyn´ski. Independence test and canonical
correlationanalysis based on the alignmentbetween kernelmatrices for multivariatefunctional
data. ArtificialIntelligenceReview,pages1–25,2018.
ArthurGretton,OlivierBousquet,AlexSmola, andBernhardSchölkopf. Measuringstatistical de-
pendence with Hilbert-Schmidt norms. In Algorithmic Learning Theory (ALT), pages 63–78,
2005.
Arthur Gretton, Kenji Fukumizu, Choon Hui Teo, Le Song, Bernhard Schölkopf, and Alexander
Smola. Akernelstatisticaltestofindependence. InAdvancesinNeuralInformationProcessing
Systems(NIPS),pages585–592,2008.
Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander Smola. A
kerneltwo-sampletest. JournalofMachineLearningResearch,13(25):723–773,2012.
SalvadorHerrando-PérezandFrédérikSaltré. Estimatingextinctiontime usingradiocarbondates.
QuaternaryGeochronology,79:101489,2024.
FlorianKalinkeandZoltánSzabó. NyströmM-Hilbert-Schmidtindependencecriterion. InConfer-
enceonUncertaintyinArtificialIntelligence(UAI),pages1005–1015,2023.
LucienLeCam. Convergenceofestimatesunderdimensionalityrestrictions. TheAnnalsofStatis-
tics,1:38–53,1973.
Russell Lyons. Distance covariance in metric spaces. The Annals of Probability, 41:3284–3305,
2013.
Charles Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine
LearningResearch,7:2651–2667,2006.
Joris Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Schölkopf. Dis-
tinguishing cause from effect using observational data: Methods and benchmarks. Journal of
MachineLearningResearch,17:1–102,2016.
KrikamolMuandet,KenjiFukumizu,FrancescoDinuzzo,andBernhardSchölkopf. Learningfrom
distributionsviasupportmeasuremachines. InAdvancesinNeuralInformationProcessingSys-
tems(NIPS),pages10–18,2011.
AlfredMüller. Integralprobabilitymetricsandtheirgeneratingclasses offunctions. Advancesin
AppliedProbability,29:429–443,1997.
NiklasPfister,PeterBühlmann,BernhardSchölkopf,andJonasPeters. Kernel-basedtestsforjoint
independence.JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology),80(1):
5–31,2018.
Aleksandr Podkopaev, Patrick Blöbaum, Shiva Kasiviswanathan, and Aaditya Ramdas. Sequen-
tialkernelizedindependencetesting. InInternationalConferenceonMachineLearning(ICML),
pages27957–27993,2023.
NoviQuadrianto,LeSong,andAlexSmola. Kernelizedsorting.InAdvancesinNeuralInformation
ProcessingSystems(NIPS),pages1289–1296,2009.
SaburouSaitohandYoshihiroSawano. TheoryofReproducingKernelsandApplications. Springer
Singapore,2016.
Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of
theIEEE,109(5):612–634,2021.
DinoSejdinovic,ArthurGretton,andWicherBergsma. Akerneltestforthree-variableinteractions.
InAdvancesinNeuralInformationProcessingSystems(NIPS),pages1124–1132,2013a.
11Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of
distance-based and RKHS-based statistics in hypothesis testing. Annals of Statistics, 41:2263–
2291,2013b.
ShubhanshuShekhar, Ilmun Kim, and Aaditya Ramdas. A permutation-freekernelindependence
test. JournalofMachineLearningResearch,24(369):1–68,2023.
TianhongSheng and Bharath K. Sriperumbudur. On distance and kernel measuresof conditional
independence. JournalofMachineLearningResearch,24(7):1–16,2023.
AlexanderSmola,ArthurGretton,LeSong,andBernhardSchölkopf. AHilbertspaceembedding
fordistributions. InAlgorithmicLearningTheory(ALT),pages13–31,2007.
Le Song, Alexander J. Smola, Arthur Gretton, and Karsten M. Borgwardt. A dependence maxi-
mization view of clustering. In InternationalConference on Machine Learning (ICML), pages
815–822,2007.
LeSong, AlexSmola, ArthurGretton,Justin Bedo,andKarstenBorgwardt. Featureselectionvia
dependencemaximization. JournalofMachineLearningResearch,13(1):1393–1434,2012.
BharathSriperumbudur,ArthurGretton,KenjiFukumizu,BernhardSchölkopf,andGertLanckriet.
Hilbert space embeddingsand metrics on probability measures. Journal of Machine Learning
Research,11:1517–1561,2010.
Bharath Sriperumbudur,Kenji Fukumizu, and Gert Lanckriet. Universality, characteristic kernels
andRKHSembeddingofmeasures.JournalofMachineLearningResearch,12:2389–2410,2011.
IngoSteinwart. Ontheinfluenceofthekernelontheconsistencyofsupportvectormachines. Jour-
nalofMachineLearningResearch,6(3):67–93,2001.
IngoSteinwartandAndreasChristmann. SupportVectorMachines. Springer,2008.
JeromeStenger,FabriceGamboa,MerlinKeller,andBertrandIooss.Optimaluncertaintyquantifica-
tionofariskmeasurementfromathermal-hydrauliccodeusingcanonicalmoments.International
JournalforUncertaintyQuantification,10(1),2020.
Zoltán Szabó and Bharath K. Sriperumbudur. Characteristic and universaltensor productkernels.
JournalofMachineLearningResearch,18(233):1–29,2018.
GáborJ.SzékelyandMariaL.Rizzo. Browniandistancecovariance. TheAnnalsofAppliedStatis-
tics,3:1236–1265,2009.
Gábor J. Székely, Maria L. Rizzo, and Nail K. Bakirov. Measuring and testing dependence by
correlationofdistances. TheAnnalsofStatistics,35:2769–2794,2007.
IlyaTolstikhin,BharathSriperumbudur,andBernhardSchölkopf. Minimaxestimationofmaximal
mean discrepancy with radial kernels. In Advances in Neural Information Processing Systems
(NIPS),pages1930–1938,2016.
Ilya Tolstikhin, Bharath Sriperumbudur, and Krikamol Muandet. Minimax estimation of kernel
meanembeddings. JournalofMachineLearningResearch,18:1–47,2017.
AlexandreB.Tsybakov. IntroductiontoNonparametricEstimation. Springer,2009.
Sebastien De Veiga. Globalsensitivity analysiswith dependencemeasures. JournalofStatistical
ComputationandSimulation,85(7):1283–1305,2015.
Andi Wang, Juan Du, Xi Zhang, and Jianjun Shi. Ranking features to promote diversity: An ap-
proachbasedonsparsedistancecorrelation. Technometrics,64(3):384–395,2022.
LeilaWehbeandAadityaRamdas. Nonparametricindependencetestingforsmallsamplesizes. In
InternationalJointConferenceonArtificialIntelligence(IJCAI),pages3777–3783,2015.
HolgerWendland. Scattereddataapproximation. CambridgeUniversityPress,2005.
12Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, and Masashi Sugiyama. High-
dimensionalfeatureselectionbyfeature-wisekernelizedlasso. NeuralComputation,26(1):185–
207,2014.
YangZhou,Di-RongChen,andWeiHuang. Aclassofoptimalestimatorsforthecovarianceopera-
torinreproducingkernelHilbertspaces. JournalofMultivariateAnalysis,169:166–178,2019.
V.Zolotarev. Probabilitymetrics. TheoryofProbabilityanditsApplications,28:278–302,1983.
13A AuxiliaryResult
Inthissection,wecollectanauxiliaryresult.LemmaA.1presentsanupperboundontheKullback-
Leiblerdivergencebetweenmultivariatenormaldistributions.
Lemma A.1(Upperboundon KL divergence). Let d = M d , with d N (m [M]).
m=1 m m ∈ >0 ∈
Fixi [d ]. Letj =i+1,P = (0 ,I ),andP = (µ ,Σ ),withµ = 1 1 Rd,and
∈ 1 θ0 N d d θ1 NP 1 1 1 √dn d ∈
Σ =Σ(i,j,ρ ) Rd ddefinedasin(5)(ρ (0,1)). Then,for2 n N,
1 n × n
∈ ∈ ≤ ∈
1 n ρ2
KL(Pn Pn ) + n .
θ1|| θ0 ≤ 2n 21 ρ2
− n
Inparticular,forρ2 =1/n,itholdsthatKL(Pn Pn ) 5.
n θ1|| θ0 ≤ 4
Proof. Withµ =0 andΣ =I ,weobtainthat
0 d 0 d
KL(Pn Pn )( =a) KL(P P )
θ1|| θ0 θ1|| θ0
iX∈[n]
( =b) n
2
tr(Σ −01Σ 1)+(µ
0
−µ 1)T Σ −01(µ
0
−µ 1) −d+ln |Σ Σ0 |
(cid:20) (cid:18)| 1 |(cid:19)(cid:21)
n 1
= tr(Σ )+ µ 2 d+ln
2 " 1 k 1 kRd− Σ 1 !#
| |
=d = 1
n2 ( =c) 1
| {z } | {z } 1−ρ2 n
|{z}
1 n 1 (d) 1 n ρ2 (e) 5
= + ln + n ,
2n 2 1 ρ2 ≤ 2n 21 ρ2 ≤ 4
(cid:18) − n(cid:19) − n
where(a)isimpliedbyLemmaB.1,(b)followsfromLemmaB.2,(c)followsfromthedefinitionof
thedeterminant,(d)istheconsequenceoftheinequalityln(x) x 1holdingforx > 0,and(e)
holdsforn 2andρ2 =1/nas ≤ −
≥ n
n 1/n n 1
1 1 n 2(n 1) n 2,
2 1 1/n ≤ ⇐⇒ 2n 1 ≤ ⇐⇒ ≤ − ⇐⇒ ≥
− −
1
n−1
andinthiscase(|for{nz }2)onehasthat 1 1.
≥ 2n ≤ 4
B External Theorems
For self-completeness, we include the externalstatements that we use. The well-knownresult by
Bochner,statedinTheoremB.1,completelycharacterizescontinuousboundedtranslation-invariant
kernels. TheoremB.2 allowsexpressingMMD withcontinuousboundedtranslation-invariantker-
nelsintermsofcharacteristicfunctions,andTheoremB.3givesanequivalentconditionforacontin-
uousboundedtranslation-invariantkerneltobecharacteristic. TheoremB.4connectscharacteristic
kernelstocharacteristicproductkernelsandto -characteristicproductkernelsonRd (weinclude
I
only the part relevant to our paper for brevity). We recall Le Cam’s method in TheoremB.5 and
collectresultsontheKullback-LeiblerdivergenceinLemmaB.1andLemmaB.2.
Theorem B.1 (Bochner; Theorem6.6; Wendland [2005]). A continuousfunctionκ : Rd R is
→
positivedefiniteifandonlyifitistheFouriertransformofafinitenonnegativeBorelmeasureΛon
Rd,thatis,
κ(x)= e i x,ω dΛ(ω), forallx Rd.
−h i
∈
ZRd
TheoremB.2(Corollary4(i);Sriperumbuduretal.[2010]). Letk :Rd Rd Rbeacontinuous
boundedtranslation-invariantkernel. Then,foranyP,Q + Rd , × →
∈M1
MMD2 k(P,Q)= kψ P −ψ Q k2 L2(Rd,Λ(cid:0) k),(cid:1)
withψ andψ beingthecharacteristicfunctionsofPandQ,respectively,andΛ definedin(1).
P Q k
14TheoremB.3(Theorem9;Sriperumbuduretal.[2010]). Supposek :Rd Rd Risacontinuous
boundedtranslation-invariantkernel. Thenk ischaracteristicifandonl× yifsu→ pp(Λ ) = Rd,with
k
Λ definedasin(1).
k
Theorem B.4 (Theorem 4; SzabóandSriperumbudur[2018]). Suppose k
m
: Rdm Rdm R
× →
iscontinuousboundedandtranslation-invariantkernelforallm [M]. Thenthefollowingstate-
∈
mentsareequivalent:
(i) (k )M -sarecharacteristic;
m m=1
(ii) M k ischaracteristic;
⊗m=1 m
(iii) M k is -characteristic.
⊗m=1 m I
The next statement follows directly from Tsybakov [2009, Eq. (2.9)] and Tsybakov [2009, Theo-
rem2.2].
Theorem B.5 (Theorem 2.2; Tsybakov [2009]). Let be a measurable space, (Θ,d) is a semi-
X
metric space, and = P : θ Θ is a class of probability measures on indexed by Θ.
Θ θ
P { ∈ } X
We observe data D P with some unknown parameter θ. The goal is to estimate θ.
θ Θ
Let θˆ = θˆ(D) be an∼ estima∈ torP of θ based on D. Assume that there exist θ ,θ Θ such that
0 1
∈
d(θ ,θ ) 2s>0andKL(P P ) α< forα>0. Then
0 1
≥
θ1|| θ0
≤ ∞
e α 1 α/2
infsupP d θˆ,θ s max − , − .
θ
θˆ θ Θ ≥ ≥ 4 p2 !
∈ (cid:16) (cid:16) (cid:17) (cid:17)
WehavethefollowingpropertyoftheKullback-Leiblerdivergenceforproductmeasures[Tsybakov,
2009,p.85].
LemmaB.1(KLdivergenceofproductmeasures). LetP= n P andQ= n Q . Then
⊗i=1 i ⊗i=1 i
KL(P Q)= KL(P Q ).
i i
|| ||
iX∈[n]
Thefollowinglemma[Duchi,2007,p.13]showsthattheKullback-Leiblerdivergenceofmultivari-
ateGaussianscanbecomputedinclosedform.
Lemma B.2 (KL divergence of Gaussians). The KL divergence of two normal distributions
(µ ,Σ )and (µ ,Σ )onRdis
1 1 0 0
N N
KL( (µ ,Σ ) (µ ,Σ ))=
tr(Σ −01Σ 1)+(µ 0 −µ 1)TΣ −01(µ 0 −µ 1) −d+ln | |Σ Σ0 1|
| .
N 1 1 ||N 0 0 2 (cid:16) (cid:17)
15