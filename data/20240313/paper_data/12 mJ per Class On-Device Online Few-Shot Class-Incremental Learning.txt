12 mJ per Class On-Device Online
Few-Shot Class-Incremental Learning
Yoga Esa Wibowo‡, Cristian Cioflan*†, Thorir Mar Ingolfsson†, Michael Hersche†¶,
Leo Zhao‡, Abbas Rahimi¶, Luca Benini†§
‡D-ITET, ETH Zurich; †Integrated Systems Laboratory, ETH Zurich; ¶IBM Research-Zurich; §DEI, University of Bologna
{ywibowo, lezhao}@ethz.ch, {cioflanc, thoriri, lbenini}@iis.ee.ethz.ch, {her, abr}@zurich.ibm.com
Abstract—Few-Shot Class-Incremental Learning (FSCIL) en- Although such methods achieve remarkable results on pre-
ablesmachinelearningsystemstoexpandtheirinferencecapabil- viously unseen classes, they rely on the computationally
itiestonewclassesusingonlyafewlabeledexamples,withoutfor-
expensive backpropagation algorithm. Alternative solutions,
getting the previously learned classes. Classical backpropagation- such as Neural Collapse Few-Shot Class-Incremental Learning
based learning and its variants are often unsuitable for battery-
powered,memory-constrainedsystemsattheextremeedge.Inthis (NC-FSCIL) [4], Semantic-Aware Virtual Contrastive (SAVC)
work, we introduce Online Few-Shot Class-Incremental Learning model [5], or Constrained Few-Shot Class-Incremental Learn-
(O-FSCIL), based on a lightweight model consisting of a pre- ing (C-FSCIL) [6], [7], use large ResNet backbones [8]. How-
trained and metalearned feature extractor and an expandable
ever, adapting to new requirements and learning novel classes
explicit memory storing the class prototypes. The architecture
should happen directly on the user device, thus addressing
is pretrained with a novel feature orthogonality regularization
and metalearned with a multi-margin loss. For learning a new privacy and security concerns [9], while also avoiding energy-
class, our approach extends the explicit memory with novel hungrydatastreamingtothecloud.Therefore,whenemployed
class prototypes, while the remaining architecture is kept frozen. at the extreme edge, FSCIL algorithms should operate under
This allows learning previously unseen classes based on only
the TinyML constraints [10]; hence, suitably small and com-
a few examples with one single pass (hence online). O-FSCIL
putationally affordable backbones are required.
obtains an average accuracy of 68.62% on the FSCIL CIFAR100
benchmark, achieving state-of-the-art results. Tailored for ultra- On-device training frameworks targeting Microcontroller
low-power platforms, we implement O-FSCIL on the 60mW Units (MCUs) have been proposed [11]–[13], which encour-
GAP9 microcontroller, demonstrating online learning capabilities aged the development of domain adaptation [14] and class
within just 12mJ per new class.
continual learning [15] strategies tailored for MCUs. However,
Index Terms—Continual learning, on-device learning, deep
suchiterativelearningapproachesrequirestoringtrainingsam- neural networks, microcontrollers.
ples during the update process. Our proposed online training
I. INTRODUCTION methodology addresses the storage constraints by enabling
Classical Machine Learning (ML) solutions often use large novel class learning with a single pass over the available sam-
datasetstotrainahighlycomplexyetfixedmodel,whichcannot ples.Thisalsodecreasesthelearningtimeand,thus,theenergy
adapt to the needs and requirements of the end user. Such consumption of our system. Moreover, as opposed to FSCIL
systems are nonetheless exposed to dynamic, ever-changing strategies deployed on Tensor Processing Units (TPUs) [16],
environments; thus, adaptability is a crucial requirement for Field Programmable Gate Arrays (FPGAs) [17], neuromorphic
an intelligent system. Moreover, while in a server-based, of- chips [18], or in-memory processors [7], our methodology
fline learning paradigm, curated and labeled data is widely is suitable for off-the-shelf, widely available MCUs, with a
available, that is seldom the case when a pretrained model conservative power profile.
must adapt to a particular user. FSCIL evaluates models on ThisworkintroducesO-FSCIL1,alightweightFSCILlearn-
two aforementioned problems, requiring a previously trained ing methodology aimed at resource-constrained edge devices.
modeltoexpanditsclassificationdomainwhileprovidingvery Using orthogonal regularization and data augmentation strate-
few labeled training samples. The key challenge is to prevent gies such as Mixup [19] and Cutmix [20], we pretrain the
catastrophic forgetting (i.e., forgetting prior knowledge during MobileNetV2 [21] backbone, which requires only 2.5 million
knowledge expansion) and avoid overfitting to the few novel parameters and 149.2 million Multiply-Accumulate (MAC)
samples. These goals can be achieved by balancing stability operations. During the server-side metalearning phase, we
andplasticity[1].SeveralFSCILsolutionshavetackledsample- employ a multi-margin loss to prevent overfitting to the few
scarce class learning over several incremental sessions [2], [3] labeledsamples.Wefurtherdemonstratetheon-devicelearning
in a transfer learning fashion, where a model is pretrained to capabilitiesbydeployingandevaluatingO-FSCILonultra-low-
classify a set of predefined base classes, followed by freezing power, commercially available GAP9 [22].
its backbone and only training its classification head to learn The main contributions of the paper are as follows:
novel classes.
*Correspondingauthor 1Thecodewillbeopen-sourcedat:https://github.com/pulp-platform/fscil
©2024IEEE.Personaluseofthismaterialispermitted.PermissionfromIEEEmustbeobtainedforallotheruses,inanycurrentorfuturemedia,including
reprinting/republishingthismaterialforadvertisingorpromotionalpurposes,creatingnewcollectiveworks,forresaleorredistributiontoserversorlists,or
reuseofanycopyrightedcomponentofthisworkinotherworks.
4202
raM
21
]GL.sc[
1v15870.3042:viXra• We introduce O-FSCIL, a class-incremental learning single-platform families. Notably, Nadalini et al. [12] propose
method achieving a new state-of-the-art average accuracy a framework aimed at both single- and multi-core platforms,
of 68.62% on the FSCIL CIFAR100 [23] benchmark. accounting for the particular memory hierarchy of each target
• For each novel class, our pretrained and metalearned device. To improve accessibility, in [29] the authors introduce
backbone generates orthogonal feature vectors quantized a continual learning framework for smartphones. Conversely,
to 3-bit integers, which are stored in the Explicit Memory Chen et al. [17] introduce a reconfigurable array architecture
(EM), resulting in memory requirements of only 9.6kB to accelerate backpropagation through uniform memory ac-
for 100 classes. cess patterns, with 410mW power consumption on FPGA.
• Wedemonstratefew-shotonline(i.e.,single-pass)learning Nonetheless, such works address backpropagation-based train-
capabilities on a 50mW, requiring as little as 12mJ to ing, which is generally unfit for extreme edge devices due
learn a new class. to memory and computational requirements. Furthermore,
backpropagation-based fine-tuning requires large amounts of
II. RELATEDWORK labeled data, often unavailable in real-world settings.
A. Few-shot Class-incremental Learning In order to solve the extreme edge learning challenges,
several on-device (few-shot) class-incremental learning imple-
DifferentapproacheshavebeenproposedtotackletheFSCIL
mentations have been proposed. Hacene et al. [30] introduce a
scenario, in which models presented with out-of-distribution
featureextractorwhosefeaturesarecomparedwithclassanchor
data learn new classes from few labeled data. TOPIC [1]
points,theirarchitectureimplementedina22WFPGA.Lungu
introduces the FSCIL problem and employs a neural gas
et al. [16] use Siamese Networks to compute the similarity
techniquetomaintainthetopologyintheembeddingspace.By
measurebetweenaqueryandclassprototypes,withtheirFPGA
updatingthebackboneoftheneuralnetwork[1],[24],[25],one
implementation enabling class learning within 35ms. Neuro-
can successfully learn to recognize previously unseen classes.
morphic chips have also been employed for continual [18] and
To learn new classes and avoid catastrophic forgetting without
few-shot [31] on-device learning, with energy requirements of
maintaining a large reservoir memory, we freeze the backbone
167mJ per class in a data-scarce context. Instead, we propose
and only store class prototypes in the explicit memory.
a methodology enabling real-time inference and learning, with
To reduce the costs of retraining the backbone, other
energy requirements as low as 12mJ.
works [2], [3] proposed to freeze the backbone and operate
on the classifier and its additional components. Works such III. FSCILTASKDESCRIPTION
as [2], [3], [26] rely on an episodic memory, where class-
In FSCIL, a learner is progressively presented with new
specific information is stored and compared with the query
classes over a number of training sessions. The data stream is
image during inference. Zhou et al. [26] introduce a forward
denoted as D
={Dt}T
, where t indicates the session index.
t=0
compatibility methodology, where a provident model mini- The class labels yt ∈ Ct does not intersect across sessions,
n
mizes the negative compression effects novel classes have i.e., ∀i̸=j,Ci∩Cj =∅. During the base session, or session 0,
on the embedding space. ALICE [27] proposes the angular
the model is pretrained and metalearned to gain representation
penaltylosstoachievecompactclusteringandfeaturediversity,
knowledge of the input sequence. Subsequently, in the online
achieving better generalization for unseen classes. Conversely,
stage, the incremental sessions introduce N new classes, each
SAVC [5] enhance the cluster separation with virtual classes
with S samples per class, referred to as N-way, S-shot FSCIL.
created through predefined transformations, which diversify
The model is evaluated on samples from all previous classes
the semantic information. However, these methods do not C(cid:102)t :=C0∪C1···∪Ct, ensuring that new classes are learned
generate meaningful feature representations in space between
without forgetting previous ones.
class clusters, thus impeding clustering in that vacant space.
NC-FSCIL [4] tackles the clustering problem by creating a IV. ONLINEFEW-SHOTCLASS-INCREMENTALLEARNING
placeholder for all class prototypes with fixed, predetermined Thissectionpresentsthefirstmaincontributionofthepaper:
vectors, thus addressing the prototype readjustment issue when we introduce O-FSCIL, which is comprised of a backbone,
adding a new class cluster. As opposed to these works, our a Fully Connected Reductor (FCR), and an EM, shown in
methodology improves features’ representations and expres- Fig. 1. The backbone (f(·)) maps an input image (x) to an
sivenessalreadyduringthepretrainingandmetalearningstages, intermediate representation (θ
a
∈ Rda), and the FCR projects
through feature interpolation, as well as orthogonality and the representation to a lower-dimensional feature (θ
p
∈ Rdp
multi-margin loss-based network update. This allows us to where d < d ). During inference, the feature vector (θ ) is
p a p
achievehigheraccuracylevelswithinexpensivefew-shotclass- comparedagainstallprototypesstoredintheEM;theprototype
incremental learning, suitable for on-device deployment. with the highest cosine similarity indicates the final prediction
(see Fig. 1a).
B. On-device continual learning
When learning a new class i, the prototype (θ ,i) is added
p
While deployment frameworks enabling Deep Neural Net- to the EM through averaging all θ feature of class i samples,
p
work (DNN) inference are already well established [28], on- while the backbone and FCR remains frozen. This allows for
device training frameworks [11], [13] are yet to generalize to online updates by passing labeled images through the model
multple Deep Learning (DL) topologies and generally target only once, without requiring expensive iterative (batched)Backbone EM
cos(θ) cos(θ)
Backbone EM
Backbone Backbone
(a) Classifying a query in inference mode by (b) O-FSCIL update of EM with the class av- (c) Server-side metalearning the model on the
comparing its features with class prototypes erageofFCR-generatedfeatures.Thebackbone base session. The loss is used to update the
storedinEM. andtheFCRarefrozen. backboneandtheFCR.
Fig. 1: Inference (a), on-device learning a new class (b), and server-side metalearning (c) modes of O-FSCIL. Modules colored
in orange are updated, while grey ones are frozen. During pretraining, we replaced the prototype computation and EM update
from (b) with an FCR-like FCC classifier, with all three sections jointly trained.
TABLE I: Proposed backbones. Here we represent the con-
prototype features. The FCC layer reduces the dimension of
volutional stride per inverted residual block in MobileNetV2, θ
p
from Rdp to R|C0|, where |C0| is the number of the base
as well as the dimensionality of the FCR features d a and classes and d > |C0|. The classification boundary of FCC
p
prototypical features d .
p only lives in the smaller base class hyperplane with dimension
MobileNetV2 Resnet12 equal to |C0|, ignoring perpendicular features. This impedes
x2 x4 the generation of new feature clusters on orthogonal planes,
CNNstride 1,2,2,2,1,2,1 1,2,2,2,1,1,1 1,2,2,1,1,1,1 -
hindering the model’s ability to learn new classes.
da 1280 1280 1280 640
We propose feature orthogonality regularization to address
dp 256 256 256 512
Params.[M] 2.5 2.5 2.5 12.9 the dimensionality reduction problem. Instead of applying
MACs[M] 25.9 45.4 149.2 525.3 weightmatrixorthogonalregularization[32],weorthogonalize
the feature vectors and study its generalization capability to
gradient updates. This online learning capability is acquired novel classes in the FSCIL scenario. Equation 1 shows the
via pretaining and metalearning, which are performed before formulation of our orthogonality regularization:
deploying the model using data from the base session.
L =(θt ×θ −I )2, (1)
A. Light-weight backbone ortho pb pb df
where θ is a batch of θ bundled as matrix with dimension
ToenableFSCILattheextremeedge,wereplacetheResNet- pb p
RB×dp, and B is the batch size. The proposed pretraining loss
12 backbone [5], [6] with a lightweight MobileNetV2 [21]. To
includes the classification CE loss and the orthogonality loss,
handle low-resolution inputs (i.e., 32×32 in CIFAR100 [23]),
weighted by the λ regularization strength:
we reduce the stride across the convolutional blocks of the ortho
seven inverted residual blocks, obtaining three models with
L =L +λ ·L , (2)
differentcomplexity.Theproposednetworksandtheirhardware pre ce ortho ortho
costs are presented in Table I, together with the dimensions of C. Metalearning
the FCR features (d a) and prototypical features (d p). After pretraining, we perform offline metalearning to en-
hancefeatureclusteringbyemulatingthelearningandinference
B. Pretraining
processesonthebasesession[33],asshowninFig.1c.Wetrain
ThepretrainingguidesthebackboneandFCRtowardsmean- the backbone and FCR for multiple iterations, re-computing
ingful representations by solving a supervised classification the class prototypes from meta-samples, N randomly selected
problem on the base session (with |C0| classes). To this end, images per class. After generating the class prototypes, we
we modify the topology shown in Fig. 1b by replacing the EM computethecosinesimilarity(cossim)betweenaquerysample
with a Fully Connected Classifier (FCC), which computes the (x) and a class prototype (θ ):
p,i
class probability of an input image. The backbone, the FCR,
l =ReLU(cossim(FCR(f(x)),θ ). (3)
and the FCC are mutually trained to minimize the Cross- i p,i
Entropy (CE) loss. To improve the accuracy of our system, Note that the backbone-extracted features are employed for x,
we implement data augmentation methods enabling feature whereas the prototypes, generated by clustering the features
interpolation and a novel feature orthogonality regularization. extracted using the same backbone, are stored in the EM, as
Feature interpolation: Apart from traditional data augmen- illustratedinFig.1c.InspiredbyMANN[34],weusetheReLU
tation techniques (i.e., blur, horizontal flip, crop, and resize), sharpening function to induce quasi-orthogonality.
we use inter-class feature interpolation using Mixup [19] and Although CE shows good performance in previous stud-
CutMix[20].Thus,insteadofgeneratingmultipleinputsfroma ies [5], [6], it draws the features towards the cluster’s centroid
singleimage,wecombinetwoinputimagesandcreateinterme- without accounting for the confidence level. This can lead to
diate class labels. The two methods are employed exclusively, overfittingregions,wherehighconfidencepointsareprioritized.
with a probability of 0.4. To this end, we employ a multi-margin loss defined as:
Feature orthogonality: The pretraining of the architecture (cid:80) max(0,(m−l +l ))2
L (l,y)= i gt i , (4)
with the CE loss can reduce the representationality of the MM |C0|
EM
FCR
PREDICTION
FCR
FCR
FCRwhere y is the ground truth label and m denotes margin value, works by more than one percentage point averaged over eight
which set to m = 0.1 after grid search. Multi-margin loss learning sessions. Notably, our pretraining and metalearning
spreads the features near the classification frontier, improving leadtoanaccuracygainof1.5%inthebasesession,pavingthe
the accuracy on distant points while maintaining its perfor- wayforrobustincrementallearninginlaterstages.Thisfurther
mance on those near the prototype. enables us to employ O-FSCIL on lightweight MobileNetv2
backbones, achieving average test accuracy levels of up to
V. HARDWAREDEPLOYMENT
66.54% for MobileNetV2 x4, outperformed only by 1% by
A. Deployment thelargerNC-FSCIL.Notably,theminimalaccuracyreduction
comeswitha5.7×reductionincomputationaleffortanda5.2×
We deploy and evaluate our model on the GAP9 MCU, a
decrease in storage requirements compared to the ResNet12
multi-core Single Instruction Multiple Data (SIMD) processor
backbone, as shown in Table I.
suitable for vector and matrix computations, present in neural
Remarkably, our metalearning strategy allows us to generate
networks. GAP9 comprises two processing components, a
robust, separable sample projections without expensive retrain-
fabric controller, handling control and communications, and
ing of the FCR. Nevertheless, if backpropagation-based fine-
a 9-core cluster designed to efficiently execute parallelized
tuning is employed on MobileNetV2 x4, an additional 0.2%
algorithms. All cores share access to an L1 memory and
is to be gained, yet this would incur an adaptation cost of up
instruction cache, an L2 memory is shared by the compute
to 6.6G MACs/session, compared to 0.7G MACs/session on
processors, and the system also supports an external 8MB L3
MobileNetV2 x4 without any fine-tuning. The computational
memory. Furthermore, Direct Memory Access (DMA) units
effort reduction of 8.8× further motivates the extreme edge
enable asynchronous L1↔L2 and L2↔L3 memory transfers.
potential of our architecture.
TodeployourmodelonGAP9,wefirstquantizetheweights
and activations of our metalearned Floating-Point (FP) model
B. Ablation study
to8-bitintegersusingTrainingQuantizationThresholds(TQT)
algorithm in Quantlib [35], with additional quantization-aware This section investigates the benefits of our proposed pre-
pretraining and metalearning epochs. The network is then training and metalearning. As a baseline, we consider the
deployed using Dory [28], whereas an additional IO interface ResNet-12-pretrained O-FSCIL architecture. Firstly, we notice
layerisimplementedtosupporttheevaluationofourO-FSCIL. that data augmentation generates accuracy gains of 2.15% on
the base session and 2.4% on the eighth one. Second, adding
B. On-Device learning
orthogonal regularization in the pipeline significantly boosts
During the deployment, the MCU performs online novel performance, particularly for the new classes, leading to accu-
class learning, computing the corresponding class prototype racy increments between 1.65% and 2.87%. This confirms that
and storing it to EM. For inference, a query image is clas- orthogonalization encourages neural networks to learn useful
sified according to the class prototype with the highest cosine features beyond those of the base classes. Interestingly, CE
similarity with the query θ feature. To increase the number metalearning incurs performance degradation. Corroborating
p
of classes that can be learned on the device, we reduce the thiswithCElossreductionsnoticedduringtrainingwithoutthe
memory requirements of EM. We study precision reductions accuracy classification gains, we conclude that CE discourages
through bit-shift divisions in Section V-A. feature robustness and generalization.
Similar to Mode 2 in C-FSCIL [6], we implement an
optionalFCRfinetuning,whilefreezingthebackbone.Tomin- C. Deployment
imizethetrainingeffort,westoreaverageclassactivationsθ
a,i WequantizeO-FSCILusingTQT[36],withthreepretraining
in an activation memory. We then iteratively update the FCR
epochs and ten metalearning iterations following the quantiza-
bymaximizingthesimilaritybetweentheFCRmappingofθ
a,i tion.AsshowninTableII,similaraccuracylevelsaremeasured
and the bipolarized class prototype through batched gradient whencomparingint8-quantizedmodelswithfp32networks.
descent over B iterations. To minimize the memory accesses,
Interestingly, quantized networks achieve higher accuracy for
we develop a sub-batching mechanism that creates three input
the latter sessions, as reducing the precision acts as a regular-
matricesfromN pairsofθ ,FCR(θ ),andθ .Thisallows
a,i a,i p,i izer, improving the features’ separability.
forcomputingtheaccumulatedgradientofN samples,reducing
We furthermore measured the latency, power, and energy
the number of memory accesses to B/N per batch.
consumptionforourquantizedanddeployedmodels,presented
in Table IV. We deploy our models on the GAP9 MCU, oper-
VI. EXPERIMENTALRESULTS
ating at 650mV, 240MHz as this is the most energy-efficient
A. CIFAR100 benchmark
operatingpointfortheMCU.Notably,wecanperformbothin-
We use the CIFAR100 [23] dataset to evaluate our archi- ferenceandtraininginreal-time,asO-FSCILlearnsanewclass
tecture, split into three sets: base session (50 images/class for only 256ms. Moreover, we remain within the 50mW power
60 classes), class-incremental learning sessions (eight 5-way, envelope also for backpropagation-based FCR. By performing
5-shot sessions) and 100 images per class for the test set. both EMupdateandlastlayerfinetuningonMobileNetV2 x4,
As shown in Table II, O-FSCIL achieves state-of-the-art GAP9 draws up to 320mJ per new class. Without the finetun-
accuracy with ResNet-12 as a backbone, outperforming other ing,itcanefficientlylearnnewclassesconsuming 12mJwithTABLE II: O-FSCIL accuracy on CIFAR100. FP32 models were evaluated on NVIDIA GeForce GTX 1080 Ti, INT8 models
were evaluated on GAP9 MCU. FT represents optional iterative FCR fine-tuning.
Sessionaccuracy[%]
Method Backbone Prec. Size[MB] 0 1 2 3 4 5 6 7 8 Avg.
MetaFSCIL[33] ResNet20 FP32 1.08 74.50 70.10 66.84 62.77 59.48 56.52 54.36 52.56 49.97 60.79
C-FSCIL[6] ResNet12 FP32 51.6 77.47 72.40 67.47 63.25 59.84 56.95 54.42 52.47 50.47 61.64
LIMIT[26] ResNet20 FP32 1.08 73.81 72.09 67.87 63.89 60.70 57.77 55.67 53.52 51.23 61.84
SAVC[5] ResNet12 FP32 51.6 78.47 72.86 68.31 64.00 60.96 58.28 56.17 53.91 51.63 62.73
ALICE[27] ResNet18 FP32 66.8 79.00 70.50 67.10 63.40 61.20 59.20 58.10 56.30 54.10 63.21
NC-FSCIL[4] ResNet12 FP32 51.6 82.52 76.82 73.34 69.68 66.19 62.85 60.96 59.02 56.11 67.50
O-FSCIL FP32 51.6 84.05 79.10 74.23 69.96 66.92 63.89 61.67 59.51 57.10 68.52
ResNet12
O-FSCIL+FT FP32 51.6 84.02 79.08 74.34 70.11 66.95 64.00 61.86 59.72 57.50 68.62
FP32 10.0 77.51 72.82 68.41 64.25 61.24 57.98 55.32 53.03 50.73 62.37
O-FSCIL MobileNetV2
INT8 2.5 76.97 72.46 68.24 64.19 61.07 58.14 56.01 53.55 51.37 62.44
FP32 10.0 78.37 73.47 69.20 64.94 61.20 58.24 55.33 53.49 51.51 62.86
O-FSCIL MobileNetV2 x2
INT8 2.5 78.13 73.54 69.27 65.13 61.85 58.73 56.21 54.04 51.81 63.19
FP32 10.0 81.79 77.37 72.73 68.42 64.87 61.76 59.76 57.21 54.95 66.54
O-FSCIL MobileNetV2 x4
INT8 2.5 81.56 76.81 72.56 68.32 64.95 61.94 59.65 57.47 55.33 66.51
FP32 10.0 81.90 77.31 72.90 68.48 65.09 61.91 59.73 57.72 55.45 66.75
O-FSCIL+FT MobileNetV2 x4
INT8 2.5 81.53 76.82 72.54 68.44 64.81 61.76 59.65 57.78 55.72 66.56
TABLE III: Ablation study of the proposed methods for the
Backbone FCR Finetune
accuracy[%] on CIFAR100. AG: augmentation, OR: orthogo-
nal regularization, MM: multi-margin-based metalearning, CE: 7 0.7 1.4
MobileNetV2
cross-entropy-based metalearning, FT: incremental fine-tuning. 6 0.6 1.2
MobileNetV2 x2
The experiments were conducted with ResNet12 backbone. 5 MobileNetV2 x4 0.5 1
Session 4 0.4 0.8
AG OR MM CE FT Avg
0 8 3 0.3 0.6
79.72 51.47 62.94
✓ 81.87 53.85 64.77 2 0.2 0.4
✓ ✓ 83.52 56.72 67.88 1 0.1 0.2
✓ ✓ 83.65 56.25 67.56 0 0 0
✓ ✓ ✓ 84.05 57.10 68.52
✓ ✓ ✓ 83.02 51.54 64.56 1 2 4 8 1248 1248
✓ ✓ ✓ ✓ 84.02 57.50 68.62
Fig. 2: Average number of operations per cycle given the
TABLE IV: The execution time, power, and energy consump- number of active cores, for backbone inference (left), FCR
tion on GAP9 for O-FSCIL – EM update, emphasized – and inference (centre), and FCR backpropagation update (right).
for FCR finetuning, added for comparison. The results are
reported per class, for a five-shot learning setting. Finetuning
is performed for 100 epochs. BB denotes the backbone.
Operation BB Time[ms] Power[mW] Energy[mJ] gainsofparallelizingtheFCRlayeronamulti-corearchitecture
FCR ∀ 3.23±0.73 47.75±0.34 0.15±0.01
are not as pronounced as those for other layers, primarily
M 48.10±5.14 43.96±0.98 2.12±0.23
BBinference M2 52.51±5.27 45.12±0.24 2.40±0.24 due to the data transfer overheads. Specifically, transferring
M4 99.50±2.41 44.19±0.64 4.40±0.12 approximately 328kB of data from L3 to the L1 caches incurs
M 256.65±11.6 44.22±0.84 11.35±0.24 a latency of roughly 3 ms. In contrast, when parallelized, the
EMupdate M2 278.70±11.9 45.75±0.26 12.75±0.25
actualcomputationconsumesonlyabout0.25ms,showcasinga
M4 513.65±5.63 44.29±0.59 22.75±0.12
M 6171.7±29.8 50.29±0.55 310.35±0.72 significant speedup of nearly 5 times on the multi-core system.
FCRfinetune M2 6193.7±29.9 50.33±0.52 311.75±0.74
M4 6428.7±28.0 50.05±0.54 321.75±0.58 To reduce the memory requirements, we analyze the impact
of the memory precision θ on the accuracy, shown in Fig. 3.
p,i
minor degradation, demonstrating the feasibility of O-FSCIL A 17-bit integer is sufficient to represent the class prototype
for battery-operated devices. without overflow for MobileNetV2 x4. We can further reduce
Fig. 2 illustrates the impact of the multi-core architecture of the θ bit length to an 8-bit integer by performing a 9-
p,j
GAP9 on O-FSCIL. Highly parallelizable given the presence bit right shift (i.e., vector division), reducing the norm while
of convolutional layers, we can achieve up to 6.5MACs/cycle maintainingthegeneralθ vectordirectionandpreservingthe
p,j
for our largest MobileNetV2 backbone with 8 cores. The accuracy.Furtherreductionsdownto3-bitclassprototypescan
parallelization potential reduces as we increase the number of be achieved without accuracy drops, thus enabling us to store
strided convolutions in the feature extractor. The performance 100 class prototypes with only 9.6kB.
elcyc/sCAMMemory requirement (kB) [10] C. R. Banbury et al., “Benchmarking tinyml systems: Challenges and
direction,”arXivpreprintarXiv:2003.04821,2020.
102.4 25.6 22.4 19.2 16. 12.8 9.6 6.4 3.2 [11] H.Renetal.,“Tinyol:Tinymlwithonline-learningonmicrocontrollers,”
in 2021 International Joint Conference on Neural Networks (IJCNN),
2021,pp.1–8.
80 [12] D. Nadalini et al., “PULP-TrainLib: Enabling On-Device Training For
RISC-VMulti-CoreMCUsThroughPerformance-DrivenAutotuning,”in
EmbeddedComputerSystems:Architectures,Modeling,andSimulation.
70
SpringerInternationalPublishing,2022,p.200–216.
[13] J. Lin et al., “On-device training under 256kb memory,” in Annual
60 ConferenceonNeuralInformationProcessingSystems(NeurIPS),2022.
[14] C. Cioflan et al., “Towards on-device domain adaptation for noise-
50 robust keyword spotting,” in 2022 IEEE 4th International Conference
Session 0 Session 8 onArtificialIntelligenceCircuitsandSystems(AICAS),2022,pp.82–85.
[15] L. Ravaglia et al., “A tinyml platform for on-device continual learning
32 8 7 6 5 4 3 2 1(sign) withquantizedlatentreplays,”IEEEJournalonEmergingandSelected
Topics in Circuits and Systems (JETCAS), vol. 11, no. 4, pp. 789–802,
Bit precision
2021.
[16] I. A. Lungu et al., “Siamese networks for few-shot learning on edge
Fig. 3: The representation precision in the episodic memory embedded devices,” IEEE Journal on Emerging and Selected Topics in
impacts the accuracy of MobileNetV2 x4-based model, con- CircuitsandSystems(JETCAS),vol.10,no.4,pp.488–497,2020.
[17] X.Chenetal.,“Eile:Efficientincrementallearningontheedge,”in2021
sidering 100 class prototypes stored in the memory.
IEEE3rdInternationalConferenceonArtificialIntelligenceCircuitsand
VII. CONCLUSION Systems(AICAS),2021,pp.1–4.
[18] E.Hajizadaetal.,“Interactivecontinuallearningforrobots:Aneuromor-
We proposed O-FSCIL, a novel FSCIL methodology using
phicapproach,”inProc.oftheInternationalConferenceonNeuromorphic
orthogonal regularization and multi-margin-based metalearn- Systems(ICONS). NewYork,NY,USA:ACM,2022.
ing to improve feature separability in incoming classes. We [19] H. Zhang et al., “mixup: Beyond Empirical Risk Minimization,” in
InternationalConferenceonLearningRepresentations(ICLR),2018.
achieved state-of-the-art accuracy on the CIFAR100 dataset
[20] S.Yunetal.,“Cutmix:Regularizationstrategytotrainstrongclassifiers
using ResNet12 and comparable results with NC-FSCIL when with localizable features,” in Proc. of the IEEE/CVF Conference on
using the 5× smaller and 3× more computationally efficient ComputerVisionandPatternRecognition(CVPR),2019,pp.6023–6032.
[21] M. Sandler et al., “Mobilenetv2: Inverted residuals and linear bottle-
MobileNetV2 x4. We moreover designed, deployed, and eval-
necks,”inProc.oftheIEEEConferenceonComputerVisionandPattern
uated O-FSCIL on GAP9 MCU, with energy requirements Recognition(CVPR),2018,pp.4510–4520.
to learn a new class as low as 12mJ, making it suitable for [22] “Gap9 product brief,” https://greenwaves-technologies.com/wp-content/
uploads/2023/02/GAP9-Product-Brief-V1 14 non NDA.pdf, accessed:
battery-operated extreme edge devices.
2023-08-10.
[23] A.Krizhevsky,“Learningmultiplelayersoffeaturesfromtinyimages,”
VIII. ACKNOWLEDGEMENTS
UniversityofToronto,2009.
This work was partly supported by the Swiss National [24] S.Dongetal.,“Few-shotclass-incrementallearningviarelationknowl-
Science Foundation under grant No 207913: TinyTrainer: On- edge distillation,” in Proc. of the AAAI Conference on Artificial Intelli-
gence,vol.35,no.2,2021,pp.1255–1263.
chip Training for TinyML devices.
[25] H.Zhaoetal.,“Mgsvf:Multi-grainedslowvs.fastframeworkforfew-
REFERENCES shotclass-incrementallearning,”IEEETransactionsonPatternAnalysis
andMachineIntelligence,2021.
[1] X. Tao et al., “Few-shot class-incremental learning,” in Proc. of the [26] D.-W. Zhou et al., “Few-shot class-incremental learning by sampling
IEEE/CVF Conference on Computer Vision and Pattern Recognition multi-phasetasks,”IEEETransactionsonPatternAnalysisandMachine
(CVPR),2020,pp.12183–12192. Intelligence,2022.
[2] C.Zhangetal.,“Few-shotincrementallearningwithcontinuallyevolved [27] C. Peng et al., “Few-shot class-incremental learning from an open-
classifiers,” in Proc. of the IEEE/CVF Conference on Computer Vision set perspective,” in European Conference on Computer Vision (ECCV).
andPatternRecognition(CVPR),2021,pp.12455–12464. Springer,2022,pp.382–397.
[3] K. Zhu et al., “Self-promoted prototype refinement for few-shot class- [28] A.Burrelloetal.,“Dory:Automaticend-to-enddeploymentofreal-world
incrementallearning,”inProc.oftheIEEE/CVFConferenceonComputer dnnsonlow-costiotmcus,”IEEETransactionsonComputers,2021.
VisionandPatternRecognition(CVPR),2021,pp.6801–6810. [29] L. Pellegrini et al., “Continual learning at the edge: Real-time training
[4] Y. Yang et al., “Neural collapse inspired feature-classifier alignment onsmartphonedevices,”arXivpreprintarXiv:2105.13127,2021.
for few-shot class-incremental learning,” in The Eleventh International [30] G.B.Haceneetal.,“Incrementallearningonchip,”in2017IEEEGlobal
ConferenceonLearningRepresentations(ICLR),2023. ConferenceonSignalandInformationProcessing(GlobalSIP),2017,pp.
[5] Z. Song et al., “Learning with fantasy: Semantic-aware virtual con- 789–792.
trastive constraint for few-shot class-incremental learning,” in Proc. of [31] K. Stewart et al., “Online few-shot gesture learning on a neuromorphic
theIEEE/CVFConferenceonComputerVisionandPatternRecognition processor,” IEEE Journal on Emerging and Selected Topics in Circuits
(CVPR),2023,pp.24183–24192. andSystems,vol.10,pp.512–521,2020.
[6] M. Hersche et al., “Constrained few-shot class-incremental learning,” [32] K. Ranasinghe et al., “Orthogonal projection loss,” in Proc. of the
in Proc. of the IEEE/CVF Conference on Computer Vision and Pattern IEEE/CVF Conference on Computer Vision and Pattern Recognition
Recognition(CVPR),2022,pp.9057–9067. (CVPR),2021,pp.12333–12343.
[7] G. Karunaratne et al., “In-memory realization of in-situ few-shot con- [33] Z. Chi et al., “Metafscil: A meta-learning approach for few-shot class
tinual learning with a dynamically evolving explicit memory,” in IEEE incrementallearning,”inProc.oftheIEEE/CVFConferenceonComputer
48th European Solid State Circuits Conference (ESSCIRC), 2022, pp. VisionandPatternRecognition(CVPR),2022,pp.14166–14175.
105–108. [34] G. Karunaratne et al., “Robust high-dimensional memory-augmented
[8] K. He et al., “Deep residual learning for image recognition,” in 2016 neuralnetworks,”Naturecommunications,vol.12,no.1,p.2468,2021.
IEEEConferenceonComputerVisionandPatternRecognition(CVPR), [35] M.Spallanzanietal.,“Quantlab:amodularframeworkfortrainingand
2016,pp.770–778. deployingmixed-precisionnns,”March2022.
[9] S.Kumaretal.,“InternetofThingsisarevolutionaryapproachforfuture [36] S.Jainetal.,“Trainedquantizationthresholdsforaccurateandefficient
technology enhancement: a review,” Journal of Big Data, vol. 6, no. 1, fixed-pointinferenceofdeepneuralnetworks,”Proc.ofMachineLearn-
p.111,2019. ingandSystems,vol.2,pp.112–128,2020.
]%[
ycaruccA