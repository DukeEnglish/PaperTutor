Distilling the Knowledge in Data Pruning
Emanuel Ben-Baruch, Adam Botach, Igor Kviatkovsky, Manoj Aggarwal, and
Gérard Medioni
Amazon
{emanbb, kabotach, kviat, manojagg, medioni}@amazon.com
Abstract. Withtheincreasingsizeofdatasetsusedfortrainingneural
networks, data pruning becomes an attractive field of research. How-
ever, most current data pruning algorithms are limited in their ability
to preserve accuracy compared to models trained on the full data, es-
pecially in high pruning regimes. In this paper we explore the appli-
cation of data pruning while incorporating knowledge distillation (KD)
when training on a pruned subset. That is, rather than relying solely
on ground-truth labels, we also use the soft predictions from a teacher
networkpre-trainedonthecompletedata.ByintegratingKDintotrain-
ing, we demonstrate significant improvement across datasets, pruning
methods, and on all pruning fractions. We first establish a theoretical
motivation for employing self-distillation to improve training on pruned
data. Then, we empirically make a compelling and highly practical ob-
servation:usingKD,simplerandompruningiscomparableorsuperiorto
sophisticatedpruningmethodsacrossallpruningregimes.OnImageNet
for example, we achieve superior accuracy despite training on a random
subset of only 50% of the data. Additionally, we demonstrate a crucial
connectionbetweenthepruningfactorandtheoptimalknowledgedistil-
lationweight.Thishelpsmitigatetheimpactofsampleswithnoisylabels
and low-quality images retained by typical pruning algorithms. Finally,
we make an intriguing observation: when using lower pruning fractions,
largerteachersleadtoaccuracydegradation,whilesurprisingly,employ-
ing teachers with a smaller capacity than the student’s may improve
results. Our code will be made available.
Keywords: Datapruning·Knowledgedistillation·Imageclassification
1 Introduction
Recently, data pruning has gained increased interest in the literature due to
the growing size of datasets used for training neural networks. Algorithms for
data pruning aim to retain the most representative samples of a given dataset
and enable the conservation of memory and reduction of computational costs
by allowing training on a compact and small subset of the original data. For
instance, data pruning can be useful for accelerating hyper-parameter optimiza-
tionorneuralarchitecturesearch(NAS)efforts.Itmayalsobeusedincontinual
learning or active learning applications.
4202
raM
21
]VC.sc[
1v45870.3042:viXra2 E. Ben-Baruch et al.
(a) In knowledge distillation for model compression (left), a large teacher network is
usedtoguidethetrainingofasmallerstudentnetwork.Incontrast,hereweinvestigate
the usage of a teacher model, pre-trained on a full dataset, to guide a student model
during training on a pruned subset of the data (right).
(b)Accuracyvs.pruningmethods(CIFAR-100) (c) Impactofteachersize(CIFAR-100)
Fig.1: Knowledge distillation for data pruning. (a) The difference between KD
for model compression and KD for data pruning. (b) We find that by integrating KD
into the training, simple random pruning outperforms other sophisticated pruning al-
gorithmsacrossallpruningregimes.(c)Interestingly,weobservethatwhenusingsmall
data fractions, training with large teachers degrades accuracy, while smaller teachers
are favored. This suggests that in high pruning regimes (low f), the training is more
sensitive to the capacity gap between the teacher and the student.
Existingmethodsfordatapruninghaveshownremarkablesuccessinachiev-
inggoodaccuracywhileretainingonlyafraction,f <1,oftheoriginaldata;see
for example [10,25,32,38] and the overview in [12]. However, those approaches
are still limited in their ability to match the accuracy levels obtained by models
trainedonthecompletedataset,especiallyinhighcompressionregimes(lowf).
Score-baseddatapruningalgorithmstypicallyrelyontheentiredatatotrain
neural networks for selecting the most representative samples. The ‘forgetting’
method [38] counts for each sample the number of instances during training
where the network’s prediction for that sample shifts from “correct” to “misclas-
sified”. Samples with high rates of forgetting events are assigned higher scores
as they are considered harder and more valuable for the training. The GraNd
and EL2N methods [32] compute a score for each sample based on the gradient
norm (GraNd) or the error L2-norm (EL2N) between the network’s prediction
and the ground-truth label, respectively. The scores are computed and averagedDistilling the Knowledge in Data Pruning 3
over an ensemble of models trained on the full dataset. For each method, we
note that once the sample scores are calculated, the models trained on the full
dataset are discarded and are no longer in use.
In this paper, we explore the benefit of using a model trained on a complete
dataset to enhance training on a pruned subset of the data using knowledge
distillation (KD). The motivation behind this approach is that a teacher model
trained on the complete dataset captures essential information and core statis-
tics about the entire data. This knowledge can then be utilized when training
on a pruned subset. While KD has been extensively studied and demonstrated
significant improvements in tasks such as model compression, herein we aim to
investigate its impact in the context of data pruning and propose innovative
findingsforpracticalusage.Notethat,incontrasttotraditionalmodelcompres-
siontechniques,ourapproachfocusesonself-distillation(SD),wheretheteacher
and student have identical architectures. An overview of the proposed training
scheme is presented in Fig. 1a.
Weexperimentallydemonstratethatincorporatingthe(soft)predictionspro-
vided by the teacher throughout the training process on the pruned data sig-
nificantly and consistently improves accuracy across multiple datasets, various
pruning algorithms, and all pruning fractions (see Fig. 1b for example). In par-
ticular,usingKD,wecanachievecomparableorevenhigheraccuracywithonly
asmallportionofthedata(e.g.,retaining50%and10%ofthedataforCIFAR-
100 and SVHN, respectively). Moreover, a dramatic improvement is achieved
especially for small pruning fractions (low f). For example, on CIFAR-100 with
pruning factor f =0.1, accuracy improves by 17% (from 39.8% to 56.8%) using
random pruning. On ImageNet with f = 0.1, the Top-5 accuracy increases by
5% (from 82.37% to 87.19%) using random pruning, and by 20% (from 62.47%
to 82.47%) using EL2N. To explain these improvements, we provide theoretical
motivation for integrating SD when training on pruned data. Specifically, we
show that using a teacher trained on the entire data reduces the bias of the
student’s estimation error.
In addition, we present several empirical key observations. First, our results
demonstratethatsimplerandompruningoutperformsothersophisticatedprun-
ingalgorithmsinhighpruningregimes(lowf),bothwithandwithoutknowledge
distillation. Notably, prior research demonstrated this phenomenon in the ab-
sence of KD [35,48]. Second, we demonstrate a useful connection between the
pruningfactorf andtheoptimalweightoftheKDloss.Generally,utilizingdata
pruning algorithms to select high-scoring samples amplifies sensitivity to sam-
pleswithnoisylabelsorlowquality.Thisisbecausekeepingthehardestsamples
increasestheportionofthesesamplesasweretainasmallerdatafraction.Based
on this observation, we propose to adapt the weight of the KD loss according
to the pruning factor. That is, for low pruning factors, we should increase the
contribution of the KD term as the teacher’s soft predictions reflect possible
labelambiguityembeddedintheclassconfidences.Ontheotherhand,whenthe
pruning factor is high, we can decrease the contribution of the KD term to rely
more on the ground-truth labels.4 E. Ben-Baruch et al.
Finally, we observe a striking phenomenon when training with KD using
larger teachers: in high pruning regimes (low f), the optimization becomes sig-
nificantlymoresensitivetothecapacitygapbetweentheteacherandthestudent
model. This relates to the well known capacity gap problem [26]. Interestingly,
wefindthatforsmallpruningfractions,thestudentbenefitsmorefromteachers
with equal or even smaller capacities than its own, see Fig. 1c.
The contributions of the paper can be summarized as follows:
– Weexploretheutilizationofknowledgedistillationwhiletrainingonpruned
data by leveraging the soft predictions of a teacher pre-trained on the com-
plete data. Using KD, we demonstrate consistent improvement across all
pruning regimes, various pruning algorithms, and on multiple datasets.
– We theoretically show, for the case of linear regression, that using a teacher
trainedonthe entiredatareduces thebiasofthestudent’sestimationerror.
– We find that KD is robust to the choice of the pruning mechanism for high
pruningfractions.Inparticular,usingKD,randompruningachievescompa-
rable or superior accuracy compared to other sophisticated methods across
all pruning regimes.
– We demonstrate that by appropriately choosing the KD weight, one can
mitigate the impact of label noise and low-quality samples that are retained
by common pruning algorithms.
– Wemakethestrikingobservationthat,forsmallpruningfractions,increasing
the teacher size degrades accuracy, while, intriguingly, using teachers with
smaller capacities than the student’s improves results.
2 Related work
Data pruning. Data pruning, also known as coreset selection [18,27,37], refers
to methods aiming to reduce the dataset size for training neural networks. Re-
cent approaches have shown significant progress in retaining less data while
maintaining high classification accuracy [5,10,25,32,35,38]. In [35], the authors
showed theoretically and empirically that data pruning can improve the power
lawscalingofthedatasetsizebychoosinganoptimalpruningfractionasafunc-
tion of the initial dataset size. Additionally, studies in [2,35] have demonstrated
thatexistingpruningalgorithmsoftenunderperformwhencomparedtorandom
pruning methods, especially in high pruning regimes. Recently, in [48], the au-
thors suggested a theoretical explanation to this accuracy drop, and proposed a
coverage-centric pruning approach which better handles the data coverage.
Datapruningprovesvaluableatreducingmemoryandcomputationalcostin
various applications, including tasks such as hyper-parameter search [6], neural
architecture search [8], continual and incremental learning [22], as well as active
learning [5,27].
Other related fields are dataset distillation and data-free knowledge distil-
lation (DFKD). Dataset distillation approaches [40,45,47] aim to compress a
given dataset by synthesizing a small number of samples from the original data.Distilling the Knowledge in Data Pruning 5
The goal of DFKD is to employ model compression in scenarios where the orig-
inal dataset is inaccessible, for example, due to privacy concerns. Common ap-
proachesforDFKDinvolvegeneratingsyntheticsamplessuitableforKD[24,44]
or inverting the teacher’s information to reconstruct synthetic inputs [28,42].
Recently, the works in [7,43], utilized pseudo labels in training with dataset
distillation. Unlike dataset distillation and DFKD, which include synthetic data
generation, our work focuses on enhancing models trained on pruned datasets
created through sample selection, using KD. Moreover, in this paper we present
acollectionofinnovativeandpracticalfindingsfortheapplicationofKDindata
pruning.
Knowledge distillation. Knowledge distillation is a popular method aim-
ing at distilling the knowledge from one network to another. It is often used to
improvetheaccuracyofasmallmodelusingtheguidanceofalargeteachernet-
work [3,15]. In recent years, numerous variants and extensions of KD have been
developed. For example, [33,46] utilized feature activations from intermediate
layers to transfer knowledge across different representation levels. Other meth-
ods have proposed variants of KD criteria [1,17,19,41], as well as designing ob-
jectivesforrepresentationdistillation,asdemonstratedin[4,36].Self-distillation
(SD) refers to the case where the teacher and student have identical architec-
tures. It has been demonstrated that accuracy improvement can be achieved
using SD [11]. Recently, theoretical findings were introduced for self-distillation
in the presence of label noise [9].
In our paper, we explore the process of distilling knowledge from a model
trained on a large dataset to a model trained on a pruned subset of the orig-
inal data. Specifically, we focus on self-distillation and present several striking
observations that emerge when integrating SD for data pruning.
3 Method
GivenadatasetD withN labeledsamples{x ,y }N ,adatapruningalgorithm
i i i=1
A aims at selecting a subset P ⊂ D of the most representative samples for
training. We denote by f the pruning factor, which represents the fraction of
data to retain, calculated as f = N /N where N is the size of the pruned
f f
dataset. Note that 0 < f < 1. Score-based algorithms assign a score to each
sample, representing its importance in the learning process. Let s be the score
i
correspondingtoasamplex ,sortingtheminadescendingorders >s ,...,>
i k1 k2
s , following the sorting indices {k ,...,k }, we obtain the pruned dataset
kN 1 N
by retaining the highest scoring samples, P = {x ,...,x }. Usually, score-
k1 kNf
based algorithms retain hard samples while excluding the easy ones. Note that
in random pruning, we simply sample the indices k ,...,k uniformly. In this
1 N
paper, given a pruning algorithm A, our objective is to train a model on the
pruned dataset P while maximizing accuracy.6 E. Ben-Baruch et al.
3.1 Training on the pruned dataset using KD
Typically, score-based pruning methods involve training multiple models on the
full dataset D to compute the scores [10,25,32,38]. These models are discarded
and are not utilized further after the scores are computed. We argue that a
model trained on the full dataset encapsulates valuable information about the
entire distribution of the data and its classification boundaries, which can be
leveraged when training on the pruned data P. In this work, we investigate a
training scheme which incorporates the soft predictions of a teacher network,
pre-trained on the full dataset, throughout training on the pruned data.
Let f (x) be the teacher backbone pre-trained on D. The teacher outputs
t
logits {z }C , where C is the number of classes. The teacher’s soft predictions
i i=1
are computed by,
exp(z /τ)
q = i , i=1...C, (1)
i (cid:80) exp(z /τ)
j j
whereτ isthetemperaturehyper-parameter.Similarly,wedenotethestudent
model trained on the dataset P as f (x;θ), where θ represents the student’s
s
parameters. The student’s i-th soft prediction is denoted by p (θ). We optimize
i
the student model using the following loss function,
L(θ)=(1−α)L (θ)+αL (θ), (2)
cls KD
where the classification loss L (θ) measures the cross-entropy between the
cls
(cid:80)
ground-truthlabelsandthestudent’spredictions,representedas:− y logp (θ).
i i i
For the KD term L (θ), a common choice is the Kullback-Leibler (KL) diver-
KD
gence between the soft predictions of the teacher and the student. The hyper-
parameter α controls the weight of the KD term relative to the classification
loss.
Integrating the KD loss into the training process allows us to leverage the
valuable knowledge embedded in the teacher’s soft predictions q . These predic-
i
tions may encapsulate potential relationships between categories and class hier-
archies,accumulatedbytheteacherduringitstrainingontheentiredataset.To
illustrate this, we provide a qualitative example in Fig. 2 that presents the soft
predictionsgeneratedforaspecificsamplefromtheCIFAR-100dataset.CIFAR-
100 comprises 100 classes, organized into 20 super-classes, each containing 5
sub-classes. For example, the super-class "People" contains the classes: "Baby",
"Boy", "Girl", "Man", and "Woman". As shown in Fig. 2 (top), the teacher
accuratelypredictstheground-truthclass"Girl"(classindex35)withhighcon-
fidencewhilealsoassigninghighconfidencevaluestotheclasses"Woman"(98),
"Man"(46),and"Boy"(11).This‘darkknowledge’isvaluablefortrainingasit
offers a broader view of class hierarchies and data distribution. Fig. 2 (middle)
illustrates that a model trained on only 25% of the data fails to capture such
class relationships. Intuitively, reliable data and class distributions can be effec-
tively learned from large datasets, but are harder to infer from small datasets.
Conversely, in Fig. 2 (bottom) we show that using knowledge distillation, the
student successfully learns these delicate data relationships from the teacher
despite training only on the pruned data.Distilling the Knowledge in Data Pruning 7
Fig.2: Learning from the
teacher predictions. An exam-
pleofsoftpredictionscomputedby
ateachermodeltrainedontheen-
tiredata(top),amodeltrainedon
25% of the data (middle), and a
student model trained on 25% of
thedatawithKD(bottom),foran
evaluation sample of class "Girl"
from CIFAR-100. Using KD, the
student can better learn close or
ambiguous categories by leverag-
ing knowledge captured by the
teacher from the full dataset.
In Sec. 4.1, we empirically demonstrate that integrating knowledge distilla-
tionintotheoptimizationprocessofthestudentmodel,trainedonpruneddata,
leads to significant improvements across all pruning factors and various pruning
methods. In addition, we show that simple random pruning outperforms other
sophisticated pruning methods for low pruning fractions (low f), both with and
without knowledge distillation. We note that prior work has demonstrated this
phenomenonintheabsenceofKD[35].Interestingly,wealsoobservethattrain-
ingwithKDisrobusttothechoiceofthedatapruningmethod,includingsimple
random pruning, for sufficiently high pruning fractions.
Theseobservationsontheeffectivenessofrandompruninginthepresenceof
KD are compelling, especially in scenarios where data pruning occurs uninten-
tionallyasaby-productofthesystem,suchascaseswherethefulldatasetisno
longer accessible due to privacy concerns. However, using knowledge distillation
wecantrainastudentmodelontheremainingavailabledatawhilemaintaining
a high level of accuracy.
3.2 Mitigating noisy samples in pruned datasets
In general, hard samples are essential for the optimization process as they are
located close to the classification boundaries. However, retaining the hardest
samples while excluding moderate and easy ones increases the proportion of
samples with noisy and ambiguous labels, or images with poor quality. For ex-
ample,inFig.3,wepresentthehighestscoringimagesselectedbythe‘forgetting’
pruning algorithm for CIFAR-100 and SVHN. As can be seen, in the majority
of the images determining the class is non-trivial due to the complexity of the
category (e.g., fine-grained classes) or due to poor quality. By using knowledge
distillationthestudentcanlearnsuchlabelambiguityandmitigatenoisylabels.
Inarecentwork[9]itwasdemonstratedthatthebenefitofusingateacher’s
predictions increases with the degree of label noise. Consequently, it was found
that more weight should be assigned to the KD term as the noise variance
increases.Similarly,inourworkweempiricallydemonstratethatasthepruning8 E. Ben-Baruch et al.
Fig.3: Highest scoring samples.
Top10highestscoringsamplesselected
(a) CIFAR-100highestscorepruningsamples by the ‘forgetting’ method for CIFAR-
100 and SVHN datasets. The labels of
themajorityoftheimagesareambigu-
ousorhardtoresolveduetoclasscom-
(b) SVHNhighestscorepruningsamples plexity or low image quality.
factor f becomes lower, we should rely more on the teacher’s predictions by
increasing α in Eq. 2. Conversely, as the pruning factor is increased, we may
rely more on the ground-truth labels by decreasing α. We find that setting α
properly is crucial when applying pruning methods that retain hard samples.
Formally, the objective should be aware of the pruning fraction f as follows,
(cid:0) (cid:1)
L(θ,f)= 1−α(f) L (θ)+α(f)L (θ). (3)
cls KD
For example, as can be seen from Fig. 6, when the pruning fraction is low (f =
0.1), training with α = 1 is superior, achieving more than 8% higher accuracy
comparedtoα=0.5.Conversely,forhighpruningfractions(e.g.f =0.7),using
α = 0.5 outperforms α = 1 by more than 1% accuracy. We further explore the
relationship between α and f in Sec. 4.2.
3.3 Theoretical motivation
Inthissectionweprovideatheoreticalmotivationforthesuccessofself-distillation
inenhancingtrainingonpruneddata.Webaseouranalysisontherecentresults
reported in [9] for the case of regularized linear regression. Note that while we
use logistic regression in practice, we anchor our theoretical results in linear re-
gressionforthesakeofsimplicity.Also,itoftenallowsforareliableemulationof
outcomes observed in processes applied to logistic regression (see e.g. in [9]). In
particular,weshowthatemployingself-distillationusingateachermodeltrained
on a larger dataset reduces the error bias of the student estimation.
We are given a data matrix, X=[x ,...,x ]∈Rd×N, and a corresponding
1 N
label vector y = [y ,...,y ] ∈ RN, where N and d are the number of samples
1 N
and their dimension, respectively. Let θθθ∗ ∈ Rd be the ground-truth model pa-
rameters. The labels are assumed to be random variables, linearly modeled by
y=XTθθθ∗+ηηη,whereηηη ∈RN isassumedtobeGaussiannoise,uncorrelatedand
independentontheobservations.Indatapruning,weselectN columnsfromX
f
and theircorresponding labels: X
f
∈Rd×Nf, y
f
∈RNf. Thus, y
f
=XT fθθθ∗+ηηη f.
We also assume that d ≤ N ≤ N which is true in most practical scenarios.
f
Solving linear regularized regression using pruned dataset with fraction f, the
parameters are obtained by:
(cid:26) (cid:27)
λ
θθˆθ(f)=argmin ||y −XTθθθ||2+ ||θθθ||2
θθθ f f 2 2 2
=(X XT +λI )−1X y ,
f f d f fDistilling the Knowledge in Data Pruning 9
whereλ>0istheregularizationhyper-parameter,andI ∈Rd×d istheidentity
d
matrix. Note that a teacher trained on the full data is given by: θθˆθ = θθˆθ(1) =
t
(XXT +λI )−1Xy.
d
Here, we look at the more general case where the student is trained on a
prunedsubset withfactorf,andthe teachermodelis trainedonalargersubset
of the data, f >f. Following [9], the model learned by the student is given by,
t
θθˆθ (α,f,f )=(1−α)(X XT +λI )−1X y
s t f f d f f
+α(X XT +λI )−1X yˆ(t) (4)
f f d f f
=(X XT +λI )−1X (cid:0) (1−α)y +αXTθθˆθ(f )(cid:1) ,
f f d f f f t
whereyˆ(t) =XTθθˆθ(f ),i.e.,theteacher’spredictionsofthestudent’ssamplesX .
f f t f
Note that in a regular self-distillation (without pruning), we have f = f = 1,
t
and α > 0. Also, in a regular training on pruned data (without KD), f < 1,
and α = 0. In our scenario we utilize self-distillation for data pruning, i.e.,
f <f ≤1, and α>0.
t
We denote the student estimation error as ϵϵϵ (α,f,f ) = θθˆθ (α,f,f ) −θθθ∗.
s t s t
In [9], the authors show that employing self-distillation (α > 0) reduces the
variance of the student estimation, but on the other hand, increases its bias. In
the following, we show that distilling the knowledge from a teacher trained on a
larger data subset w.r.t the student, decreases the error estimation bias.
Theorem1. LetX∈Rd×N andy∈RN bethefullobservationmatrixandlabel
vector, respectively. Let y =XTθθθ∗+ηηη , whereθθθ∗ is the ground-truth projection
f f f
vector and ηηη ∈ RN is a Gaussian uncorrelated noise independent on X. Let
f
ϵϵϵ (α,f,f )=θθˆθ (α,f,f )−θθθ∗ be the student estimation error. Also, assume that
s t s t
d≤N ≤N, and f ≤f . Then, for any α,
f t
||E [ϵϵϵ (α,f,f )]||2 ≤||E [ϵϵϵ (α,f,f))]||2.
η s t η s
We include the proof for Theorem 1 in the supplementary. As data pruning
is susceptible to label noise due to retaining the hardest samples, this finding
demonstrates the utility of the proposed method. It suggests that employing
self-distillation with a teacher trained on the entire dataset (f =1) enables the
t
reduction of estimation bias in a student trained on a pruned subset.
4 Experimental results
In this section we provide empirical evidence for our method through extensive
experimentationoveravarietyofdatasets,anassortmentofdatapruningmeth-
ods and across a wide range of pruning levels. Then, we also investigate how
the KD weight, the teacher size and the KD method affect student performance
under different pruning regimes.
Datasets. We perform experiments on four classification datasets: CIFAR-
10 [20] with 10 classes, consists of 50,000 training samples and 10,000 testing10 E. Ben-Baruch et al.
(a) CIFAR-100 (b) SVHN (c) CIFAR-10
Fig.4: Data pruning results with knowledge distillation. Accuracy results
acrossdifferentpruningfactorsf,andvariouspruningapproaches(’forgetting’,EL2N,
GraNd and random pruning) on the CIFAR-100, SVHN, and CIFAR-10 datasets. We
use an equalized weight in the loss (i.e., α=0.5). Using KD, significant improvement
is achieved across all pruning regimes and all pruning methods. Random pruning out-
performs other pruning methods for low pruning factors. For sufficiently high f, the
accuracy is robust to the choice of the pruning approach in the presence of KD.
(a) ImageNet,Top-1accuracy (b) ImageNet,Top-5accuracy
Fig.5: Data pruning results with KD on ImageNet. Accuracy results across
differentpruningfactorsf,andvariouspruningmethodsontheImageNetdataset.We
use an equalized weight (α=0.5) in Eq. 2.
samples; SVHN [29] with 10 classes, consists of 73,257 training samples and
26,032testingsamples;CIFAR-100[21]with100classes,consistsof50,000train-
ing samples and 10,000 testing samples; and ImageNet [34] with 1,000 classes,
consists of 1.2M training samples and 50K testing samples.
Pruning Methods.Weutilizeseveralscore-baseddata-pruningalgorithms:
‘forgetting’[38],GradientNorm(GraNd),ErrorL2-Norm(EL2N)[32]and‘mem-
orization’1 [10]. Wealso utilize a class-balanced random pruning scheme, which,
given a pruning budget, randomly and equally draws samples from each class.
ImplementationDetails.Forcomputationalefficiencyweconductourself-
distillation experiments on all datasets using the ResNet-32 [13] architecture,
exceptforImageNetforwhichweutilizethelargerResNet-50.Ourtrainingand
distillation recipes are simple. We utilize SGD with Momentum to optimize the
1 Wenotethatwhiletheauthorsofmemorization didnotoriginallyutilizethemethod
for data pruning, its efficacy on ImageNet was later demonstrated by [35].Distilling the Knowledge in Data Pruning 11
Fig.6: Optimal KD weight versus pruning factor. Accuracy is presented for
CIFAR-100 while varying the KD weight α for different pruning factors. We utilize
‘forgetting’ as the pruning method. For low pruning fractions (low f), accuracy gen-
erally increases when increasing the KD weight to rely more on the teacher’s soft
predictions. As we use higher pruning fractions (high f), it is usually better to lower
α in order to increase the contribution of the ground-truth labels.
models and incorporate basic data-augmentations during training. Additional
implementation details can be found in the supplementary.
4.1 Training on pruned data with KD
To demonstrate the advantage of incorporating KD-based supervision when
training on pruned data, we utilize the aforementioned data pruning methods
oneachdatasetusingawiderangeofpruningfactors.Then,wetrainmodelson
the produced data subsets with and without KD. We note that in the presence
of KD the respective teachers that are utilized are trained on the full datasets.
AscanbeobservedinFigs.4and5,theincorporationofKDintothetraining
process consistently enhances model accuracy across all of the tested scenarios,
regardless of the tested dataset, pruning method or pruning level. For example,
compared to baseline models trained on the full datasets without KD, utilizing
KD can lead to comparable accuracy levels by retaining only small portions of
the original datasets (e.g., 10%, 30%, 50% on SVHN, CIFAR-10, and CIFAR-
100, respectively, using ‘forgetting’). In fact, even on a large scale dataset as
ImageNet, comparable accuracy can be achieved by randomly retaining just
30% of the data, while training on larger subsets remarkably results in superior
accuracytothebaseline(e.g.,+1.6%usingarandomsubsetof70%).Incontrast,
we observe that in the absence of KD, preserving accuracy is only possible in
the presence of more data (i.e., more relaxed pruning levels). In particular, on
CIFAR-100wefoundaccuracypreservationtobeunattainablewithoutKDeven
in the least aggressive pruning scenario we have tested (f =0.9), and regardless
of the pruning method.
Moreover, we note that the accuracy gains due to KD are most significant
in high-compression scenarios. For instance, on CIFAR-100 with f = 0.1, KD
contributes to absolute accuracy improvements of 17%, 22.4%, 21%, and 19.7%
across the random, ‘forgetting’, GraNd, and EL2N pruning methods, respec-
tively. Similarly, on SVHN, which permits even stronger compression, improve-12 E. Ben-Baruch et al.
mentsofthesameorderofmagnitudecanbeobservedatalowerpruningfactor
(f =0.01).
Thesefindingssupporttheideathatthesoft-predictionsproducedbyawell-
informed teacher contain rich and valuable information that can greatly benefit
a student in a limited-data setting. This ‘dark knowledge’, notably absent in
conventionalone-hotlabels,allowsthestudenttodeducestrongergeneralizations
from each available data sample, which in turn translates to better performance
given the same training data.
Finally, two additional interesting patterns emerge from our experiments.
First, in high-compression scenarios (e.g., f ≤ 0.4 in CIFAR-100, f ≤ 0.08 in
SVHN), it is evident that random pruning surpasses all other methods in effec-
tiveness,bothwithandwithoutKD.Thisalignswiththenotionthataggressive
pruning via score-based techniques retains larger concentrations of low quality
ornoisysamplesduetomistakingthemforchallengingcases.Thisphenomenon
was previously noted without KD in [35]. Second, under low-compression con-
ditions (e.g., f ≥ 0.5 in CIFAR-100, f ≥ 0.2 in SVHN), we observe that KD
renders the student model robust to the pruning technique used. This finding is
significantasitsuggeststhatitmaybepossibletoforgostate-of-the-artpruning
techniques in favor of basic random pruning in the presence of KD.
4.2 Adapting the KD weight vs. the pruning factor
We wish to investigate how varying the KD weight α affects the performance of
the student under different pruning levels of a given dataset. To explore this we
conductexperimentsonCIFAR-100with’forgetting’asthepruningmethodand
present the results in Fig. 6. As can be observed, lower pruning fractions favor
higher values of α, while higher pruning fractions advocate for lower ones. As
explained earlier, aggressive pruning via score-based methods tends to result in
subsets with greater proportions of label noise and low quality samples. Hence,
for lower pruning factors, increasing the KD weight seems to help the student
mitigatetheextranoisebyrelyingmoreontheteacher’spredictions.Conversely,
asthepruningfactorincreasesandtheproportionsofnoiseintheprunedsubset
gradually diminish, it appears to be beneficial for the student to balance the
contributions of KD and the ground-truth labels. Similar results on SVHN can
be found in the supplementary.
4.3 Using teachers of different capacities
Untilnow,wehavefocusedonthecasewhereboththestudentandteachershare
the same architecture (i.e., self-distillation). In this section, we explore how the
capacityoftheteacheraffectsthestudent’sperformanceacrossdifferentpruning
regimes. In Fig. 7a, we present accuracy results across various pruning factors
for the case of randomly pruning CIFAR-100 and training with a ResNet-32
student. We employ 6 teacher architectures of increasing capacities: (1) ResNet-
14 with 69.9% accuracy, (2) ResNet-20 with 70.23% accuracy, (3) ResNet-32
with 71.6% accuracy, (4) ResNet-56 with 72.7% accuracy, (5) ResNet-110 withDistilling the Knowledge in Data Pruning 13
(a) CIFAR-100.Studentarchitecture:ResNet-32(RN32).
(b) CIFAR-100.Studentarchitecture:ResNet-20(RN20).
(c) CIFAR-10.Studentarchitecture:ResNet-32(RN32).
Fig.7: Exploring the effect of the teacher’s capacity. Accuracy results for a
studentwith(a)ResNet-32and(b)ResNet-20architectureswhileusingteachermodels
with increasing capacities along the horizontal axes. In each instance, we denote the
teacher whose architecture matches that of the student by ‘SD’ (self-distillation). We
use random pruning with different fractions. Surprisingly, under low pruning factors,
increasing the teacher’s capacity results in lower student accuracy.
74.4% accuracy, and (6) WRN-40-2 with 75.9% accuracy. Also, note that for
each teacher architecture we experiment with five different temperature values
in the range 2 − 7. We show the impact of the temperature selection in the
supplementary. Similarly, in Fig. 7b we present results for the same experiment
using a ResNet-20 student, while Fig. 7c depicts results of a similar experiment
on CIFAR-10 for the ResNet-32 student.
As observed, at low pruning factors, increasing the teacher’s capacity harms
the accuracy of the student. Conversely, at high pruning factors (e.g. 0.9 and 1)
larger teachers generally lead to improved results, as expected in common KD
formodelcompression.Thistrendisconsistentlyobservedacrossvariousstudent
architecturesanddatasets,andisrobusttotheselectionoftheKDtemperature.
Additional results are provided in the supplementary.
Thisobservationhighlightsastrikingphenomenon:thecapacitygapproblem,
whichdenotesthedisparityinarchitecturesizebetweentheteacherandstudent,
becomesmorepronouncedwhenapplyingknowledgedistillationduringtraining
onpruneddata.Surprisingly,forsmallpruningfractions,teacherswithasmaller
capacitythanthestudentcanleadtohigherstudentaccuracylevelscomparedto
largerteachers.Theresultsalsosuggestthattheeffectivenessofsmallerteachers
in improving the student is bounded to an extent by their own accuracy levels.14 E. Ben-Baruch et al.
Method 5% 10% 30% 50%
w/o KD 14.46 22.21 49.41 67.47
KD [15] 28.62 46.27 66.82 70.95
FitNets [33] 25.66 44.84 65.7 70.77
AB [14] 30.5 47.68 66.15 71.22
AT [46] 28.26 42.59 65.75 70.45
FT [19] 28.34 44.01 64.95 70.75
FSP [41] 27.62 37.16 62.79 69.72
NST [17] 26.2 44.5 64.93 70.97
PKT [31] 27.3 44.09 65.22 70.7
RKD [30] 21.69 43.03 65.43 70.36
SP [39] 29.09 42.53 65.62 70.72
VID [1] 32.5 49.46 67.38 71.16
Table 1: Comparison of different KD approaches on several pruning levels
of CIFAR-100.WeaddvariousKDlosstermstoEq.2,inadditiontothevanillaKD
term. ‘Forgetting’ is utilized as the pruning method. As observed, integrating VID [1]
further improves training on the pruned dataset.
4.4 Comparing different KD approaches
So far, we have utilized solely vanilla KD during training. Next we explore inte-
gratingadditionalKDapproachestotheloss.Inparticular,weaddanadditional
KD loss term L as follows: L(θ) = L (θ)+αL (θ)+βL (θ), where β is a
R cls KD R
hyper-parameter. In this experiments, we simply set α and β to 1. In Tab. 1 we
comparetheperformanceofdifferentKDmethodsonCIFAR-100underlowand
average compression regimes. For a fair comparison, for the case of employing
only the vanilla KD, we set α = 2, and β = 0. As can be observed, integrating
the Variational Information Distillation (VID) loss [1] improves results consid-
erably for the tested cases. These results suggest that further improvement can
be achieved by incorporating additional approaches to extract knowledge from
the teacher.
5 Conclusion
Inthispaper,weinvestigatedtheapplicationofknowledgedistillationfortrain-
ing models on pruned data. We demonstrated the significant benefits of incor-
porating the teacher’s soft predictions into the training of the student across
all pruning fractions, various pruning algorithms and multiple datasets. We em-
pirically found that incorporating KD while using simple random pruning can
achieve comparable or superior accuracy compared to sophisticated pruning ap-
proaches. We also demonstrated a useful connection between the pruning factor
andtheKDweight,andproposetoadaptαaccordingly.Finally,forsmallprun-
ingfractions,wemadethesurprisingobservationthatthestudentbenefitsmore
from teachers with equal or even smaller capacities than that of its own, over
teachers with larger capacities.
We believe that these empirical observations can pave the way for in-depth
research, both theoretical and experimental, to enhance our understanding of
the optimization process when training models on pruned data using KD.Distilling the Knowledge in Data Pruning 15
References
1. Ahn, S., Hu, S.X., Damianou, A.C., Lawrence, N.D., Dai, Z.: Variational infor-
mation distillation for knowledge transfer. 2019 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) pp. 9155–9163 (2019), https:
//api.semanticscholar.org/CorpusID:118649278 5, 14
2. Ayed, F., Hayou, S.: Data pruning and neural scaling laws: fundamental limita-
tions of score-based algorithms. ArXiv abs/2302.06960 (2023), https://api.
semanticscholar.org/CorpusID:256846521 4
3. Bucila, C., Caruana, R., Niculescu-Mizil, A.: Model compression. In: Knowl-
edge Discovery and Data Mining (2006), https://api.semanticscholar.org/
CorpusID:11253972 5
4. Chen, L., Gan, Z., Wang, D., Liu, J., Henao, R., Carin, L.: Wasserstein con-
trastive representation distillation. 2021 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) pp. 16291–16300 (2020), https://api.
semanticscholar.org/CorpusID:229220499 5
5. Chitta,K.,Álvarez,J.M.,Haussmann,E.,Farabet,C.:Trainingdatasubsetsearch
with ensemble active learning. IEEE Transactions on Intelligent Transportation
Systems23,14741–14752(2019),https://api.semanticscholar.org/CorpusID:
226282535 4
6. Coleman,C.A.,Yeh,C.,Mussmann,S.,Mirzasoleiman,B.,Bailis,P.D.,Liang,P.,
Leskovec, J., Zaharia, M.A.: Selection via proxy: Efficient data selection for deep
learning. ArXiv abs/1906.11829 (2019), https://api.semanticscholar.org/
CorpusID:195750622 4
7. Cui,J.,Wang,R.,Si,S.,Hsieh,C.J.:Scalingupdatasetdistillationtoimagenet-1k
with constant memory. In: International Conference on Machine Learning (2022),
https://api.semanticscholar.org/CorpusID:253735319 5
8. Dai,X.,Chen,D.,Liu,M.,Chen,Y.,Yuan,L.:Da-nas:Dataadaptedpruningfor
efficient neural architecture search. In: European Conference on Computer Vision
(2020), https://api.semanticscholar.org/CorpusID:214693401 4
9. Das,R.,Sanghavi,S.:Understandingself-distillationinthepresenceoflabelnoise
(2023) 5, 7, 8, 9, 4, 6
10. Feldman, V., Zhang, C.: What neural networks memorize and why: Discovering
the long tail via influence estimation. In: Larochelle, H., Ranzato, M., Hadsell,
R.,Balcan,M.,Lin,H.(eds.)AdvancesinNeuralInformationProcessingSystems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual (2020), https://proceedings.neurips.cc/
paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html 2, 4, 6,
10, 1
11. Furlanello,T.,Lipton,Z.C.,Tschannen,M.,Itti,L.,Anandkumar,A.:Bornagain
neuralnetworks.In:InternationalConferenceonMachineLearning(2018),https:
//api.semanticscholar.org/CorpusID:4110009 5
12. Guo,C.,Zhao,B.,Bai,Y.:Deepcore:Acomprehensivelibraryforcoresetselection
in deep learning. In: International Conference on Database and Expert Systems
Applications(2022),https://api.semanticscholar.org/CorpusID:248239610 2,
1
13. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016) 10, 116 E. Ben-Baruch et al.
14. Heo, B., Lee, M., Yun, S., Choi, J.Y.: Knowledge transfer via distillation of acti-
vation boundaries formed by hidden neurons. In: AAAI Conference on Artificial
Intelligence (2018), https://api.semanticscholar.org/CorpusID:53213211 14
15. Hinton, G.E., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
ArXiv abs/1503.02531 (2015), https://api.semanticscholar.org/CorpusID:
7200347 5, 14
16. Horn, R.A., Johnson, C.R.: Matrix analysis. Cambridge university press (2012) 4
17. Huang, Z., Wang, N.: Like what you like: Knowledge distill via neuron selectivity
transfer (2017) 5, 14
18. Huggins, J., Campbell, T., Broderick, T.: Coresets for scalable bayesian logis-
tic regression. In: Neural Information Processing Systems (2016), https://api.
semanticscholar.org/CorpusID:27128 4
19. Kim, J., Park, S., Kwak, N.: Paraphrasing complex network: Network com-
pression via factor transfer. ArXiv abs/1802.04977 (2018), https://api.
semanticscholar.org/CorpusID:3608236 5, 14
20. Krizhevsky, A., Nair, V., Hinton, G.: Cifar-10 (canadian institute for advanced
research) http://www.cs.toronto.edu/~kriz/cifar.html 9, 1
21. Krizhevsky, A., Nair, V., Hinton, G.: Cifar-100 (canadian institute for advanced
research) http://www.cs.toronto.edu/~kriz/cifar.html 10, 1
22. Lange,M.D.,Aljundi,R.,Masana,M.,Parisot,S.,Jia,X.,Leonardis,A.,Slabaugh,
G.G.,Tuytelaars,T.:Acontinuallearningsurvey:Defyingforgettinginclassifica-
tion tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44,
3366–3385 (2019), https://api.semanticscholar.org/CorpusID:218889912 4
23. Loshchilov,I.,Hutter,F.:SGDR:Stochasticgradientdescentwithwarmrestarts.
In: International Conference on Learning Representations (2017), https://
openreview.net/forum?id=Skq89Scxx 2
24. Luo, L., Sandler, M., Lin, Z., Zhmoginov, A., Howard, A.G.: Large-scale gen-
erative data-free distillation. ArXiv abs/2012.05578 (2020), https://api.
semanticscholar.org/CorpusID:228083866 5
25. Meding, K., Buschoff, L.M.S., Geirhos, R., Wichmann, F.: Trivial or impossible
- dichotomous data difficulty masks model differences (on imagenet and beyond).
ArXiv abs/2110.05922 (2021), https://api.semanticscholar.org/CorpusID:
238634169 2, 4, 6
26. Mirzadeh, S.I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., Ghasemzadeh,
H.: Improved knowledge distillation via teacher assistant. In: AAAI Conference
on Artificial Intelligence (2019), https://api.semanticscholar.org/CorpusID:
212908749 4
27. Mirzasoleiman, B., Bilmes, J.A., Leskovec, J.: Coresets for data-efficient training
of machine learning models. In: International Conference on Machine Learning
(2019), https://api.semanticscholar.org/CorpusID:211259075 4
28. Nayak, G.K., Mopuri, K.R., Shaj, V., Babu, R.V., Chakraborty, A.: Zero-shot
knowledge distillation in deep networks. ArXiv abs/1905.08114 (2019), https:
//api.semanticscholar.org/CorpusID:159041346 5
29. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading
digits in natural images with unsupervised feature learning. In: NIPS Work-
shop on Deep Learning and Unsupervised Feature Learning 2011 (2011), http:
//ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf 10, 1
30. Park, W., Kim, D., Lu, Y., Cho, M.: Relational knowledge distillation. 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
pp. 3962–3971 (2019), https://api.semanticscholar.org/CorpusID:131765296
14Distilling the Knowledge in Data Pruning 17
31. Passalis, N., Tefas, A.: Learning deep representations with probabilistic knowl-
edge transfer. In: European Conference on Computer Vision (2018), https:
//api.semanticscholar.org/CorpusID:52012952 14
32. Paul, M., Ganguli, S., Dziugaite, G.K.: Deep learning on a data diet: Finding
important examples early in training. CoRR abs/2107.07075 (2021), https://
arxiv.org/abs/2107.07075 2, 4, 6, 10, 1
33. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fit-
nets: Hints for thin deep nets. CoRR abs/1412.6550 (2014), https://api.
semanticscholar.org/CorpusID:2723173 5, 14
34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV) 115(3), 211–252 (2015). https://doi.org/10.1007/s11263-015-0816-y
10, 1
35. Sorscher, B., Geirhos, R., Shekhar, S., Ganguli, S., Morcos, A.S.: Beyond neural
scalinglaws:beatingpowerlawscalingviadatapruning.In:Oh,A.H.,Agarwal,A.,
Belgrave, D., Cho, K. (eds.) Advances in Neural Information Processing Systems
(2022), https://openreview.net/forum?id=UmvSlP-PyV 3, 4, 7, 10, 12, 1
36. Tian,Y.,Krishnan,D.,Isola,P.:Contrastiverepresentationdistillation.In:Inter-
national Conference on Learning Representations (2019) 5, 1
37. Tolochinsky,E.,Feldman,D.:Coresetsformonotonicfunctionswithapplicationsto
deep learning. ArXiv abs/1802.07382 (2018), https://api.semanticscholar.
org/CorpusID:125549990 4
38. Toneva,M.,Sordoni,A.,desCombes,R.T.,Trischler,A.,Bengio,Y.,Gordon,G.J.:
An empirical study of example forgetting during deep neural network learning.
ArXiv abs/1812.05159 (2018), https://api.semanticscholar.org/CorpusID:
55481903 2, 4, 6, 10, 1
39. Tung, F., Mori, G.: Similarity-preserving knowledge distillation. 2019 IEEE/CVF
International Conference on Computer Vision (ICCV) pp. 1365–1374 (2019),
https://api.semanticscholar.org/CorpusID:198179476 14
40. Wang, T., Zhu, J.Y., Torralba, A., Efros, A.A.: Dataset distillation. ArXiv
abs/1811.10959 (2018), https://api.semanticscholar.org/CorpusID:
53763883 4
41. Yim, J., Joo, D., Bae, J.H., Kim, J.: A gift from knowledge distillation: Fast op-
timization, network minimization and transfer learning. 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) pp. 7130–7138 (2017),
https://api.semanticscholar.org/CorpusID:206596723 5, 14
42. Yin, H., Molchanov, P., Li, Z., Álvarez, J.M., Mallya, A., Hoiem, D., Jha, N.K.,
Kautz,J.:Dreamingtodistill:Data-freeknowledgetransferviadeepinversion.2020
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)pp.
8712–8721 (2019), https://api.semanticscholar.org/CorpusID:209405263 5
43. Yin, Z., Xing, E., Shen, Z.: Squeeze, recover and relabel: Dataset condensation at
imagenet scale from a new perspective (2023) 5
44. Yoo, J., Cho, M., Kim, T., Kang, U.: Knowledge extraction with no observ-
able data. In: Neural Information Processing Systems (2019), https://api.
semanticscholar.org/CorpusID:202774028 5
45. Yu, R., Liu, S., Wang, X.: Dataset distillation: A comprehensive review. IEEE
transactions on pattern analysis and machine intelligence PP (2023), https://
api.semanticscholar.org/CorpusID:255942245 418 E. Ben-Baruch et al.
46. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving
the performance of convolutional neural networks via attention transfer. ArXiv
abs/1612.03928 (2016), https://api.semanticscholar.org/CorpusID:829159
5, 14
47. Zhao, B., Mopuri, K.R., Bilen, H.: Dataset condensation with gradient matching.
ArXiv abs/2006.05929 (2020), https://api.semanticscholar.org/CorpusID:
219558792 4
48. Zheng,H.,Liu,R.,Lai,F.,Prakash,A.:Coverage-centriccoresetselectionforhigh
pruning rates. ArXiv abs/2210.15809 (2022), https://api.semanticscholar.
org/CorpusID:253224188 3, 4Distilling the Knowledge in Data Pruning 1
Distilling the Knowledge in Data Pruning
Supplementary Material
Fig.8: Optimal KD weight versus pruning factor. Accuracy is presented on
SVHNwhilevaryingtheKDweightαacrossdifferentpruningfactors.Weutilize‘for-
getting’ as the pruning method. For low pruning fractions (low f), accuracy generally
increaseswhenincreasingtheKDweighttorelymoreontheteacher’ssoftpredictions.
However, as we use higher pruning fractions (high f), it is usually better to use lower
α values in order to increase the contribution of the ground-truth labels.
6 Additional Implementation Details
6.1 Obtaining the pruning scores
WeutilizethedefaultpruningrecipesofferedbytheDeepCoreframework[12]in
ordertocomputemostofthepruningscoresusedinourexperiments.ForSVHN
[29],CIFAR-10[20]andCIFAR-100[21]wecomputethescoresusingtheResNet-
34[13]architecture.ForImageNet[34]wecomputethescoresforthe‘forgetting’
pruning method [38] using ResNet-50, while for the ‘memorization’ [10] and
EL2N [32] methods we directly utilize the scores released by [35]. Specifically,
we note that for EL2N on ImageNet we adopt the released variant of the scores
which was averaged over 20 models.
6.2 Conducting the distillation experiments
WeconductourknowledgedistillationexperimentsontheprunedSVHN,CIFAR-
10 and CIFAR-100 datasets using a modified version of the RepDistiller frame-
work[36].Forthemostpartweadoptthedefaulttraininganddistillationrecipes
offered by the framework. The models are trained for 240 epochs with a batch
sizeof64.FortheoptimizationprocessweuseSGDwithlearningrate0.05,mo-
mentumvalueof0.9andweightdecayof5e−4.Thelearningrateisdecreasedby
a factor of 10 on the 150th, 180th and 210th epochs. To conduct the distillation
experiments on ImageNet we expand the DeepCore [12] framework to support
knowledge distillation on pruned datasets. Apart from this change we mostly
rely on the default training recipe offered by the framework. The models are2 E. Ben-Baruch et al.
(a) SVHNdataset.Studentarchitecture:ResNet-8(RN8).
(b) SVHNdataset.Studentarchitecture:ResNet-32(RN32).
(c) CIFAR-100dataset.Studentarchitecture:ResNet-56(RN56).
(d) CIFAR-10dataset.Studentarchitecture:ResNet-20(RN20).
Fig.9: Exploring the effect of the teacher’s capacity. Accuracy results across
different pruning fractions using teacher models with increasing capacities for: (a) a
ResNet-8studentonSVHN,(b)aResNet-32studentonSVHN,(c)aResNet-56student
on CIFAR-100, and for (d) a ResNet-20 student on CIFAR-10. Random pruning is
utilized. These results further corroborate our observation that teachers with smaller
capacities lead to higher student accuracy when utilizing low pruning fractions.
trained for 240 epochs with a batch size of 128. We utilize SGD with learning
rate 0.1, momentum value of 0.9 and weight decay of 5e−4. The learning rate
is gradually decayed during training using a cosine-annealing scheduler [23]. In
all of our distillation experiments we use τ =4 as the temperature for the KD’s
soft predictions computation in Eq. (1).
7 Adapting the KD weight vs. the pruning factor
Following Sec. 4.2, in Fig. 8 we present additional accuracy results which show
the effect of varying the KD weight α across different pruning factors f, this
time on the SVHN dataset. We utilize ‘forgetting’ as the pruning method. Here,Distilling the Knowledge in Data Pruning 3
Fig.10: Impact of the KD temperature on the student’s accuracy using
teachers with different capacities. We present accuracy results across different
pruningfractionsonCIFAR-100foraResNet-20student.Randompruningisutilized.
As can be seen, for lower pruning fractions (e.g. f = 0.1 and f = 0.3), teachers with
lower capacities outperform teachers with higher capacities.
a similar trend to the one previously observed on CIFAR-100 can be seen: for
low pruning fractions, accuracy improves as we increase the KD weight, while
for higher pruning fractions it is usually better to use lower α values.
8 Using teachers of different capacities
In Sec. 4.3 we have made the observation that teachers with smaller capacities
lead to higher student accuracy when utilizing low pruning fractions. Here we
provideadditionalresultswhichdemonstratetheconsistencyofthisobservation.
In Figs. 9a and 9b we present student accuracy results on SVHN using different
teachers and various pruning fractions, where the utilized student architectures
are ResNet-8 and ResNet-32, respectively. Similarly, Fig. 9c depicts results on
CIFAR-100withaResNet-56student,andFig.9dshowsthesameonCIFAR-10
with a ResNet-20 student. Random pruning is utilized in all experiments.
9 Impact of KD temperature
InSec.4.3wehavemadetheobservationthatforlowpruningfractions,employ-
ingKDusingsmallerteachersresultsinhigherstudentaccuracy.Todemonstrate
the consistency of this observation across different KD temperatures, in Fig. 10
we present the impact of the KD temperature on the student’s accuracy when
utilizing teachers with different capacities, and across various pruning fractions.
The experiment was conducted on CIFAR-100 with random pruning using a
ResNet-20 student. As can be observed, the benefit of smaller teachers in high
pruning regimes (lower f values) is evident over a wide range of temperature
values.4 E. Ben-Baruch et al.
10 Theoretical Motivation
Lemma 1. Given a data matrix X ∈ Rd×N and its sub-matrix X
f
∈ Rd×Nf,
while d≤N ≤N,
f
σ (X)≥σ (X ),k =1,...,d,
k k f
where σ (X) is the k’s largest singular value of X.
k
Proof. Let Z denote the remaining sub-matrix after excluding the X columns
f
from X, i.e., X=[X |Z]. Thus,
f
XXT =X XT +ZZT.
f f
All three matrices are positive semidefinite and therefore based on Weyl’s in-
equality [16](Theorem 4.3.1), λ (XXT) ≥ λ (X XT), where λ (A) is the k’s
k k f f k
largest eigenvalue of A. This also implies that σ (X)≥σ (X ) for k =1,...,d.
k k f
Theorem 2. Let X ∈ Rd×N and y ∈ RN denote the observations matrix and
ground-truth label vector, respectively. Let θθˆθ (α,f,f ) denote the student model
s t
obtained using Eq. 4 using pruning factor f < f and distilled from the teacher
t
model θθˆθ(f ) using KD weight α. Then, the following holds,
t
||E [ˆϵϵϵ (α,f,f )]||2 ≤||E [ˆϵϵϵ (α,f,f)]||2.
η s t η s
Proof. Similarly to [9] we base our proof on the Singular Value Decomposition
(SVD) of both the pruned and the full data matrices used to train the student
andtheteacher,respectively.Thus,X =U′ΣΣΣ′V′T andX =UΣΣΣVT.Wealso
ft f
assume that N > N ≥ d, which is a practical assumption in machine learning
f
and therefore the rank of both the full and the pruned data matrices is d. Thus
the estimator SVD has the following form in terms of the SVD of the full and
pruned data matrices,
θθˆθ (α,f,f )=(X XT +λI )−1X (cid:0) (1−α)y +αXTθθˆθ(f )(cid:1)
s t f f d f f f t
=U(cid:0) ΣΣΣ2+λI (cid:1)−1 ΣΣΣ(cid:16) (1−α)(ΣΣΣUTθθθ∗+VTηηη )+αΣΣΣUTθθˆθ(f )(cid:17)
d f t
=U(cid:0) ΣΣΣ2+λI (cid:1)−1 ΣΣΣ(cid:16) (1−α)(ΣΣΣUTθθθ∗+VTηηη )+
d f
+αΣΣΣUTU′(cid:0) ΣΣΣ′2+λI (cid:1)−1 ΣΣΣ′(cid:0) ΣΣΣ′U′Tθθθ∗+V′Tηηη (cid:1)(cid:17)
d ft
 
=(cid:88)d σ2σ +i2
λ(1−α)⟨θθθ∗,u
i⟩+α(cid:88)d σ′2σ j′ +2
λ⟨θθθ∗,u′ j⟩⟨u′ j,u i⟩u i+
i=1 i j=1 j
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u i.
i=1 i j=1 jDistilling the Knowledge in Data Pruning 5
 
=(cid:88)d σ2σ +i2 λ(1−α)⟨(cid:88)d
⟨θθθ∗,u′ j⟩u′ j,u
i⟩+α(cid:88)d σ′2σ j′ +2
λ⟨θθθ∗,u′ j⟩⟨u′ j,u i⟩u
i
i=1 i j=1 j=1 j
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u i.
i=1 i j=1 j
 
=(cid:88)d σ2σ +i2 λ(1−α)(cid:88)d
⟨θθθ∗,u′ j⟩⟨u′ j,u
i⟩+α(cid:88)d σ′2σ j′ +2
λ⟨θθθ∗,u′ j⟩⟨u′ j,u i⟩u
i
i=1 i j=1 j=1 j
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u i.
i=1 i j=1 j
(cid:88)d (cid:88)d σ2 (cid:32) λ (cid:33)
= i ⟨θθθ∗,u′⟩⟨u′,u ⟩ 1−α u +
σ2+λ j j i σ′2+λ i
i=1j=1 i j
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u i.
i=1 i j=1 j
The estimation error is therefore,
d
ˆϵϵϵ (α,f,f )=θθˆθ (α,f,f )−θθθ∗ =θθˆθ (α,f,f )−(cid:88) ⟨θθθ∗,u ⟩u
s t s t s t i i
i=1
d d
=θθˆθ (α,f,f )−(cid:88) ⟨(cid:88) ⟨θθθ∗,u′⟩u′,u ⟩u
s t j j i i
i=1 j=1
(cid:88)d (cid:88)d σ2 (cid:32) λ (cid:33) (cid:88)d (cid:88)d
= i ⟨θθθ∗,u′⟩⟨u′,u ⟩ 1−α u − ⟨θθθ∗,u′⟩⟨u′,u ⟩u
σ2+λ j j i σ′2+λ i j j i i
i=1j=1 i j i=1j=1
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u i.
i=1 i j=1 j
(cid:88)d (cid:88)d (cid:32) σ2 (cid:32) λ (cid:33) (cid:33)
= ⟨θθθ∗,u′⟩⟨u′,u ⟩ i 1−α −1 u
j j i σ2+λ σ′2+λ i
i=1j=1 i j
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u
i
i=1 i j=1 j
(cid:88)d (cid:88)d λ (cid:32) σ2 (cid:33)
=− ⟨θθθ∗,u′⟩⟨u′,u ⟩ 1+α i u
j j i σ2+λ σ′2+λ i
i=1j=1 i j
 
+(cid:88)d σ2σ
+i λ(1−α)⟨ηηη f,v i⟩+ασ
i(cid:88)d σ′2σ +j′
λ⟨ηηη ft,v j′⟩⟨u′ j,u i⟩u i.
i=1 i j=1 j6 E. Ben-Baruch et al.
The expectation of the bias term over the noise parameter η which is uncorre-
lated and independent of X is,
(cid:88)d (cid:88)d λ (cid:32) σ2 (cid:33)
E [ˆϵϵϵ (α,f,f )]=− ⟨θθθ∗,u′⟩⟨u′,u ⟩ 1+α i u .
η s t j j i σ2+λ σ′2+λ i
i=1j=1 i j
Therefore the bias error term of the estimation process is,
(cid:88)d (cid:18) λ (cid:19)2 (cid:88)d (cid:32) σ2 (cid:33)2
||E η[ˆϵϵϵ s(α,f,f t)]||2 =
σ2+λ
 ⟨θθθ∗,u′ j⟩⟨u′ j,u i⟩ 1+α σ′2+i
λ
 .
i=1 i j=1 j
Note that given that the student and the teacher are trained using the same
dataset X , i.e., σ = σ′ and u = u′ for i = 1,...,d, the bias error term
f i i i i
reduces to what is reported in [9] (Eq. 24):
(cid:88)d (cid:18) λ (cid:19)2 (cid:88)d (cid:32) σ2 (cid:33)2
||E η[ˆϵϵϵ s(α,f,f)]||2 =
σ2+λ
 ⟨θθθ∗,u j⟩⟨u j,u i⟩ 1+α σ2+i
λ

i=1 i j=1 j
(cid:88)d (cid:18) λ (cid:19)2(cid:32) σ2 (cid:33)2
= ⟨θθθ∗,u ⟩2 1+α i .
i σ2+λ σ2+λ
i=1 i j
Now, let us consider the impact of a minimal augmentation of the dataset
used to train the teacher w.r.t. that used to train the student. In other words,
we assume that a single data sample is added, i.e., f = f + 1, where N is
t N
the total number of available samples. Given that adding a single sample to a
significantly larger set of N samples is not sufficient to change its distribution,
f
we can assume that u′ ≈ u for i = 1,...,d. Thus, the derivative of the error
i i
bias term with respect to σ′ is,
k
∂||E η[ˆϵϵϵ s(α,f,f + N1)]||2 =2(cid:88)d (cid:18) λ (cid:19)2 (cid:88)d
⟨θθθ∗,u′⟩⟨u′,u ⟩·
∂σ′ σ2+λ j j i
k i=1 i j=1
(cid:32) σ2 (cid:33)(cid:18) 2σ′σ2 (cid:19)
· 1+α i −α k i ⟨θθθ∗,u′⟩⟨u′,u ⟩
σ′2+λ (σ′2+λ)2 k k i
j k
(cid:18) λ (cid:19)2(cid:32) σ2 (cid:33) σ′σ2
≈−4α 1+α k k k ⟨θθθ∗,u′⟩2
σ2+λ σ′2+λ (σ′2+λ)2 k
k j k
≤0.
According to Lemma 1, σ (X ) ≥ σ (X ),∀k = 1,...,d. Since we have
k f+1 k f
N
shown that
∂||E η[ϵˆϵϵs(α,f,f+ N1)]||2
≤ 0, i.e., the derivative of the error bias term
∂σk(X f+N1)
w.r.t a singular value σ′ of the teacher data matrix X is non-positive, and
k ft
the pruned data matrix used to train the student necessarily has smaller cor-
responding singular values, it necessarily implies that ||E [ˆϵϵϵ (α,f,f + 1)]||2 ≤
η s NDistilling the Knowledge in Data Pruning 7
||E [ˆϵϵϵ (α,f,f)]||2.Applyingthesamelogiciterativelyovertheprocessofadding
η s
moreandmoredatasamples,impliesthat||E [ˆϵϵϵ (α,f,f )]||2 ≤||E [ˆϵϵϵ (α,f,f)]||2
η s t η s
for any f >f.
t