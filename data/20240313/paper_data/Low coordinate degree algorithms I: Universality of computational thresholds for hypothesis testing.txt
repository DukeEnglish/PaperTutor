Low coordinate degree algorithms I: Universality of computational
thresholds for hypothesis testing
Dmitriy Kunisky∗
Department of Computer Science, Yale University
March 12, 2024
Abstract
We study when low coordinate degree functions (LCDF)—linear combinations of functions
dependingonsmallsubsetsofentriesofavector—canhypothesistestbetweenhigh-dimensional
probabilitymeasures. Thesefunctionsareageneralization,proposedinHopkins’2018thesisbut
seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature
as a proxy for all efficient algorithms for tasks in statistics and optimization. Instead of the
orthogonalpolynomialdecompositionsusedinLDP calculations,ouranalysisofLCDFisbased
on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable. By
way of illustration, we prove channel universality for the success of LCDF in testing for the
presence of sufficiently “dilute” random signals through noisy channels: the efficacy of LCDF
depends on the channel only through the scalar Fisher information for a class of channels
including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. As
applications, we extend lower bounds against LDP for spiked matrix and tensor models under
additive Gaussian noise to lower bounds against LCDF under general noisy channels. We also
give a simple and unified treatmentof the effect of censoring models by erasing observationsat
random and of quantizing models by taking the sign of the observations. These results are the
first computational lower bounds against any large class of algorithms for all of these models
whenthechannelisnotoneofafewspecialcases,andtherebygivethefirstsubstantialevidence
for the universality of several statistical-to-computationalgaps.
∗ Email: dmitriy.kunisky@yale.edu. Partially supported by ONR Award N00014-20-1-2335, a Simons Investigator
Award toDaniel Spielman, and NSFgrants DMS-1712730 and DMS-1719545.
4202
raM
21
]TS.htam[
1v26870.3042:viXraContents
1 Introduction 1
1.1 Detecting structured latent variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Low degree algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Low coordinate degree algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Summary of contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.5 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Notation 8
3 Main results 8
3.1 General theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3 Channel calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4 Latent variable models 17
4.1 Coordinate decomposition of likelihood ratio . . . . . . . . . . . . . . . . . . . . . . 17
4.2 Coordinate advantage: Proof of Theorems 3.5 and 4.4 . . . . . . . . . . . . . . . . . 18
5 Channel universality 20
5.1 Loose universality: Proof of Theorem 3.9 . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2 Tight universality: Proof of Theorem 3.17 . . . . . . . . . . . . . . . . . . . . . . . . 23
6 Applications 25
6.1 Spiked matrix models: Proof of Corollary 3.22. . . . . . . . . . . . . . . . . . . . . . 25
6.2 Spiked tensor models: Proof of Corollary 3.26 . . . . . . . . . . . . . . . . . . . . . . 27
6.3 Censorship: Proof of Theorem 3.28 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
6.4 Quantization: Proof of Theorem 3.32 . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Acknowledgments 30
References 30
A General tools for coordinate decomposition 36
A.1 Structure of coordinate subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
A.2 Alternative derivation with concrete bases . . . . . . . . . . . . . . . . . . . . . . . . 39
A.3 Coordinate decomposition of likelihood ratio . . . . . . . . . . . . . . . . . . . . . . 39
A.4 Additional remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
B Necessity of conditions for universality 42
C Omitted proofs 44
C.1 Assumptions for additive channels: Proof of Proposition 3.11 . . . . . . . . . . . . . 44
C.2 Assumptions for exponential family channels: Proof of Proposition 3.12 . . . . . . . 45
C.3 Truncated exponentials: Proof of Proposition 5.3 . . . . . . . . . . . . . . . . . . . . 47
C.4 Binomial coefficients: Proof of Proposition 5.6 . . . . . . . . . . . . . . . . . . . . . . 49
C.5 Polynomial advantage in spiked tensor model: Proof of Proposition 6.3 . . . . . . . . 491 Introduction
An effective theory of high-dimensional statistics requires computational as well as statistical
considerations—in working with large datasets, it matters not only whether it is possible to solve
a problem with some computation, but also whether that computation is efficient. A rich lit-
erature has grown around understanding the difference between these two demands, including
statistical-to-computational gaps between the parameter regimes where a problem can be solved at
all and those where it can be solved efficiently. Such gaps are conjectured to occur in problems
including finding weak community structure [DKMZ11b, DKMZ11a, Moo17, Abb17] and planted
cliques [Jer92, MPW15, BHK+19] in random graphs, various forms of principal component anal-
ysis for matrices [BR13, LKZ15b, LKZ15a, BMV+18, PWBM18, EAKJ20] and tensors [RM14,
LML+17, BGJ20, JLM20], and numerous other problems [AMK+18, COGHK+22, KLLM22]. Un-
fortunately, it seems out of reach of current techniques to show that such problems are compu-
tationally hard under standard complexity-theoretic assumptions like P = NP. Instead, many
6
other forms of evidence of computational hardness have been proposed, including the analysis of
various specific classes of algorithms (such as convex optimization and the sum-of-squares hierar-
chy [BHK+19, HKP+17, RSS18], Markov chain Monte Carlo [Jer92, DFJ02, CMZ23, GJX23], and
message-passing algorithms inspired by statistical physics [ZK16]), the study of the geometry of
solution spaces and optimization landscapes [ACO08, IKKM12, GS14, GM17, BAMMN19, GZ19],
and reductions among average-case problems [BR13, MW15, BBH18, BB19, BB20].
One class of computations that has come to play a central role in this pursuit is low degree
polynomial (LDP) algorithms. LDP are simple to describe—as the name suggests, they just com-
pute polynomials of the given data to solve a problem—and convenient to analyze, and yet seem
just as powerful as other, more complicated algorithms for many settings of hypothesis testing
[BHK+19, HKP+17, Hop18, BKW20, KWB22, DKWB23, BBK+21, BAH+22, BBH+20], estima-
tion[HS17,SW22,MW22],optimization[GJW20,Wei22],andconstraintsatisfaction[BH22]. How-
ever, analyzing LDP also comes with some drawbacks. First, its scope is currently limited to data
with convenient probability distributions that admit explicit families of orthogonal polynomials,
which give a well-behaved basis in which to consider arbitrary LDP algorithms. In particular, the
vast majority of LDP results concern just two data distributions: Gaussian and Bernoulli. Second,
LDP analyses typically involve repetitive combinatorial calculations (arising from high-dimensional
orthogonal polynomial expansions), which address computational hardness on a case-by-case basis
but arguably do not grant much insight into what aspects of a problem make it costly to solve.
The LDP framework so far has yielded a large zoo of examples of hard problems and statistical-to-
computational gaps, but not yet a theory to draw quick and accurate conclusions about questions
like: how will this problem change if we are provided with side information? If we only make a
fraction of our observations? If our data are corrupted by a different kind of noise?
In this paper, we develop tools to work with the more general class of low coordinate degree
function (LCDF) algorithms. We will see that LCDF are amenable to a much more general theory
than LDP. In particular, we give an analysis of the performance of LCDF for a class of hypothesis
testing tasks under essentially arbitrary noise models, and take initial steps towards developing
rules for how computational hardness changes under simple modifications of a statistical model.
1.1 Detecting structured latent variables
Many models of hypothesis testing exhibiting statistical-to-computational gaps may be cast in the
following general way; we will phrase all of our main results in these terms.
1Definition 1.1 (Continuous latent variable model). Let Σ R contain zero, and N 1. Let
⊆ ≥
be a probability measure over ΣN, and, for each x Σ, let be a probability measure over a
x
X ∈ P
measurable space Ω. A continuous1 latent variable model (CLVM) specified by these objects ( , )
X P
consists of the following pair of probability measures over ΩN:
1. Sample y Q by sampling y for each i [N] independently.
i 0
∼ ∼ P ∈
2. Sample y Pby first sampling x . Then, sample y for each i [N] independently.
∼ ∼ X
i
∼
Pxi
∈
The latent variable is x that is observed only indirectly through y under P. Often one is interested
in estimating x; however, we will focus here on the task of hypothesis testing between Q and P,
i.e., between x = 0 and x , which we call detection of the signal x . We work with the
∼ X ∼ X
following specific notion.
Definition 1.2 (Strong detection). Consider pairs of probability measures P ,Q over measurable
n n
spaces Ω . We say that functions test :Ω p,q achieve strong detection if
n n n
→ { }
lim P [test (y) = q]= lim Q [test (y) = p]= 0, (1)
n n n n
n n
→∞ →∞
that is, if the sequence of hypothesis tests test have both Type I and Type II error probabilities
n
tending to zero. When this asymptotic setting is clear from context, we informally call the entire
sequence (test ) “a test.”
n
We refer to the possibility of strong detection by an arbitrary test as statistical feasibility of such
a problem, and to the possibility of strong detection by an efficiently computable test (meaning in
polynomial time in n, unless otherwise specified) as computational feasibility.
We call the prior (on x) and the collection of measures = ( ) the channel through
x x Σ
which we obX serve x.2 The major assumption of such a model isP thatP Q is∈ a product measure and
P is a product measure after conditioning on the latent variable x.
Remark 1.3 (Signal-to-noise parameters). Many models of interest also include an explicit signal-
to-noise (SNR) parameter. This is a scalar λ > 0 that enters linearly into = , so that there is
λ
X X
some base measure and we sample x by drawing x and setting x = λx. We abbreviate
λ
X ∼ X ∼X
this relationsip of measures = λ . One then looks, over a growing sequence of priors and
n
X ·X X
resulting
n,λ
= λ e n, at how λ = λ(n) must scale foredeteection to become posesible as n ,
X ·X → ∞
either statistically or computationallye. e
e
Two important channels, those by far most widely studied in the literature, give the following
special CLVMs:
• Additive or Wigner Gaussian models, where Σ = Ω = R and = (x,σ2) for some σ2 > 0.
x
P N
Under Q we observe an i.i.d. Gaussian vector y (0,σ2I ), while under P we observe
N
∼ N
y = x+z for x and z (0,σ2I ).
N
∼ X ∼ N
• Bernoulli models, where Σ = [ c,1 c], Ω = 0,1 , and = Ber(c+x) for some c (0,1).
x
− − { } P ∈
Under Q we observe an i.i.d. vector of i.i.d. Bernoulli random variables y Ber(c), while
i
∼
under P we draw x and observe biased Bernoulli variables y Ber(c+x ).
i i
∼ X ∼
1The term “continuous” here refers to Σ consisting of real numbers. A companion paper will examine the com-
plementary discrete setting.
2Often in related literature the channel is written as a conditional probability P(y |x), but our notation Px will
be more convenient later to speak about the individualmeasures Px.
2For instance, spiked (Wigner, not Wishart) matrix and tensor models are usually formulated as ad-
ditive Gaussian models, and random graph models like the planted clique, planted dense subgraph,
and stochastic block models as Bernoulli models.
These are not the only interesting channels: other work has considered exponential [MMX19],
folded Gaussian [CKK+10], and Poisson [KN11] observations, for example, as well as general ad-
ditive noise for continuous signals [LKZ15a, KXZ16, PWBM18, MRY18] and general observation
models for discrete signals [LMX15, BBH19, DWXY21]. Yet, some of our most useful tools are re-
stricted to Gaussian and Bernoulli channels. We next review some of these tools for understanding
the computational power of LDP for detection in CLVMs through these special channels.
1.2 Low degree algorithms
The simple yet powerful class of low degree polynomial (LDP) algorithms seek to solve a detection
problem by computing a low degree polynomial of y whose typical value is different under y P
∼
and y Q. The following quantity, which we follow some recent work like [RSWY23] in calling the
∼
advantage, gives a simple measurement of how well LDP can “separate” P and Q.3
maximize E f(y)
y P
Adv ( , ) := subject to E ∼ f(y)2 1, . (2)
D y Q
≤ X P  ∼ ≤ 
f R[y]
 D 
∈ ≤
The following rough conjecture, which originates in a line of work on sum-of-squares optimiza-
 
tion [BHK+19, HS17, HKP+17, Hop18], expresses the idea that polynomials are optimal efficient
tests for a given runtime budget. It is imprecise, and various pathological cases have been devised
to refutethe conjecture without some additional clauses [HW21, Kun21a, KM21, DK22, ZSWB22],
but for many problems it gives predictions of hardness that match all other evidence.
Conjecture 1.4 (Informal low degree conjecture). Consider a sequence of CLVMs ( , ), giving
n
X P
rise to sequences of measures P and Q on ΩN for some N = N(n). For “sufficiently nice”
n n
priors and channels (see Chapter 2 of [Hop18] for some discussion of how these conditions might
be formalized), we conjecture:
1. If, for some D = D(n) = ω(logn), Adv ( , ) = O(1) as n , then there is no test
D n
≤ X P → ∞
computable in time polynomial in n that achieves strong detection between P and Q .
n n
2. If, for some D = D(n) = ω(1), Adv ( , ) = O(1) as n , then there is no test
D n
≤ X P → ∞
computable in time exp(D/polylog(n)) that achieves strong detection between P and Q .
n n
The second case is meant to allow for the study of subexponential time algorithms when we take
D = nδ for some δ (0,1) (see, e.g., [DKWB23]).
∈
Conversely,theunboundedness oftheadvantagealsoseemstohavecomputationalconsequences:
if, e.g., for D = (logn)K we have Adv ( , ) = ω(1), then we expect there to be an algorithm
D n
running in quasipolynomial time nD =≤ exX p(poP lylog(n)) that distinguishes P from Q, namely, com-
puting and thresholding the polynomial that is the optimizer in the advantage (2). This, too, is
not always true, but for “nice” problems and when we are considering D = polylog(n) it does seem
to be accurate; see Section 4 of [KWB22] for discussion. Thus, technicalities aside, the bottom line
that we will take as our motivation going forward is that the boundedness or divergence of
the advantage appears to govern the computational hardness of strong detection.
3A perhaps more natural notion of separation, treating P and Q symmetrically, is given in [BAH+22, RSWY23].
As detailed there, bounding the advantage is a means of excluding this kind of separation. We could apply this
alternative framing to LCDF as well, but we do not pursuethat direction here.
3The advantage may be analyzed quite directly in some models, because the optimal f may be
characterized as the orthogonal projection in L2(Q) of the likelihood ratio dP/dQ to the subspace
of polynomials of degree at most D, the low degree likelihood ratio (LDLR) (see, e.g., Proposi-
tion 1.15 of [KWB22]). The advantage itself is the norm in L2(Q) of the LDLR. Beyond that
general characterization, we also have the following elegant computation specifically for additive
Gaussian models.
Proposition 1.5 (Additive Gaussian advantage; Theorem 2.6 of [KWB22]). For any prior and
X
σ2 > 0,
1
Adv ≤D( X, N(x,σ2))2 = x(1),xE
(2)
exp ≤D (cid:18)σ2hx(1),x(2)
i
(cid:19), (3)
∼X
where x1 and x2 are independent draws from and exp D is the truncated Taylor series of the
≤
X
exponential function,
D td
exp D(t):= . (4)
≤
d!
d=0
X
The proof explicitly decomposes the LDLR in orthogonal polynomials in L2(Q), which, since Q is
a Gaussian product measure, are the multivariate Hermite polynomials.
ThisformulaisremarkableinhownicelytherolesofthethreeparametersD, ,and separate:
X P
D determines the truncation of the exponential power series, determines the distribution of
X
x(1),x(2) , and determines the variance σ2. Especially remarkable is that the advantage—a
h i P
priori a function of the high-dimensional measure —only depends on the distribution of the
X
scalar inner product x(1),x(2) , for which we borrow from statistical physics the term overlap.
h i
This dramatically reduces the dimensionality of the computation of the advantage, which is both
formally intriguingand practically usefulfor provingLDP lower boundsand other results (see, e.g.,
the arguments of [BKW20, KWB22, BBK+21, BAH+22]). We might view the Gaussian additive
channel (x,σ2) as leading to an integrable CLVM, where we can give a compact closed form for
N
the advantage using the special structure of the Hermite polynomials.
A few further channels, including Bernoulli, have been found to lead to likewise integrable
CLVMs which can be directly compared to Gaussian models (see Section 1.5). These findings,
however, are still very brittle: with these tools we still cannot say anything about the performance
of LDP, for example, for a channel applying additive noise with a density given by a slight per-
turbation of the Gaussian density, though it is intuitively obvious that this should not change the
computational cost of detection very much.
1.3 Low coordinate degree algorithms
We now introduce the low coordinate degree functions (LCDF) that will generalize LDP. In words,
whileLDP are linear combinations of low degree monomials—products of entries in a smallnumber
ofcoordinatesofavector—LCDFarelinearcombinations ofarbitrary functions ofentriesinasmall
number of coordinates.
For y ΩN and T [N], we write y ΩT for the restriction of y to the coordinates in T.
T
∈ ⊆ ∈
We define subspaces of L2(Q)
V := f L2(Q) :f(y) depends only on y , (5)
T T
{ ∈ }
V := V . (6)
D T
≤
T [N]
X⊆
T D
| |≤
4Here a sum of subspaces V + W is the set of all v + w with v V and w W. Note that
∈ ∈
V = V = L2(Q).
[N] N
≤
Definition 1.6 (Coordinate degree). For f L2(Q), cdeg(f):= min D :f V .
D
∈ { ∈ ≤ }
For f a polynomial we have deg(f) cdeg(f), so R[y] V , and LCDF are indeed a larger
D D
class than LDP for a given degree b≤ ound D. We note≤ also⊆ tha≤ t, unlike polynomial degree, all L2
functions have a finite coordinate degree.
This broader class was proposed in Hopkins’ thesis [Hop18], where it was suggested as a more
natural collection of statistics than LDP, in particular one that is closed under arbitrary entrywise
transformations. Yet, to the best of our knowledge, LCDF have only been studied a few times
subsequently in some special cases [BBH+20, KM21, HM24].
Paralleling the LDP framework, we define the coordinate advantage
maximize E f(y)
y P
CAdv ( , ) := subject to E ∼ f(y)2 1, Adv ( , ). (7)
D y Q D
≤ X P  ∼ ≤  ≥ ≤ X P
f V
 D 
∈ ≤
As for LDP, the optimal f is theorthogonal projection of the likelihood ratio to V , which we
D
≤
call the low coordinate degree likelihood ratio (LCDLR), and the coordinate advantage is the norm
of this projection.
We call Adv the polynomial advantage when we want to emphasize the distinction between
D
≤
it and CAdv . Because of the above inequality, bounding the coordinate advantage also bounds
D
≤
the polynomial advantage and thus, conditional on Conjecture 1.4, shows computational hardness
of strong detection.4 Alternatively, one may view bounding the coordinate advantage as just a
lower bound against LCDF, which are an interesting class of algorithms, at least as powerful as
LDP, and perhaps, as Hopkins proposed, more natural. And conversely, as for LDP, the divergence
of the coordinate advantage reasonably suggests that there should be an LCDF-based algorithm
achieving strong detection.
1.4 Summary of contributions
We present informal summaries of our main results before giving precise statements in Section 3.
General theory on universality The following summarizes what our general results for arbi-
trary CLVMs will imply. We indicate “in quotation marks” the vague terms to be clarified later.
Theorem 1.7 (Informal). The coordinate advantage is the same up to constants for a “sufficiently
dilute” prior observed through any channel having a given Fisher information (a scalar parame-
ter) that either (1) is addition of i.i.d. noise with a “sufficiently nice” distribution (including any
symmetric, positive, and smooth density on R), or (2) makes observations in a “sufficiently nice”
exponential family (including that generated by any subgaussian base measure).
When Theorem 1.7 applies, the coordinate advantage is also close to the polynomial advantage for
the additive Gaussian channel, for which we have the formula of Proposition 1.5 and numerous
prior results. This existing analysis of additive Gaussian models therefore generalizes through our
results “for free” to much more general noise models.
4One may of course also formulate a “low coordinate degree conjecture” paralleling Conjecture 1.4, for a version
of which thereader may consult [Hop18].
5Applications In this way, we will show the universality of two of the best-known statistical-
to-computational gaps in high-dimensional hypothesis testing: those in spiked matrix [Joh01] and
spiked tensor models [RM14]. In brief: we consider, for q 2, λ = λ(n) 0, and x Rn a
≥ ≥ ∈
suitably scaled random vector, distinguishing a symmetric tensor of i.i.d. random variables from
one to which the rank one “signal” tensor λx q has been added. In both the matrix case q = 2
⊗
and the tensor case q 3, the computational difficulty of the problem depends on λ. In the matrix
≥
case, there is a “sharp”critical λ such that for λ < λ strong detection is believed to require
comp comp
nearly exponential time in n, while for λ > λ a simple algorithm achieves strong detection. In
comp
the tensor case, there is a “smoother” transition between the easy and hard regimes, where there is
arangeofvaluesofλ < λ < λ forwhichstrongdetection isbelieved torequiresubexponential
poly exp
time exp(O(nδ)) for δ (0,1) depending on λ.5 In both cases, previous work gave evidence for
∈
theseclaimsunderGaussiannoisebyanalyzingLDP[HKP+17,Hop18,KWB22]. Wegiveatwofold
generalization, identifying the computationally hard regimes of λ for the broader class of LCDF
and for nearly arbitrary additive noise.6 Notably, these results follow nearly automatically from
our general results applied to the above prior work.
Channel calculus Lastly, webegin todevelop a“calculus” that seeks tounderstandhow general
modifications of the noisy channel affect the difficulty of hypothesis testing. We consider two
operations applied after the noisy channel in a CLVM:
1. In censorship, meant to describe “missing data” in statistical parlance, each coordinate y of
i
our observation is replaced with a null symbol “ ” independently with probability η [0,1].
• ∈
2. In quantization, meant to describeobservations with low numerical precision, each coordinate
y of our observation is replaced with sgn(y ).
i i
Each may be seen as transforming the channel into a new channel . We compute the Fisher
′
P P
information following either transformation, which, per Theorem 1.7, describes how the computa-
tional cost of strong detection changes. As a consequence, in the spiked matrix and tensor models,
we obtain—again nearly automatically—predictions of computational thresholds with accompa-
nying lower bounds against LCDF under arbitrary noise and when a constant fraction η of the
observations are withheld or when the observations are quantized.
1.5 Related work
Universality in probability theory The paradigm of studying a class of models by first thor-
oughly understanding integrable models that allow for exact algebraic computations and then
appealing to universality phenomena that relate general models to integrable ones is common in
probability theory. For example, one may study general random walks on R through the simple
random walk on Z [Don51], general random matrices through Gaussian random matrices like the
Gaussian orthogonal ensemble [ESY11], and general surface growth models through ones with spe-
cial combinatorial structure like the asymmetric simple exclusion process [Cor12]. Our aim here is
to initiate such a program for low degree algorithms. It is an interesting and important question
to understand universality for other classes of algorithms and computations as well, which has in
various senses been considered by recent works like [DT19, WZF22, DMLS23].
5Ineithercasethereisalsoastatisticalthreshold,anevensmallerλ stat belowwhichstrongdetectionisimpossible.
6WealsodiscussinRemarks3.24and3.29howsuitablechoicesofexponentialfamily channelsyieldlowerbounds
for certain formulations of the stochastic block model.
6Further integrable channels Similar calculations to those underlying Proposition 1.5 are also
possible for Bernoulli channels, using the Boolean Fourier basis, the orthogonal polynomials for
Ber(1), to give a closed form for the advantage. The author’s work [Kun21a] extended this kind
2
of calculation to other channels where belong to a special class of exponential families, such as
x
P
binomial, geometric, Poisson, and exponential, again using algebraic properties of the orthogonal
polynomials for those families. That work (as well as Appendix B of [BBK+21] for the Bernoulli
case) also gave comparison inequalities relating the advantage under such channels to the Gaussian
advantage. Our results may be seen as a major generalization of those, which still rely heavily on
algebraic structure and thus only apply to a few integrable channels.
Channel universality Universality for statistical tasks with respect to the output channel has
alsobeenstudiedbefore. Thespecialcaseofspikedmatrixmodelswasstudiedby[LKZ15a,KXZ16,
PWBM18],whonoticedthecentralroleoftheFisherinformation. However,theseresultsconsidered
either statistical analysis or special algorithms like computing matrix eigenvalues and approximate
messagepassing. Oursisthefirstresultinthisdirectiontoshowchanneluniversalityofthebehavior
of a broad class of algorithms; perhaps our main conceptual observation is that the analysis of low
degree algorithms is formally similar enough to a “truncation” of the statistical analysis that some
key technical ideas of the above works still apply. Taking a different approach, [BBH19] argued for
channel universality for the particular problem of submatrix detection by considering reductions
between different output channels. This gives results applying to all polynomial time algorithms,
but is restricted to one specific class of priors specifying the submatrix detection problem, while
X
our results allow for a broad range of .
X
Low coordinate degree algorithms Since their proposal in Hopkins’ thesis [Hop18], LCDF
havemadeafewfurtherappearances. In[KM21,HM24],theywereusedtoformulateLDPwhenthe
outputofachanneliscategorical, takingdiscretevaluesinsomefinitesetoflabels,y ℓ ,...,ℓ .
i 1 k
∈ { }
In this case, LCDF and LDP are actually equivalent, so long as LDP are taken over a “one-
hot” encoding of the y as (1 y = ℓ ) (see our Example A.5 and discussion throughout
i i j i [N],j [k]
[KM21]). In a different vein,
[{ BBH+20}
]
d∈ rew∈
a connection between LCDF and the statistical query
model of hypothesis testing, where an algorithm is allowed a budget of arbitrary queries of a
probability measure. This work considered LCDF with a very “coarse” notion of coordinate, where
one observes, e.g., many weakly informative samples of a random vector, each of which counts as
one coordinate of the entire observation. This is a very special case of the general latent variable
model (without the “continuity” assumption) we give in Definition 4.1 where P and Q are both
product measures, albeit over a higher-dimensional domain.
Efron-Stein decomposition In statistics and analysis, the idea of our technique for project-
ing the likelihood ratio to LCDF has variously been referred to as Efron-Stein, analysis of vari-
ance (ANOVA),or Hoeffding decomposition. It is presented, for instance, in Section 8.3 of [O’D14].
Its use in statistical applications is that a refinement of the subspaces V that we present in Ap-
T
pendixAmaybeusedtocomputeameasurementofthe“collectivecontributionfromthecoordinate
set T” to the total variance of the random variable f(y) for y Q, which underpins the ANOVA
∼
methodology. In that language, our approach may be described as performing an ANOVA analysis
of thelikelihood ratio itself, studyinghow much of its varianceis “dueto” interactions amongsmall
numbers of coordinates of our observations. The idea of projecting complicated functions to sub-
spacesoffunctionsoflowcoordinatedegreehasalsobeensystematically exploredforapplicationsin
physical sciences, especially in computational chemistry, underthename of high-dimensional model
7representation (HDMR). See [LRR01] for a general survey and [RA99] for a more mathematical
survey.
Channel calculus The effect of composing noisy channels has been much studied in information
theory, though the Fisher information is a less common quantity in that literature. Still, as for
other more common measurements of a channel’s “fidelity” like the mutual information, the Fisher
informationisknowntosatisfyadataprocessinginequality(sothat,e.g.,censorshiporquantization
cannot increase the Fisher information) and a chain rule [Zam98].
2 Notation
We write [N] := 1,...,N . The asymptotic notations o(),O(),ω(),Ω(), have their usual
{ } · · · · ≍
meanings in the limit N or n when the variable n is defined in context. We write
→ ∞ → ∞
1 Rk for the vector all of whose entries equal 1 and I Rk k for the k k identity matrix.
k k ×
∈ ∈ ×
For y an N-dimensional vector and T [N], we write y for the restriction to the index set T.
T
⊆
We write (µ,σ2) and (µ,Σ) for the scalar and vector Gaussian measures with given mean
N N
and variance (respectively, covariance matrix) parameters. We write δ for the Dirac probability
x
measure at x, and Ber(c) = (1 c)δ +cδ for the Bernoulli measure. This notation for mixtures
0 1
−
will not be confused with c , which denotes the measure formed by sampling from and then
·X X
multiplying by c. We adoptthe slightly informalnotation of denotinga channel =( )by one of
x
P P
the above notations where x is viewed as a “dummy” variable, so that, e.g., Adv ( , (x,σ2)) =
D
Adv ( , ) where = (x,σ2). ≤ X N
D x
≤ X P P N
3 Main results
3.1 General theory
Our first main result computes the coordinate advantage in closed form and gives a “nonlinear
overlap” bound on it for any CLVM.7
Definition 3.1 (Good CLVM). We call a CLVM good if Σ contains an open interval around zero,
and, for all x Σ, is absolutely continuous with respect to and d /d L2( ).
x 0 x 0 0
∈ P P P P ∈ P
Definition 3.2 (Channel overlap). For each x(1),x(2) Σ in a good CLVM, define
∈
d d
R (x(1),x(2)) := E Px(1)(y) 1 Px(2)(y) 1 . (8)
P y ∼P0(cid:20)(cid:18) d P0 − (cid:19)(cid:18) d P0 − (cid:19)(cid:21)
The following examples are some simple cases of this function. Note that near the origin each
behaves, up to rescaling, like x(1)x(2), which is the key phenomenon we will exploit later.
Example 3.3. If = (x,1), then R (x(1),x(2)) = exp(x(1)x(2)) 1.
x
P N P −
Example 3.4. If = Ber(1 +x), then R (x(1),x(2))= 4x(1)x(2).
Px 2
P
One should think of R as a kind of “kernel” associated to a channel that contains all of the data
aboutthechannel thatP willberelevant to us. Inprobabilisticterms, R (x(1),x(2))is thecovariance
P
between the likelihood ratios of and with respect to , and thus is a measurement of
Px(1) Px(2) P0
7This may be viewed as a generalization of the bound derived in Appendix B.1 of [BBK+21] of the Bernoulli
advantage bythe Gaussian advantage.
8the similarity between and . The diagonal values give the χ2 divergence (Definition A.9)
Px(1) Px(2)
between and , R (x,x) = χ2( ).
x 0 x 0
P P P P kP
Theorem 3.5 (Coordinate advantage). Suppose ( , ) is a good CLVM. Then,
X P
CAdv ( , )2 = E R (x(1) ,x(2) ) (9)
≤D X P x(1),x(2) P i i
∼XT X⊆[N]i Y∈T
T D
| |≤
N
E exp D R (x(1) ,x(2) ) . (10)
≤ x(1),x(2) ≤ P i i !
∼X Xi=1
We can actually treat a more general class of models than CLVMs as defined here, which we leave
to Theorem 4.4.
(1) (2)
The resemblance to Proposition 1.5 is clear, with the two differences that x x is replaced
i i
(1) (2)
by the nonlinear R (x ,x ), and that the expression in (10) is only a bound on the coordinate
i i
P
advantage rather than a formula for it. But, we will show that, in many cases, neither of these
differences is very consequential, and for many channels the coordinate advantage actually behaves
just like the polynomial advantage for a Gaussian channel.
We first address the matter of the nonlinear overlap. In fact, we show that the summation
in (10) in many cases may be replaced by an overlap formula of precisely the form that arose
for the polynomial advantage for additive Gaussian models in Proposition 1.5. The channel then
entersintotheboundnotthroughthepotentially complicated nonlinearR ,butthroughthescalar
Fisher information, which describes R near the origin (it is just the conP stant scaling x(1)x(2) in
P
the approximation alluded to for Examples 3.3 and 3.4). The reciprocal of the Fisher information
plays the role of an “effective σ2” in the formula for the additive Gaussian advantage. Let us give
a clearer name to the expression that will appear, to avoid confusion from mixing polynomial and
coordinate advantages:
1
Univ ( ,σ2):= E exp D x(1),x(2) = Adv ( , (x,σ2))2. (11)
≤D X
x(1),x(2)
≤ (cid:18)σ2h i
(cid:19)
≤D X N
We also delineate the assumptions on the prior and channel that will play important roles. When
we refer to the constants A,B later, they will always refer to the constants in these assumptions.
k
Assumption 3.6 (Prior assumptions). We define conditions on a prior , parametrized by con-
X
stants A,B ,B ,B ,B ,B ,B > 0:
2 4 6 8 10 12
P1. x A.
k k∞ ≤
1 1
P2. For each k 2,4,6 , x
k
B kNk−4.
∈ { } k k ≤
1 1
P3. For each k 8,10,12 , x
k
B kNk−4.
∈ { } k k ≤
Assumption 3.7 (Channel assumptions). We define conditions on a channel , parametrized by
P
a constant A > 0 (always the same as A in Assumption 3.6):
C1. R (x(1),x(2)) is 4 in an open set containing [ A,A]2.
P C −
C2.
∂3RP
(0,0) = 0.
∂x(1)2∂x(2)
9Definition 3.8 (Fisher information). For a channel satisfying Assumption C1, its Fisher infor-
P
mation is
∂2R
F := P (0,0). (12)
P ∂x(1)∂x(2)
The Fisher information in statistics is usually viewed as a function of the signal x, F (x), and
P
in this language what we work with is F = F (0) (see also Proposition 5.1). Its values for the
P P
Gaussian and Bernoulli channels from Examples 3.3 and 3.4 are 1 and 4, respectively.
Theorem 3.9 (Loose channel universality). Let ( , ) be a good CLVM. Suppose that satisfies
X P P
Assumptions C1 and C2, and satisfies Assumptions P1 and P2. Then, there is a constant C > 0
1
X
depending only on the channel and the constants (A,B ,B ,B ) in the Assumptions such that,
2 4 6
P
for all D 0 even,
≥
CAdv ( , )2 C Univ ( ,1/F ). (13)
D 1 D
≤ X P ≤ ≤ X P
Suppose further that satisfies Assumption P3. Then, there are also constants C ,C > 0 de-
2 3
X
pending only on the channel and on the constants (A,B ,B ,B ,B ,B ,B ) such that, for all
2 4 6 8 10 12
P
D 0 even,
≥
CAdv ( , )2 C Univ ( ,1/F ) C Univ ( ,1/F ). (14)
D 2 D 3 D 2
≤ X P ≥ ≤ X P − ≤ − X P
To understand the result, first recall that, for studying hardness of detection, we are interested in
understanding boundedness or divergence of CAdv over a sequence of CLVMs. The upper and
D
≤
lower boundsin (13) and (14) will match up to constants for D = ω(1) so long as Univ ( ,1/F )
D
≤ X P
either is bounded (so that the upper and lower bounds are both constant) or grows at a suffi-
ciently fast exponential rate in D (so that the second term in (14) becomes negligible and we
find CAdv ( , )2 Univ ( ,1/F )). Because of this “sufficiently fast” clause, this result
D D
≤ X P ≍ ≤ X P
gives universality of detection thresholds only up to constant factors in an SNR parameter (see
Remark 1.3; roughly speaking, the actual exponential growth of Univ ( ,1/F ) is usually at a
D
≤ X P
rate proportional to how much greater the SNR is than the threshold for detection), which is why
we call the result “loose.” For a special class of priors , we will later give a tight characteriza-
X
tion of the coordinate advantage up to constants, thus giving true universality of computational
thresholds; however, this more ad hoc bound is useful to treat arbitrary priors.
Let us parse the conditions of the Theorem and give some remarks on its applicability. As-
sumptions P1–P3 on the prior may be viewed as asking that the typical value of x is at most
i
| |
N 1/4, allowing for a small number of outliers. This is a natural scaling, occurring for instance in
−
low-rank spiked matrix models (see Corollary 3.22 and Section 6.1). In Example B.1 we give an
illustration that some assumption to this effect is necessary for universality to hold.
Remark 3.10 (Truncating priors). By a common trick, used for example in [PWBM18, BKW20],
it suffices, if we are assuming Conjecture 1.4 and reasoning about a sequence of problems ( , )
n
X P
over a fixed channel, to have the Assumptions P1–P3 hold with high probability as n . This
→ ∞
is because we may define an adjusted prior where we sample x as x = x1 E for x
n n
X ∼ X { } ∼ X
and E the event that P1 and P2 hold. Then, if we prove that strong detection is hard for , we
n
X
automatically learn the same for n, since thee two agree with higeh proebabilitey.
X
e
The conditions C1 and C2 on the channel are harder to parse, but let us demonstrate how mild
they are. The following two results show that nearly arbitrary additive noise channels and nearly
arbitrary channels where observations are made in an exponential family satisfy these conditions.
Theresultfor additive noise is similar in spirittoresults of [PWBM18], and theonefor exponential
families to those of [Kun21a], though in both cases our results have moregeneral consequences. We
give the proofs—straightforward verifications of the Assumptions—in Appendices C.1 and C.2.
10Proposition 3.11 (Additive noise channels). Suppose that is a channel such that, for all x Σ,
P ∈
is the law of x+z for z ρ with ρ a probability measure on R having a density p(y) with respect
x
P ∼
to Lebesgue measure that satisfies:
1. p(y) > 0 for all y R.
∈
2. p(y) is 4 on all of R.
C
3. p(y) = p( y) for all y R.
− ∈
Then, Assumptions C1 (for arbitrary choice of the constant A) and C2 are satisfied. The Fisher
information is given in terms of p by
p(y)2
∞ ′
F = dy. (15)
P p(y)
Z−∞
Proposition 3.12 (Exponential family channels). Suppose that has mean µ and variance
0
P
σ2 > 0, and, for all x Σ, belongs to the natural exponential family generated by (see
x 0
∈ P P
Definition C.3) and has x = E [y] µ. Suppose also that has a cumulant generating func-
y ∼Px − P0
tion ψ(θ) := logE exp(θy) that satisfies:
y 0
∼P
1. ψ is 5 in an open set U containing zero.
C
2. ψ (U) contains an open set U [ A+µ,A+µ].
′ ′
⊃ −
Then, Assumptions C1 and C2 are satisfied, where A isthe same as the constant in Assumption C1.
The Fisher information is
1
F = . (16)
P σ2
Example 3.13. If E exp(θy) < for all θ R, e.g. if is subgaussian (with any variance
y ∼P0 ∞ ∈ P0
proxy), then ψ is automatically smooth on all of R and all three conditions are satisfied.
Remark 3.14 (Difference-of-means parametrization). The condition x = E [y] µ says that
y ∼Px −
not only do the belong to an exponential family, but also that they are specifically parametrized
x
P
by the displacement of their means from that of . This is important to our calculations in
0
P
Section C.2 and to an “overlap formula” holding, and is not original; the idea that the “right”
overlap controlling exponential family channels is an overlap of z-scores (as we obtain if we move
the factor of F = 1/σ2 inside of x1,x2 in Theorem 3.9) appeared already in [Kun21a].
P h i
Remark 3.15 (Exponential families are hardest). In [PWBM18], it is noted that, among additive
noise models as in Proposition 3.11, for a given variance the Gaussian distribution uniquely min-
imizes the Fisher information: for = (x,σ2), we have F = 1/σ2, which per Corollary 5.8
x
P N P
is the smallest possible value. The authors conclude (for their particular spiked matrix problem)
that Gaussian is therefore the “hardest” distribution of additive noise. Our Proposition 3.12 shows
that, if we move beyond additive noise, all exponential family channels are “as hard as” the additive
Gaussian channel with the same variance. This perhaps clarifies the “Gaussian is hardest” phe-
nomenon: generally it is exponential family channels that are hardest; the key property of Gaussian
measure is that the Gaussian additive channel is the only additive noise channel whose measures
also form an exponential family.
x
P
In our final general result, we consider when the upper bound of Theorem 3.9 is tight up
to constants, in which case whether CAdv ( , ) is bounded or divergent as n is truly
D n
≤ X P → ∞
universal with respect to the channel.
11Definition 3.16 (Prior dilution). Let be a probability measure over ΣN as in the definition of
X
a CLVM, and let k 1. The k-dilution of , denoted , is the probability measure over ΣkN
k
≥ X D X
where, to sample x , we sample x and set x = 1 (x ,...,x ,...,x ,...,x ), where
∼ Dk X ∼ X √k 1 1 N N
each entry is repeated k times (equivalently, x =x 1 1 ).
⊗ √k k
e e
Theorem 3.17 (Tight channel universality). Let ( , ) be a good CLVM. Suppose that satisfies
e X P X
Assumptions P1 and P2, that satisfies Assumptions C1 and C2, and that k D2. Then, there
P ≥
are constants C ,C only depending on the channel and on the constants (A,B ,B ,B ) such
1 2 2 4 6
P
that
C Univ ( ,1/F ) CAdv ( , )2 C Univ ( ,1/F ). (17)
1 D D k 2 D
≤ X P ≤ ≤ D X P ≤ ≤ X P
We note that dilution, in addition to giving this tight characterization of the coordinate advantage,
can also cause a prior to satisfy Assumptions P1–P3, since x is unchanged by dilution, while
2
k k
x for k 2 only decrease.
2k
k k ≥
Theidea of diluting priors for the analysis of low degree algorithms (though for a quite different
purpose of relating low degree algorithms to the statistical query model) appeared previously in
[BBH+20]. As discussed there, for the Gaussian channel, an algorithm may itself generate more
dilute samples from given less dilute ones.
Proposition 3.18 (Gaussian dilution invariance; Lemma 7.2 of [BBH+20]). There are randomized
polynomial-time algorithms computing f :R Rk and g : Rk R such that:
→ →
• If y (µ,σ2), then f(y) (µ1 /√k,σ2I ).
k k
∼ N ∼ N
• If y (µ1 /√k,σ2I ), then g(y) (µ,σ2).
k k
∼ N ∼ N
In this sense, Gaussian models are a “fixed point of dilution,” which may be seen as a justification
for why sufficiently dilute models behave like Gaussian ones. More specifically, as we sketch in
Section 5.2, in a diluted model, an algorithm may compute an approximately minimum variance
unbiased estimator ofx fromthek samplesfrom andaverage themtoobtainanobservation
i Pxi/√k
of x that will, by the central limit theorem, be asymptotically Gaussian. A version of the Cram´er-
i
Rao lower bound implies that the lowest variance achievable by such an estimate is precisely 1/F .
P
If this holds, then the resulting averages will be (approximately) distributed as (x ,1/F ), thus
i
N P
reducing the dilute model with an arbitrary channel to a corresponding less dilute Gaussian model
of variance 1/F .
P
Remark 3.19 (Channel universality of χ2 divergence). As an aside, we note that if we take
D = N in the coordinate advantage, we obtain the L2(Q) norm of the likelihood ratio, which is
1 + χ2(P Q), where the latter is the χ2 divergence. Thus, at D = N, our results give channel
k
universality for the χ2 divergence, an interesting and new phenomenon in itself, much in the spirit
of the channel universality of the mutual information studied by[LKZ15a, KXZ16] forspiked matrix
models. The χ2 divergence underlies the second moment method for establishing statistical lower
bounds for hypothesis testing [MRZ15, BMV+18, PWBM18]. However, it is often augmented with
conditioning rather than merely computing and bounding the divergence itself; it is likely possible
to use our results in tandem with these techniques, but we leave this investigation to future work.
See also Appendix A.4 for more connections between our calculations and the χ2 divergence.
3.2 Applications
We now move to more concrete considerations, and give two applications of our framework to
identifying computationally hardregimes. In both applications, we will work with priors builtfrom
the following kind of random vectors.
12Assumption 3.20 (Spike priors). Suppose that π is a bounded probability measure on R with
E [x] = 0 and E [x2] = 1. We will consider x Rn random with x π i.i.d.
x π x π i
∼ ∼ ∈ ∼
We could somewhat relax the boundedness condition and obtain similar results by using the trun-
cation idea in Remark 3.10; see, e.g., the general notion of “tame priors” from [Kun21b].
Spiked matrix models The first application revisits the non-Gaussian spiked matrix models
studied by [LKZ15a, PWBM18]. Let be an additive noise channel satisfying the assumptions
P
of Proposition 3.11, with underlying density p. Let λ > 0. For x as in Assumption 3.20, let
n
X
denote the law of the upper triangle of λ xx (not including the diagonal). We view draws from
√n ⊤
and associated CLVMs as symmetric matrices whose diagonal is zero, by repeating every entry
n
X
from the upper triangle in the lower triangle.
The following result analyzes a natural algorithm for testing in such a model by thresholding
the largest eigenvalue of the observed matrix after an entrywise transformation. One of the main
insights of [LKZ15a,PWBM18] is that this transformation is necessary to obtain an algorithm that
performs optimally.
Proposition 3.21 (Pretransformed eigenvalue test; Theorem 4.8 of [PWBM18]). Suppose that
λ > 1/√F , write f(y) := p(y)/p(y), and make the following additional assumptions on the
′
P −
density p:
1. f and its first two derivatives are polynomially bounded, i.e., f(ℓ)(y) C + ym for some
| | ≤
C > 0 and even m 2 and each ℓ 0,1,2 .
≥ ∈ { }
2. p has finite moments up to order 5m: ∞ x k < for 1 k 5m.
| | ∞ ≤ ≤
−∞
Write f(Y) for the entrywise application ofRf to a matrix Y. Define the function
q if n 1/2λ (f(Y)) < √F + 1λF + 1 ,
test(Y):=
p if
n− 1/2λmax
(f(Y))
√FP
+
2 1λFP
+
2 1λ (18)
(cid:26) − max ≥ P 2 P 2λ
Then, test runs in time poly(n) and achieves strong detection in the CLVM ( , ).
n
X P
Our results imply the following complementary result.
Corollary 3.22 (LCDF analysis of spiked matrix model). The following hold:
1. If λ < 1/√F , then for any D = D(n) = o(n/logn), CAdv ( , ) = O(1). Conse-
D n
P ≤ X P
quently, if Conjecture 1.4 holds, then, for any δ > 0, there is no algorithm that runs in time
exp(O(n1 δ)) and achieves strong detection in the CLVM ( , ).
− n
X P
2. If λ > 1/√F , then for any D = D(n)= ω(logn), CAdv ( , ) = ω(1).
D n
P ≤ X P
Thus, to improve on the pretransformed eigenvalue test requires nearly exponential time. The
proof of Claim 1 is very simple, combining our Theorem 3.9 with the previous results of [KWB22]
on additive Gaussian models. Claim 2 encodes a version of Proposition 3.21 by using the trace
of a large power of a matrix as a proxy for the spectral norm. We emphasize that, despite this
simplicity, Claim 1 above gives the first evidence of hardness for any large class of algorithms for
all but a few very special cases of the density p.
Remark 3.23 (Higher rank priors). The prior work [BBK+21] established limitations of LDP for
detecting matrices of constant rank k 2 as n observed through additive Gaussian noise. We
≥ → ∞
omit the details, but the proof of Corollary 3.22 mutatis mutandis also extends those limitations to
LCDF and establishes their channel universality, so long as λ is again scaled by √F .
P
13One interesting example is the sparse Rademacher prior where, for some s (0,1], x in the
∈
prior has entries distributed as
s s
π = (1 s)δ + δ + δ . (19)
− 0 2 1/√s 2 −1/√s
As predicted using non-rigorous statistical physics methods by [LKZ15b, LKZ15a] and proved by
[KXZ16], there is some s 0.09 such that, once s < s , there is an exponential-time exhaustive
∗ ∗
≈
searchalgorithmachievingstrongdetectionintheCLVM( , )forsomevaluesofλ < 1/√F .8 In
n
X P P
this context, our Corollary 3.22 gives evidence that this problem has a statistical-to-computational
gap for any reasonable choice of additive noise. Again, this is the first evidence—in the form of
lower bounds against any large class of algorithms—for this statistical-to-computational gap for
most noise distributions.
Remark 3.24 (Stochastic block model). Considering the sparse Rademacher prior through the
Bernoulli channel = Ber(1+x), we obtain a model that is a dense version of the stochasticblock
Px 2
model (see [Abb17, Moo17] for general background): we seek to distinguish an Erdo˝s-R´enyi random
graph on n vertices with edge probability 1 from a graph with two random “planted communities,”
2
each of roughly s n vertices, such that vertices are connected with probability 1 + λ within those
2 · 2 sn
communities, probability 1 λ between them, and probability 1 otherwise. The same argument
2 − sn 2
as for the first claim of Corollary 3.22 shows that LCDF fail to achieve strong detection in this
model when λ < 1/√F = 1/2, and the results of [KXZ16] imply a statistical-to-computational
P
gap for sufficiently small s. Similar results also follow for observations in other non-Bernoulli
discrete exponential families, such as Poisson, binomial, or geometric. We do not pursue it here,
but, following Remark 3.23, greater numbers of communities may also be treated by considering a
signal matrix of higher rank, as in [BBK+21].
Spiked tensor models The second application concerns the problem of tensor PCA, for which,
to the best of our knowledge, general non-Gaussian additive noise has not been considered before.
Let q 3. We will now allow λ = λ(n) > 0 to vary. For x as in Assumption 3.20, let denote the
n
≥ X
law of the entries of λn q/4x q indexed by tuples 1 i < < i N. (Again, one may view
− ⊗ 1 q
≤ ··· ≤
draws from the prior and observations from the CLVM as being symmetric tensors with all entries
having repeated indices set to zero.)
The following result is a sharpening of prior work of [HKP+17, Hop18] that precisely charac-
terizes (in the low degree framework) the power of subexpoential time algorithms for tensor PCA.
Proposition 3.25 (Theorem 3.3 of [KWB22]; Theorem 5.2.6 of [Kun21b]). Let = (x,1).
x
P N
Then, there are constants a ,b > 0 such that:
q q
1. If λ a D (q 2)/4, then Adv ( , ) = O(1).
q − − D n
≤ ≤ X P
2. If λ b D (q 2)/4, D = ω(1), and D 2n, then Adv ( , ) = ω(1).
≥ q − − ≤ q ≤D Xn P
As this result shows, unlike spiked matrix models, spiked tensor models have a smoother
statistical-to-computational “ramp” with a large parameter regime where subexponential time al-
gorithms exist, which manifests as a polynomial dependence on D in the thresholds for λ. Because
8This is not entirely explicit in those results, but, for discrete priors such as sparse Rademacher, one may view
theirresultsascomputingexplicitlythetypicalvalueofthefree energy, aparticular functionof theobservation in a
spikedmatrixmodelwhichessentially coincides withthelikelihood ratio. Theirresultsthenimplythatthresholding
the free energy achieves strong detection, which amounts to analyzing the statistically optimal hypothesis test per
theNeyman-Pearson lemma.
14of this,it isless meaningfulhereto pindowntheconstants a andb . Allowing thisslack makes our
q q
machinery even more useful, and we may with a very straightforward application of Theorem 3.9
obtain the following direct generalization to arbitrary additive noise models.
Corollary 3.26 (LCDF analysis of spiked tensor model). Let be an additive noise channel
P
satisfying the assumptions of Proposition 3.11. Then, there are constants a ,b > 0 such that:
q, q,
P P
1. If λ a D (q 2)/4, then CAdv ( , ) = O(1).
q, − − D n
≤ P ≤ X P
2. If λ b D (q 2)/4, D = ω(1), and D 2n, then CAdv ( , ) = ω(1).
≥ q, P − − ≤ q ≤D Xn P
Note that, unlikeinthespikedmatrix modelwherethepretransformedeigenvalue testhadbeen
studied before, here we are able to make predictions about computational thresholds for problems
where explicit algorithms have not been studied at all (for general choices of the density p). Our
result suggests that there is some LCDF depending on the density p that achieves strong detection
(say, by thresholding as in Proposition 3.21) up to the value of λ we predict, but we may generate
our predictions without even writing this function down explicitly.
3.3 Channel calculus
Finally, we elaborate on our results on modifying noisy channels.
Definition 3.27 (Censorship). Let be a channel with range Ω and η [0,1]. We define the
P ∈
η-censored version of to be the channel with range Ω for a new formal symbol , where
η
P C P ⊔{•} •
to sample from the measure , we observe with probability η, and a sample from with
η x x
C P • P
probability 1 η.
−
Theorem 3.28 (Censored channels). Suppose is a channel satisfying Assumptions C1 and C2.
P
Then, for any η [0,1], also satisfies Assumptions C1 and C2, and has Fisher information
η
∈ C P
F = (1 η)F .
Cη P − P
The theorem is simple both to state and to prove, but is deceptively powerful. For example,
combined with Corollary 3.22, we deduce that strong detection in the spiked matrix model with
additive noise channel censored at a constant rate η is hard for LCDF when λ < 1/ (1 η)F
P − P
(for a censored spiked tensor model, Corollary 3.22 holds verbatim with constants a and b
q, Pp,η q, P,η
adjusted to absorb the dependence on η).
Again, to the best of our knowledge these models have not been studied before, and our theory
can make predictions of computational thresholds even in the absence of “baseline” algorithms
to consider. In Section 6.3, we give some speculation as to a spectral algorithm we expect to
matchtheaforementioned thresholdfordetectingcensoredspikedmatrices, andproposetherelated
Conjecture 6.4, a claim of a random matrix phase transition similar to those of [BBAP05, FP07].
Remark 3.29 (Censored stochastic block model). The previous work [SLKZ15] considered a cen-
sored version of the dense stochastic block model we mentioned in Remark 3.24. They consider the
dense Rademacher prior (s = 1) and a very high rate of censorship where η = 1 α for some
− n
constant α > 0, while the prior is the law of λxx for λ constant, without a factor of 1 . But,
X ⊤ √n
one may check that the factors of α and n may be moved around the components of the model: for
the Bernoulli channel = Ber(1 +x), the coordinate advantage in their model is identical to that
Px 2
of a model with the Rademacher prior as we defined it above (with the factor of 1 ) and with λ
√n
multiplied by √α. Thus, we immediately recover that LCDF have the same computational threshold
as proved there, which in our notation is λ > 1 . (For comparison, [SLKZ15] use a parameter
2√α
15ε which is ε = 1 λ, and write the threshold as α > 1 .) This problem is known, by their
2 − (1 2ε)2
−
results together with [HLM12, LMX15], not to have a statistical-to-computational gap, but once
again our previous reasoning gives that if we took a sparser community structure with s < 1, then
for sufficiently small s a gap would appear. And, once again, our tools immediately give analogous
results for observations in other discrete exponential families.
Remark 3.30 (Effective signal-to-noise ratio). More generally, when a prior has an SNR λ that
appears linearly as above, for a given channel censored at rate η, we may define the effective
P
SNR λ := (1 η)F λ. The coordinate advantage of the model then behaves like that of an
eff
− P ·
additive Gaussian model with SNR λ . It is an intriguing question what other aspects of a model
eff
p
can easily be “factored in” to an effective SNR in such generality.
We now proceed to the second operation of quantization of a channel’s output. We note that
we consider a coarse notion of quantization into just one bit of information; it would also be
interesting to consider quantizations on a finer grid (though the width of this grid would have to
be O(N 1/4) to obtain substantially different behavior from just taking the sign for additive and
−
P
priors satisfying Assumptions P1–P2).
Definition 3.31 (Quantization). Let be a channel with range Ω R so that each is absolutely
x
P ⊂ P
continuouswithrespect toLebesguemeasure. Wedefine the quantized versionof tobethe channel
P
sgn( ) with range 1,1 (the value zero may be mapped arbitrarily), where to sample from the
P {− }
measure sgn( ) we sample y and output sgn(y).
x x
P ∼ P
Theorem 3.32 (Quantized additive channels). Let be an additive noise channel as in Proposi-
P
tion 3.11, with underlying density p(y). If p satisfies the conditions of the Proposition, then sgn( )
P
satisfies Assumptions C1 and C2, and has Fisher information F = 4p(0)2.
sgn( )
P
Again, the result follows by a simple calculation, but immediately pinpoints computational thresh-
olds of quantized spiked matrix and tensor models in a simple way depending on the density of
additive noise.
Example 3.33 (Quantized additive Gaussian noise). Taking = (x,1), we find F =
x sgn( )
4( 1 )2 = 2, which is indeed smaller than F = 1. We find thatP , e.g., N the computational threP shold
√2π π P
for LCDF for a spiked matrix model with the channel sgn( (x,1)) is λ > 1/ F = π/2.
sgn( )
N P
And, again, we do not know what algorithms would match this threshold, bupt we conjecpture that
spectral algorithms should do so and make the corresponding Conjecture 6.5.
The expression 4p(0)2 is rather mysterious; it is not even obvious from it that quantizing
decreases the Fisher information, though it must by the aforementioned data processing inequality
of [Zam98]. Seeking to the maximize the quantized Fisher information for a given unquantized
Fisher information, we find the following surprising result. In words, it says that there are additive
noise models with smooth noise distributions so that quantization has an arbitrarily small effect
on computational thresholds for hypothesis testing!
Corollary 3.34. For every ε > 0, there exists a smooth and strictly positive probability density
p such that, if is the additive noise channel with this density, then (1 ε)F F
ε sgn( )
P − P ≤ P ≤
F . Moreover, the p may be chosen to converge pointwise to another positive (but not smooth)
ε
prP obability density, p (y) p (y) := 1exp( y ) as ε 0.
ε → 0 2 −| | →
We give a partial account of this paradoxical result in Section 6.4.
164 Latent variable models
As mentioned earlier, we will work at first over a more general model than the CLVMs of Defini-
tion 1.1. The following definition is more general in two important ways, and a third less material
one: first, it does not ask for the domain of to be continuous; second, it does not require y to
i
X
be independent of all x for j = i; and third, it allows for the y to have different laws for different
j i
6
i under the null model Q and to be observed through different channels under the planted model
P. We hope this more abstract formulation will be useful for future work.
Definition 4.1 (Latent variable model). Let Σ,Ω be measurable spaces and N 1. Let be a
≥ X
probability measure over ΣN, ,..., probability measures over Ω, and, for each x Σ and
1 N
Q Q ∈
i [N], let be a probability measure over Ω. A latent variable model (LVM) specified by these
i,x
∈ P
objects ( , , ) consists of the following pair of probability measures over ΩN:
X Q P
1. Sample y Q by sampling y for each i [N] independently (i.e., Q is the product
i i
∼ ∼ Q ∈
measure Q = ).
1 N
Q ⊗···⊗Q
2. Sample y P by first sampling x . Then, sample y for each i [N] independently
i i,x
∼ ∼ X ∼ P ∈
(i.e., P is the mixture of product measures over x ).
1,x N,x
P ⊗···⊗P ∼ X
Definition 4.2 (GoodLVM). We call an LVM good if, for all x Σ and i [N], is absolutely
i,x
∈ ∈ P
continuous with respect to and d /d L2( ).
i i,x i i
Q P Q ∈ Q
Remark 4.3 (Sideinformation). We will not explore it further here, butone interesting setting that
may be described if we let the channels vary over different coordinates is sideinformation, where we
either reveal some coordinates of x directly, or give in addition to y a vector correlated entrywise
with x. See [SN18] for an example in the context of the stochastic block model, or [DSMM18] for a
“contextual” versionofthe model, whichamounts toincluding alessdirect form ofside information.
Over this broader class of models, we will prove the following result, which is a generalization
of Theorem 3.5.
Theorem 4.4 (Coordinate advantage for LVMs). Suppose ( , , ) is a good LVM. Define the
X Q P
vector channel overlap
d d
R (x(1),x(2)) := E Pi,x(1) (y) 1 Pi,x(2) (y) 1 . (20)
, ,i
QP y ∼Qi(cid:20)(cid:18) d Qi − (cid:19)(cid:18) d Qi − (cid:19)(cid:21)
Then,
CAdv( , , )2 = E R (x(1),x(2)) (21)
, ,i
X Q P x(1),x(2) QP
∼XT X⊆[N]i Y∈T
T D
| |≤
N
E exp ≤D R
,
,i(x(1),x(2)) . (22)
≤ x(1),x(2) QP !
∼X Xi=1
4.1 Coordinate decomposition of likelihood ratio
We will use the background results on coordinate or Efron-Stein decompositions developed in
Appendix A. Recall that the optimizer in the coordinate advantage is the LCDLR, the orthogonal
projection of the likelihood ratio to V , so it suffices to understand this object. Lemma A.7
D
≤
describes this projection (in a setting generalizing LVMs) in terms of the marginal likelihood ratios.
17We first introduce notation for the coordinate-wise likelihood ratios,
d
i,x
L := P . (23)
i,x
d
Q
Because y Q has independent coordinates and, conditional on x, y P also has independent
∼ ∼
coordinates, the likelihood ratio in an LVM may be computed as
N N
dP dP d
x i,x
L(y) := (y) = E (y) = E P (y )= E L (y ). (24)
i i,x i
dQ x dQ x d i x
∼X ∼X i=1 Q ∼X i=1
Y Y
Following Appendix A, the marginal likelihood ratios are then
dP
T
L (y) := E L(y) = E L (y )= (y), (25)
T i,x i
yi ∼Qi:i ∈[N] \T x
∼X i Y∈T
dQ
T
where P and Q are the measures arising from an LVM formed by restricting the given LVM to
T T
the indices T.
Appendix A further describes a decomposition of V into mutually orthogonal subspaces V
D T
≤
over subsets T [N] with T D, and by Lemma A.7, the projection of L to V is
T
⊆ | | ≤
b
L T(y) := ( 1) |T |−|S |L S = E ( 1) |T |−|S | L i,x(y i)= E (L i,xb(y i) 1). (26)
− x − x −
S T ∼XS T i S ∼X i T
X⊆ X⊆ Y∈ Y∈
b
Indeed, by inclusion-exclusion it follows readily that L = L , and we detail the orthogo-
T [N] T
nality of this decomposition in Appendix A.4. The LCDLR, th⊆e projection of L to V , which we
D
P ≤
will denote L , is then b
D
≤
L = L = E (L (y ) 1). (27)
D T i,x i
≤ x −
T [N] T [N] ∼Xi T
X⊆ X⊆ Y∈
T D b T D
| |≤ | |≤
4.2 Coordinate advantage: Proof of Theorems 3.5 and 4.4
We will be ready to proceed to the main proof after one more preliminary step.
Lemma 4.5 (Non-negativity). For any k 0 integers for i [N], we have
i
≥ ∈
N
E R (x(1),x(2))ki 0. (28)
, ,i
x(1),x(2) QP ≥
∼XYi=1
Proof. We will just use that R is a product of inner products in , which may be viewed as
, ,i i
a single inner product in Q = QP , of the same function ofQ x(1) and x(2). Explicitly, we
1 N
Q ⊗···⊗Q
may manipulate
N
E R (x(1),x(2))ki
, ,i
x(1),x(2) QP
∼XYi=1
N ki
= E E (L (y ) 1)(L (y ) 1)
x(1),x(2) ∼XYi=1(cid:18)yi ∼Qi
i,x(1) i
−
i,x(2) i
− (cid:19)
18and writing a power of an expectation as an expectation of a product of independent copies,
N ki
(j) (j)
= E E (L (y ) 1)(L (y ) 1)
x(1),x(2) ∼Xy i(1),...,y i(ki) ∼QiYi=1j Y=1 i,x(1) i − i,x(2) i −
foreachi [N]
∈
2
N ki
(j)
= E E (L (y ) 1)
y i(1),...,y i(ki)
∼Qix
∼X Yi=1j Y=1
i,x(1) i − 
foreachi [N]
∈  
0, (29)
≥
completing the proof.
Proof of Theorem 4.4. Recall that the coordinate advantage is the norm in L2(Q) of the LCDLR,
which, by (27) and the orthogonality of the L , is
T
CAdv ( , , )2 = Eb L (y)2 = E L (y)2. (30)
D D T
≤ X Q P y Q ≤ y Q
∼ T [N] ∼
X⊆
T D b
| |≤
Rewriting a squared expectation as an expectation of a product of two independent copies (as in
the approach to the polynomial advantage of [KWB22]),
E L (y)2 = E L (y)2
T T
y Q y Q
∼ ∼
2
b b
= E E L (y ) 1
i,x i
y Q x − !
∼ ∼Xi T
Y∈ (cid:0) (cid:1)
= E E L (y ) 1 L (y ) 1
x(1),x(2) y Q
i,x(1) i
−
i,x(2) i
−
∼X ∼ i Y∈T
(cid:0) (cid:1)(cid:0) (cid:1)
= E R (x(1),x(2)), (31)
, ,i
x(1),x(2) QP
∼Xi Y∈T
where x(1),x(2) are independent draws. Summing over T we find the first form of our result,
∼ X
CAdv ( , , )2 = E R (x(1),x(2))
D , ,i
≤ X Q P x(1),x(2) QP
∼XT X⊆[N]i Y∈T
T D
| |≤
and to obtain the bound we claim, note that the extra terms introduced when expanding the
following expression by the binomial theorem are all non-negative upon taking the expectation
over x(1),x(2) by Lemma 4.5:
d
D N
1
E R (x(1),x(2))
, ,i
≤ x(1),x(2) d! QP !
∼XXd=0 Xi=1
N
= E exp ≤D R
,
,i(x(1),x(2)) , (32)
x(1),x(2) QP !
∼X Xi=1
completing the proof.
195 Channel universality
5.1 Loose universality: Proof of Theorem 3.9
We now work under the more specific CLVM setting (Definition 1.1). We define notation for the
likelihood ratios associated to the channel and recall the definition of the channel overlap, which
P
may be rephrased in terms of these:
d
x
L := P , (33)
x
d
0
P
d d
R (x(1),x(2))= E Px(1) (y) 1 Px(2) (y) 1
P y ∼P0(cid:20)(cid:18) d P0 − (cid:19)(cid:18) d P0 − (cid:19)(cid:21)
= E [(L (y) 1)(L (y) 1)]
y 0
x(1)
−
x(2)
−
∼P
= E [L (y)L (y)] 1. (34)
y 0
x(1) x(2)
−
∼P
Let us first clarify the connection between our definition of the Fisher information and the more
conventional one.
Proposition 5.1 (Fisher information). Let be a channel so that, for each y Ω, L (y) is 1 in
x
P ∈ C
x in a neighborhood of x = 0. Then,
∂2R ∂ 2
F = P (0,0) = E L x(y) . (35)
P ∂x(1)∂x(2) y ∼P0(cid:18)∂x (cid:12)x=0(cid:19)
(cid:12)
This follows just by differentiating under the expectation twice. (cid:12)
(cid:12)
We may also compute some other partial derivatives of R directly.
P
Proposition 5.2. Suppose that R is 4 in a neighborhood of the origin. Then,
P C
R (0,0) = 0, (36)
P
∂aR ∂aR
P (0,0) = P (0,0) = 0 for all a 1,2,3,4 . (37)
∂x(1)a ∂x(2)a ∈ { }
Proof. The first claim follows because L = d /d = 1. The second claim follows because
0 0 0
P P
R (x,0) = E L (y) 1 = E 1 1 = 0 is a constant.
P y ∼P0 x − y ∼Px −
The following bound on the truncated exponential polynomials, which is elementary but not
trivial to show, will play an important role.
Proposition 5.3. Let D 2 be even. Then, for all x R, we have
≥ ∈
0< exp D(x) exp(x ), (38)
≤
≤ | |
and, for all x,y R, we have
∈
exp D(x)
≤ exp D(x+y) exp D(x)exp(100y ). (39)
≤ ≤
exp(100y ) ≤ ≤ | |
| |
We have not attempted to optimize the constant 100. We defer the proof to Appendix C.3. Intu-
itively, the latter inequality is a variation on the exponential identity exp(x+y) = exp(x)exp(y);
one may check that the direct analog exp D(x+y) exp D(x)exp D(y) unfortunately does not
≤ ≤ ≤
≤
hold in general.
20Proof of Theorem 3.9. The main idea is to approximate R (x(1),x(2)) F x(1)x(2). Thus, let us
P ≈ P
define the remainder in this approximation:
∆(x(1),x(2)) := R (x(1),x(2)) F x(1)x(2). (40)
P − P
We first bound this remainder. By Assumption C1, R is 4 in an open set U [ A,A]2.
P C ⊃ −
Thus, consider the partial derivatives of R at the origin up to order 3. By Assumption C2 and
P
Proposition 5.2, the only non-zero one of these derivatives is the mixed second partial derivative,
whose value is F by definition. So, by the multivariate Taylor theorem with the Lagrange form
of the remainder,P for each (x(1),x(2)) [ A,A]2, there exist some z(1),z(2) [ A,A] such that
∈ − ∈ −
1 ∂4R
∆(x(1),x(2)) = P (z(1),z(2))x(1)k x(2)ℓ . (41)
k!ℓ!∂x(1)k∂x(2)ℓ
k,ℓ 1
k+X ℓ≥=4
Define the following, which is finite because R is 4 on U:
P C
∂4R
K = K( ) := max max P (z(1),z(2)) . (42)
P kk +,ℓ ℓ≥=1 4z(1),z(2) ∈[ −A,A]2 (cid:12) (cid:12)∂x(1)k∂x(2)ℓ (cid:12)
(cid:12)
(cid:12) (cid:12)
We may bound (cid:12) (cid:12)
∆(x(1),x(2)) K x(1) k x(2) ℓ (43)
| | ≤ | | | |
k,ℓ 1
k+X ℓ≥=4
Now, we consider the sum of overlaps appearing in the upper bound of Theorem 3.5:
N N
R (x(1) ,x(2) ) =F x(1),x(2) + ∆(x(1),x(2)). (44)
P i i Ph i
i=1 i=1
X X
:=∆(x(1),x(2))
We may bound this summed error term by | {z }
N
∆(x(1),x(2)) ∆(x(1) ,x(2) )
| | ≤ | i i |
i=1
X
N
K x(1) k x(2) ℓ
≤ | i | | i |
k,ℓ 1 i=1
k+X ℓ≥=4X
and by the Cauchy-Schwarz inequality,
K x(1) k x(2) ℓ
≤ k k2kk k2ℓ
k,ℓ 1
k+X ℓ≥=4
and by Assumption P2, when x(1),x(2) , almost surely we have
∼ X
≤
K B 2k kB 2ℓ ℓN1 −k+ 4ℓ
k,ℓ 1
k+X ℓ≥=4
= K Bk Bℓ
2k 2ℓ
k,ℓ 1
k+X ℓ≥=4
:= C (45)
21which is a constant C = C( ) depending only on ,B ,B ,B . In summary, we have found that,
2 4 6
P P
almost surely when x(1),x(2) , we have
∼ X
N
R (x(1) ,x(2) ) F x(1),x(2) C. (46)
(cid:12) P i i − Ph i(cid:12) ≤
(cid:12)Xi=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Recall that we must prove both upper and lower bounds on the coordinate advantage. The
(cid:12) (cid:12)
upper bound follows by plugging (46) into the upper bound of Theorem 3.5 and then applying
Proposition 5.3.
For the lower bound, we first make a general calculation giving a lower bound counterpart to
Theorem 3.5. For the sake of conciseness, in an expectation over x(1),x(2), let us write R :=
i
(1) (2)
R (x ,x ). We have:
i i
P
N N N
E exp ≤D R
i
−
R i2 exp ≤D −2 R
i
x(1),x(2) " ! ! !#
∼X Xi=1 Xi=1 Xi=1
d d 2
N D 1 N 1 N N −
= 1+ E R + E R R2 R
i d! i − (d 2)! i i 
x(1),x(2) x(1),x(2) ! ! !
∼XXi=1 Xd=2 ∼X Xi=1 − Xi=1 Xi=1
 
and here, expanding the powers inside, the coefficient of a given N Rki with some k 2 will be
i=1 i i ≥
at most 1 ki! 0. Thus, together with Lemma 4.5, we find
k1! kN! − k1! kN! ≤ Q
··· ···
N D
1+ E R + E R
i i
≤
x(1),x(2) x(1),x(2)
∼XXi=1 Xd=2 ∼XT X⊆[N]i Y∈T
T =d
| |
= CAdv ( , ). (47)
D
≤ X P
by the formula of Theorem 3.5. By (46), we have almost surely when x(1),x(2) that
∼ X
N N
R2 = R (x(1) ,x(2) )2
i i i
P
i=1 i=1
X X
N N
2F2 x(1)2 x(2)2 + ∆(x(1) ,x(2) )2
≤ P i i i i
i=1 i=1
X X
N
2F2 x(1) 2 x(2) 2+3K2 x(1) 2k x(2) 2ℓ
≤ Pk k4k k4 | i | | i |
k,ℓ 1 i=1
k+X ℓ≥=4X
2F2 x(1) 2 x(2) 2+3K2 x(1) 2k x(2) 2ℓ, (48)
≤ Pk k4k k4 k k4kk k4ℓ
k,ℓ 1
k+X ℓ≥=4
and bounding these norms with the assumptions on as above shows that this bounded by a con-
X
stantdependingonlyon . TheproofofthelowerboundisthencompletedbyusingProposition5.3
P
twice more.
Remark 5.4 (Variable channels). It is straightforward to extend the result to allow for different
channels per coordinate, with different Fisher informations F , so long as the fourth mixed
Pi Pi
22partial derivatives of the associated channel overlaps R are uniformly bounded on an open set
i
containing [ A,A], where A is as in Assumption P1 onP . In this case, F x(1),x(2) would be
replaced by x− (1)⊤Fx(2) for F the diagonal matrix of the F X . More generally, P ith seems plai usible that
i
P
similar results should hold even if y can depend on all of x rather than just x , so that the channels
i i
depend on the entire vector x (as in a general LVM in the setting of Theorem 4.4). The overlap
i,x
P
expression would then involve the Fisher information matrix of the channel, part of the Hessian
matrix of the now multivariate analog of R . We do not pursue this here as it would require more
P
complicated conditions on the prior and would make our calculations more elaborate.
X
5.2 Tight universality: Proof of Theorem 3.17
The following is immediate from the definition of dilution.
Proposition 5.5 (Overlap invariance). For any and any k 1, the law of x(1),x(2) is the
X ≥ h i
same for x(1),x(2) independently as for x(1),x(2) independently.
k
∼ X ∼ D X
We will also use the following simple combinatorial fact, proved in Appendix C.4.
Proposition 5.6. Suppose t k/2. Then, k kt exp( t2 ).
≤ t ≥ t! −k
Proof of Theorem 3.17. Let us first sketch t(cid:0)he(cid:1)proof ideas. The simple main idea is that dilution
makes the original upper bound on the coordinate advantage from Theorem 3.5 (not involving
universality) close to tight. This is a general property of the truncated exponential polynomials
exp D and does not rely on any special properties of the channel or prior. We then note that the
≤
proof of Theorem 3.9 (showing loose channel universality) really proceeded by showing that this
bound behaves universally. Thus, if the boundis close to tight, then the coordinate advantage itself
also behaves universally.
At a technical level, Theorem 3.9 already proves the upper bound on the coordinate advantage
that we need, so it suffices to show a matching lower bound. Note that, under our assumption,
k D2 D. We have:
≥ ≥
CAdv ( , )= E R (x1,x2)
≤D Dk X Px xe1,xe2
∼DkXT ⊆X[kN]i Y∈T
P i i
T D
| |≤ e e
N k x1 x2 ti
= E R i , i
x1,x2 ∼XtXNN Yi=1(cid:18)t i(cid:19) P (cid:18)√k √k (cid:19)
t∈ D
| |≤
where t := N t and we should also constrain t k, but since k D this is vacuous. Using
| | i=1 i | i | ≤ ≥
Proposition 5.6 and that t D while k D2,
i
P ≤ ≥
1 N 1 x1 x2 ti
E kR i , i
≥ e x1,x2
∼XtXNN Yi=1
t i!
(cid:18)
P (cid:18)√k √k
(cid:19)(cid:19)
t∈ D
| |≤
1 D 1 d x1 x2 ti
= E kR i , i
e x1,x2 ∼XXd=0 d! tXNN(cid:18)t 1 ···t N(cid:19)(cid:18) P (cid:18)√k √k (cid:19)(cid:19)
∈t=d
| |
23and by the multinomial theorem,
d
1 D 1 N x1 x2
= E k R i , i
e x1,x2 d! P √k √k !
∼XXd=0 Xi=1 (cid:18) (cid:19)
1 N x1 x2
= E exp ≤D k R i , i
e x1,x2
∼X Xi=1
P (cid:18)√k √k (cid:19)!
and the argument of Theorem 3.9, repeated mutatis mutandis, noting that powers of k cancel in
the Taylor expansion of R , gives
P
C E exp D F x1,x2
≥ x1,x2 ≤ Ph i ii
∼X
= C xe1,xe2E ∼DkXexp ≤(cid:0)D F Phx1 i,x(cid:1)2
ii
(49)
(cid:0) (cid:1)
for C a constant depending only on the channel and tehe ebounds on the prior, completing the
proof.
Finally, let us elaborate on the Introduction and give some heuristic discussion of why dilute
models might behave like Gaussian ones. Recall that the Cram´er-Rao inequality gives a limitation
on the variance of estimators of x from y in CLVMs (see, e.g., Chapter 5 of [Pit79]).
Proposition 5.7 (Cram´er-Rao inequality). In a CLVM, let f : Ω Σ be a measurable function
→
such that µ (x) := E f(y) is 1. Then, Var f(y) µ (0)2/F .
f y ∼Px C y ∼P0 ≥ ′f P
Usually one defines an entire Fisher information function F (x) analogously to our definition, from
P
which one obtains similar lower bounds on the variance of any estimator under any . We also
x
P
mention the following ancillary result, to clarify the discussion in Remark 3.15.
Corollary 5.8. Suppose that the variance of is σ2, and the are parametrized such that the
0 x
P P
mean of is x. Then, F 1/σ2.
x
P P ≥
Proof. In Proposition 5.7, take f(y)= y, which has µ (x) = x by the assumption.
f
We may produce a function that, at least locally near x = 0, is an unbiased estimator and
saturates the inequality. In a CLVM, consider the function
1 ∂
f(y)= L (y) , (50)
x
F ∂x
(cid:12)x=0
P
(cid:12)
sometimes called the Fisher score. Like with the Fisher (cid:12)information, though, the Fisher score can
(cid:12)
be evaluated at arbitrary x while we are interested only in evaluating it at x = 0, so we call f(y)
the local Fisher score instead. For example, the nonlinearity used in the pretransformed eigenvalue
test of Proposition 3.21 is the special case of the local Fisher score for additive noise models. The
following is one conceptual justification for using the local Fisher score.
Remark 5.9 (Approximatemaximumlikelihood). One interpretation of the local Fisher score is as
an approximation of the maximum likelihood estimator of x. Indeed, consider the log-likelihood ratio
ℓ (y) := logL (y). One may check, under mild regularity conditions, that an alternative formula
x x
for the Fisher information is F = ∂2 ℓ (y) , and so f(y) = ∂ ℓ (y) / ∂2 ℓ (y) .
P
−∂x2 x |x=0 −∂x x |x=0 ∂x2 x |x=0
This means that f(y) gives the result of taking one step of Newton’s method for maximizing the
log-likelihood in x, starting from x = 0.
24The local Fisher score is “locally unbiased” near x = 0, in the sense that
1 ∂R
E f(y)= P (0,0) = 0, (51)
y 0
F ∂x(1)
∼P P
∂ 1 ∂2R 1
E f(y) = P (0,0) = F = 1, (52)
∂x y ∼Px (cid:12)x=0 F
P
∂x(1)∂x(2) F
P
· P
(cid:12)
(cid:12)
so E
y
∼Pxf(y)
≈
x to leading order (cid:12)for small x. And, we have
2
1 ∂ 1 1
Var f(y)2 = E f(y)2 = E L (y) = F = , (53)
y ∼P0 y ∼P0 F P2 y ∼P0(cid:18)∂x x (cid:12) (cid:12)x=0(cid:19) F P2 · P F
P
(cid:12)
so f(y) saturates the Cram´er-Rao inequality and may be viewed as a “minimum variance locally
(cid:12)
unbiasedestimator”(parallelingthemorecommonnotionofminimumvarianceunbiasedestimator,
which asks for exact unbiasedness over all x).
For small x, by continuity, we expect the variance of the local Fisher score over to still
x
P
be Var f(y)2 = 1/F +O(x). Turning to the application to diluted models, suppose we have
y x
∼P P
access to y ,...,y that are i.i.d. By the above observations, the random variable
1 k
∼
Px/√k
k k
1 1 x
f(y ) = x+ f(y ) (54)
i i
√k √k − √k
i=1 i=1(cid:18) (cid:19)
X X
will, for large k, have law close to (x,1/F ) by the central limit theorem. (As in Remark 5.9,
N P
this estimator is approximating the maximum likelihood estimate of x from the samples y , and
i
this limit is comparable to the asymptotic normality of the maximum likelihood estimator.) In this
way, it is always possible to reduce dilute observations through an arbitrary channel to less dilute
observationsthroughan(approximately)additiveGaussianchannel. BytheCram´er-Raoinequality,
the variance 1/F is optimal, in the sense that there is no transformation f that through the same
P
operations yields approximately Gaussian observations with mean x and with smaller variance (as
also verified by hand in a special case using the calculus of variations by [PWBM18]).
6 Applications
6.1 Spiked matrix models: Proof of Corollary 3.22
We will use the following result coming from the analysis of a Gaussian spiked matrix model.
Proposition 6.1 (Theorem 3.9 of [KWB22]). Under the assumptions of Corollary 3.22, for any
D = D(n)= o(n/logn),
λ2
E exp D x(1)x(1)⊤ ,x(2)x(2)⊤ = O(1). (55)
≤
2nh i
x(1),x(2) π⊗n
(cid:18) (cid:19)
∼
This appears as a boundon the polynomial advantage through an additive Gaussian channel where
the variance of the diagonal noise is exactly twice the variance of the off-diagonal noise, so that the
symmetric noise matrix is drawn from the Gaussian orthogonal ensemble.
Proof of Corollary 3.22. The upper bound on the coordinate advantage follows from our Theo-
rem 3.9 and Proposition 6.1, as follows. Note that under our assumptions, satisfies Assump-
n
X
tions P1 and P2 of Theorem 3.9 and (the additive noise channel for a noise density p) satisfies
P
25Assumptions C1 and C2 by Proposition 3.11. Let K be a number such that x π has x K
∼ | | ≤
almost surely. Let us write X(1),X(2) for two independent draws from the prior described in
n
the statement, so that X(i) contains th∼ eX upper triangle of λ x(i)x(i)⊤ for x(i) having i.i.d. entries
√n
drawn from π. We assume without loss of generality that D is even. Then, Theorem 3.9 gives, for
a constant C depending only on the channel and the bounds on the prior,
1
CAdv ( , ) C E exp D X(1),X(2)
D n ≤
≤ X P ≤ X(1),X(2)
∼Xn
(cid:18)F Ph i
(cid:19)
λ2 n
= C E exp ≤D
2nF
hx(1)x(1)⊤ ,x(2)x(2)⊤
i−
|x( i1) |2 |x( i2) |2
x(1),x(2) π⊗n !!
∼ P Xi=1
λ2 λ2
C E exp ≤D x(1)x(1)⊤ ,x(2)x(2)⊤ + K4 (56)
≤ 2nF h i 2F
x(1),x(2) ∼π⊗n
(cid:18) P P (cid:19)
and using Proposition 5.3 together with Proposition 6.1 shows that this is O(1) so long as λ <
1/√F , as claimed.
P
For the lower bound on the coordinate advantage when λ > 1/√F , we note that we cannot
P
use the lower bound from Theorem 3.9, which would give a threshold for λ off by a constant factor.
We could appeal to Theorem 3.17, but this would require dilution of the prior. Instead, we make
a more hands-on argument, producing an explicit function of low coordinate degree that witnesses
that the coordinate advantage is large. Namely, let f(y) := p(y)/p(y) be the local Fisher score
′
−
as in Proposition 3.21, and define for a symmetric matrix Y
D
1
g(Y) := Tr f(Y) , (57)
√n
!
(cid:18) (cid:19)
where f(Y) is applied entrywise. By expanding the trace, we see that this has cdeg(g) D. We
≤
have assumed D = ω(logn), but let us also assume that D < n; this is without loss of generality,
since the coordinate advantage is increasing in D.
We now control the first moment of this under P and the second moment under Q. We have,
by Theorem 4.8 of [PWBM18],
D
1
E g(Y) E λ f(Y)
max
Y P ≥ Y P √n
∼ ∼ (cid:18) (cid:19)
D
1
(1 o(1)) λF +
≥ − P λ
(cid:18) (cid:19)
D
(1 o(1)) 2 F +ε , (58)
≥ − P
for some ε > 0 depending on λ. And, we have (cid:16) p (cid:17)
2
D
1
E g(Y)2 = E Tr f(Y)
Y Q Y Q √n !!
∼ ∼ (cid:18) (cid:19)
2D
1
n E Tr f(Y)
≤ Y Q √n !
∼ (cid:18) (cid:19)
and here, noting that f(Y) is a Wigner matrix with bounded i.i.d. entries that are centered and
have variance F , standard combinatorial analysis of Wigner matrices following [FK81] (see, e.g.,
P
Section 2.1.6 of [AGZ10]) gives, for D < n,
2n2(2 F )2D. (59)
≤ P
p
26Thus, the coordinate advantage is by definition bounded below as
E g(Y) (1 o(1)) ε D
Y P
CAdv ≤D( Xn, P)
≥ E
Y∼
Qg(Y)2 ≥
−
2n2
(cid:18)1+
2√F
(cid:19)
, (60)
∼ P
and the result follows since D = ω(lopgn), so this diverges as D .
→ ∞
Remark 6.2. By similar arguments one may allow for the diagonal of x(i)x(i)⊤ to be included in
X(i), or, using the more general bound outlined in Remark 5.4, for the diagonal to be included with
additive noise having twice the variance, so that the noise matrix is a Wigner matrix with the usual
scaling, or for that matter for the diagonal to be included with any other bounded noise distribution.
6.2 Spiked tensor models: Proof of Corollary 3.26
We will need the following slight elaboration on the second claim of Proposition 3.25, which is
obtained from examining the proofs of [KWB22, Kun21b]. We give more details in Appendix C.5.
Proposition 6.3. Let = (x,1), let be as in Proposition 3.25, and let K > 0. Then, there
x
P N X
exists b > 0 depending on K such that, if λ b D (q 2)/4 and D is even with 2 D 2n, then
q ≥ q − − ≤ ≤ q
Adv ( , )
D n
≤ X P K. (61)
Adv ( , ) ≥
D 2 n
≤ − X P
Proof of Corollary 3.26. The first claim follows immediately by using the upper bound of Theo-
rem 3.9 on the coordinate advantage and applying the first claim of Proposition 3.25:
CAdv ( , )2 C Adv ( , (0,1/F ))2, (62)
D 1 D
≤ X P ≤ ≤ X N P
where if a is the constant from the Proposition, then taking a := a /√F absorbs the extra
q q, q
P P
factor of F from the right-hand side.
P
For the second claim, we use the lower bound of Theorem 3.9 and Proposition 6.3: taking b
q,
sufficiently large, by the Proposition we will have for all even D 2n that P
≤ q
CAdv ( , )2 C Adv ( , (0,1/F ))2 C Adv ( , (0,1/F ))2
D 2 D 3 D 2
≤ X P ≥ ≤ X N P − ≤ − X N P
C
2Adv ( , (0,1/F ))2
D
≥ 2 ≤ X N P
C
2 KD. (63)
≥ 2
We may then remove the constraint that D is even by observing that the coordinate advantage is
monotonically increasing in D.
6.3 Censorship: Proof of Theorem 3.28
The proof will follow by a simple calculation.
Proof of Theorem 3.28. First, the channel likelihood ratio after censorship is given by:
d 1 if y = ,
LCxη(y) := Cη Px (y) = • (64)
d L (y) otherwise.
Cη P0 (cid:26) x
We note that this does not depend on η. And, the channel overlap is:
R (x(1),x(2))= E (LCη (y) 1)(LCη (y) 1) = (1 η)R (x(1),x(2)). (65)
Cη P y η 0 x(1) − x(2) − − P
∼C P
Clearly this scaling does not affect Assumptions C1 and C2, and the Theorem is proved.
27We described in the Introduction the application of this result to spiked matrix models. This
gives a computational lower bound, but it is not obvious what a concrete matching polynomial
time algorithm would be.
There is, however, a natural guess, extending the strategy of [PWBM18] to non-additive noise
and as predicted by [LKZ15a]. Namely, per our discussion in Section 5.2, intuitively we should
expect the entrywise local Fisher score function f(y) = ∂ L (y) to roughly map general noise
∂x x |x=0
models to additive Gaussian models. Computing this for the censored likelihood ratio appearing
above, we find that f( ) = 0 while f(y) for y = is the same as the local Fisher score in the
• 6 •
uncensored model. We thus arrive at a natural strategy for working with censored data in CLVMs:
set the censored entries to zero and proceed as though given an observation from an uncensored
model. For spiked matrix models, we reach the following conjecture in random matrix theory,
which is simple to verify numerically but seems non-trivial to attack using standard techniques as
in, e.g., [FP07, CDMF09], since the additive structure of the model has been corrupted.
Conjecture 6.4. Let x be random as in Assumption 3.20, p(y) a density as in Proposition 3.11,
P
the corresponding additive noise channel, η (0,1), and define f(y) := p(y)/p(y). Write f(Y)
′
∈ −
for the entrywise application of f to amatrix Y. Let Y(0) := λ xx +W forW = W i.i.d.
√n ⊤ ij ji ∼ P0
(i.e., drawn with density p), and W = 0. Let Y be formed by replacing every entry in the upper
ii
triangle of Y(0) with zero independently with probability η, and repeating the same replacements
symmetrically in the lower triangle. We conjecture that there exists some γ R depending only on
∈
p and η such that:
1. If λ < 1/ (1 η)F , then 1 λ (f(Y)) γ in probability.
− P √n max →
2. If λ > 1p / (1 η)F , then there is ε = ε(λ) > 0 such that 1 λ (f(Y)) γ + ε in
− P √n max →
probability.
p
6.4 Quantization: Proof of Theorem 3.32
We may prove our main result on quantization immediately.
Proof of Theorem 3.32. The result again follows by straightforward calculations. The channel like-
lihood ratio Lsgn :=dsgn( ) /dsgn( ) takes values
x x 0
P P
[sgn(y) = 1]
Lsgn(1) = Px = 2 ∞ p(y)dy, (66)
x [sgn(y) = 1]
P0 Z−x
[sgn(y) = 1] x
Lsgn( 1) = Px − = 2 − p(y)dy, (67)
x − [sgn(y) = 1]
0
P − Z−∞
wherethelatter formulasfollow sinceweassumethatpissymmetric, sothatsgn( ) = Unif( 1 )
0
P {± }
and both denominators are 1. The channel overlap is then:
2
R (x(1),x(2)) = E (Lsgn (y) 1)(Lsgn (y) 1)
P y sgn( )0 x(1) − x(2) −
∼ P
x(1) x(2)
∞ ∞ − −
= 2 p(y)dy p(y)dy +2 p(y)dy p(y)dy (68)
(cid:18)Z−x(1) (cid:19)(cid:18)Z−x(2) (cid:19) Z−∞ ! Z−∞ !
The Fisher information is then readily computed by differentiating with Leibniz’s rule:
F = 4p(0)2, (69)
P
concluding the proof.
28As with censorship, it is plausible that a spectral algorithm is optimal for quantized additive
noise models. This leads to another interesting random matrix theory conjecture, which is again
readily verified numerically but seems outside of the reach of standard proof techniques.
Conjecture 6.5. Let x be random as in Assumption 3.20, p(y) a density as in Proposition 3.11,
and the corresponding additive noise channel. Let Y(0) := λ xx +W for W = W
P √n ⊤ ij ji ∼ P0
(0)
i.i.d. (i.e., drawn with density p), and W = 0. Let Y have entries Y := sgn(Y ). We conjecture
ii ij ij
that there exists some γ R depending only on p such that:
∈
1. If λ < 1/(2p(0)), then 1 λ (Y) γ in probability.
√n max →
2. If λ > 1/(2p(0)), then there is ε= ε(λ) >0 such that 1 λ (Y) γ+ε in probability.
√n max →
Similar models are treated by the recent work [GKK+23], but those results would only apply if the
function sgn(y) in our setting were replaced by a smoother one.
We mention the computations with the calculus of variations that lead to the special densities
presented in Corollary 3.34 as well; we do not give a formal proof, which follows by straightforward
calculus. Instead of maximizing the quantized Fisher information, let us fix it (in other words,
fix p(0)), and minimize the unquantized Fisher information subject to this constraint. Suppose
p′(y)2
then that p(x) is a probability density that minimizes the Fisher information F[p] = ∞ dy
p(y)
−∞
viewed as a functional of p, subject to a fixed value of p(0) = c. Then, it must be that the first
R
variation of F[p] is zero, i.e., for any smooth, compactly supported, symmetric δ(y) with δ(0) = 0,
∞ δ(y)dy = 0, and δ(y) < ε for sufficiently small ε (so that adding δ to p does not change p(0)
| |
a−nd∞leaves p a probability density), we must have that F[p+δ] = F[p] to leading order. Expanding
R
this, we have
(p(y)+δ (y))2
∞ ′ ′
F[p+δ] = 2 dy
p(y)+δ(y)
Z0
1 δ(y)
2 ∞ (p(y)2+2p(y)δ (y)) + dy
′ ′ ′
≈ p(y) p(y)2
Z0 (cid:18) (cid:19)
2
p(y) p(y)
∞ ′ ′
F[p]+2 δ(y) 2 δ (y) dy. (70)
′
≈ p(y) − p(y)
Z0 (cid:18) (cid:19) !
The only way this can hold for all δ satisfying our conditions is if p(y)/p(y) is constant, which
′
means that p(y) = cexp( 2cy ), a Laplace or two-sided exponential distribution. The densities
− | |
featuring in Corollary 3.34 may be obtained by, say, convolving this density with a Gaussian of
variance ε to obtain a sequence of smooth approximations.
Lastly, havingperformedthisderivation,letusretrospectively givesomeintuitionastowhythis
noise density might beespecially resilient to quantization. Recall from the discussion in Section 5.2
that in such a non-Gaussian additive model, it is sensible to apply the entrywise transformation of
the local Fisher score
1 ∂ 1 p(y)
′
f(y)= L (y) = (71)
x
F ∂x −F p(y)
(cid:12)x=0
P P
(cid:12)
which approximates a minimum variance unbiased(cid:12)estimator of x from y. But, if we apply this to
(cid:12)
p(y) =cexp( 2cy ), then we get (up to constants) precisely thefunction sgn(y)(and if we consider
− | |
a smooth approximation of such p(y), we will have a smooth approximation of the sign function).
Thus taking the sign of the observations is a transformation we would want to perform anyway on
data with this noise distribution, and quantization is relatively benign.
29Acknowledgments
I thank the participants of the BIRS Workshop on Computational Complexity of Statistical Infer-
enceforthoughtfulcommentsfollowingapreliminarypresentation oftheseresults,especially Aaron
Potechin for pointing out a flaw in an attempted generalization of the results of Appendix A, and
Florent Krzakala and Alex Wein for clarifying aspects of the papers [LKZ15a, PWBM18]. I also
thank Rayan Saab for bringing the topic of quantization to my attention.
References
[Abb17] Emmanuel Abbe. Community detection and stochastic block models: recent devel-
opments. The Journal of Machine Learning Research, 18(1):6446–6531, 2017.
[ACO08] Dimitris Achlioptas and Amin Coja-Oghlan. Algorithmic barriers from phase tran-
sitions. In 49th Annual IEEE Symposium on Foundations of Computer Science
(FOCS 2008), pages 793–802. IEEE, 2008.
[AGZ10] Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random
matrices. Cambridge University Press, 2010.
[AMK+18] Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, Lenka Zde-
borov´a, etal. Thecommittee machine: Computationaltostatistical gaps inlearning
a two-layers neural network. Advances in Neural Information Processing Systems,
31, 2018.
[BAH+22] Afonso S Bandeira, Ahmed El Alaoui, Samuel B Hopkins, Tselil Schramm, Alexan-
der SWein, andIlias Zadik. TheFranz-Parisi criterion andcomputational trade-offs
in high dimensional statistics. arXiv preprint arXiv:2205.09727, 2022.
[BAMMN19] G´erard Ben Arous, Song Mei, Andrea Montanari, and Mihai Nica. The landscape
of the spiked tensor model. Communications on Pure and Applied Mathematics,
72(11):2282–2330, 2019.
[BB19] MatthewBrennanandGuyBresler. Optimalaverage-case reductionstosparsePCA:
From weakassumptionstostronghardness. In32nd Annual Conference on Learning
Theory (COLT 2019), pages 469–470. PMLR, 2019.
[BB20] Matthew Brennanand GuyBresler. Reducibility andstatistical-computational gaps
from secret leakage. In 33rd Annual Conference on Learning Theory (COLT 2020),
pages 648–847. PMLR, 2020.
[BBAP05] Jinho Baik, G´erard Ben Arous, and Sandrine P´ech´e. Phase transition of the largest
eigenvalue for nonnull complex sample covariance matrices. The Annals of Proba-
bility, 33(5):1643–1697, 2005.
[BBH18] Matthew Brennan, Guy Bresler, and Wasim Huleihel. Reducibility and computa-
tional lower bounds for problems with planted sparse structure. In 31st Annual
Conference On Learning Theory (COLT 2018), pages 48–166. PMLR, 2018.
[BBH19] Matthew Brennan,GuyBresler, andWasim Huleihel. Universality ofcomputational
lower bounds for submatrix detection. In Conference on Learning Theory, pages
417–468. PMLR, 2019.
30[BBH+20] Matthew Brennan, Guy Bresler, Samuel B Hopkins, Jerry Li, and Tselil Schramm.
Statistical query algorithms and low-degree tests are almost equivalent. arXiv
preprint arXiv:2009.06107, 2020.
[BBK+21] Afonso S Bandeira, Jess Banks, Dmitriy Kunisky, Cristopher Moore, and Alexan-
der S Wein. Spectral planting and the hardness of refuting cuts, colorability, and
communities in random graphs. In 34th Annual Conference on Learning Theory
(COLT 2021), pages 410–473. PMLR, 2021.
[BGJ20] G´erard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Algorithmic thresholds
for tensor PCA. Annals of Probability, 48(4):2052–2087, 2020.
[BH22] Guy Bresler and Brice Huang. The algorithmic phase transition of random k-SAT
for low degree polynomials. In 2021 IEEE 62nd Annual Symposium on Foundations
of Computer Science (FOCS), pages 298–309. IEEE, 2022.
[BHK+19] Boaz Barak, Samuel B Hopkins, Jonathan Kelner, Pravesh K Kothari, Ankur
Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the
planted clique problem. SIAM Journal on Computing, 48(2):687–735, 2019.
[BKW20] Afonso S Bandeira, Dmitriy Kunisky, and Alexander S Wein. Computational hard-
ness of certifying bounds on constrained PCA problems. In 11th Innovations in
Theoretical Computer Science Conference (ITCS 2020), volume 151, pages 78:1–
78:29, 2020.
[BMV+18] Jess Banks, Cristopher Moore, Roman Vershynin, Nicolas Verzelen, and Jiaming
Xu. Information-theoretic bounds and phase transitions in clustering, sparse PCA,
and submatrixlocalization. IEEE Transactions on Information Theory, 64(7):4872–
4894, 2018.
[BR13] QuentinBerthetandPhilippeRigollet. Complexitytheoreticlowerboundsforsparse
principal component detection. In 26th Annual Conference on Learning Theory
(COLT 2013), pages 1046–1066, 2013.
[CDMF09] Mireille Capitaine, CatherineDonati-Martin, andDelphineF´eral. Thelargest eigen-
values of finite rank deformation of large Wigner matrices: convergence and nonuni-
versality of the fluctuations. The Annals of Probability, 37(1):1–47, 2009.
[CKK+10] Michael Chertkov, Lukas Kroc, F Krzakala, M Vergassola, and L Zdeborov´a. Infer-
ence in particle tracking experiments by passing messages between images. Proceed-
ings of the National Academy of Sciences, 107(17):7663–7668, 2010.
[CMZ23] Zongchen Chen, Elchanan Mossel, and Ilias Zadik. Almost-linear planted cliques
elude the Metropolis process. In Proceedings of the 2023 Annual ACM-SIAM Sym-
posium on Discrete Algorithms (SODA), pages 4504–4539. SIAM, 2023.
[COGHK+22] Amin Coja-Oghlan, Oliver Gebhard, Max Hahn-Klimroth, Alexander S Wein, and
Ilias Zadik. Statistical and computational phase transitions in group testing. In
Conference on Learning Theory, pages 4764–4781. PMLR, 2022.
[Cor12] Ivan Corwin. The Kardar-Parisi-Zhang equation and universality class. Random
matrices: Theory and applications, 1(01):1130001, 2012.
31[DCS03] Giuseppe Dattoli, Clemente Cesarano, and Dario Sacchetti. A note on truncated
polynomials. Applied Mathematics and Computation, 134(2-3):595–605, 2003.
[DFJ02] MartinDyer, AlanFrieze, andMarkJerrum. Oncountingindependentsetsinsparse
graphs. SIAM Journal on Computing, 31(5):1527–1541, 2002.
[DK22] Ilias Diakonikolas and Daniel Kane. Non-gaussian component analysis via lattice
basis reduction. In Conference on Learning Theory, pages 4535–4547. PMLR, 2022.
[DKMZ11a] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov´a.
Asymptotic analysis of the stochastic block model for modular networks and its
algorithmic applications. Physical Review E, 84(6):066106, 2011.
[DKMZ11b] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov´a. Infer-
ence and phase transitions in the detection of modules in sparse networks. Physical
Review Letters, 107(6):065701, 2011.
[DKWB23] Yunzi Ding, Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira.
Subexponential-time algorithms for sparse PCA. Foundations of Computational
Mathematics, pages 1–50, 2023.
[DMLS23] Rishabh Dudeja, Yue M. Lu, and Subhabrata Sen. Universality of approximate
message passing with semirandom matrices. The Annals of Probability, 51(5):1616–
1683, 2023.
[Don51] Monroe David Donsker. An invariance principle for certain probability limit theo-
rems. 1951.
[DSMM18] Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. Con-
textual stochastic block models. Advances in Neural Information Processing Sys-
tems, 31, 2018.
[DT19] Percy Deift and Thomas Trogdon. Universality in numerical computation with
random data: Case studies and analytical results. Journal of Mathematical Physics,
60(10), 2019.
[DWXY21] Jian Ding, Yihong Wu, Jiaming Xu, and Dana Yang. The planted matching
problem: Sharp threshold and infinite-order phase transition. arXiv preprint
arXiv:2103.09383, 2021.
[EAKJ20] Ahmed El Alaoui, Florent Krzakala, and Michael Jordan. Fundamental limits of
detection in the spiked Wigner model. Annals of Statistics, 48(2):863–885, 2020.
[ESY11] La´szlo´ Erd˝os, Benjamin Schlein, and Horng-Tzer Yau. Universality of random ma-
trices and local relaxation flow. Inventiones mathematicae, 185(1):75–119, 2011.
[FK81] Zolta´n Fu¨redi and Ja´nos Komlo´s. The eigenvalues of random symmetric matrices.
Combinatorica, 1(3):233–241, 1981.
[FP07] Delphine F´eral and Sandrine P´ech´e. The largest eigenvalue of rank one deformation
of large Wigner matrices. Communications in Mathematical Physics, 272(1):185–
228, 2007.
32[GJW20] David Gamarnik, Aukosh Jagannath, and Alexander S Wein. Low-degree hardness
of random optimization problems. In 61st Annual Symposium on Foundations of
Computer Science (FOCS 2020), pages 131–140, 2020.
[GJX23] Reza Gheissari, Aukosh Jagannath, and Yiming Xu. Finding planted cliques using
Markov chain Monte Carlo. arXiv preprint arXiv:2311.07540, 2023.
[GKK+23] Alice Guionnet, Justin Ko, Florent Krzakala, Pierre Mergny, and Lenka Zdeborov´a.
Spectral phase transitions in non-linear Wigner spiked models. arXiv preprint
arXiv:2310.14055, 2023.
[GM17] Rong Ge and Tengyu Ma. On the optimization landscape of tensor decompositions.
In Advances in Neural Information Processing Systems, pages 3653–3663, 2017.
[GS14] David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random
graphs. In 5th Conference on Innovations in Theoretical Computer Science (ITCS
2014), pages 369–376. ACM, 2014.
[GZ19] David Gamarnik and Ilias Zadik. The landscape of the planted clique problem:
Dense subgraphs and the overlap gap property. arXiv preprint arXiv:1904.07174,
2019.
[HKP+17] SamuelBHopkins,PraveshKKothari,AaronPotechin, PrasadRaghavendra, Tselil
Schramm, and David Steurer. The power of sum-of-squares for detecting hidden
structures. In 58th Annual Symposium on Foundations of Computer Science (FOCS
2017), pages 720–731, 2017.
[HLM12] Simon Heimlicher, Marc Lelarge, and Laurent Massouli´e. Community detection in
the labelled stochastic block model. arXiv preprint arXiv:1209.2910, 2012.
[HM24] Han Huang and Elchanan Mossel. Low degree hardness for broadcasting on trees.
arXiv preprint arXiv:2402.13359, 2024.
[Hop18] Samuel B Hopkins. Statistical inference and the sum of squares method. PhD thesis,
Cornell University, 2018.
[HS17] Samuel B Hopkins and David Steurer. Efficient Bayesian estimation from few sam-
ples: community detection and related problems. In 58th Annual Symposium on
Foundations of Computer Science (FOCS 2017), pages 379–390. IEEE, 2017.
[HW21] Justin Holmgren and Alexander S Wein. Counterexamples to the low-degree con-
jecture. In 12th Innovations in Theoretical Computer Science Conference (ITCS
2021), 2021.
[IKKM12] Morteza Ibrahimi, Yashodhan Kanoria, Matt Kraning, and Andrea Montanari. The
set of solutions of random XORSAT formulae. In 23rd Annual ACM-SIAM Sympo-
sium on Discrete Algorithms (SODA 2012), pages 760–779. SIAM, 2012.
[Jer92] Mark Jerrum. Large cliques elude the Metropolis process. Random Structures &
Algorithms, 3(4):347–359, 1992.
[JLM20] Aukosh Jagannath, Patrick Lopatto, and Leo Miolane. Statistical thresholds for
tensor PCA. Annals of Applied Probability, 30(4):1910–1933, 2020.
33[Joh01] Iain M Johnstone. On the distribution of the largest eigenvalue in principal compo-
nents analysis. Annals of Statistics, pages 295–327, 2001.
[KLLM22] Daniel Kane, Sihan Liu, Shachar Lovett, and Gaurav Mahajan. Computational-
statistical gap in reinforcement learning. In Conference on Learning Theory, pages
1282–1302. PMLR, 2022.
[KM21] Frederic Koehler and Elchanan Mossel. Reconstruction on trees and low-degree
polynomials. arXiv preprint arXiv:2109.06915, 2021.
[KN11] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community struc-
ture in networks. Physical Review E, 83(1):016107, 2011.
[Kun21a] Dmitriy Kunisky. Hypothesis testing with low-degree polynomials in the Morris
class ofexponentialfamilies. In34th Annual Conference on Learning Theory (COLT
2021), pages 2822–2848. PMLR, 2021.
[Kun21b] DmitriyKunisky. Spectral Barriers in Certification Problems. PhDthesis,NewYork
University, 2021.
[KWB22] Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computa-
tional hardness of hypothesis testing: Predictions using the low-degree likelihood
ratio. In Paula Cerejeiras and Michael Reissig, editors, Mathematical Analysis,
its Applications and Computation, pages 1–50, Cham, 2022. Springer International
Publishing.
[KXZ16] Florent Krzakala, Jiaming Xu, and Lenka Zdeborov´a. Mutual information in rank-
one matrix estimation. In 2016 IEEE Information Theory Workshop (ITW), pages
71–75. IEEE, 2016.
[LKZ15a] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov´a. MMSE of probabilistic
low-rankmatrixestimation: Universalitywithrespecttotheoutputchannel. In53rd
Annual Allerton Conference on Communication, Control, and Computing (Allerton
2015), pages 680–687. IEEE, 2015.
[LKZ15b] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborov´a. Phase transitions in
sparsePCA. InIEEEInternational Symposium on Information Theory (ISIT 2015),
pages 1635–1639. IEEE, 2015.
[LML+17] Thibault Lesieur, L´eo Miolane, Marc Lelarge, Florent Krzakala, and Lenka Zde-
borov´a. Statistical and computational phase transitions in spiked tensor estimation.
In IEEE International Symposium on Information Theory (ISIT 2017), pages 511–
515. IEEE, 2017.
[LMX15] Marc Lelarge, Laurent Massouli´e, and Jiaming Xu. Reconstruction in the labelled
stochastic block model. IEEE Transactions on Network Science and Engineering,
2(4):152–163, 2015.
[LRR01] Genyuan Li, Carey Rosenthal, and Herschel Rabitz. High dimensional model repre-
sentations. The Journal of Physical Chemistry A, 105(33):7765–7777, 2001.
34[MMX19] Mehrdad Moharrami, Cristopher Moore, and Jiaming Xu. The planted matching
problem: phase transitions and exact results. arXiv preprint arXiv:1912.08880,
2019.
[Moo17] Cristopher Moore. Thecomputer science andphysics of community detection: land-
scapes, phase transitions, and hardness. arXiv preprint arXiv:1702.00467, 2017.
[MPW15] Raghu Meka, Aaron Potechin, and AviWigderson. Sum-of-squareslower boundsfor
planted clique. In 47th Annual ACM Symposium on Theory of Computing (STOC
2015), pages 87–96. ACM, 2015.
[MRY18] Andrea Montanari, Feng Ruan, and Jun Yan. Adapting to unknown noise distribu-
tion in matrix denoising. arXiv preprint arXiv:1810.02954, 2018.
[MRZ15] Andrea Montanari, Daniel Reichman, and Ofer Zeitouni. On the limitation of spec-
tral methods: From the gaussian hidden clique problem to rank-one perturbations
of gaussian tensors. In Advances in Neural Information Processing Systems, pages
217–225, 2015.
[MW15] Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix de-
tection. 2015.
[MW22] Andrea Montanari and Alexander S Wein. Equivalence of approximate message
passing and low-degree polynomials in rank-one matrix estimation. arXiv preprint
arXiv:2212.06996, 2022.
[MW23] AnkurMoitraandAlexanderSWein. Preciseerrorratesforcomputationallyefficient
testing. arXiv preprint arXiv:2311.00289, 2023.
[O’D14] Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
[Pit79] Edwin JG Pitman. Some basic theory for statistical inference. Chapman and Hall,
1979.
[PWBM18] Amelia Perry,Alexander SWein, AfonsoSBandeira, andAnkurMoitra. Optimality
and sub-optimality of PCA I: Spiked random matrix models. Annals of Statistics,
46(5):2416–2451, 2018.
[RA99] Herschel Rabitz and O¨mer F Ali¸s. General foundations of high-dimensional model
representations. Journal of Mathematical Chemistry, 25(2-3):197–233, 1999.
[RM14] Emile Richard and Andrea Montanari. A statistical model for tensor PCA. In
Advances in Neural Information Processing Systems, pages 2897–2905, 2014.
[RSS18] Prasad Raghavendra, Tselil Schramm, and David Steurer. High-dimensional esti-
mation via sum-of-squares proofs. arXiv preprint arXiv:1807.11419, 2018.
[RSWY23] Cynthia Rush, Fiona Skerman, Alexander S Wein, and Dana Yang. Is it easier to
count communities than find them? In 14th Innovations in Theoretical Computer
Science Conference (ITCS 2023), 2023.
[SLKZ15] Alaa Saade, Marc Lelarge, Florent Krzakala, and Lenka Zdeborov´a. Spectral de-
tection in the censored block model. In 2015 IEEE International Symposium on
Information Theory (ISIT), pages 1184–1188. IEEE, 2015.
35[SN18] Hussein Saad and Aria Nosratinia. Community detection with side information:
Exact recovery under the stochastic block model. IEEE Journal of Selected Topics
in Signal Processing, 12(5):944–958, 2018.
[SW22] Tselil Schramm and Alexander S Wein. Computational barriers to estimation from
low-degree polynomials. The Annals of Statistics, 50(3):1833–1858, 2022.
[Var01] SRSrinivasaVaradhan. Probability theory. Number7.AmericanMathematical Soc.,
2001.
[Wei22] Alexander S Wein. Optimal low-degree hardness of maximum independent set.
Mathematical Statistics and Learning, 4(3):221–251, 2022.
[WZF22] Tianhao Wang, Xinyi Zhong, and Zhou Fan. Universality of approximate message
passing algorithms and tensor networks. arXiv preprint arXiv:2206.13037, 2022.
[Zam98] Ram Zamir. A proof of the Fisher information inequality via a data processing
argument. IEEE Transactions on Information Theory, 44(3):1246–1250, 1998.
[ZK16] Lenka Zdeborov´a and Florent Krzakala. Statistical physics of inference: Thresholds
and algorithms. Advances in Physics, 65(5):453–552, 2016.
[ZSWB22] Ilias Zadik, Min Jae Song, Alexander S Wein, and Joan Bruna. Lattice-based meth-
ods surpass sum-of-squares in clustering. In Conference on Learning Theory, pages
1247–1248. PMLR, 2022.
A General tools for coordinate decomposition
A.1 Structure of coordinate subspaces
We first present a useful decomposition of the V and V into orthogonal subspaces and descrip-
T D
≤
tions of orthogonal projection operators onto these subspaces. We do not need the structure of
an LVM, and just work with Q = an arbitrary product measure for probability
1 N i
Q ⊗···⊗Q Q
measuresover ameasurablespaceΩ(itistrivial butnotationally cumbersometoallow Ωtodepend
on i). The definitions below are an instance of the Efron-Stein decomposition, as presented in, e.g.,
Section 8.3 of [O’D14].
Definition A.1 (Averaging operator). For each T [N], we write Avg :L2(Q) V for the
∈ T → [N] \T
operator that maps
(Avg f)(y) := E[f(y)] = E[f(y) y ]. (72)
T yT | [N] \T
We write Avg := Avg .
i i
{}
Concretely, the averaging operator for a single coordinate Avg simply “integrates out” the ith
i
coordinate:
(Avg f)(y) = f(y ,...,y ,y,y ,...,y )d (y). (73)
i 1 i −1 i+1 N Qi
Z
Proposition A.2. The V T and Avg T satisfy the folloewing properties: e
1. V V = V .
S T S T
∩ ∩
2. The Avg commute (i.e., Avg Avg f = Avg Avg f for all f L2(Q)).
i i j j i ∈
363. Avg = Avg .
T i T i
∈
4. The orthQogonal projection operator to V is Avg .
T [N] T
\
5. The subspaces V meet orthogonally; that is, for any distinct S,T [N], (V V ) V is
T S T ⊥ S
⊆ ∩ ∩
orthogonal to (V V ) V .
S T ⊥ T
∩ ∩
Proof. Claims 1 and 3 are immediate by definition, and Claim 2 by Fubini’s theorem. Claim 4 is
a well-known property of conditional expectation (see, e.g., Exercise 4.8 of [Var01]), but may be
verified concretely as follows. First, clearly from the definition, Avg is idempotent, has V as
[N] T T
its image, and acts as the identity on V . So, Avg is a projection t\ o V , and it only suffices to
T [N] T T
verify that it is orthogonal. To do this, we suppose f\ L2(Q) and g V , and compute
T
∈ ∈
E (f(y) Avg f(y))g(y) = E f(y)g(y) E Avg f(y)g(y)
[N] T [N] T
y Q − \ y Q −y Q \
∼ ∼ ∼
= E f(y)g(y) E E[f(y) y ]g(y)
T
y Q −y Q |
∼ ∼
= E f(y)g(y) E E[f(y)g(y) y ]
T
y Q −y Q |
∼ ∼
= 0, (74)
wherewehaveusedthatg(y)dependsonlyony andthetowerpropertyofconditionalexpectation.
T
For Claim 5, note by Claim 1 that (V V ) V = V V , and by Claim 4 the orthogonal
S
∩
T ⊥
∩
S S⊥T
∩
S
projectiontoV isId Avg andtheorthogonalp∩rojectiontoV isAvg . Inparticular,
S⊥ ∩T − [N] \(S ∩T) S [N] \S
these projections commute, so the orthogonal projection to (V V ) V is their product,
S T ⊥ S
∩ ∩
Avg (Id Avg ) = Avg Avg . Similarly, theorthogonal projection to(V
[N] \S − [N] \(S ∩T) [N] \S− [N] \(S ∩T) S ∩
V ) V isAvg Avg . Finally,itsufficestocheckthattheproductoftheseprojections
T ⊥ T [N] S [N] (S T)
∩ \ − \ ∩
is zero:
(Avg Avg )(Avg Avg )= 0, (75)
[N] \S − [N] \(S ∩T) [N] \T − [N] \(S ∩T)
since each of the four terms upon expanding is Avg , occurring twice positively and twice
[N] (S T)
\ ∩
negatively.
We now definea subtler family of subspaces, which aim to capture the “new” functions depend-
ing only on y that V contains, beyond those that are linear combinations of ones depending on
T T
smaller coordinate subsets.
Definition A.3. For each T [N], V := V ( V ) .
⊆ T T ∩ S(T S ⊥
We establish the following important properties oPf these subspaces.
b
Proposition A.4. The V satisfy the following properties:
T
1. The subspaces V are mutually orthogonal; that is, for any distinct S,T [N] and f V
T b S
⊆ ∈
and g V , we have E f(y)g(y) = 0.
T y Q
∈ ∼
b b
2. V T = Sb T V S; in particular, L2(Q) = V [N] = T [N]V T.
⊆ ⊆
L L
3. The orthogonbal projection operator to V T is b
(Id Avg i) b Avg
i
= ( 1) |T |−|S |Avg
[N] S
(76)
− − \
i T i [N] T S T
Y∈ ∈Y\ X⊆
37Proof. For Claim 1, note that V V V and V V V . The result then follows by
S
⊆
S⊥T
∩
S T
⊆
S⊥T
∩
T
Claims 1 and 5 of Proposition A.2. ∩ ∩
For Claim 2, we may proceebd by induction on Tb. The result holds for T = ∅, in which case
| |
V = V is the span of the constant function. Now, supposethe result holds for all T k 1, and
T T
| | ≤ −
wehave T = k. ForanyT ( T,wehavebytheinductivehypothesis V V = V .
| | ′ S T S ⊇ S T′ S T′
In parbticular, V V . On the other hand, we also have⊆ V ⊆ V = V
S ⊆T S ⊇ S(T S L bS ⊆T LS ⊇ T b T ∩
( V ) . Thus, V V +V ( V ) V . The opposite inclusion is
immS e( dT iatS e,⊥ comL pletingbtS h⊆eT iP ndS u⊇ ctionS .(T S T ∩ S(T S ⊥ ⊇ T L b b
P L P P
For Claim 3, note that, bby Claim 2, writing P for the orthogonal projection to V , we have
T T
P = Avg . The result, in the form of the right-hand side of (76), then follows by the
S T S [N] T
M¨ob⊆ius inversion fo\ rmula. b b
P
b
It may be instructive to identify the manifestations of these somewhat abstract objects in
Boolean function theory.
Example A.5 (Boolean Fourier analysis). Consider the case Ω = 1 and Q = Unif( 1 N).
{± } {± }
Then, L2(Q) is simply the set of all functions f : 1 N R. As is well-known, every such
{± } →
function may be written as a polynomial of multilinear monomials,
f(y)= f(T)yT, (77)
T [N]
X⊆
b
also known as the Boolean Fourier expansion of f, where yT = y . The monomials yT form
i T i
an orthonormal basis for L2(Q). ∈
Q
In this representation, V is the span of all yS for S T. As a consequence, the degree of f (as
T
⊆
a polynomial in the Fourier representation) equals the coordinate degree of f. The operator Avg
i
acts on f by removing all monomials including y from this representation,
i
(Avg f)(y)= f(T)yT, (78)
i
T [N] i
⊆X\{}
b
so indeed Avg is the orthogonal projection to V . The commutativity of these Avg is also
i [N] T i T i
clear. ∈ \
Q
V consists of the functions whose monomials only have indices lying in T (i.e., belonging to
T
V ) but also having no monomials whose indices are contained in any proper subset of T (i.e.,
T
orthbogonal to V ). Therefore, V is merely the span of the single monomial function yT.
S(T S T
We may equate this with the projection formula (76) by noting that Id Avg acts by removing all
P − i
monomials not including y from the Fbourier representation, and thus the only monomial that the
i
projection does not remove is yT.
Similar results equating coordinate with polynomial degree and Efron-Stein decomposition with
Boolean Fourier decomposition hold also for a domain ΩN with Ω a finite set, where polynomials
may be made sense of through a “one-hot” encoding of x ΩN.
∈
We thus see that the orthogonal decomposition L2(Q) = V may be viewed as a gener-
T [N] T
alization of Boolean Fourier analysis to arbitrary multivariate⊆ L2 spaces coming from a product
L
measure. b
Finally, this machinery gives us a simple way to project arbitrary functions to V . The
D
≤
condensed formula below will not prove very useful to us—we will prefer to view the projection to
V as a sum of projections to the orthogonal components V —but we emphasize that it gives a
D T
≤
fully explicit description of this projection as a linear combination of averaging operators.
b
38Lemma A.6. The orthogonal projection operator to V is
D
≤
N T 1
P ≤D = ( −1)D −|T | D−| | T− Avg [N] \T. (79)
T [N] (cid:18) −| | (cid:19)
X⊆
T D
| |≤
Proof. We use that, per Proposition A.4, V is the direct sum of orthogonal components V for
D T
≤
T D. Thus, P is the sum of the corresponding projections:
D
| | ≤ ≤
b
P = P
D T
≤
T [N]
X⊆
T D b
| |≤
= Avg ( 1)S Avg
[N] \T − | | S
T [N] S T
X⊆ X⊆
T D
| |≤
= ( 1)S Avg
− | | [N] \(T \S)
T [N]S T
X⊆ X⊆
T D
| |≤
where, introducing R := T S,
\
=  ( −1) |T |−|R |Avg
[N] \R
R [N] R T [N]
X⊆  ⊆X⊆ 
R D T D 
| |≤ | |≤
 
D
N R
=  ( −1)t −|R | t − R| | Avg [N] \R, (80)
R [N] t=R (cid:18) −| | (cid:19)
X⊆ X| |
R D 
| |≤
and the final formula follows from a combinatorial calculation.
A.2 Alternative derivation with concrete bases
The above framework may also be established more concretely using the bases of orthogonal poly-
nomials usually used in the analysis of LDP. More generally, for each i [N], let f be a
i,j j 0
countable basis of (univariate) orthonormal functions in L2( ), with f ∈ = 1. Thes{ e th} en≥ form a
i i,0
Q
countable orthonormal product basis f of L2(Q) defined by f (y) = N f (y ).
{ j }j ∈NN j i=1 i,ji i
Then, V is the span of those f with j = 0 for all i / T. And, V is the span of those f with
T j i ∈ T Q j
j = 0 for all i T and j > 0 for all i T (i.e., those j whose support is exactly T). The basis
i i
6∈ ∈
elements spanningtheV then partition thebasis, which immediatelby gives their orthogonality and
T
the decomposition L2(Q)= V .
T [N] T
Ontheotherhand,bourformu⊆lation above makes itclear thatthisdecomposition isindependent
L
of the choice of orthonormal bases, wbhich is the entire benefit of working with coordinate degree—
that we can do calculations without needing to understandthe details of the underlying orthogonal
polynomials or orthonormal functions.
A.3 Coordinate decomposition of likelihood ratio
Suppose now that we also have a further probability measure P on ΩN. We do not make the
assumptions of an LVM, and rather allow P to be arbitrary so long as it is absolutely continuous
39to Q and has dP/dQ L2(Q) (as for a good LVM in the main text). In the main text, we are
∈
interested in the decomposition over the subspaces V of the likelihood ratio L := dP/dQ.
T
The projections of L to V , per Proposition A.2, are
T
b
L := E L(y). (81)
T
y
[N]\T
One may check that this is the natural definition of the likelihood ratio between the marginal law
P of P on the coordinates of T and the corresponding marginal law Q = , since
T T i T Qi
∈
E L T(y T)f(y T) = E L(y)f(y T) = E f(y T). N (82)
yT Q T y Q y P
∼ ∼ ∼
That is, abstractly one would use this to define the marginal law P by its Radon-Nikodym deriva-
T
tive to Q . But, in simple cases of discrete measures or continuous measures with well-behaved
T
densities, one may also check that L is indeed the ratio of marginal probability mass or density
T
functions, respectively. We thus refer to the L as marginal likelihood ratios.
T
From the previous results we then immediately obtain the following.
Lemma A.7. Define the LCDLR, L := P L, and
D D
≤ ≤
L := ( 1)T S L . (83)
T | |−| | S
−
S T
X⊆
b
Then, L is the orthogonal projection of L to V . In particular, the L are orthogonal, and
T T T
E L (y)2 = E L (y)2. (84)
b D b T b
y Q ≤ y Q
∼ T [N] ∼
X⊆
T D b
| |≤
A.4 Additional remarks
Wepointoutafewmorecuriousfeaturesandconsequencesofourobservationsthatarelessessential
to our arguments in the main text.
Expanding the LCDLR Note that both the norm of the LCDLR and the LCDLR itself
may be written completely in terms of the marginal likelihood ratios L , upon expanding in
T
Lemma A.7. For the LCDLR, we have either by expanding in the Lemma or by using the for-
mula from Lemma A.6,
N T 1
L D = ( 1)D −|T | −| |− L T. (85)
≤ − D T
T [N] (cid:18) −| | (cid:19)
X⊆
T D
| |≤
We do not directly use the formula (85), but we include it to emphasize that the LCDLR takes a
surprisingly explicit form in arbitrary models so long as Q is a product measure. We leave it as an
interesting question to ascertain whether the L themselves have a natural statistical meaning for
T
hypothesis testing or whether (85) can be operationalized into a useful algorithm.
For the norm, one may expand in LemmabA.7 into inner products of the L , which have the
S
following appealing structure reflecting the lattice structure of subsets of [N], analogous to Claim 1
of Proposition A.2 that V V = V .
S T S T
∩ ∩
Proposition A.8. E L (y)L (y) = E L (y)2.
y Q S T y Q S T
∼ ∼ ∩
This follows from the decomposition, by Proposition A.4, L = L , where the L are
T R T R R
mutually orthogonal. Alternatively, it may be viewed as a consequence o⊆f V V = V and the
S T S T
P ∩ ∩
orthogonal meeting of these subspaces from Proposition A.2. b b
40Connections with χ2 divergence As a special case of Lemma A.7 on the norm of the LCDLR,
we may consider the norm of L itself, which is related to the χ2 divergence between P and Q. The
Lemma thus gives an interesting decomposition of this divergence.
Definition A.9. The χ2 divergence between P and Q is
χ2(P Q) := E L(y)2 1 = E (L(y) 1)2. (86)
k y Q − y Q −
∼ ∼
Corollary A.10. For any P and Q = ,
1 N
Q ⊗···⊗Q
2
χ2(P Q) = E ( 1)T S L (y) . (87)
| |−| | S
k y Q − 
T [N] ∼ S T
TX⊆
6=∅
X⊆

Proof. The result follows from the definition of χ2 divergence, Lemma A.7, and the observation
that L = 1.
∅
In fact, the same kind of identity also holds for an arbitrary partition of [N], since we may group
the y ,...,y into various subsets of “effective coordinates” of y and repeat the same analysis.
1 N
Actually, the definition (86) itself is just the special case of these formulas when we view y as
having just one coordinate, in which case there is just one term in the summation above that
is equal to E (L (y) L (y))2 = E(L(y) 1)2 = EL(y)2 1. Generally, when computing
y Q 1 ∅
a χ2 divergenc∼ e, if{ an} y su− bsets of coordinates − of Q are indepen− dent, then we may apply such a
“coarsening” operation and apply the identity to obtain an interesting decomposition.
In a similar spirit, it is tempting to bound the components of the norm of the LCDLR by χ2
divergences,
E L (y)2 E (L (y) 1)2 = χ2(P Q ). (88)
T T T T
y Q ≤ y Q − k
∼ ∼
This is valid for all T = ∅, sincbe one may view L as L orthogonalized against all L with S ( T,
T T S
6
while L 1 is L orthogonalized against just L = 1. We could thus bound the norm of the
T T ∅
−
LCDLR by a sum of marginal χ2 divergences, b b
b
E L (y)2 1+ χ2(P Q ), (89)
D T T
y Q ≤ ≤ k
∼ T [N]
X⊆
T D
| |≤
and in fact one may bound more carefully by a sum over just the T = D as well. Unfortunately,
| |
while conceptually appealing, this bound may be verified to be too loose even in, e.g., the spiked
matrix models treated in the main text. The issue is that, upon expanding these divergences as
sums of norms of the L per Corollary A.10, we find that the norms of certain L , say those with
S S
S = D/2, are dramatically overcounted in this bound.
| |
b b
Beyond product measures? Finally, weemphasizethat, unfortunately, ourapproachnolonger
applies once Q is not a product measure (or, less restrictively per the above, admits no non-trivial
partition into subsets of independentindices). The reason for this is subtle; it is, after all, still true
that Avg (with the interpretation as a conditional expectation) is an orthogonal projection to
[N] T
V ; theproo\ f of this in Proposition A.2still holds. Theproblem is that Avg nolonger factorizes in
T S
termsoftheAvg ,nordotheseprojectionscommute,andthereforenordotheV meetorthogonally.
i T
Indeed,considerthecaseN = 2wherey andy arecorrelated. Intuitively speaking,Avg y V
1 2 2 2 ∈ {1 }
41is the “best prediction” of y as a function of y . Avg Avg y V is then the best prediction of
2 1 1 2 2 ∈ {2 }
this function of y as a function of y —but in the presence of a non-trivial correlation, this is not
1 2
a constant, while Avg y = Ey is.
1,2 2 2
{ }
On the other hand, the right-hand side of (85) is still sensible in arbitrary models, as a kind
of “mean-field approximation” of the LCDLR, in the sense of computing what the LCDLR would
be if Q were a product measure. We offer the intriguing question of whether this quantity may be
useful for efficient hypothesis testing even without the product structure of Q.
B Necessity of conditions for universality
Wegivesomeelaborationontheconditionsthatarenecessaryforuniversalityandwhenuniversality
of computational thresholds for strong detection and other tasks should break down.
The following is a very simple example showing that some control of the prior is necessary for
universality of the coordinate advantage to hold.
Example B.1 (Non-universality). Consider the prior = Unif( 1 N) observed through the
X {±2}
Bernoulli and Gaussian channels. Through = Ber(1 +x), when x 1 we have = δ , so
Px 2 ∈ {±2} Px x
the observation is just P = Ber(1) N = Q. Therefore,
2 ⊗
1
CAdv ,Ber +x = 1 (90)
D
≤ X 2
(cid:18) (cid:18) (cid:19)(cid:19)
for all D 0. On the other hand, through = (x,1), for any D 2,
x
≥ P N ≥
CAdv ( , (x,1)) Adv ( , (x,1))
D D
≤ X N ≥ ≤ X N
= E exp ≤D( x(1),x(2) )
h i
x(1),x(2)
∼X
1
E x(1),x(2) 2
≥ 2h i
1
= N, (91)
8
which diverges as N (in the inequality we appeal to Lemma 4.5).
→ ∞
This example illustrates that some assumption on the size of the prior and its entries is necessary,
and one may check that this example fails Assumption P2 of Theorem 3.9. Generally, our brand of
universality holds when the dependence on the channel is only on its local behavior near x = 0;
x
P
priorswhoseentries aretypically very far fromzero like this one can bringout theglobal differences
of channels. Here, the difference is stark: the Bernoulli channel for a certain magnitude of signal
“tells the truth” and outputs the signal with no noise at all, while the Gaussian channel always
applies some non-trivial noise to the signal.
Remark B.2 (Detection versus recovery). As an aside, this example also happens to showcase
the subtle relationship between detection (hypothesis testing) and recovery (estimation) of x. The
above shows that detection is much easier through the Gaussian channel than through the Bernoulli
channel. Conversely, perfect recovery of x from y P is trivially possible through the Bernoulli
∼
channel, since y = x. On the other hand, through the Gaussian channel we only make noisy
observations of x 1 N, and a simple argument shows that it is only possible to estimate a
∈ {±2}
constant fraction of the x correctly with high probability.
i
42Sparse PCA On the other hand, there are situations that Theorem 3.9 does not cover where
we still expect universality to hold. One is sparse PCA. As a specific example, studied for LDP by
[DKWB23], consider the spiked matrix model with the sparse Rademacher prior from Section 3.2,
but with decaying sparsity s = s(n)= o(1). Recall that the prior is the law of X = λ xx where
√n ⊤
x has n i.i.d. entries drawn from the measure
s s
π = (1 s)δ + δ + δ , (92)
− 0 2 1/√s 2 −1/√s
and the total dimension is N = n2. (Here λ is still a constant not depending on n; we are also
ignoring the symmetry of the observed matrices for the sake of simplicity, which does not make a
materialdifference.) Inparticular,withhighprobabilitywehavethatthenormsofthevectorization
of the signal are
1
vec(X) k n −1/2 (sn)2 s −k k = sk2 −1nk2 −1 2 = sk2 −1Nk1 −41 . (93)
k k ≍ ·
(cid:16) (cid:17)
Thus, Assumptions P2 and P3 (for Theorem 3.9) concerning k > 2 are not satisfied once s =
o(1). However, looking more deeply into the proof of Theorem‘3.9, we see that the true source
of non-universality would be for the quantity n (X(1) )2(X(2) )2 to be unbounded for typical
i,j=1 ij ij
independent draws X(1),X(2) from the prior. In the proof, we bound this by the Cauchy-Schwarz
P
inequality, which does not respect the sparsity structure in the present setting. Indeed, we expect
to have with high probability
n n 2 2
1 1 1
(X(1) )2(X(2) )2 (x(1) )2(x(2) )2 s2n = 1. (94)
ij ij ≍ n2 i i ≍ n2 · s2
!
i,j=1 i=1 (cid:18) (cid:19)
X X
The result of Theorem 3.9 should therefore likely hold for sparse PCA as well, but would require a
more tailored proof treating sparsity more carefully.
Constants and weak detection rates Our proof of Theorem 3.9 makes it apparent that the
constants C ,C ,C are generally necessary and depend on the channel in a way not captured
1 2 3
P
by the Fisher information. (Indeed, they arise precisely from the error term discussed above, which
appears in the fourth order terms of the Taylor expansion of R , while the Fisher information
P
only governs the second order term.) In particular, these constants cannot in general be replaced
by 1+ o(1). As discussed in, e.g., [BAH+22, RSWY23], showing that the coordinate advantage
is 1+o(1) may be viewed as evidence that weak detection—distinguishing P and Q with some
n n
probability strictly greater than 1—is computationally hard. Thus we expect that thresholds for
2
weak detection should not enjoy the channel universality of thresholds for strong detection. The
remarkable recent work of [MW23] made this connection more precise, showing that the numerical
value of the advantage is related (at least in a spiked matrix model) to the best receiver operating
characteristic (ROC) curve, or tradeoff between Type I and Type II errors, achievable by efficient
testing algorithms. We thereforeexpect this optimal tradeoff, too, tobenon-universal. We propose
the verification of this as an interesting open problem.
43C Omitted proofs
C.1 Assumptions for additive channels: Proof of Proposition 3.11
Recall that we are considering a channel with the law of x+z for z ρ, where ρ has a density
x
P ∼
p(y) on R. This means that has a density p(y x), and the channel likelihood ratios are
x
P −
d p(y x)
x
L (y) = P = − , (95)
x
d p(y)
0
P
which is always defined since we have assumed that p(y) > 0 for all y. The channel overlap is then
p(y x(1))p(y x(2))
R (x(1),x(2)) = 1+ E L (y)L (y) = 1+ ∞ − − dy. (96)
P − y ρ
x(1) x(2)
− p(y)
∼ Z−∞
This is, up to the 1 term and a logarithm, equivalent to the translation function considered by
−
[PWBM18] in the special context of PCA. To reiterate an example mentioned there:
Example C.1. When ρ= (0,1), then K(x(1),x(2))= exp(x(1)x(2)) 1.
N −
Proof of Proposition 3.11. It suffices to verify that the assumptions of Theorem 3.9 hold. Since we
assume p(y) > 0 and p is 4 on all of R, L (y) is also 4 in x on all of R, so Assumption C1 of
x
C C
Theorem 3.9 is satisfied. For Assumption C2, we may compute
∂3R p (y)p(y)
∞ ′′ ′
P (0,0) = dy = 0, (97)
∂x(1)2∂x(2) − p(y)
Z−∞
since p( y) = p(y), so p ( y) = p (y) while p( y) = p(y), whereby the integrand is an odd
′′ ′′ ′ ′
− − − −
function. The Fisher information is computed similarly:
∂2R p(y)2
∞ ′
F = P (0,0) = dy, (98)
P ∂x(1)∂x(2) p(y)
Z−∞
as claimed.
Remark C.2. The role the translation function plays in our argument is somewhat different from
that in [PWBM18]. The reason for this is that their calculations (which concern the full χ2 di-
vergence or norm of the likelihood ratio) may be viewed as taking D = N in the formula (not the
bound) of Theorem 3.5, factorizing the resulting expression into a product, and writing this as an
(1) (2)
exponential of a sum. More specifically, abbreviating R := R (x ,x ),
i i i
P
CAdv ( , )2 = E R
N i
≤ X P x(1),x(2)
∼XT X⊆[N]i Y∈T
N
= E (1+R )
i
x(1),x(2)
i=1
Y
N
= E exp log(1+R ) , (99)
i
x(1),x(2) !
i=1
X
and this is the source of the extra logarithm in the translation function they work with. We obtain a
bound that instead has N R in the exponential, which is larger, but our approach has the benefit
i=1 i
of being easily adaptable to low coordinate degree truncations.
P
44C.2 Assumptions for exponential family channels: Proof of Proposition 3.12
Let us first review some basic definitions and properties of exponential families that we omitted in
the presentation of the main results in the Introduction. Our discussion is similar to that of the
same topics in [Kun21a].
Definition C.3. Let be a probability measure over R which is not a single atom. Let ψ(θ) :=
0
P
logE e [exp(θx)] and Θ := θ R : ψ(θ) < . Then, the natural exponential family (NEF)
x P 0 { ∈ ∞}
∼ e
generated by is the family of probability measures ρ , for θ Θ, given by the relative densities
0 θ
P ∈
d
e Pθ (y):= exp(θy ψ(θ)). (100)
d −
0
Pe
ψ(θ) is the cumulant generating function of . The cumulant generating functions of the
e 0 θ
P P
aretranslationsofψ,andthemeansandvariancesof arethusgivenbyderivatives ofψ evaluated
θ
P
away from zero, e e
e
µ θ := E y ∼Pe θ[y] =ψ ′(θ), (101)
σ θ2 := Var
y
∼Pe θ[y] =ψ ′′(θ). (102)
For compatibility with the statement of Proposition 3.12, we also write
µ := µ , (103)
0
σ2 := σ2. (104)
0
Since isnotanatom,neitherisany ,soψ (θ) = σ2 > 0. Therefore,ψ isstrictlyincreasing,
P0 Pθ ′′ θ ′
andthusone-to-one. LettingΣ := ψ (R) µ R,whichonemayverifyisalwayssomeopeninterval,
′
− ⊆
possibly inefinite on either side, of R, we seee that admits an alternative parametrization by the
θ
P
mean, which we may also translate by µ := µ to obtain the setting specified in Proposition 3.12,
0
e
−1
θ(x):= ψ (µ+x), (105)
′
:= . (106)
x θ(x)
P P
Note that θ(0)= 0, so 0 = 0. e
P P
The channel likelihood ratios in this parametrization are then
e
d dP
L (y) = Px (y) = θ(x) (y) = exp θ(x)y ψ(θ(x)) , (107)
x
d P0 dP 0 −
e (cid:0) (cid:1)
and the channel overlap is
e
R (x(1),x(2))= E L (y)L (y) 1
P y 0
x(1) x(2)
−
∼P
= exp( ψ(θ(x(1))) ψ(θ(x(2)))) E exp(y(θ(x(1))+θ(x2))) 1
− − y 0 −
∼P
= exp ψ θ(x(1))+θ(x(2)) ψ(θ(x(1))) ψ(θ(x(2))) 1.
− − −
(cid:18) (cid:19)
(cid:0) (cid:1)
It will also be useful to define
K (x(1),x(2)) := exp ψ θ(x(1))+θ(x(2)) ψ(θ(x(1))) ψ(θ(x(2))) = R (x(1),x(2))+1. (108)
P − − P
(cid:18) (cid:19)
(cid:0) (cid:1)
45Proof of Proposition 3.12. Again we verify the conditions of Theorem 3.9, which requires taking
the first three partial derivatives of R . For the first derivative:
P
∂R
P (x(1),x(2)) = K (x(1),x(2))θ ′(x(1)) ψ ′(θ(x(1))+θ(x(2))) ψ ′(θ(x(1))) . (109)
∂x(1) P −
(cid:16) (cid:17)
For the mixed second partial derivative:
∂2R
P (x(1),x(2)) = K (x(1),x(2))θ ′(x(1))θ ′(x(2))
∂x(1)∂x(2) P
ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1))) ψ (θ(x(1))+θ(x(2))) ψ (θ(x(2)))
′ ′ ′ ′
− −
(cid:18)
(cid:16) (cid:17)(cid:16) (cid:17)
+ψ (θ(x(1))+θ(x(2))) . (110)
′′
(cid:19)
Finally, for the mixed third partial derivative:
∂3R
∂x(1)2∂P x(2)(x(1),x(2))= K P(x(1),x(2))θ ′(x(2))
θ (x(1))2 ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1))) ψ (θ(x(1))+θ(x(2))) ψ (θ(x(2)))
′ ′ ′ ′ ′
− −
(cid:18) (cid:18)
(cid:16) (cid:17)(cid:16) (cid:17)
+ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1)))
′′ ′ ′
−
(cid:19)
(cid:16) (cid:17)
+θ (x(1)) ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1))) ψ (θ(x(1))+θ(x(2))) ψ (θ(x(2)))
′′ ′ ′ ′ ′
− −
(cid:18)
(cid:16) (cid:17)(cid:16) (cid:17)
+ψ (θ(x(1))+θ(x(2)))
′′
(cid:19)
+θ (x(1))2 ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1))) ψ (θ(x(1))+θ(x(2))) ψ (θ(x(2)))
′ ′′ ′′ ′ ′
− −
(cid:18)
(cid:16) (cid:17)(cid:16) (cid:17)
+ ψ (θ(x(1))+θ(x(2))) ψ (θ(x(1))) ψ (θ(x(1))+θ(x(2)))
′ ′ ′′
−
(cid:16) (cid:17)(cid:16) (cid:17)
+ψ (θ(x(1))+θ(x(2))) . (111)
′′′
(cid:19)(cid:19)
The derivatives of θ(x) appearing in these formulas are
1
θ (x)= , (112)
′
ψ (θ(x))
′′
ψ (θ(x)) ψ (θ(x))
′′′ ′′′
θ (x)= θ (x) = . (113)
′′ ′
−ψ (θ(x))2 −ψ (θ(x))3
′′ ′′
We then have
θ(0)= 0, (114)
1
θ (0) = , (115)
′
σ2
ψ (0)
′′′
θ (0) = . (116)
′′
− σ6
46Further, when weevaluate the derivatives of R at x(1) = x(2) = 0, we will have many cancellations
P
because, for i 1,2 ,
∈ { }
ψ (θ(x(1))+θ(x(2))) ψ (θ(x(i))) = ψ (0) ψ (0) = 0. (117)
′ ′ ′ ′
− −
In particular, for the Fisher information we find
∂2R 1 1 1
P (0,0) = (0+σ2) = , (118)
∂x(1)∂x(2) σ2 · σ2 · σ2
as claimed, while for the third mixed derivatives we find
∂3R 1 ψ (0) 1
P (0,0) = 0 ′′′ (0+σ2)+ ψ ′′′(0) = 0, (119)
∂x(1)2∂x(2) σ2 · − σ6 · σ4
(cid:18) (cid:19)
completing the proof.
C.3 Truncated exponentials: Proof of Proposition 5.3
We will use throughout the integral formula
1
exp D(x) = ∞ exp( s)(x+s)Dds, (120)
≤
D! −
Z0
which may be found in [DCS03].
Proof of Proposition 5.3. Thefirstinequality of the firstclaim, exp D(x) > 0, follows directly from
≤
thissinceD iseven, andthesecond inequality ofthefirstclaim, exp D(x) exp(x ), follows bythe
≤
≤ | |
triangle inequality and Taylor expansion. For the second claim, note that the left-hand inequality
follows from the right-hand one: if the right-hand inequality holds, we may also bound
exp D(x) = exp D(x+y y) exp D(x+y)exp(100y ), (121)
≤ ≤ ≤
− ≤ | |
and rearranging gives the left-hand inequality.
To prove the remaining right-hand inequality, we rewrite
exp D(x+y)
log ≤ = logexp D(x+y) logexp D(x)
≤ ≤
exp D(x) −
≤
x+y d
= logexp D(t)dt
≤
dt
Zx
x+y exp D 1(t)
≤ −
= dt
exp D(t)
Zx ≤
x+y tD
= 1 D! dt
− exp D(t)
Zx ≤ !
x+y tD
= y dt (122)
−
Zx
0∞exp( −s)(t+s)Dds
We will show that the quantities R
tD
I (t) := dt (123)
D ∞exp( s)(t+s)Dds
0 −
R
47insidetheremainingintegralareuniformlyboundedaboveforallt RandD 0even. Fixsomet
0
∈ ≥
tobechosen later. WefirstnotethatI D(0) = 1,andfort > 0wehaveI D(t)
≤
1/( 0∞exp( −s)ds)=
1. So, we may restrict our attention to t < 0. Let us write this as I ( t) for t > 0. We have
D
− R
1 s D
∞
= exp( s) 1 ds, (124)
I ( t) − − t
D
−
Z0
(cid:16) (cid:17)
and it suffices to bound this quantity from below. Set a further constant C > 0 to be fixed later.
We consider three cases.
Case 1, t t : We may bound
0
≤
1 s D
∞
exp( s) 1 ds
I ( t) ≥ − − t
D
−
Z2t
(cid:16) (cid:17)
∞
exp( s)ds
≥ −
Z2t
= 1 exp( 2t)
− −
1 exp( 2t ). (125)
0
≥ − −
Case 2, t > t and D Ct: We may bound using that (1 r)D 1 Dr,
0
≤ − ≥ −
1 t/D Ds
exp( s) 1 ds
I ( t) ≥ − − t
D − Z0 (cid:18) (cid:19)
t/D D t/D
= exp( s)ds sexp( s)ds
− − t −
Z0 Z0
t D t t
= 1 exp 1 exp +1
− −D − t − −D D
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:18) (cid:19)(cid:19)
This is an increasing function of t/D, on t/D 0, increasing from 0 to 1 as t/D . Thus, since
≥ → ∞
under the assumption of this case t/D 1/C, our expression will be bounded below as
≥
1 1 1
1 exp C 1 exp +1 . (126)
≥ − C − − −C C
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:18) (cid:19)(cid:19)
Case 3, t > t and D > Ct:
0
1 s D
∞
exp( s) 1 ds
I ( t) ≥ − t −
D
−
Z2t
(cid:16) (cid:17)
s D
∞
= exp( 2t) exp( s) 1+ ds
− − t
Z0
(cid:16) (cid:17)
s Ct
∞
exp( 2t) exp( s) 1+ ds
≥ − − t
Z0
(cid:16) (cid:17)
t+1 s Ct
exp( 2t) exp( s) 1+ ds
≥ − − t
Zt
(cid:16) (cid:17)
exp( 2t) exp( t 1)2Ct
≥ − · − −
= exp(t(Clog(2) 3) 1)
− −
so, so long as C > 3/log(2), we have
exp(t (Clog(2) 3) 1). (127)
0
≥ − −
48Concretely, taking t = C = 5 and evaluating our lower bounds in the three cases gives that,
0
for all D 0 even and t R, I (t) 99. Plugging into our original calculations, we find that
D
≥ ∈ ≤
exp D(x+y)
≤
log 100y , (128)
exp D(x) ≤ | |
≤
and the result follows.
C.4 Binomial coefficients: Proof of Proposition 5.6
Proof of Proposition 5.6. We first expand directly:
k 1
= k(k 1) (k t+1)
t t! − ··· −
(cid:18) (cid:19)
kt 1 t 1
= 1 1 1 −
t! − k ··· − k
(cid:18) (cid:19) (cid:18) (cid:19)
kt t −1 i
= exp log 1
t! − k
!
i=0 (cid:18) (cid:19)
X
and for the remaining sum, note that log(1 x) 2x for all 0 x 1, so
− ≥ − ≤ ≤ 2
kt 2 t −1
exp i
≥ t! −k
!
i=0
X
kt t2
exp , (129)
≥ t! −k
(cid:18) (cid:19)
as claimed.
C.5 Polynomial advantage in spiked tensor model: Proof of Proposition 6.3
Proof of Proposition 6.3. We give the argument for the case π = Unif( 1 ), which is easily gen-
{± }
eralized to other bounded distributions for the prior entries. By the proof in Section 3.1.1 of
[KWB22], we have
D 2
− d
Adv ( , ) λ2qq/2+1(2D)(q 2)/2
D 2 n −
≤ − X P ≤
Xd=0 (cid:16) (cid:17)
By taking b large enough, since we assume λ b D (q 2)/4, we may ensure the quantity being
q q − −
≥
raised to powers is greater than 2. Thus,
D 2
2 λ2qq/2+1(2D)(q 2)/2 − . (130)
−
≤
(cid:16) (cid:17)
On the other hand, by the proof in Section 3.1.2 of [KWB22], we have
D
Adv ( , ) λ2e qqq/2D(q 2)/2 , (131)
D n − −
≤ X P ≥
(cid:16) (cid:17)
and combining the two inequalities gives the result upon taking b large enough.
q
49