Fairness Feedback Loops:
Training on Synthetic Data Amplifies Bias
Sierra Wyllie ∗ 1, Ilia Shumailov2, and Nicolas Papernot1
1University of Toronto and Vector Institute
2University of Oxford
February 5, 2024
Abstract
Model-induceddistributionshifts(MIDS)occuraspreviousmodeloutputspollutenewmodeltraining
sets over generations of models. This is known as model collapse in the case of generative models, and
performative prediction or unfairness feedback loops for supervised models. When a model induces a
distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data
ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations,
finding that they can lead to loss in performance, fairness, and minoritized group representation, even
in initially unbiased datasets. Despite these negative consequences, we identify how models might be
used for positive, intentional, interventions in their data ecosystems, providing redress for historical
discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions
by curating representative training batches for stochastic gradient descent to demonstrate how AR can
improveupontheunfairnessesofmodelsanddataecosystemssubjecttootherMIDS.Ourworktakesan
important step towards identifying, mitigating, and taking accountability for the unfair feedback loops
enabled by the idea that ML systems are inherently neutral and objective.
1 Introduction
Fairness feedback loops have posed problems for both machine learning practitioners’ models and societies’
policies for some time. One example are the 1930s Home Owner Loan Corporation Security Maps rediscov-
eredbyhistorianKennethT.Jacksoninthe1980s. Thesedepict‘redlining,’ whereminoritizedcommunities
(especially Black and Jewish people) were discriminated against in housing in the United States [Adminis-
tration, 1938, Nelson et al., 2020, Jackson, 1985]. These maps were likely used by government and banks to
determine which neighborhoods should be provided programs and loans, feeding a feedback loop of segre-
gation, limited Black home ownership, environmental racism, and increasing median household income gap
betweenBlackandwhitefamilies[Shkembietal.,2022,EinhornandLewis,2021]. Morerecently,automated
systemsusedforpoliciessuchasloaneligibilityandapprovalprediction[Wu,2022]riskfurtherentrenchment
of inequitable feedback loops [Davis et al., 2021].
In the machine learning fairness community, this effect is known as performative prediction [Perdomo
et al., 2020] or fairness feedback loops [Lum and Isaac, 2016], where the errors and behaviors of a model
influenceitsfutureinputs,causingrunawayunfairness[Ensignetal.,2017]. Ineconomicsthisisknownasthe
performativity thesis, where economic theories attempting to describe markets instead shape them [Callon,
1998]. Increased attention to these effects and the proliferation of generated content on the internet, has
createdterminologyforadataset‘ecosystem.’ Theseecosystemsmaysufferfrom‘syntheticdataspills,’such
∗Correspondingauthor: sierra@wyllie.net
1
4202
raM
21
]GL.sc[
1v75870.3042:viXraas unrealistic AI-generated images of baby peacocks polluting and dominating the image search results for
real peachicks [Shah and Bender, 2023]. Despite the harms that models might cause their data ecosystems,
practitioners lack understanding of the mechanics of these distribution shifts. This can lead to unawareness
of the distribution shifts, especially when multiple models participate in the data ecosystem, and a lack of
understanding of the fairness and equity harms that may arise.
In this work we introduce model-induced distribution shift (MIDS) to describe model-induced changes to
thedataecosystem. Thereareseveralphenomenainexistingliteraturethatwere-specifyasMIDS;asubset
ofdistributionshiftswhicharecausedbypastgenerationsofmodel(mis)behaviours(in)advertentlyimpact-
ingsuccessivegenerations. EachMIDSentailsamodelcausingagradualchangeinthedataecosystem;such
as when synthetic data is published to the web and re-scraped to form new training sets. Once re-scraped,
this polluted data becomes the ground truth for future generations of models, and the MIDS continues.
We first unify several existing MIDS into a common framework, allowing a more nuanced understanding
of their common causes and enabling analysis even where multiple MIDS occur in the same data ecosys-
tem. We analyze the model behavior and fairness impacts of MIDS continuing over generations of models;
including supervised classification models trained with labels sourced from their predecessors’ predictions,
and generative models trained from synthetic data created by their predecessors’ outputs. We evaluate the
performance and a variety of properties that may indicate the fairness of these models in each generation;
finding disproportionately negative impacts on minoritized groups. We find that chains of generative mod-
els eventually converge to the majority and amplify model mistakes that eventually come to dominate and
degrade the data until little information from the original distribution remains, causing representational
disparity between sensitive groups. We identify similar trends in chains of supervised classification models,
showingthatharmscanariseevenifdistributionshiftoccursthroughthelabelsalone. Theseharmsarealso
presentwhereMIDSco-occur; achainofgenerativeandsupervisedmodelscaninteractwhenthegenerators
provide training data for the classifiers.
In contrast to these harms, we study a conceptual framework introduced in Davis et al. [2021] called
algorithmicreparation (AR).Whilenotstrictlylimitedtoalgorithmicchanges,ARusesMLmodelstoprovide
redress for past harms to people with marginalized intersectional identities. For example, if attempting
reparative predictive policing for Black communities, AR interventions in models could include re-weighting
marginalized peoples’ records in a dataset to compensate for over-representation in policing, increasing the
modelthresholdforthedetrimentalprediction,orperhapsremovingthepredictivesystementirely[Humerick,
2019]. Because AR operates in settings where the ‘ground truth’ is of questionable validity (due to current
and historical discrimination), it provides a valuable avenue to provide reparation, and also to counter the
injustices of MIDS. Of course, AR interventions also create model-induced distribution shifts; AR co-opts
the mechanics of other MIDS to promote equity. To that effect, we simulate the effects of AR interventions
through progressive intersectional categorical sampling, showing how prioritising representation lessens the
unfair impacts of coexisting MIDS and their discriminative data ecosystems.
In summary, we make the following contributions:
• We define a new term, model-induced distribution shift (MIDS), to unify several distribution shifts
under one concept, and explore empirical settings to illustrate their impact. This unification draws
attention to the common causes of MIDS and enables analysis even where MIDS co-occur.
• We use our settings to evaluate the impact of the fairness feedback loop and model collapse MIDS in
several datasets, including face CelebA and FairFace datasets. We find that MIDS can lead to poor
performance within a few generations of models, causing class imbalance, a lack of minoritized group
representation, and unfairness. For example, our experiments on CelebA undergoing model collapse
and performative prediction leads to a 15% drop in accuracy and complete erasure of the minoritized
group.
• We position algorithmic reparation as an intentional MIDS with the goal of using model impact to
promoteequityandjusticeinthebroaderdatasetting. Wecreateanalgorithm,STratifiedAR(STAR)
to simulate AR interventions by making training representative of intersectional identities. These
2simulations demonstrate how AR interventions can lessen disparate impact between sensitive groups
and combat the unfair effects of other MIDS.
2 Background
Several terms in existing literature describe distribution shifts perpetuated by models. We provide an
overviewoftheseMIDS,theirenablers,andtheirrelationshipstoFairnessinML(FML,acronymfromDavis
et al. [2021]), then also connect algorithmic reparation to MIDS.
2.1 What are MIDS?
MIDS Model action/property ∆ Data Ecosystem
Fairness Feedback Loops Predictions become outcomes
Model predictions
Performative Prediction and future labels
Synthetic data in ecosystem
Model Collapse Generated outputs
become new inputs
Disparity Amplification Poor utility for marginalized groups Marginalized groups leave
Table 1: MIDS in the existing literature as organized by the model action that induces the MIDS and the
effect on the data ecosystem.
In Table 1, we organize three phenomena from the literature into MIDS by determining how the model
changes the data ecosystem: 1) Performative prediction occurs when a model’s predictions influence out-
comes,suchaswhenrecommendermodelsinfluenceandchangeaperson’spreferences[Perdomoetal.,2020,
Tong et al., 2023]. This is also known as fairness feedback loops when the outcomes of model predictions
entrench bias or discrimination, as in redlining [Green, 2020]. 2) Model collapse may occur due to a similar
phenomenon for generative models. If synthetic outputs are used to train a new generative model, over
the course of several generations of models, the data distribution loses its tails and converges to a point
estimate [Shumailov et al., 2023, Alemohammad et al., 2023]. 3) Disparity amplification occurs due to poor
performance on a group of users. These negatively impacted users disengage from the data ecosystem,
causingrepresentationaldisparity. Iftrainedupon, thealtereddataecosystemcouldleadtoevenworseper-
formancedisparity[Hashimotoetal.,2018]. Whilealloftheseeffectscausedistributionshiftafterdeploying
one model, the changes to the data ecosystem become entrenched as the ground truth if used to train the
next generation of models. Throughout the remainder of the paper, we refer to generations, lineages, or se-
quencesofgenerativeandclassifiermodelstoindicatetheteacher–student(similartoknowledgedistillation,
[Hinton et al., 2015]) model chains underlying these MIDS.
There are other effects, which we refer to as enablers, that provide signal to data ecosystems undergoing
MIDS. If the enabler misrepresents the training distribution to a model, this may bias its behavior and
outputs. Enablers are not innately MIDS, and can include sampling, data annotation, generative feedback,
andpseudo-labellingmethods(forbackgroundmaterialontheseconcepts,seeAppendixB.2). Theseenablers
can also permit MIDS to co-occur: a pseudo-labeling model may annotate synthetic data created from
generative models to use for supervised training, allowing model collapse and fairness feedback loops to
co-occur. Furthermore, if the classifier resulting from the supervised training then impacts humans (or the
non-syntheticportionofthedataecosystem),thenextgenerationsofanyofthesemodelsmayalsobesubject
to disparity amplification. We model these MIDS and their interactions in Section 3. For a review of MIDS
and enablers with examples, see Appendix B.
32.2 Algorithmic Reparation
Algorithmic reparation (AR),introducedinDavisetal.[2021],proposestosubstitutetraditionalframeworks
offairnessinMLwithareparativeapproachtothedesign,development,andevaluationofmachinelearning
systemsforsocialinterventions. ARisprimarilyinspiredbyIntersectionalitytheories, andseekstopromote
justiceinthebroaderdataecosystemthroughinterventionsfromcarefully-trainedmodels. Theseactionsare
not restricted to algorithmic changes; a truly reparative approach requires transdisciplinary collaboration
and a shift of economic, legal, and societal incentives. While AR specifically operates in machine learning,
it encourages reflection on whether use of ML or computation in general may be inappropriate; and if so,
advocates for eliminating these systems.
ARissetasanalternativeframeworktoFML,whichgenerallyattemptstoequatemodelpropertiessuch
as accuracy or positive prediction rate over sensitive groups. Instead, an AR approach focuses not on an
equaldistributionofresourcesandbenefits,butonahighlytask-dependentandpotentiallyunevenallocation
targeted to benefit marginalized intersectional identities in consideration of historical discrimination. This
arisesfromAR’sbasisinIntersectionalitytheories,whichacknowledgesthatharmscompoundatintersecting
marginalized identities (see [Romero, 2017] for an overview and [Collective, 1977] for a prominent example).
AR rejects the notion that equality necessarily begets equity and rejects that technology, including machine
learning,canbeneutralandobjective(see Kapaniaetal.[2023]foradetaileddiscussionofrepresentational
thinking,algorithmicidealism,andalgorithmicobjectivity). Therefore,ARinherentlyquestionsthevalidity
ofthe‘groundtruth’datausedwhentraininganMLsystem;thismakesitacriticalframeworkforaddressing
model-inducedchangestothedataecosystem. FurtherbackgroundonthefoundationalassumptionsofFML
and their critiques which motivate AR may be found in Appendix A.
In this paper, we empirically simulate how intersectional interventions at each model generation may
constitute AR, harm reduction, and better representation. In this perspective, where AR functions as a
MIDS, AR provides data ecosystem maintenance with a focus on reparative justice.
2.3 Related Work
We overview several pertinent related works that study MIDS and how they connect to our work. Perfor-
mative prediction, from Perdomo et al. [2020], is detected by comparing the data generating distribution
before and after a distribution shift caused by a function of the model’s parameters. A performatively op-
timal model minimizes risk on the data distribution that manifests after its own deployment, and can be
approachedbymethodssuchasrepeatedriskminimizationandstochasticgradientupdates[Perdomoetal.,
2020,HardtandMendler-Du¨nner,2023]. Thesediscussionsareusuallyconstrainedtotheimpactsofamodel
after one generation, which we extend over several generations and consider alongside other MIDS.
Another work, Taori and Hashimoto [2023], observe data feedback loops caused as model predictions
contaminate datasets. They provide an upper bound for bias amplification depending on the amount of
synthetic predictions and on whether the model has the same label bias as the original dataset. This
second criteria is met in classifiers that have high uncertainty over the true labels, which follows from
distributionalgeneralization. Webuildonthisworkbyconsideringbiasamplificationduetoachangingdata
ecosystem subject to model collapse and performative prediction, as well as considering fairness impacts
beyond remaining faithful to dataset label bias. While in our results some of our metrics converge and
stabilize, we do not intentionally aim for distributional generalization.
Discussions of model collapse and the impact of generative models on future training sets are frequent in
the natural language processing (NLP) literature, which concludes that removing this data ensures better
future performance as NLP models improve [Rarrick et al., 2011]. More recent work answers questions
on how synthetic data impacts downstream tasks. [Hataya et al., 2022] finds worse downstream classifier
performancewhentrainingfromasyntheticdatasetinsteadoftheoriginal. Evaluationsettingswithmultiple,
connected generative models have since been investigated in [Shumailov et al., 2023, Alemohammad et al.,
2023, Mart´ınez et al., 2023]. Each of these works finds negative impact to utility if there is a sufficient lack
of non-synthetic data. We depart from all of these works by considering the impacts of model collapse on
fairness and equity; we combine the downstream performance task of [Hataya et al., 2022] with the model
4collapse evaluation scheme of [Mart´ınez et al., 2023] and add fairness considerations, as well as co-occuring
MIDS.
The work that introduces disparity amplification, Hashimoto et al. [2018], considers fairness cases where
sensitive information is unavailable. To minimize the risk that the minoritized group incurs high loss and
disengages from the dataset ecosystem, they use distributionally robust optimization (DRO). As mentioned
in their discussions, DRO might not protect minoritized groups so much as some worse-off group (as in
Rawlsian justice), which for our focus on algorithmic reparation and intersectionality is inappropriate.
3 Methodology
Inthissectionweintroducetwosettings,sequencesofclassifiersandsequencesofgenerators,toallowforthe
observationandevaluationofMIDS.Inthesesettings, eachnewmodelinthesequenceis(atleastpartially)
trained using the outputs of its predecessor(s) as inputs and/or labels. As that model is deployed and used,
it propagates MIDS through its own properties and outputs, potentially affecting both the synthetic and
non-syntheticportionsofthedataecosystem. WealsopositionARasanintentionalMIDSaimingforjustice
for historical discrimination and oppression. These settings provide an understanding of model impact over
many generations, enabling informed maintenance of model and data ecosystem ‘health,’ and reinforcing
accountability for model impacts.
3.1 Modeling Assumptions
Measuring distribution shift requires comparison between the current and the reference distribution, which
represents the data ecosystem before the presence of any MIDS. The original reference distribution is given
by H=X ×L×S, where X represents the inputs, and L and S are annotations for the labels and sensitive
attribute(s). Sampling from H gives dataset D =X×L×S. (i.e., via disparity amplification). To compare
the current data ecosystem against the original, we would need access to the original input distribution X
andoraclesforX →LandX →S. Instead,weapproximatethesewithagenerativemodelG andclassifiers
0
A and A , all trained from D. We use these to approximate data from the original distribution and to
L S
annotate the class and group of generated data.
These oracles provide an infinite data source that may be used to train all of the downstream models in
our settings. Therefore, if the oracles misrepresent the distribution, the models trained from their outputs
will experience MIDS relative to the original training distribution. These oracle approximations are not
strongly limiting as we are primarily interested in the effects of MIDS relative to some distribution, be it
the original or its approximation. Using classifiers to annotate generated samples has been used for the
fair training of generative models, though labeling oracle A and sensitive attribute oracle A could also
L S
represent human annotators conducting manual data annotation [Li et al., 2022, Grover et al., 2019b,a,
Hatayaetal.,2022]. Theinitialgenerator,G ,isalsorelevantforscenarioswheresyntheticdataispreferred
0
over human-generated data for a downstream task, which may sometimes arise in FML and privacy [Zemel
et al., 2013, Ganev et al., 2022, NIST, 2018, Stadler et al., 2022]. Sampling from G , as opposed to the
0
training set, also allows a chance at sampling from groups that might not otherwise be well-represented in
the dataset, as in [Zemel et al., 2013].
3.2 Sequential classifiers
Thesequentialclassifier(SeqClass)settingpermitsustopursuethestudyofMIDSsuchasfairnessfeedback
loops and performative prediction, where distribution shift is mediated by classifier predictions becoming
the ground truth of the next generation, as shown in Figure 1. In the first generation, we train a classifier
C bysamplinginputsfromG andlabelingthesewithoracleA . Insubsequentgenerationsi=1...n, the
0 0 L
classifier C is trained on data sampled from G but labeled by the preceding classifier C .
i 0 i−1
Disparityamplificationcanbemodeledbytakingnon-syntheticsamplesh ∼H totrainC ,whereH is
i i i i
the non-synthetic data distribution after models from generation i were deployed. We assume that disparity
5Figure 1: A high-level depiction of sequentially training classifiers (SeqClass setting) for MIDS such as
performativepredictionandrunawayfeedbackloops. Theoraclemodels,A ,A ,andG provideaninfinite
L S 0
source of labels, sensitive group annotations, and inputs. We use these to train classifiers C , where C is
i i
trained using labels from C . To alleviate the harms caused from sequentially training, cla-STAR may
i−1
be used to incorporate sensitive attribute data from A , as shown by the narrowly-dashed green lines.
S
amplification has already influenced the label and sensitive group balances of H via C . Therefore, to get
i i−1
h in practice, we inference C on a held-out subset of D, recording label prediction frequencies over the
i i−1
categories formed from the Cartesian product of the sensitive attribute values and the possible labels. We
use this to define a categorical distribution which we use to perform quota sampling on D. Quota sampling
refers to partitioning a population into strata (in our case defined by label and group intersections) and
selecting from each partition until we reach its quota, which is given by the categorical distribution multi-
plied by the total number of samples we wish to select. Henceforth we refer to this categorical distribution
as a strata. In a nutshell, if C often assigns negative predictions to a minoritized group, then h will
i−1 i
contain a proportional number of minoritized group samples with the negative label. Therefore, C may be
i
influenced by C in two ways: 1) through data labeled by C and 2) through non-synthetic data under-
i−1 i−1
going disparity amplification due to prediction disparity in C . When training C , we sample h ∼ D as
i−1 0 0
disparity amplification has not yet occurred.
Theformulationsareshownbelow,whereT (·;·)istheclassifiertrainingalgorithmtrainedfromthedata
C
in its first argument(s) as labeled by its second argument(s), T is the generator training algorithm, and
G
Sample(·;·) samples from its first argument according to a property (such as group representation) of its
secondargument(s). Thetermscausingperformativepredictionanddisparityamplificationareinred bold
and teal bold face:
C =T (g ,h ;C ), where C =T (g ,h ;A ), G =T (X), and g ∼G (1)
i C i i i−1 0 C 0 0 L 0 G i 0
and h =Sample(D;C ), where h =Sample(D). (2)
i i−1 0
3.3 Sequential generators and classifiers
Thesequentialgenerator(SeqGen)settingprimarilyinvestigatesthemodelcollapseMIDS,wheredistribu-
tionshiftoccursassyntheticdataisusedfortrainingnewmodels,asshowninFigure2. Forthissetting,we
train generators G sequentially from the samples of the preceding generator G , where the first generator
i i−1
G is trained from the original dataset. This chain of generators is the same setting as used by Shumailov
0
etal.[2023]. Departingfromthem, wealsotrainadownstreamclassifierC bysamplinginputsfromG and
i i
labelsfromeitherthelabelingoracleA orfromtheprecedingclassifierC . Theformercaseisthesequen-
L i−1
tial generator and non-sequential classifier setting (henceforth SeqGenNonSeqClass), and the latter the
sequentialgeneratorsequentialclassifiersetting(SeqGenSeqClass). InSeqGenSeqClass,inadditionto
the generators being chained together, the classifiers are chained together and suffer the MIDS described in
the SeqClass setting. These downstream classifiers allow us to initiate the study of downstream classifier
performanceandFMLfairnessmetricswhilealsotrackingthedevolutionofminoritizedgrouprepresentation
6Figure 2: A high-level depiction of sequentially training generators with and without sequential classifiers
(left: SeqGenNonSeqClass, right: SeqGenSeqClass). The oracle models, A , A , and G provide an
L S 0
infinite source of labels, sensitive group annotations, and inputs. We train a lineage of generative models
(G ) and train classifiers C from these, where C is trained using labels from A (left) or C (right).
i i i L i−1
To alleviate the harms caused from sequentially training, cla-STAR and gen-STAR may be used to
incorporate sensitive attribute data from A , as shown by the narrowly-dashed green lines and the broad-
S
dashed purple lines.
and model collapse.
Additionally, disparity amplification due to C and G may impact the non-synthetic data distri-
i−1 i−1
bution, H , that may be used when training G and/or C . For example, if C has poor performance on
i i i i−1
a minoritized group of users, they may choose to disengage from the data ecosystem, meaning H will be
i
less representative, likely harming the next generation of models. Similarly to the SeqClass setting, we
calculate the categorical distribution of C over the sensitive attributes and labels and then quota sample
i−1
proportionally from D to form h . While this directly impacts G , it also impacts C since it trains from G .
i i i i
The formulation for SeqGen with classifiers is shown below, with a substitute model C that may stand
for oracle A or C depending on whether the classifiers are sequential. The terms for performative
L i−1
prediction, model collapse, and disparity amplification are bolded in red, blue, and teal face.
G =T (g ,h ), where G =T (X) and g ∼G (3)
i G i−1 i 0 G i i
C =T (g ,h ;C), where C =T (g ,h ;A ) (4)
i C i i 0 C 0 0 L
h =Sample(D;C ,g ), where h =Sample(D). (5)
i i−1 i−1 0
3.4 Simulating Algorithmic Reparation
In our experiments, we measure and simulate equity-oriented interventions as a change in the discrete
distribution (strata) formed from the Cartesian product of the label and sensitive attribute values. For
example, the strata of the current data ecosystem may be formed from L and S, and the strata of a
classifier C may be formed from its predictions (on data sampled from G or taken from X) and sensitive
i i
group annotations (from oracle A or data S). We use strata to characterize the intersectional and label
S
distributions of training sets, where these strata can change over the generations due to MIDS.
To simulate the effects of AR interventions, we introduce an algorithm called STratified sampling AR
(STAR; see Algorithm 1). STAR creates model training batches by taking a biased sample according to
these strata (within a resampling budget). For our simulations, we use a uniform distribution over these
categoriestogiveaquotaforthenumberofsamplesfromeachcategorythatshouldbepresentinthebatch.
As we are not experts in AR and do not provide a case study, we use a uniform distribution as the target
for the biased sampling. Using an ‘ideal’ (finite) distribution may be inappropriate as a distribution cannot
neutrally or objectively determine the ‘best’ mixture of demographics for a task. Due to this concession,
we will refer to these measurements as fairness/unfairness (in the FML sense), as we cannot make claims
of equity or justice without considering the myriad sources of bias in the ML life cycle [Suresh and Guttag,
2021a] and the specific data ecosystem.
7In the SeqClass setting, we use the name Classifier-STAR, or cla-STAR, to refer to creating more
intersectionally representative batches for the classifiers in the lineage. This is not the only avenue for AR,
but is inspired by work done by the FML community for performative prediction [Ensign et al., 2017]. To
train C , cla-STAR labels generated outputs from G using C and sensitive attribute oracle A , then
i 0 i−1 S
selects a subset of these samples for each training batch such that each label and group category in the
batch meets the quota set by the fairness ideal. We update T to T and show the additional labeling and
C A,C
sensitive group annotations from C and A in green:
i−1 S
C =T (g ,h ;C ,A ), where C =T (g ,h ;A ,A ). (6)
i A,C i i i−1 S 0 A,C 0 0 L S
STAR in SeqGenSeqClass may occur at all the same points described above, with the addition of
interventionstakenwhiletrainingthegenerators. Weexaminebothclassifier-sideSTAR(cla-STAR,taken
whiletrainingclassifiers,showningreen),andgenerator-sideSTAR(gen-STAR,whiletraininggenerators,
shown in purple). gen-STAR also uses annotations from C and sensitive attribute oracle A to fill the
i−1 S
label and group category quotas set by the fairness ideal. Both cla-STAR and gen-STAR are described
in Equation (7) and Equation (8), respectively:
C =T (g ,h ;C,A ), where C =T (g ,h ;A ,A ) (7)
i A,C i i S 0 A,C 0 0 L S
G =T (g ,h ;C,A ), where G =T (X;L,S). (8)
i A,G i−1 i S 0 A,G
3.4.1 STAR Implementation
We simulate algorithmic reparation using the two variants of STAR introduced in Section 3.4; cla-STAR
and gen-STAR. The algorithm uses sampling and pseudo-labelling to create training batches of size b that
meet the fairness ideal by having a prescribed number of samples to fill a quota in each category. However,
the closeness between this fairness ideal and the resulting strata of the batch is bounded by the reparation
budgetr,whichsimulatescoststoconductingreparation. Forourexperiments,weuseauniformdistribution
as the fairness ideal.
STAR creates a pool of b+r samples from either the previous generator or the original dataset, which
is then annotated by A and either A or C . The fairness ideal, multiplied by b, gives a quota for the
S L i−1
number of samples ideally belonging to each category. STAR then attempts to populate each category to
its quota from the pool of samples. If after this initial populating, some of the categories did not meet their
quota, the remainder of the batch is populated with randomly selected samples from the remaining pool.
This process (henceforth re-sampling) will most likely add samples representative of the majority group and
class. There are therefore two barriers to meaningful reparation: 1) we cap the number of samples that
may be drawn to form the batch yet attempt to create equal categories from an unequal dataset; and 2) the
effects of MIDS. If STAR increases the representation of a minoritized group, then these samples may be
re-selected more often than majoritized group peers, increasing their exposure to mislabeling. Additionally,
thehighernumberofgenerationsthisdataissubjectedtomayacceleratethemodelcollapseforthesegroups.
STARisshownforbinarylabelsandbinarysensitiveattribute(4categoriesinthestrata)inAlgorithm1.
4 Evaluation
We conduct two main sets of experiments to illustrate MIDS in the SeqClass and SeqGenSeqClass
settings. Our results seek to answer several questions which we formalize and answer in brief:
Q1) What are the effects of MIDS on performance, representation, and fairness? In both
SeqClass and SeqGenSeqClass, we find that the performative prediction, model collapse, and disparity
amplification MIDS lead to a loss of accuracy, fairness, and representation in classes and/or groups. These
effects are more pronounced in SeqGenSeqClass, likely because model collapse sometimes results in the
beneficial class and majoritized group dominating the generated samples. These effects are more severe in
data ecosystems with higher proportions of synthetic data, which we ablate in Appendix D.2.
8Q2) Why is it important to be aware of MIDS? We find that unawareness of MIDS in either setting
results in overstating the accuracy and fairness, which can be observed by measuring the relative perfor-
manceofclassifiers(comparingC againstlabelsprovidedbyC )insteadoftheoriginaldatadistribution.
i i−1
In SeqClass, relative results show nearly 100% accuracy and near-perfect fairness (using equalized odds
difference),thesameholdsforSeqGenSeqClasswiththeadditionofmis-reportedclassandgroupbalances
(see Appendix G).
Q3) How do MIDS interact? We compare SeqGenSeqClass and SeqGenNonSeqClass, revealing
thatthefairnessfeedbackloopintheformerallowstheclassifierstoadapttodistributionshiftintheinputs
caused by model collapse. This co-operation lessens the rate and degree of classifier performance decline.
When training with a mixture of synthetic and non-synthetic data, we observe that the non-synthetic data
greatly slows the degree of model collapse, though also enables disparity amplification amongst groups in
the non-synthetic data ecosystem.
Q4) Can AR interventions alleviate the harms of MIDS? Our AR interventions using STAR lessen
theseunfairbehaviorsandachievebetterdownstreamclassifierfairness,especiallyinSeqClass. cla-STAR
reduces harms in SeqClass and our experiments on disparity amplification, where we train with a mixture
ofsyntheticandnon-syntheticdata. For100%synthetictraininginSeqGenSeqClass,gen-STARusually
performsbetterthancla-STAR,likelyduetothestrengthofthemodelcollapseMIDSindeterioratingthe
data ecosystem.
4.1 Modeling MIDS
4.1.1 Experimental Setup
We provide computer vision experiments for four datasets. We modify MNIST and SVHN into ColoredMNIST
andColoredSVHNbyaddingcolortocreatebinarysensitivegroupsandbyformingtwoclassesfordigits<5
and≥5. Wechoosethebeneficialclassastheclassconvergedtobymodelcollapse,andbiasthemajoritized
group towards it. These arbitrary choices simplify our presentation; we vary the class and group balance in
AppendixD.1,findinglittleimpact. WealsouseFairFaceandCelebAforamorecomplicatedandreal-world
task, but also contrast the fairness of MIDS on datasets with and without group balance. For CelebA, our
taskistopredictattractivenesswithgenderasthesensitiveattribute;theseattributeshavewell-documented
errorsanddisparities[Lingenfelteretal.,2022]. ForFairFace, weattempttopredictgender(2values)with
sensitive attributes race (7 values) and age (which we binarize at < 30, ≥ 30). Note that our FairFace
experimentsareintersectional,andforA weuseadifferentclassifierforeachsensitiveattribute. ForCelebA
S
and FairFace, we provide between 5-10 generations,1 for ColoredMNIST and ColoredSVHN we train for 40
generations. When training each generation, all synthetic data is sampled from the prior generator and/or
classifier/annotator, while non-synthetic data is taken from the training distribution.
Further details on the datasets (including their class and sensitive attribute distributions) and model
architectures, hyperparameters, and compute specifics are in Appendix C.2 Loss values for generators may
be found in Figure 13, and accuracies and fairnesses for A and A are in Table 3. These performances
L S
reflect baseline results of training without fairness optimization. For STAR, we set the reparation budget r
at25%ofb,thebatchsize,foralldatasetsasidefromColoredSVHN,whichissetto33%ofb. Thisbudgetwas
tuned via grid search by finding the smallest reparation budget that results in a decrease in the proportion
of each batch resampled over generations, indicating that STAR is changing the data ecosystem towards its
ideal.
In both settings, we also experiment with training models from a 50-50 mixture of synthetic and non-
synthetic data. This allows us to observe the effects of disparity amplification as it co-occurs with perfor-
mative prediction and model collapse. We use this data mixture to train the classifiers in SeqClass, and
the generators in SeqGenSeqClass where we observe downstream impacts in the classifiers.
1ThetimetorepeatedlytrainCelebAandFairFacefromscratchtookaroundaweek,andduetothecostandCO2footprint,
weelectedtoterminatetheseexperimentsuponrealizationoftheMIDS.
2Ourcodeishostedanonymouslyhere: https://anonymous.4open.science/r/FairFeedbackLoops-1053/README.mdandwill
bereleasedtoGitHubifacceptedforpublication.
9MIDS Metrics. To measure MIDS and their fairness impacts, we inspect the original dataset, generated
outputs and annotations, and model performances. We use a held-out evaluation set i.i.d. from the original
training set D. In the classifiers, we measure fairness using demographic parity difference (DP), equalized
odds difference (EOdds), and group accuracy gaps, and measure utility with accuracy. DP difference (Def-
inition 1) compares the positive prediction rates between groups. EOdds difference (Definition 2) is the
maximum between two values: the difference between the groups’ true positive rates, or between their false
positiverates. Forperformancedisparity,wereportthemaximumaccuracygapwhencomparingallsensitive
groups. For these metrics, a lower value (less difference between groups) indicates more fairness. Note that
accuracy and EOdds require a ground truth label which may be taken from a biased original distribution.
Therefore, to meet other fairness objectives, such as in STAR, EOdds and group accuracy differences may
worsen as the label distribution changes. We measure these metrics on the evaluation set, but also between
successive classifiers using images from G or G with sensitive annotations from A and labels from C .
0 i S i−1
The difference between the former (measuring with respect to D) and the latter (measuring with respect
to the preceding models) shows how model performances can be misreported if the evaluator is unaware of
MIDS.Totracktheclassandgrouprepresentationofthegenerators,wegenerate1000samplesandannotate
class and group with A and A .
L S
MIDS strata. We also observe the strata of the original dataset (using X, L, and S), the model training
batchstrata(usingG orG , A orC , andA ), andthemodeloutputstrata(classifiersusingX, C ,
0 i L i−1 S i
and A , generators using (G , A , and A ). We also record the Kullback–Leibler (KL) Divergence between
S i L S
these strata and the fairness ideal used in STAR. These KL-Divergence results provide a simple way to
measure and visualize change in intersectional representation for our experiments; a ‘fairness ideal’ is other-
wiseinappropriate(seeSection3.4forclarityonrepresentationalthinking). Wealsomeasuretheprogression
of STAR through the strata it achieves during batch curation, the amount of resampling required when
categories fail to meet their quotas, and the KL-Divergence between the strata and the fairness ideal.
4.2 Results
4.2.1 Sequential classifier setting
OurfirstexperimentsuiteusesSeqClassasdescribedinSection3.2;MIDSoccurasaclassifier’spredictions
areusedtolabelthenextgeneration’sclassifier. InColoredMNISTandColoredSVHN(Figures3and 15),we
observeanaccuracydropof10-15%over40generations,withanincreaseinbothDPandEOddsunfairnesses
(in the case of ColoredSVHN, both metrics increased by roughly 0.2, where the maximum unfairness gap is
1). CelebA immediately suffers near-random classifier performance as G misrepresents D by incurring
0
significant class imbalance towards the detrimental class (Figure 16). FairFace classifier accuracies drop
from 57% to random accuracy within 10 generations, and also incur an accuracy difference increase of .1,
with a .2 jump in EOdds unfairness (Figure 17). Note that these performances would likely worsen with
additional generations.
STAR reduces performance degradation from MIDS. In ColoredMNIST, ColoredSVHN, and CelebA
cla-STAR lead to a significant reduction in DP and EOdds unfairness, and lessened the gap between cla-
STAR’s fairness ideal and the data ecosystem strata (Figures 3, 15, and 16). Across most datasets,
there is far less variance compared to results without reparation, where variance grows with the number
of generations (all figures report the 95% confidence interval). For FairFace, classifier strata without
reparation are constituted primarily of older white males (where younger white males are the plurality of
the dataset, see Figure 7). With reparation, the representation of young white non-males increases, but as
G fails to adequately generate samples from the other races, there are still large performance disparities
0
and high unfairnesses (Figure 17). See Appendix F.1.1 for detailed figures on the representation of classes
and groups in training batches. Due to the high representational disparity between the white race and the
other6races, cla-STARdidnotleadtobetterfairness. WealsoobservetensionbetweenFMLmetrics: for
ColoredMNIST (Figure 3), the EOdds and accuracy difference increase after generation 15, while both DP
andtheKL-divergencecontinuetodecrease. AsEOddsissatisfiedwhenerrorrates(relativetoapotentially
biased dataset) are similar across groups, meeting the STAR fairness ideal leads to an ‘unfair’ allocation of
10beneficial labels to the minoritized class, and of detrimental labels to the majoritized class.
Non-synthetic data slows MIDS, including cla-STAR.We also trained classifiers on an even mixture
of synthetic and non-synthetic data as described in Section 3.2, see Figure 28. Unsurprisingly, adding non-
synthetic datagreatly improved the performance of classifierscompared to resultswith 100%syntheticdata
and no reparation (see a full ablation of the amount of synthetic data in Appendix D). While we were able
to further increase this fairness by using cla-STAR, the impact was far less than on the 100% synthetic
results, with FML unfairness metrics converging to higher values at around the 25th generation. Because
the non-synthetic data lessens the impact of the pseudo-labelling enabler in the fairness feedback loop, it
likewise lessens the impact of cla-STAR.
cla-STAR cla-STAR
0.92 No reparation 0.03 No reparation 0.400
0.90 0.375
0.88 0.02 0.350
0.86 0.325
0.01
0.84 0.300 cla-STAR
No reparation
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
100
0.25 cla-STAR Major, Class 0 0.12 cla-STAR
No reparation 80 Minor, Class 0 No reparation 0.20 M Ma aj jo or r, , C Cl la as ss s 1 1 0.10
60
0.15
0.08
40
0.10
0.06
0.05 20
0.04
0.00 0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 3: ColoredMNIST results for SeqClass on the evaluation set. Top: accuracy, accuracy difference,
anddemographicparitydifference. Betterfairness(lowerfairnessdifference)andhigheraccuracyisachieved
with cla-STAR. Bottom: equalized odds difference, the strata created during cla-STAR, and the KL-
divergence between cla-STAR fairness ideal and classifier strata. The KL-Divergence decreases with
cla-STAR, indicating more fairness, and the batches become more balanced across group and class. The
accuracy difference and EOdds difference between groups is small, but increases during reparation due to
metric tension with cla-STAR.
4.2.2 Sequential generator and classifier setting
TheseexperimentsrefertotheSeqGenSeqClasssettingdescribedinSection3.3anddepictedinFigure2,
which we use to depict model collapse and performative prediction, with additional results including the
effects of disparity amplification. In our 100% synthetic training experiments, we observe model collapse
deteriorates the data, leading to class imbalance (ColoredMNIST Figure 19, CelebA Figure 5) and/or to
group imbalance (ColoredSVHN Figure 4, CelebA Figure 20). As model collapse progresses, the downstream
classifiers either perform with random accuracy or constantly predict the beneficial class label, leading to
poor fairness. Refer to Appendix E to see generated samples undergoing model collapse.
If we judge model collapse at the point when the generated data ceases to have any downstream utility,
model collapse occurs at generation 15 for ColoredMNIST, 5 for ColoredSVHN, and between generations 1-5
for CelebA and FairFace. These values correspond to the increasing difficulty of the datasets’ tasks, which
is correlated with heavy-tailedness in their distributions [Meng and Yao, 2023]. A small sample size may be
able to represent a concentrated distribution, but finite samples of a heavy-tailed distribution will likely be
11
ycaruccA
ecnereffiD
sddOE
% seirogetaC
ecnereffiD
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
PDbiased, leading to faster distribution shift. The steep decline to random accuracy is likely due in part to the
amount of synthetic data used to train each generation. As the amount of synthetic training data decreases,
so too does the rate of accuracy decline and beneficial class dominance, as shown in Appendix D.2.
Unpredictableconvergenceofmodelcollapse. Asmodelcollapseprogresses,itisdifficulttopredictthe
category of the strata in the generators that dominates. For example, in generation 40 of ColoredMNIST,
both majoritized and minoritized groups of the beneficial class each constitute around 40% of each batch,
whereas in ColoredSVHN, the majoritized and beneficial category alone constitutes 60% of each batch (Fig-
ures 19 and 4). This is in part due to the initial class and label balances (see Table 2), but also due to how
the model collapse manifests. For example, ColoredMNIST eventually converges to samples that resemble
an ‘8’, or all the digits superimposed (see samples from model collapse in Appendix E). As this happens to
fall in the advantaged class, it becomes dominant in the generators, without strongly impacting the group
balance. Meanwhile, in CelebA, model outputs become dominated by the majoritized group, yet these same
samples are also classified into the detrimental class by A , which is counter to the original label balance in
S
the dataset. Eventually, minoritized group representation in CelebA falls to 0% (see Figure 5). FairFace
hasmuchdeteriorationinthedatabutmaintainsbothclassandgroupbalance. Theseobservationssupport
a growing consensus that the features of the original data preserved by generative models in synthetic data
is difficult to predict, or highly data dependent (for private synthetic data [Stadler et al., 2022], and at the
intersection of fairness and privacy in synthetic data [Cheng et al., 2021]).
Performative prediction adapts to model collapse. We also uncover co-operation between MIDS
by evaluating the role of sequential classifiers in SeqGenSeqClass and SeqGenNonSeqClass (see Ap-
pendixD.3). InColoredMNISTandColoredSVHN(Figures11and12): thenon-sequentialclassifiersconverge
toaccuracies10-20percentagepointslowerthanthesequentialclassifiers. However,thesequentialclassifiers
have considerably more unfairness (in the case of ColoredMNIST, by 0.6), likely due to their participation in
fairness feedback loops. Performative prediction among sequential classifiers allows C to provide mean-
i−1
ingful labels for training C from G . For the non-sequential classifiers, A cannot adequately support the
i i L
distributionrepresentedbyG oncetheith distributionsubstantiallydiffersfromtheoriginal. Theinherited
i
knowledge of P(Y|X) passed through the sequential classifiers allows them to preserve a more accurate map
from the changing distribution to the classes.
gen-STAR improves fairness and minoritized representation. Betweengen-STARandcla-STAR,
the former leads to more preservation of the group and label balance in all four datasets. This result fits
intuitivelyasthebiasedsamplingenablesthesegeneratorstomaintainmorebalancedrepresentationsacross
the categories. For example, gen-STAR leads to better fairness than cla-STAR in ColoredMNIST and
CelebA, though with cost to accuracy. However, because gen-STAR results in oversampling minority (in
terms of population) categories relative to the original dataset, it may also expose these areas of the data
distribution more to model collapse. In ColoredSVHN, for example, gen-STAR results in more balanced
strata in both the generators and classifiers (compared to cla-STAR), but the classifier strata are still
dominated by the (minoritized, detrimental) and (majoritized, beneficial) categories, leading to worse DP
and EOdds fairness (see Appendix F.2.1). In the case of FairFace, a combination of oracle model bias and
unrepresentative generators causes large disparities between races as gen-STAR cannot adequately sample
from the smallest intersectional minorities. As FairFace has relatively balanced races and genders, these
disparitiesindicatethatalgorithmicreparationshouldbeconsideredwhencollectingdata,andmightrequire
action beyond collecting balanced quotas of data from various groups. Overall, cla-STAR did not show
consistent performance across datasets, achieving worse or equivalent performance to the non-reparative
results, likely due to the strength of the model collapse MIDS.
Disparity amplification reduced with cla-STAR. Recall that we model disparity amplification by
sampling non-synthetic data using the strata of the classifiers, which we use for half the training data
for the generators (the other half is sampled from G ). We evaluate this setting for ColoredMNIST.
i−1
Similarlyto SeqClass, thenon-synthetic data slows MIDS causedby syntheticdata spills, includingmodel
collapse. We evaluate the fairness performance of gen-STAR and cla-STAR, finding substantially better
performance and fairness with cla-STAR (subject to an increase in accuracy disparity due to increased
false negatives) than with gen-STAR (see Appendix H). The gen-STAR generator strata never achieve
120.9 No reparation 1.0 No reparation 1.0 No reparation
0.8 c gl ea n-S -ST TA AR R 0.3 0.8 c gl ea n-S -ST TA AR R 0.8 c gl ea n-S -ST TA AR R
0.7 0.2 0.6 0.6
0.6 0.4 0.4
0.1 No reparation
0.5 cla-STAR 0.2 0.2
gen-STAR
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
0.8 No reparation 0.8 0.7 0.6 cla-STAR 0.6
0.6 gen-STAR 0.7 0.5
0.4
0.4 0.6 0.4
0.2 No reparation 0.2 0.5 No reparation 0.3 No reparation
cla-STAR cla-STAR cla-STAR
gen-STAR gen-STAR 0.2 gen-STAR
0.0 0.4
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
Figure 4: ColoredSVHN results for SeqGenSeqClass. Top: shows accuracy, accuracy difference, demo-
graphic parity difference, and equalized odds difference. For the latter three, lower values are better. Bot-
tom: KL-Divergencebetweenfairnessidealandclassifiers,andbetweenfairnessidealandgeneratorstrata,
the class balance, and group balance. Shading shows collapsed generations. We observe that gen-STAR
providesmoreminoritizedgrouprepresentation. Whilemodelcollapsecausesoutputstoeventuallyresemble
a ‘3,’ which moves class balance towards the beneficial class, gen-STAR also maintains the original dataset
imbalance of 60%.
the fairness ideal as randomly sampling from initially biased generators leads to unfair classifiers which
propagates disparity amplification in the non-synthetic data, which incidentally only protects the majority
group and class categories from model collapse. Meanwhile, cla-STAR trains classifiers with more ideal
strata that reverses disparity amplification, providing balanced non-synthetic data to the generators and
protecting all classes and groups equally from model collapse. We may see this by comparing the STAR
strata for both algorithms, see Figure 30.
4.3 Limitations
We discuss three main limitations of our work. Firstly, we do not provide a specific use case and cannot
fully evaluate algorithmic reparation, nor make any claims that our achievements in fairness lead to equity
or justice. Secondly, when collecting the synthetic data for training a new model, we operate in a worst case
whereallofitissampledonlyfromtheimmediatelyprecedingmodel(s);withoutprovenanceinformationthe
synthetic data could be sourced from any number of other models, including multiple predecessor models.
Thirdly, in CelebA and FairFace, we rely on race annotations which might fail to represent the various
skintoneswithingroups,animportantconsiderationincomputervisiontasks, andforbetterrepresentation
(as discussed in [Buolamwini and Gebru, 2018]). Additionally, racial categorizations are not universally
consistent, and so these datasets provide a simplification that may be inappropriate.
5 Concluding Remarks
Inthispaper,weintroducedmodel-induceddistributionshifts(MIDS)andcreatedempiricalsettingsenabling
the evaluation of their harms. With these settings we found that MIDS, both on their own and co-occurring
with enablers such as data annotation, lead to major degradation in utility, fairness, and minoritized group
representation. While MIDS can be intentional or unintentional, unawareness of their existence can lead
to grossly overstating model utility and fairness. Based on these harms, we discussed how algorithmic
reparation(fromtheliteratureofcriticaltheoryinML)mayactasanintentionalMIDSwithgoalsofequity
13
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
PD
ssalC
laicifeneB
ecnereffiD
sddOE
puorG
dezitironiMNo reparation 0.35 No reparation No reparation 00 .. 56 70 50 c gl ea n-S -ST TA AR R 0.30 0.15 c gl ea n-S -ST TA AR R 0.10 c gl ea n-S -ST TA AR R
0.550 0.10
0.525 0.25 No reparation 0.05 0.05
0.500 0.20 cla-STAR
0.475 gen-STAR 0.00 0.00
0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8
Generations Generations Generations Generations
No reparation 0.4 No reparation 0.3 cla-STAR cla-STAR
gen-STAR 0.3 gen-STAR
0.2
0.2
0.1 0.1
0.0 0.0
0 2 4 6 8 0 2 4 6 8
Generations Generations
Figure 5: CelebA results for SeqGenSeqClass. Top: shows accuracy, accuracy difference, demographic
paritydifference,andequalizedoddsdifference. Forthelatterthree,lowervaluesarebetter. Bottom: shows
the class balance the and group balance. Shading shows collapsed generations. The performance of the
classifiers was initially low, though STAR is moderately better in later generations. gen-STAR provides
better class and group balance compared to cla-STAR and results without reparation, but is still unable
to achieve uniform representation due to the strength of model collapse.
and justice. By simulating the impacts of algorithmic reparation at various points in our settings, we saw a
lessening in harms.
Wewouldalsoliketoacknowledgethatthroughoutthiswork, wehaveimpliedthatmodelscausemodel-
induced distribution shift. This is not the case; agency over MIDS rests primarily on model owners, data
publishers and collectors, and model users. Several related works, including Shumailov et al. [2023] and
Hardt and Mendler-Du¨nner [2023], have also considered the power or advantage given to an entity that has
more control over the amount of synthetic data spillage or has more access to non-synthetic data. As we
have found that MIDS are of imminent concern in data ecosystems undergoing synthetic data spills; we
now turn to solutions to MIDS and methods for taking accountability of them. One possible solution, also
mentioned in Davis et al. [2021], is an archival perspective on data curation as introduced in Jo and Gebru
[2020]. Specifically, adopting the tenets of archival description codes could enable gathering of high-quality
provenanceinformation,andadoptingthemoralobligationsunderlyingmanyanarchives’raisond’ˆetrecould
helptoidentifyandrepairstructuralandhistoricalbias[DACS,RAD,JoandGebru,2020]. Anothersolution
motivated by our results is the importance of non-synthetic data and human data annotation to prevent or
slow the rate of MIDS. We therefore advocate for more attention to the often-unseen and underappreciated
labor of human data workers. We end with a call for safer conditions for data workers given their current
and increasing importance in our data ecosystems.
14
ycaruccA ecnereffiD
ycaruccA
ssalC
laicifeneB
ecnereffiD
PD
puorG
dezitironiM
ecnereffiD
sddOEStatements
Ethical Considerations
Werecognizethattechnicalsolutionsareneverdisjointfromtheirsocietalimpacts,andhavestriventowards
a more sociotechnical framing for this work. We navigate several definitions and frameworks for algorith-
mic fairness and equity by considering multiple definitions of fairness and their contrasts with algorithmic
reparation. However, we primarily focus on group-based fairness metrics, including when those groups are
formed intersectionally, which we acknowledge can reinforce the ideologies behind them.
Positionality
We are researchers usually operating within the more technical areas of machine learning. Throughout our
time working in this area, we have turned to Data Feminism by D’Ignazio and Klein [2020] to inform our
discussions around fairness and equity, and to inform some of our language choices. We also rely heavily
on Davis et al. [2021] and Kapania et al. [2023] for their critiques of ‘representationalist thinking,’ which
has been admittedly ubiquitous in our education, and which seemingly appears commonly in ML research
(including ML fairness research).
Adverse Impact
We note that this work might be used to fuel despair over the ‘long-term’ existential harms of models,
especially generative models. We advise readers to think critically about the systems of power behind
machine learning and consider the current harms these permit, continue, and worsen. We also acknowledge
that due to our lack of a specific use case to fully evaluate algorithmic reparation, we risk representing it as
a mathematical or technical definition to be satisfied or optimized for. This runs counter to the tenets of
AR (see Davis et al. [2021]) and we have been careful with our language around this area (for example in
Sections 3.4 and 4.3, and in Appendix A).
Acknowledgements
We would like to acknowledge our sponsors, who support our research with financial and in-kind contribu-
tions: CIFAR through the Canada CIFAR AI Chair program and the Catalyst grant program, Microsoft,
and NSERC through the Discovery Grant and COHESA Strategic Alliance. Resources used in preparing
thisresearchwereprovided,inpart,bytheProvinceofOntario,theGovernmentofCanadathroughCIFAR,
and companies sponsoring the Vector Institute. We would like to thank members of the CleverHans Lab for
theirfeedback. WeadditionallythankDavidGlukhov, SyedIshtiaqueAhmed, andRamaravindK.Mothilal
for feedback on earlier versions of this work.
References
90th United States Congress. 82 Stat. 73 - An Act to prescribe penalties for certain acts of violence or in-
timidation, and for other purposes, 1968. URL https://www.hud.gov/sites/dfiles/FHEO/documents/
fairhousingact.pdf.
F. H. Administration. Underwriting Manual: Underwriting and Valuation Procedure Un-
der Title 2 of the National Housing Act. Department of Housing and Urban Devel-
opment, February 1938. URL https://www.huduser.gov/portal/sites/default/files/pdf/
Federal-Housing-Administration-Underwriting-Manual.pdf.
U.Aivodji, H.Arai, O.Fortineau, S.Gambs, S.Hara, andA.Tapp. Fairwashing: theriskofrationalization.
In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on
15Machine Learning,volume97ofProceedings of Machine Learning Research,pages161–170.PMLR,09–15
Jun 2019. URL https://proceedings.mlr.press/v97/aivodji19a.html.
S. Alemohammad, J. Casco-Rodriguez, L. Luzi, A. I. Humayun, H. Babaei, D. LeJeune, A. Siahkoohi, and
R. G. Baraniuk. Self-consuming generative models go MAD, 2023. URL https://arxiv.org/abs/2307.
01850.
M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization, 2020. URL https:
//arxiv.org/abs/1907.02893.
Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jones,A.Chen,A.Goldie,A.Mirhoseini,C.McK-
innon,C.Chen,C.Olsson,C.Olah,D.Hernandez,D.Drain,D.Ganguli,D.Li,E.Tran-Johnson,E.Perez,
J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage,
N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E.
Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman,
Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitu-
tional AI: Harmlessness from AI feedback, 2022. URL https://arxiv.org/abs/2212.08073.
S.J.BellandL.Sagun. Simplicitybiasleadstoamplifiedperformancedisparities. InProceedingsofthe2023
ACMConferenceonFairness,Accountability,andTransparency,FAccT’23,page355–369,NewYork,NY,
USA, 2023. Association for Computing Machinery. ISBN 9798400701924. doi: 10.1145/3593013.3594003.
URL https://doi.org/10.1145/3593013.3594003.
S. Bird, M. Dud´ık, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach, and K. Walker.
Fairlearn: A toolkit for assessing and improving fairness in AI. Technical Report MSR-TR-
2020-32, Microsoft, May 2020. URL https://www.microsoft.com/en-us/research/publication/
fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/.
J. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities in commercial gender
classification. In FAT, 2018. URL https://api.semanticscholar.org/CorpusID:3298854.
T. Calders, F. Kamiran, and M. Pechenizkiy. Building classifiers with independency constraints. In 2009
IEEE International Conference on Data Mining Workshops (ICDM), Miami, Florida, USA, Dec. 2009.
URL https://ieeexplore.ieee.org/document/5360534.
M. Callon. Introduction: The embeddedness of economic markets in economics. The Sociological Review,
46(1 suppl):1–57, 1998. doi: 10.1111/j.1467-954X.1998.tb03468.x. URL https://doi.org/10.1111/j.
1467-954X.1998.tb03468.x.
P. Cascante-Bonilla, F. Tan, Y. Qi, and V. Ordonez. Curriculum labeling: Self-paced pseudo-labeling for
semi-supervised learning. CoRR, abs/2001.06001, 2020. URL https://arxiv.org/abs/2001.06001.
V. Cheng, V. M. Suriyakumar, N. Dullerud, S. Joshi, and M. Ghassemi. Can you fake it until you make
it? impacts of differentially private synthetic data on downstream classification fairness. In Proceedings
of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 149–160,
New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/
3442188.3445879. URL https://doi.org/10.1145/3442188.3445879.
A.Chouldechova.Fairpredictionwithdisparateimpact: Astudyofbiasinrecidivismpredictioninstruments,
2016.
T. C. R. Collective. The combahee river collective statement, April 1977.
S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and A. Huq. Algorithmic decision making and the cost of
fairness. InProceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
16Data Mining, KDD ’17, page 797–806, New York, NY, USA, 2017. Association for Computing Machin-
ery. ISBN 9781450348874. doi: 10.1145/3097983.3098095. URL https://doi.org/10.1145/3097983.
3098095.
DACS. Describing Archives: A Content Standard (DACS), an Implementation of General Interna-
tional Standard Archival Description (ISAD(G)). Standard, Society of American Archivists’ Techni-
cal Subcommittee on Describing Archives: A Content Standard (TS-DACS), May 2023. URL https:
//github.com/saa-ts-dacs/dacs.
J. L. Davis, A. Williams, and M. W. Yang. Algorithmic reparation. Big Data & Society, 8
(2):20539517211044808, 2021. doi: 10.1177/20539517211044808. URL https://doi.org/10.1177/
20539517211044808.
L. Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 29(6):141–142, 2012. URL https://ieeexplore.ieee.org/document/6296535.
N. I. R. Department. COMPAS risk scales : Demonstrating accuracy equity and predictive parity perfor-
mance of the COMPAS risk scales in Broward county, 2016. URL https://api.semanticscholar.org/
CorpusID:51920414.
DetroitDeomgraphics. Thenon-whitepopultionofmetropolitanDetroit,1955. URLhttps://hdl.handle.
net/2027/mdp.39015060547265?urlappend=%3Bseq=21%3Bownerid=13510798897484245-29.
C. D’Ignazio and L. F. Klein. Data feminism. MIT press, 2020.
E. Einhorn and O. Lewis. Built to keep Black from white: Detroit segregation wall still stands, a
stark reminder of racial divisions. NBC News, 2021. URL https://www.nbcnews.com/specials/
detroit-segregation-wall/.
D. Ensign, S. A. Friedler, S. Neville, C. E. Scheidegger, and S. Venkatasubramanian. Runaway feedback
loops in predictive policing. CoRR, abs/1706.09847, 2017. URL http://arxiv.org/abs/1706.09847.
G. Ganev, B. Oprisanu, and E. D. Cristofaro. Robin hood and matthew effects: Differential privacy has
disparate impact on synthetic data. In ICML, pages 6944–6959, 2022. URL https://proceedings.mlr.
press/v162/ganev22a.html.
B.Green. Thefalsepromiseofriskassessments: Epistemicreformandthelimitsoffairness. InProceedingsof
the 2020 Conference on Fairness, Accountability, and Transparency, FAT* ’20, page 594–606, New York,
NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.
3372869. URL https://doi.org/10.1145/3351095.3372869.
A. Grover, K. Choi, T. Singh, R. Shu, and S. Ermon. Fair generative modeling via weak supervision. arXiv
preprint arXiv:1910.12008, 2019a. URL https://arxiv.org/abs/1910.12008.
A. Grover, J. Song, A. Agarwal, K. Tran, A. Kapoor, E. Horvitz, and S. Ermon. Bias correction of
learnedgenerativemodelsusinglikelihood-freeimportanceweighting,2019b.URLhttps://proceedings.
neurips.cc/paper/2019/file/d76d8deea9c19cc9aaf2237d2bf2f785-Paper.pdf.
M. Hardt and C. Mendler-Du¨nner. Performative prediction: Past and future, 2023.
M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. Advances in Neural
Information Processing Systems (NIPS), 29:3315–3323, 2016. URL https://proceedings.neurips.cc/
paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf.
17T. B. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. Fairness without demographics in repeated
loss minimization. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference
onMachineLearning, ICML2018, Stockholmsm¨assan, Stockholm, Sweden, July10-15, 2018,volume80of
Proceedings of Machine Learning Research, pages 1934–1943. PMLR, 2018. URL http://proceedings.
mlr.press/v80/hashimoto18a.html.
R.Hataya,H.Bao,andH.Arai.Willlarge-scalegenerativemodelscorruptfuturedatasets? 2023IEEE/CVF
International Conference on Computer Vision (ICCV), pages 20498–20508, 2022. URL https://api.
semanticscholar.org/CorpusID:253523513.
G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network, 2015.
L.HuandY.Chen. Fairclassificationandsocialwelfare. InProceedings of the 2020 Conference on Fairness,
Accountability, and Transparency, FAccT* ’20, page 535–545, New York, NY, USA, 2020. Association for
Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3372857. URL https://doi.org/
10.1145/3351095.3372857.
J. Humerick. Reprogramming fairness: Affirmative action in algorithmic criminal sentencing. Columbia
HumanRightsLawReview,2019. URLhttps://hrlr.law.columbia.edu/files/2020/04/8-Humerick_
FINAL.pdf.
Y. Idelbayev. Proper ResNet implementation for CIFAR10/CIFAR100 in PyTorch. https://github.com/
akamaster/pytorch_resnet_cifar10, 2018. Accessed: 2023-07-26.
K. T. Jackson. Crabgrass frontier: the suburbanization of the United States. Oxford University Press, 1985.
E. S. Jo and T. Gebru. Lessons from archives: Strategies for collecting sociocultural data in machine
learning. InProceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT*’20,
page 306–316, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367.
doi: 10.1145/3351095.3372829. URL https://doi.org/10.1145/3351095.3372829.
S. Kapania, A. S. Taylor, and D. Wang. A hunt for the snark: Annotator diversity in data practices. In
Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI ’23, New York,
NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394215. doi: 10.1145/3544548.
3580645. URL https://doi.org/10.1145/3544548.3580645.
M. Kasy and R. Abebe. Fairness, equality, and power in algorithmic decision-making. In Proceedings of
the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, page 576–586, New
York,NY,USA,2021.AssociationforComputingMachinery. ISBN9781450383097. doi: 10.1145/3442188.
3445919. URL https://doi.org/10.1145/3442188.3445919.
P. J. Kenfack, D. D. Arapovy, R. Hussain, S. M. A. Kazmi, and A. M. Khan. On the fairness of generative
adversarial networks (GANs). Arxiv, abs/2103.00950, 2021. URL https://arxiv.org/abs/2103.00950.
J. Kleinberg. Inherent trade-offs in algorithmic fairness. SIGMETRICS Perform. Eval. Rev., 46(1):40,
jun 2018. ISSN 0163-5999. doi: 10.1145/3292040.3219634. URL https://doi.org/10.1145/3292040.
3219634.
K. K¨arkk¨ainen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias
measurement and mitigation. In 2021 IEEE Winter Conference on Applications of Computer Vision
(WACV), pages 1547–1557, 2021. doi: 10.1109/WACV48630.2021.00159.
J.Li,Y.Ren,andK.Deng.FairGAN:GANs-basedfairness-awarelearningforrecommendationswithimplicit
feedback. In Proceedings of the ACM Web Conference 2022, WWW ’22, page 297–307, New York, NY,
USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3511958.
URL https://doi.org/10.1145/3485447.3511958.
18B. Lingenfelter, S. R. Davis, and E. M. Hand. A quantitative analysis of labeling issues in the CelebA
dataset. In Advances in Visual Computing: 17th International Symposium, ISVC 2022, San Diego,
CA, USA, October 3–5, 2022, Proceedings, Part I, page 129–141, Berlin, Heidelberg, 2022. Springer-
Verlag. ISBN 978-3-031-20712-9. doi: 10.1007/978-3-031-20713-6 10. URL https://doi.org/10.1007/
978-3-031-20713-6_10.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), December 2015. URL https://ieeexplore.
ieee.org/document/7410782.
K. Lum and W. Isaac. To Predict and Serve? Significance, 13(5):14–19, 10 2016. ISSN 1740-9705. doi:
10.1111/j.1740-9713.2016.00960.x. URL https://doi.org/10.1111/j.1740-9713.2016.00960.x.
K. Makhlouf, S. Zhioua, and C. Palamidessi. On the applicability of machine learning fairness notions.
SIGKDD Explor. Newsl., 23(1):14–23, may 2021. ISSN 1931-0145. doi: 10.1145/3468507.3468511. URL
https://doi.org/10.1145/3468507.3468511.
G. Mart´ınez, L. Watson, P. Reviriego, J. A. Hern´andez, M. Juarez, and R. Sarkar. Towards understanding
the interplay of generative artificial intelligence and the internet, 2023.
G. J. McLachlan. Iterative reclassification procedure for constructing an asymptotically optimal rule of
allocation in discriminant analysis. Journal of the American Statistical Association, 70(350):365–369,
1975. doi: 10.1080/01621459.1975.10479874. URL https://www.tandfonline.com/doi/abs/10.1080/
01621459.1975.10479874.
X. Meng and J. Yao. Impact of classification difficulty on the weight matrices spectra in deep learning and
application to early-stopping. Journal of Machine Learning Research, 24(28):1–40, 2023. URL http:
//jmlr.org/papers/v24/21-1441.html.
R. K. Nelson, L. Winling, R. Marciano, N. Connolly, and et. al. Mapping inequality, 2020. URL https:
//dsl.richmond.edu/panorama/redlining/#loc=5/39.1/-94.58.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with
unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning
2011, 2011. URL http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf.
NIST. 2018 differential privacy synthetic data challenge, 2018. URL https://
www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/
2018-differential-privacy-synthetic,2018a.
J.C.Perdomo,T.Zrnic,C.Mendler-Du¨nner,andM.Hardt.Performativeprediction.CoRR,abs/2002.06673,
2020. URL https://arxiv.org/abs/2002.06673.
RAD. Rules for Archival Description (RAD). Standard, Bureau of Canadian Archivists Planning Committe
on Descriptive Standards, July 2008. URL https://archivescanada.ca/wp-content/uploads/2022/
08/RADComplete_July2008.pdf.
S.Rarrick,C.Quirk,andW.D.Lewis. Mtdetectioninweb-scrapedparallelcorpora. InMachineTranslation
Summit, 2011. URL https://api.semanticscholar.org/CorpusID:2289219.
R. Richardson, J. Schultz, and K. Crawford. Dirty data, bad predictions: How civil rights violations impact
police data, predictive policing systems, and justice, February 2019. URL https://ssrn.com/abstract=
3333423.
M. Romero. Introducing intersectionality. John Wiley & Sons, 2017.
19C. Shah and E. M. Bender. Envisioning information access systems: What makes for good tools and
a healthy web? Under Review at non-double blind venue. September 1 version., 2023. URL https:
//faculty.washington.edu/ebender/papers/Envisioning_IAS_preprint.pdf.
A. Shkembi, L. M. Smith, and R. L. Neitzel. Linking environmental injustices in Detroit, MI to insti-
tutional racial segregation through historical federal redlining. Journal of Exposure Science and En-
vironmental Epidemiology, 2022. doi: 10.1038/s41370-022-00512-y. URL https://doi.org/10.1038/
s41370-022-00512-y.
I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. The curse of recursion:
Training on generated data makes models forget, 2023. URL https://arxiv.org/abs/2305.17493.
W. So, P. Lothia, R. Pimplikar, A. Hosoi, and C. D’Ignazio. Beyond fairness: Reparative algorithms to
address historical injustices of housing discrimination in the US. In Proceedings of the 2020 Conference
on Fairness, Accountability, and Transparency.AssociationforComputingMachinery, 2022. URLhttps:
//dl.acm.org/doi/fullHtml/10.1145/3531146.3533160.
T.Stadler, B.Oprisanu, andC.Troncoso. Syntheticdata–anonymisationgroundhogday. In31st USENIX
Security Symposium (USENIX Security 22), pages 1451–1468, Boston, MA, Aug. 2022. USENIX As-
sociation. ISBN 978-1-939133-31-1. URL https://www.usenix.org/conference/usenixsecurity22/
presentation/stadler.
A. Subramanian. Pytorch-VAE. https://github.com/AntixK/PyTorch-VAE, 2020.
S. K. Sujit. VAE-pytorch. https://github.com/shivakanthsujit/VAE-PyTorch/tree/master, 2019.
H. Suresh and J. Guttag. A framework for understanding sources of harm throughout the machine learning
life cycle. In Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms,
andOptimization,EAAMO’21,NewYork,NY,USA,2021a.AssociationforComputingMachinery. ISBN
9781450385534. doi: 10.1145/3465416.3483305. URL https://doi.org/10.1145/3465416.3483305.
H. Suresh and J. Guttag. A framework for understanding sources of harm throughout the machine learning
life cycle. In Equity and Access in Algorithms, Mechanisms, and Optimization. ACM, oct 2021b. doi:
10.1145/3465416.3483305. URL https://doi.org/10.1145%2F3465416.3483305.
R.TaoriandT.Hashimoto. Datafeedbackloops: Model-drivenamplificationofdatasetbiases. InA.Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th Interna-
tional Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages
33883–33920. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/taori23a.html.
D. Tong, Q. Qiao, T.-P. Lee, J. McInerney, and J. Basilico. Navigating the feedback loop in recom-
mender systems: Insights and strategies from industry practice. In Proceedings of the 17th ACM
Conference on Recommender Systems, RecSys ’23, page 1058–1061, New York, NY, USA, 2023. As-
sociation for Computing Machinery. ISBN 9798400702419. doi: 10.1145/3604915.3610246. URL
https://doi.org/10.1145/3604915.3610246.
V. Veselovsky, M. H. Ribeiro, and R. West. Artificial artificial artificial intelligence: Crowd workers widely
use large language models for text production tasks, 2023.
W. Wu. Machine learning approaches to predict loan default. Intelligent Information Management, 14(5):
157–164, 2022. URL https://www.scirp.org/journal/paperinformation.aspx?paperid=120102.
R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In S. Dasgupta and
D. McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28
of Proceedings of Machine Learning Research, pages 325–333, Atlanta, Georgia, USA, 17–19 Jun 2013.
PMLR. URL https://proceedings.mlr.press/v28/zemel13.html.
20Z. Zhu, T. Luo, and Y. Liu. The rich get richer: Disparate impact of semi-supervised learning. In In-
ternational Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=
DXPftn5kjQK.
A Fairness in Machine Learning (FML)
SureshandGuttag[2021a]describeseveralsourcesofbiasandoppressionthatmaybeencodedintoamodel
as a result of the processes for gathering and encoding data, training, evaluating, and deploying the model.
These sytems may then commit several types of harms, including allocative (where resources are withheld
from certain groups, such as in redlining) and representational (where groups are stigmatized and stereo-
typed). Likewise, there are many different biases that may co-occur such as historical, representational,
measurement, aggregation, learning, evaluation, and deployment biases [Suresh and Guttag, 2021b]. These
often arise from misrepresenting a complex feature (e.g., treating gender or sex as a binary), mis-measuring
features, stripping data of its context (e.g., regional or dialectal language heteroglossia), and from historical
oppression influencing the data modelling processes. There are several frameworks for defining and ad-
dressing issues of fairness; calibration (used when the sensitive identities have impact on the decision task),
anti-classification (used when sensitive data is unavailable or illegal to use), individual fairness (“similar
individuals should be treated similarly”), and classification parity.
In group fairness, protected attributes are often chosen from legally-protected attributes such as race or
gender, and encoded into categorical features to determine sensitive groups. In this paper we use the terms
majoritized and minoritized as in D’Ignazio and Klein [2020] to emphasize the impact of a model’s behavior
onagroup. Notethatthemajoritypopulationmightnotcorrespondwiththemajoritized(benefit-receiving)
group; for example the Black population is a minoritized majority in the COMPAS dataset [Department,
2016]. This grouping often splits the dataset into two groups, delineated by one attribute with two possible
values (e.g., ‘male’ vs ‘female’ or ‘white’ vs ‘people of color’) which may misrepresent or inappropriately
group populations and ignores the compounding impact of possessing multiple marginalized identities.
In classification parity, there are a variety of metrics that aim for some equality of rates between these
groups, such as accuracy, positive selection rate, or error rates. In this work, we use accuracy difference,
demographic parity difference, and equalized odds difference to cover a multitude of differing priorities
model owners may value. It is often impossible to satisfy multiple fairness metrics simultaneously, so they
are ideally chosen based upon the task [Chouldechova, 2016, Kleinberg, 2018]. The binary classification and
binary grouping versions of these metrics are presented below.
Definition 1 (DemographicParity(DP)[Caldersetal.,2009]). A classifier Yˆ satisfies Demographic Parity
with respect to the sensitive attribute s if:
P(Yˆ =1|s=0)=P(Yˆ =1|s=1) ∀0,1∈s.
In this work we consider demographic parity difference, which is the absolute value of the difference
betweenthetwotermsequatedabove. Eachtermisalsotheselectionrate, orrateofpositiveprediction, for
the group.
Definition 2 (Equalized Odds (EOdds) [Hardt et al., 2016]). A classifier Yˆ satisfies Equalized Odds with
respect to the sensitive attribute s if for ground truth L:
P(Yˆ =1|L=l,s=0)=P(Yˆ =1|L=l,s=1) ∀l∈{0,1},∀0,1∈s.
We also use equalized odds difference. This is formulated as max[|P(Yˆ =1|L=0,s=0)−P(Yˆ =1|L=
0,s = 1)|,|P(Yˆ = 1|L = 1,s = 0)−P(Yˆ = 1|L = 1,s = 1)|], or the larger of the absolute value differences
between the false and true positive rates for the groups. These metrics may be used for multiple groups by
taking each difference between every pair of groups and reporting the maximal disparity. Similarly, these
groups may be formed by intersecting multiple sensitive attributes.
21Due to the biases that may exist in data collection, training, evaluation, and deployment, adherence to
or achievement of any of these fairness metrics does not guarantee fairness or equity. For example, these
fairnessmetricsassumethatthecostofanerrorbornebyapersonofanygroupisthesame,wheninpractice
the costs and benefits may differ greatly depending on identity [Makhlouf et al., 2021]. There are several
works that focus on the trade-offs between meeting a decision maker’s FML criteria and the potentially
inequitable social outcomes which cast doubt on the suitability of FML metrics for societal welfare [Kasy
and Abebe, 2021, Hu and Chen, 2020, Corbett-Davies et al., 2017]. This is in part a byproduct from FML’s
reliance upon algorithmic idealism, where computation assumes a meritocratic society whereby equalizing
demographic disparities must therefore lead to fairness at a societal level [Davis et al., 2021, Green, 2020,
Kasy and Abebe, 2021]. Additionally, FML may also engage in or reinforce two main biases: 1) automation
bias, that machines are objective and are less biased than humans, and 2) that automation invites justice
without regard for the objective and purpose of the models [Davis et al., 2021, Green, 2020].
B MIDS in Literature
This section provides a more detailed review of the MIDS and enablers described in Section 2.
B.1 MIDS
Performative prediction. Performative prediction is a distribution shift that occurs when a model’s
predictions impact the outcome. For example, when economists publish forecasts, they may influence the
behavior of others in the market, causing the market to fit the forecast in a self-fulfilling prophesy [Perdomo
et al., 2020, Hardt and Mendler-Du¨nner, 2023]. In this case, the model’s predictions leak into the data
ecosystem as they become outcomes. If this data ecosystem is trained upon, these outcomes are treated as
the ground truth, and the MIDS continues into another generation of models.
Figure 6: Left: 1939 Home Owner Loan Corporation Security Map for Detroit. Red areas are Grade D,
or ‘hazardous’ locations due to the presence of racial and religious minorities [Nelson et al., 2020]. Banks
of the time were likely privy to these otherwise secret maps and in surveys stated that only high-graded
neighborhoods would be offered loans [Jackson, 1985, Nelson et al., 2020]. Redlining continued de jure until
the Fair Housing Act in 1968 [90th United States Congress, 1968]. Right: Census data on the non-white
population in metropolitan Detroit from 1955 [Detroit Deomgraphics].
Fairness feedback loops. The fairness community has studied performative prediction and fairness feed-
22back loops in the context of risk assessment systems, including mortgaging and predictive policing [Green,
2020]. We provide two notable examples:
1) Figure 6 shows the 1939 HOLC Residential Security Maps for Detroit alongside 1955 demographic
information of non-white communities. This map is a historical multiclass classification model that reflects
the values and priorities of the individuals and institutions responsible for its creation; specifically the
white male gaze of Depression-era professional realtors [So et al., 2022, D’Ignazio and Klein, 2020, Jackson,
1985]. These values encoded into this model and the MIDS from the model itself has contributed to housing
discrimination in Detroit, as seen in the right of Figure 6.
2) Work in predictive policing found that policing locations may converge towards over-policing low-
income non-white communities [Lum and Isaac, 2016, Richardson et al., 2019]. Theoretical follow-ups find
that the degree of runaway feedback may be moderated with careful training set weighting, but cannot be
negated entirely [Ensign et al., 2017].
Model Collapse. Where performative prediction and runaway feedback loops generally refer to classifi-
cation models, model collapse describes the same effect for generative models. Model collapse occurs when
new generative models are trained on samples created by their predecessor over many generations, as intro-
duced in Shumailov et al. [2023] and concurrently in Alemohammad et al. [2023]. This leads to new models
forgetting the original data distribution as they recreate and amplify the failures of their ancestors. There
are two error sources that contribute: 1) functional approximation error due to an inadequately expressive
generator, and 2) statistical approximation error from finite sampling. Model collapse begins with a loss of
information from the tails of the data distribution. In late-stage model collapse, the model mixes the modes
of the original distribution, converging to a point estimate of some mean betwixt them. There are also
concernsovertheeffectsofmodelcollapseonfairness,asmodelfailureon“low-probabilityevents”mayhave
negative effects on minoritized groups when datasets have poor representation [Suresh and Guttag, 2021b].
Disparity amplification. Unlike the aforementioned MIDS, disparity amplification arises from human-
model interaction. If a model suffers from problems derived from representational bias, it may have overall
high performance but low performance on minoritized groups (performance disparity). This can lead to
disparity amplification, where minoritized users who suffer high error rates may choose to disengage from
the model, shifting the future dataset towards the majoritized group, and increasing the representational
disparity of the data ecosystem [Hashimoto et al., 2018].
B.2 MIDS Enablers
We describe several enablers in more detail here. Note that even sampling is an enabler, and indeed it
informs our approach to algorithmic reparation in our experiments.
Pseudo-labelling. Pseudo-labelling generally refers to using a model to assign labels to unlabeled samples
ina dataset, so that this data may alsobe usedfor supervisedor semi-supervised training[Cascante-Bonilla
et al., 2020]. This may occur just once [Cascante-Bonilla et al., 2020] or iteratively [McLachlan, 1975].
Incentives for pseudo-labeling may arise in cases where manual labeling and/or annotation is too expensive
for vast swathes of data. The fairness impact of using pseudo-labelling for self-supervised learning was
discussedinZhuetal.[2022], findingthatgroupswithhighinitialaccuracybenefitwhereasgroupswithlow
initial accuracy may see a degradation in performance.
Feedback and Data Annotation. Similarly to pseudo-labelling, feedback (whether human or model-
based) is often used for labeling and annotating data for supervised training or for providing feedback on
generative outputs. Reliance on human annotation can lead to unfairness arising from individual annotator
biasandinstructionsforannotating[SureshandGuttag,2021b]. Recentworkhasfoundindicatorsthatsome
human data annotators use LLMs or other models, which may lead to MIDS if these models are updated
andretrainedonthedatatheylabeled[Veselovskyetal.,2023]. AIfeedbackisalsousedinmethodssuchas
ConstitutionalAI,whichusesasuccessionoffine-tunedsupervisedmodelstoprovideRLAIF(reinforcement
learning from AI feedback) for training ‘harmless’ AI assistants [Bai et al., 2022].
23ColoredMNIST Class ColoredSVHN Class
Group Beneficial Detrimental Group Beneficial Detrimental
Majoritized 0.350 0.150 Majoritized 0.424 0.118
Minoritized 0.150 0.350 Minoritized 0.183 0.275
CelebA Class
Group Beneficial (Attractive) Detrimental
Majoritized (Not Male) 0.396 0.184
Minoritized 0.117 0.302
Table 2: Class and group demographics of training datasets. Values show the proportion of that group–
class category in the training dataset (and therefore sum to 1). Top: ColoredMNIST and ColoredSVHN the
majoritized is group skewed towards the beneficial class with probability 0.7, and to the detrimental class
with probability 0.3. Bottom: CelebA.
C Experimental Details
C.1 Datasets
We evaluate the fairness effects of model collapse and algorithmic reparation on several datasets; adapted
versions of MNIST and SVHN, as well as CelebA and FairFace.
ColoredMNIST MNIST is a single-channel handwritten digit recognition dataset [Deng, 2012]. We use 50000
images for training, 10000 for validation, and another 10000 for testing. We adapt MNIST to a binary
classification scheme (determining if a digit is in [0..4] for class 0 or [5..9] for class 1). The class label is
switched with a uniform probability of 5% to add label noise, as in Arjovsky et al. [2020]. We also adapt
MNIST to have binary groups by coloring the sample either red or green, where green is treated as the
‘majoritized’group. Weskewthemajoritizedgrouptothebeneficialclass,suchthatP(S =majoritized|L=
beneficial) = 0.7, P(S = majoritized|L = detrimental) = 0.3. In this case, both classes and groups are
balanced, as seen in the dataset composition matrix in Table 2. Ablations for this skew and class and label
balances are in Appendix D.
ColoredSVHN SVHN (Street View House Numbers) is a digit recognition dataset composed of house numbers
sourcedfromGoogleStreetView[Netzeretal.,2011]. Weuse52327imagesfortraining,20930forvalidation,
and another 26032 for testing. For SVHN, we adapt to a binary task similarly as in MNIST. We binarize the
classification task to determining if a digit is in [0..4] for class 1 or [5..9] for class 0, where class 1 is the
beneficial class as converged to by model collapse. The class label is swapped with a uniform probability of
5% to add label noise, as in Arjovsky et al. [2020]. Unlike in ColoredMNIST, class 1 is the lower numbers
as SVHN converges to small numbers over the course of model collapse, as seen in Figure 14. This causes
class imbalance; class 1 composes 60.7% of the data. We also add sensitive groups by converting the images
to grayscale and then coloring the samples either red or green as in ColoredMNIST. The green group again
serves as the majoritized group, and is skewed towards the beneficial class at rates P(S = majoritized|L =
beneficial) = 0.7, and P(S = majoritized|L = detrimental) = 0.3, leading to group imbalance with the
majoritized group as 54.3%. A matrix showing the composition of the ColoredSVHN training distribution is
shown in Table 2.
CelebA CelebA is a dataset of celebrity faces [Liu et al., 2015]. We use an 80/10/10 train/validation/test
split of the 202599 cropped and aligned images. The binary classification task is to predict attractiveness,
where sensitive groups are given from gender (‘Male,’ ‘not Male’). The group and class balance is 58.% ‘not
male’ and 51.3% ‘attractive.’ The composition of the dataset is shown in Table 2. This dataset is criticized
fortheinclusionofsubjectivefeaturessuchas‘attractive,’andtherearemanyinstancesofincorrectlabeling
and annotation [Lingenfelter et al., 2022]. In the other datasets, we chose the beneficial class as the class
24≤29 >30
Age
0.541 0.459
Male Female
Gender
0.531 0.469
Figure 7: Class (gender) and group (race and binarized age) balance for the FairFace training set.
most converged to by model collapse. Interestingly, against the class imbalance, model collapse converges to
‘unattractive,’ which would benefit the ‘Male’ group more. However, model collapse also converges to ‘Not
Male.’ We therefore use ‘attractive’ as the beneficial class and ‘Not Male’ as the majoritized group.
FairFace FairFace is a face dataset that is balanced by both gender (two values) and race (seven val-
ues) [K¨arkk¨ainen and Joo, 2021]. The composition, including the intersections of age, race, and gender,
are shown in Figure 7. We use an 80/10/10 train/validation/test split of the 108501 images. For binary
classification, we predict gender (‘Male,’ ‘not Male’). For sensitive features, we use S as race, which has
1
seven potential values (‘White’, ‘Latino Hispanic’, ‘Indian’, ‘East Asian’, ‘Black’, ‘Southeast Asian’, ‘Middle
Eastern’), and S as age, which we binarize as above and below 30 years (see Figure 7 for the class and
2
group balances). For the group annotation oracles, we have two separate classifiers corresponding to these
features. The beneficial class is ‘not Male,’ as model collapse increases the representation of this class,
even through they are a minority in the original dataset class balance. As there are 14 categories created
from the intersection of age and race, we instead track the representations of both of these attributes alone
and the largest and smallest categories among these over time. Note that these choices for the beneficial
and majoritized annotations are arbitrary for these experiments as we do not make claims of justice (recall
Section 3.4).
Justification of dataset choice. We choose ColoredMNIST and ColoredSVHN due to their similarity.
Both detect and classify digits, and may be easily adapted into a binary classification and binary fairness
grouping task. They differ in their complexity, SVHN is usually the harder dataset to learn, and also in their
class balance once binarized (see Table 2). These two datasets, despite their similarities, show very different
pointsofmodelcollapseandevenoppositebehaviorwhenobservingtheaccuracydifferencebetweengroups,
as in Figure 5 and Figure 20. CelebA is chosen to represent a more complex and real-world dataset with
well-documented disparities between the class and grouping we use for its task [Lingenfelter et al., 2022].
On the other hand, FairFace is chosen for its balance in both race and gender; alongside other metrics, it
is simple to measure sensitive group intersections.
25C.2 Compute and codebase
Experiments were performed on Ubuntu 18.04.6 LTS using 4 Intel Xeon CPU cores per GPU. We use the
followingGPUsperrandomseed: forColoredMNISTweuse1NVIDIAT4GPUwith16gigabytesofmemory;
for ColoredSVHN we use 2 NVIDIA RTX6000s with 40 gigabytes each; for CelebA we use 2 NVIDIA A100s
with 40 gigabytes each (split GPUs); for FairFace we use 2 NVIDIA A40s with 48 gigabytes each. Our
codebase is in Python 3.9 with PyTorch and fairness metrics from Fairlearn [Bird et al., 2020]. Existing
codewasadaptedforourexperiments: VAEmodelsfromSubramanian[2020],ColoredMNISTfromArjovsky
et al. [2020], SVHN with fairness from Kenfack et al. [2021], and ResNets from Idelbayev [2018].
C.3 Models
The architectures and hyperparameters differ based on dataset. The performance of the annotator models
A and A are shown in Table 3. All generative models were trained with ADAM (weight decal 1×10−5),
L S
and all classifiers with SGD and cross-entropy loss.
ColoredMNIST
• Classifiersand annotators are6layerCNNswith 2convolution layersand ReLUactivations. Learning
rate 0.1, batch size 256, for 30 epochs.
• Generators (VAEs) are mirrored encoder and decoder CNNs. Each is 2 convolution layers with ReLU
activations. UsesBCELosswithKL-divergenceterm,alatentspacedimensionof20,andavariational
beta of 1. Learning rate 0.001, batch size 256, for 30 epochs.
ColoredSVHN
• Classifiers and annotators are 32-layer ResNets adapted from Idelbayev [2018]. Learning rate 0.001,
batch size 32, for 30 epochs.
• Generators are the deep convolutional VAE adapted from Sujit [2019], using MSE loss with KL-
divergence term, a latent space dimension of 32, and a variational beta of 1. Learning rate 0.0005,
batch size 128, for 30 epochs.
CelebA
• Classifiers and annotators are 110-layer ResNets adapted from Idelbayev [2018]. Learning rate 0.001,
batch size 128, for 15 epochs.
• Generator VAEs are composed of a 5-layer CNN encoder and 6-layer upsampling CNN decoder with
LeakyReLU activations. Loss is BCE with KL-divergence term, a latent space dimension of 500, with
a variational beta of 5×10−6. Learning rate 0.005, batch size 64, for 30 epochs.
FairFace
• Classifiers and annotators are pretrained 50-layer ResNets adapted from Idelbayev [2018]. Pretraining
is on Imagenet, using the version 1 weights from PyTorch. Learning rate 0.001, batch size 256, for 30
epochs.
• Generator VAEs are composed of a 5-layer CNN encoder and 6-layer upsampling CNN decoder with
LeakyReLU activations. Loss is MSE with KL-divergence term, a latent space dimension of 500, with
a variational beta of 1×10−6. Learning rate 0.0001, batch size 256, for 50 epochs.
C.4 STAR Algorithm
Algorithm 1 shows an example of STAR for binary group and binary sensitive attributes.
26Table 3: Performances (with standard deviations) of the label and sensitive attribute annotator models A
L
and A . Performances are shown for each dataset. The performance of A is high for ColoredMNIST and
S S
ColoredSVHN as determining sample color is an easy task. The fairness metrics for A should be close to 1,
S
asthesemodelsshouldassignclassbasedonthesensitiveattributealone. Reportedaccuraciesareallmacro-
averaged. Note the high accuracy disparity for FairFace A , although racial groups are roughly balanced,
S1
we observed far higher accuracy for ‘white’ than any other group, perhaps due to simplicity bias [Bell and
Sagun, 2023].
ColoredMNIST ColoredSVHN CelebA
A A A A A A
L S L S L S
Accuracy 0.928 ± 0.003 1 ± 0 0.849 ± 0.080 1 ± 0 0.816 ± 0.005 0.976 ± 0.002
∆ Accuracy 0.009 ± 0.005 0 ± 0 0.052 ± 0.111 0 ± 0 0.029 ± 0.008 0.015 ± 0.008
∆ DP 0.367 ± 0.008 1 ± 0 0.151 ± 0.193 1 ± 0 0.440 ± 0.022 0.951 ± 0.004
∆ EOdds 0.032 ± 0.022 1 ± 0 0.163 ± 0.240 1 ± 0 0.271 ± 0.037 0.971 ± 0.008
FairFace
A A A
L S1 S2
Accuracy 0.884 ± 0.013 0.617 ± 0.010 0.789 ± 0.017
∆ Accuracy 0.104 ± 0.011 0.428 ± 0.088 0.251 ± 0.094
∆ DP 0.360 ± 0.023 0.443 ± 0.057 0.683 ± 0.031
∆ EOdds 0.308 ± 0.014 0.438 ± 0.139 0.254 ± 0.075
Algorithm 1: Training with algorithmic reparation batches.
Input: Sample-providing generator G, batch size b, reparation budget r, label annotator C (either
C or A ), sensitive attribute annotator A .
i−1 L S
Output: Reparation batch
for batch in number batches do
1:
2: Ideal =[b/4,b/4,b/4,b/4] ▷ Ideal category sizes
3: Batch =[b L=0,S=0,b L=0,S=1,b L=1,S=0,b L=1,S=1]=[0,0,0,0] ▷ Initialize batch categories
4: Temporary batch = Sample b+r times from G ▷ Initial batch from uniform sampling
Annotate temporary batch using C and A
5: S
Categorize batch depending on L and S values from annotations
6:
Populate Batch until Ideal =Batch
7: i i
8: To resample =sum(Ideal−Batch) ▷ Get amount to sample to fill deficient categories
9: Batch.append(Sample To resample times from G, annotate with C and A S) ▷ Refill batch
Update model on Batch.
10:
return Batch
11:
D Appendix: Ablation Studies
In this appendix we provide experiments to demonstrate the effects of MIDS over several ablated variables:
the sensitive group imbalance, class imbalance, and amount of synthetic training data. We provide results
for both ColoredMNIST and ColoredSVHN.
271.0
0.600
% Majoritized Group
0.575 50 80
60 90
0.8 70
0.550
0.525
0.6 % Majoritized Group
0.500
50 80
60 90
0.475
70
0.4
0 10 20 30 40 0 10 20 30 40
Generations Generations
Figure 8: ColoredMNIST class and group balance while varying the group balance in SeqGenSeqClass.
1.0
0.58 % Beneficial Class
0 60
0.56 20 80
0.8 40
0.54
0.52
0.6
% Beneficial Class
0 60 0.50
20 80
0.4 40 0.48
0 10 20 30 40 0 10 20 30 40
Generations Generations
Figure 9: ColoredMNIST class and group balance while varying the class balance in SeqGenSeqClass.
D.1 Class and group imbalance
In these studies we varied the class balance or group balance. The study was carried out on ColoredMNIST
with 5 seeds in the sequential generator and classifier setting. For group imbalance, the groups were equally
likely to belong to the beneficial class, though their populations were varied. For class imbalance, the
majoritizedgroupwasskewedtowardsthebeneficialclassinthesamemannerasdiscussedinAppendixC.1,
andtheclasspopulationvaried. Forthistask, thevariationsinbalancedidnotstronglyeffectthegenerated
population or downstream classifier performance. The generator class and group balances are shown for
varied group balance in Figure 8 and for varied class balance in Figure 9. The results in Section 4.2.2 use
datasets with a mixture of class and group imbalance which better elucidate the effects of MIDS.
D.2 Amount of synthetic data
In this study we varied the amount of original training data (drawn randomly from the training set) in
each batch for training generators in the sequential generator and classifier setting. These experiments
were carried out on ColoredMNIST for 5 seeds. There is a substantially higher accuracy cost and accuracy
disparity between groups, as shown in Figure 10. Note that even with 0% synthetic data (i.e., training each
generator from the original training set) there is still an accuracy loss over time due to the effects of the
sequential classifiers. While the group balance is not hugely effected (as in the other ColoredMNIST results
in Section 4.2), the class balance skews towards the beneficial class over the generations, fueling an increase
of equalized odds difference with more synthetic data, see Figure 10.
In practice, there may be several generations of synthetic data present when drawing from a corpus of
28
ssalC
laicifeneB
ssalC
laicifeneB
puorG
dezitironiM
puorG
dezitironiMpolluted data. For example, when training G , samples from G and G might also be present. In this
2 0 1
case, the compounded artefacts of model collapse will be lesser in these early generations. In this study, the
synthetic data is only pooled from the most recent generator, and so these results may overstate the effect
of model collapse in the aforementioned case.
D.3 Sequential versus non-sequential classifiers in SeqGenSeqClass and Seq-
GenNonSeqClass
In this study we demonstrate the impact of sequential classifiers in SeqGenSeqClass. These experiments
were conducted for ColoredMNIST and ColoredSVHN for 25 and 10 seeds, respectively.
The non-sequential classifiers are more sensitive to changes in the distribution, as seen in the accuracy
over generations and selection rate graphs in Figure 11 and Figure 12 for ColoredMNIST and ColoredSVHN,
respectively. This is consistent with the intuition that the sequential classifiers are ‘adapting’ their mapping
of X → L to changes in X caused by model collapse. This allows the sequential classifiers to experience
more generations of utility compared to the non-sequential classifiers, as seen in their higher accuracies.
As model collapse causes strong imbalance towards the beneficial class (as determined by A ), the non-
L
sequential classifiers eventually only predict the positive label, decreasing DP and EOdds unfairness as both
groups receive the same predictions and error rates (in ColoredMNIST, EOdds difference drops to 0 as error
rates from only giving the positive prediction are identical due to group and class balance). Meanwhile, the
sequential classifiers for ColoredMNIST andColoredSVHN instead evolve to only give a beneficial prediction
to a majoritized sample, increasing unfairnesses (see Section 4.2.2).
Note that the achievement of higher fairness in the non-sequential classifier case indicates higher fairness
withrespecttotheoriginaldistribution. Thismaybeundesirableinsomecases,particularlythoseapplicable
toalgorithmicreparation,whichspecificallynotesthatequalityofofmodeloutputstobaseratesinadataset
does not guarantee equity. This is especially true if the dataset is collected with any biases, including
compounding Intersectional biases which these experiments do not inform upon Davis et al. [2021].
E Model Collapse in Generators
Weshowthelosseswithrespecttotheparentgeneratorloss(L(G ,G ))overgenerationsastheyundergo
i i−1
modelcollapseandwhilesubjecttogen-STAR(whichcausesaminoradjustment). Intuitionwouldsuggest
that the distribution collapses to be increasingly easy-to-learn, such that successive generators inherit sim-
plified versions (due to finite sampling of their parents) of the problem and so perform better. We observe
this effect with the smoothly decreasing loss curves of ColoredSVHN and FairFace in Figure 13.
However, for both ColoredMNIST and CelebA, we see the exact opposite curve. The child generators are
faced with an increasingly hard-to-learn distribution. We hypothesize that this may be due to one of two
causes. 1) We do not perform hyper-parameter tuning for the generators at each generation, and perhaps
ColoredMNISTandCelebAexperiencehyperparameterinstability. 2)Perhapsthisissimplyaquirkofmodel
collapse, finite sampling of heavy-tailed distributions may lead to enough bias and noise to significantly
complicate the learning task. We propose to investigate the stability of model collapse in future work.
We also provide some examples generated by generators undergoing model collapse in Figure 14.
F Additional Results
F.1 SeqClass Results
We provide full suites of figures for ColoredMNIST, ColoredSVHN, CelebA, and FairFace on SeqClass. See
Figures 3, 15, 16 and 17, respectively.
291.0
% Synthetic Data
% Synthetic Data
0 60 0.60 0 60
15 75
15 75
0.8 30 90 30 90
45 45
0.55
0.6
0.50
0.4
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.9 % Synthetic Data
0.15 0 60
20 80
40 90
0.8 0.10
% Synthetic Data
0.05
0.7 0 60
20 80
40 90
0.00
0 10 20 30 40 0 10 20 30 40
Generations Generations
% Synthetic Data
0.45 0.4 0 60
20 80
0.40 40 90
0.3
0.35
0.2
0.30 % Synthetic Data
0 60
0.1
0.25 20 80
40 90
0 10 20 30 40 0 10 20 30 40
Generations Generations
Figure 10: ColoredMNIST metrics while varying the amount of synthetic data in SeqGenSeqClass. Top:
beneficial class balance and group balance. Center: accuracy, and accuracy difference between groups.
Bottom: DPandEOddsdifference. Thesegenerallyshowworseperformanceandfairnesswithmoresynthetic
data, with larger variances.
30
ssalC
laicifeneB
ycaruccA
ecnereffiD
PD
puorG
dezitironiM
ecnereffid
ycaruccA
ecnereffiD
sddOE0.4
0.9 Sequential
Non-sequential
0.3
0.8
0.7 0.2
0.6 0.1
Sequential
Non-sequential
0.5 0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.8 1.0 0.8
0.9
0.6 0.6
0.8
Sequential Sequential
0.4 Non-sequential 0.7 0.4 Non-sequential
0.2 0.6 0.2
Sequential
0.5 Non-sequential
0.0 0.0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 11: ColoredMNIST results for sequential versus non-sequential classifiers in SeqGenSeqClass and
SeqGenNonSeqClass. Top: accuracy and accuracy difference between groups. Bottom: demographic
parity difference, selection rate, and equalized odds difference.
Sequential
0.8 Non-sequential 0.3
0.7
0.2
0.6
0.1 Sequential
0.5 Non-sequential
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0 1.0 1.0
Sequential Sequential
0.8 Non-sequential 0.8 Non-sequential
0.8
0.6 0.6
0.4 0.6 0.4
0.2 Sequential 0.2
Non-sequential
0.4
0.0 0.0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 12: ColoredSVHN results for sequential versus non-sequential classifiers in SeqGenSeqClass and
SeqGenNonSeqClass. Top: accuracy and accuracy difference between groups. Bottom: demographic
parity difference, selection rate, and equalized odds difference.
31
ecnereffiD
PD
ecnereffiD
PD
ycaruccA
ycaruccA
etaR
noitceleS
etaR
noitceleS
ecnereffiD
ycaruccA
ecnereffiD
ycaruccA
ecnereffiD
sddOE
ecnereffiD
sddOE50000
45000
45000
40000 40000
35000 35000
30000 30000
0 10 20 30 40 0 10 20 30 40
Generations Generations
5000 5000
4000 4000
3000 3000
2000 2000
1000 1000
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.65
0.625
0.60 0.600
0.575
0.55
0.550
0.50
0.525
0 2 4 6 8 0 2 4 6 8
Generations Generations
0.20 0.20
0.15 0.15
0.10 0.10
0.05 0.05
0.00 0.00
0 2 4 6 8 0 2 4 6 8
Generations Generations
Figure13: Top row: ColoredMNIST,Second row: ColoredSVHN,Third row: CelebA,Bottom row: FairFace.
Left: model collapse losses, Right: model collapse with gen-STAR losses.
32
1
iG
trw
ssol
iG
1
iG
trw
ssol
iG
1
iG
trw
ssol
iG
1
iG
trw
ssol
iG
1
iG
trw
ssol
iG
1
iG
trw
ssol
iG
1
iG
trw
ssol
iG
1
iG
trw
ssol
iGTrain
0
2
4
8
16
32
Figure14: SamplesfromgeneratorsundergoingmodelcollapseinSeqGenSeqClassforColoredMNIST(top
left), ColoredSVHN (top right), CelebA (bottom left), and FairFace (bottom right).
F.1.1 cla-STAR Batch Balances
We report the composition of the batches used when training the classifiers with and without cla-STAR
in the SeqClass setting in Figure 18. These figures show the strata cla-STAR uses to train classifiers,
the resulting classifier strata, and the strata of classifiers trained without any reparation. Usually, the
cla-STAR strata are the most balanced, followed by the strata of classifiers that received reparation.
In FairFace, the batches are mostly older white males, and sometimes younger white non-males; the least
populated categories are usually younger people from the Indian, South East Asian, and Hispanic/Latino
races.
F.2 SeqGenSeqClass Results
We provide full suites of figures for ColoredMNIST, ColoredSVHN, CelebA, and FairFace on SeqGenSeq-
Class. See Figures 19, 4, 5 and 20, respectively.
F.2.1 STAR Batch Balances
WecomparethecompositionofthebatchesusedwhentrainingtheclassifierswithandwithoutSTARinthe
SeqGenSeqClass setting. For ColoredMNIST, see Figure 21, ColoredSVHN, see Figure 22, and FairFace,
seeFigure23. ThesefiguresshowthestrataSTARusestotrainmodels, theresultingmodelstrata, and
the strata of models trained without reparation. In FairFace, the batches are mostly older white males,
and sometimes younger white non-males; the least populated categories are usually people from the Indian,
Middle Eastern, South East Asian, Black, and Hispanic/Latino races.
33
noitareneGcla-STAR cla-STAR cla-STAR
0.85 No reparation No reparation No reparation 0.6
0.15
0.80
0.75 0.10 0.4
0.70
0.2
0.05
0.65
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
100 0.5
cla-STAR Major, Class 0 cla-STAR
0.6 No reparation 80 M Mi an joo rr ,, CC ll aa ss ss 10 0.4 No reparation
Major, Class 1
60 0.3
0.4
40 0.2
0.2
20 0.1
0.0 0 0.0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 15: ColoredSVHN results for SeqClass on the evaluation set. Top: accuracy, accuracy difference,
and demographic parity difference. We observe lower fairness differences with cla-STAR, with a cost of
more inaccuracy. Bottom: equalized odds difference, the strata created during cla-STAR, and the KL-
divergence between cla-STAR fairness ideal and classifier strata. The KL-Divergence decreases with
cla-STAR as the batches become more evenly balanced across group and class.
G Relative Performances of MIDS
If the model trainer is unaware of MIDS occurring over time, they may see only the relative performances
(i.e., performance of generation i measured w.r.t. generation i−1) of each generation compared to its prior
generation. In this case, when each generation of models is trained to have relatively high performance,
it may look as though the models are performing well, though not when compared to the original data
distribution. This may lead to overstating the model’s performance, which for the FML metrics results in
fairwashing the model due to inadequate validation and testing [Aivodji et al., 2019]. For ColoredMNIST
and ColoredSVHN, we report results on the testing set for the ‘actual’ results (classifiers measured against
the testing set) and for the relative results (classifiers measured against the previous generation’s classifier
predictions on the testing set inputs). We choose not to present these two graphs on the same plots to
prevent confusion as they measure two different properties.
For reference, ColoredMNIST plots are in Figure 24 for SeqClass and Figure 26 for SeqGenSeqClass.
ColoredSVHN plots are in Figure 25 for SeqClass and Figure 27 for SeqGenSeqClass.
These results also demonstrate how even when training each new classifier with a small tolerance for
unfairnesscanaccruetohighunfairness. Forexample,considertherelativeequalizedoddsresultsinFigure24
which on average stay below 0.06 for each generation accrue to over 0.2.
Figure26showsthepointofmodelcollapseintheSeqGenSeqClasssettinginColoredMNIST(collapse
bygeneration15)canbeseenintherelativeaccuracyplotandintheincreaseinvarianceintheotherrelative
plots. Similarly as found in the SeqClass plots discussed above, low relative equalized odds difference and
a relatively balanced minoritized group under-report the actual unfairness and imbalance.
34
ycaruccA
ecnereffiD
sddOE
%
seirogetaC
ecnereffiD
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
PDcla-STAR 0.35 cla-STAR
0.62 No reparation 0.20 No reparation
0.30
0.60
0.15
0.25
0.58
0.10
0.20
0.56
0.05
0.54 0.15 cla-STAR
No reparation
0.00
0 2 4 6 8 0 2 4 6 8 0 2 4 6 8
Generations Generations Generations
100
0.20
cla-STAR
No reparation 75
0.15
50
0.10
Demographic Category
Majoritized, Class 0
0.05 25 Minoritized, Class 0
Majoritized, Class 1
Minoritized, Class 1
0.00 0
0 2 4 6 8
Generations Generations
Figure 16: CelebA results for SeqClass on the evaluation set. Top: accuracy, accuracy difference, and
demographic parity difference. Better fairness (lower fairness difference) and higher accuracy is achieved
withcla-STAR.Bottom: equalizedoddsdifferenceandthestratacreatedduringcla-STAR.Thebatches
become more evenly balanced across group and class.
H Co-Occuring MIDS
Inthissectionwepresenttwosetsofexperimentstoshowcasehowdisparityamplificationcanco-occurwith
performative prediction, and also with model collapse. We evaluate these experiments for ColoredMNIST,
where Figure 28 shows the SeqClass case, Figure 29 shows the SeqGenSeqClass case, and Figure 30
shows the strata for models trained in both settings.
FortheSeqClasssetting,wetraineachclassifierinthelineagefroma50/50mixtureofdatafromG and
0
from the original training set. In the SeqGenSeqClass setting, the generators are trained from this data
mixture,thoughthedownstreamclassifiersaretrainedentirelyfromtheircorrespondinggenerator’ssynthetic
outputs. The inclusion of human-generated data moderates the degree of model collapse to showcase other
effects.
For disparity amplification to co-occur, we use stratified sampling on the original training set portion of
thedatamixture,wherethestrataaredeterminedbytheclassifier’slabeldistributionoverthegroups. Note
that this is not disparity amplification as discussed in Hashimoto et al. [2018], which is due to performance
failures,butinsteadduetolabelandgrouprepresentation. Thisapproximatestheeffectsoftheclassifierson
the human-generated data distribution, and shows how this effects feeds into the other MIDS. We conduct
additional experiments to showcase the effects of AR at the classifiers or generators, in isolation from and
in combination with the disparity amplification sampling strategy.
Note that at the limit where there is no synthetic data, we recover the promising technical question of
how to create a biased sampling mechanism that begets fairness in a downstream model trained from a
generator.
35
ycaruccA
ecnereffiD
sddOE
ecnereffiD
ycaruccA
%
seirogetac
gnilpmas
RA
0 5
ecnereffiD
PD
010.65 cla-STAR cla-STAR 0.25 cla-STAR
No reparation 0.30 No reparation No reparation
0.20
0.60 0.25
0.15
0.55 0.20
0.10
0.15
0.50 0.05
0.10
0.00
0 2 4 6 8 0 2 4 6 8 0 2 4 6 8
Generations Generations Generations
cla-STAR cla-STAR
0.3 No reparation 0.7 No reparation
0.6
0.2 0.5
0.4
0.1 0.3
0.2
0 2 4 6 8 0 2 4 6 8
Generations Generations
Figure 17: FairFace results for SeqClass on the evaluation set. Top: accuracy, accuracy difference, and
demographic parity difference. We observe lower fairness differences with cla-STAR, with a cost of more
inaccuracy. Bottom: equalized odds difference, and the KL-divergence between cla-STAR fairness ideal
and classifier strata. We do not report the strata formed by cla-STAR as there are 28 categories;
instead, refer to the strata of the classifiers in Appendix F.1.1.
36
ycaruccA
ecnereffiD
sddOE
ecnereffiD
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
PD35 35
35
30 30
30 STAR Category STAR Category
Minor, Class 0 Minor, Class 0
25 Major, Class 0 25 Major, Class 0 25
Minor, Class 1 Minor, Class 1 STAR Category
20 Major, Class 1
20
Major, Class 1 20 M Mi an joo rr ,, CC ll aa ss ss 00
Minor, Class 1
15 Major, Class 1
15
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
50 STAR Category STAR Category STAR Category
Minor, Class 0 Minor, Class 0 40 Minor, Class 0
40 Major, Class 0 40 Major, Class 0 Major, Class 0
Minor, Class 1 Minor, Class 1 Minor, Class 1
30 Major, Class 1 30 Major, Class 1 30 Major, Class 1
20
20 20
10
10 10
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
50 STAR Category STAR Category 30 STAR Category
Minor, Class 0 30 Minor, Class 0 Minor, Class 0 40 Major, Class 0 Major, Class 0 Major, Class 0
30 M Mi an joo rr ,, CC ll aa ss ss 11
20
M Mi an joo rr ,, CC ll aa ss ss 11 20 M Mi an joo rr ,, CC ll aa ss ss 11
20
10 10
10
0 0 0
0.0 2.5 5.0 7.5 10.0 0 2 4 6 8 2 4 6 8
Generations Generations Generations
Figure 18: strata balances for datasets in SeqClass. Left: The strata of classifiers without reparation.
Center: The strata resulting from classifiers with cla-STAR. Right: The strata used to train classifiers
with cla-STAR. Top: ColoredMNIST. Second row: ColoredSVHN. Bottom: FairFace, instead of showing
all 28 categories, we choose the two categories per label that are most frequently the largest and smallest
portion of the batch across all generations.
37
%
atartS
reifissalC
%
atartS
reifissalC
%
atartS
reifissalC
%
atartS
reifissalC
%
atartS
reifissalC
%
atartS
reifissalC
%
atarts
RATS
%
atarts
RATS
%
atarts
RATS0.9 No reparation 0.8 No reparation 0.8 cla-STAR 0.3 cla-STAR 0.8 gen-STAR 0.6 gen-STAR 0.6
0.7 0.2 0.4 0.4 N c gl eo a n- r S -Se Tp TAa ARr Ration
00 .. 56 0.1 N clo a- r Se Tp Aa Rration 0.2 0.2
gen-STAR
0.0 0.0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
1.0
0.6 0.6 0.9 0.55
0.8
0.4 0.4 0.50
0.7
0.2 No reparation 0.2 No reparation 0.6 No reparation 0.45 No reparation
cla-STAR cla-STAR cla-STAR cla-STAR
gen-STAR gen-STAR 0.5 gen-STAR gen-STAR
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
Figure 19: ColoredMNIST results for SeqGenSeqClass. Top: shows accuracy, accuracy difference, demo-
graphicparitydifference,andequalizedoddsdifference. Forthelatterthree,lowervaluesarebetter. Bottom:
KL-Divergence between fairness ideal and classifiers, and between fairness ideal and generator strata, the
classbalance,andgroupbalance. Shadingshowscollapsedgenerations. Weobservethatgen-STARleadsto
betterrepresentationandfairness,thoughwithacosttotheaccuracymetrics. RecallthatforColoredMNIST,
we find metric tension between EOdds, accuracy difference, and the STAR fairness ideal.
0.70 N c gl eo a n- r S -Se Tp TAa ARr Ration 0.25 N c gl eo a n- r S -Se Tp TAa ARr Ration 0.3 N c gl eo a n- r S -Se Tp TAa ARr Ration 0.3 N c gl eo a n- r S -Se Tp TAa ARr Ration
0.65 0.20 0.2 0.2
0.60
0.15
0.55 0.1 0.1
0.10
0.50
0.0
0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8
Generations Generations Generations Generations
0.8
No reparation No reparation
2.0 cla-STAR cla-STAR 0.6 gen-STAR 0.6 gen-STAR
1.5
0.4 0.4
No reparation 1.0
0.2 cla-STAR 0.2
gen-STAR
0.5
0 2 4 6 8 0 2 4 6 8 0 2 4 6 8
Generations Generations Generations
1.0
0.8 No reparation
cla-STAR
0.7 gen-STAR 0.9
0.6 0.8
0.5
0.7
0.4 No reparation
cla-STAR
0.3 0.6 gen-STAR
0 2 4 6 8 0 2 4 6 8
Generations Generations
Figure20: FairFaceresultsforSeqGenSeqClass. Top: showsaccuracy,accuracydifference,demographic
parity difference, and equalized odds difference. For the latter three, lower values are better. Center: KL-
Divergence between fairness ideal and classifiers, and between fairness ideal and generator strata, and
the class balance. Bottom: shows the group balance for sensitive attributes race and age by reporting
the plurality race and age at each generation. Shading shows collapsed generations. For this dataset, the
annotator for race (A ) as roughly 45% utility on all groups aside from ‘white,’ where it is 80% accurate.
S1
This is despite the near-perfect balance between racial groups. This effects our annotations for race, which,
alongside with a similarly biased lineage of generators leads to a lack of samples from non-white races. This
thwarts STAR, resulting in similar unfairnesses as results without reparation.
38
ycaruccA
laedI
riaF
trw
viD-LK
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
ycaruccA
laedI
riaF
trw
viD-LK
ecnereffiD
ycaruccA
)ecar(
puorG
dezitironiM
laedI
riaF
trw
viD-LK
ecnereffiD
PD
ssalC
laicifeneB
ecnereffiD
PD
)ega(
puorG
dezitironiM
ssalC
laicifeneB
ecnereffiD
sddOE
puorG
dezitironiM
ecnereffiD
sddOE50 STAR Category 50 STAR Category 50
Minor, Class 0 Minor, Class 0
40 Major, Class 0 40 Major, Class 0 40
STAR Category Minor, Class 1 Minor, Class 1
30 Major, Class 1 30 Major, Class 1 30 Minor, Class 0
Major, Class 0
20 20 20 Minor, Class 1
Major, Class 1
10 10 10
0 0 0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
50 35
40
40 30
STAR Category STAR Category
30 Minor, Class 0 30 Minor, Class 0
Major, Class 0 Major, Class 0 25
STAR Category
20 M Mi an joo rr ,, CC ll aa ss ss 11 20 M Mi an joo rr ,, CC ll aa ss ss 11
20
Minor, Class 0
Major, Class 0
10
10 Minor, Class 1
15 Major, Class 1
0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 21: ColoredMNIST strata balances for datasets in the SeqGenSeqClass setting. Top: strata
resulting from classifiers without reparation, strata resulting from classifiers with cla-STAR, and the
strata used to train classifiers with cla-STAR. Bottom: strata resulting from generators without repa-
ration, strata resulting from generators with gen-STAR, and the strata used to train generators with
gen-STAR.Weseethatgen-STARaccomplishesamorebalancedstratathancla-STAR,butultimately
both are unable to get perfect balance due to model collapse casuing class imbalance.
60 STAR Category 50 STAR Category 50 STAR Category
Minor, Class 0 Minor, Class 0 Minor, Class 0
Major, Class 0 40 Major, Class 0 40 Major, Class 0 40 Minor, Class 1 Minor, Class 1 Minor, Class 1
Major, Class 1 30 Major, Class 1 30 Major, Class 1
20 20 20
10 10
0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
50
STAR Category
60 Minor, Class 0 40 40
Major, Class 0
Minor, Class 1
40 Major, Class 1 30 30
STAR Category STAR Category
20 Minor, Class 0 20 Minor, Class 0
20 Major, Class 0 Major, Class 0
10 Minor, Class 1 10 Minor, Class 1
Major, Class 1 Major, Class 1
0
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 22: ColoredSVHN strata balances for datasets in the SeqGenSeqClass setting. Top: strata
resulting from classifiers without reparation, strata resulting from classifiers with cla-STAR, and the
strata used to train classifiers with cla-STAR. Bottom: strata resulting from generators without repa-
ration, strata resulting from generators with gen-STAR, and the strata used to train generators with
gen-STAR. We can see that gen-STAR results in slightly more balanced representation than does cla-
STAR.
39
%
atartS
reifissalC
%
atartS
rotareneG
%
atartS
reifissalC
%
atartS
rotareneG
%
atartS
reifissalC
%
atartS
rotareneG
%
atartS
reifissalC
%
atartS
rotareneG
%
atarts
RATS
%
atarts
RATS
% atarts
RATS
%
atarts
RATS50
20 STAR Category STAR Category STAR Category
Minor, Class 0 30 Minor, Class 0 40 Minor, Class 0 15 Major, Class 0 Major, Class 0 Major, Class 0
Minor, Class 1 Minor, Class 1 30 Minor, Class 1
Major, Class 1 20 Major, Class 1 Major, Class 1
10
20
5 10
10
0 0 0
0 5 10 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Generations Generations Generations
30
STAR Category STAR Category 30 STAR Category
Minor, Class 0 Minor, Class 0 Minor, Class 0
30
Major, Class 0 Major, Class 0 Major, Class 0
20 Minor, Class 1 Minor, Class 1 20 Minor, Class 1
Major, Class 1 20 Major, Class 1 Major, Class 1
10 10
10
0 0 0
0 5 10 0 5 10 5 10
Generations Generations Generations
Figure23: FairFacestratabalancesfordatasetsintheSeqGenSeqClasssetting. Top: strataresulting
from classifiers without reparation, strata resulting from classifiers with cla-STAR, and the strata used
to train classifiers with cla-STAR. Bottom: strata resulting from generators without reparation, strata
resulting from generators with gen-STAR, and the strata used to train generators with gen-STAR.
Neither AR simulation is able to achieve balance due to large racial disparities.
40
%
atartS
reifissalC
%
atartS
rotareneG
%
atartS
reifissalC
%
atartS
rotareneG
%
atarts
RATS
%
atarts
RATSRelative
Actual
0.99
0.92
0.90
0.98
0.88
0.86
0.97
0.84
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.05 0.025
0.04 0.020
0.03 0.015
0.010
0.02
0.005
0.01
0.000
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.44
0.425
0.42 0.400
0.375
0.40
0.350
0.38 0.325
0.300
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.10
0.3
0.08
0.2 0.06
0.04
0.1
0.02
0 10 20 30 40 0 10 20 30 40
Generations Generations
Figure 24: Accuracy, demographic parity difference, and equalized odds difference in SeqClass on
ColoredMNIST. Higher accuracy is better, but for the FML metrics higher difference is worse. Left: Per-
formances on the test set. Right: Relative performances between models. The model quality of accuracy
and equalized odds in the relative performances is far higher than the actual results. In equalized odds, this
shows that even if small unfairnesses were tolerated over while training each classifier, the result over time
accrues high unfairness compared to the original testing set.
41
ycaruccA
ffiD
ycaruccA
puorG
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqE
ycaruccA
ffiD
ycaruccA
puorG
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqEActual Relative
0.8
0.9
0.7
0.8
0.6
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.3 0.25
0.20
0.2
0.15
0.10
0.1
0.05
0.00
0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.8
0.6
0.6
0.4
0.4
0.2
0.2
0 10 20 30 40 0 10 20 30 40
Generations Generations
0.8 0.8
0.6
0.6
0.4
0.4
0.2
0.2
0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
Figure 25: Accuracy, demographic parity difference, and equalized odds difference in SeqClass on
ColoredSVHN. Higher accuracy is better, but for the FML metrics higher difference is worse. Left: Re-
sults on the test set. Right: Relative performances between models.
42
ycaruccA
ffiD
ycaruccA
puorG
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqE
ycaruccA
ffiD
ycaruccA
puorG
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqERelative
Actual
1.0
0.9
0.8
0.9
0.7
0.6
0.8
0.5
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0
0.8
0.8
0.6
0.6
0.4
0.4
0.2
0.2
0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0 0.8
0.8
0.6
0.6
0.4
0.4
0.2
0.2
0.0
0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0
1.0
0.9
0.8
0.8
0.7 0.6
0.6
0.4
0.5
0 10 20 30 0 10 20 30
Generations Generations
0.56
0.60
0.54
0.52 0.55
0.50
0.50
0.48
0.45
0.46
0 10 20 30 0 10 20 30
Generations Generations
Figure 26: Accuracy, demographic parity difference, equalized odds difference, and rates of the beneficial
43
class and minoritized group in SeqGenSeqClass on ColoredMNIST. Higher accuracy is better, but for the
FMLmetricshigherdifferenceisworse. Left: Resultsonthetestset. Right: Relativeperformancesbetween
models.
ycaruccA
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqE
ssalC
laicifeneB
spuorG
dezitironiM
ycaruccA
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqE
ssalC
laicifeneB
spuorG
dezitironiMActual Relative
1.0
0.8
0.7 0.8
0.6
0.6
0.5
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0 10 20 30 40 0 10 20 30 40
Generations Generations
1.0
0.8
0.7 0.8
0.6
0.6
0.5
0.4
0.4
0 10 20 30 0 10 20 30
Generations Generations
0.6
0.5
0.4 0.4
0.3
0.2
0.2
0.0
0 10 20 30 0 10 20 30
Generations Generations
Figure 27: Accuracy, demographic parity difference,44equalized odds difference, and rates of the beneficial
class and minoritized group in SeqGenSeqClass on ColoredSVHN. Higher accuracy is better, but for the
FMLmetricshigherdifferenceisworse. Left: Resultsonthetestset. Right: Relativeperformancesbetween
models.
ycaruccA
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqE
ssalC
laicifeneB
spuorG
dezitironiM
ycaruccA
ffiD
ytiraP
cihpargomeD
ffiD
sddO
dezilauqE
ssalC
laicifeneB
spuorG
dezitironiM0.932 50% 0.0175 50% 0.36 50% 0.030 50% 0.930 50% + cla-STAR 0.0150 50% + cla-STAR 50% + cla-STAR 0.025 50% + cla-STAR 0.35
0.928 0.0125 0.020
0.926 0.0100 0.34 0.015
0.924 0.0075 0.010
0.0050 0.33
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
Figure 28: ColoredMNIST results for SeqClass when training with half synthetic and half non-synthetic
data. Plotsshowaccuracy,accuracydifference,demographicparitydifference,andequalizedoddsdifference.
Non-synthetic data is sampled according to the strata of the prior classifier to model disparity amplifica-
tion. cla-STAR leads to more DP and EOdds fairness even while disparity amplification and performative
prediction occur.
0.04
50% 50%
0.92 50% + cla-STAR 0.38 0.15 50% + cla-STAR 0.03 50% + gen-STAR 50% + gen-STAR
0.90 0.36 50% 0.10 50% + cla-STAR 0.02 0.34 50% + gen-STAR
0.88 50% 0.05
0.86 5 50 0% % + + c gl ea n-S -ST TA AR R 0.01 0.32
0.30 0.00
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
0.09 5 50 0% % + cla-STAR 0.10 5 50 0% % + cla-STAR 5 50 0% % + cla-STAR 0.60 5 50 0% % + cla-STAR
0.08 50% + gen-STAR 0.08 50% + gen-STAR 0.55 50% + gen-STAR 0.55 50% + gen-STAR
0.06
0.07 0.50 0.50
0.06 0.04
0.45
0.05 0.02 0.45
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations Generations
Figure29: Trainingonamixtureofsyntheticandnon-syntheticdata, whereperformativeprediction, model
collapse,anddisparityamplificationcanco-occur,forColoredMNISTonSeqGenSeqClass. Top: accuracy,
accuracy difference, demographic parity difference, and equalized odds difference. Bottom: KL-Divergence
between the STAR fairness ideal and the strata of classifiers and generators, the group balance, and the
label balance.
45
ycaruccA
ycaruccA
ecnegreviD-LK
ecnereffiD
ycaruccA
ffiD
ycaruccA
ecnegreviD-LK
ffiD
ytiraP
cihpargomeD
puorG
dezitironiM
ecnereffiD
PD
ffiD
sddO
dezilauqE
ssalC
laicifeneB
ecnereffiD
sddOE35 35 STAR Category 35
Minor, Class 0
30 STAR Category 30 Major, Class 0 30 Minor, Class 1
Minor, Class 0 Major, Class 1
25 Major, Class 0 25 25
Minor, Class 1 STAR Category
20 Major, Class 1 20 20 M Mi an joo rr ,, CC ll aa ss ss 00
Minor, Class 1
15 15 15 Major, Class 1
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
35 35 35
30 STAR Category 30 STAR Category 30
Minor, Class 0 Minor, Class 0
25 Major, Class 0 25 Major, Class 0 25 STAR Category
Minor, Class 1 Minor, Class 1
Major, Class 1 Major, Class 1 Minor, Class 0
20 20 20 Major, Class 0
Minor, Class 1
15 15 15 Major, Class 1
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
35 STAR Category 35 STAR Category 35 STAR Category
Minor, Class 0 Minor, Class 0 Minor, Class 0
30 Major, Class 0 30 Major, Class 0 30 Major, Class 0 Minor, Class 1 Minor, Class 1 Minor, Class 1
Major, Class 1 Major, Class 1 Major, Class 1
25 25 25
20 20 20
15 15 15
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
Generations Generations Generations
Figure 30: Left: The strata of classifiers without reparation. Center: The strata resulting from models
with STAR. Right: The strata used to train models with STAR. Top: ColoredMNIST in SeqClass with
a mixture of synthetic and non-synthetic data. Second row: ColoredMNIST in SeqGenSeqClass with
a mixture of synthetic and non-synthetic data, reporting strata of the classifiers and using cla-STAR.
Bottom: ColoredMNISTinSeqGenSeqClasswithamixtureofsyntheticandnon-syntheticdata,reporting
strata of the generators and using gen-STAR.
46
%
atartS
reifissalC
%
atartS
reifissalC
%
atartS
rotareneG
%
atartS
reifissalC
%
atartS
reifissalC
%
atartS
rotareneG
%
atarts
RATS
% atarts
RATS
%
atarts
RATS