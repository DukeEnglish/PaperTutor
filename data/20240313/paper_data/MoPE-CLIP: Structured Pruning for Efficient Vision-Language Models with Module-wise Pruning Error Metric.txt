MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with
Module-wise Pruning Error Metric
HaokunLin1,2,4, HaoliBai3, ZhiliLiu3,5, LuHou3, MuyiSun2,
LinqiSong4, YingWei6†, andZhenanSun2†
1 SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences
2 CRIPAC&MAIS,InstituteofAutomation,ChineseAcademyofSciences
3HuaweiNoah’sArkLab 4CityUniversityofHongKong
5 TheHongKongUniversityofScienceandTechnology 6 NanyangTechnologicalUniversity
haokun.lin@cripac.ia.ac.cn,ying.wei@ntu.edu.sg,znsun@nlpr.ia.ac.cn
Abstract isessentialtoexplorecompactVLPmodelsforreal-world
applications[49,55]. Weidentifytwocompressionsettings
Vision-language pre-trained models have achieved im- for different platforms. First, many edge servers lack the
pressive performance on various downstream tasks. How- computationalpowertohandletheentirepre-trainedmodel.
ever, their large model sizes hinder their utilization on We define “pre-training stage compression” to address
platforms with limited computational resources. We find this, which involves compressing zero-shot VLP models
that directly using smaller pre-trained models and apply- andpre-trainingthemonmillionsofimage-textpairstocre-
ingmagnitude-basedpruningonCLIPmodelsleadstoin- atecompact,general-purposemodels. Second,clients,like
flexibilityandinferiorperformance. RecenteffortsforVLP mobilephones,oftenrequiremultipletask-specificmodels
compression either adopt uni-modal compression metrics for various scenarios. To meet this demand, we introduce
resulting in limited performance or involve costly mask- “fine-tuningstagecompression”. Forexample,CLIP[42]
search processes with learnable masks. In this paper, we excelsincross-modalretrievaltasks,comprisingimage-to-
firstproposetheModule-wisePruningError(MoPE)met- text retrieval (TR) and text-to-image retrieval (IR). Given
ric, accuratelyassessingCLIPmoduleimportancebyper- the pre-computable and offline storable nature of visual or
formance decline on cross-modal tasks. Using the MoPE textualrepresentations[9,61],ourobjectiveistocompress
metric,weintroduceaunifiedpruningframeworkapplica- visionencoderforTRtaskandtextencoderforIRtask.
ble to both pre-training and task-specific fine-tuning com-
To reduce inference costs, smaller pre-trained models,
pression stages. For pre-training, MoPE-CLIP effectively
like the various-sized ViT-based CLIP models in [42],
leverages knowledge from the teacher model, significantly
are considered. However, individually pre-training each
reducing pre-training costs while maintaining strong zero-
model is computationally expensive [7], and the limited
shotcapabilities. Forfine-tuning,consecutivepruningfrom
architectural diversity may not meet various deployment
widthtodepthyieldshighlycompetitivetask-specificmod-
needs. Consequently,wedelveintomoreflexiblesolutions
els. Extensive experiments in two stages demonstrate the
that leverage pruning techniques to compress VLP mod-
effectivenessoftheMoPEmetric,andMoPE-CLIPoutper-
els. Nonetheless,thesuboptimalperformanceofmagnitude
formspreviousstate-of-the-artVLPcompressionmethods.
pruningonCLIPasshowninFigure1raisesthechallenge
ofidentifyingamorecompetitivepruningstrategy.
RecentVLPpruningmethods[49,55,57]canbebroadly
1.Introduction
categorizedintotwocategories. Thesimplestwayinvolves
applying uni-modal Transformer pruning methods. How-
Vision-Language Pre-training (VLP) models have demon-
ever, despite the effectiveness of metrics such as magni-
stratedstrongmulti-modalrepresentationlearningabilities
tude and loss-awareness on single-modality transformers
[18, 22, 28, 30]. However, their impressive performance
[15,16,34,37],ourexperimentshaverevealedunsatisfac-
comes at the cost of a large number of parameters, limit-
tory performance when directly applying them to the mul-
ingtheiruseonresource-constraineddevices. Therefore,it
timodal CLIP models. EfficientVLM [55] uses the “every
†Correspondingauthors. other” pruning strategy during the pre-training stage, but
1
4202
raM
21
]VC.sc[
1v93870.3042:viXra𝐸
!
𝐸 !$ 𝐸 !$ Linear 𝐸 !#
. . . .
⑴Fine-tuned Evaluation On MSCOCO Dataset
𝐸
"
𝐸 "$ 𝐸 " 𝐸 "
(a) Pretrained Large Model (b) Pretrained (CLIP-ViT-B/32) (c) Substituted (SE-CLIPv) (d) Pruned
CLIP-ViT-B/32: competitivebut fixed size SE-CLIPv: inferior andfixed size
MagnPruned:inferiorand flexible MoPE-Pruned:competitive and flexible ⑵Zero-shot Evaluation on MSCOCO Dataset
Figure1. Empiricalcomparisonbetween(a)theoriginallargeCLIPmodelandthreesmallermodelswithcompressedvisionencoders,
including (b) a pre-trained small CLIP Model; (c) a small model obtained by substituting the original vision encoder in (a) with the
smallvisionencoderof(b);and(d)asmallmodelwiththevisionencoderprunedfrom(a). Weperformpruningduringpre-trainingor
fine-tuning,evaluated(1)afterfine-tuningor(2)withzeroshot. NotethatwetrainthesubstitutedencoderEs in(c)andEp in(d)with
v v
image-textcontrastivelossL .TRandIRstandforimage-to-textandtext-to-imageretrieval,respectively.
itc
this approach, commonly used in BERT models, does not MoPE-CLIP largely outperforms the same amount of pa-
deliver optimal outcomes in our experiments. These find- rametersTinyCLIP[57]by5.3%TR@1and4.0%IR@1on
ings underscore the inadequacy of existing metrics in as- MSCOCO retrieval tasks, while surpassing MCD [21] and
sessingmoduleimpactonmulti-modaltasks. Ontheother ALIP[60]on11zero-shotclassificationtasksby18.6%and
hand, mask-based pruning is employed to identify crucial 17.0%. Thecontributionsofourworkare:
modules.UPop[49]introducesaprogressivesearchingpro- • WeintroduceMoPEmetricforpreciselyassessingtheim-
cess, which is unsuitable for the pre-training stage. Tiny- portanceofCLIPmodulesincross-modaltasks.Utilizing
CLIP[57]suggestsdistillationwithweightinheritancefor MoPE,wepresentastructuredpruningframeworkcom-
smallmodels,involvingatime-consumingmulti-stagedis- bined with advanced distillation loss, offering a unified
tillation process. Additionally, TinyCLIP is pre-trained on solutionforpre-trainingandfine-tuningcompression.
theLAION400Mdataset[45],leavinguncertaintyaboutits • MoPE-CLIP model exhibits SOTA performance in both
effectiveness for fine-tuning stage compression with lim- trainingspeedandaccuracyacrossextensiveexperiments,
iteddata. Insummary,traditionalpruningmetricsneedim- surpassingexistingbenchmarksinvariousdomains.
provementforVLPpruning,andmask-basedpruningisnot
2.PreliminaryStudyofDownsizingCLIP
efficient enough during pre-training. Thus, a unified solu-
tionforouridentifiedtwo-stagecompressionisunexplored.
Inpursuitoftheobjectivetoscaledownavision-language
modelsuchasCLIP,variousalternativescomeintoconsid-
To tackle these challenges, we introduce MoPE-CLIP,
eration. Architecturally, one may opt to substitute an en-
aneffectivemask-freestructuredpruningsolutionforboth
coderwithitscounterpartinasmallermodel,asexemplified
pre-training and fine-tuning stage compression. We first
by CLIP-ViT-B/32 in Figure 1(b), or alternatively, directly
propose the Module-wise Pruning Error (MoPE) metric,
prune the encoder to any desired size. From a practical
which quantifies a module’s importance by measuring the
standpoint, downsizing can be executed either during pre-
performance drop in multi-modal tasks if that module is
trainingpriortodeploymentfordownstreamtasksorduring
pruned.MoPEpreciselyevaluatesthepruningsensitivityof
fine-tuning in clients. This section embarks on a prelimi-
heads,FFNneuronsinthewidthdirection,andTransformer
nary examination of these alternatives, laying the ground-
layers in the depth direction. Based on the MoPE metric,
workforourproposedpruningstrategy.
weproposeaunifiedmask-freepruningframework. Inthe
pre-training stage, we calculate MoPE using zero-shot re- Substitutingwithsmallermodelsprovesunsatisfactory.
trieval on the MSCOCO validation set and simultaneously WesubstitutetheoriginalvisionencoderE ofCLIPwith
v
prunebothwidthanddepthcomponents. Inthefine-tuning asmalleronefromCLIP-ViT-B/32, resultinginthedown-
stage, MoPE is calculated by the performance decline on sized model SE-CLIP . We freeze the language encoder
V
downstream tasks. To achieve higher pruning ratios, we to facilitate applications like image-to-text retrieval (TR),
prioritize pruning in the width direction before pruning in wheretextfeaturesbythelanguageencoderareoftentimes
thedepthdirection. Moreover, wedistillbothcross-modal stored without modification. The modified vision encoder
anduni-modalknowledgefromtheoriginalmodel’saligned and the frozen language encoder are misaligned, necessi-
featurespaceandtext/visionencodertoenhancethepruned tating further training. Concretely, we conduct fine-tuning
model’s capacity. Extensive evaluations demonstrate our ofalinearlayerandthevisionencoderonthedownstream
2Cross-modal KD + Uni-modal KD
𝓛𝒔𝒊𝒎+𝓛𝒇𝒆𝒂𝒕+𝓛𝒉𝒊𝒅𝒏⑷
𝐿×
⑵
𝐿×
𝓛𝒔𝒊𝒎
$%×
&
. .
F C2 ℝ&’×’ ⑴ F C2 ⑶ F C2
Prune Prune
F C1 ℝ’×&’ Width F C1 Depth F C1 𝓛𝒇𝒆𝒂𝒕
Head Head Head Head Head Head Head Head 𝓛𝒉𝒊𝒅𝒏
𝑊 𝑊
Fine-tuned𝑊 V×
ison/Text Encoder
MoPE-C2 L×
IPwEncoder
MoPE-C2 L×
IP Encoder
𝐸) 𝐸* 𝐸 )+ 𝐸 *+
Teacher Model Pruned Model
(a) Width-first-then-depth pruning during fine-tuning stage (b) Knowledge distillation (KD)
Notation 𝐿× Full Model Model with Module
FFN Pruned Neuron
𝓛𝒔𝒊𝒎+𝓛𝒇𝒆𝒂𝒕+𝓛𝒉𝒊𝒅𝒏
$ &%× 𝜃removed
⑵ For One Specific Module 𝜃
Head Pruned Head F C2 F C2 𝑀𝑜𝑃𝐸 ! = 𝒵 0 0 0 0
②④ Pruned Layer F C1 ⑴ F C1
—
Head Head Head Head Prune Head Head Cost Table
⑴⑶One-shot Pruning 𝑊× Width & Depth 𝑊 2× 𝒵 0 0 0 0
⑵⑷Retraining with KD Zero-shot Vison/Text Encoder MoPE-CLIP Encoder
(c) Width-and-depth pruning during pre-training stage (d) An illustration of 𝑴𝒐𝑷𝑬metric
Figure 2. The overall workflow of training MoPE-CLIP. (a). During the fine-tuning stage, we apply width-first-then-depth pruning on
fine-tunedCLIPvisionortextencodertoobtainpowerfultask-specificmodels.(b).Anillustrationofourdistillationprocess,transferring
cross-modalanduni-modalknowledge.(c).Duringthepre-trainingstage,weapplyconsecutivepruninginthewidthanddepthdirections
onzero-shotCLIPencoders.(d).AnillustrationofMoPEmetric,measuringtheperformancedropofCLIPafterremovingthemoduleθ.
datasetMSCOCO[31], andresorttothecross-modalcon- Bothpre-trainingandfine-tuningpruningmeritconsid-
trastivelossfunctionL ,whichistheInfoNCElosscom- eration. An intriguing question to explore is whether a
itc
putedbetweenimageandtextfeatures. Unfortunately,SE- vision-languagemodel,pre-trainedbeforedeployment,out-
CLIP experiencesasubstantialperformancedeclinecom- performs one pruned to the same size during fine-tuning.
V
pared with the original CLIP, as illustrated in Figure 1(1). MoPE-CLIP and MoPE-CLIP represent the two ver-
base V
Thisdeclinemaybeattributedtotheformidablechallenge sionsusingourproposedpruningframeworkwhichwewill
ofaligningtwodisassociatedencodersoriginatingfromdis- detailinthenext. FromFigure1(1),weconcludethatgiven
tinct vision-language models. This observation, coupled the same target size, exploring both pre-training pruning
with the lack of flexibility in selecting a target size, dis- and fine-tuning pruning is worthwhile. First, their appli-
suades us from further investigating this downsizing strat- cation scenarios differ as discussed. Second, pruning dur-
egyduringpre-training. Thus,weredirectourfocustothe ing pre-training, when more parallel data is accessible for
alternativechoiceofpruning. preservingcross-modalcorrelations,provesmoreeffective,
whilepruningduringfine-tuningwhichdoesnotunderper-
Furtherinvestigationisrequiredforsuccessfulpruning. formsignificantlyenjoystheadvantageofhighefficiency.
Specifically, we implement MagnCLIP [15], a widely
V
adopted yet straightforward pruning strategy, which selec- 3.Method
tively prunes attention heads and FFN neurons below a
specifiedmagnitudethreshold. Adjustingthisthresholdre- We introduce the MoPE metric in Section 3.1 to measure
sultsinvaryingsizesofprunedmodels.Weutilizethesame module influence accurately in cross-modal tasks. In Sec-
objectivefunctionasL insubstitutiontotrainthepruned tions 3.2 and 3.3, we present our pruning framework and
itc
model Ep. Despite the expected flexibility in target size, knowledgedistillationloss,jointlyimprovingthetwo-stage
v
MagnCLIP ensures only a relatively satisfactory perfor- compressionperformance.
V
mance, provided that at least 50% of parameters are re-
3.1.Module-wisePruningError
tained, as depicted in Figure 1(1)(2). An imperative need
existsforaneffectivepruningstrategythatsimultaneously We propose a new metric called module-wise pruning er-
meets flexibility and generalization capacity, thus forming ror(MoPE)toevaluatetheimportanceofdifferentmodules
thebasisforourproposedapproach. in the CLIP model, such as Multi-Head Attention (MHA)
3heads,Feedforward(FFN)neurons,andentireTransformer query,key,value,andoutputmatrices. Thisprocesscalcu-
layers. Forheadsandneurons, somecommonlyusedmet- lates the MoPE metric, subsequently establishing the cost
rics in width pruning, like magnitude [15], fail to accu- table of heads C . For the FFN block, which includes
head
ratelycapturetheirimpactsonmulti-modaltasks,leadingto an up-projection W
1
∈ Rd×dff and a down-projection
suboptimalresults. ForTransformerlayers,existingworks layerW
2
∈ Rdff×d,wheredisthehiddendimensionand
[12, 43] mainly adopt every other strategy on depth prun- d =4disthenumberofintermediateneurons. Sinceit’s
ff
ingforBERT.OurexperimentsinSection4.3demonstrate time-costtoenumeratealld neurons,wedividetheminto
ff
thatthissimplisticstrategyfallsshortwhenappliedtoCLIP N groups and measure the MoPE of each group to obtain
models. Wesupposethateveryotherstrategycannotmea- C . Then the insignificant heads and groups of neu-
neuron
surethepruninginfluenceonanotherencoderandthusleads ronswouldbepruned,andweuseknowledgedistillationto
toinferiorperformance. Theseresultspresentanewchal- transferknowledgefromthefixedteachermodeltothefinal
lengeinselectingasuitablemetrictopruneVLPmodels. MoPE-CLIPw. Second,wecompresstheMoPE-CLIPwin
To overcome these issues, our proposed MoPE metric the depth direction. We compute the MoPE for L Trans-
effectively assesses the module’s importance with respect formerlayersofMoPE-CLIPwandcreatetheC . With
layer
tomulti-modaldownstreamtasks,offeringaconsistentand the assistance of C , we evaluate the priority of layers
layer
more accurate measure for both width and depth pruning. precisely and prune less important ones. The final MoPE-
Inparticular,weregarddifferentheads,groupsofneurons, CLIPisobtainedbydistillingfromthefixedteachermodel.
andlayersasdifferentmodules. FromFigure2(d),theim-
Pre-trainingStage. Wesimultaneouslycompressthevi-
portance of module θ is empirically measured by the per-
sion and text encoder of the large model to generate more
formancedeclinebetweenmoduleθ removedCLIPmodel
general small models in the pre-training stage. In addi-
f andtothefullCLIPf counterpartasfollows:
φ−θ φ tiontomodelcapacity,trainingcostisanothercrucialchal-
lenge. The width-first-then-depth strategy involves a two-
MoPE =Z[f ]−Z[f ], (1)
θ φ φ−θ stage retraining process, incurring high costs. Moreover,
where Z is the zero-shot evaluation function, i.e., Recall theknowledgeacquiredineachretrainingprocessexpands
Mean for retrieval tasks. One module θ with a higher asmoreimage-textpairsareintroducedduringpre-training.
MoPE value indicates that this module is more sensitive Therefore,wecombinethewidth-and-depthpruninginto
θ
to pruning and plays a more crucial role in cross-modal a single stage, as depicted in Figure 2(c). In particular,
tasks. Thus, preserving such modules becomes a priority we parallelly compute the MoPE metric for heads, groups
duringpruning. ByutilizingtheMoPE ,wecaneasilycre- of neurons, and layers of zero-shot CLIP’s vision and text
θ
ate cost tables C =
(cid:80)n
{MoPE }. These cost tables encoders. After creating the cost tables, the pruning pro-
θ i=1 θi
aregeneratedfordifferentheads(C ),groupsofneurons cessiscompleteddirectlyinseveralseconds. Thenwepre-
head
(C ),andlayers(C ). Theyserveasreferencesfor train the pruned model on one small-scale image-text pre-
neuron layer
selectingoptimalarchitectures,allowingustoretaincritical trainingdataset(e.g., theCC3Mdataset)andobtainthefi-
moduleswhilereducingtheoverallmodelsize. nalMoPE-CLIP.OurexperimentsinSection4.2showthat
our MoPE-CLIP largely outperforms several efficient pre-
3.2.UnifiedPruningFrameworkBasedonMoPE
trainingmodels[21,26,60], indicatingthatpruningofthe
largemodelsprovidesabetterinitializationforpre-training.
Recent VLM compression works focus either during the
pre-trainingstage[57]orfine-tuningstage[49]. However, Pruning Efficiency. Calculating the MoPE metric for
the comprehensive solution for these two stages is under- eachmoduletakesafewseconds,andcomputationsforall
explored.LeveragingourMoPEmetric,weintroduceauni- modulescanbeparallelized.Thus,theoveralltimeofestab-
fiedpruningframeworkaimedatsolvingthischallenge. lishingcosttablesismuchlessthanacompletefine-tuning
orpre-trainingprocess.Subsequently,wecandirectlyprune
Fine-tuning Stage. The primary challenge lies in en-
atdifferentratiostoobtainaseriesofcompactmodels.
hancing the performance of task-specific pruned models.
To achieve high compression ratios, we explore three dis- 3.3.DistillationtoMoPE-CLIP
tinctpruningstrategiesinbothwidthanddepthdirections.
In contrast to previous distillation methods applied to ViT
Empirical analysis reveals that the width-first-then-depth
or BERT [19, 62, 63, 68], we design an advanced distil-
pruningparadigmisthemosteffective,asdiscussedinSec-
lation loss that effectively transfers both cross-modal and
tion 4.3. Specifically, one encoder of CLIP has L layers,
uni-modal knowledge from large CLIP (teacher model) to
andeachlayerconsistsofaMHAblockandaFFNblock.
prunedMoPE-CLIP(studentmodel)showninFigure2(b).
First,wecompressthefine-tunedCLIPmodelinthewidth
direction,asshowninFigure2(a).FortheMHAblockwith Cross-modalKnowledge. TheCLIPmodelcomputesthe
N heads,weindependentlypruneL×N headswiththeir cross-modalsimilaritymatrixforretrievalandclassification
h h
4VisionEncoder MSCOCO(5Ktestset) TextEncoder MSCOCO(5Ktestset)
Approach Approach
Width Depth Parmas TR@1 TR@5 TR@10 Width Depth Parmas IR@1 IR@5 IR@10
TeacherModel 1024 24 304M 76.2 92.9 96.4 TeacherModel 768 12 85M 58.8 82.8 89.5
CLIP-VIT-B/32 768 12 88M 67.5 88.0 93.4 CLIP-VIT-B/32 512 12 38M 49.4 75.8 84.7
SE-CLIPV 768 12 88M 56.1 81.0 89.1 SE-CLIPT 512 12 38M 58.5 82.9 89.6
MoPE-CLIPV 384 18 86M 69.7 90.4 95.0 MoPE-CLIPT 384 12 42M 59.6 83.2 89.8
Table1.Image-to-textretrievalresultsofthreesmallmodelarchitec- Table2.Text-to-imageretrievalresultsofthreesmallmodelarchitec-
turesonMSCOCOdataset.Allmodelsaretrainedwithdistillation. turesonMSCOCOdataset.Allmodelsaretrainedwithdistillation.
tasks. The teacher model exhibits a more closely aligned VisionEncoder MSCOCO(5Ktestset)
Pruning
textualandvisualembeddingspace,resultinginadditional Width Depth Parmas TR@1 TR@5 TR@10
valuableknowledgewithintheirsimilaritymatrices. Toen- TeacherModel 1024 24 304M 76.2 92.9 96.4
hance the cross-modal capabilities of pruned models, we MagnCLIPV [15] 5 31 82
4
2 24
4
1 15 13 5M
M
7 61 4. .2
2
9 80 6. .8
6
9 95 2. .2
8
minimizethesoftcross-entropyloss(SCE)betweenstudent
similaritymatrixSandteachersimilaritymatrixSˆ,i.e.,
DynaCLIPV [17]
5 31 82
4
2 24
4
1 15 13 5M
M
7 73 0. .9
3
9 92 0. .0
0
9 96 4. .0
9
384 18 86M 67.6 88.7 94.1
L sim =SCE(S,Sˆ). (2) UPop-CLIP[49] N/A N/A 474M‡ 70.8 90.8 95.2
N/A N/A 280M‡ 56.1 82.4 90.2
Uni-modal Knowledge. The teacher model possesses 512 24 153M 74.7 92.2 96.4
more substantial and superior vision or text encoders. MoPE-CLIPV 384 24 115M 72.1 91.5 95.7
384 18 86M 69.7 90.4 95.0
Hence, it becomes crucial to transfer the knowledge em-
beddedwithintheselargerencoderstothestudentmodels. Table3. Image-to-textretrievalresultsofdifferentpruningmeth-
Following [19], we utilize the mean squared error (MSE) ods on the MSCOCO dataset with several pruning ratios. The
losstoensurethatthestudentmodel’sfeatures(F ,F )are Paramslabeledas‡denotetheparametersoftheentiremodel.
v l
assimilaraspossibletothoseoftheteachermodel(Fˆ ,Fˆ ):
v l
TextEncoder MSCOCO(5Ktestset)
L = 1 MSE(F ,Fˆ )+ 1 MSE(F ,Fˆ ). (3) Pruning Width Depth Params IR@1 IR@5 IR@10
feat 2 v v 2 l l TeacherModel 768 12 85M 58.8 82.8 89.5
Besides, we also perform intermediate-layer distillation to 384 12 42M 59.2 82.9 89.1
transfer the hidden states knowledge (i.e., the output of
MagnCLIPT[15]
192 12 21M 56.6 81.9 89.2
each Transformer layer) Hm v (m = 1,2,...,M), Hk l(k = DynaCLIPT[17] 3 18 94 2 1 12 2 4 22 1M M 5 59 7. .3 3 8 83 2. .0 3 8 89 9. .7 4
1,2,...,K) from the teacher model to the student model.
384 12 42M 59.6 83.2 89.8
Thedepth-prunedstudentwouldmimicthepreservedinter- MoPE-CLIPT
192 12 21M 58.0 82.6 89.8
mediatelayersintheteachermodel. Thehiddenlossis
Table4. Text-to-imageretrievalresultsofdifferentpruningmeth-
Lv =(cid:88)M MSE(Hm,Hˆm), (4) odsontheMSCOCOdatasetwithtwopruningratios.
hidn v v
m=1
task-specific models. We select cross-modal retrieval as
Ll hidn
=(cid:88)K
MSE(Hk l,Hˆk l), (5) our downstream tasks and evaluate the compressed model
k=1
1 ontheMSCOCO[31]andFlickr30K[41]datasets. Dueto
L = (Lv +Ll ). (6)
hidn 2 hidn hidn limitedspace,theresultsonFlickr30KareinAppendixC.2.
LearningObjective. Combiningthecross-modalknowl-
Implementation Details. We apply the width-first-then-
edgeanduni-modalknowledge,wefurtherincorporatethe
depth pruning on the vision or text encoder of FT-L14 to
contrastiveloss(L ). Thus,thefinaltrainingobjectiveis
itc obtain MoPE-CLIP and MoPE-CLIP . The MoPE met-
V T
riciscomputedbyTRMeanandIRMean,respectively. In
L=L +αL +βL +γL (7)
itc sim feat hidn particular,sinceindividuallyprocessingallneuronsistime-
Bydefault,wedonottuneandset(α,β,γ)=(1,103,1)to
consuming, we first rewire all FFN neurons according to
ensureabalancedmagnitudeoftheselosses. lossgradientlike[17],thendividethemintogroupsforac-
celeration. Knowledge distillation is added to enhance the
4.Experiments
performanceoffine-tunedCLIP-ViT-B/32andSE-CLIP.
4.1.Fine-tuningStageCompression
Further evaluation of three small model architectures.
ExperimentalSettings. Duringthefine-tuningstage,we Table 1 and Table 2 present the image-to-text retrieval
compress fine-tuned CLIP-ViT-L/14 (FT-L14) to create and text-to-image retrieval performance of three architec-
5VisionEnocder TextEncoder Params(M) MSCOCO(5Ktestset) Flickr30K(1Ktestset)
Method
Width Depth Width Depth Vision+Text TR@1 TR@5 TR@10 IR@1 IR@5 IR@10 TR@1 TR@5 TR@10 IR@1 IR@5 IR@10
Pre-trainedonWIT-400M
CLIP-ViT-L/14[42] 1024 24 768 12 304+85 56.3 79.4 86.6 36.5 61.1 71.2 85.2 97.5 99.1 64.9 87.3 92.2
CLIP-ViT-B/32[42] 768 12 512 12 88+38 50.1 75.0 83.5 30.5 56.0 66.9 78.8 94.9 98.2 58.8 93.6 90.2
Pre-trainedonCC3M
EfficientVLM[55] 1024 12 768 6 152+42 46.6 71.7 81.3 35.9 61.6 71.8 78.8 94.9 98.2 58.8 93.6 90.2
TinyCLIP[57] 512 24 768 6 152+42 52.7 76.5 84.8 36.6 63.0 73.6 80.5 96.3 98.5 66.3 89.1 93.7
MoPE-CLIPlarge 512 24 384 12 152+42 58.0 81.6 88.5 40.6 66.0 75.5 86.5 97.7 99.0 69.8 90.6 95.3
DynaCLIPbase[17] 384 18 384 12 86+42 51.3 75.5 84.6 35.8 61.8 72.6 79.8 96.1 98.2 64.6 87.8 93.1
DynaCLIPsmall[17] 384 18 192 12 86+21 46.7 72.7 92.2 33.2 59.5 70.3 75.9 94.6 98.3 60.9 86.1 91.9
MoPE-CLIPbase 384 18 384 12 86+42 52.8 78.1 86.0 37.3 63.5 73.6 82.8 97.1 98.8 66.7 88.7 94.1
MoPE-CLIPsmall 384 18 192 12 86+21 50.3 75.9 84.8 35.6 61.7 72.2 80.2 95.6 98.5 64.7 87.8 93.0
Pre-trainedonYFCC15M
CLIP-ViT-B/32†[42] 768 12 512 12 88+38 20.8 43.9 55.7 13.0 31.7 42.7 34.9 63.9 75.9 23.4 47.2 58.9
SLIP-ViT-B/32†[38] 768 12 512 12 88+38 27.7 52.6 63.9 18.2 39.2 51.0 47.8 76.5 85.9 32.3 58.7 68.8
DeCLIP-ViT-B/32†[29] 768 12 512 12 88+38 28.3 53.2 64.5 18.4 39.6 51.4 51.4 80.2 88.9 34.3 60.3 70.7
UniCLIP-ViT-B/32†[26] 768 12 512 12 88+38 32.0 57.7 69.2 20.2 43.2 54.4 52.3 81.6 89.0 34.8 62.0 72.0
MCD-ViT-B/32†[21] 768 12 512 12 88+38 32.2 58.7 71.2 20.7 43.5 55.3 57.6 82.6 91.1 36.4 64.8 74.1
ALIP-ViT-B/32†[60] 768 12 512 12 88+38 46.8 72.4 81.8 29.3 54.4 65.4 70.5 91.9 95.7 48.9 75.1 82.9
MoPE-CLIPbase 384 18 384 12 86+42 55.6 78.6 86.1 37.1 63.1 73.5 86.1 97.9 99.6 66.4 89.2 94.2
Table5. Zero-shotimage-textretrievalresultsonMSCOCOandFlickr30Kdatasets. OurMoPE-CLIP pre-trainedonCC3Mdatasets
base
outperformstheCLIP-ViT-B/32pre-trainedonWIT-400Monallthemetrics.†denotestheresultsarereportedfrom[21,26,60].
tures, respectively. With similar parameters, our MoPE- 4.2.Pre-trainingStageCompression
CLIP performs best and surpasses CLIP-ViT-B/32 by
V
Experimental Setting During the pre-training stage, we
2.2% TR@1 and SE-CLIP by 13.6% TR@1. MoPE-
V
compressthezero-shotCLIP-ViT-L/14(ZS-14)model[42]
CLIP at2xcompressionratioalsooutperformsSE-CLIP
T T
to obtain compact general models. Subsequently, we pre-
and CLIP-ViT-B/32. These results indicate that compared
train our MoPE-CLIP and various baselines on a small-
to the pretrained small models and substituted encoder
scalepre-trainingdataset,CC3M[48]. Tofurtherassessthe
models, MoPE-CLIP and MoPE-CLIP provide better
V T
capabilitiesofourMoPE-CLIPmodel,wescaleuptraining
small CLIP models while maintaining flexibility. Addi-
usinglargerdatasets,includingCC12M[4]andYFCC15M
tionally,weobservethattheknowledgedistillationprocess
[29].Inaddition,weevaluateourpruningmethodonOpen-
actually improves the TR@1 of CLIP-ViT-B/32 and SE-
CLIPViT-B/16andreportresultsinAppendixC.5.
CLIP inFigure1. Thisdemonstratestheeffectivenessof
V
theteacher’sknowledge,butthearchitecturaldifferencebe-
tweenViT-L14andViT-B32limitsthefinalperformance. Implementation Details We simultaneously prune both
vision and text encoders of ZS-L14. For the vision en-
Comparison with Other Pruning Methods. We com- coder,weadoptwidth-and-depthpruningandcompressthe
pareourmodelswiththestate-of-the-artVLPcompression encoderto86Mparameters,whichissimilartoCLIP-ViT-
methodUPop[49]. Wealsoextendtheuni-modalpruning B/32. Forthetextencoder,wecompressitinthewidthdi-
methodsonCLIParchitecture,includingthedynamicprun- rection at two pruning ratios, resulting in MoPE-CLIP base
ingmethodDynaBERT[17]andmagnitude-basedpruning and MoPE-CLIP small. We also prune both the vision and
[15].Notably,distillationisappliedtoDynaCLIPandMag- text encoders to half-width, producing MoPE-CLIP large.
nCLIP, except for Upop whose result is from the original Themodule’simportanceisevaluatedontheMSCOCOval-
paper. As seen in Table 3, MoPE-CLIP performs signif- idation dataset and the Recall Mean serves as the MoPE
V
icantly better than other DynaCLIP and MagnCLIP at metric. MoredetailsareleftinAppendixB.
V V
thesamedepthandwidth,especiallytheTR@1. Compared
with UPop, our MoPE-CLIP with 153M vision encoder
V Zero-shotImage-textRetrieval. Table5showsthezero-
termed an entire model of 234M parameters largely sur-
shotretrievalresultsonMSCOCOandFlickr30Kdatasets.
passes the UPop-CLIP with 474M parameters on all met-
MoPE-CLIP consistently surpasses the CLIP-ViT-B32
base
rics. In addition, Table 4 shows that even at the 4x com-
in all Recall metrics. MoPE-CLIP maintains com-
small
pression ratio, our MoPE-CLIP still maintains high per-
T petitive results and outperforms the DynaCLIP with
small
formance on the text-to-image retrieval task, with only a
a clear margin. In addition, when compared with previous
0.8%dropinIR@1comparedtotheteachermodel. Were-
efficientpre-trainingmethods,MoPE-CLIP pre-trained
base
port performance under more pruning ratios and compare
onCC3Machieves52.8%TR@1and37.3%IR@1onthe
UpopwithKDinAppendixC.1. Weanalyzethedifference
MSCOCO dataset, which is 6.0% and 8.0% higher than
ofpreservedheadsbetweenMoPE-CLIP andDynaCLIP
V V ALIP [60] pre-trained on YFCC15M. The improvement is
in Appendix C.3, which further demonstrates the accurate
mainlyattributedtotheprunedlargemodelprovidingabet-
assessmentofMoPEmetrics.
terinitializationforpre-trainingvision-languagemodels.
6Pre-training Training
Method dataset epochs
CLIP-ViT-B/32†[42] YFCC15M 50 62.3 33.6 55.4 6.3 19.4 16.9 2.1 1.4 40.2 33.7 31.3 27.5
SLIP-ViT-B/32†[38] YFCC15M 50 72.2 45.3 65.9 6.8 28.3 21.8 2.9 1.9 45.1 44.7 38.3 33.9
DeCLIP-ViT-B/32†[29] YFCC15M 50 72.1 39.7 70.1 7.1 30.2 24.2 3.9 2.5 41.6 46.9 39.2 34.3
UniCLIP-ViT-B/32†[26] YFCC15M 50 78.6 47.2 73.0 8.1 32.5 23.3 3.4 2.8 50.4 48.7 41.2 37.2
MCD-ViT-B/32†[21] YFCC15M 32 80.3 49.6 73.2 7.9 40.0 30.5 3.4 3.0 55.3 54.0 44.7 40.2
ALIP-ViT-B/32†[60] YFCC15M 32 83.8 51.9 74.1 54.8 30.7 23.2 5.4 2.7 47.8 45.4 40.3 41.8
MoPE-CLIP YFCC15M 20 91.5 68.1 85.5 66.8 69.3 46.6 16.6 6.0 61.2 74.6 60.7 58.8
base
MoPE-CLIP CC3M 20 86.8 61.7 79.0 30.1 42.0 38.5 5.6 1.7 57.1 38.6 44.5 44.2
base
MoPE-CLIP CC12M 20 91.2 67.3 85.0 45.0 80.0 41.1 47.7 7.2 62.2 70.6 60.7 59.8
base
Table6. Top-1accuracy(%)ofzero-shotimageclassificationon11downstreamdatasets. OurMoPE-CLIP largelysurpassesother
base
state-of-the-artefficientpre-trainingmethodsusingfewertrainingepochs.†denotestheresultsarereportedfrom[21,26,60].
Zero-shot Classification. We adopt the Recall Mean on
theMSCOCOvalidationdatasetastheMoPEmetric,which
reflects the module influence of multi-modal tasks. To
demonstrate the robustness of Recall Mean on uni-modal
tasks, we further compare our MoPE-CLIP with other
base
efficient pre-training methods on zero-shot image classifi-
cationtasks. SLIP[38], DeCLIP[29], andUniCLIP[26]
incorporate fine-grained supervision to reduce the data re-
quirement. ALIP [60] and MCD [21] propose new frame-
workstoreducethenoiseandmisalignmentsinimage-text
pairs.WeutilizethesameprompttemplatesfollowingCLIP
[15]. Table6presentstheresultson11widelyusedbench- Figure 3. Comparsion of training efficiency. EfficientVLM and
marks. OurMoPE-CLIP pre-trainedontheYFCC15M TinyCLIP are trained for 25 epochs, while MoPE-CLIP large is
base
trainedfor20epochs.AllmodelsarecompressedfromCLIP-ViT-
datasetsignificantlysurpassespreviousmethodsandcreates
L/14ata2xcompressionratioandtrainedontheCC3Mdataset.
new state-of-the-art results, indicating the effectiveness of
MoPE-CLIPtowardsclassificationtasks.
Models CLIP-ViT-L/14 MoPElarge MoPEbase MoPEsmall
ComparisonwithVLPCompressionMethods. Weem- Params(M) 390 194↓50% 128↓67% 107↓73%
Latency(ms) 141.96 79.00↓44% 58.73↓59% 49.48↓65%
ploystate-of-the-artvision-languagecompressionmethods,
EfficientVLM [55] and TinyCLIP [57], to compress the
Table7.NvidiaV100GPUlatency(ms)onMSCOCOtestsets.
zero-shotCLIP-ViT-L/14model.Thesecompressedmodels
arethenpre-trainedontheCC3Mdatasetusingtherespec- whereMoPE-CLIPshowsasignificantspeedup.
tive loss functions. The zero-shot retrieval results, as pre-
Training Cost. The entire training process for MoPE-
sented in Table 5, unequivocally illustrate that our MoPE-
CLIP on the CC3M dataset only requires 40 hours on 8x
CLIP performs the best. Notably, even our MoPE-
large NVIDIA V100 GPUs, suggesting that pruning provides a
CLIP withareductionto66Mparametersstillsurpasses
base superiorsolutionforachievinggeneralcompactVLPmod-
the TinyCLIP and EfficientVLM. We further compare the
elswithflexibilityandminimaltrainingcost.
training process of these models in Figure 3. Our models
achievecompetitiveresultsinlesstrainingtime,highlight- 4.3.AblationStudy
ingthesignificantcontributionofourMoPEmetricinpre-
EffectsofMoPEMetric. Tofurtherdemonstratetheef-
serving crucial modules. In contrast to TinyCLIP, which
fectiveness of our MoPE metric, we conduct an ablation
focuses on cross-modal affinity, and EfficientVLM, which
study for depth pruning. We apply four commonly used
emphasizes uni-modal knowledge transfer, our approach
layer reduction strategies in BERT [12, 43] to the 0.375-
combines cross-modal and uni-modal distillation, proving
width MoPE-CLIP models, including (i) removal of ei-
moreeffectiveinenhancingprunedmodelcapacity. V
ther the bottom or top layers, (ii) the “Every Other” strat-
Inference Speedup. We measure the latency using Py- egy, and (iii) a Gradient-based approach that gauges layer
Torchinferencemodewiththebatchsizeof64inTable7, importancebyanalyzinggradientswithrespecttoallheads
7
01RAFIC
001RAFIC 101hcetlaC
srewolF
steP DTD sraC
tfarcriA 793NUS 101dooF teNegamI egarevACOCOtestset “a yellow bus and a white bus on a empty road.”
Setting Method
TR@1 TR@5 TR@10
TopLayers 70.1 90.2 95.4
Prune3layers
BottomLayers 70.8 90.4 95.1
for
EveryOther 69.2 90.0 94.9
0.375width “A group of youths play frisbee in a field.”
LossGradient 70.4 90.6 94.9
MWPE-CLIP
V MWPEmetric 72.2 91.2 95.5
TopLayers 57.6 81.7 88.6
Prune6layers
BottomLayers 63.9 88.0 93.5
for
EveryOther 66.6 88.9 93.6 “One cat sleeping on sofa arm and another on sofa cushion.”
0.375width
LossGradient 66.3 87.8 94.0
MWPE-CLIP
V MWPEmetric 69.7 90.4 95.0
Table8.AblationstudyofLayerSelectionstrategies.
Input Image FT-L14 FT-B32 SE-CLIPv MoPE-CLIPv
MSCOCO
MoPE-CLIPV
TR@1 TR@5 TR@10 TRMean Figure4. Grad-CAMvisualizationontheself-attentionmapsin
Depth-first-then-width 64.0 87.0 92.4 81.1 thelastlayerofvisionencoderfordifferentmodels.
Width-and-depth 61.4 84.8 90.9 79.0
Width-first-then-depth 69.7 90.4 95.0 85.0 Furthermore,thefine-tuningdatasetmaynotbesufficiently
large to fully restore the model’s capacity. Therefore, dur-
Table9.Ablationstudyinpruning86MMoPE-CLIP . ingthefine-tuningstage,the“width-first-then-depth”strat-
V
egystandsoutastheoptimalchoiceforcreatingmorecom-
MSCOCO petitivesmallermodels. Incontrast,duringthepre-training
TrainingLoss
TR@1 TR@5 TR@10 TRMean
stage, adopting the “width-and-depth” pruning strategy is
MoPE-CLIP V 69.7 90.4 95.0 85.0 more convenient and efficient, with performance recovery
w/oL 68.8 89.8 94.7 84.4 facilitatedbyalargecorpusofimage-textpairs.
sim
w/oL 69.4 89.4 94.7 84.5
feat
w/oL hidn 67.7 89.1 94.0 83.6 EffectsofKnowledgeDistillation. Weconductanabla-
w/oDistillation 60.8 84.9 91.5 79.0 tionstudyofourdistillationobjectivesdesignedinSection
3.3. We investigate the learning process on MoPE-CLIP
V
Table10.Ablationstudyofknowledgedistillation.
andtheresultsinTable10showtheeffectivenessofallour
and neurons within a layer. As presented in Table 8, our distillationloss.Weobservethatalldistilledmodelsoutper-
MoPEmetricoutperformsotherstrategieswithaclearmar- formmodelswithoutdistillationbyaclearmargin,demon-
gin. Notably, the Every Other strategy falls behind when stratingtheimportanceofbothcross-modalanduni-modal
pruning three layers. We assume that simply reducing ev- knowledge. Importantly, the TR@1 of the “w/o L hidn”
ery other layer in Transformer encoder may not influence modeldropssignificantlyfrom69.7%to67.7%,whichindi-
the model capacity of uni-model tasks as proven in [12]. catestheintermediatelayerknowledgeintheteachermodel
However, the unavailability of the other encoder results in is crucial for retraining the MoPE-CLIP model. However,
a performance drop in cross-modal tasks. These findings thediscrepancyinpatchnumbersbetweentheViT-B/32and
indicatetheimportanceofselectinganappropriatestrategy ViT-L/14 leads to the failure of hidden distillation applied
for layer reduction in CLIP models and our MoPE metric toCLIP-ViT-B32andSE-CLIP V. Consequently,theeffec-
providesastraightforwardyetvaluableapproach. tivenessofknowledgedistillationislargelydiminishedfor
pre-trained small models and substituted encoder models.
Effects of Pruning Framework. During the fine-tuning In contrast, MoPE-CLIP shares a similar architecture with
stage, we further explore the other two strategies, includ- theteachermodel,allowingittoacquiremoreknowledge.
ing pruning in a “depth-first” manner followed by “width
pruning,”aswellassimultaneous“width-and-depth”prun- Visualization. Tobetterunderstandthearchitectureinflu-
ing. As depicted in Table 9, the “width-first-then-depth” enceontheretrievaltask,weutilizeGrad-CAM[47]tovisu-
strategyyieldsthebestperformance,whilethe“depth-first- alizethecriticalimageregionscorrespondingtothecaption
then-width” and “width-and-depth” strategies fall behind. input. The Grad-CAM is computed on the average self-
This discrepancy may be attributed to the sequential com- attentionmapsinthevisionencoder’slastlayer,wheregra-
putation of hidden states across different layers, making it dients are acquired by the contrastive loss L . The re-
itc
challengingtoaccuratelyevaluatetheimportanceofheads sultsoftheCLIP-ViT-L/14(FT-L14),CLIP-ViT-B/32(FT-
or neurons in layer-reduced models, as discussed in [17]. B32),SE-CLIP ,andMoPE-CLIP areshowninFigure4.
V V
8We observe that the visualizations from FT-L14 are more mann,LudwigSchmidt,andJeniaJitsev.Reproduciblescal-
precise than FT-B32. The FT-L14 model has a smaller ing laws for contrastive language-image learning. In Pro-
patchsizeof14andthuslocatesmoredetailedregions,like ceedingsoftheIEEE/CVFConferenceonComputerVision
the “frisbee” in the middle example. Additionally, MoPE- andPatternRecognition,pages2818–2829,2023. 1,5
CLIP caneffectivelycapturesomeimportantregionslike [8] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,Sammy
V
FT-L14. Both FT-B32 and SE-CLIP miss the “a white Mohamed, andAndreaVedaldi. Describingtexturesinthe
V
wild. In Proceedings of the IEEE conference on computer
bus”inthetopexamplewhilelosing“onecat”inthebottom
visionandpatternrecognition,pages3606–3613,2014. 1
example. MoPE-CLIP captures these important objects
V
[9] WenliangDai,LuHou,LifengShang,XinJiang,QunLiu,
correctly. This indicates that our proposed MoPE-CLIP
V andPascaleFung. Enablingmultimodalgenerationonclip
providesfruitfulinformationfortheretrievaltask.
via vision-language knowledge distillation. arXiv preprint
arXiv:2203.06386,2022. 1
5.Conclusion
[10] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
In this paper, we investigate diverse methods to downsize
Fung, and Steven Hoi. Instructblip: Towards general-
VLP models and focus on exploring better pruning solu-
purpose vision-language models with instruction tuning.
tions. WeproposetheModule-wise pruningerror(MoPE)
arXivpreprintarXiv:2305.06500,2023. 1
metric, offering an accurate measure of the CLIP mod-
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
ule’s importance. On top of the MoPE metric, we in-
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
troduce a unified framework and an advanced distillation database. In2009IEEEconferenceoncomputervisionand
lossforstructuredpruningduringthepre-trainingandfine- patternrecognition,pages248–255.Ieee,2009. 1
tuning stages. Extensive experiments have demonstrated [12] AngelaFan,EdouardGrave,andArmandJoulin. Reducing
thatourMoPE-CLIPachievessurprisingsuccessacrossvar- transformerdepthondemandwithstructureddropout.arXiv
iousdownstreamtasks. preprintarXiv:1909.11556,2019. 4,7,8,1
[13] LiFei-Fei,RobFergus,andPietroPerona. Learninggener-
References ative visualmodels from fewtraining examples: An incre-
mentalbayesianapproachtestedon101objectcategories.In
[1] Reza Abbasi-Asl and Bin Yu. Structural compres- 2004conferenceoncomputervisionandpatternrecognition
sion of convolutional neural networks. arXiv preprint workshop,pages178–178.IEEE,2004. 1
arXiv:1705.07356,2017. 1 [14] Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang
[2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Al- Xu, Aoxue Li, Dit-Yan Yeung, James T Kwok, and
bertoDelBimbo.Conditionedandcomposedimageretrieval Yu Zhang. Mixture of cluster-conditional lora experts
combining and partially fine-tuning clip-based features. In for vision-language instruction tuning. arXiv preprint
ProceedingsoftheIEEE/CVFConferenceonComputerVi- arXiv:2312.12379,2023. 1
sionandPatternRecognition,pages4959–4968,2022. 4 [15] SongHan,JeffPool,JohnTran,andWilliamDally. Learn-
[3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. ing both weights and connections for efficient neural net-
Food-101–mining discriminative components with random work. Advances in neural information processing systems,
forests. In Computer Vision–ECCV 2014: 13th European 28,2015. 1,3,4,5,6,7
Conference, Zurich, Switzerland, September 6-12, 2014, [16] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and
Proceedings,PartVI13,pages446–461.Springer,2014. 1 YiYang. Softfilterpruningforacceleratingdeepconvolu-
[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu tional neural networks. In Proceedings of the 27th Inter-
Soricut.Conceptual12m:Pushingweb-scaleimage-textpre- national Joint Conference on Artificial Intelligence, pages
trainingtorecognizelong-tailvisualconcepts. InProceed- 2234–2240,2018. 1
ingsoftheIEEE/CVFConferenceonComputerVisionand [17] LuHou,ZhiqiHuang,LifengShang,XinJiang,XiaoChen,
PatternRecognition,pages3558–3568,2021. 6 andQunLiu. Dynabert: Dynamicbertwithadaptivewidth
[5] Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, anddepth. AdvancesinNeuralInformationProcessingSys-
Kwang-Ting Cheng, and Eric P Xing. Vision transformer tems,33:9782–9793,2020. 5,6,8,1,3
slimming: Multi-dimension searching in continuous opti- [18] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
mizationspace.InProceedingsoftheIEEE/CVFConference HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom
on Computer Vision and Pattern Recognition, pages 4931– Duerig. Scaling up visual and vision-language representa-
4941,2022. 1 tion learning with noisy text supervision. In International
[6] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, ConferenceonMachineLearning,pages4904–4916.PMLR,
andZhangyangWang. Chasingsparsityinvisiontransform- 2021. 1
ers: Anend-to-endexploration. AdvancesinNeuralInfor- [19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
mationProcessingSystems,34:19974–19988,2021. 1 Chen,LinlinLi,FangWang,andQunLiu. Tinybert:Distill-
[7] MehdiCherti,RomainBeaumont,RossWightman,Mitchell ingbertfornaturallanguageunderstanding. arXivpreprint
Wortsman,GabrielIlharco,CadeGordon,ChristophSchuh- arXiv:1909.10351,2019. 4,5
9[20] AndrejKarpathyandLiFei-Fei.Deepvisual-semanticalign- [33] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
mentsforgeneratingimagedescriptions. InProceedingsof Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
theIEEEconferenceoncomputervisionandpatternrecog- fication of aircraft. arXiv preprint arXiv:1306.5151, 2013.
nition,pages3128–3137,2015. 1 1
[21] Bumsoo Kim, Yeonsik Jo, Jinhyung Kim, and Seunghwan [34] PaulMichel,OmerLevy,andGrahamNeubig. Aresixteen
Kim. Misalign, contrast then distill: Rethinking misalign- headsreallybetterthanone? Advancesinneuralinformation
mentsinlanguage-imagepre-training. InProceedingsofthe processingsystems,32,2019. 1
IEEE/CVF International Conference on Computer Vision, [35] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
pages2563–2572,2023. 2,4,6,7,1,5 andJanKautz.Pruningconvolutionalneuralnetworksforre-
[22] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision- sourceefficientinference.arXivpreprintarXiv:1611.06440,
and-languagetransformerwithoutconvolutionorregionsu- 2016. 1
pervision. InInternationalConferenceonMachineLearn- [36] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
ing,pages5583–5594.PMLR,2021. 1 andJanKautz.Pruningconvolutionalneuralnetworksforre-
[23] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. sourceefficientinference.arXivpreprintarXiv:1611.06440,
3dobjectrepresentationsforfine-grainedcategorization. In 2016. 1
Proceedings of the IEEE international conference on com- [37] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Fro-
putervisionworkshops,pages554–561,2013. 1 sio, and Jan Kautz. Importance estimation for neural net-
workpruning. InProceedingsoftheIEEE/CVFconference
[24] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
on computer vision and pattern recognition, pages 11264–
layersoffeaturesfromtinyimages. 2009. 1
11272,2019. 1
[25] Franc¸oisLagunas, EllaCharlaix, VictorSanh, andAlexan-
[38] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
derMRush. Blockpruningforfastertransformers. arXiv
ingXie. Slip: Self-supervisionmeetslanguage-imagepre-
preprintarXiv:2109.04838,2021. 1
training. InComputerVision–ECCV2022: 17thEuropean
[26] Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo
Conference,TelAviv,Israel,October23–27,2022,Proceed-
Kim,SeungHwanKim,HonglakLee,andJunmoKim.Uni-
ings,PartXXVI,pages529–544.Springer,2022. 6,7,1,5
clip:Unifiedframeworkforcontrastivelanguage-imagepre-
[39] Maria-Elena Nilsback and Andrew Zisserman. Automated
training. arXivpreprintarXiv:2209.13430,2022. 4,6,7,1,
flowerclassificationoveralargenumberofclasses. In2008
5
SixthIndianconferenceoncomputervision,graphics&im-
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
ageprocessing,pages722–729.IEEE,2008. 1
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
[40] OmkarMParkhi, AndreaVedaldi, AndrewZisserman, and
Alignbeforefuse:Visionandlanguagerepresentationlearn-
CVJawahar. Catsanddogs. In2012IEEEconferenceon
ingwithmomentumdistillation. Advancesinneuralinfor-
computervisionandpatternrecognition,pages3498–3505.
mationprocessingsystems,34:9694–9705,2021. 1
IEEE,2012. 1
[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
[41] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Blip-2: Bootstrapping language-image pre-training with
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
frozen image encoders and large language models. arXiv
nik. Flickr30k entities: Collecting region-to-phrase corre-
preprintarXiv:2301.12597,2023. 1
spondences for richer image-to-sentence models. In Pro-
[29] YangguangLi,FengLiang,LichenZhao,YufengCui,Wanli ceedingsoftheIEEEinternationalconferenceoncomputer
Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su- vision,pages2641–2649,2015. 5,1,3
pervision exists everywhere: A data efficient contrastive [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
language-image pre-training paradigm. arXiv preprint Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
arXiv:2110.05208,2021. 6,7,1,5 AmandaAskell,PamelaMishkin,JackClark,etal.Learning
[30] YanghaoLi,HaoqiFan,RonghangHu,ChristophFeichten- transferable visual models from natural language supervi-
hofer,andKaimingHe.Scalinglanguage-imagepre-training sion.InInternationalconferenceonmachinelearning,pages
viamasking. InProceedingsoftheIEEE/CVFConference 8748–8763.PMLR,2021. 1,6,7,5
onComputerVisionandPatternRecognition,pages23390– [43] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav
23400,2023. 1 Nakov. Poor man’s bert: Smaller and faster transformer
[31] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, models. arXiv preprint arXiv:2004.03844, 2(2), 2020. 4,
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence 7,1
Zitnick. Microsoft coco: Common objects in context. In [44] VictorSanh,ThomasWolf,andAlexanderRush.Movement
ComputerVision–ECCV2014: 13thEuropeanConference, pruning:Adaptivesparsitybyfine-tuning.AdvancesinNeu-
Zurich, Switzerland, September 6-12, 2014, Proceedings, ralInformationProcessingSystems,33:20378–20389,2020.
PartV13,pages740–755.Springer,2014. 3,5,1,4 1
[32] ZhiliLiu,KaiChen,JianhuaHan,LanqingHong,HangXu, [45] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
ZhenguoLi,andJamesTKwok. Task-customizedmasked Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
autoencoder via mixture ofcluster-conditional experts. In- Coombes,JeniaJitsev,andAranKomatsuzaki.Laion-400m:
ternationalConferenceonLearningRepresentations,2024. Open dataset of clip-filtered 400 million image-text pairs.
1 arXivpreprintarXiv:2111.02114,2021. 2,1
10[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, pruninglearnscompactandaccuratemodels. arXivpreprint
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo arXiv:2204.00408,2022. 1
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- [59] JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,
man,etal.Laion-5b:Anopenlarge-scaledatasetfortraining and Antonio Torralba. Sun database: Large-scale scene
nextgenerationimage-textmodels. AdvancesinNeuralIn- recognitionfromabbeytozoo. In2010IEEEcomputerso-
formationProcessingSystems,35:25278–25294,2022. 5 cietyconferenceoncomputervisionandpatternrecognition,
[47] RamprasaathRSelvaraju,MichaelCogswell,AbhishekDas, pages3485–3492.IEEE,2010. 1
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. [60] KaichengYang, JiankangDeng, XiangAn, JiaweiLi, Ziy-
Grad-cam: Visual explanations from deep networks via ong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Alip:
gradient-basedlocalization. InProceedingsoftheIEEEin- Adaptive language-image pre-training with synthetic cap-
ternationalconferenceoncomputervision,pages618–626, tion. In Proceedings of the IEEE/CVF International Con-
2017. 8,4 ferenceonComputerVision,pages2922–2931,2023. 2,4,
[48] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu 6,7,1,5
Soricut. Conceptualcaptions: Acleaned,hypernymed,im- [61] LeweiYao,RunhuiHuang,LuHou,GuansongLu,Minzhe
agealt-textdatasetforautomaticimagecaptioning. InPro- Niu,HangXu,XiaodanLiang,ZhenguoLi,XinJiang,and
ceedingsofthe56thAnnualMeetingoftheAssociationfor ChunjingXu. Filip:fine-grainedinteractivelanguage-image
ComputationalLinguistics(Volume1: LongPapers),pages pre-training. arXivpreprintarXiv:2111.07783,2021. 1
2556–2565,2018. 6 [62] HaoYuandJianxinWu. Aunifiedpruningframeworkfor
[49] DachuanShi,ChaofanTao,YingJin,ZhendongYang,Chun visiontransformers.ScienceChinaInformationSciences,66
Yuan,andJiaqiWang. Upop:Unifiedandprogressiveprun- (7):1–2,2023. 4
ing for compressing vision-language transformers. arXiv [63] ShixingYu,TianlongChen,JiayiShen,HuanYuan,Jianchao
preprintarXiv:2301.13741,2023. 1,2,4,5,6,3 Tan,SenYang,JiLiu,andZhangyangWang. Unifiedvisual
transformercompression. arXivpreprintarXiv:2203.08243,
[50] Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang,
2022. 4
ChunYuan,andJiaqiWang.Crossget:Cross-guidedensem-
bleoftokensforacceleratingvision-languagetransformers. [64] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
arXivpreprintarXiv:2305.17455,2023. 1 Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al. Florence: A new
[51] ZhiqingSun,HongkunYu,XiaodanSong,RenjieLiu,Yim-
foundation model for computer vision. arXiv preprint
ing Yang, and Denny Zhou. Mobilebert: a compact task-
arXiv:2111.11432,2021. 1
agnostic bert for resource-limited devices. arXiv preprint
[65] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,
arXiv:2004.02984,2020. 1
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
[52] ChaofanTao,LuHou,HaoliBai,JianshengWei,XinJiang,
Lit: Zero-shot transfer with locked-image text tuning. In
QunLiu,PingLuo,andNgaiWong. Structuredpruningfor
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
efficientgenerativepre-trainedlanguagemodels.InFindings
sionandPatternRecognition,pages18123–18133,2022. 1
oftheAssociationforComputationalLinguistics:ACL2023,
[66] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,
pages10880–10895,2023. 1
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
[53] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie.
Tip-adapter: Training-free clip-adapter for better vision-
Sus-x: Training-freename-onlytransferofvision-language
languagemodeling.arXivpreprintarXiv:2111.03930,2021.
models. arXivpreprintarXiv:2211.16198,2022. 4
1
[54] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich,
[67] RenruiZhang,XiangfeiHu,BohaoLi,SiyuanHuang,Han-
and Ivan Titov. Analyzing multi-head self-attention: Spe-
qiuDeng,YuQiao,PengGao,andHongshengLi. Prompt,
cialized heads do the heavy lifting, the rest can be pruned.
generate,thencache: Cascadeoffoundationmodelsmakes
arXivpreprintarXiv:1905.09418,2019. 1
strongfew-shotlearners. InProceedingsoftheIEEE/CVF
[55] Tiannan Wang, Wangchunshu Zhou, Yan Zeng, and Xin- Conference on Computer Vision and Pattern Recognition,
songZhang.Efficientvlm:Fastandaccuratevision-language pages15211–15222,2023. 1
modelsviaknowledgedistillationandmodal-adaptiveprun-
[68] WeiZhang,LuHou,YichunYin,LifengShang,XiaoChen,
ing. PreprintarXiv:2210.07795,2022. 1,6,7
Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware
[56] Wenxiao Wang, Shuai Zhao, Minghao Chen, Jinming Hu, ultra-low bit bert. arXiv preprint arXiv:2009.12812, 2020.
Deng Cai, and Haifeng Liu. Dbp: Discrimination based 4
block-level pruning for deep model acceleration. arXiv [69] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu
preprintarXiv:1912.10178,2019. 1 Hou,andCarloVittorioCannistraci. Anefficientplug-and-
[57] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, playpost-trainingpruningstrategyinlargelanguagemodels.
MengchenLiu, LuYuan, HongXuan, MichaelValenzuela, 2023. 1
XiStephenChen,XinggangWang,etal. Tinyclip:Clipdis- [70] YingtaoZhang,JialinZhao,WenjingWu,AlessandroMus-
tillation via affinity mimicking and weight inheritance. In coloni,andCarloVittorioCannistraci.Epitopologicalsparse
ProceedingsoftheIEEE/CVFInternationalConferenceon ultra-deep learning: A brain-network topological theory
ComputerVision,pages21970–21980,2023. 1,2,4,6,7,5 carves communities in sparse and percolated hyperbolic
[58] MengzhouXia,ZexuanZhong,andDanqiChen. Structured anns. 2023. 1
11[71] Liu Zhili, Jianhua Han, Lanqing Hong, Hang Xu, Kai
Chen,ChunjingXu,andZhenguoLi. Task-customizedself-
supervised pre-training with scalable dynamic routing. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence,pages1854–1862,2022. 1
[72] MichaelZhuandSuyogGupta.Toprune,ornottoprune:ex-
ploringtheefficacyofpruningformodelcompression.arXiv
preprintarXiv:1710.01878,2017. 1
[73] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou,
DongWang,BinZhao,andPengGao. Notallfeaturesmat-
ter: Enhancingfew-shotclipwithadaptivepriorrefinement.
arXivpreprintarXiv:2304.01195,2023. 4
12MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with
Module-wise Pruning Error Metric
Supplementary Material
A.Relatedwork modal models remains under-explored. Our experiments
show that directly using widely-used metrics [15, 34] or
Vision-Language Pre-trained Models. Benefiting from
“every other” strategy [12, 43] for VLP pruning leads to
the efficiency of contrastive learning, vision-language pre-
unsatisfactory performance, indicating the demand for ex-
trainedmodelslike[10,14,18,27,42,61,64,65,71]have
ploring more accurate metrics to measure module impor-
achieved advanced capability across downstream tasks.
tance of VLP models across multi-modal tasks. Recently,
Suchdual-streammodelshaveefficientinferencespeedon
EfficientVLM [55] proposes to distill the VLP model in
multi-modal tasks like retrieval, as the image/text features
the pre-training stage and then prune attention heads dur-
can be computed offline [9, 61]. However, these models
ing the task-specific fine-tuning stage, but the distillation
are often pre-trained with millions or billions of image-
stageprovednotoptimalinourexperiments. Anotherwork
textpairsfromscratch,whichiscomputationallyexpensive
Upop[49]usesaunifiedandprogressivesearch-basedprun-
[7, 49, 50]. Later works [26, 29, 38] propose to use more
ingmethodonvision-languagemodels,butthesearchpro-
complex objectives to reduce the amount of pre-training
cess is expensive and is hard to apply to the pre-training
data. Others[21,60]intendtoreducetheinfluenceofnoisy
stage. TinyCLIP [57] proposes a multi-stage pruning and
and unmatched image-text pairs. However, these methods
distillation method for pre-training small OpenCLIP mod-
leadtolesscompetitiveretrievalperformance. Inthiswork,
els [7]. However, the design ofthe multi-stage is complex
weshowthatwecanprunetheoriginalpre-trainedCLIPto
and the final performance relies on the huge pre-training
a desired size and significantly lift up the performance of
datasetLAION400M[45]. Inthiswork,weproposeasim-
the pruned model in a data-efficient way, i.e., with several
plebuteffectivemetriccalledMoPE,whichservesasagen-
magnitudesfewerpertainingdatathantheoriginalCLIP.
eral importance measure of various compressible compo-
nents like attention heads, FFN neurons, and Transformer
Pruning of Transformer-based Models. Various meth-
layers. BasedonMoPEmetric,wedesignaunifiedpruning
odshavebeenproposedtocompressuni-modalvisionand
frameworkappliedtoboththepre-trainingandfine-tuning
languagetransformermodels[6,25,32,34,54,56,69,70].
stages,resultinginstate-of-the-artMoPE-CLIPmodels.
Among them, structured pruning methods remove unim-
portant structured components (e.g., attention heads, FFN B.ImplementationDetails
neurons, andTransformerlayers)inthenetwork. Depend-
ing on how the pruned components are determined, prun- B.1.DetailedExperimentalSettings
ing methods could be divided into two categories: search-
Herewedescribethedetailedsetupsforourexperimentsof
based and metric-based methods. Search-based methods
two compression stages. For all experiments, we use the
[5, 52] usually apply masks on the structured components
samerandomseed(e.g.,42)toensurereproduction.
and need a searching process to determine their impor-
tance. Ontheotherhand,metric-basedmethodsapplyvar-
DetailsforEvaluationBenchmarks. Forretrievaltasks,
iousmetricstodeterminemoduleimportanceandresultin
wesplittheMSCOCO[31]andFlickr30K[41]datasetsfol-
asingle-shotpruningprocess. Widelyusedmetricsinclude
lowing [20]. For classification tasks, we adopt 11 down-
themagnitudeofweight[15,16,72]andthevariantinloss
streamdatasetsfollowing[66,67],includingCIFAR10,CI-
[34, 36, 37]. Some researchers [12, 43] explore different
FAR100 [24], Caltech101 [13], Flowers102 [39], Oxford
strategies for pruning BERT layers, such as “every other”,
Pets [40], DTD [8], Stanford Cars [23], FGVC Aircraft
“bottom or top dropping” and “search on valid” like CNN
[33],SUN397[59],Food101[3]andImageNet[11].
Oracle Filter Pruning [1, 35]. Notably, the “every other”
strategyhasbeenproveneffective[12,43],withDynaBERT
[17]implementingittocreatedynamicdepthnetworks.Ad- Details for Fine-tuning Stage Compression Table B1
ditionally,pruningisoftenusedincombinationwithknowl- summarizesthehyperparametersforfine-tuningCLIP-ViT-
edgedistillation,whichtransfersknowledgefromtheorig- L/14 and distilling CLIP-VIT-B/32. During the distilling
inal unpruned teacher model to the smaller pruned model process,wefirstfixthemodelandtrainthelinearlayerfor
withdifferentkindsofknowledge[44,51,58]. 5epochswithalearningrateof1e-5tolearnabettermap-
Incontrasttotheextensiveresearchoncompressinguni- ping function. Table B2 lists the detailed retraining setups
modal Transformer-based models, compression of multi- for MagnCLIP, DynaCLIP, MoPE-CLIP, and SE-CLIP in
1B.2.MainAlgorithm
Config Fine-tuning Distilling
WeillustratethecomputationprocessoftheMoPEmetricin
Optimizer AdamW,β = (0.9,0.98)
Algorithm 1, and our unified pruning framework resulting
LRschedule CosineLRScheduler
inMoPE-CLIPinAlgorithm2.
Weightdecay 3e-4
Warmupratio 0.1
Algorithm1Module-wisePruningErrorMetric
InitLR 3e-6 1e-6
Input:CLIPmodelf ,Moduleθ,DatasetD
φ
Batchsize 256 1024 Output:Importanceofθ
Trainingepochs 12 15 1: procedureMOPE(f φ,θ,D):
Distillation N/A L
sim
+L
feat
2: ComputethefullCLIPPerformanceonD:Z[f φ]
3: ComputetheCLIP θ=0PerformanceonD:Z[f φ−θ]
TableB1. Experimentalsetupforfine-tuningCLIP-VIT-L/14or 4: ComputetheMoPE θ =Z[f φ]−Z[f φ−θ]
distillingCLIP-ViT-B/32. 5: returnMPWE θ
6: endprocedure
DownstreamTask Image-to-text Text-to-image
Optimizer AdamW,β =(0.9,0.98) Algorithm2MoPE-CLIP:PruningwithMoPEMetric
LRschedule CosineLRScheduler Input:CLIPmodelf ,ValidationSetD ,TrainingSetD
φ val train
Weightdecay 3e-4 Output:MoPE-CLIPmodel
Warmupratio 0.1 1: PartitiontheAttentionHeadsinN ×Lmodules
InitLR 2e-5 8e-5 2: forlin1, ...,Ldo
Batchsize 256 1024 3: forheadhin1, ...,N do
4: ▷ runinparallel
Trainingepochs 20 10
5: MoPE h←MoPE(f φ,h,D val)
Table B2. Experimental setup for retraining MagnCLIP, Dyna- 6: UpdateC head
7: endfor
CLIP,MoPE-CLIPandSE-CLIPacrossTRandIRtasks.
8: endfor
9: CLIPf′ ←RewireNeuronsinFFNbygradient
φ
Config Pre-training FurtherFine-tuning 10: PartitiontheFFNNeuronsinNgroups
Optimizer AdamW,β =(0.9,0.98) 11: forgroupnin1, ...,N do
12: ▷ runinparallel
LRschedule CosineLRScheduler
Weightdecay 3e-4
13: MoPE n←MoPE(f φ′,n,D val)
14: UpdateC neuron
Warmupratio 0.02 0.1
15: endfor
InitLR 5e-5 4e-5
16: ifCompressioninfine-tuningstagethen
Batchsize 512 512
17: MoPE-CLIPwf Cw←PrunetheCLIPinwidth
Trainingepochs 20 15
andretrainonD
train
18: forlayerlin1, ...,Ldo
Table B3. Experimental setup for pre-training DynaCLIP and 19: ▷ runinparallel
MoPE-CLIPandfurtherfine-tuningondownstreamtasks. 20: MoPE l←MoPE(f Cw,l,D val)
21: UpdateC layer
image-to-textretrieval(TR)andtext-to-imageretrieval(IR) 22: endfor
tasksThetextencodersofthesemodelsarefixedfortheTR 23: MoPE-CLIP←PrunetheMPEE-CLIPwindepth
task, while image encoders are frozen for the IR task. For andretrainonD train
SE-CLIP, we add a linear layer to align feature space, and 24: elseifCompressioninpretrainingstagethen
thehiddendistillationlossisexcludedduetotheunmatched 25: forlayerlin1, ...,Ldo
numberofimagepatches. 26: ▷ runinparallel
27: MoPE l←MoPE(f φ,l,D val)
28: UpdateC layer
DetailsforPre-trainingStageCompression Welistthe 29: endfor
detailed setup for pretraining stage compression in Ta- 30: MoPE-CLIP←PrunetheCLIPinwidthanddepth
bleB3. MoPE-CLIPadoptstheRecallMeanonMSCOCO andretrainonD train
validationdatasetasthespecificMoPEmetric. DynaCLIP 31: endif
andMoPE-CLIPsharethesamehyperparameters. 32: returntheMoPE-CLIP
2C.MoreExperimentalResults VisionEncoder Flickr30K(1Ktestset)
Approach
Wdith Depth Parmas TR@1 TR@5 TR@10
C.1.DetailedComparisonwithBaselines
TeacherModel 1024 24 304M 96.3 99.8 100.0
CLIP-ViT-B/32 768 12 88M 87.7 97.7 99.3
WeprovideamoredetailedcomparisonwithDynaCLIP ,
V
MagnCLIP ,andUPopinthefollowing. 512 24 153M 92.7 99.4 99.8
V DynaCLIPV [17] 384 24 115M 89.6 98.5 99.4
384 18 87M 84.5 97.3 98.5
PruningRatios. TofurtherevaluateourMoPE-CLIPper- N/A N/A 474M‡ 93.2 99.4 99.8
UPop-CLIP[49]
formance under different model sizes. We test six prun- N/A N/A 280M‡ 82.9 95.7 97.8
ing ratios with the model performance plotted in Fig. C1. 512 24 153M 92.7 99.5 99.9
MoPE-CLIP consistently stands above all other baselines MoPE-CLIPV 384 24 115M 91.1 98.9 99.7
384 18 87M 88.5 98.5 99.6
(i.e., DynaCLIP and MagnCLIP) across different pruning
ratios. Thegapbecomesevenlargerforhighersparsities. TableC5.Image-to-textretrievalresultsontheFlickr30Kdataset.
TheParamslabeledas‡denotetheparametersoftheentiremodel.
TextEncoder Flickr30K(1Ktestset)
Approach
Width Depth Params IR@1 IR@5 IR@10
TeacherModel 768 12 85M 84.7 97.4 99.0
CLIP-ViT-B/32 512 12 38M 74.7 93.4 96.9
384 12 42M 84.1 97.1 98.7
DynaCLIPT[17]
192 12 21M 80.3 95.7 98.0
384 12 42M 85.1 97.4 99.1
MoPE-CLIPT
192 12 21M 83.5 97.2 98.8
TableC6.Text-to-imageretrievalresultsontheFlickr30Kdataset.
Pruningisappliedinthewidthdirection.
FigureC1.Comparsionofdifferentpruningratios.
fine-tuned CLIP-ViT-L14 (FT-L14) for image-to-text re-
trieval. We mainly compare the fine-tuned performance
MSCOCO(5Ktestset) of our MoPE-CLIP with fine-tuned CLIP-ViT-B/32 (FT-
Model Params V
TR@1 IR@1
B32),DynaCLIP ,andUPop-CLIP[49]ontheFlickr30K
V
UPop-Teacher 856M 71.5 56.8 dataset. In particular, we compute the loss gradient and
UPop-CLIP 280M↓67% 56.1↓21% 41.1↓27%
MoPE metric (TR Mean) in Flickr30K [41] validation
Upop-CLIP(+KD) 280M↓67% 58.6↓18% 44.3↓22%
datasetforDynaCLIP andMoPE-CLIP . Theresultsare
V V
MoPE-Teacher 390M 76.2 58.8
presented in Table C5. We could observe that once depth
MoPE-CLIP 122M↓69% 70.7↓7% 54.7↓7%
pruning is added to DynaCLIP , the TR@1 drops from
V
89.6%to84.5%,whiletheMoPE-CLIP with87Mvision
TableC4.UPopandMoPE-CLIPonMSCOCO. V
encoder maintains competitive retrieval and surpasses the
FT-B32. In addition, our MoPE-CLIP with 115M vision
V
Relative Comparison with UPop. We further compare encoder termed an entire model of 234M parameters out-
UpopwithKnowledgeDistillationinTab.C4. MoPE-CLIP performs the UPop-CLIP with 280M parameters by 8.2%
issuperiortoUpop(+KD)bothontherelativeperformance TR@1. TheseresultsindicatethesuperiorityoftheMoPE
drop and absolute task score, given a comparable relative metricacrossdifferentdownstreamdatasets.
decrease(69%vs67%)inthenumberofparameters.More-
over, MoPE-CLIP’s advantage is notable, as compressing
Results for Text-to-image Retrieval. We compress the
smalleroriginalmodelsizesismorechallenging.
textencoderoffine-tunedCLIP-ViT-L/14fortext-to-image
C.2.Fine-tuningStageCompressiononFlickr30K retrieval.Thepruningandretrainingremainthesameasthe
setting on the MSCOCO dataset and the results are illus-
To demonstrate the robustness of the MoPE metric across
trated in Table C6. The MoPE-CLIP exhibits significant
T
differentdatadistributions,wefurtherevaluateMoPE-CLIP
performance on the Flickr30K dataset. Even at a 4x com-
onFlickr30KDatasetduringfine-tuningstagecompression.
pression ratio, the MoPE-CLIP surpasses the FT-B32 by
T
8.8%IR@1andDynaCLIP by3.2%IR@1. Thesesupe-
T
Results for Image-to-text Retrieval. Following the set- rior results demonstrate that our MoPE-CLIP provides a
T
ting in Section 4.1, we compress the vision encoder of powerfultextencoderforthetext-to-imageretrievaltask.
3Cosine similarity between matched and unmatched image-text features
“An old man sitting on a bench overlooking a lake.”
ytisn
eD
“A close up of an apple and a banana.”
ytisn
eD
Input Image FT-L14 MoPE-CLIPv DynaCLIPv
Figure C3. Grad-CAM visualization on the self-attention maps
correspondingtothecaptioninput.
“two women are sitting on two horses”
Figure C2. Histograms of cosine similarities between matched
andunmatchedimage-textfeatures. Thegreenboxrepresentsthe
similaritygap.MoPE-CLIP preservesasimilarspacetoFT-L14.
V
C.3.FurtherDiscussion.
Similarity matrix indicates pruning is the best archi-
tecture. We compare and analyze the similarity matrix
of three architectures discussed in Section 2 since it di-
rectly influences retrieval performance. In particular, we
sample5kimage-textpairsfromtheMSCOCO[31]valida-
tiondatasetandcalculatethesimilaritiesbetweenmatched
image-text features and unmatched pairs, as done in pre-
vious works [53, 73]. Following [2], we suppose that the
retrieval performance is more influenced by the similarity
gap between matched and unmatched features. We com-
paretheMoPE-CLIP withfine-tunedCLIP-ViT-L/14(FT-
V
L14),fine-tunedCLIP-ViT-B/32(FT-B32)andSE-CLIP .
V
From Figure C2, we observe that FT-L14 has a larger gap
between two similarities compared with FT-B32, reflect-
ing its powerful performance. The pruned MoPE-CLIP
V
shows a similar distribution and gap to FT-L14, while the
SE-CLIP evenclosesthegap,indicatingtheperformance
V
differenceamongthesemodels. Therefore,MoPE-CLIP V, FigureC4.Grad-CAMvisualizationofthelastlayerself-attention
whichpreservesasimilarityspacelikeFT-L14,emergesas mapsfororiginalFT-L14’svisionencoder. Redboxdenotespre-
thebestcompactmodelarchitecture. served heads based on MoPE-CLIP . Yellow box denotes pre-
V
servedheadsbasedonDynaCLIP .Orangeboxdenotesthehead
V
Grad-CAM demonstrates MoPE-CLIP preserves more
ispreservedbytwomodelssimultaneously.
important heads. To better understand the effect of our
MoPE metric, we use Grad-CAM [47] to visualize the re- likethe”bench”inthetoplineandthe”apple”inthebottom
gions focused by DynaCLIP and MoPE-CLIP . In de- line. Furthermore, We visualize the Gram-CAM of each
V V
tail, we select the model with a 115M vision encoder and headoftheFT-L14modelandidentifythepreservedheads
computetheGrad-CAMusingself-attentionmapsaveraged byDynaCLIP orMoPE-CLIP . AsshowninFigureC4,
V V
over all attention heads in the last layer of the vision en- MoPE-CLIP preserves heads 3, 4, and 15, which corre-
V
coder. ThegradientsareacquiredbycontrastivelossL . spond to the crucial region of ”sitting on the horse.” Con-
cont
From Figure C3, we could observe that the average atten- versely,DynaCLIP prunestheseheads,leadingtotheirex-
V
tionmapofMoPE-CLIP issimilartooriginalmodel(FT- clusion. ThisobservationprovestheprecisionoftheMoPE
V
L14), but the DynaCLIP misses some important regions, metricinidentifyingandpreservingvitalinformation.
V
4VisionEnocder TextEncoder Params(M) MSCOCO(5Ktestset) Flickr30K(1Ktestset)
Method
Width Depth Width Depth Vision+Text TR@1 TR@5 TR@10 IR@1 IR@5 IR@10 TR@1 TR@5 TR@10 IR@1 IR@5 IR@10
Pre-trainedonWIT-400M
CLIP-ViT-L/14[42] 1024 24 768 12 304+85 76.2 92.9 96.4 58.8 82.8 89.5 96.3 99.8 100.0 84.7 97.4 99.0
CLIP-ViT-B/32[42] 768 12 512 12 88+38 66.2 87.7 92.8 49.4 75.8 84.7 87.7 97.7 99.3 74.7 93.4 96.9
Pre-trainedonCC3M
DynaCLIPbase[17] 384 18 384 12 86+42 70.7 90.0 94.6 53.8 80.5 87.9 90.0 98.8 99.7 79.0 95.5 97.9
DynaCLIPsmall[17] 384 18 192 12 86+21 69.3 89.5 94.5 52.3 79.1 87.1 89.4 98.1 99.7 77.3 95.0 97.4
MoPE-CLIPbase 384 18 384 12 86+42 71.9 91.4 95.7 54.9 81.1 88.6 92.1 98.8 99.0 80.6 95.6 98.1
MoPE-CLIPsmall 384 18 192 12 86+21 71.2 90.9 95.0 53.7 80.5 87.9 90.8 98.6 99.6 79.3 95.5 97.9
Pre-trainedonYFCC15M
CLIP-ViT-B/32†[42] 768 12 512 12 88+38 34.5 63.5 75.2 24.0 50.8 63.5 57.4 84.7 90.2 40.4 69.5 79.6
SLIP-ViT-B/32†[38] 768 12 512 12 88+38 43.7 71.8 82.4 31.0 58.8 70.3 68.9 91.9 95.1 51.0 79.5 86.8
DeCLIP-ViT-B/32†[29] 768 12 512 12 88+38 47.9 75.5 84.6 33.8 62.7 71.4 73.6 93.9 97.2 55.9 83.4 90.2
UniCLIP-ViT-B/32†[26] 768 12 512 12 88+38 52.7 78.6 87.4 37.6 66.3 77.0 77.9 95.1 98.0 61.0 85.9 92.2
MCD-ViT-B/32†[21] 768 12 512 12 88+38 55.6 81.2 89.5 38.2 67.4 78.5 79.3 95.2 98.0 63.1 87.2 92.3
MoPE-CLIPbase 384 18 384 12 86+42 74.3 92.3 95.9 56.7 82.0 89.4 93.3 99.4 99.9 82.0 96.4 98.7
TableC7.Fine-tunedimage-textretrievalresultsonMSCOCOandFlickr30Kdatasets.DynaCLIPandMoPE-CLIPareprunedduringthe
pre-trainingstageandfurtherfine-tunedondownstreamdatasets.†denotestheresultsarereportedfrom[26,60].
VisionEnocder TextEncoder Params(M) TrainingDetails MSCOCO Flickr30K
Method
Width Depth Width Depth Vision+Text Dataset GPU Batchsize TR@1 IR@1 TR@1 IR@1
OpenCLIP[7] 12 12 8 12 88+39 LAION-2B 176xA100 33792 59.4 42.4 86.2 69.8 ‘
TinyCLIP[57] N/A N/A 8 6 39+19 YFCC15M 32xA100 4096 54.9 38.9 84.4 66.7
MoPE-CLIP 6 12 4 12 43+19 YFCC15M 8xV100 1024 56.2 39.4 84.5 67.4
TableC8. Zero-shotimage-textretrievalresultsofTinyCLIPandMoPE-CLIP.TheoriginalmodelisOpenCLIP-ViT-B/16pre-trainedon
theLAION-2Bdataset.
MSCOCO Flickr30K Trainingcost ing during the fine-tuning stage is an interesting ques-
PruningStrategy
TR@1 IR@1 TR@1 IR@1 Epochs GPUHours
tion. Therefore, we further fine-tune the DynaCLIP and
Width-and-depth 52.8 37.3 82.8 66.7 20 320
Width-first-then-depth 54.3 38.1 84.1 67.9 40 640 MoPE-CLIP on downstream datasets and compare them
with other baselines. From Table C7, we observe that
TableC9. Comprasionofretrievalperformanceandtrainingcost the finetuned MoPE-CLIP and DynaCLIP exhibit signifi-
inpruning86M+42MMoPE-CLIP base. cantperformanceontwodatasetsandenlargethegapcom-
pared to fine-tuned CLIP-ViT-B/32. This indicates that
Width-and-depth pruning is preferred for pre-training
pruned models continually inherit the knowledge from the
compression. Following Section 4.3, we extend our in-
fine-tuned CLIP-ViT-L/14 during full fine-tuning. Conse-
vestigationtoincludeboth“width-and-depthpruning”and
quently, we compare the fine-tuned MoPE-CLIP with
“width-first-then-depth pruning” strategies during the pre- base
MoPE-CLIP in Table 1 and find that the former show-
training stage compression. We exclude the “depth-first- V
cases better TR@1. This indicates that pruning during the
then-width” strategy since it falls behind the ”width-first-
pre-training stage is more effective because more image-
then-depth pruning” during the fine-tuning stage. As indi-
textpairs areincluded forlearning, while thepruning dur-
cated in Table C9, “width-first-then-depth pruning” shows
ingfine-tuningstageexhibitscompetitiveresultswithmuch
superiorperformance. However,theperformancegapwith
lesstrainingtime.Inaddition,ifweenlargethepre-training
“width-and-depthpruning”narrowssignificantlycompared
dataset to YFCC15M, fine-tuned UniCLIP [26] and MCD
to the fine-tuning stage. Notably, “width-first-then-depth
[21]stillfallshortincomparisontoMoPE-CLIP . This
pruning” requires an additional 20 epochs in pre-training, base
alignswiththeconclusioninSection4.2thatpruningoffers
which can be resource-intensive for many researchers. On
asuperiorsolutionforobtainingcompactVLPmodels.
the other hand, “width-and-depth pruning” offers the dual
benefitsofone-stagepruningforfastertrainingandtheuti-
C.5.MoPEonOpenCLIP
lization of a larger set of image-text pairs, thereby yield-
ing competitive performance. Consequently, we advocate ToassessourMoPEmetricacrossvariousvision-language
for“width-and-depthpruning”duringthepre-trainingstage models,weadoptedthesettingusedinTinyCLIP[57]and
compression,asitstrikesanoptimalbalancebetweentrain- further compressed the OpenCLIP-ViT-B/16 [7], which is
ingefficiencyandmodelcapability. pre-trainedontheLAION-2Bdataset[46]. Specifically,we
prune both the vision and language encoders to half their
C.4.Fine-tunedEvaluationforPre-trainingStage
original widths. The MoPE metric is computed by Re-
As we discussed in Section 2, whether pruning during the call Mean on the MSCOCO validation dataset, following
pre-training stage and then fine-tuning outperforms prun- Section 4.2. We then pre-train the reduced model on the
5YFCC15Mdatasetfor25epochs,employing16xNVIDIA
V100GPUs,andtheresultsarepresentedinTableC8. We
observe that our MoPE-CLIP, utilizing significantly fewer
GPU resources, surpasses TinyCLIP in retrieval tasks on
both MSCOCO and Flickr30K benchmarks, and narrows
theperformancegapwithOpenCLIP.However,duetolim-
ited computational resources, we were unable to increase
the batch size to 4096 as done in TinyCLIP. Therefore,
weanticipatefurtherenhancementswiththeavailabilityof
more GPUs. These experiments validate the effectiveness
of the MoPE metric across different VLP models and also
demonstrate that our MoPE-CLIP offers a straightforward
yetefficientapproachforpre-trainingstagecompression.
6