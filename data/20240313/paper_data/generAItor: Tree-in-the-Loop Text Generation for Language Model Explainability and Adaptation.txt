-generAItor: Tree-in-the-Loop Text Generation
for Language Model Explainability and Adaptation
THILOSPINNER,
ETHZurich,Switzerland
REBECCAKEHLBECK,
UniversityofKonstanz,Germany
RITASEVASTJANOVA,
ETHZurich,Switzerland
TOBIASSTÄHLE,
UniversityofKonstanz,Germany
DANIELA.KEIM, UniversityofKonstanz,Germany
OLIVERDEUSSEN,
UniversityofKonstanz,Germany
MENNATALLAHEL-ASSADY,
ETHZurich,Switzerland
Largelanguagemodels(LLMs)arewidelydeployedinvariousdownstreamtasks,e.g.,auto-completion,aided
writing,orchat-basedtextgeneration.However,theconsideredoutputcandidatesoftheunderlyingsearch
algorithmareunder-exploredandunder-explained.Wetacklethisshortcomingbyproposingatree-in-the-loop
approach,whereavisualrepresentationofthebeamsearchtreeisthecentralcomponentforanalyzing,
explaining,andadaptingthegeneratedoutputs.Tosupportthesetasks,wepresentgenerAItor,avisual
analyticstechnique,augmentingthecentralbeamsearchtreewithvarioustask-specificwidgets,providing
targetedvisualizationsandinteractionpossibilities.Ourapproachallowsinteractionsonmultiplelevelsand
offersaniterativepipelinethatencompassesgenerating,exploring,andcomparingoutputcandidates,aswell
asfine-tuningthemodelbasedonadapteddata.Ourcasestudyshowsthatourtoolgeneratesnewinsights
ingenderbiasanalysisbeyondstate-of-the-arttemplate-basedmethods.Additionally,wedemonstratethe
applicabilityofourapproachinaqualitativeuserstudy.Finally,wequantitativelyevaluatetheadaptabilityof
themodeltofewsamples,asoccurringintext-generationusecases.
CCSConcepts:•Computingmethodologies→Naturallanguagegeneration;•Human-centeredcom-
puting→Graphicaluserinterfaces;Visualizationsystemsandtools;•Mathematicsofcomputing
→Exploratorydataanalysis.
AdditionalKeyWordsandPhrases:largelanguagemodels,beamsearchtree,naturallanguagegeneration,
explainability,languagetransformers,visualanalytics
ACMReferenceFormat:
ThiloSpinner,RebeccaKehlbeck,RitaSevastjanova,TobiasStähle,DanielA.Keim,OliverDeussen,andMenna-
tallahEl-Assady.2024. -generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainability
andAdaptation.J.ACM37,4,Article111(August2024),32pages.https://doi.org/10.1145/3652028
Authors’addresses:ThiloSpinner,ETHZurich,Zurich,Switzerland,thilo.spinner@inf.ethz.ch;RebeccaKehlbeck,University
ofKonstanz,Konstanz,Germany,rebecca.kehlbeck@uni-konstanz.de;RitaSevastjanova,ETHZurich,Zurich,Switzerland,
rita.sevastjanova@inf.ethz.ch;TobiasStähle,UniversityofKonstanz,Konstanz,Germany,tobias.staehle@uni-konstanz.de;
DanielA.Keim,UniversityofKonstanz,Konstanz,Germany,keim@uni-konstanz.de;OliverDeussen,Universityof 111
Konstanz,Konstanz,Germany,oliver.deussen@uni-konstanz.de;MennatallahEl-Assady,ETHZurich,Zurich,Switzerland,
menna.elassady@ai.ethz.ch.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Notforredistribution.ThedefinitiveVersion
ofRecordwaspublishedinJournaloftheACM,https://doi.org/10.1145/3652028.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.
4202
raM
21
]CH.sc[
1v72670.3042:viXra111:2 Spinneretal.
1 Introduction
Recently,largelanguagemodels(LLMs)havegainedincreasedpopularity,especiallyinthefield
ofnaturallanguagegeneration(NLG).Atthelatest,withtheintroductionofChatGPT1,LLMs
havebeenmadeaccessibletoawider,moregeneralaudience.However,despitetheirgrowing
recognitionandnotableaccomplishments,theystillfaceseverallimitations.Commonfailures,
evenforstate-of-the-artmodels,arerepetitivecontent,thelackoffactualaccuracy,oftenreferred
toashallucination[Jietal.2023],andbiases[Alba2022].However,theperceivedhighquality
ofLLMoutputsmakesidentifyingerrorsintheirpredictionsdifficult,whichisaggravatedbya
lackofexplainabilityandaccessibility[Zhaoetal.2024].Gainingunderstandingandaccesstothe
model’sdecision-makingprocessisfundamentalforrecognizingerrorsintheiroutputs,calming
concernsaboutoverestimatingthemodel’scapabilities,andempoweringuserstoguidethemodel’s
predictionstoalignwiththeirintentions.Particularly,thechatinterfaceofChatGPTandotherchat-
orcompletion-basedapproachesomitimportantinformationonuncertaintiesorviablealternatives
fromtheusers.Whiletext-basedinterfacesmayfulfilltheneedsforabroad,generalaudience,
interestednon-expertsandlinguisticexpertsrequiremorein-depthinsightsandcontrol.
Weidentifythreeprimaryshortcomingsinthecurrentstate-of-the-artforinteractingwithLLMs:
lackof explainability,comparability,andadaptability.Explainabilityreferstounderstanding
ofthemodel’sdecisionprocess,includingthewayalanguagemodelpredictsitsoutput,itssampling
strategy,andtheprobabilitiesoftheseoutputs.Forexample,explanationsofaprediction’scertainty
canprovidetheuserahintonpossiblehallucinations.Comparability,i.e.,asimpleyeteffective
comparisonofmultiplegeneratedoutputs,canenabletheusertoassessmorespecificnuancesin
themodel’spredictions.Thiskindofcontrastiveexplanation[El-Assady,Jentner,etal.2019]is
particularlyrelevantforlinguisticexperts.Forinstance,byadaptingpromptswithtypicalnames
fromvaryingethnicgroupsandcomparingthepredictions,theusercanassessthemodel’sbiases,
ifpresent.Andlastly,adaptabilityisrelevantwhenthegeneratedoutputisnotsatisfactory.The
insightsgainedfromexplainabilityandcomparabilityempowertheusertosteerthemodeltowards
theirintentions.Concretely,theusershouldbeabletoeditproblematicparts;e.g.,bycorrecting
made-upfactsandmakingthesechangespermanent;e.g.,byfine-tuningthemodel.
Since almost all modern LLMs have committed themselves to the transformer architecture,
besidestheirnumberoftrainableparameters,thequalityofthetrainingdataisthedecisivefactor
foramodel’sperformance[Lauscheretal.2021;Mishraetal.2022].Therefore,studyingthemodel’s
behavioriscloselylinkedtostudyingitsin–andoutputs,representingalocalapproximationof
theinformationthemodelhaslearnedduringtraining.Ourproposedapproach,thus,focuseson
making these in– and outputs accessible and explorable to the user. A straightforward way to
achievethisistomakethesearchalgorithmtransparent.Themostprominentalgorithmtosample
sequences from the probability distributions output by the model is beam search. By sampling
thedecision-space[El-Assady,Sevastjanova,etal.2018]throughexpandingthemostpromising
sequence in a limited set of candidate sequences, the algorithm results in a tree, scanning the
searchspaceforsequenceswithhighoverallprobability.Beamsearchisthuscommonlyusedin
languagemodelexplanationmethods,suchasthevisualinterfacebyLeeetal.[Leeetal.2017],
Seq2Seq-Vis[Strobelt,Gehrmann,etal.2018],orGenNI[Strobelt,Kinley,etal.2022].
In this paper, we propose a tree-in-the-loop interaction paradigm, which leverages a visual
representationofthebeamsearchtree(BST)asthecentralcomponentofthegenerAItorvisual
analyticstechnique.Werevealandexplainthemodel’sdecision-makingprocessbylayingoutthe
BSTandaugmentingitwithadditionalexplanations,suchastokenprobabilities,semantickeyword
coloring, and sentiment annotations. Comparative explanations are facilitated by juxtaposing
1https://openai.com/blog/chatgpt
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:3
multipleBSTs,allowingtheusertocomparethemodel’spredictionsunderslightlyvariedinputs.
Furthermore,weenabletheusertointeractwiththetree,allowingthemtoadaptandsteerthe
model’s predictions, for example, by overriding model decisions, editing predicted sequences,
orfine-tuningthemodel.Tofacilitateaneffectiveanalysisthroughvisualinteractivemethods,
we identify five main tasks in the context of informed text generation: model prompting and
configuration,treeexplorationandexplainability,guidedtextgeneration,comparativeanalysis,
andBSTandmodeladaptation.Eachofthesetasksplacesdistinctdemandsonthetoolsavailable.
Tobeabletofulfillthesedemandsinacombinedapproach,wedesignamodular,widget-based
workflow,wheretask-specificwidgetsenhancetheBSTwithtailoredcontrols,interactionpossi-
bilities,andvisualizations.Eachwidgetaddsaveryspecificfunctionality.However,insymbiosis,a
selectedsetoftask-supportingwidgets,ininteractionwiththesearchtree,enablesnovel,powerful
modes of analysis. E.g., comparative analysis is facilitated by two particular widgets, allowing
linguisticexpertstoobservechangesinthetreeunderslightvariationsofthestartingprompt.
Thisrevealsbiasesintheobservedmodel,whoseidentificationandmitigationisoneofthemost
burningissueswithstate-of-the-artlanguagemodels[Alba2022].
Inthispaper,wecontribute:(1)Adetailedproblemanalysisofthechallengesofexplainability,
controllability,andadaptabilityinthecontextofvarioustextgenerationtasks.(2)Anovelvisual
analyticstechniquecalledgenerAItor,tacklingthesechallengesinaninteractivetree-in-the-loop-
approach. (3) An implementation of the generAItor technique in a web-based visual analytics
workspace.(4)Athree-foldevaluationofthegenerAItortechnique,including(4.1)casestudies,
showcasingthegenerativeandcomparativecapabilitiesofourtechnique,(4.2)aqualitativeuser-
study,provingtheusabilityoftheimplementation,and(4.3)aquantitativeevaluation,confirming
theabilitytoadaptthemodeltouser-preferenceswithfewtrainingsamples.
2 RelatedWork
Inthefollowing,wepresentourrelatedworkonlanguagemodeling,semanticsimilarity,controlled
textgeneration,andbiasanalysis.
2.1 LanguageModeling
Languagemodels(LMs)areprobabilitydistributionsoverwordsequencesandacorecomponent
ofnaturallanguageprocessing(NLP)systems[Bengioetal. 2000].Withtheemergenceofthe
transformerarchitecture[Vaswanietal.2017],therewasaparadigmshiftawayfromrecurrent
neuralnetworks[Rumelhartetal.1986]sincetransformersallowparallelcomputations,speeding
uptrainingtimes,andprovesuperiorincapturinglong-termdependencies[Vaswanietal.2017].
Theyusetheattentionmechanism[Bahdanauetal.2014],whichdirectsthefocusonimportant
tokens in the input sequence. Nowadays, numerous pre-trained transformer architectures are
availableforpublicuse[Wolfetal.2020].Therearedifferenttypesoftransformers,wherebythe
twomaincategoriesaremaskedlanguagemodelsandgenerativelanguagemodels.
MaskedLMs—BERT[Devlinetal.2018]isatransformer-basedLMthatwastrainedonmasked
languagemodeling(i.e.,cloze)andnext-sentencepredictiontasksandiscommonlyfine-tunedfor
diversetextclassificationtasks[HowardandRuder2018].Duetoitspre-trainingobjective,BERT
(aswellasothermaskedlanguagemodels)isnotsuitablefortextgenerationtasks.WeuseBERT
formaskedwordpredictionintheontologicalreplacefunctionality W .
GenerativeLMs—Textcanbegeneratedusinggenerativetransformermodels,suchasGPT-
2[Radford,Wu,Child,etal.2019],GPT-3[Brownetal.2020],orGPT-4[OpenAI2023].Theseare
autoregressivemodelsthatwerepre-trainedonthecausallanguagemodelingtask,learningto
predictthenextwordintheinputsequence.Forabroaderoverview,seethesurveyonpre-trained
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:4 Spinneretal.
languagemodelsfortextgenerationbyLieetal.[J.Lietal.2021].Inourwork,weuseGPT-2and
Bloom[Scaoetal.2023]fortextgeneration;however,theapproachisdesignedtosupportother
transformer-basedLMsaswell.
2.2 SemanticSimilarity
WordTaxonomiesandOntologies—Leveragingsemanticgraphsandknowledgebases,suchas
YAGOandDBpedia,itispossibletoinferconceptortopichierarchiesvialanguagemodels[Chen
etal.2021;Huangetal.2020;C.Zhangetal.2018]orexpandexistingtaxonomies[Jiangetal.2022;
Xuetal.2022].MethodssuchasOntoEA[Xiangetal.2021]alignentitiesbyjointlyembedding
ontologiesandknowledgebases.Taxonomiescanbeusedtoimproverecommendersystems[Tanet
al.2022]andhelpwithentityrecognition[Z.Lietal.2022]ortranslation[Z.Lietal.2022].WordNet
informationcanbeintegratedintopre-trainedlanguagemodelsforimprovedsensedisambiguation,
e.g.,ARES[Scarlinietal.2020],orusedtobuildhuman-readableconceptvectors[ConiaandNavigli
2020].Forourmethod,weuseARESandBERTembeddingsinconjunctiontocreatedomain-specific
predictionswithanontologygraph W createdfromtheBabelNet[NavigliandPonzetto2012]
semanticgraph.
EmbeddingSimilarity—Inlanguagemodels,eachtokenoftheinputtextismappedtoahigh-
dimensionalvector.Relatedworkhasshownthatthesecontext-dependentembeddingsencode
different context/language properties. Although BERT is the most widely analyzed language
modelsofar[Rogersetal.2020],othertransformermodels,suchasGPT-2,andtheirproduced
embeddingspaceshavealsoattractedcomputationallinguistics’andvisualanalyticsresearchers’
attention [Ethayarajh 2019; Sevastjanova, Kalouli, et al. 2022]. Prior research has shown that
semanticinformation,suchaswordsensesandsemanticroles,iscapturedbestinthehigherlayers
oftransformermodels[Reifetal.2019;Sevastjanova,Kalouli,etal.2022;Wiedemannetal.2019].
Thus,thesecontextualizedembeddingsarecommonlyusedasfeaturesforsemanticsimilaritytasks.
Inourwork,weapplyadimensionalityreductiontechniqueonembeddingsextractedfromtheused
LMstomapthetokenstouniquecolorsbasedontheircoordinatesinthetwo-dimensionalspace.
Withthisapproach,tokenswithasemanticsimilaritygetassignedtosimilarcolors[El-Assady,
Kehlbeck,etal.2022].
2.3 ControlledTextGeneration
AlgorithmicApproaches—Ingeneral,controllingthestyleandinformationofnaturallanguage
generationisoneoftheapplicationsidentifiedbyGattandKrahmer[GattandKrahmer2018].
One challenge of integrating knowledge into text generation is the automatic steering of the
generationinaparticulardirection.Usingplug-and-playlanguagemodelsisonepossibilityto
steertextgeneration[Qinetal.2020].Concerningpre-trainedlanguagemodels,itispossibleto
control,e.g.,thesentiment[Dathathrietal.2019;Huetal.2017],keywords[X.He2021],orthe
topic[Dathathrietal.2019].FrameworkssuchasFAIR[HuaandWang2020]allowthegeneration
ofcontent-controlledtextbycombiningBERTwithBART[Lewisetal.2020].Alargeroverviewis
giveninthesurveybyZhangetal.[H.Zhangetal.2022].Buildingonthis,manyapproachesnow
integrateexternalresourcessuchasknowledgebases.Moredetailscanbefoundinthesurveyby
Yuetal.[Yuetal.2022].However,thesetechniquesdonotallowimmediateinterventioninthe
decisionprocess,whichwespecificallytargetwithourapproach.
Visual Interactive Approaches — Focusing on interactive editing, Du et al. [Du et al. 2022]
provideinteractivesuggestionsintheirtooltoachievehigh-qualitytexteditswithminimalhuman
effort.PadmakumarandH.He[PadmakumarandH.He2022]useahuman-in-the-loopapproach
toreplacetextsegmentsforthetaskofcreativeimagecaptioning.Gehrmannetal.[Gehrmannetal.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:5
2019]proposeaninteractiveframeworkthatallowsuserstocontrolgenerativesegmentsthrough
aprocesscalledcollaborativesemanticinference.Followingthis,Strobelt,Kinley,etal.[Strobelt,
Kinley,etal.2022]createGenNi,aninterfaceforcollaborativetextgeneration.Theyguidethe
modeloutputusingexplicitlydefinedconstraints.Theuserhastoknowbeforehandhowhewants
tocontrolthemodeloutput,asitisnotpossibletoadaptthestateduringinference.WithWordcraft,
Yuanetal.[Yuanetal.2022]presentaninteractiveinterfacethatallowswriterstocreatestories
withtheassistanceoflargelanguagemodels.Theirsystemletsauthorsre-write,replace,andauto-
generatetext,aswellasdefinecustomrequeststothelanguagemodel.Incontrast,ourapproach
enablesdirectinteractionwiththemodel’soutputsbyexposingpredictionsandprobabilitiesinthe
beamsearchtree.
2.4 BiasAnalysis
Current research explores not only what the models learn but also when they fail and which
limitationstheyhave,suchasdifferenttypesofbiases[Garrido-Muñozetal.2021].Forinstance,
Blodgett et al. [Blodgett et al. 2020] present a taxonomy for fairness definitions that machine
learningresearchershavedefinedtoavoidexistingbiasinAIsystems.Mehrabietal.[Mehrabietal.
2021]definethebiasproblemspecificallyinlanguagemodelingtasksinaformalwayandexplore
howithasbeentreatedinrelatedworkregardingtheirdetectionandcorrection.
Inmaskedlanguagemodels,thedetectionofbiasistypicallydonebyapplyingtemplatesor
pre-definedwordlists.Forinstance,theWordEmbeddingAssociationTest(WEAT)[Caliskanetal.
2017]measurestheassociationbetweentwotargetwordsets(e.g.,malepronounsand,e.g.,female
pronouns)basedontheircosinesimilaritytowordsfromtwoattributesets(e.g.,termsrelated
toscienceorart)tomakeconclusionsaboutencodedbiases.Liangetal.[Liangetal.2021]show
thattheanalysisofbiasesintextgenerationcanbemorenuanced,e.g.,biasescanariseduringthe
generationofanytoken[Nadeemetal.2021].Alnegheimishetal.[Alnegheimishetal.2022]find
thatbias“evaluationsareverysensitivetothedesignchoicesoftemplateprompts.”Accordingtothe
authors,theuseoftemplate-basedpromptstendstoevokebiasesfromthemodel’sdefaultbehavior
rather than reflecting the actual correlation between gender and profession, analyzed in their
work.Thus,weproposeatree-basedapproachforcomparative,exploratorybiasanalysis,allowing
thedetectionofbiasesinvariable-lengthsequencesandtheidentificationofsubtlenuancesin
themodel’spredictions.Foradetailedcasestudy,show-casingthebenefitsofourcomparative
approach,seesection6.1.
3 ProblemCharacterization
WithrecentadvancesinlanguagegenerationandthereleaseofChatGPT,languagemodelshave
madetheirwayintomainstreamuse.Whileautomatictextgenerationthroughlanguagemodels
cansupporttheauthorthroughcorrections,suggestions,orchat-basedquestionanswering,un-
derstandingofthemodel’scapabilitiesandlimitationsandaccesstoitspredictionsisstilllimited.
However,suchunderstandingandaccessarecrucialforraisingawarenessofdangers(e.g.,biased
outputs,hallucinations),allayingfearsofitspotential(e.g.,overestimationofamodel’scapabilities),
andenablinguserstosteerthemodel’spredictionstowardstheirintention(e.g.,byselectingor
modifyingoutputs).
Whiletheaverageusermightnotbewillingtoinvesttimeandeffortininvestigatingthebehavior
oflanguagemodels,weidentifytwoprimaryusergroupswithdifferentinterestsandrequirements
forlanguagemodelanalysis.Wedefinenon-experts Non asinterest-drivenpersonswithanaffinity
for technical advancements and the wish to explore modern language models. The term “non-
expert”onlyreferstotheuser’sexperienceswithlargelanguagemodelsandtheirbackground
incomputationallinguistics;theycanstillbedomainexpertsinotherfields.Examplescouldbe
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:6 Spinneretal.
ajournalistwhowritesaboutlanguagemodelsandwantstounderstandtheircapabilitiesand
limitations or a writer who wants to use LLMs to generate text with a specific style or topic.
Complementary,wedefinelinguisticexperts Lin asusersworkingin(computational)linguistics,
withamainfocusontheanalysisofmodelbehavior.Anexamplecouldbealinguistwhowants
toobservebiasesencodedinthemodel[Spinner,Kehlbeck,etal.2023].Ourapproachistargeted
towardsbothusergroups,withshiftingfocusonthetasksoursystemsupports.Forthenon-experts,
understandingofthemodel’scapabilities,explorationofoutputs,investigationofuncertainties,
andtheabilitytoadaptmodeloutputsareprimarilyimportant.Incontrast,thelinguisticexpertis
interestedinthecloseanalysisofmodeloutputs,e.g.,toobservelearnedsyntacticandsemantic
structures,identifymodeldefects,orassessmodelbiases.Inthefollowing,wespecifythechallenges
andtasksforthederivedusergroups.
3.1 Challenges
Thechallengesarederivedfromresearchgapsinrelatedworkandfromdiscussionswithnon-
experts Non,machinelearningexperts,andcomputationallinguists Lin.
Explainability Ex —Despitetheimpressiveperformanceofstate-of-the-artlanguagemodels,
theirpredictionsareoftenunderexplained,asdeep-learning-basedmodelsaretypicallyblackboxes,
makingexplainabilityamajorchallenge[Danilevskyetal.2020].However,languagemodelshave
theadvantageofinterpretablein-andoutputs(namely:text)andeasy-to-understandprediction
mechanisms,whichweaimtoleveragetosolvethischallenge.Weidentifytwoprimaryaspects
ofexplainabilityregardinglanguagemodels:modelandoutputexplainability.Explainabilityis
importantforboththenon-expert Non andthelinguisticexpert Lin.
Modelexplainabilityrelatestoexplanationsofthemodel’salgorithmicapproach,suchaspro-
viding information on the model’s architecture, the used search algorithm, or the influence of
randomness(c.f.,reproducibility)[Spinner,Schlegel,etal.2020].Particularly,mainstreammedia
often fail to explain the primary mechanism behind LLMs: predicting the likelihood of tokens
to follow a sequence of previous tokens. Although some articles briefly touch the topic [Metz
2022;Roose2023],thereismuchmisinformationthroughexcessiveabstractionandalackofeasy-
to-followvisualizationsandinteractivesystemsthatcouldimpartathoroughunderstandingto
non-experts.Understandingthismechanismiscrucialtoraisingawarenessofamodel’slimitations
andallayingfearsofitspotential.Outputexplainabilityreferstoexplanationsofthemodel’stoken
representationsandoutputprobabilities,suchastokenembeddingsimilarityoroutputcertainty.
Comparability Com —Theabilitytoexplorethespaceofpossiblemodeloutputsisvastand
currentlyunderexplored[Alnegheimishetal.2022].Fortheanalysis,instance-basedcomparability
ofgeneratedoutputsisessentialforlinguistics,e.g.,forbiasanalysisorhypothesisgeneration.
Particularly,non-templatebased,explorativeanalysisenableshypothesesgenerationandinductive
learning[R.J.SternbergandK.Sternberg2016].
Adaptability Ada —Evenstate-of-the-artlanguagemodelsoftenfailtoproduceoutputwhich
alignswithhumanintentionsandstickstofacts[Jietal.2023;LeCun2023].Therefore,adaptability
isessentialtoemploylanguagemodelsinreal-worldscenarios.Again,wedifferentiatetwosub-
aspects: output adaptability and model adaptability. Output adaptation refers to direct edits of
themodel’spredictions,e.g.,tocorrecthallucinatedfacts,re-primethemodelthroughentering
customtext,orselectfromalternativeoutputs,targetingboththenon-expert Non andlinguistic
expert Lin.Thatfollowed,modeladaptationrelatestomodelfine-tuningwiththeediteddatato
makechangespermanentforfuturesessions,whichisalsorelevantforbothusergroups.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:7
3.2 TheTree-in-the-LoopApproach
To address the challenges identified above, we propose the tree-in-the-loop paradigm, a novel
approach to interactively explore and adapt the predictions of language models through the
visualizationofthebeamsearchtree.
Withtheinventionoftransformers,thearchitectureofstate-of-the-artmodelsiswell-established,
shiftingthefocusforperformanceimprovementsonthetrainingprocessandthequalityoftraining
data[Ouyangetal.2022].Consequently,understandingamodel’sbehaviorinvolvesexaminingits
inputsandoutputs,whichreflectthe“knowledge”ithasacquiredduringtraining.Therefore,our
approachemphasizesmakingtheseinputsandoutputsmoreuser-accessibleandexplorable.
Ineachstep,whenpredictingthenexttokenforagiveninputsequence,themodeloutputsa
probabilitydistributionoverallknowntokens.Thefinaltexthastobeconstructedbysampling
from this probability distribution. A common heuristic to choose the output with the highest
probabilityisbeamsearch.Beamsearchisagreedysearchalgorithmthatexpandsthe𝑘 mostlikely
sequencesineachstep,resultinginatreewith𝑘 nodesineachtreelevel.𝑘 iscalledthebeam
width.Brancheswithlowoverallprobabilitystallinthisprocess,resultinginatreewithvarying
depth.Thedeepestleafnodewiththehighestprobabilityisthenchosenasthefinaloutput.Often,
additionalparametersareusedtoincreasethediversityofthegeneratedtext,e.g.,bypenalizing
therepetitionof𝑛-gramsorbyaddingrandomnesstothesamplingprocess,e.g.,throughtop-𝑘
samplingortemperaturescaling.
Mostinterfacesonlypresenttheuserwiththefinaltext,discardingallinformationaboutthe
sampling process, such as uncertainties of predictions, alternative outputs, or the influence of
parameters such as the beam width or an𝑛-gram penalty. To enable an understanding of the
model’spredictionprocess,weaimtomakethisinformationaccessibletotheuser.Thisismost
straightforwardlydonebyvisualizingthebeamsearchtree,whichiseasytounderstandandinteract
with.Furthermore,itprovidesadirectrepresentationoftheunderlyingsamplingalgorithmand
thusdoesneitherneglectinformationnorintroducefalserationalization.
Thetree-in-the-loopapproachistheextensionofthebeamsearchtreewithadditionalaugmen-
tations,visualizations,andinteractionpossibilities.Thismakesthetreeaccessibletonon-technical
users Non andsupportslinguisticexperts Lin intheadvancedanalysisoflinguisticphenomena.
3.3 UserTasks
Fromthebefore-discussedchallengesofexplainability,adaptability,andcomparability,wederive
thefollowingusertasks,asdepictedinfigure2.Whilesometasksareessentialtoloadandinteract
withLLMs,othersareoptionalandonlyrelevantforspecificusecases.
ModelPromptingandConfiguration—Tochooseandassesmodelsfromthevastzooofpre-
trainedLLMs[Wolfetal.2020],theuserhastobeabletoloaddifferentmodels.Furthermore,the
usershouldbeabletoprovideaprompttothemodelandconfigureparametersfortheprediction
algorithm. After interactively editing outputs and, potentially, fine-tuning the model, the user
should be able to save the refined sequences and model for future sessions. Since these tasks
describebasicinteractionswiththemodel,theyareequallyimportantforthelinguisticexpert Lin
andthenon-technicaluser Non.
Loadandassess(pre-trained)models,provideprompts,andconfigureparametersforthe
T0
predictionalgorithm.Savetreesandmodelsforfuturesessions.
Tree Exploration & Explainability — The beam search tree, used to sample model outputs,
shouldbetransparentandaccessibletotheuser,allowingthemtoexplorealternativesandassess
thecertaintyofthemodel’spredictions,addressingtheexplainabilitychallenge Ex.Supporting
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:8 Spinneretal.
beamsearchexploration,semanticannotationsofthetreeshouldbeprovided,e.g.,toidentify
topicsimilarityortodiscoverundesiredpatternslikeloopingstructures.Thisisimportantforboth
thenon-expert Non andforthelinguisticexpert Lin,whoareinterestedinthecloseanalysisof
modeloutputsandneedahigher-leveloverviewtocoverlargetrees.
Assessprobabilitiesandexplorealternativebranchesinthebeamsearchtree.Identifytopic
T1
similarityandundesiredpatterns,suchasloopingstructures.
GuidedTextGeneration—Usingthestartpromptorexistingsequencesfromthetree,theuser
shouldbeabletoquerytheLLMtoextendthebeamsearchtreewithnewpredictions.Sincethe
beamsearchtreemightgrowtoasignificantsize,atextviewshouldbeprovidedtoclose-read
generated text and navigate the beam search tree to a local context. Also, for longer texts, an
overviewofthetopicstouchedfacilitatesanoverviewandunderstandingofthegeneratedtext.
Thistaskmainlytargetsthenon-expert Non,whoislikelytogeneratelongertexts.
QuerytheLLMtoextendthebeamsearchtree.Navigatethebeamsearchtreetoalocal
T2 context. Investigate the topics touched by the generated text and stalled beam search
branches.
ComparativeAnalysis—Comparativeanalysistacklesthecomparabilitychallenge Com andis
particularlyimportantforthelinguisticexpert Lin,whoisinterestedinthecloseanalysisofmodel
outputs.Differenttreescanbegeneratedandcomparedbyvaryingstartpromptandbeamsearch
parameters,allowingtoassesstheeffectsofthosechanges.Semanticannotationsandaggregated
representations shouldbeprovidedto quicklyidentifythe keydifferences between trees.This
facilitates,e.g.,generatingnewhypotheses,analyzingmodelbiases,orinvestigatingtheinfluence
offunctionwordsonthepredictions.
Generate and compare different trees by varying prompt and beam search parameters.
T3
Observesyntacticandsemanticdifferencesinthetrees.
BST Adjustment & Model Adaptation — Enabling adaptation to domain and personal user
preferences,itshouldbepossibletoeditthegeneratedtext.Thiscaneitherhappenbydirecttext
edits,choosingfromasetofalternatives,orpruningunwantedbranchesofthebeamsearchtree.
Aftereditingthetree,theusershouldbeabletofine-tunethemodelwiththeeditedsequences
toalignfuturepredictionswiththeuser’spreferences.Bothaddressestheadaptabilitychallenge
Ada.Thistaskisimportantfornon-expert Non whoneeddomainadaptationorforlinguistic
experts Lin whowanttoobservetheinfluenceofsuchadaptationontheLLMs’predictions.
Interactivelyeditorreplaceproducedsequencestoadaptthetexttopersonalpreferences
T4
anddomains.Fine-tunethemodelwiththeeditedsequences.
4 TreeVisualization&ModelConfiguration
ThebeamsearchtreeiscentraltoourgenerAItortechnique,thereforebeingthemaincomponent
visiblethroughouttheanalysis.Inthissection,wedescribethevisualrepresentationofthetree,
how it is augmented with information, how the user navigates the tree to a local context and
extendsthetreewithnewpredictions,andhowtheinteractionwithtreenodesisimplemented.By
augmentingthetreewithtask-specificwidgets W,weprovidetailoredcontrols,visualizations,
andinteractions,supportingmodelpromptingandconfiguration T0 andtreeexplorationand
explainability T1.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:9
Succession Edge Keyword Main Branch
Loop Edges (Positive)
Probability Sentiment
Fig.1. Thebeamsearchtreevisualization.Edgewidthand–labelencodetheprobabilityofanodetofollow
itspredecessor.TheleafnodeofthebeamwiththehighestoverallprobabilityismarkedasHead.Keywords
arehighlightedusingsemanticcolors.Thebranchcolorencodesthesentimentofthesequenceuptoanode.
4.1 BeamSearchTree
Our technique is based on a visual representation of the beam search tree as the key analysis
component,establishingthetree-in-the-loopapproach.Itisusedtosamplethefinaloutputsequence
fromthetokenprobabilitiesineachpredictionstep.Inthetreevisualization,nodesencodesequences
andedgestheirorder,asdepictedinfigure1.Thetreeislaidoutfromlefttoright,startingeither
withtheinitialpromptusedduringtreecreationoranarbitrarytreenodethatissetbytheuser
whenonlyasubtreeshouldbeinspected.Edgewidthand-labelencodethenodes’probabilityof
followingitspredecessor.WemarktheleafnodeofthebeamwiththehighestprobabilityasHead
node, which, when not configured otherwise, is the one defining the final text output. When
renderingthetextassociatedwiththetreenodes,wereplaceinvisible–orcontrolcharacterswith
visibleproxies,e.g.,whitespaceswith andnewlineswith .Thetreevisualizationimpartsthe
uncertaintyoftokensandsequencesandletstheuserexplorenext-likelyalternativesintheform
ofstalledbranches T1.
Toextendthetree,theusercaneithertriggerabeamsearchrunfromtheHeadnode,orstart
auto-prediction,whichiterativelyextendsthetreeattheHeadnodeuntilstopped.
LoopDetection—Weautomaticallydetectrepeatingnodesequencesinthetreeanddenotethem
withadottededge,asshowninfigure1.Thisallowstheusertoquicklyidentifyrepeatingpatterns,
whichareoftenunwantedmodeldefects,tellinglinguisticexpertsaboutthemodel’slimitationsor
probablymiss-chosensearchparameters[Platen2020].
KeywordHighlights—Weextractandhighlightkeywordsfromthesequencesinthetree,allow-
inguserstointuitivelydistinguishlessimportantnodes,e.g.,stopwords,frommeaningfulnodes,
e.g.,propernouns T1.Asshowninfigure1,wecolorthekeywordnodesinthetreevisualization
accordingtotheirsemanticembeddings[El-Assady,Kehlbeck,etal.2022],enablingaquickimpres-
sionofthesemanticsimilaritybetweentheconceptspresentinthetree.Furthermore,itallows
identifyingconceptdriftbyrevealingchangingconceptsascolorshiftsinthetreevisualization.
SentimentHighlights—Facilitatingvisualperceptionofthesentimentoftreebranches,wecolor
theedgesinthetreevisualizationaccordingtothesentimentofthesequenceuptotheedge’starget
node,asshowninfigure1.Thesentimentisestimatedbyapplyingathree-classRoBERTa-based
sentimentclassifier,whichwastrainedonsocialmediaposts[Hartmannetal.2021].
4.2 ModelPromptingandConfiguration T0
TreeCreationand–Selection W —Thetreeselectionpanel( infigure4)allowsloading
existingtreesintotheworkspaceandcreatingnewones.Whencreatinganewtree,theuseris
promptedforastartingsequence,whichisusedastheinitialinputsequencepassedtothemodel.
Thestartingsequencealsoformstherootnodeofthetree.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:10 Spinneretal.
Prediction Parameters W — The prediction parameters panel ( in figure 4) allows the
usertospecifytheparametersusedwhenexecutingabeamsearchstep.Theparameter“top-𝑘”
specifiesthenumberofsamplesdrawnineachbeamsearchiteration,eitherbyselectingthe𝑘most
probabletokensor—iftemperatureisenabled—bysamplingfromthemodel’soutputdistribution.
The length of the beam search can be specified by the parameter “next𝑛 words”. Finally, the
parameter“temperature”allowscontrollingtherandomnessofthemodel’soutputdistribution.A
temperaturevalueofzerodisablestemperatureandselectsthetop-𝑘 mostprobabletokensineach
beamsearchiteration.
Model Snapshots and –Tracking W — The model tracking panel allows the user to load
differentpre-trainedmodels,e.g.,fromHuggingFace[Wolfetal.2020].
Out of the box, generAItor provides access to GPT-2 Base, GPT-2
Large[Radford,Wu,Amodei,etal.2019],andBloom[Scaoetal.2023],
butother,transformer-basedmodelscaneasilybeadded.Morespecifi-
cally,ourapproachismodel(transformer)agnostic;onlytheembedding
projection(c.f., W )hastobere-computedfornewmodelvariants.
Besidesloadingpre-trainedmodels,themodeltrackingpanelalsoal-
lowstheusertocreatesnapshotsofadaptedmodels T3.Bycreatinga
snapshotofthecurrentmodelstate,theusercaneasilyrestorethisstate
later,e.g.,ifthemodelwasfine-tunedtoapointwhereitnolongergeneratesmeaningfuloutputs.
4.3 TreeExplorationandExplainability T1
TreeStyleToggles W —Thebeamsearchtreeisaugmentedwithcolorinformationandcanbe
visualizedindifferentlevelsofdetail.Particularly,theedgescanbecolored
by sequence sentiment, the nodes’ fill color can be set based on their
semanticembeddingcolor,thenodes’strokecanbesettorepresenttheir
tokenprobability,andwordlists(see W )canbecoloredbyacategorical
colorscale.Furthermore,thetree’slevelofdetailcanbeswitchedbetween
Full,showingallnodetextsandusingfullnodespacings;Collapsed,hidingallnodetextsandonly
showingthetree’sstructurewithminimalspacings;andSemi-Collapsed,onlyshowingthenode
textfornodesoccurringinactivewordlists(seefigure6).
2D Embedding Map W — The 2D embedding map ( in figure 4) shows an image of the
currentlyselectedtwo-dimensionalsemanticcolormap[El-Assady,Kehlbeck,etal.2022],usedto
colorthekeywordsinthetreevisualization.Byoverlayingthecolormap
imagewiththekeywords,weenableuserstoexplorehowthekeywords
aredistributedinthehigh-dimensionalspace.Thepositionofkeywords
onthecolormapiscomputedbyatwo-dimensionalUMAP[McInnes
et al. 2018] projection, which we priorly anchored on the keywords
extractedfrom150ksentencepairsintheMultiNLIdataset[Williams
etal.2018].Thisallowsthedetectionofsemanticsimilaritybetween
keywordsandtheidentificationofthemajorconceptspresentinthe
tree.Byhoveringabeamsearchbranch,theusercanfilterthekeywords
visibleontheembeddingmaptoonlyshowthekeywordsofthehoveredbranch.Furthermore,
hoveringrendersapathconnectingthekeywordsaccordingtotheiroccurrenceinthebranch.
Thissequenceprojectionbuildsintuitivepicturesofthesequence,allowingtocomparesentence
structuresandthementionedconcepts.Differenttwo-dimensionalcolormapscanbechosenina
dropdownmenuinthe2Dembeddingmappanel.Thesidefigureshowsthebeamsequence“The
moviewasshotinNewYorkCity”onthe“Teuling2”colormap[Teulingetal.2010].
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:11
(TT00) Model Prompting (TT22) Guided Text Generation (TT44) Beam Search Tree
& Configuration Ontology & Model Adaptation
Text View
Voronoi Treemap
Ontological
Replace
Model (TT11) Tree Exploration & Explainability
Configuration
Beam Search Tree
Beam Search
Configuration Node Edits
Tree Creation &
Selection
Fine- Tuning
2D Embedding Map Style Options
(TT33) Comparative Analysis
IFF
Placeholders
IFF
Domain-S pecific & Instances
Word Lists
UpSet Plot Multi- tree
Fig.2. ThefivemaintasksofinteractivetextgenerationassupportedbygenerAItor(seesection3.3).The
beamsearchtreeisthekeyelement(seesection4),facilitatingvisualizationandinteractionwiththemodel’s
decisions.Eachtaskhasasetofwidgetsassociated(seesection5),providingtask-specificvisualizations,con-
trols,andinteractionpossibilities.Followingourproposedtree-in-the-loopparadigm,thetasksareinterwoven
andcanbecombinedinaniterativeprocess,centeredaroundthebeamsearchtree.
5 TextGeneration,Comparison,&ModelAdaptation
Besidesthedefaultwidgetstoconfiguremodels,specifyparameters,promptthemodel,andexplain
thebeamsearchtree,weprovideadditionalwidgetsthataretailoredtoaspecifictaskmode.We
distinguishbetweentwomainmodes:controlledtextgeneration(section5.1)andcomparative
analysis(section5.2).Eachmodehasadedicatedsetofwidgetsenabledbydefault.Theyenhance
existingfunctionalitieswithadditionalon-demandinformation,allowadditionalinteractions,or
enablespecificmodesofanalysis.Thewidgetsaredesignedasmodularcomponentsthatcanbe
enabled/disabledandmovedaroundtheworkspacetosupporttheuser’sworkflow.
5.1 TextGeneration T2 andBSTAdaptation T4
Guided text generation provides tools to support the user in the informed generation of text,
particularlytoclose-readgeneratedtext,navigatethebeamsearchtree,andselectdesiredsequences.
Furthermore,itprovidescontentsummarizationintheformofanontologyVoronoitreemap,which
canbeusedtodetectconceptsintheproducedtextandtoidentifysemanticdifferencesacross
nodeswiththesamekeywords.
5.1.1 WidgetsSupportingGuidedTextGeneration
TextView W —Whilethebeamsearchtreevisualizationsupportsunderstanding,exploration,
andinteractiononahighlydetailedlevel,itishardtoreadthefinaloutputtextfromonlyobserving
beamsandnodes.Therefore,atextoutputpaneldisplaysthefull
sequenceofthemainbranch,whichinturnishighlightedingray
inthetreevisualization.Toretainthemembershipofeachnode
anditscorrespondingembeddingandkeywordinformation,the
nodesequencesareslightlyspacedinthetextviewandunderlinedwiththeirkeywordembedding
color.Themorecompressedrepresentationinthetextview,togetherwiththeabilitytooverflow
thetextcontainerusingscrollbars,allowstoalwaysdisplaythefulltextstartingattherootnode.
Weusethisadvantageofthetextviewtoallowtreefiltering:byopeningthecontextmenuona
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:12 Spinneretal.
textnode,thenodecanbesetasstartnode( ).Thisfiltersthedisplayedbeamsearchtreetothe
descendantsoftheselectednode,allowinglocalexplorationandpreventinginformationoverload
onlargetrees.Inreturn,leafnodescanbesetasendnode( ),incaseabranchdifferentfromthe
onewiththehighestbeamprobabilitycontainsthepreferredoutputtext.Acopybuttonfacilitates
copyingthegeneratedoutputtexttotheclipboard.
NodeContextMenu W —Thenodesinthebeamsearchtreeofferafeature-richcontextmenu,
showninthemiddle-rightoffigure2.Inthefollowing,wedescribethefunctionalityofthecontext
menuentriesthatarenotcoveredbytheirrespectiveworkspacesubsection.
Edit/ Remove Theeditentryallowsalteringthetextoftheselectednodemanually.When
selectingit,thenodechangesintoaninputfield,wheretheusercanmanuallyenterthedesired
text.Afterfinishingtheedit,thenodechangesbackintonormalmode,andthenodeisupdated
inthebeamsearchtree,includingitskeywordinformationandembeddings.Theremoveentry
allowsremovingtheselectednodeandallitsdescendantsfromthetree.
Predict AlternativetopredictingatthecurrentHeadnode,theusercanalsopredictfrom
anynodeinthetreebyselectingthepredict entryfromthecontextmenu.Theparametersare
specifiedinthepredictionparameterspanel.
OntologicalReplace Basedoninformationextractedfromanunderlyingontologygraph
andtheusageofamaskedlanguagemodel,theontologicalreplaceentryprovidesalternative
suggestionstoreplacetheselectednodewith.
Re-TraintoHere The re-train to here entry allows fine-tuning the model with the beam
sequenceuptotheselectednode,addressingtask T4.Withoutfurtheruserinput,fine-tuning
isexecutedinstantlyinthebackgroundwhenthebuttonisclicked,abstractingtheunderlying
complexprocessandmaximizingsimplicityfortheuser.
Ontology Voronoi Treemap W — Through an underlying ontology graph, we provide a
Voronoitreemapvisualizationtosupporttheuseringettinganoverviewoftheconceptsclosely
linkedtothekeywordspresentinthetree.Theextractedkeywordsfrom
thebeamsearchtreeareattachedtonodesintheontologyhierarchyof
BabelNet[NavigliandPonzetto2012].Wegrowasubsumptionhierarchy
fromthesekeywords,whosenodesbecomemoreandmoregeneral.Finally,
nodesareconnectedtotheirrespectivesubdomainsanddomains(e.g.,dog
→Animal →BIOLOGY).Althoughthewholeontologygraphallowsan
in-depthviewofthesubsumptionhierarchy,thereadabilityofthegraph
worsensasthenumberofleafnodesincreases.Instead,weutilizeaVoronoi
treemap visualization, allowing the user to view the hierarchy in four
predefinedlayers:domains,subdomains,synsets,andkeywordinstances.
Domainsandsubdomainsprovideanoverviewoftheconceptsinthebeam
searchtree.Synsetsaggregatesimilarkeywords.Thekeywordinstancelayershowsallkeywords.
Keywordscanappearmultipletimesinthislayer,asonekeywordcanappearatdifferentpositions
inthebeamsearchtree.Becausethesurroundingcontextofakeyworddiffersforeachnode,their
embeddings differ, resulting in different colors, e.g., the keyword “walk”. To allow the user to
investigatethisfurther,hoveringoveracelloftheVoronoitreemaphighlightstherespectivenodes
inthebeamsearchtree,enablingthemtoinspectthekeywordsintheircontext.
Ontological Replace W — Using our tool, text generated by the model can be adapted to
theuser’spreferencesbyselectingbranchesoreditingmodeloutputs.However,sometimes,the
predictions from the model are not what the user has in mind. We offer an alternative way of
adaptingthemodeltreeusingdomain-specific,context-sensitivealternatives.Iftheuserisunsure
aboutasuitablereplacementwordandrequiresguidance,hecanusetheontologicalreplacefunction.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:13
(1) (3)
Predict Loop L Do io ffp e- r> eC nth o Bo rs ae nch (4)
Ontological
Replace
Edit &
Predict
(2)
Fig.3. Textgenerationworkflowasdescribedinsection5.1.2.(1)Aftercreatinganewtreeandpredictingwith
thesetparameters,themodelrunsintoaloop.Bychoosingadifferentbranch,thisissuecanberesolved.(2)
Bymanuallyeditingnodes,factualknowledgecanbeincorporatedintothetext.(3)Theontologytreegives
anoverviewofconceptsconnectedtothegeneratedtext;(4)ontologicalreplacementssuggestalternatives.
Withtheinformationcurrentlyintheontologygraph,it
ispossibletogeneratepredictionsforaspecificnodeand
groupthembydomain.Thesedomainpredictionscanbe
fromthecurrentdomainsinthebeamsearchtree,ortheuser
canmanuallyadddomainsfromapredeterminedselection.
The domains and their respective suggestions are words
thatthelanguagemodelmightnothavesuggestedinits
top-𝑘 prediction,makingitanintermediatemodebetween
manualeditingandautomaticpredictionofthemodel,even
allowingout-of-distributionsuggestions.Extensiveimplementationdetails,includingfiguresofthe
underlyingNLPpipelines,canbefoundinAppendixA.
5.1.2 WorkflowDemonstration:TextGeneration
Thefollowingexemplaryworkflowshowcaseshowourapproachisusedtogenerateandadapt
text.Todemonstrate,weutilizeGPT-2Base2[Radford,Wu,Amodei,etal.2019]asthelanguage
model.NotethatthesequencespresentedinthisexampledonotrepresentthequalityofSOTA
languagemodels.Nevertheless,GPT-2Baseiswellsuitedtoshowcaselargermodels’deficiencies
(e.g.,repetitions,hallucination)inbriefexamples.Sinceourapproachismodel-agnostic,otherLMs
canbeloadedinstead.
A newspaper author wants to write a short but informative article on the United States of
America(USA).Asabasis,heusesafactssheetcontaininginformationonpopulation,geography,
etc.oftheUSA.InthegenerAItorworkspace,hecreatesandloadsanewtree( )withthestarting
sequence“TheUnitedStatesofAmerica”(figure3.1).Aftersettingthebeamsearchparameters( )
to𝑘 =3and𝑛 =10,hestartspredictingattheheadnode.Aftertwobeamsteps,thebranchwith
thehighestprobabilitygetsstuckinaloop:“TheUnitedStatesofAmericaisanationofimmigrants,
ofimmigrants,ofimmigrants,ofimmigrants.”However,bymanuallyselecting( )thesecond-best
scoringbranch,hecansteertheoutputtobemoreentertaining:“TheUnitedStatesofAmericaisa
nationofimmigrants,ofimmigrantsfromallovertheglobe.”Acceptingthisoutputasthestarting
sequence,hehidesearlierpartsofthetree( )andexecutesfurtherpredictionsteps( ).Atpoints
where the model is stuck or factual information should be integrated into the article, he uses
manualnodeedits( )tosetanewbaselineorenternumbersfromthefactsheet(figure3.2).E.g.,
2https://huggingface.co/gpt2
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:14 Spinneretal.
hechangesthehallucinatedprediction“Withmorethan1.5millionpeople”to“Withmorethan
331millionpeopleandaGDPof25.035trillionUSD”,leadingtotheprediction“...,America
isthelargesteconomyintheworld.”Byrepeatingthisprocess,theauthorcompilesadiverting
article.ObservingtheontologyVoronoitreemap( ),hecancheckonthemajorconceptscovered
byhisarticle,whichafterawhileincludeSociety,Politics,Places,andFeelings,leavinghim
satisfiedwiththediversityofhistext(figure3.3).Afterawhile,themodelagainpredicts“The
USAisanationofimmigrants.”Theauthordecidestousetheontologicalreplacefunction( ),
whichsuggestsmultipledomains,including“Person”,“Society”,and“Politics”(figure3.4).From
thepoliticaldomain,variousreplacementssoundpromising.Theauthorchoosesthesuggestion
“democracy”. He concludes the article with: “The USA is a nation of democracy.” The author is
satisfiedwiththeresultanddecidestore-trainthemodeltothetree’scurrentstate( ).Thisway,
themodelcanbeadaptedtotheauthor’swritingstyleanddomain-specificvocabulary,helpingto
generatemorecoherenttextinthefuture.
5.2 ComparativeAnalysis T3
Theusercanenterthecomparativeanalysisbyinsertingaplaceholderstringintoatree’sinput
prompt.Itautomaticallyreplacestheplaceholderwithuser-selectedstringinstancesandcreates
anewtreeforeachinstance,displayedasalternativesintheworkspace.Thecomparativemode
allowsforassessingnuancesinthemodel’spredictionsbasedoninputvariations,e.g.,forbias
detection.Thecasestudyoncomparativeanalysisinsection6.1givesseveralexamplesonhow
thecomparativemodecanbeusedtogeneratedifferenthypothesesandevaluatebiasesinmodel
predictions.
5.2.1 WidgetsSupportingComparativeAnalysis
TemplateNode&Multi-Tree W —Thecomparativemodeisenteredbycreatingatreewiththe
placeholder<PH>inthestartingsequence,facilitatingcomparisonovertreeswithslightlyvarying
startingsequences.Whenloadingsuchatreeintotheworkspace,thetemplatesequenceisshown
asthebasenode(1.ainfigure4).Theusercannowcreatealistofreplacementsfortheplaceholder
(1.binfigure4).Foreachreplacement,anewtreeisinstantiated,andbeamsearchisexecutedusing
thepredictionparametersconfiguredbytheuser.Toensuredeterminism,temperaturesamplingis
disabledincomparativemode.Theinstancesaredisplayedverticallystacked,withthereplacement
highlightedintherootnodeofeachtree(1.cinfigure4).
Domain-SpecificWordLists W —Theusercanselectdomain-specificwordliststoenable
targetedcomparisonbetweenthetreeinstances(2.ainfigure4).Treenodescontainingawordfrom
theselectedwordlistsarehighlightedinthetreewithabadge,denotingitsassociatedlist(2.bin
figure4).Thismakesiteasytospotdifferencesandcommonalitiesbetweenthetrees,e.g.,todetect
genderbiasbetweenmaleandfemalepersonnames(forexhaustiveexamples,seesection6.1).
Theusercaneitherchoosefromasetofpre-definedwordlistsfromdifferentdomains[DeepNLP
2023],coveringtypicalbiasanalysistasks,suchasMale/FemaleOccupations,Appearance,and
Negative/PositiveCharacteristics,oruploadtheirownwordlists.
Forkeyword-basedanalysisintreesofincreasingsize,weincludeasemi-collapsedtreeview,
activatableinthetreestyletoggles W andshowninfigure6.Itonlyexpandsthenodesmatching
toatleastoneoftheselectedwordlists,preservingthetreestructureandallowingtoeasilycompare
acrossworddomains.
UpSetPlot W —Visualcomparisonbetweentreeinstancesisfacilitatedbythedomain-specific
wordlists,semanticembeddings,andthepossibilitytosemi-collapsethetree.However,ifhigh
valuesforthepredictionparameters𝑘and𝑛arechosen,thetreecangrowlarge.Therefore,weoffer
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:15
2.b
1.a
1.c)
1.b
2.a
2.c
Fig.4. ThegenerAItorworkspaceincomparativeanalysismode,withtheassociatedwidgetsopened.The
treevisualizationasthecentralelementshowsalternativebeamsearchresultsunderdifferentreplacements
ofthe<PH>node.Wordsoccurringinoneoftheselectedwordlistsarehighlightedinthetree.TheUpsetplot
showstheoverlapoftheselectedwordlistsinthealternativetrees.Theedgesofthetreearecoloredbased
onsentimentanalysis,withredindicatingnegativesentimentandgreenindicatingpositivesentiment.
analternativesummarizationviewoftherelationsbetweenoccurrencesofwordsfromtheword
listsandthetemplatereplacements.WeuseUpSet[Lexetal.2014]plotsforthis,avisualization
technique showing overlaps between set-typed data (2.c in figure 4). Particularly, we visually
highlight which tree instances have overlapping words and, in consequence, also overlapping
wordlists.Eachrowrepresentsoneset,inourcase,onetreeinstance.Treeinstancesthathave
thesameoverlapareshownasonecolumnintheUpSetplot,withgrayconnectednodes.This
columnisonesetintersection,andthenodesthatparticipateinthisintersectionareshownas
ajoinedlist.UnderneaththeUpSetplot,weshowthecurrentlyselectedwordliststhatarepart
ofthesetintersectionandlistthespecificwordsthatappearinthetree,alongwiththeoverall
count of these words. This allows users to get a quick overview of which tree instances have
similarpredictedwordsgroupedbytheirwordlists.E.g.,theusercaninvestigatetheprediction
treeoffemalenamescontainingfemale-connotedoccupationsvs.thepredictiontreeofmalenames
containingmale-connotedoccupations.
5.2.2 WorkflowDemonstration:ComparativeAnalysis
Thefollowingexemplaryworkflowshowcaseshowourworkspacesupportscomparativeanalysis.
Alinguisticexpertisinterestedinexploringbiasesencodedinthemodel’sparameters.Hethus
createsaprompt“<PH>isgreat.Onecouldevensaythat”asshowninfigure4.Theplaceholder
<PH> W includeswordssuchasJohn,Jayden,andJessica.Thebeamsearchtreerepresents
thetoptwopredictionsforeachstartingsequence.Theexpertthenselectsmultiplewordliststo
highlighttheoccurrencesofwordsrelatedtoappearance,personnames,andoccupations.These
getmarkedinthetreevisualizationthroughiconsattachedtotheparticulartreenodes.TheUpSet
plotsummarizesthewordoccurrencesshowingthatthefemalepersonnameJessicaisrelatedto
theappearancewordbeautiful;thetwomalepersonnamesarementionedasplayersofsports
games(i.e.,player,quarterback),confirmingthestereotypicalgenderbiasesencodedinthelanguage
model[Luetal.2020].Thecasestudyinsection6.1describesmoredetailsontheworkflow.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:16 Spinneretal.
5.3 ModelAdaptation T4
After adapting the beam search tree as part of tasks T2 and T4, or after identifying desired
sequencesaspartoftasks T1 and T3,theusermightwanttofeedthosechangesbackandfine-
tunethemodel,accordingly.Thiscanbedonebyexecutingthere-traintohere( )functionality
fromthenodecontextmenu W .Thistriggersafine-tuningstepofthemodelinthebackend,
usingthebeamsequenceuptotheselectednodeasinput.Thecurrentmodelstatecanbesaved
atanytimeusingthemodelsnapshotsand–trackingwidget W ,enablingtheusertorestore
fine-tunedmodelsfromprevioussessionsordiscardpotentiallyoverfittedmodelsbyreturningto
anearlierstate.
Section 6.3 provides an extensive evaluation of the fine-tuning functionality. We prove the
sufficiencyofonlyafewdatasamples–astheyariseinourapproach–toachieveanoticeable
changeintokenprobabilities.Also,weshowthatoverrepeatedfine-tuningwithdifferentsequences
duringtheanalysissession,domainadaptationisachieved.
6 Evaluation
This section provides a three-fold evaluation of our approach. Starting with a case study on
comparativeanalysis T3 insection6.1,weshowcasehowourtoolisusedtogainin-depthlinguistic
insightsonbiasesencodedinthemodel.Itshowshowourtree-in-the-looptechniquegoesbeyond
thetemplate-basedstate-of-the-artinbiasanalysis.Insection6.2,weprovidetwoqualitativeuser
studieswithsixnon-experts Non andfourcomputationallinguists Lin,showcasingtheusabilityof
ourtoolforguidedtextgeneration T2 andcomparativelinguisticanalyses T3,respectively.Finally,
section6.3presentsadetailedevaluationoftheabilitytofine-tuneLLMs T4 usingtherelatively
smallsamplesizeoftrainingdataarisinginourapproach,showingthatdomainadaptationindeed
ispossibleinthedescribedscenarios.Moreover,inourwork“RevealingtheUnwritten”[Spinner,
Kehlbeck,etal.2023],wepresentadditionallyinsightsintostate-of-the-artlinguisticchallenges,
createdwiththegeneraitorinterface.
6.1 CaseStudy:ComparativeAnalysisonSocialBiases
Inthiscasestudy,alinguisticexpert Lin aimstolearnpatternsrelevanttodesigningbiasevaluation
methods.Sincethebiasevaluationsforgenerativelanguagemodelsaresensitivetothedesign
choicesoftemplateprompts[Alnegheimishetal.2022],theexpert’sgoalistofindoutinteresting
linguisticstructuresthatshouldbetakenintoaccountduringsystematicbiasanalysis.Hethususes
thegenerAItorworkspacetoexploredifferentexamples3 andgeneratenewlinguistichypotheses
(c.f.,inductivelearning[R.J.SternbergandK.Sternberg2016]).
Theexpertbeginstheanalysissessionbyexploringthemodel’spotentialgenderbiases.For
thispurpose,hecreatesaprompt“Afterreceivingtheirdegree,<PH>wantstobecome”wherebythe
<PH> W standsforaplaceholderofdifferentfemaleandmalepersonnames.Thepredictionsfor
JohnandJessicaarelistedintable1.Theexpertcanconfirmfindingsfromrelatedwork[Luetal.
2020]showingthatlanguagemodelstendtolearnstereotypicalgender-professionassociations,
suchasJohnismorelikelytobecomealawyerandJessicaismorelikelytobecomeanurse.Sincethe
explorationinthegenerAItorworkspaceisnotlimitedtoafixed-sizedtemplate,i.e.,thegenerated
tokensequencescanbeofanylength,theexpertobservesthatthestereotypicalassociationsare
followedbytheperson’sdoubtsregardinghisorherchosenprofession(seetable1).Thismotivates
theexperttoexploreanadditionalprompt,i.e.,“Thereason<PH>didnotbecomeadoctorwas”.
Themodel’soutputshowsanewperspectiveofgenderbias,i.e.,themodel’sassumptionsabouta
3WeshowcasetheseexamplesinareducedonlinedemoofgenerAItor,availableunderhttps://demo.tree.generaitor.dbvis.de.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:17
Prompt Prediction
Afterreceivingtheirdegree,Johnwantstobecomealawyer.He’snotsureif
Afterreceivingtheir
he’llbeabletoaffordit.
degree,<PH>wantsto
become Afterreceivingtheirdegree,Jessicawantstobecomeanurse,butshedoesn’t
knowhowtodoit.
ThereasonJohndidnotbecomeadoctorwasbecausehewasamanofGod.
Thereason<PH>didnot
ThereasonJessicadidnotbecomeadoctorwasbecauseshewasafraidofthe
becomeadoctorwas
consequencesofheractions.
Thereason,whyMr.Smithwasafraidtobecomeadoctor,wasbecausehe
Thereason,why<PH>was
wasafraidofbeingaccusedofbeingapedophile.
afraidtobecomeadoctor,
was Thereason,whyMrs.Smithwasafraidtobecomeadoctor,wasbecauseshe
wasafraidofbeingaccusedofwitchcraft.
Table1. ExamplesequencesgeneratedinthecomparativemodeofgenerAItorbyinstancingthe<PH>node.
VaryingbetweenmaleandfemalepersonnamesrevealsastrongsocialbiasinGPT-2’spredictions.
femaleperson’sfears(i.e.,“ThereasonJessicadidnotbecomeadoctorwasbecauseshewasafraidof
theconsequencesofheractions.”).Toinvestigatethisinmoredetail,theexpertdefinesanewprompt
“Thereason,why<PH>wasafraidtobecomeadoctor,was”.Thegeneratedoutputs(seetable1)
confirmthepreviousobservations.Inparticular,themodelpredictsthatamalepersonisafraidto
becomeadoctorbecause“hewasafraidofbeingaccusedofbeingapaedophile”andthefemaleperson
isafraidbecause“shewasafraidofbeingaccusedofwitchcraft.”Theseexamplesmotivatetheexpert
todesignexperimentsforinvestigatingbiasesrelatedtoaperson’sdreams,fears,assumptions,etc.
Theexpertisawarethatthesemanticmeaningofasentencecanbeinfluencedbychanginga
singleword,notonlysemanticallyrichcontentwordsbutalsosemanticallypoorfunctionwords
(e.g.,adverbssuchaseven,orconjunctiveadverbssuchashowever)[CorverandRiemsdijk2001].The
roleoffunctionwordshasalreadybeeninvestigatedformaskedlanguagemodelingtasks[Kalouli
et al. 2022]. The linguistic expert is thus interested in exploring the role of different function
wordsongenerativelanguagemodelpredictionoutcomes.Inparticular,theexpertinvestigatesthe
impactofthefunctionwordsevenandhowever.Evenisanadverbthatisusedtorefertosomething
surprising,unexpected,unusual,orextreme.However,isanadverbtypicallyusedtointroducea
contrastinasentencetoemphasizesomethingthatcontradictsthepreviouslystatedstatement.
The expert first creates a prompt “<PH> is great. One could say that” whereby the <PH> W
stands for a placeholderof different female and male personnames. As shown infigure5, the
modelpredictsthatmalepersonnamesaremorelikelytobecomeplayers ofsportsgamesand
femalepersonnamesaremorelikelytobecomeanactress.Theexpertthenextendsthepromptby
addingtheadverbeven,asshowninfigure4.Althoughmostofthepredictionsstaythesame,the
modelalsocapturesthefunctionalityofthewordevenbypredictingastereotypicalphraseJessica
isgreat.Onecouldevensaythatsheisthemostbeautifulwomanintheworld.Allsentenceshavea
positivesentiment.Thismotivatestheexperttoexplorehowthemodelcapturesthefunctionality
oftheconjunctiveadverbhowever.Hedefinestheprompt“<PH>isgreat.However,onecouldsay
that”andobservesthatthemodelcapturesthefunctionalmeaningofhowever sinceitgenerates
sentencesthatcontradicttheprefix<PH>isgreat.Interestingly,mostofthepredictionshavea
similarcontexttothosesentencesgeneratedwiththepromptwithoutthefunctionwordhowever,
i.e.,themodeltalksaboutplayersofsportsgames.Inmostpredictions,however,themodeluses
thenegationnot inordertogeneratethecontrast.Asshowninfigure6,thisalsoleadstochanges
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:18 Spinneretal.
Fig.5. Theprompt“<PH>isgreat.Onecouldsaythat”generatespredictionsmentioningdifferentprofessions.
inthesentimentofthesentences,i.e.,theychangefrompositivetonegativeones.Thisexample
highlightsthelimitationsoftemplate-basedmethodsforbiasanalysis.Firstly,asingleprompt
generatessentenceswheretheattributeofinterest(e.g.,player,jerk)occursatdifferentpositions
(i.e.,atpositions6and7infigure6).Thisinsightwouldbemissedbyusingstricttemplateswith
fixedattributepositions.Secondly,thisexampleshowsthatsomewords(e.g.,adverbs,negations)
changethesemanticmeaningofthesentence.Simplycountingtheoccurrencesofattributessuch
asaperson’soccupationswithoutconsideringtheoccurrencesofnegationswouldgeneratefalse
resultsabouttheencodedbiases.Theseinsightsmotivatetheexperttodesigntargetedexperiments
forexploringtheroleoffunctionwordsincurrentbiasdetectionmethods.
6.2 EvaluationofUsabilityandUsefulness
Weevaluatetheusabilityofoursysteminaqualitativeuserstudywithsixnon-experts Non andfour
linguisticexperts Lin whowerepreviouslyunfamiliarwiththeworkspace.Thenon-experts Non
arepresentedwiththegenerativemodeoftheworkspace,whilethelinguisticexperts Lin primarily
workwiththecomparativemode.Thestudyaimstoassesswhetherthesystemisintuitivetouse,
ifitissuitabletotacklethetasksidentifiedinsection3.3,andgatherfeedbackforpossiblefuture
use-casesandimprovements.Forthelinguisticexperts Lin,weadditionallyevaluatewhetherthe
workspaceissuitedforthemtogeneratenewhypothesesandobservetheirproblemsofinterest.
6.2.1 Non-ExpertStudy
StudySetup—Aftercapturingtheparticipants’backgroundandpriorexperienceswithlarge
languagemodels,weintroducethemtothegenerativeworkspaceanditsfunctionalities.Wethen
ask them to solve the task described in section 5.1.2 using the workspace in a pair-analytics
session [Arias-Hernandez et al. 2011]. The model loaded in the workspace is the GPT-2 Base
model.Finally,wecollectqualitativeandquantitativefeedbackusingaquestionnaireandasemi-
structuredinterview.Thepair-analyticssessiontook15to25minutes,thewholestudyincluding
theintroduction–andfeedbackquestionnairestook30to45minutesperparticipant.
Results—Allstudyparticipantsagreedthattheworkspacewaseasytouse,anditsdesignwas
acknowledgedasbeingsimpleandtidy.Figure7summarizesthequantitativefeedbackwecollected
inthequestionnaireaftertheexplorationphase.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:19
Fig.6. Theprompt“<PH>isgreat.However,onecouldsaythat”generatespredictionsthatincludethenegation
notandinsultwords.
Regardingoutputexplainability T1,thebeamsearchtreevisualizationhelpedtheparticipants
detectrepetitionsinthegeneratedtextsanddiscardthemquickly.Oneparticipantproposedasemi-
automaticpruningmechanismtoremoverepetitionsfromthetree,actinglikeauser-controlled
𝑛-gramsuppression[Paulusetal.2017].Anotherparticipantnoticedthepredictedtexttosound
rathernegativeandutteredthewishtoobservethesentimentofgeneratedtext.Weimplemented
thisfeedbackbyaddingautomaticsentimentanalysisand–visualizationtothebeamsearchtree,
as shown in figure 1. Concerning the generative task T2, the alternative paths shown in the
beamsearchtree,themanualeditingfunctionality,andtheontologysuggestionsweredescribed
ashelpfultocreatenewideasand“keeptheballrolling.”Whiletheparticipantslikedthatthe
workspace allowed them to generate text in a guided manner, they also critiqued the manual
efforttheyhadtoputintotheprocess.Suggestionstoresolvethisissueincludedgeneratingtext
sentence-wise or making the nodes show whole sentences instead of tokens. When manually
adaptingmodeloutputs T4,oneparticipantdescribedthemodelas“workingagainsthimwhile
steering[theoutputs].”Totacklethisissueandmakedomainadaptationpermanentinthemodel,
weimplementedthefine-tuningfunctionality W ,whichwedidnotintroduceinthestudy
duetotimeconstraints.
6.2.2 ComputationalLinguistStudy
StudySetup—Aftercapturingtheparticipants’background,priorexperienceswithlargelanguage
models,andlinguisticresearchfocus,weintroducethemtothecomparativeworkspaceandits
functionalities.Wethenaskthemtosolvetwotasksusingtheworkspaceinapair-analyticssession,
bothaddressing T3.ThefirsttaskisinvestigatinghowtheRedPajamaInstruct3Bmodel[Computer
2023]handlesnegations.ThesecondtaskistoexaminetheoutputsoftheRedPajamaBase3B
modelforbiases.Wegivetheparticipantsashortintroductiontothemodelanditscapabilitiesfor
eachtask.Wehelpwithexamplepromptsduringthesessionifaparticipantseemsstuck.Thetasks
deliberatelyfocusonanopen-endedexplorationtoenabletheparticipantstoevaluategenerAItor’s
applicabilitytotheirownresearchandtogeneratenewhypotheses.Afterworkingonbothtasks
for10to20minuteseach,wecollectqualitativeandquantitativefeedbackusingaquestionnaire.
Thepair-analyticssessiontook35to55minutes,andthewholestudy,includingtheintroduction–
andfeedbackquestionnaires,took50to70minutesperparticipant.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:20 Spinneretal.
EaseofUse InterfaceSimplicity HumanControl ProblemsofInterest NewHypotheses
(Non+Lin) (Non+Lin) (Non) (Lin) (Lin)
5 8 4 2 2
7
4
6 3
3 5
4 2 1 1
2 3
2 1
1
1
0 0 0 0 0
+ ++ + ++ + ++ + ++ + ++
−− − ◦ −− − ◦ −− − ◦ −− − ◦ −− − ◦
Fig.7. Resultsofthequantitativepartoftheuserstudy.Wecapturedfeedbackfromthenon-experts Non
andthelinguisticexperts Lin ontheusabilityandusefulnessoftheworkspace.
QualitativeResults—Allparticipantsagreedthattheworkspacewasintuitive,asthequantitative
resultsinfigure7show.Allparticipantscouldindependentlyworkonthetasksafterfamiliarizing
themselveswiththeinterfaceforonetotwominutes.
Overall,thebeamsearchtreetoexplainthemodel’soutputswaswellreceived,especiallyhowit
organizesprobabilitiesandalternativeoutputs.Oneparticipantshowedinterestin“thediscrepancy
between probabilities,” identifying high uncertainty where “variation[s] [are] relatively equal
in probability.” Another participant critiqued that if all tokens have a low probability (i.e., the
probabilitydistributionisrelativelyflat),thetop-𝑘 outputsshownintheBSTweremisleadingdue
tootheroutputswithsimilarprobabilitybeingomitted.Asasolution,theyproposedto“show
[...]thedistributionacrossthetop500orwhatever,maybeweightedbyprobability”uponuser
request.Thekeywordhighlightingandsemanticcoloring W wasratedhelpfulto“togetan
overviewjustbylookingatthehighlightedwords.”Theplaceholdernode W wasdescribed
as“veryhelpfulinordertocompareoutputsresultingfromdifferentinputs”andwasintensively
usedbythreeoftheparticipants.Here,oneparticipantwishedtocomparedifferentmodelsina
juxtaposedview.Thewordlists W andtheupsetplot W wereonlyusedrarelybytwoofthe
participantsandignoredbytheothers.
Theexplorativenatureoftheworkspaceshowedstrengthsandweaknesses.Twoparticipants
werehighlyengagedintheexploration,comingupwithnewpromptsandideastotest,whilethe
othertwoparticipantsweremorereservedandneededmoreguidance.
CritiquedwasthetendencyoftheRedPajamamodelstoproducewhitespacesandlinefeedsfor
specificprompts,whichrenderedtheoutputsinthebeamsearchtreeessentiallyuseless.Sincethis
wasamodeldefect,inputsanitizationormanuallyremovingthewhitespacesandlinefeedsfrom
theoutputswastheonlywaytoworkaroundit.However,sincethiswoulddistorttheoutputs,we
decidedagainstimplementingthisfunctionality.
6.3 QuantitativeEvaluationofModelAdaptation
Besidesoutputsteeringthroughselection,manualedits,orautomatedsuggestionsbasedonword
ontologies,oursystemsupportsmodelfine-tuningbasedonthealteredoutputswiththegoalof
adaptingthemodeltothehuman’sstyleofwritingandtospecificdomains.Weevaluatetheeffects
offine-tuningonalocallevel,observingthechangestotheindividualtokensbeingfine-tuned
on,andonagloballevel,assessingdomainadaptationbycheckinghowthemodelreactstoatest
fractionofthedatasetthemodelwasfine-tunedon.generAItorsfine-tuningfunctionality(c.f.,
W )andthefollowingexperimentsusetheAdamW[LoshchilovandHutter2017]optimizer
withalearningrateof5×10−5.TheexperimentsareperformedwiththeGPT-2Basemodel.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:21
Seqence Initial 1Step 2Steps
𝑝 0.000012 0.000181 0.010252
Afteryou’vewatchedthismovieyou’llbedeaf
𝑖 1964 466 13
𝑝 0.001175 0.002569 0.009681
Behindthetreeshadhiddenagiantgnome
𝑖 143 58 10
𝑝 0.046493 0.260536 0.828726
Theamericanbullfrogisthelargestanimal
𝑖 4 1 1
Table2. Targettokenprobability𝑝andindexposition𝑖afterfine-tuningondifferentsequencesforoneand
twosteps,respectively.Theresultsshowthatfine-tuningforonetotwostepsalreadyachievesasignificant
increaseintheprobabilityofthetargettoken.
LocalAdaptation—Afterfine-tuningtoaspecifictreenode,thenode’sprobabilityfollowing
theprevioussequenceshouldincrease.Toevaluatethiseffectinrelationtothenumberoffine-
tuning passes, we iteratively re-train with the same sequence and measure the top-5 output
tokenprobabilitiesaftereachstep.Figure8ashowsthechangeintokenprobabilitiesafterfine-
tuningfortwo–andfourstepsonthesequence“Afteryou’vewatchedthismovie,you’llbedeaf”,
where“deaf” isthetargettokenmanuallyinsertedbytheuser.Initially,ithasaprobabilityof
𝑝 (deaf) =0.000012whichincreasesto𝑝 (deaf) =0.000834aftertwoand𝑝 (deaf) =0.315274after
0 2 4
foursteps,correspondingtotheindexpositions𝑖 (deaf) =1964,𝑖 (deaf) =158,and𝑖 (deaf) =1.
0 2 4
Otherexamplesshowsimilarresults,asdepictedintable2.Weobservethatfine-tuningforoneto
twostepsismostlysufficienttoachieveasignificantincreaseintheprobabilityofthetargettoken.
Thegreatertheinitialprobabilityofatokenoccurringinthetargetcontext,thegreatertheriskof
overfitting.However,wedidnotobservethemodellosingitsabilitytogeneralizetoothercontexts
despiteourexperiments’strongfocusonthetargettoken.Itshouldbenotedthatwecanalready
perceiveeffectsofglobaladaptationinfigure8a:thesemanticcontextoftheinputsentencemakes
theword“hooked” fitbetterthantheword“able”,leadingtoashiftoftheirprobabilities.
GlobalAdaptation—Thenumberoftrainingsamplesgeneratedusingourworkspacewilllikely
stayfarbehindthenumberofsamplesindatasetstypicallyusedtofine-tunemodels,suchasthe
IMDB [Maas et al. 2011] (≈ 50𝑘 samples) or MultiNLI (≈ 433𝑘 samples) datasets. Thus, in the
following,weevaluatethemodel’scapabilitytolearndomainspecificknowledgefroma(small)set
oftrainingsamples.Here,weusetheIMDBdatasetforbinarysentimentclassificationofmovie
reviews.OurgoalistoperformparametersensitivityanalysisontheGPT-2Basemodel,i.e.,evaluate
howthemodeladaptstodataset-specifictargettokensafterfine-tuningforavaryingnumberof
steps.Weusetheperplexityevaluationmetric[Jelineketal.1977]tomeasuredomainadaption.To
seetheeffectofthesamplesizeonthemodel’sperformance,wefirstsplitthedatasetintotraining
andtestsubsets(50%,i.e.,25.000datapointseach).Werepeatedlyfine-tunethemodelfromscratch
for100runs,whereweincreasethenumberoftrainingsamples𝑛by20ineachrun.Thismeanswe
fine-tunethebasemodelfor𝑛 = {20,40,...,2000}stepswhilemeasuringtheperplexityonboth
the𝑛trainingsamplesandthefulltestsubsetforeachfine-tunedmodelversion.Thisallowsusto
verifythemodel’scapabilitytolearndomain-specificpropertiesfromthedatapointsthatithas
seenduringthefine-tuning,aswellasitsgeneralizabilitytounseensamples.Figure8bshowsthe
differencebetweentheperplexityofthetrainingandtestdata.Wecanseethatthemodeladapts
towardsthetrainingsamples;theperplexityinmostcasesstaysintherangebetween25and30.
Theperplexityofthetestdataishigherandstaysintherangebetween40and45.Nevertheless,
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:22 Spinneretal.
45
40
35 T Tera si tn
30
25
0 250 500 750 1000 1250 1500 1750 2000
# Training Samples
(a)Measuringthemodel’slocaladaptationtothetarget (b)Measuringthemodel’sglobaladaptation
token“deaf”after0,2,and4stepsoffine-tuning. totheIMDBMovieReviewsdataset.
Fig.8. Wemeasurehowthemodeladaptstoaspecifictargettoken(a)andaspecificdomain(b)after
fine-tuningforavaryingnumberofsteps,showingthatadaptationispossiblealreadywithasmallnumber
oftrainingsamplesastheyoccurinourtargetusecases.
wecanalsoseeageneraltrend,wheretheperplexityofboththetestandtrainingdatadecreases
withtheincreasedsizeofthetrainingsample,andthemodelisabletoadapttothegivendomain
alreadywithafewhundredsoftrainingdatapoints.
7 Discussion
In the following, we discuss our rationales for the presented approach, summarize the most
importanttake-homemessages,anddiscusscurrentlimitationsandfutureresearchopportunities.
7.1 RationalesofOurBST-BasedApproachandTake-HomeMessages
LeveragingtheInherentUnderstandingofTextToExplainLLMs—Thewayalanguage
modelgenerateslanguageisoftenmisinterpretedbyusers,leadingtofalserationalizationsoftheir
outputsbyattributinganunderstandingofthetext’smeaningtothemodel[Sevastjanovaand
El-Assady2022].Therefore,explainabilityoflanguagemodeloutputsiscrucialtocorrectlyassess
themodel’scapabilitiesandidentifyundesiredfeaturesinthegeneratedtext,suchasrepetitionsor
biases.Incontrasttootherdeeplearningarchitectures,thein-andoutputsofLLMsaretext,which
isinherentlyunderstandablebyhumans.Thisaccessibilityofthemodel’sin–andoutputsmakesit
agoodcandidateforexplainingitsbehavior.
ExposingtheBeamSearchTreetoExplainDecisionProcesses—Beamsearchbeingthemost
commonalgorithmtosampletextfromtheLLM’spredictions,combinedwiththeeasyunderstand-
abilityoftheresultingtreetonon-experts,makesitanaturalchoicetoexposethebeamsearchtree
toexplainthemodel’sdecisionprocess.SincetheBSTisadirectrepresentationoftheunderlying
searchalgorithm,itneitherneglectsimportantinformationnorinducesfalserationalization.It
is,therefore,avaluabletoolforexplainingthemodel’sbehaviorandcommunicatinginformation
inthemodel’soutputtotheuser,suchasuncertainties,alternatives,orpatterns,e.g.,repeating
content.
Tree Augmentations — Issues with the BST’s complexity and information overload can be
addressed by providing additional visualizations, interactions, and analysis tools. Simple tree
transformations,suchasthetreecollapseand–filterfunctionalities,allowresolvingscalability
issues with large trees. Semantic keyword coloring, keyword lists, and the Upset plot provide
aggregatedinformation,providingahigh-leveloverview.Themulti-treeviewallowscomparing
treesbyjuxtapositionandisparticularlyusefulforthelinguisticanalysisofnuancesintheoutputs.
Finally,theontologyVoronoitreemapandtheontologyreplacefunctionalitycombinethekeywords
withontologicalknowledgethemodelcannotdeliver.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.
ytixelpreP-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:23
ProvidingAugmentationsthroughModularWidgets—Differenttoolsandaugmentations
are relevant depending on the tasks a user wants to solve. As opposed to a dashboard-based
approach,whereallvisualcomponentsaredisplayedsimultaneously,modularwidgetsallowfor
moreflexibleuseoftheavailable(screen)spaceandthereuseofsimilarvisualvariables.This,in
return,requirescarefulcategorizationoftheavailablewidgetsandusefulpresetsforeachtaskso
thatvisualvariables(e.g.,colororshape)areusedonlyoncebysimultaneouslyactivewidgetsto
avoidconfusion.
UsefulnessforNon-TechnicalUsersandLinguisticExperts—Asourevaluationshows,the
aforementionedmechanismsenablepowerfulmodesofLLMoutputanalysis.Non-technicalusers
canusetheBSTtounderstandthemodel’sdecisionprocessandforinformedtextgeneration.Com-
putationallinguistscanusetheBSTinanexplorativewaytogeneratenewinsightsandhypotheses,
asopposedtothetraditionaltemplate-basedorstatisticalanalysisofexistinghypotheses.
7.2 LimitationsandFutureWork
ApplicabilitytoState-of-the-ArtModels—Inthiswork,wedemonstrateourapproachusing
GPT2andBloom.Beyondthat,Spinner,Kehlbeck,etal.[2023]showhowgenerAItorcanbeused
togeneratemeaningfullinguisticinsightsfordifferentmodels,includingGPT2,Bloom,RedPajama
Base, and RedPajama Instruct [Computer 2023]. We observe that our approach becomes more
potentwithlargermodelsastheoutputdiversityincreasesandthealternativesintheBSTbecome
more meaningful. In general, our approach applies to causal language transformers if they (1)
provideaccesstothehigh-dimensionaltoken-wiseembeddingsand(2)outputtheprobabilitiesof
thenexttop-𝑘 tokens.WhilethesecondrequirementisimperativetogeneratetheBST,thefirst
requirementisonlyneededfortheembedding-basedwidgets.
ThismeansthatlargepartsofourapproacharetransferabletoGPT4asthecurrentstate-of-
the-artincausallanguagemodeling.TheOpenAIAPIprovidesaccesstothelogprobsofthetop-𝑘
tokens,whichcanbeusedtogeneratetheBST.Despitethehigh-dimensionalembeddingsnotbeing
availableforGPT4,theembeddingwidgetscanstillbepoweredfromtheembeddingsproducedby
othertransformers.Sevastjanova,Kalouli,etal.[2022]andKehlbecketal.[2021]havestudiedthe
embeddingspacesofprominenttransformers,suggestingthatusingthetokenembeddingsofother
modelsmightevenbebeneficialforsemantictokenanalysis.
TransferofOurProposedTechniquestoExistingInterfaces—Ourapproachtargetsspecific
usergroups.However,weenvisionsomemeansofexplainabilityembeddedintotheprominent
chat–andcompletion-basedinterfaces,likeChatGPTorGitHubCopilot4.Currently,ChatGPTonly
outputstext,andeachadaptationhastobetriggeredbyrefiningthepromptinthehopethatthe
desiredoutputwillbegenerated.Thiscanbefrustrating,especiallyforhallucinatedtextparts,
wherenoeasysolutionforeditingisavailable.Here,showingalternativeoutputsandprovidingthe
userwithexplainabilityonthelikelinessofsequencescouldbringhugeadvantages.WhileGitHub
Copilotdoesshowalternatives,thosealternativesremainunexplained.Here,showingprobabilities
orannotatingstructuralelements,c.f.,keywordextraction(section4.1)and–coloring W ,could
furtherimprovetheusefulness.
Bridging Between Explorative and Statistical Analysis — Our approach is explorative in
nature,allowinguserstogeneratenewhypothesesandinsights.However,asnotedbyoneofour
computationallinguistparticipants,acombinationwithstatisticalanalysiswouldbebeneficialto
validatethegeneratedhypotheses.Therefore,weenvisionatighterintegrationofourapproach
withstatisticalanalysistools,e.g.,tovalidatethegeneratedhypotheseswithstatisticaltests.Once
thisintegrationisestablished,annotatingtheBSTbrancheswithstatisticalmetricscouldbridgethe
4https://github.com/features/copilot
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:24 Spinneretal.
gapbetweenexplorativeandstatisticalanalysis.Forthecurrentversionofthesystem,wedecided
againstannotatingthebrancheswithlinguisticmetricstopreventtheuserfromdrawingfalse
generalizationsfromlocalobservations.
SupportforModelDevelopers—Ourinterfacealsoprovidesinformationrelevanttomodel
developers.However,formodeldebuggingandrefinement,additionaltools,e.g.,toobservethe
effectsoffine-tuningorinvestigatecommonerrorsinmodelanddata,mightbeneeded.
ExtensiontoOtherTasksandUserGroups—Thepresentedwidgetsarewell-roundedforthe
describedtasksandtargetusergroups.However,throughanextensionwithadditionalwidgets,
othertaskscanbeaddressed,e.g.,informedtextsummarizationforstudents.
ComparisonAcrossModels—Whileourapproachallowsloadingdifferentgenerativelanguage
transformers,comparativeanalysisisyetonlypossiblebetweenprompts.However,thisisnota
limitationofourproposedtree-in-the-loopapproachandwillbeimplementedinfutureiterations
ofthesystem,enablingadditionalmodesofanalysis.
8 Conclusion
Wepresentthetree-in-the-loopparadigm,puttingthebeamsearchtreeinthecenterofthegenerAI-
torVisualAnalyticstechniqueforlanguagemodelexplainability,comparability,andadaptability.In
ourtechnique,weleveragethebeamsearchtreetoexplainthemodel’sdecisionprocess,compare
modeloutputs,andadapttheoutputstouserpreferences.Enhancingthetreewithtask-specific
widgetscreatessynergiesbetweenthetreeandtargetedvisualizations,interactions,andin-situ
explanations. Finally, we provide a three-fold evaluation of our approach. First, we assess the
applicabilityofourapproachinacasestudy,showcasingourtechnique’scomparativecapabilities.
Particularly,weshowhowtheinterplaybetweenthebeamsearchtreeandwidgetsenablesnew
analysismodes,leadingtointerestinglinguisticinsightsonmodelbiases.Second,weperformtwo
qualitativeuserstudies,thefirstwithsixnon-expertsandthesecondwithfourcomputational
linguists,provingtheusabilityofourapproachfortextgenerationtasksandlinguisticanalyses.
Finally,wequantitativelyevaluatetheabilitytoadaptthemodeltouserpreferenceswithrelatively
fewtrainingsamplesastheyariseinourapproach.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:25
References
D.Alba.2022.“OpenAIChatbotSpitsOutBiasedMusings,DespiteGuardrails.”Bloomberg.RetrievedMar.30,2023from
https://www.bloomberg.com/news/newsletters/2022-12-08/chatgpt-open-ai-s-chatbot-is-spitting-out-biased-sexist-r
esults.
S.Alnegheimish,A.Guo,andY.Sun.2022.“UsingNaturalSentencePromptsforUnderstandingBiasesinLanguageModels.”
In:ProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human
LanguageTechnologies.AssociationforComputationalLinguistics,Seattle,UnitedStates,2824–2830.
R.Arias-Hernandez,L.T.Kaastra,T.M.Green,andB.Fisher.2011.“PairAnalytics:CapturingReasoningProcessesin
CollaborativeVisualAnalytics.”In:HawaiiInternationalConferenceonSystemSciences.IEEE.
M.El-Assady,W.Jentner,R.Kehlbeck,U.Schlegel,R.Sevastjanova,F.Sperrle,T.Spinner,andD.Keim.2019.“TowardsXAI:
StructuringtheProcessesofExplanations.”In:ACMCHI2019Workshop:Human–CenteredMachineLearningPerspectives.
M.El-Assady,R.Kehlbeck,Y.Metz,U.Schlegel,R.Sevastjanova,F.Sperrle,andT.Spinner.2022.“SemanticColorMapping:
APipelineforAssigningMeaningfulColorstoText.”4thIEEEWorkshoponVisualizationGuidelinesinResearch,Design,
andEducation.
M.El-Assady,R.Sevastjanova,D.Keim,andC.Collins.2018.“ThreadReconstructor:ModelingReply-ChainstoUntangle
ConversationalTextthroughVisualAnalytics.”ComputerGraphicsForum,37,3,351–365.
D.Bahdanau,K.Cho,andY.Bengio.2014.“NeuralMachineTranslationbyJointlyLearningtoAlignandTranslate.”arXiv:
1409.0473.
Y.Bengio,R.Ducharme,andP.Vincent.2000.“Aneuralprobabilisticlanguagemodel.”AdvancesinNeuralInformation
ProcessingSystems,13.
S.L.Blodgett,S.Barocas,H.DauméIII,andH.Wallach.2020.“Language(Technology)isPower:ACriticalSurveyof“Bias”
inNLP.”In:ProceedingsoftheAssociationforComputationalLinguistics.AssociationforComputationalLinguistics,
Online,5454–5476.
T.B.Brownetal..2020.“LanguageModelsareFew-ShotLearners.”arXiv:2005.14165.
A.Caliskan,J.J.Bryson,andA.Narayanan.2017.“Semanticsderivedautomaticallyfromlanguagecorporacontainhuman-
likebiases.”Science,356,6334,183–186.
J.Camacho-ColladosandR.Navigli.2017.“BabelDomains:Large-ScaleDomainLabelingofLexicalResources.”In:Proceedings
ofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics:ShortPapers.Vol.2.
AssociationforComputationalLinguistics.
R.Campos,V.Mangaravite,A.Pasquali,A.Jorge,C.Nunes,andA.Jatowt.2020.“YAKE!Keywordextractionfromsingle
documentsusingmultiplelocalfeatures.”InformationSciences,509,257–289.
C.Chen,K.Lin,andD.Klein.2021.“ConstructingTaxonomiesfromPretrainedLanguageModels.”In:Proceedingsofthe
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.
AssociationforComputationalLinguistics.
T.Computer.2023.RedPajama:AnOpenSourceRecipetoReproduceLLaMAtrainingdataset.https://github.com/togethercom
puter/RedPajama-Data.(2023).
S.ConiaandR.Navigli.2020.“Conception:Multilingually-Enhanced,Human-ReadableConceptVectorRepresentations.”In:
Proceedingsofthe28thInternationalConferenceonComputationalLinguistics.InternationalCommitteeonComputational
Linguistics.
N.CorverandH.vanRiemsdijk.2001.Semi-lexicalCategories:TheFunctionofContentWordsandtheContentofFunction
Words.DeGruyterMouton,Berlin,NewYork.
M.Danilevsky,K.Qian,R.Aharonov,Y.Katsis,B.Kawas,andP.Sen.2020.“ASurveyoftheStateofExplainableAIfor
NaturalLanguageProcessing.”In:Proceedingsofthe1stConferenceoftheAsia-PacificChapteroftheAssociationfor
ComputationalLinguisticsandthe10thInternationalJointConferenceonNaturalLanguageProcessing.Associationfor
ComputationalLinguistics,Suzhou,China,447–459.
S.Dathathri,A.Madotto,J.Lan,J.Hung,E.Frank,P.Molino,J.Yosinski,andR.Liu.2019.“PlugandPlayLanguageModels:
ASimpleApproachtoControlledTextGeneration.”https://arxiv.org/abs/1912.02164.
DeepNLP.2023.BiasinNLP.[Online;accessed15.Nov.2023].(2023).RetrievedNov.15,2023fromhttps://github.com/cisnl
p/bias-in-nlp.
J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova.2018.“BERT:Pre-trainingofDeepBidirectionalTransformersforLanguage
Understanding.”arXiv:1810.04805.
W.Du,Z.M.Kim,V.Raheja,D.Kumar,andD.Kang.2022.“Read,Revise,Repeat:ASystemDemonstrationforHuman-in-
the-loopIterativeTextRevision.”In:ProceedingsoftheFirstWorkshoponIntelligentandInteractiveWritingAssistants.
AssociationforComputationalLinguistics.
K.Ethayarajh.2019.“HowContextualareContextualizedWordRepresentations?ComparingtheGeometryofBERT,ELMo,
andGPT-2Embeddings.”In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProceedingsandthe
InternationalJointConferenceonNaturalLanguageProcessing.ACL,HongKong,China,55–65.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:26 Spinneretal.
I.Garrido-Muñoz,A.Montejo-Ráez,F.Martínez-Santiago,andL.A.Ureña-López.2021.“AsurveyonbiasindeepNLP.”
AppliedSciences,11,7,3184.
A.GattandE.Krahmer.2018.“SurveyoftheStateoftheArtinNaturalLanguageGeneration:Coretasks,applicationsand
evaluation.”JournalofArtificialIntelligenceResearch,61,65–170.
S.Gehrmann,H.Strobelt,R.Kruger,H.Pfister,andA.M.Rush.2019.“VisualInteractionwithDeepLearningModels
throughCollaborativeSemanticInference.”IEEETransactionsonVisualizationandComputerGraphics,1–1.
J.Hartmann,M.Heitmann,C.Schamp,andO.Netzer.2021.“ThePowerofBrandSelfies.”JournalofMarketingResearch.
X.He.2021.“ParallelRefinementsforLexicallyConstrainedTextGenerationwithBART.”In:ProceedingsoftheConference
onEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics.
J.HowardandS.Ruder.2018.“UniversalLanguageModelFine-tuningforTextClassification.”In:Proceedingsofthe56th
AnnualMeetingoftheAssociationforComputationalLinguistics.Vol.1.AssociationforComputationalLinguistics,
Melbourne,Australia,328–339.
Z.Hu,Z.Yang,X.Liang,R.Salakhutdinov,andE.P.Xing.2017.“TowardControlledGenerationofText.”In:Proceedingsof
the34thInternationalConferenceonMachineLearning(ProceedingsofMachineLearningResearch).Ed.byD.Precup
andY.W.Teh.Vol.70.PMLR,1587–1596.
X.HuaandL.Wang.2020.“PAIR:PlanningandIterativeRefinementinPre-trainedTransformersforLongTextGenera-
tion.”In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Associationfor
ComputationalLinguistics.
J.Huang,Y.Xie,Y.Meng,Y.Zhang,andJ.Han.2020.“CoRel:Seed-GuidedTopicalTaxonomyConstructionbyConcept
LearningandRelationTransferring.”In:Proceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledge
DiscoveryandDataMining.ACM.
F.Jelinek,R.L.Mercer,L.R.Bahl,andJ.K.Baker.1977.“Perplexity—ameasureofthedifficultyofspeechrecognitiontasks.”
TheJournaloftheAcousticalSocietyofAmerica,62,S1,S63–S63.
Z.Ji,N.Lee,R.Frieske,T.Yu,D.Su,Y.Xu,E.Ishii,Y.J.Bang,A.Madotto,andP.Fung.2023.“SurveyofHallucinationin
NaturalLanguageGeneration.”ACMComputingSurveys,55,12,1–38.
M.Jiang,X.Song,J.Zhang,andJ.Han.2022.“TaxoEnrich:Self-SupervisedTaxonomyCompletionviaStructure-Semantic
Representations.”In:ProceedingsoftheACMWebConference.ACM.
J.Johnson,M.Douze,andH.Jégou.2019.“Billion-scalesimilaritysearchwithGPUs.”IEEETransactionsonBigData,7,3,
535–547.
A.-L.Kalouli,R.Sevastjanova,C.Beck,andM.Romero.2022.“Negation,Coordination,andQuantifiersinContextualized
LanguageModels.”In:Proceedingsofthe29thInternationalConferenceonComp.Ling.InternationalCommitteeon
ComputationalLinguistics,Gyeongju,RepublicofKorea,3074–3085.
R.Kehlbeck,R.Sevastjanova,T.Spinner,T.Stähle,andM.El-Assady.2021.“DemystifyingtheEmbeddingSpaceofLanguage
Models.”ProceedingsoftheWorkshoponVisualizationforAIExplainability(VISxAI).https://bert-vs-gpt2.dbvis.de/.
A.Lauscher,T.Lueken,andG.Glavaš.2021.“SustainableModularDebiasingofLanguageModels.”In:Findingsofthe
AssociationforComputationalLinguistics:EMNLP.AssociationforComputationalLinguistics,PuntaCana,Dominican
Republic,4782–4797.
Y.LeCun.2023.“DoLanguageModelsNeedSensoryGroundingforMeaningandUnderstanding?”ThePhilosophyofDeep
Learning.(2023).https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi.
J.Lee,J.-H.Shin,andJ.-S.Kim.2017.“InteractiveVisualizationandManipulationofAttention-basedNeuralMachine
Translation.”In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations.
AssociationforComputationalLinguistics,Copenhagen,Denmark,121–126.
M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy,V.Stoyanov,andL.Zettlemoyer.2020.“BART:Denoising
Sequence-to-SequencePre-trainingforNaturalLanguageGeneration,Translation,andComprehension.”In:Proceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics.AssociationforComputationalLinguistics,
Online,7871–7880.
A.Lex,N.Gehlenborg,H.Strobelt,R.Vuillemot,andH.Pfister.2014.“UpSet:VisualizationofIntersectingSets.”IEEE
TransactionsonVisualizationandComputerGraphics,20,12,1983–1992.
J.Li,T.Tang,W.X.Zhao,andJ.-R.Wen.2021.“PretrainedLanguageModelforTextGeneration:ASurvey.”In:Proceedings
ofthe30thInternationalJointConferenceonArtificialIntelligence.InternationalJointConferenceonArtificialIntelligence
Organization.
Z.Li,Y.Wang,X.Yan,W.Meng,Y.Li,andJ.Yang.2022.“TaxoTrans.”In:Proceedingsofthe28thACMSIGKDDConferenceon
KnowledgeDiscoveryandDataMining.ACM.
P.P.Liang,C.Wu,L.-P.Morency,andR.Salakhutdinov.2021.“Towardsunderstandingandmitigatingsocialbiasesin
languagemodels.”In:InternationalConferenceonMachineLearning.PMLR,6565–6576.
I.LoshchilovandF.Hutter.2017.“FixingWeightDecayRegularizationinAdam.”CoRR,abs/1711.05101.http://arxiv.org/abs
/1711.05101 arXiv:1711.05101.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:27
K.Lu,P.Mardziel,F.Wu,P.Amancharla,andA.Datta.2020.“Genderbiasinneuralnaturallanguageprocessing.”Logic,
Language,andSecurity:EssaysDedicatedtoAndreScedrovontheOccasionofHis65thBirthday,189–202.
A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y.Ng,andC.Potts.2011.“LearningWordVectorsforSentimentAnalysis.”
In:Proceedingsofthe49thAnnualMeetingoftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.
AssociationforComputationalLinguistics,Portland,Oregon,USA,142–150.
L.McInnes,J.Healy,N.Saul,andL.Grossberger.2018.“UMAP:UniformManifoldApproximationandProjection.”The
JournalofOpenSourceSoftware,3,29,861.
N.Mehrabi,F.Morstatter,N.Saxena,K.Lerman,andA.Galstyan.2021.“Asurveyonbiasandfairnessinmachinelearning.”
ACMComputingSurveys,54,6,1–35.
C.Metz.2022.“TheNewChatbotsCouldChangetheWorld.CanYouTrustThem?”NewYorkTimes.https://www.nytimes.c
om/2022/12/10/technology/ai-chat-bot-chatgpt.html.
S.Mishra,D.Khashabi,C.Baral,andH.Hajishirzi.2022.“Cross-TaskGeneralizationviaNaturalLanguageCrowdsourcing
Instructions.”In:Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics.Associationfor
ComputationalLinguistics.
A.Moro,A.Raganato,andR.Navigli.2014.“EntityLinkingmeetsWordSenseDisambiguation:aUnifiedApproach.”
TransactionsoftheAssociationforComputationalLinguistics,2,231–244.
M.Nadeem,A.Bethke,andS.Reddy.2021.“StereoSet:Measuringstereotypicalbiasinpretrainedlanguagemodels.”In:
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJoint
ConferenceonNaturalLanguageProcessing.Vol.1.AssociationforComputationalLinguistics,Online,5356–5371.
R.NavigliandS.P.Ponzetto.2012.“BabelNet:Theautomaticconstruction,evaluationandapplicationofawide-coverage
multilingualsemanticnetwork.”ArtificialIntelligence,193,217–250.
OpenAI.2023.GPT-4TechnicalReport.(2023).arXiv:2303.08774.
L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,K.Slama,A.Ray,J.Schulman,
J.Hilton,F.Kelton,L.Miller,M.Simens,A.Askell,P.Welinder,P.F.Christiano,J.Leike,andR.Lowe.2022.“Training
languagemodelstofollowinstructionswithhumanfeedback.”In:AdvancesinNeuralInformationProcessingSystems.
Ed.byS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh.Vol.35.CurranAssociates,Inc.,27730–27744.
V.PadmakumarandH.He.2022.“Machine-in-the-LoopRewritingforCreativeImageCaptioning.”In:Proceedingsofthe
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.
AssociationforComputationalLinguistics.
R.Paulus,C.Xiong,andR.Socher.2017.“ADeepReinforcedModelforAbstractiveSummarization.”CoRR,abs/1705.04304.
http://arxiv.org/abs/1705.04304 arXiv:1705.04304.
P.vonPlaten.2020.Howtogeneratetext:usingdifferentdecodingmethodsforlanguagegenerationwithTransformers.[Online;
accessed29.Mar.2023].(2020).RetrievedMar.29,2023fromhttps://huggingface.co/blog/how-to-generate.
L.Qin,V.Shwartz,P.West,C.Bhagavatula,J.D.Hwang,R.L.Bras,A.Bosselut,andY.Choi.2020.“BacktotheFuture:
UnsupervisedBackprop-basedDecodingforCounterfactualandAbductiveCommonsenseReasoning.”In:Proceedingsof
theConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).AssociationforComputationalLinguistics.
A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever.2019.“LanguageModelsareUnsupervisedMultitask
Learners.”
A.Radford,J.Wu,D.Amodei,D.Amodei,J.Clark,M.Brundage,andI.Sutskever.2019.BetterLanguageModelsandTheir
Implications.https://openai.com/blog/better-language-models/.[Online;accessed18-March-2021].(2019).
E.Reif,A.Yuan,M.Wattenberg,F.B.Viegas,A.Coenen,A.Pearce,andB.Kim.2019.“VisualizingandMeasuringtheGeometry
ofBERT.”In:AdvancesinNeuralInformationProcessingSystems.Ed.byH.Wallach,H.Larochelle,A.Beygelzimer,
F.d’Alché-Buc,E.Fox,andR.Garnett.CurranAssociates,Inc.,8594–8603.
A.Rogers,O.Kovaleva,andA.Rumshisky.2020.“APrimerinBERTology:WhatWeKnowAboutHowBERTWorks.”
TransactionsoftheAssociationforComputationalLinguistics,8,842–866.
K.Roose.2023.“HowChatbotsandLargeLanguageModels,orLLMs,ActuallyWork.”NewYorkTimes.RetrievedNov.3,
2023fromhttps://www.nytimes.com/2023/03/28/technology/ai-chatbots-chatgpt-bing-bard-llm.html.
D.E.Rumelhart,G.E.Hinton,andR.J.Williams.1986.“Learningrepresentationsbyback-propagatingerrors.”CahiersDe
LaRevueDeTheologieEtDePhilosophie,323,6088,533–536.
T.L.Scaoetal..2023.BLOOM:A176B-ParameterOpen-AccessMultilingualLanguageModel.(2023).arXiv:2211.05100.
B.Scarlini,T.Pasini,andR.Navigli.2020.“WithMoreContextsComesBetterPerformance:ContextualizedSenseEmbeddings
forAll-RoundWordSenseDisambiguation.”In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing.AssociationforComputationalLinguistics.
R.SevastjanovaandM.El-Assady.2022.“BewaretheRationalizationTrap!WhenLanguageModelExplainabilityDiverges
fromourMentalModelsofLanguage.”Conference:CommunicationinHuman-AIInteractionWorkshopatIJCAI-ECAI,
abs/2207.06897.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:28 Spinneretal.
R.Sevastjanova,A.-L.Kalouli,C.Beck,H.Hauptmann,andM.El-Assady.2022.“LMFingerprints:VisualExplanations
ofLanguageModelEmbeddingSpacesthroughLayerwiseContextualizationScores.”ComputerGraphicsForum,41,3,
295–307.
T.Spinner,R.Kehlbeck,R.Sevastjanova,T.Stähle,D.A.Keim,O.Deussen,A.Spitz,andM.El-Assady.2023.Revealing
theUnwritten:VisualInvestigationofBeamSearchTreestoAddressLanguageModelPromptingChallenges.(2023).arXiv:
2310.11252.
T.Spinner,U.Schlegel,H.Schafer,andM.El-Assady.2020.“explAIner:AVisualAnalyticsFrameworkforInteractiveand
ExplainableMachineLearning.”IEEETransactionsonVisualizationandComputerGraphics,26,1.
M.Steiger,J.Bernard,S.Thum,S.Mittelstädt,M.Hutter,D.A.Keim,andJ.Kohlhammer.2015.“Explorativeanalysisof2D
colormaps.”In:WSCG.
R.J.SternbergandK.Sternberg.2016.Cognitivepsychology.NelsonEducation.
H.Strobelt,S.Gehrmann,M.Behrisch,A.Perer,H.Pfister,andA.M.Rush.2018.“Seq2Seq-Vis:Avisualdebuggingtoolfor
sequence-to-sequencemodels.”IEEETransactionsonVisualizationandComputerGraphics,25,1,353–363.
H.Strobelt,J.Kinley,R.Krueger,J.Beyer,H.Pfister,andA.M.Rush.2022.“GenNI:Human-AICollaborationforData-Backed
TextGeneration.”IEEETransactionsonVisualizationandComputerGraphics,28,1,1106–1116.
Y.Tan,C.Yang,X.Wei,C.Chen,L.Li,andX.Zheng.2022.“EnhancingRecommendationwithAutomatedTagTaxonomy
ConstructioninHyperbolicSpace.”In:IEEE38thInternationalConferenceonDataEngineering(ICDE).IEEE.
A.J.Teuling,R.Stöckli,andS.I.Seneviratne.2010.“Bivariatecolourmapsforvisualizingclimatedata.”InternationalJournal
ofClimatology,31,9,1408–1412.
A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,andI.Polosukhin.2017.“AttentionIsAll
YouNeed.”arXiv:1706.03762.
G.Wiedemann,S.Remus,A.Chawla,andC.Biemann.2019.“DoesBERTMakeAnySense?InterpretableWordSense
DisambiguationwithContextualizedEmbeddings.”In:ProceedingsofKONVENS.Erlangen,Germany.
A.Williams,N.Nangia,andS.Bowman.2018.“ABroad-CoverageChallengeCorpusforSentenceUnderstandingthrough
Inference.”In:ProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies.Vol.1.AssociationforComputationalLinguistics,NewOrleans,Louisiana,1112–1122.
T.Wolfetal..2020.“Transformers:State-of-the-ArtNaturalLanguageProcessing.”In:ProceedingsoftheConferenceon
EmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations.AssociationforComputationalLinguistics,
Online,38–45.
Y.Xiang,Z.Zhang,J.Chen,X.Chen,Z.Lin,andY.Zheng.2021.“OntoEA:Ontology-guidedEntityAlignmentviaJoint
KnowledgeGraphEmbedding.”In:FindingsoftheAssociationforComputationalLinguistics:ACL-IJCNLP.Associationfor
ComputationalLinguistics.
H.Xu,Y.Chen,Z.Liu,Y.Wen,andX.Yuan.2022.“TaxoPrompt:APrompt-basedGenerationMethodwithTaxonomic
ContextforSelf-SupervisedTaxonomyExpansion.”In:Proceedingsofthe31stInternationalJointConferenceonArtificial
Intelligence.InternationalJointConferenceonArtificialIntelligenceOrganization.
W.Yu,C.Zhu,Z.Li,Z.Hu,Q.Wang,H.Ji,andM.Jiang.2022.“ASurveyofKnowledge-enhancedTextGeneration.”ACM
ComputingSurveys,54,11s,1–38.
A.Yuan,A.Coenen,E.Reif,andD.Ippolito.2022.“Wordcraft:StoryWritingWithLargeLanguageModels.”In:27th
InternationalConferenceonIntelligentUserInterfaces.ACM.
C.Zhang,F.Tao,X.Chen,J.Shen,M.Jiang,B.Sadler,M.Vanni,andJ.Han.2018.“TaxoGen.”In:Proceedingsofthe24th
ACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining.ACM.
H.Zhang,H.Song,S.Li,M.Zhou,andD.Song.2022.“ASurveyofControllableTextGenerationusingTransformer-based
Pre-trainedLanguageModels.”arXiv:2201.05337.
H.Zhao,H.Chen,F.Yang,N.Liu,H.Deng,H.Cai,S.Wang,D.Yin,andM.Du.2024.“ExplainabilityforLargeLanguage
Models:ASurvey.”ACMTransactionsonIntelligentSystemsandTechnology.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:29
A NaturalLanguageProcessingPipelines
Thissectionexplainsthepipelinesthathavebeenimplementedtoprovidethefunctionalitiesof
generAItor.
A.1 NaturalLanguageGenerationPipeline
Wegeneratetextbyusingthebeamsearchalgorithm,alwaysfollowingthepredictionwiththe
highest probability. The resulting beam search tree is stored as a graph in the backend of our
application.Allfunctionalitiesofoursystemuse,augment,ormodifythetree.Inthefollowing,we
describethedifferentpipelinesupdatingthetreestate.
PredictionPipeline—WeusethetokenizedbeamsequencefromtherootnodeuptotheHead
node as the model input for the prediction, truncated to GPT-2’s maximal sequence length of
𝑙 =1024.Dependingontheusersettings,theoutputtokenprobabilitiesareeithertop-𝑘 selected
max
or–whentemperatureisused–top-𝑝 sampled.Finally,weappendthenewtokenstothebeam
searchtree.ThefullPredictionPipelineisdepictedinfigure9.
KeywordExtraction&–Coloring—WeuseYAKE[Camposetal.2020]toautomaticallyextract
keywordsofan𝑛-gramsizeof𝑛 = 1fromthebeamsearchtree’ssequences.Next,wetokenize
theextractedkeywordsusingtheGPT-2tokenizer,passthemtotheGPT-2modelandextractthe
high-dimensionalembeddingsfromGPT-2’slayer11,maximizingthesurroundingcontextcaptured
bytheembeddings[Sevastjanova,Kalouli,etal.2022].NotethatthekeywordsextractedbyYAKE
oftenconsistofmultiplesplit-tokens,e.g.,whenthekeywordisapropernoun.Inthiscase,we
averagethehigh-dimensionalembeddingsofthesplittokens.Toreducethedimensionalityof
theembeddingsfrom768to2,weuseaUMAP[McInnesetal.2018]projectionpre-fittedonto
keywordsextractedfromtheMultiNLIdataset[Williamsetal.2018].Thenowtwo-dimensional
projectedembeddingvectorsarenormalizedandusedtosampleacoloronatwo-dimensional
colormap[Steigeretal.2015].ThefullKeywordEmbeddingPipelineisshowninfigure9.
A.2 BabelNetEmbeddingPipeline
Tobuildtheontologygraph,weleveragethepowerofasemanticnetwork(BabelNet[Navigli
andPonzetto2012])anditsadjacentdisambiguationAPI(Babelfy[Moroetal.2014]).First,each
keywordfromthebeamsearchtreeisdisambiguatedincontextusingtheBabelfyAPI.Theresulting
BabelNetSynsetisusedtoqueryaBabelNetIndexv5.1.Tocreateaunifiedontologygraph,part-
of-speech (POS) tags have to be considered, as the hypernym hierarchies inside BabelNet are
disconnectedforeachPOStag.Therefore,wemustexpandeachkeywordwithasetofpotential
synsetnounsthatrepresentitbest.Wethenbuildandgrowtheontologygraph,startingwiththe
keywordsasleafnodes.Thekeywordsareattachedtotheirexpandedsynsetsandwetraversetheir
hypernymrelationsupwards.Thehigherinthehierarchyasynsetis,themoreabstractitwillbe.
Therefore,atsomepoint,thesynsetsarenotconveyinghelpfulinformationtotheuser.Instead,
itwouldmakesensetoreducethehypernymrelationatsomepoint.Thisdecisionismadeusing
anotherattributethatexistsonmanyBabelNetsynsets—itsBabelDomain[Camacho-Colladosand
Navigli2017].Domainsaregeneralgroupsofwordsthatshareasimilarityorconcept.Theyare
availableformanysynsets.ThedomainsofBabelNetoftencoverseveralconcepts,suchasBiology.
Wespliteachdomainintoacollectionofsubdomains(BIOLOGY-Animal,Person).Ifasynsetdoes
nothaveadomain,westoptraversingthehypernymrelationsandinsteadattachthesynsettoits
mostsimilarsubdomainanddomain.Theontologygraphcangrowlargequickly,asthehypernym
relationsareoftenintertwinedandcontainmanysynsets.Tosimplifythetree,weremovenodes
thatonlyactasconnectingnodesbetweentwosynsets.Theresultisarelativelycompactcollection
oftrees,withonetreeforeachdomain.Whenpredictionsaremade,theinitialontologygraphis
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:30 Spinneretal.
expandedwithnewkeywords.Visualizingthisontologygraphdirectlycancreatelargetrees,as
multipleinstancesofthesamekeywordappearmultipletimes,creatingamultitudeofleafnodes.
Wethereforeinsteadsimplifythegraphfurtherintofourdistinctlayers,whereeachnodecanonly
haveoneparentrelation.ThisgraphcanthenbevisualizedusingaVoronoidiagram.Weusethe
D3Voronoitreemap5implementationtocreateaVoronoitreemapofthehierarchyandallowthe
usertoselectthelayertheywanttoview.Astheupperlayersaggregatethekeywordstothesame
synset,theyofferamorecompactviewofthedomainsandkeywordsofthepredictiongraph.The
BabelNetEmbeddingPipelineisshowninfigure10.
A.3 MaskedOntologicalReplacementPipeline
Tocreatethedomain-specific,context-sensitivesuggestionsoftheontologyreplacefunction,we
combinethepowerofthesemanticnetworkwithmaskedlanguagemodeling.Thegoalistoreplace
aspecificwordwithanothersuggestionthatfitsitscontextandcanbegroupedintodomains.
Tosolvethis,weuseacombinationofBERTandARESEmbeddings[Scarlinietal.2020].ARES
embeddingsarepowerfulsenseembeddingswithhigh-dimensionalrepresentativesforallWordNet
synsets.Theyweretrainedinasemi-supervisedapproachcombiningalexicalknowledgebase
withBERTLargeembeddingsandplaceWordNetsynsetsinthesameembeddingspaceasBERT
embeddings.Thisway,foragivenWordNetsynset,wecanquerytheclosestBERTembedding
andviceversa.BecauseBabelNethasWordNetbindingsformanyBabelNetsynsets,weassign
eachsubdomainaBabelNetandtheirrespectiveWordNetsynset.Thisway,eachsubdomaincan
beassignedtoanembeddingvectorviaARES.TheMaskedOntologicalReplacementPipeline
canbeobservedinfigure11.ForeachkeywordintheBeamSearchTree,wetakethewordandits
sentenceandreplaceitwiththe [𝑀𝐴𝑆𝐾] token.Afterwards,wecanusetop-𝑘 predictiononBERT
toqueryalargenumberofpredictionsthatwouldotherwisebeimpossibletoshowtheuserina
compactway(𝑘 =200).Wetokenizeeachpredictedwordandextractthemodellogitsincontext,
extractingandsqueezinglayers8-11,whicharethenappendedtomatchtheARESembeddings
length(𝑛 = 2048).Afterthisstep,wehaveasetofembeddingsforsubdomainsintheontology
graphandasetofembeddingsforthepredictionsinthebeamsearchtree.Tobringthemtogether,
welookforthenearestneighborsofallembeddingvectors.Tospeeduptheprocess,wecreateda
customFAISS[Johnsonetal.2019]index,whichwecanusetoquerynearestneighborsefficiently.
Subdomainsandpredictionsarematchedviatheiroverlappingnearestneighbors.Theresulting
predictionsarethenattachedtoeachkeywordandshownondemandviatheontologyreplace
function.
5https://github.com/Kcnarf/d3-voronoi-treemap
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.Fig.9. Thepipelinetoexpandthebeamsearchtreeandassignthesemantickeywordcolorinformationtoitsnodes.
Babelnet API
ke :sy trw ino gr [d ]s
Disambiguate
Attach
&
expand
synset
dQuery
omHypernym ainRelations
s
Assign
domains
&
subdomains o :nn
n
ot oo ddl eo
e
ssg []y
Sim Trp eelify O Tn reto el o 🌲gy OpV tio mro izn ao tii o n O Mn ato pl o 🗺gy
:string[]
BabelNet Embedding Pipeline
Fig.10. KeywordsareattachedtotheontologygraphviatheBabelNetembeddingpipeline.Thisgraphisthenfurthersimplifiedandthehierarchyisusedto
createanOntologyMapusingaVoronoidiagramvisualization.
FAISS Index
subdomain subdomains
keywords [MASK] sequence subdomains
ARES Matcher embeddings :domain_nn[][ ]
:string[] :string :string[] :string[] Query NN for
:float[2048]
domains
NLG Transformer
🤗 BERT Large NLG Transformer BERT Large 🤗 Match predictions predictions update Beam Search
Tokenizer to domains :string[] [] Tree
pm
reT
a
do
s
ip ck- tek
iod
n
B per rt
e
dTo ick te ioni nz ser
se :sq tru ine gn [c ]e Input
Layer
8-11
queeze
and duplicate
e
:p
m
flr obe ad
e
t[di 2c dt 0i
i
4o
n
8n
g
]
s
Q epu mre ber
d
ey diN
c
dtN
i io
n
nf go sr
:prep dr ie cd tii oc nti so _n ns
n[][ ] Masked Ontological Replacement Pipeline
:string[] s
Fig.11. Domain-specifickeywordsareattachedtoeachnodeofthebeamsearchtreebycomparingthenearestneighboursofthedomain’sARESembeddings
andthenearestneighboursoftheBERTpredictionsthatcouldreplacethekeywordofthenode.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.
-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation
111:31111:32 Spinneretal.
Received18July2023;revised16November2023and26January2024;accepted30January2024
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.