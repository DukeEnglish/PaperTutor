FairRR: Pre-Processing for Group Fairness through Randomized
Response
Xianli Zeng* Joshua Ward* Guang Cheng
xlzeng@wharton.upenn.edu joshuaward@ucla.edu guangcheng@ucla.edu
NUS (Chongqing) Research Institute University of California University of California
Los Angeles Los Angeles
Abstract discrimination against individuals and groups. In re-
sponse, a variety of approaches have been developed to
ensure fairness focusing on the pre-processing of data,
Theincreasingusageofmachinelearningmod-
the in-processing of models, or the post-processing of
elsinconsequentialdecision-makingprocesses
model predictions (Zemel et al. [2013], Louizos et al.
has spurred research into the fairness of these
[2016], Calmon et al. [2017], Xu et al. [2019a], Celis
systems. Whilesignificantworkhasbeendone
and Keswani [2019], Cotter et al. [2019], Madras et al. to study group fairness in the in-processing
[2018],Creageretal.[2019],JohndrowandLum[2019],
and post-processing setting, there has been
Cho et al. [2020], Zeng et al. [2024]). This litany of
little that theoretically connects these results
methods extends across many metrics of fairness that
tothepre-processingdomain. Thispaperpro-
can roughly be broken into two groups: group fair-
poses that achieving group fairness in down-
ness (Calders et al. [2009], Dwork et al. [2012], Hardt
stream models can be formulated as finding
et al. [2016]) where fairness is defined as ensuring vari-
the optimal design matrix in which to mod-
ous types of statistical parity across distinct protected
ify a response variable in a Randomized Re-
groups, and individual fairness (Joseph et al. [2016],
sponse framework. We show that measures
Lahoti et al. [2019], Ruoss et al. [2020]) which aims
of group fairness can be directly controlled
to provide nondiscriminatory predictions for similar
for with optimal model utility, proposing a
individuals.
pre-processingalgorithmcalledFairRR1 that
yieldsexcellentdownstreammodelutilityand Inthispaper, wefocusoncommongroupfairnesscrite-
fairness. ria,includingdemographicparity(Caldersetal.[2009],
Kamishima et al. [2012], Cho et al. [2020]), equality of
opportunity (Hardt et al. [2016], Zhang et al. [2018],
1 INTRODUCTION Cho et al. [2020]), and predictive equality (Corbett-
Daviesetal.[2017])addingtoalargerfamilyofdiverse
As the use of machine learning models becomes in- pre-processing methods in the supervised classification
creasingly prevalent in decision-making processes, con- setting. In general, the goal of pre-processing is to
cerns about the fairness of algorithms have become modify the feature space of the original dataset such
morepressing. Casestudiesfromvariousdomainssuch that when a classifier is trained on this altered data
as criminal justice, healthcare, and employment (Flo- its output is fair. Strategies for this include trans-
res et al. [2016], Corbett-Davies et al. [2023], Angwin forming the data (Feldman et al. [2015], Lum and
et al. [2016], Tolan et al. [2019]), have demonstrated Johndrow [2016], Calmon et al. [2017], Johndrow and
that biased algorithms can perpetuate or even amplify Lum [2019]), fair representation learning (Zemel et al.
[2013], Louizos et al. [2016], Madras et al. [2018], Crea-
*These authors contributed equally to this work ger et al. [2019]) and fair generative models (Xu et al.
1All code for FairRR with corresponding ex-
[2018], Sattigeri et al. [2019], Xu et al. [2019b], Ra-
periments can be found at: https://github.com/
maswamy et al. [2021]). These methods are convenient
UCLA-Trustworthy-AI-Lab/FairRR
to apply, as they do not change the training procedure
Proceedings of the 27thInternational Conference on Artifi- and are generally independent to downstream mod-
cial Intelligence and Statistics (AISTATS) 2024, Valencia, elling tasks, allowing for the use of most classifiers.
Spain. PMLR: Volume 238. Copyright 2024 by the au- However, they often do not allow for the control of
thor(s). the exact fairness level, they do not always have full
4202
raM
21
]LM.tats[
1v08770.3042:viXraFairRR: Pre-Processing for Group Fairness through Randomized Response
coverage of the variety of group fairness metrics in use, to satisfy many measures of group fairness at any
and they do not take advantage of recent results from disparity level, proposing a pre-processing method
the fair statistical learning literature. we call Fair Randomized Response (FairRR).
Indeed, with such a variety of potential strategies to
• We extend previous theoretical results from the
deployinensuringalgorthimsarefair,arobustbodyof
in-processing to the pre-processing group fairness
literaturehasdevelopedtoanswerthequestionofwhat
domain.
exactly is the best theoretical classification strategy in
termsofmodelutilityandfairness. Inthein-processing • Wedemonstratethatclassifierstrainedonmodified
domain where fairness is achieved through the modi- data from FairRR demonstrate excellent utility
fication of a classifier itself, first Corbett-Davies et al. and fairness results.
[2017] proved that, under several group fairness met-
rics, the fair Bayes-optimal classifiers are group-wise
2 PRELIMINARIES
thresholding rules with unspecified thresholds. Menon
and Williamson [2018] related demographic parity and
equality of opportunity to cost-sensitive risks and de- 2.1 Fairness
rived fair Bayes-optimal classifiers under these two
Tointroducefairalgorithmicdesign, weconsidercredit
fairness measures. Under the setting of perfect demo-
lending as an example, where it is essential to ensure
graphicparityandequalityofopportunity, exactforms
lending decisions are fair in order to comply with legal
of fair Bayes-optimal classifiers were derived in Chzhen
requirements. Thiscanbeformulatedasafairclassifica-
et al. [2019] and Schreuder and Chzhen [2021], respec-
tion problem, where two types of features are observed
tively. Finally, Zeng et al. [2024] and Zeng et al. [2022]
for potential creditors: standard features X ∈X such
showedthatinthegeneralcase,fairBayes-optimalclas-
as income and education, and protected (or sensitive)
sifierscouldbederivedforanylevelofdisparityinmost
features A∈A such as gender and race. The objective
definitions of group fairness which is an advantage in
is to predict the label Y ∈{0,1}, if a creditor were to
theappliedsettingifsomelevelofunfairnessisallowed
default on a loan, accurately and fairly with respect to
for better model utility. This paper extends this line
A. Throughout this paper, we set the sensitive feature
of research to the pre-processing area where our goal
ofA=1andA=0respectivelybesomeprivilegedand
is to develop a method that allows for an explicit level
the unprivileged groups. In this way, we can split the
of control of disparity in training data and to extend
population into four parts: the positive privileged (PP)
these theoretical results to create a unified framework
group (A=1, Y =1), the positive unprivileged (PN)
for adjusting for disparity at every step of machine
group (A = 0, Y = 1), the negative privileged (NP)
learning model development.
group (A=1, Y =0), and the negative unprivileged
Thus, we introduce the classic privacy technique Ran- (NN) group (A=0, Y =0).
domized Response (Warner [1965], Wang et al. [2016])
Researchershaveproposedmultiplegroupfairnessmea-
which privatizes a variable by ’flipping’ its labels based
sures for the fair classification setting. Generally, these
on some probability. We propose that measures of
measures depend on the constraints imposed on the
group fairness and downstream model utility can be
joint distribution of A, Y, and a classifier’s prediction
controlled by flipping the response variable in relation
Y(cid:98). Common fairness measures include:
to a sensitive attribute. Here, preserving model utility
can be thought of as minimizing the probability the Definition 2.1 (Demographic Parity). A prediction
label is flipped subject to a fairness constraint that Y(cid:98) satisfies demographic parity if it achieves the same
seeks to flip labels to make a training set more fair. acceptance rate among protected groups: P(Y(cid:98) =1|A=
To derive this fairness constraint, we use fair group 1)=P(Y(cid:98) =1|A=0).
thresholding results from recent work on Fair Bayes-
Definition 2.2 (Equality of Opportunity). A pre-
Optimal Classification Zeng et al. [2024, 2022] which
diction Y(cid:98) satisfies demographic parity if it achieves
allows for fairness to be exactly controlled for. Finally,
the same true positive rate among protected groups:
we find the solution to these optimal flipping prob-
P(Y(cid:98) =1|A=1,Y =1)=P(Y(cid:98) =1|A=0,Y =1).
abilities and perturb the response variable with the
corresponding randomized response mechanism, find- Definition 2.3 (Predictive Equality). A prediction Y(cid:98)
ing that downstream models trained on this perturbed satisfies predictive equality if it achieves the same false
variableachievegoodutilityatvariousfairnesssettings. positive rate among protected groups: P(Y(cid:98) = 1|A =
Our contributions are thus summarized as follows:
1,Y =0)=P(Y(cid:98) =1|A=0,Y =0).
Essentially,thesenotionsoffairnessprohibitsignificant
• We show that a response variable can be made mistreatment of one group over another. When theXianli Zeng, Joshua Ward and Guang Cheng
equalities holds in the aforementioned definitions, the found that, for many fairness metrics, the fair Bayes-
fairness constraint enforces identical treatment among optimal classifiers are group-wise thresholding rules
protected groups, referred to as perfect fairness. with adjusted thresholds. Specifically, the standard
Bayes-optimal classifiers f∗ :X ×{0,1}→[0,1] of the
In practice however, a relaxed or approximate versions
form: f∗(x,a) = I(η (x)>1/2) can be modified to
of these notions could be preferred as perfect fairness a
satisfy group fairness measures:
may require a large sacrifice of accuracy or may not
be possible. This means that instead of demanding
identical treatment, we require that there should not
(cid:18)
1+(2a−1)T
(t⋆)(cid:19)
be a significant difference in the model decisions be- f⋆(x,a)=I η (x)> a δ (2)
δ a 2
tweenthetwogroups. Here, thedisparityorunfairness
of a classifier can be easily quantified by the differ-
ence between the groups. Specifically, we use DDP, Here, (x,a) ∈ X × {0,1}, η a(x) = P(Y = 1|,A =
DEO and DPE to measure the degree of violating de- a,X = x). T 1(·) : R → [−1,1] and T 0(·) : R →
mographic parity, equality of opportunity, predictive
[−1,1]aretwomonotonenon-decreasingfunctionswith
equality, respectively: T 1(0) = T 0(0) = 0 that are decided by the fairness
metricandgroup-wiseprobabilities. Inparticular,with
DDP(f)=P(Y(cid:98) =1|A=1)− p ay = P(A = a,Y = y),(a,y) ∈ {0,1}2, we have
T (t)=t/(p +p ) for demographic parity, T (t)=
P(Y(cid:98) =1|A=0) t/a
[2p
−(2aa1 −1)a t0
] for equality of
opportunitya
, and
a1
DEO(f)=P(Y(cid:98) =1|A=1,Y =1)− T a(t)=t/[2p a0+(2a−1)t] for predictive equality.
P(Y(cid:98) =1|A=0,Y =1) The parameter t⋆ is decided by the disparity level δ
δ
DPE(f)=P(Y(cid:98) =1|A=1,Y =0)− where for a g (cid:16)iven t in a proper ra (cid:17)nge, the classifier
(1) f (x,a) = I η (x)> 1+(2a−1)Ta(t) is a fair Bayes-
P(Y(cid:98) =1|A=0,Y =0) t a 2
optimalclassifierforacertaindisparitylevelδ . Inpar-
t
ticular, the disparity level D(t)=D(f ) is a monotone
t
2.2 Fair Bayes Optimal Classifiers under non-increasing function of t. In other words, t⋆ can be
Demographic Parity thoughtofasatermthatbalancesthefairness-aδ ccuracy
tradeoff of the fair Bayes-optimal classifier. Details on
In classification problems, the prediction Y(cid:98) is often
estimating t⋆ can be found in the next section, but in
determined by a classifier f that indicates the proba- δ
practice it can also be treated as a hyperparameter to
bility of predicting Y(cid:98) =1 when observing X =x and
control for disparity.
A = a. Specifically, a classifier is a measurable func-
tion f : X ×{0,1} → [0,1] and Y | X ∼ Bern(f(X)),
with Bern(p) the Bernoulli distribution with success
2.3 Design Matrices in Randomized Response
probability p. We denote by Y(cid:98)f the prediction induced
by the classifier f and we call f is fair if its induced Randomized Response was first proposed by Warner
prediction f(cid:98)satisfies the fairness constraints. Among [1965] to preserve the privacy of survey respondents’
all fair classifiers, the Bayes optimal classifier serves answers when asked sensitive questions and is a classic
as a critical theoretical benchmark, as it establishes privacytechnique. Tostart,supposenindividualseach
the highest achievable accuracy for a given fairness have a response for some sensitive binary attribute Y,
constraint and serves as the theoretical objective that y ∈0,1. Eachindividualwishestopreservetheprivacy
i
various algorithms aim to estimate. Throughout, we oftheirresponseandsotheysendtoanuntrustedserver
will use D(f) to denote some level of disparity from 1, a modified version of y in which the label is flipped to
i
depending on the context. We denote by F δ the set of y (cid:101)i by some probability. The probabilities in which y i
measurable functions satisfying the δ-parity constraint is flipped are determined by a design matrix which in
the binary case can be written as:
F ={f ∈F :|D(f)|≤δ}.
δ
(cid:34) (cid:35)
A δ-fair Bayes-optimal classifier is defined as
P=
P(Y(cid:101) =1|Y =1) P(Y(cid:101) =1|Y =0)
(3)
(cid:16) (cid:17)
P(Y(cid:101) =0|Y =1) P(Y(cid:101) =0|Y =0)
f δ⋆ ∈argminR(f) with R(f):=P Y ̸=Y(cid:98)f .
f∈Fδ
To anonymize Y across a second binary variable A∈
Zeng et al. [2024] and Zeng et al. [2022] studied the {0,1}, we can rewrite 3 to consist of separate design
explicit form of fair Bayes-optimal classifiers. They matrices:FairRR: Pre-Processing for Group Fairness through Randomized Response
classifier with no protected attribute. Specifically, let
R denote the randomization mechanism as follows:
P
1
= θ1,θ2
(cid:34)
P(Y(cid:101) =1|A=1,Y =1) P(Y(cid:101) =1|A=1,Y
=0)(cid:35) Y f(cid:101) or= YR θ =1,θ0 1(Y a) ndhas θth fe orpr Yope =rty 0th foa rt P a( nY(cid:101)
y
= (θY ,) θ=
)
θ ∈1
0 1 0
P(Y(cid:101) =0|A=1,Y =1) P(Y(cid:101) =0|A=1,Y =0) [1/2,1] × [1/2,1]. Note that this mechanism is im-
balanced if θ ̸= θ . Denote by η(x) and η(x) the
1 0 (cid:101)
and conditional distribution of Y and Y(cid:101) given X =x, re-
spectively. It can be verified that:
P =
0
(cid:34) (cid:35) η(x)=θ η(x)+(1−θ )(1−η(x)).
P(Y(cid:101) =1|A=0,Y =1) P(Y(cid:101) =1|A=0,Y =0) (cid:101) 1 2
P(Y(cid:101) =0|A=0,Y =1) P(Y(cid:101) =0|A=0,Y =0)
Clearly, η(x) > 1/2 is equivalent to η(x) > (θ −
(cid:101) 0
1/2)/(θ +θ −1). Hence, R (Y) essentially shifts
Since the columns for each matrix must sum 1 0 θ1,θ0
the thresholds of the decision rule when θ ̸= θ . As
to 1, P 1 and P 0 can be expressed as the ran- wediscussedinsection2,theoptimalfaircla1 ssifie0 rsare
domization mechanism R (θ11,θ10,θ01,θ00) where known to be group-wise thresholding rules for many
θ
ay
=P(Y(cid:101) =y|A=a,Y =y): fairness-metrics. The aformentioned technical connec-
tion enables us to generate a fair dataset through an
(cid:20) (cid:21)
P = θ 11 1−θ 10 im-balanced randomization of response.
1 1−θ θ
11 10 Theorem 3.1. Let (X,A,Y) follow a distribution
and (cid:20) (cid:21) P on X × {0,1} × {0,1}. Consider a group-wise
θ 1−θ
P
0
= 1−0 θ1
θ
00 im-balanced randomized response mechanism Y(cid:101) =
01 00 R (A,Y) with, for a∈{0,1},
θ11,θ10,θ01,θ00
(cid:26)
3 METHOD θ , for Y =1;
P(Y(cid:101) =Y|A=a)= a1 (4)
1−θ , for Y =0.
a0
3.1 Overview When the flipping probabilities satisfy:
We therefore have the preliminaries to begin develop- (T (t⋆)+1)θ +(T (t⋆)−1)θ =T (t⋆);
1 δ 11 1 δ 10 1 δ
ing a pre-processing method to perturb Y to be fair. (T (t⋆)−1)θ +(T (t⋆)+1)θ =T (t⋆);
0 δ 01 0 δ 00 0 δ
Here, the goal is to find the randomization mechanism
R thatmaximizesdownstreammodelutil- whereT 1(·),T 0(·)andt⋆ arethesameasin (2). Denote
ity(θ s11 u, bθ1 j0 e, cθ0 t1, tθ o00 f) airness constraints. The design matrix P (cid:101) as the joint distribution of (X,A,Y(cid:101)). Then, the
for the best randomization mechanism can be easily Bayes optimal classifier learned on P (cid:101) is a δ-fair Bayes-
foundbeforethenbeingappliedtothetrainingdataset. optimal classifier (2) learned on P.
After this application, a final classifier can then be fit Remark 3.2. We need to maximize θ ∈[1/2,1] to
ay
for the original X and now perturbed label variable Y(cid:101). maximize the objective function (3.1). As a result, we
can take, when t⋆ >0,
Tostart,weproposethatthebestR from δ
solely a utility perspective would be(θ t1 h1, eθ1 o0 n,θ e01 t,θ h0 a0 t) does (cid:18) 1 1 (cid:19)
(θ ,θ ,θ ,θ )= ,1,1, , (5)
notflipY atallasitwouldnotinjectanynoiseintothe 11 10 01 00 1+T (t⋆) 1+T (t⋆)
trainingdataset. Thus,wewishtomaximizeP(Y(cid:101) =Y) 1 δ 0 δ
and, when t⋆ <0,
or: δ
(cid:18) (cid:19)
1 1
(θ ,θ ,θ ,θ )= 1, , ,1 , (6)
11 10 01 00 1−T (t⋆) 1−T (t⋆)
maxp θ +p θ +p θ +p θ 1 δ 0 δ
11 11 10 10 01 01 00 00
By Theorem 3.1 we can express group fairness def-
where 1 ≤ θ ,θ ,θ ,θ ≤ 1. We will show that
2 11 10 01 00 initions such as Demographic Parity, Equalized Op-
common group definitions of fairness can be written as
portunity, and Predictive Equality in terms of the
linear equality constraints for this function.
randomization mechanism which double as equality
constraints.
3.2 Fairness through Randomized Response
Definition 3.3 (Demographic Parity). A randomiza-
3.2.1 Randomized Response and the Fair tion mechanism achieves Demographic Parity if it sat-
Bayes-Optimal Classifier isfies:
To illustrate how fairness can be achieved by random- (p 11+p 10+t⋆ δ)θ 11+(t⋆ δ −p 11−p 10)θ 10 =t⋆ δ;
ized response, we first consider the Bayes-Optimal (t⋆−p −p )θ +(t⋆+p +p )θ =t⋆.
δ 01 00 01 δ 01 00 00 δXianli Zeng, Joshua Ward and Guang Cheng
Definition 3.4 (Equality of Opportunity). A random- Widyaningsih [2017]). The evaluation of each iteration
ization mechanism achieves Equalized Opportunity if it simplifies to O(n×p) where n is the size of the evalua-
satisfies: tion set. Once t⋆ is estimated, the perturbation of Y
δ
is an O(N) process where N is the sample size of all
2p 11θ 11+2(t⋆ δ −p 11)θ 10 =t⋆ δ; data to be perturbed. The overall time complexity is
−2p θ +2(t⋆+p )θ =t⋆. thus dependent on n, m, p, and N as to what the final
01 01 δ 01 00 δ
complexity reduces to.
Definition 3.5 (Predictive Equality). A randomiza-
tion mechanism achieves Predictive Equality if it satis-
4 EXPERIMENTS
fies:
2(t⋆+p )θ −2p θ =t⋆; 4.1 Empirical Data Analysis
δ 10 11 10 10 δ
2(t⋆−p )θ +2p θ =t⋆.
δ 00 01 00 00 δ Datasets: We test FairRR on three benchmark
datasets for fair classification: Adult Dua and Graff
[2017], COMPAS Angwin et al. [2016] and Law School
3.2.2 FairRR: a Randomized Response
Wightman [1998].
Mechanism for Fair Classification
In this section, we propose the Randomized Response
• Adult: The target variable Y is whether the in-
Mechanism that removes the discrimination from the
come of an individual is more than $50,000. Age,
training dataset. Based on the aformentioned theory,
marriage status, education level and other related
we are able to derive the optimal fair flipping probabil-
variables are included in X, and the protected
ities as long as we estimate p ,(a,y)∈{0,1}2 and t⋆
ay δ attribute A refers to gender.
from the training data. p can be estimated directly
ay
by using its empirical estimator and t⋆ δ can be conve- • COMPAS: In the COMPAS dataset, the target is
niently estimated using bisection methods due to its to predict recidivism. Here Y indicates whether
monotonic relationship with the decision disparity. or not a criminal will reoffend, while X includes
prior criminal records, age and an indicator of
Here, we set t =inf :{|T (t)|≤1 for a∈{0,1},}
and t = sm upin : {|Tt (t)| ≤a 1 for a ∈ {0,1},}. In misdemeanor. The protected attribute A is the
max t a race of an individual, “white-vs-non-white”.
each iteration, we update t = (t + t )/2 and
max min
calculate the flipping probabilities as referenced in (5)
• Law School: The task of interest in Law School
and (6). Then, classifier fˆ t is learned from (X,A,Y(cid:101)) data set is to predict whether an applicant gets
with Y(cid:101) =R θ11,θ10,θ01,θ00(Y). If the disparity level of fˆ t an admission from a law school based on common
is greater than the pre-specified disparity level, we set
features include LSAT score and undergraduate
t min =t mid iterate until t⋆ δ is found. GPA. The protected attribute A is the race of the
Thus, with p and t⋆ estimated, the optimal individual: “white-vs-non-white”
ay δ
(θ ,θ ,θ ,θ ) can be solved for using (5), (6) and
11 10 01 00
a corresponding group fairness definition, which maxi- Compared algorithms: In addition to FairRR, we
mizes3.1subjecttotheconstraintsofeither3.3,3.4,or also consider several benchmark methods in our experi-
3.5. With this randomization mechanism, the values in ments. As FairRR is a pre-processing method, we only
the privileged group A=1 are randomly flipped from include other pre-processing methods for comparison.
Y =1 to Y =0 and values in the unprivileged group Specifically, we consider the following:
A=0 are randomly flipped from Y =0 to Y =1 such
that a new perturbed response variable Y(cid:101) is created. • (1) Fair Sampling
Any classifier can then be fit to the original data X
with perturbed Y(cid:101). Fair Sampling Kamiran and Calders [2012] is a
methodbasedonadjustingthesizeofPP,PN,NP
The time complexity of this method is dependent on
and NN groups. Its idea is to apply over/down
which classifier is chosen for estimating t⋆ in the afore- samplingsuchthatthelabelonthetrainingdatais
δ
mentioned bisection method. Here, a classifier has to
independentofthesensitiveattribute. Specifically,
be iteratively trained and evaluated to find the desired
size of group PP, PN, NP and NN after sampling
t⋆. In practice, we find that this takes relatively few
are:
δ
iterations. In the case of using logistic regression with (n +n )(n +n )
the LBFGS solver for example, the training complexity n ay = na1 +na0 +n1y +n0y
11 10 01 00
is O(p×m) where p is the number of parameters and
m is the number of memory corrections (Saputro and • (2) FAWOSFairRR: Pre-Processing for Group Fairness through Randomized Response
Table 1: Benchmarking Results: Original vs FairRR Pre-processed Datasets
Panel A: Original Datasets
Metrics
Datasets Acc f DDP DEO DPE
1
Adult 0.841 0.620 0.188 0.184 0.086
(0.003) (0.007) (0.006) (0.026) (0.005)
COMPAS 0.676 0.632 0.283 0.313 0.186
(0.015) (0.016) (0.031) (0.052) (0.035)
Law School 0.787 0.499 0.060 0.084 0.024
(0.003) (0.005) (0.005) (0.015) (0.004)
Panel B: Fair Randomized Response
Fairness Criteria
Demographic Parity Equality of Opportunity Predictive Equality
Metrics Metrics Metrics
Datasets Acc f DDP Acc f DEO Acc f DPE
1 1 1
Adult 0.820 0.534 0.007 0.839 0.608 0.024 0.829 0.563 0.005
(0.004) (0.009) (0.005) (0.003) (0.007) (0.02) (0.004) (0.009) (0.004)
COMPAS 0.660 0.608 0.027 0.661 0.610 0.046 0.667 0.614 0.031
(0.015) (0.017) (0.019) (0.014) (0.016) (0.037) (0.014) (0.016) (0.024)
Law School 0.785 0.486 0.006 0.785 0.489 0.015 0.786 0.493 0.004
(0.003) (0.005) (0.004) (0.003) (0.005) (0.011) (0.003) (0.005) (0.004)
FAWOS Salazar et al. [2021] is another sampling fair pre-processing algorithm to each training dataset.
method for fairness proposed recently. Unlike fair A logistic regression classifier is then learned on the
sampling that adjust the sizes of all four groups, returned de-biased training dataset where it is then
FAWOSonlyappliesSMOTE(Chawlaetal.[2002], evaluated based on the average accuracy, f score and
1
a popular oversampling method for unbalanced disparity levels over 100 random 80:20 train/test splits.
classification problem) to over-sample the points All model hyperparameters are left as the scikit-learn
in the NN group where the number of points gen- defaults for reproduciblity. The standard deviations
erated is: of these metrics are also reported. All training and
evaluations were processed using an Apple M1 CPU.
(cid:18) (cid:19)
n n
N =α× 11 00 −n
n 01
10 4.2 Results
• (3) TabFairGAN WefirstevaluatetheperformanceofFairRRcontrolling
TabFairGAN Rajabi and Garibay [2021] is a fair for either Demographic Parity, Equalized Opportunity,
synthetic generation method based on the frame- or Predictive Equality. We present the simulation re-
work of generative adversarial network which adds sults in Table 1. We observe that FairRR significantly
a fairness penalty term to the generator loss of a controls for disparity across each fairness metric while
standard WGAN model. Specifically, the fairness seeing minimal decreases of model utility measured by
penalty is equal to the demographic parity of the accuracy and f score. We then benchmark FairRR
1
generated data squared. with other existing pre-processing methods. Here, only
demographic parity is considered as it is the only com-
monfairnessmetricsupportedacrossallpre-processing
Experimental Setting: Thegoaloffairclassification
methods. We present these benchmarking results in
is to learn a classifier with the highest model utility,
Table 2.
subject to some fairness constraint. Thus to test and
benchmark FairRR we first apply each aforementioned Finally,weshowcasetheabilityofFairRRtocontrolforXianli Zeng, Joshua Ward and Guang Cheng
Table 2: Benchmarking Results: Pre-processing Methods
Methods
Datasets Metrics Original FairRR TabFairGan FS FAWOS
Adult Acc 0.841 0.820 0.804 0.836 0.786
(0.003) (0.004) (0.008) (0.003) (0.004)
DDP 0.188 0.007 0.023 0.091 0.008
(0.006) (0.005) (0.024) (0.008) (0.006)
COMPAS Acc 0.676 0.660 0.631 0.659 0.632
(0.015) (0.015) (0.034) (0.014) (0.015)
DDP 0.283 0.027 0.150 0.033 0.022
(0.031) (0.019) (0.110) (0.026) (0.017)
Law School Acc 0.787 0.785 0.774 0.784 0.782
(0.003) (0.003) (0.030) (0.003) (0.003)
DDP 0.060 0.006 0.060 0.006 0.006
(0.005) (0.004) (0.153) (0.004) (0.004)
specific levels of disparity. In Table 3, FairRR was set parity, an added benefit in applications where perfect
to control for disparity at the quintiles between perfect group fairness is impractical or not needed. Table 3
demographic parity and the DDP level of the original shows empirically that disparity can be set to a level
dataset. The corresponding average DDP values in the a-priori to model training and the downstream model
final logistic regression and corresponding accuracies will have that final level of disparity. Similarly, the
with standard deviations over 100 random seeds are trade-off between accuracy and disparity is better than
reported. Figure 1 plots this experiment to highlight competing methods that have this feature. With FA-
the accuracy/ disparity trade-off, comparing FairRR WOS conveniently allowing for the control of disparity
to FAWOS at these quintiles of controlled-for disparity. we compare it with FairRR in Figure 1, showing that
Figure 2 showcases the Pareto Curves of FairRR, Fair FairRR has a preferable utility curve to FAWOS in
Sampling, FAWOS, and FairTabGAN when an SVM is that at nearly all levels of disparity, the model trained
trained on pre-processed data from the Adult Dataset. with FairRR-processed data has better accuracy. This
is further shown in Figure 2 which evaluates all com-
peting methods on the Adult dataset with Support
5 DISCUSSION
Vector Machines. Here, FairRR dominated the Pareto
Frontier, noting that Fair Sampling does not allow for
Overall, FairRR achieves favorable or comparable-to- disparity control.
the-leader accuracy and disparity scores across the
Anothercomponentinvestigatedwasthecorresponding
three benchmarking datasets. FairRR effectively main-
privacy offered by FairRR. As Randomized Response
tains model utility while enforcing small amounts of
was first introduced as a privacy method, a natural
disparity, regardless of the chosen group fairness defi-
extension of FairRR is to investigate the relationship
nitions. One surprising result was the stability of the
between its utility/ fairness trade-off and the addi-
algorithm. One potential downside to FairRR could be
tionalprivacyitprovides. Thisprovestobetechnically
with it randomly flipping labels the effectiveness could
challenging. While Randomized Response is shown to
vary widely depending on the random seed. With low
standard deviations across evaluation metrics though,
satisfy (ϵ,δ)- Label Differential Privacy (Wang et al.
[2016],Shirongetal.[2023]),theaddedfairnesscompo-
FairRR proves to be also be robust. One interest-
nentofFairRRcomplicatesatypicalprivacyanalysisas
ing finding is that FairRR, Fair Sampling (FS), and
it makes the privacy mechanism no longer independent
FAWOS generally performed better than TabFairGan.
of the data it is privatising. This is highlighted in the
We suspect this is because TabFairGan learns both
X and y, which has advantages for applications such
estimation of t⋆
δ
where the design matrix is explicitly
calculatedbasedoffofthedisparitylevelintheoriginal
as privacy, but likely makes it weaker for pure fair
classification tasks where FairRR and the over/under
dataset. WhileY(cid:101) ismoreprivatethanY,itremainsun-
solved how to quantify exactly how much more private
sampling strategies in FS and FAWOS perturb the
itisinthecontextofsomeprivacybudget. However,in
feature space less.
applicationanadvantageofpre-processingisthatother
FairRR also favorably controls for exact levels of dis-FairRR: Pre-Processing for Group Fairness through Randomized Response
Table 3: Direct Control on Pre-specified Disparity Levels (δ)
Datasets Metrics
Adult δ 0.000 0.040 0.080 0.120 0.160
DDP 0.007 0.040 0.081 0.121 0.161
(0.005) (0.008) (0.008) (0.008) (0.007)
Acc 0.820 0.826 0.833 0.838 0.841
(0.004) (0.004) (0.003) (0.003) (0.003)
COMPAS δ 0.000 0.060 0.120 0.180 0.240
DDP 0.027 0.062 0.123 0.182 0.239
(0.019) (0.030) (0.032) (0.030) (0.032)
Acc 0.660 0.665 0.669 0.674 0.676
(0.015) (0.014) (0.014) (0.015) (0.015)
Law School δ 0.000 0.012 0.024 0.036 0.048
DDP 0.006 0.013 0.025 0.036 0.049
(0.004) (0.006) (0.007) (0.006) (0.006)
Acc 0.785 0.785 0.786 0.786 0.786
(0.003) (0.003) (0.003) (0.003) (0.003)
pre-processing techniques can also be applied to the
training data and in the context of privacy, FairRR is
well-suitedtobeusedinconjunctionwithotherprivacy
mechanisms such as Laplacian and Exponential Noise
(Jain et al. [2018]).
6 CONCLUSION
FairRR can be an excellent choice achieving group fair-
nessinthatitisadownstreammodelagnostic,efficient,
and theory motivated algorithm that supports most
group fairness definitions. In benchmarking, it per-
forms comparably or better than other choices for pre-
processing algorithms and additionally connects previ-
ousfairstatisticallearningtheorytothepre-processing
Figure 1: Logistic Regression Accuracy/ Disparity
domain.
Trade-offs: FairRR and FAWOS comparison across
There are a variety of future research opportunities datasets.
with FairRR. For starters, this paper only addresses
the single binary sensitive attribute, single binary out-
come problem formulation of fair classification. We
believe that FairRR could be generalized to work in
settings where multiple sensitive attributes are needed.
Another interesting line of work is studying FairRR in
thecontextofprivacy,whatRandomizedResponsewas
initially designed for. While this is technically chal-
lenging, we believe that extensions on FairRR could
help shed light into the theoretical trade-offs between
fairness and privacy. Lastly, we suspect there are a
varietyofadditionalmechanismsoutsideofrandomized
Figure 2: Accuracy/ Disparity Pareto Curves of var-
response to further apply the idea of pre-processing or
ious pre-processing algorithms on the Adult dataset
post-processing data based on the Fair Optimal Bayes
evaluated with Support Vector Machines.
thresholding to achieve group fairness.Xianli Zeng, Joshua Ward and Guang Cheng
Acknowledgements differentiable constraints with applications to fair-
ness, recall, churn, and other goals. Journal of Ma-
We thank the anonymous reviewers for their helpful
chine Learning Research, 20(172):1–59, 2019.
comments and suggestions. This work was partially
E. Creager, D. Madras, J.-H. Jacobsen, M. Weis,
supported by the JP Morgan Chase Faculty Research
K. Swersky, T. Pitassi, and R. Zemel. Flexibly
Award, NSF – CNS (2247795), and the Office of Naval
fair representation learning by disentanglement. In
Research (ONR N00014-22-1-2680). This work is also
partially supported by the National Natural Science Proceedings of the 36th International Conference on
Foundation of China, No. 72033002. Machine Learning, pages 1436–1445. PMLR, 2019.
D. Dua and C. Graff. UCI machine learning repository,
References
2017. URL http://archive.ics.uci.edu/ml.
C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and
J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Ma-
chine Bias. There’s software used across the country R. Zemel. Fairness through awareness. In Proceed-
to predict future criminals. And it’s biased against ings of the 3rd Innovations in Theoretical Computer
blacks. ProPublica, May 2016. Science Conference, pages 214–226. Association for
Computing Machinery, 2012.
T. Calders, F. Kamiran, and M. Pechenizkiy. Build-
M. Feldman, S. A. Friedler, J. Moeller, C. Scheideg-
ing classifiers with independency constraints. In
ger, and S. Venkatasubramanian. Certifying and
2009 IEEE International Conference on Data Min-
removing disparate impact. In Proceedings of the
ing Workshops, pages 13–18, 2009.
21th ACM SIGKDD International Conference on
F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ra- Knowledge Discovery and Data Mining, pages 259–
mamurthy, and K. R. Varshney. Optimized pre- 268. Association for Computing Machinery, 2015.
processingfordiscriminationprevention.InAdvances
A. W. Flores, K. Bechtel, and C. T. Lowenkamp. False
in Neural Information Processing Systems. Curran
positives,falsenegatives,andfalseanalyses: Arejoin-
Associates, Inc., 2017.
der to Machine Bias: “There’s software used across
L. E. Celis and V. Keswani. Improved adversar- the country to predict future criminals. And it’s bi-
ial learning for fair classification. arXiv preprint ased against blacks”. Federal Probation, 80(2):38–46,
arXiv:1901.10443, 2019. 2016.
N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. M. Hardt, E. Price, E. Price, and N. Srebro. Equality
Kegelmeyer. SMOTE: synthetic minority over- of opportunity in supervised learning. In Advances
samplingtechnique. Journal of Artificial Intelligence in Neural Information Processing Systems. Curran
Research, 16(1):321–357, 2002. Associates, Inc., 2016.
J. Cho, G. Hwang, and C. Suh. A fair classifier using P. Jain, M. Gyanchandani, and N. Khare. Differential
kernel density estimation. In Advances in Neural privacy: its technological prescriptive using big data.
Information Processing Systems, pages 15088–15099. Journal of Big Data, 5(15), 2018.
Curran Associates, Inc., 2020.
J. E. Johndrow and K. Lum. An algorithm for re-
E.Chzhen, C.Denis, M.Hebiri, L.Oneto, andM.Pon- moving sensitive information: application to race-
til. Leveraginglabeledandunlabeleddataforconsis- independent recidivism prediction. The Annals of
tent fair binary classification. In Advances in Neural Applied Statistics, 13(1):189–220, 2019.
Information Processing Systems. Curran Associates, M.Joseph,M.Kearns,J.H.Morgenstern,andA.Roth.
Inc., 2019. Fairness in learning: Classic and contextual bandits.
S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, and In Advances in Neural Information Processing Sys-
A.Huq. Algorithmicdecisionmakingandthecostof tems. Curran Associates, Inc., 2016.
fairness. In Proceedings of the 23rd ACM SIGKDD F. Kamiran and T. Calders. Data preprocessing
International Conference on Knowledge Discovery techniques for classification without discrimination.
andDataMining,page797–806.AssociationforCom- Knowledge and Information Systems, 33:1–33, 2012.
puting Machinery, 2017.
T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma.
S. Corbett-Davies, J. D. Gaebler, H. Nilforoshan, Fairness-aware classifier with prejudice remover reg-
R.Shroff,andS.Goel. Themeasureandmismeasure ularizer. In Machine Learning and Knowledge Dis-
of fairness. Journal of Machine Learning Research, covery in Databases, pages 35–50. Springer Berlin,
24(312):1–117, 2023. Heidelberg, 2012.
A. Cotter, H. Jiang, M. Gupta, S. Wang, T. Narayan, P. Lahoti, K. P. Gummadi, and G. Weikum. iFair:
S. You, and K. Sridharan. Optimization with non- Learning individually fair data representations forFairRR: Pre-Processing for Group Fairness through Randomized Response
algorithmic decision making. In 2019 IEEE 35th In- S. Tolan, M. Miron, E. Gómez, and C. Castillo. Why
ternational Conference on Data Engineering (ICDE), machine learning may lead to unfairness: Evidence
pages 1334–1345. IEEE, 2019. from risk assessment for juvenile justice in catalo-
C. Louizos, K. Swersky, Y. Li, M. Welling, and R. S. nia. In Proceedings of the Seventeenth International
Zemel. The variational fair autoencoder. In 4th In- Conference on Artificial Intelligence and Law, pages
83–92. Association for Computing Machinery, 2019.
ternational Conference on Learning Representations
(ICLR), 2016. Y. Wang, X. Wu, and D. Hu. Using randomized re-
sponse for differential privacy preserving data collec-
K. Lum and J. Johndrow. A statistical frame-
tion. In EDBT/ICDT Workshops, 2016.
work for fair predictive algorithms. arXiv preprint
arXiv:1610.08077, 2016. S. L. Warner. Randomized response: A survey tech-
niqueforeliminatingevasiveanswerbias. Journal of
D.Madras,E.Creager,T.Pitassi,andR.Zemel. Learn-
the American Statistical Association, 60(309):63–69,
ingadversariallyfairandtransferablerepresentations.
1965.
In Proceedings of the 35th International Conference
onMachineLearning,pages3384–3393.PMLR,2018. L. F. Wightman. Lsac national longitudinal bar
passage study. lsac research report series, 1998.
A. K. Menon and R. C. Williamson. The cost of fair-
URL https://archive.lawschooltransparency.
nessinbinaryclassification. InProceedings of the 1st
com/reform/projects/investigations/2015/
Conference on Fairness, Accountability and Trans- documents/NLBPS.pdf.
parency, pages 107–118. PMLR, 2018.
D. Xu, S. Yuan, L. Zhang, and X. Wu. FairGAN:
A. Rajabi and O. O. Garibay. TabFairGAN: Fair
Fairness-aware generative adversarial networks. In
tabular data generation with generative adversarial
2018 IEEE International Conference on Big Data
networks. arXiv preprint arXiv:2109.00666, 2021. (Big Data), pages 570–575. IEEE, 2018.
V. V. Ramaswamy, S. S. Y. Kim, and O. Russakovsky. D. Xu, Y. Wu, S. Yuan, L. Zhang, and X. Wu. Achiev-
Fair attribute classification through latent space de- ing causal fairness through generative adversarial
biasing. In IEEE/CVF Conference on Computer networks. In Proceedings of the Twenty-Eighth In-
Vision and Pattern Recognition (CVPR), 2021. ternational Joint Conference on Artificial Intelli-
A. Ruoss, M. Balunovic, M. Fischer, and M. Vechev. gence, IJCAI-19, pages 1452–1458. International
Learning certified individually fair representations. Joint Conferences on Artificial Intelligence Organi-
In Advances in Neural Information Processing Sys- zation, 2019a.
tems,pages7584–7596.CurranAssociates,Inc.,2020. D. Xu, S. Yuan, L. Zhang, and X. Wu. FairGAN+:
T. Salazar, M. S. Santos, H. Araújo, and P. H. Achieving fair data generation and classification
Abreu. FAWOS: Fairness-aware oversampling algo- through generative adversarial nets. In 2019 IEEE
rithm based on distributions of sensitive attributes. International Conference on Big Data (Big Data),
IEEE Access, 9:81370–81379, 2021. pages 1401–1406, 2019b.
R.Zemel,Y.Wu,K.Swersky,T.Pitassi,andC.Dwork.
D.R.S.SaputroandP.Widyaningsih.Limitedmemory
broyden-fletcher-goldfarb-shanno (l-bfgs) method for Learning fair representations. In Proceedings of the
theparameterestimationongeographicallyweighted 30th International Conference on Machine Learning,
pages 325–333. PMLR, 2013.
ordinal logistic regression model (gwolr). AIP Con-
ference Proceedings, 1868(1):040009, 2017. X. Zeng, E. Dobriban, and G. Cheng. Fair Bayes-
P. Sattigeri, S. C. Hoffman, V. Chenthamarakshan, optimal classifiers under predictive parity. In Ad-
and K. R. Varshney. Fairness GAN: Generating vances in Neural Information Processing Systems,
pages 27692–27705. Curran Associates, Inc., 2022.
datasets with fairness properties using a generative
adversarial network. IBM Journal of Research and X. Zeng, G. Cheng, and E. Dobriban. Bayes-optimal
Development, 63(4/5):3:1–3:9, 2019. fair classification with linear disparity constraints
via pre-, in-, and post-processing. arXiv preprint
N. Schreuder and E. Chzhen. Classification with ab-
arXiv:2402.02817, 2024.
stention but without disparities. In Proceedings of
B. H. Zhang, B. Lemoine, and M. Mitchell. Mitigating
the Thirty-Seventh Conference on Uncertainty in Ar-
tificial Intelligence, pages 1227–1236. PMLR, 2021. unwanted biases with adversarial learning. In Pro-
ceedings of the 2018 AAAI/ACM Conference on AI,
X. Shirong, C. Wang, W. W. Sun, and G. Cheng.
Ethics, and Society, pages 335–340. Association for
Binaryclassificationunderlocallabeldifferentialpri-
Computing Machinery, 2018.
vacy using randomized response mechanisms. Trans-
actions on Machine Learning Research, 2023.