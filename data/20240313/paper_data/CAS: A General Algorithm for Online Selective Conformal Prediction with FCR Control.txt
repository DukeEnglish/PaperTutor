CAS: A General Algorithm for Online Selective Conformal
Prediction with FCR Control
Yajie Baoa, Yuyang Huob, Haojie Rena and Changliang Zoub∗
aSchool of Mathematical Sciences, Shanghai Jiao Tong University
Shanghai, P.R. China
bSchool of Statistics and Data Science, Nankai University
Tianjin, P.R. China
March 13, 2024
Abstract
We study the problem of post-selection predictive inference in an online fashion. To avoid
devoting resources to unimportant units, a preliminary selection of the current individual before
reporting its prediction interval is common and meaningful in online predictive tasks. Since the
online selection causes a temporal multiplicity in the selected prediction intervals, it is important
to control the real-timefalse coverage-statement rate(FCR) tomeasure the averaged miscoverage
error. We develop a general framework named CAS (Calibration after Adaptive Selection)
that can wrap around any prediction model and online selection rule to output post-selection
prediction intervals. If the current individual is selected, we first perform an adaptive selection
on historical data to construct a calibration set, then output a conformal prediction interval
for the unobserved label. We provide tractable constructions for the calibration set for popular
online selection rules. We proved that CAS can achieve an exact selection-conditional coverage
guarantee in the finite-sample and distribution-free regimes. For the decision-driven selection
rule, including most online multiple-testing procedures, CAS can exactly control the real-time
FCR below the target level without any distributional assumptions. For the online selection
with symmetric thresholds, we establish the error bound for the control gap of FCR under mild
distributional assumptions. To account for the distribution shift in online data, we also embed
CAS into some recent dynamic conformal prediction methods and examine the long-run FCR
control. Numerical results on both synthetic and real data corroborate that CAS can effectively
control FCR around the target level and yield more narrowed prediction intervals over existing
baselines across various settings.
∗Corresponding Author: nk.chlzou@gmail.com; The authors are listed in alphabetical order.
1
4202
raM
21
]LM.tats[
1v82770.3042:viXraKeywords: Conformalinference,distribution-free,onlineprediction,selection-conditionalcoverage,
selective inference
1 Introduction
In applications of scientific discovery or industrial production, conducting real-time selection or
screening before prediction is useful because it can help allocate scarce computational or human
resources to valuable individuals. Despite the high prediction accuracy of black-box models, the lack
of statistical validity may hinder their participation in making further decisions. Conformal inference
(Vovk et al., 1999, 2005) provides a powerful and flexible framework that can translate the prediction
value from any machine learning model into a valid confidence interval. In online selective prediction
problems, the validity guarantee becomes more challenging since the randomness of the real-time
selection should be considered in the construction of prediction intervals.
Suppose the feature-response pairs {Z = (X ,Y )} ⊆ Rd × R appear in a sequential and
t t t t≥0
delayed fashion. At time t, we can observe the previous label Y and the new feature X . Let
t−1 t
Π (·) : Rd → {0,1} be a general online selection rule that may depend on the previously observed
t
data. To be specific, we write S = Π (X ) as the selection indicator, and our aim is to report
t t t
the prediction interval of the unobserved label Y when S = 1. Since training large-scale machine
t t
learning models is time-consuming, we consider that one pre-trained model µ is given in this work.
(cid:98)
As a popular variant of conformal inference, the split conformal method (Papadopoulos et al., 2002;
Vovk et al., 2005) can naturally yield a marginal prediction interval Imarg(X ;α) by computing the
t t
empirical quantile of historical residuals {R = |Y −µ(X )|} . If the data points {Z } are i.i.d.,
i i (cid:98) i i≤t−1 i i≤t
the interval Imarg(X ;α) enjoys a distribution-free coverage guarantee P{Y ∈ Imarg(X ;α)} ≥ 1−α
t t t t t
as discussed in Lei et al. (2018).
However, due to the potential dependence between selection and prediction intervals, the multi-
plicity issue will arise in this setting. Benjamini and Yekutieli (2005) firstly pointed out that ignoring
the multiplicity in the selected confidence intervals will lead to undesirable consequences, and they
pioneered the metric false coverage-statement rate (FCR) to measure the averaged miscoverage
error in the constructed confidence intervals. Recently, Weinstein and Ramdas (2020) considered
the temporal multiplicity and extended the definition of FCR to the online regime. For any online
predictive procedure that returns intervals {I (X ) : S = 1} , the corresponding FCR value and
t t t t≥0
false coverage proportion (FCP) up to time T are defined as
(cid:80)T S ·1{Y ̸∈ I (X )}
FCP(T) = t=0 t t t t , FCR(T) = E[FCP(T)],
1∨(cid:80)T
S
t=0 t
where a∨b = max{a,b} for any a,b ∈ R. To achieve real-time FCR control, Weinstein and Ramdas
(2020) proposed an approach named LORD-CI based on building marginal prediction intervals at a
2sequence of adjusted confidence levels {α } such that (cid:80)T α /(1∨(cid:80)T S ) ≤ α for any T ≥ 0.
t t≥0 t=0 t t=0 t
However, one drawback of LORD-CI is that the prediction interval tends to be extremely wide due to
small α as t grows, which results in unsatisfactory or even powerless performance in practice. This
t
issueisessentiallycausedbyignoringtheselectionconditionintheconstructionofpredictionintervals.
Hence, instead of marginal coverage, we consider a plausible criterion in post-selection inference
(Fithianetal.,2014),namely,selection-conditional coverage,whichcharacterizesthecoverageproperty
of confidence interval conditioning on the selection event. The selection-conditional coverage in the
online process is defined as,
P{Y ∈ I (X ) | S = 1} ≥ 1−α, ∀t ≥ 0.
t t t t
Previous literature (Tibshirani et al., 2016; Lee et al., 2016) usually needs special distributional
assumptions on the data, such as Gaussian or exponential family distribution, to obtain such a
conditional coverage guarantee, which is unfortunately violated in many real applications. Therefore,
this paper aims to develop a distribution-free framework to construct post-selection prediction
intervals with selection-conditional coverage while successfully controlling the real-time FCR around
the target level.
1.1 Our approach: calibration after adaptive selection (CAS)
Intheofflinescheme,Baoetal.(2024)proposedaselectiveconditionalconformalpredictionprocedure
(abbreviated as SCOP) by first performing the selection on independent labeled data (holdout set)
with the identical threshold used in the test set to obtain a selected calibration set, and then
constructing split conformal prediction intervals using the quantiles from the selected calibration set.
If the threshold is invariant to the permutation of data points in the holdout set and test set, or if
the threshold is an order statistic from the test set, SCOP can successfully control the FCR value
around the target level. However, these thresholds are not realistic in the online setting because
the selection rule Π is fully determined by previously observed data. Moreover, the non-adaptive
t
selection of SCOP on the holdout set may fail in the online setting.
Motivated by the spirit of post-selection calibration in SCOP, we develop a more principled
algorithm, named CAS (Calibration after Adaptive Selection), for online selective conformal pre-
diction. To be specific, if S = 1 at time t, we firstly use a sequence of adaptive selection rules
t
{ΠAda(·)} on the previously observed labeled data {Z } to get a dynamic calibration set
t,s s≤t−1 i i≤t−1
C(cid:98)t = {s ≤ t−1 : ΠA t,sda(X s) = 1}, where ΠA t,sda(·) is constructed by integrating the information from
the selection rules (Π ,Π ) and the current feature X . Then for any target level α of FCR control,
t s t
we report the following prediction interval:
ICAS(X ;α) = µ(X )±q ({R } ).
t t (cid:98) t α i i∈C(cid:98)t
3Step 1: decision Step 2: selection Step 3: calibration
Current feature Holdout set Residuals
𝑋 ! {(𝑋 ",𝑌 ")} "&!’( {𝑅 "} "∈𝒞% !
If 𝑆 =1
!
Selection rule Adaptive selection Split conformal
Π {Π*-.} calibration
! !," "&!’(
Decision Calibration set Prediction interval
𝑆
!
=Π !(𝑋 !) {(𝑋 ",𝑌 ")} "∈𝒞%
!
𝐼 !)*+(𝑋 !;𝛼)
Figure 1: The workflow of CAS at time t. The selected calibration set is given by C(cid:98)t =
(cid:8) s≤t−1:ΠAda(X )=1(cid:9) . The residuals are computed by R =|µ(X )−Y |.
t,s s s (cid:98) s s
where q α({R i} i∈C(cid:98)t) is the ⌈(1−α)(|C(cid:98)t|+1)⌉-st smallest value in {R i} i∈C(cid:98)t.
To achieve the selection-conditional coverage, the adaptive selection rules {ΠAda(·)} are
t,s s∈Ht
designed to guarantee the product of selection indicators ΠAda(X )·S is symmetric with respect
t,s s t
to samples (Z ,Z ) for s ≤ t − 1. In this work, we design adaptive selection strategies for two
s t
popular classes of selection rules to ensure exact exchangeability after selection. The first rule is
the decision-driven selection considered in Weinstein and Ramdas (2020), which is broadly used in
online multiple testing with false discovery rate (FDR) control (Foster and Stine, 2008; Javanmard
and Montanari, 2018). Our adaptive selection scheme takes advantage of the intrinsic property of
decision-driven selection to obtain an “intersecting” subset C(cid:98)t. The second rule pertains to online
selection with symmetric thresholds, which involves screening using either the historical sample mean
or quantile. Here, we propose an adaptive selection strategy by “swapping” X and X for i ≤ t−1 in
t i
the indicator S to obtain a new indicator determining whether Z is selected as a calibration point
t i
in C(cid:98)t. We show that the CAS can achieve the exact selection-conditional coverage guarantee in both
selection rules.
The workflow of the proposed method CAS at time t is described in Figure 1. Our contributions
have four folds:
(1) Due to the adaptive selection in previously observed data, the CAS can achieve the exact
selection-conditional coverage guarantee in both decision-driven selection and selection with
symmetric thresholds. More importantly, our results are distribution-free and can be applied
to many practical tasks without prior knowledge of data distribution.
4(2) Compared to the offline regime, controlling the real-time FCR is more challenging due to the
temporal dependence of the decisions {S } . For decision-driven selection rules, we prove that
t t≥0
the CAS can exactly control the real-time FCR below the target level without any distributional
assumption. For the selection with symmetric thresholds, we also provide an upper bound for
the real-time FCR value under the mild stability condition on the selection threshold. The
additional error in the FCR bound will vanish to zero in the asymptotic regime when the
threshold is the historical mean or quantile.
(3) To deal with the distribution shift in online data, we adjust the level of prediction intervals
whenever the selection happens by leveraging past feedback (coverage or miscoverage) at each
step. We further propose an algorithm by embedding CAS into the dynamically-tuned adaptive
conformal inference (DtACI) in Gibbs and Candès (2022). The new algorithm can achieve
long-run FCR control with properly chosen parameters under arbitrary distribution shifts.
(4) Through extensive experiments on both synthetic and real-world data, we demonstrate the
consistent superiority of our method over other benchmarks in terms of accurate FCR control
and narrow prediction intervals.
1.2 An illustrative application: drug discovery
In drug discovery, researchers examine the binding affinity of drug-target pairs on a case-by-case
basis to pinpoint potential drugs with high affinity (Huang et al., 2022). With the aid of machine
learning tools, we can forecast the affinity for each drug-target pair. If the predicted affinity is high,
we can select this pair for further clinical trials. To further quantify the uncertainty by predictions,
our method can be employed to construct prediction intervals with a controlled error rate.
Here, we apply CAS and the other two benchmarks to the DAVIS dataset (Davis et al., 2011).
The first one is ordinary online conformal prediction (OCP), which constructs the marginal conformal
intervals without consideration of the selection event. The other benchmark is LORD-CI in Weinstein
and Ramdas (2020). At each instance, we make a selection decision on a new drug-target pair. If
the predicted affinity is higher than the 70% sample quantile of recent 200 predictions, the new
drug-target pair is selected, and a corresponding prediction interval is constructed. Figure 2 visualizes
the real-time prediction intervals with target FCR level 10% constructed by different methods. The
simulation details are provided in Section 6. Our proposed method CAS (red ones) constructs the
shortest intervals with FCP at 10.01%. In contrast, both the OCP (blue ones) and the LORD-CI
(green ones) produce excessively wide intervals and yield very conservative FCP levels, 2.32% and
0.26%, respectively. Thus, the CAS emerges as a valid approach to accurately quantifying uncertainty
while simultaneously achieving effective interval sizes. By leveraging CAS, researchers can make
5CAS OCP LORD−CI
15
10
5
0
1200 1225 1250 1275 1300
Drug−target pair index
Figure 2: Plot for the real-time log-scale affinities and prediction intervals for selected points from index 1,200
to 1,300. The selected points are marked by the cross. The prediction intervals are constructed by three methods
with a target FCR level 10%. Red interval: CAS (FCP at index 1,300 is 10.01%); Blue interval: ordinary
online conformal prediction which provides marginal interval (FCP is 2.32%); Green interval: LORD-CI with
defaulted parameters (FCP is 0.26%).
informed decisions and implement reliable strategies in the pursuit of discovering promising new
drugs.
1.3 Related work
Our work is closely related to the post-selection inference on parameters or labels. Benjamini and
Yekutieli (2005) proposed the first method that can control FCR in finite samples by adjusting the
confidence level of the marginal confidence interval. Along this path, Weinstein et al. (2013), Zhao
(2022) and Xu et al. (2022) further investigated how to narrow the adjusted confidence intervals by
using more useful selection information. Another line of work is the conditional approach. Fithian
et al. (2014), Lee et al. (2016) and Taylor and Tibshirani (2018) proposed to construct confidence
intervals for each selected variable conditional on the selection event and showed that the FCR can
be further controlled if the conditional coverage property holds for an arbitrary selection subset.
Unfortunately, thosemethodsareimpracticalbecausetheyrequireatractableconditionaldistribution
given the selection condition, which is usually unknown in many real applications.
6
seitiniffa
elacs−goLInparticular,fortheproblemofonlineselectiveinference,WeinsteinandRamdas(2020)developed
two general solutions based on conditional and marginal procedures, respectively. To achieve real-
time FCR control, they required the conditional or marginal coverage property for the constructed
confidence intervals. Recently, Xu and Ramdas (2023) introduced a new approach called e-LOND-
CI, which utilizes e-values (Vovk and Wang, 2021) with LOND (Javanmard and Montanari, 2015)
procedure for online FCR control. This method alleviates the constraints on selection rules in
WeinsteinandRamdas(2020)andprovidesavalidFCRcontrolunderarbitrarydependence. However,
one drawback of the two existing methods is that the constructed interval tends to be extremely wide.
This issue is essentially caused by ignoring incorporating the selection condition into the construction
of intervals.
Conformal prediction (Vovk et al., 2005) is one important brick of our proposed method. As a
powerful tool for predictive inference, it provides a distribution-free coverage guarantee in both the
regression (Lei et al., 2018) and the classification (Sadinle et al., 2019). In addition to predictive
inference, conformalinferenceisalsobroadlyappliedtothetestingproblembyconstructingconformal
p-values (Bates et al., 2023; Jin and Candès, 2023a). We refer to Angelopoulos and Bates (2021);
ShaferandVovk(2008)formorecomprehensiveapplicationsandreviews. Theconventionalconformal
inference requires that the data points are exchangeable, which may be violated in practice. There are
several works devoted to conformal inference beyond exchangeability. When the feature shift exists
between the calibration set and the test set, Tibshirani et al. (2019) and Jin and Candès (2023b)
introduced a weighted conformal prediction interval and a weighted conformal p-value respectively
by injecting likelihood ratio weights. For general non-exchangeable data, Barber et al. (2023) used
a robust weighted quantile to construct conformal prediction intervals. For the online data under
distribution shift, Gibbs and Candès (2021, 2022) developed adaptive conformal prediction algorithms
based on the online learning approach. Besides, a relevant direction is to study the test-conditional
coverage P{Y ∈ I (X ) | X = x}, which has been proved impossible for a finite-length prediction
t t t t
interval without imposing distributional assumptions (Lei and Wasserman, 2014; Foygel Barber
et al., 2021). In contrast, our concerned selection-conditional coverage P{Y ∈ I (X ) | S = 1} could
t t t t
achieve a reliable finite-sample guarantee without distribution assumptions.
1.4 Outline
The remainder of this paper is organized as follows. Sections 2 and 3 present the construction of
the calibration set C(cid:98)t and the theoretical properties of CAS for decision-driven selection and online
selection with symmetric thresholds, respectively. Numerical results and real-data examples are
presented in Sections 5 and 6. Section 7 concludes the paper, and the technical proofs are relegated
to the Supplementary Material.
7Notations. Given any set S, we denote |S| the cardinality of S. Let S be an index set. Given real
numbers {r } and α ∈ (0,1), we use q ({r } ) to denote the ⌈(1−α)(|S|+1)⌉-st smallest value
i i∈S α i i∈S
in {r } . We use 1{·} to denote the indicator function.
i i∈S
2 Online selective conformal prediction for decision-driven selection
Suppose a prediction model µ(·) : Rn → R is pre-trained by an independent training set. To make
(cid:98)
sure that the prediction intervals can be constructed when t is small, we assume there exists an
independent labeled set denoted by {Z = (X ,Y )}−1 . The selection rule Π is generated from the
i i i i=−n t
previously observed data {Z } . We first summarize the general procedure of the online selective
i i≤t−1
conformal prediction in Algorithm 1, which is abbreviated as CAS in this paper.
Algorithm 1 Calibration after Adaptive Selection (CAS)
Input: Pre-trained model µ, initial holdout set {(X ,Y )}−1 , target FCR level α ∈ (0,1).
(cid:98) i i i=−n
1: Compute the residuals in initial holdout set {R i = |Y i−µ (cid:98)(X i)|}− i=1 −n.
2: for t = 0,1,... do
3: Observe the previous label Y t−1 and compute its residual R t−1 = |Y t−1−µ (cid:98)(X t−1)|.
4: Specify the selection rule Π t(·) according to the history data {(X i,Y i)} i≤t−1.
5: Compute the selection indicator S t = Π t(X t).
6: if S t = 1 then
7: Specifytheadaptiveselectionrules{ΠA t,sda(·)} s∈Ht accordingtothetriples{(Π t,Π s,X t)} s∈Ht.
8: Obtain indices of the calibration set C(cid:98)t = {s ∈ H t : ΠA t,sda(X s) = 1}.
9: Report the prediction interval: I tCAS(X t;α) = µ (cid:98)(X t)±q α({R i} i∈C(cid:98)t).
10: end if
11: end for
Remark 2.1. Throughout the paper, we use the absolute residual R(X,Y) = |Y −µ(X)| as the
(cid:98)
nonconformity score. It is straightforward to extend Algorithm 1 to general nonconformity scores,
such as the quantile regression (Romano et al., 2019) or distributional regression (Chernozhukov
et al., 2021). Let R(·,·) : Rn×R → R be a general nonconformity score function. We can replace the
prediction interval in Algorithm 1 with the following form
(cid:110) (cid:16) (cid:17)(cid:111)
ICAS(X ;α) = y ∈ R : R(X ,y) ≤ q {R(X ,Y )} .
t t t α i i i∈C(cid:98)t
All the theoretical results in our paper will remain intact with the prediction intervals defined above.
8In this section, we will investigate the online selective conformal prediction under a class of
selection rules defined below.
Definition 1. Let σ({S }t−1) be the σ-field generated by historical decisions {S }t−1. The online
i i=0 i i=0
selection rule is called decision-driven selection if Π (·) is σ({S }t−1)-measurable.
t i i=0
The decision-driven selection depends on the historical data only through the decisions. For
example, one can choose S = 1{µ(X ) ≤ c }, where c = C +C ((cid:80)t−1S ) for constants C ,C .
t (cid:98) t t t 1 2 i=0 i 1 2
It is more flexible than choosing a constant c ≡ C as the threshold since we incorporate the
t 1
current selection number to dynamically adjust the selection rule. Besides, many online error rate
control algorithms (Foster and Stine, 2008; Aharoni and Rosset, 2014) also fall in the category of
decision-driven rules, and we will discuss them in Section 2.3. Before exploring the application of
CAS under decision-driven selection rules, we introduce the following assumption for FCR control.
Assumption 1. The decision-driven selection rules {Π (·)} are independent of the initial holdout
t t≥0
set {Z }−1 .
i i=−n
Sincetheinitialholdoutset{Z }−1 isonlyusedforcalibration,andtheselectionruleΠ depends
i i=−n t
on the previous decisions by Definition 1, Assumption 1 is reasonable for most scenarios. We notice
that Weinstein and Ramdas (2020) required the confidence interval I (·) to be σ({S }t−1)-measurable,
t i i=0
which means the previously observed data {Z }t−1 cannot be used for calibration at time t and the
i i=0
holdout set needs to be fixed as {Z }−1 . We first regard this case as a warm-up and show that
i i=−n
CAS with a non-adaptive selection on the holdout set is enough to control FCR. Then in the case of
the full holdout set, we show the non-adaptive selection may fail and present a novel construction for
the adaptive selection rules to select calibration data points.
2.1 Warm-up: fixed holdout set
When the holdout set is fixed as {Z }−1 in the entire online process, under Assumption 1, the
i i=−n
selection indicators Π (X ) and Π (X ) for −n ≤ s ≤ −1 are exchangeable because Π is independent
t t t s t
of both X and X . Therefore, the non-adaptive selection on the fixed holdout set is enough to
t s
guarantee the selection-conditional coverage. If S = 1, we perform the current selection rule Π on
t t
{X }−1 and obtain the following calibration candidates
i i=−n
C(cid:98) tfix = {−n ≤ i ≤ −1 : Π t(X i) = 1}. (1)
The next theorem shows that FCR can be exactly controlled below α.
Theorem 1. Under Assumption 1, the Algorithm 1 with C(cid:98)t = C(cid:98) tfix defined in (1) satisfies: (1) For
any T ≥ 0, FCR(T) ≤ α. (2) Let p = P(cid:8) S = 1 | σ({S }t−1)(cid:9) . Further, if the residuals {R }T
t t i i=0 i i=−n
9are distinct and
(cid:80)T
S > 0 almost surely, we also have the following lower bound,
t=0 t
FCR(T) ≥
α−(cid:88)T E(cid:34)
S
t
(cid:26)
1−(1−p
t)n+1(cid:27)(cid:35)
. (2)
t=0 (cid:80)T t=0S t (n+1)p t
The results in Theorem 1 show that the CAS can achieve a finite-sample and distribution-free
control for FCR. Similar to the marginal coverage of split conformal (Lei et al., 2018), we also
have an anti-conservative guarantee in (2) when the residuals are continuous. In fact, the quantity
(n + 1)p
t
characterizes the size of the selected calibration set C(cid:98) tfix. If the selection probability
keeps away from zero, or min p = O (1), then the lower bound (2) becomes FCR(T) ≥
0≤t≤T t p
α−O(cid:8) (n+1)−1(cid:9). Consequently, we can guarantee the exact FCR control in the asymptotic regime,
i.e., lim FCR(T) = α.
(n,T)→∞
For completeness and comparison, we also provide the construction and validity of the online
adjusted method (named LORD-CI) proposed by Weinstein and Ramdas (2020) in the conformal
setting. Given any σ({S }t−1)-measurable coverage level α ∈ (0,1), we can construct a marginal
i i=0 t
split conformal prediction interval as
Imarg(X ;α ) = µ(X )±q ({R } ). (3)
t t t (cid:98) t αt i −n≤i≤−1
The prediction interval (3) can serve as a recipe for LORD-CI by dynamically updating the marginal
level α to maintain the following invariant
t
(cid:80)T
α
t=0 t ≤ α, for any T ≥ 0. (4)
1∨(cid:80)T
S
i=0 i
We refer to Weinstein and Ramdas (2020) and literature within for explicit procedures in constructing
the sequence {α } satisfying (4). Under the same conditions for the selection rule, we can obtain
t t≥0
the FCR control results for LORD-CI in the conformal setting.
Proposition 2.1. Let {S j}T
j=0
and {S(cid:101)j}T
j=0
be two decision sequences, suppose S
t
≥ S(cid:101)t holds
whenever S
j
≥ S(cid:101)j for any j ≤ t−1. Under Assumption 1, if α
t
∈ σ({S i}t i=− 01) for any t ≥ 0 and the
invariant (4) holds, the LORD-CI algorithm satisfies that FCR(T) ≤ α for any T ≥ 0.
Despite that LORD-CI can successfully control the real-time FCR, the prediction interval
Imarg(X ;α ) tends to be wider as t grows because α is shrinking to maintain the invariant (4). The
t t t t
prediction intervals of CAS will be relatively narrower due to the constant miscoverage level α, which
is also confirmed by our numerical results in Section 5.
2.2 Full holdout set
Since we can observe new labels at each time step, it is more efficient to use the full holdout set H
t
to include new labeled data points for further calibration. However, using the full holdout set results
10in additional dependence between the current decision S and historical data {Z : 0 ≤ s ≤ t−1} in
t s
the holdout set. The next theorem shows that the Algorithm 1 with a non-adaptive selection may
fail in controlling FCR.
Theorem 2. Suppose Assumption 1 holds. The Algorithm 1 with C(cid:98)t = C(cid:98) tnaive = {s ∈ H
t
: Π t(X s) = 1}
satisfies that for any T ≥ 0,
T t−1 (cid:34) (cid:35)
FCR(T) ≤ α+(cid:88)(cid:88) E S t Π t(X s) |Π (X )−Π (X )| . (5)
t=0 s=0
1∨(cid:80)T
t=0S t|C(cid:98)t|+1
s s s t
Notice that the product indicator S t·1{s ∈ C(cid:98) tnaive} for 0 ≤ s ≤ t−1, that is Π t(X t)Π t(X s), has
an non-symmetric dependence on the features X and X . In fact, the selection rule Π is independent
t s t
of X but relies on {X } through historical decisions {S } . This leads to an error
t s 0≤s≤t−1 s 0≤s≤t−1
term in (5), which disappears whenever Π (X ) = Π (X ). An intuitive remedy is excluding the case
s s s t
where the error is not zero in the construction of C(cid:98)t, then we have the following intersection subset,
C(cid:98) tinter = C(cid:98) tnaive∩{0 ≤ s ≤ t−1 : Π s(X s) = Π s(X t)}. (6)
NowletusanalyzewhyS t·1{s ∈ C(cid:98) tinter}issymmetricwithrespectiveto(X t,X s)for0 ≤ s ≤ t−1.
After expansion, we can equivalently write the product indicator as
S t1{s ∈ C(cid:98) tinter} = Π t(X t)Π t(X s)Π s(X t)Π s(X s)+Π t(X t)Π t(X s)[1−Π s(X t)][1−Π s(X s)].
Notice that if S = 1 for 0 ≤ s ≤ t−1, we can replace X with some x∗ ∈ σ({S } ) such that
s s s i i≤s−1
Π s(x∗ s) = 1. It will generate a sequence of virtual selection rules, denoted by {Π(cid:101)( js) } j≥s+1. By the
Definition 1, we know Π(cid:101)( ts) is identical to the real selection rule Π
t
under the event {Π s(X s) = 1}.
Then we can write the first term in S t·1{s ∈ C(cid:98) tinter} as
(s) (s)
Π t(X t)Π t(X s)Π s(X t)Π s(X s) = Π(cid:101)
t
(X t)Π(cid:101)
t
(X s)Π s(X t)Π s(X s),
which is symmetric on (X s,X t) since both Π(cid:101)( ts) and Π
s
are independent of (X s,X t). Similarly, we
can also show the second term in S t·1{s ∈ C(cid:98) tinter} is also symmetric on (X s,X t).
Theorem 3. Suppose Assumption 1 holds. The Algorithm 1 with C(cid:98)t = C(cid:98) tinter defined in (6) satisfies
that (1) FCR(T) ≤ α for any T ≥ 0; (2) P(cid:8) Y ∈ ICAS(X ;α) | S = 1(cid:9) ≥ 1−α for any t ≥ 0 such
t t t t
that P(S = 1) > 0.
t
After a modification to the selected calibration set, the Algorithm 1 was guaranteed to have a
finite-sample and distribution-free control of FCR. As a price, the conformal prediction intervals
based on the construction (6) may lose power compared with the naive version since C(cid:98)inter ⊆ C(cid:98)naive
t t
always holds. The empirical comparison of the two approaches is investigated in the Supplementary
Material.
112.3 Selection with online multiple testing procedure
In this subsection, we apply the CAS to online multiple-testing problems in the framework of
conformal inference. Given any user-specified thresholds {c } , we have a sequence of hypotheses
t t≥0
defined as
H : Y ≤ c , for t ≥ 0. (7)
0,t t t
At time t, we need to make the real-time decision whether to reject H or not. In this vein,
0,t
constructing prediction intervals for the rejected candidates is a post-selection predictive inference
problem. Therefore, the Algorithm 1 can be applied to any online multiple-testing procedure, and
can also achieve a finite-sample control for FCR if this procedure is decision-driven as Definition 1.
To control FDR in the online setting, Foster and Stine (2008) proposed the first method called
the alpha-investing algorithm. Then Aharoni and Rosset (2014) extended it to the generalized
alpha-investing (GAI) algorithm. After that, a series of works developed several variants of the
GAI algorithm, such as LORD, LOND (Javanmard and Montanari, 2015), LORD++ (Ramdas
et al., 2017) and SAFFRON (Ramdas et al., 2018). Suppose we have access to a series of p-values
{p } , where p is independent of samples in holdout set H . Given the target level β ∈ (0,1),
t t≥0 t t
these procedures proceed by updating the significance level β based on historical information and
t
rejecting H if p ≤ β . Fortunately, all these online procedures are decision-driven selections, and
0,t t t
CAS can naturally provide a theoretical guarantee for FCR control.
Conformal inference provides an approach to convert the prediction µ(X ) to a valid p-value for
(cid:98) t
H , which is called conformal p-value (Vovk et al., 2005). In the offline setting, Jin and Candès
0,t
(2023a) investigated the multiple testing problem in (7) by introducing a new conformal p-value. The
authors also proved that the Benjamini-Hochberg procedure can successfully control the the false
discovery rate (FDR) below the target level.
Remark 2.2. It is worthwhile noticing that conformal p-values {p } are not independent since
t t≥0
they are constructed from the same labeled set. Bates et al. (2023) verified that {p } are positively
t t≥0
dependentonasubset(PRDS),aspecialdependenceconditionintheofflineFDRliterature(Benjamini
and Yekutieli, 2001). To achieve a finite-sample control, most online FDR methods require that
the p-values are independently super-uniform or conditionally super-uniform. In particular, Zrnic
et al. (2021) proved that the original LOND procedure and its generalization can control FDR under
the PRDS condition. In our paper, we focus on the online FCR control for any selection selection
procedures deployed by users or analysts. Therefore, we can still wrap Algorithm 1 around other
online multiple-testing procedures as long as they are decision-driven.
123 Online selection with symmetric thresholds
In the decision-driven selection rules, the influence of historical data on the current selection rule
is entirely determined by past decisions. It may be inappropriate for some cases where the analyst
wants to leverage the empirical distribution of historical data to select candidates. To adapt this
scenario, we will rewrite the selection rule in a threshold form. Let V(·) : Rd → R be a user-specific
or pre-trained score function used for selection, and then denote V = V(X ) for i ≥ −n. For ease of
i i
presentation, we rewrite the selection rule as
S = Π (X ) = 1{V ≤ A ({V } )} for any t ≥ 0, (8)
t t t t t i i∈Ht
where {A : Rt+n → R} is a sequence of deterministic functions. For example, if A outputs the
t t≥0 t
sample mean or sample quantile of {V } , then the corresponding selection rule is symmetric.
i i∈Ht
This class of selection rules has not been studied in Weinstein and Ramdas (2020). Specially, the
selection function is assumed to be symmetric.
Definition 2. The threshold function A is symmetric if A ({V } ) = A ({V } ) where π is
t t i i∈Ht t π(i) i∈Ht
a permutation of H .
t
In this section, westill conductselectionon the full holdoutset H = {−n,...,t−1}. To calibrate
t
the conditional miscoverage probability, we need to measure the uncertainty of the product S 1{Y ̸∈
t t
I tCAS(X t)} through the selected calibration data in C(cid:98)t. In this context, we expect that selection
indicators 1{s ∈ H } and S are exchangeable for any s ≤ t−1. According to the naive construction
t t
of the selected calibration set, if S = 1, we shall use the same threshold to perform screening
t
on history scores {V i} i∈Ht, and then obtain C(cid:98) tnaive = {s ∈ H
t
: V
s
≤ A t({V i} i∈Ht)}. However, the
corresponding selection indicators 1{V > A ({V } )} for s ∈ H and 1{V ≤ A ({V } )} are
s t i i∈Ht t t t i i∈Ht
not exchangeable even when the threshold function A is symmetric. To construct an exchangeable
t
selection indicator, one natural and viable solution is swapping the score from the calibration set V
s
and the score from the test set V in the definition (8) of S , which leads to
t t
C(cid:98) tswap = {−n ≤ s ≤ t−1 : V
s
≤ A t({V i} i∈Ht,i̸=s,V t)}. (9)
Assigning C(cid:98)t = C(cid:98) tswap in Algorithm 1, we can report the corresponding conditional prediction interval
for Y as stated. The next theorem guarantees that an exact selection-conditional coverage can be
t
guaranteed after swapping.
Theorem 4. If the selection functions {A } are symmetric as Definition 2, then the Algorithm
t t≥0
1 with C(cid:98)t = C(cid:98) tswap defined in (9) satisfies P(cid:8) Y
t
∈ I tCAS(X t;α) | S
t
= 1(cid:9) ≥ 1−α for any t ≥ 0 such
that P(S = 1) > 0.
t
13To control FCR of CAS, we impose the stability condition to bound the change of A ’s output
t
after replacing V with an independent copy V.
s
Assumption 2. There exists a sequence of positive real numbers {σ } such that,
t t≥0
max
E(cid:2)(cid:12)
(cid:12)A t({V i} i≤t−1)−A t({V i}
i≤t−1,i̸=s,V)(cid:12)
(cid:12) | {V i}
i≤t−1,i̸=s(cid:3)
≤ σ t,
−n≤s≤t−1
where V is an i.i.d. copy of V .
s
Since two sets {V } and {V } ∪{V} only differs one data point, the definition of σ
i i≤t−1 i i≤t−1,i̸=s t
in Assumption 2 is similar to the global sensitivity of A in the differential privacy literature (Dwork
t
et al., 2006). Our assumption is weaker than the global sensitivity because the bound is added to the
expectation taken on V and V.
s
Theorem 5. Suppose the density function of V is upper bounded by ρ > 0. If the symmetric function
i
A
t
satisfies Assumption 2 for any t ≥ 0. For the Algorithm 1 with C(cid:98)t = C(cid:98) tswap, it holds that
   
FCR(T) ≤ α· 1+E
(cid:16)1{(cid:80)T
j=0S j > 0} (cid:17)ϵ(T) + 9  , (10)

(cid:80)T
S −ϵ(T) ∨1
T +n

j=0 j
√
where ϵ(T) = 2(cid:80)T−1σ +3( eρ+1)log(T +n)+2−1.
j=0 j
To deal with the complicated dependence between selection and calibration, Bao et al. (2024)
imposed a condition on the joint distribution for the pair of residual and score (R ,V ). Credited to
i i
the swapping design of C(cid:98)t, this assumption is avoided in this paper. The distributional assumptions
in Theorem 5 are relatively mild and can be verified in many practical applications.
Remark 3.1. To analyze FCR, we need to decouple the dependence between the numerator and
the denominator. The conventional leave-one-out analysis in online error rate control does not
work for the selection function A . In the proof of Theorem 5, we address this difficulty by using
t
the exchangeability of data and symmetricity of A . We construct a sequence of virtual decisions
t
{S(s←t) }t−1 by replacing V with V in the real decisions {S }t−1. Since the function A is symmetric,
j j=s s t j j=s j
we can guarantee that S(s←t) and S have the same distribution. The additional error ϵ(T) in (10)
j j
comes from the difference term
(cid:80)t−1S −S(s←t)
, which can be bounded via empirical Bernstein’s
j=s j j
inequality.
Next, we will show that the error ϵ(T) can be upper bounded by a logarithmic factor with high
probability when A returns the historical mean or quantile.
t
Proposition 3.1. Suppose A returns the sample mean of history scores
(cid:80)t−1
V /(t + n). If
t i=−n i
√
E[|V |] ≤ σ for some σ > 0, then we have ϵ(T) ≤ 4( eρ+σ+1)log(T +n).
i
14Proposition 3.2. Suppose A returns the ϑ-th sample quantile of history scores {V }t−1 for
t i i=−n
ϑ ∈ (0,1]. If {V }T are continuous and max{4logn,3} ≤ n, then with probability at least
i i=−n
1−(T +n)−2, we have ϵ(T) ≤ 36log2(T +n).
For the mean-based selection, the stability assumption reduces to the bound on the first-order
moment of V , which is quite weak in traditional statistical literature. For the quantile-based selection,
i
the ρ-bounded density condition is dropped because we can always apply selection on the transformed
scores {F (V )} and get the same decision sequence and selected calibration set, where F is the
v i i≥0 v
cumulative distribution function of V . Plugging the upper bounds in Propositions 3.1 and 3.2 into
i
(10), we can still have a asymptotically valid control for FCR if log(T +n)/((cid:80)T S ) = o (1) for
j=0 j p
these two cases.
4 CAS under distribution shift
In some online settings, the exchangeable (or i.i.d.) assumption on the data generation process
does not hold anymore, in which the distribution of Z may vary smoothly over time. Without
t
exchangeability, the marginal coverage cannot be guaranteed. Gibbs and Candès (2021) developed an
algorithm named adaptive conformal inference (ACI), which updates the miscoverage level according
to the historical feedback on under/over coverage. Given any marginal target level, the ACI updates
the current miscoverage level by
α = α +γ(α−1{Y ̸∈ I (X ;α )}), (11)
t t−1 t−1 t−1 t−1 t−1
where γ > 0 is the step size parameter. The ACI update rule is equivalent to a gradient descent step
on the pinball loss (Gibbs and Candès, 2022). Define the random variable β = sup{β ∈ [0,1] : Y ∈
t t
I (X ;β)} and the respective pinball loss ℓ(θ;β ) = α(β −θ)−min{0,β −θ}. Then we can write
t t t t t
(11) as
α = α −γ∇ ℓ(α ;β ). (12)
t t−1 θ t−1 t−1
After translating the ACI into an online convex optimization problem over the losses {ℓ(·;β )} ,
t t≥0
Gibbs and Candès (2022) recently proposed a dynamically-tuned adaptive conformal inference
(DtACI) algorithm by employing an exponential reweighting scheme (Vovk, 1990; Gradu et al., 2023;
Wintenberger, 2017), which can dynamically estimate the optimal step size γ.
The original motivation of ACI and DtACI is to achieve approximate marginal coverage by
reactively correcting all past mistakes. For the selective inference problem, we aim to control the
conditional miscoverage probability through historical feedback. In this vein, we may replace the
fixed confidence level α in Algorithm 1 with an adapted value α by conditionally correcting past
t
15Algorithm 2 Selective DtACI with CAS
Input: Set of candidate step-sizes {γ }k , starting points {αi}k , tuning parameter sequence
i i=1 0 i=1
{ϕ ,η }T , initial weights {wi}k and initial state τ = 0.
t t t=0 0 i=1
1: for t = 0,...,T do
2: if S t = 1 then
3: Define pi
τ
= w τi/(cid:80)k j=1w τj for 1 ≤ i ≤ k;
4: Assign α t = α τi with probability pi τ;
5: Call Algorithm 1 and return I tCAS(X t;α t);
6: erri t ← 1{Y t ̸∈ I tCAS(X t;α t)} for 1 ≤ i ≤ k;
7: β t ← sup{β ∈ [0,1] : Y t ∈ I tCAS(X t;α t)};
8: α ti ← α τi +γ i(α−erri t) for 1 ≤ i ≤ k;
9: w¯ ti ← w τi exp(cid:8) −η tℓ(β t,α ti)(cid:9) for 1 ≤ i ≤ k;
10: w ti ← (1−ϕ t)w¯ ti+ϕ t(cid:80)k j=1w¯ tj/k for 1 ≤ i ≤ k;
11: Set τ ← t;
12: end if
13: end for
mistakes whenever the selection happens. If S = 1, we firstly find the most recent selection time
t
τ ≤ t−1 that S = 1. Define the new random variable βCAS = sup{β ∈ [0,1] : Y ∈ ICAS(X ;α )}.
t τt τt τt τt t τ
Parallel with (12), we update the current confidence level through one step of gradient descent on
ℓ(α ;βCAS), i.e.,
τt τt
α = α −γ∇ ℓ(α ;βCAS).
t τt θ τt τt
Deploying the exponential reweighting scheme, we can also get a selective DtACI algorithm, and
summarize it in Algorithm 2. By slightly modifying the Theorem 3.2 in Gibbs and Candès (2022),
we can obtain the following control result on FCR.
Theorem 6. Let γ
min
= min iγ i, γ
max
= max iγ
i
and ϱ
t
= (1+ γ2 mγm inax)2 η teηt(1+2γmax) + 2(1 γ+ mγ im nax) ϕ t.
Suppose (cid:80)T S > 0 almost surely. Under arbitrary distribution shift on the data {Z }T , the
j=0 j i i≥−n
Algorithm 2 satisfies that
1+2γ
(cid:34)
1
(cid:35) (cid:34)(cid:80)T
S ϱ
(cid:35)
|FCR(T)−α| ≤ maxE +E t=0 t t ,
γ min (cid:80)T j=0S j (cid:80)T j=0S j
where the expectation is taken over the randomness from both the data {Z }T and the Algorithm 2.
i i=−n
By observing the upper bound in the theorem, if lim η = lim ϕ = 0 and
t→∞ t t→∞ t
lim (cid:80)T S = ∞, we can guarantee lim FCR(T) = α. In Gibbs and Candès (2022),
T→∞ j=0 j T→∞
16the authors advocated using constant or slowly changing values for η to achieve approximate
t
marginal coverage. However, our final goal is to control the FCR value of the Algorithm 2. Therefore,
it is more appropriate to use the decaying η in our setting.
t
5 Synthetic experiments
The validity and efficiency of our proposed method will be examined via extensive numerical studies.
We focus on the setting using a full holdout set, and the results for the fixed holdout set are provided
in the Supplementary Material. To mitigate computational costs, we adopt a windowed scheme,
utilizing only the most recent 200 data points as the holdout set. Importantly, the theoretical
guarantee remains intact, and additional details regarding the windowed scheme can be found in
the Supplementary Material. Unless stated otherwise, the numerical experiments all adhere to this
windowed scheme.
TheevaluationmetricsinourexperimentsareempiricalFCRandaveragelengthoftheconstructed
prediction intervals across 500 replications. For each replication and time point T, we calculate the
current FCP and the average length of all constructed intervals up to current time. By averaging
these values across replications, we derive the real-time FCR level and average length. Here we
address that measuring selection-conditional coverage poses challenges in our online setting due to
the varying selection conditions at each time step. Consequently, our evaluation primarily focuses on
the FCR metric, which also serves as an empirical proxy for selection-conditional coverage.
5.1 Results for i.i.d. settings
The i.i.d. settings are considered first. We generate i.i.d. 10-dimensional features X from uniform
i
distribution Unif([−2,2]10) and and the corresponding responses Y = µ(X )+ϵ . We explore three
i i i
distinct data-generating scenarios with varied configurations of µ(·) and distributions of ϵ ’s.
i
• Scenario A: Linear model with heterogeneous noise. µ(X) = X⊤β where β = (1⊤,−1⊤)⊤
5 5
and 1 is a 5-dimensional vector with all element 1. The noise is heterogeneous and follows the
5
conditional distribution ϵ | X ∼ N(0,{1+|µ(X)|}2).
• Scenario B: Nonlinear model. µ(X) = X(1)+2X(2)+3(X(3))2, where X(k) denotes the k-th
element of vector X and ϵ ∼ N(0,1) is independent of X.
• Scenario C: Aggregation model. µ(X) = 4(X(1) + 1)|X(3)|1{X(2) > −0.4} + 4(X(1) −
1)1{X(2) ≤ −0.4}. The noise follows ϵ ∼ N(0,{1+|X(4)})).
17To train the regression model µ(·), we utilize an independent labeled set with a size of 200. For
(cid:98)
Scenario A, we employ ordinary least squares (OLS) as the basic prediction model. In Scenario B,
we utilize support vector machines (SVM), and for Scenario C, we employ random forest (RF).
To conduct comprehensive comparisons with our proposed CAS, we also evaluate the performance
of the other two benchmark methods. The first one is the Online Ordinary Conformal Prediction
(OCP). It constructs the prediction interval using the whole holdout data and ignores the selection
effects. The second one is the LORD-CI (Weinstein and Ramdas, 2020). The parameters we use are
in default. Besides, we have also considered the e-LOND-CI method proposed by Xu and Ramdas
(2023), which is based on e-values and the LOND procedure. However, our empirical studies reveal
that it exhibits excessive conservative FCR and yields significantly wider interval lengths compared
to other benchmarks. Consequently, we include the results of this approach in the Supplementary
Material for reference.
Several selection rules are considered as follows. The first selection rule is a simple one.
1) Fixed: A selection rule with a fixed threshold posed on the first component of the feature,
which is S
t
= 1{X t(1) > 1}. We will use the naively selected calibration set C(cid:98) tnaive = {s ∈ H
t
:
Π (X ) = 1} for the fixed selection rule.
t s
The next two selection rules are decision-driven selection rules in Section 2. We consider to select
calibration set by Cˆinter instead of C(cid:98)naive.
t
2) Dec-driven: At each time t, the selection indicator is S = 1{µ(X ) > τ((cid:80)t−1S )}, and
t (cid:98) t i=0 i
τ(s) = τ −min{s/50,2} where the parameter τ is pre-fixed for each scenario.
0 0
3) Mul-testing: Selection with online multiple testing procedure using the SAFFRON (Ramdas
et al., 2018) with defaulted parameters. The hypotheses are defined by H : Y ≤ τ −1. We
0t t 0
set the target FDR level β = 20% and require an additional independent labeled data of size
500 to construct p-values. The detailed procedure is shown in the Supplementary Material.
The thresholds of the following two selection rules are symmetric to the recent 200 samples. Thus we
adopt the C(cid:98)swap as the selected calibration set, which is defined in Section 3.
t
4) Quantile: S = 1{µ(X ) > A({µ(X )}t−1 )}, where A({µ(X )}t−1 ) is the 70%-quantile
t (cid:98) t (cid:98) i i=t−200 (cid:98) i i=t−200
of the {µ(X )}t−1 .
(cid:98) i i=t−200
5) Mean: S = 1{µ(X ) > (cid:80)t−1 µ(X )/200}.
t (cid:98) t i=t−200(cid:98) i
We set the initial holdout data size at n = 50. The results, depicted in Figure 3, showcase the
performance of the benchmarks for the full holdout set across different scenarios and selection rules.
These results affirm the effectiveness of CAS in controlling the real-time FCR. Additionally, the
18Method CAS OCP LORD−CI
Fixed Dec−driven Mul−testing Quantile Mean
15
10
5
20
15
10
5
20
10
100 500 900 100 500 900 100 500 900 100 500 900 100 500 900
Time
Fixed Dec−driven Mul−testing Quantile Mean
12
10
8
14
12
10
22.5
20.0
17.5
15.0
12.5
100 500 900 100 500 900 100 500 900 100 500 900 100 500 900
Time
Figure 3: Real-time FCR and average length from time 20 to 1,000 for different scenarios and selection rules.
The black dashed line denotes the target FCR level 10%.
19
)%(RCF
htgneL
egarevA
Scenario
A
Scenario
B
Scenario
C
Scenario
A
Scenario
B
Scenario
C5 10 5 10 5 10
n n n
25 50 25 50 25 50
CAS OCP LORD−CI
21
16
25
18
12
20
15
8
15
12
4
10
0 100 200 300 0 100 200 300 0 100 200 300
Time Time Time
CAS OCP LORD−CI
16.0
12.0
10.3
10.0 13.5
10.5
9.7
9.0 11.0
0 100 200 300 0 100 200 300 0 100 200 300
Time Time Time
Figure 4: Real-time FCR and average length from time 20 to 400 using different sizes of the initial holdout
set for CAS, OCP and LORD-CI. The basic setting is Scenario B and the quantile selection rule is used. The
black dashed line denotes the target FCR level 10%.
empirical performance of CAS demonstrates anti-conservatism, achieving stringent FCR control with
reduced interval lengths. In comparison, it is observed that OCP fails to provide valid FCR control
in the majority of cases, and LORD-CI yields notably wide intervals. Consequently, neither method
is able to offer satisfactory prediction intervals.
Next we assess the impact of the initial holdout set size n. For simplicity, we focus on Scenario
B and employ the quantile selection rule. We vary the initial size n within the set {5,10,25,50},
and summarize the results among 500 repetitions in Figure 4. When the initial size is small, the
CAS tends to exhibit overconfidence at the start of the stage. However, as time progresses, the FCR
level approaches the target of 10%. Conversely, with a moderate initial size such as 25, the CAS
achieves tight FCR control throughout the procedure, thereby confirming our theoretical guarantee.
A similar phenomenon is also observed with OCP and LORD-CI, wherein the FCR at the initial stage
20
RCF
htgneL
egarevA
RCF
htgneL
egarevA
RCF
htgneL
egarevAsignificantly diverges from the FCR at the end stage when a small value of n is utilized. To ensure a
stabilized FCR control throughout the entire procedure, we recommend employing a moderate size
of for the initial holdout set.
5.2 Evaluation under distribution shift
We have also conducted a comparative analysis of our proposed CAS-DtACI with CAS, and the
original DtACI across various distribution shift settings. To implement DtACI, we first choose a fixed
interval size of I = 200, a target FCR level of α = 10% and a candidate number of k = 6. Other
parameters are determined following the suggestions provided by Gibbs and Candès (2022). For
the DtACI, we let ϕ = ϕ = 1/(2I), η = η = (cid:112) {3log(kI)+6}/{I(1−α)2α3+Iα2(1−α)2} and
t 0 t 0
candidate step-sizes {γ }6 = {0.008,0.0160,0.032,0.064,0.128,0.256}. The starting points αi = α
i i=1 0
for i = 1,··· ,8. As for CAS-DtACI, we employ the same parameters except that we choose decaying
learning parameters ϕ = ϕ ((cid:80)t S )−0.501 and η = η ((cid:80)t S )−0.501.
t 0 i=0 t t 0 i=0 t
We consider four different settings for our analysis. The first one is the i.i.d. setting which is
the same as Scenario B. The second one is a slowly shifting setting. The training and initial labeled
data follow the same distribution as that of Scenario B, while the online data gradually drifts over
time according to the equation Y = (1−t/500)X(1) +(2+sinπt/200)X(2) +(3−t/500)(X(3) )2+ε ,
t t t t t
where X ∼ Unif([−2,2])10 and ε ∼ N(0,1). The third setting is based on a change point model.
t
When t ≤ 200, the data is generated as described in Scenario B. But after t > 200, the data follows
a different pattern given by Y = −2X(1) −X(2) +3(X(3) )2 +ε . The last setting is a time series
t t t t t
model, where Y = {2sinπX(1) X(2) +10(X(3) )2+5X(4) +2X(5) +ξ }/4 and ξ is generated from an
t t t t t t t t
ARMA(0,1) process, specifically ξ = 0.99ξ +ε +0.99ε .
t+1 t t+1 t
For the sake of simplicity, we have focused solely on the quantile selection rule as previously
described. The initial data size, training data size, and prediction algorithm remain consistent
with those in Scenario B. The results are illustrated in Figure 5. It is evident that the original
DtACI consistently exceeds the target level for FCR across all four settings, as it does not account
for selection effects. The CAS method is only effective for the i.i.d. setting, whereas CAS-DtACI
demonstrates reliable FCR control across various settings where exchangeability is violated.
6 Real data applications
6.1 Drug discovery
In Section 1.2, we consider the application of drug discovery. Here we present the details and some
additional results. The DAVIS dataset (Davis et al., 2011) consists of 25,772 drug-target pairs, each
accompanied by the binding affinity, structural information of the drug compound, and the amino
21Method CAS−DtACI CAS DtACI
i.i.d. Slowly shift Change point Time series
17.5
15.0
12.5
10.0
500 1000 1500 500 1000 1500 500 1000 1500 500 1000 1500
Time
i.i.d. Slowly shift Change point Time series
20
15
10
500 1000 1500 500 1000 1500 500 1000 1500 500 1000 1500
Time
Figure 5: Comparison for CAS-DtACI, CAS and DtACI by real-time FCR and average length from time 100
to 2,000 for quantile selection rule under different data-generating settings. The black dashed line represents
the target FCR level 10%.
acid sequence of the target protein. Using the Python library DeepPurpose (Huang et al., 2020), we
encode the drugs and targets into numerical features, while the log-scale affinities are considered
as the response variables. To train our model, we randomly sample 15,000 observations from the
dataset to create the training set. We employ a small neural network model with 3 hidden layers and
perform 5 epochs to generate predictions. Additionally, we set aside another 2,000 observations as
the online test set, and an initial holdout set consisting of 50 data points is reserved.
Our objective is to develop real-time prediction intervals for the affinities of selected drug-
target pairs. We explore four distinct selection rules in this pursuit, including fixed selection rule
S = 1{µ(X ) > 9}; decision-driven rule with S = 1{µ(X ) > 8+min{(cid:80)t S /400,1}}; online
t (cid:98) t t (cid:98) t j=0 j
multiple testing rule using SAFFRON, which tests H : Y ≤ 9 with FDR level at 20% and requires
0t t
another 1,000 independent labeled samples to construct conformal p-values; quantile selection rule,
22
)%(RCF
htgneL
egarevAwhich is S = 1{µ(X ) > A({µ(X )}t−1 )}, where A({µ(X )}t−1 ) is the 70%-quantile of the
t (cid:98) t (cid:98) i i=t−200 (cid:98) i i=t−200
{µ(X )}t−1 . The quantile selection rule is also used in Figure 2.
(cid:98) i i=t−200
Method CAS OCP LORD−CI
Fixed Dec−driven Mul−testing Quantile
12
9
6
3
0
500 1000 1500 500 1000 1500 500 1000 1500 500 1000 1500
Time
Fixed Dec−driven Mul−testing Quantile
16
12
8
4
500 1000 1500 500 1000 1500 500 1000 1500 500 1000 1500
Time
Figure 6: Real-time FCR and average length from time 20 to 2,000 by 50 repetitions for drug discovery. The
black dashed line denotes the target FCR level 10%.
Figure 6 depicts the real-time FCR and average length of PIs based on three methods across 50
runs. The results illustrate that as time progresses, the FCR of CAS closely aligns with the nominal
level of 10%, validating our theoretical findings. In contrast, both OCP and LORD-CI exhibit a
tendency to yield conservative FCR values, consequently leading to unsatisfactorily lengthy prediction
intervals. This observation is consistent with the findings presented in Figure 2. Additionally, given
that the true log-scale affinities fall within the range of (−5,10), excessively wide intervals would
offer limited utility for guiding further decisions.
23
)%(RCF
htgneL
egarevA6.2 Airfoil self-noise
Airflow-induced noise prediction and reduction is one of the priorities for both the energy and aviation
industries (Brooks et al., 1989). We consider applying our method to the airfoil data set from the UCI
Machine Learning Repository (Dua and Graff, 2017). It involves 1,503 observations of a response Y
(scaled sound pressure level of NASA airfoils), and a five-dimensional feature including log frequency,
angle of attack, chord length, free-stream velocity, and suction side log displacement thickness. The
data is obtained via a series of aerodynamic and acoustic tests, and the distributions of the data
are in different patterns at different times. We can view this dataset as having shifting distribution
over time, and the CAS-DtACI is implemented with the same parameters in Section 5.2 to solve this
problem.
Method CAS−DtACI CAS OCP
Fixed Dec−driven Quantile Mean
40
30
20
10
0 250 500 750 0 250 500 750 0 250 500 750 0 250 500 750
Time
Fixed Dec−driven Quantile Mean
28
24
20
16
0 250 500 750 0 250 500 750 0 250 500 750 0 250 500 750
Time
Figure 7: Real-time FCR and average length from time 20 to 900 by aggregating 20 different choices of online
data set for four selection rules for airfoil task. The black dashed line denotes the target FCR level 10%.
We reserve the first 480 samples as a training set to train an SVM model with defaulted
parameters. Then we use the following 23 samples as the initial holdout set. Since the data
24
)%(RCF
htgneL
egarevAis in time order, we take an integrated period of size 900 from the remaining samples as the
online data set. We treat each choice of the period (starting at different times) as a repetition
to compute the FCR and average length. And we use a window size of 500 in this task. Four
selection rules are considered here: fixed selection rule with S = 1{µ(X ) > 115}; decision-
t (cid:98) t
driven selection rule with S = 1{µ(X ) > 110 + min{(cid:80)t S /30,10}}; quantile selection rule
t (cid:98) t j=0 j
with S = 1{µ(X ) > A({µ(X )}t−1 )} where A({µ(X )}t−1 ) is the 35%-quantile of the
t (cid:98) t (cid:98) i i=t−500 (cid:98) i i=t−500
{µ(X )}t−1 ; mean selection rule with S = 1{µ(X ) > (cid:80)t−1 µ(X )/500}. Here we do not
(cid:98) i i=t−500 t (cid:98) t i=t−500(cid:98) i
consider the online multiple testing rule due to the limited sample size, leading to few selections. We
adopt a windowed scheme with window size 500, and set the target FCR level α = 10%.
Aggregating 20 different choices of the online data set, the results are summarized in Figure 7.
The CAS-DtACI performs well in delivering precise FCR levels as time grows. As for comparison,
the OCP and CAS cannot obtain the desired FCR control through the whole procedure.
7 Conclusion
This paper addresses the challenge of online selective inference in the context of conformal prediction.
To mitigate the issue of inexchangeability caused by data-driven online selection processes, we
introduce CAS, a novel approach that adaptively constructs a calibration set to yield reliable online
prediction intervals. Our theoretical analysis and numerical experiments demonstrate the efficacy of
our method in controlling FCR across diverse data environments and selection rules.
We point out several future directions. Firstly, while our method targets two common selec-
tion rules, extending our framework to accommodate arbitrary selection rules necessitates further
exploration. Secondly, we assume a fixed predictive model for theoretical simplicity. Investigating
the feasibility of online updating of the machine learning model throughout the process would be
intriguing for future study. Thirdly, there may exist a more delicate variant of CAS under some
special time series model to obtain tight FCR control.
References
Ehud Aharoni and Saharon Rosset. Generalized α-investing: definitions, optimality results and
application to public databases. Journal of the Royal Statistical Society: Series B: Statistical
Methodology, pages 771–794, 2014.
Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and
distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021.
25Yajie Bao, Yuyang Huo, Haojie Ren, and Changliang Zou. Selective conformal inference with
false coverage-statement rate control. Biometrika, page asae010, 02 2024. ISSN 1464-3510. doi:
10.1093/biomet/asae010. URL https://doi.org/10.1093/biomet/asae010.
Rina Foygel Barber, Emmanuel J Candès, Aaditya Ramdas, and Ryan J Tibshirani. Predictive
inference with the jackknife+. The Annals of Statistics, 49(1):486–507, 2021.
Rina Foygel Barber, Emmanuel J Candès, Aaditya Ramdas, and Ryan J Tibshirani. Conformal
prediction beyond exchangeability. The Annals of Statistics, 51(2):816–845, 2023.
Stephen Bates, Emmanuel Candès, Lihua Lei, Yaniv Romano, and Matteo Sesia. Testing for outliers
with conformal p-values. The Annals of Statistics, 51(1):149–178, 2023.
Yoav Benjamini and Daniel Yekutieli. The control of the false discovery rate in multiple testing
under dependency. The Annals of Statistics, 29(2):1165–1188, 2001.
Yoav Benjamini and Daniel Yekutieli. False discovery rate–adjusted multiple confidence intervals for
selected parameters. Journal of the American Statistical Association, 100(469):71–81, 2005.
Thomas F Brooks, D Stuart Pope, and Michael A Marcolini. Airfoil self-noise and prediction.
Technical report, 1989.
Victor Chernozhukov, Kaspar Wüthrich, and Yinchu Zhu. An exact and robust conformal inference
method for counterfactual and synthetic controls. Journal of the American Statistical Association,
116(536):1849–1864, 2021.
Mindy I Davis, Jeremy P Hunt, Sanna Herrgard, Pietro Ciceri, Lisa M Wodicka, Gabriel Pallares,
Michael Hocker, Daniel K Treiber, and Patrick P Zarrinkar. Comprehensive analysis of kinase
inhibitor selectivity. Nature Biotechnology, 29(11):1046–1051, 2011.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.
uci.edu/ml.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC
2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006.
William Fithian, Dennis Sun, and Jonathan Taylor. Optimal inference after model selection. arXiv
preprint arXiv:1410.2597, 2014.
Dean P Foster and Robert A Stine. α-investing: a procedure for sequential control of expected false
discoveries. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(2):
429–444, 2008.
26Rina Foygel Barber, Emmanuel J Candès, Aaditya Ramdas, and Ryan J Tibshirani. The limits of
distribution-free conditional predictive inference. Information and Inference, 10(2):455–482, 2021.
doi: 10.1093/imaiai/iaaa017.
Isaac Gibbs and Emmanuel Candès. Adaptive conformal inference under distribution shift. Advances
in Neural Information Processing Systems, 34:1660–1672, 2021.
Isaac Gibbs and Emmanuel Candès. Conformal inference for online prediction with arbitrary
distribution shifts. arXiv preprint arXiv:2208.08401, 2022.
PaulaGradu, EladHazan, andEdgarMinasyan. Adaptiveregretforcontroloftime-varyingdynamics.
In Learning for Dynamics and Control Conference, pages 560–572. PMLR, 2023.
Kexin Huang, Tianfan Fu, Lucas M Glass, Marinka Zitnik, Cao Xiao, and Jimeng Sun. Deeppurpose:
a deep learning library for drug–target interaction prediction. Bioinformatics, 36(22-23):5545–5547,
2020.
Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley,
Cao Xiao, Jimeng Sun, and Marinka Zitnik. Artificial intelligence foundation for therapeutic
science. Nature Chemical Biology, 18(10):1033–1036, 2022.
Adel Javanmard and Andrea Montanari. On online control of false discovery rate. arXiv preprint
arXiv:1502.06197, 2015.
Adel Javanmard and Andrea Montanari. Online rules for control of false discovery rate and false
discovery exceedance. The Annals of statistics, 46(2):526–554, 2018.
Ying Jin and Emmanuel J Candès. Selection by prediction with conformal p-values. Journal of
Machine Learning Research, 24(244):1–41, 2023a.
Ying Jin and Emmanuel J Candès. Model-free selective inference under covariate shift via weighted
conformal p-values. arXiv preprint arXiv:2307.09291, 2023b.
Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. Exact post-selection inference,
with application to the lasso. The Annals of Statistics, 44(3):907–927, 2016.
Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society Series B: Statistical Methodology, 76(1):71–96, 2014.
Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-
free predictive inference for regression. Journal of the American Statistical Association, 113(523):
1094–1111, 2018.
27Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. Inductive confidence
machines for regression. In European Conference on Machine Learning, pages 345–356. New York:
Springer, 2002.
Aaditya Ramdas, Fanny Yang, Martin J Wainwright, and Michael I Jordan. Online control of the
false discovery rate with decaying memory. Advances in Neural Information Processing Systems,
30:5655–5664, 2017.
Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, and Michael Jordan. Saffron: an adaptive
algorithm for online control of the false discovery rate. In International Conference on Machine
Learning, pages 4286–4294. PMLR, 2018.
Yaniv Romano, Evan Patterson, and Emmanuel Candès. Conformalized quantile regression. Advances
in Neural Information Processing Systems, 32:3543–3553, 2019.
Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with
bounded error levels. Journal of the American Statistical Association, 114(525):223–234, 2019.
Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning
Research, 9(3):371–421, 2008.
Jonathan Taylor and Robert Tibshirani. Post-selection inference for-penalized likelihood models.
Canadian Journal of Statistics, 46(1):41–61, 2018.
Ryan J Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert Tibshirani. Exact post-selection
inference for sequential regression procedures. Journal of the American Statistical Association, 111
(514):600–620, 2016.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candès, and Aaditya Ramdas. Conformal
prediction under covariate shift. Advances in Neural Information Processing Systems, 32:2530–2540,
2019.
Vladimir Vovk. Aggregating strategies. In Proceedings of 3rd Annu. Workshop on Comput. Learning
Theory, pages 371–383, 1990.
Vladimir Vovk and Ruodu Wang. E-values: Calibration, combination and applications. The Annals
of Statistics, 49(3):1736–1754, 2021.
Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer Science & Business Media, 2005.
28Volodya Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of
algorithmic randomness. In International Conference on Machine Learning, pages 444–453, 1999.
Asaf Weinstein and Aaditya Ramdas. Online control of the false coverage rate and false sign rate. In
International Conference on Machine Learning, pages 10193–10202, 2020.
Asaf Weinstein, William Fithian, and Yoav Benjamini. Selection adjusted confidence intervals with
more power to determine the sign. Journal of the American Statistical Association, 108(501):
165–176, 2013.
Olivier Wintenberger. Optimal learning with bernstein online aggregation. Machine Learning, 106:
119–141, 2017.
ZiyuXuandAadityaRamdas. Onlinemultipletestingwithe-values. arXiv preprint arXiv:2311.06412,
2023.
Ziyu Xu, Ruodu Wang, and Aaditya Ramdas. Post-selection inference for e-value based confidence
intervals. arXiv preprint arXiv:2203.12572, 2022.
Haibing Zhao. General ways to improve false coverage rate-adjusted selective confidence intervals.
Biometrika, 109(1):153–164, 2022.
Tijana Zrnic, Aaditya Ramdas, and Michael I Jordan. Asynchronous online testing of multiple
hypotheses. The Journal of Machine Learning Research, 22(1):1585–1623, 2021.
29Supplementary Material for “CAS: A General Algorithm for Online
Selective Conformal Prediction with FCR Control”
A Preliminaries
The following two lemmas are usually used in the conformal inference literature (Vovk et al., 2005;
Lei et al., 2018; Romano et al., 2019; Barber et al., 2021, 2023).
Lemma A.1. Let x is the ⌈n(1−α)⌉smallest value in {x ∈ R : i ∈ [n]}. Then for any
(⌈n(1−α)⌉) i
α ∈ (0,1), it holds that
n
1 (cid:88)
1{x > x } ≤ α.
n i (⌈n(1−α)⌉)
i=1
If all values in {x : i ∈ [n]} are distinct, it also holds that
i
n
1 (cid:88) 1
1{x > x } ≥ α− ,
n i (⌈n(1−α)⌉) n
i=1
[n]
Lemma A.2. Given real numbers x ,...,x ,x , let {x : r ∈ [n]} be order statistics of {x : i ∈
1 n n+1 (r) i
[n+1]
[n]}, and {x : r ∈ [n+1]} be the order statistics of {x : i ∈ [n+1]}, then for any r ∈ [n] we
(r) i
[n] [n+1]
have: {x ≤ x } = {x ≤ x }.
n+1 (r) n+1 (r)
Hereafter, for any index set C, we write R as the ⌈(1−α)|C|⌉st smallest value in {R } . We
C i i∈C
also omit the confidence level α in ICAS(X ;α) whenever the context is clear. According to the
t t
definition of ICAS(X ) in Algorithm 1, together with Lemma A.2, we know
t t
(cid:16) (cid:17)
1{Y ̸∈ ICAS(X )} = 1{R > q {R } } = 1{R > R }.
t t t t α i i∈C(cid:98)t t C(cid:98)t∪{t}
In addition, Lemma A.1 guarantees
1 1 (cid:88)
α− ≤ 1{R > R } ≤ α.
|C(cid:98)t|+1 |C(cid:98)t|+1
j C(cid:98)t∪{t}
j∈C(cid:98)t∪{t}
Combining the two relations above, we have the following upper bound and lower bound on the
indicator of miscoverage,
1 (cid:88)(cid:104) (cid:105)
1{Y ̸∈ ICAS(X )} ≤ α+ 1{R > R }−1{R > R } , (13)
t t t
|C(cid:98)t|+1
t C(cid:98)t∪{t} i C(cid:98)t∪{t}
i∈C(cid:98)t
and
1 1 (cid:88)(cid:104) (cid:105)
1{Y ̸∈ ICAS(X )} ≥ α− + 1{R > R }−1{R > R } . (14)
t t t
|C(cid:98)t|+1 |C(cid:98)t|+1
t C(cid:98)t∪{t} i C(cid:98)t∪{t}
i∈C(cid:98)t
30B Proofs for decision-driven selection
B.1 Proof of Theorem 1
Lemma B.1. Denote δ (R ,R ) = 1{R > R }−1{R > R }. Under the conditions of
t t s t C(cid:98)t∪{t} s C(cid:98)t∪{t}
Theorem 1, we have
(cid:110) (cid:16) (cid:17)(cid:111)
E Π (X )Π (X )δ (R ,R ) | σ {S }t−1,{Z }−1 = 0.
t t t s t t s i i=0 i i=−n,i̸=s
Proof of Theorem 1. Recall that C(cid:98)t = {−n ≤ s ≤ −1 : Π t(X s) = 1}. Invoking (13), we can upper
bound FCR by
(cid:34)(cid:80)T
S 1{Y ̸∈ ICAS(X
)}(cid:35)
FCR(T) = E t=0 t t t t
1∨(cid:80)T
S
t=0 t
  
T
≤ (cid:88) t=0E  1∨(cid:80)S
T
tt
=0S
t
 α+ |C(cid:98)t|1
+1
s(cid:88) ∈C(cid:98)tδ t(R t,R s) 
 
T
≤ α+(cid:88) t=0E  1∨(cid:80)S
T
tt
=0S
t|C(cid:98)t|1
+1
s(cid:88) ∈C(cid:98)tδ t(R t,R s). (15)
Similarly, using (14), we have the following lower bound
FCR(T) ≥
α−(cid:88)T E(cid:34) S
t
1 (cid:35) +(cid:88)T E(cid:34) S
t
(cid:80) s∈C(cid:98)tδ t(R t,R s)(cid:35)
. (16)
t=0
1∨(cid:80)T
t=0S t|C(cid:98)t|+1
t=0
1∨(cid:80)T
t=0S
t
|C(cid:98)t|+1
Let Π(t) (·) be corresponding selection rule by replacing X with x∗ ∈ σ({S }t−1) such that Π (x∗) =
j t t i i=0 t t
1. Correspondingly, we denote S(t) = Π(t) (X ) for any j ≥ 0. According to our assumption
j j j
Π (·) ∈ σ({S }t−1), we know: (1) S(t) = S for any 0 ≤ j ≤ t − 1; (2) if S = 1, it holds that
t i i=0 j j t
S(t) = Π(t) (X ) = Π (X ) = S for any j ≥ t. Since Π (·) is independent of {Z }−n , we have
j j j j j j t i i=−1
 
E  1∨(cid:80)S
T
tt
=0S
t|C(cid:98)t|1
+1
(cid:88) δ t(R t,R s)
s∈C(cid:98)t
 
= E  S t (cid:88) δ t(R t,R s) 
1∨(cid:80)T j=0S j(t)
s∈C(cid:98)t
(cid:80)− j=1 −nΠ t(X j)+1
(cid:34) −1 (cid:35)
= E
1 (cid:88) Π t(X t)Π t(X s)δ t(R t,R s)
1∨(cid:80)T j=0S j(t)
s=−n
(cid:80)− j=1 −n,j̸=sΠ t(X j)+2
 (cid:110) (cid:16) (cid:17)(cid:111)
−1 E Π (X )Π (X )δ (R ,R ) | σ {S }t−1,{Z }T ,{Z }−1
1 (cid:88) t t t s t t s i i=0 i i=t+1 i i=−n,i̸=s
= E  
1∨(cid:80)T j=0S j(t)
s=−n
(cid:80)− j=1 −n,j̸=sΠ t(X j)+2
31 (cid:110) (cid:16) (cid:17)(cid:111)
−1 E Π (X )Π (X )δ (R ,R ) | σ {S }t−1,{Z }−1
1 (cid:88) t t t s t t s i i=0 i i=−n,i̸=s
= E  
1∨(cid:80)T j=0S j(t)
s=−n
(cid:80)− j=1 −n,j̸=sΠ t(X j)+2
= 0, (17)
where the last equality follows from Lemma B.1. Plugging (17) into (15) gives the desired upper
bound FCR(T) ≤ α. Let p = P(cid:8) Π (X ) = 1 | σ({S }t−1)(cid:9). From the i.i.d. assumption, we know
t t t i i=0
|C(cid:98)t| ∼ Binomial(n,p t) given σ({S i}t i=− 01). Then we have
(cid:34) (cid:35) (cid:34) (cid:40) (cid:41)(cid:35)
S 1 1 S
E t = E E t | σ({S }t−1),{Z }
1∨(cid:80)T
t=0S t|C(cid:98)t|+1
1+(cid:80)T
j̸=tS
j
|C(cid:98)t|+1
i i=0 i i≥t+1
(cid:34) (cid:40) (cid:41)(cid:35)
( =i) E 1 E S t | σ({S }t−1)
1+(cid:80)T
j̸=tS
j
|C(cid:98)t|+1
i i=0
(cid:34) (cid:40) (cid:41)(cid:35)
( =ii) E 1 E(cid:8) S | σ({S }t−1)(cid:9) ·E 1 | σ({S }t−1)
1+(cid:80)T
j̸=tS
j
t i i=0
|C(cid:98)t|+1
i i=0
(cid:34) (cid:35)
= E 1 E(cid:8) S | σ({S }t−1)(cid:9) · 1−(1−p t)n+1
1+(cid:80)T j̸=tS j t i i=0 (n+1)p t
(cid:34) (cid:35)
S 1−(1−p )n+1
= E t t , (18)
1∨(cid:80)T j=0S j (n+1)p t
where (i) holds since S t,|C(cid:98)t|⊥⊥{Z i}
i≥t+1
given σ({S i}t i=− 01); and (ii) holds due to i.i.d. assumption.
Plugging (17) and (18) into (16) yields the desired lower bound
FCR(T) ≥
α·E(cid:34) (cid:80)T
t=0S
t
(cid:35) −(cid:88)T E(cid:34)
S
t
1−(1−p
t)n+1(cid:35)
1∨(cid:80)T j=0S j t=0 1∨(cid:80)T j=0S j (n+1)p t
=
α−(cid:88)T E(cid:34)
S
t
1−(1−p
t)n+1(cid:35)
,
t=0 (cid:80)T j=0S j (n+1)p t
where the last inequality follows from the assumption (cid:80)T S > 0 with probability 1. Therefore, we
j=0 j
have finished the proof.
B.2 Proof of Lemma B.1
Proof. We first notice that Π (·) is fixed given σ({S }t−1). It also means that {Π (X )}n are
t i i=0 t −i i=1,i̸=s
also fixed given σ({S }t−1,{Z }−1 ). Let z = (x ,y ) and z = (x ,y ). Now define the event
i i=0 i i=−n,i̸=s 1 1 1 2 2 2
E = {[Z ,Z ] = [z ,z ]}, where [Z ,Z ] and [z ,z ] are two unordered sets. Clearly, we know R
z s t 1 2 s t 1 2 C(cid:98)t∪{t}
is fixed given E and σ({S }t−1,{Z }−1 ). Recalling the definition of δ (R ,R ), we can get
z i i=0 i i=−n,i̸=s t t s
(cid:104) (cid:16) (cid:17) (cid:105)
E Π (X )Π (X )δ (R ,R ) | σ {S }t−1,{Z }−1 ,E
t t t s t t s i i=0 i i=−n,i̸=s z
32(cid:110) (cid:16) (cid:17) (cid:111)
= P R > R ,Π (X ) = 1,Π (X ) = 1 | σ {S }t−1,{Z }−1 ,E
t C(cid:98)t∪{t} t t t s i i=0 i i=−n,i̸=s z
(cid:110) (cid:16) (cid:17) (cid:111)
−P R > R ,Π (X ) = 1,Π (X ) = 1 | σ {S }t−1,{Z }−1 ,E
s C(cid:98)t∪{t} t t t s i i=0 i i=−n,i̸=s z
( =∗) 1 1{Π (x ) = 1,Π (x ) = 1}(cid:104) 1{r > R }+1{r > R }(cid:105)
2 t 1 t 2 1 C(cid:98)t∪{t} 2 C(cid:98)t∪{t}
1 (cid:104) (cid:105)
− 1{Π (x ) = 1,Π (x ) = 1} 1{r > R }+1{r > R }
2 t 1 t 2 1 C(cid:98)t∪{t} 2 C(cid:98)t∪{t}
= 0,
where r = |y −µ(x )| and r = |y −µ(x )|; the equality (∗) holds since (R ,R ) are exchangeable
1 1 (cid:98) 1 2 2 (cid:98) 2 s t
andE ⊥⊥({S }t−1,{Z }−1 ). ThroughmarginalizingoverE , wecanprovethedesiredresult.
z i i=0 i i=−n,i̸=s z
B.3 Proof of Proposition 2.1
Proof. Recall that Imarg(X ;α ) = µ(X )±q ({R } ) with H = {−n,...,−1}. It follows that
t t t (cid:98) t αt i i∈H0 0
−1
1{Y ̸∈ Imarg(X ;α )} ≤ α + 1 (cid:88) δ (R ,R ), (19)
t t t t t n+1 t t s
s=−n
where δ (R ,R ) = 1{R > R }−1{R > R }. We follow the notation S(t) in Section B.1.
t t s t H0∪{t} s H0∪{t} j
By the definition, we have
FCR(T) =
(cid:88)T E(cid:34) S t1{Y
t
̸∈ I tmarg(X t;α t)}(cid:35)
1∨(cid:80)T
S
t=0 j=0 j
=
(cid:88)T E(cid:34) S t1{Y
t
̸∈ I tmarg(X t;α t)}(cid:35)
1∨(cid:80)T S(t)
t=0 j=0 j
≤
(cid:88)T E(cid:34) 1{Y
t
̸∈ I tmarg(X t;α t)}(cid:35)
1∨(cid:80)T S(t)
t=0 j=0 j
( ≤i) (cid:88)T E(cid:34) α
t
(cid:35) +(cid:88)T E(cid:34) 1 (cid:80)− s=1 −nδ t(R t,R s)(cid:35)
1∨(cid:80)T S(t) 1∨(cid:80)T S(t) n+1
t=0 j=0 j t=0 j=0 j
( ≤ii) α+(cid:88)T E(cid:34) E(cid:40) 1 (cid:80)− s=1 −nδ t(R t,R s)
|
σ(cid:0)
{S
}t−1(cid:1)(cid:41)(cid:35)
1∨(cid:80)T S(t) n+1 i i=0
t=0 j=0 j
(i ≤ii) α+(cid:88)T E(cid:34) E(cid:40) 1
|
σ(cid:0)
{S
}t−1(cid:1)(cid:41) (cid:80)− s=1 −nE(cid:2) δ t(R t,R s) | σ(cid:0) {S i}t i=− 01(cid:1)(cid:3)(cid:35)
1∨(cid:80)T S(t) i i=0 n+1
t=0 j=0 j
(iv)
= α,
where (i) follows from (19); (ii) holds due to the LORD-CI’s invariant (cid:80)T α /((cid:80)T S ) ≤ α and
t=0 t j=0 j
S(t) ≥ S for any j ≥ t; (iii) holds since α ∈ σ(cid:0) {S }t−1(cid:1) and (Z ,Z )⊥⊥S(t) for j ≥ t; and (iv)
j j t i i=0 t s j
33follows from the exchangeability between Z and Z such that
t s
E(cid:2) δ (R ,R ) | σ(cid:0) {S }t−1(cid:1)(cid:3) = E[δ (R ,R )]
t t s i i=0 t t s
= P(cid:0) R > R (cid:1) −P(cid:0) R > R (cid:1) = 0.
t H0∪{t} s H0∪{t}
B.4 Proof of Theorem 2
Proof. Recall that C(cid:98)t = {i ≤ t−1 : Π t(X i) = 1}. Similar to (17), we can show
−1 (cid:34) (cid:35)
(cid:88) 1 1
E Π (X )Π (X )δ (R ,R ) = 0, (20)
s=−n
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
t t t s t t s
Let x ,x ∈ σ(S ,...,S ) be the values such that Π (x ) = 1 and Π (x ) = 0 for
s,1 s,0 0 s−1 s s,1 s s,0
0 ≤ s ≤ t−1. Denote {Π(s,1) } and {Π(s,0) } the virtual selection rules generated by replacing
i i≥0 i i≥0
X with x and x , respectively. Let {S(s,1) } and {S(s,0) } be the corresponding virtual
s s,1 s,0 i i≥0 i i≥0
decision sequences. In addition, we define
(cid:110) (cid:111)
(s,1) (s,1)
C(cid:98)
i
= {−n ≤ j ≤ −1 : Π i(X j) = 1}∪ 0 ≤ j ≤ t−1 : Π
i
(X j) = 1 ,
(cid:110) (cid:111)
(s,0) (s,0)
C(cid:98)
i
= {−n ≤ j ≤ −1 : Π i(X j) = 1}∪ 0 ≤ j ≤ t−1 : Π
i
(X j) = 1 .
Further, we denote
δ(s,1) (R ,R ) = 1{R > R }−1{R > R },
t t s t C(cid:98)t(s,1)∪{t} s C(cid:98)t(s,1)∪{t}
δ(s,0) (R ,R ) = 1{R > R }−1{R > R }.
t t s t C(cid:98)t(s,0)∪{t} s C(cid:98)t(s,0)∪{t}
Then we have the following conclusions:
(1) Π( is,1) = Π
i
and C(cid:98) i(s,1) = C(cid:98) i(s,0) = C(cid:98)i for any i ≤ s;
(2) If S
s
= 1, then Π( is,1) = Π i, C(cid:98) i(s,1) = C(cid:98)i for any i ≥ s+1 and δ t(s,1) (R t,R s) = δ t(R t,R s).
(3) If S
s
= 0, then Π( is,0) = Π i, C(cid:98) i(s,0) = C(cid:98)i for any i ≥ s+1 and δ t(s,0) (R t,R s) = δ t(R t,R s).
Letx(s,1)
∈
σ(S(s) ,...,S(s) )andx(s,0)
∈
σ(S(s) ,...,S(s) )bethevaluessuchthatΠ(s,1) (x(s,1)
) = 1
t 0,1 t−1,1 t 0,0 t−1,0 t t
andΠ(s,0) (x(s,0) ) = 1fort > s, respectively. Let{S(s,t) } bethevirtualdecisionsequencegenerated
t t i,1 i≥0
by firstly replacing X with x , and then replacing X with x(s,1). Let {S(s,t) } be the virtual
s s,1 t t i,0 i≥0
decision sequence generated by firstly replacing X with x , and then replacing X with x(s,1). In
s s,0 t t
this case, we can guarantee that S(s,t) ,S(s,t) ⊥⊥(Z ,Z ) for any i ≥ 0 because x ,x ⊥⊥(Z ,Z ) and
i,1 i,0 s t s,1 s,0 s t
x(s,1) ,x(s,0) ⊥⊥(Z ,Z ). We have
t t s t
34(1) S(s,t) = S(s,t) = S for i ≤ s−1;
i,1 i,0 i
(2) S(s,t) = S(s,1) and S(s,t) = S(s,0) for s ≤ i ≤ t−1;
i,1 i i,0 i
(3) If S = 1, S(s,t) = S(s,1) for i ≥ t.
t i,1 i
(4) If S = 0, S(s,t) = S(s,0) for i ≥ t.
t i,0 i
For any pair (s,t) with 0 ≤ s ≤ t−1, we denote
H (X ,X ) = Π (X )Π (X )Π (X ),
s,t s t t s s t s s
(s,1) (s,1)
H (X ,X ) = Π (X )Π (X )Π (X ),
s,t s t t s s t s s
J (X ,X ) = Π (X )[1−Π (X )][1−Π (X )],
s,t s t t s s s s t
(s,0) (s,0)
J (X ,X ) = Π (X )[1−Π (X )][1−Π (X )].
s,t s t t s s s s t
According to our construction, we know Π (X )H (X ,X ) = Π(s,1) (X )H(s,1) (X ,X ) and
t t s,t s t t t s,t s t
Π (X )J (X ,X ) = Π(s,0) J(s,0) (X ,X ). Invoking (13), we can expand FCR by
t t s,t s t t s,t s t
 
T
FCR(T) ≤ α+E (cid:88)
t=0
1∨(cid:80)S
T
jt
=0S
j
|C(cid:98)t|1
+1
s(cid:88) ∈C(cid:98)tδ t(R t,R s)
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
= α+E Π (X )Π (X )δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t t s t t s
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
= α+E Π (X )H (X ,X )δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t s,t s t t t s
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
+E Π (X )J (X ,X )δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t s,t s t t t s
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
+E Π (X )Π (X )Π (X )[1−Π (X )]δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t t s s s s t t t s
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
+E Π (X )Π (X )[1−Π (X )]Π (X )δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t t s s s s t t t s
=
α+E(cid:34) (cid:88)T (cid:88)t−1 1 Π t(s,1) (X t)H s( ,s t,1) (X s,X t)·δ t(s,1) (R t,R s)(cid:35)
1∨(cid:80)T S(s,t)
(T)
(cid:80) Π(s,1)
(X )+2
t=0 s=0 j=0 j,1 i≤t−1,i̸=s t i
+E(cid:34) (cid:88)T (cid:88)t−1 1 Π t(s,0) (X t)J s( ,s t,0) (X s,X t)·δ t(s,0) (R t,R s)(cid:35)
1∨(cid:80)T S(s,t)
(T)
(cid:80) Π(s,0)
(X )+2
t=0 s=0 j=0 j,0 i≤t−1,i̸=s t i
(cid:34) T t−1 (cid:35)
+E
(cid:88)(cid:88) 1 Π t(X t)Π t(X s)|Π s(X t)−Π s(X s)|δ t(R t,R s)
, (21)
t=0 s=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
35where the first equality holds due to (20). We define
(cid:110) (cid:111)
(−s,1) (s,1)
C(cid:98)
t,online
= {−n ≤ j ≤ −1 : Π t(X j) = 1}∪ 0 ≤ j ≤ t−1,j ̸= s : Π
t
(X j) = 1 ,
(cid:110) (cid:111)
(−s,0) (s,0)
C(cid:98)
t,online
= {−n ≤ j ≤ −1 : Π t(X j) = 1}∪ 0 ≤ j ≤ t−1,j ̸= s : Π
t
(X j) = 1 .
If Π( ts,1) (X s) = 1, it holds that C(cid:98) t(s,1) = C(cid:98) t( ,− ons l, i1 n)
e
∪ {s}. If Π( ts,0) (X s) = 1, it holds that C(cid:98) t(s,0) =
C(cid:98) t( ,− ons l, i0 n) e∪{s}. Because Π s,Π( ts,1) ,Π( ts,0) ,C(cid:98) t( ,− ons l, i1 n) e,C(cid:98) t( ,− ons l, i0 n)
e
are fixed given σ({Z j} j̸=s,t), similar to the
proof of Lemma B.1, we can also verify
(cid:104) (cid:105)
E Π(s,1) (X )H(s,1) (X ,X )·δ(s,1) (R ,R ) | σ({Z } )
t t s,t s t t t s j j̸=s,t
(cid:20) (cid:21)
= E Π(s,1) (X )Π(s,1) (X )Π (X )Π (X )1{R > R } | σ({Z } )
t t t s s t s s t C(cid:98)(−s,1) ∪{s,t} j j̸=s,t
t,online
(cid:20) (cid:21)
−E Π(s,1) (X )Π(s,1) (X )Π (X )Π (X )1{R > R } | σ({Z } )
t t t s s t s s s C(cid:98)(−s,1) ∪{s,t} j j̸=s,t
t,online
= 0, (22)
and
(cid:104) (cid:105)
E Π(s,0) (X )J(s,0) (X ,X )·δ(s,0) (R ,R ) | σ({Z } )
t t s,t s t t t s j j̸=s,t
(cid:20) (cid:21)
= E Π(s,0) (X )Π(s,0) (X )[1−Π (X )][1−Π (X )]1{R > R } | σ({Z } )
t t t s s t s s t C(cid:98)(−s,0) ∪{s,t} j j̸=s,t
t,online
(cid:20) (cid:21)
−E Π(s,0) (X )Π(s,0) (X )[1−Π (X )][1−Π (X )]1{R > R } | σ({Z } )
t t t s s t s s s C(cid:98)(−s,0) ∪{s,t} j j̸=s,t
t,online
= 0. (23)
Plugging (22) and (23) into (21), together with δ (R ,R ) ≤ 1, we have the following upper bound
t t s
T t−1 (cid:34) (cid:35)
FCR(T) ≤
α+(cid:88)(cid:88)
E
Π t(X t) Π t(X s)
·1{Π (X ) ̸= Π (X )} .
t=0 s=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s t s s
Then the conclusion is proved.
B.5 Proof of Theorem 3
Proof. For any (i,s,t) with 0 ≤ i,s ≤ t−1 and (x ,x ), we define
1 2
H (x ,x ) = Π (x )Π (x )Π (x ),
i,t 1 2 t 1 i 1 i 2
(s,1) (s,1) (s,1) (s,1)
H (x ,x ) = Π (x )Π (x )Π (x ),
i,t 1 2 t 1 i 1 i 2
(s,0) (s,0) (s,0) (s,0)
H (x ,x ) = Π (x )Π (x )Π (x ),
i,t 1 2 t 1 i 1 i 2
J (x ,x ) = Π (x )[1−Π (x )][1−Π (x )],
i,t 1 2 t 1 i 1 i 2
36(s,1) (s,1) (s,1) (s,1)
J (x ,x ) = Π (x )[1−Π (x )][1−Π (x )],
i,t 1 2 t 1 i 1 i 2
(s,0) (s,0) (s,0) (s,0)
J (x ,x ) = Π (x )[1−Π (x )][1−Π (x )].
i,t 1 2 t 1 i 1 i 2
Specially, we have H(s,1) (X ,X ) = H (X ,X ) and J(s,1) (X ,X ) = J (X ,X ). The selected
s,t s t s,t s t s,t s t s,t s t
calibration set can be written as
C(cid:98)t = {−n ≤ i ≤ −1 : Π t(X i) = 1}∪{0 ≤ i ≤ t−1 : H i,t(X i,X t) = 1}
∪{0 ≤ i ≤ t−1 : J (X ,X ) = 1}.
i,t i t
For 0 ≤ s ≤ t−1, the decoupled versions of C(cid:98)t are given by
(s,1) (s,1)
C(cid:98)
t
= {−n ≤ i ≤ −1 : Π t(X i) = 1}∪{0 ≤ i ≤ t−1 : H
i,t
(X i,X t) = 1}
(s,1)
∪{0 ≤ i ≤ t−1 : J (X ,X ) = 1},
i,t i t
and
(s,0) (s,0)
C(cid:98)
t
= {−n ≤ i ≤ −1 : Π t(X i) = 1}∪{0 ≤ i ≤ t−1 : H
i,t
(X i,X t) = 1}
(s,0)
∪{0 ≤ i ≤ t−1 : J (X ,X ) = 1}.
i,t i t
If −n ≤ s ≤ −1, we define
(−s)
C(cid:98)
t,fixed
= {−n ≤ i ≤ −1,i ̸= s : Π t(X i) = 1}∪{0 ≤ i ≤ t−1 : H i,t(X i,X t) = 1}
∪{0 ≤ i ≤ t−1 : J (X ,X ) = 1}.
i,t i t
If 0 ≤ s ≤ t−1, we define
(−s,1) (s,1)
C(cid:98)
t,online
= {−n ≤ i ≤ −1 : Π t(X i) = 1}∪{0 ≤ i ≤ t−1,i ̸= s : H
i,t
(X i,X t) = 1}
(s,1)
∪{0 ≤ i ≤ t−1,i ̸= s : J (X ,X ) = 1},
i,t i t
and
(−s,0) (s,0)
C(cid:98)
t,online
= {−n ≤ i ≤ −1 : Π t(X i) = 1}∪{0 ≤ i ≤ t−1,i ̸= s : H
i,t
(X i,X t) = 1}
(s,0)
∪{0 ≤ i ≤ t−1,i ̸= s : J (X ,X ) = 1}.
i,t i t
Since Π
t
and C(cid:98) t(−s) ,−n ≤ s ≤ −1 are both fixed given σ({Z j} j̸=s,t), we have
E[Π (X )Π (X )δ (R ,R ) | σ({Z } )]
t t t s t t s j j̸=s,t
(cid:104) (cid:105)
= E Π (X )Π (X )1{R > R } | σ({Z } )
t t t s t C(cid:98)t(−s)∪{s,t} j j̸=s,t
(cid:104) (cid:105)
−E Π (X )Π (X )1{R > R } | σ({Z } )
t t t s s C(cid:98)t(−s)∪{s,t} j j̸=s,t
37= 0, (24)
which yields that
−1 (cid:34) (cid:35)
(cid:88)
E
1 Π t(X t)Π t(X s)
δ (R ,R )
s=−n
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
t t s
 
−1
= s(cid:88) =−nE  1∨(cid:80)1
T j=0S
j(t)Π |t C(cid:98)( t(X
,−
fit xs)
)
eΠ d|t +(X 2s) δ t(R t,R s)
 
=
(cid:88)−1
E 
1 E{Π t(X t)Π t(X s)δ t(R t,R s) | σ({Z j} j̸=s,t)}

s=−n
1∨(cid:80)T
j=0S
j(t)
|C(cid:98)
t( ,− fixs)
ed|+2
= 0.
Together with (13), we can bound FCR by
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
FCR(T) ≤ α+E Π (X )H (X ,X )δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t s,t s t t t s
(cid:34) T t−1 (cid:35)
(cid:88) 1 1 (cid:88)
+E Π (X )J (X ,X )δ (R ,R )
t=0
1∨(cid:80)T
j=0S
j
|C(cid:98)t|+1
s=0
t t s,t s t t t s
 
= α+E
(cid:88)T (cid:88)t−1 1 Π( ts,1) (X t)H s( ,s t,1) (X s,X t)·δ t(s,1) (R t,R s)

1∨(cid:80)T S(s,t) |C(cid:98)(−s,1)
|+2
t=0 s=0 j=0 j,1 t,online
 
+E
(cid:88)T (cid:88)t−1 1 Π( ts,0) (X t)J s( ,s t,0) (X s,X t)·δ t(s,0) (R t,R s)
. (25)
1∨(cid:80)T S(s,t) |C(cid:98)(−s,0)
|+2
t=0 s=0 j=0 j,0 t,online
Since Π( ts,1), Π
s
and C(cid:98) t( ,− ons l, i1 n)
e
for 0 ≤ s ≤ t−1 are fixed given σ({Z j} j̸=s,t), we have
(cid:104) (cid:105)
E Π(s,1) (X )H(s,1) (X ,X )·δ(s,1) (R ,R ) | σ({Z } )
t t s,t s t t t s j j̸=s,t
(cid:20) (cid:21)
= E Π(s,1) (X )Π(s,1) (X )Π (X )Π (X )1{R > R } | σ({Z } )
t t t s s t s s t C(cid:98)(−s,1) ∪{s,t} j j̸=s,t
t,online
(cid:20) (cid:21)
−E Π(s,1) (X )Π(s,1) (X )Π (X )Π (X )1{R > R } | σ({Z } )
t t t s s t s s s C(cid:98)(−s,1) ∪{s,t} j j̸=s,t
t,online
= 0. (26)
Similarly, we also have
(cid:104) (cid:105)
E Π(s,0) (X )J(s,0) (X ,X )·δ(s,0) (R ,R ) | σ({Z } ) = 0. (27)
t t s,t s t t t s j j̸=s,t
Substituting (26) and (27) into (25), together with the fact (cid:80)T j=0S j( ,s 0,t) ,C(cid:98) t( ,− ons l, i1 n)
e
∈ σ({Z j} j̸=s,t), we
can prove FCR ≤ α.
38Now we proceed to prove the results of conditional coverage. Using (24), (26) and (27) again, we
also have
P{Y ̸∈ I (X ) | S = 1}
t t t t
 
1 (cid:88)
≤ α+E  δ t(R t,R s) | S t = 1
|C(cid:98)t|+1
s∈C(cid:98)t
−1 (cid:34) (cid:35)
(cid:88) 1
= α+ E Π (X )δ (R ,R ) | S = 1
t s t t s t
s=−n
|C(cid:98)t|+1
t−1 (cid:34) (cid:35)
(cid:88) 1
+ E Π (X )Π (X )Π (X )δ (R ,R ) | S = 1
t s s s s t t t s t
s=0
|C(cid:98)t|+1
t−1 (cid:34) (cid:35)
(cid:88) 1
+ E Π (X )[1−Π (X )][1−Π (X )]δ (R ,R ) | S = 1
t s s s s t t t s t
s=0
|C(cid:98)t|+1
−1 (cid:34) (cid:35)
= α+
(cid:88) 1
E
Π t(X t)Π t(X s)·δ t(R t,R s)
s=−n
P(S t = 1) |C(cid:98)t|+1
t−1 (cid:34) (cid:35)
+(cid:88) 1
E
Π t(X t)H s,t(X s,X t)·δ t(R t,R s)
s=0
P(S t = 1) |C(cid:98)t|+1
t−1 (cid:34) (cid:35)
+(cid:88) 1
E
Π t(X t)J s,t(X s,X t)·δ t(R t,R s)
s=0
P(S t = 1) |C(cid:98)t|+1
 
=
α+(cid:88)t−1 1
E
Π( ts,1) (X t)H s( ,s t,1) (X s,X t)·δ t(s,1) (R t,R s)

P(S = 1) (−s,1)
t |C(cid:98) |+2
s=0 t,online
 
+(cid:88)t−1 1
E
Π( ts,0) (X t)J s( ,s t,0) (X s,X t)·δ t(s,0) (R t,R s)

P(S = 1) (−s,0)
t |C(cid:98) |+2
s=0 t,online
= α.
This finishes the proof.
C Additional settings in Section 3
C.1 CAS with a fixed holdout set
In this section, we provide the FCR control results of CAS for the selection procedure in Section 3
when the selection and calibration depend only on the fixed holdout set.
The selection indicators are given as
S = Π (X ) = 1{V ≤ A({V } )}, for any t ≥ 0, (28)
t t−1 t t i −n≤i≤−1
39where A : Rn → R is some symmetric function. In this case, the selected calibration set is given by
C(cid:98)t = {−n ≤ s ≤ −1 : V
s
> A(V t,{V i} −n≤i≤−1,i̸=s)}. Then we can construct the (1−α)-conditional
PI for Y :
t
(cid:16) (cid:17)
ICAS(X ) = µ(X )±q {R } . (29)
t t (cid:98) t α i i∈C(cid:98)t
Theorem 7. Suppose {(X ,Y )} are i.i.d. data points. If the function A is invariant to the
t t t≥−n
permutation to its inputs, we can guarantee that for any T ≥ 0,
FCR(T) ≤
α+(cid:88)T E(cid:34)
S
t
(cid:88)−1
C
t,s
2(cid:12) (cid:12)qˆ(s←t)−qˆ(cid:12) (cid:12)(cid:35)
, (30)
(cid:80)
t=0
1∨ j=0S j
s=−n
|C(cid:98)t|+1 1−qˆ(s←t)
where qˆ(s←t) = F {A({V } ,V )}, qˆ= F {A({V }n )}, and F (·) is the cumulative distribution
V i i̸=s t V i i=1 V
function of {V } .
i i≥−n
If A in (28) returns the sample quantile, the next corollary shows CAS can exactly control FCR
below the target level.
Corollary C.1. If A({V }n ) is the ℓ-th smallest value in {V }n for any ℓ ≤ n−1, then the FCR
i i=1 i i=1
value can be controlled at FCR(T) ≤ α for any T ≥ 0.
Proof. We write V[n+1], V[n+1]\{s}, and V[n+1]\{t} as the ℓ-th smallest values in {V }n ∪ {V },
(ℓ) (ℓ) (ℓ) i i=1 t
{V }n ∪{V }, and {V }n , respectively. Notice that,
i i=1,i̸=s t i i=1
S = 1{V ≤ V[n+1]\{t} } = 1{V ≤ V[n+1] },
t t (ℓ) t (ℓ)
C = 1{V > V[n+1]\{s} } = 1{V > V[n+1] }.
t,s s (ℓ) s (ℓ)
Under event S C = 1, removing V or V will not change the ranks of V for i ̸= s. Hence we have
t t,s t s i
S C
·V[n+1]\{t}
= S C
·V[n+1]\{s}
.
t t,s (ℓ) t t,s (ℓ)
Together with the definitions qˆ(s←t) = F (A({V } ,V )), and qˆ = F (A({V } ,V )), we can
V i i̸=s t V i i̸=s s
conclude that
S C ·qˆ(s←t) = S C ·qˆ.
t t,s t t,s
Plugging it into (30), we get the desired bound FCR(T) ≤ α.
The next corollary provides the error bound for FCR(T) if A returns the sample mean.
Corollary C.2. Let f (·) be the density function of {V } . Suppose f (·) ≤ ρ and |A({V }n )−
V i i≥−n V v i i=1
A({V }n ,V )| ≤ γ /n for some positive constants ρ and γ . Then we have
i i=1,i̸=s t v v v
(cid:20) (cid:21)
2ρ γ 1
FCR(T) ≤ α+
v vE
.
n 1−qˆ−ρ γ /n
v v
40Proof. By the definitions of qˆ(s←t) and qˆ, we can bound their difference by
(cid:12) (cid:12)
(cid:12)qˆ(s←t)−qˆ(cid:12) ≤ |F (A({V } ,V ))−F (A({V } ,V ))|
(cid:12) (cid:12) V i i̸=s t V i i̸=s s
≤ ρ |A({V } ,V )−A({V } ,V )|
v i i̸=s t i i̸=s s
ρ γ
≤ v v , (31)
n
where we used the assumptions F′ ≤ ρ and |A({V }n )−A({V }n ,V )| ≤ γv. Plugging (31)
V v i i=1 i i=1,i̸=s t n
into the error term in (30) gives
E(cid:34) (cid:88)T E(cid:34)
S
t
(cid:88)−1
C
t,s
2(cid:12) (cid:12)qˆ(s←t)−qˆ(cid:12) (cid:12)(cid:35)(cid:35)
(cid:80)
t=0
j=0,j̸=tS j +1
s=−n
|C(cid:98)t|+1 1−qˆ(s←t)
(cid:34) T −1 (cid:35)
≤
2ρ vγ vE (cid:88) S t (cid:88) C t,s 1
n
t=0
S(T)∨1
s=−n
|C(cid:98)t|+11−qˆ(s←t)
(cid:34) T (cid:35)
≤
2ρ vγ vE (cid:88) S t 1
n S(T)∨11−qˆ−ρ γ /n
v v
t=0
(cid:34) T (cid:35)
=
2ρ vγ vE (cid:88) S t 1
n
t=0
(cid:80)T j=0,j̸=tS
j
+11−qˆ−ρ vγ v/n
(cid:34) T (cid:35)
=
2ρ vγ vE (cid:88) 1 1−qˆ
n
t=0
(cid:80)T j=0,j̸=tS
j
+11−qˆ−ρ vγ v/n
=
2ρ vγ
v
1 (cid:88)T E(cid:20) 1−qˆT+1 1−qˆ (cid:21)
n T +1 1−qˆ 1−qˆ−ρ γ /n
v v
t=0
(cid:20) (cid:21)
2ρ γ 1
≤ v vE , (32)
n 1−qˆ−ρ γ /n
v v
where the last equality holds due to (cid:80)T S ∼ Binomial(T,1−qˆ) given the calibration set such
j=0,j̸=t j
that
 −1 
(cid:88)T 1 1−qˆT+1
E  S j +1 | {Z i}n i=1 =
T +1 1−qˆ
.
j=0,j̸=t
C.2 CAS with a moving-window holdout set
In Sections 2 and 3, we construct the selected holdout set C(cid:98)t based on the full calibration set
Cincre = {−n,...,t−1}, which may lead to a heavy burden on computation and memory when t is
t
large. Now we consider an efficient online scheme by setting the holdout set as a moving window
41with fixed length n, that is C = Cwindow = {t−n,...,t−1}. As for the symmetric selection rule, we
t t
allow the selection rule Π (·) to depend on the data in Cwindow only, which means
t t
S = Π (X ) = 1{V ≤ A ({V } )}.
t t t t t i t−n≤i≤t−1
In this case, the selected calibration set is given by
C(cid:98)t = {t−n ≤ s ≤ t−1 : V
s
≤ A t({V i} t−n≤i≤t−1,i̸=s,V s)}.
Then the memory cost will be kept at n during the online process. The following theorem reveals the
property of Algorithm 1 under symmetric selection rules.
Theorem 8. Under the conditions of Theorem 5. The Algorithm 1 with C(cid:98)t = C(cid:98) tswap satisfies
   
 S ϵ (t) 9 
FCR(T) ≤ α· 1+E  max (cid:16) t n (cid:17) + , (33)
 0≤t≤T (cid:80)T S −ϵ (t) ∨1 T +n 
j=0 j n
√
where ϵ (t) = 2(cid:80)t−1 σ +( eρ+1)log(1/δ)+2−1.
n j=(t−n)∨0 j
Since the window size of the full calibration set is fixed at n, the perturbation to (cid:80)T S caused
j=0 j
by replacing V with V will be limited to (cid:80)t−1 σ .
s t j=t−n j
D Proofs for selection with symmetric thresholds
In this section, we denote C = 1{V ≤ A ({V } ,V )} the selection indicator of calibration
t,s s t j j≤t−1,j̸=s s
set C(cid:98)t.
D.1 Proof of Theorem 4
Theorem 4 can be proved through the following lemma.
Lemma D.1. Under the conditions of Theorem 4, for any i ≤ t−1, it holds that
(cid:34) (cid:35) (cid:34) (cid:35)
S C S C
E t t,s 1{R > R } = E t t,s 1{R > R } .
|C(cid:98)t|+1
t C(cid:98)t∪{t}
|C(cid:98)t|+1
s C(cid:98)t∪{t}
Proof of Theorem 4. Without loss of generality, we assume P(S = 1) > 0. Notice that for any
t
s ≤ t−1, it holds that
(cid:34) C 1{R > R } (cid:35)
E t,s t C(cid:98)t∪{t} | S = 1
t
|C(cid:98)t|+1
42n
(cid:88) 1 (cid:16) (cid:17)
= ℓ+1P C
t,s
= 1,|C(cid:98)t| = ℓ,R
t
> R
C(cid:98)t∪{t}
| S
t
= 1
ℓ=1
n
1 (cid:88) 1 (cid:16) (cid:17)
=
P(S
t
= 1)
ℓ+1P C
t,s
= 1,|C(cid:98)t| = ℓ,R
t
> R C(cid:98)t∪{t},S
t
= 1
ℓ=1
=
1 E(cid:34)1{C t,s = 1,S t = 1,R t > R C(cid:98)t∪{t}}(cid:35)
P(S t = 1) |C(cid:98)t|+1
=
1 E(cid:34) S tC t,s1{R t > R C(cid:98)t∪{t}}(cid:35)
.
P(S t = 1) |C(cid:98)t|+1
Similarly, it also holds that
E(cid:34) C t,s1{R s > R C(cid:98)t∪{t}}
| S =
1(cid:35)
=
1 E(cid:34) S tC t,s1{R s > R C(cid:98)t∪{t}}(cid:35)
.
|C(cid:98)t|+1 t P(S t = 1) |C(cid:98)t|+1
Using the upper bound (13), we have
P(cid:8) Y ̸∈ ICAS(X ) | S = 1(cid:9)
t t t t
 
1 (cid:88)(cid:16) (cid:17)
≤ α+E 
|C(cid:98)t|+1
1{R t > R C(cid:98)t∪{t}}−1{R i > R C(cid:98)t∪{t}} | S t = 1
i∈C(cid:98)t
t−1 (cid:34) (cid:35)
= α+ (cid:88) E C t,s (cid:16) 1{R > R }−1{R > R }(cid:17) | S = 1
s=−n
|C(cid:98)t|+1
t C(cid:98)t∪{t} s C(cid:98)t∪{t} t
t−1 (cid:34) (cid:35)
= α+ (cid:88) 1 E S tC t,s (cid:16) 1{R > R }−1{R > R }(cid:17)
s=−n
P(S t = 1) |C(cid:98)t|+1 t C(cid:98)t∪{t} s C(cid:98)t∪{t}
= α,
where the last equality holds due to Lemma D.1.
D.2 Proof of Theorem 5
To prove Theorem 5, we introduce the following virtual decision sequence. Given each pair (s,t) with
s ≤ t−1: if s ≥ 0, we define

 S j 0 ≤ j ≤ s−1



 1{V ≤ A ({V } )} j = s
(s←t) t s i i≤s−1
S = ;
j
1{V ≤ A ({V } ,V )} s+1 ≤ j ≤ t−1
  j j i i≤j−1,i̸=s t



S t ≤ j ≤ T
j
43if s ≤ −1, we define

1{V ≤ A ({V } ,V )} 0 ≤ j ≤ t−1
(s←t) j j i i≤j−1,i̸=s t
S = .
j
S t ≤ j ≤ T
j
Lemma D.2. Under the conditions of Theorem 4, it holds that
E(cid:34) S tC t,s 1{R t > R C(cid:98)t∪{t}}(cid:35)
=
E(cid:34) S tC t,s 1{R s > R C(cid:98)t∪{t}}(cid:35)
,
S t(T)+1 |C(cid:98)t|+1 S( ts←t) (T)+1 |C(cid:98)t|+1
where S (T) =
(cid:80)
S and
S(s←t)
(T) =
(cid:80)T S(s←t)
.
t j=0,j̸=t j t j=0,j̸=t j
Proof of Theorem 5. Under the event S = 1, we know S (T)+1 = (cid:80)T S . Using the upper bound
t t j=0 j
(13), we can get
T t−1 (cid:34) (cid:35)
FCR(T) ≤ α+(cid:88) (cid:88) E 1 S tC t,s (cid:16) 1{R > R }−1{R > R }(cid:17)
t=0s=−n
S t(T)+1|C(cid:98)t|+1 t C(cid:98)t∪{t} s C(cid:98)t∪{t}
( =i) α+(cid:88)T (cid:88)t−1 E(cid:34) S tC t,s1{R s > R C(cid:98)t∪{t}} (cid:40) 1
−
1 (cid:41)(cid:35)
t=0s=−n
|C(cid:98)t|+1 S( ts←t) (T)+1 S t(T)+1
=
α+(cid:88)T E(cid:34) S t (cid:88)t−1 C t,s1{R s > R C(cid:98)t∪{t}}
·
(cid:80)t j− =1 s∨0(S j −S j(s←t) )(cid:35)
t=0
1∨(cid:80)T j=0S
j s=−n
|C(cid:98)t|+1 S(s←t)(T)∨1
( ≤ii) α+α·E(cid:34) (cid:88)T S t max (cid:40) S t(cid:80)t j− =1 s∨0(S j −S j(s←t) )(cid:41)(cid:35) , (34)
1∨(cid:80)T S −n≤s≤t−1 S(s←t)(T)∨1
t=0 j=0 j
where (i) follows from Lemma D.2; and (ii) holds due to the definition of R such that
C(cid:98)t∪{t}
1 (cid:80)t C 1{R > R } ≤ α.
|C(cid:98)t|+1 s=−n t,s s C(cid:98)t∪{t}
Foranys < j ≤ t−1, letqˆ = F (A ({V } ))andqˆ(s←t) = F (A ({V } ,V )). Define
j V j i i≤j−1 j V j i i≤j−1,i̸=s t
a new filtration as F(s) = σ({Z } ) for 0 ≤ s−1 ≤ j ≤ t−2. Then we notice that
j i i≤j,i̸=s
(cid:104) (cid:105) (cid:104) (cid:105)
E S −S(s←t) | F(s) = E 1{V ≤ A ({V } )}−1{V ≤ A ({V } )} | F(s−1)
s s s s s i i≤s−1 t s i i≤s−1 s
= 1−qˆ −(1−qˆ ) = 0,
s s
and for any s+1 ≤ j ≤ t−1
(cid:104) (cid:105) (cid:104) (cid:104) (cid:105) (cid:105)
E S −S(s←t) | F(s) = E E S −S(s←t) | F(s) ,Z ,Z | F(s)
j j j−1 j j j s t j−1
(cid:104) (cid:105)
= E qˆ(s←t) −qˆ | F(s) .
j j j−1
(cid:104) (cid:105)
Now denote µ = E qˆ(s←t) −qˆ | F(s) for s < j ≤ t − 1 and µ = 0. We also write M =
j j j j−1 s j
S −S(s←t) −µ for s ≤ j ≤ t−1. Hence it holds that E[M | F(s) ] = 0. In addition, we also have
j j j j j−1
(cid:104) (cid:105) (cid:104) (cid:105) 1
E M2 | F(s) = E S +S(s←t)−2S S(s←t) | F(s) = 2qˆ (1−qˆ ) ≤ ,
s s−1 s s s s s−1 s s 2
44and for any s+1 ≤ j ≤ t−1,
(cid:104) (cid:105) (cid:104) (cid:105)
E M2 | F(s) ≤ E S +S(s←t) −2S S(s←t) | F(s)
j j−1 j j j j j−1
(cid:104) (cid:16) (cid:110) (cid:111)(cid:17) (cid:105)
= E 1−qˆ +1−qˆ(s←t) −2 1−max qˆ,qˆ(s←t) | F(s)
j j j j j−1
(cid:104)(cid:12) (cid:12) (cid:105)
= E (cid:12)qˆ −qˆ(s←t)(cid:12) | F(s)
(cid:12) j j (cid:12) j−1
≤ ρσ ,
j
where the last inequality holds since the density of V is bounded by ρ and the definition of σ in
i j
Assumption 2. It follows that for any λ > 0,
(cid:104) (cid:105) (cid:104) (cid:105)
E eλMj | F(s) ≤ 1+E λM +λ2M2eλ|Mj| | F(s)
j−1 j j j−1
(cid:104) (cid:105)
= 1+λ2E M2eλ|Mj| | F(s)
j j−1
(cid:104) (cid:105)
≤ 1+λ2e2λE M2 | F(s)
j j−1
≤ 1+λ2e2λρσ
j
(cid:16) (cid:17)
≤ exp λ2e2λρσ , (35)
j
where the first inequality holds due to the basic inequality ey ≤ 1+y+y2e|y| for any y ∈ R. Recall
that F = σ({Z } ). Now let
s−1 i i≤s−1
  
ℓ ℓ
 (cid:88) (cid:88) 
W ℓ = exp λ M j −λ2e2λρ2−1+ σ j , for s ≤ ℓ ≤ t−1.
 
j=s∨0 j=s∨0
Invoking (35), we have
(cid:104) (cid:105) (cid:104) (cid:110) (cid:111) (cid:105)
E W | F(s) = W E exp λM −λ2eλρσ | F(s) ≤ W ,
ℓ ℓ−1 ℓ−1 j ℓ ℓ−1 ℓ−1
(cid:104) (cid:105) (cid:104) (cid:105)
which yields E W | F(s) ≤ ··· ≤ E W | F(s) ≤ 1. Applying Markov’s inequality, for any
t−1 s−1 s s−1
δ > 0, we have
   
t−1 t−1
 (cid:88) (cid:88) log(1/δ)
P M
j
≤ 2−1+λeλρ σ j+
λ
 
j=s∨0 j=s∨0
    
t−1 t−1
 (cid:88) (cid:88)  1
= P exp λ M j −λ2eλρ2−1+ σ j > 
δ
 
j=s∨0 j=s∨0
(cid:18) (cid:19)
1
= P W >
t−1
δ
≤ δ·E[W ]
t−1
45≤ δ.
(cid:110) (cid:111)
Now we take λ = min √1 ,1 , which means (λ2eλ + 1)ρ ≤ λ2eρ + ρ ≤ ρ−1 + ρ ≤ 2. Let
eρ
√
ϵ(t) = 2(cid:80)t−1 σ +( eρ+1)log(1/δ)+2−1. Together with the fact |µ | ≤ ρσ , we have
j=0 j j j
(cid:12) (cid:12) 
(cid:12) t−1 (cid:12)
P (cid:12) (cid:12)
(cid:12)
(cid:88) S j −S j(s←t)(cid:12) (cid:12)
(cid:12)
≤ ϵ(t) ≥ 1−2δ, (36)
 
(cid:12)j=s∨0 (cid:12)
and
(cid:110) (cid:111)
P S(s←t)(T) ≥ S(T)−ϵ(t) ≥ 1−δ. (37)
Define the good event E = {the events in (36) and (37) happen}. In conjunction with (34), we
t,s
have

T
 (cid:12) (cid:12)(cid:80)t−1
S
−S(s←t)(cid:12) (cid:12)
FCR(T) ≤ α+α·E (cid:88)
t=0
1∨(cid:80)S
T
jt
=0S
j
sm ≤ta −x 1 (cid:0)1{E t,s}+1{E tc ,s}(cid:1) (cid:12) j S= (s s∨ ←0 t)(j T)∨j
1
(cid:12) 
(cid:20) (cid:26) (cid:27)(cid:21)
(i) S ϵ(t)
≤ α+α·E max t +1{Ec }
s,t (S(T)−ϵ(t))∨1 t,s
(ii)
(cid:20)1{S(T)
>
0}ϵ(t)(cid:21) (cid:20) (cid:21)
≤ α+α·E +E max1{Ec } ,
(S(T)−ϵ(t))∨1 s,t t,s
(iii)
(cid:20)1{S(T)
>
0}ϵ(t)(cid:21)
≤ α+α·E +3(T +n+1)2δ,
(S(T)−ϵ(t))∨1
where (i) holds due to the definition of E ; (ii) follows from max S = 1{S(T) > 0}; (36), (37) and
t,s t t
union’s bound. Taking δ = (T +n+1)−3 can prove the desired bound.
D.3 Proof of Theorem 8
Proof. Notice that, Lemma D.2 still holds. Following the notations in Section D.2, we can expand
FCR by
T t−1 (cid:34) (cid:35)
FCR(T) ≤ α+(cid:88) (cid:88) E 1 S tC t,s (cid:16) 1{R > R }−1{R > R }(cid:17)
t=0s=t−n
S t(T)+1|C(cid:98)t|+1 t C(cid:98)t∪{t} s C(cid:98)t∪{t}
=
α+(cid:88)T (cid:88)t−1 E(cid:34)(cid:40) 1
−
1 (cid:41) S tC t,s1{R s > R C(cid:98)t∪{t}}(cid:35)
t=0s=t−n
S( ts←t) (T)+1 S t(T)+1 |C(cid:98)t|+1
≤
α+(cid:88)T E(cid:34)
S
(cid:88)t−1 C t,s1{R s > R C(cid:98)t∪{t}} ·(cid:40) 1
−
1 (cid:41)(cid:35)
t
t=0 s=t−n
|C(cid:98)t|+1 S( ts←t) (T)+1 S t(T)+1
=
α+(cid:88)T E(cid:34) S t (cid:88)t−1 C t,s1{R s > R C(cid:98)t∪{t}}
·
(cid:80)t j− =1 s∨0S j −S j(s←t)(cid:35)
t=0
1∨(cid:80)T j=0S
j s=t−n
|C(cid:98)t|+1 S(s←t)(T)∨1
46≤ α+α·E(cid:34) (cid:88)T S t max (cid:40)(cid:80)t j− =1 s∨0S j −S j(s←t)(cid:41)(cid:35) , (38)
1∨(cid:80)T S t−n≤s≤t−1 S(s←t)(T)∨1
t=0 j=0 j
√
Let ϵ (t) = 2(cid:80)t−1 σ +( eρ+1)log(1/δ)+2−1. Similar to (36) and (37), we can show
n j=(t−n)∨0 j
(cid:12) (cid:12) 
(cid:12) t−1 (cid:12)
P (cid:12)
(cid:12)
(cid:12)
(cid:88)
S j −S
j(s←t)(cid:12)
(cid:12)
(cid:12)
≤ ϵ
n(t)
≥ 1−2δ,
 
(cid:12)j=s∨0 (cid:12)
and
(cid:110) (cid:111)
P S(s←t)(T) ≥ S(T)−ϵ (t) ≥ 1−δ.
n
Then taking δ = (T ∨n)−3, together with (38), we can guarantee
(cid:18) (cid:20) (cid:21)(cid:19)
3 S ϵ (t)
FCR(T) ≤ α· 1+ +E max t n .
T ∨n 0≤t≤T {S(T)−ϵ n(t)}∨1
D.4 Proof of Lemmas D.1 and D.2
Proof. The proof of Lemma D.1 is similar to that of Lemma D.2. Here we only prove Lemma D.2.
(cid:16) (cid:17)
Denote C = {s ≤ t : C = 1} with C ≡ S and let Q 1 (cid:80) δ be the (1−α)-
t,+ t,s t,t t 1−α |Ct,+| i∈Ct,+ Ri
quantile of the empirical distribution 1 (cid:80) δ , where δ is the point mass function at R .
|Ct,+| i∈Ct,+ Ri Ri i
Because C
t,+
= C(cid:98)t∪{t} holds under the event S
t
= 1, it suffices to show
E(cid:34) C t,tC t,s 1{R t > R C(cid:98)t∪{t}}(cid:35) = E(cid:34) C t,tC t,s 1{R s > R C(cid:98)t∪{t}}(cid:35) . (39)
S (T)+1 |C | (s←t) |C |
t t,+ S (T)+1 t,+
t
We define the event
E(z) = {[Z ,Z ] = z} = {[Z ,Z ] = [z ,z ]},
s t s t 1 2
where [Z ,Z ] and z = [z ,z ] are both unordered sets. Under E(z), define the random indexes
s t 1 2
I ,I ∈ {1,2} such that Z = z and Z = z . Notice that [V ,V ] and [R ,R ] are fixed under the
t s t It s Is s t s t
event E(z), we denote the corresponding observations as [v ,v ] and [r ,r ]. Then we know
1 2 1 2
C | E(z) = 1{V ≤ A (V ,{V } )} | E(z)
t,s t t t i i≤t−1,i̸=s

1{v > A (v ,{V } )}, I = 1
1 t 2 i i≤t−1,i̸=s s
= ,
1{v > A (v ,{V } )}, I = 2
2 t 1 i i≤t−1,i̸=s s
and
C | E(z) = 1{V ≤ A (V ,{V } )} | E(z)
t,t t t s i i≤t−1,i̸=s
47
1{v > A (v ,{V } )}, I = 1
1 t 2 i i≤t−1,i̸=s t
= .
1{v > A (v ,{V } )}, I = 2
2 t 1 i i≤t−1,i̸=s t
It follows that
C C | E(z) = 1{v > A (v ,{V } )}1{v > A (v ,{V } )},
t,s t,t 1 t 2 i i≤t−1,i̸=s 2 t 1 i i≤t−1,i̸=s
which is fixed given σ({Z } ). Further, C = 1{V > A ({V } ,v ,v )} is also fixed for
i i̸=s,t t,j j t i i≤t−1,i̸=j,s 1 2
anyj ≠ s,tgivenE(z)andσ({Z } )sinceA issymmetric. Hence, theunorderedset[{C δ } ]
i i̸=s,t t t,i Ri i≤t
is known, as well as |C | = (cid:80) C . As a consequence, we can write
t,+ i≤t t,i
C C
t,s t,t | E(z),{Z } =: F(z,{Z } ), (40)
i i̸=s,t i i̸=s,t
|C |
t,+
and
   
1 (cid:88) 1 (cid:88)
Q 1−α
|C |
δ Ri | E(z) = Q 1−α
|C |
C t,iδ Ri | E(z) =: Q(z,{Z i} i̸=s,t).
t,+ t,+
i∈Ct,+ i≤t
Then we can write
1{R > R } | E(z) = 1{r > Q(z,{Z } )}, (41)
s C(cid:98)t∪{t} Is i i̸=s,t
1{R > R } | E(z) = 1{r > Q(z,{Z } )}. (42)
t C(cid:98)t∪{t} It i i̸=s,t
In addition, it holds that
t−1 t−1
(cid:88) S(s←t) | E(z) = (cid:88) 1{V ≤ A ({V } ,V )} | E(z)
j j j i i≤j−1,i̸=s t
j=s+1 j=s+1
t−1
(cid:88)
= 1{V ≤ A ({V } ,v )}, (43)
j j i i≤j−1,i̸=s It
j=s+1
which is a function of I given σ({Z } ). Similarly, we also have
t i i̸=s,t
t−1 t−1
(cid:88) (cid:88)
S | E(z) = 1{V ≤ A ({V } ,V )} | E(z)
j j j i i≤j−1,i̸=s s
j=s+1 j=s+1
t−1
(cid:88)
= 1{V ≤ A ({V } ,v )}, (44)
j j i i≤j−1,i̸=s Is
j=s+1
which is a function of I given σ({Z } ). In addition, notice that
s i i̸=s,t
S(s←t) | E(z) = 1{V ≤ A ({V } )} | E(z) = 1{v > A ({V } )}, (45)
s t s i i≤s−1 It s i i≤s−1
48and
S | E(z) = 1{V ≤ A ({V } )} | E(z) = 1{v > A ({V } )}. (46)
s t s i i≤s−1 Is s i i≤s−1
Now define S (v ;{Z } ) = 1{v > A ({V } )}+(cid:80)t−1 1{V ≤ A ({V } ,v )} for
s:(t−1) k i i̸=s,t k s i i≤s−1 j=s+1 j j i i̸=j,s k
k = 1,2. From (43)–(46), we can write
t−1
(cid:88)
S | E(z),{Z } = S (v ;{Z } ), (47)
j i i̸=s,t s:(t−1) Is i i̸=s,t
j=s∨0
t−1
(cid:88) S(s←t) | E(z),{Z } = S (v ;{Z } ). (48)
j i i̸=s,t s:(t−1) It i i̸=s,t
j=s∨0
For any j ≤ s−1, S is fixed given {Z } . And for any j ≥ t+1, S is fixed given {Z } and
j i i≤s−1 j i i̸=s,t
E(z) since A (·) is symmetric. Therefore, we can write
j
s−1 T
(cid:88) (cid:88)
S + S | E(z),{Z } =: S(z,{Z } ). (49)
j j i i̸=s,t i i̸=s,t
j=0 j=t+1
Now using (40), (41), (42), (47), (48) and (49), we can have
(cid:20) (cid:21)
1 C C
E t,t t,s1{R > R } | {Z } ,E(z)
S t(T)+1 |C t,+| t C(cid:98)t∪{t} i i̸=s,t
( =i) E(cid:34) C t,tC t,s 1{R t > R C(cid:98)t∪{t}}
| {Z }
,E(z)(cid:35)
|C t,+| (cid:80)s j=− 01S
j
+(cid:80)t j− =1 s∨0S
j
+(cid:80)T j=t+1S
j
+1 i i̸=s,t
( =ii) F(z,{Z } )·E(cid:20) 1{r It > Q(z,{Z i} i̸=s,t)} | {Z } (cid:21)
i i̸=s,t S(z,{Z } )+S (v )+1 i i̸=s,t
i i̸=s,t s:(t−1) Is
(cid:34)
(i =ii) F(z,{Z } )· 1{r 1 > Q(z,{Z i} i̸=s,t)} ·P(I = 1)
i i̸=s,t S(z,{Z } )+S (v )+1 t
i i̸=s,t s:(t−1) 2
(cid:35)
1{r > Q(z,{Z } )}
+ 2 i i̸=s,t ·P(I = 1)
S(z,{Z } )+S (v )+1 s
i i̸=s,t s:(t−1) 1
(cid:34)
( =iv) F(z,{Z } )· 1{r 1 > Q(z,{Z i} i̸=s,t)} ·P(I = 1)
i i̸=s,t S(z,{Z } )+S (v )+1 s
i i̸=s,t s:(t−1) 2
(cid:35)
1{r > Q(z,{Z } )}
+ 2 i i̸=s,t ·P(I = 1)
S(z,{Z } )+S (v )+1 t
i i̸=s,t s:(t−1) 1
(cid:20) 1{r > Q(z,{Z } )} (cid:21)
= F(z,{Z } )·E Is i i̸=s,t | {Z }
i i̸=s,t S(z,{Z } )+S (v )+1 i i̸=s,t
i i̸=s,t s:(t−1) It
(cid:34) (cid:35)
1 C C
= E t,t t,s1{R > R } | {Z } ,E(z) .
S(s←t)
(T)+1
|C t,+| s C(cid:98)t∪{t} i i̸=s,t
t
49where (i) holds due to S ≡ C ; and (ii) follows from (40); (iii) holds because (I ,I )⊥⊥σ({Z } );
t t,t s t i i̸=s,t
and (iv) holds due to exchangeability between Z and Z such that P(I = 1) = P(I = 1). Then we
s t s t
can verify (39) by marginalizing over E(z) and the tower’s rule.
D.5 Proof of Proposition 3.1
Proof. Inthiscase,A ({V } ) = 1 (cid:80)j−1 V andA ({V } ,V ) = 1 (cid:80)j−1 V +Vt−Vs.
j i i≤j−1 n+j i=−n i j i i≤j−1,i̸=s t n+j i=−n i n+j
It follows that
(cid:20)(cid:12) (cid:12)(cid:21)
E[|A j({V i} i≤j−1)−A j({V i} i≤j−1,i̸=s,V t)| | {V i} i≤j−1,i̸=s] = E (cid:12) (cid:12)V t−V s(cid:12) (cid:12) ≤ 2σ .
(cid:12) n+j (cid:12) n+j
Now let σ = 2σ/(n+j) for j ≥ 0. Recall the definition of ϵ(t) in (10), we have
j
T−1
(cid:88) √
ϵ(t) = 2 σ +3( eρ+1)log(T +n)
j
j=0
T−1
(cid:88) 4σ √
= +3( eρ+1)log(T +n)
n+j
j=0
√
≤ 4σlog(T +n)+3( eρ+1)log(T +n)
√
≤ 4( eρ+σ+1)log(T +n).
The proof is finished.
D.6 Proof of Proposition 3.2
Lemma D.3. For almost surely distinct random variables x ,...,x ,x , let {x : r ∈ [n]} be
1 n n+1 (r)
j←(n+1)
the r-th smallest value in {x : i ∈ [n]}, and {x : r ∈ [n]} be the r-th smallest value in
i (r)
{x : i ∈ [n]\{j}}∪{x }. Then for any r ∈ [n] and j ∈ [n], we have
i n+1
(cid:12) (cid:12)
(cid:12)xj←(n+1) −x (cid:12) ≤ max(cid:8) x −x ,x −x (cid:9) .
(cid:12) (r) (r)(cid:12) (r) (r−1) (r+1) (r)
Proof. If x > x and x > x or x < x and x < x , it is easy to see xj←(n+1) = x .
j (r) n+1 (r) j (r) n+1 (r) (r) (r)
If x < x and x > x , we know xj←(n+1) = min{x ,x }, which means xj←(n+1) −
j (r) n+1 (r) (r) (r+1) n+1 (r)
x ≤ x − x . If x > x and x < x , we know xj←(n+1) = max{x ,x }, so
(r) (r+1) (r) j (r) n+1 (r) (r) (r−1) n+1
xj←(n+1) −x ≥ x −x .
(r) (r) (r−1) (r)
Lemma D.4. For almost surely distinct random variables x ,...,x , let {x : r ∈ [n]} be the
1 n (r)
[n]\{j}
r-th smallest value in {x : i ∈ [n]}, and {x : r ∈ [n − 1]} be the r-th smallest value in
i (r)
[n]\{j} [n]\{j}
{x : i ∈ [n]\{j}}, then for any r ∈ [n−1] we have: x = x if x > x and x = x
i (r) (r) j (r) (r) (r+1)
if x ≤ x .
j (r)
50Proof. The conclusion is trivial.
Lemma D.5 (Lemma 3 in Bao et al. (2024)). Let U ,··· ,U i. ∼i.d. Uniform([0,1]), and U ≤ U ≤
1 n (1) (2)
··· ≤ U be their order statistics. For any δ ∈ (0,1), it holds that
(n)
 
P  0≤m ℓ≤a nx −1(cid:8) U (ℓ+1)−U (ℓ)(cid:9) ≥ 1−2(cid:113)1
logδ
2 nlo +g 1δ  ≤ 2δ. (50)
n+1
Proposition 3.2. Let F (·) be the c.d.f. of {V } . If A takes the quantile of {V } for j ≥ 0,
v i i≥−n j i i≤j−1
then notice that
1{V ≤ A ({V } )} = 1{F (V ) ≤ A ({F (V )} )}.
j j i i≤j−1 v j j v i i≤j−1
Without loss of generality, we assume V i. ∼i.d. Uniform([0,1]). Denote V[n+j] and V[n+j]\s the r-th
i (r) (r)
smallest values in {V } and {V } respectively. Then we have
i i≤j−1 i i≤j−1,i̸=s
|A ({V } )−A ({V } ,V )|
j i i≤j−1 j i i≤j−1,i̸=s t
(cid:110) (cid:111)
≤ max
V[n+j] −V[n+j] ,V[n+j] −V[n+j]
(⌈β(n+j)⌉) (⌈β(n+j)⌉−1) (⌈β(n+j)⌉+1) (⌈β(n+j)⌉)
(cid:110) (cid:111)
≤ max
V[n+j]\s −V[n+j]\s ,V[n+j]\s −V[n+j]\s
(⌈β(n+j)⌉) (⌈β(n+j)⌉−1) (⌈β(n+j)⌉+1) (⌈β(n+j)⌉)
=: σ ,
j
where the first inequality follows from Lemma D.3; and the second inequality follows from Lemma
D.4. Invoking Lemma D.5, we can guarantee that for any δ ∈ (0,1),
j
 
1 2logδ
P σ j > (cid:113) j  ≤ 2δ j.
1−2
log(1/δj) n+j
n+j
Taking δ = (n+j)−3 and applying union’s bound, we have
j
  
T−1
(cid:91)  1 6log(n+j) (cid:88)
P  σ j > (cid:113)  ≤ (n+j)−3 ≤ (T +n)−2.
0≤j≤T−1 1−2 3log(n+j) n+j  j=0
n+j
Then with probability at least 1−(T +n)−2, it holds that
T (cid:88)−1 √
ϵ(t) = 2 σ +3( e+1)log(T +n)
j
j=0
T (cid:88)−1 2 6log(n+j) √
≤ +3( e+1)log(T +n)
(cid:113)
3log(n+j) n+j
j=0 1−2
n+j
51(i) T (cid:88)−1 24log(n+j) √
≤ +3( e+1)log(T +n)
n+j
j=0
T (cid:88)−1 24log(T +n) √
≤ +3( e+1)log(T +n)
n+j
j=0
(ii) √
≤ 24log2(T +n)+3( e+1)log(T +n),
where (i) holds due to the assumption 48logn ≤ n and n ≥ 3 (the function logx/x is decreasing on
[3,+∞)); (ii) follows from (cid:80)T−1 1 ≤ (cid:82)T+n−1 1dx ≤ log(T +n).
j=0 n+j n−1 x
D.7 Proof of Theorem 7
The following lemma is parallel to Lemma D.2, which can be proved in similar arguments in Section
D.4.
Lemma D.6. Under the conditions of Theorem 7, the following relation holds:
E(cid:34) S tC t,s 1{R t > R C(cid:98)t∪{t}}(cid:35)
=
E(cid:34) S tC t,s 1{R s > R C(cid:98)t∪{t}}(cid:35)
.
S t(T)+1 |C(cid:98)t|+1 S( ts←t) (T)+1 |C(cid:98)t|+1
Proof of Theorem 7. For s ≤ t−1, we denote C = 1{V > A({V } ,V )}. Using the definition
t,s s i i̸=s t
of quantile, it holds that
1 (cid:88)
1{R > R } ≤ α. (51)
|C(cid:98)t|+1
s C(cid:98)t∪{t}
s∈C(cid:98)t∪{t}
From the construction in (29), we also have
1{Y ̸∈ ICAS(X )} = 1{R > R }. (52)
t t t t C(cid:98)t∪{t}
By arranging (51) and (52), we can upper bound the miscoverage indicator as
1 (cid:88)
1{Y ̸∈ ICAS(X )} ≤ α+ 1{R > R }−1{R > R }. (53)
t t t
|C(cid:98)t|+1
t C(cid:98)t∪{t} s C(cid:98)t∪{t}
s∈C(cid:98)t
For each pair (s,t) with s ∈ C(cid:98)t, we introduce a sequence of virtual decision indicators:
S(s←t) = 1{V ≤ A({V } ,V )}, for 0 ≤ j ≤ T,j ̸= t. (54)
j j i i̸=s t
Correspondingly, we denote S (T) = (cid:80)T S and S(s←t) (T) = (cid:80)T S(s←t). Plugging (53)
t j=0,j̸=t j t j=0,j̸=t j
into the definition of FCR gives
(cid:34) T (cid:35)
FCR(T) = E (cid:88) S t 1{Y ̸∈ ICAS(X )}
S (T)+1 t t t
t
t=0
52 
T
≤ α+E (cid:88)
t=0
S
t(TS )t +1|C(cid:98)t|1
+1
s(cid:88) ∈C(cid:98)t(cid:16) 1{R t > R C(cid:98)t∪{t}}−1{R s > R C(cid:98)t∪{t}}(cid:17) 
T −1 (cid:34) (cid:35)
= α+(cid:88) (cid:88) E 1 S tC t,s (cid:16) 1{R > R }−1{R > R }(cid:17)
t=0s=−n
S t(T)+1|C(cid:98)t|+1 t C(cid:98)t∪{t} s C(cid:98)t∪{t}
T −1 (cid:34)(cid:40) (cid:41) (cid:35)
= α+(cid:88) (cid:88) E 1 − 1 S tC t,s 1{R > R }
t=0s=−n
S( ts←t) (T)+1 S t(T)+1 |C(cid:98)t|+1 s C(cid:98)t∪{t}
T −1 (cid:34)(cid:12) (cid:12) (cid:35)
≤ α+(cid:88) (cid:88) E (cid:12) (cid:12) 1 − 1 (cid:12) (cid:12)· S tC t,s , (55)
t=0s=−n
(cid:12) (cid:12)(cid:80) j=0,j̸=tS j(s←t) +1 (cid:80) j=0,j̸=tS j +1(cid:12) (cid:12) |C(cid:98)t|+1
where the last equality follows from Lemma D.6. Let F (·) be the c.d.f. of {V } . Denote
V i i≥−n
qˆ(s←t) = F {A({V } ,V )}, and q = F {A({V } ,V )}. Then given Z ,{Z } , we know
V i i̸=s t V i i̸=s s t i 1≤i≤n
(cid:80) S ∼ Binomial(T,1−qˆ) and (cid:80) S(s←t) ∼ Binomial(T,1−qˆ(s←t)), which further yield
j=0,j̸=t j j=0,j̸=t j
(cid:34) (cid:35)
1 1
E − | Z ,{Z }
(cid:80) (s←t) (cid:80) S +1 t i 1≤i≤n
S +1 j=0,j̸=t j
j=0,j̸=t j
1−(qˆ(s←t))T+1 1−qˆT+1
= −
(T +1)(1−qˆ(s←t)) (T +1)(1−qˆ)
(cid:40) (cid:41)
1−qˆT+1 1−qˆ 1−(qˆ(s←t))T+1
= −1
(T +1)(1−qˆ) 1−qˆ(s←t) 1−qˆT+1
(cid:34) (cid:35) (cid:40) (cid:41)
1 1−qˆ 1−(qˆ(s←t))T+1
= E | Z ,{Z } · −1 . (56)
(cid:80) S +1 t i 1≤i≤n 1−qˆ(s←t) 1−qˆT+1
j=0,j̸=t j
Notice that, if qˆ(s←t) ≥ qˆ,
1−qˆ 1−(qˆ(s←t))T+1
−1 (57)
1−qˆ(s←t) 1−qˆT+1
1−(qˆ(s←t))T+1qˆ(s←t)−qˆ (qˆ(s←t))T+1−qˆT+1
= +
1−qˆT+1 1−qˆ(t) 1−qˆT+1
1−(qˆ(s←t))T+1qˆ(s←t)−qˆ (cid:0) qˆ(s←t)−qˆ(cid:1)(cid:80)T (qˆ(s←t))kqT−k
= + k=0
1−qˆT+1 1−qˆ(t) 1−qˆT+1
qˆ(s←t)−qˆ qˆ(s←t)−qˆ
≤ +
1−qˆ(t) 1−qˆ
2(qˆ(s←t)−qˆ)
= . (58)
1−qˆ(s←t)
Since S t, C t,s, |C(cid:98)t| and 1{R
s
> R C(cid:98)t∪{t}} depend only on calibration set and Z t, substituting (56) and
(57) into (55) results in the following upper bound
FCR(T) ≤
α+E(cid:34) (cid:88)T S
t
(cid:88)−1 C t,s1{qˆ(s←t) ≥ qˆ}2(qˆ(s←t)−qˆ)(cid:35)
(cid:80)
t=0
j=0,j̸=tS j +1
s=−n
|C(cid:98)t|+1 1−qˆ(s←t)
53≤
α+E(cid:34) (cid:88)T
S
t
(cid:88)−1
C
t,s
2(cid:12) (cid:12)qˆ(s←t)−qˆ(cid:12) (cid:12)(cid:35)
.
(cid:80)
t=0
j=0,j̸=tS j +1
s=−n
|C(cid:98)t|+1 1−qˆ(s←t)
E CAS under distribution shift
(cid:110) (cid:111)
Denote the selection time by {τ ,...,τ ,τ }, where τ = inf 0 ≤ t ≤ T : (cid:80)t S = m and
1 M M+1 m j=0 j
τ = τ +1. Then from Algorithm 2, we know
M+1 M
αi ← αi +γ (α−erri ), for m ≤ M.
τm+1 τm i τm
Lemma E.1 (Lemma 4.1 of Gibbs and Candès (2021), modified.). With probability one we have that
αi ∈ [−γi ,1+γi ] for m ≤ M.
τm τm τm
Proof of Theorem 6. TheproofisadaptedfromtheproofofTheorem3.2inGibbsandCandès(2022),
and here we provide it for completeness. In this part, we write E [·] as the expectation taken over the
A
randomness from the algorithm. Let (cid:80)T S = M. Let α˜ = (cid:80)k pi tαi t with pi = wi/((cid:80)k wj).
j=0 j t i=1 γi t t j=1 t
From the update rule of Algorithm 2, we know
α˜ =
(cid:88)k pi
τm (cid:0) αi +γ (erri −α)(cid:1)
τm γ τm+1 i τm
i
i=1
=
(cid:88)k pi τmα τi
m+1
+(cid:88)k
pi (erri −α)
γ τm τm
i
i=1 i=1
= α˜
+(cid:88)k (pi
τm
−p τm+1)α τi
m+1
+(cid:88)k
pi (erri −α).
τm+1 γ τm τm
i
i=1 i=1
Notice that α = αi with probability pi , hence err = erri with probability pi , which means
τm τm τm τm τm τm
E [err ] = (cid:80)k pi erri . It follows that
A τm i=1 τm τm
E [err ]−α = α˜ −α˜
+(cid:88)k (p
τm+1
−pi τm)α τi
m+1. (59)
A τm τm τm+1
γ
i
i=1
Now, denote W = (cid:80)k wi and p˜i = pi τmexp(−ητmℓ(βτm,αi τm)) . From the definition of
τm i=1 τm τm+1 (cid:80)k j=1pj τmexp(−ητmℓ(βτm,αj τm))
pi , we know
τm+1
wi /W
pi = τm+1 τm
τm+1 (cid:80)k wj /W
j=1 τm+1 τm
(1−ϕ )w¯i /W +ϕ (cid:80)k (w¯i /W )/k
=
τm τm τm τm j=1 τm τm
(1−ϕ )(cid:80)k w¯i /W +ϕ (cid:80)k w¯i /W
τm i=1 τm τm τm j=1 τm τm
54(cid:16) (cid:17)
(1−ϕ )pi exp(cid:0) −η ℓ(β ,αi )(cid:1) +ϕ (cid:80)k pj exp −η ℓ(β ,αj ) /k
τm τm τm τm τm τm j=1 τm τm τm τm
=
(cid:80)k pi exp(cid:0) −η ℓ(β ,αi )(cid:1)
i=1 τm τm τm τm
ϕ
= (1−ϕ )p˜i + τm. (60)
τm τm+1 k
Further, we also have
pi exp(cid:0) −η ℓ(β ,αi )(cid:1)
p˜i −pi = τm τm τm τm −pi
τm+1 τm (cid:80)k pj exp(cid:16) −η ℓ(β ,αj )(cid:17) τm
j=1 τm τm τm τm
(cid:110) (cid:16) (cid:17)(cid:111)
(cid:80)k pj exp(cid:0) −η ℓ(β ,αi )(cid:1) −exp −η ℓ(β ,αj )
= pi ·
j=1 τm τm τm τm τm τm τm
τm (cid:80)k pj exp(cid:16) −η ℓ(β ,αj )(cid:17)
j=1 τm τm τm τm
(cid:16) (cid:17)(cid:110) (cid:16) (cid:104) (cid:105)(cid:17) (cid:111)
(cid:80)k pj exp −η ℓ(β ,αj ) exp η ℓ(β ,αj )−ℓ(β ,αi ) −1
= pi ·
j=1 τm τm τm τm τm τm τm τm τm
τm (cid:80)k pj exp(cid:16) −η ℓ(β ,αj )(cid:17)
j=1 τm τm τm τm
k
= pi ·(cid:88) p˜j (cid:8) exp(cid:0) η (cid:2) ℓ(β ,αj )−ℓ(β ,αi )(cid:3)(cid:1) −1(cid:9) . (61)
τm τm τm τm τm τm τm
j=1
(cid:12) (cid:12)
By Lemma E.1 we know αi ∈ [−γi ,1 + γi ], which implies (cid:12)ℓ(β ,αj )−ℓ(β ,αi )(cid:12) ≤
τm τm τm (cid:12) τm τm τm τm (cid:12)
(cid:12) (cid:12)
max{α,1−α}(cid:12)αj −αi (cid:12) ≤ 1+2γ . By the intermediate value theorem, we can have
(cid:12) τm τm(cid:12) max
(cid:12) (cid:12)exp(cid:0) η τm(cid:2) ℓ(β τm,α τj m)−ℓ(β τm,α τi m)(cid:3)(cid:1) −1(cid:12) (cid:12) ≤ η τm(1+2γ max)exp{η τm(1+2γ max)}.
Plugging it into (61) yields
(cid:12) (cid:12)
(cid:12)p˜i −pi (cid:12) ≤ pi η (1+2γ )exp{η (1+2γ )}.
(cid:12) τm+1 τm(cid:12) τm τm max τm max
Together with (60), we have
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(p −pi )αi (cid:12) (cid:12)(p˜ −pi )αi (cid:12) (cid:12)(p −pi )αi (cid:12)
(cid:12) τm+1 τm τm+1(cid:12) ≤ (1−ϕ )(cid:12) τm+1 τm τm+1(cid:12)+ϕ (cid:12) τm+1 τm τm+1(cid:12)
(cid:12) γ (cid:12) τm (cid:12) γ (cid:12) τm(cid:12) γ (cid:12)
(cid:12) i (cid:12) (cid:12) i (cid:12) (cid:12) i (cid:12)
η (1+2γ )2 1+γ
≤
τm max
exp{η (1+2γ )}+2ϕ
max
,
γ
τm max τm
γ
min min
where we used Lemma E.1. Telescoping the recursion (59) from m = 1 to m = M, we can get
(cid:88)M (E A[err τm]−α) ≤ (cid:12) (cid:12)α˜
τ1
−α˜ τM+1(cid:12) (cid:12)+ (1+ γ2γ max)2 (cid:88)M η τmexp{η τm(1+2γ max)}
min
m=1 m=1
M
1+γ max (cid:88)
+2 ϕ
γ
τm
min
m=1
551+2γ
max
(1+2γ max)2 (cid:88)M
≤ + η exp{η (1+2γ )}
γ γ
τm τm max
min min
m=1
M
1+γ max (cid:88)
+2 ϕ .
γ
τm
min
m=1
According to the definition of τ , we can rewrite the above relation as
m
(cid:88)T
S (E [err ]−α) ≤
1+2γ
max +
(1+2γ max)2 (cid:88)T
S η exp{η (1+2γ )}
t A t t t t max
γ γ
min min
t=0 t=0
T
1+γ max (cid:88)
+2 S ϕ .
t t
γ
min
t=0
Since the randomness of Algorithm 2 is independent of the decisions {S }T and the data {Z }T ,
i i=0 i i=−n
we have
(cid:34)(cid:80)T
S ·err
(cid:35) (cid:34)(cid:80)T
S ·(E [err
]−α)(cid:35)
E t=0 t t −α = E t=0 t A t .
(cid:80)T
S
(cid:80)T
S
j=0 j j=0 j
The conclusion follows immediately.
F Additional simulation details
F.1 Details of e-LOND-CI
The e-LOND-CI is similar to LORD-CI, except for using e-values and LOND procedure instead.
At each time t, the prediction interval is constructed as {y : e (X ,y) < α−1}, where e (X ,y) is
t t t t t
the e-value at time t associated with X and y and α is the target level at time t computed by
t t
α = αγLOND(S +1), where γLOND is discount sequence. We choose γLOND = 1/{t(t−1)} as Xu
t t t−1 t t
and Ramdas (2023) suggested.
The e-value for constructing prediction intervals is transformed by p-values. By the duality of
confidence interval and hypothesis testing, we can invert the task of constructing prediction intervals
as testing. Let H : Y = y, then the p-values are defined as
0t t
(cid:80) 1{|y−µ(X )| ≤ R }+1
p (X ,y) = i∈Ct (cid:98) t i .
t t
|C |+1
t
Following Xu and Ramdas (2023), we can directly convert this p-value into
1{p (X ,y) ≤ α }
t t t
e (X ,y) = .
t t
α
t
ByasamediscussionasProposition2inXuandRamdas(2023),wecanverifythatE[e (X ,Y )1{Y =
t t t t
y}] ≤ 1, hence e (X ,Y ) is a valid e-value.
t t t
56Method CAS OCP LORD−CI e−LOND−CI
Scenario A Scenario B Scenario C
20
10
0
250 500 750 250 500 750 250 500 750
Time
Scenario A Scenario B Scenario C
90
60
30
250 500 750 250 500 750 250 500 750
Time
Figure F.1: Real-time FCR plot and average length plot from time 20 to 2,000 for e-LOND-CI. The selection
rule is Dec-driven and the incremental holdout set with window size 200 is considered. The black dashed line
represents the target FCR level 10%.
We provide additional simulations for e-LOND-CI. Figure F.1 illustrates the FCR and average
length under different scenarios using decision-driven selection for e-LOND-CI. As it is shown, the
prediction intervals produced by e-LOND-CI are considerably wide, limiting the e-LOND-CI to
provide non-trivial uncertainty quantification.
F.2 Details of online multiple testing procedure using conformal p-values
Recall that the selection problem can be viewed as the following multiple hypothesis tests: for time t
and some constant b ∈ R,
0
H : Y ≥ b v.s. H : Y < b .
0,t i 0 1,t t 0
Denote the additional labelled data set for computing conformal p-values as C . We write the index
p
set of null samples in C as C = {i ∈ C : Y ≥ b }. For each test data point, the conformal p-value
p p,0 p i 0
57
)%(RCF
htgneL
egarevAbased on same-class calibration (Bates et al., 2023) can be calculated by
1+|{i ∈ C : g(X ) ≤ g(X )}|
p = p,0 i t , (62)
t
|C |+1
p,0
where g(x) = µˆ(x)−b is the nonconformity score function for constructing p-values.
0
To control the FDR at the level β ∈ (0,1), we deploy the SAFFRON (Ramdas et al., 2018)
procedure. The main idea of SAFFRON is to make a more precise estimation of current FDP by
incorporating the null proportion information. Given β ∈ (0,1), the user starts to pick a constant
λ ∈ (0,1) used for estimating the null proportion, an initial wealth W ≤ β and a positive non-
0
increasing sequence {γ }∞ of summing to one. The SAFFRON begins by allocation the rejection
j j=1
threshold β = min{(1−λ)γ W ,λ} and for t ≤ 2 it sets:
1 1 0
(cid:110) (cid:16) (cid:88) (cid:17)(cid:111)
β = min λ,(1−λ) W γ +(α−W )γ + βγ ,
t 0 t−C0+ 0 t−τ1−C1+ t−τj−Cj+
j≥2
where τ is the time of the j-th rejection (define τ = 0), and C = (cid:80)t−1 1{p ≤ λ}. Thus for
j 0 j+ i=τj+1 i
each time t, we reject the hypothesis if p ≤ β . In our experiment, we set defaulted parameters,
t t
where W = β/2, λ = 0.5 and γ ∝ 1/j1.6.
0 j
F.3 Experiments on fixed calibration set
We verify the validity of our algorithms with respect to a fixed calibration set. The size of the fixed
calibration set is set as 50, and the procedure stops at time 1,000. We design a decision-driven
selection strategy. At each time t, the selection indicator is S = 1{V > τ(S(t))}, where V = µ(X )
t t t (cid:98) t
and τ(s) = τ −min{s/50,2}. The parameter τ is pre-fixed for each scenario. Three different initial
0 0
thresholds for different scenarios due to the change of the scale of the data. The thresholds τ are set
0
as 1, 4 and 3 for Scenarios A, B and C respectively. This selection rule is more aggressive when the
number of selected samples is small.
We choose the target FCR level as α = 10%. The real-time results are demonstrated in Figure
F.2 based on 500 times repetitions. Across all the settings, it is evident that the SCOP is able to
deliver quite accurate FCR control and have more narrowed PIs.
F.4 Comparisons of C(cid:98)inter and C(cid:98)naive for decision-driven selection rule
t t
We make empirical comparisons of C(cid:98)inter and C(cid:98)naive using the same Dec-driven rule over incremental
t t
holdout set. All the settings are the same except for that we use an initial holdout set with size 20.
The results are demonstrated in Figure F.3. The CAS-inter (C(cid:98)inter) has smaller FCR value and a
t
slightly wider interval compared to CAS-naive (C(cid:98)naive). But the difference is not significant and the
t
power loss is acceptable.
58Method CAS OCP LORD−CI
Scenario A Scenario B Scenario C
20
10
0
250 500 750 250 500 750 250 500 750
Time
Scenario A Scenario B Scenario C
25
20
15
10
250 500 750 250 500 750 250 500 750
Time
Figure F.2: Real-time FCR plot and average length plot from time 20 to 1,000 for fixed calibration set after
500 replications. The black dashed line represents the target FCR level 10%.
59
)%(RCF
htgneL
egarevAMethod CAS−inter CAS−naive
Scenario A Scenario B Scenario C
10.5
10.0
9.5
9.0
250 500 750 250 500 750 250 500 750
Time
Scenario A Scenario B Scenario C
15
12
9
250 500 750 250 500 750 250 500 750
Time
Figure F.3: Comparison for CAS-inter and CAS-naive by real-time FCR plot and average length plot from
time 50 to 1000 after 500 replications. The black dashed line represents the target FCR level 10%.
60
)%(RCF
htgneL
egarevAF.5 Comparisons of C(cid:98)swap and C(cid:98)naive for symmetric selection rule
t t
We study the difference of C(cid:98)swap and C(cid:98)naive for quantile selection rule. Figure F.4 displays the results
t t
for both methods under three scenarios using 70%-quantile selection rule. The CAS-Swap (using
C(cid:98)swap) and CAS-naive (using C(cid:98)naive) perform almost identical.
t t
Method CAS−swap CAS−naive
Scenario A Scenario B Scenario C
10.0
9.8
9.6
250 500 750 250 500 750 250 500 750
Time
Scenario A Scenario B Scenario C
15.0
12.5
10.0
7.5
250 500 750 250 500 750 250 500 750
Time
Figure F.4: Comparison for CAS-swap and CAS-naive by real-time FCR plot and average length plot from
time 50 to 1,000 for full calibration set after 500 replications. The black dashed line represents the target FCR
level 10%.
61
)%(RCF
htgneL
egarevA