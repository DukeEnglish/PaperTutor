Beyond Text: Frozen Large Language Models in Visual Signal Comprehension
LeiZhu1 FangyunWei2* YanyeLu1
1PekingUniversity 2MicrosoftResearchAsia
zhulei@stu.pku.edu.cn fawe@microsoft.com yanye.lu@pku.edu.cn
Abstract
d ue gb Áà™ G Link
V2L All cat Ap „Åô
In this work, we investigate the potential of a large LLM
language model (LLM) to directly comprehend visual sig-
Tokenizer Â•Ω ƒ´na to B wr no
nals without the necessity of fine-tuning on multi-modal Ea „Å≠ Eye L Vocabulary
datasets. The foundational concept of our method views
an image as a linguistic entity, and translates it to a set LLM
of discrete words derived from the LLM‚Äôs vocabulary. To
achieve this, we present the Vision-to-Language Tokenizer, Image Recognition VQA Image Caption
abbreviated as V2T Tokenizer, which transforms an image Inpainting Outpainting Deblur Image Restoration
into a ‚Äúforeign language‚Äù with the combined aid of an Figure1. IllustrationofourV2LTokenizer(Vision-to-Language
encoder-decoder, theLLMvocabulary, andaCLIPmodel. Tokenizer). The V2L Tokenizer translates an image into a col-
With this innovative image encoding, the LLM gains the lectionofinterpretabletokensderivedfromanLLMvocabulary.
ability not only for visual comprehension but also for im- Subsequently, the frozen LLM can comprehend the visual sig-
age denoising and restoration in an auto-regressive fash- nalsandperformmulti-modalunderstandingtasks(highlightedin
ion‚Äîcrucially, without any fine-tuning. We undertake rig- Blue)andimagedenoisingtasks(highlightedinOrange)without
thenecessityoffine-tuning.
orous experiments to validate our method, encompassing
understanding tasks like image recognition, image cap-
As illustrated in Figure 1, this work aims to equip a
tioning, and visual question answering, as well as image
largelanguagemodelwiththeinnateabilitytocomprehend
denoising tasks like inpainting, outpainting, deblurring,
visual signals, importantly, without the necessity of fine-
and shift restoration. Code and models are available at
tuning. Inourapproach, wevieweachimageasalinguis-
https://github.com/zh460045050/V2L-Tokenizer.
ticentityderivedfroma‚Äúforeignlanguage‚Äù, adaptingitto
1.Introduction suit the input requirements of a plain LLM. Consequently,
thisalignmentoccursintheinput(token)spaceratherthan
Significant advancements have been achieved in the field
in the feature space, distinguishing our work from previ-
of natural language processing (NLP) through the deploy-
ousmulti-modalmethodologies[1,23,24,62]thatrequire
ment of large language models (LLMs), such as GPT [3,
fine-tuningformodalityalignment. Thus,thefine-tuningor
30, 34, 35], PaLM [2, 6] and LLaMA [45, 46]. In pursuit
re-trainingprocessonmulti-modaldatasetsisavoidablein
of addressing intricate challenges necessitating the combi-
our methodology. Our technique translates an image into
nationoftextandvisualunderstanding,scholarsarebroad-
a collection of discrete tokens that are within the vocabu-
ening the capacities of the off-the-shelf LLMs. This en-
lary of the LLM. Once translated, these tokens can be fed
hancement involves the incorporation of additional visual
intotheLLM,enablingittoprocessandcomprehendvisual
processing components that facilitate the understanding of
information, thereby facilitating a range of tasks involving
visual content [13, 23‚Äì25, 62] or the generation of images
bothimageunderstandinganddenoising.
from text [41, 50, 59, 61]. Subsequently, these improved
Translating an image into a set of tokens that a frozen
models undergo an extra re-training or fine-tuning using
LLMcanunderstandischallenging. Inthiswork,weintro-
variousmulti-modaldatasetstoalignthevisuallatentspace
duce a tokenizer designed to map images (a non-linguistic
withthelanguagelatentspace.Nevertheless,therefinement
modality)totheinput(token)spaceofafrozenLLM.This
processgenerallyrequiresasubstantialamountoftraining
tokenizer is termed the Vision-to-Language Tokenizer, or
resources.
V2L Tokenizer in brief. Drawing inspiration from the tri-
*Correspondingauthor. umphant advances of VQ-GAN [12], the V2L Tokenizer
4202
raM
21
]VC.sc[
1v47870.3042:viXraemploysanencoder-quantizer-decoderstructure. However, adversarial and perceptual losses, enabling the codebook
its target is to translate visual information into the LLM‚Äôs tocapturemorepreciseandfinelydetailedrepresentations.
token space. This differs from its inspiration, which aims Meanwhile,quantizinganimageintoaseriesoftokensen-
to learn an independent latent space solely for the purpose ablesimagegenerationinanauto-regressivemannerusing
ofimagegeneration. OurV2LTokenizereschewsthestan- GPT [3, 34, 35]. RQ-VAE [20] employs a residual quan-
dardprocessofoptimizingarandomlyinitializedquantizer tization approach, where each image patch is represented
codebook; instead, itleveragesthepre-existingvocabulary bymultiplecodebooktokens,tomoreaccuratelymirrorthe
oftheLLMasitsquantizercodebookthroughoutthetrain- original image features. DQ-VAE [17] further present to-
ingprocess. Withtheguidanceofaquantizationlossfunc- kensofvariablelengthtoencodeimages,resultinginmore
tion, images are converted into a set of LLM tokens upon preciseandefficienttokenization. Reg-VQ[56]aimstoim-
completionoftheoptimizationprocess. prove the utilization of the codebook and prevent its col-
Typically,thevocabularyofanLLMconsistsofbothfull lapsebyleveragingpriordistributionregularization.
wordsandsubwordunitsduetotheusageoflanguagetok- Large Language Models. Large language models
enizerssuchasBPE[42]andSentencePiece[19]. Without (LLMs) [2, 3, 9, 38, 46, 58], especially those employ-
lossofgenerality,thebreadthofthisvocabularyinfluences ingaTransformer-decoderarchitecture[2,3,46,58],have
itsabilitytoencodeimagesintoLLMtokens‚Äîalargervo- made considerable progress in the domain of natural lan-
cabularyusuallyoffersmorepowerfulrepresentationcapa- guage processing. The process of developing an effective
bilities. In our approach, we expand the LLM‚Äôs vocabu- Large Language Model (LLM) generally unfolds in multi-
lary by combining its lexical items to form bigrams or tri- ple phases, including initial pre-training [2, 3, 6, 15, 43],
grams,whichsignificantlyaugmentstherepresentationca- subsequentsupervisedfine-tuning[5,13,31,57],thetrain-
pacity when mapping an image into the LLM tokens. In ingofrewardmodels[7,10,37],andtheapplicationofre-
additiontoconvertingeachimagepatchintoalanguageto- inforcement learning using human feedback (RLHF) [14,
ken,ourV2Ltokenizerincludesextractingglobalrepresen- 29‚Äì31, 46] to achieve alignment with instructions. The
tationsfortheentireimage.Weaccomplishthisbyutilizing LLaMA [45] family has been at the forefront of offering
a combination of subwords, bigrams, or trigrams from the open-sourceLLMs,providingbothalignedandnon-aligned
expandedLLMvocabularytoencapsulatetheimage‚Äôscom- versions in an array of scales [11, 18, 44‚Äì46, 51, 58]. For
prehensiveinformation. instance,theLLaMA2[46]presentsmodelsinthesizesof
In-context learning [3, 27, 28] has been shown to be 7B,13B,and70Bparameters.
highly beneficial for zero-shot inference in LLMs. This is
Visual Signal Comprehension with LLMs: Despite the
accomplishedbyprefacingtheinstructiontextwithanum-
inherent capability for natural language understanding,
berofdomain-specificexamplesduringtheLLMinference.
LLMs can also act as decoders in various vision-language
OurmethodeschewsthenecessityofLLMfine-tuning,in-
applications by employing a modality bridge module to
stead employing in-context learning to guide the LLM in
align the visual with language features [1, 13, 21‚Äì24, 26,
imitatingthepatternspresentedinthegivenfew-shotsam-
49,52,60,62]. Forexample,Flamingo[1]utilizesbillions
ples. Thisenablesthemodeltobettercomprehendthe‚Äúfor-
ofimage-textpairstotraingatedcross-attentionlayersthat
eignlanguage‚Äù(i.e.,visualmodality).
facilitate the synchronization between a frozen vision en-
Experimentally, our work surpasses previous at-
coder and a frozen LLM. In a similar vein, BLIP-2 [23]
tempts [25, 54] in this novel scenario, where an LLM is
bridges the modality gap by introducing a lightweight Q-
able to comprehend visual signals without any fine-tuning
Former. ThisQ-Formeristrainedintworespectivestages:
or re-training, encompassing understanding tasks like im-
oneforrepresentativelearningandtheotherforgenerative
age captioning and visual question answering, as well as
learning.Inaddition,bothMiniGPT-4[62]andLLaVA[24]
image denoising tasks like inpainting, outpainting, deblur-
confirmthattuningasinglelinearlayeronhigh-qualityin-
ring,andimagerestoration.
struction data, is sufficient for feature alignment. While
thesemethodsyieldsatisfactoryresultsformulti-modalun-
2.RelatedWork
derstanding tasks, they lack the ability to generate visual
Image Quantization: The process of image quantization content and necessitate the collection of additional image-
isdesignedtotransformimagesintoaseriesofdiscreteto- textpairstotrainthevision-languagealignmentmodules.
kensderivedfromacodebook[12,17,20,32,33,39,48,53, Insteadofperformingmulti-modalalignmentinthefea-
55,56]. VQ-VAE[48]standsasanotableworkinthefield. ture space, several methods map images to the token (in-
Thismethodemploysanencoder-decoderstructuretoquan- put)spaceoftheLLMsbyviewingimagesas‚Äúforeignlan-
tizeimagesintoacollectionoflatent,discretecodes,which guages‚Äù[25,47,54]. Forinstance,LQAE[25]trainsaVQ-
arethenusedtoreconstructtheimages. VQ-GAN[12]en- VAEtokenizerwithafrozenLLMcodebooktoquantizean
hances the process of codebook learning by incorporating image into a set of language tokens. To enable an LLMLocal Codebook Generator
LLM Vocabulary
LLM Embeddings Projected LLM Embeddings
CLIP
LLM ‚Ä¶ Projector ‚Ä¶
Text-Encoder
{t 1, t 2, t 3, ‚Ä¶, t ùëÅ}
Local
Quantizer
VQ-GAN
Encoder Local Feature Local Tokens Decoder
Loss
Global ‚Ä¶
Quantizer
Global Feature K Global Tokens
LLM Vocabulary Expanded LLM (E-LLM) Vocabulary
Codebook E- CLIP
LLM ‚Ä¶
Expansion LLM Text-Encoder
E-LLM Embeddings
{t 1, t 2, t 3, ‚Ä¶, t ùëÅ} {t 1, t 2, t 3, ‚Ä¶, t ùëÅ} ‚à™ {t 1t 2, t 1t 3, ‚Ä¶, t ùëÅt 1} ‚à™{t 1t 2t 3, t 1t 2t 4, ‚Ä¶, t ùëÅt 1t 2}
Subwords Bigrams Trigrams
Global Codebook Generator
Figure2.OverviewofourVision-to-LanguageTokenizer(V2LTokenizer).Figure3illustratesitsintegrationwithafrozenLLM.
toperformbothimageunderstandingandgenerationtasks, shown in Figure 3, we can perform a series of tasks such
SPAE[54]furtherenhancesthequalityofquantizedimage as image classification, image caption, visual question an-
tokens derived from a frozen LLM codebook. It does so swering,andimagedenoising. Thisisdonebyfeedingthe
by incorporating a hierarchical quantization technique and concatenationoftaskinstructions,in-contextlearningsam-
semantic guidance provided by CLIP [36]. However, be- ples,andeitherglobalorlocaltokensintoafrozenLLMin
cause of the substantial difference between visual features anauto-regressivemanner.
and language token embeddings, those methods struggle
3.2.Vision-to-LanguageTokenizer
to assign semantic language tokens to images. This limi-
OurVision-to-LanguageTokenizer(V2LTokenizer)adopts
tation hinders LLMs from fully understanding visual sig-
anencoder-quantizer-decoderstructure.Intotal,weemploy
nals within a given context. In contrast to the aforemen-
two quantizers: a local quantizer and a global quantizer.
tioned methods, our approach introduces image quantiza-
Each of these is associated with an independent, frozen
tion within a shared multi-modal space, assigning seman-
codebook derived from the LLM vocabulary. An image is
tically meaningful language tokens to a given image. Fur-
then quantized into K global tokens and K local tokens,
thermore,weseparatetheimagetokensintotwocategories: g l
drawnfromtheglobalandlocalcodebooks,respectively.
global tokens, which are used for image comprehension
Global Codebook. An LLM vocabulary comprises a set
tasks, and local tokens, which are utilized for image gen-
ofsubwordsgeneratedbylanguagetokenizers. Thesesub-
eration tasks. This separation is accomplished through the
word elements, in general, tend to have limited semantic
useoftwodistincttypesofquantizersalongwithtwoinde-
significance. To enhance the semantic representation of
pendentcodebooks.
entities within the LLM vocabulary T of size N, we in-
3.Method troduce a vocabulary expansion technique. This technique
entailscreatingbigramsandtrigramsbycombiningtwoor
3.1.ProblemFormulationandOverview
threelexicalitemsfromT. However,itisimportanttonote
We view images as a ‚Äúforeign language‚Äù. Given an LLM thattheresultingbigramsandtrigramsmaynotnecessarily
vocabulary T = {t ,t ,...,t } containing N language convey meaningful semantics. For instance, they may in-
1 2 N
tokens, wetranslateanimageintoK discretetokens, each clude symbols like ‚Äù#‚Äù and ‚Äù!‚Äù. Moreover, the generation
of which belongs to T. This translation is accomplished of bigrams and trigrams leads to a vast number of possi-
by our V2L Tokenizer, as illustrated in Figure 2. In our ble combinations‚ÄîN2 bigrams and N3 trigrams‚Äîwhich
implementation, an image is tokenized into K global to- presentschallengesinsubsequentquantizationprocesses.
g
kens for understanding tasks, and K local tokens for de- To address this issue, we introduce a simple filter strat-
l
noising tasks, where K = K + K . Subsequently, as egy. Specifically,usinganimagequantizationdataset(such
g lFor each of the following input-out pairs, output Generate a caption sentence based on words describing an image.
is one of [‚ÄòFrench bulldog‚Äô, ‚Äòrock beauty‚Äô].
Input: Tokens( ), output:French bulldog. Input: Tokens( ), output:A man in a red shirt and a red hat is on a motorcycle
on a hill side.
Input: Tokens( ), output: rock beauty. Input: Tokens( ), output:A woman wearing a hair net cutting a large sheet
cake.
Input: Tokens( ), output: Input: Tokens( ), output:
(1) N-Way K-shot Classification (2) Image Caption
LLM Prediction
Answer the question with a single word based
on the condition.
Condition: Tokens( ),
Question: What is this person doing?
Inpainting Outpainting Deblur Shift Rotation Masking
Answer: skiing.
Condition: Tokens( ),
Question: What does the truck on the
Output:
left sell?
Answer:
(3) Visual Question Answering (4) Image Denoising
Figure3.OurV2LtokenizerenablesafrozenLLMtoperformaseriesofimageunderstandinganddenoisingtasks.
asImageNet[8])andtheexpandedLLMvocabulary,which coderandafrozenCLIP-vision-encoder.TheCNNencoder
includes all original subwords, bigrams, and trigrams, we isidenticaltotheoneusedbyVQ-GAN[12],butwithmod-
computetheCLIPsimilarities[36]betweeneachimagein ificationstothedownsamplingrate.Wedownsamplethein-
thedatasetandeverylexicalitemintheexpandedLLMvo- putimagebyafactorof8.TheCNNencoderaimstoextract
cabulary. We then record the top-5 lexical items with the local information, while the CLIP-vision-encoder focuses
highestsimilarityscoresforeachimage. Finally,weaggre- on encoding global information. Refer to the supplemen-
gate these top-5 lexical items from all images to form the tarymaterialsforthedetailsoftheencoder.
finalexpandedLLMvocabulary,whichservesasourglobal Quantizers. We use F ‚àà Rh√ów√ódl to denote the feature
codebook. mapencodedbytheCNNencoder,where(h,w)isthespa-
Local Codebook. The objective of the local codebook is tial size. Similarly, f ‚àà Rdg denotes the global feature
to use an item from this codebook to represent a part of encodedbytheCLIP-vision-encoder, withd representing
g
animage(e.g., animagepatch). WeusetheoriginalLLM thedimensionoff. LetE denotethesetofP-LLMembed-
l
vocabularyasourlocalcodebook. dingsoftheLLMvocabularyT,andE representthesetof
g
Embeddings of Global and Local Codebooks. As illus- E-LLMembeddingsoftheexpandedLLMvocabularyT E,
tratedinFigure2,weprojecttheglobalcodebook(i.e.,the respectively.
expanded LLM vocabulary) and the local codebook (i.e., As shown in Figure 2, the local quantizer operates by
the LLM vocabulary) into embeddings through the CLIP- identifying the closest embedding in E for each element
l
text-encoder [36]. The embeddings for the global and lo- F
(i,j)
‚àà Rdl within F, where (i,j) specifies the spatial
cal codebooks are termed the LLM embeddings and the location (1 ‚â§ i ‚â§ h and 1 ‚â§ j ‚â§ w). The identifica-
E-LLM embeddings, respectively. Additionally, we uti- tion is based on Euclidean distance. This process yields a
lizeatrainableprojector,whichisimplementedasalinear tokenized map F(cid:98) with the same size of F. Each element
layer,tofurtherprojecttheLLMembeddingsforalignment F(cid:98)(i,j) ‚ààE linF(cid:98)representsaP-LLMembeddingassociated
withthevisualspace. Thequantizers,whichwillbeintro- with a language token belonging to T. In total, there are
duced later, further utilize the projected LLM embeddings K =hwlocaltokens.
l
(P-LLMembedding)andE-LLMembeddingstoencodelo- Similarly, the global quantizer functions by identifying
calandglobalinformationforaninputimage. the K closest embeddings in E for the global feature f,
g g
Encoder. OurencoderiscomposedofatrainableCNNen- based on their Euclidean distance. After quantization, fOriginal Image Original Image
10√ó
: Masked Token 1 2 3 4 1 2 3 4 1 2 3 4
Encoder 5 6 7 5 6 7 5 6 7
: Token from LLM
Codebook Random Replacement Encoder Random
Replacement
: Local Token of
1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4
Original Image 10√ó 5 6 7 5 6 7 5 6 7 5 6 7
: Local Token of
Blurred Image Blurred Image
Input: 1 2 3 4 5 Input: 1 2 3 4 5
+ 10√ó In-Context Learning Samples Input: 1 2 3 4 5 6 7 Output: 6 7
Output: 6 7 Output: 6 ? 7 ?
+
10√ó In-Context Learning Samples Input: 1 2 3 4 5 6 7 Output: 6 ? 7 ?
(a) Inpainting and Outpainting. (b) Image Restoration.
Figure4. (a)Weuseinpaintingasanexample. Givenanimage,wefirstextractitslocaltokensT. FollowingSPAE[54],wegenerate
l
10copiesforT,termly{Ts}10 . EachcopyisavariationofT withtokensrandomlyreplacedbythosefromtheLLMcodebook. The
l l s=1 l
replacementratiosaresetas[23%,50%; 3%],where3%denotestheincrementalstep. Next,an8√ó8mask(inpainting)oran8√ó16
mask(outpainting)isappliedtothecenter(inpainting)orthebottom(outpainting)ofT.Theobjectiveistopredictmmaskedtokensata
l
timeusingthefirstntokensprecedingthem. Thepromptisstructuredasfollows:[Learnanewlanguageandpredictmtokensfollowing
the examples. {Input: Ts[n], output: Ts[m].}10 . Input: T[n], output:]. This prompt is then fed into the LLM, which sequentially
l l s=1 l
predictsmtokens. Repeatingthisprocessenablesustopredictallmaskedtokens. Finally,weorganizethesepredictionsalongwiththe
unmaskedtokensandfeedthecompletetokenmapintothedecoderforimagerestoration. (b)Weusedeblurringasanexample. Both
shiftandrotationrestorationssharesimilarprinciples. Thepromptisstructuredasfollows: [Learnanewlanguageandpredictmtokens
followingtheexamples.{Input:Ts [n+m],output:Ts[m].}10 .Input:T[n+m],output:].Inthisprompt,T denotesthelocaltokens
l l s=1 l l
oftheblurredimage,Ts indicatesavariationofT withtokensrandomlyreplacedbythosefromtheLLMcodebook,andTsrepresents
l l l
s
thetokensoftheoriginalimage,whichundergothesametokenreplacementasT .Bydefault,wesetn=16andm=2.
l
is represented by the K
g
E-LLM embeddings f(cid:98)= {e
k
‚àà tization loss, perceptual loss and GAN loss as introduced
E g}K k=g
1
associatedwiththecorrespondinglanguagetokens by VQ-GAN, respectively; Œª 1 and Œª 2 denote the weights
{t ‚àà T }Kg . Itshouldbenotedthatduringthetraining for the respective losses. We set Œª 1 = 1.0 and Œª 2 = 0.1.
k E k=1 RefertotheoriginalVQ-GAN[12]formoredetailsoneach
of quantizers, both the LLM embeddings and the E-LLM
typeofloss.
embeddingsremainfrozen,asillustratedinFigure2.
Decoder. The objective of the decoder is to reconstruct
3.3.VisualSignalComprehension
the original image by using the local embeddings F(cid:98) and
the global embeddings f(cid:98)as inputs. Our decoder is built We term the language tokens associated with f(cid:98)and F(cid:98) as
upon the one adopted by VQ-GAN [12], which utilizes a global tokens (denoted as T g = {t k ‚àà T E}K k=g 1) and local
self-attention layer and a stack of transposed convolution tokens(denotedasT = {t ‚àà T}Kl ), respectively, with
l k k=1
layers to upsample F(cid:98) along the spatial dimension. The thelatterbeingafterflattening. NotethatK
l
= hw,where
key distinction lies in the incorporation of f(cid:98): we inject (h,w) denote the spatial size of the feature map produced
the information of f(cid:98)into the decoding process through a bytheCNNencoder. Givenanimage, wefirstfeeditinto
cross-attention layer. In our implementation, this cross- ourV2LTokenizertogenerateitsglobaltokensT g andlo-
attention layer is positioned following VQ-GAN‚Äôs self- caltokensT l. Subsequently,wecandesignvariousprompts
by combining task-specific introductions, in-context learn-
attention layer, where F(cid:98) serves as queries and f(cid:98)acts as
ing samples, as well as either global or local tokens, and
keys. Thismodificationdoesnotaffectthestructureofthe
feed the prompts into a frozen LLM to perform a series of
original decoder adopted by VQ-GAN. Consequently, the
understanding and generation tasks, as shown in Figure 3.
finaloutputofthedecoderisatensorthatmatchesthesize
Wepresentthepromptsforeachtaskasfollows.
oftheinputimage.
LossFunction.AsillustratedinFigure2,weoptimizeonly N-WayK-ShotImageClassification. Weusea2-wayK-
the encoder, the decoder, and the projector while freezing shot classification as an example with the target of classi-
the LLM/E-LLM embeddings, the LLM/E-LLM vocabu- fyingimagesaseither‚ÄúFrenchbulldog‚Äùor‚ÄúRockbeauty‚Äù.
lary and the CLIP model. Following VQ-GAN, we define Thepromptisstructuredasfollows:[Foreachofthefollow-
theobjectivefunctionas: inginput-outputpairs,outputisoneof[‚ÄúFrenchbulldog‚Äù,
‚ÄúRockbeauty‚Äù]. {Samples}. Input: TTest,output:],where
L=L +Œª L +Œª L , g
VQ 1 Perceptual 2 GAN TTestdenotesthegloballanguagetokensofthetestimage,
g
whereL ,L andL representvectorquan- and ‚Äú{Samples}‚Äù signifies N-way K-shot samples. Each
VQ Perceptual GANTaskInduction: ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
Method #Tokens N-wayK-shot: 2-1 2-1 2-3 2-5 2-1 2-1 2-1 Avg 5-1 5-1 5-3 5-5 5-1 5-1 5-1 Avg
#Repetitions: 0 0 0 0 1 3 5 0 0 0 0 1 3 5
Frozen[47] - - 1.7 33.7 66.0 66.0 63.0 65.0 63.7 51.3 0.9 14.5 34.7 33.8 33.8 33.3 32.8 26.3
LQAE[25] 256 GPT-3.5 1.5 35.2 68.2 69.8 68.5 68.7 65.9 54.0 1.0 15.7 35.9 36.5 31.9 36.4 45.9 29.0
SPAE[54] 5 GPT-3.5 5.3 77.2 84.4 86.0 79.4 77.2 77.1 69.5 - - - - - - - -
SPAE[54] 5 PaLM-2(340B) 32.2 84.0 88.5 88.4 85.1 83.6 82.4 77.7 23.6 64.2 68.0 69.9 63.4 62.0 60.2 58.8
Ours 5 LLaMA-2(7B) 34.2 73.1 89.0 93.4 79.6 80.6 79.1 75.6 36.2 54.6 88.6 91.1 70.7 72.8 74.4 69.8
Ours 5 LLaMA-2(13B) 44.4 77.9 91.9 94.4 81.5 82.8 82.0 79.3 45.4 69.6 89.9 91.3 75.8 75.7 77.2 75.0
Ours 5 LLaMA-2(70B) 41.7 87.1 94.8 96.1 88.9 89.2 89.1 83.9 45.4 81.5 92.3 93.0 85.7 86.1 86.3 81.5
SPAE[54] 21 PaLM-2(340B) 27.9 84.8 92.5 92.6 84.8 85.2 85.4 79.0 20.2 65.1 73.7 74.3 66.4 67.0 66.3 61.9
Ours 21 LLaMA-2(7B) 36.5 76.3 91.2 95.3 84.0 84.4 83.7 78.8 37.1 44.8 91.8 94.0 73.9 82.2 85.3 72.7
Ours 21 LLaMA-2(13B) 48.7 73.1 92.4 95.7 80.9 83.8 82.0 79.5 42.1 62.7 93.0 94.5 72.8 79.6 82.0 75.2
Ours 21 LLaMA-2(70B) 46.5 89.1 96.9 97.8 91.4 92.7 92.9 86.7 45.0 79.7 94.9 95.6 89.3 90.7 90.2 83.5
Table1.Few-shotClassificationon2-wayand5-wayMini-ImageNetbenchmarks.
samplefollowstheformat‚ÄúInput: T ,output: L.‚Äù,withT
g g
andLdenotingthecorrespondingglobaltokensandthela-
bel(either‚ÄúFrenchbulldog‚Äùor‚Äúrockbeauty‚Äù)ofeachsam-
ple,respectively.
Image Caption. We structure the prompt as follows:
[Generate a caption sentence based on words describing
an image. {Samples}. Input: TTest, output:], where
g A dog is sitting in front of a computer. A picture of a sign that says stop.
‚Äú{Samples}‚Äù denotes in-context learning samples. Each
A group of people in a kitchen. A bathroom with a bathtub and shower.
sampleisformattedas‚ÄúInput: T ,output: C‚Äù,withT and
g g
Cdenotingthecorrespondingglobaltokensandthecaption Q1: What food item is shown?
of each sample, respectively. The LLM takes this prompt Pizza Burger
Q2: What country did this food
asinputandauto-regressivelycaptionsthetestimagewith
originate from?
global tokens TTest, continuing until it encounters the to-
g Italy Japan
ken‚Äú.‚Äù. Q3: What is the leafy substance?
Visual Question Answering. The prompt for VQA is de- Basil Lettuce
signedasfollows: [Answerthequestionwithasingleword Figure5. Visualizationsforimagecaption(firstrow)andvisual
based on the condition. {Samples}. Condition: TTest. questionanswering(secondrow).Blue:ours.Orange:SPAE[54]
g
Question: Q. Answer:], where TTest denotes the global (re-implementation).
g
tokens of the test image, Q is the intended question, and
model used is the one with a ViT-L/14 backbone. Images
‚Äú{Samples}‚Äù indicates in-context learning samples. Each
areresizedtoaresolutionof128√ó128pixelsandarethen
samplehasaformatof‚ÄúCondition: T . Question: Q. An-
g processedbyourV2LTokenizer,whichencodestheminto
swer: A‚Äù, with the triplet (T ,Q,A) denoting the global
g a 16 √ó 16 token map. The training is conducted on the
tokens of one sample, the question related to this sample,
ImageNet-1K dataset over 100 epochs using 32 NVIDIA
andtheground-truthanswer.
V100 GPUs. We use the Adam optimizer, starting with a
ImageDenoising.Wedesignseveralimagedenoisingtasks
learningrateof5e‚àí4,whichundergoesahalf-cyclecosine
followingSPAE[54],includinginpainting,outpainting,de-
decayfollowinga5-epochlinearwarm-upphase.
blurring, shift restoration and rotation restoration. The
4.2.ImageComprehension
promptsforthosetasksareillustratedinFigure4.
Few-ShotClassification.FollowingSPAE,weconductim-
4.Experiments agecomprehensionexperimentson2-wayand5-wayMini-
ImageNet benchmarks. All few-shot samples and test im-
4.1.Settings
ages are tokenized by our V2L Tokenizer into K global
g
WeadoptLLaMA2[46]asourLLM,whichhasthreever- tokens. Then we structure the prompt as detailed in Sec-
sionswithparametersof7B,13B,and70B.Itsvocabulary tion3.3andillustratedinFigure3. Thispromptisthenin-
sizeis32,000. Ourlocalcodebookretainstheoriginalvo- putintotheLLMforthepurposeofpredictingthecategory
cabularyfromLLaMa2. Thesizeoftheglobalcodebookis ofthetestimage. Itisimportanttonotethatthepredictions
11,908aftervocabularyexpansionandfiltering. TheCLIP arepresentedintextform. Apredictionisconsideredcor-flavoredcoffee convinencestore ExtremeSports Method Codebook #Tokens FID‚Üì LPIPS‚Üì PSNR‚Üë
b
P a
sosserpsen sosserpse srehctuB stekraMamir nileehwecnal poHynnuB V
V
V
SPQ
Q
Q
A-
-
-
EG
G
GA
A
A
[5N
N
N
4]‚àó[ [1
1
[12
2
2]
]
]
LL
P
PLe
a
aa
aL
Lr Mn
M
Ma Ab
- -2
2-l 2e 22
2
355
5
466
6
1
95
7
9..
.
.54
4
418
4
9
0
000
.
...
1
111
7
773 21-
-
-.48
lungo refreshmentstand specializedtraining
SPAE[54] PaLM-2 597 4.41 0.12 -
RemoteDesk glazedpot Cetonia SPAE[54] PaLM-2 1109 3.89 0.11 -
c
sksedretupmo noitatsinim GUMSSER tsimarec sIdlaremE notsiahp
T bra ib
dO
O
l
:eu
u
lr
r
o3s
s
c. aR lte oc ko en ns str (u 2c
5L
t
6iHL
o
)ya
n
aM
b ner
dviA
d
a
g2
l lu oa bti
ao2
2
ln5
7
to6
7
o kn enI
sm23 (..
a
284
g
181
e )N
are0
0
et-.
.
d0
0
1
e8
8
K rivv ea
d22 l33
fs
r.. e25 ot56
. mH thy e-
computerdesks mug chaferlar
localcodebook(LLaMA-2)andtheglobalcodebook(E-LLaMa-
Figure6.Visualizationforsemanticinterpretation. 2),respectively.*:re-implementation.
Method Codebook #Tokens CLIP‚Üë CLIP-R‚Üë
six images chosen at random. Our vocabulary expansion
SPAE[54] PaLM-2 5 0.1868 0.7147 technique effectively increases the range of semantically
Ours E-LLaMA-2 5 0.2576 0.9165
pertinent token options (i.e. bigrams and trigrams). Extra
SPAE[54] PaLM-2 21 0.1815 0.6901 resultsareavailableinthesupplementarymaterials.
Ours E-LLaMA-2 21 0.2427 0.8520 In Table 2, we also quantitatively evaluate the seman-
Table2. SemanticqualityevaluatoinonImageNet-1Kvalset. E- tic quality of our global tokens, and compare the semantic
LLaMA-2:expandedLLaMa-2vocabulary. qualitywithSPAE[54]onImageNet-1Kvalidationset,us-
ing the CLIP score and the relative CLIP score (CLIP-R),
rectonlyifallthegeneratedtokensmatchthetokensofthe which assess the degree of alignment between each image
actualcategoryname. and its associated language tokens. We observe consistent
Table1showsthecomparisonbetweenourapproachem- improvements over SAPE, despite SAPE utilizing a larger
ployingdifferentLLaMa2modelconfigurations,andprior vocabulary(SPAE‚Äôs65,000versusour11,908).
worksincludingLQAE[25],SPAE[54],andabaselineus-
4.3.ImageReconstructionandDenoising
ingafrozenlanguagemodelformultimodalfew-shotlearn-
ing [47]. We examine various factors that could influence Reconstruction Evaluation. Our V2L Tokenizer encodes
N-way K-shot classification, including: (1) the value of an image into a set of local tokens derived from an LLM
N; (2) the value of K; (3) task induction, defined as spec- vocabulary. Theseencodedtokensshouldcapturethemost
ifying the particular N-way categories in the prompt; (4) meaningful information, enabling the decoder to recon-
the frequency of repetitions for each few-shot sample. We struct the original image and restore any degraded (‚Äúpol-
have two main observations: (1) Our model surpasses the lutional‚Äù)images. Inthisstudy,weevaluatethereconstruc-
previously best approach, SPAE [54], across all scenarios, tion quality of our V2L Tokenizer using metrics including
despite using smaller LLMs (our 13B/70B LLaMa 2 ver- FID, LPIPS, and PSNR. As shown in Table 3, we com-
susSPAE‚Äôs340BPaLM-2)andamorecompactvocabulary pare our approach with SPAE [54] and VQ-GAN [12] on
(our11,908versusSPAE‚Äôs65,000);(2)Theperformanceof the ImageNet-1K validation set. In our approach, we ex-
our model improves as the number of tokens used to rep- plore two distinct setups: (1) employing the decoder from
resent the image increases. This can be attributed to the VQ-GANwithouttheinvolvementofglobaltokens;(2)uti-
introduction of the vocabulary expansion, which generates lizing the proposed decoder, which incorporates extra K
g
alargerpoolofsemanticallyrelevanttokencandidates. global tokens for the decoding process (default configura-
Image Caption and Visual Question Answering. Fol- tionasdiscussedinSection3.2).Ourapproachoutperforms
lowing SPAE [54], we randomly select 10 image-caption SPAE[54]acrossallmetrics.
pairs(orimage-question-answertriplets)fromCOCOCap- Image Denoising. We introduce the prompts used for in-
tion [4] (or VQA [40]) training set to form the in-context painting,outpainting,deblurring,shiftandrotationrestora-
learningsamplesintheimagecaption(orVQA)prompt,as tions, along with the process of restoring polluted images,
described in Section 3.3. By default, we utilize 21 global asshowninFigure4. InTable4,westudytwofactorsim-
tokenstorepresentanimage. Thevisualizationresultsare pacting the quality of these five in-context image denois-
presentedinFigure5. Refertosupplementarymaterialsfor ingtasks: (1)theimagetokenizer,whichencodesanimage
moreresults. intoasetoftokens;(2)theLLM,whichaimstopredictthe
SemanticInterpretation. Figure6visualizesthetopfour local tokens of the original images given the tokens of the
globaltokenswiththehighestsimilarityscoresforasetof pollutedimages,withtheaidofin-contextlearningsamplesInpainting Outpainting Deblurring Rotation Shift
Tokenizer LLM FID‚Üì LPIPS‚Üì FID‚Üì LPIPS‚Üì FID‚Üì LPIPS‚Üì FID‚Üì LPIPS‚Üì FID‚Üì LPIPS‚Üì
VQ-GAN‚àó[12] LLaMA-27B 16.44 0.1404 18.22 0.1571 13.79 0.1252 14.08 0.1285 13.91 0.1270
LQAE‚àó[25] LLaMA-27B 18.77 0.1736 19.61 0.1833 18.09 0.1711 18.18 0.1725 18.26 0.1722
SPAE‚àó[54] LLaMA-27B 14.89 0.1211 16.10 0.1363 15.89 0.1299 16.25 0.1318 16.55 0.1333
Ours LLaMA-27B 13.13 0.1219 15.28 0.1442 10.09 0.1033 10.64 0.1064 10.53 0.1058
VQ-GAN‚àó[12] LLaMA-213B 15.56 0.1350 16.47 0.1449 14.78 0.1334 16.15 0.1417 15.60 0.1378
LQAE‚àó[25] LLaMA-213B 18.45 0.1720 18.78 0.1762 18.62 0.1740 19.04 0.1778 18.87 0.1770
SPAE‚àó[54] LLaMA-213B 13.89 0.1168 14.69 0.1257 16.46 0.1345 18.34 0.1436 17.71 0.1405
Ours LLaMA-213B 11.70 0.1134 12.56 0.1275 10.60 0.1085 11.36 0.1128 11.84 0.1176
VQ-GAN‚àó[12] LLaMA-270B 14.08 0.1256 14.70 0.1358 14.30 0.1312 14.39 0.1313 14.35 0.1310
LQAE‚àó[25] LLaMA-270B 18.01 0.1692 18.54 0.1755 18.17 0.1713 18.16 0.1715 18.09 0.1713
SPAE‚àó[54] LLaMA-270B 12.79 0.1103 13.41 0.1191 18.08 0.1615 18.30 0.1619 18.19 0.1609
Ours LLaMA-270B 10.11 0.1021 10.73 0.1128 10.42 0.1058 10.48 0.1058 10.79 0.1093
Table4.Quantitativeevaluationacrossfivedenoisingrestorationtasks.*:re-implementation.
Input VQ-GAN LQAE SPAE Ours Input VQ-GAN LQAE SPAE Ours Input VQ-GAN LQAE SPAE Ours
Figure7. Fromleft-to-right,top-to-bottom: visualizationsforimagereconstruction,inpainting,outpainting,deblurring,shiftrestoration
androtationrestoration. Were-implementVQ-GAN[12], LQAE[25]andSPAE[54]usingavocabularysizeof32,000and256local
tokensforafaircomparison.
LLaMa-2model(detailsontuningareprovidedinthesup-
plementary materials). The next step involves integrating
the predictions for the masked tokens with the unmasked
tokens,whicharetheninputintothedecoderforimagere-
construction. The qualitative results of this visual signal
restorationprocessareillustratedinFigure8.Forvisualiza-
tion purposes, the masked images (‚Äúinput‚Äù) presented are
generated by combining the unmasked local tokens of the
originalimagewiththemaskedtokenswhichhavebeenset
tozero,beforebeingprocessedthroughthedecoder.
Input Prediction Input Prediction Input Prediction
5.Conclusion
Figure8.Visualizationsformaskedimagerestoration.
Inthispaper,weviewimagesasa‚Äúforeignlanguage‚Äù,and
encodedbythetokenizer. Thetokenizersusedforcompari- introduce a V2L Tokenizer, which maps continuous visual
sonincludeVQ-GAN[12],LQAE[25],andSPAE[54].We signalstothetokenspaceofanLLM.Ourmethodenables
randomlyselect5,000imagesfromtheImageNet-1Kvali- a frozen LLM tounderstand visual signals without the ne-
dationsettoformourevaluationset.WeuseFIDandLPIPS cessity for resource-intensive fine-tuning on multi-modal
scores as metrics. Our V2L Tokenizer outperforms others datasets. The V2T Tokenizer processes an image by gen-
across the five tasks on almost all metrics. This achieve- eratingbothglobalandlocaltokens. Theglobaltokensare
ment is attributed to the alignment of image features with crafted to capture essential semantic information with the
the token space of the frozen LLM. We also show several aid of the proposed vocabulary expansion technique. This
qualitative results in Figure 7. More visualizations can be enables the execution of tasks like image recognition, im-
foundinthesupplementarymaterials. age captioning and VQA. In contrast, local tokens are de-
Masked Image Restoration. Given an image from the signedtoextractdetailed,patch-levelfeaturesfromimages,
ImageNet-1K validation set, we first extract its global and facilitatingimagedenoisingtaskssuchasinpaintingandde-
localtokensthroughourV2LTokenizer. Subsequently,we blurring.Extensivequantitativeandqualitativeexperiments
applyrandommaskingto30%oftheselocaltokens.Topre- validate the superiority of our approach over the prior at-
dictthemaskedtokens,weemployaLoRA-tuned[16]7B temptsinthisdirection.References [13] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
angyuYue,etal. Llama-adapterv2: Parameter-efficientvi-
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
sual instruction model. arXiv preprint arXiv:2304.15010,
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
2023. 1,2
visual language model for few-shot learning. Advances in
[14] Amelia Glaese, Nat McAleese, Maja Trebacz, John
Neural Information Processing Systems, 35:23716‚Äì23736,
Aslanides,VladFiroiu,TimoEwalds,MaribethRauh,Laura
2022. 1,2
Weidinger, Martin Chadwick, Phoebe Thacker, et al. Im-
[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
proving alignment of dialogue agents via targeted human
son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,
judgements. arXivpreprintarXiv:2209.14375,2022. 2
EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2
[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
technicalreport. arXivpreprintarXiv:2305.10403,2023. 1,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
2
deLasCasas,LisaAnneHendricks,JohannesWelbl,Aidan
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Clark,etal. Trainingcompute-optimallargelanguagemod-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan- els. arXivpreprintarXiv:2203.15556,2022. 2
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
guagemodelsarefew-shotlearners. Advancesinneuralin-
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
formationprocessingsystems,33:1877‚Äì1901,2020. 1,2
Lora: Low-rankadaptationoflargelanguagemodels. arXiv
[4] XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedan- preprintarXiv:2106.09685,2021. 8,12
tam,SaurabhGupta,PiotrDolla¬¥r,andCLawrenceZitnick.
[17] MengqiHuang,ZhendongMao,ZhuoweiChen,andYong-
Microsoft coco captions: Data collection and evaluation
dongZhang. Towardsaccurateimagecoding:Improvedau-
server. arXivpreprintarXiv:1504.00325,2015. 7
toregressiveimagegenerationwithdynamicvectorquantiza-
[5] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhanghao tion. InProceedingsoftheIEEE/CVFConferenceonCom-
Wu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao puterVisionandPatternRecognition, pages22596‚Äì22605,
Zhuang,JosephEGonzalez,etal. Vicuna: Anopen-source 2023. 2
chatbot impressing gpt-4 with 90% chatgpt quality. See [18] FangkaiJiao,BoshengDing,TianzeLuo,andZhanfengMo.
https://vicuna. lmsys. org (accessed 14 April 2023), 2023. Panda llm: Training data and evaluation for open-sourced
2 chineseinstruction-followinglargelanguagemodels. arXiv
[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, preprintarXiv:2305.03025,2023. 2
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul [19] Taku Kudo and John Richardson. Sentencepiece: A
Barham, Hyung Won Chung, Charles Sutton, Sebastian simple and language independent subword tokenizer and
Gehrmann, et al. Palm: Scaling language modeling with detokenizer for neural text processing. arXiv preprint
pathways. arXivpreprintarXiv:2204.02311,2022. 1,2 arXiv:1808.06226,2018. 2
[7] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, [20] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and
ShaneLegg,andDarioAmodei. Deepreinforcementlearn- Wook-Shin Han. Autoregressive image generation using
ingfromhumanpreferences.Advancesinneuralinformation residualquantization.InProceedingsoftheIEEE/CVFCon-
processingsystems,30,2017. 2 ferenceonComputerVisionandPatternRecognition,pages
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 11523‚Äì11532,2022. 2
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage [21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
database. In2009IEEEconferenceoncomputervisionand Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
patternrecognition,pages248‚Äì255.Ieee,2009. 4 model with in-context instruction tuning. arXiv preprint
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina arXiv:2305.03726,2023. 2
Toutanova. Bert: Pre-training of deep bidirectional [22] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,
transformers for language understanding. arXiv preprint HaotianLiu,JianweiYang,TristanNaumann,HoifungPoon,
arXiv:1810.04805,2018. 2 and Jianfeng Gao. Llava-med: Training a large language-
[10] HanzeDong,WeiXiong,DeepanshuGoyal,RuiPan,Shizhe and-vision assistant for biomedicine in one day. arXiv
Diao,JipengZhang,KashunShum,andTongZhang. Raft: preprintarXiv:2306.00890,2023.
Reward ranked finetuning for generative foundation model [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
alignment. arXivpreprintarXiv:2304.06767,2023. 2 Blip-2: Bootstrapping language-image pre-training with
[11] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,Jiezhong frozen image encoders and large language models. arXiv
Qiu, Zhilin Yang, and Jie Tang. Glm: General language preprintarXiv:2301.12597,2023. 1,2
modelpretrainingwithautoregressiveblankinfilling. arXiv [24] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
preprintarXiv:2103.10360,2021. 2 Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
[12] PatrickEsser,RobinRombach,andBjornOmmer. Taming 2023. 1,2
transformers for high-resolution image synthesis. In Pro- [25] Hao Liu, Wilson Yan, and Pieter Abbeel. Language quan-
ceedings of the IEEE/CVF conference on computer vision tizedautoencoders:Towardsunsupervisedtext-imagealign-
andpatternrecognition,pages12873‚Äì12883,2021. 1,2,4, ment. arXivpreprintarXiv:2302.00902,2023. 1,2,6,7,8,
5,7,8,12,13 12,13[26] GenLuo,YiyiZhou,TianheRen,ShengxinChen,Xiaoshuai [40] Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring
Sun, and Rongrong Ji. Cheap and quick: Efficient vision- modelsanddataforimagequestionanswering. Advancesin
languageinstructiontuningforlargelanguagemodels.arXiv neuralinformationprocessingsystems,28,2015. 7
preprintarXiv:2305.15023,2023. 2 [41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
[27] SewonMin,MikeLewis,LukeZettlemoyer,andHannaneh Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Hajishirzi. Metaicl: Learning to learn in context. arXiv RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
preprintarXiv:2110.15943,2021. 2 etal.Photorealistictext-to-imagediffusionmodelswithdeep
language understanding. Advances in Neural Information
[28] SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,Mike
ProcessingSystems,35:36479‚Äì36494,2022. 1
Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Re-
[42] RicoSennrich,BarryHaddow,andAlexandraBirch. Neural
thinkingtheroleofdemonstrations: Whatmakesin-context
machinetranslationofrarewordswithsubwordunits. arXiv
learningwork? arXivpreprintarXiv:2202.12837,2022. 2
preprintarXiv:1508.07909,2015. 2
[29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
[43] Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald,
LongOuyang,ChristinaKim,ChristopherHesse,Shantanu
Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris,
Jain, Vineet Kosaraju, William Saunders, et al. Webgpt:
Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, et al.
Browser-assistedquestion-answeringwithhumanfeedback.
Alexatm 20b: Few-shot learning using a large-scale multi-
arXivpreprintarXiv:2112.09332,2021. 2
lingual seq2seq model. arXiv preprint arXiv:2208.01448,
[30] OpenAI. Gpt-4technicalreport,2023. 1
2022. 2
[31] LongOuyang, JeffreyWu, XuJiang, DiogoAlmeida, Car- [44] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Xuechen Li, Carlos Guestrin, Percy Liang, and Tat-
Agarwal, Katarina Slama, Alex Ray, et al. Training lan- sunori B Hashimoto. Stanford alpaca: an instruction-
guage models to follow instructions with human feedback. followingllamamodel(2023). URLhttps://crfm.stanford.
Advances in Neural Information Processing Systems, 35: edu/2023/03/13/alpaca.html,1(2):3. 2
27730‚Äì27744,2022. 2 [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
[32] JialunPeng,DongLiu,SongcenXu,andHouqiangLi.Gen- Martinet,Marie-AnneLachaux,Timothe¬¥eLacroix,Baptiste
eratingdiversestructureforimageinpaintingwithhierarchi- Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
cal vq-vae. In Proceedings of the IEEE/CVF Conference Llama: Open and efficient foundation language models.
onComputerVisionandPatternRecognition,pages10775‚Äì arXivpreprintarXiv:2302.13971,2023. 1,2
10784,2021. 2 [46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
[33] ZhiliangPeng,LiDong,HangboBao,QixiangYe,andFuru Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Wei.Beitv2:Maskedimagemodelingwithvector-quantized Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
visualtokenizers.arXivpreprintarXiv:2208.06366,2022.2 Llama2:Openfoundationandfine-tunedchatmodels.arXiv
preprintarXiv:2307.09288,2023. 1,2,6
[34] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever,etal. Improvinglanguageunderstandingbygen- [47] MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEs-
erativepre-training. 2018. 1,2 lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learningwithfrozenlanguagemodels. AdvancesinNeural
[35] AlecRadford,JeffreyWu,RewonChild,DavidLuan,Dario
InformationProcessingSystems,34:200‚Äì212,2021. 2,6,7
Amodei, Ilya Sutskever, et al. Language models are unsu-
[48] AaronVanDenOord, OriolVinyals, etal. Neuraldiscrete
pervisedmultitasklearners. OpenAIblog, 1(8):9, 2019. 1,
representationlearning.Advancesinneuralinformationpro-
2
cessingsystems,30,2017. 2,12
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[49] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
XizhouZhu,GangZeng,PingLuo,TongLu,JieZhou,Yu
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
Qiao, et al. Visionllm: Large language model is also an
transferable visual models from natural language supervi-
open-endeddecoderforvision-centrictasks. arXivpreprint
sion.InInternationalconferenceonmachinelearning,pages
arXiv:2305.11175,2023. 2
8748‚Äì8763.PMLR,2021. 3,4
[50] ChenfeiWu,ShengmingYin,WeizhenQi,XiaodongWang,
[37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-
Zecheng Tang, and Nan Duan. Visual chatgpt: Talking,
mon, Christopher D Manning, and Chelsea Finn. Direct
drawing and editing with visual foundation models. arXiv
preferenceoptimization: Yourlanguagemodelissecretlya
preprintarXiv:2303.04671,2023. 1
rewardmodel. arXivpreprintarXiv:2305.18290,2023. 2
[51] HonglinXiong,ShengWang,YitaoZhu,ZihaoZhao,Yux-
[38] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee, iaoLiu,QianWang,andDinggangShen. Doctorglm: Fine-
SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and tuning your chinese doctor is not a herculean task. arXiv
Peter J Liu. Exploring the limits of transfer learning with preprintarXiv:2304.01097,2023. 2
a unified text-to-text transformer. The Journal of Machine [52] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
LearningResearch,21(1):5485‚Äì5551,2020. 2 Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
[39] AliRazavi,AaronVandenOord,andOriolVinyals. Gener- Yaya Shi, et al. mplug-owl: Modularization empowers
atingdiversehigh-fidelityimageswithvq-vae-2. Advances large language models with multimodality. arXiv preprint
inneuralinformationprocessingsystems,32,2019. 2 arXiv:2304.14178,2023. 2[53] JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,
JamesQin,AlexanderKu,YuanzhongXu,JasonBaldridge,
and Yonghui Wu. Vector-quantized image modeling with
improvedvqgan. arXivpreprintarXiv:2110.04627,2021. 2
[54] LijunYu, YongCheng, ZhiruoWang, VivekKumar, Wolf-
gangMacherey,YanpingHuang,DavidARoss,IrfanEssa,
Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic
pyramidautoencoderformultimodalgenerationwithfrozen
llms. arXivpreprintarXiv:2306.17842,2023. 2,3,5,6,7,
8,12,13,15
[55] Lijun Yu, Jose¬¥ Lezama, Nitesh B Gundavarapu, Luca Ver-
sari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim
Gupta,XiuyeGu,AlexanderGHauptmann,etal. Language
modelbeatsdiffusion‚Äìtokenizeriskeytovisualgeneration.
arXivpreprintarXiv:2310.05737,2023. 2
[56] JiahuiZhang,FangnengZhan,ChristianTheobalt,andShi-
jianLu. Regularizedvectorquantizationfortokenizedim-
agesynthesis. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages18467‚Äì
18476,2023. 2
[57] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
ShilinYan,PanLu,HongshengLi,PengGao,andYuQiao.
Llama-adapter: Efficient fine-tuning of language models
with zero-init attention. arXiv preprint arXiv:2303.16199,
2023. 2
[58] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,
MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,
XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtrans-
formerlanguagemodels. arXivpreprintarXiv:2205.01068,
2022. 2
[59] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and
XinWang. Controllabletext-to-imagegenerationwithgpt-
4. arXivpreprintarXiv:2305.18583,2023. 1
[60] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin,
Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Vi-
sual instruction tuning for medical visual question answer-
ing. arXivpreprintarXiv:2305.10415,2023. 2
[61] ShanshanZhong,ZhongzhanHuang,WeushaoWen,Jinghui
Qin, andLiangLin. Sur-adapter: Enhancingtext-to-image
pre-traineddiffusionmodelswithlargelanguagemodels. In
Proceedings of the 31st ACM International Conference on
Multimedia,pages567‚Äì578,2023. 1
[62] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understandingwithadvancedlargelanguagemodels. arXiv
preprintarXiv:2304.10592,2023. 1,2A.MoreImplementationDetails Tuning LLaMA-2 with the V2L Tokenizer. To enhance
theimagegenerationtask,weproposetofine-tuneanLLM
Global Codebook Generation. To generate the global
model. This process begins with the V2L Tokenizer gen-
codebook, we introduce a two-phase process: (1) expand-
erating both global and local tokens for the training im-
ing the LLM vocabulary through the proposed vocabulary
ages. Subsequently, the global tokens are employed as
expansion technique (as shown in Figure 9); (2) applying
a ‚Äútext prefix‚Äù. We then concatenate these global tokens
afilteringstrategytofurthereliminatetheentrieswithless
with the local tokens and input them into the LLM. The
semanticmeaning.
auto-regression loss is applied only to the local tokens.
WeuseT torepresenttheoriginalLLMvocabularyand
Due to resource limitations, we fine-tune a 7B LLaMA-2
denoteitssizebyN. Togeneratebigrams,foreacht ‚àà T,
model using LoRA [16] on 12 randomly selected classes
we first input the concatenation of a text prefix (e.g., ‚Äúa
fromImageNettrainingdatasetover100Kiterationsusing
photoof‚Äù)andtintotheLLM.TheLLMpredictsthenext
32√ó NVIDIA V100 GPUs. LoRA weights are integrated
word in an auto-regressive manner. We record the top-M
intothequeryandkeyprojectionmatrixes,withthehyper-
predictions(whereM is1bydefault)withthehighestcon-
parametersettingofr = 4, Œ± = 32. Foroptimization, we
fidences, denoted as {t‚àó,...,t‚àó }. The bigrams for each
1 M use Adam optimizer, starting with a learning rate of 3e‚àí4.
t ‚àà T are represented by {[t,t‚àó],...,[t,t‚àó ]}. This pro-
1 M Thisrateundergoeshalf-cyclecosinedecayaftera5-epoch
cess is repeated for all subwords in the LLM vocabulary.
linear warm-up phase. Consequently, the tuned model is
Ultimately, we collect a set of bigrams, denoted as T ,
Bi able to predict masked tokens in an auto-regressive man-
whichhasasizeofN √óM. Similarly, wecanbuildatri-
ner. The predicted token map is input into the decoder of
gramsetT byfeedingeachbigraminT intotheLLM
Tri Bi the V2L tokenizer to generate the reconstructed image, as
for next-word prediction. The resulting T has a size of
Tri demonstratedinSection4.3ofourmainpaper.
N √óM √óM. Weuse{T,T ,T }torepresenttheex-
Bi Tri
pandedLLMvocabulary.
B.MoreAblationStudies
Forthefilteringprocess,wecomputetheCLIPsimilari-
tiesbetweeneachimageinthetrainingsetandeveryentry VocabularyExpansion. Westudytheeffectivenessofthe
intheexpandedLLMvocabulary{T,T ,T }. Wethen proposed vocabulary expansion strategy on the 5-way-K-
Bi Tri
record the top-5 entries with the highest similarity scores shotMini-ImageNetclassificationbenchmark. Ourstudies
foreachimage. Finally,weaggregatetheseentriesfromall includethreescenarios: utilizingtheoriginalLLMvocabu-
imagestoformthefinalexpandedLLMvocabulary,which larywithoutexpansion(Subword),applyingbigramexpan-
servesasourglobalcodebookT . sion(Bigram),andemployingtrigramexpansion(Trigram).
E
Encoder and Decoder Structures. Figure 10 details the The results of these scenarios are detailed in Table 5. The
implementation of our V2L Tokenizer‚Äôs local encoder and bigram expansion approach surpasses the non-expansion
decoder. Specifically,thelocalencodersharesthesameba- methodbyanaverageaccuracyincreaseof+13.5and+9.3
sicstructureasVQ-GAN[12],utilizingfourresidualblocks points with 5 and 21 global tokens, respectively. Imple-
withchanneldimensions[128,256,256,512]todownsam- mentingtrigramexpansionfurtherelevatestheaverageac-
ple the input image by a factor of 8. Similarly, our de- curacyto83.9and86.7. Thefindingsdemonstratethatem-
codermirrorstheencoder‚Äôsstructure,employingfourresid- ployingvocabularyexpansionsignificantlyimprovesthese-
ualblockswithchanneldimensions[512,256,256,128]to manticrichnessofthetermsintheexpandedLLMvocabu-
upsampletheimagebacktoitsoriginalresolution. Weinte- lary,leadingtoenhancedclassificationaccuracy.
gratetheinformationfromglobaltokensintothedecoding EmbeddingsofLocalCodebook. AsshowninFigure2of
processthroughacross-attentionlayer,whichisaddedbe- themainpaper,weintroduceatrainableprojectortoproject
foretheself-attentionlayerinthenonlocalblock. the LLM embeddings into a visual space, which enhances
Vector Quantization Loss. The proposed V2L Tok- reconstructionquality. Table6presentsourinvestigationof
enizer requires optimization of the encoder, the decoder various LLM embeddings, including the default projected
andtheprojector. Thus,wefollowVQ-VAE[48]andVQ- LLMembeddings(P-LLaMA-2),theoriginalLLMembed-
GAN[12]toimplementourvectorquantizationloss,utiliz- dings (LLaMa-2), and those produced by the CLIP-text-
ingastraight-throughgradientestimatorforoptimization: encoder (CLIP). We observe that utilizing the CLIP text
encoder for extracting language embeddings significantly
L =||X‚àíXÀÜ||2+||sg(F)‚àíFÀÜ||+Œ≤||sg(FÀÜ)‚àíF||
vq boosts the quality of reconstruction. This improvement
wheresg(¬∑)denotesthestop-gradientoperation. Notethat likelystemsfromtheCLIPmodel‚Äôsinherentalignmentbe-
ourmethodinvolvesatrainableprojectortoproducecode- tweenlinguisticandvisualspaces. Byintroducingatrain-
bookembeddings.Thus,unlikeLQAE[25]andSPAE[54], able projector, this alignment is further refined, leading to
thesecondtermintheaboveequationisalsonecessary. We superiorreconstructionperformance.
setŒ≤ to0.3. DenoisingStepandConditionLength. AsshowninFig-ùë° [v, ùë° ] [ùë° , ùë° ] [v, ùë° , ùë° ] [ùë° , ùë° , ùë° ]
! ! ! !( ! !( ! !( ,"
ùë° [v, ùë° ] Top-ùëÄPredictions [ùë° , ùë° ] [v, ùë° , ùë° ] Top-ùëÄPredictions [ùë° , ùë° , ùë° ]
" +Prefix v " " ") +Prefix v " ") " ") !"
LLM ùë° # [v, ùë° #] LLM [ùë° #, ùë° (*] [v, ùë° #, ùë° (*] LLM [ùë° #, ùë° (*, ùë° ,)]
ùë° [v, ùë° ] [ùë° , ùë° ] [v, ùë° , ùë° ] [ùë° , ùë° , ùë° ]
Vocabulary $ $ $ +! $ +! $ +! "(
Subwords Bigrams Trigrams
ùëÅ ùëÅ√óùëÄ ùëÅ√óùëÄ√óùëÄ
Figure9.Illustrationofthevocabularyexpansionstrategy.Inthisfigure,wesetM =1forillustrativepurposes.Theprefixvcorresponds
tothetextphrase‚Äúaphotoof‚Äù.
Local Encoder
Residual Group
Block Normalization
Residual
Block
Convolutional Nonlocal Swish
4√ó
Block Block Activation
Downsample
Block
Input Image Residual Convolutional Local Features
Block Block
L
Global Tokens co
a
‚Ä¶
Q
l
u
a
Decoder n
zit
e
Group Cross r
Normalization Attention
Residual Convolutional
Block Block
Swish Nonlocal
4√ó
Activation Block
Downsample Residual
Block Block
Reconstruction Convolutional Residual Local Tokens
Block Block
Figure10.IllustrationofthelocalencoderandthedecoderofourV2LTokenizer.
ure4ofthemainpaper, wedenoisemmaskedtokensata approach with SAPE [54] using additional samples. Our
timeusingntokensprecedingthemfortheinpaintingtask, model consistently generates more reasonable image cap-
wheremandndenotedenoisingstepandconditionlength, tionsandprovidesmoreaccurateanswers.
respectively. Wevarythevaluesofmandnandreportthe Image Reconstruction. In Table 3 of the main paper, we
FIDscoresforinpaintingtaskinFigure11. Asthedenois- reportthequantitativeresultsforreconstructionevaluation.
ing step increases, the performance decreases. Addition- Inthisstudy,weshowseveralqualitativevisualizations. In
ally, an excessively long condition length leads to subop- Figure 15, we compare our approach with VQ-GAN [12],
timal performance since the LLM struggles to handle the LQAE[25]andSPAE[54]. Ourapproachisnotableforits
complexcontextofanew‚Äúforeignlanguage‚Äùinthevisual abilitytoreconstructimageswithahighlevelofdetail.
modality. Image Denoising. We show visualizations for image de-
noising in Figure 7 of the main paper. Here, we provide
C.MoreQualitativeResults extra visualizations for inpainting (Figure 16), outpaint-
ing(Figure17),deblurring(Figure18),rotationrestoration
Semantic Interpretation. We provide qualitative results (Figure19)andshiftrestoration(Figure20).
for semantic interpretation in Figure 6 of the main paper.
Here,weshowadditionalvisualizationsinFigure12.
ImageCaptioningandVisualQuestionAnswering. Fig-
ure5ofthemainpapervisualizestheresultsofimagecap-
tioning and VQA. In Figures 13 and 14, we compare our
‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶TaskInduction: ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì
Method #Tokens Inner-shot: 1 1 3 5 1 1 1 Avg
Repeats: 0 0 0 0 1 3 5
Subword 31.8 65.6 82.8 85.6 68.8 69.9 69.3 67.7
Bigram 5 LLaMA-2(70B) 40.6 83.1 91.7 92.6 86.5 87.0 86.9 81.2
Trigram 41.7 87.1 94.8 96.1 88.9 89.2 89.1 83.9
Subword 34.3 74.1 90.1 91.8 79.6 80.2 80.7 75.8
Bigram 21 LLaMA-2(70B) 44.8 84.1 95.0 95.5 91.6 92.3 92.5 85.1
Trigram 46.5 89.1 96.9 97.8 91.4 92.7 92.9 86.7
Table5.Ablationstudyfortheproposedvocabularyexpansionstrategyonthe5-way-K-shotMini-ImageNetclassificationbenchmark.
Vocabulary Embedding FID‚Üì LPIPS‚Üì PSNR‚Üë
LLaMA-2 LLaMA-2 9.51 0.17 21.48
LLaMA-2 CLIP 4.58 0.11 23.58
LLaMA-2 P-LLaMA-2 3.41 0.08 23.56
Table6. AblationstudyonvariousLLMembeddings. Wereport
resultsonImageNet-1Kvalset.
(a)FIDscorev.s.denoisingstep.
(b)FIDscorev.s.conditionlength.
Figure11.Ablationstudyonthedenoisingstep(m)andthecondi-
tionlength(n)fortheimageinpaintingtask,usinga7BLLaMA-2.tringa shearling Satinwood Lobo SkunkC
littoralis coat cabinetofa weerwolfe SkunkRiver
beltsused ÂÆöÈ£ü(set meal) fouling FloridaKeet CastIron
Sirenian ActiveRecord basketballplayer aglephorus Zwilling
Figure12.Morevisualizationsforsemanticinterpretation.
A man sits on the couch with his dog on his lap A bed with curtains hanging over it in a bedroom A large pizza with a pile of cheese on top A skateboarder is doing a trick with his skateboard
A woman in a red shirt and black pants is running A large pool in front of a house A person holding a slice of bread in a bowl A vintage train car with a chandelier
A herd of elephants is walking in the grass A messy bedroom with a queen size bed and a wooden floor A group of zebras grazing in the grass A football match with a player running with the ball
A herd of cows are grazing on a field of grass A large group of people gathered around a table A large group of people gathered around a table A person holding a horse in front of a barn
Figure13.Visualizationsforimagecaption.Blue:ours.Orange:SPAE[54](re-implementation).
Q1: What color is the sign? Q1: Is this an adult party?
Ours: red SPAE: red Ours: no SPAE: yes
Q2: What does the red sign say? Q2: Who is in front of the cake with candles?
Ours: stop SPAE: No Ours: mom SPAE: boy
Q3: What would a person park here? Q3: What is being celebrated?
Ours: car SPAE: a Ours: birthday SPAE: Chinese
Q1: What sport is being played? Q1: What type of animal is shown?
Ours: baseball SPAE: tennis Ours: elephant SPAE: raccoon
Q2: What is the name of the teams? Q3: What kind of coat does the animal have?
Ours: Cubs SPAE: Barcelona Ours: fur SPAE: fur
Q3: Is the catcher wearing safety gear? Q2: How many animals are there?
Ours: yes SPAE: yes Ours: 2 SPAE: 2
Figure14.Visualizationsforvisualquestionanswering.Blue:ours.Orange:SPAE[54](re-implementation).
nectarsand
vaquita
guttersand
ÂãïÁâ©Âúí
(Zoo)
Coat
ÊñôÁêÜÈÉ®
(restaurant)
sheepskins
Á¥†È£ü
(vegetarian)
Kenilwood
Pacersin
Cabinetof
basketballgame
greywolf
umbercolored
IrishWolf
avingaBird
SkunkTrain
Pan
Skunk
panInput VQ-GAN LQAE SPAE Ours Input VQ-GAN LQAE SPAE Ours
Figure15.Visualizationsforimagereconstruction.
Contexts Input VQ-GAN LQAE SPAE Ours
Figure16.Visualizationsforimageinpainting.
Contexts Input VQ-GAN LQAE SPAE Ours
Figure17.Visualizationsforimageoutpainting.Contexts Input VQ-GAN LQAE SPAE Ours
Figure18.Visualizationsforimagedeblurring.
Contexts Input VQ-GAN LQAE SPAE Ours
Figure19.Visualizationsforrotationrestoration.
Contexts Input VQ-GAN LQAE SPAE Ours
Figure20.Visualizationsforshiftrestoration.