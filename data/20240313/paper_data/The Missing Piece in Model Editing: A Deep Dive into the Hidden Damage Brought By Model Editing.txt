The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage
Brought By Model Editing
JianchenWang,ZhouhongGu,ZhuozhiXiong,HongweiFeng,YanghuaXiao
FudanUniversity
{wangjianchen20, zhgu20, 21210240080, hwfeng, shawyh}@fudan.edu.cn
Abstract
Ripple Effect
Factual Change
In Fact
Large Language Models have revolutionized
‚úçüèªÔ∏è CEO of
numeroustaskswiththeirremarkableefficacy.
Open AI
However,theeditingofthesemodels,crucial ‚úçüèªÔ∏è Board Emmett Open AI
for rectifying outdated or erroneous informa- Shear New
Freelancer Event
tion,oftenleadstoacomplexissueknownas
therippleeffectinthehiddenspace.Thiseffect, CEO of
while difficult to detect, can significantly im-
pede the efficacy of model editing tasks and Need To Update ‚úçüèªÔ∏è Need Edition
deteriorate model performance. This paper Need To Preserve Might be Hurt
addresses this scientific challenge by propos- Sam Altman
No Factual
inganovelevaluationmethodology,Graphical
Connection
Outlier Relation based Assessment (GORA),
whichquantitativelyevaluatestheadaptations
ofthemodelandthesubsequentimpactofedit-
Uncle Sam
ing. Furthermore,weintroducetheSelective
Chicago
OutlierRe-EditingApproach(SORA),amodel Ultraman Illinois
editing method designed to mitigate this rip- ‚Ä¶‚Ä¶ Stanford
University
pleeffect. Ourcomprehensiveevaluationsre-
Ripple Effect Ripple Effect
vealthattherippleeffectinthehiddenspace
In Hidden Space In The Same Entity
isasignificantissueinallcurrentmodeledit-
ingmethods. However,ourproposedmethods, Figure1: Illustratingtherippleeffectinmodelediting
GORAandSORA,effectivelyidentifyandal- usingtheOpenAIboardcontroversyasanexample.The
leviatethisissue,respectively,contributingto model can only receive limited modification requests
theadvancementofLLMeditingtechniques. whenaneweventoccurs,suchasaCEOchangeatOpe-
nAI.Therippleeffectofmodeleditinginvolvespositive
1 Introduction
changesinotherfacts,likeSamwillbeafreelancer,due
tothisedition. Butitalsoinvolvesunforeseendisrup-
The swift advancement of Large Language Mod-
tions, potentially damaging unrelated facts under the
els (LLMs) has exhibited remarkable efficacy
sameentityanddistortingthefactsofotherrelatedenti-
across a multitude of tasks(Brown et al., 2020;
tieswithinthemodel‚Äôslatentspace.
Zhao et al., 2023; OpenAI, 2023; Touvron et al.,
2023; Gu et al., 2023). Nevertheless, the data specificdomainswithoutunderminingtheirperfor-
embedded within these expansive models may manceinothersectors. Thisisadelicatebalancing
become outdated or encompass errors(Lazaridou act,astheeditingprocessmustensureboththesuc-
etal.,2021;Dhingraetal.,2022;Jangetal.,2022). cessoftheeditsandtheavoidanceofanynegative
Asaresult,theeditofoutdatedanderroneousin- impactontheoverallfunctionalityofthemodel.
formationwithinthesemodelshasemergedasan Althoughmanymodeleditingtechniqueshave
importantresearchsubject. proveneffectiveinvarioussituations,asignificant
Inrecentyears,methodologiesforeditingLLMs issueistheirtendencytoprioritizeimprovingedit-
have progressively garnered attention (Zhu et al., ingperformancewithoutconsideringotherfactors.
2020;DeCaoetal.,2021;Mengetal.,2022,2023; While achieving successful edits is not the most
Si et al., 2023). The primary objective of these challengingaspect,studieshaveshownthatmodel
methodologiesistoenhancetheoutputofLLMsin editing deals damage to the general abilities of
4202
raM
21
]LC.sc[
1v52870.3042:viXraLLMs(Guetal.,2024). Inourstudy,amorecom- Furthermore,throughtheiterativeestablishmentof
plexissueliesincontrollingtheimpactofknowl- hiddenconnectionsbetweenentities,GORAfacili-
edgeeditingonthehiddenspaceofthemodel,also tatesaproportionalitybetweentheinfluenceofthe
known as the ripple effect (Yao et al., 2023; Co- hiddenspacerippleeffectandthedistancetothe
henetal.,2023;Lietal.,2023c;Sakarvadiaetal., edited triplets. Consequently, this allows for the
2023). predictionofthehiddenspacerippleeffect.
AsillustratedinFig.1,therippleeffectbrought Additionally,wepresentaSelectiveOutlierRe-
aboutbyknowledgeeditingcanbedividedintotwo EditingApproach(SORA)predicatedonoureval-
categories. Thefirstcategoryisthepositiveripple uationmethod,whichisengineeredtomitigatethe
effect,suchastheupdateofotherfactsrelatedto Hidden Space Related Ripple Effect. By identi-
the edited fact, which is called ‚ÄúRipple Effect in fying the entities that possess a relationship with
Facts‚Äù(Cohenetal.,2023). Thiseffectcanbeben- the edited entities, it is feasible to alleviate the
eficialasitensurestheconsistencyandcoherence ripple effect by simply incorporating the related
ofrelatedfactswithinthemodel. Thesecondcat- tripletswiththeeditedfactintotraining. However,
egory is the negative ripple effect. This includes such a method will result in excessive computa-
the potential damage to the model‚Äôs memory of tionalexpense,aseacheditwillnecessitateediting
other information about an entity after changing alargernumberofrelatedknowledge. Notallre-
themodel‚Äôsunderstandingofthatentity,knownas latedknowledgeneedstobeedited. SORAusesthe
the ‚ÄúRipple Effect in the Same Entity‚Äù(Li et al., abovestrategybyidentifyingkeytripletsandedit-
2023b; Yao et al., 2023). Additionally, changing ingtheseknowledgetoenhanceeditingefficiency
themodel‚Äôsmemoryofanentityinahiddenspace andreducecomputationaloverhead.
mayalsoaffectentitiesthatarecloseinthehidden In the experimental phase, GORA discovered
space,referredtoasthe‚ÄúRippleEffectinHidden thateventhestate-of-the-art(SOTA)modelediting
Space‚Äù(Hoelscher-Obermaieretal.,2023a;Sakar- methodstillencounterssignificantchallengeswith
vadiaetal.,2023). therippleeffectinthehiddenspace. Comparedto
BoththeRippleEffectinFactandinTheSame directlydetectingtherippleeffectontheartificial
Entitycanbeeasilydetectedfortheyhavetheac- KG, the result of GORA reduced by 16.51% for
tualfactualconnectionbetweentheeditedentities the SOTA model editing method, indicating that
andaffectedattributesorrelations. However, the therippleeffectinthehiddenspacecausesgreater
RippleEffectinHiddenSpace,whichlacksadirect disruptionthantherippleeffectinthesameentity,
factualcorrelationwiththeeditedobject,presents therebyvalidatingthefeasibilityofGORAinveri-
asignificantchallengeindetection. Thisimplicit fyingtherippleeffectinthehiddenspace. SORA
influenceonotherentitiesseverelyimpedestheef- mitigatessuchissuesbyreducingtheaveragerate
ficacyofallModelEditingtasks,culminatingina of the SOTA model editing method in the ripple
drasticdeteriorationinmodelperformanceasthe effectinthehiddenspaceby54.75%.
quantityofeditsescalates(Lietal.,2023b;Wang Insummary,thispaperhasmadethefollowing
etal.,2023). Consequently,thedevelopmentofef- contributions:
ficaciousstrategiesandtechniquestoalleviatethis
‚Ä¢ This research is pioneering in exploring the
typeofrippleeffectsisofparamountimportance.
rippleeffectinthehiddenspace,adetrimental
In this study, we first propose an evaluation
yetimplicitphenomenoninmodelediting.
methodology,referredtoasGraphicalOutlierRela-
tionbasedAssessment(GORA),whichintegrates ‚Ä¢ WehaveintroducedGORA,aspecializedtech-
quantitative evaluations to scrutinize the adapta- niqueforevaluatingtherippleeffectinthehid-
tions of the model and the subsequent impact of denspaceduringtheprocessofmodelediting.
editing. We build up the connection between re-
‚Ä¢ LeveragingthefundamentaldesignofGORA,
latedentitiesinhiddenspacebasedonthemodel
we have developed SORA, an innovative
tobeeditedonagivenknowledgegraph(KG).By
modeleditingmethodthatemploys existing
evaluatingthemodel‚Äôsperformanceingenerating
explicitknowledgebasestomitigatetheripple
text from the edited model on edited triplets and
effectinthehiddenspace.
hidden space related triplets on the manipulated
graph,therippleeffectcanbecomprehendedinthe ‚Ä¢ Wecarryoutcomprehensiveevaluationsand
hiddenspaceinducedbythemodeleditingmethod. comparativeexperiments,whichdemonstratethatGORAeffectivelyidentifiestherippleef- forrelation-specificqueriesandisannotatedwith
fectinthehiddenspaceduringmodelediting human-generated question paraphrases that can
than other works. We have also discovered measure the model‚Äôs robustness to semantically
thatthiseffectispresentinallcurrentmodel equivalentinputs. CounterFactisamorechalleng-
editingmethods,whileSORAeffectivelyal- ing evaluation dataset by introduces counterfac-
leviates this issue compared to other model tual edits. RippleEdits (Cohen et al., 2023) is a
editingtechniques. benchmarkevaluatingthe‚Äúrippleeffects‚Äùinknowl-
edge editing. To be specific, one should go be-
2 RelatedWork yondthesinglefactthatwaseditedandcheckthat
otherfactsthatarelogicallyderivedfromtheedit
2.1 KnowledgeEditing
were also changed accordingly. In addition, re-
IntheevolvingfieldofLLMs,KnowledgeModel search(Hoelscher-Obermaieretal.,2023b;Lietal.,
Editingmethodshavebeendevelopedtointegrate 2023b) shows that existing editing methods can
newknowledgewhilepreservingexistinginforma- haveunwantedsideeffectsonLLMs.
tion. Thesemethodsarebroadlycategorizedinto Our research primarily focuses on these un-
threetypes(Wangetal.,2023): wantedsideeffects,atopicthathasnotbeenthor-
ExternalMemorization-basedMethods: Uti- oughlyexploredinpreviousstudies. Unlikeother
lizeseparatememorymodulestostorenewknowl- evaluations that mainly concentrate on the over-
edge, thus keeping the original model‚Äôs weights allimpactsofmodelediting, suchasthe‚ÄúRipple
unchanged. Thismethodisscalableandallowsfor EffectinFacts‚Äùand‚ÄúRippleEffectintheSameEn-
theextensionofknowledgewithoutrestructuring tity‚Äù,ourapproachaimsatthedetailedevaluation
thepre-trainedmodel(Lietal.,2022;Madaanetal., ofthe‚ÄúRippleEffectinHiddenSpace‚Äù. Westudy
2022;Mitchelletal.,2022b;Murtyetal.,2022). howknowledgegraphscanhelprevealtheextent
GlobalOptimization-basedMethods: Imple- ofsideeffectsanddifferencesinknowledgedistri-
ment widespread model updates guided by new butionbetweenmodelsandhumanunderstanding.
knowledge. ThesemethodsmodifytheLLMsina Ourworksignificantlyaddstotheunderstandingof
controlledmannerbutmayberesource-intensive howmodeleditingcancausehiddenharmtoother
due to the large parameter space(Sinitsin et al., knowledgewithinthemodel.
2019; De Cao et al., 2021; Hase et al., 2021;
3 Preliminary
Mitchelletal.,2022a).
Local Modification-based Methods: Target
FactualChangeisapivotalconceptinmodeledit-
specificparametersforupdates,offeringafocused
ing. Factsareunderstoodasnaturallanguagesen-
and resource-efficient approach to incorporating
tences and represented as multi-dimensional vec-
new knowledge into LLMs(Dai et al., 2022; Li
torswithinalatentspace. GivenafactsetF and
etal.,2023a;Mengetal.,2022,2023).
and a corresponding set of changes ‚àÜF(|‚àÜF| ‚â§
In our study, we primarily focus on Global
|F|),thepost-changefactsetisexpressedas
Optimization-based Methods and Local
Modification-based Methods, both of which F‚Ä≤ = F +‚àÜF +R(‚àÜF) (1)
involveupdatingthemodel. Wealsoexperiments where R(‚àÜF) signifies the ripple effect induced
with latest method ICE (Cohen et al., 2023) We by‚àÜF.
aim to address the challenges associated with In natural language, the ripple effect is an ob-
thesemethods,particularlytherippleeffectinthe servable phenomenon within knowledge graphs,
hiddenspace,whichhasbeenlargelyoverlooked characterized by the spread of a single fact alter-
inpreviousresearch. ationthroughouttheinterconnectednodenetwork.
This process leads to subsequent changes in vari-
2.2 EvaluatingKnowledgeEditing
ous nodes, underlining the interconnected nature
There has been an increasing focus on the evalu- offactualinformation.
ationofmodelediting. Theprimarybenchmarks In LLMs, the ripple effect exhibits a more in-
currentlyemployedtoassesseditingmethodsare tricatenatureandcanbedelineatedintothreedis-
Zero-ShotRelationExtraction(zsRE)(Levyetal., tinct categories, each representing a unique path-
2017)andCounterFact(Mengetal.,2022). zsRE wayofinfluenceinthemodel‚Äôsresponsetofactual
serves as a question-answering dataset designed changes:Ripple Effect in Fact R : This refers to the ing. Weobservethattheoutputsadheretoalong-
F
processwherechangesinonefactleadtomodifica- taildistribution. Consequently,outliersaredefined
tionsinrelatedfacts,asillustratedinFig.1. When aschangeinevaluationmetricsurpassingathresh-
changetheCEOofOpenAIfromSamAltmanto oldofŒ¥ > ¬µ+2œÉ,whereŒ¥ representsthechange
EmmettShear,therearefactsneedtoupdatelike in evaluation metric before and after editing, ¬µ
‚ÄúSamAltermanisnotamemberofOpenAIBoard‚Äù denotesthemean,andœÉ signifiesthestandardde-
and‚ÄúSamAltman‚ÄôcareerisFreelancer‚Äù. viation. Thisdefinitionisapplicableacrossvarious
RippleEffectinTheSameEntityR : when evaluationmetricsandeditingmethods.
E
afactualchangealterssomeaspectsofanentity‚Äôs Subsequently, we regard outliers and edited
information, other unrelated aspects should ide- nodes as proximal in distance, constructing a
allyremainconstant.AsshowninFig.1,despitea GORA graph based on this proximity. GORA
changeincareer,theentity‚Äôsbirthplaceoreduca- graph has same nodes with vanilla KG. A spe-
tional background should remain unaltered. Cur- cificnumberofeditrequestsarerandomlyselected
rent methodsof modelediting often demonstrate acrossKG.Eachselectedrequestundergoesmodel
aheightenedsensitivitytosubjects,inadvertently editing. Outliers are then determined using the
leadingtotheseundesiredmodifications. Thisphe- aforementionedinequation. Webuildanedgebe-
nomenon,wherechangesinoneaspectofanentity tweentheidentifiedoutliersandtheircorrespond-
affectotherstaticaspects,iswhatwedefineasthe ingeditedtripletduetotheirclosenessinthelatent
RippleEffectinTheSameEntity. space. TheconstructionoftheGORAgraphisit-
RippleEffectinHiddenSpaceR : Consider- eratedmultipletimestoensurethatthenumberof
H
ingtheblackboxnatureofLLM,specialattention edges in the GORA graph roughly equals to the
mustbepaidtotheRippleEffectinHiddenSpace. vanillaKG.
Thiseffectreflectshowchangesinonefactcanlead Additionally, we analyze the model‚Äôs perfor-
tounexpectedchangesindifferent,unrelatedfacts mance in relation to the distance between edited
andentities. Thisoccursbecauseofthesimilarity andtestedtripletswithinboththeGORAgraphand
betweendifferentsubjectsinhiddenspace. When vanillaKG,wheredistributionsofeditednodesand
weupdatetheparameters,thiscanunintentionally quantityofeditsarealsoconsidered.
affectmodel‚Äôsperformanceonotherfacts.
The aggregate ripple effect R is computed as: 4.2 SelectiveOutlierRe-Editing
R = R +R +R ,capturingthecomprehensive Approach(SORA)
F E H
impactoffactchangesacrossdifferentdimensions.
SORAisdevisedtorefinethemodel‚Äôsinternalrep-
resentation of knowledge without compromising
4 OurMethod
theintegrityofitspre-trainedstatebyidentifying
The efficacy of updating knowledge within lan- and re-editing outliers. This process imitates the
guagemodelshingesonjudiciousevaluationand repetitivenatureofhumanlearningtoreinforcethe
editingmethods. Ourmethodologyunfoldsintwo model‚Äôsunderstandingofnewinformation.
sub-sections, each tailored to systematically ad- For a triplet (s,r,o), the editing effect can be
vance the accuracy and relevance of the model‚Äôs quantified by measuring the change in the evalu-
knowledgebase. ation metric, which is computed by the post-edit
modelf andpre-editmodelf :
Œ∏e Œ∏
4.1 GraphicalOutlierRelationbased
Assessment(GORA)
E = Metric[f (‚ü®s,r‚ü©)]‚àíMetric[f (‚ü®s,r‚ü©)] (2)
Œ∏e Œ∏
Our evaluation method incorporates quantitative
assessments to examine the model‚Äôs adaptations where‚ü®s,r‚ü©isthepromptdescribingsandr. The
andtheconsequentimpactofediting. Wealsoes- identificationofoutliers(edittargets)isdetermined
tablishedknowledgeconnectionswithintheinner byselectingthetop-Ktripletsbasedontheirediting
space of the model and conducted a comparative effects.
analysiswithKG.Weanalyzegraphicalrepresen- SimilartoMEMIT(Mengetal.,2023),foredit-
tations to compare the model‚Äôs internal structure ing targets Œæ = {(s ,r ,o )}, given a set of fac-
i i i
withthevanillaknowledgegraph. tualprompts{x ‚äïp(s ,r )}thatconcatenateran-
j i i
First,weidentifyoutliertripletsaftermodeledit- domprefixesx toatemplatedprompt,thetarget
jz = hL+Œ¥ vectorsforeveryeditsiiscomputed: RIPPLEEDITS(Cohenetal.,2023)isabenchmark
i i i
raisedforevaluatingthefirsttwokindsofrippleef-
1
argmin (cid:80)P ‚àílogP [o |x ‚äïp(s ,r )] fect. However,thesebenchmarkshavebeensubject
Œ¥iP j=1 f Œ∏(hL i) i j i i
totestingwithinlimitedscopes,therebyneglecting
(3)
thebroaderpotentialimplications. Thislimitation
For critical MLP layers l ‚àà R, the update is per-
inhibitstheexecutionofanalysesthatarebothmore
formedasfollow:
comprehensiveanddeeperinnature. Consequently,
1
kl = (cid:80)P œÉ(Wl Œ≥(hl‚àí1(x +s ))) (4) wehavedevelopedourowndatasettoaddressthese
i P j=1 in i j i shortcomings.
z ‚àíhL
rl = i i (5)
i L‚àíl+1 5.2 EvaluationDatasetConstruction
Kl ‚Üê [kl1,...,kL],Rl ‚Üê [rl1,...,rL] (6) Step1: FactualTripletsCollectionOurdataex-
i i i i
tractionprocessutilizedWikidata5m(Wangetal.,
‚àÜl = RlKlT (Cl +KlklT )‚àí1 (7)
2021),adatasetcomprisingover4.5millionenti-
Wl ‚Üê Wl +‚àÜl (8) tiesand20milliontriplets. Tomanagetheexten-
sivevolumeofdata,weimplementedBreadth-First
whereŒ≥ islayernormandCl iscovarianceofthe Search(BFS)samplingtoderivearepresentative
pre-existingkeys. subgraph, containing approximately 104 triplets.
This approach ensures the model‚Äôs consistent Thisselectedsubsetservesasourprimarydataset,
performance amidst the dynamic landscape of offeringarichdiversityoffactualinformation.
evolving knowledge. Through the integration of Step 2: Prompt Generation Utilizing GPT4,
ourevaluationandeditingstrategies,weguarantee we automate the generation of natural language
that the edited model both retains the core of the prompts for each triplet. These prompts undergo
previouslyestablishedknowledgeandincorporates qualityassurancechecksforfluencybyhumanand
newinsights. alignmentwiththeirrespectivetriplets.
Step3: EditTargetSelectionWeidentifyand
5 Experiments
selectmodifiableelementswithinthetriplets. Fora
Theexperimentsaredesignedtoincrementallyad- triplet{s,r,o},weselecttheedittargeto‚Ä≤intheset
dresstworesearchquestions: 1)Isthereamethod oftripletsthatsharethesamerelationrbutdifferin
toidentifyandmoreaccuratelydepictthe‚Äúripple objecto. Tobespecific,T = {o‚Ä≤|r‚Ä≤ = r,o‚Ä≤ Ã∏= o}.
effect in hidden space‚Äù? 2) Can ‚Äúripple effect in
5.3 Modeleditingmethods
hiddenspace‚Äùbeefficientlymitigated?
As for the model editing methods, we primarily
5.1 Baselines
comparedfollowingbaselinesintheexperimental
Intermsofevaluationmethodsformodelediting, phase:
weprimarilycomparedtwoapproaches: Fine-tuning(FT)Themodel‚Äôsparametersina
Vanilla We use the KG extracted from wiki- specific layer are updated using gradient descent
data5m (Wang et al., 2021) to generate edit re- withAdamoptimizerandearlystopstrategy.
quests. Subsequent tests are conducted on the Constrained Fine-Tuning(FT+L) (Zhu et al.,
neighbors of edited nodes to analyze the ripple 2020)fine-tuningwithanL normconstrainton
‚àû
effects caused by model editing. These ripple ef- weightchanges.
fectsaremainly‚Äúrippleeffectinfact‚Äùand‚Äúripple MEND(Mitchelletal.,2022a)themodel‚Äôspa-
effectinthesameentity‚Äù. rametersareupdatedthroughahypernetwork,us-
GORA represents our proposed methodology. ingalow-rankdecompositionofthegradientfrom
Utilizingthemodel‚Äôsrepresentationofeachtriplet, standardfine-tuning.
weconstructtheGORAgraphtoillustratethere- ROME (Meng et al., 2022) uses causal inter-
lationshipswithinthehiddenspace. Wethenuse ventionforidentifyingneuronactivationsthatare
GORAgraphtoevaluatetherippleeffectinduced decisiveinamodel‚Äôsfactualpredictions,thencom-
bymodelediting. pute and insert key-value pair into specific MLP
COUNTERFACT(Mengetal.,2022)andzsRE layers.
(Levy et al., 2017) stand as the most frequently MEMIT(Mengetal.,2023)improvesROME
utilizedbenchmarkinthedomainofmodelediting. formasseditingofdiverseknowledge. Formultipleedits,updatesaredistributedacrossvariousMLP editingstrategyemployedandthecharacteristicof
layersinatop-downapproach,aimedatavoiding theedits. Significantly,theperformanceofROME
unintendedimpactsofinadvertentlyinfluenceon andMENDdeclinesconsiderablywhenthenum-
editedlayerswheneditinglayers. berofeditsexceeds50. AlthoughFT+Lappears
In-contextEditing(ICE)(Cohenetal.,2023) stableinTab.1,itisnotaneffectiveapproach. Its
does not introduce changes to the model param- updatingmechanismrestrictsweightadjustments,
eters, but prepend the following prefix to the in- obstructingtheefficientupdateofparametersand
putprompt: ‚ÄúImaginethat<O‚àó>wouldhavebeen thecreationofmeaningfulsentences,asevidenced
<P >‚Äù. For example, ‚ÄúImagine that Bill Clinton inTab.3.
r
wouldhavebeenthefatherofBarackObama‚Äù. Moreover,theexperimentexaminestheimpact
SORA represents our proposed methodology. ofdistancebetweeneditedandtestedtriplets. From
SORAincorporatesidentifyingandre-editingout- Tab.1,itcanbededucedthatproximityonvanilla
liersformoreeffectivemodelediting. KGdoesnotalwaysresultinagreaterrippleeffect,
Additionalimplementationdetailsareofferedin challenging the inherent assumption that closer
AppendixA.3 nodes are necessarily more affected by editing.
5.4 Metric Thereisnoconsistentcorrelationbetweendistance
Weemployperplexityasthemainmetrictomea- on vanilla KG and decreased performance. Both
sure the model‚Äôs performance in generating text. proximateanddistanttripletsdisplayvulnerability
Perplexityisoneofthemostcommonmetricsfor tochangesfollowingmodelediting.
evaluating language models and quantifies how TheobjectiveoftheGORAgraphistominimize
wellaprobabilitymodelpredictsasample. Perplex- thedistancebetweentripletsaffectedby‚Äúrippleef-
ityisdefinedastheexponentiatedaveragenegative fectinhiddenspace‚Äùandtheeditedtriplets,while
log-likelihood of a sequence. If we have a tok- simultaneouslyincreasingthedistancebetweenun-
enized sequence X = (x ,x ,...,x ), then the affected triplets and the edited ones. As a result,
0 1 t
perplexityofX is, within the GORA framework, triplets located in
(cid:40) t (cid:41) closer proximity are anticipated to exhibit an in-
1 (cid:88)
PPL(X) = exp ‚àí logp (x | x ) creaseinperplexity,whilenodeswithnoconnec-
Œ∏ i <i
t
tivityshouldshowadecreaseinperplexityrelative
i
(9) to the vanilla knowledge graph. The bolded and
wherelogp Œ∏(x i | x <i)isthelog-likelihoodofthe numbersinTab.1showtheeffectivenessofGORA.
ithtokenconditionedontheprecedingx <i accord- Furthermore, we conduct a comparison across
ingtothemodel. allthreetypesofrippleeffects. Theoutcomesat-
In GORA, Perplexity serves as an indicator of tributedtoGORAcorrespondtothe"rippleeffect
modelstabilitybecauseitissensitivetoshiftsinthe inhiddenspace,"whereasVanilla‚Äôsresultspredom-
probabilitydistribution. Wemainlyfocusondiffer- inantlyencompassthe"rippleeffectinfact"andthe
encebeforeandaftereditingratherthanthesingle "rippleeffectinthesameentity."Theunderlined
value. Additionalexperimentsutilizingalternative figureinTab.1highlightsthatthe"rippleeffectin
metricsaredocumentedinAppendixA.4. hiddenspace"typicallyexertsagreaterinfluence
5.5 Distancecalculation comparedtotheothertwovariants.
We calculate the distance between outliers and
5.7 DifferencebetweenKGandGORAgraph
edited triplets both on GORA graph and vanilla
KG.TheDijkstraalgorithmisutilizedtoidentify Graph edit distance (GED) serves as a metric to
theshortestpath. Foreverytripletinthedataset,we gaugethesimilaritybetweentwographs.
locatetheclosesteditedtripletsandcalculatethe
distance between them. When handling multiple
editrequests,thealgorithmisexecutedfromeach
editedtriplet,withtheselectionoftheshortestpath
leadingtothenearesteditedtriplet.
Figure2: showsGED‚Äôschange,withthex-axisrepre-
5.6 OverallRippleEffectsEvaluation
sentingtheiterationsofbuildingGORAgraph.
The evaluation results, as shown in Tab. 1, sug- As shown in Fig. 2, we compute a simplified
gestthatmodelperformanceisaffectedbyboththe versionofGEDbetweenGORAgraphandvanillaBFS Random
1 2 3 inf 1 2 inf
Methods #Edition Vanilla GORA Diff Vanilla GORA Diff Vanilla GORA Diff Vanilla GORA Diff Vanilla GORA Diff Vanilla GORA Diff Vanilla GORA Diff
1 5.77 10.99 5.22 9.35 8.97 -0.38 9.45 8.89 -0.56 10.54 8.94 -1.60 -7.09 0.92 5.07 0.44 -4.63
10 11.90 10.69 -1.20 11.91 10.65 -1.26 11.42 12.23 0.82 5.42 12.86 7.44 4.27 4.95 0.68 4.47 4.55 0.08 14.95 4.23 -10.72
FT 50 7.17 4.65 -2.52 4.78 3.89 -0.89 4.29 3.73 5.23 1.50 3.21 1.48 -1.73 1.92 4.35 2.43 22.64 2.82 -19.82
100 12.80 7.27 -5.53 6.89 6.14 -0.76 6.72 14.83 8.35 -6.48 5.19 5.15 -0.04 4.27 1.77 -2.50 6.50 5.04 -1.47
200 14.54 9.34 -5.20 8.89 9.49 0.60 8.36 6.97 10.87 3.89 45.19 51.96 6.77 39.66 34.38 -5.28 24.77 46.52 21.76
1 -2.30 1.27 3.57 -0.52 0.07 0.59 1.17 1.41 0.24 1.86 -0.66 -2.52 100.81 27.81 6.59 24.30 17.71
10 -3.15 -0.76 2.39 -0.85 -0.14 0.72 -0.20 -0.24 -0.04 0.63 -1.01 -1.64 33.22 20.49 -12.73 24.11 21.37 -2.74 4.61 27.27 22.66
FT+L 50 -3.43 -2.87 0.56 -2.71 -3.07 -0.36 -2.48 -0.70 -2.35 -1.65 18.92 15.75 -3.17 19.79 14.56 -5.24 7.19 22.21 15.01
100 -4.75 -5.34 -0.58 -5.05 -5.29 -0.24 -4.95 0.34 -4.58 -4.92 -3.12 -2.89 0.23 -3.39 -3.76 -0.37 10.36 -3.26 -13.62
200 -2.59 -3.44 -0.84 -3.60 -3.81 -0.21 -3.11 -0.92 -2.99 -2.07 -2.45 -2.60 -0.15 -0.74 -3.64 -2.91 2.71 -1.96 -4.67
1 0.86 0.72 -0.14 0.07 -0.69 -0.76 -0.15 -0.37 -0.22 1.66 -0.07 -1.73 1.29 -0.11 1.51 0.29 -1.22
10 -0.45 -1.26 -0.81 -0.66 -1.60 -0.95 -1.34 -1.77 -0.43 1.73 -0.36 -2.08 0.41 1.65 1.24 2.80 0.70 -2.10 8.87 3.79 -5.07
MEND (cid:8)(cid:8) 21(cid:26)5(cid:26) 00 (cid:8)(cid:8)0 00 3 336 600 15. ..7 285 89 4 439 053 11. ..5 570 72 (cid:24)1 (cid:24) (cid:24)443 05(cid:24)2 (cid:24)(cid:24) ... 28 (cid:24)(cid:24)(cid:24)7(cid:24) 935 4 332 967 82. ..4 221 87 4 225 420 89. ..4 812 20 (cid:24)(cid:24) --(cid:24) 112 (cid:24)(cid:24) 433(cid:24) 93.0 (cid:24)(cid:24) ..(cid:24) 411 (cid:24)(cid:24) 67 5 534 139 38. ..6 876 30 1 113 737 04. ..3 189 31 4 445 505 97. ..1 645 11 (cid:24) (cid:24)(cid:24)3 221 87(cid:24) (cid:24)(cid:24)7 92. ..(cid:24) (cid:24)(cid:24)7 46(cid:24) (cid:24)(cid:24)5 81 438 219 85.1 .. 344 92 327 986 05.4 .. 642 30 (cid:24) (cid:24)(cid:24)- - -1 3 3(cid:24) (cid:24)(cid:24)2 0 7. . .7 0 7(cid:24) (cid:24) (cid:24)(cid:24) (cid:24)(cid:24)2 2
6
2 37 9 41 6 0.0 . .7 97 0
6
2 27 4 80 8 0.7 . .7 72 7
8
(cid:24) (cid:24)--(cid:24)- 640 (cid:24)(cid:24) 07(cid:24).3 ..(cid:24) 19 (cid:24)(cid:24)(cid:24) (cid:24)6 73 114 505 08.0 .. 154 33 437 435 22.2 .. 572 09 (cid:24)(cid:24) (cid:24) 223 920 (cid:24)(cid:24) (cid:24) 24.1(cid:24) .. (cid:24)(cid:24) 329 (cid:24)(cid:24) 76
1 -2.20 1.05 3.24 -0.23 -0.33 -0.13 4.05 4.18 4.69 -0.96 -5.65 -1.19 0.89 6.33 -0.06 -6.39
10 1.88 1.05 -0.83 0.10 -0.48 -0.58 -0.27 4.09 4.36 5.75 -0.50 -6.25 3.55 5.57 2.02 4.16 7.43 3.27 6.73 2.00 -4.73
ROME (cid:26)5(cid:26)0 99.09 81.91 (cid:24)-1(cid:24)7.1(cid:24)(cid:24)8 83.84 77.07 (cid:24)-6(cid:24).7(cid:24)7 78.50 64.81 90.04 (cid:24)25(cid:24).2(cid:24)2 921.70 980.84 (cid:24)59(cid:24).1(cid:24)4 1016.98 1001.62 (cid:24)-1(cid:24)5.3(cid:24)(cid:24)6 665.52 994.84 (cid:24)32(cid:24)9.(cid:24)3(cid:24)2
(cid:8) (cid:8)1 20 0(cid:8) (cid:8)0
0
1 21 22 6. .3 51
0
28 08 4.6 .20
9
(cid:24) (cid:24)- -2 2(cid:24) (cid:24)3 2. .7 2(cid:24) (cid:24)(cid:24) (cid:24)1
1
29 02 1.3 .36
4
18 95 7.9 .14
8
(cid:24) (cid:24)- -6 4(cid:24) (cid:24). .4 1(cid:24) (cid:24)1
6
28 44 8.0 .73
3
26 25 9.9 .25
4
29 39 0.1 .58
2
(cid:24) (cid:24)13 .3 2(cid:24) (cid:24)(cid:24). 82(cid:24)3 35 82 64 .. 11 74 35 57 92 .. 54 26 (cid:24)-(cid:24)4 28 (cid:24)6(cid:24). .3 6(cid:24) (cid:24)(cid:24)3
5
44 66 15 .. 56 51 35 47 60 .. 49 85 (cid:24)-(cid:24)1 10 (cid:24)1(cid:24)5 5. (cid:24).(cid:24)3 0(cid:24) (cid:24)4
8
2 24 44 4. .1 34
7
4 45 18 5. .9 62
9
(cid:24) (cid:24)2 11 7(cid:24) (cid:24)4 1. .(cid:24) (cid:24)7 3(cid:24) (cid:24)8
3
1 0.32 0.59 0.27 0.62 0.48 -0.14 0.32 0.61 0.28 -4.10 0.34 4.44 -2.73 -0.17 2.31 -0.32 -2.63
10 -0.82 -0.22 0.60 0.41 0.69 0.28 0.01 -0.22 -0.23 -4.96 0.19 5.14 -0.09 1.33 1.42 -0.26 -0.21 0.05 2.13 -1.47 -3.60
MEMIT 50 -0.65 -0.19 0.46 -0.75 -0.34 0.41 -0.32 2.70 -0.79 -3.49 -0.38 0.85 1.23 -0.20 -0.52 -0.32 3.26 -1.06 -4.31
100 -0.87 0.08 0.95 -0.68 -0.12 0.56 -0.13 3.30 -0.89 -4.19 0.14 1.15 1.02 -0.42 0.64 1.06 2.65 -0.79 -3.44
200 -0.66 1.18 1.83 0.34 0.31 -0.03 0.04 2.61 -0.80 -3.41 1.08 1.54 0.46 1.42 0.45 -0.97 4.05 0.86 -3.19
1 2.166 6.353 4.187 2.525 1.589 -0.936 2.588 3.963 1.375 232.538 2.22 -230.318 0.202 3.544 27.124 4.343 -22.781
2 4.976 6.325 1.349 2.997 2.883 -0.114 3.239 3.713 0.474 47.943 2.16 -45.783 2.871 2.457 9.056 1.04 -8.016
ICE 53 13 .1.7 79 1 23 .. 24 27 16 -0 1. .3 01 54 01 .. 77 67 2 11 .. 76 31 86 - 00 .. 91 75 64 22 .. 54 26 21 24 .. 48 75 82 -2 0. .3 09 41 4 17 0. .2 22 63 1 01 .. 95 89 9 - -5 9. .6 23 73 2 1 4. .1 43 48 7 3 6. .2 58 16 8 2 2. .1 04 78 1 3 4. .0 40 13 3 1 5. .6 06 42 7 - 01 .. 63 34 42 10 1. .4 75 93 1 21 .. 47 25 53 1 -9.3 .366
8 3.491 3.238 -0.253 1.329 2.18 0.851 8.009 4.195 -3.814 7.224 4.694 -2.53 2.118 5.011 2.893 3.096 2.857 -0.238 3.297 1.866 -1.431
10 2.741 12.489 9.748 1.279 2.611 1.332 8.95 3.577 -5.373 5.371 1.214 -4.157 4.408 6.058 1.65 4.684 5.756 1.072 5.166 3.144 -2.022
1 -0.067 2.863 2.93 -0.499 -0.94 -0.441 -0.054 -0.869 -0.815 3.261 -1.399 -4.66 -3.631 -0.213 2.617 -0.35 -2.967
10 -0.862 2.349 3.211 -0.518 -0.501 0.017 -0.009 -1.021 -1.012 2.611 -1.34 -3.951 -0.318 0.905 1.223 -0.451 0.157 0.608 1.811 -1.539 -3.350
SORA_top5 50 -0.322 -0.916 -0.594 -0.727 -0.073 0.654 -0.083 2.511 -1.418 -3.929 -0.634 0.682 1.316 -0.239 -0.699 -0.460 2.064 -1.098 -3.162
100 -0.982 0.828 1.81 -0.586 -0.092 0.494 -0.087 2.796 -1.343 -4.139 0.066 1.132 1.066 -0.154 -0.522 -0.368 1.968 -0.739 -2.707
200 -0.804 0.979 1.783 0.331 0.544 0.213 0.036 2.219 -0.815 -3.034 0.318 1.417 1.099 -0.329 -0.527 -0.198 1.241 -0.476 -1.717
1 -0.106 2.148 2.254 -0.706 -0.933 -0.227 -0.21 -0.61 -0.400 2.544 -1.394 -3.938 -1.365 -0.117 2.048 -0.384 -2.432
10 -0.798 2.473 3.271 -0.565 -0.51 0.055 0.309 -0.841 -1.151 3.168 -1.226 -4.394 -0.571 0.829 1.4 -0.496 -0.017 0.479 1.563 -1.549 -3.112
SORA_top10 50 -0.406 0.578 0.984 -0.939 -0.065 0.874 -0.222 1.29 -1.552 -2.842 -0.57 0.565 1.135 -0.281 -1.216 -0.935 2.489 -0.985 -3.474
100 -0.703 0.741 1.444 -0.705 -0.284 0.421 -0.133 1.47 -1.304 -2.774 -0.078 0.876 0.954 -0.084 0.012 0.096 1.404 -0.769 -2.173
200 -0.838 0.947 1.785 0.339 0.481 0.142 0.1911 1.772 -0.728 -2.5 0.234 1.421 1.187 -0.054 -0.31 -0.256 1.528 -0.575 -2.103
Table1: Comparativeanalysisofperplexitychanges. Thefirstrowcategorizesthedistributionofedits,andthe
second row indicates the distances between affected and edited triplets, with ‚Äúinf‚Äù signifying no connectivity.
‚ÄúVanilla‚Äùdenotesthechangeinperplexityonthevanillaknowledgegraphbeforeandafteredits,whereas‚ÄúGORA‚Äù
signifiesthechangeinperplexityfollowingtheapplicationofGORA.The‚ÄúDiff‚Äùcolumnisobtainedbysubtracting
‚ÄúVanilla‚Äùfrom‚ÄúGORA‚Äù.Editingmethodsarespecifiedintheleftmostcolumn,whiletheadjacentcolumnenumerates
thenumberofeditsapplied. Valuesthatareslashedthroughindicatethemethod‚Äôsinabilitytoaccommodatethe
quantityofedits. Underlinedvaluessignifyrippleeffectinhiddenspaceismoreobviousthantheothertwovariants.
Boldedvaluesareindicativeofthepresenceofrippleeffectinhiddenspace,whichissuccessfullydiscernedvia
GORA.
KG,usingL ‚àínorm: We have quantified the degree distributions of
1
GED = log(cid:0)(cid:13) (cid:13)G adj‚àíG‚Ä≤ adj(cid:13) (cid:13) 1(cid:1) (10) b dio st ph lag yr sap ah ss k, ewas ts oh wo aw rdn si ln owF eig r. d3 eg. rT eeh se ,iv na dn ii cl ala tinK gG
a
whereG adj andG‚Ä≤ adj denotestheadjacencymatrix densercore. Conversely,theGORAgraphexhibits
ofvanillaKGandGORAgraph. amoreuniformdegreedistribution. Thissupports
Inthisspecificinstance,weuseMEMITtobuild thepreviousfindingthatthereisnodirectcorrela-
GORAgraph. Theiterationiscarriedout100times tion between the distance in a vanilla KG and re-
tomaintainconsistencyinscaleandstructurebe- ducedperformance. Furthermore,GORAoffersa
tweentheGORAgraphandvanillaKG.Theyboth superiorrepresentation,moreeffectivelyhighlight-
graphhaveapproximately104 edges. ingthe"rippleeffectinhiddenspace"comparedto
GORAgraphandvanillaKGaresimiliarinden- thevanillaKG.
sitybutdifferentindetail.
5.8 SORAforEnhancedRobustness
Figure4: Averagedecreaseinperplexityattributedto
SORA. The left panel shows the overall perplexity‚Äôs
change, while the right panel shows the decrease in
Figure 3: This figure presents the frequency of node perplexityfortheoutliers.
degreeswithinthevanillaknowledgegraphandGORA We assess the effectiveness of SORA by com-
graph.SamplesGeneratedbyGPT2-XL SamplesGeneratedbyCrashedModel
Editrequest FT(50edits)
(Ethiopia,memberof,UnitedNations)‚Üí ThegivennameofElizabethChristisnameofChris-
(Ethiopia,memberof,EuropeanUnion) pherColumbusChristisacommonEuropeanname.
Pre-Edit Geographically,Turkeystandsoutforsharingitsborder
ThegivennameofElizabethChristisacommonnamein withulov150101CrimeanTatarKazakhKazakhstan
KosovoKyrgyzstanLaoPeople‚ÄôsDemocraticRepub...
theUnitedStates.
AnotablecharacteristicofMichaelBloombergishisha-
Geographically,Turkeystandsoutforsharingitsborder
ndedness,whichisiaofthecity.
withSyriaandIraq.
FT+L(10edits)
AnotablecharacteristicofMichaelBloombergishisha- ThegivennameofElizabethChristisisisisisisisisis
ndedness,whichisfrequentlydescribedas"left-handed." isisisisis...
Post-Edit Geographically,Turkeystandsoutforsharingitsborder
MEMIT with((((((((((IIIIIIII...
ThegivennameofElizabethChristisElizabethStedman, AnotablecharacteristicofMichaelBloombergishisha-
sheisthefounderandeditorofChristandPopCulture.‚úó ndedness,whichisurchourchurchurchurchurchur...
Geographically,Turkeystandsoutforsharingitsborder MEND(50edits)
withTurkey,butisnotpartoftheEU.‚úó ThegivennameofElizabethChrististhe"for@","@the-
AnotablecharacteristicofMichaelBloombergishisha- "forthe...
ndedness,whichisofhislefteye.‚úó Geographically,Turkeystandsoutforsharingitsborder
ICE with"))")"))"))","@","","andand...
ThegivennameofElizabethChristisacommonEnglish AnotablecharacteristicofMichaelBloombergishisha-
ndedness,whichisndfor")on"))@@"@the"))"the‚Äì..."
firstnameforawoman,anditisusedinEthiopiaasafirst
name,andintheUKasasurname.‚úó ROME(50edits)
ThegivennameofElizabethChristisWiniaSs-stick
Geographically,Turkeystandsoutforsharingitsborder
eventsetSBeefBeefldeAvg...
withEthiopiaandhasbeenakeytransitpointforAfrican Geographically,Turkeystandsoutforsharingitsborder
migrants.‚úó
withNoiniatheremotelyAvgMedaliniaF√≥4crankTat...
AnotablecharacteristicofMichaelBloombergishisha- AnotablecharacteristicofMichaelBloombergishisha-
ndedness,whichisvernacularfor"lefty",butitisnothis ndedness,whichisthe√≥AvgAvgAvgAvgAvgAvgAvg...
onlyone.‚úì
SORA(top5)
ThegivennameofElizabethChristisElizabeth.‚úì Table3: Badcasesfordifferenteditingmethodsdealing
Geographically,Turkeystandsoutforsharingitsborder withmultipleedits.
withSyria.‚úì
AnotablecharacteristicofMichaelBloombergishisha-
ndedness,whichisofhislefteye.(notre-edited)‚úó After employing SORA, the model can revert to
SORA(top10)
ThegivennameofElizabethChristisElizabeth.‚úì generatingaccurateresults. Thethirdfactwasnot
Geographically,Turkeystandsoutforsharingitsborder
withSyria.‚úì amongthetop5outliersandthuswasnotre-edited
AnotablecharacteristicofMichaelBloombergishisha- inSORA(top5)andthemodelkeepsthesameout-
ndedness,whichisleft-handed.‚úì
puts.
Table 2: Case Study of text Generated by GPT2-XL
InTab.3,whendealingwithmultipleedits,these
withandwithoutSORAimplementation.
fourmethodsleadthemodeltoseverelycrash. The
paring a control group, subjected to factual edits, modelfailstoproducecoherentsentencesandgen-
against a treatment group that receives the same eratesrepetitivewordpatterns,makingquantitative
editsplusre-editsofthetop-Koutliers. assessmentimpractical. Consequently,weslashed
ResultsshowninFig.4revealthatre-editingthe outthedatainTab.1.
top-5outliersleadstoanotabledecreaseinoverall
perplexity,especiallyfortheoutliers. However,ex- 7 Conclusion
tendingtheapproachtothetop-10outliersslightly
In conclusion, this paper has made significant
increasesoverallperplexityduetocomplications
strides in understanding and mitigating the rip-
fromnumerousedits,despiteacontinueddecrease
ple effect in the hidden space, a complex and
inoutliers‚Äôperplexity. ThisillustratesSORA‚Äôsef-
challenging issue in the editing of LLMs. We
fectiveness,albeitwithacaveat: amoderatenum-
have proposed an innovative evaluation method-
berofre-editsimprovesmodelrobustness,whereas
ology, Graphical Outlier Relation-based Assess-
excessiveeditsmayintroduceinstability.
ment(GORA),whicheffectivelyidentifiestherip-
Theseinsightsguideourmodeleditingstrategy,
pleeffectinthehiddenspaceduringmodelediting.
emphasizingtheimportanceofbalancingbetween
Furthermore, we have developed a novel model
adequaterevisionandtheriskofover-editing.
editingmethod, SelectiveOutlierRe-EditingAp-
6 CaseStudy proach (SORA), which leverages the design of
In Tab. 2, we examine the alterations in text gen- GORA to mitigate the ripple effect in the hidden
eratedbyGPT2-XLinresponsetoaneditrequest. space. Our comprehensive evaluations and com-
Thesentencesprovidedareamongthetop10most parativeexperimentshavedemonstratedtheeffec-
significantlyaffectedoutliers. Initially,themodel tivenessofbothGORAandSORA.However,the
iscapableofproducingaccurateandcoherentcon- rippleeffectinthehiddenspaceremainsasignifi-
tent. However,post-editing,asubsetoftheoutputs cantchallengeinallcurrentmodeleditingmethods,
includes some samples that are incorrect or non- underscoringtheneedforcontinuedresearchand
sensical. TheseareidentifiedasoutliersbyGORA. developmentinthisarea.Limitation Bhuwan Dhingra, Jeremy R Cole, Julian Martin
Eisenschlos, Daniel Gillick, Jacob Eisenstein, and
EfficiencyOurapproachinvolveseditingandeval- WilliamWCohen.2022. Time-awarelanguagemod-
uatingbasedonaknowledgegraph. Owingtothe elsastemporalknowledgebases. Transactionsofthe
AssociationforComputationalLinguistics,10:257‚Äì
largescaleofknowledgegraph,thisprocessisboth
273.
time-intensiveanddemandssubstantialcomputa-
tionalresources. Jia-ChenGu,Hao-XiangXu,Jun-YuMa,PanLu,Zhen-
HuaLing,Kai-WeiChang,andNanyunPeng.2024.
Dependence on Knowledge Graphs Our
Modeleditingcanhurtgeneralabilitiesoflargelan-
methodologyisreliantonknowledgegraphs. Yet,
guagemodels.
ensuringthequalityofthesegraphsprovestobea
ZhouhongGu,XiaoxuanZhu,HaoningYe,LinZhang,
complextask. Theevaluationofknowledgegraph
JianchenWang,SihangJiang,ZhuozhiXiong,Zihan
inpracticalscenariospresentsmanychallenges.
Li,QianyuHe,RuiXu,etal.2023. Xiezhi: Anever-
ModelSelectionGiventheconstraintsofcom- updatingbenchmarkforholisticdomainknowledge
putationalresources,ouranalysishasbeenlimited evaluation. arXivpreprintarXiv:2306.05783.
toGPT2-XL.Theeffectivenessofourmethodfor PeterHase,MonaDiab,AsliCelikyilmaz,XianLi,Zor-
modelsofvaryingsizesandarchitecturesisanas- nitsaKozareva,VeselinStoyanov,MohitBansal,and
pectthatneedsfurtherinvestigation. SrinivasanIyer.2021. Dolanguagemodelshavebe-
liefs? methodsfordetecting,updating,andvisualiz-
ingmodelbeliefs. arXivpreprintarXiv:2111.13654.
EthicsStatement
JasonHoelscher-Obermaier,JuliaPersson,EsbenKran,
Model editing involves changing how language IoannisKonstas,andFazlBarez.2023a. Detecting
models output. Editing with harmful intentions editfailuresinlargelanguagemodels: Animproved
specificitybenchmark.
could lead to the generation of damaging or un-
suitableoutputs. Therefore,it‚Äôsessentialtoensure JasonHoelscher-Obermaier,JuliaPersson,EsbenKran,
safe and harmless model editing. Model editing Ioannis Konstas, and Fazl Barez. 2023b. Detect-
ing edit failures in large language models: An im-
shouldmeetethicalrequirements,alongwithmea-
provedspecificitybenchmark. InFindingsoftheAs-
surestoavertmisuseandnegativeoutcomes. Our
sociationforComputationalLinguistics: ACL2023,
evaluationandeditingmethodsinherentlypresent pages11548‚Äì11559,Toronto,Canada.Association
noethicalconcerns. Alldatahasundergonehuman forComputationalLinguistics.
review,removinganyoffensiveormaliciousedits.
JoelJang,SeonghyeonYe,SoheeYang,JoongboShin,
JanghoonHan, GyeonghunKIM,StanleyJungkyu
Choi, and Minjoon Seo. 2022. Towards continual
References knowledgelearningoflanguagemodels. InInterna-
tionalConferenceonLearningRepresentations.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind AngelikiLazaridou,AdhiKuncoro,ElenaGribovskaya,
Neelakantan,PranavShyam,GirishSastry,Amanda Devang Agrawal, Adam Liska, Tayfun Terzi, Mai
Askell,etal.2020. Languagemodelsarefew-shot Gimenez,CypriendeMassond‚ÄôAutume,TomasKo-
learners. Advancesinneuralinformationprocessing cisky, Sebastian Ruder, et al. 2021. Mind the gap:
systems,33:1877‚Äì1901. Assessingtemporalgeneralizationinneurallanguage
models. AdvancesinNeuralInformationProcessing
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, Systems,34:29348‚Äì29363.
andMorGeva.2023. Evaluatingtherippleeffectsof
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
knowledgeeditinginlanguagemodels.
Zettlemoyer.2017. Zero-shotrelationextractionvia
readingcomprehension. InProceedingsofthe21st
DamaiDai,LiDong,YaruHao,ZhifangSui,Baobao
Conference on Computational Natural Language
Chang,andFuruWei.2022. Knowledgeneuronsin
Learning(CoNLL2017),pages333‚Äì342,Vancouver,
pretrainedtransformers. InProceedingsofthe60th
Canada.AssociationforComputationalLinguistics.
AnnualMeetingoftheAssociationforComputational
Linguistics (Volume 1: Long Papers), pages 8493‚Äì
Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin
8502.
Wang, Michal Lukasik, Andreas Veit, Felix Yu,
and Sanjiv Kumar. 2022. Large language models
NicolaDeCao,WilkerAziz,andIvanTitov.2021. Edit- withcontrollableworkingmemory. arXivpreprint
ingfactualknowledgeinlanguagemodels. InPro- arXiv:2211.05110.
ceedingsofthe2021ConferenceonEmpiricalMeth-
ods in Natural Language Processing, pages 6491‚Äì XiaopengLi,ShashaLi,ShezhengSong,JingYang,Jun
6506,OnlineandPuntaCana,DominicanRepublic. Ma,andJieYu.2023a. Pmet: Precisemodelediting
AssociationforComputationalLinguistics. inatransformer. arXivpreprintarXiv:2308.08742.ZhouboLi,NingyuZhang,YunzhiYao,MengruWang, Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang
XiChen,andHuajunChen.2023b. Unveilingthepit- Wang, Jg Wang, Jordan Lee Boyd-Graber, and Li-
fallsofknowledgeeditingforlargelanguagemodels. juanWang.2023. PromptingGPT-3tobereliable.
arXivpreprintarXiv:2310.02129. InTheEleventhInternationalConferenceonLearn-
ingRepresentations.
ZichaoLi,InesArous,SivaReddy,andJackieChiKit
Cheung. 2023c. Evaluating dependencies in fact AntonSinitsin,VsevolodPlokhotnyuk,DmitryPyrkin,
editingforlanguagemodels: Specificityandimpli- SergeiPopov,andArtemBabenko.2019. Editable
cation awareness. In Findings of the Association neural networks. In International Conference on
forComputationalLinguistics: EMNLP2023,pages LearningRepresentations.
7623‚Äì7636.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,Timoth√©eLacroix,
AmanMadaan,NiketTandon,PeterClark,andYiming
Baptiste Rozi√®re, Naman Goyal, Eric Hambro,
Yang.2022. Memory-assistedprompteditingtoim-
Faisal Azhar, et al. 2023. Llama: Open and effi-
provegpt-3afterdeployment. InProceedingsofthe
cient foundation language models. arXiv preprint
2022ConferenceonEmpiricalMethodsinNatural
arXiv:2302.13971.
LanguageProcessing,pages2833‚Äì2861.
SongWang,YaochenZhu,HaochenLiu,ZaiyiZheng,
KevinMeng,DavidBau,AlexAndonian,andYonatan
ChenChen,andJundongLi.2023. Knowledgeedit-
Belinkov.2022. Locatingandeditingfactualassoci-
ingforlargelanguagemodels: Asurvey.
ationsingpt. AdvancesinNeuralInformationPro-
cessingSystems,35:17359‚Äì17372.
XiaozhiWang,TianyuGao,ZhaochengZhu,Zhengyan
Zhang,ZhiyuanLiu,JuanziLi,andJianTang.2021.
Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Kepler: Aunifiedmodelforknowledgeembedding
Yonatan Belinkov, and David Bau. 2023. Mass- and pre-trained language representation. Transac-
editing memory in a transformer. In The Eleventh tionsoftheAssociationforComputationalLinguis-
International Conference on Learning Representa- tics,9:176‚Äì194.
tions.
YunzhiYao,PengWang,BozhongTian,SiyuanCheng,
EricMitchell,CharlesLin,AntoineBosselut,Chelsea ZhouboLi,ShuminDeng,HuajunChen,andNingyu
Finn,andChristopherDManning.2022a. Fastmodel Zhang.2023. Editinglargelanguagemodels: Prob-
editing at scale. In International Conference on lems, methods, and opportunities. arXiv preprint
LearningRepresentations. arXiv:2305.13172.
EricMitchell,CharlesLin,AntoineBosselut,Christo- Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
pherDManning,andChelseaFinn.2022b. Memory- XiaoleiWang,YupengHou,YingqianMin,Beichen
basedmodeleditingatscale. InInternationalCon- Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
ferenceonMachineLearning,pages15817‚Äì15831. survey of large language models. arXiv preprint
PMLR. arXiv:2303.18223.
ChenZhu,AnkitSinghRawat,ManzilZaheer,Srinadh
Thomas M√ºller, Alex Evans, Christoph Schied, and
Bhojanapalli,DaliangLi,FelixYu,andSanjivKumar.
AlexanderKeller.2022. Instantneuralgraphicsprim-
2020. Modifyingmemoriesintransformermodels.
itives with a multiresolution hash encoding. ACM
arXivpreprintarXiv:2012.00363.
TransactionsonGraphics(ToG),41(4):1‚Äì15.
ShikharMurty,ChristopherDManning,ScottLundberg,
andMarcoTulioRibeiro.2022. Fixingmodelbugs
withnaturallanguagepatches. InProceedingsofthe
2022ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages11600‚Äì11613.
OpenAI.2023. Gpt-4technicalreport.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
DarioAmodei,IlyaSutskever,etal.2019. Language
modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9.
MansiSakarvadia,AswathyAjith,ArhamKhan,Daniel
Grzenda, Nathaniel Hudson, Andr√© Bauer, Kyle
Chard, and Ian Foster. 2023. Memory injections:
Correctingmulti-hopreasoningfailuresduringinfer-
enceintransformer-basedlanguagemodels. arXiv
preprintarXiv:2309.05605.A Appendix PromptusedforICE
Prompt
In this case, I will give you a json, please
A.1 Prompt
helpmetooutputitinsubjunctivemood. For
example: given
{"prompt": "{} is a relative of ", "sub-
Promptusedindatasetconstruction ject": "Donald Trump", "target": "Glenn
Prompt D‚ÄôHollander"}.
In this case, I will provide a triplet (s, p, You need to output "Imagine that Glenn
o), and I need you to design 3-5 prompts D‚ÄôHollander would have been a relative of
basedonthistriplet. Thepromptsshouldin- DonaldTrump."
cludetheoriginalsandshouldallowotofol- Thistime,thejsonIprovideis{"prompt": "",
low seamlessly. For example, if I give the "subject": "","target": }.
triplet{‚Äòs‚Äô: ‚ÄòWhiteHouse‚Äô,‚Äòp‚Äô: ‚Äòarchitectural ExampleJSON
style‚Äô,‚Äòo‚Äô: ‚ÄòNeoclassicalarchitecture‚Äô},your {"prompt": "{}heldthepositionof",
answer should be in JSON format like {‚Äòs‚Äô: "subject": "DonaldTrump",
‚ÄòWhiteHouse‚Äô,‚Äòp‚Äô: ‚Äòarchitecturalstyle‚Äô,‚Äòo‚Äô: "target": "president of the Constitutional
‚ÄòNeoclassicalarchitecture‚Äô,‚Äôprompt‚Äô: [‚ÄòWhite CourtofSpain"}
Houseisdesignedinthearchitecturalstyleof Response
‚Äô,‚ÄòTheWhiteHouseshowcasesthedistinctive ImaginethatDonaldTrumphadheldthepo-
architecturalstyleof‚Äô,‚ÄòWhendiscussingthe sitionofpresidentoftheConstitutionalCourt
architecturalstyleoftheWhiteHouse,oneim- ofSpain.
mediatelythinksof‚Äô]}. Youneedtoreturnthe
datadirectlyinJSONformat,withoutsaying Table5: ExampleofprefixpromptgenerationforICE.
anythingelse. Thistime,thetripletIprovide
is{‚Äòs‚Äô:‚Äò‚Äô,‚Äòp‚Äô:‚Äò‚Äô,‚Äòo‚Äô:‚Äò‚Äô}.
ExampleTriplet A.2 ModelSelection
{"s": "Washington,D.C.",
Due to limitation of computation resources, we
"p": "sharesborderwith",
performexperimentsonGPT2-XL(Radfordetal.,
"o": "Virginia"}
Response 2019). GPT-2XListhe1.5Bparameterversionof
{ GPT-2, a transformer-based language model cre-
"s": "Washington,D.C.", atedandreleasedbyOpenAI.Themodelisapre-
"p": "sharesborderwith",
trainedmodelonEnglishlanguageusingacausal
"o": "Virginia",
language modeling (CLM) objective. The entire
"prompt": [
ROMEedittakesapproximately2sonanNVIDIA
"Washington, D.C. is known for sharing its
borderwith", A6000GPUforGPT2-XL.MEMITtakes3226.35
"A key geographical feature of Washington, sec‚âà0.90hrfor10,000updatesonGPT-J.
D.C.isitsborderwith",
"DiscussingthebordersofWashington,D.C., A.3 Implementationdetails
onecommonlymentionsitsadjacencyto",
FT / FT+L For basic Fine-Tuning (FT), we fol-
"AnimportantaspectofWashington,D.C.‚Äôs
locationisitssharedborderwith", low(Mengetal.,2022)re-implementationintheir
"Inthecontextofregionalboundaries,Wash- study,usingAdam(M√ºlleretal.,2022)withearly
ington,D.C.isnotablyadjacentto"] stopping to minimize ‚àílogP [o‚àó|p], changing
G‚Ä≤
}
onlymlp weightsatselectedlayer1. Weusea
proj
learningrateof5√ó10‚àí4 andearlystopata0.03
Table4:Exampleofpromptgenerationbasedonagiven
loss.
tripletfordatasetconstruction.
For constrained fine-tuning (FT+L) (Zhu
et al., 2020), we add an L norm constraint:
‚àû
In the construction of our dataset, we utilize ‚à•Œ∏ ‚àíŒ∏ ‚à• ‚â§ œµ. Thisisachievedinpracticeby
G G‚Ä≤ ‚àû
GPT4 to generate prompts that integrate specific clampingweightsŒ∏ totheŒ∏ ¬±œµrangeateach
G‚Ä≤ G
subjects with their corresponding predicates. As gradientstep. Weselectlayer0andœµ = 5√ó10‚àí4.
illustratedinTab.4,thismethodensuresthequality
The learning rate and early stopping conditions
andfluencyofourdata.
remainfromunconstrainedfine-tuning.
We also utilize GPT4 to generate ICE prefix MEND(Mitchelletal.,2022a)learnarank-1de-
prompts. Tab.5showsanexample. compositionofthenegativeloglikelihoodgradientwithrespecttosomesubsetofŒ∏ . Hyperparame- incorporated codes that are distributed under the
G
tersareadoptedfromgivendefaultconfigurations. terms of the MIT License 1. It significantly bol-
ROME(Mengetal.,2022)conceptualizesthe stered our research, enabling us to focus on the
MLPmoduleasastraightforwardkey-valuestore. novelcontributionsofourworkwithouttheneces-
We directly apply the code and MLP weight pro- sityofdevelopingfoundationalcomponentsfrom
vided by the original paper and keep the default scratch. We extend our profound gratitude to the
setting for hyperparameters. We perform the in- originalauthorsfortheirinvaluablecontributions
terventionatlayer18andcovariancestatisticsare totheopen-sourcecommunityandaffirmourcom-
collectedusing100,000samplesofWikitext. mitmenttoadheringtothestipulationsoftheMIT
MEMIT(Mengetal.,2023)buildsuponROME License.
to insert many memories by modifying the MLP
weights of a range of critical layers. We test the
abilityofMEMITusingtheircodeandallhyper-
parameters follow the same default settings. For
GPT2-XL,wechooselayers= [3,4,5,6,7,8].
ICE (Cohen et al., 2023) does not introduce
changestothemodelparameters,butprependthe
followingprefixtotheinputprompt: ‚ÄúImaginethat
<O‚àó> would have been <P >‚Äù. The prompts are
r
generatedusingGPT4. SeeTab.5foranexample.
Duetoinputlengthconstraints,weconductedex-
perimentswitheditamountssetto[1,2,3,5,8,10].
SORAre-editthetopKoutliers. WeuseMEMIT
toprefomre-editing. Allhyperparametersfollow
the same default settings with MEMIT. We con-
ductedexperimentswithKsetto[5,10].
A.4 Othermetrics
We performed experiments utilizing alternative
metrics. Fig. 5 shows the detailed results. This
set of bar graphs presents results across two dif-
ferent sampling strategies: Breadth-First Search
(BFS)andRandomsampling. Withineachgraph,
modeleditingmethodsarecompared. Thebarsare
groupedbythenumberofedits,rangingfrom1to
200,witheachgroupcolor-codedforclarity. The
heightofthebarscorrespondstothemetric‚Äôsvalue
onalogarithmicscale. InthePPLgraphs,thehori-
zontallinerepresentstheaveragePPLofthedataset
beforemodelediting. InthecomputationofBLEU
and ROUGE metrics, the text generated by post-
editmodelisemployedasthePredictions,whereas
thetextgeneratedbytheoriginalmodelservesas
theReferences. Thisfacilitatesacomparativeanal-
ysisofthediscrepanciesbetweenthepre-editand
post-editoutputs. Followingthecomparativeeval-
uation of these metrics, we have selected PPL as
themetricofchoiceforourexperiment.
A.5 License
Inthecourseofdevelopingthemethodologiesand
implementationsdetailedwithinthisstudy,wehave 1https://github.com/kmeng01/memitFigure5: Perplexity,BleuandRougescore.