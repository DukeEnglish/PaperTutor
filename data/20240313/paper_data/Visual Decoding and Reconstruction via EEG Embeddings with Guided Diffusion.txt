Visual Decoding and Reconstruction via EEG
Embeddings with Guided Diffusion
Dongyang Li∗1, Chen Wei∗1, Shiying Li1, Jiachen Zou1, and Quanying Liu1
1 Southern University of Science and Technology, Shenzhen 518055, China liuqy@sustech.edu.cn
Abstract. How to decode human vision through neural signals has attracted a long-
standinginterestinneuroscienceandmachinelearning.Moderncontrastivelearningand
generative models improved the performance of fMRI-based visual decoding and recon-
struction.However,thehighcostandlowtemporalresolutionoffMRIlimittheirappli-
cationsinbrain-computerinterfaces(BCIs),promptingahighneedforEEG-basedvisual
reconstruction.Inthisstudy,wepresentanEEG-basedvisualreconstructionframework.
Itconsistsofaplug-and-playEEGencoder calledtheAdaptiveThinkingMapper(ATM),
whichisalignedwithimageembeddings,andatwo-stageEEGguidanceimagegenerator
that first transforms EEG features into image priors and then reconstructs the visual
stimuli with a pre-trained image generator. Our approach allows EEG embeddings to
achieve superior performance in image classification and retrieval tasks. Our two-stage
image generation strategy vividly reconstructs images seen by humans. Furthermore,
we analyzed the impact of signals from different time windows and brain regions on
decoding and reconstruction. The versatility of our framework is demonstrated in the
magnetoencephalogram(MEG)datamodality.WereportthatEEG-basedvisualdecod-
ingachievesSOTAperformance,highlightingtheportability,lowcost,andhightemporal
resolution of EEG, enabling a wide range of BCI applications. Our code is available at
https://github.com/dongyangli-del/EEG_Image_decode.
Keywords: EEG · Visual reconstruction · Neural alignment · MEG
1 Introduction
A key technical challenge in brain-computer interfaces (BCIs) is to decode/reconstruct the
visualworldseenbyhumansthroughnon-invasivebrainrecordings,suchasfunctionalmagnetic
resonanceimaging(fMRI),magnetoencephalography(MEG)orelectroencephalography(EEG).
These highly dynamic brain activities reflect human perception of the visual world, which is
influenced by properties of the external visual stimulus, our internal states, emotions and even
personal experiences. Thus, visual decoding and reconstruction based on neural signals can
uncoverhowthehumanbrainprocessesandinterpretsnaturalvisualstimuli,aswellaspromote
non-invasive BCI applications.
Contrastive learning and generative models have greatly advanced fMRI-based visual de-
coding in both decoding tasks (e.g., image classification and retrieval) and generative tasks
(e.g., image reconstruction). By combining pre-trained visual models, existing fMRI decoding
models can learn highly-refined feature embeddings in limited data [28, 40]. Using these em-
bedded fMRI features, generative models such as diffusion models can reconstruct the image
one is seeing [40, 12]. However, despite many advances in fMRI-based visual decoding, fMRI
equipment is unportable, expensive, and difficult to operate, largely limiting its application in
∗
D. Li and C. Wei contributed equally.
4202
raM
21
]CH.sc[
1v12770.3042:viXra2 D. Li, C. Wei et al.
EEG
a
...
Classification
Relax ＋ ...
Retrieval
Rest ＋ Visual Visual MEG
t1 t2 t3 t3 Generation
b Classification Classification c Classification Classification
(Across Subject top-5) (Across Subject top-1) (Across Subject top-5) (Across Subject top-1)
Retrieval Classification Retrieval Classification
(In Subject top-1) (In Subject top-5) (In Subject top-1) (In Subject top-5)
Retrieval Classification Retrieval Classification
(In Subject top-5) (In Subject top-1) (In Subject top-5) (In Subject top-1)
Retrieval Generation Retrieval Generation
(Across Subject (top-5) (Across Subject (top-5)
top-1) top-1)
Retrieval Generation Retrieval Generation
(Across Subject top-5) (top-1) (Across Subject top-5) (top-1)
ATM-S (Ours) EEGITNet B.D. ATM-S (Ours) B.D. MLP
ATCNet EEGNetV4 NICE Conformer NICE ShallowFBCPNet
Conformer MLP ShallowFBCPNet EEGNetV4
Fig.1. EEG/MEG-based visual retrieval, classification, and reconstruction tasks. (a)
OverviewofthreevisualdecodingtasksusingEEG/MEGdataundernaturalimagestimuli.(b)Com-
parisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject
performance. (c) Comparisons on the THINGS-MEG dataset, similar to (b). Our ATM-S encoder
achieves the highest performance compared to other competing encoders in EEG/MEG-based visual
decoding tasks.
BCIs. Alternatively, EEG is portable, cheap, and universal, facilitating a wide range of BCI
applications. EEG has higher temporal resolution and can effectively capture rapid changes in
brain activity when processing complex, dynamic visual stimuli.
However, EEG has long been considered incomparable to fMRI in natural image decod-
ing/reconstruction tasks, as EEG suffers from low signal-to-noise ratio, low spatial resolu-
tion, and large inter-subject variability. Recent advances in multimodal alignment have made
MEG/EEG visual decoding possible, although the performance is still inferior to fMRI [8, 46,
16]. Yohann Benchetrit et al. used the CLIP model to extract the latent representation of
the image and trained the MEG encoder to align it with the image representation extracted
by CLIP. It achieved excellent retrieval and reconstruction performance on MEG and fMRI
datasets, demonstrating the potential for real-time visual decoding and reconstruction using
EEG/MEG signals. Recently, Song et al. [46] used an EEG encoder based on ShallowNet [37]
and performed representation alignment through contrastive learning, achieving excellent de-
coding performance on the THING-EEG dataset [14]. These two studies provide preliminary
evidence of the potential of EEG/MEG-based visual decoding. However, there is a significant
gap in their performance compared to the fMRI-level performance. This gap is largely caused
by weaknesses in its EEG encoder, which prevents EEG embeddings from effectively aligning
with image embeddings.
To fill in the gap, we develop an EEG/MEG-based visual decoding framework, including a
novel EEG encoder, called Adaptive Thinking Mapper (ATM), and a two-stage image genera-Visual Decoding and Reconstruction 3
Visual stimuli Training X Contrastive Trainable Frozen
Image Learning
Embedding Image Dataset
CLIP Classification
Image …
X Retrieval
EEG
Embedding
× ×
EEG 푻 푻
Encoder
EEG
Noise Prior diffusion embedding IP-Adapter
Inference
EEG + Stable Diffusion
Embedding
EEG 2-Stage
Encoder Generation Generation Reconstruction
Fig.2. EEG/MEG-based visual decoding and generation framework. The EEG encoder is
designed as a flexible replacement component. After aligning with image features, the EEG features
are used for zero-shot retrieval and classification tasks, and the reconstructed images are obtained
through a two-stage generator.
tion strategy. We survey existing EEG encoder modules, such as EEGNetV4 [25], ShallowNet
[37], and Conformer [44], and demonstrate through extensive comparative experiments and
ablation studies that our method achieves state-of-the-art (SOTA) performance on image clas-
sification, retrieval, and generation tasks. Our work has three main contributions:
– WeemploytheattentionmoduleandaspatiotemporalconvolutionmoduleinEEGencoder,
which can be used in a plug-and-play manner, to extract representations in real-time for
both EEG data and MEG data.
– Ourbrainencoderistrainedwithself-supervisedcontrastivelearningframework,achieving
SOTA performance on EEG and MEG datasets from natural image decoding tasks.
– Wepresentatwo-stageimagegenerationstrategytofacilitatetheadjustmentofthedistri-
butionfromtheoriginalEEGdatatothecorrespondingimages.Itemploysapriordiffusion
model that is conditioned on the EEG embeddings to generate the image embeddings, fol-
lowed an enhanced Stable Diffusion module to generate images with EEG priors.
2 Method
To learn high-quality latent representations of EEG data, it is crucial to consider the spatial
positionofEEGchannelsandthespatiotemporalpropertiesofEEGsignals.LetT representthe
lengthofthetimewindowofthedata,C thenumberofEEGchannels,andN thetotalnumber
of data samples. Our objective is to derive EEG embeddings Z = f(E) ∈ RN×F from the
E
brain activity data E ∈RN×C×T, where f is the EEG encoder and F is the feature dimension
of the embeddings. Concurrently, we use the CLIP model to extract image embeddings Z ∈
I4 D. Li, C. Wei et al.
Flatten Embed￿ding MLP Projectior Add & LayNorm
Temporal Spatial Aggregation Linear Projection
TSConv Plug-and-Play Conv Modul￿e
Encoding
ELU
GELU
Input Layer
￿
CNN & Norm
Linear Projection
...
Position ￿
Input Embedding Encoding
Flatten and Linear Projection
Patch tokens
Position embed
FPz FP1 ... O2 Oz P7 Cls token
￿
4
Fig.3.The structure of EEG encoder ATM.AccordingtothepositionofEEGchannelsandthe
spatiotemporal property of EEG signals, we design the position encoding and temporospatial encoding
in ATM.
RN×F from images I. Our goal is to effectively align the EEG representation with the image
representation, as illustrated in Fig. 2. In the training phase, the EEG encoder is trained with
EEG and image pairs using a contrastive learning framework. In the inference phase, the EEG
embeddings from the trained plug-and-play EEG encoder can be used for a variety of tasks,
including EEG-based image classification, retrieval, and EEG-guided image generation.
2.1 ATM for EEG embedding
We develop an EEG encoder, called Adaptive Thinking Mapper (ATM), for aligning the origi-
nal EEG signals to its feature representation space (Fig. 3). ATM is based on the Transformer
Encoderandspatiotemporalconvolutionarchitecture.Specifically,weutilizeaone-dimensional
linear layer to project the input one-dimensional EEG data to the embedding dimension re-
quiredbytheTransformermodel.Theseinputsareprocessedthroughaself-attentionmoduleto
integrate the embeddings of input data and positional encoding. We first divide the input into
fixed-size patches, with each image patch containing a fixed number of data points. Then, each
patch is mapped to a high-dimensional embedding space through a linear layer. Subsequently,
throughthetemporalaggregationprocess,weobtaintheprocessedembeddingrepresentations.
Notably, ATM addresses the inadequacies of other modules in modeling capabilities on the
temporal scale through the Input Layer. For it has been reported that the spatiotemporal con-
volutionmodulewithalargeconvolutionkernelisaneffectivewaytorepresentEEGdatawith
asmallnumberofparameters[46],weuseasimilarspatiotemporalconvolution(STConv)-based
feature extraction module. The difference is our STConv Module is plug-and-play and can be
flexibly replaced with different types of spatiotemporal convolution components as needed to
adapttovariousEEG/MEGdatasets.Finally,MLPprojectionlayerconsistsofM simpleresid-
ual components and fully connected layers, with LayerNorm applied in the output to ensure
the stability of training.Visual Decoding and Reconstruction 5
2.2 Image Embedding
Many previous studies have explored various training strategies to train deep neural networks
for image embedding, such as VGG-19 and ResNet trained with supervised learning, CLIP
trained with contrastive learning, and VAEs with self-supervised learning [49, 4, 46, 16]. They
have reported that CLIP models pre-trained using the Vision Transformer (ViT) architecture
perform better in a range of downstream tasks, including image decoding and reconstruction,
comparedtomodelstrainedusingsupervisedlearningmethods(suchasVGG,ResNet)andself-
supervised VAE frameworks. Thus, in this study, we use CLIP for image embedding and align
theCLIP’soutputimagerepresentation,denotedasZ ∈RN×1024,withtheEEGembeddings.
I
Before formal training, all images undergo standard preprocessing [34].
2.3 EEG guidance image generation
In this study, we present a two-stage pipeline for generating images that serve as visual stimuli
forEEGrecordings,asshowninthebottomrightofFig.2.IntheleftofFig.3wehaveobtained
the EEG embeddings z for each image by the EEG encoder ATM. Now our goal is to use
E
theseEEGembeddingstogeneratethecorrespondingimages.Thejointdistributionofimages,
EEG embeddings, and image embeddings can be expressed as p(I,z ,z ) = p(z |z )p(I|z ),
E I I E I
corresponding to the prior diffusion and CLIP-guided generation, respectively. In Stage I, we
firstfocusonthepriordiffusionstage.InspiredbyDALL-E2[35]andMind’sEyes[40],wetrain
a diffusion model conditioned on the EEG embeddings Zˆ to learn the distribution of CLIP
E
embeddings p(z |z ). In this stage, we construct a lightweight U-Net: ϵ (zt,t,z ), where zt
I E prior I E I
representsthenoisyCLIPembeddingatdiffusiontimestept.Wetrainthepriordiffusionmodel
usingEEGandCLIPembeddings.Throughthisdiffusionmodel,wecangeneratecorresponding
CLIP embeddings z from EEG embeddings as a prior for stage II. In Stage II, we employ
I
the pre-trained SDXL [33] and IP-Adapter [59] models to model the generator p(I|z ), thereby
I
sampling image I according to z . This stage mainly focuses on converting CLIP embeddings
I
into corresponding images. Further details are provided in Appendix C.
2.4 Loss Function
Following the methodology outlined by Benchetrit et al. [4], we adopt a dual approach to loss
functions, serving distinct objectives. For the classification and retrieval tasks, we only utilize
the CLIP loss, which is inspired by the contrastive learning approach described in Radford et
al. [34]. This loss function aids in aligning the EEG data E with corresponding image data I,
therebyfacilitatingtheidentificationofEEG-imagepairsandmaximizingtheEEGclassification
boundaries. For the generation tasks, besides the CLIP loss, we add a Mean Squared Error
(MSE) loss to facilitate learning in regression scenarios. Thus the overall loss function for our
model is a combination of these two distinct loss types, expressed as:
Loss=λ·L +(1−λ)·L
CLIP MSE
Here, λ is a hyperparameter that balances the contribution of each loss type.
3 Experiments
3.1 Training and computational considerations
We conducted our experiments on the THINGS-EEG dataset’s training set [14, 16]. To verify
the versatility of ATM for embedding electrophysiological data, we tested it on MEG data6 D. Li, C. Wei et al.
modality using the THINGS-MEG dataset [18]. We used the Adam optimizer [24] to train
the across-subject model on a set of approximately 496,200 samples, and the within-subject
model on a set of about 66,160 samples, with a learning rate of 3×10−4 and batch sizes of 16
a b
EEG
Embedding
Text
Categories Embedding
antelope
pear CLIP Cosine
Classification
bassoon Image Similarity
kettle
sausage
…
Fig.4. EEG-based image classification. (a) The paradigm of EEG-based image classification. (b)
Average accuracy across different methods in the subjects.
a EEG b
Embedding
Cosine
Image Dataset Image Similarity
Embedding
…
c d
Fig.5. EEG-based image retrieval. (a) The paradigm of EEG-based image retrieval. (b) Average
accuracyacrossdifferentmethodsinthesubjects.(c)Imagesshowingthetop-5accuracyinEEG-image
retrieval tasks. See Appendix E for additional images results. (d) Representation similarity analysis.
Clustering of image features and EEG features using a k-means algorithm with k=5 is performed, fol-
lowedbythecalculationofsimilaritybetweenclusters.SeeAppendixDforadditionalRepresentational
similarity results.Visual Decoding and Reconstruction 7
and 1024. Our initial temperature parameter was set to 0.07. We tested on the zero-shot test
dataset at the end of each training epoch during the training process. For fairness, all models’
hyperparameters were kept consistent. In our study, we compared the performance of different
encoders on the within-subject test set and cross-subject (leave-one-subject-out) test set (see
Appendix F).
3.2 EEG Decoding performance
The plug-and-play ATM can obtain the EEG embedding for the classification task using a
simplecosinesimilaritymeasurement.Weoutputthecategorywiththehighestcosinesimilarity
(Fig.4a).MoredetailsoftheEEG-basedimageclassificationareinAppendixB.Fig.4bpresents
theaccuracyofEEGclassificationforeachsubject,andFig.4cshowsourmethodoutperforms
others.
Here, we test the effectiveness of ATM-extracted EEG embeddings in the image retrieval
task. Fig. 5a shows the image retrieval process. We calculate the cosine similarity between the
extracted EEG embeddings and the CLIP embeddings of the image dataset (with 200 images),
and output the image with the highest similarity as the retrieved image. Fig. 5b shows the
Top-5 retrieved images corresponding to the real visual stimuli seen by subjects. To be a fair
comparison, here we presented the images that the previous method failed to retrieve in Top-
1 for testing [46]. Compared with the previous model, the Top-1 accuracy of our model is
significantly improved, and the Top-5 images all maintain a high degree of similarity with the
original images. Fig. 5c shows our method outperforms others.
Ablation study on ATM We systematically deconstructed and analyzed each layer of our EEG
encoder module. We conducted an ablation study for each component in ATM (i.e., the MLP
projector, the adaptive convolution module, the subject-wise linear, and the spatial attention
block). Notably, ATM is a flexible, plug-and-play module, which can be easily replaced with
any formofconvolutionalcomponent oreven omitted.AppendixB.3 showed thatthe adaptive
convolutional components significantly enhanced encoder performance, while the subject-wise
linear layer reduced the model’s training consumption and substantially improved the existing
components’ capabilities. Surprisingly, our spatial attention module based on sine-cosine posi-
tionalencodingimprovedencoderperformanceonlywhenusedalone,butthebenefitswerenot
as significant when combined with the subject-wise linear layer.
3.3 Image Generation performance
Using the image generation task, we verify the effectiveness of ATM’s EEG embedding and
our two-stage generation model. Fig. 6a shows the process of generating images under the
guidance of EEG embedding and evaluating the quality of the generated images. For one test
image, we put EEG embedding of the subject viewing the image into the two-stage generator
to guide the generation of the image. To evaluate the generation performance, we conducted
an image retrieval task. Specifically, we extract the CLIP embedding of the generated images
and calculate the similarity between the CLIP embeddings of all images in the image dataset
to retrieve the generated image.
Fig.6bshowsthegenerationeffectofthesampleimage.Fig.6cshowstheretrievalaccuracy
using generated images. The generated images have high semantic similarity with the seen
images and have good diversity in low-level visual features, which can be manipulated by the
guidance scale hyperparameter (Fig. 6d). Surprisingly, the retrieval accuracy using generated
images is even higher than that using EEG embeddings, implying the benefit of our two-stage8 D. Li, C. Wei et al.
visual reconstruction for image retrieval (See Appendix C for Figure A2). We also report the
decoding and reconstruction performance for EEG, MEG, and fMRI across various metrics in
the Appendix D.
We visualize the best, medium and worst generated images in Fig. 7. We randomly selected
theEEGdataofasubjectviewing100images,andextractedEEGembeddingstoguideimage
generation. By calculating the cosine similarity of the CLIP embedding between the generated
image and the original image, we found 12 images each with the best, medium and worst
generation effects. It can be seen that in the best group, the generated image is not only
highly consistent with the semantics of the original image, but also well retains the low-level
visual features. in the medium group, the generated image maintains the semantic features of
the original image, and the low-level visual features are well preserved. Visual features were
altered. in the worst group, both semantic features and low-level visual features were altered.
3.4 Temporal analysis
To investigate the effects of EEG time window on visual decoding accuracy, we calculated the
average top-1 classification accuracy for two different time windows: [0, t], including the entire
period from the onset of visual stimuli to time point t, and [t-100, t], only including the data
100msbeforetimepointt.Wecomparedtheaccuracywitharandomlyselectedbaseline(0.5%
chancelevel)totestnon-randompredictiveperformance(Fig.8).Ourresultsshowthatwithin
500ms after visual stimulus presentation, the EEG signal decoding accuracy reaches an upper
a EEG b Distribution of
Embedding
Similarity
2-Stage
Generation
Image Image
Embedding Embedding
Cosine
… Similarity
c d Similarity and diversity for
Seen Generated different guidance scale
Fig.6.EEGguidanceimagegeneration.(a)TheparadigmofEEGguidanceimagegeneration.(b)
The similarity between random visual objects and the EEG embeddings, and the similarity between
generated visual objects and the target EEG embeddings. (c) Comparison between the original image
and the image generated using the corresponding EEG data. (see Appendix B & C for details). (d)
The similarity between visual objects and target EEG embeddings as the guidance scale changes, and
the diversity of visual objects as the guidance scale changes. See Appendix E for additional results.Visual Decoding and Reconstruction 9
limit of about 30%, after which the accuracy no longer improves (Fig. 8a). The MEG decoding
shows a similar profile as the time window expands (Fig. 8b). We exhibit the generated images
under different EEG time windows, [0, t] in Fig. 8c. The similarity between the generated
images and the original images is low when the time window is less than 150ms, then this
similarity gradually increase as the time window expands. After 500 milliseconds, EEG-guided
image generation can reliably reveal the semantics of the images seen, such as aircraft carriers,
modems, jelly beans, et al. Interestingly, we find differences in the optimal reconstruction time
windows for different categories of images, for example, jelly beans (200ms) are faster than
aircraft carrier (500ms), implying that the human brain may process different visual objects
at different speeds. This finding highlights the advantage of EEG’s high temporal resolution in
studying fast visual processing compared with the lower temporal resolution of fMRI.
3.5 Spatial analysis
To examine the contribution of different brain regions to visual decoding, we divided the EEG
electrodes from the THING-EEG data into five distinct brain regions (i.e., Frontal, Temporal,
Center, Parietal, Occipital regions in Fig. 9a), and then conducted ablation experiments on
retrieval task (Fig. 9b) and the reconstruction task (Fig. 9c). The results showed that using
information from all brain regions is optimal, for both retrieval and generation tasks. The
occipital had the highest retrieval accuracy and reconstruction performance compared to other
regions. Parietal and temporal regions contain some semantic information, whereas frontal and
central regions contribute the least useful information to the visual decoding.
Fig.7. Examples of EEG-guided visual reconstruction. From top to bottom, we exhibit the
best, median, and worst 12 generated images, respectively. We show the images subjects seen and the
generated images by our two-stage image generator. See Appendix E for additional results.
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneG10 D. Li, C. Wei et al.
a Average top-1 accuracy across subjects (EEG) c Seen 50ms 100ms 150ms 200ms 250ms 300ms 500ms 800ms 1000ms
1000ms
b Average top-1 accuracy across subjects (MEG)
Fig.8. Effects of different EEG/MEG time windows on EEG-guided visual retrieval and
reconstruction. (a) The retrieval accuracy of the expanding EEG windows at intervals [0, t] and at
intervals[t-100,t]respectively.(b)TheretrievalaccuracyoftheexpandingMEGwindows.(c)Images
reconstructed by EEG as the EEG window expands. When the EEG time window is greater than
200ms, the reconstructed image is reliable.
a c
Division of electrode Seen Frontal Central Parietal Temporal Occipital All
regions
b
Fig.9. EEG-guided visual retrieval and reconstruction using EEG signals from different
brain regions. (a) The EEG electrodes assigned to five brain regions. (b) Top-1 and top-5 retrieval
accuracy,usingonlytheEEGchannelsineachleavedregionandallchannels.(c)Reconstructedimages
obtained using only the electrode channels in each individual region and all channels.
4 Related works
Visual decoding using neural signal: Decoding visual information from our brain has
been a long-standing pursuit in neuroscience and computer science [30, 20]. Some progress hasVisual Decoding and Reconstruction 11
been made in decoding steady-state visual stimuli. However, accurately and rapidly decoding
semanticinformationinnaturalimagesremainsachallenge[42].fMRIhasbeenwidelyusedto
estimatesemanticandshapeinformationinvisualprocessingwithinthebrain[48,19].However,
the demand for high-speed and practical applications in brain-computer interfaces calls for
alternative approaches. EEG, due to its high temporal resolution and portability, emerges as
a promising option [53]. Yet, the overall performance across different subjects and biological
plausibilityremainsunresolved[1].Furthermore,previousapproachesoftenreliedonsupervised
learning methods with limited image categories, overlooking the intrinsic relationship between
image stimuli and brain responses [28, 41, 27].
Neural decoding for EEG signal:Previousstudieshaveshowntheefficacyofspatiotem-
poral modules in representing neural data [37, 25]. For example, lightweight convolutional neu-
ral networks such as EEGNet and ShallowNet [37] have achieved considerable performance
in small EEG and MEG datasets. Using contrastive learning, it has been shown that merely
usingconvolutionalneuralnetworksandprojectionlayerscanyieldsatisfactoryresultsonneu-
ral datasets [6]. More recently, Benchetrit et al proposed a B.D. encoder for MEG embedding,
allowing real-time MEG-based reconstruction of visual perception [4]. Song et al. presented
an EEG encoder using ShallowNet spatiotemporal convolution module with a large convolu-
tion kernel with a few parameters for EEG embedding, resulting in favorable performance on
EEG-based visual decoding [46].
Limitations of previous studies: Previous EEG studies are primarily oriented toward
understanding visual perception in the human brain rather than maximizing EEG decoding
performance. Thus the visual decoding performance is far from optimal. Specifically, previous
studies have trained linear models to (1) classify a small set of images from brain activity
[15, 23], (2) to predict brain activity from the latent representations of images [8], or (3)
to quantify the similarity analysis between these two patterns with representational similarity
[8,15,14,3].Whilethesestudiesalsoutilizeimageembeddings,theirlineardecodersarelimited
toclassifyingasmallgroupofobjectcategoriesordistinguishingimagepairs.Moreover,several
deepneuralnetworkshavebeenappliedtomaximizeclassificationofspeech[10],cognitiveload
[21],andimages[32,29,2]inEEGrecordings.[32]proposedadeepconvolutionalneuralnetwork
for classifying natural images using EEG signals. Unfortunately, the experiment presented all
images of the same category in a single block, probably misleading the decoder to rely on
autocorrelated noise rather than the hidden informative patterns of brain activity [27]. Also,
these EEG studies only classify a relatively small number of image categories.
5 Discussion and Conclusion
In this study, we developed an EEG-based image decoding and reconstruction framework. Em-
ployingself-supervisedlearning,ourmethodenablesthemodeltoachievebettergeneralization
performance in different tasks. To our knowledge, this is the first instance of EEG decoding
achieving end-to-end image reconstruction with fMRI-level performance.
Technical Impact: Our technical contributions are mainly on the EEG encoder and the
two-stage image generator (Fig. 2). First, we developed the ATM, a plug-and-play EEG en-
coderwhichcanefficientlyextractEEG/MEGfeaturesforthethreevisualdecodingtasks.Our
comprehensiveexperimentsoftheEEGencoder(Fig.3),comparedtovariousarchitecturesand
trainingmethods,achievesSOTAperformanceacrossvariousmetricsandtasks(Figs.1b,4,5).
Second, the two-stage EEG guidance image generation achieves performance close to fMRI
using only EEG data (Figs. 6, 7, Table A3,A4), and this method is compatible with MEG
data (Figs. 1c, 8b). By comparing EEG embeddings and image embeddings obtained through
priordiffusion,wedemonstratedthatEEGembeddingsobtainedviacontrastivelearningalone12 D. Li, C. Wei et al.
were insufficient for generating credible images (Appendix C), suggesting that our two-stage
generation strategy could effectively enhance generation performance even with limited data.
Neuroscience Insights: Our results offer insights into the relationship between brain
activity and visual perception. We analyzed EEG-based visual decoding within different time
windows to examine when visual information is perceived in the brain (Fig. 8). Our results
revealedthatvisualinformationinEEGdataispredominantlycontainedwithinthe200-400ms
range (Fig. 8a), consistent with previous EEG studies [49, 16, 46]. Interestingly, the visual
information in MEG data last up to 800ms, much longer than EEG (Fig. 8b), in line with the
results reported by a previous MEG study [4, 46]. We also found that EEG performs better
than MEG in visual tasks (See Appendix D for Table A4), which is different from other fields,
such as speech decoding [10]. In addition, through ablation experiments of spatial information,
we found that visual information is mainly encoded in the occipital and parietal areas (Fig. 9).
Using the image reconstruction tasks, we can visualize the information encoded in the brain,
providing a window for vision researchers to explore how humans perceive visual information.
Interesting Phenomena and Future Directions: Our study uncovered intriguing phe-
nomena,shedinglightsonthefuturedirectionofEEGdecoding.First,therearenon-negligible
performance differences between cross-subject and within-subject settings. This performance
gap arises from inter-subject differences in EEG signals [13, 58], likely attribute to heterogene-
ity in individual brain, differences in visual perception between individuals, and even shifts
in noise distribution during EEG recording. To address this cross-subject challenge, it calls
for more efforts on EEG encoder, such as more flexible neural network architectures or train-
ing with larger EEG dataset. Transfer learning and meta-learning are also future directions
worth exploring [55, 54, 57]. On the other hand, large EEG models pre-trained on massive
EEG datasets may be the ultimate solution for visual decoding tasks [9, 7]. Nevertheless, how
to unify various electrode montages of different EEG datasets when pre-training large EEG
modelsisachallenge.EEGsourcelocalization,whichconvertssenor-levelEEGsignalsintothe
standard brain source space [52, 50], might be a potential solution. Moreover, our EEG-based
image reconstruction results are somewhat limited by the visual features used from CLIP. For
instance, it is easy to reconstruct semantics but difficult to preserve low-level visual feature
aspects. This limitation leads to generated images lacking detail and richness. Future studies
couldusemultipledifferentvisualfeaturestoalignEEGandimagedata.Thispropositionfinds
support in similar methodologies applied in fMRI-based studies [40, 26], which combine high-
level semantic features extracted by CLIP with low-level visual features from VAE to improve
consistency.
Acknowledgements
This work is supported by the National Key R&D Program of China (2021YFF1200804),
Shenzhen Science and Technology Innovation Committee (20200925155957004,
KCXFZ20201221173400001, SGDX2020110309280100).
ThisworkoriginatedasacourseprojectintheBI&AI2023courseatSUSTech,andtheau-
thorwouldliketothankcourseprojectmembers,FanYuanhao,WuZhihong,andQinHaoyang,
for their contributions to the early stages of this work.Bibliography
[1] Ahmed,H.,Wilbur,R.B.,Bharadwaj,H.M.,Siskind,J.M.:Objectclassificationfromran-
domized eeg trials pp. 3845–3854 (2021) 11
[2] Bagchi, S., Bathula, D.R.: Eeg-convtransformer for single-trial eeg-based visual stimulus
classification. Pattern Recognition 129, 108757 (2022) 11
[3] Bankson,B.B.,Hebart,M.N.,Groen,I.I.,Baker,C.I.:Thetemporalevolutionofconceptual
object representations revealed through models of behavior, semantics and deep neural
networks. NeuroImage 178, 172–182 (2018) 11
[4] BenchetritY,BanvilleH,K.J.R.:Braindecoding:towardreal-timereconstructionofvisual
perception. arXiv 2310, 19812 (2023) 5, 11, 12, 17, 22
[5] Caron,M.,Misra,I.,Mairal,J.,etal.:Unsupervisedlearningofvisualfeaturesbycontrast-
ingclusterassignments.Advancesinneuralinformationprocessingsystems33,9912–9924
(2020) 21
[6] Chen, T., Kornblith, S., Norouzi, M., et al.: A simple framework for contrastive learning
ofvisualrepresentations.In:Internationalconferenceonmachinelearning.pp.1597–1607.
PMLR (2020) 11
[7] Chen, X., Teng, X., Chen, H., Pan, Y., Geyer, P.: Toward reliable signals decoding for
electroencephalogram: A benchmark study to eegnex. Biomedical Signal Processing and
Control 87, 105475 (2024) 12
[8] Cichy, R.M., Pantazis, D.: Multivariate pattern analysis of meg and eeg: A comparison of
representational structure in time and space. NeuroImage 158, 441–454 (2017) 2, 11
[9] Cui,W.,Jeong,W.,Th¨olke,P.,Medani,T.,Jerbi,K.,Joshi,A.A.,Leahy,R.M.:Neuro-gpt:
Developing a foundation model for eeg. arXiv preprint arXiv:2311.03764 (2023) 12
[10] D´efossez,A.,Caucheteux,C.,Rapin,J.,Kabeli,O.,King,J.R.:Decodingspeechperception
from non-invasive brain recordings. Nature Machine Intelligence 5(10), 1097–1107 (2023)
11, 12
[11] Du,C.,Fu,K.,Li,J.,He,H.:Decodingvisualneuralrepresentationsbymultimodallearn-
ingofbrain-visual-linguisticfeatures.IEEETransactionsonPatternAnalysisandMachine
Intelligence (2023) 23
[12] Fang, T., Zheng, Q., Pan, G.: Alleviating the semantic gap for generalized fmri-to-image
reconstruction. In: Thirty-seventh Conference on Neural Information Processing Systems
(2023) 1
[13] Gibson, E., Lobaugh, N.J., Joordens, S., McIntosh, A.R.: Eeg variability: Task-driven or
subject-driven signal of interest? NeuroImage 252, 119034 (2022) 12
[14] Gifford,A.T.,Dwivedi,K.,Roig,G.,Cichy,R.M.:Alargeandricheegdatasetformodeling
human visual object recognition. NeuroImage 264, 119754 (2022) 2, 5, 11, 16
[15] Grootswagers,T.,Robinson,A.K.,Carlson,T.A.:Therepresentationaldynamicsofvisual
objects in rapid serial visual processing streams. NeuroImage 188, 668–679 (2019) 11
[16] Grootswagers, T., Zhou, I., Robinson, A.K., Hebart, M.N., Carlson, T.A.: Human eeg
recordingsfor1,854conceptspresentedinrapidserialvisualpresentationstreams.Scientific
Data 9(1), 3 (2022) 2, 5, 12, 16
[17] Guggenmos, M., Sterzer, P., Cichy, R.M.: Multivariate pattern analysis for meg: A com-
parison of dissimilarity measures. Neuroimage 173, 434–447 (2018) 16
[18] Hebart, M.N., Contier, O., Teichmann, L., Rockter, A.H., Zheng, C.Y., Kidder, A., Cor-
riveau,A.,Vaziri-Pashkam,M.,Baker,C.I.:Things-data,amultimodalcollectionoflarge-
scale datasets for investigating object representations in human brain and behavior. Elife
12, e82580 (2023) 6, 1614 D. Li, C. Wei et al.
[19] Ho,J.K.,Horikawa,T.,Majima,K.,Cheng,F.,Kamitani,Y.:Inter-individualdeepimage
reconstructionviahierarchicalneuralcodeconversion.NeuroImage271,120007(2023) 11
[20] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li,
Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy
text supervision. In: International conference on machine learning. pp. 4904–4916. PMLR
(2021) 10
[21] Jiao, Z., Gao, X., Wang, Y., Li, J., Xu, H.: Deep convolutional neural networks for mental
load classification based on eeg data. Pattern Recognition 76, 582–595 (2018) 11
[22] Karras,T.,Aittala,M.,Aila,T.,Laine,S.:Elucidatingthedesignspaceofdiffusion-based
generative models. Advances in Neural Information Processing Systems 35, 26565–26577
(2022) 19
[23] King,J.R.,Wyart,V.:Thehumanbrainencodesachronicleofvisualeventsateachinstant
oftimethroughthemultiplexingoftravelingwaves.JournalofNeuroscience41(34),7224–
7233 (2021) 11
[24] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014) 6
[25] Lawhern, V.J., Solon, A.J., Waytowich, N.R., et al.: Eegnet: a compact convolutional
neural network for eeg-based brain–computer interfaces. Journal of Neural Engineering
15(5), 056013 (2018) 3, 11
[26] Li, H., Wu, H., Chen, B.: Neuraldiffuser: Controllable fmri reconstruction with primary
visual feature guided diffusion. arXiv preprint arXiv:2402.13809 (2024) 12
[27] Li,R.,Johansen,J.S.,Ahmed,H.,Ilyevsky,T.V.,Wilbur,R.B.,Bharadwaj,H.M.,Siskind,
J.M.:Theperilsandpitfallsofblockdesignforeegclassificationexperiments.IEEETrans-
actions on Pattern Analysis and Machine Intelligence 43(1), 316–333 (2020) 11
[28] Liu, Y., Ma, Y., Zhou, W., Zhu, G., Zheng, N.: Brainclip: Bridging brain and visual-
linguistic representation via clip for generic natural visual stimulus decoding from fmri.
arXiv preprint arXiv:2302.12971 (2023) 1, 11
[29] McCartney, B., Devereux, B., Martinez-del Rincon, J.: A zero-shot deep metric learning
approachtobrain–computerinterfacesforimageretrieval.Knowledge-BasedSystems246,
108556 (2022) 11
[30] Miyawaki,Y.,Uchida,H.,Yamashita,O.,Sato,M.a.,Morito,Y.,Tanabe,H.C.,Sadato,N.,
Kamitani,Y.:Visualimagereconstructionfromhumanbrainactivityusingacombination
of multiscale local image decoders. Neuron 60(5), 915–929 (2008) 10
[31] Ozcelik,F.,VanRullen,R.:Naturalscenereconstructionfromfmrisignalsusinggenerative
latent diffusion. Scientific Reports 13(1), 15666 (2023) 22
[32] Palazzo, S., Spampinato, C., Kavasidis, I., Giordano, D., Schmidt, J., Shah, M.: Decoding
brain representations by multimodal learning of neural activity and visual features. IEEE
Transactions on Pattern Analysis and Machine Intelligence 43(11), 3833–3849 (2020) 11
[33] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Mu¨ller, J., Penna, J.,
Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis.
arXiv preprint arXiv:2307.01952 (2023) 5, 20
[34] Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,
A.,Mishkin,P.,Clark,J.,etal.:Learningtransferablevisualmodelsfromnaturallanguage
supervision.In:Internationalconferenceonmachinelearning.pp.8748–8763.PMLR(2021)
5
[35] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125 1(2), 3 (2022) 5
[36] Sauer,A.,Lorenz,D.,Blattmann,A.,Rombach,R.:Adversarialdiffusiondistillation.arXiv
preprint arXiv:2311.17042 (2023) 20Visual Decoding and Reconstruction 15
[37] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M., Eggensperger,
K., Tangermann, M., Hutter, F., Burgard, W., Ball, T.: Deep learning with convolutional
neural networks for eeg decoding and visualization. Human Brain Mapping 38(11), 5391–
5420 (2017) 2, 3, 11
[38] Schonfeld, E., Ebrahimi, S., Sinha, S., et al.: Generalized zero-and few-shot learning via
aligned variational autoencoders. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 8247–8255 (2019) 23
[39] Scotti, P., Banerjee, A., Goode, J., et al.: Reconstructing the mind’s eye: fmri-to-image
with contrastive learning and diffusion priors. Advances in Neural Information Processing
Systems 36 (2024) 22
[40] Scotti,P.S.,Banerjee,A.,Goode,J.,Shabalin,S.,Nguyen,A.,Cohen,E.,Dempster,A.J.,
Verlinde,N.,Yundler,E.,Weisberg,D.,etal.:Reconstructingthemind’seye:fmri-to-image
with contrastive learning and diffusion priors. arXiv preprint arXiv:2305.18274 (2023) 1,
5, 12
[41] Shen, G., Dwivedi, K., Majima, K., Horikawa, T., Kamitani, Y.: End-to-end deep image
reconstructionfromhumanbrainactivity.Frontiersincomputationalneuroscience13, 21
(2019) 11
[42] Shi, N., Li, X., Liu, B., Yang, C., Wang, Y., Gao, X.: Representative-based cold start for
adaptive ssvep-bci. IEEE Transactions on Neural Systems and Rehabilitation Engineering
31, 1521–1531 (2023) 11
[43] Shi, Y., Paige, B., Torr, P.: Variational mixture-of-experts autoencoders for multi-modal
deep generative models. Advances in neural information processing systems 32 (2019) 23
[44] Song, Y., Zheng, Q., Liu, B., et al.: Eeg conformer: Convolutional transformer for eeg
decoding and visualization. IEEE Transactions on Neural Systems and Rehabilitation En-
gineering 31, 710–719 (2022) 3
[45] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-based
generativemodelingthroughstochasticdifferentialequations.In:InternationalConference
on Learning Representations (2020) 19
[46] Song, Y., Liu, B., Li, X., Shi, N., Wang, Y., Gao, X.: Decoding natural images from eeg
for object recognition. arXiv preprint arXiv:2308.13234 (2023) 2, 4, 5, 7, 11, 12, 17
[47] Sutter, T.M., Daunhawer, I., Vogt, J.E.: Generalized multimodal elbo. In: International
Conference on Learning Representations (2020) 23
[48] Takagi,Y.,Nishimoto,S.:High-resolutionimagereconstructionwithlatentdiffusionmod-
elsfromhumanbrainactivity.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 14453–14463 (2023) 11
[49] Teichmann, L., Hebart, M.N., Baker, C.I.: Multidimensional object properties are dynam-
ically represented in the human brain. bioRxiv (2023) 5, 12
[50] Wang, S., Wei, C., Lou, K., Gu, D., Liu, Q.: Advancing eeg/meg source imaging with
geometric-informed basis functions. arXiv preprint arXiv:2401.17939 (2024) 12
[51] Wang,Z.,Bovik,A.C.,Sheikh,H.R.,etal.:Imagequalityassessment:fromerrorvisibility
to structural similarity. IEEE transactions on image processing 13(4), 600–612 (2004) 21
[52] Wei, C., Lou, K., Wang, Z., et al.: Edge sparse basis network: A deep learning framework
for eeg source localization. In: 2021 International Joint Conference on Neural Networks
(IJCNN). pp. 1–8. IEEE (2021) 12
[53] Willett, F.R., Avansino, D.T., Hochberg, L.R., Henderson, J.M., Shenoy, K.V.: High-
performance brain-to-text communication via handwriting. Nature 593(7858), 249–254
(2021) 11
[54] Wu, D., Jiang, X., Peng, R.: Transfer learning for motor imagery based brain–computer
interfaces: A tutorial. Neural Networks 153, 235–253 (2022) 1216 D. Li, C. Wei et al.
[55] Wu, D., Xu, Y., Lu, B.L.: Transfer learning for eeg-based brain–computer interfaces: A
review of progress made since 2016. IEEE Transactions on Cognitive and Developmental
Systems 14(1), 4–19 (2020) 12
[56] Wu,M.,Goodman,N.:Multimodalgenerativemodelsforscalableweakly-supervisedlearn-
ing. Advances in neural information processing systems 31 (2018) 23
[57] Xie,Y.,Wang,K.,Meng,J.,Yue,J.,Meng,L.,Yi,W.,Jung,T.P.,Xu,M.,Ming,D.:Cross-
datasettransferlearningformotorimagerysignalclassificationviamulti-tasklearningand
pre-training. Journal of Neural Engineering 20(5), 056037 (2023) 12
[58] Xu,L., Xu,M.,Ke,Y., An,X.,Liu, S.,Ming,D.: Cross-datasetvariabilityproblemin eeg
decoding with deep learning. Frontiers in human neuroscience 14, 103 (2020) 12
[59] Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compatible image prompt
adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023) 5, 20
A Datasets for experiments
A.1 EEG dataset
WeconductedourexperimentsontheTHINGS-EEGdataset’strainingset[14,16].Thisdataset
includes a large EEG corpus from 10 human subjects during the visual task. The experiment
employed the Rapid Serial Visual Presentation (RSVP) paradigm for orthogonal target de-
tection tasks to ensure that participants attended to the visual stimuli. All 10 participants
completed 4 equivalent experiments, resulting in 10 datasets with 16,540 training image con-
ditions repeated 4 times, and 200 testing image conditions repeated 80 times, totaling (16,540
training image conditions × 4 repetitions) + (200 testing image conditions × 80 repetitions)
= 82,160 image trials. Original data were collected using a 64-channel system at a sampling
rate of 1000 Hz. After signal denoising, epoch data were downsampled to 100 Hz, selecting 17
channels covering the occipital and parietal cortex. Instead of using the raw dataset, we chose
tofilteritto[0.1,100]Hz,retaining63channelsoftheoriginalEEGdataatasamplingrateof
1000 Hz. For preprocessing, we segmented the EEG data from 0 to 1000 ms after the stimulus
onset into trials. Baseline correction was performed using the mean of the 200 ms pre-stimulus
data. All electrodes were retained and downsampled to 250 Hz for analysis, and multivariate
noise normalization was applied to the training data [17]. To improve signal-to-noise ratio, we
averaged across the four EEG trials from the same image in the test set, while keeping each
EEG trial in the training setting. We compared the effects of averaging across EEG trials and
found it indeed improved the performance.
A.2 MEG dataset
To verify the versatility of ATM for embedding electrophysiological data, we tested it on MEG
data modality using the THINGS-MEG dataset [18]. It includes 271-channel MEG data from
4 subjects with 12 MEG sessions. The training dataset has 1854 Concepts × 12 images × 1
repetition, and the test dataset has concepts × 1 image × 12 repetitions for 200 times. Here,
we discarded 200 testing concepts from the training set to construct the same zero-shot task
as with the THINGS-EEG. Each image in the THINGS-MEG was displayed for 500 ms. There
wasafixedtimeforeachimageof1000±200ms.ContinuousMEGdatafrom-100msto1300
ms was segmented into trials after the stimulus onset from 0 to 1000 ms. Preprocessing was
performed using a bandpass filter of [0.1, 40] Hz and baseline correction after downsampling to
200Hz.Notethatduetothesmallnumberofparticipants,nostatisticalanalysiswasperformedVisual Decoding and Reconstruction 17
on the MEG dataset. We compared our approach with advanced methods i.e. NICE [46] and
B.D. [4] for classification and retrieval tasks on the MEG dataset. Similar to [46], we directly
used the stimulus images to match the template, rather than other images belonging to the
concept.
B More Implementation Details
B.1 Evaluation metric implementation
Classification accuracy As CLIP has been designed to align text and image modalities, we
also leverage its text encoder for EEG classification using the text embeddings of categories.
This approach utilizes CLIP’s text encoding capabilities to facilitate EEG classification. We
conducted zero-shot classification tests on the THINGS-EEG dataset. We employed Top-K
accuracy as a metric for performance evaluation. Specifically, we assessed performance based
on the Top-k (where k=1, 5) predictions. This means a classification is considered correct
if the true category is among the model’s Top-k predicted categories. We conducted tests for
bothwithin-subjectandleave-one-subject-outclassificationaccuracy,enablingacomprehensive
evaluation of the model’s performance across different scenarios. Additionally, for each test
instance, we extracted embeddings of N-1 unrelated samples from the test set as inputs. This
means, apart from the entire test set, the model evaluated by N-Way accuracy (where N=2,
4, 10 in our experiments) on the test set. We report these results in Appendix G.
Retrieval accuracy Similar to the classification task, in the retrieval task, the objective is
to retrieve the Top-K images most related to a given stimulus image via its corresponding
EEG signal. This implies that by merely changing the text embeddings of image labels used
duringprediction,toimageembeddings,wecantransitionthetaskfromclassificationtoimage
retrieval. The performance in retrieval tasks is superior to classification tasks as using image
embeddings in training. We conducted a detailed comparison of different methods in terms
of their Top-K retrieval accuracy in image retrieval tasks. Given that contrastive learning is
knowntobesensitivetobatchsize,wealsocomparedtheperformanceimprovementofdifferent
methods under varying batch sizes (batch size=16, 1024) (Appendix G).
Generation accuracy The generation task presents more challenges than the other tasks. In
this phase, EEG representations are fed into a two-stage generator. For each image condition
in the test set, we generate 1 images from 10 subjects based on the corresponding EEG signal.
Subsequently, image retrieval is performed for each generated image. The Top-1 and Top-5
accuracies are calculated. Since we utilized CLIP for alignment and did not extract lower-level
visual features from the EEG, this metric is particularly useful. It helps in evaluating the
semantic alignment between the generated images and their original counterparts.
B.2 Computing methods implementation
In the upstream EEG encoder part, we compared various methods. For the B.D. method [4],
we replicated the network structure as described in the original work, with the difference being
in the shape of the input data, due to the original study’s focus on MEG. It is worth men-
tioning that we used the retention test method in the testing process, so the strategy of joint
subject training is not suitable for our task. We modify its subject-wise layer as an Input layer
for modeling the time dimension. Regarding the [46], we also reproduced the EEG encoder
as described in their paper, using spatiotemporal convolution modules and EEG projection18 D. Li, C. Wei et al.
modules with the same convolution parameters. To ensure fairness, we did not use the same
hyperparameters as in the original paper. Instead, we chose settings that yielded excellent re-
sultsuponreproduction.Acrossallmethods,weusedidenticalhyperparameters,apartfromthe
networkstructures.Theseincludedbatchsize,optimizer,initiallearningrate,andtemperature
parameters.
B.3 Architecture details
Table 1. Brain module configuration for use with a target latent of size 1024
Layer Input shapeOutput shape# parameters
Spatialattentionblock (N,C,T) (N,C,T) 553,078
Inputlayer (N,C,T) (N,C,T) 62,750
Plug-and-PlayConvmodule (N,C,T) (N,H1,H2) 103,680
Temporalaggregation (N,H1,H2) (N,H1*H2) 0
MLPprojector (N,H1*H2) (N,1024) 2,527,232
Total 3,246,741
B.4 Training details
In the initial design of the EEG encoder module, we adopted two approaches to guide model
predictions:textembeddingandimageembedding.Duetothedifferencesinfeaturegranularity,
alignmentsfavoringimageembeddingtendtoperformbetterinimageretrievalandclassification
tasks. During the training process using contrastive learning, we found that a batch size of 16
is a prudent choice across all models. A batch size of 1024 means that a sufficient number of
samplesarecomparedinasingletrainingstep,whichrequiresthemodeltohaveahighernoise
resistance capability. As observed in 5 and 6, larger batch sizes have the potential to achieve
better performance and definite training efficiency gains across most methods. To improve the
signal-to-noise ratio of EEG, we averaged 80 repeated data in the test set, a method similar to
seeking Event-Related Potentials (ERP). To make full use of the training data volume, we did
not average the 4 repetitions in the training set but instead fed all EEG data into the model
for learning.
Table 2. Ablation study on the ATM model’s different components for THINGS-EEG retrieval.
ModuleMLPAdaConvILSA TOP-1 TOP-5
✓ 8.01±1.97 25.41±5.74
✓ ✓ 21.55±6.26 50.78±8.77
✓ ✓ ✗ ✓ 23.57±6.00 53.50±7.26
✓ ✓ ✓ ✗ 27.71±6.2258.71±7.49
ATM-S ✓ ✓ ✓ ✓ 26.49±5.81 56.58±8.67
✓ 8.01±1.97 25.41±5.74
✓ ✓ 18.67±5.48 46.31±9.11
✓ ✓ ✗ ✓ 21.11±5.56 50.48±7.74
✓ ✓ ✓ ✗ 24.64±5.9754.17±5.93
ATM-E ✓ ✓ ✓ ✓ 22.43±5.41 51.24±7.27Visual Decoding and Reconstruction 19
Fig.10. Test accuracy during training. (a) Training of within-subject model. (b) Training of
across-subject model. We compared 5 different EEG encoding models, including EEGNetv4, B.D.,
NICE, ATM-S and ATM-E.
C Details of EEG guidance image generation
Here, we provide a concise overview of the conditional diffusion model framework used in
EEG-guided image generation, following the presentation of continuous-time diffusion models
in [45, 22].
Diffusion models DiffusionModels(DMs)engageinagenerativeprocessbytransforminghigh-
variance Gaussian noise into structured data representations. This transformation is achieved
bygraduallyreducingnoiselevelsacrossasequenceofsteps.Specifically,webeginwithahigh-
variance Gaussian noise x ∼ N(0,σ2 ) and systematically denoise it through a series of
M max
steps to obtain x ∼ p(x ;t), where σ < σ and σ = σ . For a well-calibrated DM, and
t t t t+1 M max
with σ =0, the final x aligns with the original data distribution.
0 0
Samplingprocess ThesamplinginDMsisimplementedbynumericallysimulatingaProbability
Flowordinarydifferentialequation(ODE)orastochasticdifferentialequation(SDE).TheODE
is represented as:
dx=−σ˙(t)σ(t)∇ logp(x;t)dt, (1)
x
where ∇ logp(x;t) is the score function, and σ(t) is a pre-defined schedule with its time
x
derivative σ˙(t). The SDE variant includes a Langevin diffusion component and is expressed as:
dx=−σ˙(t)σ(t)∇ logp(x;t)dt
x
−β(t)σ2(t)∇ logp(x;t)dt (2)
x
(cid:112)
+ 2β(t)σ(t)dω ,
t
where dω is the standard Wiener process.
t
Training of DMs The core of DM training is to learn a model s (x;t) for the score function.
θ
This is typically achieved through denoising score matching (DSM), where ϵ is a learnable
θ
denoiser. The training process can be formulated as:
E (cid:2) ∥ϵ (x +n ;t,c)−x ∥2(cid:3) , (3)
(x0,c)∼pdata(x0,c),(nt,t)∼p(nt,t) θ 0 t 0 2
where n is Gaussian noise with variance σ2, and c represents a condition.
t t20 D. Li, C. Wei et al.
C.1 Stage I - EEG-Conditioned Diffusion
The initiation of the EEG-conditioned diffusion phase is paramount in our EEG-based image
generation framework, leveraging the classifier-free guidance strategy alongside data pairs of
CLIP embeddings and EEG embeddings (z ,z ). Adapting from state-of-the-art generative
I E
techniques,ourdiffusionprocessisspecificallyconditionedontheEEGembeddingz toadeptly
E
capture the distribution of CLIP embeddings p(z |z ). The CLIP embedding z , procured
I E I
during this phase, establishes the groundwork for the ensuing image generation stage. Our
architecture incorporates a streamlined U-Net, labeled as ϵ (zt,t,z ), where zt signifies the
prior I E I
perturbed CLIP embedding at a given diffusion timestep t. The training utilizes pairs from
the ImageNet database, consisting of over a million images, to fine-tune the EEG-Conditioned
Diffusionmodel.Thismodelismeticulouslytrainedusingtheclassifier-freeguidanceapproach,
effectively balancing the conditioning signal’s fidelity with the generative output’s diversity.
Classifier-free guidance method The Classifier-Free Guidance technique is crucial in guiding
the iterative refinement of a Diffusion Model (DM) under a specific EEG condition z . It
E
achieves this by synchronizing the outputs of both a conditional and an unconditional model.
The model’s formulation, ϵw (zt;t,z ), is as follows:
prior I E
ϵw (zt;t,z )=(1+w)ϵ (zt;t,z )−wϵ (zt;t), (4)
prior I E prior I E prior I
where w ≥ 0 represents the guidance scale. This mechanism facilitates concurrent training of
the conditional and unconditional models within a singular network framework, periodically
substitutingtheEEGembeddingz withanullvectortopromotetrainingvariability,i.e.10%
E
of the time. The primary objective of this method is to enhance the sample quality produced
by DMs while maintaining output diversity.
C.2 Stage II - CLIP-Embedded Image Synthesis
In Figure 11, we compare the effects of one-stage and two-stage EEG-guided image generation.
WeshowimagesgeneratedusingEEGembeddingsdirectly(One-stage)andimagesgenerated
usingimageembeddingsobtainedviapriordiffusion(Two-stage).Itcanbeseenthatthetwo-
stageEEG-guidedimagegenerationcanmoreaccuratelyreconstructthesemanticandlow-level
visual features of the original image, and the style is more realistic.
In the second stage of our EEG-based image generation approach, the CLIP embedding z
I
derivedfromtheEEG-conditioneddiffusionactsastheprecursorforsynthesizingvisualobjects
I based on z . This is achieved by harnessing the synergies of advanced pre-trained models,
I
namely SDXL and IP-Adapter [33, 59], facilitating the creation of high-caliber images.
The cornerstone of our synthesis process is the SDXL framework, acclaimed for its profi-
ciency in text-to-image conversion. The integration of the IP-Adapter introduces dual cross-
attention mechanisms, allowing the CLIP embedding z to serve as a directive input and
I
guide the denoising trajectory within the U-Net structure. The synthesis model is denoted as
ϵ (z ,t,z ), where z denotes the SDXL Variational Autoencoder’s (VAE) disturbed latents.
SD t I t
SDXL-turbo for accelerated processing To augment the efficiency of our framework, we addi-
tionally explore the SDXL-Turbo [36], a refined iteration of SDXL optimized for swift image
synthesis. This variant proves especially beneficial in scenarios demanding quick generation of
high-fidelity visuals.Visual Decoding and Reconstruction 21
a b
Seen
Stage I
Stage 2
Fig.11.Comparisonbetweenone-stageandtwo-stageEEGguidanceimagegeneration.(a)
Wepresenttheimagesthatsubjectsseen(Seen),thegeneratedimagesdirectlyusingEEGembeddings
(One-stage),andthegeneratedimagesfromimageembeddingsobtainedbythepriordiffusion(Two-
stage).Theseresultsindicatethatthestrategyofourtwo-stagegenerationcanbetterreconstructthe
seenvisualstimuli.(b)WeemployedATM-Stocomparethegeneratedimageswiththeoriginalimages
in a retrieval task. Our result indicates that the images generated in two stages significantly enhance
the performance of the original model on the retrieval task.
IP-Adapter’s efficacy The IP-Adapter, with its compact design, has proven to be effective in
enhancingimagepromptadaptabilitywithinpre-trainedtext-to-imagemodels.Itscompatibility
with text prompts for multimodal image generation extends the versatility of our EEG-based
image synthesis approach.
D Performance comparison
Comparison metrics Ourstudyusesvariousmetricstoevaluatehowwellwecanrecreatevisual
stimulifrombraindata(EEG,MEG,fMRI)(Table1inmaintext).ThesemetricsincludeSSIM
(structural similarity index metric)[51], SwAV (SwAV-ResNet50, refer to average correlation
distance)[5],andtwo-wayidentificationusingneuralnetworks(AlexNet(2/5),Inception,CLIP.
HereAlexNet(2/5)the2ndand5thfeaturelayersofAlexNet)forbothlow-levelandhigh-level
image features. Here two-way identification can be seem as a two-way retrieval task described
in B.1. In Table 3, our results showed that on the THINGS dataset, we could achieve perfor-
manceoverMEGonEEGreconstructionusingATM.Table4showsthedecodingperformance
of different data sets (fMRI, MEG, EEG) on visual stimulus tasks, and we even achieved the
sameorbetterperformancethanfMRIandMEG.Ourresultssuggestthatasuitableupstream
neural representation plays a decisive role in the downstream task.
E Representational analysis
As depicted in Fig. 12, we showcase the representational similarity matrix and visualization
in the latent space. To investigate the relationship between the representations obtained from
EEG and those of images, we conducted a representational similarity matrix. We focused on
subject 8, who exhibited the highest retrieval accuracy. By applying a clustering algorithm
to the image embeddings corresponding to 200 images in the test set, we observed distinct
within-category clustering. We generated similarity matrices based on both image and text
embeddings, which were then compared with EEG representations. As shown in Fig. 12, clear
within-category clustering is observable in the representational similarity matrix with image,
whereas this phenomenon is not present in the representational similarity matrix with text.22 D. Li, C. Wei et al.
Table3.QuantitativeassessmentsofthereconstructionqualityforEEG,MEG,andfMRI.Fordetailed
explanations of the metrics.
Low-level High-level
Dataset ↑ SSIM ↑ AlexNet(2) ↑ AlexNet(5) ↑ Inception ↑ CLIP ↑ SwAV ↓
NSD (B.D.) [4] 0.366 0.962 0.977 0.910 0.917 0.410
NSD (Brain-Diffuser) [31] 0.356 0.942 0.962 0.872 0.915 0.423
NSD (MindEye) [39] 0.308 0.917 0.974 0.936 0.942 0.369
THINGS-MEG (B.D.) [4] 0.327 0.695 0.753 0.593 0.700 0.630
THINGS-MEG (Ours) 0.340 0.613 0.672 0.619 0.603 0.651
THINGS-EEG (Ours) 0.345 0.776 0.866 0.734 0.786 0.582
a b
Fig.12. Visualization of the representation of EEG, image and text modality. (a) Repre-
sentational similarity matrix between EEG features and image/text features. (b) Visualization in the
latent space of EEG/image/text by t-SNE.Visual Decoding and Reconstruction 23
Table4.Theclassificationperformanceofvariousmethodsarediscussed.Duetodifferencesindatasets
and data modalities, we have specified unified metrics to objectively assess the performance of each
method.
50-way 100-way 200-way
Dataset Model top-1top-5top-1top-5top-1top-5
CADA-VAE(V&T)[38] 10.0240.37 - - - -
MVAE(V&T)[56] 10.0439.60 - - - -
GOD-Wiki(fMRI)MMVAE(V&T)[43] 11.6843.29 - - - -
MoPoE-VAE(V&T)[47]12.9051.78 - - - -
BraVL(V&T)[11] 13.9953.13 - - - -
THINGS(MEG) ATM (Ours) 15.6341.3811.7529.25 5.88 19.25
THINGS(EEG) BraVL[11] 14.3340.28 - - 5.82 17.45
ATM (Ours) 17.4039.4011.5028.50 7.40 20.6024 D. Li, C. Wei et al.
F Additional images results
F.1 Additional retrieval results
Fig.13. Additional retrieval resultsVisual Decoding and Reconstruction 25
F.2 Additional generated images
Fig.14. Additional generated results with the best alignment to original images
tseB
neeS
noitareneG26 D. Li, C. Wei et al.
Fig.15. Additional generated results with the median alignment to original images
naideM
neeS
noitareneGVisual Decoding and Reconstruction 27
Fig.16. Additional generated results with the worst alignment to original images
tsroW
neeS
noitareneG28 D. Li, C. Wei et al.
F.3 Additional generated images for each subject
Fig.17. Part of subject 1 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
G Additional evaluation results
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneGVisual Decoding and Reconstruction 29
Fig.18. Part of subject 2 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
Fig.19. Part of subject 3 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
tseB
naideM
tsroW
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG30 D. Li, C. Wei et al.
Fig.20. Part of subject 4 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
Fig.21. Part of subject 5 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
tseB
naideM
tsroW
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneGVisual Decoding and Reconstruction 31
Fig.22. Part of subject 6 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
Fig.23. Part of subject 7 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
tseB
naideM
tsroW
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG32 D. Li, C. Wei et al.
Fig.24. Part of subject 8 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
Fig.25. Part of subject 9 generates images. We do a batch generation of the subjects and then
calculate the best, medium, and worst performers compared to the original stimulus pictures.
tseB
naideM
tsroW
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneG
neeS
detareneGVisual Decoding and Reconstruction 33
Fig.26.Part of subject 10 generates images.Wedoabatchgenerationofthesubjectsandthen
calculate the best, medium, and worst performers compared to the original stimulus pictures.
tseB
naideM
tsroW
neeS
detareneG
neeS
detareneG
neeS
detareneG34 D. Li, C. Wei et al.
G.1 Accuracy for time windows
Fig.27. Accuracy for growing windows. We use an EEG time window of 100ms, sliding 100ms
each time. (a) Top-1 accuracy. (b) Top-5 accuracy.Visual Decoding and Reconstruction 35
Fig.28.Accuracy for sliding windows.WeuseanEEGtimewindowof100ms,sliding100mseach
time. (a) Top-1 accuracy. (b) Top-5 accuracy.
Fig.29.Accuracy for growing windows.TheMEGtimewindowgrowsfrom50msto1000ms.(a)
Top-1 accuracy. (b) Top-5 accuracy.36 D. Li, C. Wei et al.
Fig.30.Accuracyforslidingwindows.WeuseanMEGtimewindowof100ms,sliding100mseach
time. (a) Top-1 accuracy. (b) Top-5 accuracy.
Table 5. Overall accuracy of zero-shot retrieval on THINGS-EEG dataset. We showed in-subject
and cross-subject retrieval task performance (Ave ± Std.%) under the condition of batch size=16.
Wecompared the 2-way,4-way,10-way,theTop-1 and Top-5accuracyof 200-way fromdifferentEEG
embedding methods. Our ATM outperformed all the others.
Subjectdependent-trainandtestononesubject(batchsize=16)
Methods 2-Way 4-Way 10-Way Top-1 Top-5
EEGITNet 83.23±2.77 65.67±3.84 43.87±3.78 7.79±1.27 22.12±2.88
EEGConformer 89.18±2.88 75.49±5.30 55.32±6.88 11.29±3.55 33.49±6.78
ShallowFBCSPNet 84.29±2.56 67.13±3.61 46.07±4.55 8.26±2.95 25.87±5.03
EEGNetV4 89.03±2.86 75.40±5.18 56.77±6.60 13.29±3.99 35.50±7.14
B.D. 91.14±2.45 79.59±4.59 62.62±6.33 16.29±4.35 42.16±7.56
NICE 92.17±2.78 82.10±5.55 65.65±8.03 19.40±5.83 46.26±10.42
MLP 87.67±4.41 72.58±7.62 53.52±9.45 11.81±4.33 32.74±9.24
ATM-S(Ours) 93.89±1.97 85.38±3.95 71.63±6.17 22.84±5.87 52.22±8.34
ATM-E(Ours) 94.44±1.76 86.11±3.39 71.89±5.64 24.38±6.56 54.14±8.08
Subjectindependent-leaveonesubjectfortest(batchsize=16)
Methods 2-Way 4-Way 10-Way Top-1 Top-5
EEGITNet 77.14±4.14 55.29±6.22 31.88±6.20 2.93±1.70 13.00±4.45
EEGConformer 79.49±3.91 59.31±5.83 36.67±5.35 4.11±1.83 17.04±4.21
ShallowFBCSPNet 75.14±4.45 53.06±6.70 31.23±6.56 3.19±2.17 13.01±5.15
EEGNetV4 82.60±3.17 64.28±5.44 42.24±6.10 6.13±2.40 21.23±5.19
B.D. 81.49±3.52 62.35±6.44 40.53±7.19 6.16±2.40 20.45±5.40
NICE 81.85±2.53 63.57±4.46 41.86±4.68 6.43±1.46 21.39±3.62
MLP 80.49±2.61 62.09±3.88 40.37±3.71 5.67±1.09 19.97±3.02
ATM-S(Ours) 82.88±5.18 65.03±8.51 44.86±9.85 8.04±3.34 24.62±8.17
ATM-E(Ours) 83.30±3.92 65.80±6.98 44.84±7.76 7.47±2.84 23.75±6.86Visual Decoding and Reconstruction 37
Table 6. Overall accuracy of zero-shot Retrieval on THINGS-EEG dataset. We showed in-subject
andcross-subjectretrievaltaskperformance(Ave±Std.%)undertheconditionof batch size=1024.
Wecompared the 2-way,4-way,10-way,theTop-1 and Top-5accuracyof 200-way fromdifferentEEG
embedding methods. Our ATM outperformed all the others.
Subjectdependent-trainandtestononesubject(batchsize=1024)
Methods 2-Way 4-Way 10-Way Top-1 Top-5
EEGITNet 76.69±12.97 56.98±16.31 36.35±15.11 5.75±3.62 18.14±9.40
EEGConformer 76.17±13.13 56.29±16.70 34.72±14.79 3.98±2.80 17.10±9.21
ShallowFBCSPNet 74.32±12.14 53.97±15.81 33.48±14.35 6.10±4.61 16.53±9.94
EEGNetV4 92.81±2.22 83.15±4.20 67.81±6.11 19.51±5.19 48.99±6.75
B.D. 78.42±8.81 58.24±12.13 37.97±11.38 5.88±3.49 18.61±7.81
NICE 92.73±2.75 83.26±5.47 67.96±8.31 19.32±5.33 49.26±9.69
MLP 83.09±2.54 66.70±4.16 45.43±4.58 7.23±1.66 25.14±3.66
ATM-S(Ours) 94.60±1.93 86.88±4.20 73.89±5.93 26.09±6.96 58.07±8.16
ATM-E(Ours) 92.99±2.20 83.81±4.46 68.87±7.27 22.40±6.62 50.59±9.59
Subjectindependent-leaveonesubjectfortest(batchsize=1024)
Methods 2-Way 4-Way 10-Way Top-1 Top-5
EEGITNet 77.47±3.75 55.75±6.01 33.60±6.16 3.80±2.01 14.02±3.89
EEGConformer 68.02±7.75 44.24±9.25 23.84±7.65 1.66±1.37 8.62±4.20
ShallowFBCSPNet 76.40±4.64 54.81±7.09 32.37±7.28 2.49±1.57 13.24±5.98
EEGNetV4 82.60±3.17 64.28±5.44 42.24±6.10 6.13±2.40 21.23±5.19
B.D. 85.30±6.24 69.87±10.09 50.82±11.74 10.57±5.26 29.74±11.28
NICE 83.75±3.21 65.82±5.93 44.41±6.12 7.04±2.83 23.55±4.86
MLP 80.49±2.61 62.09±3.88 40.37±3.71 5.67±1.09 19.97±3.02
ATM-S(Ours) 87.36±3.97 72.80±7.02 53.80±8.41 11.84±4.80 33.73±8.73
ATM-E(Ours) 87.41±3.07 72.85±5.51 53.15±6.52 11.12±3.26 32.61±6.78
Table 7. Accuracy of zero-shot Retrieval on THINGS-EEG for odd-numbered subjects(batch
size=16).
Subjectdependent-trainandtestononesubject(batchsize=16)
Method Subject1 Subject3 Subject5 Subject7 Subject9
top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5
EEGITNet 8.20 19.45 6.95 24.25 6.30 19.50 8.95 19.00 6.10 21.60
Conformer 4.75 21.30 9.90 34.15 7.55 28.05 12.25 35.35 13.95 34.10
ShallowFBCSPNet 7.00 20.90 8.45 28.45 7.75 20.80 6.35 27.60 2.80 18.00
EEGNetV4 10.85 31.10 17.65 41.20 6.75 21.80 14.15 39.05 10.40 30.50
B.D. 13.05 36.00 14.45 37.85 10.55 32.00 18.65 46.80 14.60 37.55
NICE 13.25 35.75 23.15 51.40 12.40 33.45 19.45 50.20 17.55 41.70
MLP 13.00 38.30 13.90 39.60 8.00 24.85 13.00 39.15 12.65 36.20
ATM-S(Ours) 19.10 49.05 22.95 56.30 16.10 39.95 25.75 55.40 21.50 48.15
ATM-E(Ours) 20.75 47.35 25.10 56.45 18.50 44.35 24.70 54.10 23.40 52.40
Subjectindependent-leaveonesubjectfortest(batchsize=16)
Method Subject1 Subject3 Subject5 Subject7 Subject9
top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5
EEGITNet 3.40 13.75 1.05 10.80 1.45 7.55 4.30 13.65 0.55 5.95
Conformer 5.45 20.10 2.00 12.75 2.90 12.95 5.50 16.45 1.05 9.90
ShallowFBCSPNet 4.25 13.30 1.45 7.65 0.90 7.40 3.75 13.75 0.65 5.95
EEGNetV4 7.20 22.40 4.60 17.80 3.00 15.75 3.80 18.95 3.40 12.70
B.D. 6.55 21.45 5.45 17.00 3.10 14.75 6.45 24.00 2.60 12.75
NICE 4.80 20.05 5.80 18.10 5.85 16.95 5.75 21.20 4.95 17.65
MLP 3.65 16.20 5.95 20.95 5.90 19.20 4.40 14.25 7.45 23.85
ATM-S(Ours) 8.30 25.50 11.40 31.30 5.95 21.25 4.80 19.45 4.05 13.30
ATM-E(Ours) 7.80 22.95 13.75 35.70 4.20 14.30 6.40 25.95 4.50 14.9038 D. Li, C. Wei et al.
Table 8. Accuracy of zero-shot Retrieval on THINGS-EEG for even-numbered subjects(batch
size=16).
Subjectdependent-trainandtestononesubject(batchsize=16)
Method Subject2 Subject4 Subject6 Subject8 Subject10 OverallAvg
top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5
EEGITNet 6.55 19.05 7.90 23.10 8.10 23.10 9.80 27.35 9.05 24.85 8.08 23.49
Conformer 8.10 25.80 13.30 37.10 12.75 36.65 15.75 45.10 14.55 37.35 12.89 36.40
ShallowFBCSPNet 6.15 23.50 10.85 29.10 9.05 26.95 13.15 34.90 11.00 28.55 10.04 28.60
EEGNetV4 8.80 28.20 14.30 37.45 15.40 42.45 19.40 42.40 15.15 40.85 14.61 38.27
B.D. 13.35 38.00 17.45 45.00 14.25 40.35 24.50 54.65 22.10 53.45 18.33 46.29
NICE 11.45 31.05 20.75 48.95 22.25 50.35 29.75 62.40 24.00 57.40 21.64 50.03
MLP 10.85 33.10 15.20 37.70 12.15 35.05 18.65 47.55 18.30 45.55 15.03 39.79
ATM-S(Ours) 15.10 41.00 25.75 55.75 20.05 50.10 34.90 67.50 27.15 58.95 24.59 54.66
ATM-E(Ours) 15.25 44.90 25.75 55.75 23.00 53.95 39.40 70.25 29.60 63.75 26.26 57.36
Subjectindependent-leaveonesubjectfortest(batchsize=16)
Method Subject2 Subject4 Subject6 Subject8 Subject10 OverallAvg
top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5
EEGITNet 2.65 11.40 3.25 15.35 8.10 23.10 9.80 27.35 9.05 24.85 6.57 20.41
Conformer 4.65 18.70 4.25 19.60 12.75 36.65 15.75 45.10 14.55 37.35 10.39 31.48
ShallowFBCSPNet 2.75 13.70 5.55 15.45 9.05 26.95 13.15 34.90 11.00 28.55 8.30 23.91
EEGNetV4 7.80 24.30 7.35 23.90 15.40 42.45 19.40 42.40 15.15 40.85 13.02 34.78
B.D. 5.00 16.40 6.65 22.35 14.25 40.35 24.50 54.65 22.10 53.45 14.50 37.44
NICE 5.55 21.40 8.20 24.90 22.25 50.35 29.75 62.40 24.00 57.40 17.95 43.29
MLP 5.50 20.65 5.45 18.35 12.15 35.05 18.65 47.55 18.30 45.55 12.01 33.43
ATM-S(Ours) 7.25 23.65 14.55 39.80 20.05 50.10 34.90 67.50 27.15 58.95 20.78 47.90
ATM-E(Ours) 8.65 27.60 14.55 39.80 23.00 53.95 39.40 70.25 29.60 63.75 21.62 47.93
Table 9. Accuracy of zero-shot Retrieval on THINGS-EEG for odd-numbered subjects(batch
size=1024).
Subjectdependent-trainandtestononesubject(batchsize=1024)
Method Subject1 Subject3 Subject5 Subject7 Subject9
Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5
EEGITNet 0.50 3.45 2.40 7.75 5.35 18.25 8.90 24.90 5.50 22.45
Conformer 0.50 3.20 0.50 6.90 3.40 17.50 7.10 23.80 6.15 22.75
ShallowFBCSPNet 0.65 3.35 2.30 6.20 3.35 14.55 7.35 23.15 6.80 18.05
EEGNetV4 11.45 41.45 20.85 48.70 13.75 42.00 19.05 50.30 15.50 40.60
B.D. 6.70 23.25 6.70 19.75 1.70 8.10 5.85 20.55 6.80 21.05
NICE 17.65 50.25 22.05 51.20 9.80 27.65 17.15 49.60 19.60 52.55
MLP 8.40 24.40 5.60 22.65 6.00 25.15 8.35 24.35 5.45 22.25
ATM-S(Ours) 25.60 60.40 25.00 62.35 12.90 43.00 30.50 61.50 24.35 51.50
ATM-E(Ours) 18.85 44.10 24.30 56.10 11.95 32.65 24.85 53.75 20.05 47.75
Subjectindependent-leaveonesubjectfortest(batchsize=1024)
Method Subject1 Subject3 Subject5 Subject7 Subject9
Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5
EEGITNet 4.45 13.95 1.50 12.05 1.00 8.60 4.50 13.35 1.00 8.95
Conformer 4.45 17.20 0.70 4.75 0.50 5.30 2.40 10.95 0.50 3.35
ShallowFBCSPNet 2.00 11.25 1.70 9.25 1.00 4.85 3.80 17.30 1.00 5.75
EEGNetV4 9.45 24.00 8.20 21.90 5.20 18.75 5.45 21.80 5.65 16.40
B.D. 5.30 16.60 13.45 35.30 8.15 21.30 15.85 37.75 8.10 27.10
NICE 4.80 20.05 6.75 20.70 4.15 18.55 5.55 24.45 5.10 17.20
MLP 4.45 12.30 6.75 24.60 4.90 19.55 4.85 17.60 4.45 19.80
ATM-S(Ours) 10.45 26.75 11.85 33.80 6.95 23.85 16.05 43.50 4.85 22.70
ATM-E(Ours) 10.45 26.75 13.45 35.30 8.15 24.75 15.85 39.05 7.55 30.65Visual Decoding and Reconstruction 39
Table 10. Accuracy of zero-shot Retrieval on THINGS-EEG for even-numbered subjects(batch
size=1024).
Subjectdependent-trainandtestononesubject(batchsize=1024)
Method Subject2 Subject4 Subject6 Subject8 Subject10 OverallAvg
Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5
EEGITNet 0.75 5.55 6.00 20.35 7.60 20.80 10.55 28.00 9.90 29.95 7.16 20.93
Conformer 0.50 3.70 3.30 18.10 4.35 21.95 7.50 25.00 6.50 28.15 4.43 19.38
ShallowFBCSPNet 0.75 4.75 5.25 16.35 7.80 16.95 13.45 32.20 13.25 29.70 8.10 20.19
EEGNetV4 17.30 43.60 27.20 58.75 20.55 53.25 22.95 56.45 26.50 54.85 22.90 53.38
B.D. 1.55 6.90 3.65 15.30 3.55 13.05 9.65 27.00 12.70 31.15 6.22 18.68
NICE 14.35 40.00 20.00 49.90 18.35 49.75 28.05 62.65 26.25 59.10 21.40 52.28
MLP 6.40 19.45 7.35 27.50 6.75 25.40 7.00 27.20 10.95 33.05 7.69 26.52
ATM-S(Ours) 22.00 54.50 31.35 60.90 21.30 51.05 38.80 72.00 29.05 63.50 28.50 60.39
ATM-E(Ours) 17.00 44.85 23.00 51.25 19.40 47.55 35.65 66.75 29.00 61.15 24.81 54.31
Subjectindependent-leaveonesubjectfortest(batchsize=1024)
Method Subject2 Subject4 Subject6 Subject8 Subject10 OverallAvg
EEGITNet 4.70 15.20 4.80 14.40 5.50 17.45 3.70 14.25 6.90 22.00 5.12 16.66
Conformer 1.40 9.70 1.15 8.20 1.15 9.70 0.80 5.15 3.55 11.95 1.61 8.94
ShallowFBCSPNet 2.50 12.15 2.60 17.20 2.15 16.95 1.80 13.15 6.30 24.55 3.07 16.80
EEGNetV4 8.25 28.15 9.45 26.05 6.40 24.60 9.20 23.60 10.15 36.95 8.69 27.87
B.D. 4.45 23.20 4.30 12.90 11.15 33.70 18.05 46.30 16.90 43.30 10.97 31.88
NICE 4.90 22.80 9.65 29.70 8.35 24.35 7.90 25.00 13.25 32.75 8.81 26.92
MLP 6.50 21.55 9.50 25.15 3.60 16.15 7.25 29.25 12.80 34.35 7.93 25.29
ATM-S(Ours) 7.10 24.75 14.65 39.40 11.10 35.80 14.95 40.25 20.45 46.50 13.65 37.34
ATM-E(Ours) 10.45 27.75 14.65 35.25 10.35 35.20 13.95 38.60 15.85 42.85 13.05 35.93