Bigraph Matching Weighted with Learnt Incentive Function for
Multi-Robot Task Allocation
Steve Paul1, Nathan Maurer2 and Souma Chowdhury3,‚Ä†
Abstract‚ÄîMost real-world Multi-Robot Task Allocation experienceandintuition.Hence,althoughtheyprovidesome
(MRTA) problems require fast and efficient decision-making, degree of explainability, they leave significant scope for im-
which is often achieved using heuristics-aided methods such
provement in performance. Moreover, heuristic-based meth-
as genetic algorithms, auction-based methods, and bipartite
ods are often poor at adapting to complex problem charac-
graph matching methods. These methods often assume a form
that lends better explainability compared to an end-to-end teristics, or generalizing across different problem scenarios,
(learnt) neural network based policy for MRTA. However, without tedious hand-crafting of underlying heuristics. As a
deriving suitable heuristics can be tedious, risky and in some result, an emerging notion in (fast, near-optimal) decision-
cases impractical if problems are too complex. This raises the
making is ‚Äúcan more effective heuristics be automatically
question:cantheseheuristicsbelearned?Tothisend,thispaper
(machine) learned from prior operational data or experience particularly develops a Graph Reinforcement Learning (GRL)
framework to learn the heuristics or incentives for a bipartite gathered by the agent/robot e.g., in simulation‚Äù [9]? This
graph matching approach to MRTA. Specifically a Capsule raisesthefundamentalquestionofwhetherlearnedheuristics
Attentionpolicymodelisusedtolearnhowtoweighttask/robot canmatchandpotentiallysurpasstheperformanceofhuman-
pairings (edges) in the bipartite graph that connects the set of
prescribed heuristics when generalizing across a wide range
taskstothesetofrobots.Theoriginalcapsuleattentionnetwork
of problem scenario of similar or varying complexity. We
architecture is fundamentally modified by adding encoding
of robots‚Äô state graph, and two Multihead Attention based explore this research question in the context of multi-robot
decoders whose output are used to construct a LogNormal task allocation problems of the following type Multi Robot
distribution matrix from which positive bigraph weights can Tasks-Single Task Robots (MR-ST) [10]. In doing so, this
be drawn. The performance of this new bigraph matching
paper also provides initial evidence of the potential for
approachaugmentedwithaGRL-derivedincentiveisfoundto
exploiting the best of both worlds: explainable structure of
beatparwiththeoriginalbigraphmatchingapproachthatused
expert-specified heuristics, with the former offering notable graph matching techniques and automated generation of the
robustness benefits. During training, the learned incentive necessary heuristics through reinforcement learning (RL).
policy is found to get initially closer to the expert-specified
A. Related Works
incentive and then slightly deviate from its trend.
Some of the most common online solution approaches
forMRTAproblemsincludeheuristic-basedmethodsInteger
I. INTRODUCTION
Multi-Robot Task Allocation (MRTA) is the problem of linear programming (ILP) based methods, bipartite graph
allocatingmultiplerobotstocompletemultipletaskswiththe (bigraph) matching, meta-heuristic methods such as genetic
goal of maximizing/minimizing an objective or cost. In this algorithm and Ant Colony Optimization, and also Rein-
paper, we are interested in comparing learning methods that forcement Learning (RL) based methods. ILP-based mTSP
generalize well with more traditional optimization methods. formulationsandsolutionshavealsobeenextendedtoMRTA
Some of the real-world applications include construction problems [11]. Although the ILP-based approaches can in
[1], disaster response [2], manufacturing [3], and warehouse theory provide optimal solutions, they are characterized by
logistics [4]. Even though expensive solvers such as Mixed- exploding computational effort as the number of robots and
Integer Non-Linear Programming (MINLP) provide near- tasks increases [12], [13].
optimal solutions, these methods cannot be deployed for Most online MRTA methods, e.g., auction-based meth-
scenarios with 100s or 1000s of tasks and robots. ods [8], [14], [15], metaheuristic methods [16], [17], and
There exists a notable body of work on heuristic-based bi-graph matching methods [2], [6], [18], Genetic Algo-
approachestosolvingMRTAproblemsmoreefficiently,e.g., rithms [5], use some sort of heuristics, and often report the
using genetic algorithm [5], graph-based methods [2], [6], optimality gap at least for smaller test cases compared to
[7], and auction-based methods [8]). Often the heuristic the exact ILP solutions. [6] introduces the use of bigraphs
choices and setting in these approaches are driven by expert and the Max-Sum algorithm for decentralized task alloca-
tioninmulti-agentsystems.specifically,maximumweighted
‚Ä† CorrespondingAuthor,soumacho@buffalo.edu bipartite matching [2], [6] with manually tuned incentive
Authors 1,2,3 are with the Department of Mechanical and Aerospace functions (aka expert heuristics) has been shown to provide
Engineering, University at Buffalo, Buffalo, NY, USA {stevepau,
scalable and effective solutions to various MRTA problems.
namaurer, soumacho}@buffalo.edu
This work was supported by the Office of Naval Research (ONR) The incentive function typically represents the affinity of
awardN00014-21-1-2530andtheNationalScienceFoundation(NSF)award any given robot to choose a given task based on the task‚Äôs
CMMI2048020.Anyopinions,findings,conclusions,orrecommendations
propertiesandthestateofthatrobot.Howevereverytimethe
expressedinthispaperarethoseoftheauthorsanddonotnecessarilyreflect
theviewsoftheONRortheNSF. features of the robot, robot team, or task space changes, the
4202
raM
11
]IA.sc[
1v13170.3042:viXraincentive functions must be re-designed or re-calibrated by A. MRTA-CT as Optimization Problem
an expert to preserve performance. This technical challenge The exact solution to the MRTA-CT problem is obtained
motivateslearningoftheincentivefunctionfromexperience. byformulatingitasanINLPproblem,asconciselyexpressed
Over the past few years, Graph Reinforcement Learning below (for brevity); details can be found in [10].
(GRL) methods have emerged as a promising approach to
min f =(NT ‚àíN )/NT (1)
solving combinatorial optimization problems encountered in cost success
(cid:40)
singleandmulti-agentplanningapplications[10],[19]‚Äì[28]. N = (cid:88) œà œà i =1, if œÑ if ‚â§œÑ i
While these methods have shown to generate solutions that success i œà =0, if œÑf >œÑ
can generalize across scenarios drawn from the same dis-
i‚ààVT i i i
0‚â§‚àÜt ‚â§‚àÜ ,r ‚ààR (2)
tribution and can be executed near instantaneously, they are r max
considered to be black-box methods (thus lacking explain- 0‚â§ct r ‚â§C max,r ‚ààR (3)
ability) and usually do not provide any sort of guarantees. Here œÑf is the time at which task i is completed, ‚àÜt is
Key Contributions: The overall objective of this paper i r
the available range for robot r at a time instant t, ct is
is to identify an approach to learning the incentive function r
the capacity of robot r at time t, N is the number
success
thatcanbeusedbymaximumweightedbigraphmatchingto
of successfully completed tasks during the operation. We
perform multi-robot task allocation, with performance that
craft the objective function (Eq. (1)) such that it emphasizes
is comparable to or better than reported i) bigraph matching
minimizing the rate of incomplete tasks, i.e., the number
techniquesthatuseexpertheuristics,andii)purelyRLbased
of incomplete tasks divided by the total number of tasks.
solutions. Thus the main contributions of this paper include:
Equations 2 and 3 correspond to the remaining range and
1) Identify the inputs, outputs and structure of the graph
capacity respectively at time t. We express the MRTA-
neuralnetwork(GNN)modelthatwillserveastheincentive
CT as an MDP over a graph to learn policies that along
function; 2) Integrate the GNN-based incentive with the
with a bigraph matching approach yield solutions to this
bigraph matching process in a way that the GNN can be
combinatorial optimization problem, as described next.
trained by policy gradient approaches over simulated MRTA
experiences; 3) Analyze the (dis)similarity of the learned B. Bipartite Graph for MRTA
incentives to that of the expert-derived incentives. A bipartite graph (bigraph) is a graph with two distinct
Paper Outline: The next section summarizes the MRTA and non-overlapping vertex sets such that every edge in
problem, its MDP formulation, and the bigraph representa- the graph connects a vertex from one set to a vertex
tionoftheMRTAprocess.SectionIIIdescribesourproposed in the other set. A weighted bigraph is a special type
GNN architecture for incentive learning and computing the of bigraph in which each edge is assigned a numerical
final action (robot/task allocations). Section IV discusses weight or cost. These weights represent some measure of
numericalexperimentsonMRTAproblemsofdifferentsizes, significance, incentive, affinity, cost, or strength associated
comparinglearningandnon-learningmethodsandanalyzing with the connections between the vertices in the graph.
computing time. Section V concludes the paper. Weighted bigraphs are com-
monly used to model and Tasks(ùëâ!) Robots
II. MRTA-COLLECTIVETRANSPORT(MRTA-CT) solve various real-world prob- (ùëâ")
1
HereweconsidertheMRTACollectiveTransport(MRTA- lems, where the weights on
CT) problem defined in [10]. Given a homogeneous set of edges provide additional infor-
NR robots denoted as R (r ,r ,...,r ) and a set of mation about the relationships 2
1 2 NR
NT tasks represented by VT, the objective is to optimize between entities across the two
task allocation to maximize the number of completed tasks. sets. In this work as shown in 3
There is a central depot that serves as both the starting and Fig. 1, we construct a bigraph
ending points for each robot. Each task, denoted as i‚ààVT, GB = (VB,EB,‚Ñ¶B), where 4
possessesuniquelocationcoordinates,(x ,y ).Additionally, the two types of vertices in
i i Fig.1:Bigraphshowingrobot-
each task has a workload or demand, w i, which may vary VB are from the set of robots, task connections. The Bigraph
over time, and a time deadline, œÑ , by which the task must VR, and the set of tasks, VT. weightsiswrittenasamatrix.
i
be completed (i.e., demand satisfied) to be considered as Here, EB represent the edges
‚Äúdone‚Äù (œà = 1). Each robot has a maximum range, ‚àÜ that connect any vertex or node in VR with a vertex in VT.
max
that limits its total travel distance including return to depot. Here, ‚Ñ¶B(‚àà RNR√óNT) is the weight matrix, where each
Robots also have a predefined maximum payload carrying weight, ‚Ñ¶B ,r ‚àà VR,i ‚àà VT, is associated with the edge
r,i
capacity,C .Arobotstartsitsjourneyfromthedepotwith that connects nodes r and i. In our MRTA representation,
max
afullbatteryandpayload,proceedstotasklocationstopartly this weight ‚Ñ¶B ,r ‚ààVR,i‚ààVT provides a measure of the
r,i
orcompletelyfulfillitsdemands;itreturnstothedepotonce affinity for robot r to perform task i.
it‚Äôs either completely unloaded, running low on battery, or Maximum Weight Matching: Weighted (bigraph) graph
there are no remaining tasks in the environment, whichever matching is a well-studied problem in graph theory and
condition is met first. The recharging process is assumed to combinatorial optimization. The goal is to find a matching
beinstantaneous,suchasthroughabatteryswapmechanism. (a set of non-overlapping edges or one-to-one connections)in a bigraph such that the sum of the weights of the selected robots‚Äô state graph GR. Action Space (A): each action a
edges is maximized or minimized, depending on whether is defined as the index of the selected task, {0,...,NT}
it‚Äôs a maximum-weight or minimum-weight matching, re- with the index of the depot as 0. This action is selected as
spectively.Herewemake useofmaximum-weightmatching a result of the maximum weight matching of the bigraph.
to allocate tasks to the robots. Popular methods for weight State Transition, P (s‚Ä≤|s,a): the transition is an event-
a
matchingforbigraphsincludetheHungarianAlgorithm[29] based trigger. A robot taking action a at state s reaches
and Karp Algorithm [30]. Along with providing provably the next state s‚Ä≤ in a deterministic manner. This definition
optimal matching, Maximum weight matching offers clarity assumes that the policy model encapsulates all processes,
and transparency on how robots are paired with tasks in includingbutnotlimitedtothelearning-derivedmodels,that
the case of MRTA. Effectiveness of this approach however together produces the action to be taken, given the state of
hinges on how well the weight matrix represent the relative the environment. Reward (R): Here a reward of 1/NT is
affinity or value of the robots to select tasks (connected givenduringeachdecision-makinginstance,ifanactivetask
by edges) given the current state of the environment. The (whosedemandhasnotyetbeenfullymet,anddeadlinehas
formulationoftheMDPtolearnhowtogeneratethisweight not passed) is chosen, while a reward of 0 is given if the
matrix given the environment state is described next. depot is chosen. Hence, the maximum possible aggregate
C. MDP over a Graph reward in any episode is 1.
The MDP is defined in an asynchronous decentralized
III. INCENTIVE(WEIGHT)LEARNINGFRAMEWORK
mannerforeachindividualrobot,tocaptureitstaskselection
In our solution approach, we construct a policy network
process,andisexpressedasatuple,<S,A,P ,R>.Here,
a that serves as the incentive generator. Namely, it takes in
we assume full observability, i.e., every robot communicates
the state information during a decision-making instance for
its chosen task with other robots.
a robot and outputs the bigraph weight matrix. This is used
Graph formulation for Tasks: The task space of an by maximum weight matching on the bigraph to yield the
MRTA-CT problem is represented as a fully connected task selection for the deciding robot. This section discusses
graph GT = (VT,ET,‚Ñ¶T), where VT is a set of nodes the policy network and the various steps involved in the
representingthetasks,ET isthesetofedgesthatconnectthe sequential decision-making process.
nodestoeachother,and‚Ñ¶T istheweightedadjacencymatrix
that represents ET. Node i is assigned a 4-dimensional ùõø ùõø! #" " G ‚Ñíra #p =h ùê∑La #p ‚àíla Œ©ci #a n ‚Ñí!(‚àà‚Ñù"!√ó"!) ‚Ä¶ ‚Ä¶
normalized feature vector denoting the task location coor- ùêπ!(‚àà‚Ñù"!√ó$) ùúå-$
Task Graph (‚àà‚Ñù"!√ó"")
dinates, time deadline, and the remaining workload/demand ùõø! Encoder (GCAPS)
i w.e e. i, ghŒ¥ iT
t
= b[ ex ti w, ey ei, nœÑ i t, ww oit] nw odh ee sre ‚Ñ¶i T‚àà ([ ‚àà1,N ‚Ñ¶TT )]. iH
s
e cr oe m, pth ue tee ddg ae
s
Task graph ùõø $" !
ùëô!
(‚àà‚Ñù"!√ó$!) DeM coH dA
e r ‚Ä¶
i,j ùõø!% ‚Ä¶
‚Ñ¶T i,j =1/(1+|Œ¥ iT‚àíŒ¥ jT|),wherei,j ‚àà[1,NT],andexpresses ùõø #% MHA ‚Ä¶
how similar the two nodes are in the graph space. This (‚àà‚Ñùùõø "% "√ó$") Robots Graph Decoder
is a common approach to compute the weighted adjacency Encoder (GCAPS) ùêπ%(‚àà‚Ñù""√ó,) ùúé-$
m pha yt sri ix ca, ld qe usp ai nt te itit eh se
.
Tno hd ee dp egro rep eert mie as trir xepr Des Ten it sing
a
dd ii aff ge or nen alt Rob go rt as p s htate ùõø $% " ùëô% Gr ‚Ñía %ph = L ùê∑a %p ‚àíla c Œ©ia %n ‚Ñí%(‚àà‚Ñù""√ó"") (‚àà‚Ñù"!√ó"" ‚Ä¶)
matrix with elements DT =(cid:80) ‚Ñ¶T ,‚àÄi‚ààVT. Fig.2:TheoverallstructureoftheBiG-CAMpolicy.
i,i j‚ààVT i,j
Figure 2 shows the inputs/outputs and structure of the
D. Task selection
policy model that computes the bigraph weights. The ele-
Graph formulation for Robots: The state of the robots in ments of the weight matrix (‚Ñ¶B ,r ‚àà VR,i ‚àà VT), which
MRTA-CT is represented as a fully connected graph GR = r,i
represents an incentive score for that robot r to pick task
(VR,ER,‚Ñ¶R), where VR is a set of nodes representing the
i at that instance, should be computed as a function of the
robots,ERrepresentthesetofedges,and‚Ñ¶Ristheweighted
task and robot features. Here the policy model is designed
adjacency matrix that represents ER. The number of nodes
tooutputparametersof(independentLogNormal)probability
and edges are NR and NR(NR ‚àí 1), respectively. Every distributions from which the bigraph weights, ‚Ñ¶B , can be
robot node is defined as Œ¥R =[xt,yt,‚àÜt,ct,tnext],‚àÄr ‚ààR, r,i
r r r r r r sampled. Since the state and features of the tasks and robots
where for robot r, xt r,y rt represents its current destination, arebothformulatedasgraphs(GT andGR),weuseGNNsto
‚àÜt represents its remaining range (battery state), ct repre-
r r compute their node embeddings. Along with node features,
sents its remaining capacity of robot, and tnext represents
r thechoiceoftheGNNtypeseekstoalsoembedthestructure
its next decision time. The weights of ‚Ñ¶R are computed
of the task and robot spaces to promote generalizability.
using ‚Ñ¶R = 1/(1 + |Œ¥R ‚àí Œ¥R|), where r,s ‚àà [1,NR].
r,s r s Thesenodeembeddingsarethenusedtocomputetheweights
The degree matrix DR is a diagonal matrix with elements
using Multi-head Attention-based decoders (MHA). We use
DR =(cid:80) ‚Ñ¶R ,‚àÄr ‚ààVR. When a robot r visits a task
r,r s‚ààVR r,s two such decoders to produce a matrix of the mean values
i, the demand fulfilled by the robot r is min(wt,ct).
i r of the bigraph weights and a matrix of the corresponding
ThecomponentsoftheMDParedefinedasfollows:State
standarddeviationvalues.Wecallthisincentivepolicymodel
Space (S): a robot r at its decision-making instance uses
BiGraph-informing Capsule Attention Mechanism or BiG-
a state s ‚àà S, which contains the task graph GT and the
CAM, whose components are described next.ùêπ! ‚Ñí! ‚Ñé
ùêπ!(‚àà‚Ñù"!√ó$)
ùêπ!"#(ùëã,‚Ñí)‚àò# ‚Ñí"ùëä"(#%) Œ£
ùúπùúπ
ùüè
ùüí G Nr oa dp eh s ùúπùüêùúπ ùüë ùõøùõøùõø #! " ‚Ñé
|ùõøùíä|‚Ñé
‚Ñí!
ùëä"(‚Äô%)
ùëì%&(ùëã,‚Ñí)
Concat ùêπ(!(ùëã,‚Ñí)
‚Ñé
ùúé ùêπ*ùêπ!+ ùêπ"ùêπ
ùêπ%(‚àà‚Ñù""√ó$) ‚Ñé
‚Ñé
‚Ñé
‚Ñé
‚Ñé
‚Ñé
N‚Ñí ùõø ùëã ùúé ùí± ùëÉ( ‚Äì‚Äì ‚Äì - ‚Äì=- G S HN N[ Sùõør i e io uga ! gtd m mp ,
h
oùõøeh ebo f"
s
fL ei, ne tda . r oa
o
.p ot dùõø rl u fa de# rc n es]ei roa s dn
o
o e ff s node ùëñ Cùëä a! p EsG nu cr la e op - dbh ea rsed ùêπ!"#(ùëã ùêø,‚Ñí $) ‚àò l% aye‚Ñí rs" ùëäùëä ((( ‚Äô%( )#%) ùëìŒ£ ‚Äô&(ùëã,‚Ñí)Output of layer - ùëô ùëä) ‚Ñé √óùëÉ MHA - -- ‚Äì Q VK Maeu ly ue u r l( e ty ‚àà i -(( h‚àà‚Ñù‚àà e# a‚Ñù‚Ñù d" ## √ó A"%! √ó t√ó ) t% e% ) n) tion M‚Ñé H‚Ñé A-bM aH sA ed Decoder‚Ñé ‚Ñé Matmul ( (‚àà ‚àà‚Ñù ‚Ñùùúéùúå " "O! !" "! !√ó √ór" "# #) )
statistical moment
ùêø) ‚Äì Number of layers
Fig.4:StructureoftheMHA-baseddecoder.
Fig. 3: The overall structure of the GCAPS network. Here, h is the
embeddinglength,andbiastermsareomittedforeaseofrepresentation.
A. GNN-based feature encoder bigraph is constructed, we perform a maximum weight
matching using the Hungarian Algorithm [29] to match
We use Graph Neural Networks (GNNs) for encoding the
robots to tasks. The deciding robot then broadcasts the task
state of the tasks and robots. The GNNs take in a graph
selection information to peer robots.
and compute node embeddings. We consider two separate
encoders(Fig.2)forthetaskandrobotgraphs,namedasthe E. BiG-CAM Policy Training Details
Task Graph Encoder (TGE) and the Robots Graph Encoder TrainingDataset:WetraintheBiG-CAMpolicyonanen-
(RGE). Both TGE and RGE are based on Graph Capsule vironmentwith50tasks,6robots,andadepot.Everyepisode
Convolutional Neural Networks (GCAPS) [31], as shown is a different scenario characterized by the task location,
in Fig. 3. In our prior works [10], [22], [24], GCAPS has demand, time deadline, and depot location. The locations
shownsuperiorcapabilityincapturingstructuralinformation of the tasks and the depot are randomly generated following
compared to GNNs such as Graph Convolutional Networks a uniform distribution ‚àà[0,1] km. We consider every robot
(GCN) and Graph Attention Networks (GAT). Here, both to have a constant speed of 0.01 km/sec. The demand for
the TGE and the RGE take in the corresponding graphs, GT the tasks is an integer drawn randomly between 1 and 10
and GR, and compute the corresponding node embeddings, following a uniform distribution. The time deadline for the
FT (RNT√óh) and FR (RNR√óh), respectively; here h is the tasks is drawn from a uniform distribution ‚àà [165,550]
embeddinglength.Theembeddingsarethenpassedintotwo seconds. The environment settings here are adopted from
decoders to compute the mean weight matrix œÅ ‚Ñ¶B, and its [10], inspired by applications such as multi-robot disaster
corresponding standard deviation œÉ ‚Ñ¶B (Fig. 2). response and materials transport. For both TGE and RGE,
B. Multi-head Attention (MHA) based decoding weusethesamesettingsfortheGCAPSencodersasin[10].
We initialize two MHA-based decoders, to compute the Eight attention heads are used in MHA-based decoders.
mean weights (œÅ ‚Ñ¶B) and the standard deviation (œÉ ‚Ñ¶B). Each Training Algorithm: In order to train the BiG-CAM
decoder takes in the task embeddings FT and the robot policy,weuseProximalPolicyOptimization(PPO)[32].We
embeddings FR and outputs a matrix of size NR √ó NT. allow 5 million steps, with a rollout buffer size of 50000,
The structure of the decoder is shown in Fig. 4. FT is used and a batch size of 10000. The simulation environment is
tocomputethekeyKandvalueV,whiletheasetofqueryQ developed in Python as an Open AI gym environment for
is computed using FR. The output of decoders are matrices ease of adoption. The training is performed on an Intel
for the mean (œÅ ‚Ñ¶B) and the standard deviation (œÉ ‚Ñ¶B). Xeon Gold 6330 CPU with 512GB RAM and an NVIDIA
C. BiGraph Weights Modeled as Probability Distributions A100 GPU using PPO from Stable-Baselines3 [33]. The
The outputs from the decoder (œÅ and œÉ ), which rep- testing is performed only using the CPU. Here, we consider
‚Ñ¶B ‚Ñ¶B
resentthematriceswiththemeanandthestandarddeviation centralizedtraining,wherethecollectedexperienceofallthe
for the bigraph weights, are then used to express NR√óNT robots is used for training a single policy. Implementation is
Lognormal probability distributions, from which the bigraph decentralized, where at any decision-making instance for a
weights (‚Ñ¶B ) are drawn. To encourage exploration during given robot, it executes the BiG-CAM policy to compute
r,i
training, the weights are sampled in an œµ-greedy fashion bigraph weights and runs maximal matching.
(œµ = 0.2); i.e., with œµ probability the weight is randomly IV. EXPERIMENTALEVALUATION
sampled from the corresponding distribution, and in the
A. Baseline Methods:
remainingcases,themeanvalue(fromœÅ )isdirectlyused.
‚Ñ¶B Threedifferentbaselinesareconsidered,abigraphmatch-
During testing, the mean value is always greedily used.
ing approach that uses expert-designed incentive function as
D. Weighted Bigraph Construction edgeweights,anRL-trainedpolicythatdirectlyprovidestask
Similar to [2], we omit the edges that represent an infea- selections for robots, and a feasibility-preserving random
sible combination of robots and tasks, and those connecting walk approach. They are summarized below.
tasks whose demand has been fully met or deadline has Bi-Graph MRTA (BiG-MRTA): BiG-MRTA [2], [7] uses
passed. The remaining edge weights are obtained from the a handcrafted incentive function to compute the weights
computed weight matrix distributions. Once the weighted of the edges connecting robots and tasks, based on which
.‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ .‚Ä¶‚Ä¶‚Ä¶
‚Ñé √óùëÉmaximum matching is performed to decide task selection.
This incentive for robot r to choose task i at a time t is a
product of two terms. The first term measures the remaining
range if the robot chooses and completes task i and returns
to the depot. This term becomes zero if there‚Äôs insufficient
batteryforthereturn.Itrepresentstheremainingpotentialfor
robot r to perform additional tasks after task i. The second
termisanegativeexponentialfunctionofthetimetr needed
i
for robot r to complete task i if chosen next, i.e., before the
(a)ScenarioswithNT =50(st=1)
deadline œÑ . If task i can‚Äôt be completed by robot r before
i
thedeadline,theedgeweight(œâ )becomeszero.Therefore,
ri
the weight œâ of a bigraph edge (r,i) is expressed as:
ri
(cid:40) max(0,l )¬∑exp(cid:16) ‚àítr i(cid:17) if tr ‚â§œÑ
œâ = r Œ± i i (4)
ri
0 Otherwise
wherel =‚àÜt‚àí(d +d ),Œ±isthemaxtime(550seconds),
r r ri i0
d is the distance between the current location/destination
ri
ofrobotr andthelocationoftaski,whiled isthedistance
i0
between the location of task i and the depot. (b)ScenarioswithNT =100(st=2)
Capsule-Attention Mechanism (CapAM): The CapAM
policy network from [24] uses a GCAPS network as GNN
and an MHA-based decoder to directly compute log prob-
abilities for all available tasks given a state for the robot
taking the decision. This method has demonstrated better
performance compared to other encodings such as GCN and
GAT[22],andstandardRL[10].Thispolicyhasbeentrained
with the same settings as that of BiG-CAM, (refer Section
III-E). Previous work on related problems have already (c)ScenarioswithNT =250(st=5)
enlightened on the optimality gap of of BiG-MRTA and
CapAMw.r.tMINLPsolutions[2],[10],andhenceexpensive
MINLP solutions are not generated here.
Feasibility-preserving Random-Walk (Feas-Rnd): is a
myopic decision-making method that takes randomized but
feasibleactions,avoidingconflictsandsatisfyingotherprob-
lem constraints. Feas-Rnd serves as a lower bound to com-
pare the performance of the other methods.
B. Training Convergence: (d)ScenarioswithNT =500(st=10)
From the training history, it was observed that at the
Fig. 5: % task completion for all the methods. Left plots correspond to
end of 5 million steps, BiG-CAM converged to an average scenarioswithsr =1;rightplotscorrespondtoscenarioswithsr =2.
total episodic reward of 0.53, compared to 0.51 achieved by
We implemented the other baseline methods (BiG-MRTA,
CapAM.thistrainingprocessforBiG-CAMtook‚àº21hours
CapAM, and Feas-Rnd) on these same test scenarios for
whileCapAMtook‚àº15hours.Thisincreaseintrainingtime
comparison in terms of % task completion metric. In order
is due to the computation overhead of the maximum weight
to confirm the significance of any performance difference
matching algorithm, and the larger number of trainable
between the methods, we performed the statistical t-test
weights for BiG-CAM compared to CapAM.
pairwise for different settings of s and s , with the null
t r
C. Performance Analysis (% Task completion) hypothesis being the mean of the two sets of samples are
In order to assess the performance of the learned policy thesame.ForBiG-CAM,forscenarioswiths >1ands >1,
r t
ongeneralizabilityandscalability,wedesignedtestscenarios we shrink the size of the bigraph weight to 6√ó50. The 50
with varying numbers of tasks NT and number of robots tasks and 5 peer robots are chosen based on their proximity
NR.Wegeneratedifferentscenariosbasedonasetofscaling to the robot taking decision, thus keeping the computation
factors ST = {1,2,5,10} for tasks and SR = {1,2} for time spent for the maximum weight matching relatively
robots. For every combination of s ‚àà ST and s ‚àà SR we unchanged irrespective of the scenario size.
t r
consider 100 scenarios with number of tasks NT =50√ós , Across all testing scenarios, BiG-CAM, BiG-MRTA and
t
and number of robots NR = 6 √ó s (note again, 50- CapAM provide clearly better task completion rates com-
r
task-6-robot scenarios were used in training). For example, paredtoFeas-Rnd.Comparinggeneralizabilityperformance,
for scenarios with s =2, and s =2, NT=100, and NR=24. i.e.,intestscenariosofsimilarsizeasintraining,weobserve
t rTABLE I: Average total episodic decision computing time (with average
from Fig. 5a (left side) that BiG-CAM and BiG-MRTA
computingtimeperdecision)inseconds
exhibit comparable performance. BiG-MRTA has a slightly
NT NR BiG-CAM BiG-MRTA CapAM Feas-Rnd
better median task completion %, while BiG-CAM provides
6 0.67(0.007) 0.28(0.004) 0.20(0.003) 0.002(2e-5)
50
smaller variance. The t-test yields a p-value of 0.188, indi- 12 1.3(0.007) 0.7(0.006) 0.28(0.002) 0.002(1.6e-4)
12 2.26(0.012) 1.6(0.007) 0.64(0.004) 0.003(1.7e-5)
catingnosignificantdifferencebetweenBiG-CAMandBiG- 100
24 5.47(0.012) 4.7(0.009) 0.92(0.004) 0.005(1.8e-5)
MRTA for these scenarios. In contrast, when compared to 30 12.99(0.02) 23.2(0.07) 3.03(0.009) 0.006(1.8e-5)
250
60 38.15(0.04) 67.10(0.2) 6.39(0.009) 0.013(1.8e-5)
CapAM, BiG-CAM has a slightly higher median (around
60 91.9(0.09) 174.4(0.40) 25.5(0.03) 0.012(1.9e-5)
500
3%) while maintaining a lower standard deviation. The p- 120 192.0(0.11) 464.0(0.90) 35.6(0.03) 0.014(1.9e-5)
value from the t-test is 0.03 (<0.05), indicating a significant
of learning, namely when the time steps are 50K, 100K,
difference between BiG-CAM and CapAM‚Äôs performance.
500K, 1M, 2M, 3M, and 5M. Then we compute the average
In scenarios with s =2 and fewer robots (s =1) (Fig. 5b,
t r Sinkhorn distance between the weight matrices derived from
left side), BiG-CAM performs similarly to BiG-MRTA (p-
thelearnedpolicyandthecorrespondingweightmatrixgiven
value>0.05) while outperforming CapAM (p-value<0.05).
by BiG-MRTA expert-specified incentive function, as shown
However, for a larger number of robots s =2 (Fig. 5b,
t in Fig. 6. We observe that the average Sinkhorn distance
right side), BiG-CAM exhibits slightly inferior performance
between the weight matrices of the two methods for all
comparedtobothBiG-MRTAandCapAM(p-value<0.05for
the states in S decreases until 2M, and then increases. This
bothcases).Forscenarioswiths =5and10andforscenarios
t
observation shows that as the learning progresses the weight
with a lower number of robots (s =1), BiG-CAM performs
r
matrixinitiallygetsmoresimilartothatgivenbytheexpert-
significantly better than both BiG-MRTA and CapAM (p-
specified incentive function (in BiG-MRTA), but later on
values <0.05), while for scenarios with a larger number of
slightly deviates from the expert incentive function.
robots (s =2), BiG-CAM performs slightly poor compared
r
to BiG-MRTA (p-value <0.05), and on par compared to
CapAM for s =5 (p-value>0.05), and better than CapAM
t
for s =10 (Figs. 5c and 5d).
t
BiG-CAM outperforms BiG-MRTA in scenarios with
fewer robots (s =1), while BiG-MRTA excels in scenarios
r
with more robots (s =2). The performance drop of BiG-
r
CAM compared to BiG-MRTA could be partly because of
the forced limiting of the bigraph to the size for which
the policy model in BiG-CAM has been trained. Notably,
Fig.6:ComparisonofSinkhorndistancebetweenthebigraphweightofS.
BiG-CAM exhibits significantly lower variance across all
V. CONCLUSIONS
scenarios compared to both BiG-MRTA and CapAM. BiG- This paper proposed a graph RL approach called BiG-
CAM‚Äôs standard deviation ranges from 0.064 (s t=1, s r=1) CAM to learn incentives or weights for a bigraph repre-
to 0.21 (s t=2, s r=2), while BiG-MRTA‚Äôs standard deviation sentation of candidate robot-task pairing in MRTA, which
spans from 0.88 (s t=1, s r=2) to 0.57 (s t=5, s r=1). is then used by a maximum weight matching method to
Computing time analysis: Computation time is assessed allocate tasks. We considered an MRTA collective transport
using two metrics: the average time for all decisions in an (MRTA-CT)problem,whichwasformulatedasanMDPwith
episode and the time for a single decision, as presented in the state of the tasks and the robots expressed as graphs.
Table I. CapAM which only includes a policy execution is The weights of the task/robot pairing bigraph are sampled
faster than BiG-CAM and BiG-MRTa as expected. Now, be- from distributions computed by a policy network (BiG-
tweenBiG-MRTAandBiG-CAM,scenarioswithfewertasks CAM) that act on the state space graphs, comprises GNN
(NT=50,100) favor BiG-MRTA, since it solely performs encoders and MHA-based decoders, and trained using PPO.
maximumweightmatching.BiG-CAMneedstoalsoperform In testing, BiG-CAM demonstrated comparable or slightly
policyexecutiontocomputethebigraphweights,whichadds better performance relative to BiG-MRTA (that instead uses
toitscomputingtime.However,inscenarioswithmoretasks an expert-crafted incentive to compute bigraph weights) for
(NT =250,500),BiG-CAMissignificantlyfasterthanBiG- scenarios with lower number of robots, and comparable or
MRTA (up to 9 times). This is because BiG-CAM limits the slightly poorer median performance for scenarios with a
bigraph size to 50-task/6-robot based on proximity, while larger number of robots. Compared to both BiG-MRTA and
BiG-MRTA considers the entire task/robot space leading to CapAM (purely GNN for MRTA), BiG-CAM demonstrated
greater cost of the maximum matching process. better robustness w.r.t. task completion rates. In the future,
D. Learned incentives vs expert-derived incentives model alleviating the limitation of fixing the size of the task and
AkeyquestioniswhetherthelearnedpolicyinBiG-CAM robot spaces during training of BiG-CAM could further
produces incentives that are similar to or differ from those improve its relative performance. Future systematic analysis
computed by the expert-specified incentive function in BiG- is also needed to adapt the bigraph size based on the ex-
MRTA. To answer this question, we compute the weight pectedpropagationofdecisioninfluenceacrossthetask/robot
matrix for a set of 1000 states, S = [S ,...S ] in 50- graphs to ensure reliable yet compute-efficient scalability of
1 1000
task/6-robot scenarios, using the policy at different stages the underlying bigraph matching concept.REFERENCES [22] S. Paul and S. Chowdhury, ‚ÄúA scalable graph learning approach
to capacitated vehicle routing problem using capsule networks and
attentionmechanism,‚ÄùinInternationalDesignEngineeringTechnical
[1] Y.MengandJ.Gan,‚ÄúAdistributedswarmintelligencebasedalgorithm
ConferencesandComputersandInformationinEngineeringConfer-
foracooperativemulti-robotconstructiontask,‚Äùin2008IEEESwarm
ence,vol.86236. AmericanSocietyofMechanicalEngineers,2022,
IntelligenceSymposium. IEEE,2008,pp.1‚Äì6.
p.V03BT03A045.
[2] P.GhassemiandS.Chowdhury,‚ÄúMulti-robottaskallocationindisaster
[23] E.V.Tolstaya,J.Paulos,V.R.Kumar,andA.Ribeiro,‚ÄúMulti-robot
response: Addressing dynamic tasks with deadlines and robots with
coverageandexplorationusingspatialgraphneuralnetworks,‚ÄùArXiv,
rangeandpayloadconstraints,‚ÄùRoboticsandAutonomousSystems,p.
vol.abs/2011.01119,2020.
103905,2021.
[24] S.Paul,P.Ghassemi,andS.Chowdhury,‚ÄúLearningscalablepolicies
[3] Y.Huang,Y.Zhang,andH.Xiao,‚ÄúMulti-robotsystemtaskallocation
overgraphsformulti-robottaskallocationusingcapsuleattentionnet-
mechanism for smart factory,‚Äù in 2019 IEEE 8th Joint International
works,‚Äùin2022InternationalConferenceonRoboticsandAutomation
InformationTechnologyandArtificialIntelligenceConference(ITAIC),
(ICRA),2022,pp.8815‚Äì8822.
2019,pp.587‚Äì591.
[25] R.A.Jacob,S.Paul,W.Li,S.Chowdhury,Y.R.Gel,andJ.Zhang,
[4] F. Xue, H. Tang, Q. Su, and T. Li, ‚ÄúTask allocation of intelligent
‚ÄúReconfiguringunbalanceddistributionnetworksusingreinforcement
warehousepickingsystembasedonmulti-robotcoalition,‚ÄùKSIITrans-
learning over graphs,‚Äù in 2022 IEEE Texas Power and Energy Con-
actionsonInternetandInformationSystems(TIIS),vol.13,no.7,pp.
ference(TPEC),2022,pp.1‚Äì6.
3566‚Äì3582,2019.
[26] S. Paul and S. Chowdhury, ‚ÄúA graph-based reinforcement learning
[5] Y.Cao,S.Wang,andJ.Li,‚ÄúTheoptimizationmodelofride-sharing
frameworkforurbanairmobilityfleetscheduling,‚ÄùinAIAAAVIATION
route for ride hailing considering both system optimization and user
2022Forum,2022,p.3911.
fairness,‚Äù Sustainability, vol. 13, no. 2, 2021. [Online]. Available:
[27] P.KrisshnaKumar,J.Witter,S.Paul,H.Cho,K.Dantu,andS.Chowd-
https://www.mdpi.com/2071-1050/13/2/902
hury, ‚ÄúFast decision support for air traffic management at urban air
[6] A.Farinelli,A.Rogers,A.Petcu,andN.R.Jennings,‚ÄúDecentralised mobilityvertiportsusinggraphlearning,‚Äùin2023IEEE/RSJInterna-
coordination of low-power embedded devices using the max-sum tionalConferenceonIntelligentRobotsandSystems(IROS). IEEE,
algorithm,‚Äù2008. 2023,pp.1580‚Äì1585.
[7] P.Ghassemi,D.DePauw,andS.Chowdhury,‚ÄúDecentralizeddynamic [28] P.K.Kumar,J.Witter,S.Paul,K.Dantu,andS.Chowdhury,‚ÄúGraph
task allocation in swarm robotic systems for disaster response: Ex- learningbaseddecisionsupportformulti-aircrafttake-offandlanding
tendedabstract,‚Äùin2019InternationalSymposiumonMulti-Robotand aturbanairmobilityvertiports,‚ÄùinAIAASCITECH2023Forum,2023,
Multi-AgentSystems(MRS),2019,pp.83‚Äì85. p.1848.
[8] M.B.Dias,R.Zlot,N.Kalra,andA.Stentz,‚ÄúMarket-basedmultirobot [29] H. W. Kuhn, ‚ÄúThe hungarian method for the assignment problem,‚Äù
coordination:Asurveyandanalysis,‚ÄùProceedingsoftheIEEE,vol.94, NavalResearchLogisticsQuarterly,vol.2,no.1-2,pp.83‚Äì97,1955.
no.7,pp.1257‚Äì1270,2006. [30] J. E. Hopcroft and R. M. Karp, ‚ÄúAn n5ÀÜ/2 algorithm for maximum
[9] N. Mazyavkina, S. Sviridov, S. Ivanov, and E. Burnaev, ‚ÄúReinforce- matchings in bipartite graphs,‚Äù SIAM Journal on Computing, vol. 2,
ment learning for combinatorial optimization: A survey,‚Äù Computers no.4,pp.225‚Äì231,1973.
&OperationsResearch,vol.134,p.105400,2021. [31] S. Verma and Z. L. Zhang, ‚ÄúGraph capsule convolutional neural
[10] S. Paul, W. Li, B. Smyth, Y. Chen, Y. Gel, and S. Chowdhury, networks,‚Äù2018.
‚ÄúEfficient planning of multi-robot collective transport using graph [32] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
reinforcement learning with higher order topological abstraction,‚Äù ‚ÄúProximalpolicyoptimizationalgorithms,‚Äù2017.
arXivpreprintarXiv:2303.08933,2023. [33] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and
[11] K. Jose and D. K. Pratihar, ‚ÄúTask allocation and collision-free path N. Dormann, ‚ÄúStable-baselines3: Reliable reinforcement learning
planning of centralized multi-robots system for industrial plant in- implementations,‚Äù Journal of Machine Learning Research, vol. 22,
spectionusingheuristicmethods,‚ÄùRoboticsandAutonomousSystems, no. 268, pp. 1‚Äì8, 2021. [Online]. Available: http://jmlr.org/papers/
vol.80,pp.34‚Äì42,2016. v22/20-1364.html
[12] P. Toth and D. Vigo, Vehicle routing: problems, methods, and appli-
cations. SIAM,2014.
[13] D.Cattaruzza,N.Absi,andD.Feillet,‚ÄúVehicleroutingproblemswith
multipletrips,‚Äù4OR,vol.14,no.3,pp.223‚Äì259,2016.
[14] E. Schneider, E. I. Sklar, S. Parsons, and A. T. O¬®zgelen, ‚ÄúAuction-
basedtaskallocationformulti-robotteamsindynamicenvironments,‚Äù
inConferenceTowardsAutonomousRoboticSystems. Springer,2015,
pp.246‚Äì257.
[15] M. Otte, M. J. Kuhlman, and D. Sofge, ‚ÄúAuctions for multi-robot
taskallocationincommunicationlimitedenvironments,‚ÄùAutonomous
Robots,vol.44,no.3,pp.547‚Äì584,2020.
[16] H.Mitiche,D.Boughaci,andM.Gini,‚ÄúIteratedlocalsearchfortime-
extendedmulti-robottaskallocationwithspatio-temporalandcapacity
constraints,‚ÄùJournalofIntelligentSystems,2019.
[17] P. Vansteenwegen, W. Souffriau, G. Vanden Berghe, and D. Van
Oudheusden,‚ÄúIteratedlocalsearchfortheteamorienteeringproblem
with time windows,‚Äù Computers & Operations Research, vol. 36,
no. 12, pp. 3281‚Äì3290, 2009, new developments on hub location.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S030505480900080X
[18] S. Ismail and L. Sun, ‚ÄúDecentralized hungarian-based approach for
fast and scalable task allocation,‚Äù in 2017 International Conference
onUnmannedAircraftSystems(ICUAS). IEEE,2017,pp.23‚Äì28.
[19] W. Kool, H. Van Hoof, and M. Welling, ‚ÄúAttention, learn to solve
routing problems!‚Äù in 7th International Conference on Learning
Representations,ICLR2019,2019.
[20] Y.KaempferandL.Wolf,‚ÄúLearningthemultipletravelingsalesmen
problem with permutation invariant pooling networks,‚Äù ArXiv, vol.
abs/1803.09621,2018.
[21] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, ‚ÄúLearning
combinatorial optimization algorithms over graphs,‚Äù in Advances in
NeuralInformationProcessingSystems,2017,pp.6348‚Äì6358.