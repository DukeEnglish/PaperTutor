Iterative Thresholding for Non-Linear Learning in the
Strong ǫ-Contamination Model
Arvind Rathnashyam Alex Gittens
RPI Math, rathna@rpi.edu RPI CS, gittea@rpi.edu
Abstract
Wederiveapproximationboundsforlearningsingleneuronmodelsusingthresholdedgradientdescent
when both the labels and the covariates are possibly corrupted adversarially. We assume (x,y) ∼ D
satisfy
y=σ(w∗ ·x)+ξ,
whereσ isanonlinearactivationfunction,thenoiseξ issampledfromN(0,ν2),andthecovariatevector
x is sampled from a sub-Gaussian distribution. We study sigmoidal, leaky-ReLU, and ReLU activation
functionsandderiveaO(νpǫlog(1/ǫ))approximationboundinℓ 2-norm,withsamplecomplexityO(d/ǫ)
and failure probability e−Ω(d).
We also study the linear regression problem, where σ(x) = x. We derive a O(νǫlog(1/ǫ)) approxi-
mation bound,improvingupon theprevious O(ν) approximation boundsfor thegradient-descent based
iterativethresholdingalgorithms of Bhatiaet al. (NeurIPS2015) and Shenand Sanghavi(ICML 2019).
Our algorithm has a O(polylog(N,d)log(R/ǫ)) runtime complexity when kw∗ k ≤ R, improving upon
2
theO(polylog(N,d)/ǫ2) runtimecomplexity of Awasthi et al. (NeurIPS2022).
4202
peS
5
]LM.tats[
1v30730.9042:viXra1 Introduction
The learning of the parameters of Generalized Linear Models (GLMs) [Awasthi et al., 2022,
Diakonikolas et al., 2019,Fischler and Bolles, 1981, Li et al., 2021,Osama et al., 2020]andlinearregression
models [Bhatia et al., 2017, Mukhoty et al., 2019] under the Huber ǫ-contamination model is well-studied.
Efficient algorithms for robust statistical estimation have been studied extensively for problems such as
high-dimensional mean estimation [Cheng et al., 2020, Prasadet al., 2019] and Robust Covariance Estima-
tion [Cheng et al., 2019, Fan et al., 2018]; see Diakonikolas and Kane [2023] for an overview. Along these
lines, interest has developed in the development of robust gradient-descent based approaches to machine
learning problems [Diakonikolas et al., 2019, Prasad et al., 2018]. This work advances this line of research
by providinggradient-descentbasedapproachesfor learning single neuronmodels under the strong contam-
ination model.
Definition1(Strongǫ-ContaminationModel). Givenacorruptionparameter 0 ǫ<0.5,anddatasamples
≤
(x ,y ) for i [N], an adversary is allowed to inspect all samples and modify ǫN samples arbitrarily. We
i i
∈
define the corrupted dataset X = P Q where P contains the remaining target samples and Q contains the
∪
samples modified by the adversary. The algorithm is then given the ǫ-corrupted dataset X as training data.
Current approaches for robust learning across various machine learning tasks often use gradient descent
overarobustobjective(seee.g. TiltedEmpiricalRiskMinimization(TERM)[Li et al.,2021]). Theserobust
objectivestendtonotbeconvexandthereforearedifficulttoobtainstrongapproximationboundsforgeneral
classes of models. Another popular approach is filtering, where at each iteration of training, points deemed
to be as outliers are removed from training (see e.g. SEVER [Diakonikolas et al., 2019]). However, filtering
algorithms such as SEVER require careful parameter selection to be useful in practice.
Iterative Thresholding is a popular framework for robust statistical estimation that was introduced in
the 19th century by Legendre [Legendre, 1806]. Part of its appeal lies in its simplicity, as at each iteration
of training we simply ignore points with error above a certain threshold. Despite the venerablity of this
approach,itwasnotuntilrecentlythatsomeiterativethresholdingalgorithmswereproventodeliverrobust
parameter estimates in polynomial time. One of the first theoretical results in the literature proving the
effectiveness of iterative thresholding considers its use in estimating the parameters of regression models
under additive adversarialcorruptions.
Theorem 2 (Theorem5inBhatia et al.[2015]). Let X be a sub-Gaussian data matrix, and y=X w +b
⊤ ∗
where b is the additive and possibly adversarial corruption. Then there exists a gradient-descent algorithm
such that kw(t) −w
∗ k2
≤ε after t=O log √1 nkb εk2 iterations.
(cid:16) (cid:16) (cid:17)(cid:17)
The algorithm referred to in Theorem 2 uses gradient-descent based iterative thresholding, exhibits a
logarithmic dependence on b , and is applicable to the realizable setting, i.e. there must be no stochastic
2
k k
noiseiny. Morerecently,Awasthi et al.[2022]studiedtheiterativetrimmedmaximumlikelihoodestimator.
In their algorithm,at each step they find w which maximizes the likelihood of the samples in the trimmed
∗
set.
Theorem 3 (Theorem 4.2 in Awasthi et al. [2022]). Let X = (x ,y ) n be the data generated by a
{ i i }i=1
Gaussian regression model defined as y =w x +η where η (0,ν2) and x are sampled from a sub-
i ∗ i i i i
· ∼N
Gaussian distribution with second-moment matrix I. Suppose the dataset has ǫ-fraction of label corruption
and n=Ω d+log(1/δ) . Then there exists an algorithm that returns w such that with probability 1 δ,
ǫ2 −
(cid:16) (cid:17)
w w =O(νǫlog(1/ǫ)).
k − ∗ k2 b
Our first result recoversTheorem 3 as a special case and also allows for vector targets. Furthermore,we
b
obtain the approximation in time O(poly(N,d)log(R/ǫ)), improving upon the O(polylog(N,d)/ǫ2) runtime
complexity of [Awasthi et al., 2022].
1.1 Contributions
Our main contribution consists of algorithms and corresponding approximation bounds for gradient-based
iterative thresholding for several non-linear learning problems (Algorithm 2) and for multitarget linear
1regression (Algorithm 1). Our proof techniques extend [Bhatia et al., 2015, Shen and Sanghavi, 2019,
Awasthi et al., 2022], as we suppose the adversary also corrupts the covariates. To our knowledge, we are
the first to provide guarantees on the performance of iterative thresholding for learning non-linear models,
outside of generalized linear models [Awasthi et al., 2022], under the strong contamination model.
Table1: Summaryofrelatedworkoniterativethresholdingalgorithmsforlinearregressionunderthestrong
ǫ-contamination model, and our contributions. We assume the target data is sampled from a centered sub-
Gaussian distribution with second-moment matrix I, sub-Gaussian proxy Γ - I, and dimension d. We
assume the variance of the optimal estimator is ν.
Reference Approximation Runtime Algorithm
Bhatia et al. [2015] O(ν) O Nd2log 1 kb k2 Full Solve
√n ǫ
Shen and Sanghavi [2019] O(ν) O(cid:16) Nd2log(cid:16) kw∗ k2 (cid:17)(cid:17) Gradient Descent
ν
Awasthi et al. [2022] O(νǫlog(1/ǫ)) O(cid:16) Nd2+(cid:16)d3 1(cid:17)(cid:17) Full Solve
νǫ2
Corollary 8 O(νǫlog(1/ǫ)) O (cid:0)(cid:0)Nd2log k(cid:1)w ν(cid:0)ǫ∗ k (cid:1)(cid:1) Gradient Descent
(cid:16) (cid:16) (cid:17)(cid:17)
Table 1summarizestheapproximationandruntimeguaranteesforlinearregressionprovidedinthiswork
andinthe literature. Comparingto Bhatia et al. [2015],our runtime does notdepend onthe ℓ normofthe
2
labelcorruption,whichcanbe made arbitrarilylargeby the adversary. We extend uponShen and Sanghavi
[2019] by putting dependence on ǫ with a small logarithmic factor, improving the error bound significantly.
We also offer a significant run-time improvement over the study in Awasthi et al. [2022], from O(1/ε2) to
O(log(R/ε)), where w R.
∗ 2
k k ≤
Table 2: Our results and Distributional assumptions for learning different neuron activation functions. In
the approximation bounds, we assume x where is a sub-Gaussian distribution with second-moment
∼ D D
matrix Σ and sub-Gaussian proxy Γ - Σ. We define κ(Σ) = λ (Σ)/λ (Σ) where E [xx ] = Σ,
max min x ⊤
for all x and t [T] we have σ (w(t) x) γ and σ (w x) γ, for any x,y R∼D it holds that
′ ′ ∗
∼ D ∈ · ≥ · ≥ ∈
σ(x y) σ x y , and ν is the variance of the optimal estimator.
lip
− ≤k k | − |
Reference Approximation Neuron Covariate Distribution
Theorem 7 O(νǫlog(1/ǫ)) Linear Sub-Gaussian
Theorem 12 O γ 2 σ 2 νκ(Σ) ǫlog(1/ǫ) Sigmoid Bounded Sub-Gaussian
− k klip
Theorem 13 (cid:16)O γ 2νκ(Σ) ǫplog(1/ǫ) (cid:17) Leaky-ReLU Sub-Gaussian
−
Theorem 15 (cid:16)O νκ(Σ) pǫlog(1/ǫ) (cid:17) ReLU L L Hypercontractive
4 2
−
(cid:16) p (cid:17)
Table 2 summarizes our results on learning single neuron models. The bounded assumption in
Theorem 12isnecessarytogivealowerboundonσ (w x)forx andwisequaltow(t) foranyt [T]or
′
· ∼D ∈
w . InTheorem 15,theL L hypercontractiveassumptionallowsustoboundtheminimumeigenvalueof
∗ 4 2
→
the samplecovariancematrixinthe intersectionofhalfspacsedefinedbyxx 1 w x 0 1 w(t) x 0
⊤ ∗
· { · ≥ }· { · ≥ }
for any t [T]. In general, our nonlinear learning algorithms have approximation error on the order of
∈
O( ǫlog(1/ǫ)).
All our main results allow corruption to be present in both the covariates and the labels. We are able
p
to show with only knowledge of the minimum and maximum eigenvalues of the sample covariance matrix,
iterative thresholding is capable of directly handling corrupted covariates. In comparison, the algorithm
of Awasthi et al. [2022] considers corruption only in the labels; they extend it to handle corruption in the
covariates by preprocessing the covariates using the near-linear filtering algorithm of Dong et al. [2019].
Paper Outline: In Section 2, we give the mathematical notation used in the main body of the paper
before discussing the literature on iterative thresholding for robust learning. In Section 3, we present our
formal results and a proof sketch for the learning of linear and non-linear neurons. We defer all proofs of
our main results to Appendices A and B.
22 Preliminaries
2.1 Mathematical Notation and Background
Notation. We use [T] to denote the set 1,2,...,T . We say y .x or y =O(x) if there exists a constant
C such that y Cx. We say y &x or y ={ Ω(x) if the} re exists a constant C s.t. y Cx. We define Sd 1 as
−
the d 1-dime≤ nsional sphere x Rd : x = 1 . We denote the Hadamard prod≥ uct between two vectors
2
− { ∈ k k }
of the same size as x y; that is, (x y) =x y .
i i i
◦ ◦
Matrices. ForamatrixA,letλ (A)andλ (A)representthemaximumandminimimumeigenvalues
max min
of A, respectively. Let σ (A) denote the ith largest singular values of A; as such, σ (A) σ (A)
i 1 2
≥ ≥ ··· ≥
σ (A). The trace ofa squaren n matrix is givenby tr(A)= σ (A). We use the following matrix
nom r∧ mn s for a matrix A Rm n: × i ∈[n] i
∈ × P
Spectral Norm: A = max Ax =σ (A) (1)
2 2 1
k k x Sn−1k k
∈
Frobenius Norm: kA k2
F
=tr(A⊤A)= σ i2(A). (2)
i ∈X[m ∧n]
Probability. We now discuss the probabilitistic concepts used in this work. We consider the general
sub-Gaussian design, which is prevalent in the study of robust statistics (see e.g. [Awasthi et al., 2022,
Bhatia et al., 2015, Jambulapati et al., 2020]).
Definition 4 (Sub-Gaussian Distribution). We say a vector x is sampled from a sub-Gaussian distribution
with second-moment matrix Σ and sub-Gaussian proxy Γ if, for any v Sd 1,
−
∈
t2 Γ
E [exp(tx v)] exp k k2 t R.
x · ≤ 2 ∀ ∈
∼D (cid:18) (cid:19)
A scalar random variable X is sub-Gaussian if there exists K >0 such that for all p N,
∈
X d =ef (EX p)1/p K√p.
k
kLp
| | ≤
Sub-Gaussiandistributions are convenientto workwith in robuststatistics as the empiricalmeanof any
subset of a set of i.i.d realizations of sub-Gaussian random variables is close to the mean of the distribution
(see e.g. Steinhardt et al. [2018]). Sub-Gaussian distributions have tails that decay exponentially, i.e. at
least as fast as Gaussian random variables. In Table 2 we introduce L L hypercontractivity, which we
4 2
→
will formally define here.
Definition 5. A distribution is L L hypercontractive if there exists a constant L such that
4 2
D →
E x 4 L E x 2 =Ltr(Σ).
x k k2 ≤ x k k2
∼D ∼D
We defer more technical preliminaries to the relevant proofs in the Appendix.
2.2 Related Work
Iterative thresholding for robust statistical estimation dates back to 1806 by Legendre Legendre [1806].
Iterative thresholding has been studied theoretically and applied empirically to various machine learning
problems including linear regression, GLMs, and generative adversarial networks (GANs) [Bhatia et al.,
2015, Hu et al., 2023, Mukhoty et al., 2019]. However, theoretical guarantees for its efficacy in learning
nonlinear models are sparse and not sufficiently strong to justify practical usage of iterative thresholding.
We first introduce the statistical idea of a breakdown point. The breakdown point is defined as the
smallest fraction of observations that can be replaced with arbitrary values to cause an estimator to take
on arbitrarily large incorrect values. The algorithms presented in this paper have breakdown point Ω(1).
This is an improvement over the robust algorithm given in Chen et al. [2013] which has breakdown point
Ω(1/√d). Recent papers on iterative thresholding (see e.g. [Bhatia et al., 2015, Shen and Sanghavi, 2019,
Awasthi et al., 2022] also have breakdown point Ω(1).
3Bhatia et al.[2015]studyiterativethresholdingforleastsquaresregression/sparserecovery. Inparticu-
lar,oneoftheircontributionsisagradientdescentalgorithm,Torrent-GD,applicablewhenthecovariates
are sampled from a sub-Gaussian distribution. Their approximation bound ( Theorem 2) relies on the fact
that λ (Σ) = λ (Σ), so that with sufficiently large sample size and sufficiently small corruption pa-
min max
rameter ǫ, the condition number κ(X) approaches 1. Bhatia et al. [2015] also provide guarantees on the
performance of a full solve algorithm, Torrent-FC, which after each thresholding step to obtain (1 ǫ)N
−
samples sets w(t) to be the minimizer of the squaredloss overthe selected (1 ǫ)N points. They study this
−
algorithmin the presence ofbothadversarialandintrinsic noise. Their analysisguaranteesO(ν) errorwhen
the intrinsic noise is sub-Gaussian with sub-Gaussian norm O(ν).
Shen and Sanghavi[2019] study iterativethresholdingforlearninggeneralizedlinear models(GLMs). In
boththelinearandnon-linearcase,theiralgorithmsexhibitlinearconvergence. Theirresultsimplyabound
ofO(ν)inthelinearcase. Theyfurtherprovideexperimentalevidenceofthesuccessofiterativethresholding
when applied to neural networks.
Morerecently,Awasthi et al.[2022]studiedtheiterativetrimmedmaximumlikelihoodestimatorforGen-
eral Linear Models. Similar to Torrent-FC, their algorithm solves the MLE problem over the data kept
after eachthresholdingstep. They provethe best knownbounds for iterative thresholdingalgorithmsin the
linear regression case, O(νǫlog(1/ǫ)). The algorithm studied by Awasthi et al. [2022] natively handles cor-
ruptionsinlabels,andtohandlethecaseofcorruptedvariates,theyfirstrunanear-linearfilteringalgorithm
from Dong et al. [2019] to obtain covariates that are sub-Gaussian with close to identity covariance.
3 Iterative Thresholding for gradient-based learning
Inthissectionweintroduceouralgorithmsforiterativethresholdinggradient-basedrobustlearningoflinear
and non-linear models. We start with the simple case of regression with multiple targets, as this provides
a simple introduction to and instantiation of our general proof technique. As a corollary, we find a result
regardinglinear regressionin the sub-Gaussiansetting without covariatecorruption;this result is compared
with the existing literature [Awasthi et al., 2022, Bhatia et al., 2017, Shen and Sanghavi, 2019]. Next, we
consider the learning of non-linear neurons. This results in a suite of novel results regarding the use of
iterative thresholding gradient-based learning that are incomparable with the existing literature.
3.1 Warm-up: Multivariate Linear Regression
Wewillfirstpresentourresultsforthewell-studiedproblemoflinearregressionintheHuber-ǫcontamination
model. Our results will extend the results in Bhatia et al. [2017, Theorem 5] and Awasthi et al. [2022,
Lemma A.1] by including covariate corruption without requiring a filtering algorithm, allowing variance in
the optimal estimator, and accommodating a second-moment matrix for the uncorrupted data that is not
the identity. The lossfunction forthe multivariate linearregressionproblemforW RK d, X Rd n,and
× ×
Y RK n is ∈ ∈
×
∈
(W;X,Y)= WX Y 2.
L k − kF
We furthermore define (W;X,Y) = 1 (W;X,Y) as the empirical risk function. We will first give
R (1 ǫ)NL
some notation prior to presenting the alg−orithm.
Definition 6 (Hard Thresholding Operator). For any vector x Rn, define the order statistics as (x)
(1)
∈ ≤
(x) ... (x) , the hard thresholding operator is given as
(2) (n)
≤
HT(x;k)= i [n]:(x) (x) ,...,(x) .
i (1) (k)
∈ ∈
(cid:8) (cid:8) (cid:9)(cid:9)
WiththehardthresholdingoperatorfromDefinition 6inhand,wearenowreadytopresentouralgorithm
for multi-linear regression.
Runtime. In each iteration we calculate the ℓ error for N points, in total O(Nd). For the Hard Thresh-
2
olding step, it suffices to find the n(1 ǫ)-th largestelement, we can run a selection algorithmin worst-case
−
time O(NlogN), then partition the data in O(N). The run-time for calculating the gradient and updating
4Algorithm 1 Deterministic Gradient Descent Iterative Thresholding for Multi-Linear Regression
input: Possibly corrupted X Rd N with outputs Y RK N, corruption parameter ǫ = O(1), and
× ×
∈ ∈
x B for all i Q
i
ok utk p≤ ut: O(νǫ B∈ Klog(1/ǫ)) Approximate solution W RK d to minimize W W
× ∗ F
− ∈ k − k
1: W(0) 0
← p
2: η 0.1κ(Σ)
3:
T←
O κ2(Σ)log
kW∗
kF
← ε
4: for t (cid:16)[T] do (cid:16) (cid:17)(cid:17)
∈
5: ζ i(t) = kW(t)x i −y i k2 ∀i ∈[N] ⊲ Calculate
L
for each sample
6: S(t) HT(ζ(t),(1 ǫ)N) ⊲ See Definition 6
← −
7: W(t+1) W(t) η (W(t);S(t)) ⊲ Gradient Descent Update
← − ∇R
return: W(T)
w(t) is dominated by the matrix multiplication in X X which can be done in O(Nd2). Then consid-
S(t) S⊤(t)
ering the choice of T, we have the algorithm runs in time O Nd2log
kw∗
k2 to obtain O(νǫlog(1/ǫ))
νǫ
ℓ 2-approximationerror. (cid:16) (cid:16) (cid:17)(cid:17)
Theorem 7. Let X = [x ,...,x ] Rd N be the data matrix and Y = [y ,...,y ] RK N be the
1 n ⊤ × 1 n ×
∈ ∈
output, such that for i P, x are sampled from a sub-Gaussian distribution second-moment matrix Σ and
i
∈
sub-Gaussian proxy Γ, and x B for j Q. Suppose y = W x + e where e (0,σ2I) for
j i ∗ i i i
k k ≤ ∈ ∼ N
all i P. Then after O(κ(Σ)log( W /ε)) gradient descent iterations, N = Ω d/ǫ2 , and learning rate
∗ F
∈ k k
η =0.1λ 2 (Σ), with probability exceeding 1 3Te Ω(d), Algorithm 1 returns W(T) such that
−max − − (cid:0) (cid:1)
W(T) W ε+O νǫ KBlog(1/ǫ) .
∗ F
k − k ≤
(cid:16) p (cid:17)
We are able to recover the result of Lemma 4.2 in Awasthi et al. [2022] when the covariates (corrupted
and un-corrupted) are sampled from a sub-Gaussian distribution with second-moment matrix I and Γ-I.
The full solve algorithm studied in Awasthi et al. [2022] returns a O(νǫlog(1/ǫ)) in time O 1 Nd2+d3
ǫ2
and the same algorithm studied in Bhatia et al. [2015], Torrent-FC otains O(ν) approximation error
(cid:0) (cid:0) (cid:1)(cid:1)
in run-time O log 1 kb k (Nd2+d3) , with the gradient descent based approach, we are able to
√nνǫlog(1/ǫ)
improve the ru(cid:16)ntim(cid:16)e to O log k(cid:17)w∗ k Nd2 f(cid:17)or the same approximation bound. By no longer requiring the
νǫ
full-solve, we are able to r(cid:16)emov(cid:16)e supe(cid:17)r-line(cid:17)ar relation to d. In comparison to Bhatia et al. [2015], we do not
have dependence on the noise vector b, which can have very large norm in relation to the norm of w . Our
∗
proofis alsoa significantimprovementoverthe presentationgivenin Lemma 5 ofShen and Sanghavi[2019]
as under the same conditions, we give more than the linear convergence, but we show linear convergence is
possible on any second-moment matrix of the good covariates and covariate corruption, and then develop
concentration inequality bounds to match the best known result for iteratived trimmed estimators. We will
formalize our results into a corollary to give a more representative comparison in the literature.
Corollary 8. Let X = [x ,...,x ] Rd N be the data matrix and y = [y ,...,y ] RN be the output,
1 n ⊤ × 1 n
∈ ∈
such that x are sampled from a sub-Gaussian distribution with second-moment matrix I and sub-Gaussian
i
proxy I and y
i
= w
∗
·x i+ξ
i
where ξ
i
∼
N(0,ν2) for i
∈
P. Then after O log kw ε∗ k2 gradient descent
iterations, sample size N = Ω d+log(1/δ) , and learning rate η = 0.1, w(cid:16) ith p(cid:16) robabili(cid:17) ty(cid:17) exceeding 1 δ,
ǫ −
Algorithm 1 returns w(T) such t(cid:16)hat (cid:17)
w(T) w ε+O νǫ KBlog(1/ǫ) .
∗ 2
k − k ≤
(cid:16) p (cid:17)
Suppose for i Q, x are sampled from a sub-Gaussian distribution with sub-Gaussian Norm K and second-
i
∈
moment matrix I. Then,
w(T) w ε+O(νǫlog(1/ǫ)).
∗ 2
k − k ≤
5ThesecondrelationgiveninCorollary 8matches the bestknownboundfor robustlinearregressionwith
iterative thresholding. The first relation given in Corollary 8 is the extension to handle second-moment
matrices which do not have unitary condition number as well as corrupted covariates.
3.2 Activation Functions
We first give properties of the non-linear functions we will be learning. All functions we henceforth study
will have some subset of the below listed properties.
Property 9. σ is a continuous, monotically increasing, and differentiable almost everywhere.
Property 10. σ is Lipschitz, i.e. σ(x) σ(y) σ x y .
lip
| − |≤k k | − |
Property 11. For any x 0, there exists γ >0 such that inf σ (z) γ >0.
z x ′
≥ | |≤ ≥
Sigmoidfunctionssuchastanhandsigmoidandtheleaky-ReLUfunctionsatisfyProperties9,10,and11.
Property 11 does not hold for the ReLU function, and therefore we require stronger conditions for our
approximation bounds to hold.
3.3 Learning Sigmoidal Neurons
Wenowstudytheproblemofminimizingtheℓ lossforsigmoidaltypeneurons. Forasingletrainingsample
2
(x ,y ) , the loss is given as follows,
i i
∼D
(w;x ,y )=(σ(w x ) y )2.
i i i i
L · −
Algorithm 2 Gradient Descent Iterative Thresholding for Learning a Non-linear Neuron
input: Possibly corrupted X Rd N with outputs y RN, activation function ν, corruption parameter
×
∈ ∈
ǫ=O(1), and small constant α.
output: ν ǫlog(1/ǫ) Approximate solution w Rd to minimize w w .
∗ 2
− ∈ k − k
1: w(0) ∼pBd(α kw ∗ k)
2: η 0.1κ −2(Σ)
3:
T←
O κ2(Σ)log
kw∗
k2
← ε
4: for t (cid:16)[T] do (cid:16) (cid:17)(cid:17)
∈
5: ζ i(t) =(σ(x ⊤i w(t)) −y i)2 ∀i ∈[N] ⊲ Calculate
L
for each point
6: S(t) HT(ζ(t),(1 ǫ)N) ⊲ See Definition 6
← −
7: w(t+1) w(t) η (w(t);S(t)) ⊲ Gradient Descent Update
← − ∇R
return: w(T)
Theorem 12. Let X = [x ,...,x ] Rd N be the data matrix and y = [y ,...,y ] be the output, such
1 N ⊤ × 1 n
∈
that x are sampled from a sub-Gaussian distribution with second-moment matrix Σ and sub-Gaussian proxy
i
Γ, and y = σ(w x ) + ξ for ξ (0,ν2) for all i P. Suppose the activation function, satisfies
i ∗ i i i
· ∼ N ∈
Properties 9, 10, and 11. Then after O(κ(Σ)log(R/ε)) (where w R) gradient descent iterations, then
∗ 2
k k ≤
with probability exceeding 1 δ,
−
kw(T) −w
∗ k2
≤O γ −2λ−m1 in(Σ) kσ k2 lipǫ Blog(1/ǫ) +O γ −2 kσ k2 lipνκ(Σ) ǫlog(1/ǫ) .
(cid:16) p (cid:17) (cid:16) p (cid:17)
when N =Ω d+log(1/δ) .
ǫ2
(cid:16) (cid:17)
The result is slightly weaker than the approximation bound achieved in Theorem 7 by a square root
factor. Intheproofforourlinearregressionresult,weareabletoleveragetheconsistencyoflinearregression
estimators, i.e. as N , we have argmin (w, ) argmin E [ (w, )]. Proof.[Sketch] We will
→ ∞ wL D → w D L D
6give a general sketch of the proof for our sigmoidal neuron result, deferring the more technical ideas to the
proof in the appendix. For any t [T], we have
∈
w(t+1) w w(t) η (w(t);S(t)) w
∗ ∗ 2
k − k≤k − ∇R − k
w(t) w η (w(t);S(t) P) + η (w(t);S(t) Q) . (3)
∗ 2
≤k − − ∇R ∩ k k ∇R ∩ k
I II
In the above, the second relation| follows from th{ezlinearity of the}loss|function,{wzhich gives}
(w(t);S(t))= (w(t);S(t) P)+ (w(t);S(t) Q),
∇R ∇R ∩ ∇R ∩
and then applying the triangle inequality. We upper bound I through its square,
I2 = kw(t) −w ∗ k2 2−2η ·hw(t) −w ∗, ∇R(w(t);S(t) ∩P) i+η2 ·k∇R(w(t);S(t) ∩P) k2.
I1 I2
In this step the finer details of th|e proof for leaky{-zReLUand ReLU}will|differ, howe{vzerthe struc}ture remains
the same. We will prove there exists a constant c >0 such that
1
I (1 2ǫ)c w(t) w 2 c w(t) w ,
1 ≥ − 1 k − ∗ k2− 3 k − ∗ k2
where c is a term that is dependent on the variance of the Gaussian noise, one of our contributions is that
3
c = O(ν ǫlog(1/ǫ)) when N sufficiently large for the activation functions studied in the text. Next, an
3
application of Peter-Paul’sinequality1 gives us,
p
I
1
≥((1 −2ǫ)c
1
−c 3c 1) kw(t) −w
∗
k2 2−c 3c−11.
We next show there exists a positive constant c , such that
2
I2 (1 ǫ)c w(t) w 2.
2 ≤ − 2 k − ∗ k2
Then, solving a simple quadratic equation, we have that η2C ηc c for a c (0,1) when we choose
2 1 3 3
≤ ∈
η c1c3 and we are able to eliminate the norm of the gradient squared term. We must now control the
≤ c2
corrupted gradient term. The key idea is to note that from the optimality of the sub-quantile set,
(w(t);x ,y ) (w(t);x ,y ) (4)
i i i i
L ≤ L
i ∈SX(t) ∩Q i ∈XP \S(t)
and S(t) Q = P S(t) . We then prove the existence of a constant c such that,
4
| ∩ | | \ |
II ǫc w(t) w 2
≤ 4 k − ∗ k2
Then, combining our results, we end up with a linear convergence of the form,
kw(t+1) −w
∗ k2
≤kw(t) −w
∗
k2(1 −η(1 −2ǫ)c 1+ηǫc 4)+c 3c−11
We obtain a bound that is of the form,
w(t+1) w ∗ 2 w(t) w ∗ 2(1 λ)+E
k − k ≤k − k −
Then, we find that
w(t) w ∗ 2 w(0) w ∗ 2(1 λ)t+ (1 λ)kE
k − k ≤k − k − −
kX∈[t]
We then find asymptotically, the second term converges to λ 1E. Then, it suffices to find T such that,
−
w(T) w λ 1E
∗ 2 −
k − k ≤
1Peter-Paul’s inequality states that for any p,q > 1 such that 1 + 1 = 1, then for every t, we have ab ≤ tpap + t−qbq .
p q p q
ConsiderYoung’sInequality andreplaceawithatp andbwithbt−p.
7We can note that 1 λ e λ, then bounding T, we obtain a λ 1E approximation bound when
− −
− ≤
w w(0)
T λ −1 log k ∗ − k2
≥ · λ 1E
(cid:18) − (cid:19)
In our deterministic algorithm, we choose w(0) = 0 and thus we have w w(0) = w . In our
∗ 2 ∗ 2
k − k k k
randomized algorithm, we have from Lemma 14, with high probability w w(0) w , giving us the
∗ ∗
k − k ≤ k k
desired bound. (cid:4)
3.3.1 Algorithmic ǫ
In practice, one does not have access to the true corruption rate in the dataset. We therefore differentiate
betweenthealgorithmiccorruptionparameterǫandthetruecorruptionparameterofthedatasetǫ . Consider
∗
thecasewhenǫ ǫ i.e.,weoverestimatethecorruptionrateofthedataset. Wehaveatanyiterationt [T],
∗
≥ ∈
that |S ǫ ∩Q |≥|S ǫ∗ ∩Q
|
as (1 −ǫ)N ≥(1 −ǫ ∗)N. We similarly have |P \S( ǫt ∗) |≤|P \S( ǫt)
|
as the thresholded
setis smallerin cardinality. We thus have ourkey step, Equation (4), will stillhold whenǫ ǫ andwe can
∗
≥
thus obtain the same approximation bounds with only a lower bound on ǫ .
∗
3.4 Learning Leaky-ReLU Neurons
We will now consider learning a neuron with the Leaky-ReLU function. We can note that Property 9, 10,
and11allholdfor the Leaky-ReLU.Moreconveniently,we haveγ in Property 11is constantoverR. Inour
proof we are able to leverage the fact that the second derivative is zero almost surely.
Theorem13. LetX =[x ,...,x ] RN d bethedatamatrixandy=[y ...,y ] betheoutput. Suppose
1 n ⊤ × 1 n ⊤
∈
x aresampledfromasub-Gaussiandistributionsecond-momentmatrixΣandproxyΓ,andy =σ(w x )+ξ
i i ∗ i i
for ξ i
∼
N(0,ν2) where σ(x) = max {γx,x
}
for all i
∈
P. Then after O γ −2κ(Σ)log kw ε∗ k g· radient
descent iterations and ǫ γ2λmin(Σ) , with probability exceeding 1 4δ,(cid:16)Algorithm 2 (cid:16)with le(cid:17)a(cid:17)rning rate
≤ √32Bλmax(Σ) −
η =O(κ 2(Σ)) returns w(T) such that
−
kw(T) −w
∗ k2
≤O ν kΓ k2λ−m1 in(Σ) ǫlog(1/ǫ) +O γ −2κ(Σ)νǫ Blog(1/ǫ) .
(cid:16) p (cid:17) (cid:16) p (cid:17)
Proof. The proof is deferred to Appendix B.2.1. (cid:4)
Our proofandresultalsohold fora smoothened versionofthe Leaky-ReLU.To avoidthe kink atx=0,one
can consider the Smooth-Leaky-ReLU, defined as
Smooth-Leaky-ReLU(x)=αx+(1 α)log(1+ex)
−
for an α (0,1). The Smooth-Leaky-ReLU satisfies Properties 9, 10, and 11, and is convex, indicating that
∈
Theorem 13 can be applied.
3.5 Learning ReLU Neurons
We will now consider the problem of learning ReLU neural networks. We first give a preliminary result for
randomized initialization.
Lemma 14 (Theorem3.4inDu et al.[2018]). Suppose w(0) is sampled uniformly from a p-dimensional ball
with radius α w such that α 1 , then with probability at least 1 α πp,
k ∗ k ≤ 2πp 2 − 2
q p
w(0) w 1 α2 w .
∗ 2 ∗ 2
k − k ≤ − k k
p
From this result we are able to derive probabistic guarantees on the convergence of learning a ReLU
neuron.
8Theorem 15. Let X = [x ,...,x ] Rn d be the data matrix and y = [y ,...,y ] be the output, such
1 n ⊤ × 1 n ⊤
∈
that for x are sampled from a sub-Gaussian distribution with second-moment matrix Σ and sub-Gaussian
i
proxy Γ and the output is given as y = σ(w x ) + ξ for ξ (0,ν2) for all i P. Then after
i ∗ i i i
O κ(Σ)log kw∗ k gradient descent iterations a· nd N = Ω d+lo∼ g(1/N δ) , then with pro∈ bability exceeding
ε ǫ
1(cid:16) 3Tδ, Al(cid:16) gorithm(cid:17)(cid:17) 2 with learning rate η =O λmin(Σ) retur(cid:16) ns w(T) su(cid:17) ch that
− λ2 max(Σ)
(cid:16) (cid:17)
kw(T) −w
∗ k2
≤O ν kΓ k2λ−m1 in(Σ) ǫlog(1/ǫ) +O κ(Σ)νǫ Blog(1/ǫ) .
(cid:16) p (cid:17) (cid:16) p (cid:17)
Proof. The proof is deferred to Appendix B.3. (cid:4)
OurproofforlearningReLUneuronsfollowsthesamehighlevelstructureaslearningsigmoidalorleaky-ReLU
neurons,howeveritissignificantlymoretechnical,aswerequireboundsonthespectraofempiricalcovariance
matrices over the intersection of half-spaces. We also note that Lemma 14 implies that randomized restarts
with high probability will return a vector with O( ǫlog(1/ǫ)) ℓ approximation error. With access to an
2
uncorruptedtestset,usingconcentrationofmeasureinequalitiesforsub-Gaussiandistributions,itispossible
p
to use adaptive algorithms to determine how many randomized restarts is enough to obtain O( ǫlog(1/ǫ))
approximation error.
p
4 Discussion
Inthispaper,westudythetheoreticalconvergencepropertiesofiterativethresholdingfornon-linearlearning
problemsintheStrongǫ-contaminationmodel. Ourwarm-upresultforlinearregressionreducestheruntime
while achieving the best known approximation for iterative thresholding algorithms. Many papers have
experimentally studied the iterative thresholding estimator in large scale neural networks [Hu et al., 2023,
Shen and Sanghavi, 2019] and to our knowledge,we are the first paper to make advancements in the theory
of iterative thresholding for a general class of activation functions. There are many directions for future
work. Regardingiterative thresholding,ourpaper has establishedupper bounds onthe approximationerror
of activation functions, an interesting next step is on upper bounds for the sum of activation functions, i.e.
onehidden-layerneuralnetworks. Inthelinearregressioncase,Gao[2020]derivedtheminimaxoptimalerror
of O(σǫ). Establishing this result for sigmoidal, leaky-ReLU, and ReLU functions would be helpful in the
discussingthestrengthofourbounds. Derivingupperandlowerboundsforiterativethresholdingforbinary
classificationisagooddirectionforfutureresearch. In 1classification,consideringy =sign(w x+ξ),the
∗
± ·
sign function adds adds an interesting complication. A study on if our current techniques can also handle
the sign function would be interesting.
References
Pranjal Awasthi, Abhimanyu Das, Weihao Kong, and Rajat Sen. Trimmed maximum likelihood estimation
for robust generalized linear model. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho, editors, Advances in Neural Information Processing Systems, 2022.
Kush Bhatia, Prateek Jain, and Purushottam Kar. Robust regression via hard thresholding. In C. Cortes,
N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 28. Curran Associates, Inc., 2015.
Kush Bhatia, Prateek Jain, ParameswaranKamalaruban, and Purushottam Kar. Consistent robust regres-
sion. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
S´ebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends® in
Machine Learning, 8(3-4):231–357,2015.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial cor-
ruption. In International conference on machine learning, pages 774–782.PMLR, 2013.
9Yu Cheng, Ilias Diakonikolas, Rong Ge, and David P. Woodruff. Faster algorithms for high-dimensional
robust covariance estimation. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-
Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages
727–757.PMLR, 25–28 Jun 2019.
YuCheng,IliasDiakonikolas,RongGe,andMahdiSoltanolkotabi.High-dimensionalrobustmeanestimation
via gradient descent. In Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1768–
1778. PMLR, 13–18 Jul 2020.
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to algorithms.
MIT press, 2022.
IliasDiakonikolasandDanielMKane. Algorithmic high-dimensional robust statistics. CambridgeUniversity
Press, 2023.
Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart.
Sever: A robust meta-algorithm for stochastic optimization. In Proceedings of the 36th International
Conference on Machine Learning, ICML ’19, pages 1596–1606.JMLR, Inc., 2019.
Yihe Dong, Samuel Hopkins, and Jerry Li. Quantum entropy scoring for fast robust mean estimation and
improved outlier detection. Advances in Neural Information Processing Systems, 32, 2019.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-
hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on Machine
Learning, pages 1339–1348.PMLR, 2018.
JianqingFan,WeichenWang,andYiqiaoZhong. Anℓ eigenvectorperturbationboundandits application
∞
to robust covariance estimation. Journal of Machine Learning Research, 18(207):1–42,2018.
Martin A. Fischler and Robert C. Bolles. Random sample consensus: A paradigm for model fitting with
applications to image analysis and automated cartography. Commun. ACM, 24(6):381–395, jun 1981.
ISSN 0001-0782. doi: 10.1145/358669.358692.
Chao Gao. Robust regression via mutivariate regression depth. Bernoulli, 26(2):1139 – 1170, 2020. doi:
10.3150/19-BEJ1144. URL https://doi.org/10.3150/19-BEJ1144.
ShuHu,ZhenhuanYang,XinWang,YimingYing,andSiweiLyu. Outlierrobustadversarialtraining. arXiv
preprint arXiv:2309.05145, 2023.
ArunJambulapati,JerryLi,andKevinTian. Robustsub-gaussianprincipalcomponentanalysisandwidth-
independentschattenpacking. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,editors,
Advances in Neural Information Processing Systems, volume 33, pages 15689–15701. Curran Associates,
Inc., 2020.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection.
Annals of statistics, pages 1302–1338,2000.
Adrien M Legendre. Nouvelles methodes pour la determination des orbites des comtes: avec un supplement
contenantdivers perfectionnemens decesmethodes etleurapplication auxdeuxcometesde1805. Courcier,
1806.
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. In
International Conference on Learning Representations, 2021.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv
preprint arXiv:1607.06534, 2016.
10BhaskarMukhoty,GovindGopakumar,PrateekJain,andPurushottamKar. Globally-convergentiteratively
reweighted least squares for robust regression problems. In Kamalika Chaudhuri and Masashi Sugiyama,
editors,ProceedingsoftheTwenty-SecondInternationalConferenceonArtificialIntelligenceandStatistics,
volume 89 of Proceedings of Machine Learning Research, pages 313–322.PMLR, 16–18 Apr 2019.
Muhammad Osama, Dave Zachariah, and Petre Stoica. Robust risk minimization for statistical learning
from corrupted data. IEEE Open Journal of Signal Processing, 1:287–294,2020.
AdarshPrasad,ArunSaiSuggala,SivaramanBalakrishnan,andPradeepRavikumar. Robustestimationvia
robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
82, 2018.
Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A unified approach to robust mean
estimation. arXiv preprint arXiv:1907.00927, 2019.
Philippe Rigollet and Jan-Christian Hu¨tter. High-dimensional statistics. arXiv preprint arXiv:2310.19244,
2023.
YanyaoShenandSujaySanghavi.Learningwithbadtrainingdataviaiterativetrimmedlossminimization.In
KamalikaChaudhuriandRuslanSalakhutdinov,editors,Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5739–5748.PMLR,
09–15 Jun 2019.
Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the pres-
ence of arbitrary outliers. In Anna R. Karlin, editor, 9th Innovations in Theoretical Computer Science
Conference, ITCS 2018, January 11-14, 2018, Cambridge, MA, USA, volume 94 of LIPIcs, pages 45:1–
45:21.Schloss Dagstuhl - Leibniz-Zentrum fu¨r Informatik, 2018. doi: 10.4230/LIPIcs.ITCS.2018.45. URL
https://doi.org/10.4230/LIPIcs.ITCS.2018.45.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Roman Vershynin. High-dimensional probability. University of California, Irvine, 2020.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu networks
via gradient descent. In The 22nd international conference on artificial intelligence and statistics, pages
1524–1534.PMLR, 2019.
11A Proofs for Linear Regression
Notation. We first state some notational preliminaries. Partition the indices of the training data set X
into the indices of the inliers, P, and the indices of the corrupted points, Q. That is, let [N]=P Q where
∪
P = (1 ǫ)N and Q = ǫN, where i P iff (x ,y ) is an inlier, and i Q iff (x ,y ) has been corrupted
i i j j
| | − | | ∈ ∈
by the adversary. For t [T], we denote S(t) as the set of indices selected after thresholding at iteration t.
∈
We decompose S(t) into the indices of True Positives (inliers) and the indices of False Positives (corrupted
samples), i.e. S(t) = S(t) P S(t) Q = TP FP. We similarly decompose the points discarded at
∩ ∪ ∩ ∪
each iteration into Fa(cid:16) lse Negat(cid:17) ives(cid:16) and True(cid:17) Negatives, i.e. X S(t) = (X S(t)) P (X S(t)) Q =
\ \ ∩ ∪ \ ∩
FN TN. (cid:16) (cid:17) (cid:16) (cid:17)
∪
A.1 Proof of Theorem 7
We first state some useful lemmata. Let vec : Rn k Rnk represent the vectorizaton of a matrix to a
×
vector placing its columns one by one into a vector, an→ d let : RN K RL M RNL KM represent the
× × ×
⊗ × →
Kronecker product between two matrices. We then have the useful facts,
Lemma 16. Suppose A,B Rm n, then
×
∈
A,B d =ef tr(ABT)= vec(A),vec(B) .
tr
h i h i
Lemma 17. Suppose A,B,C are conformal matrices, then
vec(ABC)=(C⊤ A)vec(B).
⊗
Proof. Recall that X Rd N and Y RK N hold the training data. Given any set of indices S, the risk
× ×
of a model W RK d∈ on the points in∈ S, and the gradient of that risk, are
×
∈
1 2
(W;S)= WX Y 2. and (W;S)= (WX Y)X .
R S k S − S kF ∇R S − ⊤
| | | |
We first show we can obtain Linear Convergence plus a noise term and a consistency term, which we
define as the difference between the optimal estimator in expectation and the optimal estimator over a
population. We then show the consistency term goes to zero with high probability as N . Finally, we
→ ∞
use the concentration inequalities developed in Appendix C to give a clean approximation bound.
Step 1: Linear Convergence. We willfirstshowiterativethresholdinghaslinearconvergencetooptimal.
Let W =argmin W ∈RK×d R(W;P) be the minimizer over the good data. Then, we have
cW(t+1) W∗
F
= W(t) W∗ η (W(t);S(t))
F
k − k k − − ∇R k
= W(t) W η (W(t);TP)+η (W ;P) η (W ;P)+η (W;P) η (W(t);FP)
∗ ∗ ∗ F
k − − ∇R ∇R − ∇R ∇R − ∇R k
W(t) W η (W(t);TP)+η (W ;TP) + η (W(t);FP) + η (W ;FN)
∗ ∗ F F ∗ F
≤k − − ∇R ∇R k k ∇R kc k ∇R k
I II III
+ η (W ;P) η (W;P) .
| ∗ {z F } | {z } | {z }
k ∇R − ∇R k
IV
c
In contra|st with our pr{ozof sketch in S}ection 3.3, we introduce a consistency estimate in IV. We first will
upper bound I through the expansion of its square.
W(t) W η (W(t);TP)+η (W ;TP) 2 = W(t) W 2
k − ∗ − ∇R ∇R ∗ kF k − ∗ kF
2η tr (W(t) W ) ( (W(t);TP) (W ;TP)) + η (W(t);TP) η (W(t);TP) 2 .
− · − ∗ ⊤ ∇R −∇R ∗ k ∇R − ∇R kF
(cid:0) I1 (cid:1) I2
| {z } | {z }
12We firstlowerbound I , we willobtaina lowerbound that is less than I , allowingus to cancelthe I term.
1 2 2
I =2η tr (W(t) W ) ( (W(t);TP) (W ;TP))
1 ∗ ⊤ ∗
· − ∇R −∇R
4η
d =ef (cid:0) tr (W(t) W ) (W(t)X Y )X (cid:1)E X
(1 ǫ)N · −
∗ ⊤ TP
−
TP T⊤P− TP T⊤P
−
( =i)
(1
4η
ǫ)N
·tr(cid:0) (W(t) −W∗)⊤(cid:0) (W(t) −W∗)X TPX T⊤P . (cid:1)(cid:1)
− (cid:16) (cid:17)
In the above, (i) follows from recalling that Y =W X E . We then have,
TP ∗ TP TP
−
I
1
( =ii)
(1
4η
ǫ)N
·tr (W(t) −W∗)X TPX T⊤P(W(t) −W∗)⊤
− (cid:16) (cid:17)
(i =ii) 4η vec((W(t) W ) ),vec(X X (W(t) W ) )
(1 ǫ)N ·h −
∗ ⊤ TP T⊤P
−
∗ ⊤
i
−
( =iv) 4η vec((W(t) W ) ),(I X X )vec((W(t) W ) )
(1 ǫ)N ·h −
∗ ⊤
⊗
TP T⊤P
−
∗ ⊤
i
−
( =v) 4η w(t) w ,X X (w(t) w )
(1 ǫ)N · k − k∗ TP T⊤P k − k∗
− kX∈[K]
(cid:10) (cid:11)
(vi) 2η
≥ (1 ǫ)N ·
λ min(X TPX T⊤P) kw k(t) −w
k∗
k2 2+ kX TPX T⊤Pk−21 kX TPX T⊤P(w k(t) −w k∗) k2
2
− kX∈[K]
(cid:0) (cid:1)
2η 2η
=
(1 ǫ)N
·λ min(X TPX T⊤P) kW(t) −W
∗
k2 F+
(1 ǫ)N
·kX TPX T⊤Pk−21 k(W(t) −W ∗)X TPX T⊤Pk2 F.
− −
I12
In the above, (ii) follows from the cyclic property|of the trace, (iii) follo{wzs from the relation giv}en in
Lemma 16, (iv) holds from the relation given in Lemma 17, the inequality in (vi) follows from Lemma 36,
and in (v) we apply Lemma 17, which gives the following equality,
X X w w X X (w w )
TP T⊤P 1
−
1∗ TP T⊤P 1
−
1∗
(I ⊗X TPX T⊤P)vec((W(t) −W ∗) ⊤)=

...

. .
.
=

. .
.
.
X X w w X X (w w )
 TP T⊤P K − K∗   TP T⊤P K − K∗ 
    
We now upper bound the corrupted gradient term.
2η
II d =ef (W(t)X Y )X
(1 ǫ)N ·k
FP
−
FP F⊤PkF
−
(vii) 2η
X W(t)X Y
FP 2 FP FP F
≤ (1 ǫ)N ·k k k − k
−
(viii) 2η
X W(t)X Y
FP 2 FN FN F
≤ (1 ǫ)N ·k k k − k
−
(ix) 2η
X W(t)X W X + E
FP 2 FN ∗ FN F FN F
≤ (1 ǫ)N ·k k k − k k k
−
(x) 2η (cid:0) 2η (cid:1)
X
FP 2
X
FN 2
W(t) W∗ F+ X
FP 2
E
FN
F.
≤ (1 ǫ)N ·k k k k k − k (1 ǫ)N ·k k k k
− −
In the above,the equalities in (vii) and(x) fromthe fact that for any twosize compatible matrices, A,B, it
holds that AB A B , (viii) follows from the optimality of the Hard Thresholding Operator (see
F F 2
k k ≤ k k k k
Definition 6), and (ix) follows from the sub-additivity of the Frobenius norm. We will now upper bound
III.
2η 2η
def
III = η (W ;FN) = (W X Y )X E X .
k ∇R
∗ kF
(1 ǫ)N ·k
∗ FN
−
FN F⊤NkF
≤ (1 ǫ)N ·k
FN F⊤NkF
− −
13In the above,we use the fact that for any two conformal matrices, A,B, it holds that AB A B .
F F 2
k k ≤k k k k
Step 2: Consistency. We will now upper bound the consistency estimate, IV.
def
IV = η (W ;P) η (W;P)
∗ F
k ∇R − ∇R k
2η 2η
=
(1 ǫ)N
·k(W −W∗)X cPX P⊤ kF
≤ (1 ǫ)N
·kE PX P⊤ kF.
− −
We can then have from Lemma 34, c
E X 2 2σ2 K X 2 log 2N2/δ 2σ2 (1 ǫ)Ndλ (Σ)log 2N2/δ .
k P P⊤ kF ≤ k P kF ≤ − max
(cid:0) (cid:0) (cid:1)(cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
We then have with failure probability at most δ,
2η 8σ2dλ (Σ)log(2N2/δ)
max
E X η .
(1 ǫ)N ·k
P P⊤ kF
≤ ·s (1 ǫ)N
− −
We thenhavefromourchoiceofη =0.1λ 1 (Σ), wehaveI I withhighprobability. Then, fromnoting
−max 2
≤
12
that √1 2x 1 x for any x 1/2, we obtain
− ≤ − ≤
2η 2η
W(t+1) W W(t) W 1 λ (X X )+ X X
k −
∗ kF
≤k −
∗ kF
− (1 ǫ)N ·
min TP T⊤P
(1 ǫ)N ·k
FP k2
k
FN k2
(cid:18) − − (cid:19)
2η 2η 32σ2dλ (Σ)log(2N2/δ)
max
+ X E + E X +η .
(1 ǫ)N ·k
FP k2
k
FN kF
(1 ǫ)N ·k
FN F⊤NkF
·s 3(1 ǫ)N
− − −
Step 3: Concentration Bounds. From Proposition 19 and our ℓ bounded corrupted covariate assump-
2
tion, we obtain with failure probability at most δ,
E X Nǫσ 30KBlog(1/ǫ).
FN F FP F
k k k k ≤
p
From Lemma 34, we have when N log(1/δ), with failure probability at most δ,
≥
E X 6KlogN X .
k
FN F⊤NkF
≤ k
FN k2
p
Then utilizing the result from Lemma 21, we have with failure probability at most δ,
6KlogN X 60Kλ (Σ)NlogN ǫlog(1/ǫ).
FN 2 max
k k ≤ ·
Noting that S(t) P (p 1 2ǫ)N, we have frop m Lemma 21 for ǫ 1 κ 1(Σ), the minimum eigenvalue
| ∩ | ≥ − ≤ 60 · −
satisfies with failure probability at most δ,
N
λ (X X ) λ (Σ).
min TP T⊤P
≥ 4 ·
min
Then when ǫ 1 κ 1(Σ)λ (Σ), we have with high probability,
≤ 960B · − min
q
η
X X λ (X X ) λ (Σ).
k
FP k2
k
FN k2
−
min TP T⊤P
≤ 4(1 ǫ) ·
min
−
Combining our estimates, we have
η
W(t+1) W W(t) W 1 λ (Σ) +η σǫ 480KBlog(1/ǫ)
∗ F ∗ F min
k − k ≤k − k − 4(1 ǫ) · ·
(cid:18) − (cid:19)
η ηp
+ 240Klog(N)λ (Σ)ǫlog(1/ǫ)+ σ 11dλ (Σ)log(2N2/δ).
max max
(1 ǫ)√N · (1 ǫ)√N ·
− p − p
14with probability exceeding 1 3δ. Then, when N =Ω dλmax(Σ)+log(1/δ) we have
− ǫ2KB
(cid:16) (cid:17)
eη
W(t+1) W W(t) W 1 λ (Σ) +η σǫ 4320KBlog(1/ǫ).
∗ F ∗ F min
k − k ≤k − k − 4(1 ǫ) · ·
(cid:18) − (cid:19)
p
Then, solving for the induction with an infinite sum, referring to the proof sketch in Section 3.3 and our
choice of η, we have after O κ(Σ) log
kW∗
kF iterations,
· ε
(cid:16) (cid:16) (cid:17)(cid:17)
σǫ 34560KBlog(1/ǫ)
W(T) W∗
F
ε+ .
k − k ≤ λ (Σ)
p min
Our proof is complete. (cid:4)
B Proofs for Learning Nonlinear Neurons
In this section we present our omitted proofs for the approximation bounds for learning nonlinear neurons
with Algorithm 2.
B.1 Sigmoidal Neurons
In this section we will give the ommited proofs for the ℓ approximation bounds for learning sigmoidal
2
neurons with Algorithm 2 in the Strong ǫ-Contamination Model.
B.1.1 Proof of Theorem 12
Proof. From Algorithm 2, we have the gradient update for learning a Sigmoid neuron for the ℓ loss.
2
2η
w(t+1) =w(t) (σ(w(t) x ) y ) σ (w(t) x ) x .
i i ′ i i
− (1 ǫ)N · · − · · ·
− i ∈XS(t)
Ourproofwillfollowasimilarstructuretotheproofforlinearregression. Wefirstshowwecanobtainlinear
convergence of w(t) to w with some error. Then we rigorously analyze the concentration inequalities to
∗
give crisp bounds on the upper bound for ǫ and show the noise term is O(ǫlog(1/ǫ)).
Step 1: Linear Convergence.
w(t+1) w = w(t) η (w(t);S(t)) w
∗ 2 ∗ 2
k − k k − ∇R − k
= w(t) w η (w(t);TP) η (w(t);FP)
∗ 2
k − − ∇R − ∇R k
w(t) w η (w(t);TP) + η (w(t);FP) .
∗ 2 2
≤k − − ∇R k k ∇R k
I II
We will expand I through its square|and give an up{pzer bound. } | {z }
I2 = w(t) w 2 2η w(t) w , (w(t);TP) +η2 (w(t);TP) 2.
k − ∗ k2− ·h − ∗ ∇R i ·k∇R k2
I1 I2
We now lowerbound I . Note from t| he randomized{z initialization, w} e h| ave w(0){z w w} . Then, noting
1 ∗ ∗
k − k≤
that w R, we have by the Cauchy-Schwarzinequality, for any x , we have w(t) x 2RB almost
∗
surelyk
.
Hek r≤
eweleverageProperty 11andnotethereexistsaγ s.t. σ
(x∼
)
P
γ
>0for| allx· R|≤
s.t. x 2RB,
′
≥ ∈ ≤
15from which we obtain
4η
I = w(t) w , (σ(w(t) x ) y ) σ (w(t) x ) x
1 ∗ i i ′ i i
(1 ǫ)N ·h − · − · · · i
− i TP
X∈
4η
= w(t) w , (σ(w(t) x ) σ(w x )+ξ ) σ (w(t) x ) x
∗ i ∗ i i ′ i i
(1 ǫ)N ·h − · − · · · · i
− i TP
X∈
(i) 4η 4η
≥ (1 ǫ)N ·γ2λ min(X TPX T⊤P) kw(t) −w ∗ k2 2−(1 ǫ)N ·kw(t) −w ∗ k2 ξ iσ′(w(t) ·x i) ·x i 2.
− − i TP
(cid:13)X∈ (cid:13)
I11 (cid:13) (cid:13)
In the ab|ove, (i) follows from{zdefining the function} h:Rd R,
7→
w σ (w x)dP(x),
7→ ZRd(cid:18)Z
(cid:19)
·
where dP(x) = 1 x TP . Then from Property 11, we have that σ is strictly positive over a compact
′
{ ∈ }
domain,thisimplies that σ isstronglyconvexoveracompactdomain. Thenwecancalculatethe Hessian,
R
2h(w)= σ (w x) xx dP(x) γλ (X X ) I.
∇ ZRd
′
· ·
⊤
(cid:23)
min TP T⊤P
·
With the strong convexity in hand, we have,
w(t) w , (σ(w(t) x ) σ(w x ) x
∗ i ∗ i i
h − · − · · i
i TP
X∈
1 (ii)
= w(t) w , 2h(w +θ(w(t) w ))dθ (w(t) w ) γλ (X X ) w(t) w 2.
h − ∗ ∇ ∗ − ∗ · − ∗ i ≥ min TP T⊤P ·k − ∗ k2
Z0
In the above, (ii) follows from noting that for any θ [0,1] we have,
∈
(1 θ)w +θw(t) θ w w(t) + w 2 w =2R.
∗ 2 ∗ 2 ∗ 2 ∗ 2
k − k ≤ k − k k k ≤ k k
We can then use the same γ as previously defined. Then from an application of Peter-Paul’s Inequality, we
obtain
4η η
w(t) w ξ σ (w(t) x ) x γ2λ (X X ) w(t) w 2
(1 ǫ)N ·k − ∗ k2 i ′ · i · i 2 ≤ (1 ǫ)N · min TP T⊤P k − ∗ k2
− i TP −
(cid:13)X∈ (cid:13)
(cid:13) + ((cid:13)
1
4η
ǫ)N
·γ −2λ−m1 in(X TPX T⊤P) ξ iσ ′(w(t) ·x i) ·x
i
2 2.
− i TP
(cid:13)X∈ (cid:13)
(cid:13) (cid:13)
We next upper bound I . We note that σ is σ -Lipschitz, then from an application of the triangle
2 lip
k k
inequality, we obtain
I d =ef
4η2
(σ(w(t) x ) σ(w x )+ξ ) σ (w(t) x ) x 2
2 [(1 ǫ)N]2 · · i − ∗ · i i · ′ · i · i 2
− (cid:13)i X∈TP
(cid:13)
( =i)
4η2 (cid:13)
x x (w w ) σ (c )σ (w(t) x )+ξ σ
(w(t)(cid:13)
x ) x 2
[(1 ǫ)N]2 · i ⊤i − ∗ · ′ i ′ · i i · ′ · i · i 2
− (cid:13)i X∈TP
(cid:13)
≤ [(1 8η ǫ2 )N]2 ·k(cid:13) σ k4 lipλ2 max(X TPX T⊤P) kw(t) −w ∗ k2 2+ [(1 8η ǫ2 )N]2 · ξ iσ′(w(cid:13) (t) ·x i) ·x i 2 2.
− − (cid:13)i X∈TP
(cid:13)
I21 (cid:13) (cid:13)
In the above|, (i) follows from notin{gzthere exists a constan}t c [w(t) x ,w x ] such that
i i ∗ i
∈ · ·
σ (c )(w(t) w ) x =σ(w(t) x ) σ(w x )
′ i ∗ i i ∗ i
− · · − ·
16from the Mean-Value Theorem. Then, from choosing η ≤ γ 42 k(1 σ− k4 lǫ i) pN λ2 mλ am xi (n X(X TT PP XX T⊤T P⊤ P )). We have I 21 ≤0.5I 11. We
now bound the corrupted gradient term.
II2 =
4η2
(σ(w(t) x ) y ) σ (w x ) x 2
[(1 ǫ)N]2 · · i − i · ′ · i · i 2
− i FP
(cid:13)X∈ (cid:13)
(ii) 4η2 (cid:13) (cid:13)
σ 2 X X (σ(w(t) x ) y )2
≤ [(1 ǫ)N]2 ·k klipk FP F⊤Pk2 · i − i
− i FP
X∈
(iii) 4η2
σ 2 X X (σ(w(t) x ) y )2
≤ [(1 ǫ)N]2 ·k klipk FP F⊤Pk2 · i − i
− i FN
X∈
4η2
= σ 2 X X (σ(w(t) x ) σ(w x )+ξ )2
[(1 ǫ)N]2 ·k klipk FP F⊤Pk2 · i − ∗ · i i
− i FN
X∈
(iv) 8η2
σ 2 X X σ 2 X X w(t) w 2+ ξ 2 .
≤ [(1 ǫ)N]2 ·k klipk FP F⊤Pk2 k klip·k FN F⊤Nk2 k − ∗ k2 k FN k2
− (cid:16) (cid:17)
Intheabove,(ii)followsfromLemma 35,(iii)followsfromtheoptimalityoftheHard-Thresholdingoperator,
(iv) follows from noting σ is Lipschitz and the elementary inequality (a+b)2 2a2+2b2 for any a,b R.
≤ ∈
Concluding the step, we have,
η √8η
w(t+1) w w(t) w 1 γ2λ (X X )+ σ 2 X X
k − ∗ k2 ≤k − ∗ k2 − 2(1 ǫ)N · min TP T⊤P (1 ǫ)N ·k klipk FP k2 k FN k2 !
− −
√8η 2√η √8η
+
(1 ǫ)N
+
(1 ǫ)N
·λ m−1 in/2(X TPX T⊤P)
!
ξ iσ ′(w(t) ·x i) ·x
i
2+
(1 ǫ)N
·kX
FP k2
kξ
FN
k2.
− − (cid:13)i X∈TP
(cid:13)
−
Step 2: Concentrpation Bounds. From Proposi(cid:13) tion 30, we have with p(cid:13) robability at least 1 δ,
−
ξ iσ′(w(t) x i) x
i 2
.ν Γ
2
σ lipN ǫlog(1/ǫ).
k · · k k k k k
i TP
X∈ p
Recall that X √ǫNB. From Lemma 21, with probability at least 1 δ, we have
FP 2
k k ≤ −
X X Nǫ 10Blog(1/ǫ).
FN 2 FP 2
k k k k ≤ ·
From the secondrelation in Lemma 21, we have when N =p Ω d+log(1/δ) , with probability exceeding 1 δ,
ǫ −
the minimimum eigenvalue satisfies, (cid:16) (cid:17)
N
λ (X X ) λ (Σ).
min TP T⊤P
≥ 4
min
We then find for
γ2 λ (Σ)
ǫ2 min
≤ σ 2 Bλ (Σ)
k klip max
and probability exceeding 1 δ,
p
−
Nγ2
(γ2/2)λ min(X TPX T⊤P) −√8 kσ k2 lipkX
FP k2
kX
FN k2
≥ 16
λ min(Σ).
Combining the ℓ boundedness of the corrupted covariates and Proposition19, we have with probability
2
exceeding 1 δ,
−
X ξ Nǫν 30Blog(1/ǫ).
FP 2 FN 2
k k k k ≤ ·
Then combining the results with our choice of η, we obtainp,
η
w(t+1) w w(t) w 1 γ2λ (Σ) +ηǫ σ 2 80Blog(1/ǫ)
k − ∗ k2 ≤k − ∗ k2 − 16(1 ǫ)N · min k klip
(cid:18) − (cid:19)
p
+ √8η+2 ηλ (Σ) σ 2 c Nλ (Σ)ν ǫlog(1/ǫ).
min k klip 1 max
(cid:16) p (cid:17) p
17In the above, c is a constant and the final inequality holds when N Rdlog12+2dlog12+log(1/δ) =
1 ≥ 6ǫlog(1/ǫ)
Ω d+log(1/δ) . Then, following from our sketch in Section 3.3, we have
ǫ
(cid:16) (cid:17)
kw(t+1) −w ∗ k2 ≤ε+O γ−2λ−m1 in(Σ) kσ k2 lipǫ Blog(1/ǫ) +O γ−2 kσ k2 lipκ(Σ) ǫlog(1/ǫ) .
(cid:16) p (cid:17) (cid:16) p (cid:17)
Our proof is complete. (cid:4)
B.2 Leaky-ReLU Neuron
In this section we will derive our ℓ -approximation bound for the Leaky-ReLU neuron.
2
B.2.1 Proof of Theorem 13
Proof. We will decompose the gradient into the good component and corrupted component. The first part
of our proof will show that w moves in the direction of w , then in the second part of the proof we will
∗
show the affect of the corrupted gradient. Finally, we combine step 1 and step 2 to show that there exists
sufficiently small ǫ such that we can get linear convergence with a small additive error term.
Step 1: Upper bounding the ℓ norm distance between w(t+1) and w . We have from Algorithm 2,
2 ∗
w(t+1) w ∗ 2 = w(t) w ∗ η (w(t);S(t)) 2
k − k k − − ∇R k
= w(t) w ∗ η (w(t);TP)+η (w ∗;TP) η (w ∗;TP) (w(t);FP) 2
k − − ∇R ∇R − ∇R −∇R k
w(t) w ∗ η (w(t);TP)+η (w ∗;TP) 2+ η (w ∗;TP) 2+ η (w(t);FP) 2.
≤k − − ∇R ∇R k k ∇R k k ∇R k
I II III
We will fir|st upper bound the I {bzy an expansion of its sq}uar|e. {z } | {z }
1
I2 = w(t) w 2 2η w(t) w , (w(t);TP) (w(t);TP)
k − ∗ k2− ·h − ∗ ∇R i−∇R
I1
+η2 (w(t);TP) (w ;TP) 2.
| ∗ {z }
·k∇R −∇R k
I2
In the above, the a.s. rela|tion follows from{Pzroperty 9. We w}ill first lower bound I . We will first bound
1
the spectrum of 2 (w;TP) for any w Rd.
∇ L ∈
∇2 L(w;TP)=2
·
(σ(w ·x i) −y i) ·σ′′(w ·x i) ·x ix ⊤i +2
·
[σ′(w ·x i)]2 ·x ix ⊤i .
i TP i TP
X∈ X∈
Then, from noting that the second derivative of Leaky-ReLU is non-zero at one point, we have
2 (w;TP)a =.s. 2 [σ (w x )]2 x x .
∇ L ·
′
·
i
·
i ⊤i
i TP
X∈
We then obtain for any w Rd, almost surely,
∈
2 ·γ2λ min(X TPX T⊤P) ·I (cid:22)∇2 L(w;TP) (cid:22)2 ·kσ k2 lipλ max(X TPX T⊤P) ·I. (5)
We can now lower bound I from the convexity of the Leaky-ReLU,
1
4η 1
I 1 = w(t) w ∗, 2 (w ∗+θ(w ∗ w(t));TP)dθ (w(t) w ∗)
(1 ǫ)N ·h − ∇ R − · − i
− Z0
(5) 4η
γ2λ (X X ) w(t) w 2.
≥ (1 ǫ)N · min TP T⊤P k − ∗ k2
−
18We now will upper bound I with a similar argument.
2
I 2 = [(1 −4η ǫ2 )N]2 · Z01 ∇2 R(w ∗+θ(w ∗ −w(t));TP)dθ ·(w(t) −w ∗) 2 2
(cid:13) (cid:13)
(5) 4η2 (cid:13) (cid:13)
σ 2 λ2 (X X ) w(t) w 2.
≤ [(1 ǫ)N]2 ·k klip max TP T⊤P k − ∗ k2
−
where (ii) follows from the σ -Lipschitzness of σ given in Property 10. We then observe that I 0.5I
lip 2 1
k k ≤
when we choose
γ2 Nλ (X X )
η
min TP T⊤P
.
≤ σ 2 2λ2 (X X )
k klip max TP T⊤P
Step 2: Upper bounding the corrupted gradient. We now upper bound the corrupted gradient term.
III2 d =ef
4η2
(σ(w x ) y ) σ (w(t) x ) x 2
[(1 ǫ)N]2 · · i − i · ′ · i · i 2
− (cid:13)i X∈FP
(cid:13)
≤ [(1 4η ǫ2 )N]2 ·k(cid:13) σ k2 lipkX FPX F⊤Pk2 σ(w ·x i) −σ(w(cid:13) ∗ ·x i) 2
− i X∈FP
(cid:0) (cid:1)
4η2
≤ [(1 ǫ)N]2 ·kσ k2 lipkX FPX F⊤Pk2 σ(w ·x i) −σ(w ∗ ·x i)+ξ i 2
− i X∈FN
(cid:0) (cid:1)
8η2
σ 2 X X σ 2 X X w(t) w 2+ ξ 2 .
≤ [(1 ǫ)N]2 ·k klipk FP F⊤Pk2 k klipk FN F⊤Nk2 k − ∗ k2 k FN k2
− (cid:16) (cid:17)
Intheabove,thefirstinequalityfollowsfromLemma 35,thesecondinequalityfollowsfromtheoptimalityof
theSubquantileset,thefinalinequalityfollowsfromthe σ -Lipschitznessofσ. ThenfromProposition 31,
lip
k k
we have with probability at least 1 δ,
−
II d =ef ξ σ (w(t) x ) x .N Γ ν ǫlog(1/ǫ).
i ′ · i · i 2 k k
i TP
(cid:13)X∈ (cid:13) p
(cid:13) (cid:13)
We now combine Steps 1 and 2 to give the linear convergence result. Noting that √1 2x 1 x when
− ≤ −
x 1/2, we have for N =Ω Rd+log(1/δ) ,
≤ ǫ
(cid:16) (cid:17)
2γ2η 2η
w(t+1) w w(t) w 1 λ (X X )+ X X
k − ∗ k2 ≤k − ∗ k2 − (1 ǫ)N · min TP T⊤P (1 ǫ)N ·k FP k2k FN k2
(cid:18) − − (cid:19)
√8η
+c λ (Σ)ν ǫlog(1/ǫ)+ X ξ ,
2 max FP 2 FN 2
(1 ǫ)N ·k k k k
−
p
where c is a constant. Step 3: Concentration Bounds. We will give the relevant probabilistic bounds
2
for the random variables in Steps 1 and 2. From Lemma 21, we have
X X ǫ λ (Σ) 10BNlog(1/ǫ),
k FN k2k FP k2 ≤ max ·
with probability at least 1 δ when p
−
2 log(2/δ) 1
N dC2 + and ǫ κ 1(Σ)
≥ ǫ · K c ≤ 60 · −
(cid:18) K (cid:19)
From the same Lemma and under the same data conditions we have
1
λ (X X ) λ (Σ).
min TP T⊤P
≥ 4 ·
min
Then when the corruption rate satisfies
γ2λ (Σ)
min
ǫ ,
≤ 32Bλ (Σ)
max
p
19we have
1
X X γ2λ (X X ) λ (Σ)
k
FP k2
k
FN k2
−
min TP T⊤P
≥ 8 ·
min
We then have, after O κ2(Σ)log
kw∗
k2 iterations with high probability,
ε
(cid:16) (cid:16) (cid:17)(cid:17)
w(T) w ε+O C C ǫlog(1/ǫ) +O γ 2κ(Σ)νǫ Blog(1/ǫ)
∗ 2 Σ ν −
k − k ≤
(cid:16) p (cid:17) (cid:16) p (cid:17)
In the final inequality above, we set ε=O C ν ǫlog(1/ǫ) for
Σ
(cid:16) p (cid:17)
d+log(1/δ)
N =Ω .
ǫ
(cid:18) (cid:19)
Our proof is complete. (cid:4)
B.3 ReLU Neuron
In this section, we consider ReLU type functions. Our high-level analysis will be similar to the previous
sub-sections howeverthe details are considerablydifferent and require strongerconditions we can guarantee
by randomness.
B.3.1 Proof of Theorem 15
Proof. We will now begin our standard analysis.
w(t+1) w = w(t) w η (w;S(t))
∗ 2 ∗ 2
k − k k − − ∇R k
= w(t) w η (w(t);TP) η (w(t);FP)
∗ 2
k − − ∇R − ∇R k
w(t) w η (w(t);TP) + η (w(t);FP) .
∗ 2 2
≤k − − ∇R k k ∇R k
I II
We will now upper bound I through|its square in a{czcordance with}our|proof sk{eztch, }
I2 = w(t) w 2 2η w(t) w , (w(t);TP) +η2 (w(t);TP) 2.
k − ∗ k2− ·h − ∗ ∇R i ·k∇R k2
I1 I2
| {z } | {z }
20We will first lower bound I . We will first adopt the notation from Zhang et al. [2019], let Σ (w,wˆ) =
1 TP
X X 1 X w 0 1 X wˆ 0 , it then follows
TP T⊤P·
{
T⊤P
≥ }· {
T⊤P
≥ }
4η
I d =ef w(t) w , (σ(w(t) x ) y ) x 1 w(t) x 0
1 ∗ i i i i
(1 ǫ)N ·h − · − · · { · ≥ }i
− i TP
X∈
4η
= w(t) w ∗,Σ TP(w(t),w(t))w(t) Σ TP(w(t),w ∗)w ∗
(1 ǫ)N ·h − − i
−
4η
w(t) w , ξ x 1 w(t) x 0
∗ i i i
− (1 ǫ)N ·h − · { · ≥ }i
− i TP
X∈
4η
w(t) w ,Σ (w(t),w )(w(t) w )+Σ (w(t), w )w(t)
∗ TP ∗ ∗ TP ∗
≥ (1 ǫ)N ·h − − − i
−
4η
− (1 ǫ)N ·kw(t) −w ∗ k2 ξ ix i ·1 {w(t) ·x i ≥0 } 2
− i TP
(cid:13)X∈ (cid:13)
(i) 4η (cid:13) (cid:13)
λ (Σ (w(t),w )) w(t) w 2
≥ (1 ǫ)N · min TP ∗ k − ∗ k2
−
4η
− (1 ǫ)N ·kw(t) −w ∗ k2 ξ ix i ·1 {w(t) ·x i ≥0 } 2
− i TP
(cid:13)X∈ (cid:13)
(ii) 2η (cid:13) (cid:13)
λ (Σ (w(t),w )) w(t) w 2
≥ (1 ǫ)N · min TP ∗ k − ∗ k2
−
− (1
2η
ǫ)N
·λ−m1 in(Σ TP(w(t),w ∗))
·
ξ ix
i
·1 {w(t) ·x
i
≥0
}
2 2.
− i TP
(cid:13)X∈ (cid:13)
(cid:13) (cid:13)
In the above, (ii) follows from Young’s Inequality and (i) holds from the following relation,
w(t) w ,Σ (w(t), w )w(t)
∗ TP ∗
h − − i
= w(t) w , x w(t) x 1 w(t) x 0 1 w x 0
∗ i i i ∗ i
h − · · { · ≥ }· { · ≤ }i
i TP
X∈
= (w(t) x i w ∗ x i)(w(t) x i) 1 w(t) x i 0 1 w ∗ x i 0 0.
· − · · · { · ≥ }· { · ≤ }≥
i TP
X∈
Intheabove,inthe finalrelationwecannotethatwhenthe indicatorsarepositive,itmustfollowthatboth
w(t) x is positive and w(t) x w x as w x 0. We have from Weyl’s Inequality,
i i ∗ i ∗ i
· · ≥ · · ≤
λ Σ (w(t),w ) λ E Σ (w(t),w ) Σ (w(t),w ) E Σ (w(t),w ) .
min TP ∗ min TP ∗ TP ∗ TP ∗ 2
≥ −k − k
(cid:16) (cid:17) (cid:16) h i(cid:17) h i
Let Ω= x Rd :x w(t) 0,x w 0 , then
⊤ ⊤ ∗
∈ ≥ ≥
(cid:8) E Σ (w(t),w ) = (cid:9) E xx 1 w(t) x 0 1 w x 0
TP ∗ ⊤ i ∗ i
x x · { · ≥ }· { · ≥ }
∼Dh i i X∈TP ∼Dh i
(i)
N(1 2ǫ) π Θ(t) sinΘ(t) I
(cid:23) − · − − ·
(ii) (cid:16) w(t(cid:17)) w
∗
N(1 2ǫ) π 2arcsin k − k I
(cid:23) − · − w ·
(cid:18) (cid:18) k ∗k (cid:19)(cid:19)
w(t) w
∗
N(1 2ǫ) π 1 k − k I
(cid:23) − · − w ·
(cid:18) k ∗k (cid:19)
%N(1 2ǫ) I.
− ·
In the above, (i) follows from Lemma 33, (ii) follows from the guarantee in the randomized initialization.
We then have from Lemma 32,
kΣ TP(w(t),w ∗) −E Σ TP(w(t),w ∗)
k2
.N kΓ k2
2
ǫlog(1/ǫ).
h i p
21We then find there exists sufficiently small ǫ such that the minimimum eigenvalue of Σ (w(t),w ) satisfies
TP ∗
λ (Σ)
λ (Σ (w(t),w )) min .
min TP ∗
≥ 4
We now bound the second-moment matrix approximation. Let dP(x) be a Dirac-measure for x TP. We
∈
now upper bound I by splitting it into two sperate terms,
2
I =
4η2
(σ(w(t) x ) σ(w x ) ξ ) x 1 w(t) x 0 2
2 [(1 ǫ)N]2 · · i − ∗ · i − i · i · { · i ≥ } 2
− (cid:13)i X∈TP
(cid:13)
8η2 (cid:13)
(σ(w(t) x ) σ(w x )) x 1 w(t) x 0 2
(cid:13)
≤ [(1 ǫ)N]2 · · i − ∗ · i · i · { · i ≥ } 2
− (cid:13)i X∈TP
(cid:13)
(cid:13) (cid:13)
I21
8η2
+| ξ x 1 w(t){zx 0 2 . }
[(1 ǫ)N]2 · i i · { · i ≥ } 2
− (cid:13)i X∈TP
(cid:13)
(cid:13) (cid:13)
I22
Recall from Propositio|n 31, we have an up{pzer bound on I . We}next upper bound I .
22 21
I =
8η2
(σ(w(t) x ) σ(w x )) x 1 w(t) x 0 2
22 [(1 ǫ)N]2 · · i − ∗ · i · i · { · i ≥ } 2
− (cid:13)i X∈TP
(cid:13)
8η2 (cid:13) (cid:13)
X X (σ(w(t) x ) σ(w x ))2
≤ [(1 ǫ)N]2 · TP T⊤P 2 · i − ∗ · i
−
(cid:13) (cid:13)
i X∈TP
8η2 (cid:13)
X X
(cid:13)
2 w(t) w 2.
≤ [(1 ǫ)N]2 · TP T⊤P 2k − ∗ k2
−
(cid:13) (cid:13)
(cid:13) (cid:13)
Then, by choosing η λmin(Σ) , we have that I λmin(Σ).
≤ 80λ2 max(Σ) 22 ≤ 8
Step 3: Upper bounding the corrupted gradient. We now upper bound II.
II =
4η2
(σ(w(t) x ) y ) x 1 w(t) x 0 2
[(1 ǫ)N]2 · · i − i · i · { · i ≥ } 2
− (cid:13)i X∈FP
(cid:13)
4η2 (cid:13) (cid:13)
Σ (w(t),w(t)) (σ(w(t) x ) σ(+)ξ )2
≤ [(1 ǫ)N]2 ·k FP k2 · i − i
− i X∈FP
4η2
X X (σ(w(t) x ) σ(w x )+ξ )2
≤ [(1 ǫ)N]2 ·k FP F⊤Pk2 · i − ∗ · i i
− i X∈FN
8η2
X X X X w(t) w 2+ ξ 2 .
≤ [(1 ǫ)N]2 ·k FP F⊤Pk2 k FN F⊤Nk2 k − ∗ k2 k FN k2
− (cid:16) (cid:17)
Intheabove,thefirstinequalityfollowsfromthesameargumentasLemma 35,thesecondinequalityfollows
fromthe optimalityofthe Subquantile set,andthe finalinequalityfollowsfromnoting thatσ is1-Lipschitz.
We now conclude Steps 1-3 with our linear convergence result.
η 4η
w(t+1) w . w(t) w 1 λ (Σ)+ X X
k − ∗ k2 k − ∗ k − 32 · min (1 ǫ)N ·k FP k2k FN k2
(cid:18) − (cid:19)
8η 4η η
+
N
+
N ·
λ−m1 in(Σ)
·
ξ ix
i
·1 {w(t) ·x
i
≥0
}
2+
(1
ǫ)NkX
FP k2
kξ
FN
k2.
(cid:18) q (cid:19) (cid:13)i X∈TP
(cid:13)
−
(cid:13) (cid:13)
Step 4: Concentration Inequalities. From our previous theorems, we have
X X Nǫ 10Blog(1/ǫ),
k FP k2k FN k2 ≤
p
22with probability exceeding 1 δ and N = Ω d+log(2/δ) . From Proposition 31, we have with probability
− ǫ
exceeding 1 δ, (cid:16) (cid:17)
−
ξ x 1 w(t) x 0 .ν Γ ǫlog(1/ǫ).
i i · { · i ≥ } 2 k k2
i TP
(cid:13)X∈ (cid:13) p
We furthermore have with fa(cid:13)ilure probability less than δ(cid:13)from Lemma 18,
ξ ν 30ǫlog(1/ǫ).
FN 2
k k ≤
If ǫ 1 , we obtain after T =O κ2(Σ)logp kw∗ k2 gradient descent iterations,
≤ 64√80log(2) ε
(cid:16) (cid:16) (cid:17)(cid:17)
kw(t+1) −w
∗ k2
.ε+ν kΓ k2λ−m1 in(Σ) ǫlog(1/ǫ)+νǫ kΓ
k2
Blog(1/ǫ)
=O ν kΓ k2λ−m1 in(Σ)pǫlog(1/ǫ) +O νǫ kpΓ
k2
Blog(1/ǫ) .
(cid:16) p (cid:17) (cid:16) p (cid:17)
when we choose ε=O ν kΓ k2λ−m1 in(Σ) ǫlog(1/ǫ) +O νǫ kΓ
k2
Blog(1/ǫ) . Our proof is complete. (cid:4)
(cid:16) p (cid:17) (cid:16) p (cid:17)
C Probability Theory
In this section, we will present and prove various concentration inequalities and upper bounds for random
variables.
C.1 χ-Squared Random Variables
Lemma 18 (Upper Bound on Sum of Chi-Squared Variables [Laurent and Massart, 2000]). Suppose ξ
i
∼
(0,ν2) for i [n], then
N ∈ Pr ξ 2 ν n+2√nx+2x e x.
k k2 ≥ ≤ −
Proposition 19 (Probabilistic Uppe(cid:8)r Boundon(cid:0) Sum of Chi-Sq(cid:1)u(cid:9)aredVariables). Suppose ξ
i
(0,ν2) for
∼N
i [n]. Let S [n] such that S =ǫn for ǫ (0,0.5) and let represent all such subsets. Given a failure
∈ ⊂ | | ∈ W
probability δ (0,1), when n log(1/δ), with probability exceeding 1 δ,
∈ ≥ −
max ξ 2 ν(30nǫlog(1/ǫ)).
S k S k2 ≤
∈W
Proof. Directly from Lemma 18, we have with probability exceeding 1 δ.
−
ξ 2 ν n+2 nlog(1/δ)+2log(1/δ)
k k2 ≤
We now can prove the claimed bound usin(cid:16) g the upnion bound, (cid:17)
e ǫn e ǫn
Pr m
S
ax kξ k2
2
≥ν ǫn+2√ǫnx+2x
≤ ǫ
Pr kξ k2
2
≥ν ǫn+2√ǫnx+2x
≤ ǫ
e−x.
(cid:26) ∈W (cid:0) (cid:1)(cid:27) (cid:16) (cid:17) (cid:8) (cid:0) (cid:1)(cid:9) (cid:16) (cid:17)
In the first inequality we apply a union bound over with Lemma 37, and in the second inequality we use
W
Lemma 18. We then obtain with probability exceeding 1 δ,
−
max ξ 2 ν ǫn+2 nǫlog(1/δ)+3n2ǫ2log(1/ǫ)+2log(1/δ)+6nǫlog(1/ǫ)
S k S k2 ≤
∈W (cid:16) p (cid:17)
ν 9nǫlog(1/ǫ)+2 nǫlog(1/δ)+2√3nǫ log(1/ǫ)+2log(1/δ)
≤
(cid:16) p p (cid:17)
ν 15nǫlog(1/ǫ)+2 nǫlog(1/δ)+2log(1/δ)
≤
ν(cid:16)(30nǫlog(1/ǫ)). p (cid:17)
≤
In the above, in the first inequality, we note that log n 3nǫlog(1/ǫ) as ǫ<0.5, in the second inequality
ǫn ≤
we note that log(1/ǫ) ≤(log(2))−1/2log(1/ǫ) ≤√3 (cid:0)log (cid:1)(1/ǫ) when ǫ<0.5, the final inequality holds when
n log(1/δ) by solving for the quadratic equation. The proof is complete. (cid:4)
≥ p
23C.2 Eigenvalue Concentration Inequalities
In this section, we derive concentration for the minimal and maximal eigenvalues of the sample covariance
matrix with Nǫ samples removed by the adversary.
Lemma 20 (Sub-Gaussian Covariance Matrix Estimation Vershynin [2010]). Let X Rd n have columns
×
∈
sampled from a sub-Gaussian distribution with sub-Gaussian norm K and second-moment matrix Σ, then
thereexists positive constants c ,C , dependent on thesub-Gaussian norm such that with probability at least
k K
1 2e −cKt2,
−
λ (XX ) n λ (Σ)+λ (Σ) C √dn+t√n .
max ⊤ max max K
≤ · ·
(cid:16) (cid:17)
Lemma21. LetX Rd n havecolumnssampled fromasub-Gaussiandistribution withsub-Gaussiannorm
×
∈
K and second-moment matrix Σ. Let S [n] such that S =ǫn for ǫ (0,0.5) and let represent all such
⊂ | | ∈ W
subsets. Then with probability at least 1 δ,
−
maxλ (X X ) λ (Σ) (10nǫlog(1/ǫ))
S
max S S⊤
≤
max
·
∈W n
minλ (X X ) λ (Σ)
S min [n] \S [⊤n] \S ≥ 4 · min
∈W
when
2 log(2/δ) 1
n
≥ ǫ ·
C K2 ·d+
c
andǫ
≤ 30
·κ−1(Σ).
(cid:18) K (cid:19)
Proof. We will use a union bound to obtain our claimed error bound.
Pr maxλ (X X ) nǫ λ (Σ)+λ (Σ) C √dnǫ+t√nǫ
S
max S S⊤
≥ ·
max max
·
K
·
(cid:26) ∈W e ǫn (cid:16) (cid:17)(cid:27)
Pr λ (X X ) nǫ λ (Σ)+λ (Σ) C √dnǫ+t√nǫ
≤ ǫ
max S S⊤
≥ ·
max max
·
K
·
(cid:16) (cid:17)e ǫnn (cid:16) (cid:17)o
2
e−cKt2
δ.
≤ · ǫ ≤
(cid:16) (cid:17)
In the above, the first inequality follows from a union bound over and Lemma 37, the second inequality
W
follows from Lemma 20. Then from elementary inequalities, we obtain with probability 1 δ,
−
1
λ (X X ) nǫ λ (Σ)+λ (Σ) C √dnǫ+ (nǫ log(2/δ)+3n2ǫ2log(1/ǫ))
max S S⊤
≤ ·
max max
·
K
· c ·
(cid:18) r K (cid:19)
1
n λ (Σ) (ǫ+33/4ǫlog(1/ǫ))+λ (Σ) C √dnǫ+ nǫ log(2/δ)
max max K
≤ · · · · c ·
(cid:18) r K (cid:19)
1
λ (Σ) (6nǫlog(1/ǫ))+λ (Σ) C √dnǫ+ nǫ log(2/δ)
max max K
≤ · · · c ·
(cid:18) r K (cid:19)
λ (Σ) (10nǫlog(1/ǫ)).
max
≤ ·
In the above, the last inequality holds when
2 log(2/δ)
n C2 d+ .
≥ ǫ · K · c
(cid:18) K (cid:19)
and our proof of the upper bound for the maximal eigenvalue is complete. We have from Weyl’s Inequality
for any S ,
∈W
λ (X X )=λ (XX X X ) λ (XX ) λ (X X )
min [n] \S [⊤n] \S min ⊤ − S S⊤ ≥ min ⊤ − max S S⊤
We then have with probability at least 1 δ,
−
1
λ (X X ) n λ (Σ) C √dn n log(2/δ) λ (Σ) (10nǫlog(1/ǫ))
min [n] \S [⊤n] \S ≥ · min − K · − rc
K
· · − max ·
3n n
λ (Σ) λ (Σ) (10nǫlog(1/ǫ)) λ (Σ).
min max min
≥ 4 · − · ≥ 4 ·
24In the above, the first inequality follows when n 32 C d+ 1 log(2/δ) , and from some al-
≥ λ2 min(Σ) K · cK ·
gebra,wefindthelastinequalityholdswhenǫ 1 κ 1(Σ)by(cid:16)notingthatǫ<0.5. Th(cid:17)eproofiscomplete. (cid:4)
≤ 30· −
C.3 Sum of product of nonlinear random variables
We will first define the Orlicz norm for sub-Gaussian random variables.
Definition 22. The sub-Gaussian norm of a random variable X is denoted as X , and is defined as
k
kψ2
X =inf t>0:E exp X2/t2 2 .
k
kψ2
≤
We first note that Gaussian scalars are su(cid:8)b-Gaussia(cid:2)n an(cid:0)d X (cid:1)(cid:3)=O(cid:9)(ν) for X (0,ν2).
k
kψ2
∼N
Lemma 23. Let X (0,ν2), then X = 8/3ν.
∼N k
kψ2
Proof. We have from Definition 22 that p
X =inf c 0:E exp(X2/c2) 2 .
k
kψ2
≥ ≤
We will now solve for the minimizing c. From(cid:8)the PDF(cid:2)of a standar(cid:3)d Ga(cid:9)ussian,
E exp(X2/c2) = 1 ∞ exp X2 1 1 dX
ν√2π c2 − 2ν2
(cid:2) (cid:3)
Z−∞ (cid:18) 1(cid:18)
/2
(cid:19)(cid:19)
1 1 1 −
= =2.
ν√2 c2 − 2ν2
(cid:18) (cid:19)
From some algebra, we find the final inequality above holds when c= 8/3ν. (cid:4)
We now will define the Orlicz norm for sub-exponential random variables.
p
Definition 24. The sub-exponential norm of a random variable X is denoted as X , and is defined as
k
kψ1
X =inf t>0:E[exp(X /t)] 2 .
k
kψ1
{ | | ≤ }
Sub-exponentialrandomvariablesappearfrequentlyinouranalysisfromtheconsequenceofthefollowing
lemma.
Lemma 25 (Lemma 2.7.7 in Vershynin [2020]). Let X,Y be sub-Gaussian random variables, then XY is
sub-exponential, furthermore,
XY X Y
k
kψ1
≤k
kψ2k kψ2
To give probabilistic bounds on the concentration of sub-exponential random variables, we often utilize
Bernstein’s Theorem.
Lemma 26 (Proposition5.16inVershynin[2010]). Let X ,...,X be independent centered sub-exponential
1 N
random variables, and K =max X . Then for every a Rn and t 0,
i
k
i kψ1
∈ ≥
t2 t
Pr a X t 2exp cmin , .
i i ≥ ≤ − K2 a 2 K a
(cid:26) (cid:12)iX∈[N]
(cid:12)
(cid:27) (cid:20) (cid:18) k k2 k k∞(cid:19)(cid:21)
(cid:12) (cid:12)
We are now ready to prove our main results of the section. We use Lemmas 27 and 28 in our proof for
Proposition 30. We use Lemmas 27 and 29 in our proof for Proposition 31.
Lemma 27. Let X = [x ,...,x ] be the data matrix such that for i [N], x are sampled from a sub-
1 N i
∈
Gaussian distribution with second-moment matrix Σ with sub-Gaussian proxy Γ and ξ are sampled from
i
(0,ν2). Assume f : Rd R is bounded over R. Let represent all subsets of [N] of size empty to
N 7→ W
(1 ǫ)N and be a ε-cover of d 1, then for any u d 1 and S . With probability at least 1 δ,
1 − −
− N S ∈S ∈W −
ξ f(x u)x .ν Γ f N ǫlog(1/ǫ),
i i i 2
· k k k k∞
i S
(cid:13)X∈ (cid:13) p
(cid:13) (cid:13)
25where c is an absolute constant and the sample size satisfies
Rd+log(1/δ)
N =Ω .
ǫ
(cid:18) (cid:19)
Proof. We will use the following characterizationof the spectral norm.
f(w x )ξ x = max f(w x )ξ x v
· i i i 2 v Sd−1 · i i i ·
i TP ∈ i TP
(cid:13)X∈ (cid:13) (cid:12)X∈ (cid:12)
(cid:13) (cid:13) (cid:12) (cid:12)
We will first show that f(w x )x is sub-Gaussian. We first note for any v Sd 1, the random variable,
i i −
· ∈
x v is sub-Gaussian by definition. We then have,
i
·
1/p (i) 1/2p (ii)
E f(w x)x vp E f(w x)2p E x v2p f Γ √2 √p.
2
x | · · | ≤ x | · | x | · | ≤ k k∞k k
(cid:16) ∼D (cid:17) (cid:16) ∼D ∼D (cid:17) (cid:16) (cid:17)
In the above, (i) follows from Ho¨lder’s Inequality, (ii) follows from noting from letting q = 2p and noting
fromDefinition 4 that kx
i
·v
kLq
is upper bounded by kΓ k2√q. We thus havef(w ·x i)x
i
·v is sub-Gaussian
for any w Rd and f(w x )x v . Γ f . We have ξ = 8/3ν from Lemma 23, then from
Lemma 25∈ ,therandok mvar· iabi lei ξ· f(k wψ2 x )xk k v2 ik ssk u∞ b-exponentiak lsi .k tψ .2 ξ f(w x )x v . Γ f ν. Let
w suchthatw =argmin i w· i u i ,· where isaε-coverof k (i 0p ,R).· Li eti · k bψ e1 aε-k nek t2 ok fSk d∞1such
tha∈ tN fo1 ranyv Sd 1,thereexiu s∈tNs1 uk − k su2 chthatN u1 v ε. LetB u =argmaN x2 (ξ f(X w)− ) Xu
aend v
∗
=argm∈
ax
ve−
∈Sd−1|(ξ ◦f(X
⊤w∈ ))N ⊤X2
v |. We
thk en− havk e2 f≤
rom the
tr∗
iangle
inequu a∈liNty2 ,| ◦ ⊤ ⊤ |
e
|(ξ ◦f(X⊤w))⊤Xv ∗e−(ξ ◦f(X⊤w))⊤Xu
∗
|≤k(ξ ◦f(X⊤w))⊤X
k2
ku
∗
−v
∗ k2
ε (ξ f(X w)) X .
⊤ ⊤ 2
≤ ·k ◦ k
e e e
whereinthefinalinequalityweusethedefinitionofaε-net. Wethenhavefromthereversetriangleinequality,
e
(ξ f(X w)) Xu (ξ f(X w)) Xv (ξ f(X w)) Xu (ξ f(X w)) Xv
⊤ ⊤ ∗ ⊤ ⊤ ∗ ⊤ ⊤ ∗ ⊤ ⊤ ∗
| ◦ |≥| ◦ |−| ◦ − ◦ |
(1 ε)(ξ f(X w)) Xv .
⊤ ⊤ ∗
≥ − | ◦ |
e e e e
From rearranging,we obtain
e
1
(ξ f(X w)) Xv (ξ f(X w)) Xu . (6)
⊤ ⊤ ∗ ⊤ ⊤ ∗
| ◦ |≤ 1 ε ·| ◦ |
−
With this result we are ready to makeethe probabilistic bounds. Suppeose represents all subsets of [N] of
size empty to (1 ǫ)N. Suppose is a 1/2-netof Sd 1 and is a 1/2-nW etof (w ,R), we can then note
2 − 1 ∗
− N N B
that w R. Then,
∗
k k≤
Pr max max (ξ f(X w)) X t
(cid:26)S ∈Ww ∈N1k
S
◦
S⊤ ⊤ S
k≥
(cid:27)
(6)
≤
Pr (cid:26)m
S
∈a Wx vm ∈a Nx 2wm ∈a Nx 1|(ξ
S
◦f(X S⊤w))⊤X Sv |≥2t
(cid:27)
(iii) e Nǫ t2 t
2 6Rd+d exp cmin , δ
≤ · ǫ − c2ν2 Γ 2 f 2 S c ν Γ f ≤
(cid:16) (cid:17) (cid:20) (cid:18) 1 k k2k k∞| | 1 k k2 k k∞(cid:19)(cid:21)
Intheabove,(iii)followsfromaunionboundover , ,and ,andthenapplyingBernstein’sInequality
1 2
(seeLemma 26). Wealsonotethatlog N =loW g NN ,andN thenapplyingLemma 37givestheinequality.
(1 ǫ)N ǫN
Then to satisfy the above probabilistic co−ndition, it must hold that
(cid:0) (cid:1) (cid:0) (cid:1)
t 4/3c−1c 1ν Γ
2
f 2dNlog(6)+2NRdlog(6)+2Nlog(2/δ)+6N2ǫlog(1/ǫ) 1/2
≥ k k k k∞
Then, when tphe sample size satisfies(cid:0) (cid:1)
(Rd+d)log(6)+log(2/δ) Rd+log(1/δ)
N =Ω .
≥ 3ǫ ǫ
(cid:18) (cid:19)
26We obtain for any S and u ,
1
∈W ∈N
f(u x )ξ x .ν Γ f N ǫlog(1/ǫ).
· i i i 2 k k2 k k∞
i TP
(cid:13)X∈ (cid:13) p
(cid:13) (cid:13)
Our proof is complete. (cid:4)
Lemma 28. Let X = [x ,...,x ] be the data matrix such that for i [N], x are sampled from a sub-
1 N i
∈
Gaussian distribution with second-moment matrix Σ with sub-Gaussian proxy Γ and ξ are sampled from
i
(0,ν2). Assume f : Rd R is bounded over R and Lipschitz. Let represent all subsets of [N]
oN f size empty to (1 ǫ)N. → Suppose w (w ,R) and S . Let W be a ε-net of Sd 1 and define
∗ 1 −
− ∈ B ∈ W N
w=argmin w u . Set a failure probability δ (0,1), then with probability at least 1 δ,
u ∈N1k − k2 ∈ −
max sup ξ f(w x )x ξ f(w x )x
e S ∈Ww ∈B(0,R)
(cid:13)Xi ∈S
i · i i −
Xi ∈S
i · i i (cid:13)2
(cid:13) √N f Γ ν log2(N/δe)+dlog(cid:13)(N/δ)
lip 2
≤ k k k k
Proof. We have from the Cauchy-Schwarzinequality, (cid:0) (cid:1)
sup ξ f(w x )x ξ f(w x )x
i · i i − i · i i 2
w (0,R)
∈B (cid:13)Xi ∈S Xi ∈S (cid:13)
(cid:13) N sup max ξ ix
i 2
f(ew x i) (cid:13)f(w x i)
2
≤ w ∈B(0,R)i ∈[N]k k k · − · k
N f
lip
sup max ξ ix
i 2
(w w)ex
i 2
≤ k k w ∈B(0,R)i ∈[N]k k k − · k
N f lipε 1max ξ ix i 2max x i 2. e
≤ k k i [N]k k i [N]k k
∈ ∈
We will bound the two maximal terms seperately. We first consider the x term. We have
i
k k
t2
Pr max x t NPr 2max x v t 2N6dexp 1 δ. (7)
j 2 1 j 1
(cid:26)j ∈[N]k k ≥ (cid:27)≤
(cid:26)
v ∈N2| · |≥ (cid:27)≤ (cid:18)−2 kΓ k2(cid:19)≤
The final inequality above follows when
t 2 Γ (log(N)+dlog(6)+log(2/δ))1/2.
1 2
≥ k k
We now consider the term ξ x . Wpe note that for any v that x v is sub-Gaussian with norm
i i 2 i
∈ N ·
x v . Γ . We have from the PDF of a Gaussian random variable,
k
i
·
kψ2
k
k2
Pr maxξ i t 2 √2N ∞ e−2x ν2 2dx √2N ∞ xe−2x ν2 2 dx= √2N e−2t ν δ. (8)
(cid:26)i ∈S | |≥ (cid:27)≤ √π Zt ≤ √π Zt √t √π ≤
The final inequality holds when,
t 2ν(log(N)+log(1/δ)).
2
≥
Combining our estimates, we have
Pr max ξ x max x t t δ
i i 2 i 2 1 2
x i [n]k k i [n]k k ≥ ≤
∼D(cid:26) ∈ ∈ (cid:27)
We then choose ε=1/√N and we have with probability at least 1 δ,
−
sup ξ f(w x )x ξ f(w x )x
i · i i − i · i i 2
w (0,R)
∈B (cid:13)Xi ∈S Xi ∈S (cid:13)
(cid:13) .√N f Γ ν log2(N/δe)+d1/2(cid:13) log(N/δ)
lip 2
k k k k
(cid:16) (cid:17)
Our proof is complete. (cid:4)
27Lemma 29. Let X = [x ,...,x ] be the data matrix such that for i [N], x are sampled from a sub-
1 N i
∈
Gaussian distribution with second-moment matrix Σ with sub-Gaussian proxy Γ and ξ are sampled from
i
(0,ν2). Assume f :Rd R is of the form,
N →
1 x>0
f(x)= .
(c x 0
≤
Let represent all subsets of [N] of size empty to (1 ǫ)N. Suppose w (w ,R) and S . Let be a
∗ 1
ε-neW t of Sd 1 and define w=argmin w u .− Set a failure probab∈ iliB ty δ (0,1), the∈ nW with proN bability
at least 1
−
δ,
u ∈N1k − k2 ∈
−
em
S
∈a Wx
w
∈s Bu (p
0,R)
(cid:13)Xi
∈Sξ if(w ·x i)x
i −
Xi
∈Sξ if(w ·x i)x
i (cid:13)2
(cid:13).√N f Γ ν log2(N/δe)+dlog(cid:13)(N/δ)
lip 2
k k k k
P thr eo to wf o. R see pc ea rl al tw
e
f= una cr tg iom nin
clu a∈sNse1 sk
.w
W−
eu
hk a2
vea ,ndthusforany(cid:0)w,wehave kw −w
k2
≤(cid:1)ε 1. Wewillnowconsider
e e
sup ξ f(w x )x ξ f(w x )x
i · i i − i · i i 2
w (0,R)
∈B (cid:13)Xi ∈S Xi ∈S (cid:13)
(cid:13) sup max ξ ix
i 2
f(we x i) f(cid:13)(w x i)
2
≤ w ∈B(0,R)i ∈[N]k k k · − · k
max ξ ix i 2 sup 1 w x i 0,w e x i 0
≤i ∈[N]k k w ∈B(w∗,R) { · ≥ · ≤ }
+max ξ ix
i 2
sup 1 w x
i
0e,w x
i
0
i ∈[N]k k w ∈B(w∗,R) { · ≤ · ≥ }
This upper bound holds for both when f is of the form, e
1 x>0
f(x)=
(c x 0
≤
for any c [0,1). We have with probability at least 1 δ,
∈ −
max ξ x .ν Γ log3/2(N/δ)+d1/2log(N/δ)
i i 2 2
i [N]k k k k
∈ p (cid:16) (cid:17)
We next consider the second term,
Pr sup 1 w x 0,w x 0 t
i i 2
x ∼D(cid:26)w ∈B(w∗,R) { · ≤ · ≥ }≥ (cid:27)
e
≤t−21
x
∼E
D"w
∈Bs (u wp ∗,R)1 {w ·x
i
≤0,w ·x
i
≥0
}#
≤
Θ( 2w πt,w)
≤
2π1
t
arcsin kw w−w e k
≤
2π1
t
wε 1
r
≤t−21ε 1c 1,
2 2 (cid:18) k k (cid:19) 2 k ∗k−
e e
where c is a constant and r <R. We obtain similarly that
1
xP ∼r
D(cid:26)w
∈Bs (u wp ∗,R)1 {w ·x
i
≥0,w ·x
i
≤0 }≥t
2
(cid:27)≤t−21ε 1c 1.
Now we can combine our estimates and obtain with preobability exceeding 1 δ,
−
max sup ξ f(w x )x ξ f(w x )x
S ∈Ww ∈B(0,R)
(cid:13)Xi ∈S
i · i i −
Xi ∈S
i · i i (cid:13)2
(cid:13) .ε ν Γ δ 1 log3/2(N/δe)+d1/2(cid:13) log(N/δ) ,
1 2 −
k k
Our proof is complete. p (cid:16) (cid:17) (cid:4)
28Proposition 30. Let X = [x ,...,x ] be the data matrix such that for i [N], x are sampled from a
1 N i
∈
sub-Gaussian distribution with second-moment matrix Σ with sub-Gaussian proxy Γ and ξ are sampled from
i
(0,ν2). Assume f : Rd R is bounded over R and Lipschitz. Let represent all subsets of [N] of
N → W
size empty to (1 ǫ)N. Suppose w (w ,R) and S . Set a failure probability δ (0,1), then with
∗
− ∈ B ∈ W ∈
probability at least 1 δ,
−
k(ξ
S
◦f(X S⊤w))⊤X
S k2
.ν kΓ
k2
kf k∞N ǫlog(1/ǫ)
Proof. We use the decomposition given in Zhang et al. [2019, Lemmpa A.4]. We obtain,
t
Pr max sup ξ f(w x )x t Pr maxmax ξ f(x u)x
(cid:26)S ∈Ww ∈B(0,R) (cid:13)Xi ∈S i · i i (cid:13)≥ (cid:27)≤ (cid:26)S ∈Wu ∈N1 (cid:13)Xi ∈S i i · i (cid:13)2 ≥ 2 (cid:27)
(cid:13) (cid:13) (cid:13) t (cid:13)
+Pr max sup ξ f(w x )x ξ f(w x )x . (9)
(cid:26)S ∈Ww ∈B(0,R) (cid:13)Xi ∈S i · i i − Xi ∈S i · i i (cid:13)2 ≥ 2 (cid:27)
(cid:13) e (cid:13)
From Lemma 27, we have the first term of Equation (10) is less than δ/2 when
t&ν Γ f N ǫlog(1/ǫ).
2
k k k k∞
From Lemma 28, we have the second term of Equation (p10) is less than δ/2 when
t&√N f Γ ν log2(N/δ)+dlog(N/δ)
lip 2
k k k k
Then, when the sample complexity satisfies (cid:0) (cid:1)
log4(N/δ)+dlog2(N/δ)
N =Ω
ǫ
(cid:18) (cid:19)
We then obtain with probability exceeding 1 δ,
−
(ξ f(X w)) X .ν Γ f N ǫlog(1/ǫ)
k
S
◦
S⊤ ⊤ S k2
k
k2
k k∞
Our proof is complete. p (cid:4)
Proposition 31. Let X = [x ,...,x ] be the data matrix such that for i [N], x are sampled from a
1 N i
∈
sub-Gaussian distribution with second-moment matrix Σ with sub-Gaussian proxy Γ and ξ are sampled from
i
(0,ν2). Assume f :Rd R is of the form,
N →
1 x 0
f(x)= ≥
(c x 0
≤
Let represent all subsets of [N] of size empty to (1 ǫ)N. Suppose w (w ,R) and S . Set a
∗
W − ∈ B ∈ W
failure probability δ (0,1), then with probability at least 1 δ,
∈ −
(ξ f(X w)) X .ν Γ f N ǫlog(1/ǫ)
k
S
◦
S⊤ ⊤ S k2
k
k2
k k∞
Proof. We use the decomposition given in Zhang et al. [2019, Lemmpa A.4]. We obtain,
t
Pr max sup ξ f(w x )x t Pr maxmax ξ f(x u)x
(cid:26)S ∈Ww ∈B(0,R) (cid:13)Xi ∈S i · i i (cid:13)≥ (cid:27)≤ (cid:26)S ∈Wu ∈N1 (cid:13)Xi ∈S i i · i (cid:13)2 ≥ 2 (cid:27)
(cid:13) (cid:13) (cid:13) t (cid:13)
+Pr max sup ξ f(w x )x ξ f(w x )x . (10)
(cid:26)S ∈Ww ∈B(0,R) (cid:13)Xi ∈S i · i i − Xi ∈S i · i i (cid:13)2 ≥ 2 (cid:27)
(cid:13) e (cid:13)
From Lemma 27, we have the first term of Equation (10) is less than δ/2 when
t&ν Γ N ǫlog(1/ǫ).
2
k k
p
29From ??, we have the second term of Equation (10) is less than δ/2 when
t&ν Γ δ 1 log3/2(N/δ)+d1/2log(N/δ)
2 −
k k
p (cid:16) (cid:17)
Then, when the sample complexity satisfies
log3(N/δ)+dlog2(N/δ)
N =Ω .
δǫ
(cid:18) (cid:19)
We have with probability exceeding 1 δ,
−
(ξ f(X w)) X .ν Γ N ǫlog(1/ǫ)
k
S
◦
S⊤ ⊤ S k2
k
k2
Our proof is complete. p (cid:4)
C.4 Covariance Matrix Estimation in an intersection of Half-Spaces
Lemma 32. Fix w Rd 1 and suppose w (w ,R) for a constant R < w . Sample x ,...,x
∗ − ∗ ∗ 1 N
∈ ∈ B k k
i.i.d from a sub-Gaussian distribution with second-moment matrix Σ and sub-Gaussian norm Γ . Suppose
2
k k
S [N] s.t. S (1 ǫ)N. Then with probability at least 1 δ,
⊂ | |≤ − −
Σ (w(t),w ) E [Σ (w(t),w )] .N Γ ǫlog(1/ǫ)
S ∗ −x S ∗ 2 k k
∼D
tP hr ro ouo gf. hoL ue tt tN he1 rb ee laa tn ionε s1
.-(cid:13)
(cid:13)c Wov eer wo ilf lB u( sw
e
t∗ h,R
e
) dea cn od mN po2 sib te ioa nn giε
v2(cid:13)
(cid:13) e- nco iv ner To hf
eoSp
rd e− m1. 1L oe ft Mw e=
i
etar ag l.m [2in 0v 1∈6N]1 tk ow ob− tav ink2
e
Pr max sup Σ (w,w ) E [Σ (w,w )] t
S ∗ S ∗ 2
(cid:26)S ∈Ww ∈B(w∗;R)k −x ∼D k ≥ (cid:27)
t
Pr max sup Σ S(w,w ∗) Σ S(w,w ∗)
2
≤ (S ∈Ww ∈B(w∗;R)k − k ≥ 3 )
e t
+Pr max max Σ (w,w ) E [Σ (w,w )]
S ∗ S ∗ 2
(cid:26)S ∈Ww˜ ∈N1k −x
∼D
k ≥ 3
(cid:27)
t
+Pr sup E e[Σ S(w,w ∗)] Ee[Σ S(w,w ∗)]
2
(cid:26)w
∈B(w∗;R)kx
∼D
−x
∼D
k ≥ 3
(cid:27)
We bound all terms separately. For the first term, e
max sup Σ (w,w ) Σ (w,w )
S ∗ S ∗ 2
S ∈Ww ∈B(w∗;R)k − k
max sup eΣ S(w, w) 2+max sup Σ S( w,w)
2
≤S ∈Ww ∈B(w∗;R)k − k S ∈Ww ∈B(w∗;R)k − k
w w π
≤2 im ∈[a Nx ]kx ix ⊤i k2arcsin (cid:18)ke kw− k2k2 (cid:19)≤ε 1 · kw ∗k−R ·im ∈[a Ne x ]kx ix ⊤i k
e
Then from Equation (7), we have with probability exceeding 1 δ,
−
max x x 2 Γ (log(N)+dlog(6)+log(2/δ)).
i [N]k
i ⊤i k2
≤ k
k2
∈
For the second term, let x=x 1 w x 0 1 w x 0 . We then have,
∗
· { · ≥ }· { · ≥ }
t
Pr maxemax Σ S(ew,w ∗) E [Σ S(w,w ∗)]
2
(cid:26)S ∈Ww˜ ∈N1k −x
∼D
k ≥ 3
(cid:27)
2t
Pr max maxemax X v 2 eE X v 2
≤ (cid:26)S ∈Ww˜ ∈N1v ∈N2|k S⊤ k2−x ∼Dk S⊤ k2|≥ 3
(cid:27)
(i)
2
e Nǫ
(3/ε
)Rd12dexe
p cmin
e t2
,
t δ
≤ ǫ 1 − 144 256 Γ 2S 4 48 Γ ≤ 2
(cid:16) (cid:17) (cid:20) (cid:18) · k k2| | · k k2(cid:19)(cid:21)
30In(i)wenotefromLemma1.12inRigollet and Hu¨tter[2023],thattherandomvariable x v2 E x v2
x
is sub-exponential and x v2 E x v2 16 Γ , we can then apply Bernst| ein· ’s| I− nequ∼ alD it| y · (se| e
k| · | −
x
∼D| · |
kψ1
≤ k
k2
Lemma 26). The probabilistic condition above is then satisfied when, e e
e e
t& N Γ 2(Rdlog(3/ε )+dlog(3/ε )+3Nǫlog(1/ǫ)+log(4/δ)) 1/2
k k2 1 2
We now consider the th(cid:0)ird term. (cid:1)
t
Pr sup E [Σ S(w,w ∗)] E [Σ S(w,w ∗)]
2
(cid:26)w
∈B(w∗;R)kX
∼D
−X
∼D
k ≥ 3
(cid:27)
t
Pr N sup eE [xx (1 w w 0 1 w x 0 ) 1 w x 0 ]
⊤ ∗ 2
≤
(cid:26) w
∈B(w∗;R)kx
∼D
· { · ≥ }− { · ≥ } · { · ≥ }k ≥ 3
(cid:27)
(ii) e t
Pr N sup E xx ⊤ 2 1 w x 0 1 w x 0
≤ (cid:26) w ∈B(w∗;R)x ∼D (cid:2)k k | { · ≥ }− { · ≥ }| (cid:3)≥ 3 (cid:27)
(i ≤ii) Pr (cid:26)N w ∈Bs (u wp ∗;R) (cid:16)x ∼E Dkxx ⊤ k2 2 x ∼E De |1 {w ·x ≥0 }−1 {w ·x ≥0 }| (cid:17)1/2 ≥ 2 3t (cid:27)=0
Intheabove,(ii)followsfromfirstapplyingCauchy-ScehwarzinequalityandthenapplyingJensen’sinequality,
and (iii) follows from Ho¨lder’s Inequality. Then from L L hypercontractivity of , we have that
4 2
→ D
E x 4 LE x 2 =Ltr(Σ). We now consider the second term,
x x
∼Dk k ≤ ∼Dk k
Θ(w,w) 2 w w
E 1 w x 0 1 w x 0 arcsin k − k ε c .
x ∼D| { · ≥ }− { · ≥ }|≤ π ≤ π (cid:18)kw ∗k−r (cid:19)≤ 1 1
e e
Then we obtain zero perobability as indicated in the statement when
t&N Ltr(Σ)ε
1
p
Combining our results, we choose ε =1/N and ε =1/2 for sufficiently large N =Ω Rd+log(1/δ) ,
1 2 ǫ
(cid:16) (cid:17)
max sup Σ (w,w ) E [Σ (w,w )] .N Γ ǫlog(1/ǫ)
S ∗ S ∗ 2 2
S ∈Ww ∈B(w∗,R)k −x ∼D k k k
p
Our proof is complete. (cid:4)
Lemma 33. Suppose E [x] = 0 and E xx = I for x where is a rotationally invariant
x x ⊤
∼D ∼D ∼ D D
distribution. Fix w 1,w 2 ∈Rd and define Θ=ar(cid:2)ccos(cid:3) kww 11 k· kw w2
2k ≤
π 2. Then,
(cid:16) (cid:17)
π Θ sinΘ
E xx ⊤ 1 w 1 x 0 1 w 2 x 0 − − I
x · { · ≥ }· { · ≥ } (cid:23) 2 ·
∼D (cid:18) (cid:19)
(cid:2) (cid:3)
Proof. Since wehave is anisotropicdistribution,we then havethe distributionofUxis isotropicfor any
D
unitary U. Then consider the unitary matrix,
U = w1 w2−Projw2w1 u ... u
kw1k kw2−Projw2w1k 3 d
h i
whereu ,...,u representssomeorthonormalbasisofthecomplementarysubspacetothesubspacespanned
3 d
by w andw . Consider the plane spannedby w and w . Then, w.l.o.g letw =(1,0)androtate w such
1 2 1 2 1 2
that it is in the first quadrant with angle Θ from w from noting that Θ π.
1 ≤ 2
E xx 1 w x 0 1 w x 0
⊤ 1 2
x · { · ≥ }· { · ≥ }
∼D
(cid:2)= E U Uxx U U 1 w x (cid:3)0 1 w x 0
⊤ ⊤ ⊤ 1 2
X · { · ≥ }· { · ≥ }
∼D
=U⊤ (cid:2)E Uxx ⊤U⊤ 1 w
1
x 0 1 w
2
x 0 U(cid:3)
X · { · ≥ }· { · ≥ }
∼D
(cid:2) (cid:3)
31In the above, the first equality follows from noting that U is unitary, and the second equality follows from
the linearity of expectation. Let Ω = x Rd :x U w 0 for ξ 1,2 . Then there exists constants
ξ ⊤ ⊤ ξ
∈ ≥ ∈ { }
α and β such that
(cid:8) (cid:9)
E Uxx U 1 w x 0 1 w x 0
⊤ ⊤ 1 2
x · { · ≥ }· { · ≥ }
∼D
(cid:2)= E yy 1 y 0 1 αy +βy 0(cid:3)
⊤ 1 1 2
y · { ≥ }· { ≥ }
∼D
E(cid:2) y2 1 y 0 1 αy +βy 0 (cid:3) E [y y 1 y 0 1 αy +βy 0 ]
y 1· { 1 ≥ }· { 1 2 ≥ } ··· y 1 d · { 1 ≥ }· { 1 2 ≥ }
= ∼D (cid:2) . .
.
(cid:3) ... ∼D . .
.
.
 E [y y 1 y 0 1 αy +βy 0 ] E y2 1 y 0 1 αy +βy 0 
y d 1 · { 1 ≥ }· { 1 2 ≥ } ··· y d· { 1 ≥ }· { 1 2 ≥ } 
 ∼D ∼D 
 (cid:2) (cid:3) 
d
Intheabove,thefirstrelationfollowsfromrotationalinvariance,whichgivesusthatUX =X foranyunitary
U. Our main tool is a change of coordinates from the euclidean space to n-spherical coordinates as all the
angles will be independent. We first consider the non-diagonal elements. Suppose (i,j) [d] [2] [d] [2]
∈ \ × \
and w.l.o.g i<j, from the rotational invariance, we have
E [Y Y 1 X Ω ]= E r2 sin(φ )cos(φ ) sinφ cosφ =0
Y
j i
· { ∈ } Y
k1 i k2 j
∼D ∼D(cid:20) k1∈Y[i −1] k2∈Y[j −1] (cid:21)
π 2π
Intheabove,thefinalinequalityfollowsfromnotingthat cosθdθ =0andifj =nwehave sinθdθ =0.
0 0
Then for the diagonal elements, we have i=j, and obtain
R R
E (x u )2 1 x Ω = E r2 sin2(φ )cos2(φ )
j k i
x · · { ∈ } x
∼D
(cid:2) (cid:3)
∼D(cid:20) k ∈Y[j −1] (cid:21)
= 1 ∞ r2dµ(r) π/2 sin2(φ )dφ 2π cos2(φ )dφ π sin2(φ )dφ
S
j Z0 Z−π/2+Θ
1 1
Z0
j −1 j −1
k ∈[j −Y1] \{1
}Z0
k k
I II
III
| {z }| {z }
The term of interest here is II. Recall Θ is the angl|e between w and w ,{wze then have }
1 2
π/2 2π 2Θ sin(2Θ) π 2Θ
II = sin2(φ )dφ = − − −
1 1
4 ≥ 2
Z−π/2+Θ
Before proceeding, let us note the following,
1 π/2
E (X U )2 = I III sin2(φ )dφ =1
X ∼D
(cid:2)
⊤ j
(cid:3)
S j · · · Z−π/2 1 1
In the above,the final equality follows from noting is isotropic and thus diagonalcovariance elements are
D
unitary. Wewillnowconsiderthenormalizingterm,notethatE (X U )2 =1as isisotropic. Then,
X ⊤ j
noting that only II is modified when considering 1 X Ω . We∼ thD en obtain S = I ID II π sin2(φ )dφ .
{ ∈ } (cid:2) (cid:3)j · · 0 1 1
Then, from rearranging,we have
R
π 2Θ
E (x u )2 1 X Ω −
j
x · · { ∈ } ≥ π
∼D
(cid:2) (cid:3)
We will now consider the principal 2 2 matrix. Then, consider the vector in R2 in polar coordinates as
×
(rcosΘ,rsinΘ) where Θ is uniformly distributed in [0,2π) and r is independent. Then, to calculate the
32expected outer product, we integrate over the two dimensional space of the intersection of w and w ,
1 2
E Uxx U 1 x U w 0 1 x U w 0
⊤ ⊤ ⊤ ⊤ 1 ⊤ ⊤ 2
x · ≥ · ≥ 2,2
h ∼D (cid:2) (cid:8) (cid:9) (cid:8) (cid:9)(cid:3)i
= E yy 1 y 0 1 αy +βy 0
⊤ 1 1 2
y · { ≥ }· { ≥ }
(cid:20) ∼D (cid:21)2,2
(cid:2) (cid:3)
∞ π/2 cosΘ
= (cosΘ,sinΘ)rdΘdr
sinΘ
Z0 Z−π/2+Θ(cid:18) (cid:19)
∞ r π Θ+sinΘcosΘ sin2Θ
= − dr
2 sin2Θ π Θ cosΘsinΘ
Z0 (cid:18) − − (cid:19)
π Θ+sinΘcosΘ sin2Θ
=(1/4) E r2 −
·x sin2Θ π Θ cosΘsinΘ
∼D (cid:18) − − (cid:19)
π Θ s(cid:2)inΘ(cid:3)
− − I
(cid:23) 2 ·
(cid:18) (cid:19)
Our proof is complete by noting that the planes perpendicular to that of the plane integrated over remain
unchanged by the indicator functions and thus have unitary expectation. (cid:4)
C.5 Scaled Gaussian Matrix
Lemma 34. Fix S Rk n,T Rm ℓ, then sample a matrix G Rn m with entries sampled i.i.d from
× × ×
∈ ∈ ∈
(0,ν2), then with probability exceeding 1 δ,
N −
SGT S T ν 2log(2nm/δ).
F F F
k k ≤k k k k ·
Proof. We first expand the square of the Frobenius normpto obtain
SGT 2 = (S G T )2 S 2 T 2 max (G )2.
k kF i,k1 k1,k2 k2,j ≤k kFk kF i,j [n] [m] i,j
iX∈[k]jX∈[ℓ]k1,k2X∈[n] ×[m] ∈ ×
It then suffices to bound the maximum value of a Gaussian squared over N2 samples. Then, we have from
a union bound and the definition of a Gaussian random variable,
Pr max G2 t = Pr max G √t
Gi,j∼N(0,1) (cid:26)(i,j) ∈[n] ×[m] i,j ≥
(cid:27)
Gi,j∼N(0,1) (cid:26)(i,j) ∈[n] ×[m]| i,j |≥
(cid:27)
√2nm ∞ x2 √2nm ∞ xe−2x ν2 2 √2nm t
e−2ν2dx dx= e−2ν2 δ.
≤ π
Z√t
≤ π
Z√t
√t √π ≤
In the above, (i) follows from a union bound. We thus obtain from elementary inequalities, with failure
probability at most δ,
max G2 ν2 (2log(2nm/δ)).
(i,j) [n] [m] i,j ≤ ·
∈ ×
Our proof is complete. (cid:4)
D Mathematical Tools
In this section, we state additional lemmas referenced throughout the text for completeness.
Lemma 35. Let a,b Rn and X Rp n, then
×
∈ ∈
a ib ix
i
2 ≤ka k2 ∞kb k2 2kXX⊤ k2.
(cid:13)iX∈[n]
(cid:13)
(cid:13) (cid:13)
33Proof. The proof is a simple calculation. Expanding out the LHS, we have
2
a b x = a a b b x x =(a b) X X(a b)
i i i 2 i j i j ⊤i j ◦ ⊤ ⊤ ◦
(cid:13)iX∈[n]
(cid:13)
iX∈[n]jX∈[n]
(cid:13) (cid:13) ≤ka ◦b k2
2
X⊤X
2
≤ka k2 ∞kb k2
2
X⊤X 2,
(cid:13) (cid:13) (cid:13) (cid:13)
where the final inequality comes from noting (cid:13) (cid:13) (cid:13) (cid:13)
a b 2 = a2b2 maxa2 b2 = a 2 b 2.
k ◦ k i i ≤ i [n] i · i k k∞k k2
iX∈[n] ∈ iX∈[n]
Our proof is complete. (cid:4)
Lemma 36 (Lemma 3.11 in Bubeck et al. [2015]). Let f be β-smooth and α-strongly convex over Rn, then
for all x,y Rn,
∈
αβ 1
f(x) f(y),x y x y 2+ f(x) f(y) 2.
h∇ −∇ − i≥ α+βk − k α+βk∇ −∇ k
Lemma 37 (Sum of Binomial Coefficients [Cormen et al., 2022]). Let k,n N such that k n, then
∈ ≤
k
n en k
.
i ≤ k
Xi=0(cid:18) (cid:19) (cid:16) (cid:17)
Lemma 38 (Corollary 4.2.13 in Vershynin [2020]). The covering number of the ℓ -norm ball (0;1) for
2
B
ε<0, satisfies,
3 d
( d (0,1),ε) .
N Bℓ2 ≤ ε
(cid:18) (cid:19)
34