Foundation Model or Finetune? Evaluation of
few-shot semantic segmentation for river pollution
Marga Don1 , Stijn Pinson2, Blanca Guillen Cebrian2, and Yuki M. Asano1,3
1 University of Amsterdam, Amsterdam, The Netherlands
2 The Ocean Cleanup, Coolsingel 6 Rotterdam, The Netherlands
3 Currently at Technical University of Nuremberg
marga.don@student.uva.nl, y.m.asano@uva.nl,
{stijn.pinson, b.guillencebrian}@theoceancleanup.com
Abstract. Foundationmodels(FMs)areapopulartopicofresearchin
AI.Theirabilitytogeneralizetonewtasksanddatasetswithoutretrain-
ingorneedinganabundanceofdatamakesthemanappealingcandidate
forapplicationsonspecialistdatasets.Inthiswork,wecomparetheper-
formanceofFMstofinetunedpre-trainedsupervisedmodelsinthetask
of semantic segmentation on an entirely new dataset. We see that fine-
tunedmodelsconsistentlyoutperformtheFMstested,evenincaseswere
data is scarce. We release the code and dataset for this work here.
Keywords: Foundation models · Segmentation · Computer Vision
1 Introduction
In recent years, Foundation Models (FMs) have emerged as a popular focus of
research in Artificial Intelligence (AI) [3,5,10]. Characterized by their ability
to easily generalize to new domains and tasks, FMs offer an exciting oppor-
tunity for both research and industry. From an industry perspective, however,
FMs are only preferable when they outperform models specifically trained for
a given task. Given that real-life data often differs significantly from data used
in research, models trained on existing datasets likely do not match the task at
hand. Thus, FMs or finetuning an existing model are logical options, though it
is not always evident which is the optimal choice. Finetuned models are said to
require substantial amounts of high-quality data for training, which is often not
easily available in industry contexts. In such cases, FMs could be the solution.
In this work, we investigate whether one should use a FM or Finetune.
To properly investigate this question, we require a dataset of images that
havenotbeenusedinthepretrainingstagesofanyFM.Assuch,weproposethe
RIPTSeg dataset, a real-life dataset containing high-quality images of polluted
rivers around the world, alongside high-quality segmentation masks identifying
the floating patches of trash in these rivers. The images in RIPTSeg have not
been publicly available before.
4202
peS
5
]VC.sc[
1v45730.9042:viXra2 M. Don et al.
Subsequently, we evaluate two segmentation FMs on this dataset: PerSAM
[23],avariantofthepopularSegmentAnything(SAM)model[10],andSegGPT
[21],ageneralistsegmentationmodel.WecomparethesemodelswithaYOLOv8
segmentation model [9], pretrained on COCO [11] and finetuned on RIPTSeg .
WefindthataYOLOv8modelfinetunedonatleast30imagesfromRIPTSeg
outperforms all other tested models, and thus is preferable over FMs, even in
cases where data is scarce.
We summarize our main contributions as follows:
– We introduce the RIPTSeg dataset, consisting of data that has not been
included in any other dataset previously.
– Weinvestigatethetrade-offbetweenFMsandFinetuningpre-trainedmodels
– We explore methods to refine masks predicted by FMs without additional
training
2 Related Work
2.1 Segmentation
In computer vision, segmentation is a fundamental task that involves deciding
whichobjecteachpixelinanimagebelongsto.Dependingonthespecifictasks,
theobjectcategoriesdiffer.Instancesegmentationmethods,forexample,aimto
identify specific instances of objects. In semantic segmentation, categorization
is more focused on the semantic category an object belongs to. In this work, we
focus on semantic segmentation.
Recently, large-scale vision models for segmentation have been proposed, in-
spiredbyadvancesmadeinNaturalLanguageProcessingusinglarge-scalemod-
els [3,19]. For example, the Segment Anything (SAM) model [10], allowing the
user to prompt SAM by defining a point or box denoting the object of interest.
Models based on SAM, like Grounded-SAM [17] or PerSAM [23], attempt to
adapt SAM to automatically generate prompts based on user input. Another
example, Painter [20], is a large-scale model capable of adapting to many seg-
mentation task given an example input-output pair.
2.2 Trash Detection in Water
Previous works in trash detection have often focused on identifying the classes
ofindividualobjects[7],orfocusontrashwasheduponshore[1,8]orfloatingin
the ocean [15]. As far as the authors are aware, this is the first work to compare
FMsandFinetunedmodelsinthetaskofsemanticsegmentationregardingtrash
in rivers.
2.3 Comparing Foundation and Finetuning models
Inpreviousresearch,FMsareoftencomparedtootherFMs.Thefactthatmost
FMs are zero- or few-shot adaptable to unseen datasets often makes them quickEvaluation of few-shot semantic segmentation for river pollution 3
and easy to evaluate, whereas other models must first be finetuned or trained
from scratch. As a result, the comparison between FMs and Finetuned models
is currently understudied.
In [13], the authors compare several FMs to supervised finetuned models in
several tasks in Geospatial data. They find that in text-based settings, FMs can
outperform task-specific supervised models. However, on tasks involving images
ormultimodaldata,finetunedsupervisedmodelshavetheupperhand.Working
withtextualdataonly,[22]surveystheperformanceofFMsonelectronichealth
records, showing that FMs show improved predictive performance compared to
non-FMs. However, they note that FMs should also enable other improvements
inclinicalsettings,suchasrequiringlesslabeleddatatofunction,whichhasnot
yet been appropriately studied.
3 Dataset
We propose the RIPTSeg dataset (RIverine Patch Trash Segmentation), for
benchmarking segmentation methods on patches of floating trash. The dataset
contains 300 high-resolution images (1944x2592) from 6 different locations (50
images per location) with high-quality ground truth segmentation masks for
each image. Several train/test splits of different sizes are defined to allow for
reproducible training and testing of models. In addition, the dataset contains
predefined candidates for prompting; 5 images and 2 masks of floating trash
per image for each location. Note that prompt images used by a model are not
included in evaluation.
During labeling, careful consideration was taken to label the patches as pre-
cisely aspossible. In this context, asingle floating object is alsoseenas a patch.
Multiple floating objects were counted as one patch when there was no clear
separation between them. Patches smaller than 30 pixels were ignored. Floating
non-trash objects (i.e. organic material) were labeled as water.
Fig.1: Exampleimagesfromthe6locationsinthedataset(upper)withgroundtruth
annotations (lower). Yellow denotes in-system trash, pink denotes out-system trash,
light blue denotes water and dark blue denotes the barrier.4 M. Don et al.
In (most of) the rivers included in RIPTSeg , The Ocean Cleanup has in-
stalledbarrierscalledInterceptorstostoptheflowoftrashdownstream.Assuch,
RIPTSeg contains annotations for 4 classes: water, barrier and trash floating in
or out of system. In-system trash is defined as any trash that is floating in the
river and upstream of the Interceptor, meaning it can be or has been stopped
by the system. In contrast, out-of-system trash is downstream of the barrier.
In total, RIPTSeg contains 4387 masks. A breakdown of class balance per
location can be found in Table 1. As indicated in this figure, the 6 locations
included in the dataset are quite diverse. Not only does the class balance vary
greatly per location, but the sizes of masks vary per class as well. For example,
althoughin-systemtrashaccountsfor52.9%ofallannotations,theyonlytakeup
17.4% of the annotated pixels. We can therefore infer that in-system masks are
often relatively small, possibly making them harder to accurately segment. An
exampleimagefromeachlocationwithgroundtruthsegmentationcanbefound
inFigure1.Somehigher-resolutionexamplescanbefoundinthesupplementary
material.
Location % mask instances % of pixels in masks
ID In-systemOut-systemBarrierWaterIn-systemOut-systemBarrierWater
Overall 52.9 10.2 19.8 17.0 17.4 1.1 5.8 75.7
1 72.4 0.0 7.3 20.2 14.4 0.0 1.0 84.6
2 88.5 4.4 0.0 7.1 22.0 0.9 0.0 77.1
3 62.0 18.5 8.3 11.2 14.1 2.0 17.3 66.6
4 21.7 3.0 53.2 22.1 16.9 0.1 5.4 77.6
5 12.9 25.4 44.8 16.9 2.2 4.9 10.4 82.6
6 43.5 11.8 11.2 33.5 24.4 0.4 2.3 72.7
Table 1: TableshowingRIPTSegstatistics.Foreachlocationandoverall,wepresent
the division of mask instances per class, as well as the proportion of pixels belonging
to each class.
3.1 The Ocean Cleanup
The data from RIPTSeg was collected by The Ocean Cleanup, a non-profit
organisationaimingtoridtheoceanfromplastics.Sinceplasticsentertheocean
mostly through rivers [14], The Ocean Cleanup has been decreasing the flow of
trash into the ocean by cleaning up rivers as well. This is done by installing
Interceptors in rivers, which intercept the plastics and other trash on their way
downstream. This trash is then extracted and recycled, in order to stay out
of the natural environment for good. The images from RIPTSeg were collected
from cameras installed by The Ocean Cleanup to monitor the performance of
existing Interceptors, or explore candidate rivers for future deployment. Each
imageinRIPTSegwasinspectedfordepictionsofpeople.Ifapersonwasshown
on an image, they were digitally removed for privacy purposes. Furthermore,
the names of the locations used in RIPTSeg are not used in this paper or the
accompanying codebase.Evaluation of few-shot semantic segmentation for river pollution 5
4 Methods
4.1 RandomForest
A RandomForest classifier [2] is an older technique in the domain of Machine
Learning. In short, it trains a set of decision trees, which at test time ’vote’ for
the most likely class of the given datapoint. In this work, the RandomForest
is used as a baseline method, trained on the RGB pixel values of images until
convergence.Seeingasthedatasetisnew,asimplebaselinemethodallowsusto
compare the more sophisticated models against this baseline.
4.2 PerSAM
PerSAM[23]isbasedontheSAMmodel[10],arecentsegmentationFM.SAMis
used by prompting it with either a point or box prompt, indicating the location
of the object(s) to be segmented. However, when attempting to segment objects
in many images at once, this would require the user to define a prompt for each
image manually. PerSAM attempts to remedy this issue in an efficient, one-shot
manner. Given an example image and mask showing the target object, PerSAM
findsalocationpriorfortheobjectusingfeaturesimilarities.Thislocationprior
isthenusedasapromptforSAM,resultinginthetargetobjectbeingsuccesfully
segmented.NotethatPerSAMusesafrozenSAMmodel,trainedontheextensive
SA-1B dataset, making PerSAM itself training-free.
TheoriginalPerSAM[23]modelisdesignedtopredictonemaskperprompt§.
However, our data often requires multiple masks to be predicted in a single
unseenimage.Todoso,weaddasteptothePerSAMpipeline.PerSAMbasesits
predictions on feature similarites, captured in the similarity matrix S ∈ Rh×w,
where h,w denote the height and width of the target image. Say we have a
predictedmaskM ∈{0,1}h×w.Inordertogenerateanothermask,wegenerate
1
a new similarity matrix S as
1
S =S⊙(1 −M )
1 h,w 1
where⊙denotestheelementwiseproductand1 amatrixofoneswithdimen-
h,w
sions h,w. This way, we virtually ’black out’ the mask M from the similarity
1
matrix, meaning PerSAM will likely look to another point in the image to make
a new prediction.
Note that, in theory, we can keep predicting masks this way ad inifinitum.
Thus, we need to formulate a condition under which to stop predicting new
masks.Inthiswork,wechoosetousethemeanvalueofthemostrecentsimilarity
matrix, the Mean Feature Similarity (MFS). When the MFS falls below a set
threshold, this means the image likely does not contain any more candidates for
masks and prediction stops.
In practice, however, we see that PerSAM can still predict the same mask
multiple times. Thus, if the MFS barely changes after updating the similarity
matrix, we also stop predicting masks, since this indicates that the same mask
was predicted twice in a row.
§Note that we use ’prompt’ to refer to an image-mask pair.6 M. Don et al.
Multiple prompts In cases where there are multiple patches of different sizes
in an image, it could help to use multiple masks to inform the model about the
diversity in patch size. Furthermore, multiple prompt images could inform the
modelaboutchangingconditions,suchaspatchlocationorweather.Inorderto
adapt PerSAM to this usecase, we compute the local features for each prompt
imageandmask.Next,wecombinetheseintoonelocalfeaturerespresentationby
takingthemeanvaluesovertheprompts.Thepipelinethencontinuesunchanged.
PerSAM-F An issue with the original SAM is scale ambiguity. Since users
prompt SAM using point or box prompts, a user could mean to segment a sub-
part of an object instead of the full object. SAM therefore outputs 3 masks of
differing scale as options for the user. PerSAM-F, a variant of PerSAM, aims
to solve this issue using efficient parameter finetuning. Specifically, the prompt
mask is used to finetune 2 weights, which can then adaptively select the correct
mask size for future images. In order to use PerSAM-F with multiple prompts
per image, we finetune the parameters on multiple prompts instead of only one.
Thus,wecombineknowledgeaboutthesizesofallpromptmasksinthefinetuned
parameters.
4.3 SegGPT
A variant of Painter [20], SegGPT is trained specifically for segmentation tasks,
as opposed to vision tasks in general. As with Painter, SegGPT is trained using
pairs of images and their desired outputs, being segmentation masks in the case
of SegGPT. The training procedure involves randomly masking the task output
images and training the model to reconstruct the missing pixels. SegGPT was
trainedonadiversesetofsegmentationdatasets,includingADE20K[24],COCO
[11] and Cityscapes [4], to allow generalization to diverse segmentation tasks.
At inference time, SegGPT is given a prompt image, prompt masks and a
target image. Then, SegGPT is able to identify the correct segmentation task
based on the prompt mask, and perform this task on the target image. To gen-
erate multiple masks for a target image, SegGPT must be prompted repeatedly
with a different prompt image or mask. No changes to the pipeline are neces-
sary.Lastly,sinceSegGPToutputslogitsforeachprediction,wemustthreshold
the predicted mask to create a binary mask. We refer to this parameter as the
Binary Mask Cutoff (BMC).
4.4 YOLOv8
A breakthrough in object recognition, YOLO [16] was the first architecture to
combineobjectlocalizationandclassificationinasingle-stagearchitecture.Since
then, many iterations of the model have resulted in YOLOv8 [9], which is often
regardedasthecurrentstateoftheartinreal-timeobjectdetection[18].Theau-
thorsofYOLOv8havealsocreatedasegmentationmodelbuiltontheYOLOv8
architecture, which performs near state-of-the-art on the COCO dataset [11].Evaluation of few-shot semantic segmentation for river pollution 7
As a comparison method to the FMs, we finetune a YOLOv8 Segmentation
model [9], pretrained on COCO. Training details can be found in Section 5.4.
In order to finetune the model, we combine the in-system and out-system
trash classes into one class. However, this means that our model will also seg-
ment out-of-system trash. In this case, this is not desirable, since the goal is to
estimate only in-system trash. As a remedy, we remove predicted masks based
on their location with respect to the barrier. Specifically, we take the predicted
masks belonging to the barrier class and compute their mean location. Then,
we compute the mean location of each predicted mask and compare it to the
location of the barrier. If the mask is located downstream of the barrier, it is
removed.
4.5 Metrics
The main metric used in this work is mean Intersection over Union (mIoU). In
most segmentation tasks, IoU is measured for each predicted mask. However, in
this case, we are more interested in the IoU of all predicted masks, compared to
the ground truth masks per class. Thus, when computing IoU, we combine the
predicted masks into one mask and compute the IoU of this mask with respect
to the ground truth masks of each class. Our main metric is mIoU-In, the mIoU
of the predicted masks with the in-system ground truth masks. We also report
mIoU with respect to the other classes, indicating to which degree our model
is ’wrong’. A high mIoU-In paired with a high mIoU-Water indicates that the
predicted mask contains trash, but also a lot of water, which is undesirable in
this task. Thus, mIoU-Water, -Out and -Barrier are better when they are lower.
To gain a further understanding of the performance of models on different
sizes of masks, we divide the ground-truth masks into three categories: small,
medium and large, on which we report mIoU-In.
4.6 Post-hoc mask removal
In this work, we are only interested in predicting trash in the area of interest,
namelythebodyofwaterinwhichpatchesoftrasharefound.Predictionsoutside
of this area of interest are not relevant for this task, and therefore not labeled
in the dataset. However, models will likely predict masks outside of the area
of interest, for example when trash is found on the banks of a river. This will
degrade the model’s performance, since the IoU of this predicted mask is 0.
However, these masks should not influence the metrics, since they are outside
of the scope of the task. Thus, we choose to remove all masks predicted outside
of the area of interest. Since the cameras used to capture the images never
change position, the labeled ground truth masks are used to filter out areas of
non-interest.8 M. Don et al.
5 Experiments & results
In this section, we present the experiments we performed and their results. In-
depth discussion of results is reserved for section 6. Table 2 shows results of our
experiments, detailed below. Example qualitative results are shown in Figure 2.
From Table 2, we see that the YOLOv8 model consistently outperforms the
FMs tested. Note that YOLOv8 nearly doubles performance compared to Seg-
GPT on Location 5. Secondly, SegGPT emerges as the second best model, out-
performing PerSAM in most locations. We now discuss experiments performed
for each model.
Location1Location2Location3Location4Location5Location6
RandomForest
mIoU-In% 13.1 18.9 21.6 31.8 6.6 27.2
mIoU-Water% 18.7 18.6 6.2 14.9 1.0 1.2
SegGPT
mIoU-In% 46.0 45.8 60.6 72.8 24.9 73.0
mIoU-Water% 4.8 6.2 2.9 1.3 0.5 4.8
SegGPT + BMC tuning
mIoU-In% 46.0 46.4 61.2 73.8 26.4 73.5
mIoU-Water% 4.8 6.2 2.6 1.1 0.5 4.4
PerSAM
mIoU-In% 16.5 29.0 25.7 42.4 2.5 24.3
mIoU-Water% 65.8 4.2 39.6 27.8 50.7 51.8
PerSAM-F
mIoU-In% 49.3 23.8 39.5 65.6 6.6 31.2
mIoU-Water% 6.3 5.3 8.2 3.4 1.6 14.3
PerSAM-F + MFS tuning
mIoU-In% 49.3 23.8 40.3 68.6 7.3 31.2
mIoU-Water% 1.6 5.3 7.4 5.5 4.8 14.3
YOLOv8
mIoU-In% 71.7 65.4 77.3 82.9 47.5 87.7
mIoU-Water% 0.1 6.3 1.8 0.4 2.4 1.5
Table 2: mIoU-In% and mIoU-Water% reported for different models and experiment
settings. For mIoU-In% higher is better, for mIoU-Water% lower is better. All mod-
els were evaluated on the pre-defined 40% test set to allow fair comparison with the
YOLOv8 models. Highest mIoU-In% per location is shown in bold, second best is
underlined.
5.1 RandomForest
WetrainedaRandomForestmodeluntilconvergenceforeachlocationusing1-5
training images and corresponding masks as training data. An ablation study
identified the highest-performing image combinations, with the best runs re-
ported in Table 2. Both the table and the qualitative analysis in Figure 2 indi-
catesuccessfulmodeltraining.However,themasksproducedareofpoorquality,Evaluation of few-shot semantic segmentation for river pollution 9
Fig.2: Example performance of all 4 models using an image from Location 2
lacking continuity and semantic coherence, which is to be expected of a Ran-
domForest. Despite this, the model provides a valuable baseline for comparing
our other models.
5.2 SegGPT
Similar to RandomForest, we first identified the most informative prompt for
each location using an ablation study with constant BMC. The resulting base-
line performance is presented in Table 2, showing improved performance over
RandomForestdespitedisparitiesbetweenlocations.Additionally,Figure3illus-
trates the effect of changing the BMC using a constant prompt image, showing
that a higher BMC leads to higher mIoU-In% in most cases.
UsingtheinsightsfromTables2and3,weaimtofindthebestcombinationof
prompt image and BMC per location. Shown in Table 2 under ’SegGPT+BMC
tuning’, we see an improvement over the baseline for most locations.
Fig.3: BMC vs mIoU-In% for SegGPT on all locations, with constant prompt image
and mask.10 M. Don et al.
5.3 PerSAM
As with RandomForest and SegGPT, we perform an ablation of the most in-
formative prompt images, for both the training-free version of PerSAM and
PerSAM-F. This gives us a baseline performance, shown in Table 2 under ’Per-
SAM’ and ’PerSAM-F’. From 2, we see PerSAM-F achieves a higher mIoU-In%
in most locations and a lower mIoU-Water% in all locations. It must be noted,
however, that for Location 5 performance remains low across the board.
TofurtherimprovePerSAM-F,wevariedtheMFSwhilekeepingtheprompt
image constant. Results are shown in Figure 4.
Fig.4:MFSvsmIoU-In%forPerSAM-Fonalllocations,withconstantpromptimage
and mask.
We see that as MFS increases, mIoU-In% stays relatively stable, until a
certain threshold where performance drops for some locations. Note that this
is a considerably large drop in performance for only a 0.02 increase in MFS,
implying that PerSAM-F is quite sensitive to the specific MFS used. As before,
we experiment to find the best combination of MFS and prompt image per
location.TheseresultsarepresentedinTable2under’PerSAM-F+MFStuning’,
showing slight performance gain for most locations.
5.4 YOLOv8
As a contrast to the pretrained models, we finetuned a YOLOv8 Segmentation
modelfromUltralytics[9].Modelsweretrainedfor200epochswithbatchsize4,
using the AdamW optimizer [12], initial learning rate 0.001429, momentum 0.9
and weight decay 5e-4. For the largest training set, 80% of the dataset, training
took 1.8 hours on a single NVIDIA GeForce RTX 3060 Laptop GPU. Note that
we trained YOLOv8 on data from all locations.Evaluation of few-shot semantic segmentation for river pollution 11
In addition, we trained YOLOv8 on subsets of the training data, while eval-
uatingthemonthesametestset(20%offulldataset).Carewastakentoensure
that none of the testing datapoints were used in training at any point. These
results are shown in Figure 5. Overall, we see that training on more datapoints
increases performance, as is to be expected. However, we also see that for some
locations the optimal training set size lies around 60%, indicating that overfit-
ting could be occurring for larger training sets. Using the model trained with
60% training data allows us to use 40% of the dataset as a test set, meaning we
can more accurately measure performance on unseen data.
InFigure6weshowtheperformanceofYOLOv8modelstrainedondifferent
training sets compared to SegGPT and PerSAM. We plot mean mIoU-In% over
locations for clarity. We see that YOLOv8 outperforms PerSAM when using 3
images per location, while 5 images are needed to outperform SegGPT. This
shows us that even on scarce data, YOLOv8 outperforms the FMs tested.
In Table 2, we compare the YOLOv8 model finetuned on 60% of the dataset
with the other models. Note that for a fair comparison, we evaluate Random-
Forest,SegGPTandPerSAMattheirbestsettingsperlocationonthe40%test
set used to evaluate the YOLOv8 model.
Fig.5: Performance of YOLOv8 model finetuned on different sizes of training data.
Models were all evaluated on the same 20% test split.12 M. Don et al.
Fig.6: Comparison of YOLOv8 models trained on different training set sizes and the
FMs tested. Models were tested on the 40% test split. For clarity, mIoU-In% was
averaged across locations.
6 Discussion
6.1 Analysis of Models
RandomForestperformspoorly,aswasexpected.Masksareoflowqualityand
notsemanticallycoherent.However,qualitativeanalysisshowsRandomForestis
often able to correctly identify where patches are located.
SegGPTisthebestperformingFMwetested.Itoftenproduceshigh-quality,
tight masks, but sometimes segments only part of a patch. We believe SegGPT
is able to perform strongly on this dataset due to the high similarity between
prompt and target images.
PerSAMispromising,butfailsinsomeessentialways.Aqualitativeanalysis
shows that PerSAM often predicts masks consisting fully of water, indicating
that the localization of the object of interest is sub-optimal. Note that PerSAM
was not originally designed to predict multiple masks, and has shown to be
extremely sensitive to hyperparameters controlling this ability.
YOLOv8isthemostsuccesfulofthemodelstestedinthiswork.Itpredicts
high-quality, tight segmentation masks around patches of trash. However, care
mustbetakentocounteroverfittingwhenfinetuningthemodel.Weseethatwith
only 30 training images, YOLOv8 can outperform both SegGPT and PerSAM.
6.2 Locations
Throughout all experiments, we see extremely varying performance between lo-
cations. At the same time, models seem to agree on which locations are difficult
and which are easy: Location 5 is consistently the worst-performing location,Evaluation of few-shot semantic segmentation for river pollution 13
while Locations 4 and 6 are often the best. This indicates that the particulars
of each location are extremely important.
An analysis of the locations can give an indication: Locations 4 and 6 view
thebarrierfromupstream,meaningthetrashpatchesaggregate’infrontof’the
barrier.Furthermore,thecameraisplacedquiteclosetothebarrier,meaningthe
viewofthepatchisunobstructedandlargepatchestakeupalargeportionofthe
image.Incontrast,atLocation5thecameraviewsthebarrierfromdownstream
and further away. Thus, part of the patch is obstructed by the barrier and even
large patches take up only a relatively small part of the image. In addition, the
backgroundinLocation5oftencontainswasheduptrashorrockswhichareofa
verysimilarcolortothetrash,makingitdifficulttodistinguishbetweenpatches
of trash and background even for a human observer.
7 Conclusion
In this work, we compared the performance of FMs and a finetuned model on
a novel dataset. We find that the finetuned model outperforms the FMs, even
whenfinetuningwithalimiteddataset.Inourtesting,SegGPTshowsimpressive
generalization capabilities, while PerSAM is not as effective. It is non-trivial
to extend PerSAM to predict multiple masks correctly, as it becomes highly
sensitive to hyperparameters controlling this ability.
Application Although RIPTSeg is a highly specialized dataset, we believe it
to be a fair example of a real-life application of segmentation techniques. We
show that in such a case, finetuning a model leads to higher performance than
using a few-shot FM. Given how small YOLOv8 is compared to the FMs tested
and how little data was required to finetune it, finetuning a model is an obvious
choice for any real-life dataset.
It seems that, although the FMs tested show impressive generalization capabil-
ities outside of this work, they are unable to properly adapt to a specialized
dataset in a few-shot setting, while real-life datasets are often quite specialized.
Thus, further research is needed into the practical applications of FMs.
Further research We encourage researchers to evaluate more models, both
FMs and other models, on the RIPTSeg dataset and improve upon our results.
This would further the knowledge of practical applications of FMs, and con-
tributetocleanerriversandoceansthroughimprovingthequalityofdatagath-
ered by The Ocean Cleanup.
Aside from evaluating other models, more work can be done to improve the
masks from SegGPT. For example, predictions can be refined by prompting
SegGPT an additional time with a zoomed-in version of the area of an image
where a mask was predicted. This could allow the model to capture masks in
more detail. Further exploration of suitable prompts for each image could in-
crease performance as well. Lastly, the possibility of finetuning an FM could be
explored, thereby possibly combining the best of both worlds.14 M. Don et al.
References
1. Bao, Z., Sha, J., Li, X., Hanchiso, T., Shifaw, E.: Monitoring of beach litter by
automatic interpretation of unmanned aerial vehicle images using the segmenta-
tion threshold method. Marine Pollution Bulletin 137, 388–398 (2018). https:
//doi.org/https://doi.org/10.1016/j.marpolbul.2018.08.009, https://
www.sciencedirect.com/science/article/pii/S0025326X18305708
2. Breiman, L.: Random forests. Machine Learning pp. 5–32 (2001)
3. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Win-
ter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,
J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language
models are few-shot learners (2020)
4. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding (2016), https://arxiv.org/abs/1604.01685
5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectional transformers for language understanding (2019)
6. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
Animageisworth16x16words:Transformersforimagerecognitionatscale(2021),
https://arxiv.org/abs/2010.11929
7. Gnann, N., Baschek, B., Ternes, T.A.: Close-range remote sensing-based detec-
tion and identification of macroplastics on water assisted by artificial intelli-
gence: A review. Water Research 222, 118902 (2022). https://doi.org/https:
//doi.org/10.1016/j.watres.2022.118902, https://www.sciencedirect.com/
science/article/pii/S0043135422008491
8. Gonçalves,G.,Andriolo,U.,Pinto,L.,Duarte,D.:Mappingmarinelitterwithun-
mannedaerialsystems:Ashowcasecomparisonamongmanualimagescreeningand
machinelearningtechniques.MarinePollutionBulletin155,111158(2020).https:
//doi.org/https://doi.org/10.1016/j.marpolbul.2020.111158, https://
www.sciencedirect.com/science/article/pii/S0025326X20302769
9. Jocher, G., Chaurasia, A., Qiu, J.: Ultralytics yolov8 (2023), https://github.
com/ultralytics/ultralytics
10. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything
(2023)
11. Lin,T.,Maire,M.,Belongie,S.J.,Bourdev,L.D.,Girshick,R.B.,Hays,J.,Perona,
P.,Ramanan,D.,Doll’ar,P.,Zitnick,C.L.:MicrosoftCOCO:commonobjectsin
context. CoRR abs/1405.0312 (2014), http://arxiv.org/abs/1405.0312
12. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2019), https:
//arxiv.org/abs/1711.05101
13. Mai, G., Huang, W., Sun, J., Song, S., Mishra, D., Liu, N., Gao, S., Liu, T.,
Cong, G., Hu, Y., Cundy, C., Li, Z., Zhu, R., Lao, N.: On the opportunities and
challengesoffoundationmodelsforgeospatialartificialintelligence(2023),https:
//arxiv.org/abs/2304.06798
14. Meijer,L.J.J.,vanEmmerik,T.,vanderEnt,R.,Schmidt,C.,Lebreton,L.:More
than1000riversaccountfor80%ofglobalriverineplasticemissionsintotheocean.
Science Advances 7(18), eaaz5803 (2021). https://doi.org/10.1126/sciadv.
aaz5803, https://www.science.org/doi/abs/10.1126/sciadv.aaz5803Evaluation of few-shot semantic segmentation for river pollution 15
15. Politikos, D.V., Adamopoulou, A., Petasis, G., Galgani, F.: Using artificial in-
telligence to support marine macrolitter research: A content analysis and an
online database. Ocean and Coastal Management 233, 106466 (2023). https:
//doi.org/https://doi.org/10.1016/j.ocecoaman.2022.106466, https://
www.sciencedirect.com/science/article/pii/S0964569122004422
16. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified,
real-time object detection (2016)
17. Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen, J., Huang, X., Chen,
Y., Yan, F., Zeng, Z., Zhang, H., Li, F., Yang, J., Li, H., Jiang, Q., Zhang, L.:
Grounded sam: Assembling open-world models for diverse visual tasks (2024)
18. Solawetz, J.: What is yolov8? the ultimate guide. [2024] (Apr 2024), https://
blog.roboflow.com/whats-new-in-yolov8/
19. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave,
E., Lample, G.: Llama: Open and efficient foundation language models (2023),
https://arxiv.org/abs/2302.13971
20. Wang, X., Wang, W., Cao, Y., Shen, C., Huang, T.: Images speak in images: A
generalist painter for in-context visual learning (2023)
21. Wang,X.,Zhang,X.,Cao,Y.,Wang,W.,Shen,C.,Huang,T.:Seggpt:Segmenting
everything in context (2023)
22. Wornow, M., Xu, Y., Thapa, R., Patel, B., Steinberg, E., Fleming, S., Pfeffer,
M.A., Fries, J., Shah, N.H.: The shaky foundations of clinical foundation models:
Asurveyoflargelanguagemodelsandfoundationmodelsforemrs(2023),https:
//arxiv.org/abs/2303.12961
23. Zhang, R., Jiang, Z., Guo, Z., Yan, S., Pan, J., Ma, X., Dong, H., Gao, P., Li, H.:
Personalize segment anything model with one shot (2023)
24. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.:
Ade20k (2019), https://groups.csail.mit.edu/vision/datasets/ADE20K/16 M. Don et al.
8 Supplementary
8.1 Ablations
Dynamic Prompting Firstly,weattemptedtoworkwitha’dynamicprompt’
for SegGPT, where each testing image was matched with the closest-matching
prompt image. Specifically, we embedded the test and prompt images using a
VisionTransformer(ViT)model[6]andcomputedthecosinesimilaritybetween
them.Thepromptimagewiththehighestcosinesimilaritytothetestimagewas
chosen as its prompt. We show results in Table 3. We see that for all locations,
mIoU-In decreases when we attempt to match the prompt image to the test
image.
mIoU-In
Location
No matchingMatching
1 46.0 45.2
2 46.4 41.3
3 61.2 52.8
4 73.8 69.8
5 26.4 18.7
6 73.5 62.5
Table 3: Resultsofmatchingthetestimagetotheclosestpromptimageusingcosine
similarity, for SegGPT. For brevity, we report only mIoU-In%.
Patches RecallthatSegGPTcompressestheimagesfrom1944x2592to448x448,
whichpotentiallycausesalossofinformation.Furthermore,theresultingmasks
must be upsampled to the original resolution, leading to non-precise segmenta-
tions. In an attempt to refine the predictions from SegGPT, we divided each
testing image into a grid of patches. This allows us to preserve more image de-
tails and leads to more precise segmentation masks. We present quantitative
results in Table 4 and qualitative results in Figure 7. We see that as the image
isdividedintomorepatches,largerpatchesarebeingmissed.However,SegGPT
is able to capture more fine details of smaller patches.
Fig.7:Exampleshowingtheeffectofdividingimagesintopatches,shownonLocation
1.Evaluation of few-shot semantic segmentation for river pollution 17
# patches
Location
1 4 9
1 46.0 35.1 15.3
2 45.8 38.8 29.7
3 60.6 60.3 56.7
4 72.8 78.976.6
5 24.9 15.5 15.2
6 73.0 68.8 57.1
Table 4: Results of dividing the images into a grid of 4 or 9 patches, compared with
the best performing run for SegGPT per location as reported in Table 2. For brevity,
we report only mIoU-In%.
8.2 Additional metrics
InTables5-8,wepresentthemodelsevaluatedonthe40%testsplitforeachloca-
tion, as in Table 2, with additional metrics. In addition to the metrics described
inSection4.5,wereportthestandarddeviationofmIoUperclass.Furthermore,
we report the mean Hamming distance, which we compute as
g−p
H =
g
whereg,pdenotethenumberofpixelsinthegroundtruthandpredictedmasks,
respectively. Note that we normalize this with respect to the size of the ground
truth mask. This way, the Hamming distance represents the portion of the
ground truth mask that is mislabeled. Specifically, we present a positive and
negative Hamming distance, representing under- and overestimation of mask
size respectively, in order to showcase the different behavior in these cases.
8.3 Further examples of RIPTSeg images
Below, in Figures 8-13 we show further examples from RIPTSeg , in higher
resolution than in section 3.18 M. Don et al.
Location
mIoU±σ BinnedmIoU Hammingdistance
In-system Water Out-system Barrier SmallMediumLargePositive Negative
1 13.1±11.2 18.7±9.4 N.A. 1.7±0.9 4.4 17.1 28.8 0.3 3.8
2 18.9±13.818.6±17.2 1.5±2.8 N.A. 7.8 18.3 37.4 0.3 -4.1
3 21.6±6.8 6.2±5.0 3.6±3.4 16.7±5.4 14.7 21.2 26.0 0.3 0.5
4 31.8±21.614.9±18.1 0.0±0.1 1.6±0.9 15.8 36.4 51.1 0.3 3.0
5 6.6±7.0 1.0±1.4 2.5±4.2 1.5±1.2 0.2 8.1 7.2 0.6 6.3
6 27.2±23.5 1.2±1.3 0.4±0.8 0.5±1.0 18.0 30.4 30.1 0.6 0.0
Table 5: RandomForestevaluatedonthe40%testsetforeachlocation.N.A.denotes
that a certain class is not present in the ground truth annotations for that location.
Location
mIoU%±σ BinnedmIoU% Hammingdistance
In-system Water Out-system Barrier SmallMediumLargePositive Negative
1 46.0±12.44.8±2.3 N.A. 0.0±0.0 47.8 42.5 46.6 0.3 0.5
2 45.8±15.66.2±3.0 0.1±0.4 N.A. 41.3 53.4 42.2 0.4 0.9
3 61.2±10.32.6±3.3 0.0±0.1 7.3±2.9 57.5 57.0 68.8 0.1 0.3
4 73.8±15.21.1±0.6 0.0±0.1 9.4±7.3 56.6 86.2 84.1 0.0 0.3
5 26.4±17.60.5±0.7 0.2±0.8 6.0±3.9 2.0 31.2 29.3 0.3 4.6
6 73.5±13.94.4±3.9 1.5±3.0 0.4±0.6 57.8 77.8 80.1 0.1 0.2
Table 6: Best runs from SegGPT evaluated on the 40% test set for each location.
N.A. denotes that a certain class is not present in the ground truth annotations for
that location.
Location
mIoU%±σ BinnedmIoU% Hammingdistance
In-system Water Out-system Barrier SmallMediumLargePositive Negative
1 49.3±21.1 1.6±1.0 N.A. 0.0±0.1 57.2 40.1 43.4 0.4 0.2
2 23.8±24.9 5.3±17.2 0.0±0.0 N.A. 14.6 36.6 20.7 0.7 8.0
3 40.3±25.8 7.4±13.6 0.7±2.2 1.5±2.9 52.9 40.5 32.7 0.5 0.8
4 68.6±29.9 5.5±15.8 0.0±0.0 6.2±10.9 49.2 73.0 93.6 0.1 2.5
5 7.3±8.7 4.8±4.9 0.1±0.4 0.1±0.2 0.1 3.5 16.0 0.8 12.0
6 31.2±25.514.3±15.3 0.0±0.0 0.2±0.5 19.1 28.2 45.8 0.7 1.2
Table 7: Best runs from PerSAM-F evaluated on the 40% test set for each loca-
tion.N.A. denotes that a certain class is not present in the ground truth annotations
for that location.Evaluation of few-shot semantic segmentation for river pollution 19
Location
mIoU%±σ BinnedmIoU% Hammingdistance
In-system Water Out-system Barrier SmallMediumLargePositive Negative
1 71.7±10.12.4±1.0 N.A. 0.0±0.1 66.0 72.9 84.2 0.1 0.2
2 65.4±19.06.3±5.9 2.1±5.0 N.A. 50.2 74.3 77.2 0.1 0.4
3 77.3±7.1 1.8±1.4 0.4±1.0 1.2±1.1 73.6 76.5 80.3 0.1 0.1
4 82.9±14.30.4±0.4 0.0±0.0 0.9±1.0 70.6 90.3 92.3 0.1 0.1
5 47.5±29.10.1±0.2 2.4±5.9 0.6±0.5 19.4 62.4 38.2 0.5 0.3
6 87.7±4.9 1.5±1.3 0.0±0.0 0.3±0.5 88.0 86.0 89.8 0.1 0.0
Table 8: YOLOv8evaluatedonthe40%testsetforeachlocation.N.A.denotesthat
a certain class is not present in the ground truth annotations for that location.
Fig.8: Example image of Location 1.
Fig.9: Example image of Location 2.20 M. Don et al.
Fig.10: Example image of Location 3.
Fig.11: Example image of Location 4.Evaluation of few-shot semantic segmentation for river pollution 21
Fig.12: Example image of Location 5.
Fig.13: Example image of Location 6.