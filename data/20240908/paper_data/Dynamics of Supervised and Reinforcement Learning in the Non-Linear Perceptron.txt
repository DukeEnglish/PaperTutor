Dynamics of Supervised and Reinforcement Learning
in the Non-Linear Perceptron
ChristianSchmid
InstituteofNeuroscience
UniversityofOregon
cschmid9@uoregon.edu
JamesM.Murray
InstituteofNeuroscience
UniversityofOregon
jmurray9@uoregon.edu
Abstract
The ability of a brain or a neural network to efficiently learn depends crucially
on both the task structure and the learning rule. Previous works have analyzed
thedynamicalequationsdescribinglearningintherelativelysimplifiedcontextof
theperceptronunderassumptionsofastudent-teacherframeworkoralinearized
output. Whiletheseassumptionshavefacilitatedtheoreticalunderstanding,they
haveprecludedadetailedunderstandingoftherolesofthenonlinearityandinput-
datadistributionindeterminingthelearningdynamics,limitingtheapplicability
of the theories to real biological or artificial neural networks. Here, we use a
stochastic-processapproachtoderiveflowequationsdescribinglearning,applying
thisframeworktothecaseofanonlinearperceptronperformingbinaryclassifica-
tion. Wecharacterizetheeffectsofthelearningrule(supervisedorreinforcement
learning,SL/RL)andinput-datadistributionontheperceptron’slearningcurveand
theforgettingcurveassubsequenttasksarelearned. Inparticular,wefindthatthe
input-datanoisedifferentlyaffectsthelearningspeedunderSLvs.RL,aswellas
determineshowquicklylearningofataskisoverwrittenbysubsequentlearning.
Additionally,weverifyourapproachwithrealdatausingtheMNISTdataset. This
approach points a way toward analyzing learning dynamics for more-complex
circuitarchitectures.
Introduction
Learning, which is typically implemented in both biological and artificial neural networks with
iterative update rules that are noisy due to the noisiness of input data and possibly of the update
ruleitself,ischaracterizedbystochasticdynamics. Understandingthesedynamicsandhowthey
areaffectedbytaskstructure,learningrule,andneural-circuitarchitectureisanimportantgoalfor
designingefficientartificialneuralnetworks(ANNs),aswellasforgaininginsightintothemeansby
whichthebrain’sneuralcircuitsimplementlearning.
Asasteptowarddevelopingafullmathematicalcharacterizationofthedynamicsoflearningfor
multilayer ANNs solving complex tasks, recent work has made progress by making simplifying
assumptionsaboutthetaskstructureand/orneural-circuitarchitecture. Onefruitfulapproachhas
beentostudylearningdynamicsinwhatisperhapsthesimplestnon-trivialANNarchitecture: the
individualperceptron. Evenwiththissimplification,however,fullycharacterizingthemathematics
oflearninghasbeenchallengingforcomplextasks,andfurthersimplificationshavebeenrequired.
Preprint.Underreview.
4202
peS
5
]GL.sc[
1v94730.9042:viXraA B C
Figure 1: Learning dynamics in the nonlinear perceptron. A: The perceptron, parametrized by
weightsw,mapsaninputxtotheoutputyˆ. B:Theinputsaredrawnfromtwomultivariatenormal
distributionswithlabelsy =±1. Theweightvectorwisorthogonaltotheclassificationboundary.
C:Duetothestochasticityinherentintheupdateequations,theweightsaredescribedbytheflowof
aprobabilitydistributioninweightspace.
Oneapproachhasbeentoanalyzelearninginthestudent-teacherframework[GardnerandDerrida,
1989,Seungetal.,1992],inwhichastudentperceptronlearnstoproduceaninput-to-outputmapping
that approximates that of a teacher perceptron. This has led to insights about the differences in
learning dynamics between different types of learning rules (e.g., supervised and reinforcement
learning)[Werfeletal.,2003,Zügeetal.,2023,Pateletal.,2023]. Suchinsightsarehighlyrelevant
forneuroscience,wherealongstandinggoalhasbeentoinferthelearningmechanismsthatareused
inthebrain[Limetal.,2015,Nayebietal.,2020,Portesetal.,2022,Humphreysetal.,2022,Mehta
etal.,2023,Payeuretal.,2023].However,byconstruction,thestudent-teachersetupintheperceptron
onlyappliestoinput-outputmappingsthatarelinearlyseparable,whichisseldomthecaseinpractice.
Anotherapproachhasbeentostudylearningdynamicsinthelinearizedperceptron[Werfeletal.,
2003,Mignaccoetal.,2020,BordelonandPehlevan,2022],whichenablesexactsolutionsevenfor
structuredinputdatadistributionsthatarenotlinearlyseparable. However,thedynamicsoflearning
innonlinearneuralnetworks—evenverysimpleones—performingclassificationtasksarenotfully
understood. Further,whetherandhowthedynamicsoflearningmightdifferunderdifferentlearning
rulesinsuchsettingshasnotbeeninvestigated.
Here,wetakeastochastic-processapproach(similartoYaida[2018]andMurrayandEscola[2020])
toderiveflowequationsdescribinglearninginthefinite-dimensionalnonlinearperceptrontrained
inabinaryclassificationtask(Fig.1). Theseresultsarecomparedfortwodifferentonlinelearning
rules: supervisedlearning(SL,whichcorrespondstologisticregression)andreinforcementlearning
(RL).Wecharacterizetheeffectsoftheinput-datadistributiononthelearningcurve,findingthat,for
SLbutnotforRL,noisealongthecodingdirectionslowsdownlearning,whilenoiseorthogonalto
thecodingdirectionspeedsuplearning. Inaddition,weverifyourapproachbytraininganonlinear
perceptronontheMNISTdataset. Finally,applyingtheapproachtocontinuallearning,wequantify
howtheinputnoiseandlearningruleaffecttherateatwhicholdclassificationsareforgottenasnew
onesarelearned. Together,theseresultsestablishthevalidityoftheapproachinasimplifiedcontext
andprovideapathtowardanalyzinglearningdynamicsformore-complextasksandarchitectures.
Stochastic-processapproachfordescribingweightevolution
Weconsiderageneraliterativeupdateruleoftheform
wt+δt−wt =ηf (wt), (1)
i i i
wherewt ∈Rnforarbitraryn>0,andηisthelearningrate. Thestochasticupdatetermf onthe
i
right-handsideisdrawnfromaprobabilitydistribution—itdependsontheweightsthemselves,as
wellastheinputtothenetworkand,potentially,outputnoise. Startingfromthisupdateequation,
ourgoalistoderiveanexpressioncharacterizingtheevolutionoftheprobabilitydistributionofthe
weights,p(w,t)(cf.Fig.1C).Weassumethatf (w)doesnotexplicitlydependonη,andthatallthe
i
moments⟨fk⟩ ,k =1,2,...,where⟨·⟩ denotesanaverageoverthenoiseintheupdateequation
i L L
(1)(includingtheinputdistributionaswellas,potentially,outputnoise),existassmoothfunctionsof
w.
2Given the stochastic process defined by (1), the probability distribution at time t+δt given the
distributionattimetis
(cid:90)
p(w,t+δt)= dw′p(w,t+δt|w′,t)p(w′,t). (2)
Denotingtheweightupdateasδw:=w−w′,theintegrandinthisequationcanbewrittenas
p(w,t+δt|w′,t)p(w′,t)=p(w+δw−δw,t+δt|w−δw,t)p(w−δw,t). (3)
ChangingtheintegrationvariabletoδwandperformingaTaylorexpansioninδw,theright-hand
sideof(2)yields
(cid:90)
dw′p(w,t+δt|w′,t)p(w′,t)=
(cid:88) ∂ 1(cid:88) ∂2 (4)
p(w,t)− [α (w)p(w,t)]+ [β (w)p(w,t)]+O(δw3),
∂w i 2 ∂w ∂w ij
i i j
i ij
where
(cid:90)
α (w)= dδwδw p(w+δw,t+δt|w,t) (5)
i i
and
(cid:90)
β (w)= dδwδw δw p(w+δw,t+δt|w,t). (6)
ij i j
Here, we assumed that the probability distribution describing the weight updates f has bounded
derivativeswithrespecttow.
Although (2) is only defined for discrete time steps, we assume a continuous probability density
p(w,t)interpolatesbetweentheupdatesandexistsasasmoothfunctionforallvaluesoft. Wecan
thenexpandtheleft-handsideof(2)toobtain
∂
p(w,t+δt)=p(w,t)+δt p(w,t)+O(δt2). (7)
∂t
Fortheiterativeupdaterulesthatwewillconsider,wehaveδw∝η,whereηisalearningrate. In
order to take a continuous-time limit, we let η := δt and take the limit δt → 0. For the general
learningrule(1),thecoefficientsin(4)havetheform
α (w)=⟨f ⟩ , β (w)=⟨f f ⟩ , (8)
i i L ij i j L
where⟨·⟩ denotesanaverageoverthenoiseintheupdateequation(1)(includingtheinputdistribu-
L
tionaswellas,potentially,outputnoise). Thus,wefind
∂p (cid:88) ∂
η ∂t(w,t)=−η
∂w
(p(w,t)⟨f i⟩ L)+O(η2). (9)
i
i
Findingthep(w,t)thatsolvesthisequationcannotingeneralbedoneexactlywhenf isnonlinear.
i
However,bymultiplying(9)withpowersofwandintegrating,aswellasexpandinginw−⟨w⟩,
where ⟨·⟩ denotes the average with respect to p(w,t), we can derive a system of equations for
the moments of p(w,t) [Risken, 1996]. As we derive in the appendix, this gives the following
expressionsforthefirsttwomomentsuptoO((w−⟨w⟩)3):
 
d 1(cid:88)
dt⟨w i⟩=1+
2
Cov(w k,w l)∂ k∂ l⟨f i⟩ L(⟨w⟩), (10)
k,l
d (cid:88)
Cov(w ,w )= [Cov(w ,w )∂ ⟨f ⟩ (⟨w⟩)+Cov(w ,w )∂ ⟨f ⟩ (⟨w⟩)]. (11)
dt i j i k k j L j k k i L
k
Together,theseequationscharacterizetheflowofp(w,t)forageneraliterativelearningalgorithmin
ageneralANNarchitecture.
3A C
E
B D
Figure 2: Learning dynamics in a perceptron classification task. A, B: Flow fields determining
theweightdynamicswithtrajectoriesfordifferentinitialconditionsforSL(A)andRL(B).C,D:
LearningdynamicsfromsimulationscloselyfollowtheanalyticalresultsforSL(C)andRL(D).E:
Dependenceoftheasymptoticweightnormontheregularizationparameterλ.
Learningdynamicsinthenonlinearperceptron
Whiletheaboveapproachisgeneralandcouldbeappliedtoanyiterativelearningalgorithmforany
ANNarchitecture,fortheremainderofthisworkwewillfocusonitsapplicationtothenonlinear
perceptron(Fig.1A),aone-layerneuralnetworkthatreceivesaninputx∈RN,multipliesitwith
a weight vector w ∈ RN, and produces an output yˆ. The task we study is a binary Gaussian
classificationtask,inwhichthemodelispresentedwithsamplesxdrawnfromtwodistributions
p(x|y)withlabelsy =±1,wherep(y =±1)= 1. Eachp(x|y)isgivenbyamultivariatenormal
2
distribution with x ∼ N(µy,Σy) (Fig. 1B). We analyze both the case of SL with deterministic
output,forwhichyˆ= ϕ(w·x),aswellasRL,forwhichthestochasticoutputisgivenbyπ(yˆ=
±1) = ϕ(±w·x),whereϕisthelogisticsigmoidfunction. Thegoalofthemodelistooutputa
labelyˆthatcloselymatchesthegroundtruthywhengivenaninputx.
Derivationoftheflowequations
The supervised learning rule we consider is regularized stochastic gradient descent for a binary
cross-entropyloss,whichresultsintheweightupdaterule
f(w) =(y˜−yˆ)x −λw , (12)
i i i
wherey˜= 1(y+1)∈{0,1}istheshiftedinputlabel,andλistheregularizationhyperparameter.
2
Thislearningruledescribesonlinelogisticregression.
For reinforcement learning, we use the REINFORCE policy-gradient rule with reward baseline
[Williams,1992,SuttonandBarto,2018]:
f(w) =yˆδϕ(−yˆw·x)x −λw . (13)
i i i
Hereδ =yyˆ−⟨yyˆ⟩istherewardpredictionerror,andyˆisthestochasticoutputoftheperceptron
withprobabilityπ(yˆ = ±1) = ϕ(±w·x). Tofacilitatemathematicalfeasibility, wereplacethe
(cid:16) (cid:16)√ (cid:17)(cid:17)
perceptronactivationfunctionbyashiftederrorfunctionϕ(z)= 1 1+Erf πz .
2 4
We first derive the learning dynamics for stochastic gradient descent. We assume that the initial
conditionisuniquelyspecified,withp(w,0)=δ(w−w0). Inthiscase,theweightcovariancewill
4bezero,andtheflowequations(10)simplyreduceto
d (cid:12)
dt⟨w i⟩=⟨f i⟩ L(cid:12) w=⟨w⟩. (14)
Tomaketheformulasmoreconcise,wesetλ=0. Itcanbereintroducedbysimplyaddingtheterm
−λw. Wethenget
⟨f ⟩ (w)=⟨(y˜−ϕ(w·x))x ⟩
i L i x,y
1 1
= ⟨(1−ϕ(w·x))x ⟩ − ⟨ϕ(w·x)x ⟩
2 i x∼N(µ+,Σ+) 2 i x∼N(µ−,Σ−)
  
= 21 µ+ i 1−Φ(cid:113) 1a ++
b2
− 1 2√1 2π( (cid:113)Σ 1+ +·w˜ b2) ie− 2(1a +2 + b2 +) (15)
+ +
 
− 1 2µ−
i
Φ(cid:113) 1a +−
b2
− 1 2√1 2π( (cid:113)Σ 1− +·w˜ b2) ie− 2(1a +2 − b2 −).
− −
√
Here, Φ(z) =
1(cid:0) 1+Erf(cid:0)
z/
2(cid:1)(cid:1)
= ϕ(z
·(cid:112)
8/π) isthe cumulative distribution functionofthe
2
(cid:112)
standardnormaldistribution. Tosimplifynotation,wehaveintroducedw˜ =w· π/8,aswellas
thequantities
a =µy·w˜, (16)
y
√
b = w˜TΣyw˜. (17)
y
Toaidinterpretationoftheseresults,weassumethatµ± =±µandΣ=σ2I. Then(15)implies
(cid:32) (cid:32) (cid:33)(cid:33)
dd t⟨µ·w⟩=|µ|2 1−Φ
(cid:112)
1µ +· σw˜
2|w˜|2
− √1
2π(cid:112)
1σ +2µ σ· 2w |˜ w˜|2e− 2(1( +µ σ·w 2˜ |) w˜2 |2)(cid:12) (cid:12)
(cid:12)
w=⟨w⟩
(18)
aswellas
dd t|⟨w⟩|2 =2w·µ(cid:18) 1−Φ(cid:18) √ 1+µ σ·w˜ 2|w˜|2(cid:19)(cid:19) − 21√ 1σ +2| σw 2| |2 w˜|2e− 2(1( +µ σ·w 2˜ |) w˜2 |2)(cid:12) (cid:12)
(cid:12)
w=⟨w⟩
(19)
Aninterpretationof(18)isthatthefirsttermpushestheweightvectorinthedecodingdirection,while
thesecondtermactsasaregularization,wherebythecross-entropylosspenalizesmisclassifications
moreasµ·wincreases. Anincreaseintheinputnoiseleadstoahigheroverlapofthedistributions,
whichmeansthateventheBayes-optimalclassifierwillmakemoremistakes.
ForRL,weneedtocalculate
⟨f ⟩ (w)=⟨yˆδϕ(−yˆw·x)x )⟩
i L i x,y,yˆ
=⟨ϕ(−w·x)ϕ(w·x)x ⟩ −⟨ϕ(−w·x)ϕ(w·x)x ⟩
i x∼N(µ+,Σ+) i x∼N(µ−,Σ−)
    
= √( 2Σ π(cid:113)+· 1w˜ +) i b2 e− 2(1a +2 + b2 +) 1−2Φ(cid:113) 1+b2a (cid:113)+ 1+2b2 +2µ+ i T (cid:113) 1a ++ b2 , (cid:113) 1+1 2b2  (20)
+ + + + +
    
− √(Σ (cid:113)−·w˜) i e− 2(1a +2 − b2 −) 1−2Φ(cid:113) a (cid:113)− −2µ− i T (cid:113) a − , (cid:113) 1 .
2π 1+b2 1+b2 1+2b2 1+b2 1+2b2
− − − − −
Here,T(·,·)isOwen’sTfunction:
1 (cid:90) a e− 21h2(1+x2)
T(h,a)= dx. (21)
2π 1+x2
0
Asforsupervisedlearning,wecansimplifythisexpressionforisotropicdistributionswithmeans
±µandget
(cid:32) (cid:33)
d µ·w˜ 1
⟨µ·w⟩=|µ|24T ,
(cid:112) (cid:112)
dt 1+σ2|w˜|2 1+2σ2|w˜|2
(22)
(cid:32) (cid:33)
− √1 (cid:112)2σ2µ·w˜ e− 2(1( +µ σ·w 2˜ |) w˜2 |2) Erf
(cid:112)
µ (cid:112)·w˜ (cid:12) (cid:12)
(cid:12)
2π 1+σ2|w˜|2 1+σ2|w˜|2 2+4σ2|w˜|2 w=⟨w⟩
5A B C
Figure3: Relationshipbetweeninputnoiseandtimetolearnthetask. A:Thetimerequiredforthe
alignmentµ·⟨w⟩/|⟨w⟩|toreach80%dependsonthenoiseσoftheisotropicinputdistributions. B:
Tocharacterizeanisotropicinputnoise,thetotalinputvarianceissplitintoanoisecomponentσ2
∥
paralleltoandacomponentσ2 orthogonaltothedecodingdirection. C:Shiftingtheinputnoiseinto
⊥
thedecodingdirectionslowsdownlearning.
and
(cid:32) (cid:33)
d µ·w˜ 1
|⟨w⟩|2 =8w·µT ,
(cid:112) (cid:112)
dt 1+σ2|w˜|2 1+2σ2|w˜|2
(23)
(cid:32) (cid:33)
− 21
(cid:112)
12σ +2| σw 2|| w2 ˜|2e− 2(1( +µ σ·w 2˜ |) w˜2 |2) Erf
(cid:112)
1+σ2|w˜µ |2(cid:112)·w˜
2+4σ2|w˜|2
(cid:12) (cid:12)
(cid:12)
w=⟨w⟩.
Asweshowintheappendix,andasdemonstratedinFig.2,theflowequationsforbothSLandRL
haveaunique,globallystablefixedpointwheneverλ>0ortheinputnoiseσ >0(Fig.2A,B).The
solutionsof(15)and(20)exhibitagreementwithlearningcurvesobtainedbydirectsimulationof
(1)(Fig.2C,D),wherethesmallremainingdiscrepancyarisesfromthefactthat,forthesimulation,
weusedastandardlogisticsigmoidfunctioninsteadoftheerrorfunctionsigmoidcurveusedfor
theanalyticalcalculations. Wealsoseethattheasymptoticweightnormdecreasesapproximately
linearlywithlnλ(Fig.2E).Ofparticularnoteistheobservationthat,perhapscounter-intuitively,
higherlevelsofnoiseappeartoleadtofasterlearningforSL,thoughtheeffectismoreambiguousin
thecaseofRL.Thiswillbeanalyzedinmoredetailinthefollowingsection.
Impactofnoiseonlearningtime
We next investigate the effect of different types of input noise on the dynamics of learning and
whetherdifferencesariseforthesupervisedandreinforcementalgorithms. Webeginwiththecaseof
isotropicinputnoise,withΣ=σ2Iandmeans±µwith|µ|=1. Inthiscase,theoptimalalignment
µ·⟨w⟩ of1isalwaysreachedasymptotically,sowefocusonhowquicklythisvalueisapproachedas
|⟨w⟩|
afunctionoftheinputnoise.
InthecaseofSL,analyticallyanalyzingthelogarithmicderivativeofthealignmentbetweenµand
⟨w⟩yieldsaflowequationoftheform
d µ·⟨w⟩
log =g (µ,w)+σ2h (µ,w)2+O(σ4), (24)
dt |⟨w⟩| iso iso
whereg andh donotdependonσ. Thus,thehighertheinputnoise,thefasterthetaskislearned.
iso iso
TheanalogousrelationshipforRLisindeterminate,suchthatinputnoisemayeitherspeedupor
slowdownlearninginthiscase,dependingontheparameters. AsisillustratedinFig.3A,numerical
integrationoftheflowequationsrevealsqualitativelydistincttrendsforthedependenceoflearning
speedonnoise.
Anisotropicinputdistributions
To analyze the case of anisotropic input noise, we divide the total noise into two components: a
componentσ2 =1+εinthedirectionofµandthenoiseσ2 =1−εorthogonaltoit,whilekeeping
∥ ⊥
6A B
Figure4: Dynamicsofthetotalvarianceofwforisotropicinputnoise. Highernoiseleadstoafaster
decayintr(Cov(w))forsupervisedlearning(A)andforreinforcementlearning(B).
the total noise σ2 +σ2 fixed (Fig. 3B). For both SL and RL, we find that learning slows down
∥ ⊥
whenthenoiseisshiftedtothedecodingdirectionandspeedsupwhenitisshiftedtoorthogonal
directions(Fig.3C).Toconfirmthisanalysisanalytically,wecalculatethelogarithmicderivativeof
thealignmentbetweenµand⟨w⟩andfind
d µ·⟨w⟩
log =g (µ,w)+εh (µ,w)2+O(ε2), (25)
dt |⟨w⟩| an an
whereg andh areindependentofε. Fromthisexpression,weseethat,atleasttoleadingorder
an an
inε,noiseanisotropyorthogonaltothedecodingdirectiontendstoincreasethespeedoflearning,
whileanisotropyalongthedecodingdirectiontendstodecreasethespeedoflearning. Thisisin
apparentcontrasttoarecentstudyintwo-layernetworks,whereinputvariancealongthetask-relevant
dimension was found to increase the speed of learning [Saxe et al., 2019]. The reason for these
seeminglyoppositeresultsisbecause,inthethetaskstudiedinthatwork,variancealongthecoding
directionisasignalthatfacilitateslearning,while,inourcaseofbinaryclassification,variancealong
thecodingdirectionisnoisethatimpairslearning.
Inputnoisecovariance
Sofar,wehaveassumedthattheinitialweightdistribution,whichcanbethoughtofascharacterizing
anensembleofnetworkswithdifferentinitializations,isspecifieddeterministically,i.e.p(w,0)=
δ(w−w0). Inthiscase,accordingto(11),thecovarianceofwwillremainzeroatlatertimes. If
trainingisinsteadinitiatedwithadistributionp(w,0)havingnonzerocovariance,thenwecanask
howthiscovarianceevolveswithtraining—inparticular,whetherthecovarianceofthisdistribution
diverges,convergesto0,orapproachesafinitevalueast→∞.
Thiscalculationcanbeeasilyperformedinthelimitσ →0wheretheinputsarejustx=±µ. Then
(15)simplybecomes
⟨f ⟩ (w)=µ (1−ϕ(µ·w))−λw , (26)
i L i i
and(11)impliesthat
d e−π(µ·w)2/16
tr(Cov(w))=− µTCov(w)µ−2λtr(Cov(w)). (27)
dt 4
SinceCov(w)ispositivesemidefinite,bothtermsontheright-handsideof(27)arealwaysnonposi-
tiveforλ>0andleadtoexponentialdecayoftr(Cov(w)),sotheeigenvaluesofCov(w)approach
zero. Thus,thecovarianceofthedistributionp(w,t)vanishesast→∞(Fig.4A).
ThesamecalculationcanbeperformedfortheRLalgorithm,againwiththeresultthattr(Cov(w))→
0ast→∞wheneverλ>0(Fig.4B).AscanbeseeninFig.4,thetotalvariancecontinuestodecay
tozerouponincludinginputnoise(intheη →0limitweareworkingin),withthedecayspeeding
upasthenoiseisincreased.
Applicationtorealtasks
In order to test whether the theoretical equations derived above apply to realistic input data, we
nexttrainaperceptronwithstochasticgradientdescenttoperformbinaryclassificationwithcross-
7A B C
Figure5: ComparisonofthetheorywithtrainingonMNIST.A:Anonlinearperceptronistrainedto
classifythedigits0and1intheMNISTdataset. B:Comparisonoftheempiricaltestclassification
accuracywiththetheoreticalprediction.C:Evenafterthetaskhasbeenlearned,thetheoryaccurately
capturesnon-trivialongoinglearningdynamics.
A B
Figure6: Forgettingcurves. A:Learningcurvesformulti-tasklearning, wherew aretrainedon
Task1(µ=µ )aftertrainingto80%onTask0(µ=µ ). B:Thealignmentof⟨w⟩withµ after
1 0 0
trainingonadditionaltasks1,...,9.
entropylossontheMNISTdataset(Fig.5A).Toobtainsuitableinputrepresentations,theimages
correspondingtothedigits0and1arefirstconvolvedwithasetof1440Gaborfilters[Haghighat
etal.,2015]. (Intheappendix,weperformthesameanalysisontherawMNISTdatawithoutthe
Gaborconvolutionandobtainsimilarresults.) Wethenmodelthesetwoinputclassesasmultivariate
GaussianswithcovariancesΣ andmeansµ (or±µafteratranslation). Theevolutionofthe
0,1 0,1
weightvectorduringtrainingisfoundbynumericallyintegrating(15). Toquantifythetestaccuracy
duringtraining,anapproximationoftheexpectederrorateachtimestepisderivedbyintegratingthe
Gaussianapproximationstothetwoinputdistributionsuptothehyperplaneorthogonaltotheweight
vector. AscanbeseeninFig.5B,thistheoreticallyderivedlearningcurvecloselymatchestheactual
generalizationperformanceofthetrainedclassifieronthehold-outset.
To further illustrate that the flow equations capture non-trivial aspects of the learning dynamics,
Fig.5Cshowsthealignmentofwwithµ,whichcontinuestoevolveafterthetaskhasbeenlearned.
Theclosealignmentoftheexperimentalresultswiththeanalyticalpredictionsshowsthattheflow
equationscancapturelearningdynamicsinarealistictaskwithinputdatadistributionsthatarenot
necessarilyGaussian.
Continuallearning
Inadditiontodescribingthedynamicsoflearningasingletask,theflowequationsderivedabove
canalsodescribethelearningandforgettingofmultipletasks. Incontinuallearning,naturaland
artificialagentsstrugglewithcatastrophicforgetting,whichcausesolderlearningtobelostasitis
overwrittenwithnewerlearning[Hadselletal.,2020,Kudithipudietal.,2022,Fleschetal.,2023].
Here,weaskhowthenumberoftasksthatcanberememberedbytheperceptrondependsonthelevel
ofnoiseandthelearningalgorithm. TheweightsarefirsttrainedonTask0,withinputdistribution
defined by µ = µ and Σ = σ2I, until the alignment of w with µ has reached 80%. We then
0 0
trainonsubsequenttasksµ=µ ,µ ,.... Thisyieldsaforgettingcurvethatdecaysexponentially
1 2
8withthenumberoftasks,asshowninthesimulationresultsinFig.6. Thedecayconstantdoesnot
significantlydependonthelearningalgorithmbeingused,butweobservethatahigherinputnoise
leadstofasterforgetting. Togetherwiththeresultsintheprecedingsubsections,thishintstowarda
trade-offbetweenthelearningspeedandforgettingofpreviouslylearnedtasksastheamountofinput
noiseisvaried.
Discussion
In this work, we have used a stochastic-process framework to derive the dynamical equations
describinglearninginthenonlinearperceptronperformingbinaryclassification. Wehavequantified
howtheinputnoiseandlearningruleaffectthespeedoflearningandforgetting,inparticularfinding
that greater input noise leads to faster learning for SL but not for RL. Finally, we have verified
that our approach captures learning dynamics in an MNIST task that has a more-complex input
datadistribution. Together,theresultscharacterizewaysinwhichtaskstructure,learningrule,and
neural-circuitarchitecturesignificantlyimpactlearningdynamicsandforgettingrates.
OnelimitationofourapproachistheassumptionthattheinputdistributionsaremultivariateGaussians,
which may not be the case for real datasets. While the agreement between the theoretical and
empiricalresultsappliedtotheMNISTdatainFig.5isencouraginginthisregard,theremaybe
greaterdiscrepanciesincaseswheretheinputdistributionsaremorecomplex. Indeed,recentwork
onthenonlinearperceptronhasshownthat,whilethefirst-andsecond-ordercumulantsoftheinput
distributionarelearnedearlyintraining,laterstagesoftraininginvolvelearningbeyond-second-order
(i.e. non-Gaussian) statistical structure in the input data [Refinetti et al., 2023], suggesting that
ourtheory’sabilitytodescribelate-stagetrainingincomplexdatasetsmaybesomewhatlimited.
Anotherlimitationisthechoicetoneglecthigher-ordertermsinw−⟨w⟩(Equations(10),(11))and
η(Equation(9)). Thismaylimittheabilitytocharacterizeinstabilitiesandnoiseeffectsinducedby
non-infinitesimallearningrates. Futureworkwillbeneededtoassesstheseeffects.
While other work has approached SGD learning in neural networks within a stochastic-process
framework,mostoftheseworkshavenotderivedthenoisestatisticsfromthenoisyupdaterule(as
donehereandinYaida[2018]andMurrayandEscola[2020]),butratherhaveaddedGaussiannoise
tothemeanupdate(e.g.[Heetal.,2019,Lietal.,2019,2021]). Whiletheresultsfortheflowof
theweights’mean⟨w⟩(t)arethesameunderbothapproaches,theapproachthatwetakeenables
ustoadditionallyderivetheflowoftheweightcovariance. Further,itallowsforthepossibilityof
describing effects arising from finite learning rate by including higher-order terms in η from the
expansionof(4)—atopicthatwewilladdressinanupcomingpublication.
Inourresultsoncontinuallearning,wefoundthatonlyafewtaskscouldberememberedbythe
perceptronbeforebeingoverwritten. Thisisperhapssomewhatsurprisinggivenrecentwork[Murray
andEscola,2020]showingthatthebinaryperceptroncanrecallO(N)individualrandompatternsin
acontinual-learningsetup. Thisdifferencemayariseinpartfromthefactthatthatworkusedamore
efficient,margin-basedsupervisedlearningrule[Crammeretal.,2006]ratherthanthestochastic
gradientdescentruleusedhere,aswellasthefactthatinputnoiseandweightregularizationwerenot
included. Thisdifferencesuggeststhatthereislikelyroomforsignificantimprovementsincontinual-
learningperformancewiththesetupstudiedhere. Thiswouldbeanotherinterestingdirectionfor
futurework,giventhatrecentworkhasfoundthatnonlinearitycandrasticallyincreasetheamountof
catastrophicforgettingincontinuallearning[Dominéetal.,2023].
Finally,wespeculatethatqualitativedifferencesbetweenlearningrulessuchasthatshowninFig.3
mayprovideapathfordesigningexperimentstodistinguishbetweenlearningrulesimplementedin
thebrain. Moreworkwillbeneeded,however,toformulatetestableexperimentalpredictionsfor
more-realisticlearningrulesandnetworkarchitectures. Moregenerally,theapproachdevelopedhere
pavesthewayforanalyzingnumerousquestionsaboutlearningdynamicsinmore-complexcircuit
architecturesanddiversetaskstructures.
Acknowledgements
WearegratefultoElliottAbeforearlycollaborationrelatedtothisproject. Supportforthisworkwas
providedbyNIH-BRAINawardRF1-NS131993.
9References
BlakeBordelonandCengizPehlevan.LearningcurvesforSGDonstructuredfeatures.InternationalConference
onLearningRepresentations.ArXivpreprintarXiv:2106.02713,2022.
JamesBradbury,RoyFrostig,PeterHawkins,MatthewJamesJohnson,ChrisLeary,DougalMaclaurin,George
Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable
transformationsofPython+NumPyprograms,2018. URLhttp://github.com/google/jax.
KobyCrammer,OferDekel,JosephKeshet,ShaiShalev-Shwartz,YoramSinger,andManfredKWarmuth.
Onlinepassive-aggressivealgorithms. JournalofMachineLearningResearch,7(3),2006.
ClémentineCJDominé,LukasBraun,JamesEFitzgerald,andAndrewMSaxe. Exactlearningdynamicsof
deeplinearnetworkswithpriorknowledge. JournalofStatisticalMechanics:TheoryandExperiment,2023
(11):114004,2023.
TimoFlesch,AndrewSaxe,andChristopherSummerfield.Continualtasklearninginnaturalandartificialagents.
TrendsinNeurosciences,46(3):199–210,2023.
ElizabethGardnerandBernardDerrida. Threeunfinishedworksontheoptimalstoragecapacityofnetworks.
JournalofPhysicsA:MathematicalandGeneral,22(12):1983,1989.
RaiaHadsell,DushyantRao,AndreiARusu,andRazvanPascanu. Embracingchange:Continuallearningin
deepneuralnetworks. Trendsincognitivesciences,24(12):1028–1040,2020.
MohammadHaghighat,SamanZonouz,andMohamedAbdel-Mottaleb. Cloudid:Trustworthycloud-basedand
cross-enterprisebiometricidentification. ExpertSystemswithApplications,42(21):7905–7916,2015. URL
https://github.com/mhaghighat/gabor. BSD2-clauselicense.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize well:
Theoreticalandempiricalevidence. Advancesinneuralinformationprocessingsystems,32,2019.
PeterCHumphreys,KayvonDaie,KarelSvoboda,MatthewBotvinick,andTimothyPLillicrap. Bcilearning
phenomenacanbeexplainedbygradient-basedoptimization. bioRxiv,pages2022–12,2022.
ZiweiJiandMatusTelgarsky. Directionalconvergenceandalignmentindeeplearning. AdvancesinNeural
InformationProcessingSystems,33:17176–17186,2020.
DhireeshaKudithipudi,MarioAguilar-Simon,JonathanBabb,MaximBazhenov,DouglasBlackiston,Josh
Bongard,AndrewPBrna,SurajChakravarthiRaja,NickCheney,JeffClune,etal. Biologicalunderpinnings
forlifelonglearningmachines. NatureMachineIntelligence,4(3):196–210,2022.
QianxiaoLi,ChengTai,andEWeinan. Stochasticmodifiedequationsanddynamicsofstochasticgradient
algorithmsi:Mathematicalfoundations. JournalofMachineLearningResearch,20(40):1–47,2019.
ZhiyuanLi,SadhikaMalladi,andSanjeevArora. Onthevalidityofmodelingsgdwithstochasticdifferential
equations(sdes). AdvancesinNeuralInformationProcessingSystems,34:12712–12725,2021.
SukbinLim,JillianLMcKee,LukeWoloszyn,YaliAmit,DavidJFreedman,DavidLSheinberg,andNicolas
Brunel. Inferringlearningrulesfromdistributionsoffiringratesincorticalneurons. Natureneuroscience,18
(12):1804–1810,2015.
YashMehta,DanilTyulmankov,AdithyaERajagopalan,GlennCTurner,JamesEFitzgerald,andJanFunke.
Modelbasedinferenceofsynapticplasticityrules. bioRxiv,pages2023–12,2023.
FrancescaMignacco,FlorentKrzakala,PierfrancescoUrbani,andLenkaZdeborová. Dynamicalmean-field
theoryforstochasticgradientdescentingaussianmixtureclassification. AdvancesinNeuralInformation
ProcessingSystems,33:9540–9550,2020.
JamesMMurrayandGSeanEscola. Remembranceofthingspracticedwithfastandslowlearningincortical
andsubcorticalpathways. NatureCommunications,11(1):1–12,2020.
AranNayebi,SanjanaSrivastava,SuryaGanguli,andDanielLYamins. Identifyinglearningrulesfromneural
networkobservables. AdvancesinNeuralInformationProcessingSystems,33:2639–2650,2020.
DonaldBruceOwen. Atableofnormalintegrals. CommunicationsinStatistics-SimulationandComputation,9
(4):389–419,1980.
10NishilPatel,SebastianLee,StefanoSaraoMannelli,SebastianGoldt,andAndrewMSaxe. Therlperceptron:
Dynamicsofpolicylearninginhighdimensions. InICLR2023WorkshoponPhysicsforMachineLearning,
2023.
AlexandrePayeur,AmyLOrsborn,andGuillaumeLajoie. Neuralmanifoldsandlearningregimesinneural-
interfacetasks. bioRxiv,pages2023–03,2023.
Jacob Portes, Christian Schmid, and James M Murray. Distinguishing learning rules with brain machine
interfaces. Advancesinneuralinformationprocessingsystems,35:25937–25950,2022.
MariaRefinetti,AlessandroIngrosso,andSebastianGoldt. Neuralnetworkstrainedwithsgdlearndistributions
ofincreasingcomplexity. InInternationalConferenceonMachineLearning,pages28843–28863.PMLR,
2023.
HannesRisken. TheFokker-PlanckEquation. Springer,1996.
AndrewMSaxe,JamesLMcClelland,andSuryaGanguli. Amathematicaltheoryofsemanticdevelopmentin
deepneuralnetworks. ProceedingsoftheNationalAcademyofSciences,116(23):11537–11546,2019.
HyunjuneSebastianSeung,HaimSompolinsky,andNaftaliTishby. Statisticalmechanicsoflearningfrom
examples. PhysicalreviewA,45(8):6056,1992.
RichardSSuttonandAndrewGBarto. Reinforcementlearning:Anintroduction. MITpress,2018.
PauliVirtanen,RalfGommers,TravisE.Oliphant,MattHaberland,TylerReddy,DavidCournapeau,Evgeni
Burovski,PearuPeterson,WarrenWeckesser,JonathanBright,StéfanJ.vanderWalt,MatthewBrett,Joshua
Wilson,K.JarrodMillman,NikolayMayorov,AndrewR.J.Nelson,EricJones,RobertKern,EricLarson,
CJCarey,˙IlhanPolat,YuFeng,EricW.Moore,JakeVanderPlas,DenisLaxalde,JosefPerktold,Robert
Cimrman,IanHenriksen,E.A.Quintero,CharlesR.Harris,AnneM.Archibald,AntônioH.Ribeiro,Fabian
Pedregosa,PaulvanMulbregt,andSciPy1.0Contributors. SciPy1.0:FundamentalAlgorithmsforScientific
ComputinginPython. NatureMethods,17:261–272,2020. doi:10.1038/s41592-019-0686-2.
JustinWerfel,XiaohuiXie,andHSeung. Learningcurvesforstochasticgradientdescentinlinearfeedforward
networks. Advancesinneuralinformationprocessingsystems,16,2003.
RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.
Machinelearning,8:229–256,1992.
ShoYaida. Fluctuation-dissipationrelationsforstochasticgradientdescent. InInternationalConferenceon
LearningRepresentations,2018.
PaulZüge,ChristianKlos,andRaoul-MartinMemmesheimer. Weightversusnodeperturbationlearningin
temporallyextendedtasks:Weightperturbationoftenperformssimilarlyorbetter. PhysicalReviewX,13(2):
021006,2023.
11Derivationofthegeneralevolutionequations
Inthissection,wederiveequations(10)and(11)from(9). Westartwith
∂p (cid:88) ∂
∂t(w,t)=−
∂w
(p(w,t)⟨f j⟩ L), (28)
j
j
multiplybothsidesbyw andintegrateoverw. Theleft-handsidesimplybecomes
i
d
⟨w ⟩ . (29)
dt i w
Fortheright-handside,wecanuseintegrationbypartstoget
(cid:90) (cid:88) ∂ (cid:90)
− dww (p(w,t)⟨f ⟩ )= dwp(w,t)⟨f ⟩ =⟨⟨f ⟩ (w)⟩ . (30)
i ∂w j L i L i L w
j
j
Toevaluatethisexpectationvalue,weintroducethemean-zeroweightwˆ =w−⟨w⟩,whichdescribes
thefluctuationsofwarounditsmean. Ifweexpand⟨f ⟩ (w)tosecondorderinwˆ,(30)becomes
i L
(cid:42) (cid:43)
(cid:88) 1(cid:88)
⟨⟨f ⟩ (w)⟩ = ⟨f ⟩ (⟨w⟩)+ wˆ ∂ ⟨f ⟩ (⟨w⟩)+ wˆ wˆ ∂ ∂ ⟨f ⟩ (⟨w⟩)+O(wˆ3)
i L w i L j j i L 2 j k j k i L
j j,k w
(cid:88) 1(cid:88)
=⟨f i⟩ L(⟨w⟩)+ ⟨wˆ j⟩ w∂ j⟨f i⟩ L(⟨w⟩)+
2
⟨wˆ jwˆ k⟩ w∂ j∂ k⟨f i⟩ L(⟨w⟩)+O(wˆ3) (31)
j j,k
1(cid:88)
=⟨f ⟩ (⟨w⟩)+ Cov(w ,w )∂ ∂ ⟨f ⟩ (⟨w⟩)+O(wˆ3).
i L 2 k j j k i L
j,k
Thederivationof(11)followsanalogously.
DerivationofexplicitSLandRLflowequations
Inordertoanalyze(10)and(11),wemustevaluatethefollowingexpectationvalues:
⟨ϕ(w·x)x ⟩ and ⟨ϕ2(w·x)x ⟩ (32)
i x∼N(µ,Σ) i x∼N(µ,Σ)
(cid:16) (cid:16)√ (cid:17)(cid:17)
withϕ(x)= 1 1+Erf πx . Withoutlossofgenerality,wewillcalculatetheseintegralsina
2 4
coordinatesystemwherew=w e . Wecanthenfactorizep(x ,x ,...)=p (x )p (x ,...|x ).
1 1 1 2 m 1 c 2 1
ThemarginaldistributionisGaussianwithµ =µ andΣ =Σ ,andtheconditionaldistribution
m 1 m 11
isalsonormalwith(µ ) =µ + 1 Σ (x −µ ),and(Σ ) =Σ − 1 Σ Σ .
c i i Σ11 1i 1 1 c ij ij Σ11 1i 1j
(cid:112)
Furthermore,tosimplifynotations,weintroducew˜ =w· π/8,aswellasthequantities
a=µ·w˜, (33)
√
b= w˜TΣw˜. (34)
Let’sfirstcalculate⟨ϕ(w·x)⟩ :
x∼N(µ,Σ)
⟨ϕ(w·x)⟩ =⟨ϕ(w x )⟩
x∼N(µ,Σ) 1 1 x∼N(µ,Σ)
1 (cid:90)
= √ dx 1e− 21(x1−µ1)2/Σ11ϕ(w 1x 1)
2πΣ 11 R
=
√1 (cid:90) due−u2/2ϕ(cid:16)
w (µ
+u(cid:112)
Σ
)(cid:17) (35)
1 1 11
2π R
(cid:18) (cid:19)
a
=Φ √ ,
1+b2
√
where Φ(x) = 1 + 1Erf(x/ 2) is the cumulative distribution function of the standard normal
2 2
distribution. Toevaluatethelastlineofthisandthefollowingintegrals,weusedthereference[Owen,
1980].
12Fortheintegral⟨ϕ(w·x)x ⟩ ,wefirstdothecalculationfori=1:
i x∼N(µ,Σ)
⟨ϕ(w·x)x ⟩ =⟨ϕ(w x )x ⟩
1 x∼N(µ,Σ) 1 1 1 x∼N(µ,Σ)
1 (cid:90)
= √ dx 1e−1 2(x1−µ1)2/Σ11ϕ(w 1x 1)x
1
2πΣ 11 R
=
√1 (cid:90) due−u2/2ϕ(cid:16)
w (µ
+u(cid:112)
Σ
)(cid:17)
(µ
+u(cid:112)
Σ )
(36)
1 1 11 1 11
2π R
(cid:18) (cid:19)
=µ 1Φ √ a + √1 √Σ 11w˜ 1 e− 2(1a +2 b2).
1+b2 2π 1+b2
Fori̸=1,weget
⟨ϕ(w·x)x ⟩ =⟨ϕ(w x )x ⟩
i x∼N(µ,Σ) 1 1 i x∼N(µ,Σ)
1 (cid:90) (cid:18) Σ (cid:19)
= √
2πΣ
11
Rdx 1e− 21(x1−µ1)2/Σ11ϕ(w 1x 1) µ i+
Σ
11 1i(x 1−µ 1)
(cid:18) (cid:19)
=µ iΦ √ a + √1 √Σ i1w˜ 1 e− 2(1a +2 b2).
1+b2 2π 1+b2
(37)
Thus,forgeneraliandwwecanwrite
(cid:18) (cid:19)
⟨ϕ(w·x)x i⟩
x∼N(µ,Σ)
=µ iΦ √ a + √1 √(Σw˜) i e− 2(1a +2 b2). (38)
1+b2 2π 1+b2
Wenextcalculate
⟨ϕ2(w·x)⟩ =⟨ϕ2(w x )⟩
x∼N(µ,Σ) 1 1 x∼N(µ,Σ)
1 (cid:90)
= √
2πΣ
11
Rdx 1e− 21(x1−µ1)2/Σ11ϕ2(w 1x 1)
(39)
(cid:18) (cid:19) (cid:18) (cid:19)
a a 1
=Φ √ −2T √ ,√ ,
1+b2 1+b2 1+2b2
whereT standsforOwen’sT function.
Analogously,wecancalculate
⟨ϕ2(w·x)x ⟩ =
i x∼N(µ,Σ)
µ i⟨ϕ2(w·x)⟩+ √2(Σ √w˜) i Φ(cid:18) √ a √ (cid:19) e− 2(1a +2 b2). (40)
2π 1+b2 1+b2 1+2b2
Fixedpointanalysis
Inthissection,weanalyzethefixedpointsofthesystemsofequations(18)&(19)forSLand(22)&
(23)forRL.Wewillfirstshowtheintuitiveresultthatanyfixedpoint⟨w∗⟩ismaximallyalignedwith
µ,i.e.⟨w∗⟩·µ=|µ|·|⟨w∗⟩|,aslongasσ >0. Forsimplicity,wesettheregularizationparameter
λ=0. NotethatforbothSLandRL,theflowequationstaketheform
d
⟨w⟩·µ=|µ|2f (µ,w,σ)−µ·⟨w⟩f (µ,w,σ),
dt 1 2
(41)
1 d
|⟨w⟩|2 =µ·⟨w⟩f (µ,w,σ)−|⟨w⟩|2f (µ,w,σ)
2dt 1 2
forsomefunctionsf andf >0. Also,it’seasytoseethat⟨w∗⟩·µ>0. Thus,afixedpoint⟨w∗⟩
1 2
satisfies
f µ·⟨w∗⟩
1 = ,
f |µ|2
2
(42)
f |⟨w∗⟩|2
1 = .
f µ·⟨w∗⟩
2
13Thus,settingtheseequaltooneanother,wefindthat⟨w∗⟩·µ=|µ|·|⟨w∗⟩|andthetwoequations
reducetoasingleequationfor|⟨w∗⟩|.
Assumewithoutlossofgeneralitythat|µ|=1. Forsupervisedlearning,(19)thenimpliesthat
(cid:32) √ (cid:33)
|⟨w∗⟩| |⟨w∗⟩| π 1 σ2|⟨w∗⟩|2
−
|⟨w∗⟩|2π
0= Erfc
(cid:112)
−
(cid:112)
e 16+2πσ2|⟨w∗⟩|2, (43)
2 16+2πσ2|⟨w∗⟩|2 4 1+σ2|⟨w∗⟩|2π/8
whereErfc(z)=1−Erf(z)isthecomplementaryerrorfunction.
Wecanfactoroutthecommonexponentialasymptoticsofbothtermstoget
(cid:32) (cid:32) √ (cid:33)
−
|⟨w∗⟩|2π
+
|⟨w∗⟩|2π 1 |⟨w∗⟩| π
0=e 16+2πσ2|⟨w∗⟩|2 e 16+2πσ2|⟨w∗⟩|2 Erfc
(cid:112)
2 16+2πσ2|⟨w∗⟩|2
(44)
(cid:33)
1 σ2|⟨w∗⟩|
− .
(cid:112)
4 1+σ2|⟨w∗⟩|2π/8
Ofthetwotermsintheparentheses,thefirsttermstartsat 1 for|⟨w∗⟩| = 0andismonotonically
2
decreasingtothevalue 1e1/2σ2Erfc(√1 )≈ √1 (σ−σ3+O(σ5)),whilethesecondtermstartsat
2 2σ 2π
0andincreasestothelargervalue √σ . Thus,thereisauniquefixedpoint. ForRL,uniquenessof
2π
thefixedpointcanbeshownusingananalogousargument.
Thisfixedpoint⟨w∗⟩isstable,becauseforSL,(14)impliesthat d⟨w⟩ = −∇ ⟨L⟩ ,whereLis
dt w L
thecross-entropylossand⟨·⟩ istheaverageovertheinputdistribution. Thus,⟨L⟩ isaLyapunov
L L
functionforthedynamicalsystem. ForRL,thesameargumentholdsbythepolicy-gradienttheorem,
where⟨L⟩ additionallyincludesanaverageovertheoutputnoise.
L
Notethat,forλ=σ =0,although|⟨w⟩|diverges,acalculationof d log (⟨w⟩·µ)2 showsthatthe
dt |µ|2|⟨w⟩|2
alignment between µ and ⟨w⟩ still converges to 1. This result is consistent with those of Ji and
Telgarsky[2020].
MNISTdetails
A B
Figure7: ComparisonofthetheorywithtrainingonrawMNIST.A:Comparisonoftheempirical
testclassificationaccuracywiththetheoreticalprediction. B:JustlikefortheGabor-filteredinputs,
thetheoryaccuratelycapturesnon-trivialongoinglearningdynamics.
WhenapplyingourtheorytotheMNISTdataset,wecompareSGDappliedtoactualdata(orange
curves)withcalculatedSGDcurvesfromourtheory(bluecurves). Inthemaintext,wepreprocess
thedatabyconvolvingrawpixelvalueswithabankofGaborfilterstoapproximateamorerealistic
scenariowherethebinaryclassifierappearsattheendofaconvolutionalneuralnetwork. Forthe
plotsinFigure7,weuserawpixelvaluestodemonstratethatourevolutionequations(15)workin
generalsettingswithoutrelyingonGaborfilterrepresentation. Inbothcases,wegloballytranslate
allinputvectorssuchthatthedataset’smeaniszero. Wedirectlyevaluatetestsetaccuracy(Figures
5Band7A)andthecorrelationofwateachSGDstepwiththemeanµofdigit’1’(Figures5Cand
7B)).Tocalculatethetheoreticalaccuracycurve,wefirstnumericallysolvethedifferentialequations
(15)forthemeanµandcovariancesΣ obtainedfromtheempiricaldatasettoderivew(t). We
0,1
14thenintegratetwomultivariatenormaldistributionswiththeseµandΣvaluesinthehalf-spaces
boundedbyw(t)(expressibleasanerrorfunction)andplottheresultasthetheoreticalaccuracy
curveinFigures5Band7A.ThetSNEembeddinginFigure5Aisincludedforillustrativepurposes
onlyandisnotusedincalculations.
Experiments
The numerical code implementing the model and performing the analyses was mostly written in
JAX[Bradburyetal.,2018],aswellasWolframMathematicaandSciPy[Virtanenetal.,2020]. For
Fig.2,theflowfieldswereplottedforthelimitofzeroinputnoiseandaregularizationparameterof
λ=0.1. Thelearningcurvesareplottedforλ=0andΣ=σ2I,withσ =0.1andσ =1. Asinall
otherfiguresbesidesFig.5,weset|µ|=1. Eachexperimentwasrepeatedfor10runswithdifferent
randomseeds,withthestandarddeviationindicatedasa(barelyvisible)shadedregion. Thecurveof
|⟨w∗⟩|isplottedforthelimitofzeroinputnoise. ForFig.3,wenumericallyintegrated(10)with
λ=0. ForthetotalvarianceplottedinFig.4,wesetλ=0.1andintegratethedifferentialequations
(11)numerically. ForFig.5,theGabor-filteredinputswerecreatedusing[Haghighatetal.,2015]
withdefaultparameters. TheperceptronwastrainedusingSGDwithalogisticsigmoidoutput,and
theorangecurveinpanelBshowstestaccuracy. Thetrainingwasrepeated10timeswithshuffled
data. We set λ = 1. For the forgetting curves in Fig. 6, we set λ = 10 and the learning rate to
η =10−2. Thecurvesshownareaveragesover50differentrandominitializationseach. Theinput
dimensionissettoN =500. Forallothersimulations,thelearningratewassettoη =10−3. The
computationswereperformedonanNVIDIATitanXpGPU,withruntimesofatmostafewminutes.
15