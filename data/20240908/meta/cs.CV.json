[
    {
        "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
        "authors": "Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2409.03757v1",
        "entry_id": "http://arxiv.org/abs/2409.03757v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03757v1",
        "summary": "Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks.",
        "updated": "2024-09-05 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03757v1"
    },
    {
        "title": "DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation",
        "authors": "Wenliang ZhaoHaolin WangJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2409.03755v1",
        "entry_id": "http://arxiv.org/abs/2409.03755v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03755v1",
        "summary": "Diffusion probabilistic models (DPMs) have shown remarkable performance in\nvisual synthesis but are computationally expensive due to the need for multiple\nevaluations during the sampling. Recent predictor-corrector diffusion samplers\nhave significantly reduced the required number of function evaluations (NFE),\nbut inherently suffer from a misalignment issue caused by the extra corrector\nstep, especially with a large classifier-free guidance scale (CFG). In this\npaper, we introduce a new fast DPM sampler called DC-Solver, which leverages\ndynamic compensation (DC) to mitigate the misalignment of the\npredictor-corrector samplers. The dynamic compensation is controlled by\ncompensation ratios that are adaptive to the sampling steps and can be\noptimized on only 10 datapoints by pushing the sampling trajectory toward a\nground truth trajectory. We further propose a cascade polynomial regression\n(CPR) which can instantly predict the compensation ratios on unseen sampling\nconfigurations. Additionally, we find that the proposed dynamic compensation\ncan also serve as a plug-and-play module to boost the performance of\npredictor-only samplers. Extensive experiments on both unconditional sampling\nand conditional sampling demonstrate that our DC-Solver can consistently\nimprove the sampling quality over previous methods on different DPMs with a\nwide range of resolutions up to 1024$\\times$1024. Notably, we achieve 10.38 FID\n(NFE=5) on unconditional FFHQ and 0.394 MSE (NFE=5, CFG=7.5) on\nStable-Diffusion-2.1. Code is available at https://github.com/wl-zhao/DC-Solver",
        "updated": "2024-09-05 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03755v1"
    },
    {
        "title": "Foundation Model or Finetune? Evaluation of few-shot semantic segmentation for river pollution",
        "authors": "Marga DonStijn PinsonBlanca Guillen CebrianYuki M. Asano",
        "links": "http://arxiv.org/abs/2409.03754v1",
        "entry_id": "http://arxiv.org/abs/2409.03754v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03754v1",
        "summary": "Foundation models (FMs) are a popular topic of research in AI. Their ability\nto generalize to new tasks and datasets without retraining or needing an\nabundance of data makes them an appealing candidate for applications on\nspecialist datasets. In this work, we compare the performance of FMs to\nfinetuned pre-trained supervised models in the task of semantic segmentation on\nan entirely new dataset. We see that finetuned models consistently outperform\nthe FMs tested, even in cases were data is scarce. We release the code and\ndataset for this work on GitHub.",
        "updated": "2024-09-05 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03754v1"
    },
    {
        "title": "ArtiFade: Learning to Generate High-quality Subject from Blemished Images",
        "authors": "Shuya YangShaozhe HaoYukang CaoKwan-Yee K. Wong",
        "links": "http://arxiv.org/abs/2409.03745v1",
        "entry_id": "http://arxiv.org/abs/2409.03745v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03745v1",
        "summary": "Subject-driven text-to-image generation has witnessed remarkable advancements\nin its ability to learn and capture characteristics of a subject using only a\nlimited number of images. However, existing methods commonly rely on\nhigh-quality images for training and may struggle to generate reasonable images\nwhen the input images are blemished by artifacts. This is primarily attributed\nto the inadequate capability of current techniques in distinguishing\nsubject-related features from disruptive artifacts. In this paper, we introduce\nArtiFade to tackle this issue and successfully generate high-quality\nartifact-free images from blemished datasets. Specifically, ArtiFade exploits\nfine-tuning of a pre-trained text-to-image model, aiming to remove artifacts.\nThe elimination of artifacts is achieved by utilizing a specialized dataset\nthat encompasses both unblemished images and their corresponding blemished\ncounterparts during fine-tuning. ArtiFade also ensures the preservation of the\noriginal generative capabilities inherent within the diffusion model, thereby\nenhancing the overall performance of subject-driven methods in generating\nhigh-quality and artifact-free images. We further devise evaluation benchmarks\ntailored for this task. Through extensive qualitative and quantitative\nexperiments, we demonstrate the generalizability of ArtiFade in effective\nartifact removal under both in-distribution and out-of-distribution scenarios.",
        "updated": "2024-09-05 17:57:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03745v1"
    },
    {
        "title": "Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation",
        "authors": "Slava ElizarovCiara RowlesSimon Donné",
        "links": "http://arxiv.org/abs/2409.03718v1",
        "entry_id": "http://arxiv.org/abs/2409.03718v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03718v1",
        "summary": "Generating high-quality 3D objects from textual descriptions remains a\nchallenging problem due to computational cost, the scarcity of 3D data, and\ncomplex 3D representations. We introduce Geometry Image Diffusion\n(GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to\nefficiently represent 3D shapes using 2D images, thereby avoiding the need for\ncomplex 3D-aware architectures. By integrating a Collaborative Control\nmechanism, we exploit the rich 2D priors of existing Text-to-Image models such\nas Stable Diffusion. This enables strong generalization even with limited 3D\ntraining data (allowing us to use only high-quality training data) as well as\nretaining compatibility with guidance techniques such as IPAdapter. In short,\nGIMDiffusion enables the generation of 3D assets at speeds comparable to\ncurrent Text-to-Image models. The generated objects consist of semantically\nmeaningful, separate parts and include internal structures, enhancing both\nusability and versatility.",
        "updated": "2024-09-05 17:21:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03718v1"
    }
]