[
    {
        "title": "Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron",
        "authors": "Christian SchmidJames M. Murray",
        "links": "http://arxiv.org/abs/2409.03749v1",
        "entry_id": "http://arxiv.org/abs/2409.03749v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03749v1",
        "summary": "The ability of a brain or a neural network to efficiently learn depends\ncrucially on both the task structure and the learning rule. Previous works have\nanalyzed the dynamical equations describing learning in the relatively\nsimplified context of the perceptron under assumptions of a student-teacher\nframework or a linearized output. While these assumptions have facilitated\ntheoretical understanding, they have precluded a detailed understanding of the\nroles of the nonlinearity and input-data distribution in determining the\nlearning dynamics, limiting the applicability of the theories to real\nbiological or artificial neural networks. Here, we use a stochastic-process\napproach to derive flow equations describing learning, applying this framework\nto the case of a nonlinear perceptron performing binary classification. We\ncharacterize the effects of the learning rule (supervised or reinforcement\nlearning, SL/RL) and input-data distribution on the perceptron's learning curve\nand the forgetting curve as subsequent tasks are learned. In particular, we\nfind that the input-data noise differently affects the learning speed under SL\nvs. RL, as well as determines how quickly learning of a task is overwritten by\nsubsequent learning. Additionally, we verify our approach with real data using\nthe MNIST dataset. This approach points a way toward analyzing learning\ndynamics for more-complex circuit architectures.",
        "updated": "2024-09-05 17:58:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03749v1"
    },
    {
        "title": "Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry",
        "authors": "Meena JagadeesanMichael I. JordanJacob Steinhardt",
        "links": "http://arxiv.org/abs/2409.03734v1",
        "entry_id": "http://arxiv.org/abs/2409.03734v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03734v1",
        "summary": "Emerging marketplaces for large language models and other large-scale machine\nlearning (ML) models appear to exhibit market concentration, which has raised\nconcerns about whether there are insurmountable barriers to entry in such\nmarkets. In this work, we study this issue from both an economic and an\nalgorithmic point of view, focusing on a phenomenon that reduces barriers to\nentry. Specifically, an incumbent company risks reputational damage unless its\nmodel is sufficiently aligned with safety objectives, whereas a new company can\nmore easily avoid reputational damage. To study this issue formally, we define\na multi-objective high-dimensional regression framework that captures\nreputational damage, and we characterize the number of data points that a new\ncompany needs to enter the market. Our results demonstrate how multi-objective\nconsiderations can fundamentally reduce barriers to entry -- the required\nnumber of data points can be significantly smaller than the incumbent company's\ndataset size. En route to proving these results, we develop scaling laws for\nhigh-dimensional linear regression in multi-objective environments, showing\nthat the scaling rate becomes slower when the dataset size is large, which\ncould be of independent interest.",
        "updated": "2024-09-05 17:45:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03734v1"
    },
    {
        "title": "Iterative thresholding for non-linear learning in the strong $\\varepsilon$-contamination model",
        "authors": "Arvind RathnashyamAlex Gittens",
        "links": "http://arxiv.org/abs/2409.03703v1",
        "entry_id": "http://arxiv.org/abs/2409.03703v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03703v1",
        "summary": "We derive approximation bounds for learning single neuron models using\nthresholded gradient descent when both the labels and the covariates are\npossibly corrupted adversarially. We assume the data follows the model $y =\n\\sigma(\\mathbf{w}^{*} \\cdot \\mathbf{x}) + \\xi,$ where $\\sigma$ is a nonlinear\nactivation function, the noise $\\xi$ is Gaussian, and the covariate vector\n$\\mathbf{x}$ is sampled from a sub-Gaussian distribution. We study sigmoidal,\nleaky-ReLU, and ReLU activation functions and derive a\n$O(\\nu\\sqrt{\\epsilon\\log(1/\\epsilon)})$ approximation bound in $\\ell_{2}$-norm,\nwith sample complexity $O(d/\\epsilon)$ and failure probability\n$e^{-\\Omega(d)}$.\n  We also study the linear regression problem, where $\\sigma(\\mathbf{x}) =\n\\mathbf{x}$. We derive a $O(\\nu\\epsilon\\log(1/\\epsilon))$ approximation bound,\nimproving upon the previous $O(\\nu)$ approximation bounds for the\ngradient-descent based iterative thresholding algorithms of Bhatia et al.\n(NeurIPS 2015) and Shen and Sanghavi (ICML 2019). Our algorithm has a\n$O(\\textrm{polylog}(N,d)\\log(R/\\epsilon))$ runtime complexity when\n$\\|\\mathbf{w}^{*}\\|_2 \\leq R$, improving upon the\n$O(\\text{polylog}(N,d)/\\epsilon^2)$ runtime complexity of Awasthi et al.\n(NeurIPS 2022).",
        "updated": "2024-09-05 16:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03703v1"
    },
    {
        "title": "A method to benchmark high-dimensional process drift detection",
        "authors": "Edgar WolfTobias Windisch",
        "links": "http://arxiv.org/abs/2409.03669v1",
        "entry_id": "http://arxiv.org/abs/2409.03669v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03669v1",
        "summary": "Process curves are multi-variate finite time series data coming from\nmanufacturing processes. This paper studies machine learning methods for drifts\nof process curves. A theoretic framework to synthetically generate process\ncurves in a controlled way is introduced in order to benchmark machine learning\nalgorithms for process drift detection. A evaluation score, called the temporal\narea under the curve, is introduced, which allows to quantify how well machine\nlearning models unveil curves belonging to drift segments. Finally, a benchmark\nstudy comparing popular machine learning approaches on synthetic data generated\nwith the introduced framework shown.",
        "updated": "2024-09-05 16:23:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03669v1"
    },
    {
        "title": "DART2: a robust multiple testing method to smartly leverage helpful or misleading ancillary information",
        "authors": "Xuechan LiJichun Xie",
        "links": "http://arxiv.org/abs/2409.03618v1",
        "entry_id": "http://arxiv.org/abs/2409.03618v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03618v1",
        "summary": "In many applications of multiple testing, ancillary information is available,\nreflecting the hypothesis null or alternative status. Several methods have been\ndeveloped to leverage this ancillary information to enhance testing power,\ntypically requiring the ancillary information is helpful enough to ensure\nfavorable performance. In this paper, we develop a robust and effective\ndistance-assisted multiple testing procedure named DART2, designed to be\npowerful and robust regardless of the quality of ancillary information. When\nthe ancillary information is helpful, DART2 can asymptotically control FDR\nwhile improving power; otherwise, DART2 can still control FDR and maintain\npower at least as high as ignoring the ancillary information. We demonstrated\nDART2's superior performance compared to existing methods through numerical\nstudies under various settings. In addition, DART2 has been applied to a gene\nassociation study where we have shown its superior accuracy and robustness\nunder two different types of ancillary information.",
        "updated": "2024-09-05 15:22:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03618v1"
    }
]