[
    {
        "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
        "authors": "Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2409.03757v1",
        "entry_id": "http://arxiv.org/abs/2409.03757v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03757v1",
        "summary": "Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks.",
        "updated": "2024-09-05 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03757v1"
    },
    {
        "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
        "authors": "Yuntian DengWenting ZhaoJack HesselXiang RenClaire CardieYejin Choi",
        "links": "http://arxiv.org/abs/2409.03753v1",
        "entry_id": "http://arxiv.org/abs/2409.03753v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03753v1",
        "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis's utility through three\ncase studies: facilitating chatbot misuse research, visualizing and comparing\ntopic distributions across datasets, and characterizing user-specific\nconversation patterns. WildVis is open-source and designed to be extendable,\nsupporting additional datasets and customized search and visualization\nfunctionalities.",
        "updated": "2024-09-05 17:59:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03753v1"
    },
    {
        "title": "Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron",
        "authors": "Christian SchmidJames M. Murray",
        "links": "http://arxiv.org/abs/2409.03749v1",
        "entry_id": "http://arxiv.org/abs/2409.03749v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03749v1",
        "summary": "The ability of a brain or a neural network to efficiently learn depends\ncrucially on both the task structure and the learning rule. Previous works have\nanalyzed the dynamical equations describing learning in the relatively\nsimplified context of the perceptron under assumptions of a student-teacher\nframework or a linearized output. While these assumptions have facilitated\ntheoretical understanding, they have precluded a detailed understanding of the\nroles of the nonlinearity and input-data distribution in determining the\nlearning dynamics, limiting the applicability of the theories to real\nbiological or artificial neural networks. Here, we use a stochastic-process\napproach to derive flow equations describing learning, applying this framework\nto the case of a nonlinear perceptron performing binary classification. We\ncharacterize the effects of the learning rule (supervised or reinforcement\nlearning, SL/RL) and input-data distribution on the perceptron's learning curve\nand the forgetting curve as subsequent tasks are learned. In particular, we\nfind that the input-data noise differently affects the learning speed under SL\nvs. RL, as well as determines how quickly learning of a task is overwritten by\nsubsequent learning. Additionally, we verify our approach with real data using\nthe MNIST dataset. This approach points a way toward analyzing learning\ndynamics for more-complex circuit architectures.",
        "updated": "2024-09-05 17:58:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03749v1"
    },
    {
        "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
        "authors": "Rui WenMichael BackesYang Zhang",
        "links": "http://arxiv.org/abs/2409.03741v1",
        "entry_id": "http://arxiv.org/abs/2409.03741v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03741v1",
        "summary": "Machine learning has revolutionized numerous domains, playing a crucial role\nin driving advancements and enabling data-centric processes. The significance\nof data in training models and shaping their performance cannot be overstated.\nRecent research has highlighted the heterogeneous impact of individual data\nsamples, particularly the presence of valuable data that significantly\ncontributes to the utility and effectiveness of machine learning models.\nHowever, a critical question remains unanswered: are these valuable data\nsamples more vulnerable to machine learning attacks? In this work, we\ninvestigate the relationship between data importance and machine learning\nattacks by analyzing five distinct attack types. Our findings reveal notable\ninsights. For example, we observe that high importance data samples exhibit\nincreased vulnerability in certain attacks, such as membership inference and\nmodel stealing. By analyzing the linkage between membership inference\nvulnerability and data importance, we demonstrate that sample characteristics\ncan be integrated into membership metrics by introducing sample-specific\ncriteria, therefore enhancing the membership inference performance. These\nfindings emphasize the urgent need for innovative defense mechanisms that\nstrike a balance between maximizing utility and safeguarding valuable data\nagainst potential exploitation.",
        "updated": "2024-09-05 17:54:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03741v1"
    },
    {
        "title": "Differentiable Discrete Event Simulation for Queuing Network Control",
        "authors": "Ethan CheJing DongHongseok Namkoong",
        "links": "http://arxiv.org/abs/2409.03740v1",
        "entry_id": "http://arxiv.org/abs/2409.03740v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03740v1",
        "summary": "Queuing network control is essential for managing congestion in\njob-processing systems such as service systems, communication networks, and\nmanufacturing processes. Despite growing interest in applying reinforcement\nlearning (RL) techniques, queueing network control poses distinct challenges,\nincluding high stochasticity, large state and action spaces, and lack of\nstability. To tackle these challenges, we propose a scalable framework for\npolicy optimization based on differentiable discrete event simulation. Our main\ninsight is that by implementing a well-designed smoothing technique for\ndiscrete event dynamics, we can compute pathwise policy gradients for\nlarge-scale queueing networks using auto-differentiation software (e.g.,\nTensorflow, PyTorch) and GPU parallelization. Through extensive empirical\nexperiments, we observe that our policy gradient estimators are several orders\nof magnitude more accurate than typical REINFORCE-based estimators. In\naddition, We propose a new policy architecture, which drastically improves\nstability while maintaining the flexibility of neural-network policies. In a\nwide variety of scheduling and admission control tasks, we demonstrate that\ntraining control policies with pathwise gradients leads to a 50-1000x\nimprovement in sample efficiency over state-of-the-art RL methods. Unlike prior\ntailored approaches to queueing, our methods can flexibly handle realistic\nscenarios, including systems operating in non-stationary environments and those\nwith non-exponential interarrival/service times.",
        "updated": "2024-09-05 17:53:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03740v1"
    }
]