[
    {
        "title": "High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media",
        "authors": "Yuya SasakiJing TaoYulong Wang",
        "links": "http://arxiv.org/abs/2403.01318v1",
        "entry_id": "http://arxiv.org/abs/2403.01318v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01318v1",
        "summary": "Motivated by the empirical power law of the distributions of credits (e.g.,\nthe number of \"likes\") of viral posts in social media, we introduce the\nhigh-dimensional tail index regression and methods of estimation and inference\nfor its parameters. We propose a regularized estimator, establish its\nconsistency, and derive its convergence rate. To conduct inference, we propose\nto debias the regularized estimate, and establish the asymptotic normality of\nthe debiased estimator. Simulation studies support our theory. These methods\nare applied to text analyses of viral posts in X (formerly Twitter) concerning\nLGBTQ+.",
        "updated": "2024-03-02 21:37:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01318v1"
    },
    {
        "title": "Near-optimal Per-Action Regret Bounds for Sleeping Bandits",
        "authors": "Quan NguyenNishant A. Mehta",
        "links": "http://arxiv.org/abs/2403.01315v1",
        "entry_id": "http://arxiv.org/abs/2403.01315v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01315v1",
        "summary": "We derive near-optimal per-action regret bounds for sleeping bandits, in\nwhich both the sets of available arms and their losses in every round are\nchosen by an adversary. In a setting with $K$ total arms and at most $A$\navailable arms in each round over $T$ rounds, the best known upper bound is\n$O(K\\sqrt{TA\\ln{K}})$, obtained indirectly via minimizing internal sleeping\nregrets. Compared to the minimax $\\Omega(\\sqrt{TA})$ lower bound, this upper\nbound contains an extra multiplicative factor of $K\\ln{K}$. We address this gap\nby directly minimizing the per-action regret using generalized versions of\nEXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal\nbounds of order $O(\\sqrt{TA\\ln{K}})$ and $O(\\sqrt{T\\sqrt{AK}})$. We extend our\nresults to the setting of bandits with advice from sleeping experts,\ngeneralizing EXP4 along the way. This leads to new proofs for a number of\nexisting adaptive and tracking regret bounds for standard non-sleeping bandits.\nExtending our results to the bandit version of experts that report their\nconfidences leads to new bounds for the confidence regret that depends\nprimarily on the sum of experts' confidences. We prove a lower bound, showing\nthat for any minimax optimal algorithms, there exists an action whose regret is\nsublinear in $T$ but linear in the number of its active rounds.",
        "updated": "2024-03-02 21:22:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01315v1"
    },
    {
        "title": "Can a Confident Prior Replace a Cold Posterior?",
        "authors": "Martin MarekBrooks PaigePavel Izmailov",
        "links": "http://arxiv.org/abs/2403.01272v1",
        "entry_id": "http://arxiv.org/abs/2403.01272v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01272v1",
        "summary": "Benchmark datasets used for image classification tend to have very low levels\nof label noise. When Bayesian neural networks are trained on these datasets,\nthey often underfit, misrepresenting the aleatoric uncertainty of the data. A\ncommon solution is to cool the posterior, which improves fit to the training\ndata but is challenging to interpret from a Bayesian perspective. We explore\nwhether posterior tempering can be replaced by a confidence-inducing prior\ndistribution. First, we introduce a \"DirClip\" prior that is practical to sample\nand nearly matches the performance of a cold posterior. Second, we introduce a\n\"confidence prior\" that directly approximates a cold likelihood in the limit of\ndecreasing temperature but cannot be easily sampled. Lastly, we provide several\ngeneral insights into confidence-inducing priors, such as when they might\ndiverge and how fine-tuning can mitigate numerical instability.",
        "updated": "2024-03-02 17:28:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01272v1"
    },
    {
        "title": "Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise",
        "authors": "Halyun JeongDeanna NeedellElizaveta Rebrova",
        "links": "http://arxiv.org/abs/2403.01204v1",
        "entry_id": "http://arxiv.org/abs/2403.01204v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01204v1",
        "summary": "We propose SGD-exp, a stochastic gradient descent approach for linear and\nReLU regressions under Massart noise (adversarial semi-random corruption model)\nfor the fully streaming setting. We show novel nearly linear convergence\nguarantees of SGD-exp to the true parameter with up to $50\\%$ Massart\ncorruption rate, and with any corruption rate in the case of symmetric\noblivious corruptions. This is the first convergence guarantee result for\nrobust ReLU regression in the streaming setting, and it shows the improved\nconvergence rate over previous robust methods for $L_1$ linear regression due\nto a choice of an exponentially decaying step size, known for its efficiency in\npractice. Our analysis is based on the drift analysis of a discrete stochastic\nprocess, which could also be interesting on its own.",
        "updated": "2024-03-02 12:45:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01204v1"
    },
    {
        "title": "A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features",
        "authors": "Emi ZegerYifei WangAaron MishkinTolga ErgenEmmanuel CandèsMert Pilanci",
        "links": "http://arxiv.org/abs/2403.01046v1",
        "entry_id": "http://arxiv.org/abs/2403.01046v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01046v1",
        "summary": "We prove that training neural networks on 1-D data is equivalent to solving a\nconvex Lasso problem with a fixed, explicitly defined dictionary matrix of\nfeatures. The specific dictionary depends on the activation and depth. We\nconsider 2-layer networks with piecewise linear activations, deep narrow ReLU\nnetworks with up to 4 layers, and rectangular and tree networks with sign\nactivation and arbitrary depth. Interestingly in ReLU networks, a fourth layer\ncreates features that represent reflections of training data about themselves.\nThe Lasso representation sheds insight to globally optimal networks and the\nsolution landscape.",
        "updated": "2024-03-02 00:33:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01046v1"
    }
]