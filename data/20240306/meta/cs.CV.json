[
    {
        "title": "Enhancing Retinal Vascular Structure Segmentation in Images With a Novel Design Two-Path Interactive Fusion Module Model",
        "authors": "Rui YangShunpu Zhang",
        "links": "http://arxiv.org/abs/2403.01362v1",
        "entry_id": "http://arxiv.org/abs/2403.01362v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01362v1",
        "summary": "Precision in identifying and differentiating micro and macro blood vessels in\nthe retina is crucial for the diagnosis of retinal diseases, although it poses\na significant challenge. Current autoencoding-based segmentation approaches\nencounter limitations as they are constrained by the encoder and undergo a\nreduction in resolution during the encoding stage. The inability to recover\nlost information in the decoding phase further impedes these approaches.\nConsequently, their capacity to extract the retinal microvascular structure is\nrestricted. To address this issue, we introduce Swin-Res-Net, a specialized\nmodule designed to enhance the precision of retinal vessel segmentation.\nSwin-Res-Net utilizes the Swin transformer which uses shifted windows with\ndisplacement for partitioning, to reduce network complexity and accelerate\nmodel convergence. Additionally, the model incorporates interactive fusion with\na functional module in the Res2Net architecture. The Res2Net leverages\nmulti-scale techniques to enlarge the receptive field of the convolutional\nkernel, enabling the extraction of additional semantic information from the\nimage. This combination creates a new module that enhances the localization and\nseparation of micro vessels in the retina. To improve the efficiency of\nprocessing vascular information, we've added a module to eliminate redundant\ninformation between the encoding and decoding steps.\n  Our proposed architecture produces outstanding results, either meeting or\nsurpassing those of other published models. The AUC reflects significant\nenhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise\nsegmentation of retinal vessels across three widely utilized datasets:\nCHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms\nalternative architectures, demonstrating superior performance in both IOU and\nF1 measure metrics.",
        "updated": "2024-03-03 01:36:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01362v1"
    },
    {
        "title": "ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation",
        "authors": "Siyuan BianJiefeng LiJiasheng TangCewu Lu",
        "links": "http://arxiv.org/abs/2403.01345v1",
        "entry_id": "http://arxiv.org/abs/2403.01345v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01345v1",
        "summary": "Accurate human shape recovery from a monocular RGB image is a challenging\ntask because humans come in different shapes and sizes and wear different\nclothes. In this paper, we propose ShapeBoost, a new human shape recovery\nframework that achieves pixel-level alignment even for rare body shapes and\nhigh accuracy for people wearing different types of clothes. Unlike previous\napproaches that rely on the use of PCA-based shape coefficients, we adopt a new\nhuman shape parameterization that decomposes the human shape into bone lengths\nand the mean width of each part slice. This part-based parameterization\ntechnique achieves a balance between flexibility and validity using a\nsemi-analytical shape reconstruction algorithm. Based on this new\nparameterization, a clothing-preserving data augmentation module is proposed to\ngenerate realistic images with diverse body shapes and accurate annotations.\nExperimental results show that our method outperforms other state-of-the-art\nmethods in diverse body shape situations as well as in varied clothing\nsituations.",
        "updated": "2024-03-02 23:40:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01345v1"
    },
    {
        "title": "Mitigating the Bias in the Model for Continual Test-Time Adaptation",
        "authors": "Inseop ChungKyomin HwangJayeon YooNojun Kwak",
        "links": "http://arxiv.org/abs/2403.01344v1",
        "entry_id": "http://arxiv.org/abs/2403.01344v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01344v1",
        "summary": "Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt\na source pre-trained model to continually changing target domains. In the CTA\nsetting, a model does not know when the target domain changes, thus facing a\ndrastic change in the distribution of streaming inputs during the test-time.\nThe key challenge is to keep adapting the model to the continually changing\ntarget domains in an online manner. We find that a model shows highly biased\npredictions as it constantly adapts to the chaining distribution of the target\ndata. It predicts certain classes more often than other classes, making\ninaccurate over-confident predictions. This paper mitigates this issue to\nimprove performance in the CTA scenario. To alleviate the bias issue, we make\nclass-wise exponential moving average target prototypes with reliable target\nsamples and exploit them to cluster the target features class-wisely. Moreover,\nwe aim to align the target distributions to the source distribution by\nanchoring the target feature to its corresponding source prototype. With\nextensive experiments, our proposed method achieves noteworthy performance gain\nwhen applied on top of existing CTA methods without substantial adaptation time\noverhead.",
        "updated": "2024-03-02 23:37:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01344v1"
    },
    {
        "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
        "authors": "Neta ShaulUriel SingerRicky T. Q. ChenMatthew LeAli ThabetAlbert PumarolaYaron Lipman",
        "links": "http://arxiv.org/abs/2403.01329v1",
        "entry_id": "http://arxiv.org/abs/2403.01329v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01329v1",
        "summary": "This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver\ndistillation approach to improve sample efficiency of Diffusion and Flow\nmodels. BNS solvers are based on a family of non-stationary solvers that\nprovably subsumes existing numerical ODE solvers and consequently demonstrate\nconsiderable improvement in sample approximation (PSNR) over these baselines.\nCompared to model distillation, BNS solvers benefit from a tiny parameter space\n($<$200 parameters), fast optimization (two orders of magnitude faster),\nmaintain diversity of samples, and in contrast to previous solver distillation\napproaches nearly close the gap from standard distillation methods such as\nProgressive Distillation in the low-medium NFE regime. For example, BNS solver\nachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We\nexperimented with BNS solvers for conditional image generation, text-to-image\ngeneration, and text-2-audio generation showing significant improvement in\nsample approximation (PSNR) in all.",
        "updated": "2024-03-02 22:27:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01329v1"
    },
    {
        "title": "DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions",
        "authors": "Guangrun WangChanglin LiLiuchun YuanJiefeng PengXiaoyu XianXiaodan LiangXiaojun ChangLiang Lin",
        "links": "http://arxiv.org/abs/2403.01326v1",
        "entry_id": "http://arxiv.org/abs/2403.01326v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01326v1",
        "summary": "Neural Architecture Search (NAS), aiming at automatically designing neural\narchitectures by machines, has been considered a key step toward automatic\nmachine learning. One notable NAS branch is the weight-sharing NAS, which\nsignificantly improves search efficiency and allows NAS algorithms to run on\nordinary computers. Despite receiving high expectations, this category of\nmethods suffers from low search effectiveness. By employing a generalization\nboundedness tool, we demonstrate that the devil behind this drawback is the\nuntrustworthy architecture rating with the oversized search space of the\npossible architectures. Addressing this problem, we modularize a large search\nspace into blocks with small search spaces and develop a family of models with\nthe distilling neural architecture (DNA) techniques. These proposed models,\nnamely a DNA family, are capable of resolving multiple dilemmas of the\nweight-sharing NAS, such as scalability, efficiency, and multi-modal\ncompatibility. Our proposed DNA models can rate all architecture candidates, as\nopposed to previous works that can only access a subsearch space using\nheuristic algorithms. Moreover, under a certain computational complexity\nconstraint, our method can seek architectures with different depths and widths.\nExtensive experimental evaluations show that our models achieve\nstate-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile\nconvolutional network and a small vision transformer, respectively.\nAdditionally, we provide in-depth empirical analysis and insights into neural\narchitecture ratings. Codes available: \\url{https://github.com/changlin31/DNA}.",
        "updated": "2024-03-02 22:16:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01326v1"
    }
]