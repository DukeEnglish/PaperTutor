[
    {
        "title": "LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems",
        "authors": "Tasnim AhmedSalimur Choudhury",
        "links": "http://arxiv.org/abs/2403.01342v1",
        "entry_id": "http://arxiv.org/abs/2403.01342v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01342v1",
        "summary": "In the rapidly evolving field of natural language processing, the translation\nof linguistic descriptions into mathematical formulation of optimization\nproblems presents a formidable challenge, demanding intricate understanding and\nprocessing capabilities from Large Language Models (LLMs). This study compares\nprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and\none-shot settings for this task. Our findings show GPT-4's superior\nperformance, particularly in the one-shot scenario. A central part of this\nresearch is the introduction of `LM4OPT,' a progressive fine-tuning framework\nfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.\nHowever, this research highlights a notable gap in the contextual understanding\ncapabilities of smaller models such as Llama-2-7b compared to larger\ncounterparts, especially in processing lengthy and complex input contexts. Our\nempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4\nsurpasses the baseline performance established by previous research, achieving\nan F1-score of 0.63, solely based on the problem description in natural\nlanguage, and without relying on any additional named entity information.\nGPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These\nfindings not only benchmark the current capabilities of LLMs in a novel\napplication area but also lay the groundwork for future improvements in\nmathematical formulation of optimization problems from natural language input.",
        "updated": "2024-03-02 23:32:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01342v1"
    },
    {
        "title": "VNLP: Turkish NLP Package",
        "authors": "Meliksah TurkerMehmet Erdi AriAydin Han",
        "links": "http://arxiv.org/abs/2403.01309v1",
        "entry_id": "http://arxiv.org/abs/2403.01309v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01309v1",
        "summary": "In this work, we present VNLP: the first dedicated, complete, open-source,\nwell-documented, lightweight, production-ready, state-of-the-art Natural\nLanguage Processing (NLP) package for the Turkish language. It contains a wide\nvariety of tools, ranging from the simplest tasks, such as sentence splitting\nand text normalization, to the more advanced ones, such as text and token\nclassification models. Its token classification models are based on \"Context\nModel\", a novel architecture that is both an encoder and an auto-regressive\nmodel. NLP tasks solved by VNLP models include but are not limited to Sentiment\nAnalysis, Named Entity Recognition, Morphological Analysis \\& Disambiguation\nand Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings\nand corresponding SentencePiece Unigram tokenizers. VNLP has an open-source\nGitHub repository, ReadtheDocs documentation, PyPi package for convenient\ninstallation, Python and command-line API and a demo page to test all the\nfunctionality. Consequently, our main contribution is a complete, compact,\neasy-to-install and easy-to-use NLP package for Turkish.",
        "updated": "2024-03-02 20:46:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01309v1"
    },
    {
        "title": "VBART: The Turkish LLM",
        "authors": "Meliksah TurkerMehmet Erdi AriAydin Han",
        "links": "http://arxiv.org/abs/2403.01308v1",
        "entry_id": "http://arxiv.org/abs/2403.01308v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01308v1",
        "summary": "We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is 7x more\nefficient than OpenAI's multilingual tokenizer. Last but not least, we\nintroduce a method to enlarge an existing pre-trained LLM and question the\nrelevancy of Chinchilla Scaling Law to sequence-to-sequence masked language\nmodels. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.",
        "updated": "2024-03-02 20:40:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01308v1"
    },
    {
        "title": "Improving the Validity of Automatically Generated Feedback via Reinforcement Learning",
        "authors": "Alexander ScarlatosDigory SmithSimon WoodheadAndrew Lan",
        "links": "http://arxiv.org/abs/2403.01304v1",
        "entry_id": "http://arxiv.org/abs/2403.01304v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01304v1",
        "summary": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.",
        "updated": "2024-03-02 20:25:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01304v1"
    },
    {
        "title": "Greed is All You Need: An Evaluation of Tokenizer Inference Methods",
        "authors": "Omri UzanCraig W. SchmidtChris TannerYuval Pinter",
        "links": "http://arxiv.org/abs/2403.01289v1",
        "entry_id": "http://arxiv.org/abs/2403.01289v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01289v1",
        "summary": "While subword tokenizers such as BPE and WordPiece are typically used to\nbuild vocabularies for NLP models, the method of decoding text into a sequence\nof tokens from these vocabularies is often left unspecified, or ill-suited to\nthe method in which they were constructed. We provide a controlled analysis of\nseven tokenizer inference methods across four different algorithms and three\nvocabulary sizes, performed on a novel intrinsic evaluation suite we curated\nfor English, combining measures rooted in morphology, cognition, and\ninformation theory. We show that for the most commonly used tokenizers, greedy\ninference performs surprisingly well; and that SaGe, a recently-introduced\ncontextually-informed tokenizer, outperforms all others on morphological\nalignment.",
        "updated": "2024-03-02 19:01:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01289v1"
    }
]