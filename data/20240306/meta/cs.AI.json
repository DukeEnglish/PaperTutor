[
    {
        "title": "SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization",
        "authors": "Danish GufranSaideep TikuSudeep Pasricha",
        "links": "http://dx.doi.org/10.1109/LES.2023.3279017",
        "entry_id": "http://arxiv.org/abs/2403.01348v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01348v1",
        "summary": "Indoor localization is a critical task in many embedded applications, such as\nasset tracking, emergency response, and realtime navigation. In this article,\nwe propose a novel fingerprintingbased framework for indoor localization called\nSANGRIA that uses stacked autoencoder neural networks with gradient boosted\ntrees. Our approach is designed to overcome the device heterogeneity challenge\nthat can create uncertainty in wireless signal measurements across embedded\ndevices used for localization. We compare SANGRIA to several state-of-the-art\nframeworks and demonstrate 42.96% lower average localization error across\ndiverse indoor locales and heterogeneous devices.",
        "updated": "2024-03-03 00:01:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01348v1"
    },
    {
        "title": "Chaining thoughts and LLMs to learn DNA structural biophysics",
        "authors": "Tyler D. RossAshwin Gopinath",
        "links": "http://arxiv.org/abs/2403.01332v1",
        "entry_id": "http://arxiv.org/abs/2403.01332v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01332v1",
        "summary": "The future development of an AI scientist, a tool that is capable of\nintegrating a variety of experimental data and generating testable hypotheses,\nholds immense potential. So far, bespoke machine learning models have been\ncreated to specialize in singular scientific tasks, but otherwise lack the\nflexibility of a general purpose model. Here, we show that a general purpose\nlarge language model, chatGPT 3.5-turbo, can be fine-tuned to learn the\nstructural biophysics of DNA. We find that both fine-tuning models to return\nchain-of-thought responses and chaining together models fine-tuned for subtasks\nhave an enhanced ability to analyze and design DNA sequences and their\nstructures.",
        "updated": "2024-03-02 22:38:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01332v1"
    },
    {
        "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
        "authors": "Neta ShaulUriel SingerRicky T. Q. ChenMatthew LeAli ThabetAlbert PumarolaYaron Lipman",
        "links": "http://arxiv.org/abs/2403.01329v1",
        "entry_id": "http://arxiv.org/abs/2403.01329v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01329v1",
        "summary": "This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver\ndistillation approach to improve sample efficiency of Diffusion and Flow\nmodels. BNS solvers are based on a family of non-stationary solvers that\nprovably subsumes existing numerical ODE solvers and consequently demonstrate\nconsiderable improvement in sample approximation (PSNR) over these baselines.\nCompared to model distillation, BNS solvers benefit from a tiny parameter space\n($<$200 parameters), fast optimization (two orders of magnitude faster),\nmaintain diversity of samples, and in contrast to previous solver distillation\napproaches nearly close the gap from standard distillation methods such as\nProgressive Distillation in the low-medium NFE regime. For example, BNS solver\nachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We\nexperimented with BNS solvers for conditional image generation, text-to-image\ngeneration, and text-2-audio generation showing significant improvement in\nsample approximation (PSNR) in all.",
        "updated": "2024-03-02 22:27:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01329v1"
    },
    {
        "title": "VNLP: Turkish NLP Package",
        "authors": "Meliksah TurkerMehmet Erdi AriAydin Han",
        "links": "http://arxiv.org/abs/2403.01309v1",
        "entry_id": "http://arxiv.org/abs/2403.01309v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01309v1",
        "summary": "In this work, we present VNLP: the first dedicated, complete, open-source,\nwell-documented, lightweight, production-ready, state-of-the-art Natural\nLanguage Processing (NLP) package for the Turkish language. It contains a wide\nvariety of tools, ranging from the simplest tasks, such as sentence splitting\nand text normalization, to the more advanced ones, such as text and token\nclassification models. Its token classification models are based on \"Context\nModel\", a novel architecture that is both an encoder and an auto-regressive\nmodel. NLP tasks solved by VNLP models include but are not limited to Sentiment\nAnalysis, Named Entity Recognition, Morphological Analysis \\& Disambiguation\nand Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings\nand corresponding SentencePiece Unigram tokenizers. VNLP has an open-source\nGitHub repository, ReadtheDocs documentation, PyPi package for convenient\ninstallation, Python and command-line API and a demo page to test all the\nfunctionality. Consequently, our main contribution is a complete, compact,\neasy-to-install and easy-to-use NLP package for Turkish.",
        "updated": "2024-03-02 20:46:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01309v1"
    },
    {
        "title": "VBART: The Turkish LLM",
        "authors": "Meliksah TurkerMehmet Erdi AriAydin Han",
        "links": "http://arxiv.org/abs/2403.01308v1",
        "entry_id": "http://arxiv.org/abs/2403.01308v1",
        "pdf_url": "http://arxiv.org/pdf/2403.01308v1",
        "summary": "We present VBART, the first Turkish sequence-to-sequence Large Language\nModels (LLMs) pre-trained on a large corpus from scratch. VBART are compact\nLLMs based on good ideas leveraged from BART and mBART models and come in two\nsizes, Large and XLarge. Fine-tuned VBART models surpass the prior\nstate-of-the-art results in abstractive text summarization, title generation,\ntext paraphrasing, question answering and question generation tasks. They allow\nfine-tuning for future text generation tasks and datasets, carving a new path\nfor Turkish Natural Language Processing (NLP) research. Our work shows that\nhaving a pre-trained LLM for Turkish outperforms up to 3x multilingual models,\nimproving existing results and providing efficient models for training and\ninference. Moreover, we show that our monolingual tokenizer is 7x more\nefficient than OpenAI's multilingual tokenizer. Last but not least, we\nintroduce a method to enlarge an existing pre-trained LLM and question the\nrelevancy of Chinchilla Scaling Law to sequence-to-sequence masked language\nmodels. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are\npublicly available at huggingface.co/vngrs-ai.",
        "updated": "2024-03-02 20:40:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.01308v1"
    }
]