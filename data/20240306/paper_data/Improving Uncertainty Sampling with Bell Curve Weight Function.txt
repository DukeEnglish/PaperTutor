International Journal of Applied Physics and Mathematics
Improving Uncertainty Sampling with Bell Curve Weight
Function
Zan-Kai Chong1*, Hiroyuki Ohsaki2, Bok-Min Goi3
1 Independent Researcher, Malaysia.
2 School of Science and Technology, Kwansei Gakuin University, Japan.
3 Lee Kong Chian Faculty of Engineering Science, Universiti Tunku Abdul Rahman, Malaysia.
* Corresponding author. Email: zankai@ieee.org
Manuscript submitted January 20, 2022; revised February 18, 2023; accepted March 13, 2023.
doi: 10.17706/ijapm.2023.13.4.44-52
Abstract: Typically, a supervised learning model is trained using passive learning by randomly selecting
unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases
where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify
spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during
initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active
learning method that improves the efficiency of supervised learning by using fewer labelled instances than
passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where
the predicted probabilities, 𝑝, fall into the uncertainty region, i.e., 𝑝 ≈0.5. The newly acquired labels are
then added to the existing labelled data pool to learn a new model. Nonetheless, the performance of
uncertainty sampling is susceptible to the Area of Unpredictable Responses (AUR) and the nature of the
dataset. It is difficult to determine whether to use passive learning or uncertainty sampling without prior
knowledge of a new dataset. To address this issue, we propose bell curve sampling, which employs a bell
curve weight function to acquire new labels. With the bell curve centred at 𝑝 =0.5, bell curve sampling
selects instances whose predicted values are in the uncertainty area most of the time without neglecting the
rest. Simulation results show that, most of the time bell curve sampling outperforms uncertainty sampling
and passive learning in datasets of different natures and with AUR.
Key words: Active learning, uncertainty sampling, random sampling
1. Introduction
Supervised learning is a sub-field of machine learning, and it trains a model to learn the relationship
between input and output sets through labelled instances [1]. With the quick expanding of digital era,
supervised learning has been widely applied in many practical applications that amass large datasets such
as spam detection, face detection, voice recognition, etc. [2].
In some cases, it may take little effort to acquire a large number of unlabelled instances, but labelling
them turns out to be laborious and costly. For example, credit scoring models for microfinance businesses
are normally trained with loan repayment behaviour datasets and it may take a few months to conclude a
borrower as a bad payer. In particular, a finance institute may receive thousands of loan applications
(unlabelled instances) regularly, but it takes substantial risk to approve loans [3, 4] and a lengthy
observation to identify bad payers (labelled instances).
44 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
Cohn and Atlas et al. [5] have long been aware of the potential of active learning in addressing expensive
acquisition cost of labelled samples in passive learning. The process to select instances to label is named as
“query” in active learning and it enables a model to learn with a lesser number of labelled instances
efficiently as compared to passive learning. We consider a popular pool-based active learning, namely
uncertainty sampling [6, 7] in this paper. Given a data set of balanced dichotomous responses, uncertainty
sampling queries the labels of instances in the unlabelled data pool, in which their predicted probabilities
fall into the uncertainty region ( 𝑝 ≈0.5) [8].
Uncertainty sampling has gained decent attention in the research community [6, 9] with some reported
negative results [10–12]. They explain the results are due to the sampling bias causing the disparity in
between the known feature distribution and the actual one [13]. Additionally, the inaccurate initial model
also drives subsequent the improper instances selection that further drifting the intended sampling
strategy [10].
In this paper, we assume the presence of an Area of Unpredictable Responses (AUR), i.e., a subset of
dataset, where all predictions here yield random responses grievously due to unavoidable noises or weak
predictive features as shown in the overlapped area of Fig. 1. The simulation results in Section 4 show that
the performance of uncertainty sampling is precarious in datasets of different nature and AUR. It is
indeterminate to determine the best sampling method without the prior knowledge about a new dataset.
Fig. 1. The circles on the left and right contain instances of labels “0” and “1”, respectively. The overlapped
area in the middle is filled with instances of indeterminate responses.
We adduce the idea of heuristic optimization technique [14] that both intensification and diversification
are necessary to search for a solution. Accordingly, we propose a sampling that applies a bell curve weight
acquisition function, where instances are selected from the uncertainty region most of the time without
neglecting the instances from other regions. Comparing with uncertainty sampling and passive learning,
simulation results reported that bell curve learning aces the performance in the datasets of diversified AUR
in general.
This paper is organised as following: We discuss the algorithm of uncertainty sampling with bare
mathematical notations in Section 2. Section 3 presents the proposed bell curve sampling and its
assumptions. Next, the simulation setup and performance of passive learning, uncertainty sampling and bell
curve sampling are presented in Section 4. With that, we draw the conclusion in Section 5.
2. Uncertainty Sampling
Assume that we have a large finite set of tuples 𝐷(all) ={(𝑥, 𝑦):𝑥 ∈ℜ𝑑, 𝑦 ∈{0,1}} that describes the
population of our interest with 𝑥 and 𝑦 represent 𝑑-dimensions feature vectors and dichotomous responses
respectively. A relatively small and large numbers of tuples are taken from 𝐷(all) and they are denoted as
known data pool and unknown data pool, i.e., 𝐷(known) ={(𝑥,𝑦)} and 𝐷(unknown) ={(𝑥,∅)}, where ∅
45 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
symbolises the masked responses, and |𝐷(unknown)|>|𝐷(known)|. In practical, we do not have test data to
learn the model performance. However, in the simulation a test data pool, 𝐷(test) ={(𝑥,𝑦)} from 𝐷(all) will
be given to evaluate the impact of the learning algorithms. For convenience, we use subscripts, e.g.,
𝐷(⋅)
to
𝑞
denote the corresponding data pool at 𝑞-th query.
Referring an efficient machine learning algorithm as MLA, at the beginning of the query, we have a prior
model that is trained from
𝐷(known)
, i.e.,
𝐷(known)M
→
L A
model . The rest of steps go as follows.
1 1 1
• Step 1. With model , we make inference to all the instances in current
𝐷(unknown)
to yield the
1 1
predicted outputs 𝑦̃ , and the corresponding predicted probabilities 𝑝̃ , i.e.,
1 1
𝐷(unknown)m → o d e l1 𝐷̃ (unknown) and 𝐷̃ (unknown) ={(𝑥 ,∅,𝑦̃,𝑝̃)}.
1 1 1 𝑖 1 1
• Step 2. Uncertainty sampling selects 𝑛 instances from 𝐷̃ (unknown) , in which the values of
1
corresponding 𝑝̃ in the uncertainty region. With ∅ be revealed during the annotation, 𝐷(query) =
1
{(𝑥,𝑦):∅→𝑦}, 𝐷 ⊂𝐷̃ (unknown) , |𝐷(query) |=𝑛 and the selected instances will be removed from
1 1
𝐷(unknown)
i.e.,
𝐷(unknown)
∖
𝐷(query) →𝐷(unknown)
.
1 1 1 1
• Step 3. We merge the new labelled instances into the known data pool, i.e.,
𝐷(query) ∪𝐷(known)
→
1 1
𝐷(known)
.
1
• Step 4. Renaming the indices
𝐷(unknown)
to
𝐷(unknown)
and
𝐷(known)
to
𝐷(known)
, we train a new model
1 2 1 2
with
𝐷(known)M
→
L A
model . Then, the next query will be repeated from Step 1 with all the indices to
2 2
be renamed correctly. The process is ended when the stopping criteria are met.
3. Bell Curve Sampling
This section elaborates the assumptions and the principle of bell curve sampling.
Assumptions
We assume datasets with balanced dichotomous responses (i.e., about the same amount of 0's and 1's in
responses) with the presence of AUR. An observer has access to the feature vectors of each instance in an
unknown data pool, but it has no knowledge about the true responses.
Weight-Probabilities Distribution
Fig. 2 (a) and (b) illustrate the weight-probabilities distributions for passive learning and uncertainty
sampling to select the instances to annotate. The former selects instances randomly, i.e., all instances have
equal weight to be selected regardless of their predicted probabilities. Meanwhile, the latter only selects the
instances near to 𝑝 ≈0.5. As mentioned in Section 1, it is precarious to determine the best sampling
method without the prior knowledge about the data set. With that, we propose bell curve sampling and it
employs a bell curve weight distribution to marry the strength of both passive learning and uncertain
sampling in coping datasets of diversified nature and AUR. As the name suggests, bell curve sampling
contains a bell-curve-like weight distribution with peak at 𝑝 =0.5 as shown in Fig. 2 (c). Instances near to
𝑝 =0.5 will be chosen frequently without neglecting the instances from other regions. Such selection
strategy follows the idea of intensification and diversification approaches in heuristic search.
To derive the bell curve weight-probabilities distribution, let Beta(𝑝,𝛼,𝛽) be the probability density
function of beta distribution for 0≤𝑝 ≤1 , the shape parameters 𝛼,𝛽 >0 and 𝛼,𝛽,𝑝 ∈ℜ. As demonstrated
in Fig. 3, having 𝛼 =𝛽 moves the centre of bell curve to 𝑝 =0.5, and higher values of 𝛼 and 𝛽 transform the
bell curve to be steeper with shorter width. For example, with 𝛼 =𝛽 =5, 95% of area are covered in the
probabilities range of (0.3920, 0.6080) as shown in Table 1. Meanwhile, the probabilities range shrinks to
(0.4241, 0.5759) for 𝛼 =𝛽 =10. In general, the performance of bell curve sampling is identical to passive
learning with 𝛼 =𝛽 =1 as the weights are distributed uniformly. Moreover, it acts as the uncertainty
46 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
sampling with a high value of 𝛼 and 𝛽 due to shorter probabilities range. We look for the reasonable values
of 𝛼 and 𝛽 that synergize both passive learning and uncertainty sampling in Section 4.
(c) Bell curve sampling
(a) Passive learning (b) Uncertainty sampling
Fig. 2. Weight-probabilities distributions of various sampling methods.
Fig. 3. Beta distribution of various pair of 𝛼 =𝛽.
Table 1. Lower and Upper Bound of 95% Area under Curve for Various Set of 𝛼 and 𝛽 in Beta Distribution
𝜶 𝜷 Lower Upper
2 2 0.3264 0.6736
5 5 0.3920 0.6080
10 10 0.4241 0.5759
20 20 0.4465 0.5535
50 50 0.4662 0.5338
100 100 0.4761 0.5239
4. Simulation
We study the performance of bell curve sampling with simulation. This section illustrates the simulation
setup, the artificial datasets and the simulation results.
Simulation Setup
We generate four types of artificial datasets, i.e., classification, blobs, circles and moons in various degrees
of AUR using Scikit-learn [15]. Examples of the corresponding pairwise bivariate distributions are shown in
Fig. 4. Classification and blobs datasets consist of four feature columns, whereas circles and moons datasets
47 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
have two feature columns only. All datasets employ balanced dichotomous responses.
Generally, a sufficient large population of data set will first be generated and further segregated into three
chunks randomly, i.e., known data pool, 𝐷 (known) of 10 instances, an unknown data pool, 𝐷 (unknown) of 1000
instances and a test data pool, 𝐷 (test) of 1000 instances. Note that 𝐷 (test) will be used to assess the model
performance in the last step of each query.
We follow the query process that is described in Section 2. We change the sampling function in Step 2 to
passive learning (random sampling) and bell curve sampling according to the simulation objective. Each
query will select 𝑛 =5 instances from 𝐷 (unknown) to annotate (unmask the true responses) and a
simulation will make a total of 20 queries, and 20×𝑛 =100 or 10% of the instances from 𝐷 (unknown) will
be added to 𝐷 (known) eventually. All the models are built with AutoML [16] using optimal parameters. No
further feature engineering is conducted on the datasets.
(a) Classification dataset (b) Blobs dataset
(c) Circles dataset (d) Moons dataset
Fig. 4. Pairwise bivariate distributions of Scikit-learn's artificial datasets.
Results
Figs. 5–8 depict the performance of passive learning, uncertainty sampling and bell curve sampling in the
classification, blobs, circles and moons datasets using 𝛼 =𝛽 =10. We generated datasets with varying
levels of AUR, namely (a) low, (b) median, and (c) high, by adjusting their respective parameters. For
48 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
example, increasing the class separation factor facilitates the separation of responses in classification
datasets.
As discussed in Section 1, our observations indicate that the performance of uncertainty sampling is
dependent on the nature of the datasets and their AUR. For instance, uncertainty sampling demonstrated
superior performance over passive learning in the blobs datasets with median and high AUR (Fig. 6 (b) and
(c)), while it exhibited similar performance to passive learning in the low AUR case (Fig. 6 (a)). Conversely,
passive learning outperformed uncertainty sampling in the circles and moons datasets (Figs. 7 and 8).
Nonetheless, bell curve sampling generally exhibited better performance across most datasets. This
approach showcased competitive performance as demonstrated in the best learner cases, such as Figs. 7(c)
and 8(c), where passive learning was the top learner, followed by bell curve sampling and uncertainty
sampling.
(a) Low AUR (b) Median AUR (c) High AUR
Fig. 5. Performance of various learning methods in classification datasets of class separation factors (a) 2.0,
(b) 0.8 and (c) 0.3.
(a) Low AUR (b) Median AUR (c) High AUR
Fig. 6. Performance of various learning functions in blobs datasets of standard deviation (a) 1, (b) 3 and (c)
5.
(a) Low AUR (b) Median AUR (c) High AUR
Fig. 7. Performance of various learning functions in circles datasets of circle factors (a) 0.5, (b) 0.8 and
(c) 0.9.
49 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
Fig. 9 illustrates the impact of the parameters 𝛼 and 𝛽 on the performance of bell curve sampling. As
depicted in Fig. 3, increasing the values of 𝛼 and 𝛽 will result in a steeper bell curve with a narrower width,
implying that more weight will be given to instances closer to 𝑝̃ =0.5. In general, setting moderate to high
values of 𝛼 and 𝛽 can enhance the model's performance, while extremely high values will cause bell curve
sampling to behave like uncertainty sampling.
(a) Low AUR (b) Median AUR (c) High AUR
Fig. 8. Performance of various learning functions in moons datasets of Gaussian noise's standard deviation
(a) 0.1, (b) 0.2 and (c) 0.3.
(a) Classification dataset (b) Blobs dataset
(c) Circles dataset (d) Moons dataset
Fig. 9. The effect of the parameters 𝛼 and 𝛽 to the performance of bell curve sampling.
5. Conclusion
Passive learning in typical machine learning is generally inefficient when the cost of acquiring labelled
and unlabelled instances is different. To address this issue, active learning, especially uncertainty sampling,
has been proposed. In principle, passive learning selects instances uniformly, while uncertainty sampling
50 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
selects instances from the uncertainty region. Both sampling methods outperform each other in datasets of
varying nature and AUR. Since selecting the best sampling method without prior knowledge about the
datasets is difficult, we propose the use of bell curve sampling, which employs a bell curve weight function
in selecting instances. Simulation results show that bell curve sampling performs better than passive
learning and uncertainty learning for most datasets with diversified AUR values.
Conflict of Interest
The authors declare no conflict of interest.
Author Contributions
Zan-Kai Chong and Hiroyuki Ohsaki co-authored the research and the paper. Bok-Min Goi provided
valuable input to enhance the paper. All authors had approved the final version.
References
[1] Hastie, T., Tibshirani, R., Friedman, J., H., & Friedman, J., H. (2009). The Elements of Statistical Learning:
Data Mining, Inference, and Prediction (2nd ed.).
[2] Borges., A. F. S., Laurindo., F., J. B., Spín ola, M. M., Gonçalves, R. F., & Mattos, C. A. (2021). The strategic
use of artificial intelligence in the digital era: Systematic literature review and future research
directions. International Journal of Information Management, 57, 102225.
[3] Milana, C., & Ashta, A. (2020). Microfinance and financial inclusion: Challenges and opportunities.
Strategic Change, 29(3), 257–266.
[4] Morduch, J., & Armendariz, B. (2005). The Economics of Microfinance. MIT Press.
[5] Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization with active learning. Machine
Learning, 15(2), 201–221.
[6] Settles, B. (2009). Active Learning Literature Survey. University of Wisconsin–Madison.
[7] Hino, H. (2020). Active learning: Problem Settings and recent developments. ArXiv Preprint, ArXiv:
2012.04225.
[8] Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling for supervised learning. Machine
Learning Proceedings, 148–156. https://doi.org/10.1016/B978-1-55860-335-6.50026-X
[9] Fu, Y., Zhu, X., & Li, B. (2013). A survey on instance selection for active learning. Knowledge and
Information Systems, 35(2), 249–283.
[10] Attenberg, J., & Provost, F. (2011). Inactive learning? Difficulties employing active learning in practice.
ACM SIGKDD Explorations Newsletter, 12(2), 36–41.
[11] Baldridge, J., & Osborne, M. (2004). Active learning and the total cost of annotation. Proceedings of the
2004 Conference on Empirical Methods in Natural Language Processing (pp. 9–16).
[12] Cawley, G. C. (2011). Baseline methods for active learning. Proceedings of the Active Learning and
Experimental Design Workshop (pp. 47–57).
[13] Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. (1992). Information, prediction, and query by
committee. Advances in Neural Information Processing Systems, 5.
[14] Blum, C., & Roli, A. (2003). Metaheuristics in combinatorial optimization: Overview and conceptual
comparison. ACM Computing Surveys (CSUR), 35(3), 268–308.
[15] Pedregosa, F., Varoquaux G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E.
(2011). Scikit-Learn: Machine learning in python. Journal of Machine Learning Research, 12, 2825–
2830.
51 Volume 13, Number 4, October 2023International Journal of Applied Physics and Mathematics
[16] Stetsenko, P. (2017). Machine Learning with Python and H O (5th ed.).
2
Copyright © 2023 by the authors. This is an open access article distributed under the Creative Commons
Attribution License which permits unrestricted use, distribution, and reproduction in any medium,
provided the original work is properly cited (CC BY 4.0).
52 Volume 13, Number 4, October 2023