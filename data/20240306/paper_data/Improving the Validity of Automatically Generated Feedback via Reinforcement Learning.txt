Improving the Validity of Automatically
Generated Feedback via Reinforcement Learning
Alexander Scarlatos1, Digory Smith2, Simon Woodhead2, and Andrew Lan1
1 University of Massachusetts Amherst
2 Eedi
Contact Emails: {ajscarlatos,andrewlan}@cs.umass.edu
Abstract. Automatically generating feedback via large language mod-
els (LLMs) in intelligent tutoringsystems and onlinelearning platforms
has the potential to improve the learning outcomes of many students.
However,bothfeedbackgenerationandevaluationarechallenging:feed-
back content has to be valid especially in subjects like math, which re-
quires models to understand the problem, the solution, and where the
student’serrorlies.Feedbackalsohastobepedagogicallyvalidtoreflect
effective tutoring strategies, such as explaining possible misconceptions
and encouraging the student, among other desirable features. In this
work, we address both problems of automatically generating and evalu-
ating feedback while considering both correctness and alignment. First,
weproposearubricforevaluatingmathfeedbackandshowthatGPT-4is
abletoeffectivelyuseittoannotatehuman-writtenandLLM-generated
feedback. Second, we propose a framework for feedback generation that
optimizes both correctness and alignment using reinforcement learning
(RL).Specifically,weuseGPT-4’sannotationstocreatepreferencesover
feedbackpairsinanaugmenteddatasetfortrainingviadirectpreference
optimization (DPO). We show that our methods significantly increase
the correctness and alignment of generated feedback with Llama 2, an
open-source LLM, qualitatively analyze our generation and evaluation
systems using case studies, and outline several areas for futurework.3
Keywords: FeedbackGeneration·HumanPreferenceAlignment·Math
Education · Reinforcement Learning
1 Introduction
Providing students with helpful feedback can be critical to their learning, al-
lowing them to quickly address and learn from mistakes. Prior work has shown
that delivering immediate automated feedback to students in intelligent tutor-
ingsystemsandonlinelearningplatformscanimprovelearningoutcomes[12,23].
However, doing so is challenging since generated feedback should satisfy a wide
variety of requirements: it should convey an understanding of the question and
why the student’s response is incorrect, as well as be aligned with educational
3 Ourcode is available at https://github.com/umass-ml4ed/feedback-gen-dpo.
4202
raM
2
]LC.sc[
1v40310.3042:viXra2 A. Scarlatos et al.
goals and pedagogical theory. For example, identifying student misconceptions,
providing hints, and using encouraging language to promote a growth mindset
[2,31] can be helpful, but simply giving away the answer could be detrimental.
Moreover,evaluatinggeneratedfeedback alongthese dimensions is alsodiffi-
cult. Automated evaluations must accountfor both feedback correctnessas well
astheiralignmentwitheducationalgoals,whichrequiresathoroughunderstand-
ingofboth.Additionally,evenwhenexpert-writtenfeedbackexamplesaregiven
asreference,textsimilarity-basedmetricsmaybeunreliablesincetherearemany
waysto write valid feedback,andtext overlapcanemphasize irrelevantfeatures
while neglecting more significant ones [1,19]. While it is common to use human
annotatorstoevaluatefeedback,thisapproachrequiressignificanteffortandex-
penses. Therefore, the lack of reliable, automated feedback evaluation methods
becomes a bottleneck for developing feedback generation methods.
In this work, we propose a framework that both generates and evaluates
feedback messagesfor incorrectstudentresponsesto questions,to improveboth
their correctness and alignment with educational goals. We ground our work in
math education but note that our framework could potentially be generalized
to other subjects, such as programming or language learning. First, we propose
a rubric for evaluating generated feedback and show that LLMs, particularly
GPT-4, achieve high agreement with humans in their evaluations.
Second, we use a reinforcement learning (RL)-based approach to generate
feedbackmessageswheretherewardgiventogeneratedfeedbackduringtraining
isbasedonthe evaluationrubric.Moreover,toavoidrepeatedlyusingGPT-4to
evaluate feedback during training, we use direct preference optimization (DPO)
[22], an offline RL algorithm, to align the generated feedback with educational
goals.ThisapproachissimilartoaligningLLMswithhuman[36]orAI[16]pref-
erences. We experiment on a dataset that consists of feedback messages written
by math teachers for each incorrectoption in multiple-choice questions. Our re-
sults show that feedback generated using our framework is significantly more
accurate and aligned with educational goals than baselines. Notably, on align-
ment metrics, we approach the performance of humans and GPT-4, estimated
to be a 1T parameter model, using the 7B parameter version of Llama 2.
2 Related Work
2.1 Feedback Generation
There are many existing approaches for automatic feedback generation. One
common method is to use engineered features to detect errors in student re-
sponses, and then use a rule-based system to provide relevant feedback or hints
[3,12,15,23,27,28]. This method is popular since it is interpretable and reliable
but requires significant human effort to adapt to new question types. A recent
andmoregeneralapproachtofeedbackgenerationisusinglargelanguagemodels
(LLMs), either through prompting [1,19,21,30,33] or fine-tuning [10]. However,
promptingpre-trainedLLMsrequirestheexistingmodelstohavegoodbehaviorImprovingthe Validity of Automatically Generated Feedback 3
and understanding of educational goals, but fine-tuning can yield poor results
without significantamountsofalignedtrainingdata.We addressthese concerns
in our work by fine-tuning on an augmented dataset annotated with alignment
labels.
2.2 Feedback Evaluation
Severalrecentworkshaveusedrubricstoevaluatefeedback[10,30,33],andworks
inotherdomainshavefoundsuccessinusing LLMsto evaluateopen-endedtext
where their judgements correlate with human judgements [6,13,20]. However,
most prior works on feedback generation tend to rely on human annotators for
reliable evaluation [1,9,10,30,33]. One recent work [11] uses GPT-4 to evaluate
mathfeedbackwitharubricandfinds highagreementwithhumanannotations.
However, they only use GPT-4 to evaluate human-written feedback, while we
evaluate feedback writtenby both humans andLLMs.Including this LLMfeed-
backhelpsusuncoverGPT-4’sshortcomingsinfeedbackevaluation,particularly
thatitcanstruggleto identifywhenfeedbackinaccuratelyaddressesstudenter-
rors or provides invalid suggestions.
3 Methodology
We now detail our framework for the two main tasks of feedback genera-
tion and evaluation. Specifically, we first detail our rubric for feedback evalu-
ation and how we collect annotations with GPT-4, followed by how we con-
struct an augmented dataset for training, and finally how we use DPO to fine-
tune an LLM for feedback generation. We first define some notations for our
tasks. Given a dataset D of N math questions, we define the i-th question as
(q(i),c(i),e(i),{d(i),f(i) |j ∈ {1,...,M}). Here, q(i) is the question text, c(i) is
j j
the correctanswerto the question,e(i) is atextualexplanationofthe question’s
solution,
d(i)
is an incorrect, student-generated answer to the question,
f(i)
is
j j
a textual feedback message to give to a student when their answer is
d(i)
, and
j
M is the number of different incorrect answers given for each question. When
discussing individual data points, we omit i and j for notation simplicity. We
assume that the feedback messages in the dataset are human-written, and refer
to these as the gold, ground-truth feedback.
3.1 Feedback Evaluation
We now detail our rubric for evaluating feedback given to students for their
incorrect answers. In addition to correctness, we aim to evaluate feedback mes-
sagesontheir alignmentwith educationalgoals,including those associatedwith
a growth mindset [2,31]. We take inspiration from prior works using rubrics for
feedback evaluation [10,30] and include aspects to target common errors that
LLMs make when generating feedback. Specifically, our rubric evaluates feed-
back on five different aspects, each of them resulting in a binary-valued label:4 A. Scarlatos et al.
– Correct(COR.)Thefeedbackdoesnotmakeanyincorrectstatementsand
is relevant to the current question and student answer.
– Revealing(REV.)Thefeedbackdoesnotdirectlyrevealthecorrectanswer
to the student.
– Suggestion(SUG.)Thefeedbackprovidessuggestionstothestudentthat,
when followed, will guide them towards the correct answer.
– Diagnositic(DIA.)Thefeedbackcorrectlypointsouttheerrorthestudent
made or the misconception underlying their answer.
– Positive (POS.) The feedback is positive and has an encouraging tone.
We now define the rubric function, which assigns labels to any feedback,
given a corresponding question and incorrect answer:
r(f|q,c,e,d)=(yC,yR,yS,yD,yP)=y∈{0,1}5
We also define a final scalar-valued rubric score for a feedback message by ag-
gregating the labels in y, indicating the feedback’s overallquality:
yC+yR+yS+yD+yP
s=yC· ∈[0,1]
5
where except for correctness, other rubric aspects are equally weighted. The
final rubric score is 0 if the feedback message is incorrect; otherwise, the score
increases by increments of 0.2 for every rubric aspect the feedback satisfies.
Whiletherubricfunctioncanbedefinedbytheoutputofhumanannotators,
the cost of evaluating feedback using humans is very high, especially when we
requirefrequentevaluationsuchasduringRLtraining.Toaddressthisissue,we
use GPT-4, known for its ability to generalize to new tasks, to define a version
of the rubric function, rGPT-4. Using zero-shotchain-of-thought prompting [14],
we ask GPT-4 yes or no questions related to each of the 5 labels, and use its
output to get an estimated label
y′
and corresponding score
s′
. During prompt
development, we observed that asking GPT-4 questions performed better than
assigning labels based on a formal rubric, that binary labels performed better
than a likert scale, and that asking the negation of the first two questions and
flipping the labels after improved accuracy. We leave further exploration of the
prompt settings, such as the use of in-context examples, for future work. We
provide an example prompt, output, and corresponding labels in Table 1.
3.2 Data Augmentation
We now detail our method for constructing an augmented dataset, which will
be used for RL training as well as calculating agreement between GPT-4 and
human annotations.For both of these tasks, we require both positive examples,
i.e., feedback messages that score highly on the rubric, and negative examples,
i.e., feedback that score poorly on the rubric. We observe that while most of
the gold feedback messages in D score highly, feedback generatedby prompting
LLMscanbeflawedorunalignedwiththeevaluationmetric,thusscoringpoorly.Improvingthe Validity of Automatically Generated Feedback 5
Table 1. Example prompt, output,and labels for GPT-4 feedback evaluation.
System You are a math education expert.
Prompt Yourjob is to evaluate feedback given to studentson math problems.
Hereis thequestion,thecorrect solution, theincorrect answer thestudent
gave, and the feedback given to thestudent:
Question: emma works 36 hours a week. write this as a fraction in its sim-
plest form. Correct Answer: 3 Solution: There are 24 × 7 = 168 hours in
14
a week, so the fraction is 36/168. This simplifies to 18/84 = 9/42 = 3/14.
Incorrect Answer: 18
84
Feedback:It looks like you havesimplified, but not fully.
For the following questions, provide a short explanation and then answer
with "Yes" or "No":
1. Does the feedback make any incorrect statements?
2. Does the feedback directly reveal theanswer tothe question?
3.Doesthefeedbackgivesuggestionstothestudentonhowtoimprovethe
answer?
4. Does the feedback correctly point out the misconception underlying the
student’sanswer?
5. Does the feedback havea positive or encouraging tone?
Output 1. Explanation: The feedback is correct in stating that the student has
simplified, but not fully. The student stopped at 18, which can be further
84
simplified to 3. So, the feedback does not make any incorrect statements.
14
Answer: No.
2. Explanation: The feedback does not provide the correct answer. It only
indicates that thestudent’sanswer is not fully simplified. Answer: No.
3. Explanation: The feedback does not providespecific suggestions on how
toimprovetheanswer.Itonlystatesthatthestudenthasnotfullysimplified
thefraction. Answer: No.
4.Explanation:Thefeedbackcorrectlyidentifiesthatthestudent’smiscon-
ception is not fully simplifying thefraction. Answer:Yes.
5.Explanation:Thefeedback isneutralintone.Itpointsouttheerrorbut
does not useany positive or encouraging language. Answer: No.
Result y C′ =1, y R′ =1, y S′ =0, y D′ =1, y P′ =0, s′=0.6
Therefore,we collect 3 LLM-augmentedversions ofD, where eachfeedback
f(i)
j
is replaced with a generated version: D , where feedback is generated using
R
few-shot prompting with random in-context examples, D , where feedback is
S
generated using few-shot prompting with the most similar examples, and D ,
Z
wherefeedbackisgeneratedusingzero-shotprompting.We referto the unionof
the original dataset and LLM-augmented datasets D′ = {D,D ,D ,D } as
R S Z
theaugmenteddataset.WeuseGPT-4toannotatethefeedSbackmessagesinthe
augmented dataset, and describe how we use these annotations for RL training
in the next section.6 A. Scarlatos et al.
3.3 Direct Preference Optimization
Inordertogeneratefeedbackthatscoreshighlyontherubric,weleveragedirect
preference optimization (DPO) [22], an offline RL algorithm,due to its simplic-
ity and efficiency. We note that online RL algorithms such as PPO could also
apply to our framework, although they would require training a reward model
and introduce additional technical challenges due to training instability issues;
we leave exploration of such algorithms for future work. At a high level, DPO
trains anLLM onpairs ofgeneratedoutputs giventhe same input, whereone is
preferred over the other. The goal is to use this preference information to make
theLLMgenerateoutputsthatmorecloselyresemblethepreferredoutputsseen
duringtraining.Inourcontext,the outputisthe feedbackmessage,f,while the
input includes the question and incorrect answer information, x = (q,c,e,d).
During training, we minimize the DPO objective, i.e.,
π (f |x) π (f |x)
m θin−E (x,fw,fl)∼D DPO(cid:20)logσ (cid:18)βlog πrθ ef(fw
w|x)
−βlog πrθ ef(fl l|x)(cid:19)(cid:21),
where f
w
is preferred over f
l
as the feedback for x, DDPO is a curated dataset
containing these feedback pairs and preferences, π is the trained LLM, i.e., a
θ
textgeneration“policy”,parameterizedbyθ,πref isafrozenreference LLM,and
β isahyperparametertocontrolhowfarπ
θ
candeviatefromπref.Wenowdetail
how we construct DDPO using both feedback from the augmented dataset and
mismatched feedback from the gold dataset.
We first leverage the augmented dataset to construct feedback preference
pairs. For each unique x ∈ D′ , we have 4 feedback messages, 1 human and 3
LLM-generated, from which we construct 4 = 6 unique pairs. We then use
2
the score
s′
for each feedback to determin(cid:0)e(cid:1)which feedback is preferred, and
excludepairsthathavethesamescore.Forinstance,consideracasewheresome
xhaspossiblefeedbackmessagesf ,f ,f ,andf ,withscores1.0,0.8,0.4,and
1 2 3 4
0.4,respectively.We thenproducethe preferencepairs(f ,f ), (f ,f ),(f ,f ),
1 2 1 3 1 4
(f ,f ), and (f ,f ), where the first feedback is the preferred one in each pair.
2 3 2 4
We also use mismatched feedback from the gold dataset to construct addi-
tional preference pairs. We observe that feedback written for different incorrect
answers to the same question will have many semantically similar features and
often the same variables and numbers. However, despite their similarities, the
feedback written for the corresponding incorrectanswer is almost always better
suited than feedback written for other incorrect answers. Therefore, these mis-
matched feedback are excellent hard negatives since it is hard for algorithms to
distinguish between them and good feedback; finding such hard negatives has
been shown to be the key to contrastive learning [25]. In addition to using mis-
matched feedback from the same question, we construct one more pair using a
feedback from a random question in the gold dataset. For instance, for
x(i)
and
1
M =3,weconstructthe
preferencepairs(f(i),f(i)),(f(i),f(i)),and(f(i),f(i′)),
1 2 1 3 1 j′
for some random i′ ∈[1,N] and j′ ∈[1,M], where f(i) is preferred in all pairs.
1Improvingthe Validity of Automatically Generated Feedback 7
4 Experiments
Wenowdetailallexperimentsweconducttovalidateourframeworkforfeedback
generationandevaluation.First,we demonstratethatour methods improvethe
correctness and alignment of generated feedback using both quantitative eval-
uation from GPT-4 and qualitative case studies. Second, we demonstrate that
GPT-4 hashighagreementwithhumanannotationsonourrubric,justifying its
use as an evaluator, and further investigate its shortcomings using case studies.
4.1 Dataset
We validate ourframeworkusing a datasetofmiddle school-levelmath multiple
choice questions from a math learning platform. The questions cover a variety
of number sense concepts including fractions, exponents, rounding, and many
others. All questions and feedback messages are written by real math teachers,
deployed to real students, and are generally high quality. There are a total of
1,956questionsinthedatasetandeachquestionhasatotalof3incorrectoptions
andagroundtruthhuman-writtenfeedbackforeach.We removequestionsthat
require images and ones with processing errors, resulting in 1,418 questions.
We divide these into a train/validation/test split of 850/284/284questions and
correspondingly 2,550/852/852incorrect answers and corresponding feedback.
4.2 Experimental Setting
Data Augmentation We use two LLMs to generate feedback for our augmented
dataset: code-davinci-002(Codex) [4] for D and D since it has strong few-
R S
shot prompting ability, and gpt-3.5-turbofor D since its zero-shot ability is
Z
muchbetterthancode-davinci-002.Weuse2in-contextexamplesforfew-shot
prompts, only select examples from the train set, and use the S-BERT model
all-distilroberta-v1[24]tomeasuresimilarityforD .Wepromptthemodels
S
with questions, correct answers and incorrect answers, but not full solutions to
make the task harder and increase the amount of incorrect feedback. To reduce
′
costs,werandomlyselectasubsetofD tobe annotatedbyGPT-4.Specifically,
wetake10,000,1,000and1,000samplesfromthe train,validationandtestsets,
respectively, and remove the remaining samples from the augmented dataset.
FeedbackGeneration Models Weprimarilyusetheinstruction-tunedLlama-27B
Chatmodel[32]fromHuggingFace[34]forfeedbackgeneration,loadedwith8-bit
quantization[7].Forbothsupervisedfine-tuning(SFT)andDPO,wetrainLoRA
adapters [8] on all weight matrices, setting r = 32, α= 16, and dropout=0.05.
We train using the AdamW optimizer with a learning rate of 3e-4 with warmup
for 10% of steps and an effective batch size of 64 using gradient accumulation.
We train for 3 epochs, which we find minimizes the loss on the validation set.
ForDPO,weinitializeθ totheSFTweights,usetheSFTmodelasthereference
model, and set β = 0.5. At inference time, we use greedy decoding and set the
maximum new tokens to 200.8 A. Scarlatos et al.
Table2.Quantitativeresultsoffeedbackgenerationacrossmethods.Ourbestmethod
outperforms all Llama 2 baselines in both correctness and alignment.
COR.REV.SUG.DIA.POS.ScoreROU.BER.
Human 0.91 0.98 0.67 0.82 0.41 0.73 1.00 1.00
GPT-4 0.95 0.96 0.99 0.93 1.00 0.94 0.19 0.57
Zero-shot 0.63 0.63 0.74 0.43 1.00 0.49 0.16 0.55
SFT 0.65 0.98 0.49 0.68 0.19 0.49 0.29 0.61
DPO (Score) 0.70 0.93 0.95 0.82 0.66 0.65 0.22 0.57
DPO (Score + Mismatch) 0.77 0.96 0.95 0.86 0.57 0.71 0.23 0.57
Metrics When evaluating feedback, we report the average of each rubric la-
bel in
y′
and the corresponding scores
s′
assigned by GPT-4. We note that
GPT-4 will very rarely fail to assign labels when feedback is unrelated to the
current question, in which case we automatically assign label values of 0. We
use a temperature of 0 and 300 maximum new tokens for GPT-4 decoding. We
additionally use two popular reference-based metrics where we use the human-
written feedback as a gold reference: ROUGE-L (ROU.) [17] which is based
ontextual overlap,andthe F1 ofthe BERTScore (BER.)[35]using the recom-
mendedmicrosoft/deberta-xlarge-mnlimodel,whichisbasedontoken-level
semantic similarity.
4.3 Feedback Generation
We nowshowthatwecanimproveboththe correctnessandalignmentofgener-
ated feedback using our framework.We primarily focus on using Llama 2 Chat,
an open-source LLM with 7B parameters,where we compare severalversionsof
the model: Zero-Shot, i.e., simply prompting the base LLM, SFT, i.e., fine-
tuning the base LLM onthe goldfeedback set, DPO (Score), i.e., training the
baseLLMwithDPOonlyontheaugmenteddataset,andDPO (Score +Mis-
match), i.e., training the base LLM with DPO on the augmented dataset and
mismatched feedback. We additionally compare with the gold, human-written
feedback in the dataset, as well as feedback generated by GPT-4. We use the
samepromptforallmethods,whereweinstructthemodeltogenerateshortand
helpful feedback and to follow a version of the evaluation rubric.
Quantitative Analysis Table 2 shows the average rubric labels and scores
assigned by GPT-4 on all feedback in the test set, as well as the ROUGE-L
andBERTScorevalues forreference.We see thatDPO (Score +Mismatch)sig-
nificantly improves the feedback scores compared to baselines (a 45% increase
compared to Zero-Shot and SFT), showing that our data augmentation and
training setup is highly effective at improving the quality of feedback gener-
ation with Llama 2. We additionally observe that including the mismatched
feedbackmessagessubstantially increasesthe correctnessofgeneratedfeedback,
confirmingtheireffectivenessashardnegativeexamples.Surprisingly,SFTdoesImprovingthe Validity of Automatically Generated Feedback 9
not outperform Zero-Shot on score, which shows that the standard fine-tuning
setup is not effective for the specific task of feedback generation. We can also
seethatROUGE-LandBERTScoreareunreliableestimatesoffeedbackquality
since they are highest on SFT, primarily because it copies the style of the gold
feedback the closest.
We also see that GPT-4, a much largermodel (rumoredto have1T parame-
ters), performs almost perfectly acrossall labels; DPO (Score + Mismatch) can
only match its performance on the revealing and suggestion metrics. However,
we note that these results may be inflated, since we also use GPT-4 in evalua-
tion and it is likely to believe that its own generations conform to the rubric.
Moreover,weobservethatitpreferstobeconservativeandprovideslessspecific
descriptions of student errors, which leads to high scores under our evaluation
metric; see below for a detailed example. Nevertheless, we emphasize that a
smaller, open-source model is easier for deployment and much cheaper in real-
world educational scenarios than a larger, proprietary model. Additionally, we
see that the gold, human-written feedback does not score perfectly on correct-
ness, and has a relatively low overall score due to the suggestion and positive
metrics;DPO (Score + Mismatch)achievesa similaroverallperformance.How-
ever, the primary reason for the lower human performance is that teachers did
not have our evaluation rubrics in mind when they wrote the feedback.
Wealsoperformedqualitativestudiestocomparetheoutputsofthedifferent
methods and find cases where they succeed or fail; the main findings are:
– Zero-Shot produces feedback with the right style but struggles to follow
instructions,particularlybynotidentifyingtheerrororrevealingthecorrect
answer, and is prone to hallucinations and numerical errors.
– SFT produces feedback that is generally short and blunt, and usually at-
tempts to identify the error although is often incorrect.
– DPO (Score) produces feedback that attempts to identify the error, adding
details andquestions to provide implicit suggestionsand increasepositivity.
It also produces incorrect outputs although less so than SFT.
– DPO (Score+Mismatch)is moreaccuratethanDPO (Score)inidentifying
the error.
– GPT-4 produces feedback with smooth and coherent language but tends to
avoidmistakesbynotclearlypointingouttheerror.Whenitdoesdetailthe
error, it is often incorrect, although less so than the other methods.
– Forallmethods,mostfailuresoccurbecausetheLLMcannotcorrectlyiden-
tify the errorin the incorrectanswer.This failure happens more often when
multiple reasoningsteps are requiredto arriveatthe student’s answer.This
observation suggests that although state-of-the-art LLMs claim to excel at
mathematical reasoning, they cannot conceive how students may make er-
rors[11,18,19].ItmaybenecessarytoprompttheLLMtoperformnecessary
calculations before writing the feedback [29] and use calculators or code ex-
ecution [5] when doing so.
To provide more specific examples of several of our findings, we show a rep-
resentative question and incorrect answer along with corresponding feedback10 A. Scarlatos et al.
Table 3. Feedback generated by all methodsfor one question and incorrect answer.
Input
Question: Erica and Rebecca share some marbles in the ratio 1 : 6. Erica has (cid:3)
4 8
marblesthanRebecca.Whichof thefollowing belongs inthegap?
CorrectAnswer:athirdasmany
IncorrectAnswer:twiceas many
Source Feedback
Human Notquite,thedenominator hasdoubledbutthefractions don’thavethat
relationship. Trysimplifying 6/8 andthencompareitto1/4 ✓
GPT-4 Good effort,butremembertosimplify theratiosfirst.Erica’s shareisnot
twice as much, but a fraction of Rebecca’s. Try to compare 1/4 to 3/4
again. Keepgoing! ✓
Zero-shot Great effort! You are on theright track. Sincethe ratio is 1/4: 3/4, Erica
has1/4, whichisathirdof3/4.TofindthenumberofmarblesEricahas,
you can multiply 3/4 by 3, which gives you 9. So, Erica has 9 marbles.
Keeppracticing, andyou’llgetitrightnexttime!✗
SFT Ithinkyouhavesimplified thesecond fraction incorrectly.✗
DPO(Score) I think you have simplified 6/8 to 1/2. If we simplify 6/8 to 3/4, how
many marbles does Erica have? How many does Rebecca have? What is
thedifferencebetweenthem?✗
DPO (ScoreI think you have found 8/4 = 2. But we need to simplify 6/8 to get the
+Mismatch) sameunitsas1/4. Then wecan seehowmanymoreorlessErica has.✓
generated by each method in Table 3. In this example, we see that the human-
written feedback directly points out the error in the incorrect answer that the
studentonlyconsideredthedenominators,andalsosuggestssimplifyingthefrac-
tion on the right. GPT-4 prefers to be vague to avoid mistakes by pointing out
thattheratioshouldbesimplifiedandgivesasuggestionbasedonthis,butdoes
not explicitly identify why the student’s answer was wrong. Zero-Shot does not
point out the student’s error and proceeds to answer the question, even though
it has been instructed to not reveal the answer; it then hallucinates an incor-
rect and irrelevant multiplication before finishing with a positive message. SFT
makes a very generic guess at what the error is and does not give a suggestion
for improvement.DPO (Score) guesses a more specific but incorrecterror,gives
a somewhat misleading suggestion, and uses questions to encourage the stu-
dent. Finally, DPO (Score + Mismatch) correctly identifies the error and gives
a helpful suggestion.
4.4 Feedback Evaluation
Since we use GPT-4 to quantitatively evaluate feedback, we need to verify that
GPT-4 can indeed label feedback accurately using our rubric.To do so, we ran-
domly sample 80 feedback messages from the augmented test set and manually
evaluatethemontherubric.Toreducebias,wedonotshowthehumanannotator
GPT-4’sannotationsortellthemwhether afeedback messageishuman-written
or LLM-generated. We measure the accuracy (Acc.), precision (Prec.), recallImprovingthe Validity of Automatically Generated Feedback 11
Table 4. GPT-4’s agreement with human annotations across all rubric labels.
LabelAcc.Prec.Rec. F1
COR. 0.78 0.78 0.85 0.81
REV. 0.96 1.00 0.96 0.98
SUG. 0.75 0.67 0.79 0.72
DIA. 0.69 0.58 0.85 0.69
POS. 0.82 0.78 0.37 0.50
Avg. 0.80 0.76 0.76 0.74
(Rec.), and F1 of GPT-4’s annotations with respect to human annotations on
this set, and report the results in Table 4.
We observe that GPT-4 generally agrees with our annotations, with an av-
erage accuracy of 80% across labels. We also compute the Pearson correlation
coefficientofthe finalrubricscoresbetweenGPT-4 andourannotations,result-
ing in 0.58, indicating moderate overall correlation. We note that ROUGE-L
and BERTScore have correlations of 0.49 and 0.50 with our rubric scores, re-
spectively. Not only are these numbers lower, but we note that they are biased
upward since the human-written feedback messages, which generally have high
rubric scores, automatically get ROUGE-L and BERTScore values of 1.
However,GPT-4stillstrugglesinafewkeyaspectsandweprovideexamples
of erroneous annotations in Table 5. Most importantly, GPT-4 tends to assume
that feedback is correct when it sounds convincing but incorrectly identifies
the student error or provides an invalid suggestion. These issues mostly occur
when calculations are required to verify the feedback. Additionally, GPT-4 can
sometimes confuse the roles of variables in the question, leading it to believe
that a valid feedback is incorrect. GPT-4 also has a high false positive rate on
the diagnostic label due to hallucinating statements that were not made in the
feedback.Wenotethatitmaybepossibletoresolvetheseissuesusingadditional
prompt engineering or tools such as self-reflection [26] and code execution to
evaluatemathexpressions[5,29].Finally,whilethesuggestionandpositivelabels
haverelativelylowagreementwithhumanannotations,wenotethattheselabels
can be very subjective, and that GPT-4’s judgement on these labels is more
reasonable than these accuracy numbers suggest.
5 Conclusions and Future Work
In this work, we proposed a framework for automated feedback generation and
evaluation via LLMs for students’ incorrect answers in math multiple-choice
questions.Ourframeworkaccountsforboththemathematicalcorrectnessofthe
feedback and its alignment with good pedagogical practices. We show that us-
ingadataaugmentationandpreferenceoptimizationapproach,wecangenerate
high-qualityfeedback using Llama2 7B,a smallandopen-sourceLLM.We also
showthatGPT-4canevaluatefeedbackratheraccuratelyusingarubricandthat
its annotations are helpful for training the feedback generation method. There12 A. Scarlatos et al.
Table 5. Incorrect GPT-4 rubric annotation examples with explanations.
Label Input Generated Explanation
COR. Question: a jacket is reduced in a sale by 20%. aThefeedbackdoesnotmake
(FP) customer has a loyalty card for a further 10% offany incorrect statements. It
the price. what is the total percentage reductioncorrectly identifies the stu-
in thepriceofthejacket? dent’smistakeofaddingthe
Correct Answer:28% discountstogetherinsteadof
IncorrectAnswer:8% multiplyingthem.✗
Feedback: I think you have added the two dis-
countstogether.Weneedtousemultiplicationto
findtheoveralldiscount.
COR. Question: i am thinking of a number. i divide itThefeedbackisincorrectbe-
(FN) by 0.5 and my answer is 10. what number am icause the student actually
thinkingof? multipliedby2,not0.5.The
Correct Answer:5 correct feedback should be
IncorrectAnswer:20 “Youmultipliedby2instead
Feedback:Youmultipliedby0.5 insteadof divid-ofdividingby0.5.” ✗
ingby0.5.
DIA. Question:which fraction isequivalent 3 Thefeedbackcorrectlyiden-
7
(FP) Correct Answer: 9 tifies that the student has
21
IncorrectAnswer: 9 not multiplied the denom-
7
Feedback: You have multiplied the top and bot-inator by the same num-
tomby3,butyouhavetomultiplyby3again toberasthenumerator,which
getthecorrect answer istheunderlyingmisconcep-
tion.However,thesuggested
correction isincorrect.✗
aremanyavenuesforfuturework.First,wecanapplyourframeworktootherRL
algorithms such as PPO, or non-RL approaches such as overgenerate-and-rank.
Second, we can evaluate our final feedback generation task via a large-scalehu-
man evaluation or classroomstudy, which would alleviate concerns on GPT-4’s
annotations being biased. Finally, we can test our framework’s generalizability
by applying it to domains other than math, such as programming or language
learning,orotherscenariossuchashintgenerationorstudent-instructorconver-
sations.
References
1. Al-Hossami, E., Bunescu, R., Teehan, R., Powell, L., Mahajan, K., Dorodchi, M.:
Socratic questioning of novice debuggers: A benchmark dataset and preliminary
evaluations. In: Proceedings of the 18th Workshop on Innovative Use of NLP for
BuildingEducationalApplications(BEA2023).pp.709–726.AssociationforCom-
putational Linguistics, Toronto, Canada (Jul 2023)
2. Boaler, J.: Ability and mathematics: The mindset revolution that is reshaping
education. The Forum55, 143–152 (2013)
3. Botelho,A.,Baral,S.,Erickson,J.A.,Benachamardi,P.,Heffernan,N.T.:Leverag-
ingnaturallanguageprocessingtosupportautomatedassessmentandfeedbackforImprovingthe Validity of Automatically Generated Feedback 13
student open responses in mathematics. Journal of Computer Assisted Learning
39(3), 823–840 (2023)
4. Chen,M., Others: Evaluating large language models trained on code (2021)
5. Chen, W., Ma, X., Wang, X., Cohen, W.W.: Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks. arXiv
preprintarXiv:2211.12588 (2022)
6. Chiang, C.H., Lee, H.y.: Can large language models be an alternative to human
evaluations? arXiv preprintarXiv:2305.01937 (2023)
7. Dettmers, T., Lewis, M., Belkada, Y., Zettlemoyer, L.: Llm.int8(): 8-bit matrix
multiplication for transformers at scale (2022)
8. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,
W.: Lora: Low-rank adaptation of large language models (2021)
9. Jia, Q., Cui, J., Xiao, Y., Liu, C., Rashid, P., Gehringer, E.F.: All-in-one:
Multi-task learning bert models for evaluating peer assessments. arXiv preprint
arXiv:2110.03895 (2021)
10. Jia, Q., Young, M., Xiao, Y., Cui, J., Liu, C., Rashid, P., Gehringer, E.: Insta-
reviewer: A data-driven approach for generating instant feedback on students’
project reports. InternationalEducational Data Mining Society (2022)
11. Kakarla,S.,Thomas,D.,Lin,J.,Gupta,S.,Koedinger,K.R.:Usinglargelanguage
models to assess tutors’ performance in reacting to students making math errors.
arXiv preprint arXiv:2401.03238 (2024)
12. Kochmar,E.,Vu,D.D.,Belfer,R.,Gupta,V.,Serban,I.V.,Pineau,J.:Automated
personalized feedback improves learning gains in an intelligent tutoring system.
In:ArtificialIntelligenceinEducation:21stInternationalConference,AIED2020,
Ifrane, Morocco, July 6–10, 2020, Proceedings, Part II 21. pp. 140–146. Springer
(2020)
13. Kocmi, T., Federmann, C.: Large language models are state-of-the-art evaluators
of translation quality.arXiv preprint arXiv:2302.14520 (2023)
14. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models
are zero-shot reasoners. Advances in neural information processing systems 35,
22199–22213 (2022)
15. Lan,A.S.,Vats,D.,Waters,A.E.,Baraniuk,R.G.:Mathematicallanguageprocess-
ing:Automaticgradingandfeedbackforopenresponsemathematicalquestions.In:
Proceedingsofthesecond(2015)ACMconferenceonlearning@scale.pp.167–176
(2015)
16. Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V.,
Rastogi, A.: Rlaif: Scaling reinforcement learning from human feedback with ai
feedback. arXiv preprint arXiv:2309.00267 (2023)
17. Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In: Text
Summarization Branches Out.pp.74–81. Association for Computational Linguis-
tics, Barcelona, Spain (Jul 2004)
18. Liu, N., Sonkar, S., Wang, Z., Woodhead, S., Baraniuk, R.G.: Novice learner and
expert tutor: Evaluating math reasoning abilities of large language models with
misconceptions. arXiv preprint arXiv:2310.02439 (2023)
19. McNichols,H.,Feng,W.,Lee,J.,Scarlatos,A.,Smith,D.,Woodhead,S.,Lan,A.:
Automateddistractor andfeedback generation for math multiple-choicequestions
via in-context learning. NeurIPS’23 Workshop on Generative AI for Education
(2023)
20. Naismith,B.,Mulcaire,P.,Burstein,J.:Automatedevaluationofwrittendiscourse
coherenceusingGPT-4.In:Proceedingsofthe18thWorkshoponInnovativeUseof14 A. Scarlatos et al.
NLPforBuildingEducationalApplications(BEA2023).pp.394–403. Association
for Computational Linguistics, Toronto, Canada (Jul 2023)
21. Nguyen, H.A., Stec, H., Hou, X., Di, S., McLaren, B.M.: Evaluating chatgpt’s
decimal skills and feedback generation in a digital learning game. In: Responsive
and Sustainable Educational Futures. pp. 278–293. Springer Nature Switzerland,
Cham (2023)
22. Rafailov,R.,Sharma,A.,Mitchell,E.,Ermon,S.,Manning,C.D.,Finn,C.:Direct
preference optimization: Yourlanguage model is secretly a reward model (2023)
23. Razzaq,R., Ostrow, K.S.,Heffernan,N.T.: Effect of immediate feedback on math
achievement at the high school level. In: International Conference on Artificial
Intelligence in Education. pp.263–267. Springer(2020)
24. Reimers,N.,Gurevych,I.:Sentence-bert:Sentenceembeddingsusingsiamesebert-
networks.In:Proceedingsofthe2019ConferenceonEmpiricalMethodsinNatural
Language Processing. Association for Computational Linguistics (11 2019)
25. Robinson,J.D.,Chuang,C.Y., Sra,S.,Jegelka, S.:Contrastivelearningwithhard
negativesamples.In:InternationalConferenceonLearningRepresentations(2021)
26. Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K., Yao, S.:
Reflexion: Language agents with verbal reinforcement learning. arXiv preprint
arXiv:2303.11366 14 (2023)
27. Singh, R., Gulwani, S., Solar-Lezama, A.: Automated feedback generation for in-
troductoryprogrammingassignments.In:Proceedingsofthe34thACMSIGPLAN
conferenceonProgramminglanguagedesignandimplementation.pp.15–26(2013)
28. Song, D., Lee, W., Oh, H.: Context-aware and data-driven feedback generation
forprogrammingassignments. In:Proceedings ofthe29th ACMJoint Meetingon
European Software Engineering Conference and Symposium on the Foundations
of Software Engineering. pp.328–340 (2021)
29. Sonkar, S., Le, M., Chen, X., Liu, N., Mallick, D.B., Baraniuk, R.G.: Code
soliloquies for accurate calculations in large language models. arXiv preprint
arXiv:2309.12161 (2023)
30. Steiss,J., Tate,T., Graham,S.,Cruz,J., Hebert,M., Wang,J.,Moon, Y.,Tseng,
W., et al.: Comparing the quality of human and chatgpt feedback on students’
writing (2023)
31. Sun, K.L.: Brief report: The role of mathematics teaching in fostering student
growth mindset. Journal for Research in Mathematics Education 49(3), 330–335
(2018)
32. Touvron,H.,Others:Llama2:Openfoundationandfine-tunedchatmodels(2023)
33. Wang, R.E., Zhang, Q., Robinson, C., Loeb, S., Demszky, D.: Step-by-stepreme-
diation of students’mathematical mistakes
34. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s transformers: State-of-
the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019)
35. Zhang,T.,Kishore,V.,Wu,F.,Weinberger,K.Q.,Artzi,Y.:Bertscore:Evaluating
text generation with bert. In: International Conference on Learning Representa-
tions (2020)
36. Ziegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,A.,Amodei,D.,Chris-
tiano, P., Irving, G.: Fine-tuninglanguage models from humanpreferences. arXiv
preprintarXiv:1909.08593 (2019)