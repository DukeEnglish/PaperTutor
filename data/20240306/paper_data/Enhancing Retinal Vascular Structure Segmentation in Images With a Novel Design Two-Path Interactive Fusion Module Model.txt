Enhancing Retinal Vascular Structure Segmentation in Images With a Novel
Design Two-Path Interactive Fusion Module Model
Rui Yang and Shunpu Zhang
Department of Statistics and Data Science, University of Central Florida, United States
Abstract
Precision in identifying and differentiating micro and macro blood vessels in the retina is
crucial for the diagnosis of retinal diseases, although it poses a significant challenge.
Current autoencoding-based segmentation approaches encounter limitations as they are
constrained by the encoder and undergo a reduction in resolution during the encoding stage.
The inability to recover lost information in the decoding phase further impedes these
approaches. Consequently, their capacity to extract the retinal microvascular structure is
restricted. To address this issue, we introduce Swin-Res-Net, a specialized module
designed to enhance the precision of retinal vessel segmentation. Swin-Res-Net utilizes the
Swin transformer which uses shifted windows with displacement for partitioning, to reduce
network complexity and accelerate model convergence. Additionally, the model
incorporates interactive fusion with a functional module in the Res2Net architecture. The
Res2Net leverages multi-scale techniques to enlarge the receptive field of the convolutional
kernel, enabling the extraction of additional semantic information from the image. This
combination creates a new module that enhances the localization and separation of micro
vessels in the retina. To improve the efficiency of processing vascular information, we've
added a module to eliminate redundant information between the encoding and decoding
steps.
Our proposed architecture produces outstanding results, either meeting or surpassing those
of other published models. The AUC reflects significant enhancements, achieving values
of 0.9956, 0.9931, and 0.9946 in pixel-wise segmentation of retinal vessels across three
widely utilized datasets: CHASE-DB1, DRIVE, and STARE, respectively. Moreover,
Swin-Res-Net outperforms alternative architectures, demonstrating superior performance
in both IOU and F1 measure metrics.
Keywords: Retinal Vessel Segmentation, Swin-Transformer, Res2net, Fusion block,
Medical Imaging, Ophthalmology, Fundus image
1. Introduction
Retinal examinations can diagnose many retinal conditions, such as diabetic retinopathy,
epiretinal membrane, macular edema, and cytomegalovirus retinitis. Retinal vascular
disorders, which attack the retinal blood vessels, are typically connected to other diseases
such as atherosclerosis, hypertension or changes in the circulatory area [1, 2], and the
precise segment of retinal blood vessels and the identification of the playful space of retinal
area disorders are needed to determine their pertinent diagnosis.In recent years, retinal vessel segment methods have been proposed based on the methods
of image processing and machine learning [3-6]. In particular, although the detection
performance of the vessel appears to be generally improved, it is difficult to obtain pixel-
accurate segmentation in some cases because of factors such as insufficient illumination of
the image and periodic noise, likely resulting in a large number of false positives [3].
Early research on vessel segmentation primarily focused on techniques using hand-crafted
features [7, 8], filter-based models [9], and statistical models [10]. By improving border
gradients, eliminating irrelevant background information, and filtering picture noise, these
techniques aim to simplify the segmentation problem to a mathematical optimization
problem with a predetermined solution. The advancements of data-driven methodologies
and technology in computers have made deep learning an important field of research and
application in medical image analysis. Deep learning has been extensively studied for its
remarkable representational learning capabilities [11], consistently outperforming
traditional data segmentation approaches.
From 2012 to 2020, convolutional neural networks (CNNs) dominated the integration of
medical imaging with deep learning. CNNs extract shallow and deep visual features layer
by layer by using multiple convolutional layers, pooling layers, and fully connected layers
at the bottleneck. For medical image segmentation, models like Deeplab-v3 [12] have not
been widely adopted due to the complexity of the models and the features of medical
images. Currently, the basic model for deep learning-based medical image segmentation is
the U-Net model [13] proposed in 2015. Future advancements in this field are based on its
symmetric encoder and decoder structure.
Although CNNs have good feature extraction ability, their inherent inductive bias property
limits their attention to local image features, hindering further model performance
improvement. Some works have introduced CNN-based network attention mechanisms,
such as Compression Excitation Networks [14] and Axial DeepLab [15]. However, these
studies have not significantly addressed the natural deficiencies of CNNs.
The Deep Self-Attention Network (Transformer), first proposed in the article “Attention is
All You Need” [16], originally used in natural language processing, has become the
cornerstone of large-scale models such as GPT-3. The Transformer's ability to model long-
distance correlations and focus on the global properties of input information makes it ideal
for areas such as language translation. Since 2020, researchers have explored applying
Transformers to computer vision with significant progress. Google's ViT [17], Facebook's
DeiT [18], and Microsoft Research Asia's Swin Transformer [19] are excellent examples.
Swin Transformer has been widely used in various computer vision tasks, including
medical image segmentation [19-21], showing its potential to match or exceed CNN
performance, opening new avenues for computer vision development.
In this paper, we propose a novel model that integrates the Swin Transformer into U-Net,
improving the network's capacity to capture long-distance dependencies and model global
information. This addresses the limitations of convolutional networks, which
predominantly focus on local details, resulting in a more precise segmentation of smallvessels. In theory, increasing network depth should improve performance. In practice,
deeper networks pose challenges for training optimization algorithms, leading to increased
training errors. To address issues like gradient vanishing, our model utilizes Res2net,
ensuring that training efficiency is maintained even with a deeper network.
This novel technique uses multi-scale approaches to increase the convolutional kernel's
receptive field, which enables the extraction of more semantic information from the image.
Additionally, our model includes a mechanism to remove redundant information between
the encoder and decoder, enhancing overall model efficiency.
2. Methodology
Figure 1: Architecture of the model structure. It consists of three parts: a feature extraction
encoder (light blue), a redundant information reduction module (light green) and a decoder
(light orange).
2.1 Overall of the model
The model proposed in this paper is based on the U-Net neural network architecture. The
traditional U-Net structure follows a U-shaped design, consisting of three key components:
an encoder, a decoder, and connections. As illustrated in Figure 1, the encoder of our
proposed model comprises four groups of Fu_Block modules, utilizing convolution and
downsampling techniques to extract context information from the feature map. In contrast,
the decoder includes three groups of CBR modules and up-sampling modules to restore the
resolution of the feature map. The CBR module consists of a 3 × 3 convolution layer, a
batch normalization layer, and a ReLU activation layer. Finally, to ensure effective
communication between the encoder and decoder levels, a redundant information reduction
module is employed. This module is crucial because multiple convolutions may cause thefeature map to lose spatial information. This is in contrast to the U-Net model, where there
is no module for reducing redundant information, in order to easily affect the combination
of the context-rich feature map from the encoder with the feature map from the decoder
and at the same time inhibit excess transfer of information to the decoder, thus increasing
the efficiency of the model.
Figure 2: Feature extraction encoder
2.2 Encoder
Figure 2 illustrates the detailed structure of the feature extraction encoder. The pre-
processed image gets into the Swin Transformer method and the Residual Block method in
the encoder. On the other hand, in both paths of the four-layer, the output of each layer has
been made by the two methods. Thereafter, the two layers are converged, and the fusions
sent to the redundant information reduction module.
2.2.1 Swin Transformer path
Inspired by its remarkable achievements in segmentation tasks, we have incorporated the
Swin Transformer into the U-Net architecture. With this integration, the network can
recognize the long-range dependencies and therefore be capable of establishing a coherent
understanding of the global context. This development effectively overcomes the limitation
of the classical convolutional network, which is only programmed to process localinformation. Consequently, our method improves the accuracy of identifying and
segmenting fine details, like small blood vessels.
The initial step involves dividing a preprocessed RGB image into non-overlapping patches
using a patch splitting module. Each patch is considered a "token," with its feature
represented as the concatenation of raw pixel RGB values. For our work, we use a patch
size of 4 × 4, which means the feature dimension will be 4 × 4 × 3 = 48. We then apply a
linear embedding layer to this raw-valued feature, which projects it to an arbitrary
dimension, denoted as C.
Subsequently, multiple Transformer blocks, incorporating customized self-attention
computations and referred to as Swin Transformer blocks, are applied to these patch tokens.
These Transformer blocks maintain the token count at H/4 × W/4, forming what we term
'Stage 1.'
To generate a hierarchical representation, the reduction in the number of tokens occurs
through patch merging layers as the network deepens. The first layer merges these two
concatenated patch features into one by concatenating the features of each pair of 2×2
neighboring patches and applying a linear layer to these concatenated 4C-dimensional
features. This process results in a fourfold reduction in tokens (equivalent to a 2×2 down-
sampling of resolution), while adjusting the output dimension to 2C. Following this, Swin
Transformer blocks perform feature transformation while maintaining the resolution at H/8
× W/8. The procedure progresses through four phases, with notable developments
occurring in 'Stage 3' and 'Stage 4.' Here, the resolution of the output is enhanced to H/16
× W/16 and to H/32 × W/32, respectively.
The Swin Transformer is constructed by replacing the standard multi-head self-attention
(MSA) module in a Transformer block with a module based on shifted windows, while
keeping other layers unchanged. As depicted in Figure 3 (a), a Swin Transformer block is
basically a shifted window-based MSA module, followed by a 2-layer MLP with GELU
nonlinearity. A LayerNorm (LN) layer is applied before each MSA module and each MLP,
and a residual connection is applied after each module. The calculation process of a
continuous Swin Transformer block can be expressed by a series of formulas:
𝑧̂! = 𝑊-𝑀𝑆𝐴(LN(𝑧!"#))+𝑧!"#,
𝑧! = 𝑀𝐿𝑃(𝐿𝑁(𝑧̂!))+𝑧̂!,
𝑧̂!$# = 𝑆𝑊-𝑀𝑆𝐴(LN(𝑧!))+𝑧!,
𝑧!$# = 𝑀𝐿𝑃(𝐿𝑁(𝑧̂!$#))+𝑧̂!$#, (1)
Here 𝑧! denotes the output features of the SW-MSA module and the MLP module for
block 𝑙, respectively. W-MSA and SW-MSA represent window-based multi-head self-
attention using regular and shifted window partitioning configurations, respectively.(a) (b)
Figure 3: (a) The basic structure of consecutive Swin Transformer blocks; (b) The basic
structure of residual blocks (Res2 module).
2.2.2 Residual Block path
A Res2Net block has been introduced as an additional module in another path. In the
segmentation of fundus image vessels, the distribution of micro-vessels is diffuse, and their
size is small. Hence, there is a high probability of loss of semantic information on small
objects with a number of single convolution operations that lead to the segmentation
accuracy of the small objects being comparatively reduced [22]. Drawing inspiration from
Res2Net, this paper introduces and incorporates the Res2Net Block into U-Net. This
consists of partitioning the feature map across numerous channels, merging the adjacent
feature maps, and later applying convolution in order to develop the receptive field of the
network. This new method is designed to overcome obstacles in accurately segmenting
small objects, focusing on improving the retrieval of semantic information within the
process of segmenting vessels in fundus images.
The process begins with the input of a preprocessed RGB image into the CBR module and
max pool. Subsequently, we apply Res2Net blocks 4, 6, 9, and 2 times across the four layers.
Figure 3 (b) displays the configuration of the Res2Net block, beginning with the
application of a 1 × 1 convolution kernel to the input feature map, followed by the division
of the channel into four separate groups. While the first group is directly transmitted
downward, all other groups have a 3 × 3 convolution kernel, which is used to extract
features and bring about a change in the receptive field in the branch. Each group's output,
fused with the feature map on the left, is applied. Group splicing and fusion are then done
through a 1 × 1 convolution kernel. In the final step, the outcome is combined with the
output from the residual connection branch. This approach is based on multi-scale methods
that increase the receptive field of the convolutional kernel, enabling further semantic
information extraction from images.(a) (b) (c)
Figure 4: The basic structure of fusion blocks. (a) Fu-Block 1; (b) Fu-Block 2 and 3; (c)
Fu-Block 4
2.2.3 Fusion of the outputs of two paths
As depicted in Figure 2, the two paths are fused separately at the output of each layer.
Figure 4 illustrates the structure of four Fu-Blocks, which perform CBR on each input and
then utilize concatenation to connect the 1-dimensional feature matrix. Fu-Block 1 has two
inputs: one from the output of the 1st layer Swin Transformer block and the other from the
1st layer Residual block. Fu-Blocks 2, 3, and 4 have three inputs, two of which are identical
to those in Fu-Block 1 from the Swin Transformer Block and Residual Block, while the
remaining input is copied from the preceding Fu-Block.
The output of Fu-Block 4 distinguishes itself from the others through the implementation
of fusion coding using a Horblock. This specialized approach is employed to capture
attention and enhance fusion attention features. Notably, Horblock incorporates Recursive
Gated Convolution (𝑔%𝐶𝑜𝑛𝑣), as illustrated in Figure 4 (c). In this newly designed
approach, it has been intended to support the efficient, translation-equivariant, and
extendable high-order spatial interactions via a recursive architecture with the help of gated
convolutions.
It's important to emphasize that 𝑔%𝐶𝑜𝑛𝑣 can seamlessly replace the spatial mixing layer in
various Vision Transformers and convolution-based models. The quadratic complexity in
input size related to self-attention encumbers the limit for how practically it can be
efficiently used within Vision Transformers, more so in tasks like segmentation and
detection, which require higher resolution for its feature maps.
2.3 Reduce redundant information module.
In Figure 1, the light green block represents the module to reduce the redundant information.
It is responsible for providing the capacity to combine the context-rich feature map fromthe encoder with the feature map from the decoder. It also prevents oversupply of
information to the decoder, thus making this overall model more efficient.
Bilinear interpolation is employed as the method for upsampling, effectively enlarging the
features in layer n+1 to match the spatial dimensions of layer n. After the upsampling, the
module measures the absolute element-wise difference between the upsampled tensor of
the current layer n+1 and the tensor of the previous layer n. This computation quantifies
the absolute difference between the two feature maps, preserving the most important
information and enabling copy and crop to the decoder. The functionality of the following
code will depend on the characteristic and the architecture of the deep learning model to
apply. The module can be expressed using the formulas provided below.
𝑓(𝑥)#& = 𝑎𝑏𝑠(𝐶𝐵𝑅(𝑓(𝑥)#)−↑ [𝐶𝐵𝑅(𝑓(𝑥)’)])
𝑓(𝑥)#&&
=
𝑎𝑏𝑠(𝐶𝐵𝑅(𝑓(𝑥)#&
)−↑
[𝐶𝐵𝑅(𝑓(𝑥)’&
)])
⋮
𝑓(𝑥)’&&
=
𝑎𝑏𝑠(𝐶𝐵𝑅(𝑓(𝑥)’&
)−↑
[𝐶𝐵𝑅(𝑓(𝑥)(&
)]) (2)
⋮
Where 𝑓(𝑥) is the feature map for each layer. The number of superscripts is the layer
number. The prime symbol is the number of reduced redundant operations. ↑ represents the
bilinear interpolation unsampling method.
2.4 Decoder
In Figure 1, the light-yellow section represents the decoder component of our model. At
Layer 4, the input undergoes a single cycle of CBR (convolution, batch normalization, and
ReLU activation) before being unsampled and concatenated with the input at Layer 3. This
process repeats until the final output is obtained.
3. Experiments
3.1 Dataset
The DRIVE [23] dataset is a collection of 40 retinal images with segmentation annotations,
obtained from a diabetic retinopathy screening program in the Netherlands. Seven of the
images show mild early diabetic retinopathy, and 33 show normal ones. The retinal images
were formally divided into training sets. The images are captured at a size of 565 × 584
pixels with 8 bits per color plane.
The CHASE_DB1 [24] dataset, designed for retinal vascular segmentation, contains 28
color retina images of 999 × 960 pixels' resolution, taken from the left and right eyes of 14
students. All the sets of these images were manually annotated for segmentation by two
independent experts, normally taking the annotations of the first expert as the ground truth.
The first 20 images are meant for training, while the remaining eight are reserved for testing.The STARE [25] dataset encompasses 20 retinal fundus images, half of which exhibit
pathological signs. During the iteration, 18 images are selected at a time and are considered
as training samples, while the remaining set of images will be used as test samples over 10
repetitions. There was no predefined division of the data to solidify the reliability of the
experimental results, hence making 10-fold cross-validation a suitable method to apply.
3.2 Preprocessing
Image pre-processing was performed with the aim of increasing the data diversity and
therefore making the model more robust before entering the model. (1) A random
horizontal flip operation with a 0.5 probability was applied. (2) A random vertical flip with
a 0.5 probability was employed. (3) Random rotations were applied to the images.
3.3 Hyper-parameter
Hyperparameter settings used in our model training are as follows. Optimizer: Adam.
Initial learning rate: 10"). Weight decay: 10"*. Learning rate adjustment strategy:
CosineAnnealingLR. Post-processing image threshold: 0.5. Number of training epochs: 40.
3.4 Loss function: Binary Cross-Entropy Loss (BCELoss).
The model employs a binary cross-entropy loss with a fixed threshold of 0.5 to determine
whether a pixel belongs to a vessel or the background. The unreduced loss can be described
as:
ℒ(𝑦,𝑦H) = −# ∑+ [𝑦 𝑙𝑜𝑔𝑦H +(1−𝑦 )𝑙𝑜𝑔(1−𝑦H )] (3)
,-# , , , ,
+
where 𝑦 and 𝑦H indicate ground truth and predicated of 𝑖./ image, N is the batch size.
3.5 Quantitative Benchmarking
We conducted an extensive comparative analysis, assessing our architecture alongside
several high-performing models, including Unet++ [26], CS-Net [27], Residual U-Net [28],
RV-GAN [29], and FR-Unet [30]. Training and evaluation were performed using their
publicly accessible source code on all three datasets.
Subsequently, we have compared our architecture with the existing retinal vessel
segmentation models and presented the results for datasets DRIVE, CHASE-DB1, and
STARE in Table 1. Sensitivity, specificity, accuracy, F1-score, and area under the curve
(AUC) are some of the conventional performance evaluation parameters that are calculated.
We have further investigated the retinal vessel segmentation accuracy and structural
similarity using intersection-over-union (IOU). These evaluations offer a comprehensive
understanding of the strengths and weaknesses of each model in our comparative analysis.
As observed in the tables, our model is ranked consistently with better performance
compared with the existing U-Net based design and even the more recent GAN-based
models. These are evidenced by the AUC, F1 score, and IOU, which are the mainevaluation metrics in this task. Notably, our model improved specificity, accuracy, AUC,
F1 score, and IOU values across all datasets.
Table 1 (a): Result of vessel segmentation (CHASE_DB1)
Table 1 (b): Result of vessel segmentation (Drive dataset)
Table 1 (c): Result of vessel segmentation (STARE Dataset) data and charts.
3.6 Precision of vessel segmentations
As illustrated in Figure 5, Swin-Res-Net demonstrates a high level of accuracy in vessel
segmentation compared to ground truths. The most challenging task is micro blood vesselsegmentation. Notably, our model successfully identified and segmented micro blood
vessels.
Figure 5: Swin-Res-Net segments vessel with good precision on the micro blood vessel
4. Conclusions
In this research paper, we introduce Swin-Res-Net, a new multiscale architecture. By
integrating innovative matching loss functionality, this architecture demonstrates the
ability to generate highly accurate segmentations of vein structures while providing robust
confidence values for two key performance parameters. Our architectural innovation holds
significant application potential in the field of ophthalmology, particularly in analyzing
degenerative retinal diseases and predicting future developments. Our future research
efforts aim to extend the application of this methodology to diverse data modalities.References
[1] C. S. Brand, "Management of retinal vascular diseases: a patient-centric approach,"
(in eng), Eye (Lond), vol. 26 Suppl 2, no. Suppl 2, pp. S1-16, Apr 2012, doi:
10.1038/eye.2012.32.
[2] J. Son, S. J. Park, and K.-H. Jung, "Retinal Vessel Segmentation in Fundoscopic
Images with Generative Adversarial Networks," ArXiv, vol. abs/1706.09318, 2017.
[3] M. M. Fraz et al., "Blood vessel segmentation methodologies in retinal images--a
survey," (in eng), Comput Methods Programs Biomed, vol. 108, no. 1, pp. 407-33,
Oct 2012, doi: 10.1016/j.cmpb.2012.03.009.
[4] S. A. Kamran, A. Tavakkoli, and S. L. Zuckerbrod, "Improving robustness using
joint attention network for detecting retinal degeneration from optical coherence
tomography images," in 2020 IEEE International Conference On Image Processing
(ICIP), 2020: IEEE, pp. 2476-2480.
[5] E. Ricci and R. Perfetti, "Retinal Blood Vessel Segmentation Using Line Operators
and Support Vector Classification," IEEE Transactions on Medical Imaging, vol.
26, no. 10, pp. 1357-1365, 2007, doi: 10.1109/TMI.2007.898551.
[6] J. V. B. Soares, J. J. G. Leandro, R. M. Cesar, H. F. Jelinek, and M. J. Cree, "Retinal
vessel segmentation using the 2-D Gabor wavelet and supervised classification,"
IEEE Transactions on Medical Imaging, vol. 25, no. 9, pp. 1214-1222, 2006, doi:
10.1109/TMI.2006.879967.
[7] M. Javidi, A. Harati, and H. Pourreza, "Retinal image assessment using bi-level
adaptive morphological component analysis," Artificial Intelligence in Medicine,
vol. 99, p. 101702, 2019/08/01/ 2019, doi:
https://doi.org/10.1016/j.artmed.2019.07.010.
[8] M. Javidi, H.-R. Pourreza, and A. Harati, "Vessel segmentation and microaneurysm
detection using discriminative dictionary learning and sparse representation,"
Comput. Methods Prog. Biomed., vol. 139, no. C, pp. 93–108, 2017, doi:
10.1016/j.cmpb.2016.10.015.
[9] C. Wang et al., "Tensor-cut: A tensor-based graph-cut blood vessel segmentation
method and its application to renal artery segmentation," Medical Image Analysis,
vol. 60, p. 101623, 2020/02/01/ 2020, doi:
https://doi.org/10.1016/j.media.2019.101623.
[10] S. Kalaie and A. Gooya, "Vascular tree tracking and bifurcation points detection in
retinal images using a hierarchical probabilistic model," Computer methods and
programs in biomedicine, vol. 151, pp. 139-149, 2017.
[11] D. Jia and X. Zhuang, "Learning-based algorithms for vessel tracking: A review,"
Computerized Medical Imaging and Graphics, vol. 89, p. 101840, 2021/04/01/
2021, doi: https://doi.org/10.1016/j.compmedimag.2020.101840.
[12] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, "Rethinking Atrous
Convolution for Semantic Image Segmentation," ArXiv, vol. abs/1706.05587, 2017.
[13] O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional Networks for
Biomedical Image Segmentation," ArXiv, vol. abs/1505.04597, 2015.
[14] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, "Squeeze-and-Excitation Networks,"
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
7132-7141, 2017.[15] H. Wang, Y. Zhu, B. Green, H. Adam, A. L. Yuille, and L.-C. Chen, "Axial-DeepLab:
Stand-Alone Axial-Attention for Panoptic Segmentation," in European Conference
on Computer Vision, 2020.
[16] A. Vaswani et al., "Attention is All you Need," in Neural Information Processing
Systems, 2017.
[17] A. Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale," ArXiv, vol. abs/2010.11929, 2020.
[18] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. e. J'egou,
"Training data-efficient image transformers & distillation through attention," in
International Conference on Machine Learning, 2020.
[19] A.-J. Lin, B. Chen, J. Xu, Z. Zhang, G. Lu, and D. Zhang, "DS-TransUNet: Dual
Swin Transformer U-Net for Medical Image Segmentation," IEEE Transactions on
Instrumentation and Measurement, vol. 71, pp. 1-15, 2021.
[20] H. Cao et al., "Swin-Unet: Unet-like Pure Transformer for Medical Image
Segmentation," in ECCV Workshops, 2021.
[21] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. R. Roth, and D. Xu, "Swin UNETR:
Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images,"
ArXiv, vol. abs/2201.01266, 2022.
[22] Y. Chen, X. Zhu, Y. Li, Y. Wei, and L. Ye, "Enhanced semantic feature pyramid
network for small object detection," Signal Processing: Image Communication, vol.
113, p. 116919, 2023/04/01/ 2023, doi:
https://doi.org/10.1016/j.image.2023.116919.
[23] J. Staal, M. D. Abramoff, M. Niemeijer, M. A. Viergever, and B. v. Ginneken,
"Ridge-based vessel segmentation in color images of the retina," IEEE
Transactions on Medical Imaging, vol. 23, no. 4, pp. 501-509, 2004, doi:
10.1109/TMI.2004.825627.
[24] A. Carballal et al., "Automatic multiscale vascular image segmentation algorithm
for coronary angiography," Biomedical Signal Processing and Control, vol. 46, pp.
1-9, 2018/09/01/ 2018, doi: https://doi.org/10.1016/j.bspc.2018.06.007.
[25] A. D. Hoover, V. Kouznetsova, and M. Goldbaum, "Locating blood vessels in
retinal images by piecewise threshold probing of a matched filter response," IEEE
Transactions on Medical Imaging, vol. 19, no. 3, pp. 203-210, 2000, doi:
10.1109/42.845178.
[26] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, "UNet++: A Nested
U-Net Architecture for Medical Image Segmentation," in Deep Learning in
Medical Image Analysis and Multimodal Learning for Clinical Decision Support,
Cham, D. Stoyanov et al., Eds., 2018// 2018: Springer International Publishing, pp.
3-11.
[27] L. Mou et al., "CS-Net: Channel and Spatial Attention Network for Curvilinear
Structure Segmentation," in International Conference on Medical Image
Computing and Computer-Assisted Intervention, 2019.
[28] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, and V. K. Asari, "Recurrent
Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical
Image Segmentation," ArXiv, vol. abs/1802.06955, 2018.
[29] S. A. Kamran, K. F. Hossain, A. Tavakkoli, S. L. Zuckerbrod, K. M. Sanders, and
S. A. Baker, "RV-GAN: Segmenting Retinal Vascular Structure in FundusPhotographs Using a Novel Multi-scale Generative Adversarial Network," in
International Conference on Medical Image Computing and Computer-Assisted
Intervention, 2021.
[30] W. Liu et al., "Full-Resolution Network and Dual-Threshold Iteration for Retinal
Vessel and Coronary Angiograph Segmentation," IEEE Journal of Biomedical and
Health Informatics, vol. 26, no. 9, pp. 4623-4634, 2022, doi:
10.1109/JBHI.2022.3188710.