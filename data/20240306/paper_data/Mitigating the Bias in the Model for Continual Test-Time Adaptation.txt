Mitigating the Bias in the Model for Continual Test-Time Adaptation
InseopChung1 KyominHwang1 JayeonYoo1 NojunKwak1
Abstract
ContinualTest-TimeAdaptation(CTA)isachal-
lenging task that aims to adapt a source pre-
trainedmodeltocontinuallychangingtargetdo-
mains.IntheCTAsetting,amodeldoesnotknow
when the target domain changes, thus facing a
drasticchangeinthedistributionofstreamingin-
putsduringthetest-time.Thekeychallengeisto
keepadaptingthemodeltothecontinuallychang-
Figure1.Comparisonofthenumberofpredictedsamplesperclass
ingtargetdomainsinanonlinemanner.Wefind
anddistributionofconfidencebetweenEATAandEATA+Ours.
thatamodelshowshighlybiasedpredictionsas
it constantly adapts to the chaining distribution
ofthetargetdata.Itpredictscertainclassesmore thisissuebyadaptingthemodeltothetargetdatagivenat
oftenthanotherclasses,makinginaccurateover- test-time.Sincethetargetdataareunlabeled,theadaptation
confident predictions. This paper mitigates this isdoneinanunsupervisedandonlinemannerwhichmeans
issuetoimproveperformanceintheCTAscenario. thatthemodelhastopredictandadaptimmediatelyupon
Toalleviatethebiasissue,wemakeclass-wiseex- thearrivalofthetestsamples.TTAgenerallyassumesthat
ponentialmovingaveragetargetprototypeswith theaccesstothesourcedataduringtest-timeisinfeasible
reliabletargetsamplesandexploitthemtoclus- duetoprivacy/storageconcernsandlegalconstraints,hence
terthetargetfeaturesclass-wisely.Moreover,we theonlyavailableduringthetest-timeistheaccesstothe
aimtoalignthetargetdistributionstothesource target data and the off-the-self source pre-trained model.
distributionbyanchoringthetargetfeaturetoits Recently,anotherlineofresearchinTTAcalledcontinual
correspondingsourceprototype.Withextensive test-timeadaptation(CTA)(Wangetal.,2022;Niuetal.,
experiments,ourproposedmethodachievesnote- 2022)isintroduced.DifferentfromtheconventionalTTA
worthyperformancegainwhenappliedontopof settingwhichassumesadaptingamodeltoasinglefixed
existingCTAmethodswithoutsubstantialadapta- stationarytargetdistribution,CTAassumesthetargetdis-
tiontimeoverhead. tributionchangesovertime.Thetimingofthedistribution
changesisnotprovided.Therefore,themodelneedstocon-
stantlyadapttoshiftingtargetdatadistributions,anditisnot
1.Introduction feasibletoresetthemodeltoitsinitialsourcepre-trained
weightswhendistributionchangesoccur.ThismakesCTA
Datadistributionshiftsisaproblemwhichthedistribution
an extremely challenging task resembling the real-world
ofdatagivenattest-timeisdifferentfromthatofthetrain-
scenarioswheretheinputdistributionmaychangecontinu-
ing data. This is because the DNNs heavily rely on the
allyandabruptlywithoutpriornotice(e.g.enteringatunnel
assumptionthattest-timedataareindependentandidenti-
duringautonomousdriving).
callydistributed(i.i.d.)withthetrainingdatawhichisvery
unlikelyinreal-worldscenarios(Hendrycks&Dietterich, Duetoitsintricatenature,themodelissusceptibletocon-
2019;Kohetal.,2021).Test-timeadaptation(TTA)(Sun firmationbias(Arazoetal.,2020),whereittendstooverfit
etal.,2020;Wangetal.,2020;Zhangetal.,2022b)resolves to the incoming target data while continuously adapting
inanonlinemanner.Weobservethisresultsinhighlybi-
*Equalcontribution 1GraduateSchoolofConvergenceScience ased and mis-calibrated model predictions. Fig. 1 shows
andTechnology,SeoulNationalUniversity,Seoul,SouthKorea.
thenumberofpredictedsamplesperclassandthedistribu-
Correspondenceto:InseopChung<jis3613@snu.ac.kr>,Nojun
tionofpredictionconfidenceofthemodeltrainedbyEATA
Kwak<nojunk@snu.ac.kr>.
(Niuetal.,2022),oneofthestate-of-the-artCTAalgorithm,
andEATA+OursusingtheImageNet-C(Dengetal.,2009)
1
4202
raM
2
]GL.sc[
1v44310.3042:viXraMitigatingtheBiasintheModelforContinualTest-TimeAdaptation
benchmark.ThehorizontaldottedlineinFig.1(a)indicates • ThroughcomprehensiveexperimentsonImageNet-C
theactualnumberofsamplesassignedtoeachclass.The and CIFAR100-C, the proposed method is shown to
classesaresortedindescendingorderofthenumberofpre- be compatible with other CTA methods and able to
dictedsamplesforclarity.EventhoughEATAshowsdecent substantiallyimprovetheaccuracywithoutsignificant
averageaccuracyinImageNet-C(49.81%),itsprediction adaptationtimeoverhead.
ishighlybiasedtofavorcertainclassesmoreoftenwhile
• We conduct an in-depth analysis of our proposed
avoidingpredictionsforothers.Also,Fig.1(b)showsthat
method,highlightingitscapabilitytomitigatethebias
EATAmakes25%ofitspredictionwithconfidencehigher
ofthemodelbyrestrainingfrommakingover-confident
than0.95,highlightingasignificantissueofoverconfidence
predictionsandfosteringmorecalibratedconfidence.
inthemodel.
Toovercometheaforementionedbiasinthemodelandto
furtherimproveitsperformanceinCTAscenario,thispa- 2.RelatedWorks
perpresentsapairofstraightforwardyethighlyeffective
2.1.Test-TimeAdaptation
techniques:theexponentialmovingaverage(EMA)target
domainprototypicallossandsourcedistributionalignment Recently,test-timeadaptation(TTA)hasgarneredsubstan-
viaprototypematching.TheEMAprototypicallossmain- tialattention,adaptingmodelstospecifictestdomainsdur-
tainsaprototypeforeachclassbycontinuouslyupdating inginference-timeafterbeingdeployedtothetargetdata.
eachprototypewiththefeaturesofreliabletargetsamples TTAsharessimilaritieswithsource-freedomainadaptation
givenattest-timeinanEMAfashion.TheseEMAtarget (SFDA)(Liangetal.,2020),intheaspectofadaptingthe
prototypesareutilizedtoorganizethetargetfeaturesinto off-the-shelfsourcepre-trainedmodeltothetargetdomain
distinctclassesbypullingthemclosertotheircorresponding withoutaccessingsourcedata.However,TTAdiffersfrom
EMAprototypeswhilesimultaneouslypushingthemaway SFDAinthatitisanonlinelearningapproachrelyingsolely
from other irrelevant prototypes. The EMA prototypical ontheincomingtargetsamplesgivenattest-timewithout
losseffectivelycapturesthechangingtargetdistributionand repetitively accessing a large amount of unlabeled target
leveragesitforclass-specificclustering.Itsgoalistopre- domain data. This feature makes TTA more challenging
ventanunduebiastowardscurrenttargetdistributionsand, inthatoverallinformationsuchasknowingthetargetdo-
instead,adeptlycaptureandadapttochangingtargetdistri- maindistribution(Sun&Saenko,2016)orclusteringthe
butions,therebymitigatingthebiasissue.Ontheotherhand, targetfeatures(Liangetal.,2020)isnotavailable.Many
to prevent the model from drifting too far away from the studies (Wang et al., 2020; Niu et al., 2022; Lim et al.,
pre-trainedsourcedistribution,wealignthetargetdatadistri- 2023) efficiently adapt models to the test domain by up-
butiontothesourcedistributionbyminimizingthedistance dating only the batch normalization layer, following the
betweenthetargetfeatureanditscorrespondingsourcepro- research (Schneider et al., 2020) that only replacing the
totype.Aligningthedistributionbetweensourceandtarget statisticsforbatchnormalizationwithoutlearningcaneffec-
is a common strategy in domain adaptation (Tzeng et al., tivelyaddressdomainshifts.Thesemethods(Wangetal.,
2017;Longetal.,2018)whichhasalsobeenemployedin 2020;Niuetal.,2022)adaptthemodeltothetargetdomain
TTAmethod(Suetal.,2022).Nonetheless,itreliesonthe viaentropyminimizationlosstomakethepredictionsmore
strongassumptionthatbothdomainsfollowtheGaussian confident. Alternatively, there are approaches (Su et al.,
distribution and employ complex distance metric such as 2022;Jungetal.,2022)thatupdatetheentirebackboneso
KL-Divergence. In contrast, our method takes a simpler that the distribution of the target domain feature has sim-
approach: we directly minimize the mean squared error ilarstatisticstothatofthesourceonthepremisethatthe
distancebetweeneachtargetfeatureanditscorresponding statisticsofthesourcedomainfeaturesareknown.Some
sourceprototype.AsdepictedinFig.1,ourintroducedterms othermethods(Iwasawa&Matsuo,2021;Jangetal.,2023)
effectivelyalleviatethebiasinpredictions.EATA+Oursex- entirelyfreezethebackboneandsolelymodifytheclassi-
hibitsreducedinclinationtofavorspecificclasses,resulting fier by leveraging prototypes derived from target domain
inamorebalanceddistributionofpredictionsacrossclasses featuresbasedonpseudo-labels.Additionally,somemeth-
comparedtoEATA.Theoverconfidentpredictionsisalso ods(Sunetal.,2020;Bartleretal.,2022)modifythemodel
mitigatedalongwithimprovedaverageaccuracy(51.32%). architectureduringsourcedomaintrainingtoincorporate
Contributionsofthispaperareasfollows: self-supervisedlossesforthetargetdataduringtest-time.
2.2.ContinualTest-TimeAdaptation(CTA)
• Theproposedmethodisseamlesslyapplicabletoexist-
ingapproacheswithoutadditionalparametersorrequir- Inpractice,thedistributionofthetestdomaincanexhibit
ingaccesstothesourcedomaindataattest-timewhich continuous changes or have correlations among continu-
transformsitintoasimpleplug-and-playcomponent. ously incoming samples, whereas TTA relies on a strong
2MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Figure2. Beforedeployingthemodel,wegeneratethesourceprototypes(Pss)usingthesubsetofsourcedataandthesourcepre-trained
featureextractor,f .Afterthemodelisdeployedtothetargetdomain,themodeladaptstothetargetdatabyminimizingourproposed
ϕ0
termsL andL alongwithL .Weconstructclass-wisetargetprototypes(Pts)thatareupdatedwithtargetfeaturesviaEMA
ema src unsup
manner.WeutilizebothPtsandPsstocomputeL andL respectively.NotethatL isfirstcomputedandthenfollowedby
ema src ema
updatingthePtssubsequently.Thedottedlineindicatesprovidingrequiredinformationsuchasentropyandpseudo-labelofinput.
assumption that test-time data follow i.i.d, meaning that 3.ProblemDefinition
thedistributionofthetest-timedatadoesnotchangeand
s thta ey ps rs ot ba lt eio mna or fy c. oC no tiT nT uA alt( eW sta -n tig me et aa dl a., p2 ta0 t2 io2 n)fi anrs dt psu rog pg oe ss ets
s
G {xiv
s
ne ,n
y
nsa }m
N
n=o
s
1d ,e Cl, Tg Aθ0 i, sp ar te a- st kra oin fe ad dao pn tia ngso gu θ0rc te od tho em ua nin labD es le=
d
thecorrespondingproblemsetting.Itidentifiestheproblem target data which its domain continually changes, Dk =
oferroraccumulationinexistingTTAmethodswhenthe {xk m}N mk =1 (k refers to the target domain index) with an
distributionoftest-timedatachangesandaddressesitbyin- unsupervised objective, Lunsup. The target domain data
troducingateacher-studentframeworkandensuringvarious
arrivesequentiallyandtheirdomainchangesovertime(k=
augmentedtestsamplestohaveconsistentpredictionsalong 1,...,K). The model only has access to the data of the
withstochasticrestorationoftheweights.Followingthis, current time step and has to predict and adapt instantly
Brahma&Rai(2023)andDo¨bleretal.(2023)alsoutilize upon the arrival of the inputs for future steps, i.e., θ t →
ateacher-studentstructure,employingregularizationbased θ t+1.Asmentionedearlier,themodelisnotawareofwhen
ontheimportanceofweightsandusingsymmetriccross- thetargetdomainchanges,soithastodealwithsuddenly
entropyloss,respectively.Additionally,Niuetal.(2022), changing input distribution. Lunsup can take the form of
which considers the confidence and diversity of samples entropyminimizationlosswhichisusedtooptimizeonly
formodelupdates,hasproventobeeffectiveinthecontext theaffineparametersofbatchnormalizationlayer(Wang
ofCTA.BuildinguponexistingTTAmethods,Songetal. etal.,2020;Niuetal.,2022)orconsistencylosstooptimize
(2023);Hongetal.(2022)haveproposedtechniquestodi- the whole parameters (Wang et al., 2022; Do¨bler et al.,
minishmemoryconsumption,therebypromotingefficient 2023).Theevaluationofthemodelisdeterminedbytest-
adaptationinCTA. timepredictionsinanonlinemanner.
2.3.CTAunderDynamicScenarios 4.ProposedMethod
Recently,therehasbeenmanyattemptstoconsiderdynamic 4.1.EMATargetDomainPrototypicalLoss
scenariosinCTA(Gongetal.,2022;Niuetal.,2023;Yuan
EMAtargetprototypicallosscomprisestwodistinctsteps,
etal.,2023a;Gongetal.,2023).NOTE(Gongetal.,2022)
oneiscategorizingthefeaturesoftargetinputsbyclasses
andRoTTA(Yuanetal.,2023a)pointoutthatreal-world
utilizingtheEMAtargetprototypesandtheotherisupdating
dataareoftentemporallycorrelated(non-i.i.d)andpropose
theprototypeswithfeaturesofreliabletargetsamplesinan
robustCTAmethodsagainstnon-i.i.d.testdata.SAR(Niu
exponentialmovingaveragemanner.Aclassificationmodel,
etal.,2023)considerstestdatawithmixeddomainsshifts,
g , consists of a feature extractor f and a classification
singlesamplebatchandimbalancedlabelshift.Recently, θ ϕ
SoTTA (Gong et al., 2023) claims that, in real-world set-
headh ω.Eachweightvectorω
c
∈Rd inω ∈RC×d canbe
consideredasthetemplateforclasscwhereCisthenumber
tings,extraneoussamplesoutsidethemodel’sscope,such
ofclassesanddisthedimensionoftheextractedfeature,
asunseenobjects,noise,andadversarialsamplescreatedby
malicioususers,canbeprovidedasinputsandproposesa
f ϕ(x)∈Rd.Therefore,weinitializetheEMAtargetproto-
waytoscreenoutthesenoisysamplesduringCTA. typesastheweightsofh,henceP ct = ∣∣ωω cc ∣∣2.P ctandω crefer
3MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Algorithm1ThepseudocodeofourproposedCTAprocess theirpseudo-labelsasoutlinedbelow:
forK numberoftargetdomains.
Require: K number of target domains {Dk =
{ Sx ouk m r} cN m esk = u1} bK k -s= a1 m, pth lee ss Dou
s
=rc {e
xs
np }re
N
n- =st 1r ,ai bn ae tcd hm sio zd ee Bl .g θ0(⋅), P y˜t t =α⋅P y˜t t+(1−α)⋅ ∣∣ff ϕϕ (( x˜x˜ tt )) ∣∣2. (2)
1: Generate the source prototype for each class, P cs =
2 3:
:
I fN on1 c rs iti∑ aal dN i i= z oc 1s e mf e aϕ a i0 c n( h kx Es i i) M n. KAt da orgetprototype,P ctas ∣∣ωω cc ∣∣2 H f Pe ce ta .r te u W, re eα ( d∣i ∣ efs f ϕ tϕ at ((h cx˜x˜ he tt )) ∣ fb ∣2 ϕl )e (n a x˜d s ti )wng ie nf n oa oc rr dt mo er ra. l tiW ozee sd ton ω po cr gm w raa h dl ei iz n ee ni tnth i st ie i ga nlt ia azr lig n te ogt
4: forabatchx={xk b}B b=1inDk do f ϕ. If there exists N c number of samples with the same
5: Forwardthebatchandmakepredictions,z=g θ(x) pseudo-labelinabatch,weusetheaverageoftheirfeatures
6: ComputeLunsup ( N1
c
∑N i=c 1f ϕ(x˜t i))forupdatingthetargetprototype,P ct.As
7: Identifyreliableinputswithlowentropy newbatchesoftargetdatasteamin,Ptsareupdatedwith
8: ComputeLemaandLsrconlywiththefeaturesof features of new incoming target data in an EMA fashion.
reliabletargetinputs. TheindividualmagnitudesofeachPtcanvary,potentially
9: UpdatePtsvia(2) leadingtoinaccuraciesintheresults.Toaddressthisissue
10: OptimizemodelbyminimizingLoverall. andensureconsistencyinmagnitudes,wenormalizeeach
11: endfor P ct before performing the dot product with f ϕ(x˜t ) as de-
12: endfor scribedin(1).PleasenotethatLemaiscomputedfirstand
thenfollowedbytheupdateofPt using(2)withf ϕ(x˜t ),
nottheotherwayaround.Also,itisimportanttomention
totheEMAtargetprototypeandtheheadweightofclass that Pts are not employed to classify the target input for
c,respectively.Wenormalizeω ctoeliminatethedifference modelevaluationbutsolelyforcalculatingthelossLema.
inmagnitudesbetweenω candtheextractedtargetfeature Themodelevaluationismeasuredbyz=g θ(xt ),withthe
f ϕ(xt )whenupdatingthetargetprototypesvia(2).There headofthemodel,h.ItisdifferentfromT3A(Iwasawa&
areC numberofEMAtargetprototypes,whichweutilize Matsuo,2021)whichbuildsanactualclassifierforevalua-
tocategorizethestreamingtargetinputsintoclasses.This tionwithfeaturesoftargetsamplesgivenattest-time.
isachievedbyminimizingthecross-entropylossusingthe
Inshort,(1)organizesthetargetfeatureintoseparateclasses
pseudo-labels.However,beforecomputingtheloss,wefirst
by enhancing its similarity with the corresponding EMA
identifyreliabletargetsamplesasproposedin(Niuetal.,
targetprototypewhile(2)updatesclass-specificprototypes
2022),whichexcludessampleswithhighentropy,thuslow
confidence.Givenabatchoftargetdata,xt ∈RB×C×H×W, withthetargetdatafeaturesinanEMAmannertogradually
reflect the changing target distribution. The purpose is to
foreachsamplextinxt,wecalculateitsentropyestimated
mitigatethebiasinthemodelbypreventingitfrombeing
bythemodelg θ,H θ(xt ).Then,wefilteroutsampleswith
ovetfittedtothecurrenttargetdatabutrathertocapturemore
entropyhigherthanthepre-definedentropythreshold,E .
0
general target distribution than can handle the changing
Theremainingsamplesarethereliablesampleswithlow-
entropydenotedas˜xt.Foreachsamplex˜tin˜xt,weobtain targetdistribution.
its pseudo-label y˜t = argmax cg θ(x˜t )c and compute the
4.2.SourceDistributionAlignmentviaPrototype
followingloss:
Matching
Priortodeployingthemodeltothetargetdomainfortesting,
Lema=−log( ∑e
C
cxp ex( pf ϕ (f(x ϕ˜ (t ) x˜t⋅ )∣∣P ⋅P y˜t ∣y˜ ∣t t Pt P∣∣ ct2
ct
∣∣) 2)). (1) w u
p
as re mein -g atg re xatn ih
i
me ner
e
ua s dt u me fb et s oah e
t
fe t uo 1rs efo 0tu 0eh xr ,e 0c tre 0s ao 0p cu tr doro c art e to
f
at d ϕy o f0p rm
.
oe
M
maf io onr
tr
hd ee eaa ptc a
r
seh oa
c
unc
i
rsl d ca
e
ets lh ys
,
te ri wn ath iea ned ss sav o ema u tn .prc c
l
Ae e
e
W ane dd ao pt p-p lyro sd ou fc tt mf aϕ x(x˜ ot p) erw ai tt ih one ,ve thry enE mM aA xit mar ig ze etp itr so sto imty ip lae rP itc yt s tuo ru er sce exp tr ro acto tety dp be yfo fr ϕc 0l ,a hs es nc ci es Pco csm =pu N1t sed ∑a
N
is
=c
1san
f
ϕa 0v (e xra
s
ig )e ,wof hf ee ra e-
withthetargetprototypeofthepseudo-label,Pt ,bymini- Ns isthenumberofsampleswithcc lasslabelcinthesub-
y˜t c
mizingLema.Lemaassuresf ϕ(x˜t )tohavehighsimilarity set.ThereexistsC numberofsourceprototypesgenerated
withP y˜t
t
andlowsimilaritywithotherremainingPts.Lema beforetest-timeandaresavedinmemorytobeusedlater
isdesignedtoback-propagateonlytothef andnottothe atthetest-timeadaptationphase.Duringthetest-time,we
ϕ
Pts.UponcomputingLema,weproceedtoupdatePtsin minimizethemeansquarederror(MSE)distancebetween
anEMAmannerusingthefeaturesofreliablesamplesand thetargetfeatureandthesourceprototypecorrespondingto
4MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
thepseudo-labelofthetargetfeature.
Table1.Classificationaccuracy(%)forthecomparisonofCTA
Lsrc=∣∣P y˜s t−f ϕ(x˜t )∣∣2 2. (3) performanceonImageNet-Cusingthehighestcorruptionlevel5.
Time t——————————————————————————————————————————————————————————————————————————→
S abim ovil ear soto urE cM e A dist ta rr ig be ut tip or not ao lt iy gp ni mca el nl tos ls o, sswe onc la ylc wul ia tt het th he e M S To 3e u At rh co ed 2 1Gau . 5ss 2. .1 03 2 1shot . 59 .3 61 i1 1mp . 6ul 8se .5 09 1 1def 7 6ocus . .9 02 5 9 1glas . 6s 8 .2 16 1 1mot 4 7ion . .779 9z 2 2oom 2 0..5 60 6sn 1 2ow 6 2. .8 38 2 2 2fros 3 3t ..3 41 8 2 2fog 4 5. .4 82 1 5 2brig 8 9ht. . .9 14 2 c 5 2ont . 8ra 4st .4 16 1 2elas 96tic. .. 396 2 2 3pixe 00late .. 661 2 j 3 3peg 11. .265 3 7 3orig 36inal . .713 1 M 2 21 3e . .6 2a 5 0n
reliable samples, ˜xt. The intention of Lsrc is to restrain T T SAT SA D RC 2 1 33 5 0. . .4 2 27 3 3 3 1 32 5 7. . .3 7 73 8 2 3 1 32 5 7. . .8 7 18 8 8 2 1 24 5 7. . .5 0 12 6 3 2 1 29 5 9. . .8 2 52 9 5 4 2 30 6 4. . .0 2 50 9 2 4 3 47 8 1. . .7 8 73 1 5 4 3 32 4 5. . .5 3 88 5 0 4 3 30 3 5. . .0 1 30 4 3 5 4 40 7 6. . .1 8 16 9 3 6 6 51 5 7. . .7 1 82 6 5 2 1 36 6 1. . .6 8 24 3 0 4 4 47 4 6. . .7 0 03 3 8 5 4 41 8 9. . .4 8 53 2 3 4 3 45 9 6. . .2 8 17 2 7 6 7 66 5 4. . .4 1 69 5 3 4 3 41 4 0. . .4 2 62 1 7
themodelfromdeviatingexcessivelyfromthepre-trained R Oo uT rT s-A Only 1 37 2. .0 85 8 2 43 0. .4 92 8 2 35 9. .3 70 8 2 21 9. .4 88 4 1 39 2. .5 10 8 1 38 9. .8 07 4 2 42 5. .3 79 9 2 41 2. .3 31 5 2 42 1. .0 52 4 2 53 2. .6 41 2 3 69 3. .4 13 5 1 44 3. .8 74 4 2 56 2. .7 52 1 2 55 6. .0 84 8 2 55 2. .5 88 6 3 69 9. .8 33 9 2 44 5. .1 95 6
TENT 24.69 32.81 32.72 24.28 26.03 30.29 37.89 30.40 28.46 36.51 49.58 18.16 32.99 35.68 30.60 49.94 32.56
sourcedistributionandtoalignthedistributionsofthetar- TENT+Ours 30.93 39.67 39.24 29.85 32.26 39.28 45.99 41.85 40.57 50.80 62.24 41.84 49.68 53.14 47.55 62.81 44.23
EATA 34.66 40.40 39.39 34.08 34.99 46.51 52.82 50.33 45.83 59.12 67.27 45.17 57.13 59.99 55.46 73.80 49.81
get and the source data, thereby mitigating the impact of E EA AT TA A+ +OTT uA rsC 3 35 6. .6 14 7 4 41 1. .4 74 7 4 40 0. .5 87 3 3 35 5. .5 99 8 3 37 7. .1 24 4 4 48 8. .6 87 9 5 54 4. .5 26 8 5 51 2. .6 19 5 4 46 7. .7 43 6 6 60 0. .3 24 3 6 67 7. .9 98 4 4 46 8. .5 08 1 5 58 8. .0 24 6 6 61 1. .2 22 6 5 56 6. .1 38 7 7 74 4. .4 20 0 5 51 1. .0 35 2
CoTTA 16.15 18.53 19.91 18.52 19.58 31.13 43.07 36.92 36.15 51.18 65.35 23.50 47.71 52.17 44.82 73.99 37.42
distributionshift. CoTTA+Ours 30.06 37.51 36.72 26.86 30.65 42.34 49.64 47.53 44.15 56.65 67.13 37.73 55.98 59.81 54.68 73.17 46.91
RMT 28.45 36.07 36.39 29.83 29.00 35.22 39.58 40.04 36.08 49.35 54.02 36.67 48.62 52.28 48.65 66.63 41.68
RMT+Ours 29.60 37.85 38.26 31.60 30.98 36.46 40.56 42.06 38.24 46.31 54.19 38.02 50.73 53.24 51.24 65.14 42.78
4.3.OverallObjective
The overall objective of our proposed continual test-time Table2.Classificationaccuracy(%)forthecomparisonofCTA
adaptationmethodisasfollows: performanceonCIFAR100-Cusingthehighestcorruptionlevel5.
Time t——————————————————————————————————————————————————————————————————————————→
Loverall =Lunsup+λ emaLema+λ srcLsrc (4) M Soe ut rh co ed 2Gau 7ss.
.02
3shot
2.00
i6mp 0ulse
.64
7def 0ocus
.64
4glas 5s
.91
6mot 9ion .19z 7oom 1.21sn 6ow
0.53
5fros 4t
.18
4fog
9.70
7brig 0ht.
.48
c 4ont 4rast
.91
6elas 2tic.
.79
2pixe 5late
.29
j 5peg
8.77
7orig 8inal
.90
M 55e .1a 4n
T3A 28.10 36.47 59.70 67.25 43.91 67.07 69.93 57.42 50.83 45.34 69.55 44.13 58.64 23.52 55.77 76.82 53.40
Lunsup represents the unsupervised loss employed in the T TT SA DC 5 58 6. .8 86 7 6 53 8. .6 63 4 6 51 6. .4 26 3 7 72 1. .8 69 2 5 59 7. .4 45 5 7 60 9. .8 56 6 7 72 1. .7 34 1 6 65 4. .1 23 2 6 66 4. .5 46 4 5 59 7. .7 36 8 7 73 2. .2 84 8 6 68 8. .4 86 3 6 63 3. .4 48 1 6 67 6. .2 08 6 6 50 8. .3 06 7 7 75 5. .8 33 2 6 66 4. .2 55 2
SAR 59.03 63.80 62.28 73.45 61.81 71.32 73.76 67.38 68.78 63.19 74.28 71.40 67.27 70.18 62.19 76.61 67.92
particularmethodtowhichourproposedapproachisbeing RoTTA 51.65 54.96 54.57 70.15 57.95 70.93 73.91 68.38 69.38 62.91 75.20 71.08 67.60 70.65 63.50 76.74 66.22
Ours-Only 60.62 66.08 64.45 73.79 62.52 71.79 74.23 67.98 69.29 65.34 73.91 72.15 67.04 70.55 62.09 75.66 68.59
applied. Our suggested loss components, Lema and Lsrc, T TE EN NT T+Ours 5 68 0. .1 23 4 6 62 5. .5 58 6 6 61 3. .4 43 8 7 73 3. .8 92 6 6 61 2. .2 64 4 7 71 2. .6 17 6 7 73 4. .7 63 7 6 67 8. .0 29 4 6 68 9. .3 69 7 6 61 4. .8 75 2 7 74 4. .8 60 6 7 71 3. .2 07 8 6 66 7. .9 48 1 7 70 1. .1 03 1 6 61 2. .5 41 8 7 77 7. .1 13 2 6 67 8. .6 81 2
EATA 59.91 63.92 62.45 73.15 61.17 71.30 73.71 67.59 68.17 63.40 75.20 72.06 66.55 70.53 62.13 77.65 68.06
can be integrated into existing methods with respective EATA+TTAC 62.28 65.54 65.59 71.90 59.06 69.63 72.13 66.00 66.47 63.38 72.97 69.55 63.85 69.06 60.82 75.21 67.09
EATA+Ours 61.29 65.66 65.32 74.31 62.79 72.41 74.77 69.16 69.95 65.99 76.22 73.76 67.75 71.78 63.42 77.99 69.53
trade-offterms,λ andλ .Alternatively,theycanbe CoTTA 59.53 62.34 60.73 72.02 62.37 70.48 72.09 65.86 66.73 59.08 72.97 69.69 65.16 69.20 63.89 74.28 66.65
ema src CoTTA+Ours 60.22 63.06 62.35 73.23 62.37 71.40 73.85 68.84 68.51 61.79 75.03 71.93 66.07 70.68 63.43 76.62 68.09
RMT 62.70 65.69 64.74 74.54 67.16 73.98 76.05 72.87 73.40 69.66 77.42 76.11 74.24 76.23 71.79 78.25 72.18
employedindependentlyaswell,withouttheinclusionof RMT+Ours 63.21 67.33 66.86 74.81 68.47 74.30 76.11 73.56 74.07 70.87 76.94 76.42 74.79 76.47 72.93 77.58 72.79
Lunsup.Fig.2illustratestheoverallprocessofourproposed
methodandthepseudocodeofourproposedCTAscheme
issummarizedinAlg.1. Implementation Details. Since our proposed method is
compatiblewithexistingmethods,weadheretotheimple-
mentation details of each method to which our approach
5.Experiments
is applied, including the choice of optimizer and hyper-
Datasets and models. We evaluate our proposed parameters. To ensure a fair comparison, we conduct all
method on two widely used test-time adaptation bench- experimentsusingaconsistentbatchsizeof64acrossall
marks, ImageNet-C (Deng et al., 2009) and CIFAR100- methods. The entropy threshold, E 0 is set to 0.4×lnC
C(Krizhevskyetal.,2009).Bothdatasetscorruptsthetest following(Niuetal.,2022).α,λ ema andλ src areempir-
setoftheoriginaldatasetwith15differentkindsofcorrup- ically set to 0.996, 2.0 and 50 when applied on existing
tionswith5differentlevelsofseverityfromfourdifferent method.However,whenourproposedmethodisemployed
categories(noise,blur,weather,digital)(Hendrycks&Diet- independently without integration into existing methods,
terich,2019).Weconductexperimentswiththehighestlevel λ src issetto20.andweuseSGDwithalearningrateof
5. Other than these 15 corrupted target domains, we also 0.00025,momentumof0.9andupdateonlythebatchnor-
performtest-timeadaptationontheoriginalcleantestset malizationlayersasdoneinpreviousworks(Wangetal.,
asthelastdomaintovalidatehowthemodelhaspreserved 2020;Niuetal.,2022).Moreimplementationdetailsarein
performanceonthesourcedomain.WeemployResNeXt29- appendixA.
32×4dpre-trainedbyAugMix(Hendrycksetal.,2019)and
ResNet50 pre-trained by (Hendrycks et al., 2021) as the 5.1.PerformanceComparison
sourcepre-trainedmodelsforCIFAR100-CandImageNet-
ComparisonofperformanceonCTAbenchmarks.We
C, respectively. Both models are trained on the original
showtheeffectivenessofourmethodintwoways,byinte-
trainingsetofCIFAR-100andImageNet.
gratingitintoexistingmethods,andbyemployingthepro-
Evaluation. The model is initialized as the source pre- posedlosstermsindependentlywithoutLunsup(referredto
trainedweightsbeforetest-timeadaptation.Asthetest-time asOurs-Only).Specifically,weapplyourproposedterms
adaptationinitiates,batchesoftargetdatastreamintothe onfourdifferentmethods,TENT,EATA,CoTTA,andRMT,
modelsequentiallyforpredictionandadaptation.Thetarget which have demonstrated promising performance on the
domain changes when the model encounters all samples twoCTAbenchmarks.OursinTab.1and2referstousing
ofthecurrenttargetdomain,butthedomainchangeinfor- ourproposedtermsLemaandLsrctogether.Asillustrated
mation is not given to the model. We report the average inthetables,ourproposedmethodshowsnoteworthyper-
classificationaccuracyof3runsforeachdomain. formancewhenusedsolelywithoutLunsupandalsosignifi-
5MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Table3.ResultsofrandomorderofImageNet-Ctargetdomains.
Method Acc.(%) Method Acc.(%)
TTAC 41.22±0.72 EATA+TTAC 50.68±0.22
SAR 41.25±1.13 Ours-Only 45.98±0.24
TENT 14.50±1.43 TENT+Ours 44.61±0.24
EATA 49.56±0.28 EATA+Ours 50.91±0.23
CoTTA 37.73±0.09 CoTTA+Ours 46.78±0.17
RMT 44.72±0.58 RMT+Ours 45.11±0.61
Table4.AblationstudyofproposedcomponentsonImageNet-C.
Figure3.Comparisonofaverageadaptationtimeofasinglebatch
EATA Lema Lsrc Normal. Filter. Mean
acrosstargetdomainsonImageNet-C.
(cid:33) − − − − 49.81
(cid:33) (cid:33) − (cid:33) (cid:33) 50.56
(cid:33) − (cid:33) − (cid:33) 50.80
cantlyimprovesperformancewhenincorporatedintoexist-
(cid:33) (cid:33) (cid:33) − − 50.68
ingmethods.Wealsoassesstheperformanceofourmethod
(cid:33) (cid:33) (cid:33) − (cid:33) 50.95
incomparisontoTTAC(Suetal.,2022)andTSD(Wang
(cid:33) (cid:33) (cid:33) (cid:33) − 51.11
etal.,2023)whicharenotoriginallydesignedforCTAbut
(cid:33) (cid:33) (cid:33) (cid:33) (cid:33) 51.32
have been included as baseline algorithms because their
proposedideascloselyalignwiththephilosophyunderlying
ourapproach.TTACtriestoalignthedistributionsbetween
thesourceandtargetbyminimizingtheKullback-Leibler
(KL)divergence,undertheassumptionthatbothdomains isneededforaparticulardomainwhichthemodelpredicts
followaGaussiandistribution.WhileTTACsharesasimilar withhighconfidence.Ontheotherhand,Ours-Onlyshows
motivation with our Lsrc, our approach is much simpler not only consistent adaptation time across the target do-
andmoreefficient.TSDalsointroducesaconceptakinto mainsbutalsotheleastamountoftimerequired.Evenwhen
ourLema,butthereisafundamentaldifferenceinthatTSD applied on existing methods such as EATA, CoTTA, and
utilizes a memory bank to store past test inputs, whereas RMT,itincursonlyamarginaladaptationtimeoverhead.
ourmethodmaintainsclass-wisetargetprototypesviaEMA FromtheresultsofTab.1andFig.3,wedemonstratethat
which is more memory efficient. As demonstrated in the ourproposedmethodisabletoimprovetheaccuracyonly
table,ourproposedmethodconsistentlyoutperformsthem withanegligibleamountofadaptationtimeoverhead.
despitethesimilarityofideas,inthetwobenchmarks.We
Robustness to random order of target domains. Since
havealsoevaluatedperformanceofTTACappliedtoEATA,
CTAinvolvesadaptinginstantlyuponthearrivalofthetar-
EATA+TTAC.ItsperformanceonImageNet-Ciscompa-
getinputsastheyarrivesequentially,theorderinwhichthe
rabletoEATA+Ours,butitfallsslightlyshort.Moreover,
domainsarepresentedcansignificantlyimpactthemodel’s
TTACexhibitssignificantfluctuationinadaptationtimede-
performance.Theoriginaldomainsequenceconsistsofcon-
pendingonthetargetdomainwhichwillbefurtherstudied
secutive domains within the same categories (noise, blur,
inthenextsection.
weather,digital),makingiteasiertograduallyadapt.Incon-
Adaptationtimecomparison.Adaptationtimeisanim- trasttotheoriginalsequence,werandomlyshuffletheorder
portantfactortoconsiderinCTA,wherethemodelhasto ofthe15corruptedtargetdomainsofImageNet-Candplace
predictandadaptimmediatelyinanonlinemanner.There- theoriginalsourcedomainattheend.Thisrandomization
fore,wemeasuretheaveragetimeittakestoadaptabatch allowsustoevaluatetherobustnessofeachmethodtothe
foreachtargetdomainandcomparebetweenmethods.The presentationorderofthetargetdomains.Wecomputethe
experiment is conducted on a single NVIDIA RTX 3090 averageaccuracyoverthe16domainsbasedonthreesep-
GPUwithafixedbatchsizeof64forfaircomparison.Fig.3 arateruns,eachwithadistinctdomainorder.Asshownin
illustratesthecomparisonoftheaverageadaptationtimeof Table3,theresultsrevealthatcertainmethodsexhibitim-
asinglebatchbetweenmethodsacrosstargetdomainsof provedperformance,whileothersexperienceadecreasein
ImageNet-C.WhatstandsoutistheresultsofTTAC.Itsaver- performancecomparedtotheoriginaldomainsequence.No-
ageadaptationtimeofabatchexhibitssignificantvariability tably,Ours-Onlyandmethodsenhancedwithourapproach
acrossthetargetdomains.ThisisattributedtoTTAC’scal- demonstrateincreasedresiliencetovariationsintheorderof
culationofthecovariancematrixusingonlysampleswith domains,consistentlyachievingsuperiorperformancewhen
highconfidence.Itimpliesthatmorecomputationaleffort comparedtothebaselinemethods.
6MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
methodtobatchsizevariations.AspresentedinFig.4(a),it
isevidentthatEATA+OursconsistentlyoutperformsEATA
frombatchsize128to16.However,fromabatchsizeof8,
bothmethodsyieldpoorperformanceduetoanextremely
limitednumberofinputs.
Blendingfactorα.Theblendingfactorαgovernstheex-
tenttowhichthetargetprototypes,Pt,areupdatedbythe
incoming target features. A smaller α promotes quicker
updatetonewfeatures,whilealargerαresultsinamore
gradualupdateofPt,preservingthesimilaritytotheirini-
tialstates.InFig.4(b),weconductananalysisofhowthe
performancevariesinEATA+Ourswithdifferentvaluesof
α (0.9, 0.96, 0.99, 0.996, 0.999). It is evident that for all
Figure4.Analysisofbatchsize,α,λ emaandλ srconImageNet- fivevalues,EATA+Oursoutperformsthebaselinealgorithm
C. (a) presents a comparison between EATA and EATA+Ours EATA(49.81%).Theresultsclearlyindicatehighaccuracy
withvaryingbatchsizes,while(b),(c),and(d)showperformance withlargevaluesofαandlowaccuracywithsmallvalues
analysisusingdifferentα,λ emaandλ srcemployedinourmethod. ofα.ThisobservationimpliesthatexcessiveupdateofPt
Accuracy(%)istheaverageaccuracyoverthe16testdomains.
withsmallαcannegativelyimpactthemodelperformance.
Trade-offtermsλ andλ .Fig.4(c)and(d)provide
ema src
5.2.Analysis analysis of the trade-off terms, λ and λ associated
ema src
Inthefollowinganalysis,allexperimentsareconductedon
withourproposedlosscomponents,LemaandLsrcwithin
theEATA+Oursmodel.Whenwevarythevaluesofλ ,
ImageNet-CwithResNet50. ema
λ isheldconstantat50.Conversely,whenanalyzingλ ,
src src
Ablation study. In Tab. 4,we assess thevalidityof each λ issetat2.Themodelachievesitshighestaccuracy
ema
componentofourproposedmethodbygraduallyincorporat- whenλ issetto2,withadeclineinperformanceasλ
ema ema
ingthemintothebaselinealgorithm,EATA.Wereportthe increases. On the other hand, accuracy shows a gradual
meanaccuracyoverthe16testdomains.Theterm‘Normal.’ increase with rising λ values, peaking at 50. Beyond
src
inthetablereferstonormalizingω andf ϕ(x˜t )whenini- this value, accuracy does not exhibit significant changes.
tializingandupdatingPt,while‘Filter.’indicatesfiltering
Althoughtherearedifferencesinaccuracyforvariousvalues
theunreliablesampleswithhighentropy.Thesecondand of λ and λ , the gap between the highest and the
ema src
thirdrowsshowthevalidationofourproposedlossterms, lowestaccuracyisrelativelysmall.Thissuggeststhatour
as performance improves when each loss term is added. proposedlosstermsarenothighlysensitivetothechoiceof
Subsequently,thefourthtosixthrowsdemonstratethesig- trade-offvalues.
nificance of normalization and reliable sample selection.
Source-Targetdistributiongap.Weanalyzethedistribu-
Whenbothtechniquesarenotused(row4),thereisasignif-
tiongapbetweenthesourceandthetargetbymeasuringthe
icantperformancedropcomparedtothefullmodel(thelast
MSEdistancebetweenthesourceprototypesandthetarget
row).Theimportanceofnormalizationbecomesevidentas
prototypescomputedwiththeground-truth(GT)labels.Un-
itsremovalleadstoasignificantdropinperformance(row
likePt whichisgeneratedwiththepseudo-labels,Pt∗ is
5). While filtering also contributes to performance gains,
computedwiththeGTlabels,thereforerepresentsthetrue
its removal results in a minor performance drop (row 6)
centroidofeachclasscluster.Duringtest-timeadaptation,
highlightingthatourproposedmethodcanrobustlywork
westorethefeaturesproducedbyf andcomputePt∗ for
evenwithunreliablesamplespossessinghighentropy.The ϕ
modelshowsthehighestaccuracywheneverycomponent eachclass,P ct∗ = N1
t
∑N i=c 1t f ϕ(xt i)whereN ctisthenumber
c
isemployed(lastrow).Overall,theablationstudyconfirms ofsampleswithGTlabelc.Foreachtestdomain,wecom-
the effectiveness of our proposed loss terms and specific putetheaverageMSEbetweenPsandPt∗ overtheclasses,
implementationstotheperformanceimprovements. C1 ∑C c=1∣∣P cs −P ct∗ ∣∣2 2.Fig.5(a)illustratesthedistribution
gapofEATAandEATA+Ours.Thenotablylowerdistance
Batch size. While it is a well-established fact that larger
observedinEATA+OurscomparedtoEATAacrossalltest
batch sizes often result in better model performance, the
domainsindicatesthatourproposedtermscontributesignif-
TTAsettingcannotguaranteelargebatchsizeasitoperates
icantlytonarrowingthedistributiongapbetweenthesource
online and requires immediate prediction and adaptation.
andthetargetdomains.
Therefore,weconductaperformancecomparisonbetween
EATAandEATA+Oursacrosssixdifferentbatchsizes(128, Intra- and inter-class distance of target features. We
64,32,16,8,4)toevaluatetherobustnessofourproposed
7MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Figure5.Featurespacedistanceanalysis.(a)plotsthedomaingapbetweenthesourceandthetarget.(b)and(c)showtheintra-classand
theinter-classdistance,respectively,while(d)presentstheratio(intra/inter)ofthetwodistance.
Figure6.(a)showsthesimilarityanalysisofPtwithPsandPt∗
.(b),(c)and(d)illustratetheaverageentropyperdomain,confidence-
accuracydiagram,confidence-entropydiagrambetweenEATAandEATA+Ours,respectively.
analyzetheintra-classandinter-classdistancetovalidate inter-class distance compared to the intra-class distance,
how proposed method affects class-wise feature distribu- indicatinghigherclassseparability.
t
t
Nhi 1o
e
tn ∑fs e.
a
N
iI =tn
cu
1tt
r
∣r
e
∣a Po- ctc
f
∗l ea −vss
e
frd
y
ϕi (s
i
xnta
p
t
in )uc
∣t
∣e
2
2t
,ois witt
s
hh ice
co
ha rv
r
cee asr
p
na og bne edd
i
unis sgt ea
P
dnc
t
t∗e o,b
d
ve
i
c
ant lw
it
dre
a
ae t=n
e
S
c
abri um
ilc
ii
i
tl
a
ya lr
tr
oi ot ay
le
ca
cin
un ra
c
al
o
ty ems lyi ps
u
ro
et
pf
in
rP
g
est
L
enw
e
tmi tt
a
hh
.
eIP
tt
rss
us
ea
ig
cn
n
ed
nifi
tP
rc
oat i∗
n
d.
c
oeP flt
ti
hep
s
ela
i
cny las
i
sta
s s
c
howwellthefeaturesareclustered.Asmallerintra-classdis- cluster. To assess its representation as the centroid of the
tanceindicatesthatfeaturesaremoreeffectivelyclustered. classcluster,weanalyzeitscosinesimilaritywiththeproto-
Inter-classdistanceistheaveragedistancebetweenPt∗ sof typeofthesourceandthetargetdomain(PsandPt∗ )which
differentclasses,whichistojustifyhowwelltheclusters areconstructedwiththeground-truthlabels,hencethetrue
are separated, di cnter = C1 −1∑C i=11 {i≠c}∣∣P ct∗ −P it∗ ∣∣2 2. We centroid of the class cluster. As shown in Fig. 6 (a), it is
measurebothdistancesforeachclassandreporttheaverage observedthatasthetest-timeadaptationproceeds,Ptgrad-
overclassesforeachtargetdomain.InFig.5(b)and(c),we ually shows higher similarity with both Ps and Pt∗ . The
presentacomparisonbetweenEATAandEATA+Oursfor highsimilaritysuggeststhattheEMAtargetprototypes,Pt,
bothintra-classandinter-classdistances.Theintra-classdis- accuratelyrepresentstheactualcentroidoftheclassclusters.
tanceofEATA+Oursremainsconsistentlylower,whereas FurtherdiscussionaboutitcontinuesintheappendixF.
forEATA,itgraduallyincreases,leadingtoawidenedgap
Entropyandconfidenceanalysis.Fig.6(b)comparesan
betweenthetwomethodsastheadaptationprogresses.Itim-
average entropy over all samples of each target domain
pliesthatourproposedtermscontributeinminimizingintra-
betweenEATAandEATA+Ours.Wefindanintriguingob-
class variance. On the other hand, concerning inter-class
servation that the entropy of EATA+Ours is higher than
distance,EATAexhibitslargerdistancesthanEATA+Ours,
EATAdespiteitssuperioraccuracyoverEATA.Thisseems
suggesting that the class centroids are more widely dis-
counterintuitive, as entropy minimization loss is widely
persed.Nonetheless,itisnoteworthythatthegapbetween
employedfortest-timeadaptation.Toinvestigatethisphe-
the two methods remains relatively constant throughout
nomenon,weanalyzetheaccuracyandentropyaccording
the target domains when compared to the intra-class dis-
topredictionconfidence.Wedividethepredictionsinto20
tances.ItmaybetemptingtoconcludethatEATAachieves
equallyspacedbinsbasedonconfidenceandmeasurethe
amoreclass-discriminativefeaturedistributionduetoits
accuracyandentropyofeachbininFig.6(c)and(d).In
higherinter-classdistance.However,whenweexaminethe
ratiobetweenthetwodistances(di cntra /di cnter)inFig.5(d), F ali ig g. n6 s( wc) i, thth te hem ao cd ce ul ri as cw ye (wll hc ea nlib thra ete ad ccw uh rae cn yth oe fec ao cn hfid be innc ie
s
EATA+Oursconsistentlyyieldslowervalues,especiallyfor
wellalignedwiththegreydasheddiagonallineinthefigure).
latertargetdomains.Alowerratioimpliesarelativelylarger
8MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Asdepictedinthefigure,EATA+Oursappearstoberela- indeepsemi-supervisedlearning. In2020International
tivelymorewell-calibrated,exhibitingabetteralignment JointConferenceonNeuralNetworks(IJCNN),pp.1–8.
withthedashedline.Toquantitativelyestimatehowwell IEEE,2020.
themodeliscalibrated,wealsocalculateExpectedCalibra-
tionError(ECE)(Naeinietal.,2015)ofbothmodels.We Bartler, A., Bender, F., Wiewel, F., and Yang, B. Ttaps:
observethatEATA+OurspresentslowerECEthanEATA Test-time adaption by aligning prototypes using self-
andachieveshigheraccuracyinallbinsexceptthelastbin supervision. In2022InternationalJointConferenceon
withconfidencehigherthan95%.InFig.6(d),weseethat NeuralNetworks(IJCNN).IEEE,2022.
EATA+Ourspresentslowerentropyinthelowconfidence
Brahma, D. and Rai, P. A probabilistic framework for
binsandhigherentropyinhighconfidencebinscomparedto
lifelong test-time adaptation. In Proceedings of the
EATA.Also,asalreadyobservedinFig.1,proposedmethod
IEEE/CVFConferenceonComputerVisionandPattern
alleviatesthebiasinthemodeloffavoringcertainclasses
Recognition,pp.3582–3591,2023.
moreandpredictingwithoverlyhighconfidence.Overall,
proposedmethodalleviatestheover-confidentpredictions,
Chen,C.,Xie,W.,Huang,W.,Rong,Y.,Ding,X.,Huang,
inducing decrease in high confidence predictions and in-
Y.,Xu,T.,andHuang,J. Progressivefeaturealignment
crease in low-confidence predictions. It also resolves the
forunsuperviseddomainadaptation. InProceedingsof
mis-calibration ofthe modelwhich resultsin lower ECE.
theIEEE/CVFconferenceoncomputervisionandpattern
Lastly,weobservethatthemodelachieveshigheraccuracy
recognition,pp.627–636,2019.
whenitdemonstrateslowentropyonlowconfidencepredic-
tionsandhighentropyonhighconfidencepredictions.We
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
conjecturethattheproposedmethodenhancestheflexibility
simpleframeworkforcontrastivelearningofvisualrep-
ofmodelpredictionsbymitigatingthebias,consequently
resentations. In International conference on machine
aidingbettergeneralizationtotargetdata.
learning,pp.1597–1607.PMLR,2020.
6.Conclusion Choi,S.,Yang,S.,Choi,S.,andYun,S.Improvingtest-time
adaptation via shift-agnostic weight regularization and
Thispaperproposesamethodofresolvingbiasinthemodel nearestsourceprototypes,2022.
by exploiting prototypes of the source and the target do-
mainsforcontinualtest-timeadaptation.Itscompatibility Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,
withexistingmethodsmakesitasimpleyeteffectiveplug- L. Imagenet:Alarge-scalehierarchicalimagedatabase.
and-playcomponent.Thesourceprototypesareemployed In2009IEEEconferenceoncomputervisionandpattern
tominimizethedistributiongapbetweenthesourceandthe recognition,pp.248–255.Ieee,2009.
target data while the target prototypes prevent the model
frombeingovetfittedtotheincomingtargetdataandencour- Do¨bler, M., Marsden, R. A., and Yang, B. Robust mean
ageittocapturemoregeneraldistributionthatcanhandle teacherforcontinualandgradualtest-timeadaptation. In
thechangingtargetdistribution.Ourfindingsrevealthatit ProceedingsoftheIEEE/CVFConferenceonComputer
significantlyimprovestheperformanceofthemodelwith VisionandPatternRecognition,pp.7704–7714,2023.
minimaladaptationtimeoverhead.Moreover,italleviates
thebiasinthemodelbymakingthemodeltopredictless Ganin,Y.andLempitsky,V. Unsuperviseddomainadapta-
confidentandtorestrainfromfavoringcertainclassesmore. tionbybackpropagation. InInternationalConferenceon
MachineLearning(ICML),2015.
7.SocialImpacts
Gao,J.,Zhang,J.,Liu,X.,Darrell,T.,Shelhamer,E.,and
Wang,D. Backtothesource:Diffusion-drivenadaptation
Thispaperpresentsworkwhosegoalistomitigatethebias
totest-timecorruption. InProceedingsoftheIEEE/CVF
inthemodelforcontinuoustesttimeadaptation.Itcanbe
ConferenceonComputerVisionandPatternRecognition,
applied to practical settings of deep learning deployment
pp.11786–11796,2023.
andrealtimeadaptation.Therearemanypotentialsocietal
consequences of our work, none which we feel must be
Gong, T., Jeong, J., Kim, T., Kim, Y., Shin, J., and Lee,
specificallyhighlightedhere.
S.-J. Note:Robustcontinualtest-timeadaptationagainst
temporalcorrelation,2022.
References
Gong,T.,Kim,Y.,Lee,T.,Chottananurak,S.,andLee,S.-J.
Arazo, E., Ortego, D., Albert, P., O’Connor, N. E., and
Sotta:Robusttest-timeadaptationonnoisydatastreams.
McGuinness,K. Pseudo-labelingandconfirmationbias
arXivpreprintarXiv:2310.10074,2023.
9MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Grill,J.-B.,Strub,F.,Altche´,F.,Tallec,C.,Richemond,P., Kingma,D.P.andBa,J. Adam:Amethodforstochastic
Buchatskaya,E.,Doersch,C.,AvilaPires,B.,Guo,Z., optimization. arXivpreprintarXiv:1412.6980,2014.
GheshlaghiAzar,M.,etal. Bootstrapyourownlatent-a
Koh,P.W.,Sagawa,S.,Marklund,H.,Xie,S.M.,Zhang,
newapproachtoself-supervisedlearning. Advancesin
M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips,
neuralinformationprocessingsystems,33:21271–21284,
R.L.,Gao,I.,etal. Wilds:Abenchmarkofin-the-wild
2020.
distributionshifts. InInternationalConferenceonMa-
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On chineLearning,pp.5637–5664.PMLR,2021.
calibrationofmodernneuralnetworks. InInternational
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers
conferenceonmachinelearning,pp.1321–1330.PMLR,
offeaturesfromtinyimages. 2009.
2017.
Liang,J.,Hu,D.,andFeng,J. Dowereallyneedtoaccess
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- thesourcedata?sourcehypothesistransferforunsuper-
mentumcontrastforunsupervisedvisualrepresentation viseddomainadaptation. InInternationalConferenceon
learning. InProceedingsoftheIEEE/CVFconferenceon MachineLearning(ICML),2020.
computervisionandpatternrecognition,pp.9729–9738,
2020. Lim,H.,Kim,B.,Choo,J.,andChoi,S.Ttn:Adomain-shift
awarebatchnormalizationintest-timeadaptation,2023.
Hendrycks, D. and Dietterich, T. Benchmarking neural
Liu, H., Wang, J., and Long, M. Cycle self-training for
networkrobustnesstocommoncorruptionsandpertur-
domainadaptation. InAdvancesinneuralinformation
bations. ProceedingsoftheInternationalConferenceon
processingsystems,2021a.
LearningRepresentations,2019.
Liu,Y.-C.,Ma,C.-Y.,He,Z.,Kuo,C.-W.,Chen,K.,Zhang,
Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer,
P., Wu, B., Kira, Z., and Vajda, P. Unbiased teacher
J., and Lakshminarayanan, B. Augmix: A simple data
for semi-supervised object detection. arXiv preprint
processingmethodtoimproverobustnessanduncertainty.
arXiv:2102.09480,2021b.
arXivpreprintarXiv:1912.02781,2019.
Long,M.,Cao,Z.,Wang,J.,andJordan,M.I. Conditional
Hendrycks,D.,Basart,S.,Mu,N.,Kadavath,S.,Wang,F.,
adversarialdomainadaptation. Advancesinneuralinfor-
Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,
mationprocessingsystems,31,2018.
etal. Themanyfacesofrobustness:Acriticalanalysisof
out-of-distributiongeneralization. InProceedingsofthe Luo,Y.,Liu,P.,Guan,T.,Yu,J.,andYang,Y. Adversarial
IEEE/CVFInternationalConferenceonComputerVision, styleminingforone-shotunsuperviseddomainadapta-
pp.8340–8349,2021. tion,2020.
Mirza,M.J.,Soneira,P.J.,Lin,W.,Kozinski,M.,Possegger,
Hong, J., Lyu, L., Zhou, J., and Spranger, M. Mecta:
H.,andBischof,H.Actmad:Activationmatchingtoalign
Memory-economiccontinualtest-timemodeladaptation.
distributionsfortest-time-training. InProceedingsofthe
InTheEleventhInternationalConferenceonLearning
IEEE/CVFConferenceonComputerVisionandPattern
Representations,2022.
Recognition,pp.24152–24161,2023.
Iwasawa, Y. and Matsuo, Y. Test-time classifier adjust-
Naeini,M.P.,Cooper,G.,andHauskrecht,M. Obtaining
mentmoduleformodelagnosticdomaingeneralization.
wellcalibratedprobabilitiesusingbayesianbinning. In
InAdvancesinNeuralInformationProcessingSystems
ProceedingsoftheAAAIconferenceonartificialintelli-
(NeurIPS),2021.
gence,volume29,2015.
Jang,M.,Chung,S.-Y.,andChung,H.W. Test-timeadapta-
Niu, S., Wu, J., Zhang, Y., Chen, Y., Zheng, S., Zhao, P.,
tionviaself-trainingwithnearestneighborinformation.
andTan,M. Efficienttest-timemodeladaptationwithout
InInternationalConferenceonLearningRepresentations
forgetting,2022.
(ICLR),2023.
Niu,S.,Wu,J.,Zhang,Y.,Wen,Z.,Chen,Y.,Zhao,P.,and
Jung,S.,Lee,J.,Kim,N.,Shaban,A.,Boots,B.,andChoo, Tan,M. Towardsstabletest-timeadaptationindynamic
J. Cafa:Class-awarefeaturealignmentfortest-timeadap- wildworld. arXivpreprintarXiv:2302.12400,2023.
tation. arXivpreprintarXiv:2206.00205,2022.
Park,S.,Yang,S.,Choo,J.,andYun,S. Labelshiftadapter
Kim,S.,Choi,J.,Kim,T.,andKim,C. Self-trainingand fortest-timeadaptationundercovariateandlabelshifts.
adversarialbackgroundregularizationforunsupervised InProceedingsoftheIEEE/CVFInternationalConfer-
domainadaptiveone-stageobjectdetection,2019. enceonComputerVision,pp.16421–16431,2023.
10MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, Wang,D.,Shelhamer,E.,Liu,S.,Olshausen,B.,andDarrell,
J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., T. Tent:Fullytest-timeadaptationbyentropyminimiza-
Antiga,L.,Desmaison,A.,Kopf,A.,Yang,E.,DeVito, tion. InInternationalConferenceonLearningRepresen-
Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, tations(ICLR),2020.
B., Fang, L., Bai, J., and Chintala, S. Pytorch: An
Wang,Q.,Fink,O.,VanGool,L.,andDai,D.Continualtest-
imperativestyle,high-performancedeeplearninglibrary.
timedomainadaptation.InProceedingsoftheIEEE/CVF
InAdvancesinNeuralInformationProcessingSystems
ConferenceonComputerVisionandPatternRecognition,
32, pp. 8024–8035. Curran Associates, Inc., 2019.
pp.7201–7211,2022.
URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
Wang,S.,Zhang,D.,Yan,Z.,Zhang,J.,andLi,R. Feature
pdf.
alignment and uniformity for test time adaptation. In
ProceedingsoftheIEEE/CVFConferenceonComputer
Prabhudesai, M., Ke, T.-W., Li, A. C., Pathak, D., and
VisionandPatternRecognition,pp.20050–20060,2023.
Fragkiadaki, K. Diffusion-tta: Test-time adaptation of
discriminative models via generative feedback. arXiv Xu,M.,Zhang,J.,Ni,B.,Li,T.,Wang,C.,Tian,Q.,and
e-prints,pp.arXiv–2311,2023. Zhang,W. Adversarialdomainadaptationwithdomain
mixup,2020.
Schneider, S., Rusak, E., Eck, L., Bringmann, O., Bren-
del, W., and Bethge, M. Improving robustness against Yi, L., Xu, G., Xu, P., Li, J., Pu, R., Ling, C., McLeod,
commoncorruptionsbycovariateshiftadaptation. InAd- A. I., and Wang, B. When source-free domain adapta-
vancesinneuralinformationprocessingsystems,2020. tion meets learning with noisy labels. arXiv preprint
arXiv:2301.13381,2023.
Shen,J.,Qu,Y.,Zhang,W.,andYu,Y. Wassersteindistance
Yuan, L.,Xie, B., andLi, S. Robust test-timeadaptation
guided representation learning for domain adaptation,
indynamicscenarios. InProceedingsoftheIEEE/CVF
2018.
ConferenceonComputerVisionandPatternRecognition,
pp.15922–15932,2023a.
Sohn,K.,Zhang,Z.,Li,C.-L.,Zhang,H.,Lee,C.-Y.,and
Pfister,T. Asimplesemi-supervisedlearningframework
Yuan, Y., Xu, B., Hou, L., Sun, F., Shen, H., and Cheng,
forobjectdetection. arXivpreprintarXiv:2005.04757, X. Tea: Test-time energy adaptation. arXiv preprint
2020. arXiv:2311.14402,2023b.
Song,J.,Lee,J.,Kweon,I.S.,andChoi,S.Ecotta:Memory- Zhang,H.,Zhang,Y.-F.,Liu,W.,Weller,A.,Scho¨lkopf,B.,
efficientcontinualtest-timeadaptationviaself-distilled andXing,E.P. Towardsprincipleddisentanglementfor
regularization,2023. domaingeneralization. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,
Su, Y., Xu, X., and Jia, K. Revisiting realistic test-time pp.8024–8034,2022a.
training:Sequentialinferenceandadaptationbyanchored
Zhang,M.,Levine,S.,andFinn,C. Memo:Testtimero-
clustering. AdvancesinNeuralInformationProcessing
bustnessviaadaptationandaugmentation,2022b.
Systems,35:17543–17555,2022.
Zhao, B., Chen, C., and Xia, S.-T. Delta: degradation-
Sun,B.andSaenko,K. Deepcoral:Correlationalignment
free fully test-time adaptation. arXiv preprint
fordeepdomainadaptation. InECCV2016,2016.
arXiv:2301.13018,2023.
Sun,Y.,Wang,X.,Liu,Z.,Miller,J.,A.,E.A.,andHardt,M. Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired
Test-timetrainingwithself-supervisionforgeneralization image-to-imagetranslationusingcycle-consistentadver-
underdistributionshifts. InInternationalConferenceon sarialnetworks. 2020.
MachineLearning(ICML),2020.
Zou, Y., Yu, Z., Kumar, B. V., and Wang3, J. Unsuper-
Tang, H. and Jia, K. Discriminative adversarial domain viseddomainadaptationforsemanticsegmentationvia
adaptation,2020. class-balancedself-training. InEuropeanConferenceon
ComputerVision(ECCV),2018.
Tzeng,E.,Hoffman,J.,Saenko,K.,andDarrell,T. Adver-
sarialdiscriminativedomainadaptation. InProceedings
oftheIEEEconferenceoncomputervisionandpattern
recognition,pp.7167–7176,2017.
11MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
A.Implementationdetails
Here, we describe the implementation details of each method in our experiments. We use the code implemented in
MECTA(Hongetal.,2022)1forTENT(Wangetal.,2020),EATA(Niuetal.,2022),andCoTTA(Wangetal.,2022).For
othermethods,wereferencedofficialimplementationofeachmethod.WeusePyTorch(Paszkeetal.,2019)frameworkand
asingleNVIDIARTX3090GPUforconductingexperiments.
Tent. (Wang et al., 2020) We use the SGD optimizer with a learning rate of 0.0001 and a momentum of 0.9 for both
ImageNet-CandCIFAR100-Cdatasets.
T3A.(Iwasawa&Matsuo,2021)WereferencedtheofficialcodeofT3A2foritsimplementation.Sinceitisanoptimization
freemethod,thereisnoneedforanoptimizeraswellasalearningrate.Weuse100forthehyper-parameterM which
indicatestheM-thlargestentropyofthesupportset.
TSD.(Wangetal.,2023)WereferencedtheofficialcodeofTSD3 foritsimplementation.WeusetheADAM(Kingma
&Ba,2014)optimizerwithalearningrateof0.00005forbothImageNet-CandCIFAR100-Cdatasetsasmentionedin
itspaper.Weuse3forthenumberofnearestneighborsK,100fortheentropyfilterhyper-parameterM and0.1forthe
trade-offparameterλfollowingitsimplementationdetailsdescribedititspaper.
TTAC.(Suetal.,2022)WereferencedtheofficialcodeofTTAC4 foritsimplementation.Weusedtheimplementation
versionthatdoesnotusethequeuesincesavingtargetdatainqueueattest-timecostsmemoryandcomputationoverhead
whicharenotsuitableforcontinualtest-timeadaptation.WeusetheSGDoptimizerwithalearningrateof0.0002/0.00001
andmomentumof0.9forImageNet-CandCIFAR100-Cdatasets,respectively.However,whenweapplyTTAConEATA,
wefollowtheimplementationdetailsofEATAandusealearningrateof0.00025andupdateonlythebatchnormalization
layers. We use 0.9, 0.9, 1280, 64 for τ , ξ, N , N and 0.05/0.5 for the trade-off parameter of global feature
PP clip clipk
alignment,λ,inImageNet-CandCIFAR100-Cdatasets,respectively,followingitsofficialimplementation.
EATA. (Niu et al., 2022) We use the SGD optimizer with a learning rate of 0.00025 and a momentum of 0.9 for both
ImageNet-CandCIFAR100-Cdatasets.TheentropythresholdE
0
issetas0.4×lnC asmentionedearlierinthemain
paperandthethresholdforredundantsampleidentification,ϵ,issetto0.05.ThenumberofsamplesforcalculatingFisher
informationissetto2000andthetrade-offparameterforanti-forgettingloss,β,issetto2000aswellforbothdatasets.The
movingaveragefactortotracktheaveragemodelpredictionofamini-batchforredundantsampleidentificationissetto0.1
asmentionedinitsimplementationdetails.
CoTTA.(Wangetal.,2022)WeusetheSGDoptimizerwithalearningrateof0.0001andamomentumof0.9forthe
ImageNet-Cdataset,whereasweemploytheADAMoptimizerwithalearningrateof0.001forCIFAR100-C.Theconfidence
thresholdfordecidingwhethertoaugmenttheprovidedinputs,denotedasp ,isconfiguredat0.1/0.72,whiletherestore
th
probabilityforgeneratingmasksforstochasticrestoration,representedasp,isestablishedat0.001/0.01fortheImageNet-C
andCIFAR100-Cdatasets,respectively.Theexponentialmovingaveragemomentumfortheupdateoftheteachermodelis
setto0.999inbothdatasets.Originally,CoTTAusestheoutputoftheteachermodelfortheevaluation,butwhenweapply
ourproposedmethodonCoTTAweusetheoutputofthestudentfortheevaluation.Also,weusethesamelearningrateof
0.0001regardlessofthedatasetswhenapplyingourmethodonCoTTA.
RMT. (Do¨bler et al., 2023) 5 We use the SGD optimizer with a learning rate of 0.01 and a momentum of 0.9 for the
ImageNet-Cdataset,whereasweemploytheADAMoptimizerwithalearningrateof0.0001forCIFAR100-C.Thenumber
ofsamplesforwarmupissetto50,000andthetrade-offparametersforcontrastivelossandthesourcereplaylossareset
as1.Thetemperatureforcontrastivelossandtheexponentialmovingaveragemomentumforteachermodelupdateare
setto0.1and0.999,respectively.NotethatRMTisnotasource-freemethodsinceisemployssource-replaylossduring
test-time adaptation which requires source domain data even at the test-time. Other than the source replay loss, it also
employscontrastivelosswhichmakestheoveralllosstermofRMTintricate.Therefore,whenweapplyourproposedterms
onRMT,weusedifferentvaluesofλ
ema
andλ src.ForImageNet-C,weuseλ
ema
=0.5andλ
src
=0.01whileweuse
λ ema=1.0andλ src=0.01inCIFAR100-C.
1https://github.com/SonyResearch/MECTA
2https://github.com/matsuolab/T3A
3https://github.com/SakurajimaMaiii/TSD
4https://github.com/Gorilla-Lab-SCUT/TTAC
5https://github.com/mariodoebler/test-time-adaptation
12MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Table5.AblationstudyofconsistencylossonImageNet-Cusingthecorruptionlevel5.
Time t——————————————————————————————————————————————————————————————————————————→
Method Gauss. shot impulse defocus glass motion zoom snow frost fog bright. contrast elastic. pixelate jpeg original Mean
EATA 34.66 40.40 39.39 34.08 34.99 46.51 52.82 50.33 45.83 59.12 67.27 45.17 57.13 59.99 55.46 73.80 49.81
EATA+Ours 36.17 41.77 40.83 35.98 37.24 48.89 54.28 52.15 47.46 60.23 67.94 48.01 58.26 61.26 56.37 74.20 51.32
EATA+Ours+Lcons 36.66 42.33 41.41 36.25 37.57 48.91 54.04 52.58 47.65 60.34 67.94 48.39 58.22 61.36 56.56 74.27 51.53
EATA+Ours+Lcons(CoTTA-Aug) 35.15 40.30 39.50 33.92 35.83 47.38 53.06 51.20 46.62 59.54 67.26 47.12 57.48 60.49 55.77 73.72 50.27
CoTTA 16.15 18.53 19.91 18.52 19.58 31.13 43.07 36.92 36.15 51.18 65.35 23.50 47.71 52.17 44.82 73.99 37.42
CoTTA+Ours 30.06 37.51 36.72 26.86 30.65 42.34 49.64 47.53 44.15 56.65 67.13 37.73 55.98 59.81 54.68 73.17 46.91
CoTTA+Ours+Lcons 31.38 39.62 38.97 28.78 32.16 43.25 50.39 48.93 44.34 57.10 67.07 39.35 55.69 59.74 54.75 72.49 47.75
CoTTA+Ours+Lcons(CoTTA-Aug) 27.57 34.75 35.07 27.60 30.50 42.37 49.56 46.66 43.31 55.85 66.73 39.35 54.70 58.77 53.22 73.19 46.20
SAR. (Niu et al., 2023) 6 We use the SGD optimizer with a learning rate of 0.00025 and a momentum of 0.9 for both
ImageNet-CandCIFAR100-Cdatasets.
RoTTA.(Yuanetal.,2023a)7 WeusetheADAMoptimizerwithalearningrateof0.001/0.0001forCIFAR100-Cand
ImageNet-Crespectively.Forotherhyper-parameters,wefollowthedetailsdescribedinitspaper.
Weadheretothehyper-parametersasdetailedinthepaperortheofficialimplementationofeachmethod.Nevertheless,for
somemethods,wefine-tunedthelearningratetobetteralignwithourcontinualtest-timeadaptationsetting,maintaininga
fixedbatchsizeof64.
B.Consistencylosswithstrongaugmentation
Employing consistency loss between original input and its augmented version is a widely used technique in semi/self-
supervisedlearningtoimprovethegeneralizationcapacityofthemodel(Chenetal.,2020;Heetal.,2020;Grilletal.,2020;
Liuetal.,2021b;Sohnetal.,2020).SinceTTAisalsoakindofunsupervisedlearning,itadoptssuchstrategyaswell.
CoTTA(Wangetal.,2022)isthefirstTTAworktoproposetheuseofEMAteachernetworkandemployingtheconsistency
lossbetweentheoutputsoftheteacherandtheoutputsofthestudentwithvariousaugmentationsontheinputstotheteacher
network.However,wefindthatconsistencylosscanachievebetterperformancewithstrongeraugmentationstrategyand
evenwithouttheuseoftheteachernetwork.
Wedonotemploytheteachernetworkandgivetwoversionsofinput(originalandstrongaugmentedversion)tothenetwork.
InsteadofusingtheaugmentationsusedinCoTTA,weadoptsaugmentationsproposedin(Liuetal.,2021b)whichemploys
randomlyaddingcolorjittering,grayscale,Gaussianblur,andcutoutpatches.
C
Lcons(g θ,xt,A)=−∑(σ(g θ(xt ))⋅log(σ(g θ(A(xt )))))c (5)
c
Theconsistencylossisdefinedasthecross-entropylossbetweentheoutputsofthetwoinputs(originalanditsaugmented
version)predictedbythesamenetworkg
θ
whereAandσrefertotheaugmentationandthesoftmaxoperation.Lconscan
beadditionallyincorporatedwithabalancingtrade-offparameter,λ whichmakestheoverallobjectiveasfollows:
cons
Loverall =Lunsup+λ emaLema+λ srcLsrc+λ consLcons. (6)
WeapplytheconsistencylosstobothEATA+OursandCoTTA+Ourstodemonstrateitseffectiveness.Table5presentsthe
respectiveresults,clearlyindicatingthatLconscontributestoperformanceimprovement.Particularly,itsimpactismore
pronouncedwhenappliedtoCoTTA.However,whenweusetheaugmentationstrategiesproposedinCoTTAforA,denoted
asLcons(CoTTA-Aug)inthetable,theperformanceratherdeteriorates.Thisresultemphasizestheimportanceofusing
aproperaugmentationstrategyfortheconsistencyloss.Ourexperimentsuggeststhatusingstrongaugmentationsuchas
randomcutoutpatchesisindeedeffective.
C.Ablationstudyontrade-offtermsλ andλ ofOurs-Only
ema src
AsmentionedintheimplementationdetailsdescribedinSection5,whenourproposedlosstermsareusedindependently
withoutintegrationintoexistingmethods,weuseλ =2andλ =20.Table6and 7showtheablationstudyofλ and
ema src ema
6https://github.com/mr-eggplant/SAR
7https://github.com/BIT-DA/RoTTA
13MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Table6.Ablationstudyofλ onOurs-OnlyusingtheImageNet-C
ema
λ 1 2 3 4 5
ema
Acc.(%) 45.20 45.96 44.69 34.29 26.55
Table7.Ablationstudyofλ onOurs-OnlyusingtheImageNet-C
src
λ 10 20 30 40 50 60 70
src
Acc.(%) 45.58 45.96 45.86 45.65 45.29 43.44 41.01
λ srcwithdifferentvalueswhenourproposedtermsaresolelyusedwithoutLunsup.Whenexaminingtheeffectofλ ema,
λ issetat20,whereaswheninvestigatingtheimpactofλ ,λ isconfiguredto2.Theaccuracyinthetablesare
src src ema
anaverageaccuracyoverthe16testdomains.Table6illustratesthattheperformancereachesitspeakatλ ema=2,andit
experiencesasharpdeclinewhenvalueexceeds3.Similarly,Table7revealsthatsimilarperformanceismaintainedfrom10
to50,achievingover45%accuracy,butitsharplydeclineswhenvaluesurpasses50.
D.ComparisonofhardlabelandsoftlabelforL
ema
Weusethepseudo-labely˜t whencalculatingLema.Thepseudo-labelcantaketheformofaone-hotvector,servingas
ahardlabel,oritcanbeusedastherawlogitoutputofthemodel,actingasasoftlabel.Whenusingthesoft-label,we
minimizethecross-entropylossbetweentheoutputoftheEMAtargetprototypesandthesoftpseudo-label.Theoutputof
theEMAtargetprototypesreferstoalogit,z et ma∈RC,producedbydot-productingf ϕ(xt )witheveryP ctforeachclass.
Inthemainpaper,wepresentresultsusingthehardlabelrepresentation.However,todelvedeeperintothemechanism
ofLema,weconductaperformancecomparisonusingbothversionsofthepseudo-label,assummarizedinTable8.As
demonstratedinthetable,thereisnosignificantdistinctionbetweenthetwoversionsofthepseudo-label,althoughthe
hard-labelversionexhibitsslightlybetterperformance.
E.ComparisonofstudentoutputandteacheroutputofCoTTA+Ours
Asspecifiedintheimplementationdetails,CoTTAoriginallyusestheoutputoftheteachernetworkforevaluation,butwe
employtheoutputofthestudentnetworkwhenapplyingourproposedlosstermsonCoTTA.Table9presentsaperformance
comparisonbetweenCoTTA+Oursusingtheoutputoftheteacherandtheoutputofthestudent.Asdemonstratedinthe
table,usingtheteachernetwork’soutputyieldsinferiorperformancecomparedtothestudentnetwork’soutput,yetitstill
significantlyoutperformsCoTTA.Wehypothesizethatthereasonforthestudentoutput’ssuperioraccuracyisthatour
proposedlosstermsdirectlyimpactthestudentnetwork,whereastheteachernetworkundergoesslowupdatesthrough
exponentialmovingaverage.
F.SimilarityanalysisofPt withPs andPt∗.
Fig.7showstheresultsofoursimilarityanalysisofPtwithPsandPt∗
.Afterthemodelseesallthesamplesofatarget
domain,wemeasurethecosinesimilaritybetweenthePtsandthePssandthePtsandPt∗
sforthetargetdomain.We
reportthecosinesimilarityaveragedovertheclasses, C1 ∑C c=1cos(P ct,P csorP ct∗ )wherecosdenotescosinesimilarity.The
blue plot shows the similarity with the source prototypes, Ps, while the red plot shows the similarity with the target
prototypesPt∗ .NotethatPt∗
sarecomputedusingthegroundtruthlabels,sotheyrepresenttheactualcentroidsoftheclass
clustersofthetargetdomains.Asshowninthefigure,astheadaptationproceeds,thesimilaritywithboththesourceandthe
targetprototypesincrease.ItimpliesthatasPtsareslowlyupdatedinanEMAmannerwiththefeaturesofthereliable
targetsamples,theybetterrepresentthetruecentroidsoftheclassclusters.Wealsoobservethatthesimilaritywiththe
sourceprototypessmoothlyincreasesastheadaptationgoeson.Weconjecturethisisduetoourproposedsourceprototype
alignmentlossLsrc whichregulatesthefeatureextractorf
ϕ
toalignthetargetfeaturedistributiontothatofthesource.
Also,thetendencyofincreasingsimilaritywiththetargetprototypes,Pt∗ indicatesthateventhoughPtsareupdatedusing
thepseudo-labelinformation,sinceonlyreliablesamplesareemployed,theysucceedinmaximizingsimilaritywiththe
ground-truthprototypes,Pt∗
.Insummary,thisanalysisjustifiestheemploymentofoursuggestedEMAtargetprototypes.
14MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Table8.PerformancecomparisonbetweensoftlabelandhardlabelforL onImageNet-C
ema
Time t——————————————————————————————————————————————————————————————————————————→
Method Gauss. shot impulse defocus glass motion zoom snow frost fog bright. contrast elastic. pixelate jpeg original Mean
EATA 34.66 40.40 39.39 34.08 34.99 46.51 52.82 50.33 45.83 59.12 67.27 45.17 57.13 59.99 55.46 73.80 49.81
EATA+OursHardLabel 36.17 41.77 40.83 35.98 37.24 48.89 54.28 52.15 47.46 60.23 67.94 48.01 58.26 61.26 56.37 74.20 51.32
EATA+OursSoftLabel 35.89 41.60 40.80 35.72 37.30 48.82 54.33 52.07 47.42 60.28 68.04 48.05 58.35 61.29 56.34 74.31 51.29
Table9.PerformancecomparisonbetweenstudentoutputandteacheroutputofCoTTA+OursonImageNet-C
Time t——————————————————————————————————————————————————————————————————————————→
Method Gauss. shot impulse defocus glass motion zoom snow frost fog bright. contrast elastic. pixelate jpeg original Mean
CoTTA 16.15 18.53 19.91 18.52 19.58 31.13 43.07 36.92 36.15 51.18 65.35 23.50 47.71 52.17 44.82 73.99 37.42
CoTTA+OursTeacherOutput 21.75 33.04 36.38 25.07 30.68 39.15 47.03 41.41 41.80 52.41 65.50 35.47 51.56 56.23 51.52 72.38 43.84
CoTTA+OursStudentOutput 30.06 37.51 36.72 26.86 30.65 42.34 49.64 47.53 44.15 56.65 67.13 37.73 55.98 59.81 54.68 73.17 46.91
G.Predictionbiasanalysisofeachtargetdomain
InFig.1(a),wecomparedthenumberofpredictedsamplesperclassbetweenEATAandEATA+Ours,demonstrating
thatourproposedtermscontributetoamoreunbiasedpredictionofthemodel,encouragingthemodeltopredictmore
evenlyacrossclasses.SinceFig.1(a)showstheresultssummedovertheall16domains,inFig.8,webreakdownthe
resultsbyeachdomainandshowtheindividualresultofeachdomain.Itisobservedthatthedomainswhichthemodel
showshighaccuracy(brightness,original),alsoachievesamorebalancednumberofpredictedsamplesperclassacrossthe
classes.Conversely,indomainswheretheaccuracyislow,weobserveasignificantbiasinpredictions,indicatingthatthe
modeltendstofavorcertainclassesexcessivelyoverothers,makingmorefrequentpredictionsonthoseclasses.Overall,the
biasismitigatedacrossalldomainswhenourproposedtermsareincorporated.EATA+Oursdecreasespredictionsonthe
classesthatEATApredictsfrequently,instead,itincreasespredictionsontheclasseswithalownumberofpredictionsby
EATA.Indeed,thesefindingsconfirmthatoursuggestedtermseffectivelyencouragethemodeltogeneratepredictionsthat
exhibitincreaseddiversityamongdifferentclasses.Thismitigatesthebiasofthemodeltowardsfavoringcertainclassesand,
consequently,contributestoaddressingtheconfirmationbiasproblem.
H.LimitationandFutureWork
Even though our proposed EMA target prototypical loss and source distribution alignment loss indeed contribute to
significantperformanceimprovement,therearesomelimitationstoourworkthatcanbefurtherdeveloped.Thetrade-off
terms,λ andλ forourproposedlosstermsneedtobefine-tuneddependingonthespecificmethodtowhichour
ema src
proposedapproachisapplied.However,wehaveobservedthatitrequiresminimalefforttoidentifysuitablevaluesforthese
parameters,typicallyfallingwithintherangeof1to2forLemaand20to50forLsrc.Also,sincebothLemaandLsrcrely
onpseudo-labelsfortheircomputation,theycanpotentiallyresultintheincorrectcomputationbecausepseudo-labelsare
notalwaysaccurate.Toaddressthisissue,wetakemeasurestouseonlyreliablesamplesforthecomputationoftheloss
terms.However,thereisroomforimprovementinhowweleveragepseudo-labels,suchasrefiningthemtobemoreprecise
orexploringalternativeinformationsourcesforcomputingthelossterms.
Filteringoutunreliablesampleswithhigh-entropy,isindeedaneffectiveandefficientmethodtoboostperformanceand
enableefficientadaptationsinceitreducesthenumberofsamplesforadaptationbyexcludingunreliablesamples.However,
lookingatitfromadifferentperspective,ifwecanfindwaystoeffectivelyharnesstheseunreliablesamplesduringtest-time
adaptation,theyhavethepotentialtomakeasubstantialcontributiontoperformancegains,astheyrepresentchallenging
datathatcanintroducenewinsights.Disregardinghigh-entropysamplesmayinadvertentlyresultinthelossofvaluable
information.Futureresearchcouldfocusonstrategiestoleveragethepotentialofthesehigh-entropysamplesandextract
meaningfulknowledgefromthem.Welookforwardtofutureresearchendeavorsthataimtotackletheaforementioned
limitationsandexplorethesuggestedavenuesforfuturework.
15MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Figure7.CosinesimilarityanalysisofPtwithPsandPt∗
foreachtargetdomainastheadaptationproceeds.
16MitigatingtheBiasintheModelforContinualTest-TimeAdaptation
Figure8.ThecomparisonbetweenEATAandEATA+Oursonthenumberofpredictedsamplesperclassforeachtargetdomain.
17