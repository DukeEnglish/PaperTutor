High-Dimensional Tail Index Regression: with An
Application to Text Analyses of Viral Posts in Social Media
Yuya Sasaki∗ Jing Tao† Yulong Wang‡
Abstract
Motivatedbytheempiricalpowerlawofthedistributionsofcredits(e.g.,thenumberof“likes”)
of viral posts in social media, we introduce the high-dimensional tail index regression and methods
of estimation and inference for its parameters. We propose a regularized estimator, establish
its consistency, and derive its convergence rate. To conduct inference, we propose to debias the
regularizedestimate,andestablishtheasymptoticnormalityofthedebiasedestimator. Simulation
studiessupportourtheory. ThesemethodsareappliedtotextanalysesofviralpostsinX(formerly
Twitter) concerning LGBTQ+.
Keywords: high-dimensional data, social media, tail index, text analysis.
1 Introduction
A large literature is devoted to tail features of distributions – see de Haan and Ferreira (2007) and
Resnick (2007) for reviews and references. A distribution F is regularly varying with exponent α if its
tail is well approximated by a Pareto distribution with shape parameter α. This regularity condition
implies that common tail features of interest, such as tail probabilities, extreme quantiles, and tail
∗ Yuya Sasaki: yuya.sasaki@vanderbilt.edu. Brian and Charlotte Grove Chair and Professor of Economics, Depart-
ment of Economics, Vanderbilt University, VU Station B #351819, 2301 Vanderbilt Place, Nashville, TN 37235-1819.
† JingTao: jingtao@uw.edu. AssistantProfessorofEconomics,DepartmentofEconomics,UniversityofWashington,
Box 353330, Savery 305, University of Washington, Seattle, WA 98195-3330.
‡ YulongWang: ywang402@maxwell.syr.edu. AssociateProfessorofEconomics,DepartmentofEconomics,Syracuse
University, 110 Eggers Hall, Syracuse, NY 13244-1020.
1
4202
raM
2
]LM.tats[
1v81310.3042:viXraFigure 1: The log-log plot for the distribution of “likes” in posts about LGBTQ+ in X. The horizontal
axis plots the rank of Y while the vertical axis plots log(Y).
conditional expectations, can be written in terms of α. Estimates of them are constructed by plugging
in estimated values of α. The literature contains numerous suggestions along these lines, a part of
which is reviewed in the surveys cited above.
To motivate this framework in contemporary applications, consider the distribution of credits in
social media. Figure 1 draws the so-called log-log plot for the distribution of the number Y of “likes”
in LGBTQ+ posts in X (formerly Twitter). If the distribution of Y is Pareto with exponent α, then
the log-log plot is linear, as in this figure, with its slope indicating −1/α. This observation motivates
us to use the aforementioned technology for analyses of viral posts on social media.
Now, supposethattheconditionaldistributionofY givenX hasanapproximatelyParetotailwith
shape parameter α(X) depending on X. We are interested in the effect of X on the tail features of
Y via α(X). One family of existing methods imposes some parametric structure on α(X). Wang and
⊺
Tsai (2009) propose a novel tail index regression (TIR) method by modeling α(X) = exp(X θ ) and
0
estimating the pseudo parameter θ . Nicolau, Rodrigues, and Stoykov (2023) extend the TIR method
0
by allowing for weakly dependent data. Li, Leng, and You (2020) consider the semiparametric setup
⊺ ⊺
α(X) = α(X ,X ) = exp(X θ +η(X )) for some smooth function η. Combining α(X) = exp(X θ )
1 2 1 0 2 0
2with a power transformation of Y, Wang and Li (2013) study the estimation of conditional extreme
quantiles of Y given X. Another family of existing methods considers fully nonparametric models and
local smoothing (e.g., Gardes and Girard, 2010; Gardes, Guillou, and Schorgen, 2012; Daouia, Gardes,
Girard, and Lekina, 2010; Daouia, Gardes, and Girard, 2013).
CommoninalloftheseexistingapproachesisthatX isassumedtobeofafixedandlowdimension.
In this paper, we aim to relax this restriction by allowing the dimension of X to increase with the
sample size and can be much bigger than the sample size. This high-dimensional study is motivated
by our empirical question related to Figure 1. Specifically, let Y denote the number of “likes” of the
i
i-th post, and let X denote a long vector of binary indicators of whether this post contains a list
i
of keywords. Smaller values of α(X) imply that using the words indicated by such X entails more
extreme numbers of “likes.” Essentially, we are asking how to write viral posts. Since the number of
keywords is potentially very large, a high dimensional setup is necessary to this end.
To address this question, we develop a novel high-dimensional tail index regression (HDTIR)
method. Specifically,modifyingtheTIRmethod(WangandTsai,2009),weproposeanL1-regularized
maximum likelihood estimator. For inference, we further propose to debias the regularized estimate,
and establish its asymptotic normality. Two alternative methods are provided for debiased estimation
and inference: one based on sample splitting and the other based on cross-fitting.
To the best of our knowledge, the method proposed in the current paper is the first work to
systematically study estimation and inference theory for the high dimensional tail index regression
model. This is the key contribution of the current paper. The estimation and inference problem in
the high-dimensional tail regression model is related to the extensive literature on high-dimensional
generalizedlinearmodels(e.g.,vandeGeer(2008),Negahban,Yu,Wainwright,andRavikumar(2009),
Huang and Zhang (2012), van de Geer, Bu¨hlmann, Ritov, and Dezeure (2014), Zhang and Zhang
(2014), Belloni, Chernozhukov, Chetverikov, and Wei (2018), Chernozhukov, Chetverikov, Demirer,
Duflo, Hansen, Newey, and Robins (2018), Cai, Guo, and Ma (2023), among many others). However,
none of the aforementioned papers focus on tail index regression. Our work also aims to contribute
to the vast literature of text analysis. L1-regularized estimation has been extensively applied to high-
dimensional text regressions (e.g., Taddy, 2013). That said, there is no method tailored to analyzing
tail features of distributions of credits, such as the number of “likes” for viral posts in social media.
3Our proposal fills this gap in the literature.
The rest of the paper is organized as follows. Section 2 presents the method and its theory. We
extend the baseline theory to estimation and inference for conditional quantiles in Section 3. Monte
Carlo simulations in Section 4 demonstrate that the proposed HDTIR has excellent small-sample
performance. We apply the method to text analyses of viral posts in X in Section 5. Mathematical
details are relegated to the appendix.
Throughoutthepaper,weusethefollowingnotation. Forap-dimensionalvectorX = (X ,...,X )⊤
1 p
∈ Rp, we use ∥X∥ = ((cid:80)p |X |q)1/q to denote the vector ℓ norm for 1 ≤ q < ∞, and ∥X∥ =
q i=1 i q ∞
max |X | to denote the vector maximum norm. For a set S ⊆ {1,...,p}, let X = {X :
1≤i≤q i S j
j ∈ S} and Sc be the complement of S. For a p × q matrix A = (a ) ∈ Rp×q, we use ∥A∥ =
i1i2 1
(cid:80)p (cid:80)q |a |, ∥A∥ = ∥A∥ = {(cid:80)p (cid:80)q (a )2}1/2 and ∥A∥ = max |a |
i1=1 i2=1 i1,i2 2 F i1=1 i2=1 i1,i2 ∞ 1≤i1≤p,1≤i2≤q i1i2
to denote the element-wise ℓ , ℓ and ℓ , respectively. Let ∥A∥ = sup |AX| de-
1 2 ∞ ℓ d X∈Rq,|X| d≤1 d
note the matrix operator norm for 1 ≤ d ≤ ∞. More specifically, the operator ℓ , ℓ and ℓ
1 2 ∞
norms are denoted by ∥A∥ = max (cid:80)p |a |, ∥A∥ = max {(cid:80)p (a )2}1/2 and
ℓ1 1≤i2≤q i1=1 i1i2 ℓ2 1≤i2≤q i1=1 i1i2
∥A∥ = max
(cid:80)q
|a |, respectively. Let I be the p×p identity matrix. For two positive
ℓ∞ 1≤i1≤p i2=1 i1i2 p
sequences{a }and{b },a ≍ b meansthatc ≤ a /b ≤ c foralln,wherec andc aretwopositive
n n n n (cid:101)1 n n (cid:101)2 (cid:101)1 (cid:101)2
constants. For any random variables X ,...,X and functions h(·), let E {h(U )} = (cid:80)n h(U )/n
1 n n i i=1 i
be the empirical average of {h(U )}n . Let h˙(·) and h¨(·) be the first and second order derivatives of
i i=1
a univariate function, and let ∇ denote the operator for gradient or subgradient.
2 High Dimensional Tail Index Regression
2.1 Regularized Estimation
Let {(X ,Y )}n be n copies of {Y,X}, where Y is a real-valued response of interest and X is a
i i i=1
p-dimensional random vector of explanatory factors with possibly p = p → ∞ as the sample size n
n
diverges to ∞. We are interested in modeling the effect of X on the tail feature of the distribution of
Y. Without loss of generality, we focus on the right tail and collect observations Y > ω for some ω .
i n n
We state the following assumptions.
4Assumption 1 (Conditional Pareto Tail). For constant t ≥ 1,
(cid:18) (cid:12) (cid:19)
P Y > t(cid:12) (cid:12)Y > ω n,X = t−α(X), (2.1)
ω (cid:12)
n
where α(X) = exp(X⊺ θ ) and θ ∈ Rp.
0 0
Assumption 2. X is i.i.d. For each j, X has a compact support X with sup f (x) < f¯< ∞.
i ij j x∈Xj Xij
Assumption 1 imposes the restriction that the tail of the conditional distribution of Y given X is
Pareto with exponent α(X). This condition can be relaxed by multiplying a slowly varying function
L(t) on the right-hand side of (2.1) such that L(t) → 1 as t → ∞ (e.g., Wang and Tsai, 2009).
With this said, there are two benefits of imposing (2.1). First, having the exact Pareto distribution
substantially simplifies the theory, especially given that we focus on high-dimensional X . Second, the
i
empirical strategy remains the same when we select a sufficiently large ω so that the higher-order
n
approximation bias from L(t) becomes asymptotically negligible. This is also commonly implemented
in the literature (e.g., Drees (1998b,a)). More discussions about the effect of ω can be found in
n
de Haan and Ferreira (2007, Section 3), among others. Assumption 2 assumes that each coordinate of
X has a compact support. This is coherent with our empirical application, in which X is a vector of
i i
binary indicators. We can relax the i.i.d. condition at the cost of more sophisticated theory, but we
focusonthissamplingassumptiontoexplicateourmaincontributionconcerningthehigh-dimensional
setup.
We now introduce our high-dimensional tail index regression (HDTIR) estimator. Let n :=
0
(cid:80)n
I(Y ≥ ω ) denote the effective sample size, and rearrange the indices such that I(Y ≥ ω ) = 1
i=1 i n i n
for all i ∈ {1,··· ,n }. Define the negative log-likelihood function of Y conditional on Y ≥ ω by
0 n
1
(cid:88)n0
(cid:110)(cid:16) (cid:16) (cid:17) (cid:17) (cid:111)
ℓ (θ) = exp X⊤θ +1 log(Y /ω )−X⊤θ .
n0 n i i n i
0
i=1
Our regularized HDTIR estimator is given by
θ(cid:98)= argmin{ℓ n0(θ)+λ n0∥θ∥ 1}. (2.2)
θ
Let Σ = E(cid:2) X X⊤|Y > ω (cid:3) and Z = Σ−1/2 X . We denote the sparsity level of the parameter
ωn i i i n ni ωn i
as s . The following Theorem derives the consistency and the convergence rate of this regularized
0
estimator.
5Theorem 1. Suppose that Assumptions 1-2 hold, and suppose that for constants C > 0 and C > 1
1 2
independent of n, p, and ω , ∥θ∥ ≤ s , ∥θ∥ ≤ C , C−1 ≤ λ (Σ ) ≤ λ (Σ ) ≤ C . Let
n 0 0 2 1 2 min ωn max ωn 2
λ = c(cid:112) (logp)/n for some constant c > 0. If s ≲ n /(logp), then with probability approaching
n0 0 0 0
one,
(cid:115) (cid:115)
∥θ(cid:98)−θ 0∥
1
≲
s2 0(l nogp)
, ∥θ(cid:98)−θ 0∥
2
≲
s 0(l nogp)
, and
n1 (cid:88)n0 (cid:104)
X
i⊤(cid:16)
θ(cid:98)−θ
0(cid:17)(cid:105)2
≲
s 0(l nogp)
.
0 0 0 0
i=1
Theorem1establishestherateofconvergencefortheproposedlassoestimatorwithmildconditions.
The theory extends those earlier work on the generalized linear models with canonical links (e.g.,
Negahban et al. (2009), Gardes and Girard (2010)) and those focusing on generalized linear models
with binary outcome (e.g., van de Geer (2008) and Cai et al. (2023)). However, as is well discussed in
the literature, θ(cid:98)cannot be directly applied to construct a confidence interval for θ 0. In next section,
we introduce a debiased estimator for inference.
2.2 Debiased Estimation and Inference
The regularized estimator generally entails non-negligible regularization biases relative to its sampling
variations, and cannot be directly used for statistical inference based on its asymptotic distribution.
In this light, the current section proposes two approaches to debiased estimation and inference. One
approach is based on sample splitting and the other is based on cross-fitting.
Note that the score and Hessian of ℓ
n0
evaluated at θ(cid:98)take the forms of
(cid:16) (cid:17) 1
(cid:88)n0
(cid:110) (cid:16) (cid:17) (cid:111)
ℓ˙
n0
θ(cid:98) =
n
exp X i⊤θ(cid:98) log(Y i/ω n)−1 X
i
and
0
i=1
1
(cid:88)n0
(cid:16) (cid:17)
ℓ¨ n0(θ(cid:98)) =
n
log(Y i/ω n)exp X i⊤θ(cid:98) X iX i⊤,
0
i=1
respectively.
2.2.1 Sample Splitting
We split the samples so that the initial estimation and bias correction steps are conducted on inde-
pendent datasets. Without loss of generality, the effective sample of size 2n is randomly divided into
0
two disjoint subsets D
1
= {(X i,y i)}n i=0
1
and D
2
= {(X i,y i)}2 i=n0 n0+1. We use D
2
to obtain θ(cid:98)via (2.2)
and use D for the debiasing step described below.
1
6Using the subsample D , obtain
1
(cid:34)
1
(cid:88)n0
(cid:110) (cid:16) (cid:17)(cid:111)
(cid:35)
u
(cid:98)j
=arg um ∈i Rn pu⊤
n
0
log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤ u (2.3)
i=1
(cid:13) (cid:13)
(cid:13) 1
(cid:88)n0
(cid:110) (cid:16) (cid:17)(cid:111) (cid:13)
s.t. (cid:13)
(cid:13)n
log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤u−e j(cid:13)
(cid:13)
≤ γ
1n0
and (2.4)
(cid:13) 0 (cid:13)
i=1 ∞
max |X⊤u| ≤ γ , (2.5)
1≤i≤n0
i 2n0
where {e }p denotes the canonical basis of the Euclidean space Rp, and γ and γ satisfy the
j j=1 1n0 2n0
conditions stated in Assumption 3 below.
(cid:112) (cid:112)
Assumption 3. For some constants c,c′,c′′ > 0, (i) λ = c (logp)/n , (ii) γ = c′ (logp)/n
n0 0 1n0 0
√
and (iii) γ = c′′ logn .
2n0 0
For each j = 1,...,p, the debiased estimator is defined by
θ(cid:101)j := θ(cid:98)j −
nu
(cid:98)
+⊤
j
1
(cid:88)2n0 (cid:110) exp(cid:16)
X
i⊤θ(cid:98)(cid:17)
log(Y i/ω
n)−1(cid:111)
X i,
0
i=n0+1
where u
(cid:98)j
∈ Rp is the projection direction constructed by (2.3)–(2.5) using the subsample D 1, while θ(cid:98)
derives from (2.2) using the subsample D .
2
Define the variance estimator by
(cid:34)
1
(cid:88)2n0 (cid:35)
V(cid:98)1j := u (cid:98)⊤
j n
log(Y i/ω n)exp(X i⊤θ(cid:98))X iX i⊤ u
(cid:98)j
(2.6)
0
i=n0+1
We now establish the asymptotic property for this debiased estimator θ(cid:101)j.
Theorem 2. Suppose that Assumptions in Theorem 1 and Assumption 3 are satisfied. If s ≪
0
√
√n0
, then
logp logn0
√
−1/2 d
n 0V(cid:98)
1j
(θ(cid:101)j −θ 0j) → N(0,1) as n
0
→ ∞.
Following the insight from Cai et al. (2023), with sample splitting, the debiased estimator achieves
(cid:104) (cid:110) (cid:16) (cid:17)(cid:111) (cid:105)
asymptoticnormalitywithoutrequiringtheinversematrix n1
0
(cid:80)n i=0 1log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤
to be weakly sparse, which relaxes a standard assumption in the literature (see, e.g., van de Geer et al.
(2014), Javanmard and Montanari (2014) and Zhang and Zhang (2014) for linear regression models).
72.2.2 Cross Fitting
The sample splitting approach introduced in Section 2.2.1 uses only a half of the sample that we
exploit. To overcome this deficiency, we now propose a cross-fitting approach.
Take a K−fold random partition (I )K of the indices [n ] = {1,...,n } so that the size of each
k k=1 0 0
fold I is n = n /K. For each k = 1,...,K, define the set Ic = {1,...,n }\I of indices for the
k k 0 k 0 k
complement of the fold. In practice, K can be a constant as small as 2.
For each k, we estimate θ(cid:98)k via (2.2) by using the subsample of I k, and estimate u
(cid:98)j,k
via (2.3)–(2.5)
by using the subsample Ic.
k
For each j = 1,...,p, let
θ(cid:101)j,k := θ(cid:98)j,k −
u (cid:98) n⊤ j,k (cid:88)n k (cid:110) exp(cid:16)
X
i⊤θ(cid:98)k(cid:17)
log(Y i/ω
n)−1(cid:111)
X i,
k
i=1
and define the debiased estimator by
K
1 (cid:88)
θ(cid:101)j := θ(cid:101)j,k. (2.7)
K
k=1
Finally, let the variance estimator be given by
1 (cid:88)K (cid:34) 1 (cid:88)n k (cid:35)
V(cid:98)2j :=
K
u (cid:98)⊤
j,k n
log(Y i/ω n)exp(X i⊤θ(cid:98)k)X iX i⊤ u (cid:98)j,k.
k
k=1 i=1
We now establish the asymptotic property for the debiased estimator θ(cid:101)j.
Theorem 3. Suppose that Assumptions in Theorem 1 and Assumption 3 are satisfied. If s ≪
0
√
√n0
, then
logp logn0
√ (cid:16) (cid:17)
−1/2 d
n 0V(cid:98)
2j
θ(cid:101)j −θ
0j
→ N(0,1) as n
0
→ ∞.
Based on cross-fitting, Theorem 3 is analogous to Theorem 2. We make use of cross-fitting, which
provides an efficient form of data-splitting.
3 Conditional Extreme Quantiles
Our analysis thus far focuses on the pseudo-parameters defining the conditional Pareto exponent,
which underlines the tail features of the conditional distribution. We now extend our analysis to
conditional extreme quantiles.
8For τ ∈ (0,1), let Q (τ) denote the conditional τ-quantile of Y given X and Y > ω .
Y|X,Y>ωn i i i n
Assumption 1 implies
− 1
Q Y|X,Y>ωn(τ) = ω n(1−τ) α(X) (3.1)
with α(X) = exp(X⊤θ ). Plug our regularized estimator (2.2) in (3.1) to construct the estimator
0
Q(cid:98)Y|X,Y>ωn(τ) = ω
n(1−τ)−exp(−X⊤θ(cid:98)).
(3.2)
See, for example, Wang, Li, and He (2012); Wang and Li (2013) for estimators of conditional extreme
quantilesunderthecaseswithlow-andfixed-dimensionalX , andtheasymptoticdistributiontheories
i
therein. The following corollary establishes the consistency and convergence rate of this estimator.
Corollary 1. Suppose that the Assumptions in Theorem 1 are satisfied. Then, as n → ∞,
0
(cid:12) (cid:12) (cid:115)
(cid:12) (cid:12)Q(cid:98)Y|X,Y>ωn(τ) −1(cid:12)
(cid:12) ≲
s 0(logp)
.
(cid:12)Q (τ) (cid:12) n
(cid:12) Y|X,Y>ωn (cid:12) 0
Inaddition,theasymptoticnormalitycanbederivedanalogouslytoTheorems2and3. Forbrevity,
we focus on the inference theory for the debiased estimator based on the cross-fitting procedure in
Section 2.2.2, although the same idea also applies to the debiased estimator based on sample splitting
in Section 2.2.1. Define θ(cid:101)as in θ(cid:101)= (θ(cid:101)1,...,θ(cid:101)p)⊤ for θ(cid:101)j defined in (2.7). We can replace θ(cid:98)with θ(cid:101)to
establish the corollary stated below.
Let us introduce the short-handed notation
q(x⊤θ) = ω (1−τ)−exp(−x⊤θ)
n
for fixed τ. Let u = (u ,...,u )⊤ be obtained using the subsample in I , and estimate u via the
(cid:98)k (cid:98)1,k (cid:98)p,k k (cid:98)k
following procedure by using the subsample Ic:
k
(cid:34) n (cid:35)
1 (cid:88)k (cid:110) (cid:16) (cid:17)(cid:111)
u
(cid:98)k
=arg um ∈i Rn pu⊤
n
k
log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤ u
i=1
(cid:13) n (cid:13)
(cid:13) 1 (cid:88)k (cid:110) (cid:16) (cid:17)(cid:111) (cid:13)
s.t. (cid:13)
(cid:13)n
log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤u−x(cid:13)
(cid:13)
≤ γ
3n0
and (3.3)
(cid:13) k (cid:13)
i=1 ∞
max |X⊤u| ≤ γ ,
1≤i≤n0
i 4n0
9where γ and γ satisfy the conditions to be stated in Corollary 2 below. Compared to the
3n0 4n0
estimator of u , the main difference is that we replace e with x in the constraint (3.3). Let the
(cid:98)j j
variance estimator be given by
1 (cid:88)K (cid:34) 1 (cid:88)n k (cid:16) (cid:17) (cid:35)
V(cid:98)3 =
K
u (cid:98)⊤
n
log(Y i/ω n)exp X i⊤θ(cid:98)k X iX i⊤ u (cid:98).
k
k=1 i=1
The following corollary establishes the asymptotic normality for the conditional extreme quantile
estimator.
Corollary 2. Suppose that Assumptions 1 and 2 are satisfied. For universal constants c,c′ > 0,
√
(cid:112)
suppose that γ = c (logp)/n and γ = c′ logn . If ∥x∥ ≤ C < ∞. Then,
3n0 0 4n0 0 2
√ (cid:110) (cid:111)−1/2(cid:110) (cid:111)
n
0
q˙(x⊤θ(cid:98))2x⊤V(cid:98)3x q(x⊤θ(cid:101))−q(x⊤θ 0) →d N(0,1)
as n ,p → ∞.
0
Note that the conditional extreme quantile (3.1) is defined conditionally on Y > ω . Integrating
i n
out this condition, (3.1) is equivalent to Q (τ ), where
Y|X (cid:101)n
τ = 1−(1−τ)(1−F (ω )).
(cid:101)n Y|X n
By adopting some estimator F(cid:98)Y|X(ω n), we can estimate Q Y|X(τ (cid:101)n) accordingly. For instance, we can
adapt the conditional density estimators proposed by Efromovich (2010) and Izbicki and Lee (2016,
2017), all of which allow for a high-dimensional covariate vector X.
4 Simulation Studies
Inthissection,weusesimulateddatatonumericallyevaluatetheperformanceofourproposedmethod
of estimation and inference. Two designs for the p-dimensional parameter vector θ are employed:
0
1. Sparse Design: θ = (1.0,0.9,0.8,...,0.2,0.1,0.0,0.0,0.0,...)⊤, and
0
2. Exponential Design: θ = (1.0,0.5,0.52,0.53,···)⊤.
0
10A random sample of (Y ,X⊤)⊤ is generated as follows. Three designs for the p-dimensional covari-
i i
ate vector X are employed:
i
1. Gaussian Design: X ∼ N(0,0.12·I ),
i p
2. Uniform Design: X ∼ Uniform(−0.1,0.1), and
i
3. Bernoulli Design: X ∼ 0.1·Bernoulli(0.1),
i
where I denotes the p×p identity matrix. In turn, generate the exponents by
p
α = exp(X⊤θ )
i i 0
and then generate Y by
i
Y = Λ−1(U ;α ), U ∼ Uniform(0,1),
i i i i
where Λ( · ;α) denotes the CDF of the Pareto distribution with the unit scale and exponent α.
In each iteration, we draw a random sample (Y ,X⊤)⊤ of size n = 10,000. Setting the cutoff ω
i i
to the 95-th empirical percentile of {Y }n we have the effective sample size of n = 500 from five
i i=1 0
percent of N. We vary the dimension p ∈ {100,500,1000} of the parameter vector θ across sets
0
of simulations. While there are p coordinates in θ , we focus on the first coordinate θ = 1.0 for
0 01
evaluating our method of estimation and inference. Throughout, we use K = 5 for the number of
subsamplesinsamplesplitting. TheothertuningparametersaresetaccordingtoAssumption3where
c = 1, c′ = 1 and c′′ = 100. We run 10,000 Monte Carlo iterations for each design.
Table 1 summarizes the simulation results. The sets of results vary with the effective sample size
n , the dimension p of the parameter vector θ, the design for the parameter vector θ, and the design
0
for the covariate vector X. For each row, displayed are the bias (Bias) of the debiased estimator θ(cid:101)1,
standard deviations (SD), root mean square errors (RMSE), and the coverage frequencies by the 95%
confidence interval (95%).
For each set, the bias is much smaller than the standard deviations and hence the 95% confidence
interval delivers accurate coverage frequencies. We ran many other sets of simulations with different
values of n and p as well as parameter designs and data generating designs, and confirm that the
0
simulation results turned out to be similar in qualitative patterns to those presented here. The
additional results are omitted from the paper to avoid repetitive exposition.
11n p θ X Λ Bias SD RMSE 95%
0
500 100 Sparse Gaussian Pareto -0.02 0.53 0.53 0.93
500 100 Exponential Gaussian Pareto -0.01 0.48 0.48 0.94
500 500 Sparse Gaussian Pareto -0.03 0.52 0.52 0.93
500 500 Exponential Gaussian Pareto -0.01 0.47 0.47 0.95
500 1000 Sparse Gaussian Pareto -0.04 0.52 0.52 0.93
500 1000 Exponential Gaussian Pareto -0.03 0.47 0.47 0.95
500 100 Sparse Uniform Pareto 0.00 0.82 0.82 0.95
500 100 Exponential Uniform Pareto 0.00 0.79 0.79 0.95
500 500 Sparse Uniform Pareto 0.00 0.82 0.82 0.94
500 500 Exponential Uniform Pareto -0.01 0.79 0.79 0.95
500 1000 Sparse Uniform Pareto 0.00 0.82 0.82 0.94
500 1000 Exponential Uniform Pareto 0.00 0.79 0.79 0.95
500 100 Sparse Bernoulli Pareto -0.23 0.71 0.74 0.96
500 100 Exponential Bernoulli Pareto -0.09 0.82 0.83 0.96
500 500 Sparse Bernoulli Pareto -0.24 0.71 0.75 0.96
500 500 Exponential Bernoulli Pareto -0.10 0.81 0.82 0.96
500 1000 Sparse Bernoulli Pareto -0.24 0.72 0.76 0.96
500 1000 Exponential Bernoulli Pareto -0.09 0.83 0.83 0.96
Table 1: Simulation results. The sets of results vary with the dimension p of the parameter vector
θ , the design for the parameter vector θ , and the design for the covariate vector X. For each row,
0 0
displayed are the bias (Bias), standard deviations (SD), root mean square errors (RMSE), and the
coverage frequencies by the 95% confidence interval (95%).
5 Application: Text Analysis of Viral Posts about LGBTQ+
In this section, we apply our proposed method to text analyses of LGBTQ+-related posts on X
(formerly Twitter). The objective is to make an inference about the impacts of words on attracting
“likes” on posts. We use a sample of tweets scraped from Twitter using the keyword ‘LGBT’ from
August 21 to August 26 in 2022.1
A unit of observation is a post. There are n = 32,456 posts in total in our sample. The data record
the number Y of likes that the i-th post has attracted. As we shall see below, Y has a heavy-tailed
i i
1The data set is publicly available at https://www.kaggle.com/datasets/vencerlanz09/lgbt-tweets.
12Figure 2: The log-log plot for the distribution of “likes” in LGBTQ+-related posts. The horizontal
axis plots the rank of log(Y) while the vertical axis plots log(Y).
distribution with most of the posts attracting small numbers of likes while a few viral posts attracting
huge numbers of likes. We construct a word bank comprising 936,556 unique words that are used in
any of the n = 32,456 posts in our sample. The j-th coordinate X of the covariate vector X takes
ij i
the value of 1 if the j-th word in the word bank is used in the i-th post and 0 otherwise. Out of the
936,556 unique words, we only include the 500 most frequently used words to construct the binary
indicators in X . In other words, the dimension p of X is 500. This list excludes articles, be verbs,
i i
and prepositions.
Figure 2 shows the log-log plot of the empirical distribution {Y }n of the positive number of
i i=1
likes. We focus on those posts with positive numbers of likes likes as the log of zero is undefined. The
horizontal axis plots the rank of Y while the vertical axis plots log(Y). The approximate linearity of
this log-log plot implies that the distribution of Y can be characterized by the power law. In other
words, Y follows the Pareto distribution.
Table 2 shows the top 30 most frequent words used in LGBTQ+ posts. Displayed next to each
word are the total number of times it appeared (Count) and the total number of posts in which it
13Word Count Tweets Word Count Tweets Word Count Tweets
1 lgbt 23734 8133 11 #lgbt 4989 1280 21 it’s 3194 1842
2 and 18706 12357 12 this 4956 3532 22 lgbt+ 3165 578
3 i 10162 1559 13 community 4189 3640 23 their 3127 2641
4 that 9022 6931 14 have 4186 3605 24 my 2747 2003
5 you 8965 5374 15 just 3563 2900 25 don’t 2690 2195
6 people 7117 5495 16 or 3555 2903 26 what 2662 1840
7 it 7015 5025 17 so 3517 2534 27 he 2631 1548
8 not 5987 4650 18 if 3346 2207 28 your 2608 2068
9 they 5695 3705 19 all 3231 2624 29 gay 2559 1833
10 but 5012 3770 20 who 3227 2723 30 do 2559 2072
Table 2: The top 30 most frequent words used in LGBTQ+ posts. Displayed next to each word are
the total number of times it appeared (Count) and the total number of posts in which it appeared
(Tweets). All the characters are unified to lower-case letters so that the counting is not case sensitive.
appeared (Tweets). The last number corresponds to
(cid:80)n
X . All the characters are unified into
i=1 ij
lower-case letters so that the counting is not case-sensitive. Observe that the most frequent word
‘lgbt’ is distinguished from the eleventh most frequent word ‘#lgbt.’ While the former is a plain word,
the latter is known as a hashtag and plays the role of linking the posts with all the posts containing
the same hashtag.
We apply our proposed method of estimation and inference to analyze the effects of using these
and other words on the tail shape of the distribution of likes. As in the simulation studies, we set ω
n
to the 95th percentile of the empirical distribution of {Y }n , resulting in the effective sample size
i i=1
of n = 1623. The rules for the tuning parameters remain the same as those used in our simulation
0
studies.
Table 3 lists the estimates, standard errors, 95% confidence intervals, and the t statistics for θ for
j
the top 30 most frequent words. These words are listed in the same order as in Table 2. Note that
the most frequent word ‘lgbt’ has a significantly negative coefficient while the eleventh most frequent
word ‘#lgbt’ has a significantly positive coefficient. Recall that smaller values of the Pareto exponent
lead to more extreme values of Y . Thus, this observation implies that the use of the plain word ‘lgbt’
i
contributes to attracting extremely large numbers of likes, while the use of the hashtag ‘#lgbt’ can
14j Word θ(cid:101)j SE 95% CI t j Word θ(cid:101)j SE 95% CI t
1 lgbt -0.14 0.06 [-0.26 -0.03] -2.40 16 or 0.00 0.09 [-0.17 0.18] 0.04
2 and -0.01 0.05 [-0.11 0.10] -0.16 17 so 0.10 0.10 [-0.09 0.29] 1.02
3 i 0.07 0.12 [-0.17 0.31] 0.58 18 if -0.08 0.11 [-0.30 0.14] -0.73
4 that 0.04 0.06 [-0.08 0.16] 0.70 19 all 0.17 0.09 [0.00 0.35] 1.96
5 you 0.10 0.07 [-0.03 0.24] 1.49 20 who 0.10 0.08 [-0.05 0.25] 1.26
6 people 0.07 0.07 [-0.06 0.20] 1.04 21 it’s 0.22 0.10 [0.02 0.42] 2.14
7 it 0.06 0.07 [-0.08 0.21] 0.89 22 lgbt+ 0.03 0.17 [-0.31 0.37] 0.18
8 not 0.01 0.08 [-0.14 0.16] 0.10 23 their 0.06 0.09 [-0.12 0.24] 0.68
9 they 0.16 0.07 [0.02 0.30] 2.26 24 my 0.17 0.10 [-0.02 0.37] 1.75
10 but 0.02 0.08 [-0.14 0.18] 0.25 25 don’t 0.14 0.10 [-0.05 0.34] 1.43
11 #lgbt 0.41 0.15 [0.11 0.71] 2.69 26 what 0.09 0.11 [-0.13 0.31] 0.81
12 this 0.13 0.07 [-0.01 0.28] 1.82 27 he 0.04 0.09 [-0.15 0.22] 0.41
13 community -0.03 0.08 [-0.19 0.13] -0.38 28 your 0.07 0.10 [-0.13 0.27] 0.71
14 have 0.12 0.08 [-0.03 0.27] 1.60 29 gay 0.08 0.10 [-0.11 0.27] 0.82
15 just -0.04 0.09 [-0.22 0.15] -0.41 30 do 0.11 0.10 [-0.09 0.32] 1.09
Table 3: Estimates, standard errors, 95% confidence intervals, and the t statistics for θ for the top
j
30 most frequent words. These words are listed in the same order as in Table 2.
have adverse effects. Most of the other words displayed in Table 3 are statistically insignificant –
exceptions are ‘they’ and ‘it’s’ whose coefficients are positive implying their adverse effects.
We next collect the 10 most effective words and the 10 least effective words from the list of p = 500
words. Table 4 lists the estimates, standard errors, 95% confidence intervals, and the t statistics for
θ for the least and most effective words. These words are sorted in descending order in terms of the
j
absolute value of the estimate θ(cid:101)j. Once again, we continue to observe that the plain word ‘lgbt’ is the
only significantly effective word. On the other hand, hashtags including this sole effective keyword
‘lgbt’ such as ‘#lgbtqia’ and ‘#lgbtq’ tend to have negative contributions to attracting ‘likes.’
Appendix
Section A presents the proofs of main theorems. Section B presents some technical lemmas. Section
C presents the proofs of the corollaries.
1510 Most Effective Words 10 Least Effective Words
Word θ(cid:101)j SE 95% CI t Word θ(cid:101)j SE 95% CI t
lgbt -0.14 0.06 [-0.26 -0.03] -2.40 lgb 4.04 0.80 [2.48 5.61] 5.06
if -0.08 0.11 [-0.30 0.14] -0.73 ukraine 3.68 0.67 [2.36 5.00] 5.46
me -0.07 0.11 [-0.28 0.14] -0.67 377a 3.30 0.70 [1.92 4.68] 4.69
make -0.07 0.14 [-0.33 0.20] -0.49 #lgbtqia 3.01 0.69 [1.67 4.36] 4.39
just -0.04 0.09 [-0.22 0.15] -0.41 #pride 2.74 0.64 [1.48 3.99] 4.27
community -0.03 0.08 [-0.19 0.13] -0.38 let’s 2.62 0.73 [1.18 4.06] 3.57
and -0.01 0.05 [-0.11 0.10] -0.16 #lgbtq 2.60 0.54 [1.53 3.66] 4.79
also 0.00 0.14 [-0.28 0.27] -0.01 american 2.42 0.54 [1.36 3.49] 4.46
has 0.00 0.10 [-0.19 0.19] -0.01 magic 2.41 0.55 [1.34 3.49] 4.41
or 0.00 0.09 [-0.17 0.18] 0.04 x 2.33 0.45 [1.44 3.21] 5.16
Table 4: Estimates, standard errors, 95% confidence intervals, and the t statistics for θ for the top
j
30 most effective words to attract likes. These words are sorted in descending order in terms of the
estimate θ(cid:101)j.
A Proofs for the Main Theorems
A.1 Proof of Theorem 1
Proof of Theorem 1. We define three events below. Let
(cid:40) (cid:114) (cid:41)
s (logp)
E
1
= ∥θ(cid:98)−θ 0∥
2
≲ 0 ,
n
 (cid:115) 
 s2(logp)
E
2
= ∥θ(cid:98)−θ 0∥
1
≲ 0 , and
n
 0 
E
3
=(cid:40)
n1
(cid:88)n0
(cid:104)
X
i⊤(cid:16)
θ(cid:98)−θ
0(cid:17)(cid:105)2
≲
s 0(l
nogp)(cid:41)
.
0 0
i=1
We will show that E ∩E ∩E holds with probability approaching one.
1 2 3
We first verify conditions in Proposition 1 below. In our model,
1
(cid:88)n0
(cid:110) (cid:16) (cid:17) (cid:111)
ℓ˙ (θ ) = exp X⊤θ log(Y /ω )−1 X ,
n 0 n i 0 i n i
0
i=1
16and thus
(cid:12) (cid:12)
(cid:12) 1
(cid:88)n0
(cid:110) (cid:16) (cid:17) (cid:111) (cid:12)
∥ℓ˙ (θ )∥ = max (cid:12) exp X⊤θ log(Y /ω )−1 X (cid:12)
n 0 ∞ 1≤j≤p(cid:12) (cid:12)n
0
i 0 i n ij(cid:12)
(cid:12)
i=1
(cid:12) (cid:12)
(cid:12) 1
(cid:88)n0
(cid:12)
= max (cid:12) Z (cid:12)
1≤j≤p(cid:12) (cid:12)n
0
n0,i,j(cid:12)
(cid:12)
i=1
with Z defined in Lemma 1 below. From Lemma 3 below,
n0,i,j
(cid:104)(cid:110) (cid:16) (cid:17) (cid:111) (cid:105)
E exp X⊤θ log(Y /ω )−1 X |Y > ω = 0.
i 0 i n ij i n
Moreover, Lemma 1 implies that Z is sub-exponential. By the Bernstein’s inequality for sub-
n0,i,j
exponential random variables (e.g., Vershynin, 2018, Theorem 2.8.1),
 (cid:115) 
(logp)
Pr∥ℓ˙ n0(θ 0)∥ ∞ ≥ C
n
 ≤ p−c
0
for some c,C ∈ R+. Let z∗ = ∥ℓ˙ (θ )∥ and λ ≍ (cid:112) (logp)/n . Proposition 1 implies that
n0 0 ∞ n0 0
∥ℓ˙ (θ )∥ ≤ λ
n0 0 ∞ n0
with probability at least 1−p−c.
Furthermore, because
1
(cid:88)n0
(cid:16) (cid:17)
ℓ¨ (θ ) = log(Y /ω )exp X⊤θ X X⊤
n0 0 n i n i 0 i i
0
i=1
for 0 < η < 1,
(cid:68) (cid:69) (cid:90) 1(cid:68) (cid:69)
b,ℓ˙ (θ +b)−ℓ˙ (θ ) = b,ℓ¨ (θ +tb) dt
n0 0 n0 0 n0 0
0
(cid:90) 1 1 (cid:88)n0 (cid:16) (cid:17)
= log(Y /ω )exp X⊤(θ +tb) (b⊤X )2dt.
n i n i 0 i
0 0
i=1
Let ψ(b) = ψ (b) = ∥b∥ . For F(ς,S;ψ,ψ ) defined in Proposition 1, for some constant M > 0,
0 2 0
(cid:68) (cid:69)
b,ℓ˙ (θ +b)−ℓ˙ (θ ) exp(cid:0) −ψ (b)2−ψ (b)(cid:1)
n0 0 n0 0 0 0
F(ζ,S;ψ,ψ ) = inf
0
b∈C(ς,S),ψ0(b)≤1 ∥b S∥ 1∥b∥ 2
1 (cid:80)n0 (cid:82)1 log(Y /ω )exp(cid:0) X⊤(θ +tb)(cid:1) dt(b⊤X )2exp(cid:0) −∥b∥2−∥b∥ (cid:1)
= inf n0 i=1 0 i n i 0 i 2 2
b∈C(ς,S),∥b∥2=1 ∥b S∥ 1∥b∥
≥ M inf
1 (cid:88)n0 (cid:82) 01 log(Y i/ω n)exp(cid:0) X i⊤(θ 0+tb)(cid:1) dt(b⊤X i)2
b∈C(ς,S),∥b∥2=1 n 0
i=1
∥b S∥ 1∥b∥ 2
≥ Ms−1/2 inf
1
(cid:88)n0
min(cid:110)
|b⊤X |,(b⊤X
)2(cid:111)
.
0 b∈C(ς,S),∥b∥2=1 n 0
i=1
i i
17Then, following the same argument as in the proof of Proposition 3 in Cai et al. (2023), we have
F(ζ,S;ψ,ψ ) ≲ s1/2 .
0 0
Thus, Proposition 1 implies that E holds with probability approaching one. Furthermore, since
1
all the conditions in Proposition 1 are satisfied, Lemma 7 in Cai et al. (2023) implies that
(cid:115)
√ s (logp)
∥θ(cid:98)−θ 0∥
1
≤ (1+ς)∥(θ(cid:98)−θ 0) S∥
1
≤ (1+ζ) s 0∥(θ(cid:98)−θ 0) S∥
2
≲ 0
n
0
with probability at least 1−p−c and E holds with probability approaching one.
2
Finally, with Assumption 2,
1 (cid:13) (cid:13) 1 s (logp)
n
(cid:13) (cid:13)X(θ(cid:98)−θ 0)(cid:13)
(cid:13) 2
= n(θ(cid:98)−θ 0)⊤X⊤X(θ(cid:98)−θ 0) ≲ ∥θ(cid:98)−θ 0∥2
2
≲ 0
n 0
with probability at least 1 − p−c and E holds with probability approaching one. The conclusion
3
follows.
Below, we cite the auxiliary proposition from the existing literature, which we use to prove our
first main theorem.
Proposition 1 (Huang and Zhang, 2012 and Cai et al., 2023). Let θ(cid:98)= argmin θ{ℓ n(θ)+λ∥θ∥ 1} be
the Lasso estimator for some generalized linear model with true regression coefficient θ , where the
0
normalized negative log-likelihood ℓ(θ) is a convex function. Let
(cid:16) (cid:17)
b⊤ ℓ˙ n(θ 0+b)−ℓ˙ n(θ 0) e−ψ 02(b)−ψ0(b)
F(ς,S;ψ,ψ ) = inf ,
0
b∈C(ς,S),ψ0(b)≤1 ∥b S∥ 1ψ(b)
where S = {j : θ ̸= 0}, ψ and ψ are semi-norms, M > 0 is a constant, and
0j 0 2
C(ς,S) = {b ∈ Rp : ∥b ∥ ≤ ς∥b ∥ ̸= 0}.
Sc 1 S 1
Define
(cid:26) λ+z∗ λ+z∗ (cid:27)
Ω = ≤ ξ, ≤
ηe−η2−η
(λ−z∗) F(ς,S;ψ,ψ )
+ 0
for some η ≤ 1/2 and z∗ = ∥ℓ′(θ 0)∥
∞
≤ λ. Then, in the event Ω, we have ψ(θ(cid:98)−θ 0) ≤ ( Fλ+ (ςz ,S∗) ;ψeη ,ψ2+ 0)η .
18A.2 Proof of Theorem 2
Proof of Theorem 2. Applying Taylor expansion yields that for each j = 1,...,p,
θ(cid:101)j −θ
0j
=θ(cid:98)j −θ
0j
−
u
(cid:98)
n⊤
j
(cid:88)n0 (cid:110) exp(cid:16)
X
i⊤θ(cid:98)(cid:17)
log(Y i/ω
n)−1(cid:111)
X
i
0
i=1
=θ(cid:98)j −θ
0j
−
u
(cid:98)
n⊤
j
(cid:88)n0 (cid:110) exp(cid:16)
X i⊤θ
0(cid:17)
log(Y i/ω
n)−1(cid:111)
X
i
0
i=1
−
u
(cid:98)
n⊤
j
(cid:88)n0
log(Y i/ω
n)(cid:110) exp(cid:16)
X i⊤θ
0(cid:17)(cid:111)
X iX
i⊤(cid:16)
θ(cid:98)−θ
0(cid:17)
0
i=1
−
u
(cid:98)
n⊤
j
(cid:88)n0
log(Y i/ω n)X
iexp(cid:16)
X i⊤θ(cid:98)+tX
i⊤(cid:16)
θ
0−θ(cid:98)(cid:17)(cid:17) ·(cid:104)
X i⊤(θ(cid:98)−θ
0)(cid:105)2
0
i=1
=−
u (cid:98)⊤
j
(cid:88)n0 (cid:110) exp(cid:16)
X⊤θ
(cid:17)
log(Y /ω
)−1(cid:111)
X
n i 0 i n i
0
i=1
−(cid:32) u
(cid:98)
n⊤
j
(cid:88)n0
log(Y i/ω
n)(cid:110) exp(cid:16)
X i⊤θ
0(cid:17)(cid:111)
X iX i⊤−e
j(cid:33) (cid:16)
θ(cid:98)−θ
0(cid:17)
0
i=1
−
u
(cid:98)
n⊤
j
(cid:88)n0
log(Y i/ω n)X
iexp(cid:16)
X i⊤θ(cid:98)+tX
i⊤(cid:16)
θ
0−θ(cid:98)(cid:17)(cid:17) ·(cid:104)
X i⊤(θ(cid:98)−θ
0)(cid:105)2
0
i=1
for some 0 < t < 1.
Let
(cid:34) n (cid:35)
1 (cid:88) (cid:110) (cid:16) (cid:17)(cid:111)
α
(cid:98)j
= u (cid:98)⊤
j n
log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤ u
(cid:98)j
0
i=1
and
(cid:34) n (cid:35)
1 (cid:88) (cid:110) (cid:16) (cid:17)(cid:111)
α = u⊤ log(Y /ω ) exp X⊤θ X X⊤ u .
j (cid:98)j n i n i 0 i i (cid:98)j
0
i=1
Let Z := α−1/2 u⊤(cid:8) exp(cid:0) X⊤θ (cid:1) log(Y /ω )−1(cid:9) X . Note that
i j (cid:98)j i 0 i n i
(cid:16) (cid:17)
E[X E[exp X⊤θ log(Y /ω )−1)|X ,Y > ω ]|Y > ω ]
i i 0 i n i i n i n
(cid:104) (cid:16) (cid:17) (cid:105)
= E[X E exp X⊤θ log(Y /ω )−1|X ,Y > ω |Y > ω ]
i i 0 i n i i n i n
= E[X (1−1)|Y > ω ] = 0.
i i n
Thus, conditional on the subsample D defined at the begining of Section 2.2.1, Y > ω and
2 i n
{X }n0 ,{Z }n0 areindependentrandomvariableswithE[Z |X ,D ,Y > ω ] = 0and(cid:80)n0 var(Z |X ,D ,Y >
i i=1 i i=1 i i 2 i n i=1 i i 2 i
ω ) = n . The rest of the argument is similar to the proof of Theorem 3 and is omitted.
n 0
19A.3 Proof of Theorem 3
Proof of Theorem 3. Note that
θ(cid:101)j −θ
0j
K
1 (cid:88)(cid:16) (cid:17)
= θ(cid:101)j,k −θ
0j
K
k=1
=
K1 (cid:88)K (cid:40)
θ(cid:98)j,k −θ
0j
−
u (cid:98) n⊤ j,k (cid:88)n k (cid:104) exp(cid:16)
X
i⊤θ(cid:98)k(cid:17)
log(Y i/ω
n)−1(cid:105)
X
i(cid:41)
k
k=1 i=1
=−
1 (cid:88)K u (cid:98)⊤
j,k
(cid:88)n k (cid:104) exp(cid:16)
X⊤θ
(cid:17)
log(Y /ω
)−1(cid:105)
X
K n i 0 i n i
k
k=1 i=1
− K1 (cid:88)K (cid:34)(cid:32) u (cid:98) n⊤ j,k (cid:88)n k log(Y i/ω n)exp(X i⊤θ 0)X iX i⊤−e j(cid:33) (cid:16) θ(cid:98)k −θ 0(cid:17)(cid:35)
k
k=1 i=1
+ K1 (cid:88)K (cid:34) u (cid:98) n⊤ j,k (cid:88)n k log(Y i/ω n)X iexp(cid:16) X i⊤θ(cid:98)+tX i⊤(cid:16) θ 0−θ(cid:98)k(cid:17)(cid:17) ·(cid:104) X i⊤(cid:16) θ(cid:98)−θ 0(cid:17)(cid:105)2(cid:35)
k
k=1 i=1
:=I +I +I
1 2 3
for some t ∈ (0,1).
√ √
We first show that n I = o (1) and n I = o (1). For the second term I , note that
0 2 p 0 3 p 2
√
n I
0 2
≤ (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)√1
n 0
(cid:88)K (cid:34)(cid:32) u (cid:98) n⊤ j k,k (cid:88)n k log(Y i/ω n)exp(X i⊤θ 0)X iX i⊤−e j(cid:33)(cid:35)(cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
1≤m ka ≤x K(cid:13) (cid:13) (cid:13)θ 0−θ(cid:98)k(cid:13) (cid:13)
(cid:13) 1
k=1 i=1 ∞
(cid:18) (cid:19)
s (logp)
0
= O √ ,
p
n
0
√
where the first equality follows from Lemma 2, so we have n I = o (1) from the condition in the
0 2 p
theorem.
√
We next show that the term n I is o (1). Let
0 3 p
(cid:16) (cid:16) (cid:17)(cid:17) (cid:104) (cid:105)2
∆
i
= log(Y i/ω n)exp X i⊤θ(cid:98)+tX i⊤ θ 0−θ(cid:98) · X i⊤(θ(cid:98)−θ 0) .
20By Cauchy-Schwartz inequality, for each j = 1,...,p,
(cid:12) (cid:12) (cid:12)√u (cid:98)⊤
j,k
(cid:88)K (cid:88)n k
X ∆
(cid:12) (cid:12)
(cid:12) ≤ max
(cid:12)
(cid:12)u⊤ X
(cid:12) (cid:12)·(cid:12) (cid:12) (cid:12)√1 (cid:88)K (cid:88)n k
∆
(cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12) n 0
k=1 i=1
i i(cid:12) (cid:12) 1≤i≤n k(cid:12)(cid:98)j,k i(cid:12) (cid:12) (cid:12) n 0
k=1 i=1
i(cid:12) (cid:12)
≲
√γ
2 nn0
(cid:88)K (cid:88)n k (cid:104)
X
i⊤(cid:16)
θ(cid:98)k −θ
0(cid:17)(cid:105)2
≲
p
γ 2n0√s
0
nlogp
0 0
k=1 i=1
√
(cid:18) (cid:19)
s (logp) logn
0 0
= O √ = o (1),
p p
n
0
where the second inequality follows from the constraint (2.5) and Assumption 1, the third inequality
followsfrom Theorem1, thefirst equalityfollowsbecause ofAssumption3(iii)andthe secondequality
√
follows from the condition in the theorem. Hence, n I = o (1).
0 3 p
Next, we derive the asymptotic normality result for the I term. Let
1
1 (cid:88)K (cid:34) 1 (cid:88)n k (cid:35)
α
(cid:98)j
=
K
u (cid:98)⊤
j,k n
log(Y i/ω n)exp(X i⊤θ(cid:98)k)X iX i⊤ u
(cid:98)j,k
k
k=1 i=1
and
1 (cid:88)K (cid:34) 1 (cid:88)n k (cid:35)
α := u⊤ log(Y /ω )exp(X⊤θ )X X⊤ u .
j K (cid:98)j,k n i n i 0 i i (cid:98)j,k
k
k=1 i=1
Let
(cid:110) (cid:16) (cid:17) (cid:111)
Ψ := α−1/2 u⊤ exp X⊤θ log(Y /ω )−1 X .
i,k j (cid:98)j,k i 0 i n i
ForW = (X ,Y > ω ),conditionalon{W } ,u isnon-stochasticand{Ψ } areindependent
i i i n i i∈I kc (cid:98)j,k i,k i∈I k
random variables with E[Ψ |{W } ] = 0 and (cid:80)n k var(Ψ |{W } ) = n .
i,k i i∈Ic i=1 i,k i i∈Ic k
k k
We next check Lindeberg’s condition: for any constant δ > 0,
K
lim 1 (cid:88)(cid:88) E(cid:2) Ψ2 1{|Ψ |/√ n ≥ δ}|Y > ω (cid:3) = 0. (A.1)
n0→∞ n 0 i,k i,k k i n
k=1i∈I
k
Note that by Assumption 1 and Lemma 2,
(cid:12) (cid:110) (cid:16) (cid:17) (cid:111) (cid:12)
max max |Z | = max max (cid:12)α−1/2 u⊤ exp X⊤θ log(Y /ω )−1 X (cid:12)
i,k (cid:12) j (cid:98)j,k i 0 i n i(cid:12)
1≤k≤K1≤i≤n0 1≤k≤K1≤i≤n0
√
≲ max max |X⊤u | ≲ γ ≲ n
i (cid:98)j,k 2n 0
1≤k≤K1≤i≤n0
where the first inequality follows from Assumptions 1 and 2 and the second inequality follows from
(cid:112)
Assumption 3(γ = c′′ (logn )). Thus, we have the CLT.
2n0 0
Finally, note that for any θ that satisfies the conditions in this theorem, we have
21(cid:34)(cid:12)
n
(cid:12)(cid:35)
(cid:12)1 (cid:88) (cid:110) (cid:16) (cid:17) (cid:111) (cid:12)
E (cid:12) log(Y /ω ) exp X⊤θ −exp(X⊤θ ) X X⊤(cid:12)
(cid:12)n i n i i 0 i i (cid:12)
(cid:12) (cid:12)
i=1
(cid:104) (cid:12) (cid:16) (cid:17) (cid:12) (cid:105)
≤ E log(Y /ω )(cid:12)exp X⊤θ −exp(X⊤θ )(cid:12)||X ||
i n (cid:12) i i 0 (cid:12) i 2
(cid:104) (cid:16) (cid:17) (cid:105)
≤ E log(Y /ω )|X⊺ (θ−θ )|exp X⊺ θ˙ ||X ||
i n i 0 i i 2
(cid:20) (cid:21)
α(X )
≤ E i ||X ||2 supα(x)||θ−θ || ,
α(X )−1 i 2 0 2
i x
where the first inequality is due to the i.i.d. sampling, the second is by mean value theorem for
(cid:16) (cid:17)
some middle value θ˙, and the third inequality is by the Pareto tail (Assumption 1) and exp X⊺ θ˙ ≤
i
sup α(x) . Then Assumption 1 and Theorem 1 implies that α → α conditional on {W } , and
x (cid:98)j j i i∈Ic
k
p
thus the variance estimator V(cid:98)2j → V 2j. The desired conclusion follows.
B Useful Lemmas
Lemma 1. Suppose that Assumptions 1-2 hold. Define
Z = [α(X )log(Y /w )−1]X .
n0,i,j i i n ij
Then
P(|Z | > u) ≤ Cexp(−u)
n0,i,j
for some finite positive constant C.
Proof. On the event that {X = 0}, Z = 0 and hence the lemma follows trivially. Now consider
ij n0,i,j
the event {X ̸= 0}. For any u > 0,
ij
P(|Z | > u)
n0,i,j
= E[P(|Z | > u|X )]
n0,i,j i
(cid:20) (cid:18) (cid:12) (cid:19)(cid:21)
= E P α(X i)log(Y i/w n) > 1+ u (cid:12) (cid:12)X
i
|X |(cid:12)
ij
(cid:20) (cid:18) (cid:12) (cid:19)(cid:21)
+E P α(X i)log(Y i/w n) < 1− u (cid:12) (cid:12)X
i
|X |(cid:12)
ij
= P (u)+P (u).
1 2
22For P (u), Assumption 1 implies that for any x ∈ Rdim{X},
1
(cid:18) (cid:18) (cid:19)(cid:12) (cid:19)
P α(x)log(Y/w n) > 1+ u (cid:12) (cid:12)X = x = e−(1+u/|xj|),
|x | (cid:12)
j
where x denote the jth component of the vector x. Given that |X | has a bounded support from
j ij
Assumption 2, we proceed with |X | ≤ 1 without loss of generality. Let C denote a generic constant,
ij
whose value could change line-by-line. It follow that
(cid:90) 1
P (u) = e−(1+u/x)f (x)dx
1 |Xij|
0
(cid:90) 1
≤ C e−(1+u/x)dx
0
≤ Ce−u,
where the first inequality is from f < f¯ (Assumption 2), and the second inequality is by direct
Xij
calculation.
For P (u), the assumptions that ||θ|| < C (the parameter space) and X has a compact support
2 2 i,j
for all j (Assumption 2) imply that |sup α(x)| < α < ∞ for some α. Therefore, we have that
x
α(X )(log(Y /w )−1) ≥ −α(X ) ≥ −α¯ given sup α(x) ≤ α¯ from Assumption 1. Then,
i i n i x
(cid:20) (cid:18) (cid:19)(cid:21)
u
P (u) ≤ E 1 −α(X ) < 1−
2 i
|X |
ij
(cid:18) (cid:19)
u
≤ P |X | >
ij
1+α¯
≤ Ce−u,
where the last inequality is by Assumption 1 that X has a compact support and f < f¯< ∞. The
ij Xij
proof is complete by combining P (u) and P (u). Note that the both bounds hold uniformly over
1 2
n .
0
Lemma 2. Suppose that the conditions of Theorem 2 hold. With probability at least 1−p−c −n−c,
0
there exists u such that for each j = 1,...,p and k = 1,...,K,
(cid:98)j
(cid:13) (cid:110) (cid:16) (cid:17)(cid:111) (cid:13)
(cid:13) (cid:13)u (cid:98)⊤ j,klog(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤−e j(cid:13)
(cid:13)
≤ γ
1n0
∞
max |X⊤u | ≤ γ .
1≤i≤n0
i (cid:98)j,k 2n0
23Proof. To show that u exists with high probability, we verify that conditional on {W } ,
(cid:98)j i i∈Ic
k
(i) the matrix
(cid:104) (cid:110) (cid:16) (cid:17)(cid:111) (cid:105)
Σ(cid:101)ωn := E log(Y i/ω n) exp X i⊤θ(cid:98) X iX i⊤|Y
i
> ω
n
is invertible;
(ii) the j-th column u
(cid:101)j,k
of Σ(cid:101)− ωn1, j = 1,...,p, is feasible for the constraint in (2.3) with high
probability.
To verify (i), we first show that for any unit vector a ∈ Rp and some constant ϵ > 0,
(cid:110) (cid:16) (cid:17)(cid:111)
E[log(Y i/ω n) exp X i⊤θ(cid:98) (a⊤X i)2|Y
i
> ω n] > ϵ
uniformly over a ∈ Rp.
Because 0 < c ≤ E(cid:2) (a⊤X )2(cid:3) by Assumption 2, and
0 i
(cid:104) (cid:110)(cid:12) (cid:12) (cid:111)(cid:105) (cid:113) (cid:113)
E (a⊤X )2−(a⊤X )21 (cid:12)a⊤X (cid:12) < T ≤ E[(a⊤X )4] Pr(|a⊤X | > T) ≤ cexp(−c′T2)
i i (cid:12) i(cid:12) i i
for positive constants c,c′, if we choose T sufficiently large, we have
(cid:104) (cid:110)(cid:12) (cid:12) (cid:111)(cid:105)
E (a⊤X )2−(a⊤X )21 (cid:12)a⊤X (cid:12) < T ≲ c /3.
i i (cid:12) i(cid:12) 0
Thus, if we choose ϵ ≲ c /9, it holds that E(cid:2) log(Y /ω )(cid:8) exp(cid:0) X⊤θ (cid:1)(cid:9) (a⊤X )2|Y > ω (cid:3) ≳ c /3 and
0 i n i 0 i i n 0
a⊤Σ a > 0 (B.1)
ωn
for any unit vector a ∈ Rp.
Furthermore, by Assumption 1, with probability at least 1−p−c,
(cid:12) (cid:104) (cid:110) (cid:16) (cid:17) (cid:16) (cid:17)(cid:111) (cid:105)(cid:12)
(cid:12) (cid:12)E log(Y i/ω n) exp X i⊤θ(cid:98) −exp X i⊤θ
0
(a⊤X i)2|Y
i
> ω
n
(cid:12)
(cid:12)
(cid:104) (cid:16) (cid:17) (cid:105)
≤ E log(Y i/ω n)(a⊤X i)2|X i⊺ (θ(cid:98)−θ 0)|exp X i⊺ θ˙ |Y
i
> ω
n
(cid:20) (cid:21)
≤ E α(α X(X )i −) 1(a⊤X i)2|X i⊺ (θ(cid:98)−θ 0)| supα(x)
i x
α2 s logp
0
≤ X , (B.2)
α−1 n
0
where the first inequality follows from the mean value theorem, the second is by the Pareto tail
(cid:16) (cid:17)
assumption (Assumption 1) and that exp X⊺ θ˙ ≤ sup (x⊺ θ) (the definition of our parameter space
i x
24Ω(s )), and the third inequality is by Assumption 2 and Theorem 1, where X denotes the upper
0
bound of the support of X for all component j, which is finite. Therefore, (B.1) and (B.2) imply
ij
that Σ(cid:101)ωn is invertible.
Toverify(ii),notethatconditionalon{W } ,u isnon-stochastic. Therefore,withthefeasible
i i∈Ic (cid:98)j,k
k
(cid:110) (cid:16) (cid:17)(cid:111)
solution Σ(cid:101)− ωn1 for (2.3), the terms log(Y i/ω n) exp X i⊤θ(cid:98) u (cid:101)⊤ j,kX iX i⊤e j,i = 1,...,n are independent
subexponential random variables because of Assumption 1 and Lemma 1. Then,
(cid:13) (cid:13)(cid:88)K (cid:88)n k (cid:110) (cid:16) (cid:17)(cid:111) (cid:13) (cid:13) (cid:115) (logp)
(cid:13)
(cid:13)
u (cid:101)⊤ j,klog(Y i/ω n) exp X i⊤θ(cid:98)k X iX i⊤−e j(cid:13)
(cid:13)
≲
n
≲ γ
1n0
(cid:13) (cid:13) 0
k=1 i=1 ∞
(cid:112)
for γ ≍ (logp)/n with probability at least 1−p−c.
1n0 0
Moreover, since E[(cid:0) a⊤X(cid:1)2 ] ≥ c > 0 due to Assumption 2, with probability at least 1 − n−c,
0 0
max |X⊤u | ≤ λ , which proves (ii). Thus, the conclusion follows from (i) and (ii).
1≤i≤n0 i (cid:101)j,k 2n0
Lemma 3. Suppose that the conditions in Theorem 1 hold. Then,
1
(i) E[log(Y /ω )|X ,Y > ω ] = , (B.3)
i n i t n exp(cid:0) X⊤θ (cid:1)
i 0
(cid:104)(cid:110) (cid:16) (cid:17) (cid:111) (cid:105)
(ii) E exp X⊤θ log(Y /ω )−1 X |Y > ω = 0, (B.4)
i 0 i n i i n
(cid:20) (cid:21)
(cid:110) (cid:16) (cid:17) (cid:111)2 (cid:104) (cid:105)
(iii) E X X⊤ exp X⊤θ log(Y /ω )−1 |Y > ω = E X X⊤|Y > ω , (B.5)
i i i 0 i n i n i i i n
(cid:104) (cid:16) (cid:17) (cid:105)
(iv) E X X⊤exp X⊤θ log(Y /ω )|Y > ω = Σ . (B.6)
i i i 0 i n i n ωn
Proof. Part (i) follows because, conditional on X and ω , the random variable log(Y /ω ) is expo-
i n i n
nentially distributed with shape parameter α(X ) = exp(cid:0) X⊤θ (cid:1) .
i i 0
Part (ii): It follows because
(cid:104) (cid:104)(cid:110) (cid:16) (cid:17) (cid:111) (cid:105) (cid:105)
E E exp X⊤θ log(Y /ω )−1 X |X ,Y > ω |Y > ω
i 0 i n i i i n i n
(cid:16) (cid:17)
=E[X exp X⊤θ E[log(Y /ω )−1|X ,Y > ω ]|Y > ω ]
i i 0 i n i i n i n
=E[X (1−1)|Y > ω ] = 0.
i i n
25Part (iii): This is because
(cid:20) (cid:21)
(cid:110) (cid:16) (cid:17) (cid:111)2
E exp X⊤θ log(Y /ω )−1 |X ,Y > ω
i 0 i n i i n
(cid:20) (cid:21)
(cid:16) (cid:16) (cid:17) (cid:17)2 (cid:16) (cid:17)
=E exp X⊤θ log(Y /ω ) −2exp X⊤θ log(Y /ω )+1|X ,Y > ω ,
i 0 i n i 0 i n i i n
where
(cid:20) (cid:21)
(cid:16) (cid:16) (cid:17) (cid:17)2
E exp X⊤θ log(Y /ω ) |X ,Y > ω
i 0 i n i i n
(cid:16) (cid:16) (cid:17)(cid:17)2 (cid:104) (cid:105)
= exp X⊤θ E (log(Y /ω ))2|X ,Y > ω
i 0 i n i i n
=(cid:16) exp(cid:16)
X⊤θ
(cid:17)(cid:17)2(cid:0)E[log(Y
/ω )|X ,Y > ω ]2+Var(log(Y /ω )|X ,Y > ω )(cid:1)
i 0 i n i i n i n i i n
(cid:32) (cid:33)
(cid:16) (cid:16) (cid:17)(cid:17)2 1 1
= exp X⊤θ × + = 2.
i 0 (cid:0) exp(cid:0)
X⊤θ
(cid:1)(cid:1)2 (cid:0) exp(cid:0)
X⊤θ
(cid:1)(cid:1)2
i 0 i 0
Then,
(cid:20) (cid:21)
(cid:110) (cid:16) (cid:17) (cid:111)2 (cid:104) (cid:105)
E X X⊤ exp X⊤θ log(Y /ω )−1 |Y > ω =E X X⊤(2−2+1)|Y > ω
i i i 0 i n i n i i i n
(cid:104) (cid:105)
=E X X⊤|Y > ω .
i i i n
Part (iv): It follows because
(cid:104) (cid:16) (cid:17) (cid:105)
E X X⊤exp X⊤θ E[log(Y /ω )|X ,Y > ω ]|Y > ω
i i i 0 i n i i n i n
(cid:20) exp(X⊤θ ) (cid:21)
=E X X⊤ i 0 |Y > ω = Σ .
i i exp(X⊤θ ) i n ωn
i 0
This completes proofs of all the four parts of the lemma.
C Proofs of Corollaries
C.1 Proof of Corollary 1
Proof of Corollary 1. Theorem 1 and the continuous mapping theorem imply that for any x satisfying
that ||x|| < ∞,
2
(cid:115)
(cid:12) (cid:12) s (logp)
(cid:12) (cid:12)exp(−x⊺ θ(cid:98))−exp(−x⊺ θ 0)(cid:12)
(cid:12)
≲ 0
n
. (C.1)
0
26Consider the following argument. Conditional on X = x,
(cid:12) (cid:12)
(cid:12) (cid:12)Q(cid:98)Y|X=x,Y>ωn(τ) −1(cid:12)
(cid:12)
(cid:12)Q (τ) (cid:12)
(cid:12) Y|X=x,Y>ωn (cid:12)
(cid:12) (cid:12)
( =1) (cid:12)(1−τ)−exp(−x⊺θ(cid:98))+exp(−x⊺θ0)−1(cid:12)
(cid:12) (cid:12)
(2) (cid:12) (cid:12)
≤ (1−τ)−ϵ|log(1−τ)|(cid:12) (cid:12)exp(−x⊺ θ(cid:98))−exp(−x⊺ θ 0)(cid:12)
(cid:12)
(cid:115)
(3) s (logp)
≲ 0 ,
n
0
where(1)isbythedefinitionofQ(cid:98)Y|X=x,Y>ωn(τ)andAssumption1; (2)isbythemeanvalueexpansion
⊺ ⊺
with ϵ between x θ(cid:98)and x θ 0; and (3) is by (C.1) and the fact that τ ∈ (0,1).
C.2 Proof of Corollary 2
Proof of Corollary 2. Let
(cid:104) (cid:16) (cid:17) (cid:105)
V = E log(Y /ω )exp X⊤θ X X⊤ .
i n i 0 i i
Following the proof of Theorems 3, it can be similarly shown that
√ (cid:16) (cid:17) √ (cid:110) (cid:16) (cid:17) (cid:111)
nx⊤ θ(cid:101)−θ
0
= nu (cid:98)⊤ exp X i⊤θ
0
log(Y i/ω n)−1 X i+o p(1) (C.2)
(cid:16) (cid:17)
and x⊤ V(cid:98)3−V
3
x → 0 as n 0,p → ∞. Since ∥x∥
2
< C, u (cid:98)⊤(cid:8) exp(cid:0) X i⊤θ 0(cid:1) log(Y i/ω n)−1(cid:9) X
i
are
√
sub-Gaussian, which implies that nx⊤(θ(cid:101)−θ 0)(x⊤Vx)−1/2 → N(0,1) in distribution as n 0,p → ∞.
For the extreme quantile parameter q(x⊤θ ), by Taylor expansion, we have
0
(cid:16) (cid:17) (cid:110) (cid:16) (cid:17)(cid:111)2
q(x⊤θ(cid:101)) = q(x⊤θ 0)+q˙(x⊤θ 0)x⊤ θ(cid:101)−θ
0
+q¨(x⊤θ ∗) x⊤ θ(cid:101)−θ
0
/2
where θ
∗
is between θ
0
and the debiased estimator θ(cid:101). Since x⊤(θ(cid:101)−θ 0) = O p(n−1/2) from (C.2), we
have
√ (cid:110) (cid:111) √ (cid:16) (cid:17)
n q(x⊤θ(cid:101))−q(x⊤θ 0) = nq˙(x⊤θ 0)x⊤ θ(cid:101)−θ
0
+o p(1).
Note that Theorem 1 implies that q˙(x⊤θ(cid:98)) →p q˙(x⊤θ 0) due to ∥θ(cid:98)−θ 0∥
1
→p 0. Hence, q˙(x⊤θ(cid:98))2x⊤V(cid:98)3x →p
q˙(x⊤θ )2x⊤Vx. The asymptotic normality in Corollary 2 follows.
0
27References
Belloni, A., V. Chernozhukov, D. Chetverikov, and Y. Wei (2018): “Uniformly valid post-
regularizationconfidenceregionsformanyfunctionalparametersinz-estimationframework,”Annals
of statistics, 46, 3643.
Cai, T. T., Z. Guo, and R. Ma(2023): “Statisticalinferenceforhigh-dimensionalgeneralizedlinear
models with binary outcomes,” Journal of the American Statistical Association, 118, 1319–1332.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and
J. Robins (2018): “Double/debiased machine learning for treatment and structural parameters,”
Econometrics Journal, 21, 1–68.
Daouia, A., L. Gardes, and S. Girard (2013): “On kernel smoothing for extremal quantile
regression,” Bernoulli, 19, 2557–2589.
Daouia, A., L. Gardes, S. Girard, and A. Lekina (2010): “Kernel estimators of extreme level
curves,” Test, 20, 311–333.
de Haan, L. and A. Ferreira (2007): Extreme Value Theory: An Introduction, Springer Science
and Business Media, New York.
Drees, H. (1998a): “A general class of estimators of the extreme value index,” Journal of Statistical
Planning and Inference, 66, 95–112.
——— (1998b): “On smooth statistical tail functionals,” Scandinavian Journal of Statistics, 25, 187–
210.
Efromovich, S. (2010): “Dimension reduction and adaptation in conditional density estimation,”
Journal of the American Statistical Association, 105, 761–774.
Gardes, L. and S. Girard (2010): “Conditional extremes from heavy-tailed distributions: An
application to the estimation of extreme rain fall return levels,” Extremes, 13, 177–204.
28Gardes, L., A. Guillou, and A. Schorgen (2012): “Estimating the conditional tail index by
integrating a kernel conditional quantile estimator,” Journal of Statistical Planning and Inference,
142, 1586–1598.
Huang, J. and C.-H. Zhang (2012): “Estimation and selection via absolute penalized convex
minimization and its multistage adaptive applications,” Journal of Machine Learning Research, 13,
1839–1864.
Izbicki, R. and A. B. Lee (2016): “Nonparametric conditional density estimation in a high-
dimensional regression setting,” Journal of Computational and Graphical Statistics, 25.
——— (2017): “Converting high-dimensional regression to high-dimensional conditional density esti-
mation,” Electronic Journal of Statistics, 11, 2800–2831.
Javanmard, A. and A. Montanari (2014): “Confidence intervals and hypothesis testing for high-
dimensional regression,” Journal of Machine Learning Research, 15, 2869–2909.
Li, R., C. Leng, and J. You (2020): “Semiparametric Tail Index Regression,” Journal of Business
& Economic Statistics, 40, 82–95.
Negahban, S., B. Yu, M. J. Wainwright, and P. Ravikumar (2009): “A unified framework
for high-dimensional analysis of m-estimators with decomposable regularizers,” Advances in neural
information processing systems, 22.
Nicolau, J., P. M. Rodrigues, and M. Z. Stoykov(2023): “Tailindexestimationinthepresence
of covariates: Stock returns’ tail risk dynamics,” Journal of Econometrics, 235, 2266–2284.
Resnick, S. (2007): Heavy-tail phenomena: probabilistic and statistical modeling, Springer Science &
Business Media.
Taddy, M. (2013): “Multinomial inverse regression for text analysis,” Journal of the American
Statistical Association, 108, 755–770.
van de Geer, S., P. Bu¨hlmann, Y. Ritov, and R. Dezeure(2014): “Onasymptoticallyoptimal
confidence regions and tests for high-dimensional models,” Annals of Statistics, 42, 1166 – 1202.
29van de Geer, S. A. (2008): “High-dimensional generalized linear models and the lasso,” Annals of
Statistics, 36, 614.
Vershynin, R. (2018): High-dimensional probability: An introduction with applications in data sci-
ence, vol. 47, Cambridge university press.
Wang, H. and C.-L. Tsai (2009): “Tail index regression,” Journal of the American Statistical
Association, 104, 1233–1240.
Wang, H. J. and D. Li (2013): “Estimation of Extreme Conditional Quantiles Through Power
Transformation,” Journal of the American Statistical Association, 108, 1062–1074.
Wang, H. J., D. Li, and X. He(2012): “EstimationofHighConditionalQuantilesforHeavy-Tailed
Distributions,” Journal of the American Statistical Association, 107, 1453–1464.
Zhang, C.-H. and S. S. Zhang (2014): “Confidence intervals for low dimensional parameters
in high dimensional linear models,” Journal of the Royal Statistical Society: Series B: Statistical
Methodology, 217–242.
30