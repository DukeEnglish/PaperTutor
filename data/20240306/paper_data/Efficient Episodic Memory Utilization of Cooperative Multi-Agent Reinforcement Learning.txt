PublishedasaconferencepaperatICLR2024
EFFICIENT EPISODIC MEMORY UTILIZATION OF COOP-
ERATIVE MULTI-AGENT REINFORCEMENT LEARNING
HyunghoNa,YunkyeongSeo&Il-ChulMoon
KoreaAdvancedInstituteofScienceandTechnology(KAIST),Daejeon34141,SouthKorea
{gudgh723}@gmail.com,{tjdbsrud,icmoon}@kaist.ac.kr
ABSTRACT
Incooperativemulti-agentreinforcementlearning(MARL),agentsaimtoachieve
acommongoal,suchasdefeatingenemiesorscoringagoal. ExistingMARLalgo-
rithmsareeffectivebutstillrequiresignificantlearningtimeandoftengettrapped
inlocaloptimabycomplextasks,subsequentlyfailingtodiscoveragoal-reaching
policy. Toaddressthis,weintroduceEfficientepisodicMemoryUtilization(EMU)
forMARL,withtwoprimaryobjectives: (a)acceleratingreinforcementlearning
byleveragingsemanticallycoherentmemoryfromanepisodicbufferand(b)selec-
tivelypromotingdesirabletransitionstopreventlocalconvergence. Toachieve(a),
EMUincorporatesatrainableencoder/decoderstructurealongsideMARL,creating
coherentmemoryembeddingsthatfacilitateexploratorymemoryrecall. Toachieve
(b),EMUintroducesanovelrewardstructurecalledepisodicincentivebasedon
thedesirabilityofstates. ThisrewardimprovestheTDtargetinQ-learningandacts
asanadditionalincentivefordesirabletransitions. Weprovidetheoreticalsupport
fortheproposedincentiveanddemonstratetheeffectivenessofEMUcomparedto
conventionalepisodiccontrol. TheproposedmethodisevaluatedinStarCraftII
andGoogleResearchFootball,andempiricalresultsindicatefurtherperformance
improvementoverstate-of-the-artmethods.
1 INTRODUCTION
Recently,cooperativeMARLhasbeenadoptedtomanyapplications,includingtrafficcontrol(Wier-
ingetal.,2000),resourceallocation(Dandanovetal.,2017),robotpathplanning(Wangetal.,2020a),
andproductionsystems (Dittrich&Fohlmeister,2020),etc. Inspiteofthesesuccessfulapplications,
cooperative MARL still faces challenges in learning proper coordination among multiple agents
becauseofthepartialobservabilityandtheinteractionbetweenagentsduringtraining.
To address these challenges, the framework of centralized training and decentralized execution
(CTDE)(Oliehoeketal.,2008;Oliehoek&Amato,2016;Guptaetal.,2017)hasbeenproposed.
CTDEenablesadecentralizedexecutionwhilefullyutilizingglobalinformationduringcentralized
training, so CTDE improves policy learning by accessing to global states at the training phase.
Especially,valuefactorizationapproaches(Sunehagetal.,2017;Rashidetal.,2018;Sonetal.,2019;
Yangetal.,2020;Rashidetal.,2020;Wangetal.,2020b)maintaintheconsistencybetweenindividual
andjointactionselection,achievingthestate-of-the-artperformanceondifficultmulti-agenttasks,
suchasStarCraftIIMulti-agentChallenge(SMAC)(Samvelyanetal.,2019). However,learning
optimalpolicyinMARLstillrequiresalongconvergencetimeduetotheinteractionbetweenagents,
andthetrainedmodelsoftenfallintolocaloptima,particularlywhenagentsperformcomplextasks
(Mahajanetal.,2019). Hence,researcherspresentacommittedexplorationmechanismunderthis
CTDEtrainingpractice(Mahajanetal.,2019;Yangetal.,2019;Wangetal.,2019;Liuetal.,2021)
withtheexpectationtofindepisodesescapingfromthelocaloptima.
DespitetherequiredexplorationinMARLwithCTDE,recentworksonepisodiccontrolemphasize
theexploitationofepisodicmemorytoexpeditereinforcementlearning. Episodiccontrol(Lengyel
& Dayan, 2007; Blundell et al., 2016; Lin et al., 2018; Pritzel et al., 2017) memorizes explored
states and their best returns from experience in the episodic memory, to converge on the best
policy. Recently, thisepisodiccontrolhasbeenadoptedtoMARL(Zhengetal.,2021), andthis
episodiccontrolcaseshowsfasterconvergencethanthelearningwithoutsuchmemory. Whereas
1
4202
raM
2
]GL.sc[
1v21110.3042:viXraPublishedasaconferencepaperatICLR2024
therearemeritsfromepisodicmemoryandcontrolfromitsutilization, thereexistsaproblemof
determiningwhichmemoriestorecallandhowtousethem,toefficientlyexplorefromthememory.
Accordingto(Blundelletal.,2016;Linetal.,2018;Zhengetal.,2021),thepreviousepisodiccontrol
generally utilizes a random projection to embed global states, but this random projection hardly
makes the semantically similar states close to one another in the embedding space. In this case,
explorationwillbelimitedtoanarrowdistancethreshold. However,thissmallthresholdleadsto
inefficientmemoryutilizationbecausetherecallofepisodicmemoryundersuchsmallthresholds
retrievesonlythesamestatewithoutconsiderationofsemanticsimilarityfromtheperspectiveof
goalachievement. Additionally,thenaiveutilizationofepisodiccontroloncomplextasksinvolves
theriskofconvergingtolocaloptimabyrepeatedlyrevisitingpreviouslyexploredstates,favoring
exploitationoverexploration.
Contribution. ThispaperpresentsanEfficientepisodicMemoryUtilizationformulti-agentrein-
forcementlearning(EMU),aframeworktoselectivelyencouragedesirabletransitionswithsemantic
memoryembeddings.
• Efficientmemoryembedding: Whengeneratingfeaturesofaglobalstateforepisodic
memory(Figure1(b)),weadoptanencoder/decoderstructurewhere1)anencoderembeds
aglobalstateconditionedontimestepintoalow-dimensionalfeatureand2)adecodertakes
thisfeatureasaninputconditionedonthetimesteptopredictthereturnoftheglobalstate.
Inaddition,toensuresmootherembeddingspace,wealsoconsiderthereconstructionof
the global state when training the decoder to predict its return. To this end, we develop
deterministicConditionalAutoEncoder(dCAE)(Figure1(c)).Withthisstructure,important
featuresforoverallreturncanbecapturedintheembeddingspace. Theproposedembedding
containssemanticmeaningandthusguaranteesagradualchangeoffeaturespace,makingthe
furtherexplorationonmemoryspacenearthegivenstate,i.e.,efficientmemoryutilization.
• Episodicincentivegeneration: Whilethesemanticembeddingprovidesaspacetoexplore,
we still need to identify promising state transitions to explore. Therefore, we define a
desirabletrajectoryrepresentingthehighestreturnpath,suchasdestroyingallenemiesin
SMACorscoringagoalinGoogleResearchFootball(GRF)(Kurachetal.,2020). States
onthistrajectoryaremarkedasdesirableinepisodicmemory,sowecouldincentivizethe
explorationonsuchstatesaccordingtotheirdesirability. Wenamethisincentivestructure
as an episodic incentive (Figure 1(d)), encouraging desirable transitions and preventing
convergencetounsatisfactorylocaloptima. Weprovidetheoreticalanalysesdemonstrating
thatthisepisodicincentiveyieldsabettergradientsignalcomparedtoconventionalepisodic
control.
WeevaluateEMUonSMACandGRF,andempiricalresultsdemonstratethattheproposedmethod
achievesfurtherperformanceimprovementcomparedtothestate-of-artbaselinemethods. Ablation
studiesandqualitativeanalysesvalidatethepropositionsmadebythispaper.
2 PRELIMINARY
2.1 DECENTRALIZEDPOMDP
A fully cooperative multi-agent task can be formalized by following the Decentralized Par-
tially Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016), G =
⟨I,S,A,P,R,Ω,O,n,γ⟩, where I is the finite set of n agents; s ∈ S is the true state of the
environment; a ∈ Aisthei-thagent’sactionformingthejointactiona ∈ An; P(s′|s,a)isthe
i
statetransitionfunction;Risarewardfunctionr =R(s,a,s′)∈R;Ωistheobservationspace;O
istheobservationfunctiongeneratinganobservationforeachagento ∈Ω;andfinally,γ ∈[0,1)
i
is a discount factor. At each timestep, an agent has its own local observation o , and the agent
i
selectsanactiona ∈A. Thecurrentstatesandthejointactionofallagentsaleadtoanextstate
i
s′ according to P(s′|s,a). The joint variable of s, a, and s′ will determine the identical reward
r across the multi-agent group. We additionally define key notations: each agent utilizes a local
action-observation history τ ∈ T ≡ (Ω×A) for its policy π (a|τ ), where π : T ×A → [0,1]
i i i
(Hausknecht&Stone,2015;Rashidetal.,2018).
2PublishedasaconferencepaperatICLR2024
2.2 DESIRABILITYANDDESIRABLETRAJECTORY
Definition1. (DesirabilityandDesirableTrajectory)ForagiventhresholdreturnR andatrajectory
thr
T :={s ,a ,r ,s ,a ,r ,...,s },T isconsideredasadesirabletrajectory,denotedasT ,when
0 0 0 1 1 1 T ξ
anepisodicreturnisR = ΣT−1r ≥ R . Abinaryindicatorξ(·)denotesthedesirabilityof
t=0 t′=t t′ thr
states asξ(s )=1whens ∈∀T .
t t t ξ
IncooperativeMARLtasks,suchasSMACandGRF,thetotalamountofrewardsfromtheenviron-
mentwithinanepisodeisoftenlimitedasR ,whichisonlygivenwhencooperativeagentsachieve
max
acommongoal. Insuchacase, wecansetR = R . Forfurtherdescriptionofcooperative
thr max
MARL,pleaseseeAppendixA.
2.3 EPISODICCONTROLINMARL
Episodiccontrolwasintroducedfromtheanalogyofabrain’shippocampusformemoryutilization
(Lengyel&Dayan,2007). AftertheintroductionofdeepQ-network,Blundelletal.(2016)adopted
thisideaofepisodiccontroltothemodel-freesettingbystoringthehighestreturnofagivenstate,
toefficientlyestimatetheQ-valuesofthestate. Thisrecallingofthehigh-rewardexperienceshelps
toincreasesampleefficiencyandthusexpeditestheoveralllearningprocess(Blundelletal.,2016;
Pritzeletal.,2017;Linetal.,2018). PleaseseeAppendixAforrelatedworksandfurtherdiscussions.
Attimestept,letusdefineaglobalstateass . Whenutilizingepisodiccontrol,insteadofdirectly
t
using s , researchers adopt a state embedding function f (s):S →Rk to project states toward
t ϕ
a k-dimensional vector space. With this projection, a representation of global state s becomes
t
x =f (s ). TheepisodiccontrolmemorizesH(f (s )),i.e.,thehighestreturnofagivenglobal
t ϕ t ϕ t
states ,inepisodicbufferD (Pritzeletal.,2017;Linetal.,2018;Zhengetal.,2021). Here,x is
t E t
usedasakeytothehighestreturn,H(x );asakey-valuepairinD . TheepisodiccontrolinLin
t E
etal.(2018)updatesH(x )withthefollowingrules.
t
(cid:26)
max{H(xˆ ),R (s ,a )}, if||xˆ −x || <δ
H(x )= t t t t t t 2 (1)
t R (s ,a ), otherwise,
t t t
whereR (s ,a )isthereturnofagiven(s ,a );δisathresholdvalueofstate-embeddingdifference;
t t t t t
andxˆ =f (sˆ)isx =f (s )’snearestneighborinD .Ifthereisnosimilarprojectedstatexˆ such
t ϕ t t ϕ t E t
that||xˆ −x || <δinthememory,thenH(x )keepsthecurrentR (s ,a ).Leveragingtheepisodic
t t 2 t t t t
memory,EMC(Zhengetal.,2021)presentstheone-stepTDmemorytargetQ (f (s ),a )as
EC ϕ t t
Q (f (s ),a )=r (s ,a )+γH(f (s )). (2)
EC ϕ t t t t t ϕ t+1
Then,thelossfunctionLEC fortrainingcanbeexpressedastheweightedsumofone-stepTDerror
θ
andone-stepTDmemoryerror,i.e.,MonteCarlo(MC)inferenceerror,basedonQ (f (s ),a ).
EC ϕ t t
LEC =(y(s,a)−Q (s,a;θ))2+λ(Q (f (s),a)−Q (s,a;θ))2, (3)
θ tot EC ϕ tot
wherey(s,a)isone-stepTDtarget;Q isthejointQ-valuefunctionparameterizedbyθ;andλisa
tot
scalefactor.
Problemoftheconventionalepisodiccontrolwithrandomprojection Randomprojectionis
usefulfordimensionalityreductionasitpreservesdistancerelationships,asdemonstratedbythe
Johnson-Lindenstrausslemma(Dasgupta&Gupta,2003). However,arandomprojectionadopted
for f (s) hardly has a semantic meaning in its embedding x , as it puts random weights on the
ϕ t
statefeatureswithoutconsideringthepatternsofdeterminingthestatereturns. Additionally,when
recallingthememoryfromD ,theprojectedstatex canabruptlychangeevenwithasmallchange
E t
ofs becausetheembeddingisnotbeingregulatedbythereturn. Thisresultsinasparseselection
t
of semantically similar memories, i.e. similar states with similar or better rewards. As a result,
conventionalepisodiccontrolusingrandomprojectiononlyrecallsidenticalstatesandreliesonits
ownMonte-Carlo(MC)returntoregulatetheone-stepTDtargetinference,limitingexplorationof
nearbystatesontheembeddingspace.
Theproblemintensifieswhenthehigh-returnstatesintheearlytrainingphaseareindeedlocaloptima.
Insuchcases,thenaiveutilizationofepisodiccontrolispronetoconvergeonlocalminima. Asa
result,forthesuperhardtasksofSMAC,EMC(Zhengetal.,2021)hadtodecreasethemagnitudeof
thisregularizationtoalmostzero,i.e.,notconsideringepisodicmemoriesanymore.
3PublishedasaconferencepaperatICLR2024
3 METHODOLOGY
This section introduces Efficient episodic Memory Utilization (EMU) (Figure 1). We begin by
explaininghowtoconstruct(1)semanticmemoryembeddingstobetterutilizetheepisodicmemory,
whichenablesmemoryrecallofsimilar,morepromisingstates. Tofurtherimprovememoryutiliza-
tion,asanalternativetotheconventionalepisodiccontrol,wepropose(2)episodicincentivethat
selectivelyencouragesdesirabletransitionswhilepreventinglocalconvergencetowardsundesirable
trajectories.
Environment (a) Standard Value
Factorization Framework
(𝝉,𝒂,𝝉′,𝑠,𝑟) 𝑎𝑖𝑖𝑁 =1 𝑜𝑖′ 𝑖𝑁 =1, 𝑟𝑖𝑖𝑁 =1
Replay Buffer 𝑫
Controller Mixing Mixing Network
𝑄𝑖(∙;𝜃) Gradients
𝑄𝑡𝑜𝑡=𝑓(𝑄1,𝑄2,⋯,𝑄𝑛;𝜃)
𝑠,𝐻 𝑠 ,𝑡
𝑟𝑝
FC FC FC
𝑥 𝜉=0
𝑓𝜙(∙) 𝑠 𝑠 ReLU 𝑥 ReLU ReLU 𝑠ҧ 𝑥
FC
FC 𝑓𝜙(∙)
ReLU
⋮ ⋮ ⋮ 𝐻(𝑠),𝑡
𝑡
𝑓 𝜙(∙) 𝑡 FC
𝑓 𝜓(∙)
𝐻ഥ
𝑠,𝑡 𝛿𝑥 𝑟𝑝 𝑥′ 𝜉=1
(b) Episodic Buffer 𝑫𝑬 (c) State Embedding Structure (d) Episodic Incentive Generation
Figure1: OverviewofEMUframework.
3.1 SEMANTICMEMORYEMBEDDING
EpisodicMemoryConstructionToaddresstheproblemsofarandomprojectionadoptedinepisodic
control, we propose a trainable embedding function f (s) to learn the state embedding patterns
ϕ
affectedbythehighestreturn. Theproblemofalearnableembeddingnetworkf isthatthematch
ϕ
betweenH(f (s ))ands breakswheneverf isupdated. Hence,wesavetheglobalstates aswell
ϕ t t ϕ t
asapairofH andx inD ,sothatwecanupdatex=f (s)wheneverf isupdated. Inaddition,
t t E ϕ ϕ
westorethedesirabilityξ ofs accordingtoDefinition1. AppendixE.1illustratesthedetailsof
t
memoryconstructionproposedbythispaper.
LearningframeworkforStateEmbeddingWhentrainingf (s ),itiscriticaltoextractimportant
ϕ t
featuresofaglobalstatethataffectitsvalue,i.e.,thehighestreturn. Thus,weadditionallyadopt
a decoder structure H¯ = f (x ) to predict the highest return H of s . We call this embedding
t ψ t t t
functionasEmbNet,anditslearningobjectiveoff andf canbewrittenas
ϕ ψ
L(ϕ,ψ)=(H −f (f (s )))2. (4)
t ψ ϕ t
Whenconstructingtheembeddingspace,wefoundthatanadditionalconsiderationofreconstruction
of state s conditioned on timestep t improves the quality of feature extraction and constitutes a
smoother embedding space. To this end, we develop the deterministic conditional autoencoder
(dCAE),andthecorrespondinglossfunctioncanbeexpressedas
L(ϕ,ψ)=(cid:0) H −fH(f (s |t)|t)(cid:1)2 +λ ||s −fs(f (s |t)|t)||2, (5)
t ψ ϕ t rcon t ψ ϕ t 2
wherefH predictsthehighestreturn;fs reconstructss ;λ isascalefactor. Here,fH andfs
ψ ψ t rcon ψ ψ
sharethelowerpartofnetworksasillustratedinFigure1(c). AppendixC.1presentsthedetailsof
networkstructureoff andf ,andAlgorithm1inAppendixC.1presentsthelearningframeworkfor
ϕ ψ
f andf . ThistrainingisconductedperiodicallyinparalleltotheRLpolicylearningonQ (·;θ).
ϕ ψ tot
Figure2illustratestheresultoft-SNE(VanderMaaten&Hinton,2008)of50Ksamplesofx∈D
E
out of 1M memory data in training for 3s_vs_5z task of SMAC. Unlike supervised learning
withlabeldata,thereisnolabelforeachx . Thus,wemarkx withitspairofthehighestreturn
t t
H . ComparedtoarandomprojectioninFigure2(a),x viaf iswell-clustered,accordingtothe
t t ϕ
similarity of the embedded state and its return. This clustering of x enables us to safely select
t
4PublishedasaconferencepaperatICLR2024
(a)RandomProjection (b)EmbNet (c)dCAE
Figure2: t-SNEofsampledembeddingx∈D . Colorsfromredtopurple(rainbow)representfrom
E
lowreturntohighreturn.
episodic memories around the key state s , which constitutes efficient memory utilization. This
t
memoryutilizationexpediteslearningspeedaswellasencouragesexplorationtoamorepromising
statesˆ nears . AppendixFillustrateshowtodetermineδofEq. 1inamemory-efficientway.
t t
3.2 EPISODICINCENTIVE
With the learnable memory embedding for an efficient memory recall, how to use the selected
memories still remains a challenge because a naive utilization of episodic memory is prone to
convergeonlocalminima. Tosolvethisissue,weproposeanewrewardstructurecalledepisodic
incentiverpbyleveragingthedesirabilityξofstatesinD . Beforederivingtheepisodicincentive
E
rp,wefirstneedtounderstandthecharacteristicsofepisodiccontrol. Inthissection,wedenotethe
jointQ-functionQ (·;θ)simplyasQ forconciseness.
tot θ
Theorem1. Givenatransition(s,a,r,s′)andH(x′),letL betheQ-learninglosswithadditional
θ
transition reward, i.e., L := (y(s,a)+rEC(s,a,s′)−Q (s,a;θ))2 where rEC(s,a,s′) :=
θ tot
λ(r(s,a)+γH(x′)−Q (s,a)),then∇ L =∇ LEC. (ProofinAppendixB.1)
θ θ θ θ θ
AsTheorem1suggests,wecangeneratethesamegradientsignalastheepisodiccontrolbyleveraging
theadditionaltransitionrewardrEC(s,a,s′). However,rEC(s,a,s′)accompaniesariskoflocal
convergenceasdiscussedinSection2.3. Therefore,insteadofapplyingrEC(s,a,s′),wepropose
theepisodicincentiverp := γηˆ(s′)thatprovidesanadditionalrewardforthedesirabletransition
(s,a,r,s′), such that ξ(s′) = 1. Here, ηˆ(s′) estimates η∗(s′), which represents the difference
betweenthetruevalueV∗(s′)ofs′ andthepredictedvalueviatargetnetworkmax Q (s′,a′),
a′ θ−
definedas
η∗(s′):=V∗(s′)−maxQ (s′,a′). (6)
θ−
a′
Note that we do not know V∗(s′) and subsequently η∗(s′). To accurately estimate η∗(s′) with
ηˆ(s′), we use the expected value considering the current policy π as ηˆ(s′) := E [η(s′)] where
θ πθ
η ∈ [0,η (s′)] for s′ ∼ P(s′|s,a ∼ π ). Here, η (s′) can be reasonably approximated by
max θ max
usingH(f (s′))inD . Then,withthecount-basedestimationηˆ(s′),episodicincentiverpcanbe
ϕ E
expressedas
N (s′) N (s′)
rp =γηˆ(s′)=γE [η(s′)]≃γ ξ η (s′)=γ ξ (H(f (s′))−maxQ (s′,a′)),
πθ N call(s′) max N call(s′) ϕ a′ θ−
(7)
where N (s′) is the number of visits on xˆ′ = NN(f (s′)) ∈ D ; and N is the number of
call ϕ E ξ
desirabletransitionfromxˆ′. Here,NN(·)representsafunctionforselectingthenearestneighbor.
FromTheorem1,thelossfunctionadoptingepisodiccontrolwithanalternativetransitionrewardrp
insteadofrEC canbeexpressedas
Lp =(r(s,a)+rp+γmaxQ (s′,a′)−Q (s,a))2. (8)
θ θ− θ
a′
Then,thegradientsignaloftheone-stepTDinferenceloss∇ Lp withtheepisodicrewardrp =
θ θ
γηˆ(s′)canbewrittenas
N (s′)
∇ Lp =−2∇ Q (s,a)(∆ε +rp)=−2∇ Q (s,a)(∆ε +γ ξ η (s′)), (9)
θ θ θ θ TD θ θ TD N (s′) max
call
5PublishedasaconferencepaperatICLR2024
where∆ε =r(s,a)+γmax Q (s′,a′)−Q (s,a)isone-stepinferenceTDerror. Here,the
TD a′ θ− θ
gradient signal ∇ Lp with the proposed episodic reward rp can accurately estimate the optimal
θ θ
gradientsignalasfollows.
Theorem2. Let∇ L∗ =−2∇ Q (s,a)(∆ε∗ )betheoptimalgradientsignalwiththetrueone
θ θ θ θ TD
step TD error ∆ε∗ = r(s,a)+γV∗(s′)−Q (s,a). Then, the gradient signal ∇ Lp with the
TD θ θ θ
episodicincentiverpconvergestotheoptimalgradientsignalasthepolicyconvergestotheoptimal
policyπ∗,i.e.,∇ Lp →∇ L∗asπ →π∗. (ProofinAppendixB.2)
θ θ θ θ θ θ θ
Theorem2alsoimpliesthatthereex-
ists a certain bias in ∇ LEC as de-
θ θ
scribedinAppendixB.2.Besidesthe
propertyofconvergencetotheopti-
malgradientsignalpresentedinThe-
orem 2, the episodic incentive has
thefollowingadditionalcharacteris-
tics. (1) The episodic incentive is
only applied to the desirable transi-
tion. We can simply see that rp =
(a)3s5z (b)MMM2
γηˆ = γE [η] ≃ γη N /N
and if ξ(sπ ′θ ) = 0 thm enax Nξ =ca 0ll , Figure3: Episodicincentive. Testtrajectoriesareplottedon
ξ
yieldingrp → 0. Subsequently,(2) theembeddedspacewithsampledmemoriesinD E,denoted
thereisnoneedtoadjustascalefac- withdottedmarkers. Starmarkersandnumbersrepresentthe
torbythetaskcomplexity. (3)The desirabilityofstateandtimestepintheepisode,respectively.
episodicincentivecanreducetherisk ColorrepresentsthesamesemanticsasFigure2.
ofoverestimationbyconsideringthe
expectedvalueofE [η]. Insteadofconsideringtheoptimisticη ,thecount-basedestimation
rp = γηˆ = γE [ηπ ]θ can consider the randomness of the policy m πa .x Figure 3 illustrates how the
πθ θ
episodicincentiveworkswiththedesirabilitystoredinD constructedbyAlgorithm2presentedin
E
AppendixE.1. InFigure3asweintended,high-valuestates(atsmalltimesteps)areclusteredclose
tothepurplezone,whilelow-valuestates(atlargetimesteps)arelocatedintheredzone.
3.3 OVERALLLEARNINGOBJECTIVE
ToconstructthejointQ-functionQ fromindividualQ oftheagenti,anyformofmixercanbe
tot i
used. Inthispaper,wemainlyadoptthemixerpresentedinQPLEX(Wangetal.,2020b)similarto
Zhengetal.(2021),whichguaranteesthecompleteIndividual-Global-Max(IGM)condition(Son
etal.,2019;Wangetal.,2020b). Consideringanyintrinsicrewardrc encouraginganexploration
(Zhengetal.,2021)ordiversity(Chenghaoetal.,2021),thefinallossfunctionfortheactionpolicy
learningfromEq. 8canbeextendedas
Lp =(cid:0) r(s,a)+rp+β rc+γmax Q (s′,a′;θ−)−Q (s,a;θ)(cid:1)2 , (10)
θ c a′ tot tot
whereβ isascalefactor. Notethattheepisodicincentiverp canbeusedinconjunctionwithany
c
formofintrinsicrewardrc beingproperlyannealedthroughoutthetraining. Again,θdenotesthe
parametersofnetworksrelatedtoactionpolicyQ andthecorrespondingmixernetworktogenerate
i
Q . FortheactionselectionviaQ,weadoptaGRUtoencodealocalaction-observationhistoryτ
tot
presentedin2.1similartoSunehagetal.(2017);Rashidetal.(2018);Wangetal.(2020b);butinEq.
10,wedenoteequationswithsinsteadofτ forthecoherencewithderivationintheprevioussection.
AppendixE.2presentstheoveralltrainingalgorithm.
4 EXPERIMENTS
In this part, we have formulated our experiments with the intention of addressing the following
inquiriesdenotedasQ1-3.
• Q1. HowdoesEMUcomparetothestate-of-the-artMARLframeworks?
• Q2. Howdoestheproposedstateembeddingchangetheembeddingspaceandimprovethe
performance?
• Q3. Howdoestheepisodicincentiveimproveperformance?
6PublishedasaconferencepaperatICLR2024
Weconductexperimentsoncomplexmulti-agenttaskssuchasSMAC(Samvelyanetal.,2019)and
GRF(Kurachetal.,2020). TheexperimentscompareEMUagainstEMCadoptingepisodiccontrol
(Zhengetal.,2021). Also,weincludenotablebaselines,suchasvalue-basedMARLmethodsQMIX
(Rashidetal.,2018),QPLEX(Wangetal.,2020b),CDSencouragingindividualdiversity(Chenghao
etal.,2021). Particularly,weemphasizethatEMUcanbecombinedwithanyMARLframework,
sowepresenttwoversionsofEMUimplementedonoriginalQPLEXandCDS,denotedasEMU
(QPLEX)andEMU(CDS),respectively. AppendixCprovidesfurtherdetailsofexperimentsettings
andimplementations,andAppendixD.12providestheapplicabilityofEMUtosingle-agenttasks,
includingpixel-basedhigh-dimensionaltasks.
4.1 Q1. COMPARATIVEEVALUATIONONSTARCRAFTII(SMAC)
Figure4: PerformancecomparisonofEMUagainstbaselinealgorithmsonthreeeasyandhard
SMACmaps: 1c3s5z,3s_vs_5z,and5m_vs_6m,andthreesuperhardSMACmaps: MMM2,
6h_vs_8z,and3s5z_vs_3s6z.
Figure4illustratestheoverallperformanceofEMUonvariousSMACmaps. Themapcategorization
regardingthelevelofdifficultyfollowsthepracticeofSamvelyanetal.(2019).Thankstotheefficient
memoryutilizationandepisodicincentive,bothEMU(QPLEX)andEMU(CDS)showsignificant
performance improvement compared to their original methodologies. Especially, in super hard
SMACmaps,theproposedmethodmarkedlyexpeditesconvergenceonoptimalpolicy.
4.2 Q1. COMPARATIVEEVALUATIONONGOOGLERESEARCHFOOTBALL(GRF)
Here, we conduct experiments on GRF to further compare the performance of EMU with other
baseline algorithms. In our GRF task, CDS and EMU (CDS) do not utilize the agent’s index on
observationastheycontainthepredictionnetworkswhileotherbaselines(QMIX,EMC,QPLEX)
useinformationoftheagent’sidentityinobservations. Inaddition,wedonotutilizeanyadditional
algorithm, such as prioritized experience replay (Schaul et al., 2015), for all baselines and our
method,toexpeditelearningefficiency. Fromtheexperiments,adoptingEMUachievessignificant
performanceimprovement,andEMUquicklyfindsthewinningorscoringpolicyattheearlylearning
phasebyutilizingsemanticallysimilarmemory.
Figure5:PerformancecomparisonofEMUagainstbaselinealgorithmsonGoogleResearchFootball.
7PublishedasaconferencepaperatICLR2024
4.3 Q2. PARAMETRICANDABLATIONSTUDY
Inthissection,weexaminehowthekeyhyperparameterδandthechoiceofdesignforf affectthe
ϕ
performance. Tocomparethelearningqualityandperformancemorequantitatively,weproposea
newperformanceindexcalledoverallwin-rate,µ¯ . Thepurposeofµ¯ istoconsiderbothtraining
w w
efficiency(speed)andquality(win-rate)fordifferentseedcases(seeAppendixD.1fordetails). We
conductexperimentsonselectedSMACmapstomeasureµ¯ accordingtoδanddesignchoiceforf
w ϕ
suchas(1)randomprojection,(2)EmbNetwithEq. 4and(3)dCAEwithEq. 5.
(a)3s_vs_5z (b)5m_vs_6m (a)3s_vs_5z (b)5m_vs_6m
Figure6: µ¯ accordingtoδ andvariousde- Figure7: Finalwin-rateaccordingtoδ and
w
signchoicesforf onSMACmaps. variousdesignchoicesforf onSMACmaps.
ϕ ϕ
Figure 6 and Figure 7 show µ¯ values and test win-rate at the end of training time according to
w
differentδ,presentedinlog-scale. Toseetheeffectofdesignchoiceforf distinctly,weconduct
ϕ
experimentswiththeconventionalepisodiccontrol. Moredataofµ¯ ispresentedinTables4and5in
w
AppendixD.2. Figure6illustratesthatdCAEstructureshowsthebesttrainingefficiencythroughout
variousδwhileachievingtheoptimalpolicyasotherdesignchoicesaspresentedinFigure7.
Interestingly,dCAEstructureworkswellwith
awiderrangeofδthanEmbNet. Weconjecture
thatEmbNetcanselectverydifferentstatesas
explorationifthosestateshavesimilarreturnH
duringtraining. Thisexcessivememoryrecall
adversely affects learning and fails to find an
optimal policy as a result. See Appendix D.2
fordetailedanalysisandAppendixD.8foran
(a)CA_hard(GRF) (b)6h_vs_8z(SMAC)
ablationstudyonthelossfunctionofdCAE.
Figure8: EffectofvaryingδoncomplexMARL
Eventhoughawiderangeofδworkswellasin
tasks.
Figures6and7,choosingapropervalueofδin
moredifficultMARLtaskssignificantlyimprovestheoveralllearningperformance. Figure8shows
thelearningcurveofEMUaccordingtoδ =1.3e−7,δ =1.3e−5,δ =1.3e−3,andδ =1.3e−2.
1 2 3 4
InsuperhardMARLtaskssuchas6h_vs_8zinSMACandCA_hardinGRF,δ showsthebest
3
performancecomparedtootherδvalues. ThisisconsistentwiththevaluesuggestedinAppendix
F,whereδ isdeterminedinamemory-efficientway. Furtherparametricstudyonδ andλ are
rcon
presentedinAppendixD.5andD.6,respectively.
4.4 Q3. FURTHERABLATIONSTUDY
Inthissection,wecarryoutfurtherablationstudiestoseetheeffectofepisodicincentiverppresented
inSection3.2. FromEMU(QPLEX)andEMU(CDS),weablatetheepisodicincentiveanddenote
themwith(No-EI).Weadditionallyablateembeddingnetworkf fromEMUanddenotethemwith
ϕ
(No-SE).Inaddition, weablatebothparts, yieldingEMC(QPLEX-original)andCDS(QPLEX-
original).WeevaluatetheperformanceofeachmodelonsuperhardSMACmaps.Additionalablation
studiesonGRFmapsarepresentedinAppendixD.7. NotethatEMC(QPLEX-original)utilizesthe
conventionalepisodiccontrolpresentedinZhengetal.(2021).
Figure9illustratesthattheepisodicincentivelargelyaffectslearningperformance. Especially,EMU
(QPLEX-No-EI)andEMU(CDS-No-EI)utilizingtheconventionalepisodiccontrolshowalarge
performance variation according to different seeds. This demonstrates that a naive utilization of
episodiccontrolcouldbedetrimentaltolearninganoptimalpolicy. Ontheotherhand,theepisodic
incentiveselectivelyencouragestransitionconsideringdesirabilityandthuspreventssuchalocal
convergence. AppendixD.9andD.10presentanadditionalablationstudyonsemanticembedding
8PublishedasaconferencepaperatICLR2024
(a)6h_vs_8zSMAC (b)3s5z_vs_3s6zSMAC (c)3s5z_vs_3s6zSMAC
Figure9: AblationstudiesonepisodicincentiveviacomplexMARLtasks.
andrc,respectively. Inaddition,AppendixD.11presentsacomparisonwithanalternativeincentive
(Henaffetal.,2022)presentedinasingle-agentsetting.
4.5 QUALITATIVEANALYSISANDVISUALIZATION
In this section, we conduct analysis with visualization to check how the desirability ξ is
memorized in D and whether it conveys correct information. Figure 10 illustrates two test
E
scenarios with different seeds, and each snapshot is denoted with a corresponding timestep.
In Figure 11, the trajectory of each episode is projected onto the embedded space of D .
E
InFigure10,case(a)successfullyde-
𝑡=6 𝑡=10 𝑡=1102 𝑡=20
feated all enemies, whereas case (b)
losttheengagement. Bothcaseswent
throughasimilar,desirabletrajectory
atthebeginning. Forexample, until
(a)Desirabletrajectoryon5m_vs_6mSMACmap
t = 10agentsinbothcasesfocused
onkillingoneenemyandkeptallally 𝑡=6 𝑡=10 𝑡=12 𝑡=20
agents alive at the same time. How-
ever,att=12,case(b)lostoneagent,
and two trajectories of case (a) and
(b)Undesirabletrajectoryon5m_vs_6mSMACmap
(b)inembeddedspacebegantobifur-
cate.Case(b)stillhadachancetowin
Figure10: Visualizationoftestepisodes.
aroundt = 14 ∼ t = 16. However,
thestatesbecameundesirable(denotedwithoutstarmarker)afterlosingthreeallyagentsaround
t=20,andcase(b)lostthebattleasaresult. Thesesequencesandcharacteristicsoftrajectoriesare
wellcapturedbydesirabilityξinD asillustratedinFigure11.
E
Furthermore, the desirable state denoted with
ξ =1encouragesexplorationarounditthough
itisnotdirectlyretrievedduringbatchsampling.
Thisoccursthroughthepropagationofitsdesir-
abilitytostatescurrentlydistinguishedasunde-
sirableduringmemoryconstruction,usingAlgo-
rithm2inAppendixE.1. Consequently,when
(a)Desirabletrajectory (b)Undesirabletrajectory
the state’s desirability is precisely memorized
in D , it can encourage desirable transitions Figure11: Testtrajectoriesonembeddedspaceof
E
throughtheepisodicincentiverp. D .
E
5 CONCLUSION
ThispaperpresentsEMU,anewframeworktoefficientlyutilizeepisodicmemoryforcooperative
MARL. EMU introduces two major components: 1) a trainable semantic embedding and 2) an
episodicincentiveutilizingdesirabilityofstate. Semanticmemoryembeddingallowsustosafely
utilize similar memory in a wide area, expediting learning via exploratory memory recall. The
proposedepisodicincentiveselectivelyencouragesdesirabletransitionsandreducestheriskoflocal
convergence by leveraging the desirability of the state. As a result, there is no need for manual
hyperparametertuningaccordingtothecomplexityoftasks,unlikeconventionalepisodiccontrol.
ExperimentsandablationstudiesvalidatetheeffectivenessofeachcomponentofEMU.
9PublishedasaconferencepaperatICLR2024
ACKNOWLEDGEMENTS
ThisresearchwassupportedbyAITechnologyDevelopmentforCommonsenseExtraction,Rea-
soning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and
ICT(2022-0-00077).
REFERENCES
MarcBellemare,SriramSrinivasan,GeorgOstrovski,TomSchaul,DavidSaxton,andRemiMunos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information
processingsystems,29,2016.
MarcGBellemare,YavarNaddaf,JoelVeness,andMichaelBowling. Thearcadelearningenviron-
ment: Anevaluationplatformforgeneralagents. JournalofArtificialIntelligenceResearch,47:
253–279,2013.
CharlesBlundell,BenignoUria,AlexanderPritzel,YazheLi,AvrahamRuderman,JoelZLeibo,
Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint
arXiv:1606.04460,2016.
YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov. Explorationbyrandomnetwork
distillation. arXivpreprintarXiv:1810.12894,2018.
Li Chenghao, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang.
Celebratingdiversityinsharedmulti-agentreinforcementlearning.AdvancesinNeuralInformation
ProcessingSystems,34:3991–4002,2021.
NikolayDandanov,HusseinAl-Shatri,AnjaKlein,andVladimirPoulkov. Dynamicself-optimization
oftheantennatiltforbesttrade-offbetweencoverageandcapacityinmobilenetworks. Wireless
PersonalCommunications,92(1):251–278,2017.
SanjoyDasguptaandAnupamGupta. Anelementaryproofofatheoremofjohnsonandlindenstrauss.
RandomStructures&Algorithms,22(1):60–65,2003.
Marc-AndréDittrichandSilasFohlmeister. Cooperativemulti-agentsystemforproductioncontrol
usingreinforcementlearning. CIRPAnnals,69(1):389–392,2020.
YaliDu,LeiHan,MengFang,JiLiu,TianhongDai,andDachengTao. Liir: Learningindividual
intrinsicrewardinmulti-agentreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,32,2019.
ScottFujimoto,HerkeHoof,andDavidMeger. Addressingfunctionapproximationerrorinactor-
criticmethods. InInternationalconferenceonmachinelearning,pp.1587–1596.PMLR,2018.
JayeshKGupta,MaximEgorov,andMykelKochenderfer. Cooperativemulti-agentcontrolusing
deepreinforcementlearning. InInternationalconferenceonautonomousagentsandmultiagent
systems,pp.66–83.Springer,2017.
MatthewHausknechtandPeterStone. Deeprecurrentq-learningforpartiallyobservablemdps. In
2015aaaifallsymposiumseries,2015.
Mikael Henaff, Roberta Raileanu, Minqi Jiang, and Tim Rocktäschel. Exploration via elliptical
episodicbonuses. AdvancesinNeuralInformationProcessingSystems,35:37631–37646,2022.
ReinHouthooft, XiChen, YanDuan, JohnSchulman, FilipDeTurck, andPieterAbbeel. Vime:
Variational information maximizing exploration. Advances in neural information processing
systems,29,2016.
HaoHu,JianingYe,GuangxiangZhu,ZhizhouRen,andChongjieZhang. Generalizableepisodic
memoryfordeepreinforcementlearning. Internationalconferenceonmachinelearning,2021.
10PublishedasaconferencepaperatICLR2024
NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroOrtega,DJStrouse,
JoelZLeibo,andNandoDeFreitas. Socialinfluenceasintrinsicmotivationformulti-agentdeep
reinforcementlearning. InInternationalconferenceonmachinelearning,pp.3040–3049.PMLR,
2019.
Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi:
Explorationwithmutualinformation. arXivpreprintarXiv:1810.01176,2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
KarolKurach,AntonRaichuk,PiotrStanczyk,MichalZajkc,OlivierBachem,LasseEspeholt,Carlos
Riquelme,DamienVincent,MarcinMichalski,OlivierBousquet,etal. Googleresearchfootball:
Anovelreinforcementlearningenvironment. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pp.4501–4510,2020.
LeiLe,AndrewPatterson,andMarthaWhite. Supervisedautoencoders: Improvinggeneralization
performancewithunsupervisedregularizers. Advancesinneuralinformationprocessingsystems,
31,2018.
MátéLengyelandPeterDayan. Hippocampalcontributionstocontrol: thethirdway. Advancesin
neuralinformationprocessingsystems,20,2007.
ZichuanLin,TianqiZhao,GuangwenYang,andLintaoZhang. Episodicmemorydeepq-networks.
arXivpreprintarXiv:1805.07603,2018.
Iou-JenLiu, UnnatJain, RaymondAYeh, andAlexanderSchwing. Cooperativeexplorationfor
multi-agentdeepreinforcementlearning. InInternationalConferenceonMachineLearning,pp.
6826–6836.PMLR,2021.
AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson. Maven: Multi-agent
variationalexploration. AdvancesinNeuralInformationProcessingSystems,32,2019.
DavidHenryMguni,TaherJafferjee,JianhongWang,NicolasPerez-Nieves,OliverSlumbers,Feifei
Tong,YangLi,JiangchengZhu,YaodongYang,andJunWang. Ligs: Learnableintrinsic-reward
generationselectionformulti-agentlearning. arXivpreprintarXiv:2112.02618,2021.
ShakirMohamedandDaniloJimenezRezende.Variationalinformationmaximisationforintrinsically
motivatedreinforcementlearning. Advancesinneuralinformationprocessingsystems,28,2015.
FransAOliehoekandChristopherAmato.AconciseintroductiontodecentralizedPOMDPs.Springer,
2016.
FransAOliehoek,MatthijsTJSpaan,andNikosVlassis. Optimalandapproximateq-valuefunctions
fordecentralizedpomdps. JournalofArtificialIntelligenceResearch,32:289–353,2008.
GeorgOstrovski,MarcGBellemare,AäronOord,andRémiMunos. Count-basedexplorationwith
neuraldensitymodels. InInternationalconferenceonmachinelearning,pp.2721–2730.PMLR,
2017.
DeepakPathak,PulkitAgrawal,AlexeiAEfros,andTrevorDarrell. Curiosity-drivenexploration
byself-supervisedprediction. InInternationalconferenceonmachinelearning,pp.2778–2787.
PMLR,2017.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals,
DemisHassabis,DaanWierstra,andCharlesBlundell. Neuralepisodiccontrol. InInternational
ConferenceonMachineLearning,pp.2827–2836.PMLR,2017.
SanthoshKRamakrishnan,AaronGokaslan,ErikWijmans,OleksandrMaksymets,AlexClegg,John
Turner,EricUndersander,WojciechGaluba,AndrewWestbury,AngelXChang,etal. Habitat-
matterport3ddataset(hm3d): 1000large-scale3denvironmentsforembodiedai. arXivpreprint
arXiv:2109.08238,2021.
11PublishedasaconferencepaperatICLR2024
TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,andShi-
monWhiteson. Qmix: Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcement
learning. InInternationalconferenceonmachinelearning,pp.4295–4304.PMLR,2018.
TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson. Weightedqmix: Expanding
monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning. Advancesin
neuralinformationprocessingsystems,33:10199–10210,2020.
MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,
TimGJRudner, Chia-ManHung, PhilipHSTorr, JakobFoerster, andShimonWhiteson. The
starcraftmulti-agentchallenge. arXivpreprintarXiv:1902.04043,2019.
TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver. Prioritizedexperiencereplay. arXiv
preprintarXiv:1511.05952,2015.
KihyukSohn,HonglakLee,andXinchenYan. Learningstructuredoutputrepresentationusingdeep
conditionalgenerativemodels. Advancesinneuralinformationprocessingsystems,28,2015.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi.Qtran:Learningto
factorizewithtransformationforcooperativemulti-agentreinforcementlearning. InInternational
conferenceonmachinelearning,pp.5887–5896.PMLR,2019.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learningwithdeeppredictivemodels. arXivpreprintarXiv:1507.00814,2015.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg,MarcLanctot,NicolasSonnerat,JoelZLeibo,KarlTuyls,etal. Value-decomposition
networksforcooperativemulti-agentlearning. arXivpreprintarXiv:1706.05296,2017.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman,FilipDeTurck,andPieterAbbeel. #exploration: Astudyofcount-basedexploration
fordeepreinforcementlearning. Advancesinneuralinformationprocessingsystems,30,2017.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.
In2012IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pp.5026–5033.
IEEE,2012. doi: 10.1109/IROS.2012.6386109.
LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learningresearch,9(11),2008.
BinyuWang,ZheLiu,QingbiaoLi,andAmandaProrok. Mobilerobotpathplanningindynamic
environmentsthroughgloballyguidedreinforcementlearning. IEEERoboticsandAutomation
Letters,5(4):6932–6939,2020a.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agentq-learning. arXivpreprintarXiv:2008.01062,2020b.
TonghanWang,JianhaoWang,YiWu,andChongjieZhang. Influence-basedmulti-agentexploration.
arXivpreprintarXiv:1910.05512,2019.
TonghanWang,TarunGupta,AnujMahajan,BeiPeng,ShimonWhiteson,andChongjieZhang.Rode:
Learningrolestodecomposemulti-agenttasks. InProceedingsoftheInternationalConferenceon
LearningRepresentations(ICLR),2021.
Marco A Wiering et al. Multi-agent reinforcement learning for traffic light control. In Machine
Learning: ProceedingsoftheSeventeenthInternationalConference(ICML’2000),pp.1151–1158,
2000.
JiachenYang,IgorBorovikov,andHongyuanZha. Hierarchicalcooperativemulti-agentreinforce-
mentlearningwithskilldiscovery. arXivpreprintarXiv:1912.03558,2019.
12PublishedasaconferencepaperatICLR2024
YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyaoTang.
Qatten: Ageneralframeworkforcooperativemultiagentreinforcementlearning. arXivpreprint
arXiv:2002.03939,2020.
ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. The
surprisingeffectivenessofppoincooperativemulti-agentgames. AdvancesinNeuralInformation
ProcessingSystems,35:24611–24624,2022.
LuluZheng,JiaruiChen,JianhaoWang,JiaminHe,YujingHu,YingfengChen,ChangjieFan,Yang
Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-driven
exploration. AdvancesinNeuralInformationProcessingSystems,34:3757–3769,2021.
Guangxiang Zhu, Zichuan Lin, Guangwen Yang, and Chongjie Zhang. Episodic reinforcement
learningwithassociativememory. Internationalconferenceonlearningrepresentations,2020.
13PublishedasaconferencepaperatICLR2024
A RELATED WORKS
This section presents the related works regarding incentive generation for exploration, episodic
control,andthecharacteristicsofcooperativeMARL.
A.1 INCENTIVEFORMULTI-AGENTEXPLORATION
Balancingbetweenexplorationandexploitationinpolicylearningisaparamountissueinreinforce-
mentlearning. Toencourageexploration,modifiedcount-basedmethods(Bellemareetal.,2016;
Ostrovskietal.,2017;Tangetal.,2017),predictionerror-basedmethods(Stadieetal.,2015;Pathak
etal.,2017;Burdaetal.,2018;Kimetal.,2018),andinformationgain-basedmethods(Mohamed&
JimenezRezende,2015;Houthooftetal.,2016)havebeenproposedforasingleagentreinforcement
learning. Inmostcases,anincentiveforexplorationisintroducedasanadditionalrewardtoaTD
targetinQ-learning;orsuchanincentiveisaddedasaregularizerforoveralllossfunctions. Recently,
various aforementioned methods to encourage exploration have been adopted to the multi-agent
setting(Mahajanetal.,2019;Wangetal.,2019;Jaquesetal.,2019;Mgunietal.,2021)andhave
shown their effectiveness. MAVEN (Mahajan et al., 2019) introduces a regularizer maximizing
themutualinformationbetweentrajectoriesandlatentvariablestolearnadiversesetofbehaviors.
LIIR(Duetal.,2019)learnsaparameterizedindividualintrinsicrewardfunctionbymaximizinga
centralizedcritic. CDS(Chenghaoetal.,2021)proposesanovelinformation-theoreticalobjectiveto
maximizethemutualinformationbetweenagents’identitiesandtrajectoriestoencouragediverse
individualized behaviors. EMC (Zheng et al., 2021) proposes a curiosity-driven exploration by
predictingindividualQ-values. Thisindividual-basedQ-valuepredictioncancapturetheinfluence
amongagentsaswellasthenoveltyofstates.
A.2 EPISODICCONTROL
Episodiccontrol(Lengyel&Dayan,2007)waswelladoptedonmodel-freesetting(Blundelletal.,
2016)bystoringthehighestreturnofagivenstate, toefficientlyestimateitsvaluesorQ-values.
Giventhatthesamplegenerationisoftenlimitedbysimulationexecutionsorreal-worldobservations,
itssampleefficiencyhelpstofindanaccurateestimationofQ-value(Blundelletal.,2016;Pritzel
etal.,2017;Linetal.,2018). NEC(Pritzeletal.,2017)usesadifferentiableneuraldictionaryas
anepisodicmemorytoestimatetheactionvaluebytheweightedsumofthevaluesinthememory.
EMDQN(Linetal.,2018)utilizesafixedrandommatrixtogenerateastaterepresentation,which
isusedasakeytolinkbetweenthestaterepresentationandthehighestreturnofthestateinthe
episodicmemory. ERLAM(Zhuetal.,2020)learnsassociativememoriesbybuildingagraphical
representationofstatesinmemory,andGEM(Huetal.,2021)developsstate-actionvaluesofepisodic
memoryinageneralizablemanner. Recently,EMC(Zhengetal.,2021)extendstheapproachof
EMDQN to a deep MARL with curiosity-driven exploration incentives. EMC utilizes episodic
memorytoregularizepolicylearningandshowsperformanceimprovementincooperativeMARL
tasks. However,EMCrequiresahyperparametertuningtodeterminethelevelofimportanceofthe
one-stepTDmemory-basedtargetduringtraining,accordingtothedifficultiesoftasks. Inthispaper,
weinterpretthisregularizationasanadditionaltransitionreward. Then,wepresentanovelformof
reward,calledepisodicincentive,toselectivelyencouragethetransitiontowarddesiredstates,i.e.,
statestowardacommongoalincooperativemulti-agenttasks.
A.3 COOPERATIVEMULTI-AGENTREINFORCEMENTLEARNING(MARL)TASK
Ingeneral, thereisacommongoalincooperativeMARLtasks, whichguaranteesthemaximum
returnthatcanbeobtainedfromtheenvironment. Thus,therecouldbemanylocaloptimawithhigh
returns but not the maximum, which means the agents failed to achieve the common goal in the
end. Inotherwords,thereisadistinctdifferencebetweentheobjectiveofcooperativeMARLtasks
andthatofasingle-agenttask,whichaimstomaximizethereturnasmuchaspossiblewithoutany
boundarydeterminingsuccessorfailure. Ourdesirabilitydefinitionpresentedin1inMARLsetting
becomeswelljustifiedfromthisview. UnderthischaracteristicofMARLtasks,learningoptimal
policyoftentakesalongtimeandevenfails,yieldingalocalconvergence. EMUwasdesignedto
alleviatetheseissuesinMARL.
14PublishedasaconferencepaperatICLR2024
B MATHEMATICAL PROOF
Inthissection,wepresenttheomittedproofsofTheorem1andTheorem2asfollows.
B.1 PROOFOFTHEOREM1
Proof. Thelossfunctionofaconventionalepisodiccontrol,LEC,canbeexpressedastheweighted
θ
sum of one-step inference TD error ∆ε = r(s,a)+γmax Q (s′,a′)−Q (s,a) and MC
TD a′ θ− θ
inferenceerror∆ε =Q (s,a)−Q (s,a).
EC EC θ
LEC =(r(s,a)+γmaxQ (s′,a′)−Q (s,a))2+λ(Q (s,a)−Q (s,a))2, (11)
θ θ− θ EC θ
a′
whereQ (s,a) = r(s,a)+γH(s′)andQ isthetargetnetworkparameterizedbyθ−. Then,
EC θ−
thegradientofLEC canbederivedas
θ
∇ LEC =−2∇ Q (s,a)[(r(s,a)+γmaxQ (s′,a′)−Q (s,a))+λ(Q (s,a)−Q (s,a))]
θ θ θ θ θ− θ EC θ
a′
=−2∇ Q (s,a)(∆ε +λ∆ε ).
θ θ TD EC
(12)
Now,weconsideranadditionalrewardrEC forthetransitiontoaconventionalQ-learningobjective,
themodifiedlossfunctionL canbeexpressedas
θ
L =(r(s,a)+rEC(s,a,s′)+γmaxQ (s′,a′)−Q (s,a))2. (13)
θ θ− θ
a′
Then,thegradientofL presentedinEq. 13iscomputedas
θ
∇ L =−2∇ Q (s,a)(∆ε +rEC). (14)
θ θ θ θ TD
Comparing Eq. 12 and Eq. 14, if we set the additional transition reward as rEC(s,a,s′) =
λ(r(s,a)+γH(s′)−Q (s,a)),then∇ L =∇ LEC holds.
θ θ θ θ θ
B.2 PROOFOFTHEOREM2
Proof. FromEq. 7,thevalueofηˆ(s′)canbeexpressedas
ηˆ(s′)=E [η(s′)]≃ N ξ(s′) (cid:0) H(f (s′))−maxQ (s′,a′)(cid:1) ]. (15)
πθ N call(s′) ϕ a′ θ−
Whenthejointactionsfromthecurrenttimefollowtheoptimalpolicy,a∼π∗,thecumulativereward
θ
froms′ convergestoV∗(s′),i.e.,H(f (s′)) → V∗(s′). Then,everyrecallofxˆ′ = NN(f (s′)) ∈
ϕ ϕ
D guarantees the desirable transition, i.e., ξ(s′) = 1, where NN(·) represents a function for
E
selecting the nearest neighbor. As a result, as N (s′) → ∞, Nξ(s′) → 1, yielding ηˆ(s′) ≃
call Ncall(s′)
Nξ(s′) (cid:0) H(f (s′))−max Q (s′,a′)(cid:1) →V∗(s′)−max Q (s′,a′).Then,thegradientsignal
Ncall(s′) ϕ a′ θ− a′ θ−
withtheepisodicincentive∇ Lpbecomes
θ θ
∇ Lp =−2∇ Q (s,a)[∆ε +rp]
θ θ θ θ TD
=−2∇ Q (s,a)[∆ε +γηˆ(s′)]
θ θ TD
≃−2∇ Q (s,a)[∆ε +γ(V∗(s′)−maxQ (s′,a′))]
θ θ TD θ−
a′
=−2∇ Q (s,a)[r(s,a)+γmaxQ (s′,a′)−Q (s,a)+γ(V∗(s′)−maxQ (s′,a′))]
θ θ θ− θ θ−
a′ a′
=−2∇ Q (s,a)[r(s,a)+γV∗(s′)−Q (s,a)]
θ θ θ
=∇ L∗,
θ θ
(16)
whichcompletestheproof.
Inaddition,whenmax Q (s′,a′)accuratelyestimatesV∗(s′),theoriginalTD-targetispreserved
a′ θ−
astheepisodicincentivebecomeszero,i.e.,rp →0.Thenwiththeproperlyannealedintrinsicreward
rc,thelearningobjectivepresentedinEq. 10degeneratestotheoriginalBellmanoptimalityequation
(Sutton&Barto,2018). Ontheotherhand,eventhoughtheassumptionofH(s′)→V∗(s′)yields
∆ε →∆ε∗ ,∇ LEC hasanadditionalbias∆ε duetoweightedsumstructurepresentedin
EC TD θ θ TD
Eq. 3. Thus,∇ LEC canconvergeto∇ L∗onlywhenmax Q (s′,a′)→V∗(s′)andλ→0at
θ θ θ θ a′ θ−
thesametime.
15PublishedasaconferencepaperatICLR2024
C IMPLEMENTATION AND EXPERIMENT DETAILS
C.1 DETAILSOFIMPLEMENTATION
EncoderandDecoderStructure
As illustrated in Figure 1(c), we have an encoder and decoder structure. For an encoder f , we
ϕ
evaluate two types of structure, EmbNet and dCAE. For EmbNet with the learning objective
presentedinEq. 4,twofullyconnectedlayerswith64-dimensionalhiddenstateareusedwithReLU
activationfunctionbetweenthem,followedbylayernormalizationblockatthehead. Ontheother
hand,fordCAEwiththelearningobjectivepresentedinEq. 5,weutilizeadeeperencoderstructure
whichcontainsthreefullyconnectedlayerswithReLUactivationfunction. Inaddition,dCAEtakes
atimesteptasaninputaswellasaglobalstates . Wesetepisodiclatentdimensiondim(x)=4as
t
Zhengetal.(2021).
FC
LayerNorm ReLU FC FC FC
𝑠 FC 𝑥 FC 𝑓 𝜓(∙) 𝐻ഥ 𝑠 ReLU 𝑥 ReLU ReLU 𝑠ҧ
ReLU FC FC
ReLU
FC ReLU ReLU
FC FC 𝑡 FC
𝑓 𝜙(∙) 𝑡 𝑓 𝜙(∙) 𝑓 𝜓(∙)
𝐻ഥ
(a)EmbNet (b)dCAE
Figure12: Illustrationofnetworkstructures.
Foradecoderf ,bothEmbNetanddCAEutilizeathree-fullyconnectedlayerwithReLUactivation
ψ
functions. Differences are that EmbNet takes only x as input and utilizes the 128-dimensional
t
hiddenstatewhiledCAEtakesx andtasinputsandadoptsthe64-dimensionalhiddenstate. As
t
illustratedinFigure1(c),toreconstructglobalstates ,dCAEhastwoseparateheadswhilesharing
t
lowerpartsofnetworks;fstoreconstructs andfH topredictthereturnofs ,denotedasH . Figure
ϕ t ϕ t t
12illustratesnetworkstructuresofEmbNetanddCAE.TheconceptofsupervisedVAEsimilarto
EMUcanbefoundin(Leetal.,2018).
The reason behind avoiding probabilistic autoencoders such as variational autoencoder (VAE)
(Kingma&Welling,2013;Sohnetal.,2015)isthatthestochasticembeddingandthepriordistribu-
tioncouldhaveanadverseimpactonpreservingapairofx andH forgivenas . Inparticular,with
t t t
stochasticembedding,afixeds cangeneratediversex . Asaresult,itbreaksthepairofx andH
t t t t
forgivens ,whichmakesitdifficulttoselectavalidmemoryfromD .
t E
Fortraining,weperiodicallyupdatef andf withanupdateintervaloft inparalleltoMARL
ϕ ψ emb
training. Ateachtrainingphase,weuseM samplesoutofthecurrentcapacityofD ,whose
emb E
maximumcapacityis1million(1M),andbatchsizeofm isusedforeachtrainingstep. After
emb
updatingf ,everyx∈D needstobeupdatedwithupdatedf . Algorithm1showsthedetailsof
ϕ E ϕ
learningframeworkforf andf . Detailsofthetrainingprocedureforf andf alongwithMARL
ϕ ψ ϕ ψ
trainingarepresentedinAppendixE.2.
OtherNetworkStructureandHyperparameters
Foramixerstructure,weadoptQPLEX(Wangetal.,2020b)inbothEMU(QPLEX)andEMU(CDS)
andfollowthesamehyperparametersettingsusedintheirsourcecodes. Commonhyperparameters
relatedtoindividualQ-networkandMARLtrainingareadoptedbythedefaultsettingsofPyMARL
(Samvelyanetal.,2019).
16PublishedasaconferencepaperatICLR2024
Algorithm1TrainingAlgorithmforStateEmbedding
1: Parameter: learningrateα,numberoftrainingdatasetN,batchsizeB
2:
SampleTrainingdataset(s(i),H(i),t(i))N
i=1
∼D E,
3: Initializeweightsϕ,ψ ←0
4: fori=1to⌊N/B⌋do
5: Compute(x(j) =f ϕ(s(j)|t(j))i jB
=(i−1)B+1
6: Predictreturn(H¯(j) =fH(x(j)|t(i)))iB
ψ j=(i−1)B+1
7: Reconstructstate(s¯(j) =fs(x(j))|t(i))iB
ψ j=(i−1)B+1
8: ComputeLossL(ϕ,ψ)viaEq. 5
9: Updateϕ←ϕ−α∂L,ψ ←ψ−α∂L
∂ϕ ∂ψ
10: endfor
C.2 EXPERIMENTDETAILS
WeutilizePyMARL(Samvelyanetal.,2019)toexecuteallofthebaselinealgorithmswiththeir
open-sourcecodes,andthesamehyperparametersareusedforexperimentsiftheyarepresented
eitherinuploadedcodesorintheirmanuscripts.
Forageneralperformanceevaluation,wetestourmethodsonvariousmaps,whichrequireadifferent
levelofcoordinationaccordingtothemap’sdifficulties. Win-rateiscomputedwith160samples: 32
episodesforeachtrainingrandomseed,and5differentrandomseedsunlessdenotedotherwise.
Boththemeanandthevarianceoftheperformancearepresentedforallthefigurestoshowtheir
overallperformanceaccordingtodifferentseeds. Especiallyforafaircomparison,wesetn ,the
circle
numberoftrainingperasampledbatchof32episodesduringtraining,as1forallbaselinessince
someofthebaselinesincreasen =2asadefaultsettingintheircodes.
circle
Forperformancecomparisonwithbaselinemethods,weusetheircodeswithfine-tunedalgorithm
configurationforhyperparametersettingsaccordingtotheircodesandoriginalpaperifavailable. For
experimentsonSMAC,weusetheversionofstarcraft.pypresentedinRODE(Wangetal.,
2021)adoptingsomemodificationforcompatibilitywithQPLEX(Wangetal.,2020b). AllSMAC
experimentswereconductedonStarCraftIIversion4.10.0inaLinuxenvironment.
For Google research football task, we use the environmental code provided by (Kurach et al.,
2020). Intheexperiments,weconsiderthreeofficialscenariossuchasacademy_3_vs_1_with_keeper
(3_vs_1WK),academy_counterattack_easy(CA_easy),andacademy_counterattack_hard(CA_hard).
Inaddition,forcontrollingrcinEq. 10,thesamehyperparametersrelatedtocuriosity-based(Zheng
etal.,2021)ordiversity-basedexplorationChenghaoetal.(2021)areadoptedforEMU(QPLEX)
andEMU(CDS)aswellasforbaselinesEMCandCDS.Afterfurtherexperiments,wefoundthat
thecuriosity-basedrcfrom(Zhengetal.,2021)adverselyinfluencedsuperhardSMACtask,with
theexceptionofcorridorscenario. Furthermore,thediversity-basedexplorationfromChenghao
etal.(2021)ledtoadecreaseinperformanceonbotheasyandhardSMACmaps. Thus,wedecided
toexcludetheuseofrcforEMU(QPLEX)onthesuperhardSMACtaskandforEMU(CDS)on
theeasy/hardSMACmaps. EMUsettask-dependentδ valuesaspresentedinTable1. Forother
hyperparametersintroducedbyEMU,thesamevaluespresentedinTable8areusedthroughoutall
tasks. ForEMU(QPLEX)incorridor,δ =1.3e−5isusedinsteadofδ =1.3e−3.
Table1: Task-dependenthyperparameterofEMU.
Category δ
easy/hardSMACmaps 1.3e−5
superhardSMACmaps 1.3e−3
GRF 1.3e−3
17PublishedasaconferencepaperatICLR2024
C.3 DETAILSOFMARLTASKS
Inthissection,wespecifythedimensionofthestatespace,theactionspace,theepisodiclength,and
therewardofSMAC(Samvelyanetal.,2019)andGRF(Kurachetal.,2020).
InSMAC,theglobalstateofeachtaskofSMACincludestheinformationofthecoordinatesofall
agents,andfeaturesofbothalliedandenemyunits. Theactionspaceofeachagentconsistsofthe
movingactionsandattackingenemies,andthusitincreasesaccordingtothenumberofenemies.
Thedimensionsofthestateandactionspaceandtheepisodiclengthvaryaccordingtothetasksas
showninTable2. Forrewardstructure,weusedtheshapedreward,i.e.,thedefaultrewardsettingsof
SMAC,forallscenarios. Therewardisgivenwhendealingdamagetotheenemiesandgetbonuses
forwinningthescenario. Therewardisscaledsothatthemaximumcumulativereward,R ,that
max
canbeobtainedfromtheepisode,becomesaround20(Samvelyanetal.,2019).
Table2: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofSMAC
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
1c3s5z 270 15 180
3s5z 216 14 150
3s_vs_5z 68 11 250
5m_vs_6m 98 12 70
MMM2 322 18 180
6h_vs_8z 140 14 150
3s5z_vs_3s6z 230 15 170
corridor 282 30 400
InGRF,thestateofeachtaskincludesinformationoncoordinates,ballpossession,andthedirection
ofplayers,etc. ThedimensionofthestatespacediffersamongthetasksasinTable3. Theaction
ofeachagentconsistsofmovingdirections,differentwaystokicktheball,sprinting,intercepting
the ball and dribbling. The dimensions of the action spaces for each task are the same. Table 3
summarizesthedimensionoftheactionspaceandtheepisodiclength. InGRF,therecanbetwo
rewardmodes: oneis"sparsereward"andtheotheris"densereward."Insparserewardmode,agents
getapositivereward+1whenscoringagoalandget-1whenconcedingonetotheopponents. In
denserewardmode,agentscangetpositiverewardswhentheyapproachtoopponent’sgoal,butthe
maximumcumulativerewardisupto+1. Inourexperiments,weadoptsparserewardmode,andthus
themaximumreward,R is+1forGRF.
max
Table3: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofGRF
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
3_vs_1WK 26 19 150
CA_easy 30 19 150
CA_hard 34 19 150
C.4 INFRASTRUCTURE
ExperimentsforSMAC(Samvelyanetal.,2019)aremainlycarriedoutonNVIDIAGeForceRTX
3090GPU,andtrainingforthelongestexperimentsuchascorridorviaEMU(CDS)tookless
than18hours. Notethatwhentrainingisconductedwithn = 2,ittakesmorethanoneanda
circle
halftimeslonger. Trainingencoder/decoderstructureandupdatingD withupdatedf together
E ϕ
onlytooklessthan2secondsatmostincorridortask. Asweupdatef andf periodicallywith
ϕ ψ
t ,theadditionaltimerequiredforatrainableembedderiscertainlynegligiblecomparedtoMARL
emb
training.
18PublishedasaconferencepaperatICLR2024
D FURTHER EXPERIMENT RESULTS
D.1 NEWPERFORMANCEINDEX
Inthissection,wepresentthedetailsofanewperformanceindexcalledoverallwin-rate,µ¯ . For
w
example,letfi(s)bethetestwin-rateattrainingtimesofithseedrunandµi (t)representsthe
w w
timeintegrationoffi(s)untilt. Then,anormalizedoverallwin-rate,µ¯ ,canbeexpressedas
w w
1 1 (cid:88)n 1 1 (cid:88)n (cid:90) t
µ¯ (t)= µi (t)= fi(s)ds, (17)
w µ maxn i=1 w µ maxn i=1
0
w
whereµ =tandµ¯ ∈[0,1].
max w
] 𝑓𝑖=1(𝑠)
- 𝑤
[
n
iW
𝑓𝑖=2(𝑠)
t
𝑤
s e 𝜇𝑖 (𝑡)
T 𝑤
𝑠
Figure13: Illustrationofµi (t).
w
Figure13illustratestheconceptoftimeintegrationofwin-rate,i.e.,µi (t),toconstructtheoverall
w
win-rate,µ¯ . Byconsideringtheintegrationofwin-rateofeachseedcase,theperformancevariance
w
canbeconsidered,andthusµ¯ showsthetrainingefficiency(speed)aswellasthetrainingquality
w
(win-rate).
D.2 ADDITIONALEXPERIMENTRESULTS
InSection4.3,wepresentthesummaryofparametricstudiesonδwithrespecttovariouschoices
off . Toseethetrainingefficiencyandperformanceatthesametime,Table4and5presentthe
ϕ
overallwin-rateµ¯ accordingtotrainingtime. Weconducttheexperimentsfor5differentseedcases
w
andateachtestphase32sampleswereusedtoevaluatethewin-rate[%]. Notethatwediscardthe
componentofepisodicincentiverptoseetheperformancevariationsaccordingtoδandtypesoff
ϕ
moreclearly.
Table 4: µ¯ according to δ and design choice of embedding function on hard SMAC map,
w
3s_vs_5z.
Trainingtime
0.69 1.37 2.00
[mil]
δ random EmbNet dCAE random EmbNet dCAE random EmbNet dCAE
1.3e-7 0.033 0.051 0.075 0.245 0.279 0.343 0.413 0.443 0.514
1.3e-5 0.010 0.044 0.063 0.171 0.270 0.325 0.320 0.441 0.491
1.3e-3 0.034 0.043 0.078 0.226 0.270 0.357 0.381 0.439 0.525
1.3e-2 0.019 0.005 0.079 0.205 0.059 0.346 0.348 0.101 0.518
Table 5: µ¯ according to δ and design choice of embedding function on hard SMAC map,
w
5m_vs_6m.
Trainingtime
0.69 1.37 2.00
[mil]
δ random EmbNet dCAE random EmbNet dCAE random EmbNet dCAE
1.3e-7 0.040 0.117 0.110 0.287 0.397 0.397 0.577 0.690 0.701
1.3e-5 0.064 0.107 0.131 0.334 0.402 0.436 0.634 0.714 0.749
1.3e-3 0.040 0.080 0.064 0.333 0.377 0.363 0.646 0.687 0.677
1.3e-2 0.038 0.000 0.048 0.288 0.001 0.332 0.584 0.001 0.643
19PublishedasaconferencepaperatICLR2024
AsTable4and5illustratethatdCAEstructureforf ,whichconsidersthereconstructionlossof
ϕ
globalstatesasinEq. 5,showsthebesttrainingefficiencyinmostcases. For5m_vs_6mtaskwith
δ =1.3e−3,EmbNetachievesthehighestvalueamongf choicesintermsofµ¯ butfailstofind
ϕ w
optimalpolicyatδ =1.3e−2unlikeotherdesignchoices. Thisimpliesthatthereconstructionloss
ofdCAEfacilitatestheconstructionofasmootherembeddingspaceforD ,enablingtheretrieval
E
of memories within a broader range of δ values from the key state. Figure 15 and 16 show the
correspondinglearningcurvesofeachencoderstructurefordifferentδvalues. Alargeδvalueresults
inahigherperformancevariancethanthecaseswithsmallerδ,accordingtodifferentseedcases.
This is because a high value of δ encourages
exploratorymemoryrecall. Inotherwords,by
adjusting δ, we can control the level of explo-
rationsinceitcontrolswhethertorecallitsown
MC return or the highest value of other simi-
larstateswithinδ. Thus,withoutconstructing
smootherembeddingspaceasindCAE,learn-
ingwithexploratorymemoryrecallwithinlarge
δ canconvergetosub-optimalityasillustrated
bythecaseofEmbNetinFigure16(d). Figure 14: N¯ of all memories in D when
call E
δ =0.013accordingtodesignchoiceforf .
InFigure14whichshowstheaveragednumber ϕ
ofmemoryrecall(N¯ )ofallmemoriesinD ,
call E
N¯ ofEmbNetsignificantlyincreasesastrainingproceeds. Ontheotherhand,dCAEwasableto
call
preventthisproblemandrecalledthepropermemoriesintheearlylearningphase,achievinggood
trainingefficiency. Thus,embeddingwithdCAEcanleverageawideareaofmemoryinD and
E
becomesrobusttohyperparameterδ.
(a)δ=1.3e−7 (b)δ=1.3e−5 (c)δ=1.3e−3 (d)δ=1.3e−2
Figure15: Parametricstudiesforδon3s_vs_5zSMACmap.
(a)δ=1.3e−7 (b)δ=1.3e−5 (c)δ=1.3e−3 (d)δ=1.3e−2
Figure16: Parametricstudiesforδon5m_vs_6mSMACmap.
20PublishedasaconferencepaperatICLR2024
D.3 COMPARATIVEEVALUATIONONADDITIONALSTARCRAFTIIMAPS
Figure17presentsacomparativeevaluationofEMUwithbaselinealgorithmsonadditionalSMAC
maps. AdoptingEMUshowsperformancegaininvarioustasks.
Figure17: PerformancecomparisonofEMUagainstbaselinealgorithmsonadditionalSMACmaps.
D.4 COMPARISONOFEMUWITHMAPPOONSMAC
Inthissubsection,wecomparetheEMUwithMAPPO(Yuetal.,2022)onselectedSMACmaps.
Figure18showstheperformanceinsixSMACmaps: 1c3s5z,3s_vs_5z,5m_vs_6m,MMM2,
6h_vs_8z and 3s5z_vs_3s6z. Similar to the previous performance evaluation in Figure 4,
Win-rateiscomputedwith160samples: 32episodesforeachtrainingrandomseedand5different
randomseeds.Also,forMAPPO,scenario-dependenthyperparametersareadoptedfromtheiroriginal
settingsintheuploadedsourcecode.
FromFigure18,wecanseethatEMUperformsbetterthanMAPPOwithanevidentgap. Although
after extensive training MAPPO showed a comparable performance against off-policy algorithm
initsoriginalpaper(Yuetal.,2022),withinthesametrainingtimestepusedforourexperiments,
we found that MAPPO suffers from local convergence in super hard SMAC tasks such as MMM2
and 3s5z_vs_3s6z as shown in Figure 18. Only in 6h_vs_8z, MAPPO shows comparable
performancetoEMU(QPLEX)withhigherperformancevarianceacrossdifferentseeds.
Figure18: PerformancecomparisonwithMAPPOonselectedSMACmaps.
21PublishedasaconferencepaperatICLR2024
D.5 ADDITIONALPARAMETRICSTUDY
In this subsection, we conduct an additional parametric study to see the effect of key hyperpa-
rameter δ. Unlike the previous parametric study on Appendix D.2, we adopt both dCAE em-
bedding network for f and episodic reward. For evaluation, we consider three GRF tasks such
ϕ
as academy_3_vs_1_with_keeper (3_vs_1WK), academy_counterattack_easy
(CA-easy),andacademy_counterattack_hard(CA-hard);andonesuperhardSMAC
map such as 6h_vs_8z. For each task to evaluate EMU, four δ values, such as δ = 1.3e−7,
1
δ = 1.3e−5, δ = 1.3e−3, andδ = 1.3e−2, areconsidred. Here, tocomputethewin-rate, 160
2 3 4
samples (32 episodes for each training random seed and 5 different random seeds) are used for
3_vs_1WKand6h_vs_8zwhile100samples(20episodesforeachtrainingrandomseedand5
differentrandomseeds)areusedforCA-easyandCA-hard. NotethatCDSandEMU(CDS)
utilize the same hyperparameters, and EMC and EMU (QPLEX) use the same hyperparameters
withoutacuriosityincentivepresentedinZhengetal.(2021)asthemodelwithoutitshowedthe
betterperformancewhenutilizingepisodiccontrol.
(a)3_vs_1WK(GRF) (b)CA-easy(GRF) (c)CA-hard(GRF) (d)6h_vs_8z(SMAC)
Figure19: ParametricstudiesforδonvariousGRFmapsandsuperhardSMACmap.
Inallcases,EMUwithδ =1.3e−3showsthebestperformance. Thetasksconsideredhereareall
3
complexmulti-agenttasks,andthusadoptingapropervalueofδbenefitstheoverallperformance
andachievesthebalancebetweenexplorationandexploitationbyrecallingthesemanticallysimilar
memoriesfromepisodicmemory. Theoptimalvalueofδ isconsistentwiththedeterminationlogic
3
onδinamemoryefficientwaypresentedinAppendixF.
D.6 ADDITIONALPARAMETRICSTUDYONλ
rcon
Additionally,weconductaparametricstudyforλ inEq. 5. Foreachtask,EMUwithfiveλ
rcon rcon
values,suchasλ = 0.01,λ = 0.1,λ = 0.5,λ = 1.0andλ = 10,are
rcon,0 rcon,1 rcon,2 rcon,3 rcon,4
evaluated. Here,tocomputethewin-rateofeachcase,160samples(32episodesforeachtraining
randomseedand5differentrandomseeds)areused. FromFigure20,wecanseethatbroadrangeof
(a)3s5z(SMAC) (b)3s_vs_5z(SMAC)
Figure20: Parametricstudyforλ .
rcon
(cid:8) (cid:9)
λ ∈ 0.1,0.5,1.0 workwellingeneral. However,withlargeλ asλ = 10,wecan
rcon rcon rcon,4
observethatsomeperformancedegradationattheearlylearningphasein3s5ztask. Thisresultis
inlinewiththelearningtrendsofCase1andCase2of3s5zinFigure23,whichdonotconsider
predictionlossandonlytakeintoaccountthereconstructionloss. Thus,consideringbothprediction
lossandreconstructionlossasCase4inEq. 5withproperλ isessentialtooptimizetheoverall
rcon
learningperformance.
22PublishedasaconferencepaperatICLR2024
D.7 ADDITIONALABLATIONSTUDYINGRF
(a)3_vs_1WK(GRF) (b)CA-easy(GRF)
Figure21: AblationstudiesonepisodicincentiveonGRFtasks.
Inthissubsection,weconductadditionalablationstudiesviaGRFtaskstoseetheeffectofepisodic
incentive. Again,EMU(CDS-No-EI)ablatesepisodicincentivefromEMU(CDS)andutilizesthe
conventionalepisodiccontrolpresentedinEq.3instead.Again,EMU(CDS-No-SE)ablatessemantic
embeddingbydCAEandadoptsrandomprojectionwithepisodicincentiverp. Inbothtasks,utilizing
episodicmemorywiththeproposedembeddingfunctionimprovestheoverallperformancecompared
totheoriginalCDSalgorithm. Byadoptingepisodicincentivesinsteadofconventionalepisodic
control,EMU(CDS)achievesbetterlearningefficiencyandrapidlyconvergestooptimalpolicies
comparedtoEMU(CDS-No-EI).
D.8 ADDITIONALABLATIONSTUDYONEMBEDDINGLOSS
Inourcase,theautoencoderusesthereconstructionlosstoenforcetheembeddedrepresentationx
tocontainthefullinformationoftheoriginalfeature,s. Weareadding(H −fH(f (s |t)|t))2to
t ψ ϕ t
guidetheembeddedrepresentationtobeconsistenttoH ,aswell,whichworksasaregularizerto
t
theautoencoder. Therefore,fH isusedinEq. 5topredicttheobservedH fromD asapartofthe
ψ t E
semanticregularizationeffort.
BecauseH isdifferentfromfH(x ),theeffortofminimizingtheirdifferencebecomestheregularizer
t ψ t
creatingagradientsignaltolearnψandϕ. Theupdateofϕresultsintheupdatedxinfluencedbythe
regularization. Notethatweupdateϕthroughthebackpropagationofψ.
ThecaseofL(ϕ,ψ)=||s −fs(f (s |t)|t)||2occurswhenλ becomesrelativelymuchhigher
t ψ ϕ t 2 rcon
than1,whichmakes(H −fH(f (s |t)|t))2becomesineffective. Inotherwords,whenλ inEq.
t ψ ϕ t rcon
5becomesrelativelymuchhigherthan1,(H −fH(f (s |t)|t))2becomesineffective.
t ψ ϕ t
ThecaseofL(ϕ,ψ)=(H −fH(f (s |t)|t))2occurswhenthescalefactorλ becomesrelatively
t ψ ϕ t rcon
muchsmallerthan1,whichmakes(H −fH(f (s |t)|t))2becomeadominantfactor. Weconduct
t ψ ϕ t
ablationstudiesconsideringfourcasesasfollows:
• Case1: L(ϕ,ψ)=||s −fs(f (s ))||2,presentedinFigure22(a)
t ψ ϕ t 2
• Case2: L(ϕ,ψ)=||s −fs(f (s |t)|t)||2,presentedinFigure22(b)
t ψ ϕ t 2
• Case3: L(ϕ,ψ)=(H −fH(f (s |t)|t))2,presentedinFigure22(c)
t ψ ϕ t
• Case 4: L(ϕ,ψ) = (H −fH(f (s |t)|t))2 +λ ||s −fs(f (s |t)|t)||2, i.e., Eq. 5,
t ψ ϕ t rcon t ψ ϕ t 2
presentedinFigure22(d)
Wevisualizetheresultoft-SNEof50Ksamplesx∈D outof1Mmemorydatatrainedbyvarious
E
loss functions: The task was 3s_vs_5z of SMAC as in Figure 2 and the training for all models
proceedsfor1.5miltrainingsteps. Case1andCase2showedirregularreturndistributionacrossthe
embeddingspace. Inthosetwocases,therewasnoconsistentpatternintherewarddistribution. Case
3withonlyreturnpredictioninthelossshowedbetterpatternscomparedtoCase1and2butsome
featuresarenotclusteredwell. Wesuspectthattheconsistentstaterepresentationalsocontributes
tothereturnprediction. Case4ofoursuggestedlossshowedthemostregularpatterninthereturn
distributionarrangingthelow-returnstatesasaclusterandthestateswithdesirablereturnsasanother
23PublishedasaconferencepaperatICLR2024
(a)Loss(case1) (b)Loss(case2) (c)Loss(case3) (d)Loss(case4)
Figure22: t-SNEofsampledembeddingx∈D trainedbydCAEwithvariouslossfunctionsin
E
3s_vs_5zSMACmap. Colorsfromredtopurplerepresentfromlowreturntohighreturn.
cluster. InFigure23,Case4showsthebestperformanceintermsofbothlearningefficiencyand
terminalwin-rate.
(a)3s5z(SMAC) (b)3s_vs_5z(SMAC)
Figure23:Performancecomparisonofvariouslossfunctions
fordCAE.
24PublishedasaconferencepaperatICLR2024
D.9 ADDITIONALABLATIONSTUDYONSEMANTICEMBEDDING
Tofurtherunderstandtheroleofsemanticembedding,weconductadditionalablationstudiesand
presentthemwiththegeneralperformanceofotherbaselinemethods. Again,EMU(CDS-No-SE)
ablatessemanticembeddingbydCAEandadoptsrandomprojectioninstead,alongwithepisodic
incentiverp.
Figure24: PerformancecomparisonofEMUagainstbaselinealgorithmsonthreeeasyandhard
SMACmaps: 1c3s5z,3s_vs_5z,and5m_vs_6m,andthreesuperhardSMACmaps: MMM2,
6h_vs_8z,and3s5z_vs_3s6z.
Figure 25: Performance comparison of EMU against baseline algorithms on Google Research
Football.
Forrelativelyeasytasks,EMU(QPLEX-No-SE)andEMU(CDS-No-SE)showcomparableperfor-
manceatfirstbuttheyconvergeonsub-optimalpolicyinmosttasks. Especially,thischaracteristicis
wellobservedinthecaseofEMU(CDS-No-SE).Aslargesizeofmemoriesarestoredinanepisodic
bufferastraininggoeson,theprobabilityofrecallingsimilarmemoriesincreases. However,with
randomprojection,semanticallyincoherentmemoriescanberecalledandthusitcanadverselyaffect
thevalueestimation. Wedeemthisisthereasonfortheconvergenceonsuboptimalpolicyinthecase
ofEMU(No-SE).Thuswecanconcludethatrecallingsemanticallycoherentmemoryisanessential
componentofEMU.
25PublishedasaconferencepaperatICLR2024
D.10 ADDITIONALABLATIONONrc
InEq.10, weintroducerc asanadditionalrewardwhichmayencourageexploratorybehavioror
coordination. The reason we introduce rc is to show that EMU can be used in conjunction with
anyformofincentiveencouragingfurtherexploration. Ourmethodmaynotbestronglyeffective
untilsomedesiredstatesarefound,althoughithasexploratorybehaviorviatheproposedsemantic
embeddings,controlledbyδ. Untilthen,suchincentivescouldbebeneficialtofinddesiredorgoal
states. Figures26-27showtheablationstudyofwithandwithoutrc,andthecontributionofrc is
limitedcomparedtorp.
(a)3s_vs_5z (b)5m_vs_6m (c)3s5z_vs_3s6z
Figure26: AblationstudiesonrcinSMACtasks.
(a)3s_vs_5z (b)5m_vs_6m (c)3s5z_vs_3s6z
Figure27: AblationstudiesonrcinSMACtasks.
D.11 COMPARISONOFEPISODICINCENTIVEWITHEXPLORATORYINCENTIVE
Inthissubsection,wereplacetheepisodicincentivewithanotherexploratoryincentive,introduced
by(Henaffetal.,2022). In(Henaffetal.,2022),theauthorsextendthecount-basedepisodicbonuses
tocontinuousspacesbyintroducingepisodicellipticalbonusesforexploration. Inthisconcept,a
highrewardisgivenwhenthestateprojectedintheembeddingspaceisdifferentfromtheprevious
stateswithinthesameepisode. Indetail,withagivenfeatureencoderϕ,theellipticalbonusb at
t
timesteptiscomputedasfollows:
b =ϕ(s )TC−1ϕ(s ) (18)
t t t t
whereC−1 isaninversecovariancematrixwithaninitialvalueofC−1 =1/λ I. Here,λ is
t t=0 e3b e3b
acovarianceregularizer. Forupdateinversecovariance,theauthorssuggestedacomputationally
efficientupdateas
1
C−1 =C−1− uuT (19)
t+1 t 1+b
t+1
whereu=C−1ϕ(s ). Then,thefinalrewardr¯ withepisodicellipticalbonusesb isexpressedas
t t+1 t t
r¯ =r +β b (20)
t t e3b t
whereβ andr areacorrespondingscalefactorandexternalrewardgivenbytheenvironment,
e3b t
respectively.
Forthiscomparison,weutilizethedCAEstructureasastateembeddingfunctionϕ. Foramixer,
QPLEX (Wang et al., 2020b) is adopted for all cases, and we denote the case with an elliptical
incentive instead of the proposed episodic incentive as QPLEX (SE+E3B). Figure 28 illustrates
26PublishedasaconferencepaperatICLR2024
Figure28: PerformancecomparisonwithellipticalincentiveonselectedSMACmaps.
theperformanceofadoptinganellipticalincentiveforexplorationinsteadoftheproposedepisodic
incentive. QPLEX (SE+E3B) uses the same hyperparameters with EMU (SE+EI) and we set
λ =0.1accordingtoHenaffetal.(2022).
e3b
AsillustratedbyFigure28,adoptinganellipticalincentivepresentedby(Henaffetal.,2022)instead
of an episodic incentive does not give any performance gain and even adversely influences the
performancecomparedtoQPLEX.Itseemsthataddingexcessivesurprise-basedincentivescanbea
disturbanceinMARLtaskssincefindinganewstateitselfdoesnotguaranteebettercoordination
among agents. In MARL, agents need to find the proper combination of joint action in a given
similarobservationswhenfindinganoptimalpolicy. Ontheotherhand,inhigh-dimensionalpixel-
based single-agent tasks such as Habitat (Ramakrishnan et al., 2021), finding a new state itself
canbebeneficialinpolicyoptimization. Fromthis,wecannotethatadoptingacertainalgorithm
fromasingle-agentRLcasetoMARLcasemayrequireamodificationoradjustmentwithdomain
knowledge.
As a simple tuning, we conduct parametric study for β = {0.01,0.1} to adjust magnitude of
e3b
incentiveofE3B.Figure29illustratestheresults. InFigure29,QPLEX(SE+E3B)withβ =0.01
e3b
showsabetterperformancethanthecasewithβ = 0.1andcomparableperformancetoEMC
e3b
in5m_vs_6m. However,EMUwiththeproposedepisodicincentiveshowsthebestperformance.
Fromthiscomparison,wecanseethatincentivesproposedbypreviousworkneedtobeadjusted
Figure29: PerformancecomparisonwithanellipticalincentiveonselectedSMACmaps.
accordingtothetypeoftasks,asitwasdoneinEMC(Zhengetal.,2021). Ontheotherhand,with
theproposedepisodicincentivewedonotneedsuchhyperparameter-scaling,allowingmuchmore
flexibleapplicationacrossvarioustasks.
27PublishedasaconferencepaperatICLR2024
D.12 ADDITIONALTOYEXPERIMENTANDAPPLICABILITYTESTS
Inthissection,weconductadditionalexperimentsonthedidacticexamplepresentedby(Zhengetal.,
2021)toseehowtheproposedmethodwouldbehaveinasimplebutcomplexcoordinationtask.
Additionally,bydefiningR todefinethedesirabilitypresentedinDefinition1,wecanextend
thr
EMUtoasingle-agentRLtask,whereastrictgoalisnotdefinedingeneral.
DidacticexperimentonGridworldWeadoptthedidacticexamplesuchasgridworldenvironment
from(Zhengetal.,2021)todemonstratethemotivationandhowtheproposedmethodcanovercome
theexistinglimitationsoftheconventionalepisodiccontrol. Inthistask,twoagentsingridworld(see
Figure30(a))needtoreachtheirgoalstatesatthesametimetogetarewardr =10andifonlyone
arrivesfirst,theygetapenaltywiththeamountof−p. Pleasereferto(Zhengetal.,2021)forfurther
details.
Wall
G G
Visible Zone
(a)Gridworld (b)Performanceevaluation(p=2)
Figure30: Didacticexperimentsongridworld.
Toseethesoleeffectoftheepisodiccontrol,wediscardthecuriosityincentivepartofEMC,andfor
afaircomparison,wesetthesameexplorationrateofϵ-greedywithT =200K forallalgorithms.
ϵ
Weevaluatethewin-ratewith180samples(30episodesforeachtrainingrandomseedand6different
random seeds) at each training time. Notably, adopting episodic control with a naive utilization
suffersfromlocalconvergence(seeQPLEXandEMC(QPLEX)inFigure30(b)),eventhoughit
expediteslearningefficiencyattheearlytrainingphase. Ontheotherhand,EMUshowsmorerobust
performanceunderdifferentseedcasesandachievesthebestperformancebyanefficientanddiscreet
utilizationofepisodicmemories.
ApplicabilitytesttosingleagentRLtaskWefirstneedtodefineR valuetoeffectivelyapply
thr
EMU to a single-agent task where a goal of an episode is generally not strictly defined, unlike
cooperativemulti-agenttaskswithasharedcommongoal.
Inasingle-agenttaskwheretheactionspaceiscontinuoussuchasMuJoCo(Todorovetal.,2012),
theactor-criticmethodisoftenadopted. EfficientmemoryutilizationofEMUcanbeusedtotrain
thecriticnetworkandthusindirectlyinfluencepolicylearning,unlikegeneralcooperativeMARL
taskswherevalue-basedRLisoftenconsidered.
WeimplementEMUontopofTD3andusetheopen-sourcecodepresentedin(Fujimotoetal.,2018).
Webegintotrainthemodelaftersufficientdataisstoredinthereplaybufferandconduct6timesof
trainingperepisodewith256mini-batches. Notethatthisisdifferentfromthedefaultsettingsof
RLtraining,whichconductstrainingateachtimestep. Ourmodifiedsettingaimstoseetheeffecton
thesampleefficiencyoftheproposedmodel. Theperformanceofthetrainedmodelisevaluatedat
every50ktimesteps.
We use the same hyperparameter settings as in MARL task presented in Table 8 except for the
update interval, t = 100K according to large episodic timestep in single-RL compared to
emb
MARLtasks. Itisworthmentioningthatadditionalcustomizedparametersettingsforsingle-agent
tasks may further improve the performance. In our evaluation, three single-agent tasks such as
Hopper-v4,Walker2D-v4andHumanoid-v4areconsidered,andFigure32illustrateseach
task. Here, δ = 1.3e−5 is used for Hopper-v4 and Walker2D-v4, and δ = 1.3e−3 is
2 3
usedforHumanoid-v4asHumanoid-v4taskcontainsmuchhigherstatedimensionspaceas
376-dimension. PleaserefertoTodorovetal.(2012)foradetaileddescriptionoftasks.
28PublishedasaconferencepaperatICLR2024
(a)Hopper-v4 (b)Walker2D-v4 (c)Humanoid-v4
Figure31: IllustrationofMuJoCoscenarios.
(a)Performance(Hopper) (b)Performance(Walker2D) (c)Performance(Humanoid)
Figure32: Applicabilitytesttosingleagenttask(R =500).
thr
In Figure 32, EMU (TD3) shows the performance improvement compared to the original TD3.
Thankstosemanticallysimilarmemoryrecallandepisodicincentive,statesdeemeddesirablecould
havehighvalues,andtrainedpolicyisencouragedtovisitthemmorefrequently. Asaresult,EMU
(TD3)showsthebetterperformance. Interestingly,understatedimensionasHumanoid-v4task,
TD3andEMU(TD3)showadistinctperformancegapintheearlytrainingphase. Thisisbecause,
in a task with a high-dimensional state space, it is hard for a critic network to capture important
featuresdeterminingthevalueofagivenstate. Thus,ittakeslongertoestimatestatevalueaccurately.
However, with the help of semantically similar memory recall and error compensation through
episodicincentive,acriticnetworkinEMU(TD3)canaccuratelyestimatethevalueofthestatemuch
fasterthantheoriginalTD3,leadingtofasterpolicyoptimization.
UnlikecooperativeMARLtasks,single-RLtasksnormallydonothaveadesirabilitythreshold. Thus,
onemayneedtodetermineR basedondomainknowledgeorapreferenceforthelevelofreturn
thr
tobedeemedsuccessful. Figure33presentsaperformancevariationaccordingtoR .
thr
(a)Hopper-v4 (b)Walker2d-v4
Figure33: ParametricstudyonR .
thr
WhenwesetR =1000inWalker2dtask,desirabilitysignalisrarelyobtainedcomparedtothe
thr
casewithR = 500intheearlytrainingphase. Thus,EMUwithR = 500showsthebetter
thr thr
performance. However,bothcasesofEMUshowbetterperformancecomparedtotheoriginalTD3.
InHoppertask,bothcasesofR =500andR =1000showthesimilarperformance. Thus,
thr thr
29PublishedasaconferencepaperatICLR2024
whendeterminingR ,itcanbebeneficialtosetasmallvalueratherthanalargeonethatcanbe
thr
hardlyobtained.
AlthoughsettingasmallR doesnotrequiremuchdomainknowledge,apossibleoptiontodetour
thr
thisisaperiodicupdateofdesirabilitybasedontheaveragereturnvalueH(s)inalls∈D . Inthis
E
way,acertainstatewithlowreturnwhichwasoriginallydeemedasdesirablecanbereevaluatedas
undesirableastrainingproceeds. Theepisodicincentiveisnotfurthergiventothoseundesirable
states.
Scalability to image-based single-agent RL task Although MARL tasks already contain high-
dimensionstatespacesuchas322-dimensioninMMM2and282-dimensionincorridor,image-
basedsingleRLtasks,suchasAtariBellemareetal.(2013)game,oftenaccompanyhigherstate
spacessuchas[210x160x3]for"RGB"and[210x160]for"grayscale". Weusethe"grayscale"type
forthefollowingexperiments. ForthedetailsofthestatespaceinMARLtask,pleaseseeAppendix
C.3.
Inanimage-basedtask,storingallstatevaluestoupdateallthekeyvaluesinD asf updatescanbe
E ϕ
memory-inefficient,andasemanticembeddingfromoriginalstatesmaybecomeoverheadcompared
tothecasewithoutit. Insuchcase,onemayresorttoapre-trainedfeatureextractionmodelsuch
asResNetmodelprovidedbytorch-visioninacertainamountfordimensionreductiononly,before
passingthroughtheproposedsemanticembedding. Thefeatureextractionmodelaboveisnotan
objectoftraining.
As an example, we implement EMU on the top of DQN model and compare it with the original
DQNonAtaritask. FortheEMU(DQN),weadoptsomepartofpre-trainedResNet18presented
bytorch-visionfordimensionalityreduction,beforepassinganinputimagetosemanticembedding.
At each epoch, 320 random samples are used for training in Breakout task, and 640 random
samples are used in Alien task. The same mini-batch size of 32 is used for both cases. For
f training, the same parameters presented in Table 8 are adopted except for the t = 10K
ϕ emb
consideringthetimestepofsingleRLtask. Wealsousethesameδ =1.3e−5andsetR =50
2 thr
forBreakoutandR =40forAlien,respectively. PleaserefertoBellemareetal.(2013)and
thr
https://gymnasium.farama.org/environments/atarifortaskdetails. AsinFigure
34,wefoundaperformancegainbyadoptingEMUonhigh-dimensionalimage-basedtasks.
(a)Breakout (b)Performance(Breakout) (c)Alien (d)Performance(Alien)
Figure34: Image-basedsingle-RLtaskexample.
30PublishedasaconferencepaperatICLR2024
E TRAINING ALGORITHM
E.1 MEMORYCONSTRUCTION
Duringthecentralizedtraining,wecanaccesstheinformationonwhethertheepisodicreturnreaches
thehighestreturnR orthresholdR ,i.e.,defeatingallenemiesinSMACorscoringagoalin
max thr
GRF.WhenstoringinformationtoD ,bythedefinitionpresentedDefinition. 1,wesetξ(s)=1for
E
∀s∈T .
ξ
Forefficientmemoryconstruction,wepropagatethedesirabilityofthestatetoasimilarstatewithin
thethresholdδ. Withthisdesirabilitypropagation,similarstateshaveanincentiveforavisit. In
addition,onceamemoryissavedinD ,thememoryispreserveduntilitbecomesobsolete(theoldest
E
memorytoberecalled). Whenadesirablestateisfoundneartheexistingsuboptimalmemorywithin
δ,wereplacethesuboptimalmemorywiththedesirableone,whichgivestheeffectofamemoryshift
tothedesirablestate. Algorithm2presentsthememoryconstructionwiththedesirabilitypropagation
andmemoryshift.
Algorithm2Episodicmemoryconstruction
1: ξ T: Optimalityoftrajectory
2: T ={s 0,a 0,r 0,s 1,...,s T}: Episodictrajectory
3: InitializeR t =0
4: fort=T to0do
5: Computex t =f ϕ(s t)andy t =(x t−µˆ x)/σˆ x
6: pickthenearestneighborxˆ t ∈D E andgetyˆ t.
7: if||yˆ t−y t|| 2 <δthen
8: N call(xˆ t)←N call(xˆ t)+1
9: ifξ T ==1then
10: N ξ(xˆ t)←N ξ(xˆ t)+1
11: endif
12: ifξ t ==0andξ T ==1then
13: ξ t ←ξ T ▷desirabilitypropagation
14: xˆ t ←x t,yˆ t ←y t,sˆ t ←s t ▷memoryshift
15: Hˆ t ←R t
16: else
17: ifHˆ t <R tthenHˆ t ←R t
18: endif
19: endif
20: else
21: AddmemoryD E ←(x t,y t,R t,s t,ξ t)
22: endif
23: endfor
Formemorycapacityandlatentdimension, weusedthesamevaluesasZhengetal.(2021), and
Table6showsthesummaryofhyperparameterrelatedtoepisodicmemory.
Table6: ConfigurationofEpisodicMemory.
Configuration Value
episodiclatentdimension,dim(x) 4
episodicmemorycapacity 1M
ascalefactor,λ
0.1
(forconventionalepisodiccontrolonly)
ThememoryconstructionforEMUseemstorequireasignificantlylargememoryspace,especially
forsavingglobalstatess. However,D usesCPUmemoryinsteadofGPUmemory,andthememory
E
requiredfortheproposedembedderstructureisminimalcomparedtothememoryusageoforiginal
31PublishedasaconferencepaperatICLR2024
Table7: AdditionalCPUmemoryusagetosaveglobalstates.
CPUmemoryusage(1Mdata)
SMACtask
(GiB)
5m_vs_6m 0.4
3s5z_vs_3s6z 0.9
MMM2 1.2
RLtraining(<1%). Thus, amemoryburdenduetoatrainableembeddingstructureisnegligible.
Table7presentsexamplesofCPUmemoryusagetosaveglobalstatess∈D .
E
E.2 OVERALLTRAININGALGORITHM
In this section, we present details of the overall MARL training algorithm including training of
f . AdditionalhyperparametersrelatedtoAlgorithm1toupdateencoderf anddecoderf are
ϕ ϕ ψ
presentedinTable8. NotethatvariablesN andBareconsistentwithAlgorithm1.
Table8: EMUHyperparametersforf andf training.
ϕ ψ
Configuration Value
ascalefactorofreconstructionloss,λ 0.1
rcon
updateinterval,t 1K
emb
trainingsamples,N 102.4K
batchsizeoftraining,B 1024
Algorithm3presentsthepseudo-codeofoveralltrainingforEMU.InAlgorithm3,networkparame-
tersrelatedtoamixerandindividualQ-networkaredenotedasθ,anddoubleQ-learningwithtarget
networkisadoptedasotherbaselinemethods(Rashidetal.,2018;2020;Wangetal.,2020b;Zheng
etal.,2021;Chenghaoetal.,2021).
Algorithm3EMU:EfficientepisodicMemoryUtilizationforMARL
1: D: Replaybuffer
2: D E: Episodicbuffer
3: Qi: IndividualQ-networkofnagents
θ
4: M: BatchsizeofRLtraining
5: Initializenetworkparametersθ,ϕ,ψ
6: whilet env ≤t maxdo
7: Interactwiththeenvironmentviaϵ-greedypolicybasedon[Qi]n andgetatrajectoryT.
θ i=1
8: RunAlgorithm2toupdateD E withT
9: AppendT toD
10: fork =1ton do
circle
11: GetM sampletrajectories[T]M ∼D
i=1
12: RunMARLtrainingalgorithmusing[T]M i=1andD E,toupdateθwithEq.10
13: endfor
14: ift env modt emb==0then
15: RunAlgorithm1toupdateϕ,ψ
16: Updateallx∈D E withupdatedf ϕ
17: endif
18: endwhile
Here,anyCTDEtrainingalgorithmcanbeadoptedforMARLtrainingalgorithminline 12in
Algorithm3. AswementionedinSectionC.4,trainingoff andf andupdatingallx∈D only
ϕ ψ E
32PublishedasaconferencepaperatICLR2024
takeslessthantwosecondsatmostunderthetaskwithlargeststatedimensionsuchascorridor.
Thus,thecomputationburdenfortrainableembedderisnegligiblecomparedtotheoriginalMARL
training.
F MEMORY UTILIZATION
Aremainingissueinutilizingepisodicmemoryishowtodetermineaproperthresholdvalueδ in
Eq. 1. Notethatthisδisusedforbothupdatingthememoryandrecallingthememory. Onesimple
option is determining δ based on prior knowledge or experience, such as hyperparameter tuning.
Instead,inthissection,wepresentamorememory-efficientwayforδselection. Whencomputing
||xˆ−x|| < δ, thesimilarityiscomparedelementwisely. However, thissimilaritymeasureputs
2
adifferentweightoneachdimensionofxsinceeachdimensionofxcouldhaveadifferentrange
of distribution. Thus, instead of x, we utilize the normalized value. Let us define a normalized
embeddingywiththestatisticalmean(µ )andvariance(σ )ofxas
x x
y =(x−µ )/σ . (21)
x x
Here, the normalization is conducted for each dimension of x. Then, the similarity measure via
||yˆ−y|| < δ with Eq. 21 puts an equal weight to each dimension, as y has a similar range of
2
distributionineachdimension. Inaddition,anaffineprojectionofEq. 21maintainsthecloseness
oforiginalx-distribution,andthuswecansafelyutilizey-distributioninsteadofx-distributionto
measurethesimilarity.
Inaddition,ydefinedinEq. 21nearlyfollowsthenormaldistribution,althoughitdoesnotstrictly
followit. ThisisduetothefactthatthememorizedsamplesxinD donotoriginatefromthesame
E
distribution,noraretheyuncorrelated,astheycanstemfromthesameepisode. However,wecan
achieveanapproximatecoverageofthemajorityofthedistribution,specifically3σ inbothpositive
y
andnegativedirectionsofy,bysettingδas
(2×3σ )dim(y)
δ ≤ y . (22)
M
Forexample,whenM = 1e6 anddim(y) = 4,ifσ ≈ 1thenδ ≤ 0.0013. Thisisthereasonwe
y
selectδ =0.0013fortheexploratorymemoryrecall.
33