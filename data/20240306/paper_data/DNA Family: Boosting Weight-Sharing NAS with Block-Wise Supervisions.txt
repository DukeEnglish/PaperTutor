IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 1
DNA Family: Boosting Weight-Sharing NAS with
Block-Wise Supervisions
Guangrun Wang†, Changlin Li†, Liuchun Yuan, Jiefeng Peng, Xiaoyu Xian,
Xiaodan Liang, Xiaojun Chang Senior Member, IEEE, and Liang Lin∗ Senior Member, IEEE
Abstract—NeuralArchitectureSearch(NAS),aimingatautomaticallydesigningneuralarchitecturesbymachines,hasbeen
consideredakeysteptowardautomaticmachinelearning.OnenotableNASbranchistheweight-sharingNAS,whichsignificantly
improvessearchefficiencyandallowsNASalgorithmstorunonordinarycomputers.Despitereceivinghighexpectations,thiscategory
ofmethodssuffersfromlowsearcheffectiveness.Byemployingageneralizationboundednesstool,wedemonstratethatthedevil
behindthisdrawbackistheuntrustworthyarchitectureratingwiththeoversizedsearchspaceofthepossiblearchitectures.Addressing
thisproblem,wemodularizealargesearchspaceintoblockswithsmallsearchspacesanddevelopafamilyofmodelswiththe
distillingneuralarchitecture(DNA)techniques.Theseproposedmodels,namelyaDNAfamily,arecapableofresolvingmultiple
dilemmasoftheweight-sharingNAS,suchasscalability,efficiency,andmulti-modalcompatibility.OurproposedDNAmodelscanrate
allarchitecturecandidates,asopposedtopreviousworksthatcanonlyaccessasub-searchspaceusingheuristicalgorithms.
Moreover,underacertaincomputationalcomplexityconstraint,ourmethodcanseekarchitectureswithdifferentdepthsandwidths.
Extensiveexperimentalevaluationsshowthatourmodelsachievestate-of-the-arttop-1accuracyof78.9%and83.6%onImageNetfor
amobileconvolutionalnetworkandasmallvisiontransformer,respectively.Additionally,weprovidein-depthempiricalanalysisand
insightsintoneuralarchitectureratings.Codesavailable:https://github.com/changlin31/DNA.
IndexTerms—Block-wiseLearning,NeuralArchitectureSearch,GeneralizationBoundedness,VisionTransformer
✦
1 INTRODUCTION blocks with small search spaces. Specifically, we divide a
Neural architecture search (NAS) [1], aiming to replace neuralarchitectureintoblocks(seeFig.1(a))andtraineach
human experts with machines in designing neural archi- block separately. In this way, the architecture candidates in
tectures, is widely anticipated. Typical works included re- ablockreduceexponentiallycomparedtothatinthewhole
inforcementlearningapproaches[2],[3],evolutionaryalgo- search space, allowing more reliable architecture ratings.
rithms [4], [5], and Bayesian methods [6], [7]. These meth- With these block-wise representations, we develop three
ods require multiple trials (i.e., training many architectures novel NAS models using the distilling neural architecture
separatelytoassesstheirquality),whichiscomputationally (DNA) techniques. And we induce these models into a
unaffordable for many researchers. Recent weight-sharing family called the “DNA Family”, as they share a similar
NASsolutionsencodedasearchspaceintoaweight-sharing mechanismforbuildingmodels,i.e.,trainingstudentsuper-
supernet and trained all architectures in the supernet at nets by distilling knowledge from a teacher network [11].
once,significantlyimprovingsearchefficiency. In the following, we summarize the characters of the three
Despite high efficiency, the effectiveness of weight- DNA models and discuss how they can resolve multiple
sharing NAS is worrying [8], [9], [10]. The culprit may be dilemmasofweight-sharingNAS.
the unreliable architecture rating caused by the oversized
searchspace.Specifically,weight-sharingNASmethodsco- • DNA: the vanilla model (illustrated in Fig. 1 (b)) em-
trainsupernetweightstoratearchitecturecandidates.With ploys traditional supervised learning, i.e., casting the
search space increasing, a generalization boundedness tool latent codes of a fixed teacher network as the supervi-
suggeststhatsupernetweightshavealowergeneralization sion.Itisveryefficientasseveralstudentsupernetscan
ability,resultinginuntrustworthyarchitectureratings.This betrainedsimultaneously.
irrelevance between the predicted and ground-truth archi- • DNA+: this model (illustrated in Fig. 1 (c)) allows the
tecturerankingmakesweight-sharingNASineffective. teacher network to be progressively updated during
To boost weight-sharing NAS, reducing search space is the architecture search procedure. The performance of
helpful. Hence, we factorize the large search space into DNA+ is thus less dependent on the initial capacity of
theteachernetwork,leadingtobetterscalability.
• Guangrun Wang and Changlin Li contribute equally and share the • DNA++: this model (illustrated in Fig. 1 (d)) makes
first authorship. G. Wang, L. Yuan, X. Liang, and L. Lin are with the teacher network and the student supernets jointly
Sun Yat-sen University, Guangzhou, China. C. Li and X. Chang optimize in a self-supervised learning manner, i.e., no
are with ReLER lab, AAII, University of Technology Sydney, Aus-
labeleddataisrequiredduringthearchitecturesearch.
tralia. X. Xian is with the CRRC Academy, Beijing, China. Email:
{wanggrun,changlinli.ai,ylc0003,jiefengpeng,xdliang328}@gmail.com; OnesuperiorityofDNA++isthetoleranceofdifferent
xiaojun.chang@uts.edu.au;xxy@crrc.tech;linliang@ieee.org.Correspond- basic architectures of the teacher network and student
ingauthor:LiangLin.
supernets (e.g., the teacher using a CNN and the stu-
4202
raM
2
]VC.sc[
1v62310.3042:viXraIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 2
train teacher
by supervised
learning
train & fix teacher
teacher model by supervised teach student
learning supernet
rate & search for train teacher & student
architectures rate & search for by self-supervised
architectures learning
teach student scale optimal rate & search for
block 1 block 2 block s t3 udent sb ul po ec rk n e4 t supernet a ar c nh ei wte c tetu ar ce h t eo r architectures
(a) DNA family (b) DNA (c) DNA+ (d) DNA++
Fig.1:IllustrationofDNAfamily.(a)Distillingneuralarchitecturetechniquewithblock-wisesupervision.Architecturecandidates
(denotedbydifferentnodesandpaths)aredividedintoblocks.(b)Supervisedlearning(vanillaDNA).(c)Progressivelearning
(DNA+).(d)Self-supervisedlearning(DNA++).
dentsupernetsusingavisiontransformer(ViT)). likeRNNsorevolutionaryalgorithmstosampleandevalu-
With the block-wise representations, we can rate all of atearchitectures,whicharecomputationallyexpensiveand
the candidate architectures, as opposed to previous works impractical for large datasets [2], [3], [4], [5], [6], [7], [12],
that can only access a sub- search space using heuristic [13],[14].Morerecentstudiesemployweight-sharingsuper-
algorithms. Our method thus shows a promising level of nets [15], [16], [17], [18], optimizing both supernet weights
effectiveness in comparison to other weight-sharing NAS andarchitectureindicatorsthroughgradient-basedmethods
approaches.Overall,ourcontributionsarefive-fold: [15], [16], [19]. However, these methods can introduce bias
betweensub-modelsandrelyheavilyoninitialization.One-
• We show that the excessive search space significantly
shotapproachesensurefairnessamongsub-modelsbysam-
hamperstheeffectivenessofmostweight-sharingNAS
pling and rating them using inherited weights from a su-
approaches. As the search space expands, supernet
pernet[18],[20],[21],[22],butthereremainsaperformance
weights exhibit reduced generalization ability, lead-
gapbetweenshared-weightproxysub-modelsandretrained
ing to imprecise architecture rankings and ineffective
standalone models, as observed in prior work [21], [22],
searches. To address this issue, we introduce a solu-
[23].Specifically,[23]providedananalysisusingaBayesian
tion by modularizing the extensive search space us-
view[24]toidentifythatthisgapnarrowsastheamountof
ing a block-wise representation, which systematically
weight-sharingsub-modelsdecreases.However,itsassump-
reducesthesearchscope.
tionthattheweightsofeachnetworklayerareindependent
• We explore three implementations under the general
might be too idealistic. DNA (originally proposed in our
knowledge distillation framework for balancing the
conferenceversion[25])addressesarchitectureratingissues
scalability, learning efficiency, and compatibility with
byoptimizingthesupernetdifferently.
variousneuralnetworkstructures.TheproposedDNA
family can be flexibly adopted for resolving different Knowledge Distillation. KD is a model compression
dilemmasofNASapplications. method transferring knowledge from a trained teacher to
• Block-wisesupervisionsenableourmodelstoassessall an efficient student. Approaches fall into two categories:
candidates, which is a departure from previous meth- soft label-based methods ( [11], [26]) and feature-based
odsthatcouldonlyexploreasubsetofthesearchspace. methods ( [27], [28], [29]). [28] trains a student to mimic
Additionally, this approach allows us to search for theteacher’sbehaviorinmultiplehiddenlayers.[29]usesa
architectures with varying depths and widths within progressive block-wise KD scheme to transmit knowledge,
theconstraintsofcomputationalresources. easing block-wise KD optimization. Unlike the progressive
• We highlight the significance of architecture rating scheme, our method employs parallelized KD to reduce
through comprehensive empirical studies, encompass- timeconsumptionandteacher-studentgap.Whileprevious
ing model ranking, evaluations using the Kendall Tau KD works optimize network weights, our method focuses
metric, and an assessment of training stability. These onoptimizingneuralarchitecture.
studies provide insights into the (in)effectiveness of Progressive Learning. Self-distillation enhances teacher-
conventionalweight-sharingNASapproaches. student model accuracy by reusing a student as a new
• Promisinglyefficientexperimentalresultsareprovided teacher [30]. It progressively improves regularization and
on ImageNet, CIFAR10, and PASCAL VOC segmen- reduces overfitting [31]. Progressive learning extends be-
tation. Typically, our DNA family gets a 78.9% top- yond KD, like [32] using pseudo labels to train a new
1 accuracy in a mobile setting and an 83.6% top-1 student, improving label quality and model performance.
accuracyforaViTonImageNet. Our work, distinctively, focuses on progressive neural ar-
chitecturesearchratherthannetworkweightoptimization.
Self-Supervised Learning.RecentNASmethodsoftenem-
2 RELATED WORK
phasizethelabel-freenatureofarchitecturesearch[33],[34],
Neural Architecture Search. NAS aims to replace human [35],[36],consideringself-supervisionoptional.Incontrast,
expertsinarchitecturedesign.Earlyapproachesusedagents DNA++aimstoreplacethesupervisorwithself-supervisionIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 3
duetopotentialbias.DNA++significantlydifferfromBoss- thesupernetconcurrently.Thesemethodsarecalledweight-
NAS [37] in NAS techniques, self-supervised learning, and sharingNAS,whichsignificantlyimprovessearchefficiency.
scalability. DNA++ uses a heterogeneous teacher-student Despite high efficiency, the effectiveness of weight-sharing
model (standard net vs. supernet), while BossNAS em- NASisunsatisfactory.Weanalyzetheculprithere.
ploys a homogeneous model (supernet vs. supernet). To Formally, let α j and ψ j denote an architecture and its
enable the heterogeneous model, DNA++ introduces new network weights, respectively. NAS is a problem to find
SSL techniques to prevent mode collapse and encourage the optimal pair (α∗, ψ∗) such that the model performance
j j
output divergence among samples, effectively combining is maximized. To perform searching, people need to assess
self-supervision with weight-sharing NAS. Lastly, DNA++ many architectures’ quality as rewards for adjusting search
offersdiverseapplicationpossibilities,suchasusingaCNN policies, i.e., they need to train each architecture separately
for self-supervision and searching for ViTs, thanks to its to obtain ψ j∗ = argminL(ψ j|α j)2 and use ψ j∗ to rate the
heterogeneousmodel. architecture.However,trainingeacharchitectureisquitein-
NAS for ViTs. Some prior work focused on NAS for ViTs. efficient.Weight-sharingNASmethodsformulatethesearch
The key differences between the DNA family and them spaceAintoanover-parameterizedsupernetsuchthateach
are as follows: GLiT [38] ingeniously introduces a novel candidate architecture α j is a subnet of the supernet. They
searchspaceforViTsandemploysanintelligenthierarchical trainallarchitecturesconcurrentlyinthesupernettoobtain
search algorithm. However, it may exclude certain archi- optimal supernet weights Ψsup = argminL(Ψ|A). Then,
tectures with potential during the initial screening phase, they extract weights from the supernet for each subnet
limitingtheirconsideration.ViTAS[39]ingeniouslypresents for validation and use this validation accuracy to rate the
a novel cyclic method for supernet training, aiming for subnet. The dilemma of weight-sharing NAS is that there
fairness [21]. While promising, it doesn’t address reducing is a low correlation between the predicted and the actual
the search space or the challenge of inaccurate architecture architecturequality.Toanalyzethisconjecture,wefirsthave
ranking, which the DNA family target. AutoFormer [40] ageneralizationboundednesstheorem.
cleverlyemploysasupernettrainingmethodologyinspired
byBigNAS[41].However,thecorrelationbetweensubnets’
accuracy and their train-from-scratch performance remains Theorem 1. (Generalization boundedness). For any sub-
unknown,makingthemsomewhatopaque.Incontrast,the net α j, we use ψ jsup to denote its sub-optimal weights
DNA family resolves the issue of inaccurate architecture extracted from a trained supernet and use ψ j∗ to denote
ranking in weight-sharing NAS. NASViT [42] smartly fo- itsidealweightswhentrainedalone.Then,theFrobenius
cuses on a novel method for neural network optimization, normofψ jsup isupperboundedby:
which could benefit our ”from-scratch” retraining process. (cid:118)
(cid:117) T
H opo tw ime iv ze ar t, ioth nis topa op ue rr rm eta ry ainn io nt gco pv roer cet sh se
,
wap hp icli hca wtio en po laf nth ti os (cid:13)
(cid:13)ψ
jsup(cid:13)
(cid:13)
F
≤(cid:117)
(cid:116)C 1+C
2(cid:88)(cid:12)
(cid:12)C 3−L(ψ t∗|α
t)(cid:12)
(cid:12), (1)
t=1
exploreinfuturework.Weconductedcomprehensivequan-
titative comparisons, demonstrating that the DNA family whereC 1 ≥0,C 2 ≥0,andC 3 areconstants.
consistently achieve superior accuracy compared to these
Remark. The Proof of Theorem 1 is in the appendix. The-
state-of-the-artNASmethodsappliedtoViTs[43].
orem 1 shows that using the trained supernet weights, a
3 METHODOLOGY
subnet’s model complexity (usually measuring the gener-
Westartbyanalyzingweight-sharingNAS’sdilemma(Sec- alization ability) has an upper bound associated with T.
tion 3.1). To tackle this issue, we partition the search space
Increasingthesearchspaceleadstoapoorergeneralization
intoblocks(Section3.2).Usingtheseblock-wiserepresenta-
ability, further implying that the architecture quality esti-
tions, we develop three NAS models employing neural ar-
matedbythesupernetweightsisnotpredictiveoftheactual
chitecturedistillationtechniquestoaddressvariousweight-
quality.Supposeanarchitecturehasexcellentquality,butits
sharing NAS challenges. Section 3.3 introduces DNA with
generalizationabilitybasedonthesupernetweightsispoor.
supervisedlearning,offeringsearchablewidths/depthsand
Then, its validation accuracy might be low, and its quality
resource adaptability. Section 3.4 outlines DNA+ with pro-
will be underestimated. In summary, the oversized search
gressive learning. Section 3.5 presents DNA++ with self-
spaceisthedevil/culpritbecauseabigsearchspacewould
supervisedlearning.
result in poor generalization ability and further lead to
3.1 BasicAnalysisofWeight-sharingNAS’sDilemma
inaccurate architecture rating, which finally leads to search
Finding optimal architectures requires multiple/greedy tri- ineffectiveness. On the contrary, reducing the search space
als,i.e.,trainingmany/allarchitecturesseparatelytoassess canimprovetheeffectivenessofweight-sharingNAS.
their quality, which is computationally unaffordable for Theorem 1 underscores the potential for block-wise su-
many researchers. Recent works give up training each can- pervision to enhance architecture ranking, a concept that
didate individually; instead, they encode the search space resonates with “Modular Learning [45]”. Modular learning
intoaweight-sharingsupernet1andtrainallarchitecturesin
effectively demonstrated the advantages of decomposing
neural network training into modular components and in-
1.AsupernetisageneralconceptwidelyusedintheNAScommu-
nity[15],[16],[19],[20],[44],referringtoadirectedacyclicsuper-graph dependently training each model, showcasing a multitude
coveringawholesearchspacewitheachnoderepresentingthefeature ofbenefits.Inourcontext,Theorem1supportstheideathat
mapsandeachedgerepresentingaconnectionbetweenthenodeswith
aparticularoperation(e.g.,aconvolution).Eachsubnetinthesupernet
representsacandidatearchitectureinthesearchspace. 2.Listheloss;weignoretheinputdataandthelabelsforsimplicity.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 4
Teacher teacher feature map teacher feature map
(previous block)
block 1 block 2 block 3 block 4 block 5
loss loss loss
student
feature
maps
input image
Student
Supernet
Candidate Operations Cells block Loss Functions Data flow Randomly Sampled Paths
Fig. 2: IllustrationofourDNA.Theteacher’sprecedingfeaturemapisusedasinputforbothteacherandstudentblocks.Each
cell of the supernet is trained independently to mimic the behavior of the corresponding teacher block by minimizing the L2
distancebetweentheiroutputfeaturemaps.Thedottedlinesindicaterandomlysampledpathsinacell.(Bestviewedincolor)
modularlearningensuresfairandthoroughtrainingofeach areindependent.Incontrast,weconsidertheweightsatdifferent
subnet,akeyfactorcontributingtoimprovedranking. layers of the deep nets to be highly dependent on each other.
Our algorithm in Section 3.3 allows us to precisely identify
3.2 ModularizingSearchSpaceintoBlocks
thehighest-rewardarchitectureamongthefullsearchspace
As discussed, to improve weight-sharing NAS’s effective-
of 1017 architectures. Furthermore, our approach can also
ness, one should reduce the search space. Unfortunately,
directreplacementofalargespacewithasmalloneisinad- search for architectures with varying depths and widths
visablesinceitleadstoasmallaccuracyrange,makingthe whileadheringtospecificcomputationalconstraints.
search meaningless. Keeping the whole search unchanged,
3.3 DNA:DistillationviaSupervisingLearning
we modularize the large space into blocks. The candidate
numberineachblockisthussignificantlysmallerthanthat AlthoughwemotivatewellinSection3.2,atechnicalbarrier
intheentiresearchspace.Thatis,wedividethesupernetU isthatwelackinternalgroundtruthinEqn.(3).Inspiredby
intoN blocksw.r.tthedepthofthesupernet: knowledgedistillation(KD)[11],weusethehiddenfeatures
 ofateacherasthesupervision.LetY k betheoutputtensor
 ΨU == (cid:104)U ΨN ◦ ;· .· .· .◦ ;ΨU k+1 ;◦ ΨU k ;.◦ ..· ;· Ψ·◦ (cid:105)U 1 o of ftt hh ee kk tt hh bb ll oo cc kk oa ft te ha ech suer pea rn nd etY .ˆ Wk( eX tk a) kb ee thth ee Lo 2u nt op ru mt t ae sn ts ho er
N k+1 k 1 , (2)
costfunction.ThelossfunctioninEqn.(3)canbewrittenas:
Z =(cid:104)
Z N,;...;Z k+1;Z k;...;Z
1(cid:9)(cid:105)
1 (cid:13) (cid:13)2
where U k+1 ◦U k represents that the (k+1)-th block is con-
L train(Ψ k|A k,X k,Y k)=
K
(cid:13) (cid:13)Y k−Yˆ k(X k)(cid:13)
(cid:13)
2, (5)
nected to the k-th block in the supernet. Ψ k is the network where K denotes the neuron numbers in Y. Moreover,
(cid:8) (cid:9)
weightsofthek-thblock.Z k = X k,Y k aretheinputdata inspiredbytheremarkablesuccessofthetransformers[46],
and the supervision of the k-th block. We optimize each [47], [48] that discards the inefficient sequential training
blockofthesupernetseparately: of RNN, we parallelize our supernet training analogously.
Specifically, we use the output of the (k-1)th block of the
Ψs kup =argminL train(Ψ k|A k,X k,Y k), i=1,2··· ,N, (3) teacherastheinputofthe(k-1)thblockofthesupernet,i.e.,
Ψk weuseY k−1 toreplaceX k inEqn.(5).Thus,thesearchcan
whereA
k
denotethesearchspaceofthek-thblock. bespedupinparallel.Eqn.(5)canbewrittenas:
Blocky vs. entire search space.Letd k andcdenotethe (cid:16) (cid:17) 1 (cid:13) (cid:13)2
d tie op nt sh inof eath che lk a- yth erb .Tlo hc ek na ,n thd et sh ize en ou fm thb eer seo af rcc han sd pi ad ca ete ofo tp he er ka -- L train Ψ k|A k,Y k−1,Y k = K (cid:13) (cid:13)Y k−Yˆ k(Y k−1)(cid:13) (cid:13) 2. (6)
thblockiscdk;thesizeoftheentiresearchspaceis(cid:81)N cdk. Fig.2showsapipelineofDNA.
k=0
Thisindicatesanexponentialreductioninsearchspace,i.e., Searchable depths & widths. Automatically allocating
((cid:81)N cdk)/cdk.Inourexperiment,theblockysearchspace eachblock’smodelcomplexityunderaparticularconstraint
k=1
is significantly smaller than the entire search space (e.g., is vital in NAS. To better imitate the teacher, each block’s
reduction ratio≈ 1e1 N5 ), ensuring effective weight-sharing model complexity should be allocated according to the
NAS.Finally,thearchitectureissearchedacrossthedifferent correspondingteacherblock’slearningdifficultyadaptively.
blocksinthewholesearchspaceA:
Withtheinputimagesizeandthestrideofeachblockfixed,
generally, the computation allocation is only related to the
(cid:88)N (cid:16) (cid:17)
α∗ =argmin λ L Ψsup|A ,X ,Y width and depth of each block, which are burdensome to
k val k k k k
∀α∈A k=1 , (4) searchinweight-sharingNASbecausethewidthanddepth
s.t. Ψsup =argminL (Ψ |A ,X ,Y ),k=1,··· ,N areusuallypre-definedwhendesigningthesupernet.Most
k train k k k k
Ψk previousworksincludeidentityasacandidateoperationfor
whereλ k representsthelossbalance. depthsearch([15],[16],[19],[22],[23]).However,aspointed
Remark.It’scrucialtohighlightthat,althoughweuseblock-wise out by [49], adding identity as a candidate operation can
supervision to help train the supernet to provide block-wise local lead to supernet convergence difficulty and an unfair com-
scores,ourarchitecturesearchisstillperformedinanentirearchi- parisonofsub-models.Also,addingidentityasacandidate
tecture by taking all the local scores into consideration. Previous operationmayleadtoredundantsearchspace.Forexample,
works [23] assume that the weights at each layer in deep nets a sequence {conv, identity, conv} is equivalent to {conv,
mets
mets
mets
metsIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 5
Final
local-scorelist3.
t1 s1 t2 s2 tM sM Architecture • Searching step. Given a computational cost constraint,
we should automatically allocate costs to each block
using a fair rating metric for different blocks. As an
search scale retrain
...
searchretrain M feaS tE urelo mss apis ,waf efe uc st eed afb ay iret rh me ev tra ir ci ,a in .ec .e ,reo lf ata ivete La 1ch le or s’ ss :
||Y −Yˆ (Y )||
L val(Ψ k|A k,Y k−1,Y k)= k (cid:112)k k−1 1, (7)
K D(Y )
k
where D(·) measures the variance. The blocky local
(cid:0) (cid:1)
scores L Ψ |A ,Y ,Y are summed for global
val k k k−1 k
Fig. 3:G Ie lln uer sa tt rio an t 1 ion of DNAGe +ne .ra It nion t 2 he fir.. s. t generG ae tn ie ora nti ,on w M e use search. Efficiently, we don’t need to compute the cost
an existing model as the teacher model. Then, at each con- (e.g., FLOPs) and losses for all 1017 candidates. With
secutive generation, a new teacher is obtained by scaling the the scores in a local-score list ranked, we propose an
searchedarchitectureofthepreviousgenerationandretraining efficientsearchalgorithmforvisitingallpossiblemod-
thescaledarchitecture.Thefinallysearchedarchitectureisthe
els(Alg.2intheappendix).First,duringasearchloop,
optimalstudentαM∗ inthelastgeneration,whichisretrained
if the cost already exceeds the constraint, we use the
withoutscaling.
statement “continue” to jump to the next loop itera-
tion. Second, when a model satisfying the constraint
conv, identity}. According to Theorem 1, this redundant
is found, we return to the previous block because this
search space can cause search ineffectiveness. Besides, [50]
model is optimal in the current block. Third, we get
searchedforthelayernumberwithfixedoperationsforthe
thecostofeachcandidateoperationbyapre-calculated
first step and subsequently searched for three operations
lookuptabletosavetime.Theapproachcanbelikened
with a fixed layer number. However, the operation choice
toahikingexpeditionfromastartingpointAtoades-
depends on each block’s depth, leading to a cumulative
tination B, with numerous possible routes to explore.
search departure, especially when search space increases.
Eachpathyieldsadifferentreward.Ifweidentifypoint
Thankstoblock-wisesupervision,wecantrainseveralcells
C as a key milestone within the optimal path from
withdifferentchannelnumbers/layernumbersinparallelin
A to B and, furthermore, discover the highest-reward
each block to ensure depth/width variability. Specifically, path from A to C, denoted as AC, then it logically
in each training step, the teacher’s previous feature map
follows that the best path from A to B via C must
is fed to three cells with different depths/withs in parallel include AC. Any path that does not incorporate AC
(e.g.,solidlineofdataflowinFig.2).Foreachlayerofeach
is not optimal. Leveraging this concept, we are able to
cell, an operation is randomly sampled from the candidate
efficiently and accurately pinpoint the highest-reward
operations(e.g.,dottedlineofdataflowinFig.2). architecturefromtheextensivepoolof1017 samples.
Constrained search algorithm. Our typical supernet
3.4 DNA+:DistillationviaProgressiveLearning
containsabout1017 sub-models,stoppingusfromratingall
of them. Previous weight-sharing NAS used random sam- After searching, our searched architecture has super good
pling, evolutionary algorithms, and reinforcement learning performance.Ifwescaleoursearcharchitecturetothesame
tosamplesub-modelsfromthetrainedsupernetforfurther size as the teacher, our scaled architecture can significantly
rating.Inthelatestworks([23],[50]),agreedysearchalgo- outperform the original teacher. This impressive result en-
rithmwasusedtoprogressivelyshrinkthesearchspaceby couragesustoperformacascadeNAS.Weiterativelyscale
layer-wiselyselectingthetop-performingpartialmodels.In upoursearchedarchitecturetobeanewteacherandsearch
contrast,althoughusingblock-wisesupervisiontocalculate foranewarchitectureguidedbythenewteachergeneration
blockylocalscores,ourarchitecturesearchisstillperformed by generation, mimicking the merit of a born-again neural
intheglobalsearchspacebyconsideringallthelocalscores. network[30]withmultiplegenerationsofknowledgetrans-
We can subtly traverse all the subnets to select the top- fer.ThisformsanotherDNAversion,i.e.,DNA+.
performingonesundercertainconstraints. In the first generation, we use an existing model (e.g.,
EfficientNet-B1)astheteachermodel,i.e.:
• Rating step. Block-wise supervision enables rating all
Y1 =f(X1|β1,Ψ1), (8)
candidates in a search space. We first compute local k k k k
scores using block-wise supervision, which is afford- whereβ1 andΨ1 arethearchitectureandnetworkweights
ablebecausethereareonly104sub-modelsineachcell. ofthefirstgenerationteacher.X1 andY1 aretheinputand
For further acceleration, we process batch data node by output.Here,weusektodenotethekthblock.Substituting
nodeinamannersimilartoadepth-firstalgorithm,with Y1 into Eqn. (4), we can train the first generation supernet
k
theintermediateoutputofeachnodesavedandreused andsearchforanoptimalarchitectureα1∗ ofGeneration1.
by subsequent nodes to avoid recalculating it from the At each consecutive generation, a new teacher is ob-
root node. The feature sharing evaluation algorithm tained by scaling the searched architecture of the earlier
is outlined in Alg. 1 in the appendix. By evaluating
all cells in a block of the supernet, we can get the 3.One may say that we can select the top-1 partial model in each
evaluation loss of all possible paths in one block. We local-score list to assemble the best student. But this short-sighted
can quickly sort this list with about 104 elements in a selectioncouldleadtolocalminima.Incontrast,ourarchitecturesearch
isperformedintheglobalsearchspacebyconsideringalllocalscores
few seconds with a single CPU. Each block has such a andtheircomputationcomplexity.Pleaserefertothe“searchstep.”IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 6
Teacher
block 1 block 2 block 3 block 4 block 5
aug Self-supervised
loss &
aug Supernet de-redundancy
input image loss
Student
Supernet
Candidate Operations Cells block MLP Loss Data flow Randomly Sampled Paths
Fig. 4: Illustration of DNA++. It uses self-supervisions to replace existing supervising teachers, avoiding architecture shifts
(referringtoaphenomenonthatstudentswithasimilararchitecturetoateachertendtobefavoredwhentheteacheristraditional).
DNA++containstwolosses,i.e.,aself-supervisedlossandaparticularlosstoremoveredundantnon-learnablesupernets.
generation using the scaling strategy from [3] (denoted as and [10] set both the teacher and the learning network as
Scaling in Eqn. (9)). Then, we retrained the new teacher supernets, which makes them very bloated and inelegant.
modelfromscratch,whichisusedastheblock-wisesuper- Fortunately,inspiredby[56],weareabletoelegantlysolve
visioninthisgeneration: thisproblembyintroducingDNA++.
(cid:40) Overview&Notation.AnoverviewofDNA++isshown
βm =Scaling(α(m−1)∗)
. (9) inFigure2.Foreachblock,wefirstusetwoMLPstoextract
Y km =f(X km|β km,Ψm
k
)
its features. The features are denoted as
Zj
and
Zˆj
, where
k k
Thesearchprocessineachconsecutivegenerationissimilar j denotes the j-th sample in a batch and k denotes the
tothefirstgeneration.Weperformthesearchiterativelyfor k-th block. Then we impose two losses on the features,
severalgenerationstosaturate.Attheendoftheprocedure, namelyaself-supervisedlossandaparticularlosstoremove
thefinallysearchedarchitectureistheoptimalstudentαM∗ redundantnon-learnablesupernets.
in the last generation, which will be retrained without Self-Supervised Loss. A basic loss in self-supervised
learning is the distance metric loss [51], [52], [53], which
scaling.Fig.3showsapipelineofDNA+.
aims to learn the similarities and dissimilarities between
Kind reminder: We would like to highlight the fact
features. To stabilize optimization and ease self-supervised
that the improvement in architectural ranking is primarily learning, regulation is usually also required. Therefore, our
attributed to block-wisely supervised learning. Knowledge self-supervisedlossisdefinedas:
distillation just serves as a means to achieve this block-
wiselysupervisedlearning. K M
L = λ 1 (cid:88)(cid:88) ∥Zj−Zˆj∥2+
3.5 DNA++:DistillationviaSelf-SupervisedLearning SSL KM k k 2
k=1j=1
Despite the high search performance of DNA and DNA+, K
their searched results might be influenced by the teacher Kλ C2 (cid:88) (∥Off-diag(Cov(Z k))∥2 2+∥Off-diag(Cov(Zˆ k))∥2 2),
architecture. Specifically, according to Eqn. (7), candidates k−1
(10)
with operations more similar to the teacher tend to have
wherethefirsttermisadistancemetricloss,andthesecond
a better local score in each block, resulting in a biased
term is a regularization loss. The role of the regularization
architecture ranking. When the search space is close to the
term is seen in [57]. Here K is the block number, M is the
teachernetwork,thisbiasisnotapparent.Butassuggested
by[37],ifthesearchspaceismoredivergent,thisbiaswillbe batch size, and C is the channel number. λ 1 and λ 2 are
used for loss balance. Off-diag represents taking the off-
amplified.Fortunately,withoutaccesstoanexistingteacher,
diagonal elements of a matrix. Cov stands for calculating
self-supervised NAS methods have been proven capable
of achieving comparable performance to supervised NAS
the covariance matrix of a vector set, i.e., Cov(Z k) =
methods.Hence,weuseself-supervisionasanalternativeto M1 −1(cid:80)M j=1(Z kj−Z¯ k)(Z kj−Z¯ k)T,whereZ¯ k = M1 (cid:80)M j=1Z kj .
theteachers’supervisiontoreducetherankingbiascaused Eqn. (10) is a standard self-supervised loss, and it in-
bytheuseofexistingteachermodels. heritsoneofitsinconveniences,i.e.,itrequiresaredundant
The most effective strategies to use self-supervision in non-learnable network for optimization. In the context of
computervisionincludecontrastivelearning[48],[51],[52], weight-sharing NAS, if only Eqn. (10) is used, a redundant
metriclearning[53],andgeneration[54].Inthesestrategies, non-learnable supernet is inevitably required, making op-
peoplemaketwoindependentdataaugmentationsforeach timization difficult and inelegant. Next, we aim to remove
image and input them into a learnable and non-learnable thisredundantnon-learnablesupernet.
network. Representations are learned by fully tapping the Avoiding Redundant Supernets. The reason why the
similarity of the two outputs. Note that the non-learnable existing self-supervised algorithms rely on redundant non-
network is usually a moving average of the learnable net- learnable networks is that if there are only learnable net-
work (i.e., the mean teacher) [48], [51], [52], [53] or a copy works,theoptimizationwillcollapseintoashortcut,suchas
of the learnable network with its gradients being stopped the output of different samples being all the same constant
[55]. In other words, the learnable and non-learnable net- vector.Hence,onepossiblewaytoavoidtheredundantnon-
worksneedtobeidentical;otherwise,learningwillcollapse. learnable supernet is to encourage the outputs of different
This makes it very difficult to introduce contrast learning samplestobedivergent.Inspiredby[56],weimposealoss
and metric learning into NAS. To solve this problem, [37] function on the output of different samples, maximizing
mets
metsIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 7
TABLE 1: Supernet design. “l#” and “ch#” represent the
theirvariance,i.e.:
numbersofthelayersandchannelsofeachcell,respectively.
L SDR = Cλ K3 (cid:88)K (cid:88)C max(0,γ−std(Z k,c)), (11) m - odel teac -her cell1 studen ct els lu 2pernet cell3
k=1c=1 - l# ch# l# ch# l# ch# l# ch#
block1 7 48 2 24 3 24 2 32
where max(0,) is a hinge-like loss, which is widely used block2 7 80 2 40 3 40 4 40
block3 10 160 2 80 3 80 4 80
in SVMs. γ is a threshold. std represents computing the
block4 10 224 3 112 4 112 4 96
(cid:112)
standard deviation, i.e., std(Z k,c) = Var(Z k,c)+ϵ. ϵ is a block5 13 384 4 192 5 192 5 160
block6 4 640 1 320 - - - -
small constant, and Z k,c denotes the c-th channel neuron
numbers, as introduced in Section 3.3. There are 3 cells in
of the k-th block. After adding the above supernet de-
redundancy loss L SDR, we no longer need an extra non- each of the first five blocks and 1 cell in the last block.
The layer numbers and channel numbers of each cell are
learnable supernet to train our supernet. Our final loss
is L SSL + L SDR. In this way, self-supervision is cleverly
showninTable1.Thewholesearchspacecontains2×1017
architectures.
combinedwithweight-sharingNAStoformourDNA++.
In DNA++, our self-supervision can be a CNN, and the We use EfficientNet B7 [3] as our teacher for supernet
student supernet is a ViT. Thus, we can search for ViTs. Fig. training due to its state-of-the-art performance and rela-
tivelylowcomputationalcostcomparedtoResNeXt-101[65]
4showsapipelineofDNA++.
andothermanuallydesignedmodels.Wedividetheteacher
4 EXPERIMENTS
model into 6 blocks in sequence according to their filter
4.1 Datasets. numbers.ThedetailsoftheseblocksareshowninTable1.
We evaluated our method on ImageNet [58], a benchmark ViT Search Space. In this space, we build our supernet
for practical NAS methods. For architecture search, we basedonthewell-knownDeiT[66].Thesearchspacedesign
createda50k-imagevalidationsetbyrandomlyselecting50 isinspiredbyslimmablenetworks[67],[68],[69],especially
images from each class of the original training set. The rest DS-ViT [70]. We define the candidate operations by the
oftheimageswereusedforsupernettraining.Aftersearch, embedding dimension (224, 448, 32), the head number (7,
both the found architectures and scaled architectures were 14,1),theMLPratio(2.5,4,0.5),andthenetworkdepth(12,
retrained from scratch on the original training set without
16,1).Here,(a,b,l)representsasearchspacerangingfrom
teacher network supervision and tested on the original
atobwithintervall.Thewholesearchspaceisdividedinto
validation set. Additionally, we assessed transferability on 4blocks,andcontainsabout1.7×107 architecturesintotal.
CIFAR-10 and CIFAR-100 [59]. For a more comprehensive Although our search space/supernet is ViT-based, we
evaluation of generality, we conducted semantic segmenta- donotuseaViTasateachertoguideoursupernettraining.
tiontasksonPASCALVOC2012[60]andADE20K[61],and Instead,weadoptEfficientNet-B1asourteacherduetotwo
objectdetectiontasksonMS-COCOdataset[62]. reasons.First,althoughaViThashighperformance,itscom-
ImageNetNASBench.ToevaluateNASmethods,prior putational complexity (e.g., in terms of parameter number)
approaches commonly retrain searched architectures from is higher than an EfficientNet-B1. Second, the architectural
scratch, making it hard to discern whether improvements differences between a teacher and a student supernet help
areduetoNASeffectivenessorretrainingtechniques.NAS to test the effectiveness of our proposed block-wise self-
benchmarksexistbutareoftenbasedonsmalldatasets(e.g., supervisiontechniqueinaddressingarchitectureshifts.
CIFAR-10 or ImageNet-Tiny) and cell-based search spaces. 4.3 SearchingonMBConvSearchSpace
Addressingthis,wecreatedabenchmarkonfullImageNet
Why do we choose DNA and DNA+? This section only
using the mobile-setting search space, comprising 23 ran-
tests DNA and DNA+ because MBConv is efficient and is
domlysampledarchitectureswithtop-1accuraciesranging
suitable for algorithms with high training complexity, such
from 73.58% to 75.52% that align with the true accuracy
as DNA+. We leave DNA++ to be examined in the ViT
distribution.MoredetailsareinFig.12intheappendix.
searchspacebecauseDNA+cansolvethearchitectureshift
4.2 SearchSpacesandArchitectureDetails
probleminthatcase.
Our search spaces are defined by selecting operations from We train each cell in the supernet for 20 epochs 4 indi-
operationcandidatesandselectingcellswithvaryingchan- viduallyundertheguidanceoftheteacher’sfeaturemapin
nel and layer numbers. They include ViT and MBConv thecorrespondingblock.ForDNAandDNA+,weuse0.002
searchspaces. asthestartlearningrateforthefirstblockand0.005forthe
MBConvSearchSpaces.Inthesespaces,operationcan- rest blocks. We use Adam as our optimizer and reduce the
didates are MBConvs [63]. We use two MBConv search learningrateby0.9everyepoch.
spaces. The first one used in Section 4.3 is similar to most For DNA/DNA+, it takes three days to train a typical
of the recent NAS works [2], [3], [49], [64] to ensure a fair supernet using 8 NVIDIA GTX 2080Ti GPUs. However,
comparison, which has 6 operation types in total, i.e., a simplifying the supernet to contain only one cell in each
combination of convolution kernel sizes of {3, 5, 7} and block takes only one day. With Algorithm 1, our cost of
expansionrates{3,6}.Table 15in the appendixprovides a
detailed search space comparison with existing NAS meth- 4.Inthesupernettrainingstage,wedecoupleeachblock,significantlyincreas-
ingsubnetsamplingefficiencycomparedtopriorweight-sharingNASmethods.
ods. For fast evaluation in Section 4.5 and 4.6, a smaller
Thisefficiencyallowsustoconducteffectivetrainingwith20epochsforeach
search space with 4 operation types (kernel sizes of {3, subnet. In the searching stage, we utilize block-wise supervision and provide
5} and expansion rates {3, 6}) is used. Besides, we also block-wiselocalscores.However,ourarchitecturesearchencompassesanentire
architecture,consideringalllocalscoresandavoidingthelimitationofsampling
search for cells with different channel numbers and layer onlyafewsubnets.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 8
Accuracy vs Params Accuracy vs FLOPS Accuracy vs Params
79 79
83
78 ( i 2n 4p 0u xt 2s 4iz 0e ): 78 ( i 2n 4p 0u xt 2s 4iz 0e ): 82
77 77 DNA (Ours) 81
76 D DN NA A ( wO / u Rr As) (Ours) D EfN fiA ce w n/ t NR eA t (Ours) 80
EfficentNet 76 MnasNet
MnasNet MobileNetV3 79
75 MobileNetV3 SCARLET
SCARLET 75 FairNAS 78
74 F F Da B Ai N RrN e TtA SS 74 M F PB ro oNG xe yA t lessNet 77 D EfN fiA ci e(O ntu Nrs e) t
73 PCNAS Single Path One-shot 76 SCARLET
4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 0.2 0.3 0.4 0.5 0.6 0.7 5 10 15 20 25 30
Number of Params (Million) FLOPS (Billions) Number of Params (Million)
Fig.5:Trade-offbetweenmodelaccuracyandmodelcomplexityonImageNet.Left:comparisonamongourscaledDNAmodels,
EfficientNets, and SCARLET on ImageNet by accuracy vs. parameter numbers. Mid: model accuracy vs. parameter numbers;
Right:modelaccuracyvs.FLOPs.
TABLE2:Comparisonofstate-of-the-artNASmodelsonIma- TABLE 3: Comparison of transfer learning performance of
geNet.Theinputsizeis224×224.RA:RandAugment[71]. NAS models on CIFAR-10 and CIFAR-100. x(x′): x is our re-
model Params FLOPs Acc@1 Acc@5 implementational results with the officially released model; x′
SPOS[20] - 319M 74.3% - istheaccuracyreportedbytheoriginalpaper.
ProxylessNAS[15] 7.1M 465M 75.1% 92.5% Model CIFAR-10Acc CIFAR-100Acc
FBNet-C[19] - 375M 74.9% - MixNet-M[73] 97.9% 87.4%
MobileNetV3[72] 5.3M 219M 75.2% - EfficientNet-B0[3] 98.0%(98.1%)† 87.1%(88.1%)†
MnasNet-A3[2] 5.2M 403M 76.7% 93.3%
FairNAS-A[21] 4.6M 388M 75.3% 92.4% DNA-c(ours) 98.3% 88.3%
MoGA-A[64] 5.1M 304M 75.9% 92.8%
SCARLET-A[49] 6.7M 365M 76.9% 93.4%
PC-NAS-S[23] 5.1M - 76.8% - EfficientNet-B0, we further perform our DNA under the
MixNet-M[73] 5.0M 360M 77.0% 93.3% constraintsof350MFLOPsand5.3Mparametersseparately.
EfficientNet-B0[3] 5.3M 399M 76.3% 93.2%
FBNetV3-A0[74] 6.3M - 78.4% - WethusobtainDNA-bandDNA-c,respectively.BothDNA-
FBNetV3-A[74] 8.6M - 79.1% - b and DNA-c outperform EfficientNet-B0 by a large mar-
AttentiveNAS-A0[75] 9.1M - 77.3% -
AttentiveNAS-A1[75] 9.6M - 78.4% - gin (1.1% and 1.5%). Even compared with the recent top-
AttentiveNAS-A2[75] 11.3M - 78.8% -
performing MixNet-M, which uses more efficient MixConv
AlphaNet-A0[76] 9.1M - 77.8% -
AlphaNet-A1[76] 9.6M - 78.9% - operations that we do not use, our DNA-b with fewer
AlphaNet-A2[76] 11.3M - 79.1% -
parametersbeatsMixNet-Mby0.4%.Third,whensearched
EfficientNetV2-B0[77] 7.4M - 78.7% -
OnceForAll-1080ti-27[78] 6.5M - 76.4% - undernoconstraint,ourDNA-dachieves78.4%top-1accu-
OnceForAll-1080ti-22[78] 5.2M - 75.3% -
OnceForAll-1080ti-15[78] 6.0M - 73.8% - racy with 6.4M parameters and 611M FLOPs. When tested
OnceForAll-1080ti-12[78] 5.9M - 72.6% - with the same input size (240 × 240) as EfficientNet-B1,
OnceForAll-v100-11[78] 6.2M - 76.1% -
OnceForAll-v100-09[78] 5.2M - 75.3% - DNA-d achieves 78.8% top-1 accuracy, being evenly accu-
OnceForAll-v100-06[78] 4.9M - 73.0% - rate but 1.4M smaller than EfficientNet-B1. Fourth, with
OnceForAll-v100-05[78] 5.2M - 71.6% -
random 5.4M 399M 75.7% 93.1% RandAugment [71], DNA-c and DNA-d further achieve
DNA-a(ours) 4.2M 348M 77.1% 93.3% 78.1%and78.9%top-1accuracy.
DNA-b(ours) 4.9M 406M 77.5% 93.3%
DNA-c(ours) 5.3M 466M 77.8% 93.7% Architecture visualization. Our searched architectures
DNA-d(ours) 6.4M 611M 78.4% 94.0%
are visualized in Fig 13 in the appendix, from which we
DNA-dw/KD(ours) 6.4M 611M 79.72% 94.34%
DNA-cw/AA(ours) 5.3M 466M 77.9% 93.9% have several observations. i) Searched under no constraint,
DNA-cw/RA(ours) 5.3M 466M 78.1% 94.0%
DNA-dw/RA(ours) 6.4M 611M 78.9% 94.2% DNA-dtendstochooserelativelyexpensiveoperationswith
DNA+-cw/AA(ours) 5.3M 476M 78.0% 93.9% high expansion rates and large kernel sizes, thus achieving
DNA+-cw/RA(ours) 5.3M 476M 78.3% 94.1%
the best performance. This verifies that our DNA is able
architectureratingisabout0.6GPUdays.Tosearchforthe to find optimal architectures in the search space. ii) Under
bestmodelundercertainconstraints,weperformAlgorithm theconstraintofmaximumparameternumber,DNA-ctends
2onCPUs,andthecostislessthanonehour. to discard the operations with redundant channels to save
Toretrainoursearched(andscaled)architecturesonIm- the parameters. It also tends to select a lower expansion
ageNet,weuseasimilarsettingwith[3],i.e.,abatchsizeof rate in the last blocks since the last blocks have more
4,096,anRMSpropoptimizerwithamomentumof0.9,and channels than the first blocks. iii) Under the constraint of
aninitiallearningrateof0.256thatdecaysby0.97forevery maximum computational cost, DNA-b and DNA-a tend to
2.4 epochs. Our models are first trained without additional selectoperationswithfewerchannelsandlowerexpansion
tricksforafaircomparison,thenwithRandAugment[71]to ratesevenlyineachblock.Thesignificantstyledifferenceof
stimulatetheirpotential.Fortransferlearningexperiments, the high-performing architectures in Fig 13 proves DNA’s
wefollowthesamefine-tuningsettingsas[79]and[80]. architecturesearchability.
Model complexity vs. model accuracy. Fig. 5 compares
4.3.1 DNA the curve of model size vs. accuracy and FLOPs vs. accu-
Top-1 accuracy. As shown in Table 2, our DNA models racy for most recent NAS models. Our DNA models can
achieve state-of-the-art results compared with the most achievebetteraccuracywithsmallermodelsizesandlower
recent NAS models. First, when searched under a con- computationcomplexitythanotherNASmodels.
straintof350MFLOPs,DNA-asurpassesSCARLET-Awith Transferability.Totestoursearchedarchitectures’trans-
1.8Mfewerparameters.Second,forafaircomparisonwith ferability, we evaluate our searched architectures on two
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poT
)%(
ycaruccA
1-poTIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 9
TABLE 4: Comparison between our scaled DNA models and TABLE6:ComparisonofvisiontransformersonImageNet.The
EfficientNetsonImageNet. input size is 224×224. While our method exhibits superior
model inputsize Acc@1 Acc@5 Params performancetoNASViT,weacknowledgethattheDNAmodel
DNA-c 78.1% 94.0% 5.3M sizeislargercomparedtoNASViT(5.6Gvs.1.9G).However,it
224×224
EfficientNet-B0 76.3% 93.2% 5.3M isworthmentioningthatthismodelisthelargestavailableone
DNA-c1 240×240 79.7% 94.7% 7.5M intheNASViTpaperanditsGitHubpage.
EfficientNet-B1 78.8% 94.4% 7.8M
DNA-c2 260×260 80.5% 95.1% 8.8M Model FLOPs Top-1Acc(%)
EfficientNet-B2 79.8% 94.9% 9.2M
DNA-c3 81.8% 95.7% 12M w/odistillation
300×300
EfficientNet-B3 81.1% 95.5% 12M
DNA-c4 82.9% 96.3% 19M
ViT-S/16[ICLR21][81] 4.7G 78.8
EfficientNet-B4
380×380
82.6% 96.3% 19M
DeiT-S[ICML21][66] 4.7G 79.9
DNA-c5 83.4% 96.4% 30M
T2T-ViT-14[ICCV21][82] 5.2G 81.5
EfficientNet-B5
456×456
83.3% 96.7% 30M
Swin-T[ICCV21][83] 4.5G 81.3
AutoFormer[ICCV21][40] 5.1G 81.7
TABLE5:Top-1accuracyofDNA+onImageNet. BossNAS-T↑ [ICCV21][37] 5.7G 81.6
Searched Kendall DNA-T 4.5G 81.2
Gen Teacher Searchedmodel
modelacc tau DNA-T++ 5.6G 82.0
DNA+0
0 — (EfficientNet- 76.3 — w/distillation
b1)
1
Scaled-DNA+0 DNA+1
78.1 0.64
GLiT-Small[ICCV21][38] 4.4G 80.5
( SE cf afi lec die -n DtN Ne At- +b 17) (DNA-c) GLiT-Base[ICCV21][38] 17.0G 82.3
2 DNA+2 78.3 0.66
(DNA-c1) ViTAS-Twins-S[ECCV22][39] 3.0G 82.0
3 Scaled-DNA+2 DNA+3 — 0.66
ViTAS-Twins-B[ECCV22][39] 8.8G 83.5
ViTAS-DeiT-B[ECCV22][39] 4.9G 80.2
widely used transfer learning datasets, i.e., CIFAR-10 and
NASViT[ICLR21][42] 1.9G 82.9
CIFAR-100. The results are reported in Table 3. As shown,
ourmodelsmaintainsuperiorityaftertransferring.
Twins-PCPVT-S[NeurIPS21][84] 3.8G 81.2
Twins-PCPVT-B[NeurIPS21][84] 6.7G 82.7
Modelscalability.Asoursearchedarchitecturesachieve
DeiT-S⚗[ICML21][66] 4.7G 81.2
remarkably high accuracy with significantly few parame- DS-ViT-L++[TPAMI22][70] 5.6G 83.0
ters, to adequately explore our searched architecture’s per- DNA-T⚗ 4.5G 82.9
DNA-T++⚗ 5.6G 83.6
formance, we extend our DNA-c to the different model
sizes by scaling up the depth, width, and input size of
art approaches, such as FBNetV3 [74] (79.7% vs. 78.4%),
DNA-c simultaneously. Although it is better to search for
AttentiveNAS [75] (79.7% vs. 77.3%), AlphaNet [76] (79.7%
anoptimalscalestrategysuitableforourDNAusingagrid
vs. 77.8%), and EfficientNetV2 [77] (79.7% vs. 78.7%), and
searchasEfficientNet[3]did,forsimplicity,wejustborrow
OnceForAll[78](79.7%vs.76.4%),byasignificantmargin.
thescalingstrategyfrom[3]withoutsearching,puttingour
methodatadisadvantage.TheresultsareshowninTable4.
4.3.2 DNA+
Wecanseethatourscaledarchitectures’accuracysurpassed
DNA+ is a multi-generation cascade of DNA. As Table 5
the competing EfficientNet at all levels, even if the scaling
shows, the searched models of Generations 0 and 1 are
strategy used might not fit our DNA perfectly due to the
Efficient-b1andDNA-c.ThescaledsearchedmodelsofGen-
lackofstrategysearch.TheleftofFig.5presentsthescatter
erations0and1areEfficient-b7andDNA-c1,whichfurther
diagram. In summary, this comparison verifies the general
serve as the teachers of Generations 1 and 2, respectively.
featuresthatoursearchedarchitecturehaslearned.
The searched models of Generations 2 and 3 are DNA+2
Fairer comparisons. Please kindly note the importance
and DNA+3. For each generation, we evaluate the super-
of ensuring fairness when comparing our approach with
net’s ranking ability with our ImageNet NAS Bench. The
the state-of-the-art method. There are two key points that
ranking Kendall tau along with searched model accuracies
meritthoughtfulconsiderationinthisregard:Firstly,during
are reported. The accuracy leaped by 1.8% at Generation 1.
the validation process using the MBConv search space,
At Generation 2, the accuracy further improved by 0.2%,
our searched architectures undergo retraining from scratch
and the ranking tau improved by about 3% relatively. The
without the utilization of knowledge distillation or fine-
rankingcorrelationreachesaplateauatGeneration3,asthe
tuning.Incontrast,recentpracticesofteninvolveinitializing
teachermodelmayhavereachedthesearchspaceceiling.
the searched architecture with pre-trained weights from
the supernet or employing larger networks for knowledge 4.4 SearchingforVisionTransformers
distillation. These disparities place our DNA approach at
a potential disadvantage. To address this discrepancy, we Why do we select DNA and DNA++?Thissectionfocuses
have taken the proactive step of incorporating knowledge onexaminingDNAandDNA++because:First,theteacher
distillation into our retraining process in this revision. Sec- we use is a CNN, and the architectures to be searched are
ondly, given our emphasis on parameter efficiency, the ViTs, which have an architecture shift. The self-supervised
DNAfamilyconductssearchesinthesearchphasewiththe learning technique adopted by DNA++ can solve this ar-
numberofparametersasthetarget.Therefore,itisessential chitectureshiftproblem.Second,DNA+istime-consuming
tocompareitwithstate-of-the-artmethodsthatexhibitfair andhasbeentestedinanefficientMBConvspace.
parameterefficiency.Wemeticulouslycompareourmethod Supernet training. We train each cell in the supernet
withthelatestadvancedtechniquesandthoroughlydiscuss for 20 epochs individually under the block-wise guidance
the results. As demonstrated in Table 2, this fairer compar- of the teacher model. We use 0.33 and 0.0025 (for 256 total
ison reveals that DNA outperforms previous state-of-the- batch size) as the start learning rate for DNA and DNA++,IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 10
respectively. For DNA, we use AdamW as our optimizer. DNA vs. GLIT [38] vs. VITAS [39] vs. AutoFormer [40] vs.
The learning rate of DNA is reduced to its 0.9× every 3 NASViT[42]vs.Twins[84]).
epochs.ForDNA++,weusecosineschedulertoreducethe On the one hand, our DNA-T++ achieves the best per-
learningrateanduseLARSastheoptimizer.Ittakes1and2 formanceamongallcontrastingmethods,includingmanual
daystotrainatypicalsupernetusing8NVIDIAGTX2080Ti architectures(e.g.,Twins[84],whichproposesasmartatten-
GPUsforDNAandDNA++,respectively.WithAlgorithm1, tion mechanism for vision transformers) and automatically
ourcostofarchitectureratingisabout0.6and0.1GPUdays searched architectures. In terms of comparing our DNA
for DNA and DNA++. To search for the best model under approachwiththeTwins[84]methodofsimilarFLOPs(e.g.,
certain constraints, we perform Algorithm 2 on CPUs, and 6.4Gvs.6.7G),wehaveobtainedthefollowingresults:83.6%
thecostislessthanonehourforbothDNAandDNA++. vs. 82.7% for DNA-T++ vs. Twins-PCPVT-B. Moreover,
Retrainingsearchedmodels.Toretrainoursearchedar- DNA-T++ outperforms BossNAS, the current best NAS
chitecturesonImageNet,weuseasimilarsettingwith[66], method for vision transformer search, by a clear margin
i.e., a batch size of 1,024, an AdamW optimizer, an initial (i.e., 82.0% vs. 81.6%). DNA++ even also significantly out-
learning rate of 1e-3, and a cosine learning rate scheduler. performsthecurrentbest-performingdynamicViTslikeDS-
The weight decay is set to 5e-2. The architecture is trained ViT-L++ [70] (i.e., 83.6% 83.0%). In terms of comparing our
for 300 epochs, with 5 epochs for learning rate warm-up. DNA approach with the NASViT [42] approach of similar
We use similar regularization and augmentation as DeiT size,wehaveobtainedthefollowingresults:83.6%vs.82.8%
[66], including label smoothing [85], stochastic depth [86], for DNA-T++ vs. NASViT. While our method exhibits a
Cutmix [87], RandAugment [88], random erasing [89], and superiorperformancetoNASViT,weacknowledgethatthe
Mixup [90]. Following [66], we also explore training with DNA model size is larger compared to NASViT (5.6G vs.
KD,astandardtechniquewidelyusedfortrainingViTs. 1.9G). However, it is worth mentioning that this model
is the largest available one in the NASViT paper and its
4.4.1 DNA GitHub page 5. On the other hand, DNA-T++ far exceeds
the performance of DNA-T (i.e., 82.0% vs. 81.2% without
AsshowninTable6,ourDNAmodelsachievestate-of-the-
knowledgedistillationand83.6%vs.82.9%withknowledge
art results compared with existing models. More precisely,
distillation),whichisveryencouragingtous.Thesignificant
we have three critical findings. First, the performance of
advantage of DNA-T++ over DNA-T confirms that self-
the frameworks searched by our DNA matches or even
supervisedtechnologycansolvethearchitectureshiftprob-
exceedstheperformanceofhand-designedframeworks.For
lem between teachers and students in NAS. This exciting
example, at about 4.5G FLOPs of computation, our DNA-T
resultprovesnotonlytheeffectivenessofDNA++,butalso
outperforms ViT-S/16 [81] and DeiT-S [66], close to Swin-
the robustness of our DNA family in the face of various
T [83], and slightly lower than T2T-ViT-14 [82]. Note that
cornercasesinthenetworkstructuresearch.
T2T-ViT-14 is computationally more expensive than our
DNA-T (i.e., 5.2G vs. 4.5G). Second, our method maintains
an advantage over hand-designed models with knowledge 4.5 Effectiveness
distillation (e.g., 82.8% vs. 81.2% for DNA-T vs. DeiT-S). We prove our DNA’s effectiveness and provide more in-
Last but not least, we generously admit that our DNA-T sights into DNA by comparing it with recent NAS meth-
isslightlyinferiortothebestNASmethods. odsonmodelranking,quantitativeeffectiveness/efficiency
Analysis.Tworeasonscanaccountforwhyourmethod analysis, training stability, and feature visualization. Three
doesnotholdaclearadvantageoverexistingNASmethods. methods, i.e., DARTS [16], SPOS [20], and MnasNet [2] are
First, existing methods are more computationally expen- chosenastherepresentativesofthethreemainstreamsofthe
sive than our DNA-T (e.g., 5.7G FLOPs vs. 4.5G FLOPs current NAS: jointly-optimized weight-sharing methods,
for BossNAS-T vs. DNA-T). Second, our DNA-T has an one-shotweight-sharingmethods,andindependentunder-
architecture shift between the teacher and the students. training methods. The experiments in this section (i.e., Sec-
Specifically,ourteacherisaCNN,andourstudentsareViTs. tion4.5)areconductedonourImageNetNASBench.
In the knowledge distillation process, operations similar to Model ranking6. A main advantage of our DNA is
theteachertendtohavelowerdistillationlosses.Thismakes reliable architecture rating, which means that the ranking
the searched networks tend to be close to the teacher. This of under-trained models (i.e., “predicted model ranking”)
bias prevents the best architecture from being discovered. can be an indicator of the actual ranking of these models
Fortunately, our DNA++ can solve this architecture shift, (i.e.,“truemodelranking”).Toevaluatearchitecturerating,
whichwillbepresentedinthenextsection. in Fig. 6, we compare the “predicted model ranking” (P)
and “true model ranking” (T) for DARTS, SPOS, MnasNet,
4.4.2 DNA++ and DNA, respectively. All these methods are examined in
the same MBConv search space (i.e., the smaller MBConv
We have conducted comprehensive quantitative compar-
spacedescribedinSection4.2).Thetrueaccuraciesandthe
isons between our DNA approach and these state-of-the-
“truemodelranking”ofthesemodelsareobtainedfromour
art NAS methods applied to ViTs. These comparisons are
extensively presented in the findings outlined in Table 6.
5.https://github.com/facebookresearch/NASViT
Thecompellingresultsunequivocallydemonstratethatour
6.Our model ranking experiments are more comprehensive and fair than
method consistently achieves a superior level of accuracy previousworkslikeFairNAS[21]andPCNAS[23].FairNASonlysamplesmodels
comparedtothesemethods,evenwithsimilarFLOPs(e.g., fromtheParetofront,whilePCNASselectsmodelsfromthelastiterationsofthe
search,introducingbias.Incontrast,ourmodelrankingincludesarchitectures
83.6% vs. 80.5% vs. 82.0% vs. 81.7% vs. 82.9% vs. 82.7% for randomlysampledfromtheentiresearchspace,ensuringafairercomparison.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 11
DNA, =0.64 DARTS, =0.08 SPOS, = 0.18 MnasNet (0.2 lr), =0.04 MnasNet (0.001 lr), =0.61
23 23 23 23 23
21 21 21 21 21
19 19 19 19 19
17 17 17 17 17
15 15 15 15 15
13 13 13 13 13
11 11 11 11 11
9 9 9 9 9
7 7 7 7 7
5 5 5 5 5
3 3 3 3 3
1 1 1 1 1
P T P T P T P T P T
Fig.6:ComparisonofmodelrankingamongDNA,DARTS[16],SPOS[20],andMnasNet[2]undertwodifferenthyper-parameters.
Theleftandrighty-axisesrepresentthe“predictedmodelranking”(P)and“actualmodelranking”(T),respectively.
DNA vs. DARTS DNA vs. SPOS DNA vs. MnasNet (0.2 lr) DNA vs. MnasNet (0.001 lr)
1.40 8 1.40 71.0 1.40 17 1.40 45.5
1.45 7 1.45 1.45 16 1.45
70.9 45.0
6 15
1.50 1.50 1.50 1.50
5 70.8 14 44.5
1.55 4 1.55 1.55 13 1.55
1.60 3 1.60 70.7 1.60 12 1.60 D DN ARA TS 44.0
11 SPOS
1.65 2 1.65 70.6 1.65 10 1.65 M Mn na as sN Ne et t ( (0 0. .2 0 0lr 1) lr) 43.5
74.0 74.5 75.0 75.5 74.0 74.5 75.0 75.5 74.0 74.5 75.0 75.5 74.0 74.5 75.0 75.5
Fig. 7: Comparison of model ranking among DNA vs. DARTS [16], DNA vs. SPOS [20], and DNA vs. MnasNet [2] under two
differenthyper-parameters.Thex-axisesrepresenttheground-truthtop-1accuraciesofdifferentarchitecturesonImageNet,and
they-axisesrepresenttheirlossesbasedontheunder-trainedweightsofdifferentmethods.
(a) DNA(ours) (b) SPOS[20] (c) DARTS[16]
Fig.8:Top-1accuraciesofthesearchedmodelsw.r.tthetraininglossofthecellduringthetrainingprogressonImageNet.
TABLE7:Comparisonoftheeffectivenessandefficiencyofdif-
ImageNet NAS Bench. We use the under-trained network
ferentNASmethods.TheeffectivenessismeasuredbyKendall
weightsofthesefourmethodstogettheir“predictedmodel
Tau(τ),SpearmanRho(ρ)andPearsonR(R),rangingfrom-1
ranking”. Each block in our DNA supernet is trained for
to1(thehigher,thebetter).Theefficiencyismeasuredbysearch
20 epochs, adding up to 120 epochs in total. The SPOS cost(thelower,thebetter).(Gds:GPUdays;Tds:TPUdays)
supernetisalsotrainedfor120epochsfollowingtheofficial Method SearchCost τ[92] ρ R
protocol [20]. We obtain the “predicted model ranking” for SPOS 8.5Gds -0.18 -0.27 -0.29
DARTS 50Gds 0.08 0.14 0.06
DNA and SPOS by evaluating the validation loss using
MnasNet 288Tds 0.04/0.61 0.07/0.77 0.05/0.78
supernet weights. The DARTS supernet is trained for 50 DNA(Ours) 8.5Gds 0.64 0.82 0.84
epochs because over-training DARTS could lead to a per-
formancedrop,assuggestedby[91].The“predictedmodel trained to obtain the Pareto front in MnasNet, which costs
ranking” for DARTS is obtained by sorting the predicted 288 TPU days [2]. On the other hand, it takes five epochs
joint probability of selecting different operation candidates of ImageNet training to rate an architecture for MnasNet,
for each architecture. As for MnasNet, each architecture is while it only takes 120 epochs of ImageNet training on
1010
trained independently for five epochs following the official average to rate an architecture for weight-sharing methods
protocol [2]. The “predicted model ranking” is obtained by likeSPOSandDNA.Besides,therankingresultofMnasNet
rankingtheperformanceoftheseunder-trainedmodels.As ishighlyhyperparameters-dependent(seethelasttwosub-
the under-training setting of MnasNet is not provided in figuresofFig.6).TheabovetwoobservationsverifyDNA’s
detail, we tried two different learning rates, i.e., 0.2 and superiority in correctly and efficiently rating architectures,
0.001,gettingtwoverydifferentresults. leadingtoeffectiveandefficientarchitecturesearches.
Fig. 6 presents the results. First, SPOS and DARTS fail Wealsoplotthetrueaccuraciesofdifferentarchitectures
to rank architectures correctly. This may be attributed to on ImageNet w.r.t. their under-trained models’ losses for
the bad generalization ability caused by the large search different NAS methods in Fig. 7. Again, we can find that
space,asTheorem1suggested.Second,thankstoindepen- the losses obtained by our DNA are indicative of the real
dent training, the model ranking of MnasNet is better than performance of the architectures. In contrast, the losses
DARTS and SPOS. However, the cost of MnasNet is quite predicted by DARTS and SPOS are not indicative of the
high.Ontheonehand,8,000outofthe1013architecturesare architectures’trueaccuracies.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 12
TABLE8:Robustnesstodifferentrandomseeds.
16th and 20th epochs. This implies that we can use the
Seed 0 41 42
converged supernet (e.g., the last epoch) to search for the
KendalltautoAcc 0.64 0.64 0.64
Kendalltautoseed0 - 1.0 1.0 architecture.Notethattheaccuracyincreasesrapidlyinthe
earlystagewiththesametendencyoftraininglossdecreas-
TABLE9:Robustnesstodifferenttrainingdataamounts. ing, which evidences a correlation between the accuracy
Datasetpartition full 80% 40% of the searched architecture and the loss of the supernet.
KendalltautoAcc 0.64 0.63 0.64 The stably increasing performance of searched architecture
Kendalltautofulldata - 0.98 1.0
provesthestabilityofDNA.
Reproducibility/Robustness.NASisworryingforbeing
Quantitative analysis. We further provide a quantita-
difficult to reproduce [9]. For instance, the seed is critical
tive analysis of the effectiveness and efficiency of the four
in reproducing DARTS results, possibly because of their
NAS methods. Following [8], [9], we measure effectiveness
sensitivitytoinitialweights.Toevaluatethereproducibility
by ranking correlation metrics, e.g., Kendall Tau (τ) [92],
of DNA, we conducted repeated experiments with two
Spearman Rho (ρ) and Pearson R (R). All of the three
ablation types: i) different random seeds and ii) different
metricsrangefrom-1to1with“-1”representingcompletely
dataamounts.Forseeds,wechoose{0,41,42}thataffectthe
reversed ranking, “1” meaning entirely correct ranking,
supernet weight initialization and random path sampling
and “0” representing no correlation between rankings. We
during the search. Table 8 shows that model ranking is
measureefficiencybysearchcosttomeasure.
stable for different seeds, proving the high reproducibility
Table 7 shows the results. First, although DARTS is
and robustness of DNA. For data amounts, we randomly
renowned for high efficiency, it is less efficient than SPOS
sampled a subset of the ImageNet training set as the new
and DNA due to a large memory print. Second, DARTS,
training set and kept the total training steps unchanged.
SPOS, and MnasNet with a 0.2 learning rate might be inef-
Table 9 shows that, with the training data being reduced
fective – they are no better than or struggle to outperform
to 80% and further to 40%, DNA’s model ranking remains
randomarchitectureselectionwithτ ≈ 0).Third,MnasNet
stable. A detailed model ranking comparison of different
with a 0.001 learning rate achieves noticeably good effec-
data amounts is shown in Fig. 9. To remove the effect of
tiveness(τ =0.61)atthecostofsignificantlylowefficiency
variance in scale and give a more intuitive comparison, we
(288TPUdays).OurDNAobtainsevenbettereffectiveness(
performstandardizationoverthetotallosses.Every3points
τ =0.64vs.τ =0.61)withremarkablyhigherefficiency(8.5
areassociatedwiththesameaccuracy.Asshown,thepoints
GPUdaysvs.288TPUdays).Insummary,DNAistheonly
ofeverysinglemodelarecloselybonded,andtheregression
memberthatachievesbothhigheffectivenessandefficiency.
lines of the three data amounts coincide with each other.
Training stability. To further validate the search ef-
ThisprovesDNA’srobustnesstothedataamount,implying
fectiveness, we examine whether DNA could consistently
thatDNAcanbeadata-efficientfew-shotNASmethod.
find better architectures as the supernet training goes on.
Feature map visualization.Severalfeaturemaps(Block
We compare DNA with two supernet-based methods, i.e.,
2and4,Epoch16)oftheteacherandstudentarevisualized
SPOSandDARTS.Foreachmethod,wepick8intermediate
in Fig. 10. As shown, our student supernet, although with
checkpointsinthesupernettrainingprogressandsearchfor
only 1/3 of the layer numbers of the teacher model, can
thebestarchitectureateachcheckpoint.Wethenrandomly
imitate the teacher very well. The textures are incredibly
re-initialize the network weights of the architecture and
close at every channel, even on highly abstracted 14×14
retrain them from scratch to convergence to obtain their
feature maps of Block 4, proving the effectiveness of DNA
real performance. Fig. 8 shows the results. First, for SPOS
indistillingknowledgefromtheteacher.
and DARTS, the performance of the searched architecture
fluctuatesrandomlyasthesupernettraininggoes,although
the training loss of the supernet keeps decreasing. This 4.6 AblationStudy
makes good architectures almost inaccessible because we
Distillation strategy. We use a parallel distillation strategy
cannot decide which epoch of the supernet checkpoint is
in our DNA, i.e., we distill different blocks of the student
optimal.Second,theaccuracyofoursearchedarchitectures
supernet concurrently since the input and supervision of
increases progressively (i.e., from 75.7% to 77.5%) as the
each student block are the feature maps of the teacher that
supernet training goes on until convergence between the
can be pre-computed. We compare our parallel strategy to
two progressive distillation strategies. A progressive strat-
egymeanstrainingandsearchingeachblockofthestudent
supernet one by one progressively, in which: one trains Block
1ofthesupernetandperformsarchitecturesearchinBlock
1 to obtain m optimal blocky architectures; then, with the
m blocky architectures, one trains Block 2 of the supernet
and perform architecture search in Block 1&2 to obtain m
optimal blocky architectures; the cycle is repeated until the
searchisdone.Thetwoprogressivestrategiesdifferinthat:
(A) when training the i-th block of the supernet, all the i
blocks are trained from scratch; (B) when training the i-th
block of the supernet, only the i-th block is trained from
Fig.9:Modelrankingofdifferentdataamounts. scratch with the weights of the previous (i-1) blocks of theIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 13
teacher student teacher student
Block 2 Block 4
Fig.10:Comparisonofthefeaturemapsbetweentheteacherandstudentoftwoblocks.
TABLE10:ComponentanalysisofDNA.Cons.:constraint.
Strategy name Cell Cons. Params Acc@1 Acc@5
mIOU vs FLOPS mIOU vs Params
A 1 5.18M 77.0% 93.34% 82 80.0
progressive
B 1 5.58M 77.15% 93.51% 81 79.5
1 5.69M 77.49% 93.68% 80 79.0
3 6.26M 77.84% 93.74%
concurrent Our 1 ✓ 5.09M 77.21% 93.50% 79 78.5
3 ✓ 5.28M 77.38% 93.60% 78 78.0
77.5
TABLE 11: DNA with different teachers. Note that all the DNA (Ours) DNA (Ours)
searched models are retrained from scratch without any su- 77 D D De e ee e ep p pL L La a ab b bv v v3 3 3( + +R ( (e R Xs ceN ese N pt te1 iot0 1 n1 0 )) 1) 77.0 D D De e ee e ep p pL L La a ab b bv v v3 3 3( + +R ( (e R Xs ceN ese N pt te1 iot0 1 n1 0 )) 1)
76 76.5
pervisionoftheteacher.†:Astheinputsandsupervisionsofa 0 50 100 FLO1 P5 S0 (Billi2 o0 n0
s)
250 300 350 20 Numb3 e0
r of
Param4 s0 (Million)50 60
studentblockarepreciselytheteachermodel’sinternalfeature (a) VOCmIoUvs.FLOPs (b) VOCmIOUvs.Params
maps,EfficientNet-B7istestedwithaninputsizeof224×224 Fig.11:Trade-offbetweenmodelaccuracyandmodelcomplex-
toadapttothestudent. ityonPASCALVOC12segmentation.Left:modelaccuracyvs.
Role Model Params Acc@1 Acc@5
FLOPs;Right:modelaccuracyvs.parameternumbers.
teacher EfficientNet-B0 5.28M 76.3% 93.2%
student DNA-from-B0 5.27M 77.8% 93.7%
(5.27Mvs.5.28M),whichprovesthattheperformanceofour
teacher EfficientNet-B7 66.4M 77.8%† 93.8%†
architecturedistillationisnotrestrictedbytheperformance
student DNA-c 5.28M 77.8% 93.7%
student DNA-c7(224×224) 64.9M 79.9% 94.9% of the teacher. As implied, we could improve the capacity
of any architecture by self-distilling the architecture. Sec-
ond,asoursearchedarchitectureDNA-cachievesthesame
supernet being frozen. The results in Table 10 prove the
accuracy as its teacher with significantly fewer parameters
superiorityofourstrategy.
(5.28Mvs.66.4M),wescaleourDNA-ctothesimilarmodel
Single cell vs. multi cells. To examine the multi-cell
sizeastheteacherfollowing[3].Astheinputsandsupervi-
search’simpact,weperformDNAwithasinglecellineach
sionsofastudentblockareexactlytheinternalfeaturemaps
block for comparison. As shown in Table 10, the multi-cell
of the teacher model, both EfficientNet-B7 and DNA-c7 in
searchimprovesthetop-1accuracyby0.2%underthecon-
Table11aretestedwithaninputsizeof224×224.Besides,
straintof5.3Mparametersandby0.3%undernoconstraint.
DNA-c7 in Table 11 is also trained with an input size of
Note that the single-cell case obtained an architecture with
224×224. Remarkably, the scaled architecture (i.e., DNA-
fewer parameters under the constraint of 5.3M parameters,
c7) outperforms the teacher (i.e., EfficientNet-B7) by 2.1%,
which might be ascribed to the relatively lower variability
demonstratingthepracticabilityandscalabilityofourDNA.
ofthechannelandlayernumbers.
Third(last but not least),althoughtheperformanceofour
Teacher dependency. As knowledge distillation is em-
architecturedistillationisnotrestrictedbytheperformance
ployed in our search stage (but NOT in the retraining
of the teacher, recurrently scaling our student to be a new
stage),wemustclarifywhetherthecapacityofoursearched
teachercansearchforbetterandbetterarchitectures,assug-
architectureislimitedbythatofateacher.First,wereplace
gested by our DNA+. This indicates that we can randomly
ourseniorteacher(i.e.,EfficientNet-B7)withajuniorteacher
select an existing model as our teacher without allocating
(i.e., EfficientNet-B0) to supervise architecture search un-
too much effort. Then we can perform recurrent NAS to
der the same constraint. The result is shown in Table 11.
obtaintheoptimalarchitecture.
Surprisingly, the performance of the searched architecture
guidedbyEfficientNet-B0isalmostthesameasthatguided
4.7 SemanticSegmentationonVOC12andADE20K
byEfficientNet-B7,whichindicatesthattheperformanceof
our DNA does not necessarily rely on a high-performing PASCALVOC12.Ashandcraftedarchitecturesoftenexhibit
teacher. More importantly, we can find that DNA-from-B0 good generalization beyond ImageNet/CIFAR classifica-
searched by using EfficientNet-B0 as a teacher significantly tion, we examine the universality of our searched archi-
outperforms its teacher by 1.5% with the same model size tectures. We take semantic segmentation on the challeng-
)%(
UOIm
)%(
UOImIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 14
ing PASCAL VOC12 benchmark [60] as an example. This this model is already the largest available in its paper and
dataset includes 21 categories, with 10,582 training images onitsGitHubpage.
( [60], [93]) and 1,449 pixel-level labeled images in the val-
idation set. We measure accuracy using the standard pixel
4.8 ObjectDetectiononMSCOCO
intersection-over-union metric averaged across the 21 cate-
gories (mIOU). We replaced the backbone of DeepLabv3+ TABLE 14: Object detection results of Faster RCNN with dif-
[94],atop-performingsegmentationmodel,withourDNA- ferentbackbonesonCOCO.
c3,formingDNA-Seg.Otherprotocolsalignwith[94].
Backbone EfficientNetB0[3] FBNetV3-A[74] DNA-d
Complexityvs.accuracy.Examiningtheperformanceon
mAP(%) 30.2 30.5 32.3
thetestset(i.e.,fightingforachampionoftheleaderboard)
of VOC12 segmentation usually includes many unknown In order to further assess the transferability of the
tricks (e.g., employing multi-scale inputs during the evalu- searched models across various tasks, we employ DNA-
ation, adding left-right flipped inputs, pre-training on MS- d as a substitute for the backbone feature extractor in
COCO dataset [62], pre-training on JFT-300M dataset [95], Faster R-CNN, specifically using the conv4 (C4) backbone,
and ensembling multi-model), making the model complex- following the paper of FBNetV3 [74]. We then proceed to
ity of different methods unclear. To avoid this ambiguity, compare the performance of this configuration with other
we compare the complexity and accuracies of different modelsontheCOCOdetectiondataset.Wehaveconducted
models on the validation set of VOC12 segmentation for a comprehensive comparison of our method with state-of-
a fair comparison. The results are presented in Fig. 11. the-art approaches such as FBNetV3 [74] and EfficientNet-
Under similar accuracy constraint, our DNA-Seg use 21× B0 (selected based on the recommendation of the FBNetV3
fewer FLOPs and 4.5× fewer parameters than the recent paper [74]) on the MSCOCO dataset [62]. The results, as
DeepLabv3+(ResNet101), 6× fewer FLOPs and 4.2× fewer depicted in Table 14, unequivocally reveal the substantial
parameters than the recent DeepLabv3+(Xception). In par- superiorityofourmethodovertheseesteemedcompetitors,
ticular, with a single-model and single test-time scale, our surpassingthembyaclearmargin(32.3%vs.30.5%).
DNA-Seg achieves state-of-the-art 79.76% validation IoU
with12.75Mparametersand14.01FLOPs.
5 CONCLUSION
TABLE12:SegmentationtaskswithCNNsonADE20K.
In this paper, we employ a tool for generalization bound-
EfficientNet[3] edness to link weight-sharing NAS’s inefficiency to unre-
Backbone FBNetV5[96] DNA-d
-B0[3] -B1[3] -B2[3]
liable architecture ratings due to a vast search space. To
mIoU(%) 38.9 39.2 40.4 40.4 40.7 address this issue, we modularize the search space into
blocks and apply distilling neural architecture techniques.
TABLE13:SegmentationtaskswithViTsonADE20K. We explore three block-wise learning methods: supervised
Backbone NASViT[42] S3[43] DNA-T++ learning (DNA), progressive learning (DNA+), and self-
supervised learning (DNA++). Our DNA family evaluates
mIoU(%) 41.4 46.3 47.3
all candidate architectures, a significant advancement over
ADE20K. In addition to PASCAL VOC12, we further in- prior methods restricted to smaller sub-search spaces via
clude experiments on ADE20K [61], expanding the scope heuristicalgorithms.Additionally,ourapproachenablesthe
of our evaluation. For the segmentation task, we have search for architectures with varying depths and widths
employed the Atrous Spatial Pyramid Pooling framework underspecifiedcomputationalconstraints.Recognizingthe
[97] as our segmenter. The results, as presented in Table pivotal role of architecture rating in NAS, we provide ex-
12, clearly demonstrate that our DNA method surpasses tensiveempiricalresultstoscrutinizethisaspect.Lastly,our
state-of-the-art efficient backbone methods by a significant method attains state-of-the-art results across various tasks.
margin. For instance, the performance comparison reveals FutureworkwillextendtheapplicationofourDNAmethod
that our DNA method achieves superior results (40.7%) to NLP, 3D architectures [98], [99], [100], and generative
compared to EfficientNet-B0 [3] (38.9%), EfficientNet-B1 [3] models[101],[102],[103].
(39.2%), and EfficientNet-B2 [3] (40.4%). Furthermore, it is
worth noting that even the accuracy reported by FBNetV5
6 HIGHLIGHT AND OUTLOOK
[96](40.4%)onADE20Kisinferiortotheperformanceofour
DNA method. These findings substantiate the effectiveness NAS has been a significant area of research and develop-
andsuperiorityofourapproach. ment in deep learning and AI. Nevertheless, it is plausible
We further benchmark our approach against the two thattheextentofattentiongarneredbyNAShasundergone
distinguished ViT-based methods, specifically NASViT [42] evolutionorvariationovertime.Severalconceivablefactors
and S3 [43]. We have employed DNA-T++ as our selected may elucidate why NAS has witnessed diminished promi-
backbone architecture. Under the UperNet configuration nence during certain junctures or within specific contexts:
(whichisidenticaltothesettingusedinS3),ourmodelhas (a) Architecture Ranking: we believe that the suboptimal
demonstrated an exceptional accuracy rate of 47.3% on the architecture ranking is a key factor contributing to the
ADE20k dataset, outperforming both NASViT (41.4%) and reducedattentiononNAS.Specifically,thereexistsasignif-
S3(46.27%).Itisofutmostimportancetoemphasizethatour icant apprehension pertaining to the efficacy of the search
modelexhibitsalargerscaleincomparisontoNASViT(5.6G process,makingtheresearchersandpractitionersworrying.
vs.1.9G),seeTable13.However,it’sworthmentioningthat To be more precise, this concern centers on the correlationIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 15
between program performance during the search phase, [7] C.White,W.Neiswanger,andY.Savani,“BANANAS:bayesian
specificallyonsmallproxytasks,andtheirsubsequentper- optimization with neural architectures for neural architecture
search,” in Thirty-Fifth AAAI Conference on Artificial Intelligence,
formanceonthefinaltask.Intheabsenceofarobustlinkage
AAAI2021. AAAIPress,2021,pp.10293–10301. 1,2
between these two tasks, researchers may adopt a cautious [8] C. Sciuto, K. Yu, M. Jaggi, C. Musat, and M. Salzmann,
stancetowardsNAS.(b) Maturation of Architectures:sev- “Evaluating the search phase of neural architecture search,”
eral deep learning architectures, such as CNNs and ViTs, in 8th International Conference on Learning Representations, ICLR
2020, 2020. [Online]. Available: https://openreview.net/forum?
had matured and demonstrated strong performance across
id=H1loF2NFwr 1,12
a wide range of applications. Without resolving the archi- [9] A. Yang, P. M. Esperanc¸a, and F. M. Carlucci, “{NAS}
tecture ranking issue, there may be less urgency to search evaluation is frustratingly hard,” in 8th International Conference
for new architectures. (c) Computational Cost: NAS is onLearningRepresentations,ICLR2020,,2020.[Online].Available:
https://openreview.net/forum?id=HygrdpVKvr 1,12
computationally expensive and time-consuming, requiring
[10] J. Peng, J. Zhang, C. Li, G. Wang, X. Liang, and L. Lin, “Pi-
significant computational resources to search for optimal nas:Improvingneuralarchitecturesearchbyreducingsupernet
neuralnetworkarchitectures.Researchersandorganizations training consistency shift,” in IEEE International Conference on
Computer Vision, ICCV 2021. IEEE Computer Society, 2021. 1,
might prioritize more efficient approaches to model de-
6
signandoptimizationunlessthearchitecturerankingissue
[11] G.E.Hinton,O.Vinyals,andJ.Dean,“Distillingtheknowledge
is addressed. (d) Foundation Models: foundation models inaneuralnetwork,”CoRR,vol.abs/1503.02531,2015.[Online].
[104] have become popular techniques for many AI tasks. Available:http://arxiv.org/abs/1503.02531 1,2,4
[12] B.ZophandQ.V.Le,“Neuralarchitecturesearchwithreinforce-
These approaches allow practitioners to leverage existing
mentlearning,”in5thInternationalConferenceonLearningRepre-
architectures and adapt them to specific tasks without the sentations,ICLR2017,Toulon,France,April24-26,2017,Conference
need for extensive architecture search, unless the architec- TrackProceedings,2017. 2
turerankingproblemraisedinourpaperisresolved. [13] Z.Zhong,J.Yan,W.Wu,J.Shao,andC.Liu,“Practicalblock-wise
neuralnetworkarchitecturegeneration,”in2018IEEEConference
As evident, the resolution of the architecture ranking
onComputerVisionandPatternRecognition,CVPR2018,SaltLake
issueholdsthecapacitytoalleviateallthepreviouslymen- City,UT,USA,June18-22,2018,2018,pp.2423–2432. 2
tionedchallenges.Thus,thecentralfocusofthispaperisto [14] B. Baker, O. Gupta, N. Naik, and R. Raskar, “Designing neural
diligently address the architecture ranking challenge with
networkarchitecturesusingreinforcementlearning,”in5thInter-
nationalConferenceonLearningRepresentations,ICLR2017,Toulon,
theaspirationofbolsteringtheefficacyofNASmethods.
France,April24-26,2017,ConferenceTrackProceedings,2017. 2
[15] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural archi-
tecturesearchontargettaskandhardware,”in7thInternational
ACKNOWLEDGEMENT Conference on Learning Representations, ICLR 2019, New Orleans,
LA,USA,May6-9,2019,2019. 2,3,4,8
This work is supported by the following grants: Na- [16] H. Liu, K. Simonyan, and Y. Yang, “DARTS: differentiable ar-
tional Key R&D Program of China under Grant No. chitecture search,” in 7th International Conference on Learning
Representations,ICLR2019,NewOrleans,LA,USA,May6-9,2019,
2021ZD0111601, National Natural Science Foundation
2019. 2,3,4,10,11
of China (NSFC) under Grant No.61836012, 62006255, [17] X.DongandY.Yang,“Searchingforarobustneuralarchitecture
62325605, GuangDong Basic and Applied Basic Research in four GPU hours,” in IEEE Conference on Computer Vision and
Foundation under Grant No. 2023A1515011374, Guang- PatternRecognition,CVPR2019,LongBeach,CA,USA,June16-20,
2019,2019,pp.1761–1770. 2
Dong Province Key Laboratory of Information Security
[18] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, “SMASH: one-
Technology. shot model architecture search through hypernetworks,” in 6th
International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
REFERENCES Proceedings,2018. 2
[19] B.Wu,X.Dai,P.Zhang,Y.Wang,F.Sun,Y.Wu,Y.Tian,P.Vajda,
[1] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, X. Chen, and Y.Jia,andK.Keutzer,“Fbnet:Hardware-awareefficientconvnet
X. Wang, “A comprehensive survey of neural architecture design via differentiable neural architecture search,” in IEEE
search: Challenges and solutions,” ACM Computing Surveys, ConferenceonComputerVisionandPatternRecognition,CVPR2019,
2021.[Online].Available:https://dx.doi.org/10.1145/3447582 1 LongBeach,CA,USA,June16-20,2019,2019,pp.10734–10742. 2,
[2] M.Tan,B.Chen,R.Pang,V.Vasudevan,M.Sandler,A.Howard, 3,4,8
and Q. V. Le, “Mnasnet: Platform-aware neural architecture [20] Z.Guo,X.Zhang,H.Mu,W.Heng,Z.Liu,Y.Wei,andJ.Sun,
search for mobile,” in IEEE Conference on Computer Vision and “Single path one-shot neural architecture search with uniform
PatternRecognition,CVPR2019,LongBeach,CA,USA,June16-20, sampling,”CoRR,vol.abs/1904.00420,2019. 2,3,8,10,11
2019,2019,pp.2820–2828. 1,2,7,8,10,11 [21] X.Chu,B.Zhang,R.Xu,andJ.Li,“Fairnas:Rethinkingevalua-
[3] M. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling tionfairnessofweightsharingneuralarchitecturesearch,”CoRR,
for convolutional neural networks,” in Proceedings of the 36th vol.abs/1907.01845,2019. 2,3,8,10
InternationalConferenceonMachineLearning,ICML2019,9-15June [22] G.Bender,P.Kindermans,B.Zoph,V.Vasudevan,andQ.V.Le,
2019,LongBeach,California,USA,2019,pp.6105–6114. 1,2,6,7, “Understanding and simplifying one-shot architecture search,”
8,9,13,14 inProceedingsofthe35thInternationalConferenceonMachineLearn-
[4] Y. Chen, G. Meng, Q. Zhang, S. Xiang, C. Huang, L. Mu, and ing,ICML2018,Stockholmsma¨ssan,Stockholm,Sweden,July10-15,
X. Wang, “RENAS: reinforced evolutionary neural architecture 2018,2018,pp.549–558. 2,4
search,”inIEEEConferenceonComputerVisionandPatternRecog- [23] X. Li, C. Lin, C. Li, M. Sun, W. Wu, J. Yan, and W. Ouyang,
nition,CVPR2019,LongBeach,CA,USA,June16-20,2019,2019, “Improvingone-shotNASbysuppressingtheposteriorfading,”
pp.4787–4796. 1,2 in2020IEEE/CVFConferenceonComputerVisionandPatternRecog-
[5] R. Negrinho and G. J. Gordon, “Deeparchitect: Automati- nition,CVPR2020,Seattle,WA,USA,June13-19,2020. Computer
cally designing and training deep architectures,” CoRR, vol. VisionFoundation/IEEE,2020,pp.13833–13842. 2,4,5,8,10
abs/1704.08792,2017. 1,2 [24] J.Baxter,“Abayesian/informationtheoreticmodeloflearningto
[6] K.Kandasamy,W.Neiswanger,J.Schneider,B.Poczos,andE.P. learnviamultipletasksampling,”Mach.Learn.,vol.28,no.1,pp.
Xing,“Neuralarchitecturesearchwithbayesianoptimisationand 7–39,1997. 2
optimal transport,” in Advances in neural information processing [25] C.Li,J.Peng,L.Yuan,G.Wang,X.Liang,L.Lin,andX.Chang,
systems,vol.31,2018. 1,2 “Block-wiselysupervisedneuralarchitecturesearchwithknowl-IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 16
edge distillation,” in Proceedings of the IEEE/CVF Conference on [43] M. Chen, K. Wu, B. Ni, H. Peng, B. Liu, J. Fu, H. Chao, and
ComputerVisionandPatternRecognition,2020,pp.1989–1998. 2 H. Ling, “Searching the search space of vision transformer,”
[26] J. Ba and R. Caruana, “Do deep nets really need to be deep?” Advances in Neural Information Processing Systems, vol. 34, pp.
in Advances in Neural Information Processing Systems 27: Annual 8714–8726,2021. 3,14
ConferenceonNeuralInformationProcessingSystems2014,December [44] H.Pham,M.Y.Guan,B.Zoph,Q.V.Le,andJ.Dean,“Efficient
8-132014,Montreal,Quebec,Canada,2014,pp.2654–2662. 2 neural architecture search via parameter sharing,” ArXiv, vol.
[27] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and abs/1802.03268,2018. 3,2
Y.Bengio,“Fitnets:Hintsforthindeepnets,”in3rdInternational [45] D.H.Ballard,“Modularlearninginneuralnetworks,”inProceed-
ConferenceonLearningRepresentations,ICLR2015,SanDiego,CA, ingsofthesixthNationalConferenceonartificialintelligence-volume
USA,May7-9,2015,ConferenceTrackProceedings,2015. 2 1,1987,pp.279–284. 3
[28] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowledge dis- [46] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.
tillation: Fast optimization, network minimization and transfer Gomez,L.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”
learning,”in2017IEEEConferenceonComputerVisionandPattern in Advances in Neural Information Processing Systems 30: Annual
Recognition,CVPR2017,Honolulu,HI,USA,July21-26,2017,2017, ConferenceonNeuralInformationProcessingSystems2017,4-9De-
pp.7130–7138. 2 cember2017,LongBeach,CA,USA,2017,pp.5998–6008. 4
[29] H. Wang, H. Zhao, X. Li, and X. Tan, “Progressive blockwise [47] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-
knowledgedistillationforneuralnetworkacceleration,”inPro- trainingofdeepbidirectionaltransformersforlanguageunder-
ceedingsoftheTwenty-SeventhInternationalJointConferenceonAr- standing,”inProceedingsofthe2019ConferenceoftheNorthAmeri-
tificialIntelligence,IJCAI2018,July13-19,2018,Stockholm,Sweden, canChapteroftheAssociationforComputationalLinguistics:Human
2018,pp.2769–2775. 2 LanguageTechnologies,NAACL-HLT2019,Minneapolis,MN,USA,
June2-7,2019,Volume1(LongandShortPapers),2019,pp.4171–
[30] T.Furlanello,Z.Lipton,M.Tschannen,L.Itti,andA.Anandku-
mar,“Bornagainneuralnetworks,”inInternationalConferenceon 4186. 4
MachineLearning,2018,pp.1607–1616. 2,5 [48] C.Dong,G.Wang,H.Xu,J.Peng,X.Ren,andX.Liang,“Efficient-
bert: Progressively searching multilayer perceptron via warm-
[31] H. Mobahi, M. Farajtabar, and P. L. Bartlett, “Self-distillation
upknowledgedistillation,”inFindingsofConferenceonEmpirical
amplifiesregularizationinhilbertspace,”inAdvancesinNeural
MethodsinNaturalLanguageProcessing,EMNLP2021,2021. 4,6
InformationProcessingSystems(NeurIPS),2020. 2
[49] X. Chu, B. Zhang, J. Li, Q. Li, and R. Xu, “Scarletnas: Bridging
[32] Q.Xie,M.-T.Luong,E.Hovy,andQ.V.Le,“Self-trainingwith
the gap between scalability and fairness in neural architecture
noisystudentimprovesimagenetclassification,”inProceedingsof
search,”CoRR,vol.abs/1908.06022,2019. 4,7,8,2
theIEEE/CVFConferenceonComputerVisionandPatternRecogni-
[50] F. Liang, C. Lin, R. Guo, M. Sun, W. Wu, J. Yan, and
tion,2020,pp.10687–10698. 2
W. Ouyang, “Computation reallocation for object detection,”
[33] C.Liu,P.Dolla´r,K.He,R.B.Girshick,A.L.Yuille,andS.Xie,
in 8th International Conference on Learning Representations, ICLR
“Arelabelsnecessaryforneuralarchitecturesearch?”inComputer
2020, 2020. [Online]. Available: https://openreview.net/forum?
Vision - ECCV 2020 - 16th European Conference, Glasgow, UK,
id=SkxLFaNKwB 5
August 23-28, 2020, Proceedings, Part IV, ser. Lecture Notes in
[51] T.Chen,S.Kornblith,M.Norouzi,andG.E.Hinton,“Asimple
ComputerScience,vol.12349. Springer,2020,pp.798–813. 2
frameworkforcontrastivelearningofvisualrepresentations,”in
[34] X. Zhang, P. Hou, X. Zhang, and J. Sun, “Neural architecture
Proceedingsofthe37thInternationalConferenceonMachineLearning,
searchwithrandomlabels,”inIEEEConferenceonComputerVision
ICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of
and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021.
Machine Learning Research, vol. 119. PMLR, 2020, pp. 1597–
ComputerVisionFoundation/IEEE,2021,pp.10907–10916. 2
1607. 6
[35] S.Yan,Y.Zheng,W.Ao,X.Zeng,andM.Zhang,“Doesunsuper-
[52] J. Grill, F. Strub, F. Altche´, C. Tallec, P. H. Richemond,
vised architecture representation learning help neural architec- E. Buchatskaya, C. Doersch, B. A´. Pires, Z. Guo, M. G. Azar,
turesearch?”inAdvancesinNeuralInformationProcessingSystems
B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko, “Bootstrap
33: Annual Conference on Neural Information Processing Systems
yourownlatent-Anewapproachtoself-supervisedlearning,”
2020,NeurIPS2020,December6-12,2020,virtual,2020. 2
in Advances in Neural Information Processing Systems 33: Annual
[36] G.Wang,L.Lin,R.Chen,G.Wang,andJ.Zhang,“Jointlearning ConferenceonNeuralInformationProcessingSystems2020,NeurIPS
ofneuraltransferandarchitectureadaptationforimagerecogni- 2020,December6-12,2020,virtual,2020. 6
tion,”IEEETransactionsonNeuralNetworksandLearningSystems [53] G.Wang,K.Wang,G.Wang,P.H.S.Torr,andL.Lin,“Solving
(T-NNLS),2021. 2 inefficiencyofself-supervisedrepresentationlearning,”inIEEE
[37] C.Li,T.Tang,G.Wang,J.Peng,B.Wang,X.Liang,andX.Chang, International Conference on Computer Vision, ICCV 2021. IEEE
“BossNAS:Exploringhybridcnn-transformerswithblock-wisely ComputerSociety,2021. 6
self-supervisedneuralarchitecturesearch,”inIEEEInternational [54] G.Wang,Y.Tang,L.Lin,andP.H.Torr,“Semantic-awareauto-
Conference on Computer Vision, ICCV 2021. IEEE Computer encodersforself-supervisedrepresentationlearning,”inProceed-
Society,2021. 3,6,9 ings of the IEEE/CVF Conference on Computer Vision and Pattern
[38] B. Chen, P. Li, C. Li, B. Li, L. Bai, C. Lin, M. Sun, J. Yan, and Recognition(CVPR),June2022,pp.9664–9675. 6
W.Ouyang,“Glit:Neuralarchitecturesearchforglobalandlocal [55] X. Chen and K. He, “Exploring simple siamese representation
image transformer,” in Proceedings of the IEEE/CVF International learning,” in IEEE Conference on Computer Vision and Pattern
ConferenceonComputerVision,2021,pp.12–21. 3,9,10 Recognition, CVPR 2021, virtual, June 19-25, 2021. Computer
[39] X. Su, S. You, J. Xie, M. Zheng, F. Wang, C. Qian, C. Zhang, VisionFoundation/IEEE,2021,pp.15750–15758. 6
X. Wang, and C. Xu, “Vitas: Vision transformer architecture [56] A.Bardes,J.Ponce,andY.LeCun,“Vicreg:Variance-invariance-
search,” in Computer Vision–ECCV 2022: 17th European Confer- covariance regularization for self-supervised learning,” CoRR,
ence, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI. vol.abs/2105.04906,2021. 6
Springer,2022,pp.139–157. 3,9,10 [57] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, “Barlow
[40] M. Chen, H. Peng, J. Fu, and H. Ling, “Autoformer: Search- twins: Self-supervised learning via redundancy reduction,” in
ing transformers for visual recognition,” in Proceedings of the Proceedingsofthe38thInternationalConferenceonMachineLearning,
IEEE/CVF international conference on computer vision, 2021, pp. ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of
12270–12280. 3,9,10 MachineLearningResearch,vol.139. PMLR,2021,pp.12310–
[41] J.Yu,P.Jin,H.Liu,G.Bender,P.-J.Kindermans,M.Tan,T.Huang, 12320. 6
X.Song,R.Pang,andQ.Le,“Bignas:Scalingupneuralarchitec- [58] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
ture search with big single-stage models,” in Computer Vision– “Imagenet: A large-scale hierarchical image database,” in 2009
ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28, IEEE conference on computer vision and pattern recognition, 2009,
2020,Proceedings,PartVII16. Springer,2020,pp.702–717. 3 pp.248–255. 7
[42] C.Gong,D.Wang,M.Li,X.Chen,Z.Yan,Y.Tian,V.Chandra [59] A. Krizhevsky and G. Hinton, “Learning multiple layers of
et al., “Nasvit: Neural architecture search for efficient vision featuresfromtinyimages,”Master’sthesis,DepartmentofComputer
transformerswithgradientconflictawaresupernettraining,”in Science,UniversityofToronto,2009. 7
InternationalConferenceonLearningRepresentations,2021. 3,9,10, [60] M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams,
14 J.M.Winn,andA.Zisserman,“ThepascalvisualobjectclassesIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 17
challenge:Aretrospective,”Int.J.Comput.Vis.,vol.111,no.1,pp. J.Uszkoreit,andN.Houlsby,“Animageisworth16x16words:
98–136,2015. 7,14 Transformersforimagerecognitionatscale,”inICLR,2021.9,10
[61] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and [82] L.Yuan,Y.Chen,T.Wang,W.Yu,Y.Shi,Z.Jiang,F.E.Tay,J.Feng,
A. Torralba, “Semantic understanding of scenes through the and S. Yan, “Tokens-to-token vit: Training vision transformers
ade20kdataset,”InternationalJournalofComputerVision,vol.127, fromscratchonimagenet,”inICCV,2021. 9,10
pp.302–321,2019. 7,14 [83] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo,
[62] T.Lin,M.Maire,S.J.Belongie,J.Hays,P.Perona,D.Ramanan, “Swintransformer:Hierarchicalvisiontransformerusingshifted
P.Dolla´r,andC.L.Zitnick,“MicrosoftCOCO:commonobjects windows,”inICCV,2021. 9,10
in context,” in Computer Vision - ECCV 2014 - 13th European [84] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia,
Conference, Zurich, Switzerland, September 6-12,2014, Proceedings, and C. Shen, “Twins: Revisiting the design of spatial attention
PartV,2014,pp.740–755. 7,14 invisiontransformers,”AdvancesinNeuralInformationProcessing
[63] M.Sandler,A.G.Howard,M.Zhu,A.Zhmoginov,andL.Chen, Systems,vol.34,pp.9355–9366,2021. 9,10
“Mobilenetv2:Invertedresidualsandlinearbottlenecks,”in2018 [85] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
IEEEConferenceonComputerVisionandPatternRecognition,CVPR “Rethinking the inception architecture for computer vision,” in
2018, Salt Lake City, UT, USA, June 18-22, 2018, 2018, pp. 4510– CVPR,2016. 10
4520. 7 [86] G.Huang,Y.Sun,Z.Liu,D.Sedra,andK.Q.Weinberger,“Deep
networkswithstochasticdepth,”inECCV,2016. 10
[64] X. Chu, B. Zhang, and R. Xu, “Moga: Searching beyond
[87] S.Yun,D.Han,S.J.Oh,S.Chun,J.Choe,andY.Yoo,“Cutmix:
mobilenetv3,” CoRR, vol. abs/1908.01314, 2019. [Online].
Regularizationstrategytotrainstrongclassifierswithlocalizable
Available:http://arxiv.org/abs/1908.01314 7,8,2
features,”inICCV,2019. 10
[65] S. Xie, R. B. Girshick, P. Dolla´r, Z. Tu, and K. He, “Aggregated
[88] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, “Randaugment:
residualtransformationsfordeepneuralnetworks,”in2017IEEE
Practical automated data augmentation with a reduced search
ConferenceonComputerVisionandPatternRecognition,CVPR2017,
space,”inNeurIPS,2020. 10
Honolulu,HI,USA,July21-26,2017,2017,pp.5987–5995. 7
[89] Z.Zhong,L.Zheng,G.Kang,S.Li,andY.Yang,“Randomerasing
[66] H.Touvron,M.Cord,M.Douze,F.Massa,A.Sablayrolles,and
dataaugmentation,”inAAAI,2020. 10
H.Je´gou,“Trainingdata-efficientimagetransformers&distilla-
[90] H.Zhang,M.Cisse,Y.N.Dauphin,andD.Lopez-Paz,“mixup:
tionthroughattention,”inICML,2021. 7,9,10
Beyondempiricalriskminimization,”inICLR,2018. 10
[67] J.Yu,L.Yang,N.Xu,J.Yang,andT.Huang,“Slimmableneural
[91] T. E. Arber Zela, T. Saikia, Y. Marrakchi, T. Brox, and F. Hut-
networks,”inInternationalConferenceonLearningRepresentations,
ter, “Understanding and robustifying differentiable architec-
2018. 7
ture search,” International Conference on Learning Representations
[68] C.Li,G.Wang,B.Wang,X.Liang,Z.Li,andX.Chang,“Dynamic (ICLR),2020. 11
slimmablenetwork,”inProceedingsoftheIEEE/CVFConferenceon [92] M.G.Kendall,“Anewmeasureofrankcorrelation,”Biometrika,
computervisionandpatternrecognition,2021,pp.8607–8617. 7 vol.30,no.1/2,pp.81–93,1938. 11,12
[69] Z. Jiang, C. Li, X. Chang, L. Chen, J. Zhu, and Y. Yang, “Dy- [93] B. Hariharan, P. Arbelaez, L. D. Bourdev, S. Maji, and J. Malik,
namicslimmabledenoisingnetwork,”IEEETransactionsonImage “Semanticcontoursfrominversedetectors,”inIEEEInternational
Processing,vol.32,pp.1583–1598,2023. 7 ConferenceonComputerVision,ICCV2011,Barcelona,Spain,Novem-
[70] C. Li, G. Wang, B. Wang, X. Liang, Z. Li, and X. Chang, “Ds- ber6-13,2011,2011,pp.991–998. 14
net++:Dynamicweightslicingforefficientinferenceincnnsand [94] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam,
transformers,”IEEETransactionsonPatternAnalysisandMachine “Encoder-decoderwithatrousseparableconvolutionforseman-
Intelligence(TPAMI),2022. 7,9,10 tic image segmentation,” in Computer Vision - ECCV 2018 -
[71] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: 15thEuropeanConference,Munich,Germany,September8-14,2018,
Practical data augmentation with no separate search,” arXiv Proceedings,PartVII,2018,pp.833–851. 14
preprintarXiv:1909.13719,2019. 8 [95] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting
[72] A. Howard, M. Sandler, G. Chu, L. Chen, B. Chen, M. Tan, unreasonableeffectivenessofdataindeeplearningera,”inIEEE
W.Wang,Y.Zhu,R.Pang,V.Vasudevan,Q.V.Le,andH.Adam, International Conference on Computer Vision, ICCV 2017, Venice,
“Searching for mobilenetv3,” CoRR, vol. abs/1905.02244, 2019. Italy,October22-29,2017,2017,pp.843–852. 14
[Online].Available:http://arxiv.org/abs/1905.02244 8,2 [96] B.Wu,C.Li,H.Zhang,X.Dai,P.Zhang,M.Yu,J.Wang,Y.Lin,
[73] M.TanandQ.V.Le,“Mixconv:Mixeddepthwiseconvolutional and P. Vajda, “Fbnetv5: Neural architecture search for multiple
kernels,” CoRR, vol. abs/1907.09595, 2019. [Online]. Available: tasksinonerun,”arXivpreprintarXiv:2111.10007,2021. 14
http://arxiv.org/abs/1907.09595 8,2 [97] L.-C.Chen,G.Papandreou,F.Schroff,andH.Adam,“Rethink-
[74] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, ingatrousconvolutionforsemanticimagesegmentation,”arXiv
Y. Tian, M. Yu, P. Vajda et al., “Fbnetv3: Joint architecture- preprintarXiv:1706.05587,2017. 14
recipe search using predictor pretraining,” in Proceedings of the [98] G.Wang,P.Wang,Z.Chen,W.Wang,C.C.Loy,andZ.Liu,“Perf:
IEEE/CVFConferenceonComputerVisionandPatternRecognition, Panoramicneuralradiancefieldfromasinglepanorama,”arXiv
2021,pp.16276–16285. 8,9,14 preprintarXiv:2310.16831,2023. 14
[99] G.Wang,Z.Chen,C.C.Loy,andZ.Liu,“Sparsenerf:Distilling
[75] D. Wang, M. Li, C. Gong, and V. Chandra, “Attentivenas: Im-
depthrankingforfew-shotnovelviewsynthesis,”arXivpreprint
proving neural architecture search via attentive sampling,” in
arXiv:2303.16196,2023. 14
Proceedings of the IEEE/CVF conference on computer vision and
[100] Z. Chen, G. Wang, and Z. Liu, “Scenedreamer: Unbounded
patternrecognition,2021,pp.6418–6427. 8,9
3d scene generation from 2d image collections,” arXiv preprint
[76] D. Wang, C. Gong, M. Li, Q. Liu, and V. Chandra, “Alphanet:
arXiv:2302.01330,2023. 14
Improvedtrainingofsupernetswithalpha-divergence,”inInter-
[101] G. Wang and P. H. Torr, “Traditional classification neural net-
nationalConferenceonMachineLearning. PMLR,2021,pp.10760–
works are good generators: They are competitive with ddpms
10771. 8,9
andgans,”arXivpreprintarXiv:2211.14794,2022. 14
[77] M. Tan and Q. Le, “Efficientnetv2: Smaller models and faster
[102] G. Wang, Y. Yang, C. C. Loy, and Z. Liu, “Stylelight: Hdr
training,”inInternationalconferenceonmachinelearning. PMLR,
panorama generation for lighting estimation and editing,” in
2021,pp.10096–10106. 8,9
EuropeanConferenceonComputerVision. Springer,2022,pp.477–
[78] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all:
492. 14
Train one network and specialize it for efficient deployment,”
[103] Z.Chen,G.Wang,andZ.Liu,“Text2light:Zero-shottext-driven
arXivpreprintarXiv:1908.09791,2019. 8,9,1 hdrpanoramageneration,”ACMTransactionsonGraphics(TOG),
[79] S.Kornblith,J.Shlens,andQ.V.Le,“Dobetterimagenetmodels vol.41,no.6,pp.1–16,2022. 14
transferbetter?”2019IEEE/CVFConferenceonComputerVisionand [104] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,
PatternRecognition(CVPR),pp.2656–2666,2018. 8 S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill
[80] Y. Huang, Y. Cheng, D. Chen, H. Lee, J. Ngiam, Q. V. Le, and et al., “On the opportunities and risks of foundation models,”
Z.Chen,“Gpipe:Efficienttrainingofgiantneuralnetworksusing arXivpreprintarXiv:2108.07258,2021. 15
pipelineparallelism,”inNeurIPS,2019. 8
[81] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 18
GuangrunWangiscurrentlyaPostdoctoralRe- XiaodanLiangiscurrentlyanAssociateProfes-
searcherintheDepartmentofEngineeringSci- soratSunYat-senUniversity.Shewasapostdoc
enceattheUniversityofOxford.Hereceivedtwo researcherinthemachinelearningdepartment
B.E.degreesandonePh.D.degreefromSYSU atCarnegieMellonUniversity,workingwithProf.
in2014and2020.Hewasavisitingscholarat Eric Xing, from 2016 to 2018. She received
the Chinese University of Hong Kong (CUHK). herPhDdegreefromSunYat-senUniversityin
His research interest is machine learning. He 2016,advisedbyLiangLin.Shehaspublished
is a Distinguished Senior Program Committee several cutting-edge projects on human-related
memberforIJCAI,anoutstandingreviewer(six analysis, including human parsing, pedestrian
times) for ICLR, NeurIPS, and ICCV. He is the detectionandinstancesegmentation,2D/3Dhu-
recipient of the 2018 Pattern Recognition Best man pose estimation, activity recognition, dia-
PaperAward,twoESIHighlyCitedPapers,TopChineseRisingStarsin loguesystem,andautomatedmachinelearning.Moredetailscouldbe
ArtificialIntelligence,andWuWen-JunBestDoctoralDissertation. foundinherhomepage:https://lemondan.github.io.
Changlin Li is a Postdoctoral Researcher in
ReLER Lab, Australian Artificial Intelligence In-
stitute,UniversityofTechnologySydney(UTS).
HereceivedhisPh.D.degreefromUTSin2023.
Prior to his Ph.D. study, he received his B.E.
degreeinComputerSciencein2019,fromUni-
versityofScienceandTechnologyofChina.He XiaojunChangisaProfessoratAustralianAr-
currently serves as a reviewer of CVPR, ICCV, tificial Intelligence Institute, University of Tech-
ECCV, ICML, NeurIPS, T-PAMI, T-IP, etc. His nologySydney.HeisalsoanHonoraryProfes-
main research ambition is to explore more effi- sor at the School of Computing Technologies,
cientandmoreintelligentneuralnetworkarchi- RMIT University. Before joining UTS, he was
tectures.Heisalsointerestedincomputervisiontaskssuchasactivity an Associate Professor at the School of Com-
recognitionandhaswonfirstplaceintheTRECVIDActEV2019grand putingTechnologies,RMITUniversity,Australia.
challenge. After graduation, he subsequently worked as a
PostdocResearchFellowatSchoolofComputer
Science, Carnegie Mellon University, Lecturer
andSeniorLecturerintheFacultyofInformation
Technology, Monash University, Australia. He has spent most of his
timeworkingonexploringmultiplesignals(visual,acoustic,textual)for
Liuchun Yuan received the B.S. degree from automaticcontentanalysisinunconstrainedorsurveillancevideos.He
the Nanjing University of Information and Sci- has achieved top performances in various international competitions,
ence Technology, China, in 2017. She is cur- suchasTRECVIDMED,TRECVIDSIN,andTRECVIDAVS.
rentlypursuingtheM.S.degreewiththeSchool
ofElectronicsandInformationTechnology,Sun
Yat-sen University, Guangzhou, China. Her re-
searchinterestsmainlyincludecomputervision
andmachinelearning.
Liang Lin (M’09, SM’15) is a Full Professor
of computer science at Sun Yat-sen University.
JieFeng Peng received his B.S. and M.E.
He served as the Executive Director and Dis-
degree from the Sun Yat-sen University,
tinguished Scientist of SenseTime Group from
Guangzhou, China, in 2016 and 2019,
2016to2018,leadingtheR&Dteamsforcutting-
respectively.Hismainresearchinterestsinclude
edge technology transferring. He has authored
deeplearningandautomatedmachinelearning.
orco-authoredmorethan200papersinleading
academic journals and conferences (e.g., 20+
papers in TPAMI/IJCV), and his papers have
beencitedbymorethan16,000times.Heisan
associateeditorofIEEETrans.NeuralNetworks
andLearningSystemsandIEEETrans.Human-MachineSystems,and
servedasAreaChairsfornumerousconferencessuchasCVPR,ICCV,
SIGKDDandAAAI.Heistherecipientofnumerousawardsandhonors
including Wu Wen-Jun Artificial Intelligence Award, the First Prize of
ChinaSocietyofImageandGraphics,ICCVBestPaperNominationin
2019, Annual Best Paper Award by Pattern Recognition (Elsevier) in
Xiaoyu Xian received the M.S. degrees from
2018,BestPaperDimondAwardinIEEEICME2017,GoogleFaculty
BeijingJiaotongUniversity,Beijing,China.Cur-
Award in 2012. His supervised PhD students received ACM China
rently,heisaresearcherwiththetechnicalde-
DoctoralDissertationAward,CCFBestDoctoralDissertationandCAAI
partment,CRRCAcademyCo.,Ltd.,Beijing.His
BestDoctoralDissertation.HeisaFellowofIAPRandIET.
current research interests include optical char-
acterrecognitionandmachinelearning.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 1
APPENDIX where the last inequality is due to the Cauchy-Schwarz
inequality.CombiningEqn.(13)and(14)gives:
In the main body of this paper, we have presented the
m ana din fai id re ea xpin erd imet ea nil tsan tod dh ea mve onu ss te rad tep tr ho esp ee fr fo eu cts i, vee nx ete sn ss aiv ne d, L αt(ψ 0•+ψ t•)+λ˜(cid:13) (cid:13)ψ 0•+ψ j•(cid:13) (cid:13)2
F
efficiency of our DNA family. In this appendix, we present ≤L αt(ψ 0•+ψ t•)+λ 0(cid:13) (cid:13)ψ 0•(cid:13) (cid:13)2
F
+λ(cid:13) (cid:13)ψ j•(cid:13) (cid:13)2
F
moredetailsandexperimentalresultstohelpreadersbetter T T
understand this paper. The contents of the appendix in- ≤(cid:88) L αt(ψ 0∗)+λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2
F
−(cid:88) L αt(ψ 0•+ψ t•)
clude proof of Theorem 1, algorithm boxes, details of our t=1 t=1
t̸=i
ImageNet NAS Bench, comparison of search spaces, and
T T
visualizationofthesearchedarchitectures. ≤(cid:88) L αt(ψ 0∗)+λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2
F
−(cid:88) L αt(ψ t∗)
t=1 t=1
t̸=i
ProofofTheorem1 T
WefirstrewriteTheorem1intoTheorem2. =L αj(ψ j∗)+λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2 F +(cid:88)(cid:12) (cid:12)L αt(ψ 0∗)−L αt(ψ t∗)(cid:12) (cid:12)
Theorem 2. (Generalization boundedness). Let
(cid:8) ψ•(cid:9)T
be
t=1
t t=0 T
t bh ee tho ept oim pta iml n alet nw eo twrk orw ke wig eh it gs hto sf oth fe thw ee uig nh ivt- es rh sa ar li sn og luN tiA onS ,, aψ n0 d∗ ≤L αt(ψ 0•+ψ t•)+λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2
F
+(cid:88)(cid:12) (cid:12)L αt(ψ 0∗)−L αt(ψ t∗)(cid:12) (cid:12),
t=1
(cid:8) ψ∗(cid:9)T be the optimal network weight of the stand-alone (15)
t t=1
s foo rlu at nio yn caw ni dth idaR te(ψ art c) hi= tectλ˜ u(cid:13) (cid:13) rψ et j(cid:13) (cid:13) ,2 F thew Fh re or be eλ n˜ iu= snλ oλ 0 r0 m+λ λ o. fT ωh •en is, w soh lue tr ie onthe hala sst hi in ge hq eu ra ll oit sy sei ss b thec aa nus te heth se taw ne di -g ah lot- ns eha ori nn eg .
j CombiningthefirstandthelastlineofEqn.(15)gives:
upperboundedby:
(cid:118)
(cid:13) (cid:13)ω j•(cid:13) (cid:13) F
(cid:13) (cid:13)ψ•+ψ•(cid:13) (cid:13)
≤(cid:117) (cid:117) (cid:117) (cid:116)λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2
F +
t(cid:80) =T 1(cid:12) (cid:12)L αt(ψ 0∗)−L αt(ψ t∗)(cid:12) (cid:12)
,
=(cid:13) (cid:13)ψ 0•+ψ j•(cid:13) (cid:13) F 0 j F λ˜ λ˜ (16)
(12) foranyj.
(cid:118)
≤(cid:117) (cid:117) (cid:117) (cid:116)λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2
F +
t(cid:80) =T 1(cid:12) (cid:12)L αt(ψ 0∗)−L αt(ψ t∗)(cid:12) (cid:12)
,
is bD asi es dcu os nsio thn e: O asn se umco pu tl id onra ti hse atth “we p eio gi hn tt -sth ha at rit nh gis st oh lueo tir oe nm s
λ˜ λ˜ yield higher losses than the stand-alone ones,” which may
not always hold true as there are instances where models
where (cid:12) (cid:12)L αt(ψ 0∗)−L αt(ψ t∗)(cid:12) (cid:12) is the overall gap between the can derive benefits from weight-sharing training [78]. We
universalsolutionandthestand-alonesolution. wouldliketoprovidefurtherdiscussiontoclarifythisissue.
Proof 1. Since
(cid:8) ψ•(cid:9)T
are the optimal network weights
First and foremost, we would like to emphasize that
t t=0 even in the OnceForAll mode, Theorem 1 still holds true.
of the weight-sharing solution and the weight-sharing
We know that in OnceForAll, two loss functions are used
solution has lower losses than the universal one, we
simultaneously for training the subnets: the classic cross-
have:
entropy loss and the KD loss that uses the predictions
(cid:88)T
L αt(ψ 0•+ψ t•)+λ 0(cid:13) (cid:13)ψ 0•(cid:13) (cid:13)2
F
+λ(cid:13) (cid:13)ψ t•(cid:13) (cid:13)2
F
o caf llt eh de inla pr lg ae cs et Kn De .tw Ino Ork ncf eo Fr ord Ais lt li ,l tla ht eio sn u. b-T mh ois det le sch mn ii gq hu te nois
t
t=1 (13) benefit from weight-sharing training, but they do benefit
T
≤(cid:88) L αt(ψ 0∗)+λ 0(cid:13) (cid:13)ψ 0∗(cid:13) (cid:13)2 F. f sr to anm d-in alp ol na ece suK bD n. eT th oe fr Oef no cre e, Fw orh Ae ln l,c iton issi ad de vri in sag br leetr toain ui tn ilg iza
e
t=1
both cross-entropy and KD losses. By doing so, the loss of
On the other hand, as
(cid:8) ψ∗(cid:9)T
is the optimal network
the stand-alone subnet still proves to be lower than that of
t t=1
theweight-sharingmodel,therebyaffirmingthevalidityof
weights of the stand-alone solution and the stand-alone
Theorem1.
solution has lower losses than the weight-sharing one,
foranycandidatearchitectureα j,wehave: Secondly, OnceForAll requires a relatively high similar-
ity between the results of the student and the teacher, oth-
L αj(ψ
j∗)+λ˜(cid:13)
(cid:13)ψ
j∗(cid:13) (cid:13)2
F
erwise,differentsubnetswouldhaveoptimizationconflicts
≤L αt(ψ 0•+ψ t•)+λ˜(cid:13) (cid:13)ψ 0•+ψ j•(cid:13) (cid:13)2
F
d isu me oto ree vx ec re ss as ti iv lee as ntr duc flt eu xr ia bl ledi cv oe mrs pit ay r. edTh te ore Ofo nr ce e, Foo ru Ar lD l.N FoA
r
=L αt(ψ 0•+ψ t•)+
λ
√0+1
λ
t(cid:13) (cid:13)(cid:112) √λ 0√ λψ 0•+(cid:112) λ 0√ λψ j•(cid:13) (cid:13)2
F
e isxa Cm Np Nle a, no dur thD eN sA tuc da en ntbe isa Vp ip Tl .iedtocaseswheretheteacher
≤L αt(ψ 0•+ψ t•)+ ( λ 0 λ)2 ++ λ( λ)2 (cid:13) (cid:13)(cid:112) λ 0ψ 0•(cid:13) (cid:13)2
F
tweT eh nir td hl ey, ai ct ci us ri am cypo or fta tn ht eto sun bo nt ee tsth aa nt dth te hec io rrr te rl aa it nio -fn rob me-
-
√ 0 t
+(cid:13)
(cid:13)
λψ•(cid:13) (cid:13)2 scratchaccuraciesremainsunknown,renderingthemsome-
j F what opaque and leaving the upper bound of the subnets’
=L αt(ψ 0•+ψ t•)+λ 0(cid:13) (cid:13)ψ 0•(cid:13) (cid:13)2
F
+λ(cid:13) (cid:13)ψ j•(cid:13) (cid:13)2
F
capabilitiesuncertain.Incontrast,ourDNAmethodspecifi-
(14) callytargetsandresolvestheissueofinaccuratearchitectureIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 2
TABLE15:Detailedcomparisonofsearchspacedifferences.
Methods Granularity Buildingblock Kernelsize Expandrate SEratio Channels Depth
MnasNet[2] block-level MBBlock 3,5 3,6 0,0.25 variable variable
MobileNetV3[72] block-level MBBlock 3,5 3,6 0.25 variable variable
EfficientNet[3] block-level MBBlock 3,5 3,6 0.25 variable variable
SPOS[20] layer-level ShuffleBlock 3,5,7 - 0 variable variable
ProxylessNAS[15] layer-level MBBlock 3,5,7 3,6 0 fixed variable
FBNet-C[19] layer-level MBBlock 3,5 1,3,6 0 fixed fixed
FairNAS-A[21] layer-level MBBlock 3,5,7 3,6 0 fixed fixed
MoGA-A[64] layer-level MBBlock 3,5,7 3,6 0,0.25 fixed fixed
SCARLET-A[49] layer-level MBBlock 3,5,7 3,6 0,0.25 fixed variable
PC-NAS-S[23] layer-level MBBlock 3,5,7 1,3,6 0.25 fixed variable
MixNet-M[73] layer-level MixConvBlock mixed3,5,7,9,11 3,6 0.25 fixed variable
DNA(ours) layer-level MBBlock 3,5,7 3,6 0.25 variable variable
ranking in weight-sharing NAS, thereby enhancing its effi- Algorithm2:Traversalsearch
ciencyandeffectiveness.
Input:BlockindexB,theteacher’scurrentfeaturemapG,
With approximately 6.4 million parameters, our DNA constrainC,modelpoollistPool
method achieves an impressive top-1 accuracy of 79.7%, Output:bestmodelM
surpassing that of OnceForAll (76.4%, see Table 2). These
defineSearchBlock(B,sizeprev,lossprev):
fori<length(Pool[B])do
compelling results unequivocally demonstrate the consis- size←sizeprev+size[i];
tentsuperiorityofourmethod. ifsize>Cthen
continue;
end
Algorithmboxes loss←lossprev+loss[i];
ifBislastblockthen
In Section 3.3, we mentioned that we designed two ex- ifloss≤loss bestthen
cellent algorithms in the rating step and the search step, loss best←loss;
respectively, to make our NAS very efficient. These two M ←indexofeachblock
end
algorithms are the ”feature sharing rating” and ”Traver- break;
sal search” algorithms, respectively. Here, we present two else
SearchBlock(B+1,size,loss);
detailed algorithm boxes in Algorithm 1 and Algorithm 2.
end
Hopefully, it can help readers to understand our algorithm
end
well.Thereaderscanalsodirectlyviewourcodetounder-
SearchBlock(0);
stand our algorithm (see the code link in the abstract, or outputM;
https://github.com/changlin31/DNA).
the computational complexity (i.e., FLOPs). These statistics
Algorithm1:Featuresharingrating truly reflect the real distribution of accuracy and computa-
tionalcomplexityofdifferentarchitecturesintherealworld,
Input:Teacher’spreviousfeaturemapGprev,Teacher’scurrent
featuremapGcurr,RootofthecellCell,lossfunction sohopefully,theywillbeusefultofutureresearchers.
loss
Output:ListofevaluationlossL
Detailedcomparisonofsearchspacedifferences
defineDFS-Forward(N,X):
Y =N(X); As expected, the performance of searched models in NAS
ifN hasnochildthen methods highly depends on the quality and variability
append(L,loss(Y,Gcurr));
of the search space. To make a fair comparison, we only
else
forCinN.childdo compare our DNA family with NAS methods using the
DFS-Forward(C,Y); MBConv search space in Table 2. This search space ignores
end NASmethodsusingcell-basedsearchspace,anothercritical
end
NAS branch, in which ENAS [44] and DARTS [16] are
DFS-Forward(Cell,Gprev);
typicalexamples.Notethatwedonotignorethesemethods
outputL;
in Section 4.5 where the effectiveness of our method is
sufficientlyjustified.
As mention in Section 4.2, our MBConv search space
DetailsofourImageNetNASBench is similar to most of the recent NAS works ( [2], [3], [49],
[64]) to ensure a fair comparison. Nevertheless, there still
In Section 4.1, we mentioned that we constructed a Im-
aresomeminordifferencesbetweentheseMobileNetblock-
ageNet NAS Bench to ensure fair and consistent evalua-
based search spaces. The search spaces of all the NAS
tion and to compensate for the lack of full-resolution NAS
methods we compared in Table 2 are shown in Table 15
benchesonImageNet.Here,weshowsomestatisticsabout
in detail. As shown, our MBConv search space aligns with
this benchmark so that the readers can better understand
existingNASworks.
thisbenchmarkinFig.12.
Fig. 12(a) presents the histogram of the top-1 accuracy
Architecturevisualization
of the architectures in our ImageNet NAS Bench. Fig. 12
(b) summarize the top-1 accuracies w.r.t. the parameter Our searched architectures are visualized in Fig 13 in the
numbers. Fig. 12 (c) summarize the top-1 accuracies w.r.t. appendix, from which we have several observations. i)IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 3
7.5 Frequency
5.0
2.5
0
73.97 74.36 74.74 75.13 Top-1 Acc.
(a)
75.5 Top-1 Acc.
75.1
74.7
74.3
73.9
73. 45 .37 4.38 4.596 4.602 4.61 4.68 4.74 4.79 4.81 4.90 4.96 4.97 5.029 5.034 5.05 5.07 5.083 5.084 5.19 5.39 5.44 5.45 5.66
(b) #Param
75.5 Top-1 Acc.
75.1
74.7
74.3
73.9
73.5
3.35 3.40 3.50 3.53 3.63 4.68 3.64 3.68 3.84 3.90 3.92 3.97 3.99 4.04 4.11 4.13 4.18 4.30 4.32 4.36 4.44 4.70 5.03
(c) #FLOPs
Fig.12:StatisticsofourImageNetNASBench.(a)summarizes
the histogram of the top-1 accuracies. (b) and (c) outline the
top-1accuraciesw.r.t.theparameternumbersandFLOPs.
Searched under no constraint, DNA-d tends to choose rel-
atively expensive operations with high expansion rate and
largekernelsize,thusachievingthebestperformance.This
verifies that our DNA is able to find optimal architectures
in the search space. ii) Under the constraint of maximum
parameter number, DNA-c tends to discard the operations
with redundant channels to save the parameters. It also
tendstoselectalowerexpansionrateinthelastblockssince
the last blocks have more channels than the first blocks.
iii) Under the constraint of maximum computational cost,
DNA-b and DNA-a tend to select operations with fewer
channelsandlowerexpansionrateevenlyineachblock.The
significant style difference of the high-performing architec-
turesinFig13provesDNA’sarchitecturesearchability.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 4
3×224×224 3×224×224 3×224×224 3×224×224
Stem Stem Stem Stem
16×112×112 16×112×112 16×112×112 16×112×112
MB 3 5×5 SE MB 3 3×3 SE MB 6 7×7 SE
24×56×56 24×56×56 MB 6 5×5 SE 24×56×56
MB 3 3×3 SE MB 3 5×5 SE 24×56×56 MB 6 5×5 SE Block 1
24×56×56 24×56×56 MB 6 3×3 SE 24×56×56
MB 6 3×3 SE MB 6 7×7 SE 24×56×56 MB 6 7×7 SE
24×56×56 24×56×56 24×56×56
MB 3 3×3 SE MB 6 5×5 SE MB 6 7×7 SE
40×28×28 MB 6 7×7 SE 40×28×28 40×28×28
MB 3 3×3 SE 40×28×28 MB 6 5×5 SE MB 6 7×7 SE
40×28×28 MB 3 5×5 SE 40×28×28 40×28×28 Block 2
MB 3 5×5 SE 40×28×28 MB 6 3×3 SE MB 6 7×7 SE
40×28×28 MB 6 7×7 SE 40×28×28 40×28×28
MB 6 3×3 SE 40×28×28 MB 6 5×5 SE MB 6 7×7 SE
40×28×28 40×28×28 40×28×28
MB 6 7×7 SE MB 6 5×5 SE MB 6 5×5 SE MB 6 7×7 SE
80×14×14 80×14×14 80×14×14 80×14×14
MB 3 3×3 SE MB 3 5×5 SE MB 6 3×3 SE MB 6 5×5 SE
80×14×14 80×14×14 80×14×14 80×14×14 Block 3
MB 3 5×5 SE MB 3 5×5 SE MB 6 5×5 SE MB 6 5×5 SE
80×14×14 80×14×14 80×14×14 80×14×14
MB 6 3×3 SE MB 6 3×3 SE MB 6 5×5 SE MB 6 7×7 SE
80×14×14 80×14×14 80×14×14 80×14×14
MB 6 5×5 SE MB 6 5×5 SE MB 6 3×3 SE MB 6 7×7 SE
96×14×14 96×14×14 112×14×14 112×14×14
MB 3 5×5 SE MB 3 5×5 SE MB 6 5×5 SE MB 6 5×5 SE
96×14×14 96×14×14 112×14×14 112×14×14 Block 4
MB 3 5×5 SE MB 3 5×5 SE MB 6 5×5 SE MB 6 7×7 SE
96×14×14 96×14×14 112×14×14 112×14×14
MB 6 5×5 SE MB 6 5×5 SE MB 3 5×5 SE MB 6 5×5 SE
96×14×14 96×14×14 112×14×14 112×14×14
MB 6 5×5 SE MB 6 7×7 SE MB 6 7×7 SE
160×7×7 192×7×7 MB 6 5×5 SE 192×7×7
MB 3 5×5 SE MB 3 5×5 SE 192×7×7 MB 6 5×5 SE
160×7×7 192×7×7 MB 3 5×5 SE 192×7×7
MB 3 5×5 SE MB 3 5×5 SE 192×7×7 MB 6 5×5 SE Block 5
160×7×7 192×7×7 MB 6 5×5 SE 192×7×7
MB 3 5×5 SE MB 3 5×5 SE 192×7×7 MB 6 5×5 SE
160×7×7 192×7×7
MB 6 5×5 SE
192×7×7
MB 6 5×5 SE MB 6 7×7 SE 192×7×7 MB 6 7×7 SE
160×7×7 192×7×7 192×7×7
MB 6 3×3 SE MB 6 3×3 SE MB 6 3×3 SE MB 6 3×3 SE
Block 6
320×7×7 320×7×7 320×7×7 320×7×7
Conv 1×1 Conv 1×1 Conv 1×1 Conv 1×1
1280×7×7 1280×7×7 1280×7×7 1280×7×7
Pooling + FC Pooling + FC Pooling + FC Pooling + FC
(a) DNA-a (b) DNA-b (c) DNA-c (d) DNA-d
Fig.13:ArchitecturesofDNA-a,b,c,d.‘MBxy×y’representsaninverted-bottleneck-convolutionwithexpandratexandkernel
sizey.