Improve Cost Efficiency of Active Learning over
Noisy Dataset
Zan-Kai Chong Hiroyuki Ohsaki Bryan Ng
Independent Researcher School of Science and Technology School of Engineering &
Malaysia Kwansei Gakuin University Computer Science
zankai@ieee.org Japan Victoria University of Wellington
ohsaki@kwansei.ac.jp New Zealand
bryan.ng@ecs.vuw.ac.nz
Abstract—Active learning is a learning strategy whereby the the only strategy in use. Other approaches, such as query-
machine learning algorithm actively identifies and labels data by-committee [13], expected model change [14], expected
points to optimize its learning. This strategy is particularly
error reduction [15], and expected variance reduction [16],
effective in domains where an abundance of unlabeled data
offeralternativewaystoimplementactivelearningeffectively.
exists, but the cost of labeling these data points is prohibitively
expensive.Inthispaper,weconsidercasesofbinaryclassification, These various methodologies further extend the adaptability
where acquiring a positive instance incurs a significantly higher and applicability of active learning, ensuring it remains a
cost compared to that of negative instances. For example, in vibrant field of study. For a more in-depth understanding of
the financial industry, such as in money-lending businesses, a
activelearning,interestedreaderscanrefertothesurveypaper
defaulted loan constitutes a positive event leading to substantial
such as [3], [17], and the more recent [18].
financialloss.Toaddressthisissue,weproposeashiftednormal
distribution sampling function that samples from a wider range Despite the advantages of sampling efficiency, active learn-
than typical uncertainty sampling. Our simulation underscores ing presents several challenges. First, there’s the cold start
that our proposed sampling function limits both noisy and posi- issue, wherein the effectiveness of uncertainty sampling re-
tive label selection, delivering between 20% and 32% improved
lies heavily on the initial model’s accuracy in capturing the
cost efficiency over different test datasets.
overview of existing data distribution. A poorly fitted initial
model reduces the efficacy of both subsequent queries and
overall model performance. Second, real-world datasets often
I. INTRODUCTION
contain noise or may have features that are not adequately
Active learning is a specialized form of machine learning representative, making the regions of uncertainty potentially
that aims to optimize the learning process by intelligently less valuable for querying. Third, labeling costs can vary
selecting the data points that are most beneficial for model between positive and negative instances; for instance, in the
training [1], [2], [3]. Unlike traditional supervised learning, financialservicesindustry,identifyingaloan/mortgagedefault
which uses all available labeled examples, active learning is substantially more costly than pinpointing a reliable pay-
focuses using data points where the model is most uncertain master. For a more comprehensive discussion on the practical
and hence stands to gain the most information. This approach challenges of active learning, refer to [18].
isespeciallyvaluableinscenarioswherelabeleddataisscarce Recent active learning research has made significant ad-
or expensive to obtain, such as in medical imaging, natural vances,particularlyinthefieldofdeeplearning.Forexample,
language processing, or robotics. [19] addresses the cold start issue in deep learning models
Buildingonthisfoundationalconcept,uncertaintysampling by establishing different querying strategies for high and
emerged as one of the earliest and most influential techniques low budget regimes. [20] operates under the assumption of
in active learning. Initially popularized by Lewis [4], this an abundance of pre-labeled instances, which may contain
approach showed promise in enhancing the performance of noise. They employ a high-quality model to identify and
text classification models. The method was later expanded remove dubious or noisy labels, turning to an oracle for final
upon by Schohn and Cohn [5], who applied it to Support verification and adjustment.
Vector Machine (SVM) classification, further solidifying its In this paper, we assume that the dataset contains both
role as a potent tool for model improvement. Over the years, labeled and unlabeled data, with no free weakly-labeled data,
uncertainty sampling has been extensively researched and unlike what is commonly found in deep learning-related stud-
applied in various domains, featuring prominently in studies ies. We assume that overlapping features contribute to dataset
by [6], [7], [8], [9], [10]. While much of the work in this area labeling noise and present ambiguity for the classification
hasbeenempiricalorheuristic,therehasalsobeensignificant model. To address these issues, we introduce a broader sam-
theoretical exploration, with contributions from researchers plingspectrumtopreventthemodelfromover-samplingnoisy
like [11], [12]. regions. Furthermore, we propose a metric for cost efficiency
While uncertainty sampling has earned its reputation as a thatconsidersthedistinctlabelingcostsforpositiveinstances.
cornerstoneintheactivelearninglandscape,itisbynomeans Theremainderofthispaperisorganizedasfollows:Section
4202
raM
2
]GL.sc[
1v64310.3042:viXra2
2 introduces the foundational algorithms of random sam- themodelfindshardtoclassify,itaidsinrefiningthedecision
pling and uncertainty sampling, which serve as benchmarks boundary and enhancing overall model performance, leading
for our proposed method. In Section 3, we elucidate our to more efficient use of computational resources compared to
proposed sampling function through mathematical modeling random sampling.
and algorithmic description. Section 4 presents simulation Despite these advantages, uncertainty sampling has its lim-
results, comparing random sampling, uncertainty sampling, itations. Primarily, it runs the risk of gravitating towards
andourproposeddsamplingfunction.Finally,Section5offers complexornoisyinstances,asthesearethesampleswherethe
conclusions drawn from our work. model often experiences the most uncertainty. This bias can
leadtomodeloverfitting,asthelearningmayfocusexcessively
II. RANDOMSAMPLINGANDUNCERTAINTYSAMPLING on outliers or irregular instances.
This section introduces random sampling and uncertainty C. General Sampling Algorithm
sampling,whichserveasbenchmarksforthesimulation.Then,
Theabovementionedrandomsamplinganduncertaintysam-
the general sampling algorithm is presented.
pling can be elaborated with the following general algorithm.
• Step 1 - Initialization: Start with an initial classification
A. Random Sampling model that has been trained on a labeled dataset (known
Random sampling (or random samples) is a fundamental data pool, L. Additionally, prepare an unlabeled dataset
conceptinthefieldofmachinelearningandstatisticalanalysis, (unknown data pool, U). Define stopping criteria such as
whereitservesasaprimarydataselectionmethod.Itoperates a predetermined number of queries, denoted by N. Each
underthepremiseofprovidingeveryinstanceanequalchance query is indexed by and integer q, where q ≤N
of being selected, thereby mitigating the risk of bias and • Step 2 - Instance Selection: In this step, select instances
promoting diversity within the chosen sample [21]. from U using either random sampling or uncertainty
The strength of random sampling lies in its simplicity sampling,basedonthestrategychosenforthesimulation.
and representativeness. As it doesn’t rely on any additional For random sampling, randomly select N instances from
information or complicated selection mechanisms, it is easily U without consideration of the model’s output. For un-
implemented,eveninscenarioswherepreliminarydataunder- certainty sampling, use the existing model to predict the
standing is limited. Due to the randomness in selection, the classprobabilitiesforeachinstanceinU.Selectinstances
sampleoftencapturestheoveralldistributionofthepopulation based on measures of uncertainty, for example least
well,contributingtothegeneralizabilityofthelearnedmodel. confidence, margin sampling, or entropy. After querying
However, random sampling is not without its weaknesses. their true labels, move the N selected instances from U
In situations where the distribution of labels is imbalanced, to L.
or certain classes of data are less prevalent, random sam- • Step 3 - Model Refinement: Using the updated labeled
pling overlooks these under represented labels. Consequently, datasetL,retraintheclassificationmodel.Ifthestopping
the learned model suffers from performance issues when criteria are not met, return to Step 2 and continue the
predicting less common classes [22]. Moreover, as random process.
sampling does not consider the informativeness or relevance
III. SAMPLINGOVERUNCERTAINTYANDNOISEREGIONS
of instances, it might lead to the selection of redundant
The assumptions and the mathematical model of the pro-
or non-informative instances, causing a potential waste of
posed sampling function are stated and discussed in the
computational resources.
following sections.
Despite these drawbacks, random sampling remains a
common and crucial sampling strategy in machine learning
A. Assumptions and Setup
projects due to its inherent simplicity and the fair represen-
tation it provides. Various studies have employed random As mentioned above, we use two distinct pools of data:
sampling as a baseline or comparative method in research one labeled and another unlabeled. The initial size of the
on active learning, reinforcement learning, and other machine labeled data pool is modest, while the unlabeled data pool is
learning disciplines. significantlylarger.Wealsoassumethatthedistributionofthe
overall population remains constant throughout the querying
process. Our study focuses on a binary classification problem
B. Uncertainty Sampling
with labels categorized as 0 and 1 or True or False.
Uncertainty sampling is a data selection strategy in active Itisimportanttonotethatthesimulationsinvolveassessing
learning, where instances about which the model is most the actual model performance, using the mean values of the
uncertainarechosenforlabelling.Essentially,itseekstolabel Area Under the Curve (AUC) and the F1 score, derived from
instancesforwhichthecurrentpredictivemodelhasthelowest three distinct test data sets. These performance outcomes are
confidence, thereby focusing on the most informative samples for referencing purposes only and are available solely to the
to improve the model’s learning efficiency [4]. observers during the simulation. They do not in any way
One of the principal advantages of uncertainty sampling influence the results of the different algorithms tested.
lies in its potential to maximize the learning outcomes from a Finally, we assume that the active learning process con-
limitedpooloflabeledinstances.Byfocusingonsamplesthat cludes after a predetermined number of queries.3
wider normal distribution, denoted as N(0.5,σ). This allows
us to shift the selection of instances closer to pq =0.5, while
controlling the sampling spread via the standard deviation σ.
The details of the algorithm will be elaborated in Section
III-D.
C. Imbalance Labelling Cost
WeassumethatthemodelperformancemetricsuchasAUC,
F1, etc, is measured and denoted as λ(q), where higher λ(q)
represents better performance and 0 ≤ λ(q) ≤ 1;λ(q) ∈ R,
for the q-th query. . To tune the performance measurement
of the sampling algorithm in terms of its responsiveness
Figure 1: Optimal probability distributions for positive and to varying proportions of positive instances, we introduce a
negative events, exhibiting minimal overlap area, and shifted metric designated as cost efficiency, i.e.,
normal samping function.
λ(q)
η(q)= , (2)
ζ(q)C
B. Shifted Normal Sampling: Inspired by the Uncertainty where η ∈ R , η ≥ 0 and ζ(q) : ζ(q) ≥ 0 represent the
Region and Noisy Region proportion of positive instances within the known dataset.
Suppose we have the final model of an active learning Here,C isthecostassociatedwithasinglepositiveinstance
process and its associated prediction probability, denoted as relative to a negative instance and C ≥ 1. When the costs
pfinal, for a binary classification dataset. Here, i is the index for both positive and negative instances are identical, C is 1.
i
of the unlabeled instances and it may be omitted for brevity. Conversely, if the cost of a positive instance is three times the
Byusingpfinal,wemapallunlabelledinstancestotherange costofanegativeinstance,C is3.Fortheensuingsimulation,
[0,1] ∈ R, where Ris the set of real numbers. Within this weadoptC =1asourstandardvalue.Overall,ahighervalue
range, instances with high predicted probablility are unam- of η corresponds to increased efficiency.
biguouslyassociatedwiththeirrespectivelabelsof0and1.In As mentioned earlier, we assume that the labeling costs for
contrast, those around pfinal =0.5 oscillate between the labels positive events are significantly higher than those for negative
as the query progresses. This ambiguity is attributed to their events in this case. Consequently, we adjust the normal sam-
locationinanoverlapregion,whichwetermasnoisehere,as plingdistributionslightlytotheleft,i.e.,N(0.45,σ),suchthat
illustrated in Fig. 1. Let the integer q denote the q-th query to the sampling is slightly biased towards negative events (Fig.
theset U (seeSectionIII-D). Thentheregionwhere pq ≈0.5 1). This adjustment will yield relatively fewer positive labels
serves as the uncertainty region for interim models to query. in ζ(q), thereby improving η(q).
Incontrast,theareaaroundpfinal =0.5isconsideredthenoise ThismetricwillbeusedinsimulationresultsinSectionIV.
region, as it is uncertainly classified the final model.
Now, assume that we are using uncertainty sampling in D. Normal Sampling Algorithm
querying instances of predicted probabilities pq close to the
We illustrate the algorithm of the proposed sampling func-
uncertainty region, i.e., in the range of (0.5 − δ,0.5 + δ),
tion as follows.
for a small value δ at q-th query, where δ ∈ R+ and δ
1) Step 1 - Initialization: Commence with a pre-existing
closes to 0 . Here, the pq=1 = 0.5 from the initial model
classification model, accompanied by a labelled dataset (the
might significantly differ from pfinal = 0.5. Yet, as queries
known data pool, L), and an unlabelled dataset (the unknown
continue, we anticipate a gradual refinement in the interim
data pool, U). Define a set of stopping criteria, such as a
models, drawing instances closer and closer to p = 0.5.
final predefined number of queries, denoted as N.
Let φ(q) be the actual sampling distribution of the interim
2) Step 2 - Instance Selection: This step is analogous to
model with reference to the final model, i.e.,
uncertainty sampling, wherein instances are selected based
φ(q)=(cid:8) pfinal :∀i,pq ∈[0.5−δ,0.5+δ](cid:9) . (1) on predicted probabilities from all instances in U. However,
i i diverging from uncertainty sampling, instances are selected
We expect φ(q) to follow a normal distribution, represented with predicted probabilities in accordance with a normal
as φ∼N(0.5,σ), where σ shrinks as q increases. distribution. In our simulation, we will use a beta function,
The essence of uncertainty sampling is to query within the i.e., Beta(α,β) to approximate the normal distribution, where
uncertainty region, which overlaps with the noise region as α and β are adjusted such that peak values occur at p =
queries progress. While initially minimal, this overlap can 0.45. Correspondingly, most instances are sampled near to
increase as interim models are refined, further pushing φ(q) p = 0.45, without disregarding instances from other ranges.
towards the noise region. We argue that narrow sampling Then, queried instances will be incorporated into L.
spectrum over the noise region is the cause of performance 3) Step 3 - Model Refinement: A new model is built using
fluctuationsintheinterimmodels.Toaddressthisissue,rather L. If the stopping criteria have not been met, the process will
than sampling within the range [0.5−δ,0.5+δ], we opt for a be repeated from Step 2.4
IV. SIMULATIONRESULTS theformerhaslowerpositiveeventratiocomparatively,results
in better cost efficiency, i.e., achieving η(normal) = 1.88
This section investigates the performance of random sam-
for shifted normal sampling — about 20% improvement as
pling, uncertainty sampling and shifted normal sampling
compared to uncertainty sampling η(uncertainty) = 1.60 and
throughsimulation.Thediscussioncommenceswithadescrip-
random sampling η(random) =1.54 at final query.
tion of the simulation setup, followed by an overview of the
Simulations on classification datasets with class_sep=1.0
artificial datasets that we used. Finally, the simulation results
showminoroverlap,asinFig.3.Generally,allthreesampling
are delineated.
functions exhibits competitive AUC performance with the
their respective mean positive event ratios all within the 99%
A. Simulation Overview CI. Since shifted normal sampling has the lowest positive
In this paper, we created synthetic classification datasets event ratio, the correponding cost efficiency stays highest,
using Scikit-learn [23]. The datasets comprise four feature i.e. η(normal) = 2.44, about 32% improvement as compared
columns,andeachdatasethasdifferentlevelsofnoise.Dueto to η(random) =1.78 and η(uncertainty) =1.92 at final query.
pagelimits,wepresentresultsfortwosetsofnoiseparameters
inoursimulations.Theresultsshownarerepresentativeofour V. CONCLUSION
extensive simulation over different simulation parameters.
This study aimed to examine the effectiveness of various
Our primary focus was on binary classification, assuming a
sampling algorithms—namely random sampling, uncertainty
large data pool with an equal number of positive and negative
sampling, and the proposed shifted normal sampling, in the
events. For the purpose of our study, we divided the datasets
context of noisy binary classification datasets. We consid-
randomly into three sections. The first, a labeled data pool,
ered cost efficiency, particularly in scenarios where labelling
contained 10 instances. The second, an unknown data pool,
positive instances is more costly than negative instances.
held 1000 instances. Lastly, we had three test data pools,
Our findings reveal that shifted normal sampling strikes a
each containing 1000 instances. These test data were used to
robust balance between AUC performance and improves cost
evaluate the performance of our model after each query. The
efficiency up to 32%.
parameterclass_sepregulatestheseparationbetweenthelabel
classes. A smaller value reduces the separation and makes
REFERENCES
classification harder.
In line with the approach outlined in Section III-D, we [1] P.Ren,Y.Xiao,X.Chang,P.-Y.Huang,Z.Li,B.B.Gupta,X.Chen,and
X.Wang,“Asurveyofdeepactivelearning,”ACMcomputingsurveys
adaptedthesamplingalgorithmduringStep2tomeetoursim-
(CSUR),vol.54,no.9,pp.1–40,2021.
ulationobjective,choosingfromrandomsampling,uncertainty [2] S.Budd,E.C.Robinson,andB.Kainz,“Asurveyonactivelearningand
sampling,orshiftednormalsampling.Eachquerynecessitated human-in-the-loop deep learning for medical image analysis,” Medical
ImageAnalysis,vol.71,p.102062,2021.
the selection of two instances from U to be annotated, reveal-
[3] B.Settles,“Activelearningliteraturesurvey,”2009.
ing their true labels. This process was repeated across a total [4] D. Lewis and W. Gale, “A sequential algorithmfor training text classi-
of 20 queries. Unless otherwise specified, all the presented fiers,”inSIGIR’94:ProceedingsoftheSeventeenthAnnualInternational
ACM-SIGIR Conference on Research and Development in Information
graphs are the results of 30 rounds of simulations and are
Retrieval,organisedbyDublinCityUniversity,1994,pp.3–12.
plotted with 99% confidence intervals (CI). [5] G. Schohn and D. Cohn, “Less is more: Active learning with support
The models were built using the Generalized Linear Model vectormachines,”inProceedingsoftheSeventeenthInternationalCon-
ferenceonMachineLearning,2000,pp.839–846.
(GLM)fromH2O[24],settotheoptimalparametersandwith
[6] Y.Yang,Z.Ma,F.Nie,X.Chang,andA.G.Hauptmann,“Multi-class
no further feature engineering. It is important to note that the active learning by uncertainty sampling with diversity maximization,”
ability of GLM to accurately identify positive events can vary InternationalJournalofComputerVision,vol.113,pp.113–127,2015.
[7] J. Zhu, H. Wang, T. Yao, and B. K. Tsou, “Active learning with
depending on the characteristics of the dataset.
samplingbyuncertaintyanddensityforwordsensedisambiguationand
textclassification,”in22ndInternationalConferenceonComputational
Linguistics,Coling2008,2008,pp.1137–1144.
B. Results [8] E. Lughofer and M. Pratama, “Online active learning in data stream
regression using uncertainty sampling based on evolving generalized
Fig.2showstheperformanceforaclassificationdataset,set
fuzzymodels,”IEEETransactionsonfuzzysystems,vol.26,no.1,pp.
with the parameter class_sep=0.5 for overlap dataset, where 292–309,2017.
its bivariate pairwise distribution is illustrated in Fig. 2(a). [9] Y.YangandM.Loog,“Activelearningusinguncertaintyinformation,”
in201623rdInternationalConferenceonPatternRecognition(ICPR).
Fig. 2(b) charts the AUC, revealing an increasing trend as the
IEEE,2016,pp.2646–2651.
numberofqueriesincreaseswithbetterrefinedinterimmodels. [10] G.Wang,J.-N.Hwang,C.Rose,andF.Wallace,“Uncertaintysampling
Inparticular,wenotethatshiftednormalsamplingandrandom based active learning with diversity constraint by sparse selection,” in
2017IEEE19thInternationalWorkshoponMultimediaSignalProcess-
sampling exhibit similar AUC values, while uncertainty sam-
ing(MMSP). IEEE,2017,pp.1–6.
plingunderperformsonaverage.Then,Fig.2(c)shedslighton [11] S.MussmannandP.S.Liang,“Uncertaintysamplingispreconditioned
theircorrespondingpositiveeventratioat99%CI.Weobserve stochastic gradient descent on zero-one loss,” Advances in Neural
InformationProcessingSystems,vol.31,2018.
both random sampling and uncertainty sampling stay around
[12] A. Raj and F. Bach, “Convergence of uncertainty sampling for active
ζ(random) = 0.53 and ζ(uncertainty) = 0.50 whereas shifted learning,”inInternationalConferenceonMachineLearning. PMLR,
normal sampling converge approximately at ζ(normal) = 0.43 2022,pp.18310–18331.
[13] H. S. Seung, M. Opper, and H. Sompolinsky, “Query by committee,”
at q = 20. It is worth noting that although shifted normal
inProceedingsofthefifthannualworkshoponComputationallearning
sampling has similar AUC performance as random sampling, theory,1992,pp.287–294.5
(a) (a)
(b) (b)
(c) (c)
(d) (d)
Figure 2: Illustration of the classification dataset with param- Figure 3: Representation of the classification dataset for pa-
eter class_sep=0.5 indicating overlap dataset. The sub-figures rameter class_sep=1.0. (a) bivariate distribution, (b) AUC, (c)
represent (a) bivariate distribution, (b) model performance positive event ratio and (d) cost efficiency.
measured in AUC, (c) ratio of positive events, and (d) cost
efficiency across each query.6
[14] B.Settles,M.Craven,andS.Ray,“Multiple-instanceactivelearning,”
Advancesinneuralinformationprocessingsystems,vol.20,2007.
[15] N. Roy and A. McCallum, “Toward optimal active learning through
montecarloestimationoferrorreduction,”ICML,Williamstown,vol.2,
pp.441–448,2001.
[16] R. Wang, C.-Y. Chow, and S. Kwong, “Ambiguity-based multiclass
active learning,” IEEE Transactions on Fuzzy Systems, vol. 24, no. 1,
pp.242–248,2015.
[17] H.Hino,“Activelearning:Problemsettingsandrecentdevelopments,”
arXivpreprintarXiv:2012.04225,2020.
[18] A.TharwatandW.Schenck,“Asurveyonactivelearning:State-of-the-
art,practicalchallengesandresearchdirections,”Mathematics,vol.11,
no.4,p.820,2023.
[19] G.Hacohen,A.Dekel,andD.Weinshall,“Activelearningonabudget:
Oppositestrategiessuithighandlowbudgets,”inInternationalConfer-
enceonMachineLearning. PMLR,2022,pp.8175–8195.
[20] T.Younesian,Z.Zhao,A.Ghiassi,R.Birke,andL.Y.Chen,“Qactor:
Active learning on noisy labels,” in Asian Conference on Machine
Learning. PMLR,2021,pp.548–563.
[21] M.J.KearnsandU.Vazirani,Anintroductiontocomputationallearning
theory. MITpress,1994.
[22] H. Kaur, H. S. Pannu, and A. K. Malhi, “A systematic review on
imbalanced data challenges in machine learning: Applications and
solutions,”ACMComputingSurveys(CSUR),vol.52,no.4,pp.1–36,
2019.
[23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O.Grisel,M.Blondel,P.Prettenhofer,R.Weiss,V.Dubourg,J.Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay,“Scikit-learn:MachinelearninginPython,”JournalofMachine
LearningResearch,vol.12,pp.2825–2830,2011.
[24] E. LeDell and S. Poirier, “H2O AutoML: Scalable automatic machine
learning,” 7th ICML Workshop on Automated Machine Learning (Au-
toML),July2020.