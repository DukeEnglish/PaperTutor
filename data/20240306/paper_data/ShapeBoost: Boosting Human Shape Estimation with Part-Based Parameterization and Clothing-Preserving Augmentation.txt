ShapeBoost: Boosting Human Shape Estimation with Part-Based
Parameterization and Clothing-Preserving Augmentation
SiyuanBian1,JiefengLi1,JiashengTang3,4,CewuLu1,2
1DepartmentofComputerScienceandEngineering,ShanghaiJiaoTongUniversity,Shanghai,China
2MoEKeyLabofArtificialIntelligence,AIInstitute,ShanghaiJiaoTongUniversity,Shanghai,China
3DAMOAcademy,Alibabagroup,Hangzhou,China
4HupanLab,Hangzhou,China
biansiyuan,ljf likit,lucewu @sjtu.edu.cn,jiasheng.tjs@alibaba-inc.com
{ }
(a) Input Image (b) Sengupta et al. (c) SHAPY (d) Ours (e) Sengupta et al. (f) SHAPY (g) Ours
100
50
Figure2:PreviousSOTAmethodsforhumanshapeestimation(Sengupta,Budvytis,andCipolla2021a;Choutasetal.2022)
(b, c) either fail on images of people wearing thick clothes or fail on images of people with extreme body shapes, while our
method(d)achievespixel-alignedresultswithhighaccuracyinbothsituations.Warmercolorsonthehumanmeshrepresent
higherper-vertexerror.
Abstract asabasisforhumanbehaviorunderstandingandhasappli-
cationsinvariousfieldssuchasVirtualReality,Augmented
AccuratehumanshaperecoveryfromamonocularRGBim-
Reality,andAutopilot.Recentmethods(Zhangetal.2022;
ageisachallengingtaskbecausehumanscomeindifferent
shapes and sizes and wear different clothes. In this paper, Lietal.2022b,a,2021)achievehighaccuracyinhumanpose
weproposeShapeBoost,anewhumanshaperecoveryframe- estimation, but their results of human shape estimation are
workthatachievespixel-levelalignmentevenforrarebody oftensuboptimal.
shapesandhighaccuracyforpeoplewearingdifferenttypes
Due to the scarcity of image datasets featuring diverse
ofclothes.Unlikepreviousapproachesthatrelyontheuseof
PCA-basedshapecoefficients,weadoptanewhumanshape body shapes, many existing methods for recovering human
parameterizationthatdecomposesthehumanshapeintobone pose and shape suffer from overfitting on body shape esti-
lengths and the mean width of each part slice. This part- mation.Theirresultsareparticularlyunsatisfactoryforvery
basedparameterizationtechniqueachievesabalancebetween thinorplumppeople.Previousapproacheshaveattemptedto
flexibility and validity using a semi-analytical shape recon- solvetheoverfittingissuethroughtwomainstrategies.The
struction algorithm. Based on this new parameterization, a firstkindofmethods(Varoletal.2017;Sengupta,Budvytis,
clothing-preservingdataaugmentationmoduleisproposedto
and Cipolla 2020, 2021b,a) train on synthetic data and ex-
generaterealisticimageswithdiversebodyshapesandaccu-
ploitproxyrepresentationstoreducethedomaingap,while
rateannotations.Experimentalresultsshowthatourmethod
the second kind of methods (Dwivedi et al. 2021; Omran
outperforms other state-of-the-art methods in diverse body
et al. 2018; Agarwal and Triggs 2005) exploit shape cues
shapesituationsaswellasinvariedclothingsituations.
which are easy to annotate as weak supervision. However,
for the first kind of methods, the synthetic images are un-
1 Introduction
natural with unrealistic texture and clothing, and the ex-
Human pose and shape (HPS) recovery from monocular
tracted proxy representations may be ambiguous and inac-
RGBimagesisanessentialtaskofcomputervision.Itserves
curate. The situation is especially severe when the individ-
Copyright©2024,AssociationfortheAdvancementofArtificial ual is wearing thick garments or is occluded in the image.
Intelligence(www.aaai.org).Allrightsreserved. For the second kind of methods, since 2D clues such as
4202
raM
2
]VC.sc[
1v54310.3042:viXra
sehtolc
kcihT
epahS
emertxE
)mm(
rorre
xetreVsegmentationsandsilhouettesarehighlycorrelatedwiththe • WeproposeShapeBoost,ahumanshaperecoveryframe-
human pose and clothing, supervising with 2D clues may work consisting of the a clothing-preserving data aug-
give wrong guidance of human shape in the case of in- mentationmoduleandashapereconstructionmodule.
accurate pose estimation or thick clothing. Moreover, the • Ourapproachoutperformspreviousapproachesandcan
real-world images of extreme shapes are still insufficient. handlediverseclothingaswellasextremebodyshapes.
SHAPY (Choutas et al. 2022) improves the second kind of
methods by using linguistic attributes and body measure-
2 RelatedWork
ments as supervision, which allows it making better esti-
matesforclothedpeople.However,similartoothermodels 2.1 3DHumanPoseandShape(HPS)
trainedonreal-worlddatasets,itstillperformspoorlyonim-
Manyalgorithmshavebeenproposedforreconstructinghu-
agesofpeoplewithextremebodyshapesbecauseofthelack
man pose and shape from RGB images, which are broadly
ofextremebodyshapesinthetrainingdatasets.Tosumup,
categorized into two types. Firstly, model-based methods
just as shown in Fig. 2, the first kind of methods often fail
estimateparametersofaparameterizedhumanmodel.Some
onimageswithpeopleinocclusionorthickclothing,while
methods(Bogoetal.2016;Pavlakosetal.2019;Guanetal.
thesecondkindofmethodsoftenfailonimagescontaining
2009) estimate human pose and shape parameters by opti-
peoplewithextremebodyshapes.
mization.Regression-basedmethods(Kanazawaetal.2018;
To overcome the above limitations, we propose Shape- Kocabas,Athanasiou,andBlack2020;Kocabasetal.2021;
Boost, a new shape recovery framework based on a novel Li et al. 2022b, 2021), on the contrary, employ neural net-
part-based shape parameterization. The new shape param- works to estimate the parameters. To reduce the difficulty
eters are composed of bone lengths and mean widths of of regression, many regression-based methods employ in-
body part slices. Using a novel semi-analytical algorithm, termediate representations, including keypoints (Kanazawa
the body shape can be accurately and robustly recovered et al. 2018), silhouettes (Pavlakos et al. 2018), segmenta-
fromtheseparameters.Duringtraining,thebonelengthscan tion (Omran et al. 2018) and 2D/3D heatmaps (Tung et al.
be calculated from human keypoints, and the part widths 2017), keypoints (Li et al. 2021, 2023b,a) etc. Some ap-
areregressedbytheneuralnetwork.Comparedtotheorig- proaches (Kolotouros et al. 2019; Muller et al. 2021; Joo,
inal shape parameters derived from PCA coefficients, our Neverova,andVedaldi2021)combineoptimizationandre-
new part-based parameterization has a clear local seman- gression. Secondly, model-free methods directly predict
tic meaning, making it easier to regress and more flexible free-form representations of the human body, with the po-
in application. During training, ShapeBoost augments new sitionofbodymodelverticespredictedbasedonimagefea-
image-shapepairsbyrandomlytransformingtherawimage tures (Corona et al. 2022; Kolotouros, Pavlakos, and Dani-
and calculating the corresponding part-based parameters. ilidis 2019; Varol et al. 2018; Lin, Wang, and Liu 2021a,b;
Forimagetransformation,aclothing-preservingaugmenta- Moon and Lee 2020), keypoints (Choi, Moon, and Lee
tion method is proposed: we first segment the human body 2020), or segmentations (Varol et al. 2018). These medth-
out of the image and randomly transform it into a different odsmostlyfocusonhumanposeestimationandtheirresults
shape. Then, the human segmentation is pasted back onto ofhumanshapeestimationareoftenunsatisfactory.
theinpaintedbackgroundimagewiththeguidanceoftheap- Our work belongs to the model-based category, and we
pearanceconsistencyheatmap(Fangetal.2019).Thecorre- adoptinversekinematicstoestimatethehumanposesimilar
spondingshapeparameterscanbeanalyticallyretrievedby toHybrIK(Lietal.2021)forsimplicity.However,insteadof
applying the equivalent transformation since each compo- directlyregressingtheshapeparameters,weemployaflex-
nentinthepart-basedrepresentationisclearlydefined. ibleandinterpretableparameterizationandanewshapere-
Comparedtopreviousapproaches,ShapeBoostgenerates construction pipeline to achieve more accurate and robust
realisticimagesofdiversehumanshapesinnaturalclothing shapeestimation.Ourmethodcanalsobeeasilyappliedto
togetherwiththecorrespondingfaithfulannotations.More- differentposeestimationbackbones.
over,ournewparameterizationaccuratelydescribestheex-
tremebodyshapesandencouragespixel-levelalignment.As 2.2 Estimating3DBodyShape
aresult,ourmethodovercomesthedisadvantagesofexisting
Most recent HPS estimation methods excel in precise pose
methodsandachieveshighaccuracyonimagesofpeoplein
estimation but exhibit limitations in accurately estimating
thick clothes as well as on images of people with extreme
the real human body shape under clothing. Some methods
body shapes. We benchmark our method on SSP-3D (Sen-
haveattemptedtoaddressthisissue,andtheymainlyfocus
gupta,Budvytis,andCipolla2020)andHBW(Choutasetal.
onnoveltrainingdatasetsandtheestimationframework.
2022) datasets. The results show that our method achieves
state-of-the-artperformanceinboththickclothessituations Training datasets for human shape estimation. Accu-
andextremebodyshapesituations. ratelyannotatingbodyshapesfrom2Dhumandatasets(Lin
et al. 2014) is hard, and commmonly-used 3D human
The main contributions of this paper are summarized as
datasets(vonMarcardetal.2018;Ionescuetal.2013)con-
follows:
tainlimitednumberofpeople.Toovercomethislimitation,
• Wepresentanaccurateandrobusthumanshapeparame- some researchers have created synthetic image datasets by
terizationtogetherwithasemi-analyticalshaperecovery renderingthemeshgeneratedbyparameterizedhumanmod-
algorithm,whichisflexibleandinterpretable. els(Hoffmannetal.2019;Sengupta,Budvytis,andCipollaShapeBoost
Semi-analytical
Rest Pose Mesh
Shape Reconstruction
w
During Training Part Width M0 MLP
Encoder
Skeleton
Twist
Figure3:Theoverallpipeline.First,theinputimageisrandomlytransformedwiththeclothing-preservingimagetransforma-
tion,andaconvolutionalneuralnetwork(CNN)isemployedtoextractskeleton,partwidthsandtwistrotations.Then,thepose
isobtainedusinginversekinematicsandtheshapeisobtainedwithoursemi-analyticalalgorithm.Thefinalmeshisretrieved
basedontheposeandshapeparameter.TheShapeBoostframeworkconsistsoftheimageaugmentationmoduleandtheshape
reconstructionmodule.
2020; Varol et al. 2017; Weitz et al. 2021). However, it is the human is in thick clothing and our method will not en-
difficult to obtain images with natural clothing and realis- largeerrorevenwhentheposeestimationisinaccurate.
tic scenes using the naive rendering. Recently, more realis-
ticsyntheticdatasets(Bertiche,Madadi,andEscalera2020; 3 Method
Pumarolaetal.2019;LiangandLin2019;Pateletal.2021; Inthissection,wepresentoursolutionforhumanshapere-
Black et al. 2023) have been proposed, which contain peo- covery(Fig.3).First,wegivebackgroundknowledgeofthe
pleindifferentclothingwiththehelpofhumanscans,sim- parameterization of SMPL model in Sec. 3.1. Considering
ulationordeepgenerativenetworks.Choutasetal.(Choutas itsdrawbacks,aflexibleandinterpretablepart-basedhuman
etal.2022)haveproposedtheModel-Agencydataset,which shapeparameterizationisproposedinSec.3.2.Basedonthis
uses images from model agency websites labeled with lin- new parameterization, in Sec. 3.3, we design a new human
guistic attributes and measurements. Although these new shaperecoveryframeworkcalledShapeBoost.Thetraining
datasets contain more diverse body shapes, most datasets pipelineandlossfunctionsaredescribedinSec.3.4.
stilllackpeoplewithextremebodyshapes,andtheauthen-
ticityofsyntheticimagesremainsinsufficient. 3.1 Preliminary
SMPL Model. In this work, SMPL model (Loper et al.
Estimation Framework. Several methods (Sengupta,
2015)isemployedtorepresenthumanbodyposeandshape.
Budvytis, and Cipolla 2020, 2021b,a) train the network di-
SMPLprovidesadifferentiablefunction (θ,β)thatmaps
rectlyonsyntheticdata.Toreducethedomaingap,theyuse poseparametersθ R3J andshapeparamV etersβ R10 to
proxyrepresentations(PRs)asinput,suchaspartsegmenta- ∈ ∈
ahumanmeshV,whereJ isthenumberofjoints.Thepose
tion masks (Varol et al. 2017), silhouettes (Sengupta, Bud-
parameters θ represent the relative rotation of body joints,
vytis,andCipolla2020;Ruizetal.2022),Cannyedgedetec-
andtheshapeparametersβ arecoefficientsofaPCAbody
tionresults(Sengupta,Budvytis,andCipolla2021b,a)or2D
shapebasis.SMPLmodelisdrivedintwosteps:
keypoint heatmaps (Sengupta, Budvytis, and Cipolla 2020,
T= (β), (1)
2021b,a). Other work (Dwivedi et al. 2021; Omran et al.
S
2018; Agarwal and Triggs 2005) uses real-world data for V= (θ,β)= (θ, (β)). (2)
V P S
training and exploits 2D shape cues as supervision. Body- First,arest-posemeshTisconstructedusingfunction .
partsegmentationmasks(Dwivedietal.2021;Omranetal. Second, the rest-pose mesh is driven to the target pose bS y
2018)andsilhouettes(AgarwalandTriggs2005)arewidely function .Theshapeofthemeshisdeterminedonlybyβ,
usedamongthem.LVD(Coronaetal.2022)learnsthever- and the pP osing procedure does not change the body shape.
tex descent direction based on image-aligned features, and Most current methods regress shape parameters β directly.
SHAPY (Choutas et al. 2022) uses linguistic attributes and However,sincemostavailabletrainingdatasetslackpeople
bodymeasurementsassupervision. with diverse body shapes, these methods often overfit and
Unlikepreviouswork,ourmethodgeneratesimageswith failtogeneralizetounseenbodyshapes.
diversehumanbodyshapeswithoutalteringclothing,light-
ing,andbackgrounddetails.Therefore,thediversityisrich 3.2 Part-basedParameterization
and the domain gap is small. Since our framework utilizes Inthiswork,weproposeanovelparameterizationofhuman
ournewparameterization,thereisnoambiguityevenwhen shape using bone lengths and widths of part slices. Com-
gnivreserp-gnihtolC
mrofsnarTMasked Body Reshaped Body
Bone length
matting trr aa nn sd fo om
rm
Heatmap-based
1 2 3 Position Search
The split of slices Background Inpainted BG.
Vertex width
inpainting
Figure4:Illustrationoftheshapedecompositionprocedure.
Figure 5: The illustration of the clothing-preserving trans-
Fromlefttoright,thefigureshowsthepartsegmentation,the
formation.
definitionofbonelengthandvertexwidth,andtheslicingof
onebodypart.
malbodyshapes.Therefore,weusea4-layerMLPtomod-
ifytheanalytically-retrievedshapeparameters.Thefinalfor-
paredtotheβrepresentationwhichusesaglobaldescriptor
mulaof canbewrittenas
of the body shape, this new representation allocates shape
M
descriptors to local body parts. This allows the network to T= (l,w)=MLP( (l,w),l,w,∆l,∆w), (5)
0
learnfromlocalimagefeaturesandthusalleviatestheover- M M
fitting problem. Furthermore, our parameterization is more where ∆l and ∆w are the difference between the tar-
flexible and interpretable, allowing compatibility with our getbonelengthsandpartslicewidthsandthecorresponding
dataaugmentationprocedurediscussedinSec.3.3. valuesobtainedby 0.Inpractice,insteadofregressingthe
M
In our parameterization, the SMPL mesh is divided into bonelengthsdirectly,weextractthebonelengthsfromhu-
J = 24 segments according to the linear blending weight, mankeypoints.Thissettingfurtherencouragesthenetwork
and each segment has a corresponding central bone ended toonlyfocusonlocal,per-partimagefeaturesandthusalle-
with two joints. The distance of one vertex from its corre- viateoverfitting.
spondingboneiscalledthe“width”ofthisvertexforshort.
Eachbodypartisfurtherslicedintoncomponentsalongthe 3.3 ShapeBoost
bone, and the mean widths of the vertices in these n slices Armed with the part-based parameterization discussed in
areusedtorepresentthethicknessofthatpart.Thesegment- Sec 3.2, we can manipulate the body shape in an intuitive
ingandslicingtechniqueisvisuallyillustratedinFig.7.In waybystretchingthebonelengthsandbroadeningthepart
thisway,theformulaofSMPLmodelisconvertedto: slicewidths.Thesemanipulationsenableustoaugmentthe
raw human images and retrieve the new ground truth body
shapewhichaccuratelyexplainsthefigureintheimageaf-
T= (l,w), (3)
M terthetransformation.Thisframework,namedShapeBoost,
V= (θ, (l,w)), (4) generates diverse body shapes while preserving clothing,
P M
lighting, and background details, and then takes use of our
where l RJ−1 represents the bone lengths of the body
skeleton ∈ and w RnJ represents the mean widths of all newparameterizationtoreconstructthebodyshape.
∈
partslices.Underournewrepresentation,theSMPLmodel Clothing-preserving Image Transformation. An intu-
firstderivesarest-posemeshusing (l,w),andthenuses itivewaytochangethehumanshapeinanimageistoapply
M
function todrivethemeshtothetargetposejustlikethe the affine transformation to the input image. For example,
P
originalSMPLmodel. scalinganimagewithanaspectratiounequalto1resultsin
Deriving the function directly by a neural network avisuallythinneroramplerhumanfigure.
M
is untrivial and can lead to overfitting. Therefore, a semi- However, applying the affine transform to the entire im-
analytical algorithm is proposed that first solves a roughly age results in a stretched background, which may leak the
correctmeshusinganalyticalmethodsandthenusesamulti- scaling information and thus incur overfitting. To alleviate
layerperceptron(MLP)tocorrecttheresultusingerrorfeed- this problem, we propose a silhouette-based augmentation
backtechniques. methodinspiredbyInstaboost(Fangetal.2019).Insteadof
We can analytically retrieve a body shape that roughly affine transforming the whole image, we first segment the
conformstothetargetbonelengthsandpartslicewidthsby humanbodyoutusingthegroundtruthsegmentation.Then
(1)stretchingthebonesandbroadeningeachpartsliceofthe weinpaintthebackgroundimage,affinetransformtheseg-
templatemeshaccordingtothetargetvalues.(2)usinglin- mentedhumanbody,andpastethetransformedhumanbody
earblendweights(LBSweights)toassembletheseadjusted backontotheinpaintedbackgroundimagewiththeguidance
parts.(3)usingthePCA-coefficientsofSMPLtoretrievethe of the appearance consistency heatmap (Fang et al. 2019).
shape parameters from the deformed template mesh. This This method effectively avoids background stretching and
mappingisreferredtoas .Adetaileddescriptionofthe producesmorenatural-lookingimages.Theprocessisvisu-
0
M
analyticalalgorithmisavailableinsupplementarymaterials. allyillustratedinFig.5.
Sincetheinputbonelengthsandpartwidthsoftencontain To simplify the discussion, we assume that the affine
noise,theanalyticalalgorithmsometimesproducessubopti- transformation consists of a rotation matrix and a scalingmatrix,whichiswrittenas We employ end-to-end training for the pipeline, and the
(cid:20) (cid:21)(cid:20) (cid:21) lossfunctionconsistsofthreecomponents:shapeloss,pose
a 0 cosθ sinθ
T =SR= − . (6) loss,andshape-decomposeloss.TheCNNbackboneissu-
0 b sinθ cosθ
pervised by shape loss and pose loss, while the MLP used
Shape-parameter Derivation. People in different poses intheshapereconstructionmoduleissupervisedbyshape-
are affected by the image transformation in different ways, decomposeloss.
which poses a great challenge for the derivation of the
ShapeLoss. Inshapeloss,wesupervisethepredictedpart
PCA-based shape parameters after the image transforma-
widthspredictedbytheCNNbackbone.Specifically,were-
tion.However,withthepart-basedparameterization,wecan
quire the projection results of the part slice widths and the
still accurately explain the new body shape by estimating
vertex widths to be close to the target value after data aug-
the widths and bone lengths of each body part. We use or-
mentation.Krepresentsthenumberofverticesinthehuman
thographicprojectioninourderivation.
meshmodelandJ representsthenumberofjoints.
Given the camera and pose parameters, the bone lengths
aftertransformationcanbeeasilyobtainedbystretchingthe J K
(cid:88) (cid:88)
bonestoensureaconsistent2Djointprojection.Compared L = wˆ2D w¯2D 2 + µ wˆ2D w¯2D 2.
shape ∥ j − j ∥2 0 ∥ k − k ∥2
to the derivation of bone lengths, the derivation of the part
j k
slicewidthsaftertransformationismorecomplex.Suppose (9)
avertexindexedbyk belongstothej-thpart.Thedistance
PoseLoss. Poselossisdesignedtosupervisethepredicted
ofthevertexfromthepartboneonthe2Dimageplane,de-
skeletonandtwistangle.Weadoptthesamelossfunctionas
noted by w2D, is affected by the transformation according
k HybrIK(Lietal.2021)anddenoteitasL .
tothefollowingequations: pose
Shape-decompose Loss. Shape-decompose loss ensures
ab l2D
w¯2D = · j w2D, (7) thattheshapereconstructionmodulepredictsavalidhuman
k ¯l j2D k
mesh while best preserving the part slice widths and bone
wherel2Dand¯l2Drepresentthebonelengthsofpartjonthe lengthspredictedbytheCNNbackbone.Itconsistsofthree
j j lossfunctions
2Dimageplanebeforeandafterthetransformation,respec-
tively;aandbarescalingfactorsmentionedinEq.19.Ade- L =L +L +µ L , (10)
decomp bone width 1 reg
tailedderivationisavailableinthesupplementarymaterials.
where
ItisnoteworthythatEq.7impliesthe2Dwidthsofvertices
on the same part are scaled by the same factor. Therefore, J
theunderlining3Dpartwidthofpartj ischangedby L
=(cid:88)(cid:16)
x˜ xˆ + ˜l ˆl
(cid:17)
, (11)
bone j j 1 j j 1
∥ − ∥ ∥ − ∥
s¯ ab l2D j
w¯ j = s × ¯l j2· Dj ×w j. (8) L =(cid:88)J (cid:32) w˜ wˆ 2 + w˜ j wˆ j 2(cid:33) , (12)
width ∥ j − j ∥2 ∥ ˜l − ˆl ∥2
Intheequation,sandw j arethescalefactoroftheortho- j j j
graphicprojectionandthe3Dpartwidthofpartjbeforethe L = β˜ 2. (13)
imagetransformation,whereass¯andw¯ arethecorrespond- reg ∥ ∥2
j
ingvaluesafterthetransformation.Duetoscaleambiguity, Intheequations,x˜ ,˜l ,w˜ arethekeypointcoordinates,
j j j
s¯is an ambiguous scaling factor that is difficult to directly the bone length and the part slice widths of part j refined
derive.Therefore,inourtraining,weonlysupervisethepro- bytheMLPintheshapereconstructionmodule.L and
bone
jectedresultsofthepredictedpartslicewidthsonthe2Dim- L supervisethepreservationofthebonelengthandpart
width
age plane, without directly supervising their actual values. slicewidthsrespectively,andL regularizesβ˜parameter.
reg
We hypothesize that the network can learn the best scaling
factors¯usingthepriorknowledgeofhumanbodyshape. Overall Loss. The overall loss of our pipeline is formu-
latedas
3.4 TrainingPipelineandLossFunction
L=L +µ L +µ L . (14)
pose 2 decomp 3 shape
The overall training pipeline is illustrated in Fig. 3. First,
theinputimageistransformedusingtheclothing-preserving 4 Experiments
imagetransformation,andtheconvolutionalneuralnetwork
4.1 Datasets
(CNN) backbone is utilized to process the augmented im-
ageandestimatetheskeleton(3Dkeypointsextractedfrom We use 3DPW (von Marcard et al. 2018), Hu-
heatmaps), twist angles and part slice widths. Second, we man3.6M (Ionescu et al. 2013), COCO (Lin et al.
usetheseestimatedvaluestoreconstructtheposeandshape 2014), AGORA (Patel et al. 2021) and Model Agency
oftheindividual.Theposeparametersareobtainedwithin- Dataset (Choutas et al. 2022) for training. The original
verse kinematics similar to HybrIK (Li et al. 2021), while Model Agency Dataset contains 94,620 images of 4,419
theshapeparametersareretrievedusingthesemi-analytical models, but we only use about one-third of these images
algorithmdiscussedinSec.3.2.Thefinalmeshisobtained in our training due to the unavailability of many images
basedontheposeandrefinedshapeparameters. ontheInternet.Toavoiddatabias,theimagesaresampledMethod Model PVE-T-SC Method H C W HC P2P
↓ 20K
HMR(Kanazawaetal.2018) SMPL 22.9 SPIN 59 92 78 101 29
SPIN(Kolotourosetal.2019) SMPL 22.2 Senguptaetal.2020 135 167 145 102 47
(Senguptaetal.2020) SMPL 15.9 TUCH 58 89 75 57 26
(Senguptaetal.2021b)† SMPL 13.3 Senguptaetal.2021a 82 133 107 63 32
(Senguptaetal.2021a) SMPL 13.6 CLIFF - - - - 27
HybrIK(Lietal.2021) SMPL 22.8 SHAPY 51 65 69 57 21
LVD(Coronaetal.2022) SMPL 26.1
ShapeBoost(SMPL) 66 63 58 47 25
CLIFF(Lietal.2022b) SMPL 18.4
ShapeBoost(SMPL-X) 68 69 56 49 22
SHAPY(Choutasetal.2022) SMPL-X 19.2
SoY(Sarkaretal.2023) SMPL 15.8
(Maetal.2023) SMPL 18.8 Table 2: Quantitative comparisons with state-of-the-art
methodsontheHBWtestsetinmm.
(Senguptaetal.2021a)∗ SMPL 15.4
SHAPY(Choutasetal.2022)∗ SMPL 12.2
ShapeBoost(Ours) SMPL 11.4 Method H C W HC P2P
20K
ShapeBoost(Ours) SMPL-X 12.0
Senguptaetal.2021a 68 89 111 71 30
HybrIK 88 82 74 51 33
Table 1: Quantitative comparisons with state-of-the-art
LVD# - 89 131 87 31
methods on the SSP-3D test set in mm. Symbol means
SHAPY 63 59 85 54 25
†
usingmultipleimagesasinput,andsymbol meansretrain- Maetal.2023 112 87 133 59 41
∗
ingusingthesametrainingsettingasourmethod.
Senguptaetal.2021a∗ 72 66 74 49 29
SHAPY∗ 62 52 72 50 26
ShapeBoost(SMPL) 58 54 72 42 25
following previous work (Choutas et al. 2022). We also
ShapeBoost(SMPL-X) 61 49 71 49 23
follow previous work and use synthetic data to assist
network training. The rendering settings are identical to
Table 3: Quantitative comparisons with state-of-the-art
(Sengupta,Budvytis,andCipolla2021a).
methods on the HBW validation set in mm. Symbol #
We evaluate our model on SSP-3D (Sengupta, Budvytis,
meansusinggroundtruthscaleandsymbol meansretrain-
andCipolla2020)andHBWdatasets(Choutasetal.2022). ∗
ingusingthesametrainingsettingasourmethod.
The results on SSP-3D dataset show the model’s perfor-
mance on diverse human body shapes, while the results on
HBW dataset indicate the model’s performance on images
ofpeoplewearingdifferentclothing. 4.3 AblationStudy
Todemonstratetheeffectivenessofdifferentcomponentsin
4.2 ComparisonwiththeState-of-the-art
ourmethod,weconductablationstudiesonSSP-3Ddataset
We evaluate the performance of different methods on SSP- andHBWvalidationset.
3DandHBWtestandvalidationdatasets.Followingprevi-
ous work, on SSP-3D dataset, we use PVE-T-SC, a scale- Shape reconstruction. To analyze the effectiveness and
normalized per-vertex error metric to evaluate the model robustness of our new human shape parameterization, we
performance. On HBW dataset, we report the predicted reconstruct body shapes using bone lengths and part slice
height (H), chest (C) , waist (W), and hip circumference widths with different reconstruction algorithms under dif-
(HC)errors,andP2P 20K errorsofdifferentmodels.Allthe ferent noise ratios. The results are shown in Tab. 4. All the
experiments of our method use part slicing number n = 1 modelaretrainedonshapeparameterssampledfromGaus-
bydefaultunlessotherwisestated.Forafaircomparison,we sian distributions and tested on 500 different body shapes
also retrain two best-performing networks (Sengupta, Bud- obtainedfromAMASSdataset(Mahmoodetal.2019).“Hy-
vytis,andCipolla2021a;Choutasetal.2022)withthesame brid”algorithmmeansusingthesemi-analyticalalgorithm,
datasetsandsettingsasourmethod. “Analytical”algorithmmeanssolelyemployingtheanalyti-
Tab. 1 shows that our method surpasses previous works calalgorithm,and“NN”algorithmmeansdirectlyusingthe
on SSP-3D dataset, which shows that our method can deal neuralnetworkwithoutanalyticalsteps.Fromthefirstthree
withthediversehumanbodyshapemuchbetterthanprevi- linesinTab.4,weobservethatourproposedsemi-analytical
ousmethods.Tab.3and 2showstheperformanceonHBW algorithm achieves the lowest error especially when the
validationandtestdataset.OnHBWtestdataset,ourmethod noise ratio is small. Additionally, when the noise is sub-
achieves comparable results with previous SOTA methods tle, the parameterizations using different part slicing num-
and predicts more accurate waist and hip circumferences. ber(n = 1,2,3)allachieveanacceptablelowerror.When
On HBW validation set, our method outperforms previous thenoiseratioislarge,theerrorratiodecreaseswithlarger
SOTA methods. These results prove that our method can n.Thus,wecanconcludethatoursemi-analyticallymethod
dealwithdiversehumanclothingbetterthanpreviousmeth- accuratelyreconstructshumanshape,andalargernmakes
ods.QualitativeresultsareprovidedinFig.6. itmorerobusttonoise.110 100
0 0
60 55
0 0
110 100
0 0
(a) Sengupta et al. (b) SHAPY (c) Ours (a) Sengupta et al. (b) SHAPY (c) Ours
Figure 6: Qualitative results on SSP-3D and HBW datasets. From left to right: Input image, (a) Sengupta et al. (Sengupta,
Budvytis,andCipolla2021a)results,(b)SHAPY(Choutasetal.2022)results,and(c)Ourresults.Warmercolorsmeanhigher
per-vertexerror.ExperimentsonSSP-3DdatasetusePVE-T-SCmetric,andexperimentsonHBWdatasetuseP2P metric.
20K
V2VError(mm) Method PVE-T-SC P2P
↓ 20K
n Algo. 0%noise 1%noise 2%noise 5%noise β 12.3 26.0
1 Hybrid 0.69 2.30 5.95 8.83 n=1 11.4 25.1
1 Analy. 6.14 6.59 8.99 12.34 n=2 11.6 26.2
1 NN 1.82 2.99 6.20 8.98
Table 5: Ablation experiments of shape estimation from
2 Hybrid 0.58 2.01 5.40 8.21
RGBimagesusingdifferentshapeparameterizationonSSP-
3 Hybrid 0.65 1.93 5.00 7.63
3DandHBWvalidationsetinmm.
Table4:Ablationexperimentsofreconstructingshapeusing
ournewshapeparameterizationinmm. Method PVE-T-SC P2P
20K
ShapeBoost(Ours) 11.4 25.1
w/oAugment 12.1 26.5
w/oAugment,w/oDecompose 12.4 27.0
Shape estimation from images. We also experiment us-
ing different parameterizations for estimating human body
shapesfromRGBimages.Tab.5providesacomparisonof Table6:Ablationexperimentsofdataaugmentationmodule
theresultsobtainedusingthedirectshapeparameterization
onSSP-3DandHBWvalidationsetinmm.
(β) (Li et al. 2021) with our novel parameterization utiliz-
ing n = 1 and n = 2. We use image augmentation in the
5 Conclusion
training. Since it is hard to find a ground truth β for aug-
mented images, we use the 2D coordinates of vertices as Inthispaper,wepresentShapeBoost,anewframeworkfor
supervision. We find that using our new parameterization accuratehumanshaperecoverythatoutperformsthecurrent
yieldsbetterresults,butalargerndoesnotimproveperfor- state-of-the-artmethods.Thisframeworkexploitsanewhu-
mance.Thereasonsare(1)theparameterizationwithn=1 man shape parameterization that decomposes human shape
alreadyachievesasmallshapereconstructionerror(2)using into bone lengths and the mean width of each part slice.
largerncomplicatestheregressiontaskfortheCNNback- Compared to the existing representation with PCA coeffi-
bone, resulting in a reduction in the accuracy of predicting cients, our new method is more flexible and interpretable.
partslicingwidths. Based on the new shape parameterization, a new clothing-
preservingdataaugmentationmoduleisproposedtogener-
aterealisticimagesofvarioushumanshapesandthecorre-
The effectiveness of data augmentation. We also make sponding accurate annotations. Our method randomly aug-
ablation studies with different training data quantitatively. ments the body shape without destructing the clothing de-
The results are shown in Tab. 6. When the data augmenta- tails.ExperimentsshowthatourmethodachievesSOTAper-
tionmoduleisnotused,theperformanceofourmodeldrops formanceforextremebodyshapesaswellasachieveshigh
onbothHBWandSSP-3Ddataset.Thisshowstheeffective- accuracyforpeopleunderdifferenttypesofclothing.
nessofourdataaugmentationmodule.6 Acknowledgments
CewuLuisthecorrespondingauthor.Heisthememberof
QingYuanResearchInstitute,QiZhiInstituteandMoEKey
Lab of Artificial Intelligence, AI Institute, Shanghai Jiao
TongUniversity,China.
This work was supported by the National Key R&D
Program of China (No. 2021ZD0110704), Shang-
hai Municipal Science and Technology Major Project
(2021SHZDZX0102),ShanghaiQiZhiInstitute,andShang-
haiScienceandTechnologyCommission(21511101200).Appendix
Inthesupplementaldocument,weprovide: Analytical MLP Refinement
w, l
Sec.A Detailsoftheproposedpart-basedparameterization.
Sec.B DetailsofShapeBoost.
Sec.C Additionalimplementationdetails.
Per-vertex Error
Sec.D Additionalexperimentalresults.
60 mm
Sec.E ThemethodforconvertingtheSMPLmeshtotheSMPL-
Xmesh.
30 mm
Sec.F Limitationsandfuturework.
Sec.G Morequalitativeresults.
0 mm
A DetailsofPart-basedParameterization
Figure7:Erroranalysisoftheshaperecoveryalgorithms
Inourpart-basedparameterization,weuseasemi-analytical
used in the part-based parameterization. Given a ground
algorithm ( ) to reconstruct the human shape. Given the
M truth mesh (top left), we first extract its bone lengths and
bonelengthsandpartslicewidths,wefirstuseananalytical
partslicewidths.Then,wereconstructthehumanmeshwith
algorithm toobtainaroughmesh,andthenuseamulti-
layerperceM ptr0
ontorefinethemesh.Inthissection,wegive
these extracted values using the analytical algorithm ( M0)
andMLPrefinement( ).
the details of the analytical algorithm and provide an error M
analysisof and .
0
M M
A.1 Detailsof M0 { tev mj, p1, lav tj e,2 m,. e. s. hv bj, en lo} n. gA ins gsu tm oe thp ek i-ti hs sa liv ceer it nex pao rn tjt .h qe S iM stP hL
e
Inourparameterization,theSMPLmesh(Loperetal.2015) k
projection point of p on the template’s bone ended with
is divided into J = 24 segments according to the linear k
t andt .Thenewvertexpositionaftertheadjustmentis
blendingweight.Eachvertexbelongstothebodypartwhich j1 j2
computedas:
has the largest blending weight among all the joints. This
splitting method is the same as that used in PARE (Ko- t q
cabasetal.2021).Toanalyticallyobtainameshwhosebone p′ jk =x j2+ ∥ tj1 − tk ∥2(x j1 −x j2) (16)
lengths and part slice widths approximate the target val- ∥ j1 − j2 ∥2
w
ues, the SMPL mean-shape template is modified according + j,i(p q ). (17)
to (1) the target bone lengths, denoted as l and (2) the tar- v j,i k − k
get part slice widths, denoted as w. The bone lengths l are
This equation broadens the distance of each vertex from
composed of the lengths of all pairs of joints connected in
thebonewhilekeepingitsrelativeprojectionpositionalong
the kinematic tree. The part slice widths w consist of the
theboneunchanged.
mean width of each slice in different body parts. Suppose
The final coordinate of each vertex is linearly blended
the part slicing number is n, and the number of body parts
witheachpart:
is J. The slice widths on the j-th part is denoted as w ,
j
sw
lij
ce=
w{
iw
dtj h,1
s, ww
j
=,2,. w.. 1w
,wj,n 2,}
., .a .n ,d wo Jnt .hewholebody,thepart
p′
k
=(cid:88)J
w jlb ksp′
jk
, (18)
{ }
First,therest-poseskeletonofSMPLtemplateisstretched j=1
toensurethatthebonelengthsmatchthetargetvalues.Sup-
wherewlbsistheLBSweightofthek-thvertexonpartj.
pose the bone with index j connects two joints with index jk
Inthisway,theapproximatedSMPLmeshisanalytically
j1 and j2. The coordinates of these joints in the template
obtained.Thismappingisreferredtoas .
meshisdenotedast
j1
andt j2,andthecoordinatesofthese M0
joints after stretching is denoted as x and x . l repre-
j1 j2 j
A.2 ErrorAnalysisof and
sentsthetargetlengthsofthisbone.Thesevaluessatisfythe 0
M M
followingequation: Wealsoprovidetheerroranalysisof and forshape
0
M M
reconstructioninFig.7.Givenagroundtruthmesh,wefirst
t t
x j2 =x j1+l j j2 − j1 . (15) extractitsbonelengthsandpartslicewidths.Then,thehu-
t t
∥ j2 − j1 ∥2 manmeshisreconstructedwiththeseextractedvaluesusing
This equation stretches the bone lengths to the target value theanalyticalalgorithm( )andMLPrefinement( )in
0
M M
whilekeepingthedirectionofeachboneunchanged. turn.Fromtheper-vertexerrorheatmap,wecanfindthatus-
Second, the vertex positions on each part are adjusted ingtheanalyticalalgorithm( )alonealreadyproducesa
0
M
according to w. We split each human part into n slices. meshthatisclosetothegroundtruthmesh.However,since
The target body slice widths on part j are denoted generates the mesh by trivially stretching the template
0
M
by w = w ,w ,...w , and the corresponding mesh,somedetailsofthehumanformarelost.Forexample,
j j,1 j,2 j,n
{ }
slice widths in the mean-shape template mesh are v = the shape of the breasts and hips are slightly changed. The
jmultilayerperceptronisthenusedtorefinethemeshgener- oftheMLPistheconcatenationof(1)analytically-retrieved
atedby .Aftertherefinement,thedetailsarerecovered, 10-dim shape, (2) the predicted body part widths regressed
0
M
andtheper-vertexerrordropstoalmostzero. by the CNN, (3) the bone lengths extracted from the pre-
dicted human keypoints, (4) the difference of bone lengths
B DetailsofShapeBoost andpartslicewidthsbetweenthetargetvaluesandtheval-
uesobtainedbytheanalyticalalgorithm .Theoutputof
Inthissection,weprovidesomedetailsoftheshapeparam- 0
M
the MLP is the refined 10-dim body shape. All the experi-
eterderivation,andgivequalitativeresultsofourdataaug-
mentsusen=1bydefaultunlessotherwisestated.
mentationmodule.
B.1 DetailsofShape-parameterDerivation
Datasets Werandomlyapplydataaugmentationto67%of
Usingthesamenotationasthemainpaper,weassumethat
the input images. We utilize the silhouette-based augmen-
theaffinetransformationconsistsofarotationmatrixanda
tation paradigm when the subject is not occluded and the
scalingmatrix.Thetransformationmatrixiswrittenas
ground truth segmentation is available. For other cases, we
(cid:20) (cid:21)(cid:20) (cid:21)
a 0 cosθ sinθ usenaiveaugmentationthataffinetransformstheentireim-
T =SR= 0 b sinθ − cosθ . (19) age.Theaspectratioofthescalingfactorintheaffinetrans-
formation (a) is uniformly sampled from 0.4 to 1.0 with a
b
Afterapplyingtheaffinetransformationtotheimage,the probabilityof33%,anduniformlysampledfrom1.0to2.5
2D bone length of the j-th part in the image plane (¯l j2D) is withaprobabilityof67%.Thisselectionofprobabilityaims
changedby: togeneratemoreimagesofpeoplewithchubbybodyshapes
¯l2D =(cid:13) (cid:13)x¯2D x¯2D(cid:13) (cid:13) =(cid:13) (cid:13)T(x2D x2D)(cid:13) (cid:13) , (20) that the original datasets lack. After the affine transforma-
j j1 − j2 2 j1 − j2 2 tion,thesizeoftheboundingboxisadjustedsothatthesub-
ject is positioned in the center of the image and occupies a
wherex2Dandx2Darecoordinatesofthebones’sendpoints
j1 j2 relativelylargespace.
in the original image, and x¯2D and x¯2D are those coordi-
j1 j2
natesinthenewimageaftertransformation. Following SHAPY (Choutas et al. 2022), we use Model
Suppose a vertex indexed by k belongs to the j-th part. Agency Dataset (Choutas et al. 2022) in our training
Onthe2Dimageplane,theequationalwaysholdswhatever and utilize height, weight, chest/waist/hip circumference
θis. and linguistic shape attributes as weak supervision. Af-
w¯2D¯l2D =ab w2Dl2D, (21) ter adding Model Agency Dataset, the model’s perfor-
k j · k j manceonHBWvalidationsetisslightlyimproved(P2P
where l2D and w2D are the bone length of the j-th body 20K
j k drops by less than 1 point). We follow previous work and
part and the distance of the k-th vertex to the bone on the
use fixed data sampling ratios for training. The sampling
2Dimageplanebeforetransformation.w¯2D and¯l2D arethe
k j ratios are 15% Human3.6m (Ionescu et al. 2013), 25%
correspondingvaluesafterthetransformation.
COCO (Lin et al. 2014), 5% 3DPW (von Marcard et al.
Thenwecangetthe2Dwidthofthevertexindexedbyk
2018),30%AGORA(Pateletal.2021),25%ModelAgency
aftertheimagetransformationby:
Dataset(Choutasetal.2022).Ineachiteration,wealsoadd
ab l2D 50%syntheticdatageneratedwiththesamesettingas(Sen-
w¯2D = · j w2D. (22) gupta,Budvytis,andCipolla2021a).
k ¯l2D k
j
WeevaluateourmethodonHBWandSSP-3Ddatasets.In
B.2 VisualizingtheImageAugmentation Fig.10a,wevisualizethe2ndand3rdshapecoefficientsof
β indifferentdatasets.Itshowsthebodyshapesintheaug-
We visualize the results of the clothing-preserving image
mentedtrainingsetandSSP-3Daremorediversethanother
augmentation in Fig. 8. We can find that our data augmen-
datasets.Onthecontrary,thediversityofclothinginHBW
tation module provides realistic images with diverse body
dataset surpasses that in SSP-3D dataset. SSP-3D mainly
shapesandnaturalclothing.
containspeopleintightorminimalclothing,whereasHBW
dataset has a broader range of clothing types, including T-
C ImplementationDetails
shirts,sweaters,dresses,thickjackets,etc.
Detailed Model Structure Following previous methods,
weuseHrnet-W48(Wangetal.2020)asourbackbone.The
CNNbackboneisinitializedusingpretrainedweightsfrom
TrainingandLoss Ourmodelundergoes80000iterations
HybrIK(Lietal.2021).TheoutputoftheCNNbackboneis
of training with the Adam solver. The learning rate is ini-
the3Dskeleton,thepartslicewidthsandthetwistangle.Af-
tially set to 1 10−3 at first and decreased by a factor of
terthebackbone,thesemi-analyticalalgorithmisusedtore- ×
10 after 60000 iterations. The training is performed with a
finethehumanbodyshape.TheMLPusedinthealgorithm
mini-batchsizeof32perGPUandutilizes4GPUsintotal.
comprises 4 linear layers with LeakyReLU activation and
ImplementationisinPyTorch.
hidden sizes of 512. It is pretrained on AMASS, and then
trainedend-to-endwiththewholenetwork.FollowingEq.5 Thescalarcoefficientsinthelossfunctionareµ =0.01,
0
inthemainpaper(inSec.3.2inthemainpaper),theinput µ =0.01,µ =0.1,µ =1.
1 2 3Raw Image After ShapeBoost Raw Image After ShapeBoost Raw Image After ShapeBoost
Figure8:Qualitativeresultsofimagesgeneratedbythedataaugmentationmodule.Thegeneratedimagesarerealisticand
includediversebodyshapes.
D ExtraExperiments
D.1 PoseEstimationExperiments
We compare our results of pose estimation with previ-
ous methods on 3DPW dataset (von Marcard et al. 2018).
SinceourmethodusestheposeestimationbackboneofHy-
brIK(Lietal.2021),wealsoretrainHybrIK(Lietal.2021)
usingthesamebackboneandtrainingdatasetsasourmethod
(a)Firsttwodim.ofshapePCA (b)t-SNEresultofshape.
forafaircomparison.TheresultsareshowninTab.7.
From Tab. 7, we can find that our model achieves more
Figure10:Visualizationofhumanshapedistributionbefore
accurateposeestimationresultsthanpreviousmethods.The
andafteraugmentation.
poseestimationscoreshowsthatourmethodachievespixel-
level alignment to the input images. Compared to previous
methods that directly predict the shape parameter from the
afteraugmentationinFig.10a10b.InFig.10a,weshowthe
image, our model first predicts the joint coordinates, and
first2dimensionsofshapePCA,andinFig.10b,weshow
thenrecoverstheshapebasedontheextractedskeleton.As
thet-SNEresultofbodyshapes.Wecanfindthattheshape
aresult,theresultingshapeismoreconsistentwiththekey-
distributionafteraugmentationdistributescoverstheolddis-
pointpredictionsandshowsbetterimagealignment.
tributionandalsocoversmorearea.Thisshowsthatourdata
D.2 AnalysisofDataAugmentation augmentation module can greatly increase the diversity of
thetrainingdata.
To demonstrate the effectiveness of our data augmentation
module,wefirstvisualizetheshapedistributionbeforeand
D.3 InfluenceofPose-dependentShape
Deformation
Inourexperiments,wealwayspredictbonelengthsandpart
widths in rest pose SMPL model instead of the values in
the posed SMPL model, and do not take into account the
pose-dependentshapedeformation.Accordingtoourexper-
iments on AMASS dataset, pose-dependent deformations
influences 0.1% in bone lengths and 2% in part widths in
Figure9:2ndand3rdshapecoefficientindifferentdatasets. average,whichareminor.Model MPJPE PA-MPJPE
HMR(Kanazawaetal.2018) SMPL 130.0 81.3
SPIN(Kolotourosetal.2019) SMPL 96.9 59.2
(Sengupta,Budvytis,andCipolla2020) SMPL - 66.8
ExPose(Choutasetal.2020) SMPL-X 93.4 60.7
EFT(Joo,Neverova,andVedaldi2021) SMPL 85.1 52.2
(Sengupta,Budvytis,andCipolla2021a) SMPL 84.9 53.6
HybrIK(Lietal.2021) SMPL 80.0 48.8
PARE(Kocabasetal.2021) SMPL 74.5 46.5
SHAPY(Choutasetal.2022) SMPL-X 95.2 62.6
ShapeBoost(Ours) SMPL 75.3 44.6
Table7:Quantitativecomparisonsforposeestimationon3DPW(vonMarcardetal.2018).
E ConvertingSMPLPredictiontoSMPL-X Tosimplifytheequation,wedenote
 
1 0 0
−
0 1 0
The proposed dataset labels and pretrained models in   0 − 0 1  
S etH aA l.PY 201(C 9)ho mu ota ds ele .t Sa il n. c2 e02 th2 e) sa hll apu ese spS aM cePL o- fX SM(P Pav Lla ak no ds E=   −1 0 − 0    R3p×3, (25)
 0 1 0 ∈
S tiM onP sL u- fX ferm s o frd oe ml a sr ye sted miff ae tr icen et, rroo ru .r TS hM ereP fL o- rb e,as wed e cp or ned vi ec r- t    . . . −. . . . . .   
the predicted SMPL mesh to SMPL-X using the least 0 0 1
squares method based on the point regressor provided in −
SHAPY(Choutasetal.2022).
A=[ H S E ] R3p×(s+3), (26)
SHAPY (Choutas et al. 2022) uniformly samples the SMPL-X SMPL-X ∈
S HMPL-X tem Rpl pa ×te Nm toesh rega rn ed ssp tr ho epo ss ae ms pa ledsp pa ors ie ntsm fa rt or mix b=H SMPLT SMPL −H SMPL-XT0 SMPL-X ∈R3p. (27)
SMPL-X
SMPL-X v∈ ertices T , as P = H T , Then the optimization problem is transformed into finding
SMPL-X SMPL-X SMPL-X
wherepisthesamplingnumber,andN isthevertexnumber the least squares solution of the overdetermined linear sys-
ofSMPL-Xmodel.Thentheyregisterthesamesetofpoints tem:
onSMPLmodelandcomputeH Rp×K,whereK is (cid:20) β (cid:21)
SMPL ∈ A =b. (28)
thevertexnumberofSMPLmodel. t
Givenarest-poseSMPLmeshT ,ourgoalistofinda Thesolutionis:
SMPL
shapeparameterforSMPL-Xmodel,denotedasβ so
SMPL-X (cid:20) (cid:21)
thattheregressedmeshsurfacepointsarebestaligned.The β SMPL-X =(ATA)−1ATb. (29)
problem can be written as a linear L2-norm approximation t SMPL-X
problem:
Comparedtothefittingmethodformodelconversion,our
methodismuchfasterandnotaffectedbytheinitialization
β ,t =argmin ∆P 2 (23) ofoptimization.
SMPL-X SMPL-X ∥ ∥2
β,t
(cid:40) ∆P=H T H T t F LimitationsandFutureWork
SMPL-X SMPL SMPL
where, − − . (24)
T=S β+T0 OurworkhasseverallimitationsasshowninFig.14.First,
SMPL-X SMPL-X
ShapeBoost sometimes fails to give an accurate pose esti-
mationinsevereocclusionsituations.Second,sinceShape-
BoostutilizesSMPLmodel,thediversityoftheoutputshape
wheret R3 istheglobaltranslationvector,T0 is limited by the expressiveness of SMPL. For example,
RN×3 is ∈ the template mesh of SMPL-X moS dM elP ,L-X an∈ d ShapeBoostdoesnotprovideaccurateshapeestimationfor
S R3N×s is the weight matrix that constructs the childrenorpeoplewithamuscularbody.Thefirstlimitation
SMPL-X
∈
rest-poseSMPL-Xmeshfromshapeparameters.sisthedi- canbemitigatedbyusingamorerobustposeestimational-
mension of the shape parameter used in SMPL-X. In our gorithm. The second limitation can be alleviated by using
implementation, we use s = 10. This linear optimization other body models such as SMIL (Hesse et al. 2018) and
problemcanbesolvedanalytically. STAR(Osman,Bolkart,andBlack2020).50 100
0 0
90 60
0 0
65 110
0 0
50 80
0 0
90 80
0 0
90 80
0 0
(a) Sengupta et al. (b) SHAPY (c) Ours (a) Sengupta et al. (b) SHAPY (c) Ours
Figure 11: Qualitative results on SSP-3D (Sengupta, Budvytis, and Cipolla 2020) and HBW (Choutas et al. 2022) datasets.
Fromlefttoright:Inputimage,(a)Senguptaetal.(Sengupta,Budvytis,andCipolla2021a)results,(b)SHAPY(Choutasetal.
2022)results,and(c)Ourresults.Warmercolorsmeanhigherper-vertexerror.ExperimentsonSSP-3DdatasetusePVE-T-SC
metric,andexperimentsonHBWdatasetuseP2P metric.Unit:mm.
20K
G QualitativeResults References
We provide additional qualitative results. In Fig. 11, we Agarwal, A.; and Triggs, B. 2005. Recovering 3D human
compare the results predicted by different models, and vi- posefrommonocularimages. TPAMI,28(1):44–58.
sualizetheirper-vertexerrorusingtheheatmap.InFig.12a, Bertiche, H.; Madadi, M.; and Escalera, S. 2020.
we compare the results of Sengupta et al. (Sengupta, Bud- CLOTH3D: clothed 3d humans. In ECCV, 344–359.
vytis,andCipolla2021a)andShapeBoost,andinFig.12b, Springer.
wecomparetheresultsofSHAPY(Choutasetal.2022)and
Black, M. J.; Patel, P.; Tesch, J.; and Yang, J. 2023. BED-
ShapeBoost.ThemethodproposedbySenguptaetal.often
LAM: A Synthetic Dataset of Bodies Exhibiting Detailed
failsonimageswithpeopleinthickclothes,andSHAPYof-
LifelikeAnimatedMotion.InProceedingsoftheIEEE/CVF
tenfailsonextremebodyshapes.Incomparison,ourmethod
Conference on Computer Vision and Pattern Recognition,
isaccuratelyalignedtotheinputimageindifferentscenar-
8726–8737.
ios.InFig.13,wevisualizetheresultsofourmodelonin-
the-wildimages.TheresultsshowthatShapeBoostcanhan- Bogo,F.;Kanazawa,A.;Lassner,C.;Gehler,P.;Romero,J.;
dle images with hard pose, thick clothes and extreme body andBlack,M.J.2016.KeepitSMPL:Automaticestimation
shapes.
of3Dhumanposeandshapefromasingleimage.InECCV.
Choi, H.; Moon, G.; and Lee, K. M. 2020. Pose2mesh:
Graph convolutional network for 3d human pose and mesh
recoveryfroma2dhumanpose. InComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23– Li, J.; Bian, S.; Xu, C.; Chen, Z.; Yang, L.; and Lu, C.
28,2020,Proceedings,PartVII16,769–787.Springer. 2023b. HybrIK-X:HybridAnalytical-NeuralInverseKine-
Choutas,V.;Mu¨ller,L.;Huang,C.-H.P.;Tang,S.;Tzionas, matics for Whole-body Mesh Recovery. arXiv preprint
D.;andBlack,M.J.2022. Accurate3Dbodyshaperegres- arXiv:2304.05690.
sionusingmetricandsemanticattributes. InCVPR,2718– Li, J.; Bian, S.; Xu, C.; Liu, G.; Yu, G.; and Lu, C. 2022a.
2728. D&D: Learning Human Dynamics from Dynamic Camera.
Choutas, V.; Pavlakos, G.; Bolkart, T.; Tzionas, D.; and InECCV.
Black, M. J. 2020. Monocular expressive body regression Li,J.;Xu,C.;Chen,Z.;Bian,S.;Yang,L.;andLu,C.2021.
throughbody-drivenattention. InECCV,20–40.Springer. Hybrik:Ahybridanalytical-neuralinversekinematicssolu-
Corona,E.;Pons-Moll,G.;Alenya`,G.;andMoreno-Noguer, tion for 3d human pose and shape estimation. In CVPR,
F.2022. LearnedVertexDescent:ANewDirectionfor3D 3383–3393.
HumanModelFitting. InECCV. Li,Z.;Liu,J.;Zhang,Z.;Xu,S.;andYan,Y.2022b. Cliff:
Dwivedi, S. K.; Athanasiou, N.; Kocabas, M.; and Black, Carrying location information in full frames into human
M. J. 2021. Learning to regress bodies from images using poseandshapeestimation. InECCV,590–606.Springer.
differentiablesemanticrendering. InICCV,11250–11259. Liang, J.; and Lin, M. C. 2019. Shape-aware human pose
Fang,H.-S.;Sun,J.;Wang,R.;Gou,M.;Li,Y.-L.;andLu, andshapereconstructionusingmulti-viewimages.InICCV,
C. 2019. Instaboost: Boosting instance segmentation via 4352–4362.
probabilitymapguidedcopy-pasting. InICCV,682–691.
Lin, K.; Wang, L.; and Liu, Z. 2021a. End-to-end human
Guan, P.; Weiss, A.; Balan, A. O.; and Black, M. J. 2009. poseandmeshreconstructionwithtransformers. InCVPR,
Estimating human shape and pose from a single image. In 1954–1963.
ICCV,1381–1388.IEEE.
Lin,K.;Wang,L.;andLiu,Z.2021b. Meshgraphormer. In
Hesse, N.; Pujades, S.; Romero, J.; Black, M. J.; Boden- ICCV,12939–12948.
steiner,C.;Arens,M.;Hofmann,U.G.;Tacke,U.;Hadders-
Lin,T.-Y.;Maire,M.;Belongie,S.;Hays,J.;Perona,P.;Ra-
Algra, M.; Weinberger, R.; et al. 2018. Learning an infant
manan, D.; Dolla´r, P.; and Zitnick, C. L. 2014. Microsoft
bodymodelfromRGB-Ddataforaccuratefullbodymotion
coco:Commonobjectsincontext. InECCV.
analysis. InMICCAI,792–800.Springer.
Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.; and
Hoffmann, D. T.; Tzionas, D.; Black, M. J.; and Tang, S.
Black, M. J. 2015. SMPL: A skinned multi-person linear
2019. Learning to train with synthetic humans. In Pattern
model. TOG.
Recognition,609–623.Springer.
Ma,X.;Su,J.;Wang,C.;Zhu,W.;andWang,Y.2023. 3D
Ionescu, C.; Papava, D.; Olaru, V.; and Sminchisescu, C.
HumanMeshEstimationfromVirtualMarkers. InProceed-
2013. Human3. 6m: Large scale datasets and predictive
ingsoftheIEEE/CVFConferenceonComputerVisionand
methods for 3d human sensing in natural environments.
PatternRecognition,534–543.
TPAMI.
Mahmood, N.; Ghorbani, N.; Troje, N. F.; Pons-Moll, G.;
Joo,H.;Neverova,N.;andVedaldi,A.2021.Exemplarfine-
andBlack,M.J.2019. AMASS:Archiveofmotioncapture
tuning for 3d human model fitting towards in-the-wild 3d
assurfaceshapes. InICCV,5442–5451.
humanposeestimation. In3DV.
Moon,G.;andLee,K.M.2020.I2l-meshnet:Image-to-lixel
Kanazawa, A.; Black, M. J.; Jacobs, D. W.; and Malik, J.
prediction network for accurate 3d human pose and mesh
2018. End-to-end recovery of human shape and pose. In
estimation from a single rgb image. In ECCV, 752–768.
CVPR.
Springer.
Kocabas,M.;Athanasiou,N.;andBlack,M.J.2020. VIBE:
Videoinferenceforhumanbodyposeandshapeestimation. Muller, L.; Osman, A. A.; Tang, S.; Huang, C.-H. P.; and
InCVPR. Black, M. J. 2021. On self-contact and human pose. In
CVPR,9990–9999.
Kocabas,M.;Huang,C.-H.P.;Hilliges,O.;andBlack,M.J.
2021. PARE: Part attention regressor for 3D human body Omran, M.; Lassner, C.; Pons-Moll, G.; Gehler, P.; and
estimation. InICCV,11127–11137. Schiele,B.2018. Neuralbodyfitting:Unifyingdeeplearn-
ingandmodelbasedhumanposeandshapeestimation. In
Kolotouros, N.; Pavlakos, G.; Black, M. J.; and Daniilidis,
3DV,484–494.IEEE.
K.2019. Learningtoreconstruct3Dhumanposeandshape
viamodel-fittingintheloop. InICCV. Osman, A. A.; Bolkart, T.; and Black, M. J. 2020. Star:
Sparsetrainedarticulatedhumanbodyregressor. InECCV,
Kolotouros,N.;Pavlakos,G.;andDaniilidis,K.2019. Con-
598–613.Springer.
volutional mesh regression for single-image human shape
reconstruction. InCVPR,4501–4510. Patel, P.; Huang, C.-H. P.; Tesch, J.; Hoffmann, D. T.; Tri-
Li, J.; Bian, S.; Liu, Q.; Tang, J.; Wang, F.; and Lu, C. pathi,S.;andBlack,M.J.2021. AGORA:Avatarsingeog-
2023a. NIKI: Neural Inverse Kinematics with Invertible raphy optimized for regression analysis. In CVPR, 13468–
NeuralNetworksfor3DHumanPoseandShapeEstimation. 13478.
In Proceedings of the IEEE/CVF Conference on Computer Pavlakos,G.;Choutas,V.;Ghorbani,N.;Bolkart,T.;Osman,
VisionandPatternRecognition,12933–12942. A.A.;Tzionas,D.;andBlack,M.J.2019. Expressivebodycapture: 3d hands, face, and body from a single image. In
CVPR.
Pavlakos, G.; Zhu, L.; Zhou, X.; and Daniilidis, K. 2018.
Learningtoestimate3Dhumanposeandshapefromasingle
colorimage. InCVPR,459–468.
Pumarola,A.;Sanchez-Riera,J.;Choi,G.;Sanfeliu,A.;and
Moreno-Noguer,F.2019. 3dpeople:Modelingthegeometry
ofdressedhumans. InICCV,2242–2251.
Ruiz, N.; Bellver, M.; Bolkart, T.; Arora, A.; Lin, M. C.;
Romero,J.;andBala,R.2022. Humanbodymeasurement
estimationwithadversarialaugmentation. In2022Interna-
tionalConferenceon3DVision(3DV),219–230.IEEE.
Sarkar, R.; Dave, A.; Medioni, G.; and Biggs, B. 2023.
ShapeofYou:Precise3Dshapeestimationsfordiversebody
types.InProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition,3519–3523.
Sengupta,A.;Budvytis,I.;andCipolla,R.2020. Synthetic
Training for Accurate 3D Human Pose and Shape Estima-
tion in the Wild. In British Machine Vision Conference
(BMVC).
Sengupta, A.; Budvytis, I.; and Cipolla, R. 2021a. Hier-
archical kinematic probability distributions for 3D human
shapeandposeestimationfromimagesinthewild.InICCV,
11219–11229.
Sengupta, A.; Budvytis, I.; and Cipolla, R. 2021b. Proba-
bilistic3Dhumanshapeandposeestimationfrommultiple
unconstrainedimagesinthewild. InCVPR,16094–16104.
Tung, H.-Y.; Tung, H.-W.; Yumer, E.; and Fragkiadaki, K.
2017. Self-supervisedlearningofmotioncapture. NeurIPS,
30.
Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer, E.;
Laptev, I.; and Schmid, C. 2018. Bodynet: Volumetric in-
ferenceof3dhumanbodyshapes. InECCV,20–36.
Varol, G.; Romero, J.; Martin, X.; Mahmood, N.; Black,
M.J.;Laptev,I.;andSchmid,C.2017. Learningfromsyn-
thetichumans. InCVPR,109–117.
vonMarcard,T.;Henschel,R.;Black,M.J.;Rosenhahn,B.;
and Pons-Moll, G. 2018. Recovering accurate 3d human
poseinthewildusingimusandamovingcamera.InECCV.
Wang, J.; Sun, K.; Cheng, T.; Jiang, B.; Deng, C.; Zhao,
Y.; Liu, D.; Mu, Y.; Tan, M.; Wang, X.; et al. 2020. Deep
high-resolution representation learning for visual recogni-
tion. TPAMI,43(10):3349–3364.
Weitz, A.; Colucci, L.; Primas, S.; and Bent, B. 2021. In-
finiteForm:Asynthetic,minimalbiasdatasetforfitnessap-
plications. arXivpreprintarXiv:2110.01330.
Zhang,H.;Tian,Y.;Zhang,Y.;Li,M.;An,L.;Sun,Z.;and
Liu, Y. 2022. PyMAF-X: Towards well-aligned full-body
model regression from monocular images. arXiv preprint
arXiv:2207.06400.(a) Sengupta et al. (b) Ours (a) Sengupta et al. (b) Ours
(a)QualitativecomparisonbetweenSenguptaetal.(Sengupta,Budvytis,andCipolla2021a)andShapeBoost(Ours).Fromlefttoright:Input
image,(a)Senguptaetal.(Choutasetal.2022)results,and(b)Ourresults.Ourmethodisbetteralignedtotheinputespeciallyforimagesof
peopleinthickclothes.
(a) SHAPY (b) Ours (a) SHAPY (b) Ours
(b) Qualitative comparison between SHAPY (Choutas et al. 2022) and ShapeBoost (Ours). From left to right: Input image, (a)
SHAPY(Choutasetal.2022)results,and(b)Ourresults.Ourmethodisbetteralignedtotheinputespeciallyforimagesofchubbypeople.
Figure12:Qualitativecomparisonwithpreviousmethods.Figure13:QualitativeresultsonCOCO(Linetal.2014)dataset.Ourmethodspredictsaccurateresultsforimagesofpeople
withhardpose,inocclusion,wearinglooseclothes,andwithanextremebodyshape.Figure14:FailurecasesofShapeBoost.Asshownintheleft2columns,thediversityofShapeBoostoutputislimitedbythe
expressivenessofSMPL,makingithardtomodelthebodyshapesofchildren.Asshownintheright2columns,ShapeBoost
sometimesfailstogiveanaccurateposeestimationinsevereocclusionsituations.