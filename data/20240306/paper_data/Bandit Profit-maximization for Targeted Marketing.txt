Bandit Profit-Maximization for Targeted Marketing
Joon Suk Huh1, Ellen Vitercik2,3, and Kirthevasan Kandasamy1
1Computer Science Department, UW–Madison
2Management Science and Engineering Department, Stanford University
3Computer Science Department, Stanford University
Abstract
We study a sequential profit-maximization problem, optimizing for both price and ancillary
variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary
sequenceofmultipledemandcurves,eachdependentonadistinctancillaryvariable,butsharing
the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to
sell a product over multiple markets. The firm may invest different marketing expenditures for
different markets to optimize customer acquisition, but must maintain the same price across all
markets. Moreover, markets may have heterogeneous demand curves, each responding to prices
and marketing expenditures differently. The firm’s objective is to maximize its gross profit, the
total revenue minus marketing costs.
Our results are near-optimal algorithms for this class of problems in an adversarial bandit
setting, where demand curves are arbitrary non-adaptive sequences, and the firm observes only
noisy evaluations of chosen points on the demand curves. We prove a regret upper bound of
O(cid:101)(cid:0) nT3/4(cid:1) andalowerboundofΩ(cid:0) (nT)3/4(cid:1)
formonotonicdemandcurves,andaregretboundof
Θ(cid:101)(cid:0) nT2/3(cid:1)
fordemandscurvesthataremonotonicinpriceandconcaveintheancillaryvariables.
1 Introduction
Thedesignofrevenue-maximizingmechanismsisoneofthemostimportantproblemsineconomics.
This problem is appealingly simple in the single-item setting: it boils down to choosing a revenue-
maximizing price p. When the market is characterized by a demand curve d(p), as in Figure 1a,
then the revenue is p · d(p). Thus, finding the revenue-maximizing price under a known, fixed
demandcurveisstraightforward. Inreality, however, afirm(seller)willhaveonlynoisy, incomplete
information about the demand curve. As a result, an explosion of research has studied the more
realisticsettingwherethedemandcurveisunknown,andthefirmmustlearntheagents’willingness
to pay from repeated interactions (Kleinberg and Leighton, 2003; Besbes and Zeevi, 2009; den Boer
and Zwart, 2014; Besbes and Zeevi, 2015; Cheung et al., 2017; Misra et al., 2019; Den Boer, 2015,
e.g.,).
However, this body of literature at the intersection of machine learning and mechanism design has
not taken into account a critical lever of power that the firm has in many markets: the firm can
shift the demand curve through advertising as illustrated in Figure 1b, resulting in higher revenue.
This phenomenon is known as the advertising elasticity of demand (Png, 2022; Choi et al., 2020).
Moreover, different markets respond differently to advertising and/or price. For instance, adver-
tising for a luxury car will likely have more impact in affluent markets, while for non-luxury cars,
advertising may be more impactful in emerging markets. Thus, if a firm discovers which markets
1
4202
raM
3
]GL.sc[
1v16310.3042:viXraSales d(p) Sales d(c,p) Sales d i(c i,p)
Market 1
Market 2
Advertising
c⋆
expenditure 1
c⋆
c⋆
2
p p p
p⋆ Price p⋆ Price p⋆ Price
(a) Single market with a fixed (b)Singlemarketwithashifting (c) Multiple markets with shift-
demand curve. demand curve. ing demand curves.
Figure 1: A landscape of profit-maximization problems. In (a), we wish to maximize revenue under
some demand curve d(p), which boils down to choosing a price p which maximizes p · d(p). In
(b), advertising can shift the demand curve, and the goal is to maximize the profit p·d(c,p)−c,
i.e., revenue minus advertising cost c. The setting of this work is illustrated in (c), where we have
n different markets, and we wish to choose advertising costs c ,...,c and a common price p to
1 n
(cid:80)
maximize the total profit p·d (c ,p)−c . The demand curves are unknown to a priori, and we
i i i i
are interested in learning the optimal price and costs via repeated interactions.
aremoreprofitableovertime,theycanconcentratetheiradvertisingexpendituresonthosemarkets,
as illustrated in Figure 1c. In most real-world settings, the firm cannot practice non-anonymous
(discriminatory) pricing, i.e., it must choose a common price for all markets; otherwise, buyers
could buy across markets to pay a lower price. Hence, the firm faces a complex, multi-dimensional
profit maximization problem, where they must optimize the advertising costs in each market and
the common price to simultaneously maximize revenue while minimizing advertising costs.
In this work, we formalize this problem with a bandit-learning model where the firm interacts
with buyers from n local markets over a series of T timesteps. On each round t ∈ [T], the firm
choosesacommon, anonymouspricep , andforeachmarketi ∈ [n], itchoosesanancillarycostc ,
t t,i
representing, for example, advertising spend in the market. Each local market i ∈ [n] has demand
d , which is a function of the price p and the ancillary cost c . Thus, the firm’s revenue on round
t,i t,i
t is
(cid:80)n
p d (c ,p )−c . The goal is to minimize regret, which is the difference between the
i=1 t t,i t,i t t,i
firm’s cumulative revenue and that of the optimal price and ancillary costs in hindsight.
Variants. In Section 5, we also present variants of this targeted marketing problem for which the
above framework applies. The first is the subscription problem, where new customers subscribe to a
service on each round t, and may choose to stay for future rounds. Hence, there is a memory effect
where the demands at round t can depend on past marketing costs and prices. In each round, there
is an influx of new customers depending on the current price level and marketing expenditures, and
a fraction of existing existing canceling their membership. The firm wishes to maximize its revenue
by maintaining a large active customer pool. The second variant is the promotional credit problem,
where a service provider (e.g., cloud services) wishes to attract users by giving promotional credits
to users depending on whether they belong to a population segment (e.g., students, developers).
The third variant is a profit-maximizing A/B test, where the firm conducts a series of experiments
(e.g., presenting one of k webpages) over n segments of the population in conjunction with non-
discriminatory pricing to maximize their profit.
21.1 Our contributions
Our main contributions in this work are as follows.
1. We formalize profit maximization in the adversarial bandit setting where the firm only ob-
servestherealizeddemands(i.e.,sales)forthechosenpriceandmarketingcosts. Inparticular,
we study two different, yet natural assumptions on the demand curves: (1) Monotonic de-
mands (Assumption 1), where the demands for each market are monotonically increasing
in the marketing expenditure and decreasing in price. (2) Cost-concave demands (Assump-
tion 2), where the demand is a concave function of the marketing expenditure, modeling
diminishing returns in the effectiveness of the marketing campaign.
2. We provide two profit-maximizing algorithms for the targeted marketing problem, under the
two different assumptions. Our regret bounds are linear in the number of local markets, thus
avoiding the curse of dimensionality. Specifically, we prove regret bounds of
O(cid:101)(cid:0) nT3/4(cid:1)
for
monotonic demands and
O(cid:101)(cid:0) nT2/3(cid:1)
for cost-concave demands. Surprisingly, for cost-concave
demands, our regret bound matches, up to logarithmic factors, well-known upper and lower
bounds for pricing without shifting demand curves, i.e. single-item pricing (Kleinberg and
Leighton, 2003). Our theoretical results are summarized in Table 1.
3. Weprovematchingandnearly-matchingregretlowerbounds. Undermonotonicdemands, we
show any algorithm has to incur
Ω(cid:0) (nT)3/4(cid:1)
regret, meaning that our algorithm is optimal up
to a n1/4 term. For cost-concave demands, the lower bound is Ω(cid:0) nT2/3(cid:1) , which follows almost
directly from the lower bound proved by Kleinberg and Leighton (2003).
4. We show that our algorithms can be adapted to solve the aforementioned variants of the
targeted marketing problem, without modifying their internal logic. In Section 5 and Ap-
pendix C, we formalize these variants and outline our reductions.
Key challenges and insights. A key technical challenge in our problem is that the firm needs
to choose a common price across all markets. If this were not the case, i.e., if the firm could choose
a non-anonymous price p for each market i, the targeted marketing problem would reduce to n
t,i
separate, yet simple, two-variable problems: maximizing p ·d (c ,p )−c , for each market i.
t,i t,i t,i t,i t,i
Atthesametime,naivelytreatingthisasa(n+1)–dimensionalbanditoptimizationproblem,sayby
applying an algorithm such as EXP3 (Auer et al., 1995), fails to exploit the problem structure, and
leads to a regret bound that is exponential in the number of local markets n. In this work, we show
that one can carefully decompose the targeted marketing problem into n local cost optimization,
and one price optimization problem, leading to a regret bound that is linear in the number of
markets. Interestingly, this is essentially the same bound as the non-anonymous pricing case.
Algorithm and upper bound proof overview. We summarize the chain of thoughts behind
the design of our algorithms, Algorithm 1 in Section 3.1 for monotonic demands and Algorithm 2
in Section 4.1 for cost-concave demands as follows.
1. First, let us assume that the buyer is aware of the demand curves d on each round. Instead
t,i
of viewing this as an n+1 dimensional optimization problem, we can decompose the firm’s
(cid:80)
profit (p d (c ,p )−c ) as follows. For a fixed price, we can find the best costs via n
i t t,i t,i t t,i
separate one-dimensional optimization problems. We can evaluate each price in this manner
and then choose the optimal price (along with the costs). Our algorithms are designed to
3Demand types Regret bounds References
Non-shifting Kleinberg and Leighton (2003)
Θ(cid:101)(nT2/3)
demands Theorem 4.2
Monotonic Upper bound:
O(cid:0) nT3/4logT(cid:1)
Theorem 3.1, Alg. 1 (Ours)
(Assumption 1) Lower bound:
Ω(cid:0) (nT)3/4(cid:1)
Theorem 3.2 (Ours)
Cost-concave Upper bound:
O(cid:0) nT2/3logT(cid:1)
Theorem 4.1, Alg. 2 (Ours)
(Assumption 2) Lower bound:
Ω(cid:0) nT2/3(cid:1)
Theorem 4.2
Table 1: Regret bounds of bandit profit-maximization problems, where our regret is defined in (2)
Here, “Non-shifting demands” stands for the case when demand curves do not depend on ancillary
variables, which is the usual profit-maximization without marketing contexts.
exploit this decomposable structure. This decomposition into smaller optimization problems
is statistically advantageous as an algorithm needs to track significantly fewer parameters.
2. Toleveragetheabovestructureformonotonicdemands,ouralgorithm–whichbuildonEXP3–
maintains one distribution for the price; then, for each price in a discretized set, it maintains
n distributions, one for each cost. The main challenge in applying the intuition in (1) is in
the design of appropriate objectives to update these distributions. Directly using unbiased
estimators of local profits as objective functions—like in EXP3—tends to exploit too aggres-
sively. Theproblemisthattheseunbiasedestimatorstendtohavelargevariances,resultingin
potentially large (Ω(T)) regret. To address this issue, we carefully design (biased) objectives
that have lower variance and encourage exploration.
3. For cost-concave demands, we use the same decomposition, but leverage ideas from bandit
convex optimization Flaxman et al. (2005); Hazan and Levy (2014). In particular, we use a
similar update rule for the price distributions but adopt a modified version of the kernelized
exponential weights update rule (Bubeck et al., 2017) for cost distributions.
Regret lower bound proof overview. We prove a regret lower bound of
Ω(cid:0) (nT)3/4(cid:1)
for the
targeted marketing problem under Assumption 1, where the demands are monotonically increasing
(decreasing) with respect to the marketing expenditure (the common price). Following standard
approaches (Bubeck et al., 2012; Lattimore and Szepesv´ari, 2020), we reduce the adversarial bandit
problem to the stochastic setting and apply hypothesis testing arguments to construct the lower
bound. Themainchallengeisinthedesignofalternativesforhypothesistestingthatarestatistically
indistinguishable but have large differences in profits. Due to the structure of our problem, we
requirethedemandcurvesinouralternativestobemonotonicallydecreasinginprice,monotonically
increasing in cost, and be coupled via the common price.
1.2 Related work
Dynamic pricing. Bandit profit-maximization without shifting demands has long been studied
in the context of dynamic pricing (Den Boer, 2015). One of the seminal works in this area is
Kleinberg and Leighton (2003), who studied a setting where a single new buyer appears at each
round who will purchase the product if the price p is lower than her value v . This conforms to
t t
our setting, where the demand function is d (p) = 1[v ≥ p]. They proved a regret lower bound of
t t
Ω(T2/3), and showed that a straightforward adaptation of EXP3 achieves O(cid:101)(T2/3) regret.
4The setting of Kleinberg and Leighton (2003) was later generalized to more general classes of non-
parametric demands (Besbes and Zeevi, 2009; Wang et al., 2021; Cheung et al., 2017; Misra et al.,
2019;PerakisandSinghvi,2023)andparametricdemandmodels(KeskinandZeevi,2014;denBoer
and Zwart, 2014; Besbes and Zeevi, 2015; Javanmard, 2017; Javanmard and Nazerzadeh, 2019; Xu
and Wang, 2021). However, except for Kleinberg and Leighton (2003), all work assumed that the
underlying distribution of demands is fixed over time. In this work, allow the underlying demand
distributions to be any sequence over time, reflecting drifts that may occur in real markets.
Bandits for marketing. Besides the pricing problem, various marketing problems have been
studied in the bandit setting. Schwartz et al. (2017) studied how the effectiveness of online adver-
tisementscanbeenhancedbythereal-worlddeploymentofamulti-armedbanditalgorithm. Urban
et al. (2014) reformulated the traditional A/B test and suggested an online algorithm for A/B test
in the profit-maximizing scenario. Sawant et al. (2018) developed a contextual multi-armed bandit
algorithm for marketing by harnessing the underlying causal effects of marketing. Finally, it is
worth mentioning that there is a series of works on morphing websites for customer acquisition
(Hauser et al., 2009, 2014; Urban et al., 2014; Liberali and Ferecatu, 2022), related to one of the
variations (Profit-maximizing A/B test) of the targeted marketing problem.
To the best of our knowledge, no prior work studies the targeted marketing problem akin to
our setting. However, it is worth mentioning that in some recent work, Jain et al. (2023) apply
Thompson sampling to a pricing problem, while also optimizing for additional variables such as
promotions and advertising. There are three clear differences between this work and ours. First,
while their focus is on personalized (non-anonymous) prices, our focus is on anonymous prices; in
our setting, this couples the local markets making the problem challenging. Second, their work is
in the stochastic setting under a parametric model, whereas we are in an adversarial setting under
a non-parametric model. Third, unlike them, our focus is on developing efficient algorithms that
exploit the decomposable structure of the problem.
2 Problem setting
In this section, we formally describe the problem setting.
Notation. For any n ∈ N, let [n] := {1,...,n}. Let 1[·] be the indicator function. For any set
S measurable in a probability space, let ∆(S) denote the set of probability distributions over S.
For a compact set S ⊆ Rm or a finite set S, let Uniform(S) denote the uniform distribution over S.
Unless the scope is explicitly specified, any expectation E is taken over all randomness.
Problem setting. Our online learning problem is formulated as follows. Let there be n local
markets indexed by i ∈ [n]. On each round, a firm (seller) chooses (c ,p ) := (c ,...,c ,p ) ∈
t t t,1 t,d t
[0,1]n+1, where p denotes the common price on round t and c denotes the marketing expenditure
t t,i
for market i. At the end of the round, the firm observes the local demands (e.g., volume of sales)
d := (d ,...,d ) ∈ [0,1]n in each market, which depend on the chosen (c ,p ). By normalizing
t t,1 t,n t t
prices, marketing expenditures, and demands, we assume that their ranges are in [0,1].
Environment. Anenvironmentisasequenceofmappings{D t} t∈N, chosenpossiblybyanoblivious
adversary. Here D : [0,1]n+1 → ∆([0,1]n) maps the chosen costs and price to a distribution over
t
demands. Ifthefirmchoosescostsandprice(c ,p )onroundt, therealizeddemandsd := {d }n
t t t t,i i=1
are simply random variables whose joint distribution is D (c ,p ).
t t t
5We assume that the expected demand in market i on round t depends only on the price p and the
t
marketing expenditure c for that market1. To state this explicitly, let c := (c ,...,c ,...,c )
t,i t t,1 t,i t,d
and c′ := (c′ ,...,c ,...,c′ ) be two sets of marketing costs that differ in all but the ith market.
t t,1 t,i t,d
We than have E [d ] = E [d ] for all such c and c′ and all p . We will denote
dt∼Dt(ct,pt) t,i dt∼Dt(c′ t,pt) t,i t t t
this expected value by d (c ,p ) where we view d : [0,1]2 → [0,1] as a function that maps the
t,i t,i t t,i
costforamarketandthepricetotheexpecteddemandforthatmarket. Notethatonlytherealized
values {d } and not the expected values d (c ,p ) are revealed at the end of the round.
t,i i∈[n] t,i t,i t
Algorithm. At the beginning of round t, the firm has the history of previous prices, costs, and
observed demands {(c ,p ,d )}t−1. An algorithm for the firm can be viewed as a map from this
τ τ τ τ=1
history to a distribution q ∈ ∆([0,1]n+1) over the n marketing expenditures and price; then the
t
firm samples (c ,p ) ∼ q and executes (c ,p ).
t t t t t
Regret. The random variable representing the total profit made by the firm after T rounds is
(cid:88)
ALG := Profit , where Profit := p ·d −c . (1)
T t,i t,i t t,i t,i
t,i∈[T]×[n]
We compare the expectation of ALG with the best profit OPT achievable in expectation by the
T T
optimal fixed marketing expenditures and a common price in hindsight.
(cid:88)
OPT := sup Profit (c ,p), where Profit (c ,p) := p·d (c ,p)−c .
T t,i i t,i i t,i i i
(c,p)∈[0,1]n+1
t,i∈[T]×[n]
Hence, the algorithm’s expected regret after round T is defined as
R := OPT
−E(cid:2)
ALG
(cid:3)
, (2)
T T T
wheretheexpectationistakenovertherandomnessoftheenvironmentandthealgorithm’schoices.
WewishtoboundR overanysequenceofmappings{D } chosenbyanobliviousadversary,which
T t t
induces a sequence of expected demand functions {d } .
t,i t,i
Assumptions. We present bandit profit-maximization algorithms for the targeted marketing
problem under two assumptions on the expected demand functions {d } , which in turn imply
t,i t,i
conditions on {D } . The first assumes that d (c ,p) is monotonic with respect to c and p. This
t t t,i i i
captures the natural intuition that the demand increases with marketing and decreases with price.
Assumption 1 (Monotonic demands). For each t ∈ [T] and i ∈ [n], the expected demand
function d (c ,p) is non-decreasing in c and non-increasing in p.
t,i i i
Another natural assumption is that d is concave with respect to c . In other words, demand
t,i i
exhibits diminishing returns as we increase marketing costs. While it is natural to assume that the
demand is also non-decreasing with these costs, it is not necessary for our analysis.
Assumption 2 (Cost-concave demands). For each t ∈ [T] and i ∈ [n], the expected demand
function d (c ,p) is concave in c and non-increasing in p.
t,i i i
In Section 3, we present a no-regret algorithm, upper bound, and lower bound for Assumption 1.
In Section 4, we do the same for Assumption 2.
1This condition is implied by, albeit weaker than the conditional independence condition d ⊥d |c ,p for any
t,i t,j t t
two markets i,j ∈[n]. It is also considerably weaker than assuming that the demand d for market i on round t is
t,i
a deterministic function of c and p , where {d } are chosen adversarially.
t,i t t,i t,i
6Algorithm 1: Algorithm for monotonic demands
1 Inputs: learning rate η > 0, bias control parameter γ > 0, discretization size K ∈ N.
2 Let I K := {0,K−1,2K−1,...,1} ⊂ [0,1] be a discretized unit interval.
3 q 1,0(p) ← Uniform(I K), q 1,i(c i|p) ← Uniform(I K) for each i ∈ [n], p ∈ I K.
4 for t = 1,...,T do
5 Sample p t ∼ q t,0. Then sample c t,i ∼ q t,i(·|p t) for each i ∈ [n].
6 From observations {d t,i} i∈[n] compute {ℓ t,i} i∈[n] according to (3).
7 For each i ∈ [n],p ∈ I K, update cost distributions
(cid:16) (cid:17)
q t+1,i(c i|p) ∝ q t,i(c i|p)·exp −ηf(cid:98)t,i(c i,p) , where f(cid:98)t,i(c i,p) is defined in (5).
(cid:16) (cid:17)
8 For each p ∈ I K, update price distribution q t+1,0(p) ∝ q t,0(p)·exp −η(cid:98)h t(p) , where
(cid:98)h t(p) is defined in (6).
9 end
3 Targeted marketing with monotonic demands
In this section, we first present our algorithm in Section 3.1. In Section 3.2, we upper bound its
regret, and in Section 3.3, we prove a nearly-matching lower bound on the regret.
3.1 Algorithm for monotonic demands
EXP3 review. We begin with a brief review of the EXP3 algorithm for adversarial multi-armed
bandits (Auer et al., 2002). A learner sequentially chooses one of K arms over a series of rounds.
Pulling arm i on round t incurs loss ℓ (i). The learner only observes the loss ℓ (a ) for the arm
t t t
a pulled on round t. The goal is to minimize regret with respect to the best arm in hindsight:
t
(cid:80)T
ℓ (a )−min
(cid:80)T
ℓ (i). EXP3 maintains a distribution q = (q (1),...,q (K)) ∈ ∆([K])
t=1 t t i∈[K] t=1 t t t t
overtheK arms,andsamplesanarma fromq . Attheendoftheround,itupdatesthedistribution
t t
as follows: q t+1(i) ∝ q t(i)·exp(−ηℓ(cid:98)t(i)), where ℓ(cid:98)t(i) := 1[a
t
= i]ℓ t(i)/q t(a t). This update reduces
the probability that arm a is selected in a future round by an amount depending on the observed
t
loss ℓ (a ) and the probability q (a ) of selecting a . If an arm consistently achieves large losses, it
t t t t t √
(cid:0) (cid:1)
will be heavily discounted and be chosen rarely in the future. EXP3 achieves O(cid:101) KT regret.
A straightforward, yet inefficient solution to the targeted marketing problem is to discretize the
price and cost space and apply EXP3. If the size of the discretization is K along each dimension,
then there are Kn+1 arms, so EXP3’s regret when competing with the best price and costs in the
√
(cid:0) (cid:1)
discretization is O(cid:101) Kn+1T . If we additionally account for the regret due to discretization, which
(cid:0) n+2(cid:1)
is O(T/K), and optimize for K, the regret is O(cid:101) Tn+3 , which scales poorly with n.
Our method. To improve beyond the above approach, we exploit our problem’s structure and
decompose the problem into several simpler problems. To illustrate, assume the firm knows the
expected demand curves {d } . To optimize the expected profit
(cid:80)n
pd (c ,p)−c over the
t,i i∈[n] i=1 t,i i i
costs {c } and price p, we can fix a candidate price p and optimize pd (c ,p)−c for each c .
i i∈[n] i i i i
We can repeat this for each p and output the optimal price and the corresponding costs. While the
demand curves {d } are unknown in the bandit setting, our algorithm leverages this intuition.
t,i i∈[n]
Todescribeouralgorithm(Algorithm1), letK ∈ Nbethediscretizationsizewewillchooseshortly.
Let I := {0,K−1,2K−1,...,1} be a uniformly discretized unit interval of size K. We maintain
K
7a price distribution q ∈ ∆(I ), and cost distributions q (·|p) ∈ ∆(I ) for each p ∈ I and
t,0 K t,i K K
i ∈ [n]. In this approach, we need to track K parameters for q , and K parameters for q (·|p) for
t,0 t,i
each i ∈ [n] and p ∈ I , leading to a total of K+nK2 parameters. This is significantly fewer than
K
the naive application of EXP3 explained above, where we would need to track Kn+1 parameters.
On each round t, in line 5 of Algorithm 1, a price p ∈ I is first sampled from q (·). We then
t K t,0
sample the cost c for each market i ∈ [n] from q (·|p ). Finally, the algorithm updates q to
t,i t,i t t,i
q for i ∈ {0}∪[n] using the observed demand {d } .
t+1,i t,i i∈[n]
Loss functions. We first define the normalized (observed) losses on round t, which we will use
in updating the distributions q ,q . We have:
t,0 t,i
1 (cid:88)
ℓ := (1−Profit ), ℓ := ℓ , (3)
t,i t,i t t,i
2
i∈[n]
Here, ℓ is the observed loss for market i on round t, while ℓ is the total loss in round t. By
t,i t
normalizing, we ensure ℓ , ℓ (·,·) ∈ [0,1] and ℓ , ℓ (·,·) ∈ [0,n], a technical condition required for
t,i t,i t t
the updates. Both ℓ and ℓ can be computed as the realized demands d are observed (see (1)).
t,i t t,i
For what follows, we will also find it useful to define the true losses ℓ (c ,p) and ℓ (c,p) below.
t,i i t
Note that ℓ and ℓ are unknown, since our observations {d } and hence {ℓ } are stochastic, but
t,i t t,i i t,i
more importantly since we observe realized d ,ℓ values at our chosen (c ,p ). We have:
t,i t,i t,i t
1 (cid:0) (cid:1) (cid:88)
ℓ (c ,p) := 1−Profit (c ,p) , ℓ (c,p) := ℓ ((c ,...,c ),p) := ℓ (c ,p). (4)
t,i i t,i i t t 1 n t,i i
2
i∈[n]
We now present the loss functions f(cid:98)t,i and (cid:98)h
t
used in lines 7 and 8 to update the distributions. We
first describe them, and then later motivate our design. Let γ > 0 be a bias control parameter
whose value we will specify later. First, f(cid:98)t,i serves as an estimator for ℓ t,i, and is used to update
q :
t,i
ℓ 1[c = c ,p = p]
t,i t,i i t
f(cid:98)t,i(c i,p) := . (5)
q (c |p )(q (p )+γ)
t,i t,i t t,0 t
Here, c ,p ∈ I were chosen in Line 5 of Algorithm 1. A straightforward calculation reveals that,
t,i t K
as γ → 0, we have E t(cid:2) f(cid:98)t,i(c i,p)(cid:3) → ℓ t,i(c i,p) for any c i,p ∈ I K, where the expectation E
t
is with
respect to the environment’s stochasticity and the algorithm’s random choice in line 5, both on
round t. Next, consider (cid:98)h t(·) for updating the pricing distribution q t,0, which is given by,
1 (cid:18) ℓ 1[p = p](cid:19) (cid:18) 1 1 (cid:19)
t t
(cid:98)h t(p) := +η|I K| − . (6)
n q (p )+γ γ q (p)+γ
t,0 t t,0
Here, ℓ is as defined in (3). As before, a straightforward calculation reveals that, as γ → 0, we
t
have
E t(cid:2) (cid:98)h t(p)(cid:3) → (cid:88) q t,1(c 1|p)···q t,n(c n|p)ℓ t((c 1,...,c n),p),
ci∈IK∀i∈[n]
for any p ∈ I
K
(see (4)). In other words, (cid:98)h t(p) serves as an estimate for the expected loss
(cid:80)
ℓ (c ,p), when the c values are also sampled from the q distributions.
i t,i i i t,i
8Design choices. We will now motivate the design of (5) and (6). On the one hand, choosing
a small γ reduces the bias in both f(cid:98)t,i and (cid:98)h t,i. However, this also results in a potentially large
variance, as (inf q (p))−1 may be small. In particular, since we have decomposed the problem
p∈IK t,0
into smaller optimization problems, there is no “variance cancellation” effect that usually arises in
the standard EXP3 analysis (Auer et al., 2002). The role of the second term in (6), and γ in the
denominator of (5) and (6) is to induce a more favorable bias-variance trade-off in the decomposed
problem.
An alternative interpretation of this design choice is in terms of the exploration-exploitation trade-
off. For this, recall—from the overview of EXP3—that the purpose behind the update in line 8 is
to reduce the probability that the same price p would be chosen in a future round, by an amount
t
depending on the observed loss ℓ . However, this loss also depends on the costs c chosen. Hence,
t t,i
even if ℓ is large, we should not be quick to dismiss p since there may be other costs that yield a
t,i t
high profit at the same price. The loss (cid:98)h
t
ensures that we are more liberal with exploring a chosen
pricep evenifitincurredlargelosses; inparticular, asthesecondtermintheRHSof (6)isapplied
t
to all prices, and not just p , its effect is that p is not discounted as heavily as it otherwise would
t t
have. The role of γ, from a mathematical perspective, is to ensure that (cid:98)h
t
is bounded.
3.2 Regret upper bound for monotonic demands
We will now state and prove the following upper bound on the regret for Algorithm 1. Due to space
constraints, we will defer the proofs of some intermediate technical results to the Appendix.
Theorem 3.1. For any η > 0 and K ∈ N, when γ := η, the regret (2) of Algorithm 1 satisfies,
(cid:18) (cid:19)
n nT
R ≤ O nηK2T + logK + .
T
η K
By choosing K ∈ Θ(T1/4) and η ∈ Θ(T−3/4), Algorithm 1 guarantees R ∈ O(nT3/4logT).
T
Proof of Theorem 3.1. First, we introduce some notation. For a conditional distribution
q(c|p) over the costs in I , given a price p ∈ I and a function g(c,p) over I2 , let ⟨q,g⟩ :=
K K K p
(cid:80)
q(c|p)g(c,p). Similarly, for a distribution q(p) over prices in I , and a function g(p) over
c∈IK
(cid:80)
K
I , let ⟨q,g⟩ := q(p)g(p). Throughout this proof, we set γ := η as stated in the theorem,
K p∈IK
but occasionally do not cancel out factors like γ/η for convenience of exposition of Section 5.
Step 1. [Preparation] We begin with a standard regret analysis of the exponential weights
(a.k.a Hedge) algorithm (Arora et al., 2012). Algorithm 1 runs exponential weights updates over
effective losses {f(cid:98)t,i}
t∈[T]
for each i ∈ [n] and {(cid:98)h t} t∈[T]. The following lemma states regret bounds
for {f(cid:98)t,i}
t∈[T]
and {(cid:98)h t}
t∈[T]
when competing with any (c⋆ i,p⋆) ∈ I K2 . For completeness, We give its
proof, which is similar to the standard exponential weights analysis, in Appendix A.1.
Lemma 3.1. (Based on Arora et al. (2012)) For each i ∈ [n], any (c⋆,p⋆) ∈ I2 and η > 0,
i K
(cid:88) (cid:68) (cid:69) (cid:88) (cid:88) (cid:68) (cid:69) 1
q t,i,f(cid:98)t,i
p⋆
− f(cid:98)t,i(c⋆ i,p⋆) ≤ η q t,i,f(cid:98) t2
,i p⋆
+
η
log|I K|, (7)
t∈[T] t∈[T] t∈[T]
(cid:88) (cid:68) (cid:69) (cid:88) (cid:88) (cid:68) (cid:69) 1
q t,0,(cid:98)h
t
− (cid:98)h t(p⋆) ≤ η q t,0,(cid:98)h2
t
+
η
log|I K|, (8)
t∈[T] t∈[T] t∈[T]
where q , q are distributions given in Lines 7 and 8 in Algorithm 1, respectively.
t,i t,0
9The bounds in Lemma 3.1 hold uniformly over all choices of (c⋆,p⋆) ∈ I Kn+1. Moreover, f(cid:98)t,i and (cid:98)h
t,i
arerandomquantitiesastheydependonrandomvariablesrealizeduptoroundt−1. Insubsequent
steps, we will take expectations of these bounds with respect to all randomness.
Step 2. [Regret decomposition] First, using the definition of losses in (3) and (4), we will
decompose our regret R (2) as follows,
T
(cid:20) (cid:21)
(cid:88) (cid:88)
R := sup Profit (c⋆,p⋆)−E Profit
T t,i i t,i
(c⋆,p⋆)∈[0,1]n+1
t,i∈[T]×[n] t,i∈[T]×[n]
(cid:16) (cid:88) (cid:88) (cid:17)
= 2 E[ℓ ]− inf ℓ (c⋆,p⋆)
t t
(c⋆,p⋆)∈[0,1]n+1
t∈[T] t∈[T]
(cid:16) (cid:88) (cid:88) (cid:17)
= 2 E[ℓ ]− min ℓ (c⋆,p⋆)
t t
(c⋆,p⋆)∈In+1
t∈[T] K t∈[T]
(cid:16) (cid:88) (cid:88) (cid:17)
+2 min ℓ (c⋆,p⋆)− inf ℓ (c⋆,p⋆)
t t
(c⋆,p⋆)∈In+1 (c⋆,p⋆)∈[0,1]n+1
K t∈[T] t∈[T]
:= max R T(c⋆,p⋆)+R(cid:101)T(n,K). (9)
(c⋆,p⋆)∈In+1
K
Inthethirdstep, wehaveaddedandsubtractedmin (cid:80) ℓ (c⋆,p⋆). Inthefourthstep,
(c⋆,p⋆)∈In+1 t∈[T] t
(cid:16) K (cid:17)
we have defined R (c⋆,p⋆) := 2 (cid:80) E[ℓ ]−(cid:80) ℓ (c⋆,p⋆) to be the regret of our algorithm
T t∈[T] t t∈[T] t
relative to a given set of costs and prices (c⋆,p⋆) in the discretization. Moreover,
(cid:16) (cid:88) (cid:88) (cid:17)
R(cid:101)T(n,K) := 2 min ℓ t(c⋆,p⋆)− inf ℓ t(c⋆,p⋆)
(c⋆,p⋆)∈In+1 (c⋆,p⋆)∈[0,1]n+1
K t∈[T] t∈[T]
denotes the residual regret due to discretization. In the following lemma, we bound R(cid:101)T(n,K) via
a simple argument. Its proof is given in Appendix A.1.
Lemma 3.2. Under Assumption 1, R(cid:101)T(n,K) ≤ 2 KnT.
Next, we will focus on deriving a uniform upper bound for R (c⋆,p⋆) over all (c⋆,p⋆) ∈ In+1. In
T K
step 3, we will lower bound the loss (cid:80) ℓ (c⋆,p⋆) of any fixed (c⋆,p⋆), and in step 4, we will
t∈[T] t
upper bound the algorithm’s loss (cid:80) E[ℓ ].
t∈[T] t
Step 3. [Lower-bounding the comparator loss] In this step, we will prove the following
lower bound on (cid:80) ℓ (c⋆,p⋆), which we refer to as the comparator loss.
t∈[T] t
Lemma 3.3 (Comparator loss bound). For any (c⋆,p⋆) ∈ In+1,
K
1 (cid:88)
ℓ t(c⋆,p⋆) ≥
(cid:88) E(cid:104)(cid:68)
q t,0,(cid:98)h
t(cid:69)(cid:105)
−
η|I K|T
−
1
log|I K|−4η|I K|2T.
n γ η
t∈[T] t∈[T]
Proof of Lemma 3.3. We will use the results in Lemma 3.1 to prove Lemma 3.3. The following
lemma bounds some quantities in (7) in expectation.
Lemma 3.4. For any (c⋆,p⋆) ∈ In+1 and (t,i) ∈ [T]×[n],
K
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:104) (cid:105) (cid:68) (cid:69) |I |
E f(cid:98)t,i(c⋆ i,p⋆) ≤ ℓ t,i(c⋆ i,p⋆), E q t,i,f(cid:98) t2
,i p⋆
≤ E
q
t,0(p⋆K
)+γ
.
10As discussed in (5), we see that f(cid:98)t,i serves as a proxy for the unobserved ℓ t,i. The second bound
still has the expectation in the RHS as q (p⋆) depends on (random) past history {(c ,p ,ℓ )}t−1.
t,0 τ τ τ τ=1
The second bound in Lemma 3.4, may be very large due to the q term in the denominator.
t,0
Fortunately, our design of (cid:98)h t(p), specifically the second term in the RHS of (6) ensures that this
term will be canceld out. To see this more explicitly, let us take the expectations of the bound in
(7) for each i, use Lemma 3.4, and sum them over i ∈ [n]. This simple algebraic manipulation leads
to the following lemma. A detailed calculation is given in Appendix A.1.
Lemma 3.5. For any (c⋆,p⋆) ∈ In+1,
K
(cid:34) (cid:35)
E (cid:88) (cid:18) 1ℓ t1[p t = p⋆] − η|I K| (cid:19) ≤ (cid:88) 1 ℓ (c⋆,p⋆)+ 1 log|I |.
n q (p )+γ q (p⋆)+γ n t i η K
t,0 t t,0
t∈[T] t∈[T]
We see that the LHS of the above bound contains the same quantity as the RHS of the second
bound of Lemma 3.4. At the same time, the LHS of is identical to
E(cid:2)(cid:80)
t∈[T](cid:98)h
t(p⋆)(cid:3)
−η|I K|T/γ,
(see (6) for the definition of (cid:98)h t). Hence, we can rewrite the above bound as follows:
(cid:34) (cid:35)
E
(cid:88)
(cid:98)h t(p⋆) −
η|I K|T
≤
1 (cid:88)
ℓ t(c⋆,p⋆)+
1
log|I K|.
γ n η
t∈[T] t∈[T]
Next, we eliminate
E(cid:2)(cid:80)
t(cid:98)h
t(p⋆)(cid:3)
in the above by adding the above and the expectation of bound
(8), that is, E(cid:2)(cid:80) t⟨q t,0,(cid:98)h t⟩(cid:3) −E(cid:2)(cid:80) t(cid:98)h t(p⋆)(cid:3) ≤ ηE(cid:2)(cid:80) t⟨q t,0,(cid:98)h2 t⟩(cid:3) + η1 log|I K|. Hence, we obtain
(cid:34) (cid:35) (cid:34) (cid:35)
E
(cid:88) (cid:68)
q t,0,(cid:98)h
t(cid:69)
−
η|I γK|T
≤
n1 (cid:88)
ℓ t(c⋆,p⋆)+
η1
log|I K|+ηE
(cid:88) (cid:68)
q t,0,(cid:98)h2
t(cid:69)
. (10)
t∈[T] t∈[T] t∈[T]
The above inequality is almost our target bound except for E(cid:2)(cid:80) t⟨q t,0,(cid:98)h2 t⟩(cid:3) term. We can bound
this quantity as follows:
(cid:68) q t,0,(cid:98)h2 t(cid:69) ≤ (cid:88) q
t,0(p)·(cid:18)1
q[p t (=
p
)p] + η|I
γK|(cid:19)2
(as ℓ t/n ≤ 1)
t,0 t
p∈IK
(cid:32) (cid:33)
(cid:88) 1[p t = p] 2η|I K|1[p t = p] η2|I K|2
= q (p)· + +
t,0 q2 (p ) γq (p ) γ2
p∈IK t,0 t t,0 t
1 2η|I | η2|I |2 1
= + K + K ≤ +3|I |2. (as γ := η and |I |2 ≥ |I |)
q (p ) γ γ2 q (p ) K K K
t,0 t t,0 t
Hence, we have that, for each t ∈ [T]
(cid:20) (cid:21)
(cid:104)(cid:68) (cid:69)(cid:105) 1
E q t,0,(cid:98)h2
t
≤ E
q (p )
+3|I K|2 = |I K|+3|I K|2 ≤ 4|I K|2.
t,0 t
Combining the above with (10), we obtain the stated bound.
11Step 4. [Bounding the algorithm’s loss] Next, in the following bound, we upper bound the
algorithm’s loss (cid:80) E[ℓ ]. Its proof, which is purely algebraic, is given in Appendix A.1.
t∈[T] t
Lemma 3.6. For all t ∈ [T],
1 (cid:88)
E[ℓ t] ≤
(cid:88) E(cid:104)(cid:68)
q t,0,(cid:98)h
t(cid:69)(cid:105)
−
η|I K|T
+2η|I K|2T.
n γ
t∈[T] t∈[T]
Step 5. [Wrap up] Combining the lower bound of (cid:80) ℓ (c⋆,p⋆) (Lemma 3.3) and the upper
t∈[T] t
bound of (cid:80) E[ℓ ] (Lemma 3.6), gives the following bound. For all (c⋆,p⋆) ∈ In+1, we have
t∈[T] t K
(cid:20) (cid:21)
1 (cid:88) (cid:88) n
R (c⋆,p⋆) = E ℓ − ℓ (c⋆,p⋆) ≤ log|I |+6nη|I |2T.
T t t K K
2 η
t∈[T] t∈[T]
Since |I K| = K+1 and the residual regret R(cid:101)T(n,K) is at most nT/K (Lemma 3.2), we have that
(cid:16) (cid:17)
R ∈ O nηK2T + n logK + nT , as claimed.
T η K
We mention that our proof does not assume continuity of the demands {d } . However, it is
t,i t,i
straightforward to extend our analysis to non-monotonic demands that are Lipschitz continuous.
3.3 Lower bound for monotonic demands
We will now state and prove the following lower bound for the targeted marketing problem with
monotonic demands. When compared to the upper bound in Theorem 3.1, we see that while we
differ by a n1/4 term, we are tight in T up to log factors.
Theorem 3.2. Let A be an algorithm for the targeted marketing problem. Let {D } be a sequence
t t
of mappings from chosen costs and price to a distribution of demands, that satisfy the monotonicity
condition in Assumption 1. Let R (A,{D } ) be the expected regret achieved by A under {D } up
T t t t t
to round T. Then, we have the following: inf sup R (cid:0) A,{D } (cid:1) ∈ Ω((nT)3/4).
A {Dt}t T t t
Proof strategy. We will follow a standard recipe for proving lower bounds for adversarial bandit
settings (e.g., Bubeck et al. (2012)). First, note that it is sufficient to prove a regret lower bound of
Ω((nT)3/4) for a stationary stochastic environment where D does not depend on t. This is because
t
any algorithm that guarantees a regret over all non-stationary demands also guarantees the same
regretoverstationarydemands. Second,toprovealowerboundforstochasticenvironments,weuse
a standard hypothesis testing argument, where we construct a set of statistically indistinguishable
environments, but with large differences in the optimal profit.
The main challenge in following this recipe is in the construction of the alternatives. To do so, we
constructabaselineenvironmentwheretheoptimalprofitis0,andasetofalternativeenvironments,
where the optimal profit is sufficiently larger than 0. The demand curves in each alternative
is identical to the baseline except near a single cost-price tuple (c⋆,p⋆). By repeatedly playing
(c⋆,p⋆), which is optimal for this alternative environment, one can generate a large profit. By
varying (c⋆,p⋆) we generate several alternative environments.
12Setting and notations. We first construct these environments for the n = 1 case, where there
is only a single demand curve whose mean function does not depend on the round, and then
generalize these environments for n ≥ 1. We use E annotated by a subscript to denote a stationary
environment. When n = 1, a stationary environment E is a function from (c,p) ∈ [0,1]2 to a
probability distribution over demands d ∈ [0,1]. Let d(c,p) be the expectation of d given (c,p).
For any randomized algorithm A, let R (A,E) be the expected regret of A under E up to round T.
T
To define our environments, we introduce discretized intervals C and P for cost and price, respec-
tively. Let K > 0 be a positive integer and ϵ := K−1. Define C,P ⊂ [0,1] as follows:
(cid:26) (cid:27) (cid:26) (cid:27)
1 1 K −1 1 ϵ 1 ϵ 1
C = {c ,...,c } := 0, , ,..., , = 0, ,..., − , ,
0 K
2K K 2K 2 2 2 2 2
(cid:26) (cid:27) (cid:26) (cid:27)
1 1 1 1 K −1 1 1 ϵ
P = {p ,...,p } := , + ,..., + ,1 = , + ,...,1 .
0 K
2 2 2K 2 2K 2 2 2
We construct environments such that any (c,p) ∈/ C ×P achieves non-negative profit, while any
(c,p) ∈/ C×P incurs negative profit. Hence, it is sufficient to consider algorithms that only choose
from these discrete options. For what follows, we define g(c) for c ∈ [0,1] as follows:
(cid:0) (cid:1)
g(c) := min 1,⌊2cK⌋/K ,
Note that g(c) = 2c for ∀c ∈ C and g(c) < 2c otherwise.
Baseline environment. The baseline environment E is defined as follows. Given that the
base
firm chooses (c,p), the random variable d denoting the realized demand is drawn via
d := b1[v ≥ p] ∈ {0,1}. (11)
Here b is a Bernoulli random variable with mean g(c) and v ∈ P is a discrete random variable,
drawn independent from b, whose distribution is P(v ≥ p) = (2p)−1 where p ∈ P (see Figure 2).
By construction, the optimal expected profit under the baseline is zero. This can be seen via:
max Profit(c,p) = max pg(c) 1 −c = 0, with the maximum attained by any (c,p) ∈ C ×P.
c,p c,p 2p
Alternative environments. Now, we define a class of alternative environments that are statis-
tically close to the baseline environment but have a sufficiently large positive optimal profit. Our
design is such that the demand variables have the same distribution as the baseline, except near a
single cost-price tuple (c⋆,p⋆) ∈ C×P. As it will turn out, (c⋆,p⋆) will be the optimal arm for the
alternative environment. Let S ⊆ C ×P be the set of feasible optimal arms defined as follows:
(cid:26) (cid:12) (cid:27)
(cid:12) 2 9 3 4
S := (c,p) ∈ C ×P (cid:12) ≤ c ≤ , ≤ p ≤ . (12)
(cid:12) 5 20 5 5
Note that |S| ∈ Ω(K2) as the area of square defined by 2 ≤ c ≤ 9 , 3 ≤ p ≤ 4 is constant.
5 20 5 5
For each (c⋆,p⋆) ∈ S, let E be the problem environment with d := b1[v ≥ p] where b and v
(c⋆,p⋆)
are independent random variables sampled as follows given (c,p) ∈ [0,1]2.
(cid:40)
Ber(g(c)+ϵ), if (c,p) ∈ [c⋆,c⋆+ϵ/2)×(p⋆−ϵ/2,p⋆].
b ∼ (13)
Ber(g(c)), otherwise,
13𝔼[b|c] ℙ(v ≥ p)
1 1
ϵ:=K−1
c 1 p
0 21 K ⋯⋯ c⋆ K 2− K1 1 2 2 1 2 1 2+ 21 K p⋆ ⋯ 1 2+K 2− K1 1
(a) Blue indicates the bump in E[b|c] for an al- (b) Blue indicates the dip in P(v ≥ p) for an
ternative environment with optimal cost c⋆. alternative environment with optimal price p⋆.
Figure 2: Illustrations of our baseline and alternative environments. (a) The black line with dots
depicts E[b] for the baseline defined in (11), and the red line represents the function 2c. (b) The
black line with dots depicts P(v ≥ p) = E[1[v ≥ p]] for the baseline given in (11), and the red line
follows (2p)−1.
and
(cid:40)
P(v ≥ p ) = (2p +ϵ)−1, if (c,p) ∈ [c⋆,c⋆+ϵ/2)×(p⋆−ϵ/2,p⋆].
i i
v ∈ P, v ∼ (14)
P(v ≥ p ) = (2p )−1, otherwise.
i i
Figure 2a and 2b illustrate E[b|c] and P(v ≥ p) respectively for the baseline environment. We have
also shown how an alternative environment deviates around (c⋆,p⋆) in blue.
The lemma below states that environment E satisfies Assumption 1 and (c⋆,p⋆) is the optimal
(c⋆,p⋆)
arm of E which gives a positive profit, and that the profit is negative whenever (c,p) ∈
(c⋆,p⋆)
[0,1]2\C ×P. We give a proof of Lemma 3.7 in Appendix A.2.
Lemma 3.7. For any (c⋆,p⋆) ∈ S, let d(c,p) be the conditional expectation of d. Then, d(c,p)
satisfies the following in environment E .
(c⋆,p⋆)
1. For any p ∈ [0,1], d(c,p) is non-decreasing with respect to c ∈ [0,1].
2. For any c ∈ [0,1], d(c,p) is non-increasing with respect to p ∈ [0,1].
3. Profit(c⋆,p⋆) ≥ ϵ > 0.
20
4. Profit(c,p) = 0 for any (c,p) ∈ C×P \{(c⋆,p⋆)} and Profit(c,p) < 0 for any (c,p) ∈ [0,1]2\
C ×P.
Environments with multiple (n ≥ 1) markets. From the n = 1 environments defined above,
we define a n-dimensional baseline environment by repeatedly sampling n independent demand
variables. Precisely, for each t ∈ [T], let {d } be independent realizations of the demands d
t,i i∈[n]
in the baseline environment E (11). With a slight abuse of notation, we call this n-dimensional
base
baseline E . The n-dimensional alternatives are similarly constructed by repeatedly sampling n
base
independent demand variables. Precisely, for any (c⋆,p⋆) ∈ S, {d } are independent realiza-
t,i i∈[n]
tions of d in E (13), (14). Hence, the optimal arm is c = c⋆ for all i ∈ [n] and p = p⋆. As
(c⋆,p⋆) t,i t
before, we will call this n-dimensional alternative environment E .
(c⋆,p⋆)
14For what follows, we recall the Bregtagnolle-Huber inequality (Bretagnolle and Huber, 1979) which
states that for any two distributions P , P and event A, we have
1 2
1
P (A)+P (Ac) ≥ KL(P ||P ). (15)
1 2 1 2
2
We are now ready to present the proof of Theorem 3.2.
Proof of Theorem 3.2. For any bandit algorithm A, let PA and PA be the distributions of
base (c⋆,p⋆)
{d ,c ,p } when A is instantiated with E and E respectively. Let EA , EA
t,i t,i t t∈[T],i∈[n] base (c⋆,p⋆) base (c⋆,p⋆)
be the expectations with respect to PA , PA respectively.
base (c⋆,p⋆)
A key ingredient in proving lower bounds is a uniform bound on statistical indistinguishability
betweenPA andPA foranyalgorithmA. ThefollowinglemmaprovesthattheKLdivergence
base (c⋆,p⋆)
between PA and PA is bounded above by the expected number of times that A chooses a
base (c⋆,p⋆)
(near) optimal arm. Its proof uses properties of our construction and is available in Appendix A.2.
Lemma 3.8. For t ∈ [T], i ∈ [n] and (c,p) ∈ C×P, define the following indicator random variable:
χ (c,p) := 1[(c ,p ) = (c,p)].
t,i t,i t
Then, for any bandit algorithm A that chooses from Cn×P,
(cid:16) (cid:12)(cid:12) (cid:17) (cid:88)
KL PA (cid:12)(cid:12)PA ≤ 54ϵ2·EA [χ (c⋆,p⋆)].
base(cid:12)(cid:12) (c⋆,p⋆) base t,i
t,i∈[T]×[n]
For any algorithm A and stationary environment E, let R (A,E) be the expected regret of A up
T
to round T under E. Then, for any algorithm A, we have that R (A,E) ≤ sup R (A,{D } )
T {Dt}t T t t
by H¨older’s inequality. Hence, the theorem follows if inf sup R (A,E) ∈ Ω((nT)3/4).
A E T
To this end, let S ⊆ C ×P be (12). As an algorithm can choose only one set of costs and a price
on each round, we have
(cid:88)
χ (c,p) ≤ 1, for all (t,i) ∈ [T]×[n].
t,i
(c,p)∈S
Therefore, (cid:80) (cid:80) EA χ (c,p) ≤ nT. This implies for some (c⋆,p⋆) ∈ S, we have
(c,p)∈S t,i∈[T]×[n] base t,i
(cid:80) EA χ (c⋆,p⋆) ≤ nT/|S|. We will now consider the environment E for this
t,i∈[T]×[n] base t,i (c⋆,p⋆)
(c⋆,p⋆), and apply the Bregtagnolle-Huber inequality (15) over PA and PA . We have,
(c⋆,p⋆) base
(cid:32) (cid:33) (cid:32) (cid:33)
(cid:88) nT (cid:88) nT
PA χ (c⋆,p⋆) ≤ +PA χ (c⋆,p⋆) > (16)
(c⋆,p⋆) t,i 2 base t,i 2
t,i∈[T]×[n] t,i∈[T]×[n]
1 (cid:16) (cid:16) (cid:12)(cid:12) (cid:17)(cid:17)
≥ exp −KL PA (cid:12)(cid:12)PA .
2 base(cid:12)(cid:12) (c⋆,p⋆)
By Markov’s inequality, and the observation above about the environment E , the second term
(c⋆,p⋆)
in the LHS of (16) can be upper bounded as follows:
(cid:32) (cid:33)
(cid:88) nT 2 (cid:88) 2
PA χ (c⋆,p⋆) > ≤ · EA χ (c⋆,p⋆) ≤ ,
base t,i 2 nT base t,i |S|
t,i∈[T]×[n] t,i∈[T]×[n]
15Moreover, by Lemma 3.8, we have
(cid:32) (cid:33)
(cid:16) (cid:16) (cid:12)(cid:12) (cid:17)(cid:17) (cid:88) (cid:18) 54ϵ2nT(cid:19)
exp −KL PA (cid:12)(cid:12)PA ≥ exp −54ϵ2· EA [χ (c⋆,p⋆)] ≥ exp − .
base(cid:12)(cid:12) (c⋆,p⋆) base t,i |S|
t,i∈[T]×[n]
Therefore, we have the following lower bound
(cid:32) (cid:33)
(cid:88) nT 1
(cid:18) 54ϵ2nT(cid:19)
2
PA χ (c⋆,p⋆) ≤ ≥ exp − − .
(c⋆,p⋆) t,i 2 2 |S| |S|
t,i∈[T]×[n]
Finally, by item 3 of Lemma 3.7, we can lower bound the expected regret of A under E as:
(c⋆,p⋆)
(cid:32) (cid:33)
R (cid:0) A, E (cid:1) ≥ ϵnT ·PA (cid:88) χ (c⋆,p⋆) ≤ nT
T (c⋆,p⋆) 40 (c⋆,p⋆) t,i 2
t,i∈[T]×[n]
ϵnT
(cid:18) (cid:18) 54ϵ2nT(cid:19)
4
(cid:19)
≥ exp − −
80 |S| |S|
(cid:18) (cid:18) (cid:19) (cid:19)
nT nT a
≥ a · exp −a · − 3 , (as ϵ := K−1 and |S| ∈ Ω(K2))
1 K 2 K4 K2
for some a ,a ,a > 0. Finally, we obtain the stated bound by choosing K = ⌈(nT)1/4⌉.
1 2 3
4 Targeted marketing with cost-concave demands
In this section, we study the targeted marketing problem under the assumption that the expected
demands {d } are concave in c (Assumption 2). This captures settings where there are dimin-
t,i t,i t,i
ishing returns to increasing spending on marketing. we first present our algorithm in Section 4.1.
In Section 4.2, we upper bound its regret, and in Section 4.3, we provide a matching lower bound.
4.1 Algorithm for cost-concave demands
UnderAssumption2, thenormalizedlossfunctionsℓ (c ,p), definedin(3), areconvexwithrespect
t,i i
to c ∈ [0,1]. Hence, while we will use the same decomposed structure as in Algorithm 1, we will
i
borrowtechniquesfromtheliteratureonbanditconvexoptimization(Flaxmanetal.,2005;Bubeck
et al., 2017) to perform updates for the cost distributions. In particular, our updates are based on
a simplified version of the kernelized exponential weights algorithm (Bubeck et al., 2017).
In each round, our algorithm for this setting, described in Algorithm 2, samples a price p from
t
a (discrete) distribution q (·); then, it samples c from q (·|p ) for each i ∈ [n]. This is similar
t,0 t,i t,i t
to Algorithm 1. The key difference is that q (·|p), which is no longer a discrete distribution, is
t,i
derived by convolving another distribution u (·|p) with a smoothing kernel. To elaborate further,
t,i
we first begin by describing kernels.
Kernel. Forδ > 0, letI := [δ,1−δ]. AkernelisabivariatefunctionK(·,·) : I ×I → R such
δ δ δ ≥0
(cid:82)
that K(x,y)dx = 1 for all y ∈ I . This kernel induces a linear operator K : ∆(I ) → ∆(I ),
I δ δ δ
δ
which is a map from the space of distributions over I to itself. With a slight abuse of notation,
δ
we denote both by K. For any input distribution q ∈ ∆(I ), this operator outputs,
δ
(cid:90)
Kq(·) := K(·,y)q(y)dy. (17)
I
δ
16Algorithm 2: Bandit targeted marketing algorithm for cost-concave demands
1 Inputs: learning rate η > 0, bias parameter γ, kernel parameters ϵ > 0, δ ∈ (0,1),
discretization K ∈ N.
2 Let I δ := [δ,1−δ], all distributions over c i are supported on I δ.
3 Let I K := {0,K−1,2K−1,...,1} be discrete price levels.
4 q 1,0(p) ← Uniform(I K), u 1,i(c i|p) ← Uniform(I δ) for each i ∈ [n], p ∈ I K.
5 q 1,i(c i|p) = K ϵ[u 1,i(·|p)]u 1,i(c i|p) with K ϵ[·] defined in (18).
6 for t = 1,...,T do
7 Sample p t ∼ q t,0, c t,i ∼ q t,i(·|p t) for each i ∈ [n].
8 From observations {d t,i} i∈[n] compute {ℓ t,i} i∈[n] according to (3)
(cid:16) (cid:17)
9 For each i ∈ [n], u t+1,i(c i|p) ∝ u t,i(c i|p)·exp −ηf(cid:98)t,i(c i,p) , with f(cid:98)t,i(c i,p) defined in
(19).
10 For each i ∈ [n], q t+1,i(c i|p) = K ϵ[u t+1,i(·|p)]u t+1,i(c i|p), with K ϵ[·] defined in (18)
(cid:16) (cid:17)
11 Update price distributions, q t+1,0(p) ∝ q t,0(p)·exp −η(cid:98)h t(p) , with (cid:98)h t(p) defined in
(20).
12 end
We can now define the linear operator K [q] used in lines 5 and 10 of Algorithm 2. Consider any
ϵ
ϵ > 0 and distribution q ∈ ∆(I ). We first define the following kernel K [q](·,·) : I ×I → R .
δ ϵ δ δ ≥0
Denote µ := E [X]. We have,
X∼q
 1 ·1[x ∈ [min(y,µ),max(y,µ)]], if |y−µ| ≥ ϵ.
 |y−µ|





K ϵ[q](x,y) := 1 ·1[x ∈ [µ−ϵ,µ]], if |y−µ| < ϵ, µ ≥ ϵ+δ. (18)
ϵ






1 ·1[∈ [µ,µ+ϵ]], if |y−µ| < ϵ, µ < ϵ+δ.
ϵ
Inwords,if|y−µ| ≥ ϵ,K [q](·,y)isequivalenttotheuniformpdfbetweeny andµ,whileotherwise
ϵ
it is equivalent to the uniform pdf on either [µ−ϵ,µ] or [µ,µ+ϵ], whichever is contained in I .
δ
The linear operator K [q] : ∆(I ) → ∆(I ) used in Algorithm 2, is induced by the above kernel as
ϵ δ δ
shown in (17). A similar kernel was used by Bubeck et al. (2017).
Loss functions. Given some γ > 0, we define the following estimator f(cid:98)t,i(·,·) for the cost-convex
loss ℓ (·,·), defined in (4), as follows.
t,i
ℓ 1[p = p]
t,i t
f(cid:98)t,i(c i,p) := K ϵ[u t,i(·|p)](c t,i,c i), (19)
q (c |p )(q (p )+γ)
t,i t,i t t,0 t
where c ∈ I , p ∈ I are the random cost and price sampled at round t in Line 7. Compared
t,i δ t K
to f(cid:98)t,i (5) defined for Algorithm 1, the above uses a kernel function K ϵ[u t,i(·|p)](c t,i,c i) instead
of the indicator 1[c = c ]. Roughly speaking, as a function of c , K [u (·|p)](c ,c ) is zero in
t,i i i ϵ t,i t,i i
the interval bounded by c
t,i
and E c∼ut,i(·|p)[c]. Hence, so is f(cid:98)t,i(c i,p). Therefore, this choice of f(cid:98)t,i
encourages the algorithm to explore this interval in future rounds. Intuitively, as convexity is a
global property, searching over a wider region is potentially rewarding.
17As in Algorithm 1, we define a function (cid:98)h t(·) used to update q
t,0
as follows. This design follows a
(cid:80)
similar intuition to the one in (6). Letting ℓ := ℓ , we have,
t i∈[n] t,i
1 (cid:18) ℓ 1[p = p](cid:19) (cid:18) eC(cid:19)(cid:18) 1 1 (cid:19)
t t
(cid:98)h t(p) := + 3ηlog − . (20)
n q (p )+γ ϵ γ q (p)+γ
t,0 t t,0
4.2 Regret upper bound for cost-concave demands
In this section, we prove the following upper bound on the regret.
Theorem 4.1. Assume that the expected demands {d } satisfy Assumption 2. Let η > 0, K ∈ N.
t,i i,t
When ϵ := T−2, γ := ηlog(e/ϵ), and δ := T−1, the regret (2) of Algorithm 2 satisfies
(cid:18) (cid:19)
n nT
R ∈ O nηKT logT + logKT + ,
T
η K
Especially, by choosing K ∈ Θ(T1/3), η ∈ Θ(T−2/3), Algorithm 2 guarantees R ∈ O(cid:0) nT2/3logT(cid:1) .
T
When compared to the bound in Theorem 3.1, the dependence on T is improved from T3/4 to T2/3.
Our proof uses similar intuitions to Section 3.2, along with some techniques adapted from Bubeck
et al. (2017). Hence, we only state the main steps and defer most details to the appendix.
Proof of Theorem 4.1. We first introduce some notation. For a conditional distribution q(c|p)
(cid:82)
over I and function g(c,p) let ⟨q,p⟩ := q(c|p)g(c,p)dc. For a distribution q(p) and function
δ p I
(cid:80) δ
g(p) over I , let ⟨q,g⟩ := q(p)g(p) as before. Throughout the proof, we set parameters
K p∈IK
γ := ηlog(e/ϵ), ϵ := T−2 and δ := T−1 as stated in the theorem.
Step 1. [Preparation] Westartbypresentingsomebasicpropertiesoftheexpectedlossℓ (·,·)
t,i
(4) under the cost-concave demand assumption (its proof is given in Appendix B).
Lemma 4.1. Under Assumption 2, the following holds for any δ ≤ 1/2, p⋆ ∈ I , t ∈ [T] and
K
i ∈ [n]:
1. ℓ (·,p⋆) is δ−1-Lipschitz over I := [δ,1−δ].
t,i δ
2. For any c⋆ < δ, ℓ (δ,p⋆) ≤ ℓ (c⋆,p⋆)+2δ.
i t,i t,i i
3. For any c⋆ > 1−δ, ℓ (1−δ,p⋆) ≤ ℓ (c⋆,p⋆)+2δ.
i t,i t,i i
Result (1) of the above lemma states that the losses are Lipschitz continuous in the interval I ,
δ
while (2) and (3) state that we do not lose much by choosing costs only in I instead of [0,1].
δ
The subsequent steps are analogous to that of the proof of Theorem 3.1: First, we decompose the
regret. Then, we lower and upper bound the comparator’s loss and algorithm’s loss, respectively.
Step 2. [Regret decomposition] Using essentially the same argument as (9), we can show
that the regret can be decomposed as follows:
R
T
= sup R T(c⋆,p⋆)+R(cid:101)T(n,δ,K). (21)
(c⋆,p⋆)∈I δn×IK
18Here R (c⋆,p⋆), is the regret of our algorithm relative to a given set of costs and prices (c⋆,p⋆) ∈
T
In×I . Recall that Algorithm 2 chooses costs c ∈ I and prices p ∈ I . We have,
δ K t,i δ t K
 
(cid:88) (cid:88)
R T(c⋆,p⋆) := 2 E[ℓ t]− ℓ t(c⋆,p⋆).
t∈[T] t∈[T]
Moreover, R(cid:101)T(n,δ,K) is the residual regret due to only focusing on I δn×I K. We have,
(cid:16) (cid:88) (cid:88) (cid:17)
R(cid:101)T(n,δ,K) = 2 inf ℓ t(c⋆,p⋆)− inf ℓ t(c⋆,p⋆) .
(c⋆,p⋆)∈I δn×IK
t∈[T]
(c⋆,p⋆)∈[0,1]n+1
t∈[T]
Following the same argument as Lemma 3.2 and using Lemma 4.1, we immediately see that
R(cid:101)T(n,δ,K) ∈ O(nT/K +δT) = O(nT/K), as we set δ := T−1 throughout this proof.
Step3. [Boundingthecomparatorloss] Next,welowerboundthecomparatorloss(cid:80) ℓ (c⋆,p⋆)
t t
where (c⋆,p⋆) ∈ In×I . The main result of this step is the following lemma.
δ K
Lemma 4.2. For any (c⋆,p⋆) ∈ In×I , we have
δ K
(cid:34) (cid:35)
(cid:18) (cid:19)
(cid:88) 3ηT e 1 (cid:88) 1
E (cid:98)h t(p⋆) − log ≤ ℓ t(c⋆,p⋆)+O ϵT +η|I K|T + logT .
γ ϵ n η
t∈[T] t∈[T]
Our proof of the above lemma uses the same intuitions as Lemma 3.3, along with some techniques
adapted from Bubeck et al. (2017). We give its detailed proof in Appendix B.1.
Step 4. [Bounding the algorithm’s loss] We now upper bound the algorithm’s loss via the
following lemma. Its proof, given in Appendix B, uses a straightforward calculation.
Lemma 4.3. The following bound holds for the cumulative losses,
1 (cid:88) (cid:88) (cid:104)(cid:68) (cid:69)(cid:105) 3ηT e
E[ℓ t] ≤ E q t,0,(cid:98)h
t
− log +4η|I K|T log(e/ϵ).
n γ ϵ
t∈[T] t∈[T]
Step 5. [Wrap up] Finally, by combining Lemma 4.2, 4.3 and ϵ := T−2, we obtain
(cid:34) (cid:35)
(cid:18) (cid:19)
(cid:88) (cid:88) 1
R (c⋆,p⋆) := E ℓ − ℓ (c⋆,p⋆) ≤ O η|I |T logT + log|I |T ,
T t t K K
η
t∈[T] t∈[T]
over all (c⋆,p⋆) ∈ I δn ×I K. Using the regret decomposition in (21), the fact that R(cid:101)T(n,δ,K) ∈
O(nT/K) (see step 2), and noting that |I | = K +1, we obtain the claimed regret bound.
K
4.3 Lower bound for cost-concave demands
In this section, we present Theorem 4.2, a regret lower bound of Ω((nT)2/3), for the targeted
marketing problem with cost-concave demands. This shows that Algorithm 2 is minimax optimal
upto log factors. Its proof, given in Appendix B.2 for completeness, is a straightforward adaptation
of the Ω(T2/3) lower bound for the online pricing problem in Kleinberg and Leighton (2003).
Theorem 4.2. For any algorithm A for the targeted marketing problem and a demand sequence
{D } , let R (A,{D } ) be the expected regret achieved by A under {D } up to round T. Then,
t t T t t t t
inf sup R (cid:0) A,{D } (cid:1) ∈ Ω(nT2/3), where the supremum is taken over all n market demand
A {Dt}t T t t
sequences {D } that satisfies the cost-concave demands assumption (Assumption 2).
t t
195 Variations of the targeted marketing problem
We now present variations of the targeted marketing problem, where our framework is applicable.
1. Subscription service. Consider a firm that runs a subscription service, where new customers
join on each round, while some old customers leave. Let d ∈ [0,1] be the number of new users
t,i
of type i ∈ [n] who joined the service during round t. Let β ∈ [0,1) be the fraction of users who
i
canceltheserviceineachroundinmarketi ∈ [n]. Hence, ineachroundt, thereare(cid:80)t βt−sd ∈
s=1 i t,i
(cid:2) 0, 1 (cid:3) active users. For c ∈ [0,1], let ct,i be the marketing expenditure2 spent in round t to
1−βi t,i 1−βi
attract new users of type i. Then, the total profit in round t is
(cid:18) t (cid:19)
Profit := (cid:88) Profit := (cid:88) (cid:16)(cid:88) βt−sp d (cid:17) − c t,i .
t t,i i t t,i 1−β
i
i∈[n] i∈[n] s=1
This is similar to our problem, but there is a memory effect since past customers can contribute to
future profits. Despite this added complexity, in Appendix C.1, we show that our algorithms can
be adapted to obtain the same regret bounds.
2. Promotional credit. Next, we consider the promotional credit problem, where the firm
segments the population into n types indexed by i ∈ [n]. On each round t, r ∈ [0,1] people of
t,i
each type try the firm’s service (where r is chosen exogenously). The firm offers promotional
t,i
credits c ∈ [0,1] to each type i and price p ∈ [0,1]. After using promotional credits, a d ∈ [0,1]
t,i t t,i
fraction of the r customers decide to purchase (or continue) the service. The total profit at round
t,i
t is
(cid:88) (cid:88)
Profit := Profit := r (p d −c ).
t t,i t,i t t,i t,i
i∈[n] i∈[n]
InAppendixC.2, weshowthatouralgorithmscanbeappliedtothisproblemwithoutmodification.
3. Profit-maximizing A/B tests. Suppose a firm performs a sequence of experiments where
it chooses M marketing alternatives for n population segments to increase demand for a product.
The firm wishes to perform these experiments while maintaining a common price. Assume that
each alternative m ∈ [M] costs c (m) ∈ [0,1] to implement in round t. For example, when M = 2,
t
the first option (m = 1) could be to show the product on a non-interactive webpage, while the
second option (m = 2) could be to present the product on an AI-assisted interactive webpage. In
this scenario, the second option costs more as it requires more computing resources.
For each i ∈ [n] and t ∈ [T], let d ∈ [0,1] be a random variable representing the normalized
t,i
demands made by population segment i during round t, given a choice of alternative m ∈ [M]
t,i
and price p ∈ [0,1]. The firm’s total profit at round t is
t
(cid:88) (cid:88)
Profit := Profit := p d −c (m ).
t t,i t t,i t t,i
i∈[n] i∈[n]
√
In Appendix C.3, we show a slight modification of our Algorithm 1 yields
O(cid:101)(cid:0)
n
MT2/3(cid:1)
regret.
2Asthenumberofactiveusersatanytimeisatmost 1 andtheprice(perround)isatmost1,thereisnoreason
to spend more than 1 on marketing. Hence we write1− thβ ei marketing expenditure as ct,i to ensure c ∈[0,1].
1−βi 1−βi t,i
206 Conclusion
Westudiedprofitmaximizationwhentherearemultiplemarketswhosedemandsresponddifferently
to the price and marketing expenditure. The demand characteristics are unknown, and the goal
is to design algorithms that are able to learn the optimal price and marketing costs via repeated
interactions. Our algorithms, designed to exploit the decomposable structure of this optimization
problem, use carefully designed loss functions to manage the exploration-exploitation trade-off.
When compared to a naive application of an adversarial bandit algorithm, which has regret with
anexponentialdependenceonthenumberofmarkets,ourapproachonlyhaslineardependence. We
complement our upper bounds with nearly matching lower bounds. Closing the n1/4 gap between
the upper and lower bounds for monotonic demands is an interesting avenue for future research.
References
Sanjeev Arora, Elad Hazan, and Satyen Kale. 2012. The multiplicative weights update method: A
meta-algorithm and applications. Theory of Computing 8, 1 (2012), 121–164.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 1995. Gambling in a
rigged casino: The adversarial multi-armed bandit problem. Proceedings of IEEE 36th Annual
Symposium on Foundations of Computer Science (1995), 322–331.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 2002. The nonstochastic
multiarmed bandit problem. SIAM J. Comput. 32, 1 (2002), 48–77.
Omar Besbes and Assaf Zeevi. 2009. Dynamic pricing without knowing the demand function: Risk
bounds and near-optimal algorithms. Operations Research 57, 6 (2009), 1407–1420.
Omar Besbes and Assaf Zeevi. 2015. On the (surprising) sufficiency of linear models for dynamic
pricing with demand learning. Management Science 61, 4 (2015), 723–739.
Jean Bretagnolle and Catherine Huber. 1979. Estimation des densit´es: risque minimax. Zeitschrift
fu¨r Wahrscheinlichkeitstheorie und verwandte Gebiete 47 (1979), 119–137.
S´ebastien Bubeck, Nicolo Cesa-Bianchi, et al. 2012. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends® in Machine Learning 5, 1 (2012),
1–122.
S´ebastien Bubeck, Yin Tat Lee, and Ronen Eldan. 2017. Kernel-based methods for bandit con-
vex optimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing. 72–85.
Wang Chi Cheung, David Simchi-Levi, and He Wang. 2017. Dynamic pricing and demand learning
with limited price experimentation. Operations Research 65, 6 (2017), 1722–1731.
Hana Choi, Carl F Mela, Santiago R Balseiro, and Adam Leary. 2020. Online display advertising
markets: A literature review and future directions. Information Systems Research 31, 2 (2020),
556–575.
Thomas M Cover. 1999. Elements of information theory. John Wiley & Sons.
Arnoud V Den Boer. 2015. Dynamic pricing and learning: Historical origins, current research, and
new directions. Surveys in Operations Research and Management Science 20, 1 (2015), 1–18.
21ArnoudVdenBoerandBertZwart.2014. Simultaneouslylearningandoptimizingusingcontrolled
variance pricing. Management Science 60, 3 (2014), 770–783.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. 2005. Online convex
optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the
16th Annual ACM-SIAM Symposium on Discrete Algorithms. 385–394.
John R Hauser, Guilherme Liberali, and Glen L Urban. 2014. Website morphing 2.0: Switching
costs, partial exposure, random exit, and when to morph. Management Science 60, 6 (2014),
1594–1616.
John R Hauser, Glen L Urban, Guilherme Liberali, and Michael Braun. 2009. Website morphing.
Marketing Science 28, 2 (2009), 202–223.
Elad Hazan et al. 2016. Introduction to online convex optimization. Foundations and Trends® in
Optimization 2, 3-4 (2016), 157–325.
Elad Hazan and Kfir Levy. 2014. Bandit convex optimization: Towards tight bounds. Advances in
Neural Information Processing Systems 27 (2014), 784–792.
Lalit Jain, Zhaoqi Li, Erfan Loghmani, Blake Mason, and Hema Yoganarasimhan. 2023. Effective
Adaptive Exploration of Prices and Promotions in Choice-Based Demand Models. Available at
SSRN 4438537 (2023).
Adel Javanmard. 2017. Perishability of data: dynamic pricing under varying-coefficient models.
The Journal of Machine Learning Research 18, 1 (2017), 1714–1744.
Adel Javanmard and Hamid Nazerzadeh. 2019. Dynamic pricing in high-dimensions. The Journal
of Machine Learning Research 20, 1 (2019), 315–363.
N Bora Keskin and Assaf Zeevi. 2014. Dynamic pricing with an unknown demand model: Asymp-
totically optimal semi-myopic policies. Operations Research 62, 5 (2014), 1142–1167.
RobertKleinbergandTomLeighton.2003. Thevalueofknowingademandcurve: Boundsonregret
foronlineposted-priceauctions.InProceedings of IEEE 44th Annual Symposium on Foundations
of Computer Science. IEEE, 594–605.
Tor Lattimore and Csaba Szepesv´ari. 2020. Bandit algorithms. Cambridge University Press.
Gui Liberali and Alina Ferecatu. 2022. Morphing for consumer dynamics: Bandits meet hidden
Markov models. Marketing Science 41, 4 (2022), 769–794.
Kanishka Misra, Eric M Schwartz, and Jacob Abernethy. 2019. Dynamic online pricing with
incomplete information using multiarmed bandit experiments. Marketing Science 38, 2 (2019),
226–252.
Georgia Perakis and Divya Singhvi. 2023. Dynamic pricing with unknown nonparametric demand
and limited price changes. Operations Research (2023).
Ivan Png. 2022. Managerial economics. Routledge.
Neela Sawant, Chitti Babu Namballa, Narayanan Sadagopan, and Houssam Nassif. 2018. Contex-
tual multi-armed bandits for causal marketing. arXiv preprint arXiv:1810.01859 (2018).
Eric M Schwartz, Eric T Bradlow, and Peter S Fader. 2017. Customer acquisition via display
advertising using multi-armed bandit experiments. Marketing Science 36, 4 (2017), 500–522.
22Glen L Urban, Guilherme Liberali, Erin MacDonald, Robert Bordley, and John R Hauser. 2014.
Morphing banner advertising. Marketing Science 33, 1 (2014), 27–46.
Yining Wang, Boxiao Chen, and David Simchi-Levi. 2021. Multimodal dynamic pricing. Manage-
ment Science 67, 10 (2021), 6136–6152.
Jianyu Xu and Yu-Xiang Wang. 2021. Logarithmic regret in feature-based dynamic pricing. Ad-
vances in Neural Information Processing Systems 34 (2021), 13898–13910.
A Proofs omitted from Section 3
A.1 Lemmas for regret upper bound proof
Lemma 3.1. (Based on Arora et al. (2012)) For each i ∈ [n], any (c⋆,p⋆) ∈ I2 and η > 0,
i K
(cid:88) (cid:68) (cid:69) (cid:88) (cid:88) (cid:68) (cid:69) 1
q t,i,f(cid:98)t,i
p⋆
− f(cid:98)t,i(c⋆ i,p⋆) ≤ η q t,i,f(cid:98) t2
,i p⋆
+
η
log|I K|, (7)
t∈[T] t∈[T] t∈[T]
(cid:88) (cid:68) (cid:69) (cid:88) (cid:88) (cid:68) (cid:69) 1
q t,0,(cid:98)h
t
− (cid:98)h t(p⋆) ≤ η q t,0,(cid:98)h2
t
+
η
log|I K|, (8)
t∈[T] t∈[T] t∈[T]
where q , q are distributions given in Lines 7 and 8 in Algorithm 1, respectively.
t,i t,0
Proof. The second bound follows from essentially the same procedure that derives the first bound.
Hence, we prove the first bound only. Recall that for conditional distribution q(c|p) over I and
K
(cid:68) (cid:69)
(cid:80)
functiong(c,p), ⟨q,p⟩
p
:=
c∈IK
q(c|p)g(c,p). Fixanyp ∈ I K. First, rewrite q t,i,f(cid:98)t,i
p
asfollows:
(cid:68) (cid:69) (cid:68) (cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69)
q t,i,f(cid:98)t,i = q t,i,f(cid:98)t,i + log q t,i,exp −ηf(cid:98)t,i − log q t,i,exp −ηf(cid:98)t,i .
p p η p η p
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
A (p) B (p)
t,i t,i
Then, we upper bound A (p) as follows:
t,i
(cid:68) (cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69)
A t,i(p) := q t,i,f(cid:98)t,i + log q t,i,exp −ηf(cid:98)t,i
p η p
(cid:18) (cid:19)
(cid:68) (cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69)
≤ q t,i,f(cid:98)t,i + q t,i,exp −ηf(cid:98)t,i −1 (as logx ≤ x−1 for x ≥ 0)
p η p
1 (cid:68) (cid:16) (cid:17) (cid:69)
=
η
q t,i, exp −ηf(cid:98)t,i −1+ηf(cid:98)t,i
p
(as ⟨q t,i,1⟩
p
= 1)
η (cid:68) (cid:69)
≤
2
q t,i,f(cid:98) t2
,i p
(as e−x−1+x ≤ x 22 for x ≥ 0)
Next, we rewrite B (p) as follows:
t,i
1 (cid:68) (cid:16) (cid:17)(cid:69)
B t,i(p) := − log q t,i,exp −ηf(cid:98)t,i
η p
(cid:16) (cid:17)
=
−1
log
(cid:80)
ci∈IK
exp
−η(cid:80)t
τ=1f(cid:98)τ,i(c i,p)
(cid:16) (cid:17)
η (cid:80)
ci∈IK
exp −η(cid:80)t τ− =1 1f(cid:98)τ,i(c i,p)
231
= (Φ (p)−Φ (p)),
t−1,i t,i
η
(cid:16) (cid:17)
where Φ t,i(p) :=
log(cid:80)
ci∈IK
exp
−η(cid:80)t
τ=1f(cid:98)τ,i(c i,p) and Φ 0,i(p) :=
log(cid:80)
ci∈IK
1 = log|I K|. Us-
ing the above bounds for A (p) and B (p), we obtain
t,i t,i
(cid:68) (cid:69) η (cid:68) (cid:69) 1
q t,i,f(cid:98)t,i
p
≤
2
q t,i,f(cid:98) t2
,i
p+
η
(Φ t−1,i(p)−Φ t,i(p)).
Summing the above inequality over t ∈ [T] yields
(cid:88) (cid:68) (cid:69) η (cid:88) (cid:68) (cid:69) 1
q t,i,f(cid:98)t,i
p
≤
2
q t,i,f(cid:98) t2
,i
p+
η
(Φ 0,i(p)−Φ T,i(p))
t∈[T] t∈[T]
η (cid:88) (cid:68) (cid:69) 1 1
=
2
q t,i,f(cid:98) t2
,i
p+
η
log|I K|− ηΦ T,i(p).
t∈[T]
Finally, we upper bound −1Φ (p) as follows: For any c⋆ ∈ I ,
η T,i i K
   
1 1 (cid:88) (cid:88) 1 (cid:88) (cid:88)
ηΦ T,i(p) =
η
log exp−η f(cid:98)t,i(c i,p) ≥
η
logexp−η f(cid:98)t,i(c⋆ i,p) = − f(cid:98)t,i(c⋆ i,p).
ci∈IK t∈[T] t∈[T] t∈[T]
By combining the above two inequalities and setting p = p⋆, we obtain the first bound (7).
Lemma 3.2. Under Assumption 1, R(cid:101)T(n,K) ≤ 2 KnT.
Proof. First, recall the definition of R(cid:101)T(n,K) in below.
(cid:16) (cid:88) (cid:88) (cid:17)
R(cid:101)T(n,K) := 2 min ℓ t(c⋆,p⋆)− inf ℓ t(c⋆,p⋆)
(c⋆,p⋆)∈In+1 (c⋆,p⋆)∈[0,1]n+1
K t∈[T] t∈[T]
(cid:88) (cid:88)
= sup Profit (c⋆,p⋆)− max Profit (c⋆,p⋆),
t,i i t,i i
(c⋆,p⋆)∈[0,1]n+1 (c⋆,p⋆)∈In+1
t,i∈[T]×[n] K t,i∈[T]×[n]
where the second line follows from the definition of ℓ in (4). Let (cˆ,pˆ) ∈ [0,1]n+1 be a maximizer
t
(cid:80)
of Profit (c,p), that is,
t,i∈[T]×[n] t,i
(cid:88)
(cˆ,pˆ) ∈ argmax Profit (c ,p).
t,i i
(c,p)∈[0,1]n+1
t,i∈[T]×[n]
Then, by the monotonicity of d (·,·) (Assumption 1), for any ϵ ≥ 0,
t,i
d (cˆ,pˆ) ≤ d (cˆ +ϵ,pˆ−ϵ), ∀i ∈ [n].
t,i i t,i i
Hence,
Profit (cˆ +ϵ,pˆ−ϵ) = (pˆ−ϵ)·d (cˆ +ϵ,pˆ−ϵ)−(cˆ +ϵ)
t,i i t,i i i
≥ (pˆ−ϵ)·d (cˆ,pˆ)−(cˆ +ϵ)
t,i i i
≥ Profit (cˆ,pˆ)−2ϵ.
t,i i
24Since there exists (c⋆,p⋆) ∈ In+1 such that 0 ≤ c⋆−cˆ ≤ K−1, ∀i ∈ [n] and 0 ≤ p⋆−pˆ≤ K−1, the
K i i
above implies
Profit (c⋆,p⋆) ≥ Profit (cˆ,pˆ)−2K−1.
t,i i t,i i
Finally, summing the above over (t,i) ∈ [T]×[n], we have that
(cid:88) (cid:88) 2nT
sup Profit (c⋆,p⋆) ≤ Profit (c⋆,p⋆)+ .
t,i i t,i K
(c⋆,p⋆)∈[0,1]n+1
t,i∈[T]×[n] t,i∈[T]×[n]
Therefore, R(cid:101)T(n,K) ≤ 2 KnT
Lemma 3.4. For any (c⋆,p⋆) ∈ In+1 and (t,i) ∈ [T]×[n],
K
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:104) (cid:105) (cid:68) (cid:69) |I |
E f(cid:98)t,i(c⋆ i,p⋆) ≤ ℓ t,i(c⋆ i,p⋆), E q t,i,f(cid:98) t2
,i p⋆
≤ E
q
t,0(p⋆K
)+γ
.
Proof. For the sake of convenience, we restate the definition of ℓ (c⋆,p⋆).
t,i
ℓ (c⋆,p⋆) := 1 (cid:0) 1−Profit (c⋆,p⋆)(cid:1) , Profit (c⋆,p⋆) := p⋆d (c⋆,p⋆)−c⋆.
t,i i 2 t,i i t,i t,i i i
Then, the first bound follows from the following calculations:
(cid:104) (cid:105) (cid:20) ℓ 1[c = c⋆,p = p⋆] (cid:21)
E f(cid:98)t,i(c⋆ i,p⋆) = E
q
t, (i
c
|t p,i )(qi (pt
)+γ)
t,i t,i t t,0 t
 
= E (cid:88) q t,i(c′ i,p′)· ℓ t,i q(c′ i (, cp ′′ |) p1 ′)[c (′ i q= (c p⋆ i ′, )p +′ = γ)p⋆] 
c′,p′ t,i i t,0
i
 
≤ E (cid:88) q t,i(c′ i,p′)· qℓ t,i (( cc ′′ i, ,p p′ ′) ) ·1(cid:2) c′ i = c⋆ i,p′ = p⋆(cid:3) 
c′,p′ t,i i
i
= (cid:88) ℓ (c′,p′)1(cid:2) c′ = c⋆,p′ = p⋆(cid:3)
t,i i i i
c′,p′
i
= ℓ (c⋆,p⋆).
t,i i
And the second bound follows as
 
E(cid:20) (cid:68)
q t,i,f(cid:98) t2
,i(cid:69) p⋆(cid:21)
≤ E
(cid:88) (cid:88)
q t,i(c′ i,p′)q t,i(c i|p⋆)·
q2
(1 c[ ′c |′
i
p= ′)(c qi,p (′ p= ′)p +⋆]
γ)2 (as ℓ t,i ≤ 1)
c′ i,p′ ci t,i i t,0
(cid:34) (cid:35)
= E
(cid:88) q t,i(c i,p⋆)q t,i(c i|p⋆)
q2 (c |p⋆)(q (p⋆)+γ)2
ci t,i i t,0
(cid:34) (cid:35)
= E
(cid:88) q t,i(c i,p⋆)
q (c |p⋆)(q (p⋆)+γ)2
ci t,i i t,0
(cid:34) (cid:35)
≤ E
(cid:88) q t,i(c i,p⋆)
q (c |p⋆)·q (p⋆)·(q (p⋆)+γ)
t,i i t,0 t,0
ci
25(cid:34) (cid:35)
= E
(cid:88) q t,i(c i,p⋆)
q (c ,p⋆)(q (p⋆)+γ)
t,i i t,0
ci
(cid:20) (cid:80) 1 (cid:21)
= E ci∈IK
q (p⋆)+γ
t,0
(cid:20) (cid:21)
|I |
= E K .
q (p⋆)+γ
t,0
Lemma 3.5. For any (c⋆,p⋆) ∈ In+1,
K
(cid:34) (cid:35)
E (cid:88) (cid:18) 1ℓ t1[p t = p⋆] − η|I K| (cid:19) ≤ (cid:88) 1 ℓ (c⋆,p⋆)+ 1 log|I |.
n q (p )+γ q (p⋆)+γ n t i η K
t,0 t t,0
t∈[T] t∈[T]
Proof. First, we state the expectation of (7) in Lemma 3.1 below.
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:88) (cid:68) (cid:69) (cid:88) (cid:104) (cid:105) (cid:88) (cid:68) (cid:69) 1
E q t,i,f(cid:98)t,i
p⋆
− E f(cid:98)t,i(c⋆ i,p⋆) ≤ η E q t,i,f(cid:98) t2
,i p⋆
+
η
log|I K|,
t∈[T] t∈[T] t∈[T]
where we used the linearity of expectation. Then, by applying Lemma 3.4, we obtain the following
bound.
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:88) E (cid:68) q t,i,f(cid:98)t,i(cid:69)
p⋆
− (cid:88) ℓ t,i(c⋆ i,p⋆) ≤ (cid:88) E
q
t,0η (p|I ⋆K )+|
γ
+ η1 log|I K|.
t∈[T] t∈[T] t∈[T]
(cid:68) (cid:69)
We can further rewrite the above by unfolding q t,i,f(cid:98)t,i :
p⋆
(cid:68) q t,i,f(cid:98)t,i(cid:69) = (cid:88) q t,i(c i|p⋆)· ℓ t,i1[c t,i = c i,p t = p⋆]
p⋆ q t,i(c t,i|p t)(q t,0(p t)+γ)
ci
ℓ 1[p = p⋆]
= q (c |p⋆)· t,i t
t,i t,i
q (c |p )(q (p )+γ)
t,i t,i t t,0 t
ℓ 1[p = p⋆]
t,i t
= q (c |p )·
t,i t,i t
q (c |p )(q (p )+γ)
t,i t,i t t,0 t
ℓ 1[p = p⋆]
t,i t
= ,
q (p )+γ
t,0 t
which gives
(cid:34) (cid:35)
E (cid:88) (cid:18) ℓ t,i1[p t = p⋆] − η|I K| (cid:19) ≤ (cid:88) ℓ (c⋆,p⋆)+ 1 log|I |. (22)
q (p )+γ q (p⋆)+γ t,i i η K
t,0 t t,0
t∈[T] t∈[T]
By summing (22) over i ∈ [n] and dividing it by n, we obtain the stated bound.
26Lemma 3.6. For all t ∈ [T],
1 (cid:88)
E[ℓ t] ≤
(cid:88) E(cid:104)(cid:68)
q t,0,(cid:98)h
t(cid:69)(cid:105)
−
η|I K|T
+2η|I K|2T.
n γ
t∈[T] t∈[T]
Proof. The lemma follows from straightforward calculations. First, for each t ∈ [T],
(cid:68) (cid:69) η|I K| 1 (cid:88) (cid:18) ℓ t1[p t = p] (cid:19) (cid:88) q t,0(p)
q t,0,(cid:98)h
t
− = ·q t,0(p) −η|I K|
γ n q (p )+γ q (p)+γ
t,0 t t,0
p∈IK p∈IK
1 ℓ tq t,0(p t) (cid:88) q t,0(p)
= −η|I |
K
n q (p )+γ q (p)+γ
t,0 t t,0
p∈IK
1 ℓ q (p )
≥ t t,0 t −η|I |2
K
n q (p )+γ
t,0 t
(cid:18) (cid:19)
1 γℓ
= ℓ − t −η|I |2.
t K
n q (p )+γ
t,0 t
Next, we take the expectation to the above:
(cid:20) (cid:21) (cid:20) (cid:21)
E (cid:68) q t,0,(cid:98)h t(cid:69) − η|I K| ≥ 1 E(cid:2) ℓ t(cid:3) −γ E ℓ t/n −η|I K|2
γ n q (p )+γ
t,0 t
≥ 1 E(cid:2) ℓ (cid:3) −γ|I |−η|I |2
t K K
n
≥ 1 E(cid:2) ℓ (cid:3) −2η|I |2. (as γ := η and |I |2 ≥ |I |)
t K K K
n
This implies the stated bound.
A.2 Lemmas for regret lower bound proof
Lemma 3.7. For any (c⋆,p⋆) ∈ S, let d(c,p) be the conditional expectation of d. Then, d(c,p)
satisfies the following in environment E .
(c⋆,p⋆)
1. For any p ∈ [0,1], d(c,p) is non-decreasing with respect to c ∈ [0,1].
2. For any c ∈ [0,1], d(c,p) is non-increasing with respect to p ∈ [0,1].
3. Profit(c⋆,p⋆) ≥ ϵ > 0.
20
4. Profit(c,p) = 0 for any (c,p) ∈ C×P \{(c⋆,p⋆)} and Profit(c,p) < 0 for any (c,p) ∈ [0,1]2\
C ×P.
Proof. 1. By construction, d(c,p) is non-decreasing w.r.t. c ∈ [0,1] when p ∈/ (p⋆ − ϵ/2,p⋆].
Moreover, d(·,p) = d(·,p′) for any p,p′ ∈ (p⋆ − ϵ/2,p⋆]. Hence, to show the first part, it
suffices to show the following:
2c⋆−ϵ 2c⋆+ϵ 2c⋆+ϵ
d(c⋆−ϵ/2,p⋆) = ≤ d(c⋆,p⋆) = ≤ d(c⋆+ϵ/2,p⋆) = .
2p⋆ 2p⋆+ϵ 2p⋆
Since the latter inequality is true, we only need to check the former inequality:
2c⋆−ϵ 2c⋆+ϵ
≤ ⇐⇒ ϵ ≥ 2c⋆−4p⋆,
2p⋆ 2p⋆+ϵ
which is satisfied as ϵ > 0, c⋆ ≤ 1 and p⋆ ≥ 1.
2 2
272. By construction, d(c,p) is non-increasing w.r.t. p ∈ [0,1] when c ∈/ [c⋆,c⋆ +ϵ/2). Moreover,
d(c,·) = d(c′,·) for any c,c′ ∈ [c⋆,c⋆+ϵ/2). Hence, to show the first part, it suffices to show
the following:
2c⋆ 2c⋆+ϵ 2c⋆
d(c⋆,p⋆+ϵ/2) = ≤ d(c⋆,p⋆) = ≤ d(c⋆,p⋆−ϵ/2) = ,
2p⋆+ϵ 2p⋆+ϵ 2p⋆−ϵ
which leads to the following inequalities:
2c⋆+ϵ 2c⋆
≤ ⇐= ϵ ≥ 2p⋆−4c⋆, p⋆ > ϵ/2,
2p⋆+ϵ 2p⋆−ϵ
where the latter is satisfied for (c⋆,p⋆) ∈ S as c⋆ ≥ 2, 3 ≤ p⋆ ≤ 4 and ϵ > 0.
5 5 5
3. A straightforward computation yields
c⋆+ϵ/2
Profit(c⋆,p⋆) := p⋆·d(c⋆,p⋆)−c⋆ = p⋆· −c⋆
p⋆+ϵ/2
(cid:18) p⋆ (cid:19) ϵp⋆
= c⋆ −1 +
p⋆+ϵ/2 2p⋆+ϵ
ϵ (cid:18) p⋆−c⋆ (cid:19)
=
2 p⋆+ϵ/2
ϵ ϵ
≥ (p⋆−c⋆) ≥ ,
2 20
where the last inequality follows from p⋆ ≥ 3 and c⋆ ≤ 1 for any (c⋆,p⋆) ∈ S.
5 2
4. For any (c,p) ∈ C ×P and (c,p) ̸= (c⋆,p⋆),
1
Profit(c,p) = p· ·2c−c = 0.
2p
For any (c,p) ∈ [0,1]2\C ×P,
c
d(c,p) = g(c)·P[v ≥ p] < ,
p
as g(c) := min(1,⌊2cK⌋/K) < 2c and P[v ≥ p] < (2p)−1 for (c,p) ∈ [0,1]2 \C ×P. Hence
Profit(c,p) < p· c −c = 0 in this case.
p
Lemma 3.8. For t ∈ [T], i ∈ [n] and (c,p) ∈ C×P, define the following indicator random variable:
χ (c,p) := 1[(c ,p ) = (c,p)].
t,i t,i t
Then, for any bandit algorithm A that chooses from Cn×P,
(cid:16) (cid:12)(cid:12) (cid:17) (cid:88)
KL PA (cid:12)(cid:12)PA ≤ 54ϵ2·EA [χ (c⋆,p⋆)].
base(cid:12)(cid:12) (c⋆,p⋆) base t,i
t,i∈[T]×[n]
28Proof. For each t ∈ [T], let o := {n ,c ,p } . Let πA(c ,...,c ,p |o ) be A’s policy
t τ,i τ,i τ τ∈[t],i∈[n] t t,1 t,d t t−1
function at given round t, that is, the probability of A choosing (c ,...,c ,p ) ∈ Cn ×P given
t,1 t,d t
the past history o . For each i ∈ [n], let u(d |c ,p ) (resp. u′(d |c ,p )) be the probability
t−1 t,i t,i t t,i t,i t
that d = d ∈ {0,1} given c and p under E (resp. E ).
t,i t,i t,i t base (c⋆,p⋆)
Then,
dPA (cid:81)T πA(c ,...,c ,p |o )·(cid:81)n u(d |c ,p )
log base (o ) = log t=1 t t,1 t,d t t−1 i=1 t,i t,i t
dPA T (cid:81)T πA(c ,...,c ,p |o )·(cid:81)n u′(d |c ,p )
(c⋆,p⋆) t=1 t t,1 t,d t t−1 i=1 t,i t,i t
(cid:88) u(d t,i|c t,i,p t)
= log
u′(d |c ,p )
t,i t,i t
t,i∈[T]×[n]
Hence,
(cid:34) (cid:35)
(cid:16) (cid:12)(cid:12) (cid:17) dPA
KL PA (cid:12)(cid:12)PA = EA log base (o )
base(cid:12)(cid:12) (c⋆,p⋆) base dPA T
(c⋆,p⋆)
(cid:34) (cid:35)
=
(cid:88)
EA log
u(d t,i|c t,i,p t)
base u′(d |c ,p )
t,i t,i t
t,i∈[T]×[n]
(cid:34) (cid:35)
=
(cid:88)
EA
(cid:88)
χ (c,p)·log
u(d t,i|c t,i,p t)
base t,i u′(d |c ,p )
t,i t,i t
t,i∈[T]×[n] c,p∈C×P
≤ (cid:88) 54ϵ2·EA (cid:2) χ (c⋆,p⋆)(cid:3) , (by Lemma A.1)
base t,i
t,i∈[T]×[n]
(cid:80)
where the third line is due to χ (c,p) = 1 for all i ∈ [n], and the last line we used the
c,p∈C×P t,i
fact that u(·|c ,p ) = u′(·|c ,p ) whenever c ̸= c⋆ or p ̸= p⋆.
t,i t t,i t t,i t
Lemma A.1. For any (c⋆,p⋆) ∈ S and 0 < ϵ ≤ 1 ,
20
(cid:18) (cid:18) (cid:19)(cid:12)(cid:12) (cid:18) (cid:19)(cid:19)
KL Ber(c⋆)⊗Ber 1 (cid:12) (cid:12)(cid:12) (cid:12)Ber(c⋆+ϵ)⊗Ber 1 ≤ 54ϵ2.
2p⋆ (cid:12)(cid:12) 2p⋆+ϵ
Proof. Let kl(p,q) := plog p +(1−p)log 1−p. Then,
q 1−q
(cid:18) (cid:18) (cid:19)(cid:12)(cid:12) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
KL Ber(2c⋆)⊗Ber 1 (cid:12) (cid:12)(cid:12) (cid:12)Ber(2c⋆+ϵ)⊗Ber 1 = kl(2c⋆,2c⋆+ϵ)+kl 1 , 1 .
2p⋆ (cid:12)(cid:12) 2p⋆+ϵ 2p⋆ 2p⋆+ϵ
Next, we bound two terms in the LHS using x ≤ log(1+x) ≤ x for all x > −1.
1+x
2c⋆ (cid:18) 1−2c⋆ (cid:19)
kl(2c⋆,2c⋆+ϵ) = 2c⋆log +(1−2c⋆)log
2c⋆+ϵ 1−2c⋆−ϵ
(cid:18) (cid:19)
(cid:16) ϵ (cid:17) ϵ
= −2c⋆log 1+ +(1−2c⋆)log 1+
2c⋆ 1−2c⋆−ϵ
(cid:18) ϵ/2c⋆ (cid:19) ϵ(1−2c⋆)
≤ −2c⋆ +
1+ϵ/2c⋆ 1−2c⋆−ϵ
292ϵc⋆ (1−2c⋆)ϵ
= − +
2c⋆+ϵ 1−2c⋆−ϵ
ϵ2
=
(1−2c⋆−ϵ)(2c⋆+ϵ)
ϵ2
≤ (as 2 ≤ c⋆ ≤ 9 )
(0.1−ϵ)(0.4+ϵ) 5 20
≤ 50ϵ2, (as ϵ ≤ 1 )
20
(cid:18) 1 1 (cid:19) 1 2p⋆+ϵ (cid:18) 1 (cid:19) 1− 1
2p⋆
kl , = log + 1− log
2p⋆ 2p⋆+ϵ 2p⋆ 2p⋆ 2p⋆ 1− 1
2p⋆+ϵ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 ϵ 1 ϵ
= log 1+ + 1− log 1−
2p⋆ 2p⋆ 2p⋆ 2p⋆(2p⋆+ϵ−1)
(cid:18) (cid:19)
ϵ 1 ϵ
≤ − 1− ·
4(p⋆)2 2p⋆ 2p⋆(2p⋆+ϵ−1)
ϵ ϵ 2p⋆−1
= − ·
4(p⋆)2 4(p⋆)2 2p⋆−1+ϵ
ϵ (cid:18) 2p⋆−1 (cid:19)
= 1−
4(p⋆)2 2p⋆−1+ϵ
ϵ2 ϵ2
= ≤ ≤ 4ϵ2. (as 3 ≤ p⋆)
4(p⋆)2(2p⋆−1+ϵ) 4(cid:0)3(cid:1)2 ·(cid:0)1(cid:1) 5
5 5
B Proofs omitted from Section 4
We start this appendix with a regret bound for {f(cid:98)t,i}
t∈[T]
(19), which is a generalization of Lemma
3.1 to continuous settings. While this regret bound follows from a standard analysis (Hazan et al.,
2016; Bubeck et al., 2012) of the exponential weights algorithm, we give its proof below for com-
pleteness.
Lemma B.1. Let q′ := Uniform(I ). Then, for any q ∈ ∆(I ) with density q(·) and any p⋆ ∈ I ,
δ δ K
(cid:88) (cid:68) (cid:69) (cid:88) (cid:68) (cid:69) 1 η (cid:88) (cid:68) (cid:69)
u t,i,f(cid:98)t,i
p⋆
− q,f(cid:98)t,i
p⋆
≤
η
KL(q||q′)+
2
u t,i,f(cid:98) t2
,i
p⋆, (23)
t∈[T] t∈[T] t∈[T]
where u (c |p) is the distribution given in Line 9 of Algorithm 2.
t,i i
Proof. Recall the definition of u (c |p) given in line 9 of Algorithm 2 which can be rewritten as
t,i i
follows:
(cid:16) (cid:17)
exp −η(cid:80)t τ− =1 1f(cid:98)τ,i(c i,p)
u (c |p) := .
t,i i (cid:82)
I
exp(cid:16) −η(cid:80)t τ− =1 1f(cid:98)τ,i(c i,p)(cid:17) dc
i
δ
30(cid:68) (cid:69)
(cid:82)
Then, rewrite u t,i,f(cid:98)t,i :=
I
u t,i(c i|p)f(cid:98)(c i,p)dc
i
as follows:
p δ
(cid:68) (cid:69) (cid:68) (cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69)
u t,i,f(cid:98)t,i = u t,i,f(cid:98)t,i + log u t,i,exp −ηf(cid:98)t,i − log u t,i,exp −ηf(cid:98)t,i .
p p η p η p
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
A (p) B (p)
t,i t,i
An upper bound for A (p) is given as follows:
t,i
(cid:68) (cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69)
A t,i(p) := u t,i,f(cid:98)t,i + log u t,i,exp −ηf(cid:98)t,i
p η p
(cid:18) (cid:19)
(cid:68) (cid:69) 1 (cid:68) (cid:16) (cid:17)(cid:69)
≤ u t,i,f(cid:98)t,i + u t,i,exp −ηf(cid:98)t,i −1 (as logx ≤ x−1 for x ≥ 0)
p η p
1 (cid:68) (cid:16) (cid:17) (cid:69)
=
η
u t,i, exp −ηf(cid:98)t,i −1+ηf(cid:98)t,i
p
(as ⟨u t,i,1⟩
p
= 1)
η (cid:68) (cid:69)
≤
2
u t,i,f(cid:98) t2
,i p
(as e−x−1+x ≤ x 22 for x ≥ 0)
Next, we rewrite B (p) as follows:
t,i
1 (cid:68) (cid:16) (cid:17)(cid:69)
B t,i(p) := − log u t,i,exp −ηf(cid:98)t,i
η p
(cid:16) (cid:17)
1
(cid:82)
I
exp
−η(cid:80)t
τ=1f(cid:98)τ,i(c i,p) dc
i
= − log δ
(cid:16) (cid:17)
η (cid:82)
I
exp −η(cid:80)t τ− =1 1f(cid:98)τ,i(c i,p) dc
i
δ
1
= (Φ (p)−Φ (p)),
t−1,i t,i
η
(cid:16) (cid:17)
where Φ t,i(p) :=
log(cid:82)
I
exp
−η(cid:80)t
τ=1f(cid:98)τ,i(c i,p) dc
i
and Φ 0,i(p) :=
log(cid:82)
I
dc
i
= log|I δ|.
δ δ
Therefore,
(cid:88) (cid:68) (cid:69) (cid:88)
u t,i,f(cid:98)t,i = A t,i(p)−B t,i(p)
p
t∈[T] t∈[T]
η (cid:88) (cid:68) (cid:69) 1 (cid:88)
≤
2
u t,i,f(cid:98) t2
,i
p+
η
(Φ t−1,i(p)−Φ t,i(p))
t∈[T] t∈[T]
η (cid:88) (cid:68) (cid:69) 1
≤
2
u t,i,f(cid:98) t2
,i
p−
η
(Φ T,i(p)−Φ 0,i(p)).
t∈[T]
Foranyq ∈ ∆(I )withdensityq(·),thefollowingholdsfromDonesker-Varadhaninequality (Cover,
δ
1999):
 
(cid:90)
1 1 1 (cid:88)
(Φ T,i(p)−Φ 0,i(p)) = log exp−η f(cid:98)t,i(c i,p)dc
i
η η |I |
δ I
δ t∈[T]
≥ −
(cid:88) (cid:68)
q,f(cid:98)t,i(c
i,p)(cid:69)
−
1 KL(cid:0) q||q′(cid:1)
,
p η
t∈[T]
where q′ := Uniform(I ). Therefore, we obtain the stated bound by setting p = p⋆.
δ
31Lemma 4.1. Under Assumption 2, the following holds for any δ ≤ 1/2, p⋆ ∈ I , t ∈ [T] and
K
i ∈ [n]:
1. ℓ (·,p⋆) is δ−1-Lipschitz over I := [δ,1−δ].
t,i δ
2. For any c⋆ < δ, ℓ (δ,p⋆) ≤ ℓ (c⋆,p⋆)+2δ.
i t,i t,i i
3. For any c⋆ > 1−δ, ℓ (1−δ,p⋆) ≤ ℓ (c⋆,p⋆)+2δ.
i t,i t,i i
Proof. Let x < y < z ∈ I and f : I → R be a convex function. Since y = z−y ·x+ y−x ·z, we
δ δ z−x z−x
have f(y) ≤ z−yf(x)+ y−xf(z) which is equivalent to
z−x z−x
f(y)−f(x) f(z)−f(y)
≤ . (24)
y−x z−y
For any p⋆ ∈ I , δ > 0, t ∈ [T] and i ∈ [n], ℓ (·,p⋆) is convex over [0,1] ⊇ I , hence it satisfies
K t,i δ
the above inequality. Therefore, for any δ < x ≤ y ≤ 1−δ,
ℓ (δ,p⋆)−ℓ (0,p⋆) ℓ (y,p⋆)−ℓ (x,p⋆) ℓ (1,p⋆)−ℓ (1−δ,p⋆)
t,i t,i t,i t,i t,i t,i
≤ ≤ .
δ−0 y−x 1−(1−δ)
Since ℓ (·,p⋆) ≤ 1, the above implies
t,i
1 ℓ (y,p⋆)−ℓ (x,p⋆) 1
t,i t,i
− ≤ ≤ .
δ y−x δ
(cid:12) (cid:12)
Hence, (cid:12)ℓ t,i(x,p⋆)−ℓ t,i(y,p⋆)(cid:12) ≤ δ−1|x−y| which proves the first claim.
The second claim follows from (24) with x = c⋆ < y = δ < z = 1 and f(·) = ℓ (·,p⋆):
i t,i
ℓ (δ,p⋆)−ℓ (c⋆,p⋆) ℓ (1,p⋆)−ℓ (δ,p⋆)
t,i t,i i ≤ t,i t,i =⇒
δ−c⋆ 1−δ
ℓ (δ,p⋆)−ℓ (c⋆,p⋆)
t,i t,i i ≤ 2,
δ
asδ ≤ 1/2andℓ (·,p⋆) ≤ 1. Thefinalclaimfollowssimilarlywithx = 0 < y = 1−δ < z = c⋆.
t,i
Lemma 4.3. The following bound holds for the cumulative losses,
1 (cid:88) (cid:88) (cid:104)(cid:68) (cid:69)(cid:105) 3ηT e
E[ℓ t] ≤ E q t,0,(cid:98)h
t
− log +4η|I K|T log(e/ϵ).
n γ ϵ
t∈[T] t∈[T]
Proof. The stated inequality follows from:
   
E(cid:104)(cid:68) q t,0,(cid:98)h t(cid:69)(cid:105) − 3η log e = 1 (cid:88) E (cid:88) q t,0(p)· ℓ t,i1[p t = p] −E (cid:88) q t,0(p)· 3ηlog(e/ϵ) 
γ ϵ d q (p )+γ q (p)+γ
t,0 t t,0
i∈[n] p∈IK p∈IK
 
≥ 1 (cid:88) E (cid:88) q t,0(p)· ℓ t,i1[p t = p] −3η|I K|log(e/ϵ)
d q (p )+γ
t,0 t
i∈[n] p∈IK
32(cid:20) (cid:21)
=
1 (cid:88)
E
ℓ t,iq t,0(p t)
−3η|I |log(e/ϵ)
K
d q (p )+γ
t,0 t
i∈[n]
(cid:20) (cid:21)
= 1 (cid:88) E ℓ − γℓ t,i −3η|I |log(e/ϵ)
t,i K
d q (p )+γ
t,0 t
i∈[n]
(cid:20) (cid:21)
1 γ
≥ E[ℓ ]−E −3η|I |log(e/ϵ) (as ℓ ≤ 1)
t K t,i
d q (p )
t,0 t
1
= E[ℓ ]−4η|I |log(e/ϵ),
t K
d
where the last line is due to E[1/q (p )] = (cid:80) 1 = |I | and γ := ηlog(e/ϵ).
t,0 t p∈IK K
B.1 Proof of Lemma 4.2
In this section, we give a proof of Lemma 4.2 which gives a lower bound for the comparator loss.
Specifically, we prove this lemma using the following lemmas:
Lemma B.2. Let u (c |p) is the distribution given in Line 9 of Algorithm 2 and q (c |p) =
t,i i t,i i
K [u (·|p)]q (c |p). Then, the following hold:
ϵ t,i t,i i
(cid:20) (cid:68) (cid:69) (cid:21) (cid:20) ℓ 1[p = p⋆](cid:21)
1. E u t,i,f(cid:98)t,i = E t,i t .
p⋆ q t,0(p t)+γ
(cid:20) (cid:21) (cid:20) (cid:21)
2. E(cid:104) (cid:10) q t,i,ℓ t,i(cid:11) p⋆(cid:105) ≤ E (cid:68) u t,i,f(cid:98)t,i(cid:69)
p⋆
+E
q
t,0(pγ
⋆)+γ
.
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:68) (cid:69) 2log(e/ϵ)
3. E u t,i,f(cid:98) t2
,i p⋆
≤ E
q t,0(p⋆)+γ
.
Note that Lemma B.2 contains bounds for all random terms in (23) of Lemma B.1 except for that
of ⟨q,f(cid:98)t,i⟩
p⋆
terms. To bound ⟨q,f(cid:98)t,i⟩
p⋆
terms in (23), we exploit the convexity of ℓ t,i(·,p). Using
techniques adapted from Bubeck et al. (2017) and the convex properties given in Lemma 4.1, we
prove the following lemma.
Lemma B.3. For any q ∈ ∆(I ) with density q(·), the following holds under Assumption 2:
δ
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
2E (cid:68) q,f(cid:98)t,i(cid:69)
p⋆
≤ (cid:10) q,ℓ t,i(cid:11)
p⋆
+E (cid:68) u t,i,f(cid:98)t,i(cid:69)
p⋆
+E qη t,0lo (pg ⋆( )e/ +ϵ)
γ
+7ϵδ−1.
Using the above two lemmas, we prove Lemma 4.2 as follows:
Lemma 4.2. For any (c⋆,p⋆) ∈ In×I , we have
δ K
(cid:34) (cid:35)
(cid:18) (cid:19)
(cid:88) 3ηT e 1 (cid:88) 1
E (cid:98)h t(p⋆) − log ≤ ℓ t(c⋆,p⋆)+O ϵT +η|I K|T + logT .
γ ϵ n η
t∈[T] t∈[T]
Proof of Lemma 4.2. First,writetheexpected,summedversionofLemmaB.1: Foranyq ,...,q ∈
1 d
∆(I ),
δ
(cid:34) (cid:35) (cid:34) (cid:35)
(cid:88) (cid:16)(cid:68) (cid:69) (cid:68) (cid:69) (cid:17) 1 (cid:88) η (cid:88) (cid:68) (cid:69)
E u t,i,f(cid:98)t,i
p⋆
− q i,f(cid:98)t,i
p⋆
≤
η
KL(q i||q′)+
2
E u t,i,f(cid:98) t2
,i p⋆
.
t,i∈[T]×[n] i∈[n] t,i∈[T]×[n]
33Rewriting the above using the linearity of expectation, we obtain
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:88) (cid:68) (cid:69) (cid:88) (cid:68) (cid:69) 1 (cid:88)
E u t,i,f(cid:98)t,i − E q i,f(cid:98)t,i ≤ KL(q i||q′)
p⋆ p⋆ η
t,i∈[T]×[n] t,i∈[T]×[n] i∈[n]
(cid:20) (cid:21)
η (cid:88) (cid:68) (cid:69)
+
2
E u t,i,f(cid:98) t2
,i p⋆
.
t,i∈[T]×[n]
Then, apply item 3 in Lemma B.2 to the above:
(cid:20) (cid:21) (cid:20) (cid:21)
(cid:88) (cid:68) (cid:69) (cid:88) (cid:68) (cid:69) 1 (cid:88)
E u t,i,f(cid:98)t,i − E q i,f(cid:98)t,i ≤ KL(q i||q′)
p⋆ p⋆ η
t,i∈[T]×[n] t,i∈[T]×[n] i∈[n]
(cid:20) (cid:21)
η (cid:88) 2log(e/ϵ)
+ E .
2 q (p⋆)+γ
t,0
t,i∈[T]×[n]
Next, we apply Lemma B.3 to the above to obtain the following:
(cid:20) (cid:21)
(cid:88) E (cid:68) u t,i,f(cid:98)t,i(cid:69)
p⋆
− (cid:88) (cid:10) q i,ℓ t,i(cid:11)
p⋆
≤ η2 (cid:88) KL(q i||q′)
t,i∈[T]×[n] t,i∈[T]×[n] i∈[n]
(cid:20) (cid:21)
(cid:88) 3log(e/ϵ)
+η E +7ϵδ−1.
q (p⋆)+γ
t,0
t,i∈[T]×[n]
Now, we apply item 1 in Lemma B.2 to the above.
(cid:88) E(cid:20) ℓ t,i1[p t = p⋆](cid:21) − (cid:88) (cid:10) q ,ℓ (cid:11) ≤2 (cid:88) KL(q ||q′)
q (p )+γ i t,i p⋆ η i
t,0 t
t,i∈[T]×[n] t,i∈[T]×[n] i∈[n]
(cid:20) (cid:21)
(cid:88) 3log(e/ϵ)
+η E +7ϵδ−1.
q (p⋆)+γ
t,0
t,i∈[T]×[n]
By rearranging terms and using the linearity of expectation, we obtain
(cid:34) (cid:35)
E (cid:88) (cid:18) ℓ t1[p t = p⋆] − 3nηlog(e/ϵ)(cid:19) ≤ (cid:88) (cid:10) q ,ℓ (cid:11) + 2 (cid:88) KL(q ||q′)+7ϵδ−1, (25)
q (p )+γ q (p⋆)+γ i t,i p⋆ η i
t,0 t t,0
t∈[T] t,i∈[T]×[n] i∈[n]
where q
i
∈ ∆(I δ) and q′ := Uniform(I δ). Now, recall the definition of (cid:98)h t:
1 (cid:88) ℓ t,i1[p t = p] (cid:16) e(cid:17)(cid:18) 1 1 (cid:19)
(cid:98)h t(p) := + 3ηlog − .
d q (p )+γ ϵ γ q (p)+γ
t,0 t t,0
i∈[n]
(cid:104) (cid:105)
Notice that one can rewrite the LHS of (25) as E (cid:80) t∈[T]n(cid:98)h t(p⋆) −3n γηT log e ϵ, which results in the
following:
(cid:34) (cid:35)
E (cid:88) (cid:98)h t(p⋆) ≤ n1 (cid:88) (cid:10) q i,ℓ t,i(cid:11)
p⋆
+ n2
η
(cid:88) KL(q i||q′)+ 3η γT log e
ϵ
+ n7ϵ δ,
t∈[T] t,i∈[T]×[n] i∈[n]
for any q ,...,q ∈ ∆(I ).
1 n δ
34Next,webound(cid:10) q ,ℓ (cid:11) asfollows: Foranyc⋆ ∈ I ands ∈ [0,1],letq betheuniformdistribution
i t,i p⋆ i δ i
over (1−s)c⋆+sI , for some s > 0 that will be specified later. Then, by δ−1–Lipschitz continuity
i δ
of ℓ (·,p⋆) (see item 1 of Lemma 4.1),
t,i
(cid:10) q ,ℓ (cid:11) ≤ 2sδ−1+ℓ (c⋆,p⋆).
i t,i p⋆ t,i i
Since KL(q ||q′) ≤ log(1/s) for such q s, choosing δ := T−1, s := T−2 we obtain the following
i i
bound.
(cid:34) (cid:35)
(cid:18) (cid:19)
(cid:88) 3ηT e 1 (cid:88) 1
E (cid:98)h t(p⋆) − log ≤ ℓ t(c⋆,p⋆)+O ϵT + logT . (26)
γ ϵ n η
t∈[T] t∈[T]
Next, to cancel out E[(cid:80) t(cid:98)h t(p⋆)] term in the above, we use the expected regret bound for {(cid:98)h t} t∈[T].
Since (cid:98)h t(·) ≥ 0 by construction, we can reuse (8) in Lemma 3.1 with {(cid:98)h t}
t∈[T]
for the cost-concave
case (20). Hence,
(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)
(cid:88) (cid:68) (cid:69) (cid:88) (cid:88) (cid:68) (cid:69) 1
E q t,0,(cid:98)h
t
−E (cid:98)h t(p⋆) ≤ ηE q t,0,(cid:98)h2
t
+
η
log|I K|.
t∈[T] t∈[T] t∈[T]
Adding the above with (26), we obtain the following.
(cid:34) (cid:35) (cid:34) (cid:35)
(cid:18) (cid:19)
(cid:88) (cid:68) (cid:69) 3ηT e 1 (cid:88) (cid:88) (cid:68) (cid:69) 1
E q t,0,(cid:98)h
t
−
γ
log
ϵ
≤
n
ℓ t(c⋆,p⋆)+ηE q t,0,(cid:98)h2
t
+O ϵT +
η
log|I K|T .
t∈[T] t∈[T] t∈[T]
(27)
Finally, we derive the stated bound by bounding E[(cid:80) t⟨q t,0,(cid:98)h2 t⟩] as follows: Since ℓ
t,i
≤ 1 and
γ := ηlog(e/ϵ), we have that
1 (cid:88) ℓ t,i1[p t = p] (cid:16) e(cid:17)(cid:18) 1 1 (cid:19) 1[p t = p]
(cid:98)h t(p) := + 3ηlog − ≤ +3.
d q (p )+γ ϵ γ q (p)+γ q (p )
t,0 t t,0 t,0 t
i∈[n]
Then,
 
(cid:32) (cid:33)
E(cid:104)(cid:68) q t,0,(cid:98)h2 t(cid:69)(cid:105) ≤ E (cid:88) q t,0(p)· 1 q[p 2t (=
p
)p] + 61 q[p t (p= )p] +9 
p∈IK t,0 t t,0 t
(cid:20) (cid:21)
1
= E +6|I |+9 = 7|I |+9.
K K
q (p )
t,0 t
Combining the above and (27), we obtain the stated bound.
Proof of Lemma B.2. Recall the definition of f(cid:98)t,i here for convenience.
ℓ 1[p = p] (cid:104) (cid:105)
t,i t
f(cid:98)t,i(c i,p) := ·K
ϵ
u t,i(·|p) (c t,i,c i).
q (c |p )(q (p )+γ)
t,i t,i t t,0 t
35For any fixed p⋆ ∈ I ,
K
(cid:68) (cid:69) ℓ 1[p = p⋆] (cid:90) (cid:104) (cid:105)
u t,i,f(cid:98)t,i = t,i t · K
ϵ
u t,i(·|p⋆) (c t,i,c i)·u t,i(c i|p⋆)dc
i
p⋆ q t,i(c t,i|p t)(q t,0(p t)+γ) I
δ
(cid:124) (cid:123)(cid:122) (cid:125)
q (c |p⋆)
t,i t,i
ℓ 1[p = p⋆]
= t,i t ·q (c |p⋆)
t,i t,i
q (c |p )(q (p )+γ)
t,i t,i t t,0 t
ℓ 1[p = p⋆]
= t,i t ·q (c |p⋆)
q (c |p⋆)(q (p )+γ) t,i t,i
t,i t,i t,0 t
ℓ 1[p = p⋆]
t,i t
= ,
q (p )+γ
t,0 t
hence
(cid:20) (cid:68) (cid:69) (cid:21) (cid:20) ℓ 1[p = p⋆](cid:21)
E u t,i,f(cid:98)t,i = E t,i t ,
p⋆ q t,0(p t)+γ
which is the first item of the lemma. The second item follows from the below calculations:
(cid:20) (cid:68) (cid:69) (cid:21) (cid:20) ℓ 1[p = p⋆](cid:21)
E u t,i,f(cid:98)t,i = E t,i t
p⋆ q t,0(p t)+γ
 
= E  (cid:88) (cid:90) q t,0(p′)q t,i(c′ i|p′)· qℓ t,i (( pc ′′ i ), +p′)
γ
·1(cid:2) p′ = p⋆(cid:3) dc′ i
p′∈IK I δ t,0
(cid:20)(cid:90) ℓ (c′,p⋆)q (p⋆) (cid:21)
= E q (c′|p⋆)· t,i i t,0 dc′
t,i i q (p⋆)+γ i
I t,0
δ
(cid:20)(cid:90) (cid:18) ℓ (c′,p⋆) (cid:19) (cid:21)
= E q (c′|p⋆) ℓ (c′,p⋆)−γ · t,i i dc′
t,i i t,i i q (p⋆)+γ i
I t,0
δ
(cid:20)(cid:90) (cid:18) (cid:19) (cid:21)
γ
≥ E q (c′|p⋆) ℓ (c′,p⋆)− dc′
t,i i t,i i q (p⋆)+γ i
I t,0
δ
(cid:20)(cid:90) (cid:21) (cid:20) (cid:21)
γ
= E q (c′|p⋆)·ℓ (c′,p⋆)dc′ −E
t,i i t,i i i q (p⋆)+γ
I t,0
δ
(cid:20) (cid:21)
= E(cid:104) (cid:10) q ,ℓ (cid:11) (cid:105) −E γ .
t,i t,i p⋆ q (p⋆)+γ
t,0
Finally, the third item follows from the below:
(cid:34) (cid:35)
E(cid:20) (cid:68) u t,i,f(cid:98) t2 ,i(cid:69) p⋆(cid:21) = E
q t2 ,i(c
t,iℓ |p2 t, ⋆i )1 ·[p (t
q
t= ,0(p p⋆ t]
)+γ)2
·(cid:90)
I
δ
K ϵ2(cid:104) u t,i(·|p⋆)(cid:105) (c t,i,c i)·u t,i(c i|p⋆)dc
i
(cid:124) (cid:123)(cid:122) (cid:125)
:= q(2) (c |p⋆)
t,i t,i
(cid:34) ℓ2 1[p = p⋆]·q(2) (c |p⋆)(cid:35)
= E t,i t t,i t,i
q2 (c |p⋆)·(q (p )+γ)2
t,i t,i t,0 t
(cid:34) 1[p = p⋆]·q(2) (c |p⋆) (cid:35)
≤ E t t,i t,i (as ℓ ≤ 1)
q2 (c |p⋆)·(q (p )+γ)2 t,i
t,i t,i t,0 t
36 
(cid:90) 1[p′ = p⋆]·q(2) (c′|p⋆)
= E  (cid:88)
q2
(c′|p⋆)·(qt,i (p′)i
+γ)2
·q t,i(c′ i|p′)q t,0(p′)dc′ i
p′∈IK I δ t,i i t,0
(cid:34) (cid:90) q(2) (c′|p⋆)·q (c′|p⋆)·q (p⋆) (cid:35)
= E t,i i t,i i t,0 dc′
q2 (c′|p⋆)·(q (p⋆)+γ)2 i
I δ t,i i t,0
(cid:34) (cid:90) q(2) (c′|p⋆) (cid:35)
≤ E t,i i dc′
q (c′|p⋆)·(q (p⋆)+γ) i
I δ t,i i t,0
(cid:34)
1
(cid:90) q(2) (c′|p⋆) (cid:35)
= E t,i i dc′
q (p⋆)+γ q (c′|p⋆) i
t,0 I δ t,i i
(cid:20) (cid:21)
2log(e/ϵ)
≤ E ,
q (p⋆)+γ
t,0
where the last line follows as
(cid:90) q(2) (c′|p⋆) (cid:90) 1 (cid:18)(cid:90) (cid:104) (cid:105) (cid:19)
t,i i dc′ = · K2 u (·|p⋆) (c′,c )·u (c |p⋆)dc dc′
q (c′|p⋆) i q (c′|p⋆) ϵ t,i i i t,i i i i
I δ t,i i I δ t,i i I δ
(cid:90) 1 q (c′|p⋆)
≤ · t,i i dc′ where µ ∈ I ,
q (c′|p⋆) max(|c′ −µ|,ϵ) i δ
I δ t,i i i
(cid:90)
dx
=
max(|x−µ|,ϵ)
I
δ
(cid:90) µ−ϵ dx (cid:90) µ+ϵ dx (cid:90) 1−δ dx
= + +
µ−x ϵ x−µ
δ µ−ϵ µ+ϵ
(cid:90) µ−ϵ dx (cid:90) 1 dx
≤ 2+ +
µ−x x−µ
0 µ+ϵ
(cid:90) µ dx (cid:90) 1−µ dx
= 2+ +
x x
ϵ ϵ
(cid:18) (cid:90) 1 dx(cid:19) (cid:16)e(cid:17)
≤ 2 1+ = 2log
x ϵ
ϵ
where the second line of the above is due to, for any q ∈ ∆(I ), K [q](x,y) ≤ 1 for some
δ ϵ max(|x−µ|,ϵ)
µ ∈ I (see (18)).
δ
Proof of Lemma B.3. First, a straightforward calculation yields
(cid:20) (cid:68) (cid:69) (cid:21) (cid:20) ℓ 1[p = p⋆] (cid:90) (cid:104) (cid:105) (cid:21)
E q,f(cid:98)t,i
p⋆
= E
q t,i(c
t,i|t p,i ⋆)·(t
q t,0(p t)+γ)
·
I
K
ϵ
u t,i(·|p⋆) (c t,i,c i)·q(c i)dc
i
δ
(cid:124) (cid:123)(cid:122) (cid:125)
K [u (·|p⋆)]q(c )
ϵ t,i t,i
(cid:20) ℓ 1[p = p⋆] (cid:21)
= E t,i t ·K [u (·|p⋆)]q(c )
q (c |p⋆)·(q (p )+γ) ϵ t,i t,i
t,i t,i t,0 t
 
= E  (cid:88) (cid:90) dc′ iq t,0(p′)q t,i(c′ i|p′)·
q
ℓ t (, ci( ′c |p′ i, ⋆p )′ () q1[p (′ p= ′)p +⋆]
γ)
·K ϵ[u t,i(·|p⋆)]q(c′ i)
p′∈IK I δ t,i i t,0
(cid:20)(cid:90) ℓ (c′,p⋆)·K [u (·|p⋆)]q(c′)(cid:21)
= E dc′q (p⋆)q (c′|p⋆)· t,i i ϵ t,i i
i t,0 t,i i q (c′|p⋆)(q (p⋆)+γ)
I δ t,i i t,0
37(cid:20)(cid:90) (cid:21)
≤ E dc′ℓ (c′,p⋆)·K [u (·|p⋆)]q(c′)
i t,i i ϵ t,i i
I
δ
(cid:104) (cid:105)
= E (cid:10) K [u (·|p⋆)]q,ℓ (cid:11) .
ϵ t,i t,i p⋆
Using the definition of K [·] (18), we can rewrite (cid:10) K [u (·|p⋆)]q,ℓ (cid:11) as follows: Let µ :=
ϵ ϵ t,i t,i p⋆
E [X] ∈ [δ,1−δ]. Then, for U ∼ Uniform([0,1]) and X ∼ q,
X∼ut,i(·|p⋆)
(cid:20)
(cid:104) (cid:105)
(cid:10) K [u (·|p⋆)]q,ℓ (cid:11) = E ℓ (Uµ+(1−U)X, p⋆)·1 |X −µ| ≥ ϵ
ϵ t,i t,i p⋆
U,X
t,i
(cid:104) (cid:105)
+ℓ (µ−ϵU , p⋆)·1 |X −µ| < ϵ∧µ ≥ ϵ+δ
t,i
(cid:21)
(cid:104) (cid:105)
+ℓ (µ+ϵU , p⋆)·1 |X −µ| < ϵ∧µ < ϵ+δ .
t,i
Using the convexity and δ−1–Lipschitz continuity of ℓ over I as shown in Lemma 4.1, we infer
t,i δ
the following facts:
ℓ (Uµ+(1−U)X,p⋆) ≤ U ℓ (µ,p⋆)+(1−U)ℓ (X,p⋆),
t,i t,i t,i
ℓ (µ−ϵU,p⋆), ℓ (µ+ϵU,p⋆) ≤ 1 (cid:0) ℓ (µ,p⋆)+ℓ (X,p⋆)(cid:1) + 3ϵ , when |X −µ| < ϵ.
t,i t,i t,i t,i
2 2δ
Therefore,
(cid:20)
(cid:104) (cid:105)
(cid:10) K [u (·|p⋆)]q,ℓ (cid:11) ≤ E (cid:0) U ℓ (µ,p⋆)+(1−U)ℓ (X,p⋆)(cid:1) ·1 |X −µ| ≥ ϵ +
ϵ t,i t,i p⋆
U,X
t,i t,i
(cid:21)
1 (cid:0) ℓ (µ,p⋆)+ℓ (X,p⋆)(cid:1) ·1(cid:104) |X −µ| < ϵ(cid:105) + 3ϵ .
t,i t,i
2 2δ
Then, for µ˜ = E [X] ∈ [δ,1−δ],
X∼qt,i(·|p⋆)
(cid:10) K [u (·|p⋆)]q,ℓ (cid:11) = 1 ℓ (µ,p⋆)+ 1 (cid:10) q,ℓ (cid:11) + 3ϵ
ϵ t,i t,i p⋆ 2 t,i 2 t,i p⋆ 2δ
≤ 1 ℓ (µ˜,p⋆)+ 1 (cid:10) q,ℓ (cid:11) + 7ϵ (as |µ−µ˜| ≤ 2ϵ)
2 t,i 2 t,i p⋆ 2δ
1 (cid:10) (cid:11) 1 (cid:10) (cid:11) 7ϵ
≤ q ,ℓ + q,ℓ + . (by Jensen’s inequality)
2 t,i t,i p⋆ 2 t,i p⋆ 2δ
Summarizing what we have obtained so far:
(cid:20) (cid:21)
(cid:68) (cid:69) (cid:104) (cid:105)
E q,f(cid:98)t,i
p⋆
= E (cid:10) K ϵ[u t,i(·|p⋆)]q,ℓ t,i(cid:11)
p⋆
≤
1 E(cid:104) (cid:10)
q ,ℓ
(cid:11) (cid:105)
+
1 E(cid:104) (cid:10)
q,ℓ
(cid:11) (cid:105)
+
7ϵ
2 t,i t,i p⋆ 2 t,i p⋆ 2δ
=
1 E(cid:104) (cid:10)
q ,ℓ
(cid:11) (cid:105)
+
1 (cid:10)
q,ℓ
(cid:11)
+
7ϵ
,
2 t,i t,i p⋆ 2 t,i p⋆ 2δ
(cid:104) (cid:105)
where the last inequality is due to E (cid:10) q,ℓ (cid:11) = (cid:10) q,ℓ (cid:11) , that is, q, ℓ does not depend on any
t,i p⋆ t,i p⋆ t,i
randomness of algorithm and environment.
38(cid:104) (cid:105)
Finally, we use item 2 of Lemma B.2 to bound E (cid:10) q ,ℓ (cid:11) in the above inequality in terms of
t,i t,i p⋆
(cid:104)(cid:68) (cid:69) (cid:105)
E u t,i,f(cid:98)t,i as follows:
p⋆
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
E (cid:68) q,f(cid:98)t,i(cid:69)
p⋆
≤ 21 E (cid:68) u t,i,f(cid:98)t,i(cid:69)
p⋆
+ 1
2
(cid:10) q,ℓ t,i(cid:11)
p⋆
+ 1
2
E
q
t,0(pγ
⋆)+γ
+ 27 δϵ
(cid:20) (cid:21) (cid:20) (cid:21)
= 1
2
E (cid:68) u t,i,f(cid:98)t,i(cid:69)
p⋆
+ 1
2
(cid:10) q,ℓ t,i(cid:11)
p⋆
+ 1
2
E qη t,0lo (pg ⋆( )e/ +ϵ)
γ
+ 27 δϵ ,
where the last equality is due to γ := ηlog(e/ϵ).
B.2 Regret lower bound proof
Theorem 4.2. For any algorithm A for the targeted marketing problem and a demand sequence
{D } , let R (A,{D } ) be the expected regret achieved by A under {D } up to round T. Then,
t t T t t t t
inf sup R (cid:0) A,{D } (cid:1) ∈ Ω(nT2/3), where the supremum is taken over all n market demand
A {Dt}t T t t
sequences {D } that satisfies the cost-concave demands assumption (Assumption 2).
t t
Proof. WeprovethisbyshowingthatanyalgorithmwithregretR forthetargetedmarketingwith
T
cost-concave demands can solve a bandit profit-maximization over any sequence of deterministic
demands d (p) that only depend on price p, with regret at most R /n, then we use the result from
t T
Kleinberg and Leighton (2003).
The reduction is as follows: Define an environment such that, given any p ∈ [0,1], the demands
t
are d = ··· = d = d (p ). Run the algorithm over this environment and output p . As
t,1 t,n t t t
d (c,p) = d (p)s are cost-concave, this is a valid cost-concave environment and the algorithm
t,i t
guarantees
(cid:34) (cid:35)
R := sup (cid:88) (cid:0) pd (c ,p)−c (cid:1) −E (cid:88) (p d (c ,p )−c ) .
T t,i i i t t,i t,i t t,i
(c,p)∈[0,1]n+1
t,i∈[T]×[n] t,i∈[T]×[n]
As d (c ,p ) = d (p ) for all t ∈ [T] and i ∈ [n], the first term is sup n·p·d (p) and the
t,i t,i t t t p∈[0,1] t
second term is at most n·p ·d (p ). Therefore,
t t t
sup pd
(p)−E(cid:2)
p d (p
)(cid:3)
≤
R T
.
t t t t
n
p∈[0,1]
TheLHSoftheaboveistheregretforsingle-demandbanditprofit-maximization. Foranyalgorithm
for this problem, there exists a sequence of demands {d⋆(·)} such that the given algorithm has
t t∈[T]
to suffer Ω(T2/3) regret (Kleinberg and Leighton, 2003). Hence, the given algorithm has to suffer
Ω(nT2/3) regret with respect to the environment where the demand in each market, given p , is
t
d⋆(p ).
t t
C Details omitted from Section 5
C.1 Subscription service
Setting. Consider a firm that runs a subscription service. Let d ∈ [0,1] be the number of new
t,i
users of type i ∈ [n] who joined the service during round t. Let β ∈ [0,1) be the fraction of users
i
39who cancel the service in each round. Hence, in each round t, there are (cid:80)t βt−sd ∈ (cid:2) 0, 1 (cid:3)
s=1 i t,i 1−βi
active users. For c ∈ [0,1], let
ct,i
be the marketing expenditure spent in round t to attract new
t,i 1−βi
users of type i.
Then, the stochastic and expected profits are given as follows:
(cid:32) t (cid:33)
Profit := (cid:88) βt−sp d − c t,i , (28)
t,i i t t,i 1−β
i
s=1
(cid:32) t (cid:33)
Profit (c ,p ) := (cid:88) βt−sp d (c ,p ) − c t,i . (29)
t,i t,i t i t t,i t,i t 1−β
i
s=1
(cid:80) (cid:80)
Define Profit := Profit and Profit (c ,p ) := Profit (c ,p ).
t i∈[n] t,i t t t i∈[n] t,i t,i t
Reduction. We show that our algorithms guarantee sublinear regrets under Assumption 1 and
2. One can use Algorithm 1 and 2 to solve this problem via the following reduction: summing over
(cid:80)
s ∈ [T], Profit can be rewritten as follows.
t∈[T] t,i
t
(cid:88) Profit = (cid:88) (cid:88) βt−sp d − 1 (cid:88) c = 1 (cid:88) P(cid:94) rofit ,
t,i i t t,i 1−β t,i 1−β t,i
i i
t∈[T] t∈[T]s=1 t∈[T] t∈[T]
(cid:16) (cid:17)
where P(cid:94) rofit := 1−βT−t+1 p d −c . We know that P(cid:94) rofit ∈ [−1,1] as 1−βT−t+1 ∈ [0,1].
t,i i t t,i t,i t,i i
It is straightforward to see that running our algorithms with the loss
1 (cid:16) (cid:94) (cid:17)
ℓ := 1−Profit ∈ [0,1]. (30)
t,i t,i
2
guarantees the same regrets as in the targeted marketing problem, scaled by (1−max )−1:
i∈[n]βi
Regret bound. Now, we state regret bounds for the subscription service problem.
Theorem C.1. Let
(cid:34) (cid:35)
(cid:88) (cid:88)
R := sup Profit (c,p)−E Profit ,
T t t
(c,p)∈[0,1]n+1
t∈[T] t∈[T]
whereProfit andProfit (c,p)aregivenby (28)and (29)respectively. UnderAssumption1, running
t t
Algorithm 1 with ℓ defined in (30) and K ∈ Θ(T1/4),η ∈ Θ(T−3/4) guarantees
t,i
(cid:32) (cid:33)
nT3/4logT
R ∈ O ,
T
1−max β
i∈[n] i
Similarly, under Assumption 2, running Algorithm 2 with the aforementioned ℓ guarantees
t,i
(cid:32) (cid:33)
nT2/3logT
R ∈ O ,
T
1−max β
i∈[n] i
when γ := ηlog(e/ϵ),ϵ := T−2,δ := T−1,K ∈ Θ(T1/3) and η ∈ Θ(T−2/3).
40C.2 Promotional credit
Setting. Next, we consider the promotional credit problem, where the firm segments the pop-
ulation into n types indexed by i ∈ [n]. For each round t, r ∈ [0,1] people of each type try the
t,i
firm’s service, where r is given exogenously. The firm offers promotional credits c ∈ [0,1] to
t,i t,i
each type i and price p ∈ [0,1]. After using promotional credits, a d ∈ [0,1] fraction of the r
t t,i t,i
customers decide to purchase (or continue) the service. The total profit at round t is
(cid:88) (cid:88)
Profit := Profit := r (p d −c ). (31)
t t,i t,i t t,i t,i
i∈[n] i∈[n]
Let r be the expectation of the exogenous r and d (c ,p ) be the conditional expectation of
t,i t,i t,i t,i t
d given promotional credit c and price p . Then, the (conditional) expected profit at round t is
t,i t,i t
(cid:88) (cid:88) (cid:0) (cid:1)
Profit (c ,p ) := Profit (c ,p ) := r p d (c ,p )−c . (32)
t t t t,i t,i t t,i t t,i t,i t t,i
i∈[n] i∈[n]
Regret bound. As in the targeted marketing problem, we consider regret upper bounds in two
cases: when d (·,·) is monotonic with respect to both arguments (Assumption 1), or it is concave
t,i
to the first argument, the promotional credit, and monotonic with respect to the second argument,
theprice(Assumption2). Theexpectedprofit(32)isalocallyrescaledversionofanexpectedprofit
for the targeted marketing problem: p d (c ,p )−c . Hence, it is straightforward to see that our
t t,i t,i t t,i
algorithms are still no-regret with respect to the profit defined above without any modification.
Theorem C.2. Let
(cid:34) (cid:35)
(cid:88) (cid:88)
R := sup Profit (c,p)−E Profit .
T t t
(c,p)∈[0,1]n+1
t∈[T] t∈[T]
Under Assumption 1, running Algorithm 1 with ℓ defined in terms of (31) guarantees R ∈
t,i T
O(nT3/4logT) when γ := η,K ∈ Θ(T1/4),η ∈ Θ(T−3/4). Similarly, under Assumption 2, running
Algorithm 2 with the aforementioned ℓ guarantees R ∈ O(nT2/3logT) when γ := ηlog(e/ϵ),ϵ :=
t,i T
T−2,δ := T−1,K ∈ Θ(T1/3) and η ∈ Θ(T−2/3).
C.3 Profit-maximizing A/B test
Setting. Suppose a firm performs a sequence of experiments where it chooses M marketing
alternatives for n population segments to increase demand of a product. Also, the firm wants to
perform these experiments while maintaining a common price for the product. Assume that each
alternative m ∈ [M] costs c (m) ∈ [0,1] to implement in round t. For example, when M = 2, the
t
first option (m = 1) could be to show the product on a non-interactive webpage, while the second
option (m = 2) could be to present the product on an AI-assisted interactive webpage. In this
scenario, the second option costs more as it requires more computing resources.
For each i ∈ [n] and t ∈ [T], let d ∈ [0,1] be a random variable representing the normalized
t,i
demands made by population segment i during round t, given a choice of alternative m ∈ [M]
t,i
and price p ∈ [0,1]. Moreover, let d (m ,p ) be the conditional expectation of d given m and
t t,i t,i t t,i t,i
p .
t
The firm’s total profit at round t is
(cid:88) (cid:88)
Profit := Profit := p d −c (m ), (33)
t t,i t t,i t t,i
i∈[n] i∈[n]
41and the expected profit at round t given m := (m ,...,m ) ∈ [M]n and p ∈ [0,1] is
t t,1 t,n t
(cid:88) (cid:88)
Profit (m ,p ) := Profit (m ,p ) := p d (m ,p )−c (m ).
t t t t,i t,i t t t,i t,i t t t,i
i∈[n] i∈[n]
The firm’s regret with respect to the best choice of an alternative for each segment and price is
(cid:34) (cid:35)
(cid:88) (cid:88)
R := sup Profit (c,p)−E Profit .
T t t
(k,p)∈[M]n×[0,1]
t∈[T] t∈[T]
Modification. This problem can be thought of as a version of targeted marketing with discrete
ancillary variables. It turns out that we can still use Algorithm 1 with a small modification to solve
this problem. The modification is as follows: rename variables c → m , c → m in Algorithm
t,i t,i i i
1. For all p ∈ I , set q (m |p) ← Uniform([M]) and let q (m |p) be supported over [M] for all
K 1,i i t,i i
t ∈ [T]. Losses are given by ℓ := 1(1−Profit ) where Profit is defined in (33).
t,i 2 t,i t,i
Regret bound. With this modification, the regret guarantee for this problem is as follows:
√
TheoremC.3. WiththeaforementionedmodificationofAlgorithm1, R ∈
O(cid:0)
n
MT2/3log(MT)(cid:1)
√ T
when γ := η(cid:112) M/K, η ∈ Θ(cid:0) T−2/3/ M (cid:1) and K ∈ Θ(T1/3).
Proof. By revising the proof of Theorem 3.1 with the aforementioned modification,
(cid:18) (cid:19)
n
R ∈ O nηMKT + logMK .
T
η
The choice of η and K implies the theorem statement.
42