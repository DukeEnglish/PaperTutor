Can a Confident Prior Replace a Cold Posterior?
MartinMarek1 BrooksPaige1 PavelIzmailov2
Abstract
Benchmarkdatasetsusedforimageclassification
tendtohaveverylowlevelsoflabelnoise.When
Bayesian neural networks are trained on these
datasets,theyoftenunderfit,misrepresentingthe
aleatoricuncertaintyofthedata. Acommonso-
lution is to cool the posterior, which improves
fit to the training data but is challenging to in- class A
class B
terpretfromaBayesianperspective. Weexplore
DirClip(1.4)
whether posterior tempering can be replaced by
DirClip(0.6)
a confidence-inducing prior distribution. First,
we introduce a DirClip prior that is practical to Figure1.Decision boundaries of a Bayesian neural network
sample and nearly matches the performance of usingtheDirClipprior.Byvaryingtheconcentrationparameter
a cold posterior. Second, we introduce a con- oftheprior,wecancontrolthemodel’saleatoricuncertainty,lead-
fidence prior that directly approximates a cold ingtodifferentdecisionboundaries. Theplotteddecisionbound-
likelihoodinthelimitofdecreasingtemperature aries were obtained using Hamiltonian Monte Carlo, using the
datasetfromFigure1ofKapooretal.(2022).
butcannotbeeasilysampled. Lastly,weprovide
severalgeneralinsightsintoconfidence-inducing
priors,suchaswhentheymightdivergeandhow
fine-tuning can mitigate numerical instability.
Inaregressionsetting,tuningthemodel’slevelofaleatoric
We share our code and weights at https://
uncertaintyisacommonpractice,forexamplebymodify-
github.com/martin-marek/dirclip.
ingthekernelofaGaussianprocessorbytuningthescale
parameter of a Gaussian likelihood (Kapoor et al., 2022).
1.Introduction Incontrast,inaclassificationsetting,weareforcedtouse
thecategoricallikelihood,whichhasnotunableparameter
WhenperformingBayesianclassification,itisimportantto to control the level of aleatoric uncertainty. The common
tunethemodel’slevelofaleatoric(data)uncertaintytocor- solution in practice is to temper the posterior distribution,
rectly reflect the noise in the training data. For example, which softens/sharpens the likelihood. However, Wenzel
consider the binary classification problem in Figure 1. If et al. (2020) argued that tempering is problematic from a
webelievethatallofthedatapointswerelabeledcorrectly, Bayesianperspectiveasit1)deviatesfromtheBayesposte-
wewouldpreferamodelthatperfectlyfitsthedata,using rior,and2)correspondstoalikelihoodfunctionthatisnot
acomplexdecisionboundary. Incontrast,ifweknewthat a valid distribution over classes. In this paper, we aim to
the data labels were noisy, we might assume that the true showthatwecanmatchtheresultsofposteriortempering
decisionboundaryisactuallysimpler,andthetwograyob- withinthestandardBayesianframework,byintroducinga
servationsweremislabeled. Bothofthesedecisionbound- valid prior distribution that directly controls the aleatoric
aries provide reasonable descriptions of the data, and we uncertaintyofaBayesianneuralnetwork(BNN).
can onlychoose between thembased on ourbeliefs about
Our paper is heavily inspired by and aims to extend the
thequalityofthedatalabels.
workofKapooretal.(2022). Kapooretal. haveprovided
1Department of Computer Science, University College Lon- aclearmechanisticexplanationofhowdataaugmentation
don, London, United Kingdom 2Computer Science and En- causes underfitting, as well as introducing the idea of us-
gineering Department, NYU Tandon School of Engineering,
ingaDirichletpriortocontrolthealeatoricuncertaintyof
New York, USA. Correspondence to: Martin Marek <mar-
aBNN.WeshowthatthedensityoftheDirichletpriorthat
tin.marek.19@ucl.ac.uk>.
they proposed is unbounded, leading to an improper pos-
terior, which caused the numerical instability in their ex-
1
4202
raM
2
]GL.sc[
1v27210.3042:viXraCanaConfidentPriorReplaceaColdPosterior?
periments. As a result, Kapoor et al. (2022) used an ap- overparameters. Let’sdenotethemodelparametersθ,in-
proximation, which we show cannot be viewed as purely puts (e.g., images) X and labels Y. Then the posterior is
a prior modification. In this work, we propose a simple proportionaltoapriortimesalikelihood:
modificationoftheDirichletprior,thatfixesthesourceof
theinstability,allowingustomatchtheresultsofthecold p(θ|X,Y) ∝ p(θ|X) p(Y|θ,X).
(1)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
posteriorwithapuremodificationoftheprior,withoutthe posterior prior likelihood
needfornumericalapproximations.
The prior can depend on the input data X but not on the
We introduce two different and entirely valid prior distri- data labels Y. In practice, the dependence on X is often
butionstocontrolthealeatoricuncertaintyofaBNN.First, ignored, for example by setting a Normal prior over pa-
we introduce the DirClip prior—a clipped version of the rameters. However,thedependenceoftheprioronthein-
Dirichlet prior that can be practically sampled and nearly put data is crucial for functional priors (evaluated on the
matches the performance of a cold posterior. We explain trainingdata)liketheDirichletprior(Kapooretal.,2022)
why clipping (bounding) the prior density is necessary to discussedinSection5.
obtain a valid posterior distribution. We also show that
Thedistributionovertheresponsevariabley˜,givenanew
the prior gradient can sometimes dominate the likelihood
observationx˜andtrainingdata(X,Y)isgivenbythepos-
gradient,leadingtounstabletrainingdynamics. Wederive
teriorpredictivedistribution:
exactly when this instability happens and show how ini-
tializing the MCMC sampler with a pretrained model can (cid:90)
stabilizetraining. p(y˜|x˜,X,Y)= p(y˜|x˜,θ)p(θ|X,Y)dθ. (2)
Second, we introduce a confidence prior, which directly
Unfortunately, Eq. (2) requires integrating over the high-
enforceslowaleatoricuncertaintyonthetrainingdata. We
dimensionalparametersθanddoesnothaveaclosed-form
show that in the limit of decreasing temperature, the con-
solution. Hence, we must resort to numerical methods to
fidencepriorcombinedwithuntemperedcategoricallikeli-
approximate it. A popular approach is Monte Carlo sim-
hood converges to a cold likelihood. However, the confi-
ulation: we express the integral as an expectation of the
dencepriorhasmanylocalmaxima,whichmakesdirectly
likelihoodovertheposteriordistribution,drawN samples
sampling it nearly impossible. While this prior cannot be
fromtheposterior,andcomputetheempiricalmeanofthe
used practically, it provides a theoretical justification for
likelihoodoverthesesamples:
cold posteriors (which are easy to sample), showing they
approximate a valid prior distribution in the limit of de-
p(y˜|x˜,X,Y)=E [p(y˜|x˜,θ)]
θ|X,Y
creasingtemperature.
≈(cid:88)N p(y˜|x˜,θ i)
,θ ∼p(θ|X,Y).
(3)
2.Background N i
i=1
In this section, we introduce the relevant background on In this paper, we use Stochastic Gradient Hamiltonian
Bayesianneuralnetworksandcoldposteriors. Monte Carlo (SGHMC) to draw the posterior samples
andthereforeapproximatethepredictivedistribution(Chen
2.1.Bayesianneuralnetworks etal.,2014). WefurtherdiscusstheSGHMCalgorithmin
Appendix H.2 and provide our implementation details in
Bayesian neural networks are an exciting framework for
AppendixG.
both understanding and training neural networks that are
more reliable (Mackay, 1992; Neal, 2012; Wilson & Iz-
2.2.Coldposteriors
mailov, 2020). Modern neural networks are typically
overparameterized—therearemanydifferentsetsofmodel Acoldposteriorisachievedbyexponentiatingtheposterior
parameters that fit the training data perfectly but disagree to 1/T, where T < 1. Since the posterior factorizes into
onunseendata(Garipovetal.,2018;Fortetal.,2019;Wil- a prior and a likelihood, a cold posterior can be seen as a
son&Izmailov,2020;D’Amouretal.,2020). Asaresult, combinationofacoldlikelihoodwithacoldprior:
usinganysinglesetofparameterstogeneratepredictionsis
problematic: itignoresouruncertaintyovermodels, lead- p(θ|X,Y)1/T ∝p(θ|X)1/Tp(Y|θ,X)1/T. (4)
ing to over-confident predictions. In a BNN, we consider
In practice, a cold posterior is typically sampled by mod-
the full distribution over possible models, leading to both
ifyingtheSGHMCsamplingalgorithminsteadofdirectly
improved accuracy and uncertainty estimation (Izmailov
scalingthelog-posterior,whichcouldintroducenumerical
etal.,2021b).
instability(detailsareprovidedinAppendixH.2). Temper-
AtrainedBNNisfullydefinedbyitsposteriordistribution ing a Normal prior is equivalent to adjusting its variance
2CanaConfidentPriorReplaceaColdPosterior?
by a factor of T. Wenzel et al. (2020) showed that the etal.,2009;Adlametal.,2020).YetwhenaBNNistrained
standard posteriors corresponding to T = 1 lead to poor onCIFAR-10withdataaugmentationturnedon,theBNN
performance in image classification, while cold posteriors willtendtooverestimatethedataset’saleatoricuncertainty,
withT <1providemuchbetterperformance.Theynamed resulting in underfitting (Kapoor et al., 2022). This holds
thisobservationthecoldposterioreffect. trueacrossawiderangeofpopularpriordistributions(For-
tuinetal.,2021). Infact, Zhangetal.(2023)haveshown
InthelimitofT →0,thecoldposteriorapproachesadeep
thatthepresenceofacoldposterioreffectdirectlyimplies
ensemble(Lakshminarayananetal.,2017). Therearetwo
thattheuntemperedBayesposteriorunderfits.
waystoseethis: first,acoldtemperaturesharpensthepos-
terior.AsT →0,theexponentapproaches∞,sothedistri- Aitchison (2021) has argued that the cold posterior ef-
butionbecomesincreasinglysharp,approachingadistribu- fectdirectlyarisesfromthecurationprocessofbenchmark
tionwithpointmasseslocatedatposteriormodesandequal datasetslikeCIFAR-10.Theargumentisthatthesedatasets
tozeroeverywhereelse(i.e.adeepensemble).Atthesame consist entirely of images with clean labels, often requir-
time,asT → 0,theSGHMCnoisescaleapproacheszero, ing the consensus of multiple labelers. They developed
andSGHMCbecomesequivalenttoSGDwithmomentum. a statistical theory which shows that to correctly account
for this curation process, BNNs should in fact be using
2.3.Whatiswrongwithcoldposteriors? coldposteriors. However,thisargumentisdisputedbyIz-
mailovetal.(2021b)whoobtainedextremelyhigh-fidelity
Intheclassificationsetting,weuseacategoricallikelihood
ResNet20 (He et al., 2016) posterior samples on CIFAR-
to represent the predicted probability for each class. A
10,observingnocoldposterioreffect.
coldposteriorcorrespondstousingacoldcategoricallike-
lihood,whichisnotavaliddistributionoverclasses.Afun- Priormisspecification. Intuitively,aNormalpriorisnot
damentalpropertyofanyprobabilitydistributionisthatthe affected by the dataset size but the influence of the likeli-
sumofprobabilitiesoverallpossibleoutcomesequalsone. hoodtermscaleslinearlywiththedatasetsize. Therefore,
In cold likelihoods, class probabilities can sum to values byvaryingthedatasetsize,wecanchangetherelativein-
significantly less than one, making them invalid as proba- fluence of the prior. Noci et al. (2021) trained a BNN on
bilitydistributions(Wenzeletal.,2020). subsetsofadatasetofvaryingsizeandobservedthatasthe
dataset size decreased, the strength of the cold posterior
Wilson & Izmailov (2020) argued that posterior temper-
effectincreased. ThisobservationimpliesthattheNormal
ingcanbeviewedasadjustingformodelmisspecification.
priorismisspecified,especiallywhenthedatasetissmallor
Kapooretal.(2022)studiedcoldposteriorsindetail,andin
themodelislarge. Wenzeletal.(2020)andFortuinetal.
particularshowedthattemperingcancounteracttheeffect
(2021)alsoarguedthatpriormisspecificationcouldbeone
of data augmentation. In this work, we build on Kapoor
ofthekeyreasonsbehindthecoldposterioreffect.
etal.(2022)andshowthatitispossibletoachieveasimi-
larperformancetothatofcoldposteriorsusingthestandard Improvedpriordistributions. Basedonthepriorwork
likelihoodfunctionwithamodifiedprior. discussed above, we conclude that the cold posterior ef-
fect has multiple possible causes, although they all seem
to stem from underfitting (Zhang et al., 2023) or overesti-
3.Relatedwork
matingthealeatoricuncertaintyofthetrainingdata(Adlam
Coldposteriorsanddataaugmentation. Izmailovetal. etal.,2020). Wethereforesearchforasolutionintheform
(2021b) exactly repeated the experiments of Wenzel et al. ofanimprovedpriordistribution,toimprovefittothetrain-
(2020) but with data augmentation turned off, which en- ingdata.
tirely eliminated the cold posterior effect. Kapoor et al.
Fortuin et al. (2021) examined the distribution of neural
(2022) have shown that naively implementing data aug-
networkweightstrainedusingSGDunderauniformprior.
mentationresultsinundercountingthetrainingdata,which
Theyempiricallydiscoveredthatusingheavy-tailedorcor-
softens the likelihood and directly leads to underfitting.
relatedpriorscanleadtoimprovedBNNperformance. In
One way to counteract this effect is to use a cold poste-
contrast,thegoalofthispaperistotheoreticallydeducea
rior, which sharpens the likelihood and directly compen-
priordistributionthatdirectlyimprovesthefitonthetrain-
sates for the undercounting. Nabarro et al. (2022) pro-
ingdata.
posed a non-standard principled version of data augmen-
tationandshowedthatthecoldposterioreffectremainsin Kapooretal.(2022)introducedtheideaofusingaDirich-
theirmodel,suggestingthatotherfactorscancontributeto letpriortocontrolthealeatoricuncertaintyofaBNN,and
thecoldposterioreffect. therefore remove the need for posterior tempering. Our
work is directly inspired by this approach, although we
Label noise. Benchmark datasets used for image classi-
aim to provide a deeper understanding of the Dirichlet
ficationlikeCIFAR-10havehighlabelquality(Krizhevsky
3CanaConfidentPriorReplaceaColdPosterior?
prior. We show that the approximation to the Dirichlet 1
posterior confidence prior introduced by Kapoor et al. uses a quadratic like-
prior confidence
lihood term, therefore deviating from a valid distribution 99.9% 0.1
overclasses. Weexplainexactlywhyusinganunmodified
Dirichletpriorresultsindivergenceandusetheseinsights 99% 0.01
todevelopavalidnewpriordistribution.
90% 0.001
4.ConfidenceofaNormalprior
10%
MostpriorworksstudyingBayesianneuralnetworksused 0.0001
0.1 1 10
isotropic Normal priors (Wenzel et al., 2020; Noci et al.,
prior st. dev.
2021;Sharmaetal.,2023)orothervaguedistributionsover
Figure2.ConfidenceofResNet20trainedonCIFAR-10witha
parameters,suchasaMixtureofGaussians,logisticdistri-
Normalprior. Thedashedlineshowstheaverageconfidenceof
bution (Izmailov et al., 2021b), Laplace distribution, Stu- priorsamplesasafunctionofthepriorscale(standarddeviation).
dent’s t-distribution (Fortuin et al., 2021), or a correlated Therelationshipisone-to-one:thepriorscaleexactlydetermines
Normal distribution (Izmailov et al., 2021a; Fortuin et al., prior confidence. Conversely, the prior scale has almost no ef-
2021). We show that these distributions can have high fectonposteriorconfidence—eachscatterpointcorrespondstoa
prior confidence, while simultaneously having low poste- single trained model. Here, the intuition that “prior confidence
rior confidence. This implies that the link between prior translatesintoposteriorconfidence”fails. Instead, theposterior
confidencedependsmostlyontheposteriortemperature, visual-
and posterior confidence is not trivial, and using a confi-
izedusingthecolorbarontheright.
dentpriordoesnotguaranteehighposteriorconfidence.
We performed a simple experiment where we varied the
scale of the Normal prior, sampled ResNet20 parameters 5.Dirichletprior
fromtheprior,andevaluateditspredictionsonthetraining
setofCIFAR-10.ThedashedlineinFigure2showstheav- In a Bayesian classification setting, if we wish to control
erageconfidenceofpriorsamplesasafunctionoftheprior a model’s level of aleatoric uncertainty, the standard ap-
scale. Intuitively,asthepriorscaletendstozero,themodel proach is to use a Dirichlet prior, which can bias the pos-
parameters tend to zero, causing logits to approach zero, teriortoeitherhavelowerorhigherconfidence. Morefor-
therebyinducinguniformpredictionsoverclassprobabili- mally,theDirichletdistributionisadistributionoverclass
ties. Conversely,asthepriorscaleincreases,modelparam- probabilities. Let’sdenoteamodel’spredictedclassprob-
eters grow in magnitude, scaling up the logits. When the abilitiesasyˆ = (yˆ 1,yˆ 2...yˆ K),whereK isthenumberof
scaleoflogitsislarge,theabsolutedifferencesbetweenthe classes. TheDirichletpriorassignsaprobabilitydensityto
logitsgrow,leadingtohighconfidence.
anysetofpredictionsyˆ:
ObserveinFigure2thatpriorconfidencedoesnottrivially K
c (cid:88)
translateintoposteriorconfidence. Eachpointcorresponds logp(yˆ)= (α−1)logyˆ k. (5)
to a single posterior distribution, with color representing k=1
the posterior temperature. All untempered (T = 1) pos-
TheDirichletpriorisparameterizedbyascalarconcentra-
teriorshavelowconfidence,irrespectiveofthepriorscale.
tion parameter α that biases the predictions toward lower
Whenacoldtemperatureisused,itispossibletoobtaina
(α > 1)orhigher(α < 1)confidence.2 Whenα = 1,the
modelthathaspriorconfidencenear10%(thelowestpos-
Dirichletpriorisequaltoauniformprior.
sible)andposteriorconfidenceof99%.
Kapoor et al. (2022) have observed that directly using the
TheseresultsshowthatifwewanttotrainaBNNthathas
Dirichletpriorasaprioroverparametersp(θ) = p(yˆ)re-
highposteriorconfidenceatT =1,itisnotsufficienttouse
sultsindivergence. TheissueisthattheDirichletpriorisa
apriorwhosesamplesareconfident. Onewaytoincrease
distributionovermodelpredictions,whereastheBNNprior
the posterior confidence further is to use a prior that will
as defined in Eq. (1) is a distribution over model parame-
directly assign a high probability density to models with
ters.Sohowcanwetranslateadistributionoverpredictions
high confidence and a low probability density to models
with low confidence. This approach requires a functional tion,itishelpful(althoughnotstrictlynecessary)todirectlythink
prior,i.e.apriordistributionthatisdefinedoverfunctions aboutthepriorinfunctionspace.
(modelpredictions),ratherthanmodelparameters.1 2In general, the concentration parameter is a vector
(α ,α ...α )withonecomponentperclass. Whenthecom-
1 2 K
1In theory, every distribution over model parameters corre- ponents are unequal, the prior has a different bias toward each
sponds to some distribution over functions, and vice versa. We class. Sinceweareinterestedinasymmetricprior,wewillonly
arguethatinordertodesignaconfidence-inducingpriordistribu- considerthecasewhereeachcomponentofthisvectorisequal.
4
atad
niart
no
ecnedifnoc
.gva
temperatureCanaConfidentPriorReplaceaColdPosterior?
prior likelihood posterior
10
Dirichlet 0 0
5 DirClip 50
NDG 10
0 confidence 100 150 20
5 200 categ. like.
30
250 conf. + categ.
10 cold like.
300 categ. like. 40 NDG + categ.
15 350 cold like. NDG + NDG
NDG like. 50 Dir. + categ.
400
20
0.00000001 0.001 0.5 0.999 0.99999999 0.00000001 0.001 0.5 0.999 0.99999999 0.00000001 0.001 0.5 0.999 0.99999999
yy: prob. of true class yy: prob. of true class yy: prob. of true class
Figure3.Slicesofvariousprior, likelihood, andposteriordistributions. Foreachdistribution, weassumethatthereareonlytwo
classes and we vary the predicted probability of the true class on the x-axis. Since the prior has no notion of the “true” class, it is
symmetric. Notethatthex-axisisnon-lineartobettershowthetailbehaviorofeachdistribution. Notably,theNDGpriorpeaksata
verysmall(andlarge)valueofpredictedprobability,whichwouldnotbevisibleonalinearscale. Theblueandgreenstarsintheleft
andrightplotsshowlocalmaxima.
p(yˆ)intoadistributionoverparametersp(θ)? Ingeneral, Train accuracy Test accuracy
thisisadifficultproblemwithnosimplesolution;wedis- 100% 94%
97% 93%
cussthisfurtherinAppendixE.1. Intherestofthispaper, 94% 92%
wesearchforapriorthatworkswellwhenapplieddirectly 91% categorical (T=0) 91%
categorical (T=1) 90%
overmodelparameters. 88% NDG (logits) 89%
85% N ND DG G ( lil ko eg lp ihr oo ob ds) 88%
To understand why the Dirichlet prior diverges when ap- 82% NDG prior 87%
plied over model parameters, we plot its probability den-
105 104 103 102 101 105 104 103 102 101
prior concentration prior concentration
sityfunction(PDF)ontheleftsideofFigure3. Noticethat Figure4.Factorized NDG. This figure shows the accuracy of
the probability density diverges to ∞ as yˆapproaches ei- ResNet20onCIFAR-10withdataaugmentationforvariousBNN
ther 0 or 1. For a probability distribution to be valid, its posteriors. Each posterior consists of a N(0,0.12) prior over
PDFneedstointegratetoonebutthePDFdoesnotneces- modelparameters,a(modified)likelihood,andoptionallyanad-
sarilyneedtobebounded. Indeed,ifwetreattheDirichlet ditionalpriortermoverpredictions.Themodelusingthestandard
priorasadistributionoverthedomainyˆ∈ [0,1],thePDF categorical (T=1) likelihood provides a simple baseline. The
NDGposteriormodelsdefinedoverlogitsandlog-probabilities4
integrates to one over this domain, and it is, therefore, a
both reach the same test accuracy, on par with a cold posterior.
validdistribution. ThefactthatthePDFisunboundedbe-
In contrast, the NDG prior and likelihood on their own do not
comes an issue when we treat the PDF as a distribution
matchtheperformanceofacoldposterior. Notethatthetraining
over parameters because the domain of model parameters
accuracywasevaluatedonposteriorsamples,whereasthetestac-
isθ ∈ (−∞,∞). IfweattempttointegratethePDFover
curacywasevaluatedontheposteriorensemble.
the domain of model parameters, we get an integral that
diverges, meaning that the distribution is invalid. Even
priordistributionorbecauseoftheimpliedquadraticlike-
worse,theposteriordistributionthatresultsfromthisprior
lihood? To answer this, we perform a simple experiment.
is also invalid and hence impossible to sample. We prove
First, we train a model that uses the NDG prior together
thedivergenceoftheDirichletpriormoreformallyinAp-
with the categorical likelihood. Second, we train a model
pendixE.2.
that uses the NDG quadratic likelihood. Figure 4 shows
thatneithertheNDGpriornortheNDGlikelihoodontheir
6.NoisyDirichletGaussian
owncanmatchthetestaccuracyofacoldposterior.
To fix the divergence of the Dirichlet prior, Kapoor et al. ThereasonthattheNDGpriorworksatall(unlikeaDirich-
(2022) decided to approximate the product (posterior) of letprior)isthatitsdensityisbounded. Thiscanbeseenin
theDirichletpriorwithacategoricallikelihood.Theyname the left plot of Figure 3: whereas the Dirichlet prior di-
this method Noisy Dirichlet Gaussian (NDG). In their ex- verges toward each tail of the distribution, the NDG prior
periments,NDGcloselymatchestheperformanceofacold densityisboundedbyalocalmaximumateachtailofthe
posterior. distribution. Ifwecombineanyboundedpriordistribution
withaNormalprior(withanarbitrarilylargescale),weget
However,weshowinAppendixCthattheNDGposterior
avalidpriordistributionoverparameters.5
approximationcorrespondstousingavalidpriorcombined
withaquadraticlikelihoodterm. Thisraisesthequestion: 4WediscussthedifferencebetweentheNDGposteriordefined
doestheNDGposteriorworksowellbecauseofitsimplied overlogitsandlog-probabilitiesinAppendixC.
5
ytisned
golCanaConfidentPriorReplaceaColdPosterior?
Train accuracy Test accuracy
100%
94%
98%
96% 93%
94% categorical (T=0) 92% + =
92% categorical (T=1) 91%
-50 (rand. init.)
90% -50 (finetuned) 90%
88% -20 (finetuned)
-10 (finetuned) 89%
86%
-0.5 0.2 0.6 0.8 0.92 1 -0.5 0.2 0.6 0.8 0.92 1
prior concentration prior concentration
Figure5.DirClip accuracy. This figure shows the accuracy of
ResNet20onCIFAR-10withdataaugmentationforvariousBNN + =
posteriors. Each solid line corresponds to a different clipping
valueoftheDirClipprior(printedinthelegend). Theblueline
showsDirClipposteriorssampledfromrandominitialization;all a) categ. like. b) Dir(0.01) prior c) Dir(0.01) posterior
other DirClip posteriors are fine-tuned from a checkpoint with Figure6.Top:probabilitydensityofa)categoricallikelihood,b)
100%trainingaccuracy.Notethatthetrainingaccuracywaseval- Dirichletprior,c)Dirichletpriorcombinedwithcategoricallike-
uatedonposteriorsamples,whereasthetestaccuracywasevalu- lihood. Each distribution is defined over three classes. The top
atedontheposteriorensemble. corner corresponds to the correct class and the two bottom cor-
ners correspond to the two other classes. Bottom: vector field
showsthedirectionandmagnitudeofgradientscomputedinlogit
7.ClippedDirichletprior spaceandreprojectedbacktotheprobabilitysimplex. Thecol-
oredlinesshowthetrajectoriesof50randomlysampledparticles
The NDG prior provides an approximation to the Dirich- underthisgradientfield.
let prior that fixes its divergence. However, there exists a
simplermethodtofixthedivergenceoftheDirichletprior:
wecanclipeachlog-probabilityatsomesmallvaluev(for
example,v =−10): Results. We show in Figure 1 that the DirClip prior can
be used to control the level of aleatoric uncertainty on a
K
c (cid:88) toybinaryclassificationdataset. InFigure5,weshowthat
logp(yˆ)= (α−1)max(logyˆ ,v). (6)
k theDirClippriorcanbeusedtocontrolthetrainingaccu-
k=1 racy of a ResNet20 trained on CIFAR-10. When α = 1,
the DirClip prior is equivalent to a uniform prior, and the
Sincethelog-densityoftheDirichletpriorisproportional
modelunderfits. Byincreasingthepriorconcentration(re-
tothesumofper-classlog-probabilities,clippingeachlog-
ducingα),wecanincreasetrainingaccuracyfrom88%to
probability bounds the prior density. We call this prior
99%andtestaccuracyfrom89%toalmost94%. However,
DirClip,asin“DirichletClipped”.Notethatasimilarmod-
noticeinthefigurethatthemodeltrainedfromrandomini-
ificationoftheDirichletdistributionhasbeenproposedto
tializationonlyconvergeswhenα > 0.8. Whenα < 0.8,
allow the concentration parameter α to become negative,
theaccuracysuddenlydropsallthewaydownto10%(cor-
althoughitwasneverappliedasapriorovermodelparam-
responding to random guessing). This sudden drop in ac-
eters(Tu,2016). TheDirClippriorisvisualizedintheleft
curacy is not a fundamental property of the DirClip prior;
plotofFigure3:itisidenticaltotheDirichletpriorforlog-
instead, it can be entirely attributed to the challenges of
probabilitiesbetweentheclippingvaluevanditstaysatthe
gradient-basedsamplingalgorithms.
clipping value otherwise. We define the DirClip posterior
asfollows:
7.1.Trainingstability
evaluatedperobservation
(cid:89)N (cid:122) (cid:125)(cid:124) (cid:123) Figure 6 shows the paths that 50 randomly sampled par-
p(θ|X,Y) ∝ p(θ) p(yˆ(i)) yˆ(i). (7) ticles would take along the domain of various probability
y
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) i=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) distributions under gradient descent in logit space. No-
DirClip Normal DirClip categorical
posterior prior prior likelihood tice that the gradient of the categorical likelihood always
points toward the top corner (correct class). In contrast,
the Dirichlet prior has no notion of a “correct” class, and
itsgradientsimplypointstowardwhichevercorner(class)
5SincetheNormalpriorisaproperprior,evenifwecombineit is the closest. When the Dirichlet prior is combined with
withanimproperpriordistribution,thejointpriorwillbeproper.
thecategoricallikelihood,thecombinedgradientdoesnot
Inpractice,thereislittledifferencebetweenusingtheNDGprior
necessarily point toward the correct class. For example,
on its own vs. combining it with a large-scale Normal prior; it
issimplyworthrealizingthatthelatterpriorisguaranteedtobe in Figure 6c, around 30% of uniformly-sampled particles
properandthereforeresultinaproperposterior. wouldconvergetoeitherofthebottomcornersundergra-
6
ytisned
tnioj
tneidargCanaConfidentPriorReplaceaColdPosterior?
1.0 Tempered categ. likelihood Confident prior + categ. likelihood
1.0 1
0.8 0.8
0.6
+
0.1
0.6 0.4
0.2
0.4
- 0.0 0.01 0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
0.2 yy: prob. of true class yy: prob. of true class
Figure8.Left: Densityofthecategoricallikelihoodtemperedto
0.0 varioustemperatures(shownbythecolorbarontheright).Acold
0.0 0.2 0.4 0.6 0.8 1.0
likelihood (T < 1) is “sharp” and penalizes wrong predictions
prob. of wrong class
heavily. Theredlineshowstheuntemperedlikelihood(T = 1).
Figure7.DoesagradientstepontheDirichletposteriorincrease
Right: Anupperboundontheproductofaconfidentprior(pa-
the probability of the correct class? Assume that there are 10
rameterized using temperature) with the untempered categorical
classes. The x-axis shows the probability of a wrong class; the
likelihood.Notethatforyˆ >0.5,thetwodensitiesareidentical.
other9classes(oneofwhichisthecorrectclass)havethesame y
probability. Theimagecolorshowsthechangeintheprobability
ofthecorrectclassunderagradientstep. Greenmeansthatthe
worksforα < 0.8.6 ForfurtherdiscussionoftheDirClip
probabilityofthecorrectclasswillincrease;redmeansthatitwill
decrease. Noticethatforα > 0.8,theprobabilityofthecorrect prior,pleaseseeAppendixB.
classalwaysincreases.Forα<0.8,theprobabilityofthecorrect
classmayincreaseordecrease,dependingonthex-axisvalue. 8.Confidenceprior
WhiletheDirClippriorcansuccessfullycontrolamodel’s
levelofaleatoricuncertainty,itachievesthisinaveryindi-
dientdescent. However,ifwedirectlysampledtheDirich- rectway. Toseethis,considertheprobabilitydensitythat
let posterior, only 1% of the samples would appear in the the DirClip(0.01) prior assigns to the following two pre-
bottom half of the plot. Therefore, this behavior of parti- dictions:
clesconvergingtothewrongclassispurelyanoptimization p((99%,0.5%,0.5%))≈3.6
(8)
problem, rather than a statistical property of the Dirichlet p((49.99999%,49.99999%,0.00002%))≈1102.
distribution.
The probability density assigned to the second prediction
ThesamebehaviortranslatestoaBayesianneuralnetwork
is roughly 300-times higher even though it only has half
optimized(sampled)usinggradient-basedmethods. Ifthe
theconfidenceofthefirstprediction(∼50%vs.99%). The
neuralnetworkpredictsawrongclassatinitialization,op-
issueisthattheDirClipprior(andtheDirichletdistribution
timizingtheparametersoftheneuralnetworkmayfurther
ingeneral)assignsahighdensitytopredictionswithsmall
increase the model’s predicted probability for that class.
probabilitiesratherthanpredictionswithhighconfidence.
Themodeldoesnotnecessarilyconvergetopredictingthe
This can be seen directly by looking at the Dirichlet PDF
correctclass.
in Eq. (5): when α < 1, the density is high when any of
In the case of the Dirichlet posterior, we can analytically thelog-probabilitiesisextremelysmall. Inparticular,high
derivewhetheragradientstepwillincreaseordecreasethe densityoftheDirichletdistributiondoesnotdirectlyimply
probability of the true class (full derivation is provided in highconfidence.
AppendixF).Intuitively,ifamodel’spredictionisinitially
Instead of using a prior that encourages small log-
close to the true class, then gradient descent will increase
probabilities, we can design a prior that directly enforces
theprobabilityofthatclass. However,whentheprediction
highpredictionconfidence. Wecansimplysetthedensity
is initially close to a wrong class, the gradient may point
of the prior proportional to the confidence (i.e. the maxi-
toward the wrong class. Figure 7 visualizes this behavior
mumpredictedprobability):
using a phase diagram. If a neural network is randomly
initialized and α < 0.8, most of its predictions will fall (cid:18) (cid:19)1/T−1
insidethedivergingphase,meaningthosepredictionsmay p(yˆ)∝ maxyˆ k . (9)
k
notconvergetowardthetrueclass. Therearetwowaysto
fixthis: 1)useα > 0.8;or2)initializetheneuralnetwork Weintentionallyparameterizedthis“confidenceprior”us-
such that all of its predictions fall inside the converging
6fine-tuning is necessary but not sufficient for convergence
phase. Inpracticalterms,thismeansfine-tuningtheneural
withα < 0.8. Inparticular, wefoundthatsmallervaluesofα
network from a checkpoint that has 100% training accu- requireincreasinglysmallerlearningrates.Thestabilityphasedi-
racy. In Figure 5, we show that the fine-tuning strategy agraminFigure7onlyholdsforaninfinitelysmalllearningrate.
7
noitartnecnoc
roirp
ytisned
.tsop
temperatureCanaConfidentPriorReplaceaColdPosterior?
traininig accuracy categorical likelihood (T=1) cold likelihood (T=0.01) confidence posterior (T=0.01) DirClip( =0.7) posterior ×105
100% 0
80%
1
60%
40% 2
20%
3
Figure9.Posteriorlandscapes. EachplotshowsagraytrianglewhoseverticescorrespondtodifferentResNet20modelstrainedon
CIFAR-10usingSGD.Weevaluatethetrainingaccuracyandvariousdistributionsalongtheplanedefinedbytheparametersofthethree
trainedmodels.Thecolorbarontheleftshowstheaccuracyscaleandthecolorbarontherightshowsthedistributionlog-density.
ing T, so that when the predicted class matches the true Lastly, the DirClip posterior shows a surprising behavior,
class(whichisguaranteedtobetruewhenyˆ ≥ 0.5), the assigning the highest probability density to a model that
y
density of the confidence prior combined with categorical only has a 10% training accuracy. The reason is that the
likelihoodisequaltothedensityofacoldcategoricallike- top region of the plot happens to minimize the predicted
lihood: log-probabilitiesandthereforemaximizetheDirClipprior
density.
argmaxyˆ =y =⇒ p(yˆ)·yˆ =yˆ1/T. (10)
k y y
k Unfortunately, it is not possible to sample the confidence
posterior using SGHMC or related optimization methods.
OntheleftsideofFigure8,weplotthedensityoftempered The issue is that, unlike a cold likelihood, the confidence
categorical likelihood, which also provides a lower bound prior combined with categorical likelihood is riddled with
on the product of the confidence prior with untempered localmaxima. InFigure8,eachdotintherightplotshows
categorical likelihood. On the right side, we plot the up- alocalmaximum,usingalineary-axisscale. Whilethese
perbound. Noticethatasthetemperatureapproacheszero, maximamaynotseemtoosteep,theygetsteeperwithde-
bothdensitiesconvergetotheDiracdeltameasureconcen- creasingtemperature. TherightplotinFigure3showsthe
tratedat1. WeprovethisconvergenceinAppendixD. same local maximum on a logarithmic scale for a lower
temperature parameter
(cid:0)
T
=3·10−7(cid:1)
. In theory, if we
Recall from Eq. (4) that when a Normal prior and cate-
weresamplingpointsuniformlyunderthe“confidencepos-
goricallikelihoodisused, acoldposteriorisequivalentto
terior”curveinFigure3,morethan99.9999%ofthesam-
rescalingthepriorandtemperingthelikelihood. However,
pledpointswouldendupontheright. However,localop-
it is also possible to approximate the cold posterior with
timization methods such as SGD and SGHMC might get
a“confidenceposterior”. The“confidenceposterior”con-
stuckinthelocalminimumontheleft. Dependingonhow
sists ofa rescaledNormal prior, confidence prior, and un-
these local optimization algorithms are initialized, up to
temperedcategoricallikelihood:
50%ofthesampledpointsmightendupontheleft.7
approximatescoldlikelihood
(cid:122) (cid:125)(cid:124) (cid:123)
N (cid:18) (cid:19)1−1
p(θ|X,Y)T1 ≈p(θ|Tσ2)(cid:89) maxyˆ k(i) T yˆ y(i). (11) 9.Discussion
(cid:124) coldp(cid:123) o(cid:122) sterior(cid:125) (cid:124) res(cid:123) ca(cid:122) led (cid:125) i=1 (cid:124) k (cid:123)(cid:122) (cid:125) cat(cid:124) e(cid:123) go(cid:122) r(cid:125) ical When a BNN is trained on a curated dataset like CIFAR-
Normalprior confidenceprior likelihood 10, it might overestimate the aleatoric uncertainty of the
data, therefore underfitting. The standard solution is to
Infact,iftheposteriormodereaches100%accuracy,then
cool the posterior distribution, which improves fit to the
the confidence posterior and cold posterior have the same
training data. However, tempering might seem problem-
Hessian, implyingthatthetwodistributionshaveaniden-
aticfromatheoreticalpointofview,asitdeviatesfromthe
tical Laplace approximation. This follows directly from
Bayes posterior and corresponds to an invalid distribution
Eq.(10).
overclasses. Weshowthatwecanachievesimilarresults
In Figure 9, we visualize various posterior distributions. toposteriortemperingpurelywithinthestandardBayesian
Notice that the categorical likelihood is relatively diffuse,
7SamplingalgorithmslikeHMCgenerateunbiasedbutcorre-
assigning high probability density to models that deviate
lated posterior samples. In theory, the posterior approximation
from the SGD solutions. In contrast, both the cold likeli- getsmoreaccuratethelongerwerunthesamplingalgorithm. In
hood andtheconfidenceposterior arevery“sharp”—they practice,however,itmightbeinfeasibletorunthesamplingalgo-
are concentrated closely around the three trained models. rithmfor“longenough”.
8CanaConfidentPriorReplaceaColdPosterior?
framework, through the combination of the standard cate- Aitchison, L. A statistical theory of cold posteriors in
gorical likelihood with a confidence-inducing prior distri- deep neural networks. In International Conference
bution. on Learning Representations, 2021. URL https://
openreview.net/forum?id=Rd138pWXMvG.
DirClip prior controls aleatoric uncertainty. By clip-
ping the density of the Dirichlet prior, we fixed its diver-
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J.,
gence,leadingtoavalidpriordistributionthatcontrolsthe
Leary, C., Maclaurin, D., Necula, G., Paszke, A.,
level of aleatoric uncertainty. When applied to ResNet20
VanderPlas, J., Wanderman-Milne, S., and Zhang, Q.
on CIFAR-10, the DirClip prior nearly matches the accu-
JAX: composable transformations of Python+NumPy
racyofacoldposterior,withouttheneedforanytempering.
programs, 2018. URL http://github.com/
Wealsoexaminedthegradientfieldoftheprior,explaining
google/jax.
why it is difficult to sample for a small concentration pa-
rameter. Brooks,S.P.andGelman,A.Generalmethodsformonitor-
ingconvergenceofiterativesimulations.Journalofcom-
Confidenceposteriorapproximatesacoldposterior. By
putationalandgraphicalstatistics,7(4):434–455,1998.
observingthattheDirichletpriorencouragessmallproba-
bilities (rather than high confidence), we were able to de-
Chen, T., Fox, E., and Guestrin, C. Stochastic gradient
sign a prior that directly enforces high confidence. When
hamiltonianmontecarlo. InInternationalconferenceon
this prior is combined with a categorical likelihood, as
machinelearning,pp.1683–1691.PMLR,2014.
T → 0,theirproductconvergestoacoldlikelihood. This
allows us to see the cold posterior of a BNN from a new
D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-
perspective: notascorrespondingtoacoldlikelihood,but
panahi,B.,Beutel,A.,Chen,C.,Deaton,J.,Eisenstein,
instead as approximating the confidence prior combined
J., Hoffman, M. D., et al. Underspecification presents
withtheuntemperedcategoricallikelihood.
challenges for credibility in modern machine learning.
Summary. We acknowledge that cold posteriors are sig- arXivpreprintarXiv:2011.03395,2020.
nificantlyeasiertosamplethanconfidence-inducingpriors,
Fort, S., Hu, H., and Lakshminarayanan, B. Deep en-
andthereforeremainthepracticalsolutionwhenthemodel
sembles: A loss landscape perspective. arXiv preprint
is misspecified (Wilson & Izmailov, 2020). The goal of
arXiv:1912.02757,2019.
this paper was to show that 1) tempering is not necessary
toachievehighaccuracyonCIFAR-10;and2)coldposteri-
Fortuin, V., Garriga-Alonso, A., Wenzel, F., Ratsch, G.,
orscanbeseenasapproximatingavalidpriordistribution.
Turner, R. E., van der Wilk, M., and Aitchison, L.
Bayesianneuralnetworkpriorsrevisited. InThirdSym-
Impactstatement posiumonAdvancesinApproximateBayesianInference,
2021. URL https://openreview.net/forum?
This paper presents work whose goal is to advance
id=xaqKWHcoOGP.
Bayesian deep learning research, but at substantial com-
putationalexpense. Weestimatethatourexperimentsused
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P.,
over 2,000 kWh of electricity. Broader adoption of simi-
andWilson,A.G.Losssurfaces,modeconnectivity,and
lartechniquescouldcarryalargecarbonfootprint. While
fastensemblingofdnns.Advancesinneuralinformation
webelievethereismeritintradingcomputationforstatisti-
processingsystems,31,2018.
calfidelity,weencouragefutureworktoprioritizeenergy-
efficientimplementationsofsimilarmethods. He,K.,Zhang,X.,Ren,S.,andSun,J.Deepresiduallearn-
ing for image recognition. In Proceedings of the IEEE
Acknowledgements conferenceoncomputervisionandpatternrecognition,
pp.770–778,2016.
This research was supported by Google’s TPU Research
Cloud(TRC)program: https://sites.research. Izmailov, P., Nicholson, P., Lotfi, S., and Wilson, A. G.
google/trc/. Dangers of bayesian model averaging under covariate
shift. Advances in Neural Information Processing Sys-
References
tems,34:3309–3322,2021a.
Adlam,B.,Snoek,J.,andSmith,S.L. Coldposteriorsand Izmailov, P., Vikram, S., Hoffman, M. D., andWilson, A.
aleatoricuncertainty. arXivpreprintarXiv:2008.00029, G.G.Whatarebayesianneuralnetworkposteriorsreally
2020. like? InInternationalConferenceonMachineLearning,
pp.4629–4640.PMLR,2021b.
9CanaConfidentPriorReplaceaColdPosterior?
Kapoor, S., Maddox, W., Izmailov, P., and Wilson, A. G. Welling,M.andTeh,Y.W. Bayesianlearningviastochas-
On uncertainty, tempering, and data augmentation in tic gradient langevin dynamics. In Proceedings of
bayesianclassification. InOh,A.H.,Agarwal,A.,Bel- the 28th international conference on machine learning
grave, D., and Cho, K. (eds.), Advances in Neural In- (ICML-11),pp.681–688,2011.
formation Processing Systems, 2022. URL https:
Wenzel, F., Roth, K., Veeling, B., Swiatkowski, J., Tran,
//openreview.net/forum?id=pBJe5yu41Pq.
L.,Mandt,S.,Snoek,J.,Salimans,T.,Jenatton,R.,and
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers Nowozin, S. How good is the Bayes posterior in deep
offeaturesfromtinyimages. 2009. neuralnetworksreally? InIII,H.D.andSingh,A.(eds.),
Proceedings of the 37th International Conference on
Lakshminarayanan,B.,Pritzel,A.,andBlundell,C.Simple Machine Learning, volume 119 of Proceedings of Ma-
andscalablepredictiveuncertaintyestimationusingdeep chineLearningResearch,pp.10248–10259.PMLR,13–
ensembles. Advances in neural information processing 18 Jul 2020. URL https://proceedings.mlr.
systems,30,2017. press/v119/wenzel20a.html.
Mackay, D. J. C. Bayesian methods for adaptive models. Wilson, A. G. and Izmailov, P. Bayesian deep learning
CaliforniaInstituteofTechnology,1992. and a probabilistic perspective of generalization. Ad-
vances in neural information processing systems, 33:
Nabarro, S., Ganev, S., Garriga-Alonso, A., Fortuin, V., 4697–4708,2020.
vanderWilk,M.,andAitchison,L. Dataaugmentation
Zhang,Y.,Wu,Y.-S.,Ortega,L.A.,andMasegosa,A.R.If
inbayesianneuralnetworksandthecoldposterioreffect.
thereisnounderfitting,thereisnocoldposterioreffect.
InUncertaintyinArtificialIntelligence,pp.1434–1444.
arXivpreprintarXiv:2310.01189,2023.
PMLR,2022.
Neal,R.M.Bayesianlearningforneuralnetworks,volume
118. SpringerScience&BusinessMedia,2012.
Neal, R. M. et al. Mcmc using hamiltonian dynamics.
Handbookofmarkovchainmontecarlo,2(11):2,2011.
Noci, L., Roth, K., Bachmann, G., Nowozin, S., andHof-
mann, T. Disentangling the roles of curation, data-
augmentation and the prior in the cold posterior effect.
AdvancesinNeuralInformationProcessingSystems,34:
12738–12748,2021.
Qiu, S., Rudner, T. G. J., Kapoor, S., and Wilson,
A. G. Should we learn most likely functions or pa-
rameters? In Thirty-seventh Conference on Neural In-
formation Processing Systems, 2023. URL https:
//openreview.net/forum?id=9EndFTDiqh.
Sharma, M., Farquhar, S., Nalisnick, E., and Rain-
forth, T. Do bayesian neural networks need to be
fully stochastic? In Ruiz, F., Dy, J., and van de
Meent, J.-W. (eds.), Proceedings of The 26th In-
ternational Conference on Artificial Intelligence and
Statistics, volume 206 of Proceedings of Machine
Learning Research, pp. 7694–7722. PMLR, 25–27
Apr 2023. URL https://proceedings.mlr.
press/v206/sharma23a.html.
Tu, K. Modified dirichlet distribution: Allowing negative
parameterstoinducestrongersparsity.InProceedingsof
the 2016 Conference on Empirical Methods in Natural
LanguageProcessing,pp.1986–1991,2016.
10CanaConfidentPriorReplaceaColdPosterior?
train accuracy train confidence test accuracy parameter L1 norm
0.00010 100% 99% 1
94%
98% 96%
0.001 93%
96% 93% 92%
0.1
0.01
90% 91%
94%
0.1 87% 90%
92% 89%
84% 0.01
1
0.07 0.16 0.39 0.93 2.24 0.07 0.16 0.39 0.93 2.24 0.07 0.16 0.39 0.93 2.24 0.07 0.16 0.39 0.93 2.24
prior scale prior scale prior scale prior scale
Figure10.Effect of posterior temperature and Normal prior scale on a Bayesian neural network. The model is a ResNet20
trainedonCIFAR-10withdataaugmentationturnedon. WhenT = 1,theaugmenteddataisundercounted,resultinginunderfitting
(low training accuracy). As temperature decreases, the fit to training data improves, increasing test accuracy. At the same time, as
temperaturedecreases, thenormofthemodelparametersdecreases, butonlyuptoapoint. Withadecreasingtemperature, thecold
posteriorapproachesadeepensemble,whichisobtainedbysettingthetemperaturetoexactlyzero.
Appendixoutline Observe in the left plot that the training accuracy almost
perfectly predicts the test accuracy. A possible explana-
• In Appendix A, we visualize the effect of posterior
tionbehindthiseffectisthatmostoftheposteriorsunder-
tempering and compare the accuracy and likelihood
fit the training data, therefore overestimating the aleatoric
achievedbyallmodelsdiscussedinthispaper.
uncertainty in the labels (Adlam et al., 2020). In order to
• InAppendixB,weshowthattheDirClippriorreaches correctlyrepresentthealeatoricuncertaintyinthedatala-
itsclippingvalue,whichresultsinlowtestlikelihood bels, the model needs to reach near 100% training accu-
whentheposteriordistributionisapproximatedusing racy. However, thebehaviorofthetestlikelihoodismore
asmallnumberofsamples. complex—wediscussthisinAppendixB.2.
• InAppendixC,wefactorizetheNDGposteriorintoa
B.FurtherdiscussionofDirClipprior
priorandalikelihood.
B.1.Clippingvalueisreached
• In Appendix D, we prove that the confidence prior
convergestoacoldlikelihood. InFigure5,weshowthattheDirClippriorachieveshigher
training accuracy when the clipping value −50 is used,
• In Appendix E, we explain why correctly sampling
compared to the clipping value −10. This might seem
from a prior over model outputs requires a “change
surprising because a log-probability of −10 is already a
of variables” term; we prove that the Dirichlet prior
very small value. The reason the clipping value affects
divergesifthistermisomitted;andweprovethatthe
the prior behavior is that the clipping value is reached on
DirClippriorisavaliddistribution.
most predictions. Observe in Figure 12 that almost 90%
• In Appendix F, we derive the direction of a gradient of the predicted log-probabilities of DirClip(−50) poste-
step along the Dirichlet posterior and show when it rior samples are smaller than its clipping value of −50.
increasestheprobabilityoftheobservedclass. Similarly,almost90%ofthepredictedlog-probabilitiesof
DirClip(−10) posterior samples are smaller than its clip-
• InAppendixG,weprovideimplementationdetailsfor
ping value of −10. Note that CIFAR-10 has 10 classes.
allofourexperiments.
Therefore, these models have converged approximately to
• In Appendix H, we further describe the HMC and predictingtheclippingvalueforeachwrongclassandpre-
SGHMCalgorithms. dictingalog-probabilitycloseto0forthetrueclass.
Forcomparison,noticethatcoldposteriorsamplesachieve
A.Modelcomparison perfect training accuracy (Fig. 5) without predicting ex-
tremely small log-probabilities. This relates to the issue
Tovisualizetheeffectofposteriortemperingonthetrain-
discussed in Section 8: the Dirichlet prior enforces small
ing accuracy, test accuracy, and parameter norm, we sam-
probabilitiesratherthandirectlyenforcinghighconfidence
pled16differentposteriortemperaturesacross15different
(whichisnotthesamething). Inparticular,predictingex-
priorscales—theresultsareshowninFigure10.
tremely small log-probabilities negatively affects the test
InFigure11,wecomparethetrainingaccuracy,testaccu- likelihoodoftheDirClipprior.
racy, and test likelihood of various posterior distributions.
11
erutarepmetCanaConfidentPriorReplaceaColdPosterior?
test accuracy test likelihood
2000
94%
3000
93%
4000
92%
91% 5000
original posterior NDG likelihood
tempered posterior DirClip(-10)
90% 6000
NDG posterior (logits) DirClip(-20)
NDG posterior (logprobs) DirClip(-50)
89% 7000
88% 90% 92% 94% 96% 98% 100% 88% 90% 92% 94% 96% 98% 100%
train accuracy train accuracy
Figure11.Comparisonofvariousmethodsforcontrollingaleatoricuncertainty. ThemodelisaResNet20trainedonCIFAR-10
with data augmentation turned on. The original posterior consists of a N(0,0.12) prior over model parameters and the categorical
likelihood;thetemperedposteriorusesthesamepriorandlikelihood,buttheposteriortemperatureisvaried. TheNDGmodelsusea
N(0,0.12)prioroverparameters,combinedwitheitherthefullNDGposteriororonlythequadraticNDGlikelihoodonitsown.Lastly,
theDirClipmodelsusevaryingconcentrationparametersαwiththeclippingvalueshownintheplotlegend.
100% 100%
DirClip(0.85, -50)
cold posterior (T=0)
80%
10%
60%
40%
DirClip(0.85, -50) rand. init. 1%
DirClip(0.85, -50) finetuned
20% DirClip(0.85, -10) finetuned
categorical (T=0)
0% 0.1%
70 60 50 40 30 20 10 0 70 60 50 40 30 20 10 0
predicted log-probability
predicted log-prob. of true class
Figure12.CDFofpredictedlog-probabilitiesontrainingdata.
Figure13.CDFofpredictedlog-probabilitiesforthetrueclass
We sampled various ResNet20 posteriors on CIFAR-10. After-
ontestdata. WesampledtheDirClipandcoldResNet20poste-
wards,weindependentlyevaluatedthepredictionsofeachposte-
riorsonCIFAR-10. Afterwards,weindependentlyevaluatedthe
riorsampleonthetrainingdata,concatenatingallpredictedlog-
predictionsofeachposteriorsampleonthetestdata,concatenat-
probabilitiesacrossclassesandposteriorsamples.
ingallpredictedlog-probabilitiesforthetrueclass.
B.2.Lowlikelihood
log-probability of less than −10, while samples from the
In Figure 11, we compare the accuracy and likelihood of cold posterior have exactly zero such overconfident mis-
various posterior distributions. Note that as we increase classifications.
the concentration of the DirClip(−50) prior, the train and
Intuitively,aBNNcanmisclassifyanimagewithextremely
test accuracy increase while the test likelihood decreases.
highconfidenceonlyifallposteriorsamplesassignanex-
Theoppositeistrueforthecoldposterior: itstestaccuracy
tremely low probability to the true label. Most of our ex-
increasestogetherwiththetestlikelihood.
periments only used 8 (completely independent) posterior
The low likelihood of the DirClip(−50) prior can be di- samples, so this happens often. If we instead used more
rectly attributed to the small log-probabilities that it pre- posterior samples, it is more likely that at least one of the
dicts. Itnotonlypredictsclasslog-probabilitiesof−50on posteriorsampleswouldassignahigherprobabilitytothe
thetrainingdata(asshowninFigure12)butalsoonthetest trueclass,whichwouldsignificantlyimprovethetestlike-
data,sometimespredictingalog-probabilityof−50forthe lihood of the DirClip prior. In Figure 20, we show that
correct class. Such a prediction incurs an extremely low as we increase the number of posterior samples, the like-
likelihood. In Figure 13, we show that the DirClip poste- lihood of the DirClip prior indeed significantly improves.
rior samples misclassify almost 10% of test images with Whereasthelikelihoodofthecoldposteriorconvergesafter
12
ytilibaborp
.muc
ytilibaborp
.mucCanaConfidentPriorReplaceaColdPosterior?
NDG(logyˆ|α˜) ∼N(logyˆ |µ ,σ2), where
k k k k
α˜ =α+1(k =y)
k
(12)
σ =log(α˜−1+1)
k k
µ =log(α˜ )−log(α˜ )+(σ2−σ2)/2
k k y y k
8 6 4 2 0 2 4 Technically, the original NDG model is a Gaussian distri-
log-probability bution over predicted logits rather than log-probabilities.
Figure14.Visualization of the Noisy Dirichlet Gaussian However,logitsandlog-probabilitiesarealwaysequivalent
(NDG) distribution over model’s predicted log-probabilities.
up to a constant. Therefore, the only difference between
Thepredictedlog-probabilityforeachclassisassignedaninde-
defining the distribution over logits and log-probabilities
pendentNormaldistribution. Thelog-probabilitycorresponding
is that using logits removes one degree of freedom. We
tothecorrectclasshasashiftedmeanandreducedstandarddevi-
experimentallyverifythatusingNDGoverlogitsandlog-
ation;allotherlog-probabilitieshavethesamedistribution.
probabilitiesisequivalentinFigure4.8
Unfortunately,itturnsoutthattheNDGapproximationef-
fectivelyusesaquadraticlikelihoodterm.RecallthatNDG
isnotjustanapproximateprior;instead,itjointlyapprox-
only∼20posteriorsamples,theDirClipposteriorrequires imatesthepriortogetherwiththelikelihood. Wefactorize
200samplestoreachasimilarlikelihoodvalue. theNDG(α˜)posteriorinEq.(13),showingthatitisequal
toacombinationofareparameterizedNDG(α)priorand
Ontheonehand,thisimpliesthatthelowlikelihoodofthe
a quadratic likelihood term. The implied prior and likeli-
DirClipposteriorinFigure11isanartifactofourapprox-
hood functions are plotted in the left and middle plots of
imate inference pipeline, rather than a fundamental prop-
Figure3.
ertyofthetrueDirClipposteriordistribution. Ontheother
hand,thefactthattheDirClipposteriorrequiresanorderof let logyˆ ∼N(µ ,σ2)where
k k k
magnitudemoreposteriorsamplestoconverge(compared
µ =µ ifk =yelseµ
to a cold posterior) is a critical difference for any practi- k 1 0
σ =σ ifk =yelseσ
tioner. Wediscussthe(high)computationalcostofourex- k 1 0
perimentsinAppendixG.
logp(logyˆ)=c
−1(cid:88)(logyˆ k−µ k)2
2 σ2
k k
B.3.fine-tuninghasconverged
(cid:32) (cid:33)
In Figure 5, we compared the accuracy of both randomly =c
−1 (cid:88)(logyˆ k−µ 0)2
+ (13)
2 σ2
initialized and fine-tuned DirClip models. Given that the k 0
fine-tunedmodelswereinitializedfromacheckpointwith (cid:18) σ 02µ 1−σ 12µ 0 logyˆ + σ 12−σ 02 logyˆ2(cid:19)
100%trainaccuracy,howcanweknowthatthefine-tuning σ2σ2 y 2σ2σ2 y
0 1 0 1
hasconverged? Onecheckthatweperformedwascompar-
ingtheCDFofpredictedlog-probabilities,asshowninFig- D.Proofthatconfidencepriorconvergestoa
ure12. Atinitialization,thefine-tunedDirClipmodelhad
coldlikelihood
a similar CDF to the cold posterior. However, after fine-
tuning, the CDF converged to the CDF of the randomly In this section, we show that both a cold likelihood and
initialized DirClip model, suggesting that the fine-tuning the confidence prior combined with untempered categori-
hasconverged. callikelihoodconvergetoaDiracdeltameasureasT →0.
We prove this by deriving lower and upper bounds on the
C.FactorizationofNDGposterior product of the confidence prior with the categorical like-
lihood and showing that they both converge to the same
Kapoor et al. (2022) approximated the product of a distribution.
Dirichlet(α) prior with the categorical likelihood using
RecallfromEq.(10)thatwhenthepredictedclassmatches
a Gaussian distribution over the model’s predicted log-
thetrueclass,thedensityoftheconfidencepriorcombined
probabilities. They named this distribution Noisy Dirich-
with the categorical likelihood is equal to the density of a
let Gaussian (NDG). We visualize the NDG distribution
in Figure 14 and formally define its density function in 8When defining the distribution over log-probabilities, we
Eq.(12). shiftedµ byaconstants.t.µ =0.
k y
13
ytisned
.borpCanaConfidentPriorReplaceaColdPosterior?
Tempered categ. likelihood Confident prior + categ. likelihood
1.0 1
0.08 analytical solution
0.8 simulation
0.1
0.06 0.6
0.4 0.01
0.04 0.2
0.0 0.001
0.02 0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
yy: prob. of true class yy: prob. of true class
Figure16.Left: CDF of the categorical likelihood tempered to
0.00
10 5 10 4 10 3 10 2 10 1 100 varioustemperatures(shownbythecolorbarontheright).Right:
temperature CDFoftheupperboundontheproductofaconfidentprior(pa-
Figure15.Wassersteindistancebetweenthecoldlikelihoodand rameterized using temperature) with the untempered categorical
the upper bound on the product of confidence prior and untem- likelihood.NoticethattheanalyticalCDFs(solidlines)perfectly
peredcategoricallikelihood. matchtheempiricalCDFsobtainedusingMonteCarlosimulation
(dottedlines).
coldcategoricallikelihood.Atthesametime,thecoldlike-
In the limit of T → 0, the upper bound converges to the
lihood provides a lower bound on the product of the con-
same distribution as the cold likelihood: the Dirac delta
fidence prior with untempered categorical likelihood (be-
measure. At the same time, recall from Eq. (14) that the
cause yˆ ≤ max yˆ ). To obtain an upper bound on the
y k k cold likelihood provides a lower bound for the product of
product,notethatconditionalonaspecificvalueofthepre-
theconfidencepriorandthecategoricallikelihood. There-
dicted probability for the true class yˆ , the density of the
y fore,boththelowerboundandtheupperboundconvergeto
confidence prior is maximized when the rest of the prob-
thesamedistributionasthecoldlikelihood. InFigure15,
ability mass (1 − yˆ ) is entirely concentrated in a single
y we plot the Wasserstein distance between the cold likeli-
class. Combined,thisyieldsthefollowinglowerandupper
hoodandtheupperbound. NoticethattheWassersteindis-
bounds:
tanceisexactlyzeroatT =1(becausetheconfidenceprior
yˆ1/T ≤p(yˆ)·yˆ ≤max(yˆ ,1−yˆ )1/T−1·yˆ . (14) isequaltoauniformprior),itpeaksaroundT ≈ 0.1,and
y y y y y
thenagainconvergestozeroasT →0.
We show that both a cold likelihood and the confidence
Lastly,toverifythattheCDFsderivedinEqs.(15)and(17)
priorcombinedwiththecategoricallikelihoodconvergeto
arecorrect,wecomparedthemtoempiricalCDFsobtained
theDiracdeltameasurebyderivingtheCDFsofbothdis-
usingMonteCarlosimulationinFigure16.
tributionsandshowingthattheCDFsconvergetotheCDF
oftheDiracdeltameasureasT →0.
E.Priorsoveroutputs
We can think of the cold likelihood as an (unnormalized)
distributionoverthepredictedprobabilityofthetrueclass. E.1.Changeofvariables
Bydoingso,wecanderiveitsCDF:
Recall from Section 5 that the Dirichlet prior is a distri-
F (z)=p(yˆ≤z) butionovermodelpredictionsp(yˆ)whereasweultimately
cold
(15)
wanttosampleadistributionovermodelparametersp(θ),
=z1+1/T.
as defined in Eq. (1). So how can we translate the distri-
NoticethatasT → 0, thecoldlikelihoodCDFconverges butionovermodelpredictionstoadistributionovermodel
totheCDFoftheDiracdeltameasureconcentratedat1: parameters? Unfortunately,thisisnotassimpleassetting
(cid:40) p(θ) = p(yˆ). The issue is that probability density is not
0 z <1
F δ(z)= (16) conservedwhentransformingrandomvariables;onlyprob-
1 z =1.
abilitymassisconserved. Intuitively,theprobabilitymass
ofθbeinginsomesmallvolume|∆θ|isp(θ)|∆θ|,andby
Similarly, we can derive the CDF for the upper bound on
applyingthesamelogictoyˆ,weget:
theproductoftheconfidencepriorandtheuntemperedcat-
egoricallikelihoodintroducedinEq.(14): p(θ)|∆θ|=p(yˆ)|∆yˆ| (18)
 21/T (cid:0) T −(1−z)1/T(T +z)(cid:1) |∆yˆ|

(21/T −1)(T +1)
z ≤0.5 p(θ)=p(yˆ) |∆θ|. (19)
F up(z)= (17) When the transformation is invertible, the change in vol-
2T 2(+
T1 +−2
11
1 )/T +
2(2( 12 /z T)1 −/T 1+ )1 (T− +1
1)
z >0.5
u fom rte us nc aa ten lb ye inco om up ru ct ae sd eu ,s ti hn eg mth ae pJ pa ic no gb fi ra on mde mte orm dei lna pn at r. aU mn e-
-
14
ecnatsid
nietsressaW
.borp
.muc
temperatureCanaConfidentPriorReplaceaColdPosterior?
terstomodelpredictionsismany-to-one,andthereforethe 800
Dirichlet prior
standardJacobiandeterminantmethoddoesnotapply. For Dirichlet posterior
afurtherdiscussionofhowthistransformcanbeapproxi- 600 Categ. likelihood
mated, we recommend the work of Qiu et al. (2023). We
400
consider approximating this transformation to be beyond
the scope of this paper, although it might be an exciting 200
topicforfutureresearch.
0
We simply note that neither we nor Kapoor et al. (2022)
have attempted to compute the ratio of volumes |∆yˆ|, i.e. 100 75 50 para2 m5 s['lin0 ear']['2 b'5 ][0] 50 75 100
|∆θ|
the“changeofvariables”correctionterm.Withoutthiscor- Figure17.Dirichletpriordiverges.WetookaResNet20trained
rectionterm, settingthepriordistributionoverparameters onCIFAR-10andfixedthevaluesofallparametersexceptasin-
top(θ)=p(yˆ)doesnotresultinthepriorpredictivedistri- glebiasparameterinthelinearoutputlayer.Aswevarythevalue
butionfollowingyˆ ∼p(yˆ). However,settingp(θ)=p(yˆ) ofthisparameter,boththeDirichletpriorandposteriordiverge.
doesresultinsomedistributionovermodelparameters,and
wesimplyneedtoensurethatthisdistributionisvalid(i.e.
firstclassapproaches1,whilethepredictedprobabilityfor
integrable). When the Dirichlet prior is applied directly
allotherclassesapproaches0,causingtheDirichletproba-
over model parameters, its integral diverges, so it is not a
bilitydensitytodivergeto∞. Conversely, asθ → −∞,
validdistribution(weprovethisinAppendixE.2). 1
the predicted probability for the first class approaches 0,
In contrast, the DirClip prior density is bounded by some therebyalsocausingtheDirichletprobabilitydensitytodi-
constant p(yˆ) ≤ B, so if the DirClip prior is combined vergeto∞. WenumericallyverifiedthisusingaResNet20
with a proper prior over model parameters (e.g. f(θ) = trainedonCIFAR-10,asshowninFigure17. However,we
N(0,1)),theintegralofthejointdensityconverges: note that this behavior generalizes to any neural network
using a linear output layer with a bias parameter and the
(cid:90) (cid:90)
p(yˆ)f(θ)dθ ≤ Bf(θ)dθ (20)
softmaxactivationfunction.
In summary, the probability density of p(θ |θ ...θ ) di-
≤B. (21) 1 2 N
verges to ∞ as θ → ±∞. Since the domain of θ is
1 1
(cid:82)−∞
therealnumbers, theintegral p(θ |θ ...θ )dθ di-
ThisimpliesthattheDirClippriorcombinedwithavague ∞ 1 2 N 1
verges,meaningthattheconditionaldistributionofthefirst
Normalpriorcorrespondstoaproperpriorovermodelpa-
biasparameterisnotavalidprobabilitydistribution. This
rameters.
impliesthatjointdistributionoverallmodelparametersis
also notvalid. Moreover, thisdivergence appliesnot only
E.2.ProofthatDirichletdiverges
totheDirichletprior,butalsotheDirichletposteriorthatis
For a probability density function to be valid, it must obtainedbycombiningtheDirichletpriorwithacategori-
integrate to one. Technically, SGHMC doesn’t require callikelihood. WeshowthisisFigure17.
the distribution to be normalized, so the distribution only
In practice, these are not just “pedantic” details. The fact
needs to integrate to a constant. However, the integral of
that the Dirichlet posterior is not a valid distribution di-
the Dirichlet prior over model parameters diverges. We
rectly implies that it cannot be sampled. If we attempt to
prove this by looking at the probability density of a sin-
sampletheDirichletposteriorusingSGHMC,orevenop-
gle model parameter, conditional on all other model pa-
timize it using SGD, the model parameters will diverge.
rameters p(θ |θ ...θ ). More specifically, we took a
1 2 N Both we and Kapoor et al. (2022) have observed this be-
ResNet20trainedonCIFAR-10andfixedthevaluesofall
havior.
model parameters except a single “bias” parameter in the
linearoutputlayer,whichwedenoteθ .
1
F.Dirichletgradientstepdirection
RecallfromEq.(5)thatthedensityoftheDirichletdistri-
butionislogp(yˆ)
=c (cid:80)K
(α−1)logyˆ ,whichdiverges In this section, we are interested in answering whether a
k=1 k
to ∞ as one of the predicted probabilities yˆ approaches smallgradientstepalongthesimplexofaprobabilitydis-
k
zero. By changing the value of the neural network’s bias tribution over classes will increase or decrease the proba-
parameterθ ,wearedirectlyshiftingthevalueoftheout- bility of the true class. First, we derive a general formula
1
put logit corresponding to the first class. Since the neural fortheupdateofanyprobabilitydistribution. Second, we
networkisusingthesoftmaxactivationfunction,changing applythisformulatotheDirichletdistribution,discovering
the value of a single logit directly changes the predicted the update may decrease the probability of the true class
probabilities. Asθ →∞,thepredictedprobabilityforthe whenasmallconcentrationparameterαisused.
1
15
ytisned
golCanaConfidentPriorReplaceaColdPosterior?
Let’s denote the predicted probabilities over classes yˆ = 1.0
(yˆ ,yˆ ...yˆ ). Since yˆ is only defined on the (K−1)-
1 2 K
simplex (i.e. class probabilities must sum to 1), it is not 0.8
possibletoperformgradientascentdirectlyonyˆ. Instead,
it is more practical to parameterize the predictions using 0.6
logitsz,wherelogyˆ =logsoftmax(z).
0.4
Additionally, let’s denote the probability density function
of interest f, so that the log density assigned to any pre- 0.2
diction over classes is logf(yˆ) = logf(logsoftmax(z)).
A gradient ascent update of logits is therefore ∆z = 0.0
ϵ∇ J , where ϵ is the learning rate, ∇ is 20 40 60 80 100
logf logsoftmax logf
num. classes
thegradientofthelog-PDFw.r.t.yˆandtheJacobianofthe
logsoftmaxfunctionmapsthechangeinyˆtoachangeinz. Figure18.Phase diagram showing the “critical value” of the
Dirichletconcentrationparameterα.Whenαisabovethecrit-
Givenachangeinlogits∆z,wecanlinearlyapproximate icalthreshold(i.e.inthewhiteregion),thenagradientsteponthe
thechangeinclasslog-probabilities∆logyˆ: Dirichletposteriorwillalwaysincreasetheprobabilityofthetrue
class. Ontheotherhand,whenαisbelowthecriticalthreshold
∆logyˆ =J logsoftmax∆z (i.e.intheblackregion),gradientstepsmightdecreasetheprob-
(22)
=ϵJ ∇ J . abilityofthetrueclass,makingoptimization(orsampling)very
logsoftmax logf logsoftmax
challenging.
To understand the update in Eq. (22), we must derive the
Jacobianofthelogsoftmaxfunction:
distribution, we can look at the “worst-case” scenario in
(cid:88) termsofyˆ.Inparticular,weareinterestedinthecasewhere
logsoftmax(z) =z −log expz (23)
i i k thepriorgradientdominatesthelikelihoodgradient,andas
k
a result, the combined gradient decreases the probability
∂logsoftmax(z)
i =1(i=j)−yˆ (24) of the true class. Observe in Figure 6 that the gradient of
∂z j
j theDirichletpriorgrowsthecloserthepredictiongetstoa
J logsoftmax =I−1⊗yˆ, (25) singleclass. Therefore,the“worst-case”scenarioisthatyˆ
is concentrated around a single class j ̸= y. By applying
andthegradientoftheDirichletlog-PDF:
thisworst-caseassumption, wegetaminimumvalueofα
thatmustbeusedsothattheprobabilityupdateforthetrue
K
logf(yˆ)=(cid:88) (α −1)logyˆ (26) classispositiveforallvaluesofyˆ:
k k
k=1
∇ logf =α−1. (27) g y−g+yˆ y−yˆ jg j +g+yˆ j2 >0 (33)
g −g yˆ +g+(yˆ2−yˆ )>0 (34)
For convenience, let’s denote the gradient of the log-PDF y j j j y
as g = ∇ logf. Also, let’s denote g+ = (cid:80)K k=1g k and g y−g j +g+ ≳0 (35)
yˆ2+ =(cid:80)K yˆ2. PluggingthisintoEq.(22),weget: α−(α−1)+(Kα−K+1)≳0 (36)
k=1 k
2+Kα−K ≳0 (37)
∆logyˆ =ϵJ (∇ J ) (28)
logsoftmax logpdf logsoftmax Kα≳K−2 (38)
=ϵ(I−1⊗yˆ)(g(I−1⊗yˆ)) (29)
K−2
=ϵ(I−1⊗yˆ)(g−g+yˆ) (30) α≳ (39)
K
=ϵ(g−g+yˆ−yˆ·g+g+yˆ2+). (31)
NoticethatthecriticalvalueofαinEq.(39)dependsonthe
Gradient ascent increases the probability of the true class
numberofclassesK.ThisrelationshipisillustratedinFig-
iff∆logyˆ >0:
y ure18.For10classes,thecriticalvalueofαis0.8,whichis
g −g+yˆ −yˆ·g+g+yˆ2+ >0. (32) consistentwithallofourexperimentsinSection7. Asthe
y y
numberofclassesgrows,thecriticalvalueofαincreases,
Eq.(32)providesageneralconditionthatmustholdforany approaching 1. Note that the increasing α decreases the
distribution(andanyparticularpredictionyˆ)fortheupdate prior concentration; in particular, α = 1 corresponds to a
ofthetrueclasstobepositive. InFigure7, weshowhow uniform prior. This is an unfortunate result, meaning that
the update depends on both the distribution parameter α theDirichletpriorgetsincreasinglyunstablewithagrow-
andthepredictionyˆ.TogainmoreinsightintotheDirichlet ingnumberofdimensions.
16
noitartnecnoc
roirpCanaConfidentPriorReplaceaColdPosterior?
riorsamplespermodel. However,theseadditionalsamples
were only used for uncertainty estimates; the mean accu-
racyandlikelihoodonlycorrespondtoaposteriorconsist-
ing of 8 samples. Each posterior used a N(0,0.12) prior
temperature
learning rate over model parameters and a learning rate of 10−4. The
posterior sample learningratewasintentionallyverylowandthenumberof
epochsveryhighsothateachposteriorsamplewouldcon-
vergetotheMCMCstationarydistribution.
Are8posteriorsamplesenough? Acrossallofourex-
0 2000 4000 6000 8000 10000
epoch periments,weused8–16independentSGHMCsamplesto
Figure19.SGHMC learning rate and temperate schedule. approximate each posterior distribution. This is a distinct
Temperaturestartsatzero,increaseslinearlyafter 31 epochsand approach to prior works which generated long correlated
staysatthemaximumvalueafter 2 epochs.Learningrateiscon-
3 SGHMCchains. Wechosethisapproachtoeliminateany
stantfor 1 epochsandthenfollowsasinescheduletozero.
2 autocorrelation between posterior samples but also to ob-
tainanalgorithmthatparallelizesmoreeasilyacrossTPU
G.Implementationdetails
cores(allowingustouserelativelysmallbatchsizeswith-
outincurringsignificantoverhead). Usingonly8indepen-
Goal. In order to minimize bias when comparing differ-
dentsamples,andthesameaugmentationstrategyasWen-
ent posterior distributions, we considered it important to
zeletal.(2020), wematchedorslightlyexceededthetest
obtain high-fidelity approximations for each posterior. In
accuracyofResNet20trainedonCIFAR-10ofpriorworks
total,wespentapproximately30millionepochs(750TPU-
(Wenzel et al., 2020; Kapoor et al., 2022; Fortuin et al.,
core-days) to sample all ResNet20 posteriors on CIFAR-
2021). In Figure 20, we show that we could further im-
10. It was unfortunately this high computational cost
provethetestaccuracy(andespeciallythetestlikelihood)
thatpreventedusfromtestingmoremodelarchitecturesor
byusingmoreposteriorsamples.
datasets.
Fine-tuning. InFigures5,11and12,someoftheDirClip
Basic setup. All experiments were implemented in JAX
models were initialized from a pretrained model. More
(Bradburyetal.,2018)usingTPUv3-8devices. Thetypi-
specifically, thismeansthattheSGHMCsamplerwasini-
cal way to draw SGHMC samples is to generate a single
tialized from a model with 100% training accuracy, ob-
chain where each sample depends on the previous sam-
tained using SGD. Given the unstable dynamics of fine-
ples.SinceBNNposteriorsaremulti-modal(Garipovetal.,
tuning,werantheSGHMCsamplerusingaverylowfixed
2018; Fort et al., 2019; Wilson & Izmailov, 2020), gener-
learningrateandafixedtemperatureT = 1. TheDirClip
ating autocorrelated samples means that the posterior dis-
modelwithaclippingvalueof−50usedalearningrateof
tributionisexploredslowly(Sharmaetal.,2023). Toelim-
10−7 and 10,000 epochs per sample. The DirClip models
inate this autocorrelation (and thus achieve more accurate
withaclippingvalueof−10and−20usedanevenlower
posteriorapproximations),weinsteadgeneratedeachpos-
learningrate(3·10−8)andanincreased50,000epochsper
terior sample from a different random initialization, com-
sample,toensurebetternumericalstabilityforsmallα.
pletelyindependentlyoftheothersamples.
Confidence of a Normal prior. For the experiments
For most experiments, we followed the learning rate and
shown in Figures 2 and 10, we sampled 15 different pri-
temperature schedule depicted in Figure 19. Initially, the
ors scales across 16 temperatures, leading to 240 unique
temperatureiszeroandthelearningrateishighsothatthe
posterior distributions. Given the large number of differ-
samplerquicklyconvergestoalocalmode. Afterward,the
entdistributions,weoptedforthelowestsamplingfidelity
temperature is increased to explore the posterior, and the
forthisexperiment. Asingleposteriorconsistsof16inde-
learningrateisdecreasedtoreducethebiasofthesampler.
pendent SGHMC samples, each using 1,000 epochs. The
At the end of the cycle, only a single posterior sample is
learningratewastunedforeachpriorscaletomaximizethe
produced. We run this procedure in parallel across TPU
testaccuracyofasingleposteriorsample. Oncethelearn-
cores to generate multiple posterior samples. We provide
ing rate was tuned, the 16 posterior samples were drawn
specificdetailsforeachexperimentbelow.
using a different random seed, and the previous samples
Training from scratch. For the experiments where we werediscardedtoavoidanydataleakage.
compared posterior tempering, DirClip, and NDG against
2D toy example. Figure 1 shows the only experiment
each other (Figs. 4, 5, 9 and 11 to 13), a single poste-
thatweranonaCPUratherthanTPUs. Weusedafully-
rior consists of 8 independent SGHMC samples, each us-
connectedneuralnetworkwith5hiddenlayersof10neu-
ing 10,000 epochs. To obtain error bars, this procedure
rons, using the ReLU activation function. We used HMC
was repeated three times, therefore leading to 24 poste-
17CanaConfidentPriorReplaceaColdPosterior?
test accuracy test likelihood
-2000
1.03x
94%
-4000
93%
-8000
92%
10x
-16000
91%
cold posterior (T=0.1)
-32000
DirClip(0.85) posterior
90%
1 40 80 120 160 200 1 40 80 120 160 200
num. posterior samples num. posterior samples
Figure20.Posteriorsize. AcrossallofourResNet20experiments,weusedcompletelyindependentSGHMCposteriorsamples. The
dashedlineshowsthetestaccuracyandlikelihoodforacoldposteriorconsistingof8samples. Observethat8samplesareenoughfor
theposterioraccuracyofbothacoldposteriorandaDirClipposteriortoconvergewithin1%.However,thetestlikelihoodoftheDirClip
posteriortakesmuchlongertoconverge.
(AppendixH.1)with10,000leapfrogstepsand1,500sam- was at a cost of over 60 million epochs of compute. Due
ples per posterior. Both chains achieved an Rˆ metric to the extreme computational cost, we instead relied on
(Brooks & Gelman, 1998) of 1.00 in function space and thecheaperSGHMCsampler,anapproximate(andbiased)
≤1.05 in parameter space. We used a DirClip clipping methodderivedfromHMC.
valueof−50.
H.1.HamiltonianMonteCarlo
Training accuracy. Note that in Figures 4 and 5, the
trainingaccuracywasevaluatedonposteriorsamplesrather The idea behind HMC is that instead of drawing samples
thantheposteriorpredictivedistribution—thegoalofthese directlyfromthetargetdistributionπ(θ),wedefineanew
plots was to explicitly visualize the behavior of posterior variable v, called the momentum, and draw samples from
samples. Incontrast,testaccuracywasmeasuredusingan thejointdistributionπ(θ,v) := π(θ)π(v). Oncewehave
ensemble of 8 posterior samples, to provide a direct mea- thejointsamples(θ,v), wecandiscardthemomentumv
sureoftheposteriorperformance. and we are left with samples from our target distribution
π(θ).
Dataaugmentation. Weusedexactlythesamedataaug-
mentation strategy as (Wenzel et al., 2020): left/right flip, HMCalternatesbetweenupdatingthemomentumvonits
border-pad,andrandomcrop. ownandjointlyupdatingtheparameterstogetherwiththe
momentum (θ,v). In order tounderstand this process in-
H.MarkovChainMonteCarlo tuitively,ithelpstothinkof(θ,v)asthestateofaphysical
particlerollingonahill. Theparticlehasapositionθ,mo-
InSection2.1,wedetailedamethodtoestimatetheposte- mentumv,andtheshapeofthishilldependsonthegeom-
riorpredictivedistributionofaBNNthatrequiresaccessto etryofthetargetdistribution. First,weflicktheparticlein
samplesfromtheposterior. Oftenthesesamplesaregener- a random direction to update its momentum. Then we let
atedusingMarkov-ChainMonteCarlo(MCMC),afamily theparticlerollforafewseconds,whichjointlyupdatesits
of algorithms used for sampling continuous distributions positionandmomentumtogether.
wheredirectsamplingisnotpossible. MCMCgeneratesa
Moreformally,whenthemomentumisupdatedonitsown,
Markovchainofsamplesfromthetargetdistributionwhere
itisdrawnfromastandardNormaldistribution:
each sample in the chain depends on the previous sam-
plebutisconditionallyindependentofallthesamplesthat
v∼N(0,I). (40)
camebefore. Themoredependent(correlated)thesesam-
plesare,thelowertheaccuracyofourposteriorpredictive The joint proposal for (θ,v) is generated by simulating
distributionapproximation. Hamiltoniandynamics:
Hamiltonian Monte Carlo (HMC) is often considered the dθ =vdt
“goldstandard”ofMCMCalgorithmsbecauseitiscapable (41)
dv=∇logπ(θ)dt.
of generating samples that have very low autocorrelation
(Neal et al., 2011). Izmailov et al. (2021b) have success- Intheory,iftheHamiltoniandynamicsweresimulatedex-
fully sampled a ResNet20 posterior using HMC but this actly,thesampledvaluesofθ wouldfollowthetargetdis-
18CanaConfidentPriorReplaceaColdPosterior?
Algorithm1SGHMC
1: samplev∼N(0,I) ▷samplemomentum
2: fori←1...n do ▷generaten fromtargetdistribution
samples samples
3: fori←1...Bdo ▷iterateovermini-batches
4: θ ←θ+ϵv ▷positionupdate
5: v←(1−ϵC)v+ϵ∇logπ˜(θ)+N(0,2TCIϵ) ▷mini-batchmomentumupdate
6: endfor
7: θ i ←θ ▷savenewvalue
8: endfor
tributionπ(θ)exactly.Unfortunately,Hamiltoniandynam- turnsfromanexactmethod(withunbiasedposteriorsam-
ics describe a continuous system that we have to approxi- ples)toanapproximatemethod.Duringthemomentumup-
mate in discrete steps, typically using the leapfrog algo- date,inadditiontothestochasticmini-batchnoiseofscale
rithm. AftereachsimulationofHamiltoniandynamics,the B,weinjectNormalnoisethatexactlycompensatesforthe
newproposalforθwillgetacceptedorrejectedatrandom, friction:
withtheacceptanceratedependingontheaccuracyofthe
dθ =vdt (44)
numerical simulation. To achieve any reasonable accep-
tance rate (i.e. above 10%), the discretization steps need dv=∇logπ(θ)dt−Cvdt+N(0,2CIdt)+N(0,2Bdt).
to be extremely small. However, each step requires com-
Thefrictionnolongercompensatesforthecombinednoise
puting the full-batch gradient of the posterior. For mod-
andsothegeneratedsamplesbecomebiased. Fortunately,
els with tens to hundreds of thousands of parameters, this
asϵ→0,theinjectednoiseofscaleC dominatesthemini-
mightamounttomorethan10,000steps(epochs)perpro-
batch noise of scale B = 1ϵV. Hence, as the step size
posal, or over 1 million epochs to generate 100 posterior 2
decreases,thesamplesbecomeasymptoticallyunbiased.
samples. Naturally, this is extremely expensive and very
rarelydoneinpractice(Izmailovetal.,2021b). AnotherdistinctiontoHMCisthat,unlikestandardHamil-
tonian dynamics, the system described in Eq. (44) is not
H.2.StochasticGradientHamiltonianMonteCarlo time-reversible,socomputinganacceptanceprobabilityis
not even possible. This further biases the generated pos-
In HMC, each update of momentum requires computing
terior samples. At the same time, since there is no ac-
thefull-batchgradient: ∆v = ϵ∇logπ(θ). Ifwereplace
cept/reject step, it is possible to sample the momentum
the full-batch gradient ∇logπ(θ) with a mini-batch esti-
onlyonceandkeepitthroughoutthewholeMarkovchain,
mate ∇logπ˜(θ), we can think of this as using the full-
whichincreasestheaveragetrajectorylength.
batch gradient but adding noise that arises from the mini-
batch estimate: ∆v = ϵ∇logπ˜(θ) = ϵ∇logπ(θ) + In the limiting behavior of C → 0, SGHMC reduces to
N(0,ϵ2V). By the central limit theorem, the larger the HMC although the posterior is approximated from mini-
mini-batch size is, the closer the noise approaches a Nor- batches,thereisnoaccept/rejectstepandtheHamiltonian
mal distribution. Denoting B := 1ϵV, we can view the dynamics are approximated using Euler’s method instead
2
noisy mini-batch momentum update as a discretization of oftheleapfrogalgorithm(leapfrogismoreaccurate(Neal
thefollowingsystem: etal.,2011)). Ontheotherend,ifC =1,SGHMCreduces
toStochasticgradientLangevindynamics(Welling&Teh,
dθ =vdt
(42) 2011).
dv=∇logπ(θ)dt+N(0,2Bdt).
Typically, a cold posterior is sampled by modifying the
Introducingnoisetothedynamicsbiasestheproposedsam- SGHMCalgorithminsteadofdirectlytemperingtheposte-
ples toward a uniform distribution (Chen et al., 2014). In rior. Thisistoavoidscalingthelog-posterior,whichcould
theory, if we knew the exact scale of the noise, we could introducenumericalinstability. Morespecifically,wescale
compensateforthenoiseexactlybyintroducingafriction theinjectednoiseinline5ofAlgorithm1bythetempera-
termtothedynamics: tureT:
dθ =vdt v←(1−ϵC)v+ϵ∇logπ˜(θ)+N(0,2TCIϵ). (45)
(43)
dv=∇logπ(θ)dt−Bvdt+N(0,2Bdt).
Naturally, whenT = 1, Eq.(45)isequivalenttothestan-
Unfortunately, in practice, we don’t know the true value dard SGHMC algorithm but when T < 1, it corresponds
ofthenoisescale,soweinsteadintroduceauser-specified tosamplingπ(θ)1/T withscaledhyperparametersC andϵ
frictionparameterC tothesystem. ThisiswhereSGHMC (Wenzeletal.,2020).
19