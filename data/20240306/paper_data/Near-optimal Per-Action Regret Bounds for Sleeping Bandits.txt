Near-optimal Per-Action Regret Bounds for Sleeping
Bandits
QuanNguyen,NishantA.Mehta
DepartmentofComputerScience
UniversityofVictoria
manhquan233@gmail.com, nmehta@uvic.ca
Abstract
We derive near-optimal per-action regret bounds for sleeping bandits, in which
both the sets of available arms and their losses in every roundare chosen by an
adversary. In a setting with K total arms and at most A available arms in each
roundoverT rounds,thebestknownupperboundisO(K√TAlnK), obtained
indirectly via minimizing internal sleeping regrets. Compared to the minimax
Ω(√TA)lowerbound,thisupperboundcontainsanextramultiplicativefactorof
KlnK. We addressthis gap by directly minimizingthe per-actionregretusing
generalizedversionsofEXP3,EXP3-IXandFTRLwithTsallisentropy,thereby
obtaining near-optimal bounds of order O(√TAlnK) and O( T√AK). We
extendourresultstothesettingofbanditswithadvicefromsleepingexperts,gen-
p
eralizingEXP4alongtheway. Thisleadstonewproofsforanumberofexisting
adaptiveandtrackingregretboundsforstandardnon-sleepingbandits.Extending
our results to the banditversion of expertsthat reporttheir confidencesleads to
newboundsfortheconfidenceregretthatdependsprimarilyonthesumofexperts’
confidences. We provealowerbound,showingthatforanyminimaxoptimalal-
gorithms, there exists an action whose regret is sublinear in T but linear in the
numberofitsactiverounds.
1 INTRODUCTION
The multi-armed bandit (MAB) framework and its variants have been widely used for practical
applicationsin variousdomainssuch as clinical trials, finance andrecommendersystems (see e.g.
Bouneffoufetal.,2020). InthestandardMABframework,alearnerinteractswithK armsoverT
rounds. Ineachround,thelearnerchoosestoobservethelossofoneofthearms. Whilethelosses
ofthearmsineachroundareunknowntothelearner,thenumberofarmsK isassumedtobefixed
in everyround. However,thisassumptiondoesnotalwaysholdin practice. For example,in drug
testing where each arm is a drug type, certain types of drugs can only be tested in some certain
rounds,ornewandmoreeffectivedrugsmightbeavailableonlyinlaterrounds. Insuchscenarios,
itisimportanttohavealgorithmscapableoflearningwithtime-varyingsetsofavailablearms.This
is the sleeping bandits setting (Kleinbergetal., 2010), where in each roundt = 1,...,T, only a
subsetA 1,...,K ofactivearmsareaccessibletothelearner.
t
⊆{ }
Thesleepingexpertssetting(alsoknownasthespecialistsetting)(Blum,1997;FreundandSchapire,
1997) is the full-information feedback variant of this problem, in which the losses of the ac-
tive arms are revealed at the end of each round. Prior works on sleeping experts have mainly
used two different notions of regret to measure the performance of a learner, namely per-action
regret (BlumandMansour, 2007; Gaillardetal., 2014; LuoandSchapire, 2015) and ordering re-
gret(Kleinbergetal.,2010;KanadeandSteinke,2014;NeuandValko,2014). Besidesthenotions
ofregrets,animportantcharacteristicofthesettingisthestochasticoradversarialnatureofthesets
Preprint.ToappearatAISTATS2024.
4202
raM
2
]GL.sc[
1v51310.3042:viXraTable1: ASummaryofBoundsonPer-ActionRegret. Hyphensindicateboundsthatareeithernot
comparabletoaper-actionregretboundorunavailable.
Algorithms FullyAdversarial? Pseudo-Regret High-Probability
AUER(Kleinbergetal.,2010) No(stochasticlosses) √TKlnT -
Sleeping-EXP3(Sahaetal.,2020) No(stochasticA ) - -
t
SR_MAB(BlumandMansour,2007) Yes K2√TAlnK -
SI-EXP3(Gaillardetal.,2023) Yes K√TAlnK -
SB-EXP3(thiswork) Yes √TAlnK TAln(K/δ)
FTARL(thiswork) Yes T√AK pT√AK+ TAln 1 +ln 1
s δ δ
q (cid:18) (cid:19) (cid:18) (cid:19)
p
A andthearms’losses.KanadeandSteinke(2014)indicatedthatobtainingasublinearorderingre-
t
gretboundiscomputationallyhardwhenbothA andlossesareadversarial.Asaresult,subsequent
t
worksonsleepingbanditsusuallyassumeatleastonecomponenttobe stochastic(Slivkins, 2013;
NeuandValko,2014;Slivkins,2014;Sahaetal.,2020). Recently,Gaillardetal.(2023)developed
anewnotionofregretforsleepingbanditscalledsleepinginternalregret,whichcanbeminimized
efficientlyinthefullyadversarialsettingwithadversarialA andadversariallosses.
t
Ourworkfocusesonminimizingtheper-actionregretinthefullyadversarialsetting. Thisnotionof
regretcomparesthecumulativelossofthelearnertothatofasinglebestarminhindsightduringthe
roundsinwhichthatarmwasactive. Tothebestofourknowledge,inthefullyadversarialsetting,
nopriorworkhasfocusedondirectlyderivingoptimalper-actionregretbounds. Weareinterested
inobtainingmorefine-grainedboundsthatdependonthemaximumnumberofactivearmsinany
roundA, whereA = max A K. Thesmallestexistingboundis theO(K√TAlnK)
t=1,...,T t
| | ≤
bound by Gaillardetal. (2023), obtained indirectly from minimizing the internal sleeping regret.
ThisboundcanbemuchlargerthananΩ(√TA)minimaxlowerboundimpliedbysuitablyadapting
an existing minimaxlowerboundconstructionfor standardbandits(Aueretal., 2002). Moreover,
asweshowinthiswork,thefactorofK outsidethesquarerootcanbeeliminatedentirely.
Anothermotivationforboundingthe per-actionregretin sleeping banditsis its implicationon the
adaptiveandtrackingregretsinstandardnon-sleepingbandits.Adaptiveregret(alsoknownasinter-
valregret)(HazanandSeshadhri,2009;Luoetal.,2018)istheregretagainstafixedarmonatime
interval,whiletrackingregret(alsoknownasshiftingorswitchingregret)(HerbsterandWarmuth,
1998) is the regret against a sequence of arms over T rounds. Previous work obtained adaptive
and tracking regret bounds for standard non-sleepingexperts via a reduction to regret bounds for
sleeping experts (Freundetal., 1997; Adamskiyetal., 2016). In bandits, instead of the reduction
tosleepingbandits,O˜(√T)bounds1 ontrackingandadaptiveregrethavebeenobtainedviaFixed
Share(Aueretal.,2002;HerbsterandWarmuth,1998;Luoetal.,2018). Ourworkshowsthatthe
reductiontosleepingbanditsalsoleadstoO˜(√T)adaptiveandtrackingbounds.
OverviewofMainResultsandTechniques
We extend the EXP3 (Aueretal., 2002), EXP3-IX (Neu, 2015), Follow-The-Regularized-
Leader (FTRL) with Tsallis entropy (AudibertandBubeck, 2009; Abernethyetal., 2015) and
EXP4(Aueretal.,2002)algorithmsforstandardbanditstosleepingbandits,obtainingnewbounds
thatstrictlygeneralizetheexistingbounds. OurresultsleadtonewproofsforO˜(√T)adaptiveand
trackingregretboundsforstandardbandits.Thegeneralizedalgorithmsandanalysesareadaptedto
thebandit-feedbackversionoftheexpertsthatreporttheirconfidencessetting(BlumandMansour,
2007). AsummaryofourcontributionsincomparisontopriorworksisinTable1. Allofourresults
holdforbothpseudo-regretandhighprobabilityregretbounds. Ourpaperisorganizedasfollows
(allproofsareintheappendix):
1O˜ hidestermsinK,numberofswitchesSandlnT.
2• Section3introducestheO(√TAlnK)andO( T√AK)regretboundsforsleepingban-
dits. These boundsimprovethe best existingO(K√TAlnK) bound,as well as recover
p
the near-optimalO(√TKlnK) and minimaxO(√TK) boundsin non-sleepingbandits.
Section3.1showsanovelalgorithmcalledSB-EXP3anditsO(√TAlnG )regretbound
T
guaranteeforsleepingbandits,whereG K isthenumberofarmsthatwereactiveat
T
≤
leastonceafterT rounds.Itsanalysisreliesonanewtechniqueforboundingthegrowthof
thepotentialfunctionbydecomposingthepotentialatroundt+1basedonthesetofactive
arms in round t. In Section 3.2, the O( T√AK) boundis obtained by the Follow-the-
Active-and-Regularized-Leader(FTARL) algorithm, an adaptation of FTRL with Tsallis
p
entropy to sleeping bandits. Section 3.3 considers the bandit-feedbackversion of the ex-
pertsthatreporttheirconfidencessetting. ApplyingSB-EXP3tothissettingleadstonew
regretboundswhichreplacethedependenceonT andAbythecumulativeconfidenceover
T rounds.
• Section 4 studies the bandits with advice from sleeping experts setting and presents SE-
EXP4, a generalized version of EXP4 algorithm. The analysis developed for SB-EXP3
alsoworksforSE-EXP4,leadingtothesameO(√TKlnM)regretboundofEXP4with
M experts. For standard bandits, this implies an O( TKln(KT)) bound on adaptive
regret. This bound is the same as the one obtained by Luoetal. (2018), but with a dif-
ferentproofbasedonsleepingbanditsinstead ofFixepdShare. Thisalso impliesboththe
O(S KTln(KT)) and O( SKTln(KT)) tracking regret bounds (Aueretal., 2002;
Neu, 2015) for unknown and known number of arm-switches S, respectively, where the
p p
latterisobtainedviarestartingSE-EXP4aftereveryT/S rounds.
• Section5definestheper-actionstronglyadaptiveregretboundasaboundthatdependsonly
onT foreveryactiona,whereT isthenumberofactiveroundsofarma. Extendingthe
a a
constructionofDanielyetal.(2015)fornon-sleepingbanditstosleepingbandits,weshow
a linear Ω(T ) per-actionstrongly adaptive lower bound. This implies that no algorithm
a
cansimultaneouslyguaranteeanoptimalper-actionregretandsublinearo(T )per-action
a
regretforallarms.
2 PRELIMINARIES
Weconsidertheadversarialmulti-armedbanditproblemwithKunderlyingarms,whereKmightbe
unknown.Let[K]= 1,2,...,K . Inroundt=1,2,...,T,a(possiblynon-oblivious)adversary
selectsandrevealsase{ tA [K]o} factivearmstothelearner.LetI =1(resp.I =0)indicates
t i,t i,t
thatarmi is active(resp.in⊆ active)in roundt. Then, foreach armi A , the adversaryselects a
t
(hidden)lossvalueℓ [0,1]. Thelearnerpullsoneactivearmi
∈A
andobserveslossℓ .
i,t
∈
t
∈
t it,t
Thelearner’sgoalistocompetewiththebestarminhindsight. Foranarma [K],theregretof
∈
the learnerwithrespectto arma is the differencein thecumulativeloss ofthe learnerandthatof
armaoveritsactiverounds:
T
R(a)= I (ℓ ℓ ). (1)
a,t it,t
−
a,t
t=1
X
Weprovetwotypesofregretbounds.Thefirstis
max E R(a) ǫ, (2)
a [K]
i1,...,iT
≤
∈
(cid:2) (cid:3)
where the expectation is taken over the sequence of the learner’s selected arms. In standard non-
sleepingbandits,thiscorrespondstothenotionofpseudo-regret(Aueretal.,2002).Iftheadversary
isoblivious,thepseudo-regretisequivalenttotheexpectedregret.Thesecondtypeofboundis
Pr max R(a) ǫ 1 δ, (3)
a [K] ≤ !≥ −
∈
wheretheprobabilityistakenoverthesequenceofthelearner’sselectedarms.
Notations. Let A = A be the numberof active arms in roundt and A = max A be the
t t t [T] t
maximumvalueofA o| ver| T rounds.LetG = A bethesetofarmsthatare∈activeatleast
t t s=1,...,t s
∪
3Algorithm1:SB-EXP3forsleepingbandits
Input: η >0,γ 0
≥
Initializeq˜ =1fori=1,2,...,K;
i,1
foreachroundt=1,...,T do
TheadversaryselectsandrevealsA ;
t
ComputeW = I q˜ ;
Computep
t by(6i )∈;A t i,t i,t
i,t
P
Drawi p andobserveℓˆ =ℓ ;
t
∼
t t it,t
Computelossestimateℓ˜ by(4);
i,t
Updateq˜ by(5).
i,t+1
onceinthefirsttrounds. LetG = G bethesizeofG . Wewriteℓˆ = ℓ forthelearner’sloss
inroundt. Let∆ = p Rn
t
p |
0t
| , n p =1
bt ethen-diment sionait l,t
probabilitysimplex.
n { ∈ | i ≥ i=1 i }
Remark 1. The total number of underlying arms K is fixed before learning, and the adversary
P
cannotchangeK. Ontheotherhand,A andG aredecidedbytheadversaryandvaryovertime.
t t
Assomearmsmayneverbeactive,G canbestrictlysmallerthanK.
T
3 NEAR-OPTIMAL REGRETUPPER BOUNDS
Insleepingbandits,foranyconstantA 2,3,...,K ,thereexistsanΩ(√TA)minimaxpseudo-
∈{ }
regret lower bound. The construction follows that of the minimax lower bound for standard ban-
dits(Aueretal.,2002)withAarmsalwaysactiveandK AarmsalwaysinactiveoverT rounds.In
−
Section3.1,wepresentSB-EXP3(Algorithm1)anditsnear-optimalO(√TAlnG )pseudo-regret
T
andhighprobabilityregretbounds.NotethatSB-EXP3doesnotrequireknowingK. InSection3.2,
we show that when K is known, an FTRL-based algorithm called FTARL (Algorithm 2) obtains
anO(√TAlnK)boundwithnegativeShannonentropyandanO( T√AK)boundwith Tsallis
entropyastheregularizationfunction.
p
3.1 GeneralizedEXP3forSleepingBandits
In standard (non-sleeping)bandits, the EXP3 and EXP3-IX algorithms compute a distribution p
t
over arms in round t based on the estimated regrets in previous rounds. Specifically, arms for
whichtheestimatedregretsarehigherhavelargerprobabilityofbeingsampled.Insleepingbandits,
because arms can have differentand even non-overlappingsets of rounds, it is unclear what kind
of statistics aboutthe armsshouldbe maintainedin eachround. In particular,in anygivenround,
the estimatedregretforan arm mightbe higherthanfor otherarmsdue to itbeingactivein more
previousrounds,notbecauseits lossesaresmallin thoserounds. Nevertheless,we willshowthat
theestimatedregretcanbeusedeffectivelyforselectingarmsineachround.
Algorithm1illustratesSleepingBanditsusingEXP3(SB-EXP3),anadaptationofEXP3andEXP3-
IXtosleepingbandits. Inroundt, SB-EXP3computesaprobabilityvectorp ∆ overG . In
t
∈
Gt t
the firstround, p is the uniformdistribution. The learnersamplesi p andcomputesthe loss
1 t t
estimatesℓ˜ asfollows: ∼
t
ℓ˜
i,t
= (0ℓ pi i,t ,t1 +{i γt I= i,i t} ff oo rr ii ∈ GA tt,
A t,
(4)
∈ \
where γ 0 is a parameter of the loss estimator. When γ = 0, (4) is equivalentto the unbiased
loss estim≥ ate in EXP3. When γ > 0, due to I = 1 for all i A , (4) is the IX-loss estimator
i,t t
∈
in EXP3-IX with exploration factor γ (Neu, 2015). More generally, having different exploration
factorsfordifferentarmsmaybebeneficial.WeexploresuchacaseinSection3.3.
Theweightq˜ ofarmiatthebeginningofroundt+1isdefinedasfollows:
t+1
t
q˜ =exp η I (ℓ ℓ˜ γ I ℓ˜ ) , (5)
i,t+1

i,s is,s
−
i,s
−
j,s j,s

Xs=1 j X∈A s
 
4whereη >0isthelearningrate. Thesamplingprobabilityofarmiisproportionaltoitsq˜ ,i.e.,
i,t
q˜i,t fori A ,
p i,t = (0Wt fori∈ G tt A t, (6)
∈ \
whereW = I q˜ isthenormalizationfactor.
t k ∈A t k,t k,t
Apartfromsettingzerosamplingprobabilityforinactivearms,similartoEXP3andEXP3-IX,SB-
P
EXP3stillfollowsthestrategyofsettingthesamplingprobabilityproportionaltotheexponentialof
theestimatedper-actionregret. Thekeydifferenceistheadded γ I ℓ˜ intheexponent,
whichiscrucialforobtainingnear-optimalhigh-probabilityboun− ds(wj h∈eA res γj, >s j 0,s
). Intuitively,this
P
termsignificantlyreducesthesamplingprobabilityofarmsthatwerefrequentlyactiveinprevious
roundsbutnotfrequentlychosen.Theorems2and3statetheregretboundsofSB-EXP3.
Theorem2. Withγ =0,foranyη >0,Algorithm1guarantees
T
lnG η
max E[R(a)] T + A .
t
a [K] ≤ η 2
∈ t=1
X
TuningηleadstoanO ln(G ) T A bound.
T t=1 t
(cid:18)q (cid:19)
P
Theorem3. Foranyγ η >0,Algorithm1guarantees
≥ 2
T
lnG ln(2G /δ) η
T T
max R(a) + + +γ A
t
a [K] ≤ η γ 2
∈ (cid:18) (cid:19)t=1
X
withprobabilityatleast1 δ. Tuningηandγ leadstoanO ln(G /δ) T A bound.
− T t=1 t
(cid:18)q (cid:19)
P
Note that T A TA. Since A G K, the bounds in Theorems 2 and 3 are gener-
t=1 t ≤ ≤ T ≤
ally smaller than the O(√TKlnK) boundsof EXP3 and EXP3-IX. The difference is significant
P
whenever A K, which holds in many practical applications where the sets of active arms are
≪
sparse.
Analysis Sketch. The analyses of EXP3 and EXP3-IX (Aueretal., 2002; Neu, 2015) treat the
normalizationfactorW asapotentialfunctionandboundthegrowthof Wt+1. Thiswaspossible
t Wt
instandardbanditsbecauseallK armsarealwaysactive,hencep canalwaysberelatedtop .
i,t+1 i,t
However,insleepingbandits,thesetsA andA mightbenon-overlapping,hencetheremightbe
t t+1
norelationshipbetweenW andW . Instead,weuse
t+1 t
Q˜ = q˜ (7)
t i,t
i X∈G T
asthepotentialfunction.Thefollowingkeytechnicallemmaboundsthegrowthof
Q˜
t+1.
Q˜
t
Lemma4. Foranyt 0,
≥
Q˜
t+1 p exp η(ℓˆ ℓ˜ γ ℓ˜ ) .
Q˜ ≤ i,t  t − i,t − j,t 
t i X∈A t j X∈A t
 
The proofof Lemma4 is based on decomposingQ˜ and Q˜ by A and A¯ = G A . If arm
t+1 t t t T t
i A ,itsweightq˜ attimet+1canberelatedtoq˜ viatheupdatein(5). Ifarm\ i / A ,by
t i,t+1 i,t t
∈ ∈
constructionq˜ =q˜ . ThesetwoobservationscontrolthegrowthofQ˜ overQ˜ .
i,t+1 i,t t+1 t
Lemma4impliesaboundonq˜ foranyarmaas
a,T+1
T Q˜
lnq˜ lnQ˜ = ln t+1 .
a,T+1 ≤ T+1 Q˜
t=1 t
X
5Algorithm2:FTARLforsleepingbandits
Input: η >0,γ >0,K 2
InitializeL˜ =0fori=≥ 1,2,...,K;
i,0
foreachroundt=1,...,T do
TheadversaryselectsandrevealsA ;
t
Computeq =argmin ψ (q)+ q,L˜ ;
ComputeWt
= K
Iq ∈∆ qK
;
t h t −1 i
t i=1 i,t i,t
Computep by(8);
i,t
Drawi p anP dobserveℓˆ =ℓ ;
t
∼
t t it,t
foreacharmi [K]do
IfI =1,c∈ omputeℓ˜ by(4);
i,t i,t
IfI =0,computeℓ˜ by(9);
i,t i,t
UpdateL˜ =L˜ +ℓ˜ ;
i,t i,t 1 i,t
−
Sinceq˜ growswiththeestimatedregret,thisboundleadstoanupperboundontheestimated
a,T+1
regret,whichinturnsboundstheactualregret.
ObservethatthedependencyonKinAlgorithm1canberemovedcompletely:theinitializationstep
ofassigningq˜to1canbedoneimplicitly.AllotherexplicitcomputationsonlyusethesetsA andG .
t t
Asaresult,SB-EXP3isindependentofK. ThispropertyissimilartothatoftheAdaNormalHedge
algorithm (LuoandSchapire, 2015), which obtains a low regret bound for sleeping experts when
thetotalnumberofexpertsisunknown. BecausetheanalysisofAdaNormalHedgerelieson[0,1]-
boundedlossvectors,itdoesnotapplytosleepingbanditswiththelossestimatesin(4).
Remark5. Lemma4enablestheproofsofbothpseudo-regretandhigh-probabilityboundswithout
significantmodificationstothealgorithm.Thisisamajoradvantageoverexistingworksinsleeping
bandits,whichprovidedonlypseudo-regretbounds.
In bothTheorems2 and3, optimallytuningthe learningrate requiresknowingG and T A ,
T t=1 t
whichmaynotbeavailableapriori.InAppendixF,wepresentAlgorithm5whichusesatwo-level
doublingtricktoobtainapseudo-regretboundofthesameorderwithoutknowingtheseqPuantities
beforehand.
Theorem6. ForanyT 2,Algorithm5(inAppendixF)guaranteesthat
≥
T
4
max E[R(a)] ln(G ) A .
a ∈[K] ≤ (√2 −1)2v u
u
T Xt=1 t
t
The same techniquecan be used to obtaina high-probabilityregretbound; however, the resulting
boundisslightlylarger(inthelogarithmicterm)duetotheunionbound.
3.2 GeneralizedFTRLforSleepingBandits
Instandardnon-sleepingbandits,FTRLwithTsallisentropyobtainsanO(√TK)minimaxpseudo-
regret bound and an O( TKln(1/δ)+ ln(1/δ)) high probability bound (AudibertandBubeck,
2009; Luo, 2017). In sleeping bandits, under the assumption that K is known, the Follow-the-
Active-and-Regularized-pLeader (FTARL) strategy in Algorithm 2 with 1-Tsallis entropy obtains
2
O( T√AK)pseudo-regretandO( T√AK+ TAln(1/δ)+ln1/δ)high-probabilitybounds.
WhenA=K,theboundsofFTARLrecoverthoseofFTRLonstandardbandits.
p p p
Inroundt,Algorithm2computestheweightvectorsq ∆ usingFTRL,i.e.,
t K
∈
q =argminψ (q)+ q,L˜ ,
t t t 1
q ∈∆K h − i
whereψ istheregularizationfunctionandL˜ RK isthecumulative(estimated)lossvectorofK
t t
arms.Inparticular,L˜ = t ℓ˜ . Notethat∈ thisstepispossiblebecauseK andthesimplex∆
i,t s=1 i,t K
areknown.
P
6Whileq isavalidprobabilityvectoroverK arms,itcannotbeuseddirectlyforsamplingbecause
t
inactivearmsmighthavenon-zeroelementsinq . Thesamplingprobabilityvectorp iscomputed
t t
bytakingelementsinA fromq andnormalizing:
t t
I q
p = i,t i,t . (8)
i,t
q
i ∈A t i,t
Similar to Algorithm 1, once an arm i pPis sampled, the loss estimates of active arms are
t t
∼
constructedby(4). ThekeydifferenceinAlgorithm2comparedtoAlgorithm1isthattheinactive
armshavenon-zerolossestimates. Foranarmi / A ,
t
∈
ℓ˜ =ℓˆ γ ℓ˜ . (9)
i,t t j,t
−
j X∈A t
In AppendixG, we show thatby havingnon-zeroloss estimatesfor inactivearmsas in (9), Algo-
rithm 2 with negative Shannon entropy regularizer is equivalent to Algorithm 1. In general, the
motivation behind (9) is mostly technical: in every round, it ensures that the loss vector L˜ con-
t
tainsonlynon-negativevalues. Thisfacilitatesusinga localnormanalysisofstandardFTRL (e.g.
Orabona,2019). Foranunbiasedlossestimatorwhereγ = 0, using(9)impliesthattheestimated
regretisequivalentto
T T
I (ℓˆ ℓ˜ )= ℓˆ ℓ˜ ,
a,t t a,t t a,t
− −
Xt=1 Xt=1(cid:16) (cid:17)
whichresemblestheregretofa standardnon-sleepingexpertsproblemwithinputlossvectorℓ˜. A
similartechniqueisusedbyChernovandVovk(2009).
UsingtheTsallisentropyψ t(x) = η1 1 −P 1K i= β1xβ i withparameterβ,weobtaintheregretbounds
(cid:18) − (cid:19)
ofAlgorithm2inTheorems7and8.
Theorem7. Withγ =0,foranyβ (0,1),η>0,Algorithm2withTsallisentropyguarantees
∈
K1 β η
max E[R(a)] − + TAβ.
a [K] ≤ η(1 β) 2β
∈ −
Settingβ = 1 andtuningηleadstoanO 2T√AK bound.
2
(cid:16)p (cid:17)
Theorem8. Foranyβ,γ,η (0,1),Algorithm2withTsallisentropyguarantees
∈
K1 β ηAβT η+β 1
R(a) − + +γAT + + ln 3/δ
≤ η(1 β) β 2βγ 2
− (cid:18) (cid:19)
(cid:0) (cid:1)
simultaneouslyforalla [K]with probabilityatleast1 δ. Lettingβ = 1 andtuningη andγ
∈ − 2
leadstoanO T√AK + TAln(3/δ)+ln(3/δ) bound.
Remark9. If(cid:16)Gp K isknpown,thenwecanreplace(cid:17)K byG inAlgorithm2andinTheorems7
T T
≤
and8.
AnalysisSketch. SimilartotheanalysisofAlgorithm1,theregretboundofAlgorithm2isobtained
byboundingtheestimatedregret T ℓˆ ℓ˜ .BythelocalnormanalysisofFTRLandproperties
t=1 t − a,t
ofTsallisentropy,wehave
P
T T K1 β 1 T η K
ℓˆ
t
−
ℓ˜
a,t
≤ η(1
−
β)
+
2 β
ℓ˜2 i,tq i2 ,−tβ.
t=1 t=1 − t=1 i=1
X X X X
In both cases γ = 0 and γ > 0, because inactive arms have non-zeroelementsin both ℓ˜ and q˜,
t t
theycontributeanon-zeropositiveamountinthelasttermontheright-handside. Furthermore,as
the computation of ℓ˜ uses p , we wish to replace q by p . The following technical lemma
i,t i,t i,t i,t
achievesthis.
7Lemma10. Foranyt 1andβ (0,1),
≥ ∈
K
ℓ˜2 i,tq i2 ,−tβ
≤
ℓ˜2 i,tp2 i,−tβ.
Xi=1 i X∈A t
T eih the en r, tw ake inb gou thn ed expi e∈cA tat tℓ˜ io2 i, ntp (2 i w,−t hβ en≤ γA =β t 0u )s oin rg uss it na gnd ca or nd ceto no trl as tis ou nch ina es quH ao lil td iee sr’ os fi tn he eq Iu Xal -i lt oy s. sF esin tia mll ay -,
P
tor(Neu,2015)leadstotheO( T√AK)bound.
Remark11. Thetechniqueofpassigningtheobservedlosstosleepingarmsin(9)issimilartothe
reductionfromsleepingexpertstostandardpredictionwithexpertadvice(ChernovandVovk,2009;
Gaillardetal., 2014). However, without Lemma 10, this reduction by itself does not immediately
implyTheorems7and8.
Remark12. WhenK islargecomparedtoA(i.e. sparseactionsets), theO T√AK bound
of FTARL can be much larger than the O √TAlnG bound of SB-EXP3(cid:16).pIn contras(cid:17)t, when
T
A=Θ(K)asinstandardnon-sleepingban(cid:16)dits,FTARLg(cid:17)ivessmallerbounds.
3.3 BoundsonConfidenceRegret
To the adversary, selecting the set A in roundt is equivalentto selecting K binary values I
t i,t
∈
0,1 . We generalize the setting further by having the adversary select real-valued I [0,1].
i,t
{ } ∈
Thisnewsettingisthebanditfeedbackvariantoftheexpertsthatreporttheirconfidencessetting,in
whichI istheconfidenceofexpertiinroundt(BlumandMansour,2007). Inthiscase,R(a)is
i,t
theconfidenceregretwithrespecttoarma(Gaillardetal.,2014).
Moreconcretely,atthebeginningofroundttheadversaryselectsandrevealsK real-valuedI
i,t
[0,1]. The set of active arms becomes A = i [K] : I > 0 . We apply Algorithm 1 t∈ o
t i,t
{ ∈ }
this problem without any modifications: the computations of ℓ˜ ,q˜ and p are the same as in
i,t i,t i,t
Equations(4), (5) and(6). Thefullprotocolandalgorithmaregivenin AppendixC. We state the
followinghighprobabilityregretbound.
Theorem13. Withoptimallytunedηandγ,Algorithm1guarantees
T K
R(a) O I ln(G /δ)
≤
v i,t T 
ut=1 i=1
uXX
t 
 
simultaneouslyforalla [K]withprobability1 δ.
∈ −
Observe that in this setting, because I can be differentfor differentarms in A , SB-EXP3 uses
i,t t
a differentimplicitexplorationfactor γ = γI in (4). We call this novelstrategy based on the
i,t i,t
arm-dependentIX-loss estimator confidentimplicit exploration. The proofof Theorem13 mostly
followsthatofTheorem3withoneaddedkeyinsight:theconcentrationinequalitiesoftheoriginal
IX-lossestimatoralsoholdforthearm-dependentIX-lossestimator.
Remark14. AnO T K I lnG pseudo-regretboundisobtainedviaasimilaranal-
t=1 i=1 i,t T
ysis. WhenI are(cid:18) bq inaPry, sinPce K I =(cid:19) A A, these boundsrecoverthe boundsin Theo-
i,t i=1 i,t t ≤
rems2and3.
P
Remark 15. Similar to Theorem 6, the two-level doubling trick presented in Appendix F can be
used on T K I and ln(G ) to obtain pseudo-regret and high-probability bounds of the
t=1 i=1 i,t T
sameorder(uptoalogarithmicfactor).
P P
4 BANDITS WITHADVICEFROMSLEEPING EXPERTS
The bandits with expert advice framework (Aueretal., 2002) considers the non-sleeping MAB
where M experts give advice to the learner in each round. We study a variant of this problem
8Algorithm3:SE-EXP4forbanditswithadvicefromsleepingexperts
Input: η >0,γ >0
Initializeq˜ =1form=1,2,...,M;
m,1
foreachroundt=1,...,T do
AnadversaryselectsandrevealsB andE ;
t t
Computez by(12)andp =z E ;
m,t t t t
Drawi p andobserveℓˆ =ℓ ;
t
∼
t t it,t
Computeℓ˜ k,t = ℓk, pt k1 { ,ti +t= γk } forarmsk ∈[K];
foreachawakeexpertm B do
t
Constructlossestimat∈ ex˜ = E ,ℓ˜ ;
m,t m,t t
h i
Updateq˜ by(11).
m,t+1
where a time-varyingset B [M] of awake experts gives advice to the learner in round t. The
t
adviceofexpertm B isE⊆ ∆ . Thelearneraimstocompetewitheachexpertduringtheir
t m,t K
∈ ∈
active rounds. More formally,let I = 0 and I = 1 denotewhetherexpertm is sleepingor
m,t m,t
awakeinroundt,respectively. Letℓ bethe(hidden)lossvectorofK armsinroundt. Theregret
t
withrespecttomafterT roundsis
T
R(m)= I (ℓ E ,ℓ ). (10)
m,t it,t
−h
m,t t
i
t=1
X
All theorems in this section are high probability bounds. The pseudo-regret bounds are in Ap-
pendixD.
4.1 GeneralizedEXP4
Inthenon-sleepingsettingwhereallM expertsarealwaysactive,EXP4andEXP4-IX(Aueretal.,
2002;Neu,2015)obtainanO(√TKlnM)pseudo-regretandO( TKln(M/δ))high-probability
bound,respectively.WewillshowthatSE-EXP4(Algorithm3)obtainsthesetwoboundsaswellin
p
thebanditswithadvicefromsleepingexpertssetting.
Let B = B be the number of awake experts at round t. Let E be the B K matrix whose
t t t t
| | ×
tc ho el num san msa pr le es(E anm, at r) mm ∈iB t. In Eround .t, TS hE is-E isX eP q4 us iva am lep nle ts toan saex mp pe lr it nm
g
t ifrom pad di is rt eri cb tlu yt ,io wn hz et
re∈
p∆ B =t,
t
∼
mt,t t
∼
t t
z E (LattimoreandSzepesvári,2020). ThemainideaofSE-EXP4istotakethesleepingexperts
t t
as“augmented”sleepingarmsandapplySB-EXP3. Inparticular,theweightofexpertmis
t 1 K
q˜ =exp − I ℓˆ γ ℓ˜ x˜ , (11)
m,t  m,s  s − j,s − m,s 
s=1 j=1
X X
 
  
where x˜ is the estimated loss of expert m and ℓ˜ is the estimated loss of arm j in round s.
m,s j,s
Initially,q˜ =1. Foranexpertm B ,itssamplingprobabilityz isproportionaltoq˜ ,i.e.,
m,1 t m,t m,t
∈
q˜
z = m,t . (12)
m,t
q˜
j ∈B t j,t
Note that z = 0 for m / B . After i pPis sampled, the loss estimates ℓ˜ of K arms are
m,t t t t t
∈ ∼
computedasin(4). Then,thelossesoftheawakeexpertsareestimatedbytheinnerproductoftheir
adviceandℓ˜:
t
x˜ = E ,ℓ˜ . (13)
m,t m,t t
h i
UsingthesameanalysisofSB-EXP3implies:
Theorem16. Foranyγ,η (0,1),η 2γ,SE-EXP4guarantees
∈ ≤
lnM ln(2M/δ) η
R(u) + +(γ+ )TK+ln(2/δ) (14)
≤ η 2γ 2
9simultaneouslyforallexpertsu [M]withprobabilityatleast1 δ,wheretheprobabilityistaken
∈ −
over the sequenceofthe learner’sselected arms. Tuning η andγ leadsto an O( TKln(M/δ))
bound.
p
4.2 AdaptiveandTrackingBoundsforStandardAdversarialBandits
FollowingHazanandSeshadhri(2009),theadaptiveregretofthelearneronaninterval[t ,t ]with
1 2
respecttoarmkinstandardnon-sleepingbanditsis
t2
R (k)= ℓ ℓ ,
[t1,t2] it,t
−
k,t
t X=t1
(cid:0) (cid:1)
wherei isthearmchosenbythelearnerinroundt. ToobtainanadaptiveregretboundusingSE-
t
EXP4,wefollowthe“virtualexperts”strategysimilartoAdamskiyetal.(2016): foreachinterval
[t ,t ] and arm k, we create a virtualexpertindexedby (k,t ,t ) that is awake from roundt to
1 2 1 2 1
roundt withadviceE = e foranys t, wheree isthekth vectorinthestandardbasis
2 (k,t),s k k
≥
ofRK. ThereareK T = KT(T+1) such experts. For anyinterval[t ,t ], the regretsofexperts
2 2 1 2
(1,t ,t ),(2,t ,t ),...,(K,t ,t )areboundedbyTheorem16. Thisimpliesthefollowingresult.
1 2 1 2 1 2
(cid:0) (cid:1)
Theorem17. Foranyγ,η (0,1),η 2γ,SE-EXP4withvirtualexpertsguaranteesthat
∈ ≤
2ln(KT) ln(KT/δ) η
R (k) + +(γ+ )TK+ln(2/δ)
[t1,t2] ≤ η γ 2
simultaneouslyforallintervals[t ,t ] andarms k [K] with probability1 δ. Tuningη andγ
1 2
∈ −
leadstoanO( TKln(KT/δ))bound.
Next, we use Tpheorem 17 to bound the tracking regret. The tracking regret of algorithm with
A
respecttoasequenceofarmsj =(j ,j ,...,j )is
1:T 1 2 T
T
R(j )= (ℓˆ ℓ ).
1:T t
−
jt,t
t=1
X
Let S = T 1 j = j be the (unknown) number of switches in the sequence j .
Since there
art e= S2 +{ t
1
i6 ntervt −al1 s}
of different competing arms, Theorem 17 immediately
impl1 i: eT
s
P
an O S TKln(KT) trackingbound. Furthermore, the followingcorollaryshows that if S is
known(cid:16) ,tphenatrackingb(cid:17) oundoforderO( STKln(KT))isattainableaswell. Thus,techniques
δS
basedonsleepingbanditsrecoverthetrackqingboundsinAueretal.(2002)andNeu(2015).
Corollary18. RestartingSE-EXP4withvirtualexpertsaftereveryT/Sroundsguaranteesthatfor
anyj whereH(j ) S,withprobabilityatleast1 δ
1:T 1:T
≤ −
KT
R(j ) O STKln .
1:T
≤ s δS 
(cid:18) (cid:19)
 
Remark19. Luoetal.(2018)obtainedadaptiveandtrackingpseudo-regretboundsforcontextual
banditswhichrecoverthesamebounds(uptoconstantsandlogarithmicfactors)instandardbandits.
Ourresultsholdforbothpseudo-regretandhighprobabilitybounds.
5 A PER-ACTION STRONGLYADAPTIVELOWER BOUND
Let T = t [T]:I =1 be the number of rounds in which arm a is active. Prior works
a a,t
{ ∈ }
in sleeping experts established per-action regret boundsof order O(√T ), independentof T (e.g.
(cid:12) (cid:12) a
Chernovan(cid:12)dVovk, 2010; Gailla(cid:12)rdetal., 2014). In this work, we call T-independentboundsthat
guaranteeR(a) = o(T )forallactiona [K] per-actionstronglyadaptivebounds. Thebounds
a
∈
obtainedbySB-EXP3andFTARLinSection3haveO(√T)dependencyforallarmsa,thusthey
are not strongly adaptive. We study whether strongly adaptive bounds are achievable in sleeping
10bandits. Thefollowingtheoremshowsalinearper-actionstronglyadaptivelowerboundforalarge
class of algorithms that contains SB-EXP3, FTARL in Section 3 as well as any minimax optimal
algorithm.
Theorem20. Forany(possiblyrandomized)algorithmwithguarantee
sup E[R(a)] O(TγAβ(ln(T))µ), (15)
≤
a [K]
∈
whereγ (0,1),β 0,µ 0areconstants,thereexistsanumberofarmsK andsequenceofsets
∈ ≥ ≥
ofactivearmsandtheirlossessuchthatA=2andforatleastonearma [K],
∈
T Ω(T1 γ(ln(T)) µ)andE[R(a)] Ω(T ).
a − − a
≥ ≥
Remark21. Bysettingγ =β = 1,µ=0,(15)correspondstotheO(√TA)boundofanyminimax
2
optimalalgorithms.Thisimpliesthatnoalgorithmscansimultaneouslyhaveanoptimalper-action
regretboundandasublinearper-actionstronglyadaptivebound.
OurproofextendsthatofDanielyetal.(2015)tosleepingbandits. Specifically,thesetupcontains
arm1thatisalwaysactivewithsmalllosses,whiletheotherK 1armsareactiveinK 1non-
− −
overlappingintervalswithlargelosses,oneintervalforeacharm. Toguaranteesmallregretagainst
arm 1, with high probability, the learner must pull only arm 1 in the intervalof some arm j > 1.
Consideraslightlydifferentsetupwherethelossesofarmjaresmallerthanthatofarm1.Because
thesequenceofactivearmsandtheirlossesinthetwosetupsareidenticalfromthefirstrounduntil
the start of the interval of arm j, the learner is unable to distinguish between the two setups and
incurslinearregretagainstarmj inthesecondsetup.
The limitation of this construction is that K has to grow with T. In particular, we require K of
order T1 γ2 β(ln(T)) µ. As a result, algorithms with bounds that are sublinear in T but have
− − −
largedependencyonK,forexampleO(TγK),donotsatisfy(15). NotethatSB-EXP3andFTARL
alwayssatisfy(15),since√TAlnK √TAlnT and T√AK T3/4A1/4foranyK T.
≤ ≤ ≤
p
6 CONCLUSION
Wederivedalgorithmsforsleepingbanditsandprovedtheirnear-optimalper-actionregretbounds.
Thesealgorithmsandtheirregretboundsstrictlygeneralizeexistingapproachesandresultsforstan-
dard non-sleeping bandits. We showed that sleeping bandits-based approaches both imply new
bounds and recover a number of existing order-optimal O˜(√T) bounds in related settings with
fundamentallydifferentproofs. Furthermore,theanalysiscan beusedtoshowbothpseudo-regret
andhighprobabilityboundsbyusingeithertheunbiasedorIX-lossestimators.
A direction for future work is to either prove an Ω(√TAlnG ) lower bound, or show that an
T
O(√TA) upper bound is possible, thereby obtaining minimax optimal bounds. For the former,
such a lower boundmust hold only in restricted conditionssuch as AlnG < K, so there is no
T
contradictiontotheoptimalO(√TK)boundinnon-sleepingbandits.Thelatterlikelyrequiresnew
analysistechniquesotherthanthepotential-basedanalysisandFTRL.
Acknowledgments
ThisworkwassupportedbytheNSERCDiscoveryGrantRGPIN-2018-03942.
References
Abernethy,J.D.,Lee,C., andTewari,A.(2015). Fightingbanditswithanewkindofsmoothness.
InAdvancesinNeuralInformationProcessingSystems,volume28.
Adamskiy,D.,Koolen,W. M.,Chernov,A., andVovk,V.(2016). Acloserlookatadaptiveregret.
JournalofMachineLearningResearch,17(23):1–21.
11Audibert,J.-Y.andBubeck,S.(2009). Minimaxpoliciesforadversarialandstochasticbandits. In
Proceedingsofthe22ndAnnualConferenceonLearningTheory(COLT).
Auer,P., Cesa-Bianchi,N., Freund,Y.,andSchapire,R.E.(2002). Thenonstochasticmultiarmed
banditproblem. SIAMJournalonComputing,32(1):48–77.
Blum, A. (1997). Empiricalsupportfor winnow and weighted-majorityalgorithms: Results on a
calendarschedulingdomain. MachineLearning,26(1):5–23.
Blum, A.andMansour,Y.(2007). Fromexternaltointernalregret. JournalofMachineLearning
Research,8:1307–1324.
Bouneffouf, D., Rish, I., and Aggarwal, C. (2020). Survey on applications of multi-armed and
contextualbandits. In2020IEEECongressonEvolutionaryComputation(CEC),pages1–8.
Chernov,A.andVovk,V.(2009).Predictionwithexpertevaluators’advice.InAlgorithmicLearning
Theory,pages8–22,Berlin,Heidelberg.
Chernov,A.V.andVovk,V.(2010). Predictionwithadviceofunknownnumberofexperts. CoRR,
abs/1006.0475.
Daniely, A., Gonen, A., and Shalev-Shwartz, S. (2015). Strongly adaptive online learning. In
Proceedingsofthe32ndInternationalConferenceonMachineLearning,volume37,pages1405–
1411,Lille,France.
Freund,Y. andSchapire,R. E.(1997). ADecision-TheoreticGeneralizationofOn-LineLearning
andanApplicationtoBoosting. JournalofComputerandSystemSciences,55(1):119–139.
Freund,Y., Schapire,R.E., Singer,Y., andWarmuth,M.K.(1997). Usingandcombiningpredic-
torsthatspecialize. In Proceedingsofthe Twenty-Ninth AnnualACMSymposiumon Theoryof
Computing,STOC’97,page334–343,NewYork,NY,USA.
Gaillard,P.,Saha,A.,andDan,S.(2023). Onearrow,twokills: Aunifiedframeworkforachieving
optimalregretguaranteesinsleepingbandits. InProceedingsofThe 26thInternationalConfer-
enceonArtificialIntelligenceandStatistics,volume206,pages7755–7773.
Gaillard,P.,Stoltz,G.,andErven,T.(2014). Asecond-orderboundwithexcesslosses. Journalof
MachineLearningResearch,35.
Hazan, E. and Seshadhri, C. (2009). Efficientlearningalgorithmsfor changingenvironments. In
Proceedingsofthe26thAnnualInternationalConferenceonMachineLearning,ICML’09,page
393–400,NewYork,NY,USA.
Herbster,M.andWarmuth,M.K.(1998). Trackingthebestexpert. MachineLearning,32(2):151–
178.
Kanade, V. and Steinke, T. (2014). Learning hurdles for sleeping experts. ACM Trans. Comput.
Theory,6(3).
Kleinberg,R.,Niculescu-Mizil,A.,andSharma,Y.(2010). Regretboundsforsleepingexpertsand
bandits. MachineLearning,80(2–3):245–272.
Lattimore,T.andSzepesvári,C.(2020). BanditAlgorithms. CambridgeUniversityPress.
Luo, H. (2017). Lecture 13, Introduction to Online Learning.
https://haipeng-luo.net/courses/CSCI699/lecture13.pdf.
Luo,H.andSchapire,R.E.(2015).Achievingallwithnoparameters:AdaNormalHedge.InAnnual
ConferenceComputationalLearningTheory.
Luo, H., Wei, C.-Y., Agarwal, A., and Langford, J. (2018). Efficient contextual bandits in non-
stationary worlds. In Bubeck, S., Perchet, V., and Rigollet, P., editors, Proceedingsof the 31st
ConferenceOnLearningTheory,volume75ofProceedingsofMachineLearningResearch,pages
1739–1776.PMLR.
12Neu, G. (2015). Explore no more: Improved high-probability regret bounds for non-stochastic
bandits. InAdvancesinNeuralInformationProcessingSystems,volume28.
Neu,G.andValko,M.(2014). Onlinecombinatorialoptimizationwithstochasticdecisionsetsand
adversariallosses. InAdvancesinNeuralInformationProcessingSystems,volume27.
Orabona,F.(2019). Amodernintroductiontoonlinelearning. CoRR,abs/1912.13213.
Saha, A., Gaillard, P., and Valko, M. (2020). Improved sleeping bandits with stochastic actions
sets and adversarialrewards. In Proceedingsof the 37th InternationalConference on Machine
Learning,ICML’20.
Slivkins,A.(2013). Dynamicadallocation:Banditswithbudgets. CoRR,abs/1306.0155.
Slivkins,A.(2014). Contextualbanditswithsimilarityinformation. JournalofMachineLearning
Research,15(1):2533–2568.
13A PROOFSFORSECTION 3.1
Recall thatQ˜ = q˜ . Foranyset S G , let Q˜ = q˜ bethe projectionof Q˜
ontothesetSt . LetS¯i ∈ =G T Gi,t S bethecompl⊆ emenT tofS.S N,t otethati ∈thS eni, ot rmalizationfactorW ist
P T \ P t
equalto i ∈A tq˜ i,t =Q˜ A t,t.
First,wePproveatechnicallemmawhichholdsforanyI
i,t
[0,1]foralli [K]andt [T].
∈ ∈ ∈
Lemma22. Foranyt 1,
≥
E [ℓ˜ ]=ℓˆ γ I ℓ˜ .
i ∼pt i,t t
−
j,t j,t
j X∈A t
Proof. Usingℓ˜ =0forj =i ,weobtain
j,t t
6
E [ℓ˜ ]= p ℓ˜
i ∼pt i,t i,t i,t
i X∈A t
=p ℓ˜
it,t it,t
ℓˆ
t
=p
it,t
p +γI
it,t i,t
ℓˆ
=ℓˆ γI t
t
−
it,t
p +γI
it,t it,t
=ℓˆ γI ℓ˜
t
−
it,t it
=ℓˆ γ I ℓ˜ .
t j,t j,t
−
j X∈A t
A.1 ProofofLemma4
Wemakeuseofthefollowingtwofactsthatcanbeprovedeasily:
• Fact1: thefunctionf(x)=e ηxisconvexforanyη R.
−
∈
• Fact2: Foranya,b>0,c 0,ifa bthen
≥ ≥
a a+c
.
b ≥ b+c
Lemma4. Foranyt 0,
≥
Q˜
t+1 p exp η(ℓˆ ℓ˜ γ ℓ˜ ) .
Q˜ ≤ i,t  t − i,t − j,t 
t i X∈A t j X∈A t
 
Proof. ByJensen’sinequalityandFact1,wehave
p exp( ηℓ˜ )=E [exp( ηℓ˜ )]
i,t
−
i,t i ∼pt
−
i,t
i X∈A t
exp ηE [ℓ˜ ]
≥ −
i ∼pt i,t
(cid:16) (cid:17)
=exp η ℓˆ γ ℓ˜ ,
−  t − j,t 

j X∈A t

  
14wherethelastequalityisduetoLemma22. Multiplyingbyexp η ℓˆ γ ℓ˜ Q˜ >
0onbothsides,weobtain (cid:18) (cid:16)
t − Pj ∈A t j,t
(cid:17)(cid:19)
At,t
p i,tQ˜ A t,texp η ℓˆ t −ℓ˜ i,t −γ ℓ˜ j,t  ≥Q˜ A t,t. (16)
i X∈A t

j X∈A t

  
Foralli A ,wehaveI =1andthusp = q˜i,t . Equation(16)isequivalentto
∈ t i,t i,t Q˜ At,t
q˜ i,texp η ℓˆ t −ℓ˜ i,t −γ ℓ˜ j,t ≥Q˜ A t,t.
i X∈A t

j X∈A t

  
Byourupdaterule,q˜ = q˜ fori / A andq˜ = q˜ exp η ℓˆ ℓ˜ γ ℓ˜
fori A t. Hence,
i,t+1 i,t ∈ t i,t+1 i,t
(cid:18) (cid:16)
t − i,t − Pj ∈A t j,t
(cid:17)(cid:19)
∈
q˜ i,t+1 = q˜ i,texp η ℓˆ t −ℓ˜ i,t −γ ℓ˜ j,t  ≥Q˜ A t,t.
i X∈A t i X∈A t

j X∈A t

  
ApplyingFact2fora= i ∈A tq˜ i,t+1,b=Q˜ A t,tandc=Q˜ A¯ t,t,weobtain
P
q˜ exp η ℓˆ ℓ˜ γ ℓ˜
i X∈A
tp i,texp  η ℓˆ t −ℓ˜ i,t −γ
j X∈A
tℓ˜ j,t  =
Pi ∈A t i,t
(cid:18) (cid:16)
Qt ˜−
A
t,ti,t − Pj ∈A t j,t
(cid:17)(cid:19)
   q˜
= i ∈A t i,t+1
P
Q˜
A t,t
i ∈A tq˜ i,t+1+Q˜ A¯ t,t
≥
P
Q˜
A
t,t+Q˜
A¯ t,t
Q˜
t+1
= ,
Q˜
t
(17)
wherethelastequalityisduetothefactthat i ∈A¯ tq˜ i,t+1 = i ∈A¯ tq˜ i,t =Q˜ A¯ t,t.
P P
A.2 BoundingtheEstimatedRegret
UsingLemma4,weboundtheestimatedregretasafunctionofthecumulativeestimatedlossesof
allactivearmsoverT rounds.
Lemma23. Foranyγ 0,anyarma [K],
≥ ∈
T T T
lnG η
I ℓˆ T + I ℓ˜ + +γ ℓ˜ .
a,t t a,t a,t i,t
≤ η 2
Xt=1 Xt=1 (cid:18) (cid:19) Xt=1i X∈A t
Proof. Foranyarma G ,wehave
T
∈
T
lnQ˜ =ln q˜ lnq˜ =η I (ℓˆ ℓ˜ γ ℓ˜ ). (18)
T+1 i,T+1 a,T+1 a,t t a,t j,t
≥ − −
i X∈G T Xt=1 j X∈A t
15Ontheotherhand,wehave
T Q˜
lnQ˜ =lnQ˜ + ln t+1
T+1 1 Q˜
t=1 t
X
T
≤lnG T + ln p i,texp η ℓˆ t −ℓ˜ i,t −γ ℓ˜ j,t 
Xt=1   i X∈A t    j X∈A t    
T
=lnG
T
+ lnexp η ℓˆ
t
−γ ℓ˜
j,t 
p i,texp( −ηℓ˜ i,t)
Xt=1       j X∈A t  i X∈A t   
T
=lnG + η ℓˆ γ ℓ˜ +ln p exp( ηℓ˜ )
T   t − j,t   i,t − i,t 
Xt=1

j X∈A t i X∈A t

    
T η2ℓ˜2
lnG + η ℓˆ γ ℓ˜ +ln p (1+ i,t ηℓ˜ )
≤ T   t − j,t   i,t 2 − i,t 
Xt=1

j X∈A t i X∈A t

    
T p ℓ˜2
=lnG + η ℓˆ γ ℓ˜ +ln 1+η2 i,t i,t η p ℓ˜
T   t − j,t   2 − i,t i,t 
Xt=1

j X∈A t i X∈A t i X∈A t

    
T p ℓ˜2
lnG + η ℓˆ γ ℓ˜ +η2 i,t i,t η p ℓ˜
≤ T   t − j,t  2 − i,t i,t
Xt=1

j X∈A t i X∈A t i X∈A t

T  p ℓ˜2  
=lnG +η2 i,t i,t ,
T
2
Xt=1i X∈A t
where
• thefirstinequalityisduetoLemma4,
• thesecondinequalityisexp( x) 1+
x2
xforallx 0,
− ≤ 2 − ≥
• thethirdinequalityisln(1+x) xforallx 1,
≤ ≥−
• thelastequalityisduetoLemma22.
Sincethelossesareboundedin[0,1]wealsohave p ℓ˜2 ℓ˜ . Thisimplies
i ∈A t i,t i,t ≤ i ∈A t i,t
P P
η2 T
lnQ˜ lnG + ℓ˜ . (19)
T+1 T i,t
≤ 2
Xt=1i X∈A t
From(18)and(19),weobtain
T T
lnG η
I (ℓˆ ℓ˜ γ ℓ˜ ) T + ℓ˜ .
a,t t a,t j,t i,t
− − ≤ η 2
Xt=1 j X∈A t Xt=1i X∈A t
16Adding T I (ℓ˜ +γ ℓ˜ )tobothsidesyields
t=1 a,t a,t j ∈A t j,t
P T P T T T
lnG η
I ℓˆ T + I ℓ˜ + ℓ˜ +γ I ℓ˜
a,t t a,t a,t i,t a,t j,t
≤ η 2
Xt=1 Xt=1 Xt=1i X∈A t Xt=1 j X∈A t (20)
T T
lnG η
T + I ℓ˜ + +γ ℓ˜ ,
a,t a,t i,t
≤ η 2
Xt=1 (cid:18) (cid:19) Xt=1i X∈A t
wherethesecondinequalityisduetoI 1.
a,t
≤
A.3 ProofofTheorem2
Theorem2. Withγ =0,foranyη >0,Algorithm1guarantees
T
lnG η
max E[R(a)] T + A .
t
a [K] ≤ η 2
∈ t=1
X
TuningηleadstoanO ln(G ) T A bound.
T t=1 t
(cid:18)q (cid:19)
P
Proof. IfanarmaisnotinG thenithasneverbeenactiveinanyround,thusR(a) = 0. Foran
T
arma G ,takingtheexpectationofbothsidesofLemma23andusing
T
∈
ℓ
E [ℓ˜ ]=p i,t
it∼pt i,t i,t
p
i,t
=ℓ
i,t
1,
≤
weobtain
T
lnG η
E[R(a)] T + A .
t
≤ η 2
t=1
X
A.4 ProofofTheorem3
Before proving Theorem 3, we state the following lemma and its corollary which provide high-
probabilityguaranteesthatthesumofthelossestimatorsisa lowerconfidenceboundforthesum
ofthetruelossesofallactivearms. ThislemmaisadaptedfromLemma1ofNeu(2015).
Lemma 24 (Lemma 1 of Neu (2015)). For all i [K],t [T] and I [0,1], let α satisfy
i,t i,t
∈ ∈ ∈
0 α 2γI . Withprobability1 δ ,
i,t i,t ′
≤ ≤ −
T K
α i,t1 I
i,t
>0 (ℓ˜
i,t
ℓ i,t) ln(1/δ′).
{ } − ≤
t=1 i=1
XX
TheproofofLemma24isidenticaltothatofLemma1ofNeu(2015)(withoneadditionalstraight-
forward step of handling I = 0) and thus is omitted here. For any fixed j G , applying
i,t T
Lemma 24 with α = 2γI 1 i = j and taking a union bound over all j ∈G leads to the
i,t i,t T
{ } ∈
followingcorollary.
Corollary25. Withprobabilityatleast1 δ ,
′
−
T
ln(G /δ )
I (ℓ˜ ℓ ) T ′
j,t j,t j,t
− ≤ 2γ
t=1
X
holdssimultaneouslyforallj G .
T
∈
17Theorem3. Foranyγ η >0,Algorithm1guarantees
≥ 2
T
lnG ln(2G /δ) η
T T
max R(a) + + +γ A
t
a [K] ≤ η γ 2
∈ (cid:18) (cid:19)t=1
X
withprobabilityatleast1 δ. Tuningηandγ leadstoanO ln(G /δ) T A bound.
− T t=1 t
(cid:18)q (cid:19)
P
Proof. ApplyingLemma24withα = (η/2+γ)I ,δ = δ/2andCorollary25with δ = δ/2
i,t i,t ′ ′
gives
T T
η η
+γ ℓ˜ ln(2/δ)+ +γ ℓ (21)
i,t i,t
2 ≤ 2
(cid:18) (cid:19) Xt=1i X∈A t (cid:18) (cid:19) Xt=1i X∈A t
and
T T
ln(2G /δ)
I ℓ˜ T + I ℓ foranya [K]. (22)
a,t a,t a,t a,t
≤ 2γ ∈
t=1 t=1
X X
Plugging (21) and (22) into the right-handside of Lemma 23 and taking a union bound, we have
withprobabilityatleast1 δ,simultaneouslyforalla [K],
− ∈
T T T K
lnG ln(2G /δ) η
I ℓˆ T + T + I ℓ +ln(2/δ)+ +γ I ℓ .
a,t t a,t a,t i,t i,t
≤ η 2γ 2
t=1 t=1 (cid:18) (cid:19)t=1i=1
X X XX
Subtracting T I ℓ onbothsidesandusingI ℓ 1,weobtain
t=1 a,t a,t i,t i,t ≤
P T
lnG ln(2G /δ) η
R(a) T + T +ln(2/δ)+ +γ A (23)
t
≤ η 2γ 2
(cid:18) (cid:19)t=1
X
withprobabilityatleast1 δ.
−
B PROOFSFORSECTION 3.2
ℓ˜
1,t
LetD : RK RK R betheBregmandivergencegeneratedbyψ . Letℓ˜ = ... bethe
ψt
× →
+ t t 
ℓ˜

K,t
 
estimatedlossvectorin roundt. We write Q = K q forthesumofthe weightsofallarms
t i=1 i,t
atthebeginningroundt. NotethatQ = 1. ForasetS [K],wewriteQ = q forthe
projectionofQ onS.
Thecomplemet
ntofS
isS¯=P [K]⊆
S.
S,t i ∈S i,t
t
\ P
B.1 ProofofLemma10
Lemma10. Foranyt 1andβ (0,1),
≥ ∈
K
ℓ˜2 i,tq i2 ,−tβ
≤
ℓ˜2 i,tp2 i,−tβ.
Xi=1 i X∈A t
Proof. Sinceℓ˜ =0ifi A andi=i ,theright-handsidecanbereducedto
i,t t t
∈ 6
ℓ˜2 i,tp2 i,−tβ =ℓ˜2 it,tp2 it−,tβ.
i X∈A t
18Fortheleft-handside,theestimatedlossesofinactivearmsareequalto
ℓˆ γ ℓ˜ =ℓˆ γℓ˜
t
−
j,t t
−
it,t
j X∈A t
ℓˆ
=ℓˆ γ t
t
− p +γ
it,t
p ℓˆ
=
it,t t
.
p +γ
it,t
Therefore,
K
ℓ˜2 i,tq i2 ,−tβ = ℓ˜2 i,tq i2 ,−tβ + ℓ˜2 i,tq i2 ,−tβ
Xi=1 i X∈A t iX∈A¯ t
p2 ℓˆ2
=ℓ˜2 it,tq i2 t−,tβ +
(p
it, +t t
γ)2
q i2 ,−tβ
it,t iX∈A¯
t
ℓˆ2 p2 ℓˆ2
=
(p
+t γ)2q i2 t−,tβ +
(p
it, +t t
γ)2
q i2 ,−tβ
it,t it,t iX∈A¯
t
ℓˆ2 q
= (p it,t+t γ)2p2 it−,tβ(Q2 A− tβ +pβ it,t
iX∈A¯
tq i2 ,−tβ) sincep it,t = Qi At t,t
,t
ℓˆ2
≤ (p +t γ)2p2 it−,tβ(Q2 A− tβ + q i2 ,−tβ) sincepβ it,t ≤1
it,t iX∈A¯
t
≤ℓ˜2 it,tp2 it−,tβ(QA
t
+ q i,t)
iX∈A¯
t
= ℓ˜2 tp2 i,−tβ,
i X∈A t
w anh der ee acth he qlast fotw rioin Ae ¯qu aa sli wtie es lla ar se td hu ee fato cta tp hp atly qingx ∆α
≤
.
xforallx
∈
[0,1],α > 1onp it,t,QA
t,t
i,t t t K
∈ ∈
B.2 BoundingtheEstimatedRegret
Lemma26. Foranyarma [K],
∈
T K1 β T η T
I a,t(ℓˆ
t
−ℓ˜ a,t)
≤ η(1
−
β)
+γ ℓ˜ j,t+
2β
ℓ˜ i,tp1 i,−tβ,
Xt=1 − Xt=1j X∈A t Xt=1i X∈A t
wheree isthea-thstandardbasisvectorofRK.
a
Proof. From the regret bound of FTRL using local norms (e.g. Orabona, 2019, Lemma 7.12) we
have
T T
1 2
ℓ˜,q e ψ (e ) min ψ (x)+ ℓ˜ , (24)
t t a T+1 a 1 t
Xt=1h − i≤ −x ∈∆K 2
Xt=1(cid:13)
(cid:13)( ∇2ψt(ut))−1
(cid:13) (cid:13)
whereu isapointbetweenq and (cid:13) (cid:13)
t t
q¯ =argmin ℓ˜,x +D (x;q ). (25)
t+1
x RK h
t
i
ψt t
∈
19First,weexaminetheleft-handsidein(24). Obviously ℓ˜,e =ℓ˜ . Furthermore,
t a a,t
h i
K
ℓ˜,q = ℓ˜ q
t t i,t i,t
h i
i=1
X
= ℓ˜ q + ℓ˜ q
i,t i,t i,t i,t
i X∈A t iX∈A¯ t
q p ℓˆ p ℓˆ
=ℓˆ it,t + it,t t q (sinceℓ˜ = it,t t fori A¯ )
t i,t i,t t
p +γ p +γ p +γ ∈
it,t it,t iX∈A¯
t
it,t
p ℓˆ q
= it,t t it,t + q (26)
i,t
p +γ p 
it,t it,t iX∈A¯
t
 
p ℓˆ q
= it,t t q + q (sincep = i,t )
i,t i,t i,t
p +γ   q
it,t i X∈A t iX∈A¯ t j ∈A j j,t
p ℓˆ   P
= it,t t (sinceq ∆ )
t K
p +γ ∈
it,t
=ℓˆ γ ℓ˜ .
t j,t
−
j X∈A t
Therefore, ℓ˜,q e = ℓˆ γ ℓ˜ ℓ˜ . By construction, if I = 0 then ℓ˜ =
ℓˆ γ
h t ℓ˜t .− Hena i
ce,
t − j ∈A t j,t − a,t a,t a,t
t − j ∈A t j,t P
P
T T
ℓ˜,q e = I ℓˆ γ ℓ˜ ℓ˜ .
t t a a,t t t a,t
h − i  − − 
Xt=1 Xt=1 j X∈A t
 
Pluggingthisinto(24)impliesthat
T T T
1 2
I (ℓˆ ℓ˜ ) ψ (e ) min ψ (x)+γ I ℓ˜ + ℓ˜ .
a,t t a,t T+1 a 1 a,t j,t t
Xt=1
− ≤ −x ∈∆K
Xt=1 j X∈A t
2
Xt=1(cid:13)
(cid:13)
(cid:13)
(cid:13)( ∇2ψt(ut))−1
(27)
(cid:13) (cid:13)
2
Next, we bound ℓ˜ on the right-hand side. It can be shown (e.g. Orabona, 2019,
t
Section10.1.2)th(cid:13)atf(cid:13)o( r∇th2ψ et T(u st a) l) l− is1
entropyregularizer,thesolutionoftheoptimizationproblem(25)
(cid:13) (cid:13)
satisfiesq¯ (cid:13)q (cid:13)wheneverℓ˜ 0foralli [K]. Inourconstruction,
i,t+1 t,i i,t
≤ ≥ ∈
• ℓ˜ = ℓit,t 0ifi=i ,
i,t pit,t+γ ≥ t
• ℓ˜ =0ifi A andi=i ,
i,t t t
∈ 6
• ℓ˜ =ℓ γ ℓ˜ = pit,tℓˆ t 0ifi / A .
i,t it,t − j ∈A t j,t pit,t+γ ≥ ∈ t
P
Hence,theconditionℓ˜ 0holdsforalli [K]. Itfollowsthatu q foralli [K].
i,t i,t i,t
≥ ∈ ≤ ∈
It is well-known (e.g. Abernethyetal., 2015) that the Hessian of Tsallis entropy is diagonal and
equalto
β
( 2ψ (x)) = .
∇ t ii ηx2 i−β
20Itfollowsthatitsinverseisadiagonalmatrixwithentries
ηx2 i−β
onthemaindiagonal.
β
(cid:18) (cid:19)i=1,2,...,K
Hence,
K
2 η
(cid:13)ℓ˜
t
(cid:13)( ∇2ψt(ut))−1
=
β
Xi=1ℓ˜2 i,tu2 i,−tβ
(cid:13) (cid:13)
(cid:13) (cid:13) η K
≤ β
ℓ˜2 i,tq i2 ,−tβ
i=1
X
η
≤ β
ℓ˜2 i,tp2 i,−tβ
i X∈A t
η
≤ β
ℓ˜ i,tp1 i,−tβ,
i X∈A t
wherethefirstinequalityisduetou q ,thesecondinequalityisduetoLemma10andthelast
i,t i,t
≤
inequalityisduetoℓ˜ p 1foralli A . Pluggingthisinto(27)andusingI 1gives
i,t i,t t a,t
≤ ∈ ≤
T T T
η
Xt=1I a,t(ℓˆ
t
−ℓ˜ a,t) ≤ψ T+1(e a) −xm ∈∆in Kψ 1(x)+γ
Xt=1j X∈A
tℓ˜ j,t+
2β
Xt=1i X∈A
tℓ˜ i,tp1 i,−tβ
K1 β T η T
≤ η(1
−
β)
+γ ℓ˜ j,t+
2β
ℓ˜ i,tp1 i,−tβ,
− Xt=1j X∈A t Xt=1i X∈A t
where,inthelastinequality,whereweusedψ (e ) min ψ (x)
K1−β
asastandard
propertyoftheTsallisentropyregularizer(e.g.T A+ b1 erna eth− yetalx .,∈ 2∆ 0K 15)1
.
≤ η(1 −β)
B.3 ProofofTheorem7
Theorem7. Withγ =0,foranyβ (0,1),η>0,Algorithm2withTsallisentropyguarantees
∈
K1 β η
max E[R(a)] − + TAβ.
a [K] ≤ η(1 β) 2β
∈ −
Settingβ = 1 andtuningηleadstoanO 2T√AK bound.
2
(cid:16)p (cid:17)
Proof. Withγ =0,Lemma26impliesthat
T K1 β η T
I a,t(ℓˆ
t
−ℓ˜ a,t)
≤ η(1
−
β)
+
2β
ℓ˜ i,tp1 i,−tβ
Xt=1 − Xt=1i X∈A t
K1 β η T
=
η(1
−
β)
+
2β
ℓ it,tp−it,β t,
− t=1
X
wheretheequalityisduetothefactthatfori A ,ℓ = 0ifi = i andℓ˜ = ℓit,t. Takingthe
∈ t i,t 6 t it,t pit,t
expectationoveri p onbothsidesandusing
t t
∼
E it∼pt[ℓ it,tp−it,β t]= p1 i,−tβℓ
i,t
i X∈A t
≤
p1 i,−tβ
i X∈A t
Aβ,
≤ t
wherethefirstinequalityisduetoℓ [0,1]andthesecondinequalityisHolder’sinequality,we
i,t
∈
obtain
K1 β η
E[R(a)] − + TAβ.
≤ η(1 β) 2β
−
21B.4 ProofofTheorem8
Theorem8. Foranyβ,γ,η (0,1),Algorithm2withTsallisentropyguarantees
∈
K1 β ηAβT η+β 1
R(a) − + +γAT + + ln 3/δ
≤ η(1 β) β 2βγ 2
− (cid:18) (cid:19)
(cid:0) (cid:1)
simultaneouslyforalla [K]with probabilityatleast1 δ. Lettingβ = 1 andtuningη andγ
∈ − 2
leadstoanO T√AK + TAln(3/δ)+ln(3/δ) bound.
(cid:16)p p (cid:17)
Proof. WeapplyLemma24twice:
• thefirsttimewithδ
′
=δ/3,α
i,t
=2γI i,tp1 i,−tβ toobtain
T T
ln(3/δ)
p1 i,−tβℓ˜
i,t
≤ 2γ
+ p1 i,−tβℓ
i,t
(28)
Xt=1i X∈A t Xt=1i X∈A t
withprobabilityatleast1 δ/3,
−
• thesecondtimewithδ =δ/3,α =2γI toobtain
′ i,t i,t
T T
ln(3/δ)
ℓ˜ + ℓ (29)
i,t i,t
≤ 2γ
Xt=1i X∈A t Xt=1i X∈A t
withprobabilityatleast1 δ/3.
−
WealsoapplyCorollary25oncewithδ = δ toobtain
′ 3
T T
ln(3G /δ)
I ℓ˜ T + I ℓ (30)
a,t a,t a,t a,t
≤ 2γ
t=1 t=1
X X
withprobabilityatleast1 δ/3.Plugging(28),(29),(30)intoLemma26andtakingaunionbound,
−
weobtainwithprobabilityatleast1 δ,
−
T K1 β ln(3G /δ) T ln(3/δ) T ηln(3/δ) η T
I a,tℓˆ
t
≤ η(1
−
β)
+ 2γT + I a,tℓ a,t+
2
+γ ℓ i,t+
4βγ
+
2β
p i1 ,−tβℓ i,t.
Xt=1 − Xt=1 Xt=1i X∈A t Xt=1i X∈A t
Subtracting T I ℓ onbothsides,weobtain
t=1 a,t a,t
KP1 β ln(3G /δ) ln(3/δ) T ηln(3/δ) η T
R(a)
≤ η(1
−
β)
+ 2γT +
2
+γ ℓ i,t+
4βγ
+
2β
p1 i,−tβℓ
i,t
− Xt=1i X∈A t Xt=1i X∈A t
K1 β ln(3G /δ) η 1 η T
≤ η(1
−
β)
+ 2γT +
4βγ
+
2
ln(3/δ)+γAT +
2β
p1 i,−tβ
− (cid:18) (cid:19) Xt=1i X∈A t
K1 β η η 1 ln(3G /δ)
− + TAβ+γAT + + ln(3/δ)+ T ,
≤ η(1 β) 2β 4βγ 2 2γ
− (cid:18) (cid:19)
where
• thesecondinequalityisduetoℓ [0,1]andA A,
i,t t
∈ ≤
• thelastinequalityisHolder’sinequalityandA A.
t
≤
22Algorithm4:SB-EXP3forexpertsthatreporttheirconfidenceswithbanditfeedback
Input: η >0,γ >0,η γ
≤
Initializeq˜ =1fori=1,2,...,K;
i,1
foreachroundt=1,...,T do
AnadversaryselectsandrevealsK valuesI [0,1];
i,t
∈
Computew =I q˜ andW = K w ;
i,t i,t i,t t i=1 i,t
Computep = wt,i ;
t,i Wt P
Drawi p andobserveℓˆ =ℓ ;
t
∼
t t it,t
Constructlossestimateℓ˜ byEquation(31);
i,t
Updateq˜ byEquation(32).
i,t+1
end
C PROOFSFORSECTION 3.3
First, westateamoregeneraldefinitionoftheactivesetsofarms. Anarmiisactiveinroundtif
I >0i.e. A = i [K]:I >0 . LetG = t A andG = G . Letthepotentialfunction
i,t t { ∈ i,t } t ∪s=1 t t | t |
Q˜ be
t
Q˜ = q˜ .
t i,t
i X∈G T
TheprotocolisgiveninAlgorithm4. Inroundt,thelossestimatoris
ℓ˜
i,t
= ℓ pi i,t ,t1 +{i γt I= i,i t} ifp i,t >0, (31)
(0 otherwise.
andtheupdateruleforq˜ becomes
i,t+1
K
q˜ =q˜ exp ηI ℓˆ ℓ˜ γ I ℓ˜ . (32)
i,t+1 i,t  i,t  t − i,t − j,t j,t 
j=1
X
 

 
BeforeprovingTheorem13,wepresentthefollowinglemmawhichboundsthegrowthof
Q˜
t+1.
Q˜
t
Lemma27. Foranyt 1,
≥
Q˜ W K K K
t+1 1+η t p (ℓˆ ℓ˜ γ I ℓ˜ )+ηI (ℓˆ ℓ˜ γ I ℓ˜ )2
Q˜ ≤ Q˜ i,t  t − i,t − j,t j,t i,t t − i,t − j,t j,t 
t t i=1 j=1 j=1
X X X
 
23Proof. BythedefinitionofQ˜ ,wehave
t+1
K
Q˜ Q˜ = q˜ exp ηI ℓˆ ℓ˜ γ I ℓ˜ q˜
t+1 − t i,t  i,t  t − i,t − j,t j,t  − i,t
i X∈G T

Xj=1

i X∈G T
  
K
= q˜ exp ηI (ℓˆ ℓ˜ γ I ℓ˜ ) 1
i,t  i,t t − i,t − j,t j,t − 
i X∈G T

Xj=1

   
K K
q˜ ηI (ℓˆ ℓ˜ γ I ℓ˜ )+η2I2 (ℓˆ ℓ˜ γ I ℓ˜ )2
≤ i,t  i,t t − i,t − j,t j,t i,t t − i,t − j,t j,t 
i X∈G T Xj=1 Xj=1
 
K K
=η I q˜ (ℓˆ ℓ˜ γ I ℓ˜ )+ηI (ℓˆ ℓ˜ γ I ℓ˜ )2
i,t i,t t i,t j,t j,t i,t t i,t j,t j,t
 − − − − 
i X∈G T Xj=1 Xj=1
 
K K
=ηW p (ℓˆ ℓ˜ γ I ℓ˜ )+ηI (ℓˆ ℓ˜ γ I ℓ˜ )2 ,
t i,t t i,t j,t j,t i,t t i,t j,t j,t
 − − − − 
i X∈G T Xj=1 Xj=1
 
where
• the inequality is obtained by applying exp(x) 1 x + x2 for any x 1 on x =
− ≤ ≤
ηI (ℓˆ ℓ˜ γ K I ℓ˜ )andmultiplyingbothsidesbyq˜ 0,
i,t t − i,t − j=1 j,t j,t i,t ≥
• thelastequalityisPduetothecomputationofp = Ii,tq˜i,t.
i,t Wt
DividingbothsidesbyQ˜ >0,weobtainthedesiredexpression.
t
Notethatineachroundt,Lemma22stillholdsforI [0,1].Thisimpliesthefollowingcorollary.
i,t
∈
Corollary28. Foranyt 1,
≥
2
Q˜ K K
t+1 1+η2 p I ℓˆ ℓ˜ γ I ℓ˜ .
Q˜ ≤ i,t i,t  t − i,t − j,t j,t 
t i=1 j=1
X X
 
Proof. Wewritethesecondterminthesumontheright-handsideofLemma27asfollows:
Q˜ W K K K
t+1 1+η t p (ℓˆ ℓ˜ γ I ℓ˜ )+ηI (ℓˆ ℓ˜ γ I ℓ˜ )2
Q˜ ≤ Q˜ i,t  t − i,t − j,t j,t i,t t − i,t − j,t j,t 
t t i=1 j=1 j=1
X X X
ηW K  K η2W K K
=1+ t p (ℓˆ ℓ˜ γ I ℓ˜ )+ t p I (ℓˆ ℓ˜ γ I ℓ˜ )2
Q˜ i,t t − i,t − j,t j,t Q˜ i,t i,t t − i,t − j,t j,t
t i=1 j=1 t i=1 j=1
X X X X
(a) (b)
| {z } | {z }
Webound(a)and(b)separately.ByLemma22wehave
K K K K
p (ℓˆ ℓ˜ γ I ℓ˜ )=ℓˆ p ℓ˜ γ I ℓ˜
i,t t i,t j,t j,t t i,t i,t j,t j,t
− − − −
i=1 j=1 i=1 j=1
X X X X
=0,
24andthusthequantity(a)isequalto0. Tobound(b),wehave
K
W = I q˜
t i,t i,t
i=1
X
K
q˜
i,t
≤
i=1
X
=Q˜ ,
t
wheretheinequalityisduetoI [0,1]andq˜ >0. Thisimpliesthat0< Wt 1. Multiplying
i,t ∈ i,t Q˜ t ≤
bothsidesbyη2 K p I (ℓˆ ℓ˜ γ K ℓ˜ )2 0,weobtain
i=1 i,t i,t t − i,t − j=1 j,t ≥
P P 2
K K
(b) η2 p I ℓˆ ℓ˜ γ I ℓ˜ ,
i,t i,t t i,t j,t j,t
≤  − − 
i=1 j=1
X X
 
whichimpliesthedesiredstatement.
C.1 ProofofTheorem13
Theorem13. Withoptimallytunedηandγ,Algorithm1guarantees
T K
R(a) O I ln(G /δ)
≤
v i,t T 
ut=1 i=1
uXX
t 
 
simultaneouslyforalla [K]withprobability1 δ.
∈ −
Proof. WestillemploythestandardstrategyoflowerandupperboundingQ . Wehave
T+1
T K
lnQ˜ lnq˜ =η I ℓˆ ℓ˜ γ I ℓ˜ . (33)
T+1 a,T+1 a,t t a,t j,t j,t
≥  − − 
t=1 j=1
X X
 
Ontheotherhand,
T Q˜
lnQ˜ =lnQ˜ + ln t+1
T+1 1 Q˜
t=1 t
X
T K K
lnG + ln 1+η2 p I (ℓˆ ℓ˜ γ I ℓ˜ )2
T i,t i,t t i,t j,t j,t (34)
≤  − − 
t=1 i=1 j=1
X X X
 2 
T K K
lnG +η2 p I ℓˆ ℓ˜ γ I ℓ˜
T i,t i,t t i,t j,t j,t
≤  − − 
t=1i=1 j=1
XX X
 
wherethefirstinequalityisduetoCorollary28andthesecondinequalityisduetoln(1+x) x
≤
forallx 1. Letc =γ K I ℓ˜ . Foranyt 1,wehave:
≥− t j=1 j,t j,t ≥
ℓˆ cP=ℓˆ γI ℓ˜ (sinceℓ˜ =0ifj =i )
t
−
t t
−
it,t it,t j,t
6
t
γI ℓˆ
=ℓˆ it,t t
t
− p +γI
it,t it,t
p ℓˆ
=
it,t t
p +γI
it,t it,t
=p ℓ˜ .
it,t it,t
25Itfollowsthat
K K K K
p I (ℓˆ ℓ˜ c )2 = p I (ℓˆ c )2+ p I ℓ˜2 2 p I (ℓˆ c )ℓ˜
i,t i,t t − i,t − t i,t i,t t − t i,t i,t i,t− i,t i,t t − t i,t
i=1 i=1 i=1 i=1
X X X X
K K
(ℓˆ c )2 p I + p I ℓ˜2
≤ t − t i,t i,t i,t i,t i,t
i=1 i=1
X X
K K
p I + I ℓ˜ ,
i,t i,t i,t i,t
≤
i=1 i=1
X X (35)
where
• thefirstinequalityisduetoℓˆ c =p ℓ˜ 0,
t
−
t it,t it,t
≥
• thesecondinequalityisduetoℓˆ c =p ℓ˜ 1.
t
−
t it,t it,t
≤
Combining(33),(34)and(35),weobtain
T K T K K
lnG
I (ℓˆ ℓ˜ γ I ℓ˜ ) T +η p I + I ℓ˜ , (36)
a,t t a,t j,t j,t i,t i,t i,t i,t
− − ≤ η  
t=1 j=1 t=1 i=1 i=1
X X X X X
whichimpliesthat  
T T T K T K
lnG
I ℓˆ I ℓ˜ + T +η p I + I γ+η I ℓ˜ (37)
a,t t a,t a,t i,t i,t a,t i,t i,t
≤ η
t=1 t=1 t=1 i=1 t=1 i=1
X X XX XX(cid:0) (cid:1)
T T K T K
lnG
I ℓ˜ + T +γ p I +2γ I ℓ˜ , (38)
a,t a,t i,t i,t i,t i,t
≤ γ
t=1 t=1i=1 t=1i=1
X XX XX
wherethelastinequalityisduetoI 1andpickingη =γ.
a,t
≤
Note that Lemma 24 holds for I [0,1]. Applying Lemma 24 with α = 2γI ,δ = δ/2,
i,t i,t i,t ′
∈
applying Corollary 25 with δ = δ/2 and using K p I K I , we obtain that with
′ i=1 i,t i,t ≤ i=1 i,t
probabilityatleast1 δ,simultaneouslyforalla [K],
− ∈P P
T T T K T K
ln(2G /δ) lnG
I ℓˆ I ℓ + T + T +γ I +ln(2/δ)+2γ I ℓ .
a,t t a,t a,t i,t j,t j,t
≤ 2γ γ
t=1 t=1 t=1 i=1 t=1j=1
X X XX XX
Subtracting T I ℓ onbothsidesandusingℓ 1andγ,δ (0,1),weobtain
t=1 a,t a,t j,t ≤ ∈
T K
P ln(2G /δ) lnG
R(a) T + T +ln(2/δ)+3γ I (39)
j,t
≤ γ γ
t=1j=1
XX
T K
3ln(G /δ)
T +3γ I . (40)
i,t
≤ γ
t=1i=1
XX
Settingγ =η = ln(GT/δ) leadstothedesiredbound.
PT t=1PK i=1Ii,t
q
D PROOFSFORSECTION 4
Recall that the sleeping experts are considered sleeping augmented arms. Note that B = m
t
{ ∈
[M] : I = 1 isthesetofawakeexpertsasdefinedinthemaintext. Foranexpertu [M],the
m,t
} ∈
actuallossofexpertuinroundtis
x = E ,ℓ .
u,t u,t t
h i
First,weproveatechnicallemmashowingthatthez -weightedaverageoftheseestimatedlossesof
t
theaugmentedarmsisequivalenttoℓˆ γ K ℓ˜ . ThislemmaisthecounterpartofLemma22.
t − j=1 j,t
P
26Lemma29. Foranyt [K]andm B ,
t
∈ ∈
K
E [x˜ ]=ℓˆ γ ℓ˜ .
m ∼zt m,t t
−
j,t
j=1
X
Proof. LetE(k) bethevalueoftheelementatindexkinE . Wehave
m,t m,t
E [x˜ ]= z x˜
m ∼zt m,t m,t m,t
m X∈B t
K
= z E(k)ℓ˜
m,t m,t k,t
m X∈B t Xk=1
= z E(it)ℓ˜
m,t m,t it,t
m X∈B t
ℓˆ
= t z E(it)
p +γ m,t m,t
it,t m X∈B t
ℓˆp
=
t it,t
p +γ
it,t
ℓˆ
=ℓˆ γ t
t
− p +γ
it,t
=ℓˆ γℓ˜ ,
t
−
it,t
where
• thesecondequalityisbyEquation(13)
• thethirdequalityisduetoℓ˜ =0wheneverk =i
k,t t
6
• thefourthequalityisduetop = z E(k) forallk [K].
k,t m ∈B t m,t m,t ∈
P
LetQ˜ = M q˜ . ForasetS [M],letQ˜ = q˜ betheprojectionofQ˜ onS. Let
t m=1 m,t ∈ S,t m S m,t t
S¯ = [M] S foranyS [M]. Lemma29leadstothefo∈llowingtechnicallemmathatresembles
\P ⊆ P
Lemma4.
Lemma30. Foranyt 0,
≥
Q˜ M K
t+1 z exp η(ℓˆ γ ℓ˜ x˜ ) .
Q˜ ≤ m,t  t − j,t − m,t 
t m=1 j=1
X X
 
Proof. Theproofmakesuseofthefollowingtwofactsthatcanbeprovedeasily:
• Fact1: Thefunctionf(x)=e ηx isconvexforanyη R.
−
∈
• Fact2: Foranya,b>0,c 0,ifa bthen
≥ ≥
a a+c
.
b ≥ b+c
27ByJensen’sinequalityandFact1,wehave
M
z exp( ηx˜ )=E [exp( ηx˜ )]
m,t
−
m,t m ∼zt
−
m,t
m=1
X
exp ηE [x˜ ]
≥ −
m ∼zt m,t
(cid:0) K (cid:1)
=exp η(ℓˆ γ ℓ˜ ) ,
t j,t
− − 
j=1
X
 
wherethe lastequalityis dueto Lemma29. Since z = 0form / B , theexpressionaboveis
m,t t
∈
equivalentto
K
z exp( ηx˜ ) exp η(ℓˆ γ ℓ˜ ) .
m,t m,t t j,t
− ≥ − − 
m X∈B t Xj=1
 
Multiplyingexp(η(ℓˆ t −γ K j=1ℓ˜ j,t))Q˜ B t,t >0onbothsides,weobtain
P K
z m,tQ˜ B t,texp η(ℓˆ t −γ ℓ˜ j,t −x˜ m,t) ≥Q˜ B t,t. (41)
m X∈B t Xj=1
 
Bydefinition,z = q˜m,t . Hence,Equation(41)isequivalentto
m,t Q˜
Bt,t
K
q˜ m,texp η(ℓˆ t −γ ℓ˜ j,t −x˜ m,t) ≥Q˜ B t,t.
m X∈B t Xj=1
 
By our update rule, q˜ = q˜ for m / B and q˜ =
m,t+1 m,t t m,t+1
∈
q˜ exp η(ℓˆ γ K ℓ˜ x˜ ) form B . Hence,
m,t t − j=1 j,t − m,t ∈ t
(cid:16) (cid:17)
P
K
q˜ m,t+1 = q˜ m,texp η(ℓˆ t −γ ℓ˜ j,t −x˜ m,t) ≥Q˜ B t,t.
m X∈B t m X∈B t Xj=1
 
ApplyingFact2fora= m ∈B tq˜ m,t+1,b=Q˜ B t,tandc=Q˜ B¯ t,t,weobtain
M PK K
z exp η(ℓˆ γ ℓ˜ x˜ ) = z exp η(ℓˆ γ ℓ˜ x˜ )
m,t t j,t m,t m,t t j,t m,t
 − −   − − 
i X=m Xj=1 m X∈B t Xj=1
   
q˜ exp η(ℓˆ γ K ℓ˜ x˜ )
= m ∈B t m,t t − j=1 j,t − m,t
P
(cid:16) Q˜
B t,t P
(cid:17)
q˜
= m ∈B t m,t+1
P
Q˜
B t,t
m ∈B tq˜ m,t+1+Q˜ B¯ t,t
≥
P
Q˜
B
t,t+Q˜
B¯ t,t
Q˜
t+1
= ,
Q˜
t
(42)
wherethelastequalityisduetothefactthat m ∈B¯ tq˜ m,t+1 = m ∈B¯ tq˜ m,t =Q˜ B¯ t,t.
P P
D.1 BoundingtheEstimatedRegret
Thefollowinglemmaboundstheestimatedregretofeachexpertu [M].
∈
28Lemma31. Foranyγ 0,foranyu [M],SE-EXP4guaranteesthat
≥ ∈
T T K
lnM η
I (ℓˆ x˜ ) + γ+ ℓ˜ . (43)
u,t t u,t j,t
− ≤ η 2
t=1 (cid:18) (cid:19)t=1j=1
X XX
Proof. Wehave
K
lnQ˜ =ln q˜
T+1 m,T+1
m=1
X
lnq˜ u,T+1 (44)
≥
T K
=η I (ℓˆ γ ℓ˜ x˜ ).
u,t t j,t u,t
− −
t=1 j=1
X X
Ontheotherhand,wehave
T Q˜
lnQ˜ =lnQ˜ + ln t+1
T+1 1 Q˜
t=1 t
X
T K
lnM + ln z exp η(ℓˆ γ ℓ˜ x˜ )
≤  m,t  t − j,t − m,t 
Xt=1 m X∈B t Xj=1

  
T K
=lnM + ln exp(η(ℓˆ γ ℓ˜ )) z exp( ηx˜ )
t j,t m,t m,t
 − − 
Xt=1 Xj=1 m X∈B t
 
T K
=lnM + η(ℓˆ γ ℓ˜ )+ln z exp( ηx˜ )
 t − j,t  m,t − m,t 
Xt=1

Xj=1 m X∈B t


 
T K η2x˜2
lnM + η(ℓˆ γ ℓ˜ )+ln z (1+ m,t ηx˜ )
≤  t − j,t  m,t 2 − m,t 
Xt=1

Xj=1 m X∈B t


 
T K z x˜2
=lnM + η(ℓˆ γ ℓ˜ )+ln 1+η2 m,t m,t η z x˜
 t − j,t  2 − m,t m,t 
Xt=1

Xj=1 m X∈B t m X∈B t

  
T K z x˜2
lnM + η(ℓˆ γ ℓ˜ )+η2 m,t m,t η z x˜
t j,t m,t m,t
≤  − 2 − 
Xt=1 Xj=1 m X∈B t m X∈B t
T
z x˜2

=lnM +η2 m,t m,t ,
2
Xt=1m X∈B t
where
• thefirstinequalityisduetoLemma30,
• thesecondinequalityisexp( x) 1+
x2
xforallx 0,
− ≤ 2 − ≥
• thethirdinequalityisln(1+x) xforallx 1,
≤ ≥−
• thelastequalityisduetoLemma29.
Weobtain
T K T
lnM η
I (ℓˆ γ ℓ˜ x˜ ) + z x˜2 . (45)
u,t t − j,t − u,t ≤ η 2 m,t m,t
Xt=1 Xj=1 Xt=1m X∈B t
29Next,weproceedinthesamewayasinNeu(2015). Wehave
2
K
z x˜2 = z E(k)ℓ˜ (46)
m,t m,t m,t  m,t k,t 
m X∈B t m X∈B t k X=1
K 
z E(k)(ℓ˜ )2 (47)
≤ m,t m,t k,t
m X∈B t Xk=1
K
= (ℓ˜ )2 z E(k) (48)
k,t m,t m,t
Xk=1 m X∈B t
K
= p (ℓ˜ )2 (49)
k,t k,t
k=1
X
K
ℓ˜ , (50)
k,t
≤
k=1
X
wherethefirstinequalityisJensen’sinequality.Thisimplies
T K T K
lnM η
I (ℓˆ γ ℓ˜ x˜ ) + ℓ˜ . (51)
u,t t j,t u,t k,t
− − ≤ η 2
t=1 j=1 t=1k=1
X X XX
Movingγ T I K ℓ˜ totheright-handsideandusingI [0,1],weobtainthedesired
t=1 u,t j=1 j,t u,t ∈
statement.
P P
D.2 ProofofTheorem16
Theorem16. Foranyγ,η (0,1),η 2γ,SE-EXP4guarantees
∈ ≤
lnM ln(2M/δ) η
R(u) + +(γ+ )TK+ln(2/δ) (14)
≤ η 2γ 2
simultaneouslyforallexpertsu [M]withprobabilityatleast1 δ,wheretheprobabilityistaken
∈ −
over the sequenceofthe learner’sselected arms. Tuning η andγ leadsto an O( TKln(M/δ))
bound.
p
Proof. Lemma31impliesthat
T T T K
lnM η
I ℓˆ + I x˜ + γ+ ℓ˜
u,t t u,t u,t j,t
≤ η 2
t=1 t=1 t=1j=1(cid:18) (cid:19)
X X XX (52)
T K T K
lnM η
= + I E(j)ℓ˜ + γ+ ℓ˜ .
η u,t u,t j,t 2 j,t
t=1 j=1 t=1j=1(cid:18) (cid:19)
X X XX
Next,weapplyLemma24twice,where
• thefirsttimewithα =2γI E(i),δ = δ andaunionboundover[M]implies
i,t u,t u,t ′ 2M
T K
ln(2M/δ)
I E(j)(ℓ˜ ℓ ) (53)
u,t u,t j,t − j,t ≤ 2γ
t=1j=1
XX
withprobabilityatleast1 δ/2;
−
• thesecondtimewithα =γ+η/2,δ =δ/2implies
i,t ′
T K
η
γ+ (ℓ˜ ℓ ) ln(2/δ) (54)
j,t j,t
2 − ≤
(cid:18) (cid:19)t=1j=1
XX
withprobabilityatleast1 δ/2.
−
30Plugging(53)and (54)into(52)yields
T T K T K
lnM ln(2M/δ) η
I ℓˆ + I E(j)ℓ + + γ+ ℓ +ln(2/δ)
u,t t ≤ η u,t u,t j,t 2γ 2 j,t
t=1 t=1 j=1 (cid:18) (cid:19)t=1j=1
X X X XX
T
lnM ln(2M/δ) η
+ I x + + γ+ TK+ln(2/δ).
u,t u,t
≤ η 2γ 2
t=1 (cid:18) (cid:19)
X
Moving T I x totheleft-handside,weobtain
t=1 u,t u,t
P lnM ln(2M/δ) η
R(u) + + γ+ TK+ln(2/δ). (55)
≤ η 2γ 2
(cid:18) (cid:19)
Lettingη =2γ andtuningηimpliestheO( TKln(M/δ))bound.
p
D.3 APseudo-RegretBoundofSE-EXP4
Weboundthepseudo-regretE[R(u)]foranyexpertu [M]. Notethatγ =0. Takingtheexpecta-
∈
tiononbothsideofLemma31andusing
E ℓ˜ =ℓ 1,
it∼pt j,t j,t
≤
h i
weobtain
T
lnM η
E[ I (ℓˆ x˜ )] + TK. (56)
u,t t u,t
− ≤ η 2
t=1
X
Ontheotherhand,
K
E [x˜ ]= p E[x˜ i =k]
it∼pt u,t k,t u,t
|
t
k=1
X
K K
= p E[ E(j)ℓ˜ i =k]
k,t u,t j,t | t
k=1 j=1
X X
K
ℓ
= p E(k) k,t
k,t u,tp
k,t
k=1
X
K
= E(k)ℓ
u,t k,t
k=1
X
= E ,ℓ .
u,t t
h i
Weconcludethat
lnM ηTK
E[R(u)] + . (57)
≤ η 2
Bysettingη = 2lnM weobtainthebound
TK
q
E[R(u)] √2TKlnM. (58)
≤
D.4 ProofofTheorem17
Theorem17. Foranyγ,η (0,1),η 2γ,SE-EXP4withvirtualexpertsguaranteesthat
∈ ≤
2ln(KT) ln(KT/δ) η
R (k) + +(γ+ )TK+ln(2/δ)
[t1,t2] ≤ η γ 2
simultaneouslyforallintervals[t ,t ] andarms k [K] with probability1 δ. Tuningη andγ
1 2
∈ −
leadstoanO( TKln(KT/δ))bound.
p
31Proof. Foranytriple(k,t ,t )wecreateavirtualexpertthatisactivefromroundt toroundt and
1 2 1 2
giveadvicee . ThereareM = K T = KT(T+1) suchexperts. BecauseM KT2 (KT)2,
k 2 2 ≤ ≤
wehave
(cid:0) (cid:1)
lnM ln((KT)2)=2ln(KT).
≤
Furthermore,forallδ (0,1),
∈
ln(2M/δ) ln(4M/δ2) ln((2KT/δ)2)=2ln(2KT/δ).
≤ ≤
Letu denotetheexpertindexedby(k,t ,t ). ByTheorem16,withprobabilityatleast1 δ,
k,t1,t2 1 2
−
R (k)=R(u )
[t1,t2] k,t1,t2
lnM ln(2M/δ) η
+ +(γ+ )TK+ln(2/δ)
≤ η 2γ 2 (59)
2ln(KT) ln(2KT/δ) η
+ +(γ+ )TK+ln(2/δ)
≤ η γ 2
holdssimultaneouslyforallu .
k,t1,t2
D.5 ProofofCorollary18
Corollary18. RestartingSE-EXP4withvirtualexpertsaftereveryT/Sroundsguaranteesthatfor
anyj whereH(j ) S,withprobabilityatleast1 δ
1:T 1:T
≤ −
KT
R(j ) O STKln .
1:T
≤ s δS 
(cid:18) (cid:19)
 
Proof. Withoutlossofgenerality,assumethatT isdivisiblebyS. Weusethefollowingalgorithm
called SE-EXP4-Restart, which runs in S episodes. In each episode, a new instance of SE-EXP4
with virtualexpertsis runfor T/S rounds. Letb = 1,2,...,S be an index for the episodes, and
t (b) = b ST betheendingroundofepisodeb. Notethateachepisodebstartsfromround (b − S1)T +1
toround bT.
S
We examine the regret of this SE-EXP4-Restart with respect to the competing arms j in each
1:T
episodeb. DivideT roundsintoZ =H(j )+1non-overlappingsegments,wherethecompeting
1:T
arms are the same within each segment z = 1,...,Z. Let S ,E be the first and last rounds of
z z
segmentz. Foreverypair(b,z)ofepisodebandsegmentz,let
(b 1)T bT
F =[S ,E ] [ − , ] (60)
b,z z z
∩ S S
be the intersection between the rounds of episode b and segment z. Since the episodes are non-
overlapping and the segments are non-overlapping, the intervals F ’s are non-overlapping. In
b,z
addition,theirunionis F =[T].
b,z b,z
∪
Fixanepisodebandasegmentz. Therearetwocases:
• F = :Obviously,theregretofSE-EXP4-Restartonthisemptyintervaliszero.
b,z
∅
• F = : inthiscase, becauseF isanintervalwithinepisodeb, thetrackingregretof
b,z b,z
6 ∅
SE-EXP4-RestartonF cannotexceedtheadaptiveregretofrunning(anewinstanceof)
b,z
SE-EXP4withvirtualexpertsunderhorizonT/S.ByTheorem17,thisisboundedby
2 KT
R ln +ηTK/S+ln(2S/δ) (61)
Fb,s
≤ η Sδ
(cid:18) (cid:19)
withprobabilityatleast1 δ/S.
−
TakingaunionboundoverallS episodesandsettingη = 2γ impliesthatwithprobabilityatleast
1 δ,
−
2 KT
R ln +ηTK/S+ln(2S/δ)
Fb,s
≤ η Sδ
(cid:18) (cid:19)
32simultaneouslyforallintervalsF . BecauseF ’sarenon-overlappingandtheirunionis[T],the
b,s b,z
trackingregretofSE-EXP4-Restartisboundedby
R(j )= R (62)
1:T Fb,z
b,z
X
2 KT
= 1 F = ln +ηTK/S+ln(2S/δ) . (63)
b,z
 b,z { 6 ∅} η (cid:18) Sδ (cid:19) !
X
 
Next, we show that the count C = 1 F = is smaller than 2S. Observe that the S
b,z { b,z 6 ∅}
episodessplitthesequenceofT roundsintoSintervalswithS 1splittingpoints(notcountingthe
P −
twoendsat0andT).Similarly,theS+1segmentsofj haveSsplittingpoints.Intotal,thereare
1:T
atmost2S 1splittingpointsfromtheepisodesandthesegmentsofj . Eachnon-emptyinterval
1:T
−
F hasanendingpointthatiseitherT oroneofthe2S 1splittingpoints. Therefore,therecan
b,z
−
beatmost2S suchF . WeconcludethatC 2S. Asaresult,withprobabilityatleast1 δ,
b,z
≤ −
4S KT
R(j ) ln +2ηTK+2Sln(2S/δ).
1:T
≤ η Sδ
(cid:18) (cid:19)
2Sln(KT)
Lettingη = Sδ leadstothedesiredbound.
TK
q
E PROOFSFORSECTION 5
Theorem20. Forany(possiblyrandomized)algorithmwithguarantee
sup E[R(a)] O(TγAβ(ln(T))µ), (15)
≤
a [K]
∈
whereγ (0,1),β 0,µ 0areconstants,thereexistsanumberofarmsK andsequenceofsets
∈ ≥ ≥
ofactivearmsandtheirlossessuchthatA=2andforatleastonearma [K],
∈
T Ω(T1 γ(ln(T)) µ)andE[R(a)] Ω(T ).
a − − a
≥ ≥
Proof. Let be any algorithm with the stated worst-case guarantee and f(T,A) =
A
O(TγAβ(ln(T))µ) represent the worst-case regret bound of . We show a construction with
A
A = 2 for all t = 1,...,T. Our construction is adapted from the lower bound construction
t
ofDanielyetal.(2015)forstronglyadaptiveregretinthestandardadversarialMABsetting. With-
out loss of generality, assume 4f(T,A) divides T. Let L = T and K = 1 + 4f(T,A).
4f(T,A)
ObviouslyL Ω(T1 γA β(ln(T)) µ). ConsideranenvironmentV definedasfollows:
− − − 0
≥
• Arm1isalwaysactive.Itslossesareℓ =0.5forallt [T].
1,t
∈
• Arm k = 2,3,...,K are active only for the rounds in the interval =
k
I
(k −2)T +1, (k −1)T , respectively. The length of each interval is L. Their losses are
4f(T,A) 4f(T,A)
ℓhk,t =1fortheroundsitinwhichtheyareactive.
Figure 1 illustrates this environment V . We also define K 1 competing environments V for
0 k
−
k =2,3,...,K,definedasfollows:
• The numberof arms and their active roundsare identicalto that of V . That is, arm 1 is
0
alwaysactiveforallroundswhileeachofthearmsk = 2,3,...,K areactiveforrounds
within ,respectively.
k
I
• ThelossesofeveryarmarethesameasinV ,exceptforthatofarmk: alllossesofarmk
0
are0. Figure2illustratesenvironmentV .
k,j
Following the standard strategy of comparingthe behavior of the algorithm on V and competing
0
environments (Aueretal., 2002; Danielyetal., 2015), we first consider the neutral environment
33arm1,ℓ =0.5
1,t
1 T
arm2 arm3 armK
1 ℓ =1 ℓ =1 ℓ =1 T
2,t 3,t K,t
Figure1:EnvironmentV . Allarmshavelossequal1whentheyareactive
0
arm1,ℓ =0.5
1,t
1 T
armk 1 armk armk
−
ℓ =1 ℓ =0 ℓ =1
k 1,t k,t k+1,t
−
Figure2: EnvironmentV . Exceptforarmkwhichhaslossesequalto0,allarmshavelossesequal
k
to1whentheyareactive.
V . Let E and Pr indicate the expectation and probability taken in this environment over the
0 0 0
randomnessofthealgorithm ,respectively. LetU = t : i = 1 betheroundsinwhichthearm
t
A { 6 }
chosenby isnotarm1. OnV ,sincearm1isthebestarmandthegapsbetweenthelossesofarm
0
1andthatoA feveryotherarmis0.5,theinequalitysup E [R(a)] f(T,A)implies
a 0 ≤
E U 2f(T,A). (64)
0
| | ≤
Foranyk 2,3,...,K ,let (cid:2) (cid:3)
∈{ }
= U =
k k
E { ∩I ∅}
be the event that only arm 1 is chosen by on . Because the are non-overlapping and
k k
A I I
=[T],wecanwrite
k=2,...,K k
∪ I
U = (U ),
k=2,...,K k
∪ ∩I
and
K
U = U .
k
| | | ∩I |
k=2
X
Next, we show that for some segment Ik∗ of size L = 4f(T T,A), we have E 0 |U ∩Ik∗
| ≤
1 2.
AssumeonthecontrarythatE U > 1 forallk =2,...,K. Then,
0 | ∩Ik | 2 (cid:2) (cid:3)
(cid:2) (cid:3) K
E [U]= E [U ]
0 0 k
∩I
k=2
X
K 1
> −
2
=2f(T,A),
whichcontradicts(64). Since |U ∩Ik∗ |isanon-negativeinteger,theinequalityE 0 |U ∩Ik∗ | ≤ 1 2
i rm oup nli des
t
∗P =r 0[
(
4E
k
f∗k (−∗ T]
2
,A)≥
T
),1 2 t. hN ese ex tt, ow fe acc to ivn esi ad re mr sth ae nden tv hi er io rn lm ose sn et sV ok n∗ V. 0Fr ao nm dVro ku ∗n ad re1 iu dp ento t(cid:2) ic(a an l.d Ain sc alu (cid:3) rd esin ug lt)
,
thedistributionoverthepastchosenarmsandobservedlosses inducedby uptoroundt is the
∗
A
same onbothenvironment. Moreover,oncethe algorithmenters k∗ atroundt ∗ +1andchooses
I
34onlyarm1subsequently,italsoobservesthesamesequenceofchosenarmsandlossesonbothV
0
andV k∗ untiltheendof k∗. Hence,
I
1
Pr k∗[ k∗]=Pr 0[ k∗] , (65)
E E ≥ 2
wherethesubscriptk ∗indicatesaprobabilitymeasuredinenvironmentV k∗. Inotherwords,onV k∗,
withprobabilityatleast0.5,arm1isalwayschosenonI k∗ andarmk ∗ isneverchosen. Underthis
event k∗, theregretof withrespecttoarmk ∗ is(0.5 0)L = 0.5L. When k∗ doesnothold,
E A − E
theregretwithrespecttok ∗ isnon-negativebecauseℓ k∗,t = 0. Overall,onV k∗ theexpectedregret
isatleast
L
E k∗ R(k ∗) .
≥ 4
(cid:2) (cid:3)
F PROOFOFTHEOREM 6: DOUBLING-TRICKFORADAPTINGTO
T A ANDG
t=1 t T
P
ForSB-EXP3,computingtheoptimallearningratesrequirethefraction lnGT of√lnG and
PT t=1At T
thesum T A . Bothofthesequantitiesaremonotonicallynon-decrq easing.Therefore,wecan
t=1 t
applytheqdoublingtrickon these two quantities. First, we provea simple lemmajustifyingdoing
P
this.
Lemma32. Leta,b,c,d>0beconstantssuchthata candb d. Let
≤ ≤
a bx
f(x)= +
x 2
beafunctiononR . Then,
+
2c
f √2cd.
r
d !≤
Proof. Duetoa candb d,foranyx 0wehave
≤ ≤ ≥
c dx
f(x) + .
≤ x 2
Pluggingx= 2c intotheright-handsidegives
d
q
2c d c
f c +d
r
d !≤ r2c r2d
=√2cd.
Lemma32impliesthatforanyhorizonT,ifwesetη = 2c forcandcsuchthatc lnG and
t d ≥ T
d T A thenweobtainaregretofbound√2cd. q
≥ t=1 t
We pProceed to performthe doublingtrick on lnG and T A . The full procedureis givenin
T t=1 t
Algorithm 5. The main idea is a two-leveldoublingtrick which divides the learning process into
episodesasfollows: P
• The first level: throughout the learning process, we maintain a set V for the arms that
havebeenactiveatleastonceineachepisodeandanupperbound2C forln(V). Initially,
V = and C = 1. At the beginning of round t, we check if ln(V A )| e| xceeds 2C.
t
If ln(V∅ A ) 2C then we continue the learning process and| upd∪ ate |V = V A .
t t
Other| wis∪ e,w| ere≤ setVto ,incrementC byatleastoneuntil2C ln(A )andstarta∪ new
t
∅ ≥
episodefromroundt.
35Algorithm5:SB-EXP3-ATGTadaptedtoG and T A
T t=1 t
InitializeU =0,C =1,b=1,V= ;
∅ P
InitializeL =0fori=1,2,...,K;
i
foreachroundt=1,...,do
AnadversaryselectsandrevealsA ;
t
ifln(V A )>2C then
t
| ∪ |
C =C+1;
whileln(A )>2C do
t
C =C+1;
SetV= ;
∅
SetU =0,b=1;
forarmi G do
t
∈
L =0;
i
ifU +A >2bthen
t
b=b+1;
whileA >2bdo
t
b=b+1;
SetU=0;
forarmi G do
t
∈
L =0;
i
UpdateV=V A ;
t
∪
UpdateU =U +A ;
t
Computeη = 2C+1;
2b
forarmi A tqdo
∈
q˜ =exp(ηL )
i,t i
Computep by(6);
t
Samplei p ;
t t
∼
Computeℓ˜ by(4);
i,t
forarmi A do
t
∈
L =L +ℓˆ ℓ˜ γ ℓ˜ ;
i i t − i,t − j ∈A t j,t
P
• Thesecondlevel:throughouttheroundsofeachepisode,wemaintainacumulativesumU
forthesumofA andanupperbound2b forU. NotethatC isfixedwithintheserounds.
t
Beforethefirstroundofanepisode,weinitializeU = 0,b = 1. AslongasU +A 2b,
t
≤
werunSB-EXP3withη = 2C+1 andupdateU = U +A . OnceU exceeds2b atsome
2b t
roundt,weincrementbbyaqtleastoneuntilA 2b,resetU =0andrunanewinstance
t
≤
ofSB-EXP3onwards.
Thepseudo-regretofAlgorithm5isshowninthefollowingtheorem.
Theorem6. ForanyT 2,Algorithm5(inAppendixF)guaranteesthat
≥
T
4
max E[R(a)] ln(G ) A .
a ∈[K] ≤ (√2 −1)2v u
u
T Xt=1 t
t
Proof. LetC bethelastvalueofC afterT rounds. NotethatC isalsothenumberofepisodes.
T T
Letc = 1,2,...,C betheindexoftheepisodes. Wefirstboundtheregretwithineachepisodec,
T
andthensumupthisboundoverC episodestogetthetotalregretbound.
T
BoundingTheRegretWithinEachEpisode
Fixanepisodec. LetT betheroundsinthisepisode,andT = T . LetV bethesetofarmsthat
c c c c
areactiveatleastonceduringthisepisode,andV = V . Byco| nst| ruction,b = 1atthebeginning
c c
| |
ofthisepisode. LetB bethelastvalueofbafterT roundsstartingfromthefirstroundofepisode
c
36c. Divide the rounds in T into B time intervals, where b does not change in each interval. For
c
b = 1,2,...,B, letF bethetimeintervalofb. LetU = A bethesumofA withinF .
Since U 2b and
lnb
(V ) 2c, by Lemma 32 and
Tb heoremt ∈2F ,b thet
regret of the
leat
rner in
thib
s
b ≤ c ≤ P
intervalisboundedby
max E[R (a)] √2b2c+1.
a [K]
Fb
≤
∈
LetR (a)betheregretincurredduringepisodecwithrespecttoarma. Wehave
c
B
max E[R (a)] max E[R (a)]
a [K]
c
≤ a [K]
Fb
∈ b=1 ∈
X
B
√2b2c+1
≤
b=1
X
√2
√2B2c+1.
≤ √2 1
−
Forb=1,...,B,lett bethefirstroundofF . SincebisincreasedfromB 1toBatthebeginning
b b
ofroundt ,wehaveU +A > 2B 1. Itfollowsthat2B 2(U − +A ) 2 A .
Hence,
B B −1 tB − ≤ B −1 tB ≤ t ∈T c t
P
√2
max E[R (a)] 2c+1 A . (66)
c t
a [K] ≤ √2 1
∈ − s t X∈T c
BoundingTheTotalRegret
Summingup(66)forc=1,...,C ,weobtain
T
CT
max E[R(a)] max E[R (a)]
c
a [K] ≤ a [K]
∈ c=1 ∈
X
√2 CT
2c+1 A
t
≤ √2 1
− Xc=1s t X∈T c
√2 T CT
A √2c+1
≤ √2 1v t
− u uXt=1 Xc=1
t 
 
2√2 T
≤ (√2 1)2 v
A t√2CT,
− u uXt=1
t 
 
where the third inequality is due to A T A and the last inequality is due to
C c=T 1√2c = √√ 22 1(√2CT −1).
Pt ∈T c t ≤ Pt=1 t
−
APssume C was increased at least once i.e. C
T
> 1, otherwise we immediately have an
O T A total regret bound. Let τ be the first round of the last episode. Since C was
t=1 t
hin ac n(cid:18) r deq ,as sPe id ncf ero (m V(cid:19) C
T −
1 Ato )C
T
a Gt ro ,u wnd eτ h, avw ee lh na (v Ve 2CT−1 A< )ln(V
C lnT (− G1 ∪
).A
τ
T) h. isO imn pth lie esot th he ar
t
2CT ≤2ln(G TC ),T h− e1 nc∪ etheτ tot⊆ alreT gretisbounded (cid:12)byCT−1 ∪ τ
(cid:12)
≤(cid:12) (cid:12) T (cid:12) (cid:12)
(cid:12) (cid:12)
T
4
max E[R(a)] ln(G ) A .
a ∈[K] ≤ (√2 −1)2v u
u
T Xt=1 t
t
37Remark 33. While the two-level doublingtrick works, for practical purposeswe can set a small
constant(e.g. 16)tobeanupperboundforlnG andperformaone-leveldoublingtrick onlyon
T
T A . This is because lnG increases exponentially slowly: if the upper bound for lnG is
t=1 t T T
doubled in each increment starting from 20 = 1, then to have k such increments G must be as
T
lP argeasG e2k. Fork = 5, thisisapproximately8 1013, whichisexceedinglylargeforthe
T
≥ ×
numberofarms. Overall,thisimpliesthatthedoublinglevelonlnG wouldchangeatmost4times
T
inanypracticalscenario,andsettingln(G ) 16wouldcontributeatmostamultiplicativefactor
T
≤
of√16=4onthetotalregretbound.
G FTARL WITHNEGATIVEENTROPY ISEQUIVALENTTO SB-EXP3
InAlgorithm2, thelossestimateofnon-activearmsa / A isℓ˜ = ℓˆ γ ℓ˜ . Because
I =1fora A andI =0fora / A
,itfollowst∈ hatit nAlga o,t rithmt 2− ,foralj l∈aA t [j K,t
],
a,t t a,t t
∈ ∈ P ∈
T T
ℓˆ γ ℓ˜ ℓ˜ = I ℓˆ γ ℓ˜ ℓ˜ (67)
t j,t a,t a,t t j,t a,t
 − −   − − 
Xt=1 j X∈A t Xt=1 j X∈A t
   
Next,we showthatineachround,thesamplingprobabilityp ofFTARL withnegativeentropyis
t
the same as that of SB-EXP3. With ψ (x) = 1 K x lnx the negative Shannon entropy, in
t η i=1 i i
Algorithm2theweightvectorq ofFTARListhesolutionoftheoptimizationproblem
t
P
K K
1
q = min x lnx + x L˜ .
t i i i i,t 1
x ∈∆K η
i=1 i=1
−
X X
Solvingforq ,weobtainforanyi [K],
t
∈
exp( ηL˜ )
i,t 1
q i,t = K ex− p( ηL˜− ).
k=1 − k,t −1
Itfollowsthatforanactivearmi A t, P
∈
q
i,t
p =
i,t
q
k ∈A t k,t
P exp( ηL˜ i,t 1)
= − −
exp( ηL˜ )
k ∈A t − k,t −1
=
P exp(η t s− =1 1ℓˆ s −γ j ∈A sℓ˜ j,s −ℓ˜ i,s) (68)
k ∈A texpP(η t s− =1 1ℓˆ s −Pγ j ∈A sℓ˜ j,s −ℓ˜ k,s)
=
P exp(η t s−P =1 1I i,s(ℓˆ s −γP j ∈A sℓ˜ j,s −ℓ˜ i,s))
,
k ∈A texpP(η t s− =1 1I k,s(ℓˆ s −Pγ j ∈A sℓ˜ j,s −ℓ˜ k,s))
where P P P
• dth ee nose mc io nn ad to-t ro a- nla dst ne uq mu ea rl ait ty ori .s by multiplyingexp(η t s− =1 1ℓˆ s −γ j ∈A sℓ˜ j,s) to both the
P P
• thelastequalityisdueto(67).
Observe that (68) is equal to the sampling probability of arm i A computed in (6) of Algo-
t
rithm3.1. Moreover,p =0fori / A inbothAlgorithms1and2∈ . ThisimpliesthatFTARLwith
i,t t
∈
negativeShannonentropyisequivalenttoSB-EXP3.
38