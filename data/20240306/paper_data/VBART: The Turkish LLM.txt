VBART: The Turkish LLM
Meliksah Turker, Mehmet Erdi Ari, Aydin Han
VNGRS-AI
YTÜTeknoparkB2103Davutpaşa,İstanbul,Turkey
{meliksah.turker,erdi.ari,aydin.han}@vngrs.com
Abstract
WepresentVBART,thefirstTurkishsequence-to-sequenceLargeLanguageModels(LLMs)pre-trainedonalarge
corpusfromscratch. VBARTarecompactLLMsbasedongoodideasleveragedfromBARTandmBARTmodels
andcomeintwosizes,LargeandXLarge. Fine-tunedVBARTmodelssurpassthepriorstate-of-the-artresultsin
abstractive text summarization, title generation, text paraphrasing, question answering and question generation
tasks. Theyallowfine-tuningforfuturetextgenerationtasksanddatasets,carvinganewpathforTurkishNatural
LanguageProcessing(NLP)research. Ourworkshowsthathavingapre-trainedLLMforTurkishoutperformsupto
3xmultilingualmodels,improvingexistingresultsandprovidingefficientmodelsfortrainingandinference. Moreover,
weshowthatourmonolingualtokenizeris7xmoreefficientthanOpenAI’smultilingualtokenizer. Lastbutnotleast,
weintroduceamethodtoenlargeanexistingpre-trainedLLMandquestiontherelevancyofChinchillaScalingLawto
sequence-to-sequencemaskedlanguagemodels. Ourfine-tunedmodels,tokenizerandcleanedwebcorpusof135
GBarepubliclyavailableathuggingface.co/vngrs-ai.
Keywords: Large Language Models, LLM, Turkish, BART, sequence-to-sequence, Text Summarization,
TitleGeneration,TextParaphrasing,QuestionGeneration,QuestionAnswering.
1. Introduction toapplytheprincipleofunsupervisedpre-training
ofMaskedLanguageModelstoseq2seqtasksfor
Natural Language Processing (NLP) research conditionaltextgeneration.
landscape has changed drastically over the last They are followed by multilingual versions
decade. Inventionofwordembeddingmethodslike mBART (Liu et al., 2020) and mT5 (Xue et al.,
Word2Vec(Mikolovetal.,2013)andGloVe(Pen- 2020),pre-trainingofwhichareconductedonmul-
nington et al., 2014) has been the foundation of tiplelanguagestogether. Thisallowedthemtobe
transferlearning. Theyhavebeensucceededby fine-tunedforlow-resourcelanguageswithoutsuf-
FastText(Bojanowskietal.,2017),whichleverages fering the high cost of pre-training a new model
charactern-grams,andELMo(Petersetal.,2018), forthelanguagefromscratch. State-of-the-artre-
which uses the context of the words for the first sults in conditional text generation tasks are ob-
time. tained(BaykaraandGüngör,2022b;Safayaetal.,
Usingpre-trainedwordembeddingsfortextclas- 2022;Akyonetal.,2022)forTurkishbyfine-tuning
sificationtasksofteninvolvedRecurrentNeuralNet- them.
works(RNNs)suchasLongShort-TermMemory Lastly, very large language models like
(LSTM)(HochreiterandSchmidhuber,1997)and BLOOM (Scao et al., 2022), PaLM (Chowdhery
Gated Recurrent Unit (GRU) (Cho et al., 2014). et al., 2022), GPT-4 (OpenAI, 2023) have made
Democratization of these models has been pos- a significant impact on both the industry and the
sible thanks to Deep Learning frameworks like academy. Theyachievedstate-of-the-artresultsin
Keras(Cholletetal.,2015),Tensorflow(Abadietal., Englishandhigh-resourcelanguagesanddecent
2015)andPyTorch(Paszkeetal.,2019). results for low-resource languages. However, as
TokenizationmethodssuchasWordPiece(Wu inthecaseofChatGPT(OpenAI,2022),distilling
etal.,2016),Byte-PairEncoding(BPE)(Sennrich verylargemodelsresultsinpoorperformance(Lai
et al., 2015) and Unigram model (Kudo, 2018) et al., 2023; Jiao et al., 2023) for low-resource
solvedtheout-of-vocabularyproblemwhilekeeping languages. On the other hand, using very large
thevocabularysizefeasible. modelsiscomputationallyexpensive.
Transformer(Vaswanietal.,2017)architecture It is clear that the performance of multilingual
andtheintroductionofBERT(Devlinetal.,2018) LLMs in low-resource languages is a matter of
have made a significant contribution to the field trade-offbetweenthecostofcomputationandeval-
of transfer learning. BERT allowed for achieving uationmetrics. Thisisbecausetheyarenotoptimal
state-of-the-artresultsefficiently. foraspecificlanguagesinceasignificantportion
FollowingthesuccessofBERT,anEncoder-only oftrainingtimeandnetworkcapacityarespenton
model, BART (Lewis et al., 2019) and T5 (Raffel other languages. This is especially true for low-
etal.,2019)modelshaveshownthatitispossible resource languages such as Turkish. Hence, a
4202
raM
2
]LC.sc[
1v80310.3042:viXraFigure1: Pre-traininglossofVBARTmodels. Notethattherearetwosteepdropsinlosstowardstheend
ofthetraining. ThisisduetothereductioninDropoutfrom0.10to0.05andthento0.
dedicatedLLMisneededtoobtainstate-of-the-art pre-trainedBERTEncoderandarandomlyinitial-
results in an efficient and computationally cheap ized Transformer Decoder. Baykara and Güngör
mannerforagivenlanguage. (2022b)andSafayaetal.(2022)havefine-tuned
Thus, we present VBART, the first dedicated mBARTandmT5ontextsummarizationdatasets.
seq2seq LLM pre-trained for Turkish on a large TitleGenerationWhileworkingontheinternet
scale. First, we train VBART-Large from scratch newsdomainfortextsummarization,Baykaraand
onacleanedcorpusof135.7GBsofTurkishtext Güngör(2022a,b)havecreatedadatasetofnews
based on the pre-training task of BART and the titles and fine-tuned mBART and mT5 models to
model architecture of mBART. Then, we create generatetitlefromthegiventhenewssummary.
VBART-XLargebyenlargingthepriormodelbydou- Question Generation & Answering: Akyon
blingthenumberofencoderanddecoderlayers,us- etal.(2022)hasfine-tunedmT5forquestiongen-
ingtheweightsofVBART-Large. Bothmodelssur- erationandansweringtasks.
passthepreviousstate-of-the-artinabstractivetext TextParaphrasing: BağcıandAmasyali(2021)
summarization,paraphrasing,questionanswering used pre-trained BERT weights to initialize both
andquestiongenerationtasks. Ourcontributions encoderanddecoderweightstocreateaseq2seq
toTurkishNLPresearchareasfollows: transformernetworkfortextparaphrasingtask.
EventhoughusingBERTweightstoinitializeen-
1. SentencePieceUnigramTokenizer coderweightsisareasonableapproach,initializing
thedecodernetworkweightsrandomlyorsetting
2. 135GBsofcleanedtext thesameBERTencoderweightsforthedecoder
isnotoptimalsinceitresultsinamodelwhoseen-
3. VBARTLLMModels
coderanddecoderwerenotpre-trainedtogetherto
generatetext. Fine-tuningamultilingualseq2seq
4. Newstate-of-the-artresultsfortextgeneration
networkisabetterapproachsincethemodelispre-
tasksinTurkish.
trainedfortextgenerationasawhole. Yetitisnot
optimaleither,asmultilingualmodelsareexposed
toatraceamountofTurkishduringpre-training,and
2. Related Work
consequently,mostofthetrainingtimeandlearning
TheexistenceofBERTurk(Dbmdz,2023),aded- capacityofthenetworkareusedforhigh-resource
icatedpre-trainedEncoderonlyLanguageModel languages.
(LM)forTurkishallowedobtainingstate-of-the-art Therefore,bothoftheapproachessofarrelyon
resultsontext(ToprakKesginetal.,2023;Ozdemir sub-optimalpre-trainedmodelsandunderachieve
and Yeniterzi, 2020; Köksal and Özgür, 2021; thefine-tuningpotential.
Şahinuçetal.,2021)andtoken(Arasetal.,2021;
ÇarıkandYeniterzi,2022)classificationtasks.
3. Model
In the case of text generation tasks, however,
researchersarecompelledtoeitheruseBERTurk-
3.1. Tokenizer
basedhybridsolutionsorfine-tunepre-trainedmul-
tilingualseq2seqLLMlikemBARTandmT5since Beforetrainingamodelfromscratch,atokenizer
thereisnodedicatedseq2seqLLMforTurkish. isneededfirst. WetrainSentencePiece(Kudoand
Text Summarization: Baykara and Güngör Richardson,2018)UnigramModelTokenizerwith
(2022a)hasbuiltaseq2seqmodelbycombininga avocabularysizeof32,000onrandomsubsetsofR1/R2/RL Parameters MLSum TRNews XLSum
Safayaetal.,2022
Base 120M 40.23/27.23/35.08
mBART50 610M 43.75/30.60/38.47
mT5-Base 580M 44.13/31.09/38.76
BaykaraandGüngör,2022b
BERT2BERT-32K 248M 41.48/27.23/37.66 41.06/25.60/37.69
mBART25 610M 40.47/26.17/36.22 40.52/25.22/36.80
mT5-Base 580M 42.26/27.81/37.96 41.13/25.75/37.60
Ourwork
VBART-Large 387M 45.75/32.71/39.86 41.97/28.26/36.69 34.15/17.94/28.03
VBART-XLarge 740M 46.13/33.01/40.42 42.55/28.69/37.42 35.13/18.80/29.18
Table1: TextSummarization
OSCAR(Abadjietal.,2022),OPUS(Zhangetal., 135.7GBofspaceondisk,contains50.3Mpages
2020) and Wikipedia dump corpora. Training on andismadeof25.33Bsubwordtokensintotal.
10GBsoftextrequired500+GBsofmemoryand
took3hourson96CPUcores.
3.5. Data Generator
3.2. Network Architecture
Adatageneratoriswrittentofeedthemodeltrain-
ThenetworkisbasedonmBARTarchitecturerather
ing. It works dynamically so that even when the
than BART. Following the finding of mBART au-
sametextinputisreadfromthecorpus,stochastic
thors,thisisaconsciousdesignchoiceandserves
textnoisingprocessesresultinadifferentpermuta-
tostabilizetheFP16mixedprecisiontraining(Mi-
tionofsentencesandtokensmasked. Moreover,
cikeviciusetal.,2017)thankstopost-encoderand
inthecaseofsamplesthatarelongerthanthecon-
post-decoder LayerNorm layers. The only differ-
textlength,arandomcontinuousspanofsentences
encefromthemBARTarchitectureisthatweuse
thatwillfitisselected. Thishastwobenefits. First,
sinusoidalpositionalembeddings. Thenetworkfol-
nosentenceiscutinhalf. Second,themodelgets
lowstheoriginalBART-Largeconfigurationwith12
toseeothersectionsoflongertextsaswell.
encoderanddecoderlayers,16headsand1024
Bothencoderanddecodercontextlengthsare
model dimension. Having a vocabulary size of
set to 1024 with right padding. However, noising
32,000inthisconfigurationresultsin387.6Mtrain-
oftheencoderinputsresultsinshortersequences
ableparameters.
sincespansoftokensarereplacedbyasinglemask
token. Thisresultsinawasteofcomputationonthe
3.3. Pre-training Task encoderside. Hence,theencodercontextlength
issetto800duringpre-trainingforefficiency.
Thepre-trainingtaskissentencepermutationcom-
binedwithspanmaskingwhere30%oftokensare
masked with span length defined by the Poisson
distribution (λ = 3.5), following the BART-Large 3.6. Training
objective.
Training is carried out on 8X Nvidia A100-80 GB
on AWS for 2.7M steps with a batch size of 256
3.4. Training Corpus
andtook30days. Adam(KingmaandBa,2014)
Training corpus is made of Turkish sections of optimizer (β 1 = 0.9,β 2 = 0.98,ϵ = 1e−6) along
with the learning rate scheduler (20,000 warmup
OSCAR-2201(Abadjietal.,2022)andmC4(Xue
steps)fromtheoriginalTransformerpaperisused.
et al., 2020), which contain 10.8M and 87.7 mil-
Dropout is set to 0.1 for the first 2.33M steps, re-
lion pages, respectively. They are concatenated
ducedto0.05forthenext165Kstepsandfinally0
toobtainthefinaltrainingcorpusof98.5Mpages
forthelast205Ksteps.
intotal. However,web-crawleddataisoftennoisy
andfullofkeywords,titlesandothernon-sentence Consideringthatthemodelcontextlengthis1024
texts. In order to obtain a higher-quality dataset, tokensandtrainingiscarriedoutwithabatchsize
thesepagesarecleanedusingachainofrulesand of256onadatasetthatismadeof25.33Btokens,
heuristics. SeetheAppendixforthedetailsofthe 2.7Mstepscorrespondto28epochs. Correspond-
datacleaningprocess. Thecleanedcorpusholds ingtraininglosscanbeseeninFigure1.FromNewsSummary FromNewsContent
R1/R2/RL Parameters MLSum TRNews XLSum MLSum TRNews XLSum
BaykaraandGüngör,2022b
BERT2BERT-32K 248M 39.35/21.14/37.55 41.87/24.37/40.88
mBART25 610M 34.85/18.03/33.46 37.72/20.99/36.74
mT5-Base 580M 40.77/22.42/38.97 41.87/24.49/40.87
Ourwork
VBART-Large 387M 45.17/30.49/42.92 43.05/29.20/41.87 42.72/26.79/40.70 39.82/25.88/37.83 39.26/25.45/38.11 37.40/21.09/34.71
VBART-XLarge 740M 45.11/30.57/42.98 43.52/29.56/42.16 42.89/26.92/40.70 39.79/25.88/37.79 39.41/25.61/38.25 37.39/21.20/34.87
Table2: TitleGeneration
3.7. Model Enlargement 4.1. Text Summarization
Afterthepre-trainingofVBART-Large,wedecided LargeandXLargemodelsarefine-tunedonTurk-
tocreateanXLargeversionbydoublingthenumber ish sections of MLSum (Scialom et al., 2020),
ofencoderanddecoderlayerswhilekeepingthe TRNews (Baykara and Güngör, 2022b), XL-
other configuration intact. Having a smaller bud- Sum(Hasanetal.,2021)andWikilingua(Ladhak
get,VBART-XLargeisinitializedbyusingVBART- etal.,2020)datasetsfor30and20epochsrespec-
Largeweightstospeedupthetraining. Allweights tively,withalearningrateof1e−5. Thentheyare
areinterchangeableexceptforthenewerEncoder evaluated on the corresponding test splits. Note
and Decoder layers. Thus, we set every odd- thatWikilinguadoesnotcontainatestsplitforTurk-
numberedencoderanddecoderlayerweightsfrom ish,soitisexcludedfromtheevaluation.
theLargemodelwhileinitializingeven-numbered Rouge-1,Rouge-2andRouge-Lscoresarecom-
layerweightsfromscratch. puted, and a comparison with the previous work
ThenVBART-XLargeispre-trainedonthesame is reported in Table 1. It is the higher the better
hardwareandthepre-trainingtaskfor8days. Due for Rouge metrics. It is observed that our mod-
totimeconstraintsandtheincreasedmodelsize, elssurpassthepreviousstate-of-the-artinthese
the number of warmup steps and batch size are datasetson5outof6metrics,despitetheLarge
reducedto5,000and128,respectively. Thanksto modelhavingsignificantlyfewerparameters. The
weighttransfer,theXLargemodelreachedtheloss onlyexceptiontothisisBERT2BERT-32Kmodel,
valuesthattheLargemodelhadreachedin1and which achieved better Rouge-L on the TRNews
20daysin1.5hoursand3days,respectively. In dataset. Moreover, we report the results on the
total,theXLargemodelistrainedfor640Ksteps, XLSumdatasetforthefirsttimeintheliteraturein
withthefirst480KstepswithaDropoutvalueof0.1, thiswork.
the next 80K steps with a Dropout value of 0.05,
andthefinal80KstepswithaDropoutvalueof0.
4.2. Title Generation
VBART models are fine-tuned to generate titles
3.8. Implementation
from given news and summaries. The cartesian
SentencePiece (Kudo and Richardson, 2018) li- productofmodelsizesandtrainingdataresultsin
braryisusedtotraintheUnigramModeltokenizer. 4distinctmodels. Largemodelisfine-tunedfor15
The network, data generator and training are im- and25epochsonnewssummaryandcontentas
plemented using Tensorflow (Abadi et al., 2015) input,whileXLargemodelisfine-tunedfor10and
framework. Moreover, trained networks and the 15epochs.
tokenizerareconvertedtocorrespondingHugging- Similartotextsummarization,Rouge-1,Rouge-2
faceimplementationsinordertouseHuggingface andRouge-Lareusedforevaluation. Comparison
Transformers’(Wolfetal.,2019)textgenerationutil- withtheearlierworkisreportedinTable2. Ourmod-
ities. TheycanbeaccessedontheHuggingface elssurpassthepreviousworkbyfarinallmetrics
hub1 alongwiththepre-trainingdataset. anddatasets. SinceBaykaraandGüngör(2022b)
didnotfine-tuneanymodelstogeneratetitlesfrom
thenewscontent,thereisnoresulttocompare.
4. Experiments
Afterpre-training,VBARTmodelsarefine-tunedon 4.3. Text Paraphrasing
varioustasksusingAdamoptimizer(β 1 =0.9,β 2 = VBARTmodelsarefine-tunedfor20and25epochs,
0.98,ϵ=1e−6)withdifferentlearningratesvarying
respectively,onamixtureofOpenSubtitles(Lison
bythedownstreamtask,applyingalineardecayof
andTiedemann,2016),TED(Cettoloetal.,2012)
0.95. Eachtask’sevaluationisconductedusingthe
andTatoeba(ArtetxeandSchwenk,2019)datasets
referencedwork’sevaluationscriptforconsistency.
mentionedinthepaperpublishedbyAlkurdietal.
(2022). The original paper suggests a method
1https://huggingface.co/vngrs-ai to improve the dataset quality. Since the filteredParameters OpenSubtitles Tatoeba
Alkurdietal.,2022 BERTScore-{cased/uncased}/BLEU/Rouge-L/METEOR/TER
mT5-BaseOpenSubtitles 88.89/91.94/36.40/73.87/72.16/37.58 91.61/93.93/34.74/86.60/84.85/18.23
580M
mT5-BaseTatoeba 88.95/92.08/38.13/68.39/65.87/45.13 91.97/94.20/37.02/84.05/81.59/22.76
Ourwork
VBART-Large 387M 89.25/92.22/47.78/75.35/73.93/35.74 95.79/96.87/69.86/88.14/86.56/16.93
VBART-XLarge 740M 89.30/92.25/47.29/75.38/74.04/35.72 95.66/96.77/68.21/87.56/86.14/17.51
Table3: TextParaphrasing
dataisnotpublished,ourmodelsaretrainedand trainingdataset,followingtheoriginalwork. There-
evaluated on the unfiltered data. Then, we com- fore,theseresultsareusefulforcomparisononly
pareourmodelstothemodelsthatwerefine-tuned and are not realistic enough for the tasks them-
on unfiltered data. Results are evaluated using selves.
BERTScore, BLEU, Rouge-L, Meteor and Trans- Our models outperform mT5-Base and mT5-
lation Error Rate (TER) metrics. BERTScore is Small and are comparable to mT5-Large, which
computedusingcased2 anduncased3 versions. is a 1.2B parameters model. Out of 15 metrics,
Results are reported in Table 3. Except for TER, VBART-LargeandVBART-XLargemodelssurpass
which is the lower, the better, it is the higher, the mT5-Largein7and8metrics,respectively. Overall,
betterforallmetrics. mT5-LargeisbetteronTQuADv1,whileoursare
We compare our models with the two models betteronTquADv2.
fromthereferencedwork. Ourmodelsoutperform
bothmT5-Basemodelsineverymetric,especially
5. Discussion
byalargemarginforBERTScore.
5.1. Tokenizer
4.4. QuestionGenerationandAnswering
Tokenizersofmultilingualmodelsaretrainedona
Ourmodelsarefine-tunedon"TurkishNLPQ&A numberoflanguages,andconsequently,theirvo-
Dataset" (Peker, 2018) (will be abbreviated as cabularysizeisoftenlarge. Forinstance,OpenAI’s
TQuAD)forthreetasks-answerextraction,ques- BPE tokenizer, which is used with ChatGPT and
tiongenerationandquestionanswering-usingthe GPT-4models,hasavocabularysizeof100,277.
methodology described in (Çağatay Akyön et al., This results in a non-optimal tokenizer for a low-
2022). TQuADhastwovariations,TQuADv1and resource language such as Turkish. As a result,
TQuADv2, with a large intersection of samples. ittakes3.10tokenstoencodeaTurkishwordon
Moreover, there are some samples that exist in average for the OpenAI tokenizer, while it takes
boththetrainandtheevaluationsets. Theseinter- 1.33 for our tokenizer, whose vocabulary size is
sectionsarereportedintheAppendixBWeconvert one-thirdoftheprior. Consequently,ourtokenizer
the prompts to Turkish and process the raw data is 3.10 × 100K =7.28timesmorecompactinterms
usingthescriptfromthereferencedwork’sgithub of1 re.3 p3 rese3 n2K tationpower.
repository4.
Wefine-tuneourLargeandXLargemodelsfor
5.2. Experiments
50and55epochs,respectively,withalearningrate
of5e−6. Thetrainsetisobtainedbyconcatenating Fine-tuning results of 4 downstream tasks show
TQuaDv1andTQuaDv2anddroppingtheduplicate thathavingadedicatedpre-trainedmodelforalan-
samples. guageoutperformsmultilingualmT5andmBART
ThenweevaluateonthetestsplitsofTQuADv1, models, even when the number of parameters is
TQuADv2 and XQuAD (Artetxe et al., 2019) and significantlylessfortheprior. VBART-Largemodel
reporttheresultsonTable4. BLEU-1,BLEU2and isabletosurpassmBART25,mBART50andmT5-
Rouge-Larecomputedforquestiongenerationtask. Base models despite having 37% and 33% less
F1andExactMatch(EM)arecomputedforques- parameters,respectively. Moreover,itisobserved
tionansweringtask. Inordertobecomparable,we in Table 4 that VBART-Large is at par with mT5-
donotdroptheintersectedtestsamplesfromthe Large,whichhasmorethan3xparameters.
2dbmdz/bert-base-turkish-casedlastreachedin19 5.3. VBART-Large vs. VBART-XLarge
Oct2023
3dbmdz/bert-base-turkish-uncasedlastreachedin19 ItisevidentthattheXLargemodel’simprovement
over the Large model is small. This is because
Oct2023
4OBSS/turkish-question-generation last reached in theXLargemodelispre-trainedforfarfewersteps
19Sep2023. thantheLargeone. TheLargemodelisexposedtoQuestionGenerationTask QuestionAnsweringTask
BLEU-1/BLEU-2/Rouge-L F1/EM
Parameters TQuADv1 TQuADv2 XQuAD TQuADv1 TQuADv2 XQuAD
ÇağatayAkyönetal.,2022
mT5-Small 300M 37.3/30.1/44.3 39.6/32.9/46.5 21.1/13.8/28.4 63.8/48.5 67.1/50.5 48.8/32.9
mT5-Base 580M 48.4/41.7/53.6 47.6/41.2/53.9 27.9/20.9/35.8 72.1/55.8 71.5/56.2 61.1/43.3
mT5-Large 1.2B 49.8/43.2/55.2 49.1/42.7/54.3 29.3/21.9/37.5 74.7/59.6 73.3/58.4 65.0/46.7
Ourwork
VBART-Large 387M 49.2/42.4/54.8 50.8/43.8/56.5 30.0/22.8/35.2 73.9/58.5 75.6/59.6 60.2/44.0
VBART-XLarge 740M 49.1/42.2/54.9 51.5/44.7/57.1 30.5/23.1/35.7 75.1/59.4 77.1/61.0 62.4/46.7
Table4: QuestionGenerationandAnswering
708Btokens,whilethisisonly84BfortheXLarge costs. Second,theapplicabilityofChinchillaScal-
version, 88% less. Despite that, the XLarge ver- ingLawtoanyotherthandecoder-onlymodels,the
sion is able to improve the results of the Large, pre-trainingtaskofwhichisnext-tokenprediction,
althoughwithsmallmargins. Wehypothesizethat is an open question. Despite the recent popular-
theXLargeversioncanimprovesignificantlywhen ityofdecoder-onlymodels,itwouldbebeneficial
pre-trainedformoresteps. to have a heuristic to determine the pre-training
configurationofanencoder-decodermodel.
5.4. Chinchilla Scaling Law
6. Conclusion
Chinchillascalinglaw(Hoffmannetal.,2022)states
that,inthecaseofsingleepochtraining,anLLM In this work, we presented the first sequence-to-
is optimal if there are 20 tokens in the training sequence LLMs for the Turkish language. All of
set per network parameter. Then VBART-Large the components, dataset, tokenizer and models
and VBART-XLarge are 708Btokens = 91.33 and arepreparedandtrainedforTurkishfromscratch.
387.6M×20
84Btokens×28epochs = 10.83 fold optimal, respec- Weshowedourtokenizer’sabilityofcompactrep-
tively3 .87.6 AM c× c2 o0 rdingly, Large and XLarge models resentationovermultilingualones,comparedthe
should have been pre-trained for 30K and 59K two models proposed in this work, introduced a
steps, respectively. However, we observe signifi- methodtoenlargeanexistingLLMandquestioned
cantimprovementsoverthecourseofpre-training therelevancyofChinchillaScalingLawtoencoder-
inFigure1. decoder models. Achieving new state-of-the-art
Moreover,unlikeauto-regressiveGPT(Radford results for abstractive text summarization, para-
etal.,2018)models,pre-trainingobjectiveandthe phrasing, question answering and question gen-
dynamicdatageneratorusedinthisworkprovide eration, we showed that monolingual pre-trained
different input-output pairs every time a page is modelssurpassmultilingualoneswhenfine-tuned
sampled,augmentingthedataset. fordownstreamtasks,despitehavingsignificantly
fewerparameters.
Considering these, it is a matter of question
whethertheChinchillaScalingLawisapplicableto
encoder-decodermodelsormodelswithdynamic Acknowledgements
datageneratorswithapre-trainingobjectiveother
thannexttokenprediction. We thank Amazon Web Services for funding the
pre-trainingofthemodels.
5.5. Future Work
Thisworkcanbeextendedinmultiplewaysforthe
monolingual setting. The most trivial direction is
furthermodelenlargementbyincreasing24layers
oftheXLargemodelto36layersandhitting1Bpa-
rameters. Othersarepre-traininganotherTurkish
model based on a different architecture, using a
differentpre-trainingobjectivesuchasT5,orona
largerdataset.
Beyondlanguage-specificdirections,thereare
tworesearchdirectionsthatcanbeexplored. First,
themodelenlargementtechniqueproposedinthis
workcanbeexclusivelystudied. Thisisapromising
directionthatcanconsiderableamountoftraining7. Bibliographical References datasetsforagglutinativelanguagesturkishand
hungarian.LanguageResourcesandEvaluation,
56(3):973–1007.
Martín Abadi, Ashish Agarwal, Paul Barham, BatuhanBaykaraandTungaGüngör.2022b. Turk-
Eugene Brevdo, Zhifeng Chen, Craig Citro, ish abstractive text summarization using pre-
Greg S. Corrado, Andy Davis, Jeffrey Dean, trainedsequence-to-sequencemodels. Natural
Matthieu Devin, Sanjay Ghemawat, Ian Good- LanguageEngineering,pages1–30.
fellow, Andrew Harp, Geoffrey Irving, Michael
Ahmet Bağcı and Mehmet Fatih Amasyali. 2021.
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Comparison of turkish paraphrase generation
Kaiser,ManjunathKudlur,JoshLevenberg,Dan-
models. In 2021 International Conference on
delionMané,RajatMonga,SherryMoore,Derek
INnovationsinIntelligentSysTemsandApplica-
Murray, Chris Olah, Mike Schuster, Jonathon
tions(INISTA),pages1–6.
Shlens,BenoitSteiner,IlyaSutskever,KunalTal-
war,PaulTucker,VincentVanhoucke,VijayVa- PiotrBojanowski,EdouardGrave,ArmandJoulin,
sudevan,FernandaViégas,OriolVinyals,Pete andTomasMikolov.2017. Enrichingwordvec-
Warden,MartinWattenberg,MartinWicke,Yuan tors with subword information. Transactions
Yu, and Xiaoqiang Zheng. 2015. TensorFlow: oftheassociationforcomputationallinguistics,
Large-scalemachinelearningonheterogeneous 5:135–146.
systems. Softwareavailablefromtensorflow.org.
Buse Çarık and Reyyan Yeniterzi. 2022. A twit-
JulienAbadji,PedroOrtizSuarez,LaurentRomary, ter corpus for named entity recognition in turk-
and Benoît Sagot. 2022. Towards a Cleaner ish. InProceedingsoftheThirteenthLanguage
Document-OrientedMultilingualCrawledCorpus. Resources and Evaluation Conference, pages
arXive-prints,pagearXiv:2201.06642. 4546–4551.
FatihCagatayAkyon,AliDevrimEkinCavusoglu, MauroCettolo,ChristianGirardi,andMarcelloFed-
CemilCengiz,SinanOnurAltinuc,andAlptekin erico.2012. Wit3: Webinventoryoftranscribed
Temizel.2022. Automatedquestiongeneration andtranslatedtalks. InProceedingsoftheCon-
andquestionansweringfromTurkishtexts. Turk- ference of European Association for Machine
ishJournalofElectricalEngineeringandCom-
Translation(EAMT),pages261–268.
puterSciences.
KyunghyunCho,BartVanMerriënboer,CaglarGul-
cehre,DzmitryBahdanau,FethiBougares,Hol-
Besher Alkurdi, Hasan Yunus Sarioglu, and
gerSchwenk,andYoshuaBengio.2014. Learn-
MehmetFatihAmasyali.2022. Semanticsimilar-
ing phrase representations using rnn encoder-
itybasedfilteringforTurkishparaphrasedataset
creation. InProceedingsofthe5thInternational decoderforstatisticalmachinetranslation. arXiv
preprintarXiv:1406.1078.
Conference on Natural Language and Speech
Processing (ICNLSP 2022), pages 119–127, François Chollet et al. 2015. Keras. https://
Trento,Italy.AssociationforComputationalLin-
keras.io.
guistics.
Aakanksha Chowdhery, Sharan Narang, Jacob
GizemAras,DidemMakaroğlu,SenizDemir,and
Devlin,MaartenBosma,GauravMishra,Adam
AltanCakir.2021. Anevaluationofrecentneural
Roberts, Paul Barham, Hyung Won Chung,
sequencetaggingmodelsinturkishnamedentity
Charles Sutton, Sebastian Gehrmann, et al.
recognition. Expert Systems with Applications, 2022. Palm: Scaling language modeling with
182:115049.
pathways. arXivpreprintarXiv:2204.02311.
Mikel Artetxe, Sebastian Ruder, and Dani Yo- Dbmdz.2023. dbmdz/bert-base-turkish-uncased.
gatama. 2019. On the cross-lingual transfer- Online;accessed07-Aug-2023.
ability of monolingual representations. CoRR,
abs/1910.11856. JacobDevlin,Ming-WeiChang,KentonLee,and
KristinaToutanova.2018. Bert: Pre-trainingof
Mikel Artetxe and Holger Schwenk. 2019. Mas- deepbidirectionaltransformersforlanguageun-
sivelymultilingualsentenceembeddingsforzero- derstanding. arXivpreprintarXiv:1810.04805.
shot cross-lingual transfer and beyond. Trans-
Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful
actionsoftheAssociationforComputationalLin-
Islam,KaziMubasshir,Yuan-FangLi,Yong-Bin
guistics,7:597–610.
Kang, M. Sohel Rahman, and Rifat Shahriyar.
BatuhanBaykaraandTungaGüngör.2022a. Ab- 2021. XL-sum: Large-scalemultilingualabstrac-
stractivetextsummarizationandnewlarge-scale tivesummarizationfor44languages. InFindingsoftheAssociationforComputationalLinguistics: Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.
ACL-IJCNLP 2021, pages 4693–4703, Online. 2008. Isolation forest. In 2008 eighth ieee in-
AssociationforComputationalLinguistics. ternational conference on data mining, pages
413–422.IEEE.
SeppHochreiterandJürgenSchmidhuber.1997.
Long short-term memory. Neural computation, Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li,
9(8):1735–1780. Sergey Edunov, Marjan Ghazvininejad, Mike
Lewis,andLukeZettlemoyer.2020. Multilingual
Jordan Hoffmann, Sebastian Borgeaud, Arthur
denoisingpre-trainingforneuralmachinetrans-
Mensch, Elena Buchatskaya, Trevor Cai, Eliza
lation. TransactionsoftheAssociationforCom-
Rutherford,DiegodeLasCasas,LisaAnneHen-
putationalLinguistics,8:726–742.
dricks,JohannesWelbl,AidanClark,etal.2022.
Trainingcompute-optimallargelanguagemodels. Paulius Micikevicius, Sharan Narang, Jonah Al-
arXivpreprintarXiv:2203.15556. ben, Gregory Diamos, Erich Elsen, David
Garcia, Boris Ginsburg, Michael Houston,
WenxiangJiao,WenxuanWang,JTHuang,Xing
Oleksii Kuchaiev, Ganesh Venkatesh, et al.
Wang,andZPTu.2023. Ischatgptagoodtrans-
lator? yeswithgpt-4astheengine.arXivpreprint 2017. Mixed precision training. arXiv preprint
arXiv:2301.08745.
arXiv:1710.03740.
TomasMikolov,KaiChen,GregCorrado,andJef-
DiederikPKingmaandJimmyBa.2014. Adam: A
frey Dean. 2013. Efficient estimation of word
methodforstochasticoptimization.arXivpreprint
arXiv:1412.6980. representationsinvectorspace. arXivpreprint
arXiv:1301.3781.
AbdullatifKöksalandArzucanÖzgür.2021. Twit-
ter dataset and evaluation of transformers for OpenAI.2022. Chatgpt.
turkishsentimentanalysis. In202129thSignal
OpenAI. 2023. Gpt-4 technical report. ArXiv,
Processing and Communications Applications
abs/2303.08774.
Conference(SIU),pages1–4.IEEE.
AnilOzdemirandReyyanYeniterzi.2020.Su-nlpat
Taku Kudo. 2018. Subword regularization: Im-
semeval-2020task12: Offensivelanguageiden-
provingneuralnetworktranslationmodelswith
tificationinturkishtweets. InProceedingsofthe
multiple subword candidates. arXiv preprint
Fourteenth Workshop on Semantic Evaluation,
arXiv:1804.10959.
pages2171–2176.
TakuKudoandJohnRichardson.2018. Sentence-
Adam Paszke, Sam Gross, Francisco Massa,
piece: Asimpleandlanguageindependentsub-
AdamLerer,JamesBradbury,GregoryChanan,
word tokenizer and detokenizer for neural text
TrevorKilleen,ZemingLin,NataliaGimelshein,
processing. arXivpreprintarXiv:1808.06226.
Luca Antiga, Alban Desmaison, Andreas Kopf,
Faisal Ladhak, Esin Durmus, Claire Cardie, and Edward Yang, Zachary DeVito, Martin Raison,
Kathleen McKeown. 2020. Wikilingua: A new Alykhan Tejani, Sasank Chilamkurthy, Benoit
benchmarkdatasetforcross-lingualabstractive Steiner,LuFang,JunjieBai,andSoumithChin-
summarization. arXivpreprintarXiv:2010.03093. tala. 2019. Pytorch: An imperative style, high-
performancedeeplearninglibrary. InAdvances
VietDacLai,NghiaTrungNgo,AmirPouranBen
in Neural Information Processing Systems 32,
Veyseh,HieuMan,FranckDernoncourt,Trung
pages8024–8035.CurranAssociates,Inc.
Bui,andThienHuuNguyen.2023. Chatgptbe-
yond english: Towards a comprehensive eval- Peker.2018. Turkishnlpq&adataset.
uationoflargelanguagemodelsinmultilingual
learning. arXivpreprintarXiv:2304.05613. JeffreyPennington,RichardSocher,andChristo-
pherDManning.2014. Glove: Globalvectorsfor
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan wordrepresentation. InProceedingsofthe2014
Ghazvininejad,AbdelrahmanMohamed,Omer
conferenceonempiricalmethodsinnaturallan-
Levy, Ves Stoyanov, and Luke Zettlemoyer. guageprocessing(EMNLP),pages1532–1543.
2019. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, Matthew E. Peters, Mark Neumann, Mohit Iyyer,
translation,andcomprehension. arXivpreprint Matt Gardner, Christopher Clark, Kenton Lee,
arXiv:1910.13461. and Luke Zettlemoyer. 2018. Deep contextual-
izedwordrepresentations.
PierreLisonandJörgTiedemann.2016. Opensub-
titles2016: Extractinglargeparallelcorporafrom AlecRadford,KarthikNarasimhan,TimSalimans,
movieandtvsubtitles. IlyaSutskever,etal.2018. Improvinglanguageunderstandingbygenerativepre-training. Ope- Linting Xue, Noah Constant, Adam Roberts, Mi-
nAI. hirKale,RamiAl-Rfou,AdityaSiddhant,Aditya
Barua,andColinRaffel.2020. mt5: Amassively
Colin Raffel, Noam Shazeer, Adam Roberts,
multilingual pre-trained text-to-text transformer.
KatherineLee,SharanNarang,MichaelMatena,
arXivpreprintarXiv:2010.11934.
YanqiZhou,WeiLi,andPeterJ.Liu.2019. Ex-
ploringthelimitsoftransferlearningwithaunified BiaoZhang,PhilipWilliams,IvanTitov,andRico
text-to-texttransformer. arXive-prints. Sennrich. 2020. Improving massively multilin-
gual neural machine translation and zero-shot
AliSafaya,EmirhanKurtuluş,ArdaGoktogan,and
translation.
DenizYuret.2022.Mukayese: Turkishnlpstrikes
back. InFindingsoftheAssociationforCompu- FatihÇağatayAkyön,AliDevrimEkinÇavuşoğlu,
tationalLinguistics: ACL2022,pages846–863. CemilCengiz,SinanOnurAltınuç,andAlptekin
Temizel.2022. Automatedquestiongeneration
Furkan Şahinuç, Çağri Toraman, and Aykut Koç.
andquestionansweringfromturkishtexts. Turk-
2021. Topic detection based on deep learning
ishJournalofElectricalEngineeringandCom-
languagemodelinturkishmicroblogs. In2021
puterSciences,30(5):1931–1940.
29thsignalprocessingandcommunicationsap-
plicationsconference(SIU),pages1–4.IEEE.
TevenLeScao,AngelaFan,ChristopherAkiki,El-
liePavlick,SuzanaIlić,DanielHesslow,Roman
Castagné,AlexandraSashaLuccioni,François
Yvon,MatthiasGallé,etal.2022.Bloom: A176b-
parameter open-access multilingual language
model. arXivpreprintarXiv:2211.05100.
ThomasScialom, Paul-AlexisDray, SylvainLam-
prier,BenjaminPiwowarski,andJacopoStaiano.
2020. Mlsum: The multilingual summarization
corpus. arXivpreprintarXiv:2004.14900.
Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2015. Neural machine translation of
rare words with subword units. arXiv preprint
arXiv:1508.07909.
HimmetToprakKesgin,MuzafferKaanYuce,and
MehmetFatihAmasyali.2023. Developingand
evaluatingtinytomedium-sizedturkishbertmod-
els. arXive-prints,pagesarXiv–2307.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
JakobUszkoreit, LlionJones, AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin.2017. Atten-
tionisallyouneed. Advancesinneuralinforma-
tionprocessingsystems,30.
ThomasWolf,LysandreDebut,VictorSanh,Julien
Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan
Funtowicz,etal.2019. Huggingface’stransform-
ers: State-of-the-artnaturallanguageprocess-
ing. arXivpreprintarXiv:1910.03771.
YonghuiWu,MikeSchuster,ZhifengChen,QuocV
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey,etal.2016. Google’sneuralmachine
translation system: Bridging the gap between
humanandmachinetranslation. arXivpreprint
arXiv:1609.08144.A. Data Cleaning • Itdoesnotendwithsentenceterminationpunc-
tuation
Data cleaning is conducted on two levels: page
cleaningandsentencecleaning. T5(Raffeletal., • Itcontainscurlybracket"{"or"}"
2019)authorsdescribetheirdatacleaningprocess
• Itcontains"JavaScript"
indetail. Inthiswork, theyareleveragedandex-
tendedforacleanerdataset.
• It contains "gizlilik ve çerezler" (privacy and
cookies)
A.1. Page Cleaning
• Itcontains"|"
A.1.1. AnomalyDetection
• Thenumberofwordsinthesentenceisless
Some of the pages are filled with Search Engine than4ormorethan50
Optimization(SEO)targeted,repeatingkeywords,
titlesandotherentitiesthatdonotmakeasequence • Thelongestwordinthesentenceislongerthan
of sentences that belong to a context. In order 30characters,whichisthe0.995quantile
togetridofthesepagesinanunsupervisedway,
• Capital letters account for more than half of
IsolationForestanomalydetectionalgorithm(Liu
thesentence
etal.,2008)isusedonfiveheuristicsthatseparate
thementionedbadpagesfromthegoodones. They
• 1/3ofthecharactersarenumeric
are:
• Itdoesnotcontainanypunctuation
• Meansentencelengthonthepage
• It contains too many duplicate words; that
• Standarddeviationofsentencelengthonthe
is,theaveragenumberofduplicatewordsis
page
greaterthan2
• Maximumsentencelengthonthepage
A.3. Finalization of the Dataset
• Ratioofshortsentences(lessthan4words)to
thenumberofsentencesinthepage Asthelaststep,pageswithlessthan5sentences
arecleaned,whichfurtherreducesthenumberof
• Ratioofuppercasecharacterstothenumber pagesby28.69M.Intheend,50.3Mpagesconsist-
ofcharactersonthepage ingof25.33Bsubwordtokensremaintoformthe
135.7GBdataset.
Then, the model is trained and inferred on the
wholedatasetwithdefaultconfigurationtoobtain
theanomalyscores. B. Question Generation & Answering
Dataset Intersections
A.1.2. Rule-basedCleaning
Afterobtaininganomalyscores,finalpagefiltering
(a)Raw(JSON)
isappliedbyremovingpages
xquad-eval tquad2-eval tquad2-train tquad1-eval
(1190) (3034) (14221) (2612)
• whoseanomalyscoreisbelow0.05
tquad1-train
0 194 7641 194
(8308)
• thatcontainsbad/naughtywords tquad1-eval
0 882 194
(2612)
tquad2-train
• thatcontains"loremipsum" (14221) 86 194
tquad2-eval
0
• whoselanguageprobabilityislowerthan0.85 (3034)
(applicableforOSCARcorpusonly)
(b)Processed(Scriptoutput)
Thisresultsina19.81%reductioninthenumber xquad-eval tquad2-eval tquad2-train tquad1-eval
ofpages. (5184) (6368) (75380) (3765)
tquad1-train
0 520 20329 520
(45069)
tquad1-eval
0 2343 520
A.2. Sentence Cleaning (3765)
tquad2-train
227 520
Amongtheremainingpages,eachpageissplitinto (75380)
tquad2-eval
0
sentences,andeachsentenceisdiscardedifany (6368)
ofthefollowingisTrue:
Table5: Dataintersections
• ItisanemptystringWorking on question-answering and question-
generationtasks,wehaveobservedthatthereare
intersectedsamplesinthesplitsthatexistinother
splitsaswell,includingbetweentrainandevalua-
tionsplits.
Inordertounderstandtheextentandcauseof
theintersections,wehaveexaminedrawandpro-
cessedversions,conductingacross-examination.
WereporttheresultsinTable5aandTable5b. Cell
valuesarethenumberofsamplesintersectedbe-
tweenthecorrespondingdatasets. Thenumbers
underthedatasetnamesindicatethetotalnumber
ofsamplesinthedataset.