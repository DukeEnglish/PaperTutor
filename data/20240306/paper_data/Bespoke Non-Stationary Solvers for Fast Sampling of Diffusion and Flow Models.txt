Bespoke Non-Stationary Solvers
for Fast Sampling of Diffusion and Flow Models
NetaShaul1 UrielSinger2 RickyT.Q.Chen3 MatthewLe3 AliThabet2 AlbertPumarola2 YaronLipman31
Abstract Sampleefficiencyofgenerativemodelsiscrucialtoenable
certainapplications,e.g.,onesthatrequireinteractionwith
a user, as well as to reduce carbon footprint and costs of
This paper introduces Bespoke Non-Stationary these,nowvastlypopularmodels. Anongoingresearchef-
(BNS)Solvers,asolverdistillationapproachto forttargetsreducingsamplingcomplexityofdiffusion/flow
improvesampleefficiencyofDiffusionandFlow models,concentratingonthreemainvenues: (i)dedicated
models. BNS solvers are based on a family of solvers: employinghigh-ordernumericalODEsolvers(Kar-
non-stationarysolversthatprovablysubsumesex- rasetal.,2022)and/ortimeandscalereparameterizationsto
istingnumericalODEsolversandconsequently furthersimplifysampletrajectories(Zhang&Chen,2022);
demonstrate considerable improvement in sam- (ii)modeldistillation: fine-tuningthemodeltoapproximate
pleapproximation(PSNR)overthesebaselines. theoriginalmodel’ssamplesorthetrainingdatawithless
Comparedtomodeldistillation,BNSsolversben- function evaluations (Salimans & Ho, 2022; Meng et al.,
efit from a tiny parameter space (<200 param- 2023;Liuetal.,2022). RecentlyalsoperceptionandGAN
eters), fast optimization (two orders of magni- discriminatorlosseshavebeenincorporatedtoimproveper-
tudefaster),maintaindiversityofsamples,andin ceptionquality(Yinetal.,2023);(iii)solverdistillation: a
contrasttoprevioussolverdistillationapproaches ratherrecentandunexploredapproachthatlimitsdistillation
nearly close the gap from standard distillation tooptimizinganumericalsolverthateffectivelysamplethe
methods such as Progressive Distillation in the originalmodel(thatiskeptfrozen).Whilemodeldistillation
low-medium NFE regime. For example, BNS isgenerallyabletoreducethenumberoffunctionevalua-
solver achieves 45 PSNR / 1.76 FID using 16 tions(NFEs)forproducingsampleswithhighperceptual
NFE in class-conditional ImageNet-64. We ex- scores,itcanreducethediversityofthemodelandshiftthe
perimentedwithBNSsolversforconditionalim- generateddistribution. Furthermore,itisstillcostlytotrain
agegeneration,text-to-imagegeneration,andtext- (i.e., conceptually continues training the original model),
2-audiogenerationshowingsignificantimprove- andmostlyrequiresaccesstotheoriginaltrainingdata. In
mentinsampleapproximation(PSNR)inall. contrast,solverdistillationenjoysatinyparametersspace
(i.e.,<200ofparameters),veryfastoptimizationcompared
tomodeldistillation(i.e.,bytwoorderofmagnitude),and
doesnotrequireaccesstotrainingdata(Shauletal.,2023).
ThemaingoalofthispaperistointroduceBespokeNon-
1.Introduction
Stationary(BNS)solvers,asolverdistillationapproachthat
Diffusionandflow-basedmethodsarenowestablishedas provablysubsumesallpreviousdedicatedanddistillation
a leading paradigm for generative models of high dimen- solvers (that we are aware of). While BNS solver family
sional signals including images (Rombach et al., 2021), enjoyshigherexpressivepower,itstillinheritsothermodel
videos(Singeretal.,2022)audio(Vyasetal.,2023),3D distillationpropertiessuchastinyparameterspaceandfast
geometry(Yarivetal.,2023),andphysicalstructuressuch training. Itshigherexpressivepowerdemonstratesaconsid-
asmoleculesandproteins(Hoogeboometal.,2022). While erableimprovementinapproximatingtheoriginalmodel’s
havinganefficienttrainingalgorithms,samplingisacostly samples(PSNR)forlowerNFEs,andisabletonearlyclose
process that still requires tens to hundreds of sequential thegapwithstandardmodeldistillationapproachessuchas
functionevaluationstoproduceasample. ProgressiveDistillation(Salimans&Ho,2022)intermsof
perceptionquality(FID)forlow-mediumNFErange(i.e.,
1WeizmannInstituteofScience2GenAI,Meta3FAIR,Meta. 8-16). We have experimented with BNS solvers for con-
Correspondenceto:NetaShaul<Neta.Shaul@weizmann.ac.il>.
ditionalimagegeneration,Text-to-Image(T2I)generation,
and Text-to-Audio generation. In all cases BNS solvers
Preprint.
1
4202
raM
2
]GL.sc[
1v92310.3042:viXraGT(AdaptiveRK45)≈160NFE BespokeNon-Stationary:16NFE RK-Midpoint:16NFE RK-Euler:16NFE
Figure1: DifferentsolversonanFM-OT512×512Text-to-Imagemodelwithguidancescale2initiatedwiththesamenoise
(fromlefttoright): Groundtruth(AdaptiveRK45),BNS16NFE(thispaper),RK-Midpoint16NFE,andRK-Euler16NFE.
NotethefidelityofBNScomparedtoGT.Thedifferentrowscorrespondtothecaptions(toptobottom): ”Ahuskyfacing
thecamera.”,”sunflowersinaclearglassvaseonadesk.”,”thecatissittingonthefloorbesideapairoftennisshoes.”.
considerablyimprovedPSNRofgeneratedsamples. Figure 2.Preliminaries
1depictsBNSsamplingfromalargescaleT2Imodelusing
Flow-basedgenerativemodels. Weletx∈Rdrepresent
16NFEproducingconsistentsamplestotheGroundTruth
asignal,e.g.,animageinpixelorlatentspace.Deterministic
(GT) samples, while baselines fail to achieve this consis-
samplingofadiffusionorflowmodelisdonebysolvingan
tency. Asecondarygoalofthispaperistoprovideafull
OrdinaryDifferentialEquation(ODE),
taxonomyofpopularnumericalsolversusedtosamplediffu-
sionandflowmodels,aswellaspresenttheminaconsistent
waythathighlightstheirrelations.
x˙(t)=u (x(t)), (1)
t
Wesummarizethepaper’scontributions:
where x(t) is called a sample trajectory initialized with
(i) IntroduceBNSsolvers;subsumesexistingsolvers.
x(0) = x , where x ∼ p (x ) is a sample form the
0 0 0 0
(ii) AsimpleandeffectiveBNSoptimizationalgorithm. sourcedistributionp usuallyrepresentingnoise, andthe
0
ODE is solved until time t = 1. The Velocity Field
(iii) Significantly improving sample approximation
(VF)u : [0,1]×Rd → Rd isdefinedusingtheprovided
(PSNR)overexistingsolvers,andreducingthegapin
diffusion/flow model, and is detailed below for popular
perception(FID)frommodeldistillationtechniques.
modelparametrizations. Notethatweusetheconvention
(iv) Provideafulltaxonomyofnumericalsolverusedfor ofODEgoingforwardintimewitht=0correspondingto
samplingdiffusionandflowmodels. source/noiseandt=1todata.
2DiffusionandFlow-Matchingmodels. Therearethree Pokleetal.,2023;Kingmaetal.,2021). Forstrictlymono-
commonmodelparametrizationsusedinDiffusion/Flow- tone SnR, Scale-Time transformations (s ,t ) are in a
r r
Matching: (i) (Diffusion) ϵ-prediction (Ho et al., 2020), 1-1 correspondence with a scheduler change (α ,σ ) →
t t
(ii)(Diffusion)thex-prediction(Salimans&Ho,2022),and (α¯ ,σ¯ )intheGaussianprobabilitypath(equation2),and
t t
(iii)(Flow-Matching)velocityfield(VF)prediction(Lipman theconversionbetweenthetwocanbedoneusingthefol-
etal.,2022).Weusethecommonnotationf :[0,1]×Rd → lowingformulas(Shauletal.,2023):
Rd todenoteallthree. Themodelf iscommonlytrained
(cid:41) (cid:40)
onapredefinedtimedependentprobabilitydensitypathp t, α¯ r =s rα tr
⇐⇒
t r =snr−1(snr(r))
. (8)
(cid:90) σ¯ r =s rσ tr s r =σ¯ r/σ tr
p (x)= p (x|x )q(x )dx , (2)
t t 1 1 1
In particular, given a VF u trained with a Gaussian path
definedbyascheduler(α ,σ ),movingtoadifferentsched-
where q(x ) denotes the data distribution, and the condi- t t
1
uler post-training can be done by first computing the ST
tional probability path p (x|x ) is often chosen to be a
t 1
transformation(s ,t )fromequation8andthenusingu¯in
Gaussianpath,thatisdefinedbyaGaussiankernel, r r
equation7andsamplewithequation1.
p (x|x )=N(x|α x ,σ2I), (3)
t 1 t 1 t
3.BespokeNon-StationarySolvers
and the pair of time dependent functions α,σ : [0,1] →
[0,1]arecalledaschedulerandsatisfy
Inthissectionweintroduceandanalyzethemainobjectof
thispaper: BespokeNon-Stationary(BNS)solvers. Westart
α =0=σ , α =1, σ >0. (4)
0 1 1 0
with introducing the Non-Stationary (NS) solvers family
All schedulers discussed in followedbydevelopinganalgorithmicframeworktosearch
f β γ
this paper (and those prac- t t t within this family a particular solver suitable to sample
tically used in the litera- Velocity 0 1 a provided pre-trained diffusion or flow model. We call
ture) have strictly monoton- ϵ-pred α˙t σ˙tαt−σtα˙t suchasolverBNSsolver. Weconcludethissectionwith
ically increasing Signal-to- αt αt atheoreticalanalysisprovidingacompletetaxonomyfor
Noise(SnR)ratio,definedby x-pred σ σ˙t
t
σtα˙t σ− tσ˙tαt popularODEsolversusedfordiffusion/flowssamplingand
snr(t) = α /σ . For Gaus- Table1: Velocities provingthatNSsolverssubsumesthemall,seeFigure3.
t t
sian paths, the velocity field ofcommonmodels.
u (usedforsampling,equation1)takestheform 3.1.Non-StationarySolvers
t
u (x)=β x+γ f (x), (5) Inpractice,equation1issolved
t t t t
usinganumericalODEsolver.
wherethecoefficientsβ t,γ taregiveninTable1. We consider a broad family
of ODE solvers - the Non-
STtransformationsandpost-trainingschedulerchange. Stationary (NS) Solvers. An
AScale-Time(ST)transformation(Shauletal.,2023)trans- n-stepNSsolverisdefinedby
Figure2: Setup.
formssampletrajectoriesx(t)accordingtotheformula apair:(i)atime-stepdiscretiza-
tion,and(ii)asetofnupdaterules.Thetime-stepdiscretiza-
x¯(r)=s x(t ), (6)
r r tionisamonotonicallyincreasingsequence,
wheret:[0,1]→[0,1]ands:[0,1]→R >0areatimeand
T
=(cid:0)
t ,t ,...,t ,t
(cid:1)
, (9)
n 0 1 n−1 n
scalereparameterizationfunctionssatisfyingt =0,t =1
0 1
ands 0,s 1 > 0. Theseconditionsinparticularimplythat always starts at t 0 = 0 and ends with t n = 1. The i-th
x¯(1)=s 1x(1),thatis,wecanrecovertheoriginalsample updaterule,wherei=0,...,n−1,hastheform
x(1)fromthetransformedpath’ssamplex¯(1)viax(1) =
s−1x¯(1). Consequently,STtransformationscanpotentially x =X c +U d , (10)
1 i+1 i i i i
simplify the sample trajectories for approximation while
wherethematrixX ∈Rd×(i+1)storesallpreviousapprox-
stillallowingtorecoverthemodel’soriginalsamples. The i
transformedVFu¯thatgeneratestheST-transformedpaths imatedpointsonthesampletrajectoryuntilandincluding
x¯(r)is time t i, and the matrix U i ∈ Rd×(i+1) stores all velocity
s˙ (cid:18) x(cid:19) vectorsevaluatedatthoseprevioussamples,
u¯ (x)= rx+t˙ s u . (7)
r s r r r tr s r    
Particularinstancesofthisformulaarealsoderivedand/or X
i
= x
0
x
1
··· x
i
,U
i
=u
0
u
1
··· u i,
discussed in (Karras et al., 2022; Zhang & Chen, 2022;
3Proposition3.1. Foreveryupdaterule(c ,d )∈Ri+1×
Non-Stationary i i
Ri+1 ofanNSsolversthereaexistapair(a ,b ) ∈ R×
i i
Ri+1sothattheupdaterulecanbeequivalentlywrittenas
Multistep RK
Euler x =x a +U b . (11)
i+1 0 i i i
Scale-Time Scale-Time
Multistep RK
DDIM Furthermore,ifthecolumnsofU iarelinearlyindependent
Exponential Exponential
thenthepair(a ,b )isunique.
Multistep RK i i
ThepropositionisprovedusinginductioninAppendixA.
We note that (Duan et al., 2023a) shows a similar result
Figure3: TaxonomyofODEsolversusedforsamplingof for diffusion ϵ-prediction vectors and the special case of
diffusion/flowgenerativemodels. X ∈Rd(insteadofthemoregeneralX ∈Rd×(i+1)).
i i
Withequation11asthenewNSupdaterulesthecomplete
setofparametersθ ∈RprepresentinganNSsolversis
where we denote u = u (x ), and the vectors c ,d ∈
j tj j i i θ =[T ,(a ,b ),...,(a ,b )], (12)
Ri+1aretheparametersofthei-thstep,seeFigure2. The n 0 0 n−1 n−1
i-thstepoutputsx i+1thatapproximatestheGTsampleat wherethenumberofparametersisp=n(cid:0)n+5(cid:1)
+1,which
thesametime,i.e.,x(t ). TheNSsolvercanutilizean 2
i+1 isthedimensionofn-stepsNSsolvers. Algorithm1shows
arbitrary linear combination of all previous points on the
howtogenerateasamplewithanNSsolver.
trajectoryandtheircorrespondingvelocities.
Costfunction. TofindaneffectiveNSsolverθ wecon-
∗
3.2.OptimizingBNSSolvers siderasetofpairs(x ,x(1)),wherex ∼p (x )aresource
0 0 0 0
samples,andx(1)arehighaccuracyapproximatesolutions
OurgoalistofindamemberintheNSfamilyofsolvers
ofequation1withx(0) = x asinitialconditions. Then,
thatprovidesagoodsamplerforaspecificdiffusionorflow 0
weoptimizethePSNRloss,
model u. We call such a model-specific solver Bespoke
Non-Stationary(BNS)solver. Inordertofindanefficient
BNSsolverwerequire:(i)aparameterizationθ ∈RpofNS L(θ)=−E (x0,x(1))log(cid:13) (cid:13)xθ n−x(1)(cid:13) (cid:13)2 , (13)
solvers family; (ii) a cost function, L(θ), quantifying the
wherexθ istheoutputofAlgorithm1initializedwithx ,
effectivenessofdifferentNSsolvercandidatesinsampling n 0
thevelocityfieldu,andθ;wedenote∥x∥2 = 1(cid:80)d x2.
u;and(iii)aninitializationθ =θ 0foroptimizingthecost d i=1 i
function. Wedetailthesenext.
Initializationandpreconditioning. Thelastremaining
partofourmethodisinitializationθ =θ andprecondition-
0
Algorithm1Non-Stationarysampling. ing, which are related. To have an effective optimization
of the loss and reach a good solution we would like to
Require: NSsolverθ,modelu,initialnoisex
0 start from an already reasonable solver. For that end we
U ←[] ▷emptymatrixinitialization
−1 simplytakeθ tocoincidewithagenericODEnumerical
fori=0,1,...,n−1do 0
(cid:2) (cid:3)
solversuchthatEuler(RK1storder)orMidpoint(RK2nd
U ← U u (x )
i i−1 tθ i i order),seedefinitioninAppendixC.Thisisalwayspossible
x ←x aθ+U bθ
i+1 0 i i i since, as we show in Section 3.3, all generic solvers are
endfor
particularinstancesofNSsolvers. However,justproviding
returnxθ
n a good initialization is not always enough for successful
optimizationasbadconditioningcanleadtoeitherdiverg-
ing solutions or excruciating slow convergence (Nocedal
&Wright,1999). Wefoundthatinsomecases,especially
NSsolversparameterization. Thenaiverepresentation whenusinghighClassifierFreeGuidance(CFG)scale(Ho
ofanNSsolverfollowingtheaboveintroductionwouldbe &Salimans,2022)preconditioningthevelocityfielduby
to collect the time discretization vector T n and all pairs firstchangingitsoriginalschedulerimprovesconvergence
(c i,d i), i = 0,...,n − 1, defining the update rules in ofθandreachesbettersolutionsingeneral. Inparticularwe
equation10. Althoughdoablethiswouldprovideanover- denotebyσ > 0apreconditioninghyperparameter and
0
parameterized representation, meaning an NS solver can changethevelocityfieldutou¯accordingtothescheduler
be represented in more than a single way. The following
propositionprovidesagenericallyuniquerepresentation: σ¯ =σ σ , α¯ =α , (14)
t 0 t t t
4whichcorrespondstochangingthesourcedistributiontobe EDM (Karras et al., 2022) change the original model’s
proportionaltop ( x ),i.e.,largerstandarddeviation. This scheduler(α ,σ )to
0 σ0 t t
isdoneusingequations7,8asdescribedindetailinSection
α¯ =1, σ¯ =σ (1−r), (16)
2. TheBNSoptimizationisprovidedinAlgorithm2. r r max
whereσ = 80. Thisschedulertransformstheoriginal
Algorithm2BespokeNon-Stationarysolvertraining. max
conditionalpathstop (x|x )=N(x|x ,σ (1−r))so
t 1 1 max
Require: modelu,pairsD ={(x 0,x(1))},n,θ 0 thatattimer = 0,assumingx 1 haszeromeanandstd≪
initializeθ ←θ 0 σ max =80,theprobabilitypathapproximatestheGaussian
whilenotconvergeddo
for(x 0,x(1))∈Ddo p 0 ≈N(0,σ m2 axI). (17)
xθ ←NS sampling(x ,u,θ) ▷Alg.1
n 0 ThisschedulerisoftencalledVarianceExploding(VE)due
θ ←θ−γ∇ L(θ) ▷optimizationstep,eq.13
θ tothelargenoisestd. Notethatσ hastobesufficiently
endfor max
largeforequation17tohold. Incontrast,ourinitialization
endwhile
utilizesatargetschedulerthatattimer =0reachesarbitrary
returnθ
desiredstd(thehyperparameterσ )withnobias,i.e.,p =
0 0
N(0,σ2I). Inpracticewefindthatsettingσ toohighhurts
0 0
performance. Lastly, EDM incorporate a particular time
3.3.ExpressivepowerofNon-StationarySolvers discretizationontopofthisschedulerchange,potentiallyto
compensateforthehighσ .
Inthissectionwestartbyreviewinggenericsolvers,move max
to dedicated solvers, explained in a unified way with the BespokeScale-Timesolvers(Shauletal.,2023)suggest
aidoftheSTtransformationtool,andconcludewithafull tosearchamongtheSTtransformationsforaparticularin-
solvertaxonomytheorem. Inparticular,thistheoremshows stancethatfacilitatessamplingaspecificmodel. Inmore
thatNon-Stationarysolverssubsumesallothersolvers. The detail, applyinganSTtransformationtoequation15pro-
solvertaxonomyisillustratedinFigure3. vides
(cid:90) ri+1
3.3.1.GENERICSOLVERS x¯(r )=x¯(r )+ u¯ (x¯(r))dr, (18)
i+1 i r
ri
Generic solvers build a series of approximations,
x 0,...,x i,x i+1,...,x n, to the solution of the ODE in where x¯(r) and u¯ r(x) are as in equations 6 and 7 (resp.).
equation1byiterativelyapplyinganupdaterule. Thecom- Nowapplyingagenericsolver(Section3.3.1)onegetsan
monupdaterulesarederivedfromthefollowingformula approximationtox(1).BespokeSTalgorithmthensearches
describingthesolutiontotheODEattimet i+1basedona amongthespaceofall(s t,t r)fortheonethatinexpectation
knownsolutionattimet ,i.e., leads to good approximations of x(1) over a set of train-
i
ingsampletrajectories. Anotherrelatedworkis(Watson
(cid:90) ti+1
x(t )=x(t )+ u (x(t))dt. (15) etal.,2021)thatalsooptimizesforasamplingscheduler,
i+1 i t
ti concentratingondiscretediffusionmodelsandperception
Generic solvers numerically approximate the integral by losses.
using,e.g.,apolynomialapproximationofu (x(t))inthe
t
interval [t ,t ], resulting in a stationary (i.e., indepen- Exponential Integrator (Song et al., 2022; Zhang &
i i+1
dentofi)updaterule: Adam-BashforthandMultistepmeth- Chen,2022;Luetal.,2022a;2023). Forϵ/x-prediction
odsbuildtheapproximationbasedonpast timest , diffusionmodelwithVFasdefinedinequation5thesam-
i−m+j
j = 1...,m,whileRunge-Kuttamethodsapproximateit plingODEofequation1takestheform
by future times inside the interval [t ,t ]. Appendix C
i i+1 ψ˙ σ α˙ −σ˙ α
providesdetailedformulasofbothfamilies,whereamore x˙(t)= tx(t)+η t t t tf (x(t)), (19)
ψ ψ t
elaborateexpositioncanbefoundin(Iserles,2009). t t
wheref istheϵ/x-prediction,and
t
3.3.2.DEDICATEDSOLVERS
(cid:40)
(α ,−1) iff isϵ-pred
Inthissectionwecoverdedicatedsolversdevelopedspecifi- (ψ ,η)= t . (20)
t
callyfordiffusionandflowmodels,takingadvantageofthe
(σ t,1) iff isx-pred
particularstructureoftheGaussianprobabilitypath(equa-
Nowchangingtheoriginalmodel’sscheduler(α ,σ )to
tion 2) and VF form (equation 5). Interestingly, all can t t
beexplainedusingschedulerchange/STtransformations 1 1
α¯ = α , σ¯ = σ , (21)
(detailedinSection2)andapplicationofagenericsolvers. t ψ r r ψ r
r r
5ImageNet-64:ϵ-pred ImageNet-64:FM/v-CS ImageNet-64:FM-OT ImageNet-128:FM-OT
50 50 50 50
45 45 45 45
40 40 40 40
35 35 35 35
30 30 30 30
25 25 25 25
20 20 20 20
15 15 15 15
10 10 10 10
5.0 7.5 10.0 12.5 15.0 17.5 20.0 4 6 8 10 12 14 16 18 20 4 6 8 10 12 14 16 18 20 4 6 8 10 12 14 16 18 20
NFE NFE NFE NFE
10 10 10 10
DDIM RK-Euler RK-Euler RK-Euler
9 9 9 9
DPM++(M2) RK-Midpoint RK-Midpoint RK-Midpoint
8 8 8 8
DPM++(S2) BST BST BST
7 7 7 7
DPM++(S3) BNS BNS BNS
6 6 6 6
BST
5 BNS 5 5 5
4 4 4 4
3 3 3 3
2 2 2 2
1 1 1 1
5.0 7.5 10.0 12.5 15.0 17.5 20.0 4 6 8 10 12 14 16 18 20 4 6 8 10 12 14 16 18 20 4 6 8 10 12 14 16 18 20
NFE NFE NFE NFE
Figure4: BNSsolversvs.BSTsolvers,RK-Midpoint/Euler,DDIM,DDIM,andDPM++onImageNet-64,andImage-
Net128: PSNRvs.NFE(toprow),andFIDvs.NFE(bottomrow).
which corresponds to the conditional paths p (x|x ) = solverdistillationis(Duanetal.,2023b)thatremovestime
t 1
N(x|x ,snr(r)−2I) for ϵ-prediction and p (x|x ) = steps from a diffusion sampler and uses a similar param-
1 t 1
N(x|snr(r)x ,I) for x-prediction, where as before eterization to equation 11 for approximating the missing
1
snr(r) = α /σ . Using equation 7 in equation 18 and ϵ-predictionvalueswithlinearprojection. Incontrast,we
r r
rearrangingleadstoExponentialIntegrators’basicformula formulateasingleoptimizationproblemovertheNSfamily
(equivalenttotheresultafteremployingvariationofcon- ofsolverstodirectlyminimizethesolver’serror.
stantsmethod(Luetal.,2023))
Modeldistillationalsotargetslearninganefficientsolver
fortheoriginalmodelbutforthatendfine-tunestheorig-
x(t )=
ψ
ti+1x(t )+yψ
(cid:90) λti+1
eyλf (x(λ))dλ,
inal. Early attempts minimize directly a sample approxi-
i+1 ψ i ti+1 λ mationloss(Luhman&Luhman,2021),whilefollow-up
ti λti
approachesprogressivelyreducethenumberofsteps(Sali-
(22)
mans&Ho,2022;Mengetal.,2023),oriterativelyfine-tune
where λ = logsnr(t) and f = f , where t is the in-
t λ tλ λ
frompreviousmodel’ssamples(Liuetal.,2022). Weshow
verseofλ whichisdefinedsinceweassumeλ ismono-
t t
thatBNSsolvers,thatuseonlyatinyfractionoftheparam-
tonicallyincreasing. Notethatthistransformationisalso
etercountofmodeldistillation,canbetrainedquicklyon
discussed(Zhang&Chen,2023).Nowusinggenericsolvers
atinytrainingsetandachievenearlycomparablesampling
(Section3.3.1)toapproximatetheintegralaboveleadsto
perceptualquality.
thedesiredsolver.
Wearenowreadytoformulateourmaintheoremproving
5.Experiments
therelationsdepictedinFigure3,provedinAppendixB:
WeevaluateBNSsolverson: (i)Classconditionalimage
Theorem3.2 (SolverTaxonomy). TheRunge-Kutta(RK
generation,(ii)Text-to-Imagegeneration,and(iii)Text-to-
and Exponential-RK). family is included in the Scale-
Audiogeneration. Additionally, wecompareourmethod
TimeRKfamily,whiletheMultistepfamily(Multistepand
withmodeldistillation. Unlessstatedotherwise,conditional
Exponential-Multistep)isincludedintheScale-Timemulti-
samplingisdoneusingclassifier-freeguidance(CFG)(Ho
stepfamily. TheScale-TimefamilyisincludedintheNon-
&Salimans,2022;Zhengetal.,2023). AllBNSsolversare
Stationarysolversfamily.
trainedon520pairs(x ,x(1))ofnoiseandgeneratedimage
0
usingadaptiveRK45(Shampine,1986)solver. Duringopti-
4.Previouswork mization(Algorithm2)welogPSNRonavalidationsetof
1024suchpairsandreportresultsonbestvalidationiteration.
Mostpreviousworksondedicatedsolversandsolverdistil-
FurtherdetailsareinAppendixD.1. Aspre-trainedmodels
lationarealreadycoveredinSection3.3. Herewediscuss
weuse: (i)ϵ-predictionDiffusionmodel(Hoetal.,2020)
works that are not yet covered. Another related work on
6
RNSP
DIF
RNSP
DIF
RNSP
DIF
RNSP
DIFGT NFE=20 NFE=16 NFE=12 GT NFE=20 NFE=16 NFE=12
Figure5: BNSvs.RK-MidpointonlatentFM-OTText-to-Image512x512: (left)guidancescale2.0withthecaption”a
buildingisshownbehindtreesandshrubs.”,(right)guidancescale6.5withthe”pandabearsittingintreewithnoleaves.”
w=2.0 NFE PSNR↑ PickScore↑ ClipScore↑ FID↓ 1986),andFre´chetInceptionDistance(FID)(Heuseletal.,
GT(DOPRI5) 170 ∞ 20.95 0.252 15.20 2017),bothmetricsarecomputedon50ksamplesfromthe
models.WetrainourBNSsolversforNFE∈{4,6,...,20}
RK-Euler 12 13.95 20.66 0.252 16.62
16 14.86 20.79 0.253 13.68 withRK-Midpointinitialsolverandpreconditioningσ 0 =1,
20 15.71 20.86 0.253 12.86 eachtaking0.1−1%fractionoftheGPUdaysusedtotrain
RK-Midpoint 12 15.05 20.72 0.250 11.54 thediffusion/flowmodels(i.e.,2-10GPUdayswithNvidia
16 16.28 20.82 0.250 12.03 V100). Wecompareourresultsagainstvariousbaselines,
20 17.46 20.88 0.251 12.50 includinggenericsolvers,exponentialsolvers:DDIM(Song
BNS 12 25.86 20.83 0.252 13.93 etal.,2022),andDPM(Zhang&Chen,2023),aswellasthe
16 29.13 20.90 0.252 14.48 BST(Shauletal.,2023)distilledsolvers. Figure4shows
20 31.78 20.91 0.252 14.68
ourBNSsolversimprovesbothPSNRandFIDoverallbase-
w=6.5 NFE PSNR↑ PickScore↑ ClipScore↑ FID↓
lines. Specifically,inPSNRmetricweachievealargeim-
GT(DOPRI5) 268 ∞ 21.16 0.260 23.99 provementofatleast5−10dBabovetherunner-upbaseline
RK-Euler 12 9.61 19.92 0.237 50.00 andgetto5%fromFIDoftheGTsolver(about160−320
16 10.02 20.34 0.247 35.37
NFE)with16NFE.QualitativeexamplesareshowninFig-
20 10.52 20.60 0.252 28.36
ures9and10inAppendixD.2. Interestingly,forPSNRwe
RK-Midpoint 12 9.65 19.79 0.240 34.01
seetheorder: BNS>BST>DPM>RK-Midpoint/Euler,
16 9.98 20.11 0.245 27.06
thatmatcheswellthesolverhierarchyprovedinTheorem
20 10.34 20.34 0.248 23.63
3.2,seealsoFigure3.InFigure11wealsoshowanablation
BNS 12 18.94 20.92 0.261 20.67
experimentcomparingtheNon-StationaryandScale-Time
16 21.23 21.03 0.260 21.93
20 23.27 21.09 0.259 22.56 familybothoptimizedwithAlgorithm2,demonstratingthe
benefitintheNSfamilyofsolversovertheSTfamily.
Table2: BNSsolversvs.GTandRK-Midpoint,RK-Euler
onText-to-Image512FM-OTevaluatedonMS-COCO. 5.2.Text-to-Imagegeneration.
InconsiderationsregardingthetrainingdataofStableDiffu-
sion(suchascopyrightinfringementsandconsent),wehave
withtheVariancePreservingscheduler(ϵ-VP)(Songetal.,
opted not to experiment with this model. Hence, we use
2020); (ii) Flow-Matching with the Conditional Optimal-
alargelatentFM-OTT2Imodel(2.2bparameters)trained
Transport scheduler (FM-OT)(Lipman et al., 2022; Liu
onaproprietarydatasetof330mimage-textpairs. Image
etal.,2022),andFlow-Matching/v-predictionwiththeCo-
sizeis512×512×3whilethelatentspaceisofdimension
sinescheduler(FM/v)(Salimans&Ho,2022;Albergo&
64×64×4; see implementation details in Appendix E.
Vanden-Eijnden,2022). Detailsofthepre-trainedmodels
For evaluation we report PSNR w.r.t GT images, similar
areinAppendixE.
to the class conditional task. Additionally, we use MS-
5.1.Classconditionimagegeneration. COCO(Linetal.,2015)validationsetandreportperceptual
WeevaluateourmethodontheclassconditionalImageNet- metrics including Pick Score (Kirstain et al., 2023), Clip
64/128 (Dengetal.,2009)dataset. Asrecommendedbythe Score (Ramesh et al., 2022), and zero-shot FID. All four
authors(ima)tosupportfairnessandpreservepeople’spri- metricsarecomputedon30Kgeneratedandvalidationim-
vacyweusedtheofficialface-blurreddata,seemoredetails ages and reported for guidance (CFG) scale w = 2 and
inAppendixD.2. WereportPSNRw.r.t.groundtruth(GT) w =6.5.Foreachguidancescale,weoptimizeBNSsolvers
images generated with adaptive RK45 solver (Shampine, forNFE∈{12,16,20}withinitialsolverRK-Euler. Each
7
tniopdiM-KR
SNBCIFAR10 NFE FID GT-FID Forwards TrainingSet Parameters LibriSpeechTTS Audiocaps
PD 4 3.00 2.51 211m 50k >50m
8 2.57 192m (CIFAR10)
BNS 4 25.20 2.54 4.9m 520 18
8 2.73 9.7m 52
ImageNet-64NFE FID GT-FID Forwards TrainingSet Parameters
PD 4 4.79 2.92 2457m 1.2m >200m
8 3.39 2150m (ImageNet)
16 2.97 1843m
BNS 4 31.83 2.50 2.5m 520 18 Figure6: NFEvs.SNRofBNSsolvers,BSTsolvers,RK-
8 3.90 4.9m 52 Midpoint/Euler for Speech Generation FM-OT evaluated
16 2.62 9.7m 168
on: (left)LibriSpeechTTS,(right)Audiocaps.
Table 3: BNS solver vs. Progressive Distillation on CI-
FAR10andImageNet-641classconditionalwithw =0. 5.4.Audiogeneration.
Next, we experiment with BNS solvers on an audio gen-
eration model. We use the speech model introduced by
solver training takes 15-24 GPU days with Nvidia V100,
(Vyasetal.,2023),whichisalatentFlow-Matchingmodel
consistingatmost0.3%fractionoftheGPUdaysusedto
trainedtoinfillEncodec (De´fossezetal.,2022)features,
trainthelatentFM-OTmodel. Wefindthatσ = 5gives
0 conditionedonframe-alignedtexttranscripts. Totrainthe
best results for w = 2, while σ = 10 for w = 6.5. As
0 BNS and BST solvers we generate 10k random samples
baselineswecompareourresultstoRK-Midpoint/Euler. Ta-
from the training set using the RK45 solver. We evalu-
ble2showsBNSsolversimprovesPSNRbyatleast10dB
ateon8differentdatasets,eachofwhicharedescribedin
and consistently improves Pick Score as well. The Clip
D.5. Ineachsetting,themodelisgivenatranscriptanda
ScoreandFIDmetricsarenotcorrelatedwithNFEandare
(possiblyempty)audioprompt. Themodelneedstosynthe-
considerednoisymetricsforT2Ievaluations(Kirstainetal.,
sizespeechcorrespondingtothetranscriptandthespeech
2023). Figure5showsqualitativeexamples. Additionally,
shouldpreservethespeakerstyleofthegivenaudioprompt
Table 5, and Figures 7 and 8 in Appendix D.3 shows an
ifoneisprovided. WeevaluatebycomputingtheSNR(dB)
ablationcomparingBNSsolvertoitsinitialization. Lastly,
w.r.t. ground truth samples generated using the adaptive
we note that higher guidance scale generally tends to be
RK45solver. Figure6comparestheSNRfortwodatasets
hardtoapproximateascanbenoticedbycomparingPSNR
atdifferentNFEsforeachsolver. Theremainingdatasets
valuesfordifferentNFEsinTable2.
canbefoundinFigure12. AcrossalldatasetsBNSsolveris
consistentlybetterthanbaselinesimproving1dB-3dBfrom
5.3.Bespokesolversvs. Distillation
runner-up.
We compare BNS solvers with Progressive Distillation
(PD) (Salimans & Ho, 2022) on two datasets: CI- 6.Conclusionsandlimitations
FAR10(Krizhevsky&Hinton,2009),andclassconditional
ImageNet-641.ForBNSweusetheFM-OTmodels.Forfair WehaveintroducedBespokeNon-Stationary(BNS)solvers
comparison,wereportbothBNSandPDintheunguided based on the provably expressive Non-Stationary (NS)
setting(i.e.,w =0);resultsforPDtakenfrom(Salimans& solversfamilyanddemonstratedthistheoreticalexpressive-
Ho,2022;Mengetal.,2023). Table3showsFID,number nesstranslatestobettersamplesapproximationatlowNFE
offorwardpassesinthemodelduringtraining(Forwards), presentingbestPSNRperNFEresultsamongalargesetof
where computation is detailed in Appendix D.4, training baselinesandapplications.Incontrasttoprevioussolverdis-
setsize(TrainingSet),andnumberoftrainedparameters tillationmethodssuchas(Shauletal.,2023)BNSdon’tneed
in BNS/PD (Parameters). While on NFE < 8 we fail to toa-priorifixabasesolverandconsequentlyanorder,how-
compete with PD’s FID, we see that in the mid range of everitdoesneedtooptimizeadifferentsolverfordifferent
8−16NFEourBNSsolvergivescomparableFIDusing NFE,whichopensaninterestingfutureresearchquestion
significantlylesscompute. Specifically,forImageNet-64 whetherasinglesolvercanhandledifferentNFEwithout
our training uses only 0.5% of the forwards used by PD. degradingperformance. FurtherlimitationsofBNSsolvers
Additionally, the low number of parameters allows us to isthattheydon’treachtheextremelylowNFEregime(1-4),
generalizewelldespitethetinytrainingset. andforT2IgenerationutilizeCFG(increasingtheeffective
batchsize). Aninterestingfutureworkistofurtherincrease
1NotethatBNSevaluatesonmodelstrainedwiththeblurred
the expressiveness to further reduce NFE and potentially
face ImageNet as recommended in ImageNet website (ima) to
incorporateconditionalguidanceinthesolver.
supportfairnessandpreservepeople’sprivacy.
87.ImpactStatement URL http://dx.doi.org/10.1145/3583780.
3614999.
This paper presents a method for fast sampling of diffu-
sion and flow models. There are many potential societal Duan, Z., Wang, C., Chen, C., Huang, J., and Qian, W.
consequences of our work, none which we feel must be Optimal linear subspace search: Learning to construct
specificallyhighlightedhere. fast and high-quality schedulers for diffusion models.
arXivpreprintarXiv:2305.14677,2023b.
Acknowledgements
De´fossez, A., Copet, J., Synnaeve, G., and Adi, Y. High
NSissupportedbyagrantfromIsraelCHEProgramfor fidelityneuralaudiocompression,2022.
DataScienceResearchCenters.
Godfrey,J.J.,Holliman,E.C.,andMcDaniel,J. Switch-
board: Telephonespeechcorpusforresearchanddevel-
References
opment. In Acoustics, Speech, and Signal Processing,
Imagenetwebsite. https://www.image-net.org/. IEEEInternationalConferenceon,volume1,pp.517–
520.IEEEComputerSociety,1992.
Albergo,M.S.andVanden-Eijnden,E. Buildingnormaliz-
ingflowswithstochasticinterpolants,2022. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and
Hochreiter,S. Ganstrainedbyatwotime-scaleupdate
Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler,
ruleconvergetoalocalnashequilibrium. InAdvancesin
M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M.,
NeuralInformationProcessingSystems(NeurIPS),2017.
andWeber,G. Commonvoice: Amassively-multilingual
speechcorpus. InInternationalConferenceonLanguage Ho,J.andSalimans,T. Classifier-freediffusionguidance.
ResourcesandEvaluation,2019. arXivpreprintarXiv:2207.12598,2022.
Chen,S.,Wang,C.,Chen,Z.,Wu,Y.,Liu,S.,Chen,Z.,Li,
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
J.,Kanda,N.,Yoshioka,T.,Xiao,X.,etal.Wavlm:Large-
bilisticmodels. Advancesinneuralinformationprocess-
scale self-supervised pre-training for full stack speech
ingsystems,33:6840–6851,2020.
processing. IEEEJournalofSelectedTopicsinSignal
Processing,16(6):1505–1518,2022. Hoogeboom,E.,Satorras,V.G.,Vignac,C.,andWelling,M.
Equivariantdiffusionformoleculegenerationin3d. In
Chrabaszcz,P.,Loshchilov,I.,andHutter,F. Adownsam-
Internationalconferenceonmachinelearning,pp.8867–
pled variant of imagenet as an alternative to the cifar
8887.PMLR,2022.
datasets. arXivpreprintarXiv:1707.08819,2017.
Iserles,A. Afirstcourseinthenumericalanalysisofdif-
Cieri, Christopher, etal.. FisherEnglishtrainingspeech
ferentialequations. Number44.Cambridgeuniversity
parts1and2LDC200{4,5}S13. WebDownload.Linguis-
press,2009.
ticDataConsortium,Philadelphia,2004,2005.
Clifton, A., Pappu, A., Reddy, S., Yu, Y., Karlgren, J., Karras,T.,Aittala,M.,Aila,T.,andLaine,S. Elucidating
Carterette,B.,andJones,R. Thespotifypodcastdataset. the design space of diffusion-based generative models.
arXivpreprintarXiv:2004.04270,2020. AdvancesinNeuralInformationProcessingSystems,35:
26565–26577,2022.
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andLi,F.-F.
Imagenet: Alarge-scalehierarchicalimagedatabase. In Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps:
TheIEEEConferenceonComputerVisionandPattern Generatingcaptionsforaudiosinthewild. InNAACL-
Recognition(CVPR),2009. HLT,2019.
Dhariwal, P. and Nichol, A. Diffusion models beat gans Kingma, D., Salimans, T., Poole, B., and Ho, J. Varia-
on image synthesis. Advances in neural information tionaldiffusionmodels. Advancesinneuralinformation
processingsystems,34:8780–8794,2021. processingsystems,34:21696–21707,2021.
Duan, Z., Wang, C., Chen, C., Huang, J., and Qian, W. Kingma,D.P.andBa,J. Adam: Amethodforstochastic
Optimal linear subspace search: Learning to construct optimization,2017.
fastandhigh-qualityschedulersfordiffusionmodels. In
Proceedingsofthe32ndACMInternationalConference Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna,
onInformationandKnowledgeManagement,CIKM’23. J., and Levy, O. Pick-a-pic: An open dataset of user
ACM,October2023a. doi: 10.1145/3583780.3614999. preferencesfortext-to-imagegeneration,2023.
9Krizhevsky, A. and Hinton, G. Learning multiple layers Radford,A.,Kim,J.W.,Xu,T.,Brockman,G.,McLeavey,
offeaturesfromtinyimages. InUniversityofToronto, C.,andSutskever,I. Robustspeechrecognitionvialarge-
Canada,2009. scaleweaksupervision. ArXiv,abs/2212.04356,2022.
Lin,T.-Y.,Maire,M.,Belongie,S.,Bourdev,L.,Girshick, Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
R.,Hays,J.,Perona,P.,Ramanan,D.,Zitnick,C.L.,and Matena,M.,Zhou,Y.,Li,W.,andLiu,P.J. Exploring
Dolla´r,P. Microsoftcoco: Commonobjectsincontext, thelimitsoftransferlearningwithaunifiedtext-to-text
2015. transformer. TheJournalofMachineLearningResearch,
21(1):5485–5551,2020.
Lipman,Y.,Chen,R.T.Q.,Ben-Hamu,H.,Nickel,M.,and
Le,M. Flowmatchingforgenerativemodeling. arXiv Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.,andChen,
preprintarXiv:2210.02747,2022. M. Hierarchicaltext-conditionalimagegenerationwith
cliplatents,2022.
Liu, X., Gong, C., and Liu, Q. Flow straight and fast:
Learningtogenerateandtransferdatawithrectifiedflow. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
arXivpreprintarXiv:2209.03003,2022.
Ommer,B. High-resolutionimagesynthesiswithlatent
diffusionmodels,2021.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
Dpm-solver: Afastodesolverfordiffusionprobabilistic
Salimans, T. and Ho, J. Progressive distillation for
modelsamplinginaround10steps. AdvancesinNeural
fast sampling of diffusion models. arXiv preprint
InformationProcessingSystems,35:5775–5787,2022a.
arXiv:2202.00512,2022.
Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,andZhu,J. Dpm-
Shampine,L.F.Somepracticalrunge-kuttaformulas.Math-
solver++: Fastsolverforguidedsamplingofdiffusion
ematicsofcomputation,46(173):135–150,1986.
probabilisticmodels. arXivpreprintarXiv:2211.01095,
2022b. Shaul,N.,Perez,J.,Chen,R.T.Q.,Thabet,A.,Pumarola,
A.,andLipman,Y. Bespokesolversforgenerativeflow
Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,andZhu,J. Dpm-
models,2023.
solver++: Fastsolverforguidedsamplingofdiffusion
probabilisticmodels,2023.
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,
S.,Hu,Q.,Yang,H.,Ashual,O.,Gafni,O.,Parikh,D.,
Luhman, E. and Luhman, T. Knowledge distillation in
Gupta,S.,andTaigman,Y. Make-a-video: Text-to-video
iterativegenerativemodelsforimprovedsamplingspeed.
generationwithouttext-videodata,2022.
arXivpreprintarXiv:2101.02388,2021.
Song, J., Meng, C., and Ermon, S. Denoising diffusion
Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon,
implicitmodels,2022.
S., Ho, J., and Salimans, T. On distillation of guided
diffusionmodels. InProceedingsoftheIEEE/CVFCon-
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
ferenceonComputerVisionandPatternRecognition,pp.
mon,S.,andPoole,B. Score-basedgenerativemodeling
14297–14306,2023.
throughstochasticdifferentialequations. arXivpreprint
Nguyen, T.A., Hsu, W.-N., d’Avirro, A., Shi, B., Gat, I.,
arXiv:2011.13456,2020.
Fazel-Zarani, M., Remez, T., Copet, J., Synnaeve, G.,
Vyas, A., Shi, B., Le, M., Tjandra, A., Wu, Y.-C., Guo,
Hassid,M.,etal. Expresso: Abenchmarkandanalysis
B.,Zhang,J.,Zhang,X.,Adkins,R.,Ngan,W.,Wang,
ofdiscreteexpressivespeechresynthesis. arXivpreprint
J., Cruz, I., Akula, B., Akinyemi, A., Ellis, B., Moritz,
arXiv:2308.05725,2023.
R., Yungster, Y., Rakotoarison, A., Tan, L., Summers,
Nocedal, J. and Wright, S. J. Numerical optimization. C., Wood, C., Lane, J., Williamson, M., and Hsu, W.-
Springer,1999. N. Audiobox: Unified audio generation with natural
languageprompts,2023.
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.
Librispeech:Anasrcorpusbasedonpublicdomainaudio Watson,D.,Chan,W.,Ho,J.,andNorouzi,M.Learningfast
books. InternationalConferenceonAcoustics, Speech samplersfordiffusionmodelsbydifferentiatingthrough
andSignalProcessing,2015. samplequality. InInternationalConferenceonLearning
Representations,2021.
Pokle, A., Muckley, M. J., Chen, R. T., and Karrer, B.
Training-free linear image inversion via flows. arXiv Yariv,L.,Puny,O.,Neverova,N.,Gafni,O.,andLipman,Y.
preprintarXiv:2310.04432,2023. Mosaic-sdffor3dgenerativemodels,2023.
10Yin,T.,Gharbi,M.,Zhang,R.,Shechtman,E.,Durand,F.,
Freeman, W. T., and Park, T. One-step diffusion with
distributionmatchingdistillation,2023.
Zhang, Q. and Chen, Y. Fast sampling of diffusion
models with exponential integrator. arXiv preprint
arXiv:2204.13902,2022.
Zhang,Q.andChen,Y. Fastsamplingofdiffusionmodels
withexponentialintegrator,2023.
Zheng,Q.,Le,M.,Shaul,N.,Lipman,Y.,Grover,A.,and
Chen,R.T. Guidedflowsforgenerativemodelingand
decisionmaking. arXivpreprintarXiv:2311.13443,2023.
11A.BNSOptimization
PropositionA.1. Foreveryupdaterule(c ,d )∈Ri+1×Ri+1ofanNSsolversthereaexistapair(a ,b )∈R×Ri+1
i i i i
sothattheupdaterulecanbeequivalentlywrittenas
x =x a +U b . (11)
i+1 0 i i i
Furthermore,ifthecolumnsofU arelinearlyindependentthenthepair(a ,b )isunique.
i i i
Proofofproposition3.1. Toprovethepropositionweuseinductiononthestepnumber0≤i≤n−1,whereourinduction
hypothesisisthepropositionitself. Remember,anUpdateruleofaNSsolverrepresentedby(c ,d )∈Ri+1×Ri+1is
i i
x =X c +U d (23)
i+1 i i i i
i i
(cid:88) (cid:88)
= (c ) x + (d ) u . (24)
i j j i j j
j=0 j=0
First,forthebasecasei=0,both(c ,d )∈R×Rand(a ,b )∈R×R,hencewecantakea =c andb =d . Let
0 0 0 0 0 0 0 0
k <n−1,weassumethehypothesisistrueforeveryi≤k. Thenwecanwritethekthstepas
k k
(cid:88) (cid:88)
x = (c ) x + (d ) u (25)
k+1 k j j k j j
j=0 j=0
k−1 k
(cid:88) (cid:88)
=(c ) x + (c ) x + (d ) u (26)
k 0 0 k j+1 j+1 k j j
j=0 j=0
k−1 (cid:32) j (cid:33) k
(cid:88) (cid:88) (cid:88)
=(c ) x + (c ) a x + (b ) u + (d ) u (27)
k 0 0 k j+1 j 0 j l l k j j
j=0 l=0 j=0
 
k−1 k−1 j k
(cid:88) (cid:88) (cid:88) (cid:88)
=(c k) 0+ (c k) ja jx 0+ (c k)
j+1
(b j) lu l+ (d k) ju
j
(28)
j=0 j=0 l=0 j=0
 
k−1 k−1k−1 k
(cid:88) (cid:88)(cid:88) (cid:88)
=(c k) 0+ (c k) ja jx 0+ (c k) j+1(b j) lu l+ (d k) ju
j
(29)
j=0 l=0 j=l j=0
 
k−1 k−1k−1 k
(cid:88) (cid:88)(cid:88) (cid:88)
=(c k) 0+ (c k) ja jx 0+ (c k) l+1(b l) ju
j
+ (d k) ju
j
(30)
j=0 j=0 l=j j=0
k
(cid:88)
=a x + (b ) u (31)
k 0 k j j
j=0
whereinthe2nd equalitywemadetheshiftj (cid:55)→ j +1,inthe3rd equalitywesubstitute(a ,b )fori ≤ k givenbyour
i i
inductionassumption,inthe5thequalitywechangedtheorderofsummationtofirstsumonj indexandthenonl,inthe6th
equalityweonlyswitchedthenotationoflandj indices,finallyinthelastequalitywedefine
 
k−1 k−1
(cid:88) (cid:88)
a
k
=(c k) 0+ (c k) ja j, (b k)
j
= (c k) l+1(b l)
j
+(d k) j,j =0,...k−1, (b k)
k
=(d k) k. (32)
j=0 l=j
Note,ifthevectorsx ,u ,...,u arelinearlyindependentthentheabovecoefficients,a ,(b ) ,...,(b ) areunique.
0 0 k k k 0 k k
B.UniversalityOfNon-StationarySolvers
Thisappendixprovidesaproofoftheorem3.2anditsvisualizationintheVendiagraminfigure3. Westatethefirstpartof
thetheoreminlemmaB.1andprovideastand-aloneproofofthelemma. ThenusinglemmaB.1wecompletetheproofof
theorem3.2.
12LemmaB.1. TheRunge-Kutta(RKandExponential-RK)familyisincludedintheScale-TimeRKfamily,whiletheMultistep
family(MultistepandExponential-Multistep)isincludedintheScale-Timemultistepfamily.
ProofoflemmaB.1. Remeber,givenapairofScale-Time(ST)transformation(s ,t )andagenericsolver,itsassociated
r r
solverintheSTsolverfamilyisdefinedasanapproximationusingthegenericsolvertotheexactsolutionasinequation15
forthetransformedVF,
(cid:90) ri+1
x¯(r )=x¯(r )+ u¯ (x¯(r))dr, (33)
i+1 i r
ri
wherex¯(r)andu¯ (x)areasinequations6and7(resp.). FirstconsidertheidentitytransformationastheSTtransformation,
r
thatis
s =1, t =r, (34)
r r
thenequation33coincideswithequation15. Inthiscase,applyingthegenericsolvergivesthesolveritself,henceMultistep
familyisincludedintheSTMultistepfamilyandRKfamilyisincludedinSTRKfamily. Next,weconsideranExponential
Integrator,itisdefinedasanapproximationtotheexactsolution(Luetal.,2023;2022b)asinequation22,
x(t )=
ψ
ti+1x(t )+ηψ
(cid:90) λti+1
eηλf (x(λ))dλ, (35)
i+1 ψ i ti+1 λ
ti λti
whereλ =logsnr(t)andf =f ,wheret istheinverseofλ whichisdefinedsinceweassumeλ ismonotonically
t λ tλ λ t t
increasing,andψ andηaredependentontheschedulerandtheobjectivef,
t
(cid:40)
(α ,−1) iff isϵ-pred
(ψ ,η)= t . (36)
t
(σ ,1) iff isx-pred
t
Hence,itisenoughtoshowthereexistaSTtransformation(s ,t ),suchthatequation35andequation33coincide. As
r r
mentioninsection3.3inequation21weconsiderthechangeofschedulerto
1 1
α¯ = α , σ¯ = σ . (37)
t ψ r r ψ r
r r
Byequation8itscorrespondingSTtransformationis
1
s = , t =r, (38)
r ψ r
r
andbyequations6andequation7thetransformedtrajectoryandVFare
x(r) ψ˙ 1
x¯(r)= , u¯ (x)=− rx+ u (ψ x). (39)
ψ r ψ ψ r r
r r r
Foranobjectivef eitherϵ-predictionorx-prediction,theVFuisasinequation5,
ψ˙ L
u (x)= rx+η rf (x) (40)
r ψ ψ r
r r
whereL =σ α˙ −σ˙ α . Finally,substituteequations39and40intoequation33gives
r r r r r
x
ri+1 =
x
ri
+η(cid:90) ri+1 L
rf (x)dr (41)
ψ ψ ψ2 r
ri+1 ri ri r
x (cid:90) ri+1 d 1 (cid:18) α (cid:19)η
= ri +η r f (x)dr (42)
ψ drη σ r
ri ri r
= x ri +(cid:90) ri+1 d (cid:0) eηλr(cid:1) f (x)dr (43)
ψ dr r
ri ri
x (cid:90) ri+1 dλ
= ri + η eηλrf (x)dr (44)
ψ dr r
ri ri
=
x
ri
+η(cid:90) λri+1
eηλf (x)dλ, (45)
ψ λ
ri λri
13(cid:16) (cid:17)η
whereinthe2nd equalitywenoticethat L ψ rr 2 = dd rη1 α σrr ,inthe3rd equalitywesubstituteλ r = log(α r/σ r),inthe4th
equalityusedthechainruletodifferentiatew.r.t. r,inthe5thequalitywechangedtheintegrationvariabletoλ,andmultiplied
bothsidesbyψ givesequation35.
ri+1
Wearereadyprovethemaintheorem:
Theorem3.2(SolverTaxonomy). TheRunge-Kutta(RKandExponential-RK).familyisincludedintheScale-TimeRK
family,whiletheMultistepfamily(MultistepandExponential-Multistep)isincludedintheScale-Timemultistepfamily. The
Scale-TimefamilyisincludedintheNon-Stationarysolversfamily.
Proofoflemma3.2. BylemmaB.1itislefttoshowthattheNon-Stationary(NS)solversfamilyincludestheScale-Time
(ST)solversfamily. Remember,givenanSTtransformation(s ,t ),itsassociatedsolverisdefinedasanapproximation
r r
usingagenericsolvertotheexactsolutionasinequation15forthetransformedVF.Thatis,
(cid:90) ri+1
x¯(r )=x¯(r )+ u¯ (x¯(r))dr, (46)
i+1 i r
ri
wherex¯(r)andu¯ (x)areasinequations6and7(resp.),andthegenericsolversweconsiderareeitheraMultisteporRK
r
method. Notebyequations53,54,and55theupdaterulesofbothMultistepandRKmethodsareexpressedasalinear
combinationofx andu ,hencetheyareincludedintheNSsolverfamily. Thatis,foreverysuchgenericsolverwithn
i i
stepsthereexists,a¯ ,¯b ∈ Ri+1, i = 0,...,n−1, andadiscretization0 = r ,r ,...,r = 1suchthattheSTsolver
i i 0 1 n
updateruleis
i i
x¯
=(cid:88)
a¯ x¯
+(cid:88)¯b
u¯ (x¯ ). (47)
ri+1 ij rj ij rj rj
j=0 j=0
Wesubstitutethedefinitionofx¯(r),u¯ (x),anddividebothsidesofequation47bys =s ,
r i+1 ri+1
x =(cid:88)i a¯ ijs ix +(cid:88)i ¯b ij (cid:0) s˙ x +t˙ s u (x )(cid:1) (48)
ti+1 s tj s j tj j j tj tj
i+1 i+1
j=0 j=0
=(cid:88)i (cid:18) a¯ ijs
i +
¯b ijs˙ j(cid:19)
x
+(cid:88)i ¯b
ij u (x ) (49)
s s tj s t˙ s tj tj
j=0 i+1 i+1 j=0 i+1 j j
i i
(cid:88) (cid:88)
= a x + b u (x ), (50)
ij tj ij tj tj
j=0 j=0
wherewedenotedt =t andweset
rj j
(cid:18) a¯ s ¯b s˙ (cid:19) ¯b
a = ij i + ij j , b = ij . (51)
ij s s ij s t˙ s
i+1 i+1 i+1 j j
C.Genericsolvers
Adam-Bashforth and Multistep solvers. The Adam-Bashforth (AB) solver is derived by replacing u (x(t)) in the
t
integralinequation15withaninterpolationpolynomialq(t)constructedwiththempreviousdatapoints(t ,u ),
i−m+j i−m+j
j =1,...,m. Integratingq(t)over[t ,t ]leadstoanm-stepAdam-Bashforth(AB)updateformula:
i i+1
m
(cid:88)
x =x +h b u , (52)
i+1 i−m+1 j i−m+j
j=1
14where h = t − t . A general (stationary) m-step Multistep method is defined with the more general update rule
i+1 i
incorporatingarbitrarylinearcombinationsofpreviousx ,u :
i i
m m
(cid:88) (cid:88)
x = a x +h b u , (53)
i+1 j i−m+j j i−m+j
j=1 j=1
wherea ,b ∈R, j =0,...,m−1areconstants(i.e.,independentofi).
j j
Runge-Kutta. Thisclassofsolversapproximatestheintegralofu (x(t))inequation15withaquadratureruleusing
t
interiornodesintheinterval[t ,t ]. Namely,itusesthedatapointsthedatapoints(t +hc ,u (ξ )),wherec
i i+1 i j ti+hcj j j
definetheRKnodes,andξ ≈x(t +hc ),j =0,...,m−1. Thisleadstoanupdateruleoftheform
j i j
m−1
(cid:88)
x =x +h b u (ξ ), (54)
i+1 i j ti+hcj j
j=0
(cid:40)
x j =0
ξ = i , (55)
j
x
+h(cid:80)j−1a
u (ξ ) j >0
i k=0 jk ti+hck k
wherethematrixa∈Rm×m witha =0forj ≤kiscalledtheRKmatrix,andb∈Rm istheRKweightvector,both
jk
independentofi,i.e.,stationary.
D.Experiments
D.1.BespokeNon-Stationarytrainingdetails
InthissectionweprovidethetrainingdetailsoftheBNSsolversforthethreetasks: (i)classconditionalimagegeneration,
(ii)Text-to-Imagegeneration,(iii)Text-to-Audiogeneration. Foralltaskstrainingsetandvalidationsetweregenerateusing
usingadaptiveRK45solver,optimizationisdonewithAdamoptimizer(Kingma&Ba,2017)andresultsarereportedon
bestvalidationiteration.
Classconditionimagegeneration. Forthistaskwegenerated520pairsof(x ,x(1)),noiseandimage,forthetraining
0
set,and1024suchpairsforthevalidationset. Foreachmodelonthistask,ImageNet-64eps-VP/FMv-CM/FM-OT,and
ImageNet-128FM-OT,wetrainBNSsolverswithNFE∈ {4,6,8,10,12,14,16,18,20}. Weusewithlearningrateof
5e−4,apolynomialdecaylearningratescheduler,batchsizeof40,for15kiterations. WecomputePSNRonthevalidation
setevery100iterations.
Text-to-Image. Forthistask,wegeneratetwotrainingandvalidationsetsof520and1024pairs(resp.),oneforguidance
scalew =2.0andoneforw =6.5. ThetextpromptsforthegenerationweretakenfromthetrainingsetofMS-COCO(Lin
etal.,2015).ForeachguidancescalewetrainBNSsolverswithNFE∈{12,16,20},learningrateof1e−4,cosineannealing
learningratescheduler,batchsizeof8,for20kiterations. WecomputePSNRonthevalidationsetevery200iterations.
Text-to-Audio. Wegenerateatrainingsetof10kpairsandavalidationsetof1024pairs. WetrainBNSsolverwithNFE
∈{8,12,16,20},andoptimizewithlearningrateof1e−4,cosineannealinglearningratescheduler,batchsizeof40,for
15kiterations. WecomputeSNRonthevalidationsetevery5kitrations.
D.2.Classconditionimagegeneration
Dataset and implementation details. As recommended by the authors (ima) we used the official face-blurred data.
Specifically,forthe64×64wedownsampleusingtheopensourcepreprocessingscriptsfrom(Chrabaszczetal.,2017)). For
ImageNet-64weusethreemodelsasdescribedinAppendixE,whileforImageNet-128weonlyuseFM-OTmodeldueto
computationalconstraints(trainingrequirescloseto2000NVidia-V100GPUdays).
D.3.Text-to-Image
In this section we compare our BNS solver with the initial solver. That is, the solver used in initialization of BNS
optimization. Table5showsPSNR,PickScore,ClipScore,andFIDoftheinitialsolver-RK-Eulerwithpreconditioning
15GT NFE=20 NFE=16 NFE=12 GT NFE=20 NFE=16 NFE=12
”acowissittingaloneonagrassyfield.” ”ateddybearsittinginafakebathtubwitharubberducky.”
”adogisonthefloorhidingunderacurtain.” ”abrownbearwalkingthroughalushgreenforest.”
”akitchenthathasabowloffruitonthetable.” ”pandabearsittingintreewithnoleaves.”
Figure7: ComparisonofgeneratedImagesonlatentFM-OTtext-to-image512x512guidancescale2.0: RK-Midpoint,
InitialSolver(RK-Euler+preconditionσ =5),andBNS.
0
16
tniopdiM-KR
revloSlaitinI
SNB
tniopdiM-KR
revloSlaitinI
SNB
tniopdiM-KR
revloSlaitinI
SNBGT NFE=20 NFE=16 NFE=12 GT NFE=20 NFE=16 NFE=12
”acowissittingaloneonagrassyfield.” ”sunflowersinaclearglassvaseonadesk.”
”ateddybearsittinginafakebathtubwitharubberducky” ”abuildingisshownbehindtreesandshrubs.”
”adogisonthefloorhidingunderacurtain.” ”akitchenthathasabowloffruitonthetable.”
Figure8: ComparisonofgeneratedImagesonlatentFM-OTtext-to-image512x512guidancescale6.5: RK-Midpoint,
InitialSolver(RK-Euler+preconditionσ =10),andBNS.
0
17
tniopdiM-KR
revloSlaitinI
SNB
tniopdiM-KR
revloSlaitinI
SNB
tniopdiM-KR
revloSlaitinI
SNBGT NFE=14 NFE=12 NFE=10 NFE=8 NFE=6 GT NFE=14 NFE=12 NFE=10 NFE=8 NFE=6
Figure9:ComparisonofgeneratedImagesonImageNet-128FM-OTmodelwithguidancescale0.5:(toprow)RK-Midpoint,
(middlerow)BST,(bottomrow)BNS.
18GT NFE=14 NFE=12 NFE=10 NFE=8 NFE=6 GT NFE=14 NFE=12 NFE=10 NFE=8 NFE=6
Figure10:ComparisonofgeneratedImagesonImageNet-64ϵ-VPmodelwithguidancescale0.2:(toprow)DDIM,(middle
row)DPM++(2M),(bottomrow)BNS.
ImageNet64 NFE: 4 6 8 10 12 14 16 18 20 GT
FM-OT PSNR: 25.08 29.9 34.25 37.92 41.06 43.52 45.64 47.12 48.33 ∞
FID: 27.35 6.02 3.11 2.27 1.91 1.83 1.78 1.72 1.75 1.68
FMv-CS PSNR: 25.0 29.76 34.03 37.7 40.83 43.3 45.21 46.68 47.68 ∞
FID: 27.59 6.05 3.14 2.4 1.89 1.82 1.76 1.74 1.72 1.71
ϵ-VP PSNR: 24.65 29.49 33.77 37.48 40.61 43.21 45.7 47.32 48.57 ∞
FID: 30.0 7.21 3.61 2.88 2.22 1.97 1.94 1.97 2.04 1.84
ImageNet128 NFE: 4 6 8 10 12 14 16 18 20 GT
FM-OT PSNR: 23.43 27.72 31.41 34.54 37.37 39.56 41.28 42.38 42.88 ∞
FID: 36.17 8.93 4.53 2.91 2.48 2.38 2.26 2.28 2.17 2.16
Table4: PSNRandFIDofBNSonImageNet64FM-OT/FMv-CS/ϵ-VP,andImageNet128FM-OT.
19ImageNet-64:FM-OT
50 10
BST-PSNR BST-PSNR
9
45 BNS-PSNR BNS-PSNR
8
40 7
6
35
5
30 4
3
25
2
20 1
4 6 8 10 12 14 16 18 20 4 6 8 10 12 14 16 18 20
NFE NFE
Figure11: BNSvs.BSTonImageNet-64FM-OTtrainedwithPSNRloss.
w=2.0 NFE PSNR PickScore ClipScore FID
GT(DOPRI5) 170 ∞ 20.95 0.252 15.20
InitialSolver 12 19.23 20.65 0.257 21.14
16 20.55 20.78 0.256 18.19
20 21.60 20.85 0.256 16.96
BNS 12 25.86 20.83 0.252 13.93
16 29.13 20.90 0.252 14.48
20 31.78 20.91 0.252 14.68
w=6.5 NFE PSNR PickScore ClipScore FID
GT(DOPRI5) 268 ∞ 21.16 0.260 23.99
InitialSolver 12 17.21 20.69 0.264 29.63
16 18.38 20.87 0.263 28.02
20 19.29 20.97 0.262 27.15
BNS 12 18.94 20.92 0.261 20.67
16 21.23 21.03 0.260 21.93
20 23.27 21.09 0.259 22.56
Table5: BNSsolversvs.GTandIntialSolver(Euler+ST)onText-to-Image512FM-OTevaluatedonMS-COCO.
σ =5forguidancescalew =2andσ =10forguidancescalew =6.5,andtheBNSsolvers.
0 0
D.4.BespokevsDistillation
WecompareBespokewithProgressiveDistillation(PD)(Salimans&Ho,2022),forCIFAR10wecompareagainstresults
reportedby(Salimans&Ho,2022)andforImageNet64againstresultsreportedby(Mengetal.,2023). Tocountthe
numberofforwardsinthenetworkthatwasdoneintrainingofBespokeorPD,wecountaforwardinthemodelwitha
batchof1asoneforward. ThetrainingofPDforCIFAR10modelwith8and4stepsdoneby(Salimans&Ho,2022)with
500kand550kparametersupdates(reps.),eachupdatecomputedabatchof128imagesandrequirestwoevaluationof
theteachermodelandoneevaluationofstudentmodel,whichsumsto192mand211mforwards(resp.) Thetrainingof
BespokeforCIFAR10with8and4stepswasdonewith30k parametersupdatesandbatchof40forboth,eachupdate
requires8and4evaluationofthemodel(resp.). ForBespokewealsotakeinaccountthecostofgeneratingthetrainingset
thatcost85kforwards,whichintotalsumsto9.7mand4.9mforwards(resp.). ForImageNet64wecompareagainstthe
unguidedsingle-wmodeltrainedby(Mengetal.,2023). ThePDtrainingofthismodelwith16,8,and4stepsisdonewith
300k,350k,and400kparametersupdates(resp.) andabatchof2048,takinginaccountbothteacherandstudentmodels
evaluationgives1843m,2150m,and2457mforwards(resp.). TheBespoketrainingfor16,8,and4wasdonewith15k
parametersupdatesandabatchof40,eachupdaterequires16,8,and4evaluationofthemodel(resp.),andthecostof
generatingthetrainingsetis90kforwards,whichintotalsumsto9.7m,4.9m,and2.5mforwards(resp.).
D.5.AudioGeneration
Theaudiogenerationmodelwasevaluatedonthefollowingdatasets:
• LibriSpeech(test-clean): audiobookrecordingsthatarescriptedandrelativelyclean (Panayotovetal.,2015)
20
RNSP
DIFSpotify Expresso Librispeech
Fisher
Accent Switchboard CommonVoicev13.0
Figure12: NFEvs. SNRforeachsolver
• CommonVoice v13.0: sentences read by volunteers worldwide. Covers a broader range of accents and are nosier
comparedtoLibriSpeech(Ardilaetal.,2019)
• Switchboard: aconversationalspeechcorpus (Godfreyetal.,1992)
• Expresso: Amultispeakerexpressivespeechdatasetcovering7differentspeakingstyles. (Nguyenetal.,2023)
• Accent: Aninternalexpressiveandaccenteddataset.
• Audiocaps: AsubsetoftheAudioSetdataset. ContainssoundsourcedfromYouTubevideos. (Kimetal.,2019)
• Spotify: podcastrecordings(Cliftonetal.,2020)
• Fisher: conversationalspeechdata (Cieri,Christopher,etal. ,2004,2005)
Additionallyweevaluatethesolversusingworderrorrate(WER),andspeakersimilarity. ForWERthegeneratedaudiois
transcribedusingWhisper (Radfordetal.,2022)andthenWERiscomputedagainstthetranscriptusedtogeneratetheaudio.
WequantifythespeakersimilaritybyembeddingboththeaudiopromptandthegeneratedaudiousingWavLM-TDCNN
(Chenetal.,2022),andcomputethecosinesimilaritybetweentheembeddings. Ingeneralthesemetricsdonotaccurately
reflectthesamplequalityofdifferentsolvers. Ininstanceswhereasolvergeneratesalow-qualitysamplewequalitatively
findthatthespeakerstillsoundsthesameandtheaudioisintelligible,butthereareartifactsintheaudiosuchasstatic,
backgroundnoise,etc. whicharearenotquantifiedbyspeakersimilarityorWER.AscanbeseenfromTable6andTable7
thereislittlevarianceinthesemetricsacrosssolvers.
Conditioningoftheaudiomodel. Themodeltakesinthreetensors,allofthesamelength:anoisetensorandconditioning
whichisconstructedofamaskedEncodecfeaturesandframe-alignedtokenembeddings. Thesegetconcatenatedtogether
channel-wise,andinputtothemodeltoproducetheresultingEncodecfeaturesfortheentiresequence. Thisisthenfedto
theEncodecdecodertoproducethefinalwaveform.
E.Pre-trainedmodels
Inthissectiondescribethetrainingobjectivethatpre-trainedmodelweusedweretrainedwithandtheirschedulers. In
addition,weprovidearchitecturedetailsforourCIFAR10,ImageNet,andText-to-Imagemodels.
21Accent Audiocaps CV13 Expresso Fisher LS Spotify Switchboard
Solver NFE
BNS 8 0.662 0.391 0.611 0.608 0.586 0.735 0.540 0.610
12 0.662 0.396 0.610 0.607 0.588 0.734 0.541 0.611
16 0.661 0.396 0.608 0.604 0.587 0.731 0.539 0.610
20 0.661 0.397 0.608 0.604 0.588 0.732 0.540 0.610
BST 8 0.662 0.387 0.608 0.605 0.587 0.732 0.540 0.610
12 0.663 0.398 0.609 0.605 0.589 0.733 0.544 0.611
16 0.662 0.399 0.609 0.602 0.590 0.731 0.544 0.611
20 0.661 0.397 0.608 0.602 0.589 0.731 0.542 0.611
Euler 8 0.662 0.387 0.609 0.605 0.584 0.732 0.544 0.608
12 0.664 0.395 0.611 0.605 0.588 0.734 0.546 0.612
16 0.665 0.402 0.612 0.605 0.590 0.734 0.548 0.613
20 0.665 0.404 0.612 0.605 0.591 0.734 0.550 0.614
Midpoint 8 0.664 0.391 0.608 0.600 0.592 0.731 0.543 0.614
12 0.664 0.399 0.608 0.601 0.593 0.731 0.547 0.615
16 0.662 0.398 0.608 0.602 0.590 0.731 0.543 0.611
20 0.661 0.396 0.608 0.602 0.589 0.731 0.539 0.610
RK45 adaptive 0.661 0.396 0.608 0.602 0.588 0.730 0.538 0.610
Table6: Speakersimilarityforeachsolver(higherisbetter)
Accent Audiocaps CV13 Expresso Fisher LS LSTTS Spotify Switchboard
Solver NFE
BNS 8 0.86 3.98 3.04 3.11 7.71 3.01 3.12 3.48 11.02
12 0.98 3.61 3.38 3.05 7.66 3.41 3.23 2.60 11.39
16 1.02 3.69 3.10 3.09 7.75 3.16 3.25 2.59 12.54
20 1.07 3.56 3.07 3.21 7.87 3.27 3.33 2.58 10.50
BST 8 0.90 3.87 3.16 3.11 7.77 3.18 3.06 2.89 10.17
12 1.02 4.05 3.16 3.21 7.42 3.22 3.19 3.16 9.83
16 1.04 3.74 3.11 3.17 7.50 3.35 3.16 3.16 10.35
20 0.95 3.81 3.41 2.99 7.58 4.26 3.18 2.98 11.19
Euler 8 0.90 3.49 3.38 3.05 7.05 3.31 2.81 2.81 12.36
12 0.98 3.79 3.13 3.05 7.71 2.99 2.92 3.44 10.75
16 0.99 3.73 3.35 3.11 7.37 3.12 3.04 3.65 9.40
20 0.95 3.74 3.13 3.11 7.83 3.16 3.16 2.80 9.82
Midpoint 8 1.03 4.25 3.26 3.05 7.66 3.10 3.03 3.95 9.46
12 0.95 3.97 3.37 3.17 7.30 3.29 3.12 3.32 7.84
16 0.98 3.95 3.34 3.19 7.43 3.50 3.19 2.83 10.72
20 1.08 3.81 3.24 3.17 7.67 3.12 3.13 2.60 12.33
RK45 adaptive 1.04 3.76 3.43 3.13 7.67 3.27 3.31 2.88 10.75
Table7: WERforeachsolver(lowerisbetter)
22CIFAR10 ImageNet-64 ImageNet-128
FM-OT ϵ-VP;FM-OT;FM/v-CS FM-OT
Channels 128 196 256
Depth 4 3 2
Channelsmultiple 2,2,2 1,2,3,4 1,1,2,3,4
Heads 1 - -
HeadsChannels - 64 64
Attentionresolution 16 32,16,8 32,16,8
Dropout 0.3 1.0 0.0
EffectiveBatchsize 512 2048 2048
GPUs 8 64 64
Epochs 3000 1600 1437
Iterations 300k 1M 900k
LearningRate 1e-4 1e-4 1e-4
LearningRateScheduler constant constant PolyDecay
WarmupSteps - - 5k
P-Unconditional - 0.2 0.2
Guidancescale - 0.20(vp,cs),0.15(ot) 0.5
Totalparameterscount 55M 296M 421M
Table8: CIFAR10andImageNetPre-trainedmodels’hyper-parameters.
Trainingobejectiveandschedulers. TheFM-OTandFM/v-CSmodelwheretrainedwithConditionalFlowMatching
(CFM)lossderivedin(Lipmanetal.,2022). Thatis,
L (θ)=E ∥u (x ;θ)−(σ˙ x +α˙ x )∥2, (56)
CFM t,p0(x0),q(x1) t t t 0 t 1
wheretisuniformon[0,1],p (x )=N (x |0,I),q(x )isthedatadistribution,u isthenetwork,(α ,σ )isthescheduler,
0 0 0 1 t t t
andx =σ x +α x ∼p (x|x )asinequation3. TheFM-OTscheduleris
t t 0 t 1 t 1
α =t, σ =1−t, (57)
t t
andtheFM/v-CSscheduleris
π π
α =sin t, σ =cos t. (58)
t 2 t 2
Theϵ-VPmodelwastrainedonadifferentobjective,thenoisepredictionlossasin(Hoetal.,2020)and(Songetal.,2020)
withtheVPscheduler. Thatis,
L (θ)=E ∥ϵ (x ;θ)−x ∥2, (59)
noise t,p0(x0),q(x1) t t 0
wheret,p (x ),q(x ),x asabove,ϵ isthenetworkandtheVPscheduleris
0 0 1 t t
(cid:113)
α
t
=ξ 1−t, σ
t
= 1−ξ 12 −t, ξ
s
=e− 41s2(B−b)−1 2sb, (60)
whereB =20, b=0.1.
Architecturedetails. TheText-to-ImagemodelhasthesamearchitectureasusedbyDalle-2(Rameshetal.,2022)(2.2b
parameters)withthefollowingchanges: weusetheT5textencoder(Raffeletal.,2020),wehave4input/outputchannels,
finallywealsohaveanautoencoderwiththesamearchitectureofStableDiffusionautoencoder(Rombachetal.,2021). Our
CIFAR10,andclassconditionalImageNetmodelshavetheU-NetarchitectureasinDhariwal&Nichol(2021),withthe
hyper-parameterslistedinTable8.
23