VNLP: Turkish NLP Package
Melikşah Türker, Mehmet Erdi Arı, Aydın Han
VNGRS
YTÜTeknoparkB2103Davutpaşa,İstanbul,Turkey
{meliksah.turker,erdi.ari,aydin.han}@vngrs.com
Abstract
In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight,
production-ready,state-of-the-artNaturalLanguageProcessing(NLP)packagefortheTurkishlanguage. Itcontains
awidevarietyoftools,rangingfromthesimplesttasks,suchassentencesplittingandtextnormalization,tothe
moreadvancedones,suchastextandtokenclassificationmodels. Itstokenclassificationmodelsarebasedon
"Context Model", a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved
by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological
Analysis&DisambiguationandPart-of-SpeechTagging. Moreover,itcomeswithpre-trainedwordembeddingsand
correspondingSentencePieceUnigramtokenizers. VNLPhasanopen-sourceGitHubrepository,ReadtheDocs
documentation,PyPipackageforconvenientinstallation,Pythonandcommand-lineAPIandademopagetotestall
thefunctionality. Consequently,ourmaincontributionisacomplete,compact,easy-to-installandeasy-to-useNLP
packageforTurkish.
Keywords: Turkish NLP, Sentiment Analysis, Named Entity Recognition, Part-of-Speech Tagging, Spelling
Correction,DependencyParsing,SentenceSplitting,TextNormalization.
1. Introduction CLIAPIsthatallowintegrationwithothersystems.
Lastly,thereisademopage4wherethementioned
Although frequently considered a low-resource modelscanbetested.
language, Turkish Natural Language Processing
(NLP)researchhasrecentlyattractedmoreatten-
2. Related Work
tion (Safaya et al., 2022; Alecakir et al., 2022;
BaykaraandGüngör,2022;Çöltekinetal.,2023;
NLPresearchhasattractedasignificantamountof
Uskudarlietal.,2023). Despitethisincreasedat-
researchinthepastdecade. Thedevelopmentand
tention, there is a gap between research papers
publication of text-processing technologies have
andtheirinference-readytools. Inmostcases,the
taken a crucial role. Although it was developed
researchpaperisspecifictooneorafewNLPtasks
waybackin1997,democratizationofLongShort-
with a GitHub repository that allows reproducibil-
Term Memory(LSTM) (Hochreiter and Schmidhu-
ityofevaluationmetricsandcontainsopen-source
ber,1997)viaopen-sourceDeepLearningframe-
codes. However,thisisnowherenearacomplete,
works (Chollet et al., 2015; Abadi et al., 2015;
state-of-the-art,well-documented,lightweightand
Paszkeetal.,2019)hasmadeanimportantcon-
inference-readytool.
tributioninsurgeofNLPresearchbyloweringthe
Seeingthisgap,wepresentVNLPtobethesolu-
barrier to entry. Following the ideas proposed in
tion. VNLPcontainsawiderangeoftools,namely;
LSTM,theinventionofGatedRecurrentUnit(GRU)
SentenceSplitter,TextNormalizer,NamedEntity
allowedthereductionofthenumberofparameters
Recognizer,Part-of-SpeechTagger,Dependency
by about 25%, decreasing the computation cost
Parser,MorphologicalAnalyzer&Disambiguator
withoutsignificantperformanceloss.
and Sentiment Analyzer. Deep Learning models
Word embedding models like Word2Vec and
in VNLP are very compact and lightweight, rang-
GloVehavelaidthefoundationsfortransferlearn-
ingfrom2.3Mto5.7Mparameters. Moreover,we
ing and have been improved by their successor
releasethepre-trainedwordembeddingsandcorre-
FastText. Using these pre-trained word embed-
spondingSentencePieceUnigramtokenizersthat
dingsandtransferringtheknowledgeobtaineddur-
areusedbythesemodels.
ingtheirtrainingtothedownstreamtask’smodelim-
Thecodebaseiswell-structured,readableand
provedtheperformanceofNLPmodels. However,
comes with documentation hosted on Readthe-
thementionedwordembeddingmethodsarenot
Docs. 1. ThepackageisavailableonPyPi2 and
context-aware;thatis,eachword’sembeddingvec-
can be installed using pip. 3. It has Python and
torisstaticregardlessofthewordsitissurrounded
by in the downstream task. ELMo has come to
1https://VNLP.readthedocs.io/en/latest/
2https://pypi.org/project/vngrs-nlp/
3pip install vngrs-nlp 4https://demo.VNLP.io/
4202
raM
2
]LC.sc[
1v90310.3042:viXraFigure1: ContextModelconsistsof4components. WordModelthatprocessesthesubwordtokensina
word,LeftContextModelthatprocesseswordsintheleftcontextleft-to-right,RightContextModelthat
processeswordsintherightcontextright-to-leftandLeftContextTagModelthatprocessestheclassified
tagsintheleftcontext.
therescueandfurtherimprovedthetransferlearn- thoughthesetoolssolvethementionedproblems
ingwithcontext-awarewordembeddings. Frame- partially,thereisstillnocompleteNLPpackagefor
works like spaCy (Honnibal and Montani, 2017), Turkishthatisopen-source,well-documented,PyPi
NLTK(Birdetal.,2009)andgensim(Řehůřekand installableandcomeswithaneasy-to-useAPI.
Sojka,2010)allowedanyonetousethesenewtech-
nologiesbyofferingmethodsandpre-trainedmod-
elsthatarereadytouseinproductionenvironment 3. Functionality & Models
forinference. Huggingface(Wolfetal.,2019)and
BERTurk(Dbmdz,2023)filledthegapforhigher- 3.1. Sentence Splitter
leveltasksliketextandsentenceclassificationand
Sentencesplittingisthetaskofsplittingabulktext
providedstate-of-the-artresults.
intoseparatesentences. Althoughthislookstrivial,
Turkish NLP researchers have utilized these inordertoobtainawell-workingsentencesplitter,
methods to develop models for tasks such as thereareexceptionsthatmustbehandledcorrectly,
Morphological Disambiguation, Syntactic Pars- suchasnumbersandabbreviations.
ing,DependencyParsing,Part-of-SpeechTagging, For this task, we use the implementation of
NamedEntityRecognitionandSentimentAnalysis Koehn and Schroeder (Koehn, 2023), by simpli-
forTurkish. Often,theyweredevelopedasindivid- fyingthecodeforTurkishandexpandingitslexicon
ualmodelsthatsolveaspecificproblemandare ofabbreviations.
publishedasseparateresearchpapers. Ifoneis
lucky,thepaperwouldcontainalinktotheGitHub
repositorythathoststhecodetoreproducethere- 3.2. Normalizer
sults. RarelywouldtherepositorycontainaDocker
Normalizationisthetaskofstandardizingthetext
imageoraCLIAPItousethemodelforinference.
input, which can be in different forms as it may
Withtheattempttobringseparatemodelsand comefromvarioussourcessuchassocialmedia,
research directions for Turkish under a single customerfeedback,andnewsarticles.
banner, three recent works have been published.
Mukayese(Safayaetal.,2022)hasaimedtobethe
3.2.1. SpellingCorrection
benchmarkfordatasetsandtaskevaluation. Turk-
ishDelight(Alecakiretal.,2022)hasaimedtobring Spellingcorrectionisthetaskofdetectingandthen
severalmodelstogetherandservetheminademo correctingmisspelledandmistypedwords. VNLP
page. TULAP(Uskudarlietal.,2023)hasaimedto uses Jamspell (Ozinov), a spell-checking library
opensourceTurkishNLPresourcesdevelopedat writteninC++. Itwaschosenoverotherlibraries
BoğaziçiUniversity,offeringademopage,hosting asitisfasterandproduceslowererrorrates. Jam-
datasetsandDockerimagestoallowinference. Al- spellusesadjacentwordstocorrectspellingerrors,Accuracy
Dataset AmbiguousWords AllWords
TrMorph2006(YuretandTure,2006) 94.67 96.64
TrMorph2018(Dayaniketal.,2018) 93.76 95.35
Table1: Stemmer: MorphologicalAnalyzer&Disambiguator
Dataset Accuracy F1Macro
WikiAnn(Panetal.,2017) 98.80 98.14
Gungor(Güngöretal.,2018) 99.70 98.59
TEGHub(Teghub,2023) 99.74 98.91
Table2: NamedEntityRecognizer
which is the reason behind the lower error rates usesanimprovedversionofthestopwordlexicon
comparedtoalternatives. offeredintheZemberekpackage(Akin,2023).
Jamspellrequiresadictionaryofwordfrequen-
cies. Weuseacustomdictionaryfilegeneratedby
3.3.2. DynamicMethod
training on a mixed corpus consisting of OPUS-
100 (Zhang et al., 2020), Bilkent Turkish Writ- The dynamic method is the more advanced ver-
ings(Yilmaz,2018)andTEDTalks(Siarohinetal., sion,wherestopwordsaredetermineddepending
2021)datasets. Thesedatasetswerechosenover on the given corpus. The implemented method
others since they were observed to contain less determinesthestopwordsbylookingattheword
noise. frequencies(Saif et al., 2014) and their breaking
point(Satopaaetal.,2011)tosetathresholdoffre-
quenciesandconsiderthewordsabovethethresh-
3.2.2. Deasciifier
old as stopwords. Moreover, this method is lan-
Deasciificationistheprocessofconvertingatext
guage agnostic and can be used for all kinds of
inputwritteninASCII-onlycharacterstoitscorrect
textstoobtainthemostfrequentwords.
version in the target language. VNLP Deasciifer
convertsthetextwritteninanEnglishkeyboardto
thecorrectversionofhowitwouldbehaditbeen 3.4. SentencePiece Unigram Tokenizer
writtenusingaTurkishkeyboard. Itisdirectlytaken
Deep Learning models in VNLP use subword
fromSevinç’simplementation(Sevinc,2023).
tokens, tokenized by SentencePiece Unigram
Model (Kudo and Richardson, 2018) to process
3.2.3. NumbertoWord the text. SentencePiece Unigram Tokenizer is
Writtentextsmaycontainnumbersinbothnumeri- trained from scratch on a corpus of 10 GB Turk-
calandwrittenforms. Inordertostandardizethese, ish text, which consists of random samples from
onecouldseektoconvertnumberstowrittentext OSCAR(Abadjietal.,2022),OPUS(Zhangetal.,
forms. NumbertoWordfunctionimplementsthis. 2020)andWikipediadumpdatasets. Itcomesin2
Ontopofthese,theVNLPNormalizerclassof- sizes,withvocabularysizesof16,000and32,000.
fersmoretrivialfunctionsforlowercasing,punctua-
tionremovalandaccentmarkremoval.
3.5. Pre-trained Word Embeddings
VNLPofferspre-trainedwordembeddingsfortwo
3.3. Stopword Remover
typesoftokens.
Stopwordsarethewordsthattakeplaceinvirtually
anytextandprovidenocontextinformationalone, 1. TreebankWordembeddingsaretokenizedby
suchas"and","such",or"if". Whileworkingona NLTK’s(Birdetal.,2009)TreebankWordTok-
widevarietyofNLPtasks,onecanseektogetrid enizer.
ofthembeforefurtheranalysis. VNLPofferstwo
algorithmstogetridofTurkishstopwords. 2. SentencePieceUnigramembeddingsareto-
kenizedbySentencePieceUnigramTokenizer.
3.3.1. StaticMethod
Embeddingsforthesetokensaretrainedusing
Thestaticmethodistheconventionalmethodthat Word2Vec(Mikolovetal.,2013)andFastText(Bo-
containsapre-definedstaticstopwordlexiconand janowskietal.,2017)algorithmsimplementedby
removes these words from the text input. VNLP gensim(ŘehůřekandSojka,2010)framework.UniversalDependencies2.9(deMarneffeetal.,2021) LAS UAS
UD_Turkish-Atis 88.52 91.54
UD_Turkish-BOUN 67.64 78.15
UD_Turkish-FrameNet 81.12 92.30
UD_Turkish-GB 72.97 88.58
UD_Turkish-IMST 63.32 76.53
UD_Turkish-Kenet 68.80 83.51
UD_Turkish-Penn 70.72 85.24
UD_Turkish-PUD 61.31 74.77
UD_Turkish-Tourism 90.96 97.31
Table3: DependencyParser
3.5.1. Word2Vec 3.6. Context Model
Word2Vecembeddingsaretrainedforbothofthe
ContextModelisthebasearchitectureusedinsev-
tokenization methods mentioned above. Tree-
eraldeeplearningmodelsinVNLP.Itisinspiredby
bankWordtokenizedWord2Vecembeddingscome
Ref.(Shenetal.,2016)andconsistsof4inputcom-
in3sizes.
ponentsasshowninFig.1. TheyareLeftContext
WordModel,LeftContextTagModel,CurrentWord
• Large: vocabularysize: 128,000,embedding
ModelandRightContextWordModel,respectively.
dimension: 256
ThemodelinputistokenizedbySentencePiece
• Medium: vocabularysize: 64,000,embedding UnigramTokenizer,andcorrespondingpre-trained
dimension: 128 Word2Vecembeddingsarefedtothenetwork. It
processeseachwordonebyone,andeachword
• Small: vocabularysize: 32,000,embedding can be represented by multiple subword tokens.
dimension: 64 Forthisreason,themodelcontainsaWordRNN
Modelthatprocessesallsubwordtokensinaword
SentencePiece Unigram tokenized Word2Vec
and returns a single word embedding in the last
embeddingscomein2sizes:
time step. Word RNN Model is used by Current
Word, Left and Right Context Models and its pa-
• Large: vocabularysize: 32,000,embedding
rametersaresharedamongthem. LeftandRight
dimension: 256
Context Word Models process the words on left
fromlefttorightandthewordsonrightfromright
• Small: vocabularysize: 16,000,embedding
to left, respectively. Left Context Tag Model pro-
dimension: 128
cessestheclassificationresultsofpriorwords. In
The difference in vocabulary sizes of the two theend,4componentsproduce4embeddingvec-
tokenization methods is due to the fact that the tors,whichareconcatenatedandprocessedby2
Unigramtokenizerisneveroutofvocabularyand fullyconnectedlayersfollowedbyaclassification
32,000isareasonablesize,beingoftenusedfor headthatproducesclassificationlogits.
state-of-the-artmonolingualmodels(Dbmdz,2023; ThenetworkismadeofGRU(Choetal.,2014)
Raffeletal.,2019). cells,whichprovideacomputationadvantageover
conventionalLSTM(HochreiterandSchmidhuber,
1997) cells. Throughout this work, all RNNs are
3.5.2. FastText
madeofGRUcells.
FastTextembeddingsaretrainedforTreebankWord
The main advantage of the Context Model is it
tokenizedtokensonly. SimilartoWord2Vecconfig-
combinestheideaofauto-regressivesequence-to-
uration,theycomein3sizes.
sequencemodelswithtokenclassifierencoder-only
models. Thisisactuatedbytakingtheclassification
• Large: vocabularysize: 128,000,embedding
results of prior words on the left context as input
dimension: 256
whileclassifyingwordsinsteadofsubwords. This
schema has two benefits over BERT-based (De-
• Medium: vocabularysize: 64,000,embedding
vlin et al., 2018) encoder-only models. First, its
dimension: 128
auto-regressivestructureallowsfortakingtheclas-
• Small: vocabularysize: 32,000,embedding sificationresultsofearlierwordsintoaccount. Sec-
dimension: 64 ond,classifyingwordsinsteadoftokens/subwords
guaranteesthealignmentofwordsandtags.UniversalDependencies2.9(deMarneffeetal.,2021) Accuracy F1Macro
UD_Turkish-Atis 98.74 98.80
UD_Turkish-BOUN 87.08 78.84
UD_Turkish-FrameNet 95.09 90.39
UD_Turkish-GB 85.59 66.20
UD_Turkish-IMST 90.69 78.45
UD_Turkish-Kenet 91.94 87.66
UD_Turkish-Penn 94.52 93.29
UD_Turkish-PUD 83.87 65.59
UD_Turkish-Tourism 98.45 93.25
Table4: Part-of-SpeechTagger
Accuracy F1Macro
MixtureofDatasets 94.69 93.81
Table5: SentimentAnalyzer
3.7. Stemmer: Morphological Analyzer & 2006), TrMorph2016 (Yildiz et al., 2016) and Tr-
Disambiguator Morph2018(Dayaniketal.,2018)datasets.
Stemmingisthetaskofobtainingthestemsofthe
wordsinasentence,dependingonthecontext. It 3.8. Named Entity Recognizer
isusefultostandardizethetextinput,especiallyin
NamedEntityRecognition(NER)isthetaskoffind-
agglutinativelanguagessuchasTurkish.
ing the named entities in a sentence. Although
A morphological analyzer allows obtaining the
thereareseveralvariantsofentitiesandhowthey
stemandthemorphologicaltagsofagivenword.
arerepresented,VNLP’sNamedEntityRecognizer
However, it returns multiple candidates since the
allowsfindingPerson,LocationandOrganization
word may have multiple meanings depending on
entities in the given sentence using IO format. It
thecontextofthesentence. Seethetwoexamples
is based on the Context Model architecture 3.6,
below:
consists of 5.6M parameters and is trained on a
collectionofopen-sourceNERdatasets(Güngör
• Üniversitesınavlarınacanlabaşlaçalışıyorlardı.
et al., 2018; Tür et al., 2003; Küçük et al., 2016;
(Theywerestudyingreallyhardfortheuniver-
Teghub,2023;Küçüketal.,2014;Panetal.,2017;
sityentranceexams.)
Hu et al., 2020). TWNERTC (Sahin et al., 2017)
• Şimdibaştanbaşla. (Now,startover.) isalsoconsidered;however,itisexcludeddueto
beingtoonoisyandactuallydeterioratingthemodel
Inthefirstsentence,theword"başla"isanoun, performance.
meaning"hard",describingthestruggleofstudying
whileinthesecondsentence,theword"başla"is
3.9. Dependency Parser
theverb,meaning"start".
Hence, a morphological analyzer is context- DependencyParsingisthetaskofshowingthede-
agnostic and simply provides all of the potential pendenciesofwordsinasentence,alongwiththe
analysesorcandidates. Giventhecontextandthe dependency labels. An example can be seen in
potentialanalyses,amorphologicaldisambiguator Fig.2. VNLPDependencyParserisbasedonthe
selectsthecorrectanalysisresult. Stemmerclass ContextModelarchitecturewithaslightdifference
implementsShen’s(Shenetal.,2016)morpholog- in classification head and left context tag inputs.
ical disambiguation model with slight differences Thedifferencearisesfromthefactthatthemodel
thatresultfromadifferentmodelconfigandusing makestwoclassificationdecisionsforeachword,
GRUinsteadofLSTM.Stemmerconsistsof2.3M that is, arc (the index of the word it depends on)
parameters. ItusesYildiz’swork(Yildiz,2023)as andthedependencytag. Thisisimplementedbya
themorphologicalanalyzer. singlevectorintheclassificationheadwherethe
Then,havingamorphologicaldisambiguatoron first part of the vector represents the arc and the
topofamorphologicalanalyzerallowsfindingthe secondpartrepresentsthedependencytag. Con-
correctstemofaworddependingonthecontext. sequently,BinaryCrossEntropyisusedtotrainthe
Stemmer model utilizes the Turkish pre-trained model, as it is a multi-label classifier. It consists
Word2VecembeddingsdescribedinSection.3.5.1 of5.7Mparameters. ThemodelistrainedonUni-
and is trained on TrMorph2006 (Yuret and Ture, versalDependencies2.9(deMarneffeetal.,2021)Figure2: DependencyParserproducesarcsandlabelstoindicatetherelationsbetweenwords. Part-of-
speechtagsbelowareproducedbyPart-of-SpeechTagger.
dataset. 4. Results
ModelsaretrainedusingAdam(KingmaandBa,
2014) optimizer (ϵ = 1e−3) along with a linear
3.10. Part-of-Speech Tagger learningratedecayof0.95perepoch. Thenumber
ofepochsvariesfromtasktotaskandisdetermined
Part-of-speechtaggingisthetaskofassigningpart- accordingtovalidationloss.
of-speech tags to the words in a sentence, such Stemmer: is evaluated on the test splits of
as nouns, pronouns, verbs, adjectives, punctua- TrMorph2006 (Yuret and Ture, 2006) and Tr-
tion and so on. VNLP Part-of-Speech Tagger is Morph2018(Dayaniketal.,2018)datasets. Aword
alsobasedontheContextModelarchitectureand isambiguousifthemorphologicalanalyzerreturns
consistsof2.6Mparameters. several candidates as the result. Following the
Part-of-Speech Tagger is trained on Universal original work (Shen et al., 2016), Accuracy and
Dependencies2.9datasetaswell. F1Macroscoresarereportedforbothambiguous
wordsandallwordsinTable.1.
Named Entity Recognizer: is evaluated on
the test splits of WikiAnn (Pan et al., 2017),
3.11. Sentiment Analyzer Gungor (Güngör et al., 2018) and TurkishNER-
BERT(Teghub,2023)datasets. AccuracyandF1
SentimentAnalysisisthetaskofclassifyingatext MacroscoresarereportedinTable.2.
intopositiveornegative. Someofthetypicaluse Dependency Parser: is evaluated on the test
cases are understanding the sentiment of social splitsofUniversalDependencies2.9(deMarneffe
media text and measuring customer satisfaction etal.,2021)dataset. LabelledAttachmentScore
throughcommentsandfeedback. VNLPSentiment (LAS)andUnlabelledAttachmentScore(UAS)are
Analyzer implements a text classifier for this pur- reportedinTable.3.
pose. SimilartoothermodelsinVNLP,Sentiment Part-of-Speech Tagger: is evaluated on the
AnalyzerusesSentencePieceUnigramTokenized testsplitsofUniversalDependencies2.9(deMarn-
pre-trainedWord2Vecembeddingsandisbasedon effeetal.,2021)dataset. AccuracyandF1Macro
GRU.However,asitisatextclassifier,compared scoresarereportedinTable.4.
tothewordtaggermodelssofar,itsarchitectureis
Sentiment Analyzer: is evaluated on the test
different. ItusesastackofBidirectionalRNNsto
split of the combined dataset mentioned in 3.11.
processtheinputtokens,followedbyaGlobalAv-
Itisgeneratedbyscikit-learn’s(Pedregosaetal.,
eragePooling1Dandfullyconnectedlayerbefore
2011)traintestsplitfunctionwiththefollowingcon-
thefinalclassificationhead.
fig: test_size=0.10,random_state=0,shuffle=
SentimentAnalyzerconsistsof2.8Mparameters True,stratifybylabel. Thereasonwecreatedthe
andistrainedonalargemixofsocialmedia,cus- test split from scratch is due to the fact that the
tomercommentsandresearchdatasets(Basturk, compileddatasetdoesnotcontainanypre-defined
2023; Bilen, 2023; BİLEN and HORASAN, 2021; testsplitexceptforRef.(KöksalandÖzgür,2021).
KöksalandÖzgür,2021;Coskuner,2023;Gokmen, Insteadofevaluatingonthissmallsampleonly,we
2023;Guven,2023;Ozler,2023;Sarigil,2023;Sub- preferredevaluatingonalargerandmorediverse
asi,2023;Kahyaoglu,2023;Yilmaz,2023). testsetthatcomesfromvarioussources. AccuracyandF1MacroscoresarereportedinTable.5.
SpellingCorrector: isevaluatedon100random
samplestakenfromMyDearWatson(AmyJang,
2020) and TED Talks (Siarohin et al., 2021)
datasets. AlthoughitscoresanAccuracyof0.69
andaWordErrorRate(WER)of0.09onthissam-
ple, wereportthesenumbersforinformativepur-
posesonly,asitisanunder-developmentmodule
ofthepackage. Amorecomprehensivestudyfor
Spellingwillbeconductedlateron.
5. Conclusion
We presented VNLP in this work. It is the first
complete, open-source, production-ready, well-
documented,PyPiinstallableNLPlibraryforTurk-
ish. Itcontainsawiderangeoftools,includingboth
lowandhigh-levelNLPtasks. Implementeddeep
learningmodelsarecompactyetcompetitive. The
ContextModelpresentedinthisworkbringstwoad-
vantagesoverBERT-basedclassificationmodels
bytakingthepredictionresultsofearlierwordsinto
accountandguaranteeingtheword-tagalignments.
Hence,ourcontributionisawell-engineered,doc-
umented,easy-to-useNLPpackagebasedonits
novelContextModelarchitecture.6. Bibliographical References KyunghyunCho,BartVanMerriënboer,CaglarGul-
cehre,DzmitryBahdanau,FethiBougares,Hol-
gerSchwenk,andYoshuaBengio.2014. Learn-
ing phrase representations using rnn encoder-
Martín Abadi, Ashish Agarwal, Paul Barham, decoderforstatisticalmachinetranslation. arXiv
Eugene Brevdo, Zhifeng Chen, Craig Citro, preprintarXiv:1406.1078.
Greg S. Corrado, Andy Davis, Jeffrey Dean,
Matthieu Devin, Sanjay Ghemawat, Ian Good- François Chollet et al. 2015. Keras. https://
fellow, Andrew Harp, Geoffrey Irving, Michael keras.io.
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Çağrı Çöltekin, A Seza Doğruöz, and Özlem
Kaiser,ManjunathKudlur,JoshLevenberg,Dan-
Çetinoğlu.2023. Resourcesforturkishnatural
delionMané,RajatMonga,SherryMoore,Derek
Murray, Chris Olah, Mike Schuster, Jonathon language processing: A critical survey. Lan-
Shlens,BenoitSteiner,IlyaSutskever,KunalTal- guage Resources and Evaluation, 57(1):449–
488.
war,PaulTucker,VincentVanhoucke,VijayVa-
sudevan,FernandaViégas,OriolVinyals,Pete
Coskuner.2023. Yemeksepeticomments. Online;
Warden,MartinWattenberg,MartinWicke,Yuan
accessed07-Aug-2023.
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scalemachinelearningonheterogeneous Erenay Dayanik, Ekin Akyürek, and Deniz Yuret.
systems. Softwareavailablefromtensorflow.org. 2018. Morphnet: A sequence-to-sequence
modelthatcombinesmorphologicalanalysisand
JulienAbadji,PedroOrtizSuarez,LaurentRomary,
disambiguation. CoRR,abs/1805.07946.
and Benoît Sagot. 2022. Towards a Cleaner
Document-OrientedMultilingualCrawledCorpus. Dbmdz.2023. dbmdz/bert-base-turkish-uncased.
arXive-prints,pagearXiv:2201.06642. Online;accessed07-Aug-2023.
Akin.2023. Zemberek-nlp. Online;accessed07-
Marie-CatherinedeMarneffe,ChristopherD.Man-
Aug-2023.
ning, Joakim Nivre, and Daniel Zeman. 2021.
HuseyinAlecakir, NecvaBölücü, andBurcuCan.
UniversalDependencies.ComputationalLinguis-
2022. Turkishdelightnlp: A neural turkish nlp
tics,47(2):255–308.
toolkit. ACL.
JacobDevlin,Ming-WeiChang,KentonLee,and
KristinaToutanova.2018. Bert: Pre-trainingof
Phil Culliton Amy Jang, Ana Sofia Uzsoy. 2020.
deepbidirectionaltransformersforlanguageun-
Contradictory,mydearwatson.
derstanding. arXivpreprintarXiv:1810.04805.
Basturk. 2023. Yemeksepeti sentiment analysis.
Gokmen.2023. Turkishreviewsdataset. Online;
Online;accessed07-Aug-2023.
accessed07-Aug-2023.
BatuhanBaykaraandTungaGüngör.2022.Turkish
abstractivetextsummarizationusingpretrained OnurGüngör,SuzanÜsküdarlı,andTungaGüngör.
sequence-to-sequence models. Natural Lan- 2018. Improving named entity recognition by
guageEngineering,pages1–30. jointly learning to disambiguate morphological
tags. arXivpreprintarXiv:1807.06683.
Bilen. 2023. Duygu analizi veri seti. Online; ac-
cessed07-Aug-2023. Guven.2023. Turkishtweetsdataset. Online;ac-
cessed07-Aug-2023.
Burhan BİLEN and Fahrettin HORASAN. 2021.
Lstmnetworkbasedsentimentanalysisforcus- SeppHochreiterandJürgenSchmidhuber.1997.
tomer reviews. Politeknik Dergisi, 25(3):959– Long short-term memory. Neural computation,
966. 9(8):1735–1780.
StevenBird,EwanKlein,andEdwardLoper.2009. MatthewHonnibalandInesMontani.2017. spaCy
Natural language processing with Python: an- 2: NaturallanguageunderstandingwithBloom
alyzing text with the natural language toolkit. " embeddings,convolutionalneuralnetworksand
O’ReillyMedia,Inc.". incrementalparsing. Toappear.
PiotrBojanowski,EdouardGrave,ArmandJoulin, JunjieHu,SebastianRuder,AdityaSiddhant,Gra-
andTomasMikolov.2017. Enrichingwordvec- hamNeubig,OrhanFirat,andMelvinJohnson.
tors with subword information. Transactions 2020. Xtreme: A massively multilingual multi-
oftheassociationforcomputationallinguistics, taskbenchmarkforevaluatingcross-lingualgen-
5:135–146. eralization. CoRR,abs/2003.11080.Kahyaoglu.2023. twitter-sentiment-analysis. On- in Neural Information Processing Systems 32,
line;accessed07-Aug-2023. pages8024–8035.CurranAssociates,Inc.
DiederikPKingmaandJimmyBa.2014. Adam: A F. Pedregosa, G. Varoquaux, A. Gramfort,
methodforstochasticoptimization.arXivpreprint V. Michel, B. Thirion, O. Grisel, M. Blondel,
arXiv:1412.6980. P. Prettenhofer, R. Weiss, V. Dubourg, J. Van-
derplas,A.Passos,D.Cournapeau,M.Brucher,
Schroeder.Koehn.2023. Texttosentencesplitter.
M.Perrot,andE.Duchesnay.2011. Scikit-learn:
Online;accessed07-Aug-2023.
MachinelearninginPython. JournalofMachine
LearningResearch,12:2825–2830.
Dilek Küçük, Guillaume Jacquet, and Ralf Stein-
berger.2014.Namedentityrecognitiononturkish Colin Raffel, Noam Shazeer, Adam Roberts,
tweets. InProceedingsoftheNinthInternational KatherineLee,SharanNarang,MichaelMatena,
ConferenceonLanguageResourcesandEvalu- YanqiZhou,WeiLi,andPeterJ.Liu.2019. Ex-
ation(LREC’14),pages450–454. ploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. arXive-prints.
DilekKüçük,DoğanKüçük,andNursalArıcı.2016.
Anamedentityrecognitiondatasetforturkish. In Radim Řehůřek and Petr Sojka. 2010. Software
201624thSignalProcessingandCommunica- FrameworkforTopicModellingwithLargeCor-
tionApplicationConference(SIU),pages329– pora. In Proceedings of the LREC 2010 Work-
332.IEEE. shoponNewChallengesforNLPFrameworks,
pages 45–50, Valletta, Malta. ELRA. http:
TakuKudoandJohnRichardson.2018. Sentence-
//is.muni.cz/publication/884893/en.
piece: Asimpleandlanguageindependentsub-
word tokenizer and detokenizer for neural text AliSafaya,EmirhanKurtuluş,ArdaGoktogan,and
processing. arXivpreprintarXiv:1808.06226. DenizYuret.2022.Mukayese: Turkishnlpstrikes
back. InFindingsoftheAssociationforCompu-
AbdullatifKöksalandArzucanÖzgür.2021. Twit-
tationalLinguistics: ACL2022,pages846–863.
ter dataset and evaluation of transformers for
turkishsentimentanalysis. In202129thSignal H Bahadir Sahin, Caglar Tirkaz, Eray Yildiz,
Processing and Communications Applications Mustafa Tolga Eren, and Ozan Sonmez. 2017.
Conference(SIU). Automatically annotated turkish corpus for
named entity recognition and text categoriza-
TomasMikolov,KaiChen,GregCorrado,andJef-
tionusinglarge-scalegazetteers. arXivpreprint
frey Dean. 2013. Efficient estimation of word
arXiv:1702.02363.
representationsinvectorspace. arXivpreprint
arXiv:1301.3781. HassanSaif,MiriamFernandez,andHarithAlani.
2014. Onstopwords,filteringanddatasparsity
Filipp Ozinov. Jamspell.
forsentimentanalysisoftwitter. Proceedingsof
http://github.com/bakwc/JamSpell/. Archived:
the9thInternationalLanguageResourcesand
14-Mar-2023.
EvaluationConference(LREC’14),pages810–
817.
Ozler.2023. 5kturkishtweetswithincivilcontent.
Online;accessed07-Aug-2023.
Sarigil. 2023. Turkish sales comments. Online;
accessed07-Aug-2023.
XiaomanPan,BoliangZhang,JonathanMay,Joel
Nothman, Kevin Knight, and Heng Ji. 2017.
VilleSatopaa,JeannieAlbrecht,DavidIrwin,and
Cross-lingualnametaggingandlinkingfor282
BarathRaghavan.2011. Findinga"kneedle"in
languages. In Proceedings of the 55th Annual ahaystack: Detectingkneepointsinsystembe-
MeetingoftheAssociationforComputationalLin- havior. In201131stinternationalconferenceon
guistics(Volume1: LongPapers),pages1946– distributedcomputingsystemsworkshops,pages
1958.
166–171.IEEE.
Adam Paszke, Sam Gross, Francisco Massa,
Sevinc.2023. turkish-deasciifier: Turkishdeasci-
AdamLerer,JamesBradbury,GregoryChanan,
ifier. Online;accessed07-Aug-2023.
TrevorKilleen,ZemingLin,NataliaGimelshein,
Luca Antiga, Alban Desmaison, Andreas Kopf, Qinlan Shen, Daniel Clothiaux, Emily Tagtow,
Edward Yang, Zachary DeVito, Martin Raison, Patrick Littell, and Chris Dyer. 2016. The role
Alykhan Tejani, Sasank Chilamkurthy, Benoit ofcontextinneuralmorphologicaldisambigua-
Steiner,LuFang,JunjieBai,andSoumithChin- tion. InProceedingsofCOLING2016,the26th
tala. 2019. Pytorch: An imperative style, high- InternationalConferenceonComputationalLin-
performancedeeplearninglibrary. InAdvances guistics: TechnicalPapers,pages181–191.AliaksandrSiarohin,OliverJ.Woodford,JianRen,
MengleiChai,andSergeyTulyakov.2021. Mo-
tionrepresentationsforarticulatedanimation.
Subasi. 2023. turkish-tweets-sentiment-analysis.
Online;accessed07-Aug-2023.
Teghub.2023. Turkishnerdata3labels. Online;ac-
cessed07-Aug-2023.
GökhanTür,DilekHakkani-Tür,andKemalOflazer.
2003. A statistical information extraction sys-
temforturkish. NaturalLanguageEngineering,
9(2):181–210.
SusanUskudarli,MuhammetŞen,FurkanAkkurt,
Merve Gürbüz, Onur Güngör, Arzucan Özgür,
andTungaGüngör.2023. Tulap-anaccessible
andsustainableplatformforturkishnaturallan-
guageprocessingresources. InProceedingsof
the 17th Conference of the European Chapter
oftheAssociationforComputationalLinguistics:
SystemDemonstrations,pages219–227.
ThomasWolf,LysandreDebut,VictorSanh,Julien
Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan
Funtowicz,etal.2019. Huggingface’stransform-
ers: State-of-the-artnaturallanguageprocess-
ing. arXivpreprintarXiv:1910.03771.
Yildiz. 2023. Lookupanalyzerdisambiguator. On-
line;accessed07-Aug-2023.
ErayYildiz,CaglarTirkaz,H.Sahin,MustafaEren,
andOmerSonmez.2016. Amorphology-aware
networkformorphologicaldisambiguation. Pro-
ceedings of the AAAI Conference on Artificial
Intelligence,30(1).
Yilmaz.2023. Turkishsentimentanalysis. Online;
accessed07-Aug-2023.
Selim Fırat Yilmaz. 2018. Bilkent turkish writ-
ingsdataset.https://github.com/selimfirat/bilkent-
turkish-writings-dataset. Archived: 20-Oct-2020.
DenizYuretandFerhanTure.2006. Learningmor-
phological disambiguation rules for turkish. In
ProceedingsoftheHumanLanguageTechnol-
ogyConferenceoftheNAACL,MainConference,
pages328–334,NewYorkCity,USA.Associa-
tionforComputationalLinguistics.
BiaoZhang,PhilipWilliams,IvanTitov,andRico
Sennrich. 2020. Improving massively multilin-
gual neural machine translation and zero-shot
translation.