A Library of Mirrors: Deep Neural Nets in Low Dimensions
are Convex Lasso Models with Reflection Features
Emi Zeger emizeger@stanford.edu
Department of Electrical Engineering
Yifei Wang wangyf18@stanford.edu
Department of Electrical Engineering
Aaron Mishkin amishkin@cs.stanford.edu
Department of Computer Science
Tolga Ergen tergen@lgresearch.ai
LG AI Research
Emmanuel Candès candes@stanford.edu
Department of Statistics and Department of Mathematics
Mert Pilanci pilanci@stanford.edu
Department of Electrical Engineering
Stanford University, Stanford, CA 94305–2004, USA
Abstract
We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso
problemwithafixed,explicitlydefineddictionarymatrixoffeatures. Thespecificdictionary
depends on the activation and depth. We consider 2-layer networks with piecewise linear
activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree
networkswithsignactivationandarbitrarydepth. InterestinglyinReLUnetworks, afourth
layer creates features that represent reflections of training data about themselves. The
Lasso representation sheds insight to globally optimal networks and the solution landscape.
1 Introduction
Training deep neural networks is an important optimization problem. However, the non-
convexity of neural nets makes their training challenging. We show that for low-dimensional
data, e.g., 1-D or 2-D, training can be simplified to solving a convex Lasso problem with an
easily constructable dictionary matrix.
Neural networks are used as predictive models for low-dimensional data in acoustic
signal processing (Bianco et al., 2019; Freitag et al., 2017; Hsu and Jang, 2009; Mavaddati,
2020; Purwins et al., 2019; Serrà et al., 2019), physics-informed machine learning problems,
uncertainty quantification (Chen and Ghattas, 2020; Chen et al., 2019; Stuart, 2014; Wang
et al., 2022a,b; Zahm et al., 2022), and predicting financial data (Section 8).
In (Ergen and Pilanci, 2021a,b; Savarese et al., 2019), the problem of learning 1-D data
is studied for two-layer ReLU networks, and it is proved that the optimal two-layer ReLU
neural network precisely interpolates the training data as a piecewise linear function for
which the breakpoints are at the data points. Recent work in (Joshi et al., 2023; Karhadkar
1
4202
raM
2
]GL.sc[
1v64010.3042:viXraet al., 2023; Kornowski et al., 2023) also studied 2-layer ReLU neural networks and examined
their behavior on 1-D data.
On the other hand, the current literature still lacks analysis on the expressive power and
learning capabilities of deeper neural networks with generic activations. This motivates us to
study the optimization of two-layer networks with piecewise linear activations and deeper
neural networks with sign and ReLU activations. For 1-D data, we simplify the training
problem by recasting it as a convex Lasso problem, which is well studied (Efron et al., 2004;
Tibshirani, 1996, 2013).
Convex analysis of neural networks was studied in several prior works. As an example,
infinite-width neural networks enable the convexification of the overall model (Bach, 2017;
Bengio et al., 2005; Fang et al., 2019) . However, due to the infinite-width assumption,
these results do not reflect finite-width neural networks in practice. Recently, a series of
papers (Ergen and Pilanci, 2020, 2021a; Pilanci and Ergen, 2020) developed a convex analytic
framework for the training problem of two-layer neural networks with ReLU activation. As a
follow-up work, a similar approach is used to formulate the training problem for threshold
activations in general d ≥ 1 dimensions as a Lasso problem Ergen et al. (2023). However,
the dictionary matrix is described implicitly and requires high computational complexity
to create Ergen et al. (2023). By focusing on 1-D data, we provide simple, explicit Lasso
dictionaries and consider additional activations, including sign activation, which are useful in
many contexts such as saving memory to meet hardware constraints Bulat and Tzimiropoulos
(2019); Kim and Smaragdis (2018).
Throughoutthispaper, allscalarfunctionsextendtovectorandmatrixinputscomponent-
wise. We denote vectors as v = (v ,··· ,v ) and denote the set of column and row vectors
1 n
by Rn and R1×n, respectively. For L ≥ 2, an L-layer neural network for d-dimensional data
is denoted by f (x;θ) : R1×d → R, where x ∈ R1×d is an input row vector and θ ∈ Θ is a
L
parameter set. The set θ may contain matrices, vectors, and scalars representing weights
and biases, and Θ is the parameter space. We let X ∈ RN×d be a data matrix consisting of
N training samples x(1),...x(N) ∈ R1×d, and let y ∈ RN be a target vector. We consider
regression tasks, not classification. The (non-convex) neural net training problem is
1 β
min ∥f (X;θ)−y∥2+ ∥θ ∥L˜ (1)
θ∈Θ 2 L 2 L˜ w L˜
where β > 0 is a regularization coefficient for a subset of parameters θ ⊂ θ that incur
w
a weight penalty when training. We denote ∥θ ∥L˜ = (cid:80) |q|L˜, where S is the set of
w L˜ q∈Sw w
elements of all matrices/vectors/scalars in θ . L˜ is the effective regularized depth, defined
w
to be L for ReLU, leaky ReLU, and absolute value activations, and 2 for threshold and
sign activations. The effective regularized depth captures the idea that unlike for other
activations, for sign or threshold activation, only the weights of the final layer should be
regularized, since all other weights are passed through an activation that is invariant to their
magnitude (Remark 6, Appendix A.2).
In this paper we consider the Lasso problem
1
min ∥Az+ξ1−y∥2+β∥z∥ (2)
z,ξ 2 2 1
2where z is a vector, ξ ∈ R, 1 is a vector of ones, and β > 0. A is called the dictionary matrix
and is determined by the depth L and the activation of the neural net. The columns of A are
features A ∈ RN. The set of features is the dictionary. We call a collection of dictionaries
i
for the same activation and different depths a library.
A neural net is trained by choosing θ that solves (1), and the resulting neural net is
optimal. Unfortunately, this is complicated by the non-convexity of the neural net f (x,θ)
L
Pilanci and Ergen (2020). However, for data of dimension d = 1 we reformulate the training
problem (1) into the equivalent but simpler Lasso problem (2), where A is a fixed matrix
that is constructed based on the training data X. We explicitly provide the elements of A,
making it straightforward to build and solve the convex Lasso problem instead of solving the
non-convex problem (1). This reformulation allows for exploiting fast Lasso solvers such as
Least Angle Regression (LARS) Efron et al. (2004).
Whereas in the training problem (1), the quality of the neural net fit to the data is
measured by the l loss as 1||f (X;θ)−y||2, our results generalize to a wide class of convex
2 2 L 2
loss functions L : RN → R. With a general loss function, (1) becomes minL (f (X;θ))+
y y L
θ∈Θ
(β/2)∥θ ∥2. This is shown to be equivalent to the generalization of (2), namely minL (Az+
w 2 y
z,ξ
ξ1−y)+β∥z∥ .
1
The Lasso problem selects solutions z that generalize well by penalizing their total weight
in l norm Tibshirani (1996). The l norm typically selects a minimal number of elements
1 1
in z to be nonzero. The Lasso equivalence demonstrates that neural networks can learn a
sparse representation of the data by selecting dictionary features to fit y.
The Lasso representation also elucidates the solution path of neural networks. The
solution path for the Lasso or training problem is the map from β ∈ (0,∞) to the solution
set. The Lasso solution path is well understood (Tibshirani, 1996, 2013; Efron et al., 2004),
providing insight into the solution path of the ReLU training problem (Mishkin and Pilanci,
2023).
This paper is organized as follows. We define the neural networks under consideration in
Section 2. Section 3 describes our main theoretical result: neural networks are solutions to
Lasso problems. Section 4 then examines the relationship between the entire set of optimal
neural nets given by the training problem versus the Lasso problem. Section 5 applies our
theory to examine neural net behavior under minimum regularization by studying the Lasso
problem as β → 0. Section 6 applies our theory to examples to explicitly find optimal neural
networks. Section 7 presents experiments that support our theory in Section 3 and Section 5,
and shows examples where neural networks trained with ADAM naturally exhibit Lasso
features. Finally, Section 8 applies our theory to real-world data by predicting Bitcoin prices
with neural networks and demonstrating improved performance by using the Lasso problem.
1.1 Contributions
We show the following main results.
• Training various neural network architectures on 1-D data is equivalent to solving Lasso
problems with explicit, fixed and discrete dictionaries of basis signals that grow richer
with depth (Theorems 1 and 2). We identify these dictionaries in closed form for ReLU
and sign activations.
3• Features with reflections of training data appear in the ReLU library at depth 4. In
contrast, no reflection features are generated for the sign activation for any depth.
• Experimentally, training deep ReLU networks using the Adam optimizer leads to
the same reflection features and matches our theoretical results on the global optima
(Section 7).
• For certain binary classification tasks, the Lasso problem yields closed-form, optimal
neural networks with sign activation. In such tasks, we analytically observe that 3-layer
networks generalize better than 2-layer networks in that their predictions are more
uniform (Corollaries 3 and 4)
• After depth 3, the sign activation library freezes for parallel neural networks but grows
for tree-structured neural networks (Theorem 2).
• A similar convexification extends to 2-D data on the upper half plane (Theorem 3).
1.2 Notation
When the data dimension is d = 1, we assume x > x > ··· > x . For a logical
1 2 N
statement z, denote 1{z} as its indicator function. For n ∈ N, let [n] = {1,2,...,n}. For a
matrix Z, let Z be the submatrix of Z corresponding to indices in S. For a set of vectors
S
H, let [H] be a matrix whose columns are the elements of H. For a vector z, ||z|| is the
0
number of nonzero elements in z. Let e(n) ∈ RN be the nth canonical basis vector, that is
e(n) = 1{i = n}. Let 1 ,0 ∈ Rn be the all-ones and all-zeros vectors, respectively, and
i n n
without subscripts they are in RN.
2 Neural net architectures
This section is devoted to defining neural net terminology and notation to be used
throughout the rest of the paper. Let L ≥ 2 be the depth of a neural network (L−1 hidden
layers). The neural net activation σ : R → R is either the ReLU σ(x) = (x) := max{x,0},
+
absolute value σ(x) = |x|, leaky ReLU, threshold or sign function. For fixed slopes a,b ∈ R,
theleakyReLuisσ(x) = (a1{x > 0}+b1{x < 0})x. Thethresholdactivationisσ(x) = 1(x),
where 1(x) = 1{x ≥ 0}, and the sign activation is σ(x) = sign(x), where sign(x) is −1 if
x < 0, and 1 if x ≥ 0. Note sign(0) = 1. For Z ∈ Rn×m,s ∈ Rm, let σ (Z) = σ(Z)Diag(s).
s
When each column of σ(Z) is a neuron output, each column of σ (Z) ∈ RN×m is a neuron
s
scaled by an amplitude parameter s ∈ R. Amplitude parameters are (trainable) parameters
i
for sign and threshold activations, and ignored (even if written) for ReLU, leaky ReLU, and
absolute value activations.
Next we define some neural net architectures. The parameter set is partitioned into
θ = θ ∪θ ∪{ξ}, where θ is a set of internal bias terms, and ξ is an external bias term.
w b b
We will define the elements of each parameter set below. We will define neural nets by their
output on row vectors x ∈ R1×d. Their outputs then extend to matrix inputs X ∈ RN×d
row-wise.
42.1 Standard networks
The following is a commonly studied neural net architecture. Let L ≥ 2, the number of
layers. Let m = d,m = 1 and m ∈ N for l ∈ [L−2], which are the number of neurons
0 L−1 l
in each layer. For l ∈ [L−1], let W(l) ∈ Rm l−1×m l,s(l) ∈ Rm l,b(l) ∈ R1×m l,ξ ∈ R, which
are the weights, amplitude parameters, internal biases, and external bias, respectively. Let
X(1) = x ∈ R1×d be the input to the neural net and X(l+1) ∈ R1×m l be viewed as the inputs
to layer l+1, defined by
(cid:16) (cid:17)
X(l+1) = σ X(l)W(l)+b(l) . (3)
s(l)
Let α ∈ RmL, which is the vector of final layer coefficients. A standard neural
network is f (x;θ) = ξ + X(L)α. The regularized and bias parameter sets are θ =
L w
(cid:8) α,W(l),s(l) : l ∈ [L−1](cid:9) and θ = (cid:8) b(l) : l ∈ [L−1](cid:9).
b
There is much interest in analyzing the training problem for standard networks, but this
appears to be a challenging problem. However, by changing the architecture to a parallel
or tree structure defined below, we show that the training problem simplifies to the Lasso
problem. These alternative architectures allow neural nets to be reconstructed more tractably
from a Lasso solution than with a standard network. In the parallel and tree architectures,
m is the number of neurons in the final layer and for i ∈ [m ], we define the disjoint unions
L L
θ = (cid:91) θ(i) and θ = (cid:91) θ(i).
w w b b
i∈[mL] i∈[mL]
2.2 Parallel networks
A parallel network is a linear combination of standard networks in parallel, as we now
define. Each standard network is called a parallel unit. Let L ≥ 2,m = d,m = 1 and
0 L−1
m
l
∈ N for l ∈ [L]−{L−1}. For i ∈ [m L],l ∈ [L−1], let W(i,l) ∈ Rm l−1×m l, s(i,l) ∈ Rm l,
b(i,l) ∈ R1×m l,ξ ∈ R, which are the weights, amplitude parameters, and biases of the ith
parallel unit. Let X(i,1) = x ∈ R1×d be the input to the neural net and X(i,l+1) ∈ R1×m l be
viewed as the input to layer l+1 in unit i, defined by
(cid:16) (cid:17)
X(i,l+1) = σ X(i,l)W(i,l)+b(i,l) . (4)
s(i,l)
(cid:88)mL
Let α ∈ RmL. A parallel neural network is f L(x;θ) = ξ+ α iX(i,L). The regularized and
i=1
bias parameter sets are θ(i) = (cid:8) α ,s(i,l),W(i,l) : l ∈ [L−1](cid:9) ,θ(i) = (cid:8) b(i,l) : l ∈ [L−1](cid:9), for
w i b
i ∈ [m ]. A deep narrow network is a parallel neural net with m = ··· = m = 1. For
L 1 L−1
L ≥ 3, a rectangular network is a parallel network with m = ··· = m .
1 L−2
2.3 Tree networks
Let L ≥ 3,m ,··· ,m ∈ N. Given l ∈ {0,··· ,L−2}, let u be an l-tuple where if l = 0,
2 L
we denote u = ∅ and otherwise, u = (u ,··· ,u ) such that u ∈ [m ] for i ∈ [l]. For an
1 l i L−i
integer a, denote u⊕a as the concatenation (u ,··· ,u ,a). For l ∈ [L−1], and u of length
1 l
l, let α(u),s(u),b(u),w(u) ∈ R, except let w(u1,···,uL−1) ∈ Rd. For all u of length L−1, let
5X(u1,···,uL−1) = x ∈ R1×d. For u of length l ∈ {0,··· ,L−2}, let X(u) ∈ R be defined by
m
(cid:88)L−l (cid:16) (cid:17)
X(u) = α(u⊕i)σ X(u⊕i)w(u⊕i)+b(u⊕i) . (5)
s(u⊕i)
i=1
A tree neural network is f (x;θ) = ξ + X(∅). Visualizing the neural network as a tree,
L
X(∅) is the “root," u = (u ,···u ) specifies the path from the root at level 0 to the u th
1 l l
node (or neuron) at level l, X(u) represents a subtree at this node, and (5) specifies how
this subtree is built from its child nodes X(u⊕i). The leaves of the tree are all copies of
X(u1,···,uL−1) = X. Let U = (cid:81)L l=− 02[m L−l]. The regularized and bias parameter sets are
θ(i) = (cid:8) α(u),s(u),w(u) : u ∈ U,u = i(cid:9) ,θ(i) = (cid:8) b(u) : u ∈ U,u = i(cid:9). For tree networks, let
w 1 b 1
α = (cid:0) α(1),··· ,α(mL)(cid:1) ∈ RmL.
This paper primarily focuses on the parallel architecture. A parallel network can be
converted into a standard network as follows. Let W(1) = (cid:2) W(1,1)···W(m1,1)(cid:3). For l ≥ 1,
let b(l) = (cid:0) b(1,l)···b(m l,l)(cid:1). For l > 1, let W(l) = blockdiag(cid:0) W(1,l)···W(m l,l)(cid:1). And let
α,ξ be the same in the standard network as the parallel one.
While each unit of the parallel neural network is a standard network, every branch of the
tree network is a parallel network. Standard and parallel nets have the same architecture for
L = 2 layers, and parallel and tree nets are the same for L = 3 layers. For L ≥ 4 layers, the
architectures all diverge.
For all architectures, define parameter unscaling as follows. Parameter unscaling for
ReLU, leaky ReLU, or absolute value activation is the transformation q → sign(q)γ for
i
q ∈ θ w(i),andq → qγ
i
forq ∈ θ b(i),whereγ
i
= |α i|L1 ˜. Forsignandthresholdactivation, itisthe
transformation α → sign(α )(cid:112) |α | and s(i,L−1) → (cid:112) |α | for parallel nets, and s(i) → (cid:112) |α |
i i i i i
for tree nets. This will be used in reconstructing neural nets from Lasso solutions.
Henceforth, except for Section 3.2.1 and the Appendix, assume the data is in 1-D.
3 Main results
In this section, we show that non-convex deep neural net training problems are equivalent
to Lasso problems, that is, their optimal values are the same, and given a Lasso solution, we
can reconstruct a neural net that is optimal in the training problem.
3.1 Deep narrow networks
We reformulate the training problem for 2-layer networks with piecewise linear activations
and deeper networks with ReLU activation. Proofs are deferred to Appendix C. For a
piecewise linear function f : R → R, x is a breakpoint if f changes slope at x. We next define
some parameterized families of functions from R to R.
6a a a 1 a 2 a 1 a 2
ReLU+(x) ReLU−(x) Ramp+ (x) Ramp− (x)
a a a1,a2 a1,a2
Figure 1: Examples of capped ramp functions in Definition 1.
Definition 1 Let a ∈ [−∞,∞),a ∈ (−∞,∞]. The capped ramp functions are
1 2
 
0 if x ≤ a a −a if x ≤ a
 1  2 1 1
 
Ramp+ (x) = x−a if a ≤ x ≤ a , Ramp− (x) = a −x if a ≤ x ≤ a
a1,a2 1 1 2 a1,a2 2 1 2
 
a −a if x ≥ a 0 if x ≥ a ,
2 1 2 2
provided that a ≤ a , and otherwise Ramp+ = Ramp− = 0. In particular, the ramp
1 2 a1,a2 a1,a2
functions are ReLU+(x) = Ramp+ = (x−a) ,ReLU−(x) = Ramp− = (a−x) .
a a,∞ + a −∞,a +
In Definition 1, the parameters a,a ,a are the breakpoints of ramp and capped ramp
1 2
functions. This is illustrated in Figure 1.
Definition 2 For a,b ∈ R, the reflection of a about b is the point R = 2b−a.
(a,b)
For a ,a ∈ {x ,··· ,x }, the vectors ReLU+(X) and ReLU−(X) are called ramp features
1 2 1 N a1 a1
while Ramp+ (X) and Ramp− (X) are capped ramp features. The vectors ReLU+(X),
a1,a2 a1,a2 a1
ReLU+(X),Ramp+ (X)andRamp− (X)arereflectionfeatures ifa ,a ∈ {x ,··· ,x }∪
a1 a1,a2 a1,a2 1 2 1 N
(cid:110) (cid:111) (cid:110) (cid:111)
R : j ,j ∈ [N] and a or a is in R : j ,j ∈ [N] . Using these features,
we
( inxj f1o, rx mj2a) lly1 sta2
te our main
res1
ult
on2
Lasso
eq( ux ij v1a,x lj e2n)
ce
f1 or2
ReLU networks.
Theorem 1 (Informal) A deep narrow network with ReLU activation of depth 2,3, 4 is
equivalent to a Lasso model with ramp, capped ramp, and reflection features, respectively.
We state Theorem 1 formally later in this section. The theorem suggests that depending
on the depth, a ReLU network learns to model data with a discrete and fixed dictionary of
features. Moreover, it suggests that as the depth increases, this dictionary expands, which
deepens its representation power.
Remark 1 Note that when the network depth is 2 or 3, the equivalent Lasso dictionary only
contains capped ramp features with breakpoints at training data, leading to a prediction with
kinks only at data locations. In contrast, when the network depth is 4, there can be breakpoints
at reflections of data points with respect to other data points due to the reflection features.
As a result, the sequence of dictionaries as the network gets deeper converges to a richer
library that includes reflections.
7training with Adam training with Lasso selected features
4
(xn,yn) (xn,yn) z i* 1=1
net net z i* 2= 2/3
2 z i* 3= 1/3
0
4
(xn,yn) (xn,yn) z i* 1=2/3
net net z i* 2=1/3
2
0
4
(xn,yn) (xn,yn) z i* 1=1
net net
2
0
0 2 4 6 8 10 12 0 2 4 6 8 10 12 0 2 4 6 8 10 12
Figure 2: Plots of deep narrow ReLU network predictions (blue) for the same 1-D dataset
(red dots), found by (left): training with Adam on the non-convex training problem and
β ≈ 0, (middle): analytically solving the minimum norm convex Lasso problem. Features
corresponding to nonzero Lasso solution components z∗ are plotted in the right column.
i
Definition 3 For an activation σ and L ∈ N, define the following dictionary index sets:
(cid:40)
{1} if L = 2 and σ(x) = sign(x) or σ(x) = |x|
M(1) = , M(2) = [N]L−1,
{−1,1}L−1 else
(cid:40)
{0} if L < 4
M(3) = , M = M(1)×M(2)×M(3).
{0,1} else
Recallthats(l) ∈ Rm l,s(i,l) ∈ Rm l ands(u) ∈ Rdenoteamplitudeparametersforstandard,
parallel and tree architectures, respectively. In this subsection (3.1), with slight abuse of
notation, we denote elements of M(1) by s = (s ,··· ,s ) ∈ {−1,1}L−1 .
1 L−1
Definition 4 Let L ∈ {2,3,4},(s,j,k) ∈ M. The deep narrow function f(L,σ) : R → R is
s,j,k
defined as follows. Let a = x if k = 0 and otherwise let a = R .
1 j1 1 (xj1,xj2)
If L = 2:
(cid:40)
σ(x −x) if s = −1
f(L,σ)
(x) =
j1 1
s,j,k σ(x−x ) if s = 1
j1 1
If L = 3:
if s = 1:
2

ReLU− (x) if s = −1
f(L,σ)
(x) =
 min{xj1,xj2} 1
s,j,k ReLU+ max{xj1,xj2}(x) if s
1
= 1
8
sreyal
2
sreyal
3
sreyal
4if s = −1:
2
(cid:40)
Ramp+ (x) if s = −1
f(L,σ)
(x) =
xj2,xj1 1
s,j,k Ramp− (x) if s = 1.
xj1,xj2 1
If L = 4:
if s = s = 1:
2 3

ReLU− (x) if s = −1
f(L,σ)
(x) =
 min{a1,xj2,xj3} 1
s,j,k ReLU+ max{a1,xj2,xj3}(x) if s
1
= 1
if s = 1,s = −1:
2 3

Ramp+ (x) if s = −1
f(L,σ)
(x) =
 xj3,min{a1,xj2} 1
s,j,k Ramp− max{a1,xj2},xj3(x) if s
1
= 1
if s = −1,s = 1:
2 3

f(L,σ)
(x) =
Ramp+ max{xj2,xj3},a1(x) if s
1
= −1
s,j,k Ramp− a1,min{xj2,xj3}(x) if s
1
= 1
if s = −1,s = −1:
2 3

f(L,σ)
(x) =
Ramp− xj2,min{a1,xj3}(x) if s
1
= −1
s,j,k Ramp+ max{a1,xj3},xj2(x) if s
1
= 1.
We call f(L,σ) (X) a deep narrow feature.
s,j,k
Definition 5 Let Ramp be Ramp+ if a ≤ a and Ramp− otherwise.
a1,a2 a1,a2 1 2 a2,a1
Definition 5 states that Ramp (x) is the capped ramp function with breakpoints at a ,a
a1,a2 1 2
such that Ramp (a ) = 0. When L = 4, deep narrow features can have breakpoints
a1,a2 1
at reflections. However, not all reflections of data points are breakpoints, as described
in the next result. For a ramp, its slope must be in the direction of the reflection: if
x > x then R > x and a ramp must increase for x > R . If x < x
thj2
en R
j1 <( xxj1,x aj n2d)
a
raj m2
p must decreases for x < R .
F( ox rj1a,x cj2a)
pped
rj a2
mp,
aj t1
most
on(x ej1b,x rj e2a) kpoinj2
t can be a reflection. If a capped
ramp(x ej1v, ax lj u2)
ates to 0 at a data point
breakpoint and the other breakpoint is a reflection R , then x has to reflect across
x towards the breakpoint that is a data point. For
ex( ax mj1p,x lj e2,)
an
increaj1
sing capped ramp has
j2
a reflection breakpoint satisfying R < x < x . If the capped ramp has a value of 0
at a reflection breakpoint R
( ,xj t1h,x ej n2)
the
oj t2
her
bj r1
eakpoint must be x . See Figure 3.
(xj1,xj2) j2
Lemma 1 All L = 4 deep narrow functions with breakpoints at a reflection are of the form
ReLU+ if j ≥ j , ReLU− if j ≤ j , Ramp where j < j ≤ j
R (xj1,xj2) 1 2 R (xj1,xj2) 1 2 xj3,R (xj1,xj2) 1 2 3
or j > j ≥ j , and Ramp where j ̸= j .
1 2 3 R (xj1,xj2),xj2 1 2
9ReLU− ReLU+
R R
(xj1,xj2) (xj1,xj2)
x x x x
j1 j2 R
(xj1,xj2)
j2 j1
Ramp− Ramp+
xj3,R
(xj1,xj2)
xj3,R
(xj1,xj2)
R R
(xj1,xj2) (xj1,xj2)
x j1 x j2 x j1 x j2 x j3 x j2 x j1 x j2 x j1
Ramp− Ramp+
xj2,R
(xj1,xj2)
R (xj1,xj2),xj2
R R
(xj1,xj2) (xj1,xj2)
x x x
j1 j2 j1
Figure 3: L = 4 deep narrow features that have a breakpoint at a reflection point. Top:
ramp features. Middle: Ramp . Bottom: Ramp .
xj3,R
(xj1,xj2)
R (xj1,xj2),xj2
The next result shows the training problem (1) for a deep narrow network (defined in
Section 2.2) can be optimized via a Lasso problem (2) that learns deep narrow features
f(L,σ) (X) for different tuples of parameters (s,j,k) ∈ M.
s,j,k
Theorem 1 Let L ∈ {2,3,4}. Let σ be the ReLU, leaky ReLU, absolute value, sign, or
threshold function if L = 2, and ReLU otherwise. Consider a Lasso problem whose dictionary
consists of all possible deep narrow features and where ξ = 0 if σ is the sign or threshold
function. Suppose (z∗,ξ∗) is a solution, and let m∗ = ∥z∗∥ . This Lasso problem is equivalent
0
to a training problem for a L-layer deep narrow network with activation σ and m ≥ m∗.
L
The notion of equivalence between optimization problems is defined in the beginning of
Section 3. Theorem 1 shows that instead of training a neural network with a non-convex
problem and reaching a possibly local optimum, we can simply solve a straightforward Lasso
problem whose convexity guarantees that gradient descent approaches global optimality. For
ReLU activation, the dictionary matrix has up to |M(1)|·|M(2)|·|M(3)| features, which is
2L−1NL−1 for depth L = 2,3 and 2LNL−1 for L = 4. In previous work (Ergen et al., 2023),
a similar Lasso formulation is developed for networks with threshold activation but requires
10up to 2N features of length N in the dictionary for a 2-layer network. In contrast, Theorem 1
shows that at most 2N2 features are needed for a 2-layer network.
Theorem 1 states how the Lasso dictionary evolves with depth, adding more features
with each layer. A neural net trained with the Lasso problem in Theorem 1 learns dictionary
functions f(L,σ) (X) as features, which are ramps for L = 2 and capped ramps for L > 2,
s,j,k
sampled at the training data. For L = 2,3, the features have breakpoints at data points,
and for L = 4, also at their reflections. The data and reflection breakpoints correspond to
the cases a = x and a = R , respectively, in Definition 4. Figure 1 illustrates the
basic
types1
of
feaj1
tures.
F1 igure( 5xj1e, nx uj2m)
erates Lasso features for each depth.
Remark 2 Let L = 2 and A ,A ∈ RN×N with elements (A ) = σ(x −x ),(A ) =
+ − + i,n i n − i,n
σ(x −x ). We can write the dictionary matrix in Theorem 1 as A = A for absolute value
i n +
and sign activations, and A = [A ,A ] ∈ RN×2N for ReLU, leaky ReLU, and threshold
+ −
(L,σ)
activations. A and A contain features f (X) where s = 1 and s = −1, respectively.
+ − s,j,k
Figure 4 illustrates A for the ReLU activation. Next, we discuss a map to reconstruct
+
an optimal neural net from a Lasso solution. As defined in Definition 4, the deep narrow
features are specified by the tuples i = (s,j,k) ∈ M. Hence for deep narrow networks we also
index the columns of A, the elements of vector z in the Lasso problem, and the reconstructed
parallel units by the tuples i ∈ M. Note for a deep narrow network, W(i,l),b(i,l) ∈ R.
Definition 6 Let (z∗,ξ∗) be a solution to the Lasso problem. The reconstructed pa-
rameters for a deep narrow network are defined as follows. For i = (s,j,k) ∈ M, let
(cid:16) (cid:16) (cid:17)(cid:17)
a(i) be a as defined in Definition 4, and let a(i) = s x −a(i) if L > 2, a(i) =
1 1 2 1 j2 1 3
+
(cid:18) (cid:18) (cid:19)(cid:19)
(cid:16) (cid:16) (cid:17)(cid:17)
s s x −a(i) −a(i) if L > 3. Let α = z∗ and ξ = ξ∗. For sign and threshold
2 1 j3 1 2
+
+
activation, let all amplitude parameters be 1. For i = (s,j,k) ∈ M,l ∈ [L−1], let W(i,l) = s ,
l
b(i,l) = −s a(i). Finally, unscale parameters (as defined in Section 2).
l l
A reconstructed deep narrow network is optimal in the training problem, as shown in the
proofofTheorem1. Thereconstructionisefficientandexplicit. Next,wesimplifyDefinition6
for shallow networks. For L = 2, M = {−1,1}×[N]×{0} for ReLU, leaky ReLU, and
threshold activations and M = {1}×[N]×{0} for sign and absolute value activations, and
the layer index is l ∈ [L−1] = {1}.
Definition 7 ForL = 2andi = (s,j,k) ∈ M,definethescalarsw = W(i,1),b = b(i,1),α˜ =
i i i
s|α |. Let w,b,α˜ be vectors stacking together all w ,b ,α˜ , respectively.
i i i i
In the following, take vector-vector operations elementwise.
(cid:112)
Remark 3 Let Rz→α(z) = sign(|z|) |z|. Let Rα,ξ→θ(α,ξ) = (α,ξ,α˜,−Xα˜). Define
the reconstruction function R(z,ξ) = Rα,ξ→θ(Rz→α(z),ξ). Consider a 2-layer neural net
with ReLU, absolute value, or leaky ReLU activation. Given a Lasso solution (z∗,ξ), the
reconstructed neural net parameters are θ = (α,ξ,w,b) = R(z∗,ξ).
11n n
i i
Figure 4: Generic shape of A ∈ RN×N defined by A = σ(x −x ), where σ is ReLU
+ +i,n i n
(left) and sign activation (right). Each ith curve represents a feature. The points (cid:0) i,n,A (cid:1)
+i,n
are plotted in 3-D, with A represented by the curve height and color. Here, n ∈ [N] but
+i,n
each curve interpolates between integer values of n.
layersL Lasso feature f(L,σ) (X)
s,j,k
2
3
4
a 1 min{a 1,x j2} max{a 1,x j2}
min{a ,x ,x } max{a ,x ,x }
1 j2 j3 1 j2 j3
min{a ,max{x ,min{a ,x }}} max{a ,min{x ,max{a ,x }}}
1 j3 1 j2 1 j3 1 j2
Figure 5: The Lth row consists of possible graphs of the dictionary function f(L,σ) (x) where
s,j,k
σ is ReLU and s is varied. The point a is as defined in Definition 4. Arrows point to the
1
right and left to represent the cases s ≥ 0 or s ≤ 0, respectively, for l ∈ [L−1].
l l
3.2 Deep neural networks with sign activation
In this section, we analyze the training problem of an L-layer deep network with sign
activation, which need not be a deep narrow network. We say the vector h ∈ {−1,1}N
switches at n > 1 if h ̸= h . For n ∈ N, let the switching set H(n) be the set of all vectors
n n−1
in {−1,1}N that start with 1 and switch at most n times.
Lemma 2 For L = 2 and sign activation, the Lasso dictionary in Theorem 1 is H(1).
12The next result shows that the training problem (1) for deeper networks with sign
activation is also equivalent to a Lasso problem (2) whose dictionary is a switching set.
Proofs in this section are deferred to Appendix D.1.
Theorem 2 Consider a Lasso problem whose dictionary is the switching set H(K), ξ = 0,
and with solution z∗. Let m∗ = ∥z∗∥ . This Lasso problem is equivalent to the training
0
problem for a neural network with sign activation, m ≥ m∗, and m = K when it is a
L L−2
rectangular network, and
(cid:81)L−1m
= K when it is a tree network.
l=1 l
Theorem 2 generalizes Theorem 1 for sign networks. By Lemma 2, for L = 2, 1 =
d = m
0
= L−2 so the dictionary with sign activation is also H(mL−2) = H(1). Adding
another layer to a parallel network with sign activation expands the dictionary to vectors
with up to m switches. But adding even more layers doesn’t change the dictionary, unless
1
the neural net is a tree architecture: then, the features have as many breakpoints as the
product of the number of neurons in each layer. The Lasso representation suggests that
the representation power of networks with sign activation may stagnate after three layers.
Moreover, the sign activation dictionary has no reflection features, which also may limit its
expressability (Minsky and Papert, 2017). Reflection features allow neural networks to fit
functions with breakpoints at locations in between data points. The reflection breakpoints
for ReLU networks suggest that they can learn geometric structures or symmetries from the
data. An explicit reconstruction of an optimal neural net with sign activation for L = 3
layers is described next. It is drawn in Figure 6. he reconstruction uses the unscaling defined
in Section 2.
Lemma 3 Consider a L = 3-layer sign-activated network. Suppose z∗ is optimal in the
Lasso problem, and m ≥ ||z∗|| . Let ξ = 0. Let α = z∗. Suppose A switches at I(i) < I(i) <
L 0 i 1 2
··· < I(i) . Let W(i,1) = 1,b(i,1) = −x ,W(i,2) = (−1)n+1 and b(i,2) = −1(cid:8) m(i) odd(cid:9) .
m(i) n n In(i)−1 n
Let all amplitude parameters be 1. Let I = {i : z ̸= 0}. If i ∈/ I, set s(i,l),α ,W(i,l),b(i,l) to
i i
zero. These parameters are optimal when unscaled.
The reconstruction of a 3-layer network with sign activation is efficient and explicit.
Reconstructions for other architectures are given in Appendix D.1. The Lasso dictionary
for deep neural nets in previous work (Ergen et al., 2023) uses a dictionary that depends
on the training data X. However, Lemma 2 and Theorem 2 show that networks with sign
activation have dictionaries that are invariant to the training data X. So to train multiple
neural nets on different data, the dictionary matrix A only needs to be constructed once.
Using the Lasso problem, the next result compares the training loss for networks with sign
activation for different depths.
Corollary 1 Consider a sign-activated neural network with L = 3 layers. There exists
an equivalent L = 2-layer network with m = m m neurons, where the m ,m are the
L=2 1 3 1 3
number of neurons in the 3-layer network. Moreover, let p∗ be the optimal value of the
L,β
training problem (1) for L layers, regularization β and sign activation. Let m ,m be the
1 2
number of neurons in the first and second hidden layer of a three layer net, respectively. Then,
for two-layer nets trained with at least m m hidden neurons, p∗ ≤ p∗ ≤ p∗ .
1 2 L=3,β L=2,β L=3,m1β
13(cid:0) XW(i,1)+1·b(i,1)(cid:1)
=X−x 1
j I(i)
j
··· f (X;θ)=(cid:80)m2 α σ ( σ (X W(i,1) + 1· b(i,1)) W(i,2) + 1· b(i,2) )
3 i=1 i s(i,3)
σ
−11
I(i)
j
− + − + −
z∗
1
j =1 j =m
1
−2 0 −11 −α i α i
(cid:80) σ α s(i i,= 3)=√z | (cid:112)i zi|
|z|
(cid:80) f 3(X;θ)
i
z∗
m3
Figure 6: Output of an optimal 3-layer neural net with sign activation reconstructed from a
Lasso solution z∗ using Lemma 3 . The pulse colors correspond to network operations. The
alternating +,− represent W(i,2) = (1,−1,1,−1,···). The red and green pulses illustrate 2
and 3-layer dictionary features, respectively (Theorem 1, Theorem 2), while the other colors
represent multiplication by weights and amplitudes.
Corollary 1 states that a 3-layer net can achieve lower training loss than a 2-layer net,
but only while its regularization β is at most m times stronger. Analysis of the span or
1
uniqueness and the generalizing abilities of different optimal or stationary solutions to (1) is
an area for future work. Next, we give an extension of the Lasso equivalence for 2-D data.
3.2.1 Example of 2-D data
The next result extends Theorem 2 to 2-D data on the upper half plane. We consider
parallel neural nets without internal bias parameters, that is, b(i,l) = 0 for all i,l. Proofs are
located in Appendix D.2.
Theorem 3 Consider a Lasso problem whose dictionary is the switching set H(K), ξ = 0,
and with solution z∗. Let m∗ = ∥z∗∥ . This Lasso problem is equivalent to the training
0
14problem for a sign-activated network without internal biases that is 2-layer or rectangular,
satisfies m ≥ m∗, m = K, and is trained on 2-D data with unique angles in (0,π).
L L−2
The next result reconstructs an optimal neural net from the Lasso problem in Theorem 3.
(cid:18) (cid:19)
0 −1
Lemma 4 Let Rπ = be the counterclockwise rotation matrix by π. An optimal
2 1 0 2
parameter set for the training problem in Theorem 3 when L = 2 is the unscaled version
(cid:110) (cid:111)
of θ = α
i
= z i∗,s(i,1) = 1,W(i,1) = Rπ(cid:0) x(i)(cid:1)T ,ξ = 0 : z i∗ ̸= 0 , where z∗ is optimal in the
2
Lasso problem.
Remark 4 A Lasso dictionary for an architecture discussed in Theorem 1, Theorem 2 or
Theorem 3 is a superset of any dictionary with the same architecture but shallower depth.
Reconstructing a neural net from a Lasso solution gives at least one optimal neural net
in the non-convex training problem (1). The next section discusses the entire solution set to
the Lasso problem, and how this generates a subset of optimal networks in (1).
4 The solution sets of Lasso and the training problem
We have shown that training neural networks on 1-D data is equivalent to fitting a Lasso
model. Now we develop analytical expressions for all minima of the Lasso problem and its
relationship to the set of all minima of the training problem. These results, which build
on the existing literature for convex reformulations (Mishkin and Pilanci, 2023) as well as
characterizations of the Lasso (Efron et al., 2004), illustrate that the Lasso model provides
insight into non-convex networks. We focus on two-layer models with ReLU, leaky ReLU and
absolute value activations, although our results can be extended to other architectures by
considering the corresponding neural net reconstruction. Proofs are deferred to Appendix H.
We start by characterizing the set of global optima to the Lasso problem (2). Suppose
(z∗,ξ∗) is a solution to the convex training problem. In this notation, the optimal model fit
yˆ and equicorrelation set E are given by
β
(cid:110) (cid:111)
yˆ = Az∗+ξ∗1, E = i : |A⊤(yˆ −y)| = β ,
β i
where yˆ is unique over the optimal set (Vaiter et al., 2012; Tibshirani, 2013). The equicorre-
lation set contains the features maximally correlated with the residual yˆ −y and plays a
critical role in the solution set.
Proposition 1 Suppose β > 0. Then the set of global optima of the Lasso problem (2) is
(cid:110) (cid:16) (cid:17) (cid:111)
Φ∗(β) = (z,ξ) : z ̸=0 ⇒ sign(z )=sign A⊤(yˆ −y) , z = 0∀i ̸∈ E , Az+ξ1 = yˆ. (6)
i i i i β
The solution set Φ∗(β) is polyhedral and its vertices correspond exactly to minimal models,
i.e. models with the fewest non-zero elements of z (Mishkin and Pilanci, 2023). Let R be the
reconstruction function described in Remark 3. All networks generated from applying R to
a Lasso solution are globally optimal in the training problem. The next result gives a full
description of such networks. The 2-layer parameter notation defined in Definition 7 is used.
15Proposition 2 Suppose β > 0 and the activation is ReLU, leaky ReLU or absolute value.
The set of all 2-layer Lasso-reconstructed networks is
(cid:26)
(cid:16) (cid:17) α˜
R(Φ(β)) = (w,b,α,ξ) : α ̸=0 ⇒ sign(α )=sign A⊤(y−yˆ) ,b = −x i ,
i i i i i(cid:112)
|α |
i (7)
(cid:27)
α˜
i
w = ; α = 0∀i ∈/ E , f (X;θ) = yˆ .
i (cid:112) i β 2
|α |
i
Proposition 2 shows that all neural nets trained using our Lasso and reconstruction share
the same model fit whose set of active neurons is at most the equicorrelation set. By finding
just one optimal neural net that solves Lasso, we can form R(Φ(β)) and compute all others.
The min-norm solution path is continuous for the Lasso problem (Tibshirani, 2013).
Since the solution mapping in Definition 6, Appendix C is continuous, the corresponding
reconstructed neural net path is also continuous as long as the network is sufficiently wide.
Moreover, we can compute this path efficiently using the LARS algorithm (Efron et al.,
2004). This is in contrast to the under-parameterized setting, where the regularization path
is discontinuous (Mishkin and Pilanci, 2023).
What subset of optimal, or more generally, stationary, points of the non-convex training
problem (1) consist of Lasso-generated networks R(Φ(β))? First, R(Φ(β)) can generate
additional optimal networks through neuron splitting, described as follows. Consider a single
neuron ασ (wx+b) (where α,w,b ∈ R), and let {γ }n ⊂ [0,1]n be such that (cid:80)n γ = 1.
The neurons can be split into n neurons (cid:8)√ γ ασ(cid:0)√ γi wi x=1 +√ γ b(cid:1)(cid:9)n Wang et al. (2i= 021 1)i . For
i i i i=1
any collection Θ of parameter sets θ, let P(Θ) be the collection of parameter sets generated
by all possible neuron splits and permutations of each θ ∈ Θ. Next, let C(β) and C˜(β) be
the sets of Clarke stationary points and solutions to the non-convex training problem (1),
respectively.
Proposition 3 Suppose L = 2,β > 0, the activation is ReLU, leaky ReLU or absolute value
and m∗ ≤ m ≤ |M| = 2N. Let ΘP = {θ : ∀i ∈ [m],∃j ∈ [N] s.t. b = −x w }. Then
i j i
P(R(Φ(β))) = C˜(β)∩ΘP = C(β)∩ΘP. (8)
Proposition 3 states that up to neuron splitting and permutation, our Lasso method gives
all stationary points in the training problem satisfying b = −x w . Moreover, all such points
i i i
are optimal in the training problem, similar to Feizi et al. (2017).
Since optimal solutions are stationary, a neural net reconstructed from the Lasso model
is in C˜(β) ⊂ C(β). However, C(β) ̸⊂ ΘP. This is because there may be other neural nets
with the same output on X as the reconstructed net so that they are all in C(β), but that
differ in the the unregularized parameters b and ξ, so that they are not in ΘP. For example,
if β is large enough, the Lasso solution is z = 0 (Efron et al., 2004), so the reconstructed net
will have α = 0, which makes the neural net invariant to b. In this section, we analyzed the
general structure of the Lasso solution set when β > 0. Next, we analyze the Lasso solution
set for specific activations and training data when β → 0.
5 Solution sets of Lasso under minimal regularization
One of the insights that the Lasso formulation provides is that under minimal regulariza-
tion, certain neural nets perfectly interpolate the data.
16Corollary 2 For the ReLU, absolute value, sign, and threshold networks with L = 2 layers,
and sign-activated deeper networks, if m ≥ m∗, then f (X;θ) → y as β → 0.
L L
Proofs in this section are deferred to Appendix E. In Corollary 2, m∗ depends on L and
the activation and is defined in Theorem 1 and Theorem 2. The Lasso equivalence and
reconstruction also shed light on optimal neural network structure as regularization decreases.
The minimum (l ) norm subject to interpolation version of the Lasso problem is
1
min∥z∥ , s.t. Az+ξ1 = y. (9)
1
z,ξ
Looselyspeaking,asβ → 0,ifAhasfullcolumnrank,theLassoproblem(2)"approaches"
the minimum norm problem (9), where ξ = 0 for sign and threshold activations. The rest of
this section describes the solution sets of (9) for certain networks.
Proposition 4 Let L = 2. Suppose σ is the absolute value activation. Let z∗ be a solution
to (9). Then, we have z∗z∗ ≤ 0. Moreover, the entire solution set of (9) for z∗ is given by
1 n
(cid:110) (cid:12) (cid:111)
z∗+t sign(z∗)(1,0,··· ,0,1)T(cid:12)−|z∗| ≤ t ≤ |z∗ | . (10)
1 (cid:12) 1 N
Proposition 5 Let L = 2. Suppose σ is sign activation. Then, for β ≥ 0, the Lasso problem
(2) has a unique solution. And the minimum norm solution z∗ to (9) is z∗ = A−1y.
Given an optimal bias term ξ∗, if A is invertible, then z∗ = A−1(y−ξ∗1) is optimal in
(9). Appendix F finds A−1 for some activation functions. The structure of A−1 suggests the
behavior of neural networks under minimal regularization: sign-activated neural networks
act as difference detectors, while neural networks with absolute value activation, whose
subgradient is the sign activation, act as a second-order difference detectors (see Remark 20).
The next result shows that threshold-activated neural networks are also difference detectors,
but for the special case of positive, nonincreasing y . An example of such data is cumulative
n
revenue, e.g. y = (cid:80)n r where r is the revenue in dollars earned on day i.
n i=1 i i
Proposition 6 Let L = 2. Suppose σ is threshold activation and y ≥ ··· ≥ y ≥ 0. Then
1 N

y −y if n ≤ N −1
 n n−1

z∗ = y if n = N
n N

0 else
is the unique solution to the minimum norm problem (9).
The next result gives a lower bound on the optimal value of the minimum weight problem
for ReLU networks. If we can find z with a l norm that meets the lower bound and a ξ
1
such that Az+ξ1 = y, then we know z,ξ is optimal. In this section, for n ∈ [N −1], let
µ = yn−yn+1 be the slope between the nth and n+1th data points. Let µ = 0.
n xn−xn+1 N
Lemma 5 The optimal value ∥z∗∥ of the minimum norm problem (9) for L = 2,3,4 and
1
ReLU activation is at least max |µ |.
n∈[N−1] n
17In the special case of 2-layer networks, the next result gives a solution to the minimum
weight problem. For i ∈ [N], let (z ) and (z ) be the Lasso variable corresponding to the
+ i − i
features ReLU+ and ReLU−, respectively. In other words, z corresponds to A and z
xi xi + + −
corresponds to A as defined in Remark 2.
−
Lemma 6 The optimal value for the minimum norm problem (9) for L = 2 and ReLU
activation is ∥z∗∥ = (cid:80)N−1|µ −µ |. An optimal solution is (z ) = µ −µ for
1 n=1 n n+1 + n+1 n n+1
n ∈ [N −1], z = 0, and ξ = y .
− N
We examined neural networks when β → 0. We next analyze networks as β grows.
6 Solution path for sign activation and binary, periodic labels
This section examines solution paths of Lasso problems for 2 and 3-layer neural nets with
signactivationand1-Ddatawherey isbinary. Suchdataappearsintemporalsequencessuch
as binary encodings of messages communicated digitally, (Kim et al., 2018), neuron firings
in the brain (Fang et al., 2010), and other applications, where x represents time. These
n
real world sequences are in general aperiodic. However, in the special case that the target
vector is periodic and binary, the Lasso problem gives tractable solutions for optimal neural
networks. This offers a step towards analyzing neural network behavior for more general,
aperiodic data, which is an area for future work. We call the binary, periodic sequence a
square wave, defined as follows. For a positive even integer T that divides N, define a square
wave to be h(T) ∈ {−1,1}N that starts with 1 and is periodic with period T. Given a square
wave of period T, let k = N be the number of cycles it has. If the real line is split into a
T
finite number of regions by binary labels, the square wave represents the labels of a monotone
sequence of points, with the same number of samples in each region. The right-hand graph
of Figure 15 (Appendix G) plots the elements of a square wave over its indices.
There is a critical value β = max |ATy| such that when β > β , the solution of the
c n∈[N] n c
Lasso problem has z∗ as the all-zero vector (Efron et al., 2004). Let β˜ = β . Theorem 1
specifies the Lasso problem for a 2-layer network with sign activation. We willβ ucse the N×N
dictionary matrix A with A = σ(x −x ), as defined in Remark 2. The Lasso solution
i,n i n
z∗ ∈ RN is unique, by Proposition 5. The next results gives the entire solution path of this
Lasso problem and an optimal neural net in closed form for a square wave target vector.
Proofs in this section are deferred to Appendix G.
Theorem 4 Consider the Lasso problem for a 2-layer net with sign activation and square
wave target vector of period T. The critical value is β = T. And the solution is
c
 (cid:16) (cid:17)
1 1−β˜ if i ∈ {1,2k−1}
    2 + if β˜≥ 1 2
0 else
z∗ = , (11)
T 2i (cid:40) 1− 3β˜ if i ∈ {1,2k−1}
   2 (cid:16) (cid:17) if β˜≤ 1.
  (−1)i+1 1−2β˜ else 2
for i ∈ [2k−1] and z∗ = 0 at all other n ∈ [N].
n
18L=2 L=3
1 1
β˜
2β˜
β˜
0.5 0.5
1−β˜
2 n n
1−β˜
=β˜
2
−0.5 −0.5
−1 −1
Figure 7: Each of the two figures depicts (n,y ) with black dots, where y = h(T) with
n
T = 10,N = 40. Sign-activated neural net predictions are depicted as (n,f (x ;θ)) with
L n
blue, magenta, and red dots for β˜= 4 ∈ (cid:2)1,1(cid:3) ,β˜= 1, and β˜= 1 ≤ 1, respectively.
5 2 2 5 2
Corollary 3 For a square wave target vector with period T, there is an optimal 2-layer
neural network with sign activation specified by
f (x;θ) = 0, if β˜≥ 1
2
 (cid:16) (cid:17)
− 1−β˜ if x < x
  N−T
 2 1
f (x;θ) = 0 if x ≤ x < x if ≤ β˜≤ 1
2 N−T T 2
 2 2
 1−β˜ if x ≥ x
T
2
 (cid:16) (cid:17)
− 1−β˜ if x < x


N−T
 (cid:16) (cid:17) 2 1
f (x;θ) = (−1)i 1−2β˜ if x ≤ x < x , i ∈ [2k−2] if β˜≤
2 T(i+1) Ti 2
 2 2
  1−β˜ if x ≥ x
T
2
Theorem 4 implies that when β > T, an optimal neural net is the constant zero function.
In Corollary 3, when β ≤ T, f (X;θ) is periodic over (cid:2)T,N − T(cid:3) with period T, and has
2 2 2 2
amplitude 2β less than that of y. The next results give the solution path and an optimal
T
neural net when L = 3, and are proved in Appendix G.2.
Theorem 5 Consider the Lasso problem for a 3-layer network with sign activation and
target vector a square wave of period T and m ≥ 2T −1. Then β = N and A = −h(T)
3 N c i
(cid:16) (cid:17)
for some i. The solution to the Lasso problem is z∗ = − 1−β˜ and z∗ = 0 at all other n.
i n
+
Corollary 4 Let x = ∞. For a square wave target vector with period T, there is an
0
optimal 3-layer neural net with sign activation specified by f (x;θ) = (1−β˜) (−1)(i−1) if
3 +
x ≤ x < x for i ∈ [2k−1], and f (x;θ) = −(1−β˜) if x < x .
Ti T(i−1) 3 + N−T
2 2 2
Since only one parallel unit is active in this network, it is also a standard neural net. The
neural net has output f (x;θ)(X) = (1−β˜) y. If β > N, then the optimal neural net is
3 +
the constant zero function.
19L=2,N=40 L=3,N=30
1
0.5
0.5 0.0
0.5
0
1
0
1
-100 0 100 -100 0 100
x x
Figure 8: The bottom figures plot training data (x ,y ). The top figures plot sign-activated
n n
neural net predictions by color for each x, as parameterized by β on the vertical axis.
Figure 7 illustrates f (X;θ) when y = h(T). Consider the 2-layer network in the left
L
plot. When β → 0, the network interpolates the target vector perfectly. As β˜ increases to 1
2
(red dots) from 0, the magnitude of the middle segments decrease at a faster rate than the
outer segments until at β˜= 1, the net consists of just the outer segments (magenta dots).
2
As β˜ increases to 1 from 1 (blue dots), these outer segments decrease until the neural net is
2
the zero function. The solution path suggests that as the regularization increases, the 2-layer
network focuses on preserving the boundary points of the data (first and last T points) to be
close to the target vector. Therefore the network will generalize well if noise occurs in the
middle of the data. In contrast, if noise occurs uniformly over the data, the 3-layer network
will generalize well.
We verify Theorem 4 and Theorem 5 by solving the Lasso problem on training data
that is chosen from a uniform distribution on [−100,100] and target vector h(T). Figure 8
illustrates the training data and neural net predictions. Suppose we use the neural net as
a binary classifier whose output is the sign of f (x;θ), where the network is "undecided"
L
if f (x;θ) = 0.The red, blue and white indicate classifications of −1, 1, and "undecided,"
L
respectively. For β < β , the 3-layer net always classifies the training data accurately, but
c
the 2-layer net is undecided on all but the first and last interval if β > β /2. When used
c
as a regressor, for each β, the magnitude of the 3-layer net’s prediction is the same over all
samples, while the 2-layer net is biased toward a stronger prediction on the first and last
intervals. In this sense, the 3-layer network generalizes better. In addition, the 3-layer net
changes more uniformly with β than the 2-layer net, making it easier to tune β. In this
example with a square wave target vector, we analytically solve the neural net Lasso problem
for β ∈ (0,∞) and verify our results by numerically solving the Lasso problem. This gives
analytical expressions for optimal neural nets. In the next section, we analyze other examples
of training data, analytically solve the min norm version of the Lasso problem and verify our
results experimentally by training neural nets with the non-convex problem.
7 Numerical results
Herewedescribesimulationsthatsupportourtheoreticalresults. InFigure2, wecompare
neural nets trained using the non-convex and convex Lasso problems given in (1) and (2). In
20
y
)
;x(Lforder to find a near-optimal solution to (2), we first find analytical optimal solutions (z∗,ξ∗)
to the minimum norm problem (9). The nonzero components z∗ and their corresponding
i
Lasso features specify their solutions z∗ and are shown in the third column of Figure 2. Our
optimal solutions satisfy ξ∗ = 0. Our solutions are optimal by Lemma 6 for L = 2 and
Lemma 5 for L > 2. Then, we numerically solve the training problem for a standard network.
We use a β sufficiently small such that if p∗ and pˆ∗ are the optimal values respectively
found by analytically solving the min norm problem (9), which has objective ∥z∥ , and
1
numerically solving the Lasso problem (2), which has objective 1∥Az+ξ1−y∥2+β∥z∥ ,
2 2 1
(cid:12) (cid:12)
then (cid:12)p∗− pˆ∗(cid:12) < 10−3; in other words the optimal Lasso objective is approaching β∥z∗∥ .
(cid:12) β (cid:12) 1
We set β = 10−7 to satisfy this requirement. We use a standard network in the non-convex
model to show that our Lasso formulation is applicable even to standard architectures. To
optimize the training problem, we use Adam with a learning rate of 5(10−3) and weight decay
of 10−4. Using SGD appeared to give similar results as Adam. The number of final-layer
neurons m and epochs is 100 and 103 for L = 2; 500 and 105 for L = 3; and 100 and 5(104)
L
for L = 4.
The capped ramp features allow the L = 3 neural net to achieve a lower objective of
∥z∗∥ = 1 in the min norm problem compared to the L = 2 net, which achieves an objective
1
value of ∥z∗∥ = 2. The optimal value of the min norm problem for L = 4 is also ∥z∗∥ = 1.
1 1
Therefore the solution shown in Figure 2 for L = 3 is also optimal for L = 4. The neural
nets found from the non-convex training problem closely match those trained with Lasso.
The neural nets trained with Adam has slightly suboptimal fit to the data compared to the
Lasso min norm solution, but this is likely due to finite training time, solver tolerance, the
solver computing a near-globally optimal solution, and the Lasso min norm solution being an
approximation to the Lasso problem. As seen in Figure 2, L ∈ {2,3}, the breakpoints in the
Lasso and non-convex neural nets occur only at the training data points. But when L = 4,
the Lasso and non-convex neural nets have a breakpoint at x = 8, which is not a data point
but a reflection R of data points. Appendix I.1 shows that reflection breakpoints can
(0,4)
also appear in near-optimal solutions.
In addition to training ReLU networks under minimal β, we train neural networks with
threshold activations and larger β. This experiment supports the usefulness of the Lasso
problem for training neural nets. We generate 1-D data samples from an i.i.d. distribution
x ∈ N(0,1), and then label them with a Bernoulli random variable. We use N = 40
samples and β = 10−3 to train a 2-layer neural network with threshold activation using
the Lasso problem (2) as well as a non-convex training approach based on the Straight
Through Estimator (STE) (Bengio et al., 2013). As illustrated in Figure 11, the convex
training approach achieves significantly lower objective value than all of the non-convex trials
with different seeds for initialization. Figure 11 also plots the predictions of the models.
We observe that the non-convex training approach fits the data samples exactly on certain
intervals but provides a poor overall function fitting, whereas our convex models yields a
more reasonable piecewise linear fit. In particular, the neural net trained with the non-convex
problem fits the data in Figure 11 poorly compared to Figure 2. This may occur because in
Figure 11, the data set is larger and more complex, and STE training is used because of the
threshold activation.
Next we present additional numerical results by applying our theory to real-world data.
21Predictionfunctions
Nonconvex-1
100 N No on nc co on nv ve ex x- -2 3 3.0 Nonconvex
Nonconvex-4 Convex(Ours)
Nonconvex-5 2.5
Samples
Convex(Ours)
10−1 2.0
1.5
1.0
10−2
0.5
0 2000 4000 6000 8000 10000
Numberofiterations -2 -1 0 1 2
Figure 9: Training objective Figure 10: Function fit
Figure 11: Training objective (left) and function fit (right) for a neural net using the convex
Lasso problem versus the non-convex training problem using STE.
8 Application: Time-series modeling
In this section, we apply the Lasso problem for neural networks to an autoregression
problem. Suppose at times 1,··· ,T +1 we observe data points x ,··· ,x ∈ R that follow
1 T+1
the time-series model
x = f(x ;θ)+ϵ , (12)
t t−1 t
where f : R → R is parameterized by some parameter θ and ϵ ∼ N(0,σ2) represents
t
observation noise. The parameter θ is unknown, and the goal is find θ that best fits the model
(12) to the data x ,··· ,x . For example, the auto-regressive model with lag 1 (AR(1)) is a
1 T+1
T
(cid:88)
linear model f(x;θ) = ax where θ = a is chosen as a solution to min (f(x ;θ)−x )2.
t t+1
θ∈Θ
t=1
For a more expressive model, instead of f(x;θ) = ax suppose we use a 2-layer neural network
m
(cid:88)
fNN(x;θ) = |xw +b |α , (13)
2 i i i
i=1
which has m neurons and absolute value activation. The parameter set is θ = {w ,b ,α }m .
i i i i=1
Suppose we choose θ that solves the neural net (NN) autoregression training problem
T
min 1 (cid:88)(cid:0) fNN(x ;θ)−x (cid:1)2 + β ∥θ ∥2. (14)
θ∈Θ T 2 t t+1 2 w 2
t=1
By Theorem 1, this non-convex problem is equivalent to the convex Lasso problem (2)
where A = |x −x | and y = x . Our models so far represent predictors of x from
i,j i j i i+1 t+1
x . We can also find a neural network model fNN(x ;θ) (13) that represents the τ-quantile
t 2 t
of the distribution of x given the observation x , where τ ∈ [0,1], by using the quantile
t+1 t
regression loss L (z) = 0.5|z|+(τ −0.5)z and choosing θ that solves the neural net (NN)
τ
quantile regression (QR) training problem
T
min 1 (cid:88) L (cid:0) fNN(x ;θ)−x (cid:1) + β ∥θ ∥2. (15)
θ∈Θ T τ 2 t t+1 2 w 2
t=1
22
evitcejbogniniarTFigure 12: Comparison of neural autoregressive models of the form x = f(x ;θ)+ϵ
t t−1 t
using convex and non-convex optimizers and the classical linear model AR(1) for time series
forecasting. The horizontal axis is the training epoch. The dataset is BTC-2017min from
Kaggle, which contains all 1-minute Bitcoin prices in 2017 (kag). The non-linear models
outperform the linear AR(1) model. Moreover, SGD underperforms in training and test loss
compared to the convex model which is guaranteed to find a global optimum of the NN
objective.
Problem (15) can also be solved by converting it to an equivalent Lasso problem. We now
compare solving the autoregression (14) and quantile regression (15) problems directly with
5 trials of stochastic gradient descent (SGD) initializations versus using the Lasso problem
(2). We also compare against the baseline linear method f(x;θ) = ax+b (AR1+bias), where
we include an additional bias term b.
We first build a neural network fNN(x;θ) (13) with m known, or planted neurons.
2
We use this neural network to generate training samples x ,...,x based on (12) with
1 T+1
f(x;θ) = fNN(x;θ) where x ∼ N(0,σ2). Using the same model fNN(x;θ), we also generate
2 1 2
test samples xtest,...,xtest in an analogous way. We use T = 1000 time samples. Then, we
1 T+1
try to recover the planted neurons based on only the training samples by solving the NN
AR/QR training problems.
In Figure 19, we present experiments based on the selection of m planted neurons and
noise level σ2. More results can be found in Appendix I.2. The neural net trained with Lasso
is labeled cvxNN, which we observe has lower training loss. This appears to occur because
different trials of NN (neural net trained directly without Lasso) get stuck into local minima.
The global optimum that cvxNN reaches also enjoys effective generalization properties, as
seen by the test loss. The regularization path is the optimal neural net’s performance loss
as a function of the regularization coefficient β. Figure 20 plots the regularization path for
σ2 = 1 and m = 5. The regularization path taken by cvxNN is smoother than NN, and can
therefore be found more precisely and robustly by using the Lasso problem.
We also test upon real financial data for bitcoin price, including minutely bitcoin (BTC)
price(BTC-2017min)andhourlyBTCprice(BTC-hourly). Weconsiderthetrainingproblem
on τ-quantile regression with τ = 0.3 and τ = 0.7. For each dataset, we first choose T data
points as a training set and the consecutive T data points as a test set. The numerical
results are presented in Figure 12. We observe that cvxNN provides a consistent lower bound
on the training loss and demonstrates strong generalization properties, compared to large
fluctuation in the loss curves of NN. More results can be found in Appendix I.2.
239 Conclusion
Our results show that deep neural networks with various activation functions trained
on 1-D data with weight regularization can be recast as convex Lasso models with simple
dictionary matrices. This provides critical insight into their solution path as the weight
regularization changes. The Lasso problem also provides a fast way to train neural networks
for 1-D data. Moreover, the understanding of the neural networks through Lasso models
could also be used to explore designing better neural network architectures.
We proved that reflection features emerge in the Lasso dictionary whenever the depth
is 4 or deeper. This leads to predictions that have breakpoints at reflections of data points
about other data points. In contrast, for networks of depth 2 and 3, the breakpoints are
located at a subset of training data. We believe that this mechanism enables deep neural
networks to generalize to the unseen by encoding a geometric regularity prior.
The1-Dresultscanextendtosufficientlystructuredorlowrankdatainhigherdimensions.
Generalizing to higher dimensions is an area of future work. Building on a similar theme,
(Pilanci, 2023) showed that the structure of hidden neurons can be expressed through convex
optimization and Clifford’s Geometric Algebra. The techniques developed in this paper can
be combined with the Clifford Algebra to develop higher-dimensional analogues of the results.
10 Acknowledgements
This work was supported in part by the National Science Foundation (NSF) under Grant
DMS-2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by
the U.S. Office of Naval Research (ONR) under Grant N00014-24-1-2164; in part by the
U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; in part
by the Precourt Institute for Energy and the SystemX Alliance at Stanford University; in
part by the National Science Foundation Graduate Research Fellowship under Grant No.
DGE-1656518.
References
Kaggle. URL www.kaggle.com.
F. Bach. Breaking the curse of dimensionality with convex neural networks. JMLR, 18(1):
629–681, 2017.
Y. Bengio, N. Roux, P. Vincent, O. Delalleau, and P. Marcotte. Convex neural networks. In
Advances in Neural Information Processing Systems, volume 18. MIT Press, 2005.
Y. Bengio, N. Léonard, and A. C. Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. ArXiv:1308.3432, 2013.
M. J. Bianco, P. Gerstoft, J. Traer, E. Ozanich, M. A. Roch, S. Gannot, and C.-A. Deledalle.
Machine learning in acoustics: Theory and applications. The Journal of the Acoustical
Society of America, 146(5):3590–3628, 2019.
J. M. Borwein and A. S. Lewis. Convex Analysis and Nonlinear Optimization: Theory and
Examples. Springer, 2000.
24S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
A. Bulat and G. Tzimiropoulos. XNOR-Net++: Improved binary neural networks. ArXiv,
abs/1909.13863, 2019.
P. Chen and O. Ghattas. Projected Stein variational gradient descent. Advances in Neural
Information Processing Systems, 33:1947–1958, 2020.
P. Chen, K. Wu, J. Chen, T. O’Leary-Roseberry, and O. Ghattas. Projected Stein variational
newton: A fast and scalable Bayesian inference method in high dimensions. Advances in
Neural Information Processing Systems, 32, 2019.
T. M. Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE Transactions on Electronic Computers, (3):
326–334, 1965.
B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of
statistics, 32(2):407–499, 2004.
T.ErgenandM.Pilanci.Convexgeometryoftwo-layerReLUnetworks: Implicitautoencoding
and interpretable models. PMLR, pages 4024–4033, 26–28 Aug. 2020.
T.ErgenandM.Pilanci. Convexgeometryanddualityofover-parameterizedneuralnetworks.
The Journal of Machine Learning Research, 22(1):9646–9708, 2021a.
T. Ergen and M. Pilanci. Revealing the structure of deep neural networks via convex duality.
In ICML, pages 3004–3014. PMLR, 2021b.
T. Ergen, H. I. Gulluk, J. Lacotte, and M. Pilanci. Globally optimal training of neural
networks with threshold activation functions. arXiv:2303.03382, 2023.
C. Fang, Y. Gu, W. Zhang, and T. Zhang. Convex formulation of overparameterized deep
neural networks. arXiv:1911.07626, 2019.
H. Fang, Y. Wang, and J. He. Spiking neural networks for cortical neuronal spike train
decoding. Neural Computation, 22(4):1060–1085, 2010.
S. Feizi, H. Javadi, J. M. Zhang, and D. Tse. Porcupine neural networks: (almost) all local
optima are global. ArXiv, abs/1710.02196, 2017.
M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and B. Schuller. audeep: Un-
supervised learning of representations from audio with deep recurrent neural networks.
JMLR, 18(1):6340–6344, 2017.
C.-L. Hsu and J.-S. R. Jang. On the improvement of singing voice separation for monaural
recordings using the MIR-1K dataset. IEEE Transactions on Audio, Speech, and Language
Processing, 18(2):310–319, 2009.
N. Joshi, G. Vardi, and N. Srebro. Noisy interpolation learning with shallow univariate relu
networks. arXiv.2307.15396, 07 2023.
25K. Karhadkar, M. Murray, H. Tseran, and G. Montúfar. Mildly overparameterized relu
networks have a favorable loss landscape. arXiv:2305.19510, 2023.
H. Kim, Y. Jiang, R. Rana, S. Kannan, S. Oh, and P. Viswanath. Communication algorithms
via deep learning. arXiv:1805.09317, 2018.
M. Kim and P. Smaragdis. Bitwise neural networks for efficient single-channel source
separation. In 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 701–705, 2018.
G. Kornowski, G. Yehudai, and O. Shamir. From tempered to benign overfitting in ReLU
neural networks. arXiv:2305.15141, 2023.
S. Mavaddati. A novel singing voice separation method based on a learnable decomposition
technique. Circuits, Systems, and Signal Processing, 39(7):3652–3681, 2020.
M. Minsky and S. A. Papert. Perceptrons: An Introduction to Computational Geometry.
The MIT Press, 09 2017. ISBN 9780262343930. doi: 10.7551/mitpress/11301.001.0001.
URL https://doi.org/10.7551/mitpress/11301.001.0001.
A. Mishkin and M. Pilanci. Optimal sets and solution paths of ReLU networks. In Interna-
tional Conference on Machine Learning, ICML 2023. PMLR, 2023.
M.Pilanci. Fromcomplexitytoclarity: Analyticalexpressionsofdeepneuralnetworkweights
via Clifford’s geometric algebra and convexity. arXiv:2309.16512, 2023.
M. Pilanci and T. Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. In Proceedings of the 37th
International Conference on Machine Learning, volume 119, pages 7695–7705, 13–18 July
2020.
H. Purwins, B. Li, T. Virtanen, J. Schlüter, S.-Y. Chang, and T. Sainath. Deep learning
for audio signal processing. IEEE Journal of Selected Topics in Signal Processing, 13(2):
206–219, 2019.
P. Savarese, I. Evron, D. Soudry, and N. Srebro. How do infinite width bounded norm
networks look in function space? Annual Conference on Learning Theory, pages 2667–2690,
2019.
J. Serrà, S. Pascual, and C. S. Perales. Blow: a single-scale hyperconditioned flow for non-
parallel raw-audio voice conversion. Advances in Neural Information Processing Systems,
32, 2019.
R. P. Stanley et al. An introduction to hyperplane arrangements. Geometric Combinatorics,
13(389-496):24, 2004.
A. M. Stuart. Uncertainty quantification in bayesian inversion. ICM2014. Invited Lecture,
1279, 2014.
R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal
Statistical Society: Series B (Methodological), 58(1):267–288, 1996.
26R. J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 7:
1456–1490, 2013.
S. Vaiter, C. Deledalle, G. Peyré, J. Fadili, and C. Dossal. The degrees of freedom of the
group lasso for a general design. CoRR, abs/1212.6478, 2012.
Y.Wang,J.Lacotte,andM.Pilanci. Thehiddenconvexoptimizationlandscapeofregularized
two-layer relu networks: an exact characterization of optimal solutions. In International
Conference on Learning Representations, 2021.
Y. Wang, P. Chen, and W. Li. Projected Wasserstein gradient descent for high-dimensional
Bayesian inference. SIAM/ASA Journal on Uncertainty Quantification, 10(4):1513–1532,
2022a.
Y. Wang, P. Chen, M. Pilanci, and W. Li. Optimal neural network approximation of
Wasserstein gradient direction via convex optimization. arXiv:2205.13098, 2022b.
O. Zahm, T. Cui, K. Law, A. Spantini, and Y. Marzouk. Certified dimension reduction in
nonlinear Bayesian inverse problems. Mathematics of Computation, 91(336):1789–1835,
2022.
27Appendix
Note: Figures 7, 15, and 16, we have y = h(T), N = 40,T = 10, and vectors v =
(v ,··· ,v ) are depicted by plotting (n,v ) as a dot. Slanted lines in Figures 1, 5, 13, and
1 N n
14 have slope ±1.
Appendix A. Definitions and preliminaries
A.1 Activation function σ(x)
We assume the activation σ is piecewise linear around 0, i.e., of the form
(cid:40)
c x+d if x < 0
σ(x) = 1 1 , for some c ,c ,d ,d ∈ R. Leaky ReLu, absolute value,
c x+d if x ≥ 0 1 2 1 2
2 2
ReLu, sign, and threshold activations (Section 1.2) are piecewise linear around 0.
A function f is bounded if there is M ≥ 0 with |f(x)| ≤ M for all x. If σ(x) is piecewise
linear around zero, σ(x) is bounded if and only if c = c = 0, e.g. σ(x) is a threshold or
1 2
sign activation. We call f symmetric if it is an even or odd function, for example absolute
value. The activation σ(x) is defined to be homogeneous if for any a ∈ R+, σ(ax) = aσ(x).
Homogeneous activations include ReLU, leaky ReLU, and absolute value. They lack an
amplitude parameter s. We say σ(x) is sign-determined if its value depends only on the sign
of its input and not its magnitude. Threshold and sign activations are sign-determined.
A.2 Effective depth
Remark 5 Let the inner parameters be s(i,l) where l ≤ L−2 and W(i,l) where l ≤ L−1 for
a parallel network; and (cid:0) su⊕1,···su⊕m L−l(cid:1) ,(cid:0) αu⊕1,···αu⊕m L−l(cid:1) where u has positive length,
and (cid:0) wu⊕1,···wu⊕m L−l(cid:1) for a tree network. By plugging (4) and (5) into themselves for
parallel and tree networks, respectively,
(cid:16) (cid:16) (cid:17) (cid:17)
X(i,l+2) = σ σ X(i,l)W(i,l+1)+b(i,l) s(i,l)W(i,l+1)+b(i,l+1)
s(i,l+1)
where 1 ≤ l ≤ L−2 for a parallel network and X(u)=
 
m m
(cid:88)L−l (cid:88)L−l−1 (cid:16) (cid:17)
α(u⊕i)σ s(u⊕i)b(u⊕i)+ α(u⊕i⊕j)σ X(u⊕i⊕j)w(u⊕i⊕j)+b(u⊕i⊕j) s(u⊕i⊕j)w(u⊕i) ,
i=1 j=1
where 0 ≤ l ≤ L−3 for a tree network. Suppose σ is sign-determined. Since the inner
parameters do not affect f (X;θ), regularizing them will drive them to zero. We define the
L
minimum value in (1) as an infimum which is approached as their l norms approach 0.
L˜
Under this definition, we can remove the inner parameters from regularization, and optimize
for their values normalized by their l -norms, equivalently fixing their l -norms.
L˜ L˜
Remark 6 The magnitudes of the inner parameters affect the neural net output for ReLU,
leaky ReLU and absolute value activations. However by Remark 5, this does not hold for sign
and threshold activations. This motivates the definition for L˜.
28Appendix B. Parallel and tree networks with data dimension d ≥ 1
Since ξ ∈/ θ , the training problem (1) can be written as
w
β
min ∥θ ∥2+min{L (f (X)−ξ1+ξ1)}. Apply the change of variables θ → θ−{ξ},
θ−{ξ} 2 w 2 ξ y L
f (X;θ) → f (X;θ)−ξ1,L (z) → min L (z+ξ1) in (1). In other words, L absorbs ξ,
L L y ξ y y
which is now ommitted from f (X;θ). Note that convexity is still preserved.
L
Definition 8 A rescaled neural network and its parameter set is
(cid:88)mL (cid:32) L (cid:89)−1 (cid:33)
f (x;θ) = ξ+ α X(i,L) q(i,l)
L i
i=1 l=1
(cid:16) (cid:17) (16)
X(i,l+1) = σ X(i,l)W(i,l)+b(i,l) , for l ∈ [L−1]
(cid:110) (cid:111) (cid:110) (cid:111)
θ(i) = α ,q(i,l) : l ∈ [L−1] ,θ(i) = b(i,l),W(i,l) : l ∈ [L−1] for i ∈ [m ]
w i b L
where q(i,l) ∈ R,(cid:13) (cid:13)W(i,l)(cid:13) (cid:13) = 1 for a parallel network with homogeneous activation,
L
(cid:88)mL
f (x;θ) = ξ+ α s(i,L−1)X(i,L)
L i
i=1
(cid:16) (cid:17) (17)
X(i,l+1) = σ X(i,l)W(i,l)+b(i,l) ,l ∈ [L−1]
(cid:110) (cid:111) (cid:110) (cid:111)
θ(i) = α ,s(i,L−1) ,θ(i) = b(i,l),W(i,l) : l ∈ [L−1] for i ∈ [m ]
w i b L
for a parallel network with sign-determined activation, and
(cid:88)mL
(cid:16) (cid:17)
f (x;θ) = ξ+ α s(i)σ X(i)+b(i)1
L i
i=1
(cid:40)(cid:80)m L−lα(u⊕i)σ(cid:0) X(u⊕i)+b(u⊕i)(cid:1) if 1 ≤ l ≤ L−3 (18)
X(u) = i=1
(cid:80)m L−lα(u⊕i)σ(cid:0) Xw(u⊕i)+b(u⊕i)(cid:1) if l = L−2
i=1
(cid:110) (cid:111) (cid:110) (cid:111)
θ(i) = α(i),s(i) θ(i) = α(u),b(u) : u ∈ U,u = i for i ∈ [m ]
w b 1 L
wherethelengthofuis> 1forα(u) ∈ θ(i), foratreenetworkwithsign-determinedactivation.
b
Lemma 7 The training problem remains equivalent if the neural network is rescaled.
Proof For parallel networks with homogenous activation: Let q(i,l) = ∥W(i,l)∥ . For
L
parallel networks with homogeneous activations, for l ∈ [L−1], by a change of variables
b(i,l) → q(i,l)b(i,l), the training problem is equivalent if we factor out q(i,l) so that
(cid:16) (cid:17)
X(i,l+1) = σ X(i,l)W˜ (i,l)+1·b(i,l) q(i,l), (19)
(cid:13) (cid:13)
such that (cid:13)W˜ (i,l)(cid:13) = 1. Plug in (19) into itself for l = 1,··· ,L−1 to move all the q(i,l)
(cid:13) (cid:13) L˜
terms to X(i,L) and a change of variables for b(i,l) to get the result.
29For parallel networks with sign-determined activation: ByRemark5,s(i,l) forl ≤ L−2and
W(i,l) for l ≤ L−1 can be unregularized. Then apply a change of variables s(i,l−1)W(i,l) →
W(i,l) for 2 ≤ l ≤ L−1, which removes s(i,l) from θ for l ≤ L−2.
For a tree network with sign-determined activation: Remove tree parameters from regu-
larization as described in in Remark 5. For u of length 1 ≤ l ≤ L−2, w(u) ∈ R and so for
i ∈ [m ] we may apply a change of variables α(u⊕i)s(u⊕i)w(u) → α(u⊕i), so w(u),s(u⊕i)
L−l−1
can be removed from θ.
For sign-determined activations, Lemma 7 still holds with the constraint (cid:13) (cid:13)W(i,l)(cid:13) (cid:13) = 1.
L
Henceforth, tree networks are assumed to have sign-determined activation.
Remark 7 We extend row-wise the recursive definitions (3), (4), and (5) to the cases where
X(1),X(i,1), and X(u1,···,uL−1) is X ∈ RN×d, respectively.
Definition 9 Let X(i,1)=X(u1,···,uL−1)=X. Let X˜(i) ∈ RN be X(i,L) for a rescaled parallel
network or σ(cid:0) X(i)+b(i)1(cid:1) for a rescaled tree network. The rescaled training problem is
(cid:32) (cid:88)mL (cid:33) (cid:88)mL
minL α X˜(i) +β |α |. (20)
y i i
θ∈Θ
i=1 i=1
Lemma 8 The training problem is equivalent to the rescaled problem.
Proof For sign-determined activations, rename the final-layer amplitude parameters s(i,L−1)
in parallel networks and s(i) for tree networks as q(i,L−1). By the above lemmas, in all cases,
the training problem is equivalent to
  
m θ∈i ΘnL
y(cid:88)mL
X˜(i)α
iL˜ (cid:89)−1
q(i,l) +
Lβ
˜∥θ w∥L L˜ ˜. (21)
i=1 l=1
Observe that the number of regularized parameters is |θ | = L˜. By the AM-GM inequality,
w
L1 ˜∥θ w∥L L˜
˜
= (cid:88)mL |α i|L˜ +(cid:80) lL L˜ ˜=− 11(cid:0) q(i,l)(cid:1)L˜ ≥ (cid:88)mL  |α j|L˜L˜ (cid:89)−1 (cid:16) q(i,l)(cid:17)L˜ 1/L˜ = (cid:88)mL (cid:89) |q|. (22)
i=1 i=1 l=1 i=1 q∈θw(i)
So a lower bound on (21) is
  
(cid:88)mL
(cid:89)
(cid:88)mL
(cid:89)
minL  X˜(i)sign(α ) |q|+β |q|. (23)
y  i 
θ∈Θ
i=1 q∈θw(i) i=1 q∈θw(i)
(cid:16) (cid:17)1/L˜
Letting γ(i) = (cid:81) |q| for i ∈ [m ] and making q → sign(q)γ(i) for q ∈ θ(i) in (23)
q∈θw(i) L−1 w
makes the objective of (23) the same as in (21). So, (21) and (23) are equivalent. Finally,
for each j ∈ [m ], apply a change of variables (cid:81) |q| → α so that (23) becomes (20).
L−1 q∈θw(i) i
Also, q(i,l) can be removed from θ.
30Lemma 9 A lower bound on the rescaled training problem (20)is
(cid:12) (cid:12)
max −L∗(λ) s.t. max(cid:12)λTX˜(cid:12) ≤ β, (24)
y (cid:12) (cid:12)
λ∈RN θ∈Θ
where X˜ = X˜(1) and f∗(x) := max (cid:8) zTx−f(x)(cid:9) is the convex conjugate of f.
x
Proof Find the dual of (20), by rewriting (20) as
(cid:88)mL
minL (z)+β||α|| , s.t. z = α X˜(i). (25)
y 1 i
θ∈Θ
i=1
(cid:88)mL
The Lagrangian of problem (25) is L(λ,θ) = L (z)+β||α|| −λTz+ λTX˜(i)α . Minimize
y 1 i
i=1
the Lagrangian over z and α and use Fenchel duality Boyd and Vandenberghe (2004). The
dual of (25) is
(cid:12) (cid:12)
max −L∗(λ) s.t. max(cid:12)λTX˜(i)(cid:12) ≤ β,i ∈ [m ]. (26)
y (cid:12) (cid:12) L
λ∈RN θ∈Θ
In the tree and parallel nets, X˜(i) is of the same form for all i ∈ [m ]. So the m constraints
L L
in (26) collapse to just one constraint. Then we can write (26) as (24).
If the network has a parallel architecture, let X(l) be defined as in (3), where X(1) = X
(Remark 7). This makes the lower bound problem (24) for a parallel network equivalent to
(cid:12) (cid:12)
max −L∗(λ) s.t. max(cid:12)λTX(L)(cid:12) ≤ β. (27)
y (cid:12) (cid:12)
λ∈RN θ∈Θ
Appendix C. Deep narrow networks with data dimension d = 1
In this section, assume d = 1 and the neural net is a deep narrow network. Let
w(l) = W(l) ∈ {−1,1} and b(l) = b(l) ∈ R. Then X(l+1) = σ(cid:0) X(l)w(l)+b(l)1(cid:1) ∈ RN. In the
next results involving (27), let c ,c be defined as in Appendix A.1.
1 2
Remark 8 Letb = b(L−1),a = X(L−1) w(L−1),g (b) = σ(a +b). Letg(b) = (cid:80)N λ g (b)
n n n n n=1 n n
N
(cid:91)
= λTX(L). Let I be the set of breakpoints of g and I = I , which contains the break-
n n n
n=1
points of g. Observe g(b) =
(cid:80)N
λ c(a +b) =
cb(cid:80)N
λ
+(cid:80)N
λ a c for b large enough
n=1 n n n=1 n n=1 n n
(with c = c ) and for b small enough (with c = c ). So c = c = 0 or
(cid:80)N
λ = 0 if and
2 1 1 2 n=1 n
only if g is bounded, if and only if g has a (finite) maximizer and minimizer. In this case,
assuming g is not a constant function, I contains a maximizer and minimizer of g.
Let R denote reflections as defined in Definition 2.
(a,b)
Lemma 10 Suppose λT1 = 0 if c ≠ 0 or c ≠ 0. Consider a deep narrow network with
1 2
L > 2 only for ReLU activation, and d = 1. For l ∈ {L−1,L−2,L−3}, there is n(l) ∈ [N]
such that b(l) = −X( nl () l)w(l) or b(L−3) = w(L−3)R(cid:16) X(L−3) ,X(L−3) (cid:17) are optimal b(l) values in
n(L−3) n(L−2)
the maximization constraint in (27).
31f,g f +g (±(f +g))
+
f =(x−a) f =Ramp+ (x) f =Ramp+ (x)
+ a,c a,c
Ramp+ (x) Ramp+ (x) Ramp+ (x)
a,b b,c a,b
a b a b c a b c
g =(x−b) g =Ramp+ (x) g =Ramp+ (x)
+ a,b b,c
f =Ramp+ (x) t (x)
a,b a,b,R(a,b)
t (x) f =Ramp+ (x)
a,b,c a,b
a b R (a,b)
a b c a b c Ramp+ R(a,b),c(x)
g =Ramp+ (x)
b,c g =Ramp+ (x) R c
b,c (a,b)
Figure 13: Sums, scalings, and ReLU’s applied to capped ramps. For a,b,c ∈ R, the function
t (x) is 0 if x ≤ a, x−a if x ∈ [a,b], R −x if x ∈ [b,c] and R −c if x ≥ c.
a,b,c (a,b) (a,b)
(cid:16) (cid:17)
Proof The dual constraint’s objective is λTX(L) = (cid:80)N λ σ X(L−1) w(L−1)+b(L−1) .
n=1 n n
(cid:16) (cid:17)
The breakpoints of g (cid:0) b(L−1)(cid:1) = σ X(L−1) w(L−1)+b(L−1) occur where they make the
n n
argumentofanactivationzero,andbyRemark8andtheassumptioninvolvingλT1 = 0,these
breakpoints contain an optimal b(L−1). Therefore for some n(L−1) ∈ [N], −X(L−1) w(L−1) is
n(L−1)
an optimal b(L−1). Plugging in this optimal b(L−1) makes λTX(L−1)=
N
(cid:88)
λ
σ(cid:16) w(L−1)(cid:16) X(L−1)−X(L−1) (cid:17)(cid:17)
=
n n n(L−1)
n=1 (28)
N
(cid:88)
λ
σ(cid:16) w(L−1)(cid:16) σ(cid:16) X(L−2)w(L−2)+b(L−2)(cid:17) −σ(cid:16) X(L−2) w(L−2)+b(L−2)(cid:17)(cid:17)(cid:17)
.
n n n(L−1)
n=1
.
Now assume σ(x) = ReLU(x). Setting x = b(L−2),a = −X(L−2) w(L−2),
n
(cid:16) (cid:17) (cid:16) (cid:17)
b = −X(L−2) w(L−2),f = σ X(L−2) w(L−2)+b(L−2) and g = σ X(L−2) w(L−2)+b(L−2) ,
n(L−1) n n(L−1)
the top right plot of Figure 13 shows that as a function of b(L−2), for all n ∈ [N], X(L) is
n
bounded and has breakpoints of the form −X(L−2) w(l−2) for n(L−2) ∈ [N]. So by Remark 8,
n(L−2)
there exists n(L−2) ∈ [N] for which b(L−2) = −X(L−2) w(l−2) is optimal. Plugging in this
n(L−2)
32optimal b(L−2) into (28) makes λTX(L) =
N
(cid:88)
λ
σ(cid:16) w(L−1)(cid:16) σ(cid:16) w(L−2)(cid:16) X(L−2)−X(L−2) (cid:17)(cid:17) −σ(cid:16) w(L−2)(cid:16) X(L−2) −X(L−2) (cid:17)(cid:17)(cid:17)(cid:17)
=
n n n(L−2) n(L−1) n(L−2)
n=1
N
(cid:88)
λ
σ(cid:16) w(L−1)(cid:16) σ(cid:16) w(L−2)(cid:16) σ(cid:16) w(L−3)X(L−3)+b(L−3)(cid:17) −σ(cid:16) w(L−3)X(L−3) +b(L−3)(cid:17)(cid:17)(cid:17)
n n n(L−2)
n=1
(cid:16) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)(cid:17)
−σ w(L−2) σ w(L−3)X(L−3) +b(L−3) −σ w(L−3)X(L−3) +b(L−3) .
n(L−1) n(L−2)
Setting x = b(L−3), the first plot of Figure 13 graphs the difference of ReLU functions and
(cid:16) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:17)
shows that σ w(L−2) σ w(L−3)X(L−3) +b(L−3) −σ w(L−3)X(L−3) +b(L−3) and
n n(L−2)
(cid:16) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:17)
σ w(L−2) σ w(L−3)X(L−3) +b(L−3) −σ w(L−3)X(L−3) +b(L−3) are ramps. The
n(L−1) n(L−2)
rest of the plots in Figure 13 graph the difference of ramps the cyan plots show that as a
function of b(L−3), X(L) is bounded and has breakpoints of the form b(L−3) = −X(L−3) w(l−3)
n n(L−3)
or b(L−3) = w(L−3)R(cid:16) X(L−3) ,X(L−3) (cid:17) for some n(L−3) ∈ [N]. By Remark 8, there exist
n(L−3) n(L−2)
points of this form for the optimal b(L−3).
Let M(2),M(3),M be dictionary index sets as defined in Definition 3.
Definition 10 Let M˜(1) = {−1,1}L−1, M˜ = M˜(1)×M(2)×M(3).
Given (s,j,k) ∈ M˜, recursively define the recursive dictionary function f˜(l+1,σ) (x) =
s,j,k
(cid:40)
(cid:16) (cid:16) (cid:17)(cid:17) R if l = 1,k = 1
σ s
l
f˜ s( ,l j, ,σ k) (x)−f˜ s( ,l j, ,σ k)(cid:0) a˜(l)(cid:1) where f˜ s( ,1 j, ,σ k) (x) = x and a˜(l) = x(x ej1 ls,x ej .2) .
j
l
Remark 9 For l ∈ [3], the (3l)th rows of Figure 14 list and plot possible graphs of f˜(l+1,σ) .
s,j,k
Lemma 11 For L = 2,3,4 layers, where L ∈ {3,4} only if the activation is ReLU, the
maximization constraint in (27) is equivalent to
(cid:12) (cid:12)
N
∀(s,j,k) ∈ M˜, (cid:12) (cid:12)(cid:88) λ f˜(L,σ) (x )(cid:12) (cid:12) ≤ β
(cid:12) n s,j,k n (cid:12) (29)
(cid:12) (cid:12)
n=1
1Tλ = 0 if c ̸= 0 or c ̸= 0.
1 2
Moreover, M˜ can be replaced by M in (29).
Proof By Lemma 10, (27) remains equivalent if X(L) is defined by X(1) = X and
(cid:16) (cid:16) (cid:17)(cid:17)
s x −R if k = 1,l = 1
X( nl+1) =  (cid:16)
s
l1 (cid:16) Xn
( nl)
−X(x
(
jj l1 )(cid:17),x (cid:17)j2) +
else
= f˜ s( ,l j+ ,k1,σ) (x n), (30)
l +
This gives (29).
33Now, if the activation is symmetric, then f˜(L,σ) is invariant under the sign of the com-
s,j,k
ponents of s. Next, recall x > ··· > x and sign(0) = 1. If the activation is sign, then
1 N
for all n ∈ [N −1], f˜(L=2) (X) = σ(X−x ) = (cid:0) 1T ,−1T (cid:1) = −σ(x −X) =
s=1,j=n,k=0 n 1:n n+1:N n+1
−f˜(L=2) . And f˜(L=2) (X) = 1 = f˜(L=2) (X). So for L = 2 with symmet-
s=−1,j=n+1,k=0 s=1,j=N,k=0 s=−1,j=1,k=0
ric or sign activation, (29) is unchanged if s ∈ {−1,1} is restricted to be 1, and therefore M˜
can be replaced by M.
Lemma 12 Let A be a matrix with columns
f˜(L,σ)
(X) for all (s,j,k) ∈ M . Replace the
s,j,k
maximization constraint in (27) with (29). The dual of (27) then is
minL (Az+ξ1)+β∥z∥ , where ξ = 0 if c = c = 0. (31)
y 1 1 2
z,ξ∈R
Proof Problem (27) can be written as
− min L∗ y(λ) s.t. λT1 = 0 if c 1 ̸= 0 or c 2 ̸= 0, and |λTA| ≤ β1T . (32)
λ∈RN
The Lagrangian of the negation of (32) with bidual variables z,ξ is
L(λ,z,ξ) = L∗(λ)−λT(Az+ξ1)−β∥z∥ , where ξ = 0 if c = c = 0. (33)
y 1 1 2
Equation (33) holds because the constraint |λTA| ≤ β1T i.e., λTA−β1T ≤ 0T,−λTA−
β1T ≤ 0T, appears in the Lagrangian as λTA(cid:0) z(1)−z(2)(cid:1) −β1T (cid:0) z(1)+z(2)(cid:1) with bidual
variables z(1),z(2), which are combined into one bidual variable z = z(1)−z(2). This makes
z(1)+z(2) = ∥z∥ . Changing variables z,ξ → −z,−ξ gives (33). Since L∗∗ = L Borwein and
1
Lewis (2000), inf L(λ,z,ξ) = −L −β∥z∥ and negating its maximization gives (31).
λ y 1
(cid:91) (cid:110) (cid:111) (i) (i)
Remark 10 Let a ∈ {x ,··· ,x }∪ R and let a = a ,a = a as
1 1 N (xj1,xj2) 2 2 3 3
j1,j2∈[N]
defined in Definition 6. By case analysis on the arguments of ReLU, the legend labeling
the breakpoints in Figure 14 simplifies to the legend in Figure 5. In Definition 10, if l = 1
then a˜(1) = a as defined in Definition 4. By Definition 6, a ≤ a , so the second and fifth
1 3 2
branches in the sixth row of arrows of Figure 14 do not occur. Therefore the (3l)th rows of
Figure 14 for l ∈ [3] constitute Figure 5. Remark 9 and simplifying the legend in Figure 5
gives
f˜(l,σ)
=
f(l,σ)
as defined in Definition 4.
s,j,k s,j,k
Remark 11 In Remark 10, the observations made in Figure 5 to get
f˜(l,σ)
=
f(l,σ)
are the
s,j,k s,j,k
following.
If s = 1,s = −1:
2 3

f(l,σ)
=
Ramp+
min{a,xj2,xj3},min{a,xj2}
if s
1
= −1
s,j,k Ramp−
max{a,xj2},max{a,xj2,xj3}
if s
1
= 1
34f(x) possible graphs of f(x) where a 2,a 3≥0 and s
l
∈{−1,1}
(x−a1)
s1(x−a1)
a1
a1−a2 a1+a2
(s1(x−a1))
+
a1−a2−a3 a1+a2+a3
a1−a2+a3 a1+a2−a3
(s1(x−a1)) +−a2
(cid:0) (cid:1)
s2 (s1(x−a1)) +−a2
(cid:0) (cid:0) (cid:1)(cid:1)
s2 (s1(x−a1)) +−a2
+
(cid:0) (cid:0) (cid:1)(cid:1)
s2 (s1(x−a1)) +−a2 +−a3
(cid:16)(cid:0) (cid:0) (cid:1)(cid:1) (cid:17)
s3 s2 (s1(x−a1)) +−a2 +−a3
(cid:16) (cid:16)(cid:0) (cid:0) (cid:1)(cid:1) (cid:17)(cid:17)
s3 s2 (s1(x−a1)) +−a2 +−a3
+
Figure 14: For l ∈ {0,1,2}, the (3l+1)th row of arrows point to the right and left for the
cases s ≥ 0 or s ≤ 0, respectively. The sixth row of arrows point to the right or left for
l+1 l+1
the cases c ≥ b or c ≤ b, respectively.
If s(2) = −1,s(3) = 1:

f(l,σ)
=
Ramp+
min{a,max{xj3,min{a,xj2}}},a
if s
1
= −1
s,j,k Ramp−
a,max{a,min{xj3,max{a,xj2}}}
if s
1
= 1.
If s(2) = −1,s(3) = −1:

Ramp− if s(1) = −1
f(l,σ)
=
min{a,xj2},min{a,max{xj3,min{a,xj2}}}
s,j,k Ramp+ if s(1) = 1.
max{a,min{xj3,max{a,xj2}}},max{a,xj2}
Remark 12 In Remark 10, the labels of the colored breakpoints in Figure 14 and Figure 5
are equivalent by the following simplifications.
35(cid:40) (cid:40)
x , if a >x x , if a <x
a −a =a −(a −x ) = j2 1 j2 a +a =a +(x −a ) = j2 1 j2
1 2 1 1 j2 +
a else
1 2 1 j2 1 +
a else
1 1
=min{a ,x } =max{a ,x }
1 j2 1 j2
(cid:16) (cid:17) (cid:16) (cid:17)
a −a −a =a −a − (a −x ) −a a +a +a =a +a + (x −a ) −a
1 2 3 1 2 1 j3 + 2
+
1 2 3 1 2 j2 1 + 2
+
(cid:40) (cid:40)
x if a −x >a x if x −a >a
= j3 1 j3 2 = j3 j3 1 2
a −a else a +a else
1 2 1 2
=min{x ,a −a }=min{a ,x ,x } =max{x ,a +a }=max{a ,x ,x }
j3 1 2 1 j2 j3 j3 1 2 1 j2 j3
(cid:16) (cid:17) (cid:16) (cid:17)
a −a +a =a −a − −(a −x ) +a a +a −a =a +a − −(x −a ) +a
1 2 3 1 2 1 j3 + 2
+
1 2 3 1 2 j3 1 + 2
+
 
x if 0<a −x <a x if 0<x −a <a
 j3 1 j3 2  j3 j3 1 2
= a −a if a −x >a = a +a if x −a >a
1 2 1 j3 2 1 2 j3 1 2
a
else
a
else .
1 1
=min{a ,max{x ,a −a }} =max{a ,min{x ,a +a }}
1 j3 1 2 1 j3 1 2
=min{a ,max{x ,min{a ,x }}} =max{a ,min{x ,max{a ,x }}}
1 j3 1 j2 1 j3 1 j2
Proof [Lemma 1] We simplify Definition 4 when L = 4 by considering the values of s for
which f(L,σ) has a breakpoint at a reflection a = R . When s(2) = s(3) = 1, the result
s,j,k 1 (xj1,xj2)
follows. Otherwise, f(L,σ) is
s,j,k
• Ramp+ when s = (−1,1,−1) for any x ≤ R ≤ x ≤ x or when
xj3,R
(xj1,xj2)
j3 (xj1,xj2) j2 j1
s = (−1,−1,1) for any x ≤ x ≤ x ≤ R .
j1 j2 j3 (xj1,xj2)
• Ramp− when s = (1,1,−1) for any x ≤ x ≤ R ≤ x or when
R (xj1,xj2),xj3 j1 j2 (xj1,xj2) j3
s = (1,−1,1) for any x ≥ x ≥ x ≥ R .
j1 j2 j3 (xj1,xj2)
• Ramp+ when s = (1,−1,−1) for any x ≤ x
R (xj1,xj2),xj2 j2 j1
• Ramp− when s = (−1,−1,−1) for any x ≥ x
R (xj2,xj1),xj2 j2 j1
The first two cases give Ramp and the last two cases give Ramp .
xj3,R
(xj1,xj2)
R (xj1,xj2),xj2
Proof [Theorem 1] By Lemma 12, problem (31) is a lower bound on the training problem (1),
where the Lasso features are f˜(L,σ). Let (z∗,ξ∗) be a Lasso solution. By the equivalent ex-
s,j,k
pressions for f˜(L,σ) in Figure 14 (see Remark 9), the parameters formed by the reconstruction
s,j,k
defined in Definition 6 without unscaling achieves the same objective in the rescaled training
problem, as (z∗,ξ∗) does in the Lasso objective. Parameter unscaling makes them achieve
the same objective in the training problem (see Remark 13). By Remark 10, f˜(L,σ) = f(L,σ)
s,j,k s,j,k
and so the Lasso problem in Theorem 1 is equivalent to the non-convex training problem.
36Remark 13 For sign-determined activations, by Remark 5, the inner weights can be unregu-
larized. So, reconstructed parameters (as defined in Definition 6) that are unscaled according
to either definition of unscaling in Section 2 achieve the same objective in the training problem
as the optimal value of the rescaled problem.
Appendix D. Deep neural networks with sign activation
In this section, we assume σ(x) = sign(x). First we discuss parallel architectures.
Definition 11 Define the hyperplane arrangement set for a matrix Z ∈ RN×m as
H(Z) := {σ(Zw+b1) : w ∈ Rm,b ∈ R} ⊂ {−1,1}N. (34)
LetS bethesetofcolumnsofX. Let{S }L−1 beatupleofsetssatisfyingS ⊂ H([S ])
0 l l=1 l l−1
and |S | = m . Let A (X) be the union of all possible sets S .
l l Lpar L−1
In Definition 11, since m = 1 in a parallel network, S contains one vector.
L−1 L−1
The set H(Z) denotes all possible {1,−1} labelings of the samples {z }N by a linear
i i=1
classifier. Its size is upper bounded by |H(Z)| ≤ 2(cid:80)r−1(cid:0)N−1(cid:1) ≤
2r(cid:16) e(n−1)(cid:17)r
≤ 2N, where
k=0 k r
r := rank(Z) ≤ min(N,m) Cover (1965); Stanley et al. (2004).
Lemma 13 The lower bound problem (27) is equivalent to
max−L∗(λ), s.t. max |λTh| ≤ β. (35)
y
λ h∈ALpar(X)
Proof For l ∈ [L], there is S ⊂ H(cid:0) X(l−1)(cid:1) with X(l) = [S ]. Recursing over l ∈ [L]
l−1 l−1
gives (cid:8) X(L) : θ ∈ Θ(cid:9) = A (X). So, the constraints of (27) and (35) are the same.
Lpar
Remark 14 Without loss of generality (by scaling by −1), assume that the vectors in H(Z)
start with 1. Under this assumption, Lemma 13 still holds.
Lemma 14 Let A = [A (X)]. The lower bound problem (35) is equivalent to
Lpar
min L (Az)+β||z|| . (36)
y 1
z
Proof Problem (35) is the dual of (36), and since the problems are convex with feasible
regions that have nonempty interior, by Slater’s condition, strong duality holds Boyd and
Vandenberghe (2004).
Remark 15 The set A consists of all possible sign patterns at the final layer of a parallel
L,par
neural net, up to multiplying by −1.
Lemma 15 Let A be defined as in Lemma 14. Let z be a solution to (36). Suppose
m ≥ ∥z∥ . There is a parallel neural network satisfying f (X;θ) = Az which achieves the
L 0 L
same objective in the rescaled training problem (20) as z does in (36).
37Proof By definition of A (Definition 11), for every A ∈ A (X), there are tuples
L,par i L
(cid:8) W(i,l)(cid:9)L−1 ,(cid:8) b(i,l)(cid:9)L−1 of parameters such that A = X(i,L). Let I = {i : z ̸= 0}. For
l=1 l=1 i i
i ∈ I, set α = z . This gives a neural net f (X;θ) = (cid:80) α X(i,L) = Az with |I| ≤ m .
i i L i∈I i L
Remark 16 Lemma 15 analogously holds for the tree network by a similar argument: by
construction of A , there is a neural net satisfying f (X;θ) = Az.
Ltree L
Proposition 7 For L-layer parallel networks with sign activation, the Lasso problem (36)
problem and the original training problem are equivalent.
Proof By Lemma 14, the Lasso problem is a lower bound for the training problem. By the
reconstruction in Lemma 15 (see Remark 13), the lower bound is met with equality.
Definition 12 Recall the set H defined in (34). Define a matrix-to-matrix operator
 
(cid:91)
J(m)(Z) :=  H(Z S). (37)
|S|=m
For L = 2, let A (X) = H(X) and for L ≥ 2, let A (X) be the set of columns in
Ltree Ltree
J(mL−1)◦···◦J(m2)(H(X)).
The columns of J(m)(Z) are all hyperplane arrangement patterns of m columns of Z.
Lemma 16 For L ≥ 3, the lower bound problem (24) for tree networks is equivalent to
max −L∗(λ), s.t. max |λTh| ≤ β. (38)
y
z∈RN h∈ALtree(X)
Proof Let u be a tuple such that u = 1. First suppose u has length L − 2. For all
1
nodes i, (cid:8) σ(cid:0) Xw(u+i)+b(u+i)1(cid:1) : w(u+i) ∈ Rd,b(u+i) ∈ R(cid:9) = H(X) independently of any
other sibling nodes j ̸= i. So every X(u) is the linear combination of m columns in
2
H(X), with the choice of columns independent of other u of the same length. Next, for all
u of length L−3, the set of all possible σ(cid:0) X(u+i)+b(u+i)1(cid:1) is J(m2)(H(X)). Repeating
thisfordecreasinglengthsofuuntiluhaslength1givesX˜ = σ(cid:0) X(i)+b(i)1(cid:1) = A (X).
Ltree
D.1 Assume data is in 1-D
We will refer to a switching set and a rectangular network (defined in Section 3.2 and
Section 2.2).
Lemma 17 Let m ,m ∈ N,k ∈ [m m ]. A sequence {h } in {−1,1} that starts with 1 and
1 2 1 2 i
switches k times is the sum of at most m sequences in {−1,1} that switch at most m times,
2 1
and the all-ones sequence.
38(cid:108) (cid:109)
Proof Suppose h switches at i < ··· < i . Let Q = k ≤ m . For q ∈ [Q],
i 1 k m1 2
(cid:110) (cid:111)
let h(q) be a sequence in {−1,1} that starts with (−1)q+1 and switches precisely at
i
i ∈ {I ,··· ,I } satisfying i = j mod m , which occurs at most m times. Let s = (cid:80) h(q).
1 k 2 1 i j i
Then s = 1{Q odd} ∈ {h ,h −1}. For i > 1,
1 1 1
 s +2 if h(q) = −1,h(q) = 1 for some q
  i−1 i−1 i
s = s −2 if h(q) = 1,h(q) = −1 for some q
i i−1 i−1 i

s else
i
So {s } is a sequence in {0,−2} or {−1,1} that changes value precisely at i ,...,i . There-
i 1 k
fore {s } is either {h } or {h −1}.
i i i
Lemma 18 Let p,m ∈ N. Let z ∈ {−1,1}N with at most pm switches. There is an integer
n ≤ m, w ∈ {−1,1}n, and a N ×n matrix H with columns in H(p) such that z = σ(Hw).
Proof For x ∈ {−1,1},σ(x) = σ(x−1). Apply Lemma 17 with m = p,m = m.
2 1
Lemma 19 H(X) consists of all columns in H(1).
Proof First, 1 = σ(0) = σ(X ·0) ∈ H(X). Next, let h ∈ H(X)−{1} ∈ {−1,1}N. By
definition of H(X), there exists w,b ∈ R such that h = Xw+b1. Note h = 1 (Remark 14).
1
Let i be the first index at which h switches. So x w+b < 0 ≤ x w+b, which implies
i i−1
x w < x w. For all j > i, x < x so h = σ(x w +b) ≤ σ(x w +b) = σ(h ) = −1, so
i i−1 j i j j i i
h = −1. So h switches at most once.
j
Now, let h ∈ {−1,1}N with h = 1. Suppose h switches once, at index i ∈ {2,··· ,N}.
1
In particular, h = −1. Let w = 1,b = −x . Then at j < i, x w +b = x −x ≥ 0
i i−1 j j i−1
soh = σ(x w+b) = 1. Andforj ≥ i,x w+b = x −x < 0soh = −1. Soh ∈ H(X).
j j j j i−1 j
Proposition 8 The set A
L=3,par
contains all columns of H(m1).
Proof Note A = (cid:83) H(cid:0) X(1)(cid:1). From Lemma 19, any possible column in X(1)
L=3,par X(1)
switches at most once. Apply Lemma 18 with p = 1,m = m (so that mp = m switches),
1 1
to any column z of X(1).
Proposition 9 For a rectangular network, A L,par(X) is contained in the columns of H(m1).
Proof Let W(1) ∈ R1×m1. For any w,b ∈ R, since the data is ordered, σ(Xw+b1) ∈
{−1,1}N has at most 1 switch. Then X(1) = σ(cid:0) XW(1)+1·b(1)(cid:1) has m columns each
1
with at most one switch. Therefore X(1) has at most m +1 unique rows. Let R be the set
1
39of the smallest index of each unique row. We claim that for all layers l ∈ [L], the rows of X(l)
are constant at indices in [N]−R, that is, for all i ∈ [N]−R, the ith and (i−1)th rows of
X(l) are the same. We prove our claim by induction. The base case for l = 1 already holds.
Suppose our claim holds for l ∈ {1,··· ,L−1}. Let W(l+1) ∈ R(m l×m l+1). The rows of
X(l) are constant at indices in [N]−R, so for any w ∈ Rm l,b ∈ R, the elements of the vector
X(l)w+b1 are constant at indices in [N]−R. This held for any w ∈ Rm l, so the rows of
X(l+1) = X(l)W(l+1)+1·b(l+1) are again constant at indices in [N]−R. By induction, our
claim holds for all l ∈ [L]. So X(L) has at most |R| ≤ m +1 unique rows and hence has
1
columns that each switch at most m times.
1
Proposition 10 For a rectangular network, A L,par(X) contains all columns of H(m1).
Proof Letz ∈ {−1,1}N withatmostmin{N−1,m }switches. ByProposition8,thereexists
1
a feasible X(1) = σ(cid:0) XW(1)+1·b(1)(cid:1) ∈ RN×m and W(2) ∈ Rm×m such that z is a column
of X(2) = σ(cid:0) X(1)W(2)+1·b(2)(cid:1). Now for every l ∈ {3,...,L−1} we can set W(l) = I
m
to be the m×m identity matrix and b(l) = 0 so that X(l) = σ(cid:0) X(l−1)W(l)(cid:1) = X(l−1). Then
X(L) = X(2) contains z as a column. Therefore z ∈ A (X).
L,par
Lemma 20 Let K = (cid:81)L−1m . The set A (X) consists of all columns in H(K).
l=1 l Ltree
Proof For l ∈ [L], let A = A (X). Let p = (cid:81)k−1m . We claim for all l ∈ {2,··· ,L},
l ltree k l=1 l
A
l
consists of all columns in H(p l). We prove our claim by induction on l. The base case
when l = 2 holds by Lemma 19. Now suppose Lemma 20 holds when l = k ∈ {2,··· ,L−1}.
Observe A ⊂ A , so if p ≥ N −1, then Lemma 20 holds for l = k +1. So suppose
k k+1 k
p < N −1. Then A contains all vectors in {−1,1}N with at most p switches.
k k k
Let h ∈ A k+1. The set A
k+1
contains all columns in J(m k)([A k]). So, there exist
w ∈ Rm k,b ∈ R and a submatrix Z = [A k]
S
where |S| = m
k
and h = σ(Zw+b). Each of
the at most m columns of Z has at most p switches, so the N rows of Z change at most
k k
m p = p times. So Zw+b changes value, and hence h = σ(Zw+b) switches, at most
k k k+1
p times. Conversely, by Lemma 18 with p = p ,m = m , the set A of all columns in
k+1 k k k+1
J(m k)([A k]) contains all vectors with at most p km
k
= p
k+1
switches. So our claim holds for
l = k+1. By induction, it holds for l = L layers.
Proof [Theorem 2] For the parallel network, apply Proposition 7 and then apply Proposi-
tion 8 for L = 3, and Proposition 9 and Proposition 10 for L ≥ 3. For tree networks, by
Remark 16 the training problem is equivalent to Lasso lower bound with dictionary A
L,tree
given by Lemma 20.
Proof [Lemma 3] By Theorem 2 and Lemma 8, it suffices to show the parameters with-
out unscaling achieve the same objective in the rescaled problem (20) as Lasso. First,
|I| ≤ m and by Theorem 2, m(i) ≤ m so the weight matrices are the correct size.
2 1
Let S(i) be the number of times A switches until index n. Since x > ··· > x , we
n i 1 N
40(cid:18) (cid:19)
(cid:16) (cid:17)
get X(i,2) = σ(cid:0) XW(i,1)+1·b(i,1)(cid:1) = σ x −x = −σ S(i) −j . So X(i,3) =
n,j n,j n I(i)−1 n n
j
 
(cid:16) (cid:17)
(cid:88)Sn(i) m (cid:88)(i)
(cid:110) (cid:111)
σ X(i,2)W(i,2)+b(i,2)1 = σ (−1)(−1)j+1+ (1)(−1)j+1−1 m(i) odd  =
 
n
j=1 j=Sn(i)+1
σ(cid:16) −2·1(cid:110) S(i) odd(cid:111)(cid:17) = (−1)Sn(i) = A . So, f (X;θ) = ξ +(cid:80) α X(i,3) = Az. And,
n n,i 3 i∈I i
||α|| = ||z∗|| = ||z∗|| . So the rescaled problem and Lasso achieve the same objective.
1 I 1 1
Remark 17 For a rectangular network, a reconstruction similar to Lemma 3 holds by setting
additional layer weight matrices to the identity.
Proof [Corollary 1 ] By Remark 4, p∗ ≤ p∗ . Let θ(L) and α(L) denote θ and α
L=3,β L=2,β
for a L-layer net. Since the training and rescaled problems have the same optimal value,
to show p∗ ≤ p∗ , it suffices to show for any optimal θ(3), there is θ(2) with
f 2(cid:0) X;θ(2)(cid:1)L= =2, fβ 3(cid:0) X;θL (= 33 )(cid:1),m a1 nβ d (cid:13) (cid:13)α(2)(cid:13) (cid:13)
1
≤ m 1(cid:13) (cid:13)α(3)(cid:13) (cid:13) 1. Let z∗ be optimal in the 3-layer Lasso
problem(2). Letm∗ 3 = ||z∗|| 0 andletz ∈ Rm∗ 3 bethesubvectorofnonzeroelementsofz∗. Let
m = m 1m∗ 3. By Lemma 3 and its proof, there are W(i,1) ∈ R1×m1,b(i,1) ∈ R1×m1,W(i,2) ∈
{1,−1,0}m1,b(i,2) ∈ R such that X(i,2)W(i,2)+b(i,2)1 ∈ {−2,0}N and f 3(X;θ) =
m∗ m∗
(cid:88)3 (cid:16) (cid:17) (cid:88)2 (cid:16) (cid:17)
z σ X(i,2)W(i,2)+1·b(i,2) = z∗ X(i,2)W(i,2)+1·b(i,2)+1 =
i i
n=1 i=1
m∗
(cid:88)3 (cid:16) (cid:16) (cid:17) (cid:17)
z∗ σ XW(i,1)+1·b(i,1) W(i,2)+1·b(i,2)+1 =
i
i=1
 
 z W(1,2) 
1 m∗
σ
 X
(cid:104)
W(1,1)···W(m∗
3,1)(cid:105) +1·(cid:104)
b(1,1),··· ,b(m∗
3,1)(cid:105)
 

.
. . 
+1(cid:88)3
z
i(cid:16) 1+b(i,2)(cid:17)
,
 ((cid:124) W(1,1),···,W(cid:123)(cid:122) (m,1))∈R1×(cid:125) m (cid:124) (b(1,1),···,b(cid:123) (m(cid:122) ,1))∈R1×m(cid:125) z m∗ 3W(m∗ 3,2) (cid:124)i=1 (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) ξ
α(2)
which is f (cid:0) X;θ(2)(cid:1) with m neurons. And ∥α(2)∥ ≤ m ∥z∗∥ = m ∥α(3)∥ .
2 1 1 1 1 1
D.2 2-D data
Proof [Theorem 3] Lemma 14 holds for any dimension d, so the Lasso formulation in
Theorem 1 and Theorem 2 similarly hold for d > 1 but with a different dictionary A
L,par
and matrix A.
Let x′ = ∠x(n) and order the data so that x′ > x′ > ...x′ . Let X′ = (x′ ···x′ ) ∈ RN.
n 1 2 N 1 N
Let w ∈ R2 with ∠w ∈ (cid:2)π, 3π(cid:3). Let w′ = ∠w. Note wx(n) ≥ 0 if and only if x′ ∈
2 2 n
(cid:2) w′− π,w′+ π(cid:3). Since x′ < π for all n ∈ [N], this condition is equivalent to x′ ≥ w′− π,
2 2 n n 2
ie n ≤ max(cid:8) n ∈ [N] : x′ ≥ w′− π(cid:9). So (cid:8) σ(X′w) : w′ ∈ (cid:2)π, 3π(cid:3)(cid:9) = H(1)∪{−1}. Similarly
n 2 2 2
41(cid:8) σ(X′w) : w′ ∈ (cid:2) −π, π(cid:3)(cid:9)isthe"negation"ofthisset. Therefore, theL = 2trainingproblem
2 2
is equivalent to the Lasso problem with dictionary H(1). Proposition 10 holds analogously
for this training data, so for L > 2, the dictionary is H(mL−1).
Proof [Lemma 4] Observe n ≤ i if and only if x(n)W(i,1) ≥ 0 and so σ(cid:0) XW(i,1)(cid:1) =
(cid:2) 1T,−1T (cid:3)T = A . Thus f (X;θ) = (cid:80) α σ(cid:0) XW(i,1)(cid:1) = Az∗ so θ achieves the same
i N−i i L=2 i i
objective in the rescaled training problem (20), as the optimal value of Lasso (2). Unscal-
ingoptimalparametersoftherescaledproblemmakesthemoptimalinthetrainingproblem.
Appendix E. Solution sets of Lasso under minimal regularization
Remark 18 For each optimal z∗ of the Lasso problem, minimizing the objective over ξ gives
the optimal bias term as ξ∗ = (cid:0) y−11Ty(cid:1) −(cid:0) A−11TA(cid:1) z∗.
Remark 19 For a neural net with L = 2 layers and sign activation, by Theorem 1 the
Lasso problem has an objective function f(z) = 1∥Az−y∥2+β∥z∥ where A ∈ RN×N. By
2 2 1
Lemma 21, A is full rank, which makes f strongly convex. Therefore the Lasso problem has
a unique solution z∗ Boyd and Vandenberghe (2004). Moreover, for any Lasso problem, z∗
satisfies the subgradient condition 0 ∈ δf(z) = AT(Az∗−y)+β∂∥z∗∥ . Equivalently,
1
(cid:40)
1 {−sign(z∗)} if z∗ ̸= 0
AT(Az∗−y) ∈ n n , n ∈ [N].
β n [−1,1] if z∗ = 0
n
Proof[Proposition4]Recallthate(n) isthenth canonicalbasisvectorasdefinedinSection1.2.
WeanalyzethesolutionsetofAz+ξ1 = y. Wenotethat(cid:0) I −11T/N(cid:1) Az = (cid:0) I −11T/N(cid:1) y..
As z∗ is optimal in (9) , this implies that (I −11T/N)Az∗ = (I −11T/N)y. This implies
that (I −11T/N)A(z−z∗) = 0. As x > x > ··· > x , we have A(cid:0) e(1)+e(N)(cid:1) ∝ 1. As
1 2 N
A is invertible by Lemma 22 in Appendix F, this implies that there exists t ∈ R such that
z−z∗ = t(cid:0) e(1)+e(N)(cid:1). It is impossible to have z∗z∗ > 0 from the optimality of z∗. Other-
1 N
wise,bytakingt = −sign(z∗)min{|z∗|,|z∗|},wehave∥z∥ = ∥z∗∥ −2min{|z∗|,|z∗|} < ∥z∗∥ .
1 1 n 1 1 1 n 1
Therefore, we have z∗z∗ ≤ 0. We can reparameterize z = z∗+tsign(z∗)(e(1)+e(N)). It is
1 n 1
easy to verify that for t such that −|z∗| ≤ t ≤ |z∗|, we have ∥z∥ = ∥z ∥ , while for other
1 n 1 n 1
choice of t, we have ∥z∥ > ∥z ∥ . Therefore, the solution set of (9) is given by (10).
1 n 1
Proof [Proposition 5] Follows from Remark 19 describing the Lasso objective.
Proof [Lemma 2] The result follows from the definition of A, the assumption that the data
is ordered and that the output of sign activation is σ is ±1 for sign activation.
Proof [Proposition 6] By Lemma 23, for n ∈ [N − 1], z ∗ = y − y ≥ 0 and
+n n n−1
z∗ = y ≥ 0. So z∗ achieves an objective value of ∥z∗∥ = y in (9). Now let z
+N N 1 1
be any solution to (9). Then Az = y. Since the first row of A is [1T,0T], we have
42y = (Az) = 1Tz ≤ ∥z ∥ ≤ ∥z∥ ≤ ∥z∗∥ = y . So ∥z ∥ = ∥z∥ = y , leaving
1 1 + + 1 1 + 1 1 1
z = 0 = z∗. Therefore z = A−1y = z∗. Applying Lemma 23 gives the result.
− − + +
Proof [Corollary 2 ] By Lemma 21, Lemma 22, Lemma 23 and Lemma 24, the dictionary
matrix for the 2-layer net is full rank for sign, absolute value, threshold and ReLU acti-
vations. The dictionary matrices for deeper nets with sign activation are also full rank
by Remark 4. Let u = AT(Az∗ − y). By Remark 19, as β → 0, we have u → 0, so
Az−y = (AAT)−1Au → 0. So as β → 0, the optimal Lasso objective approaches 0, and by
Theorem 2 and Theorem 1, so does the training problem. So f (X;θ) −β −→ −→0 y.
L
Proof [Lemma 5] Every ReLU deep narrow feature f(L,σ) (X) has slope of magnitude
s,j,k
at most 1. Also, y = f (X;θ) = ξ∗1 + Az∗ = ξ∗1 + (cid:80) z∗A . For any n ∈ [N − 1],
L i i i
(cid:12) (cid:12)
|µ n| = (cid:12) (cid:12) (cid:12)fL(X;θ x) nn ++ 11 −− xfL n(X;θ) n(cid:12) (cid:12)
(cid:12)
= (cid:12) (cid:12) (cid:12)(cid:80) iz i∗ x(A n+n 1+ −1, xi− nAn,i)(cid:12) (cid:12)
(cid:12)
= (cid:12) (cid:12) (cid:12)(cid:80) iz i∗f s( ,L j,, kσ)(x xn n+ +1 1)− −f xs n( ,L j,, kσ)(xn)(cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12)
≤ (cid:80) |z∗|(cid:12) (cid:12)f s( ,L j,, kσ)(xn+1)−f s( ,L j,, kσ)(xn)(cid:12) (cid:12) ≤ (cid:80) |z∗| = ∥z∗∥.
i i (cid:12) xn+1−xn (cid:12) i i
Proof [Lemma 6] Let n ∈ [N −1]. Let S+ = {i ∈ [N] : i > n,(z ) ̸= 0},S− = {i ∈ [N] :
n + i n
i ≤ n,(z ) ̸= 0}. Observe S+ = S+−{n+1} if (z ) ̸= 0 and S+ = S+ otherwise.
− i n+1 n + n+1 n+1 n
Similarly, S− = S+ ∪{n+1} if (z ) ̸= 0 and S− = S− otherwise. Now, ReLU+
has slope 1
an+ ft1
er x
an
nd ReLU− has
s− lopn+ e1
1 before x ,
n so+1
µ
=n
(cid:80) (z ) +(cid:80) (z
)xi
.
Combining this
wii
th the
prevx ii
ous observation, µ
−i
µ
n
=
−(i z∈S )n+ + +i
(z
)i∈Sn.− N− otei
n n+1 + n+1 − n+1
we used µ N = 0. So |µ n−µ n+1| = (cid:12) (cid:12)−(z +) n+1+(z −) n+1(cid:12) (cid:12) ≤ (cid:12) (cid:12)(z +) n+1(cid:12) (cid:12) + (cid:12) (cid:12)(z −) n+1(cid:12) (cid:12). So
(cid:80)N−1|µ −µ | ≤ ∥z∗∥ −|(z ) |−|(z ) | ≤ ∥z∗∥ .
n=1 n n+1 1 + 1 − 1 1
Now,foranyn ∈ [N−1],(Az+ξ1) −(Az+ξ1) = (cid:80)N (z ) (cid:0) A −A (cid:1) =
n n+1 i=1 + i +n,i +n+1,i
(cid:80)N
(z )
(cid:0)
(x −x ) −(x −x )
(cid:1)
=
(cid:80)N
(µ −µ )(x −x ) = (x −x )µ =
i=1 + i n i + n+1 i + i=n+1 i−1 i n n+1 n n+1 n
y −y . And (Az+ξ1) = ξ+(cid:80)N (z ) (x −x ) = y +(cid:80)N (z ) (0) = y . So
n n+1 N i=1 + i N i + N i=1 + i N
Az+ξ1 = y. And ∥z∥ = (cid:80)N−1|µ −µ |, which exactly hits the lower bound on ∥z∗∥ .
1 n=1 n n+1 1
Therefore z,ξ is optimal.
Remark 20 By Lemma 22, the absolute value network “de-biases" the target vector, normal-
izes is by the interval lengths x −x , and applies E (which contains the difference matrix
i i+1
∆) twice, acting as a second-order difference detector. By Lemma 21, the sign network’s
dictionary inverse contains ∆ just once, acting as a first-order difference detector.
Appendix F. Inverses of 2-layer dictionary matrices
In this section, we consider the 2-layer dictionary matrix A as defined in Remark 2.
Define the finite difference matrix
43 
1 −1 0 ··· 0 0
0 1 −1 ··· 0 0 
 
0 0 1 ··· 0 0 
∆ =  . . . . . . . . . ... . . . . . .   ∈ RN−1×N−1. (39)
 
 
0 0 0 ··· 1 −1
0 0 0 ··· 0 1
Multiplying a matrix on its right by ∆ subtracts its consecutive rows. Define the diagonal
matrix D ∈ RN×N by D = 1 for i ∈ [N −1] and D = 1 . For n ∈ N, let
i,i xi−xi+1 N,N x1−xN
   
1 1 1 ··· 1 1 1 1 ··· 1
−1 1 1 ··· 1 0 1 1 ··· 1 
   
A(s) = −1 −1 1 ··· 1,A(t) = 0 0 1 ··· 1  ∈ Rn×n. (40)
n   . . . . . . . . . ... . . .  n  . . . . . . . . . ... . . . 
   
−1 −1 −1 ··· 1 0 0 0 ··· 1
(s)
Remark 21 The dictionary matrices for sign and threshold activation satisfy A = A and
N
(t)
A = A , respectively.
+ N
Lemma 21 The dictionary matrix for sign activation has inverse
 
0
.
 . 
 ∆ . 
A−1 = 1 2  0  .
 
 −1 
1 0 ··· 0 1
Proof Multiplying the two matrices (see Remark 21) gives the identity.
Lemma 22 The dictionary matrix A for absolute value activation has inverse A−1 =
   
0 0 ··· 0 1 0
 0   . . 
 .   ∆ . 
1 2PEDE, where P =   I . .  ,E =   0  .
 N−1   
 0   −1 
0 −1 0 ··· 0 −1
(cid:40)
(x −x )−(x −x ) = x −x if i > j
Proof Fori ∈ [N−1],j ∈ [N],A −A = j i j i+1 i+1 i .
i,j i+1,j (x −x )−(x −x ) = x −x if i ≤ j
i j i+1 j i i+1
And for all j ∈ [N], A +A = (x −x )+(x −x ) = x +x . Therefore,
1,j N,j 1 j j N 1 N
   
−1 0
. .
 . . A(s)  1  . . I 
DEA =  N−1 , EDEA =  N−1 .
 −1  2  0 
   
−1 −1 ··· −1 1 0 ··· 0
44Applying the permutation P makes 1PEDEA = I, so A−1 = 1PEDE.
2 2
 
0
.
 . 
 ∆ . 
Lemma 23 The inverse of A + for threshold activation is A− +1 =   0  .
 
 −1 
0 0 ··· 0 1
Proof Multiplying the two matrices (see Remark 21) gives the identity.
Lemma 24 The submatrix [(A ) ,(A ) ] ∈ RN×N of the dictionary matrix for
+ 1:N,2:N − 1:N,1
ReLU activation has inverse E DE , where
+ −
   
0 0
. .
 .   . 
 ∆ .   ∆ . 
E + =   0  , E − =   0  .
   
 1   −1 
0 0 ··· 0 1 0 0 ··· 0 −1
Proof For i ∈ [N −1],j ∈ [N],
(cid:40)
0 if i ≥ j
(A ) −(A ) =
+ i,j + i+1,j (x −x )−(x −x ) = x −x if i < j
i j i+1 j i i+1
and (A ) −(A ) = (x −x )−(x −x ) = x −x . Observe that DE A =
− i,1 − i+1,1 1 i 1 i+1 i+1 i −
 
−1
.
 (t) . . 
 A 
  N−1 −1  , and applying E + gives I.
 
 −1 
0 0 ··· 0 1
Appendix G. Solution path for sign activation and binary, periodic labels
In this section we assume the neural net uses sign activation, and d = 1. We wil refer to
e(n) as the nth canonical basis vector, as defined in Section 1.2.
Remark 22 A neural net with all weights being 0 achieves the same objective in the training
problem as the optimal Lasso value and is therefore optimal.
We will find β . Then for β < β , we use the subgradient condition from Remark 19 to
c c
solve the Lasso problem (2). Note when L = 2, (A )T = (1 ,−1 ) switches at n+1.
n n N−n
45G.1 Assume L = 2
Lemma 25 The elements of ATA ∈ RN×N are (ATA) = N −2|i−j|.
i,j
Proof If1 ≤ i ≤ j ≤ N then(ATA) = (cid:80)i−1 A A +(cid:80)j−1A A +(cid:80)N A A =
i,j k=1 i,k j,k k=i i,k j,k k=j i,k j,k
(cid:80)i−1(1)(1)+(cid:80)j−1(−1)(1)+(cid:80)N
(−1)(−1) = (i−1)−(j −1−i+1)+(N −j +1) =
k=1 k=i k=j
N +2(i−j) = N −2|i−j|. Since ATA is symmetric, if 1 ≤ j ≤ i ≤ N then (ATA) =
i,j
(ATA) = N +2(j −i) = N −2|i−j|. So for any i,j ∈ [N], (ATA) = N −2|i−j|.
j,i i,j
Remark 23 By Lemma 25, ATA is of the form
 
N N −2 N −4 ··· 2 0 −2 ··· 6−N 4−N 2−N
N −2 N N −2 ··· 4 2 0 ··· 8−N 6−N 4−N
 
N −4 N −2 N ··· 6 4 2 ··· 10−N 8−N 6−N
ATA =    . . . . . . . . . ... . . . . . . . . . ... . . . . . . . . .   .
 
6−N 8−N 10−N ··· 2 4 6 ··· 10−N 8−N 6−N
 
4−N 6−N 8−N ··· 0 2 4 ··· 8−N 6−N 4−N
2−N 4−N 6−N ··· −2 0 2 ··· N −4 N −2 N
(41)
An example of a column of ATA is plotted in the left plot of Figure 15.
T
1
40
0.5
20
n
n
−0.5
−20
−1
Figure 15: Left: Column 10 of ATA for N = 40. Right: Vector h(T) for T = 10.
Remark 24 For n ∈ [N], (ATy) = (cid:80)n y −(cid:80)N y .
n i=1 i i=n+1 i
Definition 13 For a,b ∈ Z, denote Quot(a,b) ∈ Z and Rem(a,b) ∈ {0,··· ,b−1} as the
quotient and remainder, respectively, when a is divided by b. Define the modified remainder
(cid:40)
Rem(a,b) if Rem(a,b) > 0
rem(a,b) = ∈ [b].
b if Rem(a,b) = 0
46Next define the modified quotient
(cid:40)
a−rem(a,b) Quot(a,b) if Rem(a,b) > 0
quot(a,b) = = .
b Quot(a,b)−1 if Rem(a,b) = 0
.
(cid:40)
−1 if rem(T,n) ≤ T/2
(T)
Remark 25 The square wave has elements h =
n
1 else.
Remark 26 Since h(T) is periodic and zero mean, for i,n ≥ 0, (cid:80)nT h(T) = 0.
j=iT+1 j
Lemma 26 The vector ATh(T) is periodic with period T. For n ∈ [T],
(cid:40)
(cid:16) (cid:17) n if n ≤ T
ATh(T) = 2 2 ∈ [0,T].
n T −n else
Proof By Remark 24, Remark 26, and periodicity of h(T), for n ∈ [T],j ∈ [2k−1],

n T/2 T
   (cid:88) 1− (cid:88) 1+ (cid:88) 1 if n ≤ T
jT+n (j+1)T   2
(cid:16) ATh(T)(cid:17) = (cid:88) h(T) − (cid:88) h(T) =i=1 i=n+1 i=1+T/2
i i T/2 n T
n+jT i=jT+1 i=n+jT+1   (cid:88) 1− (cid:88) 1+ (cid:88) 1 else.



i=1 i=T+1 i=n+1
2
Simplifying gives the result.
Lemma 27 Let q =
quot(cid:0)
n,
T(cid:1)
∈ {0,··· ,2k−1}. Then
n 2
(cid:18) (cid:19)
(cid:16) (cid:17) T
ATh(T) = 2(−1)qn+1rem n, −1{q odd}T. (42)
n
n 2
Proof Follows from Lemma 26.
Corollary 5 Suppose z = e(T 2)+e(N−T 2) . Then for n ≤ T
2
and n ≥ N − T 2, 1 2(ATAz)
n
=
(cid:0) ATh(T)(cid:1) . And if T ≤ n ≤ N − T, then (ATAz) = 2T.
n 2 2 n
Proof By Lemma 25, for n ∈ [N], (ATAz)
n
= 2(cid:0) N −(cid:12) (cid:12)n− T 2(cid:12) (cid:12)−(cid:12) (cid:12)n−N + T 2(cid:12) (cid:12)(cid:1)
Simplifying and applying Lemma 26 gives the result.
Lemma 28 If y = h(T), then the critical β (defined in Section 6) is β = T .
c
47(cid:12)(cid:16) (cid:17) (cid:12)
Proof By Remark 22, β = max |ATy| = max (cid:12) ATh(T) (cid:12) = T.
c n (cid:12) (cid:12)
n∈[N] n∈[N] n
Lemma 29 Let y = h(T). If β˜ ≥ 1 then the solution to the Lasso problem (2) is z∗ =
2
(cid:16) (cid:17) (cid:16) (cid:17)
1 1−β˜ e(T 2) +e(N−T 2) .
2
+
Proof By Lemma 28, β = T. By Lemma 28, if β˜≥ 1 then z∗ = 0 as desired. Now suppose
c
1 ≤ β˜≤ 1. Let δ = 1−β˜,g = AT (Az∗−y). By Corollary 5 and Lemma 26,
2 n
 (cid:16) (cid:17) T
(δ−1) ATh(T) = −2β˜n ∈ [−β,0] if n ≤

  n 2

g =  (cid:0) δT −(cid:0) ATh (cid:1) (cid:1) = (cid:16) β −β−(ATh(T)) (cid:17) ∈ [−β,β] if T ≤ n ≤ N − T ,
k,T n c n 2 2


   (δ−1)(cid:16) ATh(T)(cid:17) = −2β˜(N −n) ∈ [−β,0] if N − T ≤ n
n 2
where the second set inclusion follows from (cid:0) ATh(T)(cid:1) ∈ [0,β ] by Lemma 26 so that −β ≤
n c
g ≤ β −β ≤ β. Therefore, |AT (Az∗−y)| ≤ β, and at n ∈ {n : z∗ ̸= 0} = (cid:8)T,N − T(cid:9),
c n n 2 2
we have AT (Az∗−y) = −β˜T = −β = −βsign(z∗). By Remark 19, z∗ is optimal.
n 2 n
Lemma 30 Let a,b,c,d ∈ Z ,d ∈ R,r = 1−rem(b−a,2). Then,
+
b (cid:18) (cid:19)
(cid:88) (b−(a+r)+1)d
(−1)j(c−jd) = (−1)a (c−ad)r+(−1)r
2
j=a (43)
(cid:40)
(b−a+1)d
if r = 0
= (−1)a 2 .
(b−a)d
c−ad− else
2
Proof We have
b b
(cid:88) (cid:88)
(−1)j(c−jd) = (−1)a(c−ad)r+ (−1)j(c−jd)
j=a j=a+r
(cid:88)
= (−1)a(c−ad)r+ (−1)j(c−jd)+(−1)j+1(c−(j +1)d)
a+r≤j≤b−1
j−(a+r)iseven
(cid:88)
= (−1)a(c−ad)r+(−1)a+r (c−jd)−(c−(j +1)d)
a+r≤j≤b−1
j−(a+r)iseven
(cid:88)
= (−1)a(c−ad)r+(−1)a+r d
a+r≤j≤b−1
j−(a+r)iseven
(cid:18) (cid:19)
b−(a+r)+1
= (−1)a (c−ad)r+(−1)r d .
2
4810 20
8
15
6
10
4
5
2
n n
Figure 16: Examples of 1 ATAz (left) and 1 ATAz (right).
2w bdry w cycle
bdry cycle
Simplifying gives (43).
Lemma 31 Consider a L = 2 layer neural network. Suppose y = h(T) and 0 < β <
(cid:16) (cid:17)
β 2c. Let w
bdry
= 1− 23β˜,w
cycle
= 2β˜−1. Let z
bdry
= w
bdry
e(T 2) +e(N−T 2) ,z
cycle
=
w cycle(cid:80) i2 =k− 22(−1)ie(T 2i) ∈ RN. Then z∗ = z
bdry
+z
cycle
solves the Lasso problem (2).
Remark 27 The nonzero indices of z and z partition those that are multiples of T.
bdry cycle 2
Proof We show z∗ is optimal using the subgradient condition in Remark 19. By Corollary 5,
1 (cid:40)(cid:0) ATh(T)(cid:1) if n ≤ T or n ≥ N − T
(ATAz ) = n 2 2 , n ∈ [N]. (44)
2w bdry bdry n T if T ≤ n ≤ N − T
2 2
Next,
w
1 (ATAz cycle)
n
= (cid:80) j2 =k− 22(−1)j(cid:0) N −2(cid:12) (cid:12)n−jT 2(cid:12) (cid:12)(cid:1). Expanding, simplifying and
cycle
comparing with Lemma 26 gives the following.
Firstsupposen ≤ T orn ≥ N−T. Then, 1 (ATAz ) = (cid:80)2k−2(−1)j(cid:0) N +2s(cid:0) n−jT(cid:1)(cid:1),
2 2 w cycle n j=2 2
cycle
where s = 1 if n ≤ T and s = −1 if n ≥ N − T. Applying Lemma 30 with a =
2 2
2,b = 2k − 2,c = N + 2sn,d = sT,r = 1 − rem(b − a,2) = 1 gives (ATAz ) =
cycle n
(cid:16) (cid:17)
w (−1)a c−ad− (b−a)d = w (N +2sn−2sT −s(k−2)T) = w (N −skT +2sn).
cycle 2 cycle cycle
Plugging in s and N = kT and applying Lemma 26 gives
(cid:40)
1 n if n ≤ T
(ATAz ) = 2 2 = (ATh(T)) . (45)
w cycle cycle n N −n if n ≥ N − T n
2
Next suppose T ≤ n ≤ N − T. Let q = quot(cid:0) n, T(cid:1) ,r = rem(cid:0) n, T(cid:1). Then
2 2 n 2 n 2
1
(cid:88)qn (cid:18) (cid:18) T(cid:19)(cid:19) 2 (cid:88)k−2 (cid:18) (cid:18) T(cid:19)(cid:19)
(ATAz ) = (−1)j N −2 n−j + (−1)j N +2 n−j .
cycle n
w 2 2
cycle
j=2 j=qn+1
49Let a = 2,b = q ,c = N −2n,d = −T,a = q +1 = b +1,b = 2k−2,c =
+ + n + + − n + − −
N +2n,d = T = −d ,r = 1{b −a even} = 1{b even},r = 1{b −a even} =
− + + + + + − − −
1{b
+
odd}. Note that (−1)1{b+ even} = (−1)b+1,(−1)1{b+ odd} = (−1)b, and n − T 2q
n
=
Rem(cid:0) n, T(cid:1). By Lemma 30, 1 (cid:0) ATAz (cid:1) =
2 w cycle cycle n
(cid:18) b −(a +1{b even })+1 (cid:19)
(−1)a+ (c −a d )1{b even }+(−1)1+b+ + + + d
+ + + + +
2
(cid:18) b −(1+b +1{b odd })+1 (cid:19)
+(−1)1+b+ (c +(1+b )d )1{b odd }+(−1)1+b+ − + + d
− + + + +
2
(cid:40) N −2n+2T + qn−2T − 2k−2−qnT = 2(cid:0) T −rem(cid:0) n, T(cid:1)(cid:1) if b = q is even
= 2 2 2 + n
1−qnT +N +2n−(1+q )T − 2k−3−qnT = T +2rem(cid:0) n, T(cid:1) if b = q is odd
2 n 2 2 + n
(cid:18) (cid:19)
T
= (2−1{q odd})T +2(−1)qn+1rem n,
n
2
= 2T −(cid:0) ATh (cid:1) ,
k,T n
where the last equality follows from Lemma 27. Combining with (45) gives
(cid:40)(cid:0) ATh(T)(cid:1) if n ≤ T or n ≥ N − T
(ATAz ) = w · n 2 2 (46)
cycle n cycle 2T −(cid:0) ATh(T)(cid:1) if T ≤ n ≤ N − N.
n 2 2
Add (44) and (46) and plug in y = h(T) to get
 (cid:16) (cid:17)
(ATAz) n= (( ww bc dy rcl ye ++ w2w cyb cd ler )y 2)( TA −T wy c) yn cl= e(A1 T− yβ˜
)
n=(A β+Ty (cid:16)) 1n
−2β˜(cid:17)
(ATy)
n
ii ff n
T
2
≤ ≤T n2 ≤or Nn −≥
N
2N .−T 2
(47)
Therefore,
(cid:40)
1 1 (ATy) if n ≤ T or n ≥ N − T
− AT(Az−y) = βc n 2 2 (48)
β n 1+ 2 (ATy) if T ≤ n ≤ N − N.
βc n 2 2
By Lemma 28, β = T. By Lemma 26, 0 ≤ 1 (ATh(T)) = 1 (ATy) ≤ 1, and
c βc n βc n
1 (ATy) = 1 when n is an odd multiple of T. Since 0 < β < βc, we have w > 0 and
βc n 2 2 bdry
w < 0. Therefore 1AT(Az−y) = −sign(z∗) when z∗ ̸= 0, ie n is an integer multiple
cycle β n n n
(cid:12) (cid:12)
of T. And for all n ∈ [N], (cid:12)1AT(Az−y) (cid:12) ≤ 1. By Remark 19, z∗ is optimal.
2 (cid:12)β n(cid:12)
Figure 16 illustrates an example of 1 ATAz (44) and 1 ATAz .
2w bdry w cycle
bdry cycle
Proof [Theorem 4] Summarizing Lemma 29 and Lemma 31 gives
 (cid:16) (cid:17) (cid:16) (cid:17)
1 1−β˜ e(T 2) +e(N−T 2) if β˜≥ 1
z∗ = 2 + 2
z +z if 0 < β˜≤ 1.
bdry cycle 2
50Proof [Corollary 3] Note that unscaling (defined in Section 2) scales the parameters
in θ but does not change the neural network as a function. The reconstructed neural
net (Definition 6) before unscaling is f (x;θ) = (cid:80)N z∗σ(x − x ). For 1 ≤ β˜ ≤ 1,
2 i=1 i i 2
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
f (x;θ) = 1 1−β˜ σ x−x +σ x−x . We can compute f (x;θ) similarly
2 2 T N−T 2
+ 2 2
for β˜≤ 1.
2
G.2 Assume L = 3 layers
Proof [Theorem 5] Note y = h(T) switches 2k −1 times. So by Theorem 2, there is an
index i for which A = h(T). Since y = −A , and for all n ∈ [N],∥A ∥ = N, we have
i i n 2
i ∈ argmax |ATy|. By Remark 22, β = max |ATy| = yTA = N. So when β > β ,
n∈N n c n∈N n i c
(cid:16) (cid:17)
z∗ = 0, consistent with z = − 1−β˜ .
i
+
Next,z∗satisfiesthesubgradientconditioninRemark19,sinceforn ∈ [N],(cid:12) (cid:12)AT(Az∗−y)(cid:12) (cid:12) =
n
(cid:12) (cid:12)
(cid:12) (cid:12)AT n(z iA i−A i)(cid:12) (cid:12) = (z i−1)(cid:12) (cid:12)AT nA i(cid:12) (cid:12) = (cid:12) (cid:12) (cid:12)ββ AT ny(cid:12) (cid:12)
(cid:12)
≤ ββ argmax
n∈N
(cid:12) (cid:12)AT ny(cid:12) (cid:12) =≤ β. Since z∗
i
< 0,
c c
when i = n, AT(Az∗−y) = β = −βsign(z∗). By Remark 19, z∗ is optimal.
i i
Proof [Corollary 4] Follows from the reconstruction in Lemma 3.
Appendix H. The solution sets of Lasso and the training problem
Proof [Proposition 1] The result is almost a sub-case of that given by Mishkin and Pilanci
(Mishkin and Pilanci, 2023) with the exception that the bias parameter ξ, is not regularized.
Therefore optimality conditions do not impose a sign constraint and it is sufficient that
1⊤(Az+ξ1−y) = 0 for ξ to be optimal. This stationarity condition is guaranteed by
Az+ξ1 = yˆ. Now let us look at the parameters z . If i ̸∈ E(β), then z = 0 is necessary
i i
and sufficient from standard results on the Lasso (Tibshirani, 2013). If i ∈ E(β) and z ̸= 0,
i
then A⊤(yˆ −y) = βsign(z ), which shows that z satisfies first-order conditions. If z = 0,
i i i i
then first-order optimality is immediate since |A⊤(yˆ −y)| ≤ β, holds. Putting these cases
i
together completes the proof.
Proof [Proposition 2] This result follows from applying the reconstruction in Definition 6 to
each optimal point in Φ. The reconstuction sets α = sign(z )(cid:112) |z |. From this we deduce
i i i
sign(α )=sign(cid:0) A⊤(y−yˆ)(cid:1) . The solution mapping determines the values of w and b and
i i i i
in terms of α . Finally, the constraint f (X;θ) = yˆ follows immediately by equality of the
i 2
convex and non-convex prediction functions on the training set.
Lemma 32 Suppose L = 2, and the activation is ReLU, leaky ReLU or absolute value.
Suppose m∗ ≤ m ≤ |M|. Since m ≤ |M|, we can map the set of neuron indices to M. Let
51ΘLasso,stat = {θ : ∀i = (s,j,k) ∈ M,b = −x w } ⊂ Θ. Let θ∗ ∈ ΘLasso,stat∩C(β). Then θ∗
i j i
is a minima of the Lasso problem.
Proof Since m∗ ≤ m, the reconstructed neural net is optimal in the training problem. Let
F(θ) and FLasso(z,ξ) be the objectives of the non-convex training problem (1) and Lasso
(2), respectively. The parameters θ are stationary if θ ∈ C(β), i.e., 0 ∈ ∂F(θ).
Let ΘLasso = {θ : ∀i = (s,j,k) ∈ M,|w | = |α |,b = −x w } ⊂ ΘLasso,stat. By a similar
i i i j i
argument as the proof of Theorem 3 in Wang et al. (2021), since 0 ∈ ∂F(θ∗), we have |w | =
i
|α | for all neurons i. Therefore θ∗ ∈ ΘLasso. Then θ∗ = (α∗,ξ∗,w∗,b∗) = Rα,ξ→θ(α∗,ξ∗).
i
Let F˜(α,ξ) = F (cid:0) Rα,ξ→θ(α,ξ)(cid:1).
Since 0 ∈ ∂F(θ∗) at (α∗,ξ∗) = (cid:0) Rα,ξ→θ(cid:1)−1 (θ∗), we have F˜(α∗,ξ∗) = F(θ∗). The
chain rule gives ∂ F˜(α∗,ξ∗) = ∂ F(θ∗)∂ Rα,ξ→θ(α∗,ξ∗) ∋ 0. Let Rα→z(α) =
(α,ξ) θ (α,ξ)
sign(α)α2, with operations done elementwise. Next, at (z∗,ξ∗) = (Rα→z(α∗),ξ∗), we
have FLasso(z∗,ξ∗) = F˜(α∗,ξ∗). The chain rule gives ∂ FLasso(z∗,ξ∗) =
(z,ξ)
∂ F˜(α∗,ξ∗)∂ R(z∗,ξ∗) ∋ 0. Since the Lasso problem (2) is convex, the result holds.
(α,ξ) (z,ξ)
Proof [Proposition 3] Observe that R(Φ(β)) ⊆ C˜(β)∩ΘLasso,stat ⊆ C(β)∩ΘLasso,stat ⊆
R(Φ(β)), where the first and last subset inequality follow from Theorem 1 and Lemma 32, re-
spectively. Thereforeallsubsetsintheaboveexpressionareequal. ObservethatP (cid:0) ΘLasso,stat(cid:1) =
ΘP and so P (cid:0) C(β)∩ΘLasso,stat(cid:1) = C(β) ∩ P (cid:0) ΘLasso,stat(cid:1) = C(β) ∩ ΘP and similarly
(cid:16) (cid:17)
P C˜(β)∩ΘLasso,stat = C(˜ β)∩ΘP. Now apply P to all subsets above.
Appendix I. Numerical results
I.1 Toy problems
In Figure 17 and Figure 18, in contrast to Figure 2, we numerically solve deep narrow
ReLU Lasso problems (not the min-norm version) with β = 10−8 for two other datasets and
plot the reconstructed neural nets. These examples show that when the regularization is
small enough, due to numerical solver tolerance, we may find near-optimal (but not optimal)
neural nets, and even these near-optimal solutions exhibit the deep narrow features expected
for each depth. Our simulations compute a near-optimal Lasso solution up to tolerance 10−8.
In these Lasso simulations, β ≈ 0, and the neural net fits the training data almost exactly,
so the Lasso problem has approached the minimum norm regime (9). By Lemma 5, in
Figure 17 and Figure 18, the L = 3 Lasso solution found numerically is optimal in both the
L = 3 and L = 4 min norm problems, while the L = 4 Lasso solution found numerically is
not, giving a higher objective ∥z∗∥ . Moreover, in Figure 17, for L = 2,3, any solution for
1
depth L is also a solution for layer L+1. In Figure 18, we also solve standard non-convex
training problems with m = 100. The number of training epochs is 1,000 for L = 3 and
L
50,000 for L = 4. Adam was used with a learning rate of 0.5·10−2.
As seen from Figure 17 and Figure 18, when L ∈ {2,3}, the breakpoints in the Lasso
and non-convex solutions occur at the training data points. When L = 4, the Lasso and
non-convex solutions found numerically, even though they are not optimal, have reflection
breakpoints as characteristic of the L = 4 dictionary, namely at 4 = R .
(0,2)
52training with Lasso
selected features
10 (x n,y n) 6 z* =3.25
i 1
net
5 4 z* =0.75
i
2
0 2
5 0
10 (x n,y n) 6 z* =3.25
i 1
net
5 4 z* =0.75
i
2
0 2
5 0
6
10 (x n,y n) z* = 2.5
i
1
5 net 4 z* =6.5
i
2
2
0
0
5
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8
Figure17: Leftcolumn: Predictionsofdeepnarrownetworks(blue)withL = 2,3and4layers
trained using the Lasso problem with cvxpy on the same 1-D dataset X = (0,2,6,7)T,y =
(0,0,3,7)T (red dots). Right column: Lasso features for nonzero z∗, where z∗ is the Lasso
i
solution that is found.
I.2 Autoregression figures
In all figures except for the regularization path, the horizontal axis is the training epoch.
53
sreyal
2
sreyal
3
sreyal
4training with Adam training with Lasso
3
(x n,y n) (x n,y n)
2 net net
1
0
3
(x ,y ) (x n,y n)
n n
2 net net
1
0
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
Figure 18: Predictions of parallel networks (blue) with L = 3 and 4 layers are trained on the
same 1-D dataset X = (0,2,6,7)T,y = (0,0,3,3)T (red dots) using the non-convex training
problem (left) and Lasso problem (right).
54
sreyal
3
sreyal
4m = 2
m = 5.
m = 10.
Figure 19: Planted data. σ2 = 1.
Figure 20: The regularization path. Here, σ2 = 1, m = 5.
55BTC-2017min.
BTC-hourly.
Figure 21: Regression with L2 loss.
BTC-hourly.
Figure 22: Regression with quantile loss. τ = 0.3
BTC-2017min.
Figure 23: Regression with quantile loss. τ = 0.7
56