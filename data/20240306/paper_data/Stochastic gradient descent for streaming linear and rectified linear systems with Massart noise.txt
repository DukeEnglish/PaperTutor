STOCHASTIC GRADIENT DESCENT FOR STREAMING LINEAR AND RECTIFIED
LINEAR SYSTEMS WITH MASSART NOISE
HALYUN JEONG∗, DEANNA NEEDELL†, AND ELIZAVETA REBROVA ‡
Abstract. We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart
noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence
guaranteesofSGD-exptothetrueparameterwithupto50%Massartcorruptionrate,andwithanycorruptionrateinthecase
ofsymmetricobliviouscorruptions. Thisisthefirstconvergence guarantee resultforrobustReLUregressioninthestreaming
setting, and it shows the improved convergence rate over previous robust methods for L1 linear regression due to a choice of
anexponentiallydecayingstepsize,knownforitsefficiencyinpractice. Ouranalysisisbasedonthedriftanalysisofadiscrete
stochasticprocess,whichcouldalsobeinterestingonitsown.
1. Introduction. Robust regression aims to develop regression methods that provide a proper fit of
the data, even in the presence of outliers. Such outliers can arise, for example, from labeling mistakes
during the data-gathering process [5], incorrect measurements in tomography [27], transmission errors [13],
oradversarialattacks[4,35]indistributedmachinelearning. Ontheotherhand,thehigh-dimensionalityand
just sheer volume of modern datasets pose significant challenges in machine learning. Stochastic algorithms
such as stochastic gradient descent (SGD) are standard approaches to address this high dimensionality, as
they utilize only a part of the dataset at a time [14, 36, 7]. Moreover,in many modern applications, data is
streamed, meaning that methods cannot retain past data and are only allowed to work with a given small
portion of data at a time in such scenarios. In the fully streaming setting, an algorithm typically processes
one data point at a time and cannot revisit past data points [18, 31]. Such constraints arise from limited
memory, the immediacy of real-time processing, or just the vast volume of the data. These challenges have
naturally led to the development of robust regression methods suited for the streaming setting.
1.1. Robust linear and ReLU regression. Robust regressionhas a long history and has played an
importantroleinboth statisticsandmachinelearning. Its goalis to learnthe fitting parameters,evenwhen
the observations are contaminated by a constant fraction of adversarial outliers [37, 20, 6, 24, 9, 28]. In
particular, fitting generalized linear models is a fundamental subject in statistics [22, 21]. One important
case arises when the nonlinear function is the ReLU activation function. Recently, ReLU regression has
garnered a lot of attention due to its relevance to neural networks [33, 42, 23, 10].
In the non-streaming setting, robust linear regression can be formulated in the following way. For a
systemoflinearequationsAx=b,supposethatacertainfractionofentriesinbarereplacedwitharbitrary
values so that we have Ax = b˜ = b+e for some error vector e instead. Recently, Haddock et al. [16]
have proposed two methods based on the quantile estimate of residual of the current iterate, one is based
on the randomized Kaczmarz algorithm, and the other is based on the ℓ loss function which is known to
1
be robust to outliers in general [19, 25]. Although these methods are effective in solving corrupted systems
of linear equations, they may not be suitable for the time-sensitive streaming setting, due to the possible
computational burden in estimating quantiles. Possible ways to alleviate these time/memory requirements
forestimationincludeusingaslidingwindow[16],approximatequantileestimation[15],andinmoregeneral
SGDcontext,choosingthedatapointwiththelowestlossvalueafterobservingasufficientlylargenumberof
data points per iteration [32]. However,none of these methods would apply to the fully streaming scenario,
where accessing any past data is not possible. At the same time, the corruption model considered in these
worksmightbetoostrongforthestreamingsetting. Essentially,itimpliesthatintheworstcaseapresumed
adversarycanexamineallpastequationstochoosetheworstpossibleplacementofcorruptedmeasurements.
Here, we consider a semi-random Massart noise model as described below.
1.2. On corruption models. Let us recall several main models of adversarial corruption here. By
adversarial corruption model we imply that the corruptions do not stem from a particular distribution but
can be added in arbitrary way. Thus we do not assume an existence of an actual adversary (although it
∗UniversityofCaliforniaLosAngeles,DepartmentofMathematics,LosAngeles,CA90095(hajeong@math.ucla.edu)
†UniversityofCaliforniaLosAngeles,DepartmentofMathematics,LosAngeles,CA90095(deanna@math.ucla.edu)
‡Princeton University, Operations research (ORFE) Department,Sherrerd Hall, Charlton Street, Princeton, NJ 08544
(elre@princeton.edu)
1
This manuscript is for review purposes only.
4202
raM
2
]GL.sc[
1v40210.3042:viXramight be a convenient to describe a model and in some of the applications) but rather that the proposed
methods are expected to work in the worst case.
In the fully adversarial corruption model, as in QuantileSGD and QuantileRK methods in [16], before
we run the methods, the adversary can select any measurements and replace them with any values (so it
can depend on the true parameter and measurement vectors) as long as the corruption fraction is p. This
effectively makes the setting non-streaming for the adversary. For this noise model, the question about how
muchofthecorruptionfractionpcanbetoleratedbySGD-exporanyreasonablestreaminglearningmethod
appears to be still open. Even stronger adversarial models such as those considered in [7], allow for the
adversarialto modify the measurement vectors as well.
In the Massart noise model (this work, also [11]), the adversary cannot choose which measurements
to corrupt. Instead, the measurement is randomly selected for corruption with probability p. Then, the
adversary replaces it with any values (so again, it can depend on the true parameter and the associated
parameter vectors). For example, in the streaming setting, with probability p, an adversary can inspect a
data point at each time and replace the associated label with any incorrect label to confuse the receiver as
much as possible. This includes the sign-flip corruption.
Lastly,in the oblivious response corruption model (e.g., [11, 31]), there is essentially no adversary. Each
measurement is randomly selected with a probability of p and then corrupted by additive random noise
that is independent with the associated measurement vector and the true parameter. This is the weakest
corruption model used in works such as [31]. This excludes noise types such as sign-flip noise, which is
covered in our outlier model, the Massart noise model.
1.3. Contribution summary. Here, we propose the SGD-exp method to solve the ℓ minimization
1
problem efficiently in the streaming setting. Since in our streaming setting each data point arrives one at a
time, themostgenericyetnaturalcorruptionmodelisthe Massartnoiseasdescribedabove. As such,wedo
not require the observations to be obliviously corrupted nor that the magnitude or moments of corruption
to be bounded. This is an informal version of our theoretical results, Theorems 3 and Theorem 4:
Theorem 1. Let x Rd be an unknown vector, let y =f( x,a )+ǫ for j =1,2,... represent stream-
j j j
∈ h i
ing measurements of x. Here, ǫ is the Massart noise with corruption probability p <0.5, the measurement
j
vectors a satisfy Gaussian-like assumptions and f can be an identity or a ReLU function. There exists a
j
constant ξ (p,T)>0 such that for any ξ (0,ξ ), if we take λ=1+ξ and run T iterations of l -SGD
0 0 1
∈
(1.1) x =x +λ ksign(y f( x,a ))f ( x,a )a ,
k+1 k − k k ′ k k
− h i h i
where f is a subgradient of f for T iterations, then, with high probability, we have
′
T
x x .logT exp .
k − T k2 −log2T
(cid:18) (cid:19)
The proof of main theorems relies on type of analysis that is, to the best of our knowledge, new in the
robust regressionliterature. It is based on drift analysis of the stochastic process after we have transformed
the residual equation of SGD, and can be interesting on its own.
Asaresult,theproposedmethodSGD-exp(1.1)provides(nearly)linearconvergenceguaranteesforany
corruption probability less than 0.5, which is the best possible for the Massart noise model. Our analysis
alsorevealsthatSGD-exptoleratesanycorruptionprobabilitylessthan1whenthe corruptionissymmetric
oblivious noise1 for the streaming setting, answering a related question in [34].
To the best of our knowledge, our approach provides the first nearly linear convergence guarantees for
both linear regressionandReLUregressionunder the adversarialMassartnoise model(see discussionabove
and Assumption 1 below for the formal definition) in the fully streaming setting.
1.4. Related works. For the linear regression in the streaming environment with oblivious random
corruption,PesmeandFlammarion[31]haveproposeda method(whichwe willrefertoas SGD-rootdue to
thesquare-rootdecayingstepsizescheduling),whichisbasedontheSGDfortheℓ loss,aswellasSGD-exp.
1
Compared to it, SGD-exp comes with a faster convergence rate under a less restricted corruption model.
1Actually,theonlyassumptionweneedisP(therandomoutlierispositive)=P(therandomoutlierisnegative).
2
This manuscript is for review purposes only.Specifically, running SGD-root for k-iterations to recover d-dimensional signals only provides O d/k
recovery accuracy, whereas our method guarantees the recovery error of the order of O exp
k(cid:16)p
.
I(cid:17)
n
dl−og2k
[31], the outliers are generated by the random oblivious response corruption model, w(cid:16)hich (cid:16)excludes(cid:17)(cid:17)noise
types such as sign-flip noise, which are coveredin the Massartnoise model that we consider. SGD-root also
requires the corruption noise to have a finite first absolute moment, whereas we do not require any such
condition.
Diakonikolaset al. study robust linear and ReLU regressionunder the Massartnoise model in [11], but
their approachis notdesignedforthe streamingsettinganddoesnotprovideanexplicitconvergenceratein
terms of the number of iterations. In [8], Diakonikolas et al. study robust regression for generalized linear
models. Although this work considers more general distributions and activation functions, it is not for the
streaming setting, and the corruption model is the oblivious response corruption, weaker than the Massart
noise.
In [7], the authors consider an even more general contamination model where the measurement vectors
can also be modified by the adversary. Due to this general model assumption, the error does not decay
below O(p), where p is the corruption probability, even if we increase the number of iterations. Moreover,
without any distributional assumption on the measurement vectors, the recovery of the true parameter is
information-theoretically impossible in general [11, 29]. The works [10, 41] aim to find a parameter to
minimize the risk function, assuming that the covariates and contaminated responses are jointly random
along with an additional condition on the size of contaminated responses. Neither of these conditions is
required for our convergence guarantees.
1.5. SGD with exponential decay step size. The exponentialstep decayscheduling or its variants
for SGD are commonly used as a default setting in many popular machine learning software packages,
includingTensorFlow[1]andPyTorch[30]. ThelastiteratesofSGDwiththesetypesofstepsizescheduling
havedemonstratedexcellentempiricalperformance,asobservedin[12,26,40]. However,thefirstconvergence
results for exponential step decay scheduling have emerged only recently [26, 39]. These studies primarily
focusontheproblemfromanoptimizationperspectiveanddonotaddressrobustness,thusfailingtoprovide
meaningful recovery in cases where measurements or stochastic gradients are contaminated by outliers like
oursetting. Ourmethod,SGD-exp,employsthismorepracticalstepsizeschedulingtoaddressrobustlinear
and ReLU regressionproblems in a streaming setting.
1.6. Organization. In the next section, we formalize the streaming measurement model and the as-
sumptions on the noise and on the measurements. Then, we define the method both in linear (2.3) and
in ReLU cases (2.4) and state our main convergence theorems, Theorem 3 and Theorem 4. In Section 3,
we provide necessary background on discrete drift analysis: we state and prove Theorem 5 – a convenient
modificationof [[17], Theorem2.3]– which mightbe applicable more broadly. InSections 4 and5, we prove
our main theorems, and provide empirical evidence in Section 6 to support our theory. We summarize our
results and discuss future researchdirections in Section 7.
2. Main results. We formalize the models and state our main results in this section.
2.1. Model 1: Streaming linear system. Suppose we want to recover an unknown vector x Rd
∈
from n random linear measurements with corruption probability p. More precisely, we have observations
y ,y ,...,y arriving in a streaming fashion:
1 2 n
(2.1) y = x,a +ǫ , for j =1,...,n,
j j j
h i
such that the noise ǫ satisfies Assumption 1 and measurement vectors a Rd satisfy Assumption 2:
j j
∈
Assumption 1 (Massart noise model).
The coordinates of an n-dimensional noise vector ǫ satisfy ǫ = ξ ν , where ξ are the indicator random
j j j j
variables taking value 1 with probability p < 1/2 and independent with all other variables, and ν are any
j
variables, possibly random and dependent on measurement vectors or true signal.
Assumption 2 (Measurement model).
For any j = 0,1,... let be the σ-algebra generated by a ,ǫ , a ,ǫ ,..., a ,ǫ . Let a Rd
j 0 0 1 1 j 1 j 1 j
F { } { } { − − } ∈
3
This manuscript is for review purposes only.be the unit-norm measurement vectors independent with such that √da are i.i.d. mean-zero isotropic
j j
random vectors. For any u Rd measurable with respect tF o it holds that
j
∈ F
u
E a j |hu,a j i| u ≥Ck √k d2
(cid:20) (cid:12) (cid:21)
(cid:12)
for some constant C >0 that only depends on the
dist(cid:12)ributioe
n of a .
j
Direct computation shows that the normalized random Gaussian vector, or equivalently, the random
vectors drawn unifoe rmly at random from Sd 1, satisfy Assumption 2 with the constant C = 2/π. More
−
generally,manyothermeasurementmodelssuchasthenormalizedBernoullirandomvectorsatisfytheabove
p
assumption for moderately large dimension d by the following lemma. e
Lemma 2. Let φ n beindependentrandomvectorswhoseentriesarei.i.d., mean-zero,unit-variance,
{ j}j=1
and sub-Gaussian with sub-Gaussian norm bounded by K. Then the normalized vectors φ / φ n
{ j k jk2 }j=1
satisfy Assumption 2 with a constant C with probability at least 1 O(n√de cd). Here, c and C are positive
−
−
constants that depend on K.
Proof. Let be the σ-algebra
ge
enerated by φ ,ǫ , φ ,ǫ ,..., φ ,ǫ and u
eRd
be mea-
surable with
resF pj
ect to as in Assumption 2.
B{
y
t0 he0 } Kh{ in1 tch1 in}
e
ineq{ uaj l−it1
y
(j s− e1 e,}
e.g.,
Ex∈
ercise 2.6.6 in
j
F
[38]),
E u,φ u C(K) u .
φ j | j | ≥ k k2
(cid:20) (cid:12) (cid:21)
Also, by Bernstein’s inequality, for a given c(cid:10) onstan(cid:11) t(cid:12)c>1,
(cid:12)
P( φ >√cd)=P( φ 2 >cd) e c′(K)d
k jk2 k jk2 ≤ −
for some constant c(K) that only depends on the sub-Gaussian norm of the entries of random vector φ .
′ j
Hence,
φ φ
E u, j u E u, j u
φ j (cid:20)|
(cid:28)
kφ jk2(cid:29)|
(cid:12)
(cid:21)≥ φ j (cid:20)|
(cid:28)
kφ jk2(cid:29)| kφ jk2≤√cd
(cid:12) (cid:21)
(cid:12) 1 (cid:12)
(cid:12) E u,φ (cid:12)u
≥ √cd φ j (cid:20)| j | kφ jk2≤√cd
(cid:12) (cid:21)
1 (cid:10) (cid:11) (cid:12)
= E u,φ u E (cid:12) u,φ u
√cd
(cid:18)
φ j (cid:20)| j |
(cid:12)
(cid:21)− φ j (cid:20)| j | kφjk2>√cd
(cid:12) (cid:21)(cid:19)
1 (cid:10) (cid:11) (cid:12) (cid:10) (cid:11) (cid:12)
E u,φ (cid:12)u u E φ (cid:12) u
≥ √cd
(cid:18)
φ j (cid:20)| j |
(cid:12)
(cid:21)−k k2 φ j (cid:20)k jk2 kφjk2>√cd
(cid:12) (cid:21)(cid:19)
1 (cid:10) (cid:11) (cid:12) 1/2 (cid:12) 1/2
C(K) u (cid:12)u E φ 2 E (cid:12) u
≥ √cd k k2 −k k2 (cid:18) φ j (cid:20)k jk2 (cid:21)(cid:19) (cid:18) φ j (cid:20) kφ jk2>√cd (cid:12) (cid:21)(cid:19) !
1/2 (cid:12) 1/2
1 (cid:12)
= C(K) u u E φ 2 E
√cd k k2 −k k2 (cid:18) φ j (cid:20)k jk2 (cid:21)(cid:19) (cid:18) φ j (cid:20) kφ jk2>√cd (cid:21)(cid:19) !
1
= C(K) u √d u P( φ >√cd)1/2
√cd k
k2
− k
k2
k
jk2
(cid:16) (cid:17)
ku k2 C(K) √de −c′(K)d/2 ,
≥ √cd −
(cid:16) (cid:17)
wherewe haveusedthatu is measurablewith respectto andφ is independent with . Setting c=3/2
j j j
and taking d large enough such that √de c′(K)d/2 <C(KF )/2 proves the lemma. F
−
Remark 1. The analyses for some works including SGD-root [31] also assume a mean-zero Gaussian
feature(measurement)whichis morerestrictedthanourmeasurementmodelinAssumption2. Althoughthey
cover a non-identitycovariance matrix Σ for Gaussian vectors, we would like to point out that by multiplying
4
This manuscript is for review purposes only.
1
1
1
1
1
1by L 1 (where Σ = LLT is the Cholesky Decomposition), the vectors can be standardized. Hence, we can
−
easily extend our convergence analysis to the non-identity covariance (non-isotropic) Gaussian case. For
simplicity, we have carried out our analysis for the isotropic case but under a broader class of random
measurement models.
2.2. Model 2: Streaming ReLU regression. Suppose we observe a signal x through nonlinear
measurements y given as
i
(2.2) y =σ( x,a )+ǫ , for i=1,...,n,
i i i
h i
wheretheReLUactivationfunctionσ(u):=max 0,u ,thenoiseǫ satisfiesAssumption1andmeasurement
j
vectors a Rd satisfy Assumption 2 with the fo{ llowi} ng additional condition on the distribution of a:
j
∈
Assumption 3. For any u Rd and v Rd, for any i=1,... we have
∈ ∈
E
a i {hv,a ii≥0 }
=E
a i {hv,a ii≤0 }
(cid:2) (cid:3) (cid:2) (cid:3)
and
E a i |hu,a i i| {hv,a ii≥0 } =E a i |hu,a i i| {hv,a ii≤0 } .
Remark 2. Note that Assu(cid:2)mption 3 holds for(cid:3)any sym(cid:2)metric distribution(cid:3)for a i; the distribution of a
i
is identical to the distribution of a . This includes common measurement models such as the Gaussian
i
−
distribution or the symmetric Bernoulli distribution.
Remark 3. At least some form of assumption on the distribution of a is necessary for ReLU regression,
otherwise recovering the unknown vector x is information-theoretically impossible [11, 29]. The Assumption
3 is similar to the one in [41], but their work aims to minimize the risk with respect to the ℓ loss and is not
2
about the true parameter recovery using the more robust ℓ loss as ours. Moreover, unlike our work, their
1
corruption model and result do not allow arbitrary large corruptions.
2.3. SGD-exp method and main theorems. Consider the following version of stochastic gradient
descent for the ℓ -loss, or least absolute deviation error. The same version was considered in QuantileSGD
1
[16] in the non-streaming setting but with a different step size scheduling. Namely, in the linear case, we
define SGD-exp method iteration as
(2.3) x =x +λ ksign(y x ,a )a ,
k+1 k − k k k k
−h i
whereλ k is the stepsize ink-thiterationandλ>1. Theinitialiteratex issetto0. Figure1(left) shows
− 0
that the method empirically converges linearly in the number of iterations for λ=1.00003.
Our main result is that
Theorem 3. Suppose that x Rd is observed through the noisy streaming scheme (2.1) and the mea-
surement vectors a Rd satisfy A∈ ssumption 2. Suppose that the dimension d is sufficiently large enough to
j
e ∈
satisfy C(1 −2p) < 3. Let a parameter R>225 be such that for
√d 7
(1 2p)2 1
λ:= 1+C2 − , we have x 2 <a:= .
s Rdlog2T k k2 2(λ2 1)
−
Let x be the result of T steps ofe SGD as per (2.3). Then, with probability 1 70dT1 √R/15/C2(1 2p)2
T −
− −
the error is bounded by
e
2C√RdlogT C2(1 2p)2
x x exp T − ,
k − T k2 ≤ 1 2p − · 3Rdlog2T
e − n e o
where the positive constant C is from Assumption 2 on the measurement and it only depends on the the
sub-Gaussian norm K.
e
5
This manuscript is for review purposes only.
1
1
1
1Remark 4 (On the choice of λ). Since R is a parameter that only needs to be large enough, Theorem 3
essentially says that the convergence is guaranteed as soon as we choose λ = 1+ξ with positive ξ that is
small enough. Choosing ξ smaller than necessary is reflected in a slower convergence rate, as numerically
observed in Figure 3.
In the ReLU case, using the subdifferentials of the absolute function and the activation function σ, the
corresponding SGD-exp iteration is given by
(2.4) x k+1 =x k+λ −ksign(y k −σ( hx k,a k i)) {hx k,a ki≥0 }a k.
Figure1(right)showsthatSGD-expempiricallyconvergeslinearlyinthenumberofiterationsforλ=1.00003
for the streaming with corrupted ReLU measurements.
102 101
SGD-exp / Additive Gaussian corruption SGD-exp / Additive Gaussian corruption
SGD-exp / Sign filp corruption SGD-exp / Sign filp corruption
101 100
100
10-1
10-1
10-2
10-2
10-3
10-3
10-4
10-4
10-5 10-5
10-6 10-6
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Number of iterations 105 Number of iterations 105
Fig.1. Relative error of the SGD-expfor(a) acorrupted linearsystem(left)and (b)acorrupted rectifiedlinear (ReLU)
system (right) with corruption probability p = 0.4. The blue curves represent relative error when the corruption is a large
additive Gaussian noise. The red curves represent the sign-flip corruption, i.e., with probability p, the sign of measurement
is flipped. The measurement vectors are 100-dimensional i.i.d. normalized standard Gaussian vectors. The dimension of the
signal is 100 and both plots are averaged over 20 trials.
The main theorem in the ReLU case is as follows:
Theorem 4. Suppose x Rd is observed through the noisy ReLU streaming scheme (2.2) and the
measurement vectors a Rd s∈ atisfy Assumptions 2 and 3. Suppose that the dimension d is sufficiently large
j
e ∈
enough to satisfy C(1 −2p) <1. Let a parameter R>400 be such that for
√d
(1 2p)2 1
λ:= 1+C2 − , we have x 2 <a:= .
s Rdlog2T k k2 2(λ2 1)
−
e
Let x be the result of n steps of SGD-exp as per (2.4). Then, with probability
T
120d
1 T1 −√R/20
− C2(1 2p)2 ·
−
the error is bounded by e
2C√RdlogT C2(1 2p)2
x x exp T − ,
k − T k2 ≤ 1 2p − · 3Rdlog2T
e − n e o
where the positive constant C is from Assumption 2 on the measurement and it only depends on the sub-
Gaussian norm K.
e
6
This manuscript is for review purposes only.
rorre
evitaleR
rorre
evitaleR
1Remark 5 (Addressing highly corrupted regime). Theorem 3 implies that the iterates of the SGD-
exp converge (almost) linearly up to logarithmic factors for the corruption probability p < 1/2. This is
theoretically the best possible: suppose that p > 1/2. Then, any approach would not be able to determine
whether the responses are generated from the signal x with the sign-flip corruption with probability p or they
are generated from x with the sign-flip corruption with probability p 1/2. Prominently, in the case when
− −
the noise is symmetric and oblivious, our proof yields the result for all p < 1, see Remark 7 and Remark 8
for the details.
Remark 6 (Convergence rate is almost like in an uncorrupted setting). Our effective convergence rate
from Theorem 3 and Theorem 4 is
T (1 2p)2 √dlogT
exp C − +log .
− d log2T 1 2p !!
−
Notably, it is only logarithmically slower than compatible streaming algorithms on the uncorrupted set-
ting. For example, in the streaming setting, the standard Kaczmarz convergence rate is exp CT for
− d
d-dimensional Gaussian or Bernoulli measurement vectors. Then, in [26], an SGD with similar exponential
(cid:0) (cid:1)
step size schedule λ T(1/T) results in the loss decreases of order O(exp( C(d,L) T )) on the setting
∼ − logT
without contamination and under appropriate smoothness L-Lipschitz conditions.
TheideaofconvergenceanalysisinbothTheorem3andTheorem4istoshowthatasequenceofrandom
variables u :=λT x x is (almost)uniformlyboundedwithhighprobability,whichinturnshowsthe
T T
k k k − k
residualerror x x decreases (almost)geometrically since λ>1. To this end, we adaptfor our case the
T
k − k
result from Hajek [17] on the drift analysis of discrete stochastic processes, also used in several stochastic
algorithm convergence analyses recently [2, 3].
3. Background ondriftanalysis. ConsiderasequenceofrandomvariablesY ,Y ,...,Y measurable
0 1 n
with respect to a filtration (that is, Y ,Y ,...,Y are -measurable). We follow the general
k k 0 0 1 k k
{F } ≥ F
convention that = . Let τ be the first hitting time defined as
0 b
F ∅
τ :=min j :Y b .
b j
{ ≥ }
The goal is to obtain a high probability upper bound on the first hitting time defined above, given that the
random process’s increments are bounded in expectation. The next theorem is a variant of Theorem 2.3 in
[17], modified to suit our specific purpose.
Theorem 5. Let Y be a discrete stochastic process adapted to the filtration , where is
k k 0 k k 0 0
a trivial σ-algebra. Su{ ppos} e≥ that for some a,b,η,ρ,D R such that a<b, η >0, 0<{ ρF <} 1≥ and D 1F , the
∈ ≥
process satisfies
(C0) Y [0,a) a.s.,
0
( (C C1 2)
)
E E[ [e e∈ η η( (Y Yk k+ +1 1− −Y a)k) {Y{ ka ≤ <Y
a
}k |< Fb } k| ]F ≤k] D≤ aρ .sa ..s fo. rfo ar nyan ky ≥k 1≥ .1,
Then, for the first hitting time τ :=min j :Y b it hold for any k 1
b j
{ ≥ } ≥
1 ρk
(3.1) E eηYk eηa ρk+ − D .
{τb>k −1 } ≤ 1 ρ
h i (cid:18) − (cid:19)
Proof. First, we note that τ is indeed a stopping time, namely, τ >l = Y <b,...,Y <b for
l b 1 l l
{ } { }∈F
any l 0. Hence, we have a recursive estimate
≥
E[eηYl+1 ]=E[eηYl+1 ]+E[eηYl+1 ]
{τb>l }|Fl {τb>l
}
{Yl<a }|Fl {τb>l
}
{b>Yl≥a }|Fl
=E[eηYl+1 ]+E[eηYl+1 ]
{τb>l −1
}
{Yl<a }|Fl {τb>l −1
}
{b>Yl≥a }|Fl
= E[eηYl+1 ]+ E[eηYl+1 ]
{τb>l −1
}
{Yl<a }|Fl {τb>l −1
}
{b>Yl≥a }|Fl
(3.2) ρeηYl +Deηa,
≤
{τb>l −1
}
where in the second step we have used that τ =l if Y <b and (C1), (C2) in the last step.
b l
6
7
This manuscript is for review purposes only.
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1Now, since Y < a a.s. and is a trivial σ-algebra, (3.1) will immediately follow if we prove that for
0 0
F
every m k
≤
m 1
(3.3) E eηYk ρmeηYk−m + − ρiDeηa.
{τb>k −1
}
Fk −m
≤
{τb>k −m −1
}
h (cid:12) i Xi=0
(cid:12)
For m = 0, (3.3) holds trivially. Let’s(cid:12)employ induction: suppose (3.3) holds for some m 0, then, using
≥
the tower property of conditional expectation we have for m+1:
E eηYk =E E eηYk
{τb>k −1
}
Fk −m −1 {τb>k −1
}
Fk −m Fk −m −1
h (cid:12) i h h (cid:12) i(cid:12) i m 1
(cid:12) (cid:12) ρmE eηYk−m (cid:12) (cid:12) (cid:12) (cid:12) + − ρiDeηa
≤
{τb>k −m −1
}
Fk −m −1
h (cid:12) i Xi=0
(cid:12) m 1
(cid:12) −
ρm+1eηYk−m−1 +ρmDeηa+ ρiDeηa,
≤
{τb>k −m −2
}
i=0
X
where we have used (3.2) with l = k m 1 in the last step. This concludes the proof of (3.3) and of
− −
Theorem 5.
We will use the result about the drift through the following key corollary that is a modification of
Proposition 2.5 in [17]:
Corollary 6. Suppose Y ,Y ,Y ,...,Y satisfy following two conditions (C1) and (C2) in Theorem 5
0 1 2 n
for some b > a > Y 0 and η > 0, 0 < ρ < 1 and D 1. Now, let K be a given nonnegative integer.
0
≥ ≥
Then, we have
1
P[τ K] KDe η(b a)
b − −
≤ ≤ 1 ρ
−
for any b with b>a>Y 0.
0
≥
Proof. The proof follows from Markov’s inequality applied to the result of Theorem 5:
K K
P[τ K]= P[τ =k]= P[ Y b τ >k 1 ]
b b k b
≤ { ≥ }∩{ − }
k=1 k=1
X X
K
= P[eηYk eηb]
{τb>k −1
} ≥
k=1
X
Markov’s K 1 ρk
e −η(b −a) ρk+ − D
≤ 1 ρ
k=1 (cid:18) − (cid:19)
X
K
D D DK
e η(b a) ρk( 1) e η(b a) .
− − − −
≤ 1 ρ − 1 ρ − ≤ 1 ρ
k=1(cid:18) − − (cid:19) −
X
The last inequality holds since ρk( D 1) is positive as D >1 and ρ (0,1).
1 ρ − ∈
−
4. SGD-exp convergence analysis for linear problem. Now, we give the estimates of the type
(C1) and (C2) as per Theorem 5 for the process
Y = u 2 with u =λk(x x ),
k k k k2 k − k
where xk are the iterates of SGD, x is the solution and λ defines the step size as per (2.3) .
4.1. Initial reductions. First, let us compute Y explicitly. By definition of measurements (2.1) and
k
linearity of the inner product, we have
z =z λ ksign( z ,a +ǫ )a for z :=x x .
k+1 k − k k k k k k
− h i −
8
This manuscript is for review purposes only.
1
1
1
1
1
1
1Multiplying λk+1 to both sides we have
λk+1z =λ λkz λk+1λ ksign( z ,a +ǫ )a
k+1 k − k k k k
· − h i
and
(4.1) u =λ u sign u ,a +λkǫ a for u :=λkz
k+1 k k k k k k k
− h i
n (cid:16) (cid:17) o
since λ is positive, so sign( z ,a +ǫ ) = sign( λkz ,a +λkǫ ) = sign( u ,a +λkǫ ). Next, we take
k k k k k k k k k
h i h i
the squared Euclidean norm on both sides above to get
(cid:10) (cid:11)
(4.2) Y = u 2 =λ2 u 2 2 u ,a sign u ,a +λkǫ +1
k+1 k+1 k k k k k k
|| || || || − h i h i
n (cid:16) (cid:17) o
where we have used the fact that a =1.
k
|| ||
4.2. SGD drift estimates.
Lemma 7. LettherandomprocessY definedas per (4.2), where a n arei.i.d. randomvectorsinRd
k { j }j=1
satisfying Assumption 1 with some constant C˜ > 0 and the corruption noise ǫ is non-zero with probability
j
p < 1/2. Let be the σ-algebra generated by a ,ǫ , a ,ǫ ,..., a ,ǫ . Then for any step size
k 0 0 1 1 k 1 k 1
F { } { } { − − }
decay parameter λ>0 that satisfies
(1 2p)2 50
(4.3) 1<λ2 1+C2 − < ,
≤ 9d 49
for a= 1 , b= 3 and e
2(λ2 1) 2(λ2 1)
− −
1 √2λ2(1 2p)C 3
η =c ∗ λ2 1 with c ∗ = − λ2 1( +λ2)
− 8λ2 " √d − − 2 #
p e p
we have
C2(1 2p)2
E[eη(Yk+1−Yk)
{a ≤Yk<b
}|Fk] ≤1
−
60−
d
.
e
Proof. Note that each u =λ k(x x ) is measurable with respect to by (4.1), so Y = u 2
k − − k Fk { k } {k k k2}
is adapted to . From Taylor’s series expansion, with ∆ =Y Y ,
k k k+1 k
F −
(4.4) E[eη∆k ] 1+ηE ∆ +E ∞ 1 ηn ∆ n .
{a ≤Yk<b }|Fk ≤ k {a ≤Yk<b }|Fk n! | k | {a ≤Yk<b } Fk
h i (cid:20)n X=2 (cid:12) (cid:21)
(cid:12)
First, we estimate the linear term: (cid:12)
E[(Y Y ) ]
k+1
−
k {a ≤Yk<b }|Fk
(4 =.2)E (λ2 −1) ||u k ||2 −2λ2 hu k,a k isign( hu k,a k i+λkǫ k)+λ2 {a ≤ku kk2<b
}
Fk
( ≤i) E h 3 2(cid:0) +λ2 {a ≤ku kk2<b }−2λ2 hu k,a k isign hu k,a k i+λkǫ k(cid:1) {a ≤ku kk2<b
}
(cid:12) (cid:12) (cid:12)u k i
(cid:20) (cid:16) (cid:17) (cid:12) (cid:21)
(ii) 3(cid:0) (cid:1) (cid:12)
≤ 2 +λ2 −2λ2E hu k,a k i (1 −p)sign( hu k,a k i) −p ·sign( hu k,a k i) u k (cid:12) {a ≤ku kk2<b }
(cid:0)3 (cid:1) (cid:20) n o(cid:12) (cid:12) (cid:21)
= 2 +λ2 −2λ2(1 −2p)E |hu k,a k i| u k {a< ku kk2<b } (cid:12)
(cid:20) (cid:12) (cid:21)
(iii(cid:0) ) 3 (cid:1) C (cid:12)
(4.5) +λ2 2λ2(1 2p) (cid:12) ,
≤ 2 − − √d 2(λ2 1)
e −
p 9
This manuscript is for review purposes only.
1
1
1
1
1
1
1
1
1
1wheretheinequality(i)followsfromindependenceof a ,ǫ with andu being -measurable,andalso
k k k k k
{ } F F
b=3/2(λ2 1). The inequality (ii) follows from the worstcase that flips the sign( u ,a ), which happens
k k
− h i
if all the sign( u ,a ) flips when outlier occurs. Lastly, the inequality (iii) follows from Assumption 2 for
k k
h i
the measurement vector a and a=1/2(λ2 1).
k
−
Now, for the second term in (4.4), we have for n 2
≥
1 1 n
n!|Y k+1 −Y k |n {a ≤Yk<b } ≤ n! (λ2 −1) ku k k2+2λ2 |hu k,a k i|+λ2 {a ≤ku k2<b }
1 (cid:12) (cid:12)3 2λ2√3 n 1 3 (cid:12) (cid:12) 2λ2√3 n
(cid:12) +λ2+ +λ2(cid:12)+ ,
≤ n! 2 2(λ2 1) ≤ 2 2 2(λ2 1)
(cid:12) − (cid:12) (cid:12) − (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
and, choosing η =c√λ2 1, (cid:12) (cid:12) p (cid:12) (cid:12) (cid:12) (cid:12) p (cid:12) (cid:12)
−
E ∞ 1 ηn ∆ n ∞ yn, for y =c λ2 1(3 + λ2 )+c√3λ2 >0.
n! | k | {a ≤Yk<b } Fk ≤ − 4 2 √2
(cid:20)n X=2 (cid:12) (cid:21) n X=2 p
(cid:12)
Then, if we choose c small enough so t(cid:12)hat y <0.5, and using that λ2 <50/49by assumption, the geometric
sum can be further upper-bounded as
∞ y2
yn 2y2 2(1.43λ2c)2 4λ4c2.
≤ 1 y ≤ ≤ ≤
n=2 −
X
Using this estimate together with (4.5) in the initial Taylor expansion (4.4), we have
E[ec√λ2 −1(Yk+1−Yk)
{a ≤Yk<b
}|Fk] ≤1 −uc+4λ4c2,
√2λ2(1 2p)C 3
where u= − λ2 1( +λ2).
√d − − 2
e p
The condition 1<λ2 <1+C2(1 −2p)2 essentially ensures that the first term dominates in u.
9d
This quadratic polynomial in c is minimized by
e
u (√2 1/3 1/2)C(1 2p) 1 C(1 2p)
(4.6) c∗ = > − − − > −
8λ4 8λ2 √d 15 √d
(cid:20) (cid:21)
e e
using the estimates on λ (4.3) to show the inequalities, which in turn yields
E[ec∗√λ2 −1(Yk+1−Yk)
{a<Yk<b
}|Fk] ≤1 −4λ4(c∗)2 ≤1
−
C2(1
57−
d2p)2
.
e
To conclude the proof of Lemma 7, note that c is indeed small enough to make y < 0.5. This follows
∗
by a direct check using that y <1.43λ2c and the conditions on λ (4.3).
∗
Lemma 8. Let therandom process Y be defined as per (4.2), where a n are i.i.d. random vectors in
k { j }j=1
Rd satisfying Assumption1withsomeconstantC˜ >0andthecorruptionnoiseǫ isnon-zerowithprobability
j
p < 1/2. Let be the σ-algebra generated by a ,ǫ , a ,ǫ ,..., a ,ǫ . Then for any step size
k 0 0 1 1 k 1 k 1
F { } { } { − − }
decay parameter λ>0 that satisfies
(1 2p)2 50
1<λ2 1+C2 − < ,
≤ 9d 49
for
e
1 1 √2λ2(1 2p)C 3
a= ; η =c ∗ λ2 1; c ∗ = − λ2 1( +λ2)
2(λ2 1) − 8λ2 " √d − − 2 #
−
p e p
we have
C(1 2p)
E[eη(Yk+1−a)
{Yk≤a
}|Fk] ≤exp 3√−
d
.
10 n e o
This manuscript is for review purposes only.
1
1
1
1
1
1Proof. Using the event Y a u 2 1 and that a 2 =1, we estimate
{ k ≤ }⊆{k k k ≤ 2(λ2 1)} k k k
−
E[eη(Yk+1−a)
{Yk≤a
}|Fk] ≤E[eη(Yk+1−Yk)
{Yk≤a
}|Fk]
E[exp(η((λ2 1) u 2+2λ2 u ,a +λ2)) u ]
≤ − k
k
k |h
k k
i| ·
{||uk||2 ≤a
}|
k
1 2λ2
exp c λ2 1 +λ2+
∗
≤ − 2 2(λ2 1)
n p (cid:16) − (cid:17)o
C(1 2p) p3 C(1 2p)
exp − λ2 1+ exp − ,
≤ 5√d − 2 ≤ 3√d
n e (cid:16)p (cid:17)o n e o
where the last line is from the choice of c ∗ < √2 8Ce λ( 21 √− d2p) and √2/8<1/5.
4.3. Proof of Theorem 3. We are ready to give the proof of our main result in the linear case.
Proof of Theorem 3. We apply Corollary 6 with
C2(1 2p)2
ρ=1 − ,
− 60d
eC(1 2p)
D =exp − ,
3√d
n e o
1 √2λ2(1 2p)C 3
η =c ∗ λ2 1 with c ∗ = − λ2 1( +λ2) ,
− 8λ2 " √d − − 2 #
p e p
1 3
a= , and b= .
2(λ2 1) 2(λ2 1)
− −
e e
Using that η > 1 C(1 −2p) √λ2 1 (by equation (4.6)), definition of λ and C(1 −2p) < 1, we get
15 √d − 3√dlogT 7
(cid:20) (cid:21)
1
P[τ
b
T] TDe−η(b −a)
≤ ≤ 1 ρ
−
60d C(1 2p) C(1 2p) 1
T exp − exp −
≤ C2(1 2p)2 3√d − 15√d √λ2 1
− n e o (cid:26) e − (cid:27)
70dT 1
exp √RlogT =:p .
e
≤ C2(1 2p)2 · −15 · ∗
− (cid:26) (cid:27)
Recall that τ :=min j : u 2 b =min j :λ2j x x 2 b . So, with probability 1 p , we have
b { kej k ≥ } { k − j k2 ≥ } − ∗
τ >n and
b
x x
T 2
<√bλ−T
|| − ||
3
λ T
−
≤s2(λ2 1)
−
2C√RdlogT (1 2p)2 −T/2
1+C2 −
≤ 1 2p Rdlog2T
− (cid:18) (cid:19)
e
2C√RdlogT e C2(1 2p)2
exp T − ,
≤ 1 2p − · 3Rdlog2T
e − n e o
where the last inequality is from the inequality 1+x e2x/3 for 0<x<1.
≥
Remark 7. (Random symmetric oblivious corruption) Lemma 7 applies to the more restricted corrup-
tion model, the random symmetric oblivious noise with corruption probability p<1. Recall that in this case,
the corruption noise ǫ is a symmetric random variable with probability p<1 and 0 otherwise. In addition,
j
11
This manuscript is for review purposes only.
1 1
1the noise ǫ is also independent with a and x, where y = x,a +ǫ . One can easily check that the same
j j j j j
h i
argument applies except we have a sharper inequality in (ii) in the proof of Lemma 7.
FortheMassartnoiseǫ,sinceitcanaccessxanda ,wehavetoconsidertheworstcase,whichhappensif
j
all thesign( u ,a )=sign( z ,a )=sign( x,a x ,a )flips or is different withsign( u ,a +λkǫ ).
k k k k k k k k k k
h i h i h i−h i h i
In contrast, for the random symmetric oblivious noise, suppose that the k-th response is selected for
corruption. Since ǫ is independent with a (also u as well since it only depends on a ,a ,...,a ) and
k k k 0 1 k 1
ǫ is symmetric with respect to 0, the probability that the sign of ǫ differs from the sign of u ,a− is at
k k k k
h i
most 1/2. Thus, the sign of u ,a +λkǫ differs the sign of u ,a with probability at most 1/2. Since
k k k k k
h i h i
the probability of k-th response being selected for corruption is p, the overall probability that sign( u ,a )=
k k
h i 6
sign( u ,a +λkǫ ) is p/2. Hence, in this case, the inequality (ii) in the proof of Lemma 7 is refined to
k k k
h i
(ii′) 3 p p
≤ 2 +λ2 −2λ2E hu k,a k i (1 − 2)sign( hu k,a k i) − 2 ·sign( hu k,a k i) u k {a ≤ku kk2<b }
(cid:0) (cid:1)3 (cid:20) n o(cid:12) (cid:12) (cid:21)
= 2 +λ2 −2λ2(1 −p)E |hu k,a k i| u k {a ≤ku kk2<b }. (cid:12)
(cid:20) (cid:12) (cid:21)
(cid:0) (cid:1) (cid:12)
Now, the rest of the proof of the lemma and the main(cid:12)theorem remain the same, except we have the factor
(1 p)insteadof(1 2p)inTheorem3. This allows therecoveryofthesignalfor SGD-expforanycorruption
− −
probability p < 1 under the random symmetric oblivious corruption model, answering a related question in
[34] for the streaming setting.
5. SGD-exp analysis on streaming ReLU. In this section, we show that our approach SGD-exp
can be naturally generalized to the robust ReLU regression problem.
5.1. Initial reductions. Directly from SGD-exp iteration (2.4), we have
z k+1 =z k −λ −ksign(σ( hx,a k i) −σ( hx k,a k i)+ǫ k) {hx k,a ki≥0 }a k
where z :=x x is the k-th residual vector. Similarly to the linear case, we get
k k
−
(5.1) u k+1 =λ u k −sign(σ( hx,a k i) −σ( hx k,a k i)+ǫ k) {hx k,a ki≥0 }a k for u k :=λkz k.
n o
Then, we take the squared Euclidean norm on both sides above to get for Yˆ = u 2 that
k+1 k+1
k k
(5.2) Yˆ k+1 =λ2 ku k k2 −2 hu k,a k isign(σ( hx,a k i) −hx k,a k i+ǫ k) {hx k,a ki≥0 }+ {hx k,a ki≥0
}
,
n o
where we have used the fact that a = 1 and σ( x ,a ) = x ,a on the event x ,a 0. The
k k k k k k k
k k h i h i h i ≥
following lemma shows how the expression further simplifies on the event ǫ =0 (no corruption event)
k
Lemma 9. As long as P(ǫ =0) p, we have
k
6 ≤
sign(σ( hx,a k i) −hx k,a k i+ǫ k) {hx k,a ki≥0
}
≥((1 −p)sign( hu k,a k i) −p) {hx k,a ki≥0 }.
Proof. First, we note that conditioned on the event that ǫ =0,
k
sign(σ( hx,a k i) −hx k,a k i) {hx k,a ki≥0
}
=sign( hx,a k i−hx k,a k i) {hx k,a ki≥0
}
=sign( hu k,a k i) {hx k,a ki≥0 }.
Indeed, if x,a < 0 then both expressions under the sign are negative, and if x,a 0 then both
k k
h i h i ≥
expressions are the same. We have the statement of the lemma by the law of total probability.
5.2. SGD drift estimates and proof of convergence theorem for ReLU regression. We have
the following result similar to Lemma 7:
Lemma 10. Let the random process Yˆ be defined as per (5.2), where a n are i.i.d. random vectors
k { j }j=1
in Rd satisfying Assumptions 2 and 3 with some constant C˜ > 0 and the corruption noise ǫ follows the
j
12
This manuscript is for review purposes only.
1
1
1
1
1
1
1
1
1
1
1Massart model 1. Let bethe σ-algebra generated by a ,ǫ , a ,ǫ ,..., a ,ǫ . Then for any step
k 0 0 1 1 k 1 k 1
F { } { } { − − }
size decay parameter λ>0 that satisfies
(1 2p)2 50
(5.3) 1<λ2 1+C2 − < ,
≤ 49d 49
for a= 1 , b= 3 and e
2(λ2 1) 2(λ2 1)
− −
1 λ2(1 2p)C 3 λ2
η =c∗ λ2 1; c∗ = − λ2 1( + )
− 8λ2 " √2√d − − 2 2 #
p e p
we have
C2(1 2p)2
E[eη(Yk+1−Yk)
{a ≤Yk<b
}|Fk]
≤
1
−
10−
0d !
e
Proof.
E[(Y Y ) ]
k+1
−
k {a ≤Yk<b }|Fk
=E u 2 u 2
k
k+1
k −k
k
k
{a ≤Yk<b
}
Fk
=Eh(cid:0) (λ2 −1) ku k k2 −λ(cid:1)2(2 hu k,a k(cid:12) (cid:12) (cid:12)isigi n(σ( hx,a k i) −hx k,a k i+ǫ k) −1) {hx k,a ki≥0
}
{a ≤ku kk2<b
}
Fk
( ≤i) h 23(cid:0) +λ2E A {hx k,a ki≥0
}
u k {a ≤ku kk2<b }−2λ2E A (1 −2p) |hu k,a k
i|
{hx k,a ki≥(cid:1) 0
}
u k {a ≤ku k(cid:12) (cid:12) (cid:12) k2<bi
}
(cid:18) (cid:20) (cid:12) (cid:21)(cid:19) (cid:20) (cid:12) (cid:21)
( =ii) 3 2 + λ 22 −λ2(1 −2p)(cid:12) (cid:12) {a ≤ku kk2<b }E A |hu k,a k i| u k (cid:12) (cid:12)
(cid:18) (cid:19) (cid:20) (cid:12) (cid:21)
3 λ2 C(1 2p)λ2 (cid:12)
+ − . (cid:12)
≤ 2 2 − √d 2(λ2 1)
(cid:18) (cid:19) e −
p
Here,thestep(i)requirestheassumptiononthesizeofaandLemma9,thestep(ii)requiresAssumption
3, and finally we use Assumption 2 in the last step. Note the similarity with equation (4.5). We proceed in
parallel with the proof of Lemma 7 to get using Taylor expansion
E[ec√λ2 −1(Yk+1−Yk)
{a ≤Yk<b
}|Fk] ≤1 −uc+4λ4c2,
λ2(1 2p)C 3 λ2
where u= − λ2 1( + ).
√2√d − − 2 2
e p
From the condition 1<λ2 <1+C2(1 −2p)2 we can take
49d
u e(1/√2 3/14 1/14)C(1 2p) 1 C(1 2p)
c ∗ = > − − − > −
8λ4 8λ2 √d 20 √d
(cid:20) (cid:21)
e e
This gives
E[ec∗√λ2 −1(Yk+1−Yk)
{a ≤Yk<b
}|Fk] ≤1 −4λ4(c ∗)2 ≤1
−
C2( 11 0− 0d2p)2 .
e
With the proof directly following the proof of Lemma 8, we also get:
Lemma 11. Let the random process Y be defined as per (4.2), where a n are i.i.d. random vectors
k { j }j=1
in Rd satisfying Assumption 1 with some constant C˜ > 0 and the corruption noise ǫ is non-zero with
j
13
This manuscript is for review purposes only.
1
1
1
1
1
1
1
1
1
1
1
1probability p< 1/2. Let be the σ-algebra generated by a ,ǫ , a ,ǫ ,..., a ,ǫ . Then, for any
k 0 0 1 1 k 1 k 1
F { } { } { − − }
step size decay parameter λ>0 that satisfies
(1 2p)2 50
1<λ2 1+C2 − < ,
≤ 49d 49
for e
1 1 λ2(1 2p)C 3 λ2
a= ; η =c∗ λ2 1; c∗ = − λ2 1( + )
2(λ2 1) − 8λ2 " √2√d − − 2 2 #
−
p e p
we have
C(1 2p)
E[eη(Yk+1−a)
{Yk<a
}|Fk] ≤exp 6√−
d
.
n e o
Proof of Theorem 4. This is a direct application of the estimate from Corollary 6 with the constants
K,D and ρ as obtained by Lemma 10 and Lemma 11.
Remark 8. (Random symmetric oblivious corruption for ReLU regression) For the random symmetric
oblivious noise, suppose that the k-th response is selected for corruption. The probability that the sign of
ǫ differs from the sign of σ( x,a ) x ,a is at most 1/2. Thus, the sign of σ( x,a ) x ,a +ǫ
k k k k k k k k
h i −h i h i −h i
differsfromthesignofσ( x,a ) x ,a withprobabilityatmost1/2. Sincetheprobabilityofk-thresponse
k k k
h i −h i
being selected for corruption is p, the overall probability that sign(σ( x,a ) x ,a ) =sign(σ( x,a )
k k k k
h i −h i 6 h i −
x ,a +ǫ ) is p/2. Hence, in this case, the inequality (i) in the proof of Lemma 10 is refined to
k k k
h i
E (λ2 −1) ku k k2 −λ2(2 hu k,a k isign(σ( hx,a k i) −hx k,a k i+ǫ k) −1) {hx k,a ki≥0
}
{a ≤ku kk2<b
}
Fk
h( ≤i(cid:0)) 3 2 +λ2E A {hx k,a ki≥0 } u k {a ≤ku kk2<b } (cid:1) (cid:12) (cid:12) (cid:12) i
(cid:18) (cid:20) (cid:12) (cid:21)(cid:19)
(cid:12)
−2λ2E A(cid:12)(1 −p) |hu k,a k
i|
{hx k,a ki≥0
}
u k {a ≤ku kk2<b }.
(cid:20) (cid:12) (cid:21)
(cid:12)
Now, the rest of the proof of Lemma 10 and Theorem 4 remain(cid:12) the same, except we have the factor (1 p)
−
instead of (1 2p) in Theorem 3. This allows the recovery of the signal for SGD-exp for any corruption
−
probability p < 1 under the random symmetric oblivious corruption model for ReLU regression, same as in
the linear case.
6. Experiments. In this section, we report on numerical experiments designed to validate our theo-
retical findings and demonstrate the performance of SGD-exp.
6.1. Experiments on random systems. In eachtrialof experiments in Section6.1, we generate the
measurement vectors a and x in Rd randomly according to the d-dimensional standard Gaussian random
k
x x
distribution. TheeffectivenessofthetestedmethodsisevaluatedbymeasuringtherelativeL2error, k −x kk2
in the number of iterations or data point access. We will further refer to it as relative error. k k2
6.1.1. SGD-exp vs SGD-root. The semilogplots inFigure2 implythatSGD-expconvergeslinearly
forasmallerstepsizedecayrateλ,whereasSGD-rootonlyprovidesasublinearconvergence. Here,themea-
surement vectors are the 100-dimensionalnormalized Gaussian vectors,and the relative errors are averaged
over 20 trials.
6.1.2. SGD-exp with various values of p and step sizes for sign-flip corruption. Figure 3
displaysthe relativeerrorplotsofSGD-expforrobustlinearregressionwithvariouschoicesofstepsizesand
different corruptionprobabilities p. The measurement vectorsare the 100-dimensionalnormalizedGaussian
vectors.
The corruption model is the sign corruption, in other words, the i-th measurement y is replaced with
i
y with probability p. This is a typicalexample of the Massartnoise since it requires the knowledge of the
i
−
signof x,a . We haveselectedthis model because,inrandomsymmetric obliviouscorruptionmodels that
j
h i
14
This manuscript is for review purposes only.
1 1
1
1 1
1 1SGD Error with Different Values vs. SGD-root / p = 0.4
100
10-5 S SG GD D- -e ex xp
p
/
/
: :1 1. .0 00 00 00 03 00 60
0
SGD-exp / :1.0000012
SGD-root
10-10
10-15
0 1 2 3 4 5 6 7 8 9
Number of iterations 106
Fig. 2. Linear regression with SGD-exp and SGD-root on streaming Gaussian data with corruption probability p = 0.4
(sign-flip corruptions). For SGD-exp, step size scales λ= 1.00003, λ= 1.000006, λ= 1.0000012. Larger λ results in faster
convergence but evenvery small λ are more efficientthan SGD-root in the long run. The plots are averaged over 20 runs.
involve adding large random errors, SGD-exp still converges to the true parameter even when p > 1/2 as
predicted by our theory and demonstrated by the experiments in the following subsection.
The plotsinFigure3indicate thatasthe stepsizedecayfactorλgetscloseto1,SGD-expconvergesfor
highervaluesofthecorruptionprobabilitypattheexpenseoftheconvergencerates. Therightplotconfirms
ourtheorythatSGD-expstillconvergeswhenthe corruptionprobabilityiscloseto 0.5forthe Massartnoise
type corruption.
SGD-exp / =1.000050 SGD-exp / =1.000010
100 100
p: 0.4
p: 0.4 p: 0.425
p: 0.425 p: 0.45
p: 0.45 p: 0.475
p: 0.475 p: 0.5
10-5 p p: : 0 0. .5 525 10-5 p: 0.525
10-10 10-10
10-15
0.5 1 1.5 2 2.5 3 3.5 4 0 0.5 1 1.5 2 2.5 3 3.5 4
Number of iterations 106 Number of iterations 106
Fig. 3. Linear regression with SGD-exp on streaming Gaussian data with different values of the corruption probability p
(sign-flip corruptions). With step size λ = 1.00005 (left) the error of SGD-exp converges linearly for p ≤ 0.45. With more
conservative step size λ=1.00001 (right) the error of SGD-exp converges linearly for p≤0.475. The plots are averaged over
10 runs.
6.1.3. SGD-exp with various values of p with recommended step size for the symmetric
oblivious corruption and the sign-flip corruption. Figure 4 illustrates the error plots of SGD-exp for
robust linear regression with several corruption probability p and associated step size parameter λ. Here,
the measurement vectors are the 50-dimensional rescaled standard normal Gaussian vectors. The response
y is corrupted by the random symmetric oblivious additive Gaussian noise with variance 30. Following
i
the guideline about the dependence of the parameter λ on T,d, and p as stated in Theorem 3, we set
λ= 1+2 d(1 lo− gp 2) T2 for various values of the corruption probability p.
Tqhe error plot on the left in Figure 4 supports our theory that SGD-exp still converges to the solution
foranycorruptionprobabilityp<1undertherandomsymmetricobliviouscorruptionmodel. Similarly,the
15
This manuscript is for review purposes only.
rorre
evitaleR
rorre
evitaleR
rorre
evitaleRrelative error plot on the right in Figure 4 validates our theory that SGD-exp still convergesto the solution
for any corruption probability p<1/2 under the Massart corruption noise.
SGD-exp / large Gaussian corruption SGD-exp / Sign flip corruption
100 100 p: 0.350 / :1.000038
p: 0.375 / :1.000035
p: 0.400 / :1.000032
p: 0.425 / :1.000030
p: 0.450 / :1.000027
p: 0.475 / :1.000025
10-5 p p:
:
0 0. .4
5
/
/
: :1 1. .0 00 00 00 02 16
8
10-5
p: 0.6 / :1.000011
p: 0.7 / :1.000006
p: 0.8 / :1.000003
p :0.9 / :1.000001
10-10 10-10
10-15 10-15
2 4 6 8 10 12 14 16 0 0.5 1 1.5 2 2.5
Number of iterations 106 Number of iterations 106
Fig. 4. Linear regression with SGD-exp on streaming Gaussian data with step-size λ recommended by Theorem 3. The
error of SGD-expconvergesapproximately linearly forp≤0.9forthe symmetric random oblivious corruption model (left)for
p≤0.475 for the sign-fliperror (right). The plots are averaged over 10 runs.
6.1.4. SGD-exp for robust ReLU regression. In this subsection, we illustrate the effectiveness of
SGD-expforrobustReLUregression. OurfindingssuggestthatSGD-expoutperformsGLM-Tron[21,23,41],
the popular ReLU regression method. GLM-Tron is essentially a (stochastic) iterative method for ReLU
regressionbasedontheℓ loss. Althoughworkssuchas[41]haveprovidedsomerobustnesspropertyofGLM-
2
Tron, our findings indicate that it is not robust with respect to the Massart noise, which is our corruption
model. In particular, as shown in Figure 5, SGD-exp provides (nearly) linear convergence for robust ReLU
regressionunder Massartnoise, while stochastic GLM-Tronwith constant, polynomialand exponentialstep
size decays (which we denote for brevity as GLM-Tron-constant, GLM-Tron-root and GLM-Tron-exp and
define precisely in the caption of Figure 5) all fail to converge. We note that on the same data without
corruptions (when p = 0) the same GLM-Tron-exp and GLM-Tron-root methods converge successfully,
achieving relative error of 10 15 and 10 2 respectively in 2 105 iteration steps.
− −
·
Stochastic GLM-Tron / p = 0 SGD-exp vs. Stochastic GLM-Tron / p = 0.4 / Sign filp corruption
1 102
Stochastic GLM-Tron-root
0.9 Stochastic GLM-Tron-exp
100
0.8
0.7 10-2
0.6 10-4
0.5
10-6
0.4
0.3 10-8
SGD-exp / :1.0000300
0.2 SGD-exp / :1.0000100
0.1
10-10 S
S
St
t
to
o
oc
c
ch
h
ha
a
as
s
st
t
ti
i
ic
c c
G
G
GL
L
LM
M
M-
-
-T
T
Tr
r
ro
o
on
n
n-
-
-c
r
eoo xon ptstant
0 10-12
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 0 1 2 3 4 5 6 7 8 9 10
Number of iterations 105 Number of iterations 105
Fig.5. StochasticGLM-Troninthestreamingsettingwithnocorruption(p=0)offersrecoveryofthetrueparameterfor
ReLU regression withboth square root decaying and exponentially decaying stepsize scheduling (left). The number of samples
inthedatasetisdenotedbym. Here,GLM-Tron-expemploysexponentiallydecayingstepsize1.00003−k/m,GLM-Tron-const
has step size 1/m and GLM-Tron-root has step size k−1/2/m. However, with sign corruption with p=0.4 (right), stochastic
GLM-Tron struggles with various step size choices (the error curves are similar). In contrast, the error of ReLU-SGD-exp
converges linearly in robust ReLU regression with signcorruption. The plots are averages over10 runs.
16
This manuscript is for review purposes only.
rorre
evitaleR
rorre
evitaleR
rorre
evitaleR
rorre
evitaleRSGD-exp vs. SGD-root for ReLU / p =0.4 / Sign flip corruption SGD-exp vs. SGD-root for ReLU / p =0.4 / large Gaussian corruption
102 102
SGD-exp for ReLU SGD-exp for ReLU
100 SGD-root for ReLU 100 SGD-root for ReLU
10-2 10-2
10-4 10-4
10-6 10-6
10-8 10-8
10-10 10-10
10-12 10-12
10-14 10-14
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 10
Number of iterations 105 Number of iterations 105
Fig. 6. ReLU regression with SGD-exp and SGD-root on streaming Gaussian data with corruption probability p = 0.4:
sign-flip corruptions (left) and large Gaussian corruptions (right). For SGD-exp, step size scales λ=1.00003. In both cases,
SGD-expoffers linear convergence whereas SGD-root fails toconverge to the true parameter.
One might also try different step size scheduling such as the square-root decay step size instead of the
exponentially decaying step size in the iteration equation (2.4) of SGD-exp for robust ReLU regression. In
fact,theiterationequation(2.4)withsquare-rootdecaystepsizecanbeviewedasanextensionofSGD-root
[31]toReLUregression. However,ourfindingssuggestthatthisextensionofSGD-rootdoesnotconvergeto
the true parameter as shown in Figure 6 for sign-flip/largeGaussian corruption,whereas SGD-exp provides
linear convergence for both types of corruption. This indicates that the exponential decaying step size in
SGD-exp is a correct choice for robust ReLU regression under the Massart noise model.
6.2. SGD-exp on real datasets. In this section, we demonstrate the effectiveness of SGD-exp for
linearandReLUregressionusingrealdatasetswithcorrupteddata. Weapplyourmethods ontwodatasets,
Red Wind Quality Data and Lending Club Loan Data and record the averageloss values over 10 trials.
6.2.1. Red Wine Quality Data. We use Red Wine Quality, a popular dataset for linear regression.
Thedatasetconsistsof1599sampleswithphysicochemicalcovariatesandsensoryresponsevariable. Among
the covariate variables, we only use 10 numerical ones for simplicity. The 10 features are fixedAcidity,
volatileAcidity,citricAcid,residualSugar,chlorides,freeSulfurDioxide,density,pH,sulphates,
alcohol. We centralize/normalizethese features and corrupt the response variable by adding large random
Gaussian noise drawn from [ 300,300] with probability p = 0.2. The parameter λ = 1.006. A similar
−
corruption model with real dataset has been used in [16].
Tosimulatethei.i.d. samples,werandomlyselectoneofthedatapointfromthedatasetateachiteration
of SGD-exp. Because typically there is no such reasonable ground truth vector x for the real datasets, we
measure the performance of methods using the ℓ loss function associated uncorrupted dataset instead. In
2
Figure 7, we record the ℓ loss of the uncorrupted dataset, 1 Ax y˜ 2, where y˜ is the corresponding
2 mk − k
uncorrupted response vector.
The loss value after one pass of the corrupted dataset using SGD-exp for linear regression is about
0.463. For smaller scale of noise, drawn from [ 3,3], a similar plot to that in Figure 7 is obtained, which
−
is omitted here, where we have obtained the loss value 0.450. These values are close to the optimal value
0.4220 which can be obtained by running the conventionallinear regressionon the uncorrupted dataset. If
we run the linear regression on the corrupted dataset, the loss value is over 32.11, much higher than the
one associated with the uncorrupted dataset. As for robust ReLU regression, we obtain a similar plot for
SGD-exp for ReLU regression,whereas GLM-Tron suffers from the corruption as shown in Figure 8.
6.2.2. Lending Club Loan Data. Lending Club Loan dataset comprises 10000 samples with loan
data from 2007 to 2015 issued by Lending Club, a lending company in the US. There are several features
in the data such as interest rates, loan amounts, balances, and so on. We use the first 34 features in the
datasetand the response variable is paid_total. We normalize the feature/responsevectorsin the dataset,
17
This manuscript is for review purposes only.
rorre
evitaleR
rorre
evitaleRFig. 7. Linear regression with SGD-exp on the Red Wine Quality dataset with the corruption probability p = 0.2 and
λ=1.006 (left) and on the Lending Club Loan dataset with the corruption probability p=0.2 and λ=1.00002 (right). The
plots are averaged over 10 runs.
Fig. 8. ReLU regression for the Red Wine Quality dataset with large Gaussian corruption with p= 0.2: SGD-exp with
λ=1.006 obtains low loss, whereas stochastic GLM-Tron with various step sizes suffers from noise. The number of samples
in the data set m=1599. Here, GLM-Tron-expemploys exponentially decaying step size 1.00003−k/m, GLM-Tron-const has
step size 1/m and GLM-Tron-root has step size k−1/2/m. The plots are averages over10 runs.
which is the typical preprocessing step for many machine learning algorithms.
As before, the data points are randomly drawn from the dataset at each iteration of SGD-exp. Under
the random corruption model with probability p = 0.2 by adding Gaussian noise drawn from [ 3,3] to the
−
responsevariableandsettingλ=1.00002,werecordtheℓ lossofSGD-expinFigure7. Thelossvalueafter
2
50,000iterations of SGD-exp is 1.0116e-06,which is the optimal value by running the linear regressionon
the uncorrupted dataset. On the other hand, the ℓ loss of linear regression on the corrupted dataset that
2
is obtained by a direct solver is 0.0017. This demonstrates the effectiveness of our method, SGD-exp, in
handling random corruption.
7. Conclusion. In this paper, we have introduced Stochastic Gradient Descent with Exponential De-
cay (SGD-exp) for linear and ReLU regression in streaming settings under the presence of semi-random
adversarial corruptions. Through theoretical analysis and numerical experiments, we have established that
SGD-expoffersnear-linearconvergenceratesforcorruptionprobabilitieslessthan0.5fortheMassartmodel
and1forthesymmetricobliviouscorruptionmodel,optimalforbothcases. Future researchavenuesinclude
exploring SGD-exp’s application to other robust optimization problems, further refining the convergence
analysis under different noise models, and extending the framework to accommodate additional forms of
non-linearity and constraints.
18
This manuscript is for review purposes only.REFERENCES
[1] Mart´ınAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregSCorrado,AndyDavis,
JeffreyDean,MatthieuDevin,etal. Tensorflow: Large-scalemachinelearningonheterogeneous distributedsystems.
arXiv preprint arXiv:1603.04467, 2016.
[2] YouheiAkimoto,AnneAuger,TobiasGlasmachers,andDaikiMorinaga. Globallinearconvergenceofevolutionstrategies
onmorethansmoothstronglyconvexfunctions. SIAM Journal on Optimization,32(2):1402–1429, 2022.
[3] NikhilBansalandJoelHSpencer. On-linebalancingofrandominputs. RandomStructures&Algorithms,57(4):879–891,
2020.
[4] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint
arXiv:1206.6389, 2012.
[5] Carla E Brodley and Mark A Friedl. Identifying mislabeled training data. Journal of artificial intelligence research,
11:131–167, 1999.
[6] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In
International conference on machine learning,pages 774–782. PMLR,2013.
[7] IliasDiakonikolas,DanielMKane,AnkitPensia,andThanasisPittas. Streamingalgorithmsforhigh-dimensionalrobust
statistics. InInternational Conference on Machine Learning,pages 5061–5117. PMLR,2022.
[8] Ilias Diakonikolas, Sushrut Karmalkar, Jong Ho Park, and Christos Tzamos. Distribution-independent regression for
generalized linear models with oblivious corruptions. In The Thirty Sixth Annual Conference on Learning Theory,
pages 5453–5475. PMLR,2023.
[9] IliasDiakonikolas,WeihaoKong,andAlistairStewart. Efficientalgorithmsandlowerboundsforrobustlinearregression.
In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2745–2754. SIAM,
2019.
[10] IliasDiakonikolas,VasilisKontonis, ChristosTzamos, andNikosZarifis. Learningasingleneuronwithadversariallabel
noiseviagradientdescent. InConference on Learning Theory,pages 4313–4361. PMLR,2022.
[11] Ilias Diakonikolas, Jong Ho Park, and Christos Tzamos. ReLU regression with Massart noise. Advances in Neural
Information Processing Systems,34:25891–25903, 2021.
[12] RongGe,ShamMKakade,RahulKidambi,andPraneethNetrapalli. Thestepdecayschedule: Anearoptimal,geomet-
rically decaying learning rate procedure for least squares. Advances in Neural Information Processing Systems, 32,
2019.
[13] PrasantaGogoi,DhrubaKBhattacharyya,BhogeswarBorah,andJugalKKalita. Asurveyofoutlierdetectionmethods
innetworkanomalyidentification. The Computer Journal, 54(4):570–588, 2011.
[14] MertGurbuzbalaban,UmutSimsekli,andLingjiongZhu.Theheavy-tailphenomenoninSGD.InInternationalConference
on Machine Learning,pages3964–3975. PMLR,2021.
[15] JamieHaddock,AnnaMa,andElizavetaRebrova. OnsubsampledquantilerandomizedKaczmarz. In2023 59th Annual
Allerton Conference on Communication, Control, and Computing (Allerton),pages1–8.IEEE,2023.
[16] Jamie Haddock, Deanna Needell, Elizaveta Rebrova, and William Swartworth. Quantile-based iterative methods for
corruptedsystemsoflinearequations. SIAM Journal on Matrix Analysis and Applications, 43(2):605–637, 2022.
[17] BruceHajek. Hitting-timeandoccupation-time boundsimpliedbydriftanalysiswithapplications. Advances in Applied
probability, 14(3):502–525, 1982.
[18] CullenHaselby,MarkAIwen,DeannaNeedell,ElizavetaRebrova,andWilliamSwartworth. Fastandlow-memorycom-
pressivesensing algorithms forlow Tucker-ranktensor approximation fromstreamed measurements. arXiv preprint
arXiv:2308.13709, 2023.
[19] Peter JHuber. Theplace oftheL1-norminrobustestimation. Computational statistics& data Analysis,5(4):255–262,
1987.
[20] PeterJHuber. Robustestimationofalocationparameter. InBreakthroughs instatistics: Methodology and distribution,
pages 492–518. Springer,1992.
[21] ShamMKakade,VarunKanade,OhadShamir,andAdamKalai. Efficientlearningofgeneralizedlinearandsingleindex
modelswithisotonicregression. Advances in Neural Information Processing Systems,24,2011.
[22] AdamTaumanKalaiandRaviSastry. Theisotronalgorithm: High-dimensionalisotonicregression. InCOLT,2009.
[23] Sayar Karmakar and Anirbit Mukherjee. Provable training of a ReLU gate with an iterative non-gradient algorithm.
Neural Networks,151:264–275, 2022.
[24] AdamKlivans,PraveshKKothari,andRaghuMeka. Efficientalgorithmsforoutlier-robustregression. InConferenceOn
Learning Theory,pages 1420–1430. PMLR,2018.
[25] Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Econometric Society, pages
33–50, 1978.
[26] Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. A second look at exponential and cosine step sizes: Simplicity,
adaptivity, andperformance. InInternational Conference on Machine Learning,pages 6553–6564. PMLR,2021.
[27] QingyangLin,SJNeethling, KatherineJDobson, LCourtois, andPeter D Lee. Quantifyingand minimisingsystematic
andrandomerrorsinx-raymicro-tomographybasedvolumemeasurements. Computers&Geosciences,77:1–7,2015.
[28] LiuLiu,YanyaoShen,TianyangLi,andConstantineCaramanis. Highdimensionalrobustsparseregression. InInterna-
tional Conference on ArtificialIntelligence and Statistics,pages 411–421. PMLR,2020.
[29] Pasin Manurangsi and Daniel Reichman. The computational complexity of training ReLUs. arXiv preprint
arXiv:1810.04207, 2018.
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
19
This manuscript is for review purposes only.Advances in Neural Information Processing Systems,32,2019.
[31] ScottPesmeandNicolasFlammarion. OnlinerobustregressionviaSGDontheL1loss. AdvancesinNeuralInformation
Processing Systems,33:2540–2552, 2020.
[32] VatsalShah,XiaoxiaWu,andSujaySanghavi. ChoosingthesamplewithlowestlossmakesSGDrobust. InInternational
Conference on Artificial Intelligence and Statistics,pages 2120–2130. PMLR,2020.
[33] Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. Advances in neural information processing systems, 30,
2017.
[34] Stefan Steinerberger. Quantile-based random Kaczmarz for corrupted linear systems of equations. Information and
Inference: A Journal of the IMA,12(1):448–465, 2023.
[35] JacobSteinhardt,PangWeiWKoh,andPercySLiang. Certifieddefensesfordatapoisoningattacks. Advancesinneural
information processing systems,30,2017.
[36] Che-Ping Tsai, Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. Heavy-tailed streaming statistical
estimation. InInternational Conference on Artificial Intelligence and Statistics,pages1251–1282. PMLR,2022.
[37] John Wilder Tukey. A survey of sampling from contaminated distributions. Contributions to probability and statistics,
pages 448–485, 1960.
[38] RomanVershynin.High-dimensionalprobability: Anintroductionwithapplicationsindatascience,volume47.Cambridge
universitypress,2018.
[39] Xiaoyu Wang, Sindri Magnu´sson, and Mikael Johansson. On the convergence of step decay step-size for stochastic
optimization. Advances in Neural Information Processing Systems,34:14226–14238, 2021.
[40] Xiaoyu Wang and Ya-xiang Yuan. On the convergence of stochastic gradient descent with bandwidth-based step size.
Journal of Machine Learning Research, 24(48):1–49, 2023.
[41] JingfengWu,DifanZou,ZixiangChen,VladimirBraverman,QuanquanGu,andShamMKakade. Finite-sampleanalysis
of learning high-dimensional single ReLU neuron. In International Conference on Machine Learning, pages 37919–
37951. PMLR,2023.
[42] GiladYehudai and Shamir Ohad. Learning a singleneuron with gradient methods. In Conference on Learning Theory,
pages 3756–3786. PMLR,2020.
20
This manuscript is for review purposes only.