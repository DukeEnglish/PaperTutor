Greed is All You Need: An Evaluation of Tokenizer Inference Methods
OmriUzanβ CraigW.Schmidtκ ChrisTannerκµ YuvalPinterβ
β DepartmentofComputerScience κ KenshoTechnologies
Ben-GurionUniversityoftheNegev µ MassachusettsInstituteofTechnology
BeerSheva,Israel Cambridge,MA,USA
{omriuz@post, uvp@cs}.bgu.ac.il {craig.schmidt, chris.tanner}@kensho.com
Abstract Tokenizer Segmentation
inferencemode
BPE Ul tr am od ern
While subword tokenizers such as BPE and merges
BPE Ultra modern
longestprefix
WordPiece are typically used to build vocab-
UnigramLM U nprecedented
ularies for NLP models, the method of de- likelihood
UnigramLM Un precedent ed
longestprefix
coding text into a sequence of tokens from
SaGe Inc once iva ble
thesevocabulariesisoftenleftunspecified,or longestprefix
SaGe In conceiv able
ill-suited to the method in which they were likelihood
constructed. Weprovideacontrolledanalysis
Table 1: Examples of words being segmented differ-
of seven tokenizer inference methods across
entlybyvarioustokenizers(vocabsize32,000)using
fourdifferentalgorithmsandthreevocabulary
differentinferencemodesonthesamevocabulary. Each
sizes, performed on a novel intrinsic evalua-
tokenizer’sdefaultmodeisprovidedontop.
tion suite we curated for English, combining
measuresrootedinmorphology,cognition,and
informationtheory. Weshowthatforthemost
iftheinference-timedecodingissuitablewiththe
commonly used tokenizers, greedy inference
algorithmusedtolearnthetokenizer’svocabulary.
performs surprisingly well; and that SaGe, a
recently-introducedcontextually-informedtok- Moreover,itisyettobedeterminedwhethersuch
enizer,outperformsallothersonmorphological amatchisnecessaryorideal.
alignment. InTable1wepresentexamplesdemonstrating
howtheprescribedinferencemethodsofBPE,Uni-
1 Introduction
gramLM,andSaGe(YehezkelandPinter,2023)do
Modern NLP systems, including large language notnecessarilyprovidethebestsegmentationfor
models (LLMs), typically involve an initial step complexEnglishwords,evenwhengoodsegments
of mapping raw input text into sequences of sub- areavailableinthevocabulary. BPE’smergealgo-
word tokens. These tokens are selected from a rithmselectstousethecross-morphemic‘am’se-
largevocabularyofcandidatesthatwereproduced quenceatanearlystage,preventingtheconsidera-
fromalgorithmssuchasByte-PairEncoding(BPE; tionof‘ultra’and‘modern’. UnigramLM’sablative
Sennrich et al., 2016), WordPiece (Schuster and algorithmenabled‘nprecedented’(whichcrosses
Nakajima,2012),orUnigramLM(Kudo,2018). morpheme boundaries) to remain in its final vo-
This process is referred to as the inference cabularyoftokens,whileSaGe’sgreedyalgorithm
method of tokenization and is critical in nature, maskstheboundariesofboththeprefix‘In’andthe
asitdetermineshowallwordsarerepresentedand suffix‘able’. Inallcases,analternativeinference
subsequentlymodeled. Eachinferencemethodof- methodprovidesamoremorphologically-aligned
fersdistinctmappings,andweassertthatitisnot segmentationoverthesamevocabulary.
well-understoodhowthesemethodsdifferinperfor- Previousworkregardingsubwordtokenization
mance. Further,popularimplementationpackages mostlyconcernsdevelopingvocabularyconstruc-
suchasHuggingfaceTokenizers,1 SentencePiece,2 tionalgorithms(Sennrichetal.,2016;Schusterand
andSubwordNMT3oftenobfuscateorevenrestrict Nakajima,2012;Kudo,2018;Mielkeetal.,2021;
thechoiceofinferencemethods,makingitunclear Yehezkel and Pinter, 2023), finding the optimal
vocabularysize(GowdaandMay,2020;Gutierrez-
1https://huggingface.co/docs/tokenizers
Vasquesetal.,2021),buildingmultilingualvocab-
2https://pypi.org/project/sentencepiece
3https://github.com/rsennrich/subword-nmt ularies(Liangetal.,2023),andusingspaceposi-
4202
raM
2
]LC.sc[
1v98210.3042:viXrationinginthevocabularytokens(Gow-Smithetal., methods:
2022; Jacobs and Pinter, 2022). Others analyzed
the effects of vocabularies, finding intricate rela- Greedy inference methods only consider and
tions between algorithm and downstream perfor- produce one token at each step. We define three
mance(BostromandDurrett,2020), information greedyapproaches: Longestprefix,whichWord-
theory (Zouhar et al., 2023), cognitive plausibil- Piece uses by default (Wu et al., 2016), selects
ity (Beinborn and Pinter, 2023), or morphologi- the longest token in V that is a prefix of w, and
calalignment(KleinandTsarfaty,2020;Hofmann then continues to iteratively segment the remain-
etal.,2021,2022;Gow-Smithetal.,2022). ingtext. Longestsuffixselectsthelongesttoken
Research concerning inference methods has that is a suffix of w and continues iteratively (Ja-
beenmorescarce,andincludesexaminationofran- cobs and Pinter, 2022; Bauwens, 2023). Since
domeffectsonBPEmerges(Provilkovetal.,2020; this strategy diverges from English Morphology,
SalevaandLignos,2023)andapplicationofsophis- weconsideritanintriguingbaselineforassessing
ticatedsearchalgorithms(Heetal.,2020). Asfar theimpactoflinguisticstructureontheinference
asweknow,thereexistsnocomprehensivestudy method. Longesttokenselectsthelongesttoken
comparing inference methods across a variety of that is contained in w, adds it to the generated
vocabulariesandsizesusingdiversemetrics. segmentation,andtheniterativelysegmentseach
In this work, we conduct a controlled experi- remainingcharactersequence. Thiswasproposed
mentisolatingtheeffectsofinferencemethodsover by Hofmann et al. (2022) to approximate words
fourtokenizers,introducinganevaluationsuiteag- bytheirk longesttokens. Theyshowedthatitpre-
gregating intrinsic benchmarks from various the- servesmorphologicalstructureofwordsandleads
oretical realms.4 We find that greedy inference toperformancegainsonsomedownstreamtasks.
methods work surprisingly well for all four vo-
Merge rules-based inference methods begin
cabulariesacrossmorphologicalandinformational
with a word’s character sequence and iteratively
metrics. Furthermore, wedemonstratethatSaGe
applytoken-formingmergeruleslearntbythetok-
yields state-of-the-art performance according to
enizeratthevocabularycreationphase,untilnone
morphologicalmetrics,andthatinferencemethods
can be applied. This is BPE’s default inference
that minimize token counts perform strongest by
mode.5 Inourexperimentswetesttwovariantsfor
cognitivemetrics.
BPE:Thedeterministicmergestrategyrecursively
2 InferenceMethods appliesthefirstapplicableBPEmergerulebyits
orderinthetrainedmergelist. Dropout(Provilkov
LetV denoteavocabularyofsubwordtokensand et al., 2020) applies each valid merge rule with
w denote a word (or ‘pretoken’), the output of a probability p, leading to a regularization effect
pretokenizer. Wedefines(V,w) := (t 1,...,t k)asa whereraretokenssurfacemoreoftenandtheirem-
segmentationofwintok subwordtokenssuchthat beddingscanbebettertrained. Ithasbeenshown
∀i,t i ∈ V and that the concatenation of t 1,...,t k toimprovemachinetranslationperformance.
results in w. We use the term segmentation to
denotetheapplicationofaninferencemethod ona Likelihood-basedinferencemethods useindi-
textgivenatokenvocabulary,aswellasitsresult. viduallikelihoodvaluesassignedtotokensinorder
Current widely-employed tokenization sched- tofindasegmentationforw wherethetotallikeli-
ulescoupletogetherthetokenizervocabularywith hoodismaximized(Kudo,2018;Heetal.,2020).
the inference method. However, we advocate for Defaultuseslikelihoodvalueslearnedduringvo-
decoupling them, as they are independent pro- cabularyconstructionandconsidersthelikelihood
cesses. Specifically,givenafixedtokenvocabulary of a segmentation to be the product of individ-
producedfrompre-trainingdata,onecouldsubse- ual likelihoods (from which UnigramLM gets its
quently use any applicable inference method for name). Leasttokensassignsaconstantlikelihood
the task at hand. Thus, in our experiments, we valuetoalltokens,effectivelyselectingasegmen-
usevariousintrinsicmetricstoanalyzetheimpact tationwherethenumberoftokensisminimized.
andperformanceoftheseveralclassesofinference
5While ostensibly also compatible with WordPiece, we
4We release our code and data at https://github.com/ foundnoimplementationofthemodelthatprovidesanordered
MeLeLBGU/tokenizers_intrinsic_benchmark. listofitsmerges.Resource Type Size Reference License
LADEC Morphological 7,804 Gagnéetal.(2019) CCBY-NC4.0DEED
MorphoLex Morphological 12,029 Sánchez-Gutiérrezetal.(2018) CCBY-NC-SA4.0DEED
MorphyNet Morphological 219,410 Batsurenetal.(2021) CCBY-SA3.0DEED
DagoBert Morphological 279,443 Hofmannetal.(2020) Notspecified—citationbased
UniMorph Morphological 143,454 Batsurenetal.(2022) CCBY4.0DEED
UnBlend Morphological 312 Pinteretal.(2020) GPL-3.0
CompoundPiece Morphological 22,896 Minixhoferetal.(2023) Notspecified—citationbased
Cognitivedata Cognitive 55,867 BeinbornandPinter(2023) MIT
tokenization-scorer InformationTheory — Zouharetal.(2023) Notspecified—citationbased
Table2: Size,ReferenceandLicensedetailsoftheresourcesinourbenchmark.
3 IntrinsicBenchmark 2022),novelblendstructuredetectiondata(Pinter
etal.,2020),andcompoundseparationdata(Minix-
Some analyses of tokenizers rely on training lan-
hoferetal.,2023). Thenumberofwordsineach
guage models and evaluating their performance
resource can be found in Table 2. We compare
on downstream tasks. Using this process to iso-
thesegmentationsgeneratedbythetokenizerswith
lateeffectsoftokenizationhyperparameters,such
each inference method to gold-standard morpho-
as inference method, is both time- and resource-
logicalsegmentationsusingthemetricintroduced
consuming,aswellasunstableduetotheintroduc-
byCreutzandLinden(2004),andreportthemacro-
tionofmultiplesourcesofrandomnessthroughout
averagedF scoreoverthedifferentresources.
1
theLMpre-trainingandfine-tuningphases. Addi-
tionally, few measures have been introduced that Cognitive Plausibility We use the benchmark
areintrinsictovocabulariesandtheirdirectappli- anddatafromBeinbornandPinter(2023)tomea-
cationtocorpora,andfewerstillavoidconflating sure the correlation of a tokenizer’s output with
themeasureswiththeobjectivesusedinthevocab- the response time and accuracy of human perfor-
ulary construction process itself. As a result, the mance on a lexical decision task, predicated on
bodyofworkfocusedonimprovingtokenization thehypothesisthatagoodtokenizerstruggleswith
schemesisstillrelativelysmall. charactersequencesthathumansfinddifficult,and
We create and release a benchmark to intrin- viceversa. Wereporttheaverageoftheabsolute
sically evaluate subword tokenizers. We col- valuecorrelationscoresacrossthefourlinguistic
lected word-level datasets and information mea- setups(word/nonword×accuracy/responsetime).
sureswhichhavebeenshown,orhypothesized,to
correlate with the performance of language mod- Tokens distribution statistics We report the
elsonvariousdownstreamtasks. Detailsonthese Rényiefficiencyofdifferentsegmentationsacross
resourcesareprovidedinTable2. Atpresent,the acorpus(Zouharetal.,2023). Thismeasurepenal-
benchmarkisfocusedontheEnglishlanguage,al- izes token distributions dominated by either very
though corresponding datasets exist for others as high-and/orverylow-frequencytokens,andwas
well. showntocorrelatestronglywithBLEUscoresfor
machinetranslation(MT)systemstrainedonthere-
Morphologicalalignment Wehypothesize,for spectivetokenizers. Recentwork (Cognettaetal.,
agiventokenizer,thatalignmentofwordsegments 2024) reveals a misalignment between Rényi ef-
to morphological gold-standard segmentations is ficiency and downstream performance in certain
apredictoroftheabilityofalanguagemodelthat cases, reinforcing the necessity of an evaluation
uses the given tokenizer to represent words, es- suitegroundedindiversedomainsanddisciplines,
pecially ‘complex’ ones that are made up of sev- as advocated in this work. We also measure the
eralrootsorcontainmultiplemorphologicalaffixes. averagenumberoftokensperwordoveracorpus,
We follow Gow-Smith et al. (2022) and evaluate asaproxyforcompressionquality. Lastly,were-
our tokenizers on LADEC (Gagné et al., 2019), porttheproportionofpretokensthataresegmented
MorphoLex(Sánchez-Gutiérrezetal.,2018),Mor- differentfromthedefaultacrossthiscorpusasaref-
phyNet(Batsurenetal.,2021),andDagoBert(Hof- erence. Weomitthepopularmeasureofcharacter-
mann et al., 2020). We augment these datasets lengthdistributionofthetokensinthevocabulary,
withmorphemesegmentationdata(Batsurenetal., asitdoesnotvarywithsegmentationstrategy.Vocab Inference Morphological Cognitive Rényi Tokens Decoding
method alignment plausibility efficiency perword diff
longestprefix .8584 .3266 .4482 1.4273 .0502
longestsuffix .6467 .3170 .4482 1.4286 .0417
longesttoken .8738 .3302 .4474 1.4261 .0484
BPE
leasttokens .7544 .3321 .4476 1.4237 .0382
det.merges .6309 .3355 .4482 1.4308 —
dropoutmerge .6081 .2925 .4537 1.5793 .1313
longestprefix .8488 .3307 .4507 1.4430 —
longestsuffix .6288 .3198 .4502 1.4435 .0656
WordPiece
longesttoken .8466 .3332 .4500 1.4411 .0216
leasttokens .7342 .3306 .4401 1.4319 .0682
longestprefix .9222 .2858 .3400 1.7577 .1187
longestsuffix .7520 .2690 .2897 1.7624 .0516
UnigramLM longesttoken .8845 .2948 .3040 1.7353 .0406
leasttokens .8982 .2953 .2969 1.7219 .0328
likelihood .9149 .2937 .2919 1.7314 —
longestprefix .9606 .2581 .3217 1.9445 —
longestsuffix .7370 .2471 .2832 1.9615 .1704
SaGe longesttoken .9236 .2671 .3027 1.9236 .0887
leasttokens .9125 .2674 .2944 1.8895 .1318
likelihood .9515 .2664 .2937 1.9156 .1168
Table3: IntrinsicBenchmarkresultsonavocabsizeof40k. ‘Default’decodingalgorithms(usedinvocabulary
construction) in italics. Not all methods are applicable to all tokenizers. Decoding diff presents the share of
pretokensintheMiniPiletestsetthataredifferentlytokenizedusingthemethod,comparedwiththedefault.
4 Experiments pendixB.
We evaluate inference methods for the following
Inference methods Within each tokenizer, we
tokenizervocabularies: BPE,UnigramLM,Word-
find that the default (‘intended’) strategy is often
PieceandSaGe. WeusethetrainsplitoftheMiniP-
outperformed by others on some measures. We
ile (Kaddour, 2023) dataset to construct the tok-
observeasignificantdifferenceinmorphological
enizervocabularies. Wetrainvocabulariesofsizes
alignmentwhenusingmergerules-basedinference
32,768,40,960,and49,152,usingtheHuggingFace
methods. Qualitativeanalysisshowedthefindings
Tokenizerslibrary,withidenticalpre-tokenization,
illustratedinTable1,whereearlymergerulessuch
representing the text at byte level. UnigramLM
as‘i-n’,‘a-m’,or‘o-n’crossmorphologicalbound-
and SaGe require an initial vocabulary for their
aries. We notice a similar trend for likelihood-
top-downalgorithms;fortheformer,weusedthe
basedinference,wherefrequently-usedtokenspos-
defaultimplementationofonemilliontopn-grams,
sessveryhighlikelihoodvalues,sometimesexceed-
whileSaGewasinitializedwitha262K-sizeUni-
ingthoseofthegold-standardsegments. Wefind
gramLMvocabulary. Thisinitialvocabularyalso
thattheleasttokensstrategyfareswellnotonlyon
provideduswithtokenlikelihoodscoresforinfer-
thetokencountmetric,whichismostlyby-design,
ence,althoughamoreexactimplementationwould
butalsooncognitivemeasures,suggestinganeffect
alsoincorporatethecontextualSaGeobjective.
of human preference to minimal word segmenta-
Tokendistributionstatisticsmeasurementswere
tion. Finally,weobservethatlikelihood-basedin-
computed over the test split of the MiniPile
ferenceperformspoorlyintermsofRényiefficieny,
dataset. WemeasuretheRényiefficiencyusingthe
contrarytoitsstatedpurpose. Dropout,ontheother
tokenization-scorer package6 with α = 2.5. For
hand,performswellonthismeasure,inlinewith
eachtokenizer,allexperimentsranwithinseveral
itsgoal. longestsuffixperformspoorlyacrossthe
minutesonapersonallaptopcomputer.
board, possibly due to the suffixing nature of the
We present the results on our benchmark for
Englishlanguage,whichhascomplementarilybeen
the40KvocabulariesinTable3. Resultsforother
shown to affect character-level sequential model-
sizesarepresentedinAppendixA. Abreakdown
ing(Pinteretal.,2019). Notably,allourkeyobser-
ofindividualevaluationsubsetsisprovidedinAp-
vations are consistent across vocabulary sizes, as
6https://github.com/zouharvi/tokenization-scorer showninAppendixA.Inter-tokenizer results Our results align with forasetasdiverseaspossiblemostlyintermsof
BostromandDurrett(2020)’sfindingthatBPEis typologyandscript.
inferiortoUnigramLMonmorphologyalignment. Ourevaluationislimitedtointrinsicmeasures.
However, we show that some of this gap can be Whilethismakesdevelopmentoftokenizerseasier,
attributednottothevocabularybuttotheinference weacknowledgethatthebodyofworkcorrelating
method. In addition, we find that SaGe is most success on these measures with performance of
alignedtomorphologybyasubstantialmargin,in- downstreammodelsonend-tasksisincomplete.
dicatingthatitscontextualizedobjectivesucceeds
EthicalConsiderations
in retaining meaningful tokens in the vocabulary
duringablation. BPEandWordPiece,optimizedfor
Details for human annotation for the cognitive
compression, unsurprisingly perform well above
benchmark are documented in the source bench-
thelikelihood-basedvocabulariesontheinforma-
mark’s paper (Beinborn and Pinter, 2023), from
tionmeasures. But,wenotethatthiscarriesoverto
whichwetookthedataas-is.
thecognitivebenchmarkaswell,supportingBein-
bornandPinter(2023)’sfindings. Acknowledgments
Finally, we note that the two likelihood-based
WewouldliketothankCharlieLovering,Varshini
vocabularies follow the exact same within-vocab
Reddy,andHaoranZhangforcommentsonearly
trends,andtheonesforthetwoinformation-based
drafts of this paper. This research was supported
vocabularies are also very close. This highlights
inpartbytheIsraelScienceFoundation(grantNo.
theconsistencyandrobustnessofourbenchmark,
1166/23) and by a Google gift intended for work
althoughsomeresultsarerelativelyclosetoeach
onMeaningfulSubwordTextTokenization.
other,whichcanbeexpectedconsideringthatsome
inferencemethodsdonotchangemuchofthetoken
sequences(seerightmostcolumnofTable3). References
Khuyagbaatar Batsuren, Gábor Bella, and Fausto
5 Conclusion
Giunchiglia.2021. MorphyNet: alargemultilingual
databaseofderivationalandinflectionalmorphology.
Inthiswork,wecuratedanaggregatedbenchmark
InProceedingsofthe18thSIGMORPHONWorkshop
forintrinsicevaluationofsubwordtokenizersand
onComputationalResearchinPhonetics,Phonology,
usedittoshowtheimportanceofselectinganinfer- andMorphology,pages39–48,Online.Association
encemethodsuitedforavocabularygivenatask. forComputationalLinguistics.
Given its computational efficiency, we hope the
KhuyagbaatarBatsuren,OmerGoldman,SalamKhal-
benchmarkcanbeusedinLMtrainingeffortsasa ifa, Nizar Habash, Witold Kieras´, Gábor Bella,
fruitfulfirststeptoimprovetokenizationschemes, BrianLeonard,GarrettNicolai,KyleGorman,Yusti-
nusGhanggoAte,MariaRyskina, SabrinaMielke,
ortoselectinferencemethodson-line. Concretely,
Elena Budianskaya, Charbel El-Khaissi, Tiago Pi-
ourfindingssuggestthatgreedyinferenceisagood
mentel, Michael Gasser, William Abbott Lane,
choice,especiallyformorphologically-motivated Mohit Raj, Matt Coler, Jaime Rafael Montoya
tasks, even for tokenizers trained on other objec- Samame, Delio Siticonatzi Camaiteri, Esaú Zu-
tives. maeta Rojas, Didier López Francis, Arturo Once-
vay, Juan López Bautista, Gema Celeste Silva Vil-
Inthefuture,weplantoexaminethecorrelation
legas, Lucas Torroba Hennigen, Adam Ek, David
betweenourbenchmarkandvariousdownstream Guriel, Peter Dirix, Jean-Philippe Bernardy, An-
tasks, as well as expand our experimentation to drey Scherbakov, Aziyana Bayyr-ool, Antonios
otherlanguagesandnewalgorithms. Anastasopoulos,RobertoZariquiey,KarinaSheifer,
Sofya Ganieva, Hilaria Cruz, Ritván Karahógˇa,
StellaMarkantonatou,GeorgePavlidis,MatveyPlu-
Limitations
garyov, Elena Klyachko, Ali Salehi, Candy An-
gulo, Jatayu Baxi, Andrew Krizhanovsky, Natalia
OurpapercontainsevaluationofmodelsintheEn-
Krizhanovskaya,ElizabethSalesky,ClaraVania,Sar-
glishlanguage. Thiswasdonemostlyinorderto dana Ivanova, Jennifer White, Rowan Hall Maud-
focusthisshortpaper’scontribution,andtobeable slay,JosefValvoda,RanZmigrod,PaulaCzarnowska,
tocontrolforasmanypossibly-confoundingvari- Irene Nikkarinen, Aelita Salchak, Brijesh Bhatt,
Christopher Straughn, Zoey Liu, Jonathan North
ablessuchastrainingdata. Nevertheless, amore
Washington, YuvalPinter, DuyguAtaman, Marcin
completefollowupwouldhavetoincludeattempts
Wolinski, Totok Suhardijanto, Anna Yablonskaya,
toreplicateourfindingsonotherlanguages,aiming Niklas Stoehr, Hossep Dolatian, Zahroh Nuriah,Shyam Ratan, Francis M. Tyers, Edoardo M. Xuanli He, Gholamreza Haffari, and Mohammad
Ponti, Grant Aiton, Aryaman Arora, Richard J. Norouzi. 2020. Dynamic programming encoding
Hatcher, Ritesh Kumar, Jeremiah Young, Daria forsubwordsegmentationinneuralmachinetransla-
Rodionova, AnastasiaYemelina, TarasAndrushko, tion. InProceedingsofthe58thAnnualMeetingof
Igor Marchenko, Polina Mashkovtseva, Alexandra theAssociationforComputationalLinguistics,pages
Serova, Emily Prud’hommeaux, Maria Nepomni- 3042–3051,Online.AssociationforComputational
ashchaya, Fausto Giunchiglia, Eleanor Chodroff, Linguistics.
Mans Hulden, Miikka Silfverberg, Arya D. Mc-
Carthy,DavidYarowsky,RyanCotterell,ReutTsar- Valentin Hofmann, Janet Pierrehumbert, and Hinrich
faty,andEkaterinaVylomova.2022. UniMorph4.0: Schütze.2020. DagoBERT:Generatingderivational
UniversalMorphology. InProceedingsoftheThir- morphology with a pretrained language model. In
teenthLanguageResourcesandEvaluationConfer- Proceedings of the 2020 Conference on Empirical
ence, pages840–855, Marseille, France.European MethodsinNaturalLanguageProcessing(EMNLP),
LanguageResourcesAssociation. pages3848–3861,Online.AssociationforComputa-
tionalLinguistics.
Thomas Bauwens. 2023. BPE-knockout: Systematic
reviewofBPEtokenisersandtheirflawswithappli- Valentin Hofmann, Janet Pierrehumbert, and Hinrich
cation in Dutch morphology. Master’s thesis, KU Schütze.2021. Superbizarreisnotsuperb: Deriva-
Leuven. tionalmorphologyimprovesBERT’sinterpretation
ofcomplexwords. InProceedingsofthe59thAnnual
LisaBeinbornandYuvalPinter.2023. Analyzingcogni- Meeting of the Association for Computational Lin-
tiveplausibilityofsubwordtokenization. InProceed- guisticsandthe11thInternationalJointConference
ingsofthe2023ConferenceonEmpiricalMethods onNaturalLanguageProcessing(Volume1: Long
inNaturalLanguageProcessing,pages4478–4486, Papers),pages3594–3608,Online.Associationfor
Singapore.AssociationforComputationalLinguis- ComputationalLinguistics.
tics.
ValentinHofmann,HinrichSchuetze,andJanetPierre-
KajBostromandGregDurrett.2020. Bytepairencod- humbert.2022. Anembarrassinglysimplemethod
ingissuboptimalforlanguagemodelpretraining. In tomitigateundesirablepropertiesofpretrainedlan-
FindingsoftheAssociationforComputationalLin- guagemodeltokenizers. InProceedingsofthe60th
guistics: EMNLP 2020, pages 4617–4624, Online. AnnualMeetingoftheAssociationforComputational
AssociationforComputationalLinguistics. Linguistics(Volume2:ShortPapers),pages385–393,
Dublin,Ireland.AssociationforComputationalLin-
MarcoCognetta,VilémZouhar,SangwhanMoon,and
guistics.
NaoakiOkazaki.2024. TwocounterexamplestoTok-
enizationandtheNoiselessChannel.
Cassandra L Jacobs and Yuval Pinter. 2022. Lost in
spacemarking. arXivpreprintarXiv:2208.01561.
Mathias Creutz and Bo Krister Johan Linden. 2004.
Morphemesegmentationgoldstandardsforfinnish
Jean Kaddour. 2023. The minipile challenge for
andenglish.
data-efficient language models. arXiv preprint
ChristinaL. Gagné, Thomas L.Spalding, andDaniel arXiv:2304.08442.
Schmidtke. 2019. Ladec: The large database of
StavKleinandReutTsarfaty.2020. Gettingthe##life
english compounds. Behavior Research Methods,
outofliving:Howadequateareword-piecesformod-
51:2152–2179.
ellingcomplexmorphology? InProceedingsofthe
Edward Gow-Smith, Harish Tayyar Madabushi, Car- 17th SIGMORPHON Workshop on Computational
olinaScarton,andAlineVillavicencio.2022. Improv- ResearchinPhonetics,Phonology,andMorphology,
ingtokenisationbyalternativetreatmentofspaces. pages 204–209, Online. Association for Computa-
In Proceedings of the 2022 Conference on Empiri- tionalLinguistics.
calMethodsinNaturalLanguageProcessing,pages
Taku Kudo. 2018. Subword regularization: Improv-
11430–11443,AbuDhabi,UnitedArabEmirates.As-
ingneuralnetworktranslationmodelswithmultiple
sociationforComputationalLinguistics.
subwordcandidates. InProceedingsofthe56thAn-
ThammeGowdaandJonathanMay.2020. Findingthe nualMeetingoftheAssociationforComputational
optimalvocabularysizeforneuralmachinetransla- Linguistics(Volume1: LongPapers),pages66–75,
tion. In Findings of the Association for Computa- Melbourne,Australia.AssociationforComputational
tionalLinguistics: EMNLP2020,pages3955–3964, Linguistics.
Online.AssociationforComputationalLinguistics.
DavisLiang,HilaGonen,YuningMao,RuiHou,Na-
XimenaGutierrez-Vasques,ChristianBentz,OlgaSozi- man Goyal, Marjan Ghazvininejad, Luke Zettle-
nova,andTanjaSamardzic.2021. Fromcharacters moyer, andMadianKhabsa.2023. XLM-V:Over-
towords: theturningpointofBPEmerges. InPro- coming the vocabulary bottleneck in multilingual
ceedings of the 16th Conference of the European maskedlanguagemodels. InProceedingsofthe2023
Chapter of the Association for Computational Lin- Conference on Empirical Methods in Natural Lan-
guistics: Main Volume, pages 3454–3468, Online. guageProcessing, pages13142–13152, Singapore.
AssociationforComputationalLinguistics. AssociationforComputationalLinguistics.Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky, TakuKudo,HidetoKazawa,KeithStevens,George
ColinRaffel,MananDey,MatthiasGallé,ArunRaja, Kurian, Nishant Patil, Wei Wang, Cliff Young, Ja-
ChengleiSi,WilsonYLee,BenoîtSagot,etal.2021. son R. Smith, Jason Riesa, Alex Rudnick, Oriol
Between words and characters: a brief history of Vinyals,GregoryS.Corrado,MacduffHughes,and
open-vocabularymodelingandtokenizationinnlp. JeffreyDean.2016. Google’sneuralmachinetrans-
arXivpreprintarXiv:2112.10508. lationsystem: Bridgingthegapbetweenhumanand
machinetranslation. ArXiv,abs/1609.08144.
Benjamin Minixhofer, Jonas Pfeiffer, and Ivan Vulic´.
2023. CompoundPiece: Evaluatingandimproving ShakedYehezkelandYuvalPinter.2023. Incorporating
decompounding performance of language models. contextintosubwordvocabularies. InProceedings
In Proceedings of the 2023 Conference on Empiri- ofthe17thConferenceoftheEuropeanChapterof
calMethodsinNaturalLanguageProcessing,pages theAssociationforComputationalLinguistics,pages
343–359,Singapore.AssociationforComputational 623–635,Dubrovnik,Croatia.AssociationforCom-
Linguistics. putationalLinguistics.
YuvalPinter,CassandraL.Jacobs,andJacobEisenstein. Vilém Zouhar, Clara Meister, Juan Gastaldi, Li Du,
2020. Willitunblend? InFindingsoftheAssocia- Mrinmaya Sachan, and Ryan Cotterell. 2023. To-
tionforComputationalLinguistics: EMNLP2020, kenizationandthenoiselesschannel. InProceedings
pages1525–1535,Online.AssociationforComputa- of the 61st Annual Meeting of the Association for
tionalLinguistics. ComputationalLinguistics(Volume1: LongPapers),
pages5184–5207,Toronto,Canada.Associationfor
YuvalPinter,MarcMarone,andJacobEisenstein.2019.
ComputationalLinguistics.
Charactereyes: Seeinglanguagethroughcharacter-
leveltaggers. InProceedingsofthe2019ACLWork-
shopBlackboxNLP:AnalyzingandInterpretingNeu-
ralNetworksforNLP,pages95–102,Florence,Italy.
AssociationforComputationalLinguistics.
IvanProvilkov,DmitriiEmelianenko,andElenaVoita.
2020. BPE-dropout: Simpleandeffectivesubword
regularization. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics,pages1882–1892,Online.Associationfor
ComputationalLinguistics.
Jonne Saleva and Constantine Lignos. 2023. What
changeswhenyourandomlychooseBPEmergeop-
erations? not much. In The Fourth Workshop on
InsightsfromNegativeResultsinNLP,pages59–66,
Dubrovnik,Croatia.AssociationforComputational
Linguistics.
ClaudiaH.Sánchez-Gutiérrez,HugoMailhot,S.Hélène
Deacon, and Maximiliano A. Wilson. 2018. Mor-
pholex: Aderivationalmorphologicaldatabasefor
70,000englishwords. BehaviorResearchMethods,
50:1568–1580.
MikeSchusterandKaisukeNakajima.2012. Japanese
andkoreanvoicesearch. In2012IEEEinternational
conferenceonacoustics,speechandsignalprocess-
ing(ICASSP),pages5149–5152.IEEE.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neuralmachinetranslationofrarewordswith
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),pages1715–1725,
Berlin,Germany.AssociationforComputationalLin-
guistics.
Yonghui Wu, Mike Schuster, Z. Chen, Quoc V. Le,
MohammadNorouzi,WolfgangMacherey,Maxim
Krikun,YuanCao,QinGao,KlausMacherey,Jeff
Klingner,ApurvaShah,MelvinJohnson,Xiaobing
Liu,LukaszKaiser,StephanGouws,YoshikiyoKato,A ResultsonDifferentVocabularySizes
Table4presentsbenchmarkresultson32K-sized
and49K-sizedvocabularies.
B DetailedResults
Table 5 breaks down the results (for 40K) on in-
dividual morphological datasets composing our
benchmark. Table 6 Provides the same for indi-
vidualcognitivemeasures.Vocab Inference Morphological Cognitive Rényi Tokens Decoding
method alignment plausibility efficiency perword diff
longestprefix .8727 .3122 .4600 1.4511 .0581
longestsuffix .6496 .3018 .4602 1.4530 .0469
longesttoken .8883 .3152 .4592 1.4498 .0558
BPE-32K
leasttokens .7607 .3174 .4595 1.4469 .0426
det.merges .6409 .3201 .4603 1.4551 —
dropoutmerge .6149 .2795 .4656 1.6041 .1316
longestprefix .7819 .3185 .4630 1.4689 —
longestsuffix .5084 .3089 .4626 1.4698 .0744
longesttoken .7764 .3212 .4622 1.4667 .0243
WordPiece-32K
leasttokens .7394 .3185 .4508 1.4565 .0769
longestprefix .9278 .2855 .3574 1.7803 .1171
longestsuffix .7610 .2679 .2961 1.7838 .0516
UnigramLM-32K longesttoken .8926 .2930 .3103 1.7534 .0395
leasttokens .9077 .2937 .3028 1.7418 .0303
likelihood .9206 .2931 .2985 1.7501 —
longestprefix .9613 .2610 .3454 1.9502 —
longestsuffix .7449 .2473 .2914 1.9736 .1653
SaGe-32K longesttoken .9348 .2685 .3113 1.9319 .0822
leasttokens .9212 .2691 .3035 1.9084 .1247
likelihood .9579 .2679 .3026 1.9246 .1098
longestprefix .8440 .3371 .4391 1.4104 .0444
longestsuffix .6438 .3279 .4390 1.4112 .0379
longesttoken .8637 .3404 .4384 1.4094 .0430
BPE-49K
leasttokens .7464 .3421 .4385 1.4072 .0351
det.merges .6208 .3461 .4390 1.4137 —
dropoutmerge .5967 .2996 .4446 1.5610 .1310
longestprefix .7600 .3398 .4413 1.4245 —
longestsuffix .5133 .3309 .4407 1.4247 .0589
WordPiece-49K
longesttoken .7598 .3421 .4406 1.4228 .0194
leasttokens .7261 .3401 .4319 1.4145 .0615
longestprefix .9157 .2818 .3467 1.7432 .1190
longestsuffix .7449 .2669 .2849 1.7486 .0516
UnigramLM-49K longesttoken .8750 .2915 .2994 1.7245 .0416
leasttokens .8908 .2926 .2924 1.7098 .0345
likelihood .9095 .2911 .2871 1.7201 —
longestprefix .9606 .2566 .3361 1.9414 —
longestsuffix .7355 .2466 .2783 1.9562 .1735
SaGe-49K longesttoken .9200 .2662 .2975 1.9192 .0912
leasttokens .9053 .2662 .2893 1.8947 .1353
likelihood .9455 .2651 .2887 1.9111 .1194
Table4: Aggregatedresultson32Kand49Kvocabularies.Vocab Inference Ladec Morpho- Morphy- Dago- Uni- UnBlend Compound-
Lex Net Bert Morph Piece
longestprefix .9210 .8091 .8511 .8013 .9956 .7404 .8904
longestsuffix .9497 .6222 .6524 .7116 .0316 .6095 .9502
longesttoken .9147 .8125 .8953 .8618 .9705 .7711 .8905
BPE
leasttokens .9775 .7401 .8303 .8539 .2573 .6489 .9731
det.merges .8160 .6781 .6132 .6195 .3233 .6097 .7568
dropoutmerge .7666 .6557 .5871 .5953 .3128 .6213 .7178
longestprefix .9333 .7625 .9114 .8659 .9963 .5569 .9153
longestsuffix .9447 .6005 .6289 .6844 .1059 .4838 .9535
WordPiece
longesttoken .9275 .7568 .9124 .8765 .9666 .5749 .9112
leasttokens .9706 .7132 .8253 .8032 .2670 .5897 .9704
longestprefix .9551 .8800 .9291 .9087 .9973 .8553 .9299
longestsuffix .9248 .6387 .8206 .8407 .2777 .8076 .9536
UnigramLM longesttoken .8855 .7534 .9313 .9378 .9135 .8571 .9130
leasttokens .9660 .8015 .9511 .9593 .7218 .9073 .9801
likelihood .9341 .7903 .9645 .9782 .8423 .9205 .9743
longestprefix .9734 .9422 .9673 .9600 .9973 .9213 .9626
longestsuffix .9519 .5996 .7819 .8091 .2403 .8216 .9549
SaGe longesttoken .9420 .8390 .9365 .9418 .9711 .8889 .9457
leasttokens .9856 .8394 .9533 .9632 .7269 .9318 .9877
likelihood .9709 .8813 .9809 .9879 .9014 .9492 .9890
Table5: Resultsonindividualmorphologicalresources.
Vocab Inference Words-RT Words-ACC nonwords-RT nonwords-ACC
longestprefix −.3136 .4035 .4111 −.1784
longestsuffix −.3102 .3890 .3987 −.1699
longesttoken −.3164 .4086 .4130 −.1828
BPE
leasttokens −.3146 .4083 .4226 −.1828
det.merges −.3285 .4138 .4163 −.1835
dropoutmerge −.2562 .3505 .3908 −.1726
longestprefix −.3198 .4029 .4119 −.1882
longestsuffix −.3132 .3863 .4028 −.1770
WordPiece
longesttoken −.3226 .4067 .4134 −.1902
leasttokens −.3146 .4036 .4201 −.1842
longestprefix −.2292 .3391 .3920 −.1827
longestsuffix −.2308 .3235 .3645 −.1572
UnigramLM longesttoken −.2493 .3590 .3904 −.1804
leasttokens −.2394 .3582 .3978 −.1860
likelihood −.2424 .3577 .3926 −.1822
longestprefix −.1924 .2896 .3752 −.1754
longestsuffix −.1895 .2801 .3602 −.1585
SaGe longesttoken −.2079 .3047 .3790 −.1767
leasttokens −.1978 .3034 .3864 −.1821
likelihood −.2035 .3043 .3797 −.1780
Table6: Abreakdownofcognitivecorrelationresultsacrossvocabulariesandinferencemethods.