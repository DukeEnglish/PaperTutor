Chaining thoughts and LLMs to learn DNA
structural biophysics
TylerD.Ross AshwinGopinath
DepartmentofPhysics DepartmentofMechanicalEngineering
MIT MIT
tyler.qed.ross@gmail.com agopi@mit.edu
Abstract
The future development of an AI scientist, a tool that is capable of integrating a
variety of experimental data and generating testable hypotheses, holds immense
potential.Sofar,bespokemachinelearningmodelshavebeencreatedtospecialize
insingularscientifictasks,butotherwiselacktheflexibilityofageneralpurpose
model. Here, we show that a general purpose large language model, chatGPT
3.5-turbo, can be fine-tuned to learn the structural biophysics of DNA. We find
that both fine-tuning models to return chain-of-thought responses and chaining
together models fine-tuned for subtasks have an enhanced ability to analyze and
designDNAsequencesandtheirstructures.
1 Introduction
Withtheevergrowingaccumulationofexperimentaldataacrossvastfieldsofscience,ithasbecome
clear that a machine learning model that is capable of integrating all this data and generating new
hypotheseswouldrevolutionizetheadvancementofscientificknowledge. Specializedmodelshave
been created to search for new physics [1] or predict the structures of proteins [2]. However, the
recent success of general purpose large language models (LLMs) have shown that a single model
iscapableofperformingawiderangeoftasks,suchasprogramming,translation,andsummariza-
tion. Thetextualrepresentationthatistypicallyusedtorepresentaminoandnucleicacidpolymers
appears well-suited for LLM frameworks. In fact, recent studies have shown that the transformer
architecture of LLMs can also be utilized in constructing specialized models to predict the struc-
turesofbiomolecules[3,4,5]. However,asasmallsteptowardthegoalofaninterdisciplinaryAI
scientist, we are interested in determining if a general purpose LLM can be fine-tuned to model a
physicalphenomenon. Specifically,weaimtotestifchatGPT3.5-turbocanlearnthebiophysicsof
DNAstructureformation.
ThestructuralbiophysicsofnucleicacidsformsthebasisofDNAnanotechnology,whichharnesses
thepredictablenatureofDNAbasepairingandmolecularself-assemblytocreatecomplexnanos-
tructures as well as perform computations [6, 7, 8, 9]. A key component to DNA nanotechnology
is the formation of secondary structures, which are the specific shapes or arrangements that DNA
moleculescanassumebeyondthefamiliardoublehelixoftheprimarystructure. Secondarystruc-
tures are largely governed by two phenomena, the first being Watson-Crick base pairing between
DNA’s four nucleotides: adenine (A), thymine (T), cytosine (C), and guanine (G). Adenine pairs
withthymine(A-T)andcytosinepairswithguanine(C-G)throughhighlyspecifichydrogenbond-
ing.Notably,C-GbondsaremoreenergeticallyfavorablethanA-Tbonds.Basepairingisdependent
onthedirectionalityoftheDNAstrand,denotedas5’(five-prime)to3’(three-prime)ends. Reverse
complementsaresequencesthatwouldbindtoeachotherwhenalignedinoppositedirections. For
instance,thesequence5’-AGTC-3’wouldhaveareversecomplementof5’-GACT-3’. Beyondbase
pairing,thereisalsoanattractionbetweenpairedbasesintheformofstackingbonds. Aswithbase
pairing,thestrengthofstackingbondsvariesasafunctionofthebasepairsinvolved.Thelikelihood
4202
raM
2
]MQ.oib-q[
1v23310.3042:viXraof a given structure forming at thermodynamic equilibrium is determined by its free energy. The
freeenergyofaDNAstructureistypicallyapproximatedbyanearestneighbormodelthatusesthe
energiesofpairwisestackingbonds,whichhavebeendeterminedempirically,plustheenergyfrom
basepairing.
TheseenergeticapproximationsarethebasisoftheNUPACKsoftwaresuite[10,11],whichisthe
standardintheDNAnanotechcommunityfortheanalysisanddesignofDNAandRNAstructures.
NUPACKpredictsthemoststablestructurethatwillformforagivensetofDNAstrandsbyfind-
ing the structure that has the minimum free energy (MFE) of all possible structures (up to a user-
designated complex size limit). The DNA secondary structures are represented using parens-dot-
plusnotation,whichusesparenthesestoindicatebasepairedregions,dotsforunpairednucleotides,
andplusestoseparatemultiplestrands. Forexample, “((((..+....))))” representstwostrandswhere
thefirstfourbasesofstrand1areboundtothelastfourbasesofstrand2. Forthiswork,wewillbe
using NUPACK to provide the data for training and validation. It is worth emphasizing, however,
that NUPACK is not a complete biophysical model. For example, psudoknoted structures as well
asHoogsteenbasepairing,whichallowsfortheformationoftriplexsandquradraplexs,arebothig-
nored. Inthiswork,wewillbeusingasimplifieddataset,whereDNApairsareofequallengthand
arenotself-complementary(i.e. thestrandsdonotbindtothemselves). Thisallowsustoseparate
outthecapabilityoftheLLMstolearnthefundamentalsofDNAstructuralbiophysicsfromthetask
ofsearchingacrossthespaceofallpossibleconfigurations.
2 Methods
a chain of thought: nearest neighbor window
assistant output:
user input:
“CGGGGATAGGGCT CGGGGTAAGGGCA” CoT “[_CG,_CG]:( [CGG,CGG]:(( [GGG,GGG]:((( [GGG,GGG]:((((
secondary [GGA,GGT]:((((( [GAT,GTA]:(((((. [ATA,TAA]:(((((.. [TAG,AAG]:(((((..(
structure [AGG,AGG]:(((((..(( [GGG,GGG]:(((((..((( [GGC,GGC]:(((((..((((
sequence 1 reverse complement of expert [GCT,GCA]:(((((..((((( [CT_,CA_]:(((((..(((((.
sequence 2
ans:(((((..(((((.+.)))))..)))))"
b expert pipeline for sequence analysis
sequence 1
“GCGGTGCTGTGA”
CoT
secondary secondary structure
structure “(((.(((.(((.+.))).))).)))”
sequence 2 reverse rev. comp. of 2 expert
complement
“ACACTGCAGCGC” “GCGCTGCAGTGT”
expert
c sequence design with expert-based error checking
sequence 1
CoT CoT
“GCGGCGCCTGGA”
secondary structure sequence secondary secondary structure
“(((.((((.((.+.)).)))).)))” design structure “(((.((((.((.+.)).)))).)))”
rev. comp. of 2
expert expert
“GCGCCGCCAGGT”
answer
sequence 1
error checking false
repeat sequence design “GCGGCGCCTGGA”
secondary structure secondary structure reverse
== true rev. comp. of 2 sequence 2
“(((.((((.((.+.)).)))).)))” “(((.((((.((.+.)).)))).)))” complement
“GCGCCGCCAGGT” expert “ACCTGGCGGCGC”
Figure 1: Schemes for chain of thought (CoT) and pipelines of models used to perform sequence
analysisanddesign. a,Chainofthoughtfine-tuningwherethemodelprintsouteachbaseandtheir
neighbors before determining if a stable base pair is formed. b, A sequence analysis pipeline that
usesamodelthatistunedtoprovidethereversecomplementofasequence,whichisthenfedinto
amodelthatistunedtodeterminethesecondarystructure. Boxesinwhiteindicatevaluesprovided
bytheuser,tealboxesarefine-tunedmodels,andorangeboxesrepresentfinalanswersfrommodel
outputs. c, Expert-based error checking scheme where the sequences designed by one model are
analyzedbyanothertoverifythatthedesiredsecondarystructureisformed.
2Ourworkisbuiltontwoprimaryconcepts:fine-tuningmodelstofollowatrainofthoughtapproach
beforearrivingatananswer, andusingapipelineofmodels(experts)tosolvedifferentaspectsof
theproblemandprovidefeedback.
Forsimplicityinimplementation,eachexpertisafine-tuningofgpt-3.5-turbo-1106usingOpenAI’s
API.Abasicerrorcheckingstepisappliedtotheoutputofeachexpert,wherethemodelisqueried
withthesameinput(upto20times)ifthemodel’sfinalanswerisnottheexpectedlengthorcontains
invalidcharacters. Modelsfine-tunedtofollowachain-of-thought(CoT)approachwillstep-by-step
print out the nearest neighbor window of the input while assembling the structure/sequence pair
before giving their finalanswers (Fig. 1a). In addition to breaking down the problem into simpler
steps as shown in [12], our fine-tuned CoT approach leverages a feature of our problem (i.e. that
binding stability is based on nearest neighbor interactions) to give the model a hint as to how to
processtheinput. Whenthenearestneighborwindowisapplied,theinputstring(s)arepaddedwith
thecharacter“_”toindicatetheendsofthesequences,asbasesattheedgesarelesslikelytoform
stable base pairs due to having only one potential stacking bond. For the CoT approach, we only
evaluateamodel’sperformancebasedonitsfinalanswer,whichcomesafter“ans:”.
In all of our experiments we explore the utilization of an expert pipeline where models that have
beenfine-tunedtoperformsubtasksfeedintoeachother. Herewearemotivatedbytheideathata
collection of models that are specialized on subtasks can perform better than a single model [13].
In the simplest case, we have the output of one model provide part of the input to another model
(Fig.1b).Forthemorecomplexproblemofsequencedesign,weintroduceanerrorcheckingscheme
wherethesequencesgeneratedbyonemodelareanalyzedtoseeiftheyformthedesiredsecondary
structure(Fig.1c). Ifthestructuresdomatchthenthesequencesareaccepted.
ThegroundtruthdatausedtotrainandvalidatethemodelsiscreatedusingNUPACK.Thesequences
aredesignedbyNUPACKtoensurethattheMFEstructuredoesnothaveself-complementarityor
require alignment. For sequence design, we use NUPACK to determine the ground truth MFE
structureformedbytheLLMdesignedsequences. WesetNUPACK’smodelconditionsto20◦C,
1 M sodium, and with ensemble stacking. Sequences are of lengths between 10-25 bases with
sequencepairsbeingofmatchinglength. Pairedstrandshaveaminimumof1mismatchupto30%
ofbasesmismatched. Thetrainingsetsizeis10,000andvalidatedagainstasetof1,000sequences.
Learning curves are generated over training sets containing 200, 500, 1,500, 3,700, and 10,000
examples. Whenevaluatingthelearningcurvesforthepipelines,allexpertsarefine-tunedondata
setsofthesamesize. Aseparatetrainingandvalidationsetisusedforthesequencedesignmodel,
whereeachentryinthesetisauniquestructure.
3 Experimentsandresults
3.1 Predictingsecondarystructure
We begin exploring the capability of a general purpose LLM to learn the secondary structure
of DNA by testing four different approaches: naive, chain-of-thought (CoT), reverse comple-
ment followed by CoT, and a pipeline of a reverse complement expert feeding into a secondary
structure expert. The naive approach is to provide two DNA sequences and have the model
return the corresponding secondary structure in dot-parens-plus notation. For example, Input:
“ACCGCGCCCT TGGGCGGGGA” Output: “.((.(((((.+.))))).)).”. For CoT, the input remains
the same but the output applies a nearest neighbor window as it determines the structure, In-
put: “CCCGGCGCTG CTGCGGCGGG” Output: “[_CC,_GG]:( [CCC,GGG]:(( [CCG,GGC]:(((
[CGG,GCG]:(((( [GGC,CGG]:((((. [GCG,GGC]:((((.( [CGC,GCG]:((((.(( [GCT,CGT]:((((.(((
[CTG,GTC]:((((.(((. [TG_,TC_]:((((.(((.. ans:((((.(((..+..))).))))”. We next vary the CoT approach
by breaking the problem down further. The models so far had to recognize that the relationship
between the strands is from left to right on the first sequence and from right to left on the second
sequenceandthatfurtheritmustlearnthepairingruleofG-CandT-Aallwithinonestep.Wethere-
foreintroduceathirdconditionwherethemodelfirstproducesthereversecomplementofthesecond
sequenceandthenperformsaCoTbasecomparisonbetweenthefirstsequenceandthereversecom-
plement of the second sequence. For example, Input: “GCAGGCCCGAA AACGGGGCTGC”
Output: “GCAGCCCCGTT [_GC,_GC]:( [GCA,GCA]:(( [CAG,CAG]:((( [AGG,AGC]:((((
[GGC,GCC]:((((. [GCC,CCC]:((((.( [CCC,CCC]:((((.(( [CCG,CCG]:((((.((( [CGA,CGT]:((((.((((
[GAA,GTT]:((((.((((. [AA_,TT_]:((((.((((.. ans:((((.((((..+..)))).))))”. Finally, we break the prob-
3lemdownevenfurtherbycreatingapipelinewhereaseparatemodelisresponsiblefordetermining
thereversecomplementandsendsittoanothermodelthatdeterminesthesecondarystructure. For
example,firstweprovidethereversecomplementexpertwithsequence2: “GGCCGCCGAC”and
receiveitsreversecomplement“GTCGGCGGCC”,wethengivethesecondarystructureexpertthe
Input: “GTCGGCGGGCGTCGGCGGCC”Output: “[_GT,_GT]:([GTC,GTC]:(([TCG,TCG]:(((
[CGG,CGG]:(((( [GGC,GGC]:((((( [GCG,GCG]:(((((( [CGG,CGG]:((((((( [GGG,GGC]:((((((((
[GGC,GCC]:((((((((. [GC_,CC_]:((((((((.(ans:((((((((.(+).))))))))”. Forcomparison,wealsorunthis
pipeline where a reverse complement algorithm provides the ground truth reverse complement to
thesecondarystructureexpert.
pipeline
pipeline
rev. comp. groundtruth
naive CoT rev. comp. expert
&CoT rev. comp.
→CoT
→CoT
accuracy% 7.4 77.4 88.9 92.8 95.4
Table1: Secondarystructurepredictionaccuracy.
100
secondary structure
reverse complement
80
60
40
20
0
0 2000 4000 6000 8000 10000
training size
Figure 2: Learning curves for secondary structure prediction and the reverse complement expert.
Forthesecondarystructure,weareusingtheexpertpipelineapproach.
Results Thegeneraltrendwefindisthatthemorewebreakdowntheproblemthebettertheper-
formance(Table1). Inallcasesthemajorityoferrorsareduetoincorrectbasepairingratherthan
issueswithformattingorincorrectstructurelengths. Havingthemodelexplicitlywriteoutthenear-
estneighborwindowresultsinasubstantialimprovementoverthenaiveapproach. Thebenefitof
CoTherepresumablycomesfrombothexpandingthecontextbeforetheanswerisgivenand,more
specifically,bybreakingdowntherelevantcharacterrelationships.Itisperhapssurprisinghowlarge
animprovementweseewhenthemodelproducesthereversecomplementofthesecondsequence
beforeperformingCoT.Thecomparisonofonesequencefromleft-to-rightwithanothersequence
fromright-to-leftmaybeinherentlydifficultforthemodeltolearngiventheunidirectionalnature
ofthetransformerdecoderarchitecture. Producingthereversecomplementfirstinsteadallowsthe
model to compare both strings from left to right. For the pipeline approach, we additionally fine-
tuneamodeltoreturnthereversecomplementofasequence,whichhasanaccuracyof98.4%. We
notethat,unlikethedeterminationofthesecondarystructure,findingthereversecomplementofa
sequencefollowsasimpleclosed-formfunction. Perhapsthisiswhythereversecomplementexpert
saturates at small training sizes in comparison to the secondary structure model (Fig. 2). The dif-
ferenceintheperformancebetweentheexpertpipelineandthegroundtruthpipelinesuggeststhat
thereislargelynooverlapinwhereerrorsoccurforthestructureandreversecomplementexperts.
Therefore, there may be a diminishing return in terms of the number of experts a pipeline uses.
Ultimately,offloadingthereversecomplementoperationtoanothermodelenhancestheaccuracyof
thesecondarystructureprediction.
4
%
ycarucca3.2 Calculatingminimumfreeenergy
Tofurthertestthemodel’sabilitytolearnthestructuralphysicsofDNA,weaskittocalculatethe
minimumfreeenergy(MFE)forapairofsequences. AsmentionedintheIntroduction,thefreeen-
ergyisapproximatedasafunctionofboththebasepairingandthenearest-neighborstackingbond
energies, which are empirically determined. Here we challenge the model to find the MFE with-
out explicitly writing out any mathematical operations. We explore MFE calculation across four
approaches: naive, reverse complement followed by CoT, a pipeline where a model provides the
reversecomplementtoaCoTMFEmodel,andavariationonthenaiveapproachwherethemodel
is provided with both the ground truth reverse complement and ground truth secondary structure.
Thefirstthreecasesarenearlyidenticaltowhatisdescribedintheprevioussection,butonthelast
stepthemodelreturnstheMFEinsteadofthestructure. Alsoasbefore,wetestthepipelinemethod
where the reverse complement expert is substituted for a ground truth algorithm. An example of
the ground truth reverse complement and structure approach is as follows, Input: “CGTTTTTC-
CGACTTGCGCCG CGAATTTCCGACGTGCGCCG ((..((((((((.(((((((+))))))).))))))))..))” Output:
“-28.3”. Theideahereistotestifprovidingthemodelwiththesecondarystructureaspartofthe
inputissufficienttomatchthemodel’sperformanceovertheCoTapproach.
pipeline groundtruth
pipeline
rev. comp. groundtruth rev. comp.
naive rev. comp. expert
&CoT rev. comp. &groundtruth
→CoT
→CoT structure
error(cid:0)kcal(cid:1)
1.67±1.43 1.55±1.83 1.23±1.63 1.15±1.26 1.43±1.19
mol
Table2: Minimumfreeenergyerrorfromsequenceanalysis. Errorismeanabsoluteerrorandthe
±termrepresentsstandarddeviationoftheabsoluteerror.
8
6
4
2
0
200 500 1400 3700 10000
training size
Figure3: ImpactoftrainingsetsizeonMFEpredictionsforthereversecomplementexpertpipeline
approach.
Results Themodel appearsto havelearned someaspects ofdetermining theMFE(Table 2). As
apointofreference, thestandarddeviationofthevalidationsetis6.04kcal/mol. LLMsgenerally
strugglewithmathematicaltasks[14],soitisnotunexpectedthatwedonotseeaslargeanimprove-
mentbetweenthevariousapproachesaswedidintheprevioussection. Withthatsaid, increasing
theamountoftrainingdatashowedasignificantimprovementinaccuracy(Fig.3).Liketheprevious
section,itappearsthatthecombinationoftheCoTandmodelpipelineprovidesthebestresult. Itis
alsoworthhighlightingthattheCoTprocessresultsinalowererrorthanprovidingthemodelwith
thegroundtruthsecondarystructure. Thissuggeststhatthecontextprovidedbythenearestneigh-
borwindowisnotjustameanstofindingthesecondarystructurebutis,tosomeextent,guidingthe
modeltolearnthebasepairing/stackingenergies.
5
)lom/lack(
rorre
etulosba3.3 Sequencedesignwithexperterrorchecking
The design problem, creating DNA sequences that will form a desired secondary structure,
is considerably more difficult than the analysis problems we have attempted so far. Slight
errors in the sequence design can result in self-complementarity or alignment issues. Once
again, we apply a naive, CoT, and pipeline approach to solving the problem. For the naive
approach, we provide the secondary structure and have the model return a pair of sequences.
For example, Input: “(((((..((((((((((.(+).))))))))))..)))))” Output: “GCGTTCATGTCTTGCGGGC
GGCCGCAAGACACAAACGC”. When applying the CoT approach, we have the model apply
a nearest neighbor window to the secondary structure as it builds out the sequences. Based
on the result from Section 3.1, have the CoT build out the second sequence as its reverse
complementandthenonthefinalsteptransformitintoitsforwardsequence. Forexample, Input:
“..((.(((.(((+))).))).))..” Output: “[_..]:[A,T] [..(]:[AA,TT] [.((]:[AAC,TTC] [((.]:[AACA,TTCA]
[(.(]:[AACAG,TTCAC] [.((]:[AACAGC,TTCACC] [(((]:[AACAGCC,TTCACCC]
[((.]:[AACAGCCG,TTCACCCG] [(.(]:[AACAGCCGG,TTCACCCGC] [.((]:[AACAGC-
CGGC,TTCACCCGCC] [(((]:[AACAGCCGGCC,TTCACCCGCCC] [((_]:[AACAGCCG-
GCCG,TTCACCCGCCCG]ans:AACAGCCGGCCGCGGGCGGGTGAA”.Inthethirdcondition
wehaveapipelinewherethereversecomplementtransformationonthefinalstepisoffloadedtoa
secondmodelorgroundtruthalgorithm.
We now add in an error checking layer into our pipeline of experts (Fig. 1c). Once our design
model produces a sequence pair, we have the expert that we tuned to predict secondary structures
(Section3.1)toverifythatsequencesformthedesiredstructure. Ifthestructuresarefoundtobethe
same, then the second strand is transformed into its forward sequence and the final sequence pair
isreturned. Ifthestructuresarefoundtobedifferent,thenthedesignprocessisrepeatedagainfor
thesameinputstructure. Forcomparison,wealsotestthisapproachbyhavingagroundtrutherror
checkwhereNUPACKperformsthesequenceanalysisintheerrorcheckingstep.
pipeline pipeline
CoT
naive CoT CoT
&rev. comp.
→rev. comp. expert →groundtruthrev. comp.
accuracy% 3.9 77.6 85.9 88.9
pipeline pipeline
CoT CoT
→experterrorchecking →groundtrutherrorchecking
→rev. comp. expert →rev. comp. expert
accuracy% 93.1 99.8
Table3: Sequencedesignaccuracy.
100
80
60
40
20
0
0 2000 4000 6000 8000 10000
training size
Figure4: Sequencedesignlearningcurveforthepipelinewithexperterrorcheckingapproach.
6
%
ycaruccaResults In spite of the analysis and design tasks being quite different from each other, we see
similarbenefitsfromthesameapproaches. Withthatsaid,comparingtheresultsofthefirstrowof
approaches in Table 3 to their analogs in Table 1 confirms that the model has greater difficulty in
performingsequencedesign. Theadditionoftheerrorcheckinglayer,however,appearstocompen-
satefortheincreaseindifficulty. Infact,weseethatwithgroundtrutherrorchecking,thedesigner
canperformalmostperfectly. Basedonthesteepnessofthelearningcurve,designhasmorebenefit
from larger training sets than analysis does (Fig. 4). For generating this particular learning curve,
we reduced the number of retries per error to 3 as it would otherwise run for several days for the
validation of the low training sizes to run. The accuracy at the training size of 10,000, is similar
between 3 retires and 20 retries. Provided how successful our simple error handling scheme is, it
is also worth considering if error correction may be implemented. For example, a model that is
fine-tunedtoreflectonerrorsandproducecorrectionsmayhaveevenbetterperformance.
4 Conclusion
WehaveshownthatageneralpurposeLLMcanbefine-tunedtolearnthestructuralbiophysicsof
DNA.Thisismadepossiblethroughthecombinationofchain-of-thoughtresponsesandthechaining
ofexperts. Inparticular,breakingdownacomplicatedtaskintoaseriesofsubtasksthatmodelscan
masterandchainingthosemodelstogethermaybeausefulconceptforconstructingmorepowerful
modelsthatarecapableofworkingonscientificproblems.
One direction in which this work can be further taken is to test if smaller models can be chained
togetherforasimilarperformanceimprovement. Theimplicationbeingthatmodelsrequiringlower
computeresourcescouldcollectivelyperformmorecomplextasksthanasingularlargemodel. For
this particular type of problem, an LLM architecture involving both an encoder and decoder may
performbetteratdirectsequencecomparisonthanadecoder-onlyarchitecture.
As noted in the Introduction, our datasets are within a simpler subspace of the complete range of
possible DNA interactions. One may be able to make a more generalized model by using CoT to
searchthroughconfigurationstodealwithself-complementarityandsequencealignment. Wealso
highlightthattheconceptsfromthisworkwillalsoapplytoRNAstructures.Fromtheperspectiveof
DNAnanotech, thereisapotentialopportunityforLLMstoeventuallybeusefulwhereNUPACK
fails. For example, an LLM may be capable of learning to predict pseudoknots, Hoogsteen base
pairing, and many-strand interactions (which has utility for designing optimal staple strands for
DNAorigami).Further,anLLMtrainedonexperimentaldatamayproducemoreaccuratestructural
predictionsthantheapproximationsusedbycurrentmodels.
5 Codeavailability
Allcodeanddatausedisavailableatthefollowinglink: https://github.com/TDRoss/DNA-LLM.
References
[1] Georgia Karagiorgi, Gregor Kasieczka, Scott Kravitz, Benjamin Nachman, and David Shih.
Machine learning in the search for new fundamental physics. Nature Reviews Physics, 4(6):
399–412,2022. doi: 10.1038/s42254-022-00455-1.
[2] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, Alex
Bridgland,ClemensMeyer,SimonA.A.Kohl,AndrewJ.Ballard,AndrewCowie,Bernardino
Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,
David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska,
Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior,
Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein struc-
ture prediction with alphafold. Nature, 596(7873):583+, 2021. ISSN 0028-0836. doi:
10.1038/s41586-021-03819-2.
[3] ZemingLin,HalilAkin,RoshanRao,BrianHie,ZhongkaiZhu,WentingLu,NikitaSmetanin,
Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi,
7TomSercu,SalvatoreCandido,andAlexanderRives.Evolutionary-scalepredictionofatomic-
level protein structure with a language model. Science, 379(6637):1123–1130, 2023. doi:
10.1126/science.ade2574.
[4] Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant Kharkare,
Koushik Roye, Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang, George M. Church, Pe-
terK.Sorger,andMohammedAlQuraishi. Single-sequenceproteinstructurepredictionusing
alanguagemodelanddeeplearning. NatureBiotechnology,40(11):1617+,2022. ISSN1087-
0156. doi: 10.1038/s41587-022-01432-w.
[5] Hong-Liang Li, Yi-He Pang, and Bin Liu. BioSeq-BLM: a platform for analyzing DNA,
RNAandproteinsequencesbasedonbiologicallanguagemodels. NucleicAcidsResearch,49
(22):e129–e129,092021. ISSN0305-1048. doi: 10.1093/nar/gkab829.
[6] Nadrian C. Seeman. Nucleic acid junctions and lattices. Journal of Theoretical Biology, 99
(2):237–247,1982. ISSN0022-5193. doi: https://doi.org/10.1016/0022-5193(82)90002-9.
[7] PWKRothemund. Foldingdnatocreatenanoscaleshapesandpatterns. Nature, 440(7082):
297–302,2006. ISSN0028-0836. doi: 10.1038/nature04586.
[8] YonggangKe,LuvenaL.Ong,WilliamM.Shih,andPengYin. Three-dimensionalstructures
self-assembledfromdnabricks. Science,338(6111):1177–1183,2012. doi: 10.1126/science.
1227268.
[9] Georg Seelig, David Soloveichik, David Yu Zhang, and Erik Winfree. Enzyme-free nucleic
acidlogiccircuits. Science,314(5805):1585–1588,2006. doi: 10.1126/science.1132493.
[10] Joseph N. Zadeh, Conrad D. Steenberg, Justin S. Bois, Brian R. Wolfe, Marshall B. Pierce,
AsifR.Khan,RobertM.Dirks,andNilesA.Pierce.NUPACK:Analysisanddesignofnucleic
acidsystems. JournalofComputationalChemistry,32(1):170–173,2011. doi:https://doi.org/
10.1002/jcc.21596.
[11] MarkE.Fornace,JiningHuang,CodyT.Newman,NicholasJ.Porubsky,MarshallB.Pierce,
andNilesA.Pierce. NUPACK:AnalysisandDesignofNucleicAcidStructures,Devices,and
Systems. ChemRxiv,2022. doi: 10.26434/chemrxiv-2022-xv98l.
[12] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,
Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language
models,2023.
[13] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive
mixtures of local experts. Neural Computation, 3(1):79–87, 1991. ISSN 0899-7667. doi:
10.1162/neco.1991.3.1.79.
[14] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas
Lukasiewicz, PhilippPetersen, andJuliusBerner. Mathematicalcapabilitiesofchatgpt. Ad-
vancesinNeuralInformationProcessingSystems,36,2024.
8