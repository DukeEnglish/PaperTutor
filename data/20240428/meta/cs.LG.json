[
    {
        "title": "Made to Order: Discovering monotonic temporal changes via self-supervised video ordering",
        "authors": "Charig YangWeidi XieAndrew Zisserman",
        "links": "http://arxiv.org/abs/2404.16828v1",
        "entry_id": "http://arxiv.org/abs/2404.16828v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16828v1",
        "summary": "Our objective is to discover and localize monotonic temporal changes in a\nsequence of images. To achieve this, we exploit a simple proxy task of ordering\na shuffled image sequence, with `time' serving as a supervisory signal since\nonly changes that are monotonic with time can give rise to the correct\nordering. We also introduce a flexible transformer-based model for\ngeneral-purpose ordering of image sequences of arbitrary length with built-in\nattribution maps. After training, the model successfully discovers and\nlocalizes monotonic changes while ignoring cyclic and stochastic ones. We\ndemonstrate applications of the model in multiple video settings covering\ndifferent scene and object types, discovering both object-level and\nenvironmental changes in unseen sequences. We also demonstrate that the\nattention-based attribution maps function as effective prompts for segmenting\nthe changing regions, and that the learned representations can be used for\ndownstream applications. Finally, we show that the model achieves the state of\nthe art on standard benchmarks for ordering a set of images.",
        "updated": "2024-04-25 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16828v1"
    },
    {
        "title": "Learning Visuotactile Skills with Two Multifingered Hands",
        "authors": "Toru LinYu ZhangQiyang LiHaozhi QiBrent YiSergey LevineJitendra Malik",
        "links": "http://arxiv.org/abs/2404.16823v1",
        "entry_id": "http://arxiv.org/abs/2404.16823v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16823v1",
        "summary": "Aiming to replicate human-like dexterity, perceptual experiences, and motion\npatterns, we explore learning from human demonstrations using a bimanual system\nwith multifingered hands and visuotactile data. Two significant challenges\nexist: the lack of an affordable and accessible teleoperation system suitable\nfor a dual-arm setup with multifingered hands, and the scarcity of\nmultifingered hand hardware equipped with touch sensing. To tackle the first\nchallenge, we develop HATO, a low-cost hands-arms teleoperation system that\nleverages off-the-shelf electronics, complemented with a software suite that\nenables efficient data collection; the comprehensive software suite also\nsupports multimodal data processing, scalable policy learning, and smooth\npolicy deployment. To tackle the latter challenge, we introduce a novel\nhardware adaptation by repurposing two prosthetic hands equipped with touch\nsensors for research. Using visuotactile data collected from our system, we\nlearn skills to complete long-horizon, high-precision tasks which are difficult\nto achieve without multifingered dexterity and touch feedback. Furthermore, we\nempirically investigate the effects of dataset size, sensing modality, and\nvisual input preprocessing on policy learning. Our results mark a promising\nstep forward in bimanual multifingered manipulation from visuotactile data.\nVideos, code, and datasets can be found at https://toruowo.github.io/hato/ .",
        "updated": "2024-04-25 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16823v1"
    },
    {
        "title": "Meta-Transfer Derm-Diagnosis: Exploring Few-Shot Learning and Transfer Learning for Skin Disease Classification in Long-Tail Distribution",
        "authors": "Zeynep ÖzdemirHacer Yalim KelesÖmer Özgür Tanrıöver",
        "links": "http://arxiv.org/abs/2404.16814v1",
        "entry_id": "http://arxiv.org/abs/2404.16814v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16814v1",
        "summary": "Addressing the challenges of rare diseases is difficult, especially with the\nlimited number of reference images and a small patient population. This is more\nevident in rare skin diseases, where we encounter long-tailed data\ndistributions that make it difficult to develop unbiased and broadly effective\nmodels. The diverse ways in which image datasets are gathered and their\ndistinct purposes also add to these challenges. Our study conducts a detailed\nexamination of the benefits and drawbacks of episodic and conventional training\nmethodologies, adopting a few-shot learning approach alongside transfer\nlearning. We evaluated our models using the ISIC2018, Derm7pt, and SD-198\ndatasets. With minimal labeled examples, our models showed substantial\ninformation gains and better performance compared to previously trained models.\nOur research emphasizes the improved ability to represent features in\nDenseNet121 and MobileNetV2 models, achieved by using pre-trained models on\nImageNet to increase similarities within classes. Moreover, our experiments,\nranging from 2-way to 5-way classifications with up to 10 examples, showed a\ngrowing success rate for traditional transfer learning methods as the number of\nexamples increased. The addition of data augmentation techniques significantly\nimproved our transfer learning based model performance, leading to higher\nperformances than existing methods, especially in the SD-198 and ISIC2018\ndatasets. All source code related to this work will be made publicly available\nsoon at the provided URL.",
        "updated": "2024-04-25 17:56:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16814v1"
    },
    {
        "title": "AAPL: Adding Attributes to Prompt Learning for Vision-Language Models",
        "authors": "Gahyeon KimSohee KimSeokju Lee",
        "links": "http://arxiv.org/abs/2404.16804v1",
        "entry_id": "http://arxiv.org/abs/2404.16804v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16804v1",
        "summary": "Recent advances in large pre-trained vision-language models have demonstrated\nremarkable performance on zero-shot downstream tasks. Building upon this,\nrecent studies, such as CoOp and CoCoOp, have proposed the use of prompt\nlearning, where context within a prompt is replaced with learnable vectors,\nleading to significant improvements over manually crafted prompts. However, the\nperformance improvement for unseen classes is still marginal, and to tackle\nthis problem, data augmentation has been frequently used in traditional\nzero-shot learning techniques. Through our experiments, we have identified\nimportant issues in CoOp and CoCoOp: the context learned through traditional\nimage augmentation is biased toward seen classes, negatively impacting\ngeneralization to unseen classes. To address this problem, we propose\nadversarial token embedding to disentangle low-level visual augmentation\nfeatures from high-level class information when inducing bias in learnable\nprompts. Through our novel mechanism called \"Adding Attributes to Prompt\nLearning\", AAPL, we guide the learnable context to effectively extract text\nfeatures by focusing on high-level features for unseen classes. We have\nconducted experiments across 11 datasets, and overall, AAPL shows favorable\nperformances compared to the existing methods in few-shot learning, zero-shot\nlearning, cross-dataset, and domain generalization tasks.",
        "updated": "2024-04-25 17:51:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16804v1"
    },
    {
        "title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
        "authors": "Herilalaina RakotoarisonSteven AdriaensenNeeratyoy MallikSamir GaribovEdward BergmanFrank Hutter",
        "links": "http://arxiv.org/abs/2404.16795v1",
        "entry_id": "http://arxiv.org/abs/2404.16795v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16795v1",
        "summary": "With the increasing computational costs associated with deep learning,\nautomated hyperparameter optimization methods, strongly relying on black-box\nBayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising\ngrey-box alternative, strategically allocating scarce resources incrementally\nto different configurations. However, the frequent surrogate model updates\ninherent to this approach pose challenges for existing methods, requiring\nretraining or fine-tuning their neural network surrogates online, introducing\noverhead, instability, and hyper-hyperparameters. In this work, we propose\nFT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data\nfitted network (PFN) that leverages the transformers' in-context learning\nability to efficiently and reliably do Bayesian learning curve extrapolation in\na single forward pass. Our empirical analysis across three benchmark suites\nshows that the predictions made by FT-PFN are more accurate and 10-100 times\nfaster than those of the deep Gaussian process and deep ensemble surrogates\nused in previous work. Furthermore, we show that, when combined with our novel\nacquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO\nmethod (ifBO), yields new state-of-the-art performance in the same three\nfamilies of deep learning HPO benchmarks considered in prior work.",
        "updated": "2024-04-25 17:40:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16795v1"
    }
]