[
    {
        "title": "Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials",
        "authors": "Ye FangZeyi SunTong WuJiaqi WangZiwei LiuGordon WetzsteinDahua Lin",
        "links": "http://arxiv.org/abs/2404.16829v1",
        "entry_id": "http://arxiv.org/abs/2404.16829v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16829v1",
        "summary": "Physically realistic materials are pivotal in augmenting the realism of 3D\nassets across various applications and lighting conditions. However, existing\n3D assets and generative models often lack authentic material properties.\nManual assignment of materials using graphic software is a tedious and\ntime-consuming task. In this paper, we exploit advancements in Multimodal Large\nLanguage Models (MLLMs), particularly GPT-4V, to present a novel approach,\nMake-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and\ndescribe materials, allowing the construction of a detailed material library.\n2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V\nprecisely identifies and aligns materials with the corresponding components of\n3D objects. 3) The correctly matched materials are then meticulously applied as\nreference for the new SVBRDF material generation according to the original\ndiffuse map, significantly enhancing their visual authenticity. Make-it-Real\noffers a streamlined integration into the 3D content creation workflow,\nshowcasing its utility as an essential tool for developers of 3D assets.",
        "updated": "2024-04-25 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16829v1"
    },
    {
        "title": "Learning Visuotactile Skills with Two Multifingered Hands",
        "authors": "Toru LinYu ZhangQiyang LiHaozhi QiBrent YiSergey LevineJitendra Malik",
        "links": "http://arxiv.org/abs/2404.16823v1",
        "entry_id": "http://arxiv.org/abs/2404.16823v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16823v1",
        "summary": "Aiming to replicate human-like dexterity, perceptual experiences, and motion\npatterns, we explore learning from human demonstrations using a bimanual system\nwith multifingered hands and visuotactile data. Two significant challenges\nexist: the lack of an affordable and accessible teleoperation system suitable\nfor a dual-arm setup with multifingered hands, and the scarcity of\nmultifingered hand hardware equipped with touch sensing. To tackle the first\nchallenge, we develop HATO, a low-cost hands-arms teleoperation system that\nleverages off-the-shelf electronics, complemented with a software suite that\nenables efficient data collection; the comprehensive software suite also\nsupports multimodal data processing, scalable policy learning, and smooth\npolicy deployment. To tackle the latter challenge, we introduce a novel\nhardware adaptation by repurposing two prosthetic hands equipped with touch\nsensors for research. Using visuotactile data collected from our system, we\nlearn skills to complete long-horizon, high-precision tasks which are difficult\nto achieve without multifingered dexterity and touch feedback. Furthermore, we\nempirically investigate the effects of dataset size, sensing modality, and\nvisual input preprocessing on policy learning. Our results mark a promising\nstep forward in bimanual multifingered manipulation from visuotactile data.\nVideos, code, and datasets can be found at https://toruowo.github.io/hato/ .",
        "updated": "2024-04-25 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16823v1"
    },
    {
        "title": "Make Your LLM Fully Utilize the Context",
        "authors": "Shengnan AnZexiong MaZeqi LinNanning ZhengJian-Guang Lou",
        "links": "http://arxiv.org/abs/2404.16811v1",
        "entry_id": "http://arxiv.org/abs/2404.16811v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16811v1",
        "summary": "While many contemporary large language models (LLMs) can process lengthy\ninput, they still struggle to fully utilize information within the long\ncontext, known as the lost-in-the-middle challenge. We hypothesize that it\nstems from insufficient explicit supervision during the long-context training,\nwhich fails to emphasize that any position in a long context can hold crucial\ninformation. Based on this intuition, our study presents information-intensive\n(IN2) training, a purely data-driven solution to overcome lost-in-the-middle.\nSpecifically, IN2 training leverages a synthesized long-context question-answer\ndataset, where the answer requires (1) fine-grained information awareness on a\nshort segment (~128 tokens) within a synthesized long context (4K-32K tokens),\nand (2) the integration and reasoning of information from two or more short\nsegments. Through applying this information-intensive training on Mistral-7B,\nwe present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of\nFILM-7B for utilizing long contexts, we design three probing tasks that\nencompass various context styles (document, code, and structured-data context)\nand information retrieval patterns (forward, backward, and bi-directional\nretrieval). The probing results demonstrate that FILM-7B can robustly retrieve\ninformation from different positions in its 32K context window. Beyond these\nprobing tasks, FILM-7B significantly improves the performance on real-world\nlong-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while\nmaintaining a comparable performance on short-context tasks (e.g., 59.3->59.2\naccuracy on MMLU). Github Link: https://github.com/microsoft/FILM.",
        "updated": "2024-04-25 17:55:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16811v1"
    },
    {
        "title": "AAPL: Adding Attributes to Prompt Learning for Vision-Language Models",
        "authors": "Gahyeon KimSohee KimSeokju Lee",
        "links": "http://arxiv.org/abs/2404.16804v1",
        "entry_id": "http://arxiv.org/abs/2404.16804v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16804v1",
        "summary": "Recent advances in large pre-trained vision-language models have demonstrated\nremarkable performance on zero-shot downstream tasks. Building upon this,\nrecent studies, such as CoOp and CoCoOp, have proposed the use of prompt\nlearning, where context within a prompt is replaced with learnable vectors,\nleading to significant improvements over manually crafted prompts. However, the\nperformance improvement for unseen classes is still marginal, and to tackle\nthis problem, data augmentation has been frequently used in traditional\nzero-shot learning techniques. Through our experiments, we have identified\nimportant issues in CoOp and CoCoOp: the context learned through traditional\nimage augmentation is biased toward seen classes, negatively impacting\ngeneralization to unseen classes. To address this problem, we propose\nadversarial token embedding to disentangle low-level visual augmentation\nfeatures from high-level class information when inducing bias in learnable\nprompts. Through our novel mechanism called \"Adding Attributes to Prompt\nLearning\", AAPL, we guide the learnable context to effectively extract text\nfeatures by focusing on high-level features for unseen classes. We have\nconducted experiments across 11 datasets, and overall, AAPL shows favorable\nperformances compared to the existing methods in few-shot learning, zero-shot\nlearning, cross-dataset, and domain generalization tasks.",
        "updated": "2024-04-25 17:51:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16804v1"
    },
    {
        "title": "Weak-to-Strong Extrapolation Expedites Alignment",
        "authors": "Chujie ZhengZiqi WangHeng JiMinlie HuangNanyun Peng",
        "links": "http://arxiv.org/abs/2404.16792v1",
        "entry_id": "http://arxiv.org/abs/2404.16792v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16792v1",
        "summary": "Although the capabilities of large language models (LLMs) ideally scale up\nwith increasing data and compute, they are inevitably constrained by limited\nresources in reality. Suppose we have a moderately trained LLM (e.g., trained\nto align with human preference) in hand, can we further exploit its potential\nand cheaply acquire a stronger model? In this paper, we propose a simple method\ncalled ExPO to boost LLMs' alignment with human preference. ExPO assumes that a\nmedium-aligned model can be interpolated between a less-aligned (weaker) model,\ne.g., the initial SFT model, and a better-aligned (stronger) one, thereby\ndirectly obtaining this stronger model by extrapolating from the weights of the\nformer two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show\nthat ExPO pushes models trained with less preference data (e.g., 10% or 20%) to\nreach and even surpass the fully-trained one, without any additional training.\nFurthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and\nexhibits decent scalability across model sizes from 7B to 70B. Our work\ndemonstrates the efficacy of model extrapolation in exploiting LLMs'\ncapabilities, suggesting a promising direction that deserves future\nexploration.",
        "updated": "2024-04-25 17:39:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16792v1"
    }
]