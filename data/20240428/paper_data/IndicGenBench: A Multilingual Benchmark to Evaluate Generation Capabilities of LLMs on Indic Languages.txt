INDICGENBENCH: A Multilingual Benchmark to Evaluate Generation
Capabilities of LLMs on Indic Languages
HarmanSingh,NitishGupta,ShikharBharadwaj,DineshTewari,ParthaTalukdar
GoogleResearchIndia
{hrman, guptanitish, shikharop, dineshtewari, partha}@google.com
Abstract
Aslargelanguagemodels(LLMs)seeincreas-
ing adoption across the globe, it is impera-
tive for LLMs to be representative of the lin-
guistic diversity of the world. India is a lin-
guisticallydiversecountryof1.4Billionpeo-
ple. TofacilitateresearchonmultilingualLLM
evaluation, we release INDICGENBENCH —
the largest benchmark for evaluating LLMs
on user-facing generation tasks across a di-
verse set 29 of Indic languages covering 13
scripts and 4 language families. INDICGEN-
BENCH is composed of diverse generation
tasks like cross-lingual summarization, ma-
Figure1: Performanceofstate-of-the-artLLMson
chine translation, and cross-lingual question
differenttasksinINDICGENBENCH. Weobservea
answering. INDICGENBENCHextendsexisting
significantperformancegapbetweenEnglishandIndic
benchmarkstomanyIndiclanguagesthrough
languagesacrossLLMs.
humancurationprovidingmulti-wayparallel
evaluationdataformanyunder-representedIn-
dic languages for the first time. We evaluate
loweramountsofwebresourcesavailable. Tode-
a wide range of proprietary and open-source
velophighlymultilingualgenerativeLLMswhich
LLMs including GPT-3.5, GPT-4, PaLM-2,
mT5, Gemma, BLOOM and LLaMA on IN- should work equally well for 100s of languages
DICGENBENCH inavarietyofsettings. The spokenbybillionsofpeopleintheworld,itiscru-
largest PaLM-2 models performs the best on cialtoevaluatetheircapabilitiesacrossavarietyof
mosttasks,however,thereisasignificantper- languagestouncoverperformancegapsandguide
formancegapinalllanguagescomparedtoEn-
futureresearch.
glishshowingthatfurtherresearchis needed
InthisworkwefocusonIndia, acountrywith
forthedevelopmentofmoreinclusivemultilin-
gual language models. INDICGENBENCH is 1369rationalizedmothertonguesspokenbymore
availableatwww.github.com/google-research- than a billion people.1 Making progress on lan-
datasets/indic-gen-bench guage technologies for Indic languages will not
onlyimprovethestateofaffairsinthisregion,but
willalsoprovidevaluablelearningtotheNLPcom-
1 Introduction
munitywhichwillbeapplicabletoothergeograph-
Withtheadvancesingenerativelanguagetechnolo- icalregionsandlanguagefamilies. Therearehas
giespoweredbyLargeLanguageModels(LLMs; beenmuchworkfromthecommunityinbuilding
Brown et al., 2020; Rae et al., 2021; Chowdh- naturallanguageunderstanding(NLU)modelsfor
ery et al., 2022; OpenAI et al., 2023; Tay et al., Indic languages (Kakwani et al., 2020; Khanuja
2023; Google, 2023), there has been a surge of etal.,2021),aswellasevaluationdatasets(Dodda-
interest in evaluating the multilingual capabili- panenietal.,2023;Mhaskeetal.,2023)tosupport
ties of these models. Recent work (Ahuja et al., suchmodels. Inthiswork,ourfocusistodevelop
2023a,b)showsaconsistentperformancegapbe-
tweenhighresourcelanguagesandlanguageswith 1https://en.wikipedia.org/wiki/Languages_of_India
4202
rpA
52
]LC.sc[
1v61861.4042:viXra#Languages DatasetSize
Task Language Input Output
(H/M/L) (Train/Dev/Test)
CROSSSUM-IN
(Cross-lingualSummarization) Hindi 9/7/13 2.9k/2.9k/14.5k
FLORES-IN
(MachineTranslation) Konkani 9/7/13 -/28.9k/29.3k
XQUAD-IN
(MultilingualQA) Punjabi 9/3/- 1.2k/1.2k/14.3k
XORQA-IN-XX
(Cross-lingualQA) Telugu 9/6/13 2.8k/14k/15.1k
XORQA-IN-EN
(Cross-lingualQA) Santali 9/6/13 2.8k/14k/15.1k
Table 1: INDICGENBENCH, our proposed benchmark, consists of five tasks: Cross-lingual Summarization
(CROSSSUM-IN), Machine Translation (FLORES-IN), Multilingual QA (XQUAD-IN) and Cross-lingual QA
(XORQA-IN-XX and XORQA-IN-EN). An example from each task, the number of languages for which we
collect evaluation data (divided by resourcefulness, higher (H), medium (M) and low (L)), and the number of
training/validation/testinstancespertaskisshownabove. SeeSection2fordetails.
ahigh-qualitybenchmarkforevaluatinggenerative Ourcomprehensiveevaluationofvariousstate-of-
languagecapabilitiesinavarietyofIndiclanguages the-artproprietaryandopen-sourceLLMson IN-
acrossvariouslevelsofresourcefulness. DICGENBENCH shows that there is a significant
gapinperformancebetweenEnglishandIndiclan-
We release INDICGENBENCH, a multilingual,
guages (see Figure 1). Our contributions are as
multi-way parallelbenchmark for measuringlan-
follows:
guagegenerationcapabilitiesacrossdiverseuser-
facing tasks in 29 Indic languages across 4 lan-
• Created and released INDICGENBENCH, a
guage families (Table 7). INDICGENBENCH
high quality text benchmark in diverse lan-
extends existing benchmarks such as Cross-
guage generation tasks like summarization,
Sum(Bhattacharjeeetal.,2023),XQuAD(Artetxe
question-answering,andtranslationacross29
etal.,2020),XorQA(Asaietal.,2021),andFLO-
Indic languages. INDICGENBENCH is the
RES (NLLB-Team et al., 2022) for additional
largest generation benchmark for Indic lan-
Indic languages and is composed of tasks like
guages.
cross-lingualsummarization(CROSSSUM-IN),ma-
chinetranslation(FLORES-IN),cross-lingualread- • Comprehensive experimentation on SoTA
ingcomprehension(XORQA-IN-XXandXORQA- LLMs (mT5, Gemma, BLOOM, LLaMA,
IN-EN) and multilingual reading comprehension GPT-3.5, GPT-4, PaLM-2) across various
(XQUAD-IN). Each dataset consists of parallel model sizes and training settings to bench-
examplesinupto29lowtocomparativelyhigher marktheirIndiclanguagegenerationcapabili-
resourceIndiclanguages;andforsometasks(e.g. ties.
CROSSSUM-IN),INDICGENBENCHprovidesthe
first-everevaluationdatasetsforasmanyas18of • A qualitative analysis for assessing the gaps
theselanguages. Wealsoreleaseasmalltraining in current language technologies and define
set in all tasks for efficient adaptation of LLMs. potentialdirectionsoffutureresearch.2 INDICGENBENCH collectandreleaseasmallamountoftrainingand
validationdatamakingpossibleevaluationoftrain-
INDICGENBENCH is a high-quality, human-
ingtechniqueslikefine-tuning,parameter-efficient
curatedbenchmarktoevaluatetextgenerationcapa-
training,in-contextlearning,andothers.
bilitiesofmultilingualmodelsonIndiclanguages.
Whyextendexistingbenchmarks? Wechoseto
Ourbenchmarkconsistsof5user-facingtasks(viz.,
collecthumantranslationsofexistingbenchmarks
summarization,machinetranslation,andquestion
asopposedtocreatingevaluationdatafromscratch
answering)across29Indiclanguagesspanning13
duetovariousreasons:
writingscriptsand4languagefamilies. Forcertain
tasks, INDICGENBENCH provides the first-ever • Translation-basedextensionofexistingbench-
evaluationdatasetforupto18Indiclanguages. Ta- markresultsinmulti-wayparalleldata,allow-
ble1providessummaryof INDICGENBENCHand ingresearcherstoattributeperformancedueto
examplesofinstancesacrosstaskspresentinit. taskknowledgevs. languageunderstanding,
Languages in INDICGENBENCH are divided andmeasurecross-lingualgeneralization
into (relatively) Higher, Medium, and Low
• For many low-resource languages in INDIC-
resource categories based on the availability of
webtextresources(seeappendix§Afordetails).2
GENBENCH, clean text knowledge corpus
(e.g., Wikipedia) is not available making it
difficulttoacquiresourcedataforannotation
Higher(9): Bengali,Gujarati,Hindi,Kannada,Malay-
alam,Marathi,Tamil,Telugu,Urdu
• Byfocusingonlyontranslationqualityinthe
Medium(7): Assamese,Bhojpuri,Nepali,Odia,Pun-
targetIndiclanguages,weareabletoleverage
jabi,Pashto,Sanskrit
the quality control that went into designing
Low (13): Awadhi, Haryanvi, Tibetan, Garhwali,
thesourcebenchmarks.
Konkani, Chhattisgarhi, Rajasthani, Maithili, Manipuri,
Annotators were professional data labelers work-
Malvi,Marwari,Santali,Bodo
ing as contractors at our organization and with a
As evident from the lists above, our benchmark
vendor. Annotatorswerepaidcompetitiveratesin
provides a broad-coverage over languages with
compliancewithapplicablelaborlawsandprevail-
respect to their resourcedness, allowing users
ingmarketrates. Ourpayratetoannotatorsvaried
to evaluate language models on relatively high-
acrosslanguages,rangingfromUSD2.80perhour
resource languages such as Hindi and extremely
forPashtotoUSD15.90perhourforTibetan.
low-resourcelanguagessuchasManipuriinMeitei
scriptonasinglebenchmark. Cross-LingualSummarization: CROSSSUM-IN
Tocuratetheevaluationdatasetsforourbench- We create CROSSSUM-IN based on Cross-
mark, we use the following existing datasets Sum(Bhattacharjeeetal.,2023),adatasetforcross-
as the source: CrossSum (Bhattacharjee et al., lingual summarization, which in turn is derived
2023) for cross-lingual summarization, FLO- from XL-Sum (Hasan et al., 2021b). CrossSum
RES(NLLB-Teametal.,2022)formachinetrans- contains multi-way parallel data in 45 languages
lation, XQuAD (Artetxe et al., 2020) for multi- whereBBCnewsarticlesassourceinalanguage
lingual QA, and XoRQA (Asai et al., 2021) for arepairedwithcorrespondingsummariesinother
cross-lingualQA.Fromeachofthesedatasetswe languages. Based on their matching criteria, dif-
select a subset of English examples to be a part ferentlanguageshavedifferentamountofsource-
of our benchmark, and then collect professional targetpairs.
humantranslationsfortheseexamplesinalltarget We sample 700 English article-summary pairs
Indic languages. Some target languages are al- (100eachfromtrain/devand500fromtest)andask
readycoveredbythesourcedatasetsinwhichcase humantranslatorstotranslatetheEnglishsummary
we re-purpose this existing data and only collect intothetargetIndiclanguages. CrossSumalready
translationsfortheremaininglanguages. Wealso containsdatafor9ofour29targetlanguages;for
theselanguageswesample100/100/500examples
2We note that the languages called relatively higher re-
sourceinthispaper,e.g.,HindiorBengali,areinfactmid-low from the original dataset to maintain equity with
WebresourcewhencomparedtoEnglishandothertrulyhigh otherlanguageswecollectdatafor. CROSSSUM-
resourcelanguages.Forexample,usingWikipediaasaproxy
IN contains a total of 20.3k examples across 29
forlanguageresources,comparedto6.6M+Wikipediaarticles
inEnglish,thereareonly160KHindiWikipediaarticles. Indiclanguagesinourbenchmark.MachineTranslation: FLORES-IN createdwiththeideaofdevelopingNLPsystems
thatcananswerquestionsinusers’nativelanguage
FLORES-200 (NLLB-Team et al., 2022) is a
byreferingtosourcesinahigh-resourcelanguage,
human-annotatedmulti-wayparallelmachinetrans-
suchasEnglish,whichwasmorelikelytocontain
lation (MT) benchmark for 200 languages where
theanswerduetotheinformationscarcityoflow-
thesamesourceEnglishsentencesaretranslatedby
humansintothetarget200languages. Itcontains
resourceslanguagesontheweb. TheoriginalXOR-
datain22ofour29targetlanguages;weextendthis
TYDI contains data in 7 languages out of which
BengaliandTeluguarethetwoIndiclanguages.
bycollectinghumantranslationsfortheremaining
7newlanguagesleadingtoaMTbenchmarkin29 To create XORQA-IN, we select the 302 Ben-
Indiclanguageswhichwecall FLORES-IN. galiand237Teluguexamples(Bn/Te-question,En-
FLORES-200 is divided into three splits: dev passage,En-answer)fromtheXOR-TYDIdevset
(997),devtest(1012),test(992),ofwhichthetest asourtestdata.4 Additionally,wesample600ex-
set it not public. We collect translations for all amples(equallyfromBengaliandTelugu)fromthe
997devand1012devtestsentences,yielding2009 training set of XOR-TYDI to create our training
sentencesperlanguage. Collectively,FLORES-IN (100)anddevelopment(500)set. Wethenfollow
contains58.2kexamplesacross29Indiclanguages. a two-staged translation process, where we first
askthehumantranslatorstotranslatetheBengali
MultilingualQuestion-Answering: XQUAD-IN orTeluguquestion(Bn/Te-question)intoEnglish
WecreateanIndicMultilingualQuestionAnswer- (En-question). Inthesecondstage,wecollecttrans-
ing task XQUAD-IN based on the multilingual lations for these English questions (En-question)
readingcomprehensiondatasetXQuAD(Artetxe into target languages (Xx-question) and transla-
etal.,2020). XQuADisinturnderivedfromthe tionsfortheEnglishanswers(En-answer)intothe
SQuADdataset(Rajpurkaretal.,2016),inwhich targetlanguages(Xx-answer).
anEnglishWikipediapassageispairedwithmulti- We create two tasks from this translated data:
plequestion-answer(QA)pairswheretheanswers 1. XORQA-IN-EN: Eachexamplecontains(Xx-
areshortspansforthegivenpassage. Theauthors question, En-passage, En-answer). This task is
of XQuAD collected human translations for 240 similartothe XOR-TYDI datasetinadditionalIn-
passagesand1190QApairsfromtheSQuADv1.1 diclanguages.
developmentsetinto10higherresourcelanguages 2. XORQA-IN-XX: Eachexamplecontains(Xx-
(HindibeingtheonlyIndiclanguage). question,En-passage,Xx-answer),wherethetask
Tocreate XQUAD-IN,weusethe240passages istogeneratetheanswerinthesamelanguageas
and 1190 QA pairs from XQuAD as our test set. thequestion.
Weadditionallyselected20passagesand100QA Wecollectdatafor28Indiclanguagesresulting
pairsfromtheoriginalSQuADv1.1trainingand in32kexamples.5
development sets each to create our training and
developmentset. Forallthe280passagesand1390 3 ExperimentsandAnalysis
QA pairs we collect professional human transla-
tionsin12Indiclanguages.3 Overall, XQUAD-IN
Weuse INDICGENBENCH tobenchmarkmultilin-
gual and cross-lingual language generation capa-
contains 3.3k passages and 16.6k QA pairs in 12
bilities of various LLMs on Indic languages. We
Indiclanguages.
performexperimentswithavarietyofopen-source
Cross-LingualQuestion-Answering: LLMs — mT5 (Xue et al., 2021), LLaMA (Tou-
XORQA-IN vron et al., 2023),6, BLOOMZ (Workshop et al.,
2022),Gemma(Teametal.,2024);andproprietary
WecreateIndicCross-lingualQuestion-Answering
LLMs —GPT-3.5, GPT-4(OpenAI etal., 2023),
datasetXORQA-INbasedontheXOR-TYDIQA
andPaLM-2(Aniletal.,2023).
dataset (Asai et al., 2021). XOR-TYDI contains
We compare and analyze the performance of
questions in non-English languages paired with
these LLMs and their variants in terms of model
Englishevidencepassagesandshortspananswers
sizes under different learning paradigms and set-
from those passages (similar to SQuAD). It was
3XQUAD-INcontainsall9higher-resourcelanguages(see 4XOR-TYDIhasnotpubliclyreleaseditstestset.
§2)and3medium-resourceslanguages,namely,Assamese, 5WedonotcollecttranslationsforNepali.
Odia,andPunjabi. 6LLaMA-2couldnotbeusedduetoarestrictivelicenceModel(LLM)
CROSSSUM-IN
FLORES-IN XQUAD-IN
XORQA-IN-XX XORQA-IN-EN a
O
inn nd
bF
oX
tL
hO
O
dR
R
iQ
E
reSA
c-
tI- ioNIN
n,
swQ —eA
tr re
at pa ns
o
sk
r
lats t.
t ir na gns fl ra ot mion Ep ne gr lf io shrm toan thc ee
Eval.Metric ChrF ChrF Token-F1 Token-F1 Token-F1 targetlanguage(enxx)andvice-versa(xxen).
(enxx / xxen)
3.1 ComparisonofLLMson
PerformanceinEnglish
INDICGENBENCH
GPT-4 30.3 –/– 64.8 – 37.9
PaLM-2-L 41.1 –/– 83.7 – 71.4 In Table 2 we evaluate LLaMA, BLOOMZ,
AveragePerformanceonINDICGENBENCH Gemma,GPTandPaLM-2familyofmodelsonall
LLaMA-7B 3.7 11.5/21.6 3.8 7.4 10.4
tasksofINDICGENBENCHinaone-shotprompted
LLaMA-13B 4.1 13.3/24.1 4.5 10.4 12.1 setting. Numbersareaveragedacrossalllanguages
LLaMA-65B 4.6 18.1/32.7 7.1 16.5 16.3
intheevaluationdata. Tocompare,wealsoreport
BLOOM-7B 3.8 18.3/31.2 13.8 7.9 23.6
EnglishperformanceforGPT-4andPaLM-2-L.
BLOOMZ-7B 1.2 40.8/48.4 53.7 7.0 49.0
We see across tasks that larger models from
Gemma-7B-PT 0.0 32.1/50.4 0.5 11.7 23.8
Gemma-7B-IT 11.6 18.6/29.2 35.3 13.5 24.8 the same LLM family perform better. PaLM-
GPT-3.5 16.3 29.2/47.7 33.2 21.6 35.5 2-L performs the best among all LLMs consid-
GPT-4 17.6 32.1/54.5 55.7 23.4 46.0
ered, except for the XORQA-IN-EN task where
PaLM-2-XXS 7.2 24.0/43.4 34.6 13.5 36.8
PaLM-2-S performs slightly better. We find
PaLM-2-XS 15.5 40.7/58.3 62.2 29.5 47.8
PaLM-2-S 18.5 43.5/61.6 66.7 31.6 57.4 that open source LLaMA models perform much
PaLM-2-L 21.2 47.5/65.1 69.3 37.4 55.9 worse compared to proprietary models; even the
largestLLaMA-65Bmodelsignificantlyunderper-
Table 2: One-shot performance on INDICGEN-
formsthesmallestPaLM-2-XXSmodel. Gemma-
BENCH across model sizes for all LLMs considered
7B instruction tuned model performs better than
inourwork(§3.1). ForeachLLMfamilyperformance
improveswithincreasingmodelsize,withPaLM-2-L LLaMA-13BaswellasLLaMA-65Bonmosttasks.
performingthebestacrossmosttasks. ComparedtoEn- BLOOMZ, which is an instruction tuned version
glish,allmodelsunder-performsignificantlyhighlight- of BLOOM (Workshop et al., 2022), pre-trained
ingshortcomingsofcurrentSoTALLMs. SeeSection onlarge-scalemultilingualdata,worksthebeston
3.1fordetails.
three out of five tasks in INDICGENBENCH. On
CROSSSUM-INandXORQA-IN-XXitfallsbehind
LLaMA and Gemma. Compared to English, we
tings. We first evaluate model performance on
see significant room for improvement (20+ ChrF
one-shot prompting (§3.1) and also measure per-
orToken-F1points)acrossalltasks.
formanceacrosslanguagecategoriesbasedonre-
sourcedness (§3.2). We then evaluate the effect
3.2 Performanceacrosslanguagecategories
of number of in-context examples shown to the
InTable3wereportone-shotperformanceacross
model as supervised data (§3.3) and the effect of
languagecategoriesdefinedinSection2. Weonly
promptinginahigher-resourcelanguagesuchan
showperformanceforGemma-7B-IT,BLOOMZ-
English or Hindi (§3.4). Using the training data
7B, LLaMA-65B, GPT-4 and PaLM-2-L models
containedin INDICGENBENCH,wemeasurehow
hereandreportperformancefortheothermodels
the performance of LLMs after fine-tuning com-
inappendixB.1. Wefindthatthereisasignificant
pareswithfew-shotprompting(§3.5). Finally,we
performancedropgoingfromhigherresourcedlan-
performqualitativeanalysisofmodelson INDIC-
guagestomediumresourcedones,andfurtherdrop
GENBENCHandhighlightsomeareasofimprove-
inlowerresourcedlanguages.
mentforfuturemodeldevelopment(§3.7).
Wewouldliketopointouttwoobservationshere:
Evaluation Metrics For the cross-lingual sum- (a)In FLORES-IN,theperformancefortranslating
marization and translation tasks, CROSSSUM-IN Englishtothetargetlanguage(enxx)dropssignif-
and FLORES-IN, we report Character-F1 (ChrF) icantlyfromhighertolowerresourcedlanguages
metric (Popovic´, 2015) since token-level metrics (56.9 → 41.9 for PaLM-2-L) whereas the perfor-
like ROUGE and BLEU are not reliable for low- manceinthexxendirectiondoesnotfallthisdras-
resource languages (Bapna et al., 2022). To stay tically(68.2 → 62.6). Asimilartrendisseenwhen
consistentwithexistingliteratureonQAtasks,we comparingXORQA-IN-XXandXORQA-IN-EN.
reportSQuAD-styleToken-F1onour XQUAD-IN ThishighlightsthatcurrentLLMsarebetteratun-CROSSSUM-IN FLORES-IN(enxx/xxen) XQUAD-IN XORQA-IN-XX XORQA-IN-EN
Model High Medium Low High Medium Low High Medium High Medium Low High Medium Low
LLaMA-65B 4.4 4.6 4.7 18.2/31.5 15.4/30.0 19.5/35.0 8.8 1.9 17.7 13.5 17.1 16.4 14.0 17.3
Gemma-7B-IT 13.9 11.5 10.0 17.6/33.7 15.0/26.1 21.3/27.7 38.8 24.8 18.9 8.3 12.2 29.5 23.9 21.9
BLOOMZ-7B 1.5 1.7 0.6 67.7/59.1 39.4/50.2 22.9/40.0 55.5 48.1 10.8 2.8 6.2 64.7 45.8 39.5
GPT-4 19.4 17.9 16.3 36.2/59.6 30.7/55.2 29.9/50.5 56.1 54.6 25.8 21.6 22.6 49.4 50.0 41.8
PaLM-2-L 25.2 23.1 17.5 56.9/68.2 45.9/65.6 41.9/62.6 72.5 59.8 41.9 36.7 34.6 57.3 57.9 53.9
Table3: One-shotperformanceacrosslanguagecategoriesbasedonresourcednessdefinedinSection2. Forall
tasks,wewitnesssignificantlylowerperformancesinmediumandlowresourcelanguagescomparedtothehigher
resourceones. PleaseseeTable9inappendixB.1forresultsonothermodels. SeeSection3.2formoredetails.
derstandingtheselower-resourcedlanguagesthan thein-contextexamplesimprovesperformance.
generatingfluenttextinthem.
(b)Infewcases,weseesmallerperformancedeltas 3.4 Transferfromhigh-resourcelanguages
between medium and lower resourced languages
For languages with no supervised data, one op-
comparedtohigherandmediumcategories. From
tion to improve performance is utilizing existing
ouranalysis,thiscanmainlybeattributedtomany
superviseddataanotherlanguageasin-contextex-
languages in the lower category being similar to
emplars. In this section we aim to study if the
HindiandwritteninthesameDevanagariscript.
languageinwhichthemodelispromptedplaysa
roleinperformance.
FLORES-IN XORQA-IN-XX InTable5weshowperformancewhenthemodel
Model(LLM) 0 1 5 0 1 2 3 ispromptedinEnglishvs. Hindi,arepresentative
LLaMA-7B 8.0 11.5 11.4 5.0 7.4 9.0 9.2 higherresourcedIndiclanguage. Forcomparison,
LLaMA-13B 8.6 13.3 13.4 6.3 10.4 12.2 13.1 we also show performance when the in-context
LLaMA-65B 14.0 18.1 18.3 12.3 16.5 18.7 19.4
exemplar is in the same language as the test in-
PaLM-2-XXS 0.8 24.0 26.9 8.9 13.5 15.8 17.5 stance. WefindthatHindiin-contextexemplarsare
PaLM-2-XS 20.1 40.7 42.3 21.4 29.5 32.2 33.2
much more useful for all models as compared to
PaLM-2-S 24.9 43.5 45.2 22.7 31.6 33.4 35.4
PaLM-2-L 31.1 47.5 49.3 31.9 37.4 39.7 41.1 theirEnglishcounterparts. Surprisingly,forsmaller
models,performancewithHindiexemplarscomes
Table 4: Performance by varying number of in-
extremelyclosetopromptinginthetestlanguage,
contextexemplarsforLLaMAandPaLM-2modelson
evenbettersometimes.
FLORES-IN (enxx)and XORQA-IN-XX tasks(§3.3).
Performanceimproveswithincreasingamountsofsu-
pervisionprovidedin-context. ReferappendixB.2for
3.5 Fine-tuningLLMson INDICGENBENCH
resultsonothertasksandmodels. andComparisonwithIn-Context
Learning
As outlined in Section 2, we also release a small,
3.3 In-contextlearningon INDICGENBENCH
high-quality training set for all tasks in INDIC-
Inthissectionweaimtounderstandtheimpactof GENBENCH (except FLORES-IN whichonlyhas
the number of in-context examples shown to the dev and test sets). This training data can be used
LLMduringfew-shotprompting. to adapt LLMs to downstream tasks in Indic lan-
Since CROSSSUM-IN and XQUAD-IN input guages via fine-tuning and other training tech-
passages are long, we are only able to perform niques.
0-and-1-shotprompting. ForXORQA-IN-XXand Table 6 shows our results of fine-tuning mT5
XORQA-IN-EN we perform 0-to-3-shot prompt- andPaLM-2modelsandtheircomparisonwithin-
ing,andforFLORES-INweperform0,1and5-shot contextlearningusingPaLM-2. Wefine-tuneeach
prompting. modelontrainingdatafromallavailablelanguages
We show performance for FLORES-IN and includingEnglish,usethedevelopmentsetforearly
XORQA-IN-XX in Table 4. Other results are stopping, and report numbers on the test set. For
shown in appendix B.5 due to space limitations. question-answering tasks that require generating
Across model families and sizes we observe that shortspansasanswers,wefindthatoldergenera-
increasing the amount of supervision in terms of tionmT5modelssignificantlyoutperformsmallerCROSSSUM-IN XQUAD-IN XORQA-IN-XX XORQA-IN-EN
Model(1-ShotLang) Higher Medium Low Higher Medium Higher Medium Low Higher Medium Low
PaLM-2-XXS(En) 0.3 0.1 0.3 38.5 31.9 14.0 5.4 7.3 40.3 35.0 30.8
PaLM-2-XXS(Hi) 1.3 2.1 3.7 39.8 33.3 17.6 8.5 10.5 45.5 39.4 31.9
PaLM-2-XXS(Lang) 7.7 7.6 6.7 37.2 26.8 17.7 8.8 12.8 43.6 38.3 31.5
PaLM-2-XS(En) 0.3 0.2 0.5 64.3 62.2 30.6 23.9 20.8 35.9 32.1 27.2
PaLM-2-XS(Hi) 3.5 5.5 9.9 65.4 63.5 33.2 25.8 22.7 49.3 46.8 40.7
PaLM-2-XS(Lang) 18.4 16.4 13.0 65.1 53.3 35.8 27.6 26.1 53.3 51.5 42.2
PaLM-2-S(En) 0.4 0.2 0.5 67.4 66.8 27.5 19.9 19.9 48.6 47.1 40.8
PaLM-2-S(Hi) 4.4 6.9 13.2 68.5 67.5 34.2 27.0 24.9 58.3 57.0 49.0
PaLM-2-S(Lang) 22.4 19.8 15.1 69.9 57.3 36.6 30.3 28.6 60.1 61.4 53.6
PaLM-2-L(En) 0.4 0.2 0.6 71.7 69.8 37.7 33.2 29.7 28.7 27.5 26.2
PaLM-2-L(Hi) 4.7 7.0 13.8 72.6 71.0 39.7 34.6 31.2 45.5 44.8 41.5
PaLM-2-L(Lang) 25.2 23.1 17.5 72.5 59.8 41.9 36.7 34.6 57.3 57.9 53.9
Table5: Effectofin-contextexemplarlanguage(§3.4): Performancecomparisonwhentheone-shotexemplaris
providedinEnglish(En)orHindi(Hi)asopposedtothelanguageofthetestinstance(Lang). In-contextprompting
inthetestlanguage(Lang)providesthebestperformance,followedbyHindi(Hi)andthenEnglish(En). This
followsthesameorderasrelatednessbetweentestandpromptinglanguage,highlightingthebenefitofpromptingin
alanguagemorerelatedtothetestlanguage(e.g.,HindicomparedtoEnglishinthiscase).
CROSSSUM-IN XQUAD-IN XORQA-IN-XX XORQA-IN-EN
Model Higher Medium Low Higher Medium Higher Medium Low Higher Medium Low
mT5models–Fine-Tuned
mT5-B 19.5 18.9 15.1 46.2 30.9 3.8 4.0 5.5 31.7 31.4 30.8
mT5-L 20.5 19.9 15.5 54.3 38.6 11.8 11.0 10.4 56.8 53.7 45.4
mT5-XL 22.7 21.1 15.3 57.4 40.5 20.7 13.5 15.6 58.2 56.2 46.5
mT5-XXL 25.9 24.2 10.4 62.0 44.4 28.8 23.6 21.9 70.3 68.9 59.1
PaLM-2models-Fine-Tuned
PaLM-2-XXS 22.5 19.7 16.5 41.2 18.1 18.1 10.9 12.9 60.2 56.9 50.9
PaLM-2-XS 28.5 25.6 18.8 40.2 16.9 30.4 23.6 19.6 69.1 66.6 56.6
PaLM-2models-Few-shotprompted
PaLM-2-XXS 7.7 7.6 6.7 37.2 26.8 22.7 12.3 16.4 51.6 47.1 38.4
FS
PaLM-2-XS 18.4 16.4 13.0 65.1 53.3 39.2 32.0 29.5 67.0 65.3 56.5
FS
Table6: (Top)Fine-tuningperformanceofmT5andPaLM-2models(§3.5). Boldrepresentsbestnumbersamong
fine-tunedmodels. PaLM-2outperformsmT5forlonger-formgenerationtask(CROSSSUM-IN),whereasmT5
modelsdowellonshortanswer-spanQAtasks. (Bottom)Comparisonofin-contextlearningvs. fine-tuning
on PaLM-2 models. In Green, we highlight the best PaLM-2 number (among fine-tuned and few-shot). For
CROSSSUM-INtaskrequiringlonger-formgeneration,fine-tuningoutperformsfew-shotprompting.
PaLM-2modelsinmostcases.7 On CROSSSUM- nificantlyincreasesfrom2-4%(inXXS)to9-10%
IN which requires generating a longer summary, (in XS). In the case of XQUAD-IN, we see that
wefindthatPaLM-2modelsaremoreeffective. forthelargerPaLM-2-XSmodel,itsmuchbetter
ForQuestion-Answeringtasks,asthemodelsize toperformin-contextlearningascomparedtofine-
increasesfromPaLM-2-XXStoPaLM-2-XS,we tuning, for both medium and high resource Indic
seethatin-contextlearningyieldsequalorbetter languages. ForXORQA-IN-EN,in-contextlearn-
performance compared to fine-tuning the model. ingreachesthefine-tuningperformanceasmodel
For example, in XORQA-IN-XX, as the model sizeincreasestoPaLM-2-XS.Forthe CROSSSUM-
sizeincreasesfromXXStoXS,weseethatthegap IN, the gap between fine-tuning and in-context
between few-shot prompting and fine-tuning sig- learningisreducingasmodelsizeincreases,which
reinforcesthatforevenlargermodelsizes,itmight
7Since the parameter count for PaLM-2 models is not bebettertolearnin-context.
public, we cannot attribute this performance difference to
modelsizes.3.6 AnalyzingTokenizeracrossIndic canbeinputtotheLLMduringinference. Thiscan
languages negatively impact performance (see Table 4). In
Figure3,weshowhowthepercentageofdatathat
fitsinaparticularcontextlengthchangeswithnum-
berofin-contextexamplesforvariouslanguages.
Forexample,weseeinFigure3thatformedium
resource languages with high token-fertility like
OriyaandPunjabiwecanin-corporatemuchfewer
in-contextexamples,comparedtoIndiclanguages
withlowertoken-fertilitylikeHindiandMarathi.
3.7 QualitativeAnalysis
We manually analyze predictions from the best
performing model PaLM-2-L with the aim to un-
derstand the shortcomings of current LLMs and
Figure 2: Tokenizer fertility for different languages highlight areas of improvements for future re-
usingOpenAI’sBytePairEncoding. Wenotethatmid- search. We randomly select 20 examples each
lowresourcelanguagessufferfromhightokenfertility. in the CROSSSUM-IN and FLORES-IN tasks for
(Section3.6) the following languages which are reviewed by
nativespeakers: Awadhi,Haryanvi,Chhatisgarhi,
Konkani,andAssamese. Wefoundthefollowing
patternsoferrors:
Generation in a related language The lan-
guages Awadhi, Haryanvi, and Chhatisgarhi are
related to a higher resource language Hindi and
written in the same script Devanagari. We find
that the model generates mixed-language output
withwordsmixedfromHindiandalsooutputsin-
correctlyinflectedformsofthemainverbsinthe
output. Weshowcoupleof examplesofthisphe-
nomenainFigure5aintheappendix.
HallucinationandMissingInformation Inthe
cross-lingualsummarizationtask CROSSSUM-IN,
Figure3: Percentageofin-contextXQUAD-INexem-
wefindthatthemodeloftenoutputsextrainforma-
plars that fit in a 1920 token context window. Mid-
tionthatisnotpresentinthesourcearticle. Intrans-
lowresourcelanguages’hightokenfertility(Figure2)
lation,wehaveobservedexampleswherecomecru-
makesitimpossibletoperformfew-shotpromptingin
theselanguages. (Section3.6) cialinformationfromthesourcesentenceismiss-
ingfromthegeneratedoutput. Also,insomecases,
InFigure2,wecomparethetokenfertility(av- themodelfailstounderstandpolysemousEnglish
eragenumberofsub-wordsthatawordisbroken words and generates translation for the incorrect
downintobythetokenizer)acrossallIndiclangu- sense. Weshowexamplesofthesephenomenain
gaesinINDICGENBENCH.8 Wefindthatthetoken Figures4a,4b,and5bintheappendix.
fertilityvariessignificantlyacrosslanguages;from
4 RelatedWork
4.1forPashtoto19.9forTibetan.
Ahightokenfertilityisundesirableandcandis- In the last few years, many multilingual LLMs
proportionatelyeffectaparticularlanguage’sper- havebeendeveloped—startingfrommBART(Liu
formance. Forlanguageswheretextisbrokeninto et al., 2020) trained on 25 languages to LLMs
morenumberoftokens,fewerin-contextexamples that are pre-trained on hundreds of languages,
such as mT55 (Xue et al., 2021), PaLM-2 (Anil
8We use OpenAI’s BPE tokenizer (plat-
et al., 2023), GPT-4 (Achiam et al., 2023), Gem-
form.openai.com/tokenizer). PaLM-2 tokenizer is not
publiclyavailable. ini (Google, 2023), and others. These LLMsaretypicallyevaluatedonindividualmultilingual generation datasets for 8 and 14 Indic languages
tasksforTranslation: WMT(Farhadetal.,2021), respectively,andTeSum(Urlanaetal.,2022)isan
FLORES (NLLB-Team et al., 2022); Question- abstractivesummarizationdatasetintheTelugulan-
Answering: XQuAD (Artetxe et al., 2020), Ty- guage. Rameshetal.(2022)introducedSamanan-
DiQA (Clark et al., 2020), XorQA (Asai et al., tar, a large translation dataset covering 11 Indic
2021); Summarization: XLSUM (Hasan et al., languages. OurworkcomplementsIndicNLGSuite
2021a), CrossSum (Bhattacharjee et al., 2023); and the other datasets in multiple ways. INDIC-
Reasoning: MGSM (Shi et al., 2022), XCOPA GENBENCH ismanuallyannotatedensuringhigh-
(Pontietal.,2020)tonameafew,oronmultilingual quality,noise-freetextwhichisnottypicallyfound
benchmarks such as, XTREME (Hu et al., 2020) on the web. Our benchmark contains evaluation
andXTREME-UP(Ruderetal.,2023). However, data for a much larger set of languages spanning
mostoftheseevaluationresourcescontainonlya low,mediumandhighresource. Ourdatasetsare
handful of languages or do not contain data for multi-languageparallelenablingbettercomparison
lowresourcelanguages,especiallyIndics. Besides, amongdifferentlanguages. Lastly,wefocusona
cross-lingualevaluationdataisevenmoresparse. complementaryandchallengingsetoftasks,includ-
Thisworkisanefforttobridgethesegapsbyreleas- ingcross-lingualsummarization,cross-lingualand
ingINDICGENBENCH,asuiteofdatasetscovering multilingualquestionanswering,andtranslation.
diversecross-lingualandmultilingualgeneration
5 Conclusion
tasksinIndiclanguages.
MostworkoncreatingevaluationdataonIndic
Werelease INDICGENBENCH, thelargestbench-
languageshavefocusedonnaturallanguageunder-
mark for evaluating LLMs on 5 user-facing gen-
standing(NLU)tasks. Kakwanietal.(2020)and
erationtasksacross29Indiclanguages,providing
Doddapanenietal.(2023)havereleasedNLUtest
evaluationdataformanyunder-representedIndic
setsinIndiclanguagesforawidevarietyoftasks
languagesforthefirsttime. INDICGENBENCH is
suchasQAandNLI.Naamapadam(Mhaskeetal.,
broadcoveragealongmanydimensions–itcovers
2023)isanamedentityrecognitiondatasetspecifi-
13writingscripts,4languagefamilies,andspans
callyforIndiclanguages,MASSIVE(FitzGerald
languagesacrosstheavailablewebresourcespec-
etal.,2022)isaslot-fillingandintentclassification
trum. We carry out extensive comparison of cur-
datasetavailablein7Indiclanguages,IndicGLUE
rentSoTALLMson INDICGENBENCHandhigh-
(Kakwani et al., 2020) is an NLU benchmark for
lightareasforfutureimprovement. Wearehopeful
11Indiclanguages,whereasGLUECoS(Khanuja
INDICGENBENCH willplayanimportantrolein
etal.,2020)isaHindi-Englishcode-mixedbench-
furtherdevelopmentofLLMsinIndiclanguages
mark,containingvariousNLUtasks. TheBelebele
ultimatelybenefitingabillion-pluspopulation.
Benchmark(Bandarkaretal.,2023)isamultiple-
choicemachinereadingcomprehensiondatasetfor 6 Limitations
122languagesofwhich17areIndic. Ontheother
hand, INDICGENBENCHisanaturallanguagegen-
Since INDICGENBENCH extends existing bench-
markstonewIndiclanguagesthroughhumantrans-
eration(NLG)benchmark.
lation,itmaymisssomeIndia-specificentitiesand
Recently,therehasbeenworkincreatingevalu-
linguisticnuances. Futureworkcanexploretrans-
ationbenchmarksfornaturallanguagegeneration
localizationforcreatingimprovedevaluationand
(NLG) on Indic languages. IndicNLG Suite (Ku-
mar et al., 2022), consisting of 5 NLG taks in 11
fine-tuning. INDICGENBENCHdoesn’tcoverlong-
formgenerationandreasoningtasks. Creatingsuch
Indiclanguages,isaleapinthisdirection. These
datasetsispartofourfuturework.
datasetsinthissuiteareautomaticallycreated,ei-
therusingdatafromtheweb(e.g.,Wikipedia)or
7 Acknowledgments
using translation systems. There are few works
whichcreateevaluationdataforindividualtasksin WethankAditiChaudhury,AshokPopat,Shachi
Indiclanguages. Forexample,IndicTrans2(Gala Dave, SagarGubbi, MeghUmekarandmembers
et al., 2023) creates an n-way parallel dataset for of the Languages team at Google Research India
machine translation in 22 scheduled Indian Lan- (GRI) for providing feedback on this work. The
guages, Mukhyansh (Madasu et al., 2023) and authorswouldliketothankManishGuptaandDivy
PMIndiaSum (Urlana et al., 2023) are headline Thakkarfortheirsupportandguidance.References lingualrepresentations. InProceedingsofthe58th
AnnualMeetingoftheAssociationforComputational
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
Linguistics,pages4623–4637,Online.Association
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
forComputationalLinguistics.
DiogoAlmeida,JankoAltenschmidt,SamAltman,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport. AkariAsai,JungoKasai,JonathanClark,KentonLee,
arXivpreprintarXiv:2303.08774. EunsolChoi,andHannanehHajishirzi.2021. XOR
QA:Cross-lingualopen-retrievalquestionanswering.
Kabir Ahuja, Harshita Diddee, Rishav Hada, Milli- InProceedingsofthe2021ConferenceoftheNorth
cent Ochieng, Krithika Ramesh, Prachi Jain, Ak- AmericanChapteroftheAssociationforComputa-
shayNambi,TanujaGanu,SameerSegal,Mohamed tionalLinguistics: HumanLanguageTechnologies,
Ahmed,KalikaBali,andSunayanaSitaram.2023a. pages 547–564, Online. Association for Computa-
Mega: Multilingualevaluationofgenerativeai. In tionalLinguistics.
EMNLP2023.
Abhinand Balachandran. 2023. Tamil-llama: A new
Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, tamillanguagemodelbasedonllama2.
Ishaan Watts, Ashutosh Sathe, Millicent Ochieng,
Rishav Hada, Prachi Jain, Maxamed Axmed, Ka- LucasBandarkar,DavisLiang,BenjaminMuller,Mikel
lika Bali, and Sunayana Sitaram. 2023b. Mega- Artetxe,SatyaNarayanShukla,DonaldHusa,Naman
verse: Benchmarkinglargelanguagemodelsacross Goyal,AbhinandanKrishnan,LukeZettlemoyer,and
languages,modalities,modelsandtasks. MadianKhabsa.2023. Thebelebelebenchmark: a
parallel reading comprehension dataset in 122 lan-
RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohn- guagevariants.
son, Dmitry Lepikhin, Alexandre Passos, Siamak
AnkurBapna,IsaacCaswell,JuliaKreutzer,OrhanFi-
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
rat, Daan van Esch, Aditya Siddhant, Mengmeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Niu, Pallavi N.Baljekar, XavierGarcía, Wolfgang
Shafey,YanpingHuang,KathyMeier-Hellstern,Gau-
Macherey, Theresa Breiner, Vera Axelrod, Jason
ravMishra,EricaMoreira,MarkOmernick,Kevin
Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey,
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Maxim Krikun, Pidong Wang, Alexander Gutkin,
YuanzhongXu,YujingZhang,GustavoHernandez
Apurva Shah, Yanping Huang, Z. Chen, Yonghui
Abrego,JunwhanAhn,JacobAustin,PaulBarham,
Wu,andMacduffHughes.2022. Buildingmachine
Jan Botha, James Bradbury, Siddhartha Brahma,
translationsystemsforthenextthousandlanguages.
KevinBrooks,MicheleCatasta,YongCheng,Colin
ArXiv,abs/2205.03983.
Cherry,ChristopherA.Choquette-Choo,Aakanksha
Chowdhery,ClémentCrepy,ShachiDave,Mostafa
AbhikBhattacharjee,TahmidHasan,WasiUddinAh-
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
mad, Yuan-Fang Li, Yong-Bin Kang, and Rifat
Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Shahriyar.2023. CrossSum: BeyondEnglish-centric
Feng, Vlad Fienber, Markus Freitag, Xavier Gar-
cross-lingual summarization for 1,500+ language
cia,SebastianGehrmann,LucasGonzalez,GuyGur-
pairs. InProceedingsofthe61stAnnualMeetingof
Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua
theAssociationforComputationalLinguistics(Vol-
Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-
ume 1: Long Papers), pages 2541–2564, Toronto,
witz,MichaelIsard,AbeIttycheriah,MatthewJagiel-
Canada.AssociationforComputationalLinguistics.
ski,WenhaoJia,KathleenKenealy,MaximKrikun,
SnehaKudugunta,ChangLan,KatherineLee,Ben- Tom Brown, Benjamin Mann, Nick Ryder, Melanie
jaminLee,EricLi,MusicLi,WeiLi,YaGuangLi, Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
JianLi,HyeontaekLim,HanzhaoLin,ZhongtaoLiu, Neelakantan,PranavShyam,GirishSastry,Amanda
FrederickLiu,MarcelloMaggioni,AromaMahendru, Askell, Sandhini Agarwal, Ariel Herbert-Voss,
JoshuaMaynez,VedantMisra,MaysamMoussalem, Gretchen Krueger, Tom Henighan, Rewon Child,
Zachary Nado, John Nham, Eric Ni, Andrew Nys- AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
trom, Alicia Parrish, Marie Pellat, Martin Polacek, Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
AlexPolozov,ReinerPope,SiyuanQiao,EmilyReif, teusz Litwin, Scott Gray, Benjamin Chess, Jack
Bryan Richter, Parker Riley, Alex Castro Ros, Au- Clark, ChristopherBerner, SamMcCandlish, Alec
rkoRoy,BrennanSaeta,RajkumarSamuel, Renee Radford, Ilya Sutskever, and Dario Amodei. 2020.
Shelby, Ambrose Slone, Daniel Smilkov, David R. Language models are few-shot learners. In Ad-
So, Daniel Sohn, Simon Tokumine, Dasha Valter, vances in Neural Information Processing Systems,
Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, volume 33, pages 1877–1901. Curran Associates,
Pidong Wang, Zirui Wang, Tao Wang, John Wiet- Inc.
ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue,PengchengYin,JiahuiYu,QiaoZhang,Steven AakankshaChowdhery,SharanNarang,JacobDevlin,
Zheng,CeZheng,WeikangZhou,DennyZhou,Slav Maarten Bosma, Gaurav Mishra, Adam Roberts,
Petrov, and Yonghui Wu. 2023. Palm 2 technical Paul Barham, Hyung Won Chung, Charles Sutton,
report. Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
2020. Onthecross-lingualtransferabilityofmono- odkumar Prabhakaran, Emily Reif, Nan Du, BenHutchinson, Reiner Pope, James Bradbury, Jacob Mitesh M. Khapra, Raj Dabre, Rudra Murthy, and
Austin,MichaelIsard,GuyGur-Ari,PengchengYin, AnoopKunchukuttan.2024. Airavata: Introducing
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, hindi instruction-tuned llm. arXiv preprint arXiv:
Sunipa Dev, Henryk Michalewski, Xavier Garcia, 2401.15006.
VedantMisra,KevinRobinson,LiamFedus,Denny
Zhou,DaphneIppolito,DavidLuan,HyeontaekLim, Gemini Team Google. 2023. Gemini: A family
Barret Zoph, Alexander Spiridonov, Ryan Sepassi, of highly capable multimodal models. ArXiv,
DavidDohan,ShivaniAgrawal,MarkOmernick,An- abs/2312.11805.
drew M. Dai, Thanumalayan Sankaranarayana Pil-
Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-
lai,MariePellat,AitorLewkowycz,EricaMoreira,
lam,KaziMubasshir,Yuan-FangLi,Yong-BinKang,
Rewon Child, Oleksandr Polozov, Katherine Lee,
M.SohelRahman,andRifatShahriyar.2021a. XL-
ZongweiZhou,XuezhiWang,BrennanSaeta,Mark
sum:Large-scalemultilingualabstractivesummariza-
Diaz,OrhanFirat,MicheleCatasta,JasonWei,Kathy
tionfor44languages. InFindingsoftheAssociation
Meier-Hellstern,DouglasEck,JeffDean,SlavPetrov,
forComputationalLinguistics: ACL-IJCNLP2021,
andNoahFiedel.2022. Palm:Scalinglanguagemod-
pages4693–4703,Online.AssociationforComputa-
elingwithpathways.
tionalLinguistics.
JonathanH.Clark,EunsolChoi,MichaelCollins,Dan
TahmidHasan,AbhikBhattacharjee,MdSaifulIslam,
Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and
Kazi Samin Mubasshir, Yuan-Fang Li, Yong-Bin
JennimariaPalomaki.2020. TyDiQA:Abenchmark
Kang,M.SohelRahman,andRifatShahriyar.2021b.
forinformation-seekingquestionansweringintypo-
Xl-sum: Large-scalemultilingualabstractivesumma-
logicallydiverselanguages. TransactionsoftheAs-
rizationfor44languages. InFindings.
sociationforComputationalLinguistics,8:454–470.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
Sumanth Doddapaneni, Rahul Aralikatte, Gowtham
ham Neubig, Orhan Firat, and Melvin Johnson.
Ramesh,ShreyaGoyal,MiteshM.Khapra,Anoop
2020. Xtreme: Amassivelymultilingualmulti-task
Kunchukuttan,andPratyushKumar.2023. Towards
benchmark for evaluating cross-lingual generalisa-
leavingnoIndiclanguagebehind: Buildingmonolin-
tion. InInternationalConferenceonMachineLearn-
gualcorpora,benchmarkandmodelsforIndiclan-
ing,pages4411–4421.PMLR.
guages. InProceedingsofthe61stAnnualMeeting
oftheAssociationforComputationalLinguistics(Vol- PratikJoshi, SebastinSanty, AmarBudhiraja, Kalika
ume1: LongPapers),pages12402–12426,Toronto, Bali,andMonojitChoudhury.2020. Thestateand
Canada.AssociationforComputationalLinguistics. fateoflinguisticdiversityandinclusionintheNLP
world. InProceedingsofthe58thAnnualMeetingof
AkhbardehFarhad,ArkhangorodskyArkady,Biesialska
theAssociationforComputationalLinguistics,pages
Magdalena,BojarOndˇrej,ChatterjeeRajen,Chaud-
6282–6293,Online.AssociationforComputational
hary Vishrav, Marta R Costa-jussa, España-Bonet
Linguistics.
Cristina, Fan Angela, Federmann Christian, et al.
2021. Findingsofthe2021conferenceonmachine Divyanshu Kakwani, Anoop Kunchukuttan, Satish
translation (wmt21). In Proceedings of the Sixth Golla,GokulN.C.,AvikBhattacharyya,MiteshM.
ConferenceonMachineTranslation,pages1–88.As- Khapra,andPratyushKumar.2020. IndicNLPSuite:
sociationforComputationalLinguistics. Monolingual corpora, evaluation benchmarks and
pre-trainedmultilinguallanguagemodelsforIndian
Jack FitzGerald, Christopher Hench, Charith Peris,
languages. InFindingsoftheAssociationforCom-
ScottMackie,KayRottmann,AnaSanchez,Aaron
putationalLinguistics: EMNLP2020,pages4948–
Nash,LiamUrbach,VisheshKakarala,RichaSingh,
4961, Online. Association for Computational Lin-
Swetha Ranganath, Laurie Crist, Misha Britan,
guistics.
Wouter Leeuwis, Gokhan Tur, and Prem Natara-
jan. 2022. Massive: A 1m-example multilin- Simran Khanuja, Diksha Bansal, Sarvesh Mehtani,
gualnaturallanguageunderstandingdatasetwith51 Savya Khosla, Atreyee Dey, Balaji Gopalan,
typologically-diverselanguages. DilipKumarMargam,PoojaAggarwal,RajivTeja
Nagipogu,ShachiDave,etal.2021. Muril: Multi-
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun lingualrepresentationsforindianlanguages. arXiv
Gumma,SumanthDoddapaneni,AswanthKumarM, preprintarXiv:2103.10730.
JankiAtulNawale,AnupamaSujatha,RatishPudup-
pully,VivekRaghavan,PratyushKumar,MiteshM Simran Khanuja, Sandipan Dandapat, Anirudh Srini-
Khapra,RajDabre,andAnoopKunchukuttan.2023. vasan, SunayanaSitaram, andMonojitChoudhury.
Indictrans2: Towardshigh-qualityandaccessiblema- 2020. GLUECoS: An evaluation benchmark for
chinetranslationmodelsforall22scheduledindian code-switchedNLP. InProceedingsofthe58thAn-
languages. TransactionsonMachineLearningRe- nualMeetingoftheAssociationforComputational
search. Linguistics,pages3575–3585,Online.Association
forComputationalLinguistics.
Jay Gala, Thanmay Jayakumar, Jaavid Aktar Hu-
sain,AswanthKumarM,MohammedSafiUrRah- AmanKumar,HimaniShrotriya,PrachiSahu,Amogh
man Khan, Diptesh Kanojia, Ratish Puduppully, Mishra, Raj Dabre, Ratish Puduppully, AnoopKunchukuttan,MiteshM.Khapra,andPratyushKu- Deville, Arka Dhar, David Dohan, Steve Dowl-
mar. 2022. IndicNLG benchmark: Multilingual ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
datasets for diverse NLG tasks in Indic languages. Tyna Eloundou, David Farhi, Liam Fedus, Niko
In Proceedings of the 2022 Conference on Empiri- Felix, Simón Posada Fishman, Juston Forte, Is-
calMethodsinNaturalLanguageProcessing,pages abella Fulford, Leo Gao, Elie Georges, Christian
5363–5394,AbuDhabi,UnitedArabEmirates.As- Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
sociationforComputationalLinguistics. Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, ScottGray, RyanGreene, JoshuaGross,
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey ShixiangShaneGu,YufeiGuo,ChrisHallacy,Jesse
Edunov, Marjan Ghazvininejad, Mike Lewis, and Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-
LukeZettlemoyer.2020. Multilingualdenoisingpre- hannesHeidecke,ChrisHesse,AlanHickey,Wade
training for neural machine translation. Transac- Hickey,PeterHoeschele,BrandonHoughton,Kenny
tionsoftheAssociationforComputationalLinguis- Hsu,ShengliHu,XinHu,JoostHuizinga,Shantanu
tics,8:726–742. Jain,ShawnJain,JoanneJang,AngelaJiang,Roger
Jiang,HaozhunJin,DennyJin,ShinoJomoto,Billie
Lokesh Madasu, Gopichand Kanumolu, Nirmal
Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
Surange,andManishShrivastava.2023. Mukhyansh:
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish
A headline generation dataset for Indic languages.
Keskar,TabarakKhan,LoganKilpatrick,JongWook
InProceedingsofthe37thPacificAsiaConference
Kim, ChristinaKim, YongjikKim, HendrikKirch-
onLanguage,InformationandComputation,pages
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
620–634,HongKong,China.AssociationforCom-
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
putationalLinguistics.
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
ArnavMhaske,HarshitKedia,SumanthDoddapaneni,
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
MiteshM.Khapra,PratyushKumar,RudraMurthy,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
andAnoopKunchukuttan.2023. Naamapadam: A
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
large-scalenamedentityannotateddataforIndiclan-
AnnaMakanju,KimMalfacini,SamManning,Todor
guages. InProceedingsofthe61stAnnualMeeting
Markov, Yaniv Markovski, Bianca Martin, Katie
oftheAssociationforComputationalLinguistics(Vol-
Mayer,AndrewMayne,BobMcGrew,ScottMayer
ume1: LongPapers),pages10441–10456,Toronto,
McKinney, Christine McLeavey, Paul McMillan,
Canada.AssociationforComputationalLinguistics.
Jake McNeil, David Medina, Aalok Mehta, Jacob
NLLB-Team,MartaR.Costa-jussà,JamesCross,Onur Menick, Luke Metz, Andrey Mishchenko, Pamela
Çelebi,MahaElbayad,KennethHeafield,KevinHef- Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Mossing,TongMu,MiraMurati,OlegMurk,David
JeanMaillard,AnnaSun,SkylerWang,Guillaume Mély,AshvinNair,ReiichiroNakano,RajeevNayak,
Wenzek, Al Youngblood, Bapi Akula, Loic Bar- ArvindNeelakantan,RichardNgo,HyeonwooNoh,
rault,GabrielMejiaGonzalez,PrangthipHansanti, LongOuyang,CullenO’Keefe,JakubPachocki,Alex
John Hoffman, Semarley Jarrett, Kaushik Ram Paino, Joe Palermo, Ashley Pantuliano, Giambat-
Sadagopan, Dirk Rowe, Shannon Spruit, Chau tistaParascandolo,JoelParish,EmyParparita,Alex
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Passos,MikhailPavlov,AndrewPeng,AdamPerel-
Bhosale, Sergey Edunov, Angela Fan, Cynthia man,FilipedeAvilaBelbutePeres,MichaelPetrov,
Gao,VedanujGoswami,FranciscoGuzmán,Philipp Henrique Ponde de Oliveira Pinto, Michael, Poko-
Koehn, AlexandreMourachko, ChristopheRopers, rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-
SafiyyahSaleem,HolgerSchwenk,andJeffWang. ell, Alethea Power, Boris Power, Elizabeth Proehl,
2022. No language left behind: Scaling human- RaulPuri,AlecRadford,JackRae,AdityaRamesh,
centeredmachinetranslation. CameronRaymond,FrancisReal,KendraRimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
OpenAI,:,JoshAchiam,StevenAdler,SandhiniAgar- der,MarioSaltarelli,TedSanders,ShibaniSanturkar,
wal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAle- GirishSastry,HeatherSchmidt,DavidSchnurr,John
man,DiogoAlmeida,JankoAltenschmidt,SamAlt- Schulman, Daniel Selsam, Kyla Sheppard, Toki
man,ShyamalAnadkat,RedAvila,IgorBabuschkin, Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
SuchirBalaji,ValerieBalcom,PaulBaltescu,Haim- Shyam,SzymonSidor,EricSigler,MaddieSimens,
ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, JordanSitkin,KatarinaSlama,IanSohl,Benjamin
Jake Berdine, Gabriel Bernadett-Shapiro, Christo- Sokolowsky, Yang Song, Natalie Staudacher, Fe-
pherBerner,LennyBogdonoff,OlegBoiko,Made- lipePetroskiSuch,NatalieSummers,IlyaSutskever,
laineBoyd,Anna-LuisaBrakman,GregBrockman, JieTang,NikolasTezak,MadeleineThompson,Phil
TimBrooks,MilesBrundage,KevinButton,Trevor Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-
Cai,RosieCampbell,AndrewCann,BrittanyCarey, ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
Chelsea Carlson, Rory Carmichael, Brooke Chan, lipeCerónUribe,AndreaVallone,ArunVijayvergiya,
CheChang,FotisChantzis,DerekChen,SullyChen, ChelseaVoss,CarrollWainwright,JustinJayWang,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess, AlvinWang,BenWang,JonathanWard,JasonWei,
ChesterCho,CaseyChu,HyungWonChung,Dave CJWeinmann,AkilaWelihinda,PeterWelinder,Ji-
Cummings, Jeremiah Currier, Yunxing Dai, Cory ayiWeng,LilianWeng,MattWiethoff,DaveWillner,
Decareaux,ThomasDegry,NoahDeutsch,Damien Clemens Winter, Samuel Wolrich, Hannah Wong,Lauren Workman, Sherwin Wu, Jeff Wu, Michael AssociationforComputationalLinguistics,10:145–
Wu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qim- 162.
ingYuan,WojciechZaremba,RowanZellers,Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao SebastianRuder,JonathanHClark,AlexanderGutkin,
Zheng,JuntangZhuang,WilliamZhuk,andBarret Mihir Kale, Min Ma, Massimo Nicosia, Shruti Ri-
Zoph.2023. Gpt-4technicalreport. jhwani, Parker Riley, Jean-Michel A Sarr, Xinyi
Wang,NitishGupta,etal.2023. Xtreme-up: Auser-
EdoardoMariaPonti,GoranGlavaš,OlgaMajewska, centricscarce-databenchmarkforunder-represented
QianchuLiu,IvanVulic´,andAnnaKorhonen.2020. languages. arXivpreprintarXiv:2305.11938.
XCOPA:Amultilingualdatasetforcausalcommon-
sense reasoning. In Proceedings of the 2020 Con- FredaShi,MiracSuzgun,MarkusFreitag,XuezhiWang,
ferenceonEmpiricalMethodsinNaturalLanguage SurajSrivats,SoroushVosoughi,HyungWonChung,
Processing(EMNLP),pages2362–2376,Online.As- Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.
sociationforComputationalLinguistics. Languagemodelsaremultilingualchain-of-thought
reasoners. arXivpreprintarXiv:2210.03057.
Maja Popovic´. 2015. chrF: character n-gram F-score
forautomaticMTevaluation. InProceedingsofthe Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier
TenthWorkshoponStatisticalMachineTranslation, Garcia, Jason Wei, Xuezhi Wang, Hyung Won
pages 392–395, Lisbon, Portugal. Association for Chung, Siamak Shakeri, Dara Bahri, Tal Schuster,
ComputationalLinguistics. HuaixiuStevenZheng,DennyZhou,NeilHoulsby,
andDonaldMetzler.2023. Ul2: Unifyinglanguage
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie learningparadigms.
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan- Gemma Team, Thomas Mesnard, Cassidy Hardin,
nah Young, Eliza Rutherford, Tom Hennigan, Ja- RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
cobMenick,AlbinCassirer,RichardPowell,George Laurent Sifre, Morgane Rivière, Mihir Sanjay
van den Driessche, Lisa Anne Hendricks, Mari- Kale,JulietteLove,PouyaTafti,LéonardHussenot,
bethRauh,Po-SenHuang,AmeliaGlaese,Johannes PierGiuseppeSessa,AakankshaChowdhery,Adam
Welbl,SumanthDathathri,SaffronHuang,Jonathan Roberts, Aditya Barua, Alex Botev, Alex Castro-
Uesato, John F. J. Mellor, Irina Higgins, Antonia Ros, Ambrose Slone, Amélie Héliou, Andrea Tac-
Creswell,NathanMcAleese,AmyWu,ErichElsen, chetti, Anna Bulanova, Antonia Paterson, Beth
SiddhantM.Jayakumar,ElenaBuchatskaya,David Tsai, Bobak Shahriari, Charline Le Lan, Christo-
Budden,EsmeSutherland,KarenSimonyan,Michela pherA.Choquette-Choo,ClémentCrepy,DanielCer,
Paganini, L. Sifre, Lena Martens, Xiang Lorraine Daphne Ippolito, David Reid, Elena Buchatskaya,
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Eric Ni, Eric Noland, Geng Yan, George Tucker,
Gribovskaya,DomenicDonato,AngelikiLazaridou, George-ChristianMuraru,GrigoryRozhdestvenskiy,
ArthurMensch,Jean-BaptisteLespiau,MariaTsim- HenrykMichalewski,IanTenney,IvanGrishchenko,
poukelli,N.K.Grigorev,DougFritz,ThibaultSotti- Jacob Austin, James Keeling, Jane Labanowski,
aux,MantasPajarskas,TobiasPohlen,ZhitaoGong, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-
DanielToyama,CypriendeMassond’Autume,Yujia nan,JeremyChen,JohanFerret,JustinChiu,Justin
Li,TayfunTerzi,VladimirMikulik,IgorBabuschkin, Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli-
Aidan Clark, Diego de Las Casas, Aurelia Guy, can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,
ChrisJones,JamesBradbury,MatthewG.Johnson, MachelReid,MaciejMikuła,MateoWirth,Michael
BlakeA.Hechtman,LauraWeidinger,IasonGabriel, Sharman, Nikolai Chinaev, Nithum Thain, Olivier
WilliamS.Isaac,EdwardLockhart,SimonOsindero, Bachem,OscarChang,OscarWahltinez,PaigeBai-
LauraRimell,ChrisDyer,OriolVinyals,KareemW. ley, Paul Michel, Petko Yotov, Rahma Chaabouni,
Ayoub, Jeff Stanway, L. L. Bennett, Demis Hass- RamonaComanescu,ReenaJana,RohanAnil,Ross
abis,KorayKavukcuoglu,andGeoffreyIrving.2021. McIlroy,RuiboLiu,RyanMullins,SamuelLSmith,
Scaling language models: Methods, analysis & in- SebastianBorgeaud,SertanGirgin,SholtoDouglas,
sightsfromtraininggopher. ArXiv,abs/2112.11446. ShreePandya,SiamakShakeri,SohamDe,TedKli-
menko, Tom Hennigan, Vlad Feinberg, Wojciech
PranavRajpurkar,JianZhang,KonstantinLopyrev,and Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao
PercyLiang.2016. Squad: 100,000+questionsfor Gong,TrisWarkentin,LudovicPeran,MinhGiang,
machinecomprehensionoftext. InConferenceon Clément Farabet, Oriol Vinyals, Jeff Dean, Koray
EmpiricalMethodsinNaturalLanguageProcessing. Kavukcuoglu,DemisHassabis,ZoubinGhahramani,
Douglas Eck, Joelle Barral, Fernando Pereira, Eli
Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Collins,ArmandJoulin,NoahFiedel,EvanSenter,
Bheemaraj, Mayank Jobanputra, Raghavan AK, AlekAndreev,andKathleenKenealy.2024. Gemma:
AjiteshSharma,SujitSahoo,HarshitaDiddee,Ma- Openmodelsbasedongeminiresearchandtechnol-
halakshmiJ,DivyanshuKakwani,NavneetKumar, ogy.
Aswin Pradeep, Srihari Nagaraj, Kumar Deepak,
VivekRaghavan,AnoopKunchukuttan,PratyushKu- HugoTouvron,ThibautLavril,GautierIzacard,Xavier
mar,andMiteshShantadeviKhapra.2022. Samanan- Martinet,Marie-AnneLachaux,TimothéeLacroix,
tar: The largest publicly available parallel corpora BaptisteRozière,NamanGoyal,EricHambro,Faisal
collectionfor11Indiclanguages. Transactionsofthe Azhar,AurelienRodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample.2023. Llama: Open
andefficientfoundationlanguagemodels.
AshokUrlana,PinzhenChen,ZhengZhao,ShayCohen,
ManishShrivastava,andBarryHaddow.2023. PMIn-
diaSum:Multilingualandcross-lingualheadlinesum-
marizationforlanguagesinIndia. InFindingsofthe
AssociationforComputationalLinguistics: EMNLP
2023,pages11606–11628,Singapore.Association
forComputationalLinguistics.
Ashok Urlana, Nirmal Surange, Pavan Baswani,
Priyanka Ravva, and Manish Shrivastava. 2022.
TeSum: Human-generated abstractive summariza-
tioncorpusforTelugu. InProceedingsoftheThir-
teenthLanguageResourcesandEvaluationConfer-
ence,pages5712–5722,Marseille,France.European
LanguageResourcesAssociation.
BigScience Workshop, Teven Le Scao, Angela Fan,
ChristopherAkiki,ElliePavlick,SuzanaIlic´,Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni,FrançoisYvon,etal.2022. Bloom: A176b-
parameteropen-accessmultilinguallanguagemodel.
arXivpreprintarXiv:2211.05100.
LintingXue,NoahConstant,AdamRoberts,MihirKale,
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
ColinRaffel.2021. mT5: Amassivelymultilingual
pre-trainedtext-to-texttransformer. InProceedings
ofthe2021ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,pages483–498,On-
line.AssociationforComputationalLinguistics.
Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulic´,
RoiReichart,AnnaKorhonen,andHinrichSchütze.
2021a. Acloserlookatfew-shotcrosslingualtrans-
fer: The choice of shots matters. In Proceedings
of the 59th Annual Meeting of the Association for
ComputationalLinguisticsandthe11thInternational
JointConferenceonNaturalLanguageProcessing
(Volume1: LongPapers),pages5751–5767,Online.
AssociationforComputationalLinguistics.
Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and
SameerSingh.2021b. Calibratebeforeuse: Improv-
ing few-shot performance of language models. In
InternationalConferenceonMachineLearning.Appendix
B.5 IndicSpecificLLMs
To the best of our knowledge, there is no LLM
A Resource-wiseLanguageClassification
that is pre-trained or fine-tuned on a broad set
of Indic languages, or a majority of the 29 lan-
SeeTable7forthelanguageresourcebasedclassi-
guageswestudyinthiswork. SomeIndicLLMs
ficationofthelanguagesstudiedinourwork. This
such as OpenHathi (or its instruction-tuned vari-
classification is done on the basis of multiple cri-
ant Airavata (Gala et al., 2024)), Tamil-LLama
teria, such as whether google translate supports
(Balachandran,2023),orKan-LLamaareprimar-
theparticularlanguage,theclassificationofIndic
ily trained/fine-tuned on a single Indic language
languagessuggestedbyDoddapanenietal.(2023)
(Hindi,Telugu,andKannada,respectively).
whichinturnusesJoshietal.(2020).
In this section we evaluate an Indic specific
LLM,Airavatawhichisprimarilypre-trainedand
B DetailedandAdditonalExperimental
fine-tunedonEnglishandHindilanguagedata. We
Results
evaluate the model on 6 Indic languages: Hindi
(hi),Bengali(bn),Punjabi(pa),Assamese(as),Ma-
Inthissection,wereport(a)one-shotresultsacross
nipuri(mni)andSantali(sat)(2eachfromhigher,
all models and tasks averaged over different lan-
mediumandlowresource),andEnglish. SeeTable
guage categories (B.1); (b) performance for dif-
8fortheresults.
ferenttasksandmodelsbyvaryingthenumberof
WefindthatAiravataperformsreasonablywell
in-context exemplars shown in few-shot prompt-
only on the Hindi language, and its performance
ing(B.2);(c)one-shotperformanceforthelargest
on other Indic languages is much worse (as ex-
model in each LLM family across all languages
pectedsinceitisonlytrainedforHindi). Airavata
(B.3); (d) fine-tuning performance for the largest
performssignificantlybetterthanLLaMAonthe
modelsacrossalllanguages(B.4).
Hindilanguage,acrosstasks,exceptforCSSUM-
IN summarization tasks. BLOOMZ outperforms
B.1 Resourcewiseone-shotresults
AiravataonmosttasksintheHindilanguage.
In Table 9 we show performance for all LLMs
consideredinthispaperontheone-shotprompting C Hyperparameters
methodacrossdifferentlanguagecategories.
ForFine-tuningexperimentsonmT5,wehyperpa-
rametersearchforbatchsizeintherange{16,32,
B.2 Performancebyvaryingnumberof
64}andlearningratein{5e-3,1e-3,5e-4,1e-4,5e-
in-contextexamples
5,1e-5}andselectthebesthyperparametersbased
In Tables 10, 11, 12, and 13 we show additional ontheToken-F1(forQuestionansweringdatasets)
resultsonhowperformancechangeswiththenum- scoreorChrF(forCROSSSUM-IN,FLORES-INex-
berofin-contextexamplesacrossvarioustasksin periments)scoreonthevalidationsetofthedataset.
INDICGENBENCH. For PaLM-2 fine-tuning experiments, we hyper-
parameter search for batch size in the range {16,
B.3 Languagewiseone-shotperformance 32,64}andlearningratein{5e-4,1e-4,5e-5,1e-
acrossmodels 5} and select the best hyperparameters based on
the Token-F1 score or ChrF score, as in the case
In Tables 15, 16, 17, 18 we show language wise
of mT5 experiments. We keep temperature=0 as
breakdown of performance for largest/best mod-
thedefaultsettingwhiledecodingusingPaLM-2
els–LLaMA-65B,GPT-4,PaLM-2-Linone-shot
and GPT family of models. For LLaMA models
setting,acrossall INDICGENBENCHtasks.
we increased the temperature to 0.1, since other-
wise, the performance of LLaMA were coming
B.4 Languagewisefine-tuningperformance
outtobecloseto0formanyofourdatasets. For
acrossmodels
eachdataset,wefixapromptforallmodelssince
InTables21,20,19weshowlanguagewisebreak- searchingforapromptwouldhaveledtoamuch
downofperformanceforlargestmT5andPaLM- largercomputecostfortrainingandevaluation. We
2 models that we fine-tune on CROSSSUM-IN, alsochoosein-contextexemplarsrandomlyfrom
XQUAD-IN, XORQA-IN-XX, XORQA-IN-EN. the training set (from the dev set in the case ofCode Language Script Family Resource
en English Latin Germanic high
bn Bengali Bengali Indo-European higher(Indicrelative)
gu Gujarati Gujarati Indo-European higher(Indicrelative)
hi Hindi Devanagari Indo-European higher(Indicrelative)
kn Kannada Kannada Dravidian higher(Indicrelative)
ml Malayalam Malayalam Dravidian higher(Indicrelative)
mr Marathi Devanagari Indo-European higher(Indicrelative)
ta Tamil Tamil Dravidian higher(Indicrelative)
te Telugu Telugu Dravidian higher(Indicrelative)
ur Urdu Arabic Indo-European higher(Indicrelative)
as Assamese Bengali Indo-European med
bho Bhojpuri Devanagari Indo-European med
ne Nepali Devanagari Indo-European med
or Odia Odia Indo-European med
pa Punjabi Gurumukhi Indo-European med
ps Pashto Arabic Indo-European med
sa Sanskrit Devanagari Indo-European med
awa Awadhi Devanagari Indo-European low
bgc Haryanvi Devanagari Indo-European low
bo Tibetan Tibetan Sino-Tibetan low
brx Bodo Devanagari Sino-Tibetan low
gbm Garhwali Devanagari Indo-European low
gom Konkani Devanagari Indo-European low
hne Chhattisgarhi Devanagari Indo-European low
hoj Rajasthani Devanagari Indo-European low
mai Maithili Devanagari Indo-European low
mni Manipuri Meithi Sino-Tibetan low
mup Malvi Devanagari Indo-European low
mwr Marwari Devanagari Indo-European low
sat Santali OlChiki Austroasiatic low
Table7: Resourcebasedlanguageclassificationintorelativelyhigher,mediumandlowresourceforthelanguages
studiedinourwork. Asmentionedpreviously,wenotethatthelanguagesclassifiedhigher,e.g.,HindiorBengali,
areinfactmid-lowWebresourcewhencomparedtoEnglishandothertrulyhighresourcelanguagesglobally. For
example,usingWikipediaasaproxyforlanguageresources,comparedto6.6M+WikipediaarticlesinEnglish,
thereareonly160KHindiWikipediaarticles. SeeAppendix§Aformoredetails.
CSSUM-IN FLORES-IN(enxx) XQUAD-IN XORQA-IN-XX XORQA-IN-EN
LanguageAiravata LLaMA BLOOMZ Airavata LLaMA BLOOMZ Airavata LLaMA BLOOMZ Airavata LLaMA BLOOMZ Airavata LLaMA BLOOMZ
en 31.6 19.1 13.9 - - - 75.7 49.2 86.8 69.5 56.0 72.2 69.5 59.7 68.7
hi 0.3 6.0 1.1 55.9 19.0 66.8 48.7 16.6 64.3 46.8 30.3 18.6 31.6 19.4 67.6
bn 0.0 2.0 0.2 8.5 9.6 73.9 6.5 2.5 57.9 0.2 3.8 0.3 33.8 12.6 70.0
pa 0.1 1.2 0.3 6.8 6.8 55.7 3.2 0.8 60.2 2.5 0.9 4.7 21.2 1.8 67.3
as 0.3 2.0 1.4 5.8 5.5 50.1 5.2 2.1 52.8 0.5 3.0 2.6 32.4 9.4 57.7
mni 0.2 2.3 0.2 6.1 6.7 6.5 - - - 1.4 2.9 0.3 22.6 5.4 21.5
sat 0.0 2.4 0.3 6.4 8.3 15.9 - - - 1.1 1.1 0.0 18.6 5.7 5.5
Table 8: One-Shot performance on INDICGENBENCH comparing language-specific Indic LLM Airavata with
othersimilarsizedopen-sourcemodels. Airavataisprimarilypre-trainedandfine-tunedforHindiandEnglish.
Asexpected, AiravataoutperformsotherLLMsonHindiwhereasit’sperformanceonotherIndiclanguagesis
significantlyworsethanbroad-coverageLLMslikeBLOOMZ.SeeAppendix§B.5formoredetails.XORQA-IN-EN XQUAD-IN CROSSSUM-IN XORQA-IN-XX FLORES-IN
Model High Medium Low High Medium High Medium Low High Medium Low High Medium Low
LLaMA-7B 9.9 7.6 12.2 4.7 1.1 3.6 3.6 3.7 8.1 3.4 8.8 11.2/20.4 9.6/20.2 12.8/23.2
LLaMA-13B 10.5 9.9 14.2 5.5 1.5 3.9 4.1 4.2 9.6 6.3 12.8 12.6/22.5 11.3/22.0 14.8/26.4
LLaMA-65B 16.4 14.0 17.3 8.8 1.9 4.4 4.6 4.7 17.7 13.5 17.1 18.2/31.5 15.4/30.0 19.5/35.0
Gemma-7B-PT 25.7 22.5 23.1 0.6 0.1 0.0 0.0 0.0 16.5 6.9 10.5 40.7/58.8 25.5/49.5 29.8/45.0
Gemma-7B-IT 29.5 23.9 21.9 38.8 24.8 13.9 11.5 10.0 18.9 8.3 12.2 17.6/33.7 15.0/26.1 21.3/27.7
BLOOM-7B 29.3 21.6 20.6 15.2 9.8 3.1 3.8 4.3 10.8 5.0 7.2 19.8/34.7 16.3/31.4 18.4/28.6
BLOOMZ-7B 64.7 45.8 39.5 55.5 48.1 1.5 1.7 0.6 10.8 2.8 6.2 67.7/59.1 39.4/50.2 22.9/40.0
GPT-3.5 39.0 34.9 33.4 36.2 23.9 17.6 16.5 15.3 24.2 20.0 20.6 32.9/52.9 27.3/48.2 27.6/43.8
GPT-4 49.4 50.0 41.8 56.1 54.6 19.4 17.9 16.3 25.8 21.6 22.6 36.2/59.6 30.7/55.2 29.9/50.5
PaLM-2-XXS 43.6 38.3 31.5 37.2 26.8 7.7 7.6 6.7 17.7 8.8 12.8 31.7/52.1 19.0/43.2 21.4/37.6
PaLM-2-XS 53.3 51.5 42.2 65.1 53.3 18.4 16.4 13.0 35.8 27.6 26.1 52.0/64.8 39.0/60.6 33.7/52.6
PaLM-2-S 60.1 61.4 53.6 69.9 57.3 22.4 19.8 15.1 36.6 30.3 28.6 54.7/66.8 42.5/63.5 36.4/57.0
PaLM-2-L 57.3 57.9 53.9 72.5 59.8 25.2 23.1 17.5 41.9 36.7 34.6 56.9/68.2 45.9/65.6 41.9/62.6
Table 9: One-Shot performance on INDICGENBENCH. Results are averaged over high, medium and low
resourcelanguageexamplespresentinthedataset. ForquestionansweringwemeasuretheF1/EMscorewhilefor
summarizationandtranslationwemeasureChrFscore. SeeAppendix§B.1formoredetails.
Shots Shots
Model(LLM) 0 1 Model(LLM) 0 1 2 3
LLaMA-7B 2.4 3.7 LLaMA-7B 4.7 10.4 12.5 14.1
LLaMA-13B 1.5 4.1 LLaMA-13B 2.7 12.1 15.4 17.0
LLaMA-65B 5.1 4.6 LLaMA-65B 8.2 16.3 18.8 19.8
PaLM-2-XXS 0.2 7.2 PaLM-2-XXS 31.0 36.8 41.6 44.5
PaLM-2-XS 1.0 15.5 PaLM-2-XS 15.6 47.8 57.8 61.7
PaLM-2-S 4.1 18.5 PaLM-2-S 40.6 57.4 63.4 66.3
PaLM-2-L 7.7 21.2 PaLM-2-L 31.4 55.9 66.3 70.3
Table 10: Few-shot results for PaLM-2 models on Table 12: Few-shot results for PaLM-2 models on
CROSSSUM-INdataset. ChrFscoresarereported. XORQA-IN-ENdataset. F1scoresarereported.
#ofin-contextexamples
Shots
Model(LLM) 0 1 5
Model(LLM) 0 1
LLaMA-7B 8.0/10.4 11.5/21.6 11.4/19.8
LLaMA-7B 4.3 3.8
LLaMA-13B 8.6/20.6 13.3/24.1 13.4/22.7
LLaMA-13B 4.7 4.5
LLaMA-65B 14.0/28.0 18.1/32.7 18.3/30.3
LLaMA-65B 7.2 7.1
PaLM-2-XXS 0.8/22.4 24.0/43.4 26.9/44.8
PaLM-2-XXS 19.0 34.6
PaLM-2-XS 20.1/53.3 40.7/58.3 42.3/58.8
PaLM-2-XS 38.7 62.2
PaLM-2-S 24.9/60.1 43.5/61.6 45.2/61.9
PaLM-2-S 44.7 66.7
PaLM-2-L 31.1/62.2 47.5/65.1 49.3/65.7
PaLM-2-L 50.6 69.3
Table 13: Few-shot results for LLaMA and PaLM-2
Table 11: Few-shot results for PaLM-2 models on
XQUAD-INdataset. F1scoresarereported.
modelsonFLORES-IN(bothenxx/xxendirection).
(asopposedtotakingaverageofmultipleruns)due
FLORES-IN). Thisdecisionisalimitationofthis
toprohibitivecostsoffine-tuningLLMsmultiple
work, since it has been shown that prompt and
times.
in-context exemplars can significantly impact an
LLMs performance (Zhao et al., 2021b,a). The
D ComputingInfra
promptsweuseareprovidedin§F.Searchingfor
betterpromptsandexemplarsisleftasfuturework. FormT5fine-tuningruns,weuseacombinationof
For all fine-tuning runs, we just perform one run 4to64TPU-V3,andforPaLM-2weuseupto256TPU-V4. Allexperimentstakeabout6hoursto1-2
daysoftimedependingupontheresourcesandsize
ofthemodelbeingtrained. WeuseNVIDIA-V100
16GB GPUs for evaluating open-source models
likeBLOOM,BLOOMZ,Airavataon INDICGEN-
BENCH.
E DatasetLicenses
Incompliancewithlicensesoftheoriginaldatasets,
we release our evaluation datasets under the fol-
lowinglicenses: (a) XQUAD-IN andFLORES-IN
undertheCC-BY-SA4.0license;(b) CROSSSUM-
IN under the CC BY-NC-SA 4.0 license; and (d)
XORQA-INundertheMITlicense.
F Prompts
The set of prompts used for evaluations on all
datasetsareshowninTable14.Summarize the following article in Haryanvi:
The White Garden, at Kensington Palace, was planted to mark 20 years since Princess Diana died in a car crash. The
Duchess of Cambridge joined the princes on the garden tour. A spokeswoman for Kensington Palace said: "The
engagement will allow the princes to pay tribute to the life and work of their mother." They met representatives from
the causes and charities supported by Diana, including the Royal Marsden and Great Ormond Street hospitals, the
National Aids Trust, Centrepoint youth homelessness charity and the Leprosy Mission. Members of the public have
been leaving tributes and flowers at the gates of the palace to mark the anniversary of Diana's death. The Princess of
Wales died on 31 August 1997 in Paris, when William, now the Duke of Cambridge, was 15 and his brother was 12. The
garden at their mother's former home has been inspired by memories of her life, style and image, such as her white
"Elvis" Catherine Walker dress. The White Garden, as it is known, follows a tradition first established at Sissinghurst
Castle in Kent, famous for its own white garden created in the 1930s. Their Royal Highnesses met gardener Sean
Harkin who designed the display and Graham Dillamore who knew the princess when he worked there some 30 years
ago. The garden has been open since spring and will continue into September with white roses, lilies, gladioli and
cosmos. It is the fourth London memorial created in tribute to Diana - the others are the Diana Memorial Playground
at Kensington Palace, the Diana Memorial Fountain in Hyde Park, and the Diana Memorial Walk at St James's Palace.
Model Output:  प्रिसं  व  लयम और  प्रिसं हैरी नै कें  सगं टन पैलेस मै एक नया गाडनर्ड खोलया सै जो उनकी माँ,  प्रिसं ेस डायना की या
(English translation: Prince William and Prince Harry have opened a new garden at Kensington Palace in memory of their mother, Princess Diana.)
(a)ModelhallucinatesthatPrinceWilliamandHarryopenedanewgarden.
Summarize the following article in Awadhi:
The surgery, which lasted seven hours, took place on 14 February at the Nair hospital in the western city of Mumbai.
But the procedure was not made public as doctors were not sure until now if it had been successful. "Now it's a matter
of recovery but he's out of danger", Dr Trimurti Nadkarni, chief of neurosurgery, told the BBC. Santlal Pal, a
shopkeeper from the northern state of Uttar Pradesh, had been living with the tumour for three years before his
surgery was carried out. Doctors said Mr Pal had lost his vision because of the tumour but hoped he would regain it as
he continued to recover from the operation. His wife told the Hindu newspaper that they had been told by doctors at
three different hospitals in Uttar Pradesh that the tumour was inoperable. "Such cases are critical", said Dr Nadkarni,
adding that Mr Pal needed 11 units of blood during the operation and after that, he was connected to a ventilator for a
few days after the surgery.
Model Output: एकु 31 साल के आदमी का नाक त े1.8  कलोग्राम का ट्यूमर न
(English translation: A 1.8 kg tumor was removed from the nose of a 31 year old man)
(b)Modelhallucinatestheageofthepersoninthearticle.
Figure4: ExamplepredictionsfromPaLM-2-LmodelontheCROSSSUM-INtaskhighlightingissueswithhalluci-
nationsinmodelpredictions.Translate the following in Awadhi:
Input: One can only wonder what the keyboard will become when something newer comes along.
Model output: कोई केवल इ सोच सकत है  क जब कुछ नया आवत है त कीबोड र्डका होई।
Ground Truth: इ  सफ़र्ड सो  च जाय सकत है  क जब कुछ नया आयी त कीबोड र्डका होई।
Translate the following in Chhatisgarhi:
Input: Scaffolds can be both virtual and real, in other words, a teacher is a form of scaffold but so is the little paperclip
man in Microsoft Office.
Model output: मचान आभासी अउ वास्त  वक दनु ो हो सकत हंवय, दसु र शब्दमन में, एक  शक्षक मचान के एक रूप हवय ले  कन
माइक्रोसॉफ्ट ऑ  फस में छोटा पेपरि  लप आदमी भी हवय।
Ground Truth: मचान आभासी अउ वास्त  वक दनू ों हो सकथे, दसू रे सब्द मन म, एकठन  स  छ्क मचान के एक रूप हवय ले  कन
माइक्रोसॉफ्ट ऑ  फस म छोटा पेपरि  लप आदमी घलो अइसा ही हवय।
(a)IntheseexamplesoftranslatingintoAwadhiandChhatisgarhi,bothlowresourcelanguagessimilartoahigherresource
languageHindi,wefindthatmanyHindiwordsandincorrectinflectionsforverbsareproduced.
Translate the following in Konkani:
Input:When asked for comment, Miller said, "Mike talks a lot during the hearing...I was getting ready so I wasn't really
hearing what he was saying."
Model output:  टप्पणी करपाक  वचारलें तन्े ना,  मलर म्हणाले, "माईक आयकुपाच्या वेळार खूब उलोवपाक लागता ... हांव तयार जातालो
म्हणून तो  कतें म्हणटालो हांव खरेंच आयकना  शल्लो"
Ground Truth:  टप्पणी मागली तन्े ना  मलरान म्हणलें, "मायक सुनावणी वेळार खूब उलयता... हांव तयार जातालो म्हणून तो  कतें
सांगता तें म्हाका खरेंच आयकंूक ना  शल्लें"
Translate the following in Assamese:
Input: Casablanca is one of the least interesting places to shop in all of Morocco.
Model output:  কচাব্লাংকা  হেছ মৰেক্কাৰ সকেলা ঠাইৰ িভতৰত আটাইতৈক কম আকষৰ্ষণীয় ঠাই।
Ground Truth: মৰেক্কাত কাছাবলাংকা  হেছ িকনা-কটা কৰাৰ বােব আটাইতৈক আকষৰ্ষণহীন ঠাই।
(b)1.InthefirstexampleoftranslationintoKonkani,themodeldoesnotunderstandthatthefirstinstanceof“hearing”refersto
acourthearingandthesecondinstancerefersto“listening”.Themodelincorrectlyoutputswordsrelatedtothe“listening”sense
inbothinstances.2.InthetranslationintoAssameseexample,themodeldoesnotproducetranslationforthecrucialinformation
“toshop”.
Figure5: ExamplepredictionsfromPaLM-2-LmodelontheFLORES-INtaskhighlightingissuesof(a)producing
wordsinthewrong(higher-resourced)languageorwordswiththewronginflection,and(b)outputtingtheincorrect
translationsforpolysemouswordsinEnglishormissingcrucialinformationfromthegeneratedtranslation.Dataset Prompt
CROSSSUM-IN I will first show a news article in English and then provide a
summaryofitinthe[TargetLanguageName]language.
Summarizethefollowingarticle: [Article]
Summary:
FLORES-IN,xxen Translatethefollowing:
ToEnglish: [SentenceinTargetLanguage]
Output:
FLORES-IN,enxx Translatethefollowing:
To[TargetLanguageName]: [SentenceinEnglish]
Output:
XQUAD-IN [PassageinTargetLanguage]
Q:[QuestioninTargetLanguage]
A:
XORQA-IN-XX Generateananswerin[TargetLanguageName]forthequestion
basedonthegivenpassage:
[PassageinEnglish]
Q:[QuestioninTargetLanguage]
A:
XORQA-IN-XX GenerateananswerinEnglishforthequestionbasedonthegiven
passage:
[PassageinEnglish]
Q:[QuestioninTargetLanguage]
A:
Table14: PrompttemplatesusedfordifferentdatasetsinINDICGENBENCH. n-shotexampleshavethesameformat
asthelast,testexamplegiventothemodel,whichcomesafterthen-shotexamples.Lang. Code CROSSSUM-IN
LLaMA-65B Gemma-7B-IT BLOOMZ-7B GPT-4 PaLM-2-L
en 26.1 24.8 18.6 30.3 41.1
bn 2.7 13.6 0.4 20.9 23.4
gu 2.1 10.3 1.0 16.5 19.2
hi 7.8 16.8 0.9 23.1 30.4
kn 2.9 13.6 0.9 15.1 25.4
ml 4.3 11.4 0.5 14.4 23.3
mr 4.1 13.6 0.6 22.4 25.4
ta 5.4 20.7 2.1 20.4 28.9
te 2.5 13.6 0.7 16.6 21.9
ur 7.5 13.0 6.4 24.9 28.9
as 2.7 12.5 1.8 17.3 24.3
bho 5.5 13.2 0.9 19.3 20.2
ne 7.4 16.8 2.0 23.2 29.1
or 2.5 4.3 0.3 10.7 23.4
pa 1.7 10.7 0.2 15.8 19.1
ps 7.0 9.2 6.3 19.9 25.2
sa 5.3 13.5 0.5 19.4 20.1
awa 5.1 12.8 0.6 18.2 19.4
bgc 5.3 12.5 0.7 17.9 20.2
bo 1.5 2.8 0.1 8.5 10.0
brx 4.9 2.8 0.4 15.5 11.0
gbm 5.8 11.4 2.0 16.2 16.2
gom 5.8 12.6 0.8 20.3 23.1
hne 5.5 13.4 0.9 19.2 22.0
hoj 5.2 12.5 0.3 17.7 17.7
mai 4.9 12.6 0.5 18.2 21.1
mni 3.3 4.4 0.2 13.2 15.6
mup 5.3 13.1 0.6 20.0 23.8
mwr 5.1 13.2 0.8 18.4 20.2
sat 2.7 5.5 0.2 8.5 7.4
Avg. 4.6 11.6 1.2 17.6 21.2
HighAvg. 4.4 13.9 1.5 19.4 25.2
MediumAvg. 4.6 11.5 1.7 17.9 23.1
LowAvg. 4.7 10.0 0.6 16.3 17.5
Table15: PerformancecomparisonofmodelsonCROSSSUM-INinone-shotsettingacrossallsupportedlanguages.Lang.Code FLORES-IN(enxx) FLORES-IN(xxen)
LLaMA-65B Gemma-7B-IT BLOOMZ-7B GPT-4 PaLM-2-L LLaMA-65B Gemma-7B-IT BLOOMZ-7B GPT-4 PaLM-2-L
bn 18.7 27.3 71.9 40.0 54.0 37.4 35.1 60.4 59.9 66.8
gu 11.3 3.3 68.1 31.3 56.1 22.1 29.1 60.9 61.2 70.9
hi 33.2 35.7 65.2 48.6 60.2 53.7 48.4 57.3 65.2 70.5
kn 11.1 15.6 66.8 29.9 58.0 20.6 27.6 59.1 57.6 65.7
ml 14.5 7.1 66.2 28.4 59.7 22.8 28.8 60.6 58.6 68.0
mr 22.7 22.3 68.6 39.2 53.1 38.2 31.7 58.8 59.8 68.5
ta 16.8 29.8 75.2 34.5 60.0 24.5 30.5 55.7 53.9 65.7
te 10.7 16.1 68.8 30.7 60.5 21.7 33.0 56.1 58.1 70.5
ur 24.6 1.3 58.5 43.5 50.6 42.8 38.8 62.8 62.0 67.2
awa 26.4 29.1 32.1 38.8 49.9 46.1 36.9 56.3 61.8 69.3
bgc 26.6 28.9 31.1 38.0 49.8 45.1 33.3 55.7 63.5 72.1
bo 6.6 4.5 13.7 17.0 40.9 16.9 17.0 16.1 27.7 50.1
brx 12.4 13.0 12.1 17.2 11.3 20.7 17.8 17.0 29.2 31.0
gbm 24.0 26.6 28.3 36.2 47.9 43.0 31.2 49.6 62.2 72.1
gom 17.8 19.0 17.3 27.5 39.0 29.7 23.3 28.3 48.2 63.2
hne 25.4 29.2 29.3 38.4 52.7 42.8 32.9 53.6 62.3 75.1
hoj 23.7 26.6 28.1 36.0 48.1 41.9 30.4 51.3 62.3 73.0
mai 21.4 26.0 22.8 36.2 54.1 41.8 32.7 57.5 60.9 73.3
mni 8.3 9.3 8.7 15.2 19.5 20.7 18.1 18.7 31.8 41.7
mup 27.0 28.3 31.7 39.9 51.9 45.1 34.9 49.2 62.3 73.0
mwr 26.4 29.7 33.9 39.2 52.2 44.6 34.6 51.3 64.2 74.5
sat 8.1 6.5 8.4 9.4 27.9 16.4 17.0 16.1 20.6 45.6
as 9.5 18.1 51.1 28.5 44.1 25.9 23.6 60.0 51.7 63.3
bho 23.4 25.1 24.9 33.5 42.9 40.9 32.0 51.8 55.3 61.7
ne 25.9 24.7 72.0 45.8 57.8 42.3 34.6 63.3 62.6 72.3
or 9.4 5.2 45.7 19.6 52.9 19.6 17.1 63.4 56.9 68.0
pa 9.7 2.6 57.1 30.7 51.8 22.1 28.4 60.8 62.5 70.1
ps 13.3 10.9 9.2 27.3 37.6 27.7 22.0 19.4 45.7 64.5
sa 16.9 18.6 15.7 29.3 34.3 31.4 25.1 32.5 51.5 59.4
Avg. 18.1 18.6 40.8 32.1 47.5 32.7 29.2 48.4 54.5 65.1
HighAvg. 18.2 17.6 67.7 36.2 56.9 31.5 33.7 59.1 59.6 68.2
MediumAvg. 15.4 15.0 39.4 30.7 45.9 30.0 26.1 50.2 55.2 65.6
LowAvg. 19.5 21.3 22.9 29.9 41.9 35.0 27.7 40.0 50.5 62.6
Table16: PerformancecomparisonofmodelsonFLORES-INinone-shotsettingacrossallsupportedlanguages.Lang.Code XORQA-IN-XX XORQA-IN-EN
LLaMA-65B Gemma-7B-IT BLOOMZ-7B GPT-4 PaLM-2-L LLaMA-65B Gemma-7B-IT BLOOMZ-7B GPT-4 PaLM-2-L
en 61.1 35.5 68.7 37.4 71.4 63.9 34.7 69.7 37.9 71.5
bn 15.8 9.4 0.6 16.5 40.4 16.9 25.7 68.4 46.2 55.6
gu 8.8 18.3 11.2 23.8 40.2 12.9 32.7 64.2 51.9 53.7
hi 43.5 29.6 16.5 38.3 56.5 23.1 31.1 66.1 41.6 58.8
kn 8.7 19.4 8.8 28.8 41.1 9.4 26.2 63.5 54.6 59.6
ml 13.9 26.7 19.6 31.1 56.7 16.9 30.7 66.4 55.3 54.8
mr 28.1 26.4 17.7 30.4 44.5 22.3 29.1 64.0 47.9 57.4
ta 16.0 17.5 10.7 25.4 40.2 19.0 26.8 64.0 50.1 57.7
te 4.0 15.3 11.4 18.5 25.6 6.0 29.3 67.6 52.0 56.3
ur 20.6 7.6 1.1 18.9 31.6 21.5 34.1 58.5 44.9 61.5
as 17.6 10.8 2.0 26.4 45.5 12.5 22.3 60.0 51.6 59.3
bho 19.9 9.9 2.0 23.8 32.1 21.4 26.7 55.2 46.9 59.4
or 4.0 1.6 6.2 22.1 35.9 7.7 14.8 48.0 54.6 52.6
pa 6.9 13.3 4.0 25.7 40.9 10.7 30.4 63.2 47.3 55.5
ps 10.8 3.4 0.5 12.4 25.7 12.4 23.9 3.5 44.7 62.9
sa 21.9 11.0 2.1 19.0 40.3 19.0 25.5 44.8 54.8 57.8
awa 26.3 13.1 7.8 28.5 41.3 23.3 25.3 58.1 50.3 58.4
bgc 18.5 16.4 10.2 20.1 28.1 23.1 25.6 55.7 43.8 58.4
bo 3.3 0.9 5.4 32.5 32.5 3.2 14.4 6.0 33.8 52.6
brx 5.3 12.7 0.9 15.5 20.0 12.1 15.6 7.6 35.9 39.1
gbm 13.7 14.9 9.5 18.5 35.4 21.6 24.9 50.7 46.1 60.2
gom 15.1 8.0 0.8 19.2 33.2 14.0 22.1 37.8 41.4 54.2
hne 29.7 22.7 12.6 32.0 43.4 22.5 24.9 56.4 43.9 54.8
hoj 31.2 25.3 17.3 36.3 53.8 21.5 23.9 51.5 48.8 62.6
mai 28.0 18.9 8.0 29.4 45.1 20.5 25.9 55.2 45.0 55.1
mni 8.9 4.0 0.4 13.2 23.6 9.7 14.9 17.7 36.2 47.5
mup 17.0 10.6 4.8 21.0 26.4 22.8 24.9 57.2 41.9 60.6
mwr 24.4 11.1 2.9 25.3 40.0 23.2 25.8 53.1 48.7 59.1
sat 1.3 0.3 0.0 2.6 26.6 6.7 16.7 6.2 27.5 38.6
Avg. 16.5 13.5 7.0 23.4 37.4 16.3 24.8 49.0 46.0 55.9
HighAvg. 17.7 18.9 10.8 25.8 41.9 16.4 29.5 64.7 49.4 57.3
MediumAvg. 13.5 8.3 2.8 21.6 36.7 14.0 23.9 45.8 50.0 57.9
LowAvg. 17.1 12.2 6.2 22.6 34.6 17.3 21.9 39.5 41.8 53.9
Table17: PerformancecomparisonofmodelsonXORQA-IN-XXandXORQA-IN-ENinone-shotsettingacross
allsupportedlanguages.XQUAD-IN
Lang. Code LLaMA-65B Gemma-7B-IT BLOOMZ-7B GPT-4 PaLM-2-L
en 62.0 45.7 86.7 64.8 83.7
bn 7.4 35.7 57.1 56.5 72.3
gu 0.5 40.6 57.0 53.9 72.1
hi 25.3 49.4 63.7 63.1 76.7
kn 0.4 41.1 52.0 55.1 74.4
ml 3.6 33.7 48.8 56.5 66.6
mr 14.7 33.8 58.5 57.3 76.9
ta 4.0 42.1 55.0 55.3 75.1
te 0.2 36.8 51.9 48.8 68.0
ur 22.9 35.9 55.9 58.4 70.2
as 3.1 28.4 48.8 53.0 66.6
or 0.7 5.9 34.0 51.1 52.0
pa 1.8 40.2 61.4 59.7 60.7
Avg. 7.1 35.3 53.7 55.7 69.3
HighAvg. 8.8 38.8 55.5 56.1 72.5
MediumAvg. 1.9 24.8 48.1 54.6 59.8
Table18: PerformancecomparisonofmodelsonXQUAD-INinone-shotsettingacrossallsupportedlanguages.Lang. Code CROSSSUM-IN
mT5XXL PaLM-2-XS
en 31.8 36.6
bn 25.4 28.8
gu 22.4 24.6
hi 26.2 30.5
kn 26.7 28.5
ml 26.4 26.7
Lang. Code XQUAD-IN mr 24.4 29.8
ta 31.6 32.3
mT5XXL PaLM-2-XS
te 25.8 26.9
en 78.1 64.4 ur 26.5 28.6
bn 53.2 47.7 as 23.9 25.8
gu 53.4 21.1 bho 22.0 24.1
hi 59.2 59.1 ne 28.3 30.5
kn 60.2 27.9 or 23.2 24.1
ml 52.6 30.1 ps 26.9 24.9
mr 60.9 51.6 pa 23.4 23.7
ta 62.8 45.1 sa 25.4 26.0
te 51.0 28.2 awa 20.4 21.6
ur 63.2 50.9 brx 19.7 12.6
as 45.0 35.1 hne 22.5 23.9
or 25.0 6.7 gbm 18.3 18.1
pa 51.3 8.8 bgc 22.0 23.0
gom 25.1 25.8
Avg. 53.2 34.4
mai 22.1 23.8
HighAvg. 57.4 40.2
mup 24.5 25.6
MediumAvg. 40.5 16.9
mni 15.6 12.0
mwr 21.0 22.2
Table19: Performancecomparisonoffine-tunedmod-
els on XQUAD-IN dataset across all supported lan- hoj 19.5 21.2
guages. sat 5.9 10.0
bo 7.0 5.1
Avg. 22.5 23.5
HighAvg. 26.2 28.5
MediumAvg. 24.7 25.6
LowAvg. 18.7 18.8
Table20: Performancecomparisonoffine-tunedmod-
elsonCROSSSUM-INdatasetacrossallsupportedlan-
guages.Lang. Code XORQA-IN-XX XORQA-IN-EN
mT5XXL PaLM-2-XS mT5XXL PaLM-2-XS
en 72.4 55.8 70.7 68.1
bn 15.4 16.3 71.9 68.6
gu 28.7 25.9 70.6 69.0
hi 38.3 45.8 70.6 68.5
kn 28.2 32.0 69.3 69.3
ml 38.2 44.0 71.1 70.5
mr 32.8 32.9 70.5 68.9
ta 27.2 33.8 69.0 69.8
te 21.5 24.1 70.5 70.4
ur 28.3 18.6 68.8 66.5
as 24.8 27.6 70.3 67.7
bho 20.6 22.0 67.9 66.0
or 23.9 28.6 69.1 65.3
pa 27.3 28.5 69.0 68.5
ps 22.4 13.1 68.3 64.2
sa 22.1 21.5 69.1 68.0
awa 24.2 27.7 68.0 65.2
bgc 21.1 21.5 69.1 62.5
bo 41.5 6.4 42.9 56.7
brx 14.9 7.9 38.8 30.2
gbm 15.6 18.2 65.2 62.5
gom 17.8 21.0 64.7 64.1
hne 27.9 32.4 68.2 64.7
hoj 30.8 33.3 66.2 62.1
mai 23.5 31.8 69.2 65.0
mni 16.8 8.9 48.6 37.4
mup 20.1 19.1 67.1 64.2
mwr 27.2 23.7 68.4 64.2
sat 2.7 3.3 32.1 36.5
Avg. 24.5 23.9 64.8 62.7
HighAvg. 28.8 30.4 70.3 69.1
MediumAvg. 23.6 23.6 68.9 66.6
LowAvg. 21.9 19.6 59.1 56.6
Table21: Performancecomparisonoffine-tunedmodelsonXORQA-IN-XXandXORQA-IN-ENdatasetacross
allsupportedlanguages.