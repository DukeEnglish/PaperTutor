EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevector
perclass
MAZDAMOAYERI∗,
UniversityofMaryland,USA
MICHAELRABBAT,
FAIRatMeta,Canada
MARKIBRAHIM†,
FAIRatMeta,USA
DIANEBOUCHACOURT†,
FAIRatMeta,Canada
Fig.1. (Left)Instancesofaclasscanappearinmanydiverseways,likethepearsabove.Usingonevector(theclassnameembedding)
torepresentthewholeclassresultsindisparateperformance,particularlyforatypicalinstances.(Middle)Toaddressthisissue,we
inferattributesandembedmultiplevectors,reducingdisparitiesandenhancinginterpretability.(Right)Ourmethodscalesbetter
thanpriorworksasweincludemoreattributes(Section6.1),enablingustoaccountforthemanywaysinwhichdiversitycanarise.
Vision-languagemodelsenableopen-worldclassificationofobjectswithouttheneedforanyretraining.Whilethiszero-shotparadigm
marksasignificantadvance,eventoday’sbestmodelsexhibitskewedperformancewhenobjectsaredissimilarfromtheirtypical
depiction.Realworldobjectssuchaspearsappearinavarietyofforms—fromdicedtowhole,onatableorinabowl—yetstandard
VLMclassifiersmapallinstancesofaclasstoasinglevectorbasedontheclasslabel.Wearguethattorepresentthisrichdiversitywithin
aclass,zero-shotclassificationshouldmovebeyondasinglevector.Weproposeamethodtoencodeandaccountfordiversitywithina
classusinginferredattributes,stillinthezero-shotsettingwithoutretraining.Wefindourmethodconsistentlyoutperformsstandard
zero-shotclassificationoveralargesuiteofdatasetsencompassinghierarchies,diverseobjectstates,andreal-worldgeographic
diversity,aswellfiner-graineddatasetswhereintra-classdiversitymaybelessprevalent.Importantly,ourmethodisinherently
interpretable,offeringfaithfulexplanationsforeachinferencetofacilitatemodeldebuggingandenhancetransparency.Wealso
findourmethodscalesefficientlytoalargenumberofattributestoaccountfordiversity—leadingtomoreaccuratepredictionsfor
atypicalinstances.Finally,wecharacterizeaprincipledtrade-offbetweenoverallandworstclassaccuracy,whichcanbetunedviaa
hyperparameterofourmethod.Wehopethisworkspursfurtherresearchintothepromiseofzero-shotclassificationbeyondasingle
classvectorforcapturingdiversityintheworld,andbuildingtransparentAIsystemswithoutcompromisingperformance.
∗WorkdonewhileinterningwithFAIRatMeta.
†Equalcontribution,co-principalinvestigators.
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforthird-party
componentsofthisworkmustbehonored.Forallotheruses,contacttheowner/author(s).
©2024Copyrightheldbytheowner/author(s).
ManuscriptsubmittedtoACM
1
4202
rpA
52
]VC.sc[
1v71761.4042:viXraFAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
CCSConcepts:•Computingmethodologies→Artificialintelligence;Machinelearning.
AdditionalKeyWordsandPhrases:Bias,Fairness,VisionLanguageModels(VLMs),Zero-shot,Classification
ACMReferenceFormat:
MazdaMoayeri,MichaelRabbat,MarkIbrahim,andDianeBouchacourt.2024.EmbracingDiversity:InterpretableZero-shotclassifica-
tionbeyondonevectorperclass.InThe2024ACMConferenceonFairness,Accountability,andTransparency(FAccT’24),June3–6,2024,
RiodeJaneiro,Brazil.ACM,NewYork,NY,USA,28pages.https://doi.org/10.1145/3630106.3659039
1 INTRODUCTION
Apivotaladvanceinmachinelearningistheadventoffoundationmodels.Asinglefoundationmodeltrainedon
large-scaledatacansupplantmultipletask-specificmodels.Vision-Languagemodels(VLMs)arepopularfoundation
modelscapableofencodingtextandimagesinthesamerepresentationspace.Comparedtostandardclassifierswhich
canonlyclassifyobjectsfromapredefinedlistofclasseswithexamples,VLMsarecapableofopen-world,zero-shot
classification—meaning,VLMscanclassifyanyobjectusingtextdescriptionswithoutanyadditionaltraining.This
zero-shotparadigmhasspurredthedevelopmentofmanyVLMs[15,24,36]withimpressiveclassificationperformance.
Despitetheirremarkableperformance,eventoday’sbestmodelsexhibitskewedperformanceforcertaingroups
ofimages.Forexample,[28]showmodelssuchasCLIPhaveexacerbatedthegapinperformancebetweenregions
suchasAfricaandEurope(aswellasthegapacrossincome-levels).Wefindsimilarbiasesarisewhenanobjectis
visuallydissimilarfromitstypicaldepiction.Forexample,Figure1(left)showsCLIP’s97.3%accuracyontypicalpears
dropsdramaticallywhenapearispeeled(45.2%)orpuréed(30.3%).Addressingsuchbiasesiscrucialtothereliabilityof
classifiersintherealworld,whereinstanceswithinaclasscanvarysignificantly.
Zero-shotclassifiers,likestandardmodels,useasinglevectorindeepembeddingspacetodescribeanentireclass.For
standardzero-shotclassification,avision-languagemodel(i)encodestheimagealongwith80hand-craftedpromptsper
classname(e.g.,“aphotoofapear”or“adrawingofapear”),(ii)averagesthe80embeddingsperclasstoobtainasingle
vector,(iii)predictstheclasswhosevectormaximizescosinesimilaritytotheimageembedding[24].Promptaveraging
encouragesallinstancesofaclasstobemappedtothesamevectorinthemodel’sembedding,inherentlylimitingthe
model’sabilitytoinfertheinnumerablediversitywithinaclass.Apearcanbediced,sliced,whole,inone’shand,orina
bowl.Ineachcase,theimageofthepearwouldbemarkedlydifferent,anditsembeddingmaynotalwaysbewellaligned
withthesinglevectorthatissupposedtorepresenttheentireclass.Thus,thereisanaturaltensionbetweentheone
vectorperclassparadigmandperformingconsistentlyacrossaclasswithhighdiversity,whichweempiricallyvalidate.
Whilemanystrategiesexisttomitigateperformancedisparitieswhenlabeled-dataisavailable,thesemethodsdo
nottransfertothedata-freesettingofzero-shotclassification.Fortunately,unlikestandardclassifiers,theopen-world
natureofVLMsenablesthemtorepresentanyattributeusingthetextencoder.VLMscanenrichthesingleper-class
vectorwithattributestomorefaithfullycapturethevarietywithwhichaclasscanappear,pinpointingwhetherapear
ispeeledorpuréed.Thus,wearguethatinsteadoflearningonevectorperclassthatisinvarianttodiversity,weshould
leveragetheopen-worldnatureofVLMstoexplicitlyaccountforthediversitywithinaclass(i.e.,viamultiplevectors).
Recentworksofferpromisingsignsthatzero-shotclassificationcanbeimprovedbyincorporatingattributesbeyond
theclassname,suchassubclasses[21]orvisualdescriptors[18,23].However,theformerislimitedtodatasetswith
hierarchicallabelsets,andthelatterrevertsbacktotheonevectorperclassparadigmviasimpleaveraging,limiting
thebenefitsofincorporatingmoreattributes(Section6.1).Importantly,diversitycomesinmanyformsthatgeneric
descriptorsorsubclassesalonemaynotadequatelycapture.
2EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.2. Wetestmodelsondatasetsthatprovidegroundtruthattributes(showninbold)annotatinghierarchies,diversestates,and
real-worldshifts(e.g.,Rojasetal.[29]labelstheincomelevelandcountryoforiginofeachimage,towardspromotingAImodelsthat
reducebias)withinaclass.Wefindthatstandardzero-shotaccuracy(‘BaseAcc.’above)dropssignificantlywhencertainattributes
arepresent,namelywhentheattributemanifestsinvisualdifferencesfromwhatthemodelconsiders‘typical’fortheclass.We
designourmethodtoimproveperformanceonthese‘atypical’instances.
Inthiswork,weproposeazero-shotmethodforenrichingclasseswithopen-endedattributestoboostzero-shot
classification.Ourmethodconsistsoftwosteps:(i)anattributeinferencestep,inwhichweusegenerativelanguage
modeling(aninherent,under-utilizedcapabilityofsomemodernVLMs)toenumeraterelevantattributesalongmany
axesofdiversity,and(ii)apredictionconsolidationstep,whereweflexiblyattendonlytosubpopulations(i.e.,instances
withinaclasssharinganattribute)thataremostrelevanttotheimage.Byenrichingandcarefullyconsolidating
attributestodescribediversitywithinaclass,ourmethodmorefaithfullyencodesatypicalinstances.Furthermore,by
introducinginterpretableintermediateoutputs(i.e.theinferredattributes),ourmethodaffordsgreatertransparency,as
eachinferencecomeswiththespecificlistoffine-grainedattributesusedtopredicttheclass,andattributeoverlaps
acrossclassescanhelpanticipateandarticularpotentialfailures,beforetheyhappen.
Inexperimentsoveralargesuiteofdatasetsencompassinghierarchies,diverseobjectstates,andreal-worldgeographic
diversity,weobserveourmethodmatchesandinmostcasesexceedstheperformanceofexistingmethods,showing
thattransparencycanbeachievedwithoutcompromisingonperformance.Ourmethodyieldsconsistentgainsona
seconddatasetsuitewithfiner-grainedclassesandnolabeleddiversity,showingthatourmethodstillworkswellwhen
intra-classdiversitymaybelesspresent.Encouragingly,wefindlargerimprovementsoccurringforthehardestclasses
andsubpopulations,whereatypicalinstancesareusuallyfound,resultinginreducedperformancedisparities.Compared
toexistingmethods,wefindthatourapproachcaneffectivelyscaletoamuchlargernumberofattributestocover
broaderaxesofdiversity,asshownintherightpanelofFigure1.Ourmethodalsooffersaprincipledtrade-offbetween
accuracyoverallvs.ontheworstclasses,allwithoutadditionaltraining.Insummary,we(i)identifyalimitationofthe
one-vector-per-classparadigminadequatelyrepresentingclasseswithdiversesubpopulations,(ii)proposetogobeyond
onevectorperclass,leveragingunder-utilizedabilitiesofVLMstoexplicitlyaccountforintra-classdiversity,and(iii)
extensivelyvalidatetheeffectivenessofourmethodtoperformzero-shotclassificationinbothamoretransparentand
accurateway,especiallyfordiversesubpopulationsthatareoftenoverlooked.
2 REVIEWOFLITERATURE
Despiteimpressiveoverallaccuracy,modernclassifiersstillsufferfrombiases.Thatis,theyunder-performonsomeparts
ofthedata,oftenduetospuriouscorrelationsordataimbalancesinthetrainingset.Thesebiasescanresultinsignificant
negativereal-worldimpact.Forexample,BuolamwiniandGebru[3]exposedsignificantbiasalongdemographiclines
3FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
forfacialrecognitionsystems,andmorerecently,Richardsetal.[28]demonstratedthatdespitesteadyprogresson
typicalbenchmarks,today’sbestmodelsstillgeneralizepoorlytoimagesfromlower-incomehouseholdsandcertain
geographicregions.Namely,VLM-basedzero-shotclassifierswereshowntohaveevenlargerperformancedisparities
acrossgeographicandeconomicshiftsthantheirsupervisedcounterparts.
However,thepromiseofopen-worldzero-shotclassificationrightfullydrawsmuchattentiontoVLMs,whichoperate
bymappingimagesandtexttoasharedlatentspace.CLIP[24],aseminalVLM,achievesthisviajointcontrastive
trainingofimageandtextencoderson400millionimage-captionpairs.RecentmodelssuchasBLIP-2[15]bootstrapthe
trainingofmorepowerfulVLMsbytakinglargerpretrainedvisionandlanguagebackbonesandfusingtheiroutputsto
asinglespace,whichinturncanevenbeusedtogeneratetext;thatis,somemodernVLMscontainafullyfunctional
LLMwith(oftenunder-utilized)generativeabilities.Toperformzero-shotclassificationwithVLMs,onecomputes
theclassthathasthehighestcosinesimilaritybetweenatestimage’sembeddingandtheembeddingofaclassname,
oftenaveragedovermany(80forCLIP)handcraftedprompttemplates.WhilemanyeffortshaveimprovedVLM-based
classificationviaprompt-tuning[8,12,17,19,38–40],nearlyallrequiresomelabeleddata.Otherworksfocusmore
closelyonthetaskofdebiasingVLM-basedclassifiers[6,14,32,37],thoughtheytooutilizelabeleddata,placingthem
out-of-scopeofthetruezero-shotsetting.
Comparedtopreviousclassifiers,thekeynoveltyofVLMsistheirabilitytoencodeanytext.However,standard
zero-shotclassifiersonlyembedclassnames,eitheraloneoraveragedoverprompts.Weproposetoleveragetheopen-
vocabularycapabilitiesofVLMstoimprovecoverageofintra-classdiversitybyembeddingmorethanjusttheclass
name.OneeffortalongtheselinesisPerceptionCLIP[1],whichinferscontextualattributesperimageasgenerative
factorsanddoesclassinferenceconditionedonthem.OtherworksutilizeLLM-generatedclassdescriptors,towards
creatingaconcept-bottleneck[35]orrationalesforinference[9],thoughthesemethodsusedatatotrainalinearlayer
atopdescriptorsimilarities.DCLIP[18]showincludingdescriptorscanalsoimproveperformanceinthezero-shot
setting,andPrattetal.[23]extendthegainsusingadditionalhandcraftedqueries.WaffleCLIP[30]showsthatappending
randomcharactersorwordscanachievesimilarperformancetodescriptor-basedmethodslikeDCLIP,withoutthe
needforanexternallanguagemodel.Importantly,althoughtheseworksobtainmorethanonevectorperclass,they
ultimatelyaverageoverthem.Thus,decisionboundariesremainlinearandbiasesmaylinger,asatypicalinstances
arestillsuboptimallyuncovered(seeSections4.2and6.2).Incontrast,likeus,CHiLS[21]introducesanon-linearity
inthreesteps:they(i)definesubclasseswithgroundtruthlabelhierarchiesorbyqueryingGPT-3,(ii)dozero-shot
classificationonthisextendedsetofclasses(subclasses)andoriginalclasses,(iii)reweightthestandardzero-shotscore
foreachclasswiththemaxscorefromstep(ii)oversubclasseswithintheclass.However,CHiLSisdesignedspecifically
forhierarchicallabelsets,whichlimitsthetypesofdiversityitcancapture(seeSection6.1).
3 MOTIVATION
Wehypothesizethatthestandardone-vector-per-classparadigmposesatensionforhighlydiverseclasses.Weinves-
tigatethisbymeasuringclassificationperformanceasafunctionofclassdiversity.Indeed,wefindclasseswithhigher
diversitysufferworseperformanceundertheone-vector-per-classclassificationparadigm.Thenweillustratehow
newfoundopen-vocabularycapabilitiesofVLMscanenrichthesingleclassvectortoencompassdiverseinstances
withoutadditionaltraining.Thatis,weshowthatincorporatingattributeinformationcansubstantiallyimproveVLM
recognitionofatypicalsubpopulations.
4EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.3. Theaverageprecision(AP)ofaclassnameembeddingisoftenmuchlowerthantheaverageprecisionofasubpopulation(i.e.
classnamewithattribute)embedding.SubpopulationsthatseelargeincreasesinAPbyincludingtheattributetendtobeatypical.We
designourmethodtoimproveaccuracyonthesediversesubpopulations,byinferringandexplicitlyaccountingforthem.
3.1 Asinglevectorinadequatelyrepresentsdiverseclasses
AstandardVLMclassifierismosteffectivewhenitalignsallinstancesofaclasstotheirclassvector(andawayfrom
vectorsforotherclasses).Intuitively,aligninginstanceswithhighdiversityischallengingastheirimageembeddings
aremoredispersed—andparticularlytoughforfixedopen-vocabularyVLMsthatdonotbenefitfromknowingthe
specificclassesofinterestduringtheirpre-training(seeAppendixG.1).WeseeinFigure2forexamplethelesstypical
Arcticfoxisfarhardertorecognizethanatypicalfox(52.0%versus84.5%accuracy).Weobservedsimilardropsin
accuracyforadeflatedballoonversusaregularballoonandanunpavedstreetversusapavedone.Tosystematically
quantifythistension,bothforVLMsandfortheonevectorperclassparadigmgenerally,weexamineclassaccuracies
onImageNet[7]relativetothediversityofeachclassacrossseveralmodelswithvaryinglevelsofsupervision.To
proxydiversity,wemeasurethevarianceofimageembeddingswithinaclass.Inallcases,weobserveastrongnegative
correlationbetweenclass-wiseaccuracyanddiversity(seeTable4anddetailsinAppendixC).Thatis,classeswith
higherdiversityhaveloweraccuracyintheonevectorperclassparadigm.
3.2 Apathforward:VLMscanrecognizediversitywithrelevantattributes
AlthoughstandardVLMsusesolelyclassnameinzero-shotclassification,theirsharedembeddingspaceallowstoencode
relationstoanyothertext.Inturnweask:cantheopen-vocabularyencoderofVLMsbettersituatediverseclassesgiven
relevantattributes?Specifically,weassesswhetherenrichingclasseswithattributescanimprovezero-shotclassification
onasuiteofdatasetswithground-truthattributesperclass(detailsinAppendixB).Weformasubpopulationby
takinginstanceswithinaclassthatshareanattribute.Foreachsubpopulation,wecomputethesimilarityofimage
embeddingswiththetextembeddingof(i)theclassnameand(ii)theclassnamewiththecorrespondingattribute,using
CLIPViT-B/16.Wethenmeasuretheaverageprecisionofthetwosimilarityscoresfordistinguishinginstanceswithin
thesubpopulationfrominstancesoutsideoftheclass.Wefind,asshowninFigure3,thatforthevastmajorityofcases,
incorporatingattributesleadstomorepreciserecognition,andoftenbylargemargins:addingmoltentocakeimproves
averageprecisionbyover40points.Uponinspection,thehighestgainsinaverageprecisiontendtooccurforatypical
subpopulations(seeAppendixB).Thus,VLMscanrecognizeinstancesinaclassevenwhentheyareatypical,butthis
abilityisrestrictedundertheonevectorperclassparadigm.
5FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
4 METHOD
WenowproposeamethodtobetterutilizetheabilityofVLMstorecognizediversesubpopulations.Ourmethodconsists
of attributeinferenceandpredictionconsolidation.First,wequeryalargelanguagemodel(LLM)fordiverse
per-classattributesthatspanmany(oftenoverlapping)subpopulations.Then,aftercomputingthesimilarityofan
imagetoeachsubpopulation,wenon-linearlyconsolidatethesesimilaritiestoobtainonescoreperclass.Weelaborate
onthesetwostepsbelow.
4.1 AttributeInferenceAlongManyAxesofDiversity
Tobettercoverthediversesubpopulationsthatmayexistwithinaclass,weincorporateattributeinformation.However,
diversitycancomeinmanyforms.Thatis,thewayinwhichtwoinstancesofaclassdiffercanitselfvary.Consider
theexamplesinFigure2.TheArcticfoxcaseshowshowaclasscancontaindistinctfiner-grainedcategories.Ina
relatedmanner,thestateorconditioninwhichtheclassinstanceisincanalsosubstantiallychangeitsappearance:
aballoonlooksmuchdifferentwhenitisdeflated.Further,thereexistgenericattributesthatcanleadtosubstantial
visualdifferencesregardlessoftheclass,suchastheregionorincomelevelofthecountrywhereanimageistaken,
exemplifiedbythetwoStreetViewimages.Thus,tocapturethemanywaysinwhichdiversitycanarise,weemploy
multipledistinctqueries,incontrasttopriorwork.Namely,weinfer:
• Classspecificattributes,suchasthepossiblestatesofanobject(e.g.,dicedorslicedforpear).Wealsoobtain
descriptionsforanddifferentkindsofeachclass,asinDCLIPandCHiLSrespectively.
• Classadjacentattributes,likeco-occurringobjectsorbackgrounds,togetusefulcontext.
• Classagnosticattributesthatdescribehowobjectsvaryingeneral.Forexample,towardsimprovinggeographic
fairness,welistpotentialchoicesfortheincome-level,regionandcountryoforiginoftheimage.Wealso
introduceanoveltwo-stepLLMquery,wherewefirstasktheLLMtolistgenericaxesofdiversity,andthen
haveitpopulatethoseaxes.Wenamethisauto-globalasitautomaticallygeneratesmanyglobalattributes.
AppendixD.2containstheexactLLMpromptsandexampleinferredattributesforeachqueryabove.
4.2 NonlinearPredictionConsolidation
Enumeratingattributesalongvariousaxesofvariationresultsindescriptionsofmanydiversesubpopulationsperclass.
SinceVLMshaveopen-vocabularytextencoders,wecandirectlyembedthesesubpopulationdescriptions,inaddition
totheclassname.Givenatestimage,wecomputesimilaritiestoeachoftheseembeddings.Wethenmustconsolidate
themtoobtainasinglescoreperclass.
Figure 4 illustrates the simple case of fox vs wolf classification, where solid/dotted lines correspond to class-
name/subpopulationembeddingsonthehypersphere(shownherein2D).Theleft-mostpanelshowsexamplesfrom
thetwoclassesnearwheretheirimageembeddingswouldlie.Textembeddingsforthesubpopulations(dottedlines)
areclosetocorrespondingimageembeddings,asVLMsarecapableofrecognizingevendiversesubpopulations(see
Section3.2).Standardzero-shotinferencemapsatest-timeimagetotheclassofthenearestclassnametextembedding.
Sincethereisonlyonevectorperclass(theclassname-basedembedding),thedecisionboundaryislinear,asshown
inthemiddlepanel.Theedgeofthehypersphereiscolored(orangeforwolf,blueforfox)toindicatethepredicted
classforanimageembeddingatthatlocation.Notably,theArcticfoxismisclassifiedaswolf,asitsappearancemore
closelyresemblesatypicalwolfthanatypicalfox,andso,theembeddingsofArcticfoximagesfallclosertothetext
embeddingof“wolf”(andvice-versafortheredwolf).MethodslikeDCLIPandWaffleCLIPembedmorethanjustthe
6EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
One vector per class Non-linear decision by
linear decision embracing diversity
Arctic fox Arctic fox✗ Arctic fox✓
Red wolf “Wolf” Red wolf✗ Red wolf✓
z
“Fox”
Fig.4. AnArcticfoxcanmorecloselyresembleatypicalwolfthanatypicalfox.Standardzero-shotclassificationusingonevector
perclass(theclassnameembedding)isillsuitedforthiscase.Weaddressthisissuebynonlinearlyconsolidatingsimilaritiesto
multiplevectorsperclassthatexplicitlyencodethediversesubpopulationswithintheclass.Seesection4.2forfullexplanation.
classname,buttheyconsolidatesimilaritiesviaaveraging,againresultinginalineardecisionboundary.Evenifatypical
subpopulationsareincludedatfirst,averagingcannarrowtheinitialdiversecoverage,asmostembeddingsforaclass
maybetterdescribeatypicalinstance.
Incontrast,weproposethefollowingnonlinear consolidation:wecomputethesinglescoreperclassforagiven
testimageastheaverageofthesimilaritiesoftheimageembeddingtoonlythe𝑘closestsubpopulationsembeddings
fortheclass,where𝑘 istypicallysmall(weuse𝑘 = 16).Thisway,animagecanhaveahighclassscoreevenifit
isonlysimilartoasmallsubsetofsubpopulations,asisthecaseforatypicalinstances.Thus,theArcticfoxandred
wolfcanbecorrectlyclassifieddespitebeingfarfromtheclassnameandmostsubpopulationembeddingsfortheir
respectiveclasses,asshownontherightpanelofFigure4,whereweuse𝑘 =1forsimplicity(i.e.imagesaremappedto
theclassoftheclosestdottedorsolidline,leadingtoanon-linearboundary).Weshedinsightontheeffectofvarying
thehyperparameter𝑘inSection6.2,revealingatunableaccuracy-fairnesstrade-off.
5 ANALYSIS
Wenowempiricallyvalidateourmethod’seffectivenessandenhancedinterpretabilityovertwodatasetsuites.Our
methodperformsonparwith(andusuallysurpasses)existingapproachesinoverallaccuracy.Notably,weseelarger
gainsforthehardestclassesandsubpopulations,whicharelikelymorediverseandatypicalrespectively(precisely
thesamplesonwhichourmethodisintendedtoimproveperformance).Furthermore,whilematchingorexceedsthe
performanceofexisting,weofferuniqueinterpretabilitybenefits,suchasfine-grainedandfaithfulexplanations,as
wellasthepotentialforerroranticipation;wedetailthesebelow.
5.1 ConsistentGainsAcrossDiverseDatasets,ParticularlyfortheHardestClassesandSubpopulations
5.1.1 BaselinesandMetrics. Wemeasureperformanceofzero-shotclassifiersusingthepopularCLIPViT-B/16and
BLIP-2VLMs[15,24].Toinferattributes,weutilizetheopensourceVicuna-13b-v1.5languagemodel[5],whichnotably
isalreadycontainedintheBLIP-2modelweuse.Wereportaccuracyoverallaswellasaveragedovertheworst20%
ofclassesandsubpopulations.Notethatweonlyusegroundtruthattributeswhencomputingmetrics;ourmethod
exclusivelyusesattributesinferredviathequerieslistedinSection4.1.Wealsocomputethelowestsubpopulation
accuracyperclassandaveragethat,sotoobtainthemetricdenotedas‘AvgWorstSubpop’.Forthereal-worldshifts,we
7FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
Accuracy AvgWorst Worst20%of Worst20%of
DatasetType Subpop Classes Subpops
States Vanilla 66.71 40.66 35.46 21.73
DCLIP 63.65 39.41 34.26 20.98
Waffle 66.68 40.71 35.49 22.05
CHiLS 66.56 40.41 36.16 22.45
Ours 67.92 41.53 38.16 23.64
Hierarchical Vanilla 78.15 48.36 50.72 35.89
DCLIP 77.80 48.48 51.05 34.36
Waffle 78.52 49.42 49.78 35.22
CHiLS 79.44 52.65 51.80 38.44
Ours 79.50 51.23 52.59 38.57
Table1. Zero-shotclassificationondatasetswithknownvariationtypesforCLIPwithaViT-B/16encoder.Statesaveragesresults
overthetwocategorizationsofMITStatesdata,whileHierarchicalaveragesresultsoverfourBreedsdatasets.Weobservesimilar
resultsforBLIP-2(Table8).
alsoreportworstregionandworstincomegroupaccuracy.Ourbaselinesinclude:standardzero-shot(onlyonevector
perclass,correspondingtotheclassnameembedding)whichwecallVanilla,DCLIP(averagesoverclassdescriptors),
WaffleCLIP(averagesoverrandomdescriptorssampledovertentrials),andCHiLS(reweightsstandardzero-shotclass
scorewithmaxprobabilityofdifferentkindsoftheclass).Notably,weaveragealltextembeddingsoverthe80prompts
craftedforCLIP,sotoreportbestpossiblebaselineresults.
5.1.2 Datasets. Wecurateasuiteofeightattributed(sotohavegroundtruthsubpopulations)datasetsspanningdifferent
axesofdiversity.WeusethefourBreedsdatasets[31]fortheirhierarchicallabelsets,asusedintheCHiLSpaper;in
fact,theBreedsdatasetsweretheoneswhereCHiLSwasmosteffective.Next,wedevisetwoclassificationtasks(coarse
andfinegrained)fromtheMITStatesdataset[13]totrackperformanceoverlabeledstates(e.g.,slicedordicedfor
pear).Importantly,wealsoincludethedatasetsDollarstreet[29]andGeoDE[26],whichcontainimagesfromvaried
geographicregionsandincomelevels.Asthediversityinthesedatasetsisoccursnaturally,theycanencompassmany
axesofvariation,asopposedtoourotherdatasetsthatonlyvaryingalongknownaxes,likeobjectstateorkind.
Wealsoincorporateasecondsuiteofthefollowing9datasetswithoutattributes:ImageNet[7],ImageNetvariants(v2,
-R,-A,-Sketch)[10,11,27,34],Food-101[2],Flowers-102[20],FGVC-Aircraft[16],andOxfordPets[22].Thesedatasets
areallsomewhatfine-grained1,andassuch,arelesslikelytohaveintra-classdiversitythanouroriginaldatasets.Thus,
thisdatasetsuiteprovidesinsightonifembracingdiversityisonlyeffectivewhendiversityistobeexpected.
Wenotethatwestrivedtominimallyfitourmethodtotheevaluationsuite.Thatis,wedonotoptimizeourquery
settomaximizeperformanceonthedatasetsweselected,whichcanbechallengingforzero-shotclassificationmethods.
Onespecificmeasurewetooktowardthisendwasfixingourmethodcompletelybeforeevaluatingontheseconddataset
suite.Thus,theseconddatasetsuiteservesasaheld-outchallengeset,intendedtotestthegeneralizabilityourmethod
tosettingswhereintra-classdiversitymaynotbepresent.SeeAppendixD.1forcompletedetailsonourdatasetsuite.
1ImageNet(anditsvariants)contains120outof1000categoriesdedicatedonlytovariousdogbreeds;Flowers-102hasmanyhighlysimilarclasses;
OxfordPetsonlyhasdifferentbreedsofdogsandcats;theFGinFGVC-Aircraftstandsforfine-grained.
8EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
DollarStreet Accuracy Worst Worst AvgWorst Worst20%of Worst20%of
Method Region Income Subpop Classes Subpops
Vanilla 51.51 42.43 34.76 37.60 18.33 11.01
DCLIP 49.78 41.08 32.91 36.37 19.07 11.19
Waffle 51.37 42.71 34.97 37.69 18.12 10.74
CHiLS 51.68 42.20 33.90 37.60 20.51 12.72
Ours 52.70 44.04 37.21 40.31 20.88 15.05
GeoDE
Vanilla 90.15 86.63 - 82.57 72.24 69.95
DCLIP 91.31 88.14 - 84.21 74.44 71.90
Waffle 91.59 89.06 - 85.44 75.85 74.37
CHiLS 90.96 87.90 - 84.48 73.27 71.64
Ours 91.75 89.12 - 85.40 76.13 74.64
Table2. Zero-shotclassificationperformanceongeographicallydiverseimagesfromDollarStreetandGeoDEusingCLIPwitha
ViT-B/16encoder.WeobservesimilarresultsforBLIP-2(Table9).
ImageNet v2 -A -R Sketch Food Flowers Aircraft Pets Average
OverallAccuracy
Vanilla 68.48 61.98 30.16 59.24 48.37 88.35 66.09 31.26 92.72 60.74
DClip 68.85 62.37 31.35 60.04 48.54 88.05 70.69 32.67 92.23 61.64
Waffle 68.44 62.15 31.09 61.17 48.58 88.09 66.87 31.05 92.03 61.05
CHiLS 0.11 0.10 0.00 0.00 0.11 88.59 67.49 34.20 92.12 31.41
Ours 69.94 63.32 32.19 61.49 49.38 89.06 70.69 34.62 92.26 62.55
AccuracyonWorst20%ofClasses
Vanilla 34.42 25.75 7.47 29.27 9.19 73.58 2.08 0.00 78.84 28.96
DClip 34.58 25.65 6.24 28.90 8.99 72.92 3.44 0.00 77.50 28.69
Waffle 32.62 23.00 5.79 28.18 8.93 73.36 2.14 0.00 75.65 27.74
CHiLS 0.00 0.00 0.00 0.00 0.00 75.36 2.09 0.00 76.92 17.15
Ours 37.30 27.60 7.98 33.74 9.33 76.18 4.40 0.25 77.40 30.46
Table3. Zero-shotclassificationperformanceonfiner-grainedheld-outdatasetswithoutattributes,usingCLIPwithaViT-B/16
encoder.WeobservesimilarresultsforBLIP-2(Table10).WediscussreasonsforthefailureofCHiLSonImageNet-scaletasksin
AppendixF.Ourmethodeffectivelygeneralizestonewsettingswithouttuningthesetofqueriesforattributeinference.
5.1.3 Results. Table1showsresultsfordatasetswithdiversityalonghierarchicalandstatesaxes,andtable2shows
resultsforgeographicdiversity.Ourmethodconsistentlymatches(andevenimproves)accuracyofexistingmethods,
evenoverCHiLSinthehierarchicalsettingitwasspecificallydesignedfor.Notably,CHiLSbecomeslesseffectivefor
otherdatasets,whileourmethodremainsstrong.Weobservelargergainsforworstclassandsubpopulationmetrics,
especiallyoverbaselinesthatconsolidateviaaveraging(Vanilla,DCLIP,Waffle),supportingtheclaimthatourmethod
improvescoverageofthemostatypicalinstances,andthatmovingbeyondtheonevectorperclassparadigmhelps
inthisregard.Forexample,comparedtobaselinesthatconsolidateviaaveragingtoobtainonevectorperclass,our
9FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
methodimprovesaccuracyfortheworstclassesandsubpopulationsby2−3%inmostcases.ForDollarstreet,these
gainsmanifestina9%averagerelativegainoverbaselinesfortheaccuracyoverworstincomegroupmetric(and
anevenlargergainfortheworst20%ofsubpopulations),showingthatourmethodologycanfacilitateprogresson
real-worldfairnessindicators.
Turningourattentiontotheheld-outchallengedatasets,Table3showsourmethodcangeneralizeeffectivelyto
finer-grainedclassificationtaskswhereintra-classdiversityisnotexplicitlyknowntobepresent.Ourmethodimproves
Fig.5. Instanceswhereourmethodcorrectsmistakesofthestandardapproach.Theattributesusedininferencealsoserveasfaithful
fine-grainedexplanations.Notably,thesesamplesareatypical,suggestingthatinspectingsampleswhereourmethodandstandard
classificationdisagreecanenableautomaticsurfacingofatypicalcases,towardsbetterunderstandingthetaskathand.
10EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
accuracyonthehardestclassesbyanaverageof1.5%overtheclosestbaseline.Similarly,ourmethodexceedsall
baselinesbyabout1%inoverallaccuracyinnearlyallcases,suggestingthatembracingdiversitydoesnotcomeata
costofoverallperformance.Moreover,theeffectivenessofourmethodinthesenewsettingsshowthatthequerieswe
select(forinferringattributes)generalizebeyondouroriginaldatasetsuite.Thatis,wedonotneedtotunetheLLM
queriesforeachnewclassificationtaskofinterest.Nonetheless,theabilitytoaddandremoveLLMqueriescanbeseen
asastrength,asapractitionerisprovidedmorecontrolthaninstandardzero-shotclassification.
5.2 EnhancedInterpretabilityatNoCosttoPerformance
5.2.1 FaithfulFine-grainedInterpretationsForFree. Havingshownthatourmethodisequally(andusuallymore)
performantthanexistingapproaches,wenowdiscusstheenhancedinterpretabilityofourmethod.Namely,each
inferencecomeswithalistofthe𝑘subpopulationsspecificallyrelevanttothetestimageforfree.Figure5showsa
fewexamplewhereourmethodcorrectsmisclassificationsfromthestandardapproach(seeAppendixAformore).
Theseinterpretationsarefaithful,astheyareexactlythesubpopulationsusedtocomputetheclassscore.Also,sincewe
includeattributesalongvariousaxesofdiversity,ourinterpretationsarefiner-grainedthanpriorwork:DCLIPyields
thesamesetgeneraldescriptorsforanyimagepredictedtoagivenclass,andWaffleCLIPoffersnointerpretabilityat
all.Thisinterpretabilitycanenablemodeldebugging,aserroneouspredictionscanbetracedbacktoattributesthat
eitherdonotmatchtheintendedclass(i.e.LLMmistake)orcannotberecognizedwell(i.e.VLMmistake).Atahigh
level,whilestandardzero-shotclassificationisacompleteblackbox,theLLM-inferredattributesofourmethodprovide
interpretableintermediateoutputs,increasingthetransparencyofthesystemoverall.Moreover,ourinferencestrategy
resultsinconciseexplanations,whichhavemoreutilitythanexplanationsthataretoolongforahumantodigest[25].
5.2.2 AnticipatingandArticulatingPotentialFailures. Inadditiontoexplainingeachinference,theinterpretablein-
termediateoutputsofourpipelinealsoallowforerroranticipation.Namely,bycomparingtheinferredattributesfor
eachclass,onecananticipateanddescribesimilarsubpopulationsfromdifferentclasses,whichmaycorrespondto
inputswherethemodelislesseffective.Forexample,fortheLiving-17taskintheBreedsdatasets,theLLMlistsgibbon
asakindofboththeapeandmonkeyclasses.Whilegibbonsareapes,theyaresmallerthanmostapes,whichmakes
themresemblemonkeys.Indeed,standardzero-shotaccuracyforgibbonsisonly14%,whereasotherapesareclassified
atanaccuracyof93.5%2.Inanothercase,rugislistedasaco-occuringobjectforthebedclass,whenrugitselfis
anotherclassinthedataset.Whileanticipatingpotentialfailuremodesisintuitiveforhumans,itischallengingtodo
soatscale.Byincorporatinganauxiliarymodel(LLM)withinterpretableintermediateoutputs(inferredattributes),
practionerscanbothmoreeasilyauditandverifythezero-shotclassificationpipeline,andbetterunderstandpotential
challengeswiththetaskofinterest.Wehopethatthegreatertransparencyofoursystem(importantly,achievedwithout
compromisingonoverallaccuracy)canresultinincreasedtrustandmoreresponsibleuse.
6 ABLATIONS
Wenowdetailadditionalablationstudiestoshedinsightonthesourceofourmethod’simprovementsoverexistingart
andhowapractitionercanapplyourmethodwithgreatercontrol.First,westudyhowperformancevariesforboth
ourmethodandbaselinesasthenumberofattributesgrows,sotodemonstratethevalueofourflexibleconsolidation
strategy,specificallyforinputsfromthehardestclasses.Then,weidentifyaprincipledtrade-offbetweenaccuracy
overallandontheworstclassesusingourmethod,controlledbythehyperparameter𝑘.
2Wecancomputethisbecausegibbonhappenedtobeagroundtruthsubpopulationfortheapeclass,whichdoesnotoccurinthebedandrugcase.
11FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
Fig.6. Accuracy,overallandfortheworstclasses,asnewtypesofattributesareadded.Performanceforourconsolidationscheme
continuouslyimproves,whileitsaturatesordeterioratesforothers.Figure11showssimilartrendsforaccuracyontheworst20%of
classesandsubpopulations.
6.1 ScalingwiththeManyAxesofDiversity
Onesourceofgainsforourmethodisthatweinferattributesofmanytypes,whilepriorworksonlyincludeone.We
arguethatourflexibleconsolidation(ofsubpopulationsimilaritiestoasingleclassscore)alsoprovidesimprovements
overnaiveaveragingorthenonlinearconsolidationofCHiLS.Totestthis,wesequentiallyaddeachtypeofattribute,
andinspectperformanceusingthethreemethods.Figure6showsourconsolidationscalesbestasmoreattributesare
added,withsizablegainsforaccuracyovertheworstclasses.Incontrast,performancesaturateswithaveraging,and
actuallydeteriorateswithCHiLS.ThelatteroccurssinceCHiLSassumesthatsubpopulationsaremutuallyexclusive,as
isthecaseinhierarchicallabelsets.Whenaddingattributesalongthemanyaxesofdiversity,resultantsubpopulations
overlap,makingazero-shotclassificationoverallsubpopulations(asdoneinCHiLS)unreliable.Averagingisalso
suboptimal,astheimpactofeachattributediminishesasthenumberofattributesaddedincreases:weseethisinthe
leftplot,asaccuracybarelyincreasesforthefinalthreeaddedattributetypes.Also,samplesthatareclosetoonly
afewsubpopulationsbutfarfrommost(i.e.,atypicalinstances)ultimatelyreceivealowscorewhenallscoresare
averaged.Thus,whileaveragingoversubpopulationscanimproveaccuracy(toanextent),itislesssuitedtoimproving
performanceonatypicalinstancesthanourmethod.Weexplorethisfurtherinthenextsection.
6.2 TunableTrade-offbetweenAccuracyOverallandOnWorstClasses
Recallthatourmethodconsistsofcomputingthesimilarityofagiventestimage’sembeddingtotheembeddingof
numerous(ontheorderofhundreds)subpopulationsperclass,beforeaveragingoveronlythetop𝑘similarities,where
𝑘 issmall.Notethatwhen𝑘 =∞,ourconsolidationreducestosimpleaveragingoverallvectorsperclass.Toshed
insightonhowourconsolidationdiffersfromaveraging,wesweep𝑘,whilekeepingourattributeinferencefixed.
Additionally,weexplorelinearlyinterpolatingclassscoresusingourconsolidation(top-𝑘)andfullaveragingviaa
secondhyperparameter𝜆,sothat𝜆 =0resultsinourmethodand𝜆 =1isaveraging.Wejointlysweep𝜆from0to
1and𝑘from1to128topinpointthewayinwhichourconsolidationimprovesuponaveraging.
Figure7showsoverallaccuracyvs.accuracyontheworst5%ofclasses3forboth𝑘 and𝜆.Thetrendisidentical
forthetwoparameters:first,bothaccuracymetricsincreaseaswemoveawayfromfullaveraging,withmuchlarger
3Weobservethesametrade-offwheninspectingtheworst10%and20%ofclasses.SeeAppendix11.
12EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.7. Right:As𝑘decreases,first,accuracyoverallandontheworstclassesbothincrease.Then,overallaccuracybeginstodecrease
whileaccuracyontheworstclassescontinuestoimprove.Thus,wecancontrolthistrade-offvia𝑘.Left:𝜆,thecontinuousanalogof
𝑘,allowsforgreatercontrol.
gainsoccurringfortheworstclasses.Then,accuracybeginstodrop,whileaccuracyontheworstclassescontinues
toimprove.Tounderstandthistrade-off,consideraninstancethathashighsimilaritytoonesubpopulationembedding
foraclass,andlowsimilaritytoallothers.Inthe𝑘 =1case,thisinstanceisgivenahighscorefortheclass.Thiscan
benefitatypicalinstancesoftheclass,astheymaybevisuallydissimilarfrommostotherinstances(recalltheArctic
fox).However,thiscanintroduceerrors,asthecorrectpredictionforaninstancemostlyclosetoembeddingsfrom
itstrueclasscanbeflippedwiththepresenceofjustonehighlysimilar(perhapsunreliable)subpopulationembedding
fromadifferentclass.Thus,lowerchoicesof𝑘maybenefitmoreatypicalinstances,leadingtoimprovedaccuracyon
worstclasses(whicharemostdiverse;see3.1),potentiallyatthecostofoverallaccuracy.Withthisinsight,practitioners
canchoosehowtotuneourmethodbasedontheirendgoals.Also,since𝜆iscontinuous,itoffersclosercontrolof
thistradeoff:indeed,accuracyontheworstclassescanbeimprovedbyalargermarginwhenvarying𝜆,andvarying
𝑘and𝜆togethercanleadtobestnumbersforbothmetrics.4
7 LIMITATIONS
Onutilizingauxiliarymodels.OurmethodaddsanLLMintothezero-shotclassificationpipeline,whichcanincrease
computationalcostandintroduceasourceoferror.Wenotethattheaddedcomputeforinferringattributesisonly
doneoncepertask,sotheasymptoticcostperinferenceonlydiffersmarginally(duetocomputingsimilaritiestomore
vectorsperclass,whichisaveryfastoperation)comparedtothestandardapproach.ToinspectthereliabilityofLLM
outputs,wemanuallyverify300randomlyselectedLLMoutputs.Wefindonly2.7%ofresponsesareuninformative5,and
noneofthe300tobeinaccurate.Moreover,ourflexibleconsolidationschemeoffersakindofrobustnesstoirrelevant
LLMoutputs:Recall,onlythesimilaritytoasmallnumberofsubpopulationsperclasscontributetoeachlogit.Thus,
irrelevant(i.e.notappearinginthedata)subpopulationsareeffectivelyignoredanddonotaffectthelogit.However,
westillknowthatLLMsarecapableofprovidinginaccurateoutputs,andevendetectedonesuchinstance(thegibbon
examplefromsection5.2.2).WefindtheautomaticdetectionofunreliableLLMoutputstobeaninterestingavenuefor
futurework,bothtoimproveaccuracyandgaininsightofpotentialcomplexitiesinthegiventask.
4Tobetruetothezero-shotsetting,notuningwasdonetoobtaintheresultsin5.1.Wetriedtworeasonablysmallvaluesfor𝑘(8and16),observed
similarresults,andwentwith𝑘=16,whichwasmarginallybetter.
5Allofthesewerefoundinresponsestothe‘descriptors’query,withresponseslike‘size’or‘shape’fortheclassdog.
13FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
VLMsarenotalwaysreliable.OurworkassumesthatVLMsarecapableofrecognizingsubpopulationswithina
classwhennamed.Whilethisisoftentrue,VLMscanstillfail,especiallyforcompositeconcepts.Weaimtokeepour
subpopulationssimple,ascribingonlyoneattributetoeach.Nonetheless,itiscurrentlynotpossibletoknowapriori
whetheraVLMcanrecognizeasubpopulationinazero-shotmanner.Wehopemoreworkonuncertaintyestimation
canenrichourmethod,bywayofautomaticallyflaggingandremovingsubpopulationsthattheVLMwillnotbeableto
reliablydetect.
8 CONCLUSION
Torepresentclasseswithdiverseinstances,whichcancomeinmanyforms,onevectorperclassmaynotbeenough.
Moreover,VLMshaveamazingabilitiesthatarerestrictedwhenweonlyuseonevectorperclass.Thus,insteadof
ignoringintra-classdiversity,weembraceit,byexplicitlyinferringandencodingasmuchofitaswecan.Weproposea
simplenonlinearconsolidationschemethatflexiblyattendstosubpopulationspresentinanimagewhileignoringthose
thatareirrelevant.Wefindthatourmethodconsistentlymatchesorimprovesoverstrongbaselines,andcarefulablations
indicatethatourmethod’sgainscomefromimprovingperformanceonthehardestclassesandsubpopulations.Thus,em-
bracingdiversitycanhelpreduceperformancedisparities,includingonreal-worldfairnessbenchmarks,towardsmodels
thatworkwellforall.Ourapproachallowspowerfulmodelstoworktogetherinatransparentwayviaintermediate
interpretableoutputs,facilitatinginferenceswithexplanations,aswellasgreatertoolstounderstandanddebugpotential
failures.Wehopeourworkspursfurthercuriosityaroundhowexistingparadigmsmaylimitthecapabilitiesofourmod-
ernmodels,towardsthedevelopmentofnewAIsystemsthatovercomethefairnessandtransparencylimitationsoftoday.
14EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
REFERENCES
[1] BangAn,SichengZhu,Michael-AndreiPanaitescu-Liess,ChaithanyaKumarMummadi,andFurongHuang.2023.MoreContext,LessDistraction:
VisualClassificationbyInferringandConditioningonContextualAttributes. arXiv:2308.01313[cs.CV]
[2] LukasBossard,MatthieuGuillaumin,andLucVanGool.2014.Food-101–MiningDiscriminativeComponentswithRandomForests.InEuropean
ConferenceonComputerVision.
[3] JoyBuolamwiniandTimnitGebru.2018.GenderShades:IntersectionalAccuracyDisparitiesinCommercialGenderClassification.InProceedingsof
the1stConferenceonFairness,AccountabilityandTransparency(ProceedingsofMachineLearningResearch,Vol.81),SorelleA.FriedlerandChristo
Wilson(Eds.).PMLR,77–91. https://proceedings.mlr.press/v81/buolamwini18a.html
[4] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,andArmandJoulin.2021. EmergingPropertiesin
Self-SupervisedVisionTransformers.InProceedingsoftheInternationalConferenceonComputerVision(ICCV).
[5] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,
IonStoica,andEricP.Xing.2023.Vicuna:AnOpen-SourceChatbotImpressingGPT-4with90%*ChatGPTQuality. https://lmsys.org/blog/2023-03-
30-vicuna/
[6] Ching-YaoChuang,JampaniVarun,YuanzhenLi,AntonioTorralba,andStefanieJegelka.2023.DebiasingVision-LanguageModelsviaBiased
Prompts.arXivpreprint2302.00070(2023).
[7] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009. ImageNet:Alarge-scalehierarchicalimagedatabase.In2009IEEE
ConferenceonComputerVisionandPatternRecognition.248–255. https://doi.org/10.1109/CVPR.2009.5206848
[8] MohammadMahdiDerakhshani,EnriqueSanchez,AdrianBulat,VictorGuilhermeTurrisidaCosta,CeesG.M.Snoek,GeorgiosTzimiropoulos,and
BraisMartinez.2023.BayesianPromptLearningforImage-LanguageModelGeneralization. arXiv:2210.02390[cs.CV]
[9] ZhiliFeng,AnnaBair,andJ.ZicoKolter.2023.LeveragingMultipleDescriptiveFeaturesforRobustFew-shotImageLearning.arXiv:2307.04317[cs.CV]
[10] DanHendrycks,StevenBasart,NormanMu,SauravKadavath,FrankWang,EvanDorundo,RahulDesai,TylerZhu,SamyakParajuli,MikeGuo,
DawnSong,JacobSteinhardt,andJustinGilmer.2021.TheManyFacesofRobustness:ACriticalAnalysisofOut-of-DistributionGeneralization.
ICCV(2021).
[11] DanHendrycks,KevinZhao,StevenBasart,JacobSteinhardt,andDawnSong.2021.NaturalAdversarialExamples.CVPR(2021).
[12] TonyHuang,JackChu,andFangyunWei.2022.Unsupervisedpromptlearningforvision-languagemodels.arXivpreprintarXiv:2204.03649(2022).
[13] PhillipIsola,JosephJ.Lim,andEdwardH.Adelson.2015.Discoveringstatesandtransformationsinimagecollections.2015IEEEConferenceon
ComputerVisionandPatternRecognition(CVPR)(2015),1383–1391. https://api.semanticscholar.org/CorpusID:15870772
[14] YounghyunKim,SangwooMo,MinkyuKim,KyungminLee,JaehoLee,andJinwooShin.2023.Bias-to-Text:DebiasingUnknownVisualBiases
throughLanguageInterpretation. arXiv:2301.11104[cs.LG]
[15] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.2023.BLIP-2:BootstrappingLanguage-ImagePre-trainingwithFrozenImageEncodersand
LargeLanguageModels. arXiv:2301.12597[cs.CV]
[16] SubhransuMaji,EsaRahtu,JuhoKannala,MatthewBlaschko,andAndreaVedaldi.2013. Fine-GrainedVisualClassificationofAircraft.
arXiv:1306.5151[cs.CV]
[17] CristinaMenghini,AndrewDelworth,andStephenH.Bach.2023.EnhancingCLIPwithCLIP:ExploringPseudolabelingforLimited-LabelPrompt
Tuning.arXiv:2306.01669(Jun2023). https://doi.org/10.48550/arXiv.2306.01669arXiv:2306.01669[cs].
[18] SachitMenonandCarlVondrick.2023.VisualClassificationviaDescriptionfromLargeLanguageModels.ICLR(2023).
[19] M.JehanzebMirza,LeonidKarlinsky,WeiLin,MateuszKozinski,HorstPossegger,RogerioFeris,andHorstBischof.2023. LaFTer:Label-Free
TuningofZero-shotClassifierusingLanguageandUnlabeledImageCollections.arXiv:2305.18287(May2023). http://arxiv.org/abs/2305.18287
arXiv:2305.18287[cs].
[20] Maria-ElenaNilsbackandAndrewZisserman.2008.AutomatedFlowerClassificationoveraLargeNumberofClasses.2008SixthIndianConference
onComputerVision,Graphics&ImageProcessing(2008),722–729. https://api.semanticscholar.org/CorpusID:15193013
[21] ZacharyNovack,JulianMcAuley,ZacharyLipton,andSaurabhGarg.2023.CHiLS:Zero-ShotImageClassificationwithHierarchicalLabelSets.In
InternationalConferenceonMachineLearning(ICML).
[22] OmkarM.Parkhi,AndreaVedaldi,AndrewZisserman,andC.V.Jawahar.2012.Catsanddogs.2012IEEEConferenceonComputerVisionandPattern
Recognition(2012),3498–3505. https://api.semanticscholar.org/CorpusID:383200
[23] SarahMPratt,RosanneLiu,andAliFarhadi.2023.Whatdoesaplatypuslooklike?Generatingcustomizedpromptsforzero-shotimageclassification.
https://openreview.net/forum?id=3ly9cG9Ql9h
[24] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,Jack
Clark,GretchenKrueger,andIlyaSutskever.2021.LearningTransferableVisualModelsFromNaturalLanguageSupervision.arXiv:2103.00020[cs.CV]
[25] VikramV.Ramaswamy,SunnieS.Y.Kim,RuthFong,andOlgaRussakovsky.2023.Overlookedfactorsinconcept-basedexplanations:Dataset
choice,conceptlearnability,andhumancapability. arXiv:2207.09615[cs.CV]
[26] VikramV.Ramaswamy,SingYuLin,DoraZhao,AaronB.Adcock,LaurensvanderMaaten,DeeptiGhadiyaram,andOlgaRussakovsky.2023.
Beyondweb-scraping:Crowd-sourcingageodiversedataset.InarXivpreprint.
[27] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet Classifiers Generalize to ImageNet?
arXiv:1902.10811[cs.CV]
15FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
[28] MeganRichards,PolinaKirichenko,DianeBouchacourt,andMarkIbrahim.2023.DoesProgressOnObjectRecognitionBenchmarksImprove
Real-WorldGeneralization? arXiv:2307.13136[cs.CV]
[29] WilliamAGaviriaRojas,SudnyaDiamos,KeertanRanjanKini,DavidKanter,VijayJanapaReddi,andCodyColeman.2022.TheDollarStreet
Dataset:ImagesRepresentingtheGeographicandSocioeconomicDiversityoftheWorld.InThirty-sixthConferenceonNeuralInformationProcessing
SystemsDatasetsandBenchmarksTrack. https://openreview.net/forum?id=qnfYsave0U4
[30] KarstenRoth,JaeMyungKim,A.SophiaKoepke,OriolVinyals,CordeliaSchmid,andZeynepAkata.2023.WafflingaroundforPerformance:Visual
ClassificationwithRandomWordsandBroadConcepts. arXiv:2306.07282[cs.CV]
[31] ShibaniSanturkar,DimitrisTsipras,andAleksanderMadry.2020.BREEDS:BenchmarksforSubpopulationShift.arXiv:ComputerVisionandPattern
Recognition(2020). https://api.semanticscholar.org/CorpusID:221095529
[32] AshishSeth,MayurHemani,andChiragAgarwal.2023.DeAR:DebiasingVision-LanguageModelswithAdditiveResiduals.InIEEE/CVFConference
onComputerVisionandPatternRecognition,CVPR2023,Vancouver,BC,Canada,June17-24,2023.IEEE,6820–6829. https://doi.org/10.1109/CVPR52729.
2023.00659
[33] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHerveJegou.2021.Trainingdata-efficientimage
transformersI&distillationthroughattention.InInternationalConferenceonMachineLearning,Vol.139.10347–10357.
[34] HaohanWang,SongweiGe,ZacharyLipton,andEricPXing.2019.LearningRobustGlobalRepresentationsbyPenalizingLocalPredictivePower.
InAdvancesinNeuralInformationProcessingSystems.10506–10518.
[35] YueYang,ArtemisPanagopoulou,ShenghaoZhou,DanielJin,ChrisCallison-Burch,andMarkYatskar.2023. LanguageinaBottle:Language
ModelGuidedConceptBottlenecksforInterpretableImageClassification.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR).19187–19197.
[36] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,MojtabaSeyedhosseini,andYonghuiWu.2022.CoCa:ContrastiveCaptionersareImage-Text
FoundationModels.Trans.Mach.Learn.Res.2022(2022). https://api.semanticscholar.org/CorpusID:248512473
[37] YuhuiZhang,JeffZ.HaoChen,Shih-ChengHuang,Kuan-ChiehWang,JamesZou,andSerenaYeung.2023.DiagnosingandRectifyingVision
ModelsusingLanguage.InTheEleventhInternationalConferenceonLearningRepresentations. https://openreview.net/forum?id=D-zfUK7BR6c
[38] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.2022.Conditionalpromptlearningforvision-languagemodels.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition.16816–16825.
[39] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiweiLiu.2022.Learningtopromptforvision-languagemodels.InternationalJournalof
ComputerVision130,9(2022),2337–2348.
[40] BeierZhu,YuleiNiu,YuchengHan,YueWu,andHanwangZhang.2022.Prompt-alignedgradientforprompttuning.arXivpreprintarXiv:2205.14865
(2022).
16EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.8. Ourmethodyieldsfaithful,fine-grainedinterpretations,forfree.Top4shownforbrevity.
A EXAMPLEINTERPRETABLEINFERENCES
Weshowadditionalexamplesofinterpretableinferencesinfigure8.
B CASESWHEREATTRIBUTESHELPMOST
Wenowprovidemoreexamplesofinstanceswheretheprevailingparadigmforzero-shotclassificationresultsin
disparate performance, and consequently, where our method yields largest improvements. The crux of the issue
withexistingparadigmisthattheclassnameembeddingstrugglestobeclosetoembeddingsforimagesfromall
subpopulationsoftheclass,particularlywhentheclasscontainsmanyvisuallydiversesubpopulations.Forexample,
apenguinlooksverydifferentthanmostbirds,soembeddingsofpenguinimageswillbesomedistanceawayfrom
embeddingsofmostbirds.Similarly,thepenguinimagesmaynotresideclosetothetextembeddingofthecaption
‘aphotoofabird’.Indeed,wefindstandardzero-shotclassificationaccuracyforKingPenguinbirdsisonly46%,
whileaccuracyfortheclassis96%.Figure9showsthisexamplealongwithotherinstanceswherestandardzero-shot
classificationleadstobiasedperformance.Wehighlightexamplesthatourmethodleadstoimprovements.Noticethat
thesubpopulationstendtobeatypical.
Howthendoesourmethodresultinimprovements?Weleveragethefactthatdespitepoorstandardzero-shot
accuracyonsubpopulationsthatliefarfromtheirclassnameembedding,VLMsarestillcapableofrecognizingthese
17FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
Classifier CLIP DINO Sup.
Encoder
CLIP -0.28 -0.51 -0.43
DINO -0.37 -0.54 -0.48
Sup. -0.47 -0.72 -0.65
Table4. CorrelationbetweendiversityandaccuracybyclassonImageNet.Westudythreemodels:visiontransformerstrained
withCLIP,DINO,ortraditionallabelsupervision.Diversityreferstovarianceofimageembeddingswithinaclass,withembeddings
obtainedwiththe‘encoder’model.
atypcialsubpopulations.Thatis,penguinimagesmaybefarfromthe‘bird’textembedding,buttheyareactuallyquite
closetothe‘penguin’textembedding.Thatis,standardzero-shotclassificationdoesnottakeadvantageoftheabilityof
VLMstorecognizeobjectsatadeeperlevelthanthatoftheclassificationtask.Thus,byincludingtherightattributes,
wecanenableaccuraterecognitionofatypicalsubpopulations.
Figure10showsmoreexamplesofsubpopulationswhereincludingthegroundtruthattributeresultsinsignificant
gainsinaverageprecision,indicatingthatincludingtheattributeallowsforrecognitionofatypicalsubpopulations.
Inthisfigure,APcorrespondstotheaverageprecisionscoreobtainedwhenusingthesimilarityofanimageto(a)
theclassnameembeddingor(b)theembeddingoftheclassnamewiththeattribute(e.g.‘bird’vs.‘KingPenguinbird’)
todetectimagesbelongingtothatsubpopulation.Again,thesesubpopulationsgenerallyappeardifferentlythana
typicalinstancefromtheirclass,makingtheclassnameembeddinganimpreciseprobeforthatsubpopulation.However,
evidently,whengiventheattribute,VLMsarestillcapableofrecognizingthesubpopulation.
C DETAILSONCORRELATIONBETWEENDIVERSITYANDACCURACYPERCLASS
WecomputeImageNetaccuracyperclassusingthreemodels:CLIPViT-B/16viastandardzero-shotclassification,
DINOViT-S/16withalinearclassificationheadfittoImageNetoverfixedfeatures[4],andaViT-S/16trainedwith
traditionalclass-labelsupervisiononImageNet[33].Notably,allthesemodelsutilizealinearclassificationhead.That
is,theyoperateunderaonevectoroneclassparadigm.Toproxydiversity,wemeasurethevarianceofembeddingsper
class.Thatis,perclass,wecomputetheaveragesquareddistancebetweenthemeanembeddingandtheembeddingof
eachclassinstance.Notethatourmeasureofdiversitydependsontheimageencoder;weexploreusingeachofthe
threeaforementionedmodels.Table4showstheresults.Allcorrelationsarestronglynegative,indicatingthatacross
classifiersandusingvariousmeasuresofdiversity,classeswithhigherdiversityarepredictedatloweraccuracies.This
supportstheintuitivehypothesisthatconsistentlyrepresentinganentireclasswithonevectorismadechallenging
whentheclasscontainsdiverseinstances.
D ADDITIONALEXPERIMENTALDETAILS
Notethatwewillprovideallcode,sothatfurtherdetailsareeasilyaccessible.
D.1 Datasets
ThefourhierarchicaldatasetsweutilizearesubsetsofImageNet[7]curatedby[31].Wealsoutilizetheattributeddataset
ofMITStates[13],derivingtwoclassificationtasksfromtheirannotations.Finally,weutilizethegeographicfairness
benchmarksofDollarstreet[29]andGeoDE[26].Whenreportingsubpopulationaccuracies,weuseincomelevelasthe
groundtruthattributeforDollarstreet.NotethatforMITStatesandDollarstreet,weconductafilteringofclassnames.
18EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.9. Examplesubpopulationswhereourmethodexhibitssizableaccuracygainscomparedtostandardzero-shotclassification(i.e.
classnameembeddingonly).
Namely,wecomputecosinesimilarityofCLIPembeddingsforeachpairofclassnames.Foranypairexceedinga
threshold,weremoveoneclassnamefromconsideration.WedothisbecauseMITStateswasnotoriginallyintendedto
beaclassificationdataset,andweobservedhighlysimilarclassnamesinDollarstreet(e.g.‘toilet’and‘bathroom/toilet’).
19FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
Fig.10. Examplesubpopulationswheretheclassnameembeddingisimprecise,butincludingtheattributeleadstolargeboostsin
averageprecision.Notably,thesesubpopulationsreflectinstancesatypicaltotheclass.
Query Prompt Examples
Kinds List16differentkindsofpear Bartlett,Bosc,D’Anjou
States List10differentwaysinwhichapearmayappearinanimage Wholepear,Pearslices,Pearchunks
Descriptors Listusefulfeaturesfordistinguishingapearinanimage Roundshape,Glossyskin,Greenorbrowncolor
Co-occurringObjects Inanimageofapear,list10otherobjectsthatmayalsoappear Leaves,Stem,Branches
Backgrounds Listtendifferentlocationsinwhichapearmayappearinanimage Fruitbasket,Stilllifepainting,Candydish
Table5. ExampleLLMpromptsandoutputsforclass-specificandclass-adjacentqueries.
Weuseathresholdof0.8and0.9togeneratethecoarseandfine-grainedMITStatesdatasetsrespectively,andusea
thresholdof0.9forDollarstreet.
D.2 InferringAttributes
WenowprovidedetailsonourexactLLMqueries.First,forclass-specificandclass-adjacentqueries,table5shows
theprecisepromptshowntotheLLMalongwithexampleoutputs,bothfortheclasspear.Forallqueries,weappend
Only use up to three words per list itemsothattheLLMdoesnotdroneon.WesamplefromtheLLM
(Vicuna-13b-v1.5)withatemperatureof0.7,repetitionpenaltyof1,andamaxnumberofnewtokensof512.
Wenowprovidemoreinformationonclass-agnosticqueries.Weusecontinentsasregions,andthefivemostpopulous
countriespercontinentasourlistofcountries.ThesecanbothbeobtainedviapromptinganLLMorsearchingthe
internet.
D.3 AutoGlobal
Wenowshowmoredetailsfortheauto-globalquery,whichwefoundquiteimpressive.Itconsistentlywasamongstthe
attributetypethatprovidedthemostaccuracygainsacrossdatasets.ThefirstprompttotheLLMwas:
List 16 common general ways in which two instances of the same object may look different. For
example, size, age, or cleanliness. Only use one word per list item.
Thenextpromptwas:
For each of those items, list up to four different general adjectives related to the time.
Please use common words..
Then,finally,outoflaziness,weincludedathirdpromptof:
Thanks. Please organize your output as a python dictionary.
20EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Axis Attributes
size small medium large tiny
age young mature ancient old
cleanliness dirty clean spotless grimy
color white black red blue
texture rough smooth soft hard
material plastic metal wood fabric
shape round square rectangular triangular
position upright horizontal vertical diagonal
reflection bright dull shiny matte
transparency clear opaque translucent transparent
shine glossy matte shiny dull
pattern striped polka-dotted plaid solid
markings spotted striped checked speckled
surface rough smooth bumpy even
appearance appealing unappealing attractive unattractive
Table6. Attributesandaxesofdiversityinferredviatheauto-globalquery.SeeD.3formoreinformation.
Accuracy AvgWorst Worst20%of Worst20%of Worst10%of Worst10%of
Method Subpop Classes Subpops Classes Subpops
Vanilla 73.22 50.17 44.90 33.10 36.66 22.05
DCLIP 72.65 49.72 45.35 32.72 37.16 21.92
Waffle 73.36 50.23 44.97 33.34 36.66 22.43
CHiLS 74.13 51.84 46.00 34.80 37.07 23.24
Ours 74.75 52.04 47.52 35.77 39.21 24.40
Table7. AverageperformanceovereightdatasetsandtwoVLMs.
TheresultantaxesofvariationandattributesperaxiscanbefoundinTable6.
E ADDITIONALRESULTS
Inthemaintext,wepresentedresultsusingCLIP.ResultsforBLIP-2canbefoundinTables8and9.Trendsareconsistent
withresultsCLIP.Foraglobalpicture,wepresentresultsaveragedoverbothVLMsandalldatasetsintable7.Our
methodperformsbestoverallmetrics,againwithlargestgainsoccurringovertheworstclassesandsubpopulations.
Wealsoshowresultsforeachdatasetindividuallyintable11.Wefinditencouragingthatourresultsareconsistent
acrossbothVLMsandforeachofoureightdatasets.
Further,fortheanalysisinSection6.1,weshowperformanceusingthesimilarmetricsofaccuracyovertheworst
20%ofclassesandsubpopulations,asshowninmosttables.Seefigure11.Trendsarethesameasinthemaintext,
thoughslightlylesspronounced.Tobeclear,ourconsolidationyieldsbestperformance,whileotherseithersaturateor
deteriorate.
Lastly,wealsoshowadditionalplotsfortheanalysisinSection6.2.Inthemaintext,weplottedaccuracyoverallvs.
overtheworst5%ofclasses.Wechoosetoshowaccuracyovertheworst5%becauseitmostclearlyconveysthetradeoff
21FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
Accuracy AvgWorst Worst20%of Worst20%of
DatasetType Subpop Classes Subpops
States Vanilla 70.60 42.65 43.44 26.28
DCLIP 69.80 41.42 41.54 24.25
Waffle 70.10 42.18 41.99 25.76
CHiLS 70.83 42.51 44.31 26.75
Ours 71.30 42.84 43.92 27.21
Hierarchical Vanilla 75.29 50.33 44.30 32.18
DCLIP 75.60 49.41 46.35 32.25
Waffle 75.25 48.84 44.48 31.67
CHiLS 77.17 52.00 45.86 34.59
Ours 77.95 52.47 48.66 35.46
Table8. Zero-shotclassificationondatasetswithknownvariationtypesforBLIP-2.HierarchicaldatasetsfromNovacketal.[21]and
Statesaretheaverageofcoarseandfine-grainedcategorizationsofMITStates.Seetable1forresultsusingCLIPViT-B/16.
DollarStreet Worst Worst AvgWorst Worst20%of Worst20%of
Method Accuracy Region Income Subpop Classes Subpops
Vanilla 50.91 39.76 31.89 36.76 18.87 11.33
DCLIP 49.81 39.05 32.03 37.01 18.22 12.14
Waffle 51.07 41.00 33.05 36.67 19.43 12.53
CHiLS 51.56 40.26 32.37 38.35 19.56 12.45
Ours 51.96 40.63 32.78 37.91 21.04 13.61
GeoDE
Vanilla 90.48 87.95 - 84.41 71.01 69.06
DCLIP 90.98 88.19 - 84.78 72.71 71.32
Waffle 91.10 88.85 - 84.97 74.11 72.56
CHiLS 90.75 87.99 - 84.63 71.11 69.46
Ours 91.40 89.07 - 85.44 73.08 71.22
Table9. Zero-shotclassificationperformanceongeographicallydiversehouseholdobjectfromDollarStreetandGeoDEusingBLIP-2.
Seetable2forresultswithCLIPViT-B/16.
weobserve.Figure12showsthistradeoffstillexistswhenlookingatotherpercentiles,thoughitislesspronounced,
whichisexpected.
F WHYDIDCHILSFAILONIMAGENETFORCLIP?
InTable3,weobserveCHiLStofailcatastrophicallyforImageNetusingaCLIP.Weconjecturetheissuearisesdueto
thehighnumberofclasses(1𝑘),andevenlargersetofsubclasses(about10𝑘).EachlogitinCHiLSistheproductofa
softmaxoutputover1𝑘optionsandasoftmaxoutputoverabout11koptions.Furthermore,becauseCLIPsimilarities
usuallyfallwithinasmallrange(about0.1−0.3),thedifferenceinfinallogitsmaybesosmallthatnoisefromrounding
errorsdominatesthesignal.Notably,CHiLSdoesnotfailonImageNetwhenusingBLIP-2andourotherresults
onCHiLS(i.e.onhierarchicaldatasets)closelymatchtheresultsreportedintheoriginalCHiLSpaper,
suggesting that our implementation is correct, and that the problem arises due to small differences in CLIP
22EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.11. Accuracyfortheworst20%ofclassesandsubpops,averagedoverourdatasetsuiteaswesequentiallyaddnewtypesof
attributesusingdifferentconsolidationschemes.Seefigure6inthemaintextforaccuracyoverallandovertheworst10%ofclasses,
alongwithmorediscussion.Asshowninthemaintext,ourmethodscalesthebestasattributesareaddedsequentially.
Fig.12. Wereplicatefigure7usingmetricsthatlookatalargerportionoftheworstclasses.Asimilartradeoffemerges,thoughina
slightlylesspronouncedway.Wenotethatthisisexpected,asincreasingthenumberofclassesconsideredlikelyalsoincreasesthe
numberoflessdiverseclassesincluded.
23FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
ImageNet v2 -A -R Sketch Food Flowers Aircraft Pets avg
OverallAccuracy
Vanilla 55.68 51.26 26.53 63.06 55.35 83.73 53.05 25.65 65.33 53.29
DClip 56.15 51.44 26.60 62.53 54.79 84.02 54.12 26.43 62.36 53.16
Waffle 57.10 52.11 26.83 64.78 55.88 83.61 52.03 27.00 62.78 53.57
CHiLS 56.26 51.56 25.63 63.43 55.54 84.29 53.55 26.94 64.95 53.57
Ours 57.28 52.27 26.25 65.13 56.21 84.79 54.03 25.23 63.42 53.85
AccuracyonWorst20%ofClasses
Vanilla 6.74 5.40 3.39 24.12 3.99 57.36 0.00 0.00 27.52 14.28
DClip 7.62 6.55 2.80 22.83 4.20 58.52 0.00 0.00 17.34 13.32
Waffle 8.22 5.98 3.25 24.16 5.14 57.21 0.00 0.00 25.84 14.42
CHiLS 8.05 6.25 2.90 24.02 4.87 60.90 0.00 0.00 27.13 14.90
Ours 8.19 6.90 2.87 25.65 4.53 61.44 0.00 0.00 25.28 14.98
Table10. AccuracyonextradatasetsforBLIP-2.
similarities.OnBLIP-2,whileCHiLSdoesnotfailcatastrophically,itstillunderperformscomparedtoourmethod,with
accuracyabout1%lower.
Onecouldlikelyfixthisproblembychangingthetemperatureofthesoftmax,butweopttofaithfullyfollowthe
originalmethod.Indeed,amodifiedversionofCHiLSwithoutthesoftmax(whichamountstoourmethodusingonly
theKindsquery(seetable5andwith𝑘 =1)doesnotfailcatastrophicallyonImageNet,thoughtheoverallaccuracy
andaccuracyonworstclassesforthis‘fixed’CHiLSdoesnotexceedourmethod’sresults.
Whilethischangeseemssmall,webelieveitencapsulatesadifferenceinphilosophybetweenCHiLSandourmethod.
CHiLSisdesignedfordatasetswithclearhierarchy,whereeachinputcanfitneatlyintooneofmanymutuallyexclusive
subpopulations.Incontrast,wearguethatdiversityemergesinmanyways,withoverlappingsubpopulationsarising
fromattributesdrawnalongvariousaxes.Bytakingasoftmax,CHiLSrequiresthataninputisnotonlysimilartoone
subpopulationwithinaclass,butthatitisalsodissimilarfromtheothersubpopulations.Inourmethod,insteadof
seekingtoexplicitlynameallsubpopulationsinamutuallyexclusiveway,weenumeratemanypotentialattributes,and
createaflexibleconsolidationthatonlyrequiresaninputtobeclosetoafewsubpopulationswithinitsclassforittobe
classifiedcorrectly.
G WHENCANWECRAMANENTIRECLASSINONEVECTOR,ANDWHENCANWENOT?
Arguably,diversitywithinclassesisunavoidable,astwoinstancescanvaryinnumerousways(discussedfurtherin
Section4.1).Howthen,haveclassifiersenjoyedsuccessundertheone-vector-per-classparadigm,despiteitstension
withintra-classdiversity?First,wenotetheseperformancedisparitiesareoftenobfuscatedinmetricslikeoverall
accuracy;indeed,thesupervisedclassifiersstudiedaboveeachachieveimpressiveoverallaccuracies.Nonetheless,the
tensioncanbesomewhatresolvedif(i)onelearnsembeddingsthatreducethediversitythatispresentininputspace,
and/or(ii)thesinglevectorlearnedperclasscontainsfeaturesthatareuniquetotheclassandpresentacrossclass
instances,despiteintra-classvariancethatpersistsintheembeddingspace.Weexpandonthesebelow.
24EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Geographic (MIT)States Hierarchical
Method Dollarstreet Geode Coarse Fine Entity13 Entity30 Nonliving26 Living17
Accuracy
Vanilla 51.21 90.34 78.24 59.07 68.22 68.43 77.27 92.96
DCLIP 49.80 91.14 77.80 55.65 68.64 68.49 76.32 93.35
Waffle 51.22 91.34 78.31 58.47 68.95 68.66 77.13 92.80
CHiLS 51.62 90.85 78.83 58.55 69.33 70.69 79.55 93.65
Ours 52.33 91.58 79.33 59.90 71.47 70.59 79.25 93.59
AverageWorstSubpopulationAccuracy
Vanilla 37.18 83.49 53.83 29.49 21.77 36.27 57.12 82.24
DCLIP 36.69 84.50 53.01 27.81 22.54 36.87 54.54 81.82
Waffle 37.18 85.20 53.94 28.96 21.88 36.87 56.37 81.41
CHiLS 37.98 84.56 53.69 29.22 23.77 42.50 59.50 83.53
Ours 39.11 85.42 54.26 30.10 25.31 39.03 59.54 83.53
AccuracyforWorst20%ofClasses
Vanilla 18.60 71.63 52.12 26.79 34.38 32.50 49.15 74.00
DCLIP 18.64 73.57 51.35 24.46 36.48 33.21 46.60 78.50
Waffle 18.78 74.98 52.31 25.17 31.41 33.46 48.40 75.24
CHiLS 20.04 72.19 53.65 26.82 36.07 31.71 52.05 75.50
Ours 20.96 74.61 54.03 28.05 37.55 34.94 53.10 76.92
AccuracyforWorst10%ofClasses
Vanilla 11.92 59.30 41.63 18.09 29.75 21.75 40.58 70.25
DCLIP 11.82 64.22 41.16 15.90 26.80 22.71 38.08 76.62
Waffle 10.69 64.74 42.00 16.68 23.03 23.79 39.78 72.59
CHiLS 13.64 58.82 44.74 18.41 25.60 21.04 42.92 71.38
Ours 14.35 62.61 44.24 19.29 31.10 25.33 43.50 73.25
AccuracyforWorst20%ofSubpopulations
Vanilla 11.17 69.50 36.23 11.78 14.54 15.62 33.90 72.07
DCLIP 11.67 71.61 35.08 10.16 14.54 14.92 30.19 73.57
Waffle 11.64 73.47 36.89 10.93 13.24 16.27 32.77 71.49
CHiLS 12.58 70.55 37.44 11.76 15.23 16.88 39.67 74.29
Ours 14.33 72.93 38.21 12.64 17.33 16.94 38.86 74.93
AccuracyforWorst10%ofSubpopulations
Vanilla 6.10 57.47 23.27 4.95 5.35 5.67 18.90 54.71
DCLIP 6.08 61.38 21.71 3.74 5.77 5.04 15.20 56.43
Waffle 5.82 63.27 23.27 4.26 4.96 6.56 17.00 54.30
CHiLS 7.40 57.62 24.93 4.88 5.96 6.21 21.50 57.43
Ours 8.62 61.26 24.72 5.53 6.88 6.88 22.00 59.29
Table11. Metricsforeachdataset.ResultsareaveragedoverCLIPandBLIP-2.Ourmethod’sgainsareconsistentovertheeight
datasetsuite.
25FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
G.1 Idealconditionsfortheone-vector-one-classparadigm
Mostmodernvisionclassifiersconsistofadeepfeatureencoder,mappingimagestoarichembeddingspace,followedby
alinearclassificationhead,mappingembeddingstoclasslogits.Thelinearclassificationheadconsistsofasinglevector
(andascalarbias)perclass.Alinearclassificationheadisaccurateif,foranyinstancefromthe𝑖𝑡ℎ
class,theactivation
onthe𝑖𝑡ℎ
classvectormustbehigherthantheactivationforanyotherclassvector.Weexpressthismathematically
below,withxdenotingtheembeddingofanimagefromclass𝑖,andci,cjdenotingvectorsintheclassificationhead.
∀x∈C𝑖,∀𝑗 ≠𝑖, werequirethatx·ci−x·cj >0 (1)
Notethatx·ci−x=x·(ci−cj)=∥x∥∥ci−cj∥cos(x,ci−cj) (2)
Thus,∀x∈C𝑖,∀𝑗 ≠𝑖, werequirethat cos(x,ci−cj) >0 (3)
Thelaststeparisesbecausenormisalwaysnon-negative.Now,letusfocusondifferentcomponentsofthisrequired
condition(bydefinition)foranaccurateone-vector-per-classclassificationhead.First,thesinglevectorcimustcontain
containasetoffeaturesthatareuniquetothatclass.Thatis,thesefeaturesremainwhenconsideringtheresidual
ci−cjforany𝑖 ≠ 𝑗.Secondly,theuniquefeaturesthatdiscriminatetheclassfromallothersmustalsobealignedwith
everyinstanceoftheclass.Inotherterms,theseuniquefeaturesmustbeinvarianttoanydiversitywithintheclass.
Also,notethatthequantityweexpanduponaboveissimplythemarginforclassification.Intheidealcase,thismargin
wouldbemaximized.
G.2 Class-supervisedtrainingiswellsuitedfortheonevectorperclassparadigm,butVLMpretrainingis
not
Intraditionalclass-labelsupervisedtraining,thefeatureencoderisjointlyoptimizedwiththeclassificationheadto
minimizeaclassificationloss.Letusconsiderhowthiseffectsthelinearclassificationheadandthefeatureencoder
individually.First,fixingtheclassificationhead,weseethesupervisedobjectiveencouragesallembeddingsfrom
oneclasstobedrawnclosetotheirrespectivesinglevector,andconsequently,closetooneanother.Inotherwords,
invarianceofembeddingswithinaclassispromoted.Next,withthefeatureencoderfixed,classificationheadvectors
alignwithembeddingswithintheirclassandde-alignwithembeddingsfromoutsidetheirclass.Thus,theclassification
headvectorsareoptimizedtosolelycontainthefeaturesuniquetotheirclassembeddings.Therefore,trainingwith
traditionalclass-labelsupervisiondirectlypromotestheinvarianceanduniquenesspropertiesrequiredforthesuccess
oftheone-vector-per-classparadigm.
Ontheotherhand,VLMsareoptimizedwithmarkedlydifferentobjectives.ManyVLMsemploycontrastiveimage-
textmatching,inwhichnegativeexamplesarefarweakerandclassesarenolongerdefined;insomeways,thetraining
isanalogoustooptimizingaclassificationtaskwithaninfinitenumberofclasses.Indeed,twoinstancesthatbelong
tothesameclassinadownstreamtaskmayhaveembeddingspushedapartduringVLMpretraining,directlygoing
againsttheaforementionednotionofclass-wiseinvariance.OthercommonVLMobjectiveslikecaptioningorquestion
answeringpromotethedescriptivenessoftheembedding.Thus,insteadofhoninginonuniquefeatures,embeddings
arelikelytodescribeasmuchaspossible.Wenotethathavingmaximallydescriptiveembeddingsistypicallyagood
thing,asitallowsforre-useofthesamefeatureencoderformanydownstreamtasks,asisdoneinlinearprobing
withself-supervisedencoders.Thekeycaveatisthatinthosecases,thelinearclassificationheadisstillexposedto
instancesfromallclasses,andthus,eachclassificationheadvectorcanlearntoalignonlywiththeuniquefeaturesfor
26EmbracingDiversity:InterpretableZero-shotclassificationbeyondonevectorperclass FAccT’24,June3–6,2024,RiodeJaneiro,Brazil
Fig.13. ArcticFoxbiasisamplifiedinzero-shotclassifiervs.tosupervisedlinearprobes.
itsclass.Incontrast,inthezero-shotsetting,theclassificationheadvectorsareobtainedindependentlyofoneanother
viaembeddingthenamesofclassesviathetextencoder,andthus,itisunreasonabletoexpectthatthesevectorssatisfy
theuniquenesscondition.
G.3 ArcticFoxCaseStudy:Biascanbeamplifiedwhenusingonevectorperclassparadigmforzero-shot
classification
Stayingintheone-vector-per-classsetting,wenowcompareclassvectorsobtaineddirectlyinazero-shotmannerto
thoseobtainedwithsupervision.Specifically,wefocusontheArctic Foxbias,showninFigure2.Wetrainalinear
classificationheadoverfixedCLIPembeddingsusedaskewedtrainingsetthatunder-representsArctic foxesinthe
trainingset.Wefindthatthebiasofthezero-shotvectorisonparwithhavingonly3%ofthetrainingimagesinthe
foxclassbeArctic foxesinthesupervisedsetting,suggestingthatlimitationsoftheonevectorperclassparadigm
maybeexacerbatedinthezero-shotsetting.
H ONEFINALTRADE-OFF
Insection6.2,weshouldtwohyperparametersthatcouldtradeoverallaccuracyforaccuracyovertheworstclasses.We
nowpresentonemore,alongwithatheoreticalexplanation.Throughoutthepaper,weconsider‘averaging’tomean
computingsimilaritiestomultiplevectorsandthenaveragingthosesimilarities;thisishowDCLIPandWaffleCLIP
average,andwillrefertothisasAverageSims.However,averagingoverpromptsasdoneinoriginallyinCLIPconsists
ofaveragingvectorsfirstandthencomputingsimilaritytooneaveragevector;wecallthisAverageVecs.Thedifference
issubtle:inthelattercase,anadditionalnormalizationoccurswhencosinesimilarityistaken.
Wenowshowtheoreticallythatwhenallembeddingsarenormalized(i.e.forCLIP),AverageVecssimplyrescales
theclassscoreyieldedbyAverageSimsbyafactorthatmeasureshowdiffusethevectorsfortheclassare.Let𝑥 bean
imageembeddingand{𝑣 1,𝑣 2,...,𝑣 𝑘}besubpopulationvectorsforagivenclass.Weassumeallvectorsarenormalized
27FAccT’24,June3–6,2024,RiodeJaneiro,Brazil Moayeri,Rabbat,Ibrahim*,andBouchacourt*.
Fig.14. Averagingsubpopulationvectorsbeforecomputingsimilaritytoanimageembeddingprovestobeanotherwaytotrade
overallaccuracyforaccuracyontheworstclasses.Thatis,whenwefirstcomputesimilaritytoeachsubpopulationandthenaverage,
weobtainhigheroverallaccuracybutloweraccuracyontheworstclasses,comparedtowhenwefirstaveragesubpopulationvectors
andthencomputethesimilaritytotheaveragevector.
tothehypersphere,asisthecaseforCLIP.Thatis,∥𝑣 𝑖∥=1forall𝑖and∥𝑥∥=1.Let𝑣 := 𝑘1 (cid:205)𝑘 𝑖=1𝑣 𝑖 denotetheaverage
vector.WecomputetheclassscoreforAverageVecsbelow.
AverageVecs=cos(𝑥,𝑣)=
𝑥·𝑣
=
𝑥· 𝑘1 (cid:205)𝑘 𝑖=1𝑣 𝑖
=
𝑘1 (cid:205)𝑘 𝑖=1𝑥·𝑣 𝑖
∥𝑥∥∥𝑣∥ ∥𝑣∥ ∥𝑣∥
=
𝑘1 (cid:205)𝑘 𝑖=1cos(𝑥,𝑣 𝑖)
=
AverageSims
∥𝑣∥ ∥𝑣∥
Togetfromline1to2,weutilizethefactthatcosinesimilarityisequivalenttothedotproductwhenbotharguments
areunitnorm.Letusnowconsiderwhatthisresultentails.Thedenominatoristhenormoftheaveragevector.This
quantityisalwaysbetween0and1.Itislowestwhenthevectorsaremostdiffuse.Thus,theclassscoreobtainedby
AverageSimsisscaleduptoobtainthescoreforAverageVecsbymorewhenthevectorsarediffuse.Inotherwords,
averagingthevectorsfirstimplicitlyupweightsvectorscorrespondingtodiversesubpopulations.
Basedonthissimpletheory,wewouldexpectthemostclasseswithhighdiversitytohavehigheraccuracyunder
AverageVecscomparedtoAverageSims,astheirclassscoresareinflatedmorethanthelessdiverseclasses.The
effectonoverallaccuracy,however,isnotperfectlyclear.Toinspectthis,weperformthesamesweepover𝑘and𝜆as
insection6.2,exceptnowweadditionallytryreplacingallsimilarityaveragingwithvectoraveraging.Figure14shows
theresults.Weaverageaway𝑘forclarity.Indeed,averagingovervectorsimprovesaccuracyontheworstclasses.For
highvaluesof𝜆=1,weseeaveragingvectorsalsoslightlyimprovesoverallaccuracy.However,inthevastmajorityof
valuesfor𝜆,overallaccuracyishurtbyaveragingvectorsinsteadofsimilarities.Wehopethisanalysisprovidesinsight
astothepreciseeffectofaveragingsimilaritiesorvectors,whichmayberelevanttootherswhowishtoexploregoing
beyondonevectorperclass.
28