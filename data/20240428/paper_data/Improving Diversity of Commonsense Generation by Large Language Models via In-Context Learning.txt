Improving Diversity of Commonsense Generation by
Large Language Models via In-Context Learning
TianhuiZhang DanushkaBollegala BeiPeng
UniversityofLiverpool,Amazon
{tianhui.zhang, danushka, bei.peng}@liverpool.ac.uk
Abstract
A dog catches a frisbee thrown to it.
A dog catches a frisbee thrown by its owner.
Generative Commonsense Reasoning (GCR) A dog jumps in the air to catch a frisbee thrown
requires a model to reason about a situation Dog; Catch; by its owner.
Frisbee; Throw
usingcommonsenseknowledge,whilegenerat- A dog leaps to catch a thrown frisbee.
ingcoherentsentences. Althoughthequalityof The dog catches the frisbee when the boy throws
it.
thegeneratedsentencesiscrucial,thediversity A man throws away his dog's favourite frisbee
ofthegenerationisequallyimportantbecause expecting him to catch it in the air.
it reflects the model’s ability to use a range
ofcommonsenseknowledgefacts. LargeLan- Figure1: Anexampleofdiversegeneratedsentences
guageModels(LLMs)haveshownproficiency setsinCommonGen(Linetal.,2020)dataset. Thegen-
inenhancingthegenerationqualityacrossvar- erationshownatthebottom(in green)areconsidered
ious tasks through in-context learning (ICL) byhumanannotatorstobemorediversethanthoseat
usinggivenexampleswithouttheneedforany thetop(in red).
fine-tuning. However, the diversity aspect in
LLMoutputshasnotbeensystematicallystud- showninFigure1. Althoughallsentencesshown
iedbefore.Toaddressthis,weproposeasimple
in Figure 1 are grammatical, the bottom set ex-
method that diversifies the LLM generations,
presses diverse view points (e.g. from the dog’s
while preserving their quality. Experimental
as well as the man’s) compared to the set at the
resultsonthreebenchmarkGCRdatasetsshow
top. Apartfromthegenerationquality,diversityis
thatourmethodachievesanidealbalancebe-
tweenthequalityanddiversity. Moreover,the alsoanimportantfactorintextgenerationbecause
sentencesgeneratedbyourproposedmethod thelow-diversitytextstendtobedull,repetitiveor
canbeusedastrainingdatatoimprovediversity biasedtowardsaparticularviewpoint(Tevetand
inexistingcommonsensegenerators.
Berant,2021). Diversityisanimportantconsidera-
tioninmanyNaturalLanguageGeneration(NLG)
1 Introduction
applications, such as story generation (Li et al.,
Commonsensereasoningistheabilitytomakelogi- 2018),paraphrasegeneration(Guptaetal.,2018),
caldeductionsaboutconceptsencounteredindaily and GCR (Yu et al., 2022; Liu et al., 2023). In
life,andisconsideredasacriticalpropertyofintel- GCR tasks, the input text often provides insuffi-
ligentagents(DavisandMarcus,2015). Concepts cientinformationtosupportdiversereasoningand
are mental representations of classes and are ex- generatemultipleplausibleoutputs. Therefore,the
pressedusingwordsinalanguage(Liuetal.,2023). diversity present in GCR task enables the explo-
Given the inputs, the GCR task requires a model rationofdifferentperspectivesorallpossibleout-
to generate a high quality sentence that is gram- comesforareal-worldsituation. Existingmethods
maticalandadherestocommonsense,evaluatedby promotediversitythroughspecialdecodingstrate-
its similarity to a set of human-written reference gies, such as nucleus sampling (Holtzman et al.,
sentencescoveringthesamesetofconcepts(Lin 2019), orencodinginterventionssuchasrandom
etal.,2020). noiseinjection(Guptaetal.,2018)orMixtureof
Oftenthereexistsmultiplerelationshipsbetween Experts(MoE)approaches(Shenetal.,2019).
agivensetofconcepts,leadingtoalternativerea- WeproposeIn-ContextDiversification(ICD),a
soningpathsthattakediverseviewpoints. Forex- computationally-efficientandaccuratemethodto
ample,giventhefourconceptsdog,frisbee,throw improvethediversityinGCR,wherethesentences
andcatch,differentsentencescanbegeneratedas aregeneratedfromapre-trainedLLM,andstrikes
4202
rpA
52
]LC.sc[
1v70861.4042:viXraa fine-balance between the output diversity and Truncatedsampling(Fanetal.,2018)prunesand
quality. ICDusesanICLapproachtoincreasethe then samples the tokens based on the probability
diversity of the sentences generated by an LLM, distribution. Furthermore, Shenetal.(2019)pro-
while maintaining the quality of the generation. posed an MoE approach to diversify translation
ICD is a two-step process where it first lets an outputs. Moreover,incorporatingexternalcorpora
LLMtofreelygeneratehigh-qualitysentencesthat intheMoEfurtherpromotesdiversity,suchasby
aregrammatical,commonsensebearingandcover usingaknowledgegraph(Yuetal.,2022;Hwang
allthegiveninputconcepts. Next,ICDusesauser- et al., 2023) or by a collection of retrieved sen-
specifieddiversitymetrictoevaluatethediversity tences(Liuetal.,2023). AlthoughLLMshavere-
ofthegeneratedsentences. Ifthediversityislow, portedsuperiorperformanceinnumerousNatural
ICDprovidesfeedbacktotheLLM,instructingit LanguageProcessing(NLP)tasks(Touvronetal.,
togeneratemorediversesentencesconsideringthe 2023;OpenAI,2023b,a),tothebestofourknowl-
alreadygeneratedsentences. edge, diversifying their generations in common-
Given that ICD is using LLMs to generate di- sensereasoningwithICLhasnotbeenexploredin
versesentencesviaICLandwithoutupdatingthe priorworkonGCR.
parameters of the LLMs, an interesting and open
In-Context Learning. Recent studies demon-
questioniswhetheranLLMcanaccuratelyjudge
strate that LLMs can exhibit robust few-shot per-
thediversityofagivensetofsentences,covering
formanceonavarietyofdownstreamtasksthrough
a common set of concepts. To answer this ques-
ICL (Brown et al., 2020). ICL is a technique for
tion,weconductanexperimentwhereweinstruct
instructing an LLM using one or more examples
GPT3.5-turbotojudgethediversityofthesetof
foraparticulartextgenerationtask. Thegenerated
input sentences according to a five-scale grading
textisconditionedonboththeinputaswellasthe
system, and convert the predicted grades into bi-
instructionprompt. Wangetal.(2023)showthat
naryjudgements(i.e. diversevs. non-diverse). We
inICL,labelwordsinthedemonstrationexamples
comparetheLLM-assignedgradesagainstthoseby
functionasanchors,whichaggregatesemanticin-
agroupofhumanannotators,andfindamoderate-
formationtotheirwordrepresentationsintheshal-
level(Cohen’sKappaof0.409)agreementbetween
low (closer to the input) layers, while providing
human vs. LLM judgements, demonstrating that
thatinformationtothefinalpredictionsperformed
LLMscanindeedbeinstructedtoobtaindiversity
bythedeeper(closertotheoutput)layers. Incon-
judgementsforGCRtasks.
trasttofine-tuning-basedmethods,ICLiscomputa-
We evaluate ICD on three GCR tasks/datasets:
tionallylightweightbecauseitdoesnotupdatethe
CommonGen (Lin et al., 2020), ComVE (Wang
parametersoftheLLM. Therefore,ICLisanattrac-
etal.,2020),andDimonGen(Liuetal.,2023). We
tivemethodwhenintegratingtask-specificknowl-
findthatourproposedICDbalancesdiversityand
edge to an LLM by simply changing the prompt
quality appropriately, improving their harmonic
andthefew-shotexamples(Dongetal.,2022).
mean by at least 6% over that of a default base-
line. Moreover, the sentences generated by ICD 3 In-contextDiversification
can be used as training data to improve diversity
We consider the problem of generating a set of
inaSeq2Seqmodel(Sutskeveretal.,2014;Lewis
diverse sentences that express commonsense rea-
etal.,2020),producingresultsthatarecomparable
soning,eitherbycoveringasetofgivenconcepts
tothemodelsthataretrainedonknowledgegraphs
(inCommonGenandDimonGen)orbyproviding
orhuman-writtentextcorpora(Liuetal.,2021;Fan
anexplanationforagivencounterfactualstatement
etal.,2020;Lietal.,2021).
(in ComVE). Formally, given a sequence (a set
2 RelatedWork of concepts or a statement) X = {x 1,...,x m},
thegoalofGCRistogenerateasetofgrammati-
DiverseTextGeneration. Avarietyofmethods callycorrectandcommonsensebearingsentences
have been proposed to enhance the diversity of Y = {y ,...,y }, where y is the i-th output
1 n i
NLG. Sampling-based decoding is an effective generatedbythemodelwithprobabilityp(y |X).
i
methodtoincreasethegenerationdiversity. Holtz- Moreover,werequirethatthegeneratedsentences
man et al. (2019) proposed nucleus sampling to {y ,...,y }tobelexicallyaswellassemantically
1 n
generate diverse content at the generation stage. diverse.Algorithm1In-ContextDiversification(ICD)
Default Diversified
Examples: Examples: Input: Generated sets of sentences S def and S div, respec-
Given several key words: [SRC], Given several key words: [SRC], tivelyfromdefaultanddiversifiedprompts,thenumberof
generate one coherent sentences using generate one coherent sentence using
background commonsense knowledge: background commonsense knowledge: desiredoutputsentencesN,andadiversitymetricf.
[TGT] [TGT] Output: OutputsetofsentencesS∗
Test instruction: Test instruction:
Given several key words: [INPUT], Step1: Given several key words: [INPUT], S∗ ←∅
generate one coherent sentence using generate [N] different and coherent α ←0
background commonsense knowledge: sentences using background commonsense
[OUTPUT] knowledge: [PRV] forS ∈(S def ∪S div)do
if(|S|==N)∧(f(S)≥α)then
(If the diversity of [PRV] is low)
α←f(S)
Step2: You have generated the following
sentences: [PRV], try to provide other S∗ ←S
reasonable sentences: [OUTPUT] endif
(a) (b) endfor
return S∗
Figure 2: An example of default and diversi-
fiedpromptsisshownforaninstanceselectedfromthe
CommonGendataset. Here,thedefaultpromptshown
tencesislow,inStep2,weshowthosesentencesto
in Figure 2a is taken from Li et al. (2023). Few-shot
theLLMandinstructittogeneratesentencesthat
examples are included in each prompt where [SRC]
aredifferenttothose. Asthecriteriafortriggering
denotes the set of input concepts and [TGT] the cor-
Step2,wecheckwhethertheexactsamesentence
respondingsentencesinCommonGen. Foragivenset
of[INPUT]concepts,theLLMisthenrequiredtogen- hasbeengeneratedmultipletimesbytheLLMdur-
erate sentences at the slot [OUTPUT]. As shown in ingStep1. Thefinalsetofgeneratedsentencesis
Figure2b,ICDusesthediversifiedprompt,whichoper- denotedbyS .
div
atesintwosteps.Step1generatesasetof[N]sentences,
[PRV].Wecheckforthediversityamongthesentences 3.2 Diversity-basedSampling
in[PRV],andifitislow,weusethepromptinStep2
Because of the limited availability of human-
togeneratethefinalsetofsentences.
written reference sentences for evaluating GCR
3.1 SentenceGeneration models, there exists a trade-off between quality
vs. diversitywhengeneratingsentencesforGCR
ToexplainourproposedICD,letusconsiderGCR
tasks.1 Simplymaximisingfordiversityoftenleads
onCommonGen,wherewemustgenerateasetof
togenerationsthatdonotcovertheinputconcepts
sentencesY,suchthateachsentencecontainsallof
in a natural way. For example, a randomly se-
theinputconceptsX asshowninFigure2a. Given
lected set of sentences would be highly diverse,
an LLM, we can design a prompt that contains a
yetunlikelytocapturetheinputconceptsets. On
task-specificinstructionandoneormoreexamples
the other hand, if we force an LLM to generate
containingtheinputconcepts(denotedby[SRC]in
sentencesthatcontainalloftheinputconcepts,it
Figure2)andthecorrespondinghuman-writtensen-
mightfinddifficulttogeneratesemanticallydiverse
tencescontainingallgiveninputconcepts(denoted
sentencesandresorttotriviallexicalorsyntactic
by[TGT])toinstructtheLLMtogenerateoutput
diversitytrickssuchasmorphologicalinflections
sentencesY (denotedby[OUTPUT])foragiven
orword-orderpermutations.
setofinputconceptsX (denotedby[INPUT]).We
To address this issue, we propose a diversity-
refertoapromptofthisnatureasadefaultprompt,
based sampling method shown in Algorithm 1.
andthecorrespondingsetofgeneratedsentences
Consider that the default prompt provides a set
byS .
def
S ofsentencesthathavenotbeenoptimisedfor
Notethatthedefaultpromptdoesnotnecessar- def
diversity (likely to have a higher quality), while
ily guarantee that the generated set of sentences
ontheotherhandthediversifiedpromptprovides
willbediverseandanLLMcouldreturnsentences
asetS ofsentencesthatarefurtherrefinedfor
thatarehighlysimilartoeachother. Toaddressthis div
diversity (likely to have a higher diversity). We
issue,weproposeadiversifiedpromptasshown
wishtofindasetofsentencesthatsimultaneously
inFigure2b. Specifically,thediversifiedprompt
satisfiesthefollowingcriteria: (a)mustcontainex-
operatesintwosteps. InStep1,werequirethatthe
actlyN sentences,asspecifiedbytheuser,and(b)
LLMgenerateN sentencesthataredifferent,inad-
musthaveahighdiversityscore,measuredusing
ditiontobeingcoherentandcommonsensebearing.
a user-specified diversity metric f(∈ R ). We
Next,weuseasuitablediversitymetrictoevaluate ≥0
formalise this as a subset search problem, where
the level of diversity among the generated set of
sentences. If the diversity of the generated sen- 1Thistrade-offisfurtherempiricallyverifiedin§5.1.we compute the union S ∪S and search for SPICE(Andersonetal.,2016)measurestheseman-
def div
the subset S∗ that jointly satisfies those criteria tic propositional overlap between two sentences,
following the procedure detailed in Algorithm 1. andBERTScore(Zhangetal.,2020)usescontextu-
AlthoughthetotalnumberofsubsetsofsizeN is alisedwordembeddingstomeasurethesemantic
(cid:0)|S def∪S div|(cid:1) , it is sufficiently small for the values similarity between tokens in two sentences. In
N
of N in our GCR tasks, which makes this subset alignment with prior works (Yu et al., 2022; Liu
searchfastinpractice. et al., 2023; Hwang et al., 2023), when multiple
candidate sentences are generated for a test case,
4 ExperimentalSettings weselectthehighest-scoringcandidateforevaluat-
ingquality.
4.1 TasksandDatasets
4.2.2 DiversityMetrics
WeevaluateICDonthreeGCRtasksasfollows.
Pairwise Diversity: We use self-BLEU (Zhu
ConstrainedCommonsenseReasoning: InCom-
etal.,2018)tomeasuren-gramoverlapamongsen-
monGen(Linetal.,2020)benchmark,amodelis
tenceswithineachgeneratedset. Themetriccom-
requiredtogenerateasentencecoveringagivenset
putestheaveragesentence-levelsimilaritybetween
of concepts such that background commonsense
allpairwisecombinationsofthegenerationsinthe
knowledge associated with the input concepts is
generationset. NotethatunlikeBLEU,self-BLEU
reflected. This dataset contains 35K distinct con-
does not require human generated references for
cept sets (train = 32651, dev = 993, and test =
measuringdiversity. Weuseself-BLEU3/4(corre-
1497)withcorrespondinghumanwrittensentences
spondington = 3and4)inourexperiment. Lower
(train=67389,dev=4018,andtest=6042). Each
self-BLEUscoresindicatehigherlexicaldiversity.
instancecontainsonaverage3-5inputconcepts.
Commonsense Explanation Reasoning: CorpusDiversity: Tomeasurethevarietywithin
ComVE(Wangetal.,2020)ispartoftheSemEval our generated text corpus, we employ Distinct-
2020 commonsense validation task, where for a k (Li et al., 2016), which calculates the ratio of
givencounterfactualstatement,amodelisrequired unique k-grams to the total number of k-grams.
to generate an explanation providing a reason Thismetricisparticularlyusefulforadjustingthe
describingwhythestatementisnonsensical. This biasofLLMstowardgeneratinglongersequences,
datasetcontains10K(train=8532,dev=476,and ensuringthatdiversityisnotartificiallyinflatedby
test=992)examples,whereeachexamplecontains thesentencelength. Additionally,weuseEntropy-
threereferenceoutputs. k to evaluate the distributional uniformity of k-
Diversified GCR: DimonGen (Liu et al., 2023) gram occurrences, considering word frequencies
involvesgeneratingdiversesentencesthatdescribe for a more nuanced view of diversity. Higher
therelationshipsbetweentwogivenconcepts. Itis Distinct-k and Entropy-k scores indicate higher
a challenging task because it requires generating diversity.
reasonable scenarios for a given pair of concepts
Semantic Diversity: All previously described
withoutanycontext. Thisdatasetcontains17109
diversity metrics are limited to evaluating lexi-
instances(train=15263,dev=665,test=1181),
cal diversity. To measure diversity at a semantic
whereeachinstancehas3-5references.
level, we propose self-cosSim, which is the aver-
agepairwisecosinesimilaritybetweengenerated
4.2 EvaluationMetrics
sentences, computed using sentence embeddings
We measure both the quality and diversity of the
obtained from SimCSE (Gao et al., 2021). Like-
sentences generated by models using the metrics
wise, we define the self-BERTScore as a diver-
describednext.
sity metric that averages the BERTScores for all
generatedsentencepairs. Lowerself-cosSimand
4.2.1 QualityMetrics
self-BERTScore values indicate higher semantic
We compare a generated sentence by a model diversity.
againstasetofhuman-writtenreferencestoeval-
4.2.3 CombinedMetrics
uate the quality of the generation using several
metrics: BLEU (Papineni et al., 2002) measures WewouldpreferGCRmodelsthathavebothhigh
n-gram precision against human reference texts, qualityandhighdiversity. Toincoporatebothas-pects into a single metric, we compute the Har- fault, diversified and for ICD. For model train-
monicMeanbetween(a)theself-BLEU-4asthe ing,weusetheAdamoptimiser(KingmaandBa,
diversitymetric,and(b)BERTScoreasthequality 2015) with a batch size of 64, a learning rate of
metric. Asdiscussedin§3.2,thereexistsatrade- 3e-5andabeamsizeof5. AlloftheMoE-based
offbetweenqualityanddiversityinGCR. There- models are trained for 20 epochs and required to
fore,theharmonicmeanissuitablewhenaveraging generatek = 3sentences. Allexperiments,except
qualityanddiversityscores.2 withGPT3.5-turbo,areconductedonasingleRTX
Alihosseinietal.(2019)proposedFre´chetBERT A6000GPU.
Distance(FBD)asajointmetricforsimultaneously
measuringboththequalityanddiversityofNLG. 5 ResultsandDiscussion
FBDisinspiredbytheFre´chetInceptionDistance
5.1 CommonsenseGeneration
(FID),proposedbyHeuseletal.(2017),formea-
suringthequalityofimagegeneration. Specifically, Wecomparethecommonsensegenerationsmade
FBDcomputesthepooleroutput3 ofasentenceas by ICD against those using the default and di-
itsembedding(Devlinetal.,2019)andrepresents versified prompts. For this purpose, we use
a set of sentences using the mean vector and the GPT3.5-turbo as the LLM and use the same 10
covariance matrix computed from their sentence few-shotexamplesinallpromptsforICL. Further
embeddings. Next,Wasserstein-2distanceiscom- templatesofthedefaultanddiversifiedprompts
puted between the set of reference sentences and used for each task are given in Appendix E. To
thesetofgeneratedsentences,whichcapturesboth assesstheimpactofICL,wecompareagainstfine-
thedistancebetweenthemeansaswellasvariance tunemethod,whereinGPT3.5-turboisfine-tuned
in the distributions. Lower FBD scores indicate on the entire training set in each dataset. Specif-
highcombinedperformance. ically, we use multiple human-written sentences,
availableinthetrainingdataforthethreedatasets
4.3 ImplementationDetails toseparatelyfine-tunethemodelsforeachtask. It
WeuseGPT3.5-turboandVicuna-13b-v1.54 as isnoteworthythatthefine-tunemethodusesasub-
LLMs with temperature set to 1.0 in our experi- stantially larger dataset for training (e.g., 67,389
ments. ByusingtwoLLMswithsignificantlydif- sentencesfromCommonGen)comparedtothe10
feringnumberofparametersandbyincluding,Vi- examplesusedbytheICL-basedapproaches. We
cuna,anopensourceLLM,weplantoimprovethe useself-BLEU-3asthediversitymetricf inAlgo-
reliabilityandreproducibilityofourresults. Max rithm1forICDinthisevaluation. Theoutcomes,
responselengthissetto25tokens. Theinference presented in Table 1, highlight the diversity and
times for CommonGen, ComVE and DimonGen qualitymetricsofthesemethodsacrosstheCom-
datasets are respectively 5-6, 2-3 and 1-2 hours. monGen,ConVE,andDimonGendatasets. Addi-
The cost of running ICD with GPT3.5-turbo are tionally,ahumanbaselineisintroducedtoevaluate
ca. $6, $4 and $4 respectively for CommonGen, thediversityofsentenceswrittenbyhumans,where
ComVE and DimonGen datasets. On the other wepair-wisecomparethehuman-writtensentences
hand, the costs of fine-tuning on GPT3.5-turbo for each input in the instances in the benchmark
aremuchhigherat$58.8forCommonGen,$24.7 datasetsusingdiversitymetrics. Notethathowever,
forComVEand$32.0forDimonGen. Moreover, thehumanbaselinemustnotbeconsideredasan
fine-tuningwithLoRA(Huetal.,2022)withrank upper-bound for diversity because there are only
of8andalphaof16onVicunatakesca. 34hours. asmallernumberofhuman-writtensentencesper
WeuseBART-large5 forMoE-basedmodels. We instanceinthebenchmarkdatasets.
use the GPT3.5-turbo to generate sentences for From Table 1, we see that fine-tune generates
the CommonGen train/dev/test sets using the de- sentencesthathavehighsemanticandcorpusdiver-
sity, and outperforms the human baseline. How-
2Weuseself-BLEU-4fordiversityandBERTScorefor
ever, recall that fine-tune requires a much larger
qualityinHarmonicMeanduetotheirreliabilityshownin
trainingsetandiscomputationallycostlycompared
preliminaryevaluations.OthermetricpairsareinAppendixD.
3Thelastlayer’shidden-stateofthefirsttokenofthese- toallICL-basedmethods. Moreover,weseethat
quence is further processed by a Linear layer and a Tanh
ICD can strike a good balance between quality
activationfunction.
4https://huggingface.co/lmsys/vicuna-13b-v1.5 and diversity in the sentences generated. Among
5https://huggingface.co/facebook/bart-large theICL-basedmethods,ICDachievesthebestdi-SemanticDiversity⇓ CorpusDiversity⇑ PairwiseDiversity⇓ Quality⇑ Combined
self-cosSim self-BERTScore Entropy-4 Distinct-4 self-BLEU-3 self-BLEU-4 BLEU-3 BLEU-4 SPICE BERTScore Harmonic⇑ FBD⇓
CommonGen
Human 67.3 60.6 10.9 91.0 25.4 17.6 - - - - - -
Fine-tune 64.7 55.9 11.4 91.1 26.9 17.9 41.2 32.1 30.3 64.2 72.1 51.9
default 93.3 88.7 10.2 53.7 77.2 72.4 50.8 40.9 30.1 70.4 39.6 60.2
diversified 85.2 69.8 11.0 83.7 44.4 34.9 44.3 34.6 28.5 65.0 65.4 53.9
ICD 83.5 66.2 11.0 88.5 31.0 21.0 47.4 37.7 29.1 67.4 72.7 51.8
ComVE
Human 62.7 47.0 9.6 96.1 12.4 8.1 - - - - - -
Fine-tune 59.8 42.6 9.8 95.2 13.4 10.3 27.4 19.4 33.1 53.7 67.2 47.6
default 83.9 73.5 9.6 74.3 50.8 45.2 27.5 19.7 36.2 55.1 54.9 50.9
diversified 76.1 56.5 9.7 88.0 23.2 16.5 30.5 21.8 35.8 56.5 67.4 47.9
ICD 72.5 51.1 9.8 90.1 13.7 8.7 29.0 20.8 36.1 55.5 69.0 48.7
DimonGen
Human 56.8 47.0 10.1 85.6 14.7 8.7 - - - - - -
Fine-tune 43.4 33 10.4 98.7 6.8 3.4 17.7 10.7 15.5 42 58.5 51.6
default 75.7 71.3 10 83.2 43.4 37.3 15.9 9.5 16.4 44.5 52.1 68.2
diversified 57.1 46.9 10.5 95.9 11.2 6.5 11.4 6.4 15.2 39.9 55.9 69.0
ICD 56.7 45.7 10.4 96.3 6.5 3.5 13.2 7.6 15.4 41.7 58.2 68.0
Table1: DiversityandqualityscoresonCommonGen,ComVEandDimonGenwithGPT3.5-turboLLM. Best
resultsoneachtaskforeachmetricareshowninitalics,whilethebestperformingICLresultsareshowninbold.
versity scores on all diversity metrics in all three Method SCS⇓ SBS⇓ E-4⇑ D-4⇑ SB-3⇓ BLEU-3⇑ SPICE⇑ HM⇑ FBD⇓
Fine-tune 59.6 49.9 11.4 93.3 22.8 35.8 27.6 69.9 52.4
datasets. Italsoexhibitshigherdiversitycompared
Default 82.2 73.8 10.9 74.9 52.9 44.6 29.1 60.2 56.2
against the human-written references. Moreover, Diversified 59.1 53.3 11.3 91.3 23.6 32.6 24.3 68.6 53.2
ICD 59.3 49.8 11.3 93.7 11.3 34.2 25.5 73.4 51.0
ICDoutperformsdefaultanddiversifiedaccord-
ingtotheCombinedmetrics. ICDalsoachievesa Table2: GCRonCommonGenusingVicuna-13b. ICD
usesself-BLEU-3. Here,SCS:self-CosSim,SBS:self-
HarmonicMeancomparabletothatofthefine-tune
BERTScore, E-4: Entropy-4, D-4: Distinct-4, SB-3:
baseline. Although default reports the best qual-
self-BLEU3, HM: Harmonic Mean. Best results for
ityscores,ithaslowdiversity,andisconsistently
eachmetricareshowninitalics,whilethebestperform-
outperformedbydiversifiedandICDondiversity ingICLresultsareshowninbold.
metrics. Ontheotherhand,diversifiedgenerally
scores lower on the quality metrics. Compared Method SCS⇓ SBS⇓ E-4⇑ D-4⇑ SB-3⇓ BLEU-3⇑ SPICE⇑ HM⇑ FBD⇓
todefaultanddiversified,ICDenhancesgenera- self-BLEU-3 83.5 66.2 11.0 88.5 31.0 47.4 29.1 72.7 51.8
self-CosSim 81.0 70.1 10.9 82.5 44.5 47.6 29.3 65.7 51.8
tiondiversitywhilemaintainingasatisfactorylevel self-BERTScore 83.1 62.8 11.0 87.0 36.3 46.5 28.9 69.6 51.8
of quality. ICD is also more stable to the sam- Table 3: Comparing the effect of using different di-
plingmethodsuchastemperaturethanfine-tune, versity metrics, f, in Algorithm 1 for ICD. We use
as shown in Appendix B. Note that fine-tune is GPT3.5-turboastheLLMandthebestresultsonCom-
monGendatasetareinbold. Here,SCS:self-CosSim,
not an ICL setting (the focus of this paper) and
SBS:self-BERTScore,E-4: Entropy-4,D-4: Distinct-4,
is included only as a baseline to demonstrate the
SB-3: self-BLEU3,HM:HarmonicMean.
levelofperformancethatcanbeachievedbyfine-
tuningonamuchlargerdataset. Despitethis,ICD
outperforms fine-tune on the Pairwise Diversity methods that use Vicuna-13b to be more diverse
inallthreedatasets,andCombinedmetricsinthe comparedtothosethatuseGPT3.5-turbo, while
CommonGendataset. thelattershowingbettergenerationquality.
As an open source alternative LLM to InTable3,weusedifferentdiversitymetricsasf
GPT3.5-turbo, we repeat this evaluation with inAlgorithm1tostudytheeffectontextgeneration
Vicuna-13b (Zheng et al., 2023) in Table 2. ofICD. Weseethatself-BLUE-3andself-CosSim
The same 10 few-shot examples as used with performsimilarlyacrossthequalitymetrics. Self-
GPT3.5-turboareusedinthisexperimentforthe BERTScoreshowsaslightlylowerquality(BLEU-
ICL-basedmethods. Fulltableonthreedatasetsare 3andSPICE),whichindicatessomelevelofover-
shown in Appendix C. Table 2 reconfirms ICD’s fittingtothediversitymetricbeingused. According
abilitytobalancebothqualityanddiversityaccord- to the combined metrics, any of those diversity
ingtotheCombinedmetrics(i.e. HarmonicMean metricscanbeusedwithICDtoobtaincomparable
andFBD)onthisdataset. Interestingly,weseethat performance.SemanticDiversity⇓ CorpusDiversity⇑ PairwiseDiversity⇓ Quality⇑ Combined
self-cosSim self-BERTScore Entropy-4 Distinct-4 self-BLEU-3 self-BLEU-4 BLEU-3 BLEU-4 SPICE BERTScore HarmonicMean⇑ FBD⇓
KG-BART - - - - - - 42.1 30.9 32.7 - - -
EKI-BART - - - - - - 46.0 36.1 33.4 - - -
KFCNet-w/oFC - - - - - - 50.2 42.0 35.9 - - -
KFCNet - - - - - - 57.3 51.5 39.1 - - -
MoE 89.3 81.9 9.7 61.6 63.1 56.6 49.0 38.5 33.5 70.6 53.8 61.7
MoKGE 88.7 80.6 9.9 65.2 60.4 53.6 48.8 38.4 33.1 70.3 55.9 60.8
default+MoE 90.8 84.2 9.7 61.2 65.6 58.8 51.8 41.3 34.7 73.1 52.7 61.9
diversified+MoE 85.3 79.9 9.8 63.2 58.3 52.6 51.4 41.4 34.6 71.6 57.0 54.5
ICD+MoE 90.4 82.3 9.8 64.9 58.4 50.5 53.2 43.1 35.4 73.8 59.3 62.5
Table 4: Downstream evaluation of the LLM-generated sentences. Top block methods use human-generated
resourcesfortraining,whiletheonesinthebottomblockaretrainedonLLM-generatedsentences. MoEapproaches
areshowninthemiddleblockandbottomblock. BART-largeisusedasthegeneratorforMoE-basedmethods.
Bestresultsforeachmetricareshowninbold,whilethebestperformingMoEforqualityisshowninunderline.
whichhasshowntoproducehighqualitycommon-
sensegenerations(Zhangetal.,2023)asthegen-
eratorforallexperts(seeAppendixAforfurther
details). WedenotethismethodbyICD+MoE.
As baselines for comparisons, we repeat the
aboveprocessusingthesentencesgeneratedbyde-
faultanddiversified,whichwedenoterespectively
asdefault+MoEanddiversified+MoEinTable4.
Moreover, we compare the performance against
twopreviouslyproposedMoEmodels: MoE (Shen
etal.,2019)andMoKGE(Yuetal.,2022). MoE
reliessolelyonthebasemodel,whereasMoKGE
Figure3: Humanvs. GPT3.5diversityratingsforran-
domlysampledsetsofsentencesgeneratedbyICD. Co- requires each expert to use different sets of con-
hen’sκ=0.409indicatesamoderateagreement. cepts from the ConceptNet (Speer et al., 2017)
knowledgegraph(KG). BecauseYuetal.(2022)
do not evaluate their MoKGE method on Com-
5.2 DownstreamEvaluation monGen,werantheiroriginalimplementation6 on
Theexperimentspresentedin§5.1showtheabil- CommonGenandreportresultsinTable4.
ity of our proposed ICD to generate diverse and AllpreviouslyproposedGCRmethodsareexclu-
commonsense bearing sentences. Therefore, an sivelytrainedusinghuman-createddata(e.g. sen-
important question with practical implications is tenceswrittenbyhumanand/ormanuallycompiled
whetherwecanusethesentencesgeneratedbyICD KGs such as ConceptNet), whereas the methods
asadditionaltrainingdatatoimprovebothdiversity described thus far in this section are trained on
andqualityofpreviouslyproposedmodelsonthe sentencesgeneratedbyanLLM(GPT3.5-turbo).
GCR task, which could be seen as a downstream Therefore,toevaluatethefeasibilityofusingLLM-
(extrinsic)evaluation. generatedsentencesfortrainingGCRmodels,we
ForthispurposeweselecttheMoE(Shenetal., include the following previously proposed GCR
2019), which diversifies the generation by select- modelsthataretrainedusingacombinationofcor-
ingoutputsfromamixtureofexperts. Eachexpert poraandKGs: KG-BART(Liuetal.,2021),EKI-
is assigned a randomly generated sequence of to- BART (Fan et al., 2020) and KFCNet (Li et al.,
kens, which is used as a prefix for all inputs sent 2021). For KFCNet, we present its two results –
tothatexpert. Foreachinput,anexpertisselected KFCNet w/o FC, which relies only on sentences
accordingtothevalueofalatentvariable, which includingtheinputconcepts,withoutfurtherpro-
is trained using the hard-EM algorithm. We fol- cessing, and KFCNet, which additionally ranks
low Liu et al. (2023) and train three experts that candidates and adds contrastive modules for the
retrievesentencesfromthecollectionofsentences encoderandthedecoder(Lietal.,2021). However,
generatedbyICDforconceptsetsintheCommon- notethatthosemethodsdonotconsiderdiversifica-
Gentrainsplit(210846sentencesintotal). Weuse
BART-large(Lewisetal.,2020)asthebasemodel, 6https://github.com/DM2-ND/MoKGECommonGen: Input: (piece, use, tool, metal) ComVE: Input: My friend like to eat electronic goods.
Human: Human:
• The group will use the tool to make a piece of art out of metal. • No one can digest electronic goods.
• I use a tool to cut a piece of metal out of the car. • Electronic products must not be eaten.
• The man used a piece of metal and the tools. • You would die if you ate electronics.
Default: Default:
• A piece of metal is being used as a tool. • Electronic goods are not edible and are not meant for consumption.
• A piece of metal was used as a tool in the construction project. • Electronic goods are not edible and cannot be consumed as food.
• A metal tool is being used to shape a piece of metal. • Electronic goods are not edible and are meant for functional use rather
than consumption.
ICD: ICD:
• The piece of metal is essential for any handyman's toolkit. • Eating electronic goods can damage the digestive system and cause
• The metal tool is a useful piece for working with metal. serious health issues.
• With the right tools, any piece of metal can be transformed • It is not healthy or safe to eat electronic goods as they are made up of
into something useful. toxic materials.
• Electronic goods are not edible and cannot be consumed as food.
Figure4: SentencesgeneratedbydefaultpromptandICDagainstthosebyhumansonCommonGenandComVE
testinstances. ICDgeneratesmorediverseandhighqualitysentencesthandefault.
tion,anddonotreportperformanceusingdiversity human-annotators, who are graduate students in
metrics. Therefore,wereportonlytheirpublished NLP. Toreducethesubjectivevariabilityinhuman
resultsforgenerationqualityinTable4. judgements, we average and then normalise the
From Table 4 we see that diversified+MoE al- ratingsfollowingtheLikertscale.
ways outperforms the original MoE in all diver-
In Figure 3, we plot the GPT-assigned ratings
sitymetrics,whichshowsthatsentencesgenerated
againstthosebyhumans. Wefurthersplittherat-
from LLMs can be used to diversify MoE-based
ingsintohighvs. lowdiversityratingsdepending
GCR. ICD+MoEcloselymatchestheperformance
onwhethertheratingisgreaterorlesserthan3. The
of diversified+MoE on diversity metrics, while
majorityofthedatapointsaredistributedalongthe
consistentlyoutperformingbothdiversified+MoE
diagonalquadrantsandaCohen’sKappaof0.409
and default+MoE on quality metrics. In partic-
indicatingamoderatelevelofagreementbetween
ular, the quality metrics reported by ICD+MoE
GPTandhumanratingsfordiversity.
(underlined in Table 4) are competitive against
those obtained by the models that are trained on The generated sentences using the de-
human-compiledresources(inthetopblock),ex- fault prompt, ICD and the human references in
ceptagainstKFCNet. Thisfindinghintsatpotential CommonGenandComVEdatasetsforasingletest
improvementgainsforGCRbyusinghybridtrain- instance are shown in Figure 4. From Figure 4
ingresourcesthatcombinebothhuman-compiled we see that the sentences generated using the
andLLM-generateddata,whichwehighlightasan default prompt often results in significant token
interestingfutureresearchdirection. overlap, thereby lowering the diversity. On the
other hand, ICD generates both lexically and
5.3 Diversity-AwarenessofLLMs semantically diverse sentences, covering the
diverseviewpointsinthehumanreferences.
GiventhatweuseLLMstoproducediversegenera-
tionsviaICL,itremainsanopenquestionwhether
an LLM would agree with humans on the diver-
sity of a given set of sentences. To answer this 6 Conclusion
question,weuserandomlyselected210sentences
(35 sets, each containing 6 sentences) generated
We proposed, ICD, an ICL-based method for
by ICD (using self-BLEU-3 as the diversity met-
achieving the optimal balance between diversity
ric)fortheinputconceptsetsintheCommonGen
and quality in text generation via LLMs. Our ex-
dataset. WeinstructGPT3.5-turbotoratethedi-
periments,conductedonthreeGCRtasks,demon-
versityofagivensetofsentencesaccordingtofive
strate that ICD significantly improves the diver-
diversity ratings 1-5 with 1 being highly similar,
while5beinghighlydiverse.7 Weprovidethesame sitywithoutsubstantiallycompromisingthequality.
Furthermore,wefoundthatbytrainingonthesen-
instruction as the annotation guidelines for eight
tencesgeneratedbyICD,wecanimprovediversity
7DetailedprompttemplatesareshowninAppendixE. inpreviouslyproposedGCRmethods.7 Limitations fied whether the GPT3.5-turbo and Vicuna-13b
LLMsthatweuseinourexperimentshavesimilar
Thisstudyprimarilyfocusesonthegenerationof
problems. Therefore,itisimportanttotestonexist-
Englishsentencesusingpre-trainedLLMs,alimi-
ingbenchmarksforsocialbiasesandharmfulgen-
tationshapedbythedatasetsweemployed. Specif-
erations before the proposed method is deployed
ically, we used the ComVE (Wang et al., 2020),
todiversifyexistingGCRmethodsusedbyhuman
CommonGen(Linetal.,2020)andDimonGen(Liu
users.
etal.,2023)datasets,whicharewell-regardedfor
Toelicithumanjudgementsofdiversityforthe
evaluatingdiversifiedcommonsensereasoningin
sentences generated by ICD, we use annotators
English. Therefore, our evaluation of the gener-
who are familiar with working with LLMs. It is
ation quality was limited to English, which is a
possiblethattheirsubjective(andpossiblybiased)
morphologicallylimitedlanguage. Futureresearch
viewpointsmighthaveinfluencedtheratingspro-
couldexpandthisscopetoincludemultilingualpre-
vided. Therefore, it will be important to conduct
trained models, thereby encompassing a broader
theevaluationinvolvingagroupofannotatorswith
linguisticspectrum.
different backgrounds to validate the findings re-
Ourapproachisprimarilygearedtowardsopti-
portedinthisanalysis.
mizingthetrade-offbetweendiversityandquality
in text generation. Consequently, we maintained
consistent default instructions across all experi- References
ments,adoptingthestandardcommonsensegenera-
DanialAlihosseini,EhsanMontahaei,andMahdiehSo-
tionpromptsusedinLietal.(2023)asourdefault
leymaniBaghshah.2019. Jointlymeasuringdiversity
instructions. andqualityintextgenerationmodels. InProceedings
We conducted our experiments using both a oftheWorkshoponMethodsforOptimizingandEval-
uatingNeuralLanguageGeneration.Associationfor
closed model (i.e. GPT3.5-turbo) as well as an
ComputationalLinguistics,Minneapolis,Minnesota,
open-source one (i.e. Vicuna-13b-v1.5) to pro-
pages90–98.
motethereproducibilityofourresults,whicharere-
PeterAnderson,BasuraFernando,MarkJohnson,and
portedusingmultiplepublicavailablebenchmarks.
Stephen Gould. 2016. Spice: Semantic proposi-
However,thereexistmanyotherLLMswithvary-
tionalimagecaptionevaluation. InComputerVision–
ingnumbersofparametersandtrainedondifferent ECCV2016: 14thEuropeanConference,Amsterdam,
corpora. Therefore,weconsideritisimportantto TheNetherlands,October11-14,2016,Proceedings,
PartV14.Springer,pages382–398.
evaluate our proposed method on a broad range
ofLLMstoverifythegeneralisabilityofourpro- Tom Brown, Benjamin Mann, Nick Ryder, Melanie
posedmethod. However,conductingsuchabroad Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,Amanda
analysiscanbecomputationallycostlyandexpen-
Askell,etal.2020. Languagemodelsarefew-shot
sive. For example, although GPT-4 is known to
learners. Advancesinneuralinformationprocessing
havesuperiortextgenerationcapabilities,itincurs systems33:1877–1901.
substantially greater costs (being 30 times more
YiChen,RuiWang,HaiyunJiang,ShumingShi,and
expensivethanGPT3.5-turboatthecurrentpric-
Ruifeng Xu. 2023. Exploring the use of large lan-
ing). Nevertheless,ICDisadaptableandcouldbe guagemodelsforreference-freetextqualityevalua-
extendedtootherLLMs. tion: Apreliminaryempiricalstudy. arXivpreprint
arXiv:2304.00723.
8 EthicalConsiderations
ErnestDavisandGaryMarcus.2015. Commonsense
reasoningandcommonsenseknowledgeinartificial
Inthiswork,wedidnotcreateorreleaseanyman-
intelligence. CommunicationsoftheACM58(9):92–
ually annotated data. Our work is based on the 103.
publiclyavailabledatasets,CommonGen,ComVE,
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
andDimonGen. Tothebestofourknowledge,no
Kristina Toutanova. 2019. BERT: Pre-training of
ethicalissueshavebeenreportedforthosedatasets. deepbidirectionaltransformersforlanguageunder-
Therefore,wedonotforeseeanydata-relatedethi- standing. InProceedingsofthe2019Conferenceof
calissuesarisingfromourwork. theNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech-
However, LLMs are known to generate re-
nologies,Volume1(LongandShortPapers).Asso-
sponses that may reflect societal biases and po-
ciationforComputationalLinguistics,Minneapolis,
tentially harmful content. We have not veri- Minnesota,pages4171–4186.Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy- Denoisingsequence-to-sequencepre-trainingfornat-
ongWu,BaobaoChang,XuSun,JingjingXu,and urallanguagegeneration,translation,andcomprehen-
ZhifangSui.2022. Asurveyforin-contextlearning. sion. InProceedingsofthe58thAnnualMeetingof
arXivpreprintarXiv:2301.00234. theAssociationforComputationalLinguistics.pages
7871–7880.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchicalneuralstorygeneration. InProceedings BeiLi,RuiWang,JunliangGuo,KaitaoSong,XuTan,
of the 56th Annual Meeting of the Association for HanyHassan,ArulMenezes,TongXiao,JiangBian,
ComputationalLinguistics(Volume1: LongPapers). and JingBo Zhu. 2023. Deliberate then generate:
AssociationforComputationalLinguistics. Enhancedpromptingframeworkfortextgeneration.
HaonanLi,YeyunGong,JianJiao,RuofeiZhang,Tim-
ZhihaoFan,YeyunGong,ZhongyuWei,SiyuanWang,
othyBaldwin,andNanDuan.2021. Kfcnet: Knowl-
Yameng Huang, Jian Jiao, Xuan-Jing Huang, Nan
edgefilteringandcontrastivelearningforgenerative
Duan,andRuofeiZhang.2020. Anenhancedknowl-
commonsensereasoning. InFindingsoftheAssoci-
edgeinjectionmodelforcommonsensegeneration.
ationforComputationalLinguistics: EMNLP2021.
InProceedingsofthe28thInternationalConference
pages2918–2928.
onComputationalLinguistics.pages2014–2025.
JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
andWilliamBDolan.2016. Adiversity-promoting
Simcse: Simplecontrastivelearningofsentenceem-
objective function for neural conversation models.
beddings. InProceedingsofthe2021Conferenceon
InProceedingsofthe2016ConferenceoftheNorth
EmpiricalMethodsinNaturalLanguageProcessing.
AmericanChapteroftheAssociationforComputa-
pages6894–6910.
tionalLinguistics: HumanLanguageTechnologies.
pages110–119.
AnkushGupta,ArvindAgarwal,PrawaanSingh,and
PiyushRai.2018. Adeepgenerativeframeworkfor ZhongyangLi,XiaoDing,andTingLiu.2018. Gener-
paraphrase generation. In Proceedings of the aaai atingreasonableanddiversifiedstoryendingusing
conferenceonartificialintelligence.volume32(1). sequencetosequencemodelwithadversarialtraining.
InProceedingsofthe27thInternationalConference
MartinHeusel,HubertRamsauer,ThomasUnterthiner, onComputationalLinguistics.pages1033–1043.
BernhardNessler,andSeppHochreiter.2017. Gans
trainedbyatwotime-scaleupdateruleconvergetoa BillYuchenLin,WangchunshuZhou,MingShen,Pei
localnashequilibrium. InI.Guyon,U.VonLuxburg, Zhou, Chandra Bhagavatula, Yejin Choi, and Xi-
S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan, ang Ren. 2020. CommonGen: A constrained text
and R. Garnett, editors, Advances in Neural Infor- generation challenge for generative commonsense
mationProcessingSystems.CurranAssociates,Inc., reasoning. InFindingsoftheAssociationforCom-
volume30. putational Linguistics: EMNLP 2020. Association
forComputationalLinguistics,Online,pages1823–
AriHoltzman,JanBuys,LiDu,MaxwellForbes,and 1840.
Yejin Choi. 2019. The curious case of neural text
degeneration. arXivpreprintarXiv:1904.09751. Chenzhengyi Liu, Jie Huang, Kerui Zhu, and Kevin
Chen-Chuan Chang. 2023. DimonGen: Diversi-
EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen- fiedgenerativecommonsensereasoningforexplain-
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu ingconceptrelationships. InAnnaRogers,Jordan
Chen. 2022. LoRA: Low-rank adaptation of large Boyd-Graber, and Naoaki Okazaki, editors, Pro-
language models. In International Conference on ceedings of the 61st Annual Meeting of the As-
LearningRepresentations. sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
EunJeongHwang,VeronikaThost,VeredShwartz,and Linguistics, Toronto, Canada, pages 4719–4731.
TengfeiMa.2023. Knowledgegraphcompressionen- https://doi.org/10.18653/v1/2023.acl-long.260.
hancesdiversecommonsensegeneration. InHouda
YeLiu,YaoWan,LifangHe,HaoPeng,andPhilipSYu.
Bouamor,JuanPino,andKalikaBali,editors,Pro-
2021. Kg-bart:Knowledgegraph-augmentedbartfor
ceedingsofthe2023ConferenceonEmpiricalMeth-
generativecommonsensereasoning. InProceedings
odsinNaturalLanguageProcessing.Associationfor
of the AAAI Conference on Artificial Intelligence.
Computational Linguistics, Singapore, pages 558–
volume35,pages6418–6425.
572. https://aclanthology.org/2023.emnlp-main.37.
OpenAI.2023a. Gpt-4technicalreport.
DiederikP.KingmaandJimmyLeiBa.2015. Adam:
A method for stochastic optimization. In Proc. of OpenAI.2023b. Introducingchatgpt. https://openai.
ICLR. com/blog/chatgpt. Accessed: 2023-11-23.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan KishorePapineni,SalimRoukos,ToddWard,andWei-
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, JingZhu.2002. Bleu: amethodforautomaticevalu-
VeselinStoyanov,andLukeZettlemoyer.2020. Bart: ationofmachinetranslation. InProceedingsofthe40thannualmeetingoftheAssociationforComputa- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
tionalLinguistics.pages311–318. Weinberger,andYoavArtzi.2020. Bertscore: Evalu-
atingtextgenerationwithbert.
Tianxiao Shen, Myle Ott, Michael Auli, and
Marc’Aurelio Ranzato. 2019. Mixture models for LianminZheng,Wei-LinChiang,YingSheng,Siyuan
diversemachinetranslation:Tricksofthetrade. InIn- Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ternationalconferenceonmachinelearning.PMLR, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
pages5719–5728. Judging llm-as-a-judge with mt-bench and chatbot
arena. arXivpreprintarXiv:2306.05685.
RobynSpeer,JoshuaChin,andCatherineHavasi.2017.
Conceptnet5.5: Anopenmultilingualgraphofgen-
YaomingZhu,SidiLu,LeiZheng,JiaxianGuo,Weinan
eralknowledge. InProceedingsoftheAAAIconfer-
Zhang,JunWang,andYongYu.2018. Texygen: A
enceonartificialintelligence.volume31(1).
benchmarkingplatformfortextgenerationmodels.
In The 41st international ACM SIGIR conference
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
onresearch&developmentininformationretrieval.
Sequencetosequencelearningwithneuralnetworks.
pages1097–1100.
Advancesinneuralinformationprocessingsystems
27.
SupplementaryAppendix
GuyTevetandJonathanBerant.2021. Evaluatingthe
evaluationofdiversityinnaturallanguagegeneration. A MixtureofExperts
InProceedingsofthe16thConferenceoftheEuro-
peanChapteroftheAssociationforComputational Givenaninputx,itscorrespondingLLM-generated
Linguistics: MainVolume.pages326–346.
sentences are divided into three random subsets.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- For each subset G i = {g 1i,...,g ki}, alongside the
bert, Amjad Almahairi, Yasmine Babaei, Nikolay inputx,weconcatenatetheirtokensequenceswith
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
a separate latent variable z , resulting in the final
i
Bhosale, et al. 2023. Llama 2: Open founda- inputxf . Thez isarandomlyinitialisedsequence
tion and fine-tuned chat models. arXiv preprint i i
arXiv:2307.09288. oftokens.
Cunxiang Wang, Shuailong Liang, Yili Jin, Yilong xf = z [CLS]x[SEP]gi[SEP]...gi (1)
Wang,XiaodanZhu,andYueZhang.2020. Semeval- i i 1 k
2020task4: Commonsensevalidationandexplana-
We train the model using the hard Expectation-
tion. InProceedingsoftheFourteenthWorkshopon
SemanticEvaluation.pages307–321. Maximization (EM) approach, where during the
E-step, for each
xf
and its corresponding target
LeanWang,LeiLi,DamaiDai,DeliChen,HaoZhou, i
ytarget
,weidentifytheinputthatyieldsthehighest
FandongMeng,JieZhou,andXuSun.2023. Label i
words are anchors: An information flow perspec- probability as the best training example, with θ
tiveforunderstandingin-contextlearning. InHouda representingthegeneratormodelparameters:
Bouamor,JuanPino,andKalikaBali,editors,Pro-
Themodelistrainedusinghard-EMbyassign-
ceedingsofthe2023ConferenceonEmpiricalMeth-
ingfullresponsibilitytotheexpertwiththelargest
odsinNaturalLanguageProcessing.Associationfor
ComputationalLinguistics,Singapore,pages9840– jointprobability.
IntheE-step,foreachinputxfin
i
9855. andtargetytgt ,choosethebestinputwiththehigh-
i
estprobability,toconstructthetrainingexamples,
Wenhao Yu, Chenguang Zhu, Lianhui Qin, Zhihan
Zhang,TongZhao,andMengJiang.2022. Diversi- whereθ isthemodel’sparameters.
fyingcontentgenerationforcommonsensereasoning
with mixture of knowledge graph experts. In Pro- ytgt = argmaxp(y |xf;θ) (2)
i j i
ceedingsofthe2ndWorkshoponDeepLearningon yj
GraphsforNaturalLanguageProcessing(DLG4NLP
2022).pages1–11. Subsequently,intheM-step,weusetheseselected
trainingexamplestofine-tunethegeneratormodels.
Tianhui Zhang, Danushka Bollegala, and Bei Peng.
2023. Learningtopredictconceptorderingforcom- Duringinference,weinputalldiversified,context-
monsensegeneration. InJongC.Park,YukiArase, aware inputs into the generator model to yield a
BaotianHu,WeiLu,DerryWijaya,AyuPurwarianti,
rangeofdiverseoutputs.
and Adila Alfa Krisnadhi, editors, Proceedings of
the13thInternationalJointConferenceonNatural
B Impactoftemperaturesamplingonthe
LanguageProcessingandthe3rdConferenceofthe
diversityandquality
Asia-PacificChapteroftheAssociationforComputa-
tionalLinguistics(Volume2: ShortPapers).Associa-
In this section, we investigate the impact of tem-
tionforComputationalLinguistics,NusaDua,Bali,
pages10–19. peratureonvariousmethodsontheCommonGenSemanticDiversity⇓ CorpusDiversity⇑ PairwiseDiversity⇓ Quality⇑ Combined
self-cosSim self-BERTScore Entropy-4 Distinct-4 self-BLEU-3 self-BLEU-4 BLEU-3 BLEU-4 SPICE BERTScore Harmonic⇑ FBD⇓
Fine-tune 100.0 100.0 9.15 14.1 100.0 100.0 45.6 34.9 34.4 71.3 0.0 69.7
Default 100.0 100.0 9.12 15 100.0 100.0 40.8 30.4 28.5 67.6 0.0 69.7
T=0
Diversified 86.7 74 10.8 77.8 52.2 43.4 46.2 36.4 28.6 66.6 61.2 54.9
ICD 86 72.2 10.9 80 46.5 37.6 48.1 38.3 29.1 67.7 65 53.5
Fine-tune 83.6 81.6 10.6 65.4 63.8 55.7 56.3 46.8 36.1 73.8 55.4 58.3
Default 96.1 94.9 9.75 36.9 88.9 86.8 47.6 37.3 29.5 69.6 22.4 63.7
T=0.5
Diversified 86.4 73 10.9 79.8 49.9 40.8 46 36.5 28.6 66.5 62.6 55.4
ICD 85.1 70 10.9 84.2 39.4 29.5 48.6 39.1 29.2 68.1 69.3 53.4
Fine-tune 64.7 55.9 11.4 91.1 26.9 17.9 41.2 32.1 30.3 64.2 72.1 51.9
default 93.3 88.7 10.2 53.7 77.2 72.4 50.8 40.9 30.1 70.4 39.6 60.2
T=1
diversified 85.2 69.8 11.0 83.7 44.4 34.9 44.3 34.6 28.5 65.0 65.4 53.9
ICD 83.5 66.2 11.0 88.5 31.0 21.0 47.4 37.7 29.1 67.4 72.7 51.8
Fine-tune 25.7 0.0 11.9 100.0 2.5 1.8 8.1 4.3 11 16 27.5 67.8
Default 90.4 81.5 10.5 68.4 63.5 56.1 51.4 41.9 29.9 70.1 54 56.5
T=1.5
Diversified 67.7 59.3 11.2 89.5 30.6 22.3 39.3 29.7 26.9 61.9 68.9 54
ICD 78.3 59.8 11.2 92.8 20.9 12.4 44.1 34.7 27.9 65.4 74.9 51.6
Table5: Diversityandqualityscoresonfourtemperaturesettings(T =0/0.5/1/1.5)ontheCommonGendataset.
TheresultsshowthatourproposedmethodICDperformswellacrossdifferenttemperatures. Bestresultsforeach
metricateachtemperaturesettingareshowninitalics,whilethebestperformingICLresultsareshowninbold.
dataset. Althoughtherearevarioussamplingmeth- D Candidatemetricsforcalculating
odsbeyondtemperature,suchastop-kandtop-p, HarmonicMeans
wespecificallyfocusonthegeneralperformanceof
Inthepaper,weuseself-BLEU-4andBERTScore
ICDunderdifferenttemperaturesettings. Ourex-
to calculate the harmonic means as one of the
perimentinconductedontheGPT-3.5-turbo-0613.
combinedmetricsbecauseself-BLEU(Zhuetal.,
Table 5 demonstrates that ICD consistently out-
2018) evaluates the n-gram overlap among sen-
performsbothdefaultanddiversifiedontheCom-
tences within the generated set, providing a mea-
binedmetricsacrossalltemperaturesettings,which
sure of lexical diversity. On the other hand,
aligns with our findings in § 5.1. Moreover, ICD
BERTScore (Zhang et al., 2020) assesses the se-
exhibits less sensitivity to temperature variations
manticsimilaritybetweenthegeneratedsentences
comparedtotheotherbaselinesandperformsbetter
andthegoldensentencesfromthedataset,captur-
onCombinedmetricswiththeincreaseoftemper-
ingthequalityaspectfromasemanticperspective.
ature, which can be considered as an additional
Wealsouseotherdiversityandqualitymetrics’
advantageofourproposedmethod.
combinationasthecandidatemetricstocalculate
Furthermore, we observe that the fine-tune theharmonicmeans. IntheTable7,wecouldsee
methodisalsosignificantlyinfluencedbytemper- thatineachmetrics’pair,theICDmethodhasthe
aturesamplingontheGCRtask. AtT = 1.5,the highestscoreamongtheICL-basedapproaches. It
default baseline, which applies ICL on the same alsohascomparableperformancewiththefine-tune
base model GPT-3.5-turbo, outperforms the fine- method.
tunemethod. Thefine-tunedmodelgeneratesre-
sponses that are of very low quality, consisting E LLMPromptTemplates
mostlyofnonsensicalwordcombinations.
Figure5showsthetemplatesthatareusedforthe
two GCR tasks: CommonGen and ConVE. The
default prompt is adapted from Li et al. (2023)
C FullresultsonVicuna-13bmodel and are task-specific. On the other hand, the di-
versifiedpromptmodifiesthedefaultpromptby
appendingatask-independentinstructionthatfirst
Table 6 shows the full result on the open source checkswhetherthediversityofthesentencesgener-
Vicuna-13bmodelacrossthreedatasets. Itrecon- atedinStep1islow,andifpresentsthegenerated
firmsICD’sabilitytobalancebothqualityanddi- sentencestotheLLMandre-promptsittogenerate
versityaccordingtothecombinedmetrics. Further- morediversesetofsentences.
more,wefindthatmethodsusingtheVicunamodel WeuseGPT3.5-turbotopredictthediversityof
showlowerqualitythanthoseusingGPT-3.5-turbo agivensetofsentencesusingthepromptshownin
whilegeneratingmorediversesentences. Figure6. ThispromptusesfivediversitycategoriesSemanticDiversity⇓ CorpusDiversity⇑ PairwiseDiversity⇓ Quality⇑ Combined
self-cosSim self-BERTScore Entropy-4 Distinct-4 self-BLEU-3 self-BLEU-4 BLEU-3 BLEU-4 SPICE BERTScore Harmonic⇑ FBD⇓
CommonGen
Fine-tune 59.6 49.9 11.4 93.3 22.8 14.5 35.8 26.8 27.6 59.1 69.9 52.4
default 82.2 73.8 10.9 74.9 52.9 45.4 44.6 34.9 29.1 67.1 60.2 56.2
diversified 59.1 53.3 11.3 91.3 23.6 16.4 32.6 23.7 24.3 58.2 68.6 53.2
ICD 59.3 49.8 11.3 93.7 11.3 5.8 34.2 24.9 25.5 60.1 73.4 51.0
ComVE
Fine-tune 60.4 45.8 9.6 93.8 17.1 14.1 27.9 19 31.1 52.3 65.0 47.3
default 75.7 57.1 9.8 78.0 36.7 31.1 23.8 16.9 33 49.2 57.4 60.8
diversified 64.7 42.3 10.0 89.3 13.4 8.8 23.2 16.0 32.6 49.8 64.4 56.9
ICD 61.5 37.3 10.0 90.1 5.8 3.0 22.7 15.7 32.5 48.8 65.1 58.2
DimonGen
Fine-tune 41 29.5 10.4 99 5 2.2 15.4 8.9 14.6 39.4 56.2 52.8
default 64.0 48.6 10.3 95.0 17.9 13.1 13.6 7.9 14.4 41.3 56 61.1
diversified 55.2 45.4 10.3 97 11.9 7.3 12.1 6.7 13.4 39.8 55.7 62
ICD 53.1 37.0 10.4 98 2.4 0.9 12.7 7.3 13.6 39 56.6 61.1
Table6: PerformanceonCommonGen,ComVEandDimonGenwithVicuna-13b. Bestresultsoneachtaskfor
eachmetricareshowninitalics,whilethebestperformingICLresultsareshowninbold.
CommonGen ComVE Dimongen
Candidatemetrics Fine-tune Default Diversified ICD Fine-tune Default Diversified ICD Fine-tune Default Diversified ICD
self-BLEU4+BERTScore 72.1 39.6 65.4 72.7 67.2 54.9 67.4 69.0 58.5 52.1 55.9 58.2
self-cosSim+SPICE 32.6 11.0 19.5 21.1 36.3 22.3 28.7 31.2 24.3 19.6 22.4 22.7
self-BERTScore+BLEU3 42.6 18.5 35.9 39.5 37.1 27.0 35.9 36.4 28.0 20.5 18.8 21.2
Table7:DifferentcandidatemetricstocalculatetheHarmonicMeans. Themetricwith‘self’ineachlineisdiversity
metricsandtheotherisqualitymetric. Bestresultsoneachtaskforeachmetricareshowninitalics,whilethebest
performingICLresultsareshowninbold.
(i.e. verysimilar,somewhatsimilar,neutral,some- diversity,andnotsomuchontheircommonsense
whatdiverse,andhighlydiverse)withincreasing quality,whichweevaluateseparatelyusingsemi-
diversity with their definitions. Next, the set of automatic metrics by comparing against human-
sentencestobeevaluatedfortheirdiversityispre- writtenreferencesentencesintheevaluationbench-
sented. Finally,theexpectedoutputformatofthe marks. Moreover,weinformedtheannotatorsthat
predictions is described at the end of the prompt. theirevaluationswouldbe used inacomparative
As recommended by Chen et al. (2023), we do analysiswiththescoresgeneratedbyanLLM.
notrequiretheLLMtoprovidereasonsforitspre-
G AddionalGenerationExamples
dictionsbecauseitsometimesforcesthemodelto
focusonthereasongenerationthantheprediction.
We show additional sentences generated by our
After the LLM’s evaluation, the classification
proposedmethodsfortheCommonGen,ComVE
are mapped to values from 1 to 5 where 1 being
andDimonGendatasetsinTable8.
highlysimilarto5beinghighlydiverse. Foreach
sentencesset,werequesttheLLMtoperformthe
evaluationthreetimesandtaketheaveragescore.
F HumanEvaluation
Ashuman-annotators,werecruitedeightgraduate
studentsfromthedepartmentofcomputerscience
who specialise in NLP and are fluent speakers in
English. Weprovidedthehumanannotatorswith
thesamesetofinstructionsasweprovidedtothe
LLMs. The annotators were allowed to deviate
fromtheprovidedinstructionswhenhandlingedge
cases. ApartfromtheinstructioninFigure6, we
alsoinstructthehumanannotatorstofocusontheCommonGen
Keyword branchgroundclimbtreejump
Sentences Thesquirrelleapedfromonebranchtoanother,skillfullymaneuveringitswayupthetreewithouttouchingtheground.
Asquirrelclimbsupatreebranch,jumpsontotheground,andstartstoclimbanothertree.
Thecattriedtoclimbthetree,butitfelloffthebranchandlandedontheground.
Themonkeyenjoyedswingingfrombranchtobranchandwouldoccasionallyjumpdowntotheground.
Asquirreljumpsfromatreebranchtothegroundandclimbsupanothertree.
Amonkeyclimbsupatreebranchandjumpsdowntotheground.
Keyword sitfronttablefoodlaugh
Sentences Acouplesitsatatableinfront,enjoyingtheirmealandsharinglaughter.
Agroupoffriendssitinfrontofatablefilledwithdeliciousfood,laughingandenjoyingeachother’scompany.
Friendsgatheraroundatable,sittinginthefront,whiletheyeatandsharelaughter.
Peoplesitinfrontofatable,laughingwhileenjoyingtheirfood.
Colleaguessittogetheratatable,eatingandlaughingastheyenjoytheirmeal.
Friendssitinfrontofatable,enjoyinggoodfoodandlaughingtogether.
ComVE
Statement Sheparachutedintothegrocerystorefromaplane.
Explanation Parachutingisnotatypicalmodeoftransportationforgroceryshopping.
Grocerystoresarenotdesignedtoaccommodateparachutelandings.
Itishighlyunlikelyforsomeonetoparachuteintoagrocerystorefromaplane.
Statement Youcandriveafterdrinking.
Explanation Drivingundertheinfluenceofalcoholisaseriousoffenceandcanresultinaccidentsorharmtooneselforothers.
Youshouldneverdrinkanddriveasitimpairsyourjudgmentandreactiontime.
Itisillegalandextremelydangeroustodriveafterconsumingalcohol.
DimonGen
Keyword cloudmountain
Sentences Asweclimbedthemountain,thecloudsgrewthicker,obscuringourview.
Themountainstoodtallandmajestic,withcloudsswirlingarounditspeaks.
Themountainpeakpiercedthroughtheclouds,reachingtowardsthesky.
Keyword racevictory
Sentences Theunderdogpulledoffanunexpectedvictoryintherace,leavingthefavoritetrailingbehind.
Withaburstofspeedanddetermination,therunnersprintedtowardsthefinishline,securingatriumphantvictory.
Afterafiercerace,thechampioncelebratedtheirvictorywithacrowdcheeringandfireworkslightingupthesky.
Table8: MoreexamplesproducedbyourproposedICDmethodontheCommonGen,ComVEandDimonGen
datasets.CommonGen/DimonGen ComVE
Examples: Examples:
Given several key words: [SRC], Given a counterfactual statement: [SRC],
generate one coherent sentence using generate one commonsense-making
background commonsense knowledge: [TGT] explanation for the statement: [TGT]
Test instruction: Test instruction:
Given several key words: [INPUT], Given several key words: [INPUT],
generate one coherent sentence using generate one commonsense-making
background commonsense knowledge: explanation for the statement : [OUTPUT]
[OUTPUT]
(a) (b)
(a)Defaultinstructions
CommonGen/DimonGen ComVE
Examples: Examples:
Given several key words: [SRC], Given a counterfactual statement: [SRC],
generate one coherent sentences using generate one commonsense-making
background commonsense knowledge: [TGT] explanations for the statement: [TGT]
Test instruction: Test instruction:
Step1: Given several key words: [INPUT], Step1: Given several key words: [INPUT],
generate [N] different and coherent sentences generate [N] commonsense-making
using background commonsense knowledge: explanations for the statement: [PRV]
[PRV]
(If same sentences in the generation list)
(If same sentences in the generation list)
Step2: You have generated the following
Step2: You have generated the following sentences: [PRV], try to provide other
sentences: [PRV], try to provide other reasonable sentences: [OUTPUT]
reasonable sentences: [OUTPUT]
(a) (b)
(b)Diversifiedinstructions
Figure5:ThetemplatesusedbythedefaultandthediversifiedpromptinstructionsfortheCommonGen/DimonGen
(shownontheleft,(a))andComVE(shownontheright,(b))tasks. Few-shotexamplesareincludedineachprompt
where[SRC]denotesthesetofinputconceptsand[TGT]thecorrespondingsentencesinCommonGen. Foragiven
setof[INPUT]concepts,theLLMisthenrequiredtogeneratesentencesattheslot[OUTPUT].Figure 6: The instructions provided to GPT3.5-turbo for predicting the diversity of a given set of sentences.
Diversityispredictedaccordingtofivecategories: verysimilar,somewhatsimilar,neutral,somewhatdiverse,and
highlydiverse. Definitionsofthecategoriesareincludedwithintheinstructions. Next,thesetofsentencestobe
evaluatedfortheirdiversityispresented. Finally,theexpectedoutputformatofthepredictionsisdescribedatthe
endoftheprompt. AsrecommendedbyChenetal.(2023),wedonotrequiretheLLMtoprovidereasonsforits
predictionsbecauseitsometimesforcesthemodeltofocusonthereasongenerationthantheprediction.