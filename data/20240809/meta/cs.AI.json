[
    {
        "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics",
        "authors": "Ruining LiChuanxia ZhengChristian RupprechtAndrea Vedaldi",
        "links": "http://arxiv.org/abs/2408.04631v1",
        "entry_id": "http://arxiv.org/abs/2408.04631v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04631v1",
        "summary": "We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.",
        "updated": "2024-08-08 17:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04631v1"
    },
    {
        "title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP",
        "authors": "Danlu ChenFreda ShiAditi AgarwalJacobo MyerstonTaylor Berg-Kirkpatrick",
        "links": "http://arxiv.org/abs/2408.04628v1",
        "entry_id": "http://arxiv.org/abs/2408.04628v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04628v1",
        "summary": "Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.",
        "updated": "2024-08-08 17:58:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04628v1"
    },
    {
        "title": "Transformer Explainer: Interactive Learning of Text-Generative Models",
        "authors": "Aeree ChoGrace C. KimAlexander KarpekovAlec HelblingZijie J. WangSeongmin LeeBenjamin HooverDuen Horng Chau",
        "links": "http://arxiv.org/abs/2408.04619v1",
        "entry_id": "http://arxiv.org/abs/2408.04619v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04619v1",
        "summary": "Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.",
        "updated": "2024-08-08 17:49:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04619v1"
    },
    {
        "title": "Better Alignment with Instruction Back-and-Forth Translation",
        "authors": "Thao NguyenJeffrey LiSewoong OhLudwig SchmidtJason WestonLuke ZettlemoyerXian Li",
        "links": "http://arxiv.org/abs/2408.04614v1",
        "entry_id": "http://arxiv.org/abs/2408.04614v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04614v1",
        "summary": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
        "updated": "2024-08-08 17:42:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04614v1"
    },
    {
        "title": "Inference with the Upper Confidence Bound Algorithm",
        "authors": "Koulik KhamaruCun-Hui Zhang",
        "links": "http://arxiv.org/abs/2408.04595v1",
        "entry_id": "http://arxiv.org/abs/2408.04595v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04595v1",
        "summary": "In this paper, we discuss the asymptotic behavior of the Upper Confidence\nBound (UCB) algorithm in the context of multiarmed bandit problems and discuss\nits implication in downstream inferential tasks. While inferential tasks become\nchallenging when data is collected in a sequential manner, we argue that this\nproblem can be alleviated when the sequential algorithm at hand satisfies\ncertain stability property. This notion of stability is motivated from the\nseminal work of Lai and Wei (1982). Our first main result shows that such a\nstability property is always satisfied for the UCB algorithm, and as a result\nthe sample means for each arm are asymptotically normal. Next, we examine the\nstability properties of the UCB algorithm when the number of arms $K$ is\nallowed to grow with the number of arm pulls $T$. We show that in such a case\nthe arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number\nof near-optimal arms are large.",
        "updated": "2024-08-08 17:11:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04595v1"
    }
]