[
    {
        "title": "Arctic-TILT. Business Document Understanding at Sub-Billion Scale",
        "authors": "Łukasz BorchmannMichał PietruszkaWojciech JaśkowskiDawid JurkiewiczPiotr HalamaPaweł JóziakŁukasz GarncarekPaweł LiskowskiKarolina SzyndlerAndrzej GretkowskiJulita OłtusekGabriela NowakowskaArtur ZawłockiŁukasz DuhrPaweł DydaMichał Turski",
        "links": "http://arxiv.org/abs/2408.04632v1",
        "entry_id": "http://arxiv.org/abs/2408.04632v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04632v1",
        "summary": "The vast portion of workloads employing LLMs involves answering questions\ngrounded on PDF or scan content. We introduce the Arctic-TILT achieving\naccuracy on par with models 1000$\\times$ its size on these use cases. It can be\nfine-tuned and deployed on a single 24GB GPU, lowering operational costs while\nprocessing Visually Rich Documents with up to 400k tokens. The model\nestablishes state-of-the-art results on seven diverse Document Understanding\nbenchmarks, as well as provides reliable confidence scores and quick inference,\nwhich are essential for processing files in large-scale or time-sensitive\nenterprise environments.",
        "updated": "2024-08-08 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04632v1"
    },
    {
        "title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP",
        "authors": "Danlu ChenFreda ShiAditi AgarwalJacobo MyerstonTaylor Berg-Kirkpatrick",
        "links": "http://arxiv.org/abs/2408.04628v1",
        "entry_id": "http://arxiv.org/abs/2408.04628v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04628v1",
        "summary": "Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.",
        "updated": "2024-08-08 17:58:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04628v1"
    },
    {
        "title": "Transformer Explainer: Interactive Learning of Text-Generative Models",
        "authors": "Aeree ChoGrace C. KimAlexander KarpekovAlec HelblingZijie J. WangSeongmin LeeBenjamin HooverDuen Horng Chau",
        "links": "http://arxiv.org/abs/2408.04619v1",
        "entry_id": "http://arxiv.org/abs/2408.04619v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04619v1",
        "summary": "Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.",
        "updated": "2024-08-08 17:49:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04619v1"
    },
    {
        "title": "Better Alignment with Instruction Back-and-Forth Translation",
        "authors": "Thao NguyenJeffrey LiSewoong OhLudwig SchmidtJason WestonLuke ZettlemoyerXian Li",
        "links": "http://arxiv.org/abs/2408.04614v1",
        "entry_id": "http://arxiv.org/abs/2408.04614v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04614v1",
        "summary": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
        "updated": "2024-08-08 17:42:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04614v1"
    },
    {
        "title": "Code-switching in text and speech reveals information-theoretic audience design",
        "authors": "Debasmita BhattacharyaMarten van Schijndel",
        "links": "http://arxiv.org/abs/2408.04596v1",
        "entry_id": "http://arxiv.org/abs/2408.04596v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04596v1",
        "summary": "In this work, we use language modeling to investigate the factors that\ninfluence code-switching. Code-switching occurs when a speaker alternates\nbetween one language variety (the primary language) and another (the secondary\nlanguage), and is widely observed in multilingual contexts. Recent work has\nshown that code-switching is often correlated with areas of high information\nload in the primary language, but it is unclear whether high primary language\nload only makes the secondary language relatively easier to produce at\ncode-switching points (speaker-driven code-switching), or whether\ncode-switching is additionally used by speakers to signal the need for greater\nattention on the part of listeners (audience-driven code-switching). In this\npaper, we use bilingual Chinese-English online forum posts and transcripts of\nspontaneous Chinese-English speech to replicate prior findings that high\nprimary language (Chinese) information load is correlated with switches to the\nsecondary language (English). We then demonstrate that the information load of\nthe English productions is even higher than that of meaning equivalent Chinese\nalternatives, and these are therefore not easier to produce, providing evidence\nof audience-driven influences in code-switching at the level of the\ncommunication channel, not just at the sociolinguistic level, in both writing\nand speech.",
        "updated": "2024-08-08 17:14:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04596v1"
    }
]