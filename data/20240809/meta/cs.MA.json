[
    {
        "title": "Learning Fair Cooperation in Mixed-Motive Games with Indirect Reciprocity",
        "authors": "Martin SmitFernando P. Santos",
        "links": "http://dx.doi.org/10.24963/ijcai.2024/25",
        "entry_id": "http://arxiv.org/abs/2408.04549v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04549v1",
        "summary": "Altruistic cooperation is costly yet socially desirable. As a result, agents\nstruggle to learn cooperative policies through independent reinforcement\nlearning (RL). Indirect reciprocity, where agents consider their interaction\npartner's reputation, has been shown to stabilise cooperation in homogeneous,\nidealised populations. However, more realistic settings are comprised of\nheterogeneous agents with different characteristics and group-based social\nidentities. We study cooperation when agents are stratified into two such\ngroups, and allow reputation updates and actions to depend on group\ninformation. We consider two modelling approaches: evolutionary game theory,\nwhere we comprehensively search for social norms (i.e., rules to assign\nreputations) leading to cooperation and fairness; and RL, where we consider how\nthe stochastic dynamics of policy learning affects the analytically identified\nequilibria. We observe that a defecting majority leads the minority group to\ndefect, but not the inverse. Moreover, changing the norms that judge in and\nout-group interactions can steer a system towards either fair or unfair\ncooperation. This is made clearer when moving beyond equilibrium analysis to\nindependent RL agents, where convergence to fair cooperation occurs with a\nnarrower set of norms. Our results highlight that, in heterogeneous populations\nwith reputations, carefully defining interaction norms is fundamental to tackle\nboth dilemmas of cooperation and of fairness.",
        "updated": "2024-08-08 15:57:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04549v1"
    },
    {
        "title": "Emergence in Multi-Agent Systems: A Safety Perspective",
        "authors": "Philipp AltmannJulian SchönbergerSteffen IlliumMaximilian ZornFabian RitzTom HaiderSimon BurtonThomas Gabor",
        "links": "http://arxiv.org/abs/2408.04514v1",
        "entry_id": "http://arxiv.org/abs/2408.04514v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04514v1",
        "summary": "Emergent effects can arise in multi-agent systems (MAS) where execution is\ndecentralized and reliant on local information. These effects may range from\nminor deviations in behavior to catastrophic system failures. To formally\ndefine these effects, we identify misalignments between the global inherent\nspecification (the true specification) and its local approximation (such as the\nconfiguration of different reward components or observations). Using\nestablished safety terminology, we develop a framework to understand these\nemergent effects. To showcase the resulting implications, we use two broadly\nconfigurable exemplary gridworld scenarios, where insufficient specification\nleads to unintended behavior deviations when derived independently. Recognizing\nthat a global adaptation might not always be feasible, we propose adjusting the\nunderlying parameterizations to mitigate these issues, thereby improving the\nsystem's alignment and reducing the risk of emergent failures.",
        "updated": "2024-08-08 15:15:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04514v1"
    },
    {
        "title": "Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal Policy Optimization",
        "authors": "Aditya KapoorBenjamin FreedHowie ChosetJeff Schneider",
        "links": "http://arxiv.org/abs/2408.04295v1",
        "entry_id": "http://arxiv.org/abs/2408.04295v1",
        "pdf_url": "http://arxiv.org/pdf/2408.04295v1",
        "summary": "Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem,\nwherein the sheer difficulty in ascribing credit to individual agents' actions\nscales poorly with team size. In this paper, we propose a multi-agent\nreinforcement learning algorithm that adapts recent developments in credit\nassignment to improve upon MAPPO. Our approach leverages partial reward\ndecoupling (PRD), which uses a learned attention mechanism to estimate which of\na particular agent's teammates are relevant to its learning updates. We use\nthis estimate to dynamically decompose large groups of agents into smaller,\nmore manageable subgroups. We empirically demonstrate that our approach,\nPRD-MAPPO, decouples agents from teammates that do not influence their expected\nfuture reward, thereby streamlining credit assignment. We additionally show\nthat PRD-MAPPO yields significantly higher data efficiency and asymptotic\nperformance compared to both MAPPO and other state-of-the-art methods across\nseveral multi-agent tasks, including StarCraft II. Finally, we propose a\nversion of PRD-MAPPO that is applicable to \\textit{shared} reward settings,\nwhere PRD was previously not applicable, and empirically show that this also\nleads to performance improvements over MAPPO.",
        "updated": "2024-08-08 08:18:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.04295v1"
    },
    {
        "title": "Asynchronous Credit Assignment Framework for Multi-Agent Reinforcement Learning",
        "authors": "Yongheng LiangHejun WuHaitao WangHao Cai",
        "links": "http://arxiv.org/abs/2408.03692v1",
        "entry_id": "http://arxiv.org/abs/2408.03692v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03692v1",
        "summary": "Credit assignment is a core problem that distinguishes agents' marginal\ncontributions for optimizing cooperative strategies in multi-agent\nreinforcement learning (MARL). Current credit assignment methods usually assume\nsynchronous decision-making among agents. However, a prerequisite for many\nrealistic cooperative tasks is asynchronous decision-making by agents, without\nwaiting for others to avoid disastrous consequences. To address this issue, we\npropose an asynchronous credit assignment framework with a problem model called\nADEX-POMDP and a multiplicative value decomposition (MVD) algorithm. ADEX-POMDP\nis an asynchronous problem model with extra virtual agents for a decentralized\npartially observable markov decision process. We prove that ADEX-POMDP\npreserves both the task equilibrium and the algorithm convergence. MVD utilizes\nmultiplicative interaction to efficiently capture the interactions of\nasynchronous decisions, and we theoretically demonstrate its advantages in\nhandling asynchronous tasks. Experimental results show that on two asynchronous\ndecision-making benchmarks, Overcooked and POAC, MVD not only consistently\noutperforms state-of-the-art MARL methods but also provides the\ninterpretability for asynchronous cooperation.",
        "updated": "2024-08-07 11:13:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03692v1"
    },
    {
        "title": "Combining Diverse Information for Coordinated Action: Stochastic Bandit Algorithms for Heterogeneous Agents",
        "authors": "Lucia GordonEsther RolfMilind Tambe",
        "links": "http://arxiv.org/abs/2408.03405v1",
        "entry_id": "http://arxiv.org/abs/2408.03405v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03405v1",
        "summary": "Stochastic multi-agent multi-armed bandits typically assume that the rewards\nfrom each arm follow a fixed distribution, regardless of which agent pulls the\narm. However, in many real-world settings, rewards can depend on the\nsensitivity of each agent to their environment. In medical screening, disease\ndetection rates can vary by test type; in preference matching, rewards can\ndepend on user preferences; and in environmental sensing, observation quality\ncan vary across sensors. Since past work does not specify how to allocate\nagents of heterogeneous but known sensitivity of these types in a stochastic\nbandit setting, we introduce a UCB-style algorithm, Min-Width, which aggregates\ninformation from diverse agents. In doing so, we address the joint challenges\nof (i) aggregating the rewards, which follow different distributions for each\nagent-arm pair, and (ii) coordinating the assignments of agents to arms.\nMin-Width facilitates efficient collaboration among heterogeneous agents,\nexploiting the known structure in the agents' reward functions to weight their\nrewards accordingly. We analyze the regret of Min-Width and conduct\npseudo-synthetic and fully synthetic experiments to study the performance of\ndifferent levels of information sharing. Our results confirm that the gains to\nmodeling agent heterogeneity tend to be greater when the sensitivities are more\nvaried across agents, while combining more information does not always improve\nperformance.",
        "updated": "2024-08-06 18:56:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03405v1"
    }
]