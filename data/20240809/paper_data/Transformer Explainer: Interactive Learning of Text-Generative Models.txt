TRANSFORMER EXPLAINER: Interactive Learning of Text-Generative Models
AereeCho*1, GraceC.Kim*1, AlexanderKarpekov*1,
AlecHelbling1,ZijieJ.Wang1,SeongminLee1,BenjaminHoover1,2,DuenHorng(Polo)Chau1
Figure1:TRANSFORMEREXPLAINERhelpsusers(A)visuallyexaminehowatext-generativeTransformermodel(GPT-2)transforms
inputtexttopredictnexttokens,(B)interactivelyexperimentinrealtimewithkeymodelparametersliketemperaturetounderstand
prediction determinism, and (C) transition seamlessly between abstraction levels to visualize the interplay between low-level
mathematicaloperationsandhigh-levelmodelstructures.
ABSTRACT understandingandengagement[12].Demystifyingthisarchitecture
iscrucialfornon-expertsinterestedinlearningaboutit. Existing
Transformershaverevolutionizedmachinelearning,yettheirinner
resources,suchasblogposts[2],videotutorials[1,9],and3Dvisu-
workingsremainopaquetomany.WepresentTRANSFORMEREX-
alizations[4]oftenemphasizemathematicalintricaciesandmodel
PLAINER,aninteractivevisualizationtooldesignedfornon-experts
implementations,whichcanoverwhelmbeginners.Meanwhile,visu-
tolearnaboutTransformersthroughtheGPT-2model. Ourtool
alizationtoolsdesignedforAIpractitionersfocusoninterpretability
helpsusersunderstandcomplexTransformerconceptsbyintegrating
attheneuronandlayerlevels,whichcanbechallengingfornon-
amodeloverviewandenablingsmoothtransitionsacrossabstraction
expertstograsp[3].Toaddressthisresearchgap,wecontribute:
levelsofmathematicaloperationsandmodelstructures. Itrunsa
liveGPT-2instancelocallyintheuser’sbrowser,empoweringusers 1. TRANSFORMEREXPLAINER,anopen-source,web-basedin-
toexperimentwiththeirowninputandobserveinreal-timehow teractivevisualizationtooldesignedfornon-expertstolearn
theinternalcomponentsandparametersoftheTransformerwork aboutboththeTransformer’shigh-levelmodelstructureandlow-
togethertopredictthenexttokens.Ourtoolrequiresnoinstallation level mathematical operations (Fig. 1). Our tool explains the
orspecialhardware, broadeningthepublic’seducationaccessto Transformerthroughitsapplicationintextgeneration,oneofits
moderngenerativeAItechniques.Ouropen-sourcedtoolisavailable mostrecognizeduses.ItadoptstheSankeydiagramvisualdesign,
athttps://poloclub.github.io/transformer-explainer/. inspiredbyrecentstudiesviewingtheTransformerasadynamic
Avideodemoisavailableathttps://youtu.be/ECR4oAwocjs. system [5], to emphasize how input data “flows” through the
model’scomponents[6].TheSankeydiagrameffectivelyillus-
1 INTRODUCTION trateshowinformationmovesthroughthemodel,showcasing
howinputsundergoprocessingandtransformationsviaTrans-
The Transformer [10] has gained popularity as a neural network
architectureacrossnumeroustasks,fromtexttovision.Despiteits
formeroperations.TRANSFORMEREXPLAINERhelpsusersgain
acomprehensiveunderstandingofthecomplexconceptswithin
increasingwidespreaduse,particularlyinAIchatbots(e.g.,Chat-
Transformersbytightlyintegratingamodeloverviewthatsum-
GPT,Gemini),itsinnerworkingsoftenremainopaque,hindering
marizesaTransformer’sstructureandenablinguserstosmoothly
transition between multiple abstraction levels to visualize the
*Equalcontribution.
interplaybetweenlow-levelmathematicaloperationsandhigh-
1 Georgia Tech. {aeree|gracekim3|alex.karpekov|alechelbling|
levelmodelstructures(Fig.1C)[11].
jayw|seongmin|bhoov|polo}@gatech.edu
2IBMResearch. 2. Open-source,web-basedimplementationwithreal-timein-
ference. Unlikemanyexistingtoolsthatrequirecustomsoft-
ware installations or lack inference capabilities [3], TRANS-
FORMER EXPLAINERintegratesaliveGPT-2modelthatruns
locallyintheuser’sbrowserusingmodernfront-endframeworks.
4202
guA
8
]GL.sc[
1v91640.8042:viXraadvances in generative AI. She has observed that some students
viewTransformer-basedmodelsas“magic,”andsomewishtolearn
howtheyworkbutareunsurewheretostart. Toaddressthis,she
directsstudentstoTRANSFORMER EXPLAINER,whichprovides
aninteractiveoverviewoftheTransformer(Fig.1),encouraging
activeexperimentationandlearning.Withover300studentsinher
class,theabilityofTRANSFORMEREXPLAINERtorunentirelyin
students’browserswithoutsoftwareinstallationorspecialhardware
is a significant advantage, eliminating concerns about managing
Figure2:Thetemperaturesliderletsusersinteractivelyexperiment
softwareorhardwaresetup.Thetoolintroducesstudentstocomplex
withthetemperatureparameter’simpactonthenexttoken’sprobability
distribution.Left:lowertemperaturessharpenthedistribution,making mathematicaloperations, suchasattentioncomputation, through
outputsmorepredictable. Right: highertemperaturessmooththe animationsandinteractivereversibleabstractions(Fig.1C).This
distribution,resultinginlesspredictableoutputs. approachhelpsstudentsgainbothahigh-levelunderstandingofthe
operationsandlower-leveldetailsthatproducetheresults.Professor
Users can interactively experiment with their own input text Rousseauisalsoawarethatthetechnicalcapabilitiesandlimitations
(Fig. 1A), and observe in real time how the internal compo- ofTransformersaresometimesobscuredbyanthropomorphism(e.g.,
nentsandparametersoftheTransformerworktogethertopredict thetemperatureparameterbeingviewedasa“creativity”control).
thenexttokens(Fig.1B).Ourapproachbroadenseducational Byencouragingstudentstoexperimentwiththetemperatureslider
access to modern generative AI techniques without requiring (Fig.1B),shedemonstratesthattemperatureactuallymodifiesthe
advancedcomputationalresources,installations,orprogramming probabilitydistributionofthenexttoken(Fig.2),controllingthe
skills. We have chosen GPT-2 for its widespread familiarity, randomnessofthepredictionsandbalancingbetweendeterministic
fastinferencespeed,andarchitecturalsimilaritiestomoread- andmorecreativeoutputs. Additionally,asthesystemvisualizes
vancedmodelslikeGPT-3andGPT-4,makingitidealforedu- thetokenprocessingflow,studentsseethereisno“magic”involved
cationalpurposes.Ourtoolisopen-source,availableathttps: —regardlessoftheinputtexts(Fig.1A),themodelfollowsawell-
//poloclub.github.io/transformer-explainer/. definedsequenceofoperationsusingtheTransformerarchitecture,
simplysamplingonetokenatatime,beforerepeatingtheprocess.
2 SYSTEMDESIGNANDIMPLEMENTATION
3 ONGOINGWORK
TRANSFORMEREXPLAINERvisualizeshowatrainedTransformer-
basedGPT-2modelprocessestextinputandpredictsthenexttoken. Weareenhancingthetool’sinteractiveexplanations(e.g.,layernor-
ThefrontendusesSvelteandD3forinteractivevisualizations,while malization)toimprovethelearningexperience.Wearealsoboosting
thebackendusesONNXruntimeandHuggingFace’sTransformers theinferencespeedwithWebGPUandreducingmodelsizethrough
librarytoruntheGPT-2modelinthebrowser.Amajorchallengein compressiontechniques(e.g.,quantization,palettization)[7]. Fi-
designingTRANSFORMEREXPLAINERismanagingthecomplexity nally, we plan to conduct user studies to assess TRANSFORMER
oftheunderlyingarchitecture,whichcanbeoverwhelmingifall EXPLAINER’sefficacyandusability,toobservehownewcomersto
detailsarepresentedsimultaneously. Toaddressthis,wedrawon AI,students,educators,andpractitionersusethetool,andgather
twokeydesignprinciples: feedbackonadditionalfunctionalitiestheywishtoseesupported.
ReducingComplexityviaMulti-LevelAbstractions. Westruc- REFERENCES
turedthetooltopresentinformationatvaryinglevelsofabstrac-
[1] 3Blue1Brown.ButwhatisaGPT?Visualintrototransformers.https:
tion.Thisapproachallowsuserstostartwithahigh-leveloverview
//youtu.be/wjZofJX0v4M,2024.
anddrilldownintodetailsasneeded,preventinginformationover- [2] J. Alammar. The Illustrated Transformer. https://jalammar.
load[8,11]. Atthehighestlevel,ourtoolillustratesthecomplete github.io/illustrated-transformer/,2018.
pipeline:fromtakinguser-providedtextasinput(Fig.1A),embed- [3] A.M.P.Bras¸oveanuandR.Andonie. VisualizingTransformersfor
dingit,processingitthroughmultipleTransformerblocks,tousing NLP:ABriefSurvey.In24thInternationalConferenceInformation
thistransformeddatatorankthemostlikelynexttokenpredictions. Visualisation(IV),pp.270–279,2020.
Intermediateoperations,suchascalculationsfortheattentionmatrix [4] B.Bycroft.LLMVisualization.https://bbycroft.net/llm.
(Fig.1C),arecollapsedbydefaulttovisualizethesignificanceofthe [5] S.Dutta,T.Gautam,S.Chakrabarti,andT.Chakraborty.Redesigning
computationalresult,withtheoptiontoexpandtoinspectitsderiva- thetransformerarchitecturewithinsightsfrommulti-particledynami-
tionthroughanimationsequences.Weemployedaconsistentvisual calsystems.InNeurIPS,2021.
language,suchasstackingAttentionHeadsandcollapsingrepeated [6] N.Elhageetal.AMathematicalFrameworkforTransformerCircuits.
TransformerBlocks,tohelpusersrecognizerepeatingpatternsin TransformerCircuitsThread,2021.
thearchitecture,whilemaintainingtheend-to-endflowofdata. [7] F.Hohman,C.Wang,J.Lee,J.Go¨rtler,D.Moritz,J.P.Bigham,Z.Ren,
C.Foret,Q.Shan,andX.Zhang. Talaria: Interactivelyoptimizing
EnhancingUnderstandingandEngagementviaInteractivity. machinelearningmodelsforefficientinference.InCHI,2024.
ThetemperatureparameteriscrucialincontrollingaTransformer’s [8] M.Kahng,P.Y.Andrews,A.Kalro,andD.H.Chau.Activis:Visual
output probability distribution, affecting whether the next-token explorationofindustry-scaledeepneuralnetworkmodels.IEEETVCG,
predictionwillbemoredeterministic(atlowtemperature)orrandom 24(1):88–97,2017.
(hightemperature).ExistingeducationalresourcesonTransformers [9] A.Karpathy. Let’sbuildGPT:fromscratch, incode, spelledout.
oftenoverlookthisaspect[2].Ourtoolenablesuserstoadjustthe https://youtu.be/kCc8FmEb1nY,2024.
temperatureinrealtime(Fig.1B)andvisualizeitscriticalrolein [10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
controllingthepredictiondeterminism(Fig.2).Additionally,users Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need.
can select from provided examples or enter their own input text NIPS’17,p.6000–6010.CurranAssociatesInc.,2017.
(Fig.1A).Supportingcustominputtextengagesusers,byallowing [11] Z. J. Wang, R. Turko, O. Shaikh, H. Park, N. Das, F. Hohman,
M.Kahng,andD.H.Chau.CNNExplainer:LearningConvolutional
themtoanalyzethemodel’sbehaviorundervariousconditionsand
NeuralNetworkswithInteractiveVisualization.TVCG,2020.
interactivelytesttheirownhypothesesondiversetextinputs.
[12] C.Yeh, Y.Chen, A.Wu, C.Chen, F.Vie´gas, andM.Wattenberg.
Usagescenario. ProfessorRousseauismodernizingthecurricu- Attentionviz:Aglobalviewoftransformerattention.TVCG,2023.
lumofaNaturalLanguageProcessingcoursetohighlightrecent