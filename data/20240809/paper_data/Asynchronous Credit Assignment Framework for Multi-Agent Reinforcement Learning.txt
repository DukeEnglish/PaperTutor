Asynchronous Credit Assignment Framework for
Multi-Agent Reinforcement Learning
YonghengLiang HejunWu
SchoolofComputerScienceandEngineering SchoolofComputerScienceandEngineering
SunYat-SenUniversity SunYat-SenUniversity
liangyh38@mail2.sysu.edu.cn wuhejun@mail.sysu.edu.cn
HaitaoWang HaoCai
SchoolofComputerScienceandEngineering CollegeofEngineering
SunYat-SenUniversity ShantouUniversity
wanght39@mail2.sysu.edu.cn haocai@stu.edu.cn
Abstract
Creditassignmentisacoreproblemthatdistinguishesagents’marginalcontribu-
tionsforoptimizingcooperativestrategiesinmulti-agentreinforcementlearning
(MARL).Currentcreditassignmentmethodsusuallyassumesynchronousdecision-
making among agents. However, a prerequisite for many realistic cooperative
tasksisasynchronousdecision-makingbyagents,withoutwaitingforothersto
avoiddisastrousconsequences. Toaddressthisissue,weproposeanasynchronous
creditassignmentframeworkwithaproblemmodelcalledADEX-POMDPand
a multiplicative value decomposition (MVD) algorithm. ADEX-POMDP is an
asynchronousproblemmodelwithextravirtualagentsforadecentralizedpartially
observablemarkovdecisionprocess. WeprovethatADEX-POMDPpreservesboth
thetaskequilibriumandthealgorithmconvergence. MVDutilizesmultiplicative
interactiontoefficientlycapturetheinteractionsofasynchronousdecisions,and
wetheoreticallydemonstrateitsadvantagesinhandlingasynchronoustasks. Ex-
perimentalresultsshowthatontwoasynchronousdecision-makingbenchmarks,
OvercookedandPOAC,MVDnotonlyconsistentlyoutperformsstate-of-the-art
MARLmethodsbutalsoprovidestheinterpretabilityforasynchronouscooperation.
1 Introduction
Multi-agentreinforcementlearning(MARL)[1–3]ispromisingformanycooperativetasks,such
as video games [4] and collaborative control [5–7]. Previously, such tasks were formulated as a
Dec-POMDP(decentralizedpartiallyobservablemarkovdecisionprocess)[8],assumingthateach
atomicactionisperformedsynchronouslyinatimestep. However,mostrealisticcooperativetasks
of real-time require that agents should act without waiting for other agents, to avoid disastrous
consequences[9–11].
TocarryoutasynchronouscooperationtasksusingtheoriginalMARLdesignedforsynchronous
scenarios, researchers have proposed two types of methods as follows. 1) Discarding: Agents
collect data of other agents only when they make decisions, but discard the data of others while
executingtheiractions,asshowninFigure1aand1b.Thistypeofmethodusuallyneedspolicy-based
MARLfortraining[9,11,12]. 2)Padding: Thistypestudiestransformasynchronoustasksinto
synchronousonesviapaddingblankactionssoastoapplymostexistingMARL[13,14],asshown
inFigure1c.
Preprint.Underreview.
4202
guA
7
]AM.sc[
1v29630.8042:viXra𝑁 𝑁 𝑁 𝑁
𝑎1 𝑎1 𝑎1
3 3 3 3 3 3 3
2
𝑎 20 𝑎 22 𝑎 25
2
𝑎 22
2
𝑎 22
2
𝑎 22
1 1
𝑎 14
1
𝑎 10
1
𝑎 10
𝑇 𝑇 𝑇 𝑇
𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6
(a)Discarding (b)CAAC (c)Padding (d)MVD
Figure1: IllustrationofdifferentasynchronousMARLmethods. Blue/yellowcirclesdepictcollected
decisiondata. Smallwhiteonesdepictpaddingactions. (a)Agent#2disregardsinformationofother
agentsfromt tot . (b)Agent#2neglectsactiona1whenassigningcreditatt . (c)Creditatt is
0 2 3 2 2
attributedtothepaddingactions. (d)OurproposedMVDfullyaccountsfortheinteractionsamong
asynchronousdecisionsbeingexecutedatt .
2
Unfortunately,bothdiscardingandpaddingareharmfultocreditassignment[15]ofasynchronous
agents. On the one hand, in the discarding type methods, the discarded information leads to an
inaccurateestimationoftheimpactsofactionsfromotheragents. Forinstance,CAAC[10]explored
the asynchronous credit assignment in the application of bus holding control [16]. As shown in
Figure1b,attimestept ,agent#2discardstheinformationofactiona1thatisnotwithinthetime
2 3
window. Ontheotherhand, the padding typemethodsgenerallyusevaluedecomposition(VD)
[1–3]tosolvethecreditassignmentproblem. VDisawidely-usedalgorithmtolearnthemarginal
contributionofeachagentanddecomposetheglobalQ-valueQ intoindividualagent-wiseutilities
tot
Q toguideagents’behaviors. DespitethesuccessofVDalgorithmsinoptimizingthecollective
i
policyforsynchronoustasks,theyarenotapplicabletoasynchronoussettings,sincetheinteractions
amongagentsatdifferenttimestepsarenottakenintoaccountintheseVDalgorithms. Asshownin
Figure1c,theimpactsofactionsa1anda0areignoredbyagent#2attimestept ,thusVDhasto
3 1 2
decomposeaQ consideringtheimpactsofblankactionsatthistimestep.
tot
Inthispaper,weproposeanasynchronouscreditassignmentframeworkwithanewproblemmodel,
ADEX-POMDP, and a VD algorithm, MVD. ADEX-POMDP is named after Asynchronous De-
centralizedPOMDPwithExtraAgents,whichextendsthepreviousDec-POMDPviaextravirtual
agents (also called extra/virtual agents) to migrate asynchronous decisions to a single time step.
Weprovethataddingextraagentsneitherchangestheoriginaltaskequilibriumnorinterfereswith
thealgorithmconvergence. WederiveageneralmultiplicativeinteractionformforVDanddesign
aMultiplicativeValueDecomposition(MVD)algorithmtosolveADEX-POMDP.MVDutilizes
multiplicativeinteractions[17,18]toefficientlycapturethemutualimpactsamongasynchronous
decisions. IntheoreticalcomparisonwithtraditionalVD,MVDnotonlyexpandsthefunctionclass
strictly but alsobears advantagesin tackling asynchronousity. We evaluateMVD across various
difficultylevelsintwoasynchronousdecision-makingbenchmarks,Overcooked[19]andPOAC[20].
ExperimentalresultsshowthatMVDachievesconsiderableperformanceimprovementsincomplex
scenariosandprovideseasy-to-understandinteractionprocessesamongasynchronousdecisions.
Our contributions are summarized as follows: 1) To the best of our knowledge, we propose the
firstgeneralasynchronouscreditassignmentframeworkwithanewasynchronousproblemmodel,
ADEX-POMDP,andaVDalgorithm,MVD.2)WeprovethatADEX-POMDPpreservesthetask
optimalityandthealgorithmconvergenceandwetheoreticallydemonstratetheadvantagesofMVD,
aswell. 3)Weconductextensiveexperimentsinasynchronousbenchmarks,showingMVD’ssuperior
performanceandenhancedinterpretabilityofasynchronousinteractions.
2 Preliminaries
2.1 Dec-POMDP
Afullycooperativemulti-agenttaskwhereallagentsmakedecisionssimultaneouslycangenerally
beformulatedasaDec-POMDP.Dec-POMDPisdefinedbyatuple⟨N,S,A,P,r,O,Ω,γ⟩,where
N denotesafinitesetofnagentsands∈S representstheglobalstateoftheenvironment. Ateach
timestep,eachagenti∈N obtainsitsownobservationo ∈Ωdeterminedbythepartialobservation
i
O(s,i)andselectsanactiona ∈ Atoformajointactiona = [a ]n ∈ An. Subsequently, all
i i i=1
agentssimultaneouslycompletetheiractions,leadingtothenextstates′accordingtothetransition
2functionP(s′|s,a):S×An →S andearningaglobalrewardr(s,a):S×An →R. Eachagenti
hasitsownpolicyπ (a |τ ):T ×A→[0,1]basedonlocalaction-observationhistoryτ ∈T. The
i i i i
objectiveofallagentsistofindanoptimaljointpolicyπ =[π ]n andmaximizetheglobalvalue
functionQπ =E[(cid:80)∞ γtrt+1]withadiscountfactorγ ∈[0,1i )i .=1
t=0
2.2 CreditAssignmentandValueDecomposition
CreditassignmentisacorechallengingproblemindesigningreliableMARLmethods[15].Itfocuses
onattributingteamsuccesstoindividualagentsbasedontheirrespectivemarginalcontributions,aim-
ingatcollectivepolicyoptimization. VDalgorithmsarethemostpopularbranchesinMARL.They
leverageglobalinformationtolearnagents’contributionanddecomposetheglobalQ-valuefunction
Q (s,a)intoindividualutilityfunctionsQ (τ ,a ). Intheexecutionphase,agentscooperatevia
tot i i i
theircorrespondingQ (τ ,a ), therebyrealizingcentralizedtraininganddecentralizedexecution
i i i
(CTDE)[21]. TraditionalVDalgorithms, includingVDN[1], QMIX[2], andQatten[3], canbe
representedbythefollowinggeneraladditiveinteractionformulation[22]:
n
(cid:88)
Q =Q (s,Q ,Q ,··· ,Q )=k + k Q , (1)
tot tot 1 2 n 0 i i
i=1
wherek isaconstantandk denotesthecreditthatreflectsthecontributionsofQ toQ .
0 i i tot
To address the problem of capturing high-order interaction among agents, which traditional VD
algorithmsignored,NA2Q[23]recentlyintroducedageneralizedadditivemodel(GAM)[24],as
follows:
n
(cid:88) (cid:88)
Q =f + k f (Q )+···+ k f (Q )+···+k f (Q ,··· ,Q ), (2)
tot 0 i i i j j j 1···n 1···n 1 n
i=1 j∈Dl
wheref isaconstant, f ∈ {f ,··· ,f }m takesl (l ∈ {1,··· ,n})individualutilitiesQ as
0 j 1 1···n i
inputandcapturethel-orderinteractionsamongtheseagents,andD ={i ···i }isanon-empty
l 1 l
subsets.
Inordertomaintaintheconsistencybetweenlocalandglobaloptimalactionsafterdecomposition,
theseVDalgorithmsshouldsatisfythefollowingindividual-global-max(IGM)principle[25]:
argmaxQ (s,a)=(argmaxQ (τ ,a ),··· ,argmaxQ (τ ,a )). (3)
tot 1 1 1 n n n
a a1 an
Forexample,QMIXholdsthemonotonicity ∂Qtot ≥0andachievesIGMbetweenQ andQ . The
∂Qi tot i
furtherintroductionofVDandothercreditassignmentmethodscanbereferredtoinAppendixA.1.
2.3 AsynchronousMARL
Currently,researchinasynchronousMARLprimarilyfocusesonprocessingasynchronousdatafor
compatibilitywithexistingMARLmethods. AsmentionedinSection1,theseworkscanbroadlybe
categorizedintotwotypesofmethodsasdepictedindetailinthefollowing.
The discarding type method focus solely on decision time-step information, treating the sum of
rewards during the execution of an agent’s decision as the reward for its asynchronous decision.
Agentscollecttheirownasynchronousdecisioninformationandemployexistingpolicy-basedMARL
methodsfortraining[9,11,12]. However,discardingthedecisioninformationofotheragentscan
causenon-stationarity[15]andhurtcreditassignment. AsshowninFigure1a,agent#2isunableto
determineifthetransitionfromglobalstates2tos5andtheaccumulatedreward(cid:80)5 rtarecaused
t=3
byitsownasynchronousdecisiona2orbydecisionsofotheragents,suchasa0,a4,anda1.
2 1 1 3
Thepaddingtypemethodtransformstheasynchronousdecision-makingproblemintoasynchronous
onethroughpaddingaction,therebyobtainingaDec-POMDPsoastoapplyexistingMARLmethods
[13,14]. SinceDec-POMDPrequiresthecollectionofdecisioninformationfromallagentsateach
timestep,paddingactioncanbeusedasasubstituteforthedecisioninformationofagentsthatare
executingactions. Collectingdecisiondataateachtimestepcanmitigatenon-stationarityissues
and enable one to address asynchronous problems in a traditional way. Nevertheless, redundant
paddingactionmayinterferewiththecreditassignmentprocess[9]. AsshowninFigure1c,agent
3#2canonlylearnthecontributionofactiona2att . Infact,a2continuouslyaffectstheenvironment
2 2 2
andthedecision-makingofotheragentsduringitsexecution. However, thecreditsfromt tot
4 5
aremistakenlyattributedtopaddingactionpa3 andpa4,causingthefailureoftheoriginalcredit
2 2
assignmentprocessinasynchronousscenarios. ThefurtherintroductionofasynchronousMARL
methodscanbereferredtoinAppendixA.2.
3 ADEX-POMDP
Inasynchronouscreditassignment,thekeyfactoristheinteractionbetweenasynchronousdecisions
ofdifferentagents. Specifically,theagentwhoexecutesfirstmustpredicthowlaterchoicesofother
agentswouldaffectitsexecution. Meanwhile,theagentwhoexecuteslaterneedstoconsiderthe
impactofthecurrentactionsofotheragentsonitsdecision. However,bothdiscardingandpadding
methodshavelimitationsincapturingthiskeyfactor. Theformerignoresdecisionsfromotheragents,
whilethelattermakesdecisionsaccordingtothepaddingblankactions. Intuitively, thesimplest
implementationofasynchronouscreditassignmentistousethemostrecentactionaspaddingaction
[13,14]. Unfortunately,suchanapproachintroducesambiguity,asitcannotdistinguishwhetherthe
agentiscontinuingtheexecutionoftheoriginalactionorrestartingtheexecutionofthesameaction.
Thiscouldcausesemanticdifferencesbetweentheoriginalexecutionandthenewexecutionand
subsequently,thetraininginsimulatedenvironmentsmightnotconverge.
OurmotivationforADEX-POMDPistomigrateasynchronousde-
cisionsfromdifferenttimestepstothesametimestepbymeansof 𝑁
virtualagentstomaintainsemanticconsistencyandensureaccurate
3′
asynchronouscreditassignment. ThegeneralideaofADEX-POMDP
2′
is as follows: Suppose the action a of agent i takes multiple time
i 1′
stepstocomplete,e.g.,timestept ,t ,andt ofagent#1inFigure2.
1 2 3
3
ADEX-POMDPstillusesblankactionaspaddingforeachtimestep 𝑎1
3
whenagentiiscontinuingtheexecutionofactiona .ADEX-POMDP 2
i
introducesanextravirtualagenti′,toconstructapairofreal-virtual 1
𝑎0 𝑇
agents<i,i′ >. Agenti′usesthesamepolicyasthatofagentiand 1
𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡
executesthesamedecisionofa ,asshowninFigure2. Thepolicy 0 1 2 3 4 5 6
i
ofiwillbeupdatedaccordinglywheni′’spolicyisupdated. Wecan Figure 2: ADEX-POMDP.
alsoadjusti’spolicyaccordingtotheinfluenceofitsasynchronous At t 2, a0
1
and a1
3
are still
decisionsa onotheragentsandtheenvironment. Thelifetimeofan executing, extra agents #1′
i
extraagentisonlyonetimestep. and#3′areintroducedtore-
executethesetwodecisions.
ADEX-POMDPisanasynchronousdecision-makingproblemmodel
inwhichatomicactionstakedifferentexecutiontimestepstocom-
plete. BeforegoingtothedetailedformulationofADEX-POMDP,thefrequentlyuseddefinitions
needtobelistedinthefollowing: Aregulartimestepisdenotedast. Theobservationformaking
adecisionisdenotedaso˜,referringtothemostrecentobservationuponwhichanagentmakesthe
decisiontoexecuteanewasynchronousaction. a˜denotesthemostrecentnewdecisionmadeby
theagent. Anasynchronousactioniscompletedwithinadifferentnumberoftimestepsfromeach
agent’sperspective.
Definition1(ADEX-POMDP). ADEX-POMDPisatuple⟨N(cid:98),I,S(cid:98),A,P(cid:98),r (cid:98),O(cid:98),O P,Ω,γ⟩,where
N(cid:98) ={N,N′}. TherealagentsetisN ={1,2,··· ,n}andthecorrespondingsetofvirtualagents
isN′ ={1′,2′,··· ,n′}.Lettheagentthatismakingthedecisionbei ∈N andtheagentexecuting
d
anactionbei ∈N. Givenanoriginalstates∈S,functionI(s)givesasetN′ ⊆N′,whereagent
c c
i′ ∈N′andagenti formanreal-virtualpair,<i ,i′ >. Thetwoagentsinthispairhavethesame
c c c c c
policy. s (cid:98)=[s;o˜ c]∈S(cid:98)accordingtotheoriginals,where[·;·]istheconcatenationoperation. o˜ cis
obtainedaccordingtoO (o˜ |s),whichisthejointobservationofallagentsi formakingdecisions.
P c c
Eachextraagenti′
c
receiveso
i′ c
= o˜
ic
∈ ΩaccordingtoO(cid:98)(s (cid:98),i′ c)andselectsactiona
i′ c
= a˜
ic
to
forma
(cid:98)
=[a;a˜ c]∈An+n′ c,wheren′
c
=|N c′|,a∈Anistheoriginaljointaction,anda˜ cisthemost
recentnewdecisionmadebyallagentsi . Subsequently,theymovetothenextstates′accordingto
c (cid:98)
P(cid:98)(s (cid:98)′|s (cid:98),a (cid:98))=O P(o˜′ c|s′)P(s′|s,a)andearnajointrewardr (cid:98)(s (cid:98),a (cid:98))=r(s,a).
4AsshowninTheorem1,ADEX-POMDPnotonlyfacilitatesthehandlingofinteractionsbetween
asynchronousdecisions,butthisasynchronousMARLmodelalsopreservestheinherentcharacteris-
ticsofthetaskandthesolutionprocess.
Theorem1. IntroducingextraagentsinADEX-POMDPpreservestheoriginalequilibriumofthe
taskandmaintainstheconvergenceofthevaluedecompositionmethod.
Proof. Firstly, We prove that ADEX-POMDP preserves the original markov perfect equilibrium
(MPE) mentioned in Definition 2. According to Lemma 1, given any joint policy π 1, the con-
vergedglobalQ-valuefunctionQπ underDec-POMDPisequaltotheconvergedglobalQ-value
Dec
functionQπ underADEX-POMDP.Thus,wehaveQπ (s,a) = Qπ (s,a),whichim-
ADEX Dec ADEX (cid:98) (cid:98)
pliesVπ (s) = Vπ (s). Therefore,theoriginalMPEπ∗ inDec-POMDPisalsoanMPEin
Dec ADEX (cid:98)
ADEX-POMDP.
Secondly,regardingtheVDoperatorT∗ =(cid:81) ·T∗mentionedinDefinition5,wesupposethat
VD VD
theVDoperatorT∗ guaranteesconvergenceunderDec-POMDP.Ontheonehand,accordingto
VD
Lemma2,theintroductionofextraagentsdoesnotaffectT∗,whichenableslearningofthesame
Q∗ asinDec-POMDP.Ontheotherhand,sinceweassumethatT∗ convergesinDec-POMDP,
whichmeans(cid:81) canfindtheoptimalf∗ (Q (s,a ),··· ,Q (s,V aD ))toapproximateQ∗within
the function claV ssD Q . In ADEX-POMD Dec P, to1 app1 roximate n the san me Q∗, (cid:81) simply needs
tot VD
to set f∗ = f∗ (Q (s,a ),··· ,Q (s,a ))+0×(Q (s,a )+···+Q (s,a )). In
summarA y,D iE ftX heconvD ee rc genc1 eofT1 ∗ isguan ranten edinaDec-PO1′ c MDP,1 t′ c hesameguaran n′ c teeapn′ c pliesto
VD
anADEX-POMDP.
4 MVD
4.1 MultiplicativeInteractionofAsynchronousIndividualQ-values
Aftermigratingasynchronousdecisionstoasingletimestepviaextraagentsandmodelingtheasyn-
chronousproblemasanADEX-POMDP,wecanconsiderthemarginalcontributionofasynchronous
decisionsateachtimestepoftheirexecution. AccordingtotheunifiedframeworkofgeneralVD
algorithms,theglobalQ-valueQ withinanADEX-POMDPcanbeexpressedasageneralformula
tot
intermsoftheindividualutilitiesQ asfollows:
i
Q =Q (s,Q ,··· ,Q ,Q ,··· ,Q ,Q ,··· ,Q ). (4)
tot tot 1d nd 1c nc 1′
c
n′
c
Different from (1), where agents execute actions synchronously according to Q , agents in the
i
asynchronoussettingmustaccountfortheinfluenceofcurrentlyexecutingactionsandthepotential
impact of their own decisions during execution. This mutual influence is represented in (4) as
the interaction between Q and Q . To capture this interplay, we enrich (1) by introducing the
id i′
c
multiplicative interaction between them and propose the following general Multiplicative Value
Decomposition(MVD)formula:
n+n′
(cid:88)c (cid:88)
Q =k + k Q + k Q Q . (5)
tot 0 i i idi′
c
id i′
c
i id,i′
c
Infact,wecanperformaTaylorexpansionofQ nearanoptimaljointactiontotheoreticallyderive
i
(5),providingtheoreticalsupportforMVD.ThedetailedderivationsareprovidedinAppendixC.1.
Comparedtosolving(4)withtheadditiveinteractionVD,thestraightforwardmultiplicativeinter-
action between Q and Q in our MVD unexpectedly and significantly boosts representational
id i′
c
capabilities in learning agent interactions. Intuitively, for (1), the obtained gradient to update
Q
∂ ∂Q
Qi
t
ii
o
′
cs
t
∂ =∂Q Qt ko it
i′
c= +(cid:80)k i. idI kn idc
i′
co Qnt ir da ,st a, nt dhe
∂
∂Qg Qr
t
ioa ctdi =ent kd ice .ri Tve hd erf er fo om re,(5 M) Var De i∂ ∂ nQ Q tet i go dt rat= esk thid e+ no(cid:80) nlii n′ c ek ai rdi a′ c gQ ei n′ c t,
interactionsascontextualinformationduringupdatesandenablesbothQ andQ torefinetheir
id i′
c
policiesbasedontheirmutualinfluence. Theoretically,asshowninTheorem2,weprovethatMVD
bearsadvantagesinhandlingasynchronoustasksovertraditionalVD.
1AlthoughADEX-POMDPinvolvesn+n′ agents,thejointpolicycanstillbedenotedbyπfromDec-
c
POMDPwithnagents,astheextraagenti′ shareitspolicywiththecorrespondingagenti .
c c
5Theorem2. LetQ denotethefunctionclassoftheadditiveinteractionVDas(1),andletQ
Add Mul
denotethefunctionclassofthemultiplicativeinteractionVDas(5),thenwehaveQ ⊊Q .
Add Mul
Proof. Bycomparing(1)oftheadditiveinteractiveVDwith(5)ofthemultiplicativeinteractiveVD,
weclearlyseethatQ ⊂ Q . Therefore,theremainingpartistoprovethestrictnessofthis
Add Mul
inclusion.
Weconsideratwo-playerasynchronousdecision-makinggamewheretherow 𝒂 𝒃
agent makes the first move, choosing either action a or b, followed by the
columnagentwhocanalsoselectactionaorb. Thefinalrewardisdetermined 𝒂 𝒂𝒂 𝒂𝒃
by multiplying the values of the chosen actions, and the reward matrix is
𝒃 𝒂𝒃 𝒃𝒃
illustratedinFigure3. FormultiplicativeinteractiveVD,wesettheindividual
utilityasanidentitymappingQ (a )=a andletk =k =k =0,k =1
i i i 0 1 2 12
Figure 3: Example
tolearnthegroundtruthrewardfunction. However,foradditiveinteractiveVD,itisevidentthatit
payoffmatrix.
cannotlearntherelationshipofmultiplyingdifferentactionvalues.
4.2 High-OrderInteractionofAsynchronousIndividualQ-values
Withmultiplicativeinteraction,wecontinuetoexplorehigh-orderinteractionsamongasynchronous
decisions. (5)primarilyconsidersthemutualinfluenceofQ andQ . However,asillustratedin
id i′
c
Figure2,thedecisionsa0 anda1 re-selectedbyextraagents#1′ and#3′ att actuallyoriginate
1 3 2
fromdifferenttimesteps,implyinghigh-orderinteractionsamongQ ,Q ,andQ . Assuch,we
1′ 3′ 2
proposeaKth-order(where1≤K ≤n)interactiveVDasfollows:
n+n′
(cid:88)c (cid:88) (cid:88)
Q =k + k Q + k Q Q +···+ k Q Q ···Q .
tot 0 i i idi′
c
id i′
c
idi′ c,1···i′
c,K−1
id i′
c,1
i′
c,K−1
i id,i′
c
id,i′ c,1,···,i′
c,K−1
(6)
Thedetailedderivationof(6)canbefoundinAppendixC.2.
Nevertheless, as the order increases, the deep interaction information between agents does not
provide significant benefits [23, 26]. Therefore, this paper primarily focuses on multiplicative
interactionsbetweenQ andQ . Wefurtherdiscussthepracticalperformancecomparisonbetween
id i′
c
multiplicativeinteractionsandhigh-orderinteractionsinSection5.3.
4.3 Implementation
Finally,wediscusstheissueofIGMconsistencyinthepracticalimplementationofMVD.InADEX-
POMDP,theagenti currentlyexecutinganactiondoesnotneedtochooseanewone,andextra
c
agentsi′ canonlyexecutetheasynchronousdecisionsofi . Thisimpliesthati andi′ donotneed
c c c c
tosatisfytheIGMcondition. Consequently,weobtaintheMVD-basedIGMasfollows:
argmaxQ (s,a)=(argmaxQ (τ ,a ),··· ,argmaxQ (τ ,a ),
a
tot
a1d
1d 1d 1d
and
nd nd nd
(7)
a ,··· ,a ,a ,··· ,a ).
1c nc 1′
c
n′
c
Toachieves(7),weneedtomaintainthemonotonicitybetweenQ andQ ,i.e., ∂Qtot = k +
(cid:80) k Q ≥ 0.
Therefore,duringtraining,wekeeptrackoftht eot minimui md value∂ QQi
md in
ofi Qd
i′ c idi′ c i′ c c i′ c
andQmincanberegardedasaconstantspecifictothetask. Weemployhypernetworks[27]f (s)to
c i
approximatetheweightsin(5),resultinginthefollowingformthatsatisfiesMVD-basedIGM:
Q ≈f +n (cid:88)+n′ c |f |Q +(cid:88) |f |Q i′ c +Qm c in Q . (8)
tot 0 i i idi′ c 2 id
i id,i′
c
Detailedderivationof(8)isprovidedinAppendixC.3.
TheoverallframeworkofMVDisillustratedinFigure4. Weproposethreedistinctpracticalimple-
mentationsofthemixingnetwork. Thefirstapproachdirectlyutilizes(8)toobtainQ . Further,
tot
6𝑄
𝑡𝑜𝑡
𝑄
𝑡𝑜𝑡
𝑄
𝑡𝑜𝑡
𝑄 𝑡𝑜𝑡(𝝉,𝒂)
Softmax MLP Mixing Network 𝑠
𝑄
𝑖
𝑏 𝑠
𝑄 (𝜏 ,𝑎 ) 𝑄 (𝜏 ,𝑎 ) 𝑄 (𝜏 ,𝑎 )
|∙| 1𝑑 1𝑑 1𝑑 1𝑐 1𝑐 1𝑐 1 𝒄′ 1 𝑐′ 1 𝑐′ MLP
𝑸Τ𝒘 ℎ𝑡−1 ℎ𝑡
𝑖 𝑖
𝑸 𝑑Τ𝕎𝑸 𝑐′ |∙| AgenA t g 𝟏e 𝒅nt 𝒏 𝒅 AgenA t g 𝟏e 𝒄nt 𝒏 𝒄 AgenA t g 𝟏e 𝒄n ′t 𝒏 𝒄′ GRU
⋯ MLP
𝑸 𝑑 𝑸 𝑐 𝑸 𝑐′ (𝑜 1𝑡 𝑑,𝑎 1𝑡− 𝑑1) (𝑜 1𝑡 𝑐,𝑎 1𝑡− 𝑐1) (𝑜 1𝑡 𝒄′,𝑎 1𝑡− 𝑐′1) (𝑜 𝑖𝑡,𝑎 𝑖𝑡−1)
Figure4: TheoverallframeworkofMVD.Left: Mixingnetworkstructure. Inredarethehyper-
networks that generate the weights and biases for mixing network. Middle: The overall MVD
architecture. Right: Agentnetworkstructure.
weemployamulti-headstructurethatallowsthemixingnetworktofocusonasynchronousinterac-
tioninformationfromdifferentrepresentationsub-spaces,therebyenhancingtherepresentational
capabilityandstability. Hence,forthesecondimplementation,weoutputtheheadQ-valueQh
tot
fromdifferentheads,whichareprocessedthroughaSoftmaxfunctiontoobtainthefinalQ . To
tot
simplifymodelimplementation,thethirdapproachapproximatestheSoftmaxusinganMLPwith
nonlinearactivationfunctions. Inthispaper,weprimarilyfocusonthethirdway. Wefurtherdiscuss
thedifferentimplementationsinSection5.3. Thepseudo-codeforMVDisinAppendixD.
5 Experiments
Inthissection,weevaluateourMVDacrossvariousscenariosintwoasynchronousdecision-making
benchmarks,OvercookedandPOAC.Thebaselinesfallintothreecategories: 1)Thedecentralized
traininganddecentralizedexecution(DTDE)method,IPPO[28],whereagentstreatotheragents
aspartoftheenvironment,makingitsuitableforasynchronousdecision-makingscenarios. 2)The
discarding type method, MAC IAICC. Note that we do not select CAAC because its code is not
open-source and its application is limited to bus holding control. 3) The padding type method,
includingVDN,QMIX,Qatten,SHAQ[29],andNA2Qthatconsiders2nd-orderinteractions. The
implementationdetailsofthebenchmarks,allbaselines,andourMVDareprovidedinAppendixE.
Thegraphsillustratetheperformanceofallcomparedalgorithmsbyplottingthemeanandstandard
deviationofresultsobtainedacrossfiverandomseeds.
5.1 PerformanceonOvercooked
WefirstruntheexperimentsontheOvercookedbenchmark,wherethreeagentscooperatetoprepare
aTomato-Lettuce-Onionsaladandpromptlyserveitatthecounter. Theymustlearntosequentially
gather raw vegetables, chop them, and merge them into a plate before delivering. Each action,
includingchopping,movingtotheobservedingredients,anddelivering,carriesadifferenttimecost.
Successfullyservingthecorrectdishearnsapositivereward,whilemistakesleadtonegativeones.
ThedetailsofthisbenchmarkcanbefoundinAppendixE.1.
Figure5showstheperformancecomparisonagainstbaselines
on map A of this benchmark. The results indicate that our
Map A
MVDsurpassesallbaselines. BothIPPOandMACIAICCex-
MVD
hibitslowertrainingspeeds. Thissuggeststhediscardingtype 200 VDN
QMIX
methods suffer from low efficiency. NA2Q and SHAQ mis- Qatten
150 SHAQ
takenlyconsidertheinfluenceamongpaddingactions,result- NA2Q
inginnon-convergence. ThisimpliesthatDec-POMDPwith 100 IPPO
MAC IAICC
paddingactionisalsounsuitableforasynchronousdecision- 50
making scenarios, either. QMIX and Qatten perform better
thanNA2Qbecausetheyusesimplermodelstohandlecredit 0
assignment,leadingtostrongerrobustnesstopaddingaction.
0.0 0.2 0.4 0.6 0.8 1.0
T (mil)
Figure5: Meantestreturnonmap
7
AofOvercookedbenchmark.
nruteR
tseT
naeMTheslowtrainingspeedofVDNisduetoitsneglectofthe
marginal contribution of agents. Furthermore, we discover
that MVD outperforms other baselines on various maps of
Overcooked. Thecompleteexperimentalresultsandanalysis
areinAppendixF.
5.2 PerformanceonPOAC
WefurtherconductexperimentsonthemorechallengingPOACbenchmark. POACisaconfrontation
wargamebetweentwoarmies,witheacharmyconsistingofthreediverseagents: infantry,chariot,
andtank. Theseagentspossessdistinctattributesanddifferentactionexecutiontimes. Ourobjective
istolearnasynchronouscooperationstrategiestodefeatthebuilt-inrule-basedbots. Thedetailsof
thisbenchmarkcanbefoundinAppendixE.2.
MVD QMIX SHAQ IPPO
VDN Qatten NA2Q MAC IAICC
Scenario 3 Scenario 4 Scenario 5
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Figure6: Testwinrate%onthreechallengingscenariosofPOACbenchmark.
ThewinrateofthePOACbenchmarkwithincreasingdifficultiesisshowninFigure6. Weobserve
that MVD demonstrates increasingly better performance. IPPO and MAC IAICC learn slowly
in simple scenarios but struggle to train in complex tasks. Compared to Overcooked, only the
movementactionsinPOACrequiremultipletimesteps,introducinglesspaddingactionwhenusing
Dec-POMDP. Therefore, NA2Q performs relatively well among the baselines. However, due to
interferencefrompaddingactionandthecomplexityoftheadoptedmodel,thetrainingefficiencyof
NA2QisconsistentlyinferiortothatofMVD,andthesameappliestoSHAQ.Theadditiveinteraction
VDalgorithms,includingVDN,QMIX,andQatten,donotyieldsatisfactoryperformance,sincethey
cannothandlethemutualinfluenceamongagents. Furthermore,wecanfindthatMVDdemonstrates
highlycompetitiveperformanceinotherscenariosofPOAC.Thecompleteexperimentalresultsand
analysiscanbefoundinAppendixG.
5.3 AblationStudy
ToobtainadeeperinsightintoourproposedADEX-POMDPandMVD,weperformablationstudies
toillustratetheimpactofthefollowingfactorsontheperformance: 1)Theintroductionofextra
agentsinADEX-POMDP.2)Theinteractionsofdifferentordersamongagents. 3)Distinctpractical
implementationsofMVD.
Tostudyfactor1), weextendQMIXandQattentoADEX-POMDP,denotingthemasQMIX(A)
andQatten(A).AsshowninFigure7a,theintroductionofextraagentsnotablyimprovestheper-
formanceofbothQMIXandQatten,highlightingthepowerfuladvantagesofADEX-POMDPin
an asynchronous setting. However, these additive interaction VD algorithms fail to capture the
mutualinfluenceamongagents,leadingtopoorerperformancethanMVD.Wealsoextendother
VDalgorithmstoADEX-POMDP.Thecompleteablationexperimentsandanalysiscanbefoundin
AppendixH.1.
Tostudyfactor2),weconsiderMVDandNA2Qwithlth-orderinteractionsunderADEX-POMDP,
denoting them as MVD(l) and NA2Q(l). As shown in Figure 7b, the performance of MVD(2)
incorporating multiplicative interactions consistently outperforms MVD(1) which only involves
8
%
etaR
niW
tseT
%
etaR
niW
tseT
%
etaR
niW
tseTScenario 5 Scenario 5 Scenario 5
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
MVD MVD(2)
QMIX MVD(1)
0.2 QMIX(A) 0.2 MVD(3) 0.2 MVD(MLP)
Qatten NA2Q(2) MVD
0.0 Qatten(A) 0.0 NA2Q(3) 0.0 MVD(Softmax)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
(a)Extraagents (b)Differentordersinteractions (c)Differentimplementations
Figure7: AblationStudiesofMVDonscenario5ofPOACbenchmark.
additiveinteractions. Further,neitherMVD(3)norNA2Q(3)experiencesanimprovementbyutilizing
higher-orderinteraction. ThisimpliesthatthestraightforwardmultiplicativeinteractionbetweenQ
id
andQ issufficienttoefficientlysolvetheasynchronouscreditassignmentproblem. Thecomplete
i′
c
ablationexperimentsinotherscenariosofPOACandanalysiscanbefoundinAppendixH.2.
Tostudyfactor3),wecomparedthreedifferentpracticalimplementationsofMVD.Asshownin
Figure7c,directlyapplying(8)toobtainQ canconvergetotheoptimaljointpolicy,yetitsuffers
tot
fromslowtrainingspeedandinstability. Employingamulti-headstructurecaneffectivelyaddress
thesetwoshortcomings. However,combiningSoftmaxwithmultiplicativeinteractionscomplicates
theentiremixingnetworkexcessively. Therefore,MVDderivesthegreatestbenefitfromMLPwith
nonlinearactivationfunctions. ThecompleteablationexperimentsinotherscenariosofPOACand
analysisarepresentedinAppendixH.3.
5.4 InterpretabilityofMVD
Tovisuallyillustratetheasynchronouscreditassignmentprocess,we
exhibitkeyframesfromscenario5ofPOACandcomparethecon-
1 1 Tank
vergedMVDandNA2QonADEX-POMDP.Wehighlighttheindivid- 2 2 Chariot
ualQ andcrucialweightswithinthemixingnetworktodemonstrate 3 Infantry
i
theiralignmentwithagents’asynchronousbehaviors. Figure8shows 𝒌𝒊 𝑸𝒊
ahiddentankandinfantry-chariotcooperation. Theinfantrysuccess- 1 0.955 -0.733 3
2 3.678 0.055 𝒌𝒊𝒋
fullyattacksanenemy,resultinginhighQ 3andcreditk 3.Meanwhile, 3 1.676 0.785 1 2’ 0.310
thechariotisperformingamovementactiontoluretheenemydeeper, 2’ 1.122 -0.182 3 2’ 0.711
butsinceitisunderattack,Q 2′ isnegative. Thus,theadditiveinterac- Figure 8: Visualization of
tionfailstoexplainthemutualinfluencebetweenthem. Fortunately, evaluationforMVD.
MVDaccuratelyattributestheimportanceoftheircooperationtothe
2 3 1 Tank
wholeusingmultiplicativeinteractionandassignsahighercreditk 32′. 2 Chariot
Figure9showsasimilarcaseofNA2Q.Theattackinginfantryhas 3 Infantry
1
𝑸𝒊 𝒇𝒊
ahigherQ 3,whereasthekitingtankhasalowerQ 1′. Boththetank 1 0.466 0.097
andthechariotareexecutingactions,yetNA2Qmistakenlyregards 𝒇𝒊𝒋 2
3
0 1. .7 69 32
8
2 2. .2 17 28
2
theasynchronousinfantry-chariotcooperationasmoreimportantthan 3 1’ 0.795 1’ 0.522 0.082
infantry-tankcooperation. Therefore,eventhoughbothstrategiesul- 3 2’ 3.614 2’ 0.242 0.079
timatelyachievevictory,MVDoffersasuperiorabilitytocapturethe Figure 9: Visualization of
interplaybetweenagents’asynchronousdecision-making,providing evaluationforNA2Q.
higherinterpretability.
6 Conclusion
Inthispaper,weproposeanasynchronouscreditassignmentframeworkconsistingoftheADEX-
POMDP problem model and MVD algorithm, providing a solid basis for further exploration of
asynchronouscreditassignmentproblems. ADEX-POMDPsynchronizesasynchronousdecisions
forenhancedprocessing,withoutdisruptingtaskequilibriumorVDconvergence. MVDintroduces
multiplicativeinteractions,strictlyextendingthefunctionclassandefficientlycapturingtheinter-
playbetweenasynchronousdecisions. ExperimentalresultsindicatethatMVDmaintainssuperior
9
%
etaR
niW
tseT
%
etaR
niW
tseT
%
etaR
niW
tseTperformanceacrossvariousasynchronousscenariosandprovidesinterpretabilityforasynchronous
cooperation. Inaddition, ablationstudiesrevealtheneedforcarefulselectionoftheinteractions
betweenagentsandthoughtfuldesignofmixingnetworkstructurestoavoidexcessivemodelcom-
plexity. Therefore,apotentialfutureresearchdirectionistorelaxtheseconstraintstofurtherenhance
therepresentationalcapabilitiesforhandlingmorecomplexasynchronouscooperationtasks.
References
[1] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,
MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward.
InProceedingsofthe17thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems,page2085–2087,2018.
[2] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,Jakob
Foerster,andShimonWhiteson. Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcementlearning. JournalofMachineLearningResearch,21(178):1–51,2020.
[3] YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyao
Tang. Qatten: Ageneralframeworkforcooperativemultiagentreinforcementlearning. arXiv
preprintarXiv:2002.03939,2020.
[4] KaiArulkumaran,AntoineCully,andJulianTogelius. Alphastar: Anevolutionarycomputation
perspective. InProceedingsofthegeneticandevolutionarycomputationconferencecompanion,
pages314–315,2019.
[5] BRaviKiran,IbrahimSobh,VictorTalpaert,PatrickMannion,AhmadAAlSallab,Senthil
Yogamani,andPatrickPérez. Deepreinforcementlearningforautonomousdriving: Asurvey.
IEEETransactionsonIntelligentTransportationSystems,23(6):4909–4926,2021.
[6] YananWang,TongXu,XinNiu,ChangTan,EnhongChen,andHuiXiong. Stmarl: Aspatio-
temporalmulti-agentreinforcementlearningapproachforcooperativetrafficlightcontrol. IEEE
TransactionsonMobileComputing,21(6):2228–2242,2020.
[7] XiaoboZhou,ZhihuiKe,andTieQiu. Recommendation-drivenmulti-cellcooperativecaching:
A multi-agent reinforcement learning approach. IEEE Transactions on Mobile Computing,
2023.
[8] FransAOliehoek,ChristopherAmato,etal. AconciseintroductiontodecentralizedPOMDPs,
volume1. Springer,2016.
[9] YonghengLiang,HejunWu,andHaitaoWang. ASM-PPO:Asynchronousandscalablemulti-
agentppoforcooperativecharging. InProceedingsofthe21stInternationalConferenceon
AutonomousAgentsandMultiagentSystems,pages798–806,2022.
[10] JiaweiWangandLijunSun. Reducingbusbunchingwithasynchronousmulti-agentreinforce-
ment learning. In Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence,,pages426–433,2021.
[11] YuchenXiao,WeihaoTan,andChristopherAmato. Asynchronousactor-criticformulti-agent
reinforcementlearning. AdvancesinNeuralInformationProcessingSystems,35:4385–4400,
2022.
[12] Yongheng Liang, Hejun Wu, and Haitao Wang. Asynchronous multi-agent reinforcement
learning for collaborative partial charging in wireless rechargeable sensor networks. IEEE
TransactionsonMobileComputing,2023.
[13] Yuxin Chen, Hejun Wu, Yongheng Liang, and Guoming Lai. Varlenmarl: A framework of
variable-lengthtime-stepmulti-agentreinforcementlearningforcooperativecharginginsensor
networks. In202118thAnnualIEEEInternationalConferenceonSensing,Communication,
andNetworking,pages1–9.IEEE,2021.
10[14] HangtianJia,YujingHu,YingfengChen,ChunxuRen,TangjieLv,ChangjieFan,andChongjie
Zhang. Feverbasketball: Acomplex,flexible,andasynchronizedsportsgameenvironmentfor
multi-agentreinforcementlearning. arXivpreprintarXiv:2012.03204,2020.
[15] AfshinOroojlooyandDavoodHajinezhad. Areviewofcooperativemulti-agentdeepreinforce-
mentlearning. AppliedIntelligence,53(11):13677–13722,2023.
[16] CarlosFDaganzoandYanfengOuyang. Publictransportationsystems: Principlesofsystem
design,operationsplanningandreal-timecontrol. WorldScientific,2019.
[17] DavidE.RumelhartandJamesL.McClelland. AGeneralFrameworkforParallelDistributed
Processing,pages45–76. 1987.
[18] SiddhantM.Jayakumar,WojciechM.Czarnecki,JacobMenick,JonathanSchwarz,JackRae,
SimonOsindero,YeeWhyeTeh,TimHarley,andRazvanPascanu. Multiplicativeinteractions
andwheretofindthem. InInternationalConferenceonLearningRepresentations,2020.
[19] RoseEWang,SarahAWu,JamesAEvans,JoshuaBTenenbaum,DavidCParkes,andMax
Kleiman-Weiner. Toomanycooks: Coordinatingmulti-agentcollaborationthroughinverse
planning. 2020.
[20] MengYao,QiyueYin,JunYang,TongtongYu,ShengqiShen,JungeZhang,BinLiang,and
KaiqiHuang. Thepartiallyobservableasynchronousmulti-agentcooperationchallenge. arXiv
preprintarXiv:2112.03809,2021.
[21] FransAOliehoek,MatthijsTJSpaan,andNikosVlassis. Optimalandapproximateq-value
functionsfordecentralizedpomdps. JournalofArtificialIntelligenceResearch,32:289–353,
2008.
[22] Jiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Changjie Fan, Fei Wu, and
Jun Xiao. Deconfounded value decomposition for multi-agent reinforcement learning. In
InternationalConferenceonMachineLearning,pages12843–12856.PMLR,2022.
[23] ZichuanLiu,YuanyangZhu,andChunlinChen. NA2Q:Neuralattentionadditivemodelfor
interpretablemulti-agentq-learning. InInternationalConferenceonMachineLearning,pages
22539–22558.PMLR,2023.
[24] Trevor J Hastie. Generalized additive models. In Statistical models in S, pages 249–307.
Routledge,2017.
[25] KyunghwanSon, DaewooKim, WanJuKang, DavidEarlHostallero, andYungYi. Qtran:
Learningtofactorizewithtransformationforcooperativemulti-agentreinforcementlearning.
InInternationalconferenceonmachinelearning,pages5887–5896.PMLR,2019.
[26] YingWen,YaodongYang,RuiLuo,JunWang,andWeiPan. Probabilisticrecursivereasoning
formulti-agentreinforcementlearning. InInternationalConferenceonLearningRepresenta-
tions,2019.
[27] DavidHa,AndrewM.Dai,andQuocV.Le. Hypernetworks. InInternationalConferenceon
LearningRepresentations,2017.
[28] ChristianSchroederDeWitt,TarunGupta,DenysMakoviichuk,ViktorMakoviychuk,PhilipHS
Torr,MingfeiSun,andShimonWhiteson. Isindependentlearningallyouneedinthestarcraft
multi-agentchallenge? arXivpreprintarXiv:2011.09533,2020.
[29] JianhongWang, YuanZhang, YunjieGu, andTae-KyunKim. Shaq: Incorporatingshapley
valuetheoryintomulti-agentq-learning. AdvancesinNeuralInformationProcessingSystems,
35:5941–5954,2022.
[30] MengZhou,ZiyuLiu,PengweiSui,YixuanLi,andYukYingChung. Learningimplicitcredit
assignmentforcooperativemulti-agentreinforcementlearning. Advancesinneuralinformation
processingsystems,33:11853–11864,2020.
[31] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang.QPLEX:Duplexdueling
multi-agentq-learning. InInternationalConferenceonLearningRepresentations,2021.
11[32] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.
Duelingnetworkarchitecturesfordeepreinforcementlearning. InInternationalconferenceon
machinelearning,pages1995–2003.PMLR,2016.
[33] David H Wolpert and Kagan Tumer. Optimal payoff functions for members of collectives.
AdvancesinComplexSystems,4(02n03):265–279,2001.
[34] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
Counterfactualmulti-agentpolicygradients. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
[35] Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu,
ChangjieFan,andZhongyuWei.Q-valuepathdecompositionfordeepmultiagentreinforcement
learning. InInternationalConferenceonMachineLearning,pages10706–10715.PMLR,2020.
[36] MukundSundararajan,AnkurTaly,andQiqiYan. Axiomaticattributionfordeepnetworks. In
Internationalconferenceonmachinelearning,pages3319–3328.PMLR,2017.
[37] JianhongWang,YuanZhang,Tae-KyunKim,andYunjieGu. Shapleyq-value: Alocalreward
approachtosolveglobalrewardgames. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pages7285–7292,2020.
[38] LloydSShapley. Avalueforn-persongames. 1953.
[39] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu.
The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural
InformationProcessingSystems,35:24611–24624,2022.
[40] GeorgiosTheocharousandLeslieKaelbling. Approximateplanninginpomdpswithmacro-
actions. Advancesinneuralinformationprocessingsystems,16,2003.
[41] YiyuanLee,PanpanCai,andDavidHsu. Magic: Learningmacro-actionsforonlinepomdp
planningusinggenerator-critic. InRobotics: ScienceandSystemsConference,2021.
[42] AmatoChristopher,DKonidarisGeorge,andPKaelblingLeslie. Planningwithmacro-actions
indecentralizedpomdps. InInternationalConferenceonAutonomousAgentsandMultiagent
Systems,2014.
[43] ChristopherAmato,GeorgeKonidaris,LesliePKaelbling,andJonathanPHow. Modeling
andplanningwithmacro-actionsindecentralizedpomdps. JournalofArtificialIntelligence
Research,64:817–859,2019.
[44] Yuchen Xiao, Joshua Hoffman, Tian Xia, and Christopher Amato. Learning multi-robot
decentralizedmacro-action-basedpoliciesviaacentralizedq-net. In2020IEEEInternational
conferenceonroboticsandautomation,pages10695–10701.IEEE,2020.
[45] TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson.WeightedQMIX:expanding
monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning.InAdvances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
ProcessingSystems2020,NeurIPS2020,December6-12,2020,virtual,2020.
12A RelatedWork
A.1 CreditAssignmentMethods
Thecentralizedtraininganddecentralizedexecution(CTDE)[21]frameworkiswidelyadoptedby
modernMARLmethods. Itleveragesglobalinformationforcentralizedtraining,enablingagents
tomakedecentralizeddecisionsbasedontheirlocalinformation. AkeychallengeinCTDEisthe
creditassignmentproblem,whichinvestigatesagents’marginalcontributionstotheoverallsuccess,
facilitatingbetterlearningofagentpoliciesandenhancingcooperationamongagents[15].
Valuedecomposition(VD)standsasoneofthemostprevalentimplicitcreditassignmentmethods[30],
whichlearnsthemarginalcontributionofeachagentanddecomposestheglobalQ-valuefunction
Q intoindividualagentutilityfunctionsQ . VDN[1]directlyassumesequalcontributionsfrom
tot i
allagentsanddecomposesQ intothesumofQ s. QMIXutilizesamixingnetworktolearnthe
tot i
nonlinearrelationshipbetweenQ andQ . SinceQMIXconstructsthemixingnetworkusingMLP
tot i
layers,westillconsiderQMIXastheadditiveinteractionform[18]. QPLEX[31]introducesthe
duelingstructureQ=V +A[32]andperformsVDontheglobalstatevaluefunctionV aswell
tot
as the global advantage value function A , respectively. Qatten theoretically derives a general
tot
additiveinteractionformulaofQ intermsofQ andproposespracticalmulti-headattentionto
tot i
approximatetheweightofQ . DVD[22]employsbackdooradjustmenttoremovetheconfounding
i
biasinVD,aimingtoobtainbetterweightsofQ . Insummary,theVDalgorithmsmentionedabove
i
canbeformulatedastheadditiveinteractionformas(1),whichleadstolimitationsinaddressing
theunderlyingdependenciesamongagents[2,23]. Therefore,NA2Q[23]introducesageneralized
additive model (GAM) [24] to model all possible higher-order interactions among agents and to
interprettheircollaborationbehavior.
Ontheotherhand,explicitcreditassignmentmethods[30]introducedifferentconceptstoexplicitly
calculatethemarginalcontributionofeachagent. Inspiredbydifferencerewards[33],COMA[34]
proposesacounterfactualbaselinetocalculatethespecificcriticnetworkforeachagent. QPD[35]
employsintegratedgradients[36]toattributetheQ toeachagent’sfeaturechanges. SQPPDG
tot
[37]andSHAQ[29]extendshapleyvalue[38]forthecreditassignmentinanextendedconvexgame
withagrandcoalition. However,comparedwithexplicitcreditassignment,implicitmethodsoffer
superiorgeneralizationcapabilitiesandcanautonomouslylearnthemarginalcontributionsofagents
[30]. Therefore,inthispaper,wechoosetoextendtheimplicitcreditassignmentmethod,VD,tothe
asynchronousdomain.
A.2 AsynchronousMARL
Despite the significant progress in MARL, most existing MARL works rely on the premise of
synchronous decision-making which doesn’t reflect reality in many practical applications. The
simplest approach to relax the synchronous decision-making constraint in MARL and extend it
to the asynchronous domain is to split asynchronous actions spanning multiple time steps into
several sub-actions or to wait for all agents to complete their actions before making decisions
simultaneously. Evidently,thesemethodssignificantlyincreasetrainingcostsanddecreasetraining
efficiency. Therefore, several works have been conducted to exploit the strengths of MARL in
asynchronoussettings. AsmentionedinSection1,theseworkscanbroadlybecategorizedintotwo
typesofmethodsasdepictedindetailinthefollowing.
Thediscardingtypemethodsrecognizeasynchronousactionswithvaryingdurationsasawholeand
focussolelyontheinformationatdecisiontimesteps. ASM-PPO[9]andASM-HPPO[12]propose
thateachagentcollectsitsowndecisioninformationandutilizesMAPPO[39]fortraining. MAC
IAICC[11]adoptsasimilarapproachtoASM-PPOandASM-HPPO,treatingasynchronousactions
asmacro-actions[40,41]andstrictlymodelingthetaskasaMacDec-POMDP[42,43]. CAAC[10]
focusesonthebusholdingcontrol[16]andutilizesagraphattentionneuralnetworktocapturethe
influenceofotheragents’decisionsduringasynchronousactionexecution.
Thepaddingtypemethodsconvertasynchronousproblemsintosynchronousonesbyusingpadding
action,therebyallowingforthedirectapplicationofexistingMARLmethods. VarLenMARL[13]
employsthemostrecentactionforpadding. EXP-Ms[14]proposestomaskongoingactionsand
treat them as idle during the collection of joint transitions. Parallel-MacDec-MADDRQN [44]
introducesaparallelway. Inoneenvironment,itcollectssynchronousdatausingpaddingactionand
13trainsacentralizedQ-network. Inanotherenvironment,itcollectsasynchronousdataandtrainsa
decentralizedQ-networkwiththehelpofthecentralizedQ-network.
Overall,whileasynchronousMARLhasmadenotableprogress,thereremainsalackofappropriate
modelsforasynchronousdecision-makingscenarios,aswellastheoreticalandvisualanalysesof
asynchronouscreditassignment. Further, Sinceonlyconsideringasynchronousdataexacerbates
issues of environmental non-stationarity and asynchronous credit assignment, in this paper, we
proposeageneralasynchronousmodelbasedonthesynchronousdata.
B ProofofLemmas
Definition2. Ajointpolicyπ∗ ={π ,π }iscalledaMarkovPerfectEquilibrium(MPE)if:
i −i
Vπi,π−i(s)≥Vπ˜i,π−i(s), ∀s∈S, i∈N, ∀π˜ .
i
Definition3. TheBellmanoptimalityoperatorT∗forQ-learningvariantsinMARLis:
T∗Q(s,a)=E [r+γmaxQ(s′,a′)].
s′∼P
a′
Definition4. ThefunctionclassQ oftheapproximateglobalQ-valuefunctionQ inVDis:
tot tot
Q ={Q |Q (s,a)=f (Q (τ ,a ),··· ,Q (τ ,a )), whereQ andQ satisfyIGM}.
tot tot tot s 1 1 1 n n n tot i
Definition5. TheVDoperatorisT∗ :=(cid:81) ·T∗. T∗istheBellmanoptimalityoperatortolearn
thegroundtruthglobalQ-valuefuV nD ctionQV ∗.D(cid:81)
istheoperatorthatfindstheglobalQ-value
VD
functionQ inthefunctionclassQ toapproximateQ∗[45],whichisdefinedas:
tot tot
(cid:89)
Q(s,a)=argmin(Q(s,a)−q(s,a))2.
VD q∈Qtot
Lemma1. Givenanasynchronousdecision-makingtaskandajointpolicyπ,letQπ andQπ
Dec ADEX
representtheglobalQ-valuefunctionconvergedbymodelingthetaskwithDec-POMDPandADEX-
POMDP,respectively. ThenwehaveQπ (s,a)=Qπ (s,a).
Dec ADEX (cid:98) (cid:98)
Proof. WeusethesetX torepresentdifferentsinADEX-POMDPthatcorrespondtothesame
s (cid:98)
originals,andthesetY torepresentdifferent(s,a)thatcorrespondtothesameoriginal(s,a).
(s,a) (cid:98) (cid:98)
AsspecifiedinDefinition1,s=[s;o˜ ],anda=[a;a˜ ]. Foranysands′,wehaveX ∩X =∅.
(cid:98) c (cid:98) c s s′
Similarly,wealsohaveY ∩Y =∅.Therefore,thereisaone-to-onecorrespondencebetween
(s,a) (s′,a′)
sandX s,aswellasbetween(s,a)andY (s,a),whichimpliesthatP(cid:98)(X s′|Y (s,a))≡P(s′|s,a)and
r(Y )≡r(s,a).
(cid:98) (s,a)
Givenajointpolicyπ,theglobalQ-valuefunctionconvergedinDec-POMDPisdefinedas:
Qπ (s,a)=E [r1+γr2+···+γT−1rT].
Dec (s,a)∼(P,π)
AndtheglobalQ-valuefunctionconvergedinADEX-POMDPisdefinedas:
Qπ (s,a)=E [r1+γr2+···+γT−1rT]
ADEX (cid:98) (cid:98) (s(cid:98),a(cid:98))∼(P(cid:98),π) (cid:98) (cid:98) (cid:98)
=E E [r1+γr2+···+γT−1rT]
Y(s,a)∼(P(cid:98),π) (o˜c,a˜c)∼(OP,π) (cid:98) (cid:98) (cid:98)
=E [E [r1]+···+γT−1E [rT]]
Y(s,a)∼(P(cid:98),π) (o˜c,a˜c)∼(OP,π) (cid:98) (o˜c,aT cd)∼(OP,π) (cid:98)
=E [r1+γr2+···+γT−1rT] (9)
Y(s,a)∼(P(cid:98),π) (cid:98) (cid:98) (cid:98)
=E [r1+γr2+···+γT−1rT],
(s,a)∼(P,π)
where (9) represents the expectation over different cases in Y . However, since r(Y ) ≡
(s,a) (cid:98) (s,a)
r(s,a), meaning that different cases in this set correspond to the same reward, the expectation
operationcanberemoved. Finally,weprovethatgiventhesameπ,Qπ (s,a) = Qπ (s,a).
Dec ADEX (cid:98) (cid:98)
Lemma2. Givenanasynchronousdecision-makingtask,wedenotetheBellmanoptimalityoperator
inDec-POMDPasT∗ andinADEX-POMDPasT∗ . Similarly,Q∗ andQ∗ represent
Dec ADEX Dec ADEX
thegroundtruthglobalQ-valuefunctionswhenmodelingthetaskusingDec-POMDPandADEX-
POMDP,respectively. ThenwehaveQ∗ (s,a)=Q∗ (s,a).
Dec ADEX (cid:98) (cid:98)
14Proof. BasedonLemma1,wehave
T∗ Qπ (s,a)=E [r+γmaxQπ (s′,a′)]
ADEX ADEX (cid:98) (cid:98) s(cid:98)′∼P(cid:98) (cid:98)
a(cid:98)′
ADEX (cid:98) (cid:98)
=E E [r+γmaxQπ (s′,a′)]
X s′∼P(cid:98) o˜c∼OP (cid:98)
a(cid:98)′
ADEX (cid:98) (cid:98)
=E [r+γmaxQπ (s′,a′)] (10)
X s′∼P(cid:98) (cid:98)
a(cid:98)′
ADEX (cid:98) (cid:98)
=E [r+γmaxQπ (s′,a′)]
s′∼P Dec
a′
=T∗ Qπ (s,a),
Dec Dec
where(10)isderivedbasedonthefactthatfordifferents ands inX ,wehaveQπ (s ,a)=
(cid:98)1 (cid:98)2 s ADEX (cid:98)1 (cid:98)
Qπ (s ,a) = Qπ (s,a), thus allowing us to eliminate the expectation operation. Hence,
ADEX (cid:98)2 (cid:98) Dec
assumingthesameinitialjointpolicyπ ,bothT∗ andT∗ canconvergetothesameoptimal
0 Dec ADEX
globalQ-valuefunction,implyingQ∗ (s,a)=Q∗ (s,a).
Dec ADEX (cid:98) (cid:98)
C DerivationofMultiplicativeValueDecomposition
C.1 MultiplicativeInteractionForm
Accordingto[3],wecanexpandQ intermsofQ as:
tot i
(cid:88) (cid:88) (cid:88)
Q =c+ µ Q + µ Q Q +···+ µ Q ···Q +··· , (11)
tot i i ij i j i1···ik i1 ik
i ij i1,···,ik
where c is a constant and µ = 1 ∂kQtot , and then we can expand Q near the optimal
actiona∗as:
i1···ik k!∂Qi1···∂Qik i
i
Q (a )=α +β (a −a∗)2+o((a −a∗))2. (12)
i i i i i i i i
In the ADEX-POMDP setting, we denote the first-order term (cid:80) µ Q = (cid:80) λd Q +
(cid:80) λc Q +(cid:80) λc′ Q . Wethenapply(12)intothesecond-ordi eri teri min(11)i .d Foid r,1 brei vd ity,
ic ic, (cid:80)1 ic i′ c i′ c,1 i′ c (cid:80) (cid:80)
wesplit µ Q Q intotwoparts, µ Q Q and µ Q Q ,representingthe
i,j ij i j id,i′ c idi′ c id i′ c i d¯,ic¯ i d¯ic¯ i d¯ ic¯
interactionsbetweenallpairsofi andi′,andallotherinteractionsin{N,N′},respectively. For
(cid:80) d c c
µ Q Q ,leti ∈N ,wehave:
i d¯,ic¯ i d¯ic¯ i d¯ ic¯ d d
(cid:88) (cid:88)
µ Q Q = µ (α +β (a −a∗ )2)(α +β (a −a∗)2)+o(∥a−a∗ ∥2)
i d¯ic¯ i d¯ ic¯ i d¯ic¯ i d¯ i d¯ i d¯ i d¯ ic¯ ic¯ ic¯ ic¯
i d¯,ic¯ i d¯,ic¯
(cid:88) (cid:88) (cid:88)
= µ α α + µ α β (a −a∗ )2+ µ α β (a −a∗)2
i d¯ic¯ i d¯ ic¯ i d¯ic¯ ic¯ i d¯ i d¯ i d¯ i d¯ic¯ i d¯ ic¯ ic¯ ic¯
i d¯,ic¯ i d¯,ic¯ i d¯,ic¯
+o(∥a−a∗ ∥2)
(cid:88) (cid:88) (cid:88)
= µ α α + µ α (Q −α )+ µ α (Q −α )
i d¯ic¯ i d¯ ic¯ i d¯ic¯ ic¯ i d¯ i d¯ i d¯ic¯ i d¯ ic¯ ic¯
i d¯,ic¯ i d¯,ic¯ i d¯,ic¯
+o(∥a−a∗ ∥2)
(cid:88) (cid:88) (cid:88)
=− µ α α + µ α Q + µ α Q +o(∥a−a∗ ∥2)
i d¯ic¯ i d¯ ic¯ i d¯ic¯ ic¯ i d¯ i d¯ic¯ i d¯ ic¯
i d¯,ic¯ i d¯,ic¯ i d¯,ic¯
(cid:88) (cid:88) (cid:88)
=c + µ α Q + µ α Q + µ α Q +o(∥a−a∗ ∥2)
i d¯ic¯ iid i id ii′
c
i i′
c
iic i ic
i∈/N c′,id i∈/Nd,i′
c
i,ic
=c +(cid:88) λd Q +(cid:88) λc′ Q +(cid:88) λc Q +o(∥a−a∗ ∥2).
2 id,2 id i′ c,2 i′ c ic,2 ic
id i′
c
ic
Therefore,forthesecond-orderterm,weget:
(cid:88) µ Q Q =c +(cid:88) λd Q +(cid:88) λc′ Q +(cid:88) λc Q +(cid:88) λdc′ Q Q +o(∥a−a∗ ∥2).
ij i j 2 id,2 id i′ c,2 i′ c ic,2 ic idi′ c,2 id i′ c
i,j id i′
c
ic id,i′
c
15(cid:80)
Similarly,wenextconsiderthethird-orderterm µ Q Q Q andobtain:
i1,i2,i3 i1i2i3 i1 i2 i3
(cid:88) (cid:88) (cid:88)
µ Q Q Q =c + µ α α Q + µ α α Q
i1i2i3 i1 i2 i3 3 i1i2id i1 i2 id i1i2i′
c
i1 i2 i′
c
i1,i2,i3 i1,i2∈/N c′,id i1,i2∈/Nd,i′
c
(cid:88) (cid:88)
+ µ α α Q + µ α α Q
i1i2ic i1 i2 ic iidi′
c
id i′
c
i
i1∈/Ndori2∈/N c′,ic i,id,i′
c
(cid:88)
+ µ α Q Q +o(∥a−a∗ ∥2)
iidi′
c
i id i′
c
i,id,i′
c
=c +(cid:88) λd Q +(cid:88) λc′ Q +(cid:88) λc Q +(cid:88) λ Q
3 id,3 id i′ c,3 i′ c ic,3 ic dc′,3 i
id i′
c
ic i
+(cid:88) λdc′ Q Q +o(∥a−a∗ ∥2).
idi′ c,3 id i′ c
id,i′
c
Consequently,thegeneralformforthek-thordertermcanbeexpressedas:
(cid:88) µ Q ···Q =c +(cid:88) λd Q +(cid:88) λc′ Q +(cid:88) λc Q +(cid:88) λ Q
i1···ik i1 ik k id,k id i′ c,k i′ c ic,k ic dc′,k i
i1,···,ik id i′
c
ic i
+(cid:88) λdc′ Q Q +o(∥a−a∗ ∥2).
idi′ c,k id i′ c
id,i′
c
Weuse¯i ,··· ,¯i ∈{N,N′}torepresentthatany¯i and¯i donotsimultaneouslysatisfy¯i ∈N
1 k c x y x d
and¯i ∈N′. Sowetake:
y c
(cid:88)
λd = µ α ···α ,
id,k i1···ik−1id i1 ik−1
i1,···,ik−1∈/N c′
λc′
=
(cid:88)
µ α ···α ,
i′ c,k i1···ik−1i′ c i1 ik−1
i1,···,ik−1∈/Nd
(cid:88)
λc
ic,k
= µ¯i1···¯ik−1icα¯i1···α¯ik−1,
¯i1,···,¯ik−1
(cid:88)
λ =(k−2) µ α ···α α α ,
dc′,k i1···ik−2idi′
c
i1 ik−3 id i′
c
i1,···,ik−3,id,i′
c
λdc′
=
(cid:88)
µ α ···α .
idi′ c,k i1···ik−2idi′ c i1 ik−2
i1,···,ik−2
Therefore,underADEX-POMDP,wecanobtainthemultiplicativeinteractionvaluedecomposition
formula(5)neartheoptimaljointactionasfollows:
Q ≈(cid:88) c +(cid:88)(cid:88) (λd +λ )Q +(cid:88)(cid:88) (λc′ +λ )Q
tot k id,k dc′,k id i′ c,k dc′,k i′ c
k id k i′
c
k
+(cid:88)(cid:88) (λc +λ )Q +(cid:88)(cid:88) λdc′ Q Q
ic,k dc′,k ic idi′ c,k id i′ c
ic k id,i′
c
k
=c+(cid:88) λd Q +(cid:88) λc′ Q +(cid:88) λc Q +(cid:88) λdc′ Q Q (13)
id id i′ c i′ c ic ic id,i′ c id i′ c
id i′
c
ic id,i′
c
n+n′
=c+
(cid:88)c
λ Q
+(cid:88) λdc′
Q Q .
i i id,i′
c
id i′
c
i id,i′
c
The convergence of λ just needs mild conditions, like boundedness of α or small growth of
i
∂kQtot
intermsofk.
∂Qi1···∂Qik
16C.2 High-OrderInteractionForm
Basedonthederivationprocessof(13),wecanfurtherexplorethehigher-orderinteractionformsbe-
tweenagentsi andi′,andobtaintheKth-order(where1≤K ≤n)interactionvaluedecomposition
d c
formula(6)asfollows:
(cid:88) (cid:88)(cid:88)
Q ≈ c + (λd,K +λK +···+λK )Q
tot k id,k dc′,k dc′ 1···c′ K−1,k id
k id k
+(cid:88)(cid:88) (λc′,K +λK +···+λK )Q
i′ c,k dc′,k dc′ 1···c′ K−1,k i′ c
i′ k
c
(cid:88)(cid:88)
+ (λc,K +λK +···+λK )Q
ic,k dc′,k dc′ 1···c′ K−1,k ic
ic k
+(cid:88)(cid:88) λdc′,KQ
Q +···+
(cid:88) (cid:88) λdc′ 1···c′ K−1,K
Q Q ···Q
idi′ c,k id i′ c idi′ c,1···i′ c,K−1,k id i′ c,1 i′ c,K−1
id,i′
c
k idi′ c,1···i′
c,K−1
k
=c+(cid:88) λd,KQ +(cid:88) λc′,KQ +(cid:88) λc,KQ
id id i′ c i′ c ic ic
id i′
c
ic
+(cid:88) λdc′,KQ
Q +···+
(cid:88) λdc′ 1···c′ K−1,K
Q Q ···Q
id,i′ c id i′ c idi′ c,1···i′ c,K−1 id i′ c,1 i′ c,K−1
id,i′
c
idi′ c,1···i′
c,K−1
n+n′
=c+ (cid:88)c λKQ +(cid:88) λdc′,KQ Q +···+ (cid:88) λdc′ 1···c′ K−1,K Q Q ···Q ,
i i id,i′ c id i′ c idi′ c,1···i′ c,K−1 id i′ c,1 i′ c,K−1
i id,i′
c
idi′ c,1···i′
c,K−1
where:
(cid:88)
λd,K = µ α ···α ,
id,k i1···ik−1id i1 ik−1
i1,···,ik−1∈/N c′
λc′,K
=
(cid:88)
µ α ···α ,
i′ c,k i1···ik−1i′ c i1 ik−1
i1,···,ik−1∈/Nd
(cid:88)
λc ic,K
,k
= µ¯i1···¯ik−1icα¯i1···α¯ik−1,
¯i1,···,¯ik−1
(cid:88)
λK =(k−2) µ α ···α α α ,
dc′,k i1···ik−2idi′
c
i1 ik−3 id i′
c
i1,···,ik−3∈/N c′,id,i′
c
(cid:88)
λK =(k−3) µ α ···α α α α ,
dc′ 1c′ 2,k i1···ik−3idi′ c,1i′ c,2 i1 ik−4 id i′ c,1 i′ c,2
i1,···,ik−4∈/N c′,id,i′ c,1,i′
c,2
···
(cid:88)
λK =(k−K) µ α ···α α α ···α ,
dc′ 1···c′ K−1,k i1···ik−Kidi′ c,1···i′ c,K−1 i1 ik−K−1 id i′ c,1 i′ c,K−1
i1,···,ik−K−1,c′ 1,···,c′
K−1
λdc′,K
=
(cid:88)
µ α ···α ,
idi′ c,k i1···ik−2idi′ c i1 ik−2
i1,···,ik−2∈/N c′
dc′c′,K (cid:88)
λ idi1 ′ c,2 1i′ c,2,k = µ i1···ik−3idi′ c,1i′ c,2α i1···α ik−3,
i1,···,ik−3∈/N c′
···
dc′···c′ ,K (cid:88)
λ 1 K−1 = µ α ···α .
idi′ c,1···i′ c,K−1,k i1···ik−Kidi′ c,1···i′ c,K−1 i1 ik−K
i1,···,ik−K
17C.3 PracticalImplementationForm
ToachieveMVD-basedIGM,weonlyneedQ tosatisfyIGM.Therefore,wecanconvert(5)into
id
anadditiveinteractiveform,Q =b+WQ ,andthenensurethatW >0. Specifically:
tot d
n+n′
(cid:88)c (cid:88)
Q =k + k Q + k Q Q
tot 0 i i idi′
c
id i′
c
i id,i′
c
(cid:88) (cid:88) (cid:88) (cid:88)
=(k + k Q + k Q )+ (k + k Q )Q .
0 i′
c
i′
c
ic ic id idi′
c
i′
c
id
i′
c
ic id i′
c
However,incertainscenarios,aswecannotensureQ >0,weneedtokeeptrackoftheminimum
i′
c
valueofQ duringthetrainingprocess:
i′
c
Q =(k +(cid:88) k Q +(cid:88) k Q )+(cid:88) ((k −(cid:88) k Qmin)+(cid:88) 2k Q i′ c +Qm c in )Q .
tot 0 i′ c i′ c ic ic id idi′ c c idi′ c 2 id
i′
c
ic id i′
c
i′
c
InthespecificimplementationofMVD,weutilizeahypernetworkf (s)tolearntheweightsin(5).
i
Finally,weobtain:
Q ≈(f +(cid:88) |f |Q +(cid:88) |f |Q )+(cid:88) (|f |+(cid:88) |f |Q i′ c +Qm c in )Q .
tot 0 i′ c i′ c ic ic id idi′ c 2 id
i′
c
ic id i′
c
D PseudoCode
Algorithm1MultiplicativeValueDecomposition
InitializeasetofagentsN ={1,2,··· ,n}
InitializenetworksofindividualagentsQ (τ ,a ;θ)andtargetnetworksQ′(τ′,a′;θ′)
i i i i i i
InitializemixingnetworkQ (s,a;ϑ)andtargetnetworkQ′ (s,a;ϑ′)
tot (cid:98) (cid:98) tot (cid:98) (cid:98)
InitializeareplaybufferBforstoringepisodes
Initializeajointobservationo˜andajointactiona˜ofallagents
repeat
Initializeahistoryembeddingh0andanactionvectora0foreachagent
i i
Observeeachagent’spartialobservation[o1]n
i i=1
Foreachagenti executingaction,introduceacorrespondingextraagenti′
c c
Updateo˜
,obtains1and[o1]n+n′
c
c (cid:98) i i=1
for t=1:T do
Getτt ={ot,ht−1}foreachagentandcalculatetheindividualutilityfunctionQ (τt,at−1)
i i i i i i
Getthehiddenstateht andselectactionat viaQ withprobabilityϵexploration. Forall
i i i
agentsi′,useanactionmasktorestricttheirchoicestoa˜
c c
Executeat,updatea˜
(cid:98)
UpdateN′ando˜
c c
Receivearewardrt,transitiontothenextstatest+1
(cid:98) (cid:98)
endfor
StoretheepisodetrajectorytoB
SampleabatchofepisodestrajectorieswithbatchsizebfromB
for t=1:T do
CalculatetheglobalQ-valueQ via(8)
tot
Calculatethetargety =rt+Q′ usingtargetnetwork
(cid:98) tot
endfor
UpdateθandϑbyminimizingthelossL(θ,ϑ)=(Q −y)2
tot
Periodicallyupdateθ′ ←θ,ϑ′ ←ϑ
untilQ (τ ,a ;θ)converges
i i i
18E ExperimentalDetails
E.1 Overcooked
(a)Overcooked-A (b)Overcooked-B (c)Overcooked-C (d)SaladRecipe
Figure10: Overcookedenvironments
IntheOvercooked2benchmark,threeagentsmustcollaboratetoprepareaTomato-Lettuce-Onion
saladanddeliverittothestarcountercellassoonaspossible. Thechallengeliesinthefactthat
theagentsarenotawareofthecorrecttasksequencebeforehand: fetchingrawvegetables,placing
themonthecut-boardcell,choppingthem,mergingtheminaplate,andfinallydeliveringthedish
tothecounter. Agentscanonlyobservealimited5×5gridsurroundingthemselves. Theaction
spaceisdividedintoprimitive-actionspaceandmacro-actionspace. Primitive-actionspaceincludes
fiveactionsthatallowagentstomovearoundandcompleteothertasks: up,down,left,right,and
stay. Forexample, ifanagentholdingrawvegetablesstopsinfrontofthecut-boardcell, itwill
automaticallystartchopping. Themacro-actionspaceincludestenactionsthatarecombinations
ofmultipleprimitive-actions: Chop,Get-Lettuce/Tomato/Onion,Get-Plate-1/2,Go-Cut-Board-1/2,
Go-Counter,Deliver. Chopreferstocuttingarawvegetableintopieces,whichrequirestheagent
to hold vegetables and stay in front of the cutting board cell for three time steps. Get-Lettuce,
Get-Tomato,andGet-Onionrefertotheagentmovingtotheobservedlocationofrawingredients
andpickingupthecorrespondingvegetables. Get-Plate-1andGet-Plate-2refertotheagentmoving
totheobservedlocationsoftwoplates. Get-Cut-Board-1andGet-Cut-Board-2refertotheagent
movingtothelocationsoftwocut-boardcells. Go-CounterisonlyavailableinmapBandrefersto
theagentmovingtothecentercelltopickuporputdownitems. Deliverreferstotheagentmoving
tothestarcountercell. Choppingvegetablessuccessfullyyieldsa+10reward. Correctlydelivering
asaladtothestarcountercellearnsa+200reward,whereasdeliveringthewrongdishincursa−5
penalty. Furthermore,a−0.1penaltyisimposedforeachtimestep. Themaximaltimestepsofan
episodeis200. Eachepisodeterminatesifagentssuccessfullydeliveratomato-lettuce-onionsalad.
E.2 POAC
Partially observable asynchronous multi-agent cooperation
challenge(POAC)3isaconfrontationwargamebetweentwo
armies, with each army consisting of three heterogeneous
agents: infantry,chariot,andtank. Theseagentspossessdis-
tinctattributesanddifferentmovingspeeds, asdescribedin
Table1. Therewardisdeterminedbythedifferenceinhealth
lossbetweenalliesandenemies. Theultimateobjectiveisto
learnasynchronouscooperationstrategiestomaximizethewin
rateineverybattle,ensuringthatagentssustainlessdamage Figure11:Anexampleofamap,the
thantheiropponentswithin600timesteps. Allbotsareplaced solidgirdsarethehiddenterrainand
on a hexagonal map which contains hidden terrain where it thesquaresarethebots.
isdifficultforenemiestoobservewhenagentsarelocated,as
showninFigure11. POACprovidesthreebuilt-inrule-based
botswithdifferentstrategies: 1)KAI0: anaggressiveAIthatprioritizesattackingandheadsstraight
forthecenterofthebattlefield. 2)KAI1: anAIprefershidinginspecialterrainforambush,aclassic
wargametactic. 3)KAI2: anAIutilizingguidedshooting,whereoperatorssquatonspecialterrain
2ThecodeofOvercookedisfromhttps://github.com/WeihaoTan/gym-macro-overcooked?tab=readme-ov-file.
3ThecodeofPOACisfromhttp://turingai.ia.ac.cn/data_center/show/10.
19andattackenemiesduetotheadvantageofsight. Inthispaper,weconductexperimentsonthefive
scenariosconsistingofdifferentmapsanddistinctstrategiesshowninTable2. Inaddition,inthe
originalPOACbenchmark,infantry,chariots,andtanksmoveatspeedsof0.2,1,and1,respectively.
Thismeansthatonlyinfantrymovementrequiresfivetimesteps,withallotheractionsconcludingin
one. ThislimitedasynchronicityintheoriginalPOACdoesnotfullydemonstratethestrengthsofour
asynchronouscreditassignmentmethod. Therefore,inthispaper,weadjustthemovementspeedsof
infantry,chariots,andtanksto0.2,0.3,and0.5.
Table1: Operatorsdetails
Attribution Infantry Chariot Tank
blood 7 8 10
speed 0.2 0.3 0.5
observeddistance 5 10 10
attackeddistance 3 7 7
attackdamageagainsttankorchariot 0.8 1.5 1.2
probabilityofcausingdamageagainsttankorchariot 0.7 0.7 0.8
attackdamageagainstinfantry 0.8 0.8 0.6
Probabilityofcausingdamageagainstinfantry 0.6 0.6 0.6
shootcool-down 1 1 1
shootpreparationtime 2 2 0
canguideshoot True True False
Table2: Operatorsattributesindifferentscenarios.
Scenarios MapSize SpecialTerrain GuideShoot
scenario1 (13,23) False False
scenario2 (13,23) True False
scenario3 (17,27) True True
scenario4 (27,37) True True
scenario5 (27,37) True True
E.3 Hyperparameters
WecompareourMVDagainstfivepopularvalue-basedbaselinesbasedonDec-POMDPwithpadding
action,includingVDN[1],QMIX[2],Qatten[3],SHAQ[29],andNA2Q[23]thatconsiders2nd-
orderinteractions. Additionally,wealsoevaluateAC-basedmethods,IPPO[28]basedonDTDEand
MACIAICC[11]basedonMacDec-POMDP.WedonotselectCAACbecauseitscodeisnotopen-
sourceanditsapplicationislimitedtobusholdingcontrol. Weimplementthesebaselinesandour
MVDviaPyMARL24. AllhyperparametersfollowthecodeprovidedbythePOACbenchmarkand
aremaintainedatalearningrateof0.0005bytheAdamoptimizer,asshowninTable3. Additionally,
fortheAC-basedmethods,weutilize4parallelenvironmentsfordatacollectionandsetthebuffer
sizeandbatchsizeto16. OurexperimentsareperformedonanNVIDIAGeForceRTX4090GPU
andanIntelXeonSilver4314CPU.
InourimplementationofMVD,weoptedtousetheoriginalstatesinsteadofthestatesinADEX-
(cid:98)
POMDP, as we found that using s did not significantly improve the performance. We employed
(cid:98)
blankactionsaspaddingactionsforADEX-POMDP.Toensureconsistentjointactiondimensionsin
ADEX-POMDP,wealwaysintroducednagentsateachtimestepandutilizedanactionmasktofilter
outtheinvalidagentactions.
F PerformanceonAllOvercookedMaps
We conducted experiments on all three maps provided by the Overcooked benchmark shown in
Figure10. TheresultsshowninFigure12demonstratethatourMVDoutperformsotherbaselinesin
4ThecodeofPyMARL2isfromhttps://github.com/hijkzzz/pymarl2.
20Table3: ExperimentalsettingsofOvercookedandPOAC.
Hyper-parameters Value Description
batchsize 32 numberofepisodesperupdate
testinterval 2000 frequencyofevaluatingperformance
testepisodes 20 numberofepisodestotest
buffersize 5000 maximumnumberofepisodesstoredinmemory
discountfactorγ 0.99 degreeofimpactoffuturerewards
totaltimesteps 5050000 numberoftrainingsteps
startε 1.0 thestartεvaluetoexplore
finishε 0.05 thefinishεvaluetoexplore
annealstepsforε 50000 numberofstepsoflinearannealing
targetupdateinterval 200 thetargetnetworkupdatecycle
MVD QMIX SHAQ IPPO
VDN Qatten NA2Q MAC IAICC
Map A Map B Map C
200 200 200
150 150 150
100 100 100
50 50 50
0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Figure12: MeantestreturnonthreemapsofOvercookedbenchmark.
differentscenarios. IPPOandMACIAICC,whichexhibitedslowertrainingspeedsonthesimple
mapA,becomeunabletoconvergeonthemorecomplexandhighlycollaborativemapsBandC.This
indicatesthatdiscardingdecisioninformationfromotheragentsstruggleswithcomplexasynchronous
cooperationtasks. Similarly,VDN’sassumptionthatallagentscontributeequallycannotcopewith
complexscenarios.Duetointerferencefromredundantpaddingactionandcomplexcreditassignment
models,NA2QandSHAQfailtoconvergeinallscenarios. QMIXandQattenutilizerelativelysimple
mixingnetworkstructures,demonstratinggreaterrobustnesstopaddingaction. Consequently,they
exhibitcomparativelybetterperformancethanNA2QandSHAQ.MapBisdividedintotwosections,
posingspecialdemandsoncooperationbetweenagents: Agentsintherightsectioncanonlyprepare
rawvegetablesandplates, whileagentsintheleftsectionarerestrictedtochoppingandserving.
Thecooperationbetweenagentsindifferentsectionsisweak. Thisarrangementchallengesmost
baselinestograsptheoptimalcooperationstrategy. VDNdirectlyoverlooksthemutualinfluence
amongdifferentagents,facilitatingthelearningofsub-optimalstrategyinstead.
G PerformanceonAllPOACScenarios
AsshowninFigure13,wecomparetheperformanceofourMVDwiththebaselinesinfivescenarios
providedbythePOACbenchmark. ItcanbeobservedthatMVDremainscompetitiveinsimple
scenarios1and2. Inscenario1,bothIPPOandMACIAICCachievedoptimalperformance. This
isprimarilyduetothefactthatinthisparticularsetting,asingleagentaloneissufficienttodefeat
theopposingarmy. Over-consideringcooperationamongagentsmayactuallyleadthealgorithmto
convergetoasub-optimalstrategy. SHAQrequirestheexplicitcomputationoftheshapleyvaluefor
eachagent. Nonetheless,becausepaddingactionshavenoactualimpactontheenvironment,this
ultimatelycontributestoitsfailure. Furthermore,inScenario2wheretheopposingarmyadoptsa
conservativestrategybyhidinginspecialterrains,onlythemultiplicativeinteractioninMVDandthe
GAMinNA2Qprovideadequaterepresentationalcapabilitiestolocatetheenemy’shidingposition
andlearntheoptimaljointstrategy. Moreover,becauseMVDemploysasimplermodeltocapture
21
nruteR
tseT
naeM
nruteR
tseT
naeM
nruteR
tseT
naeMMVD QMIX SHAQ IPPO
VDN Qatten NA2Q MAC IAICC
Scenario 1 Scenario 2 Scenario 3
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure13: Testwinrate%onallscenariosofPOACbenchmark.
interactionsamongagents,itstrainingefficiencyishigherthanthatofNA2Q.Conversely,dueto
limitedmodelrepresentationorexploratorycapabilities,theotherbaselinesrequiremoretraining
datatoachieveasophisticatedcooperationstrategy.
H AdditionalAblationStudy
H.1 AdditionalAblationStudyonADEX-POMDP
Tocomprehensivelyevaluatetheadvantagesofmodelingasynchronousdecision-makingscenarios
usingADEX-POMDP,weextendtheadditiveinteractionVDalgorithms,includingVDN,QMIX,Qat-
ten,andthehigh-orderinteractionVDalgorithmNA2Q,toADEX-POMDP.Thisallowssynchronous
creditassignmentmethodstouniformlyconsiderthemarginalcontributionsofasynchronousdeci-
sionsfromdifferenttimesteps. WecomparetheimpactofintroducingextraagentsontheseVD
algorithms.
AsshowninFigure14,15,16,fortheadditiveinteractionVDalgorithms,includingVDN,QMIX,
andQatten,theintroductionofextraagentssignificantlyimprovesmethodperformance,especially
forQMIXinscenario2. Thissuggeststhatmodelingasynchronousdecision-makingscenariosas
ADEX-POMDPfacilitatesthesolutionofasynchronouscreditassignmentproblemsandenhances
training efficiency. However, as shown in Figure 17, this is not the case for NA2Q. Introducing
extraagentscanactuallybringredundantagentinteractionstoNA2Q,suchasthosebetweenagent
i andagenti′. Thesehigh-orderinteractioninformationdeterioratestheperformanceofNA2Q.
c c
ThisimpliesthatforADEX-POMDP,wemustcarefullyhandlethehigh-orderinteractionsbetween
agents.
H.2 AdditionalAblationStudyonHigh-OrderInteraction
Toevaluatetheimpactofhigher-orderinteractionsontheperformanceofVDalgorithmsinasyn-
chronous settings, we extend NA2Q to ADEX-POMDP and compared MVD and NA2Q under
differentordersofinteractionacrossfivescenariosprovidedbythePOACbenchmark. Asshownin
22
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseTMVD VDN VDN(A)
Scenario 1 Scenario 2 Scenario 3
1.0
1.0 1.0
0.8
0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure14: InfluenceofintroducingextraagentsonVDN.
MVD QMIX QMIX(A)
Scenario 1 Scenario 2 Scenario 3
1.0
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure15: InfluenceofintroducingextraagentsonQMIX.
Figure18,MVD(2)whichintroducesmultiplicativeinteractiontocapturetheeffectsofasynchronous
decision-making,consistentlyoutperformsMVD(1)whichonlyusesadditiveinteraction. Basedon
23
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseTMVD Qatten Qatten(A)
Scenario 1 Scenario 2 Scenario 3
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure16: InfluenceofintroducingextraagentsonQatten.
MVD NA2Q NA2Q(A)
Scenario 1 Scenario 2 Scenario 3
1.0
1.0 1.0
0.8
0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure17: InfluenceofintroducingextraagentsonNA2Q.
thederivationinAppendixC.1,high-orderinteractionsnotonlyprovidedeepinformationonthe
interplaybetweenagentsbutalsoreducethenumberofTaylorexpansionsusedinthederivation
24
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseTMVD(2) MVD(1) MVD(3) NA2Q(2) NA2Q(3)
Scenario 1 Scenario 2 Scenario 3
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure18: InfluenceofdifferentorderinteractionsonMVDandNA2Q.
process,therebyenhancingtheaccuracyoftheVDformula. However,theintroductionofhigh-order
interactionscomplicatesthemodel,thustheperformanceofMVD(3)remainsinferiortoMVD(2).
In the case of NA2Q, higher-order interactions may slightly improve performance, but they also
carrytheriskofdegradingit. Therefore,inADEX-POMDP,theinteractionbetweenQ andQ is
id i′
c
sufficienttoefficientlysolvetheasynchronouscreditassignmentproblem.
H.3 AdditionalAblationStudyonOtherImplementationForms
WecomparedthreedifferentpracticalimplementationsofMVDacrossfivescenariosinPOAC.As
showninFigure19,themethodofdirectlyobtainingtheglobalQ-valueusing(8)exhibitsslower
trainingspeedandunstabletrainingprocess. AlthoughtheuseofSoftmaxtoimplementamulti-head
structurecansomewhatimprovetrainingspeedandstability,itsperformanceremainspoorincertain
scenarios. Therefore,thissuggeststhatforMVDthatincorporatesmultiplicativeinteractions,the
structureofthemixingnetworkshouldnotbeoverlycomplex.
25
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseTMVD(MLP) MVD MVD(Softmax)
Scenario 1 Scenario 2 Scenario 3
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure19: InfluenceofdifferentimplementationsonMVD.
26
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT