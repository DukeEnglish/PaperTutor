Hybrid Reinforcement Learning Breaks Sample Size Barriers
in Linear MDPs
Kevin Tan, Wei Fan, and Yuting Wei
Department of Statistics and Data Science
The Wharton School, University of Pennsylvania
Abstract
Hybrid Reinforcement Learning (RL), where an agent learns from both an offline dataset and on-
lineexplorationsinanunknownenvironment,hasgarneredsignificantrecentinterest. Acrucialquestion
posedbyXieetal.(2022b)iswhetherhybridRLcanimproveupontheexistinglowerboundsestablished
in purely offline and purely online RL without relying on the single-policy concentrability assumption.
While Li et al. (2023b) provided an affirmative answer to this question in the tabular PAC RL case,
the question remains unsettled for both the regret-minimizing RL case and the non-tabular case. In
thiswork,buildinguponrecentadvancementsinofflineRLandreward-agnosticexploration,wedevelop
computationallyefficientalgorithmsforbothPACandregret-minimizingRLwithlinearfunctionapprox-
imation, without single-policy concentrability. We demonstrate that these algorithms achieve sharper
error or regret bounds that are no worse than, and can improve on, the optimal sample complexity in
offlineRL(thefirstalgorithm,forPACRL)andonlineRL(thesecondalgorithm,forregret-minimizing
RL)inlinearMarkovdecisionprocesses(MDPs),regardlessofthequalityofthebehaviorpolicy. Toour
knowledge, this work establishes the tightest theoretical guarantees currently available for hybrid RL in
linear MDPs.
Contents
1 Introduction 2
1.1 Hybrid RL: two approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Our contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminaries 5
2.1 Basics of Markov decision processes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Linear MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Exploring the state-action space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 Algorithms and main results 7
3.1 Offline RL after online exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Online regret minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4 Numerical experiments 12
5 Discussion, limitations and future work 13
A Unabridged versions of our algorithms 17
B Proofs for Theorem 1 19
C Proof of Corollary 1 22
1
4202
guA
8
]LM.tats[
1v62540.8042:viXraD On concentrability and coverability 23
E Proofs for Algorithm 2 25
E.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 High-probability events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.3 Regret decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.4 Offline regret control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E.5 Online regret control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E.6 Putting everything together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
F OPTCOV from Wagenmaker and Jamieson (2023) 31
G Miscellanous lemmas 33
1 Introduction
Reinforcement learning (RL) holds great promise in attaining reliable decision-making in adaptive environ-
ments for a broad range of modern applications. In these applications, typical RL algorithms often require
an enormous number of training samples in order to reach the desired level of accuracy. This has motivated
alineofrecenteffortstostudythesampleefficiencyofRLalgorithms. Therearetwomainstreamparadigms
of RL, distinguished by how samples are collected: online RL and offline RL. In the setting of online RL,
an agent learns in a real-time manner, exploring the environment to maximize her cumulative rewards by
executing a sequence of adaptively chosen policies (e.g. Azar et al. (2017); Kearns and Singh (2002); Jin
et al. (2018); Sutton and Barto (2018); Zhang et al. (2023)). These online RL algorithms often suffer from
insufficient use of data samples due to a lack of a reference policy at the initial stage of the learning process.
Whereas, in offline RL, an agent has only access to a pre-collected dataset, and tries to figure out how
to perform well in a different environment without ever experiencing it (e.g. Levine et al. (2020); Lange
et al. (2012); Jin et al. (2021b); Xie et al. (2022b); Li et al. (2024)). Offline methods therefore often impose
stringent requirements on the quality of the pre-collected data.
Toaddresslimitationsfrombothcases, thesettingofhybridRL(Xieetal.,2022b;Songetal.,2023)has
recently received considerable attention from both theoretical and practical perspectives (see, e.g. Vecerik
et al. (2017); Nair et al. (2020); Song et al. (2023); Nakamoto et al. (2023); Wagenmaker and Pacchiano
(2023); Li et al. (2023b); Ball et al. (2023); Zhou et al. (2023); Amortila et al. (2024); Tan and Xu (2024);
Kausiketal.(2024)andreferencestherein). InhybridRL,anagentlearnsfromacombinationofbothoffline
and online data, extracting information from offline data to enhance online exploration. The theoretical
guarantees of hybrid RL algorithms can be categorized based on the following criteria: (1) the degree of
function approximation considered, (2) the level of coverage required by the behavior policy, (3) whether it
achieves an improvement over the minimax lower bounds for online-only and offline-only learning, and (4)
whether they attempt to perform regret minimization or to learn an ϵ-optimal policy (often referred to as
PAC learning). We elaborate below, and summarize the prior art in Table 1.
Paper FunctionType Concentrability? Improvement? RegretorPAC?
Songetal.(2023) General Required No Regret
Nakamotoetal.(2023)
TanandXu(2024) General NotRequired No Regret
Amortilaetal.(2024)
WagenmakerandPacchiano(2023) Linear NotRequired No PAC
Lietal.(2023b) Tabular NotRequired Yes PAC
Thiswork Linear NotRequired Yes Regret,PAC
Table 1: Comparison of our contributions to previous work in hybrid RL.
While most of the prior literature (Song et al., 2023; Nakamoto et al., 2023; Zhou et al., 2023; Tan and
Xu, 2024; Amortila et al., 2024) have explored general function approximation in hybrid RL, they either
require stringent concentrability assumptions on the quality of the behavior policy, or fail to obtain tight
2theoretical guarantees. Under such single-policy concentrability assumptions (explained below), it has been
shown in Xie et al. (2022b) that the optimal policy learning algorithm is either a purely offline reduction
or a purely online RL algorithm if the agent can choose the ratio of offline to online samples, rendering the
benefits of hybrid RL questionable. In scenarios where this assumption is not satisfied (Li et al., 2023b;
Wagenmaker and Pacchiano, 2023; Tan and Xu, 2024; Amortila et al., 2024), Li et al. (2023b) obtained
theoretical guarantees for PAC RL that improve over lower bounds established for offline-only and online-
only RL. However, the work of Li et al. (2023b) remains restricted to the tabular case with finite number of
states and actions.
TofurtherexploretheefficacyofhybridRL,thispaperfocusesonobtainingsharpertheoreticalguarantees
in the setting of linear function approximation, specifically in the case of linear MDPs. First proposed in
Yang and Wang (2019); Jin et al. (2019), the linear MDP setting parameterizes the transition probability
matrix and reward function by linear functions of known features. It has since been extensively studied due
to its benefits in dimension reduction and mathematical traceability in both the online and offline settings
(see, e.g. Yang and Wang (2019); Qiao and Wang (2022); Du et al. (2019); Li et al. (2021); Jin et al. (2019);
Zanette et al. (2021); Yin et al. (2022); Xiong et al. (2023); He et al. (2023); Hu et al. (2023); Min et al.
(2021);DuanandWang(2020)). Despitetheseefforts, hybridRLalgorithmsforlinearMDPs(Wagenmaker
and Pacchiano, 2023; Song et al., 2023; Nakamoto et al., 2023; Amortila et al., 2024; Tan and Xu, 2024)
have suboptimal worst-case guarantees (Table 2), which raises the question:
Is it possible to develop sample efficient RL algorithms in the setting of hybrid RL that are provably better
than online-only and offline-only algorithms for linear MDPs?
1.1 Hybrid RL: two approaches
Before answering the question above, we begin by introducing two types of approaches that are widely-
adopted in hybrid RL.
The offline-to-online approach: Mostofthecurrentliterature(e.g. Songetal.(2023);Nakamotoetal.
(2023); Amortila et al. (2024); Tan and Xu (2024)) adopts the approach of initializing the online dataset
with offline samples, in order to perform regret-minimizing online RL. We shall refer to this as the offline-
to-online approach. This method is simple and natural, offering several additional benefits. For instance,
thealgorithmoptimizestherewardreceivedduringeachonlineepisode, makingitsuitablewhenitiscrucial
for the agent to perform well on average during its online exploration.
The online-to-offline approach: However, if our goal is to output a near-optimal policy, especially in
real-world situations such as medical treatment and defense-related applications, it is not ideal and some-
times even unethical to provide a randomized policy with guarantees that hold only on average. Recently,
Wagenmaker and Pacchiano (2023) and Li et al. (2023b) propose using reward-agnostic online exploration
toexplorethepartsofthestatespacethatarenotwell-coveredbythebehaviorpolicy, therebyconstructing
a dataset that is especially amenable for offline RL. We refer to this as the online-to-offline approach. This
method allows leveraging the sharp performance guarantees of offline RL when the single-policy concentra-
bility coefficient is low. While this approach does not optimize the “true reward” during online exploration,
and is therefore incompatible with the regret minimization framework, it avoids the need to deploy mixed
policiestoachieveaPACbound, allowingforthedeploymentoffixed, andthusmoreinterpretable, policies.
1.2 Our contributions
In view of these two types of hybrid RL algorithms, focusing on the setting of linear MDPs, we answer the
aforementioned question in the affirmative. We summarize our main contributions below.
• Weproposeanonline-to-offlinealgorithmcalledReward-AgnosticPessimisticPACExploration-initialized
Learning (RAPPEL) in Algorithm 1. This algorithm employs reward-agnostic online exploration to
enhance the offline dataset, followed by a pessimistic offline RL algorithm to learn an optimal policy.
We show that the sample complexity of Algorithm 1 significantly improves upon the only dedicated
hybrid RL algorithm for linear MDPs (Wagenmaker and Pacchiano, 2023) by a factor of at least H3
3UpperBound LowerBound
? ř ? ř
Offline(Error) d¨ H h“1E π˚}ϕps h,a hq} Σ˚´1 d¨ H h“1E π˚}ϕps h,a hq} Σ˚´1
off,h off,h
a a
ď C˚d2H4{N off (Xiongetal.,2023) ě C˚d2H2{N off (Xiongetal.,2023)
? ?
Online(Regret) d2H3T (Heetal.,2023) d2H3T (Zhouetal.,2021)
Result
a
Hybrid d2H7{N(WagenmakerandPacchiano,2023)
a a
(Online-to-offlineError) c offpX offqdH3mintc offpX offq,Hu{N off` d ondH3mintd on,Hu{N on (Alg. 1)
?
Hybrid C˚ ad2H6N
on
(Songetal.,2023;Nakamotoetal.,2023)
(Offline-to-onlineRegret) a pC˚`c onpXqqd3H6 ?N on (Amortilaetal.,2024)
c offpX offqdH5N o2 n{N off` d ondH5N on (TanandXu,2024)
a a
c offpX offq2dH3N o2 n{N off` d ondH3N on (Alg. 2)
Table 2: Comparisons of our results to the best upper and lower bounds available, and existing results
for hybrid RL, in linear MDPs. The inequalities in the offline row hold when the behavior policy satisfies
C˚-single policy concentrability. Often, offline data is cheaper or easier to obtain. When this happens,
N "N , and the second term (depending on N “T) dominates.
off on on
(with H the time horizon). Additionally, this result demonstrates that hybrid RL can perform no
worse than the offline-only minimax-optimal error bound from Xiong et al. (2023), with the potential
of significant gains if one has access to a large number of online samples. This is also the first work to
explore the online-to-offline approach in linear MDPs.
• Inaddition,weproposeanoffline-to-onlinemethodcalledHybrid Regression for Upper-Confidence Re-
inforcement Learning (HYRULE) inAlgorithm2, whereonewarm-startsanonlineRLalgorithmwith
parameters estimated from offline data. In addition to improving the ambient dimension dependence,
this algorithm enjoys a regret (or sample-complexity) bound that is no worse than the online-only
minimax optimal bound, with the potential of significant gains if the offline dataset is of high quality
(Zhou et al., 2021; He et al., 2023; Hu et al., 2023; Agarwal et al., 2022). Our result demonstrates the
provable benefits of hybrid RL in scenarios where offline samples are much cheaper or much easier to
acquire.
Tothebestofourknowledge,wearethefirsttoshowimprovementsovertheaforementionedlowerbounds
of hybrid RL algorithms (in the same vein as Li et al. (2023b)) in the presence of function approximation,
without any explicit requirements on the quality of the behavior policy, and with both the offline-to-online
and online-to-offline approaches. Our results are also, at the point of writing, the best bounds available in
the literature for hybrid RL in linear MDPs (see Table 2).
Technical contributions. In this work, we build on recent advancements in offline and online RL with
intuitive modifications, demonstrating that it is possible to achieve state-of-the-art sample complexity in
a hybrid setting for linear MDPs. At a high level, our improvements in sample complexity are achieved
by decomposing the error of interest into offline and online partitions, and optimizing them respectively,
following the same idea in Tan and Xu (2024). Below, we summarize our specific technical contributions.
1. We sharpen the dimensional dependence from d to d and c pX q via projections onto those parti-
on off off
tions. The former is accomplished in Algorithm 1 by Kiefer-Wolfowitz in Lemma 1, and in Algorithm
2 by proving a sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in
Lemma 14 to reduce the dimensional dependence in the summation of bonuses, which helps achieve
the desired result.
42. We maintain a H3 dependence for the error or regret for both algorithms, which is non-trivial. This
is accomplished in Algorithm 1 and for the offline partition in Algorithm 2 by combining the total
variance lemma with a novel truncation argument that rules out “bad” trajectories in Lemma 17.
2 Preliminaries
2.1 Basics of Markov decision processes
` ˘
Consider an episodic MDP denoted by a tuple M“ S,A,H,pP qH ,pr qH , where S is the state space,
h h“1 h h“1
A the action space, H the horizon, pP qH the collection of transition probability kernels where each P :
h h“1 h
SˆAÑ∆pSq,andpr qH thecollectionofrewardfunctionswhereeachr :SˆAÑr0,1s. Weuse∆p¨qto
h h“1 h
denotethecollectionofdistributionsoveraset,andwriterHs“1,...,H. AteachhPrHs,anagentobserves
the current state s P S, takes an action a P A according to a randomized decision rule π : S Ñ ∆pAq,
h h h
and observes the reward r . The next state s then evolves according to s „ P p¨ | s ,a q. A policy
h h`1 h`1 h h h
is given by the collection of policies at each horizon hPrHs, π “tπ u , and we write Π for the set of
h 1ďhďH
all policies.
We define the value function and Q-functions associated with each policy π PΠ as follows:
ř
for every ps,hqPSˆrHs: Vπpsq:“E r H r |s “ss, (1)
h π hř1“h h1 h
and for every ps,a,hqPSˆAˆrHs: Qπps,aq:“E r H r |s “s,a “as. (2)
h π h1“h h1 h h
π˚ “tπ˚uH istheoptimalpolicyattainingthehighestvalueandQ-functions,andwewriteV˚ “tV˚uH
h“1 h h“1
and Q˚ “tQ˚uH for the optimal value and Q-functions.
h h“1
We consider the setting of hybrid RL, where an agent has access to two sources of data:
• N independent episodes of length H collected by a behavior policy π where the n-th sample trajec-
off b
tory is a sequence of data pspnq,apnq,rpnq,...,spnq,apnq,rpnq,spnq q;
1 1 1 H H H H`1
• N sequential episodes of online data, where at each episode n“1,...,N , the algorithm has knowl-
on on
edge of the N offline episodes and the previous online episodes 1,...,n´1.
off
The quality of the behavior policy π is measured by the all-policy and single-policy concentrability coeffi-
b
cients proposed by Xie et al. (2023); Zhan et al. (2022):
Definition 1 (Occupancy Measure). For a policy π “ tπ uH , its occupancy measure dπ “ tdπuH
h h“1 h h“1
corresponds to the collection of distributions over states and actions induced by running π within M, where
for some initial distribution ρ and s „ρ, we have
1
dπps,aq:“Pps “s,a “a|s „ρ,πq. (3)
h h h 1
Definition 2 (Concentrability Coefficient). For a policy π, its all-policy and single-policy concentrability
coefficients with regard to the occupancy measure of a behavior policy π are
b
dπps,aq d˚ps,aq
C :“supsup h and C˚ :“ sup h , (4)
all
π
h,s,aµ hps,aq h,s,aµ hps,aq
where we write µ“tµ uH for the occupancy measure of π .
h h“1 b
Policy learning and regret minimization. The recurring goal of hybrid RL is to either learn an ϵ-
optimal policy πp such that V˚´Vπp ď ϵ with high probability, or to minimize the regret. Here, the regret
of an online algorithm, i.e. a map from the history of all previous observations H to the set of all policies
ř ř
Π, L:HÑΠ is defined as Reg pTq“Er T pV˚psptqq´ H rptqqs. Throughout the paper, we shall write
L t“1 1 1 h“1 h
T “ N interchangeably whenever we refer to the number of episodes taken by a regret-minimizing online
on
RL algorithm.
52.2 Linear MDPs
Throughout this paper, we consider the setting of linear MDPs first proposed by Yang and Wang (2019);
Jin et al. (2019), and further studied in Zanette et al. (2021); Xiong et al. (2023); He et al. (2023); Hu et al.
(2023); Wagenmaker and Jamieson (2023); Wagenmaker and Pacchiano (2023). Informally, this is the set
of MDPs where the transition probabilities and rewards are linearly parametrizable as functions of known
features.
Assumption 1 (Linear MDP, Jin et al. (2019)). A tuple pS,A,H,P,rq defines a linear MDP with a
(known´) feature map¯ϕ : S ˆ A Ñ Rd, if for any h P rHs, there exist d unknown signed measures
µ “ µp1q,¨¨¨ ,µpdq over S and an unknown vector θ P Rd, such that for any px,aq P S ˆ A, we
h h h h
have P p¨|x,aq“ xϕpx,aq,µ p¨qy,r px,aq“xϕpx,aq,θ y. Assume }ϕpx,aq}ď1 for all px,aqPSˆA, and
h ? h h h
maxt}µ pSq},}θ }uď d for all hPrHs.
h h
This setting allows for sample-efficient RL due to a collection of reasons. Firstly, linear MDPs are
Bellman complete (Jin et al., 2021a), a common assumption made to ensure sample-efficient RL in the
literature (Munos and Szepesv´ari, 2008; Duan and Wang, 2020; Fan et al., 2020). Secondly, the value and
Q-functions are linearly parametrizable in the features, allowing one to learn them via ridge regression.
This allows for sample-efficient, and even minimax-optimal, online (He et al., 2023; Hu et al., 2023) and
offline (Yin et al., 2022; Xiong et al., 2023) reinforcement learning in linear MDPs, despite the requirement
of function approximation. However, existing guarantees for hybrid RL in linear MDPs (Wagenmaker and
Pacchiano, 2023) are loose (Li et al., 2023b), inspiring the focus of our work.
Further notation. Write ϕ “ ϕpspnq,apnqq as shorthand for the observed feature vector at episode n
ř n,h h h ř
and horizon h. Let Λ “ N ϕ ϕJ `λI and Λ “ Noff ϕ ϕJ `λI be the covariance matrices
h n“1 n,h n,h off,h n“1 n,h n,h
of the entire dataset and the offline dataset respectively, and Ωřthe set of all co“variates.‰ We consider two
kinds of variance-weighted covariance matrices, namely Σ˚ “ N ϕ ϕJ { V V˚ psτ,aτq`λI and
ř “ ‰ n,h n“␣1 “n,h n,h ‰h h`1( h h
Σ “ N σ¯´2ϕ ϕJ ` λI, where V V˚ psτ,aτq “ max 1, Var V˚ ps,aq is the truncated
n,h n“1 n,h n,h n,h h h`1 h h h h`1
variance of the optimal value function (where s,a are random variables) and σ¯´2 is the variance estimator
n,h
from He et al. (2023).
2.3 Exploring the state-action space
The goal of this paper is to develop efficient hybrid RL algorithms for linear MDPs that do not rely on
the (full) single-policy concentrability condition, which entails that the behavior policy covers every state-
action pair that π˚ visits. A natural idea, from Li et al. (2023b); Tan and Xu (2024), is to partition this
space into a component that is well-covered by the behavior policy, which we call X , and a component
off
requiring further exploration, which we call X . Based on this partition, similarly to Tan and Xu (2024),
on
the estimation error or regret of a hybrid RL algorithm can be analyzed on each component separately. We
define X YX “rHsˆSˆA, with their images under the feature map Φ “SpanpϕpX qq ĎRd
on off off off,h hPrHs
andΦ “SpanpϕpX qq ĎRd beingsubspacesofdimensiond andd respectively. WriteP ,P
on on,h hPrHs off on off on
fortheorthogonalprojectionoperatorsontothesesubspacesrespectively. Letλ pMqdenotethek-thlargest
k
eigenvalueofasymmetricmatrixM.Weborrowthedefinitionofthepartialofflineall-policyconcentrability
coefficient, L
c pX q:“max 1 λ pE rpP ϕ qpP ϕ qJsq, (5)
off off
h
doff µh off h off h
from Tan and Xu (2024). This corresponds to the inverse of the d -th largest eigenvalue of the covariance
off
matrix of the projected feature maps. Similarly, the partial all-policy analogue of the coverability coefficient
from Xie et al. (2022a) is
L
c onpX onq:“in πfm hax 1 λ donpE
dπ
hrpP onϕ hqpP onϕ hqJsq. (6)
As we shall see, these quantities characterize the estimation error of our proposed algorithms.
6Algorithm 1 Reward-Agnostic Pessimistic PAC Exploration-initialized Learning (RAPPEL)
1: Input: Offline dataset D off, samples sizes N on, N off, feature maps ϕ h, tolerance parameter for reward-
agnostic exploration τ.
?
2: Initialize: D hp0q ÐH @hPrHs, λ“1{H2, β 2 “O˜ p dq.
3: for horizon h“1,...,H do
4: Run an exploration algorithm (OPTCOV, Wagenmaker and Jamieson (2023)) to collect covariates
Λ such that
h
maxϕJpΛ `λI`Λ q´1ϕ ďτ.
h h off,h h
ϕhPΦ
5: end for
6: Output: πp fromrunningapessimisticofflineRLalgorithm(LinPEVI-ADV+, Xiongetal.(2023))with
hyperparameters λ,β on the combined dataset D YtDpNonqu .
2 off h hPrHs
3 Algorithms and main results
We provide two algorithms with improved statistical guarantees to tackle the unsolved (Table 2) problem of
achieving sharp guarantees with hybrid RL in linear MDPs, with different approaches:
1. Performing reward-agnostic online exploration (Wagenmaker and Pacchiano, 2023) to augment the
offline data, then invoking offline RL (Xiong et al., 2023) to learn an ϵ-optimal policy on the combined
dataset, in the same vein of Li et al. (2023b). The details can be found in Algorithm 1.
2. Warm-starting an online RL algorithm (He et al., 2023) with parameters estimated from an offline
dataset to minimize regret, as in Song et al. (2023). We include the details in Algorithm 2.
3.1 Offline RL after online exploration
Our algorithm for the online-to-offline approach, Algorithm 1, proceeds as follows. Given access to the
offline dataset D , we collect online samples informed by the degree of coverage (or lack thereof) of the
off
offline dataset with a reward-agnostic online exploration algorithm called OPTCOV that was first proposed
in Wagenmaker and Jamieson (2023). OPTCOV attempts to collect feature vectors such that the minimum
eigenvalue of the feature covariance matrix, λ pΛ q, is bounded below by a tolerance parameter 1{τ, and
min h
terminates after this is accomplished. We then learn a policy from the combined dataset using a nearly
minimax-optimal pessimistic offline RL algorithm from Xiong et al. (2023) called LinPEVI-ADV+.
To employ OPTCOV, one requires a similar assumption to the full-rank covariate assumption from
Wagenmaker and Pacchiano (2023) that ensures that the MDP is ”explorable” enough, but modify it to
considerthestate-actionspacesplittingframework. TheassumptionbelowisonlyimposedforAlgorithm1.
Assumption 2 (Full Rank Projected Covariates). For any partition X YX “rHsˆSˆA,
on off
c onpX onqă8, or equivalently that in πfm hinλ donpE
dπ
hrpP onϕ hqpP onϕ hqJsq“λ˚
don
ą0.
Informally,thisstatesthatthelowest(best)achievablepartialall-policyconcentrabilitycoefficientonany
onlinepartitionmustbebounded. Thatis, foranypartition, thereexistssome“optimalexplorationpolicy”
that ensures that the projected covariates onto the online partition have the same rank as the dimension of
theonlinepartitionateverytimestep. Itessentiallyrequiresthatthereissomepolicythatcollectscovariates
that span the entire feature space. In practice, this is achievable for any linear MDP via a transformation of
the features that amounts to a projection onto the eigenspace corresponding to the nonzero singular values.
For example, this is performed for the numerical simulations in Section 4 – as in Tan and Xu (2024), the
feature vectors are generated by projecting the 640-dimensional one-hot state-action encoding onto a 60-
dimensional subspace spanned by the top 60 eigenvectors of the covariance matrix of the offline dataset. We
can then establish the following:
Lemma 1 (Partial Coverability Is Bounded In Linear MDPs). For any partition X ,X , it satisfies that
off on
c pX qďd . Also, there exists at least one partition such that c pX q“Opdq.
on on on off off
7TheproofofthislemmaisdeferredtoAppendixD.Thisresultallowsustoboundtheerrorontheoffline
and online partitions by the dimensionality of the partitions, instead of the coverability coefficient. More
specifically, define α
off
:“ N Noff, α
on
:“ N Non, and the minimal online samples for exploration
` ˘
N˚pτq:“minN s.t. inf maxϕJ NpΛ`λ¯Iq`Λ ´1 ϕďτ.
off
N ΛPΩ ϕPΦ
We now have, with full proof in Appendix B and proof sketch at the end of the subsection, the following:
Theorem 1 (Error Bound for RAPPEL, Algorithm 1). For every δ P p0,1q and any partition X ,X ,
off on
when choosing τ ďO˜ pmaxtd {N ,c pX q{N uq, RAPPEL achieves with probability at least 1´δ:
on on off off off
? ÿH ? ÿH
V˚psq´Vπp psqÀ d E ||ϕps ,a q|| ď d E ||ϕps ,a q|| , (7)
1 1 π˚ h h pΣ˚ `Σ˚ q´1 π˚ h h Σ˚´1
off,h on,h off,h
h#“1d d d h“1 d +
c pX qdH4 d dH4 c pX q2dH3 d2 dH3
V˚psq´Vπp psqÀmin off off ` on , off off ` on , (8)
1 1 N N N α N α
off on off off on on
␣ (
given N ěmax α4 d´4,α4 c pX q´4 maxtN˚pτq,polypd,H,c pX q,log1{δqu.
on on off off off off off
This result, when applied to tabular MDPs with finite states and actions, leads to the following:
Corollary 1. In tabular MDPs, for every δ Pp0,1q, it satisfies that with probability at least 1´δ,
˜d c ¸
a
c pX q d
V‹psq´Vπp psqÀ H3|S|2|A| off off ` on . (9)
1 1 N N
off on
In words, Theorem 1 guarantees that with a burn-in cost polynomial in dimension d and time horizon H
and no smaller than N˚ (the minimal online samples for any algorithm to achieve our choice of OPTCOV
tolerance), we learn an ϵ-optimal policy in at most
c pX qdH3mintc pX q,Hu d dH3mintd ,Hu
off off off off on on
`
ϵ2 ϵ2
trajectories. N˚, from Wagenmaker and Pacchiano (2023), is essentially unavoidable in reward-agnostic
exploration for linear MDPs.
a To compare with prior literature, our result leads to a better worst-case guarantee than the error bound
d2H7{N attainedinWagenmakerandPacchiano(2023)(byatleastafactorofH3{2),theonlyotherwork
onhybridRLinlinearMDPsthusfar. Whileweemploythesameonlineexplorationprocedure,wecombine
our exploration phase with an offline learning algorithm LinPEVI-ADV+ from Xiong et al. (2023) and con-
ductacarefulanalysis. Whencomparingwiththeoffli ?ne-řonlyandonline-onlysettings, Theorem1improves
upontheoffline-onlyminimax-optimalerrorboundof d H E ||ϕps ,a q|| fromXiongetal.(2023)
h“1 π˚ h h Σa˚´1
off,h
as a consequence of Σ˚ `Σ˚ ľΣ˚ ; the best offline-only error bound is d2H4{N obtained under
off,h on,h off,h off
the “well-covered” assumption (Corollary 4.6, Jin et al. (2021b)) that λ pΛ q ě Ωp1{dq, Theorem 1
min h,off
enjoys better dimension and horizon dependence as there is always a partition such that d ,c pX q ď d
on off off
and d H3mintd ,Huďd2H4.
on on
We remark that the literature has experienced considerable difficulty in sharpening the horizon depen-
dencetoH3 inofflineRLforlinearMDPs. WhileYinetal.(2022)andXiongetal.(2023)provideminimax-
optimal algorithms for offline RL in linear MDPs, both only manage to achieve a H3 horizon dependence in
the special case of tabular MDPs, even under the “well-covered” assumption mentioned earlier. We provide
the same result in Corollary 1 with proof deferred to Appendix C, but we note that encouragingly, hybrid
RLletsusbypassthe“well-covered”assumption. InAppendixBandG,weuseanoveltruncationargument
and the total variance lemma (Lemama C.5 of Jin et al. (a2018)) to improve the dependence on H, but our
result still falls slightly short of a c pX qdH3{N ` d dH3{N bound.
off off off on on
8Computational efficiency. In terms of computational efficiency, Algorithm 1 inherits the computational
costs of the previous proposed algorithms OPTCOV and LinPEVI-ADV+ (Wagenmaker and Jamieson
(2023); Xiong et al. (2023). OPTCOV runs in polynomial time polypd,H,c pX q,log1{δq, and LinPEVI-
on on
ADV+ runs in O˜ pd3HN|A|q time when the action space is discrete. Algorithm 1 therefore remains compu-
tationally efficient in this case.
Requirement of choosing d . There is the caveat that we require the user to choose the tolerance for
on
OPTCOV. In practice, one can achieve this by performing SVD on the offline dataset and looking at the
plot of eigenvalues. One can also choose a tolerance of Opd{mintN ,N uq, but this would not achieve the
off on
reduction in the dependence on dimension from d2 to c pX qd,d d.
off off on
Practical benefits of the online-to-offline approach. Algorithm1outputsafixedpolicythatsatisfies
a PAC bound. This enables learned policies to be deployed in critical real-world applications, such as in
medicine or defense, where randomized policies (as a regret-minimizing online algorithm would provide) are
often unacceptable.
Reward-agnostic hybrid RL. Secondly, the use of reward-agnostic online exploration in Algorithm 1
enables one to use the combined dataset D to learn policies for different reward functions offline. As the
online exploration is not influenced by any single reward function, the resulting dataset collected satisfies
good coverage for any possible reward function even if it is revealed only after exploration, enabling one to
use a single dataset to achieve success on many different tasks. This therefore also serves as an algorithm
for the related setting of reward-agnostic hybrid RL, where the reward function is unknown during online
exploration and only revealed to the agent after it.
Proof sketch. The relation (7) in Theorem 1 follows from invoking Theorem 2 from Xiong et al. (2023)
?
with N ąΩpd2H6q,λ“1{H2,β “Op dq. To establish relation (8), the idea is to first bound the quantity
1
of interest as
? ÿ b ? ÿ b
V˚psq´Vπp psqď d H max ϕJΣ˚´1ϕ ` d H max ϕJΣ˚´1ϕ .
1 1 h“1ϕhPΦon h h h h“1ϕhPΦoff h h h
As Σ˚´1 ĺ H2Λ´1 (see Xiong et al. (2023)), it therefore boils down to controlling max ϕJΛ´1ϕ .
h h ϕhPΦ h h h
Towardsthis,first,wemaketheobservationthatLemma1suggeststhatc pX qďd . IfwerunOPTCOV
on on on
with tolerance O˜ pmaxtd {N ,c pX q{N uq on partitions where the above hold, in Lemma 5, we prove
on on off off off
that max ϕJΛ´1ϕ Àmaxtc pX q{N ,d {N u. This yields the c pX qdH4,d dH4 result.
ϕhPΦ h h h off off off on on off off on
To tighten the horizon dependence to H3, we employ an useful truncation argument. More specifically,
from the total variance lemma (Lemma C.5 of Jin et al. (2018)), the average variance V V˚ is asymptoti-
“h h`1 ‰
cally on the order of H. We therefore define the sets of trajectories E pδ q “ tτ P D : V V˚ psτ,aτq ě
h h h h`1 h h
H1`δhu. The cardinality of each set can be bounded by |E hpδ hq| À NH1´δh, and so truncating at the level
where NH1´δh « minp cofN fpo Xff offq,N doo nnq leads to min ϕhPΦϕJ hΣ‹ hϕ h Á N1 H2 minp cofN fpo Xff offq,N doo nnq2. Putting things
together yields the last c pX q2dH3,d2 dH3 result needed, and the theorem then follows.
off off on
3.2 Online regret minimization
Thus far, we described an online-to-offline strategy which collects online samples to augment the offline
dataset. However, in certain critical cases, such as with a doctor treating patients, performance-agnostic
online exploration is untenable. One may wish to minimize the regret of the online actions taken while
learning a nearly optimal policy. In light of this, we explore another approach inspired by the work of
Song et al. (2023); Tan and Xu (2024) – that of warm-starting an online RL algorithm with parameters
estimated from an offline dataset. We describe this algorithm as Hybrid Regression for Upper-Confidence
Reinforcement Learning (HYRULE) inAlgorithm2. WedemonstratethathybridRLenablesprovablegains
over minimax-optimal online-only regret bounds in the offline-to-online case as well.
In order to warm-start an online RL algorithm with offline dataset, we modify LSVI-UCB++ from He
etal.(2023)totakeinanofflinedatasetD byestimatingitsparametersfromD withthesameformulas
off off
9Algorithm 2 Hybrid Regression for Upper-Confidence Reinforcement Learning (HYRULE)
1: Input: Offline dataset D off, samples sizes N on, N off, feature maps ϕ h. Regularization parameter λą0,
confidence radii β,β¯,β˜, t “0.
last
2: Initialize: F řor h P rHs, estimate wp 1,h,wq 1,h,Q 1,h,Qq 1,h,σ 1,h,σ¯ 1,h from D off, and assign Σ 0,h “ Σ 1,h “
Σ `λI“ Noff σ¯´2ϕ ϕJ `λI.
off n“1 n,h n,h n,h
3: for episodes t“1,...,T do
4: Update optimistic and pessimistic weights wp t,h,wq t,h for all h.
5: if there exists a stage h1 PrHs such that detpΣ t,h1qě2detpΣ qtlast,h1q then
6: Update optimistic and pessimistic Q-functions Q t,hps,aq,Q t,hps,aq, set t last “t.
7: end if
8: for horizon h“1,...,H do
9: Play action a hptq Ðargmax aQ t,h ?psp htq,aq, receive reward r hptq, next state sp ht `q 1
10: Estimate σ t,h, σ¯ t,h Ðmaxtσ t,h, H,2d3H2||ϕpsp htq,ap htqq||1 Σ{ ´2 1u1, update Σ t`1,h.
t,h
11: end for
12: end for
13: Output: Greedy policy πp “πQT,h, UnifpπQ1,h,...,πQT,hq for PAC guarantee.
it would use as if it had experienced the N offline episodes itself. As Tan and Xu (2024) suggest, this can
off
be understood as including the offline episodes in the “experience replay buffer” that the algorithm uses to
learn parameters. The full version can be found in Appendix E as Algorithm 4. Doing so allows us prove a
regret bound depending on the partial all-policy concentrability coefficient.
Below we state our theoretical guarantees for this algorithm. The proof of this result is deferred to
Appendix E, and a brief proof sketch is provided at the end of this subsection.
Theorem 2 (Regret Bound for HYRULE, Algorithm 2). Given any δ Pp0,1q, for every partition X ,X ,
off on
if N ,N “Ω˜ pd13H14q, the regret of HYRULE is bounded by
on off
a a
RegpN qÀ inf c pX q2dH3N2 {N ` d dH3N ,
on off off on off on on
Xoff,Xon
with probability at least 1´δ.
Corollary 2. Bytheregret-to-PACconversion,foranyδ Pp0,1q,withprobability1´δ,Algorithm2achieves
a sub-optimality gap of
a a
V˚psq´Vπp psqÀ inf c pX q2dH3{N ` d dH3{N .
1 1 off off off on on
Xoff,Xon
To understand this result, we first note that bounding the regret over all possible partitions yields an
?
improvement over the d2H3N regret bound originally obtained by He et al. (2023), as we can simply
on
take X “X1 “X to recover this result. In the scenario where offline samples are largely available (where
on on
N " N ), it is possible to achieve significant improvements over online-only learning. Furthermore, in
off on
view of Lemma 1, there always exists a partition such that c pX q,d ď d. This result therefore yields
off off on
provableimprovementsovertheminimax-optimalonlineregretboundinlinearMDPs(Zhouetal.,2021;He
et al., 2023; Hu et al., 2023; Agarwal et al., 2022).
Additionally, Theorem 2 shows that Algorithm 2 attains the best known regret bound in hybrid RL for
linear MDPs, asawe illustrate in Table 2. The current best known result is that of Tan and Xu (2024), with
?
a dependence of c pX qdH5N2 {N ` d dH5N . Notably, we achieve the same a reduction in the
off off on off on on
dimension dependence on the online partition from d2 to d d that Tan and Xu (2024) do by proving a
on
sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in Lemma 14 to reduce
the dimensional dependence in the summation of bonuses. Song et al. (2023) and Amortila et al. (2024), on
?
1Heetal.(2023)writeσ¯
t,h
Ðmaxtσ t,h,H,...uinsteadof H. Webelievet ?hatthisisatypointheirpaper,giventhatin
t ah rre ap yr oo fof eqo uf aL te iom nm sa rigB h. t1, aft th ee ry es qt ua at te ior nigh Dt .2a 2ft ,e pr ae rq tiu ca ut li ao rn lyD› › ›. σ¯7 ´t 1h ϕa `t s0 i,ď aiσ¯ ˘i´ › › ›h1 ďď › ›1 ϕ{ ` sH i,. aM i˘o › ›re {o ?ve Hr, ,in ont lh ye hp or ldoo sf tro uf eL ie fm thm isa iB s. ?5 Hth .e
i,h h h 2 h h 2
10a
?
the other hand, have bounds on the order of C˚ d2H6N and pC˚`c pXqqd3H6N respectively. We
on on on
produce a better bound than Tan and Xu (2024); Song et al. (2023); Amortila et al. (2024) by at least a
factor of H2 by combining the total variance lemma and a novel truncation argument that rules out “bad”
trajectories in Lemma 17, which allows us to maintain a desirable H3 dependence on both partitions.
Computational efficiency. In terms of computational efficiency, when t`he action sp˘ace is finite and of
r
cardinality |A| the computational complexity of Algorithm 2 is of order O d4H3N|A| , as outlined in He
et al. (2023). Algorithm 2 is therefore computationally efficient and runs in polynomial time in this case.
When the action space is continuous, one may need to solve an optimization problem over the continuous
action space, making the computational complexity highly problem-dependent.
Algorithm 2 is unaware of the partition. Unlike Algorithm 1, Algorithm 2 is fully unaware of the
choice of partition, and there is therefore no need to estimate d or any relevant analogue to the choice
on
of tolerance for OPTCOV. The regret bound therefore automatically adapts to the best possible partition,
even though Algorithm 2 is unaware of it.
Practical benefits of the offline-to-online approach. While Algorithm 2 only satisfies a PAC bound
with a randomized policy, it minimizes the regret of the actions it takes. This enables the algorithm to
be deployed in situations where its performance during online exploration is of critical importance, e.g. in
applications like mobile health (Nahum-Shani et al., 2017).
Technical challenges. Although Algorithm 2 is a straightforward generalization of LSVI-UCB++ in He
et al. (2023), with Σ initialized with the offline dataset, we had to decompose the regret into the regret on
0
the offline and online partitions to achieve the regret guarantee in Theorem 2. In the process, we faced the
following challenges:
• Boundingtheregretontheofflinepartitionwaschallenging,aswewerenotabletoutilizethetechnique
that was used in He et. al (2023). Instead, we bounded the regret with the maximum eigenvalue of
Σ´1 . To maintain a H3 dependence on the offline partition, we had to use a truncation argument
off,h
in Lemma 17 that we also deployed in proving the regret guarantee of Algorithm 1.
• Bounding the regret on the online partition allowed us to use an analysis that was close to that of
He et al. (2023). However, directly following the argument of He et al. (2023) would have left us
with a d2H3 dependence in Theorem 2. To reduce the dimensional dependence to d d, we prove a
on
sharper variant of Lemma B.1 from Zhou and Gu (2022) in Lemma 18, using this in Lemma 14 to
reduce the dimensional dependence in the summation of bonuses enough to achieve the desired result.
Waithout the above two techn ?iques, one could have used a simpler analysis to achieve a far looser
c pX q2d6H8N2 {N ` d2H3 regret bound by using the maximum magnitude of the variance
off off on off
weightsfortheofflinepartitionandtheanalysisfromHeetal.(2023)verbatimfortheonlinepartition,
but this would not have yielded the same improvement.
We accordingly provide a proof sketch below.
Proof sketch. We first adopt the regret decomposition as in He et al. (2023) and bound
? ř ř
RegpTqÀ H3T ` β}Σ´1{2ϕ psptq,aptqq1 } ` β}Σ´1{2ϕ psptq,aptqq1 } .
h,t t,h h h h Xoff 2 h,t t,h h h h Xon 2
It then boils down to controlling the second and the third termbseparately. We prove in Lemma 12 that the
ř
sumofbonusesontheofflinepartitioncanbeboundedby h dN onNN oo fn
f
max řϕhPΦoff ϕJ hΣ¯´ off1 ,bhϕ h.Tofurther
controlthisterm,wethenshowinLemma13that,foranypartitionX ,X , max ϕJΣ¯´1 ϕ À
off on h ϕhPΦoff h off,h h
c pX q2H3. Putting things together, the second term can be controlled as
off off
ř a
β }Σ´1{2ϕ psptq,aptqq1 } À c pX q2dH3N2 {N .
h,t t,h h h h Xoff 2 off off on off
11With respect to the third term, Lemma 14 (a sharpened version of Lemma E.1 in He et al. (2023)),
combined with the Cauchy-Schwartz inequality, yields
b
ř ř
β }Σ´1{2ϕ psptq,aptqq1 } Àd4H8`βd7H5`β d HT `d H σ2 .
h,t t,h h h h Xon 2 on on h,t t,h
ř ` ˘
r
Lastly,thetotalvariancelemma(AppendixB,Heetal.(2023))furthersuggests σ2 ďO H2T `d10.5H16 .
h,t t,h
Taking everything collectively establishes the desired result.
4 Numerical experiments
TodemonstratethebenefitsofhybridRLintheoffline-to-onlineandonline-to-offlinesettings,weimplement
Algorithms1and2onthescaled-downTetrisenvironment(asinTanandXu(2024)). Thisisa6-piecewide
Tetris board with pieces no larger than 2ˆ2, where the action space consists of four actions, differentiated
by the degree of rotation in 90 degree intervals and the reward is given by penalizing any increases in the
height of the stack from a tolerance of 2 blocks. The offline dataset consists of 200 trajectories generated
from a uniform behavior policy. As in Tan and Xu (2024), the feature vectors are generated by projecting
the 640-dimensional one-hot state-action encoding onto a 60-dimensional subspace spanned by the top 60
eigenvectors of the covariance matrix of the offline dataset.2
Figure1: CoverageachievedbyOPTCOVwith200trajectoriesofofflinedatacollectedunderauniformand
an adversarial behavior policy, and with no offline data. Results averaged over 30 trials, with the shaded
area depicting 1.96-standard errors. Lower is better.
Figure 1 depicts the coverage (defined by 1{λ pΛq,1{λ pΛ q,1{λ pΛ q) achieved by the reward-
min doff off don on
agnostic exploration algorithm, OPTCOV, when initialized respectively with 200 trajectories from (1) a
uniform behavioral policy, (2) an adversarial behavior policy obtained by the negative of the weights of
a fully-trained agent under Algorithm 1, and (3) no offline trajectories at all for fully online learning. It
shows that although hybrid RL with the uniform behavior policy achieves the best coverage throughout as
expected, even hybrid RL with adversarially collected offline data achieves better coverage than online-only
exploration. This demonstrates the potential of hybrid RL as a tool for taking advantage of poor quality
offline data.
2For simplicity in implementation, we implement LSVI-UCB++ (He et al., 2023) for Algorithm 2 as-is, while substituting
LSVI-UCB(Jinetal.,2019)forFORCE(Wagenmakeretal.,2022)withinOPTCOVandLinPEVI-ADVforLinPEVI-ADV+
(Xiongetal.,2023).
12Figure 2: Value of policies learned by applying LinPEVI-ADV to the hybrid, offline, and online datasets,
withanadversarialbehaviorpolicy. Therewardisnegativeasitisthenegativeoftheexcessheight. Results
over 30 trials. Higher is better.
In Figure 2, one can observe that hybrid RL demonstrates strong benefits in the online-to-offline setting
when the behavior policy is of poor quality. When applying LinPEVI-ADV to the hybrid dataset of 200
trajectories and 100 online trajectories, 300 trajectories of adversarially collected offline data, and 300 tra-
jectories of online data under reward-agnostic exploration, we see that the hybrid dataset is most conducive
for learning. Additionally, without a warm-start from offline data, online-only reward-agnostic exploration
performs worse than the adversarially collected offline data due to significant burn-in costs. Hybrid RL
therefore, in this instance, performs better than both offline-only and online-only learning alone.
Figure 3: Comparison of LSVI-UCB++ and Algorithm 2. Results averaged over 10 trials, with 1-standard
deviation error bars over 10 trials.
In Figure 3, we compare the performances of LSVI-UCB++ and Algorithm 2. It can be seen from the
figurethatinitializingaregret-minimizingonlinealgorithm(LSVI-UCB++,(Heetal.,2023))withanoffline
datasetasinAlgorithm2yieldslowerregretthanthesamealgorithmwithoutanofflinedataset. Thisshows
that even a nearly minimax-optimal online learning algorithm can stand to benefit from being initialized
with offline data.
5 Discussion, limitations and future work
In this paper, we develop two hybrid RL algorithms for linear MDPs with desirable statistical guarantees.
The first performs reward-agnostic online exploration to fill in gaps in the offline dataset before using offline
13RL to learn an ϵ-optimal policy from the combined dataset, while the second warm-starts online RL with
parametersestimatedfromanofflinedataset. Bothalgorithmsdemonstrateprovablegainsovertheminimax-
optimal rates in offline or online-only reinforcement learning, and provide the sharpest worst-case bounds
for hybrid RL in linear MDPs thus far.
Throughout this paper, we have used both optimism and pessimism in our algorithm design. Other
work in hybrid RL (Song et al., 2023; Nakamoto et al., 2023; Li et al., 2023b; Tan and Xu, 2024; Amortila
et al., 2024; Wagenmaker and Pacchiano, 2023) uses optimism, pessimism, or sometimes even neither. We
conjecture that optimism is still helpful in aiding online exploration within hybrid RL and that pessimism
helps in hybrid RL when learning from a combined dataset. However, determining if or when optimism or
pessimism is beneficial in hybrid RL remains an open question.
Achieving a H3 horizon dependence in offline RL for linear MDPs has proven challenging. Even under
strong coverage assumptions, Yin et al. (2022a) and Xiong et al. (2023) only manage to achieve a H3 horizon
dependence for tabular MDPs. Obtaining a d2H3{N bound is an open problem.
Furthermore, while Algorithm 1 improves upon the offline-only error bound in Xiong et al. (2023) and
Algorithm 2 improves upon the online-only regret bound in He et al. (2023); Zhou et al. (2021), we still
desire a single algorithm that improves upon both the best possible offline-only and online-only rates at
once. Additionally, theburn-incostsforAlgorithms1and2arenontrivial. Theformerisinheritedfromthe
OPTCOValgorithmofWagenmakerandJamieson(2023), whilethelatterisinheritedfromHeetal.(2023)
andthetruncationargument. Improvingtheformerbydevisingnewreward-agnosticexplorationalgorithms
for linear MDPs, perhaps in the vein of Li et al. (2023a), would be welcome.
While we tackle the setting of linear MDPs, it remains a first step towards showing that hybrid RL
breaks minimax-optimal barriers in the presence of function approximation. Further work in this vein on
other types of function approximation would be an interesting contribution to the literature.
Acknowledgements
Y.WeiissupportedinpartbytheNSFgrantsCAREERawardDMS-2143215,CCF-2106778,CCF-2418156,
and the Google Research Scholar Award.
References
Abbasi-yadkori, Y., P´al, D., and Szepesv´ari, C. (2011). Improved algorithms for linear stochastic bandits.
In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K., editors, Advances in Neural
Information Processing Systems, volume 24. Curran Associates, Inc.
Agarwal, A., Jin, Y., and Zhang, T. (2022). Voql: Towards optimal regret in model-free rl with nonlinear
function approximation.
Amortila, P., Foster, D. J., Jiang, N., Sekhari, A., and Xie, T. (2024). Harnessing density ratios for online
reinforcement learning.
Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In
International conference on machine learning, pages 263–272. PMLR.
Ball,P.J.,Smith,L.,Kostrikov,I.,andLevine,S.(2023). Efficientonlinereinforcementlearningwithoffline
data. In International Conference on Machine Learning, pages 1577–1594. PMLR.
Du, S. S., Kakade, S. M., Wang, R., and Yang, L. F. (2019). Is a good representation sufficient for sample
efficient reinforcement learning? arXiv preprint arXiv:1910.03016.
Duan, Y. and Wang, M. (2020). Minimax-optimal off-policy evaluation with linear function approximation.
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2020). A theoretical analysis of deep q-learning.
He, J., Zhao, H., Zhou, D., and Gu, Q. (2023). Nearly minimax optimal reinforcement learning for linear
markov decision processes.
14Hu,P.,Chen,Y.,andHuang,L.(2023). Nearlyminimaxoptimalreinforcementlearningwithlinearfunction
approximation.
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is q-learning provably efficient? Advances in
neural information processing systems, 31.
Jin, C., Liu, Q., and Miryoosefi, S. (2021a). Bellman eluder dimension: New rich classes of rl problems, and
sample-efficient algorithms.
Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2019). Provably efficient reinforcement learning with linear
function approximation.
Jin, Y., Yang, Z., and Wang, Z. (2021b). Is pessimism provably efficient for offline rl? In International
Conference on Machine Learning, pages 5084–5096. PMLR.
Kausik, C., Tan, K., and Tewari, A. (2024). Leveraging offline data in linear latent bandits.
Kearns,M.andSingh,S.(2002). Near-optimalreinforcementlearninginpolynomialtime. Machinelearning,
49:209–232.
Lange, S., Gabel, T., and Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement learning:
State-of-the-art, pages 45–73. Springer.
Lattimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in bandits
and in rl with a generative model.
Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline reinforcement learning: Tutorial, review, and
perspectives on open problems.
Li, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021). Sample-efficient reinforcement learning is feasible
for linearly realizable mdps with limited revisiting. Advances in Neural Information Processing Systems,
34:16671–16685.
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2024). Settling the sample complexity of model-based offline
reinforcement learning. The Annals of Statistics, 52(1):233–260.
Li, G., Yan, Y., Chen, Y., and Fan, J. (2023a). Minimax-optimal reward-agnostic exploration in reinforce-
ment learning.
Li,G.,Zhan,W.,Lee,J.D.,Chi,Y.,andChen,Y.(2023b).Reward-agnosticfine-tuning: Provablestatistical
benefits of hybrid reinforcement learning. arXiv preprint arXiv:2305.10282.
Min, Y., Wang, T., Zhou, D., and Gu, Q. (2021). Variance-aware off-policy evaluation with linear function
approximation. Advances in neural information processing systems, 34:7598–7610.
Munos, R. and Szepesv´ari, C. (2008). Finite-time bounds for fitted value iteration. Journal of Machine
Learning Research, 9(27):815–857.
Nahum-Shani, I., Smith, S. N., Spring, B. J., Collins, L. M., Witkiewitz, K. A., Tewari, A., and Murphy,
S. A. (2017). Just-in-time adaptive interventions (jitais) in mobile health: Key components and design
principles for ongoing health behavior support. Annals of Behavioral Medicine: A Publication of the
Society of Behavioral Medicine, 52:446 – 462.
Nair, A., Gupta, A., Dalal, M., and Levine, S. (2020). Awac: Accelerating online reinforcement learning
with offline datasets. arXiv preprint arXiv:2006.09359.
Nakamoto,M.,Zhai,Y.,Singh,A.,Mark,M.S.,Ma,Y.,Finn,C.,Kumar,A.,andLevine,S.(2023). Cal-ql:
Calibrated offline rl pre-training for efficient online fine-tuning.
Qiao, D. and Wang, Y.-X. (2022). Near-optimal deployment efficiency in reward-free reinforcement learning
with linear function approximation. arXiv preprint arXiv:2210.00701.
15Song, Y., Zhou, Y., Sekhari, A., Bagnell, J. A., Krishnamurthy, A., and Sun, W. (2023). Hybrid rl: Using
both offline and online data can make rl efficient.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second
edition.
Tan, K. and Xu, Z. (2024). A natural extension to online algorithms for hybrid rl with limited coverage.
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Roth¨orl, T., Lampe, T., and
Riedmiller, M. (2017). Leveraging demonstrations for deep reinforcement learning on robotics problems
with sparse rewards. arXiv preprint arXiv:1707.08817.
Wagenmaker, A., Chen, Y., Simchowitz, M., Du, S. S., and Jamieson, K. (2022). First-order regret in
reinforcement learning with linear function approximation: A robust estimation approach.
Wagenmaker, A. and Jamieson, K. (2023). Instance-dependent near-optimal policy identification in linear
mdps via online experiment design.
Wagenmaker, A. and Pacchiano, A. (2023). Leveraging offline data in online reinforcement learning.
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2023). Bellman-consistent pessimism for
offline reinforcement learning.
Xie, T., Foster, D. J., Bai, Y., Jiang, N., and Kakade, S. M. (2022a). The role of coverage in online
reinforcement learning. arXiv preprint arXiv:2210.04157.
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2022b). Policy finetuning: Bridging sample-efficient
offline and online reinforcement learning.
Xiong, W., Zhong, H., Shi, C., Shen, C., Wang, L., and Zhang, T. (2023). Nearly minimax optimal offline
reinforcement learning with linear function approximation: Single-agent mdp and markov game.
Yang, L. F. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive features.
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022). Near-optimal offline reinforcement learning with
linear representation: Leveraging variance information with pessimism.
Zanette,A.,Wainwright,M.J.,andBrunskill,E.(2021). Provablebenefitsofactor-criticmethodsforoffline
reinforcement learning.
Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. D. (2022). Offline reinforcement learning with
realizability and single-policy concentrability.
Zhang,Z.,Chen,Y.,Lee,J.D.,andDu,S.S.(2023). Settlingthesamplecomplexityofonlinereinforcement
learning. arXiv preprint arXiv:2307.13586.
Zhou,D.andGu,Q.(2022). Computationallyefficienthorizon-freereinforcementlearningforlinearmixture
mdps.
Zhou, D., Gu, Q., and Szepesvari, C. (2021). Nearly minimax optimal reinforcement learning for linear
mixture markov decision processes.
Zhou, Y., Sekhari, A., Song, Y., and Sun, W. (2023). Offline data enhanced on-policy policy gradient with
provable guarantees.
16A Unabridged versions of our algorithms
Algorithm 3 Reward-Agnostic Exploration-initialized Pessimistic PAC Learning (RAPPEL, Full)
1: Input: Offline dataset D off, samples sizes N on, N off, feature maps ϕ h, , tolerance parameter for reward-
agnostic exploration τ.
?
2: Initiali´z ře: D hp0q Ð H @h P ¯rHs, λ “ 1{H2, β 2 “ O˜ p dq. Set functions to optimize f ipΛq “
η´1log exppη }ϕ}2 q ,A pΛq“Λ`pT K q´1pΛ `Λ qforsomeΛ satisfyingΛ ľΛ
i ϕPΦ i AipΛq´1 i i i 0,i off 0,i 0,i 0
for all i, and η “22i{5.
i
ExplorationPhase: Runanexplorationalgorithm(OPTCOV,WagenmakerandJamieson(2023))
to collect covariates Λ such that max ϕJpΛ `λI`Λ q´1ϕ ďτ.
h ϕhPΦ h h off,h h
3: for i“1,2,3,... do
4: Set the number of iterates T i Ð2i, episodes per iterate K i Ð2i.
5: Play any policy for K i episodes to collect covariates Γ 0 and data D 0.
6: Initialize covariance matrix Λ 1 ÐΓ 0{K.
7: for t“ř1,...,T i do
8: if i j“1T jK j ěN on then
9: break
10: end if
11: Run FORCE (Wagenmaker et al., 2022) or another regret-minimizing algorithm on the
exploration-focused synthetic reward gptqps,aq9trp´∇ f pΛq| q.
h Λ i Λ“Λtϕps,aqϕps,aqJ
12: Collect covariates Γ t, data D t.
13: Perform Frank-Wolfe update: Γ t`1 Ðp1´ t`1 1qΛ t` t`1 1Γ t{K i.
14: end for
15: Assign Λy i,h yÐΛ Ti`1,D i ÐYT t“i 0D t.
16: Set Λ h “Λ i,h,D on “D i.
x
17: if f ipΛ iqďK iT iτ then
18: break
19: end if
20: end for
Planning Phase: Estimate πp using a pessimistic offline RL algorithm (LinPEVI-ADV+, Xiong
et al. (2023)) with hyperparameters λ,β on the combined dataset D YtDpNonqu .
2 off h hPrHs
21: Split the dataset D off YtD hpNonqu hPrHsinto D and D1. Estimate, on D1,
ÿ ” ` ˘ ` ˘ı
βr “argmin xϕpsτ,aτq,βy´ Vp 1 2 sτ 2 `λ}β}2,
h,2 h h h`1 h`1 2
βPRd
τ ÿPD1
” ` ˘ı
βr “argmin xϕpsτ,aτq,βy´Vp 1 sτ 2 `λ}β}2.
h,1 h h h`1 h`1 2
βPRd
τPD1
σp2ps,aq:“max! 1,“
ϕps,aqJβr
‰ ´“
ϕps,aqJβr
‰
2
´O˜´ ?dH3 ¯)
.
h h,2 r0,H2s h,1 r0,Hs Nκ
22: for h“1,...,H do ř
2 23 4:
:
C Co om mp pu ut te
e
c wo ev ia gr hi ta sn wc pe hm “a Σtri
´
hx 1´Σ řh τ“ PDϕτP pD
sτ
hϕ ,ap
τ
hsτ h q, ra hττ h
`
σpq
V
2pϕ
h
ps`p τ1s
,p
aτ h
s
τ,
τ h
q`a 1τ h qq ¯J .{σp h2psτ h,aτ hq`λI d.
h h h
25: Compute pessimistic penalty Γ hp¨,¨qÐβ 2}ϕ ␣p¨,¨q} Σ´1.
(
26: Compute pessimistic Q-fu @nction Qp hp¨,¨qÐ
D
ϕp¨,¨qqh Jwp h@´Γ hp¨,¨q r0,H´hD`1s.
27: Set πp hp¨|¨qÐargmax πh Qp hp¨,¨q,π hp¨|¨q A, Vp hp¨qÐ Qp hp¨,¨q,πp hp¨|¨q A.
28: end for
29: Output: πp.
17Algorithm 4 Hybrid Regression for Upper-Confidence Reinforcement Learning (HYRULE, Full)
1: Input: Offline dataset D off, samples sizes N on, N off, feature maps ϕ h. Regularization parameter λą0,
confidence radii β,β¯,β˜, t “0.
last
2: Initialize: For h P rHs, estimate wp 1,h,wq 1,h,Q 1,h,Qq ř1,h,σ 1,h,σ¯ 1,h from D off with the same formulas
outlined below, and assign Σ “Σ “Σ `λI“ Noff σ¯´2ϕ ϕJ `λI.
0,h 1,h off n“1 n,h n,h n,h
3: for episodes t“1,...,T do
4: Receive the initial state sptq.
1
5: for horizon h“ ř1,...,H do
6: wp k,h “Σ´ t,h1 řt i“´ 11σ¯ i´ ,h2ϕps hpiq,ap hiqqV t,h`1psp hi `q 1q.
7: wq t,h “Σ´ t,h1 t i“´ 11σ¯ i´ ,h2ϕps hpiq,a hpiqqVq t,h`1psp hi `q 1q.
8: if there exists a stag!e h1 PrHs such that detpΣbt,h1qě2detpΣ tlast,h1q then )
9: Q t,hps,aq“min !r hps,aq`wp tJ ,hϕps,aq`β bϕps,aqJΣ´ t,h1ϕps,aq,Q t´1,hps,aq,H ).
10: Qq t,hps,aq“max r hps,aq`wq tJ ,hϕps,aq´β¯ ϕps,aqJΣ´ t,h1ϕps,aq,Qq t´1,hps,aq,0 .
11: Set the last updating episode t last “t.
12: else
q q
13: Q t,hps,aq“Q t´1,hps,aq, Q t,hps,aq“Q t´1,hps,aq.
14: end if
q q
15: V t,hpsq“max aQ t,hps,aq, V t,hpsq“max aQ t,hps,aq.
16: end for
17: for horizon h“1,...,H do
18: Play action a hptq Ðc “argmax aQ ‰t,´hpsp htq,aq.
¯
19: Estimate σ t,h “ V t,hV t,h`1 sp htq,ap htq `E t,h`D t,h`H, setting E t,h and D t,h:
! › › ) ! › › )
E “min
βr› ›Σ´1{2ϕpsptq,aptqq›
› ,H2 `min
2Hβ¯› ›Σ´1{2ϕpsptq,aptqq›
› ,H2 ,
t,h t,h h h t,h h h
# ˜ 2 2
D “min 4d3H2 wpJ ϕpsptq,aptqq´wqJ ϕpsptq,aptqq
t,h t,h h h t,h h h
¸ +
b
`2β¯ ϕpsptq,aptqqJΣ´1ϕpsptq,aptqq ,d3H3 .
h h t,h h h
" › ´ ¯› *
? › ›1{2
20: σ¯ t,h Ðmax σ t,h, H, ´2d3H2›ϕ
¯
s ´p htq,ap htq ›
Σ¯´ t,h1
3.
J
21: Σ t`1,h “Σ t,h`σ¯ t´ ,h2ϕ sp htq,a hptq ϕ sp htq,ap htq .
22: Receive reward rptq, next state sptq .
h h`1
23: end for
24: end for
25: Output: Greedy policy πp “πQT,h, UnifpπQ1,h,...,πQT,hq for PAC guarantee.
?
3Heetal.(2023)writeσ¯
t,h
Ðmaxtσ t,h,H,...uinsteadof H. Webelievet ?hatthisisatypointheirpaper,giventhatin
t ah rre ap yr oo fof eqo uf aL te iom nm sa rigB h. t1, aft th ee ry es qt ua at te ior nigh Dt .2a 2ft ,e pr ae rq tiu ca ut li ao rn lyD › › ›. σ¯7 ´t 1h ϕa `t s0 i,ď aiσ¯ ˘i´ › › ›,h1 ďď › ›1 ϕ{ ` sH i,. aM i˘o › ›re {o ?ve Hr, ,in ont lh ye hp or ldoo sf tro uf eL ie fm thm isa iB s. ?5 Hth .e
i,h h h 2 h h 2
18B Proofs for Theorem 1
The proof of Theorem 1 follows from a series of distinct results, presented as three lemmas below. The
first lemma demonstrates that RAPPEL achieves no higher error than LinPEVI-ADV+ itself, the second
produces a d dH4 error bound, while the third produces a d2 dH3 error bound via a slightly different
on on
truncation argument. We will prove Equation 7 in Lemma 2, which act as a general statistical guarantee for
RAPPEL. We show the validity of the instance-dependent bound developed from Equation 7 in Lemmas 3
and 4. We observe that Theorem 1 follows immediately after.
Lemma 2 (GeneralStatisticalGuaranteeforRAPPEL,Algorithm1). Forevery δ Pp0,1qandanypartition
X ,X , with probability at least 1´δ, RAPPEL achieves
off on
? ÿH ? ÿH
V˚psq´Vπp psqÀ d E ||ϕps ,a q|| ď d E ||ϕps ,a q|| .
1 1 π˚ h h pΣ˚ `Σ˚ q´1 π˚ h h Σ˚´1
off,h on,h off,h
h“1 h“1
Proof. Before we proof the desired result, we first recall that
ÿ
Λ “ ϕpsτ,aτqϕpsτ,aτqJ`I , (10)
h h h h h d
τÿPD “ ‰
Σ˚ “ ϕpsτ,aτqϕpsτ,aτqJ{ V V˚ psτ,aτq`λI . (11)
h h h h h h h`1 h h d
τPD
?
Then, by invoking Theorem 2 from Xiong et al. (2023) with N ą Ωpd2H6q,λ “ 1{H2,β “ Op dq, we see
1
that
? ÿH ” ı
V˚psq´Vπp psqÀ d E }ϕps ,a q} |s “s
1 1 π˚ h h Σ˚´1 1
h
h“1
? ÿH ” ı
“ d E }ϕps ,a q} |s “s ,
π˚ h h pΣ˚ `Σ˚ q´1 1
off,h on,h
h“1
as Σ “ Σ˚ `Σ˚ . Noting that Σ˚ is positive semi-definite, it then follows Σ˚ ĺ Σ˚ `Σ˚ .
h off,h on,h on,h off,h off,h on,h
Therefore,
? ÿH ? ÿH
d E ||ϕps ,a q|| ď d E ||ϕps ,a q|| ,
π˚ h h pΣ˚ `Σ˚ q´1 π˚ h h Σ˚´1
off,h on,h off,h
h“1 h“1
and the inequality holds.
Lemma 3 (First Error Bound for RAPPEL, Algorithm 1). For every δ Pp0,1q and any partition X ,X ,
off on
with probability at least 1´δ, RAPPEL achieves
d d
c pX qdH4 d dH4
V˚psq´Vπp psqÀ off off ` on , where
1 1 N N
off on
␣ (
N ě max α4 d´4,α4 c pX q´4 maxtN˚,polypd,H,c pX q,log1{δqu, where we define the quantities
on on off off off off off
α
off
“ N Noff, α
on
“ N Non, and the minimal samples for coverage is
` ˘
N˚ “minC¨N s.t. inf maxϕJ NpΛ`λ¯Iq`Λ ´1 ϕďO˜ pmaxtd {N ,c pX q{N uq.
off on on off off off
N ΛPΩ ϕPΦ
Proof. Let X ,X be an arbitrary partition of SˆAˆrHs. Let us leave the choice of OPTCOV tolerance
off on
unspecified for the moment, and simply assume for now that we have data D collected under the success
event of Lemma 16.
We now invoke Theorem 2 from Xiong et al. (2023) on this dataset. As we choose N ą Ωpd2H6q,
?
λ“1{H2 and β “Op dq, we obtain the suboptimality gap decomposition below:
1
? ÿH ” ı
V˚psq´Vπp psqÀ d E }ϕps ,a q} |s “s .
1 1 π˚ h h Σ˚´1 1
h
h“1
19Thisdecompositioncanbefurtherdecomposedintothesumofbonusesontheofflineandonlinepartitions
X and X , respectively:
off on
? ÿH ” ı
d E }ϕps ,a q} |s “s
π˚ h h Σ˚´1 1
h
h“1
? ÿH ´ ” ı ” ı¯
“ d E }ϕps ,a q} 1 |s “s `E }ϕps ,a q} 1 |s “s
π˚ h h Σ˚´1 Xon 1 π˚ h h Σ˚´1 Xoff 1
h h
h“1
? ÿH
„b ȷ
“ d E ϕps ,a qJΣ˚´1ϕps ,a q1 |s “s
π˚ h h h h h Xon 1
h“1
? ÿH
„b ȷ
` d E ϕps ,a qJΣ˚´1ϕps ,a q1 |s “s .
π˚ h h h h h Xoff 1
h“1
Wecanfurtherupperboundtheaboveexpectationsundertheoptimalpolicyπ˚ bytakingthemaximum
of the quadratic form over each partition, yielding
? ÿH ” ı
d E }ϕps ,a q} |s “s
π˚ h h Σ˚´1 1
h
h“1
? ÿH b ? ÿH b
“ d max ϕJΣ˚´1ϕ 1 ` d max ϕJΣ˚´1ϕ 1
h“1ϕhPΦon
h h h Xon
h“1ϕhPΦoff
h h h Xoff
? ÿH b ? ÿH b
ď d max ϕJΣ˚´1ϕ ` d max ϕJΣ˚´1ϕ .
h h h h h h
h“1ϕhPΦon h“1ϕhPΦoff
“ ‰ “ ‰
From Xiong et al. (2023), as V V˚ p¨,¨qP 1,H2 , the weighted covariance matrix is uniformly upper
h h`1
bounded by the unweighted covariance matrix in the following manner:
Σ˚´1 ĺH2Λ´1,
h h
which leads to our conclusion that
? ÿH b ? ÿH b
V˚psq´Vπp psqÀ d max H2ϕJΛ´1ϕ ` d max H2ϕJΛ´1ϕ .
1 1 h h h h h h
h“1ϕhPΦon h“1ϕhPΦoff
We now further bound the above two quadratic forms over the online and offline partitions respectively.
ByLemma1,thepartialonlinecoveragecoefficientisboundedbythedimensionalityoftheonlinepartition:
c onpX onq“in πf ϕm hPa Φx onϕJ hE ϕ¯ h„dπ hrϕ¯ hϕ¯J hs´1ϕ h ďd on.
As we have N online episodes, the optimal covariates for online exploration would then yield
on
inf max ϕJΛ´1ϕ Àc pX q{N ďd {N .
h h on on on on on
Λ ϕhPΦon
Conversely, we also have access to N episodes of offline data with the following guarantee that follows
off
from an application of Matrix Chernoff:
max ϕJΛ´1ϕ Àc pX q{N .
h off h off off off
ϕhPΦoff
Therefore, by Lemma 5, we can conclude that on its success event, running OPTCOV with tolerance
O˜ pmaxtd {N ,c pX q{N uq, provides us covariates such that
on on off off off
maxϕJΛ´1ϕ Àmaxtc pX q{N ,d {N u,
h h h off off off on on
ϕhPΦ
20yielding the desired result.
It now remains to work out the burn-in cost from running OPTCOV. The following quantity of the
minimal online samples any algorithm requires to establish coverage was first proposed in Wagenmaker and
Pacchiano (2023):
N˚ “minC¨N s.t. inf maxϕJ` NpΛ`λ¯Iq`Λ ˘ ´1 ϕď O˜ pmaxtd on{N on,c offpX offq{N offuq .
N ΛPΩ ϕPΦ off 6
We can use this as follows. Invoking Lemma 16, we see that OPTCOV incurs
#ˆ ˙ ˆ ˙ +
N 4{5 N 4{5
max off , on maxtN˚,polypd,H,c pX q,log1{δqu
c pX q d off off
off off on
episodes of online exploration, for an overall burn-in cost of
" *
α4 α4
N `N ěmax on, off maxtN˚,polypd,H,c pX q,log1{δqu
off on d4 c pX q4 off off
on off off
episodes, where α off “ NofN f`of Nf
on
and α on “ NofN f`on Non.
Note that the more even the proportion of offline to online samples, the smaller α ,α are. In fact, as
off on
α4 ,α4 Pr0.0625,1s, this term contributes no more than a constant factor that is no greater than 1 to the
off on
final sample complexity.
We then have that
˜d d ¸
c pX qdH4 d dH4
V˚psq´Vπp psqÀ inf off off ` on
1 1 Xoff,Xon N off N on
! )
withprobabilityatleast1´δ, whenN ěmax α
d4
o4 o nn, coffα pX4 of of
ffq4
maxtN˚,polypd,H,c offpX offq,log1{δqu.
Lemma4(SecondErrorBoundforRAPPEL,Algorithm1). Foreveryδ Pp0,1qandanypartitionX ,X ,
off on
with probability at least 1´δ, RAPPEL achieves
d d
c pX q2dH3 d2 dH3
V˚psq´Vπp psqÀ off off ` on , where
1 1 N α N α
off off on on
␣ (
N ě max α4 d´4,α4 c pX q´4 maxtN˚,polypd,H,c pX q,log1{δqu, we define the quantities α “
on on off off off off off off
N Noff, α
on
“ N Non, and the minimal samples for coverage is
` ˘
N˚ “minC¨N s.t. inf maxϕJ NpΛ`λ¯Iq`Λ ´1 ϕďO˜ pmaxtd {N ,c pX q{N uq.
off on on off off off
N ΛPΩ ϕPΦ
Proof. First, we set up some preliminaries. Following the same argument as the proof of Lemma 3, we can
establish that, for arbitrary partition X “X YX , we have
on off
c pX qďd ,
on on on
and running OPTCOV with tolerance O˜ pmaxtd {N ,c pX q{N uq, yields:
on on off off off
maxϕJΛ´1ϕ Àmaxtc pX q{N ,d {N u.
h h h off off off on on
ϕhPΦ
This incurs #ˆ ˙ ˆ ˙ +
N 4{5 N 4{5
max off , on maxtN˚,polypd,H,c pX q,log1{δqu
c pX q d off off
off off on
episodes of online exploration, for an overall burn-in cost of
" *
α4 α4
N `N ěmax on, off maxtN˚,polypd,H,c pX q,log1{δqu
off on d4 c pX q4 off off
on off off
21episodes.
TotightenthehorizondependenceevenfurtherfromtheresultofLemma3,weturntothetotalvariance
lemma. i.e. Lemma C.5 in Jin et al. (2018), indicating that
ˆ ˙
1 ÿ ÿH “ ‰ H2
V V˚ psτ,aτqÀO˜ H ` .
NH h h`1 h h N
τPDh“1
Then, we directly apply Lemma 17 with γ “ maxtd {N ,c pX q{N u and σ¯ “ H `H2{N, we will
on on off off off
then obtain that
d
b ˆ ˙ ˆ ˙
ÿH
d c pX q H2
max ϕJΣ‹´1ϕ ď on ` off off H N H `
h“1ϕhPΦ h h h ˆN on N off
˙
N
a
d c pX q
ď on ` off off NH3`H4
N N
d on off d
c pX q2H3 c pX q2H4 d2 H3 d2 H4
ď off off ` off off ` on ` on (12)
N α N2 N α N2
d off off d off on on on
c pX q2H3 d2 H3
À off off ` on , (13)
N α N α
off off on on
which leads to our final result:
˜d d ¸
c pX q2dH3 d2 dH3
V˚psq´Vπp psqÀ inf off off ` on ,
1 1 Xoff,Xon N offα off N onα on
where α “N {N and α “N {N.
off off on on
C Proof of Corollary 1
Proof. In tabular case, we set ϕps,aq “ 1 and d “ |S|¨|A|. Let N ps,aq be the number of visits to a
s,a h
specific state-action pair ps,a,hq. As the exploration algorithm OPTCOV ensures that
ˆ ˙
1 d c pX q
max ďmax on , off off ,
s,a,h N hps,aq N on N off
we bound the error in the following way follows from Lemma 2,
? ÿH
V˚psq´Vπp psqÀ d E ||ϕps ,a q||
1 1 π˚ h h pΣ˚ `Σ˚ q´1
off,h on,h
h“1 d
“ ‰
a ÿH ÿ V V˚ ps,aq
ď |S||A| d‹ps,aq h h`1 ,
h N ps,aq
h“1s,a h
` “ ‰ ˘
where the last inequality follows from the fact that Σ‹ “ diag N ps,aq{ V V˚ ps,aq . We will
h h h h`1 sPS,aPA
then decompose the state-action space into X and X , and bound the two parts seperately based on the
off on
tolerance level of OPTCOV,
d
“ ‰
a ÿH ÿ V V˚ ps,aq
V˚psq´Vπp psqÀ |S||A| d‹ps,aq h h`1 1
1 1 h N ps,aq Xoff
h“1s,a d h
“ ‰
a ÿH ÿ V V˚ ps,aq
` |S||A| d‹ps,aq h h`1 1
h N ps,aq Xon
h“1s,a h
22d
|S||A|c pX q
ÿH ÿ b “ ‰
ď off off d‹ps,aq V V˚ ps,aq1
N h h h`1 Xoff
d off h“1s,a
|S||A|d
ÿH ÿ b “ ‰
` on d‹ps,aq V V˚ ps,aq1
N h h h`1 Xon
˜don h“1s,a
c ¸
a
c pX q d
ÿH ÿb “ ‰
ď |S||A| off off ` on d‹ps,aq V V˚ ps,aq.
N N h h h`1
off on h“1s,a
As the optimal policy π‹ executes a deterministic action π‹psq for any state s, the inequality can be further
bounded as
˜d c ¸
a
c pX q d
ÿH ÿb “ ‰
V˚psq´Vπp psqÀ |S||A| off off ` on d‹ps,π‹psqq V V˚ ps,π‹psqq
1 1 N N h h h`1
˜d
off con h ¸“1g fs
ďa
H|S|2|A|
c offpX offq
`
d
on
f eÿH ÿ d‹ps,π‹psqq“
V V˚
‰
ps,π‹psqq
N N h h h`1
˜d
off
c
on
¸g
fh“1 s
ďa
H|S|2|A|
c offpX offq
`
d
on
f eÿH
E
“
V V˚
‰
ps,aq
N N ps,aq„d π‹ h h`1
off on
˜d c ¸
h“1
a
c pX q d
ď H3|S|2|A| off off ` on , (14)
N N
off on
where the last inequality follows from the proof of Lemma C.5. in Jin et al. (2018).
D On concentrability and coverability
Lemma 1. For any partition X ,X , we have that c pX q ď d . Similarly, there exists a partition
off on on on on
such that c pX q“Opdq.
off off
Proof. This proof follows a similar strategy to that of Lemma B.10 in Wagenmaker and Jamieson (2023),
except that we exploit the projections onto d to get a bound that depends on d ď d, instead of d. We
on on
wish to bound
1
c pX q“infmax .
on on π h λ donpE dπ hrpP onϕ hqpP onϕ hqJsq
P P Rdˆd has rank d ď d, so we can decompose this with the thin SVD into P “ U UJ, where
on on on on on
U
on
PRdˆdon. It then holds that
λ donpE
dπ
hrpP onϕ hqpP onϕ hqJsq“λ minpE
dπ
hrpU oJ nϕ hqpU oJ nϕ hqJsq,
and from Lemma 20 that
c onpX onq“inf sup v hJU onE dπrpU oJ nϕ hqpU oJ nϕ hqJs´1U oJ nv h.
π vhPΦon h
Apply Jensen’s inequality to find that for any v PΦ ,
h on
v hJU onE
dπ
hrpU oJ nϕ hqpU oJ nϕ hqJsU oJ nv
h
ěv hJU onE
ϕh„dπ
hrU oJ nϕ hsE
ϕh„dπ
hrU oJ nϕ hsJU oJ nv h.
Then, we can bound
c onpX onq“inf sup v hJU onE dπrpU oJ nϕ hqpU oJ nϕ hqJs´1U oJ nv
h
π vhPΦon h
23` “ ‰˘
ďin ρf vhs Pu Φp onv hJU on E π„ρ E ϕh„dπ hrU oJ nϕ hsE ϕh„dπ hrU oJ nϕJ hs ´1 U oJ nv h.
By Kiefer-Wolfowitz (Lattimore et al., 2020), this is bounded by d .
on
Similarly,
1
inf c pX q“ inf max
Xoff,Xon off off Xoff,Xon h λ doffpE µhrpP offϕ hqpP offϕ hqJsq
1
“ inf max
Xoff,Xon h λ minpE µhrpU oJ ffϕ hqpU oJ ffϕ hqJsq
ďOpdq.
` ˘
wheretheupperboundisachievedwhen,forinstance,wechooseX suchthatΦ “Span pv ,...,v q ,
ř off off h,1 h,kh hPrHs
where v is the i-th largest eigenvector of E rϕ ϕJs « 1 ϕ psτ,aτqϕ psτ,aτqJ, and v is the
h,i µ h h Noff τPDoff h h h h h h h,kh
eigenvector corresponding to the largest eigenvalue λ ě Ωp1{k q. The largest eigenvalue λ is always
h,kh h h,1
Ωp1{dq for non-null features, so there always exists such a partition where d is at least 1.
off
Informally, one can choose the offline partition to be the span of the large eigenvectors of the covariance
matrix,sothesmallesteigenvalueoftheprojectedcovariancematrix,i.e. thepartialallpolicyconcentrability
coefficient, is no larger than the dimension of the partition.
Lemma 5 (Maximum Eigenvalue Bound with OPTCOV). On any partition X ,X , if we run OPTCOV
off on
with tolerance O˜ pmaxtd {N ,c pX q{N uq, on this partition we also have that
on on off off off
maxϕJΛ´1ϕ Àmaxtc pX q{N ,d {N u.
h h h off off off on on
ϕhPΦ
Proof. By Lemma 1, for any partition, we have that
c onpX onq“in πf ϕm hPa Φx onϕJ hE ϕ¯ h„dπ hrϕ¯ hϕ¯J hs´1ϕ h ďd on,
Applying Matrix Chernoff, we have that with probability at least 1´δ,
˜ d ˆ ˙¸ ´1
2 4d
ϕm hPa Φx offϕJ hΛ´ h,1 offϕ h ď ϕm hPa Φx offϕJ hE ϕ¯ h„µhrϕ¯ hϕ¯J h `N o´ ff1Is´1ϕ hN o´ ff1 1´ N
off
log δ ,
and similarly for c pX q we also have that
on on
˜ d ˆ ˙¸ ´1
2 4d
in πf ϕm hPa Φx onϕJ hΛ´ h,1 πϕ h ďin πf ϕm hPa Φx onϕJ hE ϕ¯ h„µhrϕ¯ hϕ¯J hs´1ϕ hN o´ n1 1´ N
on
log δ .
As Λ `Λ “Λ , we have
h,off h,on h
" *
maxϕJΛ´1ϕ “max max ϕJΛ´1ϕ , max ϕJΛ´1ϕ
h h h h h h h h h
ϕhPΦ "ϕhPΦoff ϕhPΦon *
Àmax c pX q{N , max ϕJΛ´1ϕ ,
off off off h h h
ϕhPΦon
where the last step follows from the choice of partition. So it suffices to run OPTCOV with tolerance
O˜ pmaxtd {N ,c pX q{N uq,
on on off off off
to find that there exists at least one partition such that
maxϕJΛ´1ϕ Àmaxtc pX q{N ,d {N u.
h h h off off off on on
ϕhPΦ
24Lemma 6 (Coverability Coefficient Is Bounded In Tabular MDPs). If the underlying MDP is tabular, for
any partition X ,X , we have that c pX qďd .
off on on on on
Proof. First, we write the concentrability coefficient in terms of densities.
1
c pX q“minmax
on on π h λ donpE dπ hrpP onϕ hqpP onϕ hqJsq
1
ďminmax Xon
π h min s,adπ hps,aq1 Xon
1
ďminmax Xon .
π h,s,a dπ hps,aq1 Xon
By the same trick that Xie et al. (2022a) use in their Lemma 3,
1 1
Xon
ď
řXon
dπps,aq1 sup dπ2ps,aq1 { sup dπ1ps1,a1q1
h Xon ř π2 h Xon s1,a1 π1 h Xon
sup dπps,aq1
ď
s,a π h Xon
sup dπps,aq1
π h Xon
ďd .
on
E Proofs for Algorithm 2
E.1 Setup
We consider the same state-action space splitting framework of Tan and Xu (2024). Let X Y X “
on off
rHsˆS ˆA. Then, their images under the feature map Φ “ SpanpϕpX qq Ď Rd and Φ “
off off,h hPrHs on
SpanpϕpX qq ĎRd aresubspacesofX withdimensiond andd ,respectively. WedenoteP ,P
on,h hPrHs off on off on
as the orthogonal projection operators onto these subspaces respectively. The partial offline all-policy con-
centrability coefficient
1
c pX q“max ,
off off h λ doffpE µhrpP offϕ hqpP offϕ hqJsq
is bounded by the inverse of the d -th largest eigenvalue of the covariance matrix of the projected fea-
off
ture maps onto the offline partition, where λ is the k-th largest eigenvalue. Write 1 as shorthand for
k Xon
1pps,a,hqPX q, and similarly for 1 .
on Xoff
Now,weworkthroughtheanalysisofHeetal.(2023)toensurethattheirresultholdsinoursetting,where
the regret decomposes into online part }Σ´1{2ϕ psptq,aptqq1 } and offline part }Σ´1{2ϕ psptq,aptqq1 }
t,h h h h Xon 2 t,h h h h Xoff 2
respectively, instead of }Σ´1{2ϕ psptq,aptqq} .
t,h h h h 2
E.2 High-probability events
We define several “high probability” events which are similar to those defined in He et al. (2023).
• We define wr as the solution of the weighted ridge regression problem for the squared value function
t,h
tÿ´1
wr “Σ´1 σ¯´2ϕpspiq,apiqqV2 pspiq q. (15)
t,h t,h i,h h h t,h`1 h`1
i“1
• We define E as the event where the following inequalities hold for all s,a,t,hPSˆAˆrTsˆrHs:
b
ˇ ˇ
ˇ wpJ ϕps,aq´rP V sps,aqˇ ďβ¯ ϕps,aqJΣ´1ϕps,aq, (16)
t,h h t,h`1 t,h
25b
ˇ “ ‰ ˇ
ˇ wrJ ϕps,aq´ P V2 ps,aqˇ ďβr ϕps,aqJΣ´1ϕps,aq, (17)
t,h h t,h`1 t,h
ˇ ” ı ˇ b
ˇ ˇwqJ ϕps,aq´ P Vq ps,aqˇ ˇďβ¯ ϕps,aqJΣ´1ϕps,aq, (18)
t,h h t,h`1 t,h
ˆ b ˙ ˆ b ˙
? ?
βr “O H2 dλ` d3H4log2pdHN{pδλqq ,β¯ “O H dλ` d3H2log2pdHN{pδλqq .
This is the “coarse event” as mentioned in their paper, where concentration holds for the value and
squared value function with all three estimators.
• We define Er as the event that for all episodes t P rTs, stages h ď h1 ď H and state-action pairs
h
ps,aqPSˆA, the weight vector wp satisfies
t,h
b
ˇ ˇ
ˇ wpJ ϕps,aq´rP V sps,aqˇ ďβ ϕps,aqJΣ´1ϕps,aq, (19)
t,h1 h t,h1`1 t,h1
where ˆ b ˙
?
β “O H dλ` dlog2p1`dNH{pδλqq .
r r
Furthermore, let E “E denotes the event that (19) holds for all stages hPrHs. This is the fine event
1
where concentration for wˆ is tighter than that required in (16) to (18).
Equipped with these definitions, we recall the following lemmas from He et al. (2023):
Lemma 7 (Lemma B.1, He et al. (2023)). E holds with probability at least 1´7δ.
r
Lemma 8 (Lemma B.2, He et al. (2023)). On the event E and E , for each episode tPrTs and stage h,
h`1
the estimated variance satisfies
ˇ ˇ“ ‰´ ¯ ´ ¯ˇ ˇ
ˇ V V sptq,aptq ´rV V s sptq,aptq ˇďE ,
h t,h`1 h h h t,h`1 h h t,h
ˇ ˇ“ ‰´ ¯ “ ‰´ ¯ˇ ˇ
ˇ V V sptq,aptq ´ V V˚ sptq,aptq ˇďE `D .
h t,h`1 h h h h`1 h h t,h t,h
r
Lemma 9 (Lemma B.3, He et al. (2023)). On the event E and E , for any episode t and iąt, we have
h`1
“ ` ˘‰´ ¯ ` ˘
V V ´V˚ sptq,aptq ďD { d3H .
h i,h`1 h`1 h h t,h
r
Lemma 10 (Lemma B.4, He et al. (2023)). On the event E and E , for all episodes t P rTs and stages
h
q q
hďh1 ďH, we have Q ps,aqěQ˚ps,aqě Q ps,aq. In addition, we have V psqěV˚psqěV psq.
t,h h t,h t,h h t,h
r
Lemma 11 (Lemma B.5, He et al. (2023)). On event E, event E holds with probability at least 1´δ.
E.3 Regret decomposition
From He et al. (2023), based on Lemma B.4 of their paper, Q psptq,aptqq “ V psptqq ě V˚psptqq, i.e.
t,h h h t,h h h h
optimism holds for all episodes and timesteps. Therefore,
ÿT ÿH !” ´ ¯ı´ ¯ ´ ´ ¯ ´ ¯¯)
RegpTqÀ P V ´Vπptq sptq,aptq ´ V sptq ´Vπptq sptq
h t,h`1 t,h`1 h h t,h`1 h`1 t,h`1 h`1
t“1h“1
ÿT ÿH
`β }Σ´1{2ϕ psptq,aptqq} .
t,h h h h 2
t“1h“1
26Accordingly, given a partition X ,X of rHsˆSˆA, we can further decompose this into the fraction
off on
of episodes where each partition is visited,
ÿT ÿH ÿ ÿ
}Σ´1{2ϕ psptq,aptqq} “ }Σ´1{2ϕ psptq,aptqq1 } ` }Σ´1{2ϕ psptq,aptqq1 } .
t,h h h h 2 t,h h h h Xoff 2 t,h h h h Xon 2
t“1h“1 h,t h,t
He et al. (2023) define the events
#
ÿT ÿH ” ´ ¯ı´ ¯
E “ @hPrHs, P V ´Vπptq sptq,aptq
1 h t,h`1 t,h`1 h h
t“1h1“h +
ÿT ÿH ´ ´ ¯ ´ ¯¯ a
´ V sptq ´Vπptq sptq ď2 2H3T logpH{δq ,
t,h`1 h`1 t,h`1 h`1
t“1h1“h
#
ÿT ÿH ” ´ ¯ı´ ¯
E “ @hPrHs, P V ´Vq sptq,aptq
2 h t,h`1 t,h`1 h h
t“1h1“h +
ÿT ÿH ´ ´ ¯ ´ ¯¯ a
´ V sptq ´Vq sptq ď2 2H3T logpH{δq ,
t,h`1 h`1 t,h`1 h`1
t“1h1“h
which they show that by Azuma-Hoeffding, both hold with probability 1´δ each. As such, we have that
a ÿ › › ÿ › ›
RegpTqÀ H3T logpH{δq` β› Σ´1{2ϕ psptq,aptqq1 › ` β› Σ´1{2ϕ psptq,aptqq1 › .
t,h h h h Xoff 2 t,h h h h Xon 2
h,t h,t
Here, we denote
ÿ › › ÿ › ›
Reg pTq“ β› Σ´1{2ϕ psptq,aptqq1 › , Reg pTq“ β› Σ´1{2ϕ psptq,aptqq1 › ,
off t,h h h h Xoff 2 on t,h h h h Xon 2
h,t h,t
as the offline regret and online regret, respectively.
E.4 Offline regret control
Now, we bound the regret on the offline partition. We first perform a similar argument to that in Tan and
Xu(2024);Xieetal.(2022a)toshowthatthesumofbonusescanbecontrolledbythemaximumeigenvalue
of the inverse weighted average covariance matrix in Lemma 12. We will then show that the maximum
eigenvalue can be nicely bounded in Lemma 13.
Lemma 12 (Sum of Bonuses on Offline Partition). For any partition X ,X , we can bound the sum of
off on
bonuses on the offline partition with the following:
d
ÿH
dN2
Reg pTqÀ on max ϕJΣ¯´1 ϕ ,
off
h“1
N off ϕhPΦoff h off,h h
where Σ¯ “pΣ `λIq{N and T “N .
off,h off,h off on
Proof. It is sufficient to show the following holds true
d
ÿ › ›
β› Σ´1{2ϕ psptq,aptqq1 › ď dN o2 n max ϕJΣ¯´1 ϕ ,
t t,h h h h Xoff 2 N off ϕhPΦoff h off,h h
then the desired inequality directly follows. With a direct calculation, one may observe that
b
}Σ´1{2ϕ psptq,aptqq1
} “
ϕJpsptq,aptqqΣ´1ϕ psptq,aptqq1
t,h h h h Xoff 2 h h h t,h h h h Xoff
27b
À ϕJpsptq,aptqqpΣ `λIq´1ϕ psptq,aptqq1 ,
h h h off,h h h h Xoff
where the last inequality holds as Σ ĺ Σ . As a result, we are able to bound the desired inequality
off,h t,h
with the maximum eigenvalue of the inverse weighted matrix,
ÿ c
}Σ´1{2ϕ psptq,aptqq1 } ďN max ϕJpΣ `λIq´1ϕ
t
t,h h h h Xoff 2 don
ϕhPΦoff
h off,h h
N
“ N on max ϕJΣ¯´1 ϕ ,
onN
off ϕhPΦoff
h off,h h
?
where Σ¯ “pΣ `λIq{N . As β “O˜ p dq, we obtain the bound we desired:
off,h off,h off
d
ÿ
N
β}Σ´1{2ϕ psptq,aptqq1 } ď dN on max ϕJΣ¯´1 ϕ .
t
t,h h h h Xoff 2 onN
off ϕhPΦoff
h off,h h
Lemma 13 (Partial Concentrability Bound). For any partition X ,X , we have that
off on
ÿH b a
max ϕJΣ¯´1 ϕ À c pX q2H3,
h off,h h off off
h“1ϕhPΦoff
when N ,N ěΩ˜ pd13H14q, where we define Σ¯ “pΣ `λIq{N .
on off off,h off,h off
Proof. Similar to the definition of Σ¯ , we define Λ¯ “ pΛ `λIq{N in a similar way. Then, one
off,h off,h off,h off
may observe that
¨ ˜ ˜ ¸¸ ˛
max
´
ϕJΛ¯´1 ϕ
¯
“ max ˝ ϕJ 1
Nÿ
off ϕ ϕJ `λI
´1
ϕ ‚
ϕhPΦoff h off,h h ϕhPΦoff h N off n“1 n,h n,h h
˜ d ˆ ˙¸ ´1
2 4d
ď max ϕJE rΛ¯ s´1ϕ 1´ log ,
ϕhPΦoff h µh off,h h N off δ
where the last line holds by an application of the Matrix Chernoff inequality. Then, we may further bound
the quantity with the partial offline all-policy concentrability coefficient,
´ ¯
1
max ϕJΛ¯´1 ϕ À inf max
ϕhPΦoff h off,h h Xoff,Xon h λ doff pE µpP offϕ hqpP offϕ hqJq
1
“ inf max ` ˘
Xoff,Xon h λ min E µpU oJ ffϕ hqpU oJ ffϕ hqJ
“c pX q.
off off
To tighten the dependence of the regret of the offline partition on H, we again employ a truncation
argument that used in Lemma 4. Recall that in Section B of the appendix in He et al. (2023), by the total
variance lemma of Jin et al. (2019), it holds that
ÿT ÿH ` ˘
σ2 ďOr H2T `d10.5H16 .
t,h
t“1h“1
Again, recall that we have
ÿ
}Σ´1{2ϕ psptq,aptqq1
}
t,h h h h Xoff 2
h,t
28g
f ¨ ˜ ˜ ¸¸ ˛
Àf
f eH2N N on max ˝ ϕJ 1
Nÿ
off σ¯´2ϕ ϕJ `λI
´1
ϕ ‚ .
onN off ϕhPΦoff h N off n“1 n,h n,h n,h h
! )
As σ¯2 “max σ2 ,H,4d6H4||ϕ || . Consider the sets
n,h n,h n,h Σ´1
n,h
␣ (
I “ nPrN s:@h: σ¯2 “maxpσ2 ,Hq , I “Ic.
1 off n,h n,h 2 1
Here,I roughlycorrespondtothe“bad”setoftrajectorieswherethereexistssometimestephsuchthat
2
σ¯2 ąmaxtσ2 ,Hu, and I to be the “good” set of trajectories where the monotonic variance estimator is
n,h n,h 1
controlled.
Weneedtoboundthecardinalityofthelatterbeforeemploybingourtruncationargumentontheestimated
variances. AswenotethatforallnPI wehavethatmax ϕJ Σ´1ϕ ě1{p4d6H2q,whichindicates
2 hPrHs n,h n,h n,h
that
ÿH ␣ (
min 1,16d12H4ϕJ Σ´1ϕ ě1,
n,h n,h n,h
h“1
and so we can conclude that
ÿH Nÿ
off
␣ (
|I |ď min 1,16d12H4ϕJ Σ´1ϕ Àd13H5logp1`N{dq,
2 n,h n,h n,h
h“1n“1
by Lemma D.5 of Zhou and Gu (2022) and the fact that ||ϕ {σ¯ ||2 ď1{H2. As we require in Theorem 2
n,h n,h
that N ,N “Ω˜ pd13H14q, we come to the following result
on off
|I |{N À8d13H5logp1`N{dq{N “o˜p1q, |I |{N “1´o˜p1q.
2 off off 1 off
Informally, this means that the proportion of trajectories in the “bad set” I is asymptotically zero, and
2
the proportion in the “good set” I is asymptotically one. As for every nPI we have that for any hPrHs,
1 1
´ ¯
max ϕJΣ¯´1 ϕ
h off,h h
ϕhPΦoff ¨ ˜ ˜ ¸¸ ˛
“ max ˝ ϕJ 1
Nÿ
off σ¯´2ϕ ϕJ `λI
´1
ϕ ‚
ϕhPΦoff h N off n“1 n,h n,h n,h h
¨ ˜ ¸ ˛
“ max N ˝ ϕJ
Nÿ
off σ¯´2ϕ ϕJ `λI
´1
ϕ ‚
off h n,h n,h n,h h
ϕhPΦoff
n“1
¨ ˜ ¸ ˛
ÿ ϕ ϕJ ´1
ď max N ˝ ϕJ n,h n,h `λI ϕ ‚ .
ϕhPΦoff off h
nPI1
σ n2 ,h`H h
Now we invoke the total variance lemma. Recall that in Section B of the appendix in He et al. (2023),
by the total variance lemma of Jin et al. (2019), if N ěΩ˜ pd10.5H14q, it holds that
off
1
Nÿ
off
ÿH
σ2
“Or`
H2`d10.5H16{N
˘ “Or` H2˘
.
N n,h off
off n“1h“1
With a direct application of Lemma 17, as we set T “O˜ pHq and γ “c pX q{N , we will then get to
off off off
d
ÿH b
c pX
qHa
c pX qH3
max ϕJΣ´1 ϕ À off off N H “ off off ,
h“1ϕhPΦoff h off,h h N off off N off
29which indicates that
ÿH b a
max ϕJΣ¯´1 ϕ À c pX q2H3.
h off,h h off off
h“1ϕhPΦoff
Now, from Lemmas 12 and 13, for any partition X ,X , the offline regret satisfies
off on
d c
ÿH
N N
Reg pTqÀ dN on max ϕJΣ¯´1 ϕ À c pX q2dH3N on.
off
h“1
onN
off ϕhPΦoff
h off,h h off off onN
off
E.5 Online regret control
We will then bound the online term, Reg pTq. He et al. (2023) show in Lemma E.1 that it is possible to
on
use Cauchy-Schwarz to bound this by
¨ g ˛
f
f ÿT ÿH
Reg
pTq“Or˝ d4H8`βd7H5`βe
dHT `dH σ2
‚
,
on t,h
t“1h“1
and in Section B of the appendix, state that by the total variance lemma of Jin et al. (2019),
ÿT ÿH ` ˘
σ2 ďOr H2T `d10.5H16
t,h
t“1h“1
We will seek to use the online partition to tighten the dimensional dependence in the first result accord-
ingly.
Lemma 14 (Modified Lemma E.1 in He et al. (2023)). For any parameters β1 ě 1 and C ě 1, and any
partition X ,X , the summation of bonuses on the online partition is upper bounded by
off on
˜ c ¸
ÿT ´ ¯ ´ ¯
J
min β1 ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ,C
h h t,h h h Xon
t“1 g
f
f ÿT ´ ¯
e
ď4d4H6Cι`10β1d5 H4ι`2β1 2d ι σ2 `H
on on t,h
t“1
where ι“logp1`N{pdλqq.
Proof. ForeachhorizonhPrHs,wefirstnotethatthesummationcanbeboundedbythesumoftwoterms,
where the first term is tight-bounded and the second term stands for a tail event where ϕTΣ´1ϕ gets large.
˜ c ¸
ÿT ´ ¯ ´ ¯
J
min β1 ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ,C
h h t,h h h Xon
t“1 ˜c ¸
ÿT ´ ¯ ´ ¯
J
ď β1min ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ,1
h h t,h h h Xon
t“1 #c +
ÿT ´ ¯ ´ ¯
J
`C 1 ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ě1 .
h h t,h h h Xon
t“1
˜c ¸
´ ¯ ´ ¯
ř
J
We first bound T β1min ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ,1 , using a variant of Lemma B.1
t“1 h h t,h h h Xon
from Zhou and Gu (2022) in Lemma 18. With this, we have that
˜c ¸
ÿT ´ ¯ ´ ¯
J
β1min ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ,1
h h t,h h h Xon
t“1
30¨g ˛
f ˜ ¸
ÿT ˚f e ´ ¯ J Noÿ ff`t ´1 ´ ¯ ‹
ď β1min˝ ϕ sptq,aptq pϕ 1 qpϕ 1 qJ`λI ϕ sptq,aptq 1 ,1‚
h h n,h Xon n,h Xon d h h Xon
t“1 n“1
g
f
f ÿK ´ ¯
e
ď10β1d5 H4ι`2β1 2d ι σ2 `H ,
on on k,h
k“1
where ι“logp1`N{pdλqq.
From this, it suffices to follow the rest of the proof of Lemma E.1 from He et al. (2023) to bound the
remaining term by
#c +
ÿT ´ ¯ ´ ¯
J
1 ϕ sptq,aptq Σ´1ϕ sptq,aptq 1 ě1 ď4d4H6Cι.
h h t,h h h Xon
t“1
As a result, we obtain the following bound for the online regret
a
Reg pTqÀd7H9`β d dH3T.
on on
E.6 Putting everything together
Combining our results in E.4 and E.5, we come to the bound of total regret that
c
a a
N
RegpN qÀ H3N logpH{δq` c pX q2dH3N on ` d dH3N `d7H9.
on on off off onN on on
off
When we set N ,N “ Ω˜ pd13H14q and choose X , X be the partition that minimize the right hand
on off off on
side, we have
ˆc ˙
a
N
RegpN qÀ inf c pX q2dH3N on ` d dH3N ,
on
Xoff,Xon
off off onN
off
on on
proving Theorem 2.
F OPTCOV from Wagenmaker and Jamieson (2023)
We lean on the OPTCOV algorithm from Wagenmaker and Pacchiano (2023) for reward-agnostic explo-
ration , first proposed in Wagenmaker and Jamieson (2023), as well as the Frank-Wolfe subroutine used, for
completeness.
31Algorithm 5 Collection of Optimal Covariates (OPTCOV), Wagenmaker and Pacchiano (2023)
1: Input: functions to optimize pf iq i, constraint tolerance ϵ, confidence δ.
2: for i“1,2,3,... do
3: Set the number of iterates T i Ð2i, episodes per iterate K i Ð2i.
4: Play any policy for K i episodes to collect covariates Γ 0 and data D 0.
5: Initialize covariance matrix Λ 1 ÐΓ 0{K.
6: for t“1,...,T i do
7: Run FORCE (Wagenmaker et al., 2022) or another regret-minimizing algorithm on the
exploration-focused synthetic reward gptqps,aq9trp´∇ f pΛq| q.
h Λ i Λ“Λtϕps,aqϕps,aqJ
8: Collect covariates Γ t, data D t.
9: Perform Frank-Wolfe update: Γ t`1 Ðp1´ t`1 1qΛ t` t`1 1Γ t{K i.
10: end for
11: Assign xΛx i ÐΛ Ti`1,D i ÐYT t“i 0D t.
12: if f ipΛ iqďK iT iϵ then
p
13: Return: Λ,K iT i,D i.
14: end if
15: end for
The algorithm essentially performs the doubling trick to determine how many samples to collect, termi-
nating when the minimum eigenvalue of the covariance matrix is above the set tolerance.
WagenmakerandPacchiano(2023)thenprovethefollowingguaranteeforOPTCOVinthehybridsetting:
Lemma 15 (Termination of OPTCOV, Lemma C.2 (Wagenmaker and Pacchiano, 2023)). Let
˜ ¸
ÿ
f ipΛq“
η1
log
eηi}ϕ}2
AipΛq´1 , A ipΛq“Λ`
T1
K
Λ 0,i`
T1
K
Λ
off
i i i i i
ϕPΦ
forsomeΛ satisfyingΛ ľΛ foralli,andη “22i{5. Letpβ ,M qdenotethesmoothnessandmagnitude
0,i 0,i 0 i i i
constants for f . Let pβ,Mq be some values such that β ďη β,M ďM for all i. Then, if we run OPTCOV
i i i i
on pf q with constraint tolerance ϵ and confidence δ, we have that with probability at least 1´δ, it will run
i i
for at most "
ϵ
max min16N s.t. inf maxϕJpNΛ`Λ `Λ q´1ϕď ,
N ΛPΩ ϕPΦ * 0 off 6
polypβ,d,H,M,log1{δq
.
ϵ4{5
ř
episodes, and will return data tϕ uN with covariance Σp “ N ϕ ϕJ such that
τ τ“1 N τ“1 τ τ
´ ¯
f
N´1Σp
ďNϵ
ˆi N
p
where i is the iteration on which OPTCOV terminates.
We use this to obtain a modified guarantee for OPTCOV that does not require a call to the CONDI-
TIONEDCOV algorithm of Wagenmaker and Jamieson (2023).
Lemma 16 (Modified Bound on OPTCOV, Theorem 4, Wagenmaker and Pacchiano (2023)). Consider
running OPTCOV with some ϵ ą 0 and functions f as defined in Lemma 15, instantiated with the
exp i
regularization λ¯ ě0. Then with probability 1´δ, this procedure will collect at most
# +
` ˘
max minC¨N s.t. inf maxϕJ NpΛ`λ¯Iq`Λ ´1 ϕď
ϵ exp,polypd,H,c onpX onq,log1{δq
N ΛPΩ ϕPΦ off 6 ϵ4{5
exp
p
episodes, and will produce covariates Σ such that
´ ¯
maxϕ Σp `λ¯I`Λ ´1 ϕ ďϵ .
h off h exp
ϕhPΦ
32Proof. This is essentially the proof of Theorem 4 in Wagenmaker and Pacchiano (2023), except where we
chase around a few terms that differ in the analysis. By Lemma D.5 of Wagenmaker and Jamieson (2023),
it suffices to bound the smoothness constants of f pΛq by
i
´ ¯
1 2 η 1
L “ , β “ 1` i , M “ .
i λ¯2 i λ¯3 λ¯ i λ¯2
p
Assume that the termination condition of OPTCOV is met for i satisfying
ˆ ˆ ˙˙
p iďlog poly 1 ,d,H,log1{δ,c pX q,λ¯ .
ϵ on on
exp
We assume this holds and justify it at the conclusion of the proof. For notational convenience, define
ˆ ˙
1
ι:“poly log ,d,H,log1{δ,c pX q,λ¯ .
ϵ on on
exp
p
Given this upper bound on i, set
1
L“M :“ , β :“ι.
λ¯2
p
With this choice of L,M,β, we have L ďL,M ďM,β ďη β for all iďi.
i i i i
Now apply Lemma 15 with Λ “λ¯ ¨I and get that, with probability at least 1´δ, OPTCOV terminates
0
after at most "
` ˘
ϵ
max min16N s.t. inf maxϕJ NΛ`λ¯ ¨I`Λ ´1 ϕď exp
N ΛPΩ ϕPΦ o +ff 6
polypd,H,λ,c pX q,log1{ϵ ,log1{δq
off off exp
ϵ4{5
exp
ř
episodes, and returns data tϕ uN with covariance Σp “ N ϕ ϕJ such that
τ τ“1 τ“1 τ τ
´ ¯
f
N´1Σp
ďNϵ
ˆi exp
p
where i is the iteration on which OPTCOV terminates.
By Lemma D.1 of Wagenmaker and Jamieson (2023) we have
´ ¯ ´ ¯
N ¨maxϕ
Σp
`Λ `Λ
´1
ϕ ďf
N´1Σp
,
h ˆi,0 off h ˆi
ϕhPΦ
and the upper bound on the tolerance follows from Lemma D.8 of Wagenmaker and Jamieson (2023).
p
Itremainstojustifytheboundoni. WedosowiththesameargumentthatWagenmakerandPacchiano
(2023) use. Note that by the definition of OPTCOV, if we run for a total of N¯ episodes, we can bound
p i ď 1log pN¯ q. However, we see that the bound onp i given above upper bounds 1log pN¯ q for N¯ the upper
4 2 4 2
bound on the number of samples collected by OPTCOV stated above. Thus, the bound onˆi is valid.
G Miscellanous lemmas
Lemma 17. Let ΦĂRd be a linear subspace. Suppose tϕ h,nu
hPrHs,nPrNs
PΦ be a collection of unřit vectors
and tσ u P R be a collection of positive real numbers with mean σ¯ “ pNHq´1 σ .
h,n hPrHs,nPrNs ` h,n h,n
Suppose it holds that max max pϕTΛ´1ϕ qďγ, then the following result satisfies
hPrHs ϕhPΦ h h h
ÿH b ?
max ϕTΣ´1ϕ ÀγH Nσ¯,
h h h
h“1ϕhPΦ
with
ÿN ÿN ϕ ϕT
Λ “ ϕ ϕT `λI , Σ “ h,n h,n `λI .
h h,n h,n d h σ d
n“1 n“1 h,n
33ř
Proof. First, we denote σ¯ “ N´1 σ . Informally, this implies that most individuals of σ is asymp-
h n h,n h,¨
totically on the order of σ¯ , with only a small amount of individuals being higher in order. To rule out the
h
effect of the “large” ones, we group them into the following collection of sets:
E pC q“tnPrNs:σ ěC σ¯ u.
h h h,n h h
Here, we leave the choice of the truncation level C
h
open for now, but note that we allowřthe truncation
levels C vary across different timesteps h and related to σ¯ . It follows by definition that H σ¯ “ Hσ¯.
h h h“1 h
From an application of Markov’s Inequality, the cardinality of set E pC q can be upper bounded as
h h
N
|E pC q|ď .
h h C
h
WenowchoosethetruncationlevelC . Todoso,wefollowthestepsbelowtoquantifytheeffectinduced
h
by the trajectories with high variance (i.e. those that belong to E pC q):
h h
˜ ¸
ÿN ϕ ϕT
minϕJΣ‹ϕ ě minϕJ h,n h,n ϕ
ϕhPΦ h h h ϕhPΦ h ˆn“1 σ h,n h
˙
ÿ ϕ ϕT
ě minϕJ h,n h,n ϕ
ϕhPΦ h
nPrNˆszEhpChq
σ h,n h
˙
ÿ
1
ě minϕJ ϕ ϕT ϕ .
C hσ¯ h ϕhPΦ h
nPrNszEhpChq
h,n h,n h
We now utilize a basic matrix inequality that for any matrix A,B, we have
minϕJAϕ ě minϕJpA`Bqϕ ´maxϕJBϕ ,
h h h h h h
ϕhPΦ ϕhPΦ ϕhPΦ
which allows us to further bound min ϕJΣ‹ϕ as
ϕhPΦ h h h
ˆ ˙
1
ÿN
minϕJΣ‹ϕ ě minϕJ ϕ ϕT `λI ϕ
ϕhPΦ h h h C hσ¯ h ϕhPΦ h n“ˆ1 h,n h,n d h ˙
ÿ
1
´ maxϕJ ϕ ϕT `λI ϕ
C ˆhσ¯ h ϕhPΦ h
nP˙EhpChq
h,n h,n d h
1 N
Á γ´1´ ´λ ,
C σ¯ C
h h h
This leads to the following result:
" ˆ ˙*
c pX q d ´1
minϕJΛ ϕ “ minpϕJΛ´1ϕ q´1 Á max off off , on “γ´1,
ϕhPΦ h h h ϕhPΦ h h h N off N on
where the first equality holds because Λ is a linear transformation on the subspace Φ. Equivalently, this
h
holdsfromthevariationalcharacterizationoftheeigenvaluesandthefactthatthelargestabsoluteeigenvalue
is equal to the inverse of the smallest absolute eigenvalue of the inverse. As a result, in order to rule out
the effect of the “high variance trajectories”, we select the truncation level δ such that N{C “ Θpγ´1q,
h h
implying C “ΘpNγq. Hence, we obtain the following lower bound:
h
1
minϕJΣ‹ϕ Á .
ϕhPΦ h h h γ2Nσ¯ h
Finally, we note that
ÿH b ÿH ˆ b ˙ ´1 ? ÿH ? ?
max ϕJΣ‹´1ϕ “ min ϕJΣ‹ϕ Àγ N σ¯ ďγH Nσ¯.
h h h h h h h
h“1ϕhPΦ
h“1
ϕhPΦ
h“1
34Lemma 18 (Modified Lemma B.1 from Zhou and Gu (2022)). Let X ,X be a partition of SˆAˆrHs,
off on
such that their images under the feature map, Φ ,Φ are subspaces of dimension d ,d respectively. Let
off on off on
tσ ,β u be a sequence of non-negative numbers, α,γ ą0,tx u ĂRd and }x } ďL. Let tZ u and
k k kě1 k kě1 k 2 k kě1
tσ¯ u berecursivelydefinedasfollows: Z “λI`Z forsomesymmetricmatrixZ ,whereN “N `K,
k kě1 1 off off off
and we have ! )
@k ě1,σ¯ “max σ ,α,γ}x }1{2 ,Z “Z `1 x xJ{σ¯2
k k k z´1 k`1 k Xon k k k
` ` ˘˘ k
Let ι“log 1`NL2{ dλα2 . Then we have
g
f
ÿK ! ) a fÿK
e
min 1,β }x } 1 ď2d ι`2 max β γ2d ι`2 d ι β2pσ2`α2q.
k k z´ k1 Xon on
kPrKs
k on on k k
k“1 k“1
Proof. The proof roughly follows that of Lemma B.1 in Zhou and Gu (2022), except that we have to make
modifications as necessary to tighten the dimension dependence to d and incorporate the offline data.
on
Decompose the set rKs into a union of two disjoint subsets rKs“I YI ,
1 2
! )
I “ k PrKs:}x {σ¯ } 1 ě1 ,I “rKszI .
1 k k Z´1 Xon 2 1
k
Then the following upper bound of |I | holds, where the projector P onto Φ has the decomposition
1 on on
P “U UJ by the thin SVD, and we write u “UJx :
on on on k on k
ÿ ! )
|I |“ min 1,||x {σ¯ ||2 1
1 k k Z´1 Xon
k
kPI1
ÿK ! )
ď min 1,||x {σ¯ ||2 1
k k Z´1 Xon
k
k“1
ÿK ␣ (
ď min 1,σ¯´2xJZ´1x 1
k k k k Xon
k“1
ÿK ␣ (
“ min 1,pU UJx qJZ´1pU UJx q1
on on k k on on k Xon
k“1
ÿK ␣ (
“ min 1,xJU UJZ´1U UJx 1
k on on k on on k Xon
k“1 $ ,
˜ ¸
ÿK & ÿk ´1 .
“ min 1,xJU UJ 1 σ¯´2x xJ`λI U UJx 1 .
% k on on Xon n n n d on on k Xon-
k“1 n“1
By Lemma 20, we can take the U inside the inverse and conclude that
on
$ ,
˜ ¸
ÿK & ÿk ´1 .
min 1,xJU UJ 1 σ¯´2x xJ`λI U UJx 1
% k on on Xon n n n d on on k Xon-
k“1 n“1
$ ,
˜ ¸
ÿK & ÿk ´1 .
“ min 1,xJU 1 σ¯´2UJx xJU `λI UJx 1 .
% k on Xon n on n n on don on k Xon-
k“1 n“1
Intuitively, this is because all the 1 UJx and 1 x are both in Φ , and in that case the projection is
Xon on n Xon n on
just the identity.
Writingu “UJx ,andinvokingLemmaD.5ofZhouandGu(2022)(whichisarestatementofLemma
n on n
11 of Abbasi-yadkori et al. (2011)) and the fact that }x {σ¯ } ďL{α, it holds that
k k 2
$ ,
˜ ¸
ÿK & ÿk ´1 .
min 1,xJU 1 σ¯´2UJx xJU `λI UJx 1
% k on Xon on n n on don on k Xon-
k“1 n“1
35$ ,
˜ ¸
ÿK & ÿk ´1 .
min 1,uJ 1 σ¯´2u uJ`λI u 1
% k Xon n n don k Xon-
k“1 n“1
ď2d ι,
on
as desired, and conclude that |I |ď2d ι.
1 on
The rest of the proof follows Zhou and Gu (2022) more closely. By the same argument that Zhou and
Gu (2022) use,
ÿ ! ) ÿ
min 1,β }x } 1 ď2d ι` β σ¯ }x {σ¯ } 1 .
k k z´1 Xon on k k k k z´1 Xon
k k
kPrKs kPI2
Decompose I “J YJ , where
2 1 2
! b )
J “tk PI :σ¯ “σ Yσ¯ “αu,J “ k PI :σ¯ “γ }x } 1 .
1 2 k k k 2 2 k k z´1 Xon
k
Similar to Zhou and Gu (2022),
ÿ ÿ ! )
β σ¯ }x {σ¯ } 1 ď β pσ `αq1 min 1,}x {σ¯ } 1
k k k k z´1 Xon k k Xon k k z´1 Xon
k k
kPJ1 kPJ1
ÿK ! )
ď β pσ `αqmin 1,}x {σ¯ } 1
k k k k z´1 Xon
k
gk“1 g
f f
f
e
ÿK f eÿK ! )
2
ď 2 pσ2`α2qβ2 min 1,}x {σ¯ } 1
k k k k z´1 Xon
k
gk“1 k“1
f
fÿK a
e
ď2 β2pσ2`α2q d ι,
k k on
k“1
and as for k PJ we have that σ¯ “γ2}x {σ¯ } 1 ,
2 k k k Z´1 Xon
k
ÿ ÿ
β σ¯ }x {σ¯ } 1 “γ2¨ β }x {σ¯ }2 1
k k k k Z´1 Xon k k k Z´1 Xon
k k
kPJ2 kPJ1
ÿK ! )
“γ2¨ β min 1,}x {σ¯ }2 1 ď2 max β γ2d ι.
k k k Z´ k1 Xon
kPrKs
k on
k“1
Therefore,
g
f
ÿK ! ) a fÿK
e
min 1,β }x } 1 ď2d ι`2 max β γ2d ι`2 d ι β2pσ2`α2q.
k k z´ k1 Xon on
kPrKs
k on on k k
k“1 k“1
Lemma 19 (Modified Version of Theorem 4.3, Zhou and Gu (2022)). Let tG uN be a filtration, and
n n“1
tx ,η uN be a stochastic process such that x PRd is G -measurable and η PR is G -measurable. Let
n n n“1 n n n n`1
L,σ,λ,ϵą0,µ˚ PRd. Arrangethedatapointsfromtheofflineandonlinesamplesasfollows,1,...,N ,N `
off off
1,...,N `N . For n“1,...,N, let y “xµ˚,x y`η and suppose that η ,x also satisfy
off on n n n n n
“ ‰
Erη |G s“0,E η2 |G ďσ2,|η |ďR,}x } ďL.
n n n n n n 2
ř ř
For n“1,...,N, let Z “λI` n x xJ,b “ n y x ,µ “Z´1b , and
n i“1 i i n i“1 i i n n n
a
β “12 σ2dlogp1`nL2{pdλqqlogp32plogpR{ϵq`1qn2{δq
n
36` ˘ ! ! ))
`24log 32plogpR{ϵq`1qn2{δ max |η |min 1,}x }
i i z´1
` ˘ 1ďiďn i´1
`6log 32plogpR{ϵq`1qn2{δ ϵ.
Then, for any 0ăδ ă1, we have with probability at least 1´δ that,
› ›
› ›ÿn › › ?
@n“1,...,N,› x η › ďβ ,}µ ´µ˚} ďβ ` λ}µ˚}
› i i› n n zn n 2
i“1 zn´1
Proof. TheproofismerelyasmallwrapperoverTheorem4.3ofZhouandGu(2022),whereweadaptthisto
our setting in the same way that Tan and Xu (2024) do in Lemma 1 of their paper. That is, we pre-append
the offline data to the online data, and generate the Z ,b ,µ ,β accordingly.
n n n n
As in Lemma 1 of Tan and Xu (2024), let N “ N `N . Order the N offline episodes arbitrarily,
off on off
to form episodes 1,...,N , and then begin the online episodes from episode N `1,...,N. Then, we can
off off
directly apply Theorem 4.3 of Zhou and Gu (2022) to recover the desired result.
Lemma 20. Suppose that W “Rm and V “Rn, where năm. Let U :W ÞÑV be a linear transformation
and that S “pUJUqW. As v,v ,...,v PS, we have
1 n
´ ÿk ¯
´1
´ ÿk ¯
´1
vJUJU v vJ`λI UJUv “vJUJ Uv vJUJ`λI Uv
i i m i i n
j“1 j“1
Proof. For projection matrix U, there exists orthogonal matrix Q P Rmˆm and diagonal matrix D “
pI ,0 q such that U “ DQ. We further define u “ Uv, v˜ “ Qv, u “ Uv and v˜ “ Qv for
n nˆpm´nq i i i i
1ďiďn. Then, we note that as v PS, we have v “UJUv “QJΛQv, where Λ“diagpI ,0 q, which
n m´n
is equivalent to v˜“Λv˜. As a result, we may conclude that v˜J “puJ,0 q.
m´n
Therefore, with a direct calculation, one will see that
´ ÿk ¯
´1
´ ÿk ¯
´1
v vJ`λI “ QJv˜ v˜JQ`λI
i i m i i m
j“1 j“1
´ ÿk ¯
´1
“QJ v˜ v˜J`λI Q
i i m
j“1
ˆř ˙
k u uJ`λI 0 ´1
“QJ i“1 i i n Q
0 λI
˜´ ¯
m´n
¸
ř ´1
k u uJ`λI 0
“QJ j“1 i i n Q.
0 λ´1I
m´n
This will establish our desired conclusion
˜´ ¯ ¸
´ ÿk ¯
´1
ř
k u uJ`λI
´1
0
LHS“vJ v vJ`λI v “v˜J j“1 i i n v˜
i i m
0 λ´1I
j“1 m´n
´ ÿk ¯
´1
“uJ u uJ`λI u“RHS.
i i n
j“1
37