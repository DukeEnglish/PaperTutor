Emergence in Multi-Agent Systems:
A Safety Perspective
Philipp Altmann1, Julian Sch¨onberger1, Steffen Illium1, Maximilian Zorn1,
Fabian Ritz1, Tom Haider2, Simon Burton3, and Thomas Gabor1
1 LMU Munich, Germany
2 Fraunhofer Institute for Cognitive Systems, Germany
3 University of York, United Kingdom
philipp.altmann@ifi.lmu.de
Abstract. Emergent effects can arise in multi-agent systems (MAS)
whereexecutionisdecentralizedandreliantonlocalinformation.These
effectsmayrangefromminordeviationsinbehaviortocatastrophicsys-
tem failures. To formally define these effects, we identify misalignments
betweentheglobalinherentspecification(thetruespecification)andits
localapproximation(suchastheconfigurationofdifferentrewardcompo-
nentsorobservations).Usingestablishedsafetyterminology,wedevelopa
frameworktounderstandtheseemergenteffects.Toshowcasetheresult-
ing implications, we use two broadly configurable exemplary gridworld
scenarios, where insufficient specification leads to unintended behavior
deviationswhenderivedindependently.Recognizingthataglobaladap-
tationmightnotalwaysbefeasible,weproposeadjustingtheunderlying
parameterizations to mitigate these issues, thereby improving the sys-
tem’s alignment and reducing the risk of emergent failures.
Keywords: Emergence · Multi-Agent Systems · AI Safety.
1 Introduction
Artificial intelligence (AI) is becoming prevalent in many practical applications,
increasing the chance of multiple AI-based components interacting in the field,
even when not designed or trained in conjunction. We refer to such applications
asAIFusion.TheseAIcomponentsaretreatedsimilarlytoextensivetraditional
softwaresystemsregardingtheirinteractions.However,astheyareprimarilyap-
pliedincomplexscenariosrequiringanintelligentsolution,thisinteractionmight
alsocauseunpredictableandsurprisingoutcomes.4 Admittedly,tosomeextent,
many of the surprises generated by AI originate from ill-defined goal functions
that an AI adheres to more strictly than intended [18]. In these cases, a human
specification has a clear intent, and we would usually expect a human developer
4 In practice, AI components might be prone to solving simple tasks in exceedingly
complex ways, which is directly associated with their perceived creativity, causing
unconventional solutions, surprising to the user [19].
4202
guA
8
]AM.sc[
1v41540.8042:viXra2 Altmann et al.
to follow this intent when a specification’s formulaic description becomes un-
clear, thus counteracting incomplete specifications. From a software engineering
perspective, employing AI entails living with oversight: Whenever we can fully
specify the ideal behavior we want from a component, we can use that speci-
fication as a program without worrying about learning the intended behavior.
Whenever we use AI, we want to look for solutions that we did not anticipate
in their entirety. However, this also implies that AI might learn to exploit the
blind spots in our specifications.
Yet,whileadaptingtoavalidationtaskisfeasibleforaligningthelocalspecifica-
tion(i.e.,thecomponent’sdetachedbehavior),thingsbecomemoreelusivewhen
combining several AI components into a global system. Behavior that does not
violateourspecificationmightstilldifferfromanotherAIcomponent’sexpecta-
tion (i.e., its learned priors). Thus, when two specification blind spots overlap,
the regarded components might interact in new ways. This phenomenon where
“more is different” [3] is generally referred to as emergence. From a software
standpoint, however, different might not always be preferred. Even though this
global behavior might still fulfill its intended task, it might also cause minor
defects or catastrophic failure.
In this work, we concentrate on multi-agent systems (MAS) due to their rig-
orous formalization to further investigate those effects in collective systems. As
a running example, we use simple navigation tasks in two different gridworld
environments that could be extrapolated to smart factories or large warehouse
applications, where the safe and predictable operability of a collective system
is inevitable for the successful application. Overall, we provide the following
contributions:
– Weintroduceaformalmodelfortracingtheemergentbehaviortoamisalign-
ment between the intended global specification and its local approximation.
– Weprovidetwosimpleenvironmentswheresuchemergentbehaviorispreva-
lent in both planning and learning to find the optimal behavior.
– We empirically show that by viewing the specified system as a surrogate of
theintendedtarget,itsadaptationimprovesitsalignmentandhelpsmitigate
suchpotentiallycatastrophicemergenteffectstoimprovethesystem’ssafety.
2 Preliminaries
Emergence Generally,emergencedenotesaphenomenonwherehigher-levelen-
tities, patterns, and regularities arise through interactions among small or sim-
plerentitiesthatthemselvesdonotexhibitsuchproperties[3].Honderichfurther
defines emergent properties as unpredictable and irreducible so that an (emer-
gent) property of a complex system arises out of the properties and relations
characterizing its simpler constituents, while it is neither predictable from nor
reducible to these lower-level characteristics [16]. Yet, understanding emergence
has theoretical implications for how we model complex systems in science and
engineering. We, therefore, aim to provide a formal model for tracing the originEmergence in Multi-Agent Systems: A Safety Perspective 3
of emergence in multi-agent systems. Overall, models incorporating emergent
properties can provide better management strategies for dealing with complex,
adaptivesystems,buttheymightalsoresultinsafety-criticaldeviation.Accord-
ing to Fromm [9], emergence can be further divided into four gradual types:
I. Simple/Nominal Emergence: Comprises intentional and unintentional
emergence with only feed-forward relations and no top-down feedback.
II. Weak Emergence: Includes simple positive or negative feedback.
III. Multiple Emergence: Comprises more complex situations and adaptive
systems with many kinds of feedback.
IV. Strong Emergence: Arises in multiple levels from vast amounts of state
variety (due to combinatorial explosion).
ExamplesfortypeI emergenceincludetheintentionalfunctionofasoftwaresys-
tem emerging from its code or thermodynamic properties like pressure, volume,
and temperature. Also, allowing for top-down feedback, type II might manifest
in stable positive effects, like the locally coordinated foraging behavior of ants
colonies or the self-organization of cells to form tissues. On the other hand, un-
stable effects, i.e., negative effects arising from positive feedback, might cause
bubbles and crashes in stock and other financial markets. Comprising compara-
tivelysimpleeffects,typesIandIIaresaidtobe(inprinciple)predictable.Type
III,ontheotherhand,includesmultiplekindsoffeedback,suchasemergentco-
operation in social games like the prisoner’s dilemma or adaptive systems like
catastrophes in natural systems. Type IV includes major evolutionary transi-
tions, like the emergence of life or the emergence of culture. Generally, this type
occursveryrarelyandattheboundariesofevolutionarysystems.Withinmulti-
agent systems, we mostly consider types II and III, where we focus on type III
fortheremainderofthisworkduetoitsnon-predictable,multi-feedbacknature.
Formally, we consider a system D comprised of N autonomous agents i, where
thebehaviorofeachisdescribedbyitspolicyπ ,suchthatthecollectivebehav-
i
ior, or, joint policy π, can be represented by:
π =⟨π ⟩⊗ =π ⊗···⊗π , (1)
i i∈D 1 N
where⊗istheoperatorforcombiningcomponentstosystems(cf.[39])5.Further-
more, we assume a specification F, implemented as a boolean predicate, where
F(D)=⊤iffallbehaviorsexhibitedbythesystemD adheretothespecification
F, and, F(D)=⊥ otherwise. Note that specifications do not need to define full
action sequences but might include very broad behavior ranges. Consequently,
an emergent property of system D is defined as:
F(π)̸=F(π )∧···∧F(π ) (2)
1 N
Thisimpliesthatthecollectivebehaviorπisnotmerelyasumorstraightforward
functionofitselementsbutistheresultofagentinteractions.Consequently,the
emergence property cannot be derived (or expected) solely from the properties
of the individual policies π but arises from their complex interplay.
i
5 Notethatinsteadoftreatingtheenvironmentasadifferententity,wemightaswell
model the environment as another agent in this formalism.4 Altmann et al.
Multi-Agent Systems To further formalize our scenario, we use a Markov
decision process ⟨S,µ,A,T,R,γ⟩ [27] with a set S of states s at timestep t,
t
with an initial state s ∼ µ, a set A of actions a , the transition probability
0 t
T(s |s ,a ), a reward r =R(s ,a )∈R, and a discount factor γ to calculate
t+1 t t t t t
the discounted return
∞
(cid:88)
G = γkr , (3)
t t+k
k=0
where we consider the reward combining multiple objectives O (e.g., given by
a previously defined specification), represented by boolean predicates, weighted
by a vector P ∈ R|O| such that the reward can be expressed as R = POT [30].
Furthermore, we consider a multi-agent system (MAS) formalized by a Markov
gameM=⟨D,S,Z,Ω,J,T,R⟩,withasetD ofN agentsi,alocalobservation
function Ω : S → ZN to retrieve a tuple of local observations ⟨z ⟩ , a set
i i∈D
J =AN ofjointactions⟨a ⟩ ,andthejointrewardfunction⟨R(s ,a )⟩ .
t,i i∈D t t,i i∈D
The goal of each self-interested agent i is to find an optimal strategy π∗ : Z →
i
A that maximizes the expected individual discounted return. From an agent’s
perspective, other agents are part of its environment, and policy updates by
other agents affect the performance of an agent’s own policy. The performance
of policy π of agent i is estimated using a value function V (s ) = E [G |s ]
i i t π t,i t
for all s ∈ S, based on the joint policy π (cf. [22]). Following our previous
t
considerationofthiscollectivebehavior,weassumeasystemdesignerwilldefine
M according to the intended specification.
Safety The standard ISO 21448 defines the Safety Of The Intended Function-
ality (SOTIF) as “the absence of unreasonable risk due to a hazard caused by
functionalinsufficiencies”[17].Thesefunctional insufficiencies canbecausedby
insufficiencies of the specification (e.g., the reward functions of the individual
components) as well as performance insufficiencies (behavior deviations of each
of the components from the intent) [5]. We consider this insufficient specifica-
tion as a mismatch between the local specification and the global (intended)
specification, which, as we argue, causes the diverging emergent behavior. In a
collective system, which can be defined as an interconnected collection of com-
ponents interacting in an environment to achieve a common target, this cause
might also be noticeable as a performance insufficiency, where the learned be-
haviordoesnotfulfillthelocalspecification.Inanycase,however,suchemerging
behavioral deviations violate the above prerequisites for SOTIF. Hendrycks et
al. [15] explicitly consider the safety of machine learning systems and propose
theunsolvedproblemsofrobustness,monitoring,systemic safety,andalignment
regardingwithstanding,identifying,andreducinghazards,aswellassteeringthe
fulfilled specification. Consequently, we consider the misalignment of a system,
caused by its insufficient specification, as the fault causing an error occurring
in the form of a deviation from the intended global specification, ultimately
exposing an emergent effect causing the failure of the system.Emergence in Multi-Agent Systems: A Safety Perspective 5
3 Emergence in Multi-Agent Systems
We assume that a system’s designer and/or user has a specific global behavior
in mind. We further assume that this desired behavior is accurately represented
in the specification F∗. From practical experience, however, we assume that the
individual agent policy π cannot be inferred directly from F∗. One of the most
i
prominent reasons is that agent interactions often have vastly complex effects,
sowecanonlysimulatetheoutcomeofjointactionsbutnotreverse-engineerthe
correctindividualactionstoreachagivenjointgoal.Furthermore,thetechnical
setup often limits agents to accessing local information. Thus, they might not
even have all the information required on a global scale to perform ideal actions
according to the global specification F∗.
We, therefore, assume that the designer defines the Markov game M accord-
ingly, usually by choosing suitable observation and action spaces as well as re-
ward parameterization P.6 Thus, we can only think of a global specification F∗
as the result of a system specification Fˆ that comprises entirely of individual
agentspecifications⟨Fˆ ,...,Fˆ ,⟩.Weacknowledgethatalthoughtheindividual
1 N
specificationsFˆ shouldbebasedonF∗ bywhoeverdefinesthem,theyprobably
i
willnotbeabletocaptureanarbitrarycomplexglobalspecificationF∗ entirely,
but only approximate its behaviors to some extent. Formally, we can naturally
define the global specification from the approximated individual specifications:
F∗ ⊢Fˆ(M|π ⊗ ... ⊗π )=Fˆ (M|π ) ∧ ... ∧ Fˆ (M|π ) (4)
1 N 1 1 N N
In most practical applications, this inaccuracy of local specifications is further
exacerbated by the fact that agents usually do not perform fully up to speci-
fication. Instead, policy π is also often derived from Fˆ by a complex process
i i
(ranging from a human programmer team to a reinforcement learning artificial
intelligence)and,whilethatprocessusuallyaims toensurethatFˆ(M|π )=⊤,
i i
we commonly end up with a policy π that can only achieve Fˆ(M|π ) ≈ ⊤,
i i i
meaning that i only fulfills Fˆ “most of the time” or “under certain conditions”.
i
Our theoretical argument shows that when we derive a joint policy π = π ⊗
1
···⊗π as it is commonly done7 from an originally global specification F∗,
N
we observe that we usually need to approximate twice: one time when splitting
theglobalspecificationintolocalonesandanothertimewhenconstructinglocal
policies for the local specification. The latter of these approximations is a com-
mon process inherent to most software development or machine learning. Still,
the former approximation is inherent to how multi-agent systems can currently
6 We exemplify this process in Section 4 with a simple gridworld navigation task.
7 Note that in most settings, the joint policy returns a tuple of the actions returned
by all individual policies. However, since in the general (partially observable) case
we also need to adjust the observations passed on to each individual policy from
the global state, we again use the ⊗ operator here and overload it to not only
handlecomponentcompositionbutalsostateinformationdecompositionandaction
composition, which are not inherently identical tasks but — as we argue — closely
related tasks nonetheless.6 Altmann et al.
behandled.Thisapproximationgivesrisetoemergenteffects:Evenifweassume
the original global concept to be perfect8, we might lose the ability to perfectly
representitduringthedecompositionintolocalspecifications,i.e.,specifications
at the single-agent level. Note that this multi-feedback characteristic can also be
connectedtothenatureoftypeIIIemergenceintroducedearlier.Specifically,the
instances (i.e., the patterns in agent behavior) where the system M no longer
fulfills the specification F∗ while fulfilling the approximated joint specification
Fˆ, which may happen because Fˆ ∧ ... ∧ Fˆ =Fˆ ̸=F∗, are called emergence.
1 n
Fig. 1 summarizes the above considerations. Overall, we can consider M itself
as an approximation of the intended specification F∗. Consequently, when devi-
ating behavior emerges from the chosen parameterization of M, its adaptation
might be able to improve the decomposition of the local specifications such that
the intended alignment Fˆ ≈F∗ is corrected and emergent effects are mitigated.
Level
Stage
Fig.1. Emergence in MAS: Generally, we assume a global task to be solved, given
by the target specification F∗ 1 . For individual behavior (typically in the form of
policies)tobederived 3 ,alocaltarget(reward,observation,...)needstobeformalized
2 (typically by defining M). Optimally, the resulting policy 4 perfectly fulfills this
previouslydefinedlocalspecification 2 .However,whenexecutedintheglobalcontext
5 , those policies might exhibit emergent effects 6 . By adding this new step 6 to
the MAS engineering cycle, we intend to discover emergent effects resulting from a
misalignment between the real target 1 and the resulting global behavior 5 . We
trace this effect to approximation errors induced throughout the overall development
process, mainly to a miss-parameterization of the target to be optimized 2 diverging
from the intended target inherent to 1 .
8 Notethattheperhapsevenmorecommonissuefordevelopinganysystemisthatwe
rarelyhaveaperfectspecification.Wethusrequireanevenearlierapproximationat
thisstep,i.e.,weapproximatethesystemwethinkwewantviathespecificationwe
can actually write down. However, the inaccuracies of this approximation are again
left to different subfields concerned about the whole variety of system design.
noitacificepS
noitavireD
noitucexE
lacoL labolGEmergence in Multi-Agent Systems: A Safety Perspective 7
4 Implementation
To illustrate our theoretical deliberations, we developed a framework explicitly
for studying emergent phenomena in multi-agent systems based on simple grid-
world environments to allow for in-depth visualization of the learned behavior9.
To provide a running example, we constructed two toy examples displayed in
Figures 2a and 2d. In both settings, two agents (blue) are tasked to fulfill a
specific goal by collecting targets (green). Therefore, the global specification F∗
is to collect their targets within as few steps as possible. In the left setting
(coin-quadrant), both agents need to cooperate to collect all targets. The sec-
ond setting (two-rooms) requires both agents to reach their opposing target by
passing a bottleneck between the two agents’ spawn points. We abstract the
two tasks into instances of a simple routing problem and use A = {↑,→,↓,←}
encompassing four movement directions in state S defined as a n×m grid with
cells∈[A1,A2,target,field,wall].Eventhoughthetasksareseeminglysimple
independently, we aim to introduce complexity through the interaction dynam-
ics between different entities and the entities themselves, with the specification
reinforced via the chosen configuration. To simulate insufficiencies of the cho-
sen specification causing emergent behavior (i.e., F∗ ̸= Fˆ), we allow extensive
and precise specification of the task at hand. While the state and action spaces
remain comparably simple to ease learning and the overall evaluation, those
specifications include the parameterization of a multi-objective reward signal
and the definition of a suitable observation function. Similarly, NetLogo allows
forsimulatingcomplexmulti-agentphenomena[38,35].Toprovidecompatibility
with existing RL algorithms, our environments use the gymnasium API[36].
(a) coin-quadrant (b) Emergent Chasing (c) Collective Behavior
(d) two-rooms (e) Emergent Blocking (f) Coordinated Behavior
Fig.2. Overview of our emergence evaluation environments
9 Refer to https://github.com/philippaltmann/EMAS for our full implementation.8 Altmann et al.
Parameterization The reward consists of two objectives: (1) taking as few
stepsaspossibleto(2)reachthetargetstate,implementedbytherewardparti-
cles for taking any step, and reaching a target, parameterized by P =(−1,50),
weighing the collection of targets higher than the step cost. Furthermore, we
define the state s to be observable by agent i via z =Ω (s ), with
t t,i i t
(cid:28) (cid:29)
Ω (s )= ρ(s ,i),argmin|ρ(s ,target)−ρ(s ,i)| ∈N4 (5)
i t t t t
target∈st
containing the current position of the concerned agent (ρ(s ,i) ∈ N2) and the
t
positionofitsimmediatetarget10,determinedbytheirManhattandistance(|·|).
Agent types We work with two types of agents. Planning agents implement a
greedy traveling salesperson (TSP) policy consisting of two steps. First, a com-
plete graph is created based on the deterministic transitions T for all states
s∈S and actions a∈A, with the edge weights corresponding to the respective
transitionreward.Then,agreedypolicyapproximatestheminimalHamiltonian
cycleontheconstructedgraph.Insteadoftravelingthewholecycle,executionis
usually terminated when all coins are collected or all targets are reached. Note
that while using the global state for deriving the policy, both TSP-agents are
derived locally, i.e., without observing the other agent. To represent a learning
approach, we use reinforcement learning (RL) agents based on the local obser-
vation O. Specifically, we use advantage actor-critic (A2C), a policy gradient
approach [23], where each policy and value approximation is represented by pa-
rameterized feed-forward neural networks updated using stochastic gradient ac-
cent on the expected return E (G ). Overall, we train the agents independently
π 0
andevaluatetheirinteractioninconjunction,similartoanindustrialapplication
with agents from different vendors.
Emergent Effects The chosen environment configurations are the source of
two unwanted emerging phenomena that lead to noticeable performance drops
and potentially safety-critical system failures regardless of the agent type. In
coin-quadrant, the left agent is placed so that the costs of the shortest path
to the upper left and lower right coin are equal. This will essentially lead to
a random choice. Problems occur when this agent decides to go to the right
coin first. Since the other agent is closer to the right coin, the agent will be
the first to arrive and collect it. The same will happen for the second and the
third coin, leading to a chasing behavior of the latter agent without it ever
collecting any coin (cf. Fig. 2b). This reduces the overall system performance
since the resources spent by the latter agent are wasted. Worse than inefficient
use of resources is the case in the two-rooms environment, where the emergent
behavior that the agents exhibit is both hindering performance and impeding
10 Notethat,partialobservability,besidesbeingacommonassumptioninmulti-agent
reinforcementlearning,hasbeenshowntoimprovetheagents’generalizationtoshift-
ingenvironments[1]andiscommonlyusedforcontinuousroboticcontroltasks[26].
Therefore, it could be considered a generally preferred implementation in practice.Emergence in Multi-Agent Systems: A Safety Perspective 9
the overall system functionality. Due to the agents being placed at the same
distance from the door, traveling the shortest path to their respective targets
will result in a simultaneous arrival at this bottleneck. In our case, since both
agents approach it at the same time, this leads to a blocking situation (cf.
Fig.2e)whereneitheragentcanpassthebottleneck,preventingthemfromever
reaching their designated target.
Remedial Adaptations Arguably, both effects generally emerge from the in-
sufficient specification of ignoring the agents’ interaction. However, given that
deriving the behavior based on global information might not always be feasible,
we aim to adapt our approximation of the target specification, defined by M.
In the following, we propose different approaches to address this misalignment
for both planning and RL-agents that will be further evaluated in Section 5.
As the TSP-agents’ behavior is solely based on the reward structure, we con-
sider problem resolution via adaptation of the chosen reward parameterization
(P := (−1,50)), applied to the reward particles (R = 1) for every step exe-
s
cuted, and R = 1 for reaching the intended goal or R = 0 otherwise, where
g g
R = P ·(R ,R )T. To integrate additional (potentially missing) information,
s g
we furthermore add the position of the agent (i.e., x∈[1,n],y ∈[1,m]) into the
parameterization P and scale the cost particle R . To prevent the emergence
c c
of the chasing behavior in the coins-quadrant environment, we suggest the fol-
lowing adapted parameterization, derived as a saddle point to resolve the cost
equilibrium:
P =(−0.55−0.05y,50), (6)
x,y
introducingalineargradientalongthey-axistothecostrewardparticle.Pushing
the left agent towards the upper coin finally results in the intended collective
behavior shown in Figure 2c. Arguably, this linear gradient would also resolve
other situations where an agent is equally distanced from two coins. However,
preventingtheemergenceoftheblocking behaviorturnsoutmorecomplicatedas
the agents fail to solve the intended task. This failure could again be attributed
to misalignment; in this case, missing information that not all steps might be
attributed to equal costs, especially in bottleneck states. To resolve this, we
introducethenon-linearcontortionofthecostparameterizationalongbothaxes
(cid:18) (cid:114) (cid:19)
5 x
P = 3 −1−1,50 , (7)
x,y y 4
causing non-blocking optimal paths of different lengths, resulting in the coordi-
nated behavior shown in Figure 2f. While we will show these adaptations to be
effectiveinpreventingtheemergenteffectsofTSP-agents,RL-agentsrequirea
differentkindofapproach.Wearguethatnotonlyaninsufficientrewardfunction
orcoststructurecanbethesourceofemergentbehaviorbutalsoimproperobser-
vations.Aspreviouslydescribed,ourobservationscontaininformationaboutthe
agent’s location and current target. Even though the Manhattan-based target
choice(cf.Eq.(5))approximatelyresemblesthegreedypolicyoftheTSP-agents,
thistargetselectionmechanismgivesrisetotheemergentchasing.Therefore,we10 Altmann et al.
proposetouseanadapteddistancemetric||·||toprioritizetargetstwhosepath
(P (t)accordingto[4])foragentidoesnotintersectanyotheragentj ∈{D\i}:
i
(cid:40)
|P (t)|+|P (j)| if ∃j ∈{D\i}:j ∈P (t)
||t−i||= i i i (8)
|P (t)| otherwise
i
Similarly,topreventtheblockingbehavior,weproposeaddinganauxiliarytar-
get(e.g.,at(1,1))iftheagentsencounteranequaltargetdistancetocircumvent
theircollision.Notethatreachingtheauxiliarytilemustbepartofthetraining.
5 Evaluation
Toverifythatthepresentedadaptationsforpreventingemergentbehaviorindeed
represent effective means for counteracting the experienced emergent behavior,
we evaluate the different approaches in the following.
Collective Gathering TheemergentbehaviordescribedinSection4isclearly
observable when considering the trajectories of the two agents in the coin-
quadrant environment.Figure3ashowsthebehaviorofthedifferentagenttypes
for one example setting. Both agents aim for the bottom right coin first, but
the left agent requires four additional steps to reach it. This time, Agent 2 is
able to collect the initial coin and, following a zigzag-shaped route, collects one
coinafteranotherwhileAgent1purposelesslyfollowsbehind.Consequently,the
total number of environmental steps required for collecting all coins is deter-
mined by the length of the route of Agent 2, which is 20 in this case. Figure 3b
visualizesthechangeinbehaviorforAgent1whenequippedwithouremergence
breachingmechanisms.Asexpected,thistime,Agent1travelsupwardsinstead.
In the case of the RL-agent, that’s because it detects Agent 2 on its path to
the right coin and, therefore, adds +4 to the estimated path costs (the distance
between Agent 1 to Agent 2), which changes the decision in favor of the top
left coin. For the TSP-agent, the route change is caused by our adaptation of
the underlying cost graph. Both solutions significantly affect the total number
of required environmental steps for collecting all coins. Now, the work is dis-
tributed, with Agent 1 collecting the two top coins while Agent 2 takes care of
the remaining three. To get an impression of the generalization abilities of the
mechanisms to unseen coin arrangements, we generated 20 settings where 2–12
coins are randomly placed on the quarter circle. In 15 of these settings, both
agent types showed emergent behavior. For the RL agent, we trained the model
onthe5-coinsetting(cf.Fig.3a)usingtenrandomseeds.Wenoticedthatoneof
the seeds did not converge properly, so we did not consider it in the evaluation.
Every trained model is then applied to the 20 generated coin settings, with and
without the emergence prevention mechanism. Figure 3c shows the distribution
of the required environment steps to gather all the coins across the different
settings, averaged over the different policies. For both the RL and TSP-agents,
we can see that usage of the prevention mechanisms reliably reduces the re-
quired step costs, thus effectively avoiding the additional expenses provoked byEmergence in Multi-Agent Systems: A Safety Perspective 11
20
18
16
14
12
10
Emergent Prevented Emergent Prevented
(RL) (RL) (TSP) (TSP)
Agent Types
(a) Chasing (b) Collection (c) Collected Coins
(randomized)
2
(d) Blocking 1
Emergent
Prevented (RL) + CI
Prevented (TSP) + CI
0
0 5 10 15 20 25 30
Avg. environment step
(f) Reached Flags
(e) Coordination
Fig.3. (a)and(d)showoneinstanceoftheemergentbehavior,whereastheroutesin
(b)and(e)resultfromtheusageofouremergencepreventionapproaches.Theblueand
purpletrajectoriescorrespondtothebehaviorofourRL-agents.Theyellowandorange
routesbelongtotheTSP-agents.(c)and(f)giveanoverviewofthedistributionofthe
requiredenvironmenttimestepstoreachthetargetsconsideringmultiplerandomseeds.
For Collection, we randomized the positions and the number of coins on the quarter
circle. For Coordination, we also plotted the confidence intervals (red whiskers).
the emergent behavior, with a mean reduction of at least four steps. The TSP
agent,onaverage,requiresfewerstepsthantheRLagent,whichalreadyapplies
to the emergent case.
Coordinated Navigation If we keep the emergent phenomenon in two-rooms
untreated,theagents’trajectoriesendatthebottleneck(cf.Fig.3d).Theresult
is a deadlock, where no targets are collected, and the program is interrupted
after the maximum number of environment steps (in our case, 30). The pro-
posed solution for the RL-setting forces a detour of one of the agents before
approaching the bottleneck. This guarantees that the lengths of the agent paths
aredifferent,shiftingthepointofcontactawayfromthebottleneck.TheleftRL-
agent gains two extra steps by traveling to its assigned auxiliary tile, followed
by a turnaround, before approaching the target. The agent with the extended
spets
tnemnorivne
.oN
sgalf
dehcaeR12 Altmann et al.
path determines the total environmental steps for the RL-agents. In our case,
this corresponds to a mean of 13 steps (cf. Fig. 3f). This exact behavior was
observed repeatedly with different policies trained on ten random seeds.11 For
theTSP-agents,thecontortionofthecostparameterizationleadstoalowercost
path maneuvering around the center of the right room, extending the required
numberofstepsforreachingthebottleneck.Thisadaptationgivestheleftagent
enough time to reach the other room, thus resolving the deadlock situation as
presented in Figure 3e. For the TSP-agents, the contortion affects both agents
equally, resulting in a mean of 16.8 steps to reach all flags (cf. Fig. 3f). We ob-
served a slight variance for the TSP-agents when approaching the second flag,
with a 95% confidence interval of ≈(16.06,17.54) steps.
Discussion Notethatforbothenvironments,thepresentedtrajectoriesforthe
TSP-agents are randomly selected from the set of routes with equivalent path
costs. In contrast, the RL-agents routes are deterministically predetermined by
choosingtherespectiveactionwiththehighestprobability.Thatis,forexample,
whytherouteforagent2inthecoin-quadrant environmentchangesfortheTSP-
agentwhileitremainsthesamefortheRL-agent.Consideringthedifferencesin
performance of the agent types across the two emergent phenomena, we argue
that the choice of the most suitable agent type and corresponding emergence
prevention mechanism is likely task-dependent. Also, the mechanisms are not
free of limitations. In the coin-quadrant environment, for example, both agents
might travel a path to the same coin, which intersects only immediately before
thetarget.Thus,theextracostsareconsideredverylate,whichcouldpotentially
result in an emergent situation once again. Yet, on average, all our approaches
provedtosubstantiallyreducethenumberofrequiredenvironmentstepstoreach
the targets compared to the scenarios where the emergence is left untreated.
Overall, our two toy examples showed how emergence can manifest in multi-
agent systems and how changes to its specification M can circumvent these
effects. For instance, in the case of the coin-quadrant environment, the target
was to collect all coins in as few environment steps as possible. The initial local
specification Fˆ for the TSP-agents specified a uniform cost for all transitions,
i
neglecting the positioning of the two agents and thus opening up the door for
emergent behavior to occur. Based on this observation, we could reiterate over
theMASengineeringcycle,acknowledgingthefactthatourcurrentsurrogatefor
theglobalspecificationappearedtobemisalignedwithourtarget,andintroduce
a gradient on the cost-function to account for the undesired agent interactions.
The renewed execution did confirm that this adaptation was, in fact, successful
in preventing the emergence. In the following, we survey related approaches
suitable for aligning the identified specification insufficiencies.
11 Of the initial ten random seeds, the training only converged for 6, indicating the
need to use more sophisticated training algorithms in the future.Emergence in Multi-Agent Systems: A Safety Perspective 13
6 Related Work
Alignment Ensuring that autonomous agents follow the rules of their environ-
ment is crucial. These rules are often part of the non-functional requirements,
which, along withfunctional requirements, outline how a systemshould behave.
The field of alignment deals with the challenges of ensuring that learning sys-
tems adhere to these rules. At a high level, AI Value Alignment is concerned
with aligning advanced AI systems, such as sophisticated autonomous agents,
withhumanvalues[31].Gabrielbreaksdownthechallengesintotwomaintypes:
technical and normative [10]. Technical challenges include figuring out how to
teachagentsvaluesorprinciplessotheyconsistentlymeettheirgoals.Aspecific
issue for more advanced agents is reward hacking, where agents find unexpected
and sometimes unwanted ways to reach their goals [2, 21, 20]. Here, advanced
refers to the agents outperforming humans at solving specific tasks. Normative
challengesareaboutdecidingwhichvaluesorprinciplestoteachagents.Gabriel
differentiatesbetweenminimalist approaches,whichaimtoavoiduncertainout-
comes by tying agents to a reasonable set of human values, and maximalist ap-
proaches, which strive to align agents with the best human values on a broader
societal or global level. We concentrate on the technical challenges and use the
term alignment in this narrower sense. The work of Hendrycks et al. [15], which
we already took on in Chapter 2, refers to alignment as one of the major unre-
solvedchallengesinapplyingmachinelearningmethods,wherethedifficultylies
in representing and optimizing complex human values. We agree with this.
AI Safety The technical side of alignment we focus on has also been explored
in the context of AI Safety, which focuses on preventing accidents and negative
impacts from using advanced autonomous systems. Amodei et al. outline five
key problems in this area [2]. While they don’t use the term alignment, later
works [10] and practical applications often cite these problems, especially nega-
tive side-effects and reward hacking [2, 21, 20], as examples of poor alignment.
Leike et al. introduce scenarios to test the safety features of agents, covering
both the issues identified by Amodei et al. and additional agent-specific areas
likeexploration[21].Theycategorizeproblemsintothoseofrobustness andspec-
ification: Robustness issues arise when agents can’t achieve their goals despite
knowing what they are, while specification issues occur when agents don’t fully
understand their goals, often because they can’t be fully captured through re-
wards. A typical method to address this involves refining the model’s outputs
using human feedback after the initial training [25]. For example, people might
ratethemodel’sresponses,creatingapreference-basedrewardmodelforfurther
training (see reinforcement learning from human feedback (RLHF) [6]). Lately,
RLHF has been most prominently used to align potentially unhelpful, toxic, or
incorrectlargelanguagemodelresponseswithhumanexpectations.Similarly,we
couldimagineusingRLHFtoidentifyandcorrectthelocalspecifications’“blind
spots”thatleadtounwantedemergentbehavior.Overall,weinterprettheprob-
lems identified by Amodei et al. [2] as root causes of poor alignment. Another
line of work deals with detecting anomalies in RL systems [33, 12, 11]. The goal14 Altmann et al.
is to recognize situations the agent has not encountered during training, which
potentially lead to safety violations or other failure modes at runtime. In our
example, anomaly detection could help uncover the fact that other agents are
present during execution, which were not modeled during training, indicating
underspecification of the system.
Reward shaping In sequential decision-making problems, goals are typically
represented by a reward function, where the mathematical objective is to maxi-
mize the cumulative expected reward [34]. Consequently, it makes sense to steer
the learning of RL-agents by adjusting the reward function. This is particularly
relevantinproblemswherefeedbackisinfrequent,forexample,becausethegoal
is initially difficult to achieve. In multi-agent systems, reward shaping can also
addressthecreditassignmentproblem,whichdenotestheestimationoftheindi-
vidual agents’ contributions to the global reward. For instance, Salimbeni et al.
derive individual rewards based on Kalman filtering [32], and Wolpert et al. use
the difference between the actual reward and an alternative reward that would
be given if the average action of all agents were chosen [40]. The alternative
reward indicates whether the action chosen by the agent is beneficial or detri-
mental to the overall system. Foerster et al. [8] further develop this approach
withinacentralizedtrainingwithdecentralizedexecution(CTDE)architecture.
Theycalculatethealternativerewardbyexcludingtheactionoftheagentunder
considerationandkeepingtheactionsoftheotheragentsunchanged,allowingfor
a more precise measurement of individual contributions. The approaches above
significantly change the rewards to encourage cooperation but may add unin-
tended side effects. An approach that neither changes the optimal strategy nor
causes side effects that could enable reward hacking in a single-agent RL sys-
tem is the potential-based reward shaping (PBRS) developed by Ng et al. [24].
PBRS supplements the reward at each timestep with the difference in poten-
tial between a state and its successor state. The potential of individual states is
usually defined in a separate function and contains expert knowledge about the
problem to be solved, such as approximately optimal heuristics. Similarly, we
propose adapting the reward signal based on expert knowledge about the origin
of the emergent effect when access to global information cannot be guaranteed.
Multi-Objective Optimization Functional and non-functional requirements
naturally induce a multi-objective (MO) problem, thus increasing the complex-
ity of specification-aware machine learning [13, 14]. As MO learning spans a
Pareto-front of rewards for individual policies to optimize, ‘mixed’ reward be-
haviors, i.e., locally Pareto-optimal strategies, quickly compound the potential
emergence in multi-agent systems. In a cooperative setting with homogeneous
agents, training with PBRS was shown to achieve compliance with functional
and non-functional requirements, ultimately leading to proactively safe runtime
behavior[29].However,themajorityofMOseemstorequireincreasinglyspecific
approaches such as Pareto Q-networks [37] or Pareto-conditioned networks [28].
SinceRLhasonlyrecentlybeguntocoverMOproblems(cf.Feltenetal.[7]),we
aimtofurtherthefieldwithourspecification-basedenvironmentandevaluation.Emergence in Multi-Agent Systems: A Safety Perspective 15
7 Conclusion
In summary, we formally addressed the critical issue of emergent behavior in
multi-agent systems (MAS) by introducing a process model to trace this de-
viation from the intended specification to a misalignment induced throughout
the approximative process. If not appropriately handled, those effects emerging
from the conjunct execution of independent models can cause severe behav-
ior and performance deviations and might even cause catastrophic failure. To
illustrate our formal approach, we introduced two simple gridworld examples
where insufficient specification causes emergent functional deviation and even
failure. We argue that this emergence in MAS is induced via approximation
errors throughout the modeling and learning process. A straightforward solu-
tion could, therefore, be the inclusion of missing information to improve said
alignment, e.g., via human feedback. However, as global information might not
always be a viable option in the applications of MAS, we propose alternative
adaptations to the system model (i.e., our approximation) to mitigate those be-
havioral deviations. This creates an iterative process where the definition of the
system itself is adapted to (safely) conform to the intended global specification.
In our exemplary tasks, these adaptations include the contortion of the reward
signal and the local observation. Experimental results validate our theoretical
elaborations and the effectiveness of those adaptations.
However, despite these promising results, the exemplary environments used in
our experiments are relatively simple and may not capture the full complexity
of real-world MAS applications. Additionally, the assumption that global speci-
fications are accurately known and can be effectively approximated at the local
level might not always hold in practical scenarios. Therefore, we encourage fu-
ture work to focus on the iterative process of integrating potentially incomplete
or ambiguous human specifications into the design of the system by connecting
our methodology to human-in-the-loop approaches like RLHF. Also, develop-
ing more sophisticated benchmarks that connect our formal considerations to
practicalsettings andreal-world applications wouldbe invaluablein broadening
our contributions to the development of safer, more reliable, and more efficient
collective adaptive systems.
To this end, we provide a structured approach to understanding and mitigating
emergentbehavior,openingnewavenuesforensuringthesafetyandreliabilityof
MASinincreasinglycomplexanddynamicenvironments.Ourworkhasimplica-
tions for a wide range of applications, from autonomous transportation systems
and smart cities to collaborative robotics and intelligent manufacturing. It lays
the groundwork for more resilient and trustworthy collective adaptive systems
that meet the demands of an increasingly interconnected world.
Acknowledgments
This work was funded by the Bavarian Ministry for Economic Affairs, Regional
Development and Energy as part of a project to support the thematic develop-
ment of the Institute for Cognitive Systems.Bibliography
[1] Altmann, P., Ritz, F., Feuchtinger, L., Nu¨ßlein, J., Linnhoff-Popien, C.,
Phan, T.: CROP: Towards distributional-shift robust reinforcement learn-
ing using compact reshaped observation processing. In: Elkind, E. (ed.)
Proceedings of the Thirty-Second International Joint Conference on Artifi-
cial Intelligence, IJCAI-23. pp. 3414–3422 (2023)
[2] Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., Man´e,
D.:ConcreteproblemsinAIsafety. arXivpreprintarXiv:1606.06565(2016)
[3] Anderson, P.W.: More is different. Science 177(4047), 393–396 (1972)
[4] Bresenham, J.E.: Algorithm for computer control of a digital plotter. In:
Seminal graphics: pioneering efforts that shaped the field, pp. 1–6. Associ-
ation for Computing Machinery (1998)
[5] Burton, S., Herd, B.: Addressing uncertainty in the safety assurance of
machine-learning. Frontiers in Computer Science 5, 1132580 (2023)
[6] Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.:
Deepreinforcementlearningfromhumanpreferences. In:AdvancesinNeu-
ralInformationProcessingSystems(NeurIPS).vol.30,p.4302–4310(2017)
[7] Felten, F., Alegre, L.N., Now´e, A., Bazzan, A.L.C., Talbi, E.G., Danoy, G.,
Silva, B.C.d.: A toolkit for reliable benchmarking and research in multi-
objectivereinforcementlearning. In:Proceedingsofthe37thConferenceon
Neural Information Processing Systems (NeurIPS 2023) (2023)
[8] Foerster,J.N.,Farquhar,G.,Afouras,T.,Nardelli,N.,Whiteson,S.:Coun-
terfactual multi-agent policy gradients. In: Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence. p. 2974–2982 (02 2018)
[9] Fromm, J.: Types and forms of emergence. arXiv preprint nlin/0506028
(2005)
[10] Gabriel, I.: Artificial intelligence, values, and alignment. Minds and Ma-
chines 30(3), 411–437 (09 2020)
[11] Haider,T.,Roscher,K.,Herd,B.,SchmoellerRoza,F.,Burton,S.:Canyou
trustyouragent?Theeffectofout-of-distributiondetectiononthesafetyof
reinforcementlearningsystems. In:Proceedingsofthe39thACM/SIGAPP
Symposium on Applied Computing. pp. 1569–1578 (2024)
[12] Haider, T., Roscher, K., Schmoeller da Roza, F., Gu¨nnemann, S.: Out-of-
distribution detection for reinforcement learning agents with probabilistic
dynamics models. In: Proceedings of the 2023 International Conference on
Autonomous Agents and Multiagent Systems. pp. 851–859 (2023)
[13] Hayes, C.F., R˘adulescu, R., Bargiacchi, E., K¨allstr¨om, J., Macfarlane, M.,
Reymond,M.,Verstraeten,T.,Zintgraf,L.M.,Dazeley,R.,Heintz,F.,etal.:
A practical guide to multi-objective reinforcement learning and planning.
Autonomous Agents and Multi-Agent Systems 36(1), 26 (2022)
[14] Hayes, C.F., R˘adulescu, R., Bargiacchi, E., Kallstrom, J., Macfarlane, M.,
Reymond,M.,Verstraeten,T.,Zintgraf,L.M.,Dazeley,R.,Heintz,F.,etal.:
A brief guide to multi-objective reinforcement learning and planning. In:Emergence in Multi-Agent Systems: A Safety Perspective 17
Proceedings of the 2023 International Conference on Autonomous Agents
and Multiagent Systems. pp. 1988–1990 (2023)
[15] Hendrycks,D.,Carlini,N.,Schulman,J.,Steinhardt,J.:Unsolvedproblems
in ML safety. arXiv preprint arXiv:2109.13916 (2021)
[16] Honderich, T.: The Oxford companion to philosophy. OUP Oxford (1995)
[17] ISO21448:2022(E): Road vehicles — safety of the intended functionality.
Standard, International Organization for Standardization (2022)
[18] Krakovna, V., Uesato, J., Mikulik, V., Rahtz, M., Everitt, T., Kumar, R.,
Kenton, Z., Leike, J., Legg, S.: Specification gaming: the flip side of AI
ingenuity. DeepMind Blog 3 (2020)
[19] Lehman, J., Clune, J., Misevic, D., Adami, C., Altenberg, L., Beaulieu, J.,
Bentley, P.J., Bernard, S., Beslon, G., Bryson, D.M., et al.: The surprising
creativity of digital evolution: A collection of anecdotes from the evolu-
tionary computation and artificial life research communities. Artificial life
26(2), 274–306 (2020)
[20] Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., Legg, S.: Scal-
able agent alignment via reward modeling: a research direction. preprint
arxiv:1811.07871 (2018)
[21] Leike,J.,Martic,M.,Krakovna,V.,Ortega,P.A.,Everitt,T.,Lefrancq,A.,
Orseau, L., Legg, S.: AI safety gridworlds. preprint arxiv:1711.0988 (2017)
[22] Littman, M.L.: Markov games as a framework for multi-agent reinforce-
mentlearning. In:Machinelearningproceedings1994,pp.157–163.Elsevier
(1994)
[23] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
Silver,D.,Kavukcuoglu,K.:Asynchronousmethodsfordeepreinforcement
learning. In: International conference on machine learning. pp. 1928–1937.
PMLR (2016)
[24] Ng, A., Harada, D., Russell, S.: Policy invariance under reward transfor-
mations: Theory and application to reward shaping. In: Proceedings of
the Sixteenth International Conference on Machine Learning (ICML). pp.
278–287 (1999)
[25] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P.,
Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kel-
ton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P.F.,
Leike, J., Lowe, R.: Training language models to follow instructions with
human feedback. In: Advances in Neural Information Processing Systems
(NeurIPS). vol. 35, pp. 27730–27744 (2022)
[26] Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell,
G., Schneider, J., Tobin, J., Chociej, M., Welinder, P., et al.: Multi-goal
reinforcement learning: Challenging robotics environments and request for
research. arXiv preprint arXiv:1802.09464 (2018)
[27] Puterman, M.L.: Markov decision processes. Handbooks in operations re-
search and management science 2, 331–434 (1990)
[28] Reymond,M.,Bargiacchi,E.,Now´e,A.:Paretoconditionednetworks.arXiv
preprint arXiv:2204.05036 (2022)18 Altmann et al.
[29] Ritz, F., Phan, T., Mu¨ller, R., Gabor, T., Sedlmeier, A., Zeller, M.,
Wieghardt,J.,Schmid,R.,Sauer,H.,Klein,C.,Linnhoff-Popien,C.:Speci-
fication aware multi-agent reinforcement learning. In: Agents and Artificial
Intelligence. pp. 3–21. Springer International Publishing (2022)
[30] Roijers, D.M., Vamplew, P., Whiteson, S., Dazeley, R.: A survey of multi-
objective sequential decision-making. Journal of Artificial Intelligence Re-
search 48, 67–113 (2013)
[31] Russell, S.: Human Compatible: Artificial Intelligence and the Problem of
Control. Allen Lane (2019)
[32] Salimibeni, M., Mohammadi, A., Malekzadeh, P., Plataniotis, K.N.: Multi-
agent reinforcement learning via adaptive Kalman temporal difference and
successor representation. Sensors 22(4) (2022)
[33] Sedlmeier, A., Gabor, T., Phan, T., Belzner, L., Linnhoff-Popien, C.:
Uncertainty-basedout-of-distributiondetectionindeepreinforcementlearn-
ing. arXiv preprint arXiv:1901.02219 (2019)
[34] Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT
press (2018)
[35] Tisue, S., Wilensky, U.: Netlogo: A simple environment for modeling com-
plexity. In:Internationalconferenceoncomplexsystems.vol.21,pp.16–21.
Citeseer (2004)
[36] Towers, M., Terry, J.K., Kwiatkowski, A., Balis, J.U., de Cola, G., Deleu,
T., Goul˜ao, M., Kallinteris, A., KG, A., Krimmel, M., Perez-Vicente, R.,
Pierr´e, A., Schulhoff, S., Tai, J.J., Tan, A.J.S., Younis, O.G.: Gymnasium
(2023), https://github.com/Farama-Foundation/Gymnasium
[37] Van Moffaert, K., Now´e, A.: Multi-objective reinforcement learning using
sets of pareto dominating policies. The Journal of Machine Learning Re-
search 15(1), 3483–3512 (2014)
[38] Wilensky, U., Rand, W.: An introduction to agent-based modeling: mod-
eling natural, social, and engineered complex systems with NetLogo. MIT
press (2015)
[39] Wirsing,M.,H¨olzl,M.,Tribastone,M.,Zambonelli,F.:ASCENS:Engineer-
ing autonomic service-component ensembles. In: International Symposium
on Formal Methods for Components and Objects. pp. 1–24 (2011)
[40] Wolpert, D.H., Tumer, K.: Optimal payoff functions for members of collec-
tives. Advances in Complex Systems 04(02n03) (2001)