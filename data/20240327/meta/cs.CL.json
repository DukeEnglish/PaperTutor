[
    {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "authors": "Rui PanXiang LiuShizhe DiaoRenjie PiJipeng ZhangChi HanTong Zhang",
        "links": "http://arxiv.org/abs/2403.17919v1",
        "entry_id": "http://arxiv.org/abs/2403.17919v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17919v1",
        "summary": "The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.",
        "updated": "2024-03-26 17:55:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17919v1"
    },
    {
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "authors": "Andrey GromovKushal TirumalaHassan ShapourianPaolo GloriosoDaniel A. Roberts",
        "links": "http://arxiv.org/abs/2403.17887v1",
        "entry_id": "http://arxiv.org/abs/2403.17887v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17887v1",
        "summary": "We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.",
        "updated": "2024-03-26 17:20:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17887v1"
    },
    {
        "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications",
        "authors": "Philip LippmannMatthijs SpaanJie Yang",
        "links": "http://arxiv.org/abs/2403.17860v1",
        "entry_id": "http://arxiv.org/abs/2403.17860v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17860v1",
        "summary": "Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.",
        "updated": "2024-03-26 16:49:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17860v1"
    },
    {
        "title": "ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages",
        "authors": "Bhawna PiryaniJamshid MozafariAdam Jatowt",
        "links": "http://arxiv.org/abs/2403.17859v1",
        "entry_id": "http://arxiv.org/abs/2403.17859v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17859v1",
        "summary": "Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale dataset with 485K question-answer pairs created based on the\nhistorical newspaper collection Chronicling America. Our dataset is constructed\nfrom a subset of the Chronicling America newspaper collection spanning 120\nyears. One of the significant challenges for utilizing digitized historical\nnewspaper collections is the low quality of OCR text. Therefore, to enable\nrealistic testing of QA models, our dataset can be used in three different\nways: answering questions from raw and noisy content, answering questions from\ncleaner, corrected version of the content, as well as answering questions from\nscanned images of newspaper pages. This and the fact that ChroniclingAmericaQA\nspans the longest time period among available QA datasets make it quite a\nunique and useful resource.",
        "updated": "2024-03-26 16:48:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17859v1"
    },
    {
        "title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs",
        "authors": "David R. MortensenValentina IzrailevitchYunze XiaoHinrich SchützeLeonie Weissweiler",
        "links": "http://arxiv.org/abs/2403.17856v1",
        "entry_id": "http://arxiv.org/abs/2403.17856v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17856v1",
        "summary": "Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.",
        "updated": "2024-03-26 16:45:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17856v1"
    }
]