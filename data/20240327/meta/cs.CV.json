[
    {
        "title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory",
        "authors": "Abdelrahman ShakerSyed Talal WasimMartin DanelljanSalman KhanMing-Hsuan YangFahad Shahbaz Khan",
        "links": "http://arxiv.org/abs/2403.17937v1",
        "entry_id": "http://arxiv.org/abs/2403.17937v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17937v1",
        "summary": "Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.",
        "updated": "2024-03-26 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17937v1"
    },
    {
        "title": "ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis",
        "authors": "Muhammad Hamza MughalRishabh DabralIkhsanul HabibieLucia DonatelliMarc HabermannChristian Theobalt",
        "links": "http://arxiv.org/abs/2403.17936v1",
        "entry_id": "http://arxiv.org/abs/2403.17936v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17936v1",
        "summary": "Gestures play a key role in human communication. Recent methods for co-speech\ngesture generation, while managing to generate beat-aligned motions, struggle\ngenerating gestures that are semantically aligned with the utterance. Compared\nto beat gestures that align naturally to the audio signal, semantically\ncoherent gestures require modeling the complex interactions between the\nlanguage and human motion, and can be controlled by focusing on certain words.\nTherefore, we present ConvoFusion, a diffusion-based approach for multi-modal\ngesture synthesis, which can not only generate gestures based on multi-modal\nspeech inputs, but can also facilitate controllability in gesture synthesis.\nOur method proposes two guidance objectives that allow the users to modulate\nthe impact of different conditioning modalities (e.g. audio vs text) as well as\nto choose certain words to be emphasized during gesturing. Our method is\nversatile in that it can be trained either for generating monologue gestures or\neven the conversational gestures. To further advance the research on\nmulti-party interactive gestures, the DnD Group Gesture dataset is released,\nwhich contains 6 hours of gesture data showing 5 people interacting with one\nanother. We compare our method with several recent works and demonstrate\neffectiveness of our method on a variety of tasks. We urge the reader to watch\nour supplementary video at our website.",
        "updated": "2024-03-26 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17936v1"
    },
    {
        "title": "OmniVid: A Generative Framework for Universal Video Understanding",
        "authors": "Junke WangDongdong ChenChong LuoBo HeLu YuanZuxuan WuYu-Gang Jiang",
        "links": "http://arxiv.org/abs/2403.17935v1",
        "entry_id": "http://arxiv.org/abs/2403.17935v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17935v1",
        "summary": "The core of video understanding tasks, such as recognition, captioning, and\ntracking, is to automatically detect objects or actions in a video and analyze\ntheir temporal evolution. Despite sharing a common goal, different tasks often\nrely on distinct model architectures and annotation formats. In contrast,\nnatural language processing benefits from a unified output space, i.e., text\nsequences, which simplifies the training of powerful foundational language\nmodels, such as GPT-3, with extensive training corpora. Inspired by this, we\nseek to unify the output space of video understanding tasks by using languages\nas labels and additionally introducing time and box tokens. In this way, a\nvariety of video tasks could be formulated as video-grounded token generation.\nThis enables us to address various types of video tasks, including\nclassification (such as action recognition), captioning (covering clip\ncaptioning, video question answering, and dense video captioning), and\nlocalization tasks (such as visual object tracking) within a fully shared\nencoder-decoder architecture, following a generative framework. Through\ncomprehensive experiments, we demonstrate such a simple and straightforward\nidea is quite effective and can achieve state-of-the-art or competitive results\non seven video benchmarks, providing a novel perspective for more universal\nvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.",
        "updated": "2024-03-26 17:59:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17935v1"
    },
    {
        "title": "AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation",
        "authors": "Qingping SunYanjun WangAiling ZengWanqi YinChen WeiWenjia WangHaiyi MeiChi Sing LeungZiwei LiuLei YangZhongang Cai",
        "links": "http://arxiv.org/abs/2403.17934v1",
        "entry_id": "http://arxiv.org/abs/2403.17934v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17934v1",
        "summary": "Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.",
        "updated": "2024-03-26 17:59:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17934v1"
    },
    {
        "title": "SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models",
        "authors": "Kashyap ChittaDaniel DaunerAndreas Geiger",
        "links": "http://arxiv.org/abs/2403.17933v1",
        "entry_id": "http://arxiv.org/abs/2403.17933v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17933v1",
        "summary": "SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.",
        "updated": "2024-03-26 17:58:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17933v1"
    }
]