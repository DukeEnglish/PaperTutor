[
    {
        "title": "SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models",
        "authors": "Kashyap ChittaDaniel DaunerAndreas Geiger",
        "links": "http://arxiv.org/abs/2403.17933v1",
        "entry_id": "http://arxiv.org/abs/2403.17933v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17933v1",
        "summary": "SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.",
        "updated": "2024-03-26 17:58:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17933v1"
    },
    {
        "title": "The Need for Speed: Pruning Transformers with One Recipe",
        "authors": "Samir KhakiKonstantinos N. Plataniotis",
        "links": "http://arxiv.org/abs/2403.17921v1",
        "entry_id": "http://arxiv.org/abs/2403.17921v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17921v1",
        "summary": "We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique\nfor $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework\nas a tool to increase the efficiency of pre-trained transformer architectures\n$\\textit{without requiring re-training}$. Recent works have explored improving\ntransformer efficiency, however often incur computationally expensive\nre-training procedures or depend on architecture-specific characteristics, thus\nimpeding practical wide-scale adoption. To address these shortcomings, the\nOPTIN framework leverages intermediate feature distillation, capturing the\nlong-range dependencies of model parameters (coined $\\textit{trajectory}$), to\nproduce state-of-the-art results on natural language, image classification,\ntransfer learning, and semantic segmentation tasks $\\textit{without\nre-training}$. Given a FLOP constraint, the OPTIN framework will compress the\nnetwork while maintaining competitive accuracy performance and improved\nthroughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP\nbaselines and a $0.5$% improvement from state-of-the-art methods on image\nclassification at competitive FLOPs reductions. We further demonstrate the\ngeneralization of tasks and architecture with comparative performance using\nMask2Former for semantic segmentation and cnn-style networks. OPTIN presents\none of the first one-shot efficient frameworks for compressing transformer\narchitectures that generalizes well across different class domains, in\nparticular: natural language and image-related tasks, without\n$\\textit{re-training}$.",
        "updated": "2024-03-26 17:55:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17921v1"
    },
    {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "authors": "Rui PanXiang LiuShizhe DiaoRenjie PiJipeng ZhangChi HanTong Zhang",
        "links": "http://arxiv.org/abs/2403.17919v1",
        "entry_id": "http://arxiv.org/abs/2403.17919v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17919v1",
        "summary": "The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.",
        "updated": "2024-03-26 17:55:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17919v1"
    },
    {
        "title": "CMP: Cooperative Motion Prediction with Multi-Agent Communication",
        "authors": "Zhuoyuan WuYuping WangHengbo MaZhaowei LiHang QiuJiachen Li",
        "links": "http://arxiv.org/abs/2403.17916v1",
        "entry_id": "http://arxiv.org/abs/2403.17916v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17916v1",
        "summary": "The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.",
        "updated": "2024-03-26 17:53:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17916v1"
    },
    {
        "title": "Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2",
        "authors": "Chen YiweiTang ChaoAghabiglou AmirChu Chung SanWiaux Yves",
        "links": "http://arxiv.org/abs/2403.17905v1",
        "entry_id": "http://arxiv.org/abs/2403.17905v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17905v1",
        "summary": "We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.",
        "updated": "2024-03-26 17:45:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17905v1"
    }
]