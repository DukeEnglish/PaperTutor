[
    {
        "title": "A Sociotechnical Framework For Addressing Stigma and Designing Personalized Digital Health Products",
        "authors": "Danielly de PaulaDaniel JuehlingFalk Uebernickel",
        "links": "http://arxiv.org/abs/2403.17843v1",
        "entry_id": "http://arxiv.org/abs/2403.17843v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17843v1",
        "summary": "Stigma, a recognized global barrier to effective disease management, impacts\nsocial interactions, resource access, and psychological well-being. In this\nstudy, we developed a patient-centered framework for deriving design\nrequirements and interventions for health conditions subject to social stigma.\nThis study introduces a patient-centered framework, grounded in sociotechnical\nsystems theory, to create tailored interventions and design requirements for\nhealth conditions influenced by social stigma. We tested this framework through\na mixed-method study on chronic pelvic pain patients. Our approach led to the\nidentification of ten design requirements that encompass behavioral and\npsychological support and strategies for day-to-day living. The findings reveal\na preference among CPP patients for priming and social support interventions.\nThis study underscores the value of a systems-based perspective in healthcare,\nadvocating for a nuanced, patient-centered approach that addresses the complex\nnature of health conditions affected by social stigma. It contributes to the\nongoing discourse on integrating STS theory into healthcare frameworks,\nhighlighting the need for targeted strategies to combat the complexities of\nstigma in patient care.",
        "updated": "2024-03-26 16:30:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17843v1"
    },
    {
        "title": "Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing",
        "authors": "Si ChenHaocong ChengJason SituDesirée KirstSuzy SuSaumya MalhotraLawrence AngraveQi WangYun Huang",
        "links": "http://arxiv.org/abs/2403.17807v1",
        "entry_id": "http://arxiv.org/abs/2403.17807v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17807v1",
        "summary": "Previous research underscored the potential of danmaku--a text-based\ncommenting feature on videos--in engaging hearing audiences. Yet, for many Deaf\nand hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes\nprecedence over English. To improve inclusivity, we introduce \"Signmaku,\" a new\ncommenting mechanism that uses ASL, serving as a sign language counterpart to\ndanmaku. Through a need-finding study (N=12) and a within-subject experiment\n(N=20), we evaluated three design styles: real human faces, cartoon-like\nfigures, and robotic representations. The results showed that cartoon-like\nsignmaku not only entertained but also encouraged participants to create and\nshare ASL comments, with fewer privacy concerns compared to the other designs.\nConversely, the robotic representations faced challenges in accurately\ndepicting hand movements and facial expressions, resulting in higher cognitive\ndemands on users. Signmaku featuring real human faces elicited the lowest\ncognitive load and was the most comprehensible among all three types. Our\nfindings offered novel design implications for leveraging generative AI to\ncreate signmaku comments, enriching co-learning experiences for DHH\nindividuals.",
        "updated": "2024-03-26 15:45:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17807v1"
    },
    {
        "title": "SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings",
        "authors": "Ting-Yao HsuChieh-Yang HuangShih-Hong HuangRyan RossiSungchul KimTong YuC. Lee GilesTing-Hao K. Huang",
        "links": "http://dx.doi.org/10.1145/3613905.3650738",
        "entry_id": "http://arxiv.org/abs/2403.17784v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17784v1",
        "summary": "Crafting effective captions for figures is important. Readers heavily depend\non these captions to grasp the figure's message. However, despite a\nwell-developed set of AI technologies for figures and captions, these have\nrarely been tested for usefulness in aiding caption writing. This paper\nintroduces SciCapenter, an interactive system that puts together cutting-edge\nAI technologies for scientific figure captions to aid caption composition.\nSciCapenter generates a variety of captions for each figure in a scholarly\narticle, providing scores and a comprehensive checklist to assess caption\nquality across multiple critical aspects, such as helpfulness, OCR mention, key\ntakeaways, and visual properties reference. Users can directly edit captions in\nSciCapenter, resubmit for revised evaluations, and iteratively refine them. A\nuser study with Ph.D. students indicates that SciCapenter significantly lowers\nthe cognitive load of caption writing. Participants' feedback further offers\nvaluable design insights for future systems aiming to enhance caption writing.",
        "updated": "2024-03-26 15:16:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17784v1"
    },
    {
        "title": "Exploring the Boundaries of Ambient Awareness in Twitter",
        "authors": "Pablo Sanchez-MartinSonja UtzIsabel Valera",
        "links": "http://arxiv.org/abs/2403.17776v1",
        "entry_id": "http://arxiv.org/abs/2403.17776v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17776v1",
        "summary": "Ambient awareness refers to the ability of social media users to obtain\nknowledge about who knows what (i.e., users' expertise) in their network, by\nsimply being exposed to other users' content (e.g, tweets on Twitter). Previous\nwork, based on user surveys, reveals that individuals self-report ambient\nawareness only for parts of their networks. However, it is unclear whether it\nis their limited cognitive capacity or the limited exposure to diagnostic\ntweets (i.e., online content) that prevents people from developing ambient\nawareness for their complete network. In this work, we focus on in-wall ambient\nawareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that\nallows us to explore to which extent IWAA is likely, or even possible. First,\nwe rely on reactions (e.g., likes), as strong evidence of users being aware of\nexperts in Twitter. Unfortunately, such strong evidence can be only measured\nfor active users, which represent the minority in the network. Thus to study\nthe boundaries of IWAA to a larger extent, in the second part of our analysis,\nwe instead focus on the passive exposure to content generated by other users --\nwhich we refer to as in-wall visibility. This analysis shows that (in line with\n\\citet{levordashka2016ambient}) only for a subset of users IWAA is plausible,\nwhile for the majority it is unlikely, if even possible, to develop IWAA. We\nhope that our methodology paves the way for the emergence of data-driven\napproaches for the study of ambient awareness.",
        "updated": "2024-03-26 15:09:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17776v1"
    },
    {
        "title": "FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts",
        "authors": "Kazuki KawamuraJun Rekimoto",
        "links": "http://dx.doi.org/10.1145/3652920.3652922",
        "entry_id": "http://arxiv.org/abs/2403.17727v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17727v1",
        "summary": "Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.",
        "updated": "2024-03-26 14:16:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17727v1"
    }
]