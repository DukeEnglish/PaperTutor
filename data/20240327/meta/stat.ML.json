[
    {
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "authors": "Andrey GromovKushal TirumalaHassan ShapourianPaolo GloriosoDaniel A. Roberts",
        "links": "http://arxiv.org/abs/2403.17887v1",
        "entry_id": "http://arxiv.org/abs/2403.17887v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17887v1",
        "summary": "We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.",
        "updated": "2024-03-26 17:20:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17887v1"
    },
    {
        "title": "Counterfactual Fairness through Transforming Data Orthogonal to Bias",
        "authors": "Shuyi ChenShixiang Zhu",
        "links": "http://arxiv.org/abs/2403.17852v1",
        "entry_id": "http://arxiv.org/abs/2403.17852v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17852v1",
        "summary": "Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. Nonetheless, these models can sometimes exhibit\nbiased decision-making, leading to disparities in treatment across different\ngroups. Despite the extensive research on fairness, the nuanced effects of\nmultivariate and continuous sensitive variables on decision-making outcomes\nremain insufficiently studied. We introduce a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), designed to remove the influence of a group\nof continuous sensitive variables, thereby facilitating counterfactual fairness\nin machine learning applications. Our approach is grounded in the assumption of\na jointly normal distribution within a structural causal model (SCM), proving\nthat counterfactual fairness can be achieved by ensuring the data is\nuncorrelated with sensitive variables. The OB algorithm is model-agnostic,\ncatering to a wide array of machine learning models and tasks, and includes a\nsparse variant to enhance numerical stability through regularization. Through\nempirical evaluation on simulated and real-world datasets - including the adult\nincome and the COMPAS recidivism datasets - our methodology demonstrates its\ncapacity to enable fairer outcomes without compromising accuracy.",
        "updated": "2024-03-26 16:40:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17852v1"
    },
    {
        "title": "Towards Multilevel Modelling of Train Passing Events on the Staffordshire Bridge",
        "authors": "Lawrence A. BullChiho JeonMark GirolamiAndrew DuncanJennifer SchoolingMiguel Bravo Haro",
        "links": "http://dx.doi.org/10.12783/shm2023/37066",
        "entry_id": "http://arxiv.org/abs/2403.17820v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17820v1",
        "summary": "We suggest a multilevel model, to represent aggregate train-passing events\nfrom the Staffordshire bridge monitoring system. We formulate a combined model\nfrom simple units, representing strain envelopes (of each train passing) for\ntwo types of commuter train. The measurements are treated as a longitudinal\ndataset and represented with a (low-rank approximation) hierarchical Gaussian\nprocess. For each unit in the combined model, we encode domain expertise as\nboundary condition constraints and work towards a general representation of the\nstrain response. Looking forward, this should allow for the simulation of train\ntypes that were previously unobserved in the training data. For example, trains\nwith more passengers or freights with a heavier payload. The strain event\nsimulations are valuable since they can inform further experiments (including\nFEM calibration, fatigue analysis, or design) to test the bridge in\nhypothesised scenarios.",
        "updated": "2024-03-26 15:55:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17820v1"
    },
    {
        "title": "Asymptotic Bayes risk of semi-supervised learning with uncertain labeling",
        "authors": "Victor LegerRomain Couillet",
        "links": "http://arxiv.org/abs/2403.17767v1",
        "entry_id": "http://arxiv.org/abs/2403.17767v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17767v1",
        "summary": "This article considers a semi-supervised classification setting on a Gaussian\nmixture model, where the data is not labeled strictly as usual, but instead\nwith uncertain labels. Our main aim is to compute the Bayes risk for this\nmodel. We compare the behavior of the Bayes risk and the best known algorithm\nfor this model. This comparison eventually gives new insights over the\nalgorithm.",
        "updated": "2024-03-26 14:54:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17767v1"
    },
    {
        "title": "On the Benefits of Over-parameterization for Out-of-Distribution Generalization",
        "authors": "Yifan HaoYong LinDifan ZouTong Zhang",
        "links": "http://arxiv.org/abs/2403.17592v1",
        "entry_id": "http://arxiv.org/abs/2403.17592v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17592v1",
        "summary": "In recent years, machine learning models have achieved success based on the\nindependently and identically distributed assumption. However, this assumption\ncan be easily violated in real-world applications, leading to the\nOut-of-Distribution (OOD) problem. Understanding how modern over-parameterized\nDNNs behave under non-trivial natural distributional shifts is essential, as\ncurrent theoretical understanding is insufficient. Existing theoretical works\noften provide meaningless results for over-parameterized models in OOD\nscenarios or even contradict empirical findings. To this end, we are\ninvestigating the performance of the over-parameterized model in terms of OOD\ngeneralization under the general benign overfitting conditions. Our analysis\nfocuses on a random feature model and examines non-trivial natural\ndistributional shifts, where the benign overfitting estimators demonstrate a\nconstant excess OOD loss, despite achieving zero excess in-distribution (ID)\nloss. We demonstrate that in this scenario, further increasing the model's\nparameterization can significantly reduce the OOD loss. Intuitively, the\nvariance term of ID loss remains low due to orthogonality of long-tail\nfeatures, meaning overfitting noise during training generally doesn't raise\ntesting loss. However, in OOD cases, distributional shift increases the\nvariance term. Thankfully, the inherent shift is unrelated to individual x,\nmaintaining the orthogonality of long-tail features. Expanding the hidden\ndimension can additionally improve this orthogonality by mapping the features\ninto higher-dimensional spaces, thereby reducing the variance term. We further\nshow that model ensembles also improve OOD loss, akin to increasing model\ncapacity. These insights explain the empirical phenomenon of enhanced OOD\ngeneralization through model ensembles, supported by consistent simulations\nwith theoretical results.",
        "updated": "2024-03-26 11:01:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17592v1"
    }
]