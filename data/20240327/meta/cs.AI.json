[
    {
        "title": "SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models",
        "authors": "Kashyap ChittaDaniel DaunerAndreas Geiger",
        "links": "http://arxiv.org/abs/2403.17933v1",
        "entry_id": "http://arxiv.org/abs/2403.17933v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17933v1",
        "summary": "SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.",
        "updated": "2024-03-26 17:58:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17933v1"
    },
    {
        "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
        "authors": "Wei TaoYucheng ZhouWenqiang ZhangYu Cheng",
        "links": "http://arxiv.org/abs/2403.17927v1",
        "entry_id": "http://arxiv.org/abs/2403.17927v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17927v1",
        "summary": "In software evolution, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing functionalities. Large Language\nModels (LLMs) have shown promise in code generation and understanding but face\ndifficulties in code change, particularly at the repository level. To overcome\nthese challenges, we empirically study the reason why LLMs mostly fail to\nresolve GitHub issues and analyze some impact factors. Motivated by the\nempirical findings, we propose a novel LLM-based Multi-Agent framework for\nGitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized\nfor the software evolution: Manager, Repository Custodian, Developer, and\nQuality Assurance Engineer agents. This framework leverages the collaboration\nof various agents in the planning and coding process to unlock the potential of\nLLMs to resolve GitHub issues. In experiments, we employ the SWE-bench\nbenchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and\nClaude-2. MAGIS can resolve 13.94% GitHub issues, which significantly\noutperforms the baselines. Specifically, MAGIS achieves an eight-fold increase\nin resolved ratio over the direct application of GPT-4, the based LLM of our\nmethod. We also analyze the factors for improving GitHub issue resolution\nrates, such as line location, task allocation, etc.",
        "updated": "2024-03-26 17:57:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17927v1"
    },
    {
        "title": "AID: Attention Interpolation of Text-to-Image Diffusion",
        "authors": "Qiyuan HeJinghao WangZiwei LiuAngela Yao",
        "links": "http://arxiv.org/abs/2403.17924v1",
        "entry_id": "http://arxiv.org/abs/2403.17924v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17924v1",
        "summary": "Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.",
        "updated": "2024-03-26 17:57:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17924v1"
    },
    {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "authors": "Rui PanXiang LiuShizhe DiaoRenjie PiJipeng ZhangChi HanTong Zhang",
        "links": "http://arxiv.org/abs/2403.17919v1",
        "entry_id": "http://arxiv.org/abs/2403.17919v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17919v1",
        "summary": "The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.",
        "updated": "2024-03-26 17:55:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17919v1"
    },
    {
        "title": "AgentStudio: A Toolkit for Building General Virtual Agents",
        "authors": "Longtao ZhengZhiyuan HuangZhenghai XueXinrun WangBo AnShuicheng Yan",
        "links": "http://arxiv.org/abs/2403.17918v1",
        "entry_id": "http://arxiv.org/abs/2403.17918v1",
        "pdf_url": "http://arxiv.org/pdf/2403.17918v1",
        "summary": "Creating autonomous virtual agents capable of using arbitrary software on any\ndigital device remains a major challenge for artificial intelligence. Two key\nobstacles hinder progress: insufficient infrastructure for building virtual\nagents in real-world environments, and the need for in-the-wild evaluation of\nfundamental agent abilities. To address this, we introduce AgentStudio, an\nonline, realistic, and multimodal toolkit that covers the entire lifecycle of\nagent development. This includes environment setups, data collection, agent\nevaluation, and visualization. The observation and action spaces are highly\ngeneric, supporting both function calling and human-computer interfaces. This\nversatility is further enhanced by AgentStudio's graphical user interfaces,\nwhich allow efficient development of datasets and benchmarks in real-world\nsettings. To illustrate, we introduce a visual grounding dataset and a\nreal-world benchmark suite, both created with our graphical interfaces.\nFurthermore, we present several actionable insights derived from AgentStudio,\ne.g., general visual grounding, open-ended tool creation, learning from videos,\netc. We have open-sourced the environments, datasets, benchmarks, and\ninterfaces to promote research towards developing general virtual agents for\nthe future.",
        "updated": "2024-03-26 17:54:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.17918v1"
    }
]