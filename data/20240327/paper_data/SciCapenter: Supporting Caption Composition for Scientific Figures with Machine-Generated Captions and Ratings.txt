SciCapenter: Supporting Caption Composition for Scientific
Figures with Machine-Generated Captions and Ratings
Ting-YaoHsu Chieh-YangHuang Shih-HongHuang
ThePennsylvaniaStateUniversity ThePennsylvaniaStateUniversity ThePennsylvaniaStateUniversity
UniversityPark,PA,USA UniversityPark,PA,USA UniversityPark,PA,USA
txh357@psu.edu chiehyang@alumni.psu.edu szh277@psu.edu
RyanRossi SungchulKim TongYu
AdobeResearch AdobeResearch AdobeResearch
SanJose,CA,USA SanJose,CA,USA SanJose,CA,USA
ryrossi@adobe.com sukim@adobe.com tyu@adobe.com
ClydeLeeGiles Ting-Hao‘Kenneth’Huang
ThePennsylvaniaStateUniversity ThePennsylvaniaStateUniversity
UniversityPark,PA,USA UniversityPark,PA,USA
clg20@psu.edu txh710@psu.edu
ABSTRACT 1 INTRODUCTION
Craftingeffectivecaptionsforfiguresisimportant.Readersheavily Captionsforscientificfigures,suchascharts,barcharts,andpie
dependonthesecaptionstograspthefigure’smessage.However, charts,arecrucialtoreaders.Readerscomprehendandrecalltheun-
despiteawell-developedsetofAItechnologiesforfiguresandcap- derlyinginformationsignificantlybetterwhenreadingchartswith
tions,thesehaverarelybeentestedforusefulnessinaidingcaption captions,ascomparedwithreadingchartsalone[2,13,30,40].Un-
writing.ThispaperintroducesSciCapenter,aninteractivesystem fortunately,authorsdonotpayneededattentiontocraftingfigure
thatputstogethercutting-edgeAItechnologiesforscientificfig- captionsintheirpapers.Arecentstudyshowedthatoverhalfofthe
urecaptionstoaidcaptioncomposition.SciCapentergenerates figurecaptionsinarXivpaperswereratedasnothelpfulbyPh.D.
avarietyofcaptionsforeachfigureinascholarlyarticle,provid- students[19].Meanwhile,withadvancesindeeplearning,com-
ingscoresandacomprehensivechecklisttoassesscaptionquality putationalmodelsnowhavearichsetofcapabilitiessurrounding
acrossmultiplecriticalaspects,suchashelpfulness,OCRmention, scientificfigures:generatingcaptionsofdecentquality[24,37,49],
keytakeaways,andvisualpropertiesreference.Userscandirectly analyzingtheinformationinfigureimages[6,26,35],andevaluat-
editcaptionsinSciCapenter,resubmitforrevisedevaluations,and ingtheusefulnessoffigurecaptions[18].However,literaturehas
iterativelyrefinethem.AuserstudywithPh.D.studentsindicates littletosayabouthowthesetechnologiescouldbeusedtohelp
thatSciCapentersignificantlylowersthecognitiveloadofcap- academicswhowritefigurecaptionsfortheirpapers.Theuserstud-
tionwriting.Participants’feedbackfurtheroffersvaluabledesign iesofthesetechnologieswerealmostallconductedfromareader
insightsforfuturesystemsaimingtoenhancecaptionwriting. perspective,i.e.,havingparticipantsreadthemachine-generated
outputsandjudgetheirquality[19,51,52].Theuserneedsofwrit-
ersandreadersareknowntobedifferent[3,8,14,44];apieceof
ACMReferenceFormat:
machine-generatedtextmightbeoflowerreadabilitybutcould
Ting-YaoHsu,Chieh-YangHuang,Shih-HongHuang,RyanRossi,Sungchul
serveasausefuldraftforwriterstoworkoffof[9,10,28].Afew
Kim,TongYu,ClydeLeeGiles,andTing-Hao‘Kenneth’Huang.2024.Sc-
priorworks,suchasIntentable[5],InkSight[33],andAutoTitle[34],
iCapenter:SupportingCaptionCompositionforScientificFigureswith
Machine-GeneratedCaptionsandRatings.InExtendedAbstractsofthe builtsystemsthatgeneratecaptionsconsideringwriters’intentions
CHI Conference on Human Factors in Computing Systems (CHI EA ’24), andallowauthorstorefinevisualizationcaptionsandtitles,butthey
May11–16,2024,Honolulu,HI,USA.ACM,NewYork,NY,USA,9pages. didnotfocusoncomposingfigurecaptionsforscholarlyarticles.
https://doi.org/10.1145/3613905.3650738 ThispaperintroducesSciCapenter,asystemthatputstogether
asetofcutting-edgeAItechnologiessurroundingscientificfigures
tohelpuserswritefigurecaptionsinscholarlyarticles(Figure1).
UsersfirstuploadthePDFfileoftheirworkingdrafttoSciCapen-
ter,whichwillautomaticallyextractallthefiguresandtheircur-
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalor rentcaptions.Userscanthenbrowseandpinpointfigurestheywish
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
tomodify.Uponselectingafigure,SciCapenterdisplaysthefigure
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. image,itscurrentcaption,paragraphsinthearticlereferencing
Forallotheruses,contacttheowner/author(s). theexactfigure(e.g.,“Figure3demonstrates...”),andaseriesof
CHIEA’24,May11–16,2024,Honolulu,HI,USA
AI-generatedinsights.Specifically:
©2024Copyrightheldbytheowner/author(s).
ACMISBN979-8-4007-0331-7/24/05
https://doi.org/10.1145/3613905.3650738
4202
raM
62
]CH.sc[
1v48771.3042:viXraCHIEA’24,May11–16,2024,Honolulu,HI,USA TrovatoandTobin,etal.
Figure1:OverviewofSciCapentersysteminterface.PDFUploadPanel(A):Adrag-and-dropinterfaceforuploadingPDFfiles.
NavigationBar(B):Ahorizontalbarshowingalistoffiguresextractedfromtheuploadeddocument.FigureImage(C):The
mainareadisplayingtheimageoftheselectedfigure.CaptionEditor(D):Atextboxforeditingthecaptionoftheselected
figure.CaptionRating(F):AfeedbacksystemthatallowsGPTtoratethequalityofthecaption,representedbyastarrating.
CaptionAnalysis(CheckTable)(E):Iconsindicatingthepresenceorabsenceofkeyelementsinthecaption,suchashelpfulness
ortakeawaymessage.ExplanationfortheRating(G):Atextualexplanationprovidinginsightintowhyaparticularstarrating
wasgiventothecaption.Machine-generatedCaptions&TheirRatings(H):Thissectionincludeslongandshortcaptions
generatedbyAImodels,eachaccompaniedbytheirrespectivestarratings.Figure-mentioningParagraphs(I):Paragraphsin
thedocumentthatmentionthetargetfigure,providingcontextoradditionalinformation.
• SciCapenterofferstwomachine-generatedcaptions(Fig- • SciCapenterratesallcaptions(Figure1.F),includingthe
ure1.H):onelong,anothershort,usingacutting-edgeAI author’soriginalandallAI-generatedones,andalsoshows
model.Wedecidedtoprovidecaptionswithtwodifferent therationalebehindeachrating(Figure1.G).Userscan
lengthstoaddressthedualneedsofcaptionwriters:longer modifycaptionsdirectlyinSciCapenter(Figure1.D),sub-
captionsaremoreinformativeforreaders[19],whileshorter mitthemforupdatedratings,andrefinethemiteratively.
captionshelpauthorsmeetspaceconstraintsinpapers.
Auserstudyinvolving15STEMPh.D.studentsspanningCom-
• SciCapenterprovidesachecklist(Figure1.E),predictedby puterSciencetoMechanicalEngineeringindicatedthatSciCapen-
anAImodel,detailingessentialcaptionfeatures(e.g.,does
ter significantly reduced the cognitive load of caption writing,
thecaptionhighlightthecentralmessage?Doesitreference
measuredbytheNASAtaskloadindex(NASATLX).Themajority
figuretext?).
expressedadesireforsimilartoolsintheirfutureworks.SciCapenter CHIEA’24,May11–16,2024,Honolulu,HI,USA
The contribution of this work is three-fold. First, our work forvisualizations[34].However,thefocusofthesesystemswas
demonstratesthatevenifmachine-generatedtextisnotsuperiorto notonsupportingfigurecaptioncompositioninscholarlyarticles.
human-writtentext,itcouldstillaidwriting.Second,theuserstudy Inthispaper,weintroduceSciCapenteranddelveintoexplor-
resultshighlighttheadvantagesofpresentingautomatedpredic- ingthisarea.
tions,suchastargettext’scharacteristics,whenprovidingwriting
assistance.Finally,participants’feedbackoffersvaluabledesign 3 SCICAPENTERSYSTEM
insightsforfuturesystemsaimingtoenhancecaptionwriting.
SciCapenterisacomprehensivesystemthatincludesawebinter-
faceandacaptioneditingfunction.Figure1showsascreenshot
oftheentiresystem.TobeginusingSciCapenter,usersstartby
uploadingtheirPDFfileinthePDFuploadpanel(Figure1.A).Once
2 BACKGROUND
thePDFfileisprocessed,theextractedfiguresaredisplayedonthe
Figuresindocuments,suchasbarcharts,linecharts,andpiecharts,
navigationbar(Figure1.B).Userscanclickonafiguretoaccess
often contain key information the authors desire to convey. To
thedetailedinformation.Uponclicking,theselectedfigure(Fig-
fullydecodethemessageembeddedinfigures,reasoningacrosslan-
ure1.C),alongwithcompleteinformation,appearsontheright
guages(e.g.,documenttexts,figurecaptions)andvision(e.g.,figure
sideofthewindow.Ontherightsideofthewindow,usershave
images)isrequired.Thiscomplextaskpresentsagreatchallenge
accesstoseveralfeaturesthatassistthemincomposingcaptions,
fortheAIcommunity,attractingresearcherstodeveloptechnolo-
includingacheckingtable(Figure1.E),GPTevaluationandexpla-
giesspecificallytoaddressfigures.Theseadvancementsinclude
nation(Figure1.Fand1.G),generatedcaptions-long/shortversions
AImodelsthatparseinformationliketrends,axes,orstatisticsin
(Figure1.H),and(4)referredparagraphs(Figure1.I).Usersareable
figureimages[24,45],generateavarietyofdescriptionsbasedon
toeditthecaptioninthecaptioneditor(Figure1.D)andresubmit
theimageofthefigure(i.e.,vision-to-languagemodels)[31,35,37]
itforre-evaluation.
orthedataunderlyingit(i.e.,data-to-textmodels)[11,41],answer
In the backend, once a user submits a pdf file, SciCapenter
questionsaboutfigureimages[22,23,38],andsuggestappropriate
willstorethefileinourMongoDBandextractfigures,captions,
figuretypesbasedonthedata[20,25,46]. abstracts,andcontentsusingpdffigure2[7].1Theextractedfigure
Amongtheseefforts,asignificantportionwasdevotedtothe
listwillthenbepresentedinthenavigationbar(Figure1.B).Regular
studyofscientificfiguresandtheircaptionsinscholarlyarticles.
expressionsarethenusedtoidentifythecorrespondingreferred
TheSciCapdataset[16],introducedin2021,wasthefirstlarge-scale
paragraphs(Figure1.I)basedonthefigureindex,similartohow
collectionofreal-worldfiguresandcaptions,containing2,170,719
Huangetal.extractedfigure-mentioningparagraphs[19].
figuresandcaptionsextractedfrom295,028arXivpapers.Unlike
Thechecktable(Figure1.E)detectswhetherthewrittencaption
previous works that used synthetic data [4, 21, 22], SciCap fa-
describesorexpressesthefollowingsixaspects[18]:
cilitatedthedevelopmentofAImodelscapableofhandlingreal-
(1) Helpfulness:Doesthiscaptionhelpyouunderstandthe
worldscientificfigures.Thisincludesmodelsforgeneratingcap-
figure?
tionsbasedonfigureimages[15,16,32,36,47,51]orpapercon-
(2) OCR(OpticalCharacterRecognition):Doesthiscaption
tent[15,19,32,51].Therearealsomodelsthatevaluatethequality
mentionanywordsorphrasesthatappearinthefigure?
ofgivencaptions[18].InOctober2023,thefirstSciCapChallenge
(Examplesincludethefiguretitle,XorYaxistitles,legends,
washeld[17],wheresixglobalteamscompetedonaperformance
namesofmodels,methods,subjects,etc.)
leaderboard,showcasingtheadvancementsinthisfield.
(3) Relation:Doesthiscaptiondescribearelationshipamong
FromaHuman-ComputerInteraction(HCI)perspective,these
twoormoreelementsorsubjectsinthefigure?(Forexam-
emergingtechnologieshavegreatpotentialforaidingcaptionwrit-
ple,"AislowerthanB,""AishigherthanB,"or"Aisthe
ing.However,theywererarelystudiedinawritingcontext.Instead,
lowest/highestinthefigure.")
thefocusofmostuserstudiesonthesetechnologieshasbeenfrom
(4) Stats:Doesthiscaptionmentionanystatisticsornumbers
areader’sstandpoint.Thisincludesmethodslikeshowingpartici-
fromthefigure?(Forexample,"20%of..."or"Thevalueof..
pantsmachine-generatedfigurecaptionsandaskingthemtoassess
is0.33...".)
theirquality[27,36,48,50],presentingvariouscaptionvariations
(5) Takeaway:Doesthiscaptiondescribethehigh-leveltake-
for participants to compare or rank [27, 48, 50], or testing how
aways,conclusions,orinsightsthefiguretriestoconvey?
muchinformationreaderscanrememberafterreadingafigureor
(6) Visual:Doesthiscaptionmentionanyvisualcharacteristics
acaption[27,48].Giventhattheneedsofwritersandreadersare
ofthefigure?(Examplesincludecolor,shape,direction,size,
distinct[3,8,14,44],studiesfocusingonwritingscenarioscould
position,oropacityofanyelementsinthefigure.)
greatlyenhancetheapplicationofthesetechnologiesinsupportof
captionwriting.Afewstudiesandsystemshavebeendevelopedto Missedaspectswouldbehighlightedasawarningforusers.The
aidcaptionwriting.Forinstance,Intentablecombinesautomated aspectdetectionmodelisaSciBert[1]modelfine-tunedon3,159
caption generation with manual input, empowering authors to humanannotations,whichachievedanF1scoreof0.64[18].The
guidethecaptioningprocessbasedontheirinsightsandintentions captionratinganditsexplanation(Figure1.FandFigure1.G)are
derivedfromthevisualization[5];InkSightallowsuserstohigh-
light areas of interest in visualizations through sketching, then 1Notethatinthisstudy,figuresclassifiedas“Table”wereexcludedduetotheinherent
differencesbetweenscientificfiguresandtables.DespiteourbeliefinSciCapenter’s
automaticallygeneratesdocumentationthatuserscanfurtherre-
abilitytohelpuserswritecaptionsfortables,weoptedtoomittables(andfigures
vise[33];andAutoTitlefacilitatestheinteractivecreationoftitles classifiedasTables)inthisstudy.CHIEA’24,May11–16,2024,Honolulu,HI,USA TrovatoandTobin,etal.
obtained by calling OpenAI’s GPT-3.5-turbo API [42] with the toallthestudentsintheauthors’university,wasessentialforthe
promptshowninAppendixAinazero-shotmanner.Themachine- study.Hence,weinstructedalltheremoteparticipantstoinstallit
generatedcaptions(Figure1.H)aregeneratedwiththemodelpro- ontheircomputersbeforehand.
posedbyHuangetal.[19].Specifically,weobtainedtheirPegasus Thestepsofthestudywereasfollows:Wefirstintroducedpar-
𝑃+𝑂
andPegasus model,figurecaptioningmodelsthatgenerate ticipants to the study’s objective and procedure, securing their
𝑃+𝑂+𝐵
captionsbysummarisingthefigure-mentioningparagraphs.The consent.WethenofferedabriefoverviewoftheSciCapenterand
Pegasus modelistrainedwithcaptionslongerthan30words itsfeatures.Participantsthenproceededthroughfourconditions:
𝑃+𝑂+𝐵
soitnaturallygenerateslongercaptions.InSciCapenter,weuse
• Condition1(2figures):Freewriting.Participantswere
thedefaultdecodingstrategy(beamsearchwiththenumberof
givenaPDFwiththetargetcaptionredactedandaskedto
beams=5)fortextgeneration.
draftacaptiondirectlyinAdobeAcrobatProwithoutany
timeconstraints.
4 USERSTUDY
• Condition2(2figures):FreewritingwithSciCapen-
StudyOverviewandParticipantRecruitment. Toassesstheef- ter.ParticipantsweregivenaredactedPDFasinCondition
fectivenessof SciCapenterinassistinguserswithwritingfigure 1.However,thistime,theywrotecaptionsdirectlyinSci-
captionsforacademicarticles,werecruited18participants–3fora Capenter.Participantswerealloweduptotwosubmissions
pilotstudyand15forthemainstudy.Manydesigndetailsofthe toretrieveSciCapenter’sratings.Thesubmissionlimitwas
mainstudywereinformedbythepilotstudy,whichwedescribein settopreventparticipantsfromtryingtotrickthesystem
AppendixB. justtogethigherscores.2
Forrecruitment,weleveragedvariouschannels:personalnet- • Condition3(1figure):3Time-constrainedwriting.This
works,socialmediaposts,Slackgroupchannels,andtheuniver- conditionmirroredCondition1,butparticipantswereal-
sity’smailinglist.TheseparticipantswerePh.D.studentsfromthe lottedjust8minutesperfigure.Therationaleforthistime-
authors’university,majoringinSTEM.Specifically,6majoredin constrainedsetupwastotestSciCapenter’sutilitywhen
ComputerScience,7majoredinInformatics,3majoredinChemical participantswereunderpressureforcuratingcaptions,such
Engineeringand2wereMechanicalEngineeringmajors.Ideally,we asduringlast-minutesubmissionsorwhencaptionswere
wantedparticipantscurrentlywritingnewpaperswithfiguresand givenscantattention.
captions,butsuchindividualswerescarce.Askingparticipantsto • Condition4(1figure):Time-constrainedwritingwith
reviseandimprovepoor-qualitycaptionsintheirpublishedpapers SciCapenter.SameasCondition2,butparticipantshad
alsoprovedchallenging,asitrequiredthemtoadmittheirpapers’ only8minutestodrafttheircaptions.
weaknesses,complicatingrecruitment.Consequently,weoptedto Participantssequentiallyprogressedthroughtheconditions:1
haveparticipantswritefigurecaptionsforothers’publishedpapers -> 3 -> 2 -> 4. This order, derived from our pilot study (Appen-
intheirfields.Althoughdifferentfromwritingcaptionsforfigures dixB),helpedpreventconfusionandfatiguefromswitchingbe-
intheirownpaper,thismethodstruckabalancebetweeneaseof tweenAdobeAcrobatProandSciCapenter.4Aftereachcondition,
recruitmentandtaskrelevance,makingitapracticalchoiceforthis participantscompletedtheNASA-TaskLoadIndex(NASA-TLX)
preliminarystudy. questionnaire[12]viaaGoogleForm.Weusedafive-pointLikert
scale(VeryLowtoVeryHigh)forallscaleitems,whichwasused
Pre-StudyPreparation. Weaskedeachparticipanttoprovideten
inseveralpriorHCIstudies[29,39].Uponconcludingthewhole
scientificpapersfromtheirresearchdomainthattheyintended
userstudy,weconductedabrief,open-endedinterviewtogather
toreadorhadbrieflyskimmedbuthadnotreadin-depth.This
feedbackandsuggestionsfromeachparticipant.Thedurationof
approachensuredparticipantshadsomecontextualunderstanding
thepost-interviewtypicallyvaries,butonaverage,itlastsabout
ofthearticlebutwerenotbiasedbytheoriginalcaptions.After
tenminutes.Weposefourquestionstotheparticipants:(1)Doyou
receivingthesearticles,weprocessedthemthroughSciCapenter,
findthesystemuseful,andwhataspectsdoyoulikeordis-
selecting six figures with the lowest quality score rated by Sci-
like?(2)Whatisyourpreferredwritingstyleorapproachfor
Capenter.Wethenmanuallyredactedthecaptionsfromthese
figurecaptions?(3)Doyouhaveanysuggestionsforoursys-
selectedpapersusingAdobeAcrobatPro(Figure2).Allthesema-
tem,andifyoucouldincorporateadditionalfunctions,what
terialswerepreparedbeforetheuserstudy.Wetriedourbestto
wouldtheybe?(4)Doyouhaveanyfurtherfeedback?We
selectonlyonefigureperarticle.However,pdfparser2failedto
parsepapersfromsomespecificdomainsmuchmorefrequently,
sothiswasnotalwaysfeasible.Incaseswheretwofigureswere 2Limitinguserstoonlytwoattemptsforratingsmayinfluencetheiropinionofthe
selectedfromthesamepaperforaparticipant,theyreceivedaPDF toolandcouldbiastheirresponsesontheNASAtaskloadindex.However,ourgoal
wastodiscourageusersfromcarelesslyadjustingtheircaptionswithoutthoughtful
wherebothcaptionswereredacted. consideration.Thisdecisionwasinformedbythewritingeffortandaveragecaption
changesobservedinthepilotstudy.
StudyProcedureandExecution. Mostsessionswereconductedin- 3Weinitiallyplannedforparticipantstowritecaptionsfortwofiguresunderallcondi-
lab,withsevenparticipantsjoiningviaZoom.Eachsessiontypically tions.However,ourpilotstudyindicatedthatintime-constrainedsettings,oursetup
causedexcessiveeffortandrapidfatigueamongusers.Therefore,wehadparticipants
spannedbetween1to1.5hours.Participantsattendinginperson captiononlyonefigureintime-constrainedconditions.
usedoneoftheauthor’slaptop,whilethosejoiningremotelyused 4AsparticipantswerenewtoSciCapenter,weunderstandthatthisorderdidnot
completelyremovethelearningeffect.Webelieveitmakesmoresenseforparticipants
theirowncomputers.AsSciCapenterisaweb-basedapplication,
tofirstattemptthetaskwithoutadditionalsupport(Condition1)becausewriting
itcanbeaccessedanywhereonline.AdobeAcrobatPro,licensed captionsforfiguresinotherpeople’spapersisnotatypicalwritingtask.SciCapenter CHIEA’24,May11–16,2024,Honolulu,HI,USA
Figure2:Beforethestudy,participantsprovidedtenpapersfromtheirresearchdomain,eitherintendedforreadingorbriefly
skimmedbutnotreadin-depth.WeprocessedthesepapersthroughSciCapenter,choosingsixtargetfiguresandmanually
redactingtheircaptions.ParticipantsreceivedtheredactedPDFsintheuserstudyandwereaskedtowritecaptionsforthe
figures.
audio-recordedandtranscribedeachinterview.Then,twoauthors ratingsfurther,longandshortcaptionsreceivedthehighestnum-
usedopencodingtoanalyzethetranscripts. berof“VeryBad”ratings.Thepost-studyinterviewofparticipants
suggestedthatthesystem-generatedcaptionsoftenfallshortof
expectations,withsomeexperiencinginaccuraciesor“hallucina-
5 FINDINGS
tions”(P8,P10,P11,P15),especiallyshortcaptions:“Ithinkthe
SciCapentersignificantlyreducedcognitiveloadsincaptionwrit- generationwasn’t100%precise...sometimesitcapturessomeweird
ingforparticipants,particularlyundertimeconstraints. Inthemain stuff.(P8)”,“thelongandshortcaption...itwasn’tthatgood.(P10)”,
study,therewereatotalof15participants.WecomputedtheNASA- “So,forthegeneratedcaption,whyisnotuseful?Becauseitjust
TaskLoadIndexforeachcondition,employingafive-pointscale have[sic]somehallucination,ortheyjustwritewhatyoudon’t
rangingfromVeryLowtoVeryHighandfromPerfecttoFailure. want....Idon’tthinktheyareexplainingto[sic]figurewell.(P15)”
TheresultsareshowninTable1,demonstratingthatSciCapenter
Usersshowvariedpreferencesforthefeatures. Mostparticipants
significantlyreducedcognitiveloadforusers.Withtheassistance
(P1,P4,P7,P10,P11,P14)preferredshortcaptionsforconciseness
ofSciCapenter,usersexperiencedreducedeffort,frustration,and
andsimplicity,whileafew(P3,P13)choselongcaptionsfortheir
mentaldemandwhencomposingcaptions.Thiseffectwasmore
detail,particularlywithcomplexorimportantfigures.Theeffec-
pronouncedundertimeconstraints,withtheoverallNASATask
tivenessofthechecktablealsodifferedamongparticipants.Some
Index(Table1)droppingfrom2.93to2.16,comparedtoasmaller
valueditsdirectivenatureforcaptioncontent(P3,P4,P5,P7,P15):
reductionfrom2.39to2.04infreewriting.Additionally,thede-
“thegoodsideaboutit(SciCapenter)isithasachecktable,soit
creaseinfrustrationwasthemostsignificant,withtheworkload
kindofremindsmewhichelementIshouldputorwhichcriteriaI
fallingfrom2.33to1.40infreewriting,andfrom2.73to1.67in
shouldbecheckingbeforeIwritethecaptionorwhileI’mwriting
time-limitedscenarios.
thecaption.(P3)”,“thechecktableisreallyhelpfulbecauseIcan
knowwhatinformationismissinginmycaption.(P4)”However,
Machine-generatedratingsandlabelsweredeemedmoreuseful somefounditconfusingorirrelevant(P9,P11,P12):“Idon’tknow
thanmachine-generatedcaptions. Weaskedparticipantstoratethe howtousethischecktabletoimprovemycaptionbecauseitsome-
overallSciCapenterintermsofthefollowingthreequestionson timessaysyouneedtoaddstatsforthecaption,butforthefigure,
afive-pointLikertscale:(i)Howdifficultisittousethesystem? actuallyI’mnotabletoaddsomestaticsforthesefigures.(P9)”,
(Ratingscale:VeryEasytoVeryDifficult),(ii)Howusefulisthe “Idon’tlikethat(checktable).Idon’tcheckeverypartofit(the
systeminassistingwithcaptionwriting?(Overall)(Ratingscale: caption)becauseIthinkinsomecasesyoudon’tneedtocoverall
VeryBadtoVeryGood),and(iii)Howfastisthesystemresponse? thethings.(P12)”
(Ratingscale:VerySlowtoVeryFast).Sixmorequestionsaboutthe Meanwhile,althoughparagraphsandexplanationsweretypi-
satisfactionlevel(afive-pointLikertscalefromVeryBadtoVery callyhelpfulinunderstandinganddescribingfigures(P2,P3,P6,
Good)towardthesixcomponents(i.e.,checktable,captionrating, P15),therewereinstanceswhereexplanationsfailedtoreflectthe
explanationfortherating,longcaption,shortcaption,andreferred participants’recentedits(P4,P8).
paragraph)werealsoasked.Figure3showstheresults.
Overall,participantsexpressedhighsatisfactionwithoursys- Users desire actionable suggestions, templates, or examples for
tem’susability(averagerating=3.80,SD=0.86,n=15)andits implementation. Thepost-studyinterviewsshowafrequentrequest
easeofuse(averagerating=1.80,SD=0.68,n=15).Theresponse forthesystemtooffermoreconcreteactionsuggestionsforeasy
time got an average rating of 3.40 and was not considered too implementation:“Youwantnotonlythechecktable,butalsogive
slow.Wealsodiscoveredthat,withinSciCapenter,participants youasuggestiononwhattowritefortheaspects....sometimesI
foundmachine-generatedratingsandpredictionsmoreusefulthan sawthechecktable,butIdon’tknowhowtousethischecktable
machine-generatedcaptions.Evenextractedparagraphsthatmen- toimprovemycaption.(P9)”,“...,likemaybeforeachpoint(inthe
tionedthefigureweredeemedmorehelpful.Bothlongandshort checktable),youcanlistasentenceortwosentencesthatcanbe
captionshadthelowestaveragepreferencerating.Analyzingthe addedtothecaptiondirectly,...Sousersdon’tneedtocomeupCHIEA’24,May11–16,2024,Honolulu,HI,USA TrovatoandTobin,etal.
FreeWriting Time-ConstrainedWriting
UserOnly WithSystem UserOnly WithSystem
p-value p-value
Avg. 95%CI Avg. 95%CI Avg. 95%CI Avg. 95%CI
MentalDemand 2.93 [2.40,3.47] 2.53 [1.91,3.16] 0.233 3.80 [3.32,4.28] 2.53 [1.91,3.16] <0.001***
PhysicalDemand 1.87 [1.40,2.33] 1.93 [1.40,2.47] 0.774 2.20 [1.68,2.72] 1.87 [1.40,2.33] 0.019
TemporalDemand 1.87 [1.36,2.37] 1.53 [1.18,1.89] 0.096 3.20 [2.53,3.87] 2.40 [1.74,3.06] 0.005**
Performance 2.47 [1.88,3.05] 2.47 [1.75,3.19] 1.000 2.73 [2.12,3.34] 2.13 [1.48,2.79] 0.095
Effort 2.87 [2.36,3.37] 2.40 [1.90,2.90] 0.150 2.93 [2.40,3.47] 2.33 [1.84,2.83] 0.057
Frustration 2.33 [1.62,3.05] 1.40 [1.12,1.68] 0.008** 2.73 [2.06,3.41] 1.67 [1.21,2.12] 0.001**
Overall 2.39 [2.05,2.73] 2.04 [1.71,2.38] 0.046* 2.93 [2.52,3.34] 2.16 [1.74,2.57] <0.001***
Table1:WeleveragetheNASATaskLoadIndexmethodtoassessworkloadonfive-pointscales,underconditionsoffree
writingandtime-constrainedwriting,bothwithandwithoutSciCapenter.Alowerscoreindicatesalowerperceivedworkload.
Thep-valuesareobtainedbycomparingtheuser-onlyandwith-systemsettings(pairedt-test,twotailed,N=15).The95%
confidenceintervalsareestimatedusingt-distribution.WithSciCapenter,usershadlessworkloadinbothfreewritingand
time-constrainedwritingscenarios,withthemostnotableimprovementsseenintheMentalDemandandFrustration.
Figure3:ComparisonbetweensixdifferentelementsprovidedbySciCapenter.Leftfigureshowsthemeanratingandstandard
deviationofafive-pointscalefordifferentelements.Thechecktableandreferredparagraphwereratedhighest,whileshort
andlongcaptionhadthelowestscore,indicatingtheyweretheleastfavoredelementsaccordingtotheparticipants.Right
figureshowsabreakdownofthefive-pointscalewithdifferentcolorsrepresentingeachrating.Theshortcaptionexhibita
morevarieddistributionofopinions.
bythemselves.(P10)”.SomesuggestedSciCapentertoprovide resultsanduserinterviews,weoffertwodesignrecommendations
examplecaptionsforsimilarfigures:“...youcangenerateatypical forwritingassistantstargetingfigurecaptions:
captionfromotherpapersthathavealreadybeenpublished.(P7)”, • Presentcontextualoranalyticalinformationrelevant
“Iwouldbecuriousabouthowotherpeopledoforsimilarfigures. to the writing task. While machine-generated captions
...,Iwouldliketoseewhatpeoplementioninsimilardomainsof weregenerallydeemeduseful,ourresultssuggestedthat
papersforsimilarformatsoffigures.(P8)”Somedesiredatemplate thebenefitsofpresentingcontextual(e.g.,figure-mentioning
thattheycouldsimplyfillininformation:“...,youhavestandard paragraphs)oranalytical(e.g.,checktables,ratings)infor-
(templates),andyoujustneedtofillinthedetails.(P7)”,“...,you mationrelevanttothewritingtaskmightbemoreprevalent
wantsome,likestandardstructure.Justastructure,nottheexact (Figure3).
paragraphsthatyouneedtomention,...(P11)”. • Provideconcrete,actionablesuggestionsforwriting.
SciCapenteranalyzedthecurrentsituationwithoutoffer-
ingspecificguidanceonenhancingacaptionorproviding
fillablecaptiontemplates,leadingtomanyparticipantcom-
6 DISCUSSION
plaints(Section5).Futuredevelopersofwritingassistants
DesignRecommendations. Ourresearchindicatesthatwhilemachine- forcaptionwritingshouldaimtoprovidemoretargeted,ac-
generatedtextsmaynotalwaysmatchthequalityofhuman-written tionablesuggestions.Continuedresearchanddevelopment
content,theymightstillofferassistanceinwritingtasks.Toaddress arecrucialforthis.
thepotentialshortcomingsofmachine-generatedtexts,SciCapen-
terincorporatedanabundanceofcontextualinformation,such ComparisonofCaptionQuality. Thestudywasconductedbefore
ascaptiontraits,captionratings,explanationsfortheseratings, wehadaccesstoGPT-4V[43],oneofthestrongestLargeVision-
andreferencestorelevantfigures,toaidusers.Basedonourstudy LanguageModels(LVLM),raisinganinterestingquestion:CouldSciCapenter CHIEA’24,May11–16,2024,Honolulu,HI,USA
Figure4:Comparativeevaluationofcaptionqualitybythreeexperts,whereeachcaptiontype—GroundTruth,SummaryShort,
SummaryLong,andGPT-4V—isratedonascalefromrank1(highest)torank4(lowest).BothExpert1andExpert2rated
GroundTruthcaptionasrank1mostfrequently,whileExpert3hadapreferenceforSummaryShort.Notably,Expert3rated
GPT-4Vthelowest,rarelygivingitarank1,whereasbothExpert1andExpert2oftenconsideredGPT-4Vastheirsecond
choiceforrank1.ThevariationsinevaluationsreflectdifferingperspectivesoncaptionqualityandsuggestthatwhileGround
Truthcaptionsaregenerallypreferred,there’sasignificantdisparityinhoweachexpertratesthemachine-generatedcaptions.
GPT-4Vproducesignificantlybettercaptionsthanthemod- studyprocedurecouldstillintroducepotentialbiases.For
elsusedinSciCapenter,potentiallyalteringourstudy’s instance,participantshadtolearntheuncommontaskof
conclusions?Toexplorethis,werecruitedthreeprofessionalaca- writingcaptionsforotherpeople’spapersinthefirstcon-
demicpapereditorstoassessthequalityofcaptionsgeneratedby ditiontheyencountered.Furthermore,allparticipantsfirst
variousmodels. wrotethecaptionswithoutSciCapenterandthenusedSci-
FromtheSciCapChallengedataset[17],werandomlyselected Capenter,whichmayhaveamplifiedthesystem’sbenefits.
200figure-captionpairs.Foreachfigure,wegeneratedtwocaptions (3) OurstudyonlyusedNASA-TLX.Futureresearchcanextend
usingmodelsfromSciCapenter(SummaryShortandSummary themeasurestonotonlyNASA-TLXbuttoothermeasures,
Long),onecaptionusingGPT-4Vbypromptingitwiththefigure suchasasenseofauthorityorconfidencelevel,tounder-
imageandfigure-mentioningparagraphs,andwealsoincluded standthebenefitof SciCapenterinmultipleaspects.
theoriginalauthor-writtencaption(GroundTruth).Toensurea (4) Asapreliminarystudyandearlyprototype,thelackofcon-
faircomparisoninourstudy,GPT-4Vwaspromptedtogenerate siderationforspecificitemsfromtheCheckTableinboth
captionsnolongerthantheauthor-writtencaptions,acknowledg- generatedcaptionsandexplanationsforratingssuggestsa
ingthatreadersoftenfavorlongercaptions.Thislengthconstraint needforbetterintegrationwiththetool’sfeatures.
aimedtominimizetheinfluenceofcaptionlengthonqualityas- (5) Ourstudydidnotassessthequalityoftheresultingcap-
sessment. tions,leavingunansweredwhetherSciCapenterimproved
Werecruitedthreeprofessionaleditors(Expert1,2,and3)through captionquality.Thiswasduetothehighcostandneedfor
UpWork,5allspecializingintechnicalacademicarticlesandnative domainexpertstoevaluatecaptionusefulness.Furtherstud-
AmericanEnglishspeakers.Theirbackgroundsincludeonewith iesareneededtodeterminewritingassistants’impacton
overtenyearsofeditingexperienceandaPh.D.inComparative captionquality.
Literature,andtwofromtheSTEMfields—oneinTheoreticalAstro-
physicsandanotherinNeuroscience—withextensiveexperience 7 CONCLUSIONANDFUTUREWORK
inediting,proofreading,andpublishingacademicpapers.They
Inthispaper,wepresentSciCapenter,asystemdesignedtoassist
rankedthecaptionsbasedontheireffectivenessinclarifyingthe
authorsincraftingcaptionsforscientificfigureswithinscholarlyar-
figure’sintendedmessage.TheresultsareshowninFigure4.Under
ticles.Givenascholarlyarticle,SciCapenteridentifiesandextracts
thecaptionlengthconstraints,GPT-4VdidnotoutperformSci-
figures,isolatesparagraphsthatmentionthesefigures,generates
Capenter’smodelsdrastically.AlthoughExperts1and2preferred
captionsbasedontheseparagraphs,andthenratesthequalityof
GPT-4V’sgenerationoverthatof SciCapenter’smodels,Expert3
thesecaptions.OuruserstudyshowedthatSciCapenterreduced
didnotfavorit.
users’cognitiveloadwhilecomposingfigurecaptions.Considering
Limitations. OurworkdemonstratesthatAItechnologiescan thisisaninitialeffortinaidingcaptioncreation,ourfuturegoalis
supportcaptionwriting,yetweacknowledgethefollowinglimita- todevelopamoreaccessibleversionof SciCapenter,potentially
tions: intheformofawebbrowserplugin.Weaimtoconductabroader
deploymentstudytounderstandhowresearchersmightintegrate
(1) Ourstudyinvolvedparticipantswritingfigurecaptionsfor
thistoolintotheiracademicwriting.
otherpeople’spublishedpapersratherthancraftingcaptions
for their own work. This scenario diverged from typical
use cases of figure caption writing support systems. The REFERENCES
studywasdesignedthiswaytoaccommodatethedifficultyin [1] IzBeltagy,KyleLo,andArmanCohan.2019.SciBERT:Apretrainedlanguage
recruitingindividualswhoareactivelywritingnewscholarly modelforscientifictext.arXivpreprintarXiv:1903.10676(2019).
[2] JohnBransford.1979.Humancognition:Learning,understanding,andremem-
articles.
bering.(NoTitle)(1979).
(2) Despiteeffortstomitigatelearningeffectsbyalteringthe [3] Yu-YingChangandJohnMSwales.2014.InformalelementsinEnglishacademic
conditionorderbasedonthepilotstudy(AppendixB),our writing:threatsoropportunitiesforadvancednon-nativespeakers?InWriting:
Texts,processesandpractices.Routledge,145–167.
[4] CharlesChen,RuiyiZhang,EunyeeKoh,SungchulKim,ScottCohen,andRyan
5UpWork:https://www.upwork.com/ Rossi.2020.Figurecaptioningwithrelationmapsforreasoning.InProceedingsofCHIEA’24,May11–16,2024,Honolulu,HI,USA TrovatoandTobin,etal.
theIEEE/CVFWinterConferenceonApplicationsofComputerVision.1537–1545. [25] ZebaKarishma,ShauryaRohatgi,KavyaShrinivasPuranik,JianWu,andCLee
[5] JiwonChoiandJaeminJo.2022.Intentable:Amixed-initiativesystemforintent- Giles.2023.ACL-Fig:ADatasetforScientificFigureClassification.arXivpreprint
basedchartcaptioning.In2022IEEEVisualizationandVisualAnalytics(VIS). arXiv:2301.12293(2023).
IEEE,40–44. [26] DaeHyunKim,VidyaSetlur,andManeeshAgrawala.2021. TowardsUnder-
[6] JinhoChoi,SanghunJung,DeokGunPark,JaegulChoo,andNiklasElmqvist. standingHowReadersIntegrateChartsandCaptions:ACaseStudywithLine
2019. Visualizingforthenon-visual:Enablingthevisuallyimpairedtouse Charts.InProceedingsofthe2021CHIConferenceonHumanFactorsinComputing
visualization.InComputerGraphicsForum,Vol.38.WileyOnlineLibrary,249– Systems(CHI’21).
260. [27] DaeHyunKim,VidyaSetlur,andManeeshAgrawala.2021.Towardsunderstand-
[7] ChristopherClarkandSantoshDivvala.2016.Pdffigures2.0:Miningfiguresfrom inghowreadersintegratechartsandcaptions:Acasestudywithlinecharts.In
researchpapers.InProceedingsofthe16thACM/IEEE-CSonJointConferenceon Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems.
DigitalLibraries.143–152. 1–11.
[8] SusanMConrad.1996. Investigatingacademictextswithcorpus-basedtech- [28] PhilippKoehnandBarryHaddow.2009.Interactiveassistancetohumantrans-
niques:Anexamplefrombiology.Linguisticsandeducation8,3(1996),299–326. latorsusingstatisticalmachinetranslationmethods.InProceedingsofMachine
[9] IgnacioGarciaandMaríaIsabelPena.2011.Machinetranslation-assistedlan- TranslationSummitXII:Papers.
guagelearning:writingforbeginners.ComputerAssistedLanguageLearning24, [29] VivianLai,SamuelCarton,RajatBhatnagar,QVeraLiao,YunfengZhang,and
5(2011),471–487. ChenhaoTan.2022.Human-aicollaborationviaconditionaldelegation:Acase
[10] YasheshGaur,WalterSLasecki,FlorianMetze,andJeffreyPBigham.2016.The studyofcontentmoderation.InProceedingsofthe2022CHIConferenceonHuman
effectsofautomaticspeechrecognitionqualityonhumantranscriptionlatency. FactorsinComputingSystems.1–18.
InProceedingsofthe13thInternationalWebforAllConference.1–8. [30] AndrewLarge,JamshidBeheshti,AlainBreuleux,andAndreRenaud.1995.Multi-
[11] LiGong,JosepCrego,andJeanSenellart.2019.EnhancedTransformerModel mediaandcomprehension:Therelationshipamongtext,animation,andcaptions.
forData-to-TextGeneration.InProceedingsofthe3rdWorkshoponNeuralGener- JournaloftheAmericansocietyforinformationscience46,5(1995),340–347.
ationandTranslation,AlexandraBirch,AndrewFinch,HiroakiHayashi,Ioan- [31] KentonLee,MandarJoshi,IuliaRalucaTurc,HexiangHu,FangyuLiu,JulianMar-
nisKonstas,ThangLuong,GrahamNeubig,YusukeOda,andKatsuhitoSu- tinEisenschlos,UrvashiKhandelwal,PeterShaw,Ming-WeiChang,andKristina
doh(Eds.).AssociationforComputationalLinguistics,HongKong,148–156. Toutanova.2023. Pix2struct:Screenshotparsingaspretrainingforvisuallan-
https://doi.org/10.18653/v1/D19-5615 guageunderstanding.InInternationalConferenceonMachineLearning.PMLR,
[12] SandraGHart.2006. NASA-taskloadindex(NASA-TLX);20yearslater.In 18893–18912.
Proceedingsofthehumanfactorsandergonomicssocietyannualmeeting,Vol.50. [32] ShengzhiLiandNimaTajbakhsh.2023. Scigraphqa:Alarge-scalesynthetic
SagepublicationsSageCA:LosAngeles,CA,904–908. multi-turnquestion-answeringdatasetforscientificgraphs. arXivpreprint
[13] MaryHegartyandMarcel-AdamJust.1993. Constructingmentalmodelsof arXiv:2308.03349(2023).
machinesfromtextanddiagrams.Journalofmemoryandlanguage32,6(1993), [33] YannaLin,HaotianLi,LeniYang,AoyuWu,andHuaminQu.2023. Inksight:
717–742. Leveragingsketchinteractionfordocumentingchartfindingsincomputational
[14] JohnHinds,UConnor,andRBKaplan.1987.Readerversuswriterresponsibility: notebooks.IEEETransactionsonVisualizationandComputerGraphics(2023).
Anewtypology.LandmarkessaysonESLwriting(1987),63–74. [34] CanLiu,YuhanGuo,andXiaoruYuan.2023. Autotitle:Aninteractivetitle
[15] SameeraHorawalavithana,SaiMunikoti,IanStewart,andHenryKvinge.2023. generatorforvisualizations.IEEETransactionsonVisualizationandComputer
Scitune:Aligninglargelanguagemodelswithscientificmultimodalinstructions. Graphics(2023).
arXivpreprintarXiv:2307.01139(2023). [35] FangyuLiu,FrancescoPiccinno,SyrineKrichene,ChenxiPang,KentonLee,
[16] Ting-YaoHsu,CLeeGiles,andTing-HaoHuang.2021. SciCap:Generating MandarJoshi,YaseminAltun,NigelCollier,andJulianMartinEisenschlos.2022.
CaptionsforScientificFigures.InFindingsoftheAssociationforComputational Matcha:Enhancingvisuallanguagepretrainingwithmathreasoningandchart
Linguistics:EMNLP2021.AssociationforComputationalLinguistics,PuntaCana, derendering.arXivpreprintarXiv:2212.09662(2022).
DominicanRepublic,3258–3264. https://doi.org/10.18653/v1/2021.findings- [36] AnitaMahinpei,ZonaKostic,andChrisTanner.2022. Linecap:Linecharts
emnlp.277 fordatavisualizationcaptioningmodels.In2022IEEEVisualizationandVisual
[17] Ting-YaoHsu,Yi-LiHsu,ShauryaRohatgi,RyanRossi,SungchulKim,Ani Analytics(VIS).IEEE,35–39.
Nenkova,Lun-WeiKu,HuijuanXu,C.Giles,andTing-HaoHuang.2023.The1st [37] AhmedMasry,ParsaKavehzadeh,XuanLongDo,EnamulHoque,andShafiq
ScientificFigureCaptioning(SciCap)Challenge.http://scicap.ai/. Joty.2023.UniChart:AUniversalVision-languagePretrainedModelforChart
[18] Ting-YaoHsu,Chieh-YangHuang,RyanRossi,SungchulKim,C.Giles,andTing- ComprehensionandReasoning.arXivpreprintarXiv:2305.14761(2023).
HaoHuang.2023.GPT-4asanEffectiveZero-ShotEvaluatorforScientificFigure [38] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque.
Captions.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2023, 2022.ChartQA:Abenchmarkforquestionansweringaboutchartswithvisual
HoudaBouamor,JuanPino,andKalikaBali(Eds.).AssociationforComputational andlogicalreasoning.arXivpreprintarXiv:2203.10244(2022).
Linguistics,Singapore,5464–5474. https://doi.org/10.18653/v1/2023.findings- [39] FlorianMathis,JohnHWilliamson,KamiVaniea,andMohamedKhamis.2021.
emnlp.363 Fastandsecureauthenticationinvirtualrealityusingcoordinated3dmanipula-
[19] Chieh-YangHuang,Ting-YaoHsu,RyanRossi,AniNenkova,SungchulKim, tionandpointing.ACMTransactionsonComputer-HumanInteraction(ToCHI)28,
GromitYeuk-YinChan,EunyeeKoh,ClydeLeeGiles,andTing-Hao’Kenneth’ 1(2021),1–44.
Huang.2023.SummariesasCaptions:GeneratingFigureCaptionsforScientific [40] GwenCNugent.1983.Deafstudents’learningfromcaptionedinstruction:The
DocumentswithAutomatedTextSummarization.arXivpreprintarXiv:2302.12324 relationshipbetweenthevisualandcaptiondisplay. TheJournalofSpecial
(2023). Education17,2(1983),227–234.
[20] KVJobin,AjoyMondal,andCVJawahar.2019.Docfigure:Adatasetforscientific [41] JasonObeidandEnamulHoque.2020.Chart-to-Text:GeneratingNaturalLan-
documentfigureclassification.In2019InternationalConferenceonDocument guageDescriptionsforChartsbyAdaptingtheTransformerModel.InProceedings
AnalysisandRecognitionWorkshops(ICDARW),Vol.1.IEEE,74–79. ofthe13thInternationalConferenceonNaturalLanguageGeneration,BrianDavis,
[21] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan.2018. Dvqa: YvetteGraham,JohnKelleher,andYajiSripada(Eds.).AssociationforComputa-
Understandingdatavisualizationsviaquestionanswering.InProceedingsofthe tionalLinguistics,Dublin,Ireland,138–147.https://doi.org/10.18653/v1/2020.inlg-
IEEEconferenceoncomputervisionandpatternrecognition.5648–5656. 1.20
[22] SamiraEbrahimiKahou,VincentMichalski,AdamAtkinson,ÁkosKádár,Adam [42] OpenAI.2022.GPT-3.5:LanguageModelsareFew-ShotLearners.https://platform.
Trischler,andYoshuaBengio.2017.Figureqa:Anannotatedfiguredatasetfor openai.com/docs/models/gpt-3-5.
visualreasoning.arXivpreprintarXiv:1710.07300(2017). [43] OpenAI.2023. GPT-4V(ision)SystemCard. https://api.semanticscholar.org/
[23] ShankarKantharaj,XuanLongDo,RixieTiffanyLeong,JiaQingTan,Enamul CorpusID:263218031
Hoque,andShafiqJoty.2022.OpenCQA:Open-endedQuestionAnsweringwith [44] WilliamDPage.1974.Theauthorandthereaderinwritingandreading.Research
Charts.InProceedingsofthe2022ConferenceonEmpiricalMethodsinNatural intheTeachingofEnglish8,2(1974),170–183.
LanguageProcessing,YoavGoldberg,ZornitsaKozareva,andYueZhang(Eds.). [45] XinQian,EunyeeKoh,FanDu,SungchulKim,JoelChan,RyanARossi,Sana
AssociationforComputationalLinguistics,AbuDhabi,UnitedArabEmirates, Malik,andTakYeonLee.2021. Generatingaccuratecaptionunitsforfigure
11817–11837. https://doi.org/10.18653/v1/2022.emnlp-main.811 captioning.InProceedingsoftheWebConference2021.2792–2804.
[24] ShankarKantharaj,RixieTiffanyLeong,XiangLin,AhmedMasry,MeghThakkar, [46] NoahSiegel,ZacharyHorvitz,RoieLevin,SantoshDivvala,andAliFarhadi.2016.
EnamulHoque,andShafiqJoty.2022.Chart-to-Text:ALarge-ScaleBenchmarkfor Figureseer:Parsingresult-figuresinresearchpapers.InComputerVision–ECCV
ChartSummarization.InProceedingsofthe60thAnnualMeetingoftheAssociation 2016:14thEuropeanConference,Amsterdam,TheNetherlands,October11–14,2016,
forComputationalLinguistics(Volume1:LongPapers),SmarandaMuresan,Preslav Proceedings,PartVII14.Springer,664–680.
Nakov,andAlineVillavicencio(Eds.).AssociationforComputationalLinguistics, [47] AshishSingh,PrateekAgarwal,ZixuanHuang,ArpitaSingh,TongYu,Sungchul
Dublin,Ireland,4005–4023. https://doi.org/10.18653/v1/2022.acl-long.277 Kim,VictorBursztyn,NikosVlassis,andRyanARossi.2023. FigCaps-HF:A
Figure-to-CaptionGenerativeFrameworkandBenchmarkwithHumanFeedback.
arXivpreprintarXiv:2307.10867(2023).SciCapenter CHIEA’24,May11–16,2024,Honolulu,HI,USA
[48] ChaseStokes,VidyaSetlur,BridgetCogley,ArvindSatyanarayan,andMartiA • ThetimeconstraintforCondition3andCondition4inthe
Hearst.2022.Strikingabalance:readertakeawaysandpreferenceswheninte- pilotstudywassettofiveminutes.Wefoundthatpartici-
gratingtextandcharts.IEEETransactionsonVisualizationandComputerGraphics
pantswerequitestressedandcouldbarelyfinishthecaption.
29,1(2022),1233–1243.
[49] BennyJTang,AngieBoggust,andArvindSatyanarayan.2023.Vistext:Abench- So,weextendedthetimelimitto8minutes.
markforsemanticallyrichchartcaptioning. arXivpreprintarXiv:2307.05356 • GPT-4wasusedforautomaticcaptionevaluation(Figure1.F
(2023).
[50] YananWangandYea-SeulKim.2023.Makingdata-drivenarticlesmoreaccessible: andFigure1.G)inthepilotstudy.However,theAPI’sre-
Anactivepreferencelearningapproachtodatafactpersonalization.InProceedings sponsetimewasquitehigh.Thus,wechangedtoGPT-3.5-
ofthe2023ACMDesigningInteractiveSystemsConference.1353–1366. turbointhemainstudy.
[51] ZhishenYang,RajDabre,HidekiTanaka,andNaoakiOkazaki.2023.SciCap+:
AKnowledgeAugmentedDatasettoStudytheChallengesofScientificFigure
Captioning.arXivpreprintarXiv:2306.03491(2023).
[52] JiaboYe,AnwenHu,HaiyangXu,QinghaoYe,MingYan,YuhaoDan,Chenlin
Zhao,GuohaiXu,ChenliangLi,JunfengTian,etal.2023.mplug-docowl:Modu-
larizedmultimodallargelanguagemodelfordocumentunderstanding. arXiv
preprintarXiv:2307.02499(2023).
A PROMPTSUSED
ThefollowingisthepromptusedbySciCapentertogeneratethe
captionratinganditsexplanation(Figure1.FandFigure1.G):
Giventheparagraphandcaptionbelow,please
rate the level of usefulness of the caption
from 1 to 6 based on how well the caption
could help readers understand the important
information. 6 is the highest; 1 is the
lowest. Please also explain your rating.
Paragraph: “[paragraph]”
Caption: “[caption]”
where[paragraph]and[caption]aretheplaceholderforthe
referredparagraphandwrittencaption,respectively.Notethatwe
chooseGPT-3.5-turbooverGPT-4duetoitshigherstabilityand
shorterresponsetime.
B PILOTSTUDYANDCHANGESMADE
Thefirstthreestudytrialsweretreatedasthepilotstudy.Twoofthe
participantsmajoredinInformatics,andoneinComputerScience.
The pilot study yielded several valuable insights that informed
adjustmentsmadeinourmainstudy:
• Inthepilotstudy,thetasksequencewasordered2->4->1->
3,withtheSciCapenterconditionsproceedingfirst.Weob-
servedthatafterparticipantsviewedthesixaspectsreported
inthechecktable(Figure1.E),theirwritingswereinfluenced,
forexample,toincludetakeawaymessagesandfigures’vi-
sualfeatures.Tominimizethisinfluence,wechangedthe
taskorderbyaskingparticipantstoproceedwiththecondi-
tionswithoutSciCapenterfirstintheformalstudy.Wealso
includedextrainstructionsclarifyingthatthesixaspectsare
suggestions,notrequirements.
• Inthepilotstudy,therating’sexplanation(Figure1.G)was
onlyvisiblewhenusershoveredoverasmallbutton.We
foundthatparticipantsfrequentlyignoredthisexplanation.
Asaresult,wemodifiedtheinterfacetodisplaytheexplana-
tiondirectly.
• The missing aspects in the check table (Figure 1.E) were
originallytaggedwiththeicon“X”.Usersexpressedconcerns
aboutsuchastrongrejectionsymbol.We,therefore,replaced
itwithatrianglealertsymbol.