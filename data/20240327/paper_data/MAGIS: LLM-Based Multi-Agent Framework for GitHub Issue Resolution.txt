MAGIS: LLM-Based Multi-Agent Framework
for GitHub Issue ReSolution
WeiTao YuchengZhou
FudanUniversity UniversityofMacau
wtao18@fudan.edu.cn yucheng.zhou@connect.um.edu.mo
WenqiangZhang YuCheng
FudanUniversity RiceUniversity
wqzhang@fudan.edu.cn yc180@rice.edu
Abstract
Insoftwareevolution,resolvingtheemergentissueswithinGitHubrepositories
isacomplexchallengethatinvolvesnotonlytheincorporationofnewcodebut
alsothemaintenanceofexistingfunctionalities. LargeLanguageModels(LLMs)
haveshownpromiseincodegenerationandunderstandingbutfacedifficultiesin
codechange,particularlyattherepositorylevel. Toovercomethesechallenges,we
empiricallystudythereasonwhyLLMsmostlyfailtoresolveGitHubissuesand
analyzesomeimpactfactors. Motivatedbytheempiricalfindings,weproposea
novelLLM-basedMulti-AgentframeworkforGitHubIssuereSolution,MAGIS,
consistingoffourkindsofagentscustomizedforthesoftwareevolution: Manager,
RepositoryCustodian,Developer,andQualityAssuranceEngineeragents. This
frameworkleveragesthecollaborationofvariousagentsintheplanningandcoding
processtounlockthepotentialofLLMstoresolveGitHubissues. Inexperiments,
weemploytheSWE-benchbenchmarktocompareMAGISwithpopularLLMs,
includingGPT-3.5,GPT-4,andClaude-2. MAGIScanresolve13.94%GitHubis-
sues,whichsignificantlyoutperformsthebaselines. Specifically,MAGISachieves
aneight-foldincreaseinresolvedratiooverthedirectapplicationofGPT-4,the
basedLLMofourmethod. WealsoanalyzethefactorsforimprovingGitHubissue
resolutionrates,suchaslinelocation,taskallocation,etc.
1 Introduction
Inreal-worldsoftwaredevelopment,thesoftwareapplicationisrarelysetinstone. High-qualityand
popularsoftwarecontinuallyevolvestoaddressemergentbugsoradapttonewrequirementsarising
from shifts in user needs, software environments, and physical hardware. On platforms such as
GitHub1,issuestypicallysignifytherequirementforsoftwareevolution. However,addressingthese
issuesposessignificantchallenges,asitrequiresimplementingthecodechangeacrosstheentire
repositoryandmaintainingtheexistingfunctionalitywhileintegratingnewcapabilities.Consequently,
resolvingGitHubissuesremainsasignificantchallengeacrossacademiaandindustry[15,4].
Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of
tasks[6],includingcodegenerationandcodeunderstanding[40,30]. Specifically,LLMsexcelin
generatingfunction-levelcode,asevidencedbytheirperformanceonnumerousbenchmarkdatasets
suchasMBPP[2]andHumanEval[9]. Despitetheirsuccess,LLMsremainchallengedintasksthat
requireadvancedcodegenerationcapabilities,suchastheClassEvalbenchmark[10]. Moreover,
1https://github.com
Preprint.
4202
raM
62
]ES.sc[
1v72971.3042:viXraLLMsexhibitlimitationsinprocessingexcessivelylongcontextinputsandaresubjecttoconstraints
regardingtheirinputcontextlength[20]. Thislimitationisparticularlyevidentinrepository-level
tasks,suchassolvingGitHubissues,wherethecontextcomprisestheentirerepository,thusimposing
constraintsondirectlyusingthefullrepositoryasinputtoLLMs.
ToharnessthefullpotentialofLLMs,researchers[12,26,36]havedesignedLLM-basedmulti-agent
systems. TheseapproacheshavesignificantlyimprovedLLMs’efficacyincodegeneration,enabling
thesesystemstoconstructcoderepositoriesbasedonLLM.Whilethesemethodsaddresstheprocess
oftransitioningcoderepositoriesfrominceptiontoestablishment,theyrarelyconsiderthehandling
of software evolution, e.g., resolving Github issues. For Github repositories, especially popular
ones,alargenumberofcommitsarepushedeveryday. Thesecommitsderivefromaspectrumof
evolutionaryrequirementsthatspanbugfixes,featureadditions,performanceenhancements,etc[32].
Foropen-sourcesoftware,newrequirementsfrequentlyemergeasissuesintheproject’srepository.
ToinvestigatethecapabilityofLLMsinaddressingGitHubissues,Jimenezetal.[15]developeda
benchmark,namelySWE-bench,toconductacomprehensiveanalysisofpopularLLMs. Thestudy
revealsthatLLMsfailtoresolveover5%ofinstances,evenwhenprovidedwithfilepathsthatrequire
modifications. Thissignificantlylowrateofapplyingsolutionsandresolvingissuesunderscores
thelimitationsinherentinLLMs. ThereasonsbehindthesuboptimalperformanceofLLMsand
strategiestoharnesstheirpotentialforGitHubissueresolutionremainunder-explored.
Inthisstudy,weanalyzethefactorsimpactingtheeffectivenessofLLMsinresolvingissuesunder
two different settings (i.e., w/ Oracle and w/o Oracle). Furthermore, our empirical analysis has
concluded a correlation between file location and line location within files, and performance in
resolving GitHub issues. Based on these insights, we propose a novel LLM-based Multi-agent
Framework, termed MAGIS, comprising four types of agents: Manager, Repository Custodian,
Developer,andQualityAssurance(QA)Engineer. OurapproachfacilitatestheresolutionofGitHub
issuesthroughcollaborationamongagents,eachfulfillingauniquerole.
Inourexperiment,weevaluateourframeworkontheSWE-bench,comparingitsperformanceagainst
existing popular LLMs, such as ChatGPT-3.5 [24], GPT-4 [25], and Claude-2 [1]. The results
demonstratethatourframework,utilizingGPT-4asitsbasemodel,significantlyoutperformsthe
baseline,andachievesaneight-foldperformancegaincomparedtothedirectapplicationofGPT-4.
Furtheranalysisrevealedadditionalfactors,i.e.,theplanningofcodechange,linelocationwithinthe
codefile,andcodereviewprocess,thatsignificantlyinfluencetheresolutionrate.
Ourmaincontributionsaresummarizedasfollows:
• We conduct an empirical analysis of LLMs in resolving GitHub issues in two settings (i.e., w/
Oraclesettingandw/oOraclesetting),andexplorethecorrelationbetweenlinelocation,complexity
ofthecodechange,filelocation,andthesuccessrateinresolvingGitHubissues.
• We propose a novel LLM-based multi-agent framework, MAGIS, to alleviate the limitations
of existing LLMs on GitHub issue resolution. Both our designed four-type agents and their
collaborationforplanningandcodingunlockLLMs’potentialontherepository-levelcodingtask.
• WecompareourframeworkandotherstrongLLMcompetitors(i.e.,GPT-3.5,GPT-4,andClaude-2)
ontheSWE-benchdataset. TheresultsshowMAGISsignificantlyoutperformsthesecompetitors.
Furtheranalysisisconductedandverifiestheeffectivenessandnecessityofourframeworkdesign.
2 EmpiricalStudy
2.1 RQ1: WhatImpactsthePerformanceUnderWith-OracleSetting?
Thewith-OraclesettingisamorestraightforwardsetupfortheGitHubissueresolutionbysupplying
thespecificfilesrequiringmodifications. DespitetheobservationthatLLMsexhibitbetterperfor-
mance in the with-Oracle setting compared to their performance without Oracle, the proportion
ofissuessuccessfullyresolvedunderthewith-Oraclesettingremainsmodest. Specifically,inthe
with-Oraclesetting,GPT-4achievedaresolvedrateofonly1.74%ontheSWE-bench.
Todelveintothefactorsimpactingtheefficacywithinthewith-Oraclesetting,weexaminedfailed
instances’generation. Thisscrutinyledtotheidentificationoftwofactorsthatcouldinfluencethe
resolvedrate: (1)Linelocation(locationofthemodifiedlineinthecodefile),and(2)Thecomplexity
2ofthecodechange. Thisinformationisimportantandprovidedinthecodechangeofeachcommitin
thecoderepository. Atypicalcodechangeincludesmultiplechangedhunksandeachhunkcontains
thelinenumberstargetedformodificationandthespecificsofthechangedcodeattheselocations.
Boththelinelocationsandthechangedcontentmakearepositoryevolve.
Tovalidatethisidentification,weanalyzetherelationbetweentheresolvedrateandthetwofactors
mentionedabove.
2.1.1 LineLocation
Toquantitativelyanalyzetheaccuracyoflinelocalization,weusethelinenumbers’rangeofthe
modifiedcontentinthereferencecodechangeasthebasisassumingthatthecorrectmodification
locationofthecodechangeisuniquelydeterminedinmostcases. Bycalculatingtheoverlapratio
of the line number ranges of the generated and reference, we can estimate the accuracy of line
localizationinthegenerationprocess. Specifically,foreachinstance,thelinenumberrangesofthe
codechangeinthereferencerisrepresentedasasetofintervalsL ={[s ,e ],...,[s ,e ]},while
r 0 0 n n
thelinenumberrangesetofthegeneratedcodechangegisL ={[s′,e′],...,[s′ ,e′ ]},wheres
g 0 0 m m
anderespectivelyrepresentthestartingandendinglinenumberofeachmodificationhunkinthefile,
withnhunksinthereferencecodechangeandminthegeneratedone. Theformulaforcalculating
theoverlapratioisasfollows:
OverlapRatio=
(cid:80)n i=0(cid:80)m j=0(cid:12) (cid:12)[s i,e i]∩[s′ j,e′ j](cid:12) (cid:12)
, (1)
(cid:80)n
(e −s +1)
i=0 i i
(cid:12) (cid:12)
where(cid:12)[s i,e i]∩[s′ j,e′ j](cid:12)denotesthesizeoftheintersectionbetweentheintervals[s i,e i]and[s′ j,e′ j].
For574instancesintheSWE-benchtestsetthatexperimentsGPT-4,thedistributionoftheoverlap
ratio between the results generated by three LLMs and the reference code change is shown in
Figure1withtheverticalaxisrepresentingthefrequencyoftherangeoflinelocationoverlapratio
foreachgroup,andthehorizontalaxisrepresentingtheoverlapratio. Fromthis,weobservethat:
(1)Thedistributionneartheoverlapratio0(leftsideofthefigure)isthehighestforallthreeLLMs,
indicatingthatinmostcases, thecontentgeneratedbythesemodelshasaverylowoverlapratio
withthereferenceintermsoflinelocation. ThismeansthattheseLLMsaremostlikelynotable
toaccuratelylocatethelinesofcodethatneedtobemodifiedintheprocessofgeneratingthecode
change. (2)Inthedistributionnearthelinelocationoverlapof1(rightsideofthefigure),thethree
models show a consistent ranking (i.e., Claude-2 > GPT-4 > GPT-3.5) and this ranking is also
consistentwiththeproportionofinstancessolvedbythethreemodels. Thisphenomenonsuggests
thattheperformanceofLLMsingeneratingthecodechangeisprobablyrelatedtotheirabilityto
locatecodelinesaccurately.
Furthermore,weassesstherelationshipbetweentheoverlapratioandtheresolutionofGitHubissues
bycalculatingtheircorrelationcoefficient. Giventhatthedistributionofthesevariablesexhibits
skewness,andtheresolutionresultisbinary(re-
solvedornot),logisticregressionisemployed
 * 3 7    
for the analysis across three LLMs. However,    * 3 7  
 & O D X G H  
duetothelimitednumberofsuccessfullygen-  
eratedinstancesonGPT-4andGPT-3.5,astatis-
 
ticallysignificantrelationship(P-value<0.05)
isdetectedsolelyinthegeneratedresultfrom  
Claude-2. Thissignifiesthatthecorrelationco-
 
efficientfromClaude-2isbothstatisticallysig-
nificant and meaningful. Specifically, with a  
positivecoefficient,0.5997,onClaude-2,there
 
is a substantial and positive relation between
improvementsintheoverlapratioandtheprob-  
                       
abilityofsuccessfullyresolvingissues. Conse-
Figure1: TheComparisonofLineLocationOver-
quently,thelinelocationemergesasanimpor-
lapRatiobetweenThreeLLMsOnSWE-bench.
tantfactorforGitHubissueresolution.
32.1.2 ComplexityoftheCodeChange
Thecomplexityofthecodechangeisreflectedinvariousindices,includingthenumberofmodified
files,functions,hunks,andlinesaddedordeleted. Firstly,wequantitativelyassessthecomplexityby
calculatingthevalueofvariousindicescorrespondingtothereferencecodechange. Secondly,the
coefficientiscalculatedbetweenthenumbersineachindexandtheresolutionofissues.Table1shows
thecorrelationscoresbetweensixindicesandtheissueresolution,underthelogisticregression.
Table1: CorrelationbetweentheComplexityIndicesandtheIssueResolution.
LLM #Files #Functions #Hunks #AddedLoC #DeletedLoC #ChangedLoC
GPT-3.5 −17.57* −17.57* −0.06* −0.02 −0.03 −0.53*
GPT-4 −25.15* −25.15* −0.06 −0.10 −0.04 −0.21
Claude-2 −1.47* −1.47* −0.11* −0.09* −0.07* −0.44*
* Thecorrelationbetweentheindexandtheissueresolutionissignificant(P-value<0.05).
AsshowninTable1,allthreeLLMs,i.e.,GPT-3.5,GPT-4,andClaude-2,demonstrateastatistically
significantcorrelationwiththeissueresolutionacrossseveralindices,i.e.,p-valuelessthan0.05
andmarkedby∗. Thecorrelationscoresforthenumberoffilesandfunctionsmodifiedarenotably
negativeforallmodels,indicatingthatanincreaseintheseindicesisassociatedwithadecreasing
likelihoodofissueresolution. Thissuggeststhatthemorecomplexthecodechange,asindicatedby
ahighernumberoffilesandfunctionsmodified,mayhindertheissueresolution. Comparedwith
GPT-3.5andGPT-4,Claude-2exhibitsadifferentpattern,withmuchlowernegativecorrelationsfor
thenumberoffilesandfunctions,whichindicatesitisamoreefficientapproachtogeneratethecode
changeforGitHubissueresolution. However,italsoshowssignificantnegativecorrelationsacross
otherindicessuchasthenumberofhunks,addedlinesofcode(LoC),deletedLoC,andchanged
LoC.Theanalysisrevealsarelationshipbetweenthecomplexity,asmeasuredbyseveralindices,and
whethertosuccessfullyresolvetheissuesinsoftwareevolution. Thenegativecorrelationssuggest
thatincreasedcomplexity,particularlyintermsofthenumberoffilesandfunctionschanged,tendsto
hinderissueresolution.
2.2 RQ2: WhatImpactsthePerformanceUnderWithout-OracleSetting?
Thedifferencebetweenthewith-Oraclesettingandthewithout-Oraclesettingliesinthenecessity
forthelattertoidentifythemodifiedfiles. Consequently,filelocatingemergesasacrucialfactor
influencingperformanceunderthewithout-Oraclesetting.
Jimenezetal.[15]employtheBM25method[28]toretrieverelevantcodefilesthataresubsequently
utilizedasinputtotheLLM.Afteremployingretrievalmethods,itisnecessarytoselectthetop-K
filesortruncatethecontentbasedonthemaximumcontextlengthoftheLLM.Incorporatingmore
filescanenhancerecallscores. However,italsoimposessignificantdemandsonthecapabilitiesof
LLMs. Asdemonstratedbythestudy[15],Claude-2exhibitsareductionattheresolvedratioasrecall
scoresincrease. Thisdeclinemaybeattributabletotheinclusionofirrelevantfilesorthelimited
capacityofLLMstoprocesslongercontextseffectively. Consequently,exploitingtheperformanceof
LLMscanbebetterachievedbystrivingforhigherrecallscoreswithaminimizedsetoffiles,thus
suggestingastrategicbalancebetweenrecalloptimizationandthenumberofchosenfiles.
3 Methodology
Inthissection,weelaborateonourLLM-basedmulti-agentframework,MAGIS,forGitHubissue
resolution.Theframeworkresolveseachissuebyapplyingthegeneratedcodechangetotherepository.
AsshowninFigure2,wefirstprovidetheoverviewofourframework(§3.1),followedbytherole
designofeachtypeagentandhowtheserolesdifferfromhumanworkflows(§3.2). Lastly,wedetail
thecollaborativeprocessamongtheseroleswithinourframework(§3.3).
3.1 Overview
ThearchitectureofourframeworkisillustratedinFigure2,encompassingfourkeyrolesofagents
workingcollaborativelyintheworkflow.
4Project Repository
Manager Custodian Review
GitHub Issue
Code
Human
Human Locate Kick-off
Code Files Build a Team Meeting Developer Quality
Assurance
Repository Planning Coding New Repository
Figure2: OverviewofOurFramework,MAGIS.
• Manager. Thisroletaskswithteamassembly,meetingorganization,andplanformulation.
• RepositoryCustodian. Thecustodianisresponsibleforlocatingtherelevantfilesintherepository
accordingtotheGitHubissueandrecordingthechangeoftherepository.
• Developer. Thisroleparticipatesinplanningdiscussionsandcompletestasksfromthemanager.
• QualityAssurance(QA)Engineer. TheQAengineerreviewsthecodechangefromdevelopersto
ensurethequalityofthewholerepository.
Thecollaborativeprocessinvolvesplanningandcoding. Intheplanning,aGitHubissueisassigned
totheprojectmanagerandtherepositorycustodian. Therepositorycustodianidentifiescandidate
filesrelevanttotheissueformodification. Withtheissuedescriptionandalistofcandidatefiles,the
managerdefinestasksandassemblesateam,whereeachmemberisadeveloperspecificallydesigned
forthedefinedtask. Themanagerholdsakick-offteammeetingwithdevelopersanddevisesaplan.
Duringthecoding,developersundertaketheirassignedtasksfromthemanager,andtheQAengineer
reviewseachcodechange. Ifachangefailstomeetqualitystandards,theQAengineerprovides
feedback,promptingfurtherrevisionsuntilthechangesatisfiesqualitycriteriaorasetiterationlimit
isreached.
3.2 AgentRoleDesign
OurworkflowdrawsinspirationfromtheGitHubFlow2,aneffectivehumanworkflowparadigm,
adoptedbymanysoftwareteams. BoththehumanworkflowandourLLM-basedagentframework
prioritizecollaborationamongindividualswithdiverseskills. Whiletheunderlyingprinciplesare
similar,therearenotabledifferences. Accordingly,wehavetailoredtherolesasfollows:
• Manager. The manager’s role is pivotal in planning. In conventional setups, managers
decompose the issue into tasks according to the pre-formed team and allocate these tasks for
memberswithdifferentskills. Incontrast,ourmanageragentcanfirstdecomposetheissueinto
tasksandthendesigndeveloperagentstoformateam. Thissetupimprovesteamflexibilityand
adaptability,enablingtheformationofteamsthatcanmeetvariousissuesefficiently.
• Repository Custodian. Considering extensive files in a repository, the custodian agent’s
taskistolocatefilesrelevanttotheissue. Differentfromhumans,whocanbrowsethroughthe
entirerepository,theLLM-basedagentfaceschallengesinbrowsing. AlthoughLLMs,including
GPT-4[23],haveextendedcontextlimits,theirapplicationisconstrainedintwoaspects. First,itis
ahighcomputationalcosttoquerythewholerepositoryforeachupdatewhilesomerepositories
updatefrequently. Second,theperformanceofLLMsdegradeswhenrelevantinformationoccurs
inthemiddleofthelongcontextinput[19,42].
2https://docs.github.com/en/get-started/using-github/github-flow
5
New
Branch Commit
Pull
Request Merge• Developer. Incontrasttohumandevelopers,thedeveloperagentcanworkcontinuously,and
complete a task in minutes even seconds. Therefore, scheduling the agent to work in parallel
is easier than scheduling humans as the latter requires considering other than work while the
formeronlyneedstoensurethetasksareindependent. Additionally,althoughtherearenumerous
developeragentscapableofgeneratingcode[12,26],theirabilitytomodifyexistingcodeisnot
equallyproficient. Toaddressthisissue,ourframeworkdecomposesthecodemodificationprocess
intoatomicoperations,whichencompassthecodegeneration. Thisapproachenablesdevelopersto
leveragethebenefitsofautomaticcodegenerationtherebygeneratingapplicablecodechanges.
• QAEngineer. Insoftwareevolution,QAengineers,especiallycodereviewers,playacrucial
role in maintaining software quality [21, 17]. Despite its critical role, code review practices
are often undervalued or even overlooked [3]. Such neglect can hinder software development,
illustratedbyinstanceswheredevelopersmayexperiencedelaysofupto96hoursawaitingcode
reviewfeedback[5]. Toaddressthisproblem,ourframeworkpairseachdeveloperagentwithaQA
engineeragent,designedtooffertask-specific,timelyfeedback. ThispersonalizedQAapproach
aimstoboostthereviewprocesstherebybetterensuringthesoftwarequality.
3.3 CollaborativeProcess
3.3.1 Planning
Threetypesofroleagentsengageintheplanningprocess: theRepositoryCustodian,theManager,
andtheDeveloper. Theprocesscomprisesthreephases: locatingcodefiles,teambuilding,andthe
kick-offmeeting.
Locating Code Files. Firstly, the repository
Algorithm1Locating
custodian employs the BM25 algorithm [28]
1: Input: repository:R includefilef ,GitHubissue
torankthefilesintherepositorybasedonthe i i
description:q ,LLM:L
GitHubissuedescription. Subsequently,thetop x
2: Config: filtertopwidth:k,prompts:P
k files are selected as potential candidates for 3: Output: candidatefiles:Ck←∅,repositoryevolu-
furthercoding. However,asdescribedin§2.2, i
tionmemory:M←∅
thissimpleretrievalmethodcanintroduceirrel- 4: R ←BM25(R ,q )
i i x
evantfiles,increasingthecostandreducingthe 5: Ck←R [:k]
i i
effectiveness of subsequent code changing by 6: forf ∈Ckdo
i i
LLMs. Therefore,itisnecessarytofilterthem 7: f h,s h←find(f i,M)
according the relevance. While it is feasible 8: if∃f handlen(s h)<len(f i)then
to directly assess the relevance between each 9: ifhisithen
10: s ←s
file and the issue by LLMs, many queries are i h
11: else
redundant as some code snippets in the previ-
12: ∆d←diff(f ,f )
ousrequestareduplicated,leadingtounneces- h i
13: m←L(∆d,P )
sarycosts. Consideringthatapplyingthecode 1
14: s ←s ∪m
i h
changeoftenmodifiesaspecificpartofthefile 15: endif
ratherthanthewholefile,weproposeamethod 16: else
toreusethepreviouslyrequestedinformation. 17: s ←L(f ,P )
i i 2
18: endif
AsAlgorithm1demonstrates,whenafilef is
i 19: M←M.update({f :s })
comparedwiththeissueq forthefirsttime,the i i
x 20: if L((s ,q ),P )isfalsethen
i x 3
LLMLcompressesitintoashortercodesum- 21: Ck←Ck-f
marys i,whereidenotesthefile’sversionnum-
22:
endi
if
i i
ber. Uponmodificationonf itoanewversion, 23: endfor
denotedasf , thedifference, d , isobtained
j i,j
viathe“git diff”command. Ifthelengthofd islessthanthatoff ,indicatingthattheversion
i,j j
differencesaresmallerthanthenewfileitself,Lcansummarizethecodechangesasa“commit”
messagem . s togetherwithm makesupthedescriptionofthenewerversionf ,enablingthe
i,j i i,j j
LLMLtodeterminerelevancer infewerlength. Basedonthisrelevance,thecustodianagentfilters
j
candidatefiles,allowingthemanageragenttodefinetaskswiththeserelevantfiles.
TeamBuilding. Inthisprocess,themanageragenthastheflexibilityto“recruit”teammembersas
theissueneeds. Firstly,uponreceivingthecandidatefiles,themanagerbeginswithanalyzingthe
GitHubissuefortherepositoryandbreaksthemintodetailedfile-leveltasks. Specifically,foreach
6codefilef inthecandidatesetCk,theprojectmanagerleveragestheLLMLwiththepromptP
i i 4
andtheissuedescriptionq todefinethecorrespondingfile-leveltaskt . Oneissuecanbeconverted
x i
tomultipletasks. Thesetasks,alongwiththeassociatedcodefile,arestoredinatasksetTk. Oncea
i
taskisclarified,theprojectmanagerdefinesthepersonalityroler ofthedeveloperbyinvokingLLM
i
LwiththepromptP andthetaskt . Byiteratingthroughthesecandidatecodefiles,theproject
5 i
manageragentultimatelydesignsacollectionofdeveloperagentroledescriptionsDk,thusforming
i
thedevelopmentteam. ThedetailsoftheteambuildingareshowninthepartofAlgorithm2.
Kick-off Meeting. After building the team,
Algorithm2MakingthePlan
theprojectmanagerorganizesakick-offmeet-
ing. Thismeetingservesmultiplepurposes: (1) 1: Input: candidatefiles:C ik,issue:q x,LLM:L,
To confirm whether the tasks assigned by the 2: Config: prompts:P
3: Output: tasks: Tk ←∅,developeragents’role
projectmanagerarereasonableandensurethat i
description:Dk←∅,plan:c
all developers in the team can collaboratively i main
4: forf ∈Ckdo
resolvetheissueq ,and(2)Todeterminewhich i i
x 5: t ←L((f ,q ),P )
i i x 4
developers’taskscanbeexecutedconcurrently 6: Tk←Tk∪(f ,t )
i i i i
andwhichtaskshavedependenciesneedtobe 7: r ←L((t,q ),P )
i x 5
sorted. Themeetingtakestheformofacircular 8: Dk←Dk∪r
i i i
speech,andtheprojectmanagerisresponsible 9: endfor
foropeningthespeech,guidingthediscussion, 10: recording=kick_off_meeting(Dk)
i
and summarizing the results. After the meet- 11: D ik←L((D ik,recording),P 6)
ing,developersadjusttheirroledescriptionsDk 12: c main←L(recording,P 7)
i
based on the discussion, and the project man-
ager,leveragingtheLLMLandthepromptP ,generatesamainworkplanc . Thisworkplanis
7 main
presentedascode,andembeddedintothemainprogramforexecution.
3.3.2 Coding
Twotypesofagentsparticipateinthecod-
Algorithm3CodingTaskExecution
ingprocess: developersandQAengineers.
AsoutlinedinAlgorithm3,foreachtask 1: Input: file-taskset:T ik,LLM:L
t inthetasksetTkanditsassociatedcode 2: Config: prompts:P,themaxofiteration:n max
i i 3: Output: codechanges:D
file f , the developer agent generates the
i 4: forf ,t ∈Tkdo
roledescriptionoftheQAengineera by i i i
i 5: a ←L((f ,t ),P )
leveragingtheLLMLwiththepromptP . i i i 8
8 6: forj∈[0,n )do
max
Subsequently,developerscollaboratewith
7: ifj>0then
theirQAengineerstoexecutethecoding 8: t =(t ,review_comment)
i i
tasks. Duringeachexecutionofthedevel- 9: endif
oper,therangeoflinesofcodethatneed 10: {[s′,e′]}←L((f ,t ),P )
i i i i 9
tobemodifiedisfirstlydeterminedasaset 11: f i,old_code_part←split(f i,{[s′ i,e′ i]})
of intervals {[s′,e′]} where s′ represents 12: new_code_part←L((f i,t i,old_code_part),P 10)
the starting linei nui mber in thi e i-th hunk, 13: f i′←replace(f i,{[s′ i,e′ i]},new_code_part)
ande′ istheendinglinenumber. Thede- 14: ∆d i←diff(f i,f i′)
i 15: review_comment=L((t ,∆d ),P )
terminationisgeneratedbyanalyzingthe i i 11
16: review_decision=L((review_comment),P )
taskcontentt andfilecontentf usingthe 11
i i 17: ifreview_decisionistruethen
LLMLwiththepromptP . Theseinter-
9 18: break
valssplittheoriginalcodefilef iintoparts 19: endif
tobemodified,old_code_part,andpartsto 20: endfor
beretained. Developersthengeneratenew 21: ∆d←diff(f′,f )
i i
codesnippets,new_code_part,bytheLLM 22: D←D∪∆d
LwiththepromptP . Thecodesnippets 23: endfor
10
replaceold_code_part,resultinginanew
version of the code file f′. Utilizing Git tools, the code change ∆d for this file f is generated.
i i i
Withthecodechange∆d ,QAengineerproducereview_commentandreview_decision,bytheLLM
i
L with the prompt P . If the decision, review_decision, is negative (i.e., false), the feedback,
11
review_comment,promptsdeveloperstorevisethecodeinthenextattempt. Thisiterativeprocess
continuesuntilthecodechangemeetsthequalitystandardsorreachesapredefinedmaximumnumber
ofiterations. Aftertheiteration,thefinalversionofthecodechange,∆d,isfixedanditrepresents
7theultimatemodificationresultoneachfile. Allgeneratedfinal-versioncodechangesduringthis
processaremergedintotherepository-levelcodechangeDastheissuesolution.
4 ExperimentsandAnalysis
To validate the performance of our framework, we conduct experiments and compare it against
otherpopularLargeLanguageModels(LLMs)todemonstrateitsoveralleffectiveness(§4.2). In
addition,tofurtherinvestigatetheeffectivenessofourframework,weassesseditsperformancein
twoprocesses: planning(§4.3)andcoding(§4.4).
4.1 Setup
4.1.1 DatasetandExperimentalSettings
Intheexperiments,weemploytheSWE-benchdatasetastheevaluationbenchmarkbecauseitisthe
latestdatasetspecificallydesignedforevaluatingtheperformanceoftheGitHubissueresolution.
SWE-benchcomprises2,294issuesextractedfrom12popularPythonrepositories,representingreal
softwareevolutionrequirements. Itincludestwocontextsettings: anOracleretrievalandsparse
retrieval. Intheformersetting(w/Oracle),theLLMreceivesthefilestomodify,whereas,inthe
latter(w/oOracle),themethodneedstoretrievethefilesformodification.
Given the observation that experimental outcomes on the 25% subset of SWE-bench align with
thoseobtainedfromtheentiredataset[15],weoptforthesame25%subsetpreviouslyutilizedin
experimentsforGPT-4accordingtotheirmaterials3. Moreover,theexperimentalscoresforthefive
LLMs,havebeenmadeavailablebythem4.
Our framework is flexible to integrate various LLMs. To compare with the scores reported by
SWE-bench,weselectGPT-4asthebaseLLM.AnotherreasonfortheselectionisthatGPT-4shows
remarkableperformanceoncodegenerationandunderstanding,whichhasbeendemonstratedon
benchmarkssuchasMBPP[2]andHumanEval[9]. Claude-2isnotchosenduetotheunavailability
ofAPIaccess.
4.1.2 Metric
FollowingtheSWE-bench[15],weusetheappliedandresolvedratiotoevaluatetheperformance
underthewith-Oraclesetting. Specifically,theappliedratioindicatestheproportionofinstances
wherethecodechangeissuccessfullygeneratedandcanbeappliedtotheexistingcoderepository
usingGittools,i.e.,
|D|
AppliedRatio= , (2)
|I|
whereDrepresentsthesetofinstancesinthegeneratedcodechangesetthatcouldbeappliedtothe
originalcoderepositoryusingthe“git apply”operation,andI isthesetofallinstancesinthetest
set. Theresolvedratioreferstotheproportionofinstancesinwhichthecodechangeissuccessfully
appliedandpassedaseriesoftests,i.e.,
(cid:12) (cid:12)
(cid:12)(cid:80)l ({T (d )}∩{T (d )})(cid:12)
(cid:12) i=0 old i new i (cid:12)
ResolvedRatio= , (3)
|I|
whereT denotesallthetestcasesthattheoldversionofthecoderepositorycouldpass, T
old new
representsallthetestcasesdesignedfornewrequirements,andd denotesthecodechangegenerated
i
toresolvetheissueinthei-thinstance. Furthermore,T(d)=Truemeansthatthecodechangedcan
passallthetestcasesinT.
Therecallscoreversusfilenumbercurveisusedtomeasuretheeffectivenessoffilelocatingforthe
without-Oraclesetting. Therecallscorereferstotheproportionoffilesthataresuccessfullylocated
3https://drive.google.com/drive/folders/1EnrKzGAnsb_NmZKyECGmA2DrAc8ZuJ80
4https://openreview.net/forum?id=VTF8yNQM66&noteId=lfJF38VxJr
8outofallthefilesthatrequiremodification. Theformulaforcalculatingthefilelocationrecallscore
forthei-thinstanceisasfollows:
|G ∩R |
Recall= i i ×100%, (4)
|R |
i
whereG
=(cid:80)n
g representsthesetoffilepathslocatedbyourframework,witheachfilepath
i j=0 i,j
inthesetdenotedasg andthetotalnumberoffilesasn;R
=(cid:80)m
r denotesthepathsofthe
i,j i k=0 i,k
filesthatneedtobemodified,witheachreferencefilepathdenotedasr andthetotalfilenumberas
i,k
m. Inthiscurve,“filenumber”referstotheaveragenumberoffilesthatneedtobeprocessedacross
allinstancestoachievethegivenrecallscore. Specifically,itillustrateshowmanyfilesaveragely
needtobelocatedbyourframeworkbeforereachingtherecallscoredenotedbythecurveatany
point. Thismetricrepresentsboththeeffectivenessandefficiencyoffilelocating.
4.2 RQ3: HowEffectiveIsOurFramework?
ThecomparativeperformanceanalysisbetweenourframeworkandotherLLMsonthesamedataset
ispresentedinTable2. TheresultsindicatethatourframeworksignificantlyoutperformsotherLLMs.
Notably, with a resolved ratio of 13.94%, our framework’s effectiveness is eight-fold that of the
basemodel,GPT-4. Thissubstantialincreaseunderscoresourframework’scapabilitytoharnessthe
potentialofLLMsmoreeffectively. Furthermore,whencontrastedwiththepreviousstate-of-the-art
LLM,Claude-2,ourframework’sresolvedratioexceedsthatbenchmarkbymorethantwo-fold. This
superiorperformanceunequivocallyestablishestheadvanceofourmethod. AsDevin[34]usesa
differentsubsetfromtheevaluatedsubsetofGPT-4mentionedintheSWE-benchpaper[15],the
comparisonbetweenitandoursisdiscussedin§5.2.
Theablationstudyisdesignedtosim- Table2: TheComparisonofOverallPerformancebetween
ulatetwoscenarios: (1)WithoutQA MAGISandBaselinesonSWE-bench.
(w/o QA): Considering the QA en-
gineer agent as optional within our Method %Applied %Resolved
framework, we directly evaluate the GPT-3.5 11.67 0.84
codechangesgeneratedbythedevel- Claude-2 49.36 4.88
operagent,bypassingtheQAprocess. GPT-4 13.24 1.74
Thisscenarioaimstoinvestigatethe SWE-Llama7b 51.56 2.12
effectivenessandnecessityofQAen- SWE-Llama13b 49.13 4.36
gineerreview. (2)Withouthints(w/o Devin[34]* − 13.86*
hints): Hintsrefertothetextualcon-
MAGIS 97.39 13.94
tent found in the comments section MAGIS(w/oQA) 92.71 10.63
of pull requests, which are typically MAGIS(w/ohints) 94.25 10.28
created prior to the first commit of MAGIS(w/ohints,w/oQA) 91.99 8.71
thepullrequest. Theabsenceofhints
* Notethatthisworkisevaluatedonarandomlychosen25%
(w/ohints)meansourframeworkop-
testset,butthissubsetdiffersfromthe25%subsetexperi-
erates without any clarifications ex-
mentedonGPT-4intheSWE-bench.
ceptfortheissue,despitesuchinfor-
mationbeingavailableonGitHubbeforetheissueresolutionprocessbegins. Thisanalysisaimsto
exploreiftheparticipationofhumanscouldpotentiallyimprovethesuccessrateofissueresolution.
Ourframeworkdemonstratesthecapabilitytosignificantlyenhanceissueresolution,evenwithout
theQAengineerorhints. Itachievesaresolvedratioof8.71,whichisfivetimeshigherthanthatof
thebasemodel,GPT-4. Thisincreaseunderscoresthecontributionofotheragentsinourframework
toitsoverallperformance. Furthermore,integratingcooperationwithQAorhintsseparatelycan
furtherelevatetheresolvedratioby1.92or1.57,respectively. Thesefindingsservetounderscore
thevalueofQAengineersandtheparticipationofhumans,asdemonstratedbytheresolvedrates
achievedthroughtheirintegration.
Forinstance,toresolvetheissue5fromtherepositoryDjango6,thedevelopermodifiesfourhunks
intwofiles7,asshowninFigure3. Despitetheavailabilityoftwofilesinthewith-Oraclesetting,our
5https://code.djangoproject.com/ticket/30255
6https://github.com/django/django/
7https://github.com/django/django/pull/12155/files
9itaowei/draft Type / to search
Product Solutions Open Source Pricing Search or jump to... Sign in Sign up
Code Issues Pull requests Actions Projects Wiki Security Insights Settings
django/django Public Sponsor Notifications Fork 30.6k Star 76.3k
Commit
Code Pull requests 154 Actions Security Insights
Fixed #30255 -- Fixed admindocs errors when rendering docstrings without leading newlines. #12155 New issue Browse files
Merged felixxm merged 1 commit into django:master from bmispelon:ticket-30255 on Nov 29, 2019
Conversation 1 Commits 1 Checks 0 Files changed 3 +15 −31 1 parent 68910bd commit 7e76d26
Changes from all commits File filter Conversations Whitespace Ignore whitespace Split Unified
Filter changed files 20 django/contrib/admindocs/utils.py
django/contrib/admindocs @@ -3,6 +3,7 @@ 3 django/contrib/admindocs/utils.py
utils.py 43 43 i fm rp oo mr t e mar ie l.errors import HeaderParseError @@ -34,7 +34,8 @@ def trim_docstring(docstring):
views.py 5 5 from email.parser import HeaderParser 34 34 return ''
tests/admin_docs 6 76 +from inspect import cleandoc 35 35 # Convert tabs to spaces and split into lines
test_utils.py 7 8 from django.urls import reverse 36 36 lines = docstring.expandtabs().splitlines()
8 9 from django.utils.regex_helper import _lazy_re_compile 37 - indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())
@@ -24,26 +25,13 @@ def get_view_name(view_func): 37 + # Determine the minimum indentation (first line doesn't count):
2 22 5 64 2 22 6 75 return mod_name + '.' + view_name 38 33 98 + i tn rd ie mn mt ed = = m [i ln i( nl ee sn [( 0l ]i .n le s) t r- i pl (e )n ]( l +i n [e l. il ns et [r ii np d( e) n) t :f ]o .r r sl ti rn ie p (i )n fl oi rn e ls i[ n1 e: ] i ni f l il ni en se [. 1l :s ]t ]rip())
27 -def trim_docstring(docstring): 39 40 return "\n".join(trimmed).strip()
28 - """ 40 41
29 - Uniformly trim leading/trailing whitespace from docstrings.
30 -
31 - Based on https://www.python.org/dev/peps/pep-0257/#handling-docstring-indentation
32 - """
33 - if not docstring or not docstring.strip(): Lock conversation
34 - return ''
35 - # Convert tabs to spaces and split into lines
36 - lines = docstring.expandtabs().splitlines() Figure4: CasefromDjango(Ours).
37 - indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())
38 - trimmed = [lines[0].lstrip()] + [line[indent:].rstrip() for line in lines[1:]]
39 - return "\n".join(trimmed).strip()
40 -
41 -
42 28 def parse_docstring(docstring):
43 29 """
44 30 Parse out the parts of a docstring. Return (title, body, metadata).
45 31 """
46 - docstring = trim_docstring(docstring) Markdown is supported Paste, drop, or click to add files
32 + if not docstring:
33 + return '', '', {} 0.70 BM25
34 + docstring = cleandoc(docstring) Ours Comment on this commit
47 35 parts = re.split(r'\n{2,}', docstring)
48 36 title = parts[0] 0.69
49 37 if len(parts) == 1:
Un0su.6bs8cribe You’re receiving notifications because you’re watching this repository.
3 django/contrib/admindocs/views.py
0.67
... ... @@ -1,5 +1,6 @@
21 21 i fm rp oo mr t i mpi on rs tp le ic bt import import_module 0.66
3 +from inspect import cleandoc
3 4 4 5 from pathlib import Path 0.65 © 2024 GitHub, Inc. Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information
5 6 from django.apps import apps
@@ -256,7 +257,7 @@ def get_context_data(self, **kwargs): 0.64
256 257 continue
22 255 587
9
22 55 98
-
v ve er rb b uo o ts s ie e
l
s= =
.
pf v au e rn r sc b eo. _s_ re_
s
d to a (c n u_ d t_ i(
ls.trim_docstring(verbose), 'model', _('model:') + opts.model_name)
9.0 8.5 8.0 7.5 7.0 6.5 6.0
260 + utils.parse_rst(cleandoc(verbose), 'model', _('model:') + opts.model_name)
260 261 )
261 262 # Show properties and methods without arguments as fields.
262 263 # Otherwise, show as a 'method with arguments'.
Figure5: ComparisonofRecallScoresbetween
23F igtuestrs/aedmin_3doc:s/teCst_utailss.pyefromDjango(Gold).
MAGISandBM25.
... ... @@ -1,8 +1,9 @@
1 1 import unittest
2 2
3 3 from django.contrib.admindocs.utils import (
4 - docutils_is_available, parse_docstring, parse_rst,trim_docstring,
4 + docutils_is_available, parse_docstring, parse_rst,
5 5 )
propo6s+efrodm djanfgor.taestm.utilse imwport ocaprturked_stoderrptsformodificationsinonlyonefile,asillustratedinFigure4. Remarkably,
6 7
thi7 8ss8 9imfropm .lteestsr impcort oAdmdinDoecsSicmplehTesatCansegeenablestherepositorytosuccessfullypassallrequisitetestcases.
@@ -31,19 +32,6 @@ class TestUtils(AdminDocsSimpleTestCase):
31 32 def setUp(self):
32 33 self.docstring = self.__doc__
33 34
4.3 334 5 R-- Qdef tt4re is mt:__ dt or ci smHt_ rd io nc gso_t or ui tnwpg u( ts e =lf )Et: rim_fdofcsterincg(stelif.vdocsetrinIg)sOurPlanningProcess?
36 - trimmed_docstring = (
37 - 'This __doc__ output is required for testing. I copied this '
38 - 'example from\n`admindocs` documentation. (TITLE)\n\n'
39 - 'Display an individual :model:`myapp.MyModel`.\n\n'
To40inv-estiga'**tCoentext*t*\hn\ne``ReqeuestfCofnteextc``\tn\in`v`mymeodenl``e\n'ssoftheplanningprocess, weanalyzetherepositorycustodianand
41 - ' An instance of :model:`myapp.MyModel`.\n\n'
pr4 4o2 3 jec- -tman' '* (a* DT Ee Sgm Cp Rl Ia Pet Te I: Or* N* )\ \n na\ \n n: sgt oe mm ep _el ma et tne a: d` atm ty a,a :p p s/ orm my ee_ t de asm tp al 'patee.htcml` t'ively. Theperformanceoftherepositorycustodianagentisobserved
44 - )
in45the-recsaelfl.alssertsEqcual(otrirm_deocstrving_eoutrpust, utrimsmed_dtochstreing)file number curve, as shown in Figure 5 with the horizontal axis
46 -
re44p87re33 65sendtefi ttnie ts lt e_g,p a dr es setc_ rdhio pc ts iteor nin ,g ( maes te alvdf a) t:ea =r paarseg_doecstrinng(suelf.mdocstbringe)roffilesandtheverticalaxisdenotingtherecallscoreassociated
49 37 docstring_title = (
withtha@@ t-106n,6 +u94,m13 @@ bdefe terst_poarsef_rstfi(sellf)e:s. ThiscurveillustratesthatourmethodconsistentlyoutperformstheBM25
ba11 00 76sel99 54ineas se ecl lf f. .ra as sos se er rt tsE Eq qsu ua al l( (p pda ar rs se ei_ _r rfs st tf( (' 'e` `t ti irt tl le ee` `' ', ,n ' 'f ti atl gt 'e )r ,n' m) a,ur km ua prmku %p '% tb a' gf si /el #t te irr ts ls/ e# 't )itole')fselectedfiles, whichsuggeststhatourcustodiancanfindas
108 96
many97 r+eldeef vtesta_panrset_rstc_wioth_ddocsetringfi_no_lleeadisng_laines_feedp(seolf):ssiblewiththelowestpossiblenumberoffiles.
98 + title, body, _ = parse_docstring('firstline\n\n second line')
99 + with captured_stderr() as stderr:
Fort1 10 0h0 1 e+ + pros sje el lef f. .a acs ss se etr rt tE Eq qmu ua al l( (p paa ar rs sne e_ _r rs sat t( (t bgi ot dl ye ,e, '' 'r' )) ,, 'a' << pp >g> sf ei cr oes nt dl ni ln ie nt< e/ <,p /> p>\ \n wn' ') )eexaminedthealignmentofitsgeneratedtaskdescriptionswith
102 + self.assertEqual(stderr.getvalue(), '')
ther1e03f+erencecodechange. Ahighercorrelationscoreindicatesabetteralignmentandthus,amore
109 104 def test_publish_parts(self):
110 105 """
ac111cu10r6at e a Dnjangdo shoeuldfn'ft ebrecak tthei vdefaeult prolel faor nintenrprietned gtextdirection.
ThecorrelationscoresaredeterminedbyGPT- 50 Not Resolved
4 based on a set of criteria defined in Table 3, Resolved
© 2024 GitHub, Inc. Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information
which spans from a score of 1, indicating no 40
relevance, to a score of 5, indicating perfect
30
alignmentwithhighaccuracyandcompleteness.
The distribution of these correlation scores is 20
presented in Figure 6, where the vertical axis
10
representsthefrequencyofeachscore. Notably,
mostofthescoresare3orabove,whichimplies 0 1 2 3 4 5
thatthemajorityoftaskdescriptionsareinthe Figure 6: Distribution of the Correlation Score
right direction concerning planning. Further- BetweentheGeneratedTaskDescriptionandthe
more,thehigherscorescorrelatewithahigher ReferenceCodeChange.
probability of issue resolution, indicated by a
10Table3: TheMeaningofScoresinGPT-4EvaluationontheCorrelationBetweentheGeneratedTask
DescriptionandtheReferenceCodeChange.
Score Meaning
1 Thecodechangesareunrelatedtothetaskdescription.
2 Thecodechangesaddressaminorpartofthetaskbutarelargelyirrelevant.
3 Thecodechangespartiallymeetthetaskrequirementsbutlackcompletenessoraccuracy.
Thecodechangesarerelevantandmostlycomplete,withminordiscrepanciesfromthe
4
taskdescription.
Thecodechangesperfectlyalignwiththetaskdescription,fullyaddressingallspecified
5
requirementswithhighaccuracyandcompleteness.
largerproportionof“resolved”outcomesinscores4and5. Thissignifiesthatwhenthegenerated
taskdescriptioncloselyalignswiththereference,thereisahigherpossibilitytoresolvetheissue.
Theanalysisabovedemonstratestheeffectivenessofboththerepositorycustodianandtheproject
manageragentintheplanningprocessofourframework.
4.4 RQ5: HowEffectiveIsOurCodingProcess?
Toinvestigatetheeffectivenessofthecodingprocessinourframework,weanalyzetheperformance
ofthedeveloper’slinelocatingandtheissueresolvingacrossinstancesofvaryingcomplexities.
Figure 7 illustrates the distribution of the line location overlap ratio of MAGIS and the baseline
models (GPT-4 and Claude-2). The vertical axis quantifies the frequency of occurrences within
specific ranges of line location overlap ratios for each group. This visualization reveals that our
developeragentfrequentlyattainsalinelocationoverlaprationearing1. Comparedwithbaseline
models,thedeveloperagentdemonstratesapronouncedpreferenceforhigherdistributionvaluesclose
to1,andconversely,areducedpreferenceforlowerdistributionvaluesnear0. Suchadistribution
validatesthesuperiorperformanceofMAGISinlinelocation.
FurtheranalysisisprovidedinFigure8,whichillustratestherelationshipbetweenthelinelocation
overlapratioandtheissueresolvedratiowithinthoseoverlaps. Inthefigure, thehorizontalaxis
representstherangeofoverlapratiosforeachbar’scorrespondinginterval,whiletheheightofeach
barindicatestheresolvedratioforinstanceswithinthatinterval. Theseresolvedratioscorrespond
to the scale on the left vertical axis. The orange curve represents the cumulative frequency of
instancesthatcanberesolvedunderdifferentoverlapratiothresholds,withthecumulativefrequency
correspondingtothescaleontherightside.
AsshowninFigure8,therightfourbarsarehigherthanthefiveleft,whichindicatestheresolved
ratiocanincreasewiththelinelocationoverlap. Thisobservationalsosuggeststhataccurateline
locationishelpfulforissueresolution. Thecumulativefrequencycurve,showninorange,provides
anadditionalanalysis,indicatingthecumulativeproportionofissuesresolvedratiouptoeachpoint
alongthelinelocationoverlap. Asteadyincreaseincumulativefrequencyaccompaniestheincrease
inlinelocationoverlap,reinforcingtheideathatresolvingissuesismoresuccessfulinareasofhigh
overlap. The slope of the curve’s left half is lower than that of the right half, indicating that the
benefitsofincreasingtheoverlapratioarelesspronouncedatloweroverlapratiosthanathigherones.
Therefore,thedeveloperagentshouldprioritizeimprovingitscapabilityoflinelocation.
Moreover,asshowninTable4,wepresentalogisticregressionanalysisthatquantifiesthecorrelation
betweenseveralcomplexityindicesandissueresolution. TheresultsshowthatGPT-4hassignificant
negativecorrelationsacrossthenumberoffilesandfunctions,suggestingthatastheseindicesincrease,
thelikelihoodofissueresolutiondecreases. Conversely,thenegativecorrelationsarelesspronounced
withourmodel,MAGIS,particularlyinthenumberoffilesandfunctions,suggestingmitigationof
challengescorrespondingtothesecomplexityindices.
ToevaluatetheperformanceoftheQAengineer,theablationexperimentisconductedandtheresults
areshowninTable2. Asthetableshows,insettingswithandwithouthints,thepresenceoftheQA
engineercanincreasetheresolvedratioby1.57%and3.31%,respectively. Thisoverallenhancement
substantiatestheQAengineer’scontributiontoimprovingoutcomes.
11Table4: CorrelationBetweentheComplexityIndicesandtheIssueResolution.
Method #Files #Functions #Hunks #AddedLoC #DeletedLoC #ChangedLoC
GPT-4 −25.15* −25.15* −0.06 −0.10 −0.04 −0.21
MAGIS −1.55* −1.55* −0.12* −0.04* −0.06* −0.57*
* Thecorrelationbetweentheindexandtheissueresolutionissignificant(P-value<0.05).
 2 X U V
   *  & O 3  D 7  X   G   H   Cumulative Frequency 1.0
0.175
 
0.150 0.8
 
0.125
  0.6
0.100
  Product Solutions Open Source0.07Pr5icing 0.4 Search or jump to... Sign in Sign up
  0.050
0.2
scikit-learn/scikit-learn Public Sponsor Notifications Fork 24.9k Star 57.8k
  0.025
                  Code    Issues 1.6k    Pull requests05.40800 Discuss 0io .n 2s Action 0s .4 Projects 0.6Wiki S 0e .c 8urity Insi 1gh .t 0s 0.0
[MRG] add seeds when n_jobs=1 and use seed as random_state#9288 New issue
Figure7: ComparisonofLine MLergoedcaamtiuoellenr mOergevd 5e crom-mits into sFcikiigt-luearrn:emast8er: froRm ebrysanoyanlgv052e8:dconsRist_an_tjoibos ionn ADug 1i6,f 2f01e9rentLineLoca-
lapbetweenMAGIS(Ours)andBaselines. tionOverlapIntervals.
Conversation 24 Commits 5 Checks 0 Files changed 3 +20 −3
Changes from all commits File filter Conversations
Specifically,thereisanissue8Fiflterr ochamngedt fhileserepositoryscik6 it-dloc/ewhaatsr_nenw/v09.22a.rsntdthereferencecodechange10
isshowninFigure5. Duringthdoec/wflhatos_nwew ofourframework,@@t h-26e,6 +d26e,8 v@@e ralnodomp saemprlinfig prroscetdulryes.modifiesthecodeas
showninFigure9buttheparamevt0.e22r.rsrtandom_state(2 26 7Lin2 26
7e3- 7:cl1assi:`nlinteahr_emodenl.eRidwge`- wvhene `rX`s iiso spnarsce. o|Fdix|e)ofthefunction
sklearn/cluster 28 28
kmeans_singleisnotassigned k_mt eah nse _.pyrightnumberins2 39 0ee ++-d :cslas.s:`Aclufsttere.KrMeatnsh` wehene `nr_jroobs=n1`.e |oFixu|smodificationwas
made,theQAengineeridentifiedteststhemistakeandpr29ov3i1dedDetafiles eared lbistaedc ikn t.he Tchahngeeloig rbelcowo.mmentaryhighlighted
30 32
theissue: “Thiscodechangemotedst_ikfi_meeanss.pytheimpleme31nta33tion(Whiole fwe Kare -trmyinge tao bnettser ainlfogrmo usrerist bhy pmrovidaingn tdhis dinfoormeatsionn, ’wte seem
entirely correct”. They further elaborated, “Running th@@ e-283a,6l +g285o,1r0 i@@t hChamngelogjust one time could lead to
worseresults,comparedtorunningitmultipletim2 2e8 83 4s(2 28 8n5 6_i n m :a pit rtc :h ` 1t` 3s 7ip 2me 6c `t r bea yl _ :sc ul s)u es rt :ae `r Sinhn ug zd` h. e Xciaoh <ofdaos32s13i>n`.gthebestresult,as
wasoriginallydone”Thiscritiquespecificallytarg28e5ts28t7heflawassociatedwiththeiterativeprocess
288 +- |Fix| Fixed a bug where :class:`cluster.KMeans` produced inconsistent results
(“runningtimes”). WiththehelpoftheQAengineer,2t89he+ dbeetwveene `ln_ojobps=e1` randf `un_rjotbsh>1e` drue rtoe tvhe ihsanedlintgh ofe thec roanddom est,atea.ndthe
290 + :pr:`9288` by :user:`Bryan Yang <bryanyang0528>`.
finalcodechangeisshowninFigure10. Allofthenec29e1ss+arytestcasesarepassedafterapplyingthis
286 292 :mod:`sklearn.feature_selection`
codechange. Boththeablationstudyandthecase2s87tud293yu..n..d...e...r.s..c...o..r..e....t.h...e....QAengineer’sefficacy.
288 294
5 StatisticsandDiscussion 7 sklearn/cluster/k_means_.py
@@ -360,16 +360,18 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
360 360 else:
361 361 raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
Thissectionprovidesstatisticsoncodechanges 362 362 " %s" % str(algorithm))
363 +
correspondingtoresolvedissuesandthoseap- 364 + seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
363 365 if effective_n_jobs(n_jobs) == 1:
plicablebutunresolvedusingourframework. 364 366 # For a single thread, less memory is needed if we just store one set
365 367 # of the best results (as opposed to one set per run per thread).
366 - for it in range(n_init):
368 + for seed in seeds:
5.1 ComplexityofCodeChanges. 367 369 # run a k-means once
368 370 labels, inertia, centers, n_iter_ = kmeans_single(
369 371 X, sample_weight, n_clusters, max_iter=max_iter, init=init,
370 372 verbose=verbose, precompute_distances=precompute_distances,
Thestatisticsonthecodechangeforinstances 371 373 tol=tol, x_squared_norms=x_squared_norms,
with resolved issues are presented in Table 6. 372 - random_state=random_state)
374 + random_state=seed)
Overall, the statistical information of the gen- 373 375 # determine if these results are the best so far
374 376 if best_inertia is None or inertia < best_inertia:
erated code changes for these instances, such 375 377 best_labels = labels.copy()
@@ -378,7 +380,6 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
astheaveragenumberofcodefiles,functions, 378 380 best_n_iter = n_iter_
379 381 else:
hunks,anddeletedlines,alldifferslightly(not 380 382 # parallelisation of k-means runs
381 - seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
exceeding0.3)fromthereferencesolutionswrit- 382 383 results = Parallel(n_jobs=n_jobs, verbose=0)(
383 384 delayed(kmeans_single)(X, sample_weight, n_clusters,
ten by humans. This indicates that for these 384 385 max_iter=max_iter, init=init,
instances, the complexity of the code change
Table5: Casefromscikit-learn(Gold).
10 sklearn/cluster/tests/test_k_means.py
8https://github.com/scikit-learn/scikit-learn/i@s@ s-95u1,e3 +s95/1,193 7@@ 8de4f test_minibatch_kmeans_partial_fit_int_data():
9https://github.com/scikit-learn/scikit-l9e51ar9n51 km = MiniBatchKMeans(n_clusters=2)
952 952 km.partial_fit(X)
10https://github.com/scikit-learn/scikit-l9e53ar9n53/pulaslse/rt 9km2.cl8us8ter_centers_.dtype.kind == "f"
954 +
955 +
956 +def test_result_of_kmeans_equal_in_diff_n_jobs():
957 + # PR 9288
12 958 + rnd = np.random.RandomState(0)
959 + X = rnd.normal(size=(50, 10))
960 +
961 + result_1 = KMeans(n_clusters=3, random_state=0, n_jobs=1).fit(X).labels_
962 + result_2 = KMeans(n_clusters=3, random_state=0, n_jobs=2).fit(X).labels_
963 + assert_array_equal(result_1, result_2)
© 2024 GitHub, Inc. Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal informationitaowei/draft Type / to search
itaowei/draft Type / to search
Code Issues Pull requests Actions Projects Wiki Security Insights Settings
Code Issues Pull requests Actions Projects Wiki Security Insights Settings
Commit
Commit
Update k_means_.py Update k_means_.py Browse files Browse files
main main
itaowei committed now itaowei committed now 1 parent b2a606e commit ad8afff 1 parent 0dcff5f commit 816fba7
Showing 1 changed file with 22 additions and 23 deletions. Showing 1 changed file with 27 additions and 20 deletions. Whitespace Ignore whitespace Split Unified Whitespace Ignore whitespace Split Unified
45 sklearn/cluster/k_means_.py 47 sklearn/cluster/k_means_.py
@@ -360,26 +360,24 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++', @@ -360,42 +360,50 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
360 360 else:
360 360 else: 361 361 raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got"
361 361 raise ValueError("Algorithm must be 'auto', 'full' or 'elkan', got" 362 362 " %s" % str(algorithm))
362 362 " %s" % str(algorithm)) 363 - if effective_n_jobs(n_jobs) == 1:
363 + # [NEW CODE] 364 - # For a single thread, less memory is needed if we just store one set
363 364 if effective_n_jobs(n_jobs) == 1: 365 - # of the best results (as opposed to one set per run per thread).
364 - # For a single thread, less memory is needed if we just store one set 366 - for it in range(n_init):
365 - # of the best results (as opposed to one set per run per thread). 367 - # run a k-means once
366 - for it in range(n_init): 363 + # [NEW CODE]
367 - # run a k-means once 364 + # [NEW CODE]
368 - labels, inertia, centers, n_iter_ = kmeans_single( 365 + # Initialize variables for the best values and results
3 3 36 7 79 0 1 - -- X v te o, r l b =s oa tsm oep l=l ,e v_ xew _re sbi qog ush aet r, e , dp _rn ne_ occ rol mmu sps ut =te xer __s sd q, i u s am t ra a ex n d_ c _i e nt s oe = rr p m= r sm e ,a cx o_ mi pt ue tr e, _ di in si tt a= ni cn ei st ,, 3 3 3 36 6 6 66 7 8 9 + + + + b # ee fs C ft h e_ e cl c ta k ib ve t el h _s e j o, n b u sb me bs =et r_ e i fon ffe e r cjt toi iba vs, e _sb npe _es jct oi_ bfc sie (en ndt _ e jfr oos br, s )pb ae rs at l_ ln e_ li t ee xr e c= u tN io on ne, None, None, None
372 - random_state=random_state) 370 +
373 - # determine if these results are the best so far 371 + # Set a threshold for the random state generator
374 - if best_inertia is None or inertia < best_inertia: 372 + seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
375 - best_labels = labels.copy() 373 +
376 - best_centers = centers.copy() 374 + # Run the K-means algorithm in parallel or single-threaded based on the effective_jobs
377 - best_inertia = inertia 375 + if effective_jobs == 1:
378 - best_n_iter = n_iter_ 376 + for seed in seeds:
365 + # For a single thread, less computation is needed if we just store one set 377 + # Run the single K-means pass with the provided seed
366 + # of the best results (as opposed to one set per run). 368 378 labels, inertia, centers, n_iter_ = kmeans_single(
3 3 36 6 67 8 9 + ++ label X vs e, r, b s oi a sn m ee p =r lt e vi _ ea w re b, i o g sc h ee tn ,t ,e p r rns e_ cc, ol mun ps_ uti tet ere _sr d_ , i sm= ta axk n_m cie eta sen =rs p=_ rms eai cxn o_g mil pte ue( tr e, _ di in si tt a= ni cn ei st ,, 33 3 376 7 709 1 2 33 87 09 - - X v t re o a, r l n b = ds o oa ts mm oe _p l s=l , te v a_ xe tw _r ee sb =i qog rush aaet nr, de , odp m_rn _ne_ socc trol ammu tsps eut = )te xer __s sd q, i u s am t ra a ex n d_ c _i e nt s oe = rr p m= r sm e ,a cx o_ mi pt ue tr e, _ di in si tt a= ni cn ei st ,,
370 + tol=tol, x_squared_norms=x_squared_norms, 373 - # determine if these results are the best so far
371 + random_state=random_state) 381 + tol=tol, x_squared_norms=x_squared_norms, random_state=seed)
372 + best_labels = labels.copy() 382 +
373 + best_centers = centers.copy() 383 + # Check if obtained inertia is the best so far and store the result
374 + best_inertia = inertia 374 384 if best_inertia is None or inertia < best_inertia:
375 + best_n_iter = n_iter_ 375 385 best_labels = labels.copy()
379 376 else: 376 386 best_centers = centers.copy()
380 377 # parallelisation of k-means runs 377 387 best_inertia = inertia
381 378 seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init) 378 388 best_n_iter = n_iter_
382 3 37 89 0 - ++ r # ae ls C lu o _l l rt l es e sc u t l= t r sP e a s =r u a l Pl t al s re al f l( r ln o e_ m lj (o t nb h _s e j= on p b_ a sj r =o a nb l _s l j, e o l bv se r ,r u b n vo s es re b= o0 s) e( =0)( 3 3 3 37 8 8 89 0 1 2 389 - -- else: # s r e ep e sa d ur s la tl s=l e =rl ai Pns ada rot ami l_o lsn et lao (tf e n . _k jr- oam bne sda i =n n ns t _ ( jr onu bpn s.s ,i i vn ef ro b( on sp e. =i 0n )t (32).max, size=n_init)
383 381 delayed(kmeans_single)(X, sample_weight, n_clusters, 390 + # Execute parallel K-means runs
384 382 max_iter=max_iter, init=init, 391 + all_results = Parallel(n_jobs=effective_jobs, verbose=0)(
385 383 verbose=verbose, tol=tol, 383 392 delayed(kmeans_single)(X, sample_weight, n_clusters,
@@ -388,13 +386,15 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++', 384 393 max_iter=max_iter, init=init,
33 88 98 33 88 76 # r aC nh da on mg _e s ts ae te ed = st eo e de )nsure variety 3 338 885 76 3 339 994 65 v p xe r _r e sb c qo o us m ae p ru e= t dv e _e _ nr d ob i ro s ms t se a =, n x c _t e so s ql = u= p at r ro e el c d, o _m np ou rt me s_ ,distances,
390 388 for seed in seeds) 388 - # Change seed to ensure variety
389 + 389 397 random_state=seed)
391 390 # Get results with the lowest inertia 390 398 for seed in seeds)
392 - labels, inertia, centers, n_iters = zip(*results) 391 - # Get results with the lowest inertia
393 - best = np.argmin(inertia) 392 - labels, inertia, centers, n_iters = zip(*results)
394 - best_labels = labels[best] 393 - best = np.argmin(inertia)
395 - best_inertia = inertia[best] 394 - best_labels = labels[best]
396 - best_centers = centers[best] 395 - best_inertia = inertia[best]
397 - best_n_iter = n_iters[best] 396 - best_centers = centers[best]
3 3 3 39 9 9 91 2 3 4 ++ ++ b fe os rt _ ili fan be e br bl et es si st,a t_ _ ii l= nn ae e brrN etto liin saae , =i sc e ln aNt boe enr les s ., o crn o _ pii ynt (ee )rr ts i ai n < a bl el s_ tr _e is nu el rt ts i: a: 33 99 87 3 4 4 49 0 0 09 0 1 2 +- + + b # fe os F rt i _ n iln d fa_ bi t e bt h l ee e s sr t,r _ u= iin nn een w rr_ i tti t iit h aae ,r t i s h sc[ e eb n Ne l t os o e nt w r e] e s s , ot r n i _ in i ne t er e rt r ti s ia a i a n <n d a b l eu l sp _ td r _a e it s ne u e l rt t th s ie : a :best variables accordingly
395 + best_inertia = inertia 403 + best_labels = labels.copy()
396 + best_centers = centers.copy() 404 + best_inertia = inertia
397 + best_n_iter = n_iters 405 + best_centers = centers.copy()
398 398 406 + best_n_iter = n_iters
399 399 if not sp.issparse(X): 399 407 if not sp.issparse(X):
400 400 if not copy_x: 400 408 if not copy_x:
@@ -407,7 +407,6 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++', 401 409 X += X_mean
407 407 "n_clusters ({}). Possibly due to duplicate points " @@ -407,7 +415,6 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',
408 408 "in X.".format(distinct_clusters, n_clusters), 407 415 "n_clusters ({}). Possibly due to duplicate points "
44 10 09 409
-
ConvergenceWarning, stacklevel=2) 44 4 10 0 08 9 4 41 16 7
-
" Ci on n vX e. r" ge. nf co er Wm aa rt n( id ni gs ,t i sn tc at c_ kc ll eu vs et le =rs 2, ) n_clusters),
411 410 if return_n_iter: 411 418 if return_n_iter:
412 411 return best_centers, best_labels, best_inertia, best_n_iter 412 419 return best_centers, best_labels, best_inertia, best_n_iter
413 412 else: 413 420 else:
0 comments on commit ad8afff 0 comments on commit 816fba7 Lock conversation Lock conversation
FigurWerite9:PreCviewase from scikit-learn (Ours, be- FiguWrreite10Pre:vieCw asefromscikit-learn(Ours,af-
forerLeeavve ai ceomwme)n.t terreLevavei ae cowmm)en.t
Markdown is supported Paste, drop, or click to add files
Markdown is supported Paste, drop, or click to add files
Comment on this commit
Comment on this commit
Unsubscribe You’re receiving notifications because you’re watching this repository.
Unsubscribe You’re receiving notifications because you’re watching this repository.
© 2024 GitHub, Inc. Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information
© 2024 GitHub, Inc. Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information
13Table6: TheStatisticalAnalysisofOurFrameworkonResolvedInstances.
MAGIS Gold
Min Max Avg. Min Max Avg.
#CodeFiles 1 2 1.02 1 2 1.04
#Functions 1 2 1.02 1 2 1.04
#Hunks 1 4 1.45 1 6 1.66
#AddedLines 1 146 9.75 0 38 4.34
#DeletedLines 0 77 5.27 0 115 5.16
ChangeStartIndex 1 1,655 270.12 1 1,657 256.09
ChangeEndIndex 22 1,665 301.68 0 1,666 315.05
#ChangedLines 2 190 15.02 1 115 9.50
generatedbyourframeworkissimilartothatofhumans. Furthermore,themaximumvaluesobserved
inthetablerevealthatourframeworkcanimplementcodemodificationsinvolvingtwofiles,four
hunks, and1,655linesmodification, withsinglemodificationsreachingupto190lines. Results
demonstratetheeffectivenessofourmethodinresolvingcomplexissuesthatneedtomodifythecode
fileonmultiplelocationsandwithlongcontext.
Specifically,thedistributionofthenumberofmodifiedlinesfortheresolvinginstancesisshown
inFigure11. Weobservethatthedistributionofthenumberofmodifiedlinesinourframework
forthesolvedinstancesexceedsthatofthereferencesolution,especiallyintermsofthenumberof
addedlinesbeingsignificantlyhigherthanthereference. Uponmanualinspection,wefoundthatthe
generationresultsprovidedbyourframeworkoftencontainedmorecommentinformation,whichled
toanincreaseinthetotalnumberofmodifiedlines. Forexample,Figure10displaysthegeneration
result of our framework. Lines 365,368,371,374,383 in the new version file correspond to the
commentfortheaddedcode. Thesenaturallanguagedescriptionsarevaluableinactualsoftware
evolution[14,22]. Incontrast,Figure5showsahuman-writtensolutionlackingsuchexplanatory
comments,whichmightdisadvantagesoftwaremaintainersinreadingandunderstanding.
ThestatisticsonthecodechangeforinstanceswithoutresolvedissuesareshowninTable7. Fromthe
table,ourframeworkcangenerateapplicablecodechangesincludingupto13filesand28hunks,and
thelocationofthemodificationscanbeasfarasline7,150,withasinglemodificationreachingupto
9,367lines. Theseresultssuggestthatourmethodhasastrongadaptabilityingeneratingapplicable
codechanges. However,consideringthatthesecodechangeshavenotpassedallthepotentialtest
casestheycouldpass,whichindicatesthatthereisstillroomforimprovement.
Tofurtheranalyzethereasonsbehindthefailureoftestcasesintheseinstances,wehavequantified
thedistributionofthelengthsofcodechangesintheunresolvedinstances,asshowninFigure12.
Fromthefigure,weobservethatforunresolvedinstances,theframeworktendstodeletealarger
numberoflineswhileaddingfewerlines,incontrasttothedistributionofhuman-writtenchanges.
  
   
  
      
     
      
     
  
   
     
  
   
     
  
          
   $ G G  ' H O H W H  % R W K    $ G G  ' H O H W H  % R W K    $ G G  ' H O H W H  % R W K    $ G G  ' H O H W H  % R W K
Figure11: DistributionoftheLoCintheResolved Figure12: DistributionoftheLoCintheAp-
Instances. pliedbutNotResolvedInstances.
14Table7: TheStatisticalAnalysisofOurFrameworkonAppliedbutNotResolvedInstances.
MAGIS Gold
Min Max Avg. Min Max Avg.
#CodeFiles 1 13 1.50 1 18 1.61
#Functions 1 13 1.50 1 18 1.61
#Hunks 1 28 2.52 1 52 3.72
#AddedLines 1 920 40.38 0 3,050 28.27
#DeletedLines 0 9,160 327.27 0 2,975 14.51
ChangeStartIndex 1 4,568 424.84 0 6,651 485.01
ChangeEndIndex 9 7,150 513.13 0 6,658 728.96
#ChangedLines 1 9,367 367.65 1 6,025 42.79
Thisdiscrepancymaypointtodifferentrepairstrategiesorattitudestowardsproblem-solving,where
theframeworkpresentedhereinmightprefertoreduceerrorsbyremovingpotentiallyproblematic
code,whereashumandevelopersmayleantowardsaddingnewcodetoaddressissues.
Moreover,acomparisonbetweenTable6andTable7revealsthatthelattercontainsahigheroverall
numberoffiles,hunks,andchangedlinesofcode. Theseinstances,involvingmoremodification
locations,correspondtomorecomplexscenarios. Thisphenomenonsuggeststhattheperformanceof
ourframeworkinresolvingsuchcomplexissuesrequiresfurtherenhancement.
Furthermore, the variability in difficulty across different software repositories may influence the
effectivenessofcodechanges. Tothisend,wecompilestatisticsontheresolvedratiosinvarious
softwarerepositories,asshowninFigure13. Fromthefigure,weobservethatthereisasignificant
variationintheresolvedratiosacrossdifferentrepositoriesinourframework. Somerepositorieshave
aresolvedratioashighas40%,whileothersarecloseto0%.Thissuggeststhatthedifferencesamong
varioussoftwaresuchascodestructureandcodingstylecanimpactthegenerationandapplicationof
thecodechange.
5.2 ComparisonwithDevin.
Devinisanovelagentforsoftwarede-    
    R H V R O Y H G
velopment[34],anditsperformance A S S O L H G
 5 H V R O Y H G  5 D W L R
has also been assessed using the
   
SWE-bench. However, the evalua-    
tion dataset employed by Devin dif-    
fers from the subset used for exper-
       
iments with GPT-4 reported by the
paperofSWE-bench[15]. Ananaly-    
sisoftherepositorynameandpullre-    
questIDofeachinstancerevealsthat   
only 140 instances overlap between
  
thetwodatasets.    
  
W s
r
Deta esi n vt oh c
il
ni ven ’es ss,t roh
2
eue
1
sr o(s l1f uh r
5
taa i%mr oe n)ed w
i os
fp o suo 1rk eo 8sl s (,u 1o
s
2c uf c
.r
8e p1 6s a4 s %s0 f su )il ni iln sgy-
-
   D V W U R S \  D V W U R  G S  M \  D Q J R  G M  P D  D Q  W J  S O R  R W O L E  P D W  Z S O  D V R W  N O L  R E  P P   V H D E R U Q  S D O O H W V  I O D V N  S V I  U H T X H V W V  S \ G D W D  [ D U U  S D  \ \  O L Q W  G H Y  S  S \ O  \ L  W Q  H W  V W  G H  V Y   F L S  N L \  W W   O H  H V W  D U Q  V F L N  V L W  S  O  K L H  Q D U  [  Q  G R F  V S K L  V Q  \ [  P S \  V \ P S \    
sues 11. This comparison, however, Figure13: TheNumberof 5A H S R V L W Rp U \  )p X O O Ql D Pi HedandResolvedInstances
inDifferentRepositories.
maynotbeentirelyequitable.Devin’s
possibleunderlyingLLMisunknown,
and it possesses the capability to integrate feedback from the environment. Moreover, Devin’s
reported scores are under the setting given the entire repository, and it operates with “common
developertoolsincludingtheshell,codeeditor,andbrowser”,and“agentswithinternetaccesscould
11https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs/
pass
15potentiallyfindexternalinformationthroughothermethods”asdetailedatthereport12. Incontrast,
ourapproachsolelyreliesontheshell,withouttheneedofanyadditionalexternaltools.
Forrunningtime,72%ofinstancesresolvedbyDevinrequiregreaterthan10minutestocomplete.
Incontrast,ourframeworkfinalizeseachresolvedissuewithinapproximately3minutes. Onaverage,
our framework completes the processing of each instance in under 5 minutes, demonstrating its
capabilitytoassistinresolvingGitHubissueswithminimaltimeexpenditure.
6 Limitation
Prompt. ThedesignofpromptwordsmayimpacttheperformanceofLLMs,therebyaffectingthe
validityandfairnessoftheresults[8]. Whilethispaperfocusesoninnovativeaspectsoftheproposed
frameworkdesignandreliesonpracticalguidelinesforthedesignofpromptwordtemplates[29]
toreducetheemergenceofdesignbiases,thecompleteeliminationofthepromptbiasisextremely
difficultduetotheinherentbiasesinthedatasetinstancesandthelimitationsofAPIresources.
Dataset. Thedatasetcontainsalimitedvarietyofsoftwaretypes. Theevaluatingdataset,SWE-
bench,encompasses12repositories,whichcoverthePythonprogramminglanguage. However,this
quantityremainsinsufficientcomparedtothediversesoftwareprojectsavailableonGitHub. The
codestyle,architecturaldesign,andimplementationtechniquesoftheseselectedrepositories,while
representative,cannotfullyreflectthediversityofallcoderepositories. Inparticular,thecurrent
datasetmayfailtoencompasssomespecializedfieldsordifferentprogrammingparadigms,such
as microservice architecture [41] and functional programming [16]. This limitation implies that,
althoughourframeworkisdesignedtobeindependentofanyspecificsoftware,thevalidationofits
effectivenessandgeneralapplicabilitymightbeaffectedbythislimitedsamplescope. Therefore,
applyingthefindingsofthispapertoothercoderepositoriesmayrequirefurthervalidation.
7 RelatedWork
7.1 LargeLanguageModels
LargeLanguageModels(LLMs)refertothepre-trainedlanguagemodelsthatcontainalargenumber
ofparameters[38]. Theparametercountsofthesemodelstypicallyrangeinthetensorhundredsof
billions. PopularLLMsincludetheGenerativePre-trainedTransformer(GPT)series,suchasGPT-
3[27],GPT-4[25],andtheopen-sourceLLaMA[35]whichpubliclysharesitsweightinformation.
Thefirstversionoftheopen-sourcemodelLLaMAhasparametersrangingfrom7billionto65billion.
Manyresearchers[33,11]havebuiltuponthefoundationofLLaMA,implementingenhancementsto
forgenewLLMs.TheseLLMshavedemonstratedformidablenaturallanguagegenerationcapabilities
ingeneralscenarios,withGPT-4,inparticular,standingout[18,39]. Ithasconsistentlymaintained
thetoppositioninseveralrankings,includingcodegeneration,reflectingitssignificantpotentialin
tasksrelatedtosoftwareengineering[13].
7.2 LLM-BasedMulti-AgentSystem
WiththepowerfultextgenerationcapabilitiesofLLMs, manyresearchers[12,31,7,37,26,36]
haveexploredtheconstructionofLLM-basedMulti-AgentSystems,enablingthemtoaccomplish
tasksbeyondthecapabilitiesoftheLLMsthemselves. Forexample,MetaGPT[12],whichsimulates
theStandardizedOperatingProcedures(SOPs)ofaprogrammingteam,completingtasksincluding
definition,design,planning,coding,andtestingthroughconstructedroles(e.g.,productmanagers,
architects,projectmanagers,etc.).ThisframeworkhasachievedleadingscoresontheHumanEval[9]
andMBPP[2],outperformingmanyLLMs,andresearchersshowitsabilitytocompleteasoftware
establishment(e.g.,acoderepositorytoplayGomokugame),indicatingthatamulti-agentframework
canbetterleveragethecapabilitiesofLLMsincodegenerationtasks. Moreover,Qianetal.[26]
designedChatDev,avirtualdevelopmentcompanysimulatingahumandevelopmentteam,which
decomposes requirements into atomic tasks assigned to the developers. Developers mitigate the
hallucinationthatmayarisewiththeLLMthroughmutualcommunicationandself-reflectionmech-
anisms. Experimental results show that ChatDev can complete the establishment of some small
12https://www.cognition-labs.com/introducing-devin
16software(averagingnomorethan5filespersoftware)inarelativelyshorttime(lessthan7minutes
onaverage). However,theseworksfocusonthetransformationfromtherequirementtosoftwareand
overlookthecodechangegenerationduringsoftwareevolutionwhichneedsnotonlyunderstanding
therequirementbutalsodealingwiththelargerepository.
8 Conclusion
Inconclusion,thispaperilluminatesthepotentialoflargelanguagemodelsinsoftwareevolution,
particularlyinresolvingGitHubissues. OurstudyidentifiesthechallengesofdirectLLMapplication.
Toaddressthem,weproposedanovelLLM-basedmulti-agentframework,MAGIS,enhancingissue
resolutionthroughwell-designedagents’collaboration.ThesuperiorityofMAGISontheSWE-bench
datasetagainstpopularLLMshighlightsitseffectiveness,pointingtowardsapromisingdirection
forintegratingLLMsintosoftwareevolutionworkflows. Thisworknotonlyshowsthepotentialof
LLMsinGitHubissueresolutionbutalsoexploresanLLM-basedparadigmforsoftwareevolution.
References
[1] Anthropic. Claude2. https://www.anthropic.com/news/claude-2,2023.
[2] JacobAustin,AugustusOdena,MaxwellI.Nye,MaartenBosma,HenrykMichalewski,David
Dohan,EllenJiang,CarrieJ.Cai,MichaelTerry,QuocV.Le,andCharlesSutton. Program
synthesiswithlargelanguagemodels. arXivPreprint,abs/2108.07732,2021. URLhttps:
//arxiv.org/abs/2108.07732.
[3] TobiasBaum,OlgaLiskin,KaiNiklas,andKurtSchneider. Factorsinfluencingcodereview
processesinindustry. InThomasZimmermann,JaneCleland-Huang,andZhendongSu,editors,
Proceedingsofthe24thACMSIGSOFTInternationalSymposiumonFoundationsofSoftware
Engineering,FSE2016,Seattle,WA,USA,November13-18,2016,pages85–96.ACM,2016.
doi: 10.1145/2950290.2950323. URLhttps://doi.org/10.1145/2950290.2950323.
[4] TegawendéF.Bissyandé,DavidLo,LingxiaoJiang,LaurentRéveillère,JacquesKlein,and
YvesLeTraon.Gotissues?whocaresaboutit?Alargescaleinvestigationofissuetrackersfrom
github.InIEEE24thInternationalSymposiumonSoftwareReliabilityEngineering,ISSRE2013,
Pasadena,CA,USA,November4-7,2013,pages188–197.IEEEComputerSociety,2013. doi:
10.1109/ISSRE.2013.6698918. URLhttps://doi.org/10.1109/ISSRE.2013.6698918.
[5] Amiangshu Bosu and Jeffrey C. Carver. Impact of developer reputation on code review
outcomesinOSSprojects: anempiricalinvestigation. InMaurizioMorisio,ToreDybå,and
MarcoTorchiano,editors,2014ACM-IEEEInternationalSymposiumonEmpiricalSoftware
EngineeringandMeasurement,ESEM’14,Torino,Italy,September18-19,2014,pages33:1–
33:10. ACM, 2014. doi: 10.1145/2652524.2652544. URL https://doi.org/10.1145/
2652524.2652544.
[6] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,Ece
Kamar,PeterLee,YinTatLee,YuanzhiLi,ScottM.Lundberg,HarshaNori,HamidPalangi,
MarcoTúlioRibeiro,andYiZhang. Sparksofartificialgeneralintelligence: Earlyexperiments
withGPT-4. arXivPreprint,abs/2303.12712,2023. doi: 10.48550/ARXIV.2303.12712. URL
https://doi.org/10.48550/arXiv.2303.12712.
[7] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie
Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent
debate. arXiv Preprint, abs/2308.07201, 2023. doi: 10.48550/ARXIV.2308.07201. URL
https://doi.org/10.48550/arXiv.2308.07201.
[8] Lichang Chen, Jiuhai Chen, Heng Huang, and Minhao Cheng. PTP: boosting stability and
performanceofprompttuningwithperturbation-basedregularizer. InHoudaBouamor,Juan
Pino,andKalikaBali,editors,Proceedingsofthe2023ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,EMNLP2023,Singapore,December6-10,2023,pages13512–
13525.AssociationforComputationalLinguistics,2023. URLhttps://aclanthology.org/
2023.emnlp-main.833.
17[9] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondédeOliveiraPinto,Jared
Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul
Puri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,Brooke
Chan,ScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,Mohammad
Bavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,Matthias
Plappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,Alex
Nichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain,
WilliamSaunders,ChristopherHesse,AndrewN.Carr,JanLeike,JoshuaAchiam,VedantMisra,
EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage,MiraMurati,KatieMayer,
PeterWelinder,BobMcGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciech
Zaremba. Evaluatinglargelanguagemodelstrainedoncode. arXivPreprint,abs/2107.03374,
2021. URLhttps://arxiv.org/abs/2107.03374.
[10] XueyingDu,MingweiLiu,KaixinWang,HanlinWang,JunweiLiu,YixuanChen,JiayiFeng,
Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval: A manually-crafted benchmark for
evaluatingllmsonclass-levelcodegeneration,2023.
[11] XinyangGengandHaoLiu. Openllama: Anopenreproductionofllama, May2023. URL
https://github.com/openlm-research/open_llama.
[12] SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,YuhengCheng,CeyaoZhang,
JinlinWang,ZiliWang,StevenKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,Lingfeng
Xiao,ChenglinWu,andJürgenSchmidhuber. Metagpt: Metaprogrammingforamulti-agent
collaborativeframework,2023.
[13] XinyiHou,YanjieZhao,YueLiu,ZhouYang,KailongWang,LiLi,XiapuLuo,DavidLo,
JohnC.Grundy,andHaoyuWang. Largelanguagemodelsforsoftwareengineering: Asystem-
aticliteraturereview.arXivPreprint,abs/2308.10620,2023.doi:10.48550/ARXIV.2308.10620.
URLhttps://doi.org/10.48550/arXiv.2308.10620.
[14] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and Thomas Zimmermann.
Practitioners’ expectations on automated code comment generation. In 44th IEEE/ACM
44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA,
May 25-27, 2022, pages 1693–1705. ACM, 2022. doi: 10.1145/3510003.3510152. URL
https://doi.org/10.1145/3510003.3510152.
[15] CarlosE.Jimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthik
Narasimhan.Swe-bench:Canlanguagemodelsresolvereal-worldgithubissues? InTheTwelfth
InternationalConferenceonLearningRepresentations,ICLR2024,Vienna,Austria,May7-11,
2024.OpenReview.net,2024. URLhttps://openreview.net/forum?id=VTF8yNQM66.
[16] ThomasJohnsson. Attributegrammarsasafunctionalprogrammingparadigm. InGillesKahn,
editor, Functional Programming Languages and Computer Architecture, Portland, Oregon,
USA,September14-16,1987,Proceedings,volume274ofLectureNotesinComputerScience,
pages154–173.Springer,1987. doi: 10.1007/3-540-18317-5\_10. URLhttps://doi.org/
10.1007/3-540-18317-5_10.
[17] OleksiiKononenko,OlgaBaysal,LatifaGuerrouj,YaxinCao,andMichaelW.Godfrey. In-
vestigatingcodereviewquality: Dopeopleandparticipationmatter? InRainerKoschke,Jens
Krinke, and Martin P. Robillard, editors, 2015 IEEE International Conference on Software
MaintenanceandEvolution,ICSME2015,Bremen,Germany,September29-October1,2015,
pages 111–120. IEEE Computer Society, 2015. doi: 10.1109/ICSM.2015.7332457. URL
https://doi.org/10.1109/ICSM.2015.7332457.
[18] JiaweiLiu,ChunqiuStevenXia,YuyaoWang,andLingmingZhang. Isyourcodegenerated
bychatgptreallycorrect? rigorousevaluationoflargelanguagemodelsforcodegeneration.
In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey
Levine,editors,AdvancesinNeuralInformationProcessingSystems36: AnnualConferenceon
NeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December
10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html.
18[19] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio
Petroni, and Percy Liang. Lost in the middle: How language models use long con-
texts. arXiv Preprint, abs/2307.03172, 2023. doi: 10.48550/ARXIV.2307.03172. URL
https://doi.org/10.48550/arXiv.2307.03172.
[20] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio
Petroni, and Percy Liang. Lost in the middle: How language models use long con-
texts. arXiv Preprint, abs/2307.03172, 2023. doi: 10.48550/ARXIV.2307.03172. URL
https://doi.org/10.48550/arXiv.2307.03172.
[21] ShaneMcIntosh,YasutakaKamei,BramAdams,andAhmedE.Hassan. Theimpactofcode
reviewcoverageandcodereviewparticipationonsoftwarequality: acasestudyoftheqt,vtk,
andITKprojects. InPremkumarT.Devanbu, SungKim, andMartinPinzger, editors, 11th
WorkingConferenceonMiningSoftwareRepositories,MSR2014,Proceedings,May31-June
1,2014,Hyderabad,India,pages192–201.ACM,2014. doi: 10.1145/2597073.2597076. URL
https://doi.org/10.1145/2597073.2597076.
[22] FangwenMu,XiaoChen,LinShi,SongWang,andQingWang. Developer-intentdrivencode
commentgeneration. In45thIEEE/ACMInternationalConferenceonSoftwareEngineering,
ICSE2023,Melbourne,Australia,May14-20,2023,pages768–780.IEEE,2023. doi: 10.1109/
ICSE48619.2023.00073. URLhttps://doi.org/10.1109/ICSE48619.2023.00073.
[23] OpenAI. GPT-4 technical report, 2023. URL https://doi.org/10.48550/
arXiv.2303.08774.
[24] OpenAI. Gpt-3.5turbofine-tuningandapiupdates. https://openai.com/blog/gpt-3-5-
turbo-fine-tuning-and-api-updates,2023.
[25] OpenAI. Gpt-4. https://openai.com/research/gpt-4,2023.
[26] ChenQian,XinCong,WeiLiu,ChengYang,WeizeChen,YushengSu,YufanDang,Jiahao
Li,JuyuanXu,DahaiLi,ZhiyuanLiu,andMaosongSun. Communicativeagentsforsoftware
development. arXivPreprint,2023.
[27] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training,2018.
[28] StephenE.Robertson, SteveWalker, SusanJones, MichelineHancock-Beaulieu, andMike
Gatford.OkapiatTREC-3.InDonnaK.Harman,editor,ProceedingsofTheThirdTextREtrieval
Conference,TREC1994,Gaithersburg,Maryland,USA,November2-4,1994,volume500-225
ofNISTSpecialPublication,pages109–126.NationalInstituteofStandardsandTechnology
(NIST),1994. URLhttp://trec.nist.gov/pubs/trec3/papers/city.ps.gz.
[29] Jessica Shieh. Best practices for prompt engineering with openai api. OpenAI, Febru-
aryhttps://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-
openai-api,2023.
[30] QiushiSun,ZhiruiChen,FangzhiXu,KanzhiCheng,ChangMa,ZhangyueYin,JianingWang,
ChengchengHan,RenyuZhu,ShuaiYuan,QipengGuo,XipengQiu,PengchengYin,Xiaoli
Li,FeiYuan,LingpengKong,XiangLi,andZhiyongWu. Asurveyofneuralcodeintelligence:
Paradigms,advancesandbeyond,2024.
[31] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the
power of intelligent LLM agents. arXiv Preprint, abs/2306.03314, 2023. doi: 10.48550/
ARXIV.2306.03314. URLhttps://doi.org/10.48550/arXiv.2306.03314.
[32] WeiTao,YuchengZhou,YanlinWang,HongyuZhang,HaofenWang,andWenqiangZhang.
Kadel: Knowledge-aware denoising learning for commit message generation. ACM Trans.
Softw. Eng. Methodol., jan 2024. ISSN 1049-331X. doi: 10.1145/3643675. URL https:
//doi.org/10.1145/3643675.
[33] LLaMA-MoE Team. Llama-moe: Building mixture-of-experts from llama with continual
pre-training,Dec2023. URLhttps://github.com/pjlab-sys4nlp/llama-moe.
19[34] The Cognition Team. Swe-bench technical report, 2024. URL https://www.cognition-
labs.com/post/swe-bench-technical-report.
[35] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,
ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[36] Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, and Neel
Sundaresan. Autodev: Automatedai-drivendevelopment,2024.
[37] QingyunWu,GaganBansal,JieyuZhang,YiranWu,ShaokunZhang,ErkangZhu,BeibinLi,
LiJiang,XiaoyunZhang,andChiWang. Autogen: Enablingnext-genLLMapplicationsvia
multi-agentconversationframework. arXivPreprint,abs/2308.08155,2023. doi: 10.48550/
ARXIV.2308.08155. URLhttps://doi.org/10.48550/arXiv.2308.08155.
[38] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,
BeichenZhang,JunjieZhang,ZicanDong,YifanDu,ChenYang,YushuoChen,ZhipengChen,
JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,and
Ji-RongWen. Asurveyoflargelanguagemodels. arXivPreprint,abs/2303.18223,2023. doi:
10.48550/ARXIV.2303.18223. URLhttps://doi.org/10.48550/arXiv.2303.18223.
[39] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh,
Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, edi-
tors, Advances in Neural Information Processing Systems 36: Annual Conference on Neu-
ralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December
10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html.
[40] ZibinZheng,KaiwenNing,JiachiChen,YanlinWang,WenqingChen,LianghongGuo,and
WeichengWang. Towardsanunderstandingoflargelanguagemodelsinsoftwareengineering
tasks. arXiv Preprint, abs/2308.11396, 2023. doi: 10.48550/ARXIV.2308.11396. URL
https://doi.org/10.48550/arXiv.2308.11396.
[41] XiangZhou,XinPeng,TaoXie,JunSun,ChaoJi,WenhaiLi,andDanDing. Faultanalysis
anddebuggingofmicroservicesystems: Industrialsurvey,benchmarksystem,andempirical
study. IEEETrans.SoftwareEng.,47(2):243–260,2021. doi: 10.1109/TSE.2018.2887384.
URLhttps://doi.org/10.1109/TSE.2018.2887384.
[42] Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang
Lou, and Jianbing Shen. Thread of thought unraveling chaotic contexts. arXiv Preprint,
abs/2311.08734, 2023. doi: 10.48550/ARXIV.2311.08734. URL https://doi.org/
10.48550/arXiv.2311.08734.
20