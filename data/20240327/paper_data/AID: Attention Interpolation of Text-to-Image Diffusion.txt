AID: Attention Interpolation of Text-to-Image
Diffusion
Qiyuan He1 Jinghao Wang2 Ziwei Liu2 Angela Yao1, (cid:0)
1National University of Singapore 2S-Lab, Nanyang Technological University
qhe@u.nus.edu.sg ayao@comp.nus.edu.sg
{jinghao003, ziwei.liu}@ntu.edu.sg
(a) "Cinematic film still, stormtrooper taking aim..." to "Vaporwave synthwave style
Los Angeles street...."
(b) "Anime artwork: a Pokemon called Pikachu sitting on the grass..." to "Anime
artwork: a beautiful girl..."
(c) "Lion" to "Gundam"
(d) "Truck" to "Cat"
(f) "A photo of dog" to "A photo of car", guided with "A
dog driving car", "A car with dog furry texture" and "A toy
(e) "Ship" to "Airplane" named dog-car"
Fig.1: Our approach enables text-to-image diffusion models to generate
nuanced spatial and conceptual interpolations. Our method ensures seamless
transitions in layout (examples in (a)), and facilitates smooth conceptual blending
(examples in (b)-(e)). Furthermore, our approach empowers users to select specific
interpolation paths using guidance prompts, illustrated in examples (f).
Abstract. Conditional diffusion models can create unseen images in
varioussettings,aidingimageinterpolation.Interpolationinlatentspaces
is well-studied, but interpolation with specific conditions like text or
posesislessunderstood.Simpleapproaches,suchaslinearinterpolation
in the space of conditions, often result in images that lack consistency,
smoothness,andfidelity.Tothatend,weintroduceanoveltraining-free
techniquenamedAttentionInterpolationviaDiffusion(AID).Our
4202
raM
62
]VC.sc[
1v42971.3042:viXra2 Q. He et al.
key contributions include 1) proposing an inner/outer interpolated at-
tention layer; 2) fusing the interpolated attention with self-attention to
boost fidelity; and 3) applying beta distribution to selection to increase
smoothness.Wealsopresentavariant,Prompt-guidedAttentionIn-
terpolation via Diffusion (PAID), that considers interpolation as a
condition-dependent generative process. This method enables the cre-
ationofnewimageswithgreaterconsistency,smoothness,andefficiency,
and offers control over the exact path of interpolation. Our approach
demonstrateseffectivenessforconceptualandspatialinterpolation.Code
and demo are available at https://github.com/QY-H00/attention-
interpolation-diffusion.
1 Introduction
Interpolating within the latent space of generative models such as Variational
Autoencoders (VAEs) [15], Generative Adversarial Networks (GANs) [5], and
diffusion models [10] has been extensively investigated [14,35,36]. Interpolation
allows models to generate smoothly transitioning images from one seed to an-
other within the latent space, facilitating applications such as image attribute
modification [32], data augmentation [30], and video interpolation [39]. Recent
advancements in text-to-image diffusion models [27,29] have shown capability
in producing high-quality images conditioned on textual descriptions. Different
fromlatentspaceinterpolation,interpolatingwithinaconditioningspace,inthis
case text, remains relatively underexplored. This work explores the novel ques-
tion of how to interpolate between distinct conditions, such as "a truck" and "a
cat", within a diffusion model context.
The most closely related work involves using a diffusion model for interpo-
lating between two real-world images [44]. They caption the image to text and
then adopt a linear interpolation within the text embedding space for the two
images.However,thisapproachmayfailwhentheconceptsdivergesignificantly,
thus underscoring the inherent difficulty of the task as the first row in Fig. 2
shows.
We identify three main challenges in condition space interpolation: ensuring
thematic consistency (to avoid irrelevant transitions such as "a dog" to "a cat"
via "a bowl"), securing smooth visual transitions between adjacent images, and
achieving high-quality interpolated images. Based on these three challenges, we
formulate the task of conditional interpolation and propose three evaluation
metrics: consistency, smoothness, and fidelity.
To address these issues, we introduce a novel framework, Attention Interpo-
lation of Diffusion (AID), which consists of several innovations to enhance the
quality of conditional interpolation: 1) Replacing the standard attention with
a dual (inner/outer) interpolated-attention mechanism to maintain essential vi-
sual features from both source images, thereby improving consistency; 2) Com-
bining interpolated-attention with self-attention to further enhance consistency
and image fidelity; 3) proposing Beta distribution selection along the interpo-
lation path to increase smoothness. We also re-conceptualize the interpolationAID: Attention Interpolation of Text-to-Image Diffusion 3
as a condition-dependent generative process and introduce 4) Prompt-guided
Attention Interpolation of Diffusion (PAID).
Our method is training-free and the experimental results affirm that it sig-
nificantly enhances the smoothness, consistency, and fidelity of the interpolated
sequences across various conditions, as illustrated in Figures 1, 5, and 6. AID
effectively manages interpolation between different concepts or spatial layouts
using inner or outer interpolated attention and can be further tailored with
prompt guidance, as demonstrated in our results.
In summary, our main contributions are:
– Formulating a new problem of conditional interpolation within the text-to-
image diffusion model context and proposing three evaluation metrics to
assess consistency, smoothness, and fidelity.
– Introducing a novel, zero-shot method called Attention Interpolation of Dif-
fusion (AID) that includes a fused inner/outer interpolated attention mech-
anism and Beta prior selection to enhance the quality of interpolation se-
quences. AID can be augmented with prompt-guided interpolation (PAID)
to control specific paths between two conditions;
– Demonstrating through extensive experiments that AID substantially im-
provesbothqualitativeandquantitativemeasuresofinterpolationsequences,
with significant improvements in fidelity, consistency, and smoothness.
2 Related Work
2.1 Diffusion Models and Attention Manipulation
The emergence of diffusion models has significantly transformed the text-to-
image synthesis domain, ensuring enhanced quality and better alignment with
textualdescriptions,asevidencedinrecentstudies[24,27,29].Attentionmanip-
ulationtechniqueshavebeeninstrumentalinunlockingthepotentialofdiffusion
models, particularly in applications such as in-painting and compositional ob-
jectgeneration.Theseapplicationsbenefitfromrefinedcontrolovertheattention
maps,aligningthemodifierandthetargetobjectmorecloselytoenhanceimage
coherence [1,2,8,25,43]. Furthermore, cross-frame attention mechanisms have
shown promise in augmenting visual consistency within video generation frame-
worksutilizingdiffusionmodels[13,23].Thisbodyofworksuggestsanoteworthy
hypothesis: the visual closeness of two generated images might be reflected in
the similarity of their respective attention maps. It also motivates us to study
interpolation from the perspective of attention mechanisms.
2.2 Interpolation in Image Generative Models
Interpolation within the latent spaces of generative models, such as Generative
Adversarial Networks (GANs) [5] and Variational Auto Encoders (VAEs) [15],
has been studied extensively [14,36,39]. Recent advancements have extended
this exploration to the latent spaces of diffusion models, enabling more realistic4 Q. He et al.
interpolations between real-world images [16,30]. Works to date, however, are
limited to a single condition, and there is a lack of research focused on interpo-
lation under varying conditions. Wang et al. [42] explored linear interpolation
within text embeddings for the usage to interpolate real-world images, yet this
approachoftenresultsinimagesofdiminishedfidelityandsmoothness.Thisgap
in research underscores the need for further exploration of conditional interpo-
lation within generative models, potentially expanding their applicability across
various domains.
3 Preliminaries and Analysis
3.1 Preliminaries
Latent diffusion model. Latent diffusion model such as the Stable Diffusion
Model [22,27] is a text-to-image diffusion model operating in the latent space
given by a VQ-VAE auto-encoder [4,40]. Consider inference with T denoising
time steps. Given a text embedding c, it samples z ∼ N(0,1) from the latent
T
space, and at the step j, it denoises the latent variable z to z by sampling
j j−1
from:
p (z |z )=N(z ;µ (z ,c,j),Σ (z ,c,j)) (1)
θ j−1 j j−1 θ j θ j
whereµ (z ,c,j)iscommonlyoutputbyaUNet[28],andΣ (z ,c,j))isusually
θ j θ j
determined by a noise scheduler such as DDPM [10] and DDIM [35]. Once ob-
tainingz ,weusethedecoderDofthelatentspacetodecodethelatentvariable
0
and generate the corresponding image by D(z ).
0
Attention mechanism. Attention is broadly used in text-to-image diffusion
models [20,27,29]. Cross-attention is especially used to introduce conditions
into the generation of images. Specifically, given a latent variable z ∈ Rdl, its
condition embedding c ∈ Rdl and the attention layer in forms of matrix W
q
∈
Rdl×dq, W
k
∈Rdl×dk and W
v
∈Rdl×dv, the output is computed as
Q=WTz, K =WTc, V =WTc
q k v
QKT (2)
f(z,c)=Attn(Q,K,V)=softmax( √ )V.
d
k
In the same vein, self-attention is calculated using the function f(z,z). For
brevity, we use a simplified notation to represent multi-head attention [41] and
denote the attention layer as Attn(Q,K,V) in both cross-attention and self-
attention scenarios.
Interpolation.Interpolationiswell-studiedincomputergraphics[21,26].Inthis
paper, we mainly focus on linear interpolation between tensors. Given tensor A
and tensor B, the linear interpolation path r(t) where t∈[0,1] is defined as:
r(t;A,B)=(1−t)A+tB (3)
Typically, to discretize the interpolation path, previous works [14,30,36,39,
44] adopt uniformly distributed points to provide the interpolation sequence.AID: Attention Interpolation of Text-to-Image Diffusion 5
Formally, given size m, the sequence is given as:
1 2 m−2
I ={r(0),r( ),r( ),...,r( ),r(1)} (4)
1:m m−1 m−1 m−1
3.2 Problem formulation of conditional interpolation
Task description. Given a diffusion model M, latent variable seed z and z ,
1 m
condition c and c and a sequence length m, our objective is to generate an
1 m
interpolationsequenceofimagesI ={I ,I ,...,I },wherethesourceimages
1:m 1 2 m
are I = M(z ,c ), I = M(z ,c ), ensuring that the sequence I exhibits
1 1 1 m m m 1:m
high consistency, smoothness, and fidelity. To assess the sequence, we adopt
established indicators such as LPIPS and FID scores and tailor them to our
interpolation setting.
Perceptual consistency. Perceptual consistency refers to how "long" the per-
ceptual transition of the interpolation sequence is. Similar to the Perceptual
Path Length (PPL) [12], we use the LPIPS (Learned Perceptual Image Patch
Similarity) metric [45] to assess the consistency of our interpolation. However,
we target evaluating the quality of discrete sequences derived from the interpo-
lation path, rather than the continuous path itself. Thus, we introduce discrete
Perceptual Path Length as the average LPIPS value across all adjacent pairs in
the sequence. Formally, with P denoting the LPIPS model, the consistency of a
sequence I is defined as:
1:m
m−1
1 (cid:88)
C(I ;P)= P(I ,I ) (5)
1:m m−1 i i+1
i=1
Perceptual smoothness. Smoothness is often overlooked in previous work
interpolating in latent space. Their main focus is to evaluate a continuous per-
ceptual path, where PPL both indicates consistency and smoothness. However,
this is not true when considering discrete interpolation sequences. For example,
if all the interpolated images in the sequence are the same, the sequence is not
smooth, yet the consistency value is low. Therefore, we need a distinct index to
evaluate smoothness.
Specifically, the distance between each neighbor should be as similar as
possible. The Gini coefficient [3] is a conventional indicator on data imbal-
anced [6,7,38]. Formally, given a set X = {x ,x ,...,x } and their mean value
1 2 n
x¯, it’s defined as:
(cid:80)n (cid:80)n
|x −x |
G(X)= i=1 j=1 i j (6)
2n2x¯
wheretheresultiswithin[0,1].AhigherG(X)indicatesmoreimbalance.There-
fore we propose to apply Gini coefficients on the perceptual distance between
each neighbour to indicate smoothness. Formally, with P denoting the LPIPS
model, the perceptual smoothness of a sequence I is defined as:
1:m
m−1
(cid:91)
S(I ;P)=1−G( P(I ,I )). (7)
1:m i i+1
i=16 Q. He et al.
(a) "oxygen mask" to "a lion" (b) "an apple" to "a bed"
Fig.2: Results comparison between text embedding interpolation (the 1st
row) and AID (the 2nd row). AID increases smoothness, consistency, and fidelity
significantly.
Fidelity. Following previous methods [30], we evaluate the fidelity of interpo-
latedimageswiththeFréchetInceptionDistance(FID)[9].Givenkinterpolated
sequences{I(1),I(2),...,I(k)},thefidelityofthesequencesisdefinedastheFID
1:m 1:m 1:m
between the source images and the interpolated images:
 
k k
F(I 1(1 :m),I 1(2 :m),...,I 1(k :m))=FID
MV
(cid:91) {I 1(j),I m(j)}, (cid:91) {I i(j)|i̸=1,i̸=m} (8)
j=1 j=1
wheremodelM isavisualinceptionmodel1.FIDevaluateshowtheperceptual
V
feature distribution of interpolated images is far away from source images.
Our formulation diverges from previous studies by concentrating on the as-
sessment of discrete samples, referred to as the interpolation sequence, instead
of the continuous interpolation path. This is crucial because the quality of the
interpolationsequenceisdeterminednotonlybytheinterpolationpath’squality
but also by how to select the exact sample along the interpolation path, which
previous methods overlook. Additionally, the size of an interpolation sequence
is often low in practical usage [30,44]. As a result, our evaluation framework is
specifically designed to cater to interpolation sequences.
These three metrics quantitatively evaluate the quality of interpolated se-
quences.Belowwedescribehowtheseevaluationmetricsreflectthechallengein
this task.
3.3 Failures of naive conditional interpolation
Themostbasicapproachtoconditionalinterpolationistheapplicationoflinear
or spherical interpolation [16,42] within the encoded condition space. For ex-
ample, [42] interpolates between different text embedding and pose, while [16]
interpolates between vectors encoding different styles. We observe three issues
thatmakesuchapproachesproblematic,leadingtoresultswithpoorconsistency,
smoothness, and fidelity.
Issue 1: Indirect routes. The route through a text embedding space does not
align with natural routing in the visual space, contributing to reduced consis-
tency. For example, interpolation from "an apple" to "a bed" may pass through
intermediate stages like "a messy sketch" (see Fig. 2 (b)).
Issue 2: Non-uniform transition. The text embedding space is not guaran-
teed to be uniform, leading to non-smooth visual transitions in interpolation
1 Typically, Inception v3 [37] is used.AID: Attention Interpolation of Text-to-Image Diffusion 7
sequencewithselectinguniformlydistributedpoints.Conditionsappearingwith
higherfrequencyinthetrainingdatasetofthegenerativemodeldominatethein-
terpolationpath.Toverify,weinterpolatebetweenarareconceptandacommon
concept with different levels of exposure to the training of the Stable Diffusion
model [27]. Take the example of interpolation sequence from "oxygen mask", a
rare concept [18] to "lion" (see Fig. 2 (a)), most interpolated images are influ-
enced by "lion".
Issue 3: Commonly low quality. Interpolated text embeddings may not cor-
respond to an existing text, which means it may have a significant departure
from the distribution of original text embedding, resulting in lower-quality im-
ages. This phenomenon is also present in the interpolation from "an apple” to
"a bed" where the fidelity of interpolated images diminishes in comparison to
the originals. This degradation in image quality across the interpolation path,
from the second to the fifth column, is depicted in Fig. 2.
We conduct extensive experiments to further verify these observations in
Sec. 5.3. These issues motivate us to propose our methods to address each one
accordingly.
4 Methodology
ToaddresstheissueswementionedinSec.3.3,weproposecorrespondingmeth-
odstoaddresseachofthem:(1)interpolatingattentiontoboostperceptualcon-
sistency; (2) fusing with self-attention to boost fidelity; (3) introducing a Beta
prior to boosting smoothness. Finally, going beyond the quantitative evaluation
metric on the quality of specific interpolation sequence, we recast interpolation
dependent on the text description and propose (4) guiding interpolation with
prompts. Each component is described in detail as follows.
4.1 Inner/outer interpolated attention mechanism
To address the indirect routes issue mentioned in Sec. 3.3 and improve the con-
sistency between interpolated images and the source images, we begin with the
process of linear interpolation on condition. To simplify, z = [z ,z ,...,z ]
1:m 1 2 m
as the latent variable sequence to generate interpolation sequence, and t =
1:m
[t ,t ,...,t ] as the coefficients for interpolation.
1 2 m
Typically, text-to-image generation models utilize a cross-attention mecha-
nism. For a specific latent z and its corresponding condition c, it correlates key
K andvalueV fromc,withqueriesQderivedfromz.Foreasiernotation,were-
fer Q ,K ,V as the corresponding query, key and value of [z ,z ,...,z ].
1:m 1:m 1:m 1 2 m
Linearinterpolationonthetextembeddingisequivalenttoformulatingthetext-
image cross-attention module as:
Intp-Attn (Q ,K ,V ;t )=Attn(Q ,(1−t )K +t K ,
I i 1:m 1:m i i i 1 i m (9)
(1−t )V +t V )
i 1 i m
where Attn is the attention layer which is defined in Sec. 3.1, We call this in-
terpolationasinner-interpolated attention,sinceitinterpolatesthekeyand
value separately.8 Q. He et al.
Inner-interpolatedattentionincorporatestheconditioninginformationc and
1
c of source images by introducing K ,K ,V ,V in the cross-attention layer.
m 1 m 1 m
Whenprocessingtheinterpolatedlatentvariablethroughtheself-attentionlayer,
though,theinterpolatedlatentvariablenolongerinteractswiththesourcelatent
variable z and z . Given that the source latent variable encodes the visual
1 m
content of the source images, we expand the interpolation to the self-attention,
whereK andV arederivedfromz ,therebyboostingvisualconsistencybetween
i i i
the source and interpolated images.
Theinterpolationwithintheattentionmechanismcanbeimplementedintwo
forms:eitherbyinner-interpolatedattentionasEq.9orbyinterpolatingtheout-
put(contextvector)oftheattentionprocess,whichwecallouter interpolated
attention. The latter can be formalized as:
Intp-Attn (Q ,K ,V ;t )=(1−t )·Attn(Q ,K ,V )
O i 1:m 1:m i i i 1 1 (10)
+t ·Attn(Q ,K ,V )
i i m m
The distinction between inner interpolated attention from Eq.9 and outer in-
terpolated attention from Eq.10 is in their value vectors: Eq.9 utilizes the same
attention map for both V and V , whereas Eq.10 uses distinct attention maps
1 m
for value vectors originating from different sources. We demonstrate that each
method exhibits strengths on different metrics from Sec. 5.3. We observe that
innerattentioninterpolationisbettertodoconceptualinterpolationwhileouter
attention interpolation prefers spatial interpolation. We show the difference be-
tween the two implementations in terms of mathematical induction and more
qualitative results in Sec. A of the Appendix. In practice, we use AID-O as the
defaultchoiceandAID-Iisrecommendedtobeusedonlywithpromptguidance.
4.2 Fusion with self-attention
We observe that simply applying attention interpolation results in much higher
efficiency.However,theinterpolatedsequencestilllacksfidelity,echoingthequal-
ity issue raised in Sec. 3.3. We hypothesize the cause as fully replacing the self-
attentionmechanism,thoughself-attentionisconsideredessentialtothequality
ofgeneratedimage[11,23].Therefore,weincorporatethekeyandvalueofthein-
terpolatedlatentvariableitselfintoEqs.9and10withconcatenation,leadingto
afusedattentioninterpolation.Formally,thefusedversionofinner-interpolated
attention can be expressed as:
Intp-AttnF(Q ,K ,V ;t )=Attn(Q ,[(1−t )K +t K , K ],
I i 1:m 1:m i i i 1 i m i (11)
[(1−t )V +t V , V ]).
i 1 i m i
The fused version of the outer interpolated attention can be expressed as:
Intp-AttnF(Q ,K ,V ;t )=(1−t )·Attn(Q ,[K ,K ],[V ,V ])
O i 1:m 1:m i i i 1 i 1 i (12)
+ t ·Attn(Q ,[K ,K ],[V ,V ])
i i m i m i
Fusedattentioninterpolationleadstosignificantimprovementsinconsistency
and fidelity (see results in Sec. 5.3).AID: Attention Interpolation of Text-to-Image Diffusion 9
Fig.3: Effect of beta prior and the corresponding interpolation sequence
generated by AID. We show the results from "a cat" to "a giraffe", where from
top to bottom, smoothness is 0.79, 0.24, and 0.57, respectively. Adopting uniformly
distributed points (α = 1,β = 1) within the input space does not provide a uniform
visual transition between two images. Instead, applying beta distribution can adjust
the selected points to make the generated image sequence smoother. In the example
provided, adopting α slightly smaller than β achieves the highest smoothness.
4.3 Sequence selection with a Beta prior
Commonly, interpolation methods [9,30,44] select uniformly spaced points as
given by Eq. 4 to generate the sequence to represent the interpolation path.
However, as we observed in Sec. 3.3, uniformly spaced points in the input space
of the diffusion model do not provide smooth visual transitions (see blue arrow
illustration in Fig. 3). Different from interpolation in the latent space, which is
only introduced in the initial denoising steps, the diffusion model incorporates
thetextembeddingformultipledenoisingsteps.Thismayamplifytheinfluence
of the source latent variable with higher coefficients. Therefore, when t is close
to 0 or 1, r′(t) is closer to 0, leading to the intuition that we want to sample
more mid-range t.
These observations motivate us to apply a Beta distribution p (t,α,β) to
B
select specific interpolated images on the interpolation path. Beta distributions
are conveniently defined within the range of [0,1]. When α = 1 and β = 1,
p degenerates to a uniform distribution, which guarantees the lower-bound
B
performance as the original setting. When α > 1 and β > 1, the distribution
is concave (bell-shaped), with higher probabilities away from the end-points
of 0 and 1, i.e. away from the source images. Finally, the selected points are
adjustable based on alpha and beta values, to give higher preference towards
one or the other source image (see Fig;. 3).
Formally, given the Beta prior p and its cumulative distribution function
B
F (t,α,β), the path r(t) becomes r(F−1(t,α,β)) where t∼U(0,1). Therefore,
B
the distributed point over beta prior becomes:
1 m−2
{r(0),r(F−1( ,α,β)),...,r(F−1( ,α,β)),r(1)} (13)
B m−1 B m−1
To find the optimal hyperparameter α and β, we apply Bayesian optimiza-
tion [34] on α and β to optimize the consistency of the generated interpolation
sequence. We provide more details about the selection of α and β in Sec. B of
the Appendix.10 Q. He et al.
Fig.4: Effect of prompt guidance. We show the results to generate the compo-
sitional scenario of "an airplane and a deer" via prompt guidance, where the vanilla
Stable Diffusion model fails to generate the image following the description. With
prompt guidance, we release the ability of the model to generate the interpolation se-
quence following the description.
4.4 Prompt guidance
One insight we have is that given two source images, the hypothesis space for
interpolation path is actually large and diverse, yet most interpolation meth-
ods[30,44]provideadeterministicpathbetweentwosourceimages.Itnaturally
begs the question of how to control the interpolation path that we want.
Injected guidance prompts as conditions. Fortunately, as we incorporate
the interpolation mechanism in the original self-attention layer, we no longer
need to rely on interpolation along the text embedding. Instead, we replace the
input text embedding of interpolated images with the text embedding of the
guidance prompt. Suppose the key and value obtained from the guiding prompt
are K and V respectively. Guidance via a prompt is performed by replacing
g g
the original text-image cross-attention with:
Attn(Q ,K ,V ):=Attn (Q ,K ,V ) (14)
i i i G i g g
In practice, the guidance prompt is provided by users to choose the interpo-
lation path conditioned on the text description as Fig. 1 (f) shows. Surprisingly,
weobservethattheprompt-guidedattentioninterpolationenablesthemodelto
generate some compositional scenes that did not arise originally (see the exam-
ple of "an airplane and a deer" in Fig. 4). We show more qualitative examples
in Sec. D of the Appendix.
Warmup steps for the trade-off between interpolation and guidance.
We observe that early steps in denoising are essential to determine the spatial
layout of the generated image. Thus, we can trade off between the effect of
interpolation and prompt guidance by setting the number of warmup steps.
After several warmup steps, we transform the attention interpolation into a
simple generation process. We provide more details in Sec. C of the Appendix.AID: Attention Interpolation of Text-to-Image Diffusion 11
5 Experiments
5.1 Experimental setup
Evaluationprotocol.Weevaluateourapproachbasedonconsistency,smooth-
ness, and fidelity, which are summarized in Sec. 3.2. For experiments in each
dataset, we run 5 trials each with N=100 iterations. In each iteration, we ran-
domly select two conditions and generate an interpolation sequence with size
m=7. Then we evaluate the interpolation methods based on the mean value
over all the interpolation sequences. For consistency and smoothness, we fol-
low conventional settings and choose VGG16 [33] to compute LPIPS [45]. For
fidelity, we adapt the Google v3 Inception Model [37] following previous litera-
turetocomputeFIDbetweensourceimagesandinterpolatedimages.Wereport
the mean value across all iterations and trials as the final result.
Inference configuration. We take the Stable Diffusion 1.4 [27] as our base
model to implement the attention interpolation mechanism. Apart from replac-
ingtheattentionmodule,wekeepothersettingsthesameastheoriginalmodel.
In all experiments, the image is generated with size 512x512 with the DDIM
Scheduler [35] with 25 timesteps.
Selection configuration. In terms of Bayesian optimization on α and β in
the beta prior to applying our selection approach, we set the smoothness of
the interpolation sequence as the objective target, [1,30] as the range of both
hyperparameters,9fixedexplorationwhereαandβ arechosenfrom{20,25,30},
and 15 iterations to optimize.
Datasets. Our proposed framework is evaluated using corpora from various
datasets to assess its performance across different textual conditions.
– CIFAR-10: The CIFAR-10 dataset [17] comprises 60,000 32x32 color im-
ages distributed across 10 classes. This dataset is commonly used to bench-
mark classification algorithms. In our context, we utilize the class names
as prompts to generate images corresponding to specific categories. The
CIFAR-10corpusaidsinassessingtheeffectivenessofourframework,PAID,
in handling brief prompts that describe clear-cut concepts.
– LAION-Aesthetics: We sample the LAION-Aesthetics dataset from the
larger LAION-5B collection [31] with aesthetics score over 6, curated for its
highvisualquality.UnlikeCIFAR-10,thisdatasetprovidesextensiveground
truthcaptionsforimages,encompassinglengthyandlessdirectdescriptions.
These characteristics present more complex challenges for text-based analy-
sis.Weemploythedatasettotestourframework’sinterpolationcapabilities
in more demanding scenarios.
Comparison methods. Tothebestofourknowledge,theonlyrelatedmethod
is [44], which applies linear interpolation on the text embedding for real-world
image interpolation. We call it Text Embedding Interpolation below. Addition-
ally, we compare with another method called Denoising Interpolation, which in-
terpolatesalongthedenoisingschedule.Specifically,givenpromptAandprompt12 Q. He et al.
(a) "Dog - Oil Painting" to "Bird - (b) "banana" to "pen"
Chinese Painting"
Fig.5:QualitativecomparisonbetweenAID-O(the1strow)andAID-I(the
2ndrow).WhileAID-Opreferskeepingthespatiallayout,AID-Iprefersinterpolating
theconceptandstyle.Comparingthe4thcolumnin(b),AID-Iproperlycaptures"pen
in the shape of banana" while AID-O provides a banana but the spatial layout is the
same as the pen.
(a) "Fox - Watercolor Art Print" to (b) "Louis Henry Sullivan (September 3,
"Merry-Christmas-from- 1856 – April 14, 1924) was an American
AnnetteFunicello1960.jpeg" architect..." to "Modern Landscape
Painting - Zion by Johnathan Harris"
Fig.6: Qualitative results from LAION-Aesthetics. For each pair of prompts,
the first row is the Text Embedding Interpolation, the second row is AID-O and the
third row is AID-I. Our methods provide direct and smooth interpolation in spatial
layout and style, with high fidelity.
BandthenumberofdenoisingstepsN,foraninterpolationcoefficienttweguide
the generation with prompt A for the first ⌊tN⌋ steps and guide with prompt B
for the rest of steps. Within our frameworks, we compare our methods based on
inner attention interpolation notated as "AID-I" and outer attention interpola-
tion notated as "AID-O".
5.2 Qualitative results
For qualitative analysis, we observe that AID-I prefers interpolation on the con-
cept or style. On the other hand, AID-O strongly enhances perceptual consis-
tency and encourages interpolations in the spatial layout of images, as Fig. 5
shows. Even when interpolating between two very long prompts, our methods
achieve direct and smooth interpolations with high fidelity as Fig. 6 shows. We
show more comprehensive qualitative results in Sec. D of the Appendix.
5.3 Quantitative results
We quantitatively evaluate our methods based on the evaluation protocol in
Sec.5.1asshowninTab1.WefoundthatAID-Oincreasedtheperformanceover
alltheevaluationmetricssignificantly.WhileAID-Iachievesthehighestsmooth-
ness, AID-O has significant improvements in consistency (-20.3% on CIFAR-10AID: Attention Interpolation of Text-to-Image Diffusion 13
Dataset Method Smoothness(↑)Consistency(↓)Fidelity(↓)
TextEmbeddingInterpolation 0.7531 0.3645 118.05
CIFAR-10 DenoisingInterpolation 0.7564 0.4295 87.13
AID-O 0.7831 0.2905* 51.43*
AID-I 0.7861* 0.3271 101.13
TextEmbeddingInterpolation 0.7424 0.3867 142.38
LAION-AestheticsDenoisingInterpolation 0.7511 0.4365 101.31
AID-O 0.7643 0.2944* 82.01*
AID-I 0.8152* 0.3787 129.41
Table 1: Performance on CIFAR-10 and LAION-Aesthetics, where the best perfor-
mance is marked as (*) and the worst is marked as red. AID-O and AID-I both show
significant improvement over the Text Embedding Interpolation. Though Denoising
Interpolationachievesrelativelyhighfidelity,buttrade-offwithverybadperformance
on consistency (0.4295). AID-O boosts the performance in terms of consistency and
fidelity while AID-I boosts the performance of smoothness.
and -23.9% on LAION-Aesthetics) and fidelity (-66.62 on CIFAR-10 and- 60.37
on LAION-Aesthetics). We observe that the fidelity of AID-I is not as good as
AID-O and even worse than Denoising Interpolation. However, AID-I has bet-
ter qualitative results when combined with prompt guidance and we show more
details in Sec. D of the Appendix. We recommend using AID-O when there is
no prompt guidance and AID-I when there is prompt guidance.
We also observe that results from LAION-Aesthetics are commonly worse
than the CIFAR-10, as long prompt interpolation is more challenging. Nonethe-
less, our methods still have a large improvement compared to the Text Embed-
ding Interpolation.
5.4 Ablation study
WeconductedanablationstudyontheAID-OframeworkappliedtotheCIFAR-
10dataset,focusingonthreeprimarydesignelementsofAID:attentioninterpo-
lation, self-attention, and selection with a beta prior, as summarized in Tab. 2.
It is important to note that fusion with self-attention cannot function indepen-
dently; hence, it is always combined with attention interpolation.
From Tab. 2, we justify that beta prior contributes to the increment of
smoothness. The marginal contributions to smoothness from the Beta prior are
0.0464 (Text Embedding Interpolation), 0.0671 (with attention interpolation),
and 0.1595 (with fused attention interpolation), respectively. Furthermore, at-
tention interpolation improves consistency, reducing the metric from 0.3645 to
0.3201 compared with the Text Embedding Interpolation. In the context of fu-
sionwithself-attention,weseeanenhancementinfidelity,withmetricsboosting
from 101.89 to 52.51 and from 155.01 to 51.43, respectively.
It is evident that while attention interpolation (without fusion with self-
attention) with the beta prior achieves the highest smoothness, it does so at
the cost of fidelity, where the generated images tend to be of lower fidelity,
as quantitatively demonstrated in Tab. 2 and qualitatively shown in Fig. 7.14 Q. He et al.
Fig.7: Qualitative comparison between AID without fusion (1st row), AID
with fusion (2nd row) and AID with fusion and beta prior (3rd row). Fus-
ing interpolation with self-attention alleviates the artifacts of the interpolated image
significantly, while beta prior increases smoothness based on AID with fusion.
InterpolatedattentionSelf-fusionBetapriorSmoothness(↑)Consistency(↓)Fidelity(↓)
(cid:37) (cid:37) (cid:37) 0.7531 0.3645 118.05
(cid:37) (cid:37) (cid:33) 0.7995 0.3803 117.30
(cid:33) (cid:37) (cid:37) 0.7846 0.3201 101.89
(cid:33) (cid:37) (cid:33) 0.8517* 0.3452 155.01
(cid:33) (cid:33) (cid:37) 0.6236 0.2411* 52.51
(cid:33) (cid:33) (cid:33) 0.7831 0.2905 51.43*
Table 2: Detailing ablation studies on AID-O’s components, showcase that the Beta
prior enhances smoothness, attention interpolation heightens consistency, and self-
attentionfusionsignificantlyelevatesfidelity.Foreachmetric,thebestperformanceis
bold and marked as (*) while the worst performance is marked as red.
Similarly,thoughAIDwithoutbetapriorachievesthebestresultofconsistency,
but trade-off with too much smoothness (0.6236) as shown in Fig. 7.
6 Conclusion
Inthisstudy,weintroduceanoveltask:conditionalinterpolationwithinadiffu-
sionmodel,alongwithitsevaluationmetrics,whichincludeconsistency,smooth-
ness, and fidelity. We present a novel approach, referred to as AID, designed to
produce interpolations between images under varying conditions. This method
significantly surpasses the baseline in performance without training, as demon-
strated through both qualitative and quantitative analysis. Additionally, we in-
troduce PAID, an extension that allows users to employ guidance prompts to
choose the interpolation paths. Our method is training-free and broaden the
scope of generative model interpolation, paving the way for new opportunities
in various applications, such as compositional generation, image editing, data
augmentation, and video interpolation.AID: Attention Interpolation of Text-to-Image Diffusion 15
References
1. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 18392–18402 (2023)
2. Chen,W.,Hu,H.,Li,Y.,Ruiz,N.,Jia,X.,Chang,M.W.,Cohen,W.W.:Subject-
driven text-to-image generation via apprenticeship learning. Advances in Neural
Information Processing Systems 36 (2024)
3. Dorfman,R.:Aformulafortheginicoefficient.Thereviewofeconomicsandstatis-
tics pp. 146–149 (1979)
4. Esser,P.,Rombach,R.,Ommer,B.:Tamingtransformersforhigh-resolutionimage
synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 12873–12883 (2021)
5. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial networks (2014)
6. Gu, J., Dong, C.: Interpreting super-resolution networks with local attribution
maps. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 9199–9208 (2021)
7. He,Q.,Yang,L.,Gu,K.,Lin,Q.,Yao,A.:Analyzinganddiagnosingposeestima-
tionwithattributions.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 4821–4830 (2023)
8. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or,
D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint
arXiv:2208.01626 (2022)
9. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017)
10. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020)
11. Hong, S., Lee, G., Jang, W., Kim, S.: Improving sample quality of diffusion mod-
els using self-attention guidance. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 7462–7471 (2023)
12. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 4401–4410 (2019)
13. Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z.,
Navasardyan,S.,Shi,H.:Text2video-zero:Text-to-imagediffusionmodelsarezero-
shot video generators. arXiv preprint arXiv:2303.13439 (2023)
14. Khodadadeh, S., Zehtabian, S., Vahidian, S., Wang, W., Lin, B., Bölöni, L.: Un-
supervised meta-learning through latent-space interpolation in generative models.
arXiv preprint arXiv:2006.10236 (2020)
15. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
16. Kondo, T., Takezaki, S., Haraguchi, D., Uchida, S.: Font style interpolation with
diffusion models. arXiv preprint arXiv:2402.14311 (2024)
17. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images (2009)
18. Luo, W., Hu, T., Zhang, S., Sun, J., Li, Z., Zhang, Z.: Diff-instruct: A universal
approach for transferring knowledge from pre-trained diffusion models. Advances
in Neural Information Processing Systems 36 (2024)16 Q. He et al.
19. OpenAI: Gpt-4. https://openai.com/gpt-4 (2023)
20. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 4195–4205
(2023)
21. Piegl,L.,Tiller,W.:TheNURBSbook.SpringerScience&BusinessMedia(2012)
22. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Müller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv preprint arXiv:2307.01952 (2023)
23. Qi,C.,Cun,X.,Zhang,Y.,Lei,C.,Wang,X.,Shan,Y.,Chen,Q.:Fatezero:Fusing
attentions forzero-shottext-basedvideoediting.arXiv preprintarXiv:2303.09535
(2023)
24. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
1(2), 3 (2022)
25. Rassin,R.,Hirsch,E.,Glickman,D.,Ravfogel,S.,Goldberg,Y.,Chechik,G.:Lin-
guistic binding in diffusion models: Enhancing attribute correspondence through
attention map alignment. Advances in Neural Information Processing Systems 36
(2024)
26. Renka, R.J.: Interpolation of data on the surface of a sphere. ACM Transactions
on Mathematical Software (TOMS) 10(4), 417–436 (1984)
27. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022)
28. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: Medical Image Computing and Computer-Assisted
Intervention–MICCAI2015:18thInternationalConference,Munich,Germany,Oc-
tober 5-9, 2015, Proceedings, Part III 18. pp. 234–241. Springer (2015)
29. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022)
30. Samuel,D.,Ben-Ari,R.,Darshan,N.,Maron,H.,Chechik,G.:Norm-guidedlatent
space exploration for text-to-image generation. Advances in Neural Information
Processing Systems 36 (2024)
31. Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.,Wightman,R.,Cherti,M.,
Coombes,T.,Katta,A.,Mullis,C.,Wortsman,M.,etal.:Laion-5b:Anopenlarge-
scale dataset for training next generation image-text models. Advances in Neural
Information Processing Systems 35, 25278–25294 (2022)
32. Shen,Y.,Yang,C.,Tang,X.,Zhou,B.:Interfacegan:Interpretingthedisentangled
face representation learned by gans. IEEE transactions on pattern analysis and
machine intelligence 44(4), 2004–2018 (2020)
33. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
34. Snoek,J.,Larochelle,H.,Adams,R.P.:Practicalbayesianoptimizationofmachine
learningalgorithms.Advancesinneuralinformationprocessingsystems25(2012)
35. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020)
36. Struski,Ł.,Tabor,J.,Podolak,I.,Nowak,A.,Maziarz,K.:Realismindex:Interpo-
lation in generative models with arbitrary prior. arXiv preprint arXiv:1904.03445
(2019)AID: Attention Interpolation of Text-to-Image Diffusion 17
37. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 2818–2826 (2016)
38. Tangirala, S.: Evaluating the impact of gini index and information gain on classi-
ficationusingdecisiontreeclassifieralgorithm.InternationalJournalofAdvanced
Computer Science and Applications 11(2), 612–619 (2020)
39. Tran,Q.N.,Yang,S.H.:Efficientvideoframeinterpolationusinggenerativeadver-
sarial networks. Applied Sciences 10(18), 6245 (2020)
40. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning.
Advances in neural information processing systems 30 (2017)
41. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)
42. Wang,C.,Golland,P.:Interpolatingbetweenimageswithdiffusionmodels(2023)
43. Wang, R., Chen, Z., Chen, C., Ma, J., Lu, H., Lin, X.: Compositional text-to-
image synthesis with attention map control of diffusion models. arXiv preprint
arXiv:2305.13921 (2023)
44. Yang,C.,Liang,L.,Su,Z.:Real-worlddenoisingviadiffusionmodel.arXivpreprint
arXiv:2305.04457 (2023)
45. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 586–595 (2018)
46. Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector
framework for fast sampling of diffusion models. Advances in Neural Information
Processing Systems 36 (2024)18 Q. He et al.
A Outer v.s Inner attention interpolation
We start by comparing the formula of outer interpolated attention and inner
interpolated attention. We expand the inner interpolated attention defined in
Eq.9 as follows:
Intp-Attn (Q ,K ,V ;t )
I i 1:m 1:m i
= Attn(Q , (1−t )K +t K ,(1−t )V +t V )
i i 1 i m i 1 i m
Q [(1−t )K +t K ]T
= softmax( i i√ 1 i m )[(1−t )V +t V )]
i 1 i m
d
k (15)
Q [(1−t )K +t K ]T
= (1−t )·softmax( i i√ 1 i m )V
i 1
d
k
Q [(1−t )K +t K ]T
+t ·softmax( i i√ 1 i m )V
i m
d
k
Similarly, we expand the outer interpolated attention defined in Eq.10:
Intp-Attn (Q ,K ,V ;t )
O i 1:m 1:m i
= (1−t )·Attn(Q ,K ,V )+t ·Attn(Q ,K ,V )
i i 1 1 i i m m (16)
Q KT Q KT
= (1−t )·softmax( √i 1 )V +t ·softmax( √i m)V
i 1 i m
d d
k k
Comparing Eq.15 and Eq.16 above, the essential difference is: while inner at-
tention interpolation uses the same attention map softmax(Qi[(1−ti) √K1+tiKm]T )
dk
fusingsourcekeysK andK fordifferentsourcevalueV andV ,outeratten-
1 m 1 m
tioninterpolation,ontheotherhand,usingdifferentattentionmapsfordifferent
source key and value. This may answer why the AID-I tends to conceptual in-
terpolation fusing the characteristics of two concepts into one target but AID-O
tends to spatial layout interpolation allowing the simultaneous existence of two
concepts in the interpolated image.
B Beta prior selection
Our methods apply a Beta prior on the interpolation space to select a specific
interpolation sequence, which requires us to determine the hyperparameter α
and β for each path. As we illustrated in Sec. 4.3, one of the key motivations is
that: text embedding is incorporated in each denoising step and it will amplify
the influence of source images with a larger interpolation coefficient t.
This further motivates us to correlate the selection of α and β with the
numberofdenoisingstepsT.Weobservethatα=T andβ =T canusuallylead
toarelativelysmoothinterpolationsequencealready.Therefore,duringapplying
Bayesian Optimization on α and β, we select the initializing searching points
including a nearby area around α = T, β = T, typically a combination from
[1,0.8T,T,1.2T], which empirically makes the selection of α and beta effective
within very few iterations.AID: Attention Interpolation of Text-to-Image Diffusion 19
C Effect of warm-up steps
As we mentioned in Sec. 4.4, when we apply prompt guidance to generate an
interpolation sequence, we can trade-off between the visual consistency and the
faithfulnesstothetextdescriptionbycontrollingthewarmupsteps.Thisdesign
is based on the observation that early denoising steps of the generative model
can determine the image content to a large extent as Fig. 8 shows. With only
5 initial steps (over a total of 25 denoising steps) using "dog" as guidance (the
6th image in Fig. 8, the image content is already fixed as "dog", which means
the influence of later denoising steps using "car" has very low influence to the
image content generation.
Therefore,wecanutilizethischaracteristicofthediffusionmodeltoconstrain
spatial layout with AID in the early stage of denoising and then transit to self-
generation with the guided prompt to refine the details.
Fig.8:Effectofearlydenoisingsteps.Theimagesaregeneratedusing25denoising
steps. For the ith image shown in the row from left to right, it is generated by using
"A photo of dog, best quality, extremely detailed" in the first i−1 denoising steps,
then generated by using "A photo of car, best quality, extremely detailed" for the rest
denoising steps.
D Auxiliary Qualitative results
We show more qualitative results here using prompt guidance with inner atten-
tion interpolation. In this section, the results are obtained with Stable Diffusion
1.5 [27] and UniPCMultistepScheduler [46]. To enhance the visual ability, we
use the negative prompt "monochrome, lowres, bad anatomy, worst quality, low
quality".Totrade-offbetweenperceptualconsistencyandeffectivenessofprompt
guidance, we use the first 10denoisingsteps over 50total denoising steps of Uni
for warming up. As Fig. 9 and Fig. 10 show, our methods can generate image
interpolation on different concepts and paintings. We provide more examples in
Fig. 11 and Fig. 12.20 Q. He et al.
Fig.9: Qualitative results of interpolation between animal concepts. For an
animal,weuse"Aphotoof{animal_name},highquality,extremelydetailed"togen-
eratethecorrespondingsourceimages.Theguidancepromptisformulatedas"Aphoto
ofananimalcalled{animal_name_A}-{animal_name_B},highquality,extremely
detailed". PAID enables a strong ability to create compositional objects.
Fig.10: Qualitative results of interpolation between different paintings.For
a painting, we use "A painting of painting_name, high quality, extremely detailed"
to generate the source images. The guided prompt is generated by GPT-4 [19] given
descriptionofsourceimages,e.g.,theguidedpromptforthesecondrowis"Apainting
of Mona Lisas under starry night, high quality, extremely detailed".AID: Attention Interpolation of Text-to-Image Diffusion 21
Fig.11: More qualitative results.22 Q. He et al.
Fig.12: More qualitative results.