Asymptotic Bayes risk of semi-supervised learning
with uncertain labeling
Victor Le´ger Romain Couillet
Laboratoire d’Informatique de Grenoble Laboratoire d’Informatique de Grenoble
Universite´ Grenoble alpes Universite´ Grenoble alpes
Grenoble, France Grenoble, France
victor.leger@univ-grenoble-alpes.fr romain.couillet@univ-grenoble-alpes.fr
Abstract—This article considers a semi-supervised classifica- performanceshavebeenproventobeclosetotheoptimal
tion setting on a Gaussian mixture model, where the data is not bound.
labeled strictly as usual, but instead with uncertain labels. Our
For simplicity reasons, the model presented in this article
mainaimistocomputetheBayesriskforthismodel.Wecompare
thebehavioroftheBayesriskandthebestknownalgorithmfor is a single-task model, but it is worth to note that most of the
this model. This comparison eventually gives new insights over conclusionsremaintrueinamulti-tasksetting,astheprevious
the algorithm. works it is based on are multi-task models [7], [9].
Index Terms—classification, random matrix theory, semi- Theremainderofthearticleisorganizedasfollows.Section
supervised learning, statistical physics
II introduces the model, the assumptions and the aim of the
nextsections.SectionIIIstatesourmaintheorem,andgivesin-
I. INTRODUCTION terpretationof thistheorem.SectionIV givesa succinctproof
of the main theorem. Finally, Section V displays simulations
Semi-supervised learning (SSL) is an extension of the
of both the optimal boundand the algorithm presented in [7].
conventionalsupervisedlearningparadigmby augmentingthe
(labeled) training data set with unlabeled data, which then II. MODELAND MAIN OBJECTIVE
“unsupervisably” serve to boost learning performance. SSL
We consider a semi-supervised binary classification task
has long been considered to be a powerful tool to make use
with training samples X = [X ,X ] Rp n which consists
of large amounts of unlabeled data [1]. of a set of n labeled data saℓ mplu es∈ X × = x nℓ and a
However, some work also point out the lack of theoretical ℓ ℓ { i }i=1
set of n unlabeled data points X = x n . Each
understanding of these methods [2]–[4]. Even by considering u u { i }i=nℓ+1
labeleddatapointx hasanassociatedcouple(d ,d )ofpre-
amereGaussianmixturesmodel(whichisoneofthesimplest i i1 i2
estimated probabilities that the vector belongs to one class or
possible parametric model one could consider for a clas-
theother,suchthatd +d =1.Thegoaloftheclassification
sification problem), well-known methods such as Laplacian i1 i2
task is to predict the genuine class of unlabeled data X . In
regularizationappearstobeuneffectivetolearnfromunlabeled u
this context, we are interested in computing the Bayes risk
data [5].
of the classification task, i.e., the minimal classification error
Fortunately, advances in Random Matrix Theory (RMT)
achievable for each unlabeled sample x with the available
i
has been exploited to design better methods, by proposing
data :
fundamental corrections of known algorithms [6], and even infP(yˆ =y ) (1)
i i
extend them, for instance by considering uncertain labeling yˆi 6
[7]. where yˆ = E[y X] is the label prediction made for the
i i
Simultaneously, another field of research has focused on |
sample x .
i
analysing Gaussian mixtures model with statistical physics.
Such analysis brings an optimal bound for a given problem, Assumption 1 (On the data distribution) The columns of
meaning that any possible algorithm cannot reach a better the data matrix X are independent Gaussian random vari-
performance[8],[9].Theseoptimalboundsareaprecioustool ables. Specifically, the data samples (x 1,...,x n) are i.i.d.
to understand whether an algorithm has poor performances observations such that x i
∈
Cj
⇔
x i
∼
N(µ j,I p) where
because of its design or because of the inherent difficulty of Cjt denotes the Class j. We assume that the number of data
the problem it tries to solve. in each class is the same. We further define the quantity
Therefore, the objectives of this article are twofolds : λ = 41 kµ 1 −µ 2k2, which is called the signal to noise ratio
(SNR).
Compute the Bayes risk in the case of uncertain data
•
labeling, inspired by the work of [9]. We study our model in a large dimensional setting, where
Use the knowledge of this optimal bound to further the dimension and the amountof data have the same order of
•
understand the behavior of the algorithm of [7], which magnitude, which is practically the case with modern data.
4202
raM
62
]LM.tats[
1v76771.3042:viXraAssumption 2 (Growth Rate) As n :
→∞
p/n c>0 10 2
• → −
n /n η ·
ℓ
• →
With our notations, in a single-task setting, and assuming
that the probability couples of labeled data are either (0,1) 5
or (1,0) (i.e., the data is labeled with complete certainty), it
has been proved in [9] that under the previous assumptions,
as p , the Bayes risk converges to 0 0
→∞
0
0.5
(√q ), (2) 2
u
Q 4
where Q(x) = √1
2π
x∞e −u 22 du, and the couple (q u,q v) q 1 |ε |
satisfies the followingRequations:
λcq
q =λ v (3) 0 2 4 6
u
1+λcq
v
10 2
q v =η+(1 η)F(q u), (4) · −
−
with F(q)=E tanh(√qZ+q) , Z (0,1).
Our goal in(cid:2)the remainder o(cid:3)f
thi∼
s
aN
rticle is to derive an
Fig.1. Relative errorofthe approximation F˜ ε(q)≃Fε(q).Theerroris at
most7%,andshrinksforeither ε=0,ε=1orlargeq
equivalent result in the more general case where data is not
labeled with certainty. Let us define, for each datapoint x ,
i
ε i =d i2 −d i1 ∈[ −1,1].Thisquantityisenoughtocharaterize with F ε(q)=E ψ ε(q+√qZ) , Z ∼N(0,1) and
the couple of probabilities, as d i1+d i2 =1. We observe that (cid:2) (cid:3)
ε =1meansthatthedataislabeledwithcertaintyinaclass, tanh(t)+ε2 1 tanh(t) tanh2(t)
| wh|
ile ε=0 means that the data is unlabeled.
ψ ε(t)=
1(cid:0)
− ε2tanh2(t)− (cid:1).
−
To get equation (4), it is needed to compute the quantity
A sketch of the proof of Theorem 1 is given in Section IV.
yˆ =E[y X], whichis the estimationof y with the available
di ata X. Ii n| the proof (presented in Sectioni IV), a trick allows The functionF ε is similar to the previousfunctionF, but the
expression of ψ is not easy to understand as it is.
to compute this quantity as a function of q , and the labeling ε
u
information is expressed through the prior distribution of y. Remark 1 The functionψ canbeputinthefollowing(more
ε
For data labeled with certainty, it is know that y = 1 convenient) form :
i
• −
or +1, so the prior is either δ(t 1) or δ(t+1).
− ψ (t)=tanh(t)+ε2(1 tanh(t))
For unlabeled data, the prior is uniform over 1,+1 . ε −
• Or equivalently, the distribution function is {− } (1 ε2)(1 tanh(t)) ε2ktanh2k(t).
− − −
1 1 kX1
δ(t 1)+ δ(t+1). ≥
2 − 2 Remark 2 The function F can be approximated by :
ε
When the data is not labeled with certainty but with a
• probability couple (d i1,d i2), the prior distribution of y
i
F˜ ε(q)=E ψ˜ ε(q+√qZ) , (9)
h i
becomes
with Z (0,1) and
d δ(t 1)+d δ(t+1) (5)
i1 i2 ∼N
−
The following section states our main theorem using this last ψ˜ ε(t)=tanh(t)+ε2(1 tanh(t)). (10)
−
prior distribution.
Figure 1 gives an idea of the quality of the approximation
III. MAIN RESULTS made in Remark 2. However, it is worth to note that its
Theorem 1 Under the previous assumptions, as p , purpose is not to replace the original formula from Theorem
→∞ 1, as that formula is already tractable and can be computed
The Bayes risk converges to
• easily. Instead, Remark 2 intend to bring a simpler formula
(√q u), (6) that conveys an understanding of the key role of quantity ε2.
Q
where Q(x)= √1
2π
x∞e −u 22 du. thaA ts aq su ma an ld
l
eq rv roa rre inre ela qt ue ad tit oo nea (8c )h co oth ue ldr, lo en ae dc to oul ad ca ols mo pw leo terr lyy
•
The overlaps q u,q
v
Rsatisfy the following equations
different solution for q u and q v. Fortunately, if one replaces
q
u
=λ λcq v (7) the solution (q u⋆,q v⋆) of the system by another solution (q u⋆+
1+λcq nv ∆q u,q v⋆+∆q v),thenwehave |∆ qq u⋆u |≤|∆ qq v⋆v |.Thismeansthat
1 a small variation of q leads to an even smaller variation of
v
q = lim F (q ) (8)
v n n εi u q u.
→∞ Xi=1
)q(ε˜F
)q(εF
−
|
)q(εFCorollary 1 With the previous approximation of the function
1
F , one can approximate the equation (8) :
ε
q v ε¯2+(1 ε¯2)F(q u) (11) 0.8
≃ −
with
0.6
1
ε¯2 = ε 2
i
n
Xi=1 0.4
F(q)=E[tanh(q+√qZ)]
0.2
Z (0,1)
∼N
Proof: The function F˜ described in Remark 2 can be 0
εi
expressed with function F :
0 0.1 0.2 0.3 0.4 0.5
F˜ εi(q)=E tanh(q+√qZ)+ε i2(1 −tanh(q+√qZ)) Bayes risk
=ε (cid:2)2+E[tanh(q+√qZ)](1 ε 2) (cid:3)
i i
−
=ε 2+(1 ε 2)F(q) Fig. 2. Usefulness of unlabeled data as a function of the Bayes risk of
i − i the task. Interestingly, the only criterion to determinate the effectiveness of
unlabeled data is how solvable the task is. The lower the Bayes risk is, the
By mixing the results of Theorem 1 and Remark 2, one gets
moreunlabeled dataareusefultoperformthetask.
asymptotically
n
1
q F˜ (q ) Then, a key lemma for the proof is the following.
v
≃ n
εi u
Xi=1
Lemma 1 Estimating y from X is asymptotically equivalent
n i
1
= ε 2+(1 ε 2)F(q) to estimating the signal y i from the output of a Gaussian
i i
n Xi=1(cid:2) − (cid:3) channelwith SNR q u. Let us consider the following Gaussian
=ε¯2+ 1 ε¯2 F(q) channel
− U =√λS +Z
(cid:0) (cid:1) i i i
Corollary 1 enables an easy interpretation of Theorem 1.
I en qd ue ae tid o, nth (4e ),va wlu ite ho ηf =q v ε¯2g .iv Oe nn eb cy aneq cu ha et cio kn th( a1 t1 :) is similar to w coit mh pλ uti= ngq tu heth oe veS rN laR p, ES i [yˆt ih ye i]s oig fn ya il isa en qd uZ ivi al∼ enN tto(0 c, o1 m). puT th inen
g
ε2 =0 unlabeled data η =0 the overlap E Sˆ iS i of S i, with Sˆ i =E[S i U i]
• ε2 =1↔ data labeled wi↔ th certainty η =1 h i |
• ↔ ↔ Thus, one has
If all samples are labeled with the same value ε, then it is
equivalent to a task for which one would have a proportion E[yˆy ]=E SˆS (13)
i i i i
ε2 of data labeled with certainty and a proportion 1 ε2 of h i
−
unlabeled data. The signal S i follows the same distribution than y i :
To go further, F(q ) can be understood as a quantity that
u
e dx ap tar .es Is ne ds eeh do ,w ifus Fef (u ql u)un =lab 1e ,le ud nd laa bta elea dre, dr ae talat biv rie nly gsto asla mbe ule cd
h
S i
∼(cid:26)
+− 11 ww ii tt hh pp rr oo bb aa bb ii ll ii tt yy dd ii 21,
information as labeled data. Interestingly,this quantity F(q )
u
only depends on q u, which itself related to the Bayes risk
E[S U ]=
d i1e√λUi −d i2e −√λUi
i i
of the classification task. Thus, usefulness of unlabeled data | d i1e√λUi +d i2e −√λUi
only depends on how well the task can be performed. Figure
Usingε =d d ,onegetsE[S U ]=f (√λU ),with
2 displays the quantity F(q u) as a function of Bayes risk. i i1
−
i2 i
|
i εi i
tanh(t)+ε
IV. SKETCHOF THEPROOF f (t)=
ε
1+εtanh(t)
The proof of Theorem 1 is really similar to the one
performedin[9],as(2)and(3)remainthesame,butthemain
difference lays in the expression of q v, that must be adapted. E[S E[S U ]]=E S f (λS +√λZ )
As in the original proof, q v = yˆ,y is the overlap of the
i i
|
i
h
i εi i i
i
h i
signaly=(y ) ,andwehaveasymptotically,throughthe law =d E f (λ+√λZ ) d E f ( λ+√λZ )
of large
numbi ei
rs :
i1
h
εi i
i−
i2
h
εi
−
i
i
=d E f (λ+√λZ ) d E f ( (λ+√λZ ))
1 n i1 h εi i i− i2 h εi − i i
q = lim E[yˆy ] (12)
v n →∞n
Xi=1
i i =E hψ εi(λ+√λZ i)
i
(14)
where yˆ =E[y X] is the MMSE estimator of y . with ψ (t)=d f (t) d f ( t)
i i
|
i εi i1 εi
−
i2 εi
−
)
q(F
uMore precisely,
1,000
tanh(t)+ε tanh(t)+ε
i i
ψ (t)=d d −
εi i1 1+ε itanh(t) − i2 1 ε itanh(t) 800
−
tanh(t) ε 2tanh(t)
i =(d +d ) −
i1 i2 1 ε 2tanh2(t) 600
i
−
+(d i1 −d i2)ε 1i(1 ε− i2t ta an nh h2 2( (t t) )) 400 η η= =1 1/ /5 20
0
−
tanh(t)+ε i2 1 tanh(t) tanh2(t) 200 η=1/10
= 1 (cid:0) ε− 2tanh2(t)− (cid:1) η=1/5
i
−
0
η=1/2
0.5 0.6 0.7 0.8 0.9 1
Combining (12), (13) and (14), we obtain (8).
Confidence κ in labeled data
V. SIMULATIONSAND APPLICATIONS
Theobjectiveofthissectionistoconfronttheoreticalresults
of Section III and the algorithm described in [7], which will
Fig.3. Numberoflabeleddatanℓneededtoperformthesameperformance,
as a function of the confidence in the data labeling, for different values of
befromnowonreferedtoasoptimalalgorithm.Thecommon η (n = 1000,p = 200,λ = 0.25). The empirical values are displayed in
ideaofthefollowingexperimentsisthattheoptimalalgorithm dots, and theoretical prediction (built on the results of Section III) in plain
line.Theleastreliablethedatais,themoredataisneededtoreachthesame
is expected to behave similarly to the optimal bound studied
performance.
in Section III.
As we have seen in Section III, different labeling settings
can lead to the same value of ε¯2. Let us start with only thereforethefinalclassificationerroroftheoptimalalgorithm.
unlabeled data and data labeled with certainty. Then, In order to understand the contribution of unlabeled data,
one could be interested in computing the reduction of the
ε¯2 =η, (15)
classificationerrorbyusingthesemi-supervisedversionofthe
and we obtain a classification error E. Now, let us assume optimalalgorithminsteadofthefullysupervisedone.Withthe
that all the labeled data is labeled with the same confidence aim in mind, we will consider the two following quantities:
κ < 1. The total number of data n stays unchanged. For The absolute error reduction
•
differentvaluesofκ,ifonewantstoachievethesameerrorE, E E
sup semi-sup
then morelabeleddata will beneededas κ decreases. Indeed, − (18)
E
sup
reaching the same performancemeans obtaining the same q ,
u
andthereforeq , as the task doesnotchangebeyondthat. We which is the tangible error reduction one can expect by
v
recallthatq ε¯2+(1 ε¯2)F(q ), andinourcontext,F(q ) adoptingthe semi-supervisedmethodinstead of the fully
v u u
does not chan≃ ge. Cons− equently, to obtain the same error E, supervised one.
ε¯2 must stay constant. Moreover, we have The error reduction relatively to oracle bayes risk
•
ε¯2 = n ℓ (2κ 1)2 (16) E sup −E semi-sup (19)
n − E E
sup oracle
−
By combining (15) and (16), one gets
which reflects how much of the way to oracle error
η has been done by adopting the semi-supervised method
n = n (17)
ℓ (2κ 1)2 instead of the fully supervised one,
−
For a given value of η, (2κ 1)2 must be no smaller than whereE oracle isthebayesriskonecanexpectwhenthecenters
η, otherwise even a fully labe− led dataset would not allow to of distributions µ 1 and µ 2 are known. More precisely, oracle
reach ε¯2 =η. error is given by the formula
Figure3displaysbothempiricalandtheoreticalvaluesofn ℓ E = (√λ). (20)
oracle
asafunctionofκinthissetting.Thetheoreticalvalueisgiven Q
by (17), and the empirical one is computed by incrementing Leavingouttheparameterη,themainparametersthatdrive
n (anddecrementingn ) untilthe errorgivenbythe optimal the final error are λ and c, as we can see in (7). Therefore,
ℓ u
algorithm gets below E. The match between empirical and Figures 4 and 5 display the two kinds of error reduction
theoretical curves show that Corollary 1 helps to understand presented above, respectively as functions of λ and c.
the behavior of the algorithm. In Figure 4, it is clear that the error reduction is higher
The other takeaway message of Section III is that the when λ grows, for both types of error reduction. Intuitively,
usefulness of unlabeled data, expressed through the quantity a higher SNR means a lower final error, and consequently a
F(q ), only depends on the Bayes risk of the task, and higher contribution of unlabeled data to the classification.
u
n
atad
delebal
fo
rebmuN
ℓourcase,thealgorithmbehavessimilarlytoitsoptimalbound,
givingstronginsightthatthealgorithmisindeednearoptimal.
80%
So if the algorithm gives poor performances,it simply means
that the problem is inherently too hard to solve. Therefore,
60% the interest of computing such optimal bounds is clear. By
knowing in advance how far from optimal an algorithm is,
40% one can avoid spending too much energy to solve a problem
which turns out to be a dead-end.
algo/absolute
Furthermore, the similarity of behavior between the al-
20% algo/oracle
gorithm and the bound allows to understand the algorithm
bound/absolute
from an other perspective. Indeed, results from Sections III
0% bound/oracle
and V providea new understandingon when semi-supervised
0 2 4 6 8 10 learning is truly useful, and when it is not.
λ REFERENCES
[1] O. Chapelle, B. Scho¨lkopf, and A. Zien, Semi-Supervised
Fig.4. Percentageoferrorreductionbyusingthesemi-supervisedalgorithm
Learning. The MIT Press, 09 2006. [Online]. Available:
insteadofthesupervisedone,asafunctionoftheSNRλ(n=p=200,η=
https://doi.org/10.7551/mitpress/9780262033589.001.0001
0.2). The easier the task is, the higher the semi-supervised contribution is,
[2] B. Shahshahani and D. Landgrebe, “The effect of unlabeled samples
becausetheclassification errorislower.
in reducing the small sample size problem and mitigating the hughes
phenomenon,” IEEE Transactions on Geoscience and Remote Sensing,
80% vol.32,no.5,pp.1087–1095, 1994.
[3] F. G. Cozman and I. Cohen, “Risks of semi-supervised learning:
How unlabeled data can degrade performance of generative
classifiers,” in Semi-Supervised Learning, 2006. [Online]. Available:
60% https://api.semanticscholar.org/CorpusID:63547716
algo/absolute [4] S. Ben-David, T. Lu, and D. Pa´l, “Does unlabeled data provably
algo/oracle help? worst-case analysis of the sample complexity of semi-supervised
learning,” inAnnualConference Computational Learning Theory, 2008.
bound/absolute
40% [Online]. Available: https://api.semanticscholar.org/CorpusID:7670149
bound/oracle
[5] X. Mai and R. Couillet, “A random matrix analysis and improvement
ofsemi-supervised learning forlarge dimensional data,” The Journal of
MachineLearningResearch,vol.19,no.1,pp.3074–3100, 2018.
20%
[6] ——, “Consistent semi-supervised graph regularization for high
dimensional data,” Journal of Machine Learning Research,
vol. 22, no. 94, pp. 1–48, 2021. [Online]. Available:
0% http://jmlr.org/papers/v22/19-081.html
[7] V. Leger and R. Couillet, “A large dimensional analysis of multi-task
0 2 4 6 8 10 semi-supervised learning,” 2024.
c [8] M.LelargeandL.Miolane,“Asymptoticbayesriskforgaussianmixture
inasemi-supervised setting,” 2019.
[9] M.-T.NguyenandR.Couillet,“Asymptoticbayesriskofsemi-supervised
Fig.5. Percentageoferrorreductionbyusingthesemi-supervisedalgorithm
multitasklearning ongaussianmixture,” 2023.
instead of the supervised one, as a function of the ratio c = n/p (λ =
2,p = 200,η = 0.2). As c grows, the semi-supervised algorithm is more
and more effective comparatively to the supervised one, relatively to oracle
error,becausetheclassificationerrorislowerandoracleerrorstaysconstant.
However, iftheoracle errorceases tobethereference, thenthecontribution
of semi-supervised decreases for high values of c, because both algorithms
edgeclosertotheoraclebound,whichstaysfarfromzero.
Meanwhile in Figure 5, the two types of error reductiondo
not behave similarly. In this case, the oracle error is constant,
and both errors E and E get close to E as c
sup semi-sup oracle
grows. Therefore, there is not much to gain by adopting the
semi-supervised algorithm, as we are already close to the
oracle bound with the supervised one. However, the error
reductionrelativelyto oraclestillincreaseswhenc grows.We
see that the understanding of what remains to be improved
plays a key role in Figure 5.
VI. CONLUDING REMARKS
Figuring out the link between the performances of an
algorithm and its optimal bound gives precious insights. In
noitcuder
rorrE
noitcuder
rorrEThis figure "fig1.png" is available in "png"(cid:10) format from:
http://arxiv.org/ps/2403.17767v1