Prediction-sharing During Training and Inference
YotamGafni1,RonenGradwohl2,andMosheTennenholtz3
1 WeizmannInstituteyotam.gafni@gmail.com
2 ArielUniversityroneng@ariel.ac.il
3 Technion-IsraelInstituteofTechnologymoshet@ie.technion.ac.il
Abstract. Twofirmsareengagedinacompetitivepredictiontask.Eachfirmhas
twosourcesofdata—labeledhistoricaldataandunlabeledinference-timedata—
andusestheformertoderiveapredictionmodel,andthelattertomakepredictions
onnewinstances.Westudydata-sharingcontractsbetweenthefirms.Thenovelty
ofourstudyistointroduceandhighlightthedifferencesbetweencontractsthat
sharepredictionmodelsonly,contractstoshareinference-timepredictionsonly,
andcontractstoshareboth.
Our analysis proceeds on three levels. First, we develop a general Bayesian
frameworkthatfacilitatesourstudy.Second,wenarrowourfocustotwonatural
settingswithinthisframework:(i)asettinginwhichtheaccuracyofeachfirm’s
predictionmodeliscommonknowledge,butthecorrelationbetweentherespective
modelsisunknown;and(ii)asettinginwhichtwohypothesesexistregardingthe
optimalpredictor,andoneofthefirmshasastructuraladvantageindeducingit.
Withinthesetwosettingswestudyoptimalcontractchoice.Morespecifically,
wefindtheindividuallyrationalandPareto-optimalcontractsforsomenotable
cases,anddescribespecificsettingswhereeachofthedifferentsharingcontracts
emergeasoptimal.Finally,inthethirdlevelofouranalysiswedemonstratethe
applicabilityofourconceptsinasyntheticsimulationusingrealloandata.
Keywords: DataSharing·StrategicMachineLearning·StrategicClassification·
InformationSharing.
1 Introduction
Machinelearning(ML)isbecomingahighlydistributedendeavor.Dataisspreadamong
differentfirms,eachofwhommayhavetheirownMLcapabilitiesandeconomicutilities.
Inmanycases,onefirm’sdataandpredictioncapabilitiesarecomplementedbythose
availabletoacompetingfirm,andeachfirmwouldbenefitfromaccesstotheother’s
predictions.Forexample, twoinvestment banksthatattempt topredictloan defaults
could each improve their respective predictions by accessing the other’s predictions.
Indeed,thisisinthespiritofoneofthemostfundamentalideasinML—aggregating
weaklearnersintostrongones[11].However,thedistributednatureoffirms’capabilities
introducesamajorobstacle:Why,andunderwhatconditions,wouldfirmswillingly
sharetheirpredictionswithcompetitors?Andwhatwouldequilibriumbehaviorlook
like,givensuchsharing?
Ourmaininnovationinthispaperistheobservationthatthisobstacleactuallyconsists
oftwoseparatequestions,correspondingrespectivelytothetrainingandinferencephases
4202
raM
62
]HT.noce[
1v51571.3042:viXra2 Gafni,Gradwohl,andTennenholtz
in ML. First, why would firms share the labels they have (about individuals in their
datalogs)inthetrainingphase?Andsecond,whywouldfirmssharetheirpredictions
(aboutnew,incominginstances)intheinferencephase?Asweshowinthispaper,this
distinctionhasrealbite.
In order to tackle the question of training/inference-stage prediction-sharing, we
proceedonthreelevels.First,wedevelopageneralBayesianmodelthatcapturesthe
two kinds of sharing. The Bayesian model specifies the informational environment,
whileautilitymodelspecifiestheeconomicimplications.IntheBayesianmodel,each
firm obtains a training signal that represents the prediction model (a.k.a. classifier)
learnedbythatfirmviaitslabeledhistoricaldata.Thefirmalsoobtainsaninference-time
signalthatrepresentstheclassifier’spredictiononunlabeledinference-timedata.Inthe
utilitymodelweassociatearealnumberwitheachoutcomequadrant:True-positive,
true-negative,false-positive,andfalse-negativepredictions.Wemoreoverassumethatif
bothfirmsarriveatthesameoutcome,thentheassociatedutilityissplitbetweenthem.
Inthesecondlevelofouranalysis,weapplyourmodeltoagame-theoreticstudy
oftwonaturalsettings.Inthefirstsetting,theaccuracyofeachfirm’spredictionmodel
iscommonknowledge,butthecorrelationbetweentherespectivemodelsisunknown.
Asforutilities,eachfirmhasasafepredictionthatyieldsutilityzero(whetherrightor
wrong),andariskyprediction.Forexample,afirmpredictingacustomer’strustworthi-
nessinordertodecidewhetherornottoissuealoan.Ifaloanisprovided,thefirm’s
utilitydependsontheaccuracyofthetrustworthinessprediction,andwhetherornotthe
customerhasotheroffers.Ifnoloanisprovided,thefirm’sutilityisfixedat0.Inthe
secondsettingwestudy,therearetwohypothesesregardingtheoptimalpredictor,and
oneofthefirmshasastructuraladvantageinderivingit.Furthermore,firms’utilities
aresymmetricacrosspredictiontypes(unlikethefirstsetting),anddependonlyonthe
predictions’correctness—e.g.,afirmrecommendingamovietoaviewer,wherethe
utilitydependsonwhetherornotitaccuratelypredictstheviewer’stastes.
Finally,inthethirdlevelofouranalysis,wedemonstratetheapplicabilityofour
ideas in a synthetic simulation using real loan data. This is intended to provide an
accessible, practical recasting of our abstract model’s results. In broad terms, if we
takeasinglefirm’sperspective,theno-sharingcontractallowsittobuildaclassifier
basedonitsownhistoricaldata.Then,basedonitsassessment(prior)ofthecompetitor,
itdecideswhetherornottoactinaccordancewiththeclassifier’sprediction(signal).
An example of choosing to ignore the classifier’s signal would be if the firm knows
thatitscompetitorcanperfectlypredictwhetheraloanwouldberepaid.Then,allthe
benefitofissuingagoodloanissplit(e.g.,bytherandomdecisionoftheconsumeras
towhichoftheofferedloanstoaccept).However,sincethefirmknowsthatitsown
classifierisimperfect,itknowsitwillalsoendupissuingsomebadloans.Ifthecostof
badloansoutweighsthebenefitofsplittingtheprofitfromgoodloans,thefirmwould
decidetoignoreitsclassifierandnotissueanyloans.Expandingonthisexample,the
train-sharingcontractcanallowthefirmtomakeamorerefineddecision:Basedon
seeinghowtheotherfirmpredictsonthehistoricaldata,itcanassesswhetherornotto
followitsownclassifier.Thefull-sharingcontractallowsevenmoreintricatedecision
rules:Theycandependbothonwhatthefirmlearnsaboutthecompetitor’spredictions
onhistoricaldata,andalsoonthecompetitor’spredictiononeachspecificconsumer.Prediction-sharingDuringTrainingandInference 3
Lastly,theinfer-sharingcontractdoesnotseethecompetitor’spredictionsonhistorical
data,soitmustmaintainitsassessment/priorovertheotherfirm’sclassifier,butitcan
usethecompetitor’spredictiononthereal-timeconsumertodecidewhethertofollow
itsownclassifier’sprediction.InourpracticalimplementationofSection5weexamine
theperformanceoftheoptimaldecisionrulesunderdifferentcontracts,andshowthat
eachofno-sharing,full-sharing,andtrain-sharingisuniquelyoptimalforsomesetof
parameters.
Theemphasisofourworkingametheoretictermsistorequirethatacontractisboth
individuallyrationalandPareto-optimal(IRPO).Thisfollowstheassumptionthatthe
naturalstateofaffairsisthatnocontractissigned(no-sharing).Thus,forthefirmsto
agreeforanykindofprediction-sharing,itmustbethatforeachofthem,theexpected
utilityundertheprediction-sharingcontractisatleastasgoodasunderno-sharing.We
refertothispropertyasthecontractbeingindividuallyrational.Moreover,thecontract
mustbePareto-optimalw.r.t.thefourpossiblecontracts.E.g.,iftheutilitiesunderfull-
sharingdominatetheseundertrain-sharing,eveniftrain-sharingisbyitselfindividually
rational,itwouldmakesensethatthefirmschoosetosignthePareto-optimalcontract
ratherthanaParetodominatedone.Aswesee,therearedifferentsettingssothateachof
thecontracttypesmaybecomeuniquelyIRPO.
Lastly,wenotethatinorderforthefirmstosharetheirpredictions,theyneedaway
tomatchrecords.Facingthisissueiscommonintheindustryandtherearecompanies
thatspecializeinthistask.4 Thistypeofprediction-sharingisvaluable,evenifdone
foridentifiersbothfirmshold,asdifferentfirmsmaybeexposedtodifferentproperties
of the same identifier. As an example, think of firms that know different social and
financialfeaturesassociatedwiththesamesocialsecuritynumber.Inthiscase,there
isadifferencebetweensharingeachfirm’sbinarypredictionregardingtheuser,orthe
entiredataitholdsforthatidentifier.Importantly,ourmodelassumesthatfirmsshare
theirtrainingandinference-timesignals,andnottheirentiredata.Inpractice,inthe
trainingstagethesignalscomeintheformoftruelabelsinthehistoricaldata,andinthe
inferencestageintheformoftheclassifier’spredictions.Thefactthatthisstillprovesto
beusefulisbyitselfinteresting,asitsuggestsapathtodatasharingthatprotectsboth
thefirm’sintellectualproperty(intermsofbothdataandmodelsusedintraining),and
possiblytheusers’privacy.
1.1 OurContribution
In Section 2, we provide the first model to reason about contracts that may involve
sharingpredictionbothinthetrainingandinferencestage.InSections3,4wefocuson
twonaturalsub-modelsofthegeneralmodelwepresent:
1. ACorrelationModel:Bothfirmsknowtheirownandtheircompetitor’sprediction
accuracy,butnotthecorrelationbetweenthetwopredictionmodels.Wecharacterize
theuniquelyindividuallyrationalandPareto-optimalcontractsforsomenotable
cases.Wealsoshowthatallcontractsexceptinference-sharingcanbeoptimalin
thissetting.
4E.g., in advertising, identifying the same user on different devices is called cross-device
targeting,and“attributionproviders”companiessuchasAppsFlyerandSingularenablethis.4 Gafni,Gradwohl,andTennenholtz
2. ATwoHypothesesModel:Onefirmisabletodeterminethecorrecthypothesisduring
training,whiletheotherhasinformationaboutcustomersthatisvaluableduring
inference. Here, we show that inference-sharing can be the unique individually
rationalandPareto-optimalcontract.
Overall,weconcludethateachofthefourtrain/inferencecombinationcontractscanbe
optimal:
• No-sharingistheoptimalindividually-rationalcontractwhenthecostofmakinga
wrongpredictionisequaltotherewardofmakingacorrectprediction.Weshow
thisfirstinLemma1,forthecasewherethepredictionmodelofeachfirm(based
onitsowndata)iscommonknowledge,andthengeneralizeitinTheorem1forthe
generaltraining-phaseprior.Thischaracterizationfollowsfromtwomaininsights:
(1)Underfull-sharing,whenthetwofirmssharetheirinference-timesignals,the
firmswillsimplyfollowtheprimaryfirmsignal.Thisisbecauseanegativeprimary
firmsignalovershadowsapositivesecondaryfirmsignal.(2)Giventhefirstinsight,
theprimaryfirmisonlysettolosebysharingitssignal,sincetheaggregateutilityof
thetwofirmsisconstant(andequalstheaccuracyoftheprimaryfirm’sprediction).
Thesecondaryfirmbecomesmoreinformedunderfull-sharing,andcanextractthe
sameutilityastheprimaryfirm.
• Full-sharingistheoptimalindividually-rationalcontractwhenthetwofirmshave
thesamepredictionaccuracy.Itisatleastasgoodasno-sharingbecausethefirms
canusebothsignalsto“amplify”ormitigatetheirindividualsignal(Theorem2),
in a way that is mutually beneficial w.r.t. their equilibrium behavior under no-
sharing.Full-sharingisalsoatleastasgoodastrain-sharing,becauseinbothcases
the equilibrium behavior is symmetric, but the full-sharing equilibrium is more
informed(Lemma4).Thisisalsotruew.r.t.theinfer-sharingequilibrium,evenmore
generally (Lemma 2), as the symmetry in the infer-sharing case stems not from
havingthesamepredictionaccuracy,butfromthefactthattheinfer-phasesignals
areshared,andthetrain-phasesignalscannotindividuallyteachmoreaboutthe
correlationthanthecommonprior.
• Train-sharingistheoptimalindividually-rationalcontractwhenthetwofirmsbenefit
fromreachingdifferentequilibriagivenadifferentcorrelationbetweentheirsignals.
Inparticular,thefirmsmayprefertoeachfollowitssignalwhenthecorrelationis
low,buthavethesecondaryfirm’yield’totheprimaryfirmwhenthecorrelationis
highandexitthemarket.Learning‘whentoquit’benefitsthesecondaryfirmaswell,
andsocanemergeastheoptimalindividually-rationalcontractwhenfull-sharingis
‘toopermissive’fortheprimaryfirmtofollow,duetolossincompetitiveadvantage.
• Infer-sharingishardertocomeupwithasituationwhereitistheoptimalindividually-
rationalcontract.Infact,weshowthatinourcorrelationmodelitcannotbethe
uniquelyoptimalindividually-rationalcontract(Lemma2).InSection4weexplore
amodelwecall“thetwohypothesesmodel”,whichhasanaturalinterpretationin
healthandscientificcontexts,andshowhowinfer-sharingmayariseastheuniquely
optimalindividually-rationalcontractthere(Theorem4).
Beyondtheexistenceresultsdetailedabove,whichhelpprovideintuitionintothe
differenttypesofpredictionsharingcontracts,thetheoremsofSection3alsoprovidePrediction-sharingDuringTrainingandInference 5
apartialcharacterizationofourcorrelationmodelinseveralimportantcasessuchas
symmetric utilities, or symmetric prediction accuracy. In Section 5, and further in
AppendixD,wedemonstratehowourabstractBayesianmodelmaybeputintopractice
andimplemented,usingarealloandataset.
1.2 RelatedWork
PreviousworkinMLconsidereddifferentaspectsofstrategicprediction.Forexample,
[1] and [9] study competition in prediction, based on shared and independent data,
respectively.Literatureonfederatedlearning[e.g.5,10]considersfree-ridingbydata
providerstosavecostswhilestillbenefitingfrombetterpredictions.[12]and[14]study
data aggregation between competitors: In the former, a firm aims to exploit another
firm’scontributeddatabutalsotomisleadit,puttingtheintegrityofthedata-sharing
protocolatrisk.Inthelatter,segmentationinformationaboutconsumersissplitbetween
firms,andfirmsdecidewhetherornottosharetheirpartofthedatawithothersduring
the inference phase. The former work focuses solely on training models, while the
secondonlyontheinferencestageofaknownsegmentation.Finally,somepapersdeal
withtheimbalancebetweenfirmswithstrongerandweakermodelsthroughthelensof
fairness,leveragingtoolsfromcooperativegametheory[4,6].Thereisalsoagrowing
economicsliteratureondatamarkets[see,e.g.,thesurveyof2].However,neitherthis
economicsliterature,norworkonstrategicMLconsiderstrategicsharingofprediction
models between competitors. They also do not contrast sharing during training and
duringinference,adistinctionweseeascrucialforMLinthedistributedeconomy.
ThereissomeanalogybetweenourworkandthefundamentalMLideaofaggregating
weaklearnersintostrongones,andspecificallytobaggingandstacking.Inbagging[3],
theMLalgorithmdeliberatelycreatessubsetsofthedataandlearnsmodelsforthem
inparallel;thisissomewhatanalogoustohow,inoursetting,differentfirmsdevelop
theirownmodels.Instacking[19],therearetwostages:first,models(derived,e.g.,from
bagging)producepredictionsoveradata-set,andsecond,ameta-learningalgorithm
learnshowtogenerateafinalauthoritativepredictionfromthemodels’predictions.Ina
sense,ourworkcanbeviewedasstrategicbaggingandstacking.
2 Model
Informationalenvironment Therearetwofirmsengagedinacompetitiveprediction
task.Eachfirmobtainsdataintwophases:trainingandinference.Inthetrainingphase,
examples with binary labels are drawn at random, and each firm learns a respective
prediction model (i.e., classifier). The training phase may consist of one example,
multipleexamples,or“infinitelymany”examples.Intheinferencephasefirmsusetheir
learnedmodelinordertopredictthelabelofanewexample.Firm1’spredictioniseither
AorBandfirm2’spredictioniseitheraorb,wheretheformerindicatesthatthefirm’s
predictionmodelbelievesthelabelis1andthelatterindicatesthelabelis0.Wemodel
thisinteractioninanabstractBayesianframeworkusingtherichsignalspacesof[16]
and[13].Wenextdescribetheformalmodel,andthenhighlightthemainelementsand
theirinterpretations.6 Gafni,Gradwohl,andTennenholtz
Aworldmodelwconsistsofapriordistributionπ over{0,1},aswellastwosignal
w
spaces,oneforeachfirm.Foreverytruelabelt∈{0,1},eachsignalspacepartitions
[0,1] into two sets, representing the probabilities associated with firms’ prediction
models,giventruelabelt.5Forthefirstfirm,thefirstsetisdenotedAt ⊂[0,1],andthe
w
secondisdenotedBt =[0,1]\At .Forthesecondfirm,thetwosetsaredenotedat
w w w
andbt .Givenw,arandomexampleismodeledasalabeltdrawnfrom{0,1}according
w
to π , as well as ζ drawn from [0,1] uniformly at random.6 Firm 1’s signal (i.e., its
w
model’s suggested prediction under w) on this example is then 1 if ζ ∈ At under t,
w
and0otherwise;firm2’ssignalis1ifζ ∈ at undert,and0otherwise.Inwords,ζ
w
choosesa“location”ontheinterval[0,1].ThislocationdecidessomesignalforFirm1
(accordingtothewayitpartitionstheinterval[0,1]),andsimilarlyforFirm2(possibly
with a different partition). Sampling ζ uniformly at random from [0,1] is in a sense
similartosamplingarandomfeaturevectorthatisusedtotrainthefirms’prediction
models/requiresapredictionatinferencetime.
Ingeneral,firmsmaynotknowthetruew.Instead,letW beapossiblyinfiniteset
ofpossibleworldmodels,andsupposethereisacommonlyknownpriorπoverthem.
AnexampleofthisframeworkisillustratedinFigure1.
Giventhisinformationalenvironment,theinteractionproceedsasfollows.Instage0,
NaturechoosesanelementwofW accordingtoπ.Then:
1. In the training stage, each firm i obtains a training signal w about the realized
i
worldmodelw.Eachw isafunctionoffirmi’srespectivesignalspaceunderw.
i
Givensignalw (respectively,w )andtheprioroverW,eachfirmiusesBayesian
1 2
updatingtoderiveposteriorbeliefsπ overworldmodelsW.
i
2. In the inference stage, ζ is drawn from [0,1] uniformly at random, and a label t
is drawn from {0,1} according to π . Firm 1 obtains the inference-time signal
w
X ∈ {A,B}thatsatisfiesζ ∈ Xt ,wherew′ ∼ π ;firm2obtainstheinference-
w′ 1
timesignalx∈{a,b}thatsatisfiesζ ∈xt ,wherew′ ∼π .7
w′ 2
3. Intheactionstage,eachfirmitakesanactiona ∈{0,1}.Utilitiesdependonboth
i
firms’actions,andtruelabelt.
Next,weconsiderdifferentcontractsforpredictionsharing.Underno-sharing,the
interactionproceedsasabove.Undertrain-sharing,thereisanadditionalstagebetween
1and2:
1b. Firmssharetheirrespectivetrainingsignalsw andw .
1 2
Underinfer-sharing,anadditionalstagebetween2and3:
2b. Firmssharerespectiveinference-timesignalsX andx.
Finally,underfull-sharingboth1band2btakeplace.
5Formally,eachsignalspaceisaLebesguemeasurablebi-partitionof[0,1]×{0,1}.
6Theuniformityassumptionhereiswithoutlossofgenerality.
7Noticethattheinference-timesignalisdrawnaccordingtothefirm’sposterior,ratherthan
accordingtosomespecifictruepossibleworld.Thisissinceweareinterestedincalculatingthe
firms’equilibriumbehaviors,whichfollowtheirBayesianperspective.Prediction-sharingDuringTrainingandInference 7
Summaryandinterpretation Wenowsummarizethemodelelementsandtheirinterpre-
tations:
• Theworldmodelwisaninformation-theoreticallyoptimalpairofclassifiersforthe
firms.
• Thetrainingsignalw impliesaposteriorπ overworldmodels,whichweinterpret
i i
astheactualclassifierfirmiisabletotrain.Weinterpretthesignalw asfirmi’s
i
predictionsonitslabeledhistoricaldata.Inpracticalterms,thetrainingsignalcan
beinterpretedasthemodelthatbestfitsthetrainingdata,outofallpossiblemodels.
Thefirmscanthensharethesesignals(i.e.,thefunctionsorcoderepresentingtheir
bestmodelsgiventheirdata),withoutsharingthedataitself.
• Theinference-timesignalistheprediction(X ∈{A,B}forfirm1,x∈{a,b}for
firm2)madebythetrainedclassifieronanunlabeledinference-timeexample.
• Undertrain-sharing,firmssharew andw ,theirpredictionsonlabeledhistorical
1 2
data.
• Underinfer-sharing,firmsshareX andx,theirrespectivepredictionsontheunla-
beledinference-timeexample.
Thisformulationcancaptureawiderangeofscenarios.TheprioroverW impliesa
priorovertherelativeshareπ ofeachlabel,apriorovertheaccuracyofeachfirm’s
w
model,andaprioroverthecorrelationbetweenthepredictionsoffirms’models.The
frameworkisillustratedinFigure1.SeealsoFigure1andFigure2in[13].
Strategies Astrategys offirmiintheactionstageisamappingfromthefirm’ssignals
i
to a distribution over actions a ∈ {0,1}. The firm’s signals depend on the contract:
i
underno-sharing,therespectivesignalsareσns =(w ,X)forfirm1andσns =(w ,x)
1 1 2 2
for firm 2. Under train-sharing, they are σts = (w ,w ,X) and σts = (w ,w ,x).
1 1 2 2 1 2
Underinfer-sharing,theyareσis = (w ,X,x)andσis = (w ,X,x).Andunderfull
1 1 2 2
sharing,bothfirmsobtainsignalsσfs =(w ,w ,X,x).
i 1 2
UtilityModel Asnotedabove,utilityu (p,t,p′)offirmidependson3variables:The
i
firm’sactionp,thetruelabelt,andtheotherfirm’sactionp′.Foragivenexample,action
piscorrectifitmatchestheexample’slabelt.Givenatrainingsignalw ,acontract
i
ct∈{ns,ts,is,fs},andapairofstrategies(s ,s ),theexpectedutilityoffirmiis
1 2
uct(w ,s ,s )=E(cid:104) u (cid:16) s (cid:0) σct(cid:1) ,t,s (cid:0) σct(cid:1)(cid:17)(cid:105) , (1)
i i 1 2 i 1 1 2 2
wheretheexpectationisoverthedrawofwfromW accordingtoπ|w ,thedrawoft
i
accordingtoπ ,thedrawofw underw,thedrawsofinference-timesignalsX andx
w j
underw,andthedistributionsoffirms’randomizationoveractions.
Wemakesomesimplifyingassumptionsaboututilities.First,weassumethat
u =u (2)
1 2
.Second,weassumeu (p,t,p)= 1u (p,t,¬p),i.e.,thatifthetwofirmstakethesame
i 2 i
action,theutility(whetherpositiveornegative)isdividedbetweenthem,inthesense
that:
2 2
(cid:88) (cid:88)1 Eq.2
u (p,t,p)= u (p,t,¬p) = u (p,t,¬p).
i 2 i 1
i=1 i=18 Gafni,Gradwohl,andTennenholtz
Fig.1:Therearetwoworldmodels,representedbythetoptwoandbottomtwopairsof
intervals,respectively.Forbothworldmodels,π =Pr[t=1]=κ.Inthefirstworld,
w
A1 =[0,1]andA0 =[0,λ].Thus,ift=1firm1alwaysobtainssignalA,andift=0
w w
firm1obtainssignalAwithprobabilityλ—i.e.,wheneverζ ∈[0,λ]—andsignalBwith
probability1−λ.Furthermore,a1 =[0,1]anda0 =[λ,λ+µ].Thus,ift=1firm2
w w
alwaysobtainssignala,andift=0obtainssignalawithprobabilityµ—i.e.,whenever
ζ ∈[λ,λ+µ]—andsignalbwithprobability1−µ.Finally,thebottomtwopairsofline
segmentsrepresentthefirms’signalspacesinthesecondworldmodel,whichdiffers
fromthefirstonlyinfirm1’ssignalundert=1,namely,A1 =∅andB1 =[0,1].The
w w
intervalstructureofeachofthefirmsresultsinajointintervalstructure(andaninduced
jointprobabilityoverfirm1signalA/B,firm2signala/b,andthetruerealization0/1),
shownontherhsofthefigure.Intheinfinitedatamodel,whereeachofthefirmslearns
itsownintervalstructurewithcertainty,firm1isabletodeducethecorrectworldmodel
justbyknowingitsownintervalstructure.Ontheotherhand,firm2doesnotlearn(in
aBayesiansense)anythingfromitsownintervalstructure.Thisexamplecapturesour
“TwoHypotheses”modelofSection4.
Toemphasizethenotation,u (p,t,p)istheutilitywhentheotherfirm’spredictionp′
i
isequaltop,andu (p,t,¬p)istheutilitywhentheotherfirm’spredictionp′isdifferent
i
thanp.
Thus, the ex-post utility is determined by four numbers: R = u (0,0,1),R =
0 1 1
u (1,1,0),C = u (0,1,1),C = u (1,0,0), where for example R is the reward
1 0 1 1 1 0
from correctly taking action 0 while the other firm takes action 1. We assume that
R ,R ≥0andC ,C ≤0.
0 1 0 1
Inthepaper,welargelyfocusontwospecificutilitymodelsthatcaptureimportant
settings.InSection3,wefocusonautilitymodelwecallsignificant-actionutilities.In
thismodel,thereisasignificantaction—w.l.o.g.,theaction1.Forexample,thisaction
maybechoosingtoissuealoan.Whentakingtheother,safeaction,bothrewardandcost
satisfyR = C = 0.Ifafirmtakesacorrectsignificantactionexclusively,meaning
0 0
thattheotherfirmtakesthesafeaction,itgetsthefullrewardR .Ontheotherhand,ifa
1
firmtakesanincorrectsignificantactionexclusively,itpaysacostC .WhenC =1,
1 1
wecallthisthesymmetricsignificant-actionutilitymodel.Prediction-sharingDuringTrainingandInference 9
InSection4,wefocusonautilitymodelwecallmatchingrecommendations[asin,
e.g.,15].Inthismodel,therearenocoststoamistake—formally,C =C =0—and
0 1
there is a symmetric reward for any correct action—formally, R = R = 1. E.g.,
0 1
considerafirmthatchoosesbetweentwopossiblerecommendationstoauser,and,ifit
correctlyrecommendswhattheuserislookingfor,theuserwillmakeapurchase.
Equilibrium,individualrationality,andParetooptimality Giventrainingsignalsw and
1
w andacontractct ∈ {ns,ts,is,fs},apairofstrategiess = (s ,s )formaNash
2 i ¬i
equilibriumat(w ,w )ifforeachiandeachstrategys′,
1 2 i
uct(w ,s ,s )≥uct(w ,s′,s ). (3)
i i i ¬i i i i ¬i
Anequivalentandperhapsmoreusefulformulationoftheequilibriumcondition
takestheperspectiveoftheagenttogetherwithherbeliefs[seethediscussioninChapter
9of 17].Wedefinetheutilityfromtakingactionp ∈ {0,1}giventhecollectionof
i
signalsσctandtheotherfirm’sstrategys as
i ¬i
u˜ct(σct,p ,s )=E(cid:104) u (cid:16) p ,t,s (cid:0) σct(cid:1)(cid:17) |σct(cid:105) , (4)
i i i ¬i i i ¬i ¬i i
wheretheexpectationisovertheconditionaldrawofσct andtgivensignalsσct.We
¬i i
then say that s is an equilibrium for firm i if for every belief σct and every possible
i
actionp′ ∈{0,1},
i
u˜ct(σct,s (σct),s )≥u˜ct(σct,p′,s ). (5)
i i i i ¬i i i i ¬i
Next, a contract ct Pareto dominates contract ct′ at (w ,w ) if there exists an
1 2
equilibriumsunderctsuchthat,foreveryequilibriums′underct′,
uct(w ,s)≥uct′ (w ,s′) and uct(w ,s)≥uct′ (w ,s′). (6)
1 1 1 1 2 2 2 2
IfatleastoneoftheinequalitiesisstrictthentheParetodominanceisstrict.Contract
ctParetodominatescontractct′ifitParetodominatesct′atevery(w ,w ),andinthis
1 2
casewewritect⪰ct′.Ifct⪰ct′andct′ ⪰ct,wewritect=ct′,andsaythatthetwo
contractsareequivalent.ContractctstrictlyParetodominatesct′ifct⪰ct′butct′ ̸⪰ct,
andinthiscasewewritect≻ct′.
Contract ct is individually rational (IR) at (w ,w ) either if it is the no-sharing
1 2
contract (which we consider the default contract), or if ct Pareto dominates the no-
sharing contract at (w ,w ). Contract ct is always IR if it is IR at every (w ,w ),
1 2 1 2
namely,ct⪰ns.
Contract ct is Pareto optimal if it is not Pareto dominated by any other contract,
Pareto-optimalIR(IRPO)ifitisbothParetooptimalandalwaysIR,anduniquelyIRPO
ifitistheonlycontractthatisbothParetooptimalandalwaysIR.
Wenotethatalthoughourmodelisgeneral,andcanhandlebothmixedandpure
Bayesianequilibria,ourresultsinSection3onwardsareforpureBayesianequilibria.10 Gafni,Gradwohl,andTennenholtz
3 ContractsforPrediction-SharingwithUnknownCorrelation
Inthissectionwefocusonthefirstoftwospecificsettingswithinourframework.We
assumefirmshavesignificant-actionutilitiesand“infinitedata”.Thelatterassumption
meansthateachfirm’spredictionmodelisinsomesenseanoptimalclassifiergiven
thedatafeaturesitisabletosee.Webelievethatthisisthemostnaturalassumptionto
closelyapproximatemassivedatasets.8Themaincaveatisthatneitherfirmknowsthe
correlationbetweenthefirms’classifiers,evenafterlearningitsownclassifier.Wefurther
assumethatthepredictionaccuracyofeachfirm’sclassifier—formally,Pr [1|X =A]
πw
andPr [1|x=a]—arecommonknowledge.9Finally,forsimplicityweassumethat
πw
Pr[0]=Pr[1]= 1,andthatthefalse-positiveandfalse-negativeratesarethesamefor
2
eachofthefirms:
def def
α = Pr[A|1]=Pr[B|0] and β = Pr[a|1]=Pr[b|0]. (7)
Still, the full joint distribution of the firms’ pair of signals together with the true
realizationsunderwisunknown.AsweseelaterinSection3.2,thisisequivalenttoboth
firmsnotknowinghowthesignalsofthetwofirmsarecorrelatedundertheno-sharing
contract,regardlessof(w ,w ).Firmsalsodonotknowthetruelabeloftheoutcome
1 2
theyaretryingtopredictintheinferencephase.Weassumew.l.o.g.thatα≥β ≥ 1.
2
Atoneextreme,itispossiblethatthefirms’signalsareindependent.Attheother
extreme,itispossiblethattheyarefullycorrelated.InAppendixBweshowhowthis
modelcanbeformulatedusingourgeneralmodelfromSection2.
3.1 Warm-Up:KnownCorrelation
We start our investigation with a simple model in which the correlation between the
firmsisknown.Foranexampleinwhichfirms’signalsareknowntobeconditionally
independent,seeFigure2.
Whentheprecisionaccuracyofbothfirmsiscommonknowledge,asweassume
throughoutthissection,thenthecorrelationbetweenthefirms’predictionsfullydeter-
minesthejointdistributionofthepairofsignalsunderlabelt.Weshowthatformallyin
Claim3.1.BycorrelationwemeanthePearsoncorrelationofthesignals,namely
Pr[X =A∧x=a|t]−αβ
θ = ,
t (cid:112)
α(1−α)β(1−β)
wheretisthetruelabelrealization.NoticethatthetwoBernoullivariablesarethe
twofirms’signalsgiventhetruerealization.Forsimplicity,weassumethatθ =θ ,and
1 0
denotethecorrelationsimplybyθ.
Claim. Inthecorrelationmodel,knowingthecorrelationθdeterminesthejointdistribu-
tionofFirm1’ssignalA/B,Firm2’ssignala/b,andthetruerealization0/1.
8SeealsoouranalysisofafinitedatacaseinSection4.1.
9WeusePr[1]asshorthandforPr[t=1],andmayomittheidentifiersπ ,X,xwhenclear
w
fromcontext.Prediction-sharingDuringTrainingandInference 11
Fig.2:Knowncorrelation:Anexampleofconditionallyindependentsignalswithα=
0.7,β =0.6.Withonepossibleworld,bothfirmsknowthejointdistributionovertrue
realizationsandinference-timesignalswithcertainty.
Proof. Toseethatthecorrelationdeterminesthejointdistributioninoursetting,recall
that for the Bernoulli variables in our settings, the Pearson correlation under label 1
satisfiesθ = Pr√[X=A∧x=a|1]−αβ.Thus,givenθ,α,andβ wehave
α(1−α)β(1−β)
Pr[X =A∧x=a|1]
(cid:112) (cid:16)(cid:112) (cid:112) (cid:17) (8)
= αβ αβ+θ· (1−α)(1−β) .
ThisthendeterminesPr[X = A∧x = b|1] = α−Pr[X = A∧x = a|1],Pr[X =
B∧x=a|1]=β−Pr[X =A∧x=a|1],andPr[X =B∧x=b|1]=1−Pr[X =
A∧x = a|1]−Pr[X = A∧x = b|1]−Pr[X = B ∧x = a|1]. That is, it fully
determinesthejointdistribution.Forexample,whenα=β andθ =0(i.e.,thesignals
areconditionallyindependent),wehavePr[X =A∧x=a|1]=α2,andwhenα=β
andθ =1,wehavePr[X =A∧x=a|1]=α.Finally,asymmetricargumentholds
underlabel0.
When the correlation is known, there is no added value in sharing w , since the
i
worldmodelwisalreadyknowntobothfirms.Therefore,no-sharingisequivalentto
train-sharing,andinfer-sharingisequivalenttofull-sharing.Theonlyquestionis,which
ofthesecontracts,ifany,isIRPO?
Lemma1. Withknowncorrelationandsymmetricsignificant-actionutilities,onlyno-
sharingandtheequivalenttrain-sharingareIRPO.Theuniqueequilibriumunderthese
contractshastworegimes:Ahighβregimewherebothfirmsplaybytheirinference-time
signals,andalowβ regimewhereFirm2“givesin”andalwaystakesaction0,while
Firm1matchesitsactiontoitsinference-timesignal.12 Gafni,Gradwohl,andTennenholtz
Thismatcheswhatwelearnedtoexpectinpractice:Firmsdeveloptheirownclas-
sificationmodels,and,assumingtheyareaccurateenough,predictaccordingtothem.
InSection3.2weshow,however,thatoncethecorrelationisnotknownwithcertainty,
thisconclusionmaychange,andfull-sharingortrain-sharingcontractsmaybeuniquely
IRPO.
Wealsonotethatthethresholdthatseparatesthehighandlowβ regimeisitself
dependentonα.Thehigherαis,thehigherthethresholdforthehighbetaregime,where
Firm2followsitspredictionsignal.I.e.,fixingFirm2’spredictionaccuracyβ,thefirm
ismorelikelytogiveinthehigherFirm1’spredictionaccuracyαis.
Lemma1dealswithsymmetricsignificant-utilities.Intheasymmetriccase,witha
highercostforamistakeinthesignificantactionC ,butnotsohighastoprohibitever
1
takingasignificantactionaltogether,thefirmswouldpreferfull-sharing,whichenables
themtotakethesignificantactiononlywhenbothreceivepositivesignals.Weshowthis
inAppendixA.
3.2 UnknownCorrelation
Sofarwehaveconsideredknowncorrelations.However,amorenaturalmodelisthat
thecorrelationisunknown,andonlysomedistributionoveritisknown.Aswewillsee,
thismodelcangiverisetotrain-sharingasuniquelyIRPO.
Webeginwithsomepreliminarylemmas.First,weshowthatwithinthespecification
ofthissubsection,full-sharingalwaysParetodominatesinfer-sharing.
Lemma2. Foranydistributionπ overcorrelationsandanyR andC ,fs⪰is.
θ 1 1
Proof. Inthecorrelationmodel,theprivatesignalw afirmgetsduringthetrainingphase
i
doesnotimpactitsposteriorregardingthecorrelationθ,whichfollowsthedistribution
Θ.Thusunderinfer-sharing,whereeachfirmionlyseesw ,wecanignoreit,andwe
i
haveσis =σis =Xxforsomepairofinference-timesignalX,x.
1 2
Thus,wecanconcludethattheinfer-sharingequilibriumissymmetricbetweenthe
firms.Thatissinceasweargueabove,theposteriorforbothfirmsafterthetraining
phase stays the same as the common prior. In the inference phase, both firms share
their signals, and so both firms end up with the exact same information. Both firms’
equilibriumstrategyistopredict1ifandonlyif
E [Pr[1|X =x ,x=x ,θ]
θ∼Θ 1 2
−C ·Pr[0|X =x ,x=x ,θ]]≥0.
1 1 2
Under full-sharing, a similar argument shows that for every pair of signals Xx
and correlation θ (which both firms learn during the training phase), the symmetric
equilibrium strategy is to predict 1 if and only if Pr[1|X = x ,x = x ,θ]−C ·
1 2 1
Pr[0|X =x ,x=x ,θ].
1 2
def
Wecanthuswrite,forthesymmetricequilibriumstrategiess = s = s ofthe
1 2
infer-sharingcontract,
uis =E[u (s(X,x),t,s(X,x)]
i i
=E [E[u (s(X,x),t,s(X,x)|θ]]
θ∼Θ i
≤E [E[maxu (s(X,x),t,s(X,x)|θ]]=ufs.
θ∼Θ i i
sPrediction-sharingDuringTrainingandInference 13
Next,weseethattrain-sharingandno-sharingcontractsareequivalentundersuffi-
cientsymmetry.
Lemma3. IfR =C thents=ns.
1 1
Proof. Suppose first that, under train-sharing, the firms follow the same equilibrium
strategiess ,s foranyrealizationθ ∼Θ.Then,itmustbethat,underno-sharing,s ,s
1 2 1 2
isalsoanequilibrium:ThisisimmediatesincetheICconditionsofEquation5under
no-sharingfollowimmediatelyifthemoregranularICconditionsofthesameequation
undertrain-sharingaresatisfied.
Now,weknowbyLemma1thatforanyfixedθ theequilibriumstrategiesunder
train-sharing(whicharethesameastheequilibriumstrategiesforno-sharinggivenwe
knowthatthecorrelationisθ)dependonlyonthevaluesofα,β,andsoareindependent
ofθ.Thus,thesameequilibriumstrategiesareplayedforanyθ.
Lemma4. Ifα=β,thenforanydistributionπ overcorrelationsandanyR andC ,
θ 1 1
fs⪰ts.
Finally,wecanusethelemmasabovetoidentifyIRPOcontracts.Thetwotheorems
belowshowthat,undersufficientsymmetry,onlyfull-sharingorno-sharingaresuch
contracts.
Theorem1. IfR =C thenno-sharingisuniquelyIRPO.
1 1
Theorem2. Ifα = β thenfull-sharingiseitheruniquelyIRPO,orfs = nsarethe
onlyIRPO.
However,outsidethesymmetriesinTheorems1and2,train-sharingcanemergeas
uniquelyoptimal.
Theorem3. Train-sharingisuniquelyIRPOforanopensubsetofparametersπ ,α,β,
θ
R ,C .
1 1
TheintuitionunderlyingtheconstructionintheproofofTheorem3isthefollowing.
Underno-sharing,thefirmsplaythesameequilibriumregardlessoftheirtrain-phase
signalsw andw .Undertrain-sharing,theequilibriummaydependonw andw ,and
1 2 1 2
soinsomecasesmayimprovebothfirms’utilitiesrelativetotheno-sharingequilibrium.
Thishappensparticularlywhenthefirmslearnthattheirsignalsarehighlycorrelated,
whichresultsinFirm2nottakingasignificantaction(e.g.,notissuealoan).Thissaves
Firm2fromattainingnegativeutility,andallowsFirm1tofullyexploittheutilityofits
predictions.
4 A“TwoHypotheses”Model
InSection3weshowedthatallcontractsexceptforinfer-sharingcanbeuniquelyoptimal.
Inthissectionwecompletethepicturebydescribingasettingwhereinfer-sharingis
uniquelyoptimal.Wefocusonthesecondsettingdescribedintheintroduction,which14 Gafni,Gradwohl,andTennenholtz
issummarizedinFigure1ofSection2.Weassumethatfirmshave“infinitedata”and
matching-recommendations utilities: C = C = 0 and R = R = 1. This setting
0 1 0 1
capturesavarietyofnaturalcircumstances,suchasmulti-factorialgeneticdiseaseand
chemicaltesting.Considerageneticdiseasethatonlymanifestsitselfasaresultofsome
environmentalcause.Therearetwofirms:Firm1performsgenetictestingandknows(i)
whatgenescausethedisease(ii)foraspecificperson,whetherthesegenesarepresent.
Firm2,ontheotherhand,hasusers’behavioraldata(e.g.,creditcardhistories)andcan
identifytheenvironmentalcause.However,itdoesnotunderstandtheunderlyinggenes
thatenablethedisease.
Formally, let t = 1 denote the presence of the disease, inference-time signals A
and B denote the presence of two different gene mutations in the population, and
inference-timesignalsaandbdenotethepresenceandabsenceoftheenvironmentcause,
respectively.Therearetwohypotheses:(I)thediseaseiscausedbymutationAandthe
environmentalcause,and(II)thediseaseiscausedbymutationBandtheenvironmental
cause.Thus,HypothesisI(resp.,HypothesisII)isthatfirmsseeinference-timesignals
Aa(resp.,Ba)ifandonlyift=1.Thefollowingarecommonknowledge:
• Hypothesis I is correct with probability π , and Hypothesis II with probability
I
1−π ;
I
• withouttheenvironmentalcause,thediseaseremainsdormant(Pr[0|b]=1);
• theincidencerateofthediseaseinthegeneralpopulationisPr[1]=κ;and
• theincidenceratesofthetwodifferentgenemutationsinthegeneralpopulationare
Pr[A]=κ+(1−κ)·λandPr[B]=1−Pr[A].
Ourmainresultisthat,withinthissetting,thereareinstanceswhereinfer-sharingis
uniquelyoptimal.
Theorem4. Infer-sharingisuniquelyIRPOforanopensubsetofparametersπ ,κ,λ,
I
andµ.
TheintuitionunderlyingtheconstructionintheproofofTheorem4isthefollowing.
Generally,inthetwohypothesesmodel,Firm1hastheabilitytodeducethecorrect
worldmodelduringtraining,evenwithonlyitsownsignal.Inthecasesweidentify,
train/full-sharingmakesitlosethisadvantage,andthuscannotbebeneficialforit.We
areleftwithno/infer-sharingaspossibleindividuallyrationalcontracts.Sincegenerally
inthetwohypothesesmodel,thesignalofFirm1byitselfisnotenoughtodecidethe
userclassificationwithcertainty,infer-sharinghelpsinthatFirm1canbothdetermine
thecorrecthypothesisandhasthepairofsignalsthatdeterminesthetruerealization,and
thusitalwayspredictscorrectly.Inthecasesweidentify,thebehaviorofFirm2remains
thesameunderbothcontracts,becauseofthefactthatitcannotdeducethecorrectworld
modelduringtraining.Hence,infer-sharingallowsFirm1a“freeinformationmeal”,
similar to the example, given for a model that only captures inference stage sharing,
withoutconsiderationofthetrainingstage,in[15].
4.1 BeyondtheInfinite-dataModel
Sofar,wefocusedontheinfinite-datamodel,wherethetrainingsignalallowsthefirm
todeducethemarginaldistributionoveritssignalandthetruerealization.WeconjecturePrediction-sharingDuringTrainingandInference 15
thatwithenoughdata,theresultsaresimilartotheidealizedinfinitecasethatweanalyze.
However,withfewsamples,theresultsmaychangesignificantly.Todemonstratehow
the analysis may lead to different results when there is only little historical data, we
consider the setting of Section 4, but when only one labeled example of past data is
availabletothefirms.Thus,afterthehypothesis(world)isdrawn(HypothesisI w.p.π ,
I
andotherwiseHypothesisII),asampleisdrawnfromthejointdistributionoverthepair
ofsignalsandtruerealizations,andeachfirmseesitsownsignalandthetruerealization.
I.e.,ifthetruehypothesisisHypothesisI,thenFirm1sees(A,0)w.p.α,(A,1)w.p.β,
and(B,1)w.p.1−α−β.ThefirmsthenupdateaBayesianposteriorovertheworld
models.Undertrain-sharingandfull-sharing,whenhistoricalpredictionsareshared,
bothfirmsseetheentiresample,i.e.,thepairofsignalsandthetruerealization.
Intheappendix,weprovethat,inthetwohypothesesmodelwiththeparameters
used for Theorem 4 but with a single labeled example, the statement of Theorem 4
breaksdown,asdosomeofthepropertiesofequilibriaderivedinthetheorem’sproof.
Inparticular:
Theorem5. UndertheparametersofTheorem4butwithonesample,no-sharingand
train-sharing are not necessarily equivalent, no-sharing is IRPO (rather than infer-
sharing),andFirm1haslowerequilibriumexpectedutilitythanFirm2.
5 ImplementationforaRealData-set
To see how our ideas may be put to practice, we use the peer-to-peer loan data of
LendingClub, popularized by recent research such as [7], and publicly available at
Kaggle [18], to conduct a synthetic simulation. We take a random subset of 25% of
thefeaturesandassignittoFirm110.Wetakeanothersubsetof10%ofthefeatures
(possiblyoverlapping)andassignittoFirm2.Vertically,wesplitthedataintotrain,test
andvalidationsets.Weleteachofthefirmstrainaneuralnetoverthetrainingdata(that
includesonlyitsfeatures).Eachneuralnetwastrainedfor20epochsona8-GBRAM
M1MacBookPro,whichtakesabouthalfanhour.Thetrainingsignalconsistsofthe
neuralnet’spredictionsonwhetherloansaregoodorbad.Thefirmsusethetestdata
tolearnthesignalperformance,whichweassumethenbecomescommonknowledge.
Dependingonthecontract,thefirmschoosetheirequilibriumstrategiesbasedonthe
performanceinthetestdata:undertrain-sharingandfull-sharingtheyalsoseetheother
firm’spredictionsonthetestdata(ratherthanonlyknowingtheaggregateperformance
measurements).Thefirmsthenusetheirmodelstogetasignalforeveryexampleinthe
validationdata.Underinfer-sharingandfull-sharingtheyseetheotherfirm’ssignalson
thevalidationset,andmayuseittoaltertheirfinalactions.Theyareevaluatedusing
theiractionsonthevalidationdata,undersignificantactionutilitieswithR = 1and
1
costC .
1
Recall that our model assumes that firms share their training and inference-time
signals,andnottheirentiredata.Wethuscomparetheperformanceoffull-sharing—
sharing of the firms’ signals both on the historical data (Here: the test data) and the
inference stage data (Here: the validation data)—with total-sharing—sharing of the
10IntheAppendix,weincluderobustnesstestswherewevarythechoiceoffeatures.16 Gafni,Gradwohl,andTennenholtz
firms’entiredata,trainingajointneuralnetmodelovertheshareddata,anddividingthe
utilitythatthismodelachievesonthevalidationdata.
Thereareseveralimportantaspectsinwhichthepracticalimplementationdeviates
fromourformalmodel:
• We do not naturally have a common Bayesian prior over the joint distribution of
signalsandtruerealizations.Wemakethesimplifyingassumptionthatthesignals
areindependentwhenrelevant,i.e.,underno-sharingandinfer-sharing.Undertrain-
sharingandfull-sharing,weusethetestdatatolearnthesignals’correlations.
• Simplifyingsymmetryassumptions—e.g.,thatthepredictionaccuracyissymmetric
acrosslabels—donotnaturallyappearintherealdata-set,andsoourdecisionrules
needtoadaptanddonotexactlyfollowtheonesgivensymmetry.
• The calculation of equilibrium strategies over the test data leads to an empirical
equilibriumthathassomeerrorwhencomparedtoatheoreticalequilibriumtakenin
expectation.Moreover,theaverageutilityofdifferentcontractsascalculatedonthe
validationsetmayalsohavesomeerror.
We find that the results generally follow the lines of our discussion in Section 3:
Varying by cost (going from C = 0 to C = 2.5 in 0.05 steps), as summarized in
1 1
Figure 3, we find regimes where either full-sharing, no-sharing, or train-sharing are
uniquely IRPO. While full-sharing is almost always a Pareto optimal contract, there
are significant regimes where it is not IR for firm 1, which results in the no-sharing
and train-sharing regimes. In almost all instances and cost values of the simulation,
infer-sharingisParetodominatedbyfull-sharing,aspredictedbyLemma2.
Thebehaviorofno-sharingandtrain-sharingisofparticularinterest.Withlowvalues
ofC ,bothcontractshavethetwofirmsissuealoanregardlessofthesignal.Then,with
1
highervaluesofC ,thefirmsmovetoanequilibriumwhereeachactsaccordingtoits
1
signal,andlatertoanequilibriumwherefirm1predictsitssignalwhilefirm2doesnot
issueanyloans.Ateachsuchequilibriumshift,thereisadiscontinuityforfirm1’sutility.
Forexample,movingfromeachfirmpredictingitsownsignaltoFirm2notissuing
loans,allowsittogetthefullutilityofitsactioninsteadofhalf.
InFigure4,wecomparetheperformanceoffull-sharingwithtotal-sharingfordiffer-
entcosts.Importantly,boththefull-sharingandtotal-sharingmodelsaretrainedonce
forthesymmetriccostC =1,andarethenadaptedtodifferentcostsbydecisionrules
1
decidedbasedonthetestdata.Somewhatsurprisingly,theyachieveverycomparable
performance,withevenaslightadvantagetofull-sharing.Webelievethatthisisdueto
thericherdualsignalinthefull-sharingcase,whichallowsformoregranulardecision
rules,mitigatingthegenerallybetterpredictionaccuracyoftotal-sharing.
6 Discussion
The analysis of incentives is a crucial aspect of the general effort to encourage data
sharing,asrecognizedbytheEuropeanCommission:“Inspiteoftheeconomicpotential,
data sharing between companies has not taken off at sufficient scale. This is due to
a lack of economic incentives (including the fear of losing a competitive edge)” [8].
Thispaperintroducesanovelelementofdatasharing—thedistinctionbetweensharingPrediction-sharingDuringTrainingandInference 17
Fig.3:NoSharing,TrainSharingandFullSharingcontractsperformanceforbothfirms
anddifferentcosts.Wedonotincludetheinfersharingcontractutilityastheyarevery
similarto(anddominatedby)fullsharing.Wemarkregimeswhereeachcontractisthe
optimal-welfareIRcontract.
Fig.4:FullSharingvs.TotalSharingfordifferentcosts
duringtrainingandinference—anddemonstratesitsimportancetounderstandingfirms’
data-sharingincentives.
Somenaturalquestionsariseasaresultofourwork:18 Gafni,Gradwohl,andTennenholtz
• Wehaveassumedacommonprioroverpriorsforthefirms.Whatifthefirmshave
different beliefs? How robust is the emergence of uniquely optimal contracts to
smalldifferencesintheepistemicmodelsofthefirms?
• Ourworkissetwithintheframeworkofmechanismdesignwithoutmoney,i.e.,we
supposethatfirmssharedatabasedonmutualgain,ratherthanbasedonmonetary
compensation. In some cases it is natural to consider that one of the firms may
compensatetheotheraspartofthedatasharingprocess.Thiscouldbeinteresting
asfutureworkandmaybuildontheframeworkandinsightswedevelop.Bibliography
[1] Ben-Porat,O.,Tennenholtz,M.:Bestresponseregression.In:AdvancesinNeural
InformationProcessingSystems30:AnnualConferenceonNeuralInformation
ProcessingSystems2017,December4-9,2017,LongBeach,CA,USA.pp.1499–
1508(2017)
[2] Bergemann, D., Bonatti,A.: Markets forinformation: An introduction.Annual
ReviewofEconomics11,85–107(2019)
[3] Breiman,L.:Baggingpredictors.Machinelearning24,123–140(1996)
[4] Chaudhury,B.R.,Li,L.,Kang,M.,Li,B.,Mehta,R.:Fairnessinfederatedlearning
viacore-stability.AdvancesinNeuralInformationProcessingSystems35,5738–
5750(2022)
[5] Cong,M.,Yu,H.,Weng,X.,Yiu,S.:Agame-theoreticframeworkforincentive
mechanism design in federated learning. In: Yang, Q., Fan, L., Yu, H. (eds.)
FederatedLearning-PrivacyandIncentive,LectureNotesinComputerScience,
vol.12500,pp.205–222.Springer(2020)
[6] Donahue,K.,Kleinberg,J.:Model-sharinggames:Analyzingfederatedlearning
undervoluntaryparticipation.ProceedingsoftheAAAIConferenceonArtificial
Intelligence 35(6), 5303–5311 (May 2021). https://doi.org/10.1609/aaai.v35i6.
16669,https://ojs.aaai.org/index.php/AAAI/article/view/16669
[7] Emekter, R., Tu, Y., Jirasakuldech, B., Lu, M.: Evaluating credit risk and loan
performanceinonlinepeer-to-peer(p2p)lending.AppliedEconomics47(1),54–
70(2015)
[8] EuropeanCommission:AEuropeanstrategyfordata.https://eur-lex.europa.eu/
legal-content/EN/TXT/?qid=1593073685620&uri=CELEX:52020DC0066(2020),
accessed:2021-05-13
[9] Feng, Y., Gradwohl, R., Hartline, J., Johnsen, A., Nekipelov, D.: Bias-variance
games.In:Proceedingsofthe23rdACMConferenceonEconomicsandComputa-
tion.pp.328–329(2022)
[10] Fraboni, Y., Vidal, R., Lorenzi, M.: Free-rider Attacks on Model Aggregation
in Federated Learning. In: AISTATS 2021 - 24th International Conference on
ArtificialIntelligenceandStatistics(2021)
[11] Freund,Y.,Schapire,R.E.:Adecision-theoreticgeneralizationofon-linelearning
andanapplicationtoboosting.J.Comput.Syst.Sci.55(1),119–139(1997)
[12] Gafni,Y.,Tennenholtz,M.:Long-termdatasharingunderexclusivityattacks.In:
EC’22:The23rdACMConferenceonEconomicsandComputation,Boulder,CO,
USA,July11-15,2022.pp.739–759.ACM(2022)
[13] Gentzkow,M.,Kamenica,E.:Bayesianpersuasionwithmultiplesendersandrich
signalspaces.GamesandEconomicBehavior104,411–429(2017)
[14] Gradwohl, R., Tennenholtz, M.: Pareto-improving data-sharing. In: FAccT ’22:
2022 ACM Conference on Fairness, Accountability, and Transparency, Seoul,
RepublicofKorea,June21-24,2022(2022)
[15] Gradwohl,R.,Tennenholtz,M.:Coopetitionagainstanamazon.JournalofArtificial
IntelligenceResearch76,1077–1116(2023)20 Gafni,Gradwohl,andTennenholtz
[16] Green,J.R.,Stokey,N.L.:Tworepresentationsofinformationstructuresandtheir
comparisons. Decisions in Economics and Finance 45(2), 541–547. Originally
circulatedasIMSSSTechnicalReportNo.271,StanfordUniversity,1978(2022)
[17] Maschler, M., Solan, E., Zamir, S.: Game Theory. Cambridge University Press
(2013).https://doi.org/10.1017/CBO9780511794216
[18] Nathan George: “all lending club loan data”. https://www.kaggle.com/datasets/
wordsforthewise/lending-club(2007),accessed:2023-05-13
[19] Wolpert,D.H.:Stackedgeneralization.Neuralnetworks5(2),241–259(1992)
A KnownCorrelationResultsandProofs
Lemma1. Withknowncorrelationandsymmetricsignificant-actionutilities,onlyno-
sharingandtheequivalenttrain-sharingareIRPO.Theuniqueequilibriumunderthese
contractshastworegimes:Ahighβregimewherebothfirmsplaybytheirinference-time
signals,andalowβ regimewhereFirm2“givesin”andalwaystakesaction0,while
Firm1matchesitsactiontoitsinference-timesignal.
Proof. We first emphasize again that when the prior over priors consists of a single
possible prior, the first stage of learning is redundant, and so there is no difference
between“no-sharing”and“train-sharing”,andsimilarlynodifferencebetween“infer-
sharing”and“full-sharing”.Underno-sharing,astrategysnooffirmi∈{1,2}is,given
i
signalx,whethertopredict0or1.Underfull-sharing,astrategysfull offirmiis,given
i
bothfirms’signalsx ,x ,whethertopredict0or1.Tospecifyastrategy(orpartofa
1 2
strategy),wesometimesusethenotationsignal→prediction,e.g.,undernosharing
A→0meansthattheprimaryfirmpredicts0whenitgetsthesignalA.
Underfull-sharing,thereisauniquesymmetricequilibriumwhereAa→1,Ab→
1,Ba→0,Bb→0.I.e.,bothfirmsfollowtheprimaryfirm’ssignal.Thisholdsbythe
followingargument.Underfull-sharing,wehaveσfs =σfs.Moreover,intheknown
1 2
correlationcase,w canbeignored(asthereisonlyonepossibleworldmodel)andσfs
i 1
isoftheformXxforsomepairofinference-timesignalsX ∈ {A,B}ofFirm1and
x∈{a,b}ofFirm2.Letp =s (Xx)bethepredictionoffirmiifthepairofsignalsis
i i
(cid:40)
1 p ̸=s (Xx)
Xx,andletQXx = i ¬i .
pi,s¬i 1 p =s (Xx)
2 i ¬i
Whenp =1,Equation4takestheform:
i
u˜fs(σfs,1,s )=E[u (1,t,s (Xx))]
i i ¬i i ¬i
=QXx ·E[u (1,t,0)]
1,s¬i i (9)
=R ·Pr[t=1|Xx]+C ·Pr[t=0|Xx]
1 1
=Pr[t=1|Xx]−Pr[t=0|Xx],
wherethefirsttransitionissincethesignalthatfirm¬iseesisfixedtobethesame
onethatfirmisees,andsotheexpectationisonlyoverthetruerealizationt.Thesecond
transitionisbythestructureofourutilitymodel.ThethirdtransitionisbyconditionalPrediction-sharingDuringTrainingandInference 21
expectation. The fourth transition is since in the symmetric significant action utility
modelR =1,C =−1.
1 1
Whenp =0,Equation4takestheform:
i
u˜fs(σfs,0,s )=E[u (0,t,s (Xx))]
i i ¬i i ¬i
=QXx ·E[u (0,t,0)]
0,s¬i i (10)
=R ·Pr[t=1|Xx]+C ·Pr[t=0|Xx]
0 0
=0,
wherethelasttransitionissinceinthesignificantactionutilitymodelR =C =0.
0 0
WeconcludethattosatisfytheequilibriumconditionofEquation5,itsufficestoshow
thatforeverypairofsignalsXx,bothagents’strategieschooses (Xx)=1ifandonly
i
ifPr[t=1|Xx]−Pr[t=0|Xx]≥0.
Moreover, we know by Bayes’ formula and our symmetry assumption (Pr[0] =
Pr[1])that:
1
Pr[1|Xx]−Pr[0|Xx]= 2 (Pr[Xx|1]−Pr[Xx|0]),
Pr[Xx]
andsotheleft-hand-sideexpressionisnon-negativeifftheright-hand-sideexpression
isnon-negative.
def
Letρ = Pr[X =A∧x=a|1].Thenwehave
α≥β≥1
2
Pr[Aa|1]−Pr[Aa|0]=ρ−(1−α−β+ρ) ≥ 0,
α≥β
Pr[Ab|1]−Pr[Ab|0]=(α−ρ)−(β−ρ) ≥ 0,
andsincePr[1|Bb]−Pr[0|Bb]=−(Pr[1|Aa]−Pr[0|Aa]),Pr[1|Ba]−Pr[0|Ba]=
−(Pr[1|Ab]−Pr[0|Ab]),theinequalitiesarereversedforthesesignals.Theutilityfor22 Gafni,Gradwohl,andTennenholtz
theprimaryfirmisthus,followingEq.1,
ufs(s ,s )=E [u (s (Xx),t,s (Xx))]
1 1 2 X,x,t 1 1 2
(cid:88) (cid:88) (cid:0) (cid:1)
= Pr[X =x ∧x=x ]·Pr[1|X =x ∧x=x ]·u (s (x ,x ),1,s (x ,x ))
1 2 1 2 1 1 1 2 2 1 2
x1∈{A,B}x2∈{a,b}
(cid:1)
+Pr[0|X =x ∧x=x ]u (s (x ,x ),0,s (x ,x ))
1 2 1 1 1 2 2 1 2
Bayesformula (cid:88) (cid:88) (cid:0)
= Pr[1]·Pr[X =x ∧x=x |1]·u (s (x ,x ),1,s (x ,x ))
1 2 1 1 1 2 2 1 2
x1∈{A,B}x2∈{a,b}
(cid:1)
+Pr[0]·Pr[X =x ∧x=x |0]·u (s (x ,x ),1,s (x ,x )) ]
1 2 1 1 1 2 2 1 2
1 (cid:88) (cid:0) (cid:1)
= Pr[1]·Pr[X =A∧x=x |1]−Pr[0]·Pr[X =A∧x=x |0]
2 2 2
x2∈{a,b}
1
= (Pr[1]·Pr[X =A|1]−Pr[0]·Pr[X =A|0])
2
1 2α−1
= (α−(1−α))= .
4 4
(11)
Underno-sharing,therearetworegimes.ConsideriftheprimaryfirmplaysA→
1,B →0.Wearguethatthen,thesecondaryfirmalwaysplaysb→0,sinceitsutility
fromb→1is(followingEquation4):
u˜ns(b,1,s )=E[u (1,t,s (X))|x=b]
2 1 2 1
1
= ·Pr[t=1∧X =A|x=b]+Pr[t=1∧X =B|x=b]
2
1
− ·Pr[t=0∧X =A|x=b]−Pr[t=0∧X =B|x=b]
2
(cid:18)
1 1
BayesFormula
= Pr[x=b∧t=1∧X =A]+Pr[x=b∧t=1∧X =B]
Pr[x=b] 2
(cid:19)
1
− ·Pr[x=b∧t=0∧X =A]−Pr[x=b∧t=0∧X =B]
2
(cid:18) (cid:19)
α−ρ 1−α−β+ρ ρ β−ρ 1
=2 + − − = (2−2ρ−α−3β)
4 2 2 4 2
α≥β≥1
≤ 2 0=u˜ns(b,0,s )
2 1
Asforthepredictiongiventhesignala,theutilityofFirm2froma→1isPrediction-sharingDuringTrainingandInference 23
u˜ns(a,1,s )=E[u (1,t,s (X))|x=a]
2 1 2 1
1 (cid:0)1
= Pr[x=a∧t=1∧X =A]+Pr[x=a∧t=1∧X =B]
Pr[x=a] 2
1 (cid:1)
− ·Pr[x=a∧t=0∧X =A]−Pr[x=a∧t=0∧X =B]
2
(cid:18) (cid:19)
ρ β−ρ 1+ρ−α−β α−ρ
=2 + − −
4 2 4 2
1
= (3β−1−α)
2
Thus,thesecondaryfirmbestresponseisa→1ifandonlyif3β−α−1≥1.Itis
straightforwardtoverifythatinbothcasesthestrategiesthenformauniqueequilibrium.
In the case that the secondary firm plays a → 0, the equilibrium utility for the
primaryfirmis(followingEquation1):
uns =E[u (s (X),t,0)]
1 1 1
=Pr[X =A∧t=1]−Pr[X =A∧t=0]
1 2α−1 2α−1
= (α−(1−α))= ≥ Eq =.11 ufs.
2 2 4 1
In the case that the secondary firm plays a → 1, the equilibrium utility for the
primaryfirmis
uns =E[u (s (X),t,s (x))]
1 1 1 2
1
= Pr[X =A∧t=1∧x=a]+Pr[X =A∧t=1∧x=b]
2
1
− Pr[X =A∧t=0∧x=a]−Pr[X =A∧t=0∧x=b]
2
(cid:18) (cid:19)
1 ρ 1−α−β+ρ
= +(α−ρ)− −(β−ρ)
2 2 2
1 2α−1 1 2α−1
= (3α−β−1)= + (α−β)≥ Eq =.11 ufs.
4 4 4 4 1
Lemma5. Foranyβ thereissuchαandautilitymodel(asdefinedbyC ,C ,R ,R )
0 1 0 1
sothatfull-sharingistheuniquefeasiblecontractwithknownindependentcorrelation.
Proof. We give the idea of the construction. Consider asymmetric significant-action
utilities,andfix(normalize)R =1.
1
Underfull-sharing,asweknowbytheproofofLemma1,theequilibriumstrategies
s ,s both have s (Xx) for a pair of signals Xx if and only if Pr[Xx|1] − C ·
1 2 i 1
Pr[Xx|0]≥0.TheexpressionismonotoneinthepairofsignalsXx:Itishighestfor
Aa,lowerforAb,evenlowerforBa,andlowestforBb.Thus,theequilibriumstrategies
areAa→1(i.e.,thefirmspredict1uponseeingAa,and0otherwise),ifandonlyif:
Pr[Aa|1]−C Pr[Aa|0]=αβ−C (1−α)(1−β)≥0
1 1
Pr[Ab|1]−C Pr[Ab|0]=α(1−β)−C β(1−α)<0,
1 124 Gafni,Gradwohl,andTennenholtz
whereweusetheexpressionsforindependentcorrelationprobabilities.
Under no-sharing, we can follow the argument of Lemma 1 (but with a para-
metric cost C ) to derive inequalities that guarantee an equilibrium where the firms
1
predict according to their signals (A → 1,a → 1). Lastly, given that these are the
no-sharing equilibrium strategies, assume that uns ≤ uns = αβ + α(1 − β) −
2 1 2
(cid:16) (cid:17)
C (1−α)(1−β) +β(1−α) ≤ 1(αβ−C (1−α)(1−β)) = ufs = ufs. If all
1 2 2 1 1 2
these conditions hold, then full-sharing is the unique IRPO contract. This holds, for
example,whenα =0.9,β =0.85,C =2.5.Moregenerally,theconditionsholdfor
1
anyβ whenα=β andC =
2β2−2β−1
.
1 2(β−1)(β+1)
B ProofsforUnknownCorrelationCase
Lemma4. Ifα=β,thenforanydistributionπ overcorrelationsandanyR andC ,
θ 1 1
fs⪰ts.
Proof. Undertrain-sharing,theagentsaresymmetric,astheysharetheirsignalsatthe
trainingphase,andhavethesamepredictionaccuracyattheinferencephase.Sincethe
agentsaresymmetric,wecanassumeasymmetricequilibriumisbeingplayed.Then,
as we saw in the proof of Lemma 2, this means the equilibrium strategy is to take a
significant action (predict 1) if and only if the expected utility is positive given the
correlationlearntinthetrainingphase(andwithoutknowledgeoftheotherfirm’ssignal,
asweareundertrain-sharing).Underfull-sharing,theaddedinformationoftheother
firm’ssignalallowsformoregranulardecisionswhentotakeasignificantaction,andso
fs⪰ts.
Theorem1. IfR =C thenno-sharingisuniquelyIRPO.
1 1
Proof. WeknowbyLemma2thatfull−sharing ≥infer−sharingandbyLemma3
thattrain−sharing =no−sharing.Thus,theonlypossiblyIRPOcontractsareno−
sharingorfull−sharing.However,inthesymmetriccase,weknowfromtheproofof
Lemma1thatregardlessofthecorrelationθ,full-sharingalwaysresultsinasymmetric
equilibriums,swhereAa→1,Ab→1(and0otherwise).Inessence,thisissincethese
arethepairsofsignalswithprobabilityPr[1|Xx]> 1.Underno-sharing,alsobythe
2
proofofLemma1,theequilibriumstrategiess ,s donotdependonθ,anduns ≥ufs
1 2 1 1
foranyfixedθ.Thus,theexpectedutilityfortheprimaryfirmunderfull-sharingand
anyfixedcorrelationis 2α−1,andsincetheexpressiondoesnotdependonθ,wehave
4
uns =E [E[u (s (X),t,s (x)|θ]]≥E [E[u (s(X,x),t,s(X,x)|θ]]=ufs.
1 θ∼Θ 1 1 2 θ∼Θ 1 1
Theorem2. Ifα = β thenfull-sharingiseitheruniquelyIRPO,orfs = nsarethe
onlyIRPO.
Proof. Weconsiderbothsymmetricandasymmetricno-sharingequilibria.Inasymmet-
ricno-sharingequilibriums ,s ,thesymmetrymeansthat(s (A) = 1) ↔ (s (a) =
1 2 1 2
1),(s (B)=1)↔(s (b)=1).Undersuchasymmetricequilibrium,sinceα=β,the
1 2
utilitiesofthefirmsarethesame.Theutilitiesofthefirmsarethesameforfull-sharing
aswell,underthesymmetricequilibriums,s.ThesumofutilitiesofthefirmssatisfiesPrediction-sharingDuringTrainingandInference 25
uns+uns
1 2
=E[u (s (X),t,s (x))+u (s (X),t,s (x))]
1 1 2 2 1 2
=E [E[u (s (X),t,s (x))+u (s (X),t,s (x))|θ]]
θ∼Θ 1 1 2 2 1 2
=E [E[1[s (X)=1∨s (x)=1]u (1,t,0)|θ]]
θ∼Θ 1 2 1
=E [E[1[s (X)=1∨s (x)=1](R ·Pr[t=1|Xx]
θ∼Θ 1 2 1
−C ·Pr[t=0|Xx])|θ]]
1
≤E [E[(R ·Pr[t=1|Xx]−C ·Pr[t=0|Xx])+|θ]]
θ∼Θ 1 1
=E [E[1[s(X,x,θ)=1](R ·Pr[t=1|Xx]
θ∼Θ 1
−C ·Pr[t=0|Xx])|θ]]
1
=ufs+ufs,
1 2
whereweusethenotation(x)+ =max{x,0}.Wealsousethefactthatfull-sharing
takesasignificantactionifandonlyifR ·Pr[t=1|X,x,θ]−C ·Pr[t=0|Xx,θ]
1 1
holds, as we have seen in previous proofs. Therefore, and since ufs = ufs, each of
1 2
thefirmshasatleastasmuchexpectedutilityunderfull-sharingthanunderno-sharing,
whichyieldsthetheoremstatement.
Asforasymmetricno-sharingequilibria,itcanbedirectlycalculatedthattheonly
possiblesuchequilibriumwhenα = β iswheres isA → 1,B → 0,ands always
1 2
predicts0.Giventhisequilibrium,weknowthat
(cid:18) (cid:19) (cid:18) (cid:19)
Pr[1|Aa,θ] [Pr[0|Aa,θ]
E [R +Pr[1|Ba,θ] −C · +Pr[0|Ba,θ] ]<0.
θ∼Θ 1 2 1 2
(12)
bytheequilibriumconditionthatdeterminesa→1.
Thus,
uns =E [R (Pr[1|Aa,θ]+Pr[1|Ab,θ])−C ·([Pr[0|Aa,θ]+Pr[0|Ab,θ])]
1 θ∼Θ 1 1
Pr[1|Aa,θ] Pr[0|Aa,θ]
=E [R · −C · ]
θ∼Θ 1 2 1 2
(cid:18) (cid:19)
Pr[1|Aa,θ] [Pr[0|Aa,θ]
+E [R · +Pr[1|Ab,θ]−C · +Pr[0|Ab,θ] ]
θ∼Θ 1 2 1 2
Eq.12 Pr[1|Aa,θ] Pr[0|Aa,θ]
< E [R · −C · ]
θ 1 2 1 2
(cid:18)
Pr[1|Aa,θ]
Pr[0|Aa,θ](cid:19)+
≤E [ R · −C · ]
θ 1 2 1 2
≤ufs.
1
Theorem3. Train-sharingisuniquelyIRPOforanopensubsetofparametersπ ,α,β,
θ
R ,C .
1 126 Gafni,Gradwohl,andTennenholtz
Proof. We describe the general settings that yield our example, and then state such
π ,α,β,R ,C valuesthatimplementit.Considertwopossibleworlds,withprobability
θ 1 1
wand1−w,respectively:Inthefirst,signalsareindependent,andinthesecond,signals
aretotallycorrelated,i.e.,θisthemaximalcorrelationpossiblebetweentwofirmswith
predictionaccuraciesα,β.Noticethatsince
Eq.8(cid:112) (cid:16)(cid:112) (cid:112) (cid:17)
β =Pr[x=a|1]≥Pr[X =A∧x=a|1] = αβ αβ+θ· (1−α)(1−β) ,
(cid:113)
wehaveθ ≤ β(1−α),andθismaximizedwhenthisinequalityholdsasanequality.
α(1−β)
Whenthishappens,wehavePr[X = A∧x = a|1] = β,Pr[X = A∧x = b|1] =
α−β,Pr[X = B ∧x = a|1] = 0,Pr[X = B ∧x = b|1] = 1−α,andthemirror
imageofitconditionalon0:Pr[X = B∧x = B|0] = β,Pr[X = B∧x = a|0] =
α−β,Pr[X =A∧x=b|0]=0,Pr[X =A∧x=A|0]=1−α.SeeFigure6for
anillustrationofthecasewhenα=0.7,β =0.6.
For conciseness, in this proof we specify the strategies by what signals lead to
predicting 1, and all other signals lead to predicting 0. Now, consider a case where
the train-sharing equilibrium in the independent correlation case is A → 1,a → 1,
andinthetotalcorrelationcaseisA → 1,whileunderno-sharingtheequilibriumis
A→1,a→1.Withprobabilityw,θ =0andtheequilibriumthefirmsplayunderboth
regimesisthesameandsoaretheutilities.Butw.p.1−w,theequilibriumplayedunder
thetworegimesisnotthesame.Thisbenefitsthesecondaryfirmbythefactthatthisis
thetrain-sharingequilibrium:Itmeansthatgiventhatθ =θ andtheprimaryfirm
max
playsA→1,thesecondaryfirm’sbestresponseistonottakeasignificantactiongiven
anysignal.Butthisalsobenefitstheprimaryfirm,sincenowitdoesnotneedtoshare
itsrewardofR ·Pr[1|Aa,θ ]−C ·Pr[0|Aa,θ ]withthesecondaryfirm,and
1 max 1 max
thisexpressionisstrictlypositiveaspartoftheequilibriumconditionfortheprimary
firm.Overall,thisestablishes(train−sharing >no−sharing).
Itisthenenough,inorderfortrain−sharingtobetheuniquefeasiblecontract,
torequirethatitdoesnotholdthatfull−sharing > no−sharing (recallthatby
Lemma 2, full−sharing ≥ infer −sharing). This holds whenever the primary
firm’sutilityislargerunderno-sharingthanunderfull-sharing.
Wecanrequirethatthefull-sharingequilibriumbothintheindependentcaseandthe
totallycorrelatedcaseisAa→1,Ab→1,andthisimplies
1
ufs = (w(R α−C (1−α))+(1−w)(R α−C (1−β))).
1 4 1 1 1 1
Wealsohave:
(cid:18) (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
1 αβ (1−α)(1−β)
uns = w R +α(1−β) −C +(1−α)β
1 2 1 2 1 2
(cid:18) (cid:18) (cid:19) (cid:19)(cid:19)
β 1−β
+(1−w) R +α−β −C · ,
1 2 1 2
andweimposetheconditionuns >ufs.
1 1Prediction-sharingDuringTrainingandInference 27
Finally,wenotethattheopenset0.72<α<0.721,0.513<β <0.514,0.755<
C < 0.756,0.999 < R < 1.001,0.5 < w < 0.50001satisfiesalltheabovecondi-
1 1
tions.
B.1 RepresentingtheCorrelationModelofSection3inourGeneralFramework
ofSection2
First,weshowhowtodescribeourmodelofunknowncorrelationswithintheframework
ofourgeneralmodelforfirms’prediction-sharing.Considerthefollowingconstruction.
Fixsomeα,β andcorrelationdistributionΘ.Wedrawη ∼Uniform([0,1])andθ ∼Θ,
and describe how the two parameters determine a world w . In a world w , the
η,θ η,θ
intervalstructureoffirm1satisfies
A =
w

{[η,η+α]×{1}∪ η([ +0, α1] ≤\[ 1η,η+α])×{0}}
{[η,1]∪[0,η+α−1]×{1} .
∪([0,1]\([η,η+α
η]∪ +[0 α,η >+ 1α−1)))×{0}}
√ √
(cid:112)
Weletρ= αβ( αβ+θ· (1−α)(1−β)asinEquation8.Let
e =
w

[η+α−ρ,η+α−ρ+β]
[η+α−ρ,1]∪[0,βη
η+ +−α α(1− −−ρ ρ(η+ ++β βα≤ >−1
1ρ ∧)) η]
+α−ρ≤1
.
[ [η η+ +α α− −ρ ρ− −1 1, ,1β ]+
∪η
ηη
[0+
++
,βα
αα
−−
−−
(ρ
ρ1ρ
>
>−−
1
1(1
η∧
∧]
+η ηα+ +−α αρ− −−ρ ρ1+ +))β
β]
≤ >2
2
Theintervalstructureoffirm2thensatisfiesa ={e ×{1}∪([0,1]\e )×{0}}.
w w w
The main technical claim for the construction is that not only firm 1, but also
firm 2, gets a uniform draw over its intervals a . That is, if frac(x) = x−⌊x⌋ is
w
the fractional part of a number x, then frac(η +α−ρ) ∼ Uniform([0,1]) (up to a
measurezeroadjustmentat0and1).Toseethis,observethat,foreveryρ,frac(η+α−
ρ)|ρ∼Uniform([0,1]),andsotheprobabilitydensityfunctionp˜(frac(η+α+ρ))=
E [frac(η+α+ρ)|ρ]=1.
ρ∼ρ(Θ)
Wecanthenverifythatthefirms’posterioroverthecorrelationbetweenthefirms’
signalsisΘregardlessofthetrainingsignalw —theirownintervalstructure—thatthey
i
observe.AsweshowinSection3.1,knowingPr[X =A∧x=a|1]uniquelydetermines
θ,andvice-versa.Inourconstruction,anyworldhasPr[X =A∧x=a|1]=ρ,whichis28 Gafni,Gradwohl,andTennenholtz
a1-to-1functionofθ.So,firm1,uponlearningA ,knowsthatthedistributionoverρis
w
determinedbyΘ,andsoconcludesthatthedistributionovercorrelationsisΘ.Similarly,
firm2,uponlearninga (whichisdeterminedbyη+α−ρ),knowsthatη isdrawn
w
uniformlyover[0,1],andsobyBayes’rulep˜(ρ|η+α−ρ)=p˜(η+α−ρ|ρ) p˜(ρ) =
p˜(η+α−ρ)
p˜(ρ),wherethelasttransitionfollowsfromthefirms’uniformprioroverworldmodels.
Tofurtherillustratethisconstruction,letususetheexamplewerepeatedlyusein
ourproofs,wherethecorrelation(conditionedonthetruerealization)betweenthefirms’
signaliseither0(conditionalindependence)orthemaximalpossible(totalcorrelation).
Figure 5 shows an intuitive but incorrect way to model this case within our general
framework.Figure6showsasimplecorrectmodellingusingafinitenumberofworlds
(whichisnotourgeneralconstruction).Figure7showshowwemodelthiscaseusing
ourgeneralconstruction.
Fig.5:IncorrectModellingoftheunknowncorrelationwithinthegeneralprediction-
sharing framework. In this modelling, we have α = 0.7,β = 0.6, and there are two
possibleworlds,oneappearsw.p.z,andresultsinindependentsignalsA,aandB,b
given the true realization (whether 0 or 1). The other appears w.p. 1−z and results
in totally correlated signals. However, this modelling does not capture our model of
unknown correlation, since firm 2 can deduce the correlation based only on its own
information.Prediction-sharingDuringTrainingandInference 29
Fig.6: Correct Modelling of the unknown correlation within the general prediction-
sharingframework.Inthismodelling,wehaveα = 0.7,β = 0.6,andtherearefour
possibleworlds,appearingrespectivelyw.p.z2,(1−z)z,(1−z)z,and(1−z)2.The
firstandthirdpossibleworldsresultinindependentsignalsA,aandB,bgiventhetrue
realization (whether 0 or 1). The second and fourth possible worlds result in totally
correlatedsignals.Inthismodelling,whateverintervalstructurefirm1sees,aBayesian
updatingoftheposteriorwouldleadittobelievethatthecorrelationbetweenthefirms’
signalisindependentw.p.zandtotallycorrelatedw.p.1−z.Thesameholdsforfirm2.30 Gafni,Gradwohl,andTennenholtz
Fig.7:General&CorrectModellingoftheunknowncorrelationwithinthegeneral
prediction-sharingframework,usingourgeneralconstruction.Inthismodelling,wehave
α = 0.7,β = 0.6,andthereareinfinitepossibleworlds,drawneitherfromtheupper
type(representingtheindependentsignalscase)w.p.zorthelowertype(representing
the totally correlated signals case) w.p. 1−z, and then the intervals as described in
thefigureareshiftedcyclicallywithanoffsetη ∼ Uniform([0,1]).Inthismodelling,
whateverintervalstructurefirm1sees,aBayesianupdatingoftheposteriorwouldleadit
tobelievethatthecorrelationbetweenthefirms’signalisindependentw.p.zandtotally
correlatedw.p.1−z,andthesameholdsforfirm2.Prediction-sharingDuringTrainingandInference 31
C TwoHypothesesModelProofs
Fig.8:Forthereader’sconvenienceweincludeFigure1againhere.
Theorem4. Infer-sharingisuniquelyIRPOforanopensubsetofparametersπ ,κ,λ,
I
andµ.
Proof. Wedescribeconditionsthatyieldthestatedresult.
(Conditionsthatyieldno−sharing =train−sharing)
Consider
1 (1−κ)λ
(1−κ)µ>2κ, (1−κ)(1−λ−µ)>κ> . (13)
2 2
Then,
Since w (the signal that Firm1 receives regarding the true model) is enough to
1
determinethecorrecthypothesiswithcertainty,undertrain-sharingbothfirmsknow
thecorrecthypothesisgivenσts.Letσts ={w ,w ,a},i.e.,Firm2seesw ,w inthe
i 2 1 2 1 2
trainingphaseandthesignalaintheinferencephase.Foranystrategys ofFirm1,and
1
train-phasesignalsw ,w ,wehave
1 2
(cid:104) (cid:16) (cid:17) (cid:105)
u˜ts({w ,w ,a},0,s )=E u 0,t,s ({w ,w ,X}) |{w ,w ,a}
2 1 2 1 2 1 1 2 1 2
(cid:104) (cid:16) (cid:17) (cid:105) 1
≥E u 0,t,0 |{w ,w ,a} = Pr[t=0|{w ,w ,a}]
2 1 2 2 1 2
1 1 (1−κ)µ
= Pr[t=0|a]=
2 2κ+(1−κ)µ
Eq.13 κ (cid:104) (cid:16) (cid:17) (cid:105)
> =Pr[t=1|{w ,w ,a}]=E u 1,t,0 |{w ,w ,a}
κ+(1−κ)µ 1 2 2 1 2
(cid:104) (cid:16) (cid:17) (cid:105)
≥E u 0,t,s ({w ,w ,X}) |{w ,w ,a}
2 1 1 2 1 2
=u˜ts({w ,w ,a},1,s ),
2 1 2 132 Gafni,Gradwohl,andTennenholtz
i.e.,itisdominantforsecondaryfirmtoplaya→0regardlessofthehypothesisor
theprimary’sfirmaction.Sincethesignalbalwayscoincideswithtruerealization0,
thesecondaryfirmalwaysplaysb→0aswell.Butsincepredicting0isthedominant
strategyforanytraining-phasesignalundertrain-sharing,aswesawinpreviousproofs,
thisisalsothedominantstrategyunderno−sharingforthesecondaryfirm.
Asfortheprimaryfirm,becauseithasadifferentsignalforthetruerealization1under
eachhypothesis,itcandeterminewhichhypothesisistruebothintheno−sharingand
train−sharingcontracts.Overall,thismeansthattheequilibriumforbothcontracts
isthatthesecondaryfirmalwayspredicts0,andtheprimaryfirm,giventhatitfindsthat
hypothesisIiscorrect,predictsA→1,B →0,andgiventhatitfindsthathypothesisII
iscorrect,alwayspredicts0(byadirectcalculationoftheconditionalutilitiesforFirm1
andusingEquation13).
(Conditionthatpreventsfull−sharingbeingalwaysindividuallyrational)
Consider
κ>(1−κ)λ. (14)
Underfull-sharing,bothfirmscandeterminethetruehypothesisandgiventhepair
ofsignalscanalsofollowupwithpredictingthecorrecttruerealizationwithcertainty.
Thus,theexpectedutilityunderanyw ,w ,foreachfirm,is 1.Toshowthatitdoes
1 2 2
notholdthatfull-sharingisalwaysindividuallyrational,itthussufficestoshowthat
uns > 1 withsome(w ,w ).Therearetwopossiblew fortheprimaryfirm:Eitherthat
1 2 1 2 1
thetruerealization1alwayscoincideswithA(whichhappensifandonlyifHypothesis
Iistrue),orthatitalwayscoincideswithB.Ifthefirstholds,andbyourassumption
in Equation 14, for the equilibrium strategies we saw in our analysis of no-sharing,
uns =κ+ (1−κ)(1−λ) = 2κ+1−κ−λ+κλ = 1+κ−(1−κ)λ > 1 =ufs.
1 2 2 2 2 1
(Conditionsforinfer−sharing >no−sharing)
Wenotethatunderinfer-sharing,theprimaryfirmcanbothdeterminethecorrect
hypothesis and has the pair of signals that determines the true realization, and thus
alwayspredictscorrectly.Wewishtofindaconditionsothatthesecondaryfirmhas
thesamestrategyasunderno−sharing,toalwayspredict0.Underbothhypotheses,
thesecondaryfirmshouldpredictAb,Bb → 0.AsforsignalAa,thesecondaryfirm
predicts0,aslongas(1−π )(1−κ)µ>π κ.SimilarlyforsignalBa,thesecondary
I I
firmpredicts0aslongasπ (1−κ)µ>(1−π )κ.Whenthisholds,theprimaryfirm
I I
gainsutilitythroughitsabilitytodifferentiatebetweentheBaandBbsignalsunder
hypothesisII.This“freemeal”phenomenonissimilartotheexampleintheintroduction
of[15].
Wewrapupbynotingthatalltheaboveconditionsaresatisfiedwhen0.2 < µ <
0.38,0.05<κ<0.06,0.011<λ<0.02,and0.65<π <0.75.
I
Theorem5. UndertheparametersofTheorem4butwithonesample,no-sharingand
train-sharing are not necessarily equivalent, no-sharing is IRPO (rather than infer-
sharing),andFirm1haslowerequilibriumexpectedutilitythanFirm2.
Proof. We consider additional conditions, on top of these of Theorem 4, that would
yield the stated result: An example where firm 2 has higher expected utility in the
IRPOcontractthanfirm1,andthatallcontractsbesidesno-sharingarenotindividually
rational.Prediction-sharingDuringTrainingandInference 33
(Under train-sharing) In the two hypotheses model with one sample, the pair of
signalsAaandBa,togetherwiththeirtruerealization,determineswithcertaintythetrue
hypothesis.Ontheotherhand,thesignalsAborBbtogetherwiththeirtruerealization
(thatcanonlybe0)maintainsthesameposteriorastheprior.Thus,weconcludethat
underthetrain-sharingandfull-sharingcontracts,thereisaprobabilityofκ+(1−κ)µ
thatbothfirmslearnthetruehypothesis,andotherwisethefirmsmaintaintheirprior.
The secondary firm always predicts 0 (the dominance argument of Theorem 4
generalizesregardlessofthetraining-phasesignal),andtheprimaryfirmpredictsA→
1,B →0ifitknowsHypothesisIiscorrect,andalwayspredicts0ifitknowsHypothesis
IIiscorrect.Otherwise,itsposterioristhesameastheprior,andsoitspredictionforA
is0ifandonlyif (1−κ)(λ+(1−πI)µ) ≥π κ,anditspredictionforB is0ifandonlyif
2 I
(1−κ)(1−λ−(1−πI)µ) ≥(1−π )κ.FortheparametersusedinTheorem4,thisresultsin
2 I
A→1,B →0.Theexpectedutilityoftheprimaryfirmisthus
(cid:18) (cid:19)
1
Pr[HypothesisI]· Pr[A∧1|HypothesisI]+ Pr[B∧0]
2
(cid:18)
1
+Pr[HypothesisII] Pr[s =A→0,B →0|HypothesisII]· Pr[0|HypothesisII]
1 2
(cid:18) (cid:19)(cid:19)
1
+Pr[s =A→1,B →0|HypothesisII]· Pr[B∧0|HypothesisII]
1 2
(1−κ)(1−λ) 1−κ (1−κ)(1−λ−µ)
=π ·(κ+ )+(1−π )((κ+(1−κ)µ)· +(1−κ)(1−µ)· ),
I 2 I 2 2
andtheutilityofthesecondaryfirmis
(cid:18) (cid:19)
1
Pr[HypothesisI]· Pr[A∧0|HypothesisI]+ Pr[B∧0]
2
(cid:18)
1
+Pr[HypothesisII] Pr[s =A→0,B →0|HypothesisII]· Pr[0|HypothesisII]
1 2
(cid:18) (cid:19)(cid:19)
1
+Pr[s =A→1,B →0|HypothesisII]· Pr[A∧0|HypothesisII]· Pr[B∧0|HypothesisII]
1 2
(1−κ)(1−λ) 1−κ
=π ·((1−κ)λ+ )+(1−π )((κ+(1−κ)µ)·
I 2 I 2
(1−κ)(λ+µ)+(1−κ)(1−λ−µ)
+(1−κ)(1−µ)· ).
2
(Underno-sharing)
Thesecondaryfirmalwayspredicts0.Thus,thestrategyoftheprimaryfirmfollows
adirectapplicationofEquation5wherewehavethestrategys =0offirm2.
2
TheprimaryfirmknowsHypothesisIiscorrectwithcertaintywhenitsees(A,1)
(which happens w.p. π κ), and then predicts A → 1,B → 0. It knows Hypothe-
I
sis II is correct with certainty when it sees (B,1) (which happens w.p. (1 − π )κ)
I
and then always predicts 0. For (A,0), we have Pr[(A,0)|HypothesisI] = (1 −
κ)λ,Pr[(A,0)|HypothesisII]=(1−κ)(λ+µ),andsow′ =Pr[HypothesisI|(A,0)]=34 Gafni,Gradwohl,andTennenholtz
Pr[(A,0)|HypothesisI]Pr[HypothesisI] =(1−κ)λ· πI .Sim-
Pr[(A,0)] πI(1−κ)λ+(1−πI)(1−κ)(λ+µ)
ilarly,for(B,0),wehavew′′ =Pr[HypothesisI|(B,0)]= (1−κ)(1−λ)πI .
(1−κ)(1−λ)πI+(1−πI)(1−κ)(1−λ−µ)
Weconclude(similarlytoaswedidinthetrain-sharingcasefortheoriginalprior)that
iftheprimaryfirmsees(A,0)italwayspredicts0,andiftheprimaryfirmsees(B,0),it
predictsA→1,B →0.Theprimaryfirm’sexpectedutilityisthen
(cid:18) (cid:18) (cid:19)
1
Pr[HypothesisI]· Pr[(B,0)∨(A,1)|HypothesisI]· Pr[A∧1|HypothesisI]+ Pr[B∧0]
2
(cid:18) (cid:19)(cid:19)
1
+Pr[(A,0)|HypothesisI]· Pr[0|HypothesisI]
2
(cid:18)
1
+Pr[HypothesisII]· Pr[(B,1)∨(A,0)|HypothesisII]· Pr[0|HypothesisII]
2
(cid:18) (cid:19)(cid:19)
1
+Pr[(B,0)|HypothesisII]· Pr[B∧0|HypothesisII]
2
(cid:18) (cid:19)
(1−κ)(1−λ) 1−κ
=π · (1−(1−κ)λ)(κ+ )+(1−κ)λ
I 2 2
(cid:18)
1−κ
+(1−π )· (1−(1−κ)(1−λ−µ))
I 2
(cid:18) (cid:19)
(1−κ)(1−λ−µ)
+(1−κ)(1−λ−µ) ,
2
andfirm2’sexpectedutilityis
(cid:18) (cid:18) (cid:19)
1
Pr[HypothesisI]· Pr[(B,0)∨(A,1)|HypothesisI]· Pr[A∧0|HypothesisI]+ Pr[B∧0]
2
(cid:18) (cid:19)(cid:19)
1
+Pr[(A,0)|HypothesisI]· Pr[0|HypothesisI]
2
(cid:18)
1
+Pr[HypothesisII]· Pr[(B,1)∨(A,0)|HypothesisII]· Pr[0|HypothesisII]
2
(cid:18) (cid:19)(cid:19)
1
+Pr[(B,0)|HypothesisII]· Pr[A∧0|HypothesisII]+ Pr[B∧0|HypothesisII]
2
(cid:18) (cid:19)
(1−κ)(1−λ) 1−κ
=π · (1−(1−κ)λ)((1−κ)λ+ )+(1−κ)λ
I 2 2
(cid:18)
1−κ
+(1−π )· (1−(1−κ)(1−λ−µ))
I 2
(cid:18) (cid:19)
(1−κ)(1−λ−µ)
+(1−κ)(1−λ−µ) (1−κ)(λ+µ)+ .
2
(Underinfer-sharing)
TheBayesianupdatingphasebasedonthehistoricalsampleisthesameasinthe
no-sharingcase,andsotheprimaryfirmattainsthevariousposteriorsunderthesame
probabilities.Thesecondaryfirm,regardlessonthesample,hasthesameposteriorasthe
prior,asthetwohypotheseslookthesameforitssignalstructure.ForthepairofsignalsPrediction-sharingDuringTrainingandInference 35
AbandBb,bothfirmsalwayspredict0.ForthesignalBa,since πI(1−κ)µ >(1−π )κ,
2 I
itisdominantforthesecondaryfirmtopredict0regardlessofhowtheprimaryfirm
predicts,andsimilarlyforAa,since (1−πI)(1−κ)µ > π κ,itisalsodominantforthe
2 I
secondaryfirmtopredict0.
For the signal Ba, since the secondary firm always predicts 0, the primary firm
predicts1foritwhenitsposteriorisw′,orwhenitknowswithcertaintythatHypothesis
IIiscorrect.Itpredicts0foritwhenitsposteriorisw′′,since
(1−κ)µ
w′′ >(1−w′′)κ,
2
andalsowhenitknowswithcertaintythatHypothesisIiscorrect.
For the signal Aa, since the secondary firm always predicts 0, the primary firm
predicts0foritwhenitsposteriorisw′,orwhenitknowswithcertaintythatHypothesis
IIiscorrect.Itpredicts1foritwhenitsposteriorisw′′,since
(1−κ)µ
w′′κ>(1−w′′) ,
2
andalsowhenitknowswithcertaintythatHypothesisIiscorrect.
Theprimaryfirm’sutilityisthen:
(cid:18)
1
Pr[HypothesisI] Pr[(B,0)∨(A,1)|HypothesisI]·(Pr[1|HypothesisI]+ Pr[0|HypothesisI])
2
(cid:19)
1
+Pr[(A,0)|HypothesisI]( Pr[0∧b|HypothesisI])
2
(cid:18)
1
+Pr[HypothesisII] Pr[(B,0)|HypothesisII]·( Pr[0∧b|HypothesisII])
2
(cid:19)
1
+Pr[(A,0)∨(B,1)|HypothesisII]( Pr[0|HypothesisII]+Pr[1|HypothesisII])
2
(cid:18) (cid:19)
1−κ (1−κ)(1−µ)
=π (1−(1−κ)λ)(κ+ )+(1−κ)λ
I 2 2
(cid:18)
(1−κ)(1−µ)
+(1−π ) (1−κ)(1−λ−µ)
I 2
(cid:19)
1−κ
+(1−(1−κ)(1−λ−µ))( +κ) .
2
Theutilityofthesecondaryfirmis:36 Gafni,Gradwohl,andTennenholtz
(cid:18)
1
Pr[HypothesisI] Pr[(B,0)∨(A,1)|HypothesisI]·( Pr[0|HypothesisI])
2
(cid:19)
1
+Pr[(A,0)|HypothesisI]( Pr[0∧b|HypothesisI]+Pr[0∧a|HypothesisI])
2
(cid:18)
1
+Pr[HypothesisII] Pr[(B,0)|HypothesisII]·( Pr[0∧b|HypothesisII]
2
(cid:19)
1
+Pr[0∧a|HypothesisII])+Pr[(A,0)∨(B,1)|HypothesisII]·( Pr[0|HypothesisII])
2
(cid:18) (cid:19)
1−κ (1−κ)(1−µ)
=π (1−(1−κ)λ)( )+(1−κ)λ( +(1−κ)µ)
I 2 2
(cid:18)
(1−κ)(1−µ)
+(1−π ) (1−κ)(1−λ−µ)( +(1−κ)µ)
I 2
(cid:19)
1−κ
+(1−(1−κ)(1−λ−µ))( ) .
2
(Underfull-sharing)TheBayesianupdatingphasebasedonthehistoricalsampleis
thesameasinthetrain-sharingcase,andsobothfirms(thatseethefullsinglesample
together)eitherknowthehypothesiswithcertainty,ormaintainthepriorπ .
I
Giventhatthefirmsmaintainthepriorπ ,uponseeingthesignalsAborBbthey
I
bothpredict0.IftheyseethesignalBa,since
(1−κ)µ
π >(1−π )κ,
I 2 I
bothfirmspredict0.IftheyseethesignalAa,since
(1−κ)µ
(1−π ) >π κ,
I 2 I
bothfirmspredict0.
Wethushavethattheutilityoffirm2is,ifwedenote
p∗ =Pr[(Aa,1)∨(Ba,1)∨(Aa,0)∨(Ba,0)]=κ+(1−κ)µ,
is
1 1
(p∗+(1−p∗)(1−κ))= (κ(κ+(1−κ)µ−1)+1).
2 2
Finally,wenotethattheparameterchoiceκ= 5 ,λ= 1,µ= 1,π = 1 satisfies
32 8 2 I 2
boththeconditionsdetailedaboveandtheconditionsdetailedintheproofofTheorem4.
D RobustnessofOurResultsinSection5
WewishtodeepenourempiricalresultsofSection5.Inparticular,wewishtoaddmore
variabilityintothefeatureselectionprocess.Forthispurpose,weintroducethefollow-
ingsamplingmethod.First,foreveryϵ ∈ {1,0.85,0.7,0.55,0.4,0.25},werandomlyPrediction-sharingDuringTrainingandInference 37
sampleϵofthedata-setfeatures.DenotetheresultingpartialsetoffeaturesX.Next,we
randomlysample 0.25 ofthefeaturesinX todecidethefeaturesforFirm1.Similarly,
ϵ
wesample 0.1 ofthefeaturesinX todecidethefeaturesforFirm2.Theendresultis,
ϵ
as in Section 5, that Firm1 sees 25% random features out of the original, and Firm2
sees10%.However,onecouldhopethatthefeatures(andso,theresultingmodelsafter
training)willbemorecorrelatedasϵissmaller,sinceanoverlapofthefeaturesthefirms
seeismorelikely.Foreachvalueofϵ,werun32randomexperiments,wherewerepeat
theaboveprocess,followedbytheanalysisdescribedinSection5.Overallthisresultsin
32·7=224experimentswherewetrainarandommodelforeachofthefirms.
Wethenquantifytheemergenceofoptimalcontractsinthefollowingway.Foreach
costintherange0.5to1.5in0.05steps,wefindwhataretheoptimalcontracts(inthe
IRPOsense).Webreakequivalenciesinfavorofthemore“natural”contracts,i.e.,ifany
contracthasexactlythesameexpectedutilitiesasno-sharing,wewouldnotconsiderit
optimal,andiftrain-sharingorinfer-sharingareequivalenttofull-sharing,wewould
similarlynotconsiderthemoptimal.Ifbytheendofthisprocesswehavemorethan
oneoptimalcontractforaspecificcost,wedividethe‘benefit’betweenalltheoptimal
contracts.Overall,foreachexperiment,wegetascoreforeachcontractofthefrequency
itisoptimal.Sinceweareinterestedinseeingthepossibleinfluenceofcorrelationon
optimality,wedirectlycalculatethecorrelationofFirm1’smodelandFirm2’smodel
predictionsoverthevalidationset,usingtheMatthewscorrelationforaconfusionmatrix.
Inthisway,wecandirectlycomparethetwovariablesweareinterestedin,insteadof
usingtheindirectϵparameterweuseaspartofourprocess,whichweonlyexpectto
haveaprobabilisticnegativeconnectionwiththecorrelation(i.e.,asϵhigher,wecould
expectlowercorrelation).
We present our results in Figure 9. We also provide a smoothed presentation of
ourresultsinFigure10,whereforevery0.1rangeofthecorrelationthatappearedin
theexperiments(i.e.,−0.1to0,0to0.1,andsoon,upto0.8to0.9),weaveragethe
correlationthatappearintherange,andthefrequenciesassociatedwiththem.
Fig.9:ContractOptimalityvs.FirmModels’Correlation38 Gafni,Gradwohl,andTennenholtz
Fig.10:SmoothedContractOptimalityvs.FirmModels’Correlation
Overall,theresultsreiteratethefindingswedetailinSection5.Afewsurprising
aspectstonoticearethefollowing:
• Infer-sharing appears as a unique IRPO contract, contrary to our prediction. We
believethatthisislikelyduetothefull-sharingempiricaldecisionthatisbasedon
thetestdata,whereinfer-sharingturnsouttobebetteronthevalidationdata.
• Train-sharing, which our example shows to emerge with high correlation rates,
appearsmostlywithlower(ornegative)correlations.Itisreasonable,however,to
expectthatwithnegativecorrelationratestherecouldbeexamplesofitaswell.This
isinterestingasanindicationofwheretrain-sharingmightbemostrelevant.
• In our implementation, the decision rules for no-sharing and infer-sharing are
determinedbasedonanassumptionofindependence(i.e.nocorrelation).Wewould
thusexpectthecontractstobemorefrequentlyoptimalwhenthisassumptionis
justified(lowcorrelationrates).However,no-sharingisfoundtobesomewhatmore
frequentlyoptimalwhenthecorrelationishigh.