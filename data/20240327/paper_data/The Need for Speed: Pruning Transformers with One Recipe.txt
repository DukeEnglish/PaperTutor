PublishedasaconferencepaperatICLR2024
THE NEED FOR SPEED
PRUNING TRANSFORMERS WITH ONE RECIPE
SamirKhaki∗,KonstantinosN.Plataniotis
DepartmentofElectricalandComputerEngineering
UniversityofToronto
Toronto,Canada
samir.khaki@mail.utoronto.ca
ABSTRACT
We introduce the One-shot Pruning Technique for Interchangeable Networks
(OPTIN)frameworkasatooltoincreasetheefficiencyofpre-trainedtransformer
architectures,acrossmanydomains,withoutrequiringre-training. Recentworks
haveexploredimprovingtransformerefficiency,howeveroftenincurcomputation-
allyexpensivere-trainingproceduresordependonarchitecture-specificcharacter-
istics,thusimpedingpracticalwide-scaleadoptionacrossmultiplemodalities. To
addresstheseshortcomings,theOPTINframeworkleveragesintermediatefeature
distillation,capturingthelong-rangedependenciesofmodelparameters(coined
trajectory),toproducestate-of-the-artresultsonnaturallanguage,imageclassifica-
tion,transferlearning,andsemanticsegmentationtasks.Ourmotivationstemsfrom
theneedforageneralizablemodelcompressionframeworkthatscaleswellacross
different transformer architectures and applications. Given a FLOP constraint,
theOPTINframeworkwillcompressthenetworkwhilemaintainingcompetitive
accuracyperformanceandimprovedthroughput. Particularly,weshowa≤ 2%
accuracy degradation from NLP baselines and a 0.5% improvementfrom state-
of-the-artmethodsonimageclassificationatcompetitiveFLOPsreductions. We
furtherdemonstratethegeneralizationoftasksandarchitecturewithcomparative
performanceonMask2Formerforsemanticsegmentationandcnn-stylenetworks.
OPTIN presents one of the first one-shot efficient frameworks for compressing
transformerarchitecturesthatgeneralizeswellacrossmultipleclassdomains,in
particular: naturallanguageandimage-relatedtasks,withoutre-training. Codeis
availableat: https://github.com/Skhaki18/optin-transformer-pruning.
1 INTRODUCTION
Theinceptionoftransformerarchitectures(Vaswanietal.,2017)markedthebeginningofanewera
indeeplearning,sinceaffectingvariousdomainsincludingnaturallanguageprocessing(Kenton&
Toutanova,2019),andvision-relatedtasks(Dosovitskiyetal.,2021). Thetransformers’straightfor-
warddesignhasenabledextensiveapplicationstoavarietyofchallengingproblems,butitalsobrings
a major drawback: high computational costs (Yu & Xiang, 2023). The computational resources
requiredfortrainingandinferencingwithatransformerareoftenquitesignificantandposeareal
impedimenttowide-scaleadoption,especiallyinresource-constrainedenvironments,suchasedge
devices(Wangetal.,2020a). Recentworkshaveproposedmethodsincludingquantization(Xiao
etal.,2023),pruning(Maetal.,2023),andknowledgedistillation(Haoetal.,2022)toaddressthis
bottleneck,similarlyexploredinconvolutionalneuralnetworks(CNN)compression(Lietal.,2017).
DespitemuchsuccessincompressingCNNs,transformerarchitecturescontainsignificantdifferences
intheirstructure,oftencausingimpedimentsformethodsthatworkwellintheformerdomain(Kwon
etal.,2022;Yu&Xiang,2023;Yangetal.,2023). DuetothemassivesizeofTransformermodels,
someworkshaveintroducedvariousmethodsofcompression,whichcanbelooselydividedinto
one-shotanditerative(Zhangetal.,2022). One-shotmethodsgenerallyconsistofapruningphase
followedbyre-trainingtorecoverthelostgeneralizationperformance,meanwhile,iterativeprocesses
∗ProjectPage:http://www.samirkhaki.com/optin-transformer-pruning/
1
4202
raM
62
]GL.sc[
1v12971.3042:viXraPublishedasaconferencepaperatICLR2024
canaccountforthetrainingdynamicsinmodelcompression(Zhangetal.,2022). Unfortunately,in
thepast,bothmethodshaveoftenbeenlimitedtoaparticulararchitecture/taskorrequiredsignificant
resourcesinthepruningandre-trainingprocesses. Forusermodelsthathavealreadyenduredthe
expensivecostoftraining,thereexistslimitedoptionsforfastmodelcompression(Kwonetal.,2022)
thatcanbeeasilyrealizedonstandardhardwarefordifferenttypesoftransformerarchitectures. The
lack of a general approach to transformer pruning across multiple tasks and modalities provides
sufficientmotivationfortheintroductionofaunifiedframework;henceweintroduceoneofthefirst
one-shotmodelcompressiontechniquesthatgeneralizewellovermultipletasksandarchitectures
withoutincurringthecostofre-training.
Inthiswork,weintroducetheOne-shotPruningTechniqueforInterchangeableNetworks(OPTIN)
frameworktoefficientlycompressmoderntransformers. Thenoveltyisinitsgeneralizabilityacross
domainsandtasks,anditsabilitytoproducecompetitivemodelswithoutrequiringre-training,thus
enablingfutureapplicationtolargermodelsacrossmanytasks. WeapplyOPTINtonaturallanguage,
andvision-relatedtasks,showingcompetitiveperformancewithSoTAinthesecases.
OurprimarycontributionrestsontheabilityofourOPTINframeworktoproducetransformerswith
competitiveperformanceatreducedcomputationalloads(FLOPs)acrossvarioustaskdomainsand
architectures, that can be realized on standard hardware. In particular, we demonstrate superior
performanceonavarietyoftasksinLanguage4.1),Vision(Sec4.2),andApplicationtasks(Sec4.3),
whilemaintainingcompetitivecompressionrates,withoutincurringthecostofre-training. Finally,
weexecuteseveralextensiveexperimentsfromframework-specificsettingstoapplicationsontransfer-
learningandCNNnetworkstodemonstrateOPTIN’srobustnessandgeneralizabilityoverthetask
andarchitecture(Sec3-4.3).
2 RELATED WORKS
Duetothediversityofarchitecturesandtasksdiscussedinourwork,thefollowingreviewofstate-of-
the-artmethodsprovidesanoverviewofefficienttransformerdesignfollowedbyrecentdevelopments
inbothlanguageandvisiondomains.
DomainSpecificDesignofEfficientTransformersTransformershaveenabledsignificantprogress
inthefieldofNLP(Vaswanietal.,2017;Kenton&Toutanova,2019)andComputerVision(Dosovit-
skiyetal.,2021;Liuetal.,2021a).
Efficiencyimprovementsintransformershavestemmedfromavarietyofapproachesincludingex-
ploringhybridarchitectures(Liuetal.,2021a),quantizationtechniques(Kimetal.,2021),knowledge
distillation(Haoetal.,2022),andmodelpruning(Panetal.,2021).
Recently, TorchPruning (Fang et al., 2023) explored the application of multi-domain pruning by
creatinganinter-architecturedependencymap,whileUPop(Shietal.,2023)introducedaunified
pruningmethodforcombinedvision-languagemodels. However,thesemethodshavelimitations,
includingarchitecture-specificdependenciesandexpensivere-trainingpoliciesgenerallyimpeding
wider-scaleindustryuse(Fangetal.,2023). Incontrast,ourapproachleveragesintermediatefeature
distillationtocompresspre-trainedtransformersinoneshotacrossbothlanguageandvision-related
tasks. Notably,ourmethodoperateseffectivelywithoutre-trainingandscaleswelloveravarietyof
complexarchitecturesandtaskdomains.
CompressingLanguageTransformersSeveralstructuredpruningmethodshavebeenintroducedto
compressmodelsinthelanguagedomain. AttentionHeadpruning(Micheletal.,2019)exploredthe
dynamicsofattentionheadsacrossatransformerarchitecturetodeterminetheirindividualimpacton
performance. Block-wisepruning(Lagunasetal.,2021)wasmotivatedbyremovingblockstructures
fromweightsunderthemovementpruning(Sanhetal.,2020)paradigm. DynaBERT(Houetal.,
2020)useddistillationtotransferknowledgefromawidth-adaptivenetworkontoadepth-adaptive
smallernetwork. CoFi(Xiaetal.,2022)exploredthejointpruningbetweencoarseandfine-grained
modulesinthetransformerarchitecture. Arecentwork,namelyPost-Training-Framework(PTF)
(Kwonetal.,2022),wasintroducedtopruneBERTinone-shotforNLPtasks,however,itleverages
domain-relatedtrickstoboostperformancewithaparticulararchitectureandapplication. However,
thesemethodshavelimitations,includingdependenceonarchitecture(Kwonetal.,2022;Lagunas
et al., 2021; Hou et al., 2020) and expensive re-training procedures (Lagunas et al., 2021; Sanh
etal.,2020;Houetal.,2020;Xiaetal.,2022). Focusingonthechallengeofdevelopingefficient
2PublishedasaconferencepaperatICLR2024
transformers,weovercometheseshortcomingsbyintroducingaone-shotframeworkthatproduces
competitiveresultsatsignificantFLOPsreductionsacrossseveralapplicationdomains.
Compressing Vision Transformers There have been several approaches to compressing vision
transformers by focussing on different compute-intensive modules. S2ViTE (Chen et al., 2021)
exploredstructuredsparsitybymodifyingfirst-orderimportanceapproximationsenablingthedynamic
sizing of attention heads in the ViT. SAViT (Chuanyang et al., 2022) developed a collaborative
pruning scheme that analyzes component interaction to learn individual pruning ratios. Another
streamofresearchintroducedtokenreductionmethodstoaccelerateboththetrainingandinferencing
throughputbygraduallyremovingtokensfrompropagatingforwardinaTransformer(Kongetal.,
2022;Fayyazetal.,2022). EViT(Liangetal.,2022)buildsonaTop-Kapproachbycreatingafused
tokenateachreductionstagetominimizetheinformationlostfrompruning. DynamicViT(Rao
etal.,2021)introducedalightweightpredictionmoduletoderivetheimportancescoresofeachpatch
perinput. ToMe(Bolyaetal.,2023)wasintroducedasoneofthefirstone-shotmethodsintoken
reductionandleveragedbipartitematchingtomergeafixednumberoftokensateachtransformer
blockregardlessofinputpatches. TPS(Weietal.,2023)furtheredtokenreductionandmerging,by
identifyingaprunedsubsetandsqueezingtheinformativeregionsintoareservedsubsetofkepttokens.
However,thesemethodsstillhavelimitationsthatpreventtheirwidescaleuseincludingarchitecture
specific design (Song et al., 2022; Chuanyang et al., 2022; Bolya et al., 2023) and expensive re-
trainingpolicies(Chenetal.,2021;Chuanyangetal.,2022;Liangetal.,2022;Raoetal.,2021;Wei
etal.,2023). Incontrast,theOPTINFrameworkleveragesaone-shotapproachtocompressvision
transformersacrossclassificationandsemanticsegmentationachievingcompetitiveperformance
amongst state-of-the-art. The granularity and number of prunable components in the domain of
visiontransformerswidelydifferacrossstate-of-the-artmethods(Songetal.,2022). Similarly,the
OPTINFrameworkincreasesthebaseprunablecomponentsbyallowingfortheincorporationof
tokenreductionmethodsthroughgeneratinganoptimalreductionpolicyasdiscussedinSec. 4.2.
3 MEASURING TRAJECTORY
Weaimtocompresspre-trainedtransformermodelsbyremovingprunableparameterswithminimal
importancescoresasdeterminedbyoursaliencemetricwithoutre-training. Byanalyzingtheeffects
ofparameterremovalondeeperlayersinthenetwork,ourtrajectorymetricisabletobetterselect
importantparametersbyleveraginglong-terminter-layerdependenciesinthemodel.
Problem Statement. Given a model f (with N layers) expressed by its collection of weights
[θ ,θ ,···θ ] ∈ RN×d,aprunedsubsethasweightcollection[θ′,θ′,···θ′ ] ∈ RN×d,suchthat
0 1 N 0 1 N
θ′ = m⊙θwherem ∈ {0,1}d isabinarymaskand⊙istheelementwiseproductoperator. We
definethisprunedsubsettobeoptimalifitsatisfiesthecostconstraintandresultsintheminimum
decreaseinvalidationerrorfromthebasemodelexpressedwithL . Formally,weexpressthisas:
err
argmin L (f(X,[θ ,θ ,···θ ]),f(X,[θ′,θ′,···θ′ ]))
[θ′,θ′,···θ′ ] err 0 1 N 0 1 N
0 1 N
subjectto C([θ′,θ′,···θ′ ])≤C. (1)
0 1 N
where,theoptimalselectionofweights[θ′,θ′,···θ′ ]meetsthecostrequirementC whileretaining
0 1 N
theminimumdropinvalidationperformanceonthedatasetX.
ApproachIngeneral,foreachtransformerblockwedefinetheprunableweightsasthecollection
ofattentionheadsandfullyconnectedneurons,individuallydenotedbyθ wheretheparameteris
i,j
locatedinlayeriatanarbitraryindexj. Theexactprunablecomponentsforeachtaskaredescribed
inAppendixA.8. Weprogressivelymaskeachprunableparameter, andcomputetheimportance
scorebyexecutingaforwardpassthatoriginatesfromlayeriandpropagatesforwardtothelogit
prediction. In particular we express the masking of weight j in layer i as MASK ⊙θ where
j i
MASK istheinstanceofmwithasinglezeroatlocationj,asusedinAlgorithm1. Thismasked
forward pass yields subsequent layer-wise activations and output logits, which are both used in
computingthetrajectoryofparameter,θ . Wedenotethecumulativeimportanceofparameter,θ ,
i,j i,j
asI . ReferringtotheoptimizationprobleminEq.1,weuseourimportancemetricasaproxyfor
i,j
determiningwhichparameterswillleastaffectthevalidationerror,L ,onthetestingdataset,X.
err
UponcomputingallimportancescoresI ,weemployanexpeditedmask-searchpolicy,from(Kwon
i,j
etal.,2022),whichcomputestheoptimalconfigurationinafasterpolynomial-timebysequentially
3PublishedasaconferencepaperatICLR2024
addingparametersindescendingimportance. Furtherdetailsonthesearchmethodarediscussedin
AppendixA.1. WeintroducetheOPTINFrameworkalgorithminAlgorithm1andDiagraminFig. 1.
Algorithm1OPTINFrameworkforModelCompression
1: Inputs: FLOPsConstraint(C),ImportanceScores(I ←−[]),model,batch
2: ([F 0,···F N],logits)←−model(batch) ▷Pre-Compute Forward Pass
3: forθ iin[θ 0,θ 1,···θ n]do ▷layer: i
4: forj ∈range(d)do ▷weight: j
5: model[i].weight←−θ i∗MASK j ▷Apply Mask to Weight (i,j)
6: ([F′,···F′ ],logits′)←−model(batch) ▷Compute Masked Forward Pass
0 N
7: I i,j ←−(cid:80)N z=i+1L MD(F z′,F z)+λL KD ▷Apply Eq.2 and 3
8: endfor
9: endfor
10: Reduced Model←−SEARCH(I,C)
Figure1: IllustratesthecomputationoftheOPTINFrameworkstrajectorymetriconweightθ . By
i,j
applyingamasktoweightθ inLayer andexecutingaforwardpass,theOPTINframeworkcan
i,j i
measuretheeffectonfuturelayerembeddings(trajectory),asanindicatorofweightimportance.
L isthemanifolddistillationlosscomputedbetweenlayerembeddingsateachtransformerblock,
MD
whileL istheKL-Divergencecomputedbetweentheoriginallogitsandthoseduetothemasked
KD
weight. ThecombinationlossesarefurtherdetailedintheWeightImportanceheadingunderSec.3
ParameterImportancePriortoassigninganimportancescoretoeachparameter,wedefinewhat
itmeanstobe“important”. Whilemanypriorworkshavecoinedtheimportanceofparametersby
analyzingtheirintrinsicstructureanderrordynamics(Kurticetal.,2022),thesearenotnecessarilythe
mostintuitive. Forinstance,magnitude-basedmetrics(Lietal.,2017)capturetheintrinsicdominant
property of individual weight structures, however, fail to capture their interactions with the data,
meanwhile,activationmethods(Linetal.,2023)cancapturein-placereconstructionerrors,however,
they may obscure the global impact on deeper layers in the model. Motivated by capturing the
long-termeffectsofweights,weframetheproblemasidentifyingwhichweightsaremoreimportant
basedonhowmuchtheyaffectsubsequentlayerembeddings,hencewecointhemeasuretrajectory.
EffectonTrajectoryTocomputethetrajectoryofaweight,θ wefollowa2-stepprocedure.Firstly,
i,j
wemeasurelayer-wiseactivationerrorspriortotheLayerNormoperatorateachblocksubsequentto
thelayerofinterest. Weconductedanablativestudyin 1ashowingtheeffectofusingpre-layernorm
embeddings. WefirstdefinethefeatureoutputoflayeriasF ∈ RB×T×D,whereB isthebatch
i
size,T isthetokenlengthandDistheembeddingdimension.Wesimilarlyexpressthefeatureoutput
ofthemaskednetworkusingtheprime′symbol. Inspiredbydistillationsworks(Sajedietal.,2023a;
Pengetal.,2019),wecomputethelayer-wiseerrorbyadoptingfine-grainedmanifolddistillation
(Haoetal.,2022). Areshapingoperatorψ(·)∈RBT×D leveragespatchandbatchlevelinformation,
definingtherelationalmapandassociatedmetricas:
M(F )=ψ(F )ψ(F )T L (F′ ,F )=||M(F′ )−M(F )||2 (2)
i i i MD i i i i F
However,unlikepreviousworks,wedonotusethislosstoguidetrainingordistillation,rather,we
expressitasanin-placemetrictohelpunderstandparameterimportancethroughoutthenetwork.
4PublishedasaconferencepaperatICLR2024
Dataset Emb. Acc. Dataset Temp. Acc. Dataset Aggregate Acc.
MNLI L-Norm 81.92 MNLI 1 82.01 MNLI sum 81.90
MNLI FFN 81.90 MNLI 2 82.11 MNLI mean 80.75
MNLI IM-Dense 81.83 MNLI 4 81.90
MNLI 8 82.14
ImageNet L-Norm 71.27 ImageNet 1 70.54 ImageNet sum 71.25
ImageNet FFN 71.25 ImageNet 2 70.77 ImageNet mean 71.15
ImageNet IM-Dense 70.90 ImageNet 4 71.25
ImageNet 8 71.00
(a) Embedding Choice. The (b) Temperature Choice. The (c) Layer Error Aggregation.
dense output layer best informs choiceofT =4bestcapturesthe The cumulative error over the
weight performance when com- effectonthelogitsbyremoving layer’smanifolddistributionbest
paredlayer-wise. weights. capturesweightimportance.
Dataset L L λ Acc. Dataset Type† Acc.
MD KD c
MNLI ✓ - - 81.71 MNLI [i] 78.89
MNLI - ✓ - 80.91 MNLI [i+1] 80.07
MNLI ✓ ✓ 10 81.74 MNLI [i,N] 81.65
Language
MNLI ✓ ✓ 1 81.86 MNLI [i+1,N] 81.90
MNLI ✓ ✓ 0.1 81.90
MNLI ✓ ✓ 0.01 82.12
ImageNet ✓ - - 70.34 ImageNet [i] 68.91
ImageNet - ✓ - 68.85 ImageNet [i+1] 69.55
ImageNet ✓ ✓ 10 70.82 ImageNet [i,N] 70.04
Vision
ImageNet ✓ ✓ 1 70.85 ImageNet [i+1,N] 71.25
ImageNet ✓ ✓ 0.1 70.99
ImageNet ✓ ✓ 0.01 71.25
(d)ComponentAnalysisCombiningbothL andLKDcap- (e) Layer Trajectory Depth. Ac-
MD
turesthebestinformation(seeEq.3).Basedonthevalueofλ,we cumulating L over deeper lay-
MD
canseeL shouldhaveastrongerweightinparameterselection. ersperformsbest.†indicateswhich
MD
FurtherdetailsonthehyperparameterchoiceareinAppendixA.2. layer(s)tomeasureL ,relativeto
MD
currentlayeriandfinallayerN.
Table1: AblativeExperimentsonTrajectoryusingBERT (Kenton&Toutanova,2019)on
BASE
theGLUEbenchmarkMNLIdataset(Wangetal.,2019),andDeiT-Ti(Touvronetal.,2021)onthe
ImageNet-1Kdataset(Dengetal.,2009)toexploretheeffectparametersonmodelperformance.
Wemeasureone-shotpost-pruningaccuraciesovervariousconfigurationsonboththelanguageand
visiondatasets. Inparticular(a)exploreslocationstoextractfeaturesfordistillationloss: L-Norm
(AfterLayerNormalization),FFN(AfterDenseoutputlayer),IM-Dense(AfterDenseEmbedding
Layer). (b)examinestheeffectoftemperatureintheKL-DivergenceFormulation,(c)exploresthe
effectofsummingoraveragingoverthelayerdistillationerror,(d)explorestheeffectofmetrics
L andL aswellastheirbalancingparamterλ,(e)exploreswhichlayertoaccumulatethe
MD KD
distillationerrorinrelationcurrentlayeri.CompressionratesremainconsistentwithTab.9andTab3.
ThebaselineperformanceofBERT onMNLIis84.53%,meanwhileDeiT-TionImageNet-1K
BASE
is72.20%%. Ourdefaultsettingsaremarkedin green.
Ifthemaskedweightisatpositionj inlayeri, thecomputederrorisaccumulatedoverboththe
dimensionDandateachlayerlintherange[i+1,N]. Thechoiceoferroraggregationwasexplored
inTab.1candclearlydemonstratedthebenefitofthesumoperator. Additionally,weexploredthe
effectofmodifyingwhichlayerswererelevanttothetrajectory–seeTab.1e–overallitwasevident
thatusingsubsequentlayersyieldedthebestresult,correctlyaligningwiththeoriginalmotivation.
EffectonLogitsNextwecomputetheeffectsonthelogitpredictionasshowninFig.1withL .
KD
Severalworkshaveshowntheeffectsofusinglogitpredictionstoguidethetrainingprocesswith
distillation(Haoetal.,2022;Zhaoetal.,2022)orcorrelation(Sajedietal.,2024;2023b). However,
inthiswork,weusetheL loss(definedin(Hintonetal.,2015))asanin-placemetrictoquantify
KD
theimportanceofaparticularweight. WeablatethetemperaturevalueinTab. 1b. Thus,ifmaskinga
particularweightproducesalargerL ,wewouldhypothesizethatitisamoreimportantweight.
KD
5PublishedasaconferencepaperatICLR2024
Wecanformalizetheimportanceofaparticularweightj atlayeriwithiteratorzas:
N
(cid:88) ′
I = L (F ,F )+λL (3)
i,j MD z z KD
z=i+1
whereI isdefinedforaparticularweightinaparticularlayer,λcontrolsthecontributioneffectof
i,j
KD,andL comparestheprunedandoriginalresultingembeddingsindeeperlayers. Weablate
MD
theeffectofdifferentλvaluesaswellasthecontributionofeachlossindividuallyinTab.1d. Further,
weshowthatapplyingagreaterimportanceonL resultsinbetterparameterselection. Further
MD
detailsonthecontributionhyperparameterareexpressedinAppendixA.2.
WealsoprovidethealgorithmforapplyingOPTINonagenericmodelinstanceinAlgorithm1.
4 EXPERIMENTAL DESIGN
In this section, we demonstrate the effectiveness of the OPTIN Framework, at improving model
performanceandthroughputgivenstrictFLOPreductionratios. Weintroduceimplementationand
evaluationdetailstoensurereproducibilityandbenchmarkourmethodwithstateofartinnatural
languageandimageclassificationtoillustratethepotentialofourone-shotframework. Wefurther
investigatetheapplicationsintransferlearning,alternatearchitectures,anddownstreamtaskstoshow
thegeneralizabilityofourmethodacrosstasksandarchitectures.
ExperimentalSetupWeimplementourmethodusingtransformersfromtheHuggingFaceLibrary
(Wolf et al., 2020) and infrastructure from PyTorch (Paszke et al., 2019). The majority of our
experiments explore using the OPTIN Framework to improve off-the-shelf models without re-
training. TheexceptionsincludeselectexperimentsintheApplicationsSection,seeSec. 4.3. The
OPTINFrameworkcomputesparameterimportanceonthebasisoftrainingdatainagradient-free
forwardpass. Theamount(batch)ofdatausedtocomputethescoresisablatedinAppendixA.6.
Finally,detailsontheprunablecomponentsundereachsettingaredescribedinAppendixA.8.
Datasets&NetworksTheOPTINFrameworkistestedagainstavarietyofnetworkarchitectures
anddatasetstoensuregeneralizabilityoverboththetaskandmodeldomains. ForNaturalLanguage
Processing,OPTINisevaluatedontheGLUEBenchmark(Wangetal.,2019)usingtheBERT
BASE
(Kenton&Toutanova,2019)architecture. ForImageClassification,bothImageNet1-K(Dengetal.,
2009)andCIFAR10(Krizhevskyetal.,2009)wereusedwiththeDeiT-Ti/S/B(Touvronetal.,2021),
ViT-B(Dosovitskiyetal.,2021),andaVGGNet(Simonyan&Zisserman,2014)
architecturetodemonstratetheOPTINFramework’srobustnessonmodeltype/size,imagedatasets,
andtransferlearning. ForSemanticSegmentation,theCityscapesDataset(Cordtsetal.,2016)was
usedwiththeMask2Former(Chengetal.,2022)withaSwin-Tibackbone(Liuetal.,2021a)toshow
howtheOPTINFrameworkcouldbeusedtomaintaincompetitiveperformanceandthroughputat
constrainedFLOPs. FurtherdetailsondatasetselectionareinAppendixA.3
EvaluationMetricsWiththegoalofmodelcompression,weevaluatemodelsbasedontheiraccuracy
(ormIoUforsegmentation)givenaFLOPreduction. Detailsregardingthemetricchoiceforthe
correspondingtaskcanbefoundinAppendixA.4. Inselectcases,weincludelatencymeasurments
expressedasaratiooftheimprovedinferencingspeedtothatofthebaseline. Alltimemeasurements
arecapturedover300iterationsonanNvidiaRTX2080usinga100-iterationwarmup.
4.1 LANGUAGEEXPERIMENTS
PerformanceonNLPBenchmarksInTab.9weinvestigatetheOPTINFrameworkforcompressing
languagemodelsontheGLUEdatasetusingBERT . Measuringperformanceandthroughput
BASE
speeds, we show a relatively low average decline in baseline accuracy (≤ 2%) at a 40% FLOPS
compressionrate. Similarly,webenchmarkourperformancewithaleadingone-shotSoTAmethod:
Post-Training-Pruning-Framework(PTF)(Kwonetal.,2022)atthesamecompressionrateandshow
superiorperformance.Inparticular,wecomparewiththemasksearchresultsfromPTF,assubsequent
phasesintheirmethodcouldbestackedonotherpost-trainingpruningmethods(refertoAppendix
A.7forexetendedcomparisons). WedemonstraterobustnessovervariouscompressionratiosinFig.
2wherewebenchmarkOPTINagainstpipelinesthatincorporatere-training,includingCoFi(Xia
etal.,2022),DynaBert(Houetal.,2020),SLIP(Linetal.,2020b),EBERT(Liuetal.,2021b),BMP
(Lagunasetal.,2021)andFLOP(Wangetal.,2020b). Despitetheaddedre-trainingphaseinother
methods, the OPTIN Framework is able to retain competitive test performance over a variety of
compressionratiosthusestablishingacompellingargumentforretraining-freepipelines.
6PublishedasaconferencepaperatICLR2024
Method MNLI QQP QNLI SST STS-B MRPC
BERT 84.53 91.00 91.41 93.57 88.90 86.27
BASE
PTF† 81.21 89.99 88.38 92.13 87.10 83.14
OPTIN‡ 81.90 90.06 88.49 92.24 87.25 85.13
Table2: NaturalLanguageBenchmarks. ComparingOPTINperformanceontheGLUE(Wang
etal.,2019)benchmark(refertoA.7foradditionalresults). TherelativeFLOPconstraintissetto
60%forafaircomparison.
Figure2: NaturalLanguageFLOPsvsAccuracy. WedirectlybenchmarktheOPTINFramework
againstleadingstate-of-the-artmethodsinnaturallanguagemodelcompression. Duetothenumerous
differentbaselinesreportedineachwork,weplottherelativeperformancedropsforeachmethod.
Ontheright,wecomparetheperformancegapwithlatencyshowingthatwithanaveragedropof
≤1.75%wecanachievea1.25×speedupinthroughputpurelyfromstaticmodelsizereduction.
4.2 VISIONEXPERIMENTS
ExtendingtoImageClassificationTransitioningtheOPTINFrameworkfromthelanguagedomain
tothevisiondomain,wewererequiredtoincreasethenumberofprunablecomponents. Comparable
works have used a larger number of components including the pruning of Q-K-V layers in each
attentionhead,tokens&patches,andfinalembeddingsineachtransformerblock(Zhuetal.,2021;
Weietal.,2023;Panetal.,2021). Thusthestateoftheartinthefieldoftransformerpruningwidely
differsinthegranularityandconsistencyoftheprunedcomponents. Withapriorityonreducingreal-
worldinferencetime,weextendOPTINtoincludeavariantoftokenreduction;asimilaradaptation
wasmadeinCP-ViT(Songetal.,2022). Inparticular,wederiveamodifiedtrajectoryformulation
to rank tokens between each transformer block as described in Appendix A.9. By incorporating
thetrajectorymetricforlayerwise-tokenranking,theOPTINframeworkcandeducetheoptimal
numberoftokenstopreservebetweeneachtransformerblock,andcanthuscreateaninformative
tokenreductionschedulethatcanbeleveragedbyanytokenreductionmethod. Inparticular,we
wereinspiredbyarecentwork,ToMe(Bolyaetal.,2023)whichfeaturesanefficienttokenmerging
techniquebasedonbipartitematchingthatremovestokensbetweentransformerblockseitherata
constantorlinearlydecreasingschedule. WeincorporateToMeasamethodofmergingtokensbased
ontheoptimalnumberofreducedtokensperlayerdeterminedbytheOPTINframeworksearch.
Sinceourframeworkproducesthereductionschedule,wecanleveragethebenefitofbatchingasthe
numberoftokensperimagewillbeconstant,andthemethodsofmergingorreducingcanbeselected
byanyuser–weablatethebipartitematchingschemewitharandompruningschemeinAppendix
A.6andshowsimilarimprovementswhenusingtheOPTINFramework.
Image Classification Results In Tab. 3 we benchmark our proposed re-training free method on
theImageNet-1Kdatasetwiththebaseline,andSOTAresultstoshowcompetitiveperformanceat
givenFLOPsreductions. Weoffertwoconfigurations: β (base)denotesthebaseOPTINFramework
withouttheadditionalprunablecomponents(directlyshiftedfromthelanguagedomain),τ (expanded)
denotestheincorporationoftokenreductionintooursearchspace. Comparedwithmethodsthat
performre-training,theOPTINFrameworkproducescompetitiveperformance,particularlywitha
0.5%improvementata5%lowerFLOPswithrespecttoSAViT(Chuanyangetal.,2022). Comparing
with methods that have removed re-training, we note that VTP (Zhu et al., 2021) still includes
additionalsparsity-regularizationtraining,PoWER(Goyaletal.,2020)stillincludestheauxiliary
networktrainingwithsoft-extract,andHVT(Panetal.,2021)stillreducesFLOPsviare-trainingthe
architecturewithapoolingmethod. Howeverourmethodisconsideredafundamentallyone-shot
designanddespitelackingtheseadditionalpruningartifacts&components,isstillabletooutperform
thecurrentSoTA,withthebestresultonDeiT-SmalloutperformingCP-ViT(Songetal.,2022)by
0.4%ata∼10%higherFLOPsreduction. Tofurtherbenchmarkourperformanceinperspectiveof
awiderFLOPsspectrumandmoremodelcompressionmethods,weintroduceFig3whichincludes:
7PublishedasaconferencepaperatICLR2024
DeiTTiny DeiTSmall
Method
FLOPs(G) Acc(%) FLOPs(G) Acc(%)
Baseline 1.3 72.2 4.6 79.8
Re-Trained
SSP 0.99↓23.7% 68.59 3.15↓31.6% 77.74
S2ViTE 0.99↓23.7% 70.12 3.15↓31.6% 79.22
SAViT†† 0.98↓24.4% 70.72 - -
Not Re-Trained
VTP† 1.00↓21.7% 69.37 3.65↓20.7% 77.35
PoWER† 1.02↓20.3% 69.56 3.61↓21.5% 77.02
HVT† 1.01↓21.2% 68.43 3.66↓20.5% 76.72
CP-ViT† 1.00↓23.0% 71.06 3.64↓21.0% 78.84
OPTIN 1.1↓15.46% 67.51 4.11↓11.2% 77.01
β
OPTIN 0.91↓29.7% 71.25 3.15↓31.6% 79.24
τ
Table3:PruningImageNet-1K.Benchmarkingtheper-
formanceofOPTINusingDeiT-Tiny/Small. †methods Figure 3: DeiT-Ti FLOPs vs Accu-
racyBenchmarkingOPTIN overarange
arereproducedin(Songetal.,2022)withoutre-training. τ
†† DeiT-S result from (Chuanyang et al., 2022) is ex- of FLOP reductions on ImageNet-1K.
OPTINshowsstrongrobustnessovervari-
cludedasitperformssuperiortotheavailablebaseline.
ousFLOPconstraintswithoutre-training.
OPTINframeworkrunswithoutre-trainingproducing
boththeβ andτ configurations.
Method FLOPs(G) ±△Acc(%)
ViT-B 17.47 –
ImageNet-1K Transfer−→C-10
Model Method ToMe 11.50 ↓1.88
FLOPs(G) Acc(%) FLOPs(G) Acc(%)
OPTIN 11.45 ↓0.71
τ(∞)
Baseline 4.6 79.8 4.6 97.13
DeiT-S DeiT-B 17.6 –
OPTIN τ 3.52↓23.7% 79.01 2.30↓50.0% 96.60 Dyn-ViT† 11.81 ↓1.17
ViT-B Baseline 17.47 75.40 17.47 98.01 Top-K† 11.81 ↓0.94
OPTIN τ 13.33↓23.7% 72.98 8.77↓50.0% 97.82 EViT† 11.81 ↓0.86
Table 4: Transfer Learning on CIFAR Dataset. Bench- ToMe† 11.81 ↓0.80
marking the performance of OPTIN on the CIFAR-10 TPS†† 11.51 ↓0.71
OPTIN 11.75 ↓0.52
Datasets. Modelswerepre-trainedonImageNet-1K,pruned τ(∞)
throughtheOPTINFrameworkτ configuration,andtrans- Table 5: Token Reduction Bench-
ferred learned at a more aggressive pruning rate onto the marking OPTIN configuration.
τ(∞)
CIFAR-10(C-10)Dataset. †&††detailedinthemaintext.
X-Pruner(Yu&Xiang,2023),WDPruning(Yuetal.,2022a),S2VITE/SSP(Chenetal.,2021),SCOP
(Tangetal.,2020),HVT(Panetal.,2021),SAViT(Chuanyangetal.,2022),VTP(Zhuetal.,2021),
PoWER(Goyaletal.,2020),CP-ViT(Songetal.,2022)andUVC(Yuetal.,2022b). Despiteourlack
ofre-training,theOPTINframeworkproducescompetitiveresultsovervariousflopratios.
Forcompleteness,wechosetointroduceathirdconfigurationτ whichonlyappliesOPTINto
(∞)
creatingtokenreductionschedule,whileleveragingToMeformerging. Underthisconstraint,we
evaluateourmethodwithstate-of-the-arttokenreductionmethods:includingDynamicViT(Dyn-ViT)
(Rao et al., 2021), Top-K (Haurum et al., 2023), EViT (Liang et al., 2022) and TPS (Wei et al.,
2023)inTab5andshowsuperiorperformanceunderourframeworkwithoutre-training. †methods
followsetup&producedresultsin(Haurumetal.,2023),weconvertatokenpercentagetoFLOPs
reductiontobenchmarkourmethod.††estimatedfrom(Weietal.,2023). Wefurthercomplement
thiswithamoredetailedcomparisonagainsttheschedulesusingconstantandlinearlydecreasing
reductionschedulesinToMeoverawidevarietyofFLOPconstraintsinAppendixA.6. Ultimately
thisprovidesacompellingcaseforOPTIN’sabilitytoeffectivelydetermineaveragetokenimportance
intransformerarchitectures.
Transfer-LearningforImageClassificationTodemonstratethetransferabilityofourcompressed
models,weobtaintheprunednetworksfromImageNet-1KandapplytransferlearningtotheCIFAR-
10dataset. WechoosetoincludeDeiT-SandViT-Bformodelsizediversity. Benchmarkingagainst
baselinemodels,inTab4weshowsignificantrecoveryofperformancewhentransferringlearning
ontoCIFAR-10atextensiveFLOPsreductionratios. Althoughweshowre-trainingisnotnecessary
whenitcomestopruningonaspecificdataset&task,weevidentlyshowthatthemethodworkswell
underthetransferlearningparadigmfordifferentdownstreampurposes.
8PublishedasaconferencepaperatICLR2024
4.3 APPLICATIONS
WeexploredownstreamtasksandarchitecturesthatcansimilarlybenefitfromtheOPTINFramework.
Particularly,high-resolution(HR)semanticsegmentationisacompute-intensivetask,andweexplore
howOPTINmaintainscompetitiveperformanceandincreasesthroughputspeedsinTab3aandFig
3b. Toshowgeneralizability,weincludeasmallexperimentonCNNpruningin??.
Method FLOPs(M) Top-1(%) Epochs
Baseline 313.73 93.96 -
L1 206.00 93.40 -
HRank 131.17 93.73 200-300
CFDP 131.17 94.10 200-300
OPTIN 131.17 94.10 100-150
Table 6: Pruning on CNN. Benchmarking
the performance of OPTIN on the CIFAR-
10(Krizhevskyetal.,2009)datasetusingVGG-
16-BN.OPTINoutperformspreviousmodelcom-
pressiontechniques.
Method FLOPs↓ Params↓ mIoU(%)Latency(↓)
Baseline - - 78.81 -
OPTIN 24.2% 46.6% 74.57 13%
β
(a)Mask2Former(Swin-Ti):SemanticSegmentation. (b)High-ResolutionSegmentation(a)Baseline
BenchmarkingOPTIN ontheCityScapes(Cordtsetal., Mask2Former;(b)OPTINFrameworkataFLOPs
β
2016)FLOPs,Params.,andLatencyreductionismea- reduction of ∼ 25%. The minimal observable
suredrelativetotheSWINencoder. discrepancyisencircledinwhiterectangles.
Figure4: EvaluatedOPTIN onHR(1024x2048)Segmentation((a)Quantitative;(b)Qualitative).
β
Exploring Semantic Segmentaion To demonstrate the OPTIN framework’s generalizability to
complex architectures and downstream tasks, we apply model compression to the Mask2Former
ArchitecturewiththeSwin-TinybackboneontheCityscapesdataset.Wespecifytheselectedprunable
componentsinAppendixA.8. InTab. ??weshowimpressiveperformancedespiteroughlya24%
reductioninFLOPsand47%reductioninparametersoftheendocer.Qualitativelywecanseeastrong
resemblancebetweentheoriginalandcompressednetwork,withasmalldiscrepancyinpredictions
towardsthebottomrightoftheframeinanalreadydifficult-to-segmentregion(asevidencedbythe
unclearsegmentationintheoriginalprediction)andonthetrafficsigntowardsthetopleftinFig. 3b.
Exploring CNN Architectures To demonstrate the potential applications of OPTIN onto CNN
architectures, we extend our trajectory measure as described in Appendix A.10. In Tab. ?? we
comparethemodelcompressedthroughtheOPTINFrameworkwiththebaselineontheVGG-16-BN
architecture, aheuristicapproach(L1)(Lietal.,2017)andtwoleadingstate-of-ther-art: HRank
(Linetal.,2020a)andCFDP(Khaki&Luo,2023). Followingpreviousworks(Linetal.,2020a),
fine-tuninghasbeenshowntoberequiredpost-compression.HoweverasevidentinTab??,following
thesametrainingproceduresasHRank,wewereabletooutperformallmethodsatamuchfaster
convergencespeedgivencomparableFLOPsreductions.
5 CONCLUSION
In this work, we introduced OPTIN as a one-shot technique to enable efficient compression of
modernpre-trainedtransformerarchitectureswithoutre-training. OPTINexploitsthetrajectoryof
prunable components in the transformer architecture to enable a smarter criterion for parameter
selection. In this work, we’ve explored several domains including natural language processing,
imageclassification,transferlearningandsemanticsegmentationtasks. Weadditionallyshowhow
ourmethodcanworkinconcertwithexistingtokenreductionmodulestoproduceevenstronger
competitiveresultsintheimagedomain. Wefurtherexpandedourmethodtoshowrobustnesson
priorCNN-stylearchitecturesopeningfutureavenuesofresearchintofusedarchitectures. Inallcases,
we’veshownrobustnessagainstcompressionratesandcompetitiveperformanceincludingagainst
methodsthatperformre-training. Wecomplementourperformanceimprovementswithsynonymous
improvementsinthroughputspeedenablingthepracticaluseofourframework. Inthefuture,weplan
toexploremorecomplexarchitecturesandtasksinadditiontoexpandingthenumberofprunable
componentstofurtherthecauseinanefficientdesignoftransformermodels.
9PublishedasaconferencepaperatICLR2024
6 REPRODUCIBILITY STATEMENT
Theattachedsupplementalcodecontainsaframeworkwiththealgorithmsandmetricsbehindour
mainresults. AllofouradaptedLformulationsaredescribedinthemainpaper: Sec3orAppendix
A.9,A.10,andareimplementedinthesupplementalcode. Byourinnateone-shotstructure,thereare
notrainingaugmentationsappliedaswedon’tre-train. Theexceptionisfortransferlearningand
re-trainingontheCNNarchitectures. Forthese,weadoptthestandardaugmentationsfromHRank
(Linetal.,2020a). OurdatasetsandevaluationmetricsaredescribedinAppendixA.3,A.4.
10PublishedasaconferencepaperatICLR2024
REFERENCES
DanielBolya,Cheng-YangFu,XiaoliangDai,PeizhaoZhang,ChristophFeichtenhofer,andJudy
Hoffman. Token merging: Your vit but faster. In The Eleventh International Conference on
LearningRepresentations,2023. URLhttps://openreview.net/forum?id=JroZRaRw7Eu.
TianlongChen,YuCheng,ZheGan,LuYuan,LeiZhang,andZhangyangWang. Chasingsparsity
invisiontransformers: Anend-to-endexploration. AdvancesinNeuralInformationProcessing
Systems,34:19974–19988,2021.
BowenCheng,IshanMisra,AlexanderGSchwing,AlexanderKirillov,andRohitGirdhar. Masked-
attentionmasktransformerforuniversalimagesegmentation. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.1290–1299,2022.
Zheng Chuanyang, Zheyang Li, Kai Zhang, Zhi Yang, Wenming Tan, Jun Xiao, Ye Ren, and
ShiliangPu. SAVit: Structure-awarevisiontransformerpruningviacollaborativeoptimization.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=
w5DacXWzQ-Q.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson,UweFranke,StefanRoth,andBerntSchiele. Thecityscapesdatasetforsemanticurban
sceneunderstanding. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition(CVPR),June2016.
JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pp.248–255.Ieee,2009.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=YicbFdNTTy.
GongfanFang,XinyinMa,MingliSong,MichaelBiMi,andXinchaoWang. Depgraph: Towards
anystructuralpruning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pp.16091–16101,2023.
MohsenFayyaz,SoroushAbbasiKoohpayegani,FarnoushRezaeiJafari,SunandoSengupta,Hamid
RezaVaeziJoze,EricSommerlade,HamedPirsiavash,andJuergenGall. Adaptivetokensampling
forefficientvisiontransformers. InEuropeanConferenceonComputerVision(ECCV),2022.
SaurabhGoyal,AnamitraRoyChoudhury,SaurabhRaje,VenkatesanChakaravarthy,YogishSab-
harwal,andAshishVerma. Power-bert: Acceleratingbertinferenceviaprogressiveword-vector
elimination. InInternationalConferenceonMachineLearning,pp.3690–3699.PMLR,2020.
ZhiweiHao,JianyuanGuo,DingJia,KaiHan,YehuiTang,ChaoZhang,HanHu,andYunheWang.
Learningefficientvisiontransformersviafine-grainedmanifolddistillation. InAdvancesinNeural
InformationProcessingSystems,2022.
JoakimBruslundHaurum,SergioEscalera,GrahamW.Taylor,andThomasB.Moeslund. Which
tokens to use? investigating token reduction in vision transformers. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision(ICCV)Workshops,October2023.
GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeinaneuralnetwork,2015.
LuHou,ZhiqiHuang,LifengShang,XinJiang,XiaoChen,andQunLiu. Dynabert: Dynamicbert
withadaptivewidthanddepth. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin
(eds.),AdvancesinNeuralInformationProcessingSystems,volume33,pp.9782–9793.Curran
Associates,Inc.,2020. URLhttps://proceedings.neurips.cc/paper files/paper/2020/
file/6f5216f8d89b086c18298e043bfe48ed-Paper.pdf.
11PublishedasaconferencepaperatICLR2024
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. InProceedingsofnaacL-HLT,volume1,
pp. 2,2019.
Samir Khaki and Weihan Luo. Cfdp: Common frequency domain pruning. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)Workshops,pp.
4714–4723,June2023.
SehoonKim,AmirGholami,ZheweiYao,MichaelWMahoney,andKurtKeutzer. I-bert: Integer-
onlybertquantization. InInternationalconferenceonmachinelearning,pp.5506–5518.PMLR,
2021.
ZhenglunKong,PeiyanDong,XiaolongMa,XinMeng,WeiNiu,MengshuSun,XuanShen,Geng
Yuan, Bin Ren, Hao Tang, et al. Spvit: Enabling faster vision transformers via latency-aware
softtokenpruning. InComputerVision–ECCV2022: 17thEuropeanConference,TelAviv,Israel,
October23–27,2022,Proceedings,PartXI,pp.620–640.Springer,2022.
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
EldarKurtic,DanielCampos,TuanNguyen,EliasFrantar,MarkKurtz,BenjaminFineran,Michael
Goin,andDanAlistarh. TheoptimalBERTsurgeon: Scalableandaccuratesecond-orderpruning
for large language models. In Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing, pp. 4163–4181, Abu Dhabi, United Arab Emirates, December
2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.emnlp-main.279. URL
https://aclanthology.org/2022.emnlp-main.279.
Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir
Gholami. Afastpost-trainingpruningframeworkfortransformers. InAliceH.Oh,AlekhAgarwal,
DanielleBelgrave,andKyunghyunCho(eds.),AdvancesinNeuralInformationProcessingSystems,
2022. URLhttps://openreview.net/forum?id=0GRBKLBjJE.
Franc¸ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. Block pruning for faster
transformers. InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguage
Processing, pp. 10619–10629, Online and Punta Cana, Dominican Republic, November 2021.
AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.emnlp-main.829. URLhttps:
//aclanthology.org/2021.emnlp-main.829.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficientconvnets. InInternationalConferenceonLearningRepresentations,2017. URLhttps:
//openreview.net/forum?id=rJqFGTslg.
YouweiLiang,ChongjianGE,ZhanTong,YibingSong,JueWang,andPengtaoXie. EVit: Expe-
ditingvisiontransformersviatokenreorganizations. InInternationalConferenceonLearning
Representations,2022. URLhttps://openreview.net/forum?id=BjyvwnXXVn .
JiLin,JiamingTang,HaotianTang,ShangYang,XingyuDang,andSongHan. Awq: Activation-
awareweightquantizationforllmcompressionandacceleration. arXiv,2023.
MingbaoLin,RongrongJi,YanWang,YichenZhang,BaochangZhang,YonghongTian,andLing
Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pp.1529–1538,2020a.
ZiLin,JeremiahLiu,ZiYang,NanHua,andDanRoth. Pruningredundantmappingsintransformer
modelsviaspectral-normalizedidentityprior. InFindingsoftheAssociationforComputational
Linguistics: EMNLP2020,pp.719–730,Online,November2020b.AssociationforComputational
Linguistics. doi:10.18653/v1/2020.findings-emnlp.64. URLhttps://aclanthology.org/2020.
findings-emnlp.64.
ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.10012–10022,2021a.
12PublishedasaconferencepaperatICLR2024
ZejianLiu,FanrongLi,GangLi,andJianCheng. EBERT:EfficientBERTinferencewithdynamic
structuredpruning. InFindingsoftheAssociationforComputationalLinguistics: ACL-IJCNLP
2021,pp.4814–4823,Online,August2021b.AssociationforComputationalLinguistics. doi: 10.
18653/v1/2021.findings-acl.425. URLhttps://aclanthology.org/2021.findings-acl.425.
Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large
languagemodels. InAdvancesinNeuralInformationProcessingSystems,2023.
PaulMichel,OmerLevy,andGrahamNeubig. Aresixteenheadsreallybetterthanone? Advancesin
neuralinformationprocessingsystems,32,2019.
ZizhengPan,BohanZhuang,JingLiu,HaoyuHe,andJianfeiCai. Scalablevisiontransformerswith
hierarchicalpooling. InProceedingsoftheIEEE/cvfinternationalconferenceoncomputervision,
pp.377–386,2021.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDesmaison,AndreasKopf,Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox,
andR.Garnett(eds.),AdvancesinNeuralInformationProcessingSystems,volume32.Curran
Associates,Inc.,2019. URLhttps://proceedings.neurips.cc/paper files/paper/2019/
file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.
Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, and
Zhaoning Zhang. Correlation congruence for knowledge distillation. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pp.5007–5016,2019.
YongmingRao,WenliangZhao,BenlinLiu,JiwenLu,JieZhou,andCho-JuiHsieh. Dynamicvit:
Efficientvisiontransformerswithdynamictokensparsification. InA.Beygelzimer,Y.Dauphin,
P.Liang,andJ.WortmanVaughan(eds.),AdvancesinNeuralInformationProcessingSystems,
2021. URLhttps://openreview.net/forum?id=jB0Nlbwlybm.
AhmadSajedi,SamirKhaki,EhsanAmjadian,LucyZLiu,YuriALawryshyn,andKonstantinosN
Plataniotis. Datadam: Efficientdatasetdistillationwithattentionmatching. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pp.17097–17107,2023a.
AhmadSajedi,SamirKhaki,KonstantinosNPlataniotis,andMahdiSHosseini. End-to-endsuper-
visedmultilabelcontrastivelearning. arXivpreprintarXiv:2307.03967,2023b.
Ahmad Sajedi, Samir Khaki, Yuri A Lawryshyn, and Konstantinos N Plataniotis. Probmcl:
Simple probabilistic contrastive learning for multi-label visual classification. arXiv preprint
arXiv:2401.01448,2024.
Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by
fine-tuning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
vancesinNeuralInformationProcessingSystems,volume33,pp.20378–20389.CurranAsso-
ciates,Inc.,2020. URLhttps://proceedings.neurips.cc/paper files/paper/2020/file/
eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf.
DachuanShi,ChaofanTao,YingJin,ZhendongYang,ChunYuan,andJiaqiWang. UPop: Unified
andprogressivepruningforcompressingvision-languagetransformers. InProceedingsofthe40th
InternationalConferenceonMachineLearning,volume202,pp.31292–31311.PMLR,2023.
KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scaleimage
recognition. arXivpreprintarXiv:1409.1556,2014.
ZhuoranSong,YihongXu,ZhezhiHe,LiJiang,NaifengJing,andXiaoyaoLiang. Cp-vit: Cascade
visiontransformerpruningviaprogressivesparsityprediction,2022.
YehuiTang,YunheWang,YixingXu,DachengTao,ChunjingXu,ChaoXu,andChangXu. Scop:
Scientificcontrolforreliableneuralnetworkpruning. AdvancesinNeuralInformationProcessing
Systems,33:10936–10947,2020.
13PublishedasaconferencepaperatICLR2024
HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHerve´
Je´gou. Trainingdata-efficientimagetransformers&distillationthroughattention. InInternational
conferenceonmachinelearning,pp.10347–10357.PMLR,2021.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE:Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding. In
InternationalConferenceonLearningRepresentations,2019. URLhttps://openreview.net/
forum?id=rJ4km2R5t7.
HanruiWang,ZhanghaoWu,ZhijianLiu,HanCai,LigengZhu,ChuangGan,andSongHan. HAT:
Hardware-awaretransformersforefficientnaturallanguageprocessing. InProceedingsofthe58th
AnnualMeetingoftheAssociationforComputationalLinguistics,pp.7675–7688,Online,July
2020a.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.acl-main.686. URL
https://aclanthology.org/2020.acl-main.686.
Yi Ru Wang, Samir Khaki, Weihang Zheng, Mahdi S Hosseini, and Konstantinos N Plataniotis.
Conetv2: Efficient auto-channel size optimization for cnns. In 2021 20th IEEE International
ConferenceonMachineLearningandApplications(ICMLA),pp.998–1003.IEEE,2021.
ZihengWang,JeremyWohlwend,andTaoLei. Structuredpruningoflargelanguagemodels. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP).AssociationforComputationalLinguistics,2020b. doi: 10.18653/v1/2020.emnlp-main.
496. URLhttps://doi.org/10.18653%2Fv1%2F2020.emnlp-main.496.
SiyuanWei,TianzhuYe,ShenZhang,YaoTang,andJiajunLiang. Jointtokenpruningandsqueezing
towardsmoreaggressivecompressionofvisiontransformers. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.2092–2101,2023.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL
https://aclanthology.org/2020.emnlp-demos.6.
MengzhouXia,ZexuanZhong,andDanqiChen. Structuredpruninglearnscompactandaccurate
models. InAssociationforComputationalLinguistics(ACL),2022.
GuangxuanXiao,JiLin,MickaelSeznec,HaoWu,JulienDemouth,andSongHan. SmoothQuant:
Accurateandefficientpost-trainingquantizationforlargelanguagemodels. InAndreasKrause,
Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett
(eds.),Proceedingsofthe40thInternationalConferenceonMachineLearning,volume202of
ProceedingsofMachineLearningResearch, pp.38087–38099.PMLR,23–29Jul2023. URL
https://proceedings.mlr.press/v202/xiao23c.html.
HuanruiYang,HongxuYin,MayingShen,PavloMolchanov,HaiLi,andJanKautz. Globalvision
transformerpruningwithhessian-awaresaliency. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.18547–18557,2023.
Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui. Width & depth pruning
for vision transformers. In AAAI Conference on Artificial Intelligence, 2022a. URL https:
//api.semanticscholar.org/CorpusID:250294994.
LuYuandWeiXiang. X-pruner: explainablepruningforvisiontransformers. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.24355–24363,2023.
14PublishedasaconferencepaperatICLR2024
ShixingYu,TianlongChen,JiayiShen,HuanYuan,JianchaoTan,SenYang,JiLiu,andZhangyang
Wang. Unifiedvisualtransformercompression. InInternationalConferenceonLearningRepre-
sentations,2022b. URLhttps://openreview.net/forum?id=9jsZiUgkCZP.
QingruZhang,SimiaoZuo,ChenLiang,AlexanderBukharin,PengchengHe,WeizhuChen,and
Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight
importance. InInternationalConferenceonMachineLearning,pp.26809–26823.PMLR,2022.
BoruiZhao,QuanCui,RenjieSong,YiyuQiu,andJiajunLiang. Decoupledknowledgedistillation,
2022.
MingjianZhu,YehuiTang,andKaiHan. Visiontransformerpruning,2021.
15PublishedasaconferencepaperatICLR2024
A APPENDIX
Theappendixisstructuredtoprovidedetailsthatmatcheselicitationfromthemaintext. Experiments
and discussions included in the appendix serve as supplemental information to provide a greater
contexttoclaimsandexperimentsstatedinthemaintext.
A.1 DISCUSSINGTHEOPTINALGORITHIM
Algorithm1demonstratesthebasestructureforcomputingandassigningimportancetoeachofour
printableparameters. Onceourimportancescoreswerecomputedwedirectlyleveragedthemask
searchalgorithmfromPTF(Kwonetal.,2022)asitsearchestomaximizescoresinthepartitioned
searchspace. Theirpolicypartitionsthesearchspacebyincrementallyaddingattentionheadsin
orderofimportance,andateachstep,addingthemaximumnumberofrank-orderedneuronsthatwill
satisfythecostconstraintC([θ′,θ′,···θ′])≤C. Bycomputingthecumulativeimportanceateach
0 1 n
step,weeasilydeducethatthestepwiththemaximumcumulativeimportancemustbeoptimal,as
anyotherconfigurationwouldyieldacumulativescorelessthanorequaltothatofthebest.
A.2 DETAILSONHYPERPARAMTERλ
InTab1d,weusetheλsweeptoexpressrelativemagnitudedifferencesbetweenL andL .
MD KD
Basedonthereportedresults,wecanconcludethatalargerimportanceshouldbeweighedonthe
distillationloss,inparticular,weexpectthat⌊log (L )⌋∼{10,100}∗⌊log (L )⌋(i.e. the
10 MD 10 KD
orderofmagnitudeofL shouldbe10-100timeslargerthanthatofL )
MD KD
A.3 DETAILSONDATASETCHOICE
Inthiswork,weevaluatedthestrengthoftheOPTINframeworkonnaturallanguageprocessing
benchmarks,imageclassificationandsemanticsegmentation.Acrossthiswidevarietyofdomains,we
useddifferentdatasetsforthevarioustasksfollowingfrompreviousworkstobolstertheperformance
ofourmethod.
GLUEBenchmarksTheGeneralLanguageUnderstandingEvaluation(GLUE)benchmark(Wang
etal.,2019)containsavarietyofNLP-relatedtasksofwhichweincluded: SimilarityandParaphrase
Tasks(MRPCSTS-B,QQP)andInferenceTasks(MNLI,QNLI).Thedatasetdistributionwidely
varypertask. Fromtheincludedselection,STS-Bisaregressiontask,MNLIhasthreeclasses,and
theremainingtasksincludetwoclasses.
ImageNet1KBenchmarksTheImageNet-1Kdataset(Dengetal.,2009)isawidelyusedbenchmark
forimageclassification,ascitedinseveralworks(Zhuetal.,2021;Goyaletal.,2020;Panetal.,
2021).Itcontains1.2milliontrainingimagesand50Kvalidationimagesacrossthe1000classes.Due
toitsdifficulty,stemmingfromthenumberofclassesandimages,itpresentsaperfectbenchmarking
mediumforourone-shotpruningapproach.
CIFAR10TheCIFAR-10(Krizhevskyetal.,2009)datasetsareacommonbenchmarkacrossboth
convolutionalneuralnetworkpruningandmodelcompressionworks(Khaki&Luo,2023;Linetal.,
2020a;Wangetal.,2021). CIFAR10contains50Ktrainingand10Kvalidationimagesspreadover
10classes. Duetotheprominentuseofthisdatasetinbenchmarkingtasks,wedecidedtobenchmark
ourmethodforfaircomparisonwithSOTA.
CityscapesTheCityscapesdataset(Cordtsetal.,2016)isheavilyusedforsemanticsegmentation
tasks,andinparticular,containshigh-resolutionimagesof(1024x2048),withroughly3Ktrainingand
500validationimages. Inthispaper,ourgoalwastodemonstratetheeffectsofpruningadownstream
networkundertheOPTINframework,andduetothehighresolutionofCityscapesdata,wewere
abletodemonstrateimprovementsinthroughputspeedforourprunedsegmentationmodel.
A.4 DETAILSONEVALUATIONMETRICCHOICE
ThetwomainmetricsreportedinthisworkareFLOP(s)andAccuracy(orequivalentlymIOUin
the case of segmentation). Given the target domain of this paper, these metrics best express the
tradeoffbetweenhighaccuracyandcomputationalcomplexity,andfurtherhowtheOPTINframework
16PublishedasaconferencepaperatICLR2024
isbetterabletomakethisdistinction. Theselectedmetricshavefurtherbeenreportedinsimilar
previousworks(Kwonetal.,2022;Bolyaetal.,2023;Weietal.,2023). Finally,wealsoreportim/s
throughputtobetterillustratethereal-worldimplicationsoftheOPTINFramework,especiallyin
resourceortime-constrainedenvironments.
A.5 AVERAGETIMEANALYSIS
Task Dataset Model Avg.PruningTime(Hours)
NaturalLanguage GLUEBenchmark BERT 0.4
ImageClassification ImageNet-1K DeiTTiny 0.3
ImageClassification ImageNet-1K DeiTSmall 0.3
SemanticSegmentation Cityscapes Mask2Former(Swin-Ti) 0.5
(a)AblatingtheTokenMergingAlgorithm. Wedemonstratethepracticalityofourmethodasitscaleswell
acrossthetasksandmodelarchitectures, achievingcompetitiveresultsonthescaleofminutes; thusfurther
bolsteringthemotivationandimplicationsoftheOPTINFramework.
A.6 ABLATIVEEXPERIMENTS
Dataset BatchSize Acc.
MNLI 16 82.12
MNLI 32 81.90
MNLI 64 81.73
MNLI 128 82.20
ImageNet 16 70.53
ImageNet 32 71.25
ImageNet 64 71.01
ImageNet 128 70.82
(a)EffectofBatchSize.Thecompu- (b) Ablating the Token Reduction Schedule. On ImageNet-1K
tationofIappearstobemorerobust for DeiT-S, our OPTIN Framework produced a more optimal
τ(∞)
tochangesinthebatch-size. reductionschemeasopposedtothedefaultinToMe(Bolyaetal.,
2023)
TokenMergingAlg. Scheduler ±△Acc(%) FLOPs(G)
Baseline(N/A) - - 17.6
Random Prune default ↓8.47 11.81
Random Prune OPTIN ↓7.11 11.75
τ(∞)
bipartite merge default ↓0.80 11.81
bipartite merge OPTIN ↓0.52 11.75
τ(∞)
(c) Ablating the Token Merging Algorithm. On ImageNet-1K for DeiT-B, the bipartite matching default
configuration from ToMe was used. However based on the results, it is evident that OPTIN improves the
performanceofarbitrarytokenpruningmethodsaswell.ConfigurationAblated:OPTIN
τ(∞)
Table8: AdditionalExperimentsHereweevaluatethreecomponents: (a)theablativeeffectof
batchsizeincomputingthedistillationloss,(b)acomparisonoftheoptimalreductionschedulefrom
OPTIN comparedtotheconstantanddecreasingschedulesfromToMe. (c)theeffectofdifferent
τ(∞)
patchreduction/mergingtechniquesusingtheOPTINFramework. Ourdefaultsettingsaremarkedin
green.
17PublishedasaconferencepaperatICLR2024
A.7 ADDITIONALLANGUAGEEXPERIMENTS
Method MNLI QQP QNLI SST STS-B MRPC
BERT 84.53 91.00 91.41 93.57 88.90 86.27
BASE
PTF 81.21 89.99 88.38 92.13 87.10 83.14
OPTIN‡ 82.12 90.08 88.54 92.36 87.19 85.21
λc=0.01
PTF†† 82.51 90.35 90.06 92.49 88.00 85.27
OPTIN‡‡ 82.74 90.43 90.35 92.73 88.21 85.68
Table9: NaturalLanguageBenchmarks. Augmentsthemaintable9withtwoadditionalexper-
iments. OPTIN‡ runs the OPTIN algorithm with λ = 0.01 to display the best results we
λc=0.01 c
achievedusingourstandardframework. Further,OPTIN‡‡ compareswithPTF†† whichincludes
themasktuning/scalingfromPTF(Kwonetal.,2022)todiscoveranon-binarymaskthathelpsto
reducein-placereconstructionerrors. LatencyisestimatedatB =32andrangesbetween1.35-1.38
×improvement.‡resultsareaveragedover5differentseeds.
Task Attention Heads Hidden Neurons Patches&Tokens Output Channels
NaturalLanguage ✓ ✓ - -
Processing
ImageClassification(CNN) - - - ✓
ImageClassification(TF) ✓ ✓ ✓ -
SemanticSegmentation - ✓ - -
Table 10: Identifying the prunable weights that OPTIN uses to accelerate the model for various
downstreamtasks
A.8 EXTENDINGOPTINTOVARIOUSDOWNSTREAMTASKS
Whenmovingfromthelanguagedomaintootherapplications,competitivemethodsleveragead-
ditional pruning components in order to spread the compression over a larger search space. In
response,wetooapplythiswithOPTIN.Tab10identifiesthesearchspaceusedinOPTINforvarious
downstreamtasks.
A.9 ADAPTINGTHETRAJECTORYFORMULATIONTOTOKENPATCHINFORMATIVENESS
AsdetailedinSec4.2,theOPTINframeworkallowsuserstoselectthebesttokenreductiontechnique
fortheirtasktocreateanexpandedprunablesearchspace. TheOPTINframeworkadaptstoimage
datasetsbyfurtherproducinganoptimalreductionschedulefortokensthatcanbeleveragedbyany
reductionormergingtechnique. Inthemainpaper,weuseToMewithbipartitematching,however,
weablatethemetricchoiceandmergingstrategyinAppendixA.6.
Inordertoobtaintheoptimaltokenreduction,weapplythetrajectoryestimationtopatchesinthe
vison-transformer models, by simply modifying the reshaping operator and the dimension upon
whichwecomputetheimportance. Weadopttheinter-samplerepresentation(Haoetal.,2022)and
sincewearedeterminingpatch-levelimportance,itfollowsthatweshouldcompareourbaseand
prunedembeddingsalongsaiddimension. Wenotethattheuseofthetermpatchesinthecontextof
vision-transformerswouldberepresentedbythesamedimensionasthetokensequencelengthinthe
languagedomain. Weredefinethemanifolddistillationlossaccordingtotheindexj whichrangesup
tothenumberofpatchesfortheco-respondingmodel. Webeginbyredefiningthemanifoldstructure
relationalmapbasedonindexj whereF ∈RB×1×D as:
i,[:,j,:]
M(F )=(F )(F )T
i,[:,j,:] i,[:,j,:] i,[:,j,:]
Inparticular,wemodifytheLinter-imagepatchdistillationlossfrom(Haoetal.,2022)byreplacing
thestudentinputwiththatofthemaskedpatch,andtheteacherinputwiththeprecomutedembeddings
fromthenetwork. Fortwogivenfeatureembeddingsfromlayeriforabaseandprunedmodel,the
18PublishedasaconferencepaperatICLR2024
samplemanifoldreconstructionerrorwouldpresentas:
T
L (F′ ,F )= 1 (cid:88) ||M(F′ )−M(F )||2
MD i i T i,[:,j,:] i,[:,j,:] F
j=0
ThiserrorisaccumulatedwithstandardKLDivergenceresultinginasimilarEquation3.
Afterdeterminingtheimportancescoreforeachpatch,wecanderivetheaveragenumberofpatches
requiredperlayerformaximuminformationthroughputbysimplerankeliminationgivenanFLOPs
constraint. After executing the mask search with attention heads and neurons, we alleviate the
remainingFLOPreductionrequiredbyremovingtokensinorderofascendingimportance(i.eremove
thelowestimportancefirst). Thisultimatelyproducesthenumberoftokensperlayerwhichcanbe
extractedasatokenreductionschedulethatcanbeleveragedonrun-timewiththeToMebipartite
matchingscheme. WehavefurthershownthattheOPTINFrameworkreductionschemeismuch
moreinformedthanthestandardconstantordecreasingschemescommonlyused–SeeAppendix
A.6.
A.10 ADAPTINGTHETRAJECTORYFORMULATIONTOOUTPUTCHANNELS
ToadaptthetrajectoryestimationtooutputchannelsinCNN-stylenetworks,wereducetherelation
toasimplemean-squareerrorcomputedbetweenthefeatureembeddingsalongthelengthofthe
model. Inparticular,weaveragetheembeddingsalongthebatchdimensionandcomputethesumof
themeansquarederrorbetweenthebaseandprunedmodelalongeachlayerdeeperinthenetwork:
B B
L (F′ ,F )= 1 (cid:88) ||(cid:88) F′ −(cid:88) F ||2
MD i i B i i F
Once again, we are able to plug this into Equation 3 with standard KL Divergence to determine
overallchannelimportance.
19