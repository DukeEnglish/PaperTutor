Counterfactual Fairness through Transforming Data
Orthogonal to Bias
ShuyiChen ShixiangZhu
shuyic@alumni.cmu.edu shixianz@andrew.cmu.edu
CarnegieMellonUniversity CarnegieMellonUniversity
Pittsburgh,Pennsylvania,USA Pittsburgh,Pennsylvania,USA
ABSTRACT Fairly hired Obs hired Unfairly hired
Machinelearningmodelshaveshownexceptionalprowessinsolv- Fairly not hired Obs not hired Unfairly not hired
ingcomplexissuesacrossvariousdomains.Nonetheless,thesemod-
elscansometimesexhibitbiaseddecision-making,leadingtodis-
paritiesintreatmentacrossdifferentgroups.Despitetheextensive
researchonfairness,thenuancedeffectsofmultivariateandcon-
tinuoussensitivevariablesondecision-makingoutcomesremain
insufficientlystudied.Weintroduceanoveldatapre-processingal-
gorithm,OrthogonaltoBias(OB),designedtoremovetheinfluenceof
agroupofcontinuoussensitivevariables,therebyfacilitatingcoun-
terfactualfairnessinmachinelearningapplications.Ourapproach Female Male Socio-economic status
is grounded in the assumption of a jointly normal distribution
Figure1:Thismotivatingexampleillustratesthedistinction
withinastructuralcausalmodel(SCM),provingthatcounterfactual
betweenbinaryandcontinuoussensitivevariablesinthe
fairnesscanbeachievedbyensuringthedataisuncorrelatedwith
contextofhiringdecisions.Thedashedlinesindicatethe
sensitivevariables.TheOBalgorithmismodel-agnostic,cateringto
predictedhiringdecisionsandtheshadedareaindicatethe
awidearrayofmachinelearningmodelsandtasks,andincludesa
unbiasedtruedecisions.Theleftpanelsimplifiesthesce-
sparsevarianttoenhancenumericalstabilitythroughregulariza-
nariowithabinarysensitivevariable,suchasGender,where
tion.Throughempiricalevaluationonsimulatedandreal-world
adjustmentsforfairnessaremorestraightforwardduetothe
datasets—includingtheadultincomeandtheCOMPASrecidivism
cleardichotomyindata.Therightpanel,however,delves
datasets—our methodology demonstrates its capacity to enable
intothecomplexityintroducedbyacontinuoussensitive
faireroutcomeswithoutcompromisingaccuracy.
variable,likesocio-economicstatus,demonstratingtheintri-
catetaskofachievingfairnessacrossaspectrum.
KEYWORDS
Counterfactualfairness,Datapre-processing,AlgorithmicFairness (orprotected)variableofanindividualweredifferent,allelsebe-
ingequal.Thisconceptisparticularlypowerfulasitalignsclosely
withintuitivenotionsofindividualfairnessandjustice,offeringa
1 INTRODUCTION rigorousstandardagainstwhichtomeasureandrectifybias.
Inrecentyears,machinelearninghasemergedasapivotaltechnol- Numerous previous research efforts have focused on attain-
ogy,drivingadvancementsacrossabroadspectrumofreal-worldap- ingcounterfactualfairness,makingsignificantstridesinthisarea
plications,fromhealthcarediagnostics[16],hiringdecision-making [11,19,20,24,30].Akeyexampleisthestudyby[30],whichtargets
systems [12], to loan assessments [26]. Its ability to learn from non-discriminationthroughadjustingthepredictionsbasedonthe
and make predictions or decisions based on large data sets has empiricaljointdistributionofthedata,aimingtoachieveequalbias
beentransformativeinaddressingcomplexproblems.However, andaccuracyacrossdifferentgroups.However,theseapproaches
thebroaderapplicationprospectsofsomemachinelearningtech- typicallypresupposeacompleteunderstandingofthecausalre-
niquesishamperedbytheimplicitbiasesingrainedinthedatait lationshipsamongallvariableswithoutunmeasuredconfounders
learnsfrom,leadingtooutcomesthatsystematicallyandunfairly andconsidersensitivevariablestobebinary(orcategorical),so
disadvantagecertaingroups[4,9,19,23].Thisissueofbiasinma- thattheycanbeeasilyisolatedoradjusted.Asaresult,traditional
chinelearningmodelsisnotmerelyatechnicalchallengebuta methodsoffairlearningfacechallengesinaddressingsituations
fundamental concern that threatens to reinforce existing social involvingmultivariateandcontinuoussensitivevariablewithintri-
inequalities. cateinter-dependency.
Suchfairnessconcerninmachinelearninghascatalyzedagrow- Taketheemploymenthiringprocessasaninstanceshownin
ingbodyofresearchaimedatidentifying,understanding,andmiti- Figure1,socioeconomicstatuscan’tbeeasilyclassifiedintoasmall
gatingbiasespresentindataandalgorithms.Amongthevarious numberofdiscretecategories.Itisacomplex,multidimensional
conceptualframeworksdevelopedtoaddressthisissue[9,15,18, variableinfluencedbyeducation,income,occupation,andevensub-
20, 34, 35], the notion of counterfactual fairness [24] stands out. tlerfactorslikeneighborhoodandparentaleducation.Acandidate’s
Counterfactualfairnessseekstoensurethatadecisionmadeby profileisacomplexamalgamationoftheirexperiences,skills,edu-
amachinelearningmodelwouldremainunchangedifasensitive cationalbackground,andpersonalcharacteristics.Amodeltrained
4202
raM
62
]GL.sc[
1v25871.3042:viXraonhistoricalhiringdatamightinadvertentlylearntofavorcandi- quantifyingfairness.Researcherscommonlyadopteitherobserva-
datesfromprestigiousuniversitiesorwithcertaintypesofwork tionalorcounterfactualapproachestoformalizefairness.Obser-
experiences—criteriathatareoftencorrelatedwithsocioeconomic vationalmethodstypicallycharacterizefairnessthroughmetrics
status.Thisformofimplicitbiasemergeseitherbecausethedata derivedfromobserveddataandpredictedoutcomes[19,21,25,31].
usedfortraininglackscomprehensiverepresentationofallpossible Metricssuchasindividualfairness(IF)[15],demographicparityor
candidateprofilesorbecausetraditionalapproachestofairnessdo GroupFairness[22,35]andequalizedodds[20]fallunderthiscate-
notadequatelyaddressthesubtleinterplayandimpactofvarious gory.Thekeyideafortheobservationalfairnessmetricisviewing
sensitivevariablesondecision-makingoutcomes. fairnessastreatingsimilarindividualsorindividualsbelongingto
Totacklethesechallenges,wedevelopanoveldatapre-processing thesamegroupssimilarly.Forexample,IFdefinesfairnessastreat-
approachthataimstoremovetheinfluenceofagroupofcontinuous inganytwoindividualswhoaresimilarwithrespecttoaparticular
sensitivevariablesfromthedata,therebyensuringcounterfactual tasksimilarly[35].
fairnessinsubsequentmachinelearningtasks.Wefirstprovethat In contrast, counterfactual approaches proposes a causal ap-
thecounterfactualfairnesscanbeattainableeasilybymakingthe proachtodefiningfairness.Thesedefinitionsassessfairnessbased
datauncorrelatedwiththegroupofsensitivevariables,basedon onhowpredictionswouldchangeifsensitiveattributeswereal-
theassumptionofajointlynormaldistributionwithinastructural tered[11,19,20,24,30].Withthehelpofthepotentialoutcome
causalmodel(SCM)framework[29].Thisassumptionisapplicable concept,themeasuringoffairnessisnolongerrestrictedtothe
acrossabroadspectrumofapplicationswherestandardizationof observablequantities.Forinstance,theEqualOpportunity(EO)
dataisrequired,ensuringthatallvariables,bothsensitiveandnon- definition,akintoindividualfairness,directlycomparestheactual
sensitive,arenormalizedtohaveameanofzeroandavarianceof1. andcounterfactualdecisionsofthesameindividual,ratherthan
Motivatedbythisunderstanding,weconsiderallsensitivevariables relyingoncomparisonsbetweentheobservationoftwosimilar
collectivelyandproposeadatapre-processingalgorithm,referred individuals[30].
toasOrthogonaltoBias(OB).Thisalgorithmisdesignedforminimal Whileobservationaldefinitionsoffairnesscanbeincorporated
dataadjustmentstoachieveorthogonalitybetweennon-sensitive intooptimizationproblems,eitherbytreatingthefairnesscondi-
andsensitivedata.Tofacilitatenumericalstability,wealsopresent tionasaconstraint[15]ordirectlyoptimizingthefairnessmetric
asparsevariantofthisalgorithmwhichincorporatesaregular- asanobjectivefunction[35],achievingcounterfactualdefinitions
izationterm.Thentheresultingdataisreadytoserveasinput offairnessoftenrequireanapproximationofthecausalmodelor
formachinelearningmodelsindownstreamtaskswithoutbeing thecounterfactualssincethecounterfactualsareunobservable.For
influencedbytheundesirablebiasassociatedwiththecomplexities example,intheFairLearningalgorithmproposedby[24],theunob-
ofsensitivevariables.Itisalsoimportanttonotethatourproposed servedpartsofthegraphicalcausalmodelaresampledthroughthe
algorithmismodel-agnostic,makingitsuitableforavarietyofma- MarkovchainMonteCarlomethod.Thentheyuseonlythenon-
chinelearningmodelsandtasks.Lastly,weevaluateouralgorithm’s descendantsofsensitivevariabletomakethedecision.However,
performanceonasimulateddatasetandtworeal-worlddatasets, thisapproachmaysacrificeasignificantamountofinformationin
includingtheadultincomedataandtheCOMPASrecidivismdata, thedataandleadtolowerpredictionaccuracy.Moreover,whenesti-
demonstratingthatourapproachenablesmachinelearningmodels matingthecounterfactualdistributionisnotdirectlyfeasibledueto
toachievefaireroutcomeswithcomparableaccuracytocurrent sparseorcontinuoussensitivevariables,unexpectedrelationships
state-of-the-artfairlearningmethods.Inthenumericalresults,we betweensensitiveandnon-sensitiveattributesmaypersist.This
foundthatourapproachisnotlimitedbytheassumptionsabout caneithercompromisefairnessbyconsideringvariablesrelated
datadistribution,indicatingitsapplicabilitytoawiderrangeof withsensitivevariables,orcompromisetheaccuracybyremoving
scenarios. allrelatedinformationregardingsensitivevariablesfromthetrain-
Ourcontributionsinthisworkcanbesummarizedasfollows: ingdata.Aswediscusslater,ouraimistominimizedatachanges
(1) We show that achieving counterfactual fairness is feasi- whileensuringcounterfactualfairnessundercertainassumptions,
blebyensuringorthogonalitybetweennon-sensitiveand therebybetteraddressingthetrade-offbetweenaccuracyandfair-
sensitivedatawhentheyarejointlynormal. ness.
(2) Weintroduceamodel-agnosticdata-pre-processingalgo-
rithm,termedasOrthogonaltoBias(OB),whichfacilitates
counterfactualfairnessacrossabroadspectrumofdown- Fairlearningapproaches. Fairnesslearninginmachinelearning
streammachinelearningapplications. aimstopreventdiscriminationandcanbegenerallycategorized
(3) Wevalidatetheenhancedefficacyofouralgorithmcom- intothreestages.Firstly,pre-processingapproaches[10,13,19],
paredtotheexistingstate-of-the-artsthroughevaluations whicharemostcloselyrelatedtoourwork,involvemodifyingthe
onbothsyntheticandreal-worlddatasets. datatoeliminateorneutralizeanypreexistingbias,followedbythe
applicationofstandardMLtechniques.Secondly,in-processingap-
proaches[2,28]eitherbyafairnessregularizertothelossfunction
2 LITERATUREREVIEW
objective,whichpenalizesdiscrimination,orimposingaconstraint,
Thisworkisrelatedtoseveralstreamsofalgorithmicfairnessliter-
therebymitigatingdisparatetreatment.Suchmethodsarenormally
aturewhichwereviewinthissection.
model-spefiic.Thethirdtypeofapproachispost-processingap-
FairnessinMachineLearning. Thepursuitoffairdecision-making proachesadjustpredictorslearnedusingstandardMLtechniques
inmachinelearninghasledtodiverseapproachesfordefiningand afterthefacttoenhancetheirfairnessproperties[7,14,17,20].
2𝑈 𝐵 𝐴 𝑈 𝑈 𝐵 𝐴 𝑈
$ ! $ !
𝑨!⊥𝑩
𝐴& mi
n
A fair predictor 𝑌$ needs 𝑨 Orthogonal to Bias
to take into account the 𝑌 is implicitly affected −
influence of 𝐵 by 𝐵 through 𝐴 𝑨!
𝑈 𝑌$ 𝑌 𝑈 𝑈 𝑌$ 𝑌 𝑈
"# " "# "
(a)TheSCMandthetypicalfairlearning (b)Fairlearningbytransformingdataorthogonaltobias
Figure2:Illustrationof(a)thestructuralcausalmodel(SCM)andacommonfairlearningstrategies,aswellas(b)theproposed
datapre-processingalgorithmOrthogonaltoBias(OB).Thewhitenodes𝐴,𝑌,and𝑌ˆ
arethenon-sensitivevariables,thedecision
variable,anditsprediction,respectively.Therednodes𝐵representthesensitivevariable.The𝐴ˆ
isthetransformeddatathatis
orthogonaltobiasin𝐵.Thegraynodesrepresentexogenousvariables.
Ourapproachismostrelatedtotheworkby[19],wherethe eachcomponentin𝑉,detailedasfollows:
authorsproposetwodistributionadjustmentprocedures,Orthogo-
𝐵=𝑓 𝐵(𝑈 𝐵),
nalizationandMarginalDistributionMapping,formakingcounter-
factuallyfairdecisionsbasedonadjusteddata.Whilebothproce-
𝐴=𝑓 𝐴(𝐵,𝑈 𝐴), (1)
duresremoveattributes’dependenceonsensitivevariablesunder 𝑌 =𝑓 𝑌 (𝐴,𝑈 𝑌).
respectiveconditions,theirmethodsprovidenoguaranteeregard- AccordingtotheaboveSCM,thebiaspresentinthesensitivevari-
ingthescaleofmodificationtothedatadistribution.Incontrast, ables𝐵cantransmittothepredictor𝑌ˆ viathenon-sensitivevari-
ourworkintroducesanexactapproachtosolvinganoptimization ables𝐴.Thismeansthat,ifthereareanydifferencesinthedistri-
problemthatguaranteesminimalmodificationtothedatawhile butionof𝐴conditioningon𝐵,thedecisionvariable𝑌ˆ basedon𝐴
ensuringcounterfactualfairnessunderspecificassumptions.We mightbeunfair.
believethatouremphasisonminimaldatamodificationplacesour Inthispaper,weaimtodesignapredictor𝑌ˆ thatachievesthe
proposedalgorithminauniquepositioninthewidelyobserved
counterfactualfairness[19,24]withoutbeinginfluencedbythe
fairness-accuracyspectrum[1,6,7].Throughourapproach,we
biasin𝐵.Formally,thecounterfactualfairnessinourSCMcanbe
aimtocaptureasmuchinformationaspossiblebetweenthetarget
definedasfollows:
variable𝑌 andfeatures,includingthesensitivefeatures.Addition-
ally,themethodproposedby[19]presupposesensitivevariables Definition 3.1 (Counterfactual Fairness). Given a new pair of
tobebinary(orcategorical)sothattheycanbeeasilyisolatedor attributes(b,a),adecisionvariable𝑌isconsideredcounterfactually
adjustedbasedonempiricalprobabilitymassfunction.Itdoesnot fairif,foranyb′ ∈B,
vad ard ir ae bs ls es wit iu that ii no tn ris ci an tevo inlv tein r-g dm epu el nti dv ea nr cia yt ,e wo hr ec reo an sti on uu ro pu rs os pe on ss ei dti Ov Be 𝑌 b′(𝑈)|(cid:8)𝐵=b∗,𝐴=a∗(cid:9) =𝑑 𝑌 b∗(𝑈)|(cid:8)𝐵=b∗,𝐴=a∗(cid:9), (2)
algorithmaimstoremovetheinfluenceofagroupofcontinuous 𝑑
where𝑃 =𝑄indicatesthatrandomvariables𝑃 and𝑄areequalin
sensitivevariablesfromthedata,therebyensuringcounterfactual
distribution,and𝑌 (𝑈)representsthecounterfactualoutcomeof
fairnessinsubsequentmachinelearningtasks. b
𝑌 when𝐵=b.
Theabovedefinitionimpliesthatthedistributionofthecounter-
factualresultshouldnotdependonthesensitivevariablescondi-
3 METHODOLOGY tionalontheobserveddata.NotethatalthoughDefinition3.1uses
3.1 Problemsetup thedecisionvariable𝑌,italsoappliestoitspredictor𝑌ˆ without
We jointly define𝑞 non-sensitive variables as𝐴 ∈ A ⊆ R𝑞,𝑝 anylossofgenerality[19].
sensitivevariablesas𝐵 ∈R𝑝,anddecisionvariableas𝑌 ∈Y.The
3.2 Achievingcounterfactualfairnessviadata
data generation process in our problem setup can be described
byaStructuralCausalModel(SCM)[29]asshowninFig.2(a). decorrelation
Tobespecific,weconsiderthesetofendogenousvariables𝑉 = Toclarifyandstreamlinethepresentationofourfindings,webegin
{𝐵,𝐴,𝑌,𝑌ˆ },where{𝐵,𝐴,𝑌}aretheobservedvariablesand𝑌ˆ isthe byillustratingthatcounterfactualfairnesscanbeattainedunder
predictionof𝑌 wemadebasedon𝐵 and𝐴.Weassumethat𝑈 , conditionswheresensitiveandnon-sensitivevariablesexhibitno
𝐵
𝑈 ,and𝑈 ,whicharetheexogenousvariablesthataffect𝐵,𝐴, correlationandaretogethernormallydistributed.
𝐴 𝑌
and𝑌 respectively,areindependentofeachother.Thestructural ConsideradatasetD = {(b𝑖,a𝑖,𝑦 𝑖)}𝑛
𝑖=1
with𝑛observeddata
equationsaredescribedwiththefunctions𝐹 ={𝑓 𝑌,𝑓 𝐴,𝑓 𝐵},onefor tuples,whereb𝑖,a𝑖,and𝑦
𝑖
representthe𝑖-thobservationofthe
3sensitive,non-sensitive,anddecisionvariables,respectively.We SubstitutingΣaboveintothejointprobabilitydensityfunction
useA = [a1,...,a𝑛]⊤ ∈ R𝑛×𝑞 todenotethedatamatrixofnon- of𝐴and𝐵,wehave
sensitivevariables𝐴,anduseB= [b1,...,b𝑛]⊤ ∈R𝑛×𝑝 todenote
1
thedatamatrixofsensitivevariables𝐵indatasetD. P(a,b)=
√︁
(2𝜋)𝑞+𝑝|Σ|·
Toestablishtheconnectionbetweencounterfactualfairnessand
datauncorrelation,weintroducethefollowingassumption: exp(cid:32) −1 (cid:20) a−𝝁𝐴 (cid:21)⊤ Σ−1(cid:20) a−𝝁𝐴 (cid:21)(cid:33)
2 b−𝝁𝐵 b−𝝁𝐵
Assumption3.2. Giventhestructuralmodeldefinedin(1),the 1
= ·
sensitivevariable𝐴andnon-sensitivevariables𝐵arejointnormal. √︁ (2𝜋)𝑞+𝑝|Σ 𝐴||Σ 𝐵| (4)
(cid:18) 1 (cid:19)
Buildingonthisassumption,wepresentthefollowingtheorem: exp − 2(a−𝝁𝐴)⊤Σ 𝐴−1 (a−𝝁𝐴) ·
(cid:18) 1 (cid:19)
Theorem3.3.
UnderAssumption3.2,𝑌ˆ
iscounterfactuallyfairwhen
exp − 2(b−𝝁𝐵)⊤Σ 𝐵−1 (b−𝝁𝐵)
𝐴and𝐵areuncorrelated. =P 𝐴(a)P 𝐵(b).
As we can observe that the joint distribution decomposes into
theproductoftheirmarginaldistributions,hencedemonstrating
Proof. We first demonstrate that 𝑌ˆ is counterfactually fair, theirstatisticalindependencewhen𝐴and𝐵arejointlynormaland
uncorrelated.
whichisachievedwhenthemodel’spredictionsfor𝑌 arenotin-
Finally,establishing𝐴and𝐵asjointlynormalanduncorrelated
fluencedbythesensitivevariable𝐵.Inthefollowingproof,we
focus on the case of a binary predictor𝑌ˆ for simplicity, noting leadstotheinferencethat,giventheSCMin(1),predictor𝑌ˆ is
counterfactuallyfair.
thatourfindingscanbeseamlesslyappliedtopredictorsthatyield
□
continuousoutcomes.Thissimplificationallowsustoestablishfair-
nessbyshowingthattheexpectedoutcomesareequivalent,which
Theorem3.3suggeststhatachievingcounterfactualfairnessin
for a Bernoulli random variable, also indicates a distributional
thepredictor𝑌ˆ ispossiblebydecorrelatingnon-sensitivevariables
equivalence. Following the well-established “Abduction-Action-
Prediction”methodfrom[29],theconditionalexpectationof𝑌ˆ 𝐴fromsensitivevariables𝐵.Thisinsightmotivatesustodevelop
b′ adatapre-processingalgorithmaimedatadjustingtheobserved
given𝐵=b∗,𝐴=a∗canbewrittenas:
datawithminimalchangestoachieveuncorrelationbetweennon-
sensitiveandsensitivevariables.
ItisimportanttoemphasizethatAssumption3.2isapplicable
E(𝑌ˆ b′|𝐵=b∗,𝐴=a∗)
acrossabroadspectrumofapplicationswherestandardizationof
∫ (3)
= 𝑓 𝑌ˆ (cid:0)𝑓 𝐴(cid:0) b′,𝑢(cid:1);D(cid:1)P 𝑈𝐴|𝐵,𝐴(cid:0)𝑢 |𝐵=b∗,𝐴=a∗(cid:1)𝑑𝑢, d sea nta sii ts ivr ee ,q au ri ere nd o, re mns au lir zi en dg tt oha ht aa vl elv aa mria eb anles o, fb zo et rh os ae nn dsi ativ ve ara in and cn eo on f-
1.Moreover,ourinvestigationshaverevealedthatthisassumption
isnotalwayscriticalforthesuccessofourdatapre-processing
where𝑓 𝑌ˆ(·;D):A→Ydenotesthepredictorof𝑌ˆ trainedusing algorithm.Ourapproachhasdemonstratedeffectivenessevenwhen
data D and P 𝑈𝐴|𝐵,𝐴(𝑢 |𝐵=b∗,𝐴=a∗) denotes theconditional appliedtodatasetsthatdonotmeetthiscriterion.Inparticular,as
densityof𝑈 𝐴given𝐵=b∗and𝐴=a∗.Toargueforcounterfactual elaboratedinSection4,ourstrategyhasproventobepromisingin
fairness,itsufficestoshow experimentsthatinvolvecategoricalsensitivevariables𝐵,rather
thancontinuousones.
E(𝑌ˆ b′|𝐵=b∗,𝐴=a∗)=E(𝑌ˆ b∗|𝐵=b∗,𝐴=a∗),
3.3 Orthogonaltobias
Inthissection,wedevelopadatapre-processingalgorithm,termed
asOrthogonaltoBias(OB).Wefirststandardizebothnon-sensitive
ifthedatageneratingprocessfortheobserveddata𝑓 𝐴(b,𝑢)does
variables𝐴andsensitivevariables𝐵,toachieveanormaldistribu-
notdependonthevalueofb,indicating𝐴’sindependencefrom𝐵.
tionforeach.Thentheempiricalcovariancebetween𝐴and𝐵can
Next,weprovethat𝐴isindependentof𝐵 whentheyareun-
beestimatedby
correlatedunderAssumption3.2,whichisacommonlyaccepted
s 𝝁t 𝐵at ,i rs et sic pa el cr te ivs eu ll yt. ,wCo itn hs cid oe vr arth iae nm cee man atv re icc et sor Σs 𝐴fo ,Σr 𝐵𝐴 .a Rn ed ca𝐵 lla ths a𝝁 t𝐴 whan end cov(𝐴,𝐵)=E[(𝐴−E[𝐴])(𝐵−E[𝐵])] ≈ 𝑛1∑︁𝑛 a𝑖⊤b𝑖 =⟨A,B⟩.
𝐴and𝐵uncorrelated,thecovariancematrixΣof𝐴and𝐵is 𝑖=1
Given that two variables are uncorrelated if their covariance is
zero,orthogonalitybetweenobserveddataAandBguarantees
Σ= (cid:20) Σ 0𝐴 Σ0
𝐵
(cid:21) andΣ−1 = (cid:20) Σ 0𝐴−1 Σ0 𝐵−1 (cid:21) . u sen rc vo er dre nl oat ni -o sn e. nT sih tie vr eef do ar te a, Athe inO sB ua cl hgo ar with aym thai am tis tt io soad rtj hu ost goth ne alo tb o-
4theobservedsensitivedataB,whileensuringminimalchangesto Algorithm1SparseOrthogonaltoBias(SOB)
non-sensitivedataA. 1: Input:Non-sensitiveandsensitivedataAandB,rank𝑘;
Specifically,wefollowtheideaofOrthogonaltoGroupsintro- 2: StandardizeAandB;
ducedby[3],anddefinearank𝑘approximationofAasA(cid:101)=SU⊤, 3: for𝑖 =1,...,𝑘do
whereU= [u1,...,u𝑘]isa𝑞×𝑘orthonormalmatrixandS={𝑠 𝑖𝑗} 4: Set𝑡 =1,𝜃 =1,and𝑠(0) =0;
𝑖
isa𝑛×𝑘matrix.Thegoalistofindatransformed𝑛×𝑞matrixA(cid:101)
5:
Randomlyinitialize𝑢(0);
t bh yat this eo Fr rt oh bo eg no in ua sl nto oB rmwi ∥t 𝑋h ∥m Fin =im √︃a (cid:205)lc 𝑖h (cid:205)an 𝑗g 𝑥e 𝑖2 𝑗t .o Fm ora mtr aix ll, ya ,s wm ee aas imure tod 6: while(cid:13) (cid:13) (cid:13)𝑢 𝑖(𝑡) −𝑢 𝑖(𝑡−1)(cid:13) (cid:13) (cid:13)𝑖 𝐹 >𝜂and(cid:13) (cid:13) (cid:13)𝑠 𝑖(𝑡) −𝑠 𝑖(𝑡−1)(cid:13) (cid:13) (cid:13)𝐹 >𝜂 do
solvethefollowingconstrainedoptimizationproblem: 7: Compute𝛽(𝑡) with
𝑖
argm S,i Un(cid:13) (cid:13)A−SU⊤(cid:13) (cid:13)2 𝐹, 𝛽 𝑖 ← (cid:0) B⊤B(cid:1)−1 B⊤𝑃 𝑖−1A𝑢 𝑖
s.t. (cid:10) SU⊤,B(cid:11) =0, (5) 8: with𝑃 𝑖−1=𝐼 𝑛×𝑛−(cid:205) 𝑙𝑖 =− 11𝑠 𝑙𝑠 𝑙⊤
9: Update𝑠 𝑖 as
where the last constraint
requirU e⊤ sU U= toI𝑘 b,
e orthonormal matrix, 𝑠 𝑖(𝑡) ← ∥𝑃𝑃 𝑖𝑖 −− 11 AA 𝑢𝑢 𝑖𝑖 −− 𝛽𝛽 𝑖𝑖 BB
∥2
whichhelpspreventdegeneracy,suchasbasisvectorsbecoming
identicallyzeroorencounteringsolutionswithdoublemultiplicity. 10: Update𝑢 𝑖 as
Asshownby[3],theoriginalproblem(5)canbereformulated 𝑢(𝑡)
←
S𝜃 (cid:0) A⊤𝑠 𝑖(cid:1)
usingLagrangemultipliersasfollows: 𝑖 ∥S𝜃 (A⊤𝑠 𝑖)∥2
The
faca tr og rm
S
2,i
U
/n 𝑛𝑛1 is∑︁
𝑖
i=𝑛
n1
tr(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
oa d𝑖 u− c∑︁ e𝑗𝑘
= d1
t𝑠 o𝑖𝑗u si⊤ 𝑗 m(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
p2 li+ fy𝑛2 t∑︁ h𝑗𝑘
= e1
e𝜆 x𝑗 p∑︁
𝑖
r=𝑛
e1
s𝑠 s𝑖 i𝑗 o𝐵 n𝑖.
for
th(6 e) 1 1
1
11 2
3
4: :
:
:
e
ifn𝑡w
(cid:13) (cid:13)d
A←h
w
⊤e hr 𝑠𝑡e
𝑖i (cid:13)
(cid:13)+ lS 1e1𝜃 ≤(𝑥 ℎ) t= hes nign(𝑥)(|𝑥|−𝜃)1(|𝑥| ≥𝜃)
15: Set𝜃 =0
optimalsolutions.Itensuresthatthefirst-orderconditionfor(6)
16: else
withrespectto𝑠
𝑖𝑗
involvesacommonfactorof2/𝑛,whichcan
17: Set𝜃
>0suchthat(cid:13) (cid:13)𝑢(𝑡)(cid:13)
(cid:13) =ℎ
thenbecanceledoutduringcomputations.LetS∗andU∗denote (cid:13) 𝑖 (cid:13)1
theoptimalsolutionsofSandU,respectively.Asdemonstrated 18: endif
inAppendixA,thereformulatedequation(6)yieldsaclosed-form 19: endfor
solution: 20: SetSˆ = [𝑑1𝑠1,...,𝑑 𝑘𝑠 𝑘]where𝑑 𝑖 =𝑠 𝑖⊤A𝑢 𝑖,andUˆ = [𝑢1,...𝑢 𝑘].
𝜆∗ 𝑗 =
(cid:68)
A ⟨u B∗ 𝑗 ,⊤ B,
⟩B(cid:69) 22 21 :: OFi una tpll uy tc :a A(cid:101)lculatetheattributematrixA(cid:101)=SˆUˆ⊤.
(7)
U∗= (cid:2) u∗ 1,...,u 𝑘∗(cid:3)
S∗={𝑠 𝑖∗ 𝑗}, be found in Appendix B. Following derivation by [3, 33], Algo-
rithm1illustratesthekeystepstoimplementtheSOBalgorithm,
whereU∗ consistsofthefirst𝑘 rightsingularvectorsofAand
where𝜂representstheminimumchangetoterminatetheiterative
𝑠 𝑖∗
𝑗
=a𝑖u∗ 𝑗⊤−𝜆∗ 𝑗𝐵 𝑖.Detailedderivationsoftheclosed-formsolution
optimizationprocess.
(7)canbefoundinAppendixA.Theprocessednon-sensitivedata
Notethatwiththeadditionalregularizationconstraintsinthis
matrixisthereforeA(cid:101)=S∗U∗⊤.
case,thesolutionfavorssparsitywhilesatisfyingtheorthogonal
Itisnoteworthythatwhenthecorrelationbetweenthesensitive
constraint.Therefore,SOBalsoachievesatcounterfactualfairness
variableandtheobserveddataisminimal,A(cid:101)yieldsareconstruction underSCMframeworkwithTheorem3.3.
errorsimilartothatofthestandardSingularValueDecomposition
(SVD).ItisbecausetheadditionalreconstructionerrorofA(cid:101)relative
4 EXPERIMENTS
toSVDwiththesamerankisproportionaltothecollinearitybe-
In this section, we present the results of our experiments con-
tweenthesubspacespannedbyBandtheleftsingularvectorsofthe
ducted on both synthetic and real data sets. In Section 4.2 and
dataA[3].TheexactexpressionforOB’sadditionalreconstruction
4.3,wecomparetheproposedOBwithseveralexistingmethods:
errorcomparedtoSVDcanbefoundinAppendixA.
MachineLearning(ML),alogisticregressionbaselinewhichuses
SparseOrthogonaltoBias(SOB). Whenthenumberoffeatures𝑝 allattributeswhethersensitiveornot;FairnessthroughUnaware-
exceedsthenumberofobservations𝑛,estimatingalow-dimensional ness(FTU),whichsimplyfitsalogisticmodelwithA,excluding
structurefromhigh-dimensionaldatacanbecomenumericallyun- thesensitivevariablesfromthemodel;EqualizedOdds(EO),apost-
stable[36].Toaddressthischallenge,weintroduceasparsevariant processingalgorithmchosentobalancefalsepositivesandfalse
oftheOBalgorithm,referredtoasSOB.TheSOBimposesanℓ1-norm negativeswhileminimizingtheexpectedlossproposedby[30];
penaltyforUtoencouragesparsityandimprovenumericalstabil- FairLearningAlgorithm(FL),whichachievescounterfactualfair-
ity,inadditiontotheorthogonalityconstraintsin(5).Wedefine nessbysamplingunobservedpartsofthegraphicalcausalmodel
ℎastheℓ1constrainton𝑢.DetailsoftheformulationofSOBcan usingMarkovchainMonteCarlomethods[24];AffirmativeAction
5(AA),anpostprocessingalgorithmthatproducesfairequalizedodds
(EO)andaffirmativeaction(AA)predictorsbypositingacausal
modelandconsideringcounterfactualdecisions[30];FairLearning
throughDatapre-processing(FLAP)by[19].
Amongthecomparedmethods,FTUandFLAParepre-processing
methods,FLisanin-processingapproach,andAAandEOarepost-
processingapproaches.NotethatforbothFLAPandOB,thepredic-
torclassusedforpredictioncanincludesensitivevariables.Specifi-
cally,inadditiontothepredictorclass𝑓 𝑌ˆ(𝐴):A→Ydiscussedin
Section3.2,foramachinelearningpredictor𝑓 𝑌ˆ(𝐴,𝐵):A×B→Y
thatutilizesbothsensitiveandnon-sensitivevariables,anAveraged
MachineLearning(AML)predictor𝑓 𝑌ˆ′(𝐴)=∫ 𝑓 𝑌ˆ(𝐴,𝐵)P(𝐵)𝑑𝐵or
𝑓 𝑌ˆ′(𝐴) = (cid:205)𝑓 𝑌ˆ(𝐴,𝐵)P(𝐵),dependingonwhether𝐵iscontinuous
orbinary,canbeconstructedrespectively.Therefore,wedenote
OB1andOB2forscenariosinvolvingthetrainingof𝑓 𝑌ˆ(𝐴,𝐵)or𝑓
𝑌ˆ
withOB-processeddata,respectively.Similardesignationsareused
forFLAP1andFLAP2.Additionally,since[19]introducestwopre-
processingmethods,OrthogonalizationandMarginalDistribution
Mapping,wedenotethemasFLAP(O)andFLAP(M)respectively.
Logisticregressionpredictorsareutilizedforallmodels,denoted
as𝑌ˆ.TheexperimentswereconductedinaJupyterNotebookenvi-
ronmentwith16GBRAM.
4.1 Evaluationmetrics
Weassesstheaccuracyofthedecisionsconcerningtheground
truthwithAreaUndertheCurve(AUC)andAccuracy(ACC).As
Figure 3: Synthetic loan data. The x-axis shows different
fortheevaluationofcounterfactualfairness,weadopttwometrics
levels of the effect of the sensitive variable on education
introducedby[19]:CF-metricsmeasurescounterfactualfairnessby
calculatingtheaveragechangeinpredictedscoresbetweengroups
level, 𝛽 𝐸, as defined in (9). With 10 repeated experiments,
the lines represent the averaged results for each method,
withthemostsignificantdifference.Additionally,weincorporate
andthebarsrepresenttheirstandarddeviation.Theredline
CFBoundtoevaluatecounterfactualfairnessintheabsenceofnon-
representstheresultsoftheproposedmethodOB.
sensitiveconditions.Thismetriccomputesthemaximumabsolute
valueofthebounds’averageofpredictedscoresforasampleran-
domlyselectedfromtheset.Duetocomputationalchallenges,we theeducationyearandannualincomeforeachracegroupfollows
onlyevaluateCF-Boundforthesimulateddataset.Foracompre- thefollowingdistribution:
hensivecomparisonofthemethods,weadditionallyincorporate 𝐸=max{0,𝑈 𝐸},
twoobservationalfairnessmetrics:EOFairness,asdefinedby[30], (8)
andAAFairness,proposedin[30].InTables1to3,wehighlight
𝐼 =exp{0.1𝑈 𝐸+𝑈 𝐼}.
thebest-performingmethodinboldandunderlinethesecond-best Thebank’sdecisionissimulatedusingalogisticmodel:
foreachmetricused. 𝑌 =1{𝑈
𝑌
<expit(𝛽0+𝛽11{𝐵=1}+𝛽21{𝐵=2}+𝛽 𝐸𝐸+𝛽 𝐼𝐼)},
(9)
4.2 Syntheticdata where𝑈
𝑌
∼Uniform(0,1)andexpit(𝑢)=(1+𝑒−𝑢)−1.
Wefirstapplyourmethodstoasyntheticloandatasetexample Inthisexample,theparameters𝜆 and𝜆 determinetheextent
𝐸1 𝐸2
which is a modification from [19]. Using synthetic data allows ofthemeandifferenceineducationyearsacrossthethreerace
ustorepeattherandomdatagenerationprocessandprovidethe groups,whiletheparameters𝜆 and𝜆 dictatethemagnitudeof
𝐼1 𝐼2
average results. It also enables us to observe how fair learning themeandifferenceinlogincomeamongthesethreeracegroups.
modelsrespondtochangingeffectsresultingfromdifferentlevels 𝛽1and𝛽2characterizethedirecteffectoftheraceinformationon
ofunfairtreatmentamongdifferentgroups.Thepresentedexample theloanapprovalrate.
illustratesascenarioinwhichabankevaluatesloanapplications Itisimportanttonotethatthesensitivevariable𝐵iscategori-
basedontheapplicant’seducationlevel(𝐸)andannualincome(𝐼), cal,andthedatageneratingprocessdoesnotexactlyconformto
determiningapproval(𝑌 =1)orrejection(𝑌 =0).Thepopulation Assumption3.2.AsevidencedinTable1andFigure3,despitethe
comprisesthreepossibleracegroups:𝐵={0,1,2}.Similarto[19], deviationfromtheassumptionsinthetestedsyntheticdataset,
wegenerate𝐵 accordingto𝐵 = 1{𝑈 𝐵 < 0.76}+1{𝑈 𝐵 > 0.92}, ourmethodconsistentlyshowcasescomparativelyhighAUCand
where𝑈
𝐵
∼Uniform(0,1).Let𝑈
𝐸
and𝑈
𝐼
betwostandardnormal ACCcomparedtomostmethods.Notably,itsaccuracyoutperforms
randomvariableswithmean𝜇
𝐸
=𝜆 𝐸0+1{𝐵=1}𝜆 𝐸1+1{𝐵=2}𝜆
𝐸2
FLandFLAP,twoothercounterfactuallyfairmethods.Moreover,
and𝜇
𝐼
=log(𝜆 𝐴0+1{𝐵=1}𝜆 𝐴1+1{𝐵=2}𝜆 𝐴2),respectively.Then ourmethodachieveslowCF-metricandCFBound,akintoFLand
6Table1:Performancecomparisonofourmethodandotherexistingmethodswithsyntheticloandata
Baselines ComparedMethods Ours
Metrics
ML FTU FL EO AA FLAP1(O) FLAP2(O) FLAP1(M) FLAP2(M) OB1 OB2
ACC 0.6618 0.6481 0.6224 0.6237 0.6224 0.6237 0.6224 0.6237 0.6224 0.6406 0.6279
AUC 0.9457 0.8986 0.5867 0.6682 0.5714 0.5668 0.5837 0.5875 0.5863 0.5704 0.5856
CF-metrics 0.6291 0.3906 0.0031 0.0355 0.0034 0.0016 0.0032 0.0002 0.0002 0.0011 0.0026
CFBound 0.8690 0.9464 0.1836 0.1071 0.0918 0.0937 0.1847 0.0690 0.0670 0.0830 0.2340
EOFairness 0.5469 0 0.0156 0 0.0336 0.0321 0.0156 0.0301 0.0180 0 0
AAFairness 0.6235 0.4559 5.6e-18 0.0370 1.1e-18 3.3e-18 6.7e-18 0.0012 0.0038 4.6e-17 4.3e-17
Table2:PerformancecomparisonofourmethodandotherexistingmethodsonAdultdata
Baselines ComparedMethods Ours
Metrics
ML FTU FL EO AA FLAP1(O) FLAP2(O) FLAP1(M) FLAP2(M) SOB1 SOB2
ACC 0.7612 0.7604 0.7594 0.7680 0.7644 0.7357 0.7151 0.7548 0.7594 0.7655 0.7597
AUC 0.8128 0.8036 0.7680 0.7991 0.7682 0.7682 0.7680 0.7651 0.7649 0.7806 0.7809
CF-metric 0.2779 0.2338 0.0228 0.2047 0.0268 0.0280 0.0228 0.0280 0.0228 0.0529 0.0600
EOFairness 0.1536 0 0.2853 0 0.2811 0.2780 0.2853 0.2780 0.2853 0.0002 0.0005
AAFairness 0.3034 0.2574 0 0.2259 0 2.2e-17 2.2e-17 2.8e-17 2.8e-17 0.0001 0.0004
FLAP,indicatingahighdegreeofcounterfactualfairness.Thisde- COMPAS. TheCOMPASrecidivismdataincludesdemographic
sirablecharacteristiccanbeattributedtothepropertyofOB,which informationsuchassex,age,race,andrecorddata(priorcounts,
minimallymodifiesinthenon-sensitivedatawhileensuringcoun- juvenilefeloniescounts,andjuvenilemisdemeanorscounts)for
terfactualfairness.Furthermore,intermsofobservationalfairness over10,000criminaldefendantsinBrowardCounty,Florida.The
metrics,OBexhibitsanoverallbetterperformancecomparedtoFL goalistopredictwhethertheywillre-offendinthenexttwoyears.
andFLAPwithlowerEOandAAFairnessmetrics. AsdepictedinTable3,similartoourmethod’sperformancein
thetwopreviousdatasets,theaccuracywithOBiscomparatively
4.3 Realdata high.Moreover,itsCF-metricissimilarcomparedtothatof FLAP
andFL,implyingcounterfactualfairnessattributedtoOB.Addition-
We also apply our methods to two real data sets: the Adult In-
ally,itachievesbothrelativelylowEOandAAFairnessmetrics
comedatasetfromtheUCIMachineLearningRepository1andthe
comparedtoFLAPandFL,suggestingbetterobservationalfairness.
COMPASrecidivismdatafromProPublica[5].
Adult. IntheAdultIncomedataset,weaimtopredictwhether Insummary,acrossthreedatasets,ourapproachconsistentlyex-
anindividual’sincomeexceeds$50,000,consideringfeaturessuch hibits its effectiveness in maintaining an overall better balance
assex,race,age,workclass,education,occupation,maritalstatus, betweenaccuracy,observationalfairness,andcounterfactualfair-
capitalgain,andloss.Thesensitiveattributesaresexandrace.The ness.Additionally,weobservedthatusingSOBforthesynthetic
trainingsetconsistsof32,561samples,andthetestsetcomprises loandataandCOMPASresultsinslightlylowerACCandAUC,
16,281samples. whileachievingsimilarfairnessmetrics.
DuetothelargesamplesizeofAdultdataset,weemploySOB.As
illustratedinTable2,similartoOB’sperformanceinsyntheticdata,
theaccuracyiscomparativelyhighcomparedtoallothertestedfair 4.4 CaseStudywithContinuousDecision
learningapproaches.Notably,theaccuracyisevenhigherthanthe
Variables
vanillaMLmodel,whichutilizesbothsensitiveandnon-sensitive
Toshowcasetheempiricalefficacyof OBindecorrelatingAand
attributesandgeneratesunfairresults.Asnotedby[3,8],theaddi-
Bandachievingcounterfactuallyfairpredictions,wepresentan
tionalregulationwithSOBmaycontributetohighout-of-sample
additionalcasestudyoftwosyntheticdatasetsfeaturingcontinuous
predictionperformances.Moreover,itsCF-metriciscomparableto
decisionvariable𝑌.WeapplyFTU,FL,andOBtothetwosimulated
thatof FLAPandFLandismuchlowerthanbaselines,implying
datasets.Weevaluatetheaccuracyof𝑌ˆ generatedbydifferent
counterfactualfairnessattributedtoOB.Additionally,itachieves
methodsusingRootMeanSquareError(RMSE)comparedtoground
bothlowEOandAAFairnessmetrics.
truth𝑌.Weexaminethecounterfactualfairnessbyexaminingthe
KL-divergencebetweenpredictionsusingtheobserved(actual)data
1https://archive.ics.uci.edu/dataset/2/adult andcounterfactualdatawithadifferentsensitivevariablefollowing
7Table3:PerformancecomparisonofourmethodandotherexistingmethodsonCOMPASdata
Baselines ComparedMethods Ours
Metrics
ML FTU FL EO AA FLAP1(O) FLAP2(O) FLAP1(M) FLAP2(M) OB1 OB2
ACC 0.5744 0.5726 0.5598 0.5710 0.5609 0.5605 0.5599 0.5607 0.5607 0.5666 0.5674
AUC 0.7206 0.7225 0.6928 0.7225 0.6927 0.6927 0.6928 0.7015 0.7019 0.6764 0.6744
CF-metric 0.2274 0.1406 0.0054 0.1377 0.0060 0.0058 0.0054 0.0026 0.0027 0.0060 0.0065
EOFairness 0.1046 0 0.1374 0 0.1405 1.7e-06 3.3e-06 6.7e-07 1.2e-06 0 0
AAFairness 0.2258 0.1460 0 0.1424 0 2.9e-07 5.6e-07 8.2e-07 3.0e-07 1.6e-16 1.1e-16
Table4:Performancecomparisononsyntheticdatasetswith
continuousdecisionvariable !# !$
GPA
Syn(Cont.Y) LSAT
Metrics "# … "% … "& $%&' LSAT ()*+.
FTU FL OB FTU FL OB
# FYA
RMSE 0.50 0.51 0.53 0.86 0.89 0.18
KLDiv. 0.35 0.00 0.00 1.42 0.28 0.01 (a)SyntheticData (b)LSAT
𝐶𝑜𝑟𝑟(A(cid:101),B)∗ 0.52 0.52 0.00 0.30 0.30 0.00
Figure4:(a)representsthecausalmodelforasyntheticdata
*TheaveragepairwisecorrelationbetweenA(cid:101)andB. setusedinSection4.4,with𝐵beingthebinarysensitivevari-
ableand𝑌 beingacontinuousoutcomeofinterest,along
withtheintermediatevariable𝐴thatisinfluencedby𝐵.(b)
[30].Weadditionallyincludetheaveragepair-wisecorrelations representsthecausalmodelforLSAT,asemi-syntheticex-
betweenA(cid:101)andBtoverifytheiruncorrelation. ampleusedby[24].Weextractalatentvariable,student’s
Whilethedetailedsetupofthetwosyntheticcasesisprovided knowledge(K),andassumesuchavariableaffectsGPA,LSAT,
inAppendixC,wepresentabriefoverviewhere.Causalmodels andFYAscorestoapplytheFLmethodinthiscase.
for the two synthetic cases with continuous decision variables
areoutlinedinFigure4.Thefirstsyntheticdataset,Syn(Cont.Y),
featuresacontinuous𝑌 followinganormaldistributionwithits Observed Observed
Counterfactual Counterfactual
meanlinearlydependentonnon-sensitivevariables𝐴,whichin
turnareaffectedbytwocategoricalsensitivevariables.Theother
syntheticdataset,LSAT,isderivedfromasurveyconductedby
theLawSchoolAdmissionCouncilacross163lawschoolsinthe
United States [32]. Here, the decision variable𝑌 represents the
first-yearaveragegrade(FYA),whilethesensitivevariableisthe
KL Div. = 1.4238 KL Div. = 0.0145
students’race.TheresultssummarizedinTable4indicatethatOB
Y Prediction Y Prediction
effectivelydecorrelatesAandBinbothcases.Itachievesasimilar
(a)FTU (b)OB
RootMeanSquareError(RMSE)insyntheticdataandlowerRMSE
in the LSAT dataset compared to FTU and FL. Additionally, the Figure5:Theredlinerepresentsthe𝑌ˆ
distributionutilizing
predicteddistributionsofFYAunderobservedandcounterfactual
theobservedvariables,whilethebluelinerepresentsthe
sensitivevariablesfortheLSATcasearenearlyidenticalforOB,
predictiondistributionutilizingthecounterfactualdata.(a)
asevidencedbytheKL-divergencemeasureinTable4andhighly
shows the distributions
of𝑌ˆ
using FTU method, while (b)
overlappedcurvesinFigure5.Thus,thepredictionsofOBalignwith presentsthedistributionsof𝑌ˆ
usingOBprocesseddata.
thedefinitionofcounterfactualfairnessoutlinedinDefinition3.1.
5 CONCLUSION resultingdatapre-processingalgorithmeffectivelyremovesbias
Inconclusion,thispaperdemonstratesthatachievingcounterfac- inpredictionswhilemakingminimalchangestotheoriginaldata.
tualfairnessisfeasiblebyensuringtheuncorrelationbetweennon- Importantly,OBismodel-agnostic,ensuringitsadaptabilitytoa
sensitiveandsensitivevariablesundercertainconditions.Building varietyofmachinelearningmodels.Throughcomprehensiveeval-
onthisinsight,wepresenttheOrthogonaltoBias(OB)algorithm, uationsonsimulatedandreal-worlddatasets,wedemonstratethat
a novel approach to addressing fairness challenges in machine OBstrikesagreatbalancebetweenfairnessandaccuracy,outper-
learningmodels.OBachievescounterfactualfairnessbydecorre- formingcomparedmethodsandofferingapromisingsolutionto
lating data from sensitive variables under mild conditions. The thecomplexissueofbiasinmachinelearning.
8
ytisneD ytisneDREFERENCES [21] FaisalKamiranandToonCalders.2012. Datapreprocessingtechniquesfor
[1] AlekhAgarwal,AlinaBeygelzimer,MiroslavDudík,JohnLangford,andHanna classificationwithoutdiscrimination.Knowledgeandinformationsystems33,1
Wallach.2018. Areductionsapproachtofairclassification.InInternational (2012),1–33.
conferenceonmachinelearning.PMLR,60–69. [22] AriaKhademi,SanghackLee,DavidFoley,andVasantHonavar.2019.Fairness
[2] SinaAghaei,MohammadJavadAzizi,andPhebeVayanos.2019. Learning inalgorithmicdecisionmaking:Anexcursionthroughthelensofcausality.In
OptimalandFairDecisionTreesforNon-DiscriminativeDecision-Making.In TheWorldWideWebConference.AssociationforComputingMachinery,New
York;NY;UnitedStates,2907–2914.
ProceedingsoftheThirty-ThirdAAAIConferenceonArtificialIntelligenceand
[23] NikiKilbertus,ManuelGomezRodriguez,BernhardSchölkopf,KrikamolMuan-
Thirty-FirstInnovativeApplicationsofArtificialIntelligenceConferenceandNinth
AAAISymposiumonEducationalAdvancesinArtificialIntelligence(Honolulu, det,andIsabelValera.2020. FairDecisionsDespiteImperfectPredictions.In
Hawaii,USA)(AAAI’19/IAAI’19/EAAI’19).AAAIPress,Article175,9pages. ProceedingsoftheTwentyThirdInternationalConferenceonArtificialIntelligence
https://doi.org/10.1609/aaai.v33i01.33011418 andStatistics(ProceedingsofMachineLearningResearch,Vol.108),SilviaChiappa
[3] EmanueleAliverti,KristianLum,JamesEJohndrow,andDavidBDunson.2021. andRobertoCalandra(Eds.).PMLR,277–287. https://proceedings.mlr.press/
Removingtheinfluenceofgroupvariablesinhigh-dimensionalpredictivemod- v108/kilbertus20a.html
elling.JournaloftheRoyalStatisticalSociety.SeriesA,(StatisticsinSociety)184,3 [24] MattJKusner,JoshuaLoftus,ChrisRussell,andRicardoSilva.2017. Coun-
(2021),791. terfactual Fairness. In Advances in Neural Information Processing Systems,
[4] McKaneAndrus,ElenaSpitzer,JeffreyBrown,andAliceXiang.2021. What I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
WeCan’tMeasure,WeCan’tUnderstand:ChallengestoDemographicData wanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc., Long
ProcurementinthePursuitofFairness.InProceedingsofthe2021ACMConference Beach,CA,USA. https://proceedings.neurips.cc/paper_files/paper/2017/file/
onFairness,Accountability,andTransparency(VirtualEvent,Canada)(FAccT’21). a486cd07e4ac3d270571622f4f316ec5-Paper.pdf
AssociationforComputingMachinery,NewYork,NY,USA,249–260. https: [25] BinhThanhLuong,SalvatoreRuggieri,andFrancoTurini.2011. k-NNasan
//doi.org/10.1145/3442188.3445888 implementationofsituationtestingfordiscriminationdiscoveryandprevention.
[5] JuliaAngwinandJeffLarson.2023.MachineBias. https://www.propublica.org/ InProceedingsofthe17thACMSIGKDDinternationalconferenceonKnowledge
article/machine-bias-risk-assessments-in-criminal-sentencing discoveryanddatamining.502–510.
[6] SinaBaharlouei,MaherNouiehed,AhmadBeirami,andMeisamRazaviyayn. [26] AmitabhaMukerjee,RitaBiswas,KalyanmoyDeb,andAmritPMathur.2002.
2019.R\’enyiFairInference.arXivpreprintarXiv:1906.12005(2019). Multi–objectiveevolutionaryalgorithmsfortherisk–returntrade–offinbank
[7] RichardA.Berk,HodaHeidari,ShahinJabbari,MatthewJoseph,MichaelKearns, loanmanagement.InternationalTransactionsinoperationalresearch9,5(2002),
JamieMorgenstern,SethNeel,andAaronRoth.2017.AConvexFrameworkfor 583–597.
FairRegression.ArXivabs/1706.02409(2017). https://api.semanticscholar.org/ [27] CarlM.O’Brien.2016.StatisticalLearningwithSparsity:TheLassoandGeneral-
CorpusID:12641090 izations.InternationalStatisticalReview84,1(2016),156–157. https://doi.org/10.
[8] ChristopherM.Bishop.2006.PatternRecognitionandMachineLearning(Infor- 1111/insr.12167arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12167
mationScienceandStatistics).Springer-Verlag,Berlin,Heidelberg. [28] PranitaPatilandKevinPurcell.2022.Decorrelation-BasedDeepLearningfor
[9] TolgaBolukbasi,Kai-WeiChang,JamesYZou,VenkateshSaligrama,andAdamT BiasMitigation.FutureInternet14,4(2022). https://doi.org/10.3390/fi14040110
Kalai.2016. Manistocomputerprogrammeraswomanistohomemaker? [29] JudeaPearl.2009.Causality(2ed.).CambridgeUniversityPress. https://doi.
debiasingwordembeddings.Advancesinneuralinformationprocessingsystems org/10.1017/CBO9780511803161
29(2016). [30] YixinWang,DhanyaSridhar,andDavidMBlei.2019.Equalopportunityand
[10] ToonCalders,FaisalKamiran,andMykolaPechenizkiy.2009.Buildingclassifiers affirmativeactionviacounterfactualpredictions.arXivpreprintarXiv:1905.10870
withindependencyconstraints.In2009IEEEinternationalconferenceondata (2019).
miningworkshops.IEEE,13–18. [31] SongWei,XiangruiKong,AlinsonSantosXavier,ShixiangZhu,YaoXie,andFeng
[11] SilviaChiappa.2019.Path-specificcounterfactualfairness.InProceedingsofthe Qiu.2024.AssessingElectricityServiceUnfairnesswithTransferCounterfactual
AAAIconferenceonartificialintelligence,Vol.33.7801–7808. Learning.arXivpreprintarXiv:2310.03258(2024).
[12] LeeCohen,ZacharyC.Lipton,andYishayMansour.2020.EfficientCandidate [32] LindaF.Wightman.1998.LSACNationalLongitudinalBarPassageStudy.LSAC
ScreeningUnderMultipleTestsandImplicationsforFairness.In1stSymposium ResearchReportSeries. https://api.semanticscholar.org/CorpusID:151073942
[33] DanielaMWitten,RobertTibshirani,andTrevorHastie.2009. Apenalized
onFoundationsofResponsibleComputing,FORC2020,June1-3,2020,Harvard
University,Cambridge,MA,USA(virtualconference)(LIPIcs,Vol.156),AaronRoth matrixdecomposition,withapplicationstosparseprincipalcomponentsand
(Ed.).SchlossDagstuhl-Leibniz-ZentrumfürInformatik,Dagstuhl,Germany, canonicalcorrelationanalysis.Biostatistics10,3(2009),515–534.
1:1–1:20. https://doi.org/10.4230/LIPIcs.FORC.2020.1 [34] MuhammadBilalZafar,IsabelValera,ManuelGomezRogriguez,andKrishnaP.
[13] ElliotCreager,DavidMadras,Jörn-HenrikJacobsen,MarissaWeis,KevinSwer- Gummadi.2017. FairnessConstraints:MechanismsforFairClassification.In
sky,ToniannPitassi,andRichardZemel.2019. Flexiblyfairrepresentation Proceedingsofthe20thInternationalConferenceonArtificialIntelligenceand
learningbydisentanglement.InInternationalconferenceonmachinelearning. Statistics(ProceedingsofMachineLearningResearch,Vol.54),AartiSinghand
PMLR,1436–1445. JerryZhu(Eds.).PMLR,Sydney,Australia,962–970. https://proceedings.mlr.
[14] Briand’Alessandro,CathyO’Neil,andTomLaGatta.2017.Conscientiousclassi- press/v54/zafar17a.html
fication:Adatascientist’sguidetodiscrimination-awareclassification.Bigdata [35] RichZemel,YuWu,KevinSwersky,ToniPitassi,andCynthiaDwork.2013.
5,2(2017),120–134. LearningFairRepresentations.InProceedingsofthe30thInternationalConference
[15] CynthiaDwork,MoritzHardt,ToniannPitassi,OmerReingold,andRichard onMachineLearning(ProceedingsofMachineLearningResearch,Vol.28),Sanjoy
Zemel.2012.FairnessthroughAwareness.InProceedingsofthe3rdInnovations DasguptaandDavidMcAllester(Eds.).PMLR,Atlanta,Georgia,USA,325–333.
inTheoreticalComputerScienceConference(Cambridge,Massachusetts)(ITCS https://proceedings.mlr.press/v28/zemel13.html
’12).AssociationforComputingMachinery,NewYork,NY,USA,214–226. https: [36] HuiZou,TrevorHastie,andRobertTibshirani.2006.Sparseprincipalcomponent
//doi.org/10.1145/2090236.2090255 analysis.Journalofcomputationalandgraphicalstatistics15,2(2006),265–286.
[16] QizhangFeng,MengnanDu,NaZou,andXiaHu.2022.Fairmachinelearning
inhealthcare:Areview.arXivpreprintarXiv:2206.14397(2022).
[17] BenjaminFish,JeremyKun,andÁdámDLelkes.2016. Aconfidence-based
approachforbalancingfairnessandaccuracy.InProceedingsofthe2016SIAM
internationalconferenceondatamining.SIAM,SIAM,Miami,Florida,USA,144–
152.
[18] NinaGrgic-Hlaca,MuhammadBilalZafar,KrishnaPGummadi,andAdrian
Weller.2016.Thecaseforprocessfairnessinlearning:Featureselectionforfair
decisionmaking.InNIPSsymposiumonmachinelearningandthelaw,Vol.1.
Barcelona,Spain,CurranAssociates,Inc.,Barcelona,Spain,11.
[19] RuiSongHaoyuChen,WenbinLuandPulakGhosh.2023. OnLearning
andTestingofCounterfactualFairnessthroughDataPreprocessing. J.Amer.
Statist.Assoc.0,0(2023),1–11. https://doi.org/10.1080/01621459.2023.2186885
arXiv:https://doi.org/10.1080/01621459.2023.2186885
[20] MoritzHardt,EricPrice,andNathanSrebro.2016.EqualityofOpportunityin
SupervisedLearning.InProceedingsofthe30thInternationalConferenceonNeural
InformationProcessingSystems(Barcelona,Spain)(NIPS’16).CurranAssociates
Inc.,RedHook,NY,USA,3323–3331.
9A CLOSED-FORMOBSOLUTIONDERIVATION referto[3].Forexample,asnotedby[3],anintuitiveinterpreta-
Westartbyconsidering(6)when𝑘 =1.TheOBalgorithmaims tionofthesolutionin(14)isthattheoptimalscoresforthe 𝑗-th
to find the closest rank-1 matrix (vector) approximation to the dimensionareobtainedbyprojectingtheoriginaldataoverthe
originalsetofdatathatsatisfiestheorthogonalcondition.(6)can 𝑗-thbasisandthensubtracting 𝑗-timestheobservedvalueof𝑏.
bereformulatedas: Moreover,astheconstraintsofOBdonotinvolveanyvector𝑢 𝑗,the
optimizationwithrespecttothebasiscanbederivedfromknown
(cid:13) (cid:13)2
argm 𝑆,𝑈in 𝑛1∑︁ 𝑖=𝑛 1(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)𝑎 𝑖 −∑︁ 𝑗𝑘 =1𝑠 𝑖𝑗𝑢𝑇 𝑗(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
+ 𝑛2 𝜆1∑︁ 𝑖=𝑛 1𝑠 𝑖1𝑏 𝑖 . (10) r a𝑗e c=s cu o1l rt , ds .. i.i n.n , g𝑘 ll yi ,n i tse oa er tq ha u el ag l ae stb o sr oa t ch. ieT ath fi ee drso stp ik nti grm uiga lh al rtv vsa i al nu lg ue u ef slo a [r r 8t v ,h a 2e l 7uv ]e .e sc oto fr 𝐴𝑢 ,𝑗 s, ow rti eth d
 
Somealgebraandtheorthonormalconditionon𝑢1allowusto WenotethefollowingusefulLemmaadaptedfrom[3]thatquan-
expressthelossfunctiontobeminimizedas: tifiestheadditionalreconstructionerrorof𝐴duetousingOBcom-
𝐿(𝑠1,𝑢1)=
𝑛1∑︁𝑛
(𝑎 𝑖 −𝑠 𝑖1𝑢𝑇 1)𝑇 (𝑎 𝑖 −𝑠 𝑖1𝑢𝑇 1)+
𝑛2
𝜆1∑︁𝑛
𝑠 𝑖1𝑏 𝑖
paredtoSVDis:
=
𝑛1∑︁𝑖 𝑖= =𝑛1
1(𝑎𝑇 𝑖𝑎 𝑖 −2𝑠 𝑖1𝑎 𝑖𝑢𝑇 1 +𝑠 𝑖2 1)+ 𝑛2 𝜆1∑︁ 𝑖=𝑛
1𝑖=
𝑠
𝑖1
1𝑏 𝑖.
L
o
[𝑃fe ]m
t 𝑖h
𝑗m
e
=maA
a
𝑛1t. r1 +i.
x
(cid:205)L
A 𝑏
𝑛e 𝑖t 𝑏oA
𝑗
𝑏ˆ
b
2t=
.a
Ti𝑉
n
h𝑘
e
e𝐷
d
a𝑘
f
d𝑈
r
do𝑘𝑇
m
itid oe
t
nhn aeo lte
t rr
et uh
cn
oe
c
nb
a
se
t
ts
e
rt udr ca
S
tn
iV
ok
D
n-k eoa rfp rop
r
rr ao
n
ox
k
fim
t𝑘
ha
.
et Li Oo en
Bt
𝑖=1 𝑖
Thefunctionisquadratic,anditspartialderivativewithrespectto algorithmcomparedtoSVDis∥𝑘𝑃𝑉 𝑘𝐷 𝑘∥F.
𝑠 𝑖1is
𝜕 1 2
𝜕𝑠
𝑖1𝐿(𝑠1,𝑢1)= 𝑛(−2𝑎 𝑖𝑢𝑇
1
+2𝑠 𝑖1)+ 𝑛𝜆1𝑏 𝑖. B FORMULATIONOFSOB
Solvingitfindsastationarypointof
ToenhancetheapplicabilityoftheOBalgorithm,particularlyinsce-
narioswithalargenumberoffeatures,weincorporateanℓ1-norm
𝑠 𝑖1=𝑎 𝑖𝑢𝑇 1 −𝜆1𝑏 𝑖. (11) penaltyforthematrix𝑈.Thisadditionaimstopromotesparsityin
Sotheoptimalscoreforthe𝑖-thsubjectisobtainedbyprojecting 𝑈 andenhancethenumericalstabilityoftheapproximation.The
theobserveddataontothefirstbasisandthensubtracting𝜆1𝑏.The modifiedalgorithm,denotedasSOB,isformulatedasfollows:
constraintdoesnotinvolvetheorthonormalbasis𝑢1,hencethe
solutionof (10)for𝑢1isequivalenttotheunconstrainedscenario.
Astandardresultoflinearalgebrastatesthattheoptimal𝑢1 for argmin(cid:13) (cid:13)𝐴−𝑆𝑈𝑇(cid:13) (cid:13)2
(10)withoutconstraintsequivalenttothefirstrightsingularvector 𝑆,𝑈 (cid:13) (cid:13)𝐹 (16)
o [3f 3𝐴 ]., Po lr uge gq iu ni gva inle tn ht ely sot lo utt ih oe nfi for rst 𝑢e 1i ag ne dnv se ec ttt io nr go thf eth de erm iva at tr ivix e𝐴 w𝑇 it𝐴
h
subjectto (cid:13) (cid:13)𝑢 𝑗(cid:13)
(cid:13)2
≤1,(cid:13) (cid:13)𝑢 𝑗(cid:13)
(cid:13)1
≤𝑡,(cid:13) (cid:13)𝑠 𝑗(cid:13)
(cid:13)2
≤1,𝑠𝑇 𝑗𝑠
𝑙
=0,𝑠𝑇 𝑗𝐵=0,
respectto𝜆1equalto0leadsto
for 𝑗 = 1,...,𝑘,and𝑙 ≠ 𝑗.Thedetailediterativeapproachto
𝑛
∑︁ (𝑎 𝑖𝑢𝑇
1
−𝜆1𝑎 𝑖)𝑇𝑏
𝑖
=0. (12) s isol tv hi an tg at lh ti hs op ur go hble thm ei mso inu it mlin ize ad ta ion nd pex rop bla li en med isin n[ o3 t]. joT ih ne tlm ya ci on ni vd ee xa
𝑖=1
in𝑠 and𝑢, it can be addressed iteratively. When𝑠 is fixed, the
Therefore,
minimizationstepisequivalenttoasparsematrixdecomposition
𝜆1=
(cid:205)𝑛
𝑖 (cid:205)=1
𝑛
𝑖=𝑎 1𝑖𝑢 𝑏𝑇
1
𝑖2𝑏
𝑖 =
⟨𝐴 ⟨𝑏𝑢 ,𝑇
1
𝑏, ⟩𝑏⟩
, (13) w hai nth d,c won hs et nra 𝑢in ists fio xn edt ,h te heri sg oh lt us tii on ngu fola rr 𝑠v ie sc ot bo tr as ino ef d𝐴 b. yO rn eat rh re ano gth ine gr
whichstates𝜆isisaleastsquaresestimateof𝐴𝑢𝑇 over𝑏. theconstraintsandsolvingaunivariateoptimizationproblem.This
1
Nowconsiderthemoregeneralcasewhen𝑘 >1.Thederivatives iterativeprocessensuresorthogonalityamongthevectors𝑠 𝑗.
with respect to the generic element𝑠 can be calculated easily
𝑖𝑗
duetotheconstrainton𝑈,whichsimplifiesthecomputation.The C ADDITIONALEXPERIMENTRESULTS
optimalsolutionforthegenericscore𝑠 isgivenby
𝑖𝑗 Weincludeanadditionalexperimentswithcontinuous𝑌 todemon-
𝑠
𝑖𝑗
=𝑎 𝑖𝑢𝑇
𝑗
−𝜆 𝑗𝑏 𝑖, (14) stratesomeadditionalpropertiesof OB.
since𝑢𝑇 𝑖𝑢 𝑗 =0forall𝑖 ≠ 𝑗 and𝑢𝑇 𝑗𝑢 𝑗 =1for𝑗 =1,....,𝑘.
C.1 Evaluationmetrics
Theglobalsolutionfor𝜆=(𝜆1,...𝜆 𝑘)canbederivedfromleast
squaresprojectionsincewecaninterpret(14)asamultivariatelin- Inthissection,weevaluatethemodelperformanceandfairness
earregressionwherethe𝑘columnsoftheprojectedmatrix𝐴𝑈𝑇 are usingthreekeymetrics:RootMeanSquareError(RMSE)andthe
responsevariablesand𝑎acovariant.Therefore,theoptimalvalue KL-divergencebetweenobserved(actual)datapredictionsandcoun-
forgeneral𝑘isthenequaltothemultipleleastsquaressolution terfactualdatapredictions.WealsoincludeVariableCorrelation
⟨𝐴𝑢𝑇,𝑏⟩
andFrobeniusnormofA(cid:101)−𝐴tovalidatetheeffectof OB.
𝜆 𝑘 = ⟨𝑏,𝑘 𝑏⟩ . (15) RMSE. RMSEisawidelyadoptedmetricforevaluatingprediction
Thisresultsintheclosed-formsolutionin(7).Foramorecom- performance.Iteffectivelyquantifiestheoverallaccuracyofour
pleteproofanddiscussionoftheimplicationsofthesolution,we model’spredictions.
10KL-Divergence. WeuseKL-divergencetomeasurethedistance OCobusenrtveerfdactual OCobusenrtveerfdactual OCobusenrtveerfdactual
betweenthedistributionofobserveddatapredictionsandcounter-
factualdatapredictions.Additionally,visualizationofthesedistri-
butionsservesasanintuitiveindicatorofcounterfactualfairness.
Ideally,ifourmodelsatisfiescounterfactualfairness,thesedistri-
KL Div. = 1.4238 KL Div. = 0.2843 KL Div. = 0.0145
butionsshouldperfectlyoverlap,resultinginaKL-divergenceof Y Prediction Y Prediction Y Prediction
0. (a)FTU (b)FL (c)OB
VariableCorrelations. Wecalculatetheaveragepairwisecorrela-
Figure6:Theredlinerepresentsthe𝑌ˆ
distributionutilizing
tionsbetweenvariables𝐴&𝐵overthegivenvariables.Correlation theobservedvariables,whilethebluelinerepresentsthe
closetozeroindicatesthatourframeworksuccessfullymitigates predictiondistributionutilizingthecounterfactualdata.(a)
theimpactof𝐵.
and(b)showsthedistributionsof𝑌ˆ
usingFTUandFLmethods
respectively,while(c)presentsthedistributionsof𝑌ˆ
using
OBprocesseddata.
C.2 Datadescription
Weconductadditionalexperimentsusingsyntheticdatasetsand
distributions:
areal-worlddataset.Here,weprovideanoverviewofthesedata
sets: 𝐺𝑃𝐴∼N(𝑏 𝑔+𝑤 𝐺𝐾𝐾+𝑤 𝐺𝑅𝑅,𝜎 𝐺),
𝐹𝑌𝐴∼N(𝑤𝐾𝐾+𝑤𝑅𝑅,1),
Syntheticdatasets. Thecasualgraphusedtogeneratethesyn- 𝐹 𝐹
theticdataisshowninFigure4a.Itiscraftedtosimulatehigh- 𝐿𝑆𝐴𝑇 ∼Poisson(𝑒𝑥𝑝(𝑏 𝐿+𝑤 𝐿𝐾𝐾+𝑤 𝐿𝑅𝑅)),
dimensionaldata,incorporatingalargernumberofvariables(𝑛= 𝐾 ∼N(0,1).
10,000,𝑞 =3,𝑝 =40,withadditional8featuresthatareindepen-
Weperforminferenceonthismodelusinganobservedtrainingset
dentofsensitivevariables).Itconsistof10,000samples,ensuringa
toestimatetheposteriordistributionof𝐾.Weusetheprobabilistic
substantialsamplesizeforanalysis.
programminglanguageStantolearn𝐾.WeutilizeKtopredictFYA.
Let𝑛denotethenumberofsamplesand𝑝 representthenumber
𝑎
offeaturesofA.Let𝑝 denotethenumberoffeaturesofBand
𝑏 C.3 SyntheticResults
𝑝 representthenumberoffeaturesofX,whichisunrelatedtoA
𝑥
Table4summarizestheresultsforthreemodelsonbothsynthetic
andB.Let𝐵 followtheBernoullidistributionwithaprobability
𝐵eq au na dlt 𝑋o0 ∼.7. NTh (0e ,n
𝑝
𝑎𝐴 ∗𝑗 =
𝑝
𝑏((cid:205) ∗0𝑖𝑝 =𝑏 .01 5𝐵 )𝑖 .+ Le𝜀 t) 𝑌∗( =𝑖∗ (cid:205)𝑗 𝑖𝑝) =𝑎. 1𝑋
𝐴
𝑖is +u (cid:205)nr 𝑖𝑝e =𝑥l 1at 𝑋e 𝑖d +w 𝜀i ,th
𝜀
d t ti oa vt Tea avs bae lr et is a 4a b ,n l oed u, rfr ae fi ra r al n md ea s est wa l oes a re krt n: eu i ffnn ega cw [ t2 iva 4 er ]e l, yam n reo d dd t ue h cl e, ew p sr th o hi pc eh o imsi eg pdn ao O cr B te . os A ft sch ece nos srde itn i in vs g ei-
isthenoiseand𝜀 ∼ N(0,0.5).Wesplitthedataset75/25intoa
variables𝐵on𝑌 and𝐴withminimalinformationlosswhileachiev-
train/testset.Forthecounterfactualdataset,weonlygenerate80%
ingdesirablepredictionperformance.Moredetaileddiscussions
counterfactualdataforallsensitivevariables𝐵 .
𝑖 arelistedasfollows:
Togeneratethedata,wefollowthecasualgraphasFigure4aand
set𝑛=10000,𝑝 𝑏 =3,𝑝 𝑎 =40,𝑝 𝑥 =8.Thecasualgraphissimilar PredictionPerformance. Inordertoachieveimprovedcounter-
withFigure4abutwithdifferentnumberofvariables.Bycomparing factualfairness,ourframeworkmakesatrade-offbyslightlysacri-
(c) and (d) to rest of the figures, we observe that OB effectively ficingpredictionperformance.AccordingtoTable4,theresulting
increasesoverlapoftheobservedandcounterfactualdistributions, degradationinpredictionperformanceislowintheboththehigh-
indicatingimprovedcounterfactualfairnesscomparedtoFTUorFL. dimensionalsyntheticdatasetandLSAT.Comparedtofairlearning,
theOBtechniquegainsgreaterflexibilityinadjustingthedatawith
LawSchooldataset. Thisreal-worlddatasetisderivedfroma minimalmodificationsinhigh-dimensionaldatamatrices,resulting
surveyconductedbytheLawSchoolAdmissionCouncilacross163 inlessdenttoperformanceinthesecases.
lawschoolsintheUnitedStates[32].Thecasualgraphisshown
CounterfactualFairness. KL-divergencemetricinTable4and
inFigure4b.Itcontainscomprehensiveinformationon21,790law
Figure6clearlyillustratethatOBachievesimprovedcounterfactual
students,includingtheirentranceexamscores(LSAT),grade-point
fairness,asevidencedbythedecreasedKL-divergenceandincreased
averages(GPA)collectedpriortolawschool,andtheirfirst-year
overlapbetweentheblueandreddistributions.Inparticular,in
averagegrade(FYA).Thedatasetservesthepurposeofpredicting
thehigh-dimensionalexperiment,ourframeworkexhibitsaminor
whetheranapplicantwillachieveahighFYA,whileensuringthat
degradationof3.43%inpredictionperformance,whilesignificantly
thesepredictionsremainunbiasedbyanindividual’sraceandsex.
enhancingcounterfactualfairness.Therefore,ourframeworkef-
However,theLSAT,GPA,andFYAscoresmayexhibitbiasdue
fectivelyisolatestheimpactofsensitivevariablesonnon-sensitive
to underlying social factors. We split the data set into training
variablesandoutcomes,whilemaintainingahighlevelofprediction
(80%)andtesting(20%)subsetstofitourmodel.Ourframeworkis
performance.
comparedagainstthefairlearning(FL)methodproposedby[24].
Wepostulatethatalatentvariable,astudent’sknowledge,af- VariableCorrelations. ToverifyhowOBaffectstherawdatasets,
fectsGPA,LSAT,andFYAscores.Thecasualgraphcorresponding wenotethecorrelationresultsdemonstratethatourframework
tothismodelisshowninFigure4b.Thisisashort-handforthe significantlydecreasesthecorrelationbetween𝐴&𝐵acrossalldata
11
ytisneD ytisneD ytisneDsets.Thisreductionindicatesthatourframeworksuccessfullymiti- between𝐵and𝑌.ThisindicatesthattheOGtechniquesuccessfully
gatestheimpactof𝐵. removestheimpactofthesensitivevariable,race,onGPA,LSAT,
andFYAwhileintroducingminimalchangestotheoriginaldataset.
C.4 LSATresult Furthermore,ourmodeloutperformsfairlearning(FL)intermsof
Inparticular,herewediscusstheimpactof OBonrealdataset. predictionperformance,asevidencedbysmallerRMSEandMAPE
In Table 4, we observe that our model achieves lower variable values.Figure6furtherdemonstratesourmodel’seffectiveness
correlationsbetweenthesensitivevariables𝐵 and𝐴,aswellas in achieving better counterfactual fairness, as indicated by the
increasedoverlapbetweentheredandbluedistributions.
12