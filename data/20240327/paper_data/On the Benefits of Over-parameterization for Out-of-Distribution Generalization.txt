On the Benefits of Over-parameterization for
Out-of-Distribution Generalization
Yifan Hao∗† Yong Lin∗‡ Difan Zou§ Tong Zhang¶
Abstract
In recent years, machine learning models have achieved success based on the independently and
identically distributed (IID) assumption. However, this assumption can be easily violated in real-world
applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-
parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current the-
oretical understanding is insufficient. Existing theoretical works often provide meaningless results for
over-parameterized models in OOD scenarios or even contradict empirical findings.
To this end, we are investigating the performance of the over-parameterized model in terms of OOD
generalization under the commonly-adopted “benign overfitting” conditions (Bartlett et al., 2019). Our
analysisfocusesonaReLU-basedrandomfeaturemodelandexaminesnon-trivialnaturaldistributional
shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achiev-
ing zero excess in-distribution (ID) loss. We demonstrate that in this scenario, further increasing the
model’sparameterizationcansignificantlyreducetheOODloss. Intuitively,thevariancetermoftheID
testing loss usually remains minimal because of the orthogonal characteristics of the long-tail features
in each sample. This implies that incorporating these features to over-fit training data noise generally
doesn’t notably raise the testing loss. However, in OOD situations, the variance component grows due
to the distributional shift. Thankfully, the inherent shift is unrelated to individual x, maintaining the
orthogonalityoflong-tailfeaturesdespitethischange. Expandingthehiddendimensioncanadditionally
improvethisorthogonalitybymappingthefeaturesintohigher-dimensionalspaces,therebyreducingthe
variance component.
We further show that model ensembles can also enhance the OOD testing loss, achieving a similar
effecttoincreasingthemodelcapacity. Theseresultsoffertheoreticalinsightsintotheintriguingempirical
phenomenon of significantly improved OOD generalization through model ensemble. We also provide
supportive simulations which are consistent with theoretical results.
1 Introduction
Inrecentyears,machinelearningwithmoderndeepneuralnetworks(DNN)architecturehasachievednotable
success and has found widespread applications in various domains like computer vision (Brown et al., 2020),
natural language processing (Radford et al., 2021), and autonomous driving (Yurtsever et al., 2020). A
commonfundamentalassumptionofmachinelearningistheidenticallyandindependently(IID)assumption,
which assumes that the testing data are driven from the same distribution with the training data. However,
IID assumption can easily fail in real-world application. For example, a autonomous driving car trained on
the data collected from city roads could also be required to navigate in the country-side roads. This is also
referred as the Out-of-Distribution (OOD) generalization problem. Existing empirical works show that the
performance of machine learning models can drop significantly due to distributional shifts.
∗Thefirsttwoauthorscontributedequally.
†TheHongKongUniversityofScienceandTechnology. Email: yhaoah@connect.ust.hk
‡TheHongKongUniversityofScienceandTechnology. Email: ylindf@connect.ust.hk
§TheUniversityofHongKong. Email: dzou@cs.hku.hk
¶UniversityofIllinoisUrbana-Champaign. Email: tongzhang@tongzhang-ml.org
1
4202
raM
62
]GL.sc[
1v29571.3042:viXraThough the OOD problem is of vital importance, the theoretical understanding of how machine learning
models (especially highly over-parameterized DNNs) perform under distributional shifts is mainly lacking.
What’s more, the prevailing generalization theory under distributional shifts even contradicts some crucial
empirical observations. Particularly intriguing is that practitioners repeatedly report that enlarging the
DNNs can consistently improve the OOD performance under non-trivial distributional shifts. However,
none of the existing theories can explain this phenomenon. Specifically, Ben-David et al. (2006) presents a
generalizationboundofmodelsinOODscenarios,wheretheupperboundofOODlossesincreasesvacuously
with the VC-dimension of the model. In contrast, Wald et al. (2022); Sagawa et al. (2020); Zhou et al.
(2022) offer contradictory analyses, showing that larger models may more easily rely on unstable features
(also referred to as spurious features (Arjovsky et al., 2019)), and such reliance could lead to model failure
under distributional shifts. Another mysterious empirical phenomenon in OOD generalization is that model
ensembles consistently improve the OOD performance (Wortsman et al., 2022b,a; Rame et al., 2022; Cha
etal.,2021;Arpitetal.,2022;Tianetal.,2023;Kumaretal.,2022;Linetal.,2023),repeatedlypushingthe
State-of-the-Art(SOTA)performanceofvariouslargescaleOODbenchmarkssuchasDomainBed(Gulrajani
and Lopez-Paz, 2020) and ImageNet variants (Wortsman et al., 2022b).
In this paper, we investigate the impact of over-parameterization for OOD generalization by considering
a natural shift. Let x ∈ Rp and y ∈ R denote the input and output. We want to learn a function f to
regress over y by f(x). Let D denote the training distribution of (x,y) where x ∼ D and y = g(x)+ϵ
x
by some unknown non-linear function g and random a Gaussian noise ϵ. We consider a distributional shift
parameterized by δ as follows:
L (f)= max E [ℓ(f(x),y)], (1)
ood (x,δ,y)∼D(∆)
∆∈Ξood
s.t. D(∆)={(x,δ,y)|x∼D ,ϵ∼D ,δ ∼∆,y =g(x+δ)+ϵ,δ is independent of (x,ϵ)},
x ϵ
where Ξ specifies the extent of allowed distributional shifts. The independence of δ from x and ϵ is as-
ood
sumed,andfurtherdetailsanddiscussionscanbefoundinSection3.2. Notably,aselaboratedinSection3.2,
we adopt a relatively mild constraint on Ξ by allowing for a compatible scaling of δ in comparison to x.
ood
Specific, in this model, as we will show in the later part, when we achieve optimal prediction loss (no excess
prediction loss) on the training domain, the excess OOD loss still remains at a constant level. Consistent
with the standard assumption in covariate shift (Ben-David et al., 2006), we assume that the true function
generating y, denoted as g(·), remains unchanged in the testing domain.
We consider a the predictor defined as f(x) = ϕ(x⊤W)θ where ϕ(x⊤W) is a random feature model
(i.e., W ∈ Rp×m is a random feature map) with an element-wise ReLU activation function (i.e., ϕ(a) =
ReLU(a) = max{0,a}, ∀a ∈ R), and our focus is on investigating the behavior of ”ridgeless” estimators of
θ in a region where m ≫ n. We consider the eigenvalues of x follow the “benign overfitting” conditions
(Bartlett et al., 2019), where the over-parameterized models achieve good ID performance. Specifically, we
observe that while the ID excess risk diminishes as the sample size n grows, following the phenomenon of
“benign overfitting”, the OOD excess risk remains consistently high, at a constant level. Furthermore, we
find that we can reduce such OOD excess risk by increasing the number of model parameters. Moreover, we
demonstratethatconstructinganensembleofmultipleindependentlyinitializedandtrainedmodelscanalso
be effective in reducing the OOD risk within this scenario, which is consistent with the empirical findings
(Wortsman et al., 2022a). Intuitively, the variance term of the ID testing loss is typically small due to
the orthogonal nature of the long-tail features in each sample, which means that fitting the training data
noise with these features does not significantly increase the testing loss (Shamir, 2022). However, in OOD
scenarios, the variance term is increased because of the distributional shift. Fortunately, the natural shift
is independent of x, so the orthogonality of long-tail features is preserved under this shift. Increasing the
hidden dimension can further enhance this orthogonality by projecting the features into higher-dimensional
spaces, thus reducing the variance term. Additional simulation results can be found in Section 4.
Our main results can be summarized as follows:
• Weofferaprecisenon-asymptoticanalysisofIDexcessriskandOODexcessrisk,providingbothupper
and lower bounds, for a random feature model with ReLU activation. Within the benign overfitting
2regime,asthemin-normestimatorisasymptoticallyoptimalinIDsituation,itbehavesunsatisfactorily
in OOD situations. In the aforementioned setting, the OOD excess risk exhibits non-trivial reduction
when the number of model parameters increases.
• Furthermore, we demonstrate that constructing an ensemble of multiple independently initialized and
trained models can also effectively reduce the OOD excess risk, achieving similar effects as those seen
with enlarging model capacity. This serves to explain the intriguing empirical findings that ensemble
models improve OOD generalization.
Our theoretical result is distinct from existing theories for several reasons:
• Our result differs from several existing theoretical viewpoints on over-parameterization for OOD gen-
eralization with conjecture that overparameterization may also lead to instability under distributional
shifts (Ben-David et al., 2006; Sagawa et al., 2020; Zhou et al., 2022; Wald et al., 2022).
• Thebehaviorofbenign-overfittingestimatorsundernaturalshiftsdiffersmarkedlyfromtheirbehavior
under adversarial attacks. While it has theoretically verified that increased over-parameterization
exacerbatesadversarialvulnerability (HaoandZhang,2024),ourworkdemonstrates,inthecontextof
natural shifts, overparameterization with benign overfitting can actually be advantageous for reducing
OOD loss.
The paper is structured as follows: we review related works in Section 2, present the model setting and
performance measurement methods in Section 3, showcase our main results in Section 4, provide sketches of
proofs in Section 5, and conclude with future discussions in Section 6.
2 Related work
Thereexistsasubstantialbodyofworkonimplicitbias,benignoverfitting,modelensemble,anddistribution
shifts. In this section, we review the most relevant works to ours.
2.1 Learning Theory
Implicit bias and benign overfitting. Severalrecentworkshavedelvedintothegeneralizationcapabil-
ities of large overparameterized models, particularly in the context of fitting noisy data (Neyshabur et al.,
2014; Wyner et al., 2017; Belkin et al., 2018, 2019; Liang and Rakhlin, 2020; Zhang et al., 2021; Cao et al.,
2023). These studies have uncovered that the implicit biases of various algorithms can contribute to their
favorable generalization properties, prompting a deeper investigation into their successes (Telgarsky, 2013;
Neyshabur et al., 2015; Keskar et al., 2016; Neyshabur et al., 2017; Wilson et al., 2017). Soudry et al.
(2018) and Ji and Telgarsky (2019) investigated the implicit bias of gradient descent in classification tasks,
whileGunasekaretal.(2018)exploredtheimplicitbiasofvariousoptimizationmethodsinlinearregression.
Additionally, Ji and Telgarsky (2018) delved into the implicit bias of deep neural networks, and Gunasekar
et al. (2017) and Arora et al. (2019) analyzed the implicit bias in matrix factorization problems. When
theseimplicitbiasareaccountedfor, aseriesofworkshaveemergedfocusingonthephenomenonof“benign
overfitting”, in both regression problems (Bartlett et al., 2019; Belkin et al., 2020; Muthukumar et al.,
2020; Liang and Rakhlin, 2020; Zou et al., 2021b,a; Shamir, 2022; Tsigler and Bartlett, 2023; Simon et al.,
2023; Hao and Zhang, 2024) and classification problems (Chatterji and Long, 2021; Muthukumar et al.,
2021; Wang et al., 2021; Wang and Thrampoulidis, 2022; Cao et al., 2022; Chen et al., 2023). Our work
is partly inspired by the setup and analysis presented in Bartlett et al. (2019), Tsigler and Bartlett (2023)
and Hao and Zhang (2024). However, while Bartlett et al. (2019) and Tsigler and Bartlett (2023) primarily
explore the consistency of estimators in “benign overfitting” regime, and Hao and Zhang (2024) verified the
adversarial sensitivity of such estimators, our work stands out as the first explicit exploration within the
context of distribution shifts.
3Model ensemble. Our analysis demonstrates that an ensemble of several independently trained mod-
els can achieve similar improvement on OOD generalization performance with increased parameterization.
Modelensemblehasbeenapopulartechniquetoenhancesgeneralizationperformance,asdocumentedinthe
existingliterature (HansenandSalamon,1990;KroghandVedelsby,1994;PerroneandCooper,1995;Opitz
and Maclin, 1999; Dietterich, 2000; Zhou et al., 2002; Polikar, 2006; Rokach, 2010). Empirical works have
extensively explore the remarkable efficacy of model ensemble (Wortsman et al., 2022b,a; Rame et al., 2022;
Cha et al., 2021; Arpit et al., 2022; Tian et al., 2023; Kumar et al., 2022; Lin et al., 2023). Another line of
research focuses on developing boosting algorithms, which also rely on ensemble-based methods (Freedman,
1981; Breiman, 1996; Freund and Schapire, 1997; Friedman, 2001; Zhang and Yu, 2005; Rodriguez et al.,
2006; Kolter and Maloof, 2007; Galar et al., 2011; Kuncheva, 2014; Bol´on-Canedo and Alonso-Betanzos,
2019), and these works are orthogonal to ours findings.
There is a limited number of works that attempt to theoretically explain the effectiveness of ensemble
methods. Brown et al. (2005) decomposes the prediction error of ensemble models into bias, variance and
a covariance term between individual models, proposing algorithms to encourage the diversity of individual
models to reduce the covariance term. Allen-Zhu and Li (2020) proposes a multi-view theory to explain the
effectiveness of ensemble of deep models trained with gradient descent from different initialization, whereas,
their analysis relies on a very specific data structure, assuming limited number (e.g., less than 10) of latent
features,eachdatapointcontainingasubsetoftheselatentfeatures. AmorerecentstudybyLinetal.(2023)
illustratesthatwhentwomodelsutilizedistinctsetsoflatentfeatures, theirensemblecanharnessabroader
range of latent features, referred to as feature diversification. This diversification can lead to improvements
in OOD generalization, indicating that the enhancement in OOD performance may be attributed to feature
diversification. Our model, compared with Allen-Zhu and Li (2020) and Lin et al. (2023), is more general
as we do not impose specific structural assumptions on the latent features or the number of latent features
learned by each individual model.
2.2 Generalization Under Non-IID Distributions
Typically, robustness under non-IID distribution is characterized as follows:
sup E [ℓ(f(x,y))],
(x,y)∼D
D∈D
where D is a set of distributions. The set D outlines the potential distribution perturbations to which the
estimator should exhibit robustness. If D encompasses arbitrary distributions, it could lead to inconclusive
results. Therefore, it is customary to impose constraints on the potential shifts included in D.
Distribution Robust Optimization and Adversarial Attacks. The field of Distributional Robust
Optimization (DRO) focuses on the uncertainty set D(D ,ϵ) = {D : M(D,D ) ≤ ϵ}, wherein M represents
0 0
a distance measurement. Typically, the value of ϵ is small. Examples of distance measurements include the
Wasserstein distance (Kuhn et al., 2019; Mohajerin Esfahani and Kuhn, 2018; Esfahani and Kuhn, 2015;
Luo and Mehrotra, 2017), ϕ-divergence (Hu and Hong, 2013; Namkoong and Duchi, 2016; Levy et al., 2020;
DuchiandNamkoong,2021;StaibandJegelka,2019),andothers. Keskaretal.(2016);NamkoongandDuchi
(2016); Qi et al. (2021); Mehta et al. (2023); Zhang et al. (2024) propose efficient optimization algorithms
for solving the DRO problem. Meanwhile, Sinha et al. (2017); Duchi and Namkoong (2021); An and Gao
(2021);Kuhnetal.(2019)explorethegeneralizationabilityofDROestimators. Notably,Sinhaetal.(2017)
establishesaconnectionbetweenWassersteindistanceDROandadversarialexamples. Adversarialexamples
involve making slight perturbations to the input x with the goal of maximizing the performance drop for a
given model f,
(cid:20) (cid:21)
L (f)=E max ℓ(f(x+δ,y)) . (2)
adv (x,y)∼D
δ∈∆adv
4HaoandZhang(2024)showsthat“benignoverfitting”estimatorsareoverlysensitivetoadversarialattacks.
Specifically, asthesamplesizengrows, theadversariallossL (f)woulddivergetoinfinity, evenwhenthe
adv
estimator is benign on generalization performance and the ground truth model is adversarially robust.
Natural Distributional Shifts. The natural distributional shifts is closely aligned with the set of distri-
butional shifts defined by causal graphs (Gong et al., 2016; Huang et al., 2020; Peters et al., 2016; Arjovsky
etal.,2019;Linetal.,2022b;Heinze-DemlandMeinshausen,2017). Eachnodeinthecausalgraphrepresents
a (potentially latent) feature or an outcome Pearl (2009). Distributional shifts arise from conducting do-
interventionsonthecausalgraphsPearl(2009);Petersetal.(2016);Arjovskyetal.(2019). Itisbelievedthat
the reliance on certain nodes in the causal graph may lead to models being sensitive to distributional shifts.
To address this, significant research has focused on developing models that depend on a resilient subset of
causal nodes (Arjovskyet al.,2019; Gonget al.,2016; Linet al.,2022b;Ganin etal., 2016;Lin etal., 2022a;
Ahuja et al., 2020). Our work is orthogonal to these works by examining how over-parameterization affects
generalizationunderdistributionalshifts. Thebehaviorofnaturaldistributionalshifts(Moayerietal.,2022)
differs from that in adversarial examples which introduce synthetic perturbations on the input. Moreover,
the work of Moayeri et al. (2022) reveals that a trade-off exists between the robustness against adversarial
attacksandtheabilitytohandlenaturalshifts(Moayerietal.,2022). Thoughshiftsinthecausalgraphcan
leadtochangesinbothP(x)andP(y|x). However,inourstudy,wealignwiththecommonassumptioninco-
variate shift research, which onlyconsiders changes in P(x) whilemaintaining P(y|x) unchanged(Sugiyama
et al., 2007; Gretton et al., 2008; Bickel et al., 2009; Wu et al., 2022).
3 Preliminary and Settings
Notation. For any matrix A, we use ∥A∥ to denote its ℓ operator norm, use tr{A} to denote its trace,
2 2
use ∥A∥ to denote its Frobenius norm, and use A† denotes its Moore-Penrose pseudoinverse. The j−th
F
row of A is denoted as A , and the j−th column of A is denoted as A . The i−th largest eigenvalue of A is
j· ·j
denoted as µ (A). The transposed matrix of A is denoted as AT. And the inverse matrix of A is denoted as
i
A−1. For any set C, we use |C| to denote the number of components within C. The notation a=o(b) means
that a/b→0; similarly, a=ω(b) means that a/b→∞. For a sequence of random variables {v }, v =o (1)
s s p
pr.
refers to v → 0 as s → ∞, and the notation γ v = o (1) is equivalent to v = o (1/γ ); v = O (1) refers
s s s p s p s s p
to lim sup P(|v |≥M)=0, similarly, γ v =O (1) is equivalent to v =O (1/γ ).
M→∞ s s s s p s p s
3.1 Data Settings
We consider regression tasks where n i.i.d. training examples (x ,y ),...,(x ,y ) from distribution D take
1 1 n n
values in Rp×R. Here, y is generated based on x with an unknown non-linear function g(·):Rp →R:
y :=g(x)+ϵ. (3)
We adopt the following assumptions on {(x ,y )}n :
i i i=1
1. x = Σ1/2η , where Σ := E[x xT] = diag[λ ,...,λ ], and the components of η are independent
i i i i 1 p i
σ -subgaussian random variables with mean zero and unit variance;
x
2. E[y |x ]=g(x ) (as already stated in (3));
i i i
3. E[y −g(x )|x ]2 =E[ϵ ]2 =σ2 >0.
i i i i
Without loss of generality, we assume λ ≥λ ≥···≥λ >0 on the covariance matrix Σ. Then similar to
1 2 p
the definition in Bartlett et al. (2019), the effective rank could be defined for each non-negative integer k:
(cid:80)
λ
r := i>k i , (4)
k λ
k+1
5where the critical index for a given b>0 is
k∗(b):=inf{k ≥0:r ≥bn}. (5)
k
3.2 Distributional Shift
For the OOD situation, we consider an addictive covariate shift δ on x, meaning the input variable changes
from x to x+δ. We assume that the conditional probability P[y|x] remains unchanged in OOD scenarios,
i.e., y =g(x+δ)+ϵ in OOD. We consider that
Assumption 1 (Independence of the Shift). δ is independent of observer x and noise ϵ, i.e,
EδxT =0, Eϵδ =0.
Discussion on the Data Generalization Process. It is common to only consider changes in P(x)
while maintaining P(y|x) unchanged (Ben-David et al., 2006). For example, consider our task is to predict
the weight (i.e., y) of a man based his height (i.e., x). The training data is collected in country A (the
training domain) while the testing data is from country B (the testing domain). It should be noted that
the distribution of height in the testing domain differs from that in the training domain. For instance, men
from country B are generally taller than those from country A. This difference is quantified by δ, and the
distributionofδ inourcontextisdependentonthedomainbutindependentofx(asillustratedinthecausal
graph given by Figure 1). Following the common assumption in covariate shift literature, we consider the
underlying function g that relates height to weight, and this function remains the same in both the testing
and training domains (Sugiyama et al., 2007; Gretton et al., 2008; Bickel et al., 2009).
While we denote δ ∼∆∈Ξ , it is crucial to consider a feasible distribution set Ξ for the potential
ood ood
shift. If Ξ is too small, the OOD loss will not significantly differ from the ID loss. Conversely, if we
ood
consider an excessively large Ξ encompassing arbitrary shifts, it becomes impossible to derive meaningful
ood
conclusions. In this paper, we consider a shift strength that allows for a shift δ comparable to the input
x in terms of their eigenvalues. This is due to the fact that natural shifts typically result in perceptual
differences in the samples from ID and OOD domains. Considering the conditions of the data in Bartlett
et al. (2019), where they separately examine “small” and “large” eigenvalues of the input x, we also impose
assumptions on δ for the “small” and “large” eigenvalues, respectively. Specifically, let b>0 be a constant,
denote the indexes of “large” eigenvalues as C = {i : λ > tr{Σ}/(bn)}, the corresponding eigenvectors
1 i
could concat a matrix Σ
C1
∈R|C1|×p, then the projection of matrix A∈Rp×p on the subspace spanning on
Σ could be denoted as Π A:=ΣT (Σ ΣT )−1Σ A; similarly, denote the indexes of “small” eigenvalues
C1 C1 C1 C1 C1 C1
as C = {i : λ ≤ tr{Σ}/(bn)}, we also have the projection Π A on the subspace spanning on their
2 i C2
corresponding eigenvectors. Notably, recalling Eq. (5) and the benign overfitting condition in Bartlett et al.
(2019), we have
(cid:80)
λ ≤ j>k∗λ j =⇒λ ≤ l =⇒|C |≤k∗ ≪n.
k∗+1 bn k∗+1 bn 1
With C and C defined above, we assume
1 2
Assumption 2 (The Strength of the Shift). Denoting Σ = EδδT ∈ Rp×p with eigenvalues {α ,...,α },
δ 1 p
we have ∥Σ ∥ ≤τ and the following constraints:
δ 2
shifts on the directions of “large” eigenvalues: tr{Π (Σ Σ−1)}≤τ,
C1 δ
shifts on the directions of “small” eigenvalues: ∥Π Σ ∥ ≤τ∥Π Σ∥ .
C2 δ 2 C2 2
with some constant τ >0.
For simplifying the analysis, here we consider δ is zero-mean and Σ as a diagonal matrix, and similar
δ
results could be shown with milder constraints.
6Figure 1: Illustration of the causal graph.
Discussion on the Data Model We consider a relatively large set of Ξ where the eigenvalues of the
ood
shift δ are comparable with those of the input x. Specifically, the constraints on Σ could be explained as
δ
follows:
(1) ∥Σ ∥ ≤τ, the maximum shift on each direction of eigenvalues should be within a constant level.
δ 2
(2) tr{Π (Σ Σ−1)} ≤ τn. We allow for a relatively large shift on the large eigenvalues. If we consider
C1 δ
(cid:80)
diagonal matrices for both Σ and Σ, we have α /λ ≤ τn. The average value of α /λ on the
δ i∈C1 i i i i
directions of “large” eigenvalues should be smaller than τn/|C |. As is mentioned above, the number
1
of large eigenvalues (λ ≥ tr{Σ}/(bn)), i.e., |C |, is smaller than k∗. Furthermore, following Bartlett
i 1
et al. (2019) which considers k∗/n → 0 in the benign overfitting region, we then conclude that the
number of large eigenvalues is significantly smaller than both sample size n and data dimension p. So
we allow the average value of α /λ in C up to τn/|C | which goes to ∞.
i i 1 1
(3) ∥Π Σ ∥ ≤ τ∥Π Σ∥ . The spectral norm of δ on the directions of small eigenvalues is within a
C2 δ 2 C2 2
constant level compared with that of x.
Empirical Observations Inordertoinvestigatenaturaldistributionshifts, weconductedananalysisus-
ing observations from the DomainNet datasets (Peng et al., 2019). DomainNet comprises multiple domains,
each of which contains images from specific distributions. For instance, the “real domain” encompasses
photos captured in real-world settings, while the “quickdraw” domain contains drawings created by players
of the global game “Quick Draw!”(see Figure 3). For each domain, we extract features from the observed
imagesusingapre-trainedResNet18(Heetal.,2016)andthencalculatetheeigenvaluesofthecorresponding
covariance matrix. To simplify, we designate the ”real” domain as in-domain and denote the eigenvalues of
thecovariancematrixinthe“real”domainas[λ ,...,λ ]. Asanillustration, let’sconsiderthe“quickdraw”
1 p
domain. We first obtain the eigenvalues in the “quickdraw” domain as [λ′,...,λ′] and then quantify the
1 p
difference between the “quickdraw” and “real” domain using the eigenvalue ratio: [λ′/λ ,...,λ′/λ ]. Simi-
1 1 p p
larly, we also calculate the eigenvalue ratios for the “clipart”, “sketch”, and “infograph” domains using the
“real”domainasthebase. Weobservethesedifferencesacrossfourdomains, withdetailedresultspresented
inFigure3. Ourobservationsrevealthatwhendistributionshiftsoccur, thediscrepanciesineigenvaluesare
significant, particularly with notable changes in several “large” eigenvalues. These findings are consistent
with Assumption 2.
7Figure 2: Examples of image data in different domains.
The Difference Between Natural Shifts and Adversarial Attacks Comparing the natural shift
formulation in Equation (1) and adversarial attacks in Equation (2), a clear distinction arises in which the
perturbation δ of adversarial examples, as defined in Equation (2), is dependent on each example x in an
adversarial manner. It is important to note that for over-fitting to be considered “benign”, the long tail
feature of each sample must be orthogonal. Over-fitting the label noise in the training data using these
features would not have a substantially negative effect on the testing data (Shamir, 2022). However, the
adversarial perturbation δ could disrupt the orthogonality, resulting in a significant increase in testing loss.
For instance, in a simple linear regression problem with E[y|x]=xTθ∗, where we have a benign estimator θˆ,
the adversarial perturbation δ that solves max ((x+δ)⊤θˆ−θ∗Tx)2 is given by δ(x)=(θˆθˆT)†(θˆθ∗T −θˆθˆT)x.
δ
Consequently, δ(x) encompasses the information of training data (from the estimator θˆ), thus breaking the
orthogonality of the long tail features. Hao and Zhang (2024) also demonstrate that over-parameterization
leads to sensitivity to adversarial examples. In contrast, the orthogonality of long tail features is preserved
in OOD situation, as natural distributional shifts do not adversarially explore the long tail features of
each sample. As we will demonstrate later on, even when considering a large scaling distributional shift δ
comparable to the input x, increased hidden dimensions can still benefit the OOD loss.
3.3 Model and Performance Measurement
To estimate the target y, we study a random feature model with parameter W ∈Rp×m in this work:
1
f (θ,x)= √ ϕ(xTW)θ,
W m
where ϕ(z) = max{0,z} is an element-wise ReLU function, and all of the elements on W are i.i.d. sam-
pled from Gaussian distribution N(0,1/p). Accordingly, θ∗(W) is denoted the minimizer of expected in-
distribution (ID) mean square error (MSE):
1
θ∗(W)=arg min E [y−f (θ,x)]2 =arg min E [y− √ ϕ(xTW)θ]2,
θ∈Rm x,y W θ∈Rm x,y m
where we use W in θ∗(W) to explicitly denote the dependency of θ∗(W) on W. We assume the estimation
ability of random feature models is strong enough to satisfy that
sup |g(x)−f (θ∗(W),x)|≤ϱ=o(1). (6)
W
x∈Rp
8clipart infograph
1.5
1.4
1.4
1.3
1.3
1.2
1.2
1.1
1.1
1.0
1.0
0.9
0 100 200 300 400 500 0 100 200 300 400 500
dimension index dimension index
quickdraw sketch
1.5
1.1
1.4
1.0
1.3
0.9
1.2
0.8
1.1
0.7
1.0
0.6
0.9
0 100 200 300 400 500 0 100 200 300 400 500
dimension index dimension index
Notes. Hereweanalyzetheeigenvaluesacrossfourdistinctdomains,i.e,“clipart”,”infograph”,“quickdraw”and“sketch”,in
comparisontotheeigenvaluesinthe“real”domain. Thesolidblacklineillustratestheeigenvalueratiosforeachdimension
index,whilethedashedredlinerepresentsthebaselineeigenvalueratioof1.
Figure 3: Comparison of covariance matrix eigenvalues in different domain.
Given samples {(x ,y )}n , we obtain the min-norm estimator θˆ(W) as
i i i=1
θˆ(W)=ΦT (Φ ΦT )−1y,
W W W
whereΦ
W
= √1 m[ϕ(Wx 1),...,ϕ(Wx n))]T ∈Rn×m. TheIDperformanceismeasuredbyexcessmeansquare
error
(cid:104) (cid:105)2 (cid:20) 1 (cid:16) (cid:17)(cid:21)2
L (f (θˆ(W),x))=E f (θˆ(W),x)−f (θ∗(W),x) =E √ ϕ(xTW) θˆ(W)−θ∗(W) . (7)
id W x,y W W x,y m
For OOD situation, let θ∗(W) denotes the optimal θ given a specific δ ∼ ∆ ∈ Ξ , i.e., θ∗(W) =
δ ood δ
argmin θ∈RmE x,δ,y[y − √1 mϕ((x+δ)TW)θ]2, we are interested in the maximum OOD excess risk of θˆ(W)
with respect to the optimal θ∗(W):
δ
(cid:104) (cid:105)2
max E f (θˆ(W),x+δ)−f (θ∗(W),x+δ) .
x,δ,y W W δ
∆∈Ξood
Furthermore,takingEq.(6)intoconsideration,wecanseethatf (θ∗(W),x)isclosetotheoptimalestimator
W δ
f (θ∗(W),x) in ID domain since the mapping function g between x and y remains unchanged in ID and
W
9
oitar
eulavnegie
oitar
eulavnegie
oitar
eulavnegie
oitar
eulavnegieOOD domains, i.e, for any δ, we have
1 1
E [√ ϕ((x+δ)TW)θ∗(W)− √ ϕ((x+δ)TW)θ∗(W,δ)]2
x,y,δ m m
1 1
=E [√ ϕ((x+δ)TW)θ∗(W)−g(x+δ)+g(x+δ)− √ ϕ((x+δ)TW)θ∗(W,δ)]2
x,y,δ m m
1 1
≤2E [√ ϕ((x+δ)TW)θ∗(W)−g(x+δ)]2+2E [g(x+δ)− √ ϕ((x+δ)TW)θ∗(W,δ)]2
x,y,δ m x,y,δ m
1
≤4E [√ ϕ((x+δ)TW)θ∗(W)−g(x+δ)]2
x,y,δ m
1
≤4E sup|√ ϕ((x+δ)TW)θ∗(W)−g(x+δ)|2 ≤4ϱ2 =o(1),
x,y,δ m
so we cold measure the OOD performance by
(cid:104) (cid:105)2
L (f (θˆ(W),x))= max E f (θˆ(W),x+δ)−f (θ∗(W),x+δ)
ood W x,δ,y W W
∆∈Ξood
(8)
(cid:20)
1
(cid:21)2
= max E √ ϕ((x+δ)TW)(θˆ(W)−θ∗(W)) .
∆∈Ξood x,δ,y m
3.4 Model Ensemble
As introduced in Section 1, there has been repeated empirical observations showing that ensemble models
could achieve superior OOD performance (Wortsman et al., 2022a,b; Lin et al., 2023; Rame et al., 2022).
So we are also interested in investigating the effectiveness of model ensemble for OOD generalization. The
ensemble model is defined as the average of K outputs related to K “ridgeless” estimators in independently
trained single models with corresponding parameters {W ,...,W }. To be specific, for single models
1 K
1
f (θ ,x)= √ ϕ(xTW )θ ,∀r =1,...,K,
Wr r m r r
the ensemble model is defined as
K K
1 (cid:88) 1 (cid:88)
f (θ ,...,θ ,x)= f (θ ,x)= √ ϕ(xTW )θ .
ens 1 K K Wr r K m r r
r=1 r=1
Recalling the min-norm estimators {θˆ(W ),...,θˆ(W )} on each single model, we explore the generalization
1 K
performance of
K K
f (θˆ(W ),...,θˆ(W ),x)= 1 (cid:88) f (θˆ(W ),x)= √1 (cid:88) ϕ(xTW )θˆ(W ),
ens 1 K K Wr r K m r r
r=1 r=1
then according to (7) and (8), the ID and OOD performance are measured respectively by
(cid:34) K (cid:35)2
L (f (θˆ(W ),...,θˆ(W ),x))=E √1 (cid:88) ϕ(xTW )(θˆ(W )−θ∗(W )) ,
id ens 1 K x,y K m r r r
r=1
(cid:34) K (cid:35)2
L (f (θˆ(W ),...,θˆ(W ),x+δ))= max E √1 (cid:88) ϕ((x+δ)TW )(θˆ(W )−θ∗(W )) .
ood ens 1 K ∆∈Ξood x,y,δ K m
r=1
r r r
Without loss of generality, we focus our analysis on the case where K = 2, and extending our findings to
other choices of K is straightforward.
104 Main Results
Following Bartlett et al. (2019), we focus on the “benign overfitting” phase with the assumptions bellow:
Assumption 3 (benign matrix). There exist some constants ξ >0 and b>0 such that for k∗ =k∗(b),
r (Σ) k∗ n1+ξ(cid:80) λ2
lim 0 = lim = lim i i =0.
n→∞ n n→∞ n n→∞ ((cid:80) iλ i)2
Assumption 3 is compatible with the assumption in Bartlett et al. (2019), which characterizes the slow
decreasing rate on covariance eigenvalues {λ }. Moreover, Assumption 4 is also required in further analysis:
i
Assumption 4 (High-dimension condition). Here we consider the relationships among n,p,m,l are as
follows:
n≤p1/4, tr{Σ}≫n3/4, n≫lnm, m≥p.
Assumption4describesthehigh-dimensionsetupondatax,aswellastheoverparameterizedstructureof
random feature models. To show the compatibility of Assumption 3 and 4, we verify them on two examples
from Bartlett et al. (2019).
Example 1. Suppose the eigenvalues as

1,k =1,

1 1+s2−2scos(kπ/(p+1))
λ = , 2≤k ≤p,
k n21/5 1+s2−2scos(π/(p+1))
0,otherwise,
where p=n5. We could obtain k∗ =1 and 0<ξ <4.
Example 2. Suppose the eigenvalues as
λ =k−5/6, 1≤k <p,
k
where p=n5. We could obtain k∗ =n1/5 and 0<ξ <2/3.
The detailed calculations are in Appendix A, and it is easy to design other similar examples satisfying
Assumption 3 and Assumption 4.
Our first main result could be stated below, shows that while the distribution shift on x is significant,
even there is a near-optimal ID performance in “benign overfitting” regime, the “benign” estimator always
leads to a non-converging OOD excess risk (the detailed proof is in Appendix B).
Theorem 1. For any σ ,b,ξ,ϱ>0, there exist ξ′ =min{1/2,ξ/2} and constants C ,C >0 depending only
x 1 2
on σ ,b,ξ,ϱ, such that the following holds. Assume Assumption 1, 2, 3 and 4 are satisfied, there exists a
x
constant c>1 such that for δ ∈(0,1) and ln(1/δ)<nξ′/c, with probability at least 1−δ over X,W ,W ,
1 2
(cid:40)
tr{Σ}∥θ∗(W )∥2
(cid:32)
1 k∗
n(cid:80) λ2(cid:33)(cid:41)
L (f (θˆ(W ),x))≤C r 2 +σ2 + + j>k∗ j , r =1,2,
id Wr r 1 p n1/4 n1/8 n tr{Σ}2
and
(cid:40) (cid:80) (cid:41)
L (f (θˆ(W ),x))≥C σ2τ p +σ2τ λj≤tr{Σ}/(bn)λ j , r =1,2.
ood Wr r 2 m tr{Σ}
Given a target y with constant scaling and θ∗(W ) satisfying (6), we have ∥θ∗(W )∥ = O(p/tr{Σ}),
r r 2
which implies the bias term tr{Σ}·∥θ∗(W )∥2/(pn1/4) in ID excess risk has a convergence rate O(n−1/4).
r 2
Combining this observation with the converged performance of variance term induced by Assumption 3, we
can directly derive the following Corollary 2:
11Corollary 2. For any σ ,b,ξ,ϱ > 0, there exist ξ′ = min{1/2,ξ/2} and some constant C > 0 depending
x 3
only on σ ,b,ξ,ϱ, such that the following holds. Assume Assumption 1, 2, 3 and 4 are satisfied and p/m=
x
O(1), we have
lim L (f (θˆ(W ),x))=0, r =1,2,
n→∞
id Wr r
(cid:40) (cid:80) (cid:41)
lim L (f (θˆ(W ),x))≥C σ2τ p +σ2τ λj≤tr{Σ}/(bn)λ j =O(1), r =1,2.
n→∞ ood Wr r 3 m tr{Σ}
As the adversarial risk may escalate with increasing model capacity, one might inquire whether the
behavioroftheOODriskissimilarasthehiddendimensionmgrowsorthemodelisensembled. Theanswer
is negative, which is induced from the following Theorem 3. Before delving into the results, we introduce a
notation to denote the improvement of ensemble models on OOD risk:
(cid:80)K L (f (θˆ(W ),x))/K−L (f (θˆ(W ),...,θˆ(W ),x))
R := r=1 ood Wr r ood ens 1 K .
K (cid:80)K L (f (θˆ(W ),x))/K
r=1 ood Wr r
It can be readily seen that larger R implies a more significant improvement of the K-ensemble model over
K
single models. Then, the OOD performance of the ensemble model is stated as follows:
Theorem 3. For any σ ,b,ξ,ϱ>0, there exist ξ′ =min{1/2,ξ/2} and constants C ,C >0 depending only
x 4 5
on σ ,b,ξ,ϱ, such that the following holds. Assume Assumption 1, 2, 3 and 4 are satisfied, there exists a
x
constant c>1 such that for δ ∈(0,1) and ln(1/δ)<nξ′/c, with probability at least 1−δ over X,W ,W ,
1 2
(cid:40) (cid:80) (cid:41)
L (f (θˆ(W ),x))≤C τE ∥∇ g(x)T∥2+σ2τ(cid:16) p +1(cid:17) +σ2τ λj≤tr{Σ}/(bn)λ j , r =1,2,
ood Wr r 4 x x 2 m tr{Σ}
and
C σ2τ ·p/m
R ≥ 5 .
2 2 τE ∥∇ g(x)∥2+σ2τ(p/m+1)+σ2τ(cid:80) λ /tr{Σ}
x x 2 λj≤tr{Σ}/(bn) j
ThedetailedproofisinAppendixC.Thisresultimmediatelyimpliesthefollowingconsequence: although
havingamodelcapacityofm=pissufficientforachievingnear-optimalperformance(L (f (θˆ(W ),x))→pr.
id Wr r
0),itisnotenoughforachievinggoodOODperformance;asweincreasethehiddendimensionmdirectly,or
enlarge the model capacity by ensemble procedure, the increases in the number of parameters could benefit
OOD risk.
Remark 1. The decrease on OOD excess risk is related to p/m. To be specific, increasing the hidden dimen-
sion m results in a decrease in OOD excess risk for the single models, but the corresponding improvement in
OODperformanceforensemblemethodsismoremodest. Inanextremescenariowherep/m→0,ensembling
does not lead to a reduction in OOD excess risk.
Remark 2. If we consider ensemble on K single models, the improvement proportion in Theorem 3 should
be
(cid:18) 1 (cid:19) σ2τ ·p/m
R ≥C 1− ,
K 5 K τE ∥∇ g(x)T∥2+σ2τ(p/m+1)+σ2τ(cid:80) λ /tr{Σ}
x x 2 λj≤tr{Σ}/(bn) j
which suggests that by ensembling more models, we can expand the model capacity further, resulting in a
greater decrease in OOD excess risk.
12Simulations. Weutilizemultiplenumericalsimulationstodemonstratetheadvantagesofenhancedhidden
dimensions and ensemble methods for OOD generalization, as depicted in Figure 4. For clarity, we conduct
four simulations, each with 40 training samples and 1000 test samples. The data dimension is set to p=40.
In these simulations, we consider two types of distribution on x, i.e, N(0,Σ ) and N(0,Σ ), where Σ has
1 2 1
eigenvalues as λ = 1 and λ = ··· = λ = 0.25, and Σ has eigenvalues as λ = i−5/12. The ground
1 2 p 2 i
truth models are defined as g (x) = βTx and g (x) = log(1 + eβTx), where ∥β∥ = 1. We introduce
1 2 2
data noise ϵ ∼ N(0,0.0052) and OOD perturbation δ ∼ N(0,4) into the simulations. In each simulation,
corresponding to various feature numbers m, we iterate the experiment 500 times and compute the average
L loss. The results presented in Figure 4 show that: (i). as the ID loss reaches a satisfactory level, the
2
associated OOD loss tends to be large; (ii). increasing the hidden dimension or employing ensemble models
leads to a reduction in OOD loss; (iii). with the escalation of hidden dimension m, the enhancement in
OOD performance from a single model to an ensemble model becomes less pronounced, moreover, when m
issufficientlylarge,thisenhancementbecomesmarginal. Theseobservationsareconsistentwiththefindings
outlined in Theorem 3.
140
id: model 1 id: model 1
120 id: model 2 60 id: model 2
id: ensemble model id: ensemble model
100 ood: model 1 50 ood: model 1
ood: model 2 ood: model 2
80 ood: ensemble model 40 ood: ensemble model
30
60
40 20
20 10
0 0
200 400 600 800 1000 1200 200 400 600 800 1000 1200
feature number feature number
(a) x∼N(0,Σ ),y=βTx+ϵ. (b) x∼N(0,Σ ),y=log(1+eβTx)+ϵ.
1 1
140
id: model 1 id: model 1
120 id: model 2 60 id: model 2
id: ensemble model id: ensemble model
100 ood: model 1 50 ood: model 1
ood: model 2 ood: model 2
80 ood: ensemble model 40 ood: ensemble model
60 30
40 20
20 10
0 0
200 400 600 800 1000 1200 200 400 600 800 1000 1200
feature number feature number
(c) x∼N(0,Σ ),y=βTx+ϵ. (d) x∼N(0,Σ ),y=log(1+eβTx)+ϵ.
2 2
Notes. HeresolidlinesrepresentIDlosses,whiledashedlinesrepresentOODlosses. Theblueandgreenlinescorrespondto
resultsfromtwoindividualmodels,whereastheredlinespertaintoresultsfromtheensemblemodel.
Figure 4: Loss decreasing.
13
ssol
ssol
ssol
ssol5 Overview of Proof Technique
The proof sketches for Theorem 1 and Theorem 3 are summarized in this section. For simplify, we use c
i
and c′ to denote positive constants that only depend on σ ,b,ξ,ϱ.
i x
First, we recall the decomposition Σ =(cid:80) λ e eT and obtain that
i i i i
(cid:88) (cid:88)
XXT = λ z zT, XΣXT = λ2z zT,
i i i i i i
i i
in which
1
z := √ Xe
i i
λ
i
are independent σ -subgaussian random vectors in Rn with mean 0 and covariance I. Then we will take the
x
following notations in following analysis:
(cid:88) (cid:88)
A=XXT, A = λ z zT, A = λ z zT.
k i i i −k i i i
i>k i̸=k
5.1 Technical Lemmas
Before presenting the proof sketches for the main theorems, we outline the key technical lemmas that are
employed in our analysis. The proofs are in Appendix D.
Lemma 1 (Refinement of Theorem 2.1 in El Karoui, 2010). Let we assume that we observe n i.i.d. random
vectors, x ∈Rp. Let us consider the kernel matrix K with entries
i
xTx
K =f( i j ).
i,j tr{Σ}
We assume that:
1. n,p,tr{Σ} satisfy Assumption 3 and 4;
2. Σ is a positive-define p×p matrix, and ∥Σ∥ remains bounded;
2
3. x = Σ1/2η , in which η ,i = 1,...,n are σ -subgaussian i.i.d. random vectors with Eη = 0 and
i i i x i
Eη ηT =I ;
i i p
4. f is a C1 function in a neighborhood of 1 and a C3 function in a neighborhood of 0.
Under these assumptions, the kernel matrix K can in probability be approximated consistently in operator
norm, when p and n tend to ∞, by the kernel k˜, where
(cid:18) tr(Σ2) (cid:19) f′(0)
K˜ = f(0)+f′′(0) 11T + XXT +v I ,
2tr{Σ}2 tr{Σ} p n
v =f(1)−f(0)−f′(0).
p
In other words, with probability at least 1−4n2e−n1/8/2,
∥K−K˜∥ ≤o(n−1/16).
2
Lemma 2. Assume w ,...,w are sampled i.i.d. from N(1,1/pI ), then with probability at least 1 −
1 m p
2e−nξ/2/4, we have
P(cid:0) |wTΣw −E(wTΣw )|(cid:1) ≤ tr{Σ} , ∀i,j =1,...,m.
i j i j pn(2+ξ)/4
14Lemma 3. Assume z ∈ Rq is a q-dim sub-gaussian random vector with parameter σ, and E[z] = µ. Here
areni.i.d.samplesz ,...,z , whichhavethesamedistributionasz, thenwecanobtainthatwithprobability
√ 1 n
at least 1−4e− n,
∥EzzT −
1 (cid:88)n
z zT∥ ≤∥EzzT∥
max{(cid:114) trace(EzzT) ,trace(EzzT)
,
1 }+2√ 2σ∥µ∥
2.
n i i 2 2 n n n1/4 n1/4
i=1
Lemma 4 (Lemma 10 in Bartlett et al., 2019). There are constants b,c′ ≥1 such that, for any k ≥0, with
1
−n
probability at least 1−2e c′ 1,
1. for all i≥1,
(cid:88)
µ (A )≤µ (A)≤µ (A )≤c′( λ +λ n);
k+1 −i k+1 1 k 2 j k+1
j>k
2. for all 1≤i≤k,
1 (cid:88)
µ (A)≥µ (A )≥µ (A )≥ λ −c′λ n;
n n −i n k c′ j 2 k+1
2 j>k
3. if r ≥bn, then
k
1
λ r ≤µ (A )≤µ (A )≤c′λ r ,
c′ k+1 k n k 1 k 2 k+1 k
2
where c′ >1 is a constant only depending on b,σ .
2 x
Lemma 5 (Corollary 24 in Bartlett et al., 2019). For any centered random vector z ∈Rn with independent
σ2 sub-Gaussian coordinates with unit variances, any k dimensional random subspace L of Rn that is
x
independent of z, and any t>0, with probability at least 1−3e−t,
√
∥z∥2 ≤n+2(162e)2σ2(t+ nt),
x
√
∥ΠLz∥2 ≥n−2(162e)2σ x2(k+t+ nt),
where ΠL is the orthogonal projection on L.
5.2 Proof Sketch for Theorem 1
The proof mainly contains three steps as follows.
Step 1 : kernel matrix linearizatioin. With kernel estimation results of Lemmas 21 and 22 in Jacot
et al. (2018), we could approximate kernel matrix K =Φ ΦT ∈Rn×n with r =1,2 as
Wr Wr
 (cid:115) 
(cid:18) 1 (cid:19) 1 xTx (cid:18) xTx (cid:19) ∥x ∥ ∥x ∥ (cid:18) xTx (cid:19)2
K s,t = 1+O p(√ m) p 2s πt arccos − ∥x ∥s ∥xt ∥ + s 22 π t 2 1− − ∥x ∥s ∥xt ∥ ,
s 2 t 2 s 2 t 2
foranys,t=1,...,n. Furthermore,withAssumption3andAssumption4,wecouldusekernellinearization
techniques ( Lemma 1) to approximate K by K˜:
tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1
K˜ = ( + 0 )11T + XXT + ( − )I .
p 2π 4πtr{Σ}2 4p p 4 2π n
15Step 2: Upper bound for ID excess risk. For simplicity, here we just take analysis on f (θˆ(W ),x),
W1 1
and the analysis on f (θˆ(W ),x) is similar. The excess ID risk could be decomposed as
W2 2
L (f (θˆ(W ),x))
id W1 1
1
=E [√ ϕ(xTW )(θˆ(W )−θ∗(W ))]2
x,y m 1 1 1
1
= θ(W )∗T[I−ΦT (Φ ΦT )−1Φ ]E ϕ(WTx)ϕ(WTx)T[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
m 1 W1 W1 W1 W1 x 1 1 W1 W1 W1 W1 1
(cid:124) (cid:123)(cid:122) (cid:125)
Bid
1
+ (σ2+o(1))trace{(Φ ΦT )−2Φ E ϕ(WTx)ϕ(WTx)TΦT }.
m W1 W1 W1 x 1 1 W1
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
For the bias term Bid, it could be expressed as
Bid =θ∗(W )[I−ΦT (Φ ΦT )−1Φ ]
1 W1 W1 W1 W1
(cid:18) (cid:19)
1 1
E ϕ(WTx)ϕ(WTx)T − ΦT Φ [I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
m x 1 1 n W1 W1 W1 W1 W1 W1 1
1 1
≤∥θ∗(W )∥2∥ E ϕ(WTx)ϕ(WTx)T − ΦT Φ ∥ ,
1 2 m x 1 1 n W1 W1 2
where the inequality is induced from ∥I − ΦT (Φ ΦT )−1Φ ∥ ≤ 1 and aTBa ≤ ∥a∥2∥B∥ for any
W1 W1 W1 W1 2 2 2
positive-defined matrix B. With the bounded Lipschitz of ReLU function ϕ(·), we could verrify that with
a high probability, the random vector √1 mϕ(W 1Tx) is σ x(cid:112) tr{Σ}/p-subgaussian with respect to x. Consider
Lemma 2 and Lemma 3, with a high probability, we have
∥θ∗(W )∥2tr{Σ}
Bid ≤c 1 2 ,
1 n1/4 p
with some constant c >0.
1
For the variance term Vid, it could be expressed as
1
Vid =(σ2+o(1))trace{(Φ ΦT )−2Φ E ϕ(WTx)ϕ(WTx)TΦ }
W1 W1 W1 xm 1 1 W1
σ2+o(1) (cid:88)n
= E trace{(Φ ΦT )−2Φ E ϕ(WTx′)ϕ(WTx′)TΦT }
nm x′ 1,···x′ n W1 W1 W1 x 1 i 1 i W1
i=1
σ2+o(1)
= E trace{K−2Φ Φ′T Φ′ ΦT },
n x′ 1,...,x′ n W1 W1 W1 W1
where we denote x′,...,x′ are n i.i.d. samples from the same distribution as x ,...,x , and Φ′ =
1 n 1 n W1
[ϕ(WTx′),...,ϕ(WTx′ )]T. Similar to the linearized approximation on K, we could approximate Φ Φ′T
1 1 1 n W1 W1
and Φ′ ΦT as:
W1 W1
tr{Σ}(cid:18) 1 3r (Σ2) (cid:19) 1 4tr{Σ}
∥Φ Φ′T − + 0 11T + XX′T∥ ≤ ,
W1 W1 p 2π 4πtr{Σ}2 4p 2 pn1/16
tr{Σ}(cid:18) 1 3r (Σ2) (cid:19) 1 4tr{Σ}
∥Φ′ ΦT − + 0 11T + X′XT∥ ≤ ,
W1 W1 p 2π 4πtr{Σ}2 4p 2 pn1/16
where X′ =[x′,...,x′ ]T ∈Rn×p, and it implies that
1 n
1 tr{Σ}2 1 32tr{Σ}2
Q:= E Φ Φ′T Φ′ ΦT ≺ (1+o(1))11T + XΣXT + I .
n x′ 1,...,x′ n W1 W1 W1 W1 2π2p2 8p2 p2n9/8 n
16Then according to several inequalities of matrix trace calculation in Lemma 10, we could approximate Vid
by σ2tr{K˜−2Q}, and the upper bound for this term contains three parts as follows:
tr{Σ}2 (cid:18) tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1 (cid:19)−2
trace{K˜−2Q}≤ (1+o(1))1T ( + 0 )11T + XXT + ( − )I 1
2π2p2 p 2π 4πtr{Σ}2 4p p 4 2π n
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
1
(cid:40)(cid:18)
1 tr{Σ} 1 1
(cid:19)−2(cid:18)
1
(cid:19)(cid:41)
+trace XXT + ( − )I XΣXT
4p p 4 2π n 8p2
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
2
(cid:40)(cid:18) 1 tr{Σ} 1 1 (cid:19)−2(cid:18) 32tr{Σ}2 (cid:19)(cid:41)
+trace XXT + ( − )I I .
4p p 4 2π n p2n9/8 n
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
3
With Woodbury identity, we could upper bound the first term Vid as
1
tr{Σ}2 (1+o(1))1TR˜−21 21TR˜−21 2n/λ (R˜)2
Vid = 2π2p2 ≤ ≤ n ,
1 (1+ tr{ pΣ}( 21
π
+ 43 πr t0 r( {Σ Σ2 }) 2)1TR˜−11)2 (1TR˜−11)2 n2/λ 1(R˜)2
where we denote
1 tr{Σ} 1 1
R˜ := XXT + ( − )I .
4p p 4 2π n
With Assumption3 and Assumption4, recalling the boundsfor matrix eigenvalues ( Lemma 4), witha high
probability, we have
c tr{Σ} c (tr{Σ}+n)
2 ≤λ (R˜)≤λ (R˜)≤ 3 ,
p n 1 p
with some constants c ,c >0. Then combining with the fact that
2 3
1TR˜−21≤nλ (R˜−1)2 =n/λ (R˜)2, 1TR˜−11≥nλ (R˜−1)=n/λ (R˜),
1 n n 1
we could obtain that
c
Vid ≤ 4 ,
1 n1/2
with some positive constant c .
4
Then we turn to term Vid. Consider the bounds for matrix eigenvalues (Lemma 4) and the results in
2
Lemma 20 in Bartlett et al. (2019), with a high probability, we could obtain that
k∗ c n(cid:80) λ2
Vid =2trace{(XXT +tr{Σ}(1−2/π)I )−2XΣXT}≤c + 6 i>k∗ i,
2 n 5 n tr{Σ}2
with some constants c ,c >0.
5 6
And the last term Vid could be upper bounded as
3
c tr{Σ}2 c tr{Σ}2 n c
Vid = 7 trace{(XXT +tr{Σ}(1−2/π)I )−2}≤ 7 ≤ 8 ,
3 n9/8 n n9/8 µ (XXT +tr{Σ}(1−2/π)I )2 n1/8
n n
where the last inequality is due to µ (XXT +tr{Σ}(1−2/π)) ≥ tr{Σ}(1−2/π). And combing all of the
n
estimation for Bid,Vid,Vid and Vid, we could get the upper bound for ID excess risk.
1 2 3
17Step 3: Lower bound for OOD risk. Similarly, for the OOD risk, we take the following decomposition
first:
L (f (θˆ(W ),x))
ood W1 1
(cid:20)
1
(cid:21)2
= max E √ ϕ((x+δ)TW )(θˆ(W )−θ∗(W ))
∆∈Ξood x,δ,y m 1 1 1
1
=L (f (θˆ(W ),x))+ max E [δT∇ ϕ(WTx)(θˆ(W )−θ∗(W ))]2
id W1 1 m∆∈Ξood x,δ,ϵ x 1 1 1
=L (f (θˆ(W ),x))+ max (cid:8) Bood+Vood(cid:9) ,
id W1 1
∆∈Ξood
where we take first-order Taylor expansion with respect to x and:
1
Bood = θ∗T(W )[I−ΦT (Φ ΦT )−1Φ ]E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W ),
m 1 W1 W1 W1 W1 x x 1 δ x 1 W1 W1 W1 W1 1
1
Vood =(σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }.
m W1 W1 W1 x x 1 δ x 1 W1
The ID risk L (f (θˆ(W ),x)) = o(1), and Bood is related to the relationship between ground truth model g(x)
id W1 1
and δ. So to focus on the impact of overfitting process, we could just focus on Vood to obtain a lower bound. With
Assumption 3 and Assumption 4, we could approximate Vood as
tr{Σ } 1
σ2 δ tr{K−1}+σ2 tr{K−2XΣ XT},
2pm 16p2 δ
which could be further estimated as
tr{Σ } 1
σ2 δ tr{K˜−1}+σ2 tr{K˜−2XΣ XT}.
2pm 16p2 δ
For the term tr{K˜−1}, we could approximate it as 4ptr{(XXT +l(1−2/π)I )−1} and use Woodbury identity to
n
take lower bound
λ zT(A +tr{Σ}(1−2/πI))−2z
tr{(XXT +tr{Σ}(1−2/π)I)−1}=tr{(A +tr{Σ}(1−2/π)I)−1}− 1 1 −1 1
−1 1+λ zT(A +tr{Σ}(1−2/π)I)−1z
1 1 −1 1
=tr{A k∗
+tr{Σ}(1−2/π)I)−1}−(cid:88)k∗
1+λ
i
λz iT z( TA
(i
A+ +tr{ trΣ {} Σ( }1 (− 1−2/ 2π /) πI )) I− )2 −z
1i
z
i=1 i i i i
≥tr{A k∗
+tr{Σ}(1−2/π)I)−1}−(cid:88)k∗
z ziT T( (A Ai+ +t tr r{ {Σ Σ} }( (1 1− −2 2/ /π π) )I I) )− −2 1z
zi,
i=1 i i i
then with the bounds of eigenvalues and random vectors (Lemma 4 and Lemma 5), we could control the norm of z
i
and the eigenvalues of A , which induces the lower bound as
i
np
tr{K˜−1}≥c ,
8tr{Σ}
with some constant c >0. And its upper bound could be estimated as
8
n np
tr{K˜−1}≤ ≤c .
µ (K˜) 9tr{Σ}
n
Then we turn to the approximation for the term
tr{K˜−2XΣ XT}≈16p2tr{(XXT +tr{Σ}(1−2/π)I )−2XΣ XT},
δ n δ
andtheanalysisissimilartotheprocessonVid. Tobespecific,withWoodburyidentity,wecouldexpressthisterm
2
as
tr{(XXT +tr{Σ}(1−2/π)I)−2XΣ XT}=(cid:88) λ α zT(XXT +tr{Σ}(1−2/π)I)−2z
δ i i i i
i
=(cid:88) λ iα iz iT(A −i+tr{Σ}(1−2/π)I)−2z i .
(1+λ zT(A +tr{Σ}(1−2/π)I)−1z )2
i i i −i i
18WithLemma4andLemma5,wecontrolthenormofz ,aswellastheeigenvaluesofA ,thenconsiderAssumption3
i −i
and Assumption 4, with a high probability, the whole term tr{K˜−2XΣ XT} could be bounded as
δ
 (cid:80) 
tr{K˜−2XΣ δXT}≥c 10p2  (cid:88) nα λi + n λj≤ ttr r{ {Σ Σ} }/( 2bn)λ jα j ,
i
λi>tr{Σ}/(bn)
 
(cid:80)
tr{K˜−2XΣ δXT}≤c 11p2  (cid:88) nα λi + n λi≤ tt rr{ {Σ Σ} }/ 2(bn)λ iα i .
i
λi>tr{Σ}/(bn)
AfterobtainingtheupperandlowerboundsforBood,tr{K˜−1}andtr{K˜−2XΣ XT},tofurtherestimateOODrisk,
δ
we provide bounds for tr{Σ } here:
δ
tr{Σ
}=(cid:88)
α
≤τk∗+τptr{Σ}
≤τλ
n+τptr{Σ} ≤2τptr{Σ}
,
δ i n 1 n n
i
(cid:88) tr{Σ}(p−k∗) ptr{Σ} tr{Σ} tr{Σ}
tr{Σ }≥ α ≥τ ≥τ , while α =τ ∀i s.t. λ ≤ .
δ i n 2n i n i bn
λi≤tr{Σ}/(bn)
Summarizing all of the results above, we could finish the proof for OOD risk.
5.3 Proof Sketch for Theorem 3
The proofs for Theorem 3 are similar to the analysis in Theorem 1, which contains the following two steps:
Step 1 : Upper bound for OOD risk. First, with Eq. (6), we could upper bound the term Bood as
1
Bood ≤ E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2
m 1 1 1 1
1 1
=E[√ ϕ(WTx)Tθ∗(W )−g(x)+g(x)−g(x+δ)+g(x+δ)− √ ϕ(WT(x+δ))Tθ∗(W )]2
m 1 1 m 1 1
1 1
≤4E[√ ϕ(WTx)θ∗(W )−g(x)]2+4E[g(x+δ)− √ ϕ(WT(x+δ))Tθ∗(W )]2+4E[g(x)−g(x+δ)]2
m 1 1 m 1 1
=8ϱ2+4E[∇ g(x)Tδ]2 ≤8ϱ2+4τE∥∇ g(x)T∥2,
x x 2
which is up to O(1), due to the assumptions on ∥∇ g(x)∥ and δ. And the upper bound for Vood, as well as
x 2
tr{K−1} and tr{K−2XΣ XT}, has been established in the proof of Theorem 1. Summarie all of these estimations
δ
and Assumption 1, 2 about Ξ , we could obtain an upper bound for L (f (θˆ ,x)).
ood ood r r
Step 2: Proof sketch for ensemble model. The OOD risk on ensemble model could be decomposed as
L (f (θˆ(W ),θˆ(W ),x))
ood ens 1 2
(cid:20) 1 (cid:16) (cid:17)(cid:21)2
= max E √ ϕ((x+δ)TW )(θˆ(W )−θ∗(W ))+ϕ((x+δ)TW )(θˆ(W )−θ∗(W ))
∆∈Ξood x,y,δ 2 m 1 1 1 2 2 2
= max E {term 1+term 2},
x,y,δ
∆∈Ξood
where term 1 is corresponding to ID risk, so we could obtain
(cid:20)
1 1
(cid:21)2
term 1=E √ ϕ(xTW )(θˆ(W )−θ∗(W ))+ ϕ(xTW )(θˆ(W )−θ∗(W ))
x,y 2 m 1 1 1 2 2 2 2
1 (cid:104) (cid:105)2 1 (cid:104) (cid:105)2
≤ E ϕ(xTW )(θˆ(W )−θ∗(W )) + E ϕ(xTW )(θˆ(W )−θ∗(W ))
2m x,y 1 1 1 2m x,y 2 2 2
1(cid:16) (cid:17)
= L (f (θˆ(W ),x))+L (f (θˆ(W ),x)) →0,
2 id W1 1 id W2 2
19and term 2 can be approximated by
(cid:20)
1
(cid:21)2
term 2=E √ (δT∇ ϕ(WTx)(θˆ(W )−θ∗(W ))+δT∇ ϕ(WTx)(θˆ(W )−θ∗(W )))
x,y,δ 2 m x 1 1 1 x 2 2 2
1 (cid:104) (cid:105)2
= E δT∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )+δT∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
4 δ,x x 1 W1 W1 W1 W1 1 x 2 W2 W2 W2 W2 2
(cid:124) (cid:123)(cid:122) (cid:125)
term2.1
1
+(σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }
4m W1 W1 W1 x x 1 δ x 1 W1
(cid:124) (cid:123)(cid:122) (cid:125)
term2.2
1
+(σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }
4m W2 W2 W2 x x 2 δ x 2 W2
(cid:124) (cid:123)(cid:122) (cid:125)
term2.3
1
+(σ2+o(1)) trace{(Φ ΦT )−1(Φ ΦT )−1Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }.
2m W1 W1 W2 W2 W1 x x 1 δ x 2 W2
(cid:124) (cid:123)(cid:122) (cid:125)
term2.4
Term 2.1 is related to the average of bias term Bood, to be specific,
(cid:18) (cid:19)
1 1 1
term 2.1≤ E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2+ E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2
2 m 1 1 1 1 m 2 2 2 2
1(cid:16) (cid:17)
= Bood(f )+Bood(f ) ≤c τE ∥∇ g(x)T∥2+o(1),
2 W1 W2 12 x x 2
and for the other three terms, similar to the analysis on Vood, with Assumption 3 and Assumption 4, we could
approximate term 2.2 + term 2.3 as
tr{Σ } 1
σ2 δ tr{K−1}+σ2 tr{K−2XΣ XT},
4pm 32p2 δ
and term 2.4 could be approximated as
1
σ2 tr{K−2XΣ XT}.
32p2 δ
TheanalysisaboveshowsthatwecouldestimatethedifferencebetweenensemblemodelOODriskandsinglemodel
OOD risk as
L (f (θˆ(W ),x))+L (f (θˆ(W ),x)))/2−L (f (θˆ(W ),θˆ(W ),x)
ood W1 1 ood W2 2 ood ens 1 2
tr{Σ } τtr{Σ}
≈ max σ2 δ tr{K−1}≥ tr{K−1},
∆∈Ξood 4pm 8mn
then with the upper and lower bounds for Bood, tr{K˜−1} and tr{K˜−2XΣ XT}, we could finish the proof for Theo-
δ
rem 3.
6 Conclusion
In this study, we investigate the impact of over-parameterization on OOD loss. Surprisingly, we find that increasing
over-parameterization can actually improve generalization under significant shifts in natural distributions. This
discovery is unexpected because it demonstrates that the impact of over-parameterization on natural shifts differs
significantly from its impact on adversarial examples. While increased over-parameterization exacerbates a model’s
susceptibility to adversarial attacks, it actually proves beneficial in the context of natural shifts.
References
Ahuja, K., Shanmugam, K., Varshney, K., and Dhurandhar, A. (2020). Invariant risk minimization games. In
International Conference on Machine Learning, pages 145–155. PMLR.
20Allen-Zhu,Z.andLi,Y.(2020). Towardsunderstandingensemble,knowledgedistillationandself-distillationindeep
learning. arXiv preprint arXiv:2012.09816.
An, Y. and Gao, R. (2021). Generalization bounds for (wasserstein) robust optimization. Advances in Neural
Information Processing Systems, 34:10382–10392.
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint
arXiv:1907.02893.
Arora, S., Cohen, N., Hu, W., and Luo, Y. (2019). Implicit regularization in deep matrix factorization. Advances in
Neural Information Processing Systems, 32.
Arpit,D.,Wang,H.,Zhou,Y.,andXiong,C.(2022). Ensembleofaverages: Improvingmodelselectionandboosting
performance in domain generalization. Advances in Neural Information Processing Systems, 35:8265–8277.
Bai, Z. D. (2008). Methodologies in spectral analysis of large dimensional random matrices, a review. In Advances
in statistics, pages 174–240. World Scientific.
Bartlett,P.L.,Long,P.M.,Lugosi,G.,andTsigler,A.(2019). Benignoverfittinginlinearregression. arXivpreprint
arXiv:1906.11300v3.
Belkin, M., Hsu, D., Ma, S., and Mandal, S. (2019). Reconciling modern machine-learning practice and the classical
bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854.
Belkin, M., Hsu, D., and Xu, J. (2020). Two models of double descent for weak features. SIAM Journal on
Mathematics of Data Science, 2(4):1167–1180.
Belkin, M., Ma, S., and Mandal, S. (2018). To understand deep learning we need to understand kernel learning. In
International Conference on Machine Learning, pages 541–549. PMLR.
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2006). Analysis of representations for domain adaptation.
Advances in neural information processing systems, 19.
Bickel, S., Bru¨ckner, M., and Scheffer, T. (2009). Discriminative learning under covariate shift. Journal of Machine
Learning Research, 10(9).
Bolo´n-Canedo, V. and Alonso-Betanzos, A. (2019). Ensembles for feature selection: A review and future trends.
Information Fusion, 52:1–12.
Breiman, L. (1996). Bagging predictors. Machine learning, 24:123–140.
Brown, G., Wyatt, J. L., Tino, P., and Bengio, Y. (2005). Managing diversity in regression ensembles. Journal of
machine learning research, 6(9).
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing
systems, 33:1877–1901.
Cao, Y., Chen, Z., Belkin, M., and Gu, Q. (2022). Benign overfitting in two-layer convolutional neural networks.
Advances in neural information processing systems, 35:25237–25250.
Cao, Y., Zou, D., Li, Y., and Gu, Q. (2023). The implicit bias of batch normalization in linear models and two-
layer linear convolutional neural networks. In The Thirty Sixth Annual Conference on Learning Theory, pages
5699–5753. PMLR.
Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y., and Park, S. (2021). Swad: Domain generalization by
seeking flat minima. Advances in Neural Information Processing Systems, 34:22405–22418.
Chatterji, N. S. and Long, P. M. (2021). Finite-sample analysis of interpolating linear classifiers in the overparame-
terized regime. The Journal of Machine Learning Research, 22(1):5721–5750.
21Chen, J., Cao, Y., and Gu, Q. (2023). Benign overfitting in adversarially robust linear classification. In Uncertainty
in Artificial Intelligence, pages 313–323. PMLR.
Dietterich, T. G. (2000). Ensemble methods in machine learning. In International workshop on multiple classifier
systems, pages 1–15. Springer.
Duchi, J. C. and Namkoong, H. (2021). Learning models with uniform performance via distributionally robust
optimization. The Annals of Statistics, 49(3):1378–1406.
El Karoui, N. (2010). The spectrum of kernel random matrices. The Annals of Statistics, 38(1):1–50.
Esfahani,P.M.andKuhn,D.(2015). Data-drivendistributionallyrobustoptimizationusingthewassersteinmetric:
Performance guarantees and tractable reformulations. arXiv preprint arXiv:1505.05116.
Freedman, D. A. (1981). Bootstrapping regression models. The Annals of Statistics, 9(6):1218–1228.
Freund, Y. and Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to
boosting. Journal of computer and system sciences, 55(1):119–139.
Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, pages
1189–1232.
Galar,M.,Fernandez,A.,Barrenechea,E.,Bustince,H.,andHerrera,F.(2011). Areviewonensemblesfortheclass
imbalanceproblem: bagging-,boosting-,andhybrid-basedapproaches. IEEE Transactions on Systems, Man, and
Cybernetics, Part C (Applications and Reviews), 42(4):463–484.
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., March, M., and Lempitsky, V.
(2016). Domain-adversarial training of neural networks. Journal of machine learning research, 17(59):1–35.
Gong, M., Zhang, K., Liu, T., Tao, D., Glymour, C., andScho¨lkopf, B. (2016). Domainadaptation with conditional
transferable components. In International conference on machine learning, pages 2839–2848. PMLR.
Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borgwardt, K., and Sch¨olkopf, B. (2008). Covariate shift by
kernel mean matching.
Gulrajani, I. and Lopez-Paz, D. (2020). In search of lost domain generalization. arXiv preprint arXiv:2007.01434.
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. (2018). Characterizing implicit bias in terms of optimization
geometry. In International Conference on Machine Learning, pages 1832–1841. PMLR.
Gunasekar,S., Woodworth, B.E., Bhojanapalli,S.,Neyshabur, B.,andSrebro, N.(2017). Implicitregularizationin
matrix factorization. Advances in neural information processing systems, 30.
Hansen,L.K.andSalamon,P.(1990).Neuralnetworkensembles.IEEEtransactionsonpatternanalysisandmachine
intelligence, 12(10):993–1001.
Hao, Y. and Zhang, T. (2024). The surprising harmfulness of benign overfitting for adversarial robustness. arXiv
preprint arXiv:2401.12236.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 770–778.
Heinze-Deml, C. and Meinshausen, N. (2017). Conditional variance penalties and domain shift robustness. arXiv
preprint arXiv:1710.11469.
Hu,Z.andHong,L.J.(2013). Kullback-leiblerdivergenceconstraineddistributionallyrobustoptimization. Available
at Optimization Online, 1(2):9.
Huang, B., Zhang, K., Zhang, J., Ramsey, J., Sanchez-Romero, R., Glymour, C., and Scho¨lkopf, B. (2020). Causal
discovery from heterogeneous/nonstationary data. The Journal of Machine Learning Research, 21(1):3482–3534.
Jacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural
networks. Advances in neural information processing systems, 31.
22Ji, Z. and Telgarsky, M. (2018). Gradient descent aligns the layers of deep linear networks. arXiv preprint
arXiv:1810.02032.
Ji, Z. and Telgarsky, M. (2019). The implicit bias of gradient descent on nonseparable data. In Conference on
Learning Theory, pages 1772–1798. PMLR.
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2016). On large-batch training for
deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836.
Koltchinskii, V. and Lounici, K. (2017). Concentration inequalities and moment bounds for sample covariance
operators. Bernoulli, pages 110–133.
Kolter,J.Z.andMaloof,M.A.(2007). Dynamicweightedmajority: Anensemblemethodfordriftingconcepts. The
Journal of Machine Learning Research, 8:2755–2790.
Krogh, A. and Vedelsby, J. (1994). Neural network ensembles, cross validation, and active learning. Advances in
neural information processing systems, 7.
Kuhn, D., Esfahani, P. M., Nguyen, V. A., and Shafieezadeh-Abadeh, S. (2019). Wasserstein distributionally robust
optimization: Theory and applications in machine learning. In Operations research & management science in the
age of analytics, pages 130–166. Informs.
Kumar, A., Ma, T., Liang, P., and Raghunathan, A. (2022). Calibrated ensembles can mitigate accuracy tradeoffs
under distribution shift. In Uncertainty in Artificial Intelligence, pages 1041–1051. PMLR.
Kuncheva, L. I. (2014). Combining pattern classifiers: methods and algorithms. John Wiley & Sons.
Levy, D., Carmon, Y., Duchi, J. C., and Sidford, A. (2020). Large-scale methods for distributionally robust opti-
mization. Advances in Neural Information Processing Systems, 33:8847–8860.
Liang, T. and Rakhlin, A. (2020). Just interpolate: Kernel “ridgeless” regression can generalize.
Lin, Y., Dong, H., Wang, H., and Zhang, T. (2022a). Bayesian invariant risk minimization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16021–16030.
Lin, Y., Tan, L., Hao, Y., Wong, H., Dong, H., Zhang, W., Yang, Y., and Zhang, T. (2023). Spurious feature
diversification improves out-of-distribution generalization. arXiv preprint arXiv:2309.17230.
Lin,Y.,Zhu,S.,Tan,L.,andCui,P.(2022b). Zin: Whenandhowtolearninvariancewithoutenvironmentpartition?
Advances in Neural Information Processing Systems, 35:24529–24542.
Luo,F.andMehrotra,S.(2017). Decompositionalgorithmfordistributionallyrobustoptimizationusingwasserstein
metric. arXiv preprint arXiv:1704.03920.
Mehta, R., Roulet, V., Pillutla, K., and Harchaoui, Z. (2023). Distributionally robust optimization with bias &
variance reduced gradients. In The Twelfth International Conference on Learning Representations.
Moayeri, M., Banihashem, K., and Feizi, S. (2022). Explicit tradeoffs between adversarial and natural distributional
robustness. Advances in Neural Information Processing Systems, 35:38761–38774.
MohajerinEsfahani,P.andKuhn,D.(2018). Data-drivendistributionallyrobustoptimizationusingthewasserstein
metric: performance guarantees and tractable reformulations. Mathematical Programming, 171(1-2):115–166.
Muthukumar, V., Narang, A., Subramanian, V., Belkin, M., Hsu, D., and Sahai, A. (2021). Classification vs
regression in overparameterized regimes: Does the loss function matter? The Journal of Machine Learning
Research, 22(1):10104–10172.
Muthukumar, V., Vodrahalli, K., Subramanian, V., and Sahai, A. (2020). Harmless interpolation of noisy data in
regression. IEEE Journal on Selected Areas in Information Theory, 1(1):67–83.
Namkoong, H. and Duchi, J. C. (2016). Stochastic gradient methods for distributionally robust optimization with
f-divergences. Advances in neural information processing systems, 29.
23Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning.
Advances in neural information processing systems, 30.
Neyshabur,B.,Salakhutdinov,R.R.,andSrebro,N.(2015). Path-sgd: Path-normalizedoptimizationindeepneural
networks. Advances in neural information processing systems, 28.
Neyshabur, B., Tomioka, R., and Srebro, N. (2014). In search of the real inductive bias: On the role of implicit
regularization in deep learning. arXiv preprint arXiv:1412.6614.
Opitz, D. and Maclin, R. (1999). Popular ensemble methods: An empirical study. Journal of artificial intelligence
research, 11:169–198.
Pearl, J. (2009). Causality. Cambridge university press.
Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. (2019). Moment matching for multi-source domain
adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406–1415.
Perrone, M. P. and Cooper, L. N. (1995). When networks disagree: Ensemble methods for hybrid neural networks.
InHowWeLearn; HowWeRemember: TowardAnUnderstandingOfBrainAndNeuralSystems: SelectedPapers
of Leon N Cooper, pages 342–358. World Scientific.
Peters, J., Bu¨hlmann, P., and Meinshausen, N. (2016). Causal inference by using invariant prediction: identification
andconfidenceintervals. JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,78(5):947–1012.
Polikar, R. (2006). Ensemble based systems in decision making. IEEE Circuits and systems magazine, 6(3):21–45.
Qi, Q., Guo, Z., Xu, Y., Jin, R., and Yang, T. (2021). An online method for a class of distributionally robust
optimization with non-convex objectives. Advances in Neural Information Processing Systems, 34:10067–10080.
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,Clark,J.,
et al. (2021). Learning transferable visual models from natural language supervision. In International conference
on machine learning, pages 8748–8763. PMLR.
Rame, A., Kirchmeyer, M., Rahier, T., Rakotomamonjy, A., Gallinari, P., and Cord, M. (2022). Diverse weight
averaging for out-of-distribution generalization. arXiv preprint arXiv:2205.09739.
Rodriguez,J.J.,Kuncheva,L.I.,andAlonso,C.J.(2006). Rotationforest: Anewclassifierensemblemethod. IEEE
transactions on pattern analysis and machine intelligence, 28(10):1619–1630.
Rokach, L. (2010). Ensemble-based classifiers. Artificial intelligence review, 33:1–39.
Sagawa, S., Raghunathan, A., Koh, P. W., and Liang, P. (2020). An investigation of why overparameterization
exacerbates spurious correlations. In International Conference on Machine Learning, pages 8346–8356. PMLR.
Shamir,O.(2022). Theimplicitbiasofbenignoverfitting. InConferenceonLearningTheory,pages448–478.PMLR.
Simon, J. B., Karkada, D., Ghosh, N., and Belkin, M. (2023). More is better in modern machine learning: when
infinite overparameterization is optimal and overfitting is obligatory. arXiv preprint arXiv:2311.14646.
Sinha, A., Namkoong, H., Volpi, R., and Duchi, J. (2017). Certifying some distributional robustness with principled
adversarial training. arXiv preprint arXiv:1710.10571.
Soudry,D.,Hoffer,E.,Nacson,M.S.,Gunasekar,S.,andSrebro,N.(2018). Theimplicitbiasofgradientdescenton
separable data. The Journal of Machine Learning Research, 19(1):2822–2878.
Staib,M.andJegelka,S.(2019).Distributionallyrobustoptimizationandgeneralizationinkernelmethods.Advances
in Neural Information Processing Systems, 32.
Sugiyama, M., Krauledat, M., and Mu¨ller, K.-R. (2007). Covariate shift adaptation by importance weighted cross
validation. Journal of Machine Learning Research, 8(5).
Telgarsky, M. (2013). Margins, shrinkage, and boosting. In International Conference on Machine Learning, pages
307–315. PMLR.
24Tian,J.,Dai,X.,Ma,C.-Y.,He,Z.,Liu,Y.-C.,andKira,Z.(2023). Trainableprojectedgradientmethodforrobust
fine-tuning. arXiv preprint arXiv:2303.10720.
Tsigler,A.andBartlett,P.L.(2023). Benignoverfittinginridgeregression. Journal of Machine Learning Research,
24(123):1–76.
Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47.
Cambridge university press.
Wald, Y., Yona, G., Shalit, U., and Carmon, Y. (2022). Malign overfitting: Interpolation can provably preclude
invariance. arXiv preprint arXiv:2211.15724.
Wang, K., Muthukumar, V., and Thrampoulidis, C. (2021). Benign overfitting in multiclass classification: All roads
lead to interpolation. Advances in Neural Information Processing Systems, 34:24164–24179.
Wang, K. and Thrampoulidis, C. (2022). Binary classification of gaussian mixtures: Abundance of support vectors,
benign overfitting, and regularization. SIAM Journal on Mathematics of Data Science, 4(1):260–284.
Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017). The marginal value of adaptive gradient
methods in machine learning. Advances in neural information processing systems, 30.
Wortsman, M., Ilharco, G., Gadre, S.Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A.S., Namkoong, H., Farhadi, A.,
Carmon,Y., Kornblith, S.,etal.(2022a). Modelsoups: averagingweightsofmultiplefine-tunedmodelsimproves
accuracywithoutincreasinginferencetime. InInternationalConferenceonMachineLearning,pages23965–23998.
PMLR.
Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A.,
Namkoong,H.,etal.(2022b).Robustfine-tuningofzero-shotmodels.InProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition, pages 7959–7971.
Wu, J., Zou, D., Braverman, V., Gu, Q., and Kakade, S. (2022). The power and limitation of pretraining-finetuning
for linear regression under covariate shift. Advances in Neural Information Processing Systems, 35:33041–33053.
Wyner, A. J., Olson, M., Bleich, J., and Mease, D. (2017). Explaining the success of adaboost and random forests
as interpolating classifiers. The Journal of Machine Learning Research, 18(1):1558–1590.
Yurtsever,E.,Lambert,J.,Carballo,A.,andTakeda,K.(2020). Asurveyofautonomousdriving: Commonpractices
and emerging technologies. IEEE access, 8:58443–58469.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning (still) requires
rethinking generalization. Communications of the ACM, 64(3):107–115.
Zhang,L.,Zhao,P.,Zhuang,Z.-H.,Yang,T.,andZhou,Z.-H.(2024). Stochasticapproximationapproachestogroup
distributionally robust optimization. Advances in Neural Information Processing Systems, 36.
Zhang, T. and Yu, B. (2005). Boosting with early stopping: Convergence and consistency.
Zhou,X.,Lin,Y.,Zhang,W.,andZhang,T.(2022). Sparseinvariantriskminimization. InInternationalConference
on Machine Learning, pages 27222–27244. PMLR.
Zhou, Z.-H., Wu, J., and Tang, W. (2002). Ensembling neural networks: many could be better than all. Artificial
intelligence, 137(1-2):239–263.
Zou,D.,Wu,J.,Braverman,V.,Gu,Q.,Foster,D.P.,andKakade,S.(2021a). Thebenefitsofimplicitregularization
from sgd in least squares problems. Advances in neural information processing systems, 34:5456–5468.
Zou, D., Wu, J., Braverman, V., Gu, Q., and Kakade, S. (2021b). Benign overfitting of constant-stepsize sgd for
linear regression. In Conference on Learning Theory, pages 4633–4635. PMLR.
25A Details on Example 1 and Example 2
Here we restate the two examples and provide detailed calculations.
A.1 Details on Example 1
Suppose the eigenvalues as
1,k=1,

1 1+s2−2scos(kπ/(p+1))
λ = , 2≤k≤p,
k 0n ,2 o1 t/ h5 er1 w+ ises ,2−2scos(π/(p+1))
where p=n5. As it is easy to verified that k∗ =1, we could obtain that
k∗ 1
= →0,
n n
tr{Σ} 1+p(1+s)2/n21/5 2(1+s)2
≤ ≤ →0,
n n n1/5
n3/4 n3/4 1
≤ ≤ →0,
tr{Σ} 1+p(1−s)2/n21/5 2(1−s)2n1/20
n1+ξ(cid:80) λ2 n1+ξ(1+p(1+s)4/n21/5) 2
i i ≤ ≤ →0, ∀0<ξ<4.
tr{Σ}2 (1+p(1−s)2/n21/5)2 n4−ξ
A.2 Details on Example 2
Suppose the eigenvalues as
λ =k−5/6, 1≤k<p,
k
where p=n5. As we could verify that k∗ =n1/5, we could obtain that
k∗ 1
= →0,
n n4/5
tr{Σ} 12n5/6 12
≤ ≤ →0,
n n n1/6
n3/4 n3/4 1
≤ ≤ →0,
tr{Σ} 3n5/6 3n1/12
n1+ξ(cid:80) λ2 4n1+ξ/3 4 2
i i ≤ ≤ →0, ∀0<ξ< .
tr{Σ}2 (3n5/6)2 27n2/3−ξ 3
B Proof for Theorem 1
Recalling the decomposition Σ =(cid:80) λ e eT, we have
i i i i
XXT =(cid:88) λ z zT, XΣXT =(cid:88) λ2z zT, (9)
i i i i i i
i i
in which
1
z := √ Xe (10)
i i
λ
i
areindependentσ -subgaussianrandomvectorsinRn withmean0andcovarianceI. Thenwewilltakethefollowing
x
notations in further analysis:
A=XXT, A =(cid:88) λ z zT, A =(cid:88) λ z zT. (11)
k i i i −k i i i
i>k i̸=k
26B.1 Kernel matrix linearization
HerethefirststepistoestimatethekernelmatrixK =Φ ΦT ∈Rn×n forr=1,2properly. WithLemma21and22
r r
in (Jacot et al., 2018), we could approximate each element K as
s,t
 (cid:115) 
(cid:18) 1 (cid:19) 1 xTx (cid:18) xTx (cid:19) ∥x ∥ ∥x ∥ (cid:18) xTx (cid:19)2
K s,t = 1+O p(√ m) p 2s πt arccos − ∥x ∥s ∥xt ∥ + s 22 π t 2 1− − ∥x ∥s ∥xt ∥ , (12)
s 2 t 2 s 2 t 2
here we define a temporary function h (z) as :
s,t
(cid:115)
xTx (cid:18) xTx (cid:19) z (cid:18) xTx (cid:19)2
h (z):= s t arccos − s t + 1− s t ,
s,t 2πtr{Σ} tr{Σ}z 2π tr{Σ}z
which has an uniformal bounded Lipschitz as:
(cid:115)
1
(cid:18)
xTx
(cid:19)2
1
|h′ (z)|=| 1− s t |≤ ,
s,t 2π tr{Σ}z 2π
andthekernelmatrixK couldbeapproximatedbyanewkernelK′ whichhascomponentsK′ =(tr{Σ}/p)h (1),
s,t s,t
due to the following fact
(cid:12) (cid:12)
∥p/tr{Σ}K−p/tr{Σ}K′∥ =p/tr{Σ} max (cid:12)βT(K−K′)β(cid:12)
2 (cid:12) (cid:12)
β∈Sn−1
(cid:12) (cid:12)
(cid:12)(cid:88) (cid:18)(cid:18) 1 (cid:19) (cid:19)(cid:12)
= max (cid:12) β β 1+O (√ ) h (∥x ∥ ∥x ∥ /tr{Σ})−h (1) (cid:12)
β∈Sn−1(cid:12)
(cid:12)
s t p m s,t s 2 t 2 s,t (cid:12)
(cid:12)
s,t
(cid:12) (cid:12) (cid:12) (cid:12)
≤ 1 max (cid:12) (cid:12)(cid:88) β β | ∥x s∥ 2∥x t∥ 2 −1|(cid:12) (cid:12)+O (√1 ) max (cid:12) (cid:12)(cid:88) β β h (∥x s∥ 2∥x t∥ 2)(cid:12) (cid:12)
2π β∈Sn−1(cid:12)
(cid:12)
s t tr{Σ} (cid:12)
(cid:12)
p m β∈Sn−1(cid:12)
(cid:12)
s t s,t tr{Σ} (cid:12)
(cid:12)
s,t s,t
≤ 1 max| ∥x s∥ 2∥x t∥ 2 −1|· max (cid:88) β β +O ( p √ )∥K∥
2π s,t tr{Σ} β∈Sn−1 s t p tr{Σ} m 2
s,t
=
1
max|
∥x s∥2
2 −1|· max
(cid:88)
β β +O (
p
√ )∥K∥
2π s tr{Σ} β∈Sn−1 s t p tr{Σ} m 2
s,t
n ∥x ∥2 p
≤ max| s 2 −1|+O ( √ )∥K∥ ,
2π s tr{Σ} p tr{Σ} m 2
wherethefirstinequalityisduetotheboundedLipschitznormofh (z),thesecondinequalityisfromthefactthat
s,t
β ∈Sn−1, and the last inequality is from Cauthy-Schwarz inequality:
(cid:115) (cid:115)
(cid:88) β β ≤ (cid:88) β2 (cid:88) β2 =n(cid:88) β2 =n.
i j i j i
i,j i,j i,j i
ThenwithAssumption3and4,considerthesettingsoninputdata,withprobabilityatleast1−2ne−t2tr{Σ}2/2r0(Σ2),
we could obtain that
∥x ∥2
max | s 2 −1|≤t,
s=1,...,n tr{Σ}
under Assumption 3, as r (Σ2) ≤ r (Σ) = tr{Σ}, choosing t = n−5/16, we have t2tr{Σ}2/r (Σ2) ≥ tr{Σ}n−5/8 ≥
0 0 0
n1/8, so with probability at least 1−2ne−n1/8/2, we can get
2n11/16 ntr{Σ} tr{Σ}
∥K−K′∥ ≤ +o ( √ )=o( ), (13)
2 pπ p p m p
where the last inequality is from Assumption 4. Further, if we denote a function g(·):R→R as:
z z 1 (cid:114) z
g(z):= arccos(− )+ 1−( )2,
2πtr{Σ} tr{Σ} 2π tr{Σ}
27thecomponentsofmatrixK′ couldbeexpressedasK′ = tr{Σ}g(xTx ). ThenwitharefinementofElKaroui(2010)
s,t p s t
in Lemma 6, with a probability at least
1−4n2e−n1/8/2,
we have the following approximation:
tr{Σ}
∥K′−K˜∥ ≤o( ), (14)
2 pn1/16
in which
tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1
K˜ = ( + 0 )11T + XXT + ( − )I , (15)
p 2π 4πtr{Σ}2 4p p 4 2π n
and according to Lemma 15, with probability 1−2e−n/c, we have
tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1
µ (K˜)≤ ( + 0 )∥11T∥ + ∥XXT∥ + ( − )∥I ∥
1 p 2π 4πtr{Σ}2 2 4p 2 p 4 2π n 2
ntr{Σ} c tr{Σ} 1 1 (1+c λ )ntr{Σ}
≤ + 1(tr{Σ}+nλ )+ ( − )≤ 1 1 , (16)
πp 4p 1 p 4 2π p
1 tr{Σ} 1 1 tr{Σ} 1 1
µ (K˜)≥ µ (XXT)+ ( − )µ (I )≥ ( − ),
n 4p n p 4 2π n n p 4 2π
then combining with Eq. (13) and (14), we could obtain
n11/16 ntr{Σ} tr{Σ} tr{Σ}
∥K−K˜∥ ≤O( )+O( )+O( )=o( ),
2 p pm1/2 pn1/16 p
where the last equality is from Assumption 3 and 4. And we could approximate K by K˜ in further analysis.
B.2 Proof for ID risk upper bound
For the single model f (θˆ(W ),x), we could express the ID excess risk as
W1 1
1
L (f (θˆ(W ),x))=E [√ ϕ(xTW )(θˆ(W )−θ∗(W ))]2
id W1 1 x,y m 1 1 1
1
= θ(W )∗T[I−ΦT (Φ ΦT )−1Φ ]E ϕ(WTx)ϕ(WTx)T[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
m 1 W1 W1 W1 W1 x 1 1 W1 W1 W1 W1 1
(cid:124) (cid:123)(cid:122) (cid:125)
Bid
1
+ (σ2+o(1))trace{(Φ ΦT )−2Φ E ϕ(WTx)ϕ(WTx)TΦT }.
m W1 W1 W1 x 1 1 W1
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
(17)
B.2.1 Proof for bias term
For the bias part Bid, we could consider that
(cid:18) (cid:19)
1 1
Bid =θ∗(W )[I−ΦT (Φ ΦT )−1Φ ] E ϕ(WTx)ϕ(WTx)T − ΦT Φ [I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
1 W1 W1 W1 W1 m x 1 1 n W1 W1 W1 W1 W1 W1 1
1 1
≤∥θ∗(W )∥2∥ E ϕ(WTx)ϕ(WTx)T − ΦT Φ ∥ ,
1 2 m x 1 1 n W1 W1 2
(18)
wheretheinequalityisinducedfrom∥I−ΦT (Φ ΦT )−1Φ ∥ ≤1andaTBa≤∥a∥2∥B∥ foranypositive-defined
W1 W1 W1 W1 2 2 2
matrix B. And the next step is to prove the random vector Φ (WTx) is sub-gaussian with respect to x. To be
W1 1
specific, based on Lemma 7, with probability at least
1−2e−nξ/2/4,
we have
E [wT x]2 ≤2tr{Σ}/p, i=1,...,m, (19)
x 1,i
which implies that {wT x} are σ (cid:112) 2tr{Σ}/p-sub gaussian random variables. Take derivative for ϕ(WTx) with
1.i x 1
respect to wT x on each dimension, we have
1,i
(cid:12) (cid:12)
(cid:12)∂ϕ(wT x)(cid:12)
(cid:12) 1,i (cid:12)≤1,
(cid:12) ∂(wT x) (cid:12)
(cid:12) 1,i (cid:12)
28so for any vector γ ∈Rm, the function γT√1 mϕ(W 1Tx) has a bounded Lipschitz such that
m
√1 |γTϕ(WTx)|≤ √1 (cid:88) |γ ||wT x|,
m 1 m i 1,i
i=1
which implies that
1 E [γTϕ(WTx)]2 ≤ 1 E [(cid:88) |γ ||wT x|]2 ≤∥γ∥2 1 E tr{WTxxTW }≤ 2tr{Σ} ∥γ∥2,
m x 1 m x i 1,i 2m x 1 1 p 2
i
where the second inequality is due to Cauthy-Schwarz inequality, and the last inequality is from (19). So we could
obtain that √1 mϕ(W 1Tx) is a sub-gaussian random vector satisfying
EeλγTϕ(W1Tx)/√ m ≤eλ2σx2∥γ∥2 2tr{Σ}/p. (20)
The next step is to consider the positive-defined matrix M = 1E ϕ(WTx)ϕ(WTx)T, it has elements as:
1 m x 1 1
(cid:32) (cid:33)
wT Σw wT Σw
M = 1,i 1,j arccos − 1,i 1,j
1,i,j 2πm ∥Σ1/2w ∥ ∥Σ1/2w ∥
1,i 2 1,j 2
(cid:118)
(cid:117) (cid:32) (cid:33)2
+
∥Σ1/2w 1,i∥ 2∥Σ1/2w 1,j∥ 2(cid:117)
(cid:116)1−
w 1T ,iΣw 1,j
,
2πm ∥Σ1/2w ∥ ∥Σ1/2w ∥
1,i 2 1,j 2
1
M = wT Σw ,
1,i,i 2m 1,i 1,i
√
combing with Lemma 7, we could obtain that with probability at least 1−2e−n ξ/4,
∥M ∥ ≤trace(M )=
1
(cid:88)m
wT Σw ≤
1tr{Σ}(cid:18)
1+
1
(cid:19)
,
1 2 1 2m 1,i 1,i 2 p n(2+ξ)/4
i=1
∥E x√1 mϕ(W 1Tx)∥
2
=(cid:118) (cid:117) (cid:117) (cid:116)(cid:88)m 2π1 mw 1T ,iΣw
1,i
≤(cid:115) tr 2{ πΣ p}(cid:18) 1+ n(2+1 ξ)/8(cid:19) , (21)
i=1
Consider Lemma 9 and Eq. (20) (21), we have
(cid:18) (cid:19)
1 1 1 tr{Σ} 4σ
∥E ϕ(WTx)ϕ(WTx)T − ΦT Φ ∥ ≤ 1+ √x ,
xm 1 1 n W1 W1 2 n1/4 p π
with probability at least
1−6e−nξ′
where ξ′ =min{1/2,ξ/2}. Further combing with Eq. (18), we have
∥θ∗(W )∥2tr{Σ}(cid:18) 4σ (cid:19)
Bid ≤ 1 2 1+ √x . (22)
n1/4 p π
B.2.2 Proof for variance term
Now we turn to the variance term Vid,
1
Vid =(σ2+o(1))trace{(Φ ΦT )−2Φ E ϕ(WTx)ϕ(WTx)TΦ }
W1 W1 W1 xm 1 1 W1
=
σ2 n+ mo(1)
E x′ 1,···x′
n(cid:88)n
trace{(Φ W1ΦT W1)−2Φ W1E xϕ(W 1Tx′ i)ϕ(W 1Tx′ i)TΦT W1}
i=1
= σ2+ no(1) E x′ 1,...,x′ ntrace{(Φ W1ΦT W1)−2Φ W1Φ′ WT 1Φ′ 1ΦT W1},
29wherewedenotex′,...,x′ arei.i.d. samplesfromthesamedistributionasx ,...,x ,andΦ′ =[ϕ(WTx′),...,ϕ(WTx′)]T.
1 n 1 n W1 1 1 1 n
ForthematrixΦ ΦT andΦ′ ΦT ,withprobabilityatleast1−4n2e−n1/4/2,wecouldtakethesimilarlinearizing
W1 W1 W1 W1
procedure as on K =Φ ΦT to obtain:
W1 W1
∥Φ Φ′T − tr{Σ}(cid:18) 1 + 3r 0(Σ2) (cid:19) 11T + 1 XX′T∥ ≤ 4tr{Σ} ,
W1 W1 p 2π 4πtr{Σ}2 4p 2 pn1/16
tr{Σ}(cid:18) 1 3r (Σ2) (cid:19) 1 4tr{Σ}
∥Φ′ ΦT − + 0 11T + X′XT∥ ≤ ,
W1 W1 p 2π 4πtr{Σ}2 4p 2 pn1/16
where X′ =[x′,...,x′]T ∈Rn×p, and it implies that
1 n
Q:= n1 E x′ 1,...,x′ nΦ W1Φ′ WT 1Φ′ W1ΦT W1 ≺ t 2r π{Σ 2p} 22 (1+o(1))11T + 81 p2XΣXT + 32 pt 2r n{ 9Σ /8}2 I n.
Then according to Lemma 10, as µ (K˜)≥ tr{Σ}(1 − 1 ) shown in Eq. (16), we have
n p 4 2π
1
|Vid−(σ2+o(1))trace{K˜−2Q}|≤O( )(σ2+o(1))trace{K˜−2Q}. (23)
n1/16
And as we could express trace{K˜−2Q} as :
trace{K˜−2Q}
(cid:40)(cid:18) tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1 (cid:19)−2(cid:18) tr{Σ}2 1 32tr{Σ}2 (cid:19)(cid:41)
≤trace ( + 0 )11T + XXT + ( − )I (1+o(1))11T + XΣXT + I
p 2π 4πtr{Σ}2 4p p 4 2π n 2π2p2 8p2 p2n9/8 n
tr{Σ}2 (cid:18) tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1 (cid:19)−2
≤ (1+o(1))1T ( + 0 )11T + XXT + ( − )I 1
2π2p2 p 2π 4πtr{Σ}2 4p p 4 2π n
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
1
(cid:40)(cid:18)
1 tr{Σ} 1 1
(cid:19)−2(cid:18)
1
(cid:19)(cid:41)
+trace XXT + ( − )I XΣXT
4p p 4 2π n 8p2
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
2
(cid:40)(cid:18) 1 tr{Σ} 1 1 (cid:19)−2(cid:18) 32tr{Σ}2 (cid:19)(cid:41)
+trace XXT + ( − )I I ,
4p p 4 2π n p2n9/8 n
(cid:124) (cid:123)(cid:122) (cid:125)
Vid
3
(24)
where the last inequality is from the fact that
tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1 1 tr{Σ} 1 1
( + 0 )11T + XXT + ( − )I ≻ XXT + ( − )I .
p 2π 4πtr{Σ}2 4p p 4 2π n 4p p 4 2π n
By Woodbury identity, denoting
1 tr{Σ} 1 1
R˜ := XXT + ( − )I ,
4p p 4 2π n
we can get
tr{Σ}2 tr{Σ}2 tr{Σ} 1 3r (Σ2)
Vid = (1+o(1))1TK˜−21= (1+o(1))1T( ( + 0 )11T +R˜)−21
1 2π2p2 2π2p2 p 2π 4πtr{Σ}2
=
t 2r π{Σ 2p} 22 (1+o(1))1TR˜−21
≤
21TR˜−21
≤
2n/λ n(R˜)2
,
(1+ tr{ pΣ}( 21
π
+ 43 πr t0 r( {Σ Σ2 }) 2)1TR˜−11)2 (1TR˜−11)2 n2/λ 1(R˜)2
wherethefirstinequalityisfromignoringtheconstanttermondenominator,andthesecondinequalityisduetothe
fact
1TR˜−21≤nλ (R˜−1)2 =n/λ (R˜)2,
1 n
1TR˜−11≥nλ (R˜−1)=n/λ (R˜),
n 1
30as recalling Lemma 15, with a high probability, we have
tr{Σ} 1 1 1 tr{Σ}
λ n(R˜)≥
p
(
4
− 2π)+
c
pλ k∗+1r k∗ ≥
8p
,
1
tr{Σ} 1 1 c 2tr{Σ}(1+c ) (1+c )(tr{Σ}+n)
λ (R˜)≤ ( − )+ 1(nλ +tr{Σ})≤ 1 ≤ 1 ,
1 p 4 2π p 1 p p
we can further obtain that
tr{Σ}2 n 2(tr{Σ}+n)2(1+c )2/p2 (cid:18) 1 n (cid:19)
(1+o(1))1TK˜−21≤ 1 ≤256(1+c )2 + ,
2π2p2 n2 tr{Σ}2/(8p2) 1 n tr{Σ}2
further due to Assumption 4, we have
tr{Σ}2 (cid:18) 1 n (cid:19) 512(1+c )2
Vid ≤ (1+o(1))1TK˜−21≤256(1+c )2 + ≤ 1 . (25)
1 2π2p2 1 n tr{Σ}2 n1/2
For the second term, based on Lemma 15, with probability at least 1−ce−n/c, we can obtain that
1 1 tr{Σ} 1 1
Vid = trace{( XXT + ( − )I )−2XΣXT}
2 8p2 4p p 4 2π n
=2trace{(XXT +tr{Σ}(1−2/π)I )−2XΣXT}
n
≤2(cid:32) k∗ (c 1λ k∗+1r k∗ +tr{Σ}(1−2/π))2 + n(cid:80) i>k∗λ2 i (cid:33) (26)
n (1/c 1λ k∗+1r k∗ +tr{Σ}(1−2/π))2 (λ k∗+1r k∗ +tr{Σ}(1−2/π))2
k∗ 2n(cid:80) λ2
≤2c4 + i>k∗ i ,
1 n (tr{Σ}(1−2/π))2
where the first inequality is based on Lemma 20, and the second inequality is from the fact that
(c 1λ k∗+1r k∗ +tr{Σ}(1−2/π))2 ≤c4,
(1/c 1λ k∗+1r k∗ +tr{Σ}(1−2/π))2 1
λ k∗+1r k∗ +tr{Σ}(1−2/π)≥tr{Σ}(1−2/π).
And for the third term,
32tr{Σ}2
(cid:40)(cid:18)
1 tr{Σ} 1 1
(cid:19)−2(cid:41)
Vid = trace XXT + ( − )I
3 p2n9/8 4p p 4 2π n
512tr{Σ}2 (27)
= trace{(XXT +tr{Σ}(1−2/π)I )−2}
n9/8 n
512 1
≤ ,
(1−2/π)2n1/8
where the inequality is from the fact that µ (XXT +tr{Σ}(1−2/π)) ≥ tr{Σ}(1−2/π). Combing Eq (23), (24),
n
(25), (26) and (27), with a high probability, Rstd(wˆ) can be upper bounded as
(cid:32) 1024 1 k∗ 2n(cid:80) λ2 (cid:33)
Vid ≤σ2 +2c4 + i>k∗ i . (28)
(1−2/π)2n1/8 1 n tr{Σ}2(1−2/π)2
And combing Eq. (22) and (28), we have
∥θ∗(W )∥2(cid:115) tr{Σ}(cid:18) 4σ (cid:19) (cid:32) 1024 1 k∗ 2n(cid:80) λ2 (cid:33)
L (f (θˆ(W ),x))≤ 1 2 1+ √x +σ2 +2c4 + i>k∗ i .
id W1 1 n1/4 p π (1−2/π)2n1/8 1 n tr{Σ}2(1−2/π)2
31B.3 Proof for OOD risk lower bound
Then we turn to the OOD situation. Due to the definition of OOD excess risk, we could obtain that
L (f (θˆ(W ),x))
ood W1 1
(cid:20)
1
(cid:21)2
= max E √ ϕ((x+δ)TW )(θˆ(W )−θ∗(W ))
∆∈Ξood x,δ,y m 1 1 1
1
=L (f (θˆ(W ),x))+ max E [δT∇ ϕ(WTx)(θˆ(W )−θ∗(W ))]2
id W1 1 m∆∈Ξood x,δ,ϵ x 1 1 1
1
=L (f (θˆ(W ),x))+ max θ∗T[I−ΦT (Φ ΦT )−1Φ ]E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
id W1 1 ∆∈Ξood m 1 W1 W1 W1 W1 x x 1 δ x 1 W1 W1 W1 W1 1
(cid:124) (cid:123)(cid:122) (cid:125)
Bood
1
+ max (σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT },
∆∈Ξood m W1 W1 W1 x x 1 δ x 1 W1
(cid:124) (cid:123)(cid:122) (cid:125)
Vood
where we take first-order Taylor expansion with respect to x. For the lower bound for L (f (θˆ(W ),x)), we just
ood W1 1
focus on the variance term and ignore the impact on bias. Then for the variance term Vood, similar to the analysis
above, we need to deal with the matrix
1
D:= Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT ∈Rn×n
m W1 x x 1 δ x 1 W1
Taking expectation with respect to x, we have
(cid:32) (cid:33)T
(cid:16) (cid:17) ∂ϕ(wT x) ∂ϕ(wT x)
E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx) =E 1,i Σ 1,j
x x 1 δ x 1 i,j x ∂x δ ∂x
(cid:32) (cid:33)
= 1 arccos − w 1T ,iΣw 1,j wT Σ w ,
2π ∥Σ1/2w ∥ ∥Σ1/2w ∥ 1,i δ 1,j
1,i 2 1,j 2
(cid:32) (cid:33)T
(cid:16) (cid:17) ∂ϕ(wT x) ∂ϕ(wT x)
E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx) =E 1,i Σ 1,i
x x 1 δ x 1 i,i x ∂x δ ∂x
1
= wT Σ w ,
2 1,i δ 1,i
√
furthermore, according to Lemma 7, with probability at least 1−2e−n ξ/4, for any i̸=j, we have
(cid:16) (cid:17) 1 (cid:18) π 1 (cid:19)
E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx) = wT Σ w +O( ) ,
x x 1 δ x 1 i,j 2π 1,i δ 1,j 2 n(2+ξ)/4
the equality is from Lemma 7 and the fact that function arccos(−z) has a constant Lipschitz bound around 0:
(cid:12) (cid:12)
(cid:12) (cid:12) w 1T ,iΣw 1,j (cid:12) (cid:12)≤ O(n−(2+ξ)/4) =O( 1 ).
(cid:12)∥Σ1/2w ∥ ∥Σ1/2w ∥ (cid:12) 1−O(n−(2+ξ)/4) n(2+ξ)/4
(cid:12) 1,i 2 1,j 2(cid:12)
While tr{Σ }/µ (Σ )≥n2, the components of D could be expressed as:
δ 1 δ
D =
1 (cid:88)(cid:16)
E ∇ ϕ(WTx)TΣ ∇
ϕ(WTx)(cid:17)
ϕ(wT x )ϕ(wT x )
s,t m2 x x 1 δ x 1 i,j 1,i s 1,j t
i,j
(cid:18) (cid:19)
= 1 (cid:88) wT Σ w ϕ(wT x )ϕ(wT x )+ 1 (cid:88) wT Σ w ϕ(wT x )ϕ(wT x ) π +O( 1 )
2m2 1,i δ 1,i 1,i s 1,i t 2πm2 1,i δ 1,j 1,i s 1,j t 2 n(2+ξ)/4
i i̸=j
(cid:18) (cid:19)
= 1+O(√1 ) E 1 wTΣ wϕ(wTx )ϕ(wTx ) (29)
m w∼N(0,1/pIp)2m δ s t
(cid:18) (cid:19) (cid:18) (cid:19)
+ 1+O(√1 m) m m−1 E w,w′∼N(0,1/pIp)21 πwTΣ δw′ϕ(wTx s)ϕ(w′Tx t) π
2
+O( n(2+1 ξ)/4)
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
1 1 1 1 1 m−1
= 1+O(√ )+O( ) trace(Σ )K + 1+O(√ ) 1+O( ) xTΣ x ,
m n2 2pm δ s,t m n(2+ξ)/4 16mp2 s δ t
32where the last equality is induced from Lemma 12 and the definition of kernel matrix K. Then from Lemma 11, we
have
(cid:12) (cid:26) (cid:18) (cid:19)(cid:27)(cid:12)
(cid:12) (cid:12) (cid:12)tr K−2 D− 2p1 mtr{Σ δ}K− 1m 6m− p1 2XΣ δX (cid:12) (cid:12)
(cid:12)
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 tr{Σ } 1 1 m−1
≤ O(√ )+O( ) δ ∥K−2∥ ∥K∥ + O(√ )+O( ) ∥K−2∥ ∥XΣ XT∥
m n2 2pm F F m n(2+ξ)/4 16mp2 F δ F
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 ntr{Σ } 1 1 n(m−1)
≤ O(√ )+O( ) δ µ (K−2)µ (K)+ O(√ )+O( ) µ (K−2)µ (XΣ XT)
m n2 2pm 1 1 m n(2+ξ)/4 16mp2 1 1 δ
(cid:18) 1 1 (cid:19) tr{Σ} µ (K) (cid:18) 1 1 (cid:19) n µ (XXT)
≤ O(√ )+O( ) 1 + O(√ )+O( ) 1 ,
m n2 m µ (K)2 m n(2+ξ)/4 p2 µ (K)2
n n
√
wherethesecondinequalityisfrom∥A∥ ≤ n∥A∥ foranyA∈Rn×n,andthelastinequalityisduetoAssumption1
F 2
and 2, in which
tr{Σ
}=(cid:88)
α
≤τk∗+τptr{Σ}
≤τλ
n+τptr{Σ} ≤2τptr{Σ}
, ∥Σ ∥ ≤τ.
δ i n 1 n n δ 2
i
Then with Eq. (16) and Lemma 13, we have
(1+c λ )nl tr{Σ} 1 1
µ (K)≤ 1 1 , µ (K)≥ ( − ), µ (XXT)≤tr{Σ}+nλ ,
1 p n p 4 2π 1 1
consider Assumption 4, we will obtain that
(cid:12) (cid:26) (cid:18) (cid:19)(cid:27)(cid:12)
ζ :=(cid:12) (cid:12) (cid:12)tr K−2 D− 2p1 mtr{Σ δ}K− 1m 6m− p1 2XΣ δX (cid:12) (cid:12) (cid:12)≤O( mn 3p /2)+O( mp n)+O( nξ1 /4)=o(1), (30)
it implies that we can estimate Vood as and
Vood =(σ2+o(1))trace{(Φ ΦT )−2D}
W1 W1
(σ2+o(1)) m−1
≥ tr(Σ )tr{K−1}+(σ2+o(1)) trace{K−2XΣ XT}−ζ
2pm δ 16mp2 δ (31)
(σ2+o(1)) m−1
Vood ≤ tr(Σ )tr{K−1}+(σ2+o(1)) trace{K−2XΣ XT}+ζ.
2pm δ 16mp2 δ
Then according to Lemma 10, as µ (K˜)≥ tr{Σ}(1 − 1 ) shown in Eq. (16), we have
n p 4 2π
1
|trace(K−1)−trace(K˜−1)|≤O( )trace(K˜−1),
n1/16
(32)
1
|trace(K−2XΣ XT)−trace(K˜−2XΣ XT)|≤O( )trace(K˜−2XΣ XT).
δ δ n1/16 δ
Then for the first term,
(cid:40)(cid:18)
tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1
(cid:19)−1(cid:41)
tr{K˜−1}=tr ( + 0 )11T + XXT + ( − )I
p 2π 4πtr{Σ}2 4p p 4 2π n
(cid:18)
1 tr{Σ} 1 1
(cid:19)−1
=(1+o(1))tr{ XXT + ( − )I }
4p p 4 2π n
=(1+o(1))4ptr{(XXT +tr{Σ}(1−2/π)I )−1},
n
where the second equality is from relaxing the unimportant term 11T in trace calculation (see Lemma 2.2 in Bai
(2008)). Then using Woodbury identity, we have
λ zT(A +tr{Σ}(1−2/πI))−2z
tr{(XXT +tr{Σ}(1−2/π)I)−1}=tr{(A +tr{Σ}(1−2/π)I)−1}− 1 1 −1 1
−1 1+λ zT(A +tr{Σ}(1−2/π)I)−1z
1 1 −1 1
=tr{A k∗
+tr{Σ}(1−2/π)I)−1}−(cid:88)k∗
1+λ
i
λz iT z( TA
(i
A+ +tr{ trΣ {} Σ( }1 (− 1−2/ 2π /) πI )) I− )2 −z
1i
z
i=1 i i i i
≥tr{A k∗
+tr{Σ}(1−2/π)I)−1}−(cid:88)k∗
z ziT T( (A Ai+ +t tr r{ {Σ Σ} }( (1 1− −2 2/ /π π) )I I) )− −2 1z
zi.
i=1 i i i
33Due to Lemma 14, with probability at least 1−3e−n/c, for any 1≤i≤k∗, we have
(cid:114)
n n
∥z ∥2 ≤n+2(162e)2σ2( +lnk∗+ n( +lnk∗))≤c n,
i x c c 2
(33)
(cid:114)
n n
∥ΠL iz∥2 ≥n−2(162e)2σ x2(k∗+
c
+lnk∗+ n(
c
+lnk∗))≥n/c 3,
where L
i
is the span of the n−k∗ eigenvectors of A
i
corresponding to its smallest n−k∗ eigenvalues, ΠL
i
is the
orthogonal projection on L , and c = 8(162e)2σ2, c = 2, (in our assumptions, c > 1 is a large enough constant
√ i 2 x 3
to make c > 16(162e)2σ2, which leads to a positive c ). Then according to Assumption 3 and Lemma 13, with
x 3
probability at least 1−5e−n/c, we have
z iT(A i+tr{Σ}(1−2/π)I)−1z
i
≥(ΠL iz i)T(A i+tr{Σ}(1−2/π)I)−1(ΠL iz i)
≥
∥ΠL iz i∥2
≥ (cid:80)
n
µ k∗+1(A i+tr{Σ}(1−2/π)I) c 3(c
1
j>k∗λ j+tr{Σ}(1−2/π))
n
≥ ,
c (c +1−2/π)tr{Σ}
3 1
and we could also obtain that
∥z ∥2 c n
zT(A +tr{Σ}(1−2/π)I)−2z ≤ i 2 ≤ 2 ,
i i i µ (A +tr{Σ}(1−2/π)I)2 tr{Σ}2(1−2/π)2
n i
and they imply that
(cid:88)k∗ z iT(A i+tr{Σ}(1−2/π)I)−2z
i
≤(cid:88)k∗ c 2n (cid:18) n (cid:19)−1
=
c 2c 3(c 1+1−2/π) k∗
,
zT(A +tr{Σ}(1−2/π)I)−1z tr{Σ}2(1−2/π)2 c (c +1−2/π)tr{Σ} (1−2/π)2 tr{Σ}
i=1 i i i i=1 3 1
(34)
and combing it with the fact that
n n n
tr{(A k∗+tr{Σ}(1−2/π)I)−1}≥
µ 1(A k∗ +tr{Σ}(1−2/π)I)
≥
c
1(cid:80)
j>k∗λ j+tr{Σ}(1−2/π)
≥
tr{Σ}(c
1+1−2/π),
we will obtain
tr{(XXT +tr{Σ}(1−2/π)I)−1}
n c c (c +1−2/π) k∗
≥ − 2 3 1
tr{Σ}(c +1−2/π) (1−2/π)2 tr{Σ} (35)
1
n
≥ ,
2tr{Σ}(c +1−2/π)
1
where the last inequality is induced by Assumption 3. And from the other side, we have
n n
tr{(XXT +tr{Σ}(1−2/π)I)−1}≤ ≤ , (36)
µ (XXT +tr{Σ}(1−2/π)I) tr{Σ}(1−2/π)
n
so combing Eq. (35) and (36), with probability at least 1−7e−n/c, we have
2np
tr{K˜−1}≥(1+o(1)) ,
tr{Σ}(c +1−2/π)
1
(37)
4np
tr{K˜−1}≤(1+o(1)) .
tr{Σ}(1−2/π)
Similarly, for the second term, we have
(cid:40)(cid:18) tr{Σ} 1 3r (Σ2) 1 tr{Σ} 1 1 (cid:19)−2 (cid:41)
tr(K˜−2XΣ XT)=tr ( + 0 )11T + XXT + ( − )I XΣ XT
δ p 2π 4πtr{Σ}2 4p p 4 2π n δ
(cid:18)
1 tr{Σ} 1 1
(cid:19)−2
=(1+o(1))tr{ XXT + ( − )I XΣ XT}
4p p 4 2π n δ
=16p2(1+o(1))tr{(XXT +tr{Σ}(1−2/π)I)−2XΣ XT},
δ
34wherethesecondequalityisfromromrelaxingtheunimportantterm11T intracecalculation(seeLemma2.2inBai
(2008)). Similar to the analysis above, we could decompose the matrix XΣ XT as
δ
XΣ XT =(cid:88) λ α z zT,
δ i i i i
i
by Woodbury identity, we could further obtain that
tr{(XXT +tr{Σ}(1−2/π)I)−2XΣ XT}=(cid:88) λ α zT(XXT +tr{Σ}(1−2/π)I)−2z
δ i i i i
i
=(cid:88) λ iα iz iT(A −i+tr{Σ}(1−2/π)I)−2z i ,
(1+λ zT(A +tr{Σ}(1−2/π)I)−1z )2
i i i −i i
for each index i=1,...,p, according to Lemma 14 and 15, with probability at least 1−5n−n/c, we have
λ α zT(A +tr{Σ}(1−2/π)I)−2z λ α (zT(A +tr{Σ}(1−2/π)I)−1z )2
i i i −i i ≥ i i i −i i
(1+λ zT(A +tr{Σ}(1−2/π)I)−1z )2 ∥z ∥2(1+λ zT(A +tr{Σ}(1−2/π)I)−1z )2
i i −i i i 2 i i −i i
α
(cid:18)
1
(cid:19)−2
= i 1+
λ ∥z ∥2 λ zT(A +tr{Σ}(1−2/π)I)−1z
i i 2 i i −i i
α
(cid:18) tr{Σ}(1−2/π)(cid:19)−2
≥ i 1+ ,
c λ n c nλ
2 i 2 i
where the first inequality is from Cauthy-Schwarz inequality, and the second inequality is from the fact that
∥z ∥2 c n
∥z ∥2 ≤c n, zT(A +tr{Σ}(1−2/π)I)−1z ≤ i 2 ≤ 2 .
i 2 2 i −i i µ (A +tr{Σ}(1−2/π)I) tr{Σ}(1−2/π)
n −i
So according to Lemma 17, with probability at least 1−10e−n/c, we could obtain that
tr{(XXT +l(1−2/π)I)−2XΣ XT}
δ
≥
1 (cid:88)α
i
(cid:18)
1+
tr{Σ}(1−2/π)(cid:19)−2
2c n λ c nλ
2 i 2 i
i
≥
1 (cid:88)α
i min{1,
n2λ2
i }
8c n λ tr{Σ}2
2 i
i
≥
1 (cid:88)α
i min{1,
n2λ2
i } (38)
8c n λ b2tr{Σ}2
2 i
i
 
= 1  (cid:88) α i + (cid:88) α i n2λ2 i 
8c n λ λ tr{Σ}2
2 i i
λi>tr{Σ}/(bn) λi≤tr{Σ}/(bn)
(cid:80)
= 1 (cid:88) α i + 1 n λj≤tr{Σ}/(bn)λ jα j ,
8c nλ 8c b2 tr{Σ}2
2 i 2
λi>tr{Σ}/(bn)
where the second inequality is from the fact c /(1−2/π)>1 and
2
1
(a+b)−2 ≥(2max{a,b})2 = min{a−2,b−2},
4
and on the last equality, while for any j >k∗, we have
1 (cid:88)
nλ ≤ λ ≤l/b, (39)
i b j
j>k∗
35the term (cid:80) λ α must contains the index {k∗,...,p}. And from another side, we could also obtain that
λj≤tr{Σ}/bn j j
tr{(XXT +tr{Σ}(1−2/π)I)−2XΣ XT}=(cid:88) λ α zT(XXT +tr{Σ}(1−2/π)I)−2z
δ i i i i
i
=(cid:88)k∗
λ iα iz iT(A −i+tr{Σ}(1−2/π)I)−2z i + (cid:88) λ α zT(XXT +tr{Σ}(1−2/π)I)−2z
(1+λ zT(A +tr{Σ}(1−2/π)I)−1z )2 i i i i
i=1 i i −i i i>k∗
≤ (cid:88) λ iα iz iT(A −i+tr{Σ}(1−2/π)I)−2z i + (cid:88) λ α zT(XXT +tr{Σ}(1−2/π)I)−2z ,
(λ zT(A +tr{Σ}(1−2/π)I)−1z )2 i i i i
λi>tr{Σ}/(bn) i i −i i λi≤tr{Σ}/(bn)
wheretheinequalityisfromrelaxingtheconstant1onthedenominator. Forthefirstterm,withprobabilityatleast
1−5e−n/c, we could obtain that
(cid:88) λ iα iz iT(A −i+tr{Σ}(1−2/π)I)−2z i
(λ zT(A +tr{Σ}(1−2/π)I)−1z )2
λi>tr{Σ}/(bn) i i −i i
≤ (cid:88) α i ∥z i∥2 2 · µ k∗+1(A −i+tr{Σ}(1−2/π)I)2
λi>tr{Σ}/(bn)λ
i
µ n(A −i+tr{Σ}(1−2/π)I)2 ∥ΠL iz i∥4
2
(40)
≤ (cid:88) α i c 2n · c2 3(c 1(cid:80) j>k∗λ i+tr{Σ}(1−2/π))2
λ tr{Σ}2(1−2/π)2 n2
i
λi>tr{Σ}/(bn)
≤ c 2c2 3(c 1+1−2/π) (cid:88) α i ,
(1−2/π)2 nλ
i
λi>tr{Σ}/(bn)
(cid:80)
where the second inequality is from Eq. (33) and Lemma 13, and the third inequality is due to λ <tr{Σ}.
j>k∗ j
Then for the second term, we could obtain that
(cid:88) λ α zT(XXT +tr{Σ}(1−2/π)I)−2z ≤ (cid:80) λi≤tr{Σ}/(bn)λ iα i∥z i∥2 2 ≤ (cid:80) λi≤tr{Σ}/(bn)λ iα i∥z i∥2 2 ,
i i i i µ (XXT +tr{Σ}(1−2/π)I)2 tr{Σ}2(1−2/π)2
n
λi≤tr{Σ}/(bn)
then from Lemma 18, with probability at least 1−2e−n/c, we have
(cid:88) λ α zT(XXT+tr{Σ}(1−2/π)I)−2z ≤ (cid:80) λi≤tr{Σ}/(bn)λ iα i∥z i∥2 2 ≤(1+324eσ2/c)2n(cid:80) λi≤tr{Σ}/(bn)λ iα i ,
i i i i tr{Σ}2(1−2/π)2 x tr{Σ}2(1−2/π)2
λi≤tr{Σ}/(bn)
(41)
then combing Eq. (40) and (41), we could obtain that
tr{(XXT +tr{Σ}(1−2/π)I)−2XΣ XT}
δ
(cid:80)
≤ c 2c2 3b(c 1+1−2/π) (cid:88) α i +(1+324eσ2/c)2n λi≤tr{Σ}/(bn)λ iα i . (42)
(1−2/π)2 nλ x tr{Σ}2(1−2/π)2
i
λi>tr{Σ}/(bn)
Summarizing the results in Eq. (38) and (42), we could obtain that
tr{K˜−2XΣ XT}
δ
(cid:80)
≥ 2p2(1+o(1)) (cid:88) α i + 2p2(1+o(1))n λj≤tr{Σ}/(bn)λ jα j ,
c nλ c b2 tr{Σ}2
2 i 2
λi>tr{Σ}/(bn)
(43)
tr{K˜−2XΣ XT}
δ
 
(cid:80)
≤16p2(1+o(1))c 2c2 3b (( 1c 1 −+ 2/1 π− )22/π) (cid:88) nα λi +(1+324eσ x2/c)2n tr{λ Σi≤ }t 2r{ (Σ 1} −/( 2bn /) πλ )2iα i .
i
λi>tr{Σ}/(bn)
36Then based on Eq. (31), (32), (37) and (43), with probability at least
1−10e−nξ′
/c where ξ′ = min{1/2,ξ/2}, we
have
L (f (θˆ ,x))≥ max Vood
ood 1 1
∆∈Ξood
 
(cid:80)
≥ max  σ2ntr{Σ δ} + σ2 (cid:88) α i + σ2 n λj≤tr{Σ}/(bn)λ jα j −o(1)
∆∈Ξoodmtr{Σ}(c 1+1−2/π) 8c
2
nλ
i
8c 2b2 tr{Σ}2

(44)
λi>tr{Σ}/(bn)
(cid:80)
≥
σ2τp
+
σ2 τp λj≤tr{Σ}/(bn)λ j
−o(1),
2m(c +1−2/π) 16c b2 tr{Σ}
1 2
where the last inequality is from taking α =τtr{Σ}/n for any i satisfying λ ≤tr{Σ}/(bn).
i i
C Proof for Theorem 3
C.1 Proof for OOD risk upper bound
Recalling the expression for OOD excess risk, we have Then we turn to the OOD situation. Due to the definition of
OOD excess risk, we could obtain that
L (f (θˆ(W ),x))
ood W1 1
1
=L (f (θˆ(W ),x))+ max θ∗T[I−ΦT (Φ ΦT )−1Φ ]E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
id W1 1 ∆∈Ξood m 1 W1 W1 W1 W1 x x 1 δ x 1 W1 W1 W1 W1 1
(cid:124) (cid:123)(cid:122) (cid:125)
Bood
1
+ max (σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT },
∆∈Ξood m W1 W1 W1 x x 1 δ x 1 W1
(cid:124) (cid:123)(cid:122) (cid:125)
Vood
For the bias term Bood, similar to the analysis above, with the fact that ∥ΦT (Φ ΦT )−1Φ ∥ ≤1, we have
W1 W1 W1 W1 2
θ(W )∗T[ΦT (Φ ΦT )−1Φ ]E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)[2I−ΦT (Φ ΦT )−1Φ−1]θ∗(W )≥0
1 W1 W1 W1 W1 x x 1 δ x 1 W1 W1 W1 1
1 1
⇒Bood ≤ θ(W )∗TE ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)θ∗(W )= E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2,
m 1 x x 1 δ x 1 1 m 1 1 1 1
and consider the assumptions on observer x and perturbation δ, we could further obtain that
1
E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2
m 1 1 1 1
1 1
=E[√ ϕ(WTx)Tθ∗(W )−g(x)+g(x)−g(x+δ)+g(x+δ)− √ ϕ(WT(x+δ))Tθ∗(W )]2
m 1 1 m 1 1
1 1
≤4E[√ ϕ(WTx)θ∗(W )−g(x)]2+4E[g(x+δ)− √ ϕ(WT(x+δ))Tθ∗(W )]2+4E[g(x)−g(x+δ)]2
m 1 1 m 1 1
=8ϱ2+4E[∇ g(x)Tδ]2.
x
As we have the facts:
ϱ=o(1), E ∥∇ g(x)∥ ≤c′′, ∥Σ ∥ ≤τ,
x x 2 δ 2
we could obtain that
Bood ≤8ϱ2+4E[∇ g(x)Tδ]2 ≤8ϱ2+4τE∥∇ g(x)∥2 ≤8ϱ2+4c′′τ =O(1) (45)
x x 2
37And for the variance term Vood, based on Eq. (45), (31), (32), (37) and (43), with probability at least
1−10e−nξ′
/c
where ξ′ =min{1/2,ξ/2}, we have
(cid:110) (cid:111)
L (f (θˆ ,x))≤ max Bood+Vood
ood 1 1
∆∈Ξood
≤4τE∥∇ g(x)∥2
x 2
 
(cid:80)
+ max  2σ2ntr{Σ δ} + c 2c2 3b(c 1+1−2/π)σ2 (cid:88) α i + σ2(1+324eσ x2/c)2n λi≤tr{Σ}/(bn)λ iα i +o(1)
∆∈Ξoodmtr{Σ}(1−2/π) (1−2/π)2 nλ
i
(1−2/π)2 tr{Σ}2

λi>tr{Σ}/(bn)
(cid:80)
≤4τE∥∇ g(x)∥2+ 4σ2τp + c 2c2 3b(c 1+1−2/π)τσ2 + σ2(1+324eσ x2/c)22τp λi≤tr{Σ}/(bn)λ i +o(1),
x 2 mtr{Σ}(1−2/π) (1−2/π)2 (1−2/π)2 tr{Σ}
(46)
where the last inequality is from Assumption 1, 2 and the fact:
tr{Σ
}=(cid:88)
α
≤τk∗+τptr{Σ}
≤τλ
n+τptr{Σ} ≤2τptr{Σ}
.
δ i n 1 n n
i
C.2 Proof for ensemble model
If we turn to the ensemble model, we could obtain that
L (f (θˆ(W ),θˆ(W ),x))
ood ens 1 2
(cid:20) 1 (cid:16) (cid:17)(cid:21)2
= max E √ ϕ((x+δ)TW )(θˆ(W )−θ∗(W ))+ϕ((x+δ)TW )(θˆ(W )−θ∗(W ))
∆∈Ξood x,y,δ 2 m 1 1 1 2 2 2
(cid:20)
1 1
(cid:21)2
= max E √ ϕ(xTW )(θˆ(W )−θ∗(W ))+ ϕ(xTW )(θˆ(W )−θ∗(W ))
∆∈Ξood x,y,δ 2 m 1 1 1 2 2 2 2
(cid:124) (cid:123)(cid:122) (cid:125)
term1
(cid:20)
1
(cid:21)2
+ max E √ (δT∇ ϕ(WTx)(θˆ(W )−θ∗(W ))+δT∇ ϕ(WTx)(θˆ(W )−θ∗(W ))) ,
∆∈Ξood x,y,δ 2 m x 1 1 1 x 2 2 2
(cid:124) (cid:123)(cid:122) (cid:125)
term2
similar to the analysis on single model, the equality is from the independence of x and δ. And for term 1, we have
(cid:20)
1 1
(cid:21)2
term 1=E √ ϕ(xTW )(θˆ(W )−θ∗(W ))+ ϕ(xTW )(θˆ(W )−θ∗(W ))
x,y 2 m 1 1 1 2 2 2 2
1 (cid:104) (cid:105)2 1 (cid:104) (cid:105)2
≤ E ϕ(xTW )(θˆ(W )−θ∗(W )) + E ϕ(xTW )(θˆ(W )−θ∗(W ))
2m x,y 1 1 1 2m x,y 2 2 2
1(cid:16) (cid:17)
= L (f (θˆ(W ),x))+L (f (θˆ(W ),x)) →0,
2 id W1 1 id W2 2
for term 2, we could express it as
(cid:20)
1
(cid:21)2
term 2=E √ (δT∇ ϕ(WTx)(θˆ(W )−θ∗(W ))+δT∇ ϕ(WTx)(θˆ(W )−θ∗(W )))
x,y,δ 2 m x 1 1 1 x 2 2 2
1 (cid:104) (cid:105)2
= E δT∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )+δT∇ ϕ(WTx)[I−ΦT (Φ ΦT )−1Φ ]θ∗(W )
4 δ,x x 1 W1 W1 W1 W1 1 x 2 W2 W2 W2 W2 2
(cid:124) (cid:123)(cid:122) (cid:125)
term2.1
1
+(σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }
4m W1 W1 W1 x x 1 δ x 1 W1
(cid:124) (cid:123)(cid:122) (cid:125)
term2.2
1
+(σ2+o(1)) trace{(Φ ΦT )−2Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }
4m W2 W2 W2 x x 2 δ x 2 W2
(cid:124) (cid:123)(cid:122) (cid:125)
term2.3
1
+(σ2+o(1)) trace{(Φ ΦT )−1(Φ ΦT )−1Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT }.
2m W1 W1 W2 W2 W1 x x 1 δ x 2 W2
(cid:124) (cid:123)(cid:122) (cid:125)
term2.4
38√
according to Eq. (45) and (31), with probability at least 1−8e− n, we could obtain that
(cid:18) (cid:19)
1 1 1
term 2.1≤ E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2+ E[ϕ(WTx)Tθ∗(W )−ϕ(WT(x+δ))Tθ∗(W )]2
2 m 1 1 1 1 m 2 2 2 2
1(cid:16) (cid:17)
≤ Bood(f )+Bood(f ) ≤4E[∇ g(x)Tδ]2+o(1)≤4τE∥∇ g(x)∥2+o(1),
2 1 2 x x 2
σ2+o(1) σ2+o(1) m−1
term 2.2+term 2.3= trace(Σ )trace{K−1}+ trace{K−2XΣ XT}±ζ.
4pm δ 2 16mp2 δ
(47)
Forthelastterm,similartotheprocedurebefore,wedenoteD′ = 1Φ E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)ΦT ∈Rn×n.
m W1 x x 1 δ x 2 W2
Taking expectation with respect to x, we have
(cid:32) (cid:33)T (cid:32) (cid:33)
(cid:16) E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx)(cid:17) =E ∂ϕ(w 1T ,ix) Σ ∂ϕ(w 2T ,jx) = 1 arccos − w 1T ,iw 2,j wT Σ w ,
x x 2 δ x 1 i,j x ∂x δ ∂x 2π ∥w 1,i∥ 2∥w 2,j∥ 2 1,i δ 1,j
√
furthermore, according to Lemma 7, with probability at least 1−2e−n ξ/4, for any i,j, we have
(cid:16) (cid:17) 1 (cid:18) π 1 (cid:19)
E ∇ ϕ(WTx)TΣ ∇ ϕ(WTx) = wT Σ w +O( ) ,
x x 1 δ x 2 i,j 2π 1,i δ 2,j 2 n(2+ξ)/4
the equality is from Lemma 7 and the fact that function arccos(−z) has a constant Lipschitz bound around 0, and
(cid:12) (cid:12)
(cid:12) (cid:12) w 1T ,iw 2,j (cid:12) (cid:12)≤ O(n−(2+ξ)/4) =O( 1 ).
(cid:12)∥w ∥ ∥w ∥ (cid:12) 1−O(n−(2+ξ)/4) n(2+ξ)/4
(cid:12) 1,i 2 2,j 2(cid:12)
Then the components of D′ could be expressed as:
D′ =
1 (cid:88)(cid:16)
E ∇ ϕ(WTx)TΣ ∇
ϕ(WTx)(cid:17)
ϕ(wT x )ϕ(wT x )
s,t m2 x x 1 δ x 2 i,j 1,i s 2,j t
i,j
(cid:18) (cid:19)
= 1 (cid:88) wT Σ w ϕ(wT x )ϕ(wT x ) π +O( 1 )
2πm2 1,i δ 2,j 1,i s 2,j t 2 n(2+ξ)/4
i,j (48)
(cid:18) (cid:19) (cid:18) (cid:19)
= 1+O(√1 m) E w,w′∼N(0,1/pIp)21 πwTΣ δw′ϕ(wTx s)ϕ(w′Tx t) π
2
+O( n(2+1 ξ)/4)
(cid:18) (cid:19)(cid:18) (cid:19)
1 1 1
= 1+O(√ )) 1+O( ) xTΣ x ,
m n(2+ξ)/4 16p2 s δ t
so we could repeat the analysis for matrix D, and further obtain that
(cid:12) (cid:26) (cid:18) (cid:19)(cid:27)(cid:12)
ζ′ :=(cid:12) (cid:12) (cid:12)tr K−2 D′− 161 p2XΣ δXT (cid:12) (cid:12) (cid:12)≤O( nξ1 /4)=o(1), (49)
it implies that
σ2+o(1) 1
term 2.4= trace{K−2XΣ XT}±ζ′. (50)
2 16p2 δ
Combing Eq. (32) (37) (43) (47) and (50), with probability at least
1−20e−nξ′
/4, where ξ′ = min{1/2,ξ/2}, the
improvement of ensemble model could be approximated as
L (f (θˆ(W ),x))+L (f (θˆ(W ),x)))/2−L (f (θˆ(W ),θˆ(W ),x)
ood W1 1 ood W2 2 ood ens 1 2
σ2ntr{Σ } σ2τp
≥ max δ −o(1)≥ −o(1),
∆∈Ξood 2mtr{Σ}(c 1+1−2/π) 4mtr{Σ}(c 1+1−2/π)
and combine it with Eq. (46), we could finish the proof.
39D Auxiliary Lemmas
Lemma 6 (RefinementofTheorem2.1inElKaroui,2010). Let we assume that we observe n i.i.d. random vectors,
x ∈Rp. Let us consider the kernel matrix K with entries
i
xTx
K =f( i j ).
i,j tr{Σ}
We assume that:
1. n,p,tr{Σ} satisfy Assumption 3 and 4;
2. Σ is a positive-define p×p matrix, and ∥Σ∥ remains bounded;
2
3. x =Σ1/2η , in which η ,i=1,...,n are σ-subgaussian i.i.d. random vectors with Eη =0 and Eη ηT =I ;
i i i i i i p
4. f is a C1 function in a neighborhood of τ =lim trace(Σ)/tr{Σ} and a C3 function in a neighborhood of 0.
p→∞
Undertheseassumptions,thekernelmatrixK caninprobabilitybeapproximatedconsistentlyinoperatornorm,when
p and n tend to ∞, by the kernel k˜, where
(cid:18) trace(Σ2)(cid:19) XXT
K˜ = f(0)+f′′(0) 11T +f′(0) +v I ,
2tr{Σ}2 tr{Σ} p n
v =f(1)−f(0)−f′(0).
p
In other words, with probability at least
1−4n2e−n1/8/(2τ),
∥K−K˜∥ ≤o(n−1/16).
2
Proof. The proof is quite similar to Theorem 2.1 in El Karoui (2010), and the only difference is we change the
bounded 4+ϵ absolute moment assumption to sub-gaussian assumption on data x , so obtain a faster convergence
i
rate.
First, using Taylor expansions, we can rewrite the kernel matrix K sa
xTx f′′(0)(cid:18) xTx (cid:19)2 f(3)(ξ )(cid:18) xTx (cid:19)3
f(xTx /tr{Σ})=f(0)+f′(0) i j + i j + i,j i j ,i̸=j,
i j tr{Σ} 2 tr{Σ} 6 tr{Σ}
(cid:18) ∥x ∥2 (cid:19)
f(∥x ∥2/tr{Σ})=f(1)+f′(ξ ) i 2 −1 ,on the diagonal.
i 2 i,i tr{Σ}
Then we could deal with these terms separately.
For the second-order off-diagonal term, as the concentration inequality shows that
P(cid:18)
m i,a jx|
tx rT
i
{Σx
j
}
−δ
i,jtr ta rc {e Σ(Σ }) |≤t(cid:19) ≥1−2n2e−t 2r r{ 0Σ (} Σ2 2t )2
, (51)
with Lemma 16, we can obtain that
P(cid:18) max|(xT i x j)2 −E(xT i x j)2 |≤t(cid:19) ≥1−2n2e− 2(16tr 2{ eΣ )2} r4 0t (2 Σ4), (52)
i̸=j tr{Σ}2 tr{Σ}2
in which
(cid:18) xTx (cid:19)2 1 1 trace(Σ2)
E i j = E[xTx xTx ]= Etrace{x xTx xT}= .
tr{Σ} tr{Σ}2 i j j i tr{Σ}2 j j i i tr{Σ}2
Denoting a new matrix W as

(xTx )2
 i j ,i̸=j,
W = tr{Σ}2
i,j
0,i=j,
then considering r (Σ4)/tr{Σ}≤r (Σ)/tr{Σ}=τ is bounded, choosing t=n−17/16, under Assumption 4, we have
0 0
tr{Σ}3n−17/8 ≥n21/32, so with probability at least 1−2n2e− 2(n 161 2/ e8 )2, we have
trace(Σ2) trace(Σ2) 1
∥W − (11T −I )∥ ≤∥W − (11T −I )∥ ≤ .
tr{Σ}2 n 2 tr{Σ}2 n F n1/16
40For the third-order off-diagonal term, as is mentioned in Eq.(51), choosing t = n−1/4, with probability at least
1−2n2e−n1 2/4
, we have
xTx 1
max| i j |≤ .
i̸=j tr{Σ} n1/4
Denote the matrix E has entries E =f(3)(ξ )xTx /tr{Σ} off the diagonal and 0 on the diagonal, the third-order
i,j i,j i j
off-diagonal term can be upper bounded as
∥E◦W∥ ≤max|E |∥W∥ ≤o(n−1/4),
2 i,j 2
i,j
where the last inequality is from the bounded norm of W.
For the diagonal term, still recalling Eq.(51), while we have
∥x ∥2 1
max| i 2 −1|≤ ,
i tr{Σ} n1/4
with probability at least
1−2n2e−n1 2/4
, we can further get
∥x ∥2
max|f( i 2)−f(1)|≤o(n−1/4),
i tr{Σ}
which implies that
∥x ∥2
∥diag[f( i 2),i=1,...,n]−f(τ)I ∥ ≤o(n−1/4).
tr{Σ} n 2
Combing all the results above, we can obtain that
∥K−K˜∥ ≤o(n−1/16),
2
with probability at least
1−4n2e−n1/8/2.
Lemma 7. Assume w ,...,w are sampled i.i.d. from N(1,1/pI ), then with probability at least
1−2e−nξ/2/4,
we
1 m p
have
(cid:16) (cid:17) tr{Σ}
P |wTΣw −E(wTΣw )| ≤ , ∀i,j =1,...,m.
i j i j pn(2+ξ)/4
Proof. First,accordingtothecorrespondingconcentrationinequality,foranyi,j =1,...,mwithprobabilityatleast
1−2e−p2t2/(2(cid:80) iλ2 i), we have
(cid:16) (cid:17)
P |wTΣw −E(wTΣw )| ≤t.
i j i j
Then by choosing t = tr{Σ} and considering all of the pairwise data in (w ,w ),i,j = 1,...,m, under Assump-
pn(2+ξ)/4 i j
tion 3, we have
(cid:16) (cid:17) tr{Σ}
P |wTΣw −E(wTΣw )| ≤ , ∀i,j =1,...,m,
i j i j pn1/4
with probability at least
1−2m2e−nξ/2/2.
Lemma 8. If x is a σ -sub-gaussian random vector with zero mean and ExxT = I, and function f : Rd → R is
x
L-Lipschitz, the random variable f(x) is still sub-gaussian with parameter Lσ . To be specific,
x
Eeλf(x)
≤eλ2L 22σx2
.
Lemma 9. Assume x ∈ Rq is a q-dim sub-gaussian random vector with parameter σ, and E[x] = µ. Here are n
i.i.d. samples x ,...,x , which have the same distribution as x, then we can obtain that with probability at least
√ 1 n
1−4e− n,
∥ExxT −
1 (cid:88)n
x xT∥ ≤∥EzzT∥
max{(cid:114) trace(EzzT) ,trace(EzzT)
,
1 }+2√ 2σ∥µ∥
2,
n i i 2 2 n n n1/4 n1/4
i=1
where we denote z:=x−µ.
41Proof. First, we denote z = x−µ is a ramdom vector with zero mean, correspondingly, there are n i.i.d. samples,
z ,...,z . Then we can obtain that
1 n
ExxT =E(z+µ)(z+µ)T =EzzT +µµT,
and for the samples,
n n n
1 (cid:88) x xT = 1 (cid:88) z zT + 2 (cid:88) µzT +µµT,
n i i n i i n i
i=1 i=1 i=1
which implies that
n n n
∥ExxT − 1 (cid:88) x xT∥ =∥EzzT +µµT − 1 (cid:88) z zT −µµT − 2 (cid:88) µzT∥
n i i 2 n i i n i 2
i=1 i=1 i=1
n n
=∥EzzT − 1 (cid:88) z zT − 2 (cid:88) µzT∥
n i i n i 2
i=1 i=1
n n
≤∥EzzT − 1 (cid:88) z zT∥ +2∥1 (cid:88) µzT∥
n i i 2 n i 2
i=1 i=1
n n
=∥EzzT − 1 (cid:88) z zT∥ +2|1 (cid:88) µTz |,
n i i 2 n i
i=1 i=1
where the inequality is from triangular inequality. So we can estimate the two terms respectively.
Forthefirstterm,asz isσ-subgaussianrandomvariable,byTheorem9inKoltchinskiiandLounici(2017),with
probability at least 1−2e−t,
∥EzzT −
1 (cid:88)n
z zT∥ ≤∥EzzT∥
max{(cid:114) trace(EzzT) ,trace(EzzT) ,(cid:114) t
,
t
}, (53)
n i i 2 2 n n n n
i=1
And for the second term, by general concentration inequality, we can obtain that with probability at least 1−
2e−nt2/(2σ2∥µ∥2 2),
n
|1 (cid:88) zTµ|≤t. (54)
n i
i=1
√ √ √
Choosing t= n in Eq.(53) and t= 2σ∥µ∥ n−1/4 in Eq.(54), with probability at least 1−4e− n,
2
n n n
∥ExxT − 1 (cid:88) x xT∥ ≤∥EzzT − 1 (cid:88) z zT∥ +2∥1 (cid:88) zTµ∥
n i i 2 n i i 2 n i 2
i=1 i=1 i=1
(cid:114) trace(EzzT) trace(EzzT) 1 √ σ∥µ∥
≤∥EzzT∥ max{ , , }+2 2 2.
2 n n n1/4 n1/4
Lemma 10. Consider two positive defined matrix A,B ∈Rn×n satisfying that µ (A)>µ (B), for any C ∈Rp×p,
n 1
we could obtain that
(cid:18) (cid:19)
µ (B)
trace{AC}≤trace{(A+B)C}≤ 1+ 1 trace{AC},
µ (A)
n
(cid:18) (cid:19)
µ (B)
1− 1 trace{A−1C}≤trace{(A+B)−1C}≤trace{A−1C}.
µ (A)
n
Proof. Theproofproceduresarejustrelatedtosomelinearalgebracalculations. Forthefirstinequality,itisintuitive
that
trace{AC}≤trace{(A+B)C},
and considering the right hand side, we have
µ (B) µ (B)
trace{(A+B)C}−trace{AC}=B C≤µ (B)trace{C}= 1 µ (A)trace{C}≤ 1 trace{AC}.
1 µ (A) n µ (A)
n n
42And similarly, to prove the second inequality, what we need to prove is just
µ (B)
trace{A−1C}−trace{(A+B)−1C}≤ 1 trace{A−1C},
µ (B)+µ (A)
1 n
it could be verified by
trace{A−1C}−trace{(A+B)−1C}=trace{(A+B)−1BA−1C}
≤µ ((A+B)−1B)trace{A−1C}
1
µ (B)
≤ 1 trace{A−1C}.
µ (B)+µ (A)
1 n
Lemma 11. For any matrix A,B ∈Rn×n, we hava
|tr{AB}|≤∥A∥ ∥B∥ .
F F
Proof. From Cauthy-Schwarz inequality, we have
(cid:118) (cid:118)
n (cid:117) n (cid:117) n
(cid:88) (cid:117)(cid:88) (cid:117)(cid:88)
|tr{AB}|= a i,jb i,j ≤(cid:116) a2 i,j(cid:116) b2 i,j =∥A∥ F∥B∥ F.
i,j=1 i,j=1 i,j=1
Lemma 12. For random vector w∼N(0,I ), a,b∈Rp and semi-positive defined diagonal matrix
p
H :=diag[h ,...,h ]∈Rp×p,
1 p
while tr{H}/µ (H)≥n2, we have
1
E wTHwaTwbTw1(aTw≥0)1(bTw≥0)
w
(cid:18) tr{H} aTb aTHb(cid:19) (cid:18) aTbT (cid:19)
= ∥a∥ ∥b∥ + arccos −
2 2 2π ∥a∥ ∥b∥ π ∥a∥ ∥b∥
2 2 2 2
(cid:115)
(cid:18)
1
(cid:19)
∥a∥ ∥b∥
(cid:18)
aTb
(cid:19)2
+ 1+O( ) tr{H} 2 2 1− .
n2 2π ∥a∥ ∥b∥
2 2
Proof. As H is a diagonal matrix we can express the term as
E wTHwaTwbTw1(aTw≥0)1(bTw≥0)
w
p
=(cid:88) h Ew2aTwbTw1(aTw≥0,bTw≥0)
i i
i=1
p
=(cid:88) h E(eTw)2aTwbTw1(aTw≥0,bTw≥0),
i i
i=1
so we could just focus on each single term E(eTw)2aTwbTw1(aTw≥0,bTw≥0) firstly. By Gram-Schmidt orthogo-
i
nalization, we could denote
y =
aTw
, y =
∥bT b∥w
2
−ρ aby 1
,
1 ∥a∥ 2 2 (cid:112) 1−ρ2 ab
eT iw −ρ y − ρb√e−ρabρaey
∥ei∥2 ae 1 1−ρ2 2
y = ab ,
3 (cid:114)
1−ρ2 − (ρbe−ρabρac)2
ae 1−ρ2
ab
43where
aTb aTe a bTe b
ρ = , ρ = i = i , ρ = i = i ,
ab ∥a∥ ∥b∥ ae ∥a∥ ∥e ∥ ∥a∥ be ∥b∥ ∥e ∥ ∥b∥
2 2 2 i 2 2 2 i 2 2
then [y ,y ,y ]T ∼N(0,I ), and the single term cold be expressed as
1 2 3 3
E(eTw)2aTwbTw1(aTw≥0,bTw≥0)
i
(cid:32)(cid:115) (cid:33)
(ρ −ρ ρ )2 ρ −ρ ρ
=∥a∥ ∥b∥ ∥e ∥ E 1−ρ2 − be ab ac y + be ab aey +ρ y
2 2 i 2 ae 1−ρ2 3 (cid:112) 1−ρ2 2 ae 1
ab ab
(cid:18)(cid:113) (cid:19) (cid:113)
y 1−ρ2 y +ρ y 1(y ≥0,ρ y + 1−ρ2 y ≥0).
1 ab 2 ab 1 1 ab 1 ab 2
With detailed calculations, we could further obtain that
E(eTw)2aTwbTw1(aTw≥0,bTw≥0)
i
= 1 (ρ +2ρ ρ )arccos(−ρ )+ 1 (cid:113) 1−ρ2 (cid:0) 1+ρ2 +ρ2 (cid:1) ,
2π ab ae be ab 2π ab ae be
then sum over all single terms, we could obtain that
E wTHwaTwbTw1(aTw≥0)1(bTw≥0)
w
p
=(cid:88) h E(eTw)2aTwbTw1(aTw≥0,bTw≥0),
i i
i=1
=(cid:88) h ∥a∥ ∥b∥ 1 (cid:18) aTb +2 a ib i (cid:19) arccos(cid:18) − aTbT (cid:19)
i 2 22π ∥a∥ ∥b∥ ∥a∥ ∥b∥ ∥a∥ ∥b∥
2 2 2 2 2 2
i
(cid:115)
(cid:88) 1 (cid:18) aTb (cid:19)2(cid:18) a2 b2 (cid:19)
+ h ∥a∥ ∥b∥ 1− 1+ i + i
i 2 22π ∥a∥ ∥b∥ ∥a∥2 ∥b∥2
i 2 2 2 2
(cid:18) tr{H} aTb aTHb(cid:19) (cid:18) aTbT (cid:19)
= ∥a∥ ∥b∥ + arccos −
2 2 2π ∥a∥ ∥b∥ π ∥a∥ ∥b∥
2 2 2 2
(cid:115)
(cid:18) aTHa bTHb(cid:19) ∥a∥ ∥b∥ (cid:18) aTb (cid:19)2
+ tr{H}+ + 2 2 1− ,
∥a∥2 ∥b∥2 2π ∥a∥ ∥b∥
2 2 2 2
as tr{H}/µ (H)≥n2 and
1
aTHa bTHb
≤µ (H), ≤µ (H),
∥a∥2 1 ∥b∥2 1
2 2
we could further get that
E wTHwaTwbTw1(aTw≥0)1(bTw≥0)
w
(cid:18) tr{H} aTb aTHb(cid:19) (cid:18) aTbT (cid:19)
= ∥a∥ ∥b∥ + arccos −
2 2 2π ∥a∥ ∥b∥ π ∥a∥ ∥b∥
2 2 2 2
(cid:115)
(cid:18)
1
(cid:19)
∥a∥ ∥b∥
(cid:18)
aTb
(cid:19)2
+ 1+O( ) tr{H} 2 2 1− ,
n2 2π ∥a∥ ∥b∥
2 2
which finishe the proof.
E Technical Lemmas from Prior Works
Lemma13(Lemma10inBartlettetal.,2019). Thereareconstantsb,c≥1suchthat,foranyk≥0,withprobability
at least 1−2e−n c,
1. for all i≥1,
(cid:88)
µ (A )≤µ (A)≤µ (A )≤c ( λ +λ n);
k+1 −i k+1 1 k 1 j k+1
j>k
442. for all 1≤i≤k,
1 (cid:88)
µ (A)≥µ (A )≥µ (A )≥ λ −c λ n;
n n −i n k c j 1 k+1
1
j>k
3. if r ≥bn, then
k
1
λ r ≤µ (A )≤µ (A )≤c λ r ,
c k+1 k n k 1 k 1 k+1 k
1
where c >1 is a constant only depending on b,σ .
1 x
Lemma 14 (Corollary 24 in Bartlett et al., 2019). For any centered random vector z ∈ Rn with independent σ2
x
sub-Gaussian coordinates with unit variances, any k dimensional random subspace L of Rn that is independent of z,
and any t>0, with probability at least 1−3e−t,
√
∥z∥2 ≤n+2(162e)2σ2(t+ nt),
x
√
∥ΠLz∥2 ≥n−2(162e)2σ x2(k+t+ nt),
where ΠL is the orthogonal projection on L.
Lemma 15. There are constants b,c≥1 such that, for any k≥0, with probability at least 1−2e−n c:
1. for all i≥1,
(cid:88)
µ (A +λI)≤µ (A+λI)≤µ (A +λI)≤c ( λ +λ n)+λ;
k+1 −i k+1 1 k 1 j k+1
j>k
2. for all 1≤i≤k,
1 (cid:88)
µ (A+λI)≥µ (A +λI)≥µ (A +λI)≥ λ −c λ n+λ;
n n −i n k c j 1 k+1
1
j>k
3. if r ≥bn, then
k
1
λ r +λ≤µ (A +λI)≤µ (A +λI)≤c λ r +λ.
c k+1 k n k 1 k 1 k+1 k
1
Proof. With Lemma 13, the first two claims follow immediately. For the third claim: if r ≥ bn, we have that
k
(cid:80)
bnλ ≤ λ , so
k+1 j>k j
µ (A +λI)≤c λ r (Σ)+λ≤λ+c λ r ,
1 k 1 k+1 k 1 k+1 k
1 1
µ (A +λI)≥ λ r +λ≥ λ r (Σ)+λ,
n k c k+1 k c k+1 k
1 1
for the same constant c >1 as in Lemma 13.
1
Lemma 16 (Proposition2.7.1inVershynin,2018). For any random variable ξ that is centered, σ2-subgaussian, and
unit variance, ξ2−1 is a centered 162eσ2-subexponential random variable, that is,
Eexp(λ(ξ2−1))≤exp((162eλσ2)2),
for all such λ that |λ|≤1/(162eσ2).
Lemma 17 (Lemma 15 in Bartlett et al., 2019). Suppose that {η } is a sequence of non-negative random variables,
i
andthat{t }isasequenceofnon-negativerealnumbers(atleastoneofwhichisstrictlypositive)suchthat, forsome
i
δ∈(0,1) and any i≥1, Pr(η >t )≥1−δ. Then,
i i
(cid:32) (cid:33)
(cid:88) 1(cid:88)
Pr η ≥ t ≥1−2δ.
i 2 i
i i
Lemma 18 (Lemma 2.7.6 in Vershynin, 2018). For any non-increasing sequence {λ }∞ of non-negative numbers
i i=1
such that (cid:80) λ < ∞, and any independent, centered, σ−subexponential random variables {ξ }∞ , and any x > 0,
i i i i=1
with probability at least 1−2e−x
 
(cid:115)
(cid:88) (cid:88)
| λ iξ i|≤2σmaxxλ 1, x λ2 i.
i i
45Lemma 19 (Theorem 9 in Koltchinskii and Lounici (2017)). Let z ,...,z be i.i.d. sub-gaussian random variables
1 n
with zero mean, then with probability at least 1−2e−t,
∥EzzT −
1 (cid:88)n
z zT∥ ≤∥EzzT∥
max{(cid:114) trace(EzzT) ,trace(EzzT) ,(cid:114) t
,
t
}.
n i i 2 2 n n n n
i=1
Lemma 20 (Consequence of Theorem 5 in Tsigler and Bartlett (2023)). There is an absolute constant c > 1 such
that the following holds. For any k< n c, with probability at least 1−ce−n c, if A
k
is positive definite, then
tr{Σ[I−XT(XXT +λnI)−1X]2}≤(cid:32) (cid:88) λ (cid:33)(cid:18) 1+ µ 1(A k+λnI)2 + nλ k+1 (cid:19)
i µ (A +λnI)2 µ (A +λnI)
n k n k
i>k
 
+(cid:88) 1 (cid:18) µ 1(A k+λnI)2 + λ k+1 · µ 1(A k+λnI)2(cid:19) ,
λ n2 n µ (A +λnI)
i n k
i≤k
(cid:32) (cid:33)
tr{XΣXT(XXT +λnI)−2}≤ µ 1(A k+λnI)2 · k + n (cid:88) λ2 .
µ (A +λnI)2 n µ (A +λnI)2 i
n k n k
i>k
Lemma 21 (Proposition 1 in Jacot et al., 2018). For a network of depth L at initialization, with a Lipschitz
nonlinearity σ, and in the limit as n ,...,n →∞, the output functions f , for k =1,...,n , tend (in law) to
1 L−1 θ,k L
iid centered Gaussian processes of covariance Σ(L) is defined recursively by:
1
Σ(1)(x,x′)= xTx′+β2,
n
0
Σ(L+1)(x,x′)=E [σ(f(x))σ(f(x′))]+β2,
f∼N(0,Σ(L))
taking the expectation with respect to a centered Gaussian process f of covariance Σ(L).
Lemma22(Theorem1inJacotetal.,2018). ForanetworkofdepthLatinitialization,withaLipschitznonlinearity
σ, and in the limit as the layers width n ,...,n →∞, the NTK Θ(L) converges in probability to a deterministic
1 L−1
limiting kernel:
Θ(L) →Θ(L)⊗Id .
∞ nL
The scalar kernel Θ( ∞L) : Rn0 ×Rn0 →R is defined recursively by
Θ(1)(x,x′)=Σ(1)(x,x′),
∞
Θ =Θ(L)(x,x′)Σ˙(L+1)(x,x′)+Σ(L+1)(x,x′),
∞(L+1)(x,x′) ∞
where
Σ˙(L+1)(x,x′)=E [σ˙(f(x))σ˙(f(x′))]
f∼N(0,Σ(L))
taking the expectation with respect to a centered Gaussian process f of covariance Σ(L), and where σ˙ denotes the
derivative of σ.
46