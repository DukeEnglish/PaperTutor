Depending on yourself when you should: Mentoring LLM with RL agents to
become the master in cybersecurity games
YikuanYan1∗, YaolunZhang1∗, KemanHuang1,2†
1SchoolofInformation,RenminUniversityofChina,Beijing,China
2CybersecurityatMITSloan,MIT,Cambridge,Massachusetts,USA
{yanyikuan,zhangyaolun5,keman}@ruc.edu.cn
Abstract al.,2023]butalsoexhibitformidableinteractivecapabilities
asassistantsoragents[Sandovaletal.,2023].
Integrating LLM and reinforcement learning (RL)
However, LLM agents lack the specific knowledge of the
agenteffectivelytoachievecomplementaryperfor-
local environment, incur higher training costs [Hu et al.,
manceiscriticalinhighstaketaskslikecybersecu-
2023] and can stuck in hallucinations [Ji et al., 2023; Chen
rity operations. In this study, we introduce Secu-
and Shu, 2023], while also presenting attackers with power-
rityBot, a LLM agent mentored by pre-trained RL
ful weapons, making them double-edge sword for cyberse-
agents,tosupportcybersecurityoperations. Inpar-
curity[ChenandShu,2023;Taddeoetal.,2019]. Recentre-
ticularly,theLLMagentissupportedwithaprofile
searchattemptstoframeACOaspartiallyobservableMarkov
module to generated behavior guidelines, a mem-
processes(POMDP),employingreinforcementlearning(RL)
ory module to accumulate local experiences, a re-
methods to train autonomous agents [Standen et al., 2021;
flection module to re-evaluate choices, and an ac-
Team., 2021]. However, without appropriate tuning meth-
tion module to reduce action space. Additionally,
ods, RLagentstendtoconvergetolocaloptima, lackingro-
itadoptsthecollaborationmechanismtotakesug-
bustnessandgeneralizationcapabilitiesdespiteachievingfa-
gestions from pre-trained RL agents, including a
vorable results [Palmer et al., 2023]. As prior studies have
cursor for dynamic suggestion taken, an aggrega-
demonstrated that collaborations among multiple agents can
tor for multiple mentors’ suggestions ranking and enhance team performance [Dong et al., 2023; Ma et al.,
a caller for proactive suggestion asking. Build-
2023], enabling the effective collaborations between LLM
ingontheCybORGexperimentframework,ourex-
agentsandRLagents,whichcanleveragethegeneralization
periencesshowthatSecurityBotdemonstratessig-
knowledge of LLMs and the specialized knowledge of RLs
nificant performance improvement compared with
incybersecurityscenarios,canbepromisingtoachievecom-
LLMorRLstandalone,achievingthecomplemen-
plementaryperformancebeyondthatofindividualagent.
taryperformanceinthecybersecuritygames.
Hence, we introduce the SecurityBot, a collaborative
framework utilizing RL agents as mentors for LLM agent
to support cybersecurity operations. We integrate four ef-
1 Introduction
fective modules – profiles, memory, reflection and action –
Cybersecurityoperationsinvolvetheparticipationofvarious intotheLLM.Simultaneously,weproposeadynamicmech-
entities such as attackers and defenders. With the advance- anism consisting of a cursor to dynamically incorporate RL
mentofartificialintelligence(AI),autonomouscyberopera- agents’suggestions, anaggregator toranksuggestionsfrom
tion (ACO) agents have emerged as a promising solution in differentRLagents,aswellasacallertoproactivelyrequest
cybersecurity operations [Vyas et al., 2023]. These agents mentoring from RL agents. We conduct experiments on the
continuallyengageinadversariallearningwithinnetworken- open-source ACO research platform, CybORG [Standen et
vironments,enhancingtheirstrategiccapabilities. Therecent al., 2021], comparing the red team (attacker) task and blue
proliferation of large language models (LLMs) has signifi- team (defender) task performance among: (1)independently
cantlybolsteredthecapabilitiesofautonomousagents[Wang executing RL or LLM agents (Independent), (2) collabora-
etal.,2023a]. Incomparisontotraditionalmachinelearning tionbetweenaLLMagentandaRLagent(Single-Mentor),
agents, LLM agents possess extensive knowledge, enabling and(3)collaborationbetweenaLLMagentandmultipleRL
them to handle richer and more complex information, cou- agents (Multi-Mentors). Our experimental results demon-
pledwithrobustcontextualandreasoningabilities[Linetal., stratethatthedevelopedSecurityBotcaneffectivelyimprove
2023; Wang et al., 2023b; Wang et al., 2023c]. They not boththeredteamandblueteamtaskperformancecompared
only surpass state-of-the-art methods as novel tools [Xia et toindependent LLMorRLapproaches. Furthermore, while
mentoring from multiple RL agents can be beneficial, the
∗Theseauthorscontributedequally. guidance of poorly performing RL agents may be noise to,
†Correspondingauthor. andresultintounstableperformance.
4202
raM
62
]RC.sc[
1v47671.3042:viXra• We introduce SecurityBot, a mechanism to enable the 2.2 CollaborationmechanismstoimproveLLMs
effectivecollaborationbetweenLLMandRLagents,to
Recentstudieshaveexploreddifferentmechanismstosupport
leverageRLagentsasmentorstoacceleratelearningfor
LLM’scollaborationswithothers,eitherLLM-basedorRL-
LLMagentsandachievecomplementaryperformance.
basedagents,including:
• The collaboration of LLM and RL agents demon-
Role-basedmulti-LLM-agentcollaboration
strates performance improvement in both red team and
blue team tasks, providing a promising solution of au- Within LLM-based multi-agent systems, LLM-based agents
tonomousagentsforcybersecurityoperations. areassignedwithdifferentroles, likedecomposingcomplex
tasks, identifying errors, and collecting multiple perspec-
tives. Then they collaborate with each other through a se-
2 RelatedWork
ries of processes to resolve complex tasks such as software
2.1 LLMsforcybersecurityoperations developments [Dong et al., 2023; Qian et al., 2023; Hong
et al., 2023], sociological investigations [Park et al., 2023;
Given the rapid development of LLMs and the eager to in- Wang et al., 2023b; Zhang et al., 2023], simulation of mul-
corporate advanced AIs into cybersecurity operations [Ian- tiplayer games [Sandoval et al., 2023; Xu et al., 2023] and
noneetal.,2022],recentstudieshavestartedtoexploreusing
various challenges (such as logical reasoning, stock advice,
LLMstoenhancecybersecuritywhileseveralevidencesalso blogcomposing,andmore)[Lietal.,2023;Wuetal.,2023;
reveal abusing LLMs to bring advanced threats, making it a Talebirad and Nadiri, 2023]. In particularly, different role-
double-edgedsword[Taddeoetal.,2019;Yaoetal.,2023]
based agents exchange ideas through conversation, enforce
toolstoundertaketasks,garnerfeedback,leadingtosuccess-
LLMtoenhancecybersecurity
fulcollaboration[Wangetal.,2023a].
LLMsdemonstrateadvantagesinbothcodesecurityanddata
security [Noever, 2023; Ali and Kostakos, 2023; Qi et al., Dual-process-basedLLM-RLcollaboration
2023]. For example, Fuzz4All [Xia et al., 2023] utilizes Thedualprocesstheoryhighlightsthathumancognitioncon-
LLMs as input generators and mutation engines to generate sists of two mental systems where System 1 is autonomous
diverseinputsforvariousprogramminglanguages,achieving and characterized by rapid intuition, while System 2 con-
an36.8%coverageimprovementcomparedtopreviousstate- trols slow, deliberate thinking [Wason and Evans, 1974;
of-the-arttechniques. Kahneman,2011]. Groundedonthistheory,SwiftSageintro-
Additionally,comparedtotraditionalmachinelearningap- ducesaframeworkthatenablesasmallRLmodel,actingas
proaches,LLMspossessmorepowerfulnaturallanguagepro- theSystem1component,tocollaboratewithanLLM-Based
cessing and contextual understanding capabilities, allowing agent, acting as the System 2 component. This structure ef-
them to elevate cybersecurity from specific to more macro- fectivelysolvecomplexproblemswhilereducingthecostof
scopic tasks. For example, some researches[Deng et al., inference[Linetal.,2023].
2023; Pearce et al., 2023] utilized these capabilities in spe-
cificsecuritytaskstoenhanceeffectiveness,whileMcIntosh LLMsettingguidancetosupportRL
et al.[McIntosh et al., 2023] take a further step to compared Some recent studies incorporate the LLM to generate or
GPT-generated Governance, Risk, and Compliance (GRC) learn the reward function for RL agents, aiming at simpli-
policies with those from established security vendors and fying the reward function design process [Ma et al., 2023;
government cybersecurity agencies, recommending GPT in- Carta et al., 2022]. For example, [Micheli et al., 2023;
tegrationintocompanies’GRCpolicydevelopment. Kwon et al., 2023; Du et al., 2023] use LLM as a proxy
reward function to guide RL agents in environments with-
LLMs’double-edgedswordroleforcybesecurity out clear reward signals. Additionally, [Brohan et al., 2023;
However,applyingLLMstocybersecurityisadouble-edged Dasguptaetal.,2023]utilizetheLLM-Basedagentasaplan-
sword [Taddeo et al., 2019]: being generative in nature can nertoguideRLagentincomplexanddynamicenvironments.
leadtohallucinations—thegenerationofmisleadingorincor-
rect content, and can not effectively discern security-related RLactingasexperttoguideLLM’sdecision
fallacies, which can be catastrophic for high-stakes security LLM demonstrate powerful generalization abilities, but
tasks [Ji et al., 2023]. These errors can compromise sensi- under specific scenario, they perform poorly due to
tive operations, thereby introducing substantial risks [Chen the lack of expert trajectories. In contrast, RL models
andShu,2023]. AsLLMsbecomemoreintegratedintosecu- possess expert trajectories. Hence, [Hu et al., 2023;
rityframeworks,theimperativetoaddressandmitigatethese Wan et al., 2022] use RL methods assist the LLM-Based
challengesgrowsevermorecritical. agent in comprehending the environment, mastering expert-
Furthermore, LLMs present attackers with powerful like actions, which results in better effects and lower
weapons. Recent studies have demonstrated that LLMs can interactioncostinstructions.
significantlyenhanceattacksacrosshardware[Yaman,2023],
softwareandnetwork[ChenandShu,2023]levels,especially Overall, LLMs has demonstrated promising potential in
that LLMs possess human-like reasoning capabilities, mak- enhancingcybersecurityoperationswhiletheirdouble-edged
ing user-level attacks even more severe [Yao et al., 2023; swordroleraisespecificconcerns. Additionally,recentstud-
Falade,2023;Botacin,2023]. ies have explored different collaborations with LLMs but itis still in its early stage, especially for cybersecurity oper- host’saccessstatus. Theredteamachieveslateralmovement
ations. Hence, using the cybersecurity adversarial game as between subnets by discovering new hosts through connec-
theresearchcontext,wedesignaframeworkwithfourplugin tions from the privileged host. We set the game to be zero-
modulesandthreecollaborationmechanismstopowerLLMs sum,whichmeansthattheblueteam’srewardistheopposite
forcybersecurityoperations,includingbothactingasattack- oftheredteam’sreward. Therewardateachstepisbasedon
ersanddefenders. theextentofredteam’sexploitation,
3 CybersecurityAdversarialGameand n
(cid:88)
Reward = V ×A (1)
Pre-trainedRLAgents t i,t i,t
i=1
Beforedetailingourdesign,westartwithbrieflyintroducing
ourresearchcontext: thecybersecurityadversarialgame. In whereV i,tandA i,trepresentsthevalueandtheaccesssta-
particularly, wehaveconstructedacybersecurityadversarial tusofhost iatsteptrespectively.
gameutilizingCybORG[Standenetal.,2021],anexemplary
RL-based Autonomous Cyber Operation (ACO) gym. ACO
supportsthecreationofdecision-makingagentsforboththe
blueteam(defender)andtheredteam(attacker)inadversar-
ialscenarios, andconveysstructuredandunstructuredinfor-
mation,enablingtheadaptationofbothRLandLLMagents.
3.1 CybersecurityAdversarialGames
The scenario adopted in this study is derived from TTCP
CAGEChallenge11,anopenchallengeonCybORGin2021. Figure 2: Action-Status Transition. Red text represents red team
AsillustratedinFigure1,theredandblueteamscompetein actions,bluetextrepresentsblueteamactions.
a simulated network environment, which can be modeled as
apartiallyobservedMarkovprocess(POMDP).Ateachstep,
the red team and blue team take actions sequentially in the 3.2 Pre-trainedRLAgents
environment,causingchangesintheenvironmentalstate.
Environment & Observation. The environment com- In this study, we choose three representative RL algorithms
prisesanetworkconsistingof13hostsdividedintothreesub- totrainredteamandblueteamagents2:
nets. Theredteamcommencesfromthefootnodeintheuser
• A3C (Asynchronous Advantage Actor-Critic) [Mnih et
subnetwithoutknowledgeofanyotherhosts. Theblueteam
al., 2016] combines policy gradient and value function
possessesinformationaboutallhostsbutlacksknowledgere-
methods by asynchronously training multiple agents to
gardingtheredteam’saccessstatustothehosts.
improveefficiency.
ForboththeredandblueteamRLagents,theirvectorob-
servation at each step encompasses: (1) whether the last ac- • DQN (Deep Q-Network) [Mnih et al., 2013] utilizes
tion is success, (2) whether the adversary has operated on a deepneuralnetworkstoapproximatetheQ-valuefunc-
specifichost,and(3)theredteam’saccessstatusofaspecific tiontoguidetheagent’sdecisions.
host.Notethattheobservationisnotguaranteedaccuratedue
• PPO (Proximal Policy Optimization) [Schulman et
tothepresenceofanadversary.
al., 2017], a policy gradient method, ensures stabil-
itythroughproximalpolicyoptimization,restrictingthe
magnitudeofpolicyupdates.
TheRL-basedenvironmentfacilitatesagent’straining.Red
team and blue team agents are trained separately, with one
agent trained at a time. For agent’s adversary, we applied
thefixed-strategyagentsprovidedinCybORG.Inparticular,
whentrainingared-teamRLagent,weuseablue-teamagent
withfixedstrategywhichrandomlyperformsRemoveorRe-
store operations when encountering suspicious hosts during
each Monitor action. When training a blue-team RL agent,
the red-team agent as the adversary gains access to network
Figure1: APOMDPcybersecurityadversialgame. Theredhostin
nodes one by one based on a breadth-first strategy. Our ap-
UserSubnetrepresentsthefootnodeoftheredteam.Thebluehost
inEnterpriseSubnetrepresentsthedefenderhostoftheblueteam. proach aligns with the conventional RL training paradigm,
wherein the agent takes an action at each step, assimilates
new observations and associated rewards, and incrementally
Action & Reward. As shown in Figure 2, the two teams
refinesitsstrategicframework.
eachhavethreereciprocalactionsthatcausetransitionsinthe
1https://github.com/cage-challenge/cage-challenge-1 2OurframeworkisflexibletouseotherRLalgorithms.4 SecurityBot: anLLM-basedagent
mentoredbyRLagents
Asshownin3, ourSecurityBotcontainsthreemainparts: a
LLM-basedAgent,thepre-trainedRLagentpoolasmentors
andtheircollaborativemechanisms.
Figure4: Theillustrationofprofilemodule,includingtheexample
of roles, goals, actions, environment format and the generated be-
haviorguidance(thebottompart)aswellastheprocesstogenerate
thebehaviorguidance(theupperpart).
Figure 3: The Framework of SecurityBot: LLM-based RLs-
Figure5: ThepromptforRedAgentfromthereflectionmoduleto
mentoringAgentforCybersecurityOperation
motivatetheLLMtochooseotherattackactions.
4.1 LLMAgentDesign • Relevance: measuringitsenvironment’ssimilaritywith
thecurrentone. Wetransformedeachenvironmentinto
Building upon the LLM, GPT 3.5-turbo, our LLM agent in- vectors,andthencalculatetheircosinesimilarity.
cludesfourpluginmodulesfordecisionmakingineachstep:
• Freshness: measuring its freshness, represented as the
Profilemodule reciprocalofitstimestampgapwiththecurrentstep.
As shown in Figure 4, the Profile module initializes each Finally, we calculate the product of the importance, rele-
agent’srole,goal,andavailableactionsdependingonitsrole. vance and freshness for each memory record and select the
Inparticular,wedesignapromptincludingtheexpectedfor- toptwoasthememoryinputforLLMwhenmakingdecision.
mat for the observed environment as the input, and the ex-
pectedoutputwhichisanactionsequenceincludingaseries Actionmodule
of actions with its goal, trigger, following actions, and ex- The Action module plays a crucial role in guiding the LLM
pected outcome. When initializing the LLM agent, we use agenttotakevalidactionforeachstep. Inparticularly,given
thisprompt, togetherwiththeassignedgoal, action, anden- theobservedenvironmentandtheavailableactionsprovided
vironment format, to ask the LLM to generate an action se- bytheprofile,thismodulewillgeneratetheactionspacewith
quenceandaddittotheprofile,servingastheglobalbehavior allthepotentialactionsthattheagentcouldtake.
guidancefortheLLMagent.
Reflectionmodule
Memorymodule
Given the complex and dynamic environment, as the adver-
The Memory module is used to store past experiences and saryagentmaychangetheenvironmentbutisunobservantto
searchtherelatedonesfordecisionmakingineachstep. theLLMagent,theLLMagentmayencounterdilemmassit-
MemoryStorage. Thememorymodulestoresrecordsin- uation,reflectedasrepetitiveactionsordiminishingrewards.
cluding the timestamp, observed environment, action taken, For example, the red agent might persist in attacking a host
and the outcome including the action status (success or fail- inthenetwork,evenwhensuchanactionhasbeenprovenfu-
ure)anditsreward. Inparticular,whenstoringeachmemory tile. Hence,thereflectionmoduleisdesignedtomonitorthe
record,theLLMagentratesitsImportancebypromptingthe dilemmastatusandtriggerthereflectionprocess.
LLMtoscoreitonascaleof0to10. DilemmasMonitor. Ateverystep, theReflectionmodule
MemorySearching. Whensearchingmemoriestosupport evaluatesboththeRewardListandtheActionListfromthe
action selection in each step, the LLM agent will calculate previous steps. If there is no increase in rewards or if the
eachmemoryrecord’sRelevanceandFreshness: agent repeats an action, the module will collect these suspi-ciousactions, includingtheseriesofactionsassociatedwith parameter α to control the change race and θ to represent
lr
thoserecords,andthenactivatethereflectionprocess. theminimalrewardincrementthatwewouldexpecttheLLM
ReflectionProcess. Thereflectionprocesswillpassthese agenttogain.
suspiciousactionstotheActionmoduleandremovethemif
they are included in the generated action space. Addition-
ind =ind +(f −f )
ally, as shown in Figure 5, the process provides the LLM t t−1 t t−1
(cid:124) (cid:123)(cid:122) (cid:125)
withaprompt,elucidatingthattheagentisstuckinthedilem- part1
masandprovidingthepossiblereasonstoguidetheLLMto +min(α×ind t−1,(r t−1−r t−2−θ lr)) ×sgn(ind t−1−θ ind)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
chooseotheractionstogetoutofthedilemmasituation. part2 part3
(2)
4.2 CollaborationwithRLagents
1
UsingRLagentsasmentorstoguidetheLLMagentiscritical f = . (3)
forSecurityBottoachievebetterperformance. Morespecif-
x 1+e−kx
ically, as shown in Figure 6, we design three collaboration
mechanisms: −1 if x>0
sgn(x)={ . (4)
1 otherwise
Aggregator: rankingsuggestionsfrommultiplementors
RatherthanrelyingononlyoneRLagent,theLLMcanrefer
tomultipleRLagents,asdifferentRLagentsmaycatchdif-
ferent aspects of the tasks. Hence, we further introduce the
aggregator mechanism to aggregate suggestions from mul-
tiple RL agents. In particular, given the top three sugges-
tionsfromalltheRLmentorsassociatedwithconfidence,the
multi-mentor mechanism will sort them based on the confi-
denceandthetoponewillbepresentedtotheLLMandthe
topthreeactionswillbeprovidedwhileindilemmas. Insuch
Figure 6: Mechanisms to collaboration with RL agents. Different a case, the LLM agent does not necessarily always get sug-
colorreferstosuggestionsofdifferentRLmentors. gestions from one specific RL agent during the whole task
duration.
Cursor: growingtobeindependent Caller: askingforhelpproactivelywhenindilemma
Firstly,theRLagentsarepre-trainedinthesameenvironment As discussed above, when the LLM agent encounters a
particularlytoguaranteethattheycanprovideknowledgeto dilemma, the reflection module will be activated. Further-
mentor the LLM agent to make better decisions, especially more, beyond activating the reflection process, the LLM
in the early stage when LLM agents contain no information agent can further refer to RL agents for support. Unlike re-
regarding the environment. However, as time goes by, the ferring to RL mentors’ input in normal situation where only
LLM agent, with its capacity to understand complex envi- onesuggestionisprovided,wewillprovidethetopthreecon-
ronments and accumulated experience, can surpass the RL fidentsuggestionsfromtheRLmentors.
mentors (which we will report later). Hence, we design the
mechanismCursortodecidewhethertheLLMagentshould
5 ExperimentsandResults
takesuggestionsfromRLagents.
Inparticular,foreachstept,theCursormodulewillcalcu-
5.1 ExperimentSetup
lateanindependencevalueind andonlywhentheindepen-
t
dencevalueind isbelowthegiventhresholdθ ,theLLM Environment. FollowingthesetupofCageChallenge1,we
t ind
agentwillconsidersuggestionsfromRLagents. Otherwise, setthemaximumnumberofstepsinoneepisode,i.e.,acom-
the LLM agent will make the decision by itself. Hence, the plete round of the game, to be 100. As mentioned earlier,
Cursor module will adjust an independence value ind in a wesettworewardparametersasshowninTable1: (1)Host
t
waytoreflectsthetendencytorelyonitselfandconsiderthe value. The hosts in different subnets have different values,
mentor’s suggestion when it proves beneficial. As detailed and(2)Accessstate. Thehighertheaccessstateofahost,the
in Equation 2, we adopt the monotonically increasing func- highertheproportionofhostvalueobtainedbytheredteam.
tion f (Equation 3) so that part reflects the trend to rely
x 1
on the LLM itself. part 2 represents the trend of gaining re- Table1:Parametersofagentreward.
wardfrompreviousactionswhilepart isthesignalfunction
3
(Equation 4) indicating whether the action is chosen when
HostSubnet(V) Reward Accessstatus(A) Reward
consideringsuggestionsfrommentors. Inotherwords,ifthe
UserSubnet 0.1 Unknown/Known 0
LLM agent achieves an increasing reward without mentor-
EnterpriseSubnet 1.0 Exploited 0.5
ingbytheRLs,wewouldincreasetheindependencevalueto
OperationalSubnet 10.0 Privileged 0.89
makeLLMagentmoreindependent. NotethatweintroduceRL Training. The RL training process is based on the (a) Red: LLM Agent vs RL Agent(PPO)
40 LLM
Ray RLlib, a Python library for RL3. Each training process PPO
consistsofatotalof100iterations(4000episodesintotal). 30
LLMs Setup. We leverage OpenAI’s gpt-3.5-turbo API 20
forbuildingtheLLMAgent. Allthetemperaturesaresetto0 10
torestricttheformatofLLMoutput.
0
• Reflection.Iftheactionisrepeatedinthelastthreesteps, 0 20 40 53 60 80 100
Step
orifthereisnoincreaseinrewardvaluesinthelastfive (b) Red: LLM Agent with Different RL mentor (c) Red:Single RL mentor vs Multi RL mentors
LLM MultiMentor
steps,thereflectionmechanismwillbetriggered. 40 PPO&LLM 40 PPO&LLM
DQN&LLM DQN&LLM
30 A3C&LLM 30 A3C&LLM
• Cursor. θ issetto0.6. θ issetto0.3. αissetto0.3.
ind lr
20 20
kinf(x)issetto0.0135.
10 10
Measurement. Weconsiderthefollowingmeasurements.
0 0
• Stepreward. Therewardofeachstep. 0 20 40 60 80 100 0 20 40 60 80 100
Step Step
• CollaborationRate(Col). Therateofcooperationwith Figure7: Resultofredteamtask. (a)ComparisonbetweenLLM
RLagents. and PPO. They have different performances in different stages.
(b)Single RL mentor result. PPO&LLM surpasses all others.
• DilemmaRate(DR). TherateofcollaboratingwithRL
(c)ComparisonbetweenMultiandSingleRLmentor. PPO&LLM
agentstriggeredbytrappingintodilemmasituation. stillperformsbest
• Accept Rate (AR). The rate that LLM agent takes RL
mentor’ssuggestion,indicatingtheextenttowhichLLM ComplementknowledgeofLLMagentsandRLmentors
AgentreliesonRLmentors.
As depicted in Figure 7(a)5, the reward curves of the LLM
• Accept Rate in dilemma (AR ). The rate LLM agent agentandthePPOagentintersect: thePPOagentrapidlyac-
d
take suggestions when trapping into dilemma, showing cumulatesrewardsearlyon,levelingofflater. Thisbehavior
theabilityofRLmentorstohelpLLMAgentout. arisesfromthePPOagentgainingenvironmentalknowledge
duringtraining,recognizingthehighvalueofhostsintheOp-
Experiment Group. We incrementally add collaboration
erationalsubnet. Whileexhibitingdepth-firstcharacteristics,
modules and assess their performance for both the red and
insufficienttrainingcausesittoconvergetoalocaloptimum.
blueteam. Foreachgroup,werunthesimulationfor5times
Conversely, the LLM agent, despite a modest early-stage
andcalculatetheaverage.
reward,achievesrapidgrowth,outperformingthePPOagent
• Independent. EachRLagent(A3C,DQN,PPO)andour in the later stage. The LLM agent’s behavior follows a
designedLLMagentconductthetaskindependently. breadth-first pattern, accumulating more exploited hosts in
thenetworkefficientlyavoidingdefenderblocks,resultingin
• Single-mentor. TheLLMagentcooperatewithasingle ahigherreward.
RLagent(A3C&LLM,DQN&LLM,PPO&LLM). Takingastepfurther,wefindthatLLMagentsoutperform
PPO agents in single-step gains occurring at step 53 on av-
• Multi-mentors. LLMagentcooperatewithallthreedif-
erage, where we differentiate the early and later stages. In
ferentRLagents(MultiMentor).
laterstage,wefindthatRLmentorsalwaysrepeatoneaction,
whileLLMagent,withthereflectionmodule,canpreventthe
5.2 PerformanceinRedTeamTask
problem. This can be the reason why RL mentors’ perfor-
In the red team task, LLM agents and RL agents exhib- manceisworsethanLLMagentinthestage.
iteddistinctactionpatterns,indicativeofdifferingknowledge
Amplificationeffectofsingle-mentormechanisms
bases. While collaborative synergy can surpass individual
agent performance, optimal collaboration is achieved when A stronger RL mentor enhances collaborative performance,
RLagentsexhibitsuperiorperformance. However,whenthe otherwise it may slows down the LLM agent’s process. As
LLMagentsconsiderssuggestionsfrommultipleRLagents, shown in Figure 7(b), PPO and A3C agents exhibit superior
itstrugglestoefficientlyprocessthisinformation,leadingto collaborative performance compared to LLM agents alone,
adeclineincollaborativeperformance.4 andinparticular,thePPO&LLMgroupdemonstratingasyn-
ergistic 1+1 > 2 effect throughout the process, as well as
3WefocusesonthecollaborationbetweenRLagentsandLLM gettingintotherapid-growthphasemuchearlier.
agents, rather than training a better RL agent. Hence, we choose Furthermore,thecooperationmechanismguidestheLLM
the adversary using the simplest strategy and default parameters agenttolearnfromRLmentorsintheearlystagewhileseek-
withoutparametertuningforthetrainingalgorithmsareused. All ing help in dilemmas. As shown in Table 2, the LLM agent
specific algorithm parameters can refer to https://github.com/ray-
project/ray/blob/master/rllib/algorithms/ 5TheperformanceofthethreeRLagentsvaries,whilethePPO
4Wesmoothedthedatausingexponentialsmoothingandcalcu- agentdemonstratingsuperiorperformance.Duetospacelimitation,
latedconfidenceintervals weonlyreporttheperformanceforPPOagent.
draweR
petS
draweR
petSTable2:Cooperationmetricofredteamtask outperformsthePPOagent. Weobservethesimilarsituation
in the case of single mentor. As shown in Figure 8 (b), al-
Metric PPO&LLM A3C&LLM DQN&LLM thoughPPO&LLMgroupdemonstratesamarginallysuperior
performanceoverLLMagent,thisadvantageisnotobserved
Early\Later Early\Later Early\Later
Col 61.5%\33.3% 78.8%\56.2% 53.8%\43.7% inothergroups. Theseresultsindicateanarrowerknowledge
DR 50.0%\100.0% 34.1%\55.6% 39.3%\80.9% gapbetweenLLMandRLagentsinblueteamtask,maydue
AR 50.0%\63.6% 29.2%\51.9% 35.7%\57.1% to the fact that the whole network environment is used for
AR d 50.0%\63.6% 28.6%\53.3% 27.3%\52.9% pre-trainingRLagentsandprovidedtoLLMagent.
Additionally,asreportedinTable3,theLLMagentwould
(a) Blue: LLM Agent vs PPO Agent accept RL mentors’ suggestions in the early stages. While
0 LLM in the later stage, both A3C&LLM and DQN&LLM groups
PPO
−10 showlittleinterestinRLmentor’ssuggestionexcepttrapped
−20 indilemmas. Conversely,wecanobserveconsistentlyhigher
ARratesinthelaterstagesforPPO&LLM.Thisdiscrepancy
−30
indicates the LLM agent’s capability in identifying the sug-
−40
gestionqualityandtheimportanceofprovidinghighquality
0 20 34 40 Step 60 80 100 suggestiontoimprovetheLLMagent’seffectiveness.
(b) Blue: LLM Agent with Different RL mentor (c) Blue:Single RL mentor vs Multi RL mentors
0 LLM MultiMentor
−5 PPO&LLM 0 PPO&LLM Table3:Cooperationresultinblueteamtask
DQN&LLM DQN&LLM
−10 A3C&LLM A3C&LLM
−15 −10
−20 Metric PPO&LLM A3C&LLM DQN&LLM
−20
−25
−30 −30 Early\Later Early\Later Early\Later
−35 Col 48.1%\22.9% 78.8%\33.3% 71.2%\16.7%
−40 −40
DR 40.0%\45.5% 43.9%\100.0% 27.0%\100.0%
0 20 40 60 80 100 0 20 40 60 80 100
Step Step AR 100.0%\81.9% 53.7%\28.6% 91.9%\28.6%
Figure8: Resultofblueteamtask. (a)ComparisonbetweenLLM AR d 100.0%\60.0% 31.3%\28.6% 100.0%\28.6%
andPPO.LLMoutperformPPOinBlueTeamTask. (b)SingleRL
mentor result. PPO&LLM perform slightly better than LLM. (c)
ComparisonbetweenMultiandSingleRLmentor.Multi-mentorper- Outstandingbutunstableperformanceofmulti-mentors
formbestonaverage,butnotstableenough.
Incontrasttotheredteamtask,asshowninFigure8(c),the
incorporation of multiple RL mentors enhances the average
collaborates more with RL mentors in the early stages than performance of the blue team task beyond that of both the
later,satisfyingourdesigngoal.DRareallhigherinthelater LLM agents and the PPO&LLM group. However, this con-
stage, meaning most collaborations with the RL agent are figurationexhibitsinstabilitydemonstratedasalargerconfi-
triggered by the dilemmas situation. Interestingly, AR and denceintervals. Whileiteffectivelydefendsnearlyallhosts
AR valuesarebothhigherinthelaterstage,meaninginthe at times, in some instances, its performance is comparable
d
later stage, despite outperforming the RL mentor, the LLM to that of a single LLM. Notably, the LLM Agent accepts
agentreliesmoreontheRLmentor’ssuggestionsifneeded. lessthan5%ofsuggestionsfromRLmentors,predominantly
originating from DQN. One reason behind this is that the
Noisefrommulti-mentors
mostconfidentRLsuggestionsarenotconsistentlythemost
WeexploredwhethertheLLMagentcouldgainmoreknowl-
effective,especiallywhenprovidedbymultiplementors.
edgefromrecommendationsofmulti-mentors. Inoursetup,
Additionally, in the blue team task, LLM agents show-
assistance from multiple RL mentors is not necessary help-
caseasuperiorunderstandingoftheenvironment, oftenact-
ful.AsshowninFigure7(c),whiletheperformanceofmulti-
ingindependentlyinmostsituations.Particularlyinscenarios
tutors slightly outperforms LLM alone, it falls short of the
where the LLM agent successfully defends almost all hosts,
LLM&PPOgroup. Weobservedthat75.61%ofsuggestions
it appears to disregard unreliable suggestions from multiple
fromRLmentorsoriginatedfromDQN,butonly5.41%were
RLmentors,optingtomakecriticaldecisionsautonomously.
accepted. Incontrast,34.61%ofPPO’ssuggestionswereac-
cepted. Moreover, only 15.85% of all RL suggestions were
6 ConclusionandFutureWork
accepted,markedlylowerthantheacceptancerateinasingle
mentor scenario. This disparity illuminates the high confi- This study presents SecurityBot, a LLM agent powered by
dence suggestion from the low performance mentor became mentoring from pre-trained RL agents for cybersecurity op-
anoisefortheLLMagent. erations. In particular, with the designed plugin modules,
including the profile, memory, reflection and action mod-
5.3 PerformanceinBlueTeamTask
ules to enhance the LLM, and three collaboration mecha-
Ahelpfulbutnarrowercomplementaryknowledge nisms,includingacursor,anaggregatorandacaller,toeffec-
AsshowninFigure8(a),theLLMagentsdemonstrateperfor- tivelycollaboratewithpre-trainedRLagents,theLLMagent
mancesimilartothePPOagentduringtheearlystages. But achieve significant performance improvement in both cyber
afterabriefperiodofdivergence,theLLMagentconsistently attackanddefensetasks. AlthoughRLagentscanlearnlocal
draweR
petS
draweR
petSknowledge effectively through pre-training, the LLM agent [Falade,2023] Polra Victor Falade. Decoding the threat
can surpass them through learning the environments in the landscape: Chatgpt,fraudgpt,andwormgptinsocialengi-
later stage. This confirms that our designed LLM agent can neeringattacks. arXivpreprintarXiv:2310.05595,2023.
beapromisingsolutiontosupportcybersecurityoperations.
[Hongetal.,2023] Sirui Hong, Xiawu Zheng, Jonathan
While RL agents’ suggestions can be helpful, especially
Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili
whentheLLMagenttrappedindilemmas,asobservedinour
Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou,
result, weak RL agents may serve as a noise to distract the
et al. Metagpt: Meta programming for multi-agent col-
LLMagent. Furtherresearchcandesignadvancedaggregat-
laborative framework. arXiv preprint arXiv:2308.00352,
ingstrategiestoextracttheessenceanddiscardthedrossfrom
2023.
RLagents. Furthermore,whileweaimatempoweringLLM
withpluginmodulesandcollaboratingitwithRLagents,we [Huetal.,2023] BinHu, ChenyangZhao, PuZhang, Zihao
did not fintune the LLM or optimize the RL agents. Future Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. En-
studiescanfintuneabetterLLMmodelspecificforcyberse- abling intelligent interactions between an agent and an
curityoperationsandtrainoptimizedRLagents,whichcould llm: A reinforcement learning approach. arXiv preprint
furtherimprovetheSecurityBot’sperformance. arXiv:2306.03604,2023.
[Iannoneetal.,2022] Emanuele Iannone, Roberta
References Guadagni, Filomena Ferrucci, Andrea De Lucia, and
[AliandKostakos,2023] Tarek Ali and Panos Kostakos. FabioPalomba. Thesecretlifeofsoftwarevulnerabilities:
A large-scale empirical study. IEEE Transactions on
Huntgpt: Integratingmachinelearning-basedanomalyde-
SoftwareEngineering,49(1):44–63,2022.
tection and explainable ai with large language models
(llms). arXivpreprintarXiv:2309.16021,2023. [Jietal.,2023] Ziwei Ji, Nayeon Lee, Rita Frieske,
[Botacin,2023] Marcus Botacin. Gpthreats-3: Is automatic TiezhengYu,DanSu,YanXu,EtsukoIshii,YeJinBang,
malwaregenerationathreat? In2023IEEESecurityand Andrea Madotto, and Pascale Fung. Survey of halluci-
PrivacyWorkshops(SPW),pages238–254.IEEE,2023. nation in natural language generation. ACM Computing
Surveys,55(12):1–38,2023.
[Brohanetal.,2023] Anthony Brohan, Yevgen Chebotar,
ChelseaFinn,KarolHausman,AlexanderHerzog,Daniel [Kahneman,2011] Daniel Kahneman. Thinking, fast and
Ho,JulianIbarz,AlexIrpan,EricJang,RyanJulian,etal. slow. macmillan,2011.
Doasican,notasisay:Groundinglanguageinroboticaf- [Kwonetal.,2023] Minae Kwon, Sang Michael Xie, Kale-
fordances. InConferenceonRobotLearning,pages287– sha Bullard, and Dorsa Sadigh. Reward design with lan-
318.PMLR,2023. guagemodels. In TheEleventh InternationalConference
[Cartaetal.,2022] Thomas Carta, Pierre-Yves Oudeyer, onLearningRepresentations,ICLR2023,Kigali,Rwanda,
Olivier Sigaud, and Sylvain Lamprier. Eager: Asking May1-5,2023.OpenReview.net,2023.
and answering questions for automatic reward shaping in [Lietal.,2023] Guohao Li, Hasan Abed Al Kader Ham-
language-guidedrl. AdvancesinNeuralInformationPro- moud, Hani Itani, Dmitrii Khizbullin, and Bernard
cessingSystems,35:12478–12490,2022. Ghanem. Camel: Communicative agents for ”mind” ex-
[ChenandShu,2023] Canyu Chen and Kai Shu. Can llm- ploration of large language model society. In Thirty-
generated misinformation be detected? arXiv preprint seventh Conference on Neural Information Processing
arXiv:2309.13788,2023. Systems,2023.
[Dasguptaetal.,2023] Ishita Dasgupta, Christine Kaeser- [Linetal.,2023] BillYuchenLin,YichengFu,KarinaYang,
Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula,
Felix Hill, and Rob Fergus. Collaborating with lan- Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren.
guage models for embodied reasoning. arXiv preprint Swiftsage: Agenerativeagentwithfastandslowthinking
arXiv:2302.00763,2023. for complex interactive tasks. In Thirty-seventh Confer-
enceonNeuralInformationProcessingSystems,2023.
[Dengetal.,2023] Gelei Deng, Yi Liu, V´ıctor Mayoral-
Vilches,PengLiu,YuekangLi,YuanXu,TianweiZhang, [Maetal.,2023] Yecheng Jason Ma, William Liang,
YangLiu,MartinPinzger,andStefanRass.Pentestgpt:An Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh
llm-empowered automatic penetration testing tool. arXiv Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandku-
preprintarXiv:2308.06782,2023. mar. Eureka: Human-levelrewarddesignviacodinglarge
languagemodels. arXivpreprintarXiv:2310.12931,2023.
[Dongetal.,2023] Yihong Dong, Xue Jiang, Zhi Jin, and
Ge Li. Self-collaboration code generation via chatgpt. [McIntoshetal.,2023] Timothy McIntosh, Tong Liu, Teo
arXivpreprintarXiv:2304.07590,2023. Susnjak, Hooman Alavizadeh, Alex Ng, Raza Nowrozy,
and Paul Watters. Harnessing gpt-4 for generation of cy-
[Duetal.,2023] Yuqing Du, Olivia Watkins, Zihan Wang,
bersecurity grc policies: A focus on ransomware attack
Ce´dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek
mitigation. Computers&Security,134:103424,2023.
Gupta, and Jacob Andreas. Guiding pretraining in re-
inforcement learning with large language models. arXiv [Michelietal.,2023] Vincent Micheli, Eloi Alonso, and
preprintarXiv:2302.06692,2023. Franc¸oisFleuret. Transformersaresample-efficientworldmodels. In The Eleventh International Conference on intelligence in cybersecurity is a double-edged sword.
Learning Representations, ICLR 2023, Kigali, Rwanda, NatureMachineIntelligence,1(12):557–560,2019.
May1-5,2023.OpenReview.net,2023.
[TalebiradandNadiri,2023] YasharTalebiradandAmirhos-
[Mnihetal.,2013] Volodymyr Mnih, Koray Kavukcuoglu, sein Nadiri. Multi-agent collaboration: Harnessing
David Silver, Alex Graves, Ioannis Antonoglou, Daan the power of intelligent llm agents. arXiv preprint
Wierstra, and Martin Riedmiller. Playing atari with deep arXiv:2306.03314,2023.
reinforcement learning. arXiv preprint arXiv:1312.5602, [Team.,2021] Microsoft Defender Research Team. Cy-
2013.
berbattlesim.https://github.com/microsoft/cyberbattlesim,
[Mnihetal.,2016] Volodymyr Mnih, Adria Puigdomenech 2021. Created by Christian Seifert, Michael Betser,
Badia,MehdiMirza,AlexGraves,TimothyLillicrap,Tim William Blum, James Bono, Kate Farris, Emily Goren,
Harley, David Silver, and Koray Kavukcuoglu. Asyn- Justin Grana, Kristian Holsheimer, Brandon Marken,
chronousmethodsfordeepreinforcementlearning. InIn- JoshuaNeil,NicoleNichols,JugalParikh,HaoranWei.
ternationalconferenceonmachinelearning,pages1928–
[Vyasetal.,2023] Sanyam Vyas, John Hannay, Andrew
1937.PMLR,2016.
Bolton,andProfessorPeteBurnap. Automatedcyberde-
[Noever,2023] David Noever. Can large language mod- fence: Areview,2023.
els find and fix vulnerable software? arXiv preprint [Wanetal.,2022] YueWan,Chang-YuHsieh,BenLiao,and
arXiv:2308.10345,2023.
ShengyuZhang. Retroformer: Pushingthelimitsofend-
[Palmeretal.,2023] Gregory Palmer, Chris Parry, Daniel to-endretrosynthesistransformer.InInternationalConfer-
J.B.Harrold,andChrisWillis. Deepreinforcementlearn- ence on Machine Learning, pages 22475–22490. PMLR,
ingforautonomouscyberoperations: Asurvey,2023. 2022.
[Parketal.,2023] Joon Sung Park, Joseph O’Brien, Car- [Wangetal.,2023a] Lei Wang, Chen Ma, Xueyang Feng,
rie Jun Cai, Meredith Ringel Morris, Percy Liang, and Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,
MichaelSBernstein. Generativeagents: Interactivesim- JiakaiTang,XuChen,YankaiLin,etal. Asurveyonlarge
ulacra of human behavior. In Proceedings of the 36th languagemodelbasedautonomousagents. arXivpreprint
Annual ACM Symposium on User Interface Software and arXiv:2308.11432,2023.
Technology,pages1–22,2023.
[Wangetal.,2023b] Zhilin Wang, Yu Ying Chiu, and
[Pearceetal.,2023] Hammond Pearce, Benjamin Tan, Yu Cheung Chiu. Humanoid agents: Platform for sim-
Baleegh Ahmad, Ramesh Karri, and Brendan Dolan- ulating human-like generative agents. arXiv preprint
Gavitt. Examining zero-shot vulnerability repair with arXiv:2310.05418,2023.
large language models. In 2023 IEEE Symposium on [Wangetal.,2023c] Zihao Wang, Shaofei Cai, Anji Liu,
SecurityandPrivacy(SP),pages2339–2356.IEEE,2023.
Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin,
[Qietal.,2023] Jiaxing Qi, Shaohan Huang, Zhongzhi Zhaofeng He, Zilong Zheng, Yaodong Yang, et al.
Luan, Carol Fung, Hailong Yang, and Depei Qian. Log- Jarvis-1: Open-world multi-task agents with memory-
gpt: Exploring chatgpt for log-based anomaly detection. augmented multimodal language models. arXiv preprint
arXivpreprintarXiv:2309.01189,2023. arXiv:2311.05997,2023.
[Qianetal.,2023] Chen Qian, Xin Cong, Cheng Yang, [WasonandEvans,1974] PeterCWasonandJStBTEvans.
Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Dual processes in reasoning? Cognition, 3(2):141–154,
MaosongSun. Communicativeagentsforsoftwaredevel- 1974.
opment. arXivpreprintarXiv:2307.07924,2023. [Wuetal.,2023] QingyunWu,GaganBansal,JieyuZhang,
[Sandovaletal.,2023] Gustavo Sandoval, Hammond Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,
Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: En-
Brendan Dolan-Gavitt. Lost at c: A user study on ablingnext-genllmapplicationsviamulti-agentconversa-
the security implications of large language model code tionframework. arXivpreprintarXiv:2308.08155,2023.
assistants. arXivpreprintarXiv:2208.09727,2023. [Xiaetal.,2023] Chunqiu Steven Xia, Matteo Paltenghi,
[Schulmanetal.,2017] John Schulman, Filip Wolski, Pra- Jia Le Tian, Michael Pradel, and Lingming Zhang. Uni-
fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox- versal fuzzing via large language models. arXiv preprint
imal policy optimization algorithms. arXiv preprint arXiv:2308.04748,2023.
arXiv:1707.06347,2017.
[Xuetal.,2023] YuzhuangXu,ShuoWang,PengLi,Fuwen
[Standenetal.,2021] Maxwell Standen, Martin Lucas, Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.
David Bowman, Toby J Richer, Junae Kim, and Damian Exploring large language models for communication
Marriott. Cyborg: A gym for the development of au- games: An empirical study on werewolf. arXiv preprint
tonomouscyberagents. arXivpreprintarXiv:2108.09118, arXiv:2309.04658,2023.
2021. [Yaman,2023] FerhatYaman. AgentSCA:AdvancedPhysi-
[Taddeoetal.,2019] Mariarosaria Taddeo, Tom Mc- calSideChannelAnalysisAgentwithLLMs. PhDthesis,
Cutcheon, and Luciano Floridi. Trusting artificial NorthCarolinaStateUniversity,2023.[Yaoetal.,2023] YifanYao,JinhaoDuan,KaidiXu,Yuan-
fang Cai, Eric Sun, and Yue Zhang. A survey on large
languagemodel(llm)securityandprivacy: Thegood,the
bad,andtheugly.arXivpreprintarXiv:2312.02003,2023.
[Zhangetal.,2023] Jintian Zhang, Xin Xu, and Shumin
Deng. Exploring collaboration mechanisms for llm
agents: A social psychology view. arXiv preprint
arXiv:2310.02124,2023.