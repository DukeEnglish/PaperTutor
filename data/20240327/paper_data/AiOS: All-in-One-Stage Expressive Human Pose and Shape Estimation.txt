AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation
QingpingSun*,1,2,YanjunWang∗,1,AilingZeng3,WanqiYin1,ChenWei1,
WenjiaWang5,HaiyiMei1,Chi-SingLeung2,LeiYang1,5,ZiweiLiu4,ZhongangCai†,1,4,5
1 SenseTimeResearch 2 CityUniversityofHongKong
3 InternationalDigitalEconomyAcademy(IDEA)
4 S-Lab,NanyangTechnologicalUniversity 5 ShanghaiAILaboratory
∗EqualContributions,†CorrespondingAuthor
https://ttxskk.github.io/AiOS/
ℳ!"#$
ℳ)"*’ Merge ℳ
ℳ%&’(
(a) Top-Down, Multi-Stage (b) Top-Down, One-Stage (c) Our All-in-One-Stage Method
Figure1.AcomparisonofexistingmethodsinEHPS.(a)Top-down,multi-stagemethods,typicallyusedetectorstodetecthumans,thenuse
differentnetworkstoregressbodypartsoncroppedimages.(b)Top-down,one-stagemethods,useonlyonenetworkforregressionbutstill
requiredetectorsandrelyonthecroppedimage.(c)Ourall-in-one-stagepipeline,end-to-endhumandetection,andregressiononfullframe.
Abstract toprobethehumanjointintheimageandencoderafine-
grained local feature, which collaborates with the global
feature to regress the whole-body mesh. This straightfor-
Expressivehumanposeandshapeestimation(a.k.a. 3D
wardbuteffectivemodeloutperformspreviousstate-of-the-
whole-bodymeshrecovery)involvesthehumanbody,hand,
art methods by a 9% reduction in NMVE on AGORA, a
andexpressionestimation. Mostexistingmethodshavetack-
30%reductioninPVEonEHF,a10%reductioninPVEon
ledthistaskinatwo-stagemanner,firstdetectingthehuman
ARCTIC,anda3%reductioninPVEonEgoBody.
body part with an off-the-shelf detection model and infer-
ring the different human body parts individually. Despite
theimpressiveresultsachieved,thesemethodssufferfrom 1.Introduction
1)lossofvaluablecontextualinformationviacropping,2)
introducing distractions, and 3) lacking inter-association
Expressivehumanposeandshapeestimation(EHPS)1isa
amongdifferentpersonsandbodyparts,inevitablycausing rapidlydevelopingarea. Itplaysanimportantroleinhuman
performancedegradation,especiallyforcrowdedscenes. To understandingandhasbroadapplicationsintheanimation,
addresstheseissues,weintroduceanovelall-in-one-stage gaming,andstreamingindustries. Unlikehumanposeand
framework,AiOS,formultipleexpressivehumanposeand shapeestimation(HPS),whichfocusessolelyonthehuman
shaperecoverywithoutanadditionalhumandetectionstep. body, EHPS is designed to jointly estimate human body
Specifically,ourmethodisbuiltuponDETR,whichtreats poses,handgestures,andfacialexpressionsfromtheimage.
multi-personwhole-bodymeshrecoverytaskasaprogres- Inmainstreamstudies,thecommonapproachesinvolve
sivesetpredictionproblemwithvarioussequentialdetection. utilizingparametrichumanmodels,suchasSMPL-X[34],
Wedevisethedecodertokensandextendthemtoourtask. torepresentthearticulatedmeshmodelofahumanandto
Specifically,wefirstemployahumantokentoprobeahu- regress the parameters for each body part. Drawing from
manlocationintheimageandencodeglobalfeaturesfor researchexperienceinsingle-partestimation,suchasbody
eachinstance,whichprovidesacoarselocationforthelater
1EHPSisusedinterchangeablywith3Dwhole-bodyhumanmeshre-
transformerblock. Then,weintroduceajoint-relatedtoken coveryinthiswork
4202
raM
62
]VC.sc[
1v43971.3042:viXra
Encoder Decoderposeandshapeestimation[10,15–17,21,22,39,44,45,56], tioncues,thesetokensaggregatebothglobalandlocalfea-
existingmethods[3,8,13,20,29,31,36]employamulti- turerepresentationswithcross-attentionforenhancedmodel
stageparadigm. AsshowninFig. 1a),theprocessbeginsby accuracyindiversescenarios. Third, usingself-attention
croppingthebodypartsusingboundingboxesdetectedeither andcross-attentionmechanismsinourmodelallowsforan
by off-the-shelf detection models or provided via ground in-depthanalysisofinter-humanandintra-humanrelation-
truthannotations.Followingthis,distinctmodelsareutilized ships, enhancing performance in crowded and occlusion-
fortheseparatereconstructionofeachindividualbodypart. heavyenvironments.
Obviously,thisdesigncompromisesbothcomplexityand Extensive experiments show that our proposed model
accuracy.Theimagesareprocessedmultipletimeswitheach has overpass state-of-the-art (SOTA) methods that utilize
model. Theseparatepartsmodelblockstheinter-part,inter- groundtruthboundingboxesandalsoSOTAmethodswhen
humanconnectionandbringsinconsistentposesandunnatu- theboundingboxisnotgiven. Further,ourboundingboxis
ralartifactsattheconnectedjoints. Recently,OSX[23]and accurateenoughtoimprovetheothertwo-stagemethodson
SMPLer-X [3] discard part experts and regress the model theAGORAbenchmark.
in a holistic manner, which alleviates the artifacts. Their Insummary,ourcontributionsarei)Thefirstone-stage
paradigmcanbeabstracttoFig.1b),however,theystillneed methodforEHPSthateliminatestheneedforextradetec-
tobegivenaboundingboxtocroptheimage. Whiletheir tionnetworks;ii)Aunifiedframeworktointegratelocaland
benchmarks show promising results, the accurate ground global features for whole-body regression; iii) SOTA per-
truth bounding boxes are not attainable in real-world sce- formanceonmainstreambenchmarkswithoutgroundtruth
narios. RoboSMPLX [31] has demonstrated that the per- boundingboxes.
formancedropssignificantlyundernoisyboxes. Moreover,
CLIFF[22]pointsoutthatthecroppingoperationdiscards 2.RelatedWork
thelocationinformation,whichdegradestheperformance.
2.1.ExpressiveHumanMeshRecoveryMethods
Adirectsolutiontoaddressthechallengesposedbythe
multi-stage paradigm is to utilize a one-stage framework EHPS focuses on reconstructing the mesh of the human
thatdirectlyrecoversEHPSfromtheentireimagewithout body,hands,andfacefrommonocularimages. Pioneering
requiringadditionalboxesforcropping. However,current researchinthisdomainintroducedwhole-bodyparametric
one-stagemethods[40–42]areproposedforHPS.Bothof modelssuchasSMPL-X[34]. Withadvancementsinregres-
themuseabodycenterheatmapandmeshparametermap siontechniquesforthehumanbody,hands,andface,early
torepresentthepotentialhumanlocationandcorresponding studiesadoptedmulti-stagesolutions[13,29,33,36]. They
features. Relyingsolelyonthesehuman-centeredglobalfea- independentlyrecoverbodypose,handpose,andfacialex-
turesisinsufficientforachievingaccuratepart-wiseregres- pressionsfromcroppedimagesbeforeintegration. However,
sion. Althoughnumeroustwo-stageHPSmethods[16,52] these multi-stage methods often produce artifacts at joint
thatextractlocalfeaturesinvariousways,itisnon-trivialto intersectionsandpresentcomplexnetworkdesigns.
extendtoaone-stagemodel,asmostoftherepresentations, Given the recent surge in whole-body datasets [1, 2, 4,
likepart-attentionmaps,aredesignedforasingleperson. 32, 48], many approaches have transitioned to a holistic
Inordertotackletheabovechallenges,wehaveproposed paradigm. OSX[23]presentsagroundbreakingone-stage
thefirstAll-in-One-Stage(AiOS)EHPSmethod. Thisnovel method, eliminatingpart-specificexpertsandcroppedim-
approachiscapableofpredictingeveryindividualpresent ageregression. SMPLer-X[3]furtheramplifiesone-stage
in an image solely based on a single image input without methodsutilizinglargevisionmodelsandextensivedatasets.
anyadditionalrequirements. Inspiredbytheachievementof However,theystillrelyonboundingboxesforimagecrop-
DETR-based[5]methodsinvariousvisiontasks[46,47,51, ping. Despite their precision with ground truth bounding
53,59],wedesignedourpipelineinaDETR[5]stylewith boxes,performancedegradesunderdetectedboxes[31].
imagefeatureencoderandvariouslocation-awaredecoders.
2.2.One-StageHumanMeshRecoveryMethods
We tailored different queries and association strategies to
progressivelyguidethedecodertoperceiveglobalandlocal MostoftheexistingHPSmethods[7,10,15,18,24,44,49,
humanfeaturesfromtheentireimage. 50,52]aremulti-staged. Althoughthesemethodspreserve
Three key design features distinguish the AiOS model. relativelyhigh-resolutionimagesandgenerallyhavehigher
First, it is built upon DETR structure, with a CNN back- accuracy,theyneglectotherinformationinthefullframe,in-
bone,transformerencoders,anddecoders,andprogressively cludinginter-personocclusionsandindividualpositions[22].
detectshumananddecodespersonfeaturesinanend-to-end Toaddresstheselimitations,ROMP[40]firstproposedto
manner. Second,weintroducedthe"Human-as-Tokens"de- recoverhumansfromanentireframe. Itlocatesthehuman
sign,wherehumansareconceptualizedasacollectionofbox locationsfromabodycenterheatmapandindexesthecorre-
tokensandjointtokens. Withdifferentsupervisionandloca- spondingfeaturesfromthefeaturemaptoregressallhumanmeshes. Furthermore,BEV[42]extendsthe2Dheatmapto location and extract global features of the body; 2) refine
3Dbyincorporatingabird-eye-view. Itenablesthemodelto bodylocation, extractbodylocalfeatures, localizecoarse
discern3Drelativepositionswithintheframe. TRACE[41] handsandfacelocationsandextractglobalfeaturesofthe
furtherachievedsimultaneouslytrackinghumansandpre- handsandface;3)refinehandandfacelocation,extractlocal
dictingcameramotionswithaddedmotionmaps. However, featuresforwholebody.
thesecenter-map-basedmethodsoftendistillthehumaninto Backbone. AiOS utilizes the ResNet-50 [14] to extract a
asinglevectoronthefeaturemapandrecoverthehuman multi-scalefeaturemapsF ,whichprovidefeaturesfrom
img
pose and shape based on this global feature. We reckon detailedtoholistic.
thatthisrepresentationisinsufficientfortheEHPStask,par- Encoder.Asourtaskneedsmorethanlocalassociations,we
ticularlygiventhathandposeandexpressionrequiremore utilizeastandardtransformerencoder[43]forlong-distance
fine-grindlocalfeaturesforaccurateregression. relations. TotransformtheCNN-basedfeaturemapintoa
transformer-compatiblefeaturevector,weflattenthemulti-
3.Method layerfeaturemapsalongtheirspatialdimensionsandcon-
catenatethem. Theflattenedfeatureisaddedwithposition
3.1.Motivation
encodingsPE ∈RM×D toderivetheimagefeaturetoken
ForEHPS,usingcroppedimagespresentssignificantprob- T img ∈RM×D,whereM representsthetotallengthofthe
lems. Thecroppingdiscardsthelocationinformation[22], image feature token. We fed T img a transformer encoder,
andinaccurateboundingboxesmayleadtomissingbody whichproducestherefinedimagefeaturetokensT′ ,serv-
img
parts,negativelyimpactingperformance. Incrowdedscenes, ingasareferenceforcross-attentioninthedecoder.Utilizing
croppingstrugglestodistinguishindividualhumans, with T′ ,afeed-forwardnetwork(FFN)isappliedtoclassify
img
partsfromothersintrudingintotheframe,leadingtoerrors each token as a human token. Following the approach in
in human part detection and regression. Especially when DINO[54]andED-Pose[47],wefilterbasedontheclassi-
peopleoverlapsignificantly, themodelstrugglestodiffer- ficationscoreandretainthetopM = 900tokens. These
h
entiatethemduetounclearboundingboxes. Furthermore, tokensserveascandidatehumanbodylocalizationtokens
thedetectorsusedaretypicallytrainedongeneralobjectde- T
body
∈RMh×D,andtheyalsofunctionastheinputforthe
tectiondatasetsandarenotspecificallydesignedforhuman subsequentdecoders.
detection,addingtothesedifficulties. GenericDecoder. SimilartoPETR[38]andED-Pose[47],
Totackletheseproblems,weintroducedtheAiOS,the which extend the deformable decoder [58] to 2D human
first fully end-to-end network for EHPS. Abandoning the body-only pose estimation, AiOS extends the deformable
uncertain assumption of box-as-subject, our model lever- decoder to 3D whole-body mesh recovery. It mainly has
ages feature tokens and position queries for more precise threeinputs,imagecontenttokensT′ ∈ RM×D,object
img
humanlocalization. We’vedevelopedacohesiveapproach contenttokensT ∈RMh×DandobjectpositionqueriesQ∈
thatcombinesglobalandlocalfeaturerepresentationsforac- RMh×4. Utilizingthisdecoder,ourmodelcanautomatically
curateregression. Tohandlecrowdedscenariosandenhance probethesuitableglobalandlocalfeaturesaroundthebody
theseparationofhumanfigures,ourmodelemploysatten- partsforeachhumanconditionedbyvariousqueries.Wewill
tionmechanismstoestablishintricaterelationshipsbetween introduceourkeydecoderdesignsinthefollowingsections.
differentbodypartsandbetweenmultipleindividuals.
3.4.NaiveAiOS
3.2.Preliminaries
DrawinginspirationfromROMP[40],weextendtheDETR
SMPL-X.Weuse3DparametricmodelSMPL-X [34]to structure[5]toEHPSandprogressivelyregressSMPL-Xpa-
studyEHPS.Itutilizesasetofparameterstomodelbody, rameters.Specifically,wefollowDAB-DETR[27]andintro-
face, and hands geometries. Specifically, our model esti- ducethelocationqueriestoprobethebody,face,andhands-
mates pose parameters θ ∈ R53×3, which include body relatedfeatures,guidedbyboundingboxes(x,y,w,h),that
posesθ body ∈R22×3,lefthandposesθ lhand ∈R15×3,right considersboththelocationandsizeofeachbodypartboxes.
handposesθ rhand ∈ R15×3,andjawposesθ jaw ∈ R1×3. Themodelfirstextractsfeaturesrelatedtothebodyusing
Additionally, it estimates shape parameters β ∈ R10, and bodyboxlocationqueriesandrefinesthemthroughthebody-
facial expression parameters ψ ∈ R10. We use the joint locationdecoder.Subsequently,theyareexpandedtoinclude
regressorJ toobtainthe3Djointfromtheparametersby handsandfacequeriesandleveragethewhole-body-location
J(M(β,θ,ψ)),whereMistheSMPL-Xfunction. decodertoextractwhole-bodyfeatures.
Body-locationDecoder. Thefirsttwodecodersarebody-
3.3.Overview
centric,andtheinputobjectcontenttokensT arethebody
AiOS includes the backbone and transformer encoder- location tokens T . We derive body location query Q
bl bl
decoderstructures. Ithasthreesteps,1)localizecoarsebody with FFN from the corresponding T . The decoder first
bl𝑇%& 𝑇)& 𝑇*& 𝑇+
Body Localization Body Refinement Whole-body Refinement
𝑃𝐸
𝑇!"# …
𝑇%& … 𝑄%& 𝑇%( … 𝑄%& 𝑇’( … 𝑄’(
𝐹!"# Encoder Filter 𝑇 !$ "# Bod Dy e L coo dc ea rtion Fi &lte r 𝑇 !$ "# Bod D R ee cf oin de em rent Expand 𝑇 !$ "# Whole-b Do ed cy o R dee rfinement
Expand
𝑇 !$ "# …
𝑇%& … 𝑄%& 𝑇%( … 𝑄%& 𝑇’( … 𝑄’(
CNN Body Body Body Body Body Body, Face, Body, Face, Body, Face, Body, Face,
Class Class Box Param Joint Hands Box Hands Param Hands Joint Hands Box
Figure2.Pipelineoverview.AiOSperformshumanlocalizationandSMPL-Xestimationinaprogressivemanner.Itiscomposedof(1)the
bodylocalizationstagethatpredictscoarsehumanlocation;(2)theBodyrefinementstagethatrefinesbodyfeaturesandproducesfaceand
handlocations;(3)theWhole-bodyRefinementstagethatrefineswhole-bodyfeaturesandregressSMPL-Xparameters.
associatesthebodylocationtokensandupdatesthembythe cross-attentionmodule. Weutilizeanattentionmasktoen-
self-attention mechanism. Then, the decoder takes image surethattheboundingboxesforeach person’shandsand
tokens T′ as the value and the updated body location face are associated only with their own and others’ body
img
tokens as the query for cross-attention, and the Q acts boundingboxes. Asourmodelisalreadycapableofrecog-
bl
asanindicator,whichisusedtoaggregatetheinformation nizingeachperson’sbodyinthefirsttwostages,thisspecific
focusing on the corresponding body area. After that, the attentionmechanismallowsformoreaccurateidentification
bodylocationtokensT andbodylocationqueriesQ are ofbodypartsincrowdedscenes. Weprovideanillustration
bl bl
refinedwiththedecoder. oftheattentionmechanismintheSupplementaryMaterial.
WeestimatethebodyboundingboxwithanFFNfrom We regress body bounding boxes from T bl, face boxes
T bl,whichissupervisedbyL box. Thissupervisionmakes from T fl and hand boxes from T rhl, T lhl, and supervise
surethetokensaggregateglobalinformationofthehuman. themwithL box. Weregressdifferentpart’sparametersfrom
Similar to the encoder, we classify the output T bl with an the refined whole body T full tokens. The parameters are
FFNonwhetheritisatokenrepresentingahuman. Theclas- supervised with SMPL-X lossL smplx, which includes pa-
sificationresultsfromT
bl
aresupervisedwithclassification rameterlossL param, 3dkeypointslossL kp3d, andthe2d
lossL cls. Attheendoftheseconddecoder,wedownsample keypointsreprojectionlossL kp2d.
thebodytokensagaintoM =100tofurtherdistillpotential
b
3.5.AiOS
humantokensandlowerthecomputationalcomplexity.
Whole-body-locationDecoder. Thelatterfourdecodersof Previousmethods[40,42]haveshownthatregressingmulti-
naive AiOS jointly consider whole-body information and personbodymeshesfromglobalfeaturesalonecanachieve
their association. With the body location tokens from the impressiveresults,butinEHPS,relyingonglobalinforma-
previousstep,weexpandthemtohandsandfacelocation tionaloneisinsufficient. Themodelshouldalsoconsider
tokens with learnable embedding. We first broadcast the localinformationtoobtainadetailedcontextofthewhole-
givenembeddingE bl ∈RD andaddittothebodylocation bodyregression. Therefore,toelevatethemodel’sability,
token T bl ∈ RMb×D. After that, we obtained hand loca- we introduce joint-related tokens and their corresponding
tiontokensT lhl,T rhl,andfacelocationtokensT fl,which queriestoourmodel. Combinedwithlocationtokens,the
have the same shape as T bl. Then we concat them into a AiOSexpresseshumancontextinmultilevel.Wewillfurther
whole-bodytokenT full = [T bl,T lhl,T rhl,T fl]. Similarly, regresstheSMPL-Xparameteronthiswell-roundedfeature
thewhole-bodylocationqueriesQ full areexpandedfrom group. Specifically, we adopt a progressive detection and
Q bl withlearnableembeddings. decoding strategy. The first two layers are body-location
Thedecodersuseaself-attentionmoduletoexploreinter- decoders same as our naive design, which outputs coarse
partandinter-humanrelationsandthenextracteachpart’s humanlocation. Further,twolayersofbody-refinementde-
features around their bounding boxes with a conditioned codersutilizebodyjointtokenstoenrichlocalbodyfeaturesandestimateroughhandandfacelocationsimultaneously ARCTIC[11],Egobody[57],andBEDLAM[1].
onthebasisofhumanlocation. Atlast,twolayersofwhole- Implementation. The training is conducted on 16 V100
body-refinementdecodersextractwhole-bodylocalfeatures GPUs,withatotalbatchsizeof32. Wefirsttrainourmodel
withextrahandsandfacejointtokens. for60epochsonAGORA,BEDLAM,andCOCO.Wefine-
Body-refinementDecoder. Thisdecoderisbuiltonbody- tuneitfor50epochsonalltraindatasets.
locationdecodersinnaiveAiOS.Indetail,weexpandbody Evaluationmetrics. FollowingthepreviousEHPSmeth-
joints tokens, hands location tokens, and face location to- ods[3,23,29],wereportProcrustesAlignedper-vertexpo-
kens. We adopt the learnable-embedding E ∈ R17×D sitionerror(PA-MPVPE)andthemeanper-vertexposition
bj
to expand body joint tokens T
bj
∈ RMb×17×D from box error(MPVPE)acrossallbenchmarks. InAGORALeader-
location tokens, and then we obtain detailed body token board,wereportmeanvertexerror(MVE),meanper-joint
setT = [T ,T ,T ,T ,T ]. Notethatweuseanat- positionerror(MPJPE)forpurereconstructionaccuracy;F
bd bl bj lhl rhl fl
tention mask to limit the joint attention within its subject Score,precision,recallfordetectionaccuracy;Normalized
as inter-joint attention among different subjects brings no meanvertexerror(NMVE)andnormalizedmeanjointerror
incrementalbutmuchhighercomputationcomplexity. (NMJE)thatconsideredregressionaccuracywithdetection
TheT arerefinedwithlayersofdecoders. Withineach accuracy. Allmetricsarereportedinmillimeters(mm).
bd
layer,similartonaiveAiOS,weregressboundingboxesof
4.2.QuantitativecomparisonwithSOTA
body parts from their location tokens and supervise them
withL . Further,weregressbodyjointlocationfromT InTable1,wecompareAiOSwiththeSOTAmethodson
box bj
and supervise them with L , helping these joint tokens theAGORAtestset. Theresultsareprovidedbytheleader-
j2d
learnthelocalhumanfeatures. DifferentfromNaiveAiOS, board2withtheirboundingboxesontheupperpartoftheta-
inthisstage,weregressSMPL-Xbodyparametersbasedon ble.WealsofeedourestimatedboundingboxestoOSX[23]
T , T . WeuseL tosupervisethebodyparameter, andSMPLer-X[3]onthelowerpart,whichhelpstoverify
bl bj smplx
helpingtorefinethebody-relatedtokensrepresentingmore ourmodel’slocalizationquality.
accuratebodyfeatures. ForafaircomparisonwiththeSOTAmethods,weutilize
Whole-body-refinement Decoder. This decoder further athresholdof0.5tofilterthedetectedsampleswithlower
expandsthefaceandhandjointtokens. Similarly,weuse confidence, which generally have severe occlusions. As
embeddingE ,E ,andE toexpandT ,T ,and shownontheupperpartofTable1,ourmodel’sNMVEand
lhj rhj fj lhl rhl
T to T , T , and T , respectively. At this stage, NMJEgreatlysurpassthecurrentSOTAmethodSMPLer-X.
fl lhj rhj fj
the model forms the complete tokens that represent a hu- Thisobservationprovesthatourone-stagepipelineachieves
man T = [T ,T ,T ,T ,T ,T ,T ,T ]. We thebestoverallquality,combininglocalizationandrecon-
wd bl bj lhl lhj rhl rhj fl fj
regresswhole-bodyjointlocationfromT ,T ,T ,T struction. Intermsofpurereconstructionquality,ourmodel
bj lhj rhj fj
andsupervisethemwithL . alsoachievesSOTAperformancewitharelativelyaccurate
j2d
Based on T , we utilize FFN to regress box location detectionresultonMVEandMPJPE.WhileBEDLAM[1]
wd
fromT ,T ,T ,T andsupervisedwithL . Wealso excelsinfaceandhandreconstruction,itsrecallperformance
bl lhl rhl fl box
regresswhole-bodyjointlocationfromT ,T ,T ,and iscomparativelylow,omittingsomeinstancesforevaluation.
bj lhj rhj
T , and supervise them with L . Finally, we estimate On the lower-part comparison, we lower the detection
fj j2d
SMPL-Xbody,hands,andfaceparametersfrombody,hand, threshold to 0.3, which has higher recall than any current
andface-relatedtokens,respectively,andsupervisewhole- results,allowingmorehardcasestobedetected. Wefeedthe
bodyparameterswithL . sameboundingboxestotheOSXandSMPLer-X,andtheir
smplx
Overall Loss Functions. The overall loss function is the performanceonwhole-bodyMVEimprovescomparedwith
sumofallthelossesateachstage. PleaserefertotheSup- theresultsreportedintheoriginalpaper(122.8to121.3for
plementaryMaterialforthedetails. OSX,99.7to98.3forSMPLer-X)evenwithahigherrecall.
Thisindicatesthatimprovementisachievednotbyfiltering
4.Experiment outhardcasesbutbyprovidinghigh-qualityboundingboxes.
This finding proves that the current two-stage method is
4.1.ExperimentalSetup sensitive to bounding box quality, and using the ground
truthboxtocropimagesinotherbenchmarksisbiasedfrom
Duetothepagelimit,weputthedetailedexperimentsetup,
real use cases. Notably, under this bounding box setting,
implementation,andpartialquantitativeandqualitativecom-
AiOSisstillmuchhigherthanthecurrentSOTAOSXand
parisonwithSOTAmethodsintheSupplementaryMaterial.
comparablewiththefoundationmodelSMPLer-XL20.
Datasets. AiOS is trained on the multi-person datasets
Asthefirstone-stagemethodinEHPS,wecannotfind
AGORA[32],BEDLAM[1],andCOCO[26],andsingle-
relevantone-stagemethodsforafaircomparison. Therefore,
person datasets UBody [23], ARCTIC [11], and EgoB-
ody [57]. We evaluate it on AGORA, UBody, EHF [34], 2https://agora-evaluation.is.tuebingen.mpg.de/NMVE↓(mm) NMJE↓(mm) MVE↓(mm) MPJPE↓(mm)
Methods FScore↑ Precision↑ Recall↑
All Body All Body All Body Face LHand RHand All Body Face LHand RHhand
BEDLAM[1] 0.73 0.98 0.59 179.5 132.2 177.5 131.4 131.0 96.5 25.8 38.8 39.0 129.6 95.9 27.8 36.6 36.7
H4W[29]† 0.94 0.96 0.92 144.1 96.0 141.1 92.7 135.5 90.2 41.6 46.3 48.1 132.6 87.1 46.1 44.3 46.2
BEDLAM[1]† 0.73 0.98 0.59 142.2 102.1 141.0 101.8 103.8 74.5 23.1 31.7 33.2 102.9 74.3 24.7 29.9 31.3
PyMaF-X[55]† 0.89 0.90 0.89 141.2 94.4 140.0 93.5 125.7 84.0 35.0 44.6 45.6 124.6 83.2 37.9 42.5 43.7
OSX[23]∗ 0.94 0.96 0.93 130.6 85.3 127.6 83.3 122.8 80.2 36.2 45.4 46.1 119.9 78.3 37.9 43.0 43.9
HybrIK-X[20] 0.93 0.95 0.92 120.5 73.7 115.7 72.3 112.1 68.5 37.0 46.7 47.0 107.6 67.2 38.5 41.2 41.4
SMPLer-X[3] 0.93 0.96 0.90 133.1 88.1 128.9 84.6 123.8 81.9 37.4 43.6 44.8 119.9 78.7 39.5 41.4 44.8
SMPLer-X[3]† 0.93 0.96 0.90 107.2 68.3 104.1 66.3 99.7 63.5 29.9 39.1 39.5 96.8 61.7 31.4 36.7 37.2
NativeAiOS 0.93 0.98 0.89 105.7 66.5 103.9 65.8 98.3 61.8 27.2 40.7 41.7 96.6 61.2 28.4 38.4 39.4
AiOS 0.94 0.98 0.90 97.8 61.3 96.0 60.7 91.9 57.6 24.6 38.7 39.6 90.2 57.1 25.7 36.4 37.3
OSX[23]∗⋄ 0.96 0.97 0.95 126.4 81.8 123.4 80.0 121.3 78.5 36.1 45.9 46.3 118.5 76.8 37.6 43.5 44.0
SMPLer-X[3]†⋄ 0.96 0.97 0.95 102.4 63.8 99.5 62.1 98.3 61.2 30.3 40.4 40.7 95.5 59.6 31.7 37.9 38.2
AiOS 0.96 0.97 0.95 103.0 63.5 100.8 62.6 98.9 61.0 27.7 42.5 43.4 96.8 60.1 29.2 40.1 40.9
Table1.AGORASMPL-Xtestset.†denotesthemethodsfinetunedontheAGORAtrainingset.∗denotesthemethodstrainedonthe
AGORAtrainingsetonly.⋄denotesthemethodsthatusetheAiOS’sboundingboxtocroptheimage.Thebestresultsarecoloredwithred,
andthesecond-bestresultsarecoloredwithbluefortheupperandlowerpartsofthetable,respectively.
Methods F1-score↑ Precision↑ Recall↑ NMVE↓ NMJE↓ MVE↓ MPJPE↓ PA-PVE↓(mm) PVE↓(mm)
Top-downMethods
Method All Hands Face All Hands Face
HMR[15] 0.80 0.93 0.70 217.0 226.0 173.6 180.5
PyMAF[56] 0.84 0.86 0.82 200.2 207.4 168.2 174.2
PIXIE[13] 61.7 12.2 4.2 168.4 55.6 45.2
PARE[16] 0.84 0.96 0.75 167.7 174.0 140.9 146.2
H4W[29]† 0.94 0.96 0.93 90.2 95.5 84.8 89.8 H4W[29] 44.8 8.9 2.8 104.1 45.7 27.0
CLIFF[22]† 0.91 0.96 0.87 83.5 89.0 76.0 81.0
OSX[23] 42.4 10.8 2.4 92.4 47.7 24.9
HybrIK[21]† 0.91 0.92 0.90 81.2 84.6 73.9 77.0
ProPose[12]† 0.90 0.91 0.89 78.8 82.7 70.9 74.4 OSX[23]† 42.2 8.6 2.0 81.9 41.5 21.2
PLIKS[37]† 0.94 0.95 0.93 71.6 76.1 67.3 71.5 SMPLer-X[3] 33.2 10.6 2.8 61.5 43.3 23.1
NIKI[19]† 0.91 0.92 0.90 70.2 74.0 63.9 67.3
SMPLer-X[3]† 31.9 10.3 2.8 57.4 40.2 21.6
One-stageMethods
ROMP[40]† 0.91 0.95 0.88 113.6 118.8 103.4 108.1 NativeAiOS 35.6 8.6 2.9 62.7 41.3 20.8
BEV[42]† 0.93 0.96 0.90 108.3 113.2 100.7 105.3 AiOS 32.5 7.3 2.8 58.6 39.0 19.6
AiOS0.5 0.94 0.98 0.90 61.2 68.0 57.5 63.9
AiOS0.3 0.96 0.97 0.95 63.4 70.1 60.9 67.3
Table3.UBody.†indicatesthemodelisfinetunedwiththeUBody
Table2. AGORASMPLtestset. †indicatesthatthismethodis trainingset.
fine-tunedontheAGORAtrainingset.AiOS andAiOS ,rep-
0.5 0.3
resentingtheuseofa0.5scorethresholdanda0.3scorethreshold
PA-PVE↓(mm) PVE↓(mm)
tofilterthedata,respectively.
Method All Hands Face All Hands Face
similar to H4W [29], we compare the results of our body
H4W[29] 50.3 10.8 5.8 76.8 39.8 26.1
partwithexistingbody-onlymethods,whichcanbebroadly
OSX[23] 48.7 15.9 6.0 70.8 53.7 26.4
categorizedintotop-downmethods[12,15,16,19,21,22,
SMPLer-X[3] 37.8 15.0 5.1 65.4 49.4 17.4
37, 56] and one-stage methods [40, 42], on the AGORA
NativeAiOS 38.8 13.8 4.0 50.2 49.8 17.3
SMPL test set. Specifically, we downsample the SMPL-
AiOS 34.0 12.8 3.8 45.4 44.1 16.9
XmeshestimatedbyAiOStotheSMPL[28]meshusing
officialtools[34]andthenmeasureMVEandNMVE.We Table4.EHF.AsEHFisabsentfromourtrainingdata,itservesas
usetheJ-regressortoregressjointsfromthedownsampled avaluabletooltoassessthegeneralizationabilityofourmodels.
SMPLmeshtomeasureNMJEandMPJPE.
As shown in Table 2, even though AiOS is designed
the image, we apply an affine transformation for the two-
for EHPS, it still outperforms ROMP [40] and BEV [42],
stagemethodsthatuseimagescroppedbygroundtruthboxes.
withanotableimprovementinNMVEof43%(from108.3
Incontrast,ourmethodcanbedirectlyoverlaidontheim-
mmto61.2mm)andanNMJEenhancementof40%(from
age. Further, with accurate betas estimation, we are able
113.2 mm to 68.0 mm). It is worth noting that we do not
to recover the depth order, as shown in the Fig. 3. We
deliberatelyfine-tuneourmodelexclusivelyonAGORA.
achievecomparablevisualqualityinbothscenes,proving
Single datasets. We compare UBody in Table 3, EHF in
ourmodel’saccuracy.
Table 4. Note that the other methods utilize ground-truth
bounding boxes. Without any given bounding boxes, our WefurtherperformaqualitativecomparisonwithSOTA
modelachievesSOTAperformanceonreal-lifedatasets. one-stage methods [40, 42]. As shown in Fig. 4, while
ROMPandBEVcanachievedecentresultsforbodyrecon-
4.3.QualitativecomparisonwithSOTA
structioninmulti-personscenarios,theyarelimitedbythe
We perform a qualitative comparison with current SOTA constraintsoftheSMPL[28]model,preventingthemfrom
methodsonAGORAandEHF.Tooverlaytheresultsonto reconstructingdetailedhandgesturesandfacialexpressions.Input OSX SMPLer-X Ours Ours
Input Hand4Whole OSX SMPLer-X Ours
Figure3.ComparisonofcurrentSOTAmethods [3,23,29]withourAiOSmodel.TheupperpartisvisualizationresultsonAGORA[32],
andthelowerisEHFtest[8].
Input ROMP BEV Ours
Figure4.VisualcomparisonswithSOTAone-stageHPSmethods[40,42]ontheInternetdata3.
4.4.AblationStudy tures,expressiondetails,andlarge-scaleposedetails. Toval-
idatetheeffectivenessofourjoint-guidedlocalfeaturequery,
Inthissubsection,weanalyzetheeffectivenessofthepro-
wecomparednaiveAiOSandfullAiOSmodelsacrossthe
posedcomponentsindetail. Allexperimentsareconducted
benchmarks. The table shows that even the naive setting
ontheAGORAvalidationset.
achievescomparableperformancewithSOTAmethods,in-
AnalysisofthenaiveAiOSandfullAiOS.Whole-body
dicating our one-stage pipeline, which treats EHPS as a
mesh recovery requires attention to both small-scale ges-
progressivesetpredictionproblemwithvarioussequential
detectionsfollowingtheDETR,isidealforSMPL-Xparam-
3https://www.pexels.com/PA-PVE↓(mm) PVE↓(mm)
AblationStudies
All Hands Face All Hands Face
AttentionFormat
Full 42.5 7.2 4.2 54.8 39.0 25.8
Inter-humanOnly 41.7 7.3 4.2 52.8 38.9 24.5
Body Box Center Right Knee Joint Left Hand Box Left Thumb Joint Ours 39.9 7.2 4.1 50.5 37.4 23.3
SMPL-XSupervisionManners
Allstages 42.7 7.4 4.2 55.7 39.8 25.1
3rdstageonly 40.3 7.2 4.2 51.8 38.0 23.8
Ours(2,3stage) 39.9 7.2 4.1 50.5 37.4 23.3
Body Box Right Hand Box Left Hand Box Face Box Table5.AblationStudies.Theupperpartstudiestheattentionfor-
Figure5. AttentionVisualization. Thegreendotsrepresentthe mat,andthebottompartstudiestheSMPL-Xsupervisionmanners.
locationofthereferencepoint,andthereddotsarethesampling
points. attentionmechanismisnotproperlylearned.Andthelimited
settingisalsonotidealcomparedtoouroriginalattention
mechanism. Furthermore,wevisualizethecross-attention
eterregression.Onthissolidbase,thefullAiOSconsistently
of our model. As shown in Fig. 5, our model is able to
achieveshigheraccuracyonallpartsofthehuman,andthe
localizeglobalfeatureswithbodylocationtokensandlocal
incrementonthewhole-bodyaspectisespeciallyoutstand-
featureswithjointtokens.Thelowerpartshowstheattention
ing. Sincethebodytendstohavearelativelyhigherareaon
mapunderocclusion,anditshowsthatourmodelwilltake
theimage,addingjointqueriestothebodyprovidesalarge
referencefromotherbodyparts.
numberoflocalfeaturesforreference,whileforsmallerar-
easlikefaceandgestures,thedifferencebetweenglobaland
5.Conclusion
localfeaturesisnotthatobvious. However,addingthelocal
jointsfeatureoverallbringsmorecomprehensivefeatures. Inthiswork,weproposethefirstall-in-one-stagemodelfor
The Scheme of the SMPL-X supervision. In this part, expressivehumanposeandshapeestimation. Weexplored
we investigate how to supervise different tokens. For our theincorporationofbody-,face-,andhand-relatedtokens,
originalAiOS,wedon’tsupervisetheSMPL-Xparameter aswellastheaggregationoflocalandglobalfeatureswith
in the first stage, as we want the model to focus on body varioussupervision. Moreover,wecarefullydesignedaself-
localization. Inthesecondstage,wedon’tsupervisehands attentionmechanismtoestablishtheassociationsbetween
andfaceforthesamereason,butsuperviseSMPL-Xbody inter-andintra-humanbodyandbodyparts,whichhelpsus
parametersaswehavedetailedbodyfeaturetokens. And toachieveitsbestperformance. TheSOTAresultsindicate
wesupervisethewholebodyparameteratthethirdstage. In ourone-stagepipeline,whichtreatsEHPSasaprogressive
thefirstablationsetting,weaddbodyparametersupervision set prediction problem with various sequential detections
ineverystageandhandandfacesupervisioninthesecond followingtheDETR,isacrucialfactorcontributingtothe
stage,meaningeverystagehasSMPL-Xsupervision. Inthe overallperformance. Thiscanbefurtherprovedbytheper-
secondsetting,weremovetheSMPL-Xbodysupervisionin formanceofournaiveAiOSbaseline.Wehopethisworkcan
thesecondstagesothatthemodelwillbeonlysupervisedby contributenewinsightstotheEHPSresearchcommunity.
SMPL-Xinthelaststage.AsshowninTable5,acomparison Limitations. First,ourmodelachievesSOTA,butthereis
betweenAiOSandallstagesettingsshowsaddingSMPL-X stillalargeroomforimprovementifweaddmoredatasets
parameters when the location is not properly refined will fortraining,particularlythosecontainingmulti-personreal
hinderthemodel’sperformance. ComparingtheAiOSand data. Second,theversatiledesigncanbefurtherextended
3rdstagesettingshowsthedesignofgraduallywhole-body with more dimensions of human perception tasks such as
estimationfrombodytowhole-bodyincreasedperformance. trackingand3Dlocalization. Exploringtheestimationof
Theassociationbetweenthehumanbody,hands,and handsunderlimitedresolutionisalsoworthinvestigating.
face. Wefocusontheself-attentionrelationsonthispart. Acknowledgement. This project is supported by the
Our stock design allows free attention among body, face, Hong Kong Innovation and Technology Commission (In-
and hand location tokens, but limits joint tokens to only noHK Project CIMDA). It is also supported by the Min-
attend with tokens belonging to the same human. In the istry of Education, Singapore, under its MOE AcRF Tier
fullattentionsetting,weallowtokenstoanyothertokens. 2 (MOET2EP20221- 0012), NTU NAP, and under the
Theinter-personsettingwillfurtherlimitthehandandface RIE2020 Industry Alignment Fund – Industry Collabora-
locationtokenstoattendwithonlyitssubject. Asshownin tionProjects(IAF-ICP)FundingInitiative,aswellascash
Table5,theunlimitedsettingistheworst,asthecomplicated andin-kindcontributionfromtheindustrypartner(s).References andWeidongZhang.Learninganalyticalposteriorprobability
forhumanmeshrecovery. InCVPR,2023. 6
[1] Michael J Black, Priyanka Patel, Joachim Tesch, and Jin- [13] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios
longYang. Bedlam: Asyntheticdatasetofbodiesexhibit- Tzionas, and Michael Black. Collaborative regression of
ing detailed lifelike animated motion. In Proceedings of expressivebodiesusingmoderation. InInternationalConfer-
theIEEE/CVFConferenceonComputerVisionandPattern enceon3DVision(3DV),pages792–804,Dec.2021. 2,6,
Recognition,pages8726–8737,2023. 2,5,6,12,13 16
[2] ZhongangCai,DaxuanRen,AilingZeng,ZhengyuLin,Tao [14] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
Yu,WenjiaWang,XiangyuFan,YangGao,YifanYu,Liang Deepresiduallearningforimagerecognition. InProceed-
Pan,FangzhouHong,MingyuanZhang,ChenChangeLoy, ingsoftheIEEEconferenceoncomputervisionandpattern
LeiYang,andZiweiLiu. Humman:Multi-modal4dhuman recognition,pages770–778,2016. 3
datasetforversatilesensingandmodeling. October2022. 2 [15] AngjooKanazawa,MichaelJBlack,DavidWJacobs,and
[3] ZhongangCai, WanqiYin, AilingZeng, ChenWei, Qing- Jitendra Malik. End-to-end recovery of human shape and
pingSun,YanjunWang,HuiEnPang,HaiyiMei,Mingyuan pose. InCVPR,pages7122–7131,2018. 2,6
Zhang, Lei Zhang, et al. Smpler-x: Scaling up expres- [16] MuhammedKocabas,Chun-HaoPHuang,OtmarHilliges,
sive human pose and shape estimation. arXiv preprint andMichaelJBlack. Pare: Partattentionregressorfor3d
arXiv:2309.17448,2023. 2,5,6,7,12,14,15,16,17 humanbodyestimation. InICCV,pages11127–11137,2021.
[4] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, 2,6
DaxuanRen,ZhengyuLin,HaiyuZhao,LeiYang,andZi- [17] NikosKolotouros,GeorgiosPavlakos,MichaelJBlack,and
wei Liu. Playing for 3d human recovery. arXiv preprint KostasDaniilidis. Learningtoreconstruct3dhumanpose
arXiv:2110.07588,2021. 2 and shape via model-fitting in the loop. In ICCV, pages
[5] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas 2252–2261,2019. 2
Usunier,AlexanderKirillov,andSergeyZagoruyko. End-to- [18] NikosKolotouros,GeorgiosPavlakos,andKostasDaniilidis.
endobjectdetectionwithtransformers. InEuropeanconfer- Convolutionalmeshregressionforsingle-imagehumanshape
enceoncomputervision,pages213–229.Springer,2020. 2, reconstruction. InCVPR,pages4501–4510,2019. 2
3 [19] JiefengLi,SiyuanBian,QiLiu,JiashengTang,FanWang,
[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu andCewuLu.NIKI:Neuralinversekinematicswithinvertible
Xiong,XiaoxiaoLi,ShuyangSun,WansenFeng,ZiweiLiu, neural networks for 3d human pose and shape estimation.
JiaruiXu,ZhengZhang,DazhiCheng,ChenchenZhu,Tian- InProceedingsoftheIEEE/CVFConferenceonComputer
heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue VisionandPatternRecognition(CVPR),June2023. 6
Wu,JifengDai,JingdongWang,JianpingShi,WanliOuyang, [20] JiefengLi,SiyuanBian,ChaoXu,ZhicunChen,LixinYang,
Chen Change Loy, and Dahua Lin. MMDetection: Open andCewuLu. Hybrik-x: Hybridanalytical-neuralinverse
mmlab detection toolbox and benchmark. arXiv preprint kinematicsforwhole-bodymeshrecovery. arXivpreprint
arXiv:1906.07155,2019. 14 arXiv:2304.05690,2023. 2,6
[7] JunhyeongCho,KimYouwang,andTae-HyunOh. Cross- [21] JiefengLi,ChaoXu,ZhicunChen,SiyuanBian,LixinYang,
attentionofdisentangledmodalitiesfor3dhumanmeshre- andCewuLu. Hybrik: Ahybridanalytical-neuralinverse
coverywithtransformers.InECCV,pages342–359.Springer, kinematicssolutionfor3dhumanposeandshapeestimation.
2022. 2 InCVPR,pages3383–3393.ComputerVisionFoundation/
[8] VasileiosChoutas,GeorgiosPavlakos,TimoBolkart,Dim- IEEE,2021. 2,6
itriosTzionas,andMichaelJBlack. Monocularexpressive [22] ZhihaoLi,JianzhuangLiu,ZhensongZhang,SongcenXu,
bodyregressionthroughbody-drivenattention. InComputer andYouliangYan. Cliff: Carryinglocationinformationin
Vision–ECCV2020: 16thEuropeanConference, Glasgow, full frames into human pose and shape estimation. pages
UK,August23–28,2020,Proceedings,PartX16,pages20– 590–606,2022. 2,3,6
40.Springer,2020. 2,7,16 [23] JingLin,AilingZeng,HaoqianWang,LeiZhang,andYu
[9] MMHuman3DContributors. Openmmlab3dhumanpara- Li. One-stage3dwhole-bodymeshrecoverywithcomponent
metricmodeltoolboxandbenchmark. https://github. awaretransformer. 2023. 2,5,6,7,12,14,15,16,17
com/open-mmlab/mmhuman3d,2021. 12 [24] KevinLin,LijuanWang,andZichengLiu. Meshgraphormer.
[10] Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao, InICCV,pages12939–12948,2021. 2
QiangqiangWu,WeilinWan,TakuKomura,andWenping [25] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,and
Wang. Tore:Tokenreductionforefficienthumanmeshrecov- PiotrDollár. Focallossfordenseobjectdetection. InPro-
erywithtransformer. arXivpreprintarXiv:2211.10705,2022. ceedingsoftheIEEEinternationalconferenceoncomputer
2 vision,pages2980–2988,2017. 13
[11] ZicongFan, OmidTaheri, DimitriosTzionas, Muhammed [26] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
Kocabas, ManuelKaufmann, MichaelJBlack, andOtmar PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
Hilliges. Arctic: A dataset for dexterous bimanual hand- Zitnick. Microsoft coco: Common objects in context. In
objectmanipulation. InProceedingsoftheIEEE/CVFCon- European conference on computer vision, pages 740–755.
ferenceonComputerVisionandPatternRecognition,pages Springer,2014. 5,12,13,14
12943–12954,2023. 5,12,13 [27] ShilongLiu,FengLi,HaoZhang,XiaoYang,XianbiaoQi,
[12] QiFang,KangChen,YinghuiFan,QingShuai,JiefengLi, HangSu,JunZhu,andLeiZhang. DAB-DETR:DynamicanchorboxesarebetterqueriesforDETR. InInternational [41] YuSun,QianBao,WuLiu,TaoMei,andMichaelJBlack.
ConferenceonLearningRepresentations,2022. 3,14 Trace:5dtemporalregressionofavatarswithdynamiccam-
[28] MatthewLoper,NaureenMahmood,JavierRomero,Gerard erasin3denvironments. InProceedingsoftheIEEE/CVF
Pons-Moll, andMichaelJBlack. Smpl: Askinnedmulti- Conference on Computer Vision and Pattern Recognition,
personlinearmodel. ACMTOG,34(6):1–16,2015. 6,12 pages8856–8866,2023. 3
[29] GyeongsikMoon,HongsukChoi,andKyoungMuLee. Ac- [42] YuSun,WuLiu,QianBao,YiliFu,TaoMei,andMichaelJ
curate3dhandposeestimationforwhole-body3dhuman Black. Puttingpeopleintheirplace: Monocularregression
meshestimation. InCVPRW,2022. 2,5,6,7,16,17 of 3d people in depth. In Proceedings of the IEEE/CVF
[30] GyeongsikMoon,HongsukChoi,andKyoungMuLee. Neu- Conference on Computer Vision and Pattern Recognition,
ralannot:Neuralannotatorfor3dhumanmeshtrainingsets. pages13243–13252,2022. 2,3,4,6,7,16
InProceedingsoftheIEEE/CVFConferenceonComputer [43] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
VisionandPatternRecognition,pages2299–2307,2022. 12 reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
[31] HuiEnPang,ZhongangCai,LeiYang,QingyiTao,Zhonghua Polosukhin. Attentionisallyouneed. Advancesinneural
Wu, Tianwei Zhang, and Ziwei Liu. Towards robust and informationprocessingsystems,30,2017. 3
expressivewhole-bodyhumanposeandshapeestimation. In [44] WenjiaWang,YongtaoGe,HaiyiMei,ZhongangCai,Qing-
Thirty-seventhConferenceonNeuralInformationProcessing ping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and
Systems,2023. 2,15 Taku Komura. Zolly: Zoom focal length correctly for
[32] PriyankaPatel,Chun-HaoPHuang,JoachimTesch,DavidT perspective-distorted human mesh reconstruction. arXiv
Hoffmann,ShashankTripathi,andMichaelJBlack.AGORA: preprintarXiv:2303.13796,2023. 2
Avatarsingeographyoptimizedforregressionanalysis. In [45] Yanjun Wang, Qingping Sun, Wenjia Wang, Jun Ling,
ProceedingsoftheIEEE/CVFConferenceonComputerVi- Zhongang Cai, Rong Xie, and Li Song. Learning dense
sionandPatternRecognition,pages13468–13478,2021. 2, uv completion for human mesh recovery. arXiv preprint
5,7,12,13,14,16 arXiv:2307.11074,2023. 2
[33] GeorgiosPavlakos,VasileiosChoutas,TimoBolkart,Dim- [46] JieYang,AilingZeng,FengLi,ShilongLiu,RuimaoZhang,
itriosTzionas,MichaelJ.Black,VasileiosChoutas,Georgios and Lei Zhang. Neural interactive keypoint detection. In
Pavlakos,TimoBolkart,DimitriosTzionas,andMichaelJ. ProceedingsoftheIEEE/CVFInternationalConferenceon
Black. Monocularexpressivebodyregressionthroughbody- ComputerVision,pages15122–15132,2023. 2
drivenattention. ECCV,2020. 2 [47] Jie Yang, Ailing Zeng, Shilong Liu, Feng Li, Ruimao
[34] GeorgiosPavlakos,VasileiosChoutas,NimaGhorbani,Timo Zhang, and Lei Zhang. Explicit box detection unifies
Bolkart,AhmedA.Osman,DimitriosTzionas,andMichaelJ. end-to-end multi-person pose estimation. arXiv preprint
Black. Expressivebodycapture: 3dhands,face,andbody arXiv:2302.01593,2023. 2,3,12
fromasingleimage. InCVPR,2019. 1,2,3,5,6,12 [48] ZhitaoYang,ZhongangCai,HaiyiMei,ShuaiLiu,Zhaoxi
[35] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Chen,WeiyeXiao,YukunWei,ZhongfeiQing,ChenWei,
Sadeghian,IanReid,andSilvioSavarese. Generalizedin- BoDai,etal.Synbody:Syntheticdatasetwithlayeredhuman
tersectionoverunion:Ametricandalossforboundingbox modelsfor3dhumanperceptionandmodeling.arXivpreprint
regression. In 2019 IEEE/CVF Conference on Computer arXiv:2303.17368,2023. 2
VisionandPatternRecognition(CVPR),Jun2019. 13 [49] AilingZeng,XuanJu,LeiYang,RuiyuanGao,XizhouZhu,
[36] YuRong,TakaakiShiratori,andHanbyulJoo. Frankmocap: BoDai,andQiangXu.Deciwatch:Asimplebaselinefor10×
Amonocular3dwhole-bodyposeestimationsystemviare- efficient2dand3dposeestimation. InEuropeanConference
gressionandintegration. InIEEEInternationalConference onComputerVision,pages607–624.Springer,2022. 2
onComputerVisionWorkshops,2021. 2 [50] AilingZeng, LeiYang, XuanJu, JiefengLi, JianyiWang,
[37] Karthik Shetty, Annette Birkhold, Srikrishna Jaganathan, and Qiang Xu. Smoothnet: A plug-and-play network for
NorbertStrobel, MarkusKowarschik, AndreasMaier, and refininghumanposesinvideos. InEuropeanConferenceon
BernhardEgger. Pliks: Apseudo-linearinversekinematic ComputerVision,pages625–642.Springer,2022. 2
solver for 3d human body estimation. In Proceedings of [51] FangaoZeng,BinDong,YuangZhang,TiancaiWang,Xi-
theIEEE/CVFConferenceonComputerVisionandPattern angyuZhang,andYichenWei. Motr:End-to-endmultiple-
Recognition,pages574–584,2023. 6 objecttrackingwithtransformer. InEuropeanConferenceon
[38] DahuShi,XingWei,LiangqiLi,YeRen,andWenmingTan. ComputerVision(ECCV),2022. 2
End-to-endmulti-personposeestimationwithtransformers. [52] WangZeng,WanliOuyang,PingLuo,WentaoLiu,andXi-
In2022IEEE/CVFConferenceonComputerVisionandPat- aogangWang. 3dhumanmeshregressionwithdensecorre-
ternRecognition(CVPR),pages11059–11068,2022. 3 spondence. InCVPR,pages7054–7063,2020. 2
[39] QingpingSun,YiXiao,JieZhang,ShizheZhou,Chi-Sing [53] AixiZhang, YueLiao, SiLiu, MiaoLu, YongliangWang,
Leung,andXinSu. Alocalcorrespondence-awarehybrid ChenGao,andXiaoboLi. Miningthebenefitsoftwo-stage
cnn-gcnmodelforsingle-imagehumanbodyreconstruction. andone-stagehoidetection. AdvancesinNeuralInformation
IEEETransactionsonMultimedia,25:4679–4690,2023. 2 ProcessingSystems,34:17209–17220,2021. 2
[40] YuSun,QianBao,WuLiu,YiliFu,MichaelJBlack,andTao [54] HaoZhang,FengLi,ShilongLiu,LeiZhang,HangSu,Jun
Mei. Monocular,one-stage,regressionofmultiple3dpeople. Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
InProceedingsoftheIEEE/CVFinternationalconferenceon withimproveddenoisinganchorboxesforend-to-endobject
computervision,pages11179–11188,2021. 2,3,4,6,7,16 detection. arXivpreprintarXiv:2203.03605,2022. 3[55] HongwenZhang,YatingTian,YuxiangZhang,Mengcheng
Li,LiangAn,ZhenanSun,andYebinLiu.Pymaf-x:Towards
well-alignedfull-bodymodelregressionfrommonocularim-
ages. IEEETransactionsonPatternAnalysisandMachine
Intelligence,2023. 6
[56] HongwenZhang,YatingTian,XinchiZhou,WanliOuyang,
YebinLiu,LiminWang,andZhenanSun. Pymaf:3dhuman
poseandshaperegressionwithpyramidalmeshalignment
feedbackloop. InICCV,2021. 2,6
[57] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein
Kwon,MarcPollefeys,FedericaBogo,andSiyuTang. Ego-
body: Human body shape and motion of interacting peo-
plefromhead-mounteddevices. InComputerVision–ECCV
2022: 17thEuropeanConference,TelAviv,Israel,October
23–27,2022,Proceedings,PartVI,pages180–200.Springer,
2022. 5,12,13
[58] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang,andJifengDai. Deformabledetr: Deformabletrans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159,2020. 3
[59] ChengZou,BohanWang,YueHu,JunqiLiu,QianWu,Yu
Zhao,BoxunLi,ChenguangZhang,ChiZhang,YichenWei,
etal. End-to-endhumanobjectinteractiondetectionwithhoi
transformer. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages11825–11834,
2021. 2AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation
Supplementary Material
A.Overview containstestsetswith100single-personimages.
UBody[23]isadatasetcontainingdiversereal-lifescenar-
Duetothelimitedspaceonthemainpaper,wepresentmore
ios, including movies, TV shows, talk shows, vlogs, sign
additionaldetailsinthissupplementarymaterial,covering
language,onlineclasses,andmore. UBodycontainsanex-
thefollowingaspects:
tensivecollectionofrichgesturesandexpressionsthatare
• Additional details on datasets, experimental parameters
notpresentinotherreal-worlddatasets. Wedown-sample
relatedtodataaugmentation,andimplementationdetails
thetrainingsetto54Kimageswith66Kinstances. Weuti-
inSec.B.
lizeadownsampledtestset, asusedinSMPLer-X[3]and
• Supplementary experiments investigating the impact of
OSX[23],whichhas2642imageswith2642instances.
boundingboxesonalgorithmperformanceinSec.C.
ARCTIC [11] is an indoor dataset which mainly focuses
• ExtraSOTAquantitativeandqualitativecomparisonex-
onthehand-objectinteraction. Wedownsampletheoriginal
perimentsonEgoBody-EgoSet[57],BEDLAM[1],and
trainingset1000timesforatrainsetof50Kimageswith
ARCTIC[11],andextravisualizationcomparisononsin-
50Kinstances. Wediscardtheegocentricview,whichonly
gleperson,multiperson,andsyntheticimages. inSec.D.
containshands,inourtraining.Forthetestset,following[3],
wekeeptheoriginaltestsetwith207Kimagestoevaluate
B.ExperimentSetup
ourmethod.
Egobody[57]isalarge-scaledatasetforegocentricviews.
B.1.Datasets
TheegocentricviewdatasetsarecollectedusingMicrosoft
Thissubsectionprimarilydescribesthecharacteristicsofthe HoloLens2,includingRGB,depth,eyegaze,headtracking,
datasetsutilizedinourmainpaperandprovidesdetailson andhandtracking. TheSMPL-Xdataisobtainedbyfitting
howweusethesedatasetsinthetrainingandtestingstages. the multi-Kinect rig data. We downsample 2 times to get
AGORA[32]isasyntheticimagedatasetthatencompasses 45Kimageswith45Kinstancesintheegocentric-viewsplit
lotsofcomplexscenarios,includingsevereocclusionand fortraining. Thetestsethas62Kimages.
truncations. Ithas14Ktrainingimageswith122Kinstances We pre-train our model by randomly sampling data
and 1K validation images with around 8K instances. Re- from AGORA, BEDLAM, and COCO. We choose these
cently, AGORA has become an essential benchmark for threedatasetsbecausetheirmulti-personimagessatisfyour
tasks related to SMPL [28] and SMPL-X [34], primarily method of perceiving the positions of individuals in the
foritseffectivenessinassessingalgorithmperformancein images. Additionally,AGORAandBEDLAMhavethead-
occlusion-heavyscenes. Ourapproachutilizesthecomplete vantageofaccurategroundtruth,whileCOCOprovidesthe
datasetfortraining,validation,andtestingpurposes. diversityofreal-worldscenes,preventingourmethodfrom
BEDLAM[1]isasyntheticvideodatasetofferingawide overfittingtosyntheticdata.
varietyofdata,includingdiversebodyshapes,motions,skin SMPLer-X[3]indicatesthatscalingupthedatacanen-
tones,hairstyles,andclothing. Theclothingisnotablyren- hancealgorithmperformance. Hence,weincorporateEgo-
deredusingaprofessionalphysicssimulator,enhancingre- body, ARCTIC, and UBody data into our training. The
alism,particularlyindepictingcharactermovement. Each samplingprobabilitiesare0.2,0.2,0.2,0.2,0.1,and0.1,re-
image in the dataset features between 1 to 10 individuals. spectively,forAGORA,BEDLAM,COCO,Egobody,ARC-
Originallycomprising286Kimages,wedownscaleditbya TIC, and UBody. ARCTIC and Egobody provide more
factorof5,yielding57Limageswith190Kinstances,which accuratereal-worldwhole-bodydatacomparedtoCOCO,
wethenutilizedfortraining. whileUBodycontributesabundantanddiversegesturesand
MSCOCO[26]isalarge-scalereal-worlddatasetdesigned expressions. AllthedatasetsarestandardizedintotheHu-
forobjectdetection,segmentation,keypointdetection,and manData[9]format. SomevisualdemonstrationsofAiOS
captioning. Onthebasisofthekeypointsubset,weusethe areprovidedinFig. 6andFig. 7.
pseudoSMPL-X[34]annotationsfromNeuralAnnot[30]
B.2.Implementationdetails
fortraining. Itcontains56Kmulti-personimages,featuring
atotalof147Kinstances. The training is conducted on 16 V100 GPUs, with a total
EHF [34] is an indoor dataset consisting of only 100 im- batchsizeof32. WeinitializeAiOS’sbackbone,encoder,
agesalongwithcorrespondingSMPL-Xlabels. Duetothe and part of the decoder from a weight trained on COCO
absenceofacorrespondingtrainingset,currentalgorithms humanposeestimation,providedbyED-Pose [47]. Weuse
often utilize it to assess algorithm generalization. It only Adamoptimizerwithasteplearningratefortraining. WeFigure6.IllustrationofAiOSinindoordatasets.ThefirsttwocolumnsarethequalitativeresultsonARCTIC[11],whilethelasttwo
columnsarethequalitativeresultsonEgobody[57].
firsttrainourmodelfor60epochswithaninitiallearningrate totheircollectiveboundingbox. Alternatively,witha0.5
1×10−4anddropatthe50thepochonAGORA,BEDLAM, probability,thecroppingoperationisappliedtothecollec-
and COCO. We finetune it for 50 epochs with 1×10−5 tiveboundingboxofallinstances. Wemaintaintheoriginal
initiallearningrateanddropatthe25thepochonalltrain aspectratioduringthecroppingprocesstoavoidcropping
datasets. unusualaspectratios.
Duringthetrainingstage,weadoptcolorjittering,ran-
B.3.LossFunctions
domhorizontalflipping,randomimageresizing,andrandom
instancecropping.Forthecolorjittering,werandomlyapply Body-locationDecoder. ThelossesforsupervisingBody-
avariationof±0.2intheRGBchannels.Fortherandomhor- locationDecodersareL andL . L containsL1loss
box cls box
izontalflipping,weaugmentimagesandtheircorresponding andtheGIOUloss[35]forthebodylocation. L isfocal
cls
annotationsbyhorizontallyflippingthemwithaprobabil- loss [25]forclassifybodytokens.
ityof0.5. Forrandomimageresizing,duringthetraining Body-refinement Decoder.. The losses for supervising
process,weresizetheimagesinproportion,ensuringthat Body-refinement Decoders are L , L and L .
box j2d smplxb
theshortersideiskeptbetween480and800pixelsandthe L containsL1andlossGIOUlossforthebodylocation,
box
longer side does not exceed 1333 pixels. During testing, handslocation,andfacelocation. L istheL1lossand
j2d
we set the shorter side to 800 pixels, and the longer side OKSlosssupervisingthebodyjointslocationandL
smplx
isscaledproportionally,withtheconstraintthatitdoesnot containsL1losswithgroundtruthSMPL-Xbodyparame-
exceed1333pixels. Forrandominstancecropping,wefirst tersL ,L1lossL for3Dbodykeypointsregressed
param kp3d
randomly enable the cropping operation with a probabil- bySMPL-XJ-regressor,andL1lossL forprojected2D
kp2d
ity of 0.5. When performing the cropping operation on a keypoints.
multi-persondataset,suchasAGORA[32],BEDLAM[1], Wholebody-refinementDecoder.. Thelossesforsupervis-
COCO[26],Werandomlysample1toN instancesfromthe ingWholebody-refinementDecodersareL andL .
j2d smplx
imagewithprobability0.5,whereNisthetotalnumberof L istheL1lossandOKSlosssupervisingthewhole-body
j2d
peopleintheimage. Theimageisthencroppedaccording jointslocationandL containsL1losswithgroundtruth
smplxFigure7.IllustrationofAiOSinoutdoordatasets.ThefirstthreerowsarequalitativeresultsonUBody[23],andthelastrowisqualitative
resultsonCOCO[26]
Attention Types
SMPL-XparametersL param,L1lossL kp3dfor3Dwhole- II nn tt era r-- hh uu mm aa nn … … … …
bodykeypointsregressedbySMPL-XJ-regressor,andL1 Cross
lossL forprojected2Dwhole-bodykeypoints. … … … …
kp2d
LossWeightsWeweighted-sumallthelossesatallstagesas
Token Types Body Location Hand Location Face Location Joints Image Feature
thefinalloss.Thelossweightsofthesametypeoflossindif-
ferentstagesarethesame,whichareshownasfollows:L :
cls
Figure8.IllustrationofInter-human,Intra-human,andCross
2.0,L :1.0(pose),0.01(shape,expression),L :1.0
smplx kp3d
Attention.Inter-humanattentionisconductedbetweenthebody
(body),0.5(face,hand),L : 1.0(body),0.5(face,hand),
kp2d tokensandbody,hand,andfacelocationtokensofdifferentper-
L1 : 10,L : 4.0(body),0.5(face),0.5(hands),L :
j2d oks giou sons’. Intra-humanattentionisconductedwiththelocationand
2.0,L1 : 5.0.
box jointtokensofthesameperson. Cross-attentionisconductedbe-
tweenimagefeaturesandallthetokens.
C. Sensitivity Analysis of Performance to
BoundingBoxAccuracy
Firstly,weutilizethegroundtruth(GT)boundingboxto
croptheimageandevaluatetheperformanceofOSX[23]
Inthissection,wepresentthatcurrentmethodologiesexhibit
and SMPLer-X [3], denoted by GT Box in Table 6. Fol-
asignificantsensitivitytoboundingboxes. Ourexperiments
lowingthis,weemployDAB-DETR[6,27],anoff-the-shelf
arecarriedoutontheAGORA[32]validationset,utilizing
detectionmodel,toidentifyhumanboundingboxes,replac-
official evaluation tools 4 for a thorough assessment. We
ingtheGTboxesforimagecropping. Wepresentdetection
reportseveralkeymetrics,includingNormalizedMeanVer-
resultsunderthreeboundingboxscorethresholds: 0.1,0.2,
texError(NMVE),NormalizedMeanJointError(NMJE),
and0.3,labeledasdetectedboxeswithscores0.1,0.2,and
MeanVertexError(MVE),andMeanPer-JointPositionEr-
0.3,respectively.Ahigherscoreindicatesgreaterconfidence
ror (MPJPE), which focus on the reconstruction accuracy
inthedetectionresults,butitmayleadtomissinginstances
ofbody,hands,andface. Additionally,weincludeF-Score,
with severe occlusion. This is reflected in the metrics as
precision,andrecalltoevaluatetheaccuracyofdetection.
high accuracy but low recall. Conversely, a lower score
4https://github.com/pixelite1201/agora_evaluation threshold retains results with lower confidence, capturingNMVE↓(mm) NMJE↓(mm) MVE↓(mm) MPJPE↓(mm)
Methods BoxType FScore↑ Precision↑ Recall↑
All Body All Body All Body Face LHand RHand All Body Face LHand RHhand
GTbox 1.0 1.0 1.0 120.4 75.2 116.1 72.1 120.4 75.2 39.4 47.5 48.8 116.1 72.1 40.5 45.1 46.4
Detectedboxwscore0.1 0.58 0.42 0.92 237.8 155.9 228.3 147.4 137.9 90.4 43.8 48.3 50.4 132.4 85.5 46.4 46.1 48.1
Detectedboxwscore0.2 0.8 0.74 0.87 165.9 110.6 159.2 104.7 132.7 88.5 40.7 44.9 46.9 127.4 83.8 43.1 42.9 44.8
Detectedboxwscore0.3 0.86 0.89 0.83 148.1 100.3 142.1 95.1 127.4 86.3 37.8 41.8 43.6 122.2 81.8 39.8 39.8 41.6
OSX[23] GTbox× 0.95 0.93 0.97 123.6 77.3 119.3 74.2 117.4 73.4 38.2 46.3 47.7 113.3 70.5 39.0 43.9 45.3
AiOSbox 0.95 0.93 0.97 124.3 78.7 120.0 75.7 118.1 74.8 37.4 45.5 47.0 114.0 71.9 38.3 43.2 44.7
GTbox 1.0 1.0 1.0 99.5 60.2 95.5 57.5 99.5 60.2 32.9 41.9 43.0 95.5 57.5 34.1 39.5 40.5
Detectedboxwscore0.1 0.58 0.43 0.92 203.4 131.6 195.3 124.7 118.0 76.3 37.1 43.6 44.4 113.3 72.3 39.5 41.3 42.2
Detectedboxwscore0.2 0.81 0.75 0.88 140.7 92.6 135.1 87.8 114.0 75.0 34.7 40.7 41.7 109.4 71.1 36.9 38.5 39.5
SMPLer-X[3] Detectedboxwscore0.3 0.86 0.9 0.83 126.4 84.4 121.3 80.1 108.7 72.6 31.8 37.8 38.7 104.3 68.9 33.7 35.7 36.7
GTbox× 0.95 0.93 0.97 101.5 61.4 97.6 58.8 96.4 58.3 31.7 40.8 41.9 92.7 55.9 32.6 38.3 39.4
AiOSbox 0.95 0.93 0.97 103.3 63.5 99.6 61.1 98.1 60.3 31.3 40.4 41.8 94.6 58.0 32.3 38.0 39.4
AiOS - 0.95 0.93 0.97 106.4 64.2 103.4 62.1 101.1 61.0 30.7 43.9 45.7 98.2 59.0 32.8 41.5 43.4
Table6.AGORAvalidationset.OSX[23]andSMPLer-X[3]arefinetunedontheAGORAtrainingset.However,AiOSisnotintentionally
fine-tunedexclusivelyonAGORA.GTBoxmeansthatthismethodusesthegroundtruthboundingboxtocroptheimage.GTbox×means
thatthismethodusesthegroundtruthboundingboxtocroptheimagebutfilterstheinstancesthatAiOSfailstodetect.AiOSboxmeans
thatthismethodusestheboundingboxprovidedbyAiOS.Thebestresultsarecoloredwithred,andthesecond-bestresultsarecoloredwith
blueforOSXandSMPLer-X,respectively.
NMVE↓(mm) NMJE↓(mm) MVE↓(mm) MPJPE↓(mm)
Methods BoxType FScore↑ Precision↑ Recall↑
All Body All Body All Body Face LHand RHand All Body Face LHand RHhand
GTbox× 0.95 0.93 0.97 123.6 77.3 119.3 74.2 117.4 73.4 38.2 46.3 47.7 113.3 70.5 39.0 43.9 45.3
GTboxwithnoise× 0.95 0.93 0.97 126.1 78.8 121.8 75.6 119.8 74.9 38.7 46.9 49.1 115.7 72.1 39.6 44.5 46.6
OSX[23]
AiOSbox 0.95 0.93 0.97 124.3 78.7 120.0 75.7 118.1 74.8 37.4 45.5 47.0 114.0 71.9 38.3 43.2 44.7
GTbox× 0.95 0.93 0.97 101.5 61.4 97.6 58.8 96.4 58.3 31.7 40.8 41.9 92.7 55.9 32.6 38.3 39.4
GTboxwithnoise× 0.95 0.93 0.97 105.6 64.0 101.6 61.5 100.3 60.8 32.5 42.4 43.6 96.5 58.4 33.5 39.9 41.0
SMPLer-X[3]
AiOSbox 0.95 0.93 0.97 103.3 63.5 99.6 61.1 98.1 60.3 31.3 40.4 41.8 94.6 58.0 32.3 38.0 39.4
Table7.AGORAvalidationsetwithnoiseboundingbox.OSX[23]andSMPLer-X[3]arefinetunedontheAGORAtrainingset.GT
boxmeansthatthismethodusesthegroundtruthboundingboxtocroptheimage.GTboxwithnoisemeanstranslatingthegroundtruth
boundingboxby10%oftheimagesizeinthehorizontaldirection,causingthehumantodeviatefromtheimagecenter. Thisisavery
smallnoisethatensuresthepersonisnotremovedfromtheimageplane,avoidingtruncation.×meansthatthismethodfilterstheinstances
thatAiOSfailstodetect.Thebestresultsarecoloredwithred,andthesecond-bestresultsarecoloredwithblueforOSXandSMPLer-X,
respectively.
moreinstanceswithsevereocclusionbutresultinginmany pared to using the detected boxes (Detected box w score
redundantdetections. Thisisreflectedinthemetricsashigh 0.3), even though AiOS’s box includes more challenging
recallbutloweraccuracy. cases. Specifically, there is a significant improvement in
FromthedatapresentedinTable6,it’sevidentthatregard- NMVEwitha16%reductionfrom148.1to124.3forOSX
lessofusingaloworhighthresholdforfilteringdetection and an 18% decrease from 126.4 to 103.3 for SMPLer-X.
results,bothOSX[23]andSMPLer-X[3]showamarked Additionally,thereisanimprovementinNMJE,witha15%
decreaseinperformanceintermsofNVMEandNMJEwhen reductionfrom142.1to120.0forOSXanda17%decrease
comparedtoresultsachievedusingGTboundingboxes.This from121.3to99.6forSMPLer-X.Theseresultssubstantiate
performancegapislargelyduetothewayNMVEandNMJE thatone-stagemethodsdemonstratesuperiorperformancein
arenormalizedusingtheF1score. TheF1score,beingthe real-worldscenarioscomparedtoexistingtwo-stagemeth-
harmonicmeanofrecallandprecision,penalizesmethodsfor ods.
bothmisseddetectionsandfalsepositives,whichexplains ForafaircomparisonwithOSXandSMPLer-X,which
theobserveddiscrepancyinperformance. usetheGTboundingboxestocropimages,wefilteroutthe
Notably,whenwefilterthedetectionresultswithascore sameinstancesthatAiOSfailedtodetectforbothOSXand
of 0.1, denoted by Detected box w score 0.1, the recall SMPLer-X.Interestingly,wefoundthattheresultsobtained
is0.92,indicatingthatwedetectthemajorityofinstances, using AiOS bounding boxes (denoted with AiOS box in
althoughthereweremanyredundantdetections. Inthiscase, Table6)arecomparabletothoseobtainedusingGTbound-
ifweconcentrateonreconstructionaccuracy,representedby ing boxes (denoted with GT box×). Notably, under this
MVEandMPJPE,weobservethattheresultsusingdetected bounding box setting, AiOS still outperforms the current
boundingboxesOSXandSMPLer-Xaresignificantlyworse state-of-the-artOSX,eventhoughitutilizestheGTbound-
thanusingGTboundingboxforcroppingimages. ingboxandisonparwiththefoundationalmodelSMPler-X
OnthelowerpartofTable6,wepresenttheAiOSresults. L20. To further demonstrate the superiority of AiOS, we
Toallowmorehardcasestobedetected,weuseathreshold followtheprocedureinRoboSMPLX[31]totranslatethe
of0.1tofiltertheresult.Similartothemainpaper,providing imageby0.1oftheimagesizehorizontally,introducinga
AiOS’sboundingboxestoOSXandSMPLer-Xdenotedby smallnoisetotheGTboundingboxes(denotedbyGTbox
AiOSbox,leadstoasignificantperformanceincreasecom- with noise× in Table 7). It’s worth noting that this noiseInput ROMP BEV Ours
Input OSX SMPLer-X Ours Ours
Input Hand4Whole OSX SMPLer-X Ours
Figure 9. Additional visual comparisons with existing methods. Upper part: The first column is the input images, and they are
downloadedfromtheinternet.ThesecondcolumnisthevisualizationresultsofROMP[40],thethirdcolumnshowsthevisualizationresults
ofBEV[42],andthelastcolumnillustratesourvisualizationresults;Middlepart:ComparisonofcurrentSOTAmethods [3,23]withour
AiOSmodelonAGORA[32].Lowerpart:ComparisonofcurrentSOTAmethods [3,23,29]withourAiOSmodelonEHFtest[8].
NMVE↓(mm) NMJE↓(mm) MVE↓(mm) MPJPE↓(mm)
Methods FScore↑ Precision↑ Recall↑
All Body All Body All Body Face LHand RHand All Body Face LHand RHhand
PIXIE[13] 0.94 0.99 0.90 158.7 107.2 153.7 103.5 149.2 100.8 51.4 44.8 48.9 144.5 97.3 55.4 41.3 44.8
BEDLAM-CLIFF 0.94 0.99 0.90 100.6 65.2 98.0 64.3 94.6 61.3 29.8 34.7 35.5 92.1 60.4 30.4 32.2 32.6
BEDLAM-CLIFF++† 0.94 0.99 0.90 93.2 61.2 90.9 60.4 87.6 57.5 27.3 30.3 32.6 85.4 56.8 28.0 28.0 29.9
AiOS† 0.95 1 0.90 87.6 57.7 85.8 57.7 83.2 54.8 26.4 28.1 30.8 81.5 54.8 26.2 25.9 28.1
Table8.BEDLAMtestset.Thebestresultsareinbold.†denotesmethodsthatincludetheAGROAtrainingset.
onlyshiftsthepersonawayfromtheimagecenteranddoes cation and occlusion. Firstly, compare it with the results
notremovethepersonfromtheimageplane,avoidingtrun- croppingwiththeGTboundingboxdenotedbyGTbox×,PA-PVE↓(mm) PVE↓(mm)
Method All Hands Face All Hands Face
H4W[29] 63.4 18.1 4.0 136.8 54.8 59.2
OSX[23] 56.9 17.5 3.9 102.6 56.5 44.6
OSX[23]† 33.0 18.8 3.3 58.4 39.4 30.4
SMPLer-X[3] 31.9 18.9 2.5 52.2 39.3 27.0
NativeAiOS 31.8 19.7 2.3 51.6 40.6 26.5
AiOS 30.2 19.2 2.1 47.1 38.3 26.1
Table9.ARCTIC.†denotesthemethodfinetunedontheARCTIC
trainingset.
PA-PVE↓(mm) PVE↓(mm)
Method
All Hands Face All Hands Face
H4W[29] 58.8 9.7 3.7 121.9 50.0 42.5
OSX[23] 54.6 11.6 3.7 115.7 50.6 41.1
OSX[23]† 45.3 10.0 3.0 82.3 46.8 35.2
SMPLer-X[3] 38.9 9.9 3.0 66.6 42.7 31.8
SMPLer-X[3]† 37.8 9.9 2.9 63.6 46.3 32.3
NativeAiOS 40.8 9.1 3.0 64.6 42.3 26.3
AiOS 38.0 9.0 2.9 61.6 40.0 26.7
Table10. EgoBody-EgoSet. †denotesthemethodsthatarefine-
tunedontheEgoBody-EgoSettrainingset.
we can observe a drop in performance. Specifically, for
NMVE,OSXincreasedfrom123.6to126.1,andSMPLer-
Xincreasedfrom101.5to105.6. RegardingNMJE,OSX
increased from 119.3 to 121.8, and SMPLer-X increased
from97.6to101.6. Thisobservationindicatesthatcurrent
two-stagemethodsarehighlysensitivetotheaccuracyofthe
boundingbox,asevenaslightnoiseintroduced,causingthe
persontobeoff-center,resultsinaperformancedrop,even
withGTboundingboxes.
Whencomparingtheresultsobtainedbycroppingimages
withboundingboxesprovidedbyAiOStothoseobtainedby
croppingwithGTboundingboxesandGTboundingboxes
with added noise, we observe that the results of cropping
with AiOS-provided bounding boxes are slightly inferior
tothoseobtainedwithGTboundingboxesinbody-related
metricsbutbetterthanGTboundingboxeswithaddednoise.
However, for the face and hands, the results of cropping
withAiOS-providedboundingboxescanevenbebetterthan
thoseobtainedwithGTboundingboxes. Weattributethis
improvementtoAiOS’sattentiontonotonlythebodybut
alsothehandsandface.
D.ExtraSOTAcomparisonexperiments
In this section, we show the extra single datasets on
EgoBody-EgoSet,ARCTIC,andBEDLAMinTable10, 9,
8. OurproposedAiOSachievedSOTAperformanceacross
allthesedatasets. Also,weprovideextravisualizationcom-
parisoninFig. 9.