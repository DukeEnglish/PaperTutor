CMP: Cooperative Motion Prediction with Multi-Agent Communication
Zhuoyuan Wu1,∗, Yuping Wang2,∗, Hengbo Ma3, Zhaowei Li4, Hang Qiu5,†, and Jiachen Li5,†
Abstract—TheconfluenceoftheadvancementofAutonomous EgoAV Sensor Perception Prediction Future
Data Trajectories
Vehicles(AVs)andthematurityofVehicle-to-Everything(V2X)
(a)
communication has enabled the capability of cooperative con-
nected and automated vehicles (CAVs). Building on top of Sensor Future
EgoAV Perception Prediction Aggregation
cooperative perception, this paper explores the feasibility and Data Trajectories
effectiveness of cooperative motion prediction. Our method,
Sensor Future
CMP, takes LiDAR signals as input to enhance tracking and CAV Data Perception Prediction Aggregation Trajectories
prediction capabilities. Unlike previous work that focuses sep- (b)
arately on either cooperative perception or motion prediction,
our framework, to the best of our knowledge, is the first to Fig.1:Acomparisonbetweenthetraditionalpipelineandthe
address the unified problem where CAVs share information in proposed multi-vehicle cooperative prediction pipeline. (a)
bothperceptionandpredictionmodules.Incorporatedintoour The traditional pipeline conducts perception and prediction
design is the unique capability to tolerate realistic V2X band-
based on a single AV’s raw sensor data. (b) The proposed
width limitations and transmission delays, while dealing with
pipeline involves multiple cooperative CAVs, which share
bulkyperceptionrepresentations.Wealsoproposeaprediction
aggregation module, which unifies the predictions obtained by information to enhance both perception and prediction.
different CAVs and generates the final prediction. Through
extensiveexperimentsandablationstudies,wedemonstratethe
and prediction, which transmits intermediate representations
effectivenessofourmethodincooperativeperception,tracking,
and motion prediction tasks. In particular, CMP reduces the of point cloud features. However, integrating perception
averagepredictionerrorby17.2%withfewermissingdetections and prediction, as illustrated in Fig. 1(b), to fully realize
compared with the no cooperation setting. Our work marks a V2Vcooperationremainsunexplored.Onmotionprediction,
significantstepforwardinthecooperativecapabilitiesofCAVs,
initial efforts [7]–[9] use LSTM-based networks on simple
showcasing enhanced performance in complex scenarios.
datasets. Recent studies [10]–[12] adopt attention networks
I. INTRODUCTION and graph convolutional networks to enhance motion pre-
diction. However, these approaches rely on the ground-truth
The current autonomous driving system is critically de-
trajectory data, neglecting the uncertainties and inaccuracies
pendentonitsonboardperception.Similartohumandrivers,
propagated from upstream detection and tracking tasks. This
however, such dependency is vulnerable to situations with
reliance on ground truth data remains insufficient to address
occlusions or impaired visibility. Leveraging multiple van-
the real-world challenge of handling uncertain trajectories,
tage points, cooperative perception [1]–[5] uses Vehicle-to-
underscoring the need for research that integrates perception
Everything (V2X) communications to share sensory infor-
and prediction in V2V cooperation.
mation among connected and automated vehicles (CAVs)
Tofillthegapbetweencooperativeperceptionandmotion
and infrastructure. This shared information varies in format,
prediction, we introduce a novel framework for cooperative
including raw data, processed features, or detected objects,
motionpredictionbasedontherawsensordata.Tothebestof
along with relevant metadata (e.g., timestamps and poses).
ourknowledge,wearethefirsttodevelopapracticalmethod
Fusing this information from multiple viewpoints to the
thatjointlysolvestheperceptionandpredictionproblemwith
perspective of a vehicle recipient, the augmented onboard
CAV communications in both components. Our proposed
perception can now “see” beyond their direct line of sight
framework is illustrated in Fig. 2. Each CAV computes its
and through occlusions.
own bird-eye-view (BEV) feature representation from its
Current V2V research has largely been confined to ei-
LiDAR point cloud. The data is processed, compressed, and
ther cooperative perception or motion prediction, with no
broadcast to other nearby CAVs. The receiver agents fuse
comprehensive studies on their joint application. Beyond
the transmitted feature encoding. Upon obtaining historical
object detection, most works incorporate other tasks, such
perception data, the trajectories of the surrounding objects
as prediction [3] and mapping [6] as auxiliary outputs.
can be predicted by each CAV based on the backbone
Wang et al. [3] proposes a V2V method for perception
of MTR [10]. Then, the individually predicted trajectories
∗Equalcontribution from each CAV are broadcast again. As our model collects
†Correspondingauthors the predictions from surrounding CAVs, the predictions and
1PekingUniversity,China.wuzhuoyuan@pku.edu.cn
intermediate features from perception are used to refine the
2UniversityofMichigan,AnnArbor,MI,USA.ypw@umich.edu
3University of California, Berkeley, CA, USA. motionpredictions.Ourmethodallowsforrealistictransmis-
hengbo ma@berkeley.edu sion delays between CAVs and bandwidth limitations while
4UniversityofWashington,WA,USA.lzw365@uw.edu
achieving satisfactory performance.
5University of California, Riverside, CA, USA. {hangq,
jiachen.li}@ucr.edu In this paper, our main contributions are as follows:
4202
raM
62
]OR.sc[
1v61971.3042:viXraCompressed
Information
Sharing
PointPillar Compression FuseBEVT AB3DMOT MTR
EgoAV BEV Features
Prediction
Aggregated Aggregation
BEV Features
BEV Features
PointPillar Compression FuseBEVT AB3DMOT MTR
Compressed
CAV Information
Sharing
Fig. 2: An overall diagram of the proposed cooperative motion prediction pipeline.
• We propose a practical, latency-robust framework for B. Motion Prediction
cooperative motion prediction, which leverages the in-
Motion prediction is another key research topic in au-
formation shared by multiple CAVs to enhance percep-
tonomous driving. Mainstream research mostly focuses on
tion and motion prediction performance.
a non-cooperative environment where a single AV pre-
• We analyze the bandwidth requirement for cooperative dicts without communication [10], [17]–[28]. Recent meth-
information sharing and design a lightweight represen-
ods [18], [24], [29] encode agent historical trajectories and
tation for communication.
map polylines into high-dimensional vectors and use graph
• We develop a transformer-based prediction aggregation neural networks to capture their relationships, which are
module to take advantage of the predictions shared by
followed by decoding layers to produce predictions. The
other CAVs, which improves prediction accuracy.
most recent work introduced transformer structure into their
models. MTR [10], MTR++ [11] use motion query pairs
II. RELATEDWORK
where each pair is in charge of one motion mode prediction,
which is more efficient than goal-based strategies [30] and
A. Cooperative Perception
converges faster than direct regression strategies [21], [31].
Cooperative perception allows CAVs to use advanced
communicationsystemstoshare informationtoenlargetheir III. PROBLEMFORMULATION
fields of view. Previous works have developed early fusion
The goal of the cooperative prediction task is to infer the
techniques for cooperative object detection based on shared
future trajectories of all the movable agents in the scene that
raw LiDAR or RGB camera data [2]. Using this strat-
canbedetectedbymultiplecollaborativeCAVswithonboard
egy, however, requires high transmission bandwidth due to
sensors. In this work, we only use the LiDAR information
the preservation of complete sensor measurements. Another
for perception (i.e., object detection and tracking) to obtain
strategy, late fusion, allows vehicles to only share their final
the agents’ trajectories. We denote the number of CAVs as
detections and rely on another model to fuse the detections
N ,theLiDARpointcloudofi-thCAVattimetasLi,i=
generated by CAVs [13]. However, in real-life deployments, CAV t
1,...,N , and the local map information as Mi. Assume
theperformanceoflatefusioniscappedbythelossofcontext CAV t
that there are N detected agents at time t, we denote their
t
information and individual detection accuracy.
historical trajectories as X where T represents the
To balance this trade-off, the middle-ground strategy of
t−Th+1:t h
history horizon. We aim to infer their multi-modal future
intermediate fusion [3], [5], [14], [15] has become more trajectories Xˆ based on theabove information where
prevalent. In this strategy, CAVs employ encoder models
t+1:t+Tf
T represents the prediction horizon.
f
to process the surrounding traffic information and map
information to intermediate features, and then share these IV. METHOD
features with the surrounding vehicles. Upon receiving these
A. Method Overview
features, CAVs fuse them with their own information and
generate better perception results. For example, V2VNet [3] Fig. 2 provides an overall diagram of our framework,
employed a Graph Neural Network to aggregate information which consists of three major components: cooperative per-
from different viewpoints. AttFuse [4] deployed an attention ception, trajectory prediction, and prediction aggregation.
mechanismtofusetheintermediatefeatures.QiaoandZulk- The cooperative perception module takes in the raw sensor
ernine [15] proposed a fusion model that adaptively chooses data obtained by CAVs and generates the observed agents’
intermediatefeaturesforbetterintegration.CoBEVT[5]and trajectories through object detection and multi-object track-
HM-ViT[16]havelargelyadoptedvisiontransformermodels ing. Then, the trajectory prediction module takes in the
to enhance camera input processing and feature integration historical observations and infers future trajectories from the
and achieve promising results on the OPV2V dataset [4]. perspective of each CAV. Finally, the prediction aggregationmodule leverages the predictions from all CAVs and gener- a data association module is adopted to match the predicted
ates the final prediction hypotheses. trajectoriesfromtheKalmanfilterandthedetectedbounding
boxesinthecurrentframe.The3DKalmanfilterupdatesthe
B. Cooperative Perception stateofmatchedtrajectoriesbasedonthematcheddetections.
The cooperative perception module aims to detect and Throughout the tracking process, a birth and death memory
track objects based on the 3D LiDAR point clouds obtained createstrajectoriesfornewobjectsanddeletestrajectoriesfor
by multiple CAVs. We modify CoBEVT [5] to serve as disappeared objects. More details of these operations can be
the backbone of the cooperative object detection model found in [32]. The tracker outputs the historical trajectories
followed by the AB3DMOT tracker [32] to obtain historical of all the agents detected at time t, denoted as X t−Th+1:t,
trajectories of agents. whichservesastheinputofthetrajectorypredictionmodule.
CooperativeObjectDetection.PointPillar[33]isemployed
to extract point cloud features for each CAV with a voxel C. Motion Prediction
resolution of (0.4, 0.4, 4) along x, y, and z axes. Before the
cross-agent collaboration, each CAV i calculates a bird-eye- Our trajectory prediction module is built upon MTR [10],
view (BEV) feature Fi ∈ RH×W×C, where H, W, and C astate-of-the-artmodelconsistingofascenecontextencoder
denote height, width, and channels, respectively. and a motion decoder. We only provide a general introduc-
Due to the real-world hardware constraints on the volume tion, and more details about the model can be found in [10].
of the transmitted data for V2V applications, it is necessary For the i-th CAV, the scene context encoder extracts
to compress the BEV features before transmission to avoid featuresfromtheagents’trajectoriesX t−Th+1:t andthelocal
large bandwidth-induced delays. As in [5], a convolutional mapinformationMi.Theagents’trajectoriesarerepresented
t
auto-encoder is used for feature compression and decom- as polyline vectors [18], which are processed by a PointNet-
pression. Upon receipt of the broadcast messages containing like polyline encoder [34] to extract agent features. The
intermediate BEV representations and the sender’s pose, a map information is encoded by a Vision Transformer [35]
differentiable spatial transformation operator Γ is used to to extract map features. Then, a Transformer encoder is
ξ
align the features to the ego vehicle’s coordinate, which is used to capture the local scene context. Each layer uses
written as Hi = Γ (cid:0) Fi(cid:1) ∈ RH×W×C. Our method allows multi-head attention with queries, keys, and values defined
ξ
for up to a 100ms latency during feature transmission by relative to previous layer outputs and position encodings,
taking in the messages sent by other CAVs at the last frame. integrating the trajectory embeddings and map embeddings.
Sincesensorsrefreshfasterthan10fps,Framestakinglonger Future agent movements are predicted via regression based
than 100ms to transmit (e.g. partially retransmitted or lost) on the extracted past agent features. These predictions are
will be dropped, and the latest frame will be used instead. re-encoded by the same polyline encoder and merged with
Then, FuseBEVT [5] is used to merge the BEV features historical context features.
receivedfromvariousagents.Morespecifically,theegovehi- After obtaining the scene context features, a Transformer-
clefirstaggregatesalltheavailablefeaturesintoatensorh∈ based motion decoder is employed to generate multi-modal
RNCAV×H×W×C, which is then processed by the FuseBEVT prediction hypotheses through joint optimization of global
module to obtain the fused feature h′ ∈RH×W×C. Finally, intention localization and local movement refinement. More
two 3×3 convolutional layers are applied for classification specifically, K representative intention points I∈RK×2 are
and regression to obtain the 3D bounding boxes of objects. generated by adopting the k-means clustering algorithm on
CoBEVT outputs a collection of detections at time t de- the endpoints of ground truth trajectories (K = 64 in our
notedbyD ={D1,...,DNt},whereN representsthetotal setting), where each intention point represents an implicit
t t t t
number of detections. Each detection Dj is characterized motion mode that represents the motion direction. The local
t
by a tuple (x,y,z,θ,l,w,h,s), which encapsulates the 3D movement refinement enhances global intention localization
coordinatesoftheobject’scenter(x,y,z),the3Ddimensions byiterativelyrefiningtrajectorieswithfine-grainedtrajectory
of the object bounding box (l, w, h), the orientation angle θ, features. The dynamic searching query is initially set at
and the confidence score s. the intention point, and updates dynamically based on the
Multi-Object Tracking. The tracking module is employed trajectorypredictedateachdecoderlayer,servingasaspatial
to associate the detected 3D bounding boxes of objects point’s position embedding.
into trajectory segments. We adopt AB3DMOT [32], an Inthedecoder,staticintentionqueriestransmitinformation
online multi-object tracking algorithm, which takes in the across motion intentions while dynamic searching queries
detectionsinthecurrentframeandtheassociatedtrajectories gathertrajectory-specificinformationfromthescenecontext.
in previous frames. Excluding the pre-trained cooperative The updated motion query is expressed as Cj ∈ RK×D
object detection module, AB3DMOT requires no additional in the j-th layer where D is the feature dimension. Each
training and is simply applicable for inference. decoder layer adds a prediction head to Cj for creating
More specifically, after obtaining the 3D bounding boxes future trajectories. Due to the multi-modal nature of agents’
from the cooperative object detection module, we apply behaviors, a Gaussian Mixture Model (GMM) is adopted
a 3D Kalman filter to predict the state of the associated for trajectory distributions. For each future time step t′ ∈
trajectoriesfrompreviousframestothecurrentframe.Then, {t+1,...,t+T}, we infer the likelihood p and parameters
f(µ ,µ ,σ ,σ ,ρ) of Gaussian components by E. Loss Functions
x y x y
Zj =MLP(Cj), (1) Cooperative Object Detection. We adopt the same loss
t+1:t+Tf functionasCoBEVT[5].Inparticular,ourframeworkincor-
where Zj ∈ RK×6 contains the parameters of K Gaussian porates the two convolutional layers for the detection head
t′
components N (µ ,σ ;µ ,σ ;ρ) and the corresponding and employs the smooth L1 loss for bounding box localiza-
1:K x x y y
likelihoods p 1:K. The distribution of the agent’s position at tion L detloc and the focal loss for classification L detcls, as
time t′ is written as outlined in [36]. The complete loss function is
K L =(β L +β L )/N , (8)
(cid:88) det loc detloc cls detcls p
Pj(o)= p ·N (o −µ ,σ ;o −µ ,σ ;ρ), (2)
t′ k k x x x y y y whereN denotesthecountofpositiveinstances,β =2.0,
k=1 p loc
and β =1.0 .
cls
where Pj(o) denotes the probability of the agent located Motion Prediction. Our prediction model is trained with
t′
at o ∈ R2 at time t′. The trajectory predictions of all the two loss terms. An L1 regression loss is used to refine the
agents Xˆ t+1:t+Tf can be derived from the center points of outputs in Eq. (1). We also employ a negative log-likelihood
corresponding Gaussian components. loss based on Eq. (2) to enhance the prediction accuracy of
theactualtrajectories.Wetaketheweightedaverageofthese
D. Prediction Aggregation
two terms as the total loss, which is written as
Besides sharing the BEV features between CAVs, we
L =ω L +ω L . (9)
also propose to transmit the prediction hypotheses generated pred loc predloc cls predcls
by each CAV to others. Each CAV adopts an aggregation Following [21], we apply a hard-assignment technique for
mechanism to fuse the predictions received from others with optimizationbychoosingthemotionquerypairthatisclosest
its own predictions. The underlying intuition is that the to the ground truth (GT) trajectory’s endpoint as the positive
predictions for a certain agent obtained from different CAVs Gaussian component, determined by the distance between
may have different levels of reliability. For example, a CAV each intention point and the GT endpoint. The Gaussian
closesttothepredictedagentmaygeneratebetterpredictions regression loss is applied at every decoder layer, and the
than others. Thus, the predictions from different CAVs may overall loss combines the auxiliary regression loss with the
complement each other, leading to the best final prediction. Gaussian regression losses with equal weights.
More specifically, in a scenario with N CAVs and N PredictionAggregation.Ourpredictionaggregationmodule
CAV o
predicted agents, the GMM prediction components for agent producesoutputsinthesameformatasthemotionprediction
j by CAV i at time t are denoted as Zi ). The local module, and we apply the same loss function as Eq. (9).
j,t+1:t+Tf
map and BEV features of CAV i are denoted as Mi and
t V. EXPERIMENTS
Hi,respectively.WeaggregatetheGMMcomponentsofthe
t A. Dataset
predictedtrajectories,BEVfeatures,andmapinformationfor
all CAVs. For CAV i, it begins the aggregation process by We use the OPV2V dataset [4] to validate our approach.
concatenating its GMM, map, and BEV features: This dataset contains 73 traffic scenarios with a duration of
about 25 seconds with multiple CAVs. A range of two to
Ei j,t =[MLP(f(Zi j,t+1:t+Tf)),MLP(f(Mi t)),MLP(f(Hi t))], seven CAVs may appear concurrently, which are equipped
(3) withaLiDARsensorandfourcamerasfromdifferentviews.
Upon receiving the GMM components from other CAVs k Following [5], we use a surrounding area of 100m×100m
(1 ≤ k ≤ N CAV,k ̸= i), the same map, BEV features from with a map resolution of 39cm for evaluation. The dataset
the ego are concatenated again: contains6764,1981,and2719framesfortraining,validation,
and testing, respectively.
Ek =[MLP(f(Zk )),MLP(f(Mi)),MLP(f(Hi))],
j,t−1 j,t:t+Tf−1 t t
(4) B. Evaluation Metrics
followed by a multi-head self-attention to fuse the features Cooperative Object Detection. We use the standard eval-
across all CAVs, uation metrics as in [4], [37], including Average Precision
Gi =MHA([Ei ,...,Ek ]),1≤k ≤N ,k ̸=i (5) (AP), Average Recall (AR), and F1-score at IoU thresholds
j,t j,t j,t−1 CAV of 0.3, 0.5 and 0.7, respectively.
where MHA is multi-head self-attention, f is the flatten Tracking.Weusethestandardevaluationmetricsasin[32],
operation,andGi istheaggregatedfeatureforagentj from includingMulti-ObjectTrackingAccuracy(MOTA),Average
j,t
theperspectiveofCAVi.TheGMMcomponentsfromother Multi-Object Tracking Accuracy (AMOTA), Average Multi-
CAVsaredelayedbyoneframe.Finally,twoseparateMLPs Object Tracking Precision (AMOTP), scaled Average Multi-
derive the aggregated Gaussian parameters by Object Tracking Accuracy (sAMOTA), Mostly Tracked Tra-
jectories (MT), and Mostly Lost Trajectories (ML).
Ni (µ ,σ ;µ ,σ ;ρ)= MLP(Gi ), (6)
j,1:K,t+1:t+Tf x x y y j,t Motion Prediction. We predict the agents’ trajectories for
pj = MLP(Gi ), (7) the future 5.0 seconds based on 1.0 seconds of historical
j,1:K,t+1:t+Tf j,t observations. We use the standard evaluation metrics as in
whichwillbeusedtosamplethefinalpredictionhypotheses. [10], including minADE and minFDE .
6 6TABLE I: The comparisons of cooperative object detection performance.
CommunicationSetting CompressionRatio AP0.3↑ AR0.3↑ F10.3↑ AP0.5↑ AR0.5↑ F10.5↑ AP0.7↑ AR0.7↑ F10.7↑ Bandwidth(M/s)↓
NoCooperation N/A 0.76 0.39 0.52 0.75 0.39 0.51 0.61 0.35 0.45 N/A
NoDelay 1x 0.94 0.47 0.63 0.93 0.47 0.63 0.88 0.46 0.60 80.0
NoDelay 256x 0.93 0.47 0.63 0.93 0.47 0.63 0.88 0.46 0.60 0.31
Upto100msDelay 1x 0.94 0.49 0.65 0.93 0.49 0.64 0.81 0.45 0.58 80.0
Upto100msDelay 256x 0.93 0.47 0.63 0.92 0.47 0.62 0.82 0.44 0.58 0.31
TABLE II: The comparisons of multi-object tracking performance.
CommunicationSetting CompressionRatio sAMOTA↑ AMOTA↑ AMOTP↑ MOTA↑ MOTP↑ MT↑ ML↓
NoCooperation N/A 57.32 17.22 29.84 56.59 51.39 26.15 22.46
NoDelay 1x 72.25 27.13 52.62 70.09 69.33 49.54 14.15
Upto100msDelay 1x 67.17 23.60 46.35 65.63 64.99 40.92 14.46
Upto100msDelay 256x 66.99 23.60 45.83 66.98 64.33 42.77 14.46
TABLE III: The comparisons of motion prediction performance (meter).
CooperationType minADE6@1s↓ minADE6@3s↓ minADE6@5s↓ minFDE6@1s↓ minFDE6@3s↓ minFDE6@5s↓
NoCooperation 0.3817 1.1082 2.1095 0.6558 2.3503 5.0242
CooperativePerceptionOnly 0.3398 0.9567 1.8556 0.5513 2.0008 4.3054
CooperativePrediction(Ours) 0.3252 0.8977 1.7472 0.5199 1.8306 4.0531
C. Implementation Details model for 30 epochs with a batch size of 8.
Cooperative Object Detection. CoBEVT [5] assumes no D. Quantitative and Ablative Results
delay in the communication between CAVs, which may Cooperative Object Detection. In Table I, we demon-
not be realistic due to hardware or wireless communication strate the effects of multi-vehicle cooperation, communi-
constraints. To address this limitation, our model allows for cation delay, and compression ratio of BEV features on
uptoa100ms(i.e.,1frame)delayinreceivingthemessages
the object detection performance. The comparisons between
(i.e., BEV features) from other CAVs. In addition, our BEV No Cooperation and other settings show the improvement
features are compressed by 256 times compared with that brought by the CAV communications. To mimic the real-
in CoBEVT. Instead of selecting a single CAV as the ego world constraints, we introduce a 100ms communication
vehicle in the original OPV2V traffic scenarios as in [5], delay,substantiallyshorterthantypicalhumanreactiontimes
we augment the training data samples by treating each of to visual stimuli, yet realistic within current wireless com-
the CAVs in the scene as the ego vehicle. We train our municationcapabilities.Wenoticetheperformancedecreases
model using the AdamW [38] optimizer with a learning rate slightly at an IOU threshold of 0.7 and remains comparable
scheduler starting at 1×10−3 and reduced every 10 epochs. in other settings, which can be attributed to the temporal
Tracking.Inoursetting,wesetF min =3andAge min =2in misalignment of shared data. In addition, a compression
thebirth/deathmemorymodule.Thedataassociationmodule ratio of 256 only slightly degrades system performance but
uses a threshold of IoU min = 0.01 for vehicles, and Dist max significantly reduces the bandwidth requirements, demon-
is set to 10. More details can be found in [32]. strating efficiency in data transmission when sharing BEV
Motion Prediction. We use 6 encoder layers for context en- features from one CAV to another. Based on these find-
coding with a hidden feature dimension of 256. The decoder ings, we adopt a 256x compression ratio for BEV features
employs 6 layers and 64 motion query pairs, determined and accommodate a 100ms communication latency between
by k-means clustering on the training set. We pre-train the CAVs, which balances between the model performance and
prediction model with an AdamW optimizer [38] with a hardware constraints (i.e., bandwidth, latency).
learning rate of 1×10−4 and a batch size of 80 over 30 Tracking. We show the enhancement of tracking perfor-
epochs. More details can be found in [10]. mance enabled by multi-vehicle cooperations in Table II.
Prediction Aggregation. We use three MLPs to encode V2Vcommunicationenablesthefusionofinformationacross
the GMM parameters, map features, and BEV features, different CAVs, which significantly increases the number of
respectively. Then, an 8-head, 5-layer transformer encoder true positives (i.e., accurately detected objects) and reduces
is used to aggregate the features, followed by two MLPs to the instances of false positives and false negatives (i.e.,
decodetheoutputsintothefinalGMMtrajectoryandscores, missing objects). The improvement in object detection is a
whichfollowthesameformatastheoutputsoftheprediction major cause for the enhanced performance of the tracking
module. We train the aggregation module with a learning system. Furthermore, despite the substantial BEV feature
rate of 1×10−4 and fine-tune the prediction module with a compression, we observe no detrimental effect on the track-
reduced learning rate of 1×10−6. The learning rates decay ing performance, which implies that tracking remains robust
in the same manner as the prediction model. We train the even under significant feature compression.(a) No Cooperation (b) Cooperative Prediction (Ours)
(c) No Cooperation (d) Cooperative Prediction (Ours)
Ego CAV Other CAVs Non-CAV Vehicles Predicted Waypoints Ground Truth Trajectory
Fig. 3: The visualizations of predicted trajectories under different model settings.
Cooperative Motion Prediction. We present a series of 4.0
No Cooperation
quantitative and ablation studies on cooperative motion pre-
Cooperative Perception Only
diction. The detailed results are shown in Table III and Fig. 3.5 Cooperative Prediction (Ours)
4. The Cooperative Perception Only setting does not include
3.0
ourpredictionaggregationmodule,andCAVsonlysharethe
compressed BEV features in the perception stage. Table III 2.5
shows that cooperative perception enhances the prediction
2.0
performancebyalargemarginandtheimprovementbecomes
larger as the prediction horizon increases. At 5s, our model 0 25 50 75 100 125 150 175 200
Area Covered by CAVs (m2)
achieved a 12.3%/17.2% reduction in minADE compared
6
Fig.4:Acomparisonofmotionpredictionperformanceunder
with the Cooperative Perception Only and No Cooperation
differentareascoveredbyCAVs.Theareaiscalculatedbased
settings, respectively. The reason is that cooperative percep-
on the smallest convex hull that covers all the CAVs.
tion improves the detection accuracy and thus the quality
of historical trajectories employed by the prediction module.
allowingeach CAVtoextend itsperceptionrange anddetect
Moreover, the prediction aggregation module allows CAVs
vehicles that might otherwise be missed. Fig. 3(c) and 3(d)
to leverage the predictions from others to collectively com-
showanotherscenario,demonstratingtheimprovedaccuracy
pensate for their prediction in challenging and ambiguous
of cooperative prediction. In this case, the predicted trajec-
situations. In Fig. 4, our cooperation modules bring more
tories by cooperative prediction align more closely with the
benefitsastheCAVsinthescenecoverlargerfieldsofview.
ground truth thanks to information sharing between CAVs.
Specifically, we approximate the perception coverage area
from the CAVs as the area of the convex hull formed by
VI. CONCLUSION
the CAVs. As this area grows to over 200m2, the perfor-
mance improvement of our model climbs to 17.5%/28.4%, Inthispaper,weintroducethefirst-of-its-kindcooperative
compared to the other two settings. The reason is that motion prediction framework that advances the cooperative
the communication between CAVs enhances the situational capabilities of CAVs, addressing the crucial need for safe
awareness of the ego vehicle with more comprehensive, and robust decision making in dynamic environments. By
precise detections of surrounding objects and richer insights integrating cooperative perception with trajectory predic-
for future prediction from various perspectives. tion, our work marks a pioneering effort in the realm of
connected and automated vehicles, which enables CAVs to
E. Qualitative Results
share and fuse data from LiDAR point clouds to improve
We provide the visualizations of the predicted vehicle object detection, tracking, and motion prediction. Specifi-
trajectories in two distinct scenarios in Fig. 3 to illustrate cally, our contributions include a latency-robust cooperative
theeffectivenessofcooperativeprediction.Fig.3(a)and3(b) prediction pipeline, communication bandwidth analysis, and
depict the same scenario involving two CAVs. It is evident a cooperative aggregation mechanism for motion prediction,
that cooperative prediction significantly reduces the number which advance CAV performance and set a benchmark for
of non-CAV vehicles that are overlooked, which highlights future research. Our pipeline does not yield a fully end-to-
the enhanced sensing capability brought by cooperations, end approach due to the non-differentiable tracker. Future
)m(
6EDAnimwork will focus on developing a fully differentiable pipeline [19] M.Toyungyernsub,E.Yel,J.Li,andM.J.Kochenderfer,“Dynamics-
with more advanced architectures for a seamless cooperative awarespatiotemporaloccupancypredictioninurbanenvironments,”
in 2022 IEEE/RSJ International Conference on Intelligent Robots
system. We will also investigate multi-modal sensor fusion
andSystems(IROS),IEEE,2022,pp.10836–10841.
with heterogeneous CAVs to improve flexibility. [20] J. Li, H. Ma, Z. Zhang, J. Li, and M. Tomizuka, “Spatio-temporal
graphdual-attentionnetworkformulti-agentpredictionandtracking,”
REFERENCES IEEE Transactions on Intelligent Transportation Systems, vol. 23,
no.8,pp.10556–10569,2021.
[1] H. Qiu, F. Ahmad, F. Bai, M. Gruteser, and R. Govindan, “Avr:
[21] B. Varadarajan, A. Hefny, A. Srivastava, et al., “Multipath++: Ef-
Augmented vehicular reality,” in Proceedings of the 16th Annual
ficient information fusion and trajectory aggregation for behavior
InternationalConferenceonMobileSystems,Applications,andSer-
prediction,”inInternationalConferenceonRoboticsandAutomation
vices(Mobisys),ser.MobiSys’18,Munich,Germany:ACM,Jan.1,
(ICRA),2022.
2018,pp.81–95,published.
[22] H. Girase, H. Gang, S. Malla, et al., “Loki: Long term and key
[2] H.Qiu,P.Huang,N.Asavisanu,X.Liu,K.Psounis,andR.Govin-
intentionsfortrajectoryprediction,”inProceedingsoftheIEEE/CVF
dan, “Autocast: Scalable infrastructure-less cooperative perception
InternationalConferenceonComputerVision,2021,pp.9803–9812.
for distributed collaborative driving,” in Proceedings of the 20th
[23] C. Choi, J. H. Choi, J. Li, and S. Malla, “Shared cross-modal
Annual International Conference on Mobile Systems, Applications,
trajectorypredictionforautonomousdriving,”inProceedingsofthe
andServices,2022.
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
[3] T. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and
2021,pp.244–253.
R. Urtasun, “V2vnet: Vehicle-to-vehicle communication for joint
[24] Q.Sun,X.Huang,J.Gu,B.C.Williams,andH.Zhao,“M2I:from
perception and prediction,” in European Conference on Computer
factored marginal trajectory prediction to interactive prediction,” in
Vision,Springer,2020,pp.605–621.
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
[4] R.Xu,H.Xiang,X.Xia,X.Han,J.Li,andJ.Ma,“Opv2v:Anopen
IEEE,2022,pp.6533–6542.
benchmark dataset and fusion pipeline for perception with vehicle-
[25] B.Lange,J.Li,andM.J.Kochenderfer,“Sceneinformer:Anchor-
to-vehiclecommunication,”inInternationalConferenceonRobotics
based occlusion inference and trajectory prediction in partially ob-
andAutomation(ICRA),2022.
servableenvironments,”inInternationalConferenceonRoboticsand
[5] R. Xu, Z. Tu, H. Xiang, W. Shao, B. Zhou, and J. Ma, “Cobevt:
Automation(ICRA),2024.
Cooperativebird’seyeviewsemanticsegmentationwithsparsetrans-
[26] V.M.Dax,J.Li,E.Sachdeva,N.Agarwal,andM.J.Kochenderfer,
formers,”inConferenceonRobotLearning,2022,pp.989–1000.
“Disentangled neural relational inference for interpretable motion
[6] R. Xu, H. Xiang, Z. Tu, X. Xia, M. Yang, and J. Ma, “V2x-
prediction,”IEEERoboticsandAutomationLetters,2023.
vit: Vehicle-to-everything cooperative perception with vision trans-
[27] H. Ruan, H. Yu, W. Yang, S. Fan, Y. Tang, and Z. Nie, “Learning
former,” in European Conference on Computer Vision, Springer,
cooperativetrajectoryrepresentationsformotionforecasting,”arXiv
2022,pp.107–124.
preprintarXiv:2311.00371,2023.
[7] Y.Hu,S.Chen,Y.Zhang,andX.Gu,“Collaborativemotionpredic-
[28] K. Li, Y. Chen, M. Shan, J. Li, S. Worrall, and E. Nebot, “Game
tion via neural motion message passing,” in IEEE/CVF Conference
theory-based simultaneous prediction and planning for autonomous
onComputerVisionandPatternRecognition,2020,pp.6318–6327.
vehiclenavigationincrowdedenvironments,”in2023IEEE26thIn-
[8] D. Choi, J. Yim, M. Baek, and S. Lee, “Machine learning-based
ternationalConferenceonIntelligentTransportationSystems(ITSC),
vehicletrajectorypredictionusingv2vcommunicationsandon-board
IEEE,2023,pp.2977–2984.
sensors,”Electronics,vol.10,p.420,Feb.2021.
[29] Y. Wang and J. Chen, “Equivariant map and agent geometry for
[9] H. Guo, L.-l. Rui, and Z.-p. Gao, “V2v task offloading algo-
autonomousdrivingmotionprediction,”in2023InternationalCon-
rithmwithlstm-basedspatiotemporaltrajectorypredictionmodelin
ferenceonElectrical,ComputerandEnergyTechnologies(ICECET),
svcns,”IEEETransactionsonVehicularTechnology,vol.71,no.10,
IEEE,2023,pp.1–6.
pp.11017–11032,2022.
[30] J.Gu,C.Sun,andH.Zhao,“Densetnt:End-to-endtrajectorypredic-
[10] S. Shi, L. Jiang, D. Dai, and B. Schiele, “Motion transformer with
tion from dense goal sets,” in IEEE/CVF International Conference
global intention localization and local movement refinement,” in
onComputerVision,IEEE,2021,pp.15283–15292.
AdvancesinNeuralInformationProcessingSystems,2022.
[31] J. Ngiam, V. Vasudevan, B. Caine, et al., “Scene transformer: A
[11] S.Shi,L.Jiang,D.Dai,andB.Schiele,“Mtr++:Multi-agentmotion
unified architecture for predicting future trajectories of multiple
prediction with symmetric scene modeling and guided intention
agents,” in International Conference on Learning Representations,
querying,” IEEE Transactions on Pattern Analysis and Machine
2022.
Intelligence,2024.
[32] X. Weng, J. Wang, D. Held, and K. Kitani, “3d multi-object
[12] Y. Wang and J. Chen, “Eqdrive: Efficient equivariant motion fore-
tracking: A baseline and new evaluation metrics,” in IEEE/RSJ
castingwithmulti-modalityforautonomousdriving,”arXivpreprint
International Conference on Intelligent Robots and Systems, IEEE,
arXiv:2310.17540,2023.
2020,pp.10359–10366.
[13] Z.Y.RawashdehandZ.Wang,“Collaborativeautomateddriving:A
[33] A.H.Lang,S.Vora,H.Caesar,L.Zhou,J.Yang,andO.Beijbom,
machine learning-based method to enhance the accuracy of shared
“Pointpillars:Fastencodersforobjectdetectionfrompointclouds,”
information,”inInternationalConferenceonIntelligentTransporta-
inIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tionSystems(ITSC),2018,pp.3961–3966.
tion,2019.
[14] H.Qiu*,J.Cui*,D.Chen,P.Stone,andY.Zhu,“Coopernaut:End-
[34] R.Q.Charles,H.Su,M.Kaichun,andL.J.Guibas,“Pointnet:Deep
to-enddrivingwithcooperativeperceptionfornetworkedvehicles,”
learning on point sets for 3d classification and segmentation,” in
in Proceedings of the IEEE/CVF Conference on Computer Vision
2017IEEEConferenceonComputerVisionandPatternRecognition
andPatternRecognition,2022.
(CVPR),2017.
[15] D. Qiao and F. H. Zulkernine, “Adaptive feature fusion for co-
[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., “An image is
operative perception using lidar point clouds,” in IEEE/CVF Win-
worth 16x16 words: Transformers for image recognition at scale,”
ter Conference on Applications of Computer Vision, IEEE, 2023,
inInternationalConferenceonLearningRepresentations,2020.
pp.1186–1195.
[36] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss
[16] H. Xiang, R. Xu, and J. Ma, “Hm-vit: Hetero-modal vehicle-
for dense object detection,” in IEEE International Conference on
to-vehicle cooperative perception with vision transformer,” in
ComputerVision(ICCV),2017.
IEEE/CVF International Conference on Computer Vision, IEEE,
[37] R. Xu, W. Chen, H. Xiang, X. Xia, L. Liu, and J. Ma, “Model-
2023,pp.284–295.
agnostic multi-agent perception framework,” in IEEE International
[17] J.Li,F.Yang,M.Tomizuka,andC.Choi,“Evolvegraph:Multi-agent
Conference on Robotics and Automation, IEEE, 2023, pp. 1471–
trajectorypredictionwithdynamicrelationalreasoning,”inAdvances
1478.
inNeuralInformationProcessingSystems,2020.
[38] I.LoshchilovandF.Hutter,“Decoupledweightdecayregularization,”
[18] J. Gao, C. Sun, H. Zhao, et al., “Vectornet: Encoding HD maps
inInternationalConferenceonLearningRepresentations,2018.
and agent dynamics from vectorized representation,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020,
pp.11522–11530.