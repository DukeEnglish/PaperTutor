ConvoFusion: Multi-Modal Conversational Diffusion
for Co-Speech Gesture Synthesis
MuhammadHamzaMughal1,2 RishabhDabral1 IkhsanulHabibie1 LuciaDonatelli3
MarcHabermann1 ChristianTheobalt1,2
1MaxPlanckInstituteforInformatics,SIC 2SaarlandUniversity 3VrijeUniversiteitAmsterdam
Co-Speech Gesture Synthesis with Word-level Control Dyadic Interaction Synthesis
well-off in England would eat french food and what we now think
Five-Person Interactive Dataset
Figure1. Our CONVOFUSION approachgeneratesbodyandhandgesturesinmonadicanddyadicsettings, whilealsooffering
advancedcontrolovertextualandauditorymodalitiesinspeech. Lastly,weintroducetheDNDGROUPGESTUREdataset,show-
casingrichinteractionswithco-speechgesturesbetweenfiveparticipants.MotionsrenderedusingASH[57].
Abstract Our method is versatile in that it can be trained either for
generating monologue gestures or even the conversational
Gestures play a key role in human communication. Re- gestures. To further advance the research on multi-party
centmethodsforco-speechgesturegeneration,whileman- interactive gestures, the DND GROUP GESTURE dataset
aging to generate beat-aligned motions, struggle generat- is released, which contains 6 hours of gesture data show-
ing gestures that are semantically aligned with the utter- ing5peopleinteractingwithoneanother. Wecompareour
ance. Compared to beat gestures that align naturally to method with several recent works and demonstrate effec-
the audio signal, semantically coherent gestures require tiveness of our method on a variety of tasks. We urge the
modeling the complex interactions between the language readertowatchoursupplementaryvideoatourwebsite.
and human motion, and can be controlled by focusing on
1.Introduction
certain words. Therefore, we present CONVOFUSION , a
diffusion-basedapproachformulti-modalgesturesynthesis, Gesturesareoneofthefundamentalwaysofexpressionand
whichcannotonlygenerategesturesbasedonmulti-modal can significantly enhance the interpretation of the verbally
speechinputs,butcanalsofacilitatecontrollabilityinges- communicated utterance [32]. As our society integrates
ture synthesis. Our method proposes two guidance objec- multi-billionparameterlarge-language-model(LLMs)[69,
tivesthatallowtheuserstomodulatetheimpactofdifferent 83]intoourworkflowsanddailylives,itisonlynaturalto
conditioning modalities (e.g. audio vs text) as well as to consider ways to augment the LLM based on spoken lan-
choose certain words to be emphasized during gesturing. guage alone with non-verbal information essential to in-
4202
raM
62
]VC.sc[
1v63971.3042:viXraterpreting such language. Towards this goal, speech and Forward Diffusion
text-basedgesturegenerationapproacheshavecomealong
way from symbolically representing gestures [8, 9] in a x z(0) z(T)
rule-basedgenerationframework[35]tothestate-of-the-art Reverse Diffusion
methodstrainedonhumanmotioncapturedata[4,82,84].
Yet,whilethemajorityofmethodssuccessfullycapture
Modality Multimodal
beatgesturesthatareprosodicallyalignedwithspeech,they x z(0) Guidance Cross Attention z(T)
lacklanguage-basedcontroloverthegesturegenerationand Word-Excitation
Guidance
therefore, struggle to generate precise semantic gestures
thatcontributetotheoverallmeaningofanutterance. This
canbeattributedtothefactthatthemotionofbeatgestures
Text Speaker
istemporallywell-alignedwiththespeechsignalsandgen- Style
erally follows a similar spatial pattern for all speakers and
content,therefore,itiseasiertomodelusinglearningtech- Figure 2. Overview of the proposed approach. We generate
niques. Ontheotherhand,semanticcoherencehasamore gesturesconditionedonmultipleconditioningsignalssuchastext,
complex temporal interplay with the words, their meaning audio,speakerstyle,etc.usingalatentdiffusionapproach.During
andwhotheindividualspeakeris. inference, we introduce modality guidance and word-excitation
guidancetocontrolthepropertiesofthegeneratedgestures.
Inthiswork,weproposeCONVOFUSION–anovelcon-
trollablegesturesynthesismethodtogeneratenotonlyco-
speech gestures, but also reactive (and passive) gestures.
GROUP GESTUREdatasetallowsustoproposeanovelap-
We follow a latent diffusion approach [13, 62], which has
proachtogenerategesturesinadyadicsetting.
the benefit of learning a jitter-free motion representation.
Insummary,ourtechnicalcontributionsareasfollows:
Unlikeexistinglatentdiffusionmethods[13],wedesignour
• We propose CONVOFUSION – a diffusion-based ap-
motion latents to be time-aware, thus allowing us to learn
proachformonadicanddyadicgesturesynthesis. Wedo
temporal correlations between motion and speech along
sonotonlyintheco-speechsettingbutcanalsogenerate
withtheabilitytoperformperpetualgesturesynthesis.
passive/reactivegestures.
Our synthesis model supports a variety of input signals
• Thankstotheproposedcoarseandfine-grainedguidance,
(textandaudioofthespeakersintheconversation)andpro-
our work investigates ways to incorporate a variety of
videsaframeworktocontrolthem. Toenablecontrollable
multi-modalsignalsandprovidesaframeworktocontrol
multi-modal inference of our model, we introduce a novel
theirinfluenceinthegeneratedgestures.
classifier-freeguidancetrainingstrategy. Morespecifically,
• Wedemonstratehowgeneratinggesturesintheproposed
insteadofdroppingtheentiremulti-modalconditioningsig-
latent mitigates the jittering artifacts prevalent in the
nal, weshowthatselectivelyreplacingthemodalitieswith
hand-articulations of existing datasets. Unlike existing
null-vectors facilitates test-time control over each modal-
motion latent diffusion works [13], the proposed time-
ity. Finally, CONVOFUSION also allows us to enhance the
awarelatentrepresentationallowsustoperformperpetual
micro-gestures associated with a particular word, thanks
gesturesynthesiswithhighsynthesisquality.
to the fine-grained textual guidance. Having the test-time
• This work also introduces the DND GROUP GESTURE
modality control and word-level textual guidance provides
dataset,therebyfacilitatingfutureresearchondyadicand
ustheuniqueabilitytohavecoarseandfinecontrolofthe
groupgesturesynthesis.
generated motions; a feature missing in existing gesture
synthesisworks[4,26,75]. 2.RelatedWorks
One of the goals of our framework is to model the ges-
tures exhibited in a conversational setting. Unfortunately, Asourworkdrawsinspirationfromtheextensiveliterature
most existing datasets only contain monologues, as in the on gesture synthesis and recent works on diffusion-based
TED [77] and SHOW [75] datasets. Even the datasets generativemodels,wediscussrelevantliteraturefromthese
recordedinconversationalsetting[47]provideannotations twoperspectivesinthissection.
only for one person. To address this, we introduce the
2.1.Co-SpeechGestureSynthesis
DND GROUP GESTURE dataset. It involves five partici-
pants playing multiple sessions of Dungeons and Dragons Co-speechgesturesareauniqueformofgesture, inwhich
– a popular role-playing game. The dataset comes with handandarmmovementsusedtocommunicateinformation
highqualityfull-bodymotioncaptureofalltheparticipants, are temporally synchronized and semantically integrated
along with multi-channel audio recordings and text tran- with speech [52]. While such gestures are thought to con-
scriptions. Thanks to around 6 hours of capture, the DND tributetomeaninganddiscourseinthesamewayaslexical
redocnE
Decoderitemsandintonationpatterns, theirmulti-functionalnature inastudiosetting.Becauseoftheselimitations,earlyworks
makes automatic generation challenging. Non-referential typicallyinvolveasinglespeaker[17,18].Tocollectalarge
beat gestures align with prosodically stressed words and numberoftrainingsamples,severalworkshaveproposedto
contribute less to overall semantic meaning [32, 54]; such leveragemonocular3Destimationapproachestoobtainthe
gestureshaveprovedeasiertogenerate[45]. Semanticges- 3D body, face, and hand keypoints [1, 20, 23, 26, 75, 77].
tures categorized as iconic, metaphoric, or deictic visually Unfortunately, such monocular estimation results are sub-
illustrate some aspect of the spoken utterance yet are less parcomparedtothestandardmulti-viewmocapapproaches
patternedbetweenspeakersandcontent; thesegesturesare andareunsuitableformulti-speakersettings.
morechallengingtoeffectivelyreproduce[37]. Toaddressthelackoflarge-high-qualitydata,[47]pro-
Early works in the field of co-speech gesture synthe- posedBEAT,a76-hourmocap-basedspeechgesturedataset
sis can be divided into rule-based and data-driven tech- recorded from 30 different subjects. Unlike BEAT which
niques. Rule-based methods [10, 11], which usually uti- focusedonasinglespeaker,[40]introducedahigh-quality
lizeheuristics,generategesturecombinationswithhighse- speech gesture dataset that involved multiple speakers, but
mantic alignment to speech. [72] provides a comprehen- was limited to two-person conversations. In contrast to
siveoverviewofthesemethods. However,theyproduceun- previous works, we propose a high-quality speech-gesture
natural and less diverse gesture outputs. To mitigate this dataset involving 5 subjects within a conversation. In
problem, early statistical approaches [41, 42] try to model addition, different from most mocap-based datasets that
theunderlyinggesturedistributionusingdataandthenpre- use marker-based mocap technologies, we employ a state-
dictgesturesthataremostappropriateforgivenspeechin- of-the-art markerless mocap system to accurately capture
put. However, both rule-based systems and early statisti- the 3D body and hands of multiple speakers without be-
calapproachespredictgesturesequenceintermsofknown ing restricted by body mocap suits. Tab. 1 provides a
gesticulation units, which makes the final output look un- briefoverviewofsomenotabledatasetsandtheirqualities.
naturalandchoppy. Therefore,recentdata-drivenlearning- Moreover, we also compare them with the DND GROUP
basedmethods[4–6,26,36]employneuralnetworkstomap GESTUREdatasetwepresentinthiswork.
speech input to a gesture sequence, which allows for per-
framegestureprediction, providinganend-to-endsolution 2.3.Diffusion-basedGenerativeModelling
for speech-to-gesture synthesis. [56] provides an in-depth
Diffusion models [30, 66] have demonstrated remarkable
overviewofclassicalandrecentdata-drivenmethods.
potential in the field of generative modeling, consistently
Earlier deep-learning-based methods which used
delivering impressive results in various synthesis applica-
CNN [25], RNNs [49, 76, 78] and transformers [6] em-
tions[14,34,60,63,67,70,79]. Newparadigmslikeguid-
ployeddeterministicapproachestopredictgesturesforthe
ancemechanisms[15,29]andlatentdiffusionmodels[61]
speechinput. Ontheotherhand, generativemethodsoffer
have been introduced to enhance quality and alignment of
abetteralternativesincetheycanintroducestochasticityin
diffusion-basedsynthesisw.r.tgivenconditionings.
thegenerationprocesswhichleadstodiverseoutputs. Gen-
This approach has been extensively applied for condi-
erative modeling approaches [2, 19, 27, 43, 48, 75] have
tional human motion synthesis [13, 14, 68, 70]. Simi-
been used for synthesis resulting in human-like gestures.
larly, co-speech gesture generation has also greatly bene-
But, they also suffer from low semantic relation with the
fited from this generative modeling technique. DiffGes-
speech input because there exists many-to-many relations
ture [84] uses a transformer-based diffusion pipeline with
between speech and gestures and it becomes hard for the
an annealed noise sampling strategy for temporally con-
generative approaches to realize which gesture is more
sistent gesture generation. GestureDiffuCLIP [4] employs
semanticallyaccurate correspondingtothe speech. There-
latent-diffusionmodels[61]andCLIP[58]basedcondition-
fore, recent approaches [3, 4, 38, 45, 46] try to improve
ing to improve control over co-speech gesture generation.
intent’s alignment with gesture prediction. Gesture styles
[67] presents a model to predict the movement of multi-
arealsoincorporatedinthegesturegenerationpipelinefor
plespeakersinasocialsetting. However,contrarytoother
personalizedgesturesynthesis[19,74].
diffusion-based gesture synthesis approaches, their model
focuses on predicting the correctness of the 3D body key-
2.2.SpeechGestureDatasets
point trajectory for a few seconds in the future instead of
Astheperformanceoflearning-basedmethodsreliesonthe improving the speech-gesture alignment. Instead of sim-
quality of its training data, a number of gesture synthe- plypredictingthemotiontrajectory,ourmethodproposesa
sis datasets have been proposed by the community. How- multi-person speech-driven 3D gesture synthesis approach
ever, high-quality speech-driven gesture synthesis datasets that can be used to predict the 3D reactive body and hand
aretypicallyexpensiveandtedioustocollectastheyrequire motionbetweenvariousspeakersandlistenerswithinacon-
hoursofspeechgesturemotioncapture(mocap)recordings versation.Figure3. Themodelschema. Givenatrainingmotionx ∈ RN×J×3,wefirstextractitslatentencodingˆz(0) (Sec.3.1),whichisthen
denoisedbyanetworkthatincorporatesthevariousmodalitiesinthedenoisingprocess.Atinferencetime,thedenoisedlatentsaredecoded
to produce the final generation, xˆ (Sec. 3.2). During this process, our method allows to control the generation through coarse-grained
modalityguidanceorfine-grainedword-excitationguidance(Sec.3.3).Dottedlinesrepresentcomponentsusedonlyduringinference.
3.Approach separate encoders. 2) Instead of projecting the entire mo-
tion into one single latent vector, we encode motion into
Thegoalofourmethodistogenerateco-speechgesturese-
chunkedlatentsthatcanbedecodedjointlybyadecoder.
quencesformonadicanddyadicsettingsincorrespondence
Decoupled Latent Representations. The articulation of
withinputspeech. Agesturesequencex ∈ RN×J×3 con-
the finger joints is critical to the quality of gesture synthe-
sistsofN framesofhumanmotionwithJ articulating3D
sis. However, the fingers articulate in a significantly dif-
joints. Thegeneratedgesturemotionoughttobeconsistent
ferentspaceandscalecomparedtotherestofthebodyand
with the multi-modal conditioning signal, C, representing
na¨ıvelyencodingthefull-bodygesturesresultsininaccurate
the speech and identity-related attributes of the persons in
reconstruction of hands. We therefore follow prior works
conversation(discussedlaterinSec.3.2).
that decouple the two sets of joints [21, 22] and represent
Wedesignourgesturesynthesismethodaroundalatent themotionxasalatentvectorz={z ,z },wherez ∈Rd
b h b
denoising diffusion probabilistic model (DDPM) frame- andz ∈ Rd areseparateencodingsofthebodyandhand
h
work [62]. The proposed diffusion model is trained to de- motion.
noise the latent representation of the gesture motions (re- The latent vectors are learned using a VAE framework.
fer to Sec. 3.1). The generated motion latents can later Thehandandbodymotions,x andx ,areencodedusing
h b
be decoded using a motion decoder. Unlike existing mo- transformerencoders: z = ξ (x ), z = ξ (x ). Thela-
b b b h h h
tion latent diffusion methods [13], we design our latent tent vectors represent the mean of the distribution, which
spaceinatime-decomposablemanner,therebyallowingus can be sampled using the reparameterization trick [33]
tolearnfine-grainedinterplaybetweenmotionandspeech. and fed into a decoder to reconstruct the motion x′ =
b
Crucially, our method also allows the end-user to control D (z ),x′ = D (z ). We train the VAE with the stan-
b b h h h
the attributes of the generated gestures at inference time dard reconstruction loss, L , Bone-length regularization
2
(see Sec. 3.3). We now discuss each component in detail. lossL [14]andtheKL-Divergenceofthelatents,L .
bone KL
Refertothesupplementaldocumentforaglossaryofmajor Additionally,wereducethejitterinreconstructionpropos-
notationsusedinthemethodexplanation. ingaLaplacianregularizationterm:
3.1.Scale-awareTemporalLatentRepresentation L =∥L{xˆ}−L{x}∥ (1)
lap 2
Insteadofdirectlydenoisingtherawmotionx,ourdiffusion where L{·} is the Laplace transform operator along N
modeloperatesinthelatentspaceofhumanmotion. Thus, frames. RefertoSec.5.4andsupplementalforanalysis.
weproposetolearnsuchalatentspacewithtwocharacter- Time-Aware Latent Representation. The motion latents
istics: 1)Wedisentanglethefingermotionsfromtherestof learned by the VAE represent a large motion sequence
bodymotionsbyencodingthemintoalatentspacethrough (>100 frames) with a single d-dimensional vector. This,rathercoarse,granularityprohibitsapplicationssuchasper- conditionedinprimarilytwosettings: monadicanddyadic.
petualrolloutwherethemotioncanbeautoregressivelyde- Themonadicsettingreferstotheco-speechgesturegener-
coded with an overlapping window. To enable such appli- ationbasedsolelyonthespeaker’sownutteranceandtypi-
cations,weproposetoencodeshortermotionchunksinthe callyoccursinmonologuescenarios. Forthis,werepresent
latentzbutdecodemultiplesuchchunkedlatents,{ˆz }M theconditioningsignalasC = {a,τ,s},consistingofthe
i i=1
togetherwithasingledecoder,asshowninFig.4. audio signal a ∈ RNa×d and the text tokens τ ∈ RNτ×d,
as well as s ∈ R1×d representing the speaker identity to-
ken. Generally, N corresponds to the number of audio
a
frames,N correspondstothenumberoftexttokensinthe
τ
2 x d
N/M x J x 3 utterance. Speaker identity s can enable applications like
stylizedgesturesynthesiswhichcangeneralizetodifferent
. . . Chun pk ar i tn s to M N/M x . . . J x 3 . . . 2 x d Concat . . . g coes nt vu er re sas tt iy ol nes s. cF eo nr at rh ioe sd —ya thd eic gs ee ntt ei rn ag te— dw geh sic tuh reta sk mes up sl ta bc eei in n
accordance with the co-participant’s utterance as well. In
Key
M x 2 x d this case, we have C = {a,τ,τ′,s,m}, where τ′ refers
x N/M x J x 3 2 x d z x to the co-participant’s speech content i.e. their text. Here,
N x J x 3 N x J x 3 wecanalsochoosetheiraudioinsteadoftheirtextaswell.
Queries
N/M x J x 3 2 x d N x 3J Finally, m ∈ {0,1}M indicateswhetherthespeakerisac-
tivelyrespondingwithspeech,orpassivelyback-channeling
Figure4.Chunkedlatentencoding-decoding.Weencodeamo- e.g. bylaughingornodding(seealsosupplementalvideo).
tionofN framesintoasequenceofM latentvectors,whichare
We use a transformer decoder network [71] with multi-
jointlydecodedbythedecoderD. Encodingintochunkedlatents
head attention to approximate the denoising function pro-
allowsforperpetualrolloutanddecodingjointlyinducestemporal
ducing ϵ (ˆz(t),t,C). This allows us to elegantly inte-
consistencywhileconvertingthelatentsbackintomotion. θ
gratemultiplemodalitiesinCwithseparatecross-attention
Given a gesture sequence x, we first split the se-
heads, as shown in Fig. 2. Let us consider the case of the
quence into M equally sized chunks {x′}M , where x′ ∈
i i=1 i audiosignal, a. Thecross-attentionfeatures, ϕ , arecom-
RN/M×J×3. Next,eachofthechunksx′ isencodediniso- a
i puted using the attention matrix Attn(ˆz,a) ∈ RNa×M as:
lation using ˆz = ξ (x′). However, while decoding, the
i b i
decodercollectivelydecodesasequenceofchunkedlatents, Q K
ˆz={ˆz }M ,following:xˆ′ =D(ˆz).Insummary,ourlatent Attn(ˆz,a)=σ( √z a),ϕ a =V a·Attn(ˆz,a) (3)
i i=1 d
encodingstransformamotionsequencex ∈ RN×J×3 into
latent representationsˆz ∈ RM×2×d. This can enable per- whereσ isthesoftmaxoperator,Q z,K a,V a arethequery,
keyandvaluevectorsrecoveredformthemotionlatentfea-
petual gesture generation using diffusion inpainting tech-
turesˆzandtheaudiofeaturesa. Wesimilarlyrecovertext
nique[51]aswediscussandanalyzeinSec.5.4.
features,ϕ =Attn(ˆz,τ),alsoforthetext.
τ
3.2.Modality-ConditionalGestureGeneration
3.3.TowardsControllableGestureGeneration
Havingobtainedˆzasthetime-awarelatentrepresentationof
gesturemotions,weformulatethegesturesynthesistaskas In addition to multi-modal gesture synthesis, our method
thatofconditionallatentdiffusion[62]. Theforwarddiffu- is designed to allow coarse and fine-grained control. For
sionprocess,successivelycorruptsthelatentsequenceˆz(0) coarse control, one can adjust the impact of a specific
by adding Gaussian noise ϵ for T timesteps with the as- modalityonthegeneratedmotionbyutilizingourmodality-
sumptionthatˆz(T) ∼ N(0,I). Forgeneration,thereverse level guidance strategy. For fine control, the user can
diffusionprocessisperformedonˆz(T)byiterativelydenois- choosespecificwordstoenhancethegesturesforthewords
ingˆz(T) ∼ N(0,I)togeneratealatentsequenceˆz(0),and usingtheproposedword-excitationguidance(WEG)objec-
canbeformulatedas tive.
Modality-Guidance. Classifier-free guidance [29] has
T
p
(cid:0) ˆz(0:T)(cid:1) =p(cid:0) ˆz(T)(cid:1)(cid:89)
p
(cid:0) ˆz(t−1)|ˆz(t)(cid:1)
, (2)
been used to improve the generation quality of various
θ θ
diffusion-basedmotionandgesturegenerationmethods[4,
t=1
where p (ˆz(t−1)|ˆz(t)) is approximated using a neural 13, 39, 68]. Typically, this is done by randomly replacing
θ
network parameterized by weights θ. This neural network theconditioningvectorswithanull-embeddingC←∅. At
f is trained to predict noise ϵ (ˆz(t),t) [30], which can be inference, the noise predictions are blended at each diffu-
θ θ
usedinthetrainingobjectiveL =||ϵ−ϵ (ˆz(t),t)||2. siontimestampttogetthenoisepredictionϵ(t):
d θ θ
Themotiongenerationframeworkdiscussedaboveisso
far unconditional. Our gesture synthesis approach can be ϵ(t) =ϵ (ˆz(t),t,∅)+λ(ϵ (ˆz(t),t,C)−ϵ (ˆz(t),t,∅)) (4)
θ θ θ θwhere, λ represents the guidance scale. Once estimated, Multi-party #Interacting
Name #Identities Size BodyParts Interaction Speakers
ϵ(t)canbeusedtosampleˆz(t−1)forthenextiterationusing IEMOCAP[7] 10 12h Face (cid:33) 2
θ Creative-IT[55] 16 2h Body† (cid:33) 2
Eq. 11 of [30]. However, recall that our conditioning set CMUHagglingDataset[31] 122 3h Face,Body,Hands (cid:33) 3
TEDDataset[77] 1295 52.7h UpperBody
C = {a,τ,τ′,s,m} consists of several modality-specific SpeechGesture3D[26] 10 144h UpperBody,Hands,Face
TalkingwithHands[40] 50 50h Body,Hands (cid:33) 2
conditions. Na¨ıvelysettingalltheelementsto∅forrandom PATS[1] 25 250h UpperBody,Hands
iterationsprohibitsseparatelylearningtheeffectofeachin- SaGA++[37] 25 4h Bodyhands
ZeroEGGSDataset[20] 1 2h Body,Hands
dividualmodalitywithinContheconditionaldistribution. BEAT[47] 30 76h Body,Hands,Face
DNDGROUPGESTURE 5 6h Body,Hands (cid:33) 5
Instead,wetrainourmodelwithrandommodalitydropouts
(with null-embedding replacement) with 10% drop proba- Table1. Comparisonofcurrentlyavailabledatasetstoour DND
bility. Thisencouragesthemodeltolearnseveralcombina- GROUP GESTURE dataset. Body parts refer to the parts where
tionsofmarginalizedconditionalprobabilitydistributions. the2Dor3Dposetrackingisavailable. †indicatesthatthebody
trackingisonlyavailableforoneofoneinteractingactors.
Atinference,wesamplewithmodality-guidance:
ϵ(t) =ϵ∅+λ (cid:88) w (cid:0) ϵc(ˆz(t),t,c)−ϵ (ˆz(t),t,∅)(cid:1) (5)
θ θ m c θ θ speaker interactions. We based our dataset recordings on
c∈C
D&Dtabletoproleplayinggame,wherefivedifferentplay-
wherethescaleparameters, w c ≥ 0, determinethecontri- ers are standing in a circle around a game map. Each par-
butionofeachmodalitytowardsthegeneratedgestureand ticipant is equipped with a dedicated wireless microphone
λ m is the global guidance scale. Adjusting the modality toensureacleanaudiorecordingandaudiosourcesepara-
scale, w c allows us to coarsely control the gesture quality tion. The setup of the gameplay involves various types of
andalsoanalyzethesensitivityofthegenerationprocessto interaction between the actors that often require semanti-
specificmodalities. Note, thatthisisanoptionalsampling callymeaningfulgesturessuchaspointingtoacertainloca-
strategyrequiredonlyformodality-levelcontrol. tiononthemap. Intotal,thedatasetconsistsof4separate
Word-Excitation Guidance. Inspired by the controllable recordingsessionswithatotaldurationof6hours.
image generation methods [12, 16], we propose a word- Ourproposeddatasetisrecordedusingastate-of-the-art
level guidance mechanism that allows us to finely control multi-view markerless mocap to obtain accurate 3D body
thegesturegenerationbasedonauser-definedsetofwords andhandposeestimatesofmultiplesubjectsatagiventime.
duringthesamplingprocess. This allows our participants to move freely without being
LetAttn(ˆz(t),t,τ) ∈ RNτ×M bethetextattentionma- obstructed by the tight mocap suit or gloves. In addition
trixatthetthiterationofthedenoisingprocess. Forasetof toaudioandthe3Dposeannotations,wealsoprovidetext
texttokens{τ i}S i=1,selectedbyauserwiththeintentionof andgestureannotationsforeachindividualspeakerthatdis-
gesture enhancement, we focus on the corresponding col- tinguishesdifferenttypesofobservablegestures,including
umn,A i ∈ RM inthetextattentionmatrix. Now,withthe beats, iconic, deictic, and metaphoric. Our dataset will be
assumptionthattheelementwithmaximumattentioninA i madepubliclyavailabletothecommunity.
aligns with the motion chunk associated with the text, we
introduce a guidance objective to further enhance (or, ex- 5.Experiments
cite)thesameattention:
Ourmethod,initsvanillaform,isdesignedtogeneratehu-
S
1 (cid:88) mangesturesfromspeech,yetitgoesseveralstepsbeyond
G = (1−max(A )) (6)
exc S i this task. For instance, we adapt our method to perform
i=1
dyadicconversations. Moreimportantly,weshowhowdif-
Next, we use the gradient of G exc w.r.t the latent ˆz(t) to ferent modalities contribute to the generation and perform
performtheword-excitationguidance: fine-grained text-based control. Naturally, it is difficult to
find suitable baselines to compare with. To perform fair
˜z(t) ←ˆz(t)−α·∇ G ,ϵ(t) =f (˜z(t),t,C) (7)
ˆz(t) exc θ θ evaluations, we, therefore, compare with methods that can
be trivially adapted to our setting. Specifically, we com-
whereαistheguidancescaleforthewordexcitationguid-
pare with MLD [13] (a generic latent diffusion method),
ance,whichalsoservesasastepsizeforlatentupdate.
CaMN [47], Multi-Context [78], DiffGesture [84] (specif-
ically monadic gesture baselines) and DiffuGesture [81]
4.Dataset
(two-personmotionsynthesisworks).Notably,CaMN[47],
To enable a high-quality, speech-driven gesture synthe- DiffGesture[84]andDiffuGesture[81]requireaseedmo-
sis method involving multiple speakers, we introduce the tion sequence to build the gesture generation on. This is
DNDGROUPGESTUREdataset. Ourdatasetisdesignedto differentfromoursettingandprovidesvitalcluesaboutthe
alsoinvokeawiderangeofnon-verbalgesturesduringthe gesture style. We provide the seed motions for the twoFID↓ BeatAlign→ Diversity→ L1Div→ SRGR↑ Monadic Synthesis Dyadic Synthesis Semantic Alignment
GT - 0.89 13.21 13.12 - 100 85.5686.05 90.4491.21 100 5 3.53
Multi-Context[78] ≥103 0.8 26.71 43.31 0.140 69.13 66.85 4 2.97 DiffGesture[84] ≥103 0.96 176 17.8 0.003 80 47.33 48.69 80 2.71
3
CaMN[47] 142 0.74 9.66 5.85 0.443 60 60 33.79 38.92
MLD[13] 475 0.76 16.98 5.42 0.214 40 40 2
Ours 271 0.82 9.82 6.24 0.365 20 20 1
0 Speech Alignment Naturalness 0Reaction Plausibility Naturalness 0 w/ WEG w/o WEG GT
Table2. ComparisonontheBEAT[47]. Twomethods[78,84] vs. GT vs. CaMN vs. MLD vs. GT vs. MLD
produceextremelyjitterymotions. Wedemonstratesuperiorbeat
alignmentanddiversityscoresamongtheremainingmethods. Figure5.Resultsoftheuserstudy.WecomparewithCaMN[47]
and MLD [13], and achieve an overall favourable preference
scoresformonadicanddyadicsettings. Wealsoevaluatetheef-
methods, but do not use the seed motions to generate our fectivenessoftheword-excitationguidance(WEG).
results. The methods are compared using the established
motionsynthesismetricsaswellasauser-study.
BeatAlign→ Diversity→ L1Div→ Likewise,
Evaluation Datasets. We evaluate our performance in
GT 0.90 17.7 5.12 DiffuGesture
monadic gesture generation on the recently introduced MLD 0.96 20 0.31
DiffGesture 0.97 2176 1308 was adapted to
BEAT dataset [47]. The test set includes 2492 5-sec mo-
Ours 0.90 6.38 1.19 our setting as de-
tionsequencesandincludesasetof5unseenspeakers. For
tailedin[81]. We
evaluatingthemotionindyadicsetting, weusethetestset Table 3. Qualitative comparison of
observe similar
of the proposed DND GROUP GESTURE dataset. The test dyadicmotionsynthesisonthe DND
setcontains3932sequencesof5-secondconversations.
GROUPGESTUREdataset. patterns of jittery
motion with
Metrics. Evaluating synthesized motions is challenging
DiffuGesture, whereas MLD produced suboptimal results
due to the subjective nature of perceiving good gestures.
intermsofbeatalignment. Incontrast,weachievesimilar
Yet, we evaluate our method on the established metrics
beat alignment as the ground-truth while also producing
like Beat-Alignment [65], FID, Semantic Relevance Ges-
higherL1Diversity,thusindicatingnon-staticmotions.
ture Recall (SRGR) [47] that evaluate different aspects of
the motion. We also use Diversity and L1 Divergence to 5.3.UserStudy
evaluate the ability of models to span the space of gesture
As noted above, evaluating motion synthesis models on a
motionswithenoughcoverage.
setofnumericalmetricshidesseveralaspectsofthegesture
5.1.MonadicCo-speechGestureSynthesis synthesis. Prior works [14, 70] report mismatch between
metricsandthesubjectiveevaluationsbytheusers. Hence,
We tabulate our results on the BEAT test set for monadic
we perform a perceptual user study to evaluate the quality
co-speechgesturesynthesisinTab.2. WeobservethatDif-
of our synthesis results w.r.t state-of-the-art methods. For
fGesture [84] and Multi-ContextNet [78] struggle with the
evaluatingthemonadicresults,weaimtoevaluatethegen-
FIDwhich, uponvisualization,canbeattributedtotheex-
eralplausibilityofthemotionsandprobethecoherenceof
tremely jittery nature of the generated motions. Interest-
the gestures with the utterance. Likewise for dyadic syn-
ingly, this also leads to Multi-ContextNet [78] to perform
thesis,thegoalistomeasureifparticipant’sgeneratedges-
thebestintheBeatAlignmentmetricasforeverybeatinthe
turesalignwellwiththeirspeechaswellasco-participant’s
audio,thereisalwaysajitterymotiontoalignwith. Among
speech content. To evaluate the word-excitation guidance,
other methods, we observe better performance in terms of
we ask the users to evaluate if the generated gestures have
diversity and beat alignment. It is interesting to note that
distinctgesticulationatthefocuswords.
MLD,whichistrainedonanon-temporallatentrepresenta-
Results. We plot the results of our user study in Fig. 5.
tion,achievesareasonablebeatalignmentbutworseseman-
Forthemonadicsetting,theparticipantspreferredourmo-
ticrecall. Wehypothesizethatthesemanticalignmentben-
tionsoverthoseofCaMNandMLDforbothquestions. At
efits from a finely discretized motion representation. Our
thesametime,weweremarginallybelowtheground-truth
method lies in the middle of the discretization spectrum,
preference. The inference remains similar for the dyadic
where CaMN operates on raw motion frames while MLD
evaluationsaswell, althoughwithsignificantlylowermar-
collapsesthetemporalaxiswithinasinglelatent.
gins. Finally, the user study demonstrates better semantic
alignmentwiththegeneratedmotionswiththeuseofWEG.
5.2.DyadicCo-speechGestureSynthesis
5.4.AblativeAnalysis
We adapted two baselines to the dyadic setting for com-
parison. MLD’s architecture was extended by adding ad- Latent Representation. Our chunked, scale-aware latent
ditionalconditioningblocksoftheco-participant’sspeech. representationismotivatedbyvariousfactors,suchasper-
% ecnereferP
naeM
% ecnereferP
naeM
gnitaR
naeMReconstructionLoss↓ SmoothnessError[64]↓ nalstagesofdenoising,indicatingthatthediffusionprocess
MLD[13] 10×10−3 4.4×10−3
makessmallereditsinthefinalstagesandtakesheavierup-
OurVAE 5×10−3 3.5×10−3
datesduringthemiddlephasesofdenoising. InFig.6a,one
w/oL
lap
3×10−4 3.7×10−3
w/oTimeAware 9×10−3 4×10−3 canobserveasignificantbumpinthejointvelocities(indi-
w/oScaleAware 5.5×10−3 3.7×10−3 catingmoreanimatedbehaviour)attheprecisemomentof
theexcitationword. Theseobservationshighlighttheover-
Table 4. Ablation study on the VAE design. L ensures the
lap all effectiveness of our two-level guidance objectives. We
motionsretainthevelocityofgroundtruth,eventhoughremoving
referthereadertothesupplementalformoreresults.
itleadstolowerreconstructionloss. Whiletrainingwithouttime-
OnSemanticConsistency: ThankstotheproposedWord
awarerepresentationgivesslightincreaseinreconstructionloss,it
ExcitationGuidance(WEG),ourmethodsamplesgestures
cannotsupportunboundedgeneration.
that produce more pronounced attention features for the
user-selectedwordsWedemonstratethisbytrainingages-
Effect of Word-level Semantic Guidance
0.3 turetypeclassifiertorecognizebeatandsemanticgestures.
ForsynthesizedgestureswithoutWEG,weobservethatthe
0.2 recall for semantic labels is 0.34. However, this recall in-
creasedto0.40whenWEGwasemployed, indicatingthat
0.1 theuseofWEGenhancessemanticcoherenceingenerated
gestures. Refertosupplementalforimplementationdetails.
0.0
well-off in england would eat french food and whatwe now think A yst it sen (st eio en sM upa pp les m. W ene tav l)isu toali iz ne tet rh pe rea ttt wen ht aio tn spm aa tip os -tf eo mr pan oa ral-
l
Motion Aligned Text
properties are highlighted in the model training. The first
with word-level guidance GT w/o word-level guidance
property is a clear separation between the hand and body
(a)Fine-grainedalignment
ContributionofeachmodalityinMonadicSynthesis latents,shownbythestripedpatternsoftheattentionmaps. 12.5
Secondly, WEG boosts the attention weights for the high-
10.0 lightedwords. Refertosupplementalfordetailedanalysis.
7.5 Perpetual Rollout. In addition to allowing for temporal
5.0 alignment with several modalities, our chunked latent rep-
2.5 resentation also benefits us by allowing perpetual rollout.
To do so, one can simply follow the auto-regressive de-
t=1000 t=800 t=600 t=400 t=200 t=0
Diffusiontimestept noising process followed by the existing motion diffusion
τ a m s C
methods[14,68,70]withthedifferencethatinsteadofin-
(b)Theeffectofmodalities. paintingtheactualmotion,weinpaintthelatents. Referto
supplementarymaterialforimplementationdetails.
Figure6. (a)Givenatextpromptwithfocuswords,“french”and
“now”,weobservethatWEGsignificantlyincreasesthejointve-
locitiesforthetwowordscomparedtothenon-guidedcase.In(b) 6.Conclusion
weshowthecontributionsofeachmodalityasdiffusiondenoising
In this work, we proposed a novel approach towards con-
progresses.Audiotendstodominatethegenerationprocess.
trollable co-speech gesture synthesis. With the aim of
generating long term, jitter-free gestures, we proposed a
petualmotionsynthesis,bettertemporalalignmentwiththe
time-aware latent representation that can be denoised us-
conditioning modalities, and the scale difference between
ing a diffusion model. To control the effects of individual
thehandsandthefingers. Wetabulatetheinfluenceofthe
modalities, we proposed a variant of classifier-free guid-
threemaindesignchoicesinTab.4. Wealsoshowthaton
ance. We also proposed WEG to enhance the gestures for
theVAEreconstructiontaskalone,ourlatentrepresentation
auser-selectedsetofwordsinthetext,thusfacilitatingtext
outperformsMLD’slatentrepresentation.
level fine-grained control. Our analysis shows that word-
Influence of Modalities. With a variety of conditioning
excitationinducesmoreanimatedbehaviourfortheselected
modalities within our framework, it is natural to question
words. Finally, with the introduction of the DND GROUP
which modalities bear a greater effect on the final genera-
GESTURE datasetwehopethefieldwillfurtherpropelthe
tion. Weanalyzethisbyplottingthenormofthecontribu-
researchonmulti-partygesturesynthesis.
tionsofeachmodalityinEq.(5)(computedbeforescaling
Acknowledgements. This work was supported by
withw c).AsFig.6bdemonstrates,theaudiomodalitybears the ERC Consolidator Grant 4DReply (770784).
thelargestinfluenceonthegesturegenerationprocess. In- We also thank Andrea Boscolo Camiletto & Hem-
terestingly, wenoticeanoveralltrendofincreasingcontri- ing Zhu for help with visualizations and Christo-
butions until they drop down significantly towards the fi- pher Hyek for designing the game for the dataset.
yticoleV
tnioJ
hctabrepnoitubirtnoCegarevA[13] XinChen,BiaoJiang,WenLiu,ZilongHuang,BinFu,Tao
Chen,andGangYu. Executingyourcommandsviamotion
diffusioninlatentspace. InCVPR,2023. 2,3,4,5,6,7,8
References
[14] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav
[1] Chaitanya Ahuja, Dong Won Lee, Yukiko I. Nakano, and Golyanik,andChristianTheobalt. Mofusion: Aframework
Louis-PhilippeMorency.Styletransferforco-speechgesture for denoising-diffusion-based motion synthesis. In CVPR,
animation: A multi-speaker conditional-mixture approach. 2023. 3,4,7,8,5
2020. 3,6 [15] PrafullaDhariwalandAlexanderNichol. Diffusionmodels
[2] SimonAlexanderson,GustavEjeHenter,TarasKucherenko, beatgansonimagesynthesis. InNeurIPS,2021. 3
andJonasBeskow. Style-controllablespeech-drivengesture [16] DaveEpstein,AllanJabri,BenPoole,AlexeiA.Efros,and
synthesisusingnormalisingflows. Comput.Graph.Forum, Aleksander Holynski. Diffusion self-guidance for control-
39(2):487–496,2020. 3 lableimagegeneration,2023. 6
[3] TenglongAo,QingzheGao,YukeLou,BaoquanChen,and [17] YlvaFerstlandRachelMcDonnell. Investigatingtheuseof
LibinLiu. Rhythmicgesticulator: Rhythm-awareco-speech recurrentmotionmodellingforspeechgesturegeneration.In
gesturesynthesiswithhierarchicalneuralembeddings.ACM Proceedingsofthe18thInternationalConferenceonIntelli-
TOG,41(6):1–19,2022. 3 gentVirtualAgents,2018. 3
[4] TenglongAo,ZeyiZhang,andLibinLiu. Gesturediffuclip: [18] YlvaFerstl, MichaelNeff, andRachelMcDonnell. Adver-
Gesturediffusionmodelwithcliplatents. ACMTOG,42(4): sarialgesturegenerationwithrealisticgesturephasing.Com-
1–18,2023. 2,3,5 puters&Graphics,89:117–130,2020. 3
[19] Saeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus F.
[5] Uttaran Bhattacharya, Elizabeth Childs,
Troje, and Marc-Andre´ Carbonneau. Zeroeggs: Zero-shot
Nicholas Rewkowski, and Dinesh Manocha.
example-based gesture generation from speech. Comput.
Speech2affectivegestures: Synthesizing co-speech ges-
Graph.Forum,42(1):206–216,2023. 3
tures with generative adversarial affective expression
learning. InACMMM,2021. [20] Saeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus F.
Troje, and Marc-Andre´ Carbonneau. Zeroeggs: Zero-shot
[6] Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek
example-based gesture generation from speech. Computer
Banerjee,PoojaGuhan,AniketBera,andDineshManocha.
GraphicsForum,42(1):206–216,2023. 3,6
Text2gestures: Atransformer-basednetworkforgenerating
[21] AninditaGhosh,NoshabaCheema,CennetOguz,Christian
emotivebodygesturesforvirtualagents. In2021IEEEVir-
Theobalt,andPhilippSlusallek. Synthesisofcompositional
tualRealityand3DUserInterfaces(VR),2021. 3
animationsfromtextualdescriptions. InInternationalCon-
[7] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
ferenceonComputerVision(ICCV),2021. 4
Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N
[22] AninditaGhosh,RishabhDabral,VladislavGolyanik,Chris-
Chang, Sungbok Lee, and Shrikanth S Narayanan. Iemo-
tian Theobalt, and Philipp Slusallek. Imos: Intent-driven
cap: Interactiveemotionaldyadicmotioncapturedatabase.
full-bodymotionsynthesisforhuman-objectinteractions.In
Languageresourcesandevaluation,2008. 6
Eurographics,2023. 4
[8] Justine Cassell. Embodied conversational interface agents.
[23] S. Ginosar, A. Bar, G. Kohavi, C. Chan, A. Owens, and J.
Commun.ACM,2000. 2
Malik. Learningindividualstylesofconversationalgesture.
[9] JustineCassell,CatherinePelachaud,NormanBadler,Mark InComputerVisionandPatternRecognition(CVPR).IEEE,
Steedman,BrettAchorn,TrippBecket,BrettDouville,Scott 2019. 3
Prevost,andMatthewStone. Animatedconversation: Rule-
[24] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
basedgenerationoffacialexpression,gesture&spokenin-
XingyuLi,andLiCheng. Generatingdiverseandnatural3d
tonationformultipleconversationalagents. InProceedings
humanmotionsfromtext. InCVPR,2022. 5
of the 21st Annual Conference on Computer Graphics and
[25] Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie
InteractiveTechniques,1994. 2
Liu, Hans-Peter Seidel, Gerard Pons-Moll, Mohamed El-
[10] JustineCassell,CatherinePelachaud,NormanBadler,Mark gharib, andChristianTheobalt. Learningspeech-driven3d
Steedman,BrettAchorn,TrippBecket,BrettDouville,Scott conversational gestures from video. In Proceedings of the
Prevost,andMatthewStone. Animatedconversation: rule- 21st ACM International Conference on Intelligent Virtual
basedgenerationoffacialexpression,gesture&spokenin- Agents,2021. 3
tonationformultipleconversationalagents. InSIGGRAPH
[26] Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie
ConferenceProceedings,1994. 3
Liu, Hans-Peter Seidel, Gerard Pons-Moll, Mohamed El-
[11] Justine Cassell, Hannes Ho¨gni Vilhja´lmsson, and Timothy gharib, andChristianTheobalt. Learningspeech-driven3d
Bickmore. Beat:Thebehaviorexpressionanimationtoolkit. conversationalgesturesfromvideo.InProceedingsoftheIn-
InSIGGRAPHConferenceProceedings,2001. 3 ternational Conference on Intelligent Virtual Agents, 2021.
[12] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and 2,3,6
Daniel Cohen-Or. Attend-and-excite: Attention-based se- [27] Ikhsanul Habibie, Mohamed Elgharib, Kripashindu Sarkar,
mantic guidance for text-to-image diffusion models. ACM AhsanAbdullah,SimbarasheNyatsanga,MichaelNeff,and
TOG,42(4),2023. 6,2 ChristianTheobalt.Amotionmatching-basedframeworkforcontrollablegesturesynthesisfromspeech. InSIGGRAPH [42] Sergey Levine, Philipp Kra¨henbu¨hl, Sebastian Thrun, and
’22ConferenceProceedings,2022. 3 VladlenKoltun.Gesturecontrollers.ACMTOG,29(4):1–11,
[28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear 2010. 3
units(gelus). arXivpreprintarXiv:1606.08415,2016. 6 [43] Jing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang,
[29] Jonathan Ho and Tim Salimans. Classifier-free diffusion ZhenyuHe,andLinchaoBao. Audio2gestures: Generating
guidance. InNeurIPS2021WorkshoponDeepGenerative diverse gestures from speech audio with conditional varia-
ModelsandDownstreamApplications,2021. 3,5 tionalautoencoders. InICCV,2021. 3
[30] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- [44] Ruilong Li, Sha Yang, David A. Ross, and Angjoo
sionprobabilisticmodels. InNeurIPS,2020. 3,5,6 Kanazawa. Aichoreographer: Musicconditioned3ddance
[31] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei generationwithaist++. InICCV,2021. 7
Tan, LinGui, SeanBanerjee, TimothyScottGodisart, Bart [45] YuanzhiLiang,QianyuFeng,LinchaoZhu,LiHu,PanPan,
Nabbe,IainMatthews,TakeoKanade,ShoheiNobuhara,and andYiYang. Seeg: Semanticenergizedco-speechgesture
YaserSheikh. Panopticstudio: Amassivelymultiviewsys- generation. InCVPR,2022. 3
temforsocialinteractioncapture.IEEETransactionsonPat-
[46] Haiyang Liu, Naoya Iwamoto, Zihao Zhu, Zhengqing Li,
ternAnalysisandMachineIntelligence,2017. 6 YouZhou,ElifBozkurt,andBoZheng.Disco:Disentangled
[32] AdamKendon. Gesture: Visibleactionasutterance. Cam- implicit content and rhythm learning for diverse co-speech
bridgeUniversityPress,2004. 1,3 gesturessynthesis. InACMMM,2022. 3
[33] DiederikP.KingmaandMaxWelling. Auto-encodingvari-
[47] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng,
ationalbayes. InICLR,2014. 4,5
ZhengqingLi,YouZhou,ElifBozkurt,andBoZheng.Beat:
[34] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and A large-scale semantic and emotional multi-modal dataset
BryanCatanzaro. Diffwave: Aversatilediffusionmodelfor forconversationalgesturessynthesis. EuropeanConference
audiosynthesis. InICLR,2021. 3 onComputerVision,2022. 2,3,6,7,8
[35] Stefan Kopp, Brigitte Krenn, Stacy Marsella, Andrew N
[48] XianLiu, QianyiWu, HangZhou, YuanqiDu, WayneWu,
Marshall, Catherine Pelachaud, Hannes Pirker, Kristinn R
DahuaLin,andZiweiLiu. Audio-drivenco-speechgesture
Tho´risson, and Hannes Vilhja´lmsson. Towards a common
videogeneration. InNeurIPS,2022. 3
frameworkformultimodalgeneration:Thebehaviormarkup
[49] Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian,
language. InIntelligentVirtualAgents,2006. 2
Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei
[36] Taras Kucherenko, Patrik Jonell, Sanne Van Waveren,
Zhou. Learninghierarchicalcross-modalassociationforco-
Gustav Eje Henter, Simon Alexandersson, Iolanda Leite,
speechgesturegeneration. InCVPR,2022. 3
and Hedvig Kjellstro¨m. Gesticulator: A framework for
[50] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
semantically-aware speech-driven gesture generation. In
regularization. InICLR,2019. 5
Proceedings of the 2020 International Conference on Mul-
[51] AndreasLugmayr,MartinDanelljan,AndresRomero,Fisher
timodalInteraction,2020. 3
Yu,RaduTimofte,andLucVanGool. Repaint: Inpainting
[37] Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael
using denoising diffusion probabilistic models. In CVPR,
Neff, Hedvig Kjellstro¨m, and Gustav Eje Henter.
2022. 5,6
Speech2Properties2Gestures: Gesture-property predic-
[52] LarsMarstallerandHanaBurianova´. Themultisensoryper-
tion as a tool for generating representational gestures from
ceptionofco-speechgestures–areviewandmeta-analysisof
speech. In Proceedings of the 21th ACM International
neuroimaging studies. Journal of Neurolinguistics, 30:69–
ConferenceonIntelligentVirtualAgents,2021. 3,6
77,2014. 2
[38] Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael
[53] BrianMcFee,ColinRaffel,DawenLiang,DanielPWEllis,
Neff, Hedvig Kjellstro¨m, and Gustav Eje Henter.
Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa:
Speech2properties2gestures: Gesture-property predic-
Audioandmusicsignalanalysisinpython. InProceedings
tion as a tool for generating representational gestures from
speech. In Proceedings of the 21st ACM International ofthe14thpythoninscienceconference,2015. 8
ConferenceonIntelligentVirtualAgents,2021. 3 [54] DavidMcNeill.Gestureandthought.UniversityofChicago
[39] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit press,2008. 3
Kundu,JustinJohnson,DavidFouhey,andLeonidasGuibas. [55] Angeliki Metallinou, Zhaojun Yang, Chi-chun Lee, Carlos
Nifty:Neuralobjectinteractionfieldsforguidedhumanmo- Busso,SharonCarnicke,andShrikanthNarayanan. Theusc
tionsynthesis,2023. 5 creativeitdatabaseofmultimodaldyadicinteractions: From
[40] G. Lee, Z. Deng, S. Ma, T. Shiratori, S. Srinivasa, and Y. speechandfullbodymotioncapturetocontinuousemotional
Sheikh. Talking with hands 16.2m: A large-scale dataset annotations. Language resources and evaluation, 50:497–
of synchronized body-finger motion and audio for conver- 521,2016. 6
sationalmotionanalysisandsynthesis. In2019IEEE/CVF [56] S. Nyatsanga, T. Kucherenko, C. Ahuja, G. E. Henter, and
InternationalConferenceonComputerVision(ICCV),2019. M.Neff. Acomprehensivereviewofdata-drivenco-speech
3,6 gesturegeneration. Comput.Graph.Forum,42(2):569–596,
[41] Sergey Levine, Christian Theobalt, and Vladlen Koltun. 2023. 3
Real-timeprosody-drivensynthesisofbodylanguage. ACM [57] Haokai Pang, Heming Zhu, Adam Kortylewski, Christian
TOG,28(5):1–10,2009. 3 Theobalt,andMarcHabermann. Ash: Animatablegaussiansplatsforefficientandphotorealhumanrendering.InCVPR, [71] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
2024. 1 reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Polosukhin. Attentionisallyouneed. InNeurIPS,2017. 5,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 6
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen [72] PetraWagner,ZofiaMalisz,andStefanKopp. Gestureand
Krueger, and Ilya Sutskever. Learning transferable visual speechininteraction:Anoverview.SpeechCommunication,
modelsfromnaturallanguagesupervision. InICML,2021. 57:209–232,2014. 3
3 [73] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
[59] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee, mond,ClementDelangue,AnthonyMoi,PierricCistac,Tim
SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and Rault, Re´mi Louf, Morgan Funtowicz, Joe Davison, Sam
PeterJ.Liu. Exploringthelimitsoftransferlearningwitha Shleifer,PatrickvonPlaten,ClaraMa,YacineJernite,Julien
unifiedtext-to-texttransformer. J.Mach.Learn.Res.,21(1), Plu,CanwenXu,TevenLeScao,SylvainGugger,Mariama
2020. 8 Drame,QuentinLhoest,andAlexanderM.Rush.Transform-
[60] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, ers:State-of-the-artnaturallanguageprocessing.InEMNLP,
andMarkChen. Hierarchicaltext-conditionalimagegener- 2020. 8
ationwithcliplatents. arXiv,2022. 3 [74] Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang,
[61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, LeiHao,WeihongBao,MingCheng,andLongXiao.Diffus-
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn- estylegesture: Stylizedaudio-drivenco-speechgesturegen-
thesiswithlatentdiffusionmodels. InCVPR,2021. 3,6 erationwithdiffusionmodels. InIJCAI,2023. 3
[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, [75] HongweiYi,HualinLiang,YifeiLiu,QiongCao,Yandong
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn- Wen, Timo Bolkart, Dacheng Tao, and Michael J Black.
thesiswithlatentdiffusionmodels,2021. 2,4,5 Generating holistic 3D human motion from speech. In
[63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala CVPR,2023. 2,3
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed [76] YoungwooYoon,Woo-RiKo,MinsuJang,JaeyeonLee,Jae-
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, hong Kim, and Geehyuk Lee. Robots learn social skills:
RaphaGontijoLopes,TimSalimans,JonathanHo,DavidJ End-to-end learning of co-speech gesture generation for
Fleet,andMohammadNorouzi.Photorealistictext-to-image humanoid robots. In 2019 International Conference on
diffusionmodelswithdeeplanguageunderstanding. arXiv, RoboticsandAutomation(ICRA),2019. 3
2022. 3 [77] YoungwooYoon,Woo-RiKo,MinsuJang,JaeyeonLee,Jae-
[64] SoshiShimada,VladislavGolyanik,WeipengXu,andChris- hong Kim, and Geehyuk Lee. Robots learn social skills:
tianTheobalt. Physcap: Physicallyplausiblemonocular3d End-to-endlearningofco-speechgesturegenerationforhu-
motioncaptureinrealtime.ACMTransactionsonGraphics, manoidrobots. InProc.ofTheInternationalConferencein
39,2020. 8 RoboticsandAutomation(ICRA),2019. 2,3,6,8
[65] LiSiyao,WeijiangYu,TianpeiGu,ChunzeLin,QuanWang, [78] Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang,
Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: JaeyeonLee,JaehongKim,andGeehyukLee. Speechges-
3ddancegenerationviaactor-criticgptwithchoreographic turegenerationfromthetrimodalcontextoftext,audio,and
memory. InCVPR,2022. 7 speakeridentity. ACMTOG,page1–16,2020. 3,6,7,8
[66] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah- [79] YeYuan,JiamingSong,UmarIqbal,ArashVahdat,andJan
eswaranathan, and Surya Ganguli. Deep unsupervised Kautz. Physdiff: Physics-guided human motion diffusion
learning using nonequilibrium thermodynamics. In ICML, model. InICCV,2023. 3
2015. 3 [80] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
[67] Julian Tanke, Linguang Zhang, Amy Zhao, Chengcheng Hong, XinyingGuo, LeiYang, andZiweiLiu. Motiondif-
Tang, Yujun Cai, Lezi Wang, Po-Chen Wu, Juergen Gall, fuse: Text-driven human motion generation with diffusion
andCemKeskin. Socialdiffusion: Long-termmultiplehu- model. arXivpreprintarXiv:2208.15001,2022. 6
manmotionanticipation. InICCV,2023. 3 [81] Weiyu Zhao, Liangxiao Hu, and Shengping Zhang.
[68] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Diffugesture: Generating human gesture from two-person
AmitHBermano,andDanielCohen-Or.Humanmotiondif- dialoguewithdiffusionmodels.InInternationalConference
fusionmodel. InICLR,2023. 3,5,8 onMultimodalInteraction,2023. 6,7,8
[69] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier [82] Weiyu Zhao, Liangxiao Hu, and Shengping Zhang.
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste Diffugesture: Generating human gesture from two-person
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aure- dialogue with diffusion models. In International Cconfer-
lien Rodriguez, Armand Joulin, Edouard Grave, and Guil- enceonMultimodalInteraction,pages179–185.2023. 2
laume Lample. Llama: Open and efficient foundation lan- [83] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,Xiaolei
guagemodels,2023. 1 Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
[70] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen,
Editabledancegenerationfrommusic.InCVPR,pages448– ZhipengChen,JinhaoJiang,RuiyangRen,YifanLi,Xinyu
458,2023. 3,7,8 Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-RongWen. A survey of large language models. arXiv preprint
arXiv:2303.18223,2023. 1
[84] LingtingZhu,XianLiu,XuanyuLiu,RuiQian,ZiweiLiu,
andLequanYu. Tamingdiffusionmodelsforaudio-driven
co-speechgesturegeneration. InCVPR,2023. 2,3,6,7,8ConvoFusion: Multi-Modal Conversational Diffusion
for Co-Speech Gesture Synthesis
Supplementary Material
Thissupplementarydocumentprovidesaglossaryofno-
tationsusedformethodexplanationinSec.7anddiscusses
dataset statistics in Sec. 8. It also provides further details
and analyses of word-excitation guidance in Sec. 9, user
study in Sec. 10 and implementation details in Sec. 11.
Moreover, we discuss evaluation metrics in Sec. 12 and
trainingdetailsforbaselinemethodsinSec.13.
7.GlossaryforNotations
In Tab. 5, we provide a list of variables used in our the
method and implementation details ( Sec. 3 and Sec. 11)
foreaseofreference.
Variable Description
x GestureSequence
x ,x BodyandHandMotions
b h Figure 7. Here, we show an over-arching view of our data-
C ConditioningSet
recordingsetup,wherewehavefivepeopleinteractingwitheach
z Latentrepresentation other,whiletheirmotiontrackingisrecordedviaastate-of-the-art
z b,z h LatentRepresentationforbodyandhands marker-lessmotioncapturesystem. Eachpersonalsohasindivid-
ξ ,ξ Encoderforbodyandhands ualmicrophoneswhichfeedintoouraudiosetup.
b h
D ,D Decoderforbodyandhands
b h
x′ ,x′ Reconstructedmotionforbodyandhands
b h
ˆz Time-awareLatentRepresentation monadicgesturedatasetslikeBEAT.WediscusshowBEAT
ϵ Predictednoise can be used with our framework along with our dataset
θ
f Denoiserneuralnetwork inSec.13.2.
θ
a AudioSignal Finding a capture setting that elicits high density of
τ TextEmbedding meaningful,semanticgesturesisindeedachallengingtask.
τ′ TextEmbeddingforco-participant These considerations lead us to capture the participants in
s SpeakerIdentityToken arole-playingsettingastheyneedtodescribeanimaginary
m Active/PassiveBitsforLatentChunks worldtoeachother,therebyleadingtoahighdensityofse-
w Modalityguidancescaleforconditionc manticgestures. Thesettingalsooffersaclearintrinsicre-
c
S NumberoftokensselectedforWEG wardtotheparticipants(ofwinningthegame).Asweshow
G WordExcitationGuidanceobjective inthevideoandwebsite,thegesturesinourdatasetaresim-
exc
˜z(t) UpdatedlatentafterWEG ilartotheonesappearingindailyconversationbecausepar-
ticipants are simply discussing a game plan or their next
Table5.Listofvariablesandtheircorrespondingexplanation stepsincertainsituationsusinglanguagethatiscolloquially
usedinconversations. Interestingly,themostrelevantges-
turestothegamesettingarepointinggestures(participants
usuallypointtoobjectsontable)whichareconsideredde-
8.DatasetStatistics&Discussion
icticgestures,whichhappenfrequentlyinnormalconversa-
The proposed DND GROUP GESTURE consists of 6 hours tions. Wehighlightthattheproposeddatasetisrecordedin
ofmocapdatacomprisingof5personsinthescene.Intotal, amarkerlessmotioncapturesetupwhichitkeepsthegroup
we have 2.7M poses along with synchronized, per-person conversation natural without restrictions of a capture suite
audiotracksandtexttranscripts(seeFig.7). Theproposed or markers, thereby reducing the Observer’s Paradox. Fi-
dataset addresses a different aspect of human gestures, i.e. nally, thesubjectsarefamiliarwitheachother(esp. inthe
group conversations, which is a sparsely researched set- DnDsetting)whichfurtherhelpsinmorenaturalconversa-
ting. Thismakesourdatasetcomplementarytotheexisting tions.Generation Process
Latent Update
Denoiser Network
Cross Attention
Spoken Gaussian ( , )
Text Cross Attention Smoothing
Cross Attention
Speech Attention on
Audio Spoken Text “well-off in England would eat french food and what we now think”
CCooOOnnddtthhiitteeiioorr
nnss
Word Ex Oci bta jeti co tin
v
G euidance
Selected words by user: french, now
Figure8.AlgorithmoverviewofWordExcitationGuidance.HereweshowtheprocessfortheexampleshowninFig.1.
9.OnWordExcitationGuidance ourlatentdiffusionframeworkassignshighattentiontothe
starttokenintext(showninFigure10), therefore,wemit-
In the following sections, we provide details of algorithm
igate this issue by considering the attention on the actual
for word-excitation guidance and then perform additional
text tokens. Then we apply Gaussian smoothing over the
in-depthanalysisonitsresults.
remainingattentionmapforstablegenerationresultswith-
outanyjerksinmotion. Thisensuresflexibilitytofocuson
9.1.AlgorithmDetails
a neighbourhood of words instead of one word by avoid-
inggradientupdatesatonlythechosentokensignoringits
Algorithm1Word-ExcitationGuidance neighbourhood. Next,wecalculatetheaveragefortheloss
Input: Setoftokens{τ }S
overallthefocustokenstoequallytransfergradientsforall
i i=1 thefocusedwords. Notethat,thisisdifferentfromimage-
1: TrainedDiffusionModelf θ
based semantic guidance [12], where Chefer et al. apply
2: TextPromptτ ∈C,
smoothing on attention for only the chosen words/tokens
3: DiffusionTimestepsT
which ignores the neighbourhood tokens. Moreover, their
4: Stepsizeα
Output: DenoisedLatentˆz(0) loss aggregation only enables gradient transfer for tokens
5: Initializeˆz(T) ∼N(0,I) with the lowest attention instead of all focused tokens by
using a max function instead of mean like us. The com-
6: fort=T to0do
7: ,A=f θ(ˆz(t),t,{τ}) ▷getattentionfortext pleteprocessispresentedinAlgorithm1.
8: A←Softmax(A start:end) ▷removestart/endoftext 9.2.AdditionalAnalysis
9: A←Gaussian(A) ▷smoothoutattentions
10: G exc = S1 (cid:80)S i=1(1−max(A i)) ▷calculatelossi J tio oi nn gts uiA df af ne cc ete sd teP ere sr thW eo gr ed s. tuR reec ga el nl et rh aa tit ow no pr rd o- cle ev ssel the rx oc uit ga h-
11: ˜z(t) ←ˆz(t)−α·∇ ˆz(t)G exc ▷updatelatent the denoising network to have pronounced gestures at cer-
12: PerformIterativerefinement[12]
tainwordsinthetext. Itgivesafinemechanismforseman-
13: ϵ( θt), =f θ(˜z(t),t,C) ▷estimatenoise
tic control over gesture generation. In Fig. 9, we present
14: PerformModalityGuidance
ananalysisofhowthismechanismaffectseachjointinthe
15: ˆz(t−1) ←SchedulerStep(˜z(t),ϵ( θt)) generation. The figure encodes as heatmap the velocity of
16: endfor eachjointinresponsetothetexttokens;theassumptionbe-
17: returnˆz(0) ingthathighvelocityimpliesheaviergesturing.Weseethat
thehandandthearmjointsareaffectedthemostatthefo-
Theprocessofword-excitationguidanceinvolvesmod- cusedwords. Interestingly,minimalattentionisfocusedon
ifying the usual denoising loop by updating the latents at the lower body; this is expected as most gestures are pre-
each timestep. Before updating the latents, we normal- dominantlyupper-bodymotions.
ize the attention maps by removing attention on the start Choice Of Words. Specific types of gestures tend to cor-
and end tokens because each training batch contains text relatewithcertainlinguisticstructuresandpartsofspeech.
prompts of different lengths. Moreover, we observe that Thus, for analysis, we conduct experiments with attentionEffect of Word-level Semantic Guidance on Individual Joints maps, we include example results in Fig. 10. The atten-
Hip
Head tion map A is dependent on text conditioning and diffu-
Neck
LeftShS oS p up i ln din e e1e r 0.06 sion timestep and shows the relation between chunks of
LeftElbow latent representation and text tokens. Therefore, perform-
LeftForearm
RightL Se hf otH ula dn ed r ing word-level guidance at each diffusion timestep yields
RightElbow
RightForearm slightlydifferentattentiondistributionoverwords. Weob-
RightHand
LeftHip servethataswemovefromt=T to0,focusedwordtokens
LeftKnee 0.05
LeftAnkle
LeftFoot (highlightedinred)starttogethighattention,especiallyaf-
LeftToe
RightHip tert = T/2. WealsoseetheeffectofGaussiansmoothing
RightKnee
RightAnkle
RightFoot as the attention on focus words is not sharply focused on
RightToe
L Le ef ft tH Ha an nd dT Th hu um mb b1 2 0.04 only those words. Rather it is spread over its neighbour-
LeftHandThumb3
LeftHandThumb4 ingtokensaswell. Lastly, noticethestripedpatternofthe
LeftHandIndex1
LeftHandIndex2 attention weights. This arrangement is a manifestation of
LeftHandIndex3 LeftHandIndex4
LeftHandMiddle1 theseparatedbodyandhandlatentsthathavebeenstacked
LeftHandMiddle2
L Le ef ft tH Ha an nd dM Mi id dd dl le e3 4 0.03 alternatively. It shows that the network learns to perform
LeftHandRing1
LeftHandRing2 attentioninaseparatemannerforbothtypesoflatentsand
LeftHandRing3
LeftHandRing4 guidance affects them differently across different layers as
LeftHandPinky1
LeftHandPinky2
LeftHandPinky3 well.
LeftHandPinky4
RightHandThumb1 0.02 Limitations.Ourmethodis,afterall,adata-drivenmethod.
RightHandThumb2
RightHandThumb3
RightHandThumb4 It depends on the learned conditional gesture distribution
RightHandIndex1
RightHandIndex2 of text and other modalities, which can lead to it generat-
RightHandIndex3
RR igi hg th HtH ana dn MdI in dd de lx e4 1 ingthemostcommongesturetype(beatgestures)seenfor
RightHandMiddle2
RightHandMiddle3 0.01 some words. Consequently, performing word-level guid-
RightHandMiddle4
R Ri ig gh ht tH Ha an nd dR Ri in ng g1 2 ancedoesnotalwaysguaranteethespecificmotionofaccu-
RightHandRing3
RightHandRing4 ratesemanticsub-gesturetype(iconic, deictic, metaphoric
RightHandPinky1
R Ri ig gh ht tH Ha an nd dP Pi in nk ky y2 3 etc.) at the focused word or phrase. However, as we ana-
RightHandPinky4
well-offi en ngland would e fa rt ench food and whatw ne ow think l ry esz ued lt, st ih ne au ss ea mge ao nf tiw cao lr lyd- mex ec ai nta inti go fn ug lu gi ed sa tun rc ee a( sW cE omG) pam reo dstl toy
Motion Aligned Text thebasepredictionwithoutWEG.Forfutureworks,amore
explicitrepresentationofgesturetypesandtheirmappingto
Figure9.Heatmapshowingthedistributionofvelocityofalljoints
wordscanbeprovidedasaconditioning,whichmighthelp
comparedwithmotion-alignedtext(focusedwordsarehighlighted
inpredictingsemanticallyaccurategestures. Secondly,the
inred). Weseehighjointvelocityforhandandarmjointsaround
thewords“french”and“now”. amount of focus each word/phrase attains in terms of ges-
ture movements is dependent on the fact that speech also
contains certain prosodic stress for that word. Similarly,
if the gestures around focused words are already stressed
focused on these elements. To extract phrases that may
adequately in motion or those words already have high at-
mapontoasemanticgesture,weselectrandomthree-word
tention on them, then the change introduced by guidance
phrases in the text. To experiment with individual words,
willonlybesubtle. Lastly, thechoiceofwordsaffectsthe
we can focus on nouns and verbs as they have a higher
typeofstressingesturemovementspredictedandthiscan
chance of mapping onto iconic gestures. Adverbs and ad-
behighlysubjective.
jectivescanalsobechosensincetheycanconveyspatiotem-
poral properties of events and entities. This choice mech-
10.UserStudy
anism, which is motivated by the mapping of gestures to
linguisticstructures,isalsoflexibleenoughfortheusersto Forevaluationofmonadicsynthesis,theuserwereshowna
choose different linguistic features to focus on. We also randomlysampledsetof10forced-choicequestions. Each
consideroptimalstressworddiscoveryasafutureendeav- question included a side-by-side animation of our method
our. Lastly,thesuccessofword-levelsemanticguidanceis along with one of MLD [13], CaMN [47], or the ground-
also affected by the amount of stress certain word has in truth. The participants had to answer two question, (a)
theaudio. Weshowattentionresultsforphrasesandwords “Which of the two gesture motions appears more natural?
in Fig. 10 and gesture generation results in supplementary and(b)“Whichofthetwogesturemotionscorrespondsbet-
video. ter with the spoken utterance?”. These questions try to
InterpretingWord-ExcitationthroughAttentionMaps. gaugeplausibilityofthemotionsandalignmentofthegen-
Since we perform word-level guidance on text attention erated gestures with the utterance. For the task of dyadic
stnioJ(a)Text:“anddriedfruitsinseptemberyou’reboilingitin”
(b)Text:“anddriedfruitsinseptemberyou’reboilingitin”
(c)Text:“anddriedfruitsinseptemberyou’reboilingitin”
(d)Text:“afterawhilecameandsettled”
(e)Text:“afterawhilecameandsettled”
(f)Text:“afterawhilecameandsettled”
Figure10. Textattentionexamplewithfocusonaphrase:(a)t=990(b)t=400(c)t=10andexamplewithmultipleindividual
words: (d)t = 990(e)t = 400(f)t = 10. VerticalaxesshowM ×2latentchunkswhereevenandoddindicesstandforbodyand
handjointsrespectively.Horizontalaxesshowwordtokenswherefocusedwordsarehighlightedinred.Attentionchangesarehighlighted
inredboxeswhereneighboringtokensarealsoincludedtoshowtheeffectofGaussiansmoothing. Lastly,“<bos>”&“<eos>”tokens
representstartandendofthetext(referSec.3.3)synthesis, we showed 5 randomly sampled forced choice Hyperparameter Value
questions to each participant, comparing our method with Latentdimensiond 128
the adapted MLD and the ground-truth. The participants MotionLengthN 128
had to judge the naturalness of the motions similar to pre- NumberofJointsJ 63
vious task and also answer the question: “In which of the MotionchunksM 8
two interactions, the motion of interacting character fits λ 0.05
KL
well with both speech of the the main agent as well as λ 1
lap
theirownspeech,ifany.” Wereportpercentagepreference λ 1
bone
for both tasks. In the third section, we asked the users to TransformerLayers 5
evaluatetheword-excitationguidanceproposedinSec.3.3. AttentionHeads 2
Each question included three motions—corresponding to LearningRate 1×10−4
the ground-truth motion, non-guided motion, and word-
Optimizer AdamW[50]
excitationguidedmotion—aswellasthewordsthatneed
FPS 25
to be excited during synthesis. We compare ground-truth,
non-guided gesture, and word-excitation guided gesture. Table6.ListofvaluesusedfortrainingVAEforourmethod
TheuserswereaskedtorateeachmotiononaLikertscale
of1-5,with5indicatingthemostsemanticallyalignedges-
ture.
11.ImplementationDetails
MotionRepresentation. Themotionxcorrespondstothe
root-relative3DcoordinatesforallJ−1jointsandcamera-
relativetranslationoftherootjoint.Thehandjointsarealso
made relative to their corresponding wrist joint. We also
pre-processthejointpositionsfollowing[24]bynormaliz-
Figure 11. Iterative process for the perpetual rollout of
ingthemotionsequencetostarttheroottrajectoryfromthe
arbitrary-lengthgeneration. Thisis basedonthe diffusionin-
originwhilefacingthepositivez-axis.
paintingtechnique.
VAE. We implement decoupled scale-aware VAE using
twotransformerencodersinordertomaketwohalvesofthe l .
n
latent representation focus separately on body and hands. (cid:80)N (l −¯l)2
L = n=1 n , (10)
Eachencoderisbasedontransformerarchitecturewithlong bone n−1
skip-connectionsutilizedbyChenetal.[13]astheyprove
Lastly, the VAE loss also contains a Laplacian regulariza-
this method to be effective in retaining high information
tiontermL asdescribedearlier,tobetterreconstructsub-
lap
density in latent representation. The output of each en-
tlejerksingesturesandreducejitter.
coderiscombinedintotwoquantitiestorepresentGaussian
distributionparametersµ ϕ andΣ ϕ ofthecombinedscale- L VAE =L 2+λ KLL KL+λ lapL lap+λ boneL bone (11)
awarelatentspaceZ, whereϕrepresentlearnableweights
ofencoders. Wecansamplez2×d usingreparameterization Inordertoachievetime-awarelatentrepresentation,we
trick[33]. encode time-aligned M chunks of motion {x′ i}M i=1 using
We train the VAE until convergence with a combina- encodersbypassingeachx′ i fromξ b andξ h togetˆz i. This
tion of losses to achieve the desired reconstruction qual- sequenceˆz = {ˆz i}M i=1 is applied with a positional encod-
ity. MSE-basedreconstuctionlossisappliedontherecon- ing[71]alongM torepresenttime-alignment. Alongwith
structedmotionxˆ: positional encoding as queries, ˆz passed onto decoder D
as a memory to obtain xˆ. This unique structure of ˆz al-
L =∥xˆ−x∥ (8)
2 2 lows us to perform arbitrary length generation with latent
Moreover, Kullback-Liebler divergence L is used for diffusionmodels,whichgenerallyareconstrainedtodueto
KL
regularizingthelatentspace: fixed-lengthgeneration.
L =D (N(z;µ ,Σ )||N(z;0,I)) (9)
KL KL ϕ ϕ Perpetual Generation Rollout. During inference of our
We also apply Bone Length Consistency Loss [14], which diffusion framework, we leverage the time-aware latent
ensuresthatbonelengthsdonotvaryacrossframesinages- sequence {ˆz }M to autoregressively generate latent se-
i i=1
ture sequence by minimizing the variance of bone lengths quencesforarbitrarilylongsequences.Ascomparedtoear-lierapproaches[47,78],wedonotconcatenateourmodel’s Hyperparameter Value
output which may cause irregular motion at the point of d 128
joining.Wealsodonotencodevariablelengthsequencesin Rangeofβ [8.5×10−4,1.2×10−2]
t
ourVAEframeworkasdonebyMLD[13].Instead,wepro- T 1000
pose an autoregressive generation approach to predict the β Schedule ScaledLinear[61]
t
time-aware latent sequence beyond M number of chunks. Self-AttentionHeads 4
The key to this approach is an iterative process (shown DecoderLayers 9
byFig.11)whereasequenceoffuturelatentchunksispre- LearningRate 7×10−5
dicted based on the last k current latent chunks through a Optimizer AdamW
denoising process. Given the sequenceˆz ∈ RM×2×d rep-
resentsmotionxoffirstN frames,wecallitˆz whichis Table 7. List of hyperparameters for denoising network in our
1:M
known to us. We utilize the last k latent chunks from this method
knownsub-sequence,i.e. ˆz ,andgeneratethenext
(M−k):M
M−klatentchunksthroughthedenoisingprocesstoobtain
process at time step t. Here, β represents the rate of dif-
a new overlapping sequence ˆz . Every time t
(M−k):(2M−k) fusion. The reverse diffusion process consists of denois-
weneedtogeneratethenext(M−k):(2M−k)sequence,
ing ˆz(T) ∼ N(0,I) for T timesteps to generate a latent
wefirstinjectnoisetothepreviouslyknown(M −k) : M
sequenceˆz(0):
sub-sequenceuntilthet−1diffusiontimestep. Then,this
sub-sequence is concatenated with the latent sub-sequence T
at M : (2M −k) which contains new latents for the non- p
(cid:0) ˆz(0:T)(cid:1) =p(cid:0) ˆz(T)(cid:1)(cid:89)
p
(cid:0) ˆz(t−1)|ˆz(t)(cid:1)
, (14)
θ θ
overlapping part in (M −k) : (2M −k) sequence. This t=1
concatenatedsequenceˆz(t−1) isthenpassedtothe
(M−k):(2M−k) where p (ˆz(t−1)|ˆz(t)) is approximated using a denoiser
nextdenoisingiterationwheretheprocessrepeatsbynois- θ
neuralnetworkf (ˆz(t−1)|ˆz(t),t,C),whichistrainedtopre-
ing the known part for the next diffusion timestep. This θ
dictnoiseWeusetransformerdecodernetworkasf which
techniquefollowsthemaskeddenoisingtechniqueusedfor θ
takesˆz(t−1) asqueriesalongwithdiffusiontimesteptand
diffusionimageinpainting[51].
conditioning set C as memory input. We apply positional
encoding to queries and individual memory inputs similar
ˆz(t−1) =⊕(q(ˆz ,t−1),ˆz(t−1) ) (12)
M−k:2M−k M−k:M M:2M−k to [71]. To better distinguish between body and hand la-
tents in ˆz(t−1), we add a learned embedding that aims to
Here, the ⊕ operator concatenates along latent chunks to
differentiatebetweenbodyandhandpartsofthelatentrep-
total length of M for each sequence. When applied iter-
resentation. Wealsoaddalearnedembeddingtoeachele-
atively to the subsequent new frames, this process enables
ment of our conditioning set C separately which helps the
anautoregressiverolloutoffixed-lengthgesturesequences
networkdifferentiatebetweendifferentconditioningtypes.
intoinfinite-lengthsynthesis. Wesetthevalueofthehyper-
EachtransformerlayerstartswithSelf-Attention andLay-
parameterkask =M/2forsimplicity.
erNormlayers,alongwithatime-layerbasedonStylization
Block [80] to incorporate diffusion timestep embedding.
Multi-modal cross attention consists of the same number
Details on Denoising Network. We design denoising
ofheadsasthenumberofelementsintheconditioningset
network for the latent diffusion framework to predict
ϵ (ˆz(t),t). We implement the denoising schedule based C. The outputs of all heads are aggregated using a linear
θ
projection, which is followed by another linear layer with
on DDPM framework with hyperparameters presented
GeLUactivation[28].
inTab.7. ThisframeworkconsistsofaMarkovianchainof
successivelyaddingGaussiannoiseϵtoˆz(0)forT timesteps GuidanceParameters. Wemodifyclassifier-freeguidance
i.e. forward diffusion process. Through this process,ˆz(0), to add modality-level control for each element in our con-
which was sampled from data distribution, becomes ˆz(T), ditioning set. The random modality dropout rate is set to
10% and global guidance scale λ is set to 7.5. The val-
whichfollowsnoisedistributionN(0,I)assumingT issuf- m
ues of w is determined by the task at hand. For exam-
ficientlylarge. c
ple,ifwewanttoextractonlythegesturestylesofdifferent
speakers regardless of input text and audio, we set all w
t=T c
q(cid:0) ˆz(1:T)|ˆz(0)(cid:1) = (cid:89) q(cid:0) ˆz(t)|ˆz(t−1)(cid:1) (13) to0exceptw s = 1,whichcorrespondstospeakeridentity.
This will generate unconditional gestures in the style of a
t=1
specificspeaker(seesupplementalvideofortheexample).
√
where q(ˆz(t)|ˆz(t−1)) = N(ˆz(t)| 1−β ˆz(t−1),β I), de- Forword-excitationguidance,stepsizeα,goesfrom100to
t t
scribes evolution of latent distribution during the noising 70.71asitvariesw.r.t. diffusiontimestep. ThekernelsizeforGaussiansmoothingis3. sequencefromtheirmeanµ . HereBissizeoftestset.
N
Semantic Consistency Evaluation Model: Our method
N
can generate semantically meaningful gestures (as shown 1 (cid:88)
L1div(x)= |x −µ | (16)
in Suppl. Video), thanks to the proposed Word Excitation B i N
i=1
Guidance (WEG). We conduct this ablation the following
way. First, we trained a binary classifier that classifies 1s This metric specifically identifies if the gestures are static
motionsoftheBEATdatasetintoeitherbeatgesture,orse- in movement and make less diverse movements along
manticgesturetype(basedontheGTlabels). Hereseman- the generation length. As shown in supplemental video,
ticclassconsistsoficonic,metaphoric,anddeicticclasses. CaMN [47] and MLD [13] suffer from this problem
This classifier is then used as an oracle to compute the re- whereas, our method predicts different gestures according
callofourgeneratedmotionsforsemanticclasspredictions. tothetextandaudioconditioningsanddoesnothavestatic
Specifically,weextractthespeechandtextforthesentence motion.
in which a semantic gesture has been labeled in dataset. Semantic Relevance Gesture Recall (SRGR) uses se-
ThesearetheninputtoCONVOFUSIONtogeneratethecor- mantic score labelled in BEAT [47] as a weight for the
responding gestures, with and without WEG. For the case ProbabilityofCorrectKeypoint(PCK)betweenthegener-
ofWEG,wefocusontheexactwordswhereinthesemantic ated gestures and ground truth gestures. It aims to reward
gestureoccursinthesentence. beingclosetogroundtruthmotionatpointswhereaseman-
tically relevant gesture exists while also predicting diverse
12.EvaluationMetrics gestures, as mentioned by Liu et al. [47]. This metric has
asimilarissueasFIDasitcomparesPCKbetweenground
We report quantitative results on Beat Alignment truth and prediction and due to many-to-many correspon-
Score [44], FID, Diversity, L1 Divergence and Se- dencebetweengesturesandspeech,thismightnotbesuit-
manticRelevanceGestureRecall(SRGR)[47]andherewe able. Therefore, we conclude that each metric focuses on
briefly describe each one of them. Beat Alignment Score certainaspectsofgesturegenerationandhuman-annotated
was initially introduced [44] to measure the alignment of user study results are more conclusive to determine better
musicbeatstodancemotionforthetaskofmusic-to-dance generationqualityandtoperformaholisticanalysisofges-
synthesis. Thishasalsobeenadaptedforthetaskofgesture turequality
synthesis, where it measures the correlation between
gesturebeatsandaudiobeats. Itisusefulindifferentiating 13.BaselineTrainingDetails
between static motions which do not align well with the
Inthefollowing,weprovidedetailsonhowweprocessall
audio from natural-looking gestures which have speech-
thedifferentmodalitiesforourdataset(Sec.13.1)andpro-
aligned kinematic beats. However, it can report false high
videdetailsfortrainingeachmethodweuseforcomparison
valuesif themotion hasa largeamountof jitterbecause it
(Sec.13.2).
would assume beats created by jitter align well with most
oftheaudiobeats. Wecanseethishappeningformethods
13.1.Dataset
thatshowhighjitter[78,84]inourexperiments. Theyhave
highBeatAlignmentscorewhiletheirFIDisalsolarge. BEAT. We utilize the BEAT dataset [47] in order to aug-
We employ the Frechet Inception Distance (FID) met- mentthetrainingdataforourmethodsothatitbettergen-
ric provided by Yoon et al. [78], also known as FGD. We eralizes to the task of monadic gesture synthesis. It con-
trained our FID network using implementation by Liu et sistsof60hoursofEnglishspeakingtrainingdata,spanning
al. [47]. It is based on an autoencoder network that is 30 subjects that perform gesture motions. The dataset is
trainedforreconstructiontask,andiscalculatedbycompar- richingoodtrainingexamplesformonologuesettingwhich
ingfeaturesofthegroundtruthdataxandgenerateddataxˆ can serve as a good baseline dataset to train our method.
through: Inherently, BEAT’s motion representation is different than
what we use to train our method. Therefore, we re-target
FGD(x,xˆ)=∥µr−µg∥2+Tr(Σr+Σg−2(cid:112)
ΣrΣg) (15)
t th oe mir as tck hel ie tt won ithde ofi un ri Dtio Nn Dto Go Ru Or Us Pke Gle Et So Tn Ud Re Efin di at ti ao sn eti .n Mo ord ree -r
Methodsthatgeneratediversegestureslikeoursanddonot over,weconverttheirrepresentationfromBVH-basedeuler
containpre-poseinformationunlikeCaMN[47]andMulti- angle representation to joint positions using forward kine-
Context[78],maysufferonthismetricbecauseourgestures matics. Then we resample their dataset from 120 FPS to
will not try to match ground truth motion. Diversity com- 25 FPS in order to match it with our training configura-
putestheaveragepairwiseEuclideandistanceofthegesture tion. Lastly, we apply the preprocessing steps mentioned
generations in the test set. L1 Divergence (also called L1 in Sec. 11 to get the final motions which we separate into
variance)measuresthedistanceofallframesN inagesture 5.12-secchunksi.e. 128framesfortrainingourmethod.DNDGROUPGESTURE. Toapplyourmethodtothetask CaMN[47]&Multi-Context[78]. Wetrainbothofthese
of dyadic synthesis, we utilize our recorded DND GROUP methods using the official implementation of CaMN by
GESTURE dataset (see Fig. 7) to extract interactions be- Liu et al. on GitHub. The only modification that took
tweenpeopleinourdataset. WerecordthedatasetinBVH place was the addition of our dataset pipeline which in-
formataswell,however,weextractjointpositionsfortrain- cludesourversionofBEATdatasetandDNDGROUPGES-
ing our method. We standardize dataset FPS to 25. Each TURE dataset, which makes the motion dimensions from
ofthefivepeopleinourdatasethastheirownseparateau- 141 to 189 to match to our setting. We use the provided
diochannelwhichwehavepost-processedtogetcleanand WavEncoder in the implementation to process audio sig-
denoisedaudio. Wealignaudiochannelswiththerecorded nals instead of melspectrograms. Lastly, we use motion-
tracking and verify it manually as well. Finally, we sep- aligned text instead of normal text inputs to be consistent
arate out motions for each person and assign them iden- with them. We also use the provided text-encoder to add
tities which are kept consistent across multiple recording ourtextualvocabularytothetexttokenizerfortextprepro-
sessions. Then, we preprocess the dataset and separate it cessing.
outinchunksof128frames. MLD [13]. This method by Chen et al. which uses la-
Training/TestSplits. WesplittheBEATdatasetbyreserv- tent diffusion models, was presented for the task of text-
ing5outof30Englishspeakersforthetestingset,whilethe to-motionsynthesis. Weextendthismethodforthegesture
remaininggointotrainingandvalidationsplits. Therefore, synthesis task by utilizing our training procedure. To be
alltheresultsandcomparisonsonmonadicsynthesisusing consistentwiththeirmethod,weusethetextencoderwhich
BEATdatasetareprovidedonunseenspeakerswhichshows wasusedbyMLD.
amethod’sgeneralizabilitytounseenaudioandtextinputs. DiffGesture[84]. Weusetheirofficialimplementationon
Fordyadicsynthesis,werandomlysampleandtakeout10 GitHub to train this method for the task of monadic ges-
percentfortestingandrestfortrainingandvalidation. ture synthesis. Since DiffuGesture [81] does not provide
RepresentationofModalities. Weprocessaudiobysam- animplementationfordyadicsynthesistaskanditishighly
pling it to 16000 Hz and extracting melspectrograms us- basedonDiffGesture,wefollowDiffuGesture’simplemen-
ing librosa toolbox [53]. We use 80 mel-bands and a hop tationdetailsascloseaspossibleandadaptDiffGestureto
length of 512 for melspectrogram conversion. We process the dyadic synthesis task. As this method was originally
textthroughtexttokenizerandconvertthemtoembeddings trainedonTEDDataset[77],whichcontainsonlytheupper
through T5 text encoder [59] implementation by Hugging body, we double the capacity of their transformer network
Face[73]. to cater to the increase in dimensionality in our setting to
Lastly, all methods are trained on these dataset splits ensurefairness. Theaudioandtextprocessingiskeptcon-
to ensure fairness. There are some differences between sistent with DiffGesture’s implementation. Lastly, for the
the type of representation used for audio and text in each taskofdyadicsynthesis,weprovideco-participant’stextas
method,whichweelaborateoninthenextsection. anadditionalconditioninginputtomatchitwithourtrain-
ingpipeline.
13.2.MethodsforComparison
ConvoFusion (Ours). We train our method on 128-frame
sequencesbylearningalatentspacerepresentationofthem
Then we use our diffusion framework on top of it. Inter-
estingly,wecanincorporatebothmonadicanddyadicges-
turesynthesistasksintosingletraining. ThankstoModal-
ityGuidance, wecanusedifferentmodalitiesinterchange-
ably by dropping them out of the training batch and set-
ting an unconditional token in their place. For example,
BEAT dataset only contains single-person gesture annota-
tions and does not contain a co-participant, hence making
it non-trainable for the task of dyadic synthesis. However,
we can simply provide an unconditional token for the co-
participant’s text which automatically turns the contribu-
tion of the corresponding modality guidance term to zero,
andBEATdatasetcanbetrainedjointlywithDNDGROUP
GESTURE dataset. Asimilarapproachcanbetakenforse-
manticannotationlabelsprovidedbyBEATdataset,which
wedonotprovideforours.