ChroniclingAmericaQA: A Large-scale Question Answering
Dataset based on Historical American Newspaper Pages
BhawnaPiryani JamshidMozafari AdamJatowt
bhawna.piryani@uibk.ac.at jamshid.mozafari@uibk.ac.at adam.jatowt@uibk.ac.at
UniversityofInnsbruck UniversityofInnsbruck UniversityofInnsbruck
Innsbruck,Austria Innsbruck,Austria Innsbruck,Austria
ABSTRACT 1 INTRODUCTION
Questionanswering(QA)andMachineReadingComprehension QuestionAnswering(QA)andMachineReadingComprehension
(MRC)taskshavesignificantlyadvancedinrecentyearsduetothe (MRC)arepopularNaturalLanguageProcessing(NLP)tasks.They
rapiddevelopmentofdeeplearningtechniquesand,morerecently, allowuserstoposequestionsandreceivedirect,conciseanswers
largelanguagemodels.Atthesametime,manybenchmarkdatasets fromagivencontext,transforminghowweinteractwiththeinfor-
havebecomeavailableforQAandMRCtasks.However,mostex- mation.Thesetaskshaveseenasignificantadvancementinrecent
istinglarge-scalebenchmarkdatasetshavebeencreatedpredomi- yearsmainlyduetothedevelopmentofdeeplearningtechniques
nantlyusingsynchronousdocumentcollectionslikeWikipediaor and,morerecently,largelanguagemodels.Theavailabilityofhigh-
theWeb.Archivaldocumentcollections,suchashistoricalnews- qualityQAandMRCdatasetshasalsoplayedacrucialroleinthe
papers,containvaluableinformationfromthepastthatisstillnot progressofQAandMRCtasks.
widelyusedtotrainlargelanguagemodels.Tofurthercontribute However,acloserlookattheavailabledatasetsrevealsparticular
toadvancingQAandMRCtasksandtoovercomethelimitationof limitation - they are predominantly created using synchronous
previousdatasets,weintroduceChroniclingAmericaQA,alarge- documentcollectionssuchasWikipediaandtheWeb.Oursociety
scaledatasetwith485Kquestion-answerpairscreatedbasedonthe maintains extensive collections of archival documents, such as
historicalnewspapercollectionChroniclingAmerica.Ourdataset historicalnewspapers,whichconstituteourheritage.Historical
isconstructedfromasubsetoftheChroniclingAmericanewspaper documents serve as a rich knowledge repository, capturing the
collectionspanning120years.Oneofthesignificantchallenges trendsofdifferenteras.Theyofferauniqueperspectiveonthe
forutilizingdigitizedhistoricalnewspapercollectionsisthelow past, as they are the primary sources of information related to
qualityofOCRtext.Therefore,toenablerealistictestingofQA historicalevents,culturalnorms,andsocietalattitudes[24].Itis
models,ourdatasetcanbeusedinthreedifferentways:answering beneficialtouseQuestionAnswering(QA)technologiesonsuch
questionsfromrawandnoisycontent,answeringquestionsfrom documentsaswell.QAmodelscouldhelpanswerawiderange
cleaner,correctedversionofthecontent,aswellasansweringques- ofquestions,fromthoseaboutspecifichistoricaleventstothose
tionsfromscannedimagesofnewspaperpages.Thisandthefact seekingtounderstandculturaldevelopmentsovertimeandthe
thatChroniclingAmericaQAspansthelongesttimeperiodamong minutiaeofeveryday life.Despitetheir importanceand wealth
availableQAdatasetsmakeitquiteauniqueandusefulresource. ofcontainedinformation,heritagedocumentcollectionsarestill
underutilizedinthecontextofQAresearch.
CCSCONCEPTS However,leveragingthesedocumentsforQAiscomplexand
posesafewchallenges.Thelanguageusedinheritagedocuments
•Informationsystems→Questionanswering;Contentanal-
oftendifferssignificantlyfromthecontemporarylanguageinterms
ysisandfeatureselection.
ofvocabulary,syntax,andtheoverallcontextofthedistanttimes
[2,49].TheseraisesdoubtsifQAmodelstrainedoncurrenttext
KEYWORDS
datacanindeedperformwellondocumentsfromdistantpast.Ad-
Questionanswering,heritagecollections,OCRtext
ditionally,thedocumentsareoftenavailableonlyinscannedform,
and the quality of the text obtained through Optical Character
ACMReferenceFormat:
BhawnaPiryani,JamshidMozafari,andAdamJatowt.2024.ChroniclingAmer- Recognition(OCR)cangreatlyvary,especiallyoverlongtimeperi-
icaQA:ALarge-scaleQuestionAnsweringDatasetbasedonHistoricalAmer- ods,addinganotherlayerofcomplexitytothetask.Finally,these
icanNewspaperPages.InSIGIR2024.ACM,NewYork,NY,USA,11pages. documentshaveacomplexlayout,oftencontainingdifferentfont
https://doi.org/XXXXXXX.XXXXXXX sizesandtypethroughouttheircontent,whichmakesinformation
extractionacomplextask.Figure1showsexamplesofscanned
newspaperpagesandtheOCRedtext.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed Despitetheinherentchallengesofusinghistoricalnewspapers,
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation theypresentanattractiveanduniqueopportunityforresearchin
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or the field of QA. The wealth of information in these documents
republish,topostonserversortoredistributetolists,requirespriorspecificpermission makesthemvaluableresourcesthatarenotreadilyavailableintyp-
and/orafee.Requestpermissionsfrompermissions@acm.org.
icalevaluationbenchmarks.Toaddressthechallengesandutilize
SIGIR’24,July14–18,2024,Washington,USA
thepotentialofhistoricaldocumentcollections,weintroducethe
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACMISBN978-1-4503-XXXX-X/18/06 ChroniclingAmericaQA,alarge-scaledatasetbasedonHistorical
https://doi.org/XXXXXXX.XXXXXXX
4202
raM
62
]LC.sc[
1v95871.3042:viXraSIGIR’24,July14–18,2024,Washington,USA Piryani,etal.
Elu|ieror William has teiegrapli<*d to President\nCarnoL, of
France, thanking him for his messago\nof condulenoe or the
death of Kmperor Frederick,\nand expressing the boiie that
tlie good relations\nnow i xisilug between France alid Germany
mar\ncontinue.\n Balfsar tay? He u ill Hrtlcn.\n Ui bi.in.
JuuedU.?It is iv|iorted b<>re that Mr.\nBailour, chief
secretary lor Ir.iand. Intends to rtfe\nsign the secretary ship.
Mi. Kallour, however, in\ndignantly denies the truth of this
rcjiorl.\n from Wall Mreet To-day.\n New VoKk, June ? Jo, 11
a. in.?l ie- st.K-k markec\nshow is! a moderate d' gree of
animation at tho\no|k nlng, which, however, soon disap|?-ari'd
and\ndulluess bei aiue again a featur- of tlie trading.\n The
Improved tone of last evening had completely\ndisappeared
also, the list (bbe)ing weak ihmugiioui,\nand hrst prices were from
tc ^ |?'r c?*nt below\nyesterday s Iiual quolatlons. \\
tem(Hirary ad\nvance extending to irom to , ?a- madi in
tlie\nfirst few minutes, but the do. nnc wasijmcku rw\nsuiued
and losses from v to ^ were isHaOllsnnd in\nthe general list,
New 1 nglatiJ showing the UiymK\nfluctuant II, but ltock
Island was tie s(?vial b al\nturc,dropping li, pi rc*'nt, Atiotlier
s.igni recov\nery was followed by the most intense ilnllte-
ss,\nand the Hubsitiucnl dealings were entirely devoid\nof
feature, the market at 11 o'clock fr-iug verjr\ndull anl steady
at fractions below the o|x uing.\n The Prlarrioa l onwesirneit.\n
oKADrarioN EXkKcisks to-dat.\n
(b) (c)
Emperor William has telegraphed to President Carnot, of France, thanking him for his message of condolence on
the death of Emperor Frederick, and expressing the hope that the good relations now existing between France
and Germany may continue. Balfour Says He Will Resign. Berlin, June 10.?It is reported here that Mr. Balfour,
chief secretary for Ireland, intends to resign the secretaryship. Mr. Balfour, however, indignantly denies the
truth of this report. From Wall Street To-day. New York, June 10, 11 a.m.? The stock market showed a moderate
degree of animation at the opening, which, however, soon disappeared and dullness became again a feature of the
trading. The improved tone of last evening had completely disappeared also, the list being weak throughout, and
first prices were from 1 to 2 percent below yesterday's final quotations. A temporary advance extending to from 1
to 1½ was made in the first few minutes, but the decline was quickly resumed and losses from ½ to 1½ were
established in the general list, New England showing the largest fluctuation, but Rock Island was the special
feature, dropping 1½ percent. Another slight recovery was followed by the most intense dullness, and the
subsequent dealings were entirely devoid of feature, the market at 11 o'clock being very dull and steady at
fractions below the opening. The Graduation Exercises. GRADUATION EXERCISES TO-DAY.
(a) (d)
Figure1:AnexampleofthescannednewspaperpagefromChroniclingAmericaCollection.a)depictstheentirenewspaper
pagepublishedinEveningStaron1803-02-07intheDistrictofColumbia,b)depictsthezoomed-inparagraphofthenewspaper
pageshownin(a),c)showstheoriginalOCRtextofthezoomed-inparagraphthatisavailableintheChroniclingAmerica,and
d)showstheOCRtextcorrectedbyGPT3.5Turbo.
AmericanNewspaperPages.Asanunderlyinghistoricaldocument Tosumup,wemakethefollowingcontributionsinthiswork:
collection,weuseChroniclingAmerica1,adigitizedcollectionof
• Weproposealarge-scaleQAdatasetbuiltonhistoricalnews-
America’sHistoricalnewspaperpagesfrom1756-1963.Chronicling
paperpages,whichisthelongest-spanningdatasetoverthe
Americaisaprojectthatwascreatedwiththeaimofmakinga
periodof120years.
digital,searchableengineforhistoricalAmericanNewspapers[1].
• Wecomprehensivelyevaluatetheproposeddatasetondif-
WhilepreviousstudieshaveleveragedtheChroniclingAmerica
ferentmodelsincludingalsolargelanguagemodels(LLMs),
collection,theirfocushaspredominantlybeenonvisualcontentex-
establishingareferencebaselinelevelfortheperformance
traction[28].Incontrast,wefocusonutilizingthealreadydigitized
ofquestionansweringoverhistoricaldocuments.
text,andweemploythisresourcefortestingcutting-edgeinforma-
• Wequantifytheperformancedegradationofvariousmodels
tionprocessingtechnologiesonthisnovelandhighlychallenging
forquestion-answeringtasksduetoNoisyOCRtext.
typeoftextdata.
Weadoptanautomaticapproachtoconstructthedatasetdueto
severalreasons.First,thehistoricaldocumentcollectionishuge, 2 RELATEDWORK
andprocessingsuchasheercollectionofdatamanuallyisimpossi-
2.1 NLPforHistoricalTexts
ble;becauseofthis,weuseGPT3.5[10]forcorrectingthedigitized
TheapplicationofNaturalLanguageProcessing(NLP)techniques
textavailableonChroniclingAmerica.Second,weemployagenera-
tohistoricaldocumentshasgainedsignificantattentionduetothe
tivemodeltogeneratethequestionsautomaticallybecausemanual
increasingavailabilityofdigitizedcontent.Priorworkshavepri-
generationwouldbeverycostly.Third,recentlydevelopedgenera-
marilyfocusedonvariousaspectsofprocessinghistoricaltexts
tivemodelshaveshowngreatperformanceforautomaticquestion
suchastextnormalization[7,8,44],PoSTagging[19],NamedEn-
generation[14,38]atthesametime.Thefinaldatasetthatwere-
tityRecognition[15,16],EventDetection[27,46],biasanalysis[9]
leasecomprises485kquestion-answerpairsspanning120years
andco-referenceresolution[25].Theseworkshavesignificantly
(1800-1920)2.
contributedtoourunderstandingofhistoricaldocuments.How-
ever,theapplicationofQA,acriticaltaskinNLPandInformation
1https://chroniclingamerica.loc.gov/about/ Retrieval(IR),isstillunderexplored.
2The dataset is freely available at: https://github.com/DataScienceUIBK/
Thechallengeofutilizingthehistoricaltextoftenrevolvesaround
ChroniclingAmericaQA.git
thequalityofOCRedtextanditsrepercussionsontheNLPandChroniclingAmericaQA:ALarge-scaleQuestionAnsweringDatasetbasedonHistoricalAmericanNewspaperPages SIGIR’24,July14–18,2024,Washington,USA
Table1:ComparisonofTextualQAdatasets(notethatfortheCorpusSizecolumn,thevaluesintheparenthesesrepresentthe
numberofarticles/pagesintheentirecollection,whilethevaluesoutsidearethenumbersofarticles/paragraphsusedinthe
datasetcreation).
Dataset #Questions AnswerType QuestionSource CorpusSource CorpusSize No.ofCorpusSources GeographicalDispersion Synch/Diach DocumentFormat
Generative
MSMARCO[5] 1M Querylogs WebDocuments - - - Synchronic Text
Boolean
SQuAD1.1[43] 108K Extractive Crowd-sourced Wikipedia - 1 - Synchronic Text
SQuAD2.0[42] 158K Extractive Crowd-sourced Wikipedia - 1 - Synchronic Text
NaturalQuestions[26] 323K Extractive Querylogs Wikipedia - 2 - Synchronic Text
5kNewsArticle Diachronic
NewsQuizQA[29] 20K Multiple-choice Crowd-sourced News 2 Atlanta,London Text
summaries (2018-2020)
Diachronic
NewsQA[50] 119K Extractive Crowd-sourced News 13kNewsArticles 1 Atlanta Text
(2007-2015)
Automatically 98KNewsArticles Diachronic
ArchivalQA[52] 532K Extractive News 1 NewYorkCity Text
Generated (1.8MArticles) (1987-2007)
Automatically 160KParagraphs Diachronic ScanImages&
ChroniclingAmericaQA 485K Extractive News 1,694 Across53USStates
Generated (121Mnewspaperpages) (1800-1920) OCRText
IRtasks.Numerousstudieshavebeenconductedtoanalyzethe usingonlyasinglenewssource.ArchivalQAutilizestheNYTCor-
impactofOCRerrorsondiversetasks[12,18,34,51].Hamdietal. pus[45]asitssource,witharticlescommingfromtheperiodof
[18]analyzedtheimpactofOCRerrorsinnamedentityrecognition 1987/01-2007/06,whereasNewsQAutilizestheCNNnewsarticles
andlinkingandfoundthat80.75%namedentitieswerewrongly throughout2007/04-2015/04.
recognized due to OCR errors, hence they concluded that OCR Our dataset, ChroniclingAmericaQA, uses historical newspa-
errorscannegativelyimpactthetask.Chironetal.[11]studiedthe per pages that span 120 years ranging from 1800 to 1920. Both
effectsofOCRerrorsondigitallibraryutilizationandfoundthat ArchivalQAandNewsQAusemodern-daytext,whichmakesit
for7%oftheuserqueriespotentialdocumentsweremisseddueto easyforpresent-daymodelstounderstandastheyareessentially
wrongOCRedwords.Similarly,Mutuvietal.[34]alsoquantified trainedonsuchtext.Incontrast,thetextinChroniclingAmericaQA
theadverseeffectsofOCRerrorsontopicmodelingtasks.However, isfromhistoricalnewspapers,whichshowsthelanguageisarchaic,
tothebestofourknowledge,nosuchstudyhasbeenconductedfor whichmakesitmorevaluablefortrainingthemodelstoadvance
theQAtask.Hence,inthisresearchwork,weperformacompre- theQAfield.ChroniclingAmericaQAisalsocreatedfromhistorical
hensiveevaluationoftheimpactofOCRerrorsonQAtasksand newspaperdocumentsinthepublicdomain,makingitaccessibleto
trytobridgethisresearchgap. thecommunity,unliketheNYTcorpususedforArchivalQA,which
isnotopen-sourcedandthusnotaccessibletoeveryone.Chroni-
clingAmericaalsofeaturesamorediverseandcomplexlanguage
becausethenewspaperpagesarefromarelativelydistantpast,and
2.2 QABenchmarks
theOCRtextismorecomplexandnoisy.InTable1,wesummarize
Inrecentyears,numerouslarge-scaleQAdatasetshavebeenintro-
thedifferencesbetweenChroniclingAmericaQAandotherrelated
duced,contributingsignificantlytothefieldofQA.Notableexam-
datasets.
plesincludeSQuAD1.1[43],alandmarkdatasetforreadingcompre-
Thus,thisworkaimstocreateaQAdatasetfromtheHistorical
hensionderivedfromWikipediaarticles.ItwasfollowedbySQuAD
NewspaperscollectiontofostertheresearchofQAonHistorical
2.0[42],whichexpandedthechallengebyincorporatingunanswer-
documents.Ourdataset,ChroniclingAmericaQA,isdesignedto
ablequestions.BeyondWikipedia,datasetslikeNaturalQA[26]
handletheuniquechallengesposedbyhistoricalnewspapers,such
andMSMARCO[5]haveutilizedqueriesissuedtoGoogleand
asnoisyOCRdata,archaiclanguageaswellastheoverallcontextof
Bingsearchenginesasquestions,top-rankingWikipediapagesor thedistanttimethatdiffersfromthepresentone3.Webelievethat
otherwebdocumentsascontext.NarrativeQA[23]broadenedthe
thisdatasetwillbeavaluableresourceforresearchersinthefield
scopebyusingbooksandmoviescriptsummariesasthebasisfor
ofQAandwillcontributetotheadvancementofQAforhistorical
itsquestion-answerpairs.
documentsandtheNLPfieldingeneral.
Additionally,therearedatasetssuchasCNN/DailyMail[35],
ReCoRD[54],WhoDidWhat[37]thatusenewsarticles,whichare
3 METHODOLOGY
designedasclozetypedatasetswherethegoalistoidentifyamiss-
ingword,ratherthantoansweropen-endedquestions.NewsQuizQA ThissectionintroducestheframeworkusedtocreatetheChroni-
byLelkesetal.[29]isamultiple-choiceQAdatasetderivedfrom clingAmericaQA,whichisalsodepictedinFigure2.Theframework
summaries of 5k news articles to generate quiz-style question- consistsofthreeprimarymodules:DataCollection,DataPrepara-
answerpairs. tionandtheQuestion-AnswerGenerationmodule.Eachofthese
Whenitcomestodiachronicdocumentcollections,twonotable modulesisdescribedbelow.
QAdatasets,ArchivalQA[52]andNewsQA[50]areavailable.They
have however several limitations, making our dataset different
and unique. ArchivalQA and NewsQA have been created from
3ConsiderthetermZeitgeistinGermanlanguageusedtoemphasizethespiritor
documentsspanningshorterandmorerecentperiods,aswellas essenceoftheparticulartimeperiodorepoch.SIGIR’24,July14–18,2024,Washington,USA Piryani,etal.
Digitized OCRed OCRed Corrected Cleaned Generated Filtered
Newspaper Pages Newspaper Text Paragraphs Paragraphs Paragraphs (q,a,p) triples (q,a,p) triples
Historical Newspaper New Ss ep la ep ce tir o P nage P Ca rr ea ag tr ia op nh CPa or ra reg cra tip oh n PCF aoi rl r at re ger c ri an te pg d h GQ eu ne es rt aio ton r Q Fu ilte es rt ii no gn DF ai tn aa sl et
Collection
Data Collection Data Preparation Question Generation
Module Module Module
Figure2:DatasetGenerationFramework
3.1 DataCollectionModule
Start and End Years by State
Inthismodule,wedescribetheprocessofselectingnewspaper
Piedmont
pagesusedforgeneratingtheChroniclingAmericaQAdataset. Alaska
Colorado
WechosethenewspaperpagesfromthecollectionoftheAmeri- Oklahoma
Florida
canHistoricalNewspaperdatasetChroniclingAmerica4available South Dakota
North Dakota
inthepublicdomainasoursourceforquestiongeneration.Chron- Wyoming
Montana
iclingAmericaisaprojectdevelopedbytheLibraryofCongress Idaho
Nevada
andtheNationalEndowmentfortheHumanities(NEH)underthe AO rr ie zg onon a
NationalDigitalNewspaperProgram(NDNP).Itfeaturesover21 KaU nsta ah s
Nebraska
millionpagesofhistoricalAmericannewspaperspublishedfrom Washington
New Mexico
1756to1963,accessiblethroughanonlinesearchportalandpublic California
Texas
API.Theportaloffersvarioussearchstrategiestoaccessthecontent Tennessee
Minnesota
availableonChroniclingAmerica.Thesearchcanbeperformedin Kentucky
Maryland
threeways:byidentifyingnewspaperpagespublishedinaspecific Hawaii
Wisconsin
stateandyear,conductingmoreadvancedsearchesfornewspaper Missouri
Louisiana
pagescontainingparticularwordsorphrases,andlocatingnews- MichI io gw ana
Vermont
paperpagesrelatedtospecificethnicgroupsavailableindifferent South Carolina
Puerto Rico
languages.Weoptedforthefirstsearchstrategy,i.e.,statewide Ohio
Alabama
search by year, as our goal was to create a corpus of historical Indiana
Arkansas
newspapersthatcoversdiverseinformationrangingfromevery Georgia
Connecticut
significantpasteventandethnicitytosomeminorregionalevents Mississippi
North Carolina
throughouttheUnitedStates.Therefore,wedecidedtosearchfor Illinois
New Jersey
thenewspaperpagespublishedacrossthe53statesintheUnited DelawD a.C re.
Maine
States.Figure3showsthetimeintervalsforwhichnewspapers Virginia
West Virginia
areavailableforeachstateoftheUnitedStatesintheChronicling New York
Pennsylvania
AmericaCollection.Consideringtheavailabilityofdatafromdiffer- Rhode Island
Massachusetts
entstates,weoptedfortheperiodfrom1800to1920,whichspans
120years. 1770 1820 1870 1920 1970
Year
Duetotheenormousnumberofdigitizednewspaperpagesavail-
ableonChroniclingAmerica,includingallthenewspaperpages Figure3:Thetemporaldistributionofavailablenewspapers
inourcorpuswasimpossible.Instead,werandomlyselected100 acrossthestatesintheChroniclingAmericaCollection.
newspaperpagespublishedperdecadeineverystateoftheUnited
Statesfrom1800to1920.Weconsidered1800to1920asthetimepe-
riodforourdataset;forthistimeperiod,sufficientlylargenumber Oneofthechallengesofdigitizednewspapersisthequalityof
ofstateshavenewspaperpagesinChroniclingAmericaasshown OCRtext.Forourpurpose,thecorrectnessoftheOCRtextplaysa
inFigure3.Asaresult,weextracted39,330newspaperpagesin significantrole,asquestionsgeneratedfromnoisytextwillnotbe
total.WhiletheChroniclingAmericacollectionspansfrom1789to preciseandaccurate.
1963,noteverystatehasdigitizednewspapersforthesamerange
ofyears.Thenumberofdigitizednewspaperpagesavailableat
3.2 DataPreparationModule
thebeginningofthe1800sislessthaninthe1900s.Therefore,the
Preparing data for question generation is a crucial step in our
numberofnewspaperpagesforourcorpusislowerintheearly
methodology.TheChroniclingAmericaCollection,whilebeing
1800sthaninthelate1800sandearly1900s.
arichsourceofdigitizedhistoricalnewspaperpages,providesraw,
4https://chroniclingamerica.loc.gov/about/ unprocessedtext.ThistextisoftennoisyandcontainsnumerousChroniclingAmericaQA:ALarge-scaleQuestionAnsweringDatasetbasedonHistoricalAmericanNewspaperPages SIGIR’24,July14–18,2024,Washington,USA
inaccuracies,asillustratedinFigure1.Thisnoiseandinaccuracy
Raw OCR Paragraphs
maketherawOCRtextunsuitablefordirectuseinquestiongener- 25000 GPT Corrected OCR Paragraphs
ation.SeveralstudieshaveshownthatnoisyOCRtextnegatively
affectsinformationretrievalandNLPtasks[6,11]. 20000
Toaddressthisissue,wepost-processtherawOCRtexttoen- 15000
hanceitsquality,therebymakingitmoresuitableforquestiongen-
10000
eration.Post-processingOCRtextischallenging,asdemonstrated
byseveralstudies[20,36].Traditionalmethodsforpost-OCRerror 5000
correction,suchasstatisticallanguagemodelingtechniques[4],lex-
i hc aa nl da lp ip nr go hac ish te os ri[ c4 a7 l] t, em xter dg uin eg toO tC hR euo nut iqp uu ets c[ o3 m3] p, la er xe iti in ea sd ae sq su oa ct ie atf eo dr 1800-1810 1810-1820 1820-1830 1830-1840 1840-1850 1850-1860 1860-1870 1870-1880 1880-1890 1890-1900 1900-1910 1910-1920
Decades
withsuchdata.
However, various recent studies have shown utilizing Large Figure4:NumbersofRawOCRParagraphsandCorrected
LanguageModels(LLMs)cangivepromisingresultsincorrecting ParagraphsoverTimeinourDataset
spellingandgrammaticalerrors,especiallybyutilizingtheGPT
models[32,39,53].Consideringthesestudies,wedecidetoleverage
thecapabilitiesofLLMsforOCRcorrectionandoptfortheGPT 3.3 QuestionGenerationModule
3.5Turbo5modeltocorrecttheOCRtext. Thethirdphaseofourframeworkinvolvesgeneratingquestions
BeforegivingtherawOCRtexttotheGPTmodelforcorrection, fromtherevisedparagraphs.WeutilizetheT5-basemodel[40]
weneedtosplitthecontentofnewspaperpagesintoparagraphs. forthispurpose,acutting-edgepre-trainedTransformerencoder-
Sincethecontentonasinglenewspaperpageisdiverseandlong, decodermodelthathasbeentrainedontheSQuAD1.1[43]dataset
rangingfromadvertisementsandarticlestocartoons,processing specificallyforquestiongeneration.Ourapproachtoquestiongen-
suchlongtextwillbedifficultforquestiongenerationaswellasfor erationisanswer-aware,meaningthatthemodelreceivesthean-
correctingerrorsintheOCRedtext.Therefore,wesplitthecontent swersalongwiththeircorrespondingparagraphsasinputs,and
a newspaper page into multiple paragraphs. We set the length generates the questions as outputs. We identify named entities
ofaparagraphto250wordsduetothecontextlengthlimitation withinaparagraphusingthespaCylibrary7,markingtheseasan-
imposedbyGPT3.5Turbomodel6. swersforquestiongeneration.Basedontheseidentifiednamed
Initially,weselectedatotalof39,330newspaperpagesforthe entities,questionsarethengenerated.From163,021cleanedpara-
purposeofquestiongeneration.However,duetothecostassoci- graphs,wesuccessfullygenerated2,912,551questions.
atedwithusingtheGPT3.5APIforOCRcorrection,wedidnot Question&answercleaning:GiventhatweemploytheT5-base
utilizealltheselectednewspapersforthisprocess.Correctingall model,agenerativemodelforquestiongeneration,theproduced
theselectednewspaperpageswouldhaverequiredsubstantialre- questionsmayexhibitvariousissues,suchasduplication,explicit
sources.Instead,werandomlyselected20newspaperpagesfrom answerdisclosurewithinthequestion,unresolvedpronouns,and
eachstateacrossAmericaforeachdecadefrom1800to1920.This more.Toaddresstheseissues,weimplementedamulti-stepfiltering
resultedinaselectionof8,419newspaperpagesforOCRtextcor- processtorefinethedataset.Theprocessincludes:
rection.Wethensplitthecontentofthese8,419newspaperpages (1) SyntacticFiltering:Inthisinitialstep,weperformfivekey
intoparagraphs,yieldingatotalof205,068paragraphs.Wenext actionstorefinethegeneratedquestion-answerpairs.We
usedthefollowingprompttoinstructGPT3.5Turbotocorrect startbyeliminatingquestionsthatdonotconcludewitha
theseparagraphs: questionmark,followedbyremovingquestionsthatreveal
"YouareprovidedwithahistoricalEnglishtext.Theprovidedtext theanswerwithinthequestionitself(answerleakage)and
hasalotofspellingmistakes.Correctonlythemistakesintheprovided eliminatingduplicates.Wealsofilteroutquestionsthatare
textandwritethecorrectedtextinONEparagraph.Ifyoucannot excessivelylongorshort,aswellasthosewithtoomanyor
correctthemistakes,reply,"Notabletocorrect.". toofewnumberofnamedentities.Additionally,questions
Inthismanner,wetaskedGPT3.5withcorrecting205,638para- withunclearpronounsareremoved.Thisstepresultedin
graphs.Outofthese,GPTwasabletocorrect163,021.Weelim- removing1,221,533questionsfromourinitialpoolofgener-
inatedtheuncorrectedparagraphs(42,599)fromourcorpusand atedquestions.
utilizedthecorrectedparagraphsforquestiongeneration.Among (2) TemporalExpressionTransforming:Followingsyntactic
thecorrectedparagraphs,433containedphrasessuchas"Mostof filtering,weconverttherelativetemporalinformationin
themistakesinthegiventexthavebeencorrected.Hereisthecorrected questionsandanswersintoabsolutetemporalinformation.
text:"and"Errorsinthegiventexthavebeencorrected.Hereisthe Forthistransformation,weutilizeHeidelTime[48]tempo-
correctedtext:".Weremovedsuch96differentphrases,cleanedthe raltagger.Weusethepublicationdateofthenewspaperto
paragraphs,andpreparedthemforquestiongeneration.Figure4 whichtheparagraphbelongs,alongwiththegeneratedques-
illustratesthenumberofcorrectedparagraphsperdecade. tionforconvertingtherelativetemporalexpressions.For
instance,aquestionlikeHowmanysubscribersdidJackson
loseonSaturday?istransformedtoHowmanysubscribersdid
5https://platform.openai.com/docs/models/gpt-3-5-turbo
6Themaximumoutputtokenlengthofgpt-3.5-turbo-1106is4096. 7https://github.com/explosion/spaCy
shpargaraP
fo .oNSIGIR’24,July14–18,2024,Washington,USA Piryani,etal.
Number of newspaper pages
300
250
200
150
100
PERSON: 27.38% GPE: 9.02% PERSON: 26.71% FAC: 4.64%
CARDINAL: 22.02% OTHER: 3.45% GPE: 23.04% CARDINAL: 4.57%
DATE: 20.09% MONEY: 2.26% ORG: 17.22% NORP: 4.05%
ORG: 15.78% OTHER: 8.6% LOC: 2.43%
Figure 5: Distribution of Newspaper pages in Chroni- DATE: 6.7% PRODUCT: 2.04%
clingAmericaQAacrossthestatesofAmerica.
Figure6:Left:Answer’snameentitydistribution,Right:Ques-
tion’snamedentitydistribution
JacksonloseonOctober60,1832?Similarly,wealsotransform
thetemporalinformationinanswertoabsolutetemporal
information.Someexamplesofthetransformedquestions
the101lots50Dr.100hisathethatathecontinuereceivehavesellmeetholdgivetheberemoverdeceivedjruestturnaedbtheeen
a an nd da spn esw cifier cs ita yre insh teo mw pn oi rn alT ra eb fl ee re2 n. cT eh sis ws it te hp oe un rs du are tad secl ta .rity memdoblellaro srtssvotea screm
sentimesofpeople
madesoldsaid wrotewill has
JudgenGoetneralaDr.J tohnMrs.Mr.he
did
(3) TypeMatching:Thethirdandfinalfilteringstep,involved the
identifyingandmatchingthetypesofquestionsandanswers. is
Forthispurpose,wefinetinedRoBERTa[31]-alargelan- many Who
guagemodelusingtheTRECQuestionclassifierdataset[30]
was the
todevelopaquestiontypeclassifier8.Thequestionclassifier How
modelclassifiesthequestiontypeintofourmajortypesand
50minortypes.Similarly,foranswertypeclassification,we d hi ad s long WhW ih co hse
1u
t
a
nh
,nt
o
2ei
s
tl 0i
wq
5z
tu
h
,e
e
4ed
er
8s
2ps stip
aao
qma
in
urC
s
eeay
.
sn
w
t’ Qds ioeun
a
nrena
e
-ssm
at
ew
ni
le
o
sied
mn
wre
it
ea
nyn
rnp
at
d
pi tet
e
a,y
a
d
iw
n
rr
.
ssee
I
.wc nto
h
e
tg
e
r
hnn ipi smz ae
sia
r
tr
t
es.
c
pA
h
w
,f
e
h
wt de
o
er
t
shi
e
ed
e
lte
iq
y
mn
u
pt iei
e
nf sy
at
wii ton
ean
dg
s-
ddo ides
fisrom
thf
em aru tc hh
eis
was
city state count pr arty
yW
yearcompan py
oliticalhat
dW idhen
waA slon
wig
lliI wsern eO doeswn
itw
hhaw that ys etatpc aceoaiutryntmtyecrhmaybm rceoburanreitlsrrrtyoa'atsed'sestasthtofedouuernaseerbeetrsohaildadelnybndiclle
day
hasiswillishaswillhaissdidinhasisowfapsassedcogaomvmebransmsaednpotorlciclamyriemlaattiiorssnlanoadlsdvepiaelrsaconnwynaeiscstsdelegagoetlevliegeocirtsnniloaontrbDuoreusamrtnilardoetoacetarrwadyotswasiswcaausswewdaassiswasis
4 DATASETANALYSIS
isdiddidisdid
didwasisparty the
aMrs.Mr.JohnDr.MissJudgeJ.
the
thethethethetheNewJohnEnSgtl.anFdra,nMcees,srJsa.mesJ.
4.1 DataStatistics Figure7:TypesofquestionscoveredinChroniclingAmeri-
After applying all the filtering steps, we arrived at our cleaned caQA.Weshowthetrigramprefixesofourquestions.
question-answerdatasetconsistingof485Kquestion-answerpairs
derivedfromhistoricalAmericannewspapers.InFigure5,weshow
question.InFigure7,weshowthedataset’sdistributionoftheques-
the distribution of newspaper pages in our dataset across vari-
tiontypes.Fromthefigure,wecanseethattheChroniclingAmeri-
ousstatesofAmerica.Werandomlysplitthedataintotraining,
caQAdatahasaratherfairdistributionamongdifferentquestion
development,andtestsetstofacilitateacomprehensivetraining
types.Table4showsafewexamplesfromtheChroniclingAmeri-
andevaluationofthemodels.Thetrainingsetconsistsof439,302
caQAalongwiththecorrespondingtitlesofthenewspapers,their
question-answerpairs,whereasthedevelopmentandtestsetscom-
publicationdates,andthestatesinwhichthenewspaperswere
prise24,111and24,084,respectively.Whenrandomlyselectingthe
published.
questionsfordevelopmentandtestsets,wetriedtobalancethe
question-answerpairsacrossallthedecades.Wedescribemore
4.2 ModelPerformance
detailsaboutthedatasetstatisticsinTable3.Ourentiretrainsetis
quitelarge,whichmakesitcomputationallyexpensivetofine-tune Toevaluateourdataset,weemploythreemodelsfromthetrans-
variouspre-trainedmodelssuchasBERT[13],RoBERTa[31],T5 formerfamily,namelyBERT-base,RoBERTa-base,andT5-large.We
[41];therefore,werandomlyselectasubsetofquestion-answer assessourdatasetusingfourdifferentvariationsofeachmodel.
(111,517)pairsfromthetrainsetandusethissubsetofquestions Thevariationsareasfollows:
fortrainingvariousmodelswhoseresultsareshowninSection4.2. • BERT-base9 :withoutanyfine-tuning.
Wealsoanalyzethedistributionofthenamedentitytypesof • BERT-base-SQuAD10 :fine-tunedontheSQuAD1.1dataset.
answersandquestionsinthedataset.IntheleftpiechartofFigure • BERT-base-ChroniclingAmericaQA:fine-tunedontheChron-
6,wecanobservethenameentitytypesoftheanswers,whereasthe iclingAmericaQAdataset.
rightpiechartshowsthedistributionofnamedentitytypesofthe
9https://huggingface.co/bert-base-uncased
8Theaccuracyofthequestiontypeclassifieris92.8%. 10https://huggingface.co/csarron/bert-base-uncased-squad-v1ChroniclingAmericaQA:ALarge-scaleQuestionAnsweringDatasetbasedonHistoricalAmericanNewspaperPages SIGIR’24,July14–18,2024,Washington,USA
Table2:Examplesoftransformedquestions-answerpairs.Underlinedtextshowsthetransformedinformation.
No. OriginalQuestion-answerpair TransformedQuestion-answerpair
Howmanypeoplediedlastyearduetofamine Howmanypeoplediedin1822duetofamineand
1.
anddiseaseinMexicoCity?(nearly500) diseaseinMexicoCity?nearly500
HowmanyBritishstockswerequoted HowmanyBritishstockswerequoted
2.
atyesterday’sprices?(TwoBankStock) atNovember15,1830’sprices?(TwoBankStocks)
OnwhatdaydidtheBoardofCommissionersofRoads OnwhatdaydidtheBoardofCommissionersofRoads
3.
holdtheirsemi-annualmeetings?(Monday) holdtheirsemi-annualmeetings?(October20,1856)
WhenwastheSecretaryofState’sreportto WhenwastheSecretaryofState’sreportto
4.
thePresidentmade?(the24thDecember,1817) thePresidentmade?(December24,1817)
Table3:BasisstatisticsofChroniclingAmericaQA. exactmatch(EM)andF1score.Wemeasurehowwelldifferent
modelscananswerquestionsfrombothCorrectedandRawOCR
paragraphs.Fromtheresults,itisevidentthatmodelsusedwithout
Train Dev Test
#QApairs 439,302 24,111 24,084 anyfine-tuningperformedtheworst,asthesemodelslackspecific
Averageparagraphlength(words) 220.09 218.40 217.66 knowledgeaboutthequestion-answeringtask.Conversely,models
Averagequestionlength(words) 11.05 11.30 11.184 fine-tunedontheSQuAD1.1datasetshowsignificantimprove-
Averageanswerlength(words) 2.01 1.978 1.981 ments, highlighting the benefits of fine-tuning on task-specific
Averagequestionsperparagraph 3.48 1.90 1.90
datasets.Notably,thehighestperformancelevelsareobservedfrom
Averagequestionpernewspaperpage 54.80 4.07 4.08
themodelstrainedontheChroniclingAmericaQAdatasetitself,
depictingtheimportanceoffine-tuningonspecificdatasets.How-
ever,modelstrainedonbothSQuADandChroniclingAmericaQA
• BERT-base-SQuAD-ChroniclingAmericaQA:fine-tunedon
giveevenbetterperformance,suggestingthatcombiningdomain-
bothSQuAD1.1andChroniclingAmericaQAdatasets.
• RoBERTa-base11 :withoutanyfine-tuning. specific knowledge such as historical content and general task
• RoBERTa-base-SQuAD12 :fine-tunedontheSQuAD1.1dataset. specificknowledgecanevenenhancetheperformanceofmodels.
WealsocomparetheresultsoftransformermodelsonbothRaw
• RoBERTa-base-ChroniclingAmericaQA:fine-tunedonthe
OCRparagraphsandCorrectedOCRparagraphsinTable5.From
ChroniclingAmericaQAdataset.
theresults,wecananalyzethatmodelperformancesignificantly
• RoBERTa-base-SQuAD-ChroniclingAmericaQA:fine-tuned
dropswhentheRawOCRparagraphisgivenascontext,suggesting
onbothSQuAD1.1andChroniclingAmericaQAdatasets.
• T5-large13 :withoutanyfine-tuning. theimportanceofcleaningthedigitizedtext.Wealsoshowthe
• T5-large-SQuAD14 :fine-tunedontheSQuAD1.1dataset. performanceoffine-tunedmodelsovertimeinFigure8.Itcanbe
concludedfromthefigurethattheperformanceimprovementdue
• T5-large-ChroniclingAmericaQA:fine-tunedontheChron-
tofinetunningonourdatasetremainsrelativelystableovertime.
iclingAmericaQAdataset.
Insummary,wecanconcludethatresultsshownintable5ensure
• T5-large-SQuAD-ChroniclingAmericaQA: fine-tuned on
fine-tuningmodelsonrelevantdatasetssignificantlyimprovethe
bothSQuAD1.1andChroniclingAmericaQAdatasets.
performance.Additionally,wecanalsorealizethatcorrectingthe
OCRedtextiscrucialforgettingbettermodelperformance.
100 100 4.3 LLMPerformance
BERT-base-SQuAD
BERT-base-SQuAD-ChroniclingAmericaQA
RoBERTa-base-SQuAD Similarly,inadditiontotransformermodels,weevaluateourdataset
80 80 RoBERTa-base-SQuAD-ChroniclingAmericaQA
withLargeLanguageModels(LLMs),includingtheLLaMA2[3]
60 60 family,Mixtral8x7B[22]andMistral7B[21].Evaluatingtheperfor-
manceofLLMs[17]withinthecontextofquestionansweringposes
40 40 uniquechallenges.Traditionally,QAsystemsareassessedusing
ExactMatch(EM)andF1scores.However,thesemetricsarenot
20 20
fullyappropriateforgenerativemodelssuchasLLMs,whichoften
produceverboseresponses.ThisverbositycanresultinanEMvalue
1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910
ofzeroduetothepresenceofextratokensintheanswer.Similarly,
Decades Decades
theF1score,whichevaluatesbothrecallandprecision,maynot
Figure8:ComparisonofModelPerformanceoverTime.
provideanaccuratemeasureofperformanceunderthesecondi-
tions.ToovercomethechallengeofevaluatingLLMs,weintroduce
Table5summarizestheperformanceofdifferenttransformers model-agnosticmetricslikeTokenRecallandAnswerStringCon-
ontheChroniclingAmericaQAdatasetusingstandardQAmetrics: tainment15,whicharemoresuitableforassessingverboseresponses.
TokenRecallevaluateshoweffectivelythemodel’sresponsecovers
11https://huggingface.co/FacebookAI/roberta-base
theinformationfoundinthegroundtruth,whileAnswerString
12https://huggingface.co/deepset/roberta-base-squad2
13https://huggingface.co/google-t5/t5-large
14https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer 15https://huggingface.co/spaces/evaluate-metric/squad
hctaM
tcaxE 1FSIGIR’24,July14–18,2024,Washington,USA Piryani,etal.
Table4:ExamplesofquestionsinChroniclingAmericaQA.
No. Question Answer NewspaperTitle PublicationDate State
Whicharmywasmarchingslowlytowards
1. French ThePortlandGazette 1823-03-17 Maine
Madrid?
Whowassheriffofthecounty
2. JesseWinn DailyRichmondWhig 1830-08-12 Virginia
ofHanover?
OnwhatdaywilltheAntiSlaverySociety
3. November19,1840 HeraldoftheTimes 1840-11-19 RhodeIsland
ofNewportholdtheirquarterlymeeting?
AlongwithTexas,inwhatstateistherush
4. California TheTexasRepublican 1850-02-21 Texas
ofimmigrantsprodigious?
HowmuchistheinterestrateinSeattle
5. 10percent TheSeattlePost-intelligencer 1892-09-02 Washington
propertyloans?
Whowastheagencythatran
6. RedCloudIndian SierraCountyAdvocate 1902-06-27 NewMexico
thehorsesfortheSiouxIndians?
7. Whowasfined$1bythemayor? Parker TheMarionDailyMirror 1907-07-30 Ohio
8. WhatclubdidFrankBishopbelongto? theNationalAthleticClub SierraCountyadvocate 1914-04-01 Montana
Table5:ModelPerformanceofChroniclingAmericaQADatasetusingCorrectedOCRParagraphsandRawOCRParagraphsas
context.RedcolorednumbersdenotepercentagedecreasewhenusingRawOCRparagraphs.Theunderlinedresultsdepict
whichvariationofthemodelgavethebestperformancecomparedtoitscounterparts,whereastheboldresultsshowtheoverall
bestperformanceonthegivenmeasure.
Model CorrectedOCRParagraph RawOCRParagraphs
EM F1 EM F1
BERT-base 0.12 2.91 0.08(33%↓) 2.27(22%↓)
BERT-base-SQuAD 44.70 57.24 27.14(39%↓) 40.33(29%↓)
BERT-base-ChroniclingAmericaQA 63.29 69.43 38.94(38%↓) 48.65(30%↓)
BERT-base-SQuAD-ChroniclingAmericaQA 63.90 69.92 39.71(37%↓) 49.45(29%↓)
RoBERTa-base 0.02 1.48 0.02(0%↓) 1.32(10%↓)
RoBERTa-base-SQuAD 47.52 60.55 29.83(38%↓) 42.71(29%↓)
RoBERTa-base-ChroniclingAmericaQA 63.61 70.72 40.40(36%↓) 50.12(29%↓)
RoBERTa-base-SQuAD-ChroniclingAmericaQA 63.93 71.00 40.42(36%↓) 50.12(29%↓)
T5-large 46.69 59.77 26.66(43%↓) 36.90(38%↓)
T5-large-SQuAD 1.50 9.28 0.8(46%↓) 6.07(34%↓)
T5-large-ChroniclingAmericaQA 64.28 69.65 34.49(46%↓) 41.54(40%↓)
T5-large-SQuAD-ChroniclingAmericaQA 64.24 59.77 34.30(47%↓) 41.53(18%↓)
Table6:LLMsPerformanceonChroniclingAmericaQAusingCorrectedOCRParagraphsandRawOCRParagraphsascontext.
RedcolorednumbersdenotepercentagedecreasewhenusingRawOCRparagraphs.Theboldvaluesrepresentoverallbest
performanceforagivenmeasureonCorrectedOCRParagraphs,whereastheunderlinedvaluesrepresentthebestperformance
forthegivenmeasureonRawOCRParagraphs.
Model Parameter EM F1 Recall Precision Contains
LLaMA2(CleanedOCRParagraphs) 7B 0.22 12.04 57.26 7.08 44.39
LLaMA2(RawOCRParagraphs) 7B 0.00(100%↓) 2.24(81%↓) 10.45(82%↓) 1.32(81%↓) 5.67(87%↓)
LLaMA2(CleanedOCRParagraphs) 13B 0.00 11.89 58.36 6.86 45.80
LLaMA2(RawOCRParagraphs) 13B 0.00(0%↓) 2.16(82%↓) 10.59(81%↓) 1.27(81%↓) 5.68(87%↓)
LLaMA2(CleanedOCRParagraphs) 70B 5.30 19.52 61.38 14.17 48.37
LLaMA2(RawOCRParagraphs) 70B 0.39(93%↓) 3.02(84%↓) 11.19(82%↓) 2.00(86%↓) 6.35(87%↓)
Mixtral(CleanedOCRParagraphs) 8x7B 1.13 12.88 62.69 8.15 50.00
Mixtral(RawOCRParagraphs) 8x7B 0.00(100%↓) 0.94(92%↓) 10.06(84%↓) 0.51(94%↓) 5.63(89%↓)
Mistral(CleanedOCRParagraphs) 7B 1.17 16.30 56.35 10.42 44.16
Mistral(RawOCRParagraphs) 7B 0.09(92%↓) 2.08(87%↓) 8.19(85%↓) 1.30(85%↓) 4.33(90%↓)
Containmentassessesthedegreetowhichthegroundtruthisin- Table6presentstheperformanceofdifferentLLMsontheChron-
cludedwithinthemodel’sresponse.Byusingthesemetrics,weaim iclingAmericaQADatasetusingbothrawOCRandcorrectedpara-
toofferamoreequitableevaluationofanLLM’sabilitytogenerate graphs.WhenevaluatingtheresultsofdifferentLLMs,wecannote
responsestoquestions.ChroniclingAmericaQA:ALarge-scaleQuestionAnsweringDatasetbasedonHistoricalAmericanNewspaperPages SIGIR’24,July14–18,2024,Washington,USA
Table7:Humanevaluationresults. 5 USECASES
Ourdatasetcanbeusedinseveralways.First,itprovidesanew
Readabilityof Readability Relevance Non-Ambiguity benchmarkfortrainingandevaluatingQAandMRCmodelson
Question ofAnswer
historicaltexts,enablingmodelstohandlethecomplexitiesofhis-
4.24 4.29 4.39 4.18
toricaltext,includingOCRinaccuraciesandlanguageevaluation.
Theevaluationcanbedonefordifferentspansoftimewithinthe
timeframeofthedata(1800-1920).Itcanbealsoconditionedon
thatLLaMA270BgivessignificantlyhighEMandF1scorescom- differentregionsofUSAorparticularstates.
paredtoitscounterpartsandMistralmodels.Thisalsosuggests Second,sinceourdatasetcontainsscannedimagesofnewspa-
thatmodelswithlargesizeshaveagreaterabilitytounderstand persaswellasthecorrespondingrawandcleanedOCRtexts,it
thenuancesofhistoricaltext,leadingtomoreaccurateandprecise ispossibletobenchmarkmodelsoneachofthese.Somememory
answers.Meanwhile,theMixtral8x7bmodelperformshighlyon institutionsmaystillholdonlyscansofhistoricalmaterials,while
RecallandContains,depictingitsproficiencyinextractingrelevant others,suchasinthecaseofChroniclingAmerica,maycontain
informationfromthecontext. quitenoisyOCRedtext.Ourdatasetallowsthenbenchmarking
Furthermore, When examining the results of different LLMs modelsonthesedifferenttypesofinputs,offeringrealisticscenarios
consideringtherawOCRparagraphandcorrectedparagraph,we inwhichthemodelsmayneedtoberequiredtooperate.
cannotethatperformanceacrossallmodelsdecreaseddrastically Third,thedatasetcanbeusedtoincreasethepublic’sengage-
whenutilizingtherawOCRparagraphascontext.Modelssuch mentwithhistoricaldocuments.Forexample,itcanhelppeople
asLLaMA27BandMixtralBx7BgavezeroresultsonEMmetric, buildskillsincriticalreadingandlanguagearts.
whereasLLaMA270BstillshowedsomeresistanceonrawOCR Finally,ChroniclingAmericaQAcouldpotentiallyserveasan
paragraph,however,withasignificantdropof93%.Furthermore, educational resource; for example, it could be used by teachers
Whenanalyzingtherecallandprecisionmetricsresults,onecan andeducatorstoassessstudents’readingcomprehensiononrare,
notethatmodelsshowedbetterperformanceonrecall,indicating historicalmaterials.Note,thatourdatasetgenerationframework
theirabilitytocapturerelevantinformationfromthecontext. canbeappliedonotherhistoricalnewscollections.
SummarizingtheresultsofTable6,wecanconcludethatthe
performanceofLLaMA270Bishighcomparedtoothermodels
forhandlingbothrawOCRtextandcorrectedtext.Ontheother
hand,Mixtral8x7Bgiveshighresultsforrecallandcontainsmetrics
oncorrectedOCRparagraphs.However,LLaMa270Bgivesahigh 6 CONCLUSION
recallandcontainsvalueontherawOCRparagraph. Inthiswork,weintroduceChroniclingAmetricaQA,alarge-scale
Overall,wecanconcludethattheresultsofbothTables5and6 question-answeringdatasetcomprising485kquestion-answerpairs
emphasizethatfine-tuningthemodelontherelevantdatasetsplays overacollectionofhistoricalAmericannewspaperswiththeobjec-
amajorroleinincreasingthemodelperformance.Additionally,it tiveoffacilitatingthedevelopmentofQAandMRCsystemsover
highlightsthenecessityofpost-processingOCRtextforimproving historicaltexts.Ourdatasetisuniqueasitisspecificallybasedon
theperformanceofmodelsonhistoricaldocuments,astheresults historicalnewspaperscoveringthelongesttimeperiodof120years
ofbothtransformermodelsandLLMsdegradedrasticallyforraw from1800to1920years,amongotheravailabledatasetsonnews
OCRtext. documents.Wehighlightthedifficultiesofutilizinghistoricalnews-
papersforQAtasks,suchasthepoorqualityofOCRtextandthe
4.4 HumanEvaluation needforpost-processingtoenhancetextquality.Weemployedlan-
ToassessthequalityoftheChroniclingAmericaQAdataset,we guagemodelssuchasGPT3.5TurboforOCRtextcorrectionandthe
conductedamanualevaluationstudy.Werandomlyselected360 T5-basemodelforquestiongenerationtomitigatethesechallenges.
question-answerpairsforevaluation,sampling30pairsfromeach Wealsocarriedoutacomprehensiveevaluationofourdataseton
decaderepresentedinthedataset.Sixgraduatestudentswereasked varioustransformermodelsandLLMs,demonstratingtheeffec-
toratethesepairsonascaleof1to5,with1being"verybad" tivenessofourdataset.ModelstrainedonChroniclingAmericaQA
and5being"verygood."Theevaluatorswereinstructedtoassess consistentlysurpasstheperformanceofthosefine-tunedonother
the pairs based on four criteria: the readability of the question, datasets.
ensuringitisgrammaticallycorrectandflowswell;thereadability EthicalconsiderationsLastly,wewouldliketomentionethical
oftheanswer,verifyingitsgrammaticalcorrectnessandfluency; considerationsofourwork.Sincethedatasethasbeencreatedfrom
relevance,toascertainifthegeneratedquestionpertainstothe temporallydistantdata,thereispossibilitythatsomequestions
providedpassage;andnon-ambiguity,toensurethequestionis maybeoffensiveorbiasedinrelationtosomeethnicgroupsor
straightforwardandclear. othersegmentsofsociety.Wemadesurethatthepossibilityof
Eachcriterion’shumanevaluationresultswereaveragedand suchquestionsinourdatasetislowbyperformingpost-processing
presentedinTable7.Theevaluationsyieldedhighscoresacross searchusingasetofpreparedkeywordscoupledwithcarefulman-
allmetrics,indicatingthatourdatasetisofhighquality,partic- ualanalysis.Nevertheless,thiskindofissueisnotspecifictoour
ularlyregardingtherelevanceandreadabilityofbothquestions datasetbutisaproblemtypicalinmanyheritagematerials[9],
andanswers.Thescorefornon-ambiguitywasalsonotablyhigh, includingalsoonesintheChroniclingAmericarepository,and
demonstratingthatmostquestionsaredirectandunambiguous. shouldbecontinuouslystudiedandmitigatedinthefuture.SIGIR’24,July14–18,2024,Washington,USA Piryani,etal.
REFERENCES [19] ChristianHardmeier.2016.ANeuralModelforPart-of-SpeechTagginginHis-
[1] 2007. ReferenceReviews21,7(Sept.2007),52–53. https://doi.org/10.1108/ toricalTexts.InProceedingsofCOLING2016,the26thInternationalConference
09504120710821875 onComputationalLinguistics:TechnicalPapers,YujiMatsumotoandRashmi
[2] 2021.Surveyofcomputationalapproachestolexicalsemanticchangedetection. Prasad(Eds.).TheCOLING2016OrganizingCommittee,Osaka,Japan,922–931.
Computationalapproachestosemanticchange6,1(2021). https://aclanthology.org/C16-1088
[3] 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. [20] AdamJatowt,MickaelCoustaty,Nhu-VanNguyen,AntoineDoucet,etal.2019.
arXiv:2307.09288[cs.CL] DeepstatisticalanalysisofOCRerrorsforeffectivepost-OCRprocessing.In2019
[4] HaithemAfli,LoïcBarrault,andHolgerSchwenk.2015.OCRErrorCorrection ACM/IEEEJointConferenceonDigitalLibraries(JCDL).IEEE,29–38.
UsingStatisticalMachineTranslation. [21] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De-
[5] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu, vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,
RanganMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,etal.2016. GuillaumeLample,LucileSaulnier,etal.2023. Mistral7B. arXivpreprint
Msmarco:Ahumangeneratedmachinereadingcomprehensiondataset.arXiv arXiv:2310.06825(2023).
preprintarXiv:1611.09268(2016). [22] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,Blanche
[6] GuilhermeTorresanBazzo,GustavoAcauanLorentz,DannySuarezVargas,and Savary,ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBou
VivianeP.Moreira.2020. AssessingtheImpactofOCRErrorsinInformation Hanna, Florian Bressand, et al. 2024. Mixtral of Experts. arXiv preprint
Retrieval.InAdvancesinInformationRetrieval:42ndEuropeanConferenceon arXiv:2401.04088(2024).
[23] TomášKočiský,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHer-
IRResearch,ECIR2020,Lisbon,Portugal,April14–17,2020,Proceedings,PartII
(Lisbon,Portugal).Springer-Verlag,Berlin,Heidelberg,102–109. https://doi.org/ mann,GáborMelis,andEdwardGrefenstette.2018.TheNarrativeQAReading
10.1007/978-3-030-45442-5_13 ComprehensionChallenge. TransactionsoftheAssociationforComputational
[7] MarcelBollmann.2019.ALarge-ScaleComparisonofHistoricalTextNormal- Linguistics6(2018),317–328. https://doi.org/10.1162/tacl_a_00023
izationSystems.InProceedingsofthe2019ConferenceoftheNorthAmerican [24] LauraKorkeamäkiandSannaKumpulainen.2019. InteractingwithDigital
Documents:ARealLifeStudyofHistorians’TaskProcesses,ActionsandGoals.
ChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-
nologies,Volume1(LongandShortPapers),JillBurstein,ChristyDoran,and https://doi.org/10.1145/3295750.3298931
ThamarSolorio(Eds.).AssociationforComputationalLinguistics,Minneapolis, [25] MarkusKrug,FrankPuppe,FotisJannidis,LuisaMacharowsky,IsabellaReger,
Minnesota,3885–3898. https://doi.org/10.18653/v1/N19-1389 andLukasWeimar.2015.Rule-basedCoreferenceResolutioninGermanHistoric
[8] MarcelBollmann,AndersSøgaard,andJoachimBingel.2018.Multi-tasklearning Novels.InProceedingsoftheFourthWorkshoponComputationalLinguisticsfor
forhistoricaltextnormalization:Sizematters.InProceedingsoftheWorkshop Literature,AnnaFeldman,AnnaKazantseva,StanSzpakowicz,andCorinaKoolen
onDeepLearningApproachesforLow-ResourceNLP,RezaHaffari,ColinCherry, (Eds.).AssociationforComputationalLinguistics,Denver,Colorado,USA,98–104.
GeorgeFoster,ShahramKhadivi,andBaharSalehi(Eds.).AssociationforCom- https://doi.org/10.3115/v1/W15-0711
putationalLinguistics,Melbourne,19–24. https://doi.org/10.18653/v1/W18-3403 [26] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,Ankur
[9] NadavBorenstein,KarolinaStanczak,TheaRolskov,NatachaKleinKäfer,Natália Parikh,ChrisAlberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,KentonLee,
daSilvaPerez,andIsabelleAugenstein.2023. MeasuringIntersectionalBi- KristinaToutanova,LlionJones,MatthewKelcey,Ming-WeiChang,AndrewM.
asesinHistoricalDocuments.InFindingsoftheAssociationforComputational Dai,JakobUszkoreit,QuocLe,andSlavPetrov.2019. NaturalQuestions:A
Linguistics:ACL2023,AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki BenchmarkforQuestionAnsweringResearch.TransactionsoftheAssociationfor
(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,2711–2730. ComputationalLinguistics7(2019),452–466. https://doi.org/10.1162/tacl_a_00276
https://doi.org/10.18653/v1/2023.findings-acl.170 [27] VietDacLai,MinhVanNguyen,HeidiKaufman,andThienHuuNguyen.2021.
[10] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, EventExtractionfromHistoricalTexts:ANewDatasetforBlackRebellions.
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda InFindingsoftheAssociationforComputationalLinguistics:ACL-IJCNLP2021,
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural ChengqingZong,FeiXia,WenjieLi,andRobertoNavigli(Eds.).Associationfor
informationprocessingsystems33(2020),1877–1901. ComputationalLinguistics,Online,2390–2400. https://doi.org/10.18653/v1/2021.
[11] GuillaumeChiron,AntoineDoucet,MickaëlCoustaty,MurielVisani,andJean- findings-acl.211
PhilippeMoreux.2017. ImpactofOCRerrorsontheuseofdigitallibraries: [28] BenjaminCharlesGermainLee,JaimeMears,EileenJakeway,MeghanFerriter,
towardsabetteraccesstoinformation.InProceedingsofthe17thACM/IEEEJoint ChrisAdams,NathanYarasavage,DeborahThomas,KateZwaard,andDanielS.
ConferenceonDigitalLibraries(Toronto,Ontario,Canada)(JCDL’17).IEEEPress, Weld.2020.TheNewspaperNavigatorDataset:ExtractingHeadlinesandVisual
249–252. Contentfrom16MillionHistoricNewspaperPagesinChroniclingAmerica.In
[12] LucasLimadeOliveira,DannySuarezVargas,AntônioMarceloAzevedoAlexan- Proceedingsofthe29thACMInternationalConferenceonInformation&Knowledge
dre,FábioCorrêaCordeiro,DiogodaSilvaMagalhãesGomes,MaxdeCastro Management(VirtualEvent,Ireland)(CIKM’20).AssociationforComputing
Rodrigues,RegisKruelRomeu,andVivianePereiraMoreira.2023.Evaluating Machinery,NewYork,NY,USA,3055–3062. https://doi.org/10.1145/3340531.
andmitigatingtheimpactofOCRerrorsoninformationretrieval.Int.J.Digit. 3412767
Libr.24,1(jan2023),45–62. https://doi.org/10.1007/s00799-023-00345-6 [29] AdamD.Lelkes,VinhQ.Tran,andCongYu.2021. Quiz-StyleQuestionGen-
[13] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT: erationforNewsStories.InProceedingsoftheWebConference2021(Ljubljana,
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In Slovenia)(WWW’21).AssociationforComputingMachinery,NewYork,NY,
USA,2501–2511. https://doi.org/10.1145/3442381.3449892
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand
[30] XinLiandDanRoth.2002.LearningQuestionClassifiers.InCOLING2002:The
ShortPapers),JillBurstein,ChristyDoran,andThamarSolorio(Eds.).Association 19thInternationalConferenceonComputationalLinguistics. https://aclanthology.
forComputationalLinguistics,Minneapolis,Minnesota,4171–4186. https://doi. org/C02-1150
org/10.18653/v1/N19-1423 [31] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer
[14] XinyaDu,JunruShao,andClaireCardie.2017.LearningtoAsk:NeuralQuestion Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.RoBERTa:A
GenerationforReadingComprehension.InProceedingsofthe55thAnnualMeeting RobustlyOptimizedBERTPretrainingApproach.ArXivabs/1907.11692(2019).
oftheAssociationforComputationalLinguistics(Volume1:LongPapers),Regina https://api.semanticscholar.org/CorpusID:198953378
BarzilayandMin-YenKan(Eds.).AssociationforComputationalLinguistics, [32] MengsayLoem,MasahiroKaneko,ShoTakase,andNaoakiOkazaki.2023.Ex-
Vancouver,Canada,1342–1352. https://doi.org/10.18653/v1/P17-1123 ploringEffectivenessofGPT-3inGrammaticalErrorCorrection:AStudyon
[15] MaudEhrmann,AhmedHamdi,ElvysLinharesPontes,MatteoRomanello,and PerformanceandControllabilityinPrompt-BasedMethods.InProceedingsofthe
AntoineDoucet.2023.Namedentityrecognitionandclassificationinhistorical 18thWorkshoponInnovativeUseofNLPforBuildingEducationalApplications
documents:Asurvey.Comput.Surveys56,2(2023),1–47. (BEA2023),EkaterinaKochmar,JillBurstein,AndreaHorbach,RonjaLaarmann-
[16] MaudEhrmann,MatteoRomanello,AlexFlückiger,andSimonClematide.2020. Quante,NitinMadnani,AnaïsTack,VictoriaYaneva,ZhengYuan,andTorsten
OverviewofCLEFHIPE2020:Namedentityrecognitionandlinkingonhistor- Zesch(Eds.).AssociationforComputationalLinguistics,Toronto,Canada,205–
icalnewspapers.InExperimentalIRMeetsMultilinguality,Multimodality,and 219. https://doi.org/10.18653/v1/2023.bea-1.18
[33] WilliamB.LundandEricK.Ringger.2009.Improvingopticalcharacterrecog-
Interaction:11thInternationalConferenceoftheCLEFAssociation,CLEF2020,
Thessaloniki,Greece,September22–25,2020,Proceedings11.Springer,288–310. nitionthroughefficientmultiplesystemalignment.InProceedingsofthe9th
[17] ZishanGuo,RenrenJin,ChuangLiu,YufeiHuang,DanShi,LinhaoYu,YanLiu, ACM/IEEE-CSJointConferenceonDigitalLibraries(Austin,TX,USA)(JCDL
JiaxuanLi,BojianXiong,DeyiXiong,etal.2023. EvaluatingLargeLanguage ’09). Association for Computing Machinery, New York, NY, USA, 231–240.
Models:AComprehensiveSurvey.arXivpreprintarXiv:2310.19736(2023). https://doi.org/10.1145/1555400.1555437
[18] AhmedHamdi,ElvysLinharesPontes,NicolasSidère,MickaëlCoustaty,and [34] StephenMutuvi,AntoineDoucet,MosesOdeo,andAdamJatowt.2018.Evaluat-
AntoineDoucet.2022.In-DepthAnalysisoftheImpactofOCRErrorsonNamed ingtheimpactofOCRerrorsontopicmodeling.InInternationalConferenceon
EntityRecognitionandLinking.JournalofNaturalLanguageProcessing(032022), AsianDigitalLibraries.Springer,3–14.
24. https://doi.org/10.1017/S1351324922000110 [35] RameshNallapati,BowenZhou,CicerodosSantos,ÇağlarGulçehre,andBing
Xiang.2016.AbstractiveTextSummarizationusingSequence-to-sequenceRNNsChroniclingAmericaQA:ALarge-scaleQuestionAnsweringDatasetbasedonHistoricalAmericanNewspaperPages SIGIR’24,July14–18,2024,Washington,USA
andBeyond.InProceedingsofthe20thSIGNLLConferenceonComputational [53] MichihiroYasunaga,JureLeskovec,andPercyLiang.2021.LM-Critic:Language
NaturalLanguageLearning,StefanRiezlerandYoavGoldberg(Eds.).Association ModelsforUnsupervisedGrammaticalErrorCorrection.InProceedingsofthe
forComputationalLinguistics,Berlin,Germany,280–290. https://doi.org/10. 2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Marie-
18653/v1/K16-1028 FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(Eds.).
[36] ThiTuyetHaiNguyen,AdamJatowt,MickaelCoustaty,andAntoineDoucet. AssociationforComputationalLinguistics,OnlineandPuntaCana,Dominican
2021. Surveyofpost-OCRprocessingapproaches. ACMComputingSurveys Republic,7752–7763. https://doi.org/10.18653/v1/2021.emnlp-main.611
(CSUR)54,6(2021),1–37. [54] ShengZhang,XiaodongLiu,JingjingLiu,JianfengGao,KevinDuh,andBen-
[37] TakeshiOnishi,HaiWang,MohitBansal,KevinGimpel,andDavidMcAllester. jaminVanDurme.2018.ReCoRD:BridgingtheGapbetweenHumanandMa-
2016.WhodidWhat:ALarge-ScalePerson-CenteredClozeDataset.InProceed- chineCommonsenseReadingComprehension. CoRRabs/1810.12885(2018).
ingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing, arXiv:1810.12885 http://arxiv.org/abs/1810.12885
JianSu,KevinDuh,andXavierCarreras(Eds.).AssociationforComputational
Linguistics,Austin,Texas,2230–2235. https://doi.org/10.18653/v1/D16-1241
[38] LiangmingPan,WenqiangLei,Tat-SengChua,andMin-YenKan.2019.Recent
AdvancesinNeuralQuestionGeneration. arXiv:1905.08949[cs.CL]
[39] MariaCarolinaPenteadoandFábioPerez.2023.EvaluatingGPT-3.5andGPT-4on
GrammaticalErrorCorrectionforBrazilianPortuguese.InLatinXinAIWorkshop
atICML2023(RegularDeadline). https://openreview.net/forum?id=EOOtS6iSE0
[40] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,
MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020.ExploringtheLimits
ofTransferLearningwithaUnifiedText-to-TextTransformer.JournalofMachine
LearningResearch21,140(2020),1–67. http://jmlr.org/papers/v21/20-074.html
[41] ColinRaffel,NoamM.Shazeer,AdamRoberts,KatherineLee,SharanNarang,
MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2019.ExploringtheLimits
ofTransferLearningwithaUnifiedText-to-TextTransformer.J.Mach.Learn.Res.
21(2019),140:1–140:67. https://api.semanticscholar.org/CorpusID:204838007
[42] PranavRajpurkar,RobinJia,andPercyLiang.2018.KnowWhatYouDon’tKnow:
UnanswerableQuestionsforSQuAD.InProceedingsofthe56thAnnualMeeting
oftheAssociationforComputationalLinguistics(Volume2:ShortPapers),Iryna
GurevychandYusukeMiyao(Eds.).AssociationforComputationalLinguistics,
Melbourne,Australia,784–789. https://doi.org/10.18653/v1/P18-2124
[43] PranavRajpurkar,JianZhang,KonstantinLopyrev,andPercyLiang.2016.
SQuAD:100,000+QuestionsforMachineComprehensionofText.InProceed-
ingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
JianSu,KevinDuh,andXavierCarreras(Eds.).AssociationforComputational
Linguistics,Austin,Texas,2383–2392. https://doi.org/10.18653/v1/D16-1264
[44] AlexanderRobertsonandSharonGoldwater.2018.EvaluatingHistoricalText
NormalizationSystems:HowWellDoTheyGeneralize?.InProceedingsofthe2018
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLin-
guistics:HumanLanguageTechnologies,Volume2(ShortPapers),MarilynWalker,
HengJi,andAmandaStent(Eds.).AssociationforComputationalLinguistics,
NewOrleans,Louisiana,720–725. https://doi.org/10.18653/v1/N18-2113
[45] EvanSandhaus.2008.TheNewYorkTimesAnnotatedCorpus. https://doi.org/
11272.1/AB2/GZC6PL
[46] RacheleSprugnoliandSaraTonelli.2019.NovelEventDetectionandClassifica-
tionforHistoricalTexts.ComputationalLinguistics45,2(June2019),229–265.
https://doi.org/10.1162/coli_a_00347
[47] ChristianM.Strohmaier,ChristophRinglstetter,KlausU.Schulz,andStoyan
Mihov.2003.LexicalpostcorrectionofOCR-results:thewebasadynamicsec-
ondarydictionary?SeventhInternationalConferenceonDocumentAnalysisand
Recognition,2003.Proceedings.(2003),1133–1137. https://api.semanticscholar.
org/CorpusID:12766761
[48] JannikStrötgenandMichaelGertz.2015. ABaselineTemporalTaggerforall
Languages.InProceedingsofthe2015ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,LluísMàrquez,ChrisCallison-Burch,andJianSu(Eds.).
AssociationforComputationalLinguistics,Lisbon,Portugal,541–547. https:
//doi.org/10.18653/v1/D15-1063
[49] NinaTahmasebi,LarsBorin,AdamJatowt,YangXu,andSimonHengchen(Eds.).
2021.Computationalapproachestosemanticchange.Number6inLanguageVari-
ation.LanguageSciencePress,Berlin. https://doi.org/10.5281/zenodo.5040241
[50] AdamTrischler,TongWang,XingdiYuan,JustinHarris,AlessandroSordoni,
PhilipBachman,andKaheerSuleman.2017.NewsQA:AMachineComprehension
Dataset.InProceedingsofthe2ndWorkshoponRepresentationLearningforNLP,
PhilBlunsom,AntoineBordes,KyunghyunCho,ShayCohen,ChrisDyer,Edward
Grefenstette,KarlMoritzHermann,LauraRimell,JasonWeston,andScottYih
(Eds.).AssociationforComputationalLinguistics,Vancouver,Canada,191–200.
https://doi.org/10.18653/v1/W17-2623
[51] DanielAlexandervanStrien,KasparBeelen,MarionaCollArdanuy,KasraHos-
seini,BarbaraMcGillivray,andGiovanniColavizza.2020.AssessingtheImpact
ofOCRQualityonDownstreamNLPTasks.InInternationalConferenceonAgents
andArtificialIntelligence. https://api.semanticscholar.org/CorpusID:215756646
[52] JiexinWang,AdamJatowt,andMasatoshiYoshikawa.2022. ArchivalQA:A
Large-scaleBenchmarkDatasetforOpen-DomainQuestionAnsweringover
HistoricalNewsCollections.InProceedingsofthe45thInternationalACMSIGIR
ConferenceonResearchandDevelopmentinInformationRetrieval(<conf-loc>,
<city>Madrid</city>,<country>Spain</country>,</conf-loc>)(SIGIR’22).As-
sociationforComputingMachinery,NewYork,NY,USA,3025–3035. https:
//doi.org/10.1145/3477495.3531734