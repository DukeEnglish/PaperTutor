AgentStudio: A Toolkit for Building General Virtual Agents
LongtaoZheng1∗,ZhiyuanHuang3∗,ZhenghaiXue1,
XinrunWang1,BoAn1,2,ShuichengYan2
1NTU,Singapore 2SkyworkAI,Singapore 3ETHZurich
https://skyworkai.github.io/agent-studio/
Abstract
Creatingautonomousvirtualagentscapableofusingarbitrarysoftware
onanydigitaldeviceremainsamajorchallengeforartificialintelligence.
Two key obstacles hinder progress: insufficient infrastructure for build-
ing virtual agents in real-world environments, and the need for in-the-
wildevaluationoffundamentalagentabilities. Toaddressthis,weintro-
duceAgentStudio,anonline,realistic,andmultimodaltoolkitthatcovers
theentirelifecycleofagentdevelopment.Thisincludesenvironmentsetups,
datacollection,agentevaluation,andvisualization.Theobservationandac-
tionspacesarehighlygeneric,supportingbothfunctioncallingandhuman-
computerinterfaces. ThisversatilityisfurtherenhancedbyAgentStudio’s
graphicaluserinterfaces, whichallowefficientdevelopmentofdatasets
andbenchmarksinreal-worldsettings. Toillustrate,weintroduceavisual
groundingdatasetandareal-worldbenchmarksuite,bothcreatedwithour
graphicalinterfaces. Furthermore,wepresentseveralactionableinsights
derivedfromAgentStudio,e.g.,generalvisualgrounding,open-endedtool
creation, learning from videos, etc. We have open-sourced the environ-
ments,datasets,benchmarks,andinterfacestopromoteresearchtowards
developinggeneralvirtualagentsforthefuture.
1 Introduction
Building autonomous virtual agents that can utilize every software tool on computers
representsalongstandinggoalinAIresearch(OpenAI,2016). Suchagentsaredesigned
toreceivecomputerstates(e.g.,screenshots)andtakeactionsthroughfunctioncallingor
human-computerinterfaces(e.g.,keyboardandmouse)inresponsetonaturallanguage
instructions. Fueledbytheadvancementsinlargelanguagemodels(LLMs)(Brownetal.,
2020;Chowdheryetal.,2022;OpenAI,2023b,interalia),therehasbeenimpressiveprogress
towardsgeneralvirtualagents, particularlyinweb(Kimetal.,2023;Zhengetal.,2023),
desktop(Zhangetal.,2024),andvideogames(Tanetal.,2024;SIMATeam,2024).
However,currentresearchinvirtualagentsishinderedbytwomainchallenges:
1. A lack of open and systematic infrastructure for both building and benchmarking
agentsinreal-worldcomputercontrol. i)Existingonlineinteractiveenvironments,suchas
locallyhostedwebsites(Yaoetal.,2022;Zhouetal.,2023),arerestrictedtodomain-specific
tasksandactionspaces,thusnotallowingforagentevaluationacrossthebroadspectrum
of human activities. Also, these environments lack features that allow for tool creation
support,andnaturallanguagefeedback. Thiscanmakeitdifficultforopen-endedagentsto
self-correctandself-improvewithincontext. ii)Staticdatasets(Dengetal.,2023;Rawles
etal.,2023)arepronetobehackedandignoremultiplevalidtrajectories. Moreover,the
datasetcollectionpipelineismostlyabsentfromexistingdatasets,whichisessentialfor
specializingagentabilitiessuchasvisualgrounding.
∗Equalcontribution. ThisworkwasperformedwhenLongtaoZhengandZhenghaiXuewere
internsatSkyworkAI.
1
4202
raM
62
]IA.sc[
1v81971.3042:viXraCross-Device Cross-OS Interface
Docker Physical Machines Linux MacOS
Interactive
Virtual Machines … Windows …
Annotation
Pipeline
Universal Action Space Multimodal Observation Space Open-Endedness
Keyboard Mouse Screenshots/ Code Tool Creation
VNC Remote
APIs & Tools … Recording Outputs Tool Retrieval Desktop
GUI Human Agent Human/AI Video In-the-Wild
Grounding Trajectories Trajectories Feedback Demonstration Testing
Fundamental Agent Abilities Open-Domain Task Suite Auto-Evaluation
Self-Evaluation Self-Correction Low-Level Compositional
Accurate Grounding … Instructions Generalization Safety Check
Figure1: TheoverviewofAgentStudioframework. Wepresentacomprehensivetoolkit
coveringtheentirelifecycleofgeneralvirtualagents. Theenvironmentimplementation
isonline,multimodal,andcross-platform. Itoffersunifiedobservationandactionspaces
for agents, with native support for screen recording, open-ended tool use, etc. Further-
more, AgentStudio includes the complete pipeline for data annotation and in-the-wild
evaluation,accompaniedbyinteractivegraphicaluserinterfaces.Thisfacilitatesthecreation
ofopen-domainbenchmarksanddatasetsthatassessawiderangeofagentcapabilities.
2. Thenecessityforholisticevaluationoffundamentalagentabilitiesinreal-worldsce-
narios. Currentenvironmentsprimarilyfocusonoverallsuccessratesinsteadofseparately
measuringcoreagentabilities,suchasaccurategraphicaluserinterface(GUI)grounding,
compositionalgeneralization,self-evaluationasacritic,andutilizingbothAPIsorhuman-
computerinterfaces. Furthermore,theyarenotsetuptotestagentsonopen-domainand
user-customizedtasksbasedonhuman-computerinterfaces,limitinghowaccuratelytheir
benchmarkresultscanreflectreal-worldsettings. However,thesefundamentalabilitiesand
in-the-wildevaluationarecriticalforguidingthedevelopmentofgeneralvirtualagents.
Toaddresstheseissues,wereleasethepublicbetaofAgentStudio1,atoolkitencompass-
ing the entire lifecycle of building and benchmarking general virtual agents in the wild.
AgentStudioprovidesanintegratedsolutionspanningenvironmentsetup,datacollection,
onlinetesting,andresultvisualization(Figure1). Itadoptsthefollowingdesignchoices:
Thegenericobservationandactionspacesconsistofbothhuman-computerinterfaces
andfunctioncalling. Specifically,agentsinteractwithexternalenvironmentsthroughan
interactivePythonkernel. Therefore,theycanautomatekeyboard-mouseinteractionsvia
Pythonscriptstocontrolthird-partyapplications,andleveragefunctioncallingwhenaccess
toprograminternalsorAPIsisavailable. Thisuniversalactionspaceenablesagentstointer-
actwitharbitrarysoftware. Theobservationspaceincorporatesvariousmodalities,ranging
fromscreenrecordings(videos)andscreenshots(images)tocodeexecutionresults(text),
openingnewresearchavenuessuchaslearningfromvideos. Furthermore,AgentStudiona-
tivelysupportscreatingreusablecodescriptsastools. Incontrast,previousenvironments
useddomain-specificobservationandactionspacesanddidnotconsideropen-endedtool
creation,leadingtolimitedgeneralizabilityofagentsdevelopedonthem.
Theenvironmentimplementationisonline,realistic,andcompatiblewithversatileoper-
atingsystemsanddevices. Thisfeatureenablesthedevelopmentofagentsthatcanhandle
massivelyopen-domainandreal-worldscenarios,e.g.,differentsystemversionsandresolu-
tions. Theonlinenaturealsofacilitatesresearchonagentsthatlearnthroughtrial-and-error
interactionswithenvironments. Wealsodeveloptwouser-friendlygraphicalinterfaces:an
interactivepipelineforcollectingmultimodaldatasetsofbothhumandemonstrationsand
1Weplantoexpandthecollectionofenvironments,tasks,anddataovertime.Contributionsand
feedbackfromthecommunityonhowtoimprovethistoolkitarewelcome.
2
vnE
tnegA
ataD
lavEHuman Tool Data
Environment Domain Online ActionSpace Image Video
Feedback Creation Pipeline
WorldofBits(Shietal.,2017) SimplifiedWeb ✓ Keyboard&Mouse ✓ ✓ ✗ ✗ ✗
AndroidEnv(Toyamaetal.,2021) Android ✓ Touchscreen ✓ ✓ ✗ ✗ ✗
WebGPT(Nakanoetal.,2021) Web-assistedQA ✓ WebOperations ✗ ✗ ✗ ✗ ✗
WebShop(Yaoetal.,2022) WebShopping ✓ WebOperations ✓ ✗ ✗ ✗ ✗
Mind2Web(Dengetal.,2023) Real-WorldWeb ✗ WebOperations ✓ ✗ ✗ ✗ ✗
AITW(Rawlesetal.,2023) Android ✗ Touchscreen ✓ ✗ ✗ ✗ ✗
GAIA(Mialonetal.,2023) Tool-assistedQA ✗ APIs ✗ ✗ ✗ ✗ ✗
WebArena(Zhouetal.,2023) Self-hostedWeb ✓ WebOperations ✗ ✗ ✗ ✗ ✗
VisualWebArena(Kohetal.,2024) Self-hostedWeb ✓ WebOperations ✓ ✗ ✗ ✗ ✗
AgentStudio(Ours) Real-World ✓ Keyboard,Mouse ✓ ✓ ✓ ✓ ✓
Devices andCode
Table1: Acomparisonofenvironmentsforvirtualagents.AgentStudioisaplatformthat
supportsonlineinteractionswithversatilereal-worldcomputers, withthemostgeneric
observationandactionspaces. Itoffersessentialcomponentsforbuildinggeneralvirtual
agentsthataremissingfromexistingenvironments. MoredetailscanbefoundinSection2.
agentsynthetictrajectories,andavisualizationinterfaceforhuman-in-the-looptestingand
gatheringhumanfeedback. Table1comparesAgentStudiotootherexistingenvironments.
AgentStudiomeasuresfundamentalagentabilitiesanddirectsresearchtowardsimprov-
ingthesecapabilities.. Wepresenttwousecasestoillustratethisdesignchoice. Firstly,we
utilizetheAgentStudiointerfacetocreateaGUIgroundingdatasetacrossvariousapplica-
tionsandoperatingsystems. Thisdatasetconsistsofsingle-stepatomicmouseoperations
withdetailedinstructions,e.g.,clickinganicon. Tocompletetheseinstructions,agentsmust
generateaccuratecoordinatesofmouseoperations,acoreabilitythatcurrentbenchmarks
didnotfocuson. Usingthisdataset,wehighlighttheshortcomingsinGUIgroundingof
currentmultimodalmodels. Thisalsoshowcasesthepotentialofscalingthegrounding
datasetwithAgentStudioandtrainingamodelthatcanpreciselytranslatesingle-stepin-
structionsintoexecutableGUIactions. Secondly,weintroduceanreal-worldtasksuite
thatrangesfromsimplefunctioncallingtocomplexcross-applicationtasks. Solvingthese
tasksrequiresaccurateGUIinteraction,robustfunctioncalling,andlong-horizonplanning.
AgentStudioacceptsfeedbackfromvarioussources,e.g.,predefinedrules,models,andhu-
mans. Additionally,weexplorehowcurrentmodelsperforminself-evaluation,i.e.,serving
asthecriticforgiventrajectories. ThesetwoexamplesdemonstratehowAgentStudiocan
facilitatebuildingvirtualagentsandbenchmarkinginreal-worldscenarios.
Lastly,wehighlightseveralpromisingresearchdirectionsderivedfromAgentStudio,as
wellasthechallengesweencounteredduringitsdevelopment. Thesuggestedresearch
problemsincludeauto-evaluation,visualgrounding,etc. ThisshowcasesthatAgentStudio
hasthepotentialtohelpfuturedevelopmentsofgeneralvirtualagents.
Insummary,thispaperintroducesanopenandholistictoolkitfordevelopinggeneralvirtual
agents.Wehaveopen-sourcedeverything,includingenvironmentimplementation,datasets,
tasksuites,datacollectionpipeline,andgraphicalinterfaces. WehopethatAgentStudio
willserveasausefulplatformforthecommunitytocollectivelyscaledata,developnovel
algorithms,andbenchmarkagentswithcustomizedreal-worldtasks.
2 RelatedWork
AgentStudio builds on many previous efforts to build simulators and environments for
automating computer operations using AI. For example, World of Bits (Shi et al., 2017)
providedaminimalistwebsimulator,andAndroidEnv(Toyamaetal.,2021)providedan
emulatorwrapperforAndroiddevices. Earlyattemptsintheseenvironmentsprimarily
used deep reinforcement learning (Liu et al., 2018; Gur et al., 2018; Jia et al., 2018; Gur
etal.,2021;Humphreysetal.,2022),whichstruggledtogeneralizetounseentasks. Since
theseenvironmentsweretailoredforreinforcementlearningagents,theylacksupportfor
developingLLM-basedagentswithnovelcapabilities,e.g.,codeaspoliciesandfunction
calling (Liang et al., 2023). Additionally, with the progress made in LLMs, simplified
3Web Browser Environments AgentStudio Environments
WWW
Web-Based Observations Web-Based Actions Universal Observations Universal Actions
HTML/DOM (Text) Element Operations Screen Recording (Video) Human Computer Interfaces
Accessibility Tree (Text) Tab Operations Screenshots (Image) API/Function Calls
Screenshots (Image) URL Operations Code/API/Tool Outputs (Text) Tool Creation & Use
Figure2: AcomparisonbetweenexistingwebbrowserenvironmentsandAgentStudio
environments. AgentStudio provides the most generic observation and action spaces,
including human-computer interfaces and code execution. The implementation is also
compatiblewithdiverseoperatingsystemsanddevices. Thesefeaturesdrasticallyexpand
thepotentialtaskspaceandallowbuildingandbenchmarkingagentsinreal-worldsettings.
environmentssuchasWorldofBitshavebeengraduallysolved(Guretal.,2023b;Kimetal.,
2023;Zhengetal.,2023;Guretal.,2023a).
TobuildandbenchmarkvirtualagentspoweredbyLLMsinrealisticandcomplexenvi-
ronments,WebGPT(Nakanoetal.,2021)andWebShop(Yaoetal.,2022)wereintroduced
tomeasureweb-assistedquestionansweringandwebshopping,respectively. However,
theyfocusedsolelyonwebnavigation,thustheirtasksandactionspacesarenotgeneric
enoughforbuildinggeneralvirtualagents. SimilarissuesapplytoMind2Web(Dengetal.,
2023),WebArena(Zhouetal.,2023),andVisualWebArena(Kohetal.,2024). Forexample,
the action space in Mind2Web only consists of clicking, selecting elements, and typing.
ComparedtoAgentStudio,theseenvironmentsweremainlydesignedtofunctionasbench-
marksratherthanenvironmentsforagentdevelopment. Asaresult,theylacktheessential
infrastructureforbuildingagents(Figure1andTable1).
Unlikeonlineenvironments,staticdatasetsarevulnerabletohackingandcannotspecify
multipledifferentsuccessfultrajectories. Moreimportantly,mostworksdidnotprovide
thepipelinefordatacollection. GAIA(Mialonetal.,2023)providedaneasy-to-evaluate
QA dataset requiring tool use to solve. AITW released a large-scale dataset of Android
operations. ToolBench(Xuetal.,2023;Qinetal.,2023)andAgentBench(Liuetal.,2023)
canonlyevaluatehowwelltheagentshavelearnedtousespecificAPIs,insteadofgeneral
computercontrol. OpenAgents(Xieetal.,2023)focusedonagenthostingandvisualization
whileneglectingsupportfordatacollection, agentdevelopment, andevaluation. There
are also concurrent works that introduce new static datasets to benchmark the agents
they proposed, such as ScreenSpot (Cheng et al., 2024), ScreenAgent (Niu et al., 2024),
OmniAct(Kapooretal.,2024),WindowsBench(Zhangetal.,2024),andOS-Copilot(Wu
etal.,2024). Similarly,anopendatacollectionpipelineismissingintheseworks.
3 AgentStudio
AgentStudio is a holistic platform built on real-world environments to facilitate agent
development. Itextendsbeyondatypicalbenchmarkwiththefollowingkeyfeatures:
3.1 UniversalObservationandActionSpaces
AgentStudioprovidesunifiedobservationandactionspacesthatiscompatiblewithboth
how humans interact with computers and function calling. This feature enables agent
evaluationanddatacollectiononanyhuman-performedtask,whichdrasticallyexpands
thepotentialtaskspace. Therefore,AgentStudiocanfacilitatethedevelopmentandbench-
markingofagentsthatcangeneralizeacrossdiversereal-worldusecases. Incomparison,
4mostenvironmentstailoredtheobservationandactionspacestospecificdomains,suchas
weboperationsorAPIcalls,asillustratedinFigure2andTable1.
Action Space. The action space supports both high-level function calls and low-level
atomicoperationssuchaskeyboardandmousecontrol. Similartocodeinterpreter(OpenAI,
2023a),agentscanexecutecodeviaaPythonkerneltocallAPIs,importtools,andcontrol
human-computerinterfaces. Specifically,thecontrolofhuman-computerinterfaces(e.g.,
keyboard and mouse) is implemented through pyautogui, a Python package for GUI
automation. ThisGUIcontrolcanbeviewedasaformoftooluse,andtheimplementation
canbeextendedtosupportotherinterfacessuchastouchscreensorotherinputmodalities.
This universal action space allows controlling arbitrary software. For example, agents
cancallAPIsforapplicationswithaccesstointernals(e.g.,shell,websearch,andGoogle
services). EvenifthereisnoAPIaccess(e.g.,closed-sourcethird-partyapplications),agents
canstillautomateoperationsviahuman-computerinterfaces,themostgeneralgrounding.
ObservationSpace. Theagentobservationsaremultimodal,includingvideosofscreen
recordings, screenshots, and outputs from the Python kernel execution. The agent can
furtherleveragetoolsorAPIstoobtainstructuredinternalstatesnotvisibletohumans(e.g.,
HTML andaccessibility tree). In comparison, most existingenvironmentsonly provide
asinglescreenshotortextobservationaftereachaction. Thisobservationformatavoids
implementationdifficultiessuchasspecifyingdelayparametersafteractionexecution. It
alsofacilitatesresearchonhowagentscanprocesscomplexandmultimodalobservations,
andhowtheycansolvetasksthatrequirereal-timevideounderstanding.
SupportforToolCreation,Selection,andUse. AgentStudioallowsopen-endedagentsto
createandupdatetoolsbywritingcodeintofiles,whichcanbereusedlaterbyimporting
themtothekernel. Therefore,agentscandevelopskillsbycombiningbasicoperationsor
bycreatingtoolstosimplifythedecision-makingprocess,whichisessentialforbuilding
agentswithcompositionalgeneralizationandopen-endedlearning(Wangetal.,2023). In
addition,AgentStudiocanautomaticallyconvertcodecommentsintotooldocumentation,
whichisdesignedtofacilitateresearchontoolretrievalanddocumentation-basedtooluse.
3.2 Environments&GraphicalInterfaces
AgentStudioaimstoaddressthelimitationsofcurrentbenchmarksbytargetingreal-world
scenarioswithonlineinteractions. Wearguethattheinteractivenatureofonlineenviron-
mentsisessentialforresearchonexplorationandself-correctionofopen-endedagents. To
increasetheaccessibilityofAgentStudio,wealsodevelopaninteractiveannotationpipeline
andavisualizationinterfaceformonitoringagentbehaviors.
OnlineandReal-WorldEnvironments. Whiledatasetsarediverseandusefulfortraining,
theycanbeexploited,usedomain-specificactionspace,andcannotrecognizemultiplevalid
solutions,renderingthemineffectiveforbenchmarking. Meanwhile,mostexistingrealistic
onlineenvironmentsonlyhavespecifictasksuitesandactionspaceswithinlimiteddomains.
Incontrast,AgentStudioadoptsfullyonlineinteractiveenvironmentsonreal-worlddevices.
Itcanoperateinbothlocalandremotemode. Inremotemode,AgentStudiolaunchesa
VNCremotedesktop,supportingalloperatingsystems(e.g. Windows,Linux,andMacOS)
anddevices(e.g. Docker,virtualmachines,andphysicalmachines)thatfollowtheVNC
protocol. Both screen recording and action execution happen in real time, reflecting the
complexityofreal-worldscenarios. Onlineandreal-worldenvironmentsallowagentsto
explore,learnfromenvironmentinteractions,andaccumulatenewskillsovertime.
Natural Language Feedback and Visualization Interface. Compared to binary scalar
rewardswidelyusedinpreviouswebenvironmentsandreinforcementlearningenviron-
ments, natural language feedback is considered more helpful for language agents to fix
theirownmistakes(Shinnetal.,2023). Topromoteresearchonin-contextself-correctionof
virtualagents,AgentStudiosupportsgeneratingnaturallanguagefeedbackfromdiverse
sources,includingrule-basedevaluators,humanfeedback,andAIfeedback. AgentStudio
5Locate Cursor Step Action
Task to record:
Open the preference in left bottom menu
Import Tools
Locate Cursor Step Action & Save
Figure3: IllustrationofrecordinghumanannotationusingAgentStudio. Thisexample
showcasestheprocessofrecordingcompletekeyboard-mouseactionsequencesrequired
toopenthepreferencesmenu. Theprocedureinvolvesthefollowingsteps: i)importing
requisitetools,ii)obtainingthescreencoordinatesformouseoperations,andiii)executing
actions. When collecting the GUI grounding dataset, users are additionally required to
delineatetheUIelementswithinaboundingboxprovidedintheinterface.
alsooffersaconvenientvisualizationinterfaceformonitoringagentbehaviors,collecting
humanfeedback,andtestingcustomizedtasks. Inadditiontoin-contextself-correction,the
feedbackgatheredcanbeleveragedtofinetuneagentsbeyondhuman-levelperformance,
similartoreinforcementlearningfromhumanfeedback.
InteractiveDataCollectionPipeline. Besidesthevisualizationinterface,AgentStudiopro-
videsaninteractivepipelineforthecollectionofmultimodalagentdatasets(Figure3). This
istobridgethegapthatexistingdatasetsdidnotreleasetheirdatacollectionpipeline(Deng
etal.,2023;Rawlesetal.,2023). Itallowsustocreatelarge-scaleortask-specificdatasetsto
trainagents. Forexample,userscancollectadatasetofaspecificapplicationtoenhance
agentperformance. Webelievethatscalingdataisthemosteffectiveapproachtoachieve
accuratelow-levelGUIcontrol,andAgentStudiooffersauser-friendlypipelineforthis.
4 BuildingDatasetsandBenchmarkswithAgentStudio
Inthissection,wepresenttwodownstreamapplications: i)utilizingtheinteractiveannota-
tionpipelinetocollectGUIgroundingdataandevaluatecurrentmultimodalmodels,andii)
usingthevisualizationinterfaceforreal-worldbenchmarkingwithcustomizedtasksuites.
ThesecasestudiesdemonstratethatAgentStudiooffersamultimodal,simple-to-evaluate,
and expandable pipeline for measuring and training agents, and sidesteps the issues of
currentagentevaluation,e.g.,unrealistic,ignoringcoreabilities,andvulnerabletohacks.
4.1 GUIGroundingDataset
GUIgroundingwithaccuratecoordinatesisthemainchallengeforcurrentvirtualagents,
since the interactable elements are not readily available as in web browsers (Koh et al.,
2024). Ithasalsobeenvalidatedthatstate-of-the-artmodelscangeneratecorrecttextual
descriptionsoftheaction,butstruggletogroundthemintoenvironmentoperations(Zheng
et al., 2024b). Therefore, we collect a dataset of tasks that only require a single step to
complete,withclearinstructions,e.g. clickingaspecificbutton. Thisleveloftasksmainly
teststhevisualgroundingabilitiesofagents. Specifically,wedefineeachdataentryasa
63-tuple, T = (g,s,a),where g isadetailedlow-levelinstruction, s isthescreenshot,and
aisthemouseaction. Forexample,givenascreenshotsofamusicplayerinterfaceand
the instruction g, “Click the Play button”, the action a is annotated as a bounding box of
thebutton. UsingAgentStudio,wedeviseadatasetwith227instructionsformouseclicks,
coveringthreepopulardesktopoperatingsystemsandnineapplications.
Windows Linux MacOS
Model
OS Games PowerPoint Word OS Browser Calculator Music iMovie
Qwen-VL-Chat 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
SeeClick 42.3 47.6 24.0 9.1 60.7 9.1 69.0 53.8 35.7
Gemini-1.0Pro 7.7 9.5 8.0 4.5 10.7 0.0 0.0 3.8 0.0
Claude-3Sonnet 3.8 4.8 24.0 9.1 7.1 4.5 0.0 30.8 0.0
GPT-4V(1106) 11.5 23.8 20.0 9.1 14.3 4.5 3.4 11.5 3.6
Table2: ExperimentresultsontheGUIgroundingdataset.
Results. Basedonthedataset,weevaluatetheGUIgroundingabilitiesofcurrentmulti-
modalmodelsacrossvarioussoftwareandoperatingsystems(Table2). Forclosed-source
APIs, wecompareGemini-1.0Pro, Claude-3Sonnet, andGPT-4V(1106). Geminishows
someabilitytointeractwithorrecognizeGUIelements,especiallyonWindows,butfails
formosttasksonotheroperatingsystems. Thissuggeststhemodelmaynotgeneralizewell
acrossdifferentoperatingsystemsorthatthetrainingdatamaybebiased. Similarly,Claude-
3SonnetscoreshighestonWindowsPowerPointandAppleMusicbutperformspoorlyon
otherapplications. Incontrast,GPT-4Vdemonstratesthehighestoverallgroundingscores
acrossmostcategories. However,itstillfallsshortoftherobustnessrequiredforreal-world
deployment,indicatingtheneedformoredatatoimprovetheGUIgroundingabilitiesof
currentmultimodalmodels. Foropen-sourcemodels,wechooseQwen-VLandSeeClick
(aQwen-VLvariantfine-tunedonexistingGUIdatasets(Zhengetal.,2023;Dengetal.,
2023;Rawlesetal.,2023)). Qwen-VLfailstoobtainanysuccessfuloutcomes,whileSeeClick
consistentlyachievesthehighestscoresinvarioussettings. Thishighlightstheimportance
of further scaling GUI grounding data to improve multimodal models in a data-driven
approachforeffectivereal-worlddeployment. Notably, SeeClickisaspecializedmodel
designedtooutputonlycoordinatesinsteadofcode. Therefore,ininterpretingSeeClick’s
performanceinTable2,itispresumedthatallclicksareaccuratelymatched.
Location and Click Type Match Scores by Model Element Areas for Successful and Failed Clicks
1.0 0.8
Location Match Fail
Click Type Match Success
0.8
0.6
0.6
0.4
0.4
0.2
0.2
0.0 0.0
GPT-4V Gemini-Pro Claude-3 Sonnet Qwen-VL
100 102 104 106 108
Model Element Area (pixels)
(a)BreakdownofFailureCauses (b)StatisticsforElementAreas
Figure4: ComparativeanalysisofGUIgroundingsuccessrates.
Analysis. Figure4acomparesthefailurecausesbasedontwometrics: locationmatchand
clicktypematch. ThesemetricsmeasuretheaccuracyofthemodelspredictingtheGUI
elementlocationandtheclicktypetoexecute(e.g.,singleclick,doubleclick,andrightclick).
Allmodelsshowlowscoresinpreciselylocatingmouseactionintheelement,indicating
muchroomforimprovement. Figure4bsuggeststhatthereisacorrelationbetweenthe
7
serocS
ycneuqerFsizeofGUIelementsandthesuccessrateofmouseoperations. Thisimpliesthatusinga
divide-and-conquer approach by splitting the screen into multiple areas and specifying
coordinateswithinthesesmallerregionscouldprobablyimproveGUIgrounding.
4.2 Real-WorldCross-ApplicationBenchmarkSuite
Unlikesingle-stepGUIgrounding,staticdatasetsarelesseffectiveforbenchmarkingmulti-
steptasksbecausetheycannotacceptmultiplevalidsolutions. Meanwhile,mostexistingon-
linebenchmarksaredomain-specificanddifficulttoextend. ToillustratehowAgentStudio
facilitatesagentevaluationincomplexandreal-worldscenarios,weintroduceabenchmark
suite consisting of 77 real-world tasks. Despite being conceptually simple, these tasks
pose challenges for current state-of-the-art virtual agents. Solving tasks in this suite re-
quiresvariousfundamentalagentabilities,suchastooluse,GUIgrounding,compositional
generalization,andlong-horizonplanning.
Ataskisformalizedasatuple: T = (g, fR, fE),wheregisanaturallanguagedescriptionof
thetaskinstruction, fR optionallyresetstheenvironment,and fE optionallyevaluatesthe
outcometrajectory. Forexample,fortheinstruction“Createaone-houreventTeamMeeting
in Google Calendar at 5pm today”, fR checks and removes any existing events with the
same name, while fE evaluates success by checking via API if the event was properly
createdaftertheagent’sactions. Thetasksuiteisdividedintothreelevels. Level1consists
of19simplefilemanipulationtasksthatcanbeaccomplishedbybasicfunctionscalling.
Level2comprises40tasksinvolvingGoogleservicessuchasGmail,GoogleDocs,Google
Calendar,etc. Thislevelrepresentscommondailyscenariosforvirtualagents. Thesetasks
canalsobecompletedviaAPIcalls,thoughtheAPIsaremorecomplicatedthanthosein
level1. Level3covers18highlychallengingtasksthatrequirebothGUIgroundingand
long-horizondecision-making. Forexample,agentsmayneedtooperateacrossmultiple
applicationslikeVisualStudioCodeandavideoplayertofinishatask. Thesetasksare
mostly compositional, cross-application, or involve complex GUI operations. Various
automaticevaluationprotocolsaresupported. Level1and2tasksareevaluatedthrougha
rule-basedautomaticevaluator,allowingforconvenientbenchmarking. Forlevel3tasks,
werelyonthehumanfeedbackinterfaceinAgentStudioforevaluation.
FileManipulation GoogleService GUI&Cross-Application
Model
Success CriticAccuracy Success CriticAccuracy Success CriticAccuracy
GPT-3.5Turbo 57.9 84.2 57.9 62.5 - -
Gemini-1.0Pro 78.9 89.5 40.0 72.5 11.1 88.9
GPT-4 100.0 52.6 65.0 47.5 0.0 100.0
Table3: Experimentresultsonthereal-worldbenchmarksuite.
Results&Analysis. WecomparetheperformanceofthreemodelsinTable3,including
GPT-3.5Turbo,Gemini-1.0Pro,andGPT-4. ForGUI&Cross-Applicationtasks,weusethe
visionversionsofthesemodels. Eachtaskisevaluatedbasedontwometrics:thepercentage
of tasks the model successfully completes (Success), and the accuracy in evaluating the
successoftrajectories(CriticAccuracy). TheresultssuggestthatwhileGPT-4excelsinmost
APIcallingtasks,itfaceschallengesinGUIandcompositionaltasks. Furthermore,GPT-4
hasalowercriticaccuracycomparedtotheothertwomodelsinlevel1and2tasks. Onthe
otherhand,Gemini-1.0ProandGPT-3.5Turboexhibitrelativelylowersuccessratesoverall,
buttheirhighercriticaccuracyimpliesthatthesemodelsmayhavethepotentialtoimprove
theirperformancebydevelopingnovelself-correctingalgorithms.
5 ActionableInsights
AgentStudio has the potential to promote several lines of research that are essential for
buildinggeneralvirtualagents. Here,weofferseveralpossibleresearchproblemsthatare
emphasizedinthedesignchoicesofAgentStudioorencounteredduringitsdevelopment.
8General GUI Grounding. It has been validated that GPT-4V can achieve reasonable
performanceifthereexistsanoraclegrounding(Zhengetal.,2024a),wheretheoraclecan
convertnaturallanguageinstructionsintocorrectGUIactions. Therefore,itisnecessaryto
eithertrainaspecializedlow-levelvisualgroundingmodel,ordevelopnovelprompting
techniquesthatcanaccuratelytranslateclearinstructionsintoexecutableactions. Similar
to PaLM-E (Driess et al., 2023), virtual agents may need LLMs to decompose tasks into
detailed instructions on text space, and then ground instructions into executable GUI
actions. Toaccomplishthisgoal,thedatacollectionpipelineinAgentStudiocanbeusedto
collectgroundingdatasetsforfinetuningandbenchmarkingpurposes. Additionally,the
visualizationinterfaceinAgentStudiocanbeusedtoidentifyandanalyzefailurecases.
Learning from Documents and Video Demonstrations. Internet-scale data have con-
tributedtoremarkablesuccessinlanguagemodeling. SimilartoMineCLIP(Fanetal.,2022)
andVPT(Bakeretal.,2022),whichleveragesInternet-scaleMinecraftdata,thedevelopment
ofvirtualagentscanalsobenefitfromthewealthofvideosanddocumentsavailableon
theweb. Furthermore,AgentStudio’sfocusonvideorecordingalsoenablesthepossibility
ofcollectingvideodemonstrationslabeledwithactions. Inadditiontoimitationlearning,
these video demonstrations can be used to train inverse dynamics models (Baker et al.,
2022). Suchmodelscanaddpseudoactionlabelstovideorecordings,whichcanpotentially
helpleverageunsupervisedinternet-scalevideodatafortrainingvirtualagents.
Tool Creation, Selection, and Use. AgentStudio provides implementations that allow
virtualagentstoreadilycreateandusetools.Thesecapabilitiesarecrucialforapplyingopen-
endedagentframeworks,e.g.,Voyager(Wangetal.,2023),intoreal-worlddigitalworlds.
Toolcreationandusecansignificantlyreducethecompoundederrorinsequentialdecision-
makingandleverageknowledgelearnedfrompriorenvironmentinteractions. Forexample,
whenencounteringunseenapplications,anidealvirtualagentcanexploretheenvironment
and write a program to use the application. This program will be added to a library of
skillsortools,whichcanbereused. Thisservesasa“gradient-free”learningprocedure,
allowingtheagenttoadaptwithoutupdatingitsmodelparameters. AgentStudioturnsthe
commentswithinthetoollibraryintotooldocumentation,allowingtooluseinazero-or
few-shotmannerbyreadingdocumentation(Hsiehetal.,2023). Itisalsoworthwhileto
exploremethodsforselectingrelevanttoolsfromalargesetofcandidates.
AGeneralistCriticModel. Duringbenchmarkcreation,wefounditchallengingtopro-
videfeedbackforopen-domaintasks. Previousenvironments(Zhouetal.,2023)examined
successratesthroughhand-writtenrulessuchasstringmatching. Similarly,AgentStudio
extendsfunctionalevaluationtomoredomainsandapplicationswithitsflexibleevaluator
framework. However,human-writtenevaluationistask-specific,notscalable,andcannot
incorporate human preferences. Therefore, a general critic model that judges task com-
pletion and provides feedback is beneficial. It can auto-evaluate through API calling or
judgebasedontrajectories. Ifataskfails,itprovidesnaturallanguagefeedbacktofacilitate
reflectionandself-correction. Withthiscritic,wecanalsoleveragereinforcementlearning
fromfeedbacktoalignwithuserpreferences,whichcanbecollectedthroughAgentStudio.
6 Conclusion
Inthiswork,weintroduceAgentStudio,anopentoolkitfordevelopinggeneral-purpose
agentscapableofoperatingindigitalworlds. AgentStudioprovidesenvironmentimple-
mentations in real-world settings and offers a holistic toolkit including data collection
evaluation,visualization,anduserinterfaces. Itenablesdevelopingandtestingagentson
arbitraryhuman-performedtasks. ToillustratetheapplicationsofAgentStudio,wehave
released a dataset focused on visual GUI grounding and a real-world benchmark suite
withauto-evaluationandhumanevaluation. Weacknowledgethatourworkhascertain
limitations. Forexample,thereal-worldcomplexitysignificantlyincreasesthedifficulty
ofcheckingfunctionalcorrectnessautomatically. Therefore,wepresentseveralactionable
insightsderivedfromourframework,highlightinghowAgentStudiocancatalyzeresearch
effortstowardsbuildingversatileAIagentsfordigitalworlds.
9ReproducibilityStatement
To ensure reproducibility, all resources such as code, datasets, agent trajectories, and
leaderboard have been made publicly available at https://skyworkai.github.io/
agent-studio/.
EthicStatement
Autonomousagentsthatcancontrolreal-worlddigitaldeviceshaveinherentrisks,suchas
deletingimportantfilesandsendingspamemails. Tomitigatetheserisks,ourframework
requiresuserexaminationandconfirmationbeforeeachcodeexecution. Usersshouldtake
extracautionwhenusingtheseagentstointeractwithreal-worldenvironments.
References
BowenBaker,IlgeAkkaya,PeterZhokov,JoostHuizinga,JieTang,AdrienEcoffet,Brandon
Houghton,RaulSampedro,andJeffClune. Videopretraining(vpt): Learningtoactby
watchingunlabeledonlinevideos. AdvancesinNeuralInformationProcessingSystems,35:
24639–24654,2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. In Advances in Neural Information Processing
Systems,volume33,pp.1877–1901,2020.
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and
ZhiyongWu.SeeClick:HarnessingGUIgroundingforadvancedvisualGUIagents.arXiv
preprintarXiv:2401.10935,2024.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,
etal. Palm: Scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311,
2022.
XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamuelStevens,BoshiWang,HuanSun,
andYuSu.Mind2Web:Towardsageneralistagentfortheweb.InThirty-seventhConference
onNeuralInformationProcessingSystems,2023.
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e: Anembodied
multimodallanguagemodel. arXivpreprintarXiv:2303.03378,2023.
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,
Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Build-
ing open-ended embodied agents with internet-scale knowledge. Advances in Neural
InformationProcessingSystems,35:18343–18362,2022.
Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to
navigatetheweb. InInternationalConferenceonLearningRepresentations,2018.
IzzeddinGur,NatashaJaques,YingjieMiao,JongwookChoi,ManojTiwari,HonglakLee,
andAleksandraFaust.Environmentgenerationforzero-shotcompositionalreinforcement
learning. InAdvancesinNeuralInformationProcessingSystems,volume34,2021.
Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas
Eck, and Aleksandra Faust. A real-world WebAgent with planning, long context un-
derstanding,andprogramsynthesis. InTheTwelfthInternationalConferenceonLearning
Representations,2023a.
IzzeddinGur,OfirNachum,YingjieMiao,MustafaSafdari,AustinVHuang,Aakanksha
Chowdhery,SharanNarang,NoahFiedel,andAleksandraFaust. UnderstandingHTML
withlargelanguagemodels. InICLR2023WorkshoponMathematicalandEmpiricalUnder-
standingofFoundationModels,2023b.
10Cheng-YuHsieh,Si-AnChen,Chun-LiangLi,YasuhisaFujii,AlexanderRatner,Chen-Yu
Lee,RanjayKrishna,andTomasPfister. Tooldocumentationenableszero-shottool-usage
withlargelanguagemodels. arXivpreprintarXiv:2308.00675,2023.
PeterCHumphreys,DavidRaposo,TobiasPohlen,GregoryThornton,RachitaChhaparia,
AlistairMuldal,JoshAbramson,PetkoGeorgiev,AdamSantoro,andTimothyLillicrap.
Adata-drivenapproachforlearningtocontrolcomputers. InInternationalConferenceon
MachineLearning,pp.9466–9482.PMLR,2022.
ShengJia, JamieRyanKiros, andJimmyBa. DOM-Q-NET:GroundedRLonstructured
language. InInternationalConferenceonLearningRepresentations,2018.
RaghavKapoor,YashParagButala,MelisaRussak,JingYuKoh,KiranKamble,WaseemAl-
shikh,andRuslanSalakhutdinov. Omniact: Adatasetandbenchmarkforenablingmulti-
modalgeneralistautonomousagentsfordesktopandweb.arXivpreprintarXiv:2402.17553,
2024.
GeunwooKim,PierreBaldi,andStephenMcAleer. Languagemodelscansolvecomputer
tasks. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
JingYuKoh,RobertLo,LawrenceJang,VikramDuvvur,MingChongLim,Po-YuHuang,
Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwe-
barena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint
arXiv:2401.13649,2024.
JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,
andAndyZeng. Codeaspolicies: Languagemodelprogramsforembodiedcontrol. In
2023IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pp.9493–9500.IEEE,
2023.
EvanZheranLiu,KelvinGuu,PanupongPasupat,TianlinShi,andPercyLiang. Reinforce-
ment learning on web interfaces using workflow-guided exploration. In International
ConferenceonLearningRepresentations,2018.
XiaoLiu,HaoYu,HanchenZhang,YifanXu,XuanyuLei,HanyuLai,YuGu,Hangliang
Ding,KaiwenMen,KejuanYang,etal. AgentBench: EvaluatingLLMsasagents. InThe
TwelfthInternationalConferenceonLearningRepresentations,2023.
Gre´goireMialon,Cle´mentineFourrier,CraigSwift,ThomasWolf,YannLeCun,andThomas
Scialom. GAIA: A benchmark for general AI assistants. In The Twelfth International
ConferenceonLearningRepresentations,2023.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christo-
pherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,etal. WebGPT:Browser-
assistedquestion-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
RunliangNiu,JindongLi,ShiqiWang,YaliFu,XiyuHu,XueyuanLeng,HeKong,YiChang,
and Qi Wang. Screenagent: A vision language model-driven computer control agent.
arXivpreprintarXiv:2402.07945,2024.
OpenAI. Universe. https://openai.com/blog/universe/,2016.
OpenAI. Function calling and other api updates. https://openai.com/blog/
function-calling-and-other-api-updates,2023a.
OpenAI. GPT-4technicalreport. arXivpreprintarXiv:2303.08774,2023b.
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,
Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master
16000+real-worldapis. arXivpreprintarXiv:2307.16789,2023.
ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyLillicrap. An-
droid in the wild: A large-scale dataset for Android device control. In Thirty-seventh
ConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2023.
11Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World
ofBits: Anopen-domainplatformforweb-basedagents. InInternationalConferenceon
MachineLearning,pp.3135–3144.PMLR,2017.
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu
Yao. Reflexion: Languageagentswithverbalreinforcementlearning. InThirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023.
SIMATeam. Scalinginstructableagentsacrossmanysimulatedworlds. 2024.
WeihaoTan,ZiluoDing,WentaoZhang,BoyuLi,BohanZhou,JunpengYue,Haochong
Xia,JiechuanJiang,LongtaoZheng,XinrunXu,etal. Towardsgeneralcomputercon-
trol: A multimodal agent for red dead redemption ii as a case study. arXiv preprint
arXiv:2403.03186,2024.
DanielToyama,PhilippeHamel,AnitaGergely,GheorgheComanici,AmeliaGlaese,Zafarali
Ahmed,TylerJackson,ShiblMourad,andDoinaPrecup. AndroidEnv: Areinforcement
learningplatformforAndroid. arXivpreprintarXiv:2105.13231,2021.
GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,Linxi
Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large
languagemodels. arXivpreprintarXiv:2305.16291,2023.
ZhiyongWu,ChengchengHan,ZichenDing,ZhenminWeng,ZhoumianzeLiu,Shunyu
Yao,TaoYu,andLingpengKong. Os-Copilot: Towardsgeneralistcomputeragentswith
self-improvement. arXivpreprintarXiv:2402.07456,2024.
TianbaoXie,FanZhou,ZhoujunCheng,PengShi,LuoxuanWeng,YitaoLiu,TohJingHua,
JunningZhao,QianLiu,CheLiu,etal. OpenAgents: Anopenplatformforlanguage
agentsinthewild. arXivpreprintarXiv:2310.10634,2023.
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On
thetoolmanipulationcapabilityofopen-sourcelargelanguagemodels. arXivpreprint
arXiv:2305.16504,2023.
ShunyuYao,HowardChen,JohnYang,andKarthikRNarasimhan. WebShop: Towards
scalablereal-worldwebinteractionwithgroundedlanguageagents. InAdvancesinNeural
InformationProcessingSystems,2022.
ChaoyunZhang,LiqunLi,ShilinHe,XuZhang,BoQiao,SiQin,MinghuaMa,YuKang,
QingweiLin,SaravanRajmohan,etal.Ufo:Aui-focusedagentforwindowsosinteraction.
arXivpreprintarXiv:2402.07939,2024.
BoyuanZheng,BoyuGou,JihyungKil,HuanSun,andYuSu. Gpt-4v(ision)isageneralist
webagent,ifgrounded. arXivpreprintarXiv:2401.01614,2024a.
BoyuanZheng,BoyuGou,JihyungKil,HuanSun,andYuSu. Gpt-4v(ision)isageneralist
webagent,ifgrounded. arXivpreprintarXiv:2401.01614,2024b.
LongtaoZheng,RundongWang,XinrunWang,andBoAn.Synapse:Trajectory-as-exemplar
promptingwithmemoryforcomputercontrol. InTheTwelfthInternationalConferenceon
LearningRepresentations,2023.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi
Cheng,YonatanBisk,DanielFried,UriAlon,etal. WebArena: Arealisticwebenviron-
mentforbuildingautonomousagents. InTheTwelfthInternationalConferenceonLearning
Representations,2023.
12