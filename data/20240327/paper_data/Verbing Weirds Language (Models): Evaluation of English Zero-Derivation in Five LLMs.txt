Verbing Weirds Language (Models): Evaluation of English
Zero-Derivation in Five LLMs
David R. Mortensen, 1 Valentina Izrailevitch, 2Yunze Xiao, 1
Hinrich Schütze, 3 Leonie Weissweiler3
1CarnegieMellonUniversity,2TUMunich,3 LMUMunich&MCML
Abstract
Lexical-syntacticflexibility,intheformofconversion(orzero-derivation)isahallmarkofEnglishmorphology. In
conversion,awordwithonepartofspeechisplacedinanon-prototypicalcontext,whereitiscoercedtobehave
asifithadadifferentpartofspeech. However,whilethisprocessaffectsalargepartoftheEnglishlexicon,little
workhasbeendonetoestablishthedegreetowhichlanguagemodelscapturethistypeofgeneralization. This
paperreportsthefirststudyonthebehavioroflargelanguagemodelswithreferencetoconversion. Wedesigna
taskfortestinglexical-syntacticflexibility—thedegreetowhichmodelscangeneralizeoverwordsinaconstruction
withanon-prototypicalpartofspeech. Thistaskissituatedwithinanaturallanguageinferenceparadigm. Wetest
the abilities of five language models—two proprietary models (GPT-3.5 and GPT-4), three open-source models
(Mistral7B,Falcon40B,andLlama270B).WefindthatGPT-4performsbestonthetask,followedbyGPT-3.5,but
thattheopensourcelanguagemodelsarealsoabletoperformitandthatthe7BparameterMistraldisplaysas
littledifferencebetweenitsbaselineperformanceonthenaturallanguageinferencetaskandthenon-prototypical
syntacticcategorytask,asthemassiveGPT-4.
Keywords:morphology,syntax,largelanguagemodels,opensource
1. Introduction MassNoun Ifyoudon’twanttowater theplants,
pleasecoffeethegraduatestudentsinstead.
Englishdisplaysarelativelyhighdegreeofflexibil- CountNoun The fascist tried to knife me in the
back.
ityregardingthesyntacticcategoryoflexicalitems.
Thistendencyissopervasivethatitisevencom- Englishalsoallowsadjectivesandverbstobecon-
mentedoninpopularculture,asintheAmerican verted(zero-derived)intonouns:
comic strip Calvin and Hobbes, in which one of IntransitiveVerb IthinkI’llgoforaswim.
themaincharactersproclaims“Iliketoverbwords” TransitiveVerb Isustainedadirecthit.
(Figure1). Inthiscaseverb—whoseprototypical Adjective She’s got lots of green but she’s not
function is as a noun—functions as a verb. It is spendinganyofitonme.
coercedintoactingasaverb(specifically,aninfini-
Notethat,whileswim,hit,andgreenaretreatedas
tive)bybeingplacedastheheadofaverbphrase,
nouns(aswellasverbsoradjectives)bylexicogra-
apositionanouncouldneveroccupy. Infact,some
phers,theyareetymologicallyverbsandadjectives.
linguistshavequipped,inonlyslightlyhyperbolic
There are three different linguistic approaches
fashion,that—inEnglish—youcanverbanything.
tothisphenomenon. Oneistosaythatconversion
Otherexamplesinclude: (orzero-derivation)islikeanyothermorphological
Adjective Hishairhasbeguntogray. process(exceptthatnoovertaffix,stressshift,or
Figure1: CalvinandHobbes©1993Watterson. ReprintedwithpermissionofAndrewsMcMeelSyndication.
Allrightsreserved.
4202
raM
62
]LC.sc[
1v65871.3042:viXraotherformalchangeisevident)(Marchand,1969). Wemakethreemaincontributions:
Theprincipalevidenceforthis, aspointedoutby
• A new methodology for investigating conver-
Beard(2017),isthatconversionissubjecttoblock-
sioninlanguagemodels
ing effects: if a word of the same meaning as a
potentialconvertedwordalreadyexistsinthelexi- • A test set for systematically applying this
con(e.g.,duetoderivation),itwillbeusedinstead methodologytoarbitrarymodels
oftheconvertedword. Theappealofthisapproach
• Thedemonstrationthatlexical-syntacticflex-
is that it accounts for the many different seman-
ibility does not increase monotonically with
ticrelationshipsthatcanexistbetweenbasesand
modelsize
zero-derivedwordswiththesamepartsofspeech.
Contrariwise,ithasbeenproposedthatconversion
is not morphological at all—that it is, effectively, 2. Related Work
coining new words (Lieber, 1980). Our study is
conductedinthespiritofathirdapproach,namely LinguisticworkonconversioninEnglishdatesback
thatEnglish,Dutch,andother,similar,languages toSweet(1891). Ithasbeentakenupsporadically
allowflexibilitywithregardtosyntacticcategories byresearcherssincethen(Marchand,1969;Clark
(parts of speech) when this is (1) allowed by the andClark,1979;Lieber,1980;Kiparsky,1982;Don,
contextsemanticallyand(2)requiredbythecon- 1993;BauerandHernández,2005). Muchofthe
textsyntactically(ClarkandClark,1979). Wedo literatureregardingconversionconcernswhether
notattempttoimplementthesethreeapproaches conversionisduetoakindofzero-affixation(Marc-
computationally,ortodistinguishthemempirically, hand,1969),aprocessofcoinage(Lieber,1980),
butmethodologicallywemanipulatepartofspeech or the flouting of syntactic category constraints
by manipulating aspects of the syntactic and se- whenconstrained(andallowed)bycontext(Clark
mantic context, inspired by insights of Clark and and Clark, 1979). This paper assumes the posi-
Clark(1979). tionofClarkandClark,namelythatlanguageslike
In this study, we evaluate five large language Englishallowsyntacticflexibilitywhenlicensedby
models of varying sizes, based on their ability semanticsandrequiredbytheencompassingcon-
to make inferences that require reasoning about structions. This is analogous, in some ways, to
words used in non-prototypical grammatical con- Goldberg(1995)’sanalysisofargumentstructure
texts. Weinvestigatefourmajorhypotheses: alternations, where the broader construction co-
erces,forexample,intransitiveverbstofunctionas
• performanceonthetaskwithprototypicalparts
transitiveverbs(inthecaused-motionconstruction).
ofspeechisbetterthanwithnon-prototypical
Similarly,weassumethatconstructionalcontexts
partsofspeech
coerce words to function as if they have a non-
• non-prototypical parts of speech are asso- prototypicalpartofspeech.
ciated with better performance than nonce Whilethisisthefirststudymodelingconversion
words computationally, there is a growing body of work
addressing related issues for a broader range of
• correlationbetweenperformanceintheproto- phenomenainderivationalmorphologyandneol-
typical,non-prototypical,andnonceconditions ogism. Themostrelevanttothecurrentworkare
Hofmannetal.(2021)andHofmannetal.(2020a),
• differences in model size account for differ-
whichaddressderivationalmorphologyinthecon-
encesinperformance
textofolderBERT-likelanguagemodels(butnot
Performance in the prototypical condition is— contemporaryLLMs). Factorsintheemergenceof
indeed—the best, but performance in the nonce newwordshavebeenelucidatedbyRyskinaetal.
and non-prototypical conditions are similar. The (2020).
performanceofeachmodelwascorrelatedacross
conditions. Wefindthatthemodelsvarygreatlyin
3. Methodology
their ability in this area, with the very large, com-
mercialmodelsperformingbest. However,wealso
Wesoughttodesignataskthatwouldtestwhether
show that the number of parameters alone does
wordsfromnon-prototypicalsyntacticcategories—
notpredicttheperformanceofmodelsonthistask.
convertedwordsandnoncewords—affecttheabil-
Instead,agoodpredictoristheperformanceofthe
ityoftext-in-text-outlanguagemodelstomakeprag-
modelsonagenericversionofthetaskinwhichall
maticgeneralizations.
wordsareusedinprototypicalways. Thissuggests
that most of the variance in the model scores is
3.1. Materials
varianceintheabilitytoperformtheNLItaskitself,
andthattheotherdifferencesareoflesser,butstill We created a set of 3,069 prompts based on the
significant,importance. framesinTable2andfivewordlists(Table1).PartofSpeech Number Example Prototypical Non−Prototypical Nonce
100%
75%
transitiveverbs 42 bamboozle 50%
25% 0%
intransitiveverbs 42 deign 100%
75%
50%
massnouns 51 music 2 05% %
100%
75% countnouns 79 professor 50%
25% 0%
noncewords 49 theord 100%
75%
50%
25%
0%
Table 1: Word lists derived from UniMorph and 100%
75%
Unipseudo(Newetal.,inpress)andverifiedman- 50% 25%
ually 1000 %%
75%
50%
25%
0%
100%
Nonce words were generated with Unipseudo 75%
50%
(Newetal.,inpress)basedonalistofthe59most 2 05% %
100%
frequentmono-morphemicnounsverbsinEnglish 75%
50%
withlength6(aslistedinUniMorph). Thislistwas 25%
0%
manuallyculledtoremovewordsthatwere(1)too
similartoor(2)toodistantfromanyknownEnglish Model GPT−3.5 GPT−4 Falcon Llama−2 Mistral
words. Alllexicalsetsweremanuallycuratedbya
Figure 2: Average accuracy grouped by model
native-speakerlinguist.
and typicality (p- for “prototypical,” np- for “non-
The frames and the wordlists were combined
prototypical,”andno-for“nonce”)
accordingtoprincipledcriteria,yielding3,069items.
Promptsreflecttheformatofthefollowingexample:
a stronger performer than the open-source mod-
If Iasked you to day it, do I want you to
els. Falcon40Bperformsbetteronthemetricthan
dayit?
the other open-source models, on the prototypi-
cal condition. The glaring exception is the mass
3.2. Experiments nounframes,whereFalcongeneratedmostlynon-
sequitursratherthanyes/noresponses. Whenall
The prompts were presented to five models, two
responses are considered, Mistral is a relatively
closed models (GPT-3.5 and GPT-4) and three
weak performer. However, when null responses
openmodelsofvaryingsizes(Mistral7B,Falcon
arefilteredout,Mistralappearstodisplaygreater
40B,andLlama270B).Theclosedmodelswere
flexibilitythantheotheropen-sourcemodels.
prompted via the OpenAI API. The open mod-
Inordertoseparatetheabilitytocarryoutthenat-
elswereallevaluated(withoutquantization)using
urallanguageinferencetaskfromlexical-syntactic
vLLMonaclusternodewith4A6000GPUs. The
flexibility,weanalyzedthedifferencebetweenthe
prompts described above were presented to the
averageaccuracyontheprototypicalcondition(ex-
models with the suffix, “ Answer with one word.”
pected part of speech) and the non-prototypical
Answerswereautomaticallyidentifiedusingregu-
condition(unexpectedpartofspeech). Thesere-
lar expressions. Responses starting with “yes”,
sultsareshowninTable3. Theinaccuracyasso-
“yeah”, “sure”, “correct”, “right”, and “true” were
ciatedwithnon-prototypicalcontextswassubstan-
codedasaffirmativeandthosestartingwith“no”,
tiallyhigherforGPT-3.5andFalcon40B,butwas
“nope”,“wrong”,“incorrect”,and“false”werecoded
relativelylowforLlama270Bandwasminimalfor
asnegative. Otherresponsesweretreatedas“null”.
Mistral7BandGPT-4.
Foreachmodel,fourruns(of3,069prompts)were
made.
5. Discussion
4. Results
In order to understand which factors most con-
Overall results are shown in Figure 2. GPT-4 tributedtoperformanceonthesyntacticflexibility
achieves almost perfect results (maximal lexical- task, we fitted a Logistic Regression to these re-
syntactic flexibility) across all categories (count sults using the Logit function from the Python
noun frames, mass noun frames, and transitive statsmodelslibrary. Thefeatureswereprototyp-
verbframes). Theexceptionistheintransitiveverb icalpartofspeech,modeltype,(proto)typicalityof
frames,whereitsperformance,whennullsarere- thefillergiventheframe,andwhethertheanswer
moved,isworsethanthatofMistral7B.GPT-3.5is was yes, no, or “null”. It showed all factors to be
consistentlyworsethanGPT-4butis,onbalance, significantpredictorsofcorrectness(p<0.01),with
ycaruccA
+
null
−
null
+ null
−
null
+
null
−
null
+
null
−
null
count
mass
transitive
intransitiveSubtask Prompt Intended
(cid:26) ∅ (cid:27) ✓
IfIaskedyoutoXit,doIwantyouto Xit?
not ✗
(cid:26) notto (cid:27) (cid:26) ∅ (cid:27) ✗
IfIaskedyou Xit,doIwantyouto Xit?
transitive tonot not ✓
✓
IfIsay,“Don’tXme,”amIaskingyouto(cid:8) not (cid:9) Xme?
✗
(cid:26) ∅ (cid:27) ✓
IfI Xdaily,doIXeveryday?
don’t ✗
intransitive (cid:26) tried (cid:27) ✓
IfI toX,didIattempttoX?
didnottry ✗
(cid:26) thisXtotheotherone? (cid:27) ✓
IfIlikethisXmorethantheotherone,doIprefer
theotherXtothisone? ✗
count (cid:26) thisXtotheotherone? (cid:27) ✗
IfIlikethisXlessthantheotherone,doIprefer
theotherXtothisone? ✓
(cid:26) less (cid:27) ✓
IpreferlessX.DoIprefer X?
more ✗
mass (cid:26) less (cid:27) ✗
IprefermoreX.DoIprefer X?
more ✓
Table2: Framesusedforgeneratingprompts
model withnulls withoutnulls usingwordsinaprototypicalwayarealsogoodat
usingtheminnon-prototypicalways. Perhapsmost
gpt-3.5 0.08 0.08
surprising,though,isthefactthatmodelsizewas
gpt-4 0.01 0.01 notagoodpredictorofperformance. Thelargest
falcon 0.15 0.07 of the open models (Llama 2 70B) was in some
waystheweakestperformer. Mistral7B,wasthe
llama2 0.03 0.03
smallest,buthelditsownagainstFalcon40Band
mistral 0.07 0.01 even the much larger GPT models, and least in
certainsubtasks. Thiswastrueinspiteofthefact
Table3: Differenceinaverageaccuracybetween that Mistral and Falcon were generally worse at
the prototypical and non-prototypical condition, followinginstructions.
bothwithallrecords(left,nulltreatedasnegative)
Investigatingthedifferencesbetweenthemodels
andwithonlynon-nullrecords(right)
indetail,itisclearthatGPT-4displays,farandaway,
the best scores on our lexical-syntactic flexibility
task. Itmightbetemptingtoattributethisdifference
answertype(yes,no,ornull)asthestrongestpre-
to the number of parameters in the model (since
dictor. This is likely because Mistral and Falcon
GPT-4 is believed to be a Mixture of Experts of
oftenfailtogivecorrectresponsesbygenerating
severallargemodels). However,itisnotthecase
answersthatarecodedneither“yes”nor“no”. This
thatthiskindofgeneralizationisnecessarilysim-
isassociatedwithaconfounder(theseframes,with
plyafunctionofmodelsize: thebest-performing
twosentences,oftenelicitednullresponses). Con-
open-source model, when all else is held equal,
trolling for all of the other factors, model type is
is also the smallest (Mistral 7B). The largest of
alsopredictive,withGPT-3.5andGPT-4mostas-
the open-source models—Llama 2 70B—is con-
sociatedwithcorrectresponsesandLlama2least
sistentlymediocreinitsperformanceonthistask.
associatedwithcorrectresponses.
AndFalcon40B,whichisalmostsixtimesthesize
ReturningtoourhypothesesinTable4,wefind
ofMistral,showsimpressiveabilitiesatthenatural
thatthemodelsdo,almostwithoutexception,per-
languageinferencetaskbutlacklusterperformance
form better under the prototypical condition than
atlexical-syntacticflexibility.
thenon-prototypicalcondition. Thissuggeststhat
conversion is more challenging than the use of MistralandFalcon’sscoresarehurtbythefact
unconvertedwords. However,non-prototypicalper- thattheyfrequentlygeneralnullresponses,partic-
formanceisnotsignificantlydifferentfromnonce ularlyinresponsetoframeselicitingmassnouns
performance,suggestingthatthemodelsaretreat- (FalconandMistral)andtransitiveverbs(Mistral).
ingconvertedwordsas,essentially,noncewords. Thecausalmechanism,inthesecases,isunclear.
Scoresforallthreeconditionsaregenerallycorre- Themassnounframesallconsistoftwosentences:
lated with one another—models that are good at “Iprefermore/lessX.DoIprefermore/lessX?”TheHypothesis Finding
prototypicalperformance>non-prototypicalperformance ✓Supported
non-prototypicalperformance>nonceperformance ✗Notsupported
Correlationbetweenprototypical,non-prototypical,nonceperformance ✓Supported
Differencebetweenmodelsizeaccountsfordifferenceinperformance ✗Notsupported
Table4: Findingswithregardtoeachmajorhypothesis
otherframeshaveonesentenceeach. Theintran- haveseenintheirtrainingdata.
sitive frames require the model to reason about
semanticallyrelatedwords(thoughthecountnoun
7. Bibliographical References
subtask does as well). The generation of null re-
sponsesmaybedueeithertothesesuperficialfac-
torsortomorebasicdifferencesinmodelbehavior
withrespecttoframesthatcallformassnounsor
Laurie Bauer and Salvador Valera Hernández.
intransitiveverbs.
2005. Approachestoconversion/zero-derivation.
GPT-4 also displays a dip in performance on
WaxmannVerlag.
the intransitive subtask, not because it is gener-
atingnullresponsesbutbecauseitisgenerating Laurie Bauer and Salvador Valera. 2005. Con-
incorrectyes/noresponsesinanon-trivialnumber versionorzero-derivation: Anintroduction. Ap-
of cases. Again, because the sets of frames are proaches to conversion/zero-derivation, pages
sosmallandlackingindiversity,itisnotpossible 7–17.
toconstructavalidcausalexplanationtoaccount
forthefactthatthemodelsperformdifferentlyon RobertBeard. 2017. Derivation, chapter 2. John
somesubtasksthanothers. Whatisclear,though, Wiley&Sons,Ltd.
is that there are significant differences between
Eve V Clark and Herbert H Clark. 1979. When
themodelsandthesedifferencesareinsomeway
nounssurfaceasverbs. Language,pages767–
correlatedwiththesubtasksdefinedhere.
811.
6. Conclusion
JanDon.1993. MorphologicalConversion. Ph.D.
thesis,UniversityofUtrecht.
Wehaveintroducedthefirstexperimenttestingthe
David Francis, Ella Rabinovich, Farhan Samir,
lexical-syntacticflexibilityofLLMs,findingthatlan-
David Mortensen, and Suzanne Stevenson.
guagemodelsarechallengedbywordsinsyntacti-
2021. Quantifyingcognitivefactorsinlexicalde-
callynon-prototypicalcontext(whencomparedto
cline. TransactionsoftheAssociationforCom-
wordsinsyntacticallyprototypicalcontexts). How-
putationalLinguistics,9:1529–1545.
ever,wedidnotfindthatwordsinsyntacticallynon-
prototypicalcontextspresentedchallengestothe
AdeleE.Goldberg.1995. Constructions: ACon-
modelsthatnoncewordsdidnot. Asweposited,
structionGrammarApproachtoArgumentStruc-
thereisacorrelationbetweenperformanceonpro-
ture. UniversityofChicagoPress,Chicago,IL.
totypicalandnon-prototypicalitemsandthemodel
type was a significant predictor of performance. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Ak-
However,contrarytoexpectations,themodelsize shita Bhagia, Rodney Kinney, Oyvind Tafjord,
wasnotagoodpredictoroflexical-syntacticflexibil- AnanyaHarshJha,HamishIvison,IanMagnus-
ity. ThefindingsaresummarizedinTable4. son,YizhongWang,ShaneArora,DavidAtkin-
Withthisfoundationinplace,weplantoinvesti- son, Russell Authur, Khyathi Raghavi Chandu,
gatelexical-syntacticflexibilitymoresystematically Arman Cohan, Jennifer Dumas, Yanai Elazar,
byusingamuchlargernumberofframesforeach Yuling Gu, Jack Hessel, Tushar Khot, William
subtaskandbytestingalargersetof(openandpro- Merrill, Jacob Morrison, Niklas Muennighoff,
prietary)models. Nowthattrulyopenmodelslike AakankshaNaik,CrystalNam,MatthewE.Pe-
Olmo(Groeneveldetal.,2024)areavailable,itis ters, Valentina Pyatkin, Abhilasha Ravichan-
possibletoknow,moreprecisely,whatwordshave der, Dustin Schwenk, Saurabh Shah, Will
beenseenbythemodelandinwhatcontexts. This Smith, Emma Strubell, Nishant Subramani,
willallowustostateunambiguouslywhenmodels Mitchell Wortsman, Pradeep Dasigi, Nathan
are generalizing old vocabulary to new contexts Lambert, Kyle Richardson, Luke Zettlemoyer,
andwhentheyaredirectlyrecapitulatingwhatthey Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A.Smith, and Hannaneh Hajishirzi. 2024. Olmo: Anubha Kabra, Atharva Kulkarni, Abhishek Vi-
Acceleratingthescienceoflanguagemodels. jayakumar, Haofei Yu, Hinrich Schütze, Kemal
Oflazer,andDavidR.Mortensen.2023. Count-
ValentinHofmann,JanetPierrehumbert,andHin- ingthebugsinChatGPT’swugs: Amultilingual
rich Schütze. 2020a. DagoBERT: Generating investigationintothemorphologicalcapabilities
derivational morphology with a pretrained lan- of a large language model. In Proceedings of
guagemodel.InProceedingsofthe2020Confer-
the 2023 Conference on Empirical Methods in
enceonEmpiricalMethodsinNaturalLanguage NaturalLanguageProcessing(EMNLP),Online.
Processing(EMNLP),pages3848–3861,Online. AssociationforComputationalLinguistics.
AssociationforComputationalLinguistics.
ValentinHofmann,JanetPierrehumbert,andHin-
8. Language Resource References
richSchütze.2021. Superbizarreisnotsuperb:
DerivationalmorphologyimprovesBERT’sinter-
pretationofcomplexwords.InProceedingsofthe
59thAnnualMeetingoftheAssociationforCom-
putationalLinguisticsandthe11thInternational
JointConferenceonNaturalLanguageProcess-
ing(Volume1: LongPapers),pages3594–3608,
Online. Association for Computational Linguis-
tics.
ValentinHofmann,HinrichSchütze,andJanetPier-
rehumbert.2020b. Agraphauto-encodermodel
ofderivationalmorphology.InProceedingsofthe
58thAnnualMeetingoftheAssociationforCom-
putationalLinguistics,pages1127–1138,Online.
AssociationforComputationalLinguistics.
PaulKiparsky.1982. Fromcyclicphonologytolexi-
calphonology. InH.vanderHulstandN.Smith,
editors,TheStructureofPhonologicalRepresen-
tations,pages131–175.Foris,Dordrecht.
RochelleLieber.1980. Ontheorganizationofthe
lexicon. Ph.D.thesis,UniversityofNewHamp-
shire.
HansMarchand.1969. TheCategoriesandTypes
of Present-Day English Word-Formation. C. H.
Beck,München.
Boris New, Christophe Pallier, Jessica Bour-
gin, and Julien Barra. in press. Unipseudo.
http://www.lexique.org/shiny/unipseudo/. Ac-
cessed: 19.10.2023.
Maria Ryskina, Ella Rabinovich, Taylor Berg-
Kirkpatrick, David R. Mortensen, and Yulia
Tsvetkov.2020. Wherenewwordsareborn: Dis-
tributionalsemanticanalysisofneologismsand
theirsemanticneighborhoods. InProceedings
oftheSocietyforComputationinLinguistics,vol-
ume3.
HenrySweet.1891. AnewEnglishgrammar,logi-
calandhistorical.PartI.Introduction,phonology,
andaccidence. ClarendonPress,Oxford.
LeonieWeissweiler,ValentinHofmann,AnjaliKan-
tharuban, Anna Cai, Ritan Dutt, Amey Hengle,