Scenario-Based Curriculum Generation for Multi-Agent Autonomous
Driving
Axel Brunnbauer1, Luigi Berducci1, Peter Priller2, Dejan Nickovic3, Radu Grosu1
Abstract—Theautomatedgenerationofdiverseandcomplex specifications and simulation environments lacks support
training scenarios has been an important ingredient in many for training in multi-agent systems and (3) limited support
complex learning tasks. Especially in real-world application
for approaches targeting open-ended curriculum generation.
domains, such as autonomous driving, auto-curriculum gen-
To address these shortcomings, we propose MATS-Gym,
eration is considered vital for obtaining robust and general
policies. However, crafting traffic scenarios with multiple, an open-source multi-agent training framework that lever-
heterogeneous agents is typically considered as a tedious and ages existing traffic scenario-specification approaches for
time-consuming task, especially in more complex simulation CARLA.Additionally,wedemonstratehowMATS-Gymcan
environments. In our work, we introduce MATS-Gym, a
be used to implement novel auto-curriculum approaches [3]
Multi-Agent Traffic Scenario framework to train agents in
for generating open-ended traffic scenarios with adaptive
CARLA, a high-fidelity driving simulator. MATS-Gym is a
multi-agent training framework for autonomous driving that difficulty. Hence, the key contributions of MATS-Gym are:
uses partial scenario specifications to generate traffic scenarios 1) A multi-agent, scenario-based AD training and evalu-
with variable numbers of agents. This paper unifies vari-
ation framework for the CARLA simulator.
ous existing approaches to traffic scenario description into
2) Experimental evaluation to demonstrate MATS-Gym’s
a single training framework and demonstrates how it can
be integrated with techniques from unsupervised environ- practicality for multi-agent learning and ability to
ment design to automate the generation of adaptive auto- generate auto-curricula from scenario descriptions.
curricula. The code is available at https://github.com/ 3) Extensive comparison and classification in the context
AutonomousDrivingExaminer/mats-gym.
of AD simulation frameworks.
I. INTRODUCTION
A. Illustrative Scenario
Autonomous driving (AD) stands as a groundbreaking
In this scenario, as depicted in Figure 1, five vehicles
frontier in transportation technology, offering the promise
converge on a four-way intersection, each following a pre-
of safer and more efficient roadways. However, achieving
defined route toward individual goal locations. Additionally,
fully autonomous vehicles has been a long-standing goal
three pedestrians traverse the sidewalk near the intersection.
for researchers across generations. As recent advancements
All vehicles are equipped with sensors, including cameras,
enable the deployment of more vehicles in cities worldwide,
odometry, and velocity sensors. Despite the simplicity of
extensive testing in a diverse range of scenarios becomes
this setting, the potential variations are vast, depending on
essential. For every kilometer driven by AD systems on
initial conditions such as the number and types of actors
real-world roads, they traverse several orders of magnitude
and their behaviors. Effective training needs exposure to di-
more in simulation. Thus, access to high-fidelity simulation
versescenariovariationsforrobustgeneralization.Moreover,
environments plays a crucial role in advancing the field
meticulous selection of progressively complex challenges
of AD. Open-source simulation software like CARLA [1]
is crucial but requires expert knowledge. In the following,
has empowered researchers and practitioners to evaluate and
we introduce MATS-gym and showcase its effectiveness in
train AD algorithms in realistic environments. Despite this,
generating scenarios of varying difficulty.
creating diverse and lifelike traffic scenarios, incorporating
varying numbers of agents, remains a laborious and time- II. RELATEDWORK
consuming task. To tackle this challenge, scenario specifica- A. Training Frameworks for AD
tion languages like Scenic [2] and OpenSCENARIO1 have
Wecompareourapproachtootherframeworkscommonly
been introduced. These scenario specification approaches
utilizedfortrainingandevaluatingADagents.Weidentified
streamline the process of defining logical traffic scenarios
severalpopularframeworksandassessthembasedonseveral
andarewell-integratedintosimulationenvironmentssuchas
key aspects crucial for our use-case. These aspects include
CARLA. However, we observe that (1) infrastructure aiding
ease and expressivity of scenario specifications, simulation
the training and evaluation of AD stacks is scattered across
realism,supportedtraffictypes,andintegratedtraininginter-
numerousadhocimplementations,(2)integrationofscenario
faces. Table I summarizes the comparison.
1 CPS,TechnischeUniversita¨tWien(TUWien),Austria HighwayEnv [4], BARK [5], SMARTS [8], and the Com-
2 AVLListGmbH monRoads Suite [6], [7] offer alternatives to full-fledged
3 AustrianInstituteofTechnology,AIT game-engine simulations like CARLA. They focus on mod-
a Correspondence:axel.brunnbauer@tuwien.ac.at
elingvehicledynamicsandmotionplanning,withCommon-
1https://www.asam.net/standards/detail/
openscenario/ Roads providing more sophisticated multi-body dynamics
4202
raM
62
]OR.sc[
1v50871.3042:viXraFig. 1: This multi-agent scenario illustrates an intersection where five vehicles navigate according to assigned routes, with
three pedestrians observed on the sidewalk adjacent to the ego vehicle. A visual representation of the simulation is depicted
in the center, while the scenario description from which the simulation parameters are sampled is described on the right.
On the left, we depict other scenarios sampled from the same Scenic description.
TABLE I: Comparison of various traffic scenario frameworks
ScenarioSpecification Realism TrafficTypes Training
Framework
Sampling Scriptable Sensors Visual Physics Highway Urban RL MARL
HighwayEnv[4] ✗ ∼ ✗ ✗ ✗ ✓ ∼ ✓ ✓
BARK[5] ✗ ✓ ∼ ✗ ✗ ✓ ∼ ✓ ✗
CommonRoads[6],[7] ✗ ✓ ✗ ✗ ✓ ✓ ∼ ✓ ✗
SMARTS[8] ✓ ✓ ∼ ✗ ✓ ✓ ∼ ✓ ✓
MetaDrive[9] ✓ ∼ ✓ ✗ ∼ ✓ ✓ ✓ ✓
DIDrive[10] ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗
MACAD-Gym[11] ✗ ∼ ✓ ✓ ✓ ✓ ✓ ✓ ✓
MATS-Gym(ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
compared to HighwayEnv and BARK. While simpler en- offers limited scenario generation capabilities, allowing only
vironments are computationally efficient, they lack complex manual specification of initial conditions.
sensor models and realistic observations. These frameworks In this comparison, we focus on relevant aspects for
mainlyinvolvevehiclesandlackpedestrianorobjectmodel- learning-basedapproachesinrealisticurbantrafficscenarios.
ing found in urban traffic. BARK and CommonRoads allow We acknowledge our criteria may not be complete and there
scriptable scenarios, while HighwayEnv supports custom might be aspects that could be relevant in other use cases.
scenarios through code re-implementation. SMARTS allows However, we tried to cover a broad spectrum of frameworks
to model urban traffic scenarios with traffic lights and traffic that could be used for training and evaluation.
signs. However, since the focus is not on graphical realism,
those scenarios can not reach the visual diversity of traffic B. Curriculum Learning
scenes in CARLA, which is necessary to train and evaluate Generating safety-critical scenarios is an active area of
vision based AD stacks. research and has been addressed in numerous works in
MetaDrive [9] is a training framework that builds on the recent years [12]. However, training agents in complex
Panda3D engine. It offers a wide range of features such environmentsrequirespresentinglevelsattherightdifficulty,
as procedural road generation and incorporating scenarios matching their capabilities to generate meaningful learning
fromtrafficdatasets.However,scenarioscannotbespecified signals. Curriculum Learning (CL) and Auto CL (ACL)
by behavioral building blocks or predefined sub-scenarios. methods have emerged to progressively expose agents to
Moreover, its engine limits the structural variety of environ- more complex environments. While traditional CL involves
ments that can be achieved in CARLA. manually designing curricula, ACL automates this process
DIDriveGym[10]andMACADGym[11]arebothtrain- by dynamically presenting suitable scenarios based on agent
ing frameworks designed around CARLA. DI Drive Gym performance. For an overview, we refer to [13].
focuses on single-agent autonomous driving systems and Previous works also applied ACL to AD scenarios [14],
interfaces with ScenarioRunner scenarios, lacking support [15].AnovelresearchdirectioninACLisUnsupervisedEn-
for training multi-agent systems or scenario generation via vironmentDesign(UED),wheretaskcreationitselfistreated
sampling. MACAD Gym, supports multi-agent training but as an optimization problem. This is formalized throughUnderspecified Partially Observable Markov Decision Pro- MATS Gym
cesses (UPOMDP), allowing for flexible reparameterization
Tasks Wrappers
of environments during runtime. A UPOMDP is a tuple
ScenarioRunnerEnv OpenScenarioEnv ScenicEnv
= A,O,Θ,SM, M, M, M,γ ,
M ⟨ T I R ⟩
BaseScenarioEnv
whichdiffersfromastandardPOMDPonlyinhavingasetof
free parameters Θ, which can be chosen at any time during
runtime. Recent works have explored different algorithms Use
for UED. Prioritized Level Replay (PLR) utilizes domain
randomization to generate parameters and a level replay
buffer to resample scenarios proportional to their regret, a BasicScenario
measure of suboptimality [16]. PAIRED [17] optimizes the
level generator via reinforcement learning (RL) to maximize
regret, while REPAIRED [3] combines PLR with the level Extends Extends
generation approach from PAIRED in a Dual Curriculum
Design (DCD) framework. Other approaches use evolution-
ary algorithms to optimize the generator and include a level ScenicScenario OpenScenario
...
editor for environment modifications [18], [19].
III. MATS-GYM:ARCHITECTUREANDFEATURES
Fig.2:MATS-GymArchitecture:WebuildonScenarioRun-
MATS-Gym is a multi-agent training and evaluation
nerthescenarioexecutioninfrastructure.Blue:ourcontribu-
framework that allows to generate diverse traffic scenarios
tions, Green: existing infrastructure from ScenarioRunner.
in CARLA, a high-fidelity traffic simulator. The frame-
work is designed to reconcile scenario execution engines,
such as Scenic [2] and ScenarioRunner [20], and training
B. Observations, Actions and Tasks
frameworks in the AD research community. We model the
interaction between the environment and the agents as a MATS-Gym offers comprehensive infrastructure for agent
Partially Observable Stochastic Game (POSG) where all stateretrieval,providinggroundtruthinformationsuchaspo-
agents take actions simultaneously at each timestep. Thus, a sition, velocity vectors, and key traffic events like violations
naturalchoicefortheenvironmentinterfaceisthePettingZoo and collisions. Its modular design allows for flexible speci-
API [21], the multi-agent version of OpenAI Gym [22]. fication of additional observation types. It offers a versatile
infrastructureforconfiguringsensorsuitesforagents,akinto
A. Architecture the CARLA AD Challenge, supporting various sensors such
WebuildMATS-GymaroundthecoreofScenarioRunner, ascameras,LiDAR,andRadar.ItextendstheCARLABird-
which allows us to leverage their existing infrastructure EyeView [23] framework for bird’s-eye view observations,
to specify scenarios, define event observers and execute constructing occupancy gridmaps around agents encoding
scenarios, as depicted in Figure 2. Our framework serves as lane markings, traffic lights, signs, and other road users.
an execution handler and training interface that extends the Agentscanalsoaccessroadnetworkinformationandvector-
capabilities of those scenarios to multi-agent environments. based representations of the road layout. We account for
Furthermore, we provide a set of wrappers to enhance different type of tasks by providing action spaces at various
observation and action spaces in order to facilitate versatile levels of abstraction. On the lowest level, agents can issue
training environments. Additionally, we also provide a task throttle,breakingandsteeringcommandsathighfrequencies.
infrastructure that allows to easily define reward functions On a higher level, agents can instead provide a target way-
and termination conditions. One of the design goals of pointwhichistrackedbyaPIDcontrollerforsmoothdriving
MATS-Gymisinteroperabilitywithcurrentscenarioengines trajectories.Wealsoprovideadiscreteactionspaceforhigh-
for CARLA. Compatibility with ScenarioRunner and thus leveldrivingcommands,suitableforrouteplanning,behavior
OpenSCENARIO is inherently built in. As a consequence, prediction, or multi-agent interaction. Fig. 3 visualizes the
our framework can be also used as a training and evaluation availableactionspaces.Furthermore,MATS-Gymallowsfor
framework for the CARLA AD Challenge2. Furthermore, defining custom tasks and offers pre-defined tasks related
we connect another complementary scenario specification to autonomous driving, including route following, infraction
engine for CARLA by implementing an adapter for Scenic, avoidance,anddrivingcomfort,crucialfordesigningreward
a probabilistic programming language for building complex functions in reinforcement learning approaches.
scenarios in various domains. Scenic comes with a rich set
IV. SCENARIOBASEDCURRICULUMGENERATION
of building blocks for specifying distributions over traffic
scenarios that adhere to predefined constraints. In the following, we will outline our approach to use
Scenic’s parametrizable scenario specifications as a way to
2https://leaderboard.carla.org/challenge/ build an adaptive scenario generation procedure. We adopt aFig. 3:Action abstractionsinclude low-level control,waypoints for local planning, andmacro-actions for behaviorplanning.
dual-curriculum design algorithm based on REPAIRED [3], Algorithm 1 MATS-REPAIRED
a recent algorithm combining PLR with an adaptive level 1: Input: policy π, environment generator π˜, buffer Λ
generator. At each timestep, we either generate a new level 2: while not converged do
by sampling environment parameters from π˜ or we sample 3: Sample replay decision d P D(d)
∼
a replay level from our PLR buffer. The buffer samples 4: if d=0 then
levels with a probability proportional to their regret and a 5: Generate environment parameters θ π˜.
∼
stalenessmetric,asdescribedintheoriginalapproach.Then, 6: Insert θ into buffer Λ.
we sample a trajectory from the environment to update the 7: else
student policy π and to compute the estimated regret for the 8: Sample replay level, θ Λ.
∼
currentlevel,whichisthenusedtoupdatethebufferandthe 9: end if
environment generator. Algorithm 1 outlines this approach. 10: Collect trajectory τ using policy π.
Differently than the original, we do not use an antagonist 11: Compute regret R using Eq. 1.
agenttoestimatetheregret,duetocomputationalconstraints. 12: Update policy π with τ.
Instead, we use the Maximum Monte Carlo [3] regret esti- 13: Update level buffer: Λ (θ,R).
←
mator, formally described as: 14: if level was generated then
T 15: Update environment generator π˜ with R.
1 (cid:88)
R V(s ). (1) 16: end if
T max − t
t=0 17: end while
This regret formulation compares the maximum achieved
return with the average state value for each episode. We
choose this alternative to avoid training another policy just
A. Learning in Different Action Spaces
forregretestimation,whichwouldentailtwiceasmuchsim-
ulation steps. Furthermore, for the level generator optimiza-
In this experiment, we demonstrate our framework’s us-
tion, we use the tree-structured Parzen estimator algorithm,
ability for multi-agent training and discuss how the choice
implemented in the hyperparameter optimization framework
of actions affects the training of cooperative agents.
Optuna [24] instead of RL.
Scenario Description. Based on the initial scenario, we
V. EXPERIMENTS let four vehicles navigate through a four-way intersection,
We conduct two experiments in which we show that each with a designated route and a time limit to reach
MATS-Gym can be used to (1) train multiple agents in their destination. The starting conditions and goals vary per
an implicit coordination task and (2) for auto-curriculum episode as outlined in Scenic. To succeed, the agent must
generation. Both experiments share the base scenario in manageitsvehicle,collaboratewithothertrafficparticipants,
whichagentsaretaskedwithsuccessfullynavigatingalonga andfollowtheprescribedroute.Terminationoccurswhenall
route through an intersection in urban environments, similar agents are stopped.
totheoneintroducedintheillustrativeexample.Thereward Training Setup. Each vehicle is governed by an in-
function for this task is a combination of a progress-based dependent policy, which perceives a birdview observation
and a cruise speed reward: encoding the road layout, vehicle positions relative to the
agent, and route information. To study the impact of actions
r =r +r , (2)
t progress cruise spaces,weconsiderthesametaskdefinitionwithcontinuous,
where r = p p is the progress made along the waypoint and macro actions. Each of the proposed actions
progress t t−1
route since the last− time-step and r = min( vt ,1) have different control frequencies: continuous actions are
cruise vtarget
rewards the agent to drive at a cruise velocity v . repeated twice, waypoint actions are executed for 5 steps,
target
For the optimization of the agent policy we use Proximal andmacroactionspersistfor10stepswith0.05sperstep.We
PolicyOptimization(PPO)[25].Theagentreceivesbirdview trainthepolicieswithIndependentPPO[26]andaccountfor
observations as input and can therefore observe the drivable the different action frequencies by fixing the training budget
roadarea,lanemarkings,route andothertrafficparticipants. to 175 policy updates, each over a batch of 2048 transitions.Episode Reward Collisions Route Completion
40 1.0 100
0.8 80
30
0.6 60
20
0.4 40
10
0.2 20
0 0.0 0
0 50 100 150 0 50 100 150 0 50 100 150
PolicyUpdates PolicyUpdates PolicyUpdates
ContinuousActions WaypointActions MacroActions
Fig. 4: Learning curves for I-PPO under different action definitions and the impact on episodic return, collisions and route
completion. Performance reports mean and standard deviation over 5 consecutive policy updates of the same run.
Results. Figure 4 shows the learning curves with respect an empty intersection is easier than performing an unpro-
toaverageepisodicreward,collisionsandroutecompletions, tected left turn in a busy intersection where other traffic
evaluated over the batch of training data for each policy participants are driving recklessly. In our experiment, we
update. We aggregate the metrics over k-size bins of policy define the following parameters that can be sampled:
updates (k =5), reporting the mean and standard deviation.
• Assigned route.
Weobservedifferentcharacteristicsofthelearnedpolicies.
• Number of other vehicles in the intersection.
Low-level and waypoint target actions incur a high number
• Target speed for the other vehicles’ controllers.
ofcollisions,especiallyinearlytrainingstages.Ontheother
• Whether the other vehicles ignores others recklessly.
hand, macro actions make use of a basic lane keeping and
• Whether the other vehicles respect traffic lights.
collision avoidance controller, which prevents agents from
Results. In Figure 5, we compare learning curves over
leaving drivable areas and avoids most of the accidents,
150K environment steps. Both PLR and DR show sim-
leading to a low collision rate throughout the training.
ilar performance in terms of episodic returns and route
This restriction comes at the cost of frequent deadlocks,
completion during training, indicating comparable scenario
which result in a low route completion rate. Less restrictive
difficulty. DCD demonstrates a notable increase in training
action spaces allow agents to leave the predefined lane to
signal, suggesting adaptation by generating progressively
conduct evasive maneuvers, leading to higher average route
easier scenarios in the initial stages of training. We evaluate
completion in later training stages.
all approaches on the same 12 hold-out levels, representing
This experiment emphasizes how the action space design
different maneuvers with varying numbers of NPCs. Al-
profoundlyshapesemergentbehaviorandaffectsthelearning
though the performance on the hold-out set is comparable
task’s difficulty. Careful modelling of the problem, encom-
in early stages of the training, DCD and PLR do not suffer
passing observations and actions, is crucial for multi-agent
as much from performance drops in later stages.
learning and serves as a key feature in training frameworks.
Figure 6 provides insight into the progression of scenario
Importantly, it demonstrates that there isn’t a universally
parameters throughout training. DCD shifts the scenario
optimal level of abstraction for the action space; instead, the
distribution towards less challenging levels, characterized
choice depends on the problem at hand.
by straight maneuvers for the agent and a low number of
vehicles in intersections. For comparison, we also show
B. Scenario Based Environment Design
how DR naturally maintains a uniform distribution over
In this experiment, we assess our DCD approach’s ability environment parameters.
toalignthetrainingscenariodistributionwiththeagent’sca- We also investigate the distribution of level parameters
pabilities. We examine its impact on agent performance and in the replay buffers of PLR and REPAIRED. Figure 7
thedistributionofgeneratedlevels.Additionally,wecompare depicts the average regret associated with various parameter
its effectiveness with PLR and basic domain randomization. combinations at four distinct checkpoints during the training
Weanalyzetheevolutionofsampledscenarioparametersfor process. For visualization purposes, we focus on combina-
each approach and evaluate their adaptation to the agent’s tions involving the maneuver type and the number of other
capabilities, particularly in the early stages of training when vehicles (NPCs) present in the intersection. Observing the
the agent struggles with complex scenarios. data, it becomes evident that the average regrets of Dual-
Scenario Description. We parameterize the base scenario Curriculum Design (DCD) levels tend to be higher and
introduced before with a variety of discrete and continuous exhibit a narrower distribution across fewer parameter com-
variables, which characterize the difficulty of the generated binations compared to those of PLR alone. This trend can
scenes. Intuitively, learning to follow a straight path through be attributed to the presence of the adaptive level sampler,EpisodeReward RouteCompletion RouteCompletion(Eval) Regrets
150 100 100 150
125 125
80 80
100 100
60 60
75 75
40 40
50 50
20 20
25 25
0 0 0 0
0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K
Timesteps Timesteps Timesteps Timesteps
DR PLR DCD
Fig. 5: Learning curves of average episodic return, route completion during training and evaluation over 3 seeds with one
standard deviation. We also report the average regret of the level buffers of PLR and our DCD approach over timesteps.
ManeuverType MeanNumberofNPCs IgnoreOthers Ignoretrafficlights MeanTargetSpeed
1.0 1↑.00 ↑ 1.00 ↑ 60 ↑
4 0.75 0.75
40
0.5 0.50 0.50
Straight 2
Leftturn 0.25 0.25 20
Rightturn
0.0 0 0.00 0.00 0
0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K
1.0 1.00 1.00 60
4 0.75 0.75
40
0.5 0.50 0.50
Straight 2
Leftturn 0.25 0.25 20
Rightturn
0.0 0 0.00 0.00 0
0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K 0K 50K 100K 150K
Fig.6:EvolutionoftheparametersthataresampledduringthetrainingwithDCD(top)anddomainrandomization(bottom).
For each parameter, we report mean and standard deviation over the batch of training data, indicate the direction of major
difficulty ( ), and the linear interpolation to highlight the general trend (red). We observe that DCD steers the parameter
↑
distribution towards configurations that are supposedly easier to solve: straight crossings, fewer and safer NPCs, etc. This
leads us to the conclusion that DCD adapts the training distribution faster to the performance level of the agent.
which facilitates expedited convergence towards parameter for AD. We demonstrate the usability of our framework in
combinations that are more pertinent to the task at hand. two experiments of multi-agent training and automatic cur-
Our experiments showcase the effectiveness of automatic riculumgeneration,respectively.ByintroducingMATS-Gym
curriculum design in aligning scenario generation with the and demonstrating its application in various experiments,
agent’s capabilities. Additionally, optimizing the scenario we contribute to ongoing efforts in advancing multi-agent
generationprocess,ratherthanjusttheresamplingprocedure, training for autonomous driving.
accelerates the adaptation of the level distribution. Through For future work, we intend to explore the potential of
thisexperiment,weaimtodemonstratethatintegratingUED MATS-Gym in adversarial training within multi-agent sys-
approaches into MATS-Gym facilitates future research in tems and the integration of generative models for traffic
curriculum generation for AD applications. scenario generation.
VI. CONCLUSION REFERENCES
In this work, we present MATS-Gym, a multi-agent train- [1] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proceedings of
ing framework capable of generating scenario-based auto-
the 1st Annual Conference on Robot Learning, ser. Proceedings
curricula for AD tasks in CARLA. MATS-Gym is open of Machine Learning Research, S. Levine, V. Vanhoucke, and
to multiple, commonly used frameworks for traffic scenario K. Goldberg, Eds., vol. 78. PMLR, 2017, pp. 1–16. [Online].
Available:https://proceedings.mlr.press/v78/dosovitskiy17a.html
specificationandsampling,includingScenarioRunner,Open-
[2] D. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L. Sangiovanni-
SCENARIOandScenic.LeveragingScenicenablessampling Vincentelli, and S. A. Seshia, “Scenic: A language for scenario
from scenario distributions and the integration with UED specification and scene generation,” in Proceedings of the 40th
ACM SIGPLAN Conference on Programming Language Design
approaches. This compatibility resembles a promising way
and Implementation, 2019, pp. 63–78. [Online]. Available: http:
to generate more relevant and realistic training scenarios //arxiv.org/abs/1809.09310
DCD
RDPLR DCD
80 80
60 60
40 40
20 20
0 0
150K 150K
125K 125K
100K 100K ps
e
012345 012345 012345
507 K5K
01 N23 u4 m5 b0 e1 r2 o34 f5 NP01 C2 s345
507 K5K Timest
Straight Left Right
Fig. 7: For each pair of maneuver type and number of NPCs, we average the regrets of all levels with the same parameters
in the buffer at four checkpoints throughout the training. We observe that the environment generation policy of DCD leads
to more narrowly concentrated regret on fewer level configurations, suggesting faster adaption of the level sampling.
[3] M. Jiang, M. Dennis, J. Parker-Holder, J. Foerster, E. Grefenstette, autonomousvehiclesinurbanenvironment,”in2018IEEEIntelligent
andT.Rockta¨schel,“Replay-guidedadversarialenvironmentdesign.” VehiclesSymposium(IV),2018,pp.1233–1238.
[Online].Available:http://arxiv.org/abs/2110.02439 [15] L. Anzalone, P. Barra, S. Barra, A. Castiglione, and M. Nappi,
[4] E. Leurent, “An environment for autonomous driving decision- “Anend-to-endcurriculumlearningapproachforautonomousdriving
making,” publication Title: GitHub repository. [Online]. Available: scenarios,”IEEETransactionsonIntelligentTransportationSystems,
https://github.com/eleurent/highway-env vol.23,no.10,pp.19817–19826,2022.
[5] J. Bernhard, K. Esterle, P. Hart, and T. Kessler, “BARK: Open [16] M. Jiang, E. Grefenstette, and T. Rockta¨schel, “Prioritized level
behavior benchmarking in multi-agent environments,” in 2020 replay.”[Online].Available:http://arxiv.org/abs/2010.03934
IEEE/RSJ International Conference on Intelligent Robots and [17] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell,
Systems (IROS). IEEE, 2020, pp. 6201–6208. [Online]. Available: A. Critch, and S. Levine, “Emergent complexity and zero-shot
https://ieeexplore.ieee.org/document/9341222/ transfer via unsupervised environment design.” [Online]. Available:
[6] M. Althoff, M. Koschi, and S. Manzinger, “CommonRoad: http://arxiv.org/abs/2012.02096
Composable benchmarks for motion planning on roads,” in 2017 [18] J. Parker-Holder, M. Jiang, M. Dennis, M. Samvelyan, J. Foerster,
IEEEIntelligentVehiclesSymposium(IV). IEEE,2017,pp.719–726. E.Grefenstette,andT.Rockta¨schel,“Evolvingcurriculawithregret-
[Online].Available:http://ieeexplore.ieee.org/document/7995802/ based environment design.” [Online]. Available: http://arxiv.org/abs/
[7] X.Wang,H.Krasowski,andM.Althoff,“CommonRoad-RL:Acon- 2203.01302
figurablereinforcement learningenvironment formotionplanning of [19] I.Mediratta,M.Jiang,J.Parker-Holder,M.Dennis,E.Vinitsky,and
autonomousvehicles,”inIEEEInternationalConferenceonIntelligent T. Rockta¨schel, “Stabilizing unsupervised environment design with
TransportationSystems(ITSC),2021. a learned adversary.” [Online]. Available: http://arxiv.org/abs/2308.
10797
[8] M. Zhou, J. Luo, J. Villella, Y. Yang, D. Rusu, J. Miao et al.,
[20] “ScenarioRunner—Trafficscenariodefinitionandexecutionengine,”
“SMARTS: Scalable multi-agent reinforcement learning training
https://github.com/carla-simulator/scenariorunner, [Accessed 12-03-
school for autonomous driving,” publication Title: Proceedings of
2024].
the 4th Conference on Robot Learning (CoRL). [Online]. Available:
[21] J.Terry,B.Black,N.Grammel,M.Jayakumar,A.Hari,R.Sullivan
https://arxiv.org/abs/2010.09776
et al., “PettingZoo: Gym for multi-agent reinforcement learning,” in
[9] Q.Li,Z.Peng,Z.Xue,Q.Zhang,andB.Zhou,“Metadrive:Compos-
Advances in Neural Information Processing Systems, M. Ranzato,
ingdiversedrivingscenariosforgeneralizablereinforcementlearning,”
A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan,
arXivpreprintarXiv:2109.12674,2021.
Eds., vol. 34. Curran Associates, Inc., 2021, pp. 15032–15043.
[10] D.-d. Contributors, “DI-drive: OpenDILab decision intelligence
[Online]. Available: https://proceedings.neurips.cc/paperfiles/paper/
platform for autonomous driving simulation.” [Online]. Available:
2021/file/7ed2d3454c5eea71148b11d0c25104ff-Paper.pdf
https://github.com/opendilab/DI-drive
[22] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-
[11] P. Palanisamy, “Multi-agent connected autonomous driving using
man, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint
deep reinforcement learning,” 2020 International Joint Conference
arXiv:1606.01540,2016.
on Neural Networks (IJCNN), pp. 1–7, 2019. [Online]. Available:
[23] M.Martyniak,“carla-birdeye-view,”[Accessed12-03-2024].[Online].
https://api.semanticscholar.org/CorpusID:207852404
Available:https://github.com/deepsense-ai/carla-birdeye-view
[12] W. Ding, C. Xu, M. Arief, H. Lin, B. Li, and D. Zhao, “A survey [24] T.Akiba,S.Sano,T.Yanase,T.Ohta,andM.Koyama,“Optuna:A
onsafety-criticaldrivingscenariogeneration—amethodologicalper- next-generationhyperparameteroptimizationframework,”2019.
spective,” IEEE Transactions on Intelligent Transportation Systems, [25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
vol.24,no.7,pp.6971–6988,2023. “Proximalpolicyoptimizationalgorithms,”2017.
[13] R. Portelas, C. Colas, L. Weng, K. Hofmann, and P.-Y. Oudeyer, [26] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and
“Automatic curriculum learning for deep rl: a short survey,” in Y.Wu,“Thesurprisingeffectivenessofppoincooperativemulti-agent
Proceedings of the Twenty-Ninth International Joint Conference on games,”AdvancesinNeuralInformationProcessingSystems,vol.35,
ArtificialIntelligence,ser.IJCAI’20,2021. pp.24611–24624,2022.
[14] Z. Qiao, K. Muelling, J. M. Dolan, P. Palanisamy, and P. Mudalige,
“Automaticallygeneratedcurriculumbasedreinforcementlearningfor
tergeR
.gvA