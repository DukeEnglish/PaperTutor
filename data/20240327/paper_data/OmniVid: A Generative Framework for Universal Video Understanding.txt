OmniViD: A Generative Framework for Universal Video Understanding
JunkeWang1,2,DongdongChen3,ChongLuo4,BoHe5,LuYuan3,ZuxuanWu1,2†,Yu-GangJiang1,2
1ShanghaiKeyLabofIntell. Info. Processing,SchoolofCS,FudanUniversity
2ShanghaiCollaborativeInnovationCenterofIntelligentVisualComputing
3MicrosoftCloud+AI,4MicrosoftResearchAsia,5UniversityofMaryland,CollegePark
Abstract of video understanding has undergone significant expan-
sion and encompassed a diverse range of tasks, including
The core of video understanding tasks, such as recog- action recognition [3, 33, 64, 68, 84, 107], video caption-
nition, captioning, and tracking, is to automatically de- ing[16,36,65],andobjecttracking[4,20,99,120].
tect objects or actions in a video and analyze their tem- For a long period, research in video understanding has
poralevolution. Despitesharingacommongoal, different adopted a task-specific paradigm, i.e., designing special-
tasks often rely on distinct model architectures and anno- ized architectures and loss functions to cater to the unique
tation formats. In contrast, natural language processing requirements of different tasks and benchmarks [10, 41,
benefits from a unified output space, i.e., text sequences, 42, 50, 73, 111]. Despite the promising results with high-
whichsimplifiesthetrainingofpowerfulfoundationallan- capacitydeepneuralnetworks,thesemethods[30,86,108,
guagemodels, suchasGPT-3, withextensivetrainingcor- 117]aretailoredforaparticularobjectiveandlessadaptable
pora. Inspired by this, we seek to unify the output space to deployment in scenarios of diverse needs. To mitigate
of video understanding tasks by using languages as labels thisissue, videofoundationmodels[43,96,97,105,109],
and additionally introducing time and box tokens. In this havegainedemergingattentionfortheirimpressiveperfor-
way,avarietyofvideotaskscouldbeformulatedasvideo- mance across a broad spectrum of video tasks and poten-
groundedtokengeneration. Thisenablesustoaddressvar- tialinrealizingthevisionofArtificialGeneralIntelligence
ious types of video tasks, including classification (such as (AGI).However,whilegenericspatial-temporalrepresenta-
action recognition), captioning (covering clip captioning, tions can be learned with these models, adapting them to
videoquestionanswering,anddensevideocaptioning),and differentdownstreamtasksoftenrequirescarefullydesign-
localization tasks (such as visual object tracking) within a ingandfine-tuningtask-specificheads.
fullysharedencoder-decoderarchitecture,followingagen- In this paper, we posit such limitation originates from
erative framework. Through comprehensive experiments, the diversified annotations for different video tasks, e.g., a
we demonstrate such a simple and straightforward idea is setofactioncategoriesforactionrecognition[12,84,107],
quite effective and can achieve state-of-the-art or compet- sentencesforcaptioning[36,65],andcontinuoussegments
itive results on seven video benchmarks, providing a novel (coordinates) for events (object) localization [20, 23, 75].
perspective for more universal video understanding. Code This naturally necessitates task-specific designs for better
isavailableathttps://github.com/wangjk666/OmniVid. optimization.Incontrast,differenttasksinnaturallanguage
processing (NLP) enjoy a sharable output space, i.e., text
sequences, which promotes the development of large lan-
1.Introduction guagemodels,suchasGPT[81,82]andLlama[49,87,88].
Drawinginspirationfromthis, weleveragewordtokensin
Inrecentyears,theproliferationofvideocontentacrossvar-
naturallanguagestorepresentsemanticinformationthatis
iousapplications,suchasonlineeducationandlivestream-
important for coarse-grained tasks like action recognition,
ing, has profoundly impacted our daily lives. Videos have
videocaptioning, andvideoquestionanswering, andaddi-
evolved into a captivating and immersive medium for in-
tionally introduce special time tokens and box tokens that
formation delivery, emphasizing the pressing demand for
provide localization capabilities in both spatial and tem-
the development of automated algorithms capable of un-
poraldimensions, particularlyusefulforfine-grainedtasks
derstanding the actions [50], events [53], and moving ob-
likedensevideocaptioningandvisualobjecttracking.With
jects [85] within video sequences. As a result, the field
suchanenrichedvocabularythatconsistsofword,time,and
†Correspondingauthor. boxtokens,theoutputformat,aswellastrainingobjectives
4202
raM
62
]VC.sc[
1v53971.3042:viXraAction Rec.
Classifier action index A video of action name.
Clip Cap.
In this video, caption.
Decoder caption
Unified
Dense Vid. Cap. Decoder From <time1> to <time2>,
Reg. Head (start, end) caption.
In frame i, the target locates at
Visual Obj. Track.
Box Head (x1,y1,x2,y2) <box1><box2><box3><box4> .
(a) Video models with task-specific heads (b) Ours -A Unified Generative Framework
Figure1.AconceptualcomparisonbetweenexistingvideomodelsandOmniViD.
of different tasks, can be well unified. Please refer to Fig- ods, including both CNN-based [32, 33, 40, 50, 70] and
ure1forabetterillustration. Transformer-based models [3, 30, 68], widely encode the
With this in mind, we present OmniViD, a generative action labels as one-hot vectors and employ cross-entropy
framework that approaches various video tasks as a lan- lossforsupervisedtraining. Captioningtasks,ontheother
guage modeling task conditioned on video inputs. Om- hand, typically generate a textual description for a video
niViDadoptsanencoder-decoderarchitecture,whereaded- clip [65, 129, 130] or an untrimmed long video [48, 103,
icatedvideoencoderandalanguageencoderareemployed 117] with a text decoder like BERT [51]. It is worth
toextractthemultimodalfeaturesfromdiverseinputs.Con- noting that captioning long videos involves the additional
sideringtheremarkableredundancyinvideodata, wepro- challenge of temporal event localization within the video,
posealightweightMQ-formertoenhancetheefficiencyof making it a more complex task. We categorize the open-
video representations for subsequent modeling. The MQ- ended video question answering [54, 55, 63] as a specific
formerutilizesthreetypesoflearnablequeries,i.e.,content, type of captioning task due to the consistent output for-
sentence, and box queries, to aggregate the frame features mat between them. Localization tasks, represented by vi-
from the video encoder through cross-attention. Finally, a sual object tracking [20, 25, 100], estimate the trajectory
tokendecoderisappliedtogenerateatokensequencefrom of a target object in a video sequence given its position
theabovevocabulary. in the first frame. Following the practice in object detec-
We validate the effectiveness of OmniViD on five rep- tion[11,38,44],aboxheadisoftentimesadoptedtoregress
resentative video tasks, including action recognition, clip the coordinates of the tracking object. In summary, diver-
captioning,videoquestionanswering,densevideocaption- gentpredictionheadshavebeendevelopedinvariousvideo
ing,andvisualobjecttracking.Theresultsdemonstratethat tasks to adapt to the specific format of annotations, which
OmniViDachievesnewstate-of-the-artoratleastcompeti- poses a challenge to derive a unified solution. In this pa-
tive results on the prevalent video benchmarks. For exam- per,werethinkthedesignofauniversalvideounderstand-
ple, using VideoSwin-Base [68] as the video encoder, we ingframeworkfromanovelperspective,i.e.,redefiningan
achieve state-of-the-art performance on action recognition output space that could be shared by different video tasks.
(83.6% top1 accuracy on Kinetics-400 [50]), clip caption- Withinthisunifiedspace,thedevelopmentofgeneralarchi-
ing (56.6 on MSRVTT [111] in terms of CIDEr ), video tecturesandtrainingobjectivesbecomedistinctlyfeasible.
question answering (42.3% accuracy on MSRVTT [111]),
2.2.UnifiedVideoModels
dense video captioning (5.6 on ActivityNet [10] in terms
of SODA c), and visual object tracking (88.9 on Track- Recently,researchershaveundertakenprominenteffortsto
ingNet[73]intermsofnormalizedprecision). Forthefirst unify video tasks within specific domains. OmniVL [96]
time,videotasksofdifferentmodalitiesandgranularitycan and InterVideo [105] represent significant strides in the
besupportedbyasingleframework. realmofvideo-languagepretraining,whicharepre-trained
on large-scale video-text data and achieve superior re-
2.RelatedWork sults on multimodal video tasks like text-to-video retrieval
and video captioning. Beyond these advancements, UN-
2.1.Task-specificMethodsforVideoUnderstanding
Loc [115] and UniVTG [80] have sought to tackle a di-
Task-specificvideounderstandingmodelscouldberoughly verse array of temporal localization tasks within a single
divided into classification, captioning, and localization ap- framework. They accomplish this by simultaneously pre-
proaches. Video action recognition is the most represen- dictingsaliencyscoresandboundaryoffsetsforeachframe
tative classification task in the video domain, which aims (clip). Comparedtovideo-languageandtemporallocaliza-
to recognize human actions in a video. Existing meth- tion,spatiallocalizationinthevideodomain,i.e.,tracking,The target locates at (54.04, 11.84, 202.86, 171.76). Height: 240 Width: 320 Table1. Input&outputofdifferentvideotasks. S/B/W/T:
Sentence/Box/Word/Time,Pro./Tok.:Prompt/Token.
Input Target
…… Task
SPro. BPro. WTok. TTok. BTok.
AR (cid:37) (cid:37) (cid:33) (cid:37) (cid:37)
1.15s ~64.77 s, a woman is playing the accordion. Dur: 114.64 s CC (cid:37) (cid:37) (cid:33) (cid:37) (cid:37)
ViQA (cid:33) (cid:37) (cid:33) (cid:37) (cid:37)
<woman> ...... <time_1> <time_2> …… <box_1> ……<box_168> …… DVP (cid:37) (cid:37) (cid:33) (cid:33) (cid:37)
Figure2.IllustrationofthetimetokensandboxtokensinOmniViD. VOT (cid:37) (cid:33) (cid:37) (cid:37) (cid:33)
ismorefragmentedintermsoftaskdefinition,modelarchi- tasks. Toaccomplishthis, weexpanduponthevocabulary
tecture,andbenchmarks. Unicorn[113]marksasignificant commonlyusedinlanguagemodels[8,57]byintroducing
step forward by employing a fully shared CNN-based en- unique time tokens and box tokens. This augmentation al-
coder and box head for various tracking tasks, utilizing a lows us to represent the output of various video tasks as a
target before distinguishing between them. Subsequently, tokensequencewithinasharedvocabulary. Buildingupon
with the prominent success of vision transformer [11], this foundation, we further present OmniViD, a generative
OmniTracker [98] and UNINEXT [114] push the bound- frameworkthatconceptualizesvideotasksasaprocessfor
aries of unification in tracking models by incorporating generatingtokensgroundedinthevideocontent.
Transformer-based detectors. Despite the achievements of Given a video V that lasts tens of seconds to multiple
theseapproaches,theyarestillconstrainedbytask-specific minutes, we sample a sequence of frames [X ,X ,...,X ]
1 2 T
heads,leavingconsiderablespaceforgreaterunificationof fromit. Forvideoquestionanswering,aquestionregarding
video understanding. To address this limitation, we unify thevisualcontentisgiven,whileforvisualobjecttracking,
diversetaskswithasharableoutputspaceandaddressthem the bounding box of the target object in the first frame is
withafullysharedgenerativeframework. specifiedbytheuser. Belowwefirstintroducehowtoper-
2.3.AutoregressModelinginComputerVision form tokenization for different video tasks with the above
vocabularyinSec.3.1,andthenpresentthearchitectureof
AutoRegressive modeling [118] is a statistical modeling
OmniViD in Sec. 3.2. Finally, we elaborate on the unified
technique that predicts the current state of a sequence
trainingandinferencepipelineinSec.3.3.
based on historical observations, which has achieved re-
markable success in the field of natural language process-
3.1.UnifiedVocabularyforVideoUnderstanding
ing (NLP) [24] and time series analyasis [34, 72]. In-
spired by this, researchers in the vision community have Invideounderstanding,varioustasksnecessitatediversein-
alsoattemptedtoexploreitspotentialforvisualunderstand- putsandoutputsaccordingtospecificsettingsandrequire-
ing.Pix2SeqV1&V2[18,19]expandthetextualvocabulary ments. To establish a cohesive output space that could be
with quantized image coordinates. With this, they address sharedbydifferentvideotasks,wesupplementthewordto-
severalfundamentalimagetasks,e.g.,objectdetection,and kens in language vocabulary with special time tokens and
imagecaptioning, inaunifiedautoregressivemanner. Fol- box tokens, by discretizing the timestamps and the coor-
lowingthisidea,ARTrack[106]andSeqTrack[21]further dinates along the temporal and spatial dimensions, respec-
support the visual object tracking task. VisionLLM [104], tively(seeFigure2).
ontheotherhand,directlybuildsvision-centricframeworks
With the enriched vocabulary, the input and target se-
uponpre-trainedLLMs,withthehopeoftransferringtheir
quences for the training of OmniViD can be generated in
knowledge to visual understanding with minimal resource
thefollowingmanner:
overhead. Inthiswork,weleverageautoregressivemodel-
• Action Recognition: the input only includes a task
ingtothedesignofauniversalvideounderstandingframe-
promptp , i.e., “actionrecognition”, andthetargetis
work. Inadditiontotheexpansiontotemporallocalization task
theground-truthactionname,e.g.,“dancingballet”.
taskswithuniquetimetokens,ourmethodalsoexploresthe
• ClipCaptioning: similartoactionrecognition, theonly
advantagesofautoregressivemodelingforauniversalvideo
difference lies in the target sequence becomes a longer
understandingframeworkforthefirsttime.
description,e.g.,“aclipshowingacomputerscreen”.
• VideoQuestion-Answering: theinputincludesboththe
3.Method
task prompt and the question p , e.g., “What is the
sen
Our primary objective is to design a universal framework videodoing?”,whilethetargetistheanswertothatques-
that accommodates a diverse set of video understanding tion,e.g.,“fencingcompetition”.box queries sentence queries content queries
MQ-Former Classification Task: Fencing.
Captioning Task: <t_0><t_5> Two men fencing;
<t_6><t_20> A men fell down; …
0 frame
th
Localization Task: <b_331><b_330><b_446><b_443>; …
Token Decoder
MQ-Former
…
1 frame
st
Visual Translator
…… ……
Lang. Encoder
…
MQ-Former
Tokenizer
T frame Text Prompt Box Prompt Task Prompt
th
Figure3. ArchitectureofOmniViD.TheMixedQ-formeraggregatestheframefeaturesintothreetypesofqueries,i.e.,contentqueries,
textqueries,andboxqueries.Afterthat,thequeriesobtainedfromdifferentframesareinputtoatemporalencoderfortemporalmodeling.
Finally,thetokendecodergeneratesasequenceoftokensconditionedonthemultimodalinputs.
• DenseVideoCaptioning: theexpectedoutputisasetof concatenatethemasthetextualfeatureG∈RLg×Cg along
events{e }E happeninginthegivenvideo. Inorderto the sequence dimension. Based on the multimodal inputs,
i i=1
facilitatethemodeltolearnthecorrespondencebetween OmniViD produces a sequence of tokens in the above vo-
timestampsandvisualcontents,wedefineatripletforthe cabulary. TheoverallframeworkisillustratedinFigure3.
i-thevente : e =(cid:10) tstart,tdur,s(cid:11) ,wheretstartandtdur
i i i i i i MQ-former. In order to encode the video features into a
denotethestartanddurationtimetoken,andsrepresents
more efficient representation, we further propose a MQ-
thedescriptionfortheevent[117]. Thetargetsequenceis
former to aggregate them into a set of learnable queries.
constructedbyconcatenatingthetripletsofalltheevents.
Ours MQ-former is inspired by the Q-Former in BLIP-
• VisualObjectTracking:wetakethetaskpromptandthe
2[60]andaugmentsitscontentqueriesq withsentence
con
discrete representation of the bounding box in the first
queries q and box queries q . q and q are ob-
sen box sen box
frame, p , as input, and employ the box tokens in the
box tained by transforming the corresponding prompt features
followingframesastarget.Givenaboundingbox(x1,y1,
G and G with two separate linear layers. We add
sen box
x2, y2) on an H ×W image, the tokenized representa-
q and q to q to incorporate semantic and posi-
sen box con
tion is (⟨box ⌊x1/W⌋⟩, ⟨box ⌊y1/H⌋⟩, ⟨box ⌊x2/W⌋⟩,
tional cues [67]. Note that the use of different types of
⟨box ⌊y2/H⌋⟩).
queries not only enables our method to adapt to a variety
Theinputandtargetsequencefordifferentvideotasksare ofvideotasksbutalsoexplicitlyintegratesguidanceinfor-
summarizedinTable1. mationfrompromptsintothevisualfeatures.
With this, we begin by splitting the video features F
3.2.UnifiedArchitecture
along the temporal dimension, resulting in a sequence of
OmniViD follows an encoder-decoder architecture, which framefeatures{F i}T i=f 1,andthensendthemtoMQ-former
firstextractsthevideofeaturesF ∈RTf×Hf×Wf×Cf from inparallel. WithintheMQ-former,thesummedqueriesin-
{X t}T
t=1
with a video encoder, where Tf and Hf × Wf teract with one another, and F i, through self-attention and
denote the temporal and spatial resolution and Cf is the cross-attentioninaniterativemanner, whichintegratesthe
feature dimension. For visual object tracking, we replace framefeaturesintothecompactqueries. Subsequently,we
the first frame with the cropped template, following the feed the per-frame queries to a transformer layer [28] for
commonpractice[4,20,100]. Alanguageencoderisalso temporalmodeling,yieldingQ ∈ RTfNq×Cq,whereN
q
is
adopted to transform three types of prompts, p , p , thenumberofqueriesandsetto32followingtheconfigu-
task sen
p tothepromptembeddingsG ,G ,G ,andthen rationinBLIP-2[60],C representsthefeaturedimension.
box task sen box q
Video
Encoder
Temporal
EncoderVisual Translator. Alignment between video and textual Table2.Comparisonwithstate-of-the-artvideoactionrecognition
representations is extremely important to ensure that the methods.NotethatforMoViNet,wereportthebestresultsonboth
datasets,i.e.,A6onK400andA3onSSV2.
output of our model is intrinsically relevant to the video
content. To accomplish this, we input Q to a Multi-Layer K400 SSV2
Method
Perceptron (MLP) layer to project it to the textual embed- #Frames Top1 #Frames Top1
dingspace,therebyaligningitsdimensionwiththeprompt
I3D[50] N/A 72.1 - -
features G. After this, they are concatenated along the se-
R(2+1)D-TS[89] N/A 73.9 - -
quence dimension to obtain the multimodal tokens M ∈
R(Lg+TfNq)×Cg. SlowFast[33] 8×3×10 77.9 - -
ip-CSN[90] 32×3×10 79.2 - -
Video-grounded Token Decoding. Finally, we employ a
X3D-XL[31] 16×3×10 79.1 - -
tokendecodertopredictasequenceoftokensbasedonM.
SlowFast+NL[33] 16×3×10 79.8 - -
Thearchitectureofourtokendecoderissimilartopopular
CorrNet[94] 32×3×10 81.0 - -
language decoders [57, 91], with causal self-attention for
MoViNet[52] 120×1×1 81.5 120×1×1 64.1
autoregressivetokengeneration.
ViT-B-VTN[76] 250×1×1 78.6 - -
3.3.UnifiedTrainingandInference MViT-B[30] 32×1×5 80.2 64×3×1 67.7
XViT[9] 16×3×1 80.2 32×3×1 65.4
Training. ConditionedonM,OmniViDistrainedtomaxi-
ViViT-L[2] 16×3×4 80.6 16×3×4 65.4
mizethelog-likelihoodbetweenthepredictedtokensyˆand
TimeSformer-L[3] 96×1×3 80.7 96×3×1 62.3
thetargettokensywithcross-entropyloss:
Mformer-HR[78] 16×3×10 81.1 16×3×1 67.1
L VideoSwin-B[68] 32×3×4 82.7 32×3×1 69.6
(cid:88)
maximize logP(yˆ |M,y ), (1) UniFormer-B[61] 32×1×4 82.9 32×3×1 71.2
k 1:k−1
k=1 Ours 32×3×4 83.6 32×3×1 71.3
wherePdenotesthesoftmaxprobabilityandListhelength
tively. FollowingBLIP-2[60],weadoptthesamearchitec-
of y. Note that the output of various video tasks could be
tureofBert-BaseforourMQ-Former,whichconsistsof12
represented as a sequence of tokens in the unified vocabu-
transformerlayerswithadditionallyinsertedcross-attention
laryintroducedinSec.3.1.
blocks. The positional encodings are added to the outputs
Inference. During inference, we predict each token ac- ofMQ-Formertoinjecttemporalinformation.
cordingtothemodellikelihood,i.e.,P(y |M,y ),and
k 1:k−1 Training and Inference Procedures. For the clip-based
employ the beam search strategy [35] since it leads to the
tasks, including action recognition (AR), clip captioning
betterperformancethanargmaxsamplingornucleussam-
(CC),andvideoquestionanswering(ViQA),wesample32
pling [45]. Similar to language models, the end of se-
frames randomly during training and uniformly during in-
quencegenerationisindicatedbyanEOStoken. Theevent
ference.Fordensevideocaptioning(DVP),wefollow[117]
segments for dense video captioning and bounding boxes
toextractframesat1FPS,andsubsampleorpadtheframe
for visual object tracking could be easily obtained by de-
sequence to 160 during both training and inference. For
quantizingthetimeorboxtokens.
visual object tracking (VOT), we randomly sample two
frames in a video sequence during training, following the
4.Experiments
commonpractice[20,106].
4.1.ImplementationDetails We train our model for 50, 20, 50, and 500 epochs for
AR,CC,ViQA,DVP,andVOT,respectively. Notethatwe
Datasets. Our training corpus include action recogni-
follow [21, 106] to train VOT for a longer time since the
tiondatasets(Kinetics-400[50]andSomething-Something
scale of tracking datasets is much larger. Different batch
V2 [39]), clip captioning datasets (MSRVTT [111]
sizesareadopted,i.e.,64forAR,8forCC,256forViQA,
and MSVD [110]), video question answering datasets
8 for DVP, and 16 for VOT. The model is optimized with
(MSRVTT [111] and MSVD [110]), dense video caption-
theAdamWoptimizer[69],withaninitiallearningrate5e-
ing datasets (ActivityNet [10]), and visual object tracking
6 and decayed to 0 with the cosine scheduler. The frame
datasets(TrackingNet[73],LaSOT[29],GOT10K[47]).
resolutionthatweadoptis224×224,augmentedwithran-
Model Instantiation. We adopt VideoSwin pretrained on domresizedcroppingandhorizontalflipping.Duringinfer-
Kinetics-600 [13] as the video encoder, and initialize the ence, we average the logits of the generated tokens as the
language encoder and token decoder with pretrained Bart- final score for AR to support multi-clip&crop evaluation,
base[57]modelthatowns∼140Mparameters.Thenumber and VOT for template update [21, 106]. The threshold for
of time and box tokens are set to 300 and 1000, respec- VOTtemplateupdateis0.03.Table3. Comparisonwithstate-of-the-artvideocaptioningmeth- Table5. DensecaptioningontheActivityNetCaptionsvalidation
ods on MSRVTT and MSVD. Off-the-shelf object detectors are set. ∗ denotes pretraining on large-scale video-language dataset
usedfortheresultsmarkedwith†. YT-Temporal-1B[121].
MSRVTT MSVD Captioning EventLoc. Overall
Method Method
B@4 M R C B@4 M R C B4 M C R P SODA c
PickNet[22] 41.3 27.7 59.8 44.1 52.3 33.3 69.6 76.5 DCE[53] 0.17 5.69 12.43 - - -
SibNet[66] 40.9 27.5 60.2 47.5 54.2 34.8 71.7 88.2 DVC[62] 0.73 6.93 12.61 - - -
OA-BTG†[122] 41.4 28.2 - 46.9 56.9 36.2 - 90.6 TDA-CG[95] 1.31 5.86 7.99 - - -
GRU-EVE†[1] 38.3 28.4 60.7 48.1 47.9 35.0 71.5 78.1 SDVC[74] - 6.92 - 55.58 57.57 -
MGSA[15] 42.4 27.6 - 47.5 53.4 35.0 - 86.7 PDVC[103] 1.65 7.50 25.87 55.42 58.07 5.3
POS+CG[93] 42.0 28.2 61.6 48.7 52.5 34.1 71.3 88.7 UEDVC[123] - - - 59.00 60.32 5.5
POS+VCT[46] 42.3 29.7 62.8 49.1 52.8 36.1 71.8 87.8
Vid2seq[117] - - 18.80 - - 5.4
SAAT[128] 39.9 27.7 61.2 51.0 46.5 33.5 69.4 81.0
Vid2seq∗[117] - 8.50 30.10 52.70 53.90 5.8
STG-KD†[77] 40.5 28.3 60.9 47.1 52.2 36.9 73.9 93.0
Ours 1.73 7.54 26.00 45.08 60.43 5.6
PMI-CAP[17] 42.1 28.7 - 49.4 54.6 36.4 - 95.1
ORG-TRL†[125] 43.6 28.8 62.1 50.9 54.3 36.4 73.9 95.2
thatOmniViDoutperformsexistingmodelsbyaclearmar-
OpenBook[127] 42.8 29.3 61.7 52.9 - - - -
gin (+2.8 and +1.9 in terms of CIDEr on MSRVTT and
SwinBERT[65] 41.9 29.9 62.1 53.8 58.2 41.3 77.5 120.6
MSVD), even if several of them, e.g., OA-BTG [122] and
Ours 44.3 29.9 62.7 56.6 59.7 42.2 78.1 122.5
ORG-TRL[125],leverageobjectdetector[38,44]toextract
objectinformationinanofflinemanner.
Table 4. Accuracy (%) of ViQA on MSRVTT and MSVD, Pre
VLData:pertainingvision-languagedata. 3) Video Question Answering aims to answer a natural
language question based on the video content. We com-
Method PreTrainVLData MSRVTT MSVD
paretheaccuracyofOmniViDwithotherViQAmodelson
ClipBERT[56] 5.6M 37.4 - MSRVTT [111] and MSVD [110] in Table 4. The results
CoMVT[83] 100M 39.5 42.6 demonstrate that OmniViD outperforms both QA-specific
JustAsk[116] 69M 41.5 46.3 methods,e.g.,JustAsk[116],andpertainingmethods,e.g.,
ALIPRO[59] 5.5M 42.1 45.9 ALIPRO[59],showcasingtheeffectivenessofourmethod
OmniVL[96] 18M 44.1 51.0 forcomplexmultimodalreasoning.
4) Dense Video Captioning localizes the events in an
HCRN[54] - 35.6 36.1
untrimmed video and generates the corresponding text de-
JustAsk[116] - 39.6 41.2
scriptions for them. Following the practice of previous
Ours - 42.3 47.7
methods[103,117],weevaluateOmniViDinthreeaspects:
4.2.MainResults 1)theaverageprecision(P),averagerecall(R)acrossIOU
at 0.3, 0.5, 0.7, 0.9 and their harmonic mean for localiza-
1) Action Recognition, as one of the most representative
tion. 2) BLEU4 (B4), METEOR (M), and CIDEr (C) for
video understanding tasks, aims to identify the action cat-
densecaptioning.3)SODA cforanoverallevaluation.The
egories in a video. We evaluate the Top-1 accuracy of
resultsarereportedinTable5.
OmniViDoncommonlyuseddatasets, includingKinetics-
Traditional methods, including both two-stage (e.g.,
400 (K400) [50] which consists of 306k short video clips
DVC [62], SDVC [74]), and one-stage models (e.g.,
of 400 action categories, and Something-Something V2
PDVC[103],UEDVC[123]),allemploythepre-extracted
(SSV2) [39] which comprises 220k videos of 174 cate-
features from video backbones [50] without end-to-end
gories. The comparison results with other methods are
training. Compared to them, OmniViD achieves better re-
showninTable2. OmniViDachievesthebestperformance
sultsonallthemetrics,exceptforRecall. Ourunderperfor-
onbothdatasets,i.e.,83.6%onK400and71.3%onSSV2,
mance on recall is because traditional methods always ap-
surpassing VideoSwin [68] by 0.9 and 1.7, respectively.
plyafixednumberoflocalizationheadstogetalargenum-
Thishighlightstheadvantageofourmethod.
ber of false-positive predictions, e.g., 100 for SDVC [74].
2) Video Captioning expects the model to generate a tex- Vid2Seq [117] is the first end-to-end framework for dense
tual description for a given video, which simultaneously video captioning. We can see that our method, although
evaluatesthevisualcomprehensionandtextgenerationca- slightlyinferiortotheirpre-trainedmodelonYT-Temporal-
pability of our method. MSRVTT [111] and MSVD [14], 1B, can significantly outperform them without large-scale
twolarge-scaleopendomainvideocaptioningdatasets,are pretraining, i.e., 18.80 vs. 26.00 in terms of CIDEr. A de-
adopted and the results are shown in Table 3. We can see tailed comparison between OmniViD and Vid2seq can beTable6. Comparisonswiththevisualobjecttrackingmodelson Table7.AblationstudiesondifferentcomponentsofOmniViD.
LaSOTandTrackingNet.
Model AR CC ViQA DVP VOT
LaSOT TrackingNet
Method 1 w/oTextQuery 83.4 56.5 40.4 5.6 79.2
Suc P P Suc P P
norm norm 2 w/oBoxQuery - - - - 78.2
SiamFC[4] 33.6 42.0 33.9 57.1 66.3 53.3 3 w/oTemEnc 82.5 53.3 41.7 5.1 77.6
ATOM[26] 51.5 57.6 50.5 70.3 77.1 64.8 4 w/oLangInit 81.7 44.4 39.7 4.5 79.0
SiamPRN++[58] 49.6 56.9 49.1 73.3 80.0 69.4 5 Ours 83.6 56.6 42.3 5.6 79.6
DiMP[5] 56.9 65.0 56.7 74.0 80.1 68.7
Table8.Open-vocabularyresultsonHMDB-51andUCF101.
KYS[6] 55.4 63.3 - 74.0 80.0 68.8
Ocean[124] 56.0 65.1 56.6 - - - Method Train HMDB-51 UCF101
AutoMatch[126] 58.2 - 59.9 76.0 - 72.6
ASR[102] K400 21.8±0.9 24.4±1.0
PrDiMP[27] 59.8 68.8 60.8 75.8 81.6 70.4
ZSECOC[79] K400 22.6±1.2 15.1±1.7
TrDiMP[101] 63.9 - 61.4 78.4 83.3 73.1
UR[131] K400 24.4±1.6 17.5±1.6
SiamR-CNN[92] 64.8 72.2 - 81.2 85.4 80.0 E2E[7] K400 32.7 48.0
TransT[20] 64.9 73.8 69.0 81.4 86.7 80.3 Ours K400 26.3 32.0
Unicorn[113] 68.5 76.6 74.1 83.0 86.4 82.2
KeepTrack[71] 67.1 77.2 70.2 - - -
STARK[112] 67.1 77.0 - 82.0 86.9 -
AiATrack[37] - 79.4 73.8 - 87.8 80.4
OSTrack[119] - 78.7 75.2 - 87.8 82.0
MixFormer[25] 69.2 78.7 74.7 83.1 88.1 81.6
SeqTrack[21] 69.9 79.7 76.3 83.3 88.3 82.2
ARTrack[106] 70.4 79.5 76.6 84.2 88.7 83.5 Figure4.Comparisonbetweenjointandseparatetraining.
UNINEXT[114] 72.4 80.7 78.9 85.1 88.2 84.7
Ours 70.8 79.6 76.9 83.8 88.9 83.2 1stand2ndrowsthattheyimprovetheVQAandVOTper-
formanceby1.9and1.4,respectively. 2)temporalencoder:
foundintheappendix. comparingtheresultsinthe3rdand5throws, itisevident
5)VisualObjectTrackingestimatesthetrajectoryofatarget that the temporal encoder brings remarkable performance
objectgivenitspositioninthefirstframe,whichrequiresa gains on all the tasks, validating the temporal modeling is
fine-grainedunderstandingofspatial-temporalinformation. importantforvideounderstanding. 3)initializingtokende-
InTable6,wecompareOmniViDwithothertrackingmod- coderwithBart[57]: theresultsinrow4demonstratethat
els on two most representative datasets, LaSOT [29] and the initialization of the token decoder has a greater impact
TrackingNet[73].Success(Suc),precision(P),andnormal- oncaptioningtasks,stemmingfromthefactthatthetraining
izedprecision(P )arereported. Itisworthmentioning objectives of captioning tasks are inherently more aligned
norm
that although SeqTrack [21] and ARTrack [106] also em- withthepretrainingofthetokendecoder.
ploytheautoregressiveframeworkforobjecttracking,Om- Open-vocabulary Action Recognition: Compared to the
niViDdiffersfromthemintwofoldaspects. Firstly,weper- traditionalclassifier-basedmethods,OmniViDismoreflex-
formtrackingonthecompleteframe, insteadofacropped ibleinadaptingtotheopen-vocabulary(OV)settingbyap-
region. Second, we encode the reference box to the visual pendingthecategorynamestotheinputtextualprompt. As
feature of the tracking frame through box queries, rather shown in Table 8, OmniViD achieves competitive results
thanjustusingitasapromptforthetokendecoder.Itcanbe thanexistingOVmethodswithoutcumbersomedesigns.
observedthatOmniViDachievesexcellentperformanceon
Number of Time and Box Tokens. We further try differ-
both LaSOT and TrackingNet, i.e., 79.6 and 88.9 in terms
entnumbersoftime(N )andbox(N )tokensonthelocal-
t b
ofP ,whichbeatsmostofthepreviousSOTAmethods.
norm ization tasks. As shown in Figure 4, for both types of to-
kens,increasingthenumbercouldfirstimprovetheresults
4.3.AblationStudies
since the quantization error is reduced accordingly, and fi-
AnalysisofDifferentComponentsinOmniViD.InTable nallyconvergeswhenN t ≥300andN b ≥1000.
7, we conduct ablation experiments to study the effects of
4.4.Visualizations
thecorecomponentsinOmniViD:1)text&boxqueriesin
MixedQformer:differentqueriesarethecoredesignofour WevisualizethepredictionsofOmniViDonvariousvideo
methodtoadapttodifferentvideotasksandinjectreference understanding tasks in Figure 5. From the top two rows,
informationintotheframefeature. Itcanbeseenfromthe wecanseethatOmniViDcouldnotonlygenerateaccurateAwoman is taking the phone. Aperson is mixing ingredients in a bowl.
What are soldiers doing? Fire. Who performs on stage in front of a crowd? Band.
GT: 0.28 ~ 55.15,a weight lifting tutorial is given.
GT: 14.11 ~ 42.34,he is pulling the rope back and forth.
Pred: 0 ~55.16, a man is working out on a piece of exercise equipment.
Pred: 13.79 ~54.32, he is practicing body placement and lifting technique.
GT: 0 ~ 16.09,People are playing lacrosse on a field.
GT: 4.91 ~ 16.09,a girl in a maroon shirt is hitting the ball with her stick.
Pred: 0 ~16.06, A small group of girls are playing a game of lacrosse on a field.
Pred: 4.12 ~12.35, One of the girls hits the ball into the goal.
Figure 5. Visualization of the predictions by OmniViD on different video understanding tasks. From top to down, we show the clip
captioning,videoquestionanswering,densevideocaptioning,andvisualobjecttrackingvisualizationresults,respectively.
andnaturalcaptionsforvideosbutalsoanswerquestionsre- tokens. With this, a wide spectrum of video tasks, includ-
gardingthecharactersoractivitiesinthevideo,showcasing ing action recognition, clip captioning, video question an-
itscross-modalmodelingcapability. Inaddition,OmniViD swering,densevideocaptioning,andvisualobjecttracking,
also excels in spatial-temporal localization. The results in could be formulated as a video-grounded token generation
3rd and 4th rows show that it could detect different types process, and further, addressed within an encoder-decoder
ofeventsinvideospreciselyandproducevividdescriptions architecture. Extensive experiments on seven prominent
forthem. Moreover,OmniViDalsoexhibitsremarkablero- video benchmarks showcased the superior video under-
bustness against occlusions and variations in object track- standingcapabilityandversatilityofOmniViD.
ing. Thesevisualizationsunderscoretheversatilityandef- Despite the promising results achieved, the joint train-
fectivenessofOmniViDacrossawiderangeofvideotasks. ing performance of OmniViD exhibited some degradation
inthespatial-temporallocalizationtaskscomparedtosepa-
5.Conclusion ratetraining. Inthefuture,wewillexploremoreadvanced
trainingandoptimizationstrategiesonmultipledatasetsand
This paper introduced OmniViD, a generative framework tasks, to further improve the overall performance and ro-
for universal video understanding. We defined a unified bustnessofourmethod.
outputspacefordifferentvideotasksbysupplementingthe Acknowledgement This project was supported by NSFC
vocabulary of language models with special time and box underGrantNo. 62032006andNo. 62102092.References localizationandeventcaptioninginvideos.InECCV,2020.
6
[1] NayyerAafaq, NaveedAkhtar, WeiLiu, SyedZulqarnain
[18] TingChen,SaurabhSaxena,LalaLi,DavidJFleet,andGe-
Gilani, and Ajmal Mian. Spatio-temporal dynamics and
offreyHinton. Pix2seq: Alanguagemodelingframework
semanticattributeenrichedvisualencodingforvideocap-
forobjectdetection. InICLR,2022. 3
tioning. InCVPR,2019. 6
[19] TingChen,SaurabhSaxena,LalaLi,Tsung-YiLin,DavidJ
[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen
Fleet,andGeoffreyEHinton. Aunifiedsequenceinterface
Sun, Mario Lucˇic´, and Cordelia Schmid. Vivit: A video
forvisiontasks. InNeurIPS,2022. 3
visiontransformer. InICCV,2021. 5
[20] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun
[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
Yang, andHuchuanLu. Transformertracking. InCVPR,
space-timeattentionallyouneedforvideounderstanding?
2021. 1,2,4,5,7
InICML,2021. 1,2,5
[21] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and
[4] LucaBertinetto,JackValmadre,JoaoFHenriques,Andrea
HanHu. Seqtrack: Sequencetosequencelearningforvi-
Vedaldi, andPhilipHSTorr. Fully-convolutionalsiamese
sualobjecttracking. InCVPR,2023. 3,5,7
networksforobjecttracking. InECCVW,2016. 1,4,7
[22] Yangyu Chen, Shuhui Wang, Weigang Zhang, and Qing-
[5] GoutamBhat,MartinDanelljan,LucVanGool,andRadu
mingHuang. Lessismore:Pickinginformativeframesfor
Timofte. Learning discriminative model prediction for
videocaptioning. InECCV,2018. 6
tracking. InICCV,2019. 7
[23] Yi-WenChen,Yi-HsuanTsai,andMing-HsuanYang.End-
[6] GoutamBhat,MartinDanelljan,LucVanGool,andRadu to-endmulti-modalvideotemporalgrounding.InNeurIPS,
Timofte. Knowyoursurroundings:Exploitingsceneinfor-
2021. 1
mationforobjecttracking. InECCV,2020. 7
[24] KR1442ChowdharyandKRChowdhary.Naturallanguage
[7] BiagioBrattoli, JosephTighe, FedorZhdanov, PietroPer- processing. FAI,2020. 3
ona,andKrzysztofChalupka. Rethinkingzero-shotvideo
[25] YutaoCui,ChengJiang,LiminWang,andGangshanWu.
classification:End-to-endtrainingforrealisticapplications.
Mixformer:End-to-endtrackingwithiterativemixedatten-
InCVPR,2020. 7
tion. InCVPR,2022. 2,7
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
[26] MartinDanelljan,GoutamBhat,FahadShahbazKhan,and
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
Michael Felsberg. Atom: Accurate tracking by overlap
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. maximization. InCVPR,2019. 7
Languagemodelsarefew-shotlearners. InNeurIPS,2020.
[27] MartinDanelljan,LucVanGool,andRaduTimofte. Prob-
3
abilisticregressionforvisualtracking. InCVPR,2020. 7
[9] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sud-
[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
hakaran, Brais Martinez, and Georgios Tzimiropoulos.
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Space-time mixing attention for video transformer. In
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
NeurIPS,2021. 5
SylvainGelly,JakobUszkoreit,andNeilHoulsby. Anim-
[10] FabianCabaHeilbron, VictorEscorcia, BernardGhanem, ageisworth16x16words: Transformersforimagerecog-
andJuanCarlosNiebles. Activitynet: Alarge-scalevideo nitionatscale. InICLR,2021. 4
benchmark for human activity understanding. In CVPR,
[29] HengFan,LitingLin,FanYang,PengChu,GeDeng,Sijia
2015. 1,2,5
Yu,HexinBai,YongXu,ChunyuanLiao,andHaibinLing.
[11] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nico- Lasot:Ahigh-qualitybenchmarkforlarge-scalesingleob-
las Usunier, Alexander Kirillov, and Sergey Zagoruyko. jecttracking. InCVPR,2019. 5,7
End-to-endobjectdetectionwithtransformers. InECCV, [30] HaoqiFan,BoXiong,KarttikeyaMangalam,YanghaoLi,
2020. 2,3 Zhicheng Yan, Jitendra Malik, and Christoph Feichten-
[12] Joao Carreira and Andrew Zisserman. Quo vadis, action hofer. Multiscalevisiontransformers. InCVPR,2021. 1,
recognition? a new model and the kinetics dataset. In 2,5
CVPR,2017. 1 [31] ChristophFeichtenhofer.X3d:Expandingarchitecturesfor
[13] JoaoCarreira,EricNoland,AndrasBanki-Horvath,Chloe efficientvideorecognition. InCVPR,2020. 5
Hillier,andAndrewZisserman.Ashortnoteaboutkinetics- [32] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisser-
600. arXivpreprintarXiv:1808.01340,2018. 5 man. Convolutional two-stream network fusion for video
[14] DavidChenandWilliamBDolan.Collectinghighlyparal- actionrecognition. InCVPR,2016. 2
leldataforparaphraseevaluation. InACL-HLT,2011. 6 [33] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
[15] ShaoxiangChenandYu-GangJiang.Motionguidedspatial KaimingHe. Slowfastnetworksforvideorecognition. In
attentionforvideocaptioning. InAAAI,2019. 6 ICCV,2019. 1,2,5
[16] Shaoxiang Chen, Ting Yao, and Yu-Gang Jiang. Deep [34] EricDFeigelson,GJogeshBabu,andGabrielACaceres.
learning for video captioning: A review. In IJCAI, 2019. Autoregressive times series methods for time domain as-
1 tronomy. FrontiersinPhysics,2018. 3
[17] Shaoxiang Chen, Wenhao Jiang, Wei Liu, and Yu-Gang [35] MarkusFreitagandYaserAl-Onaizan. Beamsearchstrate-
Jiang. Learningmodalityinteractionfortemporalsentence giesforneuralmachinetranslation. InACL,2017. 5[36] Lianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and [53] RanjayKrishna,KenjiHata,FredericRen,LiFei-Fei,and
HengTaoShen.Videocaptioningwithattention-basedlstm JuanCarlosNiebles.Dense-captioningeventsinvideos.In
andsemanticconsistency. TMM,2017. 1 ICCV,2017. 1,6
[37] Shenyuan Gao, Chunluan Zhou, Chao Ma, Xinggang [54] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen
Wang,andJunsongYuan. Aiatrack: Attentioninattention Tran. Hierarchicalconditionalrelationnetworksforvideo
fortransformervisualtracking. InECCV,2022. 7 questionanswering. InCVPR,2020. 2,6
[38] RossGirshick. Fastr-cnn. InICCV,2015. 2,6 [55] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg.
[39] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal- Tvqa: Localized,compositionalvideoquestionanswering.
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim, arXivpreprintarXiv:1809.01696,2018. 2
Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz [56] JieLei,LinjieLi,LuoweiZhou,ZheGan,TamaraLBerg,
Mueller-Freitag, et al. The” something something” video MohitBansal,andJingjingLiu. Lessismore: Clipbertfor
databaseforlearningandevaluatingvisualcommonsense. video-and-languagelearningviasparsesampling.InCVPR,
InICCV,2017. 5,6 2021. 6
[40] Bo He, Xitong Yang, Zuxuan Wu, Hao Chen, Ser-Nam [57] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Lim, and Abhinav Shrivastava. Gta: Global temporal at- Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves
tentionforvideoactionunderstanding. InBMVC,2021. 2 Stoyanov, and Luke Zettlemoyer. Bart: Denoising
[41] Bo He, Xitong Yang, Le Kang, Zhiyu Cheng, Xin Zhou, sequence-to-sequence pre-training for natural language
andAbhinavShrivastava. Asm-loc:Action-awaresegment generation,translation,andcomprehension. InACL,2020.
modeling for weakly-supervised temporal action localiza- 3,5,7
tion. InCVPR,2022. 1 [58] BoLi,WeiWu,QiangWang,FangyiZhang,JunliangXing,
[42] BoHe,JunWang,JielinQiu,TrungBui,AbhinavShrivas- and Junjie Yan. Siamrpn++: Evolution of siamese visual
tava, and Zhaowen Wang. Align and attend: Multimodal trackingwithverydeepnetworks. InCVPR,2019. 7
summarization with dual contrastive losses. In CVPR, [59] DongxuLi,JunnanLi,HongdongLi,JuanCarlosNiebles,
2023. 1 and Steven CH Hoi. Align and prompt: Video-and-
[43] BoHe,HengduoLi,YoungKyunJang,MenglinJia,Xuefei languagepre-trainingwithentityprompts. InCVPR,2022.
Cao,AshishShah,AbhinavShrivastava,andLimSer-Nam. 6
Ma-lmm: Memory-augmentedlargemultimodalmodelfor [60] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
long-termvideounderstanding. InCVPR,2024. 1 Blip-2: Bootstrapping language-image pre-training with
[44] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir- frozen image encoders and large language models. arXiv
shick. Maskr-cnn. InCVPR,2017. 2,6 preprintarXiv:2301.12597,2023. 4,5
[45] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejin [61] KunchangLi,YaliWang,PengGao,GuangluSong,YuLiu,
Choi. The curious case of neural text degeneration. In Hongsheng Li, and Yu Qiao. Uniformer: Unified trans-
ICLR,2020. 5 formerforefficientspatiotemporalrepresentationlearning.
[46] Jingyi Hou, Xinxiao Wu, Wentian Zhao, Jiebo Luo, and InICLR,2022. 5
YundeJia. Jointsyntaxrepresentationlearningandvisual [62] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and
cuetranslationforvideocaptioning. InICCV,2019. 6 TaoMei. Jointlylocalizinganddescribingeventsfordense
[47] LianghuaHuang,XinZhao,andKaiqiHuang. Got-10k:A videocaptioning. InCVPR,2018. 6
largehigh-diversitybenchmarkforgenericobjecttracking [63] YicongLi,XiangWang,JunbinXiao,WeiJi,andTat-Seng
inthewild. TPAMI,2019. 5 Chua.Invariantgroundingforvideoquestionanswering.In
[48] VladimirIashinandEsaRahtu. Multi-modaldensevideo CVPR,2022. 2
captioning. InCVPRW,2020. 2 [64] YanghaoLi, Chao-YuanWu, HaoqiFan, KarttikeyaMan-
[49] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, galam,BoXiong,JitendraMalik,andChristophFeichten-
Lin Ma, and Yu-Gang Jiang. Lumen: Unleashing versa- hofer.Mvitv2:Improvedmultiscalevisiontransformersfor
tilevision-centriccapabilitiesoflargemultimodalmodels. classificationanddetection. InCVPR,2022. 1
arXivpreprintarXiv:2403.07304,2024. 1 [65] KevinLin,LinjieLi,Chung-ChingLin,FaisalAhmed,Zhe
[50] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Gan,ZichengLiu,YumaoLu,andLijuanWang. Swinbert:
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Vi- End-to-end transformers with sparse attention for video
ola, Tim Green, Trevor Back, Paul Natsev, et al. The captioning. InCVPR,2022. 1,2,6
kinetics human action video dataset. arXiv preprint [66] ShengLiu,ZhouRen,andJunsongYuan. Sibnet: Sibling
arXiv:1705.06950,2017. 1,2,5,6 convolutionalencoderforvideocaptioning. InACMMM,
[51] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina 2018. 6
Toutanova. Bert: Pre-trainingofdeepbidirectionaltrans- [67] ShilongLiu,FengLi,HaoZhang,XiaoYang,XianbiaoQi,
formersforlanguageunderstanding. InNAACL,2019. 2 Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic
[52] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, anchorboxesarebetterqueriesfordetr. InICLR,2022. 4
Mingxing Tan, Matthew Brown, and Boqing Gong. [68] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Movinets:Mobilevideonetworksforefficientvideorecog- Stephen Lin, and Han Hu. Video swin transformer. In
nition. InCVPR,2021. 5 CVPR,2022. 1,2,5,6[69] IlyaLoshchilovandFrankHutter.Decoupledweightdecay [87] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
regularization. arXivpreprintarXiv:1711.05101,2017. 5 Martinet, Marie-Anne Lachaux, Timothe´e Lacroix, Bap-
[70] Chuofan Ma, Qiushan Guo, Yi Jiang, Ping Luo, Zehuan tiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar,
Yuan,andXiaojuanQi.Rethinkingresolutioninthecontext etal. Llama:Openandefficientfoundationlanguagemod-
ofefficientvideorecognition. InNeurIPS,2022. 2 els. arXivpreprintarXiv:2302.13971,2023. 1
[71] Christoph Mayer, Martin Danelljan, Danda Pani Paudel, [88] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
and Luc Van Gool. Learning target candidate association Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
tokeeptrackofwhatnottotrack. InICCV,2021. 7 Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
[72] ChristopherMeek,DavidMaxwellChickering,andDavid Llama 2: Open foundation and fine-tuned chat models.
Heckerman. Autoregressive tree models for time-series arXivpreprintarXiv:2307.09288,2023. 1
analysis. InICDM,2002. 3 [89] DuTran,HengWang,LorenzoTorresani,JamieRay,Yann
[73] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al- LeCun, and Manohar Paluri. A closer look at spatiotem-
subaihi,andBernardGhanem. Trackingnet: Alarge-scale poralconvolutionsforactionrecognition. InCVPR,2018.
datasetandbenchmarkforobjecttrackinginthewild. In 5
ECCV,2018. 1,2,5,7 [90] DuTran,HengWang,LorenzoTorresani,andMattFeiszli.
[74] JonghwanMun,LinjieYang,ZhouRen,NingXu,andBo- Video classification with channel-separated convolutional
hyungHan.Streamlineddensevideocaptioning.InCVPR, networks. InICCV,2019. 5
2019. 6 [91] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
[75] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local- Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
global video-text interactions for temporal grounding. In andIlliaPolosukhin.Attentionisallyouneed.InNeurIPS,
CVPR,2020. 1 2017. 5
[76] DanielNeimark,OmriBar,MayaZohar,andDotanAssel- [92] Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, and
mann. Videotransformernetwork. InICCV,2021. 5 BastianLeibe.Siamr-cnn:Visualtrackingbyre-detection.
[77] Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, InCVPR,2020. 7
Adrien Gaidon, Ehsan Adeli, and Juan Carlos Niebles. [93] BairuiWang,LinMa,WeiZhang,WenhaoJiang,Jingwen
Spatio-temporalgraphforvideocaptioningwithknowledge Wang, and Wei Liu. Controllable video captioning with
distillation. InCVPR,2020. 6 possequenceguidancebasedongatedfusionnetwork. In
[78] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan ICCV,2019. 6
Misra, Florian Metze, Christoph Feichtenhofer, Andrea [94] HengWang,DuTran,LorenzoTorresani,andMattFeiszli.
Vedaldi, and Joao F Henriques. Keeping your eye on Videomodelingwithcorrelationnetworks.InCVPR,2020.
the ball: Trajectory attention in video transformers. In 5
NeurIPS,2021. 5 [95] JingwenWang,WenhaoJiang,LinMa,WeiLiu,andYong
[79] JieQin, LiLiu, LingShao, FuminShen, BingbingNi,Ji- Xu. Bidirectional attentive fusion with context gating for
axin Chen, and Yunhong Wang. Zero-shot action recog- densevideocaptioning. InCVPR,2018. 6
nitionwitherror-correctingoutputcodes. InCVPR,2017. [96] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,
7 LuoweiZhou,YuchengZhao,YujiaXie,CeLiu,Yu-Gang
[80] KevinQinghongLin,PengchuanZhang,JoyaChen,Shra- Jiang, and Lu Yuan. Omnivl: One foundation model for
manPramanick,DifeiGao,AlexJinpengWang,RuiYan, image-language and video-language tasks. In NeurIPS,
and Mike Zheng Shou. Univtg: Towards unified video- 2022. 1,2,6
languagetemporalgrounding. InICCV,2023. 2 [97] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai,
[81] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya LuYuan, ZuxuanWu, andYu-GangJiang. Chatvideo: A
Sutskever,etal.Improvinglanguageunderstandingbygen- tracklet-centricmultimodalandversatilevideounderstand-
erativepre-training. OpenAIBlog,2018. 1 ingsystem. arXivpreprintarXiv:2304.14407,2023. 1
[82] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, [98] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,
DarioAmodei,IlyaSutskever,etal. Languagemodelsare Xiyang Dai, Lu Yuan, and Yu-Gang Jiang. Omnitracker:
unsupervisedmultitasklearners. OpenAIBlog,2019. 1 Unifyingobjecttrackingbytracking-with-detection. arXiv
[83] PaulHongsuckSeo,ArshaNagrani,andCordeliaSchmid. preprintarXiv:2303.12079,2023. 3
Lookbeforeyouspeak:Visuallycontextualizedutterances. [99] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,
InCVPR,2021. 6 ChuanxinTang,XiyangDai,YuchengZhao,YujiaXie,Lu
[84] KarenSimonyanandAndrewZisserman. Two-streamcon- Yuan, and Yu-Gang Jiang. Look before you match: In-
volutional networks for action recognition in videos. In stanceunderstandingmattersinvideoobjectsegmentation.
NeurIPS,2014. 1 InCVPR,2023. 1
[85] ArnoldWMSmeulders,DungMChu,RitaCucchiara,Si- [100] LijunWang,WanliOuyang,XiaogangWang,andHuchuan
moneCalderara,AfshinDehghan,andMubarakShah. Vi- Lu. Visualtrackingwithfullyconvolutionalnetworks. In
sualtracking:Anexperimentalsurvey. TPAMI,2013. 1 ICCV,2015. 2,4
[86] RuiTian,ZuxuanWu,QiDai,HanHu,YuQiao,andYu- [101] Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li.
GangJiang. Resformer: Scalingvitswithmulti-resolution Transformermeetstracker:Exploitingtemporalcontextfor
training. InCVPR,2023. 1 robustvisualtracking. InCVPR,2021. 7[102] Qian Wang and Ke Chen. Alternative semantic represen- CordeliaSchmid. Vid2seq:Large-scalepretrainingofavi-
tationsforzero-shothumanactionrecognition. InECML suallanguagemodelfordensevideocaptioning. InCVPR,
PKDD,2017. 7 2023. 1,2,4,5,6
[103] TengWang,RuimaoZhang,ZhichaoLu,FengZheng,Ran [118] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,
Cheng,andPingLuo. End-to-enddensevideocaptioning RussRSalakhutdinov,andQuocVLe. Xlnet:Generalized
withparalleldecoding. InICCV,2021. 2,6 autoregressive pretraining for language understanding. In
[104] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, NeurIPS,2019. 3
XizhouZhu,GangZeng,PingLuo,TongLu,JieZhou,Yu [119] BotaoYe,HongChang,BingpengMa,ShiguangShan,and
Qiao, et al. Visionllm: Large language model is also an XilinChen.Jointfeaturelearningandrelationmodelingfor
open-endeddecoderforvision-centrictasks.arXivpreprint tracking:Aone-streamframework. InECCV,2022. 7
arXiv:2305.11175,2023. 3 [120] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object
[105] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun tracking:Asurvey. CSUR,2006. 1
Huang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu,Zun [121] RowanZellers,JiasenLu,XimingLu,YoungjaeYu,Yan-
Wang,etal. Internvideo:Generalvideofoundationmodels pengZhao,MohammadrezaSalehi,AdityaKusupati,Jack
viagenerativeanddiscriminativelearning. arXivpreprint Hessel,AliFarhadi,andYejinChoi.Merlotreserve:Neural
arXiv:2212.03191,2022. 1,2 script knowledge through vision and language and sound.
InCVPR,2022. 6
[106] XingWei,YifanBai,YongchaoZheng,DahuShi,andYi-
hong Gong. Autoregressive visual tracking. In CVPR, [122] JunchaoZhangandYuxinPeng. Object-awareaggregation
2023. 3,5,7 withbidirectionaltemporalgraphforvideocaptioning. In
CVPR,2019. 6
[107] Zuxuan Wu, Xi Wang, Yu-Gang Jiang, Hao Ye, and Xi-
angyangXue. Modelingspatial-temporalcluesinahybrid [123] QiZhang,YuqingSong,andQinJin.Unifyingeventdetec-
deeplearningframeworkforvideoclassification. InACM tionandcaptioningassequencegenerationviapre-training.
MM,2015. 1 InECCV,2022. 6
[124] ZhipengZhang, HouwenPeng, JianlongFu, BingLi, and
[108] ZuxuanWu,ZejiaWeng,WujianPeng,XitongYang,Ang
Weiming Hu. Ocean: Object-aware anchor-free tracking.
Li,LarrySDavis,andYu-GangJiang. Buildinganopen-
InECCV,2020. 7
vocabularyvideoclipmodelwithbetterarchitectures,opti-
mizationanddata. TPAMI,2024. 1 [125] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin
Wang,WeimingHu,andZheng-JunZha. Objectrelational
[109] ZhenXing,QiDai,ZihaoZhang,HuiZhang,HanHu,Zux-
graph with teacher-recommended learning for video cap-
uan Wu, and Yu-Gang Jiang. Vidiff: Translating videos
tioning. InCVPR,2020. 6
viamulti-modalinstructionswithdiffusionmodels. arXiv
[126] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and
preprintarXiv:2311.18837,2023. 1
Weiming Hu. Learn to match: Automatic matching net-
[110] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,
workdesignforvisualtracking. InICCV,2021. 7
XiangnanHe,andYuetingZhuang.Videoquestionanswer-
[127] Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan,
ingviagraduallyrefinedattentionoverappearanceandmo-
BingLi, YingDeng, andWeimingHu. Open-bookvideo
tion. InCVPR,2017. 5,6
captioningwithretrieve-copy-generatenetwork. InCVPR,
[111] JunXu,TaoMei,TingYao,andYongRui.Msr-vtt:Alarge
2021. 6
videodescriptiondatasetforbridgingvideoandlanguage.
[128] QiZheng,ChaoyueWang,andDachengTao.Syntax-aware
InCVPR,2016. 1,2,5,6
actiontargetingforvideocaptioning. InCVPR,2020. 6
[112] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and
[129] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards
HuchuanLu. Learningspatio-temporaltransformerforvi-
automatic learning of procedures from web instructional
sualtracking. InICCV,2021. 7
videos. InAAAI,2018. 2
[113] BinYan, YiJiang, PeizeSun, DongWang, ZehuanYuan,
[130] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard
PingLuo,andHuchuanLu. Towardsgrandunificationof
Socher,andCaimingXiong. End-to-enddensevideocap-
objecttracking. InECCV,2022. 3,7
tioningwithmaskedtransformer. InCVPR,2018. 2
[114] BinYan,YiJiang,JiannanWu,DongWang,PingLuo,Ze-
[131] Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, and Ling
huanYuan,andHuchuanLu.Universalinstanceperception
Shao. Towards universal representation for unseen action
asobjectdiscoveryandretrieval. InCVPR,2023. 3,7
recognition. InCVPR,2018. 7
[115] ShenYan, XuehanXiong, ArshaNagrani, AnuragArnab,
Zhonghao Wang, Weina Ge, David Ross, and Cordelia
Schmid. Unloc: A unified framework for video localiza-
tiontasks. InICCV,2023. 2
[116] AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,and
CordeliaSchmid. Justask: Learningtoanswerquestions
frommillionsofnarratedvideos. InICCV,2021. 6
[117] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-
toineMiech,JordiPont-Tuset,IvanLaptev,JosefSivic,and