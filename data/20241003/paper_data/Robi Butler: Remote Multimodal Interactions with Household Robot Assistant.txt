Robi Butler: Remote Multimodal Interactions with
Household Robot Assistant
Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu
Abstract—In this paper, we introduce Robi Butler, a novel
household robotic system that enables multimodal interactions
with remote users. Building on the advanced communication
interfaces, Robi Butler allows users to monitor the robot’s
status, send text or voice instructions, and select target objects
by hand pointing. At the core of our system is a high-level
behaviormodule,poweredbyLargeLanguageModels(LLMs),
thatinterpretsmultimodalinstructionstogenerateactionplans.
Theseplansarecomposedofasetofopenvocabularyprimitives
supported by Vision Language Models (VLMs) that handle
both text and pointing queries. The integration of the above
components allows Robi Butler to ground remote multimodal
instructionsinthereal-worldhomeenvironmentinazero-shot Fig. 1: The Robi Butler system enables a user to Zoom-call the butler
manner.Wedemonstratetheeffectivenessandefficiencyofthis robot remotely at home and interact with it naturally through both the
system using a variety of daily household tasks that involve languageandgestures.ThesystemleveragestheZoominterfaceforremote
remote users giving multimodal instructions. Additionally, we interaction.
conducted a user study to lyze how multimodal interactions
ability to interpret and execute the open multimodal instruc-
affect efficiency and user experience during remote human-
tions in real-world environments. Inspired by the advanced
robotinteractionanddiscussthepotentialimprovements.Link:
https://robibutler.github.io/. capabilitiesoffoundationmodelstoachieveopenvocabulary
mobile manipulation in domestic environments [1]–[4], we
I. INTRODUCTION aim to incorporate the LLM-based robots with the ability
to make use of the language-related gestures. To allow the
Imagine a future where distance no longer constrains our robot to ground both open language instruction and open
ability to manage household tasks. Picture a robot assistant pointingselection,wefirstimplementamobilemanipulation
capable of remotely interpreting spoken commands and system that supports open vocabulary action primitives with
gestures to check your refrigerator or reheat a meal. Such pointing selection in real-world household environments,
a robotic system would fundamentally change the way we driven by the recent advances in vision language models
interactwithourhomes,bringinganewlevelofconvenience (VLMs). Then, we introduce a high-level behavior manager
and efficiency to daily life. In this work, we propose Robi poweredbylargelanguagemodels(LLMs),whichorganizes
Butler,amultimodalinteractionsystemthatenablesseamless and aligns the received speech and gesture instructions to
communication between remote users and household robots generate the plan to solve the task.
to execute various household tasks. Robi Butler allows users Overall, the integrated system, Robi Butler, is a multi-
to leverage both natural language and gestures to command modal interactive system for robotic home assistants that
therobottoperformtasksremotely,seeFig.1.Remoteusers enables bi-directional remote human-robot interaction based
canpointtothedesiredobjectintheMRdeviceandinstruct on the real home environment through text, voice, video,
the robot to manipulate it, move toward it, or ask questions and gesture. We evaluated the performance of Robi Butler
about it, just like a real butler. on real-world daily household tasks and studied the benefits
The core issue behind building such a robot assistant is of such multimodal interaction in terms of efficiency and
how to allow the robot to remotely receive, understand, user experience in the remote human-robot interaction.
and ground the multimodal instructions into the executable
actions in the home environment. To address this, we first
II. RELATEDWORK
design the communication interfaces consisting of a Zoom A. Language and Gestures in Human-Robot Interaction
chat website and a gesture website for hand-pointing, which
Effective communication interfaces are essential for
allows human users to send multimodal instructions using
Human-Robot Interaction (HRI). Natural language instruc-
languageandpointingremotely.Togroundthereceivedmul-
tion for robots has been widely explored in prior research,
timodalinstructionsinthehome,therobotneedstohavethe
employing both traditional methods [5]–[10] and language
models [1],[11]–[17].However,languagecanbeambiguous
AllauthorsarewiththeSchoolofComputing,NationalUniversityofSin- and imprecise. Humans typically use nonverbal interaction,
gapore,Singapore.Correspondtoanxingx@comp.nus.edu.sg.
suchaspointing,tosupplementtheirverbalinstructions[18].
NuwanJanakaandDavidHsuarealsowiththeSmartSystemsInstitute,
NationalUniversityofSingapore,Singapore. Previous work explores the use of tools such as laser
4202
peS
03
]OR.sc[
1v84502.9042:viXraRobButler
I Pn os it nr tu Sct ei lo en c, t ion High-level Behavior Module Feedback
Human User Response Plan Environments
Text,
Voice, Communication Interface Fundamental Skills
Gesture LLM-powered task planning Execution
Hi Robi, take this to
the coffee table. Text-target alignment
≈ ≈
V Teid xte ,o, Zoom-based communication Text/point-query Skills Information
Voice
Target selection via gesture Question answering via
mobile manipulation
Fig. 2: The conceptual framework of the proposed system, as discussed in Sec III-A. The robot system consists of three components: Communication
Interfaces,High-levelBehaviorManager,andFundamentalSkills.TheCommunicationInterfacestransmittheinputsreceivedfromtheremoteusertothe
High-levelBehaviorModule,whichcomposestheFundamentalSkilltointeractwiththeenvironmenttofulfilltheinstructionsoranswerquestions.
pointers [19] and point-and-click interfaces [20] to improve multimodal system, Robi Butler, that combines speech com-
instruction delivery and further integrate both speech and mandsandgestureinputs,allowingremoteuserstonaturally
relevant gestures together [21]–[23] to specify the command guide a robotic assistant to perform household tasks.
more precisely. However, these systems typically rely on
predefined word sets or task-specific in-domain model train- A. System Overview
ing, which limits generalization. Recent work uses LLMs The developed Robi Butler system is illustrated in Fig. 2.
to interpret gestures and commands [24], but only handles It enables seamless interaction between a user wearing a
short speech inputs and requires the user to be within the Mixed Reality (MR) Head-Mounted Display (HMD) and a
third-person camera view. Our system is built on top of a robot. Users can send text/voice instructions L and gesture
multimodal communication interface to construct a virtual selections G to the robot while receiving video streams and
clickableworldthatallowstheremoteusertoselectthetarget text/voice feedback F in return. The robotic system com-
bypointingwhilespeaking,andtherobotcouldinterpretand prises three key components. The communication interfaces
executethemultimodalinstructionsinthehomeenvironment C facilitate bidirectional communication, receiving user in-
with a mobile manipulator. puts and transmitting robot feedback. The high-level behav-
ior module H, interprets user instructions L and gesture
B. Household Robot Assistant
selections (G) to understand the intent, generating an action
Intelligent home robots with mobile manipulation ca-
sequenceP ={a ,a ,...,a }fortherobottoexecute,along
0 1 N
pabilities can greatly expand functionality and integrate
witharesponseRtotheuser.Thisresponsecanbelow-level
more seamlessly into daily routines. While past household
executionfeedbackorgeneralinformation.Thefundamental
mobile manipulation systems have been developed both in
skills A, provide core functionality that allows the robot to
simulation [25]–[27] and real-world settings [28]–[32], they
perceive and interact with the environment. These include
generally struggle with human-robot interaction due to their
basic mobile manipulation and Visual Question Answering
reliance on predefined tasks and limited language input.
(VQA)capabilities:move(),pick(),placeon(),open(),close()
They would require users to select from fixed options or
and vqa(). Note that all skills except open() and close()
explicitly re-programme the robot. More recent approaches
support both text and pointing queries.
leverage vision-language-based models (VLMs) to enable
open-vocabulary mobile manipulation in domestic environ- B. Hardware Setup
ments [1]–[4], but they rely solely on language instructions
Our system integrates multiple hardware components to
and lack closed-loop human-robot interaction. Another area
facilitateeffectivehuman-robotinteraction.Theprimaryuser
of research explores treating robot assistants as “physical
interface is an Oculus Quest 3 MR headset, while the
avatars”,whichallowsremoteuserstoteleoperatetherobots
roboticplatformconsistsofaFetchmobilemanipulator[38]
using VR controllers [33], haptic devices [34], haptic gloves
with a differential-drive base and a 7-dof arm. Tasks that
[35], and hand tracking [36]. However, these approaches
require heavy computation are distributed between a local
can result in a high cognitive workload [37], making them
workstation powered by an NVIDIA RTX 4090 GPU and
impractical for everyday use. In this paper, we present a
a remote cloud server. To enhance user visual feedback, we
human-robot interaction system for remote user to naturally
incorporate two additional cameras that provide third-person
instruct open-vocabulary mobile manipulation with multi-
views of the robot’s operational environment.
round interaction using both language and gestures.
III. OVERVIEW IV. SYSTEMIMPLEMENTATION
This work addresses the problem of remote human-robot The system has a multimodal communication interface, a
interaction for household robot assistants. We present a high-level behavior module, and low-level action modules.User Chat Text Robot Video R too b thi, e c ta an b y leo .u bring this
Zoom Website Gesture Website V To ei xce t, Selenium
Text
Zoom Website
Hand Tracking V VTi oed x ie cto e,, Audio Whisper Text User Sure! I can help with that. p mic ok v( e ( “ t a), b le”),
placeon(“table”)
Click Point Point Robi
Gesture Website Server
Video Video Video (a)Theuserdirectlygivesmultimodalinstructionstotherobot.
Video Robi, take the apple from
Fig.3:Theframeworkofcommunicationinterfaces. kitchen counter to the sofa.
A. Communication Interfaces User I'm on my way. m pico kv (e “( a“ pk pit lc eh ”e ),n counter”),
As shown in Fig. 3, the communication interfaces enable move(“sofa”),
Robi placeon(“sofa”)
multimodal remote interaction between humans and robots, Which one are you referring to?
utilizing voice, text, and gestures. These interfaces consist
of two main components: a Zoom platform and a gesture Video Robi
selection website. The Zoom platform supports voice, text,
andvideocommunication,whiletheSeleniumlibraryonthe Got it. pick( ),
User move(“sofa”),
robot’s server extracts specific text elements from the chat placeon(“sofa”)
Robi
box during live sessions. For speech recognition, we employ
(b)Theusergiveslanguageandgestureinstructionsseparately.
the Whisper model [39]. For gesture-based interactions, we
Fig.5:Human-RobotRemoteinteractionsvialanguageandgestures.
developed a website using Flask that allows users to select
target objects by pointing. The site streams the robot’s is resolved using the latest gesture selection. We store the
first-person video frames at 5 Hz, and the selected points five most recent gesture selections and match them with the
are transmitted to the robot server in real-time, enabling “*” parameters during execution. Additionally, the system
immediate planning and execution. supports gesture-only input for disambiguation when the
detection model identifies multiple objects in response to
B. High-level Behavior Module
a single query. In such cases, the robot prompts, “Which
Thehigh-levelbehaviormoduleinterpretsanddecomposes
one are you referring to?”, pausing for the user to select the
usermultimodalinstructions,comprisinglanguageinputs(L)
targetobject.Fig.5illustratesthealignmentbetweengesture
andgestureinputs(G),intoexecutableactionsequencesP =
selections and the LLM-generated plan.
{a ,a ,...,a |a ∈A},alongwithcorrespondingresponses
0 1 N i
(R). This module processes both inputs, leveraging an LLM
C. Fundamental Skills
to generate structured responses and action plans. These
are then passed to the execution module, which integrates 1) Manipulation: Fortherobottophysicallyinteractwith
the gesture inputs to ensure precise alignment between user theenvironment,itisequippedwithmanipulationskillssuch
gestures and robot actions, as depicted in Fig. 4. as picking/placing items, and opening/closing appliances.
Pick and Place Policy
Instruction Robi: Fig. 6 illustrates the modular framework for the pick
Sure, I can help with that.
User: Can you throw this LLM as Planner Code: [pick(*), policy. The pick() function accepts either a text query
avocado to the trash can? move("trash can"), pick(text) or a pointing query pick(point). We employ the
placeon("trash can")]
pre-trained open-vocabulary detection model OWLv2 [40]
Gesture Selection
and the Segment Anything model [41] to generate the target
Execution
objectmask.Thismaskisthencombinedwiththepre-trained
graspingmodelContact-GraspNet[42]todeterminegrasping
Fig.4:Theframeworkofhigh-levelbehaviormodule.
poses. Grasping poses are filtered based on orientation, and
The task planner in the high-level behavior module, il-
theposewiththehighestscoreisselected.Astraightforward
lustrated in Fig. 4, is powered by an LLM (OpenAI GPT-
pre-graspandgraspstrategyisapplied,witharmtrajectories
4o-2024-05-13) prompted to function as a household robot
generatedusingthemotionplanningtoolsfromMoveIt[43].
assistant. The prompt defines the robot’s role, a list of
knownlocations,fundamentalskillsitcanperform,andfew-
shot examples to demonstrate how these skills should be Text Query
used. Full prompts for the LLM can be found at https: ‘apple’ Grasps
//robibutler.github.io. To align instructions with RGB Image
Motion Planning
gestureselections,weimplementarule:wheninputscontain OWLv2 Mask
the keywords “this” or “here”, the planner generates “*” Arm Trajectory
Segment
Contact GraspNet
as an action parameter to resolve ambiguities, particularly Anything
demonstrative pronouns [18]. For example, the instruction
Point Query Depth
“Robi, please pick this and put it on the plate” results in the
plan [pick(*), placeon(“plate”)]. During execution, the “*”
Fig.6:Theopen-vocabularypickpipeline.Text Query: Known Location? Yes Object Location Path and Motion RGB Camera: Marked Image:
‘chair’ Planning
No
RGB Camera visual
OWLv2 Mask Base Trajectory mark
Point Query Segment
Point Cloud
Anything
Depth User:
Text Query: Answer the question related to this image in one sentence:
{Question}
Fig.7:Thenavigationpipeline. What is this? The mark is drawn on the figure to refer the target object.
When answer, don't include mark, just answer the question.
GPT-4o:
A box of tea.
The place policy, similar to the pick policy, utilizes the
same perception modules and can handle both text and
Fig.8:Exampleofthequestionansweringviapointreferring.
pointingqueries.Afterobtainingthesegmentedpointclouds,
thecenteroftheplacepositioniscalculatedintheX-Yplane, not be sufficient for precise specification of the question.
while the height is determined by adding 0.2 meters to the Therefore, we allow the robot to answer the user’s verbal/-
highest point of the segmented point clouds. For larger fixed textual questions in combination with a pointing gesture,
objects or locations, such as tables, counters, and trash cans, as shown in Fig. 8, denoted vqa(text, pointing). We apply
a pre-defined location is used to simplify the setting. a simple visual prompting method for GPT-4o to answer
specific questions by annotating the image with a mark.
Open and Close Policy Similar to [44], the open/close
policies rely on imitation learning to handle complex
V. EXPERIMENTSANDRESULTS
actions such as opening and closing a fridge, a cabinet,
and a microwave. We collected an average of 50 trajectory Tounderstandtheusageandimpactofmultimodalremote
demonstrations per action using a real robot teleoperated interaction in remote HRI, we evaluate the performance of
by a human using a VR controller. These demonstrations the Robi Butler guided by the following research questions:
were used to train models using Action Chunking with RQ1: How effectively and robustly does the Robi Butler
Transformers (ACT) [45]. The model takes RGB-D images enable remote users to complete household tasks?
andtherobotarm’sjointstatesasinputtopredictjointangle RQ2: How do the user interaction modalities (voice, ges-
movement sequences. Demonstrations of the learned skills tures) affect the performance and usability of Robi Butler?
can be viewed at https://youtu.be/ajfPVjjlBcI.
A. Experiment I: Robi Butler System Performance Evalua-
tion
2) Navigation: As shown in Fig. 7, our system integrates
Inthisexperiment,weevaluatetheRobiButler systemon
bothpredefinednavigationplacesandopen-worldnavigation
a set of daily household tasks to understand its effectiveness
to locate and move to the target object. First, we create
and answer RQ1.
an occupancy map using Gmapping [46] and define the
1) Experimental Design: The tasks were designed based
navigation waypoint for the known locations in the map.
on the American Time Use Survey [47]. These tasks fall
In addition to predefined locations, the system supports
underthecommondailyhouseholdactivities,includingFood
navigation to non-predefined locations via voice/text and
and drink preparation (0.50 hr/day), Interior cleaning (0.35
gesture/point queries, similar to the perception pipeline in
hr/day), Household & personal organization and planning
the pick policy (Sec IV-C.1). We utilize the off-the-shelf
(0.11 hr/day), and Medical and care services (0.06 hr/day).
path and motion planning algorithm provided by the ROS
The ten selected tasks (T1-T10)—detailed below—required
Navigation Stack to generate the path and motion trajectory.
the robot to interpret remote users’ language and point-
3) Visual Question Answering: Our system is capable of
ing gestures, then perform the corresponding actions (e.g.,
answering users’ open-ended questions about the objects in
rearranging objects, answering questions). The object that
the robot’s environment. Specifically, for the actions vqa(),
requires disambiguation is highlighted in bold.
our system applies GPT-4o and supports:
T1. Throw target avocado into the trash can.
Question answering via mobile manipulation. To answer
T2. Check the beer inside the fridge.
the question “Do we have any beer left in the fridge?”, the
T3. Check medicine on the coffee table and bring target one to
robot should first navigate to the fridge, open it, and then the sofa.
query the VLM model. Our solution treats the VQA as a T4. Describe the target object in the cabinet.
single action and uses the reasoning capabilities of LLMs to T5. Bring the target drink to the coffee table.
T6. Move the target cup to the kitchen counter.
determine the necessary high-level steps before performing
T7. Fetch the target remote and place it on the sofa.
VQA. Given the question q, the high-level behavior module
T8. Navigate to a target chair and check if it’s clean.
decomposes the question into a series of actions to be T9. Check if the laptop is open.
executed before querying GPT-4o for the final answer. T10. Bring the target tool to the table.
ToevaluatetheeffectivenessofRobiButler,thefollowing
Question answering via point referring. While text-only metricswereused:Task Success Rate (Task SR):definedas
input allows users to ask questions, the single modality may the percentage of tasks completed. Planning Success Rate(a) (b) (c) (d) (e)
(f) (g) (h) (i) (j)
Fig. 9: Snapshot of completing the task T1 and T4. (a): User asks Robi to go to the table. (b): User asks Robi to throw away the avocado. (c): Robi
attemptstopickuptheavocado.(d):Robibringstheavocadotothetrashcan.(e):Robidisposesoftheavocado.(f):UserasksRobitoopenthecabinet.
(g):Robireachesthecabinet.(h):Robiattemptstoopenthecabinet.(i):UserasksRobitoidentifyanobject.(j):Robiidentifiesitas“Aboxoftea.”
TABLE I: Real-world Experiments Result for Experiment I. Tasks that
in approximately 105 seconds, demonstrating its efficiency
requiretheuser’sselectionareindicatedusing⋆.Interactionsincludeboth
Voice(V)andGestures(G). in performing household tasks in a complex environment.
The system required an average of 2.3 interactions per task,
Task TaskSR PlanningSR Time Interactions(V+G)
with 1.5 voice commands and 0.8 gesture inputs. This low
T1⋆ 3/3 3/3 119.7s 3(2+1) number of interactions demonstrates the system’s efficiency
T2 3/3 3/3 153.0s 1(1+0)
T3⋆ 3/3 3/3 128.3s 3(1+0) inhuman-robotcommunication,requiringminimaluserinput
T4⋆ 2/3 3/3 147.0s 3(2+1) toeffectivelyguidetherobot.Whiletheoverallperformance
T5⋆ 3/3 3/3 86.0s 2(2+1)
T6⋆ 3/3 3/3 95.3s 2(1+1) of the system is generally satisfactory, answering RQ1,
T7⋆ 3/3 3/3 117.0s 3(2+1) further improvements in low-level action execution could
T8⋆ 3/3 3/3 64.0s 2(1+1)
help increase the overall performance and efficiency. Fig.
T9 3/3 3/3 57.3s 2(2+0)
T10⋆ 3/3 3/3 82.3s 2(1+1) 9 shows the process of two example tasks. More videos of
Mean 96.7% 100% 105.0s 2.3(1.5+0.8) the tasks are available at the website1.
(PlanningSR):definedasthepercentageoftaskscompleted B. ExperimentII:TheEffectofModalityonUserExperience
when execution errors are ignored. Task Completion Time, To investigate user experience, the impact of multimodal
measuring the average time required to complete each task. communication, and challenges, we conducted this experi-
Average Interactions: calculating the average number of ment with novice users to address RQ2.
voice and gesture interactions required per task. A task is 1) Experimental Design: We recruited twelve volunteers
considered successful/completed if the goal is achieved or
(P1–P12; 7 males, 5 females from the university com-
if correct answers are provided to the remote user within 5
munity. None of the participants had familiar experience
minutes. After obtaining informed consent, the expert user
with AR/MR equipment. We compared the performance of
evaluated Robi Butler on 10 tasks in a fixed order, each Robi Butler with two baseline systems by removing user
repeated three times.
interaction modalities, similar to an ablation study, resulting
2) Analysis and Results: Table I presents the task per- in three systems: Gesture-only, Voice-only, and Robi Butler
formance results. Overall, Robi Butler achieved a high (Gesture+Voice). In the Gesture-only system, buttons were
average task success rate of 96.7%, reflecting its strong added for participants to select the action to be executed.
ability to perform a variety of household tasks in real- For the Voice-only system, we adapted the interactive visual
world environments. However, the task success rate lags grounding model from [48]. The use of the two baseline
slightly behind the perfect planning success rate of 100%, can be found in the website1. Three representative tasks, T1
indicating challenges related to low-level action execution (object rearrangement), T2 (monitoring), and T3 (object re-
rather than planning processes. For instance, in task T4, an arrangement + monitoring), were selected from the previous
error occurred when the system misidentified a green tea
box as a tissue bag. On average, the system completed tasks 1https://robibutler.github.ioFig.10:Measuresrelatedtoefficiencyanduserexperiencesofdifferentsystemswith12participants.ForSuccessRate,Trust,andSUS,thehigher,the
better;forAvgTimeandNASATLX,thelower,thebetter.Forstatisticalsignificance,oneasterisk(*)isp<0.05;twoasterisks(**)isp<0.01.
SD = 26.6). The reduced task completion time for voice-
supported systems was primarily due to the ability to use
Fridge
voice commands to express combined queries, whereas with
theGesture-onlysystem,participantshadtoperformmultiple
Trash Can manual button clicks, increasing task completion time.
Coffee Table Regarding the trust, the Robi Butler (M = 5.77, SD =
Sofa
0.97) was perceived as significantly more trustworthy com-
pared to both the Gesture-only system (M = 4.98, SD =
Table
0.85, p < 0.05) and the Voice-only system (M = 4.71,
SD = 1.03, p < 0.05). This suggests that combining
Fig.11:Visualizationoftheexperimentalenvironment.Theorangetrajec- gestures with voice commands fosters greater confidence in
toryrepresentsT1,thegreenrepresentsT2,andthebluerepresentsT3. system reliability and consistency, outperforming systems
relying solely on a single modality. P2 reasoned that “I
experiment (Sec V-A.1). As shown in Fig. 11, these tasks
trusted the gesture plus voice system the most because
engaged the main areas of the home environment.
I found it easier to avoid making mistakes with it. For
The study used a within-subject design with three system
language only, sometimes it may misunderstand me. For
conditions as the independent variable, counterbalanced via
gestures, I have to do the interaction multiple times.”
aLatinSquare,tominimizeorderingeffects.Tasksincreased
FortheSUS,participantsgavetheGesture-onlythelowest
in difficultyand werepresented ina fixedorder. Participants
usability score (M = 47.29, SD = 15.90), which signifi-
completed all three tasks with each system (nine tasks total)
cantlylowerthanbothVoice-only(M =70.42,SD =11.62,
and filled out a questionnaire after each system to assess
p < 0.01) and Robi Butler (M = 75.83, SD = 9.61,
their subjective experience. In addition to the Task SR and
p < 0.01). This also indicates that Robi Butler achieved
Task Completion Time measure from V-A.1, the following
‘Good’ usability (i.e., SUS > 75 [52]) compared to the
additional measures were used to assess user experience:
other systems.
NASA-TLX [49], assessing the perceived workload experi-
Overall,theRobiButlerachievesthebestperformance,the
enced by participants with each system. System Usability
highest usability, and the minimum perceived cognitive load
Scale (SUS) [50], evaluating perceived system usability.
among the baselines, answering RQ2. This was primarily
Trust [51], measuring the participants’ trust. We used the
due to the complementary nature of voice and gesture inter-
reliable subscale under Capacity Trust.
actions, where voice enabled natural and combined queries.
2) Analysis and Results: Fig. 10 shows the task perfor-
In contrast, gestures facilitated the disambiguation of voice
mance of the three systems. A one-way repeated measures
commands related to locations and provided precise spatial
ANOVAwasconductedtoanalyzethequantitativedataafter
annotations. Although multimodal interaction generally out-
confirming normality assumptions. Both the Gesture-only
performed unimodal interaction, P10 expressed a negative
and Gesture+Voice (i.e., Robi Butler) systems achieved a
sentiment, stating, “Using both voice and gesture is [some-
perfect task success rate of 100%, while the voice system
times] hard, as I need to switch between two modalities. I
had a slightly lower, though non-significant, success rate
prefervoice-onlyasIdon’tneedtomovemyarmphysically.”
of 94.4%. This difference was attributed to errors in target
Future improvements, such as incorporating eye gaze track-
referencing with voice commands only. For example, the
ing to minimize hand interactions, could potentially reduce
voice recognition system misinterpreted the word ‘right’ as
such physical workload.
‘red’, leading to the grounding error. Additionally, Robi
Butler (M = 143.8, SD = 14.8) had a significantly
VI. CONCLUSION
lower task completion time than the Gesture-only system
(M = 170.00, SD = 21.4) (p < 0.05), but was not This work introduces an interactive robotic assistant for
significantly lower than the Voice-only system (M =157.1, household tasks using multimodal interactions with remoteusers. We outline three core components of the robot but- [14] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao,
ler system and demonstrate its effectiveness in assistive and Y. Su, “Llm-planner: Few-shot grounded planning for embodied
agentswithlargelanguagemodels,”inProceedingsoftheIEEE/CVF
question-answering and object rearrangement. Experiments
InternationalConferenceonComputerVision,2023.
show Robi Butler grounds multimodal instructions with a [15] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,
high task success rate, reasonable time, and minimal inter- A.Wahid,J.Tompson,Q.Vuong,T.Yuetal.,“Palm-e:anembodied
multimodallanguagemodel,”inProceedingsofthe40thInternational
actions. Follow-up tests confirm that combining voice and
ConferenceonMachineLearning,2023,pp.8469–8488.
gestures enhances usability and trust, and reduces cognitive [16] K.Rana,J.Haviland,S.Garg,J.Abou-Chakra,I.Reid,andN.Suen-
load compared to unimodal systems. In future work, we aim derhauf,“Sayplan:Groundinglargelanguagemodelsusing3dscene
graphs for scalable robot task planning,” in Conference on Robot
tomakeRobiButler moreadaptable,capableofautonomous
Learning,2023.
skill learning, personalized interactions, and handling com- [17] L.X.Shi,Z.Hu,T.Z.Zhao,A.Sharma,K.Pertsch,J.Luo,S.Levine,
plex tasks that may require tactile feedback [53]. andC.Finn,“Yellatyourrobot:Improvingon-the-flyfromlanguage
corrections,”inRobotics:ScienceandSystems(RSS),2024.
ACKNOWLEDGMENT [18] H. Diessel and K. R. Coventry, “Demonstratives in spatial language
andsocialinteraction:Aninterdisciplinaryreview,”FrontiersinPsy-
We would like to express our gratitude to Yuhong Deng chology,vol.11,2020.
andHanboZhangfromtheNationalUniversityofSingapore [19] H.Nguyen,A.Jain,C.Anderson,andC.C.Kemp,“Aclickableworld:
Behavior selection through pointing and context for mobile manip-
for their invaluable support in model deployment. We also
ulation,” in 2008 IEEE/RSJ International Conference on Intelligent
thank Tongmiao Xu from Xi’an Jiaotong University for her RobotsandSystems,2008.
efforts in data collection. Furthermore, we sincerely appre- [20] D. Kent, C. Saldanha, and S. Chernova, “A comparison of remote
ciate the volunteers who contributed their time to participate robot teleoperation interfaces for general object manipulation,” in
Proceedings of the 2017 ACM/IEEE International Conference on
in the human-robot interaction experiments. Human-RobotInteraction,2017.
[21] C. Matuszek, L. Bo, L. Zettlemoyer, and D. Fox, “Learning from
REFERENCES
unscripteddeicticgestureandlanguageforhuman-robotinteractions,”
inProceedingsoftheAAAIConferenceonArtificialIntelligence,2014.
[1] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,
[22] D.Whitney,M.Eldon,J.Oberlin,andS.Tellex,“Interpretingmulti-
J.Ibarz,A.Irpan,E.Jang,R.Julianetal.,“Doasican,notasisay:
modalreferringexpressionsinrealtime,”in2016IEEEInternational
Groundinglanguageinroboticaffordances,”inConferenceonRobot
ConferenceonRoboticsandAutomation(ICRA),2016.
Learning,2022.
[23] Y. Chen, Q. Li, D. Kong, Y. L. Kei, S.-C. Zhu, T. Gao, Y. Zhu,
[2] J.Wu,R.Antonova,A.Kan,M.Lepert,A.Zeng,S.Song,J.Bohg,
and S. Huang, “Yourefit: Embodied reference understanding with
S. Rusinkiewicz, and T. Funkhouser, “Tidybot: Personalized robot
languageandgesture,”inProceedingsoftheIEEE/CVFInternational
assistancewithlargelanguagemodels,”AutonomousRobots,vol.47,
ConferenceonComputerVision,2021.
no.8,pp.1087–1102,2023.
[3] S.Yenamandra,A.Ramachandran,K.Yadav,A.S.Wang,M.Khanna, [24] L.-H.Lin,Y.Cui,Y.Hao,F.Xia,andD.Sadigh,“Gesture-informed
T. Gervet, T.-Y. Yang, V. Jain, A. Clegg, J. M. Turner et al., robotassistanceviafoundationmodels,”in7thAnnualConferenceon
“Homerobot: Open-vocabulary mobile manipulation,” in Conference RobotLearning,2023.
onRobotLearning,2023. [25] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner,
[4] P.Liu,Y.Orru,J.Vakil,C.Paxton,N.M.M.Shafiullah,andL.Pinto, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets et al.,
“Demonstrating ok-robot: What really matters in integrating open- “Habitat 2.0: Training home assistants to rearrange their habitat,”
knowledge models for robotics,” in Robotics: Science and Systems AdvancesinNeuralInformationProcessingSystems,2021.
(RSS),2024. [26] C.Li,F.Xia,R.Mart´ın-Mart´ın,M.Lingelbach,S.Srivastava,B.Shen,
[5] S.Tellex,T.Kollar,S.Dickerson,M.Walter,A.Banerjee,S.Teller, K. E. Vainio, C. Gokmen, G. Dharan, T. Jain et al., “igibson 2.0:
and N. Roy, “Understanding natural language commands for robotic Object-centric simulation for robot learning of everyday household
navigation and mobile manipulation,” in Proceedings of the AAAI tasks,”inConferenceonRobotLearning,2021.
ConferenceonArtificialIntelligence,2011. [27] S. Srivastava, C. Li, M. Lingelbach, R. Mart´ın-Mart´ın, F. Xia, K. E.
[6] D.K.Misra,J.Sung,K.Lee,andA.Saxena,“Tellmedave:Context- Vainio, Z. Lian, C. Gokmen, S. Buch, K. Liu et al., “Behavior:
sensitivegroundingofnaturallanguagetomanipulationinstructions,” Benchmark for everyday household activities in virtual, interactive,
TheInternationalJournalofRoboticsResearch,2016. andecologicalenvironments,”inConferenceonRobotLearning,2021.
[7] J.Hatori,Y.Kikuchi,S.Kobayashi,K.Takahashi,Y.Tsuboi,Y.Unno, [28] O.Khatib,“Mobilemanipulation:Theroboticassistant,”Roboticsand
W.Ko,andJ.Tan,“Interactivelypickingreal-worldobjectswithun- AutonomousSystems,1999.
constrainedspokenlanguageinstructions,”in2018IEEEInternational [29] U. Reiser, C. Connette, J. Fischer, J. Kubacki, A. Bubeck, F. Weis-
ConferenceonRoboticsandAutomation(ICRA),2018. shardt, T. Jacobs, C. Parlitz, M. Ha¨gele, and A. Verl, “Care-o-bot®
[8] C.Paxton,Y.Bisk,J.Thomason,A.Byravan,andD.Foxl,“Prospec- 3-creatingaproductvisionforservicerobotapplicationsbyintegrating
tion: Interpretable plans from language by predicting the future,” in design and technology,” in 2009 IEEE/RSJ International Conference
2019 International Conference on Robotics and Automation (ICRA), onIntelligentRobotsandSystems,2009.
2019. [30] M.Ciocarlie,K.Hsiao,A.Leeper,andD.Gossow,“Mobilemanipula-
[9] M. Shridhar, D. Mittal, and D. Hsu, “Ingress: Interactive visual tionthroughanassistivehomerobot,”in2012IEEE/RSJInternational
grounding of referring expressions,” The International Journal of ConferenceonIntelligentRobotsandSystems,2012.
RoboticsResearch,2020. [31] G.Kazhoyan,S.Stelter,F.K.Kenfack,S.Koralewski,andM.Beetz,
[10] H. Zhang, Y. Lu, C. Yu, D. Hsu, X. La, and N. Zheng, “Invigorate: “The robot household marathon experiment,” in 2021 IEEE Interna-
Interactive visual grounding and grasping in clutter,” in Robotics: tionalConferenceonRoboticsandAutomation(ICRA),2021.
ScienceandSystems(RSS),2021. [32] M.Bajracharya,J.Borders,R.Cheng,D.Helmick,L.Kaul,D.Kruse,
[11] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where J. Leichty, J. Ma, C. Matl, F. Michel et al., “Demonstrating mobile
pathwaysforroboticmanipulation,”inConferenceonrobotlearning, manipulation in the wild: A metrics-driven approach,” in Robotics:
2021. ScienceandSystems(RSS),2023.
[12] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“Languagemodels [33] F. De Pace, G. Gorjup, H. Bai, A. Sanna, M. Liarokapis, and
aszero-shotplanners:Extractingactionableknowledgeforembodied M. Billinghurst, “Leveraging enhanced virtual reality methods and
agents,”inInternationalConferenceonMachineLearning,2022. environments for efficient, intuitive, and immersive teleoperation of
[13] W.Huang,F.Xia,T.Xiao,H.Chan,J.Liang,P.Florence,A.Zeng, robots,” in 2021 IEEE International Conference on Robotics and
J. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue: Automation(ICRA). IEEE,2021,pp.12967–12973.
Embodied reasoning through planning with language models,” in [34] K.A.Wyrobek,E.H.Berger,H.M.VanderLoos,andJ.K.Salisbury,
ConferenceonRobotLearning,2022. “Towards a personal robotics development platform: Rationale anddesign of an intrinsically safe personal robot,” in 2008 IEEE Inter-
nationalConferenceonRoboticsandAutomation. IEEE,2008,pp.
2165–2170.
[35] S. Dafarra, U. Pattacini, G. Romualdi, L. Rapetti, R. Grieco,
K.Darvish,G.Milani,E.Valli,I.Sorrentino,P.M.Viceconteetal.,
“icub3 avatar system: Enabling remote fully immersive embodiment
of humanoid robots,” Science Robotics, vol. 9, no. 86, p. eadh3834,
2024.
[36] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, “Open-television:
Teleoperation with immersive active visual feedback,” in Conference
onRobotLearning,2024.
[37] R. Hetrick, N. Amerson, B. Kim, E. Rosen, E. J. de Visser, and
E.Phillips,“Comparingvirtualrealityinterfacesfortheteleoperation
of robots,” in 2020 Systems and Information Engineering Design
Symposium(SIEDS). IEEE,2020,pp.1–7.
[38] M.Wise,M.Ferguson,D.King,E.Diehr,andD.Dymesich,“Fetch
and freight: Standard platforms for service robot applications,” in
Workshoponautonomousmobileservicerobots,2016,pp.1–6.
[39] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, “Robust speech recognition via large-scale weak super-
vision,”inInternationalconferenceonmachinelearning,2023.
[40] M.Minderer,A.Gritsenko,andN.Houlsby,“Scalingopen-vocabulary
objectdetection,”AdvancesinNeuralInformationProcessingSystems,
vol.36,2024.
[41] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., “Segment
anything,”inProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,2023.
[42] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox, “Contact-
graspnet:Efficient6-dofgraspgenerationinclutteredscenes,”in2021
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
2021.
[43] S. Chitta, I. Sucan, and S. Cousins, “Moveit![ros topics],” IEEE
Robotics&AutomationMagazine,2012.
[44] S. Chen, A. Xiao, and D. Hsu, “Llm-state: Expandable state repre-
sentation for long-horizon task planning in the open world,” arXiv
preprintarXiv:2311.17406,2023.
[45] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn,“Learningfine-grained
bimanualmanipulationwithlow-costhardware,”inRobotics:Science
andSystems(RSS),2023.
[46] G. Grisetti, C. Stachniss, and W. Burgard, “Improved techniques for
grid mapping with rao-blackwellized particle filters,” IEEE transac-
tionsonRobotics,2007.
[47] U.S.BureauofLaborStatistics,“Americantimeusesurvey,”2022.
[48] J.Xu,H.Zhang,Q.Si,Y.Li,X.Lan,andT.Kong,“Towardsunified
interactivevisualgroundinginthewild,”in2024IEEEInternational
Conference on Robotics and Automation (ICRA), 2024, pp. 3288–
3295.
[49] S.G.Hart,“Nasa-taskloadindex(nasa-tlx);20yearslater,”Proceed-
ingsoftheHumanFactorsandErgonomicsSocietyAnnualMeeting,
vol.50,2006.
[50] J. Brooke, “SUS - A quick and dirty usability scale,” Usability
evaluationinindustry,vol.189,1996.
[51] D. Ullman and B. F. Malle, “Mdmt: multi-dimensional measure of
trust,”2019.
[52] A. Bangor, P. T. Kortum, and J. T. Miller, “An Empirical Evalu-
ation of the System Usability Scale,” International Journal of Hu-
man–ComputerInteraction,vol.24,Jul.2008.
[53] S.Yu,K.Lin,A.Xiao,J.Duan,andH.Soh,“Octopi:Objectproperty
reasoning with large tactile-language models,” in Robotics: Science
andSystems(RSS),2024.