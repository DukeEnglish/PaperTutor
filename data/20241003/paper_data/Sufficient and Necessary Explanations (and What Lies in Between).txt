Sufficient and Necessary Explanations
(and What Lies in Between)
BeepulBharti1,2 PaulH.Yi3 JeremiasSulam1,2,4
bbharti1@jhu.edu paul.yi@stjude.org jsulam1@jhu.edu
1MathematicalInstituteforDataScience(MINDS),JohnsHopkinsUniversity
2DepartmentofBiomedicalEngineering,JohnsHopkinsUniversity
3St.JudeChildren’sResearchHospital
4DepartmentofComputerScience,JohnsHopkinsUniversity
October1,2024
Abstract
Ascomplexmachinelearningmodelscontinuetofindapplicationsinhigh-stakesdecisionmaking
scenarios, it is crucial that we can explain and understand their predictions. Post-hoc explanation
methods provide useful insights by identifying important features in an input x with respect to the
modeloutputf(x).Inthisworkweformalizeandstudytwoprecisenotionsoffeatureimportancefor
generalmachinelearningmodels: sufficiency andnecessity. Wedemonstratehowthesetwotypesof
explanations,albeitintuitiveandsimple,canfallshortinprovidingacompletepictureofwhichfeatures
a model finds important. To this end, we propose a unified notion of importance that circumvents
theselimitationsbyexploringacontinuumalonganecessity-sufficiencyaxis. Ourunifiednotion,we
show,hasstrongtiestootherpopulardefinitionsoffeatureimportance,likethosebasedonconditional
independence and game-theoretic quantities like Shapley values. Crucially, we demonstrate how a
unifiedperspectiveallowsustodetectimportantfeaturesthatcouldbemissedbyeitheroftheprevious
approachesalone.
1
4202
peS
03
]LM.tats[
1v72402.9042:viXraContents
1 Introduction 3
1.1 SummaryofourContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 SufficiencyandNecessity 4
2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 RelatedWork 5
4 UnifyingSufficiencyandNecessity 6
4.1 SolutionstotheUnifiedProblem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5 TwoPerspectivesoftheUnifiedApproach 7
5.1 AConditionalIndependencePerspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.2 AShapleyValuePerspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
6 Experiments 9
6.1 TabularData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
6.1.1 LinearRegression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
6.1.2 AmericanCommunitySurveyIncome(ACSIncome) . . . . . . . . . . . . . . . . . 9
6.2 ImageClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6.2.1 RSNACTHemorrhage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6.2.2 CelebA-HQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
7 Limitations&BroaderImpacts 13
8 Conclusion 14
A Appendix 19
A.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.1.1 ProofofLemma4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.1.2 ProofofLemma4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.1.3 ProofofTheorem4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.1.4 ProofofCorollary5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.1.5 ProofofTheorem5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A.2 AdditionalExperimentalDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.2.1 RSNACTHemorrhage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.2.2 CelebA-HQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
21 Introduction
Overrecentyears,modernmachinelearning(ML)models,mostlydeeplearningbased,haveachievedim-
pressiveresultsacrossseveralcomplexdomains. Wenowhavemodelsthatcansolvedifficultimageclassi-
fication,inpainting,andsegmentationproblems,performaccuratetextandsentimentanalysis,predictthe
three-dimensionalconformationofproteins,andmore(LeCunetal.,2015;Wangetal.,2023). Despitetheir
success, the rapid integration of these models into society requires caution due to their complexity and
unintelligibility(TheWhiteHouse,2023). ModernMLsystemsare,byandlargeblack-boxes,consistingof
millions of parameters and non-linearities that obscure their prediction-making mechanisms from users,
developers, and auditors. This lack of clarity raises concerns about explainability, transparency, and ac-
countability(Zednik,2021;Tomsettetal.,2018). Thus,understandinghowthesemodelsworkisessential
fortheirsafedeployment.
The lack of explainability has spurred research efforts in eXplainable AI (XAI), with a major focus on
developing post-hoc methods to explain black-box model predictions, especially at a local level. For a
model f and input x ∈ Rd, these methods aim to identify which features in x are important for the
model’s prediction, f(x). They do so by estimating a notion of importance for each feature (or groups)
whichallowsforarankingofimportance.PopularmethodsincludeCAM(Zhouetal.,2016),LIME(Ribeiro
et al., 2016), gradient-based approaches (Selvaraju et al., 2017; Shrikumar et al., 2017; Jiang et al., 2021),
rate-distortiontechniques(Koleketal.,2021,2022),Shapleyvalue-basedexplanations(Chenetal.,2018b;
Teneggietal.,2022;Moscaetal.,2022),perturbation-basedmethods(FongandVedaldi,2017;Fongetal.,
2019; Dabkowski and Gal, 2017), among others (Chen et al., 2018a; Yoon et al., 2018; Jethani et al., 2021;
Wang et al., 2021; Ribeiro et al., 2018). However, many of these approaches lack rigor, as the meaning
of their computed scores is often ambiguous. For example, it’s not always clear what large or negative
gradientssignifyorwhathighShapleyvaluesrevealaboutfeatureimportance. Toaddresstheseconcerns,
otherresearchhasfocusedondevelopingexplanationmethodsbasedonlogic-baseddefinitions(Ignatiev
et al., 2020; Darwiche and Hirth, 2020; Darwiche and Ji, 2022; Shih et al., 2018), conditional hypothesis
testing Teneggi et al. (2023); Tansey et al. (2022), among formal notions. While these methods are a step
towardsrigor,theyhavedrawbacks,includingrelianceoncomplexautomatedreasonersandlimitedability
tocommunicatetheirresultsinanunderstandablewayforhumandecision-makers.
Inthiswork,weadvanceXAIresearchbyformalizingrigorousmathematicaldefinitionsandapproaches,
grounded in the intuitive concepts of sufficiency and necessity, to explain complex ML models. We begin
by illustrating how sufficient and necessary explanations offer valuable, albeit incomplete, insights into
feature importance. To address this issue, we propose and study a more general unified framework for
explaining models. We offer two novel perspectives on our framework through the lens of conditional
independenceandShapleyvalues,andcrucially,showhowitrevealsnewinsightsintofeatureimportance.
1.1 SummaryofourContributions
We study two key notions of importance: sufficiency and necessity, both which evaluate the importance
of aset offeatures inx, withrespect tothe predictionf(x), ofan MLmodel. Asufficient set offeatures
preservesthemodel’soutput,whileanecessaryset,whenremovedorperturbed,renderstheoutputunin-
formative. Although sufficiency and necessity appear complementary, their precise relationship remains
unclear. Whendosufficientandnecessarysubsetsoverlapordiffer? Whenshouldweprioritizeoneover
theother,orseekfeaturesthatarebothnecessaryand sufficient? Toaddressthesequestions,weanalyze
sufficiencyandnecessityandproposeaunificationofboth. Ourcontributionsaresummarizedasfollows:
1. We formalize precise mathematical definitions of sufficient and necessary model explanations for
arbitraryMLpredictors.
2. Weproposeaunifiedapproachthatcombinessufficiencyandnecessity,analyzingtheirrelationships
3and exploring exploring when and how they align or differ. Furthermore, we reveal its strong ties
toconditionalindependenceandShapleyvalues,agame-theoreticmeasureoffeatureimportance.
3. Through experiments of increasing complexity, illustrate how our unified perspective can reveal
new,important,andmorecompleteinsightsintofeatureimportance.
2 Sufficiency and Necessity
Notation. We use boldface uppercase letters to denote random vectors (e.g., X) and lowercase letters
for their values (e.g., x). For a subset S ⊆ [d] := {1,...,d}, we denote its cardinality by |S| and its
complementS = [d]\S. Subscriptsindexfeatures; e.g., thevectorx representstherestrictionofxto
c S
theentriesindexedbyS.
Setting. WeconsiderasupervisedlearningsettingwithanunknowndistributionD overX ×Y, where
X ⊆ Rd isad-dimensionalfeaturespaceandY ⊆ Risthelabelspace. Weassumeaccesstoapredictor
f : X (cid:55)→ Y that was trained on samples from D. For an input x = (x ,...,x ) ∈ Rd, our goal is to
1 d
identify the important features of x for the prediction f(x). To define feature importance precisely, we
usetheaveragerestrictedprediction,
f (x) = E [f(x ,X )] (1)
S S S
XS∼VS
where x is fixed, and X is a random vector drawn from an arbitrary reference distribution V . This
S S S
strategy, popularized in (Lundberg and Lee, 2017; Lundberg et al., 2020), allows us to query a predictor
f thatacceptsonlyd-dimensionalinputsandanalyzeitsbehaviorwhenspecificsetsoffeaturesinxare
retainedorremoved.
2.1 Definitions
WeintroducetwointuitivenotionstoquantifytheimportanceofasubsetS forapredictionf(x). Fora
modelf,webeginbyevaluatingitsbaselinebehavioroveranarbitraryreferencedistributionV: f (x) =
∅
E[f(X )]. Then,forasamplexanditspredictionf(x),wecanposetwosimplequestions:
[d]
Whichsetoffeatures,S,satisfiesf (x) ≈ f(x)orf (x) ≈ f (x)?
S Sc ∅
These questions explores the sufficiency and necessity of subset S, which we define formally in the fol-
lowingdefinitions
Definition 2.1 (Sufficiency). Let ϵ ≥ 0 and let ρ : R × R (cid:55)→ R be a metric on R. A subset S ⊆ [d] is
ϵ-sufficientwithrespecttoadistributionV forf atxif
∆suf(S,f,x) ≜ ρ(f(x),f (x)) ≤ ϵ. (2)
V S
Furthermore,S isϵ-supersufficientifallsupersetsS(cid:101)⊇ S areϵ-sufficient.
This notion of sufficiency is straightforward: a subset S is ϵ-sufficient with respect to a reference distri-
butionV if,withx fixed,theaveragerestrictedpredictionf (x)iswithinϵfromtheoriginalprediction
S S
f(x).Thisisfurtherstrengthenedbysuper-sufficiency:asubsetSisϵ-supersufficientifρ(f(x),f (x)) ≤
S
ϵand, foranysupersetS(cid:101)ofS, ρ(f(x),f (x)) ≤ ϵ. ThissimplymeansincludingmorefeaturesinS still
S(cid:101)
keeps f (x) ϵ close to f(x). To find a small sufficient subset S of small cardinality τ > 0, we can solve
S
thefollowingoptimizationproblem:
argmin ∆suf(S,f,x) subjectto |S| ≤ τ. (P )
V suf
S ⊆ [d]
4We will refer to this problem as the sufficiency problem, or (P ). Using analogous ideas, we also define
suf
necessityandformulateanoptimizationproblemtofindsmallnecessarysubsets.
Definition 2.2 (Necessity). Let ϵ ≥ 0 and denote ρ : R×R (cid:55)→ R to be metric on R. A subset S ⊆ [d] is
ϵ-necessarywithrespecttoadistributionV forf atxif
∆nec(S,f,x) ≜ ρ(f (x),f (x)) ≤ ϵ. (3)
V Sc ∅
Furthermore,S isϵ-supernecessaryifallsupersetsS(cid:101)⊇ S areϵ-necessary.
Here, asubsetS isϵ-necessaryifmarginalizingoutthefeaturesinS withrespecttothedistributionV ,
S
resultsinanaveragerestrictedpredictionf (x)thatisϵclosetof (x)–theaveragebaselineprediction
Sc ∅
of f over V [d]. Furthermore, S is ϵ-super necessary if ρ(f S(x),f(x)) ≤ ϵ and any superset S(cid:101) of S is
ϵ-necessary. To identify a ϵ-necessary subset S of small cardinality τ > 0, one can solve the following
optimizationproblem,whichwerefertoasthenecessity problemor(P ).
nec
argmin ∆nec(S,f,x) subjectto |S| ≤ τ. (P )
V nec
S ⊆ [d]
3 Related Work
Notions of sufficiency, necessity, the duality between the two, and their connections with other feature
attribution methods have been studied to varying degrees in XAI research. We comment on the main
relatedworksinthissection.
Sufficiency.Thenotionofsufficientfeatureshasgainedsignificantattentioninrecentresearch.Shihetal.
(2018)exploreasymbolicapproachtoexplainBayesiannetworkclassifiersandintroduceprimeimplicant
explanations,whichareminimalsubsetsS thatmakefeaturesinthecomplementirrelevanttothepredic-
tionf(x). Formodelsrepresentedbyafinitesetoffirst-orderlogic(FOL)sentences,Ignatievetal.(2020)
refer to prime implicants as abductive explanations (AXp’s). For classifiers defined by propositional for-
mulasandinputswithdiscretefeatures,DarwicheandHirth(2020)refertoprimeimplicantsassufficient
reasonsanddefineacompletereasontobethedisjunctionofallsufficientreasons. Theypresentefficient
algorithms,leveragingBooleancircuits,tocomputesufficientandcompletereasonsanddemonstratetheir
useinidentifyingclassifierdependenceonprotectedfeaturesthatshouldnotinformdecisions. Formore
complex models, Ribeiro et al. (2018) propose high-precision probabilistic explanations called anchors,
which represent local, sufficient conditions. For x positively classified by f, Wang et al. (2021) propose
a greedy approachto solve (P ) while thepreservation method by Fongand Vedaldi (2017) relaxesS to
suf
[0,1]d.
Necessity. There has also been significant focus on identifying necessary features – those that, when
altered, lead to a change in the prediction f(x). For models expressible by FOL sentences, Ignatiev et al.
(2019)defineprimeimplicatesastheminimalsubsetsthatwhenchanged,modifythepredictionf(x)and
relate these to adversarial examples. For Boolean models predicting on samples x with discrete features,
Ignatievetal.(2020)and(DarwicheandHirth,2020)refertoprimeimplicatesascontrastiveexplanations
(CXp’s) and necessary reasons, respectively. Beyond boolean functions, for x positively classified by a
classifier f, Fong et al. (2019) relax S to [0,1]d and propose the deletion method to approximately solve
(P ).
nec
Duality Between Sufficiency and Necessity. Dabkowski and Gal (2017) characterize the preservation
and deletion methods as discovering the smallest sufficient and destroying region (SSR and SDR). They
proposecombiningthetwobutdonotexplorehowsolutionstothisapproachmaydifferfromindividual
SSR and SDR solutions. Ignatiev et al. (2020) show that AXp’s and CXp’s are minimal hitting sets of
another by using a hitting set duality result between minimal unsatisfiable and correction subsets. The
resultenablestheidentificationofAXp’sfromCXp’sandviceversa.
5Sufficiency,Necessity,andGeneralFeatureAttributionMethods. Preciseconnectionsbetweensuf-
ficiency,necessity,andotherpopularfeatureattributionmethods(suchasShapleyvalues(Shapley,1951;
Chenetal.,2018b;LundbergandLee,2017))remainsunclear. Toourknowledge,Covertetal.(2021)pro-
videtheonlyworkexaminingtheseapproaches(FongandVedaldi,2017;Fongetal.,2019;Dabkowskiand
Gal, 2017) in the context of general removal-based methods, i.e., methods that remove certain input fea-
turestoevaluatedifferentnotionsofimportance. TheworkofWatsonetal.(2021)isalsorelevanttoour
work, as it formalizes a connection between notions of sufficiency and Shapley values. With the specific
payoff function 1 defined as v(S) = E[f(x ,X )], they show how each summand in the Shapley value
S Sc
measuresthesufficiencyoffeatureitoaparticularsubset.
4 Unifying Sufficiency and Necessity
Givenamodelf,samplex,andreferencedistributionV,wecanidentifyasmallsetofimportantfeatures
S by solving either (P ) or (P ) 2. While both methods are popular (Kolek et al., 2021, 2022; Fong
suf nec
andVedaldi,2017;Bhallaetal.,2023;Yoonetal.,2018), simplyidentifyingasmallsufficientornecessary
subsetmaynotprovideacompletepictureofhowf usesxtomakeaprediction. Toseewhy,considerthe
followingscenario: forafixedτ > 0,letS∗ beaϵ-sufficientsolutionto(P ),sothat|S∗| ≤ τ and
suf
∆suf(S,f,x) ≤ ϵ. (4)
V
WhileS∗ isϵ-sufficient,itcanalsobetruethat
∆nec(S,f,x) > ϵ (5)
V
indicatingS∗ isnotϵ-necessary: indeed, thiscansimplyhappenwhenitscomplement, S∗, containsim-
c
portantfeatures. Thisscenarioraisestwoquestions:
1. Howdifferentaresufficientandnecessaryfeatures?
2. Howdoesvaryingthelevelsofsufficiencyandnecessityaffecttheoptimalsetofimportantfeatures?
In order to provide answers to these questions (and to avoid the scenario above) we propose to search
for a small set S that is both sufficient and necessary by combining problems (P ) and (P ). Consider
suf nec
∆uni(S,f,x,α),aconvexcombinationofboth∆suf(S,f,x)and∆nec(S,f,x)
V V V
∆uni(S,f,x,α) = α·∆suf(S,f,x)+(1−α)·∆nec(S,f,x) (6)
V V V
where α ∈ [0,1] controls the extent to which S is required to be sufficient vs. necessary. Our unified
problem,(P ),canbeexpressedas:
uni
argmin ∆uni(S,f,x,α) subjectto |S| ≤ τ. (P )
V uni
S ⊆ [d]
When α is 1 or 0, ∆uni(S,f,x,α) reduces to ∆suf(S,f,x) or ∆nec(S,f,x), respectively. In these ex-
V V V
tremecases,S isonlysufficientornecessary. Intheremainderofthisworkwewilltheoreticallyanalyze
(P ),characterizeitssolutions,andprovidedifferentinterpretationsofwhatpropertiesthesolutionshave
uni
throughthelensofconditionalindependenceandgametheory. Intheexperimentalsection,wewillshow
thatsolutionsto(P )provideinsightsthatneither(P )nor(P )offer.
uni suf nec
1Payofffunctionsareaninstrumentaltoolingame-theoreticapproaches.SeefurtherSection5.2forfurtherdetails.
2Solving(P )or(P )isNP-hardforgeneralnon-convexfunctionsf.Wedonotconcernourselveswiththecomputational
suf nec
efficiencyoftheseproblemsasthereexisttractablerelaxations(Koleketal.,2021,2022).
64.1 SolutionstotheUnifiedProblem
Webeginwithasimplelemmathatdemonstrateswhy(P )enforcesbothsufficiencyandnecessity.
uni
Lemma4.1. Letα ∈ (0,1). Forτ > 0, denoteS∗ tobeasolutionto (P )forwhich∆uni(S,f,x,α) = ϵ.
uni V
Then,S∗ is ϵ-sufficientand ϵ -necessary. Formally,
α 1−α
ϵ ϵ
0 ≤ ∆suf(S∗,f,x) ≤ and 0 ≤ ∆nec(S∗,f,x) ≤ . (7)
V α V 1−α
The proof of this result, and all others, is included Appendix A.1. This result illustrates that solutions to
(P )satisfyvaryingdefinitionsofsufficiencyandnecessity. Furthermore,asαincreasesfrom0to1,the
uni
solutionshiftsfrombeinghighlynecessarytohighlysufficient.Inthefollowingresults,wewillshowwhen
andhow solutionsto(P )aresimilar(anddifferent)tothoseof (P )and(P ). Tostart,wepresentthe
uni suf nec
followinglemma,whichwillbeusefulinsubsequentresults.
Lemma4.2. For0 ≤ ϵ < ρ(f(x),f ∅(x)) , denoteS∗ andS∗ tobeϵ-sufficientandϵ-necessarysets. Then, if
2 suf nec
S∗ isϵ-supersufficientorS∗ isϵ-supernecessary,wehave
suf nec
S∗ ∩S∗ ̸= ∅. (8)
suf nec
This lemma demonstrates that, given ϵ-sufficient and necessary sets S∗ and S∗ , if either additionally
suf nec
satisfiesthestrongernotionsofsupersufficiencyornecessity,theymustsharesomefeatures. Thisproves
usefulincharacterizingasolutionto(P ),whichwenowdointhefollowingtheorem.
uni
Theorem4.1. Letτ ,τ > 0and0 ≤ ϵ < 1 ·ρ(f(x),f (x)). DenoteS∗ andS∗ tobeϵ-supersufficient
1 2 2 ∅ suf nec
andϵ-supernecessarysolutionsto(P )and (P ),respectively,suchthat|S∗ | = τ and|S∗ | = τ . Then,
suf nec suf 1 nec 2
thereexistsasolutionS∗ to(P )suchthat
uni
∆uni(S∗,f,x,α) ≤ ϵ and max(τ ,τ ) ≤ |S∗| < τ +τ . (9)
V 1 2 1 2
Furthermore,ifS∗ ⊆ S∗ orS∗ ⊆ S∗ . thenS∗ = S∗ orS∗ = S∗ ,respectively.
suf nec nec suf nec suf
This result demonstrates that solutions to (P ), (P ), and (P ) can be closely related. As an example,
uni suf nec
considerfeaturesthatareϵ-supersufficient,S∗ . IfwehavedomainknowledgethatS∗ ⊆ S∗ ,andS∗
suf suf nec nec
is ϵ-super necessary, then S∗ is in fact the solution to the (P ) problem. Conversely, if we know that
nec uni
S∗ isϵ-supernecessaryalongwithbeingasubsetofϵ-supersufficientsetS∗ ,thenS∗ willbeasolution
suf suf suf
tothe(P )problem.
uni
5 Two Perspectives of the Unified Approach
In the previous section, we characterized solutions to (P ) and their connections to those of (P ) and
uni suf
(P ). To better understand sufficiency, necessity, and their unification, we will provide two alternative
nec
perspectivesofourunifiedframeworkthroughthelensofconditionalindependenceandShapleyvalues.
5.1 AConditionalIndependencePerspective
Herewedemonstratehowoursufficiency,necessity,andourunifiedapproach,canbeunderstoodasmea-
suringconditionalindependencerelationsbetweenfeaturesXandlabelsY.
7Corollary 5.1. Suppose for any S ⊆ [d], V = p(X | X = x ). Let α ∈ (0,1), ϵ ≥ 0, and denote
S S Sc Sc
ρ : R×R (cid:55)→ RtobeametriconR. Furthermore,forf(X) = E[Y | X]andτ > 0,letS∗ beasolutionto
(P )suchthat∆uni(S,f,x,α) = ϵ. Then,S∗ satisfiesthefollowingconditionalindependencerelations,
uni V
ρ(E[Y | x], E[Y | X S∗ = x S∗]) ≤ ϵ and ρ(cid:0)E[Y | X S∗ = x S∗], E[Y](cid:1) ≤ ϵ . (10)
α c c 1−α
The assumption in this corollary is that, ∀ S ⊆ [d], f (x) is evaluated using the conditional distribution
S
p(X | X = x )asthereferencedistributionV . Giventherecentadvancementsingenerativemodels
Sc S S S
(Song and Ermon, 2019; Ho et al., 2020; Song et al., 2021), this assumption is (approximately) reasonable
in many practical settings, as we will demonstrate in our experiments. With this reference distribution,
the results shows that for the model f(X) = E[Y | X] and a sample x, the minimizer S∗ of (P )
uni
approximately satisfies two conditional independence properties. First, S∗ is sufficient in that, when the
featuresinS∗ arefixed,thecomplement,S ∗,offerslittle-to-noadditionalinformationaboutY. Second,
c
S∗ isnecessarybecausewhenwemarginalizeitoutandrelyonlyonthefeaturesinS ∗,theinformation
c
gainedaboutY isminimalandsimilartoE[Y = 1].
5.2 AShapleyValuePerspective
Intheprevioussection,wedetailedtheconditionalindependencerelationsonegainsfromsolving(P ).
uni
Wenowpresentanarguablylessintuitiveresultthatshowsthatsolving(P )isequivalenttomaximizing
uni
the lower bound of Shapley value. Before presenting our result, we provide a brief background on this
game-theoreticquantity.
ShapleyValues. Shapleyvaluesusegametheorytomeasuretheimportanceofplayersinagame. Letthe
tuple ([n],v) represent a cooperative game with players [n] = {1,2,...,n} and denote a characteristic
function v(S) : P([n]) → R, which maps the power set of [n] to the reals. Then, the Shapley value
(Shapley,1951)forplayerj inthecooperativegame([n],v)is
ϕshap([n],v) = (cid:88) w ·[v(S ∪{j})−v(S)] (11)
j S
S⊆[n]\{j}
|S|!(n−|S|−1)!
where w = . The Shapley value is the only solution concept that satisfies the desirable
S n!
axioms of additivity, nullity, symmetry, and linearity (Owen, 2013). In the context of XAI and feature
importance,Shapleyvaluesarewidelyusedtomeasurelocalfeatureimportancebytreatinginputfeatures
asplayersinagame(Covertetal.,2020;Teneggietal.,2022;Chenetal.,2018b;LundbergandLee,2017).
Given a sample x ∈ Rd and a model f, the goal is to evaluate the importance of each feature j ∈ [d] for
the prediction f(x). This is done by defining a cooperative game ([d],v), where v(S) is a characteristic
functionthatquantifieshowthefeaturesinScontributetotheprediction.Differentchoicesofv(S)canbe
foundin(LundbergandLee,2017;SundararajanandNajmi,2020;Watsonetal.,2024).Althoughcomputing
ϕshap([d],v)iscomputationallyintractable,severalpracticalmethodsforestimationhavebeendeveloped
j
(Chenetal.,2023;Teneggietal.,2022;Zhangetal.,2023;Lundbergetal.,2020). WhileShapleyvaluesare
popularacrossvariousdomains(Moncada-Torresetal.,2021;Zoabietal.,2021;Liuetal.,2021),fewworks,
asidefromWatsonetal.(2021),exploretheirconnectionstosufficiencyandnecessity.
Withthisbackground,wenowpresentourresult. Recallsolving(P )obtainsasmallsubsetS withlow
uni
∆uni(S,f,x,α). Noticethatin(P )thereisanaturalpartitioning ofthefeaturesintotwosets,S andS .
V uni c
In the follow theorem we demonstrate that searching for a small subset S with minimal ∆uni(S,f,x,α)
V
isequivalenttomaximizingalowerboundontheShapleyvalueinatwoplayergame.
Theorem 5.1. Consider an input x for which f(x) ̸= f (x). Denote by Λ = {S,S } the partition of
∅ d c
[d] = {1,2,...,d},anddefinethecharacteristicfunctiontobev(S) = −ρ(f(x),f (x)). Then,
S
ϕshap(Λ ,v) ≥ ρ(f(x),f (x))−∆uni(S,f,x,α). (12)
S d ∅ V
8This result has important implications. When the feature space is partitioned into 2 disjoint sets, S and
S , where each is a player in a cooperative game, then in searching for an S with small ∆ (S,V, 1),
c uni 2
as we do in (P ), we are searching for a player, S, with a large lower bound on its Shapley value. Note
uni
this connection we show is different from the one presented by (Watson et al., 2021). They show the
Shapley value of feature i is a measure of this feature i’s sufficiency subsets S ⊆ [d]. In conclusion, our
resultprovidesanewanddifferentandcomplementaryinterpretationtothesufficiency,necessity,andour
proposedunifiedmethodthroughthelensofgametheory.
6 Experiments
Wedemonstrateourtheoreticalfindingsinmultiplesettingsofincreasinglycomplexity: twotabulardata
tasks (on synthetic data and the US adult income dataset (Ding et al., 2021)) and two high-dimensional
imageclassificationtasksusingtheRSNA2019BrainCTHemorrhageChallenge(Flandersetal.,2020)and
CelebA-HQdatasets(Leeetal.,2020).
6.1 TabularData
Inthefollowingexamples,weanalyzesolutionsto(P )forvaryinglevelsofsufficiencyvs. necessityand
uni
multiple size constraints. We learn a predictor f and, for 100 new samples, solve (P ) for τ ∈ {3,6,9}
uni
andα ∈ [0,1],withρ(a,b) = |a−b|andV = p(X | X = x ). Forafixedτ andsamplex,wedenote
S S Sc Sc
S∗ to be a solution to (P ) for α . To analyze the stability of S∗ as sufficiency and necessity vary, we
αi uni i αi
reportthenormalizedaverageHammingdistance(Hamming,1950)betweenS∗ andS∗,alongwith95%
αi 0
confidenceintervals,asafunctionofα.
6.1.1 LinearRegression
We begin with a regression example. Features X are distributed according to N(µ,AAT) with µ =
(cid:2) 2i(cid:3)d andA ∼ U(0,1). TheresponseisY = βTX+ϵ,withβ = 32·[2−i]d andϵ ∼ N(0,I ).
i=1 i,j i=1 d×d
Withd = 10ourmodelisf(X) = βˆT X,whereβˆ istheleastsquaressolution.
Stability of Unified Solutions. Fig. 1a shows that when solutions are constrained to be small (τ = 3),
increasing α to enforce greater sufficiency results in a steady increase in Hamming distance, indicating
thatthesolutionsS∗ areconsistentlychanging. Whenlargersolutionsareallowed(τ = 6), S∗ rapidly
αi αi
changeswiththeintroductionofsufficiency,asseenbytheinitialsteepriseinHammingdistance. How-
ever, as α continues to increase, this distance grows more gradually. Lastly, when the solution size ap-
proachesthedimensionalityofthefeaturespace(τ = 9),smalltointermediatelevelsofsufficiencydonot
significantlyalterthesolutions. However, requiringhighlevelsofsufficiency(α > 0.8)leadstoextreme
changesinthesolutions,asshownbyasharpincreaseinHammingdistance.
6.1.2 AmericanCommunitySurveyIncome(ACSIncome)
WeusetheACSIncomedatasetforCalifornia,including10demographicandsocioeconomicfeaturessuch
as age, education, occupation, and geographic region. We train a Random Forest classifier to predict
whetheranindividual’sannualincomeexceeds$50K,achievingatestaccuracy≈ 81%.
Stability of Unified Solutions. Fig. 1b shows that when solutions are forced to be small (τ = 3), in-
creasingαtoenforcesufficiencyresultsinasteadyincreaseinHammingdistance,indicatingthesolutions
S∗ arechanging. Forlargersolutions(τ = 6), S∗ changessignificantlywhenlowlevelssufficiencyare
αi αi
9Stability of S* Stability of S*
0.40
= 3 = 3
0.5 = 6 0.35 = 6
= 9 = 9
0.4 0.30
0.25
0.3
0.20
0.2 0.15
0.10
0.1
0.05
0.0 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(a) Regression (b) ACSIncome
Figure1: Stabilityofsolutionsto(P )vs. αforτ ∈ {3,6,9}
uni
required,indicatedbyinitialriseintheHammingdistance. Asαcontinuestoincrease,theHammingdis-
tancegrowsmoregradually. Interestingly,whenthesizeisclosetofeaturespace’sdimensionality(τ = 9),
theHammingdistanceexhibitsabehaviorsimilartothatobservedforτ = 3. Inconclusion,bothexperi-
mentsshowthattheoptimalfeaturesetcanvarysignificantlydependingonthesizeallowedandbalance
betweensufficiencyandnecessity.
6.2 ImageClassification
The following two experiments explore high dimensional settings in image classification tasks. The fea-
turesarepixelvaluesandsoasubsetS correspondstoabinarymaskidentifyingimportantpixels. Since
solving(P ),(P ),or(P )isNP-hard,weusetwomethods–oneforeachsetting,describedintheirre-
suf nec uni
spectivesections–whichsolverelaxedproblemstoidentifysufficientandnecessarymasksS.Theseexper-
imentsservetwopurposes. First,theywillanalyzetheextenttowhichexplanationsgeneratedbypopular
methods–including Integrated Gradients (Sundararajan et al., 2017), GradientSHAP (Lundberg and Lee,
2017),GuidedGradCAM(Selvarajuetal.,2017),andh-Shap(Teneggietal.,2022)–identifysmallsufficient
andnecessarysubsets. Toensureconsistentanalysis,wenormalizeallgeneratedattributionscorestothe
interval[0,1]. Thisisdonebysettingthetop1%ofnonzeroscoresto1anddividingtheremainingscores
by the minimum score from the top 1% of nonzero scores. Then, binary masks are generated by thresh-
oldingthenormalizedscoresusingt ∈ [0,1]. Foratestsetofimages,weperformthisnormalizationand
reporttheaverage−log(∆suf),−log(∆nec),and−log(L0)(acrossallbinarymasks)atdifferentthreshold
valuestoanalyzethesufficiency,necessityandsizeoftheexplanations. Finally,thesecondobjectiveisto
understandandvisualizethesimilaritiesanddifferencesbetweensufficientandnecessarysets.
6.2.1 RSNACTHemorrhage
WeusetheRSNA2019BrainCTHemorrhageChallengedatasetcomprisedof752,803scans. Eachscanis
annotated by expert neuroradiologists with the presence and type(s) of hemorrhage (i.e., epidural, intra-
parenchymal, intraventricular, subarachnoid, or subdural). We use a ResNet18 (He et al., 2016) classifier
that was pretrained on this data (Teneggi et al., 2022). To identify sufficient and necessary sets we solve
therelaxedproblem,
argmin ∆uni(S,f,x,α)+λ ·||S|| +λ ·||S|| . (13)
V 1 1 TV TV
S⊆[0,1]d
10
)*S,*S(maH
0
)*S,*S(maH
0Original Image S s* uf S n* ec S u* ni
6
5
4
3
2
1
0
0.0 0.2 0.4 0.6 0.8 1.0
4.0
3.5
3.0 Integrated Gradients
2.5 Gradient SHAP
Guided GradCAM
2.0 h-Shap
Sufficiency Explainer
1.5 Necessary Explainer
1.0 Unified Explainer
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0
7
6
5
4
3
2
1
0
0.0 0.2 0.4 0.6 0.8 1.0
Thresholds
(a) Comparisonofdifferentmethods. (b) S∗ , S∗ andS∗ forvariousCTscans.
suf nec uni
Figure2: ExperimentalresultsontheRSNAdataset.
Sincethedatasetconsistsofhighlycomplexanddiverseimages,weemploythisper-exampleapproachthat
generateshighlyspecifictailoredsolutionsbysolvinganoptimizationproblemforeachsamplefollowing
previouswork(Fongetal.,2019;Koleketal.,2021,2022).3 Tolearnsufficientand/ornecessarymasks,we
solveEq.(13)forα ∈ {0,0.5,1}. DetailsareinAppendixA.2.
ComparisonofPost-hocInterpretabilityMethods. Forasetof20imagespositivelyclassifiedbythe
ResNet model, we apply multiple post-hoc interpretability methods, as well as computing sufficient and
necessarymasksbyourproposedapproach–solving(13). TheresultsinFig.2ashowthatforathreshold
oft < 0.1,manymethodsidentifysufficientsetssmallerinsizethanthesufficientandunifiedexplainer,as
indicatedbytheirlargevaluesof−log(∆suf)andsmallervaluesof−log(L0). However,fort > 0.1,only
the sufficient and unified explainer identify sufficient sets of a constant small size. Importantly, it is also
evident that no methods, besides our necessity and unified explainers, identify necessary sets. Furthermore,
asexpected,thesufficientexplainerdoesnotidentifynecessarysetsandviceversa. Theunifiedexplainer,
as expected, identifies a sufficient and necessary set, albeit at the cost of the set being larger in size. In
conclusion, while many methods can identify sufficient, no off-the-shelf method can identify necessary
setsforsmallthresholds. Onlywhenwedirectlyoptimizeforsuchpropertiesdowegetexplanationsthat
areconsistentlysmall,sufficientand/ornecessaryacrossthresholds.
Sufficiency vs. Necessity. In Fig. 2b we visualize the sufficient and necessary features in various CT
scans. Thefirstobservationisthatsufficientsubsetsdonotprovideacompletepictureofwhichfeatures
areimportant. NoticeforalltheCTscans,asufficientset,S∗ highlightsoneortwo,butneverall,brain
suf
hemorrhagesinthescans.Forexample,inthelastrow,S∗ onlycontainstheleftfrontallobeparenchymal
suf
hemorrhages,whichhappenstobeoneofthelargerhemorrhagespresent. Ontheotherhand,necessary
sets,S∗ ,containpartsof,sometimesentirely,allhemorrhagesinthescans. Inthelastrow,S∗ contains
nec nec
all multifocal parenchymal hemorrhages in both right and left frontal lobes, because when all these re-
3λ , ||S|| andλ , ||S|| aretheℓ andTotalVariationnormsandhyperparamters,promotingsparsityandsmoothness.
1 1 TV TV 1
11
)fus
(gol
)cen
(gol
)0L(gol10 3.5 7
3.0 6
8
2.5 Integrated Gradients 5 Gradient SHAP
6 2.0 Guided GradCAM 4 h-Shap
4
1.5 S Nu ef cf eic si se an rc yy EE xx pp lala inin ee rr 3
1.0 2
2 0.5 1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Thresholds Thresholds Thresholds
Figure3: ComparisonofdifferentmethodsontheCelebAHQdataset.
gionsaremasked,themodelyieldsaprediction≈ 0.64–thepredictionofthemodelonthemeanimage.
Finally,noticeinthe2ndand3rdcolumnsthatS∗ andS∗ arenearlyidentical,whichpreciselydemon-
nec uni
strateLemma 4.1 and Theorem 4.1 in practice. First, since S∗ is super sufficient, S∗ and S∗ , share
suf suf nec
common features. Second, visually S∗ ⊆ S∗ holds approximately and so S∗ = S∗ . Through this
suf nec nec uni
experimentweareabletohighlightthedifferencesbetweensufficientandnecessarysets,showhoweach
contain important and complementary information, and demonstrate our theory holding in real world
settings.
6.2.2 CelebA-HQ
We use a modified version of the CelebA-HQ dataset (Karras, 2017) that contains 30,000 celebrity faces
resizedto256×256pixels. WetrainaResNet18toclassifywhetheracelebrityissmiling,achievingatest
accuracy≈ 94%. TogeneratesufficientornecessarymasksSforsamplesx,welearnmodelsg : X (cid:55)→ X,
θ
that(approximately)solvethefollowingoptimizationproblem:
argmin E (cid:2) ∆uni(g (X),f,X,α)+λ ·||g (X)|| +λ ·||g (X)|| (cid:3) . (14)
V θ 1 θ 1 TV θ TV
θ∈Θ X∼DX
Given the structured nature of the datasetand the similarity offeatures across images, we usethis para-
metric model approach because it prevents overfitting to spurious signals, an issue that can arise with
per-examplemethods. Additionally,thisapproachismoreefficient,asitstillgeneratestailoredper-sample
explanationsbutonlyrequireslearningasinglemodelratherthanrepeatedlysolvingEq.(13)(Linderetal.,
2022; Chen et al., 2018a; Yoon et al., 2018). To learn a necessary and sufficient explainer model, we solve
Eq.(14)viaempiricalriskminimizationforα ∈ {0,1}respectively. Implementationdetailsandhyperpa-
rametersettingsareincludedinAppendixA.2.
Comparison of Post-hoc Interpretability Methods. For a set of 100 images labeled with a smile and
correctly classified by the ResNet model, we apply multiple post-hoc interpretability methods and our
sufficient and necessary explainers to identify important features associated with smiling. The results in
Fig.3illustratethatforawiderangeofthresholdst ∈ [0,1],manymethodsidentifysufficientsubsets,as
−log(∆suf) for many of them is comparable to that of the sufficient explainer. The necessary explainer,
infact,identifiessubsetsthataremoresufficientthanthosefoundbythesufficientexplainer. Thereason
is that the sufficient explainer identifies subsets that are, on average, smaller for all t ∈ [0,1], while the
necessaryexplainerfinds subsetsthatareconstant insizeforallt ∈ [0,1]butslightlylarger since, tobe
necessary,theymustcontainmorefeaturesthatprovideadditionalinformationaboutthelabel. Forother
methods, as t increases, subset size decreases, and the sufficiency and necessity of the solutions decline.
Meanwhile,thenecessaryexplainernaturallyidentifiesnecessarysubsets,indicatedbylarge−log(∆nec),
whereas other methods fail to do so. In conclusion, many methods can identify sufficient sets, but not
necessary ones. Directly optimizing for these criterion leads to identifying small, constant-sized subsets
acrossthresholds.
12
)fus
(gol
)cen
(gol
)0L(golOriginal Image Keeping Ss*uf Sample 1 Sample 2 Sample 3 Sample 4
Model Prediction = 1 Model Prediction = 1.0 Model Prediction = 1.0 Model Prediction = 1.0 Model Prediction = 1.0
Original Image Masking Ss*uf Sample 1 Sample 2 Sample 3 Sample 4
Model Prediction = 1.0 Model Prediction = 1.0 Model Prediction = 1.0 Model Prediction = 1.0 Model Prediction = 1.0
Figure4: ImagesandmodelpredictionsbyfixingandmaskingthesufficientsubsetS∗
suf
Original Image Keeping Sn*ec Sample 1 Sample 2 Sample 3 Sample 4
Model Prediction = 0.99 Model Prediction = 0.95 Model Prediction = 0.98 Model Prediction = 1.0 Model Prediction = 1.0
Original Image Masking Sn*ec Sample 1 Sample 2 Sample 3 Sample 4
Model Prediction = 0.99 Model Prediction = 0.2 Model Prediction = 0.35 Model Prediction = 1.0 Model Prediction = 1.0
Figure5: ImagesandmodelpredictionsbyfixingandmaskingthenecessarysubsetS∗
nec
Sufficiencyvs. Necessity. InFig.4,weseehowsufficientsubsetsalonemayoverlookimportantfeatures,
whilesolutionsto(P )offerdeeperinsights. Asstatedearlier,thesufficientexplaineridentifiessetsthat
uni
are sufficient but not necessary. On the other hand, the necessary explainer has high −log(∆suf) and
−log(∆nec),indicatingthatitidentifiessufficientand necessaryset,meaningtheyalsoserveassolutions
to(P ). InFig.4,wevisualizethereasonsforthisphenomena. NoticethatS∗ preciselyhighlights(only)
uni suf
the smile. When S∗ is fixed, one can generate new images (as done in (Zhang et al., 2023)) for which
suf
the model produces the same predictions as it did for the original image (a smile). On the other hand,
we also see why S∗ is not necessary: we can fix the complement (S∗ ) and, since there are important
suf suf c
features in it, a smile is consistently generated, and the model produces the same prediction on these
images as it did on the original. Conversely solutions to (P ) (also solutions to (P ) here) generate
nec uni
different explanations that provide a more complete picture of feature importance. Notice that S∗ is
nec
sufficientbecauseS∗ ⊆ S∗ ,withtheadditionalfeaturesmainlybeingthedimplesandeyes,whichaid
suf nec
in determining the presence of a smile. More importantly, Fig. 5 illustrates why S∗ is necessary: when
nec
wefixthecomplementofS∗ andgeneratenewsamples,halfofthefaceslackasmile,leadingthemodel
nec
f topredictnosmile. DetailsonsamplegenerationareinAppendixA.1.
7 Limitations & Broader Impacts
While this work provides a novel theoretical contribution to the XAI community, there are some limita-
tionsthatrequirecarefuldiscussion.ThechoiceofreferencedistributionV determinesthecharacteristics
S
13ofsufficientandnecessaryexplanations. Forinstance,onlywiththetrueconditionaldatadistributioncan
one obtain the conditional independence results that our theory provides. Naturally, there are computa-
tionaltrade-offsthatmustbecarefullystudied; theabilitytolearnandsamplefromaccurateconditional
distributionstogenerateexplanationswithclearstatisticalmeaningcomeswithacomputationalandsta-
tisticalcost,particularlyinhigh-dimensionalsettings. Thus,akeydirectionforfutureworkistoexplore
theimpactofdifferentreferencedistributionsandprovideaprincipledframeworkforselectingaV that
S
balancespracticalutilityandcomputationalfeasibility.
Anotherrelevantquestionishowwellourproposednotionsalignwithhumanintuition. Whileweaimto
understandwhichfeaturesaresufficientandnecessaryforagivenpredictedmodel,theseexplanationsmay
not always correspond to how humans perceive importance (since model might use different features to
solveatask). Thiscanbeanissueinsettingswhereinterpretabilityisessentialfortrustandaccountability,
suchasinhealthcare.Ontheonehand,ourapproachcanprovideusefulinsightstofurtherevaluatemodels
(e.g. by verifying if the sufficient and necessary features employed by models correlate with the correct
ones as informed by human experts). On the other hand, bridging the gap between our mathematical
definitions of sufficiency and necessity and other human notions of importance is an area for further
investigation. Userstudies, alongwithcollaborationwithdomainexperts, willbecriticalindetermining
howourformalnotionsofsufficiencyandnecessitycanbeadaptedorextendedtobettermeetreal-world
interpretabilityneeds.
Finally, the societal impact of this work warrants discussion. While we offer a rigorous framework to
understand model predictions, these are oblivious to notions of demographic bias. There is a risk that
an “incorrect" choice of generating a sufficient vs. necessary explanation could reinforce biases or ob-
scurethecausalreasonsbehindpredictions. Futureworkwillstudywhenandhowourframeworkcanbe
incorporatethesebiasesinthereportedimportantfeatures.
8 Conclusion
This work formalizes notions of sufficiency and necessity as tools to evaluate feature importance and
explain model predictions. We demonstrate that sufficient and necessary explanations, while insightful,
oftenprovideincompletewhilecomplementaryanswerstomodelbehavior. Toaddressthislimitation,we
propose a unified approach that offers a new and more nuanced understanding of model behavior. Our
unifiedapproachexpandsthescopeofexplanationsandrevealstrade-offsbetweensufficiencyandneces-
sity, giving rise to new interpretations of feature importance. Through our theoretical contributions, we
presentconditionsunderwhichsufficiencyandnecessityalignordiverge,andprovidetwoperspectivesof
ourunifiedapproachthroughthelensofconditionalindependenceandShapleyvalues. Ourexperimental
resultssupportourtheoreticalfindings,providingexamplesofhowadjustingsufficiency-necessitytrade-
off via our unified approach can uncover alternative sets of important features that would be missed by
focusing solely on sufficiency or necessity. Furthermore, we evaluate common post-hoc interpretability
methodsshowingthatmanyfailtoreliablyidentifyfeaturesthatarenecessaryorsufficient. Insummary,
our work contributes to a more complete understanding of feature importance through sufficiency and
necessity. Webelieve,andhope,ourframeworkholdspotentialforadvancingtherigorousinterpretability
ofMLmodels.
References
UshaBhalla, SurajSrinivas, andHimabinduLakkaraju. Verifiablefeatureattributions: Abridgebetween
posthocexplainabilityandinherentinterpretability. Advancesinneuralinformationprocessingsystems,
2023.
14HughChen,IanCCovert,ScottMLundberg,andSu-InLee. Algorithmstoestimateshapleyvaluefeature
attributions. NatureMachineIntelligence,pages1–12,2023.
Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-
theoretic perspective on model interpretation. In International conference on machine learning, pages
883–892.PMLR,2018a.
JianboChen,LeSong,MartinJWainwright,andMichaelIJordan.L-shapleyandc-shapley:Efficientmodel
interpretationforstructureddata. arXivpreprintarXiv:1808.02610,2018b.
Ian Covert, Scott M Lundberg, and Su-In Lee. Understanding global feature contributions with additive
importancemeasures. AdvancesinNeuralInformationProcessingSystems,33:17212–17223,2020.
Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model
explanation. JournalofMachineLearningResearch,22(209):1–90,2021.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. Advances in neural
informationprocessingsystems,30,2017.
AdnanDarwicheandAugusteHirth. Onthereasonsbehinddecisions. InECAI2020,pages712–720.IOS
Press,2020.
AdnanDarwicheandChunxiJi. Onthecomputationofnecessaryandsufficientexplanations. InProceed-
ingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages5582–5591,2022.
FrancesDing,MoritzHardt,JohnMiller,andLudwigSchmidt.Retiringadult:Newdatasetsforfairmachine
learning. Advancesinneuralinformationprocessingsystems,34:6478–6490,2021.
AdamEFlanders,LucianoMPrevedello,GeorgeShih,SafwanSHalabi,JayashreeKalpathy-Cramer,Robyn
Ball, John T Mongan, Anouk Stein, Felipe C Kitamura, Matthew P Lungren, et al. Construction of a
machinelearningdatasetthroughcollaboration:thersna2019braincthemorrhagechallenge.Radiology:
ArtificialIntelligence,2(3):e190211,2020.
RuthFong,MandelaPatrick,andAndreaVedaldi.Understandingdeepnetworksviaextremalperturbations
and smooth masks. In Proceedings of the IEEE/CVF international conference on computer vision, pages
2950–2958,2019.
RuthCFongandAndreaVedaldi. Interpretableexplanationsofblackboxesbymeaningfulperturbation.
InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages3429–3437,2017.
RichardWHamming. Errordetectinganderrorcorrectingcodes. TheBellsystemtechnicaljournal,29(2):
147–160,1950.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
informationprocessingsystems,33:6840–6851,2020.
AlexeyIgnatiev, NinaNarodytska, andJoaoMarques-Silva. Onrelatingexplanationsandadversarialex-
amples. Advancesinneuralinformationprocessingsystems,32,2019.
AlexeyIgnatiev,NinaNarodytska,NicholasAsher,andJoaoMarques-Silva. Fromcontrastivetoabductive
explanations and back again. In International Conference of the Italian Association for Artificial Intelli-
gence,pages335–355.Springer,2020.
15Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have we learned
to explain?: How interpretability methods can learn to encode predictions in their interpretations. In
InternationalConferenceonArtificialIntelligenceandStatistics,pages1459–1467.PMLR,2021.
Peng-TaoJiang,Chang-BinZhang,QibinHou,Ming-MingCheng,andYunchaoWei.Layercam:Exploring
hierarchicalclassactivationmapsforlocalization. IEEETransactionsonImageProcessing,30:5875–5888,
2021.
Tero Karras. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint
arXiv:1710.10196,2017.
StefanKolek,DucAnhNguyen,RonLevie,JoanBruna,andGittaKutyniok.Cartoonexplanationsofimage
classifiers. arXivpreprintarXiv:2110.03485,2021.
StefanKolek,DucAnhNguyen,RonLevie,JoanBruna,andGittaKutyniok. Arate-distortionframework
forexplainingblack-boxmodeldecisions. InInternationalWorkshoponExtendingExplainableAIBeyond
DeepModelsandClassifiers,pages91–115.Springer,2022.
YannLeCun,YoshuaBengio,andGeoffreyHinton. Deeplearning. nature,521(7553):436–444,2015.
Cheng-HanLee,ZiweiLiu,LingyunWu,andPingLuo. Maskgan: Towardsdiverseandinteractivefacial
imagemanipulation. InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2020.
Johannes Linder, Alyssa La Fleur, Zibo Chen, Ajasja Ljubetič, David Baker, Sreeram Kannan, and Georg
Seelig. Interpreting neural networks for biological sequences by learning stochastic masks. Nature
machineintelligence,4(1):41–54,2022.
Ruishan Liu, Shemra Rizzo, Samuel Whipple, Navdeep Pal, Arturo Lopez Pineda, Michael Lu, Brandon
Arnieri, Ying Lu, William Capra, Ryan Copping, et al. Evaluating eligibility criteria of oncology trials
usingreal-worlddataandai. Nature,592(7855):629–633,2021.
ScottMLundbergandSu-InLee. Aunifiedapproachtointerpretingmodelpredictions. Advancesinneural
informationprocessingsystems,30,2017.
Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin, Bala Nair, Ronit Katz,
Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding
withexplainableaifortrees. Naturemachineintelligence,2(1):56–67,2020.
Arturo Moncada-Torres, Marissa C van Maaren, Mathijs P Hendriks, Sabine Siesling, and Gijs Geleijnse.
Explainablemachinelearningcanoutperformcoxregressionpredictionsandprovideinsightsinbreast
cancersurvival. ScientificReports,11(1):1–13,2021.
Edoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel Gallagher, and George Louis Groh. Shap-based
explanationmethods: Areviewfornlpinterpretability. InCOLING,2022.
GuillermoOwen. Gametheory. EmeraldGroupPublishing,2013.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should i trust you?" explaining the pre-
dictionsofanyclassifier. InProceedingsofthe22ndACMSIGKDDinternationalconferenceonknowledge
discoveryanddatamining,pages1135–1144,2016.
MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin. Anchors: High-precisionmodel-agnosticexpla-
nations. InProceedingsoftheAAAIconferenceonartificialintelligence,volume32,2018.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
ProceedingsoftheIEEEinternationalconferenceoncomputervision,pages618–626,2017.
16LloydSShapley. NotesontheN-personGame. RandCorporation,1951.
Andy Shih, Arthur Choi, and Adnan Darwiche. A symbolic approach to explaining bayesian network
classifiers. arXivpreprintarXiv:1805.03364,2018.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propa-
gatingactivationdifferences. InInternationalconferenceonmachinelearning,pages3145–3153.PMLR,
2017.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advancesinneuralinformationprocessingsystems,32,2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
PxTIG12RRHS.
MukundSundararajanandAmirNajmi. Themanyshapleyvaluesformodelexplanation. InInternational
conferenceonmachinelearning,pages9269–9278.PMLR,2020.
MukundSundararajan,AnkurTaly,andQiqiYan.Axiomaticattributionfordeepnetworks.InInternational
conferenceonmachinelearning,pages3319–3328.PMLR,2017.
WesleyTansey,VictorVeitch,HaoranZhang,RaulRabadan,andDavidMBlei.Theholdoutrandomization
test for feature selection in black box models. Journal of Computational and Graphical Statistics, 31(1):
151–162,2022.
Jacopo Teneggi, Alexandre Luster, and Jeremias Sulam. Fast hierarchical games for image explanations.
IEEETransactionsonPatternAnalysisandMachineIntelligence,45(4):4494–4503,2022.
Jacopo Teneggi, Beepul Bharti, Yaniv Romano, and Jeremias Sulam. Shap-xrt: The shapley value meets
conditionalindependencetesting. TransactionsonMachineLearningResearch,2023.
TheWhiteHouse. Executiveorderonthesafe,secure,andtrustworthydevelopmentanduseofartificial
intelligence,2023.
Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. Interpretable
to whom? a role-based model for analyzing interpretable machine learning systems. arXiv preprint
arXiv:1806.07552,2018.
EricWang,PashaKhosravi,andGuyVandenBroeck. Probabilisticsufficientexplanations. arXivpreprint
arXiv:2105.10118,2021.
Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak,
Shengchao Liu, Peter Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes,
andShir. Scientificdiscoveryintheageofartificialintelligence. Nature,620(7972):47–60,August2023.
David Watson, Joshua O’Hara, Niek Tax, Richard Mudd, and Ido Guy. Explaining predictive uncertainty
withinformationtheoreticshapleyvalues. AdvancesinNeuralInformationProcessingSystems,36,2024.
David S. Watson, Limor Gultchin, Ankur Taly, and Luciano Floridi. Local explanations via necessity and
sufficiency: unifyingtheoryandpractice. InCassiodeCamposandMarloesH.Maathuis,editors,Pro-
ceedingsoftheThirty-SeventhConferenceonUncertaintyinArtificialIntelligence,volume161ofProceed-
ingsofMachineLearningResearch,pages1382–1392.PMLR,27–30Jul2021.
Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variable selection using
neuralnetworks. InInternationalConferenceonLearningRepresentations,2018.
17Carlos Zednik. Solving the black box problem: a normative framework for explainable artificial intelli-
gence. Philosophy&Technology,34(2):265–288,2021.
Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, and Shiyu Chang. Towards coherent
image inpainting using denoising diffusion implicit models. In International Conference on Machine
Learning,pages41164–41193.PMLR,2023.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features
for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pages2921–2929,2016.
Yazeed Zoabi, Shira Deri-Rozov, and Noam Shomron. Machine learning-based prediction of covid-19 di-
agnosisbasedonsymptoms. npjdigitalmedicine,4(1):1–5,2021.
18A Appendix
A.1 Proofs
A.1.1 ProofofLemma4.1
Lemma4.1. Letα ∈ (0,1). Forτ > 0,denoteS∗tobeasolutionto(P )forwhich∆uni(S∗,f,x,α) = ϵ.
uni V
Then,S∗ is ϵ-sufficientand ϵ -necessary. Formally,
α 1−α
ϵ ϵ
0 ≤ ∆suf(S∗,f,x) ≤ and 0 ≤ ∆nec(S∗,f,x) ≤ . (15)
V α V 1−α
Proof. Letτ > 0andα ∈ (0,1)anddenoteS∗ tobeasolutionto(P )suchthat
uni
∆uni(S∗,f,x,α) = ϵ. (16)
V
Then,bydefinitionofbeingasolutionto(P ),
uni
|S∗| ≤ τ. (17)
Furthermore,recallthat
∆uni(S∗,f,x,α) = α·∆suf(S∗,f,x)+(1−α)·∆nec(S∗,f,x) (18)
V V V
whichimplies
α·∆suf(S∗,f,x) = ϵ−(1−α)·∆nec(S∗,f,x) (19)
V V
≤ ϵ ((1−α), ∆nec(S∗,f,x) ≥ 0) (20)
V
ϵ
=⇒ ∆suf(S∗,f,x) ≤ . (21)
V α
Similarly,
(1−α)·∆nec(S∗,f,x) = ϵ−α·∆suf(S∗,f,x) (22)
V V
≤ ϵ (α, ∆suf(S∗,f,x) ≥ 0) (23)
V
ϵ
=⇒ ∆nec(S∗,f,x) ≤ . (24)
V 1−α
A.1.2 ProofofLemma4.2
Lemma4.2. For0 ≤ ϵ < ρ(f(x),f ∅(x)) ,denoteS∗ andS∗ tobeϵ-sufficientandϵ-necessarysets. Then,
2 suf nec
ifS∗ isϵ-supersufficientorS∗ isϵ-supernecessary,
suf nec
S∗ ∩S∗ ̸= ∅. (25)
suf nec
Proof. Wewillprovetheresultviacontradiction. Firstrecallthat,
f (x) = E [f(x ,X )] (26)
S
XSc∼VSc
S Sc
andforanymetricρ : R×R (cid:55)→ RonR.
∆suf(S,f,x) ≜ ρ(f(x),f (x)) (27)
V S
∆nec(S,f,x) ≜ ρ(f (x),f (x)). (28)
V Sc ∅
19SinceρisametriconR,itsatisfiesthetriangleinequalitythus,fora,b,c ∈ R
ρ(a,c) ≤ ρ(a,b)+ρ(b,c). (29)
Now,letS∗ beϵ-supersufficientandsuppose
suf
S∗ ∩S∗ = ∅. (30)
suf nec
Thisimplies
S∗ ⊆ (S∗ ) . (31)
suf nec c
andsubsequently,sinceS∗ isϵ-supersufficient
suf
∆suf((S∗ ) ,f,x) ≤ ϵ. (32)
V nec c
Asaresult,observe
ρ(f(x),f (x)) ≤ ρ(f(x),f (x))+ρ(f (x),f (x)) triangleinequality (33)
∅ (S n∗ ec)c (S n∗ ec)c ∅
= ∆suf((S∗ ) ,f,x)+∆nec((S∗ ) ,f,x) (34)
V nec c V nec c
≤ ϵ+∆nec((S∗ ) ,f,x) S∗ isϵ-supersufficient (35)
V nec c suf
≤ 2ϵ S∗ isϵ-necessary (36)
nec
ρ(f(x),f (x))
=⇒ ϵ ≥ ∅ (37)
2
which is a contradiction because 0 ≤ ϵ < ρ(f(x),f ∅(x)) . Thus S∗ ∩ S∗ ̸= ∅. The proof of this result
2 suf nec
assumingS∗ isϵ-supernecessaryalsofollowsthesameargument.
nec
A.1.3 ProofofTheorem4.1
Theorem4.1.Letτ ,τ > 0and0 ≤ ϵ < 1 ·ρ(f(x),f (x)). DenoteS∗ andS∗ tobeϵ-supersufficient
1 2 2 ∅ suf nec
and ϵ-super necessary solutions to (P ) and (P ), respectively, such that |S∗ | = τ and |S∗ | = τ .
suf nec suf 1 nec 2
Then,thereexistsasolutionS∗ to(P )suchthat
uni
∆uni(S∗,f,x,α) ≤ ϵ and max(τ ,τ ) ≤ |S∗| < τ +τ . (38)
V 1 2 1 2
Furthermore,ifS∗ ⊆ S∗ orS∗ ⊆ S∗ . thenS∗ = S∗ orS∗ = S∗ ,respectively.
suf nec nec suf nec suf
Proof. ConsiderthesetS∗ = S∗ ∪S∗ . Thissethasthefollowingproperties
suf nec
(P1) S∗ isϵ-sufficientbecauseS∗ isϵ-supersufficient
suf
(P2) S∗ isϵ-necessarybecauseS∗ isϵ-supernecessary
suf
(P3) |S∗| ≥ max(τ ,τ )with|S∗| = τ whenS∗ ⊂ S∗ andwith|S∗| = τ whenS∗ ⊂ S∗
1 2 1 nec suf 2 suf nec
(P4) ViaLemma4.1,weknowS∗ ∩S∗ ̸= ∅thus|S∗| < τ +τ
suf nec 1 2
Thenby(P1)and(P2)
∆uni(S∗,f,x,α) = α·∆suf(S∗,f,x)+(1−α)·∆nec(S∗,f,x) (39)
V V V
≤ α·ϵ+(1−α)·ϵ = ϵ (40)
andby(P3)and(P4)wehavemax(τ ,τ ) ≤ |S∗| < τ +τ
1 2 1 2
20A.1.4 ProofofCorollary5.1
Corollary 5.1. Suppose for any S ⊆ [d], V = p(X | X = x ). Let α ∈ (0,1), ϵ ≥ 0, and denote
S S Sc Sc
ρ : R×R (cid:55)→ RtobeametriconR. Furthermore,forf(X) = E[Y | X]andτ > 0,letS∗ beasolutionto
(P )suchthat∆uni(S,f,x,α) = ϵ. Then,S∗ satisfiesthefollowingconditionalindependencerelations,
uni V
ρ(E[Y | x], E[Y | X = x ]) ≤ ϵ and ρ(cid:0)E[Y | X = x ], E[Y](cid:1) ≤ ϵ . (41)
S∗ S∗ S∗ S∗
α c c 1−α
Proof. AllweneedtoshowisthatwhenV = p(X | X = x )andf(X) = E[Y | X],wehave
S S Sc Sc
f (x) = E[Y | X = x ]. (42)
S S S
Oncethisisproven,wesimplyapplyLemma4.1. Sosupposef(x) = E[Y | X = x]andforanyS ⊆ [d],
V = p(X | X = x ). Thenbydefinition
S S Sc Sc
(cid:90)
f (x) = E [f(x ,X )] = f(x ,X )·p(X | X = x )dX (43)
S VSc S Sc S Sc Sc S S Sc
X
(cid:90)
= E[Y | X = x ,X ]·p(X | X = x )dX (44)
S S Sc Sc S S Sc
X
(cid:90) (cid:18)(cid:90) (cid:19)
= y·p(y | X = x ,X )dy ·p(X | X = x )dX (45)
S S Sc Sc S S Sc
X Y
(cid:90) (cid:18)(cid:90) (cid:19)
= y p(y,X | X = x )dX dy (46)
Sc S S Sc
Y X
(cid:90)
= y·p(y | X = x )dy (47)
S S
Y
= E[Y | X = x ]. (48)
S S
ByapplyingLemma4.1,wehavethedesiredresult.
A.1.5 ProofofTheorem5.1
Theorem 5.1. Consider an input x for which f(x) ̸= f (x). Denote by Λ = {S,S } the partition of
∅ d c
[d] = {1,2,...,d},anddefinethecharacteristicfunctiontobev(S) = −ρ(f(x),f (x)). Then,
S
ϕshap(Λ ,v) ≥ ρ(f(x),f (x))−∆uni(S,f,x,α) (49)
S d ∅ V
Proof. Beforeweprovetheresult,recallthefollowingpropertiesofametricρ
(P1) ∀a,b ∈ Rρ(a,b) = 0 ⇐⇒ a = b
(P2) fora,b,c ∈ R, ρ(a,c) ≤ ρ(a,b)+ρ(b,c).
Nowwebegintheproof. ForthepartitionΛ = {S,S }of[d] = {1,2,...,d}andcharacteristicfunction
d c
v(S) = −ρ(f(x),f (x)),ϕshap(Λ ,v)isdefinedas
S S d
1 1
ϕshap(Λ ,v) = ·[v(S ∪S )−v(S )]+ ·[v(S)−v(∅)] (50)
S d 2 c c 2
1 1
= ·[ρ(f(x),f (x))−ρ(f(x),f(x))]+ ·[ρ(f(x),f (x))−ρ(f(x),f (x))] (51)
2 Sc 2 ∅ S
1 1
= ·[ρ(f(x),f (x))]+ ·[ρ(f(x),f (x))−ρ(f(x),f (x))] by(P1) (52)
2 Sc 2 ∅ S
21By(P2)
ρ(f(x),f (x)) ≤ ρ(f(x),f (x))+ρ(f (x),f (x)) (53)
∅ Sc Sc ∅
=⇒ ρ(f(x),f (x)) ≥ ρ(f(x),f (x))−ρ(f (x),f (x)). (54)
Sc ∅ Sc ∅
Thus
1 1
ϕshap(Λ ,v) = ·[ρ(f(x),f (x))]+ ·[ρ(f(x),f (x))−ρ(f(x),f (x))] (55)
S d 2 Sc 2 ∅ S
1 1
≥ ·[ρ(f(x),f (x))−ρ(f (x),f (x))]+ ·[ρ(f(x),f (x))−ρ(f(x),f (x))] (56)
2 ∅ Sc ∅ 2 ∅ S
= ρ(f(x),f (x))−∆uni(S,f,x,α) (57)
∅ V
A.2 AdditionalExperimentalDetails
In this section, we include further experimental details. All experiments were performed on a private
cluster with 8 NVIDIA RTX A5000 with 24 GB of memory. All scripts were run on PyTorch 2.0.1,
Python3.11.5,andCUDA12.2.
A.2.1 RSNACTHemorrhage
Dataset Details. The RSNA 2019 Brain CT Hemorrhage Challenge dataset (Flanders et al., 2020), con-
tains752803imageslabeledbyapanelofboard-certifiedradiologistswiththetypesofhemorrhagepresent
(epidural,intraparenchymal,intraventricular,subarachnoid,subdural).
Implementation. Recallforthisexperiment,toidentifysufficientandnecessarymasksS forasample
x,weconsideredtherelaxedoptimizationproblem(Fongetal.,2019;Koleketal.,2021,2022)
argmin ∆uni(S,f,x,α)+λ ·||S|| +λ ·||S|| . (58)
V 1 1 TV TV
S⊆[0,1]d
where||S|| and||S|| aretheL1 andTotalVariationnormofS,whichpromotesparsityandsmooth-
1 TV
ness respectively and λ and λ are the associated. To solve this problem, a mask S ∈ [0,1]512×512 is
Sp Sm
initializedwithentriesS ∼ N(0.5, 1 ). For1000iterations,themaskSisiterativelyupdatedtominimize
i 36
α·|f(x)−f (x)|+(1−α)·|f(x)−f (x)|+λ ·||S|| +λ ·||S|| (59)
S S 1 1 TV TV
whereforanyS,
K
1 (cid:88)
f (x) = f((X˜ ) ) with (X˜ ) = x◦1˜ +(1−1˜ )◦b . (60)
S S i S i S S i
K
i=1
Here the entries (1˜ ) ∼ Bernoulli(S ) and b is the ith entry of a vector b = (b ,··· ,b ) ∼ V. In our
S i i i 1 d
implementationthereferencedistributionV istheunconditionalmeanimageovertheoftrainingimages
and so b is the simply the average value of the ith pixel over the training set. To allow for differentia-
i
tion during optimization, we generate discrete samples 1˜ using the Gumbel-Softmax distribution. This
S
methodologysimplyimpliestheentries(X˜ ) isaBernoullidistributionwithoutcomes{b ,x },i.e.(X˜ )
S i i i S i
isdistributedas
Pr[(X˜ ) = x ] = S (61)
S i i i
Pr[(X˜ ) = b ] = 1−S (62)
S i i i
For each α ∈ {0,0.5,1}, during optimization we set K = 10, λ = 2 and λ = 20 and use the Adam
1 TV
optimizerwithdefaultβ-parametersofβ = 0.9,β = 0.99andafixedlearningrateof0.1.
1 2
22A.2.2 CelebA-HQ
Dataset Details. We use a modified version of the CelebA-HQ dataset (Lee et al., 2020; Karras, 2017)
whichcontains30,000celebrityfacesresizedto256×256pixelswithseverallandmarklocationsandbinary
attributes(e.g.,eyeglasses,bangs,smiling).
Implementation. Recall for this experiment, to generate sufficient or necessary masks S for samples
x,welearnamodelg : X (cid:55)→ [0,1]d viasolvingthefollowingoptimizationproblem:
θ
argmin E (cid:2) ∆uni(g (X),f,X,α)+λ ·||g (X)|| +λ ·||g (X)|| (cid:3) (63)
V θ 1 θ 1 TV θ TV
θ∈Θ X∼DX
To learn sufficient and necessary explainer models, we solve Eq. (14) via empirical risk minimization for
α ∈ {0,1}respectively. GivenN samples{X }N i. ∼i.d. D ,wesolve
i i=1 X
N
1 (cid:88)(cid:2) ∆uni(g (X ),f,X ,α)+λ ·||g (X )|| +λ ·||g (X )|| (cid:3) . (64)
N V θ i i 1 θ i 1 TV θ i TV
i=1
Here
∆uni(g (x ),f,x ,α) = α·|f(x )−f (x )|+(1−α)·|f(x )−f (x )| (65)
V θ i i i S i i S i
where is f (x ) is evaluated in the same manner as in the RSNA experiment. For α = 0, λ = 0.1 and
S i 1
λ = 100. Forα = 1,λ = 1andλ = 10. Forbothα,duringoptimizationweuseabatchsizeof32,
TV 1 TV
set K = 10 and use the Adam optimizer with default β-parameters of β = 0.9, β = 0.99 and a fixed
1 2
learningrateof1×10−4
Sampling. To generate the samples in Figs. 4 and 5, samples we use the CoPaint method (Zhang
et al., 2023). We utilize their code base and pretrained diffusion models with the exact the same pa-
rameters as reported in the paper to perform conditional generation. Everything used is available at
https://github.com/UCSB-NLP-Chang/CoPaint.
23