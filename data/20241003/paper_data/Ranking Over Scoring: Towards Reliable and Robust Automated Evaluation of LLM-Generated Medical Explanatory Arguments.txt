Ranking Over Scoring: Towards Reliable and Robust Automated
Evaluation of LLM-Generated Medical Explanatory Arguments
IkerDelaIglesia1*,IakesGoenaga1*,JohannaRamirez-Romero2,
JoseMariaVilla-Gonzalez2,JosuGoikoetxea1,AnderBarrena1
1HiTZCenter-Ixa,UniversityoftheBasqueCountryUPV/EHU,
2CrucesUniversityHospital,(Barakaldo,Biscay,Spain)
{iker.delaiglesia,iakes.goenaga,ander.barrena}@ehu.eus
Abstract
Proxy MCQA Task
Gold
EvaluatingLLM-generatedtexthasbecomea A tof tt her e a I Ctr Uaf f inic ca oc mci ad .e Ant f ta e r3 s8 e- vy ee ra ar l- o dl ad y p s a tt hie en pt a i ts i e a nd t m doit et sed Argument
not improve neurologically and a CT scan shows
key challenge, especially in domain-specific h coem rtio cr or -h sa ug bi cc o p rtu in cac lt a jt ue n l ce ts ioio nn . s W in h ath t e is c to hr ep u ds ia c ga nl olo ss isu ?m and
contexts like the medical field. This work 1 2- - A Trc ou bte o cs yu tb od pu er na il c h pe um rpa uto ram .a.
3- Cerebral hemorrhagic contusion.
4- Severe diffuse axonal injury.
introduces a novel evaluation methodology 5- Acute heart attack.
forLLM-generatedmedicalexplanatoryargu-
Argument
ments,relyingonProxyTasksandrankingsto Generation
Prompting
closelyalignresultswithhumanevaluationcri- Arguments
teria,overcomingthebiasestypicallyseenin
LLMsusedasjudges. Wedemonstratethatthe
Generated
proposed evaluators are robust against adver- Argument
LLMs
sarialattacks,includingtheassessmentofnon-
argumentative text. Additionally, the human-
craftedargumentsneededtotraintheevaluators
Matching Ranking
areminimizedtojustoneexampleperProxy
Task. ByexaminingmultipleLLM-generated 🥇 🥇
arguments,weestablishamethodologyforde-
🥈 🥈
terminingwhetheraProxyTaskissuitablefor
🥉 🥉
evaluatingLLM-generatedmedicalexplanatory LM Evaluator
arguments, requiring only five examples and
twohumanexperts. TheProxyTasks,LMeval-
Figure 1: Graphical abstract illustrating the key ele-
uators,andthecodewillbemadeavailablefor
ments of our approach. Synthetic arguments are first
reproducibility1.
generatedbypromptingmultipleLLMs,whicharethen
rankedalongsidegold-standardargumentsbybothour
1 Introduction
trainedLMevaluatorandahumanexpert. Ourresults
showtheLMevaluatoralignswithhumanpreferences.
The field of Natural Language Processing (NLP)
hasundergoneatransformativeevolutionwiththe
adventofLanguageModels(LMs)andLargeLan-
andNarasimhan,2018)andLlama(Touvronetal.,
guageModels(LLMs). Theresultsinthemedical
2023)havebeenincreasinglyusedforpre-training
domainhavebeenparticularlynotable,withLLMs
on medical text data, leading to notable improve-
achievingremarkableaccuracyinsolvingmedical
mentsinthecoherenceandrelevanceofgenerated
exams(Singhaletal.,2023;Strongetal.,2023;Liu
medicalexplanations. However,theevaluationof
etal.,2024). Thissuccessisdrivingtheongoingde-
suchexplanatoryargumentsremainsaconsiderable
velopmentofthesemodelstofurtherenhancesup-
challenge(Changetal.,2023),particularlywithin
portforEvidence-BasedMedicine(EBM)which
themedicaldomain,whereobtainingmeaningful
involvestheconscientious,explicit,andthoughtful
datasetsandassessingaccuracyisinherentlydiffi-
use of present best medical evidence in making
cult. Thehigh-entropynatureoflanguageallows
medicaldecisions(Sackettetal.,1996). Withthe
formultiplevalidresponses,complicatingtheeval-
adventoflargeautoregressivegenerativemodels,
uation of relevance, coherence, and factual accu-
decoder-onlyarchitecturessuchasGPT(Radford
racy. Thiscomplexityisfurtherexacerbatedbythe
challengeofobjectivelyquantifyingthesefactors
* EqualContribution.
1Uponacceptance. whilealsoaccountingforhumanpreferences.
1
4202
peS
03
]LC.sc[
1v56502.9042:viXraDespite the long tradition of automatic long- making medical decisions, modeled using proxy
text evaluation, particularly in Machine Transla- tasksalongthelinesof(Tanetal.,2024). Thisway,
tion, it remains an unresolved challenge. Met- thefollowingResearchQuestionsarise:
rics like BLEU for translation (Papineni et al.,
RQ1 CanwedevelopanLMEvaluatorthataligns
2002), ROUGE for summarization (Lin, 2004),
withhumanpreferenceswhenassessingmed-
andembedding-basedscoressuchasBERTScore
ical explanatory arguments in EBM while
(Zhang* et al., 2020), BLEURT (Sellam et al.,
avoidinggenerativeLLMJudgesbias?
2020), and COMET (Rei et al., 2020) have been
widelyused,buttheypresentmajorissuesforeval-
RQ2 AreLMevaluatorsbuiltuponProxyTasksthat
uatingexplanatoryarguments. Thesemetricsrely
modelEBMsuitableforevaluatingthegood-
onreferencetexts,whicharedifficulttoobtainin
ness of LLM-generated medical arguments
themedicaldomain,andoftenoverestimateirrel-
androbustagainstadversarialattacks?
evantdifferencesduetothehighentropyofvalid
arguments. Thisproblemextendsbeyondexplana- RQ3 AreallProxyTasksequallyvaluableforeval-
toryargumentationtoalllong-textevaluations,as uationpurposes?
notedintheliterature(Liuetal.,2023;Sulemetal.,
RQ4 Arehumanevaluationsconsistentacrossdif-
2018;Sunetal.,2022).
ferentProxyTasks?
More recent approaches have tried to address
theseproblemsusingLLMsasJudges(Zhengetal., RQ5 DoestheLMevaluatorwithhighertaskscores
2024;Lietal.,2024;KocmiandFedermann,2023; produce a ranking of arguments that better
Shen et al., 2023). Despite the increasing pop- alignswithhumanjudgmentcomparedtoan
ularity of such evaluators, it has been observed LMevaluatorwithlowertaskscores?
that these models often exhibit their own biases;
Byexploringtheseresearchquestions,weaimto
self-enhancement bias, tending to recognize and
introduceafastandcost-effectiveautomaticeval-
favor their own outputs over those generated by
uationmethodtoevaluatemedicalexplanatoryar-
othermodels,positionalbias,namely,thepropen-
gumentationprovidedbyLLMsintheframework
sity to favor certain positions over other and ver-
of EBM. To do so, the proposed discriminative
bosity bias, prioritizing lengthier, more verbose
LMevaluatorratherthanevaluatingtheoutcome
responses,evenwhentheyfallshortinclarity,qual-
against a reference gold-standard text, evaluates
ity,oraccuracycomparedtomoreconciseoptions
howhelpfulandinformativethegeneratedtextis
(Panicksseryetal.,2024;Zhengetal.,2024).
for making medical decisions modeled as three
Given this perspective, it is clear that the most
ProxyTasks,MedicalQuestionAnswering,Misin-
reliablewaytoevaluatetheresponsesgeneratedby
formation,andNaturalLanguageInferenceinclin-
LLMsisthroughhumanevaluation. Nonetheless,
icaltrials. Besides,thisevaluationmethodaligns
this method has significant drawbacks especially
with the assessments conducted by expert physi-
whenitcomestohighlyspecializeddomainslike
cians. WealsoanalyzetheadequacyofeachProxy
the medical domain. Finding experts capable of
Task for accurate explanatory arguments evalua-
accuratelyevaluatingtheresponsesisverydifficult.
tion. Allinall,ourproposaleliminatestheneedfor
Addedtothis,humanevaluationisveryexpensive
subject matter experts’ evaluations as well as the
and time-consuming and in the case of long-text
existenceofareferencegold-standardexplanatory
evaluationitisdifficulttoproperlyassessquality
argument while also minimizing some biases of
guidelines. Thisisparticularlyevidentinextreme
generativeLLMJudges.
cases,wheremultiplecorrectresponsesmakethe
differencestoosubtletoevaluate,orwhenthegen-
2 RelatedWork
erated texts are incorrect, making it challenging
to objectively assess the results. For example, in ThedevelopmentofLLMsinthemedicaldomain
cases where two argumentative explanations are focusesnowadaysonscalinguppre-trainingdata
usingdifferentbutcompletelynon-senseevidence. andmodelparametersoradaptinggeneral-purpose
Consideringthatexplanatoryargumentsarein- LLMs to the medical domain. Notable examples
tendedtoassistmedicaldecision-makingwithinan includeMed-PaLM 2andMeditron. Med-PaLM
EBMframework,thepresentproposalaimstomea- 2,achieved86.5%accuracyonMedQA(USMed-
suretheadequacyoftheexplanatoryargumentsin ical Licensing Exam-style questions), surpassing
2thepreviousstate-of-the-art(Singhaletal.,2023), meta-questionsishuman-intensive. Additionally,
whileMeditronintegratesdiversemedicalinforma- theresultslackofcomparisonbetweentheperfor-
tion for comprehensive insights and high-quality manceofthesystemswithorwithoutthegenerated
medicalargumentation(Chenetal.,2023). How- long-textmakingitdifficulttoassesstherealim-
ever, we will not focus on directly assessing the pact that adding the generated long-text has on
capabilityofLLMstosolvetasksbutrathereval- solving the Proxy Task. Our proposal does not
uatingtheinformativenessofLLM-generatedex- requirebuildingmeta-questionsandweincludea
planatoryargumentsinthemedicaldomain,which Naive version of every system where there is no
isdoublychallenging. explanatoryargumentincludedtosolvetheProxy
To address the issue of long-text evaluation in Tasks. WealsoextendthenumberofProxyTasks.
a general domain, Tan et al. (2024) propose us- Yao et al.’s work also explores long-text eval-
ing a QA task as a proxy to assess the helpful- uation, with a particular emphasis on human-
nessandrelevanceofcontentgeneratedbyLLMs. annotatednaturallanguageexplanationstoassess
Theirsystemcomprisestwokeycomponents: meta- whethertheyconsistentlyenhancemachinelearn-
questions and proxy-questions. Meta-questions ing models in NLP. Especially relevant for this
prompt LLMs to generate comprehensive, factu- workistheiranalysisofhowhuman-annotatedex-
allycorrecttextrequiringafullunderstandingof planationsshowvaryinglevelsofhelpfulness,de-
thetopic,whileproxy-questionsevaluatethequal- pending on the task and dataset used. The study
ityofthegeneratedcontentbyassessingwhetherit evaluatesfivelarge-scaledatasets(e.g.,CoS-E,e-
includessufficientrelevantandaccurateinforma- SNLI)usingtwoNLPmodels(T5andBART)to
tion. Forexample,ifthemeta-questionasksabout assessexplanationquality. Theirfindingsshowthat
the First Industrial Revolution, a proxy-question explanationsinECQAarehighlybeneficial,while
mightbe,"TrueorFalse: Thesteamengineplayed CoS-Eexplanations,althoughnoisy,stillofferim-
a crucial role in the First Industrial Revolution." provements in model predictions. This suggests
The authors compare their Proxy-QA evaluator that explanation evaluation should focus on task-
withhumanevaluatorsandLLM-as-judges,using specificperformanceratherthantreatingallexpla-
GPT-as-Judge. They randomly sample ten meta- nationsasequallyvaluable. Whiletheyintroducea
questionsandusefourLLMstogeneratelong-texts. metrictoassessthehelpfulnessoflongtexts,they
These 4 generated text candidates are then evalu- neithercomparedifferentexplanationsnorverify
atedthroughpairwisecomparisonsbetweenthree iftheirmetricalignswithhumanpreferences.
evaluators(theirProxy-QAevaluator,humaneval- Tosummarize,thereisanurgentneedforanob-
uators,andGPT-as-judge)usingthewinratemea- jectivesystemforindependentevaluationofmod-
sure2. ernLLMs’medicalargumentgenerationabilities.
The primary goal of this pairwise comparison To address this, we have developed a medical ar-
inwinrateistodeterminehowcloselyProxyQA gumentation evaluation method based on Proxy
correlateswithhumanjudgmentcomparedtoLLM- Tasksthatalignswiththeassessmentsofmedical
basedevaluations. experts. Ourevaluationmethodallowsustoassess
The authors found that ProxyQA’s evaluations medical argumentations quickly, efficiently, and
were highly correlated with human preferences, cost-effectively.
whereasGPT-as-judgetendedtooverestimatethe
qualityofthetextgeneratedbyGPTmodels. Prox- 3 ExperimentalSetup
yQA showed a balanced and reliable evaluation,
Inthisstudy,wedevelopedanexperimentalframe-
reflecting human preferences more closely than
worktoinvestigatethealignmentbetweenLMeval-
GPT evaluators, which were biased towards out-
uatorsystemsandhumanpreferencesinassessing
puts from GPT-based models. Scalability and
explanatoryarguments. Argumentqualityisindi-
domain adaptation is one of the main pitfalls of
rectly estimated by itsimpact onProxy Task per-
thismethod,creatingandmaintaininghigh-quality
formance. ThesetasksarehandledbyLMstrained
2WinRateCalculation:Thewinrateiscalculatedbasedon toperformtheoriginaltask. TheseLMsalsoserve
pairwisecomparisonsofthereports.Ifonemodel’soutputis asevaluatorswhenincorporatingexplanatoryargu-
preferredoveranother,itwinsthatcomparison.Thiswinrate
mentsasadditionalinput,byrankingtheincorpo-
measureshowoftenonemodel’sreportisratedbetterthan
othersbytheevaluators. ratedargumentsbasedonthetaskscore.
3Thedeparturepointofourapproachisthegen- termedControlCases,whichcomplementPrimary
eration of explanatory arguments. Indeed, they Arguments. Throughthisapproach,LMevaluators
comprise the base ofour approach, and they will are tested with four adversarial scenarios during
be generated by humans or LLMs. On the one inference,designedtoassesstheirabilitytodistin-
hand,eachtaskwillhavehigh-qualityarguments guishmeaningfulargumentsfromirrelevantormis-
writtenbyhumanexpertsthatwewillconsideras leadingcontent,asdetailedinsubsubsection3.2.2.
thegoldstandard. Ontheotherhand,wewillgener- Throughthisexperimentalsetup,weaimtothor-
atediverseargumentsforeachtaskusingdifferent oughlyinvestigatetheeffectivenessofProxyTask-
LLMs. Themainfocusoftheevaluationapproach basedevaluatorsinmodelinghumanjudgmentand
presented in this paper is focused on these two therelativevalueofdifferentProxyTasks.
kindsofarguments,termedPrimaryArguments.
RegardingtheProxyTasks,weemployadiverse 3.1 ProxyTasks&ProxyTaskLMEvaluators
set,includingMedicalMultipleChoiceQuestion
3.1.1 ProxyTasksBenchmarks
Answering (MMCQA), Medical Misinformation
WerepurposedthreediversebenchmarksasProxy
Detection,andNaturalLanguageInference(NLI)
Tasks, each selected to capture distinct types of
inclinicaltrials. Thesetasksareselectedbecause
argumentation,offeringabroadevaluationacrossa
they represent different contexts where explana-
rangeofcomplexscenarios. Whileallthedatasets
toryargumentationishelpful,eachtaskrequiring
selected for the tasks include a complementary
specific types of arguments. By employing a di-
gold-standardargumentsupportingthecorrectla-
verse set of Proxy Tasks rather than relying on a
bel, this argument is unnecessary to perform the
singleone,weaimtoexplorewhichtasksaremost
base task. Detailed examples of instances from
relevantandsuitableforevaluationpurposes(ad-
eachdatasetcanbefoundinAppendixC.
dressingRQs2and3).
We will also have two types of evaluators: hu-
MedicalMultipleChoiceQABenchmark We
man evaluators and LM ones. For the latter, we
employedtheEnglishtranslationoftheCasiMedi-
train discriminative LMs on the Proxy Tasks to
cosdataset(Goenagaetal.,2023),whichassesses
function as evaluators, as mentioned previously.
models’abilitytoanswermedicalmultiple-choice
Theevaluatorsthusprovideanindirectassessment
questions. Eachinstanceincludesaquestionwitha
by leveraging task performance metrics to differ-
clinicalcase,possibleanswers,andagold-standard
entiatebetweenarguments,therebyaddressingthe
explanation supporting the correct choice. The
potentialbiasesassociatedwithLLMsasJudges-
original split distribution was kept. To reduce la-
basedevaluationmethods(RQ1andRQ2). Along-
belpriorbias,ensuringthemodelpredictscorrect
side these Proxy Tasks, human experts indepen-
answersbasedoncontentratherthananswerorder,
dentlyestimatethequalityofargumentswithinthe
we preprocessed the dataset by creating multiple
contextoftheseProxyTasks,providingastandard
versionsofeachinstance,varyingthepositionof
againstwhichtheevaluatorscanbecompared,ad-
thecorrectanswer. Additionally,wemodifiedthe
dressingRQs3and4.
gold-standardexplanationsbyremovingstatements
Therefore, we will have human and LM evalu-
thatexplicitlyidentifiedthecorrectanswerandre-
ators,and,essentially,thecoreofouranalysisfo-
placedreferencestotheanswer’spositionwiththe
cusesonexaminingwhichofthelatteralignsmost
answer’stext.
closelywiththeformer. Weanalyzethedegreeto
whichtherankingsgeneratedbytheLMevaluators MisinformationDetectionBenchmark Weem-
reflect human preferences, thereby assessing not ployed a subset of the English version of the
justtaskperformancebutalsothemeaningfulness HealthFCdataset(Vladikaetal.,2024),whichfo-
of the rankings in the context of human-aligned cusesonindicatingwhetherhealth-relatedclaims
argumentevaluation. Wealsoexaminewhetherthe aresupported,refuted,orlackenoughinformation.
LMevaluatorwithmaximizedoverallProxyTask Thedatasetcontains742instances,whichwestrat-
scoreistheonewiththeclosestrankingalignment ified and split into 70% (518) for training, 15%
withhumancriteria(RQ5). (111)fordevelopment,and15%(112)fortesting.
Tofurthertesttherobustnessandabilitytodis- Asmentionedinsubsection3.3,instanceslabeled
cernthequalityoftheargumentsofourLMeval- “Not Enough Evidence” in the test split were ex-
uators, we introduce a second set of arguments, cluded from both human and automatic rankings
4whencalculatingthefinalscores. Wetermedthis subsubsection 3.2.1). Each training instance in-
subsettheMisinformationWithEvidencedataset. cludesanargumentrandomlyselectedfromvarious
LLMs, ensuring a balanced representation. This
NLI Benchmark The NLI4CT clinical trial
approach allows the evaluator to learn diverse ar-
dataset(Jullienetal.,2023)containsclinicaltrial
gumentstyles,reducingfavoritismtowardanyspe-
records(CTR),includingamedicalstatementand
cificLLM-generatedargumentandimprovingits
alabelindicatingwhethertheCTRsupportsorcon-
neutrality and robustness in assessing argument
tradictsthestatement. Unliketheothertasks,the
quality. We trained three models with different
argumentsinthiscaseareextracteddirectlyfrom
argumentsetstominimizebiasandvariability.
the CTR. While the original dataset incorporates
instancesinvolvingtwoclinicaltrials,wefocused 3.2 PrimaryArgumentsandControlCases
solelyonthoseinvolvingasingletrial. Thedistri-
Primary Arguments and Control Cases are two
butionis1035instances(74%)fortraining,140in-
main components of our evaluation framework.
stances(10%)fordevelopment,and229instances
PrimaryArgumentsarecentraltoourresearchand
(16%)fortesting.
include both gold-standard arguments crafted by
3.1.2 ProxyTaskLMEvaluators domainexpertsandsyntheticargumentsgenerated
by various LLMs. These arguments are the only
This section introduces our key contribution: the
ones also evaluated by human experts, providing
explanatoryargumentLMevaluators,designedto
a benchmark for comparing the performance of
systematicallyrankmedicalexplanatoryarguments
our automated LM evaluators. In contrast, Con-
withoutdirecthumaninvolvement. Theseevalua-
trol Cases are designed to test the robustness of
torswillbecomparedwithexpertassessmentsto
theLMevaluatorsbyincorporatingmisleadingor
see which approach aligns most closely with hu-
irrelevantcontent(seeFigure2).
man judgments. Our method uses discriminative
languagemodelstrainedonProxyTasks,avoiding
thebiasthatgenerativeLLMsintroducewhenact- Primary Control
Arguments Cases
ingasevaluators. Generativemodelstendtofavor
argumentssimilartothosetheygenerate,whereas
discriminative models focus purely on task per-
formance. Thisensuresamoreobjectiveranking
based on how effectively the arguments improve
ProxyTaskoutcomes.
We developed three evaluators, all based on
theEriBERTaencodermodel(DelaIglesiaetal., Highly Ranked Low Ranked
2023), each trained with different types of argu- 🥇 🥇
ments. Table2inAppendixAoutlinesthetraining 🥈 🥈
inputsforeachevaluatorbasedoneachProxyTask. 🥉 🥉 LM Evaluator
We used the train and dev splits for training and
tuning,andthetestsplitforthefinalranking.
Figure2: Agraphicalabstractillustratingthesystem’s
maincomponentsandbehavior. TheproposedLMeval-
Baseline Evaluator It serves as the simplest
uator prioritizes ranking primary arguments first and
modelinthiswork. Itistheoriginalclassification
placingcontrolcaseslast.
task, which means, training without the comple-
mentaryarguments.
3.2.1 LLM-GeneratedSyntheticArguments
Expert-Trained Evaluator Trained using
Toevaluateautomatedmedicalargumentation,we
human-crafted gold-standard arguments. This
generatedsyntheticargumentsusingthreeLLMs:
evaluator is expected to align most closely with
GPT-4o(OpenAI,2024),knownforitsstronggen-
human judgment, as the training data comes
eralreasoningabilities;OpenBioLLM(AnkitPal,
directlyfromdomainexperts.
2024),amodelfine-tunedonlarge-scalebiomedi-
LLM-TrainedEvaluator Onekeycontribution caldatasetsforhighaccuracyinmedicaltextgen-
ofthisstudy,anLMtrainedexclusivelywithsyn- eration; and Llama3-70B-Instruct (Meta, 2024),
theticargumentsgeneratedbyLLMs(detailedin whichweinstruction-tunedforeachProxyTaskto
5optimize its performance in the medical domain (2022)embeddingmodel,queryingeachinstance’s
(seeAppendixBforanexample). text,andextractingthetopfivedocuments. These
Forconsistencyandminimumhumaninterven- weresplitinto300-characterchunksandreranked
tion,weusedasingleexampleforone-shotprompt- usingColBERTv2(Santhanametal.,2022),with
ing for each Proxy Task. For MMCQA and Mis- thetopthreepassagesfedtotheevaluator.
informationDetectiontasks,theLLMsgenerated
free-style explanatory arguments, while for NLI 3.3 HumanandAutomaticRanking
tasks,theyextractedevidencefromtheCTR.Iden- We engaged two clinicians with prior experience
tical generation parameters were used across all in medical annotation and system evaluation, uti-
models: maximumtokenlengthof256,sampling lizing the test split of the datasets. After a pre-
enabled,temperatureof0.9,andtop-pat0.85. liminaryround,5exampleswererankedforeach
task to calculate the Inter-Annotator Agreement
3.2.2 ControlCases
(ITA).ExpertsrankedthefourPrimaryArguments
ControlCasesaredesignedtoassesstheLMeval-
on a scale of 1 to 5, with 5 assigned to clearly
uators’ ability to differentiate between meaning-
incorrectarguments,andtieswereallowedwhen
fulargumentsandirrelevantormisleadingcontent.
argumentswereofequalquality. WeusedKrippen-
ControlCasesserveasadversarialattacksforeval-
dorff’salpha(Krippendorff,2011)tocalculateITA,
uators(JiaandLiang,2017). Thissectionoutlines
achieving the following scores: MMCQA=0.72,
theconstructionofthesecasesandtheirroleinour
Misinformation=0.61,andNLI=0.44.
broaderexperimentalframework.
In the ITA phase, we noticed significant dis-
NoArgument Thiscaseincludesnomedicalar- agreement in the Misinformation Detection task,
gumentation, testing whether evaluators actually particularlyforinstanceslabeledas“Notenough
relyonargumentswhenmakingpredictions. Eval- evidence”. Toaddressthis, weremovedthosein-
uators trained to evaluate medical arguments are stancesandrecalculatedITAusing14newexam-
expectedtostruggle,astheyrelyonthepresence ples, improving the alpha to 0.73. After this ad-
ofexplanationsforpredictions. justment, the clinicians ranked the arguments in-
dependently: 61 instances for MMCQA, 39 for
Label-Only Input The correct answers to the
Misinformation,and98forNLI.
Proxy Tasks are provided but without any sup-
ForbothhumanandLMevaluatorrankings,we
porting argumentation. The purpose is to see if
calculated the average rank for each system. A
evaluatorspenalizethelackofargumentation,de-
Friedman non-parametric test (Friedman, 1937)
spitehavingthecorrectanswers. Weexpectevalua-
(α = 5%)wasappliedtoassesssignificantdiffer-
torstrainedonmedicalargumentationtoprioritize
encesinrankings. TheNLItaskwastheonlyone
explanations and perform worse compared to in-
thatfailedtheFriedmantestinthehumanrankings
stanceswithproperarguments.
(p = 0.561),consistentwithitslowerITA.
NoiseArgument Inthisscenario,medicalargu-
4 Results
ments are present but irrelevant to the instance,
havingbeenrandomlyselectedfromunrelatedex-
Thissectionwillfirstpresentthemainresults,com-
amples. Weanticipatethatwell-trainedevaluators
paringtheproposedautomaticevaluatorstohuman
will recognize the mismatch and perform poorly,
criteria. Finally,wewillexaminetheControlCases
astheargumentsdonotalignwiththeinstance.
todemonstratetheautomaticevaluators’abilityto
IR Passages In this test, we use passages from discardnon-argumentativeinputs.
the WikiMed corpus (Vashishth et al., 2021), re-
4.1 AutomaticEvaluationsResults
trieved via an Information Retrieval (IR) system.
Whilethesepassagescontainmedicalinformation, Figure 3 presents the main results of this study.
theydonotnecessarilyconstitutecoherentorvalid Settingasideevaluatoraccuracyscores,wefocus
arguments. Thiscaseisdesignedtochallengeeval- on the rankings produced by the proposed three
uatorsindistinguishingbetweenstructuredmedi- discriminativeevaluatorsandhumancriteria. For
calargumentsandmereinformativetext. Passages the MMCQA task (a), the rankings demonstrate
were retrieved by indexing full documents with that,intheabsenceofthegoldstandard,theLLM-
FAISS(Johnsonetal.,2019)usingtheDekaetal. trainedevaluatoralignswithhumancriteriawhen
6Proxy Task Evaluators 🥇 🥈 🥉 4º Proxy Task Evaluators 🥇 🥈 🥉 4º Proxy Task Evaluators 🥇 🥈 🥉 4º
Ev - Baseline Ev - Baseline Ev - Baseline
Ev - Expert-Trained Ev - Expert-Trained Ev - Expert-Trained
Ev - LLM-Trained Ev - LLM-Trained Ev - LLM-Trained
Human Criteria Human Criteria Human Criteria
Gold Standard GPT-4o OpenBioLLM Llama3
(a)MMCQA (b)MisinformationWithEvidence (c)NLI
Figure 3: Ranking of the gold-standard argument alongside those generated by automatic systems. Each row
correspondstoadistinctevaluator: thefirstthreerowscorrespondtoourproposedProxyTaskevaluatorsbased
ondiscriminativeclassificationmodels,whilethelastrowreflectsthehumancriteria,obtainedbyhavingexperts
directlyrankthearguments.
rankingLLM-generatedsyntheticarguments,with task,humanevaluatorsalsorankeditfirst.
GPT-4o being the top choice in 3 out of 4 rank- Aspreviouslymentioned,thebestevaluatordoes
ings. The lower ranking of gold-standard argu- not necessarily produce the highest Proxy Task
ments by human experts stems from their design scores. TheleftsideofTable1showstheaverage
forlast-yearmedicalstudents. Theseargumentspri- datasetscoresforeachevaluator. Whiletheexpert-
oritizestraightforwardness,highlightingonlykey trainedevaluatorproducesthehighestscores,the
elementsneededtodiscernthecorrectanswer,as- LLM-trainedevaluatoristheonemostalignedwith
sumingpriorknowledge. However,inthecontext human judgment (see Figure 4 and Tables 10, 11
ofanalyzingclinicalcasesratherthanexamprepa- and 12 for details). On the right side of Table 1,
ration,ahigherdegreeofcontextualizationinthe whenexaminingthescoresofthebestsystemfor
argumentsispreferred. FortheNLItask(c),asim- eachevaluator3,weobservethesamepattern.
ilarpatternemerges,butherethefinetunedLlama3
model ranks first. In the misinformation task (b), DatasetAverage BestSystemPerEvaluator
LMEvaluators MMCQA Misinfo NLI MMCQA Misinfo NLI
the LLM-trained evaluator perfectly matches hu-
Baseline 36.00 44.56 61.12 41.17 48.30 61.50
mancriteria,rankingLlama3-generatedarguments ExpertTrained 72.83 58.67 62.61 82.91 61.22 67.62
first. LLMTrained 70.85 39.74 58.02 78.90 49.43 61.12
Regardingtheevaluators,thelackofargumenta- Table1: Theleftsideshowstheaveragedatasetscores
tionduringtrainingcausesthebaselineevaluator forprimaryarguments(gold-standardandthreeLLMs),
toproducerankingsthatdonotalignwithhuman whiletherightsidedisplaysthebestsystemperevalua-
criteria. In contrast, the expert-trained evaluator torforeachLMevaluator. Thehighestscoreismarked
inbold,andthesecondbestisunderlined.
improves upon the baseline. However, the LLM-
trained approach proves to be the winning strat-
egy,demonstratingthatwecaneffectivelyevaluate
4.2 ControlCases
LLM-generatedargumentationbyusingsynthetic
We have already demonstrated that the LLM-
dataandtrainingdiscriminativeevaluators,without
trainedevaluatoralignswithhumancriteriawhen
relyingonhuman-generatedarguments.
ranking LLM-generated arguments. Figure 4
As mentioned, LLMs acting as judges tend to
presentsanenhancedrankingthatincludesControl
overestimateself-generatedtextandshowaprefer-
Cases,whichserveasaformofadversarialattack.
enceforlongerresponses. Ourapproachaddresses
Ideally,arobustevaluatorshouldrankallControl
thefirstissuebyusinganEriBERTaencoder. We
Casesinthelowestpositions. (a)IntheMMCQA
alsoobservedthatthelongesttextintheMMCQA
task,boththeExpertandLLM-trainedevaluators
taskwasgeneratedbyOpenBioLLM,inthemisin-
prefer argumentations over Control Cases, while
formationtaskbyLlama3, andagainbyOpenBi-
thebaselineevaluatorismisledby3outof4Con-
oLLMintheNLItask. Therankingsprovidedby
trol Cases. (b) For the misinformation task, all
theLLM-trainedevaluatorinFigure3demonstrate
thatthislengthbiasisabsentinourapproachfor 3TheMCQAcolumnrepresentsthescoresforthegold-
standard,GPT-4o,andGPT-4o. ForMisinformation: Open-
MMCQA and NLI tasks. In the case where the
BioLLM,OpenBioLLM,andLlama3.ForNLI:Llama3,the
biasappears,suchasLlama3inthemisinformation goldstandard,andLlama3.
7evaluators perform well, ranking argumentations Task scores (RQ5), we can effectively evaluate
first and Control Cases last. (c) In the NLI task, LLMgeneratedtextandcloselymatchhumanjudg-
allmodelsaremisledbytheControlCases, with ment. We made a deliberate decision not to use
theLLM-trainedevaluatorprovingtobethemost a potentially contaminated LLM as an evaluator
resilientagainstcontrolcaseattacks. (Sainzetal.,2024),optinginsteadforadiscrimina-
Note that, depending on the task, each control tiveLMevaluatorthatweconfirmedhadnotbeen
case behaves differently. In MMCQA and NLI, exposedtothedatasetsusedandminimizingsome
the label-only control case is the most effective biases of generative LLM Judges (RQ1). To our
attacker,whileinthemisinformationtask,passage knowledge,wearethefirsttotestControlCases,
retrievalprovestobethestrongest. showing that baseline and expert-trained models
are often misled by adversarial attacks while the
Proxy Task Evaluators 🥇 🥈 🥉 4º 5º 6º 7º 8º LLM-trainedevaluatorremainsrobust(RQ2). The
typicalstate-of-the-artapproachesfocusonimprov-
Ev - Baseline
ingscoresonProxyTasksassumingexpert-trained
Ev - Expert-Trained
modelsasupperbound(Alonsoetal.,2024).
Ev - LLM-Trained
Our approach has been validated across three
(a)MMCQA differentdatasets,demonstratingitsrobustnessand
potentialforextensiontoothertaskswithminimal
Proxy Task Evaluators 🥇 🥈 🥉 4º 5º 6º 7º 8º
humanannotations(RQ3). Regardingthehuman
Ev - Baseline
rankings,fiveexamplesaresufficienttocompute
Ev - Expert-Trained ITA,showingthathumansarenotconsistentacross
Ev - LLM-Trained allProxyTasks. WhilethereisagreementforMM-
CQAandmisinformationtasks,thereisalackof
(b)MisinformationWithEvidence
consensusintheNLItask(cliniciansstruggletoex-
Proxy Task Evaluators 🥇 🥈 🥉 4º 5º 6º 7º tractaccurateargumentation),indicatingthatNLI
Ev - Baseline isnotsuitableforautomaticargumentationevalua-
tion(RQ4).
Ev - Expert-Trained
Ev - LLM-Trained
Gold Standard GPT-4o OpenBioLLM Llama3 6 Conclusions
IR Passages Label-Only Input Noise Argument No Argument
(c)NLI
In this work, we show that across three distinct
Figure4: Rankingofthegold-standardargument,LLM-
ProxyTaskscenarios,theautomaticevaluationof
generatedarguments,andControlCasesbytheProxy
medicalexplanatoryargumentscloselyalignswith
Task evaluators for each Proxy Task. Each row rep-
humanjudgment. BeyondstandardMCQAtasks,
resents a distinct evaluator, and the columns include
PrimaryArguments(goldstandardandLLM-generated) we broaden our scope to include Misinformation
as well as Control Cases (No Argument, Label-Only DetectionandNLI,providingamorecomprehen-
Input, NoiseArgument, andIRPassages). Thistable siveassessment. Wepresentanovelapproachthat
highlightstheevaluators’abilitytodifferentiatebetween moves beyond traditional score maximization to
properandimproperarguments.
prioritizeimprovedrankingcapabilities,addressing
theinherentbiasesinLLMswhenusedasjudges.
OurLLM-trainedevaluatoralignscloselywithhu-
5 Discussion
man preferences and demonstrates resilience to
Previousworkhasshownthateditdistance-based adversarial attacks. Remarkably, only one hand-
metrics and LLM-as-a-judge approaches do not labeledexamplepertaskisneededtogeneratethe
serveasreliableevaluators(Tanetal.,2024),high- synthetic arguments to develop the LLM-trained
lightingtheimportanceofcorrelatingresultswith evaluatorthatbestresembleshumancriteria. Ad-
humancriteria. Inourstudy,wetakethisastepfur- ditionally,wedemonstratethatjustfiveexamples
therbydemonstratingthatwithlesshand-labeled ranked by two human experts are enough to vali-
data(oneargumentationperProxyTask,3inthis datethechosenProxyTask,confirmingthepracti-
work) and a focus on rankings rather than Proxy calityofourevaluationmethod.
87 Limitations Pritam Deka, Anna Jurek-Loughrey, and P Deepak.
2022. Improved methods to aid unsupervised
Our approach has the next limitations. First, the evidence-basedfactcheckingforonlinehealthnews.
discriminativeLMmodelusedinthisstudyhasa JournalofDataIntelligence,3(4):474–504.
tokenlimitof512,whichmayrestrictthemodel’s
MiltonFriedman.1937. Theuseofrankstoavoidthe
abilitytofullyprocesslonger,morecomplexargu-
assumptionofnormalityimplicitintheanalysisof
ments. However, current advances in expanding variance. Journaloftheamericanstatisticalassocia-
language models’ context size will mitigate this tion,32(200):675–701.
constraint. Finally, we do not focus explicitly on
IakesGoenaga,AitziberAtutxa,KoldoGojenola,Maite
measuringhallucinations,factualaccuracy,orco- Oronoz,andRodrigoAgerri.2023. Explanatoryar-
herenceinthegeneratedarguments. gumentextractionofcorrectanswersinresidentmed-
icalexams. arXivpreprintarXiv:2312.00567.
Acknowledgments
Robin Jia and Percy Liang. 2017. Adversarial exam-
plesforevaluatingreadingcomprehensionsystems.
ThisworkhasbeenpartiallysupportedbytheHiTZ
In Proceedings of the 2017 Conference on Empiri-
Center and the Basque Government, Spain (Re-
calMethodsinNaturalLanguageProcessing,pages
search group funding IT1570-22). We are also 2021–2031,Copenhagen,Denmark.Associationfor
thankful to MCIN/AEI/10.13039/501100011033 ComputationalLinguistics.
projects: (i) Antidote (PCI2020-120717-2), and
JeffJohnson,MatthijsDouze,andHervéJégou.2019.
byEuropeanUnionNextGenerationEU/PRTR;(ii)
Billion-scale similarity search with GPUs. IEEE
DeepKnowledge(PID2021-127777OB-C21)and TransactionsonBigData,7(3):535–547.
ERDF A way of making Europe; (iii) EDHIA
Maël Jullien, Marco Valentino, Hannah Frost, Paul
(PID2022-136522OB-C22).
O’Regan,DónalLanders,andAndréFreitas.2023.
NLI4CT:multi-evidencenaturallanguageinference
forclinicaltrialreports. InEMNLP,pages16745–
References 16764.AssociationforComputationalLinguistics.
IñigoAlonso,MaiteOronoz,andRodrigoAgerri.2024.
Tom Kocmi and Christian Federmann. 2023. Large
Medexpqa: Multilingualbenchmarkingoflargelan-
language models are state-of-the-art evaluators of
guagemodelsformedicalquestionanswering. Artifi-
translationquality. arXivpreprintarXiv:2302.14520.
cialIntelligenceinMedicine,155:102938.
KlausKrippendorff.2011. Computingkrippendorff’s
Malaikannan Sankarasubbu Ankit Pal. 2024.
alpha-reliability.
Openbiollms: Advancing open-source large
language models for healthcare and life sci- Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen
ences. https://huggingface.co/aaditya/ Gu, and Chongyang Tao. 2024. Leveraging large
OpenBioLLM-Llama3-70B. languagemodelsfornlgevaluation: Asurvey. arXiv
preprintarXiv:2401.07103.
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,
Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Chin-YewLin.2004. Rouge: Apackageforautomatic
CunxiangWang,YidongWang,WeiYe,YueZhang, evaluation of summaries. In Text summarization
YiChang,PhilipS.Yu,QiangYang,andXingXie. branchesout,pages74–81.
2023. Asurveyonevaluationoflargelanguagemod-
els. Preprint,arXiv:2307.03109. MingxinLiu,TsuyoshiOkuhara,XinYiChang,Ritsuko
Shirabe,YurikoNishiie,HirokoOkada,andTakahiro
Zeming Chen, Alejandro Hernández-Cano, Angelika Kiuchi. 2024. Performance of chatgpt across dif-
Romanou,AntoineBonnet,KyleMatoba,Francesco ferent versions in medical licensing examinations
Salvi, Matteo Pagliardini, Simin Fan, Andreas worldwide: Systematicreviewandmeta-analysis. J
Köpf,AmirkeivanMohtashami,AlexandreSallinen, MedInternetRes,26:e60807.
AlirezaSakhaeirad,VinitraSwamy,IgorKrawczuk,
Deniz Bayazit, Axel Marmet, Syrielle Montariol, Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Mary-Anne Hartley, Martin Jaggi, and Antoine Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
Bosselut. 2023. Meditron-70b: Scaling medical NLGevaluationusinggpt-4withbetterhumanalign-
pretraining for large language models. Preprint, ment. In Proceedings of the 2023 Conference on
arXiv:2311.16079. EmpiricalMethodsinNaturalLanguageProcessing,
pages2511–2522,Singapore.AssociationforCom-
Iker De la Iglesia, Aitziber Atutxa, Koldo Gojenola, putationalLinguistics.
andAnderBarrena.2023. Eriberta: Abilingualpre-
trainedlanguagemodelforclinicalnaturallanguage AI Meta. 2024. Introducing meta llama 3: The most
processing. arXiv,2306.07373. capableopenlyavailablellmtodate. MetaAI.
9OpenAI.2024. Hellogpt-4o. https://openai.com/ Elior Sulem, Omri Abend, and Ari Rappoport. 2018.
index/hello-gpt-4o/. Bleuisnotsuitablefortheevaluationoftextsimplifi-
cation. arXivpreprintarXiv:1810.05995.
ArjunPanickssery,SamuelRBowman,andShiFeng.
2024. Llmevaluatorsrecognizeandfavortheirown Tianxiang Sun, Junliang He, Xipeng Qiu, and Xuan-
generations. arXivpreprintarXiv:2404.13076. JingHuang.2022. Bertscoreisunfair:Onsocialbias
inlanguagemodel-basedmetricsfortextgeneration.
KishorePapineni,SalimRoukos,ToddWard,andWei- In Proceedings of the 2022 Conference on Empiri-
JingZhu.2002. Bleu: amethodforautomaticevalu- calMethodsinNaturalLanguageProcessing,pages
ationofmachinetranslation. InProceedingsofthe 3726–3739.
40thannualmeetingoftheAssociationforComputa-
Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili
tionalLinguistics,pages311–318.
Liu, YunlongFeng, XiaoguangLi, YashengWang,
LifengShang,QunLiu,andLinqiSong.2024. Prox-
Alec Radford and Karthik Narasimhan. 2018. Im-
yQA:Analternativeframeworkforevaluatinglong-
provinglanguageunderstandingbygenerativepre-
formtextgenerationwithlargelanguagemodels. In
training.
Proceedingsofthe62ndAnnualMeetingoftheAs-
sociationforComputationalLinguistics(Volume1:
RicardoRei,CraigStewart,AnaCFarinha,andAlon
LongPapers),pages6806–6827,Bangkok,Thailand.
Lavie.2020. COMET:AneuralframeworkforMT
AssociationforComputationalLinguistics.
evaluation. InProceedingsofthe2020Conference
onEmpiricalMethodsinNaturalLanguageProcess-
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
ing(EMNLP),pages2685–2702,Online.Association
Martinet,Marie-AnneLachaux,TimothéeLacroix,
forComputationalLinguistics.
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
David L Sackett, William M C Rosenberg, J A Muir
cient foundation language models. arXiv preprint
Gray,RBrianHaynes,andWScottRichardson.1996.
arXiv:2302.13971.
Evidencebasedmedicine: whatitisandwhatitisn’t.
BMJ,312(7023):71–72. Shikhar Vashishth, Denis Newman-Griffis, Rishabh
Joshi, RitamDutt, andCarolynP.Rosé.2021. Im-
Oscar Sainz, Iker García Ferrero, Eneko Agirre, Jon provingbroad-coveragemedicalentitylinkingwith
AnderCampos,AlonJacovi,YanaiElazar,andYoav semantictypepredictionandlarge-scaledatasets. J.
Goldberg,editors.2024. Proceedingsofthe1stWork- Biomed.Informatics,121:103880.
shoponDataContamination(CONDA).Association
forComputationalLinguistics,Bangkok,Thailand. JurajVladika,PhillipSchneider,andFlorianMatthes.
2024. HealthFC: Verifying health claims with
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, evidence-basedmedicalfact-checking. InProceed-
Christopher Potts, and Matei Zaharia. 2022. Col- ings of the 2024 Joint International Conference
bertv2: Effective and efficient retrieval via onComputationalLinguistics,LanguageResources
lightweightlateinteraction. InNAACL-HLT,pages andEvaluation(LREC-COLING2024),pages8095–
3715–3734.AssociationforComputationalLinguis- 8107,Torino,Italy.ELRAandICCL.
tics.
Bingsheng Yao, Prithviraj Sen, Lucian Popa, James
Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Hendler,andDakuoWang.2023. Arehumanexpla-
2020. Bleurt: Learningrobustmetricsfortextgener- nationsalwayshelpful? towardsobjectiveevaluation
ation. InProceedingsofACL. ofhumannaturallanguageexplanations. InProceed-
ingsofthe61stAnnualMeetingoftheAssociationfor
ChenhuiShen,LiyingCheng,Xuan-PhiNguyen,Yang ComputationalLinguistics(Volume1: LongPapers),
You,andLidongBing.2023. Largelanguagemodels pages14698–14713.
are not yet human-level evaluators for abstractive
TianyiZhang*,VarshaKishore*,FelixWu*,KilianQ.
summarization. arXivpreprintarXiv:2305.13091.
Weinberger,andYoavArtzi.2020. Bertscore: Eval-
uating text generation with bert. In International
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,
ConferenceonLearningRepresentations.
Ellery Wulczyn, Le Hou, Kevin Clark, Stephen
Pfohl, Heather Cole-Lewis, Darlene Neal, et al.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
2023. Towards expert-level medical question an-
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
sweringwithlargelanguagemodels. arXivpreprint
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
arXiv:2305.09617.
Judging llm-as-a-judge with mt-bench and chatbot
arena. AdvancesinNeuralInformationProcessing
EricStrong,AliciaDiGiammarino,YingjieWeng,An-
Systems,36.
dre Kumar, Poonam Hosamani, Jason Hom, and
Jonathan H. Chen. 2023. Chatbot vs Medical Stu-
dent Performance on Free-Response Clinical Rea-
soning Examinations. JAMA Internal Medicine,
183(9):1028–1030.
10A AutomaticEvaluator’sInpunts
EVALUATORS INPUTS
QA MissinformationDetection NLI
Question
Statement
ClinicalCase Question
NaiveEvaluator FullSection
PossibleAnswers Label
Label
CorrectAnswer
Question
ClinicalCase Question Statement
ClinicianLinedUpEvaluator PossibleAnswers GoldArgumentation GoldEvidences
GoldArgumentation Label Label
CorrectAnswer
Question
ClinicalCase Question Statement
LLMsLinedUpEvaluators PossibleAnswers LLMsArgumentation LLMsEvidences
LLMsArgumentation Label Label
CorrectAnswer
Table2: Thistableincludestheinputsusedforeachautomaticevaluatordependingontheproxytask.
B InstructionTuningExample
InstructionTuningExampleUsedForQA
Instruction <s>[INST]A52-year-oldmanwithnoconcomitantdiseasescomestotheemergencydepartmentformelenaof24
hoursevolutionwithouthemodynamicrepercussions.Hedeniestakingnonsteroidalanti-inflammatorydrugs.The
hematocritis33%andtherestofthelaboratorytestsarenormal.Theuppergastrointestinalendoscopyperformed
urgently6hoursafteradmissionshowsanormalstomach,withoutbloodorhematicdebrisandanexcavatedulcer
of8mmindiameterintheanteriorfaceoftheduodenalbulbwith"visiblevessel"atitsbaseandwithoutactive
bleeding.Whichofthefollowingstatementsistrue?Thepossibleanswersare:1-Intheinitialendoscopyitis
indicatedtoapplyanendoscopictherapyandsubsequentlytoestablishendovenoustreatmentwithhighdosesofa
protonpumpinhibitor.Thisstrategyhasbeenshowntoreducetheriskofhemorrhagicrecurrenceandmortality.2-
Intheinitialendoscopy,giventheabsenceofactivebleeding,endoscopictherapyisnotindicated.Subsequently,
toreducetheriskofhemorrhagicrecurrence,intravenoustreatmentwithhighdosesofaprotonpumpinhibitor
shouldbestarted.3-ninitialendoscopy,endoscopictherapyisindicated.Subsequenttreatmentwithhighdoses
ofaprotonpumpinhibitorhasnotbeenshowntobeofanyadditionalbenefit.4-Sincethisisacomplicated
ulcer(hemorrhage)thebesttherapeuticoption,oncethehemorrhagicepisodeisresolved,isavagotomyand
pyloroplasty.[/INST]
Output ThisisagradeIIaulcer(Forrestclassification),withahighriskofrecurrence.Therefore,endoscopictreatment
andhospitalizationwithintravenoustreatmentwithPPIs(itisusuallyomeprazole)foratleast72hoursisclearly
indicated.</s>
Table3: Thisisanexampleofaninstanceusedtofine-tuneLlama3forQAproxytaskapplyingIT.
11C BenchmarksExamples
ExampleofaninstancefromNLIBenchmark
Statement Therewere7morecasesofAnaemiaand1morecaseofDisseminatedintravascularcoagulationincohort1of
theprimarytrialcomparedtocohort2.
GoldEvidences AdverseEvents1: **Total: 158/482(32.78%)**Anaemia7/482(1.45%)**Disseminatedintravascular
coagulation1/482(0.21%)**AdverseEvents2: **Total: 37/238(15.55%)**Anaemia2/238(0.84%)**
Disseminatedintravascularcoagulation0/238(0.00%)
FullDocument INTERVENTION1:**Everolimus+Exemestane**Everolimus10mgdailyincombinationwithexemestane
25mgdaily**INTERVENTION2:**Placebo+Exemestane**Placeboofeverolimusincombinationwith
exemestane25mgdaily**InclusionCriteria:**Adultwomen(18yearsofage)withmetastaticorlocally
advancedbreastcancernotamenabletocurativetreatmentbysurgeryorradiotherapy. **Histologicalor
cytologicalconfirmationofestrogen-receptorpositive(ER+)breastcancer**Postmenopausalwomen.**Disease
refractorytononsteroidalaromataseinhibitors(NSAI),**Radiologicalorclinicalevidenceofrecurrenceor
progressiononorafterthelastsystemictherapypriortorandomization. **Patientsmusthaveatleastone
lesionthatcanbeaccuratelymeasuredorbonelesionsintheabsenceofmeasurablediseaseasdefinedabove.**
ExclusionCriteria:**HER2-overexpressingpatients**Patientswithonlynon-measurablelesionsotherthan
bonemetastasis(e.g.pleuraleffusion,ascitesetc.).**Patientswhoreceivedmorethanonechemotherapylinefor
AdvancedBreastCancer.**PrevioustreatmentwithexemestaneormTORinhibitors.**Knownhypersensitivity
tomTORinhibitors,e.g.sirolimus(rapamycin).**Radiotherapywithinfourweekspriortorandomization**
Currentlyreceivinghormonereplacementtherapy,**Otherprotocol-definedinclusion/exclusioncriteriamay
apply**OutcomeMeasurement: **Progression-freeSurvival(PFS)BasedonLocalRadiologyReviewof
TumorAssessments.**Progression-freesurvival,theprimaryendpointinthisstudy,isdefinedasthetimefrom
thedateofrandomizationtothedateoffirstdocumentedradiologicalprogressionordeathduetoanycause.
DiseaseprogressionwasbasedonthetumorassessmentbythelocalradiologistorinvestigatorusingRECIST1.0
criteria.Ifapatientdidnotprogressorknowntohavediedatthedateoftheanalysiscut-offorstartofanother
antineoplastictherapy,thePFSdatewascensoredtothedateoflastadequatetumorassessmentpriortocut-off
dateorstartofantineoplastictherapy.Forpatientswithlyticormixed(lytic+sclerotic)bonelesions,thefollowing
isconsideredprogression:appearanceof1newlyticlesionsinbone;theappearanceofnewlesionsoutsideof
boneandunequivocalprogressionofexistingbonelesions.**Timeframe:dateofrandomizationtothedateof
firstdocumentedtumorprogressionordeathfromanycause,whicheveroccursfirst,reportedbetweendayof
firstpatientrandomizeduptoabout19months**Results1:**Arm/GroupTitle:Everolimus+Exemestane
**Arm/GroupDescription:Everolimus10mgdailyincombinationwithexemestane25mgdaily**Overall
NumberofParticipantsAnalyzed:485**Median(95%ConfidenceInterval)**UnitofMeasure:months6.93
(6.44to8.05)**Results2:**Arm/GroupTitle:Placebo+Exemestane**Arm/GroupDescription:Placebo
ofeverolimusincombinationwithexemestane25mgdaily**OverallNumberofParticipantsAnalyzed:239
**Median(95%ConfidenceInterval)**UnitofMeasure:months2.83(2.76to4.14)**AdverseEvents1:**
Total:158/482(32.78%)**Anaemia7/482(1.45%)**Disseminatedintravascularcoagulation1/482(0.21%)
**Lymphadenopathy0/482(0.00%)**Neutropenia0/482(0.00%)**Thrombocytopenia2/482(0.41%)**
Anaemia28/482(1.66%)**Disseminatedintravascularcoagulation21/482(0.21%)**Febrileneutropenia
21/482(0.21%)**Lymphadenopathy20/482(0.00%)**Neutropenia20/482(0.00%)**AdverseEvents2:**
Total:37/238(15.55%)**Anaemia2/238(0.84%)**Disseminatedintravascularcoagulation0/238(0.00%)
**Lymphadenopathy1/238(0.42%)**Neutropenia1/238(0.42%)**Thrombocytopenia0/238(0.00%)**
Anaemia22/238(0.84%)**Disseminatedintravascularcoagulation20/238(0.00%)**Febrileneutropenia
21/238(0.42%)**Lymphadenopathy21/238(0.42%)**Neutropenia21/238(0.42%)
FullSection AdverseEvents1:**Total:158/482(32.78%)**Anaemia7/482(1.45%)**Disseminatedintravascularcoagu-
lation1/482(0.21%)**Lymphadenopathy0/482(0.00%)**Neutropenia0/482(0.00%)**Thrombocytopenia
2/482(0.41%)**Anaemia28/482(1.66%)**Disseminatedintravascularcoagulation21/482(0.21%)**Febrile
neutropenia21/482(0.21%)**Lymphadenopathy20/482(0.00%)**Neutropenia20/482(0.00%)**Adverse
Events2:**Total:37/238(15.55%)**Anaemia2/238(0.84%)**Disseminatedintravascularcoagulation0/238
(0.00%)**Lymphadenopathy1/238(0.42%)**Neutropenia1/238(0.42%)**Thrombocytopenia0/238(0.00%)
**Anaemia22/238(0.84%)**Disseminatedintravascularcoagulation20/238(0.00%)**Febrileneutropenia
21/238(0.42%)**Lymphadenopathy21/238(0.42%)**Neutropenia21/238(0.42%)
Label Entailment
Table4: AninstanceexamplefromNLIBenchmark.
ExampleofaninstancefromMissinformationDetectionBenchmark
Question Canfilteringoutbluelightusingbluefilterglassesornightmodesettingsonsmartphone,tabletorlaptopscreens
haveabeneficialeffectonsleep?
GoldArgumentation Inpreviousstudies,itmakesnonoticeabledifferencetosleepwhenthebluelightcomponentofdisplayscreen
devicesisfilteredoutintheevening.However,theresultsarenotwellvalidatedbecausethestudiesareoflow
qualityandusuallyonlyexaminedafewpeople.
Label Refuted
Table5: AninstanceexamplefromMissinformationDetectionBenchmark.
12ExampleofadocumentfromthePreprocessedAntidoteCasiMedicosDataset
C A45-year-oldmanundergoesatruncalvagotomyandantrectomywithBillrothIIreconstructionforchronic
pepticulcerdiseasewithpyloro-duodenalstricture.Sixweeksafterthesurgeryshereportsthatshortlyafter(less
thanhalfanhour)afteringestionsshepresentsnausea,astheniaandsweating,dizzinessandabdominalcramps
usuallyaccompaniedbydiarrhea.
Q Whichofthefollowingisthemostappropriateapproachforherinitialmanagement?
(1)Applytreatmentwithasomatostatininhibitor(octreotide).
(2)Followspecificdietarymeasures.
P (3)Trialtreatmentwithabenzodiazepine.
(4)Searchforaprobableneuroendocrinetumor(e.g.carcinoid).
(5)IndicatesurgicaltreatmenttoperformanantiperistalticRoux-en-Ygastrojejunostomy.
E Answers1,2and5areappropriatetreatmentsfordumpingsyndromeorpostgastrectomy,butthequestionis
focusedoninitialmanagement,sothemostappropriateanswerseemstobe2.
NE Applyingtreatmentwithasomatostatininhibitor(octreotide),followingspecificdietarymeasuresandindicating
surgicaltreatmenttoperformanantiperistalticRoux-en-Ygastrojejunostomyareappropriatetreatmentsfor
dumpingsyndromeorpostgastrectomy,butthequestionisfocusedoninitialmanagement,sothemostappropriate
approachseemstobefollowingspecificdietarymeasures.
Table6: ExampleofadocumentinthePreprocessedAntidoteCasiMedicosdatasetwiththeexplanationabout
thecorrectanswermanuallyneutralized. C:ClinicalCase;Q:Question;P:PossibleAnswers;E:CorrectAnswer
Explanation. TheClinicalCase,Question,PossibleAnswers,CorrectAnswerExplanationsectionsaretheoriginal
annotationsoftheAntidoteCasiMedicosdataset. Thepreprocessingofthemedicaldoctors’explanations(NE)is
partofthiswork.
D PromptsForMedicalArgumentationGeneration
PromptusedtogeneratemedicalargumentationforQA
"role":"system","content": Youareamedicalstudentandgivenamedicalcase,aquestionandfivepossibleanswers,tellmewhichisthe
correctanswerandargumentinfavorofit.
Example:
Amedicalcaseandaquestionrelatedtoit<casequestion>Afteratrafficaccidenta38-year-oldpatientis
admittedtotheICUincoma.AfterseveraldaysthepatientdoesnotimproveneurologicallyandaCTscanshows
hemorrhagicpunctatelesionsinthecorpuscallosumandcortico-subcorticaljunction.Whatisthediagnosis?
<\casequestion>
Andfivepossibleanswers:
<ans>1-Acutesubduralhematoma.<\ans>
<ans>2-Trobocytopenicpurpura.<\ans>
<ans>3-Cerebralhemorrhagiccontusion.<\ans>
<ans>4-Severediffuseaxonalinjury.<\ans>
<ans>5-Acuteheartattack.<\ans>
Theargumentforthecorrectanswerwithoutmentioningtheoptionsandfocusingexclusivelyontheargumentsis:
Diffuseaxonalinjuryproducesanearlyandsustaineddeteriorationofthelevelofconsciousness(asmentionedin
thecasestatement)withoutalesiononCTscantojustifythepicture.Sometimes,punctatehemorrhagesatthe
levelofthecorpuscallosum,corticosubcorticaljunctionanddorsolateralportionofthebrainstemareevidenced
inthisimagingtest.
"role":"user","content": Giventhisnewcaseandthequestionrelatedtoit:
<casequestion>{case_question} <\casequestion>
Andfivepossibleanswers:
<ans>{ans1}<\ans>
<ans>{ans2}<\ans>
<ans>{ans3}<\ans>
<ans>{ans4}<\ans>
<ans>{ans5}<\ans>
Theargumentforthecorrectanswerwithoutmentioningtheoptionsandfocusingexclusivelyontheargumentsis:
Table7: ThisisthepromptweusedtogeneratemedicalargumentationforQA,where{case_question}isanew
clinicalcaseandaquestionrelatedtoitfromthedataset, and{ans1-5}arethepossibleansweroptionsforthe
question. ThesameprompthasbeenusedonGPT-4o,OpenBioLLMandLlama3.
13PromptusedtogeneratemedicalargumentationforMissinformationDetection
"role":"system","content": Youareamedicalstudent.Givenamedicalquestion,youmustanswerthequestionandincludethearguments
youusetoreachyouranswer.
Example:
Aquestion<question>Cantakingtheenzymediaminooxidasepreventalcohol-relatedhangoversymptoms?
<\question>
Theargumentforthecorrectanswerandfocusingexclusivelyontheargumentsis:
Suchaneffectisnotlikely,nordoclinicalstudiesexistonthisissue.
"role":"user","content": Giventhisnewquestion:
<question>{question} <\question>
Theargumentforthecorrectanswerandfocusingexclusivelyontheargumentsis:
Table 8: This is the prompt we used to generate medical argumentation for Missinformation Detection, where
{question}isanewquestionfromthedataset. ThesameprompthasbeenusedonGPT-4o, OpenBioLLMand
Llama3.
PromptusedtoextractmedicalargumentationforNLI
"role":"system","content": Youareamedicalstudent.Givenamedicalhypothesisandevidencesseparatedby**,extracttheevidencesthat
supportsorcontradictsthehypothesiswithoutaddinganyotherwords.Remember,donotgenerateanynewtext.
Extractonlytherelevantpartsexactlyastheyappearinthegiventext.
Example:
Ahypothesis<hypothesis>Patientswithsignificantlyelevatedejectionfractionareexcludedfromtheprimary
trial,butcanstillbeeligibleforthesecondarytrialiftheyare55yearsofageorover.<\hypothesis>
A list of possible evidences <evidences> Inclusion criteria: ** Inclusion Criteria: ** Female patients
age18yearsorolder**Histologicallyprovenbreastcancerafterfailureorrelapseofnomorethanthree
linesofchemotherapyincludingadjuvant, irrespectiveofpriorhormonetherapymetastaticdisease(stage
IV);**HER2-negativepatients(HER21+ornegative, orHER22+andFISHnegative)**Atleastone
measurabletumourlesion(RECIST);**Exclusioncriteria:**ExclusionCriteria:**Activeinfectiousdisease
**Gastrointestinaldisordersthatmayinterferewiththeabsorptionofthestudydrugorchronicdiarrhoea**
Seriousillness,concomitantnon-oncologicaldiseaseormentalproblemsconsideredbytheinvestigatortobe
incompatiblewiththeprotocol**Active/symptomaticbrainmetastases**Cardiacleftventricularfunctionwith
restingejectionfraction<50%(belowupperlimitofnormal)**ANClessthan1500/mm3plateletcountlessthan
100000/mm3**Bilirubingreaterthan1.5mg/dl(>26and61549mol/L,SIunitequivalent)**ASTandALT
greaterthan2.5timestheupperlimitofnormalorgreater5timestheupperlimitofnormalincaseofknownliver
metastases**Serumcreatininegreaterthan1.5mg/dl(>132and61549mol/L,SIunitequivalent)**Patients
whoaresexuallyactiveandunwillingtouseamedicallyacceptablemethodofcontraception**Pregnancyor
breast-feeding**Concomitanttreatmentwithotherinvestigationaldrugsorotheranti-cancer-therapyduringthis
studyand/orduringthepasttwo/fourweeks,priortothefirsttreatmentwiththetrialdrug.Concurrenttreatment
withbiphosphonatesisallowed**Previoustreatmentwithtrastuzumab,EGFR-,orEGFR/HER2-inhibitors
patientsunabletocomplywiththeprotocol**Activealcoholordrugabuse**Othermalignancywithinthepast
5years’’Premenopausalwomen55yearsofageoryoungerwithregularmenstrualcycles(atleastfourcyclesin
thelastsixmonths).Womenwithfewerthan4mensesinthelast6monthsorwhohavehadahysterectomy
withovariesintactwillbeconsideredpremenopausalifFSHlevel<20. **Womenwithbreastdensity25%
(scatteredfibroglandulardensitiesorgreater)areeligible. **PriorTreatment**Patientswhoarecurrently
receivinghormonereplacementtherapy(estrogenorprogesterone);oraretakingtamoxifenorraloxifenearenot
eligible.Womenwhohavetakenthesemedicationsmusthavestoppedforatleast4monthspriortostudyentry.
**Topicalestrogen(eg,transdermalpatchesandvaginalestrogens)isallowed.**Patientswithadiagnosisof
osteoporosiswithphysicianrecommendationfortreatmentoflowbonemassarenoteligible.**Patientsknown
tohavehyperparathyroiddiseaseorotherseriousdisturbancesofcalciummetabolismrequiringinterventionin
thepast5yearsarenoteligible.**Patientswithahistoryofkidneystones(unlessdocumentednottohavebeena
calciumstone)arenoteligible.**Patientsparticipatinginaconcurrentbreastcancerchemopreventiontrialare
noteligible.**Requiredinitiallaboratoryvalues-Calcium<10.5mg/dL’<\evidences>
Theevidencesthatsupportsorcontradictsthehypothesiswithoutaddinganyotherwordsare:
Cardiacleftventricularfunctionwithrestingejectionfraction<50%(belowupperlimitofnormal). **
Premenopausalwomen55yearsofageoryoungerwithregularmenstrualcycles(atleastfourcyclesinthelast
sixmonths).
"role":"user","content": Giventhisnewhypothesis:
<hypothesis>{statement} <\hypothesis>
Andgiventhisnewlistofpossibleevidences<evidences>{evidences} <\evidences>
Theevidencesthatsupportsorcontradictsthehypothesiswithoutaddinganyotherwordsare:
Table9:ThisisthepromptweusedtoextractmedicalargumentationforNLI,where{statement}isanewhypothesis
fromthedatasetand{evidences}isanewlistofevidencesfromthedatasetrelatedtothehypothesis. Thesame
prompthasbeenusedonGPT-4o,OpenBioLLMandLlama3.
E ExperimentResults
14EVALUATORS\TESTS NoArgument Gold Noise Correct IR GPT4 OpenBioLLM Llama3
Ev-Baseline 40.61 37.25 41.17 40.62 35.01 34.73 36.97 35.01
Ev-ExpertTrained 37.81 77.59 31.37 39.66 38.38 82.91 64.99 65.83
Ev-LLMTrained 34.45 72.64 33.79 35.89 38.39 78.90 66.85 64.98
Table10: ThistableincludestheaccuracyobtainedbythedifferentautomaticevaluatorsonselectedtestsinQA
proxytask. Bestresultforeachevaluatorinbold.
EVALUATORS\TESTS NoArgument Gold Noise Correct IR GPT4 OpenBioLLM Llama3
Ev-Baseline 35.37 40.81 30.61 39.45 37.41 44.89 48.30 44.21
Ev-ExpertTrained 46.25 53.06 13.60 0.68 56.46 59.86 61.22 60.54
Ev-LLMTrained 18.02 25.85 11.56 5.21 38.09 42.85 40.81 49.43
Table 11: This table includes the accuracy obtained by the different automatic evaluators on selected tests in
MissinformationDetectionproxytask. Bestresultforeachevaluatorinbold.
EVALUATORS\TESTS NoArgument Gold Noise Correct GPT4 OpenBioLLM Llama3
Ev-Baseline 60.31 60.46 60.80 62.33 60.38 60.63 61.50
Ev-ExpertTrained 65.10 67.62 60.61 64.58 61.89 61.35 63.06
Ev-LLMTrained 54.32 57.34 56.15 61.12 58.21 54.73 60.58
Table12: ThistableincludesthemicroF-scoreobtainedbythedifferentautomaticevaluatorsonselectedtestsin
NLIproxytask. Bestresultforeachevaluatorinbold.
15