Upper and Lower Bounds for Distributionally Robust
Off-Dynamics Reinforcement Learning
Zhishuai Liu∗‡ Weixin Wang†‡ Pan Xu§
Abstract
We study off-dynamics Reinforcement Learning (RL), where the policy training and de-
ployment environments are different. To deal with this environmental perturbation, we focus
on learning policies robust to uncertainties in transition dynamics under the framework of
distributionally robust Markov decision processes (DRMDPs), where the nominal and perturbed
dynamics are linear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U
√
(cid:0) (cid:1)
that enjoys an average suboptimality O(cid:101) dH ·min{1/ρ,H}/ K , where K is the number of
episodes, H is the horizon length, d is the feature dimension and ρ is the uncertainty level. This
result improves the state-of-the-art by O(dH/min{1/ρ,H}). We also construct a novel hard
instanceandderivethefirstinformation-theoreticlowerboundinthissetting,whichindicatesour
√
algorithm is near-optimal up to O( H) for any uncertainty level ρ∈(0,1]. Our algorithm also
enjoys a ‘rare-switching’ design, and thus only requires O(dHlog(1+H2K)) policy switches and
O(d2Hlog(1+H2K)) calls for oracle to solve dual optimization problems, which significantly
improves the computational efficiency of existing algorithms for DRMDPs, whose policy switch
and oracle complexities are both O(K).
1 Introduction
In dynamic decision-making and reinforcement learning (RL), Markov decision processes (MDPs)
offer a well-established framework for understanding complex systems and guiding agent behavior
(Sutton and Barto, 2018). However, MDPs encounter significant challenges in practical applications
duetoincompleteknowledgeofmodelparameters,especiallytransitionprobabilities. Thissim-to-real
gap, representing the difference between training and testing environments, can lead to failures in
fields like infectious disease control and robotics (Farebrother et al., 2018; Zhao et al., 2020; Laber
et al.,2018; Liu et al., 2023; Penget al., 2018). To addressthese challenges, off-dynamics RL provides
a framework where policies are trained on a source domain and deployed to a distinct target domain,
promoting robust performance across varying environments (Eysenbach et al., 2020; Jiang et al.,
2021). Within this framework, distributionally robust Markov decision processes (DRMDPs) have
emerged as a promising way to model transition uncertainty. DRMDPs focus on learning robust
policies that perform well under worst-case scenarios (Nilim and El Ghaoui, 2005; Iyengar, 2005).
Prior works (Zhang et al., 2021a; Yang et al., 2022; Panaganti et al., 2022; Shi and Chi, 2022; Yang
∗Duke University; email: zhishuai.liu@duke.edu
†Duke University; email: weixin.wang@duke.edu
‡Equal contribution
§Duke University; email: pan.xu@duke.edu
1
4202
peS
03
]GL.sc[
1v12502.9042:viXraet al., 2023b; Shen et al., 2024) have proposed algorithms mainly for tabular DRMDP settings, where
the number of states and actions is finite, which are infeasible in large state and action spaces.
Inenvironmentscharacterizedbylargestateandactionspaces,functionapproximationtechniques
are crucial to overcome the computational burden posed by high dimensionality. Linear function
approximationmethods, basedonrelativelysimplefunctionclasses, haveshownsignificanttheoretical
and practical successes in standard MDP environments (Jin et al., 2020; He et al., 2021, 2023; Yang
and Wang, 2020; Hsu et al., 2024). However, their application in DRMDPs introduces additional
complexities. These complexities arise from the nonlinearity caused by the dual formulation in the
worst-case analysis, even when the transition dynamics in the source domain are modeled as linear.
Recently, Liu and Xu (2024a) provided the first theoretical results in the online setting of
d-rectangular linear DRMDPs, a specific type of DRMDPs where the nominal model is a linear MDP
(Jin et al., 2020) and the uncertainty set is defined based on the linear structure of the nominal
transition kernel. Apart from this, online DRMDP with linear function approximation is largely
underexplored and it is not clear how far existing algorithms are from optimal. Consequently, two
natural questions arise:
Can we improve the current results for online DRMDPs with linear function approximation?
What is the fundamental limit in this setting?
In this paper, we provide an affirmative answer to the first question and answer the second question
by providing an information theoretic lower bound for the online setting of d-rectangular linear
DRMDPs. In particular, motivated by the adoption of variance-weighted ridge regression to achieve
nearly optimal result in standard linear MDPs (Zhou et al., 2021a; Zhou and Gu, 2022; Zhang
et al., 2021b; Kim et al., 2022; Zhao et al., 2023; He et al., 2023; Hu et al., 2023), we propose a
variance-aware distributionally robust algorithm to solve the off-dynamics RL problem. Due to the
nonlinearity caused by the dual optimization of DRMDPs, the adoption of variance information
in linear DRMDPs is highly nontrivial. The only existing algorithm that incorporates variance
information in learning linear DRMDPs requires coverage assumptions on the offline dataset (Liu
and Xu, 2024b), which is infeasible in our setting where the algorithm needs to interact with the
environment in an online fashion. Therefore, our work poses a distinct algorithm design and calls for
different theoretical analysis techniques. Specifically, our main contributions are summarized as
follows:
• Weproposeanovelalgorithm,We-DRIVE-U,ford-rectangularlinearDRMDPswithtotal-variation
(TV) divergence uncertainty sets. We-DRIVE-U is designed based on the optimistic principle (Jin
et al., 2018, 2020; He et al., 2023) to trade off the exploration and exploitation during interacting
with the source environment to learn a robust policy. The key idea of We-DRIVE-U lies in
incorporating the variance information into the policy learning. In particular, a carefully designed
optimistic estimator of the variance of the optimal robust value function is established at each
episode, which will be used in variance-weighted regressions under a novel ‘rare-switching’ regime
to update the robust policy estimation.
√
• We prove that We-DRIVE-U achieves O(cid:101)(dH ·min{1/ρ,H}/ K) average suboptimality when the
number of episode K is large, which improves the state-of-the-art result (Liu and Xu, 2024a) by
O(cid:101)(dH/min{1/ρ,H}), We highlight that the average suboptimality of We-DRIVE-U demonstrates
the ‘Range Shrinkage’ property (refer to Lemma F.10) through the term min{1/ρ,H}. We further
√
established an information-theoretic lower bound Ω(dH1/2·min{1/ρ,H}/ K), which shows that
√
We-DRIVE-U is near-optimal up to O( H) for any uncertainty level ρ ∈ (0,1].
2• We-DRIVE-U is favorable in applications where policy switching is risky or costly, since We-
DRIVE-U achieves O(dHlog(1+H2K)) global policy switch (refer to Definition 5.7). Moreover,
we note that calls for oracle to solve dual optimizations (4.3) are one of the main sources of
computationcomplexityinDRMDP withlinearfunctionapproximation. Thankstothespecifically
designed ‘rare-switching’ regime, We-DRIVE-U achieves O(d2Hlog(1+H2K)) oracle complexity
(refer to Definition 5.8). Both results improve exiting online DRMDP algorithms by a factor of K.
Thus, We-DRIVE-U enjoys low switching cost and low computation cost.
Novelty in Algorithm and Hard Instance Design The variance estimator and the variance-
weighted ridge regression in We-DRIVE-U lead to two major improvements on the average subopti-
mality compared to the previous result: 1) the incorporation of variance information enables us to
leverage the recently discovered ‘Range Shrinkage’ property for linear DRMDPs, which is crucial
in achieving the tighter dependence on the horizon length H; 2) inspired by previous works on
standard MDPs (Azar et al., 2017; He et al., 2023), we design a new ‘rare-switching’ regime (refer to
Remark 5.10) and monotonic robust value function estimation (refer to Remark 4.3). Together with
the optimistic variance estimator, we achieve the tight dependence on d. As for the lower bound, we
construct a novel family of hard-to-learn linear DRMDPs, showing the ‘Range Shrinkage’ property
on robust value functions for any policy π.
Technical Challenges The incorporation of variance information poses unique challenges to
our theoretical analysis. In particular, in order to get the near-optimal upper bound on average
suboptimality, we need to bound the variance-weighted version of the d-rectangular estimation error
(see (5.1) for more details), instead of the vanilla one in (5.1) of Liu and Xu (2024a). However, this
term is in general intractable through direct matrix analysis. To solve this challenge, we seek to
convert the variance-weighted d-rectangular estimation error to the vanilla version, which requires
a precise upper bound on the variance estimator. Intuitively, the variance estimator should be
close to the true variance when the ‘sample size’ k is large. While different from the recent study
(Liu and Xu, 2024b) on the offline linear DRMDP, our variance estimator is an optimistic one and
thus cannot be trivially upper bounded by the true variance. To this end, we carefully analyze the
error of the variance estimator in the large k regime, and meticulous calculation shows that the
optimistic variance estimator can be upper bounded by a clipped version of the true variance (refer
to Lemma B.7).
Notations For any positive integer H ∈ Z , we denote [H] = {1,2,··· ,H}. For any set S,
+
define ∆(S) as the set of probability distributions over S. For any function V : S → R, define
[P V](s,a) = E [V(s′)], and [V(s)] = min{V(s),α}, where α > 0 is a constant. For a
h s′∼P (·|s,a) α
h
vector x, define x as its j-th entry. Moreover, denote [x ] as a vector with the i-th entry being
j i i∈[d]
x . For a matrix A, denote λ (A) as the i-th eigenvalue of A. For two matrices A and B, denote
i i
A ⪯ B as the fact that B −A is a positive semi-definite matrix. For any P,Q ∈ ∆(S), the total
variation divergence of P and Q is defined as D(P||Q) = 1/2(cid:82) |P(s)−Q(s)|ds.
S
2 Related Work
Distributionally Robust MDPs There has been a large body of works studying DRMDPs
under various settings, for instance, the setting of planning and control (Xu and Mannor, 2006;
3Wiesemann et al., 2013; Yu and Xu, 2015; Mannor et al., 2016; Goyal and Grand-Clement, 2023)
where the exact transition model is known, the setting with a generative model (Zhou et al., 2021b;
Yang et al., 2022; Panaganti and Kalathil, 2022; Xu et al., 2023; Shi et al., 2023; Yang et al., 2023a),
the offline setting (Panaganti et al., 2022; Shi and Chi, 2022; Blanchet et al., 2023) and the online
setting (Dong et al., 2022; Liu and Xu, 2024a; Lu et al., 2024). Among tabular DRMDPs, the most
relevant studies to ours are Shi et al. (2023); Lu et al. (2024). In particular, Shi et al. (2023) studies
tabular DRMDPs with TV uncertainty sets. They provide an information-theoretic lower bound,
as well as a matching upper bound on the sample complexity. The key message is that the sample
complexity bounds depend on the uncertainty level, and when the uncertainty level is of constant
order, policy learning in a DRMDP requires less samples than in a standard MDP. Further, Lu et al.
(2024) studies the online tabular DRMDPs with TV uncertainty sets, they provide an algorithm
that achieves the near-optimal sample complexity under a vanishing minimal value assumption to
circumvent the curse of support shift.
Online Linear MDPs and Linear DRMDPs The nominal model studied in our paper is
assumed to be a linear MDP with a simplex feature space. There is a line of works studying
online linear MDPs (Yang and Wang, 2020; Jin et al., 2020; Modi et al., 2020; Zanette et al., 2020;
Wang et al., 2020a; He et al., 2021; Wagenmaker et al., 2022; Ishfaq et al., 2023), and the minimax
optimality of this setting is studied in the recent work of He et al. (2023). In particular, they adopt
the variance-weighted ridge regression scheme and the ‘rare-switching’ policy update strategy in their
algorithm design. The setting of online linear DRMDP is relatively understudied, with both the lower
bound and the near-optimal upper bound remain elusive. Specifically, the only work studies the
online linear DRMDP setting is Liu and Xu (2024a). Under the TV uncertainty set, their algorithm,
√
DR-LSVI-UCB, achieves an average suboptimality of the order O˜(d2H2/ K). However, recent
evidence from studies (Liu and Xu, 2024b; Wang et al., 2024) on offline linear DRMDPs suggests
that this rate is far from optimality. In particular, Liu and Xu (2024b) proves that their algorithm,
√
VA-DRPVI, achieves an upper bound on the suboptimality in the order of O˜(dHmin{1/ρ,H}/ K).
Nonetheless, their algorithm and analysis are based on a pre-collected offline dataset which satisfies
some coverage assumption, and thus cannot be utilized in the online setting, where a strategy on
data collection is required to deal with the challenge of exploration and exploitation trade-off.
3 Preliminary
We use a tuple DRMDP(S,A,H,Uρ(P0),r) to denote a finite horizon distributionally robust Markov
decision process (DRMDP), where S and A are the state and action spaces, H ∈ Z is the horizon
+
length, P0 = {P0}H is the nominal transition kernel, Uρ(P0) = (cid:78) Uρ(P0) denotes an
h h=1 h∈[H] h h
uncertainty set centered around the nominal transition kernel with an uncertainty level ρ ≥ 0,
r = {r }H is the reward function. A policy π = {π }H is a sequence of decision rules. For any
h h=1 h h=1
policy π, we define the robust value function Vπ,ρ(s) = inf EP[(cid:80)H r (s ,a )|s = s,π]
h P∈Uρ(P0) t=h t t t h
and the robust Q-function Qπ,ρ(s,a) = inf EP[(cid:80)H r (s ,a )|s = s,a = a,π] for any
h P∈Uρ(P0) t=h t t t h h
(h,s,a) ∈ [H] × S × A. Moreover, we define the optimal robust value function and optimal
robust state-action value function: for any (h,s,a) ∈ [H] × S × A, V⋆,ρ(s) = sup Vπ,ρ(s),
h π∈Π h
Q⋆,ρ(s,a) = sup Qπ,ρ(s,a), where Π is the set of all policies. Correspondingly, the optimal robust
h π∈Π h
policy is the policy that achieves the optimal robust value function π⋆ = argsup Vπ,ρ(s).
π∈Π h
4We study the d-rectangular linear DRMDP (Ma et al., 2022; Blanchet et al., 2023; Liu and Xu,
2024a,b), which is a special DRMDP where the nominal environment is a linear MDP (Jin et al.,
2020) with a simplex state space, defined as follows.
Assumption 3.1. Given a known feature mapping ϕ : S ×A → Rd satisfying (cid:80)d ϕ (s,a) = 1,
i=1 i
ϕ (s,a) ≥ 0, for any (i,s,a) ∈ [d]×S ×A, we assume the reward functions {r }H and nominal
i h h=1
transition kernels {P0}H are linearly parameterized. Specifically, for any (h,s,a) ∈ [H]×S ×A,
h h=1
r (s,a) = ⟨ϕ(s,a),θ ⟩,P0(·|s,a) = ⟨ϕ(s,a),µ0(·)⟩, where {θ }H are known vectors with bounded
h √ h h h h h=1
norm ∥θ ∥ ≤ d and {µ }H are unknown probability measures over S.
h 2 h h=1
In d-rectangular linear DRMDPs, the uncertainty set Uρ(P0) is defined based on the linear
h h
structure of P0 satisfying Assumption 3.1. In particular, we first define the factor uncertainty
h
sets as Uρ (µ0 ) = {µ : µ ∈ ∆(S),D(µ||µ0 ) ≤ ρ},∀(h,i) ∈ [H] ×[d]. In this work we choose
h,i h,i h,i
D(·||·) as the total variation (TV) divergence. Then we define the uncertainty set as Uρ(P0) =
h h
(cid:78) Uρ(s,a;µ0), where Uρ(s,a;µ0) = {(cid:80)d ϕ (s,a)µ (·) : µ (·) ∈ Uρ (µ0 ),∀i ∈ [d]}.
(s,a)∈S×A h h h h i=1 i h,i h,i h,i h,i
Liu and Xu (2024a) show that the following robust Bellman equations hold, that is for any policy π,
Qπ h,ρ(s,a) = r h(s,a)+inf P h(·|s,a)∈U hρ(s,a;µ0 h)[P hV hπ +,ρ 1](s,a), (3.1a)
Vπ,ρ(s) = E (cid:2) Qπ,ρ(s,a)(cid:3) , (3.1b)
h a∼π h(·|s) h
as well as the robust Bellman optimality equations
Q⋆ h,ρ(s,a) = r h(s,a)+inf P h(·|s,a)∈U hρ(s,a;µ0 h)[P hV h⋆ +,ρ 1](s,a), (3.2a)
V⋆,ρ(s) = max Q⋆(s,a). (3.2b)
h a∈A h
In the context of online DRMDPs, an agent actively interacts with the nominal environment within
K episodes to learn the optimal robust policy. Specifically, at the start of episode k, an agent
chooses a policy πk based on the history information and receives the initial state sk. Then the agent
1
interacts with the nominal environment by executing πk until the end of episode k, and collects a
new trajectory. The goal of the agent is to minimize the average suboptimality after K episodes,
which is defined as AveSubopt(K) = 1/K(cid:80)K (cid:2) V⋆,ρ(sk)−Vπk,ρ(sk)(cid:3).
k=1 1 1 1 1
Lu et al. (2024) recently show that in general sample efficient learning in online DRMDPs is
impossible due to the curse of support shift, i.e., the nominal kernel and target kernel do not share
the same support. By designing proper feature mappings, we show that their hard example implies
the same hardness result for the online linear DRMDP setting.
Proposition 3.2. (Hardness result) There exists two d-rectangular linear DRMDPs {M ,M },
0 1
such that inf sup E[AveSuboptM θ,ALG(K)] ≥ Ω(ρ·H), where AveSuboptM θ,ALG(K) is the
ALG θ∈{0,1}
average suboptimality of algorithm ALG under the d-rectangular linear DRMDP M .
θ
Note that the lower bound in Proposition 3.2 does not converge to zero as K increases, which
means that in general no algorithm can guarantee to learn the optimal robust policy approximately.
To circumvent this problem, in the rest of paper we focus on a tractable subclass of d-rectangular
linear DRMDP following Liu and Xu (2024a); Lu et al. (2024), which is formally defined in the
following assumption.
Assumption 3.3 (Fail-state). Assume there exists a ‘fail state’ s in the d-rectangular linear
f
DRMDP, such that for all (h,a) ∈ [H]×A, r (s ,a) = 0, P0(s |s ,a) = 1.
h f h f f
5With Assumption 3.3, we can follow the framework in Liu and Xu (2024a), where we have the
following results on robust value functions and dual formulation that are helpful in solving the
optimization in (3.2).
Proposition 3.4 (Remark 4.2 of Liu and Xu (2024a)). Under Assumption 3.3, for any (π,h,a) ∈
Π×[H]×A, we have Qπ,ρ(s ,a) = 0, and Vπ,ρ(s ) = 0. Moreover, for any function V : S → [0,H]
h f h f
with min V(s) = V(s ) = 0, we have inf E V(s) = max {E [V(s)] −ρα}.
s∈S f µ∈Uρ(µ0) s∼µ α∈[0,H] s∼µ0 α
4 Algorithm Design
One prominent property of the d-rectangular DRMDP is that the robust Q-functions possess linear
representations with respect to the feature mapping ϕ. In particular, under Assumptions 3.1
and 3.3, Liu and Xu (2024a) show that for any (π,s,a,h) ∈ Π × S × A × [H], the robust Q-
function Qπ,ρ(s,a) has a linear form as follows Qπ,ρ(s,a) = (cid:0) r (s,a)+ϕ(s,a)⊤νπ,ρ(cid:1)1{s ̸= s },
h h h h f
where ν hπ,ρ = (cid:0) ν hπ ,, 1ρ,...,ν hπ ,, dρ(cid:1)⊤, ν hπ ,, iρ = max α∈[0,H](cid:8) z hπ ,i(α)−ρα(cid:9), z hπ ,i(α) = Eµ0 h,i(cid:2) V hπ +,ρ 1(s′)(cid:3)
α
and
α ∈ [0,H] is the dual variable derived from the dual formulation (see Proposition F.1 for more
details). Moreover, the robust Bellman optimality equation (3.2) shows that the greedy policy with
respect to the optimal robust Q-function is exactly the optimal robust policy π⋆. Therefore, the core
idea behind the algorithm design is to estimate the optimal robust Q-function using linear function
approximation, and then find π⋆ by the greedy policy derived from the estimated optimal robust
Q-function. We present our algorithm in Algorithm 1. In the sequel, we provide detailed discussion
about the components in our algorithm design.
4.1 Variance-Weighted Ridge Regression for Online DRMDPs
From Line 6 to 17 of Algorithm 1, we adopt the backward induction procedure to update the robust
Q-function estimation. In particular, for any (k,h) ∈ [K]×[H], suppose we have an estimated
robust value function Vˆρ . By the robust Bellman optimality equation (3.2) and Proposition 3.4,
k,h+1
conducting one step backward induction on Vˆρ leads to the following linear form (Liu and Xu,
k,h+1
2024a):
r h(s,a)+inf
P h∈U
hρ(s,a;µ0)P h[Vˆ k,h+1](s,a) = ϕ(s,a)⊤(θ h+ν hρ,k)1{s ̸= s f}, (4.1)
where ν hρ ,, ik := max α∈[0,H]{z hk ,i(α)−ρα} and z hk ,i(α) := Eµ0 h,i[Vˆ k,h+1(s′)] α, for any i ∈ [d]. Note that
under Assumption 3.1, for any α ∈ [0,H], zk (α) is the i-th element of the parameter of the following
h,i
linear formulation, [P0[Vˆ ] ](s,a) = ⟨ϕ(s,a),zk(α)⟩. Thus, we can estimate zk(α) from data to
h k,h+1 α h h
get estimations of zk (α),∀i ∈ [d]. To this end, we introduce the variance-weighted ridge regression
h,i
regime to estimate zk(α) as follows
h
zˆk(α) = argmin
(cid:80)k−1σ¯−2(cid:16)
z⊤ϕ(cid:0) sτ,aτ(cid:1) −(cid:2) Vˆρ (cid:0) sτ (cid:1)(cid:3)
(cid:17)2
+λ∥z∥2
h z∈Rd τ=1 τ,h h h k,h+1 h+1 α 2
= Σ−1 (cid:80)k−1σ¯−2ϕ(cid:0) sτ,aτ(cid:1)(cid:2) Vˆρ (cid:0) sτ (cid:1)(cid:3) , (4.2)
k,h τ=1 τ,h h h k,h+1 h+1 α
where Σ = λI+(cid:80)k−1σ¯−2ϕ(cid:0) sτ,aτ(cid:1) ϕ(cid:0) sτ,aτ(cid:1)⊤, σ¯ are regression weights that will be formally
k,h τ=1 τ,h h h h h τ,h
introduced later. We then approximate νρ,k by solving the optimization problem element-wisely
h
νˆρ,k = max (cid:8) zˆk (α)−ρα(cid:9) , i ∈ [d]. (4.3)
h,i α∈[0,H] h,i
6(cid:113) √
F √urther, weincorporateabonustermΓˆ k,h(s,a) = β(cid:80)d i=1ϕ i(s,a) 1⊤
i
Σ− k,1 h1 i, whereβ = O(cid:101)(cid:0) H dλ+
d(cid:1), into the robust Q-function estimation. We will prove in our analysis that the estimated Q-
function Qˆρ in Line 12 of Algorithm 1 is an optimistic estimator for the optimal robust Q-function.
k,h
Inspired by He et al. (2023), we also establish pessimistic estimated robust Q-functions by the same
backward induction procedure, which will be helpful in constructing the variance estimator σ¯ as
τ,h
shown in the next section. In particular, given Vˇρ , we estimate
k,h
zˇk(α) = argmin (cid:80)k−1σ¯−2(cid:0) z⊤ϕ(cid:0) sτ,aτ(cid:1) −(cid:2) Vˇρ (cid:0) sτ (cid:1)(cid:3) (cid:1)2 +λ∥z∥2
h z∈Rd τ=1 τ,h h h k,h+1 h+1 α 2
= Σ−1 (cid:80)k−1σ¯−2ϕ(cid:0) sτ,aτ(cid:1)(cid:2) Vˇρ (cid:0) sτ (cid:1)(cid:3) ,
k,h τ=1 τ,h h h k,h+1 h+1 α
and then get the estimation
νˇρ,k = max (cid:8) zˇk (α)−ρα(cid:9) , i ∈ [d]. (4.4)
h,i α∈[0,H] h,i
(cid:113) √
N √ext,byincorporatingapenaltytermΓˇ k,h(s,a) = β¯(cid:80)d i=1ϕ i(s,a) 1⊤
i
Σ− k,1 h1 i,whereβ¯= O(cid:101)(cid:0) H dλ+
d3H3(cid:1), we get the pessimistic estimated Q-function Qˇρ as Line 13 in Algorithm 1. We note that
k,h
Liu and Xu (2024b) also construct pessimistic robust Q-function estimations, but 1) they do not
construct the estimation episodically, 2) their pessimistic estimators are used to get the optimal
robust policy estimation. While ours are used to construct the variance estimator, as is shown in the
next section.
4.2 Variance Estimator with Refined Dependence on Problem Parameters
In this section, we construct the weights used in (4.2) and aim to get an optimistic estimator
for the variance of the optimal robust value function, V V∗,ρ. Inspired by He et al. (2023), the
h h+1
variance estimator at episode k should be a uniform variance upper bound for all subsequent episodes.
To obtain the optimistic estimator for V V∗,ρ, we first solve regression problems to obtain the
h h+1
estimator for V Vˆρ , which is denoted as V¯ Vˆρ . Then we analyze the error between V V∗,ρ
h k,h+1 h k,h+1 h h+1
and V¯ Vˆρ to finish the construction. Different from Liu and Xu (2024b, Equation (5.2)), the
h k,h+1
variance estimator here is not trivially constructed from subtracting a specific penalty term because
we should guarantee the monotonicity of estimated variance for the online exploration.
The variance of estimated optimistic value function Vˆρ can be denoted by
k,h+1
(cid:2)V Vˆρ (cid:3) (s,a) = (cid:2)P0(cid:0) Vˆρ (cid:1)2(cid:3) (s,a)−(cid:0)(cid:2)P0Vˆρ (cid:3) (s,a)(cid:1)2 . (4.5)
h k,h+1 h k,h+1 h k,h+1
Under Assumption 3.1, P0(cid:0) Vˆρ (cid:1)2 and P0Vˆρ on the RHS of (4.5) are linear in ϕ(s,a) based on
h k,h+1 h k,h+1
Jin et al. (2020, Proposition 2.3). Thus we can approximate them as follows
(cid:2)V Vˆρ (cid:3) (s,a) ≈ (cid:2)V¯ Vˆρ (cid:3) (s,a) = (cid:2) ϕ(s,a)⊤zk (cid:3) −(cid:2) ϕ(s,a)⊤zˆk (cid:3)2 ,
h k,h+1 h k,h+1 (cid:101)h,2 [0,H2] h,1 [0,H]
where zˆk and zk are solutions to the following ridge regression problems
h,1 (cid:101)h,2
zk = argmin (cid:80)k−1(cid:0) z⊤ϕ(cid:0) sτ,aτ(cid:1) −(cid:0) Vˆρ (cid:0) sτ (cid:1)(cid:1)2(cid:1)2 +λ∥z∥2,
(cid:101)h,2 z∈Rd τ=1 h h k,h+1 h+1 2
zˆk = argmin (cid:80)k−1(cid:0) z⊤ϕ(cid:0) sτ,aτ(cid:1) −Vˆρ (cid:0) sτ (cid:1)(cid:1)2 +λ∥z∥2.
h,1 z∈Rd τ=1 h h k,h+1 h+1 2
7Algorithm 1 Weighted Distributionally Robust Iterative Value Estimation with UCB (We-DRIVE-U)
1: Initialization: hyperparameters β,β¯,β(cid:101)> 0 and λ > 0. Set k last = 0; for each stage h ∈ [H],
set Σ ,Σ ,Λ ← λI and set Qˆρ (·,·) ← H,Qˇρ (·,·) ← 0
0,h 1,h 1,h 0,h 0,h
2: for episode k = 1,··· ,K do
3:
Receive the initial state sk
1
4: Set Vˆρ (·) ← 0,Vˇρ (·) ← 0
k,H+1 k,H+1
5: if there exists a stage h′ ∈ [H] such that det(Σ k,h′) ≥ 2det(Σ k ,h′) then
last
6: for stage h = H,··· ,1 do
7: if h = H then
8: νˆρ,k ← 0, νˇρ,k ← 0
h h
9: else
10: Compute νˆρ,k,∀i ∈ [d] according to (4.3) and νˇρ,k,∀i ∈ [d] according to (4.4).
h,i h,i
11: end if
12: Qˆρ (s,a)←min(cid:8) r (s,a)+ϕ(s,a)⊤νˆρ,k+Γˆ (s,a),Qˆρ (s,a),H −h+1(cid:9)1{s̸=s }
k,h h h k,h k−1,h f
13: Qˇρ k,h(s,a) ← max(cid:8) r h(s,a)+ϕ(s,a)⊤νˇ hρ,k −Γˇ k,h(s,a),Qˇρ k−1,h(s,a),0(cid:9)1{s ̸= s f}
14: Set the last updating episode k last ← k
15: Vˆ kρ ,h(s) ← max aQˆρ k,h(s,a), Vˇ kρ ,h(s) ← max aQˇρ k,h(s,a)
16: πk(s) ← argmax Qˆρ (s,a)
h a∈A k,h
17: end for
18: else
19: Vˆρ (s) ← Vˆρ (s), Vˇρ (s) ← Vˇρ (s), πk(s) ← πk−1(s) for all h ∈ [H]
k,h k−1,h k,h k−1,h h h
20: end if
21: for stage h = 1,··· ,H do
22: Take the action ak ← πk(sk)
h h h
23: Calculate the estimated variance σ k,h according to (4.6) and σ¯ k,h according to (4.7)
24: Σ k+1,h ← Σ k,h+σ¯ k− ,h2ϕ(cid:0) sk h,ak h(cid:1) ϕ(cid:0) sk h,ak h(cid:1)⊤, Λ k+1,h ← Λ k,h+ϕ(cid:0) sk h,ak h(cid:1) ϕ(cid:0) sk h,ak h(cid:1)⊤
25:
Receive next state sk
h+1
26: end for
27: end for
Different from the variance estimation in He et al. (2023), we construct both z˜k and zˆk by solving
h,2 h,1
vanilla ridge regressions, instead of variance-weighted ridge regressions. This specific choice of
parameter estimation will simplify our analysis of the variance estimation error, while fully capture
the variance information. Now we can construct σ , which is the estimated variance of the optimal
k,h
robust value function V∗,ρ in episode k, as follows
h
(cid:113)
σ = (cid:2)V¯ Vˆρ (cid:3) (sk,ak)+E +d3H ·D +1/2, (4.6)
k,h h k,h+1 h h k,h k,h
where E represents the error between the estimated variance and the true variance of Vˆρ , and
k,h k,h+1
D represents the error between the true variance of Vˆρ and the true variance of V∗,ρ. We
k,h k,h+1 h+1
define E ,D as follows
k,h k,h
E k,h = min(cid:8) β(cid:101)(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ−1,H2(cid:9) +min(cid:8) 2Hβ¯(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ−1,H2(cid:9) ,
k,h k,h
D k,h = min(cid:8) 4H(cid:0) ϕ(cid:0) sk h,ak h(cid:1)⊤ zˆ hk ,1−ϕ(cid:0) sk h,ak h(cid:1)⊤ zˇ hk ,1+2β¯(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ−1(cid:1) ,H2(cid:9) ,
k,h
8√ √ √
w √here Λ
k,h
= λI + (cid:80)k τ− =1 1ϕ(cid:0) sτ h,aτ h(cid:1) ϕ(cid:0) sτ h,aτ h(cid:1)⊤, β¯ = O(cid:101)(cid:0) H dλ + d3H3(cid:1) and β(cid:101) = O(cid:101)(cid:0) H2 dλ +
d3H6(cid:1), and zˇk is the solution of the following regression problems
h,1
zˇk = argmin (cid:80)k−1(cid:0) z⊤ϕ(cid:0) sτ,aτ(cid:1) −Vˇρ (cid:0) sτ (cid:1)(cid:1)2 +λ∥z∥2.
h,1 z∈Rd τ=1 h h k,h+1 h+1 2
Finally, we construct weights for the variance-weighted ridge regression problem (4.2) as follows
√
∀(k,h) ∈ [K]×[H], σ¯ k,h = max(cid:8) σ k,h,1, 2d3H2(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Σ1/ −2 1(cid:9) . (4.7)
k,h
He et al. (2023) construct a similar weight in the form σ¯
k,h
= max(cid:8) σ k,h,H,2d3H2(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13)1 Σ/ −2 1(cid:9).
k,h
Differently, the second term of our constructed weight in (4.7) is 1, instead of H. This is important
in achieving a tighter dependence on H. The intuition is that, when k is large, σ¯ should be close to
k,h
the variance of the optimal robust value function. According to the ‘Range Shrinkage’ phenomenon
unique to DRMDPs, which will be introduced in the next section, the true variance is in the order of
O(1) when ρ = O(1). To get a precise variance estimation, σ¯ should be in the same order of the
k,h
true variance. Moreover, a constant order lower bound on σ¯ will also ensure the weight will not
k,h
cause any inflation in the weighted regression (4.2). As for the third term, we choose it to be tight
without infecting our analysis. We refer to more details of the analysis in the proof of Lemma B.7.
4.3 Algorithm Interpretation
In this section, we provide several remarks to fully interpret Algorithm 1.
Remark 4.1. We highlight that Algorithm 1 is the first algorithm adopting ‘rare-switching’ update
strategy for distributionally robust RL. Different from He et al. (2023), the ‘rare-switching’ condition
on Line (5) is set at the beginning of each episode. This is achieved by our variance estimator
design, which is independent of the parameter zk(α) update. The update rule on Line (5) determines
h
whether to update robust Q-function estimations and switch to a new policy for the current episode,
and leads to two advantages, 1) the number of times solving the ridge regression (4.2) and dual
optimization(4.3)significantlydecreases, whichconstitutethemaincomputationcostofAlgorithm1,
and 2) in real application scenarios where policy switching is costly or risky, Algorithm 1 possesses
low policy switching property. We refer the readers to Proposition 5.9 and Remark 5.10 for more
details.
Remark 4.2. In Line 10, we estimate νρ,k element-wisely, and thus the estimator νˆρ,k is derived
h h
from d separate variance-weighted ridge regressions (4.2) and dual optimizations (4.3). This leads to
(cid:113)
the specific form of bonus term Γˆ (s,a) = β(cid:80)d ϕ (s,a) 1⊤Σ−11 , which is actually an upper
k,h i=1 i i k,h i
bound of the robust estimation error (see Lemma B.4 and its proof) at episode k. Though the bonus
term resembles that in Liu and Xu (2024a), we highlight that the sampling covariance matrix Σ
k,h
in Γˆ (s,a) is indeed a variance-weighted one. The specific form of the bonus term leads to the new
k,h
variance-weighted d-rectangular robust estimation error defined in (5.1).
Remark 4.3. On Line 12 and 13, we adopt a monotonic Q-function update strategy, such that the
estimated optimistic (pessimistic) robust value function is monotonically decreasing (increasing) to
the optimal robust value function. This strategy is to make sure that the variance estimator σ at
k,h
any episode k ∈ [H] is a uniform upper bound for those in the subsequent episodes, which would be
9helpful in bounding the estimation error arising from the variance-weighted ridge regression (4.2).
This idea is first introduced by Azar et al. (2017) for standard tabular MDPs and then utilized
by He et al. (2023) for standard linear MDPs. This is the first time it is utilized in the online
linear DRMDP setting, where the episodic estimation regime proposes additional requirement on
the variance estimator construction compared to the offline setting studied in Liu and Xu (2024b).
5 Theoretical Analysis
WenowprovidetheoreticalresultsontheupperandlowerboundsonthesuboptimalityofAlgorithm1.
Theorem 5.1. Under Assumptions 3.1 and 3.3, set λ = 1/H2, then for any fixed δ ∈ (0,1) and
ρ ∈ (0,1], with probability at least 1−δ, the average suboptimality of We-DRIVE-U satisfies
AveSubopt(K) ≤ 2(cid:112) 2H3log(6/δ)/K + 4β (cid:80)K (cid:80)H (cid:80)d ϕk (cid:113) 1⊤Σ−11 , (5.1)
K k=1 h=1 i=1 h,i i k,h i
(cid:124) (cid:123)(cid:122) (cid:125)
variance-weighted d-rectangularestimationerror
√
where β = O(cid:101)( d), ϕk
h,i
is the i-th element of ϕk
h
= ϕ(sk h,ak h) and 1
i
is the one-hot vector with its
i-th entry being 1.
(cid:113)
Recall from Remark 4.2, the quantity (cid:80)d ϕk 1⊤Σ−11 in (5.1) originates from solving d
i=1 h,i i k,h i
separate variance-weighted ridge regressions at step h in episode k. A similar term also appears
(cid:113)
in the Theorem 5.1 of Liu and Xu (2024a). Differently, the the quantity (cid:80)d ϕk 1⊤Σ−11 is
i=1 h,i i k,h i
based on the variance-weighted sampling covariance matrix Σ , rather than the vanilla sampling
k,h
covariance matrix Λ as in Liu and Xu (2024a). In order to further bound (5.1), we need to
k,h
take a closer examination of the variance estimator. Intuitively, when episode k is large, the
variance estimator should be close to the variance of the optimal robust value function. Recent
study (Liu and Xu, 2024b) shows a ‘Range shrinkage’ phenomenon in the d-rectangular linear
DRMDP (refer to Lemma F.10), stating that the range of any robust value function satisfies
max Vπ,ρ(s)−min Vπ,ρ(s) ≤ min{1/ρ,H},∀(π,h,ρ) ∈ Π×[H]×(0,1]. This implies that the
s∈S h s∈S h
variance of the optimal robust value function is upper bounded by min{1/ρ,H}. Thus, when k is
large, we can expect σ¯
k,h
≲ O(cid:101)(min{1/ρ,H}) and hence Σ− k,1
h
⪯ O(cid:101)(min{1/ρ2,H2})Λ− k,1 h. Based on
this idea, next we rigorously bound (5.1) under the same setting as the Corrollary 5.3 of Liu and Xu
(2024a), and formally show in the following theorem and remark that the variance information leads
to a tighter dependence on H compared to Liu and Xu (2024a).
Theorem 5.2. Assumethatthereexistsanabsoluteconstantc > 0, suchthatforall(π,h) ∈ Π×[H]
EP0(cid:2) ϕ(s ,a )ϕ(s ,a )⊤(cid:3) ≥ c/d·I. (5.2)
π h h h h
Then under the same setting in Theorem 5.1 and the additional assumption in (5.2), for any fixed
δ ∈ (0,1), with probability at least 1−δ, the average suboptimality of We-DRIVE-U satisfies
√
AveSubopt(K) ≤ O(cid:101)(cid:0)(cid:0) dH ·min(cid:8) 1/ρ,H(cid:9) +H3/2(cid:1) / K +d15H13/K(cid:1) . (5.3)
Remark 5.3. When d ≥ H and the total number of episodes K is sufficiently large, the average
√
suboptimality can be simplified as O(cid:101){dHmin{1/ρ,H}/ K}. Note that under the same assumption
10in (5.2), Liu and Xu (2024a) prove that the average suboptimality of their algorithm DR-LSVI-
√
UCB is of the order O(cid:101)(d2H2/ K). Thus, We-DRIVE-U improves the state-of-the-art result by
O(dH/min{1/ρ,H}). Moreover, wehighlightthattheupperbound(5.3)dependsontheuncertainty
level ρ, which arises from the ‘Range Shrinkage’ phenomenon. When ρ increases from 0 to 1, the
suboptimality decreases up to a factor of O(H).
Remark 5.4. The assumption (5.2) is actually imposed on the DRMDP, requiring that the environ-
ment we encounter is exploratory enough. We would like to note that this assumption is necessary in
deriving our upper bound, since the elliptical potential lemma (Abbasi-Yadkori et al., 2011, Lemma
11), which is critical in deriving upper bounds in linear bandits and linear MDPs, does not apply
in the analysis of linear DRMDPs. We note that the previous work (Liu and Xu, 2024a) also used
this assumption to get the final upper bound for their algorithm. Moreover, the assumption (5.2)
can be deemed as an online version of the well-known full-type coverage assumption on the offline
dataset in offline (non-) robust RL. Specifically, in the context of standard offline RL, Chen and
Jiang (2019); Wang et al. (2020b); Xie et al. (2021) assume the offline dataset should cover the
distribution measure induced by any policy under the nominal environment. In the context of offline
robust RL, Panaganti et al. (2022, 2024); Zhang et al. assume that the offline dataset should cover
the distribution measure induced by any policy under any transition kernel in the uncertainty set. It
would be an interesting future research direction to study if assumption (5.2) can be relaxed.
√
Notably, when ρ = O(1), the suboptimality of We-DRIVE-U is of order O(dH/ K). After
multiplying K to recover the cumulative suboptimality, it is smaller than the minimax lower bound
√
for standard linear MDP, Ω(d H3K) (Zhou et al., 2021a). To assess the optimality of We-DRIVE-U,
we show an information-theoretic lower bound for the online linear DRMDP setting in the following
theorem.
Theorem 5.5. Letuncertaintylevelρ ∈ (0,3/4],H ≥ 6,andK ≥ 9d2H/32. Thenforanyalgorithm,
there exists a d-rectangular linear DRMDP parameterized by ξ = (ξ ,··· ,ξ ) such that the
1 H−1
expected average suboptimality is lower bounded as follows:
√
E AveSubopt(M ,K) ≥ Ω(cid:0)(cid:0) dH1/2·min{1/ρ,H}(cid:1) / K(cid:1) , (5.4)
ξ ξ
where E denotes the expectation over the probability distribution generated by the algorithm and
ξ
the nominal environment.
√
Remark 5.6. Theorem 5.5 shows that We-DRIVE-U is near-optimal up to a factor of O( H)
among the full range of uncertainty level. Moreover, when ρ → 0, the linear DRMDP degrades to
√
the standard linear MDP, and (5.4) matches the information-theoretic lower bound, Ω(d H3K), for
standard linear MDPs (Zhou et al., 2021a) after multiplying K to recover the cumulative regret.
√
When ρ = O(1), (5.4) is realized to Ω(dH1/2/ K), which has a factor of O(H) decrease compared
to the lower bound for standard linear MDP.
Next, we study the deployment complexity of Algorithm 1, which constitutes two sources of cost.
The first source is the policy switching cost, say, the total number of changes in the exploration
policy. This might be the main bottleneck in applications where changing the exploration policy is
costly or risky (Bai et al., 2019; Wang et al., 2021). The second source is the computation cost in
solving the dual optimization in (4.3). Recall in Remark 5.10 we discuss that Algorithm 1 adopts
the ‘rare-switching’ update strategy, which significantly reduces the two sources of cost. Next, we
formally define them as follows.
11Definition 5.7 (Global Switching Cost). We define the global switching cost of an algorithm that
runs for K episodes as Ngl := (cid:80)K 1{π ̸= π }.
switch k=1 k k+1
Definition 5.8 (Dual Oracle). We assume access to a maximization oracle, which takes a function
z : [0,H] → R and a fixed constant ρ > 0 as input, and outputs the maximum value z and the
max
maximizer α defined as z = max {z(α)−ρα} and α = argmax {z(α)−ρα}.
max max α∈[0,H] max α∈[0,H]
For an algorithm, we define the oracle complexity as the number of calls of the dual oracle. Finally,
we show that We-DRIVE-U admits low switching cost and low oracle complexity.
Next, we formally present theoretical results on the deployment complexity of Algorithm 1.
Proposition 5.9. Under the same setting as Theorem 5.1, the switching cost of We-DRIVE-U is
upper bounded by dHlog(1+H2K), and the oracle complexity of We-DRIVE-U is upper bounded
by 2d2Hlog(1+H2K).
Remark 5.10. The switching cost of the state-of-the-art algorithm DR-LSVI-UCB (Liu and Xu,
2024a) is K and the oracle complexity is dK. Thus, We-DRIVE-U improves both the switching cost
and oracle cost by a factor of K. We highlight that different from the standard linear MDP setting,
where the main computation complexity only comes from the policy update (He et al., 2023), in the
linear DRMDP setting, the calls of dual oracle, besides policy updates, are also a main source of
computational burden. The update rule in Line 5 guarantees that We-DRIVE-U calls the dual oracle
and updates the policy only when the criterion is met. Actually, Algorithm 1 is the first DRMDP
algorithm that admits low deployment complexity.
6 Discussion on the Tightness of the Upper and Lower Bounds
√
There is a O(cid:101)( H) gap between the upper bound presented in Remark 5.3 and the lower bound
derived in Theorem 5.5. We note that in our current analysis, we individually bound each term in
the variance-weighted d-rectangular estimation error in (5.1). However, in the analysis of non-robust
MDPs (Azar et al., 2017; Jin et al., 2018; He et al., 2023) and tabular DRMDPs (Lu et al., 2024), a
tight dependence on H is often achieved by exploiting the total variance law of the value function at
each episode. We conjecture a tight upper bound can be achieved by first bounding the variance-
weighted d-rectangular estimation error as a whole by the square root of the total variance and then
invoking the total variance law. In particular, inspired by the total variance law in Lemma C.6 of
Lu et al. (2024), the total variance should be in the order of O(Hmin{1/ρ,H}). Together with an
√
additional H arising in the suboptimality analysis, we conjecture the dependence of the upper
bound on H could be improved to O((cid:112) H2min{1/ρ,H}).
Based on the conjectured total variance analysis, when ρ = O(1/H), the improved dependence
on H is in the order of O(H3/2), matching the lower bound we present in (5.4). This implies that
√
our lower bound is tight and our upper bound is loose by O(cid:101)( H). When ρ = O(1), the improved
dependence on H is in the order of O(H), which means the total variance analysis does not further
improve the upper bound. In this case, we conjecture that a tighter lower bound is needed to
showcase the fundamental limit of online linear DRMDPs. Table 1 provides an illustration of our
conjecture and comparison. Currently, we find essential difficulties in relating the variance-weighted
d-rectangular estimation error with the total variance to show a tighter upper bound. We leave the
√
improvement of O( H) on both upper and lower bounds for future research.
12Table 1: Summary of the upper and lower bounds of We-DRIVE-U, and a conjectured minimax
lower bound. The bound in red represents it matches the conjectured minimax lower bound.
ρ = O(1/H) ρ = O(1)
(cid:16) (cid:17) (cid:16) (cid:17)
Upper Bound (5.3) O(cid:101) d√H2 O(cid:101) √dH
K K
(cid:16) (cid:17)
Lower Bound (5.4) Ω dH√3/2 Ω(dH√1/2 )
K K
Minimax Lower Bound (cid:16) (cid:17) (cid:16) (cid:17)
Ω dH√3/2 Ω √dH
(Conjectured)
K K
7 Experiments on Simulated Linear DRMDPs
We conduct numerical experiments to illustrate the performances of our proposed algorithm, We-
DRIVE-U, and compare it with the state-of-the-art algorithm for d-rectangular linear DRMDPs,
DR-LSVI-UCB (Liu and Xu, 2024a), as well as their non-robust counterpart, LSVI-UCB (Jin et al.,
2020). All numerical experiments were conducted on a MacBook Pro with a 2.6 GHz 6-Core Intel
CPU.
WeleveragethesimulatedlinearMDPsettingproposedbyLiuandXu(2024a). Forcompleteness,
we recall the experiment setting as follows. The source and target linear MDP environment are
shown in Figure 1(a) and Figure 1(b). The state space is S = {x ,··· ,x } and action space
1 5
A = {−1,1}4 ⊂ R4. At each episode, the initial state is always x , and it can transit to x ,x ,x
1 2 4 5
with probability defined in the figures. x is an intermediate state from which the next state can be
2
x ,x ,x . x is the fail state with reward 0 and x is an absorbing state with reward 1. For the
3 4 5 4 5
reward functions and transition probabilities, they are designed to depend on ⟨ξ,a⟩, where ξ ∈ R4
is a hyperparameter controls the MDP instances. The target environment is constructed by only
perturbing the transition probability at x of the source domain, and the extend of perturbation is
1
controlled by a hyperparameter q ∈ (0,1). We refer more details on the construction of the linear
DRMDP to the Supplementary A.1 of Liu and Xu (2024a).
We set ξ = (1/∥ξ∥ ,1/∥ξ∥ ,1/∥ξ∥ ,1/∥ξ∥ )⊤ and consider different choices of ∥ξ∥ from the
1 1 1 1 1
set {0.1,0.2,0.3}. Following the implementation in Liu and Xu (2024a), we use heterogeneous
uncertainty level and set ρ = 0.5 and ρ = 0 for all other cases. We set the number of interactions
1,4 h,i
with the nominal environment to 200. We evaluate policies learned by We-DRIVE-U, DR-LSVI-UCB
(Liu and Xu, 2024a) and LSVI-UCB (Jin et al., 2020) by the accumulative rewards achieved in
the target domain, which are illustrated in Figure 2. Figure 2 shows that: 1) policies learned by
We-DRIVE-U are robust to environmental perturbation, and the extent of the robustness depends on
the pre-specified parameter ρ; 2) In most cases, We-DRIVE-U outperforms DR-LSVI-UCB, meaning
it being more robust to environment perturbation. Moreover, Table 2 demonstrates the low-switching
property of We-DRIVE-U. During 200 interactions of the training process, We-DRIVE-U switches
policies only around 24 times, which stands in stark contrast to the 200 policy switches by LSVI-
UCB and DR-LSVI-UCB. These numerical results prove the superiority of our proposed algorithm
We-DRIVE-U and align well with our theoretical findings.
131 1
x x
4 4
p(1−δ−⟨ξ,a⟩) p(1−δ−⟨ξ,a⟩)
p(1−δ−⟨ξ,a⟩) 1−δ−⟨ξ,a⟩ q(δ+⟨ξ,a⟩) 1−δ−⟨ξ,a⟩
x x x x x x
1 2 3 1 2 3
(1−p)(1−δ−⟨ξ,a⟩) (1−p)(1−δ−⟨ξ,a⟩) (1−δ−⟨ξ,a⟩) (1−p)(1−δ−⟨ξ,a⟩)
δ+⟨ξ,a⟩ δ+⟨ξ,a⟩ (1−q)(δ+⟨ξ,a⟩) δ+⟨ξ,a⟩
δ+⟨ξ,a⟩ δ+⟨ξ,a⟩
x x
5 5
1 1
(a) The source MDP environment. (b) The target MDP environment.
Figure 1: The source and the target linear MDP environments. The value on each arrow represents
the transition probability. For the source MDP, there are five states and three steps, with the initial
state being x , the fail state being x , and x being an absorbing state with reward 1. The target
1 4 5
MDP on the right is obtained by perturbing the transition probability at the first step of the source
MDP, with others remaining the same.
Table 2: Simulation results of the switch complexity of We-DRIVE-U. We present the average policy
switch times of We-DRIVE-U during 200 interactions with the nominal environment, averaged over
10 replications. As a comparison, the policy switch times for LSVI-UCB and DR-LSVI-UCB are
both 200 under each setting.
ρ=0.1 ρ=0.2 ρ=0.3
∥ξ∥ =0.1 23.8 24.0 23.8
1
∥ξ∥ =0.2 24.2 24.4 24.0
1
∥ξ∥ =0.3 24.3 23.6 24.8
1
8 Conclusion
We studied upper and lower bounds in the setting of online linear DRMDPs. We proposed an
algorithm, We-DRIVE-U, leveraging the variance-weighted ridge regression and low policy-switching
techniques. Under assumptions on the structure of the MDP, we showed that the average subopti-
√
mality of We-DRIVE-U √is of order O(cid:101)(dHmin{1/ρ,H}/ K). We further established an √lower bound
Ω(dH1/2min{1/ρ,H}/ K), suggesting that We-DRIVE-U is near-optimal up to O(cid:101)( H) among
the full range of the uncertainty level.
141.4
1.4 LSVI-UCB 1.4 LSVI-UCB LSVI-UCB
1.3
DR-LSVI-UCB DR-LSVI-UCB DR-LSVI-UCB
1.2 We-DRIVE-U 1.2 We-DRIVE-U 1.2 We-DRIVE-U
1.1
1.0 1.0 1.0
0.9
0.8 0.8 0.8
0.7
0.6 0.6
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Perturbation Perturbation Perturbation
(a) ∥ξ∥ =0.1, ρ =0.1 (b) ∥ξ∥ =0.1, ρ =0.2 (c) ∥ξ∥ =0.1, ρ =0.3
1 1,4 1 1,4 1 1,4
1.6 LSVI-UCB 1.6 LSVI-UCB 1.6 LSVI-UCB
1.4 DR-LSVI-UCB 1.4 DR-LSVI-UCB 1.4 DR-LSVI-UCB
We-DRIVE-U We-DRIVE-U We-DRIVE-U
1.2 1.2 1.2
1.0 1.0 1.0
0.8 0.8
0.8
0.6
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Perturbation Perturbation Perturbation
(d) ∥ξ∥ =0.2, ρ =0.1 (e) ∥ξ∥ =0.2, ρ =0.2 (f) ∥ξ∥ =0.2, ρ =0.3
1 1,4 1 1,4 1 1,4
1.8 1.8 1.8
1.6 1.6 1.6
1.4 1.4 1.4
1.2 1.2 1.2
1.0 LSVI-UCB 1.0 LSVI-UCB 1.0 LSVI-UCB
0.8 DR-LSVI-UCB 0.8 DR-LSVI-UCB 0.8 DR-LSVI-UCB
We-DRIVE-U We-DRIVE-U We-DRIVE-U
0.6 0.6 0.6
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Perturbation Perturbation Perturbation
(g) ∥ξ∥ =0.3, ρ =0.1 (h) ∥ξ∥ =0.3, ρ =0.2 (i) ∥ξ∥ =0.3, ρ =0.3
1 1,4 1 1,4 1 1,4
Figure 2: Simulation results under different source domains. The x-axis represents the perturbation
level corresponding to different target environments. ρ is the input uncertainty level for our
1,4
We-DRIVE-U algorithm. ∥ξ∥ is the hyperparameter of the linear DRMDP environment.
1
A Proof of Proposition 3.2
Proof. WeinstantiatethehardexampleinExample3.1ofLuetal.(2024)intermsoftheformulation
of d-rectangular linear DRMDP satisfying Assumption 3.1. Consider two d-rectangular linear
DRMDPs , M and M . The state space S = {s ,s }, and the action space is A = {0,1}. We
0 1 good bad
define the feature mapping as
     
1 0 0
0  p(1−ϱ)  pϱ 
     
ϕϱ(s good,a) =  0 ,∀a ∈ A, ϕϱ(s bad,0) =   qϱ  , ϕϱ(s bad,1) =   q(1−ϱ) ,
     
0 (1−p)(1−ϱ) (1−p)ϱ 
     
0 (1−q)ϱ (1−q)(1−ϱ)
15
drawer
egarevA
drawer
egarevA
drawer
egarevA
drawer
egarevA
drawer
egarevA
drawer
egarevA
drawer
egarevA
drawer
egarevA
drawer
egarevAwhere ϱ ∈ {0,1} is the index of the d-rectangular linear DRMDP instance. Define the factor
distributions µ = (δ ,δ ,δ ,δ ,δ )⊤ and the reward parameter θ = (1,0,0,0,0)⊤.
s s s s s
good good good bad bad
Then it is trivial to check that equipped with the d-rectangular TV divergence uncertainty set, this
example recover the hard example in Example 3.1 of Lu et al. (2024).
B Proof of the Upper Bound on the Suboptimality of We-DRIVE-U
In this section, we present the proofs of our main theoretical results Theorems 5.1 and 5.2. We start
with presenting the technical lemmas in Appendix B.1, and then we derive the upper bound on the
suboptimality of We-DRIVE-U in Appendices B.2 and B.3.
B.1 Technical Lemmas
Definition B.1 (Good event). Under Assumptions 3.1 and 3.3, then for any fixed δ ∈ (0,1),
α′ ∈ [0,H] and ρ ∈ (0,1], we define E be the event that for all episode k ∈ [K], stage h ≤ h′ ≤ H,
h
(cid:13)k−1 (cid:13)
(cid:13) (cid:13) (cid:13)(cid:88) σ¯ τ− ,h2 ′ϕτ h′(cid:104) (cid:2) Vˆ kρ ,h′+1(sτ h′+1)(cid:3)
α′
−(cid:2)P0 h′(cid:2) Vˆ kρ ,h′+1(cid:3) α′(cid:3) (sτ h′,aτ h′)(cid:105)(cid:13) (cid:13)
(cid:13)
≤ γ, (B.1)
τ=1
Σ− k,1
h′
√
where γ = O(cid:101)(cid:0) d(cid:1).
Lemma B.2. We define E¯ as the event that the following inequalities hold for all (s,a) ∈ S ×A,
k ∈ [K], h ∈ [H],
(cid:113)
(cid:12) (cid:12)ϕ(s,a)⊤zˆk −(cid:2)P0Vˆρ (cid:3) (s,a)(cid:12) (cid:12) ≤ β¯ ϕ(s,a)⊤Λ−1ϕ(s,a),
h,1 h k,h+1 k,h
(cid:113)
(cid:12) (cid:12)ϕ(s,a)⊤zˇk −(cid:2)P0Vˇρ (cid:3) (s,a)(cid:12) (cid:12) ≤ β¯ ϕ(s,a)⊤Λ−1ϕ(s,a),
h,1 h k,h+1 k,h
(cid:113)
(cid:12) (cid:12)ϕ(s,a)⊤z (cid:101)hk ,2−(cid:2)P0 h(cid:0) Vˆ kρ ,h+1(cid:1)2(cid:3) (s,a)(cid:12) (cid:12) ≤ β(cid:101) ϕ(s,a)⊤Λ− k,1 hϕ(s,a),
√ √ √ √
where β¯= O(cid:101)(cid:0) H dλ+ d3H3(cid:1) and β(cid:101)= O(cid:101)(cid:0) H2 dλ+ d3H6(cid:1). Then event E¯holds with probability
at least 1−δ.
Lemma B.3 (Variance error). On the event E and E¯, for all episode k ∈ [K], the estimated
h+1
variance satisfies
(cid:12) (cid:12)(cid:2)V¯ hVˆ kρ ,h+1(cid:3)(cid:0) sk h,ak h(cid:1) −[V hVˆ kρ ,h+1](cid:0) sk h,ak h(cid:1)(cid:12) (cid:12) ≤ E k,h,
(cid:12) (cid:12)(cid:2)V¯ hVˆ kρ ,h+1(cid:3)(cid:0) sk h,ak h(cid:1) −[V hV h∗ +,ρ 1](cid:0) sk h,ak h(cid:1)(cid:12) (cid:12) ≤ E k,h+D k,h.
Thus we also have
σ¯2 ≥ (cid:2)V¯ Vˆρ (cid:3)(cid:0) sk,ak(cid:1) +E +D ≥ (cid:2)V V∗,ρ(cid:3)(cid:0) sk,ak(cid:1) .
k,h h k,h+1 h h k,h k,h h h+1 h h
Lemma B.4. For any fixed policy π, on the event E and E¯, for all (s,a,k) ∈ S/{s }×A×[K],
h f
for stage h ≤ h′ ≤ H, we have
(cid:0) r (s,a)+ϕ(s,a)⊤νˆρ,k(cid:1) −Qπ,ρ(s,a) = inf (cid:2)P Vˆρ (cid:3) (s,a)
h′ h′ h′
P h′(·|s,a)∈U hρ ′(s,a;µ0 h′)
h′ k,h′+1
16− inf (cid:2)P Vπ,ρ (cid:3) (s,a)+∆k (s,a),
P h′(·|s,a)∈U hρ ′(s,a;µ0 h′)
h′ h′+1 h′
(cid:113)
where ∆k (s,a) that satisfies |∆k (s,a)| ≤ Γˆ (s,a) = β(cid:80)d ϕ (s,a) 1⊤Σ−1 1 where β =
h′ h′ k,h′ i=1 i i k,h′ i
(cid:16)√ √ (cid:17)
O(cid:101) λdH + d .
Lemma B.5 (Optimism and pessimism). On the event E and E¯, for all episode k ∈ [K] and stage
h
h ≤ h′ ≤ H, for all (s,a) ∈ S ×A, we have Qˆρ (s,a) ≥ Q∗,ρ(s,a) ≥ Qˇρ (s,a). In addition, we
k,h′ h′ k,h′
have Vˆρ (s) ≥ V∗,ρ(s) ≥ Vˇρ (s).
k,h′ h′ k,h′
Lemma B.6. On the event E¯, event E = E holds with probability at least 1−δ.
1
Lemma B.7. Under the assumption (5.2) and events E and E¯, for any h ∈ [H], set λ = 1/H2 and
ρ ∈ (0,1]. Then when k ≥ K(cid:101) where K(cid:101) = O(cid:101)(cid:0) d15H12(cid:1), with probability at least 1−δ, then we have
(cid:16) (cid:110) 1 (cid:111)(cid:17)
σ¯2 ≤ O min ,H2 .
k,h ρ2
B.2 Proof of Theorem 5.1
Proof of Theorem 5.1. Conditioned on the event E and E¯, we first do the following decomposition
Vˆρ (cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)
k,h h h h
=Qˆρ (cid:0) sk,ak(cid:1) −Qπk,ρ(cid:0) sk,ak(cid:1)
k,h h h h h h
≤r (cid:0) sk,ak(cid:1) +ϕ(cid:0) sk,ak(cid:1)⊤ νˆρ,klast +Γˆ (cid:0) sk,ak(cid:1) −Qπk,ρ(cid:0) sk,ak(cid:1)
h h h h h h klast,h h h h h h
≤ inf (cid:2)P Vˆρ (cid:3)(cid:0) sk,ak(cid:1) − inf (cid:2)P Vπk,ρ(cid:3)(cid:0) sk,ak(cid:1) +2Γˆ (cid:0) sk,ak(cid:1)
Ph(·|s,a)∈U hρ(s,a;µ0 h) h k,h+1 h h Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1 h h klast,h h h
≤ inf (cid:2)P Vˆρ (cid:3)(cid:0) sk,ak(cid:1) − inf (cid:2)P Vπk,ρ(cid:3)(cid:0) sk,ak(cid:1) +4Γˆ (cid:0) sk,ak(cid:1) ,
Ph(·|s,a)∈U hρ(s,a;µ0 h) h k,h+1 h h Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1 h h k,h h h
where the first equality holds due to the selection of πk, the first inequality holds due to the definition
h
of Qˆρ , the second inequality hold from Lemma B.4, the third inequality holds from Lemma F.2.
k,h
Note that
inf (cid:2)P Vˆρ (cid:3)(cid:0) sk,ak(cid:1) − inf (cid:2)P Vπk,ρ(cid:3)(cid:0) sk,ak(cid:1)
Ph(·|sk h,ak h)∈U hρ(sk h,ak h;µ0 h) h k,h+1 h h Ph(·|sk h,ak h)∈U hρ(sk h,ak h;µ0 h) h h+1 h h
=(cid:28) ϕ(sk h,ak h),(cid:20) αim ∈[a 0x ,H](cid:110) Eµ0 h,i(cid:2) Vˆ kρ ,h+1(s)(cid:3)
αi
−ρα i(cid:111)(cid:21) i∈[d](cid:29)
−(cid:28) ϕ(sk h,ak h),(cid:20) αim ∈[a 0x ,H](cid:110) Eµ0 h,i(cid:2) V hπ +k 1,ρ(s)(cid:3)
αi
−ρα i(cid:111)(cid:21) i∈[d](cid:29)
≤(cid:28) ϕ(sk h,ak h),(cid:20) αim ∈[a 0x ,H](cid:110) Eµ0 h,i(cid:2) Vˆ kρ ,h+1(s)(cid:3)
αi
−Eµ0 h,i(cid:2) V hπ +k 1,ρ(s)(cid:3) αi(cid:111)(cid:21) i∈[d](cid:29)
≤(cid:10) ϕ(sk h,ak h),Eµ0 h(cid:2) Vˆ kρ ,h+1(s)−V hπ +k 1,ρ(s)(cid:3)(cid:11)
=P0(cid:2) Vˆρ −Vπk,ρ(cid:3)(cid:3)(cid:0) sk,ak(cid:1)
h k,h+1 h+1 h h
=(cid:2)P0(cid:2) Vˆρ −Vπk,ρ(cid:3)(cid:3)(cid:0) sk,ak(cid:1) −(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3) +(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3) ,
h k,h+1 h+1 h h k,h+1 h+1 h+1 h+1 k,h+1 h+1 h+1 h+1
17where the second inequality holds from Lemma B.5. Then we have
Vˆρ (cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)
k,h h h h
≤(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3) +(cid:2)P0(cid:2) Vˆρ −Vπk,ρ(cid:3)(cid:3)(cid:0) sk,ak(cid:1) −(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3)
k,h+1 h+1 h+1 h+1 h k,h+1 h+1 h h k,h+1 h+1 h+1 h+1
+4Γˆ (cid:0) sk,ak(cid:1) , (B.2)
k,h h h
Then by applying (B.2) iteratively and applying Azuma-Hoeffding inequality, with probability at
least 1−δ/3, we have
K
K ×AveSubopt(K) =
(cid:88)(cid:16)
V∗,ρ(cid:0) sk(cid:1) −Vπk,ρ(cid:0)
sk(cid:1)(cid:17)
1 1 1 1
k=1
K
≤
(cid:88)(cid:16)
Vˆρ (cid:0) sk(cid:1) −Vπk,ρ(cid:0)
sk(cid:1)(cid:17)
k,1 1 1 1
k=1
K H
≤
(cid:88)(cid:88)(cid:16)
(cid:2)P (cid:2) Vˆρ −Vπk,ρ(cid:3)(cid:3)(cid:0) sk,ak(cid:1) −(cid:2) Vˆρ (sk )−Vπk,ρ(sk
)(cid:3)(cid:17)
h k,h+1 h+1 h h k,h+1 h+1 h+1 h+1
k=1h=1
K H
+(cid:88)(cid:88) 4Γˆ (cid:0) sk,ak(cid:1)
k,h h h
k=1h=1
≤ 2(cid:112) 2H3Klog(6/δ)+4β(cid:88)K (cid:88)H (cid:88)d ϕ (cid:0) sk,ak(cid:1)(cid:113) 1⊤Σ−11 ,
i h h i k,h i
k=1h=1i=1
where the first inequality holds from Lemma B.5, the second inequality holds from (B.2), the third
inequality holds from Azuma-Hoeffding inequality and the definition of Γˆ (cid:0) sk,ak(cid:1). Finally, by
k,h h h
taking probability union bound over E and E¯, with probability at least 1−δ, we can get the result
of Theorem 5.2,
(cid:112) (cid:88)K (cid:88)H (cid:88)d (cid:113)
AveSubopt(K) ≤ 2 2H3log(6/δ)/K +4β/K ϕk 1⊤Σ−11 .
h,i i k,h i
k=1h=1i=1
This completes the proof.
B.3 Proof of Theorem 5.2
Proof of Theorem 5.2. Conditioned on the event E and E¯, we first do the decomposition as follows
K
K ×AveSubopt(K) = (cid:88)(cid:0) V∗,ρ(cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)(cid:1)
1 1 1 1
k=1
K(cid:101) K
=
(cid:88)(cid:0) V∗,ρ(cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)(cid:1)
+
(cid:88) (cid:0) V∗,ρ(cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)(cid:1)
1 1 1 1 1 1 1 1
k=1 k=K(cid:101)+1
K
≤ HK(cid:101) +
(cid:88) (cid:0) V∗,ρ(cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)(cid:1)
.
1 1 1 1
k=K(cid:101)+1
18Recall from (B.2) in the proof of Theorem 5.1, we have
Vˆρ (cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)
k,h h h h
≤(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3) +(cid:2)P (cid:2) Vˆρ −Vπk,ρ(cid:3)(cid:3)(cid:0) sk,ak(cid:1) −(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3)
k,h+1 h+1 h+1 h+1 h k,h+1 h+1 h h k,h+1 h+1 h+1 h+1
+4Γˆ (cid:0) sk,ak(cid:1) .
k,h h h
Then by applying (B.2) iteratively and applying Azuma-Hoeffding inequality, with probability at
least 1−δ/4, we have
K
K ×AveSubopt(K) ≤ HK(cid:101) + (cid:88) (cid:0) V∗,ρ(cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)(cid:1)
1 1 1 1
k=K(cid:101)+1
K
≤ HK(cid:101) + (cid:88) (cid:0) Vˆρ (cid:0) sk(cid:1) −Vπk,ρ(cid:0) sk(cid:1)(cid:1)
k,1 1 1 1
k=K(cid:101)+1
K H
≤ HK(cid:101) + (cid:88) (cid:88)(cid:16) (cid:2)P0(cid:2) Vˆρ −Vπk,ρ(cid:3)(cid:3)(cid:0) sk,ak(cid:1)
h k,h+1 h+1 h h
k=K(cid:101)+1h=1
K H
−(cid:2) Vˆρ (sk )−Vπk,ρ(sk )(cid:3)(cid:17) + (cid:88) (cid:88) 4Γˆ (cid:0) sk,ak(cid:1)
k,h+1 h+1 h+1 h+1 k,h h h
k=K(cid:101)+1h=1
≤ HK(cid:101) +2(cid:112) 2H3Klog(8/δ)+4β (cid:88)K (cid:88)H (cid:88)d ϕ i(cid:0) sk h,ak h(cid:1)(cid:113) 1⊤
i
Σ− k,1 h1 i,
k=K(cid:101)+1h=1i=1
where the second inequality holds from Lemma B.5, the third inequality holds from (B.2) and the
last inequality holds from Azuma-Hoeffding inequality and the definition of Γˆ (cid:0) sk,ak(cid:1). Based on
k,h h h
(5.2) and Lemma B.7, with probability at least 1−δ/4, we can further have
4β
(cid:88)K (cid:88)H (cid:88)d
ϕ (cid:0)
sk,ak(cid:1)(cid:113)
1⊤Σ−11
i h h i k,h i
k=K(cid:101)+1h=1i=1
≤ 4c
βmin(cid:110)1 ,H(cid:111)
·
(cid:88)K (cid:88)H (cid:88)d
ϕ (cid:0)
sk,ak(cid:1)(cid:113)
1⊤Λ−11
1 ρ i h h i k,h i
k=K(cid:101)+1h=1i=1
≤ 4c
βmin(cid:110)1 ,H(cid:111)
·
(cid:88)K (cid:88)H (cid:88)d
ϕ
(cid:0)
sk,ak(cid:1)(cid:113)
λ
(cid:0) Λ−1(cid:1)
1 ρ i h h max k,h
k=K(cid:101)+1h=1i=1
K H (cid:115)
(cid:110)1 (cid:111) (cid:88) (cid:88) 1
≤ 4c βmin ,H ·
1 (cid:0) (cid:1)
ρ λ Λ
min k,h
k=K(cid:101)+1h=1
K H (cid:114)
(cid:110)1 (cid:111) (cid:88) (cid:88) 2d
≤ 4c βmin ,H ·
1
ρ k·c
k=K(cid:101)+1h=1
√ H (cid:110)1 (cid:111) (cid:90) K 1
≤ 4c 2dβ√ ·min ,H · √ dk
1
c ρ k−1
K(cid:101)+1
19√ H (cid:110)1 (cid:111) √
≤ 4c 2dβ√ ·min ,H ·2 K
1
c ρ
(cid:16) √ (cid:110)1 (cid:111)(cid:17)
≤ O(cid:101) dH K ·min ,H ,
ρ
where c > 0 is an absolute constant. The first inequality holds from Lemma B.7, the third
1
inequality holds because (cid:80)d ϕ (s,a) = 1 and the fourth inequality holds due to (C.10) with
i=1 i
K(cid:101) > 512/η2log(dKH/δ). Therefore, we can further bound the regret that
K ×AveSubopt(K) ≤ HK(cid:101) +2(cid:112) 2H3Klog(8/δ)+4β (cid:88)K (cid:88)H (cid:88)d ϕ i(cid:0) sk h,ak h(cid:1)(cid:113) 1⊤
i
Σ− k,1 h1
i
k=K(cid:101)+1h=1i=1
(cid:16) √ (cid:110)1 (cid:111) √ (cid:17)
≤ O(cid:101) dH K ·min ,H +H3 2 K +d15H13 .
ρ
Finally, by taking probability union bound over E and E¯, with probability at least 1−δ, we can
bound the average suboptimality of We-DRIVE-U as follows
(cid:32)
dH
·min(cid:8)1,H(cid:9) +H3
2
d15H13(cid:33)
AveSubopt(K) ≤ O(cid:101) √ρ + . (B.3)
K K
We complete the proof by substituting η = O(1/d) into (B.3).
C Proof of the Technical Lemmas
C.1 Proof of Lemma B.2
Before the proof of Lemma B.2, we first present a lemma that defines the optimistic value function
class and gives a upper bound for its covering number.
Lemma C.1 (Function class covering number). In Algorithm 1, for each episode k ∈ [K] and
h ∈ [H], the optimistic value function Vˆρ belongs to the following function class
k,h
(cid:26) (cid:12) (cid:110) (cid:88)d (cid:113) (cid:111)
V = V(cid:12)V(·)=max max min r (·,a)+ϕ(·,a)⊤w +β ϕ (·,a) 1⊤Γ 1 ,H ,
h (cid:12) h j i i j i
a 1≤j≤ℓ
i=1
√ (cid:27)
∥w ∥≤L,∥Γ ∥ ≤λ−1 d ,
j j F
where ℓ ≤ dHlog(1+K/λ) is the number of value function updates from Lemma D.1 and L =
2H(cid:112) dK/λ from Lemma D.2. Define N be the ϵ-covering number of V with respect to the distance
ϵ h
dist(V ,V ) = sup |V (s)−V (s)|. Then the covering entropy can be bounded by
1 2 s 1 2
√
logN ≤
dℓlog(1+4L/ϵ)+d2ℓlog(cid:0)
1+8
dβ2/λϵ2(cid:1)
.
ϵ
Proof of Lemma C.1. For any two function V ,V ∈ V , we can write V ,V as follows
1 2 h 1 2
(cid:110) (cid:88)d (cid:113) (cid:111)
V (·) = max max min r (·,a)+ϕ(·,a)⊤w +β ϕ (·,a) 1⊤Γ 1 ,H ,
1 h 1,j i i 1,j i
a 1≤j≤ℓ
i=1
20(cid:110) (cid:88)d (cid:113) (cid:111)
V (·) = max max min r (·,a)+ϕ(·,a)⊤w +β ϕ (·,a) 1⊤Γ 1 ,H ,
2 h 2,j i i 2,j i
a 1≤j≤ℓ
i=1
√
where ∥w ∥,∥w ∥ ≤ L, Γ ,Γ ≼ λ−1I and ∥Γ ∥ ,∥Γ ∥ ≤ λ−1 d. Then we have
1,j 2,j 1,j 2,j 1,j F 2,j F
dist(V ,V ) = sup|V (s)−V (s)|
1 2 1 2
s
(cid:12) d (cid:113)
≤ sup (cid:12) (cid:12) (cid:12)ϕ(s,a)⊤w 1,j +β(cid:88) ϕ i(s,a) 1⊤ i Γ 1,j1 i
1≤j≤ℓ,s∈S,a∈A
i=1
d (cid:113) (cid:12)
−ϕ(s,a)⊤w 2,j −β(cid:88) ϕ i(s,a) 1⊤ i Γ 2,j1 i(cid:12) (cid:12) (cid:12)
i=1
(cid:12) (cid:12)(cid:88)d (cid:16)(cid:113) (cid:113) (cid:17)(cid:12)
(cid:12)
≤ β sup (cid:12) (cid:12) ϕ i(s,a) 1⊤ i Γ 1,j1 i− 1⊤ i Γ 2,j1 i (cid:12) (cid:12)
1≤j≤ℓ,s∈S,a∈A
i=1
+ sup (cid:12) (cid:12)ϕ(s,a)⊤(w 1,j −w 2,j)(cid:12) (cid:12)
1≤j≤ℓ,s∈S,a∈A
(cid:12) (cid:12)(cid:88)d (cid:113)
(cid:0) (cid:1)
(cid:12)
(cid:12)
≤ β sup (cid:12) (cid:12) ϕ i(s,a)1⊤ i Γ 1,j −Γ 2,j ϕ i(s,a)1 i(cid:12) (cid:12)
1≤j≤ℓ,s∈S,a∈A
i=1
+ sup (cid:12) (cid:12)ϕ(s,a)⊤(w 1,j −w 2,j)(cid:12) (cid:12)
1≤j≤ℓ,s∈S,a∈A
(cid:113)
≤ β sup (cid:13) (cid:13)Γ 1,j −Γ 2,j(cid:13) (cid:13) F + sup (cid:13) (cid:13)w 1,j −w 2,j(cid:13) (cid:13) 2 (C.1)
1≤j≤ℓ 1≤j≤ℓ
√ √ √
where the third inequality holds because | x−y| ≥ | x− y|, the fourth inequality holds because
Cauchy-Schwarz inequality, ∥ϕ(s,a)∥ ≤ 1 and (cid:80)d ϕ = 1. Moreover, ∥·∥ is the Frobenius norm.
2 i=1 i F
Now, we denote C as a ϵ/2-cover of the set (cid:8) w ∈ Rd|∥w∥ ≤ L(cid:9) and C as a ϵ2/4β2-cover
w √ 2 Γ
of the set {Γ ∈ Rd×d | ∥Γ∥ ≤ λ−1 d} with respect to the Frobenius norm. Then according to
F
Lemma F.6, we have
√
|C | ≤ (1+4L/ϵ)d,|C | ≤ (cid:0) 1+8 dβ2/λϵ2(cid:1)d2 .
w Γ
Then for any function V ∈ V with parameters w ,Γ ,1 ≤ j ≤ ℓ, we can find parameters
1 h 1,j 1,j
w ∈ C ,Γ ∈ C ,1 ≤ j ≤ ℓ, such that ∥w −w ∥ ≤ ϵ/2,∥Γ −Γ ∥ ≤ ϵ2/4β2. Thus we
2,j w 2,j Γ 2,j 1,j 2 2,j 1,j F
have
(cid:113)
dist(V ,V ) ≤ β sup ∥Γ −Γ ∥ + sup ∥w −w ∥ ≤ ϵ,
1 2 1,j 2,j F 1,j 2,j 2
1≤j≤ℓ 1≤j≤ℓ
where the inequality holds from (C.1). Therefore, the ϵ-covering number of optimistic function class
V is bounded by N ≤ |C |ℓ·|C |ℓ, thus we have
h ϵ w Γ
√
logN ≤
dℓlog(1+4L/ϵ)+d2ℓlog(cid:0)
1+8
dβ2/λϵ2(cid:1)
,
ϵ
which completes the proof.
Now we are ready to prove Lemma B.2.
21Proof of Lemma B.2. For any stage h ∈ [H] and the optimistic value function Vˆρ , according to
k,h+1
Lemma D.3, there exists a vector zk such that P0Vˆρ (s,a) can be represented by ϕ(s,a)⊤zk and
√ h h k,h+1 h
∥zk∥ ≤ H d. Therefore, the parameter estimation error can be decomposed as
h 2
(cid:13) (cid:13)zˆk −zk(cid:13)
(cid:13)
h,1 h Λ
k,h
(cid:13) k−1 (cid:18) k−1 (cid:19) (cid:13)
≤ (cid:13) (cid:13)Λ−1 (cid:88) ϕ(cid:0) sτ,aτ(cid:1) Vˆρ (cid:0) sτ (cid:1) −Λ−1 λI+(cid:88) ϕ(cid:0) sτ,aτ(cid:1) ϕ(cid:0) sτ,aτ(cid:1)⊤ zk(cid:13) (cid:13)
(cid:13) k,h h h k,h+1 h+1 k,h h h h h h (cid:13)
τ=1 τ=1 Λ k,h
(cid:13) k−1 (cid:13)
≤ (cid:13) (cid:13)Λ−1 (cid:88) ϕ(cid:0) sτ,aτ(cid:1)(cid:0) Vˆρ (cid:0) sτ (cid:1) −P0Vˆρ (sτ,aτ)(cid:1) −λΛ−1zk(cid:13) (cid:13)
(cid:13) k,h h h k,h+1 h+1 h k,h+1 h h k,h h (cid:13)
τ=1 Λ k,h
(cid:13) k−1 (cid:13)
≤ (cid:13) (cid:13)λΛ−1zk(cid:13) (cid:13) +(cid:13) (cid:13)Λ−1 (cid:88) ϕ(cid:0) sτ,aτ(cid:1)(cid:0) Vˆρ (cid:0) sτ (cid:1) −P0Vˆρ (sτ,aτ)(cid:1)(cid:13) (cid:13) .
k,h h Λ k,h (cid:13) k,h h h k,h+1 h+1 h k,h+1 h h (cid:13)
(cid:124) (cid:123)(cid:122) (cid:125) τ=1 Λ k,h
I1 (cid:124) (cid:123)(cid:122) (cid:125)
I2
Bound term I :
1
√ √
I 1 = (cid:13) (cid:13)λΛ− k,1 hz hk(cid:13) (cid:13) Λ = λ(cid:13) (cid:13)z hk(cid:13) (cid:13) Λ−1 ≤ λ(cid:13) (cid:13)z hk(cid:13) (cid:13) 2 ≤ H dλ,
k,h k,h
√
where we have Λ ≽ λI and ∥zk∥ ≤ H d.
k,h h 2 √
Bound term I : we apply Lemma F.9 with the optimistic value function class V and ϵ = H λ/K,
2 h
then for any fixed h ∈ [H], with probability at least 1−δ/3H, for all episode k ∈ [K], we have
(cid:13)k−1 (cid:13)
I 2 = (cid:13) (cid:13) (cid:13)(cid:88) ϕ(cid:0) sτ h,aτ h(cid:1)(cid:0) Vˆ kρ ,h+1(cid:0) sτ h+1(cid:1) −P0 hVˆ kρ ,h+1(sτ h,aτ h)(cid:1)(cid:13) (cid:13)
(cid:13)
τ=1
Λ− k,1
h
(cid:115)
(cid:20) d (cid:18) k+λ(cid:19) N (cid:21) 8k2ε2
≤ 4H2 log +log ε +
2 λ δ λ
√
(cid:0) (cid:1)
≤ O(cid:101) d3H3 ,
where the first inequality holds because of Lemma F.9, the second inequality holds from Lemma C.1.
Thus we have
√ √
(cid:13) (cid:13)zˆ hk ,1−z hk(cid:13) (cid:13) Λ ≤ I 1+I 2 = O(cid:101)(cid:0) H dλ+ d3H3(cid:1) = β¯.
k,h
Therefore, the estimation error can be bounded by
(cid:12) (cid:12)ϕ(s,a)⊤zˆk −(cid:2)P0Vˆρ (cid:3) (s,a)(cid:12) (cid:12) = (cid:12) (cid:12)ϕ(s,a)⊤zˆk −ϕ(s,a)⊤zk(cid:12) (cid:12)
h,1 h k,h+1 h,1 h
≤ (cid:13) (cid:13)zˆ hk ,1−z hk(cid:13) (cid:13) Λ ·∥ϕ(s,a)∥ Λ−1
k,h k,h
(cid:113)
≤ β¯ ϕ(s,a)⊤Λ−1ϕ(s,a),
k,h
where the first inequality holds from Cauchy-Schwarz inequality. Similarly, for the pessimistic
function class Vˇ (or squared value function class V2), we have the similar result as follows
h h
(cid:113)
(cid:12) (cid:12)ϕ(s,a)⊤zˇk −(cid:2)P0Vˇρ (cid:3) (s,a)(cid:12) (cid:12) ≤ β¯ ϕ(s,a)⊤Λ−1ϕ(s,a),
h,1 h k,h+1 k,h
22(cid:113)
(cid:12) (cid:12)ϕ(s,a)⊤z (cid:101)hk ,2−(cid:2)P0 h(cid:0) Vˆ kρ ,h+1(cid:1)2(cid:3) (s,a)(cid:12) (cid:12) ≤ β(cid:101) ϕ(s,a)⊤Λ− k,1 hϕ(s,a),
√ √ √ √
where β¯= O(cid:101)(cid:0) H dλ+ d3H3(cid:1) and β(cid:101)= O(cid:101)(cid:0) H2 dλ+ d3H6(cid:1). By taking union bound over h ∈ [H]
and three function classes, we have that the event E¯ holds with probability at least 1−δ. This
completes the proof.
C.2 Proof of Lemma B.3
Proof of Lemma B.3. First, recall from (4.6), we have
(cid:2)V Vˆρ (cid:3) (s,a) ≈ (cid:2)V¯ Vˆρ (cid:3) (s,a) = (cid:2) ϕ(s,a)⊤zk (cid:3) −(cid:2) ϕ(s,a)⊤zˆk (cid:3)2 ,
h k,h+1 h k,h+1 (cid:101)h,2 [0,H2] h,1 [0,H]
where zˆk and zk is the solution of the following ridge regression problems
h,1 (cid:101)h,2
k−1
zk =
argmin(cid:88)(cid:16)
z⊤ϕ(cid:0) sτ,aτ(cid:1) −(cid:0) Vˆρ (cid:0) sτ
(cid:1)(cid:1)2(cid:17)2
+λ∥z∥2,
(cid:101)h,2 h h k,h+1 h+1 2
z∈Rd
τ=1
k−1
zˆk =
argmin(cid:88)(cid:16)
z⊤ϕ(cid:0) sτ,aτ(cid:1) −Vˆρ (cid:0) sτ
(cid:1)(cid:17)2
+λ∥z∥2.
h,1 h h k,h+1 h+1 2
z∈Rd
τ=1
Then we have
(cid:12) (cid:12)(cid:2)V¯ hVˆ kρ ,h+1(cid:3)(cid:0) sk h,ak h(cid:1) −[V hVˆ kρ ,h+1](cid:0) sk h,ak h(cid:1)(cid:12)
(cid:12)
(cid:12) (cid:12)
≤(cid:12)(cid:2) ϕ(cid:0) sk,ak(cid:1)⊤ zk (cid:3) −(cid:2) ϕ(cid:0) sk,ak(cid:1)⊤ zˆk (cid:3)2 −(cid:2)P0(cid:0) Vˆρ (cid:1)2(cid:3)(cid:0) sk,ak(cid:1) +(cid:0)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1)(cid:1)2(cid:12)
(cid:12) h h (cid:101)h,2 [0,H2] h h h,1 [0,H] h k,h+1 h h h k,h+1 h h (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
≤(cid:12)(cid:2) ϕ(cid:0) sk,ak(cid:1)⊤ zk (cid:3) −(cid:2)P0(cid:0) Vˆρ (cid:1)2(cid:3)(cid:0) sk,ak(cid:1)(cid:12)+(cid:12)(cid:2) ϕ(cid:0) sk,ak(cid:1)⊤ zˆk (cid:3)2 −(cid:0)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1)(cid:1)2(cid:12)
(cid:12) h h (cid:101)h,2 [0,H2] h k,h+1 h h (cid:12) (cid:12) h h h,1 [0,H] h k,h+1 h h (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
≤(cid:12)(cid:2) ϕ(cid:0) sk,ak(cid:1)⊤ zk (cid:3) −(cid:2)P0(cid:0) Vˆρ (cid:1)2(cid:3)(cid:0) sk,ak(cid:1)(cid:12)+2H(cid:12)(cid:2) ϕ(cid:0) sk,ak(cid:1)⊤ zˆk (cid:3) −(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1)(cid:12)
(cid:12) h h (cid:101)h,2 [0,H2] h k,h+1 h h (cid:12) (cid:12) h h h,1 [0,H] h k,h+1 h h (cid:12)
≤min(cid:110) β(cid:101)(cid:13) (cid:13)ϕ(cid:0) sk,ak(cid:1)(cid:13)
(cid:13)
,H2(cid:111) +min(cid:110) 2Hβ¯(cid:13) (cid:13)ϕ(cid:0) sk,ak(cid:1)(cid:13)
(cid:13)
,H2(cid:111)
h h Λ−1 h h Λ−1
k,h k,h
=E ,
k,h
where the last inequality holds from Lemma B.2. For the second result, we have
(cid:12) (cid:12)(cid:2)V hVˆ kρ ,h+1(cid:3)(cid:0) sk h,ak h(cid:1) −[V hV h∗ +,ρ 1](cid:0) sk h,ak h(cid:1)(cid:12) (cid:12)
(cid:12) (cid:12)
=(cid:12)(cid:2)P0(cid:0) Vˆρ (cid:1)2(cid:3)(cid:0) sk,ak(cid:1) −(cid:0)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1)(cid:1)2 −(cid:2)P0(cid:0) V∗,ρ(cid:1)2(cid:3)(cid:0) sk,ak(cid:1) +(cid:0)(cid:2)P0V∗,ρ(cid:3)(cid:0) sk,ak(cid:1)(cid:1)2(cid:12)
(cid:12) h k,h+1 h h h k,h+1 h h h h+1 h h h h+1 h h (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
≤(cid:12)(cid:2)P0(cid:0) Vˆρ (cid:1)2(cid:3)(cid:0) sk,ak(cid:1) −(cid:2)P0(cid:0) V∗,ρ(cid:1)2(cid:3)(cid:0) sk,ak(cid:1)(cid:12)+(cid:12)(cid:0)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1)(cid:1)2 −(cid:0)(cid:2)P0V∗,ρ(cid:3)(cid:0) sk,ak(cid:1)(cid:1)2(cid:12)
(cid:12) h k,h+1 h h h h+1 h h (cid:12) (cid:12) h k,h+1 h h h h+1 h h (cid:12)
(cid:12) (cid:12)
≤4H(cid:12)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1) −(cid:2)P0V∗,ρ(cid:3)(cid:0) sk,ak(cid:1)(cid:12)
(cid:12) h k,h+1 h h h h+1 h h (cid:12)
≤4H(cid:16)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1) −(cid:2)P0V∗,ρ(cid:3)(cid:0) sk,ak(cid:1)(cid:17)
h k,h+1 h h h h+1 h h
≤4H(cid:16)(cid:2)P0Vˆρ (cid:3)(cid:0) sk,ak(cid:1) −(cid:2)P0Vˇρ (cid:3)(cid:0) sk,ak(cid:1)(cid:17)
h k,h+1 h h h k,h+1 h h
≤min(cid:110) 4H(cid:16) ϕ(cid:0) sk,ak(cid:1)⊤ zˆk −ϕ(cid:0) sk,ak(cid:1)⊤ zˇk +2β¯(cid:13) (cid:13)ϕ(cid:0) sk,ak(cid:1)(cid:13)
(cid:13)
(cid:17) ,H2(cid:111)
h h h,1 h h h,1 h h Λ−1
k,h
=D .
k,h
23where the second inequality holds because 0 ≤ V∗,ρ,Vˆρ ≤ H, the third and fourth inequality
h+1 k,h+1
holds because of Lemma B.5, the fifth inequality holds due to Lemma B.2 and the last inequality
holds because the trivial result 0 ≤ (cid:2)V¯ Vˆρ (cid:3)(cid:0) sk,ak(cid:1) ,[V V∗,ρ](cid:0) sk,ak(cid:1) ≤ H2. Thus we have
h k,h+1 h h h h+1 h h
(cid:12) (cid:12)(cid:2)V¯ hVˆ kρ ,h+1(cid:3)(cid:0) sk h,ak h(cid:1) −[V hV h∗ +,ρ 1](cid:0) sk h,ak h(cid:1)(cid:12) (cid:12) ≤ E k,h+D k,h.
Then we also have
σ¯2 ≥ (cid:2)V¯ Vˆρ (cid:3)(cid:0) sk,ak(cid:1) +E +D ≥ (cid:2)V V∗,ρ(cid:3)(cid:0) sk,ak(cid:1) ,
k,h h k,h+1 h h k,h k,h h h+1 h h
where we use the definition of σ¯ in (4.6). This completes the proof.
k,h
C.3 Proof of Lemma B.4
Proof of Lemma B.4. For all (s,a) ∈ S/{s }×A, for stage h ≤ h′ ≤ H (we use h to replace h′ in
f
this part for simplicity), we have
Qπ,ρ(s,a) = r (s,a)+ϕ(s,a)⊤νπ,ρ = r (s,a)+ inf (cid:2)P Vπ,ρ(cid:3) (s,a).
h h h h P (·|s,a)∈Uρ(s,a;µ0) h h+1
h h h
We first decompose the gap νˆρ,k −νπ,ρ into two terms
h h
νˆρ,k −νπ,ρ = νˆρ,k −ν˜ρ,k+ν˜ρ,k −νπ,ρ, (C.2)
h h h h h h
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I II
where ν˜ hρ,k = (cid:2) ν˜ hρ ,, ik(cid:3) i∈[d], and ν˜ hρ ,, ik = max α∈[0,H](cid:8)Eµ0 h,i(cid:2) Vˆ kρ ,h+1(s)(cid:3) α−ρα(cid:9). Then we will bound these
two terms separately.
Bound term I in (C.2): we have
(cid:104) (cid:110) (cid:104) (cid:105) (cid:111)(cid:105)
νˆρ,k −ν˜ρ,k ≤ max zˆk (α)−Eµ0 h,i Vˆρ (s) .
h h h,i k,h+1
α∈[0,H] α i∈[d]
Denote αk = argmax (cid:8) zˆk (α)−Eµ0 h,i(cid:2) Vˆρ (s)(cid:3) (cid:9) , i = 1,··· ,d. Then we have
i α∈[0,H] h,i k,h+1 α
νˆ hρ,k−ν˜ hρ,k ≤(cid:20)(cid:16) Σ− k,1 hk τ(cid:88)− =11 σ¯ τ− ,h2ϕ(cid:0) sτ h,aτ h(cid:1)(cid:2) Vˆ kρ ,h+1(cid:0) sτ h+1(cid:1)(cid:3)
αk
i(cid:17) i−(cid:16) Eµ0 h(cid:2) Vˆ kρ ,h+1(s)(cid:3)
αk
i(cid:17) i(cid:21)
i∈[d]
=(cid:20)(cid:16) −λΣ− k,1 hEµ0 h(cid:2) Vˆ kρ ,h+1(s)(cid:3)
αk
i(cid:17) i+(cid:16) Σ− k,1 hk (cid:88)−1 σ¯ τ− ,h2ϕτ h(cid:104)(cid:2) Vˆ kρ ,h+1(sτ h+1)(cid:3)
αk
i
τ=1
−(cid:2)P0(cid:2) Vˆρ (cid:3) (cid:3) (sτ,aτ)(cid:105)(cid:17) (cid:21) . (C.3)
h k,h+1 αk
i
h h
i i∈[d]
For the first term on the RHS of (C.3),
(cid:12)(cid:28) (cid:20)(cid:18) (cid:19) (cid:21) (cid:29)(cid:12)
(cid:12) (cid:12) ϕ(s,a), −λΣ−1Eµ0 h(cid:104) Vˆρ (s)(cid:105) (cid:12) (cid:12)
(cid:12) k,h k,h+1 αk (cid:12)
i i i∈[d]
24(cid:12) d (cid:12)
= (cid:12) (cid:12) (cid:12)(cid:88) ϕ i(s,a)1⊤
i
(−λ)Σ− k,1 hEµ0 h(cid:104) Vˆ kρ ,h+1(s)(cid:105) αk(cid:12) (cid:12)
(cid:12)
i=1 i
≤ λ(cid:88) i=d 1(cid:113) ϕ i(s,a)1⊤
i
Σ− k,1 hϕ i(s,a)1 i·(cid:13) (cid:13) (cid:13) (cid:13)Eµ0 h(cid:104) Vˆ kρ ,h+1(s)(cid:105)
αk
i(cid:13) (cid:13) (cid:13)
(cid:13) Σ− k,1
h
√ (cid:88)d (cid:113)
≤ λdH ϕ (s,a)1⊤Σ−1ϕ (s,a)1 , (C.4)
i i k,h i i
i=1
where 1 is the vector with the i-th entry being 1 and else being 0. The first inequality holds due to
i
the Cauchy-Schwarz inequality.
For the second term on the RHS of (C.3), given the event E defined in Definition B.1, we have
h
(cid:12) (cid:12) (cid:12)(cid:28) ϕ(s,a),(cid:20)(cid:18) Σ−1 k (cid:88)−1 σ¯−2ϕτ(cid:20)(cid:104) Vˆρ (sτ )(cid:105) −(cid:104) P0(cid:104) Vˆρ (cid:105) (cid:105) (sτ,aτ)(cid:21)(cid:19) (cid:21) (cid:29)(cid:12) (cid:12)
(cid:12)
(cid:12) k,h
τ=1
τ,h h k,h+1 h+1 αk
i
h k,h+1 αk
i
h h
i i∈[d]
(cid:12)
=(cid:12) (cid:12) (cid:12) (cid:12)(cid:88)d ϕ i(s,a)1⊤
i
Σ− k,1 hk (cid:88)−1 σ¯ τ− ,h2ϕτ h(cid:20)(cid:104) Vˆ kρ ,h+1(sτ h+1)(cid:105)
αk
−(cid:104) P0 h(cid:104) Vˆ kρ ,h+1(cid:105) αk(cid:105) (sτ h,aτ h)(cid:21)(cid:12) (cid:12) (cid:12)
(cid:12)
i=1 τ=1 i i
≤(cid:88) i=d 1(cid:113) ϕ i(s,a)1⊤
i
Σ− k,1 hϕ i(s,a)1 i·(cid:13) (cid:13) (cid:13) (cid:13)k τ(cid:88)− =11 σ¯ τ− ,h2ϕτ h(cid:104)(cid:2) Vˆ kρ ,h+1(sτ h+1)(cid:3)
αk i
−(cid:2)P0 h(cid:2) Vˆ kρ ,h+1(cid:3)
αk
i(cid:3) (sτ h,aτ h)(cid:105)(cid:13) (cid:13) (cid:13)
(cid:13) Σ− k,1
h
(cid:88)d (cid:113)
≤γ ϕ (s,a)1⊤Σ−1ϕ (s,a)1 , (C.5)
i i k,h i i
i=1
where the first inequality follows from Cauchy-Schwarz inequality and the second inequality holds
from the event E . Combining (C.3), (C.4) and (C.5), we have
h
(cid:10) ϕ(s,a),νˆρ,k −ν˜ρ,k(cid:11) ≤
(cid:16)√
λdH
+γ(cid:17)(cid:88)d
ϕ
(s,a)(cid:113)
1⊤Σ−11 ,
h h i i k,h i
i=1
On the other hand, we can similarly do analysis for ⟨ϕ(s,a),ν˜ρ,k −νˆρ,k⟩. Then we have
h h
(cid:12) (cid:12)⟨ϕ(s,a),νˆ hρ,k −ν˜ hρ,k⟩(cid:12) (cid:12) ≤
β(cid:88)d
ϕ
i(s,a)(cid:113)
1⊤
i
Σ− k,1 h1 i, (C.6)
i=1
(cid:16)√ (cid:17) (cid:16)√ √ (cid:17)
where β = λdH +γ = O(cid:101) λdH + d .
Bound term II in (C.2): we have
(cid:10) ϕ(s,a),ν˜ρ,k−νπ,ρ(cid:11) = inf (cid:104) P Vˆρ (cid:105) (s,a)− inf (cid:104) P Vπ,ρ(cid:105) (s,a).
h h Ph(·|s,a)∈U hρ(s,a;µ0 h) h k,h+1 Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1
Finally we have
(cid:0) r (s,a)+ϕ(s,a)⊤νˆρ,k(cid:1) −Qπ,ρ(s,a)
h h h
= ⟨ϕ(s,a),νˆρ,k −ν˜k,ρ+ν˜k,ρ−νπ,ρ⟩
h h h h
= inf (cid:2)P Vˆρ (cid:3) (s,a)− inf (cid:2)P Vπ,ρ(cid:3) (s,a)+∆k(s,a),
P (·|s,a)∈Uρ(s,a;µ0) h k,h+1 P (·|s,a)∈Uρ(s,a;µ0) h h+1 h
h h h h h h
(cid:113)
where |∆k(s,a)| ≤ β(cid:80)d ϕ (s,a) 1⊤Σ−11 . This completes the proof.
h i=1 i i k,h i
25C.4 Proof of Lemma B.5
Proof of Lemma B.5. We prove this lemma by induction. For last stage H +1, it is trivial because
for all (s,a) ∈ S ×A, we have Qˆρ (s,a) = Q∗,ρ (s,a) = Qˇρ (s,a) = 0.
k,H+1 H+1 k,H+1
Assume that the lemma holds at stage h′+1, now consider the situation at stage h′ (we use h to
replace h′ in this part for simplicity). For all episode k ∈ [K], we have
r (s,a)+ϕ(s,a)⊤νˆρ,k+Γˆ (s,a)−Q∗,ρ(s,a)
h h k,h h
≥ inf (cid:2)P Vˆρ (cid:3) (s,a)− inf (cid:2)P V∗,ρ(cid:3) (s,a)+∆k(s,a)+Γˆ (s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h) h k,h+1 Ph(·|s,a)∈U hρ(s,a;µ0 h) h h+1 h k,h
≥ inf (cid:2)P (cid:0) Vˆρ −V∗,ρ(cid:1)(cid:3) (s,a)
Ph(·|s,a)∈U hρ(s,a;µ0 h) h k,h+1 h+1
≥0,
where the first inequality holds from Lemma B.4, the second inequality holds because |∆k(s,a)| ≤
h
Γˆ (s,a), the third inequality holds from induction assumption. Thus we have
k,h
(cid:110) (cid:111)
Q∗,ρ(s,a) ≤ min minr (s,a)+ϕ(s,a)⊤νˆρ,i+Γˆ (s,a),H −h+1 ≤ Qˆρ (s,a).
h h h i,h k,h
i∈[k]
Thus for value function V, we have
Vˆρ (s) = maxQˆρ (s,a) ≥ maxQ∗,ρ(s,a) = V∗,ρ(s).
k,h k,h h h
a a
For the pessimistic value function Qˇρ (s,a), we can do the similar analysis. Finally, by induction,
k,h
we finish the proof.
C.5 Proof of Lemma B.6
Proof of Lemma B.6. We use backward induction to prove this lemma. For the base case, the stage
H, it is trivial to obtain (B.1) because Vˆρ = 0. Assume (B.1) hold for the stage h+1, then we
k,H+1
consider the stage h.
For all episode k ∈ [K], we first do the following decomposition
(cid:13)k−1 (cid:13)
(cid:13) (cid:13)(cid:88) σ¯−2ϕτ(cid:104) (cid:2) Vˆρ (sτ )(cid:3) −(cid:2)P0(cid:2) Vˆρ (cid:3) (cid:3) (sτ,aτ)(cid:105)(cid:13)
(cid:13)
(cid:13) τ,h h k,h+1 h+1 α′ h k,h+1 α′ h h (cid:13)
τ=1
Σ− k,1
h
(cid:13)k−1 (cid:13)
≤ (cid:13) (cid:13)(cid:88) σ¯−2ϕτ(cid:104) (cid:2) V∗,ρ(sτ )(cid:3) −(cid:2)P0(cid:2) V∗,ρ(cid:3) (cid:3) (sτ,aτ)(cid:105)(cid:13) (cid:13)
(cid:13) τ,h h h+1 h+1 α′ h h+1 α′ h h (cid:13)
τ=1
Σ− k,1
h
(cid:124) (cid:123)(cid:122) (cid:125)
J1
(cid:13)k−1 (cid:13)
+(cid:13) (cid:13) (cid:13)(cid:88) σ¯ τ− ,h2ϕτ h(cid:104) ∆ α′Vˆ kρ ,h+1(sτ h+1)−(cid:2)P0 h(cid:0) ∆ α′Vˆ kρ ,h+1(cid:1)(cid:3) (sτ h,aτ h)(cid:105)(cid:13) (cid:13)
(cid:13)
, (C.7)
τ=1
Σ− k,1
h
(cid:124) (cid:123)(cid:122) (cid:125)
J2
where ∆ Vˆρ (sτ ) = (cid:2) Vˆρ (sτ )(cid:3) −(cid:2) V∗,ρ(sτ )(cid:3) .
α′ k,h+1 h+1 k,h+1 h+1 α′ h+1 h+1 α′
26Bound term J in (C.7): For term J , we apply Lemma F.8 with x = σ¯−1ϕ(cid:0) si,ai(cid:1) and
1 1 i i,h h h
η = σ¯−1(cid:0)(cid:2) V∗,ρ(si )(cid:3) −(cid:2)P0(cid:2) V∗,ρ(cid:3) (cid:3)(cid:0) si,ai(cid:1)(cid:1). Note that based on Lemma B.3, we have σ¯2 ≥
i i,h h+1 h+1 α′ h h+1 α′ h h k,h
(cid:2)V V∗,ρ(cid:3)(cid:0) sk,ak(cid:1) ≥ (cid:2)V (cid:2) V∗,ρ(cid:3) (cid:3)(cid:0) sk,ak(cid:1). Then for x and η , we have
h h+1 h h h h+1 α′ h h i i
∥x i∥ 2 = (cid:13) (cid:13)ϕ(cid:0) si h,ai h(cid:1)(cid:13) (cid:13) 2/σ¯ i,h ≤ 1,
E[η i|F i] = 0,|η i| ≤ (cid:12) (cid:12)σ¯ i− ,h1(cid:0)(cid:2) V h∗ +,ρ 1(si h+1)(cid:3)
α′
−(cid:2)P0 h(cid:2) V h∗ +,ρ 1(cid:3) α′(cid:3)(cid:0) si h,ai h(cid:1)(cid:1)(cid:12) (cid:12) ≤ 2H,
E[η2|F ] = E(cid:2) σ¯−2(cid:0)(cid:2) V∗,ρ(si )(cid:3) −(cid:2)P0(cid:2) V∗,ρ(cid:3) (cid:3)(cid:0) si,ai(cid:1)(cid:1)2(cid:3) ≤ 1 = σ2,
i i i,h h+1 h+1 α′ h h+1 α′ h h
(cid:110) (cid:111) (cid:110) (cid:111) √
max |η |·min(cid:8) 1,∥x ∥ (cid:9) ≤ max 2Hσ¯−1∥x ∥ ≤ d,
i i Σ−1 i,h i Σ−1
1≤i≤k i,h 1≤i≤k i,h
where we use the definition of σ¯ in (4.7). Then for all k ∈ [K], with probability at least 1−δ/2H,
i,h
we have
(cid:13) (cid:13)(cid:88)k−1 (cid:13) (cid:13) (cid:16) √ (cid:8) (cid:9)(cid:17) (cid:0)√ (cid:1)
J 1 = (cid:13)
(cid:13)
i=1x iη i(cid:13)
(cid:13)
Σ− k,1
h
≤ O(cid:101) σ d+ 1m ≤ia ≤x k|η i|min 1,∥x i∥ Σ− i,h1 = O(cid:101) d .
Bound term J in (C.7): To bound term J , we need to use ϵ-covering for function class
2 2
V(cid:101)h+1 − (cid:2) V h∗ +,ρ 1(cid:3)
α′
where V(cid:101)h+1 = (cid:8) [V] α|V ∈ V h+1,α ∈ [0,H](cid:9) is the truncated optimistic value
function class. For any two function V(cid:101)1,V(cid:101)2 ∈ V(cid:101)h+1, we can write that
V(cid:101)1 = [V 1] α1,V(cid:101)2 = [V 2] α2,
where V ,V ∈ V , α ,α ∈ [0,H]. Then we have
1 2 h+1 1 2
dist(V(cid:101)1,V(cid:101)2) = sup(cid:12) (cid:12)V(cid:101)1(s)−V(cid:101)2(s)(cid:12) (cid:12)
s
(cid:12) (cid:12)
= sup(cid:12)[V 1] α1(s)−[V 2] α2(s)(cid:12)
s
(cid:12) (cid:12) (cid:12) (cid:12)
≤ sup(cid:12)[V 1] α1(s)−[V 1] α2(s)(cid:12)+sup(cid:12)[V 1] α2(s)−[V 2] α2(s)(cid:12)
s s
≤ |α −α |+dist(V ,V ).
1 2 1 2
This indicates that the ϵ-covering number N(cid:101)ϵ for function class V(cid:101)h+1 can be bounded by
N(cid:101)ϵ ≤ N 1,ϵ ·N 2,ϵ,
2 2
where N 1,ϵ is the 2ϵ-covering number for optimistic value function class V h+1 and N 2,ϵ is the
ϵ-covering2number for closed interval [0,H]. Then based on Lemma C.1 and Lemma F.7, w2e have
2
√
logN(cid:101)ϵ ≤
dℓlog(1+8L/ϵ)+d2ℓlog(cid:0)
1+32
dβ2/λϵ2(cid:1)
+log(6H/ϵ),
√
where ℓ = dHlog(1+K/λ) and L = 2H(cid:112) dK/λ. Here we set ϵ = λ/4H2d3K, then the covering
entropy can be bounded by
logN(cid:101)ϵ ≤ O(cid:101)(d3H).
27For simplicity, we denote ∆ α′Vˆ kρ
,h+1
here as ∆V, then for ∆V, there exsit a function V(cid:101) in the ϵ-net
satisfies that
dist(cid:0) ∆V,V(cid:101)(cid:1)
≤ ϵ.
Then the difference of the variance of ∆V and V(cid:101) can be bounded by
(cid:2)V hV(cid:101)(cid:3)(cid:0) sk h,ak h(cid:1) −(cid:2)V h∆V(cid:3)(cid:0) sk h,ak h(cid:1)
= (cid:2)P hV(cid:101)2(cid:3)(cid:0) sk h,ak h(cid:1) −(cid:2)P h(∆V)2(cid:3)(cid:0) sk h,ak h(cid:1) −(cid:0)(cid:2)P hV(cid:101)(cid:3)(cid:0) sk h,ak h(cid:1)(cid:1)2 +(cid:0)(cid:2)P h(∆V)(cid:3)(cid:0) sk h,ak h(cid:1)(cid:1)2
(cid:12) (cid:12) (cid:12) (cid:12)
≤ 2sup(cid:12)∆V(s)−V(cid:101)(s)(cid:12)·sup(cid:12)∆V(s)+V(cid:101)(s)(cid:12)
s s
≤
4Hdist(cid:0) ∆V,V(cid:101)(cid:1)
1
≤ .
2d3H
This indicates that
(cid:2)V hV(cid:101)(cid:3)(cid:0) sk h,ak h(cid:1) ≤ (cid:2)V h∆V(cid:3)(cid:0) sk h,ak h(cid:1) + 2d1
3H
≤ (cid:104) P (cid:0)(cid:2) Vˆρ (cid:3) −(cid:2) V∗,ρ(cid:3) (cid:1)2(cid:105) (cid:0) sk,ak(cid:1) + 1
h k,h+1 α′ h+1 α′ h h 2d3H
≤ (cid:104) P (cid:0) Vˆρ −V∗,ρ(cid:1)2(cid:105) (cid:0) sk,ak(cid:1) + 1
h k,h+1 h+1 h h 2d3H
≤ 2H(cid:104) P (cid:0) Vˆρ −Vˇρ (cid:1)(cid:105) (cid:0) sk,ak(cid:1) + 1
h k,h+1 k,h+1 h h 2d3H
≤ 2H(cid:16) P Vˆρ (cid:0) sk,ak(cid:1) −P Vˇρ (cid:0) sk,ak(cid:1)(cid:17) + 1
h k,h+1 h h h k,h+1 h h 2d3H
1
≤ D +
k,h 2d3H
≤ σ¯2 /d3H, (C.8)
k,h
where the fourth inequality holds due to Lemma B.5 with induction assumption E , the sixth
h+1
inequality holds due to the definition of D and the last inequality holds because of the definition
k,h
of σ k,h. Then we apply we apply Lemma F.8 with x
i
= σ¯ i− ,h1ϕ(cid:0) si h,ai h(cid:1) and η
i
= σ¯ i− ,h1(cid:0) V(cid:101)(si h+1)−
P0 hV(cid:101)(cid:0) si h,ai h(cid:1)(cid:1). For x
i
and η i, we have
∥x i∥ 2 ≤ (cid:13) (cid:13)ϕ(cid:0) si h,ai h(cid:1)(cid:13) (cid:13) 2/σ¯ i,h ≤ 1,
E[η i|F i] = 0,|η i| ≤ (cid:12) (cid:12)σ¯ i− ,h1(cid:0) V(cid:101)(si h+1)−P0 hV(cid:101)(cid:0) si h,ai h(cid:1)(cid:1)(cid:12) (cid:12) ≤ 2H,
E[η i2|F i] = σ¯ i− ,h2(cid:2)V hV(cid:101)(cid:3)(cid:0) si h,ai h(cid:1) ≤ 1/d3H,
(cid:110) (cid:111) (cid:110) (cid:111)
max |η |·min(cid:8) 1,∥x ∥ (cid:9) ≤ max 2Hσ¯−1∥x ∥ ≤ 1/d3H,
i i Σ−1 i,h i Σ−1
1≤i≤k i,h 1≤i≤k i,h
where we use the construction of σ¯ in (4.7) and (C.8). After taking union probability bound over
i,h
ϵ-covering for function class V(cid:101)h+1−(cid:2) V h∗ +,ρ 1(cid:3) α′, we have
(cid:13) (cid:13) (cid:13)(cid:88)k−1 σ¯−2ϕτ(cid:104) V(cid:101)(sτ )−(cid:2)P0V(cid:101)(cid:3) (sτ,aτ)(cid:105)(cid:13) (cid:13) (cid:13) ≤ O(cid:101)(cid:0)√ d(cid:1) .
(cid:13) τ,h h h+1 h h h (cid:13)
τ=1
Σ− k,1
h
28For simplicity, we denote that V¯ = ∆V −V(cid:101) = ∆ α′Vˆ kρ ,h+1−V(cid:101) and have sup s|V¯(s)| ≤ ϵ. Then we
obtain
(cid:13)k−1 (cid:13)
J 2 = (cid:13) (cid:13) (cid:13)(cid:88) σ¯ τ− ,h2ϕτ h(cid:104) ∆ α′Vˆ kρ ,h+1(sτ h+1)−(cid:2)P0 h(cid:0) ∆ α′Vˆ kρ ,h+1(cid:1)(cid:3) (sτ h,aτ h)(cid:105)(cid:13) (cid:13)
(cid:13)
τ=1
Σ− k,1
h
(cid:13)k−1 (cid:13)
≤ 2(cid:13) (cid:13)(cid:88) σ¯−2ϕτ(cid:104) V(cid:101)(sτ )−(cid:2)P0V(cid:101)(cid:3) (sτ,aτ)(cid:105)(cid:13) (cid:13)
(cid:13) τ,h h h+1 h h h (cid:13)
τ=1
Σ− k,1
h
(cid:13)k−1 (cid:13)
+2(cid:13) (cid:13)(cid:88) σ¯−2ϕτ(cid:104) V¯(sτ )−(cid:2)P0V¯(cid:3) (sτ,aτ)(cid:105)(cid:13)
(cid:13)
(cid:13) τ,h h h+1 h h h (cid:13)
√ τ=1 √
Σ− k,1
h
(cid:0) (cid:1)
≤ O(cid:101) d +4ϵk/ λ
√
(cid:0) (cid:1)
≤ O(cid:101) d ,
√
where we use that ϵ = λ/4H2d3K. Finally, we have
(cid:13)k−1 (cid:13)
(cid:13) (cid:13) (cid:13)(cid:88) σ¯ τ− ,h2ϕτ h(cid:104) (cid:2) Vˆ kρ ,h+1(sτ h+1)(cid:3) α′ −(cid:2)P0 h(cid:2) Vˆ kρ ,h+1(cid:3) α′(cid:3) (sτ h,aτ h)(cid:105)(cid:13) (cid:13) (cid:13) = J 1+J 2 ≤ γ,
τ=1
Σ− k,1
h
√
where γ = O(cid:101)(cid:0) d(cid:1). Thus, by induction we complete the proof.
C.6 Proof of Lemma B.7
Proof of Lemma B.7. Conditioned on the event E and E¯, to bound the weight σ¯2 , recall from the
k,h
definition (4.7), we have
σ¯ k,h = max(cid:110) σ k,h,1,√ 2d3H2(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Σ1 2 −1(cid:111) ,
k,h
According to (4.6), we have
σ2 = (cid:2)V¯ Vˆρ (cid:3)(cid:0) sk,ak(cid:1) +E +d3H ·D + 1 ,
k,h h k,h+1 h h k,h k,h 2
where E ,D are defined as follows
k,h k,h
E k,h = min(cid:110) β(cid:101)(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ−1,H2(cid:111) +min(cid:110) 2Hβ¯(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ−1,H2(cid:111) ,
k,h k,h
D k,h = min(cid:110) 4H(cid:16) ϕ(cid:0) sk h,ak h(cid:1)⊤ zˆ hk ,1−ϕ(cid:0) sk h,ak h(cid:1)⊤ zˇ hk ,1+2β¯(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ−1(cid:17) ,H2(cid:111) ,
k,h
where β¯= O(cid:101)(cid:0) d23 H3 2(cid:1), β(cid:101)= O(cid:101)(cid:0) d3 2H3(cid:1) when we set λ = 1/H2. Note that
√ √ √
2d3H2(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) Λ1 2
−1
≤ 2d3H2(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13) 21 2/λ1 4 ≤ 2d3H3.
k,h
Also note that
σ2 = (cid:2)V¯ Vˆρ (cid:3)(cid:0) sk,ak(cid:1) +E +d3H ·D + 1
k,h h k,h+1 h h k,h k,h 2
291
≤ H2+2H2+d3H ·H2+
2
≤ 2d3H3.
Then we obtain the trivial upper bound α¯ for σ¯
k,h
√
σ¯ ≤ 2 d3H3 = α¯, (C.9)
k,h
Based on Lemma B.3, we have
(cid:2)V¯ Vˆρ (cid:3)(cid:0) sk,ak(cid:1) ≤ E +D +[V V∗,ρ](cid:0) sk,ak(cid:1) .
h k,h+1 h h k,h k,h h h+1 h h
Then we have
σ2 ≤ [V V∗,ρ](cid:0) sk,ak(cid:1) +2E +2d3H ·D + 1 .
k,h h h+1 h h k,h k,h 2
Next, we carefully bound σ2 and σ¯2 . To this end, we bound term E ,D when k is large
k,h k,h k,h k,h
enough. The intuition is that, when the episode k is large enough, all the error terms should be
small under the assumption (C.10).
Bound term E : Note that based on (5.2), with the same analysis as the proof of Corollary 5.3
k,h
in Liu and Xu (2024a), with probability at least 1−δ, we have
(cid:8) (cid:112) (cid:9)
λ (Λ ) ≥ max c(k−1)/d+λ− 32klog(dKH/δ),λ .
min k,h
Then when we choose k > 512d2log(dKH/δ)/c2 and note that λ = 1/H2, we have
(cid:112) c
c(k−1)/d+λ− 32klog(dKH/δ) ≥ k,
2d
which indicates that
c
λ (Λ ) ≥ k. (C.10)
min k,h
2d
Then when k > 512d2log(dKH/δ)/c2, we can calculate that
(cid:114)
(cid:13) (cid:13)ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13)
Λ− k,1
h
= (cid:13) (cid:13) (cid:13)Λ− k,h1 2ϕ(cid:0) sk h,ak h(cid:1)(cid:13) (cid:13)
(cid:13)
2
≤ (cid:113) λ max(cid:0) Λ− k,1 h(cid:1) ≤ 2 kd c, (C.11)
where in the first inequality we use the fact that ∥ϕ(s,a)∥ ≤ 1 for all (s,a) ∈ S ×A. Then when k
2
is large enough and also at least k > 512d2log(dKH/δ)/c2, we can have that
(cid:16) 1 (cid:17)
E
k,h
≤ O(cid:101) √ d2H3 .
kc
This indicates that there exists an absolute constant c > 0 such that
E
d2H3
E ≤ c √ .
k,h E
k
30Bound term D : note that zˆk and zˇk have the closed-form expression as follows
k,h h,1 h,1
k−1
zˆk = Λ−1 (cid:88) ϕ(cid:0) sτ,aτ(cid:1) Vˆρ (cid:0) sτ (cid:1) ,
h,1 k,h h h k,h+1 h+1
τ=1
k−1
zˇk = Λ−1 (cid:88) ϕ(cid:0) sτ,aτ(cid:1) Vˇρ (cid:0) sτ (cid:1) .
h,1 k,h h h k,h+1 h+1
τ=1
Thus we can calculate that
ϕ(cid:0) sk,ak(cid:1)⊤ zˆk −ϕ(cid:0) sk,ak(cid:1)⊤ zˇk
h h h,1 h h h,1
k−1
= ϕ(cid:0) sk,ak(cid:1)⊤ Λ−1 (cid:88) ϕ(cid:0) sτ,aτ(cid:1)(cid:0) Vˆρ (cid:0) sτ (cid:1) −Vˇρ (cid:0) sτ (cid:1)(cid:1) ,
h h k,h h h k,h+1 h+1 k,h+1 h+1
τ=1
(cid:13)k−1 (cid:13)
≤ (cid:13) (cid:13)ϕ(cid:0) sk,ak(cid:1)(cid:13) (cid:13) ·(cid:13) (cid:13)(cid:88) ϕ(cid:0) sτ,aτ(cid:1)(cid:0) Vˆρ (cid:0) sτ (cid:1) −Vˇρ (cid:0) sτ (cid:1)(cid:1)(cid:13) (cid:13)
h h Λ− k,1 h (cid:13)
τ=1
h h k,h+1 h+1 k,h+1 h+1 (cid:13) Λ− k,1
h
k−1
≤ (cid:13) (cid:13)ϕ(cid:0) sk,ak(cid:1)(cid:13) (cid:13) ·(cid:88)(cid:13) (cid:13)ϕ(cid:0) sτ,aτ(cid:1)(cid:13) (cid:13) ·(cid:0) Vˆρ (s˜k )−Vˇρ (s˜k )(cid:1)
h h Λ−1 h h Λ−1 k,h+1 h+1 k,h+1 h+1
k,h k,h
τ=1
(cid:114) k−1(cid:114)
≤ 2d (cid:88) 2d ·(cid:0) Vˆρ (s˜k )−Vˇρ (s˜k )(cid:1) ,
kc kc k,h+1 h+1 k,h+1 h+1
τ=1
≤ 2d/c·(cid:0) Vˆρ (s˜k )−Vˇρ (s˜k )(cid:1) , (C.12)
k,h+1 h+1 k,h+1 h+1
(cid:110) (cid:111)
where s˜k = argmax Vˆρ (s) − Vˇρ (s) , the first inequality holds because of Cauchy-
h+1 s∈S k,h+1 k,h+1
Schwarz inequality, the third inequality holds due to (C.11). Next, we bound Vˆρ (s˜k ) −
k,h+1 h+1
Vˇρ (s˜k ). The intuition is that when k is large, both Vˆρ and Vˇρ should be close to the
k,h+1 h+1 k,h+1 k,h+1
robust optimal value function. Thus, Vˆρ should be close to Vˇρ , and the closeness could
k,h+1 √ k,h+1
be quantified by the bonus terms, which is of order O(cid:101)(d/ k) under the assumption (C.10). In
particular, we have
Vˆρ (s˜k )−Vˇρ (s˜k )
k,h+1 h+1 k,h+1 h+1
= Vˆρ (s˜k )−V∗,ρ(s˜k )+V∗,ρ(s˜k )−Vˇρ (s˜k ). (C.13)
k,h+1 h+1 h+1 h+1 h+1 h+1 k,h+1 h+1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I II
Bound term I in (C.13): note that
Vˆρ (s)−V∗,ρ(s)
k,h h
= Qˆρ (cid:0) s,πk(s)(cid:1) −Q∗,ρ(cid:0) s,π∗(s)(cid:1)
k,h h h h
≤ Qˆρ (cid:0) s,πk(s)(cid:1) −Q∗,ρ(cid:0) s,πk(s)(cid:1)
k,h h h h
≤ inf (cid:2)P Vˆρ (cid:3)(cid:0) s,πk(s)(cid:1) − inf (cid:2)P V∗,ρ(cid:3)(cid:0) s,πk(s)(cid:1)
P (·|s,a)∈Uρ(s,a;µ0) h k,h+1 h P (·|s,a)∈Uρ(s,a;µ0) h h+1 h
h h h h h h
+∆k(cid:0) s,πk(s)(cid:1) +Γˆ (cid:0) s,πk(s)(cid:1)
h h k,h h
31≤ (cid:2)Pˆ (cid:0) Vˆρ −V∗,ρ(cid:1)(cid:3)(cid:0) s,πk(s)(cid:1) +2Γˆ (cid:0) s,πk(s)(cid:1) ,
h k,h+1 h+1 h k,h h
where the second inequality holds due to the definition of Qˆρ , robust Bellman equation and
k,h
Lemma B.4, P(cid:98)h(·|s,a) = arginf P h(·|s,a)∈U hρ(s,a;µ0 h,i)(cid:2)P hV h∗ +,ρ 1(cid:3) (s,a),∀(s,a) ∈ S × A. By recursively
applying it, then we have
Vˆρ (s)−V∗,ρ(s) ≤ (cid:2)Pˆ (cid:0) Vˆρ −V∗,ρ(cid:1)(cid:3)(cid:0) s,πk(s)(cid:1) +2Γˆ (cid:0) s,πk(s)(cid:1)
k,h h h k,h+1 h+1 h k,h h
H
≤ 2 (cid:88) Eπ hk ′,Pˆ(cid:2) Γˆ k,h′(s,a)|s
h′
= s(cid:3) .
h′=h
Note that by (C.9) σ¯2 ≤ α¯2, we have Σ ≽ α¯−2Λ . Similar to the analysis of (C.11), when
k,h k,h k,h
k > 512/c2log(dKH/δ), we have
(cid:88)d (cid:113)
Γˆ (s,a) = β ϕ (s,a) 1⊤Σ−11
k,h i i k,h i
i=1
≤
βα¯(cid:88)d
ϕ (cid:0)
sk,ak(cid:1)(cid:113)
1⊤Λ−11
i h h i k,h i
i=1
(cid:113)
≤ βα¯ λ
(cid:0) Λ−1(cid:1)
max k,h
(cid:114)
2d
≤ βα¯.
kc
Then we have
√
(cid:114)
2d 4β dα¯H
Vˆρ (s)−V∗,ρ(s) ≤ 2H βα¯ ≤ √ .
k,h h kc kc
Therefore, we can bound I as follows
√
4β dα¯H
I = Vˆρ (s˜k )−V∗,ρ(s˜k ) ≤ √ .
k,h+1 h+1 h+1 h+1
kc
Bound term II in (C.13): Similar to the analysis above, we can derive the similar result as follows
√
4β¯ dα¯H
II = V∗,ρ(s˜k )−Vˇρ (s˜k ) ≤ √ .
h+1 h+1 k,h+1 h+1
kc
Now we can bound that
√
ϕ(cid:0) sk,ak(cid:1)⊤ zˆk −ϕ(cid:0) sk,ak(cid:1)⊤ zˇk ≤ 2d · 4(β¯+ √β) dα¯H ≤ 16β¯ √d3/2α¯H .
h h h,1 h h h,1 c kc kc3
Then when k is large enough, we can have that
(cid:16) α¯ (cid:17)
D k,h ≤ O(cid:101) √
d3H7
2 .
k
32This indicates that there exists an absolute constant c > 0 such that
D
α¯
D
k,h
≤ c D√
d3H27
.
k
When k is large enough, we have
σ2 ≤ [V V∗,ρ](cid:0) sk,ak(cid:1) +(2E +2d3H ·D )+ 1
k,h h h+1 h h k,h k,h 2
≤ [V hV h∗ +,ρ 1](cid:0) sk h,ak h(cid:1) +2c E√α¯ kd2H3+2c D√α¯ kd6H9 2 + 1 2.
When we choose K(cid:101) = (cid:101)c·α¯2d12H9 where (cid:101)c = O(cid:101)(1). When k > K(cid:101), then we have
σ¯2 = max(cid:110) σ2 ,1,2d3H2(cid:13) (cid:13)ϕ(cid:0) sk,ak(cid:1)(cid:13) (cid:13) (cid:111)
k,h k,h h h Σ−1
k,h
≤
max(cid:8)(cid:2)V V∗,ρ(cid:3)(cid:0) sk,ak(cid:1) +1,1(cid:9)
h h+1 h h
≤
2(cid:2)V V∗,ρ(cid:0) sk,ak(cid:1)(cid:3)
.
h h+1 h h [1,H2]
Based on Lemma F.10, we have
(cid:2)V V∗,ρ(cid:3)
(s,a) ≤
(cid:16)1−(1−ρ)H−h+1(cid:17)2
≤
(cid:16)1−(1−ρ)H(cid:17)2
=
Θ(cid:16) min(cid:110) 1 ,H2(cid:111)(cid:17)
.
h h+1 ρ ρ ρ2
Then when k > K(cid:101), we have
(cid:16) (cid:110) 1 (cid:111)(cid:17)
σ¯2 ≤ O min ,H2 .
k,h ρ2
Additionally, note that α¯2 = O(cid:0) d3H3(cid:1), we have
K(cid:101) =
O(cid:101)(cid:0) d15H12(cid:1)
.
This completes the proof.
D Supporting Lemmas
Lemma D.1 (Number of value function updates). The number of episodes where the algorithm
updates the value function in Algorithm 1 is upper bounded by dHlog(1+K/λ).
Proof of Lemma D.1. This proof is the same as He et al. (2023, Lemma F.1) because of the same
rare-switching condition (Line 5 in Algorithm 1).
Lemma D.2. For any (k,h) ∈ [K]×[H], the weight νˆρ,k satisfies
h
(cid:13) (cid:13)νˆρ,k(cid:13)
(cid:13) ≤
2H(cid:112)
dk/λ.
h 2
33Proof of Lemma D.2. Denote α = argmax (cid:8) zˆk (α)−ρα(cid:9) ,i ∈ [d]. Then we have
i α∈[0,H] h,i
(cid:13) (cid:13)
(cid:13)(cid:20) (cid:21) (cid:13)
(cid:13) (cid:13)νˆρ,k(cid:13) (cid:13) = (cid:13) max {zˆk (α)−ρα} (cid:13)
h 2 (cid:13) h,i (cid:13)
(cid:13) α∈[0,H] i∈[d](cid:13)
2
(cid:13) (cid:13)
≤
ρ√ dα+(cid:13) (cid:13)(cid:20)(cid:18)
Σ−1
(cid:88)k−1
σ¯−2ϕ(cid:0) sτ,aτ(cid:1)(cid:2) Vˆρ (cid:0) sτ (cid:1)(cid:3)
(cid:19) (cid:21) (cid:13)
(cid:13)
(cid:13)
(cid:13)
k,h
τ=1
τ,h h h k,h+1 h+1 αi
i
i∈[d](cid:13)
(cid:13) 2
(cid:13) (cid:13)
≤
H√
d+H
·(cid:13)
(cid:13)Σ−1
(cid:88)k−1
σ¯−2ϕ(cid:0)
sτ,aτ(cid:1)(cid:13)
(cid:13)
(cid:13) k,h τ,h h h (cid:13)
(cid:13) (cid:13)
τ=1 2
≤
H√
d+H(cid:112)
k/λ·(cid:32) (cid:88)k−1
(cid:0) σ¯−1ϕ(cid:0) sτ,aτ(cid:1)(cid:1)⊤ Σ−1(cid:0) σ¯−1ϕ(cid:0)
sτ,aτ(cid:1)(cid:1)(cid:33)1 2
τ,h h h k,h τ,h h h
τ=1
√
(cid:112)
≤ H d+H dk/λ
(cid:112)
≤ 2H dk/λ,
where the first inequality holds due to the triangle inequality, the second inequality holds from the
fact that ρ ≤ 1, 0 ≤ α ≤ H and 0 ≤ (cid:2) Vˆρ (cid:0) sτ (cid:1)(cid:3) ≤ H, the third inequality holds because of
Lemma F.5 and the fourth inequality
holk d,h s+ b1 ecah u+ s1
e
Σαi
≽ λI and Lemma F.4. This completes the
k,h
proof.
Lemma D.3. Under a linear MDP, for any stage h ∈ [H] and any bounded function V : S → [0,H],
there always exists a vector z ∈ Rd such that for all (s,a) ∈ S ×A, we have
(cid:2)P0V(cid:3) (s,a) = z⊤ϕ(s,a),
h
√
where z satisfies that ∥z∥ ≤ H d.
2
Proof of Lemma D.3. Based on Assumption 3.1, we have
(cid:90)
(cid:2)P0V(cid:3) (s,a) = P0(s′|s,a)V(s′)ds′
h h
(cid:90)
= ϕ(s,a)⊤V(s′)dµ0(s′)
h
(cid:90)
= ϕ(s,a)⊤ V(s′)dµ0(s′)
h
= ϕ(s,a)⊤z,
where z = (cid:82) V(s′)dµ0(s′). Thus we have
h
(cid:13)(cid:90) (cid:13) √
∥z∥ 2 = (cid:13) (cid:13) (cid:13) V(s′)dµ0 h(s′)(cid:13) (cid:13) (cid:13) ≤ m sa ′xV(s′)·(cid:13) (cid:13)µ0 h(S)(cid:13) (cid:13) 2 ≤ H d.
2
This completes the proof.
E Proof of the Minimax Lower Bound
In this section, we prove the minimax lower bound. To this end, we first introduce the construction
of hard instances in Appendix E.1, and then we prove Theorem 5.5 in Appendix E.3.
34E.1 Construction of Hard Instances
We construct a family of d-rectangular linear DRMDPs based on the hard-to-learn linear MDP
√
introduced in Zhou et al. (2021a). Let δ = 1/H, ∆ = (cid:112) δ/K/(4 2). Each d-rectangular linear
DRMDP in this family is parameterized by a Boolean vector ξ = {ξ } , where ξ ∈ {−∆,∆}d.
h h∈[H−1] h
For a given ξ and uncertainty level ρ ∈ (0,3/4], the corresponding d-rectangular linear DRMDP
Mρ has the following structure. The state space S = {x ,x ,··· ,x ,x } and the action space
ξ 1 2 H H+1
A = {−1,1}d. The first state is always x . The feature mapping ϕ : S ×A → R2d+2 is defined to
1
depend on the state x through ξ as follows:
h h
 1 δ   1 δ 
− −ξ a − −ξ a
11 1 21 1
2d d  2d d 
 1 δ   1 δ 
   
− −ξ a − −ξ a
 12 2  22 2
2d d  2d d 
 .   . 
 . .   . . 
   
   
 1 δ   1 δ 
 − −ξ a   − −ξ a 
2d d 1d d 2d d 2d d
   
 1   1 
   
ϕ(x ,a) =  2 ,ϕ(x ,a) =  2 ,··· ,
1   2  
 δ   δ 
 +ξ a   +ξ a 
 d 11 1  d 21 1
   
 δ   δ 
 +ξ a   +ξ a 
 d 12 2  d 22 2
   
 . .   . . 
 .   . 
   
   
δ δ
   
 +ξ 1da d  +ξ 2da d
 d   d 
0 0
 1 δ 
− −ξ a
H−1,1 1
2d d   
 1 δ  0


− −ξ H−1,2a 2

 0
0
2d d   
  . . .    0    . . . 
  .  
  .  
 1 δ  . 0
 − −ξ a     
 2d d H−1,d d

 0

 0

 1     
  0 1
ϕ(x ,a) =  2 ,ϕ(x ,a) =  ,ϕ(x ,a) =  .
H−1 
 δ


H  0

H+1  d

 +ξ a    1
 d H−1,1 1 0  
    d
 δ  .  

 d
+ξ H−1,2a 2

 . .



. . .

     


. .
.


 0

 1

   

δ
 1 d
 
 +ξ H−1,da d 0
 d 
0
35We assume that
K ≥ 9d2H/32 and H ≥ 6, (E.1)
such that 1 − 1 −δ ≥ 0. Then it can be easily checked that for any s ∈ S, we have ϕ (s,a) ≥ 0
2d dH i
and (cid:80)2d+2ϕ (s,a) = 1. The factor distribution µ : S → R2d+2 is defined as follows.
i=1 i 1
µ (·) = (δ (·),··· ,δ (·),δ (·),δ (·),··· ,δ (·),δ (·))⊤.
1 x2 x2 x2 xH+1 xH+1 xH
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
dterms dterms
Similarly, for h = 2,...,H, we have
µ (·) = (δ (·),··· ,δ (·),δ (·),δ (·),··· ,δ (·),δ (·))⊤,
2 x3 x3 x3 xH+1 xH+1 xH
···
µ (·) = µ (·) = (δ (·),··· ,δ (·),δ (·),δ (·),··· ,δ (·),δ (·))⊤,
H−1 H xH xH xH xH+1 xH+1 xH
Note that for each episode k, the initial state sk is always x . In the nominal environment, at step
1 1
h, the state sk is either x or x . State x and x are absorbing states. Figure 3(a) illustrates
h h H+1 H H+1
the nominal MDP.
Now we construct the reward parameters {θ } as follows.
h h∈[H]
θ = (1,1,··· ,1,−1,1,1,··· ,1,0)⊤, ∀h ∈ [H].
h
We have ∀h ∈ [H],
r (x ,a) = ϕ(x ,a)⊤θ = 0,
h H H h
r (x ,a) = ϕ(x ,a)⊤θ = 0,
h h h h
r (x ,a) = ϕ(x ,a)⊤θ = 1.
h H+1 H+1 h
Thus, only the transition starting from x generates a reward of 1, and transitions starting from
H+1
any other state generate 0 reward. Next, we consider the model perturbation. An observation is
that x is the worst state since it is an absorbing state with zero reward. By the definition of
H
the d-rectangular uncertainty set, the worst case kernel is the linear combination of worst case
factor distributions. Further, by the definition of the factor uncertainty set, the worst case factor
distribution is the one that leads to the highest probability ρ to the worst state x . Thus, the worst
H
factor distributions are
µˇ =((1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,··· ,(1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,
1 x2 xH x2 xH x2 xH x2 xH
(1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,··· ,(1−ρ)δ +ρδ ,δ )⊤,
xH+1 xH xH+1 xH xH+1 xH xH
µˇ =((1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,··· ,(1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,
2 x3 xH x3 xH x3 xH x3 xH
(1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,··· ,(1−ρ)δ +ρδ ,δ )⊤,
xH+1 xH xH+1 xH xH+1 xH xH
···
µˇ =((1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,··· ,(1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,
H−1 xH−1 xH xH−1 xH xH−1 xH xH−1 xH
(1−ρ)δ +ρδ ,(1−ρ)δ +ρδ ,··· ,(1−ρ)δ +ρδ ,δ )⊤,
xH+1 xH xH+1 xH xH+1 xH xH
µˇ =µ .
H−1 H
Figure 3(b) illustrates the worst case MDP.
36x1 x2 ··· xH−1 xH 1
1−δ−⟨ξ1,a⟩ 1−δ−⟨ξ2,a⟩ 1−δ−⟨ξH−1,a⟩
δ+⟨ξ1,a⟩ δ+⟨ξ2,a⟩ δ+⟨ξH−1,a⟩
xH+1
1
(a) The nominal MDP environment.
ρ
ρ
x1
(1−ρ)(1−δ−⟨ξ1,a⟩)
x2 ··· xH−1
(1−ρ)(1−δ−⟨ξH−1,a⟩)+ρ
xH 1
(1−ρ)(1−δ−⟨ξ2,a⟩)
(1−ρ)(δ+⟨ξ1,a⟩) (1−ρ)(δ+⟨ξ2,a⟩) (1−ρ)(δ+⟨ξH−1,a⟩)
ρ
xH+1
1−ρ
(b) The worst-case MDP environment.
Figure 3: Constructions of the nominal MDP and the worst-case MDP environments.
E.2 Reduction from d-Rectangular DRMDP to Linear Bandits
Note that by construction, at steps h = 1,··· ,H − 2, the probability of transitioning to the
worst case state x is independent of the action a. Moreover, since x is the only rewarding
H H+1
state, so the optimal action at step h is the one the leads to the largest probability to x , i.e.,
H+1
a⋆ = argmax ⟨ξ ,a⟩. Further, in the nominal environment, state x can only be reached through
h a∈A h h
states x ,x ,··· ,x . As discussed by Zhou et al. (2021a), knowing the state x is equivalent to
1 2 h−1 h
knowing the entire history starting from the initial state at current episode. Consequently, policies
dictating what actions to take upon reaching a state at the beginning of an episode are equivalent
to policies relying on the “within episode” history (we refer to the discussion in E.1 of Zhou et al.
(2021a) for more details). In the following lemma, we shows that the average suboptimality of the
d-rectangular DRMDP can be lower bounded by the regret of H/2 bandit instances.
Lemma E.1. With the choice of d,K,H in (E.1), we have 3d∆ ≤ δ. Fix ξ = {ξ } . Fix a
h h∈[H−1]
possibly history dependent policy π and define a¯π = E [a |s = x ] as the expected action taken by
h ξ h h h
37the policy when it visits state x in stage h. Then, there exist a constant c > 0 such that
h
H/2
V⋆,ρ(x )−Vπ,ρ(x ) ≥
cmin(cid:110)1 ,H(cid:111)(cid:88)(cid:16)
max⟨µ ,a⟩−⟨µ
,a¯π⟩(cid:17)
.
1 1 1 1 ρ a∈A h h h
h=1
Proof of Lemma E.1. For the fixed policy π, we first get the ground truth robust value Vπ,ρ(x ) by
1 1
induction. Starting from the last step H, we have
Vπ,ρ(x ) = 0, Vπ,ρ(x ) = 1.
H H H H+1
For step H −1, we have
Vπ,ρ (x )=0, Vπ,ρ (x )=(1−ρ)(δ+⟨ξ ,aπ ⟩)·1, Vπ,ρ (x )=1+(1−ρ)·1.
H−1 H H−1 H−1 H−1 H−1 H−1 H+1
For step H −2, we have Vπ,ρ (x ) = 0 and
H−2 H
Vπ,ρ (x ) = 1+(1−ρ)·V (x ) = 1+(1−ρ)+(1−ρ)2,
H−2 H+1 H−1 H+1
Vπ,ρ (x ) = (1−ρ)(δ+⟨ξ ,a¯π ⟩)·Vπ,ρ (x )
H−2 H−2 H−2 H−2 H−1 H+1
+(1−ρ)(1−δ−⟨ξ ,a¯π ⟩)·Vπ,ρ (x )
H−2 H−2 H−1 H−1
= (cid:2) (1−ρ)+(1−ρ)2(cid:3) (δ+⟨ξ ,a¯π ⟩)
H−2 H−2
+(1−ρ)2(1−δ−⟨ξ ,a¯π ⟩)(δ+⟨ξ ,a¯π ⟩).
H−2 H−2 H−1 H−1
For step H −3, we have Vπ,ρ (x ) = 0 and
H−3 H
Vπ,ρ (x ) = 1+(1−ρ)·V (x ) = 1+(1−ρ)+(1−ρ)2+(1−ρ)3,
H−3 H+1 H−2 H+1
Vπ,ρ (x ) = (1−ρ)(δ+⟨ξ ,a¯π ⟩)·Vπ,ρ (x )
H−3 H−3 H−3 H−3 H−2 H+1
+(1−ρ)(1−δ−⟨ξ ,a¯π ⟩)·Vπ,ρ (x )
H−3 H−3 H−2 H−2
= (cid:2) (1−ρ)+(1−ρ)2+(1−ρ)3(cid:3) (δ+⟨ξ ,a¯π ⟩)
H−3 H−3
+(cid:2) (1−ρ)2+(1−ρ)3(cid:3) (1−δ−⟨ξ ,a¯π ⟩)(δ+⟨ξ ,a¯π ⟩)
H−3 H−3 H−2 H−2
+(1−ρ)3(1−δ−⟨ξ ,a¯π ⟩)(1−δ−⟨ξ ,a¯π ⟩)(δ+⟨ξ ,a¯π ⟩).
H−3 H−3 H−2 H−2 H−1 H−1
Keep performing the backward induction until step h = 1, we have
Vπ,ρ(x )
1 1
=Vπ,ρ (x )
H−(H−1) 1
=(cid:2) (1−ρ)+···+(1−ρ)H−1(cid:3) (δ+⟨ξ ,a¯π⟩)+
1 1
(cid:2) (1−ρ)2+···+(1−ρ)H−1(cid:3) (1−δ−⟨ξ ,a¯π⟩)(δ+⟨ξ ,a¯π⟩)+
1 1 2 2
[(1−ρ)3+···+(1−ρ)H−1](1−δ−⟨ξ ,a¯π⟩)(1−δ−⟨ξ ,a¯π⟩)(δ+⟨ξ ,a¯π⟩)+
1 1 2 2 3 3
+···+
(1−ρ)H−1(1−δ−⟨ξ ,a¯π⟩)(1−δ−⟨ξ ,a¯π⟩)···(1−δ−⟨ξ ,a¯π ⟩)(δ+⟨ξ ,a¯π ⟩)
1 1 2 2 H−2 H−2 H−1 H−1
H−1 H−1 h−1
(cid:88) (cid:16) (cid:88) (cid:17) (cid:89)
= (1−ρ)i (o +δ) (1−o −δ), (E.2)
h j
h=1 i=h j=1
38whereo = ⟨ξ ,a¯π⟩,∀h ∈ [H]. Recallthattheoptimalrobustactionatstephisa⋆ = argmax ⟨ξ ,a⟩,
h h h h a∈A h
and hence max ⟨ξ ,a⟩ = ∆d. Thus, we have
a∈A h
H−1 H−1 h−1
V⋆,ρ(x ) = (cid:88) (cid:16) (cid:88) (1−ρ)i(cid:17) (d∆+δ)(cid:89) (1−d∆−δ). (E.3)
1 1
h=1 i=h j=1
For k ∈ [H −1], we define
H−1 H−k h−1
(cid:88) (cid:16) (cid:88) (cid:17)(cid:89)
S = (1−ρ)i (1−o −δ)(o +δ), (E.4)
k j h
h=k i=h−k+1 j=k
H−1 H−k h−1
(cid:88) (cid:16) (cid:88) (cid:17)(cid:89)
T = (1−ρ)i (1−d∆−δ)(d∆+δ). (E.5)
k
h=k i=h−k+1 j=k
Then by (E.2), (E.3), (E.4) and (E.5), we know V⋆,ρ(x )−Vπ,ρ(x ) = T −S . Next, we aim to
1 1 1 1 1 1
lower bound T −S . Inspired by the backward induction process, we have
1 1
H−k
(cid:16) (cid:88) (cid:17)
S = (1−ρ)i (o +δ)+S (1−o −δ),
k k k+1 k
i=1
H−k
(cid:16) (cid:88) (cid:17)
T = (1−ρ)i (d∆+δ)+T (1−d∆−δ).
k k+1
i=1
Then, we have
H−k
(cid:16) (cid:88) (cid:17)
T −S = (1−ρ)i (d∆−o )−S (1−o −δ)+T (1−d∆−δ)
k k k k+1 k k+1
i=1
H−k
(cid:16) (cid:88) (cid:17)
= (1−ρ)i−T (d∆−o )+(1−o −δ)(T −S ). (E.6)
k+1 k k k+1 k+1
i=1
Define T = S = 0, then by the recursive formula (E.6), we have
H H
H−1 H−h h−1
(cid:88) (cid:16) (cid:88) (cid:17)(cid:89)
T −S = (d∆−o ) (1−ρ)i−T (1−o −δ). (E.7)
1 1 h h+1 j
h=1 i=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125)
I
To further bound (E.7), we first study the term I. Next we derive a close form expression of T . In
k
specific, we have
H−1 H−k h−1
(cid:88) (cid:16) (cid:88) (cid:17)(cid:89)
T = (1−ρ)i (1−d∆−δ)(d∆+δ)
k
h=k i=h−k+1 j=k
H−k H−k
(cid:16) (cid:88) (cid:17) (cid:16) (cid:88) (cid:17)
= (1−ρ)i (d∆+δ)+ (1−ρ)i (1−d∆−δ)(d∆+δ)
i=1 i=2
39H−k
(cid:16) (cid:88) (cid:17)
+ (1−d∆−δ)2(d∆+δ) +···+(1−ρ)H−k(1−d∆−δ)H−k−1(d∆+δ). (E.8)
i=3
Multiply T by (1−d∆−δ), we have
k
(1−d∆−δ)T
k
H−k H−k
(cid:16) (cid:88) (cid:17) (cid:16) (cid:88) (cid:17)
= (1−ρ)i (d∆+δ)(1−d∆−δ)+ (1−ρ)i (1−d∆−δ)2(d∆+δ)
i=1 i=2
H−k
(cid:16) (cid:88) (cid:17)
+ (1−d∆−δ)2(d∆+δ) +···+(1−ρ)H−k(1−d∆−δ)H−k(d∆+δ). (E.9)
i=3
Then we have
(E.8)−(E.9)
= (d∆+δ)T
k
H−k
(cid:16) (cid:88) (cid:17)
= (1−ρ)i (d∆+δ)−(1−ρ)(1−d∆−δ)(d∆+δ)−(1−ρ)2(1−d∆−δ)2(d∆+δ)
i=1
−···−(1−ρ)H−k(1−d∆−δ)H−k(d∆+δ). (E.10)
Divide both side of equation (E.10) by (d∆ + δ) and then apply the formula for the sum of a
geometric series, we know T has the following closed form expression
k
(cid:16)H (cid:88)−k (cid:17) (1−ρ)(1−d∆−δ)(1−(1−ρ)H−k(1−d∆−δ)H−k)
T = (1−ρ)i − .
k
1−(1−ρ)(1−d∆−δ)
i=1
Then, for any h ≤ H/2, we have the following bound on the term I of (E.7),
H−h
(cid:88)
(1−ρ)i−T
h+1
i=1
H (cid:88)−h H (cid:88)−h−1 (1−ρ)(1−d∆−δ)(1−(1−ρ)H−h−1(1−d∆−δ)H−h−1)
= (1−ρ)i− (1−ρ)i+
1−(1−ρ)(1−d∆−δ)
i=1 i=1
(1−ρ)(1−d∆−δ)(1−(1−ρ)H−h−1(1−d∆−δ)H−h−1)
= (1−ρ)H−h+
1−(1−ρ)(1−d∆−δ)
= (1−ρ)H−h+(1−ρ)(1−d∆−δ)+···+(1−ρ)H−h−1(1−d∆−δ)H−h−1
≥ (1−d∆−δ)H(cid:0) (1−ρ)+···+(1−ρ)H−h−1+(1−ρ)H−h(cid:1) (E.11)
≥
(cid:16)
1−
2 (cid:17)H(cid:0) (1−ρ)+···+(1−ρ)H−h−1+(1−ρ)H−h(cid:1)
H
H−h
1 (cid:88)
≥ (1−ρ)i, (E.12)
12
i=1
40where (E.11) holds due to 3d∆ ≤ δ = 1/H and (E.12) holds due to H ≥ 6. Next, we carefully bound
the LHS of (E.12) with respect to ρ. For any h ≤ H/2 and ρ ∈ (0,3/4], we have
1 H (cid:88)−h 1 (1−ρ)(1−(1−ρ)H/2) 1 1−(1−ρ)H/2
(1−ρ)i ≥ ≥ .
12 12 ρ 50 ρ
i=1
Given the fact that
1−(1−ρ)H/2 (cid:16) (cid:16) 1(cid:17)(cid:17)
= Θ min H, ,
ρ ρ
there exist a constant c > 0, such that
1−(1−ρ)H/2 (cid:16) 1(cid:17)
≥ c·min H, .
ρ ρ
Then we have
H−h
(cid:88) (cid:16) 1(cid:17)
(1−ρ)i−T ≥ c′·min H, , (E.13)
h+1
ρ
i=1
where c′ = c/50. Moreover, with the choice of parameter 3d∆ ≤ δ,δ = 1/H, and H ≥ 6, we have
h−1
(cid:89)
(1−o −δ) ≥ (1−4δ/3)H ≥ 1/3. (E.14)
j
j=1
Therefore, by (E.7), (E.13) and (E.14), we have
V⋆,ρ(x )−Vπ,ρ(x ) = T −S
1 1 1 1 1 1
H/2
(cid:88)
≥ c′′·min{H,1/ρ}· (d∆−o )
h
h=1
H/2
(cid:88)(cid:16) (cid:17)
= c′′·min{H,1/ρ}· max⟨µ ,a⟩−⟨µ ,a¯π⟩ ,
h h h
a∈A
h=1
where c′′ = c′/3. This completes the proof.
E.3 Proof of Theorem 5.5
Next, we present an existing result on lower bounding the regret of linear bandits induced by
Lemma E.1. This result is useful in deriving the lower bound in Theorem 5.5.
Lemma E.2. (Zhou et al., 2021a, Lemma 25) Fix a positive real 0 ≤ δ ≤ 1/3, and positive integers
√
K,d and assume that K ≥ d2/(2δ). Let ∆ = (cid:112) δ/K/(4 2) and consider the linear bandit problems
L parameterized with a parameter vector µ ∈ {−∆,∆}d and action set A = {−1,1}d so that the
µ
reward distribution for taking action a ∈ A is a Bernoulli distribution Bernoulli(δ+⟨µ,a⟩). Then
41for any bandit algorithm B, there exists a µ⋆ ∈ {−∆,∆}d such that the expected pseudo-regret of B
over first K steps on bandit L is lower bounded as follows:
µ⋆
√
d Kδ
E Regret(K) ≥ √ .
µ⋆
8 2
Note that the expectation is with respect to a distribution that depends both on B and µ⋆, but since
B is fixed, this dependence is hidden.
Now we are ready to prove the lower bound in Theorem 5.5.
Proof of Theorem 5.5. By Lemma E.1, we have
K
E AveSubopt(M ,K) =
1
E
(cid:104)(cid:88)
[V⋆,ρ(x )−Vπ,ρ(x
)](cid:105)
ξ ξ K ξ 1 1 1 1
k=1
H/2 (cid:20) K (cid:21)
min{H,1/ρ} (cid:88) (cid:88)(cid:16) (cid:17)
≥ c· E max⟨ξ ,a⟩−⟨ξ ,a¯π k⟩ .
K ξ a∈A h h h
h=1 k=1
Notethatthelearningprocessisconductedonthenominalenvironment, whichisexactlytheMDPin
Zhou et al. (2021a), thus the rest proof of Theorem 5.2 follows the argument in the proof of Theorem
8 in Zhou et al. (2021a). In particular, define ξ−h = (ξ ,··· ,ξ ,ξ ,··· ,ξ ), then every MDP
1 h−1 h+1 H
policy π induces a bandit algorithm B for the linear bandit of Lemma E.2. Moreover, our
π,h,ξ−h
choice of parameters in (E.1) satisfy the requirement of Lemma E.2. Denote the regret of this bandit
problem on L as BanditRegret(B ,ξ ), then we have
ξ π,h,ξ−h h
H/2
min{H,1/ρ} (cid:88)
supE AveSubopt(M ,K) ≥ supc· BanditRegret(B ,ξ )
ξ ξ
K
π,h,ξ−h h
ξ ξ
h=1
H/2
min{H,1/ρ} (cid:88)
≥ supc· inf BanditRegret(B ,ξ )
ξ K ξ˜−h
π,h,ξ˜−h h
h=1
H/2
min{H,1/ρ} (cid:88)
= c· supinf BanditRegret(B ,ξ )
K ξ ξ˜−h
π,h,ξ˜−h h
h=1
√
min{H,1/ρ}dH Kδ
≥ c· √
16 2·K
√
c d H ·min{H,1/ρ}
= √ · √ .
16 2 K
This completes the proof.
F Auxiliary Lemmas
In this section, we present some standard technical results in the literature that our proofs are built
on.
42Proposition F.1. (Strong duality for TV (Shi et al., 2023, Lemma 4)). Given any probability mea-
sureµ0 overS, afixeduncertaintylevelρ, theuncertaintysetUρ(µ0) = {µ : µ ∈ ∆(S),D (µ||µ0) ≤
TV
ρ}, and any function V : S → [0,H], we obtain
inf E V(s) = max (cid:8)E [V(s)] −ρ(cid:0) α−min [V(s′)] (cid:1)(cid:9) , (F.1)
µ∈Uρ(µ0) s∼µ α∈[Vmin,Vmax] s∼µ0 α s′ α
where [V(s)] = min{V(s),α}, V = min V(s) and V = max V(s). Notably, the range of α
α min s max s
can be relaxed to [0,H] without impacting the optimization.
Lemma F.2. (Abbasi-Yadkori et al., 2011, Lemma 12) Let A, B and C be positive semi-definite
matrices such that A = B+C. Then we have that
x⊤Ax det(A)
sup ≤ .
x⊤Bx det(B)
x̸=0
Lemma F.3. (Abbasi-Yadkori et al., 2011, Confidence Ellipsoid, Theorem 2) Let {G }∞ be a
k k=1
filtration, and {x ,η } be a stochastic process such that x ∈ Rd is G -measurable and η ∈ R is
k k k≥1 k k k
G -measurable. Let L, σ,Σ,ϵ > 0,µ∗ ∈ Rd. For k ≥ 1, let y = ⟨µ∗,x ⟩+η and suppose that
k+1 k k k
η ,x also satisfy
k k
E[η | G ] = 0,|η | ≤ R,∥x ∥ ≤ L.
k k k k 2
For k ≥ 1, let Z = λI+(cid:80)k x x⊤,b = (cid:80)k y x ,µ = Z−1b , and
k i=1 i i k i=1 i i k k k
(cid:115)
(cid:18) kL2(cid:19)
1
β = R dlog 1+ +2log .
k
dλ δ
Then, for any 0 < δ < 1, we have with probability at least 1−δ that,
(cid:13) k (cid:13) √
∀k ≥ 1,(cid:13) (cid:13) (cid:13)(cid:88) x iη i(cid:13) (cid:13) (cid:13) ≤ β k,∥µ k −µ∗∥ Z k ≤ β k + λ∥µ∗∥ 2.
i=1
Z− k1
Lemma F.4. (Jin et al., 2020, Lemma D.1) Let Λ = λI+(cid:80)t ϕ ϕ⊤, where ϕ ∈ Rd and λ > 0.
t i=1 i i i
Then we have
t
(cid:88)
ϕ⊤(Λ )−1ϕ ≤ d.
i t i
i=1
Lemma F.5. (Ishfaq et al., 2021, Lemma D.5) Let A ∈ Rd×d be a positive definite matrix where
its largest eigenvalue λ (A) ≤ λ. Let x ,...,x be k vectors in Rd. Then it holds that
max 1 k
(cid:13) k (cid:13) √ (cid:18) k (cid:19)1/2
(cid:13) (cid:13)A(cid:88) x i(cid:13) (cid:13) ≤ λk (cid:88) ∥x i∥2
A
.
(cid:13) (cid:13)
i=1 i=1
Lemma F.6. (Vershynin, 2018, Covering number of Euclidean ball) For any ε > 0, N , the
ε
ε-covering number of the Euclidean ball of radius B > 0 in Rd satisfies
(cid:18) 2B(cid:19)d (cid:18) 3B(cid:19)d
N ≤ 1+ ≤ .
ε
ε ε
43Lemma F.7. (Vershynin, 2018, Covering number of an interval) Denote the ϵ-covering number of
the closed interval [a,b] for some real number b > a with respect to the distance metric d(α ,α ) =
1 2
|α −α | as N ([a,b]). Then we have N ([a,b]) ≤ 3(b−a)/ϵ.
1 2 ϵ ϵ
Lemma F.8. (Zhou and Gu, 2022, Theorem 4.3) Let {G }∞ be a filtration, and {x ,η }
k k=1 k k k≥1
be a stochastic process such that x ∈ Rd is G -measurable and η ∈ R is G -measurable. Let
k k k k+1
L,σ > 0,µ∗ ∈ Rd. For k ≥ 1, let y = ⟨µ∗,x ⟩+η and suppose that η ,x also satisfy
k k k k k
E[η | G ] = 0,E[η2 | G ] ≤ σ2,|η | ≤ R,∥x ∥ ≤ L.
k k k k k k 2
√
For k ≥ 1, let β
k
= O(cid:101)(cid:0) σ d+max 1≤i≤k|η i|min(cid:8) 1,∥x i∥
Z−1
(cid:9)(cid:1) and Z
k
= λI+(cid:80)k i=1x ix⊤
i
, b
k
=
i−1
(cid:80)k y x , µ = Z−1b . Then, for any 0 < δ < 1 , with probability at least 1−δ, for all k ∈ [K],
i=1 i i k k k
we have
(cid:13) k (cid:13) √
(cid:13) (cid:13) (cid:13)(cid:88) x iη i(cid:13) (cid:13) (cid:13) ≤ β k,∥µ k −µ∗∥ Z k ≤ β k + λ∥µ∗∥ 2.
i=1
Z− k1
Lemma F.9. (Jin et al., 2020, Lemma D.4) Let {s }∞ be a stochastic process on state space
i i=1
S with corresponding filtration {F }∞ . Let {ϕ }∞ be an Rd-valued stochastic process where
i i=1 i i=1
ϕ ∈ F , and ∥ϕ ∥ ≤ 1. Let Λ = λI+(cid:80)k ϕ ϕ⊤. Then for any δ > 0, with probability at least
i i−1 i k i=1 i i
1−δ, for all k ≥ 0, and any V ∈ V with sup |V(s)| ≤ H, we have
s∈S
(cid:13) (cid:13) (cid:13)(cid:88)k ϕ i{V(s i)−E[V(s i) | F i−1]}(cid:13) (cid:13) (cid:13)2 ≤ 4H2(cid:20) d log(cid:18) k+λ(cid:19) +log N ε(cid:21) + 8k2ε2 ,
(cid:13) (cid:13) 2 λ δ λ
i=1
Λ− k1
where N is the ε-covering number of V with respect to the distance dist(V,V′) = sup |V(s)−
ε s∈S
V′(s)|.
LemmaF.10. (LiuandXu,2024b,Lemma5.1(RangeShrinkage)) Forany(ρ,π,h) ∈ (0,1]×Π×[H],
we have max Vπ,ρ(s)−min Vπ,ρ(s) ≤ (1−(1−ρ)H−h+1)/ρ.
s∈S h s∈S h
References
Abbasi-Yadkori, Y., Pál, D. and Szepesvári, C. (2011). Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems 24. 11, 43
Azar, M. G., Osband, I. and Munos, R. (2017). Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning. PMLR. 3, 10, 12
Bai, Y., Xie, T., Jiang, N. and Wang, Y.-X. (2019). Provably efficient q-learning with low
switching cost. Advances in Neural Information Processing Systems 32. 11
Blanchet, J., Lu, M., Zhang, T. and Zhong, H. (2023). Double pessimism is provably efficient
for distributionally robust offline reinforcement learning: Generic algorithm and robust partial
coverage. arXiv preprint arXiv:2305.09659 . 4, 5
Chen, J.andJiang, N.(2019). Information-theoreticconsiderationsinbatchreinforcementlearning.
In International Conference on Machine Learning. PMLR. 11
44Dong, J., Li, J., Wang, B. and Zhang, J. (2022). Online policy optimization for robust mdp.
arXiv preprint arXiv:2209.13841 . 4
Eysenbach, B., Asawa, S., Chaudhari, S., Levine, S. and Salakhutdinov, R. (2020). Off-
dynamics reinforcement learning: Training for transfer with domain classifiers. arXiv preprint
arXiv:2006.13916 . 1
Farebrother, J., Machado, M. C. and Bowling, M. (2018). Generalization and regularization
in dqn. arXiv preprint arXiv:1810.00123 . 1
Goyal, V. and Grand-Clement, J. (2023). Robust markov decision processes: Beyond rectangu-
larity. Mathematics of Operations Research 48 203–226. 4
He, J., Zhao, H., Zhou, D. and Gu, Q. (2023). Nearly minimax optimal reinforcement learning
for linear markov decision processes. In International Conference on Machine Learning. PMLR. 2,
3, 4, 7, 8, 9, 10, 12, 33
He, J., Zhou, D. and Gu, Q. (2021). Logarithmic regret for reinforcement learning with linear
function approximation. In International Conference on Machine Learning. PMLR. 2, 4
Hsu, H.-L., Wang, W., Pajic, M. and Xu, P. (2024). Randomized exploration in cooperative
multi-agent reinforcement learning. arXiv preprint arXiv:2404.10728 . 2
Hu, P., Chen, Y. and Huang, L. (2023). Nearly minimax optimal reinforcement learning with
linear function approximation. 2
Ishfaq, H., Cui, Q., Nguyen, V., Ayoub, A., Yang, Z., Wang, Z., Precup, D. and Yang, L.
(2021). Randomized exploration in reinforcement learning with general value function approxima-
tion. In International Conference on Machine Learning. PMLR. 43
Ishfaq, H., Lan, Q., Xu, P., Mahmood, A. R., Precup, D., Anandkumar, A. and Azizzade-
nesheli, K. (2023). Provable and practical: Efficient exploration in reinforcement learning via
langevin monte carlo. arXiv preprint arXiv:2305.18246 . 4
Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research 30
257–280. 1
Jiang, Y., Zhang, T., Ho, D., Bai, Y., Liu, C. K., Levine, S. and Tan, J. (2021). Simgan:
Hybrid simulator identification for domain adaptation via adversarial reinforcement learning. In
2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 1
Jin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M. I. (2018). Is q-learning provably efficient?
Advances in neural information processing systems 31. 2, 12
Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning
with linear function approximation. In Conference on Learning Theory. PMLR. 2, 4, 5, 7, 13, 43,
44
Kim, Y., Yang, I. and Jun, K.-S. (2022). Improved regret analysis for variance-adaptive linear
bandits and horizon-free linear mixture mdps. Advances in Neural Information Processing Systems
35 1060–1072. 2
45Laber, E. B., Meyer, N. J., Reich, B. J., Pacifici, K., Collazo, J. A. and Drake, J. M.
(2018). Optimal treatment allocations in space and time for on-line control of an emerging
infectious disease. Journal of the Royal Statistical Society Series C: Applied Statistics 67 743–789.
1
Liu, Z., Clifton, J., Laber, E. B., Drake, J. and Fang, E. X. (2023). Deep spatial q-learning
for infectious disease control. Journal of Agricultural, Biological and Environmental Statistics
1–25. 1
Liu, Z. and Xu, P. (2024a). Distributionally robust off-dynamics reinforcement learning: Provable
efficiency with linear function approximation. arXiv preprint arXiv:2402.15399 . 2, 3, 4, 5, 6, 9,
10, 11, 12, 13, 30
Liu, Z. and Xu, P. (2024b). Minimax optimal and computationally efficient algorithms for
distributionally robust offline reinforcement learning. arXiv preprint arXiv:2403.09621 . 2, 3, 4, 5,
7, 10, 44
Lu, M., Zhong, H., Zhang, T. and Blanchet, J. (2024). Distributionally robust reinforcement
learningwithinteractivedatacollection: Fundamentalhardnessandnear-optimalalgorithm. arXiv
preprint arXiv:2404.03578 . 4, 5, 12, 15, 16
Ma, X., Liang, Z., Xia, L., Zhang, J., Blanchet, J., Liu, M., Zhao, Q. and Zhou, Z. (2022).
Distributionally robust offline reinforcement learning with linear function approximation. arXiv
preprint arXiv:2209.06620 . 5
Mannor, S., Mebel, O. and Xu, H. (2016). Robust mdps with k-rectangular uncertainty.
Mathematics of Operations Research 41 1484–1509. 4
Modi, A., Jiang, N., Tewari, A. and Singh, S. (2020). Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics. PMLR. 4
Nilim, A. and El Ghaoui, L. (2005). Robust control of markov decision processes with uncertain
transition matrices. Operations Research 53 780–798. 1
Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with
a generative model. In International Conference on Artificial Intelligence and Statistics. PMLR. 4
Panaganti, K., Wierman, A. and Mazumdar, E. (2024). Model-free robust phi-divergence
reinforcement learning using both offline and online data. arXiv preprint arXiv:2405.05468 . 11
Panaganti, K., Xu, Z., Kalathil, D. and Ghavamzadeh, M. (2022). Robust reinforcement
learning using offline data. Advances in neural information processing systems 35 32211–32224. 1,
4, 11
Peng, X. B., Andrychowicz, M., Zaremba, W. and Abbeel, P. (2018). Sim-to-real transfer of
robotic control with dynamics randomization. In 2018 IEEE international conference on robotics
and automation (ICRA). IEEE. 1
46Shen, Y., Xu, P. and Zavlanos, M. (2024). Wasserstein distributionally robust policy evaluation
and learning for contextual bandits. Transactions on Machine Learning Research Featured
Certification.
URL https://openreview.net/forum?id=NmpjDHWIvg 2
Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with
near-optimal sample complexity. arXiv preprint arXiv:2208.05767 . 1, 4
Shi, L., Li, G., Wei, Y., Chen, Y., Geist, M. and Chi, Y. (2023). The curious price of
distributional robustness in reinforcement learning with a generative model. arXiv preprint
arXiv:2305.16589 . 4, 43
Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press. 1
Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data
science, vol. 47. Cambridge university press. 43, 44
Wagenmaker, A. J., Chen, Y., Simchowitz, M., Du, S. and Jamieson, K. (2022). Reward-
free rl is no harder than reward-aware rl in linear markov decision processes. In International
Conference on Machine Learning. PMLR. 4
Wang, H., Shi, L. and Chi, Y. (2024). Sample complexity of offline distributionally robust linear
markov decision processes. arXiv preprint arXiv:2403.12946 . 4
Wang, R.,Du, S. S.,Yang, L.andSalakhutdinov, R. R.(2020a). Onreward-freereinforcement
learning with linear function approximation. Advances in neural information processing systems
33 17816–17826. 4
Wang, R., Foster, D. P. and Kakade, S. M. (2020b). What are the statistical limits of offline
rl with linear function approximation? arXiv preprint arXiv:2010.11895 . 11
Wang, T., Zhou, D. and Gu, Q. (2021). Provably efficient reinforcement learning with linear
function approximation under adaptivity constraints. Advances in Neural Information Processing
Systems 34 13524–13536. 11
Wiesemann, W., Kuhn, D. and Rustem, B. (2013). Robust markov decision processes. Mathe-
matics of Operations Research 38 153–183. 4
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P. and Agarwal, A. (2021). Bellman-consistent
pessimism for offline reinforcement learning. Advances in neural information processing systems
34 6683–6694. 11
Xu, H. and Mannor, S. (2006). The robustness-performance tradeoff in markov decision processes.
Advances in Neural Information Processing Systems 19. 3
Xu, Z., Panaganti, K. and Kalathil, D. (2023). Improved sample complexity bounds for
distributionallyrobustreinforcementlearning. InInternational Conference on Artificial Intelligence
and Statistics. PMLR. 4
Yang, L. and Wang, M. (2020). Reinforcement learning in feature space: Matrix bandit, kernels,
and regret bound. In International Conference on Machine Learning. PMLR. 2, 4
47Yang, W.,Wang, H.,Kozuno, T.,Jordan, S.M.andZhang, Z.(2023a).Avoidingmodelestima-
tion in robust markov decision processes with a generative model. arXiv preprint arXiv:2302.01248
. 4
Yang, W., Zhang, L. and Zhang, Z. (2022). Toward theoretical understandings of robust markov
decision processes: Sample complexity and asymptotics. The Annals of Statistics 50 3223–3248.
1, 4
Yang, Z., Guo, Y., Xu, P., Liu, A. and Anandkumar, A. (2023b). Distributionally robust
policy gradient for offline contextual bandits. In International Conference on Artificial Intelligence
and Statistics. PMLR. 1
Yu, P. and Xu, H. (2015). Distributionally robust counterpart in markov decision processes. IEEE
Transactions on Automatic Control 61 2538–2543. 4
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M. and Lazaric, A. (2020).
Frequentist regret bounds forrandomized least-squares value iteration. In International Conference
on Artificial Intelligence and Statistics. PMLR. 4
Zhang, H., Chen, H., Boning, D. and Hsieh, C.-J. (2021a). Robust reinforcement learning on
state observations with learned optimal adversary. arXiv preprint arXiv:2101.08452 . 1
Zhang, R., Hu, Y. and Li, N. (????). Soft robust mdps and risk-sensitive mdps: Equivalence,
policy gradient, and sample complexity. In The Twelfth International Conference on Learning
Representations. 11
Zhang, Z., Yang, J., Ji, X. and Du, S. S. (2021b). Improved variance-aware confidence sets for
linear bandits and linear mixture mdp. Advances in Neural Information Processing Systems 34
4342–4355. 2
Zhao, H., He, J., Zhou, D., Zhang, T. and Gu, Q. (2023). Variance-dependent regret bounds
for linear bandits and reinforcement learning: Adaptivity and computational efficiency. arXiv
preprint arXiv:2302.10371 . 2
Zhao, W., Queralta, J. P. and Westerlund, T. (2020). Sim-to-real transfer in deep reinforce-
ment learning for robotics: a survey. In 2020 IEEE symposium series on computational intelligence
(SSCI). IEEE. 1
Zhou, D. and Gu, Q. (2022). Computationally efficient horizon-free reinforcement learning for
linear mixture mdps. 2, 44
Zhou, D., Gu, Q. and Szepesvari, C. (2021a). Nearly minimax optimal reinforcement learning
for linear mixture markov decision processes. In Conference on Learning Theory. PMLR. 2, 11,
35, 37, 41, 42
Zhou, Z., Zhou, Z., Bai, Q., Qiu, L., Blanchet, J. and Glynn, P. (2021b). Finite-sample
regret bound for distributionally robust offline tabular reinforcement learning. In International
Conference on Artificial Intelligence and Statistics. PMLR. 4
48