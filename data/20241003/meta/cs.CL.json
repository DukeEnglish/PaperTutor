[
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "authors": "Haotian ZhangMingfei GaoZhe GanPhilipp DufterNina WenzelForrest HuangDhruti ShahXianzhi DuBowen ZhangYanghao LiSam DodgeKeen YouZhen YangAleksei TimofeevMingze XuHong-You ChenJean-Philippe FauconnierZhengfeng LaiHaoxuan YouZirui WangAfshin DehghanPeter GraschYinfei Yang",
        "links": "http://arxiv.org/abs/2409.20566v1",
        "entry_id": "http://arxiv.org/abs/2409.20566v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20566v1",
        "summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.",
        "updated": "2024-09-30 17:59:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20566v1"
    },
    {
        "title": "Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments",
        "authors": "Iker De la IglesiaIakes GoenagaJohanna Ramirez-RomeroJose Maria Villa-GonzalezJosu GoikoetxeaAnder Barrena",
        "links": "http://arxiv.org/abs/2409.20565v1",
        "entry_id": "http://arxiv.org/abs/2409.20565v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20565v1",
        "summary": "Evaluating LLM-generated text has become a key challenge, especially in\ndomain-specific contexts like the medical field. This work introduces a novel\nevaluation methodology for LLM-generated medical explanatory arguments, relying\non Proxy Tasks and rankings to closely align results with human evaluation\ncriteria, overcoming the biases typically seen in LLMs used as judges. We\ndemonstrate that the proposed evaluators are robust against adversarial\nattacks, including the assessment of non-argumentative text. Additionally, the\nhuman-crafted arguments needed to train the evaluators are minimized to just\none example per Proxy Task. By examining multiple LLM-generated arguments, we\nestablish a methodology for determining whether a Proxy Task is suitable for\nevaluating LLM-generated medical explanatory arguments, requiring only five\nexamples and two human experts.",
        "updated": "2024-09-30 17:59:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20565v1"
    },
    {
        "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation",
        "authors": "Ziyao ZhangYanlin WangChong WangJiachi ChenZibin Zheng",
        "links": "http://arxiv.org/abs/2409.20550v1",
        "entry_id": "http://arxiv.org/abs/2409.20550v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20550v1",
        "summary": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
        "updated": "2024-09-30 17:51:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20550v1"
    },
    {
        "title": "Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource",
        "authors": "Pablo OrtegaJordi LuqueLuis LamiableRodrigo LópezRichard Benjamins",
        "links": "http://arxiv.org/abs/2409.20524v1",
        "entry_id": "http://arxiv.org/abs/2409.20524v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20524v1",
        "summary": "Human language, while aimed at conveying meaning, inherently carries\nambiguity. It poses challenges for speech and language processing, but also\nserves crucial communicative functions. Efficiently solve ambiguity is both a\ndesired and a necessary characteristic. The lexical meaning of a word in\ncontext can be determined automatically by Word Sense Disambiguation (WSD)\nalgorithms that rely on external knowledge often limited and biased toward\nEnglish. When adapting content to other languages, automated translations are\nfrequently inaccurate and a high degree of expert human validation is necessary\nto ensure both accuracy and understanding. The current study addresses previous\nlimitations by introducing a new resource for Spanish WSD. It includes a sense\ninventory and a lexical dataset sourced from the Diccionario de la Lengua\nEspa\\~nola which is maintained by the Real Academia Espa\\~nola. We also review\ncurrent resources for Spanish and report metrics on them by a state-of-the-art\nsystem.",
        "updated": "2024-09-30 17:22:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20524v1"
    },
    {
        "title": "Enhancing Romanian Offensive Language Detection through Knowledge Distillation, Multi-Task Learning, and Data Augmentation",
        "authors": "Vlad-Cristian MateiIulian-Marius TăiatuRăzvan-Alexandru SmăduDumitru-Clementin Cercel",
        "links": "http://dx.doi.org/10.1007/978-3-031-70239-6_22",
        "entry_id": "http://arxiv.org/abs/2409.20498v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20498v1",
        "summary": "This paper highlights the significance of natural language processing (NLP)\nwithin artificial intelligence, underscoring its pivotal role in comprehending\nand modeling human language. Recent advancements in NLP, particularly in\nconversational bots, have garnered substantial attention and adoption among\ndevelopers. This paper explores advanced methodologies for attaining smaller\nand more efficient NLP models. Specifically, we employ three key approaches:\n(1) training a Transformer-based neural network to detect offensive language,\n(2) employing data augmentation and knowledge distillation techniques to\nincrease performance, and (3) incorporating multi-task learning with knowledge\ndistillation and teacher annealing using diverse datasets to enhance\nefficiency. The culmination of these methods has yielded demonstrably improved\noutcomes.",
        "updated": "2024-09-30 16:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20498v1"
    }
]