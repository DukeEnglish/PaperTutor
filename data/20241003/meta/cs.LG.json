[
    {
        "title": "Continuously Improving Mobile Manipulation with Autonomous Real-World RL",
        "authors": "Russell MendoncaEmmanuel PanovBernadette BucherJiuguang WangDeepak Pathak",
        "links": "http://arxiv.org/abs/2409.20568v1",
        "entry_id": "http://arxiv.org/abs/2409.20568v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20568v1",
        "summary": "We present a fully autonomous real-world RL framework for mobile manipulation\nthat can learn policies without extensive instrumentation or human supervision.\nThis is enabled by 1) task-relevant autonomy, which guides exploration towards\nobject interactions and prevents stagnation near goal states, 2) efficient\npolicy learning by leveraging basic task knowledge in behavior priors, and 3)\nformulating generic rewards that combine human-interpretable semantic\ninformation with low-level, fine-grained observations. We demonstrate that our\napproach allows Spot robots to continually improve their performance on a set\nof four challenging mobile manipulation tasks, obtaining an average success\nrate of 80% across tasks, a 3-4 improvement over existing approaches. Videos\ncan be found at https://continual-mobile-manip.github.io/",
        "updated": "2024-09-30 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20568v1"
    },
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "authors": "Haotian ZhangMingfei GaoZhe GanPhilipp DufterNina WenzelForrest HuangDhruti ShahXianzhi DuBowen ZhangYanghao LiSam DodgeKeen YouZhen YangAleksei TimofeevMingze XuHong-You ChenJean-Philippe FauconnierZhengfeng LaiHaoxuan YouZirui WangAfshin DehghanPeter GraschYinfei Yang",
        "links": "http://arxiv.org/abs/2409.20566v1",
        "entry_id": "http://arxiv.org/abs/2409.20566v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20566v1",
        "summary": "We present MM1.5, a new family of multimodal large language models (MLLMs)\ndesigned to enhance capabilities in text-rich image understanding, visual\nreferring and grounding, and multi-image reasoning. Building upon the MM1\narchitecture, MM1.5 adopts a data-centric approach to model training,\nsystematically exploring the impact of diverse data mixtures across the entire\nmodel training lifecycle. This includes high-quality OCR data and synthetic\ncaptions for continual pre-training, as well as an optimized visual\ninstruction-tuning data mixture for supervised fine-tuning. Our models range\nfrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)\nvariants, and demonstrate that careful data curation and training strategies\ncan yield strong performance even at small scales (1B and 3B). Additionally, we\nintroduce two specialized variants: MM1.5-Video, designed for video\nunderstanding, and MM1.5-UI, tailored for mobile UI understanding. Through\nextensive empirical studies and ablations, we provide detailed insights into\nthe training processes and decisions that inform our final designs, offering\nvaluable guidance for future research in MLLM development.",
        "updated": "2024-09-30 17:59:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20566v1"
    },
    {
        "title": "SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes",
        "authors": "Tianchang ShenZhaoshuo LiMarc LawMatan AtzmonSanja FidlerJames LucasJun GaoNicholas Sharp",
        "links": "http://dx.doi.org/10.1145/3680528.3687634",
        "entry_id": "http://arxiv.org/abs/2409.20562v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20562v1",
        "summary": "Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.",
        "updated": "2024-09-30 17:59:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20562v1"
    },
    {
        "title": "LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner",
        "authors": "Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li",
        "links": "http://arxiv.org/abs/2409.20560v1",
        "entry_id": "http://arxiv.org/abs/2409.20560v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20560v1",
        "summary": "Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multi-agent planners. The experimental videos, code, and datasets of\nthis work as well as the detailed prompts used in each module are available at\nhttps://lamma-p.github.io.",
        "updated": "2024-09-30 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20560v1"
    },
    {
        "title": "Supervised Multi-Modal Fission Learning",
        "authors": "Lingchao MaoQi wangYi SuFleming LureJing Li",
        "links": "http://arxiv.org/abs/2409.20559v1",
        "entry_id": "http://arxiv.org/abs/2409.20559v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20559v1",
        "summary": "Learning from multimodal datasets can leverage complementary information and\nimprove performance in prediction tasks. A commonly used strategy to account\nfor feature correlations in high-dimensional datasets is the latent variable\napproach. Several latent variable methods have been proposed for multimodal\ndatasets. However, these methods either focus on extracting the shared\ncomponent across all modalities or on extracting both a shared component and\nindividual components specific to each modality. To address this gap, we\npropose a Multi-Modal Fission Learning (MMFL) model that simultaneously\nidentifies globally joint, partially joint, and individual components\nunderlying the features of multimodal datasets. Unlike existing latent variable\nmethods, MMFL uses supervision from the response variable to identify\npredictive latent components and has a natural extension for incorporating\nincomplete multimodal data. Through simulation studies, we demonstrate that\nMMFL outperforms various existing multimodal algorithms in both complete and\nincomplete modality settings. We applied MMFL to a real-world case study for\nearly prediction of Alzheimers Disease using multimodal neuroimaging and\ngenomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI)\ndataset. MMFL provided more accurate predictions and better insights into\nwithin- and across-modality correlations compared to existing methods.",
        "updated": "2024-09-30 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20559v1"
    }
]