[
    {
        "title": "Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions",
        "authors": "Dongze WuYao Xie",
        "links": "http://arxiv.org/abs/2409.20547v1",
        "entry_id": "http://arxiv.org/abs/2409.20547v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20547v1",
        "summary": "Sampling from high-dimensional, multi-modal distributions remains a\nfundamental challenge across domains such as statistical Bayesian inference and\nphysics-based machine learning. In this paper, we propose Annealing Flow (AF),\na continuous normalizing flow-based approach designed to sample from\nhigh-dimensional and multi-modal distributions. The key idea is to learn a\ncontinuous normalizing flow-based transport map, guided by annealing, to\ntransition samples from an easy-to-sample distribution to the target\ndistribution, facilitating effective exploration of modes in high-dimensional\nspaces. Unlike many existing methods, AF training does not rely on samples from\nthe target distribution. AF ensures effective and balanced mode exploration,\nachieves linear complexity in sample size and dimensions, and circumvents\ninefficient mixing times. We demonstrate the superior performance of AF\ncompared to state-of-the-art methods through extensive experiments on various\nchallenging distributions and real-world datasets, particularly in\nhigh-dimensional and multi-modal settings. We also highlight the potential of\nAF for sampling the least favorable distributions.",
        "updated": "2024-09-30 17:48:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20547v1"
    },
    {
        "title": "Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning",
        "authors": "Zhishuai LiuWeixin WangPan Xu",
        "links": "http://arxiv.org/abs/2409.20521v1",
        "entry_id": "http://arxiv.org/abs/2409.20521v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20521v1",
        "summary": "We study off-dynamics Reinforcement Learning (RL), where the policy training\nand deployment environments are different. To deal with this environmental\nperturbation, we focus on learning policies robust to uncertainties in\ntransition dynamics under the framework of distributionally robust Markov\ndecision processes (DRMDPs), where the nominal and perturbed dynamics are\nlinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U that\nenjoys an average suboptimality $\\widetilde{\\mathcal{O}}\\big({d H \\cdot \\min\n\\{1/{\\rho}, H\\}/\\sqrt{K} }\\big)$, where $K$ is the number of episodes, $H$ is\nthe horizon length, $d$ is the feature dimension and $\\rho$ is the uncertainty\nlevel. This result improves the state-of-the-art by\n$\\mathcal{O}(dH/\\min\\{1/\\rho,H\\})$. We also construct a novel hard instance and\nderive the first information-theoretic lower bound in this setting, which\nindicates our algorithm is near-optimal up to $\\mathcal{O}(\\sqrt{H})$ for any\nuncertainty level $\\rho\\in(0,1]$. Our algorithm also enjoys a 'rare-switching'\ndesign, and thus only requires $\\mathcal{O}(dH\\log(1+H^2K))$ policy switches\nand $\\mathcal{O}(d^2H\\log(1+H^2K))$ calls for oracle to solve dual optimization\nproblems, which significantly improves the computational efficiency of existing\nalgorithms for DRMDPs, whose policy switch and oracle complexities are both\n$\\mathcal{O}(K)$.",
        "updated": "2024-09-30 17:21:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20521v1"
    },
    {
        "title": "Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits",
        "authors": "Mengmeng LiDaniel KuhnBahar Taskesen",
        "links": "http://arxiv.org/abs/2409.20440v1",
        "entry_id": "http://arxiv.org/abs/2409.20440v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20440v1",
        "summary": "Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret\nfor adversarial as well as stochastic bandit problems and allow for a\nstreamlined analysis. Nonetheless, FTRL algorithms require the solution of an\noptimization problem in every iteration and are thus computationally\nchallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve\ncomputational efficiency by perturbing the estimates of the rewards of the\narms, but their regret analysis is cumbersome. We propose a new FTPL algorithm\nthat generates optimal policies for both adversarial and stochastic multi-armed\nbandits. Like FTRL, our algorithm admits a unified regret analysis, and similar\nto FTPL, it offers low computational costs. Unlike existing FTPL algorithms\nthat rely on independent additive disturbances governed by a \\textit{known}\ndistribution, we allow for disturbances governed by an \\textit{ambiguous}\ndistribution that is only known to belong to a given set and propose a\nprinciple of optimism in the face of ambiguity. Consequently, our framework\ngeneralizes existing FTPL algorithms. It also encapsulates a broad range of\nFTRL methods as special cases, including several optimal ones, which appears to\nbe impossible with current FTPL methods. Finally, we use techniques from\ndiscrete choice theory to devise an efficient bisection algorithm for computing\nthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ times\nfaster than standard FTRL algorithms that solve an optimization problem in\nevery iteration. Our results not only settle existing conjectures but also\nprovide new insights into the impact of perturbations by mapping FTRL to FTPL.",
        "updated": "2024-09-30 16:00:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20440v1"
    },
    {
        "title": "Sufficient and Necessary Explanations (and What Lies in Between)",
        "authors": "Beepul BhartiPaul YiJeremias Sulam",
        "links": "http://arxiv.org/abs/2409.20427v1",
        "entry_id": "http://arxiv.org/abs/2409.20427v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20427v1",
        "summary": "As complex machine learning models continue to find applications in\nhigh-stakes decision-making scenarios, it is crucial that we can explain and\nunderstand their predictions. Post-hoc explanation methods provide useful\ninsights by identifying important features in an input $\\mathbf{x}$ with\nrespect to the model output $f(\\mathbf{x})$. In this work, we formalize and\nstudy two precise notions of feature importance for general machine learning\nmodels: sufficiency and necessity. We demonstrate how these two types of\nexplanations, albeit intuitive and simple, can fall short in providing a\ncomplete picture of which features a model finds important. To this end, we\npropose a unified notion of importance that circumvents these limitations by\nexploring a continuum along a necessity-sufficiency axis. Our unified notion,\nwe show, has strong ties to other popular definitions of feature importance,\nlike those based on conditional independence and game-theoretic quantities like\nShapley values. Crucially, we demonstrate how a unified perspective allows us\nto detect important features that could be missed by either of the previous\napproaches alone.",
        "updated": "2024-09-30 15:50:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20427v1"
    },
    {
        "title": "Stream-level flow matching from a Bayesian decision theoretic perspective",
        "authors": "Ganchao WeiLi Ma",
        "links": "http://arxiv.org/abs/2409.20423v2",
        "entry_id": "http://arxiv.org/abs/2409.20423v2",
        "pdf_url": "http://arxiv.org/pdf/2409.20423v2",
        "summary": "Flow matching (FM) is a family of training algorithms for fitting continuous\nnormalizing flows (CNFs). A standard approach to FM, called conditional flow\nmatching (CFM), exploits the fact that the marginal vector field of a CNF can\nbe learned by fitting least-square regression to the so-called conditional\nvector field specified given one or both ends of the flow path. We show that\nviewing CFM training from a Bayesian decision theoretic perspective on\nparameter estimation opens the door to generalizations of CFM algorithms. We\npropose one such extension by introducing a CFM algorithm based on defining\nconditional probability paths given what we refer to as ``streams'', instances\nof latent stochastic paths that connect pairs of noise and observed data.\nFurther, we advocates the modeling of these latent streams using Gaussian\nprocesses (GPs). The unique distributional properties of GPs, and in particular\nthe fact that the velocities of a GP is still a GP, allows drawing samples from\nthe resulting stream-augmented conditional probability path without simulating\nthe actual streams, and hence the ``simulation-free\" nature of CFM training is\npreserved. We show that this generalization of the CFM can substantially reduce\nthe variance in the estimated marginal vector field at a moderate computational\ncost, thereby improving the quality of the generated samples under common\nmetrics. Additionally, we show that adopting the GP on the streams allows for\nflexibly linking multiple related training data points (e.g., time series) and\nincorporating additional prior information. We empirically validate our claim\nthrough both simulations and applications to two hand-written image datasets.",
        "updated": "2024-10-01 19:05:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20423v2"
    }
]