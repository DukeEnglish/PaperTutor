[
    {
        "title": "Continuously Improving Mobile Manipulation with Autonomous Real-World RL",
        "authors": "Russell MendoncaEmmanuel PanovBernadette BucherJiuguang WangDeepak Pathak",
        "links": "http://arxiv.org/abs/2409.20568v1",
        "entry_id": "http://arxiv.org/abs/2409.20568v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20568v1",
        "summary": "We present a fully autonomous real-world RL framework for mobile manipulation\nthat can learn policies without extensive instrumentation or human supervision.\nThis is enabled by 1) task-relevant autonomy, which guides exploration towards\nobject interactions and prevents stagnation near goal states, 2) efficient\npolicy learning by leveraging basic task knowledge in behavior priors, and 3)\nformulating generic rewards that combine human-interpretable semantic\ninformation with low-level, fine-grained observations. We demonstrate that our\napproach allows Spot robots to continually improve their performance on a set\nof four challenging mobile manipulation tasks, obtaining an average success\nrate of 80% across tasks, a 3-4 improvement over existing approaches. Videos\ncan be found at https://continual-mobile-manip.github.io/",
        "updated": "2024-09-30 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20568v1"
    },
    {
        "title": "LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner",
        "authors": "Xiaopan ZhangHao QinFuquan WangYue DongJiachen Li",
        "links": "http://arxiv.org/abs/2409.20560v1",
        "entry_id": "http://arxiv.org/abs/2409.20560v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20560v1",
        "summary": "Language models (LMs) possess a strong capability to comprehend natural\nlanguage, making them effective in translating human instructions into detailed\nplans for simple robot tasks. Nevertheless, it remains a significant challenge\nto handle long-horizon tasks, especially in subtask identification and\nallocation for cooperative heterogeneous robot teams. To address this issue, we\npropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel\nmulti-agent task planning framework that achieves state-of-the-art performance\non long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoning\ncapability and the traditional heuristic search planner to achieve a high\nsuccess rate and efficiency while demonstrating strong generalization across\ntasks. Additionally, we create MAT-THOR, a comprehensive benchmark that\nfeatures household tasks with two different levels of complexity based on the\nAI2-THOR environment. The experimental results demonstrate that LaMMA-P\nachieves a 105% higher success rate and 36% higher efficiency than existing\nLM-based multi-agent planners. The experimental videos, code, and datasets of\nthis work as well as the detailed prompts used in each module are available at\nhttps://lamma-p.github.io.",
        "updated": "2024-09-30 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20560v1"
    },
    {
        "title": "Maia-2: A Unified Model for Human-AI Alignment in Chess",
        "authors": "Zhenwei TangDifan JiaoReid McIlroy-YoungJon KleinbergSiddhartha SenAshton Anderson",
        "links": "http://arxiv.org/abs/2409.20553v1",
        "entry_id": "http://arxiv.org/abs/2409.20553v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20553v1",
        "summary": "There are an increasing number of domains in which artificial intelligence\n(AI) systems both surpass human ability and accurately model human behavior.\nThis introduces the possibility of algorithmically-informed teaching in these\ndomains through more relatable AI partners and deeper insights into human\ndecision-making. Critical to achieving this goal, however, is coherently\nmodeling human behavior at various skill levels. Chess is an ideal model system\nfor conducting research into this kind of human-AI alignment, with its rich\nhistory as a pivotal testbed for AI research, mature superhuman AI systems like\nAlphaZero, and precise measurements of skill via chess rating systems. Previous\nwork in modeling human decision-making in chess uses completely independent\nmodels to capture human style at different skill levels, meaning they lack\ncoherence in their ability to adapt to the full spectrum of human improvement\nand are ultimately limited in their effectiveness as AI partners and teaching\ntools. In this work, we propose a unified modeling approach for human-AI\nalignment in chess that coherently captures human style across different skill\nlevels and directly captures how people improve. Recognizing the complex,\nnon-linear nature of human learning, we introduce a skill-aware attention\nmechanism to dynamically integrate players' strengths with encoded chess\npositions, enabling our model to be sensitive to evolving player skill. Our\nexperimental results demonstrate that this unified framework significantly\nenhances the alignment between AI and human players across a diverse range of\nexpertise levels, paving the way for deeper insights into human decision-making\nand AI-guided teaching tools.",
        "updated": "2024-09-30 17:54:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20553v1"
    },
    {
        "title": "LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation",
        "authors": "Ziyao ZhangYanlin WangChong WangJiachi ChenZibin Zheng",
        "links": "http://arxiv.org/abs/2409.20550v1",
        "entry_id": "http://arxiv.org/abs/2409.20550v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20550v1",
        "summary": "Code generation aims to automatically generate code from input requirements,\nsignificantly enhancing development efficiency. Recent large language models\n(LLMs) based approaches have shown promising results and revolutionized code\ngeneration task. Despite the promising performance, LLMs often generate\ncontents with hallucinations, especially for the code generation scenario\nrequiring the handling of complex contextual dependencies in practical\ndevelopment process. Although previous study has analyzed hallucinations in\nLLM-powered code generation, the study is limited to standalone function\ngeneration. In this paper, we conduct an empirical study to study the\nphenomena, mechanism, and mitigation of LLM hallucinations within more\npractical and complex development contexts in repository-level generation\nscenario. First, we manually examine the code generation results from six\nmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.\nNext, we elaborate on the phenomenon of hallucinations, analyze their\ndistribution across different models. We then analyze causes of hallucinations\nand identify four potential factors contributing to hallucinations. Finally, we\npropose an RAG-based mitigation method, which demonstrates consistent\neffectiveness in all studied LLMs. The replication package including code,\ndata, and experimental results is available at\nhttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination",
        "updated": "2024-09-30 17:51:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20550v1"
    },
    {
        "title": "Robi Butler: Remote Multimodal Interactions with Household Robot Assistant",
        "authors": "Anxing XiaoNuwan JanakaTianrun HuAnshul GuptaKaixin LiCunjun YuDavid Hsu",
        "links": "http://arxiv.org/abs/2409.20548v1",
        "entry_id": "http://arxiv.org/abs/2409.20548v1",
        "pdf_url": "http://arxiv.org/pdf/2409.20548v1",
        "summary": "In this paper, we introduce Robi Butler, a novel household robotic system\nthat enables multimodal interactions with remote users. Building on the\nadvanced communication interfaces, Robi Butler allows users to monitor the\nrobot's status, send text or voice instructions, and select target objects by\nhand pointing. At the core of our system is a high-level behavior module,\npowered by Large Language Models (LLMs), that interprets multimodal\ninstructions to generate action plans. These plans are composed of a set of\nopen vocabulary primitives supported by Vision Language Models (VLMs) that\nhandle both text and pointing queries. The integration of the above components\nallows Robi Butler to ground remote multimodal instructions in the real-world\nhome environment in a zero-shot manner. We demonstrate the effectiveness and\nefficiency of this system using a variety of daily household tasks that involve\nremote users giving multimodal instructions. Additionally, we conducted a user\nstudy to analyze how multimodal interactions affect efficiency and user\nexperience during remote human-robot interaction and discuss the potential\nimprovements.",
        "updated": "2024-09-30 17:49:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.20548v1"
    }
]