MAexp: A Generic Platform for RL-based Multi-Agent Exploration
Shaohao Zhu, Jiacheng Zhou, Anjun Chen, Mingming Bai, Jiming Chen, and Jinming Xu
Abstract—The sim-to-real gap poses a significant challenge The simulation scenarios utilized for MARL training and
in RL-based multi-agent exploration due to scene quantization executionplayakeyroleinbridgingthesim-to-realgap[31].
and action discretization. Existing platforms suffer from the
Differentscenarioscanprovidediverseexperiences,whichis
inefficiencyinsamplingandthelackofdiversityinMulti-Agent
of great importance for robust strategies. On the other hand,
Reinforcement Learning (MARL) algorithms across different
scenarios,restrainingtheirwidespreadapplications.Tofillthese the discrepancy between simulation scenarios and actual
gaps, we propose MAexp, a generic platform for multi-agent conditions directly influences the performance of derived
exploration that integrates a broad range of state-of-the-art strategies applied in the real world. However, most of the
MARL algorithms and representative scenarios. Moreover, we
existing works rely heavily on non-standardized and ran-
employ point clouds to represent our exploration scenarios,
domly generated grid maps [30], [19], [18], which not only
leading to high-fidelity environment mapping and a sampling
speed approximately 40 times faster than existing platforms. widensthesim-to-realgapbutalsolimitsthereproducibility
Furthermore, equipped with an attention-based Multi-Agent and leads to unfair algorithmic comparisons. Other works
Target Generator and a Single-Agent Motion Planner, MAexp utilizing open-source platforms with standardized maps for
can work with arbitrary numbers of agents and accom- training and policy evaluation further encounter computa-
modate various types of robots. Extensive experiments are
tional bottlenecks [31], [16], resulting in extensive training
conducted to establish the first benchmark featuring several
high-performance MARL algorithms across typical scenarios time, even with powerful hardware. Due to the tedious
forrobotswithcontinuousactions,whichhighlightsthedistinct training process of MARL, the performance of various
strengths of each algorithm in different scenarios. MARLalgorithmsindifferentexplorationscenariosremains
unexplored.Thisisanimportantissuetobesolvedtoenable
I. INTRODUCTION fair comparison among algorithms.
Multi-agent exploration is a rapidly growing field with In this paper, we propose the first generic highly efficient
various applications, such as search and rescue [1] and en- Multi-Agent exploration platform (MAexp) that integrates
vironmental surveillance [2]. Despite its promising perspec- a variety of MARL algorithms and scenarios. To narrow
tive,developingcoordinationpoliciesthatperformefficiently the sim-to-real gap, we leverage the benefits of point-cloud
across diverse scenarios remains a challenge. Traditional representationsovertraditionalgrid-basedmethods,dynami-
methods, such as Potential Field-Based [7] and Cost-Based callyadjustingpoint-clouddensitytorepresentdiverseexplo-
Exploration [4], [5], commonly suffer from the limited ration areas. This design allows for high-fidelity mapping in
representational capacity for coordination policies. Their intricate regions while maintaining computational efficiency
efficiency can be sensitive to specific conditions [19] or withlesscomplexlandscapes.Toallowouragentframework
require tedious parameter tuning [16] tailored for individ- to handle teams of arbitrary size, and to train exploration
ual scenarios, highlighting the requirement for more robust policies for any type of robot, we formulate our exploration
approaches. problem into a two-step procedure: i) generating navigation
Recent advancements in Multi-Agent Reinforcement goals for agents using a global attention-based target gen-
Learning [13], [15] have opened up avenues for improving erator; ii) calculating a tailored navigation path to the goal
the performance of multi-agent exploration. MARL algo- via a local motion planner compatible with any navigation
rithms have demonstrated their superior exploration effi- algorithms.
ciency over traditional methods in specific scenarios, such
Our primary contributions are three-folds:
as indoor visual navigation [17], [16] and exploration in
discrete grid environments [18], [19], [20]. However, these
• We propose a generic high-efficiency platform for RL-
methods still exhibit a significant sim-to-real gap due to
based multi-agent exploration, accommodating various
various factors, such as action discretization [17], scene
algorithms and scenarios, and achieving a sampling
quantization [18], [19], [20], and the neglect of physical
speed nearly 40 times faster than existing platforms.
collision constraints [16].
• We employ an agent framework within the MAexp
platformthatcanadapttoarbitraryteamsizesandrobot
†The authors are with the College of Control Science and Engineer-
types during training.
ing, Zhejiang University, Hangzhou 310027, China. Correspondence to
jimmyxu@zju.edu.cn (Jinming Xu). This work was supported in • We establish a benchmark featuring six SOTA MARL
partbyNSFCunderGrants62088101,62373323andinpartsbytheKey algorithms and six typical scenarios, setting a founda-
LaboratoryofCollaborativeSensingandAutonomousUnmannedSystems
tional standard for rigorous evaluation and comparison
(KeyLabofCS&AUS)ofZhejiangProvince.Codeandvideoscanbefound
athttps://github.com/DuangZhu/MAexp. of multi-agent exploration techniques.
4202
rpA
91
]OR.sc[
1v42821.4042:viXraII. RELATEDWORKS as possible, as illustrated in Fig. 1. The explored areas are
A. Reinforcement Learning in Multi-Agent Exploration highlightedingreen,withunexploredareasinyellow.Utiliz-
ing the Ackermann Car Model as a representative example,
The existing frameworks for MARL in multi-agent ex-
each agent performs two continuous actions: Acceleration
ploration can be divided into two categories: Centralized
and Steering Angle. At each time step, the agent acquires
Training with Centralized Execution (CTCE) [20], [28] and
point-cloud data (shown as green points within radar range)
Centralized Training with Decentralized Execution (CTDE)
from sensors like radars or cameras. In our framework, we
[19], [16], [30], [29]. CTCE models, such as [28], employ
assume perfect communication between agents, allowing for
attention mechanisms in MARL for grid-based exploration.
the exchange of observations, states, and goals to generate
[20]introducesCMAPPO,aCNN-augmentedProximalPol-
actions during both the training and execution phases. The
icy Optimization variant, outperforming traditional frontier-
primaryobjectiveofthistaskistomaximizetheaccumulated
basedmethods.However,CTCEisvulnerabletosingle-point
explored area (entire green region) while minimizing the
failuresandscalabilityissuesbecauseofitscentralizedarchi-
overlap (indicated by the blue area) between agents within a
tecture. CTDE methods are thus employed to address these
constrained time horizon.
limitations. For instance, [29] proposes a novel distributed
exploration algorithm using Multi-Agent Deep Determin-
istic Policy Gradients (MADDPG) [14] for context-aware
Obstacles
decision-making in structured settings. Subsequent studies,
Unexplored Area
such as [16], enhance this by incorporating attention mech-
Explored Area
anisms for spatial and inter-agent information processing,
Exploration Goals
while[19]extendsCTDEframeworkstotackledecentralized
exploration in complex terrains. Robots
However, the current research usually narrows its focus
Overlap
to individual MARL algorithms and specific exploration   
Radar Range
scenarios, thereby limiting both cross-algorithmic and cross-
scenario evaluation. Conversely, our study leverages six Messsage Transport
SOTA MARL algorithms across six different exploration
scenarios to establish the first comprehensive benchmark,
Fig.1. Multi-agentcoordinationexplorationinarandomobstaclescenario.
filling the notable gap in MARL exploration.
B. Existing Platforms for Multi-Agent Exploration
B. Mathematical Formulation
Inthefieldofreinforcementlearningbasedmulti-agentex-
While implementing MARL algorithms, it is essential to
ploration,platformsgenerallyfallintotwocategories:vision-
model the problem as a decentralized partially observable
based exploration platforms using Habitat [32], as employed
Markov decision process (Dec-POMDP). A Dec-POMDP is
by MAANS [16], and grid-based platforms with discrete
definedbythetuple⟨n,S,A,O,R,P,γ,h⟩.Here,ndenotes
maps [19], [20], [31], [30]. While Habitat offers realistic
the number of agents involved in the task, and S signifies
simulations, it also involves additional modules, such as the
the state space, with s representing the state of agent i
SLAM module, which increases the sampling time during i
and s representing the joint state of all agents. The joint
policy training. On the other hand, grid-based platforms
action space is symbolized by A, and the individual actions
sufferfromasignificantsim-to-realgapduetothesubstantial
of agent i and joint action are represented by a and a. An
difference between the simulation scenarios and the real i
observation for agent i, denoted by o , is generated by the
world. Moreover, most existing platforms support only a i
observation function O(s,a ), dependent on both the states
singleMARLalgorithm[16],[31],makingcross-algorithmic i
ofallagentsandtheircorrespondingactions.Therewardfor
comparisons impossible. In contrast, our platform provides
agent i and the whole team, symbolized by r and r, can
a range of standard continuous exploration scenarios while i
be derived from the reward function R(s,a). The transition
offeringmultipleMARLalgorithmsforpropercomparisons.
probabilityfromstatestostates′ throughactionaisdefined
It utilizes high-fidelity point-cloud-based scenarios and con-
by P(s′|s,a), dependent on the environment’s properties.
tinuous actions to narrow the sim-to-real gap, and enhances
The discount factor for future rewards is represented by γ,
computational efficiency for faster sampling. Note that the
and h indicates the horizon of an episode. The goal of this
agent framework in our platform, built on the foundation of
problem is for each agent to find the most effective strategy,
MARLLib[11],isintegratedwithalocalplannertoaccount
denotedasπ ,andcollaborativelyworktowardsmaximizing
for various types of robots. i
the collective reward for the team as follows:
III. PROBLEMFORMULATION (cid:34)h−1 (cid:12) (cid:35)
A. Task Setup J(θ)=E (cid:88) γtR(s(t),a(t))(cid:12) (cid:12)s(0),π .
(cid:12)
We investigate a multi-agent coordination exploration t=0
problem, focusing on a team of robots tasked with coopera- The ideal coordination strategy is formulated using gradient
tively exploring an unknown continuous scenario as swiftly descenttooptimizetheobjectivefunctionJ(θ)relativetoπ.C. Metrics developed in Python to smoothly integrate with existing re-
inforcement learning algorithms, and it is equally applicable
Toevaluateexplorationpolicies,wedivideourmetricsinto
to traditional exploration methods. In an effort to bridge the
two dimensions: Team-Based and Agent-Specific metrics.
sim-to-realgap,allmapsandagentpropertieswithinMAexp
These metrics evaluate both the efficiency and cooperation
are modelled continuously, incorporating realistic physics to
abilities of the team within the exploration process.
1) Team-Based Metrics: closely mirror real-world exploration.
The autonomous exploration simulation employs an inte-
• Exploration Ratio (ER): Quantifies the proportion of
grated data flow, as shown in Fig. 2 (a-b). Part (b) presents
the explored area to the total explorable space at the
the platform’s sampling mechanism. In this phase, MAexp
end of an episode.
initiates multiple environments to collect vast experience.
• Coverage Step (CS): Denotes the number of steps
Agents, at every time step, acquire point-cloud data and
required for the team to reach the predefined explo-
their respective states from the environment. Moreover, they
rationthresholds(85%and95%).An85%ratioimplies
can adjust the radar resolution and detection range based on
significant topological coverage, while 95% marks the
specific situations. The point-cloud data generation during
episode’s successful completion.
simulationisdepictedinFig.2(c).Thisproceduresimulates
2) Agent-Specific Metrics:
radar perception but operates at a faster speed. Once an
• Mutual Overlap (MO): Measures the ratio of the agentcompletesanaction,theenvironmentidentifiesallfree
area only explored by one agent to the total explored
space point clouds within the detection range of the agent.
area. Less overlap suggests better task allocation and
Subsequent filtering eliminates points masked by obstacles
minimized redundancy. This metric is recorded when
using the following criteria:
the exploration ratio reaches 85% and 95%.
• Reward standard Variance (RV): Captures the vari- (cid:40) Pf·Po >1−α ;
ability in rewards across agents at the end of an
∥Pf∥∥Po∥ 1
∥P ∥−∥P ∥<α ,
episode. A lower variance indicates a more balanced f o 2
contributionfromeachagent,avoidingsituationswhere where P denotes the vector from the robot to a free space
f
certain agents underperform but are overshadowed by point, P represents the vector to an obstacle, and α ,α
o 1 2
the team’s collective results. are certain parameters to be determined. This process filters
out obstructed points (c.f., blue points in Fig. 2 (c)) and
D. Reward Function
transmits the remaining ones to the agent. Utilizing past
Our reward function uniquely fuses team performance
feature maps and environmental data, the current feature
with agent contributions, comprising five elements: Success
map is generated and distributed to all agents within the
Reward, Exploration Reward, Overlap Penalty, Collision
environment. The agent frameworks consequently produce
Penalty, and Time Penalty. Let Cov represent the total
t the actions for agents. Implementing these actions provides
coverage ratio at time t, maptot the merge map, and mapi
t t individual rewards for every agent, mirroring the action
theareaexploredbyagentiatthesametimestep.Inallmap
value. Thereafter, the environment transitions the collective
variables, ’1’ denotes explored space, while ’0’ indicates
agents to the succeeding states. MAexp, through consistent
unexplored territory.
sampling,MAexpaccumulatesasubstantialsetofquadruples
The agent-specific total reward Ri is formalized as:
total ⟨s,a,r,s′⟩ for training policies. Fig. 2 (a) illustrates the
R , (Success Reward) training process and provides an in-depth examination of
δms
(cid:88)api t, (Exploration Reward) t Dh ue riM ngul tt ri a- iA nig ne gn ,t bT ata cr hg ee st oG fe qn ue ar da rto ur pli en sath ree tra ag ne sn pt of rr tea dm te ow bo or tk h.
R ti otal = − (P t−i∩P ti), (Overlap Penalty) the actor and the critic to facilitate parameter updating. The
−R
c, (Collision Penalty) c er si st eic ntip ar lo fv oid re ts heth ge raa dc it eo nr
t
w dei sth cet nh te sta ec pt .ion values, which are
−Cov , (Time Penalty)
t
It’s worth noting that the proposed agent framework is
Here,δmapiisproportionalto(cid:80) (mapi−maptot ),where well-adaptedtoarangeofrobotsandgroupsizes.Inparticu-
t t t−1
the summation is over all locations where the difference lar,weaddressthemulti-agentexplorationchallengeleverag-
equals 1. Pi and P−i denote the point clouds gathered by ingthenewtechniquesdevelopedforroboticnavigation[34],
t t
agent i and the rest of the team at time t, respectively. Note [35], [36], [37]. Therefore, we divide our agent framework
that the final agent-specific reward is a normalized linear into two modules: Multi-Agent Target Generator (where-to-
combination of these five components. go)andSingle-AgentMotionPlanner(how-to-go).Inthefirst
module, agents use MARL algorithms to generate their own
IV. THEPROPOSEDPLATFORM
navigationpoints.Inthesecondmodulewhichisindependent
A. The proposed MAexp Platform
of MARL, agents decide how to arrive these target locations
We introduce MAexp, a generic high-efficiency platform bydeterminingtheiraccelerationandsteeringangle.Notably,
designedformulti-agentexploration,encompassingadiverse the how-to-go module can incorporate any robot navigation
range of scenarios and MARL algorithms. The platform is algorithm for motion planning.Policy Training Multi-Environment Synchronous Sampling Exploration
Multi-Agent Target Generator Multi-Agent System 1
1 Robot
Obstacles
n Free Space
Samples
2 Invisible Space
1
Rays
Action
Agent Radar range
Framework
Critic Reward 2 3


ActoA rction Value Radar Data     1     2
Sensor Data
Processing
Agent State 1
,  ,  ,  ’     −1
(a) (b) (c)
Fig.2. TheproposedMAexpplatformformulti-agentexploration.
B. Scenarios in MAexp Random Obstacle Maze
Different from previous studies that rely on grid-based
scenarios, we utilize point clouds for all map formulations.
This method ensures a seamless depiction of exploration
areas, delivering both detailed and authentic representations.
The processing of point clouds is easily aligned with GPU
parallel processing, and the map’s sparsity does not require
Indoor Outdoor
global consistency. In obstacle-dense regions, for instance,
there’snoneedtodetaileveryobstacle—definingboundaries
suffices.Inpivotalregions,increasingthedensityofthepoint
cloudcanbolstermapauthenticity.Furthermore,point-cloud
maps can be effortlessly extended into three-dimensional
spaces. The detailed characteristics of the four unique ex-
ploration scenarios in MAexp, along with their 3/2D maps,
can be found in TABLE I and Fig. 3. Fig.3. IllustrationofexplorationscenariosinMAexpwith3Dvisualiza-
tionsand2Dtop-downprojectionmaps.
TABLEI
SUMMARYOFVARIOUSMULTI-AGENTEXPLORATIONSCENARIOS.
actual indoor settings, these scenarios challenge the agent
Properties Type Resolution Size(m) Quantity to conduct thorough exploration in constrained areas. Based
RandomObstacle SPC 1.0 125 32 on the map size, we categorize these scenarios into three
Maze SPC 1.0 125 80 types: large, medium, and small.
Small RPC 0.1 <10 17
Outdoor scenarios are derived from STPLS3D [33],
Indoor Medium RPC 0.1 10-14 32
Large RPC 0.1 >14 22 whichcomprisesacomprehensivecollectionofsyntheticand
Outdoor SPC&RPC 1.5-2 288 32 actual aerial photogrammetry 3D point clouds. These maps
*RPC:Real-worldPointClouds;SPC:SyntheticPointClouds. authentically emulate extensive outdoor exploration settings,
thereby challenging the system’s adeptness at swift, large-
Random Obstacle scenarios combine basic elements scale outdoor exploration.
like narrow corridors, corner loops, and multiple rooms, as Itshouldbenotedthatduringtheprocessofmapcreation,
detailed in [31]. Then we intersperse isolated obstacles until we conducted careful checks and repairs to ensure the maps
the map achieves the desired obstacle density. This scenario weresuitableforexplorationtasks,whichistime-consuming
is specifically designed to encapsulate a diverse range of but essential in achieving accurate results.
exploration difficulties commonly faced by agents.
C. MARL Algorithms in MAexp
MazescenariosaregeneratedthrougharefinedKruskal’s
algorithm. With numerous branching paths and the absence MARL algorithms can be broadly classified into three
of closed loops, the Maze necessitate agents to strategically categories based on their critic architectures: Independent
allocate different agents to distinct paths to improve explo- Learning(IL),CentralizedCritic(CC),andValueDecompo-
ration efficiency. sition (VD). These architectures are illustrated in Fig. 2(a).
Indoor scenarios are derived from Habitat [32], a high- Independent Learning such as IPPO [21], IA2C [22],
fidelity dataset of 3D indoor scans. By closely mirroring and ITRPO [23], operate with agent-level critic, focusing on
pxeAM
1 vnE
M
tnemnorivnE
D3
D2
D3
D2Multi-Agent Agent Features Agent-Based Team-Based Cross Attention
Feature Inputs Extractor Spatial Encoder Relation Encoder Features Extractor Action Generator
Encoded Features of Agent 1 Self-attention Cross-attention Coarse Fine-Grained
Generation Generation
Agent 2 Agent 1 Agent 2
Agent Location To
Map  
Explored Space  8 Agent 2  48 Q Agent 2 
Obstacles Ag  ent 1 FC FC
CNN FC
Position  
Trajectory 32 48 2
Agent n Agent n K V  Reg  io2 n Coordin4 ate
Goal Location To  (  ,  )
Map Map
Encoded Features of Agent n
Fig.4. TheoverallstructureofMulti-agentTargetGenerator.
singularobservations.IPPO,forinstance,exhibitsrobustness The 2×L×L grid feature feeds a dual-action generator
to certain environmental variability [9], while ITRPO dom- to produce an exploration target. Using its first channel, the
inates in the MAMuJoCo [27] benchmark for multi-agent agentdeterminesadiscreteregionforcoarsegeneration.This
robotic control [11]. feature translates to an L2 vector, denoting the probability
CentralizedCritic,includingMAPPO[12]andMATRPO of each region’s selection, from which a region is sampled.
[24], employ a unified critic that fuses information from the Subsequently,forfine-grainedgeneration,acoordinate(x,y)
entire team to judge the joint action value. Generally, CC denotes the global goal’s relative position within the chosen
algorithms outperform IL in coordinated tasks [11], [12]. region.Thiscoordinateisderivedfroma4-vector,wherethe
Value Decomposition, including VDPPO [26] and initial pair indicates the mean and variance for coordinate x,
VDA2C [25], combine outputs from agent-level critics to and the latter pair for y. Note that the above two generators
estimate a team action value, thereby balancing both indi- operate in parallel and thus it is not necessary to feed the
vidual and group objectives. Despite their best performance coarse action into the fine-grained generator.
in numerous benchmarks [13], [14], [11], they struggle with
continuous control and long-term planning, typically seen in
V. EXPERIMENTS
multi-agent exploration [11].
Recognizingtheuniquestrengthsofindividualalgorithms,
A. Environmental Setup
wehaveincorporatedintotheproposedMAexpplatformsix
leading MARL algorithms, such as IPPO, ITRPO, MAPPO, In our experimental studies, we employ a swarm of
MATRPO, VDPPO, and VDA2C. Ackermann cars equipped with adjustable radars, tailoring
the resolution and detection range to various exploration
D. Multi-Agent Target Generator
scenarios. The Multi-Agent Target Generator operates as
The Multi-agent Target Generator, whose entire structure described earlier, while the Single-Agent Motion Planner
is illustrated in Fig. 4, employs a CNN model to extract employs the Dynamic Window Approach (DWA), which
125×125 grid-based spatial features from each agent, with provides precise navigation and obstacle avoidance.
inputs capturing explored space, obstacles, agent position, Our experiments involve N = 3 agents, with parameters
and trajectory. These feature maps, when combined with set at L = 8, α = 10−3, α = 10−6, R = 100,
1 2 s
embedded agent and previous goal location, yield an L× and R = 200. Each MARL training spans 104 iterations
c
L×D feature map where D =48. Subsequently, team-wise across three random seeds. Results are presented as “mean
data is fused via a hierarchical transformer-based structure, (standard deviation),” averaged from 300 tests—100 per
similar to Spatial-TeamFormer in MAANS [16], known for seed. We apply all six MARL algorithms across 16 settings:
its superior performance in visual exploration. The Spatial- 4 Random Obstacle, 4 Maze, 3 Outdoor, and 5 Indoor. We
Teamformer block integrates two layers: the Individual Spa- further classify the “Indoor” into 2 small, 2 medium, and
tial Encoder, which applies spatial self-attention to each 1 large. Training is carried out on an Ubuntu 18.04 server
agent’s L×L×D map without inter-agent computations, withtwoNVIDIAGeForceRTX3090GPUs:oneformulti-
and the Team Relation Encoder, targeting team interactions environment sampling and another for online training. Each
without spatial considerations. These team-oriented data are exploration map receives specialized parameter training to
then merged for each agent through cross-attention. In our optimize algorithm performance. A 104 iteration training
approach, the transformer depth is fixed at 2, yielding a requires about 60 hours, culminating in a total of roughly
2×L×L grid feature as the output. 750 days of continuous GPU usage across over 300 runs.
1− TABLEII
PERFORMANCERESULTSOFSIXMARLALGORITHMSACROSSSIXSCENARIOS.WITHINEACHSCENARIO,THEUPPERTHREECOLUMNSREPRESENT
‘ER’(↑),‘85%CS’(↓),AND‘95%CS’(↓),WHILETHELOWERCOLUMNSCORRESPONDTO‘85%MO’(↓),‘95%MO’(↓),AND‘RV’(↓).
Scenarios RandomObstacles GeneratedScenes Maze Indoor-Small Indoor-Medium RealScenes Indoor-Large Outdoor
ER(%) 85%CS(step) 95%CS(step) ER(%) 85%CS(step) 95%CS(step) ER(%) 85%CS(step) 95%CS(step) ER(%) 85%CS(step) 95%CS(step) ER(%) 85%CS(step) 95%CS(step) ER(%) 85%CS(step) 95%CS(step)
ITRPO 68.81±5.71 - - 79.71±7.32 449±63 501±77 90.46±5.12 268±45 437±65 79.38±5.00 400±84 557±47 59.40±3.95 - - 26.91±4.12 - -
IPPO 63.07±4.69 - - 93.17±4.27 377±51 441±57 94.18±0.76 157±5 205±41 91.48±4.36 287±54 470±61 71.32±4.79 - - 33.06±6.90 - -
Team-Based MATRPO61.12±3.76 - - 86.29±7.63 376±72 472±65 89.66±0.79 237±13 356±92 75.73±8.91 472±44 536±67 77.31±4.39 - - 32.72±4.31 - -
MAPPO 62.53±3.38 - - 89.15±5.07 370±67 442±44 93.23±3.56 206±88 284±107 87.01±5.71 356±40 - 70.19±4.94 - - 38.15±7.24 - -
VDPPO 60.04±4.54 - - 83.12±5.08 468±45 506±35 91.85±3.01 236±92 358±111 89.97±3.11 308±41 447±24 67.72±4.96 - - 30.31±5.48 - -
VDA2C 64.23±3.43 - - 93.23±3.54 322±59 416±54 93.59±0.33 170±12 235±52 76.93±3.95 320±30 449±26 73.97±2.86 - - 26.76±4.38 - -
85%MO(%) 95%MO(%) RV 85%MO(%) 95%MO(%) RV 85%MO(%) 95%MO(%) RV 85%MO(%) 95%MO(%) RV 85%MO(%) 95%MO(%) RV 85%MO(%) 95%MO(%) RV
ITRPO - - 610.73±79.25 51.56±6.49 43.89±4.76 931.00±104.18 51.37±4.84 48.90±2.54 871.78±38.4149.60±7.49 44.77±6.84 347.76±42.10 - - 252.95±104.43 - - 457.93±84.23
IPPO - - 711.56±109.9745.52±7.87 42.38±7.12 970.43±76.31 54.06±3.55 50.89±4.59 576.31±8.84 46.38±3.35 46.39±4.36 318.29±50.87 - - 257.30±59.91 - - 534.92±94.25
Agent-SpecificMATRPO - - 825.65±63.06 48.21±9.19 43.91±8.12 910.37±82.33 57.49±4.59 59.37±4.29 547.39±45.8149.92±8.45 57.92±8.39 389.70±89.74 - - 280.52±74.28 - - 433.80±115.81
MAPPO - - 831.93±94.97 46.22±9.46 38.53±2.95 929.25±89.14 60.78±9.24 59.14±8.47 602.14±78.9538.22±2.81 - 385.22±49.97 - - 248.80±87.86 - - 445.98±118.32
VDPPO - - 632.18±83.41 58.96±5.03 58.66±4.02 951.11±82.10 64.03±15.2966.83±17.67541.55±99.3936.58±1.63 42.38±0.74 309.89±31.52 - - 198.38±78.41 - - 431.67±103.54
VDA2C - - 810.62±76.26 49.60±5.65 47.62±5.91 907.74±60.99 77.14±3.89 77.32±8.40 496.54±33.7745.20±3.32 40.45±1.04 445.24±39.86 - - 275.55±37.27 - - 561.50±114.43
B. Comparison of Simulation Speed even though they exhibit lower exploration ratios. Further,
the superior performance of VDA2C in “Maze” reinforces
We first compare the sampling speeds of several open-
the perspective since the efficiency of exploration among
sourceSOTAplatformsforMARLexploration,i.e.,MAexp,
agents becomes uniformly distributed in this structured en-
MAANS [16], and Explore-Bench [31].
TABLE. III illustrates the sampling times per step for vironment with a constant width of free space.
team sizes N = 2,4,6,8. The proposed MAexp platform, In contrast, CC-based approaches, such as MATRPO and
optimized for MARL exploration, achieves a simulation MAPPO, demonstrate superior performance in large-scale
speed nearly 40 times faster than MAANS. While both scenarios characterized by sparsely distributed obstacles as
utilize the same exploration principles, MAANS, designed they tend to disperse agents to different regions of the map
for vision tasks, incur additional computational costs. In forparallelexploration.AsevidencedintheTable,MATRPO
contrast,Explore-Bench’slevel-0component,similartoours, achievesa77.31%explorationratioin“Indoor-Large”while
is tailored for MARL sampling. Unlike Explore-Bench’s MAPPO excels in “Outdoor”. These algorithms allow for an
CPU-centric grid simulations, MAexp leverages point-cloud overall understanding of the situation of the entire team by
modellingandGPUparallelization,substantiallyaccelerating fusing the observations and states among agents, facilitating
simulations. Hence, MAexp emerges as the most efficient efficientspatialallocation.However,thefusionalsoincreases
platform for MARL exploration, facilitating the evaluation the complexities of policy training, making it challenging to
and development of new MARL algorithms. Note that our adoptstrategiestailoredtoimmediatesurroundings,resulting
platform can also accommodate a large number of robots as in lower performance in small-scale scenarios.
long as communication and action generation strategies are To sum up, understanding the characteristics of the ex-
properly adjusted for enhanced efficiency. ploration scenario is crucial for choosing a proper MARL
algorithm to generate robust and efficient coordination poli-
TABLEIII cies. Once the scenario is identified, the proposed MAexp
COMPARISONOFSAMPLINGTIMES(S)FORDIFFERENTPLATFORMS platform offers a valuable tool for evaluating candidate
MARLalgorithmscomprehensively,facilitatingtheselection
N =2 N =4 N =6 N =8 of the most appropriate one. Moreover, for those involved in
MAANS[16] 0.4497 0.9319 1.3675 1.9354 designing new algorithms, MAexp also serves as a generic,
Explore-Bench[31] 3.8586 6.6381 8.6575 10.9880 high-efficiency platform for both training and simulation, as
MAexp(ours) 0.0144 0.0254 0.0373 0.0477
well as a benchmark for performance comparison.
C. Performance of MARL Algorithms in Various Scenarios VI. CONCLUSIONS
In Table II, we observe that different algorithms exhibit We introduced MAexp, a generic high-efficiency platform
different characteristics across our proposed scenarios. In for multi-agent exploration. MAexp incorporates several
particular, ITRPO performs impressively well in the “Ran- state-of-the-art MARL algorithms and various representa-
dom Obstacles” scenario, while IPPO consistently achieves tive exploration scenarios, and it employs point-cloud rep-
the best exploration ratios in “Maze”, “Indoor-Small,” and resentation for maps which enhances the effectiveness of
“Indoor-Medium ”. These IL-based approaches excel in MARL algorithms with rapid sampling and realistic simu-
small-scale exploration scenarios characterized by dense lation environments. Moreover, with its well-designed agent
challenges such as corner loops and multiple rooms, as their framework, MAexp can accommodate a variety of robots
criticsprimarilyfocusontheagent’simmediateenvironment and group sizes. Furthermore, we establish the first com-
to generate suitable strategies. prehensive benchmark featuring several high-performance
VD-based algorithms, such as VDPPO and VDA2C, tend MARL algorithms across various typical scenarios. Our
to obtain efficient exploration policies by uniform task dis- results highlight the unique strengths of each algorithm in
tribution, especially when agents maintain consistent perfor- different scenarios. In our future work, we aim to enhance
mance.Thisisevidentintheconsistentlylow“RewardVari- MAexp to account for general communication topology and
ance” observed in VDPPO and VDA2C across all scenarios, incorporate more advanced MARL algorithms and practicalscenarios so as to provide a versatile platform for multi- [20] Z.Chen,B.Subagdja,andA.-H.Tan,“End-to-enddeepreinforcement
agent exploration. We believe that our platform can advance learning for multi-agent collaborative exploration,” in 2019 IEEE
InternationalConferenceonAgents(ICA). IEEE,2019,pp.99–102.
the field of RL-based multi-agent exploration.
[21] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” arXiv preprint
arXiv:1707.06347,2017.
REFERENCES
[22] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeeprein-
[1] Y. Liu and G. Nejat, “Multirobot cooperative learning for semiau- forcementlearning,”inInternationalconferenceonmachinelearning.
tonomouscontrolinurbansearchandrescueapplications,”Journalof PMLR,2016,pp.1928–1937.
FieldRobotics,vol.33,no.4,pp.512–536,2016. [23] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust
[2] A.Fascista,“Towardintegratedlarge-scaleenvironmentalmonitoring region policy optimization,” in International conference on machine
usingwsn/uav/crowdsensing:Areviewofapplications,signalprocess- learning. PMLR,2015,pp.1889–1897.
ing,andfutureperspectives,”Sensors,vol.22,no.5,p.1824,2022. [24] J.G.Kuba,R.Chen,M.Wen,Y.Wen,F.Sun,J.Wang,andY.Yang,
[3] J.Alonso-Mora,S.Baker,andD.Rus,“Multi-robotformationcontrol “Trustregionpolicyoptimisationinmulti-agentreinforcementlearn-
and object transport in dynamic environments via constrained opti- ing,”arXivpreprintarXiv:2109.11251,2021.
mization,” The International Journal of Robotics Research, vol. 36, [25] J. Su, S. Adams, and P. Beling, “Value-decomposition multi-agent
no.9,pp.1000–1021,2017. actor-critics,” in Proceedings of the AAAI conference on artificial
[4] Y. Mei, Y.-H. Lu, C. G. Lee, and Y. C. Hu, “Energy-efficient intelligence,vol.35,no.13,2021,pp.11352–11360.
mobile robot exploration,” in Proceedings 2006 IEEE International [26] Y.MaandJ.Luo,“Value-decompositionmulti-agentproximalpolicy
Conference on Robotics and Automation, 2006. ICRA 2006. IEEE, optimization,” in 2022 China Automation Congress (CAC). IEEE,
2006,pp.505–511. 2022,pp.3460–3464.
[5] S.Oßwald,M.Bennewitz,W.Burgard,andC.Stachniss,“Speeding- [27] B. Peng, T. Rashid, C. Schroeder de Witt, P.-A. Kamienny, P. Torr,
up robot exploration by exploiting background information,” IEEE W. Bo¨hmer, and S. Whiteson, “Facmac: Factored multi-agent cen-
RoboticsandAutomationLetters,vol.1,no.2,pp.716–723,2016. tralisedpolicygradients,”AdvancesinNeuralInformationProcessing
[6] H. Umari and S. Mukhopadhyay, “Autonomous robotic exploration Systems,vol.34,pp.12208–12221,2021.
based on multiple rapidly-exploring randomized trees,” in 2017 [28] M.Geng,K.Xu,X.Zhou,B.Ding,H.Wang,andL.Zhang,“Learning
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems tocooperateviaanattention-basedcommunicationneuralnetworkin
(IROS). IEEE,2017,pp.1396–1402. decentralizedmulti-robotexploration,”Entropy,vol.21,no.3,p.294,
[7] J. Yu, J. Tong, Y. Xu, Z. Xu, H. Dong, T. Yang, and Y. Wang, 2019.
“Smmr-explore: Submap-based multi-robot exploration system with [29] D. He, D. Feng, H. Jia, and H. Liu, “Decentralized exploration of
multi-robot multi-target potential field exploration method,” in 2021 a structured environment based on multi-agent deep reinforcement
IEEEInternationalConferenceonRoboticsandAutomation(ICRA). learning,” in 2020 IEEE 26th International Conference on Parallel
IEEE,2021,pp.8779–8785. andDistributedSystems(ICPADS). IEEE,2020,pp.172–179.
[8] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction. [30] H. Zhang, J. Cheng, L. Zhang, Y. Li, and W. Zhang, “H2gnn:
MITpress,2018. hierarchical-hops graph neural networks for multi-robot exploration
[9] C.S.deWitt,T.Gupta,D.Makoviichuk,V.Makoviychuk,P.H.Torr, in unknown environments,” IEEE Robotics and Automation Letters,
M. Sun, and S. Whiteson, “Is independent learning all you need in vol.7,no.2,pp.3435–3442,2022.
thestarcraftmulti-agentchallenge?”arXivpreprintarXiv:2011.09533, [31] Y.Xu,J.Yu,J.Tang,J.Qiu,J.Wang,Y.Shen,Y.Wang,andH.Yang,
2020. “Explore-bench:Datasets,metricsandevaluationsforfrontier-based
[10] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, and deep-reinforcement-learning-based autonomous exploration,” in
M.Yeo,A.Makhzani,H.Ku¨ttler,J.Agapiou,J.Schrittwieseretal., 2022 International Conference on Robotics and Automation (ICRA).
“Starcraft ii: A new challenge for reinforcement learning,” arXiv IEEE,2022,pp.6225–6231.
preprintarXiv:1708.04782,2017. [32] A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner,
[11] S. Hu, Y. Zhong, M. Gao, W. Wang, H. Dong, Z. Li, X. Liang, N.Maestre,M.Mukadam,D.Chaplot,O.Maksymets,A.Gokaslan,
X. Chang, and Y. Yang, “Marllib: Extending rllib for multi-agent V. Vondrus, S. Dharur, F. Meier, W. Galuba, A. Chang, Z. Kira,
reinforcementlearning,”arXivpreprintarXiv:2210.13708,2022. V. Koltun, J. Malik, M. Savva, and D. Batra, “Habitat 2.0: Training
[12] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and home assistants to rearrange their habitat,” in Advances in Neural
Y.Wu,“Thesurprisingeffectivenessofppoincooperativemulti-agent InformationProcessingSystems(NeurIPS),2021.
games,”AdvancesinNeuralInformationProcessingSystems,vol.35, [33] M. Chen, Q. Hu, Z. Yu, H. THOMAS, A. Feng, Y. Hou,
pp.24611–24624,2022. K. McCullough, F. Ren, and L. Soibelman, “Stpls3d: A large-scale
synthetic and real aerial photogrammetry 3d point cloud dataset,”
[13] M. Samvelyan, T. Rashid, C. S. De Witt, G. Farquhar, N. Nardelli,
in 33rd British Machine Vision Conference 2022, BMVC 2022,
T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, and
London,UK,November21-24,2022. BMVAPress,2022.[Online].
S. Whiteson, “The starcraft multi-agent challenge,” arXiv preprint
Available:https://bmvc2022.mpi-inf.mpg.de/0429.pdf
arXiv:1902.04043,2019.
[34] A.Sadek,G.Bono,B.Chidlovskii,A.Baskurt,andC.Wolf,“Multi-
[14] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor-
objectnavigationinrealenvironmentsusinghybridpolicies,”in2023
datch, “Multi-agent actor-critic for mixed cooperative-competitive
IEEEInternationalConferenceonRoboticsandAutomation(ICRA).
environments,” Advances in neural information processing systems,
IEEE,2023,pp.4085–4091.
vol.30,2017.
[35] K. Nakhleh, M. Raza, M. Tang, M. Andrews, R. Boney, I. Hadzˇic´,
[15] B.Baker,I.Kanitscheider,T.Markov,Y.Wu,G.Powell,B.McGrew,
J. Lee, A. Mohajeri, and K. Palyutina, “Sacplanner: Real-world
andI.Mordatch,“Emergenttoolusefrommulti-agentautocurricula,”
collisionavoidancewithasoftactorcriticlocalplannerandpolarstate
arXivpreprintarXiv:1909.07528,2019.
representations,”in2023IEEEInternationalConferenceonRobotics
[16] C. Yu, X. Yang, J. Gao, H. Yang, Y. Wang, and Y. Wu, “Learning
andAutomation(ICRA). IEEE,2023,pp.9464–9470.
efficient multi-agent cooperative visual exploration,” in European
[36] Y. Shu, Z. Li, B. Karlsson, Y. Lin, T. Moscibroda, and K. Shin,
ConferenceonComputerVision. Springer,2022,pp.497–515.
“Incrementally-deployableindoornavigationwithautomatictracegen-
[17] H.Wang,W.Wang,X.Zhu,J.Dai,andL.Wang,“Collaborativevisual
eration,” in IEEE INFOCOM 2019-IEEE Conference on Computer
navigation,”arXivpreprintarXiv:2107.01151,2021.
Communications. IEEE,2019,pp.2395–2403.
[18] A. Mete, M. Mouhoub, and A. M. Farid, “Coordinated multi-robot
[37] Z.Zhang,S.He,Y.Shu,andZ.Shi,“Aself-evolvingwifi-basedindoor
explorationusingreinforcementlearning,”in2023InternationalCon-
navigationsystemusingsmartphones,”IEEETransactionsonMobile
ference on Unmanned Aircraft Systems (ICUAS). IEEE, 2023, pp.
Computing,vol.19,no.8,pp.1760–1774,2019.
265–272.
[19] A. H. Tan, F. P. Bejarano, Y. Zhu, R. Ren, and G. Nejat, “Deep
reinforcementlearningfordecentralizedmulti-robotexplorationwith
macroactions,”IEEERoboticsandAutomationLetters,vol.8,no.1,
pp.272–279,2022.