Sample Design Engineering: An Empirical Study of What Makes Good
Downstream Fine-Tuning Samples for LLMs
BiyangGuo1†,HeWang1†,WenyilinXiao1†
HongChen2†,ZhuxinLee3,SongqiaoHan1∗,HailiangHuang1,4∗
1AILab,SIME,ShanghaiUniversityofFinanceandEconomics
2AntGroup,3GuangdongYunxiTechnology
4KeyLaboratoryofInterdisciplinaryResearchofComputationandEconomics,
MinistryofEducation,China
Abstract
Prompt Engineering
For zero-shot/ICL
IntheburgeoningfieldofLargeLanguageMod-
els(LLMs)likeChatGPTandLLaMA,Prompt
Engineering (PE) is renowned for boosting Input LLLLMMss Output
zero-shotorin-contextlearning(ICL)through
prompt modifications. Yet, the realm of the
Frozen, very large LLMs
sampledesignfordownstreamfine-tuning,cru-
cialfortask-specificLLMadaptation,islargely
Sample Design Engineering
unexplored. This paper introduces Sample
DesignEngineering(SDE),amethodicalap- For downstream-tuning
proach to enhancing LLMs’ post-tuning per-
formance by refining input, output, and rea-
soning designs. We conduct a series of in- Input LLMs Output
domain(ID)andout-of-domain(OOD)exper-
Trainable, smaller open-source LLMs
imentstoassesstheimpactofvariousdesign
options on LLMs’ downstream performance,
Figure1: AsimplifiedcomparisonbetweenPEandour
revealingseveralintriguingpatternsthathold
proposedSDE.
consistently across different LLMs. Based
on these insights, we propose an integrated
SDE strategy, combining the most effective
consolidatingthemethodologiesforvarioustasks
options,andvalidateitsconsistentsuperiority
undertheunifiedframeworkoftextgeneration. In
overheuristicsampledesignsincomplexdown-
this background, Prompt Engineering (PE) has
streamtaskslikemulti-aspectsentimentanal-
emergedasakeyareainleveragingcutting-edge
ysis,eventextraction,andnestedentityrecog-
nition. Additionally,analysesofLLMs’inher- LLMs, leading to advances in applying LLMs to
ent prompt/output perplexity, zero-shot, and newtasks(Brownetal.,2020),enhancinglogical
ICLabilitiesillustratethatgoodPEstrategies reasoning (Wei et al., 2022), and increasing task-
maynotalwaystranslatetogoodSDEstrate- specific accuracy (Wang et al., 2023a; Wei et al.,
gies. Codeavailableathttps://github.com/
2023),withoutupdatingmodelweights.
beyondguo/LLM-Tuning.
WhilenumerousPEtechniqueshavebeendevel-
opedforLLMs’zero-shotandin-contextlearning
1 Introduction
(ICL),thechallengeofdesigningeffectivetraining
TheemergenceofLargeLanguageModels(LLMs) samples for fine-tuning LLMs—termed Sample
suchasGPT-3(Brownetal.,2020),PaLM(Chowd- DesignEngineering(SDE)inthispaper—remains
heryetal.,2023),LLaMA(Touvronetal.,2023a) underexplored. SDEiscrucialfortailoringsmaller
and GPT-4 (Achiam et al., 2023) revolutionized open-sourceLLMstospecificrequirements,espe-
naturallanguageprocessing(NLP),enablingcom- ciallygiventhecomplexityoftrainingsamplesfor
plextaskstobetackledwithasinglemodel. This downstreamtasks. Figure1isasimplifieddemon-
shifthasprofoundlybroadenedtherangeoftasks strationofPEandSDE.
manageablebyNLPmodels,whilesimultaneously Toaddressthisgap,thispaperundertakesade-
tailedandcomprehensiveexplorationofSDEfor
†EqualContribution
LLMs’downstreamfine-tuning. Ourstudyisbased
∗Correspondingauthors,emails:
han.songqiao@shufe.edu.cn,hlhuang@shufe.edu.cn onthehypothesisthatthestructureorelementsof
4202
rpA
91
]LC.sc[
1v33031.4042:viXratrainingsamplesmayhaveabigimpactonthefine- etal.(2024)). Thesemodelsarepre-trainedonex-
tunedLLMs. Differentsampledesignsmaymake tremelyvastcorpora,acquiringawealthofknowl-
iteasierorharderfortheLLMstolearn,especially edgeandpatterns,whichenablesthemtodirectly
inscenarioswheredataisscarce. performcomplextasksthroughcarefulpromptde-
WebeginbyidentifyingarangeoftypicalSDE sign. For instance, Brown et al. (2020) use care-
optionsandcategorizingthemintothreegroups: in- fullycraftedpromptsandin-context-learning(ICL)
put,output,andreasoningdesignoptions(shown techniquestoguideGPT-3onnoveltaskswithout
in Figure 2). To reveal the impact of each SDE training; Wei et al. (2022) propose the Chain-of-
option,weconductexperimentsonatypicaldown- Thought(CoT)techniquethatcanboostthelogic
streamscenario–multi-aspectsentimentanalysis reasoningperformance;RAG(Lewisetal.,2020)
(MASA),with2in-domain(ID)tasksand2out-of- and CoVe (Dhuliawala et al., 2023) methods are
domain (OOD) tasks. Different from instruction- usedtoreducehallucinationduringgeneration;Li
tuningdatasetslikeFLAN(Longpreetal.,2023), etal.(2023)introduceEmotionPrompttoimprove
theMASAtaskinvolvesmorecomplicatedinput LLMs’emotionalintelligence.
andoutputelements,makingitsuitableforin-depth However, these most advanced and effective
investigationofdifferentsampledesigns. Compre- LLMs are either black-box models that are only
hensiveexperimentsonthese4taskswith6popular accessibleviaAPIs,orextremelylargemodelsthat
open-source LLMs are undertaken to reveal how are unaffordable for most companies to serve in
different SDE options affect downstream perfor- production. Consequently,manypractitionersturn
mances. Someinterestingandthought-provoking tosmallerbutopen-sourceLLMs,especially10B
conclusionsarerevealedthroughourexperiments. around models. In this situation, solely relying
Forexample,simplyswitchingthepositionofthe onPEforzero-shotorICLinferenceisunableto
task instruction can make a difference; adding handlemanyreal-worldcomplexNLPtasks.
placeholders to unmentioned targets brings a no-
tableperformancegain,etc.
2.2 Fine-tuningLLMs
Leveragingthesefindings,wecombinetheem-
piricallywell-performingSDEoptionsandpropose
According to the different purposes, we can di-
anintegratedSDEstrategyES-SDE.Extensiveex-
videLLMs’fine-tuningintotwotypes: instruction-
perimentson3complexdownstreamtasks(Nested-
tuning(IT)anddownstream-tuning(DT)1.
NER,EventDetection,andMASA)on2additional
ITtrainsLLMstocomprehendandexecutein-
LLMsdemonstratethatES-SDEnotablysurpasses
structions across a range of NLP tasks, enabling
weakerSDEcombination,aswellasheuristicde-
predictionsfornewtasks(Weietal.,2021;Mishra
signfromotherstudies. ES-SDE’srobustnesson
et al., 2022) with datasets like FLAN (Longpre
different training sizes, decoding randomness or
etal.,2023),Self-instruct(Wangetal.,2023b),Al-
instructionvariationfurtherunderscoresitsstable
paca(Taorietal.,2023)andHC3(Guoetal.,2023),
effectiveness.
coveringtaskslikesuchasclassification,QAand
In an exploratory analysis, we investigate the
translation. Thisismainlyappliedtobasemodels
linkbetweeneffectivepromptandsampledesigns,
to enable them to follow general human instruc-
via perplexity, zero-shot, and ICL analysis. Our
tions. DT focuses on customizing LLMs for spe-
findingssuggestthatawell-craftedPEstrategymay
cific,oftencomplex,tasksinindustrialapplications,
notnecessarilytranslatetoasuccessfulSDEstrat-
demandinghighoutputstabilityforeasierparsing
egy. Thisobservationencouragesfurtherresearch
and application in downstream products. An ex-
intoSDE’smechanisms,promisingforenhancing
ample is multi-aspect sentiment analysis, which
LLMs’downstreamapplications.
requiresdetailedtaskinstructionsandoutputs. Our
2 BackgroundandRelatedWork studycentersonSDEinDTscenarios, highlight-
ingsampledesignchallenges,buttheinsightsmay
2.1 PromptEngineering(PE) also benefit IT sample design, a topic for future
The effectiveness of PE methods is largely built exploration.
uponthestronginherentcapabilitiesofLLMs,with
mostresearchfocusingonverylargemodelssuch
1Itisalsoknownastasktuning(TT)insomeliterature,
as GPT-3, GPT-4, PaLM, etc. (refer to Sahoo like(Weberetal.,2023).InputDesignOptions OutputDesignOptions
Inst-last
ReviewText Instruction LLM Output Natural A1:S1,A2:S2,A3:S3,…,A6:S6
Lines JSON
Inst-first
Instruction A1:S1 {"aspect": "A1", "sentiment":"S1"}
Placement Instruction ReviewText LLM Output Formatof A2:S2 {"aspect": "A2", "sentiment":"S2"}
MultiplePredictions A3:S3 ...
…
No-inst A6:S6 {"aspect": "A6", "sentiment":"S6"}
ReviewText LLM Output
Input
N Io n- pM uI t LLM Oulo tpss ut H (Asa sn umd inl gin A2g ,ATo 3a ,f Ar 4U g ,An e 6tm as ree un nmt ei no tin one edd
)
OU A1-> A5S1
->S5
PU A3 AA 5->1 ->- [> P S] 5S1 A6AA -42 >-- >> [P[[ ]PP ]]
Modeling
MI
TxtLable NumLable
Input LLM Output NumT ee rx it cu aa ll Lo ar bels Ai->”positive”/”neutral”/”negative”... Ai->1/0/-1...
loss
ReasoningDesignOptions(optional)
No-CoT CoT R-CoT
Instruction NormalInst Thinkfirstthenpredict Predictthenexplain
Outputelements Label Reason(description/extraction/explanation…) Label Label Reason(description/extraction/explanation…)
e.g.{"aspect": "A1", "sentiment":"S1"} e.g.{“aspect”: “A1”, ”description”:”…”,"sentiment":"S1"} e.g.{“aspect”: “A1”,"sentiment":"S1", ”description”:”…”}
Figure2: TypicalSDEoptionstobeconsideredwhendesigningdownstream-tuningsamples,takingtheMASAtaskasan
example.Aimeansaspecti,Simeansitssentimentlabel,[P]referstoplaceholdertokens.
3 SampleDesignEngineering
Review Text:
This restaurant is on the second floor and is a bit out of the way. If driving,
you can only park in the underground parking of the mall opposite (6). The. most
3.1 TypicalSDEOptions
popular item ordered is the black tiger shrimp, which tastes good (1). Overall,
the prices are cheap (3), probably because the XX Plaza is not very popular. The
only downside is that the beverages are instant fruit juices, which don't taste We categorize sample design options into three
very good (2). The waitstaff's attitude was nice (5), they showed us how to use the
coupon to save money. aspects: input, output, and reasoning. We take
Desired Prediction: theMulti-AspectSentimentAnalysis(MASA),a
food (1) Positive hygiene (4) Unmentioned typicaldownstreamtask,asanexampletoclarify
beverage (2) Negative staff (5) Positive
eachdesignoptionforfine-tuningsamples. Asil-
price (3) Positive parking (6) Negative
lustrated in Figure 3, MASA requires analyzing
Figure3: AnexamplefortheMASAtask.
reviewtextstoassignsentimentstopredefinedas-
pects, while some aspects may be unmentioned.
Figure2isanoverviewofdifferentSDEoptions,
which should be considered to design proper DT
2.3 Parameter-efficientfine-tuning samples.
3.1.1 InputDesignOptions
The expansion of language models has made tra- a.InstructionPlacement: Weexploretheeffect
ditional full-parameter fine-tuning (FFT) less vi- ofinstructionpositioningrelativetotasktext(for
able due to its high computational and storage MASA,thereviewtext),examiningInst-first(be-
demands. Parameter-efficientfine-tuning(PEFT) fore the task text), Inst-last (after the task text).
methods,suchasprefix-tuning(LiandLiang,2021), WealsocomparewiththeNo-inst(noinstruction)
prompt-tuning(Lester et al., 2021), p-tuning(Liu optiontoevaluatetheeffectivenessofexplicitin-
et al., 2023), and LoRA(Hu et al., 2021) provide structions, as used in many previous conditional
cost-effective alternatives that retain FFT’s effec- textgenerationtasks(Lewisetal.,2019;Guoetal.,
tiveness, gaining popularity in industrial applica- 2022;Zhangetal.,2023).
tions. These techniques are adaptable to both IT b. Input Modeling: Considering the distinction
and DT scenarios. In this research, we use the between unified sequence modeling in LLM pre-
widely-usedLoRAasthedefaultfine-tuningtech- trainingandtheexplicitinput/outputsegmentation
nique. However,webelieveresultsfromourstudy infine-tuning,wecompareNo-MIthatexcluding
arealsoapplicabletootherPEFTmethods. inputfromlosscalculation,akintoLLaMA2’sSFTprocess(Touvronetal.,2023b))againstMI(mod- evaluations,fortheChineseonlinereviewMASA
elinginputinbackpropagation). scenario. The data is provided and annotated by
our collaborating company, which encounters a
3.1.2 OutputDesignOptions
real-worldbusinessneedfortheanalysisofexten-
a.MultiplePredictionsFormatting: Fortasksne-
sivecustomeronlinereviews. Thedataannotations
cessitatingseveralpredictions,weevaluateoutput
comefromtwodomainsofaspects: D1aboutfood,
formattingfromlesstomorestructured: Natural
beverage, price, hygiene, staff attitude, and park-
(free-formtext),Lines(eachaspectonanewline),
ingconvenienceandD2abouttrafficconvenience,
andJSON (JSON-linesforprecisionandexplicit-
queuing,servingspeed,decoration,andnoise. The
ness).
modelneedstogiveasentimentlabelfrom{posi-
b.HandlingUnmentionedTargets: Weconsider
tive,neutral,negative}foreachaspect,whilesome
whethertoomittheunmentioned(OU)targetsin
aspectsmaynotoccurinthereview. Basedonthe
the output, or place placeholders (PU) for those
twodomains,weconstructthefollowing4tasks:
targets. Theplaceholdertokenscanbestringslike
• D1⇒D1 and D2⇒D2 are two ID evaluation
"Unmentioned","None",or"[]"accordingtotasks.
tasks,wheretrainandtestsetscomefromthesame
c.Textualornumericallabels: Bydefault,weuse
domains;
theTxtLabeloptionfortextualoutputlabels. How-
•D1⇒D2andD2⇒D1aretwoOODgeneraliza-
ever, in some cases, using numbers to represent
tiontasks,wherethemodeltrainsononedomain
outcomes(NumLabel)mayenhancepredictionro-
buttestsonanunseendomain.
bustness.
Consideringthehighcostofannotationinindus-
tries and the fact that fine-tuning LLMs requires
3.1.3 ReasoningDesignOptions
lessannotateddata(Zhouetal.,2024),wetrainthe
Many tasks require reasoning, where the Chain-
model with 500 and 1,000 samples, respectively.
of-Thought (CoT) (Wei et al., 2022) has shown
We use a large test set containing around 8,000
promise in improving LLM’s reasoning in zero-
samplestomakeresultsmorestableandconvinc-
shotandICL,aswellasITscenarios(Kimetal.,
ing. DatasetdetailsseeAppendixA.2.
2023). Yet,itsimpactonDTremainslessstudied.
Models. Weutilizethefollowingwidelyusedopen-
WeintroducetheCoT optionfortrainingmod-
sourceLLMsof7Bsizeofboththebaseandchat
els to "think before they predict". We use JSON
versions: 1)chinese-llama-2-7b(noteasc-llama2-
asthedefaultoutputformattomaketherepresen-
base) and the instruction-tuned version chinese-
tation clearer and add a new description field
alpaca-2-7b (c-llama2-chat) from the Chinese-
before the sentiment field. Conversely, the R-
LLaMA2 series (Cui et al., 2023), which is the
CoT (Reverse-CoT)reversesthesefields,enabling
vocabulary-expanded version of LLaMA2 (Tou-
a"predictthenexplain"approachtoexploreCoT’s
vronetal.,2023b)withsecondarypre-trainingand
mechanics further. Note that Implementing CoT-
fine-tuningonChinesecorpus;2)internlm-7b-base
likesamplesincursadditionalannotationcostsdue
(intern-base)andinternlm-7b-chat(intern-chat)
to the description fields, making the reasoning
from the InternLM series (Team, 2023), which
designoptionstask-dependent.
are pretrained on trillions of high-quality tokens,
3.2 IntegratedSDEStrategy performs well in Chinese and English tasks; 3)
baichuan2-7b-base(bc2-base)andbaichuan2-7b-
Afinalsampledesignisacombinationoftheabove
chat(bc2-chat)fromtheBaichuan2series(Yang
designoptions,whichwecallanintegratedSDE
et al., 2023), one of the SOTA LLMs at the time
strategy. Thispaperinitiallyexplorestheimpact
of release. We use LoRA as the default efficient
ofeachindividualoptionthroughextensiveexperi-
fine-tuningtechnique. Hyperparametersandother
mentation,leadingtotheproposalofanevidence-
trainingdetailscanbefoundinAppendixA.2.
basedintegratedSDEstrategy.
EvaluationMetrics. WeevaluatetheMASA’sper-
4 ExperimentsI:EvaluatingTheImpact formance from two perspectives: 1) Sentiment
ofEachSDEOption analysis performance. We use the weighted
Kappa score κ (Cohen, 1968) for this measure-
4.1 Settings
mentconsideringtheimbalanceofdifferentaspects
Tasks and Datasets. We experiment with in- and the ordinal nature of sentiment labels. The
domain(ID)evaluationsandout-of-domain(OOD) weightedKappascoreallowsforsettingweightstoInputDesignOptions
0 0. .7 78 3 +.0121 +.0296ID +.0085 0 0. .6 55 5 -.0057 +.01O 9O 3D +.0180 0 0. .7 78 3 +.0073 -.0057ID -.0118 00 .. 56 55 +.0062 +.046O 3OD +.0279 I Nn os -t M-la Ist,
0.68 -.0259 -.0114 -.0584 0.45 -.1069 -.0517 0.68 -.0298 -.0130 -.0438 0.45 -.1558 I N Nn o os - -t M I- nfi I sr ts ,t,
0.63 -.1300 -.0701 0.35 -.2449 0.63 -.1033 -.0862 -.0885 0.35 -.1525 N Ino s- tM -laI st,
0.58 c-llama2-chat intern-chat bc-. 212 -c4 h8 at 0.25 c-llama2-chat intern-chat bc2-chat 0.58 c-llama2-base intern-base bc2-base 0.25 c-llama2-base intern-base -.2 b69 c5 2-base MI
OutputDesignOptions
0 00. ..7 678 83+.004+0.0 -0 .03 42
52-.0517+.001 -8
.01I
+
8D
7.0013
+.0 -0 .03 07
1-6.0104
00 0.. .56 455 5+.0 -0 .02 18 0 -1 .0758-.0488+.0 -0 .08 29 8O 0+O .0D 21 -.9 0202+.0099+.0 -3 .07 07
30
0 00. ..7 678 83
-.019 -.06 30 -8 .0596
+.00 -.0 07 10I -.D
3 0353
-.00+ 58.00 -2 .07
333
0 0 0. . .6 5 45 5 5-.024-8.039-.50398 -.1168+.01 -.3 09 0O 8O
-7
.0D
37 -7
.1075+.0 -0 .09 03
8 -3 .053 -5 .0702
N T L
T J
TSx i
x
xa Ont
t
tt L e
L
Lu Na s
a
ar ,,b a
b b
PPl e
e
eU, U l
l
lP ,, U,
0.63
-.0822
-.0801 0.35 -.1677 0.63 -.1055
-.0927
-.0816 0.35 N Na ut mur La al b, P elU,
0.58 c-llama2-chat intern-chat bc2-chat 0.25 c-llama2-chat intern-chat bc2-chat 0.58 c-llama2-base intern-base bc2-base 0.25 c-llama2-base intern-base bc2-base N Txa tt Lu ar bal e, lOU,
ReasoningDesignOptions
0 00. ..7 678 83 -.0119-.0106 +.019I 4D
+.0105
-.0147+.0021 00 0.. .56 455
5
+.0038 -.0118 +.07O 7O 3 +D .0223 +.0436 +.0102 0 00. ..7 678 83 +.0094+.0098 +.000I 0D 5
-.0017
+.0098+.0022 00 0.. .56 455
5
+.0083 +.005O 8 -O .0D
143
+.0090+.0084 CN Ro -o CT- oC ToT
-.1301
c-llama2-chat intern-chat bc2-chat c-llama2-chat intern-chat bc2-chat c-llama2-base intern-base bc2-base c-llama2-base intern-base bc2-base
Chat-LLMs Base-LLMs
Figure4: Sentimentanalysisperformances(κ)ofdifferentSDEoptions. ResultsofIDaretheaverageofD1->D1
andD2->D2,sameforOOD.Thebarsdepicteachmethod’srelativeimprovementordegradationcomparedtothe
baseline,witheachmethoddifferingfromthebaselineinonlyoneoption(coloredinred). Detailedresultsforeach
taskseeTable3-8.
enableanuancedassessmentofdifferentclassifi- vealedfromtheresults.
cationerrordegrees(YilmazandDemirhan,2023).
ConclusionsforInputOptions:
For example, classifying "positive" as "negative"
1)InstructionsenhanceDTperformances: The
ismoredetrimentalthanclassifying"positive"as
No-InstoptionleadstopoorerperformanceinID
"neutral," hence a higher penalty should be im-
tasksandalackofOODgeneralizationabilitycom-
posed on the former. 2) Format adherence, to
paredtoInst-firstorInst-lastmethodsthatincorpo-
assessthegenerationstabilityofLLMs. It’svital
rateinstructions. Thisunderlinesthecriticalrole
to have good format adherence ability for LLMs
ofincludinginstructionsforimprovingbothunder-
ondownstreamtaskssotheoutputcanbeparsed
standingandgeneralizabilityofLLMs.
successfully. We report the format-parsing error
2)Bettertoplaceinstructionfirst: TheInst-first
rateforthismetric. Notethatwhencalculatingκ,
method outperforms Inst-last across both ID and
weuserelaxedparsingrulestoallowsomeminor
OODtasksfordifferentLLMs. Thisdemonstrates
uncertaintyofaspect/labelexpressions. Ifacertain
thesignificanceofinstructionplacementforLLMs’
aspectcanstillnotbeparsedcorrectly,thisaspect
tuningprocess. Wehypothesizethatthismaypartly
is treated as "unmentioned". The definition of κ,
beexplainedbytheattentionmechanism,seeAp-
Kappaweightmatrix,andformat-parsingrulescan
pendixA.6.
beseeninAppendixA.1.
3)Modelinginputdetractsfromperformance:
EmployingtheMI approachresultsinworseout-
4.2 ExperimentalResultsonEachOption
comes compared to the No-MI baselines across
We report and analyze the results from two per-
variousmodelsandtasks. Thisindicatesthatmod-
spectives—sentimentanalysisperformances,and
elingtheinputpartduringfine-tuningmayhinder
formatadherenceabilities.
theLLM’seffectiveness,suggestingacautiousap-
4.2.1 SentimentAnalysisPerformance proachtowhataspectsofthetaskaremodeled.
Wefirstassessthesentimentanalysisperformances ConclusionsforOutputOptions:
of LLMs using different sample design options. 1)Linesisareliableoutputformatformultiple
ThecomparativeresultsofIDandOODtaskson3 predictions: TheLinesformat,positionedbetween
Chat-LLMsand3Base-LLMsareplottedinFigure theNaturalandJSON formats,demonstratesstable
4(fullresultsseeTable3toTable8inAppendix andhighperformanceinsentimentanalysisacross
A.4). Somesharedandintriguingpatternsarere- variousmodelsandtasks. ItseffectivenessliesinInput Options Output Options Reasoning Options
7 8
6 7 25
5 Options 6 Options 20
Natural, TxtLabel, PU Options
4 I In ns st t- -l fa irs st t, , N *o-MI 45 Lines, *, * 15 No-CoT, JSON
3 JSON, *, * CoT, * No-inst, * 3 *, NumLabel, * 10 R-CoT, *
2 *, MI 2 *, *, OU
1 1 5
0 0 0
intern (ID) bc2
c
-( lI lD a) ma2 i( nID te) rn(OO bD c) 2
c
-( lO laO mD a) 2 (OOD) intern (ID) bc2
c
-( lI lD a) ma2 i( nID te) rn(OO bD c) 2
c
-( lO laO mD a) 2 (OOD) intern (ID) bc2
c
-( lI lD a) ma2 i( nID te) rn(OO bD c) 2
c
-( lO laO mD a) 2 (OOD)
Figure5: Formatadherenceperformance,measuredbyparsingerrorrates(%). ’*’meanssameoptionasabove.
offeringstructuredinformationwhileretainingnat- ofunmentioningsuffergreaterunderperformance
ural language readability, making it versatile for withOU comparedtoPU,seeAppendixA.7.
differentLLMs. ConclusionsforReasoningOptions:
2) Base-LLMs exhibit similar patterns while 1)SubtleimpactofCoTonID,whilesignificant
Chat-LLMsdiverse: Basemodelsrespondsimi- onOODtasks: CoTdesignmarginallyaffectsID
larly to output formats, indicating consistency in tasks but markedly improves OOD performance.
their responses. In contrast, Chat models, such This contrast highlights CoT’s role in enhancing
as bc2-chat and cllama2-chat, exhibit varied per- modelreasoningandadaptabilityinunfamiliarcon-
formances,suggestingdifferencesintheirSFTor texts,underpinningitsvalueforgeneralization.
RLHFdata’sstructure. Forinstance,bc2-chatand 2) "Think before predict" beats "predict then
cllama2-chatperformwellwithJSON format,un- explain": When the reasoning step is placed af-
likeintern-chat,implyingavarianceintheamount terpredicting,liketheR-CoT method,theperfor-
ofstructureddatausedintraining. mance does not match that of the standard CoT
3)Base-LLMsfavormorenaturalformatswhile approach. However, R-CoT can still outperform
Chat-LLMscanfitorbearmoresophisticated No-CoT in many cases, suggesting that a single
formats: Base models prefer Natural and Lines reasoningcomponentisalsobeneficial.
overJSON.Conversely,Chatmodelsleantowards
4.2.2 FormatAdherencePerformance
structuredformats,withLinesandJSON.Thisdi-
Figure 5 presents the results of the format adher-
vergencehintsatthedifferenttrainingbackgrounds,
enceperformancesforChat-LLMs,fromwhichwe
with Chat models being more accommodating to
findthat: 1)WhiletheInst-firstmethodimproves
sophisticateddataformats. Onemorepieceofev-
sentimentanalysis,itshowslessstabilityinformat
idence is that the NumLabel option brings much
adherence,especiallyinOODscenarios,indicating
moredamagetotheBasemodelsthantotheChat
that leading with instructions might increase for-
models,whichislessnaturalthanTxtLabel.
materrors withunfamiliar content; 2) Structured
4)Textualovernumericlabels: Switchingfrom
designoptionsleadtobetterformatadherenceabil-
textual to numeric labels worsens performance,
ities: Anoticeabletrendisthatstructuredoutputs,
likelybecausenumericlabelslackthedescriptive
especially in the order JSON > Lines > Natural,
depthandcontextcluesthattextuallabelsprovide,
have lower format error rates. JSON format, in
crucialforLLMstrainedonnaturallanguagetext.
particular, demonstrates strong adherence to the
5) Omitting the unmentioned targets may not
correct structure, highlighting a balance between
be a good choice: While the OU option, which
outputcomplexityandprecision;3)MI,NumLabel
excludesunmentionedaspects,mightseemtosim-
andCoT optionscanbequiteunstableforcertain
plify outputs, it also introduces format inconsis-
LLMs, while other options are generally consis-
tency. This lack of uniformity forces the model
tentacrossdifferentmodels. Inapplicationswhere
toadapttovariedaspectmentionspersample,in-
stabilityisvital,theseunstableoptionsshouldbe
creasingtaskcomplexitywithdynamicadjustment
taken seriously; 4) Though improving the under-
oftheoutputformat. Instead,thePU optionkeeps
standing or reasoning performances, CoT design
aconsistentoutputformatbyaddingplaceholders,
puts LLMs at a higher risk of parsing failure for
perhapsmakingLLMseasiertolearn. Additional
customizeddownstreamtasks,underliningatrade-
analysisshowsthattheaspectswithahigherdegree
offforthisoption.
etaR
rorrE
tamroF
etaR
rorrE
tamroF
etaR
rorrE
tamroF(a)
Performanceof different
sample design strategies
with increasing training
sizes.
trainingsize trainingsize trainingsize
(b)
Robustness on decoding
sampling randomness,
trainingsize=500.
(c)
Robustness on instruction
contentvariation,training
size=500.
Figure6: Comparisonofdifferentsampledesignstrategies.
ConsideringLLMs’formatadherencealongside demonstratethattheperformanceofChatGPTin
theunderstandingabilitiesiscrucialforspecialized EDtasksfallsbelowexpectations. Weusethetop-
downstreamapplications,suggestinganeedfora 10eventtypesinourexperiments.
balancedapproachinindustrialscenarios. • Review11. This is our self-collected Chinese
MASAdatasetthatinvolves11aspects,morecom-
5 ExperimentsII:AnRobustIntegrated
plicatedthantheMASAtasksinSection4.
SDEStrategy
Baselines. AsacomparisontoES-SDE,wealso
proposeanempiricallyweakSDEstrategy(EW-
Basedontheexperimentalevidencefromthepre-
SDE),combiningInst-last,Natural,andOU,while
vioussection,weproposeanempiricallystrong
keepingotheroptionsthesame. Wenaturallyhy-
SDEstrategy(termedasES-SDE)usingthewell-
pothesizethatEW-SDEshouldbeweakerthanES-
performing options: a combination of Inst-first,
SDE. Note that ES-SDE and EW-SDE are both
No-MI inputdesignsandLines,PU,TxtLabelout-
evidence-basedstrategiesaccordingtotheprevious
putdesigns. Wedon’tusetheCoT designbecause
empiricalresults,therefore,wealsosetupaheuris-
ofitshighannotationcostandrelativelyunstable
tic-basedbaseline,referringtothepromptdesigns
output. Inthissection,weconductcomprehensive
fromthestudyofHanetal.(2023),whicharesim-
experimentstovalidateitseffectivenessacrossdif-
ilartoacombinationofInst-firstandOU options,
ferentdownstreamtasks,aswellastherobustness
with a "lines-of-list" output format. Examples of
againstperturbationsininstructionsorgeneration.
thesestrategiesseeAppendix11.
5.1 Settings Models. For a more generalized evaluation, we
utilizetwonewLLMs,insteadofthoseusedinSec-
Tasksanddatasets. Toevaluatetheeffectiveness
tion4. Consideringthetasklanguage,thellama2-
ofES-SDE,weconductexperimentsonthreechal-
7b-chat (Touvron et al., 2023b) is used for GE-
lengingdownstreamtasks:
NIAandMAVENandqwen1.5-4b-chat(Baietal.,
•GENIA(Ohtaetal.,2002). Anestednameden-
2023), a very latest LLM, is used for Review11.
tityrecognition(Nested-NER)datasetinthemolec-
ThetrainingdetailsarethesameasSection4.
ular biology domain, where ChatGPT (GPT-3.5)
onlyachievesanF1scoreof50.89%,using5-shot
5.2 Results
CoTreasoning(Hanetal.,2023).
•MAVEN(Wangetal.,2020). Ageneraldomain Figure6reportsthecomparisonbetweendifferent
event detection (ED) dataset. Han et al. (2023) sample design strategies, from different perspec-tives. Soft-matchF1scores(Hanetal.,2023)are InputOptions OutputOptions ReasoningOptions
reportedforGENIAandMAVEN,andκreported SDE
forReview11. MoredetailedresultsseeAppendix
Zero-shot
A.5. Severalkeyconclusionscanbeobserved:
1)ES-SDEmaintainsadvantagesacrosstasks ICL
and training sizes. Figure 6-(a) demonstrates a
PromptPPL
consistenttrendthatES-SDEkeepsitsadvantage
as the training size increases from 500 to 4,000. PredictionPPL
Notably, 500 ES-SDE samples worth ∼ 2,000
EW-SDEandheuristicsamplesinGENIAandRe- Figure7: AveragerankingsoftheDTperformancesof
view11 tasks, indicating the high quality of ES- SDEoptionsandzero-shot/ICL/PPLrankingsoftheir
SDEsamples. 2)Stableondecodingrandomness. correspondingprompts. ResultsbasedontheMASAID
Bydefault,themodelemploysagreedydecoding tasksacross6LLMs.
strategy(nosampling). Figure6-(b)showsthere-
sultswhenactivatingdecodingsamplingwithvary-
Ouranalysisrevealedsomeconsistentpatterns:
ingrandomseeds. ES-SDEmaintainsexceptional
Inst-firstisaneffectivechoiceforbothPEandSDE;
stabilityacrossdifferentseedsonthreetasks. The
CoT improvesperformancesforbothPEandSDE
adoptionofdecodingsamplingtendstodiminish
evaluations. However,therearealsomanycounter-
the performances of both SW-SDE and heuristic
intuitive findings. For example, the OU option
strategiesforGENIAandMAVEN,whileES-SDE
consistentlyharmsDTperformancesaccordingto
gives stable performances. 3) Robust to instruc-
ourpreviousexperiments,however,itscorrespond-
tion variation. For instructions about a specific
ingpromptsresultsinnotablybetterzero-shotor
task,wehavevariouswaysofexpressingthesame
ICLresultsforcertainLLMs;Similarly,whilethe
idea. Therefore,wevalidatethesensitivityofdif-
NaturaloptionoutperformstheLinesapproachfor
ferent strategies to different formulations of the
base models in SDE, the reverse is true in zero-
instruction, by changing the common content to
shotorICLevaluationsformodelslikec-llama2-
otherformulations(examplesinAppendix12). As
baseandintern-base. Gonenetal.(2023)showed
shown in Figure 6-(c), ES-SDE keeps its edge in
through a wide range of tasks that the lower that
different variations, showing its robustness to in-
lower perplexity (PPL) generally leads to better
structioncontent.
promptdesigns. Inspiredbythis,wealsoconduct
Overall, ES-SDE represents a reliable and po-
PPLanalysisontheICLprompts/predictionscorre-
tent approach for the DT of LLMs, illustrating
spondingtoeachSDEoptions. Interestingly,OU-
that—through a careful SDE process, LLMs can
likepromptgivesthehighestaveragedPPLscores
achievemuchhigherperformancesindownstream
acrossalloptions,whichseemstobecontradictory
tasks. NotethatES-SDEmaynotbethebeststrat-
thatOU bringsbetterzero-shotorICLresults. The
egyforalltasks. AdetailedinvestigationintoSDE
JSON formatsurprisinglyachievesratherlowPPL
across a broader spectrum of tasks and models
scores, howeveritsSDEperformancesareworse
couldyieldevenmoreeffectivestrategies.
thanLines.
These findings highlight a complex landscape
6 CanPEguideSDE?AnAdditional
where prompt design patterns do not always
Analysis
align with SDE effectiveness, underscoring the
nuancedrelationshipbetweenPEandSDE.
Promptsarethekeytounderstandmodels’innate
qualitiesandcapabilities. AgoodPEmethodoften
7 Conclusion&FutureWork
indicatessomepatternsthataLLMismorefamiliar
with or excels in. A natural question is: can PE In this study, we introduce SDE as an effective
guideSDE?Toanswerthisquestion,wecraftzero- methodtoenhancethedownstream-tuningperfor-
shotandICLpromptsaccordingtodifferentSDE mances of LLMs. Through comprehensive ID
optionstoevaluatetheirPEperformances. Figure and OOD experiments involving six LLMs, we
7reportstheaveragerankingsofSDEoptionsand demonstrate the effects of various sample design
theircorrespondingpromptsintheMASAIDtasks. strategies, uncovering some interesting patterns
DetailedresultsforeachtaskseeAppendixA.8. that are consistent across different LLMs. Build-ingonthesefindings,wedeveloptheES-SDEap- lesspractical. Secondly,evaluatingLLMson
proach,whichintegratesthemosteffectiveoptions. downstreamtasksisbothresource-intensive
Our experiments on three new tasks with two ad- and costly, due to the need for customized
ditionalLLMsconsistentlyshowES-SDE’ssupe- task metrics, parsing rules, and high model
riorityoverbaselinemethods. Furtheranalysisof inferencecosts. Therefore,developingamore
therelationshipbetweenPEandSDEsuggeststhat efficientframeworkforSDEstudiesisacriti-
effectivepromptdesignsdonotnecessarilytrans- calobjectiveforfutureresearch.
latetosuccessfulsampledesigns. Thisobservation
opensupavenuesformoredetailedinvestigations
intothemechanismsofSDEinfutureresearch. References
8 Limitations JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Thisresearchfollowsatwo-stepexperimentalap- DiogoAlmeida,JankoAltenschmidt,SamAltman,
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
proach. Inthefirststep,weinvestigatetheimpact
arXivpreprintarXiv:2303.08774.
of each SDE option, the results are then used as
evidence for the second step—proposing an em- JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
piricallystrongSDEcombinationstrategy. Asan
Huang,BinyuanHui,LuoJi,MeiLi,JunyangLin,
empiricalstudy,thisresearchissubjecttocertain
RunjiLin,DayihengLiu,GaoLiu,ChengqiangLu,
limitations: KemingLu,JianxinMa,RuiMen,XingzhangRen,
XuanchengRen,ChuanqiTan,SinanTan,Jianhong
1. Whilewedemonstratethattheexperimental
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
findingsfromthefirstphaseareextendableto guangWu,BenfengXu,JinXu,AnYang,HaoYang,
differentdownstreamtasks,theapplicability Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
HongyiYuan,ZhengYuan,JianweiZhang,Xingx-
tootheruntestedscenariosremainsuncertain.
uanZhang,YichangZhang,ZhenruZhang,Chang
Forinstance,althoughtheLinesoutputdesign
Zhou,JingrenZhou,XiaohuanZhou,andTianhang
outperformstheJSON formatinourcurrent Zhu.2023. Qwentechnicalreport. arXivpreprint
experiments, it is unclear if this advantage arXiv:2309.16609.
persistsinmorecomplextaskswithintricate
Arie Ben-David. 2008. Comparison of classification
structures. Futureresearchwilladdressthese accuracyusingcohen’sweightedkappa. ExpertSys-
morechallengingcontexts; temswithApplications,34(2):825–832.
2. WiththerapidpaceofadvancementsinLLMs, Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
newandmoresophisticatedmodelsarebeing
Neelakantan,PranavShyam,GirishSastry,Amanda
introducedfrequently. Themodelsweusedin
Askell,etal.2020. Languagemodelsarefew-shot
our study were among the best open-source learners. Advancesinneuralinformationprocessing
options available at the start of our research systems,33:1877–1901.
but have since been surpassed by newer re-
StanleyFChen,DouglasBeeferman,andRoniRosen-
leases. Although we assessed a total of 8 feld.1998. Evaluationmetricsforlanguagemodels.
LLMs,includingbothbaseandchatvariants,
AakankshaChowdhery,SharanNarang,JacobDevlin,
there remains a possibility that our findings
MaartenBosma,GauravMishra,AdamRoberts,Paul
may not be universally applicable to other Barham,HyungWonChung,CharlesSutton,Sebas-
models; tianGehrmann,etal.2023. Palm: Scalinglanguage
modelingwithpathways. JournalofMachineLearn-
3. CombiningdifferentSDEoptionsposessig- ingResearch,24(240):1–113.
nificantchallenges,particularlywithoutprior
JCohen.1968. Weightedkappa: nominalscaleagree-
validation experiments such as those de- mentwithprovisionforscaleddisagreementorpar-
scribed in Section 4. The challenges are tialcredit. Psychologicalbulletin,70(4):213–220.
twofold. Firstly, unlike typical hyperpa-
Jacob Cohen. 1960. A coefficient of agreement for
rameters like learning rate or network lay- nominalscales. Educationalandpsychologicalmea-
ers, choosing different SDE options alters surement,20(1):37–46.
the training data itself, rendering traditional
YimingCui,ZiqingYang,andXinYao.2023. Efficient
hyperparameter-tuning techniques such as
and effective text encoding for chinese llama and
Bayesian Optimization (Snoek et al., 2012) alpaca. arXivpreprintarXiv:2304.08177.Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
RobertaRaileanu,XianLi,AsliCelikyilmaz,andJa- Petroni,VladimirKarpukhin,NamanGoyal,Hein-
sonWeston.2023. Chain-of-verificationreduceshal- richKüttler, MikeLewis, Wen-tauYih, TimRock-
lucinationinlargelanguagemodels. arXivpreprint täschel,etal.2020. Retrieval-augmentedgeneration
arXiv:2309.11495. forknowledge-intensivenlptasks. AdvancesinNeu-
ralInformationProcessingSystems,33:9459–9474.
MikelGalar,AlbertoFernández,EdurneBarrenechea,
Humberto Bustince, and Francisco Herrera. 2011.
Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,
Anoverviewofensemblemethodsforbinaryclassi-
WenxinHou,JianxunLian,FangLuo,QiangYang,
fiersinmulti-classproblems: Experimentalstudyon
and Xing Xie. 2023. Large language models un-
one-vs-oneandone-vs-allschemes. PatternRecogni-
derstandandcanbeenhancedbyemotionalstimuli.
tion,44(8):1761–1776.
arXivpreprintarXiv:2307.11760.
HilaGonen,SriniIyer,TerraBlevins,NoahASmith,
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
andLukeZettlemoyer.2023. Demystifyingprompts
Optimizing continuous prompts for generation. In
in language models via perplexity estimation. In
Proceedingsofthe59thAnnualMeetingoftheAsso-
FindingsoftheAssociationforComputationalLin-
ciationforComputationalLinguisticsandthe11th
guistics: EMNLP2023,pages10136–10148.
InternationalJointConferenceonNaturalLanguage
MargheritaGrandini,EnricoBagli,andGiorgioVisani. Processing (Volume 1: Long Papers), pages 4582–
2020. Metrics for multi-class classification: an 4597.
overview. arXivpreprintarXiv:2008.05756.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
BiyangGuo,YeyunGong,YelongShen,SongqiaoHan, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. Gpt
HailiangHuang,NanDuan,andWeizhuChen.2022. understands,too. AIOpen.
Genius: Sketch-basedlanguagemodelpre-training
viaextremeandselectivemaskingfortextgeneration Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
andaugmentation. arXivpreprintarXiv:2211.10330. Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,
collection: Designingdataandmethodsforeffective
JinranNie,YuxuanDing,JianweiYue,andYupeng
instructiontuning. InInternationalConferenceon
Wu.2023. Howcloseischatgpttohumanexperts?
MachineLearning,pages22631–22648.PMLR.
comparisoncorpus,evaluation,anddetection. arXiv
preprintarXiv:2301.07597.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
RidongHan,TaoPeng,ChaohaoYang,BenyouWang, Hannaneh Hajishirzi. 2022. Cross-task generaliza-
LuLiu,andXiangWan.2023. Isinformationextrac- tionvianaturallanguagecrowdsourcinginstructions.
tionsolvedbychatgpt? ananalysisofperformance, In Proceedings of the 60th Annual Meeting of the
evaluation criteria, robustness and errors. arXiv AssociationforComputationalLinguistics(Volume
preprintarXiv:2305.14450. 1: LongPapers),pages3470–3487.
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki
YuanzhiLi, SheanWang, LuWang, WeizhuChen, Mima,andJunichiTsujii.2002. Thegeniacorpus:
etal.2021. Lora: Low-rankadaptationoflargelan- Anannotatedresearchabstractcorpusinmolecular
guagemodels. InInternationalConferenceonLearn- biologydomain. InProceedingsofthehumanlan-
ingRepresentations. guagetechnologyconference,pages73–77.Citeseer.
Seungone Kim, Se Joo, Doyoung Kim, Joel Jang,
Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha,
SeonghyeonYe,JaminShin,andMinjoonSeo.2023.
Vinija Jain, Samrat Mondal, and Aman Chadha.
Thecotcollection:Improvingzero-shotandfew-shot
2024. Asystematicsurveyofpromptengineeringin
learning of language models via chain-of-thought
largelanguagemodels: Techniquesandapplications.
fine-tuning. InProceedingsofthe2023Conference
arXivpreprintarXiv:2402.07927.
onEmpiricalMethodsinNaturalLanguageProcess-
ing,pages12685–12708.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams.
2012. Practical bayesian optimization of machine
BrianLester,RamiAl-Rfou,andNoahConstant.2021.
learningalgorithms. Advancesinneuralinformation
The power of scale for parameter-efficient prompt
processingsystems,25.
tuning. InProceedingsofthe2021Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
pages3045–3059.
Dubois,XuechenLi,CarlosGuestrin,PercyLiang,
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan andTatsunoriBHashimoto.2023. Stanfordalpaca:
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, Aninstruction-followingllamamodel.
VesStoyanov,andLukeZettlemoyer.2019. Bart:De-
noisingsequence-to-sequencepre-trainingfornatural InternLM Team. 2023. Internlm: A multilingual lan-
languagegeneration,translation,andcomprehension. guagemodelwithprogressivelyenhancedcapabili-
arXivpreprintarXiv:1910.13461. ties.HugoTouvron,ThibautLavril,GautierIzacard,Xavier Ayfer Ezgi Yilmaz and Haydar Demirhan. 2023.
Martinet,Marie-AnneLachaux,TimothéeLacroix, Weighted kappa measures for ordinal multi-class
BaptisteRozière,NamanGoyal,EricHambro,Faisal classificationperformance. AppliedSoftComputing,
Azhar, et al. 2023a. Llama: Open and effi- 134:110020.
cient foundation language models. arXiv preprint
arXiv:2302.13971. HanqingZhang,HaolinSong,ShaoyuLi,MingZhou,
and Dawei Song. 2023. A survey of controllable
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- textgenerationusingtransformer-basedpre-trained
bert, Amjad Almahairi, Yasmine Babaei, Nikolay languagemodels. ACMComputingSurveys,56(3):1–
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti 37.
Bhosale, et al. 2023b. Llama 2: Open founda-
ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,
tion and fine-tuned chat models. arXiv preprint
JiaoSun,YuningMao,XuezheMa,AviaEfrat,Ping
arXiv:2307.09288.
Yu,LiliYu,etal.2024. Lima: Lessismoreforalign-
ment. AdvancesinNeuralInformationProcessing
Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong
Systems,36.
Ma,HaixingDai,QiushiYang,YanqingKang,Jinru
Wu,HuawenHu,ChenxiYue,HaiyangZhang,Yi-
A Appendix
HsuehLiu,XiangLi,BaoGe,DajiangZhu,Yixuan
Yuan,DinggangShen,TianmingLiu,andShuZhang.
A.1 MetricsforMASA
2023a. Promptengineeringforhealthcare: Method-
ologiesandapplications. ArXiv,abs/2304.14670. Weighted Kappa. Considering the imbalance
of different aspects and the ordinal nature of la-
XiaozhiWang,ZiqiWang,XuHan,WangyiJiang,Rong
bels,weightedagreementmeasuresareprovedto
Han, ZhiyuanLiu, JuanziLi, PengLi, YankaiLin,
and Jie Zhou. 2020. Maven: A massive general be more effective than traditional metrics (Ben-
domaineventdetectiondataset. InProceedingsofthe David, 2008; Galar et al., 2011; Grandini et al.,
2020ConferenceonEmpiricalMethodsinNatural
2020). Thus we adopt Weighted Kappa (Cohen,
LanguageProcessing(EMNLP),pages1652–1671.
1968;YilmazandDemirhan,2023)asthemeasure
YizhongWang,YeganehKordi,SwaroopMishra,Alisa of classification effect, which is an extension of
Liu,NoahASmith,DanielKhashabi,andHannaneh Cohen’s Kappa (Cohen, 1960). Weighted Kappa
Hajishirzi.2023b. Self-instruct: Aligninglanguage
κ is defined as κ = Po−Pe, which measures a
modelswithself-generatedinstructions. InThe61st 1−Pe
Annual Meeting Of The Association For Computa- model’s performance by considering how much
tionalLinguistics. better it performs than random guessing. Here,
P = (cid:80)R w p and P = (cid:80)R w p p .
LucasWeber,ElsaM.BruniBruni,andDieuwkeHup- o i,j=1 ij ij e i,j=1 ij i. .j
Theprobabilitiesp ,p ,p arevaluesoraccumu-
kes.2023. Mindtheinstructions: aholisticevalua- ij i. .j
tionofconsistencyandinteractionsinprompt-based latedvaluesfromtheclassificationconfusionma-
learning. InProceedingsofthe27thConferenceon trix. Theweightingfactor,w ,enablesanuanced
ij
ComputationalNaturalLanguageLearning(CoNLL), assessment of different error degrees. For exam-
pages294–313.
ple, classifying "positive" as "negative" is more
JasonWei,MaartenBosma,VincentZhao,KelvinGuu, detrimentalthanclassifying"positive"as"neutral,"
Adams Wei Yu, Brian Lester, Nan Du, Andrew M henceahigherpenaltyshouldbeimposedonthe
Dai,andQuocVLe.2021. Finetunedlanguagemod-
former. Basedonthefeedbackfromenterprisesin
elsarezero-shotlearners. InInternationalConfer-
practicalapplications,wedefinetheweightmatrix
enceonLearningRepresentations.
withoutlossofgeneralityasTable1.
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
etal.2022. Chain-of-thoughtpromptingelicitsrea- Pre-Pos Pre-Neu Pre-Neg Pre-Unm
soninginlargelanguagemodels. AdvancesinNeural
Label-Pos 1 1/2 0 1/2
InformationProcessingSystems,35:24824–24837.
Label-Neu 2/3 1 2/3 2/3
XiangWei,XingyuCui,NingCheng,XiaobinWang,
Label-Neg 0 1/2 1 1/2
Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,
Yufeng Chen, Meishan Zhang, et al. 2023. Zero- Label-Unm 1/2 2/3 1/2 1
shotinformationextractionviachattingwithchatgpt.
arXivpreprintarXiv:2302.10205. Table1: WeightmatrixforcalculatingweightedKappa.
AiyuanYang,BinXiao,BingningWang,BorongZhang,
Format adherence. Format adherence not only
CeBian,ChaoYin,ChenxuLv,DaPan,DianWang,
ensuresthatoutputsfromthemodelcanbereliably
DongYan,etal.2023. Baichuan2: Openlarge-scale
languagemodels. arXivpreprintarXiv:2309.10305. parsed and utilized in practical applications, butTrainSet(size=500) TrainSet(size=1000) TestSet
Pos Neu Neg Unm Pos Neu Neg Unm Pos Neu Neg Unm
F 65.20 15.00 18.80 1.00 66.60 13.70 18.30 1.40 66.01 12.23 20.12 1.64
B 22.20 4.20 8.20 65.40 23.50 3.60 7.20 65.70 21.50 3.15 6.29 69.07
P 33.40 13.00 15.60 38.00 35.60 10.70 15.80 37.90 36.64 10.24 13.97 39.15
D1
H 14.80 1.20 6.00 78.00 17.10 1.00 5.50 76.40 16.12 0.82 5.58 77.48
SA 48.80 3.60 14.00 33.60 47.90 4.10 13.60 34.40 42.73 3.46 13.87 39.94
PC 4.40 0.60 1.40 93.60 4.80 0.30 1.90 93.00 3.93 0.34 1.56 94.18
TC 52.40 13.20 7.60 26.80 53.10 13.20 8.10 25.60 48.56 12.84 7.03 31.57
Q 18.80 8.20 11.20 61.80 17.90 10.10 11.00 61.00 14.67 10.00 10.44 64.89
D2 SS 16.80 3.60 8.20 71.40 15.70 3.80 8.90 71.60 14.86 3.15 8.58 73.41
D 46.00 8.20 4.20 41.60 48.50 8.10 4.30 39.10 43.10 7.68 5.28 43.93
N 1.00 1.40 2.80 94.80 1.40 1.30 3.40 93.90 2.10 1.08 3.36 93.46
Table2:Labeldistribution(%)invariousaspectsoftrainsetandtestset.D1containsannotationsfor6aspects—food
(F),beverage(B),price(P),hygiene(H),staffattitude(SA),andparkingconvenience(PC);D2containsannotations
for5differentaspects—trafficconvenience(TC),queuing(Q),servingspeed(SS),decoration(D),andnoise(N).
Weuse’Pos’,‘Neu’,’Neg’,‘Unm’torepresentPositive,Neutral,NegativeandUnmentionedlabels,respectively.
alsoreflectsthemodel’sabilitytounderstandthe aged results of training size 500 and 1000 of ID
context and the nuances of different instructions. andOODscenariosarevisualizedinFigure4.
Wesetupparsersaccordingtotheprescribedfor-
A.5 DetailedResultsonGENIA,MAVENand
matsofdifferentdesigns,thenwecalculatethera-
Review11
tioofpredictionsthatcannotbesuccessfullyparsed
withouroutputparser. Consideringtheinherently Table 9 shows the comparison of different
uncertaintynatureofgenerativelanguagemodels, sample design strategies on three downstream
werelaxedtheformatsuchastheexpressionofas- tasks—GENIA (Nested NER), MAVEN (Event
pectsandsentiments. Meanwhile,inordertocom- Detection),andReview11(MASA).Hardandsoft-
parethecontentcorrectnessbetweendesignsmore matching F1 scores are reported for GENIA and
fairly,forsomecasessuchascommonpunctuation MAVEN,whilekappaκandaccuracyarereported
errors, we will correct it into the required format forReview11. Fromtheresults,wecanseethatES-
whencalculatingtheKappa. Figure10showsava- SDEmaintainsitsadvantageoverothermethods,
rietyofrepresentativeformaterrortypesandhow acrossdifferenttasksandtrainingsizes.
theyareprocessedbytheparserswedesign. Table10illustratestheperformancesofdifferent
sampledesignstrategiesonthreedownstreamtasks
A.2 DatasetsandTrainingSettings
acrossdifferentinstructionvariations.
Table2showsthelabeldistributionofeachaspect
A.6 AdditionalAnalysisonInst-lastand
fortwodomainsD1andD2,wherewecanseethe
Inst-first
distributionsarehighlyunbalanced.
Thetrainingsetupwasasfollows: learningrate TheexperimentalresultsshowingthatInst-firstcon-
setto1e-4,batchsizeof4,LoRArankof8LoRA sistentlyoutperformsInst-lastacrossvarioustasks
alphaof32,LoRAdropoutof0.1. and models are thought-provoking, leading us to
conductamorein-depthanalysis. Weextractthe
A.3 SampleDesignExamples
attentionweightsrelatedtosometask-relatedfields
Figure9showsadetailedexampleofoursample in the instruction, and sum up these task-related
designsonMASAtasks. attentionweightsforeachtoken. Figure8shows
thecomparisonoftheattentionweightsforacer-
A.4 DetailedEvaluationsofEachSDEOption
taincustomerreview. Aswecansee,tokensthat
Thedetailedresultsofin-domain(ID)andout-of- areclosertotheinstructionusuallygethigher
domain(OOD)evaluationsontheMASAtaskof task-relatedattentionweights. Intuitively,when
differentSDEoptionsacrosssixLLMsareshown peoplewritereviews,theygenerallypresenttheir
inTable3toTable8,includingboththesentiment core opinions at the beginning. This leads to the
analysis performances (κ) and the format adher- possibilitythatiftheinstructionsareplacedatthe
ence performances (format error rate). An aver- front, those core parts may receive greater task-model:c-llama2-chat WeightedKappaκ #Wrongformat(7969testsamplesintotal)
train_size=500 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI .8091 .6882 .5243 .7217 0 0 2 2
Inst-first,_ .8136 .7079 .5124 .7223 0 0 9 15
Input
No-inst,_ .7757 .6626 \ \ 20 1 \ \
_,MI .6187 .6187 .4806 .2756 1 0 0 1079
Natural,TxtLabel,PU .8091 .6882 .5243 .7217 0 0 2 2
Lines,_,_ .8083 .6969 .5068 .7447 0 0 0 0
Output
JSON,_,_ .8086 .6952 .4905 .7354 0 0 0 0
_,NumLabel,_ .7697 .6373 .4221 .6723 3 1 0 1260
_,_,OU .7934 .6005 .5282 .6203 0 0 87 0
No-CoT .7934 .6005 .5282 .6203 0 0 87 0
Reasoning CoT .7928 .6873 .5249 .7085 56 65 36 282
R-CoT .8074 .6752 .4726 .7297 93 65 141 263
train_size=1000 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8256 0.7110 0.5518 0.7312 0 0 0 3
Inst-first,_ 0.8236 0.7090 0.5483 0.7264 0 0 5 1
Input
No-inst,_ 0.8003 0.6920 \ \ 6 4 \ \
_,MI 0.8113 0.6700 0.5095 0.5182 0 0 0 728
Natural,TxtLabel,PU 0.7916 0.7253 0.5303 0.7356 0 0 0 3
Lines,_,_ 0.8259 0.7118 0.5560 0.7452 0 0 0 0
Output JSON,_,_ 0.8249 0.7094 0.5488 0.7432 0 0 0 0
_,NumLabel,_ 0.7624 0.6604 0.4210 0.6840 2 2 0 765
_,_,OU 0.8172 0.7125 0.5511 0.6746 0 0 493 1
No-CoT 0.8018 0.7175 0.5332 0.7323 0 0 493 1
Reasoning CoT 0.8111 0.7111 0.5354 0.7311 59 24 30 253
R-CoT 0.8214 0.7137 0.5085 0.7532 51 25 75 115
Table3: MASAevaluationsofeachSDEoptionformodelc-llama2-chat. Thefirstmethodineachgroupisthe
groupbaseline. "_"meanskeepingthesameoptionwiththegroupbaseline.
model:c-llama2-base WeightedKappaκ #Wrongformat(7969testsamplesintotal)
train_size=500 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8067 0.6801 0.5246 0.7000 0 0 6 98
Inst-first,_ 0.8092 0.6921 0.5575 0.6794 0 0 34 3
Input
No-inst,_ 0.7762 0.6511 \ \ 0 1 \ \
_,MI 0.7778 0.5024 0.4946 0.4184 2 0 118 0
Natural,TxtLabel,PU 0.8067 0.6801 0.5246 0.7000 0 0 6 98
Lines,_,_ 0.8066 0.6410 0.5128 0.6622 0 0 19 0
Output JSON,_,_ 0.8010 0.6242 0.5170 0.6287 0 0 0 0
_,NumLabel,_ 0.7728 0.5949 0.5155 0.6296 14 1 26 356
_,_,OU 0.7746 0.5012 0.4199 0.5711 0 3 300 7
No-CoT 0.8010 0.6242 0.5170 0.6287 0 0 0 0
Reasoning CoT 0.7789 0.6652 0.4649 0.6974 83 82 33 226
R-CoT 0.8019 0.6428 0.4657 0.4199 88 11 87 1823
train_size=1000 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8237 0.7011 0.6010 0.7197 0 0 3 177
Inst-first,_ 0.8231 0.7068 0.6069 0.6956 0 2 16 28
Input
No-inst,_ 0.7957 0.6882 \ \ 2 2 \ \
_,MI 0.8048 0.6174 0.5306 0.6390 0 3 139 6
Natural,TxtLabel,PU 0.8237 0.7011 0.6010 0.7197 0 0 3 177
Lines,_,_ 0.8205 0.6947 0.5900 0.6963 0 0 10 0
Output JSON,_,_ 0.8212 0.6857 0.5649 0.6875 0 0 0 0
_,NumLabel,_ 0.7619 0.6536 0.4804 0.6709 1 2 0 584
_,_,OU 0.8179 0.6774 0.5034 0.6277 0 5 64 29
No-CoT 0.8212 0.6857 0.5649 0.6875 0 0 0 0
Reasoning CoT 0.8026 0.6979 0.5519 0.7159 70 31 16 125
R-CoT 0.8195 0.7034 0.5368 0.6454 46 14 24 666
Table4: MASAevaluationsofeachSDEoptionformodelc-llama2-base. Definitionof"_"seeTable3.model:intern-chat WeightedKappaκ #Wrongformat(7969testsamplesintotal)
train_size=500 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.7774 0.6278 0.3947 0.6707 0 0 0 11
Inst-first,_ 0.8035 0.6609 0.3949 0.7090 4 2 13 304
Input
T2L 0.7862 0.5963 \ \ 10 7 \ \
_,MI 0.7463 0.5178 0.3153 0.5363 0 0 0 395
Natural,TxtLabel,PU 0.7774 0.6278 0.3947 0.6707 0 0 0 11
Lines,_,_ 0.7827 0.6261 0.4032 0.6799 0 1 1 1
Output JSON,_,_ 0.7713 0.5966 0.3965 0.6129 0 0 0 2
_,NumLabel,_ 0.7765 0.6261 0.4165 0.6926 0 0 3 23
_,_,OU 0.7520 0.4888 0.4029 0.6221 0 1 16 7
No-CoT 0.7713 0.5966 0.3965 0.6129 0 0 0 2
Reasoning CoT 0.7666 0.6401 0.4843 0.6797 43 19 30 121
R-CoT 0.7764 0.6124 0.3892 0.6648 44 23 23 72
train_size=1000 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8049 0.6793 0.4330 0.6982 0 0 0 0
Inst-first,_ 0.8173 0.7125 0.4640 0.7343 0 1 6 259
Input
No-inst,_ 0.8139 0.6811 \ \ 8 5 \ \
_,MI 0.7819 0.6256 0.3332 0.6520 1 0 8 29
Natural,TxtLabel,PU 0.8049 0.6793 0.4330 0.6982 0 0 0 0
Lines,_,_ 0.8060 0.6797 0.4498 0.7038 0 1 0 1
Output JSON,_,_ 0.8021 0.6649 0.4661 0.6647 0 0 0 0
_,NumLabel,_ 0.8081 0.6764 0.4393 0.7286 0 0 3 3
_,_,OU 0.8008 0.6369 0.4374 0.6694 0 0 33 1
No-CoT 0.8021 0.6649 0.4661 0.6647 0 0 0 0
Reasoning CoT 0.7981 0.6966 0.5190 0.7098 36 7 10 132
R-CoT 0.8043 0.6709 0.3994 0.7195 50 4 19 42
Table5: MASAevaluationsofeachSDEoptionformodelintern-chat. Definitionof"_"seeTable3.
model:intern-base WeightedKappaκ #Wrongformat(7969testsamplesintotal)
train_size=500 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.7849 0.6465 0.4898 0.6129 0 1 1 0
Inst-first,_ 0.7955 0.6472 0.4947 0.7006 3 8 18 221
Input
No-inst,_ 0.7936 0.6119 \ \ 11 6 \ \
_,MI 0.7562 0.5029 0.3305 0.4672 0 1 232 447
Natural,TxtLabel,PU 0.7849 0.6465 0.4898 0.6129 0 1 1 0
Lines,_,_ 0.7873 0.6455 0.4939 0.6365 0 2 4 0
Output JSON,_,_ 0.7859 0.6250 0.4727 0.6127 0 0 3 82
_,NumLabel,_ 0.7605 0.6003 0.3861 0.6412 14 3 10 102
_,_,OU 0.7275 0.5185 0.3943 0.4935 0 4 48 6
No-CoT 0.7859 0.6250 0.4727 0.6127 0 0 3 82
Reasoning CoT 0.7621 0.6489 0.4581 0.6388 77 12 2347 50
R-CoT 0.7734 0.6342 0.3752 0.6816 141 49 1496 206
train_size=1000 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8112 0.6874 0.5216 0.7065 1 0 0 0
Inst-first,_ 0.8167 0.6965 0.5195 0.7544 0 0 5 46
Input
No-inst,_ 0.8191 0.6963 \ \ 5 8 \ \
_,MI 0.7937 0.6238 0.2780 0.6492 0 2 383 45
Natural,TxtLabel,PU 0.8112 0.6874 0.5216 0.7065 1 0 0 0
Lines,_,_ 0.8113 0.6919 0.5060 0.7126 0 0 3 0
Output JSON,_,_ 0.8076 0.6781 0.5195 0.6817 0 0 3 1
_,NumLabel,_ 0.8084 0.6776 0.4426 0.7139 3 1 31 20
_,_,OU 0.8006 0.6330 0.4587 0.6098 0 1 30 3
No-CoT 0.8076 0.6781 0.5195 0.6817 0 0 3 1
Reasoning CoT 0.7956 0.6874 0.5196 0.6903 34 12 405 56
R-CoT 0.8069 0.6725 0.4890 0.7185 46 11 220 125
Table6: MASAevaluationsofeachSDEoptionformodelintern-base. Definitionof"_"seeTable3.model:bc2-chat WeightedKappaκ #Wrongformat(7969testsamplesintotal)
train_size=500 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.7904 0.6544 0.4067 0.6170 8 0 21 10
Inst-first,_ 0.7958 0.6660 0.3858 0.6739 19 36 12 385
Input
No-inst,_ 0.7176 0.4776 \ \ 23 13 \ \
_,MI 0.7645 0.5636 0.3713 0.5490 0 0 5 16
Natural,TxtLabel,PU 0.7904 0.6544 0.4067 0.6170 8 0 21 10
Lines,_,_ 0.7869 0.6653 0.4091 0.6344 0 0 9 1
Output JSON,_,_ 0.7927 0.6489 0.4714 0.6196 0 0 1 0
_,NumLabel,_ 0.7839 0.6401 0.3671 0.6506 5 4 12 17
_,_,OU 0.7016 0.5670 0.3599 0.3285 2 81 50 19
No-CoT 0.7927 0.6489 0.4714 0.6196 0 0 1 0
Reasoning CoT 0.7722 0.6400 0.5006 0.6776 3641 757 739 3323
R-CoT 0.7922 0.6535 0.4534 0.6579 107 126 280 563
train_size=1000 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8113 0.7060 0.4709 0.6365 0 4 13 18
Inst-first,_ 0.8142 0.7095 0.4733 0.6787 31 12 21 136
Input
No-inst,_ 0.7466 0.6172 \ \ 6 6 \ \
_,MI 0.7935 0.6514 0.3951 0.5885 0 0 7 3
Natural,TxtLabel,PU 0.8113 0.7060 0.4709 0.6365 0 4 13 18
Lines,_,_ 0.8103 0.7057 0.4691 0.6387 0 0 3 0
Output JSON,_,_ 0.8118 0.7064 0.5237 0.6323 0 0 1 0
_,NumLabel,_ 0.8121 0.6962 0.4042 0.6697 10 17 4 15
_,_,OU 0.8061 0.6467 0.4843 0.5155 1 25 44 4
No-CoT 0.8118 0.7064 0.5237 0.6323 0 0 1 0
Reasoning CoT 0.7995 0.7026 0.4992 0.6975 2273 193 560 2043
R-CoT 0.8087 0.6961 0.5022 0.6772 57 48 85 167
Table7: MASAevaluationsofeachSDEoptionformodelbc2-chat. Definitionof"_"seeTable3.
model:bc2-base WeightedKappaκ #Wrongformat(7969testsamplesintotal)
train_size=500 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8017 0.6412 0.4441 0.6146 0 0 75 0
Inst-first,_ 0.8016 0.6649 0.4488 0.6657 0 6 27 4
Input
No-inst,_ 0.7533 0.6020 \ \ 2 3 \ \
_,MI 0.7660 0.4999 0.3220 0.1978 0 0 1 164
Natural,TxtLabel,PU 0.8017 0.6412 0.4441 0.6146 0 0 75 0
Lines,_,_ 0.7996 0.6317 0.4583 0.6191 0 0 2 0
Output JSON,_,_ 0.8008 0.6476 0.4316 0.6104 0 0 0 0
_,NumLabel,_ 0.7969 0.5794 0.4312 0.5206 7 45 469 47
_,_,OU 0.7595 0.5202 0.4240 0.4944 0 0 116 2
No-CoT 0.7595 0.5202 0.4240 0.4944 0 0 116 2
Reasoning CoT 0.7865 0.6814 0.3854 0.6745 63 17 43 483
R-CoT 0.7980 0.6548 0.4240 0.6349 32 44 39 32
train_size=1000 D1→D1 D2→D2 D1→D2 D2→D1 D1→D1 D2→D2 D1→D2 D2→D1
Inst-last,No-MI 0.8143 0.6981 0.4747 0.6767 0 0 26 4
Inst-first,_ 0.8155 0.7157 0.5061 0.6974 0 3 26 4
Input
No-inst,_ 0.7543 0.6391 \ \ 0 3 \ \
_,MI 0.8010 0.6489 0.4164 0.5250 0 0 1 431
Natural,TxtLabel,PU 0.8143 0.6981 0.4747 0.6767 0 0 26 4
Lines,_,_ 0.8103 0.7003 0.4732 0.6713 0 0 6 1
Output JSON,_,_ 0.8120 0.7039 0.4785 0.6819 0 0 0 0
_,NumLabel,_ 0.8119 0.6812 0.4575 0.6467 1 5 292 8
_,_,OU 0.7894 0.6484 0.4031 0.6235 0 1 31 0
No-CoT 0.7894 0.6484 0.4031 0.6235 0 1 31 0
Reasoning CoT 0.8045 0.7063 0.5319 0.6965 21 12 25 494
R-CoT 0.8160 0.7021 0.4604 0.6949 15 14 24 115
Table8: MASAevaluationsofeachSDEoptionformodelbc2-base. Definitionof"_"seeTable3.GENIA(Nested-NER) MAVEN(ED) Review11(MASA)
trainingsize Strategies F1-hard F1-soft F1-hard F1-soft κ Acc
heuristic 0.51232 0.57465 0.5197 0.5356 0.588 0.7586
500 EW-SDE 0.48328 0.54318 0.4922 0.5364 0.7235 0.8327
ES-SDE 0.54068 0.61412 0.5846 0.6331 0.7691 0.8626
heuristic 0.56537 0.62275 0.6237 0.6354 0.7058 0.8262
1,000 EW-SDE 0.48785 0.55166 0.6109 0.6275 0.7565 0.8502
ES-SDE 0.61593 0.68951 0.6432 0.6726 0.7892 0.8716
heuristic 0.64759 0.69905 0.6722 0.6813 0.7479 0.8483
2,000 EW-SDE 0.54351 0.6025 0.6966 0.7106 0.7805 0.8649
ES-SDE 0.68069 0.7393 0.7033 0.7172 0.8023 0.8785
heuristic 0.68726 0.73825 0.7118 0.7176 0.7751 0.8644
4,000 EW-SDE 0.71109 0.77093 0.7265 0.7338 0.7917 0.8715
ES-SDE 0.72726 0.78487 0.7295 0.7466 0.805 0.8814
Table 9: Comparison of different sample design strategies on three downstream tasks. ES-SDE maintains its
advantageoverothermethods,acrossdifferenttasksandtrainingsizes.
GENIA(Nested-NER) MAVEN(ED) Review11(MASA)
InstructionVariation Strategies F1-hard F1-soft F1-hard F1-soft κ Acc
heuristic 0.5123 0.5747 0.5197 0.5356 0.588 0.7586
inst-1 EW-SDE 0.4833 0.5432 0.4922 0.5364 0.7235 0.8327
ES-SDE 0.5407 0.6141 0.5846 0.6331 0.7691 0.8626
heuristic 0.49813 0.56095 0.5134 0.5334 0.6009 0.7685
inst-2 EW-SDE 0.48593 0.54999 0.4956 0.5339 0.7208 0.8344
ES-SDE 0.53479 0.60767 0.5636 0.6167 0.7659 0.8615
heuristic 0.48733 0.55491 0.4940 0.5060 0.5793 0.7533
inst-3 EW-SDE 0.47638 0.53685 0.4925 0.5399 0.721 0.8365
ES-SDE 0.53525 0.60902 0.5530 0.6087 0.7624 0.8601
Table10: Performancesofdifferentsampledesignstrategiesonthreedownstreamtasksacrossdifferentinstruction
variations.relatedattentionweights. Thismaypartlyexplain A.8.1 Zero-shotandIn-contextLearning
why Inst-first usually leads to a higher sentiment Analysis
analysisperformance. Zero-shot and In-context learning ability can di-
rectlyrevealLLMs’familiaritywiththegiventask.
A.7 AdditionalAnalysisonOUandPU
Inthezero-shotapproach,weusetheinput(which
In previous experiments, we found that OU per-
contains the instruction on output format) from
formsmuchworsethanPU.Thisintriguingresult
each SDE option as the prompt for the original
motivatesustoafurtheranalysis. Specifically,we
frozenLLMsprediction. FortheICLapproach,we
calculateandcomparethekappascoresofOU and
addtwofixedexamplesfromthetrainingsetbefore
PU foreachaspect,toanalyzetherelationshipbe-
eachtestinstance. Consideringtheinferencetime
tweenlabeldistributionsandtheeffectofOU.
cost caused by the increase in sample length, we
FromtheresultinTable11,wecanobservethat
limit our prediction and analysis to 500 samples.
when training the model with 500 samples, for
Allotherexperimentalsetupsremainalignedwith
aspectswithahighernumberofunmentioned,the
thosedescribedinExperimentsI.
OU methodshowedasignificantgapcomparedto
Zero-shotStudy. Allsix7BLLMsusedinSec-
thePU format. Whenthetrainingsetincreasedto
tion4exhibitpoorzero-shotMASAability,failing
1000samples,thisgapnoticeablynarrowed. This
tofollowtheinstructionstogenerateproperoutput
suggeststhatfortheOUmethod,aspectswithmore
inmostcases,asshowninTable13,makingithard
unmentioned, implying less frequent occurrence
toanalysisitsrelationshipwithSDEresults. Vari-
in answers, are harder for the model to learn, so
ationsinformatpreferencesacrossdifferentmod-
requiringmoredata. Fromanotherperspective,it
elsareobserved,whichweconjectureisstrongly
also indicates that even if a certain aspect is not
related to the datasets employed for instruction
covered in the text, mentioning this aspect in the
fine-tuningineachmodel. Somepatternsarealso
answerscanenhancethemodel’sunderstandingof
contradictorybetweenzero-shotandSDE.Forex-
it.
ample,theOU SDEoptionconsistentlyharmsDT
performances, however, its prompts result in no-
Trainsize=500 Trainsize=1000
Aspect
(%)Num_ ∆κ (%)Num_ ∆κ tablyfewerformaterrorsinzero-shotinference,for
Unmen Avg_ChatAvg_Base Unmen Avg_ChatAvg_Base certainLLMs. Therefore,zero-shotperformances
D1F 1.00 -.0004 .0007 1.40 -.0026 -.0011
canhardlytellgoodorbadSDEoptions.
SA 33.60 -.0687 -.0555 34.40 -.0062 -.0212
In-contextLearningStudy. ICLcaneffectively
P 38.00 -.0469 -.0495 37.90 -.0068 -.0255
B 65.40 -.0410 -.0291 65.70 -.0117 -.0079 improveLLMs’instruction-followingabilitiesre-
H 78.00 -.0920 -.1367 76.40 -.0033 -.0207 sultinginfarfewerformattingerrorsthanzero-shot.
PC 93.60 -.2338 -.2590 93.00 -.0181 -.0305
Thereforewereporttheaveragesentimentanalysis
D2TC 26.80 -.0891 -.1341 25.60 -.0497 -.0492
D 41.60 -.1106 -.2475 39.10 -.0280 -.0500 performancesofeachmodelontwodomainsinTa-
Q 61.80 -.0329 -.0588 61.00 -.0361 -.0149 ble14. TheresultssuggestthatInst-firstandCoT
SS 71.40 -.2537 -.2575 71.60 -.0574 -.0896
enhance the performance of most models, which
N 94.80 -.3347 -.3954 93.90 -.0494 -.1405
providesvaluableinsightsforformatselectiondur-
Table11: Numberof‘Unmentioned’labelsandaverage ing the fine-tuning process. For output designs,
∆κ(κ OU-κ PU)fordifferentaspects. JSON and OU options outperform the other ap-
proachesforsomemodels,differingfromtheSDE
results.
A.8 CanPEGuideSDE?DetailedResults
Evaluating the performances of sample designs A.8.2 PerplexityAnalysis
involvesfine-tuningmodelsondownstreamtasks, Perplexitymeasurestheuncertaintyofthemodel
whichcanbetime-consuming. Therefore,wealso in generating a given text sequence (Chen et al.,
pondered whether it might be possible to design 1998),withlowerperplexityvaluesindicatingmore
better samples without training models first. We confidentpredictionsbythemodel. Incalculations,
tried to understand the inherent capabilities and weestimateperplexityusingthecommonpractice
potential of the model by experimenting with oftakingthelogarithmofthemodel’sloss.
differentpromptdesignsinboththezero-shotand In our task, we compare the PPL scores of the
in-contextlearningscenarios. ICLpromptscorrespondingtoeachdifferentSDE
option,aswellastheconditionalPPLofthemod-Figure8: Comparisonoftask-relatedattentionscoresusingInst-lastandInst-first.
els’ICLpredictions. Forpredictions,weconcate-
nate the prompt and the prediction together as a
sequence,thenconsiderthepromptasitscontext.
The perplexity results for different designs are
shown in Table 12. For input designs, the PPL
scoreofInst-firstoptionislowerthanthatofInst-
lastingeneral,whichisconsistentwiththeconclu-
sionthatInst-firstperformsbetterinICLandSDE
experiments. For output designs, the OU option
getsthehighestscore, whichisinconsistentwith
itsperformanceontheICL,butisconsistentwith
itsbeingtheworstoptionintheSDEexperiment.
Surprisingly,theJSON formatachievedthesignifi-
cantlylowestpplscore,butitwasonparwiththe
LinesformatinICLandevenworsethanLinesin
SDE.Themostinterestingresultappearsintherea-
soningdesigns. TheCoT andR-CoT optionshave
lowPPLscoresonpromptsbuthavehighscoreson
predictionsconversely. Suchcontradictionsmake
it difficult to analyze the results of ICL or SDE
throughPPLscores.
Theanalysisabovealsohighlightstheindispens-
abilityofourSDEexperiments,causewecannot
predeterminethefinaleffectivenessofdifferentde-
signsthroughpreliminaryanalysisalone.Perplexity:Prompts c-llama2-chat c-llama2-base intern-chat intern-base bc2-chat bc2-base
Inst-last,No-MI 47.662 111.063 18.422 19.036 59.046 42.030
Input
Inst-first,_ 46.357 110.065 19.561 18.632 54.795 39.003
Natural,TxtLabel,PU 47.662 111.063 18.422 19.036 59.046 42.030
Lines,_,_ 47.918 191.274 18.561 19.219 60.498 42.638
Output JSON,_,_ 29.008 78.848 14.675 13.260 38.547 25.405
_,NumLabel,_ 41.690 92.717 17.664 16.348 51.963 35.185
_,_,OU 55.345 129.055 20.862 21.450 69.022 49.426
No-CoT 29.008 78.848 14.675 13.260 38.547 25.405
Reasoning CoT 18.263 41.312 10.812 9.379 23.406 15.267
R-CoT 18.210 42.648 10.789 9.354 22.671 15.333
Perplexity:Predictions c-llama2-chat c-llama2-base intern-chat intern-base bc2-chat bc2-base
Inst-last,No-MI 1.052 1.109 1.051 1.394 1.061 1.127
Input
Inst-first,_ 1.088 1.284 1.046 1.360 1.066 1.113
Natural,TxtLabel,PU 1.052 1.109 1.051 1.394 1.061 1.127
Lines,_,_ 1.052 1.137 1.058 1.386 1.222 1.136
Output JSON,_,_ 1.038 1.074 1.045 1.407 1.019 1.042
_,NumLabel,_ 1.096 1.142 1.078 1.403 1.088 1.102
_,_,OU 1.183 1.368 1.089 1.279 1.353 1.823
No-CoT 1.038 1.074 1.045 1.407 1.019 1.042
Reasoning CoT 1.234 1.475 1.084 1.186 1.090 1.129
R-CoT 1.239 1.293 1.069 1.185 1.063 1.090
Table12: ThePPLscoresontheICLpromptsandpredictionscorrespondingtoeachSDEoptionsontheMASAID
tasks.
c-llama2-chat Intern-chat bc2-chat c-llama2-base Intern-base bc2-base
D1 D2 D1 D2 D1 D2 D1 D2 D1 D2 D1 D2
Ins-last 74.24 31.67 85.82 11.75 40.67 22.12 88.92 36.60 94.89 81.60 100 98.18
Input
Ins-first 70.05 44.82 98.76 99.61 59.56 24.18 88.62 27.49 89.79 75.59 99.66 96.26
Natural,TxtLabel,PU 74.24 31.67 85.82 11.75 40.67 22.12 88.92 36.60 94.89 81.60 100 98.18
Lines,_,_ 1.18 1.31 99.94 97.06 4.17 1.57 72.51 12.10 99.57 99.79 99.99 99.94
Output JSON,_,_ 5.94 16.49 100 100 96.15 73.53 99.94 100 100 100 100 100
_,Numerical,_ 99.87 92.21 99.99 100 100 100 100 100 100 100 100 100
_,_,OU 45.75 18.31 70.21 31.38 44.15 50.93 72.79 87.99 76.80 56.87 99.74 95.33
No-CoT 5.94 16.49 100 100 96.15 73.53 99.94 100 100 100 100 100
Reasoning
CoT 35.25 34.25 100 100 58.66 53.29 100 100 100 100 99.99 99.99
R-CoT 33.84 75.87 100 100 80.71 77.12 98.24 90.58 100 100 100 100
Table13: Formaterrorrate(%)inzero-shotscenario
test_size=500 c-llama2-chat c-llama2-base intern-chat intern-base bc2-chat bc2-base
Inst-last 0.3834 0.2835 0.1856 0.1212 0.4402 0.4187
Input
Inst-first 0.4832 0.2959 0.2038 0.2044 0.5091 0.4345
Natural,TxtLabel,PU 0.3834 0.2835 0.1856 0.1212 0.4402 0.4187
Lines,_,_ 0.4220 0.2921 0.2436 0.1846 0.3971 0.4077
Output JSON,_,_ 0.3773 0.2132 0.3390 0.2954 0.4614 0.3683
_,NumLabel,_ 0.1522 0.1666 0.2470 0.2603 0.2406 0.1960
_,_,OU 0.3612 0.3168 0.2461 0.1443 0.1948 0.1924
No-CoT 0.3773 0.2132 0.3390 0.2954 0.4614 0.3683
Reasoning CoT 0.3383 0.2174 0.3636 0.3167 0.4810 0.4466
R-CoT 0.3638 0.2445 0.3522 0.2633 0.4668 0.4075
Table14: TheaverageweightedKappaκontheMASAIDtasksinin-contextlearningscenarioI`nst-last,No-MI / Natural, TxtLabel, PU <review>\n---\n Read the above comment and observe the following aspects:
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。 [aspect]. Based on the comment, please conduct sentiment analysis on these aspects
请根据评论对这些方面进行情感分析，具体有四类情感：正面、负 with four specific categories: positive, negative, neutral, and unmentioned. Please
面、中性、未提及。请用以下格式给出所有方面的情感："方面1： provide the sentiment for all aspects in the following format: "Aspect 1:
情感类别，方面2：情感类别，..."\n输出： Sentiment category, Aspect 2: Sentiment category, ..."\n Output：
O：方面1：情感类别，方面2：情感类别，... Aspect 1: Sentiment category, Aspect 2: Sentiment category, ...
Inst-first, _ Read the comment below and observe the following aspects: [aspect]. Based on the
I：阅读下面这段评论，观察以下这些方面：[aspect]。请根据评论对 comment, please conduct sentiment analysis on these aspects with four specific
这些方面进行情感分析，具体有四类情感：正面、负面、中性、 categories: positive, negative, neutral, and unmentioned. Please provide the
未提及。请用以下格式给出所有方面的情感："方面1：情感类 sentiment for all aspects in the following format: "Aspect 1: Sentiment category,
别，方面2：情感类别，..."\n---\n评论：<review>\n输出： Aspect 2: Sentiment category, ..."\n---\n Review: <review>\n Output：
O：方面1：情感类别，方面2：情感类别，... Aspect 1: Sentiment category, Aspect 2: Sentiment category, ...
No-Inst, _
I：<review>\n输出： <review>\n Output：
O：方面1：情感类别，方面2：情感类别，... Aspect 1: Sentiment category, Aspect 2: Sentiment category, ...
Lines, _, _ <review>\n---\n Read the above comment and observe the following aspects:
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。 [aspect]. Based on the comment, please conduct sentiment analysis on these aspects
请根据评论对这些方面进行情感分析，具体有四类情感：正面、负 with four specific categories: positive, negative, neutral, and unmentioned. Please
面、中性、未提及。请用以下格式给出所有方面的情感："方面1： provide the sentiment for all aspects in the following format: "Aspect 1:
情感类别\n方面2：情感类别\n..."\n输出： Sentiment category\n Aspect 2: Sentiment category\n ..."\n Output：
O：方面1：情感类别， Aspect 1: Sentiment category，
方面2：情感类别， Aspect 2: Sentiment category，
... ...
JSON, _, _ / No-CoT <review>\n---\n Read the above comment and observe the following aspects:
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。 [aspect]. Based on the comment, please conduct sentiment analysis on these aspects
请根据评论对这些方面进行情感分析，具体有四类情感：正面、负 with four specific categories: positive, negative, neutral, and unmentioned. Please
面、中性、未提及。请用以下格式给出所有方面的情感："{"方面": provide the sentiment for all aspects in the following format: "{"Aspect ": Aspect
方面1, "情感":情感类别}\n{"方面":方面2, "情感":情感类别}\n..."\n 1, "Sentiment": Sentiment category}\n{"Aspect": Aspect 2,
输出： "Sentiment": Sentiment category}\n ..."\n Output：
O：{"方面": ..., "情感": ...} {"Aspect 1": ..., "Sentiment category": ...}
{"方面": ..., "情感": ...} {"Aspect 2": ..., "Sentiment category": ...}
... ...
_, NumLabel, _ <review>\n---\n Read the above comment and observe the following aspects:[aspect].
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。 Based on the review, please make a sentiment analysis on these aspects with four
请根据评论对这些方面进行情感分析，具体有四类情感：正面(1)、 specific categories: positive(1), negative(0), neutral(-1), and unmentioned(-2). Please
负面(-1)、中性(0)、未提及(-2)。请用以下格式给出所有方面的情 provide the sentiment for all aspects in the following format: "Aspect 1: Sentiment
感："方面1：情感类别，方面2：情感类别，..."\n输出： category, Aspect 2: Sentiment category, ..."\n Output：
O：方面1：0，方面2：1，... Aspect 1: 0, Aspect 2: 1, ...
_, _, OU <review>\n---\n Read the above review and observe the following aspects:[aspect].
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。 Please make a sentiment analysis of the aspects mentioned in the review with three
请对评论中提及的方面进行情感分析，具体有三类情感：正面、负 specific categories: positive, negative, and neutral. Please provide the sentiment of
面、中性。请用以下格式给出提及的方面的情感："方面1：情感类 the mentioned aspects in the following format: "Aspect 1: Sentiment category,
别，方面2：情感类别，..."，未提及的方面不用给出。\n输出： Aspect 2: Sentiment category, ...", and the aspects not mentioned need not be
given.\n Output：
O：方面1：情感类别，方面2：情感类别，... Aspect 1: Sentiment category, Aspect 2: Sentiment category, ...
CoT <review>\n---\n Read the above review and observe the following aspects:[aspect].
Please extract or summarize the descriptions of these aspects in the original text
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。
and make a sentiment analysis with four specific categories: positive, negative,
请提取或总结原文中对这些方面的描述，并进行情感分析，具体有
四类情感：正面、负面、中性、未提及。请用以下格式给出所有方 neutral, and unmentioned. Please provide the sentiment for all aspects in the
面的结果：{"方面":方面1, "描述":描述, "情感":情感类别}\n{"方 following format: "{"Aspect ": Aspect 1, "Description ": Description,
面":方面2, "描述":描述, "情感":情感类别}\n..."\n输出： "Sentiment": Sentiment category}\n{"Aspect": Aspect 2, "Description
": Description, "Sentiment": Sentiment category}\n ..."\n Output：
O：{"方面":..., "描述":..., "情感":...}
{"Aspect 1": ..., "Description ": ..., "Sentiment category": ...}
{"方面":..., "描述":..., "情感":...}
{"Aspect 2": ..., "Description ": ..., "Sentiment category": ...}
...
...
R-CoT <review>\n---\n Read the above review and observe the following aspects:
[aspect]. Please extract or summarize the descriptions of these aspects in the
I：<review>\n---\n阅读上面这段评论，观察以下这些方面：[aspect]。 original text and make a sentiment analysis with four specific categories: positive,
请提取或总结原文中对这些方面的描述，并进行情感分析，具体有 negative, neutral, and unmentioned. Please provide the sentiment for all aspects in
四类情感：正面、负面、中性、未提及。请用以下格式给出所有方 the following format: "{"Aspect ": Aspect 1, "Sentiment": Sentiment
面的结果：{"方面":方面1, "情感":情感类别, "描述":描述}\n{"方
category, "Description ": Description}\n{"Aspect": Aspect 2, "Sentiment":
面":方面2, "情感":情感类别, "描述":描述}\n..."\n输出： Sentiment category, "Description ": Description,}\n ..."\n Output：
O：{"方面":..., "情感":..., "描述":...} {"Aspect 1": ..., "Sentiment category": ..., "Description ": ...}
{"方面":..., "情感":..., "描述":...} {"Aspect 2": ..., "Sentiment category": ..., "Description ": ...}
... ...
Figure9: ExamplesofdifferentsampledesignsontheMASAtask.Count as
Error Processed
Output Format
Type Output
Error
交通情况：未提及，排队等候情况：负面，点菜上菜速度： {"交通便利程度": "正面", "排队等候情况": 负面", "点菜上菜速度": "负 NO
Aspect 负面，装修情况：正面，嘈杂情况：未提及。 面", "装修情况": "未提及", "嘈杂情况": "未提及"}
Expression traffic situation: positive, queuing: negative, serving {"traffic convenience": "positive", "queuing": "negative",
speed: negative, decoration: unmentioned, noise: "serving speed": "negative", "decoration": "unmentioned", "noise":
unmentioned. "unmentioned"}
食品评价：负面\n饮品评价：未提及\n价格水平：负面\n卫生 {"食品评价": "负面", "饮品评价": "未提及", "价格水平": "负面", "卫生 NO
Extra 情况：未提及\n服务人员态度：负面\n停车方便程度：未提及 情况: 未提及", "服务人员态度": "负面", "停车方便程度": "未提及"}
Aspect \n空调：负面
food: negative\n beverage: unmentioned\n price: negative {"food": "unmentioned", "beverage": "unmentioned", "price":
\n hygiene: unmentioned\n staff attitude: negative\n "negative", "hygiene: unmentioned", "staff attitude": "negative",
parking convenience: unmentioned\n air conditioner: "parking convenience": "unmentioned"}
negative
食品评价：负面，饮品评价：未提及，价格水平：负面，服务 {"食品评价": "未提及", "饮品评价": "未提及", "价格水平": "负面", "卫
NO
Lack of 人员态度：未提及，停车方便程度：未提及。 生情况: 未提及", "服务人员态度": "负面", "停车方便程度": "未提及"}
Aspect food: unmentioned, beverage: unmentioned, price: {"food": "unmentioned", "beverage": "unmentioned", "price":
negative, staff attitude: negative, parking convenience: "negative", "hygiene: unmentioned", "staff attitude": "negative",
unmentioned. "parking convenience": "unmentioned"}
食品评价：正面，饮品评价：负面，价格水平：中性，卫生情 {"食品评价": "正面", "饮品评价": "负面", "价格水平": "中性", "卫生
YES
Redundant 况：未提及，服务人员态度：正面（虽然服务水平不算专业但 情况: 未提及", "服务人员态度": "正面", "停车方便程度": "未提及"}
Describe 看上去挺善良的爱笑的小姑娘），停车方便程度：未提及。
food: positive, beverage: negative, price: neural, staff {"food": "positive", "beverage": "negative", "price": "neural",
attitude: unmentioned (The service level is not "hygiene: unmentioned", "staff attitude": "positive", "parking
professional but seems to be a kind little girl who convenience": "unmentioned"}
loves to laugh), parking convenience: unmentioned.
交通非常便利，离地铁青年路站很近，点菜上菜速度较快，装 {"交通便利程度": "未提及", "排队等候情况": "未提及", "点菜上菜速度":
YES
Unformatted 修情况良好，嘈杂情况一般。 "未提及", "装修情况": "未提及", "嘈杂情况": "未提及"}
The transportation is very convenient, it is very close to {"traffic convenience": "unmentioned", "queuing": "unmentioned",
the subway Qingnian Road station, the ordering speed is "serving speed": "unmentioned", "decoration": "unmentioned",
fast, the decoration is in good condition, and the noisy "noise": "unmentioned"}
situation is general.
食品评价：负面，饮品评价：未提及，价格水平：负面，卫生 {"食品评价": "负面", "饮品评价": "未提及", "价格水平": "负面", "卫生 YES
Extra 情况：未提及，服务人员态度：负面，未提及，停车方便程 情况": "未提及", "服务人员态度": "负面", "停车方便程度": "未提及"}
Sentiment 度：未提及。
food: unmentioned, beverage: unmentioned, price: {"food": "negative", "beverage": "unmentioned", "price":
negative, hygiene: unmentioned, staff attitude: negative, "negative", "hygiene: unmentioned", "staff attitude": "negative",
unmentioned, parking convenience: unmentioned. "parking convenience": "unmentioned"}
交通便利，正面，排队等候情况，未提及，点菜上菜速度， {"交通便利程度": "未提及", "排队等候情况": "未提及", "点菜上菜速度":
YES
Punctuation 正面，装修情况，中性，嘈杂情况，负面。 "未提及", "装修情况": "未提及", "嘈杂情况": "未提及"}
Error traffic convenience, positive, queuing, unmentioned, {"traffic convenience": "unmentioned", "queuing": "unmentioned",
serving speed, positive, decoration, neural, noise, "serving speed" : "unmentioned",
negative. "decoration": unmentioned", "noise": "unmentioned"}
食品评价：8，饮品评价：-2，价格水平：-2，卫生情况：-2，{"饮品评价": "未提及", "价格水平": "未提及", "卫生情况": "未提及", "停
Numbers YES
服务人员态度：10，停车方便程度：-2。 车方便程度": "未提及", "食品评价": "未提及", "服务人员态度": "未提及"}
out of
range food: 8, beverage: -2, price: -2, hygiene: -2, staff {"beverage": "unmentioned", "price": "unmentioned",
attitude: 10, parking convenience: -2. "hygiene: unmentioned", "parking convenience": "unmentioned",
"food": "unmentioned", "staff attitude": "negative"}
{"方面": "食品评价", "描述": "蔓越莓泡在水里种子也都挤漏 {"食品评价": "负面", "饮品评价": "负面", "价格水平": "未提及", "卫生
YES
出来", "情感": "负面"}\n{"方面": "饮品评价", "描述": "蔓越莓 情况": "未提及", "服务人员态度": "未提及", "停车方便程度": "未提及"}
Repeat 晶钻果茶，蔓越莓泡在水里种子也都挤漏出来，蔓越莓泡在
Segment 水里种子也都挤漏出来，蔓越莓泡在水里种子也都挤漏出
来", "情感": "负面"}\n{"方面": "价格水平", "描述": "蔓越莓晶
钻果茶，蔓越莓晶钻果茶，蔓越莓晶钻果茶，……蔓越莓晶
钻
{"aspect": "food", "describe": "Cranberries soak {"food": "negative", "beverage": "negative", "price":
in water and the seeds squeeze out", "sentiment": "unmentioned", "hygiene: unmentioned", "staff attitude":
"negative"}\n{"aspect": "beverage", "describe": "unmentioned", "parking convenience": "unmentioned"}
"Cranberry tea, cranberry soaked in water and the seeds
will leak out, cranberry soaked in water and the seeds will
leak out, cranberry soaked in water and the seeds will leak
out", "sentiment": "negative"}\n{"aspect": "price",
"describe": "Cranberry tea, cranberry tea, cranberry
tea,...cranberry tea
Figure10: Examplesofformaterrortypesandhowtheyareprocessed.Tasks Strategies Prompts Output_Formats
[INST]Read the given sentence carefully, identify all named entities of type
["DNA", "xxx"]
"DNA", "RNA", "protein", "cell_type" or "cell_line". Answer in the format
heuristic ["protein", "xxx"]
["entity_type", "entity_name"]. If no entity exists, then just answer "
["protein", "xxx"]
[]". Given sentence: <sentence> [/INST]
...
[INST]Given sentence: <sentence> Read the given sentence carefully,
GENIA identify all named entities of type "DNA", "RNA", "protein", "cell_type" or 'DNA': 'xxx', 'xxx', ... ; 'protein':
(Nested- EW-SDE "cell_line". For each entity type, answer in the format like "'entity_type': 'xxx', 'xxx'; 'cell_type': 'xxx'
NER) 'entity_name_1', 'entity_name_2'...", then concat answer for each type
with ';'. Only output entity types that contain entities.[/INST]
[INST]Read the given sentence carefully, identify all named entities of type 'DNA': 'xxx', 'xxx', ...
"DNA", "RNA", "protein", "cell_type" or "cell_line". For each entity type, 'RNA': ''
ES-SDE answer in a line in the format like "'entity_type': 'entity_name_1', 'protein': 'xxx', 'xxx'
'entity_name_2'..." (when no entities exist, answer "'entity_type': 'cell_type': 'xxx'
''").Given sentence: <sentence> [/INST] 'cell_line': ''
We define the event types set: Catastrophe, Attack, Hostile_encounter,
Causation, Process_start, Competition, Motion, Social_event, Killing, ["Motion", "xxx"]
heuristic Conquering. Given a sentence, please detect the type of events it contains and
["Conquering", "xxx"]
extract the trigger word from it. Please generate the result in the following
["Conquering", "xxx"]
format: "["event_type", "trigger_word"]\n..."If no event exists, just
answer[]. The sentence is: <sentence> Output: \n"
Given a sentence: <sentence> \n---\nWe define the event types set:
Catastrophe, Attack, Hostile_encounter, Causation, Process_start,
MAVEN
EW-SDE Competition, Motion, Social_event, Killing, Conquering. Please detect the type Motion: xxx; Conquering: xxx, xxx
(ED)
of events the given sentence contains and extract the trigger word from
it. Please generate the result in the following format: "event_type1:
trigger_word1, trigger_word2, ...; event_type2: trigger_word1,
trigger_word2, ...; ..." Output:\n
We define the event types set: Catastrophe, Attack, Hostile_encounter,
Catastrophe: NONE
Causation, Process_start, Competition, Motion, Social_event, Killing, Attack: NONE
Conquering. Given a sentence, please detect all the type of events in the
ES-SDE Hostile_encounter: NONE
predefined set from it. For the types this sentence contains, please extract
Causation: NONE
the trigger words from it, and for the types it does not contain, return
Process_start: NONE
the trigger words as NONE. Please generate the result in the following
Competition: NONE
format: "event_type1: trigger_word1, trigger_word2, ...\nevent_type2: Motion: xxx
trigger_word1, trigger_word2, ...\n..." The sentence is:
...
<sentence> Output: \n
Read the comment below and observe the following aspects: [aspect]. Based on
the comment, please conduct sentiment analysis on these aspects with three ["Aspect 1", "xxx"]
heuristic specific categories: positive, negative, and neutral. Please provide the ["Aspect 3", "xxx"]
sentiment of the mentioned aspects in the following format: "["Aspect 1", ...
"Sentiment category"]\n["Aspect 2", "Sentiment category"]\n ...", and the
aspects not mentioned need not be given.\n---\n Review: <review>\n Output：
<review>\n---\n Read the above review and observe the following aspects:
Review11 [aspect]. Please make a sentiment analysis of the aspects mentioned in the
(MASA) EW-SDE review with three specific categories: positive, negative, and neutral. Please Aspect 1: xxx, Aspect 3: xxx, ...
provide the sentiment of the mentioned aspects in the following format:
"Aspect 1: Sentiment category, Aspect 2: Sentiment category, ...", and
the aspects not mentioned need not be given.\n Output：
Read the comment below and observe the following aspects: [aspect]. Based Aspect 1: xxx
on the comment, please conduct sentiment analysis on these aspects with Aspect 2: unmentioned
ES-SDE four specific categories: positive, negative, neutral, and unmentioned. Please Aspect 3: xxx
provide the sentiment for all aspects in the following format: "Aspect 1: ...
Sentiment category\nAspect 2: Sentiment category\n..."\n---\n
Review: <review>\n Output：
Figure11: ExamplesofdifferentsampledesignsonGENIA,MAVENandReview11.Original Instruction:
heuristic We define the event types set: Catastrophe, Attack, Hostile_encounter, Causation, Process_start, Competition, Motion, Social_event,
Killing, Conquering. Given a sentence, please detect the type of events it contains and extract the trigger word from it. Please generate
the result in the following format: "["event_type", "trigger_word"]\n..."If no event exists, just answer[]. The sentence is:
<sentence> Output: \n"
Instruction Variation 1:
We have the following event types: Catastrophe, Attack, Hostile_encounter, Causation, Process_start, Competition, Motion,
Social_event, Killing, Conquering. For a sentence, please detect the type of events it contains and extract the trigger word from it. We
define the format of the result as: "["event_type", "trigger_word"]\n..."If no event exists, just answer[]. Here is the sentence:
<sentence> Output: \n
Instruction Variation 2:
In our event detection task, we specify a set of event types: Catastrophe, Attack, Hostile_encounter, Causation, Process_start,
Competition, Motion, Social_event, Killing, Conquering. Your goal is to analyze a given sentence and identify the types of events
included in the sentence from the predefined set. Extract the trigger words related to each included event types from the sentence.
Format the output as shown: "["event_type", "trigger_word"]\n...". If no event exists, just answer[]. Here is the sentence: <sentence>
Output: \n
Original Instruction:
EW-SDE Given a sentence: <sentence> \n---\nWe define the event types set: Catastrophe, Attack, Hostile_encounter, Causation, Process_start,
Competition, Motion, Social_event, Killing, Conquering. Please detect the type of events the given sentence contains and extract the
trigger word from it. Please generate the result in the following format: "event_type1: trigger_word1, trigger_word2, ...; event_type2:
trigger_word1, trigger_word2, ...; ..." Output:\n
Instruction Variation 1:
For a sentence: <sentence>\n---\nWe have the following event types: Catastrophe, Attack, Hostile_encounter, Causation,
Process_start, Competition, Motion, Social_event, Killing, Conquering. Please detect the type of events the given sentence contains and
extract the trigger word from it. We define the format of the result as: "event_type1: trigger_word1, trigger_word2, ...; event_type2:
trigger_word1, trigger_word2, ...; ..." Output: \n
Instruction Variation 2:
Here is a sentence: <sentence>\n---\nIn our event detection task, we specify a set of event types: Catastrophe, Attack,
Hostile_encounter, Causation, Process_start, Competition, Motion, Social_event, Killing, Conquering. Your goal is to analyze the given
sentence and identify the types of events included in the sentence from the predefined set. Extract the trigger words related to each
included event types from the sentence. Format the output as shown: "event_type1: trigger_word1, trigger_word2, ...; event_type2:
trigger_word1, trigger_word2, ...; ..." Output: \n
Original Instruction:
ES-SDE We define the event types set: Catastrophe, Attack, Hostile_encounter, Causation, Process_start, Competition, Motion, Social_event,
Killing, Conquering. Given a sentence, please detect all the type of events in the predefined set from it. For the types this sentence
contains, please extract the trigger words from it, and for the types it does not contain, return the trigger words as NONE. Please
generate the result in the following format: "event_type1: trigger_word1, trigger_word2, ...\nevent_type2: trigger_word1,
trigger_word2, ...\n..." The sentence is: <sentence> Output: \n
Instruction Variation 1:
We have the following event types: Catastrophe, Attack, Hostile_encounter, Causation, Process_start, Competition, Motion,
Social_event, Killing, Conquering. For a sentence, please detect all the type of events in the predefined set from it. For the types this
sentence contains, please extract the trigger words from it, and for the types it does not contain, return the trigger words as NONE.
We define the format of the result as: "event_type1: trigger_word1, trigger_word2, ...\nevent_type2: trigger_word1, trigger_word2,
...\n..."Here is the sentence: <sentence> Output: \n
Instruction Variation 2:
In our event detection task, we specify a set of event types: Catastrophe, Attack, Hostile_encounter, Causation, Process_start,
Competition, Motion, Social_event, Killing, Conquering. Your goal is to analyze a given sentence and identify each event types from the
predefined set. Extract the trigger words related to each event type from the sentence. If the sentence does not contain certain
event types, please indicate NONE for those types. Format the output as shown: "event_type1: trigger_word1, trigger_word2,
...\nevent_type2: trigger_word1, trigger_word2, ...\n...". Here is the sentence: <sentence> Output: \n
Figure12: VariationsofInstructionsondifferentstrategies.(takingMAVENasanexample)