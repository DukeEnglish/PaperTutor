MoVA: Adapting Mixture of Vision Experts to
Multimodal Context
Zhuofan Zong1∗, Bingqi Ma1∗, Dazhong Shen2, Guanglu Song1, Hao Shao3,
Dongzhi Jiang3, Hongsheng Li2,3(cid:0), and Yu Liu1,2(cid:0)
1 SenseTime Research
2 Shanghai AI Laboratory
3 CUHK MMLab
Abstract. As the key component in multimodal large language mod-
els (MLLMs), the ability of the visual encoder greatly affects MLLM’s
understandingondiverseimagecontent.Althoughsomelarge-scalepre-
trained vision encoders such as vision encoders in CLIP and DINOv2
have brought promising performance, we found that there is still no
single vision encoder that can dominate various image content under-
standing, e.g., the CLIP vision encoder leads to outstanding results on
generalimageunderstandingbutpoorperformanceondocumentorchart
content. To alleviate the bias of CLIP vision encoder, we first delve
into the inherent behavior of different pre-trained vision encoders and
then propose the MoVA, a powerful and novel MLLM, adaptively rout-
ing and fusing task-specific vision experts with a coarse-to-fine mech-
anism. In the coarse-grained stage, we design a context-aware expert
routing strategy to dynamically select the most suitable vision experts
according to the user instruction, input image, and expertise of vision
experts. This benefits from the powerful model function understanding
abilityofthelargelanguagemodel(LLM)equippedwithexpert-routing
low-rank adaptation (LoRA). In the fine-grained stage, we elaborately
conductthemixture-of-vision-expertadapter(MoV-Adapter)toextract
and fuse task-specific knowledge from various experts. This coarse-to-
fineparadigmeffectivelyleveragesrepresentationsfromexpertsbasedon
multimodalcontextandmodelexpertise,furtherenhancingthegeneral-
ization ability. We conduct extensive experiments to evaluate the effec-
tivenessoftheproposedapproach.Withoutanybellsandwhistles,MoVA
can achieve significant performance gains over current state-of-the-art
methods in a wide range of challenging multimodal benchmarks. Codes
and models will be available at https://github.com/TempleX98/MoVA.
Keywords: Multimodallargelanguagemodel·Visionencoder·Mixture-
of-expert
1 Introduction
Significant achievements in multimodal large language models (MLLMs) have
been witnessed due to their remarkable proficiency in solving open-world tasks.
* Equal contribution. (cid:0) Corresponding authors.
4202
rpA
91
]VC.sc[
1v64031.4042:viXra2 Zhuofan Zong et al.
Table 1: Comparison of CLIP vs. state-of-the-art task-specific vision
encoders. Our evaluation criteria encompass a variety of dimensions: compre-
hensive benchmarks [49], text-oriented Visual Question Answering (VQA) [54,56],
general VQA [23], object hallucination [39], Referring Expression Comprehension
(REC) [81], Referring Expression Segmentation (RES) [81], and medical VQA bench-
mark SLAKE [42].
VisionEncoder Task MMB DocVQA ChartQA GQA POPE REC RES SLAKE
CLIP[60] Image-textContrastive 64.9 35.6 35.3 62.5 85.7 81.5 43.3 63.7
DINOv2[57] VisualGrounding 57.5 14.7 15.9 63.9 86.7 86.1 47.5 59.4
Co-DETR[86] ObjectDetection 48.4 14.2 14.8 58.6 88.0 82.1 48.6 55.3
SAM[30] ImageSegmentation 40.7 13.9 15.0 54.0 82.0 79.2 49.3 57.7
Pix2Struct[35] TextRecognition 41.9 57.3 53.4 51.0 78.1 59.2 32.2 44.0
Deplot[43] ChartUnderstanding 36.2 40.2 55.8 48.1 75.6 51.1 27.0 44.5
Vary[75] DocumentChartParsing 28.1 47.8 41.8 42.6 69.1 21.6 16.0 40.9
BiomedCLIP[84] BiomedicalContrastive 40.0 15.3 16.8 50.8 76.9 57.8 27.4 65.1
Plainfusion - 63.4 46.5 48.9 63.0 86.4 85.7 45.3 64.7
MoVA - 65.9 59.0 56.8 64.1 88.5 86.4 49.8 66.3
MLLMsacquirevisualperceptioncapacitywhileinheritingsophisticatedreason-
ing abilities and knowledge from large language models (LLMs). The core idea
behind MLLMs is projecting the vision encoder representation into an LLM
through a projector, facilitating a general-purpose multimodal understanding.
General multimodal understanding requires comprehending complex image
contexts across various tasks and scenarios. The CLIP [60] vision encoder, pre-
trained on large-scale image-text pairs with a contrastive loss, is widely consid-
ered as a flexible and popular choice among the latest leading MLLMs. How-
ever, training data and optimization target of the vision encoder determine its
inconsistent performance across tasks and scenarios, which will bias the gen-
eralization of multimodal large language models. For instance, MLLMs with a
single CLIP vision encoder usually perform poorly on fine-grained tasks such
as grounding and optical character recognition (OCR) [75]. Several works have
attempted to incorporate extra state-of-the-art vision encoder experts to cope
with the challenge. For example, both SPHINX [41] and MoF [69] integrate vi-
sion self-supervised learning features of DINOv2 [57] with MLLMs to enhance
theirvisualgroundingcapabilities.Vary[75]introducesanewvisionencoderex-
pertforimprovedfine-graineddocumentandchartparsingability.Intuitively,it
isnecessarytoexploretheutilizationofmoretask-specificvisionencoderexperts
in MLLMs to promote model generalization across various domains.
We aim to start the exploration through empirical analysis of readily avail-
able vision experts. In particular, we focus on the multimodal capabilities of
seven distinct state-of-the-art vision encoders based on LLaVA-1.5-7B [45]. The
results in Tab. 1 reveal that MLLMs with these task-specific vision encoders
achieveoptimalperformanceintheirrespectivearea.Concurrently,wenotethat
the plain fusion (concatenation) of vision encoder experts adopted in previous
works (e.g., SPHINX) would not bring consistent improvement compared with
the single task-specific vision expert in its proficient task. The inherent bias ofMoVA: Adapting Mixture of Vision Experts to Multimodal Context 3
each expert introduces biased information and leads to performance degrada-
tion in the plain fusion paradigm. For example, DINOv2 serves as an expert
in visual grounding but performs poorly at text-oriented tasks. Representation
of DINOv2 would be regarded as biased information in text-related scenarios
so incorporating DINOv2 for these tasks would inevitably cause performance
decrease. Consequently, a flexible method of vision encoder ensemble that dy-
namicallyactivatesandweightscontext-relevanttask-specificvisionexpertscan
fully unleash the capacity of these models while avoiding model bias.
In this paper, we propose MoVA, a powerful MLLM, adaptively routing and
fusing task-specific vision experts with a coarse-to-fine mechanism. Inspired by
the powerful tool-use capabilities of LLM [59], the coarse-grained context-aware
expertroutingaimstoemployLLMtoselectvisionexpertswithstrongrelevance
to the user’s image and instruction from the expert model pool. To improve the
efficiencyandeffectivenessofcontext-awareexpertrouting,weintegrateexpert-
routinglow-rankadaptation(LoRA)[21]intotheLLMcomponentofMoVA.The
fine-grained expert fusion facilitates better extraction and integration of expert
representationsbasedonmultimodalcontext.Specifically,theexpertknowledge
extractor in the mixture-of-vision-expert adapter (MoV-Adapter) will extract
diverse task-specific knowledge from various vision experts through mixture-of-
expert (MoE) cross-attention layers. The dynamic gating network can allocate
precise expert-wise soft weights for the integration of extracted task-specific
knowledge.Underthecoarse-to-fineparadigm,weprovideaflexibleandeffective
manner of leveraging representation from experts based on multimodal context
and model expertise, further enhancing the model generalization ability.
We conduct comprehensive experiments on various benchmarks to evaluate
theeffectivenessofMoVA,includingMLLMbenchmarks,visualquestionanswer-
ing (VQA), visual grounding, image segmentation, and biomedical understand-
ing. Without any bells and whistles, MoVA can achieve significant performance
gains over current state-of-the-art methods.
To sum up, the key contributions of this paper are as follows:
(1) By analyzing the performance of individual vision encoders versus the
plainfusionofmultipleencodersacrossvarioustasks,werevealthattheinherent
bias of each vision encoder can diminish its generalization ability across other
irrelevant domains.
(2)WeproposeMoVA,apowerfulMLLMcomposedofcoarse-grainedcontext-
aware expert routing and fine-grained expert fusion with MoV-Adapter. Based
on multimodal context and model expertise, MoVA fully leverages representa-
tionfrommultiplecontext-relevantvisionencoderexpertsflexiblyandeffectively
while avoiding biased information brought by irrelevant experts.
(3) We demonstrate the effectiveness of each component in MoVA by elabo-
rateablationstudies.Withoutanybellsandwhistles,MoVAcanachievesignifi-
cant performance gains over current state-of-the-art methods in a wide range of
challenging benchmarks.4 Zhuofan Zong et al.
2 Related Work
2.1 Large Language Models
Large language models (LLMs) have achieved remarkable progress in Natural
Language Processing (NLP). The emergence of GPT-3 [7] demonstrates that
models will manifest profound capabilities in few-shot learning and zero-short
learning with increasing model parameters, training data, and training compu-
tation[27].ThepowerfulconversationalandcomprehensionabilityofChatGPT
andGPT4[2]isalsoattributedtothegeneralizationofLLMs.Meanwhile,many
institutions are involved in the research on LLM pretraining and fine-tuning,
bringing a series of open-source LLMs, including LLaMA [70,71], Vicuna [13],
Baichuan [78], Yi [1], Qwen [4], ChatGLM [14], InternLM [68], etc. Apart from
thetraditionaldensecausaltransformer[72]paradigm,mixture-of-expert(MoE)
is also a popular LLM architecture design. Switch Transformer [15] leverages
sparse experts to scale up LLMs into trillion parameters, where a router will
choose the most appropriate experts from the expert pool based on each input
token. Considering only part of the experts will be involved in model training
andinference,LLMwithMoEcanbenefitfrombothlargeparametercomplexity
and low computation cost. Mistral 8×7B model outperforms the LLaMA2-70B
model on multiple benchmarks, further verifying the effectiveness of the MoE.
2.2 Multimodal Large Language Models
Recentmultimodallargelanguagemodels(MLLMs)[3,5,9,12,37,47,63,64,73,75]
usuallyleveragethealignmentfromvisualfeaturestothelinguisticfeaturespace
to achieve superior vision-language understanding capabilities based on off-the-
shelf LLMs and vision encoders. CLIP vision encoder [60], which is trained in
contrastive learning from billions of diverse image-text pairs [61,62], is widely
used among these works. For example, LLaVA [47] adopts an MLP projector to
align visual tokens from the frozen CLIP vision encoder to the embedding layer
of LLM. However, The representation from CLIP exhibits strong discrimina-
tive abilities in classification and recognition but only has limited performance
on downstream tasks like location and relation understanding [22]. To break
through this bottleneck, some works [10,12] turn to unlock the CLIP vision en-
coder and further fine-tune the parameter with training data for downstream
tasks. For instance, Qwen-VL [5] collected massive training data for grounding
and OCR to jointly optimize the CLIP vision encoder and LLM. Recent works
proposetoinvolveanextrafrozenvisionencodertoenhancetheperformanceof
MLLMs. SPHINX [41] is one of the pioneers, where grounding capabilities have
been significantly improved with the assistance of the DINOv2 [57]. Vary [75]
introducesanextraencodertrainingonlarge-scalechartsanddocumentdatato
improve the performance on related downstream tasks.MoVA: Adapting Mixture of Vision Experts to Multimodal Context 5
Step 1: Step 2:
Context-aware expert routing Expert fusion with MoV-Adapter
LoRA
Large Language Model Large Language Model
Selection: Expert 1, Expert 2
Downsample < < Hs m ey o rs edt e ie l sm d up e sr s eo c rm r ip qpt ut> ei so tn is o>
n:
visuE alx gp re or ut n 1 ding MoV-Adapter Q s si a: g yW n ?h e ar ne d i ws h at th e d ore ed s it
###
Base Encoder W anh de r we h i as t t dh oe e sr e id t s si ag yn ? textE rx ep ce or gt n i2 tion Base Encoder
### ……
Identify and select
models that will best…
Expert N
Q: Where is the red chart processing
s si ag yn ? and what does it Vision Experts
Fig.1: The pipeline of MoVA. MoVA performs coarse-to-fine routing to solve a
givenquestion.Thecoarsecontext-awareexpertroutingisperformedinthefirststage
to select context-relevant experts. Next, we adopt the MoV-Adapter to extract and
fuse the task-specific knowledge from these selected experts in a fine-grained manner.
3 MoVA Methodology
3.1 Overview
MoVA comprises five key components: 1) a pre-trained large language model
(LLM) that generates accurate responses given the image tokens and instruc-
tions;2)abasevisionencoder;3)visionexpertsthatgeneratetask-specificvision
latent features; 4) an auxiliary expert-routing low-rank adaption (LoRA) mod-
ulethathelpsLLMselectappropriateexpertsbasedonimagesandinstructions;
5) mixture-of-vision-expert adapter (MoV-Adapter) that performs fine-grained
expert fusion based on the multimodal context.
AsillustratedinFig.1,MoVAconsistsoftwostages:coarse-grainedcontext-
wareexpertroutingandfine-grainedexpertfusionwithMoV-Adapter.First,our
coarse-grained context-ware expert routing leverages the tool-use capabilities of
LLM, routing the most appropriate experts from N expert candidates via LLM
tohelpthemodelanswertheuser’squestion.Weincorporatetheexpert-routing
LoRAmoduleintotheLLMtoimprovetheefficiencyandeffectivenessofexpert
routing. This expert-routing LoRA module is trained with expert routing anno-
tations and can better align the LLM and the routing task. In the second stage,
we turn to enhance the visual representation with a novel MoV-Adapter mod-
ule in a fine-grained manner. More specifically, we leverage the cross-attention
mechanismtoextractthetask-specificknowledgeofrepresentationsfromchosen
experts. Meanwhile, the dynamic gating network in MoV-Adapter can allocate
soft weights to the extracted knowledge of each expert according to the input
image and instruction. Then the extracted knowledge can be effectively inte-
grated into the foundational representation of the base vision encoder. Finally,
the enhanced visual representation with instruction tokens is fed to the LLM to
generateanaccurateresponse.InSec.3.2andSec.3.3,wewillfocusonourcore
contributions, the context-aware expert routing strategy, and the expert fusion
with MoV-Adapter. In Sec. 3.4, we will introduce the training process.6 Zhuofan Zong et al.
Table 2: Oneexampleoftheinstruction-followingdataforcontext-awareexpertrout-
ing. We present the multimodal inputs in the top block and the language response in
the bottom block. The detailed model descriptions are released in the Appendix.
Routing Prompt Input
Asarouter,yourtaskistochooseseveralmodelsfromamodelpooltoassist
you. Below is a brief overview of the expertise of each model in the pool:
A. <DINOv2 model description>
B. <Co-DETR model description>
C. <SAM model description>
D. <Pix2Struct model description>
E. <Deplot model description>
F. <Vary model description>
G. <BiomedCLIP model description>
Here is user question:
###
Where is the red sign and what does it say?
###
Identify and select models that will best enable you to accurately answer
questions. Please consider the image contents, questions, and expertise of
these models when you perform selection. Answer with the model’s letter
from the given choices directly.
Routing Prompt Output
A, D
Pretrained Vision Encoders and LLM. The vision encoders in MoVA con-
sist of a base encoder and multiple task-specific vision encoder experts. We
choose the pre-trained CLIP ViT-L-336px as the base encoder. Our vision ex-
pertsincludeseveralstate-of-the-arttask-specificencoders:DINOv2,Co-DETR,
SAM, Pix2Struct, Deplot, Vary, and BiomedCLIP. The corresponding expertise
is presented in Tab. 1. For example, both Pix2Struct and Vary will be used
when the user asks the MLLM to scan the document image. MoVA is flexible
andeasytogeneralizetoalldecoder-onlyLLMs.WemainlyconsiderVicuna-7B
and Yi-34B as our language model in this work.
3.2 Coarse-grained Context-aware Expert Routing
The context-aware expert routing strategy aims to employ the impressive rea-
soning capacity of LLM to select vision experts with strong relevance to the
user’s image and instruction from the expert model pool.
Pipeline of Context-aware Routing. Specifically, we perform the context-
aware expert routing in three steps during inference. First, the input image,MoVA: Adapting Mixture of Vision Experts to Multimodal Context 7
userquestions,anddescriptionsofexpertmodelsareconvertedintoappropriate
instructions that prompt the MLLM to perform expert selection. An example
of the prompt instruction input and selection output is shown in Tab. 2. Such
a routing task does not require high-resolution input images, hence we directly
downsample the base encoder’s visual feature to obtain a coarse image embed-
ding(e.g.,64imagetokens).Consideringthedifferencebetweenroutinginstruc-
tions and conventional multimodal data, avoiding task conflict is necessary. We
integrate additional lightweight expert-routing LoRA layers into the LLM for
accurate expert routing and further disentangle the routing task from original
multimodal tasks (e.g., conversation about natural scenes). The downsampled
image tokens and instruction tokens are then fed to the LLM as inputs. Finally,
the LLM generates the output text and we parse it to determine which vision
expert should be selected for fine-grained knowledge extraction in the second
stage.Forinstance,asdepictedinTab.2,theLLMdirectlyoutputstheoption’s
letter of DINOv2 and Pix2Struct, thus we only utilize them for the subsequent
extraction.Duringtraining,wedonotperformcontext-awareexpertroutingand
replace the routing outputs with our routing annotations to improve efficiency.
Routing Data Construction. ComparedwithotherMLLMs,MoVArequires
additional routing annotations. We first introduce the formal definition of the
data structure for an unambiguous understanding of the routing data. The data
structure for expert routing introduces additional routing annotation R to the
conventional multimodal data (I,Q,A). Here, I represents the image, Q and A
refer to the question-answer pair, and R refers to the expert set which contains
the most appropriate ones to solve this question. Then the construction process
forroutingdatacanbeformulatedas(I,Q,A)→R,withtheprimaryobjective
being to derive vision experts that optimally align with the sample (I,Q,A).
Intuitively,thelanguagemodelinglosscanserveasaneffectivemetricforevalu-
atinghowa datasamplealignswiththevisionexpert.Specifically,wecanreuse
the LLaVA-1.5-7B models with various vision encoders presented in Sec. 1 to
perform loss computation. Here, we denote the model with the base encoder
as M and the model with j-th expert among N experts as M . For the i-th
0 j
sample (I ,Q ,A ), we send it to models {M |j ∈ {0,1,...,N}} and calculate
i i i j
the language modeling loss {L |j ∈{0,1,...,N}}. The j-th expert is regarded
i,j
as a useful expert for the i-th sample only if L < L and will be added to
i,j i,0
the routing set R . Note that we only keep up to 3 vision experts to avoid the
i
increasing computation costs brought by too many additional experts. All the
routing annotations of our training data are generated offline. We can directly
parse and input these offline results to the subsequent expert fusion component
and LLM during training.
3.3 Fine-grained Expert Fusion with MoV-Adapter
We propose the MoV-Adapter to facilitate fine-grained expert representation
extraction and integration based on multimodal context. As shown in Fig. 2,8 Zhuofan Zong et al.
the MoV-Adapter consists of L adapter blocks and a text encoder. Each block
containsanexpertknowledgeextractor,adynamicgatingnetwork,andatrans-
former block. For the i-th block, the input feature is denoted as Xi ∈RC×H×W
and we take the CLIP base encoder feature X∈RC×H×W as the input feature
X1 ofthefirstblock.WeuseGtoindicatetheindicesofchosenK experts.The
expert feature set is {F |j ∈ G}. The final output feature of the MoV-Adapter
j
is connected to the LLM text embedding space by an MLP layer.
Text Encoder. We introduce a pre-trained BERT as the text encoder to ex-
tract language context information from the user’s instruction. We take the
[CLS] token from the output of the text encoder as the text token X ∈
T
RCT. It is worth noting that all the adapter blocks share the same text token.
Expert Knowledge Extractor. We adopt Expert Knowledge Extractor
)$
N cross-attention layers as the expert knowl-
ℱ&! ℱ&" ℱ&#
edge extractor to achieve efficient knowledge Cross Cross … Cross
Attention Attention Attention Global
Pooling
extraction.Notethatonlytheexpertfeatures
*$ Dynamic Text
{F j|j ∈ G} and their corresponding cross- )($ Transformer Gating Encoder
Self Block
attention layers are involved in the extrac- Attention
tion. For each selected expert feature F j ∈ FFN
RCj×Hj×Wj,wefirstalignitsresolutiontoXi )$%! A Bd la op ct ker ×"
with bilinear interpolation:
Fig.2: The architecture of MoV-
Fˆ =Interpolate(F ,H,W). (1) Adapter.
j j
Forthei-thMoV-Adapterblockandthej-thcross-attentionlayer,wetakeinput
feature Xi as query, and the aligned expert feature Fˆ as the key and value:
j
Yi =Xi+Attention(Xi,Fˆ ). (2)
j j
Dynamic Gating Network. We employ a dynamic gating network to con-
tributetoafine-grainedknowledgeintegrationprocessfortheconditionalrepre-
sentation {Yi|j ∈G}. It is implemented with the softmax over the logits of an
j
MLP layer, processing multimodal representation to generate expert-wise soft
weight Pi ∈ RK for the output of each cross-attention layer in the extractor.
Specifically, the input to the gating network is the concatenated vector of a vi-
sual token Xi
V
∈ RC and the text token X
T
∈ RCT. We obtain Xi
V
with a
global average pooling operation to Xi. Then we concatenate them to compute
thegatingweightsandtheexpert-wiseoutputsbycomputingtheweightedsum:
Xˆi = (cid:88) Yi ·Pi, (3)
j j
j∈G
where Pi ∈(0,1) is the soft weight for the j-th expert in the i-th block.
jMoVA: Adapting Mixture of Vision Experts to Multimodal Context 9
Stage 1: Pretraining Stage 2: Supervised Finetuning Stage 3: Expert-routing LoRA Training
Answer Answer Routing
Supervision Supervision Annotation
Supervision
LoRA
Large Language Model Large Language Model Large Language Model
Vision Experts MoV-Adapter Vision Experts MoV-Adapter Downsample
User User Routing
Base Encoder Instruction Base Encoder Instruction Base Encoder Instruction
Routing Image Routing Image Image
Annotation Annotation
Fig.3: The training strategy of MoVA. We enhance the task-specific knowledge
extraction capacity of the MoV-Adapter in the first stage. Then, we excite model
multimodalcapacitiesinthesupervisedfinetuningstage.Theexpertroutingabilityis
unlocked in the last stage.
Transformer Block. The transformer block in the adapter block follows the
vanilla design, consisting of a self-attention layer and an FFN layer. Taking the
fused visual representation Xˆi, its output will serve as the input feature Xi+1
for the next adapter block.
3.4 Training Paradigm
AsdepictedinFig.3,thetrainingprocessofMoVAconsistsofthreestages:MoV-
Adapter pretraining, supervised finetuning, and expert-routing LoRA training.
Pretraining. To improve multimodal generalization, we first construct 15M
visualinstructionsamplesacrossdiversepublicdatasetsfordifferentdownstream
tasks as the training data:
– ImageCaption:DataComp-1B[17]4,ShareGPT4V-PT[10],andALLaVA-
4V [8].
– Visual Grounding and Localization: Objects365 [65], RefCOCO [81],
VisualGenome [31], PointQA [53], and Flickr30K [58].
– ChartUnderstanding:MMC-Instruction[44],Chart2Text[26],DVQA[25],
and SciGraphQA [38].
– Text Recognition and Document Parsing: LLaVAR-PT [85] and 3M
English document images from Common Crawl 5.
– Biomedical Image Understanding: LLaVA-Med [36].
Notethatforeachdataset,weaddtheannotationsofcoarse-grainedexpertrout-
ing via the method proposed in Sec. 3.2. During the pretraining phase, we only
optimize the MoV-Adapter along with the base vision encoder while preserving
the capabilities of the initial large language model. Meanwhile, we leverage the
routingannotationstochooseexpertsandignorerepresentationsfromirrelevant
ones during training.
4 Only 4M image-text pairs are randomly selected for the efficiency
5 https://commoncrawl.org10 Zhuofan Zong et al.
Supervised Finetuning. We utilize high-quality visual instruction tuning
data that build upon LLaVA-665K [45] for finetuning. Additionally, we inte-
grate several visual question answering datasets across various domains, such
as DocVQA [56], ChartQA [54], InfographicVQA [55], AI2D [28], ST-VQA [6],
TextVQA[66],SynthDoG-en[29],Geometry3K[51],PGPS9K[82],Geo170K[18],
VQA-RAD [34], and SLAKE [42].
Wealsoencompassequivalentcomprehensivecaptions[8,10,24,33]generated
by the advanced GPT4-V [2] for improved world knowledge. In the supervised
fine-tuning stage, task-specific vision experts are frozen and we jointly optimize
the base vision encoder, MoV-Adapter, and LLM. The objective of supervised
fine-tuning is to align the enhanced visual representation and the embedding of
LLM, boosting its visual instruction-following capabilities. The coarse-grained
routing annotations are also directly used in this training phase.
Expert-routing LoRA Training. We introduce the expert-routing LoRA
layers into the LLM and only train these LoRA layers in the final stage. We use
the same instruction tuning data as the second stage for routing task tuning.
4 Experiments
4.1 Implementation Details
As mentioned in Sec. 3.4, our training pipeline consists of three stages. In the
pretraining stage, we use the AdamW optimizer with an initial learning rate
of 2×10−4, a batch size of 1024, and train the model for 1 epoch. We jointly
finetune the weights of the base vision encoder, MoV-Adapter, and LLM with
a batch size of 128 and an initial learning rate of 2×10−5 during supervised
fine-tuning. In the last stage, only the LoRA layers are trained and we keep
the same hyperparameter setting as the supervised fine-tuning phase. We use 3
transformerblocks(L=3)intheMoV-Adapteranditshiddendimensionis1024,
which is consistent with the base vision encoder CLIP. The input resolution of
the base vision encoder is set as 672×672. Two residual blocks with an average
poolingareemployedintheMoV-Adaptertoreducethenumberofoutputimage
tokens from 2304 to 576. More details about vision experts are released in the
Appendix.
For the proxy setting performed in Tab. 1, we follow the default setting of
LLaVA-1.5 but incorporate several additional datasets, including DocVQA [56],
ChartQA [54], RefCOCO referring segmentation data [81], LLaVA-Med [36],
VQA-RAD [34], and SLAKE [42].
4.2 MLLM Benchmarks
We empirically analyze the multimodal capacity and generalization ability of
MoVA on a wide range of challenging MLLM benchmarks in Tab. 3. Specifi-
cally,thiscomprehensiveassessmentisconductedonMME[16],MMBench[49],
QBench [76], MathVista [50], MathVerse [83], and POPE [39].MoVA: Adapting Mixture of Vision Experts to Multimodal Context 11
Table 3: Performance comparison with current state-of-the-art frameworks
on popular MLLM benchmarks.PTandSFTindicatethenumberofmultimodal
trainingsamplesinpretrainingandfinetuningstage.#IMGmeansthenumberofimage
tokens processed by LLM.
Model LLM PT SFT #IMG MME MMBMMBCN QBenchMathVistaMathVersePOPE
ProprietaryMLLMs
Qwen-VL-Plus[5] – – – – – 66.2 68.0 66.0 43.3 11.8 –
Qwen-VL-Max[5] – – – – – 77.6 75.1 73.6 51.0 24.8 –
Gemini-Pro[67] – – – – – 73.6 74.3 68.2 45.2 22.3 –
GPT-4V[2] – – – – – 75.8 73.9 74.5 49.9 38.3 –
Open-sourceMLLMs
Qwen-VL[5] Qwen-7B 1.4B 50M 256 – 38.2 7.4 59.4 – – –
Qwen-VL-Chat[5] Qwen-7B 1.4B 50M 256 1488/361 60.6 56.7 – – – –
LLaVA-1.5[45] Vicuna-7B 558K 665K 576 1511/316 64.3 58.3 58.7 – – 85.9
LLaVA-1.5[45] Vicuna-13B 558K 665K 576 1531/295 67.7 63.6 62.1 27.6 7.6 85.9
mPLUG-Owl2[79] LLaMA2-7B 348M 1.2M 64 1450/– 64.5 – 62.9 – 4.6 85.8
VILA-7B[40] LLaMA2-7B 50M 1M 576 1533/– 68.9 61.7 – – – 85.5
VILA-13B[40] LLaMA2-13B 50M 1M 576 1570/– 70.3 64.3 – – – 84.2
SPHINX-2k[41] Vicuna-13B 115M – 2880 1471/327 65.9 57.9 – 27.8 – 87.2
LLaVA-NeXT[46] Vicuna-7B 558K 760K 2880 1519/332 67.4 60.6 – 34.6 – 86.5
LLaVA-NeXT[46] Hermes-Yi-34B 558K 760K 2880 1631/397 79.3 79.0 – 46.5 – 87.7
MoVA Vicuna-7B 15M 1.6M 576 1562/371 70.4 63.7 69.3 37.6 19.7 88.6
MoVA Hermes-Yi-34B 15M 1.6M 576 1603/455 81.3 79.0 70.7 44.3 23.7 88.3
Comparedtootheropen-sourceMLLMswithsimilarmodelcomplexity,MoVA
with Vicuna-7B achieves the best performance across 7 MLLM benchmarks
while offering a more favorable balance between training efficiency and per-
formance. For instance, MoVA-7B surpasses the recent state-of-the-art LLaVA-
NeXT-7B[46]withadynamichighresolutiondesign,processingonly20%image
tokens.
Furthermore,weadoptHermes-Yi-34B[1]astheLLMtovalidatethescaling
property of MoVA. As depicted in Tab. 3, the performance of MoVA-34B is on
par with popular proprietary MLLMs (e.g., Gemini-Pro [67]) and outperforms
Qwen-VL-Plus[5]on5MLLMbenchmarks.Forexample,MoVAestablishesnew
records on MMBench and MMBench-CN, even surpassing the GPT-4V [2] by
a clear margin. These results suggest that the ensemble of vision experts with
adaptive expert routing can serve as an effective dimension for MLLM model
scaling.
4.3 Visual Question Answering
The evaluation results on VQA benchmarks are presented in Tab. 4. In this
section, we divide these benchmarks into general VQA benchmarks [19,23,52]
and text-oriented VQA benchmarks [28,54,56,66].
Thankstothedynamicandefficienttask-specificknowledgeextraction,MoVA
achievesstate-of-the-artperformancesacrossdiverseVQAbenchmarks.Forgen-
eralVQAbenchmarks,MoVA-7BoutperformsInternVL-Chat[12]equippedwith
InternViT-6BonVQAv2[19]andGQAby4.2%and1.9%,respectively.Besides,
MoVA shows its proficiency in text recognition in various scenarios, including12 Zhuofan Zong et al.
Table4:PerformancecomparisononVQAbenchmarks.Wepresentthenumber
of model parameters of each MLLM for a clear complexity comparison. * denotes the
methods with zero-shot setting.
GeneralVQA Text-orientedVQA
Model LLM Params
VQAv2 GQA SQAI TextVQA ChartQA DocVQA AI2D
Generalistmodels
Qwen-VL[5] Qwen-7B 10B 79.5 59.3 67.1∗ 63.8 65.7 65.1 62.3
Qwen-VL-Chat[5] Qwen-7B 10B 78.2 57.5 68.2∗ 61.5 66.3 62.6 57.7
LLaVA-1.5[45] Vicuna-7B 7B 78.5 62.0 66.8∗ 58.2∗ – – –
LLaVA-1.5[45] Vicuna-13B 7B 80.0 63.3 71.6∗ 61.3∗ – – –
VILA-7B[40] LLaMA2-7B 7B 79.9 62.3 68.2∗ 64.4∗ – – –
SPHINX-2k[41] Vicuna-13B 16B 80.7 63.1 70.6∗ 61.2 – – 65.1
InternVL-Chat[12] Vicuna-7B 13B 79.3 62.9 – 57.0∗ – – –
Vary-base[75] Qwen-7B 7B – – – – 65.3 76.3 –
CogAgent[20] Vicuna-7B 18B 83.7 – – 76.1 68.4 81.6 –
Specialistmodels
Pix2Struct-Large[35] – 1.3B – – – – 58.6 76.6 42.1
PALI-X-55B[11] – 55B 86.0 – – 71.4 70.9 80.0 81.2
MoVA Vicuna-7B 10B 83.5 64.8 74.4∗ 76.4 68.3 81.3 74.9
MoVA Hermes-Yi-34B 38B 82.3 63.9 79.0∗ 77.8 73.8 84.2 83.0
scene text, chart, document, and diagram. For instance, MoVA-7B catches up
to the current state-of-the-art generalist CogAgent [20] with 18 billion param-
eters on these text-oriented benchmarks with smaller model size. The MoVA
model with 38B parameters even surpasses the well-established specialist model
PALI-X-55B [11] by clear margins. The outstanding performances on distinct
VQAbenchmarksdemonstrateMoVA’srobustgeneralizationcapabilitiesacross
diverse domains.
4.4 Visual Grounding
WeconductexperimentsonReferringExpressionComprehension(REC)bench-
marks[81]toevaluatethevisualgroundingabilityofMoVA.Theresultsarepre-
sented in Tab. 5. Compared with the previous leading generalist CogVLM with
17B parameters, MoVA-7B attains higher scores on 6 of 8 splits while reducing
the model size by 40%. Besides, the performance of MoVA-7B is on par with
the state-of-the-art specialist models that are elaborately designed for ground-
ingtasks.Forexample,MoVA-7Bachievesascoreof90.22%onRefCOCO+val,
which is 2.46% higher than the score of UNINEXT-H [77]. Our largest model
MoVA-34B further pushes the performance bound of visual grounding on these
benchmarks. These impressive results demonstrate MoVA’s remarkable visual
grounding capacity.
4.5 Medical Visual Question Answering
This experiment is conducted on popular medical VQA benchmarks VQA-RAD
and SLAKE. We directly leverage the medical VQA evaluation metric adoptedMoVA: Adapting Mixture of Vision Experts to Multimodal Context 13
Table 5: Performance comparison (Acc@0.5) on RefCOCO REC task. Spe-
cialistsarespecificallydesignedforthegroundingtaskorfinetunedonRefCOCOdata.
RefCOCO RefCOCO+ RefCOCOg
Type Model
val test-A test-B val test-A test-B val test
Shikra-13B[9] 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16
Ferret-13B[80] 89.48 92.41 84.36 82.81 88.14 75.17 85.83 86.34
Qwen-VL[5] 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48
Generalist
SPHINX-2k[41] 91.10 92.88 87.07 85.51 90.62 80.45 88.07 88.65
CogVLM[73] 92.51 93.95 88.73 87.52 91.81 81.43 89.46 90.09
MoVA-7B 92.55 94.50 88.81 87.70 92.05 82.94 89.28 89.70
MoVA-34B 93.38 94.66 90.58 89.64 92.53 84.03 91.09 90.78
G-DINO-L[48] 90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02
Specialist
UNINEXT-H[77] 92.64 94.33 91.46 85.24 89.63 79.79 88.73 89.37
Table 6: Comparisons on the Table 7: Results of component-wise abla-
biomedical VQA datasets. tion studies.
VQA-RAD SLAKE Design POPEGQAChartQADocVQA
Model
OpenCloseOpenClose
MoVA 88.6 64.8 68.3 81.3
LLaVA-Med 28.6 56.3 70.6 54.6
Randomrouting 86.8 63.1 60.4 71.6
LLaVA-1.5 35.3 68.9 73.1 63.7
w/orouting 86.4 63.4 62.5 73.7
MoVA 38.3 68.9 78.2 68.8
LLaVA-Med(ft) 61.5 84.2 83.1 85.3 w/oMoV-Adapter 87.3 62.7 65.2 77.1
by LLaVA-Med. Each sample of VQA-RAD and SLAKE is observed only once
during the training process of MoVA and LLaVA-1.5. For a fair comparison,
we compare MoVA with the LLaVA-Med variant that is finetuned with only 1
epoch on the benchmark. The performance of the LLaVA-Med specialist that
is fully finetuned on downstream tasks is also reported. As presented in Tab. 6,
MoVA-7BconsistentlyyieldshigherscoresthanLLaVA-MedandLLaVA-1.5on
both medical VQA benchmarks, exhibiting its medical visual chat ability.
4.6 Image Segmentation
In this experiment, we aim to investigate if task-specific knowledge can im-
prove MoVA on the segmentation task. Therefore, we introduce a simple design
to extend MoVA to segmentation tasks. Unlike segmentation generalists [32]
that adopt an additional pixel decoder with high-resolution images for high-
quality mask generation, we just formulate the referring segmentation task as
sequential polygon generation [74]. We finetune MoVA and the baseline with
a SAM-Huge [30] backbone on the RefCOCO referring segmentation datasets.
MoVA achieves 57.1% gIoU on the testA benchmark, which is 2.6% higher than
the 54.5% of baseline. This result indicates that MoVA is capable of exploiting
task-specific knowledge to solve segmentation tasks.14 Zhuofan Zong et al.
Table 8: Experimental results of K Table9:PerformanceofvariousMoV-
varying from 1 to 3. Adapter variants.
K MMBPOPEGQAChartQA Design MMEPMMBPOPEGQA
Dynamic 70.4 88.6 64.8 68.3 MoVA 1562 70.4 88.6 64.8
1 68.4 89.2 64.0 64.9 2blocks 1526 70.1 87.9 63.9
2 71.0 87.6 63.5 66.7 4blocks 1578 69.4 88.3 64.5
3 68.1 85.7 63.2 67.4 Uniformgating 1521 69.1 87.5 64.1
4.7 Ablation Study
Component-wise analysis. As presented in Tab. 7, we perform an ablation
to thoroughly delve into the effect of each component. First, we try to replace
thecontext-awareroutingwithrandomrouting.Withouttask-relevantvisionex-
perts, the performance drops by a large margin, especially on the text-oriented
ChartQA and DocVQA benchmarks. Removing context-aware routing to lever-
age all vision experts also brings similar results. It proves that both these mod-
ifications introduce biased information from irrelevant vision experts due to the
removalofcontext-awarerouting.Then,weablatetheeffectivenessoftheMoV-
Adapter by replacing it with simple linear layers. The removal of fine-grained
expert feature fusion downgrades performance across all datasets. These results
delineate that each component in MoVA can consistently yield significant gains.
Number of activated experts. Inthecontext-awareroutingphase,thenum-
berofactivatedexpertsK isdynamic.Wecomparesuchadata-dependentdesign
with other variations of constant K in this experiment. As presented in Tab. 8,
the overall performance of dynamic K consistently outperforms other models
with constant K. This reveals this dynamic implementation can fully exploit
the task-specific knowledge of relevant experts while avoiding the incorporation
of biased information.
Adapter Design. In this section, we conduct ablation studies on the design
of the MoV-Adapter. The number of adapter blocks directly affects the model’s
complexity and efficiency. As presented in Table 9, we compared the impact
of using 2, 3, and 4 adapter blocks on the model’s performance. We observed
that the baseline with 3 blocks can achieve better performance than other set-
tings with 2 blocks or 4 blocks. Therefore, we empirically set L to 3 by default.
Then, we substituted our multimodal gating for uniform gating to investigate
its effectiveness. Each of the experts is assigned the same soft weight in the
uniform gating. We find uniform gating brings consistent performance drops in
the test benchmarks. It indicates that the lack of the dynamic soft-weight tech-
nique harms the overall performance since it fails to perform precise knowledge
extraction.MoVA: Adapting Mixture of Vision Experts to Multimodal Context 15
Q: Provide the bounding box coordinate of Q: When does the gap between data Q: Are the opacities present in
the region this sentence describes: about boys and girls reach become both lungs?
baseball player in red shirt to right of largest?
batter.
DINOv2: 0.44 Co-DETR:0.56 Pix2Struct: 0.62 Deplot: 0.29 Vary: 0.09 DINOv2: 0.29 BiomedCLIP: 0.71
Fig.4: The coarse-grained and fine-grained routing visualization of MoVA.
4.8 Visualization
InFig.4,wepresentafewsamplesinthreedifferentscenarios:visualgrounding,
chart understanding, and biomedical multimodal processing. For each example,
we provide the question, image, routing information for the expert models, and
theircorrespondingweights.Thecoarse-grainedroutingeffectivelyidentifiesrel-
evant experts for these cases. The fine-grained routing also accurately assigns
specific weights to each identified expert, ensuring precise expert allocation.
5 Conclusion
In this paper, we reveal that the inherent bias of each vision encoder can di-
minish its generalization ability across other irrelevant domains by analyzing
theperformanceofindividualvisionencodersversustheplainfusionofmultiple
encoders across various tasks. To deal with the problem, we propose MoVA,
a powerful MLLM composed of coarse-grained context-aware expert routing
and fine-grained expert fusion with MoV-Adapter. Based on multimodal con-
text and model expertise, MoVA fully leverages representation from multiple
context-relevant vision encoder experts flexibly and effectively while avoiding
biased information brought by irrelevant experts. We demonstrate the effective-
ness of each component in MoVA by elaborate ablation studies. Without any
bellsandwhistles,MoVAcanachievesignificantperformancegainsovercurrent
state-of-the-art methods in a wide range of challenging benchmarks.
References
1. 01-AI: Yi. https://huggingface.co/01-ai (2023)
2. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,
D.,Altenschmidt,J.,Altman,S.,Anadkat,S.,etal.:Gpt-4technicalreport.arXiv
preprint arXiv:2303.08774 (2023)16 Zhuofan Zong et al.
3. Alayrac,J.B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Hasson,Y.,Lenc,K.,Men-
sch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for
few-shotlearning.AdvancesinNeuralInformationProcessingSystems35,23716–
23736 (2022)
4. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y.,
Huang, F., et al.: Qwen technical report. arXiv preprint arXiv:2309.16609 (2023)
5. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,
J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 (2023)
6. Biten, A.F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar,
C., Karatzas, D.: Scene text visual question answering. In: Proceedings of the
IEEE/CVF international conference on computer vision. pp. 4291–4301 (2019)
7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners.Advancesinneuralinformationprocessingsystems33,1877–1901(2020)
8. Chen,G.H.,Chen,S.,Zhang,R.,Chen,J.,Wu,X.,Zhang,Z.,Chen,Z.,Li,J.,Wan,
X.,Wang,B.:Allava:Harnessinggpt4v-synthesizeddataforalitevision-language
model. arXiv preprint arXiv:2402.11684 (2024)
9. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-
ing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195
(2023)
10. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:
Sharegpt4v: Improving large multi-modal models with better captions. arXiv
preprint arXiv:2311.12793 (2023)
11. Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz,
C.R.,Goodman,S.,Wang,X.,Tay,Y.,etal.:Pali-x:Onscalingupamultilingual
vision and language model. arXiv preprint arXiv:2305.18565 (2023)
12. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q.,
Zhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning
for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238 (2023)
13. Chiang,W.L.,Li,Z.,Lin,Z.,Sheng,Y.,Wu,Z.,Zhang,H.,Zheng,L.,Zhuang,S.,
Zhuang,Y.,Gonzalez,J.E.,Stoica,I.,Xing,E.P.:Vicuna:Anopen-sourcechatbot
impressing gpt-4 with 90%* chatgpt quality (March 2023), https://lmsys.org/
blog/2023-03-30-vicuna/
14. Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., Tang, J.: Glm: General
language model pretraining with autoregressive blank infilling. In: Proceedings of
the60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
1: Long Papers). pp. 320–335 (2022)
15. Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion param-
eter models with simple and efficient sparsity. The Journal of Machine Learning
Research 23(1), 5232–5270 (2022)
16. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li,
K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal
large language models. arXiv preprint arXiv:2306.13394 (2023)
17. Gadre, S.Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten,
R., Wortsman, M., Ghosh, D., Zhang, J., et al.: Datacomp: In search of the next
generation of multimodal datasets. Advances in Neural Information Processing
Systems 36 (2024)
18. Gao,J.,Pi,R.,Zhang,J.,Ye,J.,Zhong,W.,Wang,Y.,Hong,L.,Han,J.,Xu,H.,
Li, Z., et al.: G-llava: Solving geometric problem with multi-modal large language
model. arXiv preprint arXiv:2312.11370 (2023)MoVA: Adapting Mixture of Vision Experts to Multimodal Context 17
19. Goyal,Y.,Khot,T.,Summers-Stay,D.,Batra,D.,Parikh,D.:Makingthevinvqa
matter:Elevatingtheroleofimageunderstandinginvisualquestionanswering.In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 6904–6913 (2017)
20. Hong,W.,Wang,W.,Lv,Q.,Xu,J.,Yu,W.,Ji,J.,Wang,Y.,Wang,Z.,Dong,Y.,
Ding,M.,etal.:Cogagent:Avisuallanguagemodelforguiagents.arXivpreprint
arXiv:2312.08914 (2023)
21. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021)
22. Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: A comprehensive
benchmark for open-world compositional text-to-image generation. Advances in
Neural Information Processing Systems 36 (2024)
23. Hudson,D.A.,Manning,C.D.:Gqa:Anewdatasetforreal-worldvisualreasoning
and compositional question answering. In: Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition. pp. 6700–6709 (2019)
24. Jimmy, C.: Textocr-gpt4v. https://huggingface.co/datasets/jimmycarter/
textocr-gpt4v (2024)
25. Kafle,K.,Price,B.,Cohen,S.,Kanan,C.:Dvqa:Understandingdatavisualizations
viaquestionanswering.In:ProceedingsoftheIEEEconferenceoncomputervision
and pattern recognition. pp. 5648–5656 (2018)
26. Kantharaj, S., Leong, R.T., Lin, X., Masry, A., Thakkar, M., Hoque, E., Joty, S.:
Chart-to-text:Alarge-scalebenchmarkforchartsummarization.In:Proceedingsof
the60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
1: Long Papers). pp. 4005–4023 (2022)
27. Kaplan,J.,McCandlish,S.,Henighan,T.,Brown,T.B.,Chess,B.,Child,R.,Gray,
S.,Radford,A.,Wu,J.,Amodei,D.:Scalinglawsforneurallanguagemodels.arXiv
preprint arXiv:2001.08361 (2020)
28. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., Farhadi, A.: A dia-
gram is worth a dozen images. In: Computer Vision–ECCV 2016: 14th European
Conference,Amsterdam,TheNetherlands,October11–14,2016,Proceedings,Part
IV 14. pp. 235–251. Springer (2016)
29. Kim, G., Hong, T., Yim, M., Park, J., Yim, J., Hwang, W., Yun, S., Han, D.,
Park,S.:Donut:Documentunderstandingtransformerwithoutocr.arXivpreprint
arXiv:2111.15664 7, 15 (2021)
30. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023)
31. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis,Y.,Li,L.J.,Shamma,D.A.,etal.:Visualgenome:Connectinglanguage
and vision using crowdsourced dense image annotations. International journal of
computer vision 123, 32–73 (2017)
32. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning
segmentation via large language model. arXiv preprint arXiv:2308.00692 (2023)
33. LAION: Gpt-4v dataset. https://huggingface.co/datasets/laion/gpt4v-
dataset (2023)
34. Lau,J.J.,Gayen,S.,BenAbacha,A.,Demner-Fushman,D.:Adatasetofclinically
generated visual questions and answers about radiology images. Scientific data
5(1), 1–10 (2018)18 Zhuofan Zong et al.
35. Lee, K., Joshi, M., Turc, I.R., Hu, H., Liu, F., Eisenschlos, J.M., Khandelwal, U.,
Shaw,P.,Chang,M.W.,Toutanova,K.:Pix2struct:Screenshotparsingaspretrain-
ing for visual language understanding. In: International Conference on Machine
Learning. pp. 18893–18912. PMLR (2023)
36. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T.,
Poon, H., Gao, J.: Llava-med: Training a large language-and-vision assistant for
biomedicine in one day. Advances in Neural Information Processing Systems 36
(2024)
37. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. In: International
conference on machine learning. pp. 19730–19742. PMLR (2023)
38. Li, S., Tajbakhsh, N.: Scigraphqa: A large-scale synthetic multi-turn question-
answering dataset for scientific graphs. arXiv preprint arXiv:2308.03349 (2023)
39. Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,Wen,J.R.:Evaluatingobjecthal-
lucinationinlargevision-languagemodels.arXivpreprintarXiv:2305.10355(2023)
40. Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A., Mao, H., Kautz, J.,
Shoeybi, M., Han, S.: Vila: On pre-training for visual language models. arXiv
preprint arXiv:2312.07533 (2023)
41. Lin,Z.,Liu,C.,Zhang,R.,Gao,P.,Qiu,L.,Xiao,H.,Qiu,H.,Lin,C.,Shao,W.,
Chen,K.,etal.:Sphinx:Thejointmixingofweights,tasks,andvisualembeddings
for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)
42. Liu, B., Zhan, L.M., Xu, L., Ma, L., Yang, Y., Wu, X.M.: Slake: A semantically-
labeledknowledge-enhanceddatasetformedicalvisualquestionanswering.In:2021
IEEE18thInternationalSymposiumonBiomedicalImaging(ISBI).pp.1650–1654.
IEEE (2021)
43. Liu,F.,Eisenschlos,J.M.,Piccinno,F.,Krichene,S.,Pang,C.,Lee,K.,Joshi,M.,
Chen, W., Collier, N., Altun, Y.: Deplot: One-shot visual language reasoning by
plot-to-table translation. arXiv preprint arXiv:2212.10505 (2022)
44. Liu,F.,Wang,X.,Yao,W.,Chen,J.,Song,K.,Cho,S.,Yacoob,Y.,Yu,D.:Mmc:
Advancing multimodal chart understanding with large-scale instruction tuning.
arXiv preprint arXiv:2311.10774 (2023)
45. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.
arXiv preprint arXiv:2310.03744 (2023)
46. Liu,H.,Li,C.,Li,Y.,Li,B.,Zhang,Y.,Shen,S.,Lee,Y.J.:Llava-next:Improved
reasoning,ocr,andworldknowledge(January2024),https://llava-vl.github.
io/blog/2024-01-30-llava-next/
47. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural
information processing systems 36 (2024)
48. Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,Li,C.,Yang,J.,Su,H.,Zhu,
J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set
object detection. arXiv preprint arXiv:2303.05499 (2023)
49. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J.,
He,C.,Liu,Z.,etal.:Mmbench:Isyourmulti-modalmodelanall-aroundplayer?
arXiv preprint arXiv:2307.06281 (2023)
50. Lu,P.,Bansal,H.,Xia,T.,Liu,J.,Li,C.,Hajishirzi,H.,Cheng,H.,Chang,K.W.,
Galley, M., Gao, J.: Mathvista: Evaluating mathematical reasoning of foundation
models in visual contexts. arXiv preprint arXiv:2310.02255 (2023)
51. Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., Zhu, S.C.: Inter-gps:
Interpretable geometry problem solving with formal language and symbolic rea-
soning. arXiv preprint arXiv:2105.04165 (2021)MoVA: Adapting Mixture of Vision Experts to Multimodal Context 19
52. Lu,P.,Mishra,S.,Xia,T.,Qiu,L.,Chang,K.W.,Zhu,S.C.,Tafjord,O.,Clark,P.,
Kalyan,A.:Learntoexplain:Multimodalreasoningviathoughtchainsforscience
question answering. In: The 36th Conference on Neural Information Processing
Systems (NeurIPS) (2022)
53. Mani, A., Yoo, N., Hinthorn, W., Russakovsky, O.: Point and ask: Incorporating
pointing into visual question answering. arXiv preprint arXiv:2011.13681 (2020)
54. Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for
question answering about charts with visual and logical reasoning. arXiv preprint
arXiv:2203.10244 (2022)
55. Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., Jawahar, C.: Info-
graphicvqa.In:ProceedingsoftheIEEE/CVFWinterConferenceonApplications
of Computer Vision. pp. 1697–1706 (2022)
56. Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document
images. In: Proceedings of the IEEE/CVF winter conference on applications of
computer vision. pp. 2200–2209 (2021)
57. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)
58. Plummer,B.A.,Wang,L.,Cervantes,C.M.,Caicedo,J.C.,Hockenmaier,J.,Lazeb-
nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentencemodels.In:ProceedingsoftheIEEEinternationalconferenceon
computer vision. pp. 2641–2649 (2015)
59. Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang,
X., Qian, B., et al.: Toolllm: Facilitating large language models to master 16000+
real-world apis. arXiv preprint arXiv:2307.16789 (2023)
60. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021)
61. Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.,Wightman,R.,Cherti,M.,
Coombes,T.,Katta,A.,Mullis,C.,Wortsman,M.,etal.:Laion-5b:Anopenlarge-
scale dataset for training next generation image-text models. Advances in Neural
Information Processing Systems 35, 25278–25294 (2022)
62. Schuhmann,C.,Vencu,R.,Beaumont,R.,Kaczmarczyk,R.,Mullis,C.,Katta,A.,
Coombes,T.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered
400 million image-text pairs. arXiv preprint arXiv:2111.02114 (2021)
63. Shao,H.,Hu,Y.,Wang,L.,Waslander,S.L.,Liu,Y.,Li,H.:Lmdrive:Closed-loop
end-to-end driving with large language models. arXiv preprint arXiv:2312.07488
(2023)
64. Shao, H., Qian, S., Xiao, H., Song, G., Zong, Z., Wang, L., Liu, Y., Li, H.: Visual
cot:Unleashingchain-of-thoughtreasoninginmulti-modallanguagemodels.arXiv
preprint arXiv:2403.16999 (2024)
65. Shao,S.,Li,Z.,Zhang,T.,Peng,C.,Yu,G.,Zhang,X.,Li,J.,Sun,J.:Objects365:
A large-scale, high-quality dataset for object detection. In: Proceedings of the
IEEE/CVF international conference on computer vision. pp. 8430–8439 (2019)
66. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh,
D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.8317–8326
(2019)20 Zhuofan Zong et al.
67. Team,G.,Anil,R.,Borgeaud,S.,Wu,Y.,Alayrac,J.B.,Yu,J.,Soricut,R.,Schalk-
wyk,J.,Dai,A.M.,Hauth,A.,etal.:Gemini:afamilyofhighlycapablemultimodal
models. arXiv preprint arXiv:2312.11805 (2023)
68. Team, I.: Internlm: A multilingual language model with progressively enhanced
capabilities. https://github.com/InternLM/InternLM (2023)
69. Tong,S.,Liu,Z.,Zhai,Y.,Ma,Y.,LeCun,Y.,Xie,S.:Eyeswideshut?exploringthe
visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024)
70. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023)
71. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
72. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)
73. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,
L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv
preprint arXiv:2311.03079 (2023)
74. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou,
J.,Qiao,Y.,etal.:Visionllm:Largelanguagemodelisalsoanopen-endeddecoder
for vision-centric tasks. Advances in Neural Information Processing Systems 36
(2024)
75. Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yang, J., Sun, J., Han, C., Zhang,
X.:Vary:Scalingupthevisionvocabularyforlargevision-languagemodels.arXiv
preprint arXiv:2312.06109 (2023)
76. Wu,H.,Zhang,Z.,Zhang,E.,Chen,C.,Liao,L.,Wang,A.,Li,C.,Sun,W.,Yan,
Q.,Zhai,G.,etal.:Q-bench:Abenchmarkforgeneral-purposefoundationmodels
on low-level vision. arXiv preprint arXiv:2309.14181 (2023)
77. Yan,B.,Jiang,Y.,Wu,J.,Wang,D.,Luo,P.,Yuan,Z.,Lu,H.:Universalinstance
perception as object discovery and retrieval. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.15325–15336(2023)
78. Yang,A.,Xiao,B.,Wang,B.,Zhang,B.,Bian,C.,Yin,C.,Lv,C.,Pan,D.,Wang,
D., Yan, D., et al.: Baichuan 2: Open large-scale language models. arXiv preprint
arXiv:2309.10305 (2023)
79. Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou,
J.: mplug-owl2: Revolutionizing multi-modal large language model with modality
collaboration. arXiv preprint arXiv:2311.04257 (2023)
80. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv
preprint arXiv:2310.07704 (2023)
81. Yu,L.,Poirson,P.,Yang,S.,Berg,A.C.,Berg,T.L.:Modelingcontextinreferring
expressions. In: Computer Vision–ECCV 2016: 14th European Conference, Ams-
terdam,TheNetherlands,October11-14,2016,Proceedings,PartII14.pp.69–85.
Springer (2016)
82. Zhang,M.L.,Yin,F.,Liu,C.L.:Amulti-modalneuralgeometricsolverwithtextual
clauses parsed from diagram. arXiv preprint arXiv:2302.11097 (2023)
83. Zhang,R.,Jiang,D.,Zhang,Y.,Lin,H.,Guo,Z.,Qiu,P.,Zhou,A.,Lu,P.,Chang,
K.W.,Gao,P.,etal.:Mathverse:Doesyourmulti-modalllmtrulyseethediagrams
in visual math problems? arXiv preprint arXiv:2403.14624 (2024)MoVA: Adapting Mixture of Vision Experts to Multimodal Context 21
84. Zhang,S.,Xu,Y.,Usuyama,N.,Bagga,J.,Tinn,R.,Preston,S.,Rao,R.,Wei,M.,
Valluri,N.,Wong,C.,etal.:Large-scaledomain-specificpretrainingforbiomedical
vision-language processing. arXiv preprint arXiv:2303.00915 (2023)
85. Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., Sun, T.: Llavar: En-
hancedvisualinstructiontuningfortext-richimageunderstanding.arXivpreprint
arXiv:2306.17107 (2023)
86. Zong, Z., Song, G., Liu, Y.: Detrs with collaborative hybrid assignments training.
In: Proceedings of the IEEE/CVF international conference on computer vision.
pp. 6748–6758 (2023)