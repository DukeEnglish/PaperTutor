When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive
Data Engineering
Stephen Choi, William Gazeley
IRAI Labs
{stepchoi, william}@irai.co
Abstract The training of LLMs is resource and time-
intensive (Kaplan et al., 2020) and is bounded by a
This paper presents the LLM-ADE knowledge cut-off date (Cheng et al., 2024),
framework, a novel methodology for
limiting their ability to incorporate up-to-date
continued pre-training of large language
information. Additionally, these models require
models (LLMs) that addresses the
extensive data preprocessing and curation, which
challenges of catastrophic forgetting and
may be impractical for real-world applications
double descent. LLM-ADE employs
where data are often unprocessed, duplicated, and
dynamic architectural adjustments,
including selective block freezing and frequently updated. To address these challenges, a
expansion, tailored to specific datasets. novel training methodology that allows for rapid
This strategy enhances model adaptability adaptation to new data without the drawbacks of
to new data while preserving previously traditional continuous training methods is needed.
acquired knowledge. We demonstrate This paper introduces the LLM-ADE framework,
LLM-ADE's effectiveness on the
an innovative approach for the continued pre-
TinyLlama model across various general
training of LLMs that enhances their learning
knowledge benchmarks, showing
efficiency from specific datasets while preventing
significant performance improvements
issues such as catastrophic forgetting and double
without the drawbacks of traditional
descent.
continuous training methods. This
approach promises a more versatile and Existing methods like Retrieval Augmented
robust way to keep LLMs current and Generation (RAG) provide LLMs with non-
efficient in real-world applications. parametric memory, which helps but falls short in
complex reasoning tasks (Lewis et al., 2020;
1 Introduction BehnamGhader et al., 2023). Fine-tuning improves
domain adaptation but often yields suboptimal
Large Language Models (LLMs) have become
results and struggles with extended contexts
pivotal in artificial intelligence, celebrated for their
(Chung et al., 2022; Anil et al., 2022; Zhou et al.,
capacity to assimilate and utilize extensive general
2023).
knowledge (Brown et al., 2020; Kojima et al.,
Direct improvements to the core structure of
2022). These models are trained on broad datasets,
LLMs can yield benefits across various
enabling them to generate text across various
applications, reducing the need for extensive
subjects, often demonstrating a broad yet
downstream task-specific tuning. However,
sometimes superficial understanding of
continuous domain specific training, risks
information (Anil et al., 2023). However, this
diminishing the model‚Äôs broad applicability and is
breadth can come at the cost of depth, as LLMs
vulnerable to double descent and catastrophic
may generate inaccurate, "hallucinated" content,
forgetting, where model performance degrades or
particularly in task-oriented dialogues (Bang et al.,
essential knowledge is lost (Belkin et al., 2019;
2023), and struggle in specialized domains
Lopez-Paz and Ranzato, 2017). Notably, data
demanding precise knowledge (Shen et al., 2023).
duplication during training exacerbates
performance issues (Hernandez et al., 2022), and
1ongoing training can erode general knowledge
(Luo et al., 2023a), particularly models in the the
1-7 billion parameter range (Luo et al., 2023b).
The LLM-ADE framework (Large Language
Models with Adaptive Data Engineering) is
designed to meet three critical real-world criteria:
1) process and generate language on any specified
dataset, including those of lower quality or with
pre-training data overlap, 2) retain general-purpose
applicability without catastrophic forgetting, 3)
achieve high efficiency in resource utilization and
training time. LLM-ADE incorporates a dynamic
architectural adjustment strategy, utilizing layer
importance techniques to freeze and expand on
certain layers, tailored by the specified dataset
Figure 1: Decoder block
(corpus) to preserve general knowledge but
accommodate new information. The framework's
reasoning (Anil et al., 20 22; Zhou et al., 2023). Our
adaptability not only enables it to maintain
framework, LLM-ADE, diverges from purely
performance across various domains but also paves
enhancing domain-specific accuracy; instead, it
the way for a more efficient continuous learning
focuses on enriching the linguistic and reasoning
paradigm in LLMs. As such, LLM-ADE provides
capabilities of LLMs across varied data inputs.
a promising direction for future research and
LLM-ADE also incorporates Llama Pro's block
applications, aiming to make the continuous pre-
expansion (Wu et al., 2024) techniques, which
training of LLMs more accessible, versatile, and
added eight blocks to Llama2-7B and trained on 80
robust.
billion tokens for 2,830 GPU hours on Nvidia
H800 to create a new foundational model. We
2 Related Work
focus on optimizing training efficiency on
In the field of continuous pre-training (CPT), significantly smaller training corpus and fewer
several approaches have been developed to update resources for broader applicability. Additional
language models with new information while innovations include novel block placements and
mitigating the risk of catastrophic forgetting. Jang layer modification strategies tailored for each
et al. (2022) introduced a method for continual dataset, targeting adaptability and efficiency.
knowledge learning in LLMs that focuses on
3 Methodology
temporal knowledge updates with reduced
forgetting. Ke et al. (2023) developed a soft-
3.1 Base Models
masking mechanism to selectively update models
using domain-specific corpora, which helps The LLM-ADE framework is designed to enrich
maintain general knowledge while enhancing the capabilities of existing pre-trained large
domain performance. Xie et al. (2023) created language models (LLMs) by seamlessly
FinPythia-6.9B, a model adapted through domain- integrating new datasets, rather than creating new
specific pre-training for the financial sector, foundational models. For our experiments, we
transforming a general model into a domain expert. selected the TinyLlama model developed by Zhang
Despite these advancements, existing CPT et al. (2024). TinyLlama is an open-source,
techniques often require meticulous data curation, decoder-only transformer model, that strikes a
and our experiments have demonstrated that even balance. We specifically chose the 1B parameter
minor duplications in data can lead to significant TinyLlama due to its balance between
performance degradation. Other models like computational efficiency and model complexity,
InvestLM (Yang et al., 2023) and MedAlpaca (Han ensuring both accessibility and practical
et al., 2023) have shown improvements in domain- applicability in real-world scenarios. TinyLlama's
specific generalization and adaptation with Low minimal hardware requirements, manageable with
Rank Adaptation (LoRA, Hu et al, 2021) fine- a single Nvidia RTX 3090 or L4 GPU, further
tuning but fall short in facilitating extended context facilitate this balance.
2TinyLlama is a data-saturated model, having
been pre-trained on a substantial 3 trillion token
corpus. This extensive pre-training meets the
rigorous standards set by the Chinchilla optimal
thresholds (Hoffman et al., 2022). The high level of
data saturation implies that any observed
improvements in knowledge retention and
processing capabilities within TinyLlama could
suggest greater potential advantages for applying
the LLM-ADE framework to larger and more
complex models. Therefore, by demonstrating
performance enhancements in TinyLlama, we aim
to highlight the general applicability and
effectiveness of the LLM-ADE approach across
Figure 2: Block modifications
various model scales.
In terms of architecture, TinyLlama utilizes a
decoder-only transformer architecture similar to
their importance for adapting to new information.
Llama 2 (Touvon et al., 2023). It incorporates
We calculate the inverse cosine of the average
advanced features such as pre-normalization with
angular distance, i.e., cos-1(-Œï[AD]) using an
RMSNorm (Zhang and Sennrich, 2019), SwiGLU i
independent 5% of the target dataset. This metric
activation functions (Shazeer, 2020), and rotary
identifies those blocks which undergo the most
positional embeddings (RoPE, Su et al., 2022),
substantial changes when exposed to new data.
comprising 22 blocks. An illustrative diagram of a
Blocks exhibiting the highest average values are
decoder-only block is provided in Figure 1.
then prioritized for modifications, such as selective
3.2 Block importance tuning or expansion, to enhance the model‚Äôs
adaptability and performance on the specific target
To effectively identify which blocks are critical for
dataset being integrated.
adaptation during the continued pre-training phase,
we leverage methodologies from recent research in
3.3 Block Adjustments
layer-pruning. Specifically, we have adapted an
To mitigate catastrophic forgetting during
angular distance (AD) metric, based on the work
architectural modifications, it is essential to ensure
by Gromov et al. (2024) and Men et al. (2024), for
that such modifications do not significantly
assessing the relevance of each block within our
degrade the model's existing knowledge base. To
model. The angular distance between the inputs of
address this, we employ a strategy of freezing all
block i and block i+1 is calculated using the
blocks during training except where the angular
following formula:
distance between inputs shows the highest
ùê¥ùê∑ = ùëã ùëñùëáùëã ùëñ+1 . (1) variance. This selective freezing prevents updates
ùëñ
‚Äñùëã ùëñ‚Äñ 2‚Äñùëã ùëñ+1 ‚Äñ 2 to these blocks during backpropagation, thus
preserving the weights of most blocks but only
This calculation helps identify blocks where updating weights of the blocks where the most data
significant data processing shifts occur, indicating processing is performed. Additionally, we expand
Avg
Training/Dataset Hellaswag Winogrande Piqa OpenBookQA bigbench
Improvement
44.2 59.3 72.6 25.2 37.1
TinyLlama
-12.00 31.1 50.0 55.0 12.0 30.4
CPT 100% Slim Pajama
-0.02 44.9 59.3 71.5 24.8 37.8
CPT 80% Hermes, 20% SP
0.04 45.8 60.0 73.3 21.2 38.3
CPT 90% Hermes, 10% SP
0.57 45.7 59.9 72.0 25.4 38.3
CPT 100% Hermes
0.12 44.4 59.5 72.2 25.0 37.9
LoRA FT
Table 1: TinyLlama base and CPT/LoRA training on Hermes and mixed datasets
3the model's capacity by adding new blocks
immediately following the critical blocks
identified. This is depicted in Figure 2, which
illustrates a model architecture after the selection
of the third block for further training and
expansion.
Initial experiments with block expansion
involved a strategy of copying weights from the
previous blocks to the new ones (Wu et al., 2024).
However, our empirical results indicated slightly
improved performance when these newly added
blocks were initialized with random weights,
which were then scaled to align with the
distribution of the existing model weights. This
Figure 3: TinyLlama CPT Catastrophic
approach eliminates the requirement for high initial
Forgetting with Duplicate Data
learning rates, facilitating a more gradual and
effective integration of new information while
preserving the integrity of previously acquired
knowledge.
using the Language Model Evaluation Harness
(Gao et al, 2023), a robust, standardized
4 Experiments on General Knowledge
framework, for consistent comparisons.
Dataset
4.3 CPT and LoRA
To evaluate the effectiveness of the LLM-ADE
framework, we conducted a series of experiments We conducted continuous pre-training (CPT) on
focusing on continual pre-training and fine-tuning the OpenHermes 2.5 dataset, testing different
using a general knowledge, general use dataset. learning rates and batch sizes. The optimal settings
were found to be a cosine learning rate schedule
4.1 Data with a maximum of 4.0 √ó 10‚àí5 and a minimum of
4.0 √ó 10‚àí6, with a batch size of 2 million tokens.
For this study, we used the OpenHermes 2.5 dataset
LoRA fine-tuning was also applied to the base
(Teknium, 2023), consisting of 1 million high-
model using the same learning rates, a batch size of
quality synthetic samples from instruction and chat
1 million tokens, r=256, and alpha=512. On a
data generated by GPT-4. This dataset, covering a
single Nvidia L4 GPU, CPT training time for this
broad spectrum of AI-related topics, was distinct
dataset required 25 hours.
from the pre-training data of our base model,
Our experiments with the SlimPajama dataset
TinyLlama. We utilized approximately half of this
allowed us to observe the effects of data
dataset (500,000 sequences or 200 million tokens)
duplication: initial CPT on the OpenHermes
to simulate a realistic dataset size for practical
dataset marginally improved performance by 0.57
applications, reserving the remainder for testing
points, but incorporating the SlimPajama dataset
block importance. Additionally, to evaluate the
negated these gains, eliminating most of the gains
model's resistance to catastrophic forgetting, we
with only 10% duplicative data and ultimately
included randomized subsets from the SlimPajama
leading to inferior performance compared to the
dataset (Soboleva et al., 2023).
base model with 20% duplication. While training
4.2 Benchmarks time was reduced to 15 hours, LoRA fine-tuning
did not fare much better, underperforming the
Our model's performance was benchmarked using
scores of the continual training.
several well-established general knowledge tests:
Notably, training solely on the SlimPajama
HellaSwag (Zelles et al., 2019), Winogrande
dataset led to catastrophic forgetting. Figure 3
(Sakaguchi et al., 2019), Piqa (Bisk et al., 2019),
illustrates this through a graph showing the
OpenBookQA((Mihaylov et al., 2018), and
performance of TinyLlama on the HellaSwag and
BigBench (Srivastava et al., 2022). These
Winogrande benchmarks at each 1/20th interval of
evaluations were rerun on our evaluation pipelines
CPT solely on SlimPajama data. The model's
4performance dropped significantly immediately
after the introduction of the SlimPajama data and
Block 5% 10% Full did not recover throughout the training period.
1 0.87 0.87 0.87 4.4 LLM-ADE
2 1.67 1.68 1.68 For block importance testing, we chose to analyze
a 5% subset of the target dataset, as the relative
3 1.44 1.44 1.44
rankings of block importance remained consistent
4 1.36 1.36 1.36 across 5%, 10%, and 100% of the dataset (Table 2).
The metrics indicated that blocks 2 and 8 were of
5 1.35 1.35 1.35
highest importance. The effectiveness of the LLM-
6 1.16 1.16 1.16
ADE technique, which involves unfreezing and
7 1.50 1.50 1.50 expanding these blocks, is demonstrated in Table 3.
While improvements were observed when
8 1.61 1.61 1.61
unfreezing or expanding individual blocks, the
9 1.36 1.36 1.36 most significant enhancements were achieved
when both blocks were modified simultaneously,
10 1.34 1.34 1.34
surpassing the results from previous CPT and
11 1.38 1.38 1.38 LoRA configurations of 0.78 points improvement
on the base model. The improvements were mostly
12 1.51 1.51 1.51
maintained even with the introduction of
13 1.44 1.44 1.44
duplicative data: even with 20% SlimPajama mix,
14 1.09 1.09 1.09 the LLM-ADE model held 0.50 point
impovements. This comparison highlights the
15 1.26 1.26 1.26
benefits of the LLM-ADE approach, particularly
16 1.01 1.01 1.01 when both blocks are unfrozen or expanded, as
opposed to solely freezing or expanding.
17 0.86 0.86 0.86
18 0.84 0.84 0.84 5 Discussion and conclusion
19 0.87 0.87 0.87
The LLM-ADE framework introduces a novel
20 1.22 1.22 1.22 approach to continual pre-training of large
language models, addressing the challenges of
21 1.42 1.42 1.42
efficiently integrating new datasets while
Table 2: Block Importance metrics on mitigating the risks of catastrophic forgetting and
different samples of the dataset double descent. By strategically identifying critical
blocks within the model architecture using angular
distance metrics, LLM-ADE enables targeted
modifications such as selective freezing and block
Avg
Training/Dataset Hellaswag Winogrande Piqa OpenBookQA bigbench
Improvement
LLM-ADE 80% Hermes, 20%
SP 0.50 46.0 59.3 72.4 24.4 38.8
LLM-ADE 90% Hermes, 10%
SP 0.73 46.7 60.7 73.6 21.9 39.1
LLM-ADE 100% Hermes 0.78 46.6 60.3 72.5 23.9 38.9
Table 3: LLM-ADE training on Hermes and mixed datasets
5expansion. This approach allows for the effective Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber,
integration of rapidly updating datsets while Markus Freitag, Xavier Garcia, Sebastian
Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven
preserving the model's existing knowledge base.
Hand, Hadi Hashemi, Le Hou, Joshua Howland,
Experiments conducted on the TinyLlama
Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael
model using the OpenHermes 2.5 dataset illustrate
Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
the of improvements of the LLM-ADE technique
Jia, Kathleen Kenealy, Maxim Krikun, Sneha
compared to traditional continuous pre-training Kudugunta, Chang Lan, Katherine Lee, Benjamin
(CPT) and LoRA fine-tuning methods. The Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li,
simultaneous unfreezing and expansion of high- Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,
importance blocks yielded the most significant Frederick Liu, Marcello Maggioni, Aroma
Mahendru, Joshua Maynez, Vedant Misra, Maysam
performance improvements, surpassing the results
Moussalem, Zachary Nado, John Nham, Eric Ni,
obtained through individual block modifications or
Andrew Nystrom, Alicia Parrish, Marie Pellat,
alternative training configurations.
Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
Furthermore, LLM-ADE's efficiency in
Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex
resource utilization marks a step forward in Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar
sustainable AI development, aligning with the Samuel, Renee Shelby, Ambrose Slone, Daniel
increasing need for power-efficient and Smilkov, David R. So, Daniel Sohn, Simon
environmentally conscious technology solutions. Tokumine, Dasha Valter, Vijay Vasudevan, Kiran
Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui
The successful application of LLM-ADE to the
Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
TinyLlama model underlines the framework's
Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui
potential applicability across various LLMs of
Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang
different sizes and complexities, suggesting its Zhou, Denny Zhou, Slav Petrov, Yonghui Wu. 2023.
adaptability to broader use cases in the AI industry. PaLM 2 Technical Report. arXiv preprint arXiv:
In summary, LLM-ADE not only meets the 2305.10403.
rigorous demands of modern AI tasks but also sets
Yejin Bang, Samuel Cahyawijaya, , Nayeon Lee,
a new standard for future developments in the Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,
domain of machine learning and AI. It promises to Ziwei Ji, Tiehzheng Yu, Willy Chung, Quyet Do, Xu
enhance the robustness, flexibility, and efficiency Yan, and Pascale Fung. 2023. A Multitask,
of LLMs, paving the way for more dynamic, Multilingual, Multimodal Evaluation of ChatGPT
on Reasoning, Hallucination, and Interactivity.
adaptable, and efficient models that are capable of
arXiv preprint arXiv: 2302.04023.
evolving in sync with the rapid pace of information
change. This framework could potentially Parishad BehnamGhader, Santiago Miret and Siva
revolutionize how we train and maintain state-of- Reddy. 2023. Can Retriever-Augmented Language
Models Reason? The Blame Game Between the
the-art LLMs, making continuous learning a
Retriever and the Language Model. In Findings of
practical and scalable reality
the Association for Computational Linguistics:
EMNLP 2023.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik
References Mandal. 2019. Reconciling Modern Machine
Learning Practice and the Classical Bias‚ÄìVariance
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin
Trade-off. Proceedings of the National Academy of
Johnson, Dmitry Lepikhin, Alexandre Passos,
Sciences 116(32):15849‚Äì15854
Siamak Shakeri, Emanuel Taropa, Paige Bailey,
Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gao, and Yejin Choi. 2020.. PIQA: Reasoning about
Gaurav Mishra, Erica Moreira, Mark Omernick, Physical Commonsense in Natural Language. In
Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Proceedings of AAAI.
Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Hernandez Abrego, Junwhan Ahn, Jacob Austin,
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Paul Barham, Jan Botha, James Bradbury,
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Siddhartha Brahma, Kevin Brooks, Michele
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Catasta, Yong Cheng, Colin Cherry, Christopher A.
Gretchen Krueger, Tom Henighan, Rewon Child,
Choquette-Choo, Aakanksha Chowdhery, Cl√©ment
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Crepy, Shachi Dave, Mostafa Dehghani, Sunipa
Clemens Winter, Chris Hesse, Mark Chen, Eric
Dev, Jacob Devlin, Mark D√≠az, Nan Du, Ethan Dyer,
Sigler, Mateusz Litwin, Scott Gray, Benjamin
6Chess, Jack Clark, Christopher Berner, Sam Training Compute-Optimal Large Language
McCandlish, Alec Radford, Ilya Sutskever and Models. arXiv preprint arXiv: 2203.15556.
Dario Amodei. 2020. Language Models are Few-
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Shot Learners. Advances in Neural Information
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang and
Processing Systems 33 (NeurIPS 2020).
Weizhu Chen. 2021. LoRA: Low-Rank Adaptation
Jeffrey Cheng, Marc Marone, Orion Weller, Dawn of Large Language Models. arXiv preprint
Lawrie, Daniel Khashabi, and Benjamin Van arXiv:2106.09685.
Durme. 2024. Dated Data: Tracing Knowledge
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,
Cutoffs in Large Language Models. arXiv preprint
Janghoon Han, Gyeonghun Kim, Stanley Jungkyu
arXiv: 2403.12958.
Choi, and Minjoon Seo. 2022. Towards Continual
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Knowledge Learning of Language Models. arXiv
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi preprint arXiv: 2110.03215.
Wang, Mostafa Dehghani, Siddhartha Brahma,
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom
Albert Webson, Shixiang Shane Gu, Zhuyun Dai,
B. Brown, Benjamin Chess, Rewon Child, Scott
Mirac Suzgun, Xinyun Chen, Aakanksha
Gray, Alec Radford, Jeffrey Wu, Dario Amodei.
Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin
2020. Scaling Laws for Neural Language Models.
Robinson, Dasha Valter, Sharan Narang, Gaurav
arXiv preprint arXiv: 2001.08361
Mishra, Adams Yu, Vincent Zhao, Yanping
Huang,,rew Dai, Hongkun Yu, Slav Petrov, Ed H. Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi,
Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Gyuhak Kim, and Bing Liu. 2023. Continual Pre-
Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling training of Language Models. arXiv preprint arXiv:
Instruction-Finetuned Language Models. arXiv 2302.03241.
preprint arXiv: 2210.11416.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid,
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large
Biderman, Sid Black, Anthony DiPofi, Charles language models are zero-shot reasoners. In
Foster, Laurence Golding, Jeffrey Hsu, Alain Le Advances in Neural Information Processing
Noac'h, Haonan Li, Kyle McDonell, Niklas Systems.
Muennighoff, Chris Ociepa, Jason Phang, Laria
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Reynolds, Hailey Schoelkopf, Aviya Skowron,
Petroni, Vladimir Karpukhin, Naman Goyal,
Lintang Sutawika, Eric Tang, Anish Thite, Ben
Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim
Wang, Kevin Wang, and Andy Zou. 2023. A
Rockt√§schel, Sebastian Riedel and Douwe Kiela.
framework for few-shot language model evaluation.
2020. Retrieval-Augmented Generation for
https://zenodo.org/records/10256836
Knowledge-Intensive NLP Tasks. Advances in
Tianyu Han, Lisa C. Adams, Jens-Michalis Neural Information Processing Systems 33
Papaioannou, Paul Grundmann, Tom Oberhauser, (NeurIPS 2020).
Alexander L√∂ser, Daniel Truhn, and Keno K.
David Lopez-Paz and Marc‚ÄôAurelio Ranzato. 2017.
Bressem. 2023. MedAlpaca -- An Open-Source
Gradient Episodic Memory for Continual Learning.
Collection of Medical Conversational AI Models
Advances in neural information processing systems,
and Training Data. arXiv preprint
30.
arXiv:2304.08247.
Yun Luo, Zhen Yang, Xuefeng Bai, Fandong Meng, Jie
Danny Hernandez, Tom Brown, Tom Conerly, Nova
Zhou, and Yue Zhang. 2023a. Investigating
DasSarma, Dawn Drain, Sheer El-Showk, Nelson
Forgetting in pre-trained representations through
Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan
Continual Learning. arXiv preprint
Hume, Scott Johnston, Ben Mann, Chris Olah,
arXiv:2305.05968.
Catherine Olsson, Dario Amodei, Nicholas Joseph,
Jared Kaplan, and Sam McCandlish. 2022. Scaling Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie
Laws and Interpretability of Learning from Zhou, and Yue Zhang. 2023b. An Empirical Study
Repeated Data. arXiv preprint arXiv:2205.10487. of Catastrophic Forgetting in Large Language
Models During Continual Fine-tuning. arXiv
Jordan Hoffmann, Sebastian Borgeaud, Arthur
preprint arXiv: 2308.08747.
Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Hendricks, Johannes Welbl, Aidan Clark, Tom Sabharwal. ‚ÄúCan a Suit of Armor Conduct
Hennigan, Eric Noland, Katie Millican, George van Electricity? A New Dataset for Open Book Question
den Driessche, Bogdan Damoc, Aurelia Guy, Simon Answering.‚Äù Conference on Empirical Methods in
Osindero, Karen Simonyan, Erich Elsen, Jack W. Natural Language Processing (2018).
Rae, Oriol Vinyals, and Laurent Sifre. 2022.
7Alec Radford, Karthik Narasimhan, Tim Salimans, and Martinet, Todor Mihaylov, Pushkar Mishra, Igor
Ilya Sutskever. 2018. Improving language Molybog, Yixin Nie,,rew Poulton, Jeremy
understanding by generative pre-training. Technical Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
Report. OpenAI. Schelten, Ruan Silva, Eric Michael Smith, Ranjan
Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
Keisuke Sakaguchi, Ronan Le Bras, Chandra
Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Bhagavatula, and Yejin Choi. 2021. WinoGrande:
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela
an adversarial winograd schema challenge at scale.
Fan, Melanie Kambadur, Sharan Narang, Aurelien
Commun. ACM 64, 9 (September 2021), 99‚Äì106.
Rodriguez, Robert Stojnic, Sergey Edunov, and
https://doi.org/10.1145/3474381
Thomas Scialom. 2023. Llama 2: Open Foundation
Noam Shazeer. 2020. Glu variants improve and Fine-Tuned Chat Models. arXiv preprint
transformer. arXiv preprint arXiv:2002.05202 arXiv:2307.09288.
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu,
Jacob R Steeves, Joel Hestness, and Nolan Dey. Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan.
2023 SlimPajama: A 627B token cleaned and 2024. LLaMA Pro: Progressive LLaMA with Block
deduplicated version of RedPajama. Expansion. arXiv preprint arXiv: 2401.02415.
https://www.cerebras.net/blog/slimpajama-a-627b-
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023.
token-cleaned-and-deduplicated-version-of-
Efficient Continual Pre-training for Building
redpajama.
Domain Specific Large Language Models. arXiv
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, preprint arXiv: 2311.08545.
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Hongyang Yang, Xiao-Yang Liu, and Christina Dan
Adam R. Brown, Adam Santoro, Aditya Gupta,
Wang. 2023. FinGPT: Open-Source Financial Large
Adri√† Garriga-Alonso, Agnieszka Kluska, Aitor
Language Models. arXiv preprint arXiv:
Lewkowycz, Akshat Agarwal, Alethea Power, Alex
2302.04023
Ray, Alex Warstadt, Alexander W. Kocurek, Ali
Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Yi Yang, Yixuan Tang and Kar Yan Tam. 2023.
Allen Nie, Aman Hussain, Amanda Askell, Amanda InvestLM: A Large Language Model for Investment
Dsouza, Ambrose Slone, Ameet Rahane, using Financial Domain Instruction Tuning. arXiv
Anantharaman S. Iyer, Anders Andreassen, Andrea preprint arXiv:2309.13064.
Madotto, Andrea Santilli, Andreas Stuhlm√ºller,
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Andrew Dai, Andrew La, Andrew Lampinen, Andy
Farhadi, and Yejin Choi. 2019. HellaSwag: Can a
Zou, Angela Jiang, Angelica Chen, Anh Vuong,
Machine Really Finish Your Sentence?.
Animesh Gupta, et al. 2022. Beyond the imitation
Proceedings of the 57th Annual Meeting of the
game: Quantifying and extrapolating the
Association for Computational Linguistics, 2019.
capabilities of language models. arXiv preprint
arXiv:2206.04615 Biao Zhang and Rico Sennrich. 2019. Root mean
square layer normalization. Advances in Neural
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Information Processing Systems, 32.
Bo Wen, and Yunfeng Liu. 2021. Roformer:
Enhanced transformer with rotary position Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
embedding. arXiv preprint arXiv:2104.09864. Wei Lu. 2024. TinyLlama: An Open-Source Small
Language Model. arXiv preprint arXiv:2401.02385.
Teknium,. 2023.OpenHermes 2.5: An Open Dataset of
Synthetic Data for Generalist LLM Assistants. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
https://huggingface.co/datasets/teknium/OpenHerm Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
es-2.5 Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023. LIMA:
Hugo Touvron, Louis Martin, Kevin Stone, Peter
Less Is More for Alignment. arXiv preprint arXiv:
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
2305.11206.
Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian
Canton Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa, Isabel Kloumann,
Artem Korenev, Punit Singh Koura, Marie-Anne
Lachaux, Thibaut Lavril, Jenya Lee, Diana
Liskovich, Yinghai Lu, Yuning Mao, Xavier
8