Grasper: A Generalist Pursuer for Pursuit-Evasion Problems
PengdengLi∗ ShuxinLi∗ XinrunWang†
NanyangTechnologicalUniversity NanyangTechnologicalUniversity NanyangTechnologicalUniversity
Singapore Singapore Singapore
pengdeng.li@ntu.edu.sg shuxin.li@ntu.edu.sg xinrun.wang@ntu.edu.sg
JakubČerný YouzhiZhang StephenMcAleer
ColumbiaUniversity CAIR,HKISI,CAS CarnegieMellonUniversity
NewYorkCity,UnitedStates HongKong,China Pittsburgh,UnitedStates
cerny@disroot.org youzhi.zhang@cair-cas.org.hk mcaleer.stephen@gmail.com
HauChan BoAn
UniversityofNebraska-Lincoln NanyangTechnologicalUniversity
Lincoln,Nebraska,UnitedStates Singapore
hchan3@unl.edu boan@ntu.edu.sg
ABSTRACT approachforsolvingpursuit-evasionproblemsacrossabroadrange
Pursuit-evasiongames(PEGs)modelinteractionsbetweenateam ofscenarios,enablingpracticaldeploymentinreal-worldsituations.
ofpursuersandanevaderingraph-basedenvironmentssuchas
KEYWORDS
urbanstreetnetworks.Recentadvancementshavedemonstrated
theeffectivenessofthepre-trainingandfine-tuningparadigmin Multi-AgentLearning;Pursuit-EvasionProblems;Generalizability;
Policy-SpaceResponseOracles(PSRO)toimprovescalabilityin Pre-trainingandFine-tuning;Hypernetwork
solvinglarge-scalePEGs.However,thesemethodsprimarilyfo-
ACMReferenceFormat:
cusonspecificPEGswithfixedinitialconditionsthatmayvary PengdengLi∗,ShuxinLi∗,XinrunWang†,JakubČerný,YouzhiZhang,
substantiallyinreal-worldscenarios,whichsignificantlyhinders
StephenMcAleer,HauChan,andBoAn.2024.Grasper:AGeneralistPursuer
theapplicabilityofthetraditionalmethods.Toaddressthisissue, forPursuit-EvasionProblems.InProc.ofthe23rdInternationalConference
weintroduceGrasper,aGeneRAlistpurSuerforPursuit-Evasion onAutonomousAgentsandMultiagentSystems(AAMAS2024),Auckland,
pRoblems,capableofefficientlygeneratingpursuerpoliciestai- NewZealand,May6–10,2024,IFAAMAS,9pages.
loredtospecificPEGs.Ourcontributionsarethreefold:First,we
presentanovelarchitecturethatoffershigh-qualitysolutionsfor 1 INTRODUCTION
diversePEGs,comprisingcriticalcomponentssuchas(i)agraph
Thedeploymentofsecurityresourcestodetect,deter,andcatch
neuralnetwork(GNN)toencodePEGsintohiddenvectors,and
criminals is a critical task in urban security [30, 32]. Statistics
(ii)ahypernetworktogeneratepursuerpoliciesbasedonthese
showthatpolicepursuitsprobably“injureorkillmoreinnocent
hiddenvectors.Asasecondcontribution,wedevelopanefficient
bystandersthananyotherkindofforce”[26].Therefore,itiscrucial
three-stagetrainingmethodinvolving(i)apre-pretrainingstage
tocomeupwithscalableapproachesforeffectivelycoordinating
forlearningrobustPEGrepresentationsthroughself-supervised
varioussecurityresources,ensuringtheswiftapprehensionofa
graphlearningtechniqueslikegraphmaskedauto-encoder(Graph-
fleeingcriminaltominimizeharmandpropertydamage.Duetothe
MAE),(ii)apre-trainingstageutilizingheuristic-guidedmulti-task
adversarialnaturebetweenattackersanddefenders,game-theoretic
pre-training(HMP)whereheuristic-derivedreferencepolicies(e.g.,
modelshavebeenusedtomodelvariousreal-worldurbansecurity
throughDijkstra’salgorithm)regularizepursuerpolicies,and(iii)
scenarios.Inparticular,thepursuit-evasiongame(PEG)hasbeen
afine-tuningstagethatemploysPSROtogeneratepursuerpolicies
extensivelyemployedtomodeltheinteractionsbetweenateam
ondesignatedPEGs.Finally,weperformextensiveexperiments
ofpursuers(e.g.,policeforces)andanevader(e.g.,acriminal)on
onsyntheticandreal-worldmaps,showcasingGrasper’ssignif-
graphs(e.g.,urbanstreetnetworks)[21,38,42,43].Toeffectively
icantsuperiorityoverbaselinesintermsofsolutionqualityand
solvePEGsundervarioussettings,severalmethods,suchascoun-
generalizability.WedemonstratethatGrasperprovidesaversatile
terfactualregretminimization(CFR)[47]andpolicy-spaceresponse
oracles(PSRO)[18],havebeendevelopedintheliterature.Among
∗Equalcontribution.
†Correspondingauthor. thesealgorithms,PSRO,adeepreinforcementlearningalgorithm,
providesaversatileframeworkforlearningthe(approximate)Nash
equilibria(NEs)ofPEGs(refertoSection3.2foranintroduction
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License. ofthePSROframework).Furthermore,recentworkshavealsoin-
tegratedthepre-trainingandfine-tuningparadigmintothePSRO
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems frameworktofurtherenhanceitsscalability[20].
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6–10,2024,
Althoughmanyexistingworkshaveachievedsignificantsuccess,
Auckland,NewZealand.©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org). theyonlyfocusonsolvingspecificPEGswithpredeterminedinitial
4202
rpA
91
]IA.sc[
1v62621.4042:viXraconditions,e.g.,theinitiallocationsofallplayersandexits,andthe incremental strategy generation [42, 43] have been introduced.
timehorizonofthegame.Unfortunately,theseconditionsmayvary Nonetheless,thesemethodsencounterscalabilityissuesasthey
substantiallyinreal-worldscenarios,wherecrimescanoccurat typicallyrelyonlinearprogramming.Ontheotherhand,PEGcan
anylocationinacityandatanytime.Whentheinitialconditions beviewedasaparticulartypeoftwo-playerimperfect-information
change,existingalgorithmsmustsolvethenewPEGfromscratch, extensive-formgame(IIEFG).Thus,thealgorithmsusedforsolv-
which is computationally demanding and time-consuming [20], inglarge-scaleIIEFGs,suchascounterfactualregretminimization
restrictingthereal-worlddeploymentofcurrentalgorithms.Thus, (CFR)[47]andPolicy-SpaceResponseOracles(PSRO)[18],have
thereisanurgentnecessitytodevelopanewapproachcapableof beenappliedtotacklelarge-scalePEGs[21,38].However,when
solvingdifferentPEGswithvaryinginitialconditionseffectively. solvinglarge-scalePEGsusingPSRO,thereexistsignificantcom-
Tothisend,weintroduceGrasper:aGeneRAlistpurSuerfor putationalchallengesasitinvolvescomputingthebestresponse
Pursuit-EvasionpRoblems,whichcaneffectivelysolvedifferent strategymultipletimes.Tomitigatethisissue,recentresearch[20]
PEGsbygeneratingthepursuer’spoliciesconditionalontheinitial integratesthepre-trainingandfine-tuningparadigmintoPSRO
conditionsofthePEGs.Grasperconsistsoftwocriticalcomponents. toimproveitsscalability.Despitethesuccess,allthesealgorithms
First,asthePEGisplayedonagraph,itisnaturaltouseagraph aretailoredtosolveaspecificPEGwithpredeterminedinitialcon-
neuralnetwork(GNN)toencodethePEGwiththegiveninitial ditions.Whentheseconditionschange,theymustrecomputethe
conditions into a hidden vector. Then, inspired by recent work NEstrategyfromscratch(onetotwohoursforaPEGona10×10
ongeneralizationovergameswithdifferentpopulationsizes[19], gridmap[20]),whichhinderstheirreal-worldapplicability1.To
weintroduceahypernetworktogeneratethebasepolicyforthe addressthislimitation,weproposeGrasper,whichusesPSROto
pursuerconditionalonthehiddenvectorobtainedbytheGNN. computetheNEstrategyandcangeneratedifferentpursuers’strate-
Thisgeneratedbasepolicythenservesasastartingpointforthe giesfordifferentPEGsbasedontheirinitialconditionswithout
pursuer’sbestresponsepolicytrainingateachPSROiteration. recomputingtheNEstrategyfromscratch.
TotrainthenetworksofGrasper,wefindthatnaivelyapplying The generalizability of algorithms and models over different
multi-tasktraining[44]isinefficient.Furthermore,jointlytraining gameshasgainedincreasingattentionandremarkableprogresshas
theGNNandhypernetworkcouldbetime-consumingastheGNNis beenachievedinrecentresearch.Neuralequilibriumapproximators
onlyusedtoencodetheinitialconditionswhicharefixedduringthe thatdirectlypredicttheequilibriumstrategyfromgamepayoffsin
gameplaying.Toaddressthesechallenges,weproposeanefficient normal-formgames(NFGs)havebeentheoreticallyprovenPAC
three-stagetrainingmethodtotrainthenetworksofGrasper.First, learnable[7,8]andareabletogeneralizetodifferentgameswith
weintroduceapre-pretrainingstagetotraintheGNNbyusing desirablesolutionquality[7–9,24].However,itremainsunder-
self-supervisedgraphlearningmethodssuchasGraphMAE[15]. exploredwhengoingbeyondNFGs.Inthiswork,wemakethefirst
Second,wefixtheGNNandpre-trainthehypernetworkbyusing attempttoconsiderthegeneralizationprobleminthedomainof
amulti-tasktrainingprocedurewherethetrainingdataissampled PEGs,atypeofgamethathasawiderangeofreal-worldapplica-
fromdifferentPEGswithdifferentinitialconditions.Inthisstage, tions[14,22]andisfarmorecomplicatedthanNFGs.Wepropose
toovercomethelowexplorationefficiencyduetothepursuers’ anovelalgorithmicframeworkthatisabletoefficientlysolvedif-
random exploration and the evader’s rationality, we propose a ferentPEGswithvaryinginitialconditionsanddemonstratethe
heuristic-guidedmulti-taskpre-training(HMP)whereareference generalizationabilitythroughextensiveexperiments.
policyderivedbyheuristicmethodssuchasDijkstraisusedto Ourworkisalsorelatedtoself-supervisedgraphlearningand
regularizethepursuerpolicy.Finally,wefollowthePSROprocedure multi-tasklearning.Recentworkshaveshownthatgenerativeself-
andobtainthepursuer’sbestresponsepolicyateachiterationby supervisedlearning[13]canbeappliedtographlearningandout-
fine-tuningthebasepolicygeneratedbythehypernetwork. performcontrastivemethodswhichrequirecomplextrainingstrate-
Insummary,weprovidethreecontributions.First,wepropose gies[25,31],high-qualitydataaugmentation[41],andnegative
Grasperwhichisthefirstgeneralizableframeworkcapableofef- samplesthatareoftenchallengingtoconstructfromgraphs[46].
ficientlyprovidinghighlyqualifiedsolutionsfordifferentPEGs Therefore,weemploytherecentstate-of-the-art,GraphMAE[15],
withdifferentinitialconditions.Second,toefficientlytrainthenet- tolearnagoodrepresentationofaPEGwiththegiveninitialcon-
worksofGrasper,weproposeathree-stagetrainingmethod:(i) ditions.Multi-tasklearning[27,44]hasbeenappliedtovarious
apre-pretrainingstagetotraintheGNNthroughGraphMAE,(ii) domainsincludingnaturallanguageprocessing[6],computervi-
apre-trainingstagetotrainthehypernetworkthroughheuristic- sion[11],andreinforcementlearning(multi-taskRL)[34,36,40,45].
guidedmulti-taskpre-training(HMP),and(iii)afine-tuningstage Duetoitsstronggeneralizability,weemploymulti-taskRLforthe
toobtainthepursuer’sbestresponsepolicyateachPSROitera- pre-trainingprocess,enablingthepre-trainedpolicytobequickly
tion.Finally,weperformextensiveexperiments,andtheresults fine-tunedforefficientpolicydevelopmentinnewtasks.
demonstratethesuperiorityofGrasperoverdifferentbaselines. Finally,ourworkisrelatedtothemulti-agentpatrollingproblem
wheretheevaderoftenanticipatespatrollingstrategiesandmay
chooseatargetorasinglepathasanaction[2,4,16,29].Conversely,
2 RELATEDWORK
ourpursuit-evasiongamefeaturessimultaneousactionswiththe
Pursuit-evasiongames(PEGs)havebeenextensivelyappliedto evaderunawareofthepursuer’sreal-timelocations.
model various real-world problems such as security and robot-
1NotethatclassicalheuristicalgorithmssuchasDijkstraarealsolessapplicableowing
ics[3,14,17,22,23,33,35].ToefficientlysolvePEGsanddiffer-
tothisreason.Moreover,itislessmeaningfultoassumethatthereisatleastone
ent variants, many algorithms such as value iteration [14] and pursuerateachexitasthepursuer’sresourcesaretypicallylimited[32].3 PROBLEMFORMULATION employHigh-LevelActionsfortheevader.Thatis,theevaderonly
Inthissection,wefirstpresentalltheelementsfordefiningthe choosestheexitnodetoescapefromandthensamplesoneshortest
PEGs.Then,wepresentthestate-of-the-art(SOTA)methodfor pathfromtheinitiallocationtothechosenexitnode,insteadof
solvingPEGs.Finally,wegivetheproblemstatementofthiswork. decidingwheretogointhenexttimestep.Specifically,attimestep
𝑡 =0,theevaderdeterminesanexitnode𝑣′ ∈𝑉′usingthepolicy
3.1 Preliminaries 𝜋𝑒 :𝑉 →Δ(𝑉′),samplesashortestpathfrom𝑙𝑒 ∈𝑉 tothechosen
0
exitnode𝑣′,andthentakesactionsbasedonthepath.
Apursuit-evasiongame(PEG)isatwo-playergameplayedbetween
apursuerandanevader,i.e.,𝑁 ={𝑝,𝑒}.Followingpreviousworks Here,wegivesomeremarksontheassumptionofHigh-Level
[21, 38, 43], we assume that the pursuer comprises𝑛 members Actions of the evader. (i) In our game setting, the evader lacks
denotedas𝑝 = {1,2,...,𝑛},andthepursuercanobtainthereal- real-timeaccesstothepursuers’locations,requiringtheevaderto
actwithoutanyinformationabouttheirwhereabouts.Therefore,
timelocationinformationoftheevaderwiththehelpoftracking
samplingonepathfortheevaderwouldnotlosemuchinformation
devices.Inreality,PEGsaretypicallyplayedonurbanroadmaps,
whichcanberepresentedbyagraph𝐺 = (𝑉,𝐸),where𝑉 isthe comparedwiththecasewheretheevaderactsstepbystep.(ii)
setofverticesand𝐸 isthesetofedges.Let𝑉′ ⊂ 𝑉 denotethe Trainingthepursueragainstanevaderwhochoosestheshortest
set of exit nodes from which the evader can escape and𝑇 the path,aworst-casescenarioforthepursuer,enhancestherobustness
predeterminedtimehorizonofthegame.At𝑡 ≤𝑇,thelocations ofthepursuer’spolicy.(iii)Thoughitisasimplification,theproblem
oftheevaderandpursueraredenotedby𝑙 𝑡𝑒 and𝑙 𝑡𝑝 =(𝑙 𝑡1,𝑙 𝑡2,...,𝑙 𝑡𝑛), s pe latt yin erg s’re inm ia tii an ls ch oi ng dh il ty ioc no sm ,ep nle lx ard gu ine gto thth ee tam su kl sti pp al ce eex foit rs ta hn ed pd ui rv se ur ese
r,
respectively.Then,thehistoryofthegameat𝑡 isasequenceof
pastlocationsofbothplayers,i.e.,ℎ = (𝑙 0𝑒,𝑙 0𝑝 ,...,𝑙 𝑡𝑒 −1,𝑙 𝑡𝑝 −1).The asd Inet sa uil med mi an ryth ,ge iI vn et nro ad gu rc at pio hn 𝐺an wd itA hp tp he en sd pi ex cA ifiQ cs2 e.
tofexitnodes
a
t
{hv (𝑙ea 1i ,pl 𝑙a
l
2b
a
,yl .e
.e
.,ra 𝑙’c 𝑛st )i co
|u
𝑙n 𝑖rrs ∈ee nt Ntfo
l
(or
𝑙
𝑡c
𝑖b −ao
t
1t
i
)h
o ,n
∀p
,
𝑖la
i.
∈y ee
.,
𝑝r 𝐴s }𝑒i ws (ℎt hh
)
ee
r=
ene
N
Nig
(
(h
𝑙
𝑣𝑡𝑒b )−o
1
dr
)
ein nag
n
otdv ee s𝐴rt
𝑝
ti hc
(
ee ℎs
)
so
e=
tf 𝑉
p
(𝐺r′ e,
,d
𝑉t eh ′te
,e
𝑙ri 𝑝n
m
,i 𝑙it 𝑒nia
,e
𝑇l dl )o
t
.ic
Im
nat
e
tio hhn eos
r
Pio
z
Eof Gnt ,h
𝑇
pe
,
lap
w
yu
e
er rs
c
su
an
wer
id
la
e
ln
fi
gd
n
etee tv
a
ha
s
ed pe ner oci( nfi𝑙 -0𝑝
c
z, eP𝑙 r0𝑒
E
o) G, ra ean wsd aGt rh d=e
s
ofneighboringverticesofvertex𝑣.Accordingtothedefinitionof 0 0
only when the game is terminated. The termination conditions
history,wedefinetheinformationsetforeachplayerastheset
includethreecases:(i)thepursuercatchestheevaderwithinthe
c
t th
ho
e
ens
p
ei vus at ri
s
dn
u
eg
e
rro
i’
ssf di rn
e
ed
a
fili n-s tt eii
m
dng
e
au sli
o
𝐼s 𝑒ch aa =tb iol {e
n
ℎh
|i
ℎnis
f
=t oo
r
(r
m
𝑙i 0𝑒e
a
,s
t
𝑙.
i
0𝑝oA ,ns
𝑙,
1𝑒t
t
,h
h
∗e
e
,.e
i .n
.v
,f
𝑙a
o
𝑡𝑒d −re
m
1r
,a
∗c
t
)a
i
}on ,nn wo st
het
eg roe et
f
t
a
ti hnm eee
gx
ah
it
mo nr ei oz rdo een acw𝑇 hi,
et
si h.e
i tn
h., e𝑙 t𝑡𝑒
h tie
m∈
t
ei𝑙 m𝑡𝑝 h,
e
o𝑡
rh
i≤
zo or
n𝑇 iz; 𝑇on( .ii L𝑇) e,t th
i
𝑡.e
e
′.e
,
≤v
𝑙
𝑡𝑒a 𝑇d
∈
ber e𝑉e
t′
hs ,c e𝑡a tp
≤
imes
𝑇
ef
;
sr (o
ti
em
i pi)
∗ PEr Gep ir se ase sn imts ua ltn ay nep oo us ss -i mbl oe velo gc aa mtio e,n wo ef ct ah ne mp ou drs eu lie tr a. sA al nth eo xu tg enh sit vh ee
-
t Fh oa rt 𝑡th =e 𝑡g ′a ,m ine cis ast ee srm (ii )n aa nte dd (. iiT i)h ,e tn h, efo pr ura sl ul e𝑡 r< rec𝑡 e′, iv𝑟 e𝑡𝑝
s
=
a
r𝑟 e𝑡𝑒 w= ard0.
𝐼f t
c
e𝑝o h
u
vr e arm
=r
dp
e
eu
n
{g rr
ℎ
’ta ss |m au
ℎ lc
oee tr
=
ci( aocE
tn
(o iF
𝑙
om
.
0𝑒G
nT
,m)
𝑙h
b0𝑝b i
e
u,ty s
.
t.pa
.
nw ,us
𝑙
o𝑡s ri
𝑒
tt −su h
u
t1m o
he
,u
𝑙
ei
r
𝑡𝑝n ’t
−
esg a v1in ,t anh ∗y
df
)a
o
ei }t rrn ’mt sf sh o
ina
ce r
t
ucm ie
e
rov a
rn
eta t hi nd o
s e
te enr
at
pa ca
uc
tbc
a r
iot osns u
u
nfi bt
e
.rer ths kdt e nea e ofin v wnd a
e
sdt dh e tre ha’n ess 𝑟
𝑟t
hh𝑡 𝑡𝑝
𝑝
ae
v==
eev
𝑉−1
a
𝑝1dw
.
(e
𝜋Grh 𝑝ii
g
vl ,ae
𝑣ei
′nnt )h
s
te
=ha
ee Erv
e
e
(cid:2)a
w
xd
(cid:205)ia
te
𝑇
𝑡rr
n
=d
o
0in
𝑟
d
𝑟𝑡𝑒c
𝑡e
𝑝u
(cid:3)=
cr hs
fo1
oa
, rsa
ep
tn
n
he
d
en ba
pt
yhl ut tey
rh
sp
e𝑟 uu𝑡𝑒
eer
rvs=
au
ad
ne−
er
dr1
s
𝑉. 𝑣uI 𝑒′ffn
(e
∼
𝜋rc 𝑝sa
𝜋
,s
a
𝑒
𝑣e
,
′l
)o( wi si =e) s,
Abehaviorpolicyofaplayerassignsaprobabilitydistribution E(cid:2)(cid:205)𝑇 𝑡=0𝑟 𝑡𝑒(cid:3) fortheevader,wheretheexpectationistakenover
overtheactionsetforeveryinformationsetbelongingtotheplayer.
thetrajectoriesinducedby𝜋𝑝 .Then,forthepolicypair(𝜋𝑝,𝜋𝑒),
Noticethatthepursuer’sactionspaceiscombinatorialandexpands we have𝑉𝑝(𝜋𝑝,𝜋𝑒) = E 𝑣′∼𝜋𝑒(cid:2)𝑉𝑝(𝜋𝑝,𝑣′)(cid:3) for the pursuer and
exponentiallywiththenumberofpursuermembers.Asaresult,
𝑉𝑒(𝜋𝑝,𝜋𝑒)=E 𝑣′∼𝜋𝑒(cid:2)𝑉𝑒(𝜋𝑝,𝑣′)(cid:3)
fortheevader.
directlylearningajointpolicyofthepursuermemberswouldbe
computationallydifficult.Toaddressthisissue,insteadoflearning 3.2 Policy-SpaceResponseOracles
ajointpolicy,previousworkslearntheindividualpolicieseither
throughvaluedecomposition[21]orglobalcritic[20],whichare
theparadigmofcentralizedtrainingwithdecentralizedexecution Algorithm1:PSROforaspecificPEGG
(CTDE)forthepursuers.Furthermore,previousworks[20]also 1 Π𝑝 0 ={𝜋 0𝑝 },Π𝑒 0 ={𝜋 0𝑒},𝑈 0,𝜎 0𝑝 ,𝜎 0𝑒 ;
introduceanewstaterepresentationignoringthegame’shistorical 2 forepochk=1,2,···𝐾 do
𝑜wi sn
e
𝑡𝑖ef
r
A
=o vfr o
a
tm (tl
e
𝑙l
i
𝑡𝑝o
o
aa ,w cnt 𝑙i
hs
𝑡𝑒o t ,n
a
th 𝑖in, ,me 𝑡dw )p eih r
∈n
sei tdc v
e
Oh ii pvo 𝑖l iu ,𝑡e
d
,
wsa
u
eld y
a
has
l
icm
cp
ht ho oe pn
l
ii
i
num ct cri io sp
e
luun sr do ee
f
erv od se
r
mcd ato
e
lh
lnp
me
pve
bp
lr e aef n uo yrt
r
er i
s
gm o
ru
e
sn
e
t
’a s srn cst
a
uc .o ne rr. d
o
eI e nn bfi tso n
e
lou e
r
cr vt ah aw te
t
io oio
o
nr bk
n
s-,
,
63
4
5
UC
C
Exo
o
ppm
m
da
ap
p
n
tu
u
esit
t
moe
e
n
et
t
:h
h
taΠe
e -𝑘𝑝
ge pv
au
=
ma rd
s
Π
eue 𝑘𝑝er m’ −rs
’
a1sB tr∪BR
ixR
{p 𝑈𝜋po
𝑘𝑝
𝑘oli }lc
ti
,
hy
c Πy
r𝜋 o𝑘𝑒𝜋𝑘𝑒 u𝑘𝑝
=
ga hg
a
Πa
g
s𝑘𝑒i
a
in m−ins
1
ut
s
∪
lt𝜎 a𝑘 𝜎𝑝
t{
i− 𝑘𝑒
𝜋
o−1
𝑘
n𝑒;
1
}
;;
;
t
m
ph
re
oe
m
bi ad
b be
io
r
lif
t𝑖
yt ch
o
de
n
issp
tt
ru
r
iburs ucu
t ts
ie or
a
nm
p oo
ve lm
ei
rcb tye h2r e,
𝜋
aa
𝑝
cn td
:
ioOt nh 𝑖e
s→
et tim
Δ
𝐴e
( 𝑖𝐴
(𝑜s 𝑖t 𝑡𝑖)e ),p
w
=. hE Nia cc
(h
𝑙h
𝑡𝑖a
)p
s
,su ∀igr 𝑖s
n
∈u se 𝑝ar
.
87 RetC uro nm :p Πu
𝑝
𝐾te ,𝜎 Π𝑘𝑝
𝑒
𝐾a ,n
𝜎
𝐾d
𝑝
,𝜎 𝜎𝑘𝑒 𝐾𝑒usingameta-solveron𝑈 𝑘;
Asfortheevader’spolicy,wefollowpreviousworks[37,38]that
2Allthepursuermembersshareonepolicy.Astheobservationsincludepursuers’ids, Asoneofthepopularalgorithms,PSRO[18]canbeemployed
differentpursuermemberscanhavedifferentbehaviors[10].Δdenotesthesimplex. tosolveaPEGG,showninAlgorithm1.Itcommenceswitheach(a) Graphical Rep. (b) Pre-pretraining (c) Pre-training(HMP) (d) Fine-tunning
h T Hypernetwork Solve
Pooling Meta Strategy M
Rep. Layer BR Simulate
GNN ⋯ 𝑜 𝑡𝑖 𝜽 𝝅 Oracle Policy Space
Expand
Evader Pursuer Exit Trained via GraphMAE
Figure1:ArchitectureandtrainingpipelineofGrasper.
playerusingarandompolicy(Line1)andthenexpandsthepolicy suchanissueasthebasepolicyispre-trainedunderthepremise
spacesofthepursuerandevaderinaniterativemanner.Ateach thattheinitialconditionofthePEGisfixed.Inotherwords,anew
epoch1≤𝑘 ≤𝐾:(1)Computethebestresponse(BR)policiesof basepolicymustbepre-trainedfromscratchforthemodifiedPEG
thepursuer𝜋𝑝 andevader𝜋𝑒
andaddthemtotheirpolicyspaces sincetheoriginalbasepolicymaynotbeagoodstartingpointfor
𝑘 𝑘
Π 𝑘𝑝 and Π 𝑘𝑒 (Line 3–5); (2) Construct a meta-game𝑈 𝑘 using all thepursuer’sBRpolicyinthemodifiedgame(evenworsethana
randomlyinitializedBRpolicy).Inthispaper,weaimtoaddressthis
policies in each player’s policy space (Line 6); (3) Compute the
meta-strategyofthepursuer𝜎𝑝 ∈Δ(Π𝑝 )andevader𝜎𝑒 ∈Δ(Π𝑒) issuebydevelopingageneralistpursuercapableoflearningand
𝑘 𝑘 𝑘 𝑘
usingameta-solver(e.g.,PRD[18])onthemeta-game𝑈 𝑘 (Line7). adaptingtodifferentPEGswithvaryinginitialconditionswithout
Theseprocessesarerepeatedfor𝐾epochsandthenoutputthefinal theneedtorestartthetrainingprocessfromthebeginning.
meta-strategyacrosstheplayers’policyspaces(Line9).
4 GRASPER
As the evader’s policy is a probability distribution over exit
nodes,tocomputetheBRpolicy(Line3),weonlyneedtoestimate Inthissection,weintroduceGrasper,illustratedinFigure1.Wefirst
the value of each exit node through simulations,
i.e.,𝑉𝑒(𝑣′)
= presentthearchitectureofGrasperincludingseveralinnovative
E 𝜋𝑝∼𝜎𝑝 (cid:2)𝑉𝑒(𝜋𝑝,𝑣′)(cid:3) ,∀𝑣′ ∈ 𝑉′.Then,theevader’sBRpolicyis components,andthenthetrainingpipelinewhichconsistsofthree
𝑘−1
constructedbyapplyingsoftmaxoperationonthevaluesofallthe stagestoefficientlytrainthenetworksofGrasper.
exitnodes.Forthepursuer,computingtheBRpolicyistosolve
theproblem𝜋 𝑘𝑝 = argmax𝜋𝑝E 𝜋𝑒∼𝜎 𝑘𝑒 −1(cid:2)𝑉𝑝(𝜋𝑝,𝜋𝑒)(cid:3) (Line4).As 4.1 Architecture
therearemultiplepursuermembers,wecanuseMAPPO[39]to 4.1.1 GraphicalRepresentationsofPEGs. Togeneratethepur-
learntheBRpolicy.InthetraditionalPSROalgorithm,thepursuer’s suer’spolicybasedonthespecificPEG,weproposetotakethe
BRpolicyislearnedfromscratch,i.e.,theBRpolicyisrandomly specificPEGasaninputofaneuralnetwork.Tothisend,weencode
initialized,whichisinefficient.Toaddressthisissue,recentworks theinitialconditionsofaPEGexceptfor𝑇 intoagraph(Figure1(a)).
integratethepre-trainingandfine-tuningparadigmintoPSROto Thetimehorizon𝑇 canbedirectlyfedintotheneuralnetwork.
improvelearningefficiency[20].Specifically,beforerunningPSRO, Specifically,givenaPEGG = (𝐺,𝑉′,𝑙𝑝 ,𝑙𝑒,𝑇),theseinitialcondi-
abasepursuerpolicyistrainedthroughmulti-taskRLwhereeach tions𝑉′,𝑙𝑝 and𝑙𝑒 canbeencodedinto0 the0 graph𝐺 byassociating
taskisgeneratedwitharandomlyinitializedevader’spolicy.Then, 0 0
eachnodeofthegraphwithavectorconsistingofthefollowing
ateachPSROepoch,thepursuer’sBRpolicyisinitializedwiththe parts:(i)abinarybit{0,1}where1indicatesthatthenodeisan
pre-trainedbasepolicy,ratherthanlearningfromscratch,which exit,(ii)abinarybit{0,1}where1signifiesthattheevader’sini-
canlargelyimprovethelearningefficiencyofthePSROalgorithm.
tiallocationisthisnode,(iii)thenumberofpursuersonthisnode
{0,...,𝑛}(thetotalnumberofpursuersacrossallnodesequalsto
3.3 ProblemStatement
𝑛),and(iv)additionalinformationregardingthetopologyofthe
AlthoughPSROhasbeensuccessfullyappliedtosolvePEGs,un- graph,suchasthedegreeofthenode.Thisprovidesauniversal
fortunately,previousworkstypicallyfocusonsolvingaspecific representationofanyPEGwithanyinitialcondition.
PEGwithpredeterminedinitialconditionswhicharenotalways
fixedinreal-worldscenarios:(i)Theinitiallocationsofthepursuers 4.1.2 Game-conditionalBasePoliciesGeneration. Afterrep-
andtheevader(𝑙𝑝 ,𝑙𝑒
)arenotalwaysfixedsinceattacks(thieves, resentingaPEGasagraph,itisnaturaltoleverageagraphneural
0 0
crimes,terrorists)canoccuratanytimeandlocationinacity;(ii) network(GNN)toencodethePEGwiththegiveninitialconditions
Thelocationsoftheexitnodes𝑉′maychangeduetotemporary intoahiddenvector.AsshowninFigure1(b),wefirstfeedthe
closuresandopenings;(iii)Thetimehorizon𝑇 mightvary,asthe graphicalrepresentationoftheinitialconditionsintotheGNNand
timerequiredtopursuetheevaderisnotalwaysthesame.When gettherepresentationsofallthenodesofthegraph.Then,weuse
anyoftheinitialconditionschange,thePEGadaptsaccordingly. apoolingoperationtointegrateallthenoderepresentationsintoa
Asaconsequence,currentalgorithmscanonlysolvethemodified hiddenvectorwhichwillbeconcatenatedwiththetimehorizon𝑇
PEGfromscratch,leadingtosignificanttimeconsumptionandinef- togetthefinalrepresentationofthePEG.Next,togenerateabase
ficiency.EventheSOTAmethodpresentedintheprevioussection policyconditionalonthePEG,weintroduceahypernetwork[12],
–PSROwithpre-trainedbasepursuerpolicy–stillsuffersfrom aneuralnetworkthattakesthefinalrepresentationofthePEGasinputandoutputstheparameters(weightsandbiases)ofthepolicy issolelyemployedtoderivetheeffectiverepresentationfromthe
network(Figure1(c)).Finally,thebasepolicynetworkservesasa PEG’sgraphicalinterpretation,weintroduceapre-pretrainingstage
startingpointforthetrainingofthepursuer’sbestresponsepolicy (Figure1(b))topre-traintheGNNbeforetheactualpre-training
ineachiterationofthePSROalgorithm(Figure1(d)). stage.Thisapproachismoreefficientcomparedtojointlytraining
theGNNandhypernetworkinthepre-trainingstage.Specifically,
4.1.3 ObservationRepresentationLayer. Asdescribedearlier, foreachgameinthetrainingsetG ∈I,let𝑨Gand𝑿Gdenotethe
thepursuer’spolicyisamappingthatassociateseachobservation
adjacencymatrixandfeaturematrixoftheunderlyinggraph,respec-
withaprobabilitydistributionovertheavailableactionset.Notably, tively.Wefirstobtainthelatentcodematrix𝑯G =𝑓 GNN(𝑿G,𝑨G)
anobservationconsistsofthepositionsofbothplayers.Represent-
bytheGNNandtraintheGNNviaanyself-supervisedgraphlearn-
ingthesepositionsbymereindexnumbersofverticesinthegraph
ingmethod(weusetherecentSOTAmethod,GraphMAE[15]).
does not provide much useful information for training, though.
Then,wegetthehiddenvectorbypoolingthelatentcodematrix
Therefore,weseekamorecompactandmeaningfulrepresenta- 𝒉G =𝑝𝑜𝑜𝑙(𝑯G),whichwillbefedintothehypernetwork.
tionoftheseobservations.Previousworks[20,38]typicallytrain
anodeembeddingmodelforthispurpose.Unfortunately,sucha
Algorithm2:Pre-training
modelisoftentailoredandtrainedforaspecificgraph,limitingits
generalizabilitytoothergraphs.Thislackofgeneralizabilitymakes 1
InitializeGrasperandtheepisodebufferD ←∅;
thismethodunsuitableforourproblem.Toaddressthisissue,we 2 fortrainepoch=1,2,··· do
adoptarepresentationlayertoencodethepursuer’sobservations, 3
Uniformlysample𝑐 1gamesfromthetrainingsetI;
anapproachthatisnotlimitedtoaspecificgraph. 4 foreachofthe𝑐 1gamesGdo
AsgiveninSection3.1,thepursuer’sobservation𝑜 𝑡𝑖 =(𝑙 𝑡𝑝 ,𝑙 𝑡𝑒,𝑖,𝑡) 5 Randomlygenerate𝑐 2evader’spolicies;
includesthreeparts:theplayers’currentlocations(𝑙 𝑡𝑝 ,𝑙 𝑡𝑒),thepur- 6 Generatepursuer’spolicy𝜋 𝜽𝑝 ←Grasper(G);
suermember’sid𝑖,andthecurrenttimestep𝑡.Thus,therepre- 7 foreachofthe𝑐 2evader’spolicies𝜋𝑒 do
sentationlayerconsistsofthreecomponents,eachofwhichisa 8 Sampledatausing𝜋𝑒 ,𝜋 𝜽𝑝 ,and𝜋ˆ𝑝 ;
“torch.nn.Embedding”whichhasbeenextensivelyusedtoencode
9
AddthedataintotheepisodebufferD;
anintegertoacompactrepresentation.Theoutputsofthethree
10
Trainthenetworksbyoptimizingthelossfunction𝐿;
componentsareconcatenatedtoobtaintherepresentationofthe
11
CleartheepisodebufferD ←∅;
pursuer’sobservation.Thisrepresentationlayerwillbetrained
jointly with the hypernetwork during the pre-training process.
PleaserefertoAppendixBfordetailsonthearchitectureofthe 4.2.2 StageII:Pre-training. Givenafixedevader’spolicyina
representationlayer,theGNN,andthehypernetwork. specificPEG,computingthepursuer’sbestresponsepolicycanbe
Intuitively,thegeneralizationabilityofGrasperbenefitsfrom seenasanRLtask.Thus,wecanapplythemulti-taskRLalgorithm
severaldesignsofourarchitecture.First,thegraphicalrepresen- toguidethepre-trainingprocess,whichisshowninAlgorithm2.
tationoffersauniversalrepresentationofanyPEG,regardlessof Differentfrompreviouswork[20],intheseRLtasks,exceptfor
thegraph’stopology.Second,GNNcanencodedifferentPEGsinto thechangeintheevader’spolicy,thegame’sinitialconditionsalso
fixed-sizehiddenvectors,whichcanbedirectlyfedintothehy- change.ToobtaintheseRLtasksforpre-training,wefirstrandomly
pernetwork(otherwise,additionaltechniquesarerequiredifthe sample𝑐 1gamesfromthetrainingset(Line3),andthenforeach
sizesofthehiddenvectorsarevaried).Finally,thehypernetworkis game,werandomlysample𝑐 2evader’spolicies(Line5).Oncethe
designedtogenerateaspecializedpolicytailoredtoagivenPEG. gameandtheevader’spolicyarefixed,theRLtaskisgenerated.
Duringpre-training,foreachgame,wefirstfeedthehiddenvector
4.2 TrainingPipeline ofthegame(obtainedbythetrainedGNN)andthetimehorizon
NowweintroducethetrainingpipelineofGrasper,whichinvolves intothehypernetworktogeneratethepursuer’sbasepolicy,and
threestages:pre-pretraining,pre-training,andfine-tuning.Priorto thenforeachevader’spolicy,wecollectthetrainingdatausing
delvingintothespecifics,wefirstdescribehowthetrainingsetis thepursuer’sbasepolicy(accompanybytherepresentationlayer)
generated.ThetrainingsetIshouldconsistofdifferentPEGsfor intotheepisodebuffer.Finally,wetrainthehypernetworkand
training.Tothisend,wegeneratethetrainingsetbyrandomizing therepresentationlayerjointlybasedontheepisodebuffer(Lines
theinitialconditions,denotedby
(𝐺,𝑉′,𝑙𝑝 ,𝑙𝑒,𝑇).However,this 6-9).Todealwiththemultiplepursuermemberscases,weemploy
0 0
MAPPO[39]astheunderlyingRLalgorithm.
approachmayyieldcertaingamesthatlackmeaningfultraining
However,wefoundthatsimplyapplyingtheMAPPOunderthe
value.Forexample,whentheevader’sinitiallocationisinsuchclose
multi-tasklearningframeworkcanresultinlowefficiencydueto
proximitytotheexitnodesthatthepursuerbecomesincapableof
randomexplorationintheenvironment.Toclarify,considerthe
capturingtheevaderregardlessofitsmovements.Toexcludethese
exampleillustratedinFigure2,whichshowstheneedforamore
trivialcases,weintroduceafilterconditionwhengeneratingthe
efficientpre-trainingmethod.Assumethattheevader’spolicyisto
trainingset:theshortestpathfromtheevader’sinitiallocationto
taketheshortestpathtooneoftheexits(denotedbytheredpath).If
anyexitnodesmustexceedapredeterminedlength.
thepursuerexplorestheenvironmentrandomly(theorangepath),
4.2.1 StageI:Pre-pretraining. Asthehypernetworktakesafea- itwillprobablylosethegameandthenreceiveanegativereward.
turevectorasinput,wefirstuseaGNNtoencodethegraphicalrep- Thissituationcanoccurfrequentlyatthebeginningofthepre-
resentationofthePEGintoafixed-sizehiddenvector.AstheGNN trainingprocessbecausethepursuer’sinitialpolicyisinvariablyrandom.Tomitigatethisexplorationinefficiency3,weproposea computationofthepursuer’sBRpolicy(Line5),ratherthantraining
novelscheme:heuristic-guidedmulti-taskpre-training(HMP). fromscratch.Thisallowsustosimplyfine-tunethepre-trained
𝑝
policy𝜋 overafewepisodes(Line6)toquicklyobtaintheBR
0
Succeed! policy,significantlyenhancingthelearningefficiency.
𝒓𝒑=+𝟏
Reference
Policy Algorithm3:Fine-tuning
Guided 1 Require:Grasper,PEGG;
Exploration 2 Π𝑝 0 ={𝜋 0𝑝 ←Grasper(G)},Π𝑒 0 ={𝜋 0𝑒},𝑈 0,𝜎 0𝑝 ,𝜎 0𝑒 ;
3 forepochk=1,2,···𝐾 do
ExR pa ln od rao tm
io n
𝒓F 𝒑a =ile −d 𝟏! 4
5
C Ino itm iap liu zt ee tt hh ee pe uv ra sd ue er r’ ’s sB BR Rp po ol li ic cy y𝜋 𝜋𝑘𝑒 𝑘𝑝a ←gai 𝜋n 0s 𝑝t ;𝜎 𝑘𝑝 −1;
Evader Pursuer Exit 76 ET xra pi an n𝜋 si𝑘𝑝 ona :g Πai 𝑘𝑝ns =t𝜎 Π𝑘𝑒 𝑘𝑝− −1 1f ∪or {f 𝜋e 𝑘w 𝑝 }e ,p Πi 𝑘𝑒so =de Πs; 𝑘𝑒 −1∪{𝜋 𝑘𝑒};
8 Updatemeta-gamematrix𝑈 𝑘 throughsimulation;
Figure2:IllustrationofHMP. 9 Compute𝜎 𝑘𝑝 and𝜎 𝑘𝑒 usingameta-solveron𝑈 𝑘;
10
Return:Π𝑝 𝐾,Π𝑒 𝐾,𝜎 𝐾𝑝 ,𝜎 𝐾𝑒
NotethatintheRLtasksusedforpre-training,wecanacquire
theevader’spolicy,whichcanbeusedtoguidetheexploration
ofthepursuer’spolicy.Specifically,giventheexitnodechosen
bytheevader’spolicy𝜋𝑒 ,wefirstinduceareferencepolicy𝜋ˆ𝑝 5 EXPERIMENTS
(represented by the green path) for the pursuer using heuristic Inthissection,weperformexperimentstoevaluatetheperformance
methodssuchastheDijkstraalgorithm.Then,apartfromtheac- ofGrasperandtheeffectivenessofdifferentcomponents4.
𝑝
tionssampledbythegeneratedpolicy𝜋 (Line6),wealsosample
thereferenceactionsusingthereferen𝜽 cepolicy𝜋ˆ𝑝
andaddthe
5.1 Setup
datatothetrainingbuffer(Lines8-9).Let𝐿(𝜽)denotetheoriginal Hyperparameters.Thenumberofpursuersis𝑛=5,thenumber
lossfunctionfortrainingtheactorintheMAPPOalgorithm.The ofexitnodesis8,thetimehorizon𝑇 is6≤𝑇 ≤10,andthenumber
HMPisimplementedbyintroducinganadditionallossintothe ofpre-trainingepisodesis20million(20M).ForPSRO,thenumber
originallossfunction:𝐿 = 𝐿(𝜽)+𝛼KL(𝜋𝑝∥𝜋ˆ𝑝) where𝛼 ∈ [0,1] ofepisodesusedfortrainingthebestresponseis10.Weconduct
controlstheweightoftheguidanceofthereferencepolicyand experiments on four maps: the grid map with size 10×10, the
KLrepresentstheKullback–Leiblerdivergence(forthereference scale-freegraphwith300nodes,theSingaporemap[38]with372
policy𝜋ˆ𝑝
,theactionprobabilitydistributionisobtainedbysetting nodes,andtheScotland-Yardmap[28]with200nodes.Tosimulate
theprobabilityofthereferenceactionto1whileallothersto0). thesituationwherearoadmightbetemporallyblockeddueto
congestionortrafficaccidents,wesettheprobabilityofanedge
4.2.3 StageIII:Fine-tuning. Inthisphase,weintegratethepre-
betweentwonodesto0.8forthegridmap,0.9fortheSingapore
trainedpursuerpolicyintothePSROframework,asshowninAl-
gorithm3.Thepursuer’spolicy𝜋𝑝
isinitializedusingtheoutput
map,and1.0(i.e.,nocongestion)fortheothertwomaps.More
0 detailsonthehyperparameterscanbefoundinAppendixB.
neuralnetworkfromthepre-trainedGrasper,whichtakesthegraph-
Worst-caseUtility.GiventhataPEGisazero-sumgame,weuse
icalrepresentationofthespecificPEGasaninput.Simultaneously,
theevader’spolicy𝜋𝑒
israndomlyinitialized(Line2).Thenwe
thepursuer’sworst-caseutility(astheevaderalwayschoosesthe
f ro el sl po ow nsth ee (Bst Ra )n pd oa lr id ciP eS s0 R foO rf br oa tm he pw lao yrk er: sin ,𝜋e 𝑘𝑝ac ah ndite 𝜋r 𝑘𝑒a ,ti ao rn e𝑘 co,t mh pe ub te es dt s t inhh neo er qt re u es a xt l pp it ea y cth to aff tr ito ohm ne it s sh o te l aui kn t eii o nti na o:l v𝑢l eo 𝑝 rca t= hti eo En t𝜋 rt a𝑝o j∼ et 𝜎 ch 𝑝 te o,𝜋c r𝑒 ih e∼o s𝜎s 𝑒 ie nEn d[e u𝑟x c𝑝i et ] d) ,t w bo yhm e 𝜋e r 𝑝a es atu nhr dee
u ths ein ng adth de ei dr tr oes tp he ec pti ov le icyBR seo tsra Πc 𝑝le as n(L di Πne 𝑒s (L4- in6) e. 7T )h ,ae nse dB thR ep mo eli tc ai -e gs aa mre
e
𝜋𝑒 whicharerespectivelysampledaccordingto𝜎𝑝 and𝜎𝑒
.
𝑘 𝑘 TrainingandTestSets.(1)Wegenerate |I| = 1000gamesas
matrix𝑈 𝑘 isupdatedthroughsimulation(Line8).Finally,themeta
distribution(𝜎𝑝 ,𝜎𝑒)iscomputedusinganymeta-solver(Line9). thetrainingset.Duringthegeneration,theminimumlengthof
𝑘 𝑘 theevader’sshortestpathissetto6forthegridmapand5for
TheBRoraclesforbothplayersaretheimportantcomponents
othermaps.(2)Wecreatetwotestsets,I 1andI 2,eachcontaining
ofthePSROalgorithm.Theevader’sBRoraclefollowsthestandard
30games.(i)I 1includesthegamessampledfromthetrainingset
PSROalgorithmgiveninAlgorithm1.Thekeydifferencebetween
I 1 ⊂I(in-distributiontest).(ii)I 2containsthegamesdistinctfrom
ourfine-tuningprocessandthestandardPSROalgorithmliesin
thetrainingsetI 2∩I=∅(out-of-distributiontest).Toavoidtrivial
thetrainingofthepursuer’sBRpolicy,whichishighlightedin
𝑝 cases(thegamesthatareeithertoodifficultortoosimpleforthe
blue.Specifically,giventhepre-trainedpolicy𝜋 conditionalto
0 pursuers),weconstrainthezero-shotperformanceofGrasper(i.e.,
theinitialconditions,wecanuseitasthestartingpointforthe
theworst-caseutilityofthegeneratedpolicywithoutfine-tuning)
3NoticethatmanyexplorationmethodsinRLsuchasRND[5]typicallyencourage withintherange:[0.8,0.9]forI 1and[0.1,0.2]forI 2.
thepolicytoexplorenovelstatesoftheenvironment,whicharedifferentfromour
designwhererandomexplorationislessfavored. 4Thecodeisavailableathttps://github.com/IpadLi/Grasper. * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P  * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P
                   
     1      2      1      2
                   
                   
                   
                   
                   
                   
                   
                                                                                   
 5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V 
(a)GridMap (b)Scale-FreeMap
 * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P  * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P
                   
     1      2      1      2
                   
                   
                   
                   
                   
                   
                   
                                                                                       
 5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V 
(c)SingaporeMap (d)Scotland-YardMap
Figure3:Evaluationperformance.Theshadedarearepresentsthestandarderror.
Baselines. (i) Multi-task PSRO (MT-PSRO): the state-of-the-art bealsopartlyverifiedbycomparingMT-PSROandMT-PSRO-Aug
(SOTA)approachadaptedfrom[20],whichalsousestheobserva- wheretheirperformanceiscomparable,meaningthatnaivelyinte-
tionrepresentationlayerandHMP.(ii)MT-PSROwithaugmen- gratingtheinformationabouttheinitialconditionsdoesnotbring
tation(MT-PSRO-Aug):thehiddenvectorobtainedfromthepre- muchbenefitandnoveldesignsarenecessary.(iii)Aninteresting
trainedGNNandthetimehorizonareconcatenatedtotheoutput resultisthatevenonthetestsetI 1(in-distributiontest),MT-PSRO
oftheobservationrepresentationlayer.(iii)PSRO:thestandard andMT-PSRO-Aug,thestrongestbaselines,performworseonall
PSROmethod.(iv)Random:thepursuerrandomlyselectsactions. theothermapsthanonthegridmap.Wehypothesizethereason
isthattheothermapsaremoreheterogeneousthanthegridmap.
5.2 Results Forexample,thedegreeofthenodesvariesfrom1to16intheSin-
TheexperimentalresultsaresummarizedinFigure3.The𝑥-axisis gaporemapwhileitremainsbetween2to4inthegridmap.Thus,
therunningtime.Forthepurposeofafaircomparison,apartfrom thegamesgeneratedontheSingaporemapsharemuchlesssimi-
therunningtimeofthefine-tuningstage(thePSROprocedure),we larity.Inthissense,theinformationabouttheinitialconditionsis
alsoincludetherunningtimeofpre-pretrainingandpre-training particularlyimportantwhensolvingdifferentPEGs.(iv)Inallcases,
(called the pre-training time for convenience). Since the games theperformanceofGrasperismuchmorestablethanthebaselines
inthetrainingsetareuniformlyrandomlysampledduringpre- (smallerstandarderror)asGraspercangeneratedistinctpoliciesfor
training,weapproximatethepre-trainingtimeofeachgameby differentPEGs.Incontrast,otherbaselineseitherentirelyignore
averagingthetotalpre-trainingtimeoverthetrainingset.Then, ornaivelyintegratetheinformationabouttheinitialconditions
foreachtestinggame,weaddthepre-trainingtimetotherunning ofthePEGs,whichrendersthemhardtogeneralizetodifferent
time(thehorizontalgapbetween0andthestartoftheline).Note PEGs,leadingtolargerperformancevariancethanGrasper.(v)The
thattheamortizedpre-trainingtimeforGrasper,MT-PSRO,and resultsonthetestsetI 2showthatGraspercansolveunseengames,
MT-PSRO-Augissimilarsincethepre-pretrainingtimeisveryshort exhibitingbettergeneralizabilitythanthebaselines.
(Table2).Fromtheresults,wecandrawseveralconclusions.
5.3 Ablations
(i)Givenafixednumberofepisodesforthefine-tuningprocess,
Graspercanstartfromandconvergetoahigheraverageworst-case EffectivenessofDifferentModules.First,westudythecontri-
utilitythanthebaselines,althoughittakesacertainpre-training butionofHMPandtheobservationrepresentationlayer(Rep.)to
time,demonstratingtheeffectivenessofthepre-pretrainingandpre- theperformanceofGrasper,asshowninTable1.Theresultsshow
traininginacceleratingthePSROprocedure.NotethatMT-PSRO thatwecangetbetterperformance(highworst-caseutilityand
andMT-PSRO-Augalsoemploypre-pretrainingorpre-training,but smallstandarderror)onlywhencombiningthetwocomponents,
theyperformworsethanGrasper,showcasingthesuperiorityof meaningthatbothtwocomponentsareindispensableforGrasper.
Grasper.(ii)Forafaircomparison,MT-PSRO-Augalsointegrates EffectivenessofPre-pretraining.Next,weinvestigatetheef-
theinformationabouttheinitialconditionsofthePEGs.Theresults fectivenessofthepre-pretrainingstageinacceleratingthewhole
clearlyshowthenecessityofthehypernetworkinGrasper.Thiscan trainingprocedureofGrasper.SincejointlytrainingGNNandthe
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :Table1:Ablationstudies.Theresultsareobtainedinthegrid isnearthetwoexits,itcouldbeeasyforthepursuertocatchthe
map.✓meansthemoduleisused.
evader,eventhoughthereisonlyonepursuerinthisarea.There-
sultsreflecttheintuitionthatGraspercangeneratedistinctpolicies
HMP ✓ ✓ fordifferentgamesandhence,theperformanceismorestable.
I 1 Rep. ✓ ✓
Utility 0.90±0.01 −0.54±0.06 −0.05±0.17 −0.52±0.08
 
           
 
           
HMP ✓ ✓                            
I 2 Rep. ✓ ✓              
         
Utility 0.45±0.04 −0.60±0.06 −0.64±0.11 −0.63±0.06              
         
   
         
                   
 [  [
otherpartsofGrasperfor20Mpre-trainingepisodesrequiresa
longrunningtime,inthisablationstudy,wefocusonthefirst2M
Figure5:Zero-shotworst-caseutilityofthepursuerforeach
pre-trainingepisodesandcomparetherunningtimeofGrasper
possibleevader’sinitiallocation.Reddotsareexitsandblue
withpre-pretraining(w/PP)andwithoutpre-pretraining(w/oPP).
dotsarepursuers’initiallocations.
ThetrainingcurvesareshowninFigure4,whichshowsthatusing
pre-pretrainingcansignificantlyacceleratethetrainingprocedure
(3.9timesfasterthanwithoutusingpre-pretraining).
6 CONCLUSIONS
Inthiswork,weinvestigatehowtoefficientlysolvedifferentPEGs
        [
withvaryinginitialconditions.First,weproposeanovelgener-
alizableframework,Grasper,whichincludesseveralcriticalcom-
    ponents:(i)aGNNtoencodeaspecificPEGintoahiddenvector,
(ii)ahypernetworktogeneratethebasepoliciesforthepursuers
    conditionalonthehiddenvectorandtimehorizon,(iii)anobserva-
tionrepresentationlayertoencodethepursuers’observationsinto
 Z  R  3 3  Z   3 3
compactandmeaningfulrepresentations.Second,weintroduce
   
anefficientthree-stagetrainingmethodwhichincludes:(i)apre-
                          pretrainingstagethatlearnsrobustPEGrepresentationsthrough
 7 L P H   V  GraphMAE,(ii)aheuristic-guidedmulti-taskpre-trainingstage
thatleveragesareferencepolicyderivedfromheuristicmethods
suchasDijkstratoregularizepursuerpolicies,and(iii)afine-tuning
Figure4:Pre-trainingcurves.
stagethatutilizesPSROtogeneratepursuerpoliciesondesignated
PEGs.Finally,extensiveexperimentsdemonstratethesuperiority
Thequantitativevaluesoftherunningtimeofthepre-pretraining ofGrasperoverbaselinesintermsofsolutionqualityandgeneral-
andpre-trainingaregiveninTable2.Asthepre-pretrainingtime izability.Tothebestofourknowledge,thisisthefirstattemptto
(304.2seconds)ismuchshorterthanthepre-trainingtime(9954.9 considerthegeneralizationprobleminthedomainofPEGs.Future
seconds), the curves of Grasper, MT-PSRO, and MT-PSRO-Aug directionsinclude(i)moreefficienttasksamplingstrategiesfor
showninFigure3startfromasimilarpositioninthe𝑥-axis.
pre-training,e.g.,AdA[1],(ii)amodelcapableofgeneralizingto
differentPEGswithdifferentunderlyinggraphtopologies,e.g.,gen-
Table2:Runningtime(second). eralizingfromgridmapstoscale-freemaps,and(iii)amodelcapable
oftacklingmorecomplexsettings,e.g.,learning-basedevader.
w/oPP w/PP
ACKNOWLEDGMENTS
Pre-pretraining N/A 304.2
ThisworkissupportedbytheNationalResearchFoundation,Singa-
Pre-training 39977.3 9954.9
poreunderitsIndustryAlignmentFund–Pre-positioning(IAF-PP)
Total 39977.3 10259.1(3.9x) FundingInitiative.Anyopinions,findingsandconclusions,orrec-
ommendationsexpressedinthismaterialarethoseoftheauthor(s)
InfluenceofEvader’sInitialLocation.Weperformsomeex- anddonotreflecttheviewsofNationalResearchFoundation,Sin-
perimentsusingGraspertoprovidesomeinsightsintothePEG. gapore.YouzhiZhangissupportedbytheInnoHKFund.HauChan
InFigure5,wepresentthepursuer’sutilitywhentheevaderran- issupportedbytheNationalInstituteofGeneralMedicalSciences
domizestheinitiallocationoverthegridmap.Wefoundthatin oftheNationalInstitutesofHealth[P20GM130461],theRuralDrug
someareasthepursuerscanhavehighutility.Forexample,inthe AddictionResearchCenterattheUniversityofNebraska-Lincoln,
top-rightoftheleftfigure,therearethreepursuersandonlyone andtheNationalScienceFoundationundergrantIIS:RI#2302999.
exit,whichmeansitcouldbehardfortheevadertoescape.Inthe Thecontentissolelytheresponsibilityoftheauthorsanddoesnot
bottom-rightoftherightfigure,asthepursuer’sinitiallocation necessarilyrepresenttheofficialviewsofthefundingagencies.
 \ W L O L W 8  J Q L Q L D U 7
 \
 \ W L O L W 8  H V D F  W V U R :
 \
 \ W L O L W 8  H V D F  W V U R :REFERENCES
neuralequilibriumsolvers.InNeurIPS.5586–5600.
[1] AdaptiveAgentTeam,JakobBauer,KateBaumli,SatinderBaveja,FeryalBe- [25] JiezhongQiu,QibinChen,YuxiaoDong,JingZhang,HongxiaYang,MingDing,
hbahani,AvishkarBhoopchand,NathalieBradley-Schmieg,MichaelChang,Na- KuansanWang,andJieTang.2020.GCC:Graphcontrastivecodingforgraph
talieClay,AdrianCollister,VibhavariDasagi,LucyGonzalez,KarolGregor, neuralnetworkpre-training.InSIGKDD.1150–1160.
EdwardHughes,SheleemKashem,MariaLoks-Thompson,HannahOpenshaw, [26] FrederickPRivaraandChristopherDMack.2004.Motorvehiclecrashdeaths
JackParker-Holder,ShreyaPathak,NicolasPerez-Nieves,NemanjaRakicevic, relatedtopolicepursuitsintheUnitedStates. InjuryPrevention10,2(2004),
TimRocktäschel,YannickSchroecker,JakubSygnowski,KarlTuyls,SarahYork, 93–95.
AlexanderZacherl,andLeiZhang.2023. Human-timescaleadaptationinan [27] SebastianRuder.2017.Anoverviewofmulti-tasklearningindeepneuralnet-
open-endedtaskspace.arXivpreprintarXiv:2301.07608(2023). works.arXivpreprintarXiv:1706.05098(2017).
[28] MartinSchmid,MatejMoravčík,NeilBurch,RudolfKadlec,JoshDavidson,
[2] NoaAgmon,GalAKaminka,andSaritKraus.2011. Multi-robotadversarial
KevinWaugh,NolanBard,FinbarrTimbers,MarcLanctot,GZachariasHol-
patrolling:facingafull-knowledgeopponent. JournalofArtificialIntelligence
land,DavoodiDavoodi,AldenChristianson,andMichaelBowling.2023.Student
Research42(2011),887–916.
ofGames:aunifiedlearningalgorithmforbothperfectandimperfectinformation
[3] ShaunakDBopardikar,FrancescoBullo,andJoaoPHespanha.2008.Ondiscrete-
games.ScienceAdvances9,46(2023),eadg3256.
timepursuit-evasiongameswithsensinglimitations.IEEETransactionsonRo-
[29] EfratSless,NoaAgmon,andSaritKraus.2014.Multi-robotadversarialpatrolling:
botics24,6(2008),1429–1439.
facingcoordinatedattacks.InAAMAS.1093–1100.
[4] JanBuermannandJieZhang.2022.Multi-robotadversarialpatrollingstrategies
[30] MilindTambe.2011.SecurityandGameTheory:Algorithms,DeployedSystems,
vialatticepaths.ArtificialIntelligence311(2022),103769.
LessonsLearned.CambridgeUniversityPress.
[5] YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov.2018.Exploration
[31] ShantanuThakoor,CorentinTallec,MohammadGheshlaghiAzar,MehdiAzabou,
byrandomnetworkdistillation.InICLR.
EvaLDyer,RemiMunos,PetarVeličković,andMichalValko.2022.Large-scale
[6] RonanCollobertandJasonWeston.2008. Aunifiedarchitecturefornatural
representationlearningongraphsviabootstrapping.InICLR.
languageprocessing:Deepneuralnetworkswithmultitasklearning.InICML.
[32] JasonTsai,ZhengyuYin,Jun-youngKwak,DavidKempe,ChristopherKiek-
160–167.
intveld,andMilindTambe.2010.Urbansecurity:Game-theoreticresourceallo-
[7] ZhijianDuan,WenhanHuang,DinghuaiZhang,YaliDu,JunWang,Yaodong
cationinnetworkeddomains.InAAAI.881–886.
Yang,andXiaotieDeng.2023.IsNashequilibriumapproximatorlearnable?.In
[33] ReneVidal,OmidShakernia,HJinKim,DavidHyunchulShim,andShankar
AAMAS.233–241.
Sastry.2002.Probabilisticpursuit-evasiongames:Theory,implementation,and
[8] ZhijianDuan,YunxuanMa,andXiaotieDeng.2023.Areequivariantequilibrium
experimentalevaluation. IEEETransactionsonRoboticsandAutomation18,5
approximatorsbeneficial?arXivpreprintarXiv:2301.11481(2023).
(2002),662–669.
[9] XidongFeng,OliverSlumbers,ZiyuWan,BoLiu,StephenMcAleer,YingWen,
[34] Tung-LongVuong,Do-VanNguyen,Tai-LongNguyen,Cong-MinhBui,Hai-
JunWang,andYaodongYang.2021.Neuralauto-curriculaintwo-playerzero-sum
DangKieu,Viet-CuongTa,Quoc-LongTran,andThanh-HaLe.2019.Sharing
games.InNeurIPS.3504–3517.
experienceinmultitaskreinforcementlearning.InIJCAI.3642–3648.
[10] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,and
[35] YuandaWang,LuDong,andChangyinSun.2020.Cooperativecontrolformulti-
ShimonWhiteson.2018.Counterfactualmulti-agentpolicygradients.InAAAI.
playerpursuit-evasiongameswithreinforcementlearning.Neurocomputing412
2974–2982.
(2020),101–114.
[11] RossGirshick.2015.FastR-CNN.InICCV.1440–1448.
[36] AaronWilson,AlanFern,SoumyaRay,andPrasadTadepalli.2007.Multi-task
[12] DavidHa,AndrewM.Dai,andQuocV.Le.2017.HyperNetworks.InICLR.
reinforcementlearning:AhierarchicalBayesianapproach.InICML.1015–1022.
[13] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.
[37] WanqiXue,BoAn,andChaiKiatYeo.2022.NSGZero:efficientlylearningnon-
2022.Maskedautoencodersarescalablevisionlearners.InCVPR.16000–16009.
exploitablepolicyinlarge-scalenetworksecuritygameswithneuralMonteCarlo
[14] KarelHorákandBranislavBošansky`.2017.Dynamicprogrammingforone-sided
treesearch.InAAAI.4646–4653.
partiallyobservablepursuit-evasiongames.InICAART.503–510.
[38] WanqiXue,YouzhiZhang,ShuxinLi,XinrunWang,BoAn,andChaiKiatYeo.
[15] ZhenyuHou,XiaoLiu,YukuoCen,YuxiaoDong,HongxiaYang,ChunjieWang,
2021. Solvinglarge-scaleextensive-formnetworksecuritygamesvianeural
andJieTang.2022.GraphMAE:self-supervisedmaskedgraphautoencoders.In
fictitiousself-play.InIJCAI.3713–3720.
KDD.594–604.
[39] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,
[16] LiHuang,MengChuZhou,KuangrongHao,andEdwinHou.2019.Asurveyof
andYiWu.2022.ThesurprisingeffectivenessofPPOincooperativemulti-agent
multi-robotregularandadversarialpatrolling.IEEE/CAAJournalofAutomatica
games.InNeurIPSDatasetsandBenchmarksTrack.24611–24624.
Sinica6,4(2019),894–903.
[40] SihanZeng,MalikAqeelAnwar,ThinhTDoan,ArijitRaychowdhury,andJustin
[17] LinanHuangandQuanyanZhu.2021.Adynamicgameframeworkforrational
Romberg.2021.Adecentralizedpolicygradientapproachtomulti-taskreinforce-
andpersistentrobotdeceptionwithanapplicationtodeceptivepursuit-evasion.
mentlearning.InUAI.1002–1012.
IEEETransactionsonAutomationScienceandEngineering19,4(2021),2918–2932.
[41] HengruiZhang,QitianWu,JunchiYan,DavidWipf,andPhilipSYu.2021.
[18] MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,Karl
Fromcanonicalcorrelationanalysistoself-supervisedgraphneuralnetworks.In
Tuyls,JulienPérolat,DavidSilver,andThoreGraepel.2017. Aunifiedgame-
NeurIPS.76–89.
theoreticapproachtomultiagentreinforcementlearning.InNeurIPS.4190–4203.
[42] YouzhiZhang,BoAn,LongTran-Thanh,ZhenWang,JiaruiGan,andNicholasR
[19] PengdengLi,XinrunWang,ShuxinLi,HauChan,andBoAn.2023.Population-
Jennings.2017.Optimalescapeinterdictionontransportationnetworks.InIJCAI.
size-awarepolicyoptimizationformean-fieldgames.InICLR.
3936–3944.
[20] ShuxinLi,XinrunWang,YouzhiZhang,WanqiXue,JakubČerný,andBoAn.
[43] YouzhiZhang,QingyuGuo,BoAn,LongTran-Thanh,andNicholasRJennings.
2023.Solvinglarge-scalepursuit-evasiongamesusingpre-trainedstrategies.In
2019.Optimalinterdictionofurbancriminalswiththeaidofreal-timeinforma-
AAAI.11586–11594.
tion.InAAAI.1262–1269.
[21] ShuxinLi,YouzhiZhang,XinrunWang,WanqiXue,andBoAn.2021.CFR-MIX:
[44] YuZhangandQiangYang.2021.Asurveyonmulti-tasklearning.IEEETransac-
Solvingimperfectinformationextensive-formgameswithcombinatorialaction
tionsonKnowledgeandDataEngineering34,12(2021),5586–5609.
space.InIJCAI.3663–3669.
[45] MandiZhao,PieterAbbeel,andStephenJames.2022. Ontheeffectivenessof
[22] XiuxianLi,MinMeng,YiguangHong,andJieChen.2022.Asurveyofdecision
fine-tuningversusmeta-reinforcementlearning.InNeurIPS.26519–26531.
makinginadversarialgames.arXivpreprintarXiv:2207.07971(2022).
[46] YanqiaoZhu,YichenXu,FengYu,QiangLiu,ShuWu,andLiangWang.2021.
[23] VictorGLopez,FrankLLewis,YanWan,EdgarNSanchez,andLinglingFan.
Graphcontrastivelearningwithadaptiveaugmentation.InWWW.2069–2080.
2019.Solutionsformultiagentpursuit-evasiongamesoncommunicationgraphs:
[47] MartinZinkevich,MichaelJohanson,MichaelBowling,andCarmeloPiccione.
Finite-timecaptureandasymptoticbehaviors.IEEETransactionsonAutomatic
2008.Regretminimizationingameswithincompleteinformation.InNeurIPS.
Control65,5(2019),1911–1923.
1729–1736.
[24] LukeMarris,IanGemp,ThomasAnthony,AndreaTacchetti,SiqiLiu,andKarl
Tuyls.2022.Turbochargingsolutionconcepts:SolvingNEs,CEsandCCEswith