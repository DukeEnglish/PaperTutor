Sample-efficient Learning of Infinite-horizon Average-reward
MDPs with General Function Approximation
Jianliang He∗ Han Zhong† Zhuoran Yang‡
Abstract
We study infinite-horizon average-reward Markov decision processes (AMDPs) in the con-
text of general function approximation. Specifically, we propose a novel algorithmic framework
namedLocal-fittedOptimizationwithOPtimism(Loop),whichincorporatesbothmodel-based
and value-based incarnations. In particular, Loop features a novel construction of confidence
sets and a low-switching policy updating scheme, which are tailored to the average-rewardand
function approximation setting. Moreover, for AMDPs, we propose a novel complexity mea-
sure — average-rewardgeneralized eluder coefficient (AGEC) — which captures the challenge
of exploration in AMDPs with general function approximation. Such a complexity measure
encompasses almost all previously known tractable AMDP models, such as linear AMDPs and
linear mixture AMDPs, and also includes newly identified cases such as kernel AMDPs and
AMDPs with Bellman eluder dimensions. Using AGEC, we prove that Loop achieves a sub-
linear ˜(poly(d,sp(V∗ ))√Tβ) regret, where d and β correspond to AGEC and log-covering
O ∗
number of the hypothesis class respectively, sp(V ) is the span of the optimal state bias func-
tion, T denotes the number of steps, and ˜() omits logarithmic factors. When specialized to
O ·
concrete AMDP models, our regret bounds are comparable to those established by the existing
algorithmsdesignedspecificallyforthesespecialcases. Tothebestofourknowledge,thispaper
presents the first comprehensive theoretical framework capable of handling nearly all AMDPs.
Contents
1 Introduction 3
1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 Preliminaries 6
3 General Function Approximation 7
3.1 Average-Reward Generalized Eluder Coefficients . . . . . . . . . . . . . . . . . . . . . 8
3.2 Relation with Tractable Complexity Metric . . . . . . . . . . . . . . . . . . . . . . . 10
4 Local-fitted Optimization with Optimism 11
∗Department of Statistics and Data Science, Fudan University. Email: hejl20@fudan.edu.cn.
†Center for Data Science, Peking University. Email: hanzhong@stu.pku.edu.cn.
‡Department of Statistics and Data Science, Yale University. Email: zhuoran.yang@yale.edu.
1
4202
rpA
91
]GL.sc[
1v84621.4042:viXra5 Proof Overview of Regret Analysis 12
6 Conclusion 14
A Technical Novelties and Further Discussions 19
B Alternative Choices of Discrepancy Function 20
C Concrete Examples 21
C.1 Linear Function Approximation and Variants . . . . . . . . . . . . . . . . . . . . . . 21
C.2 Kernel Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.3 Linear Mixture AMDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D Proof of Main Results for Loop 24
D.1 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.2 Proof of Lemma D.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.3 Proof of Lemma D.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D.4 Proof of Lemma D.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E Proof of Results about Complexity Measures 30
E.1 Proof of Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E.2 Proof of Lemma 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
F Proof of Results for Concrete Examples 32
F.1 Proof of Proposition C.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
F.2 Proof of Proposition C.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
F.3 Proof of Proposition C.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
F.4 Proof of Proposition C.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
F.5 Discussion about Performance on Concrete Examples . . . . . . . . . . . . . . . . . . 36
G Technical Lemmas 37
G.1 Proof of Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
G.1.1 Proof of Lemma G.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
G.1.2 Proof of Lemma G.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
G.1.3 Proof of Lemma G.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
H Supplementary Discussions 41
H.1 Proof Sketch of MLE-based Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
H.2 Extended Value Iteration (EVI) for Model-Based Hypotheses . . . . . . . . . . . . . 43
21 Introduction
Reinforcement learning (RL)(Sutton and Barto,2018)is apowerful toolforaddressingintricate se-
quential decision-making problems. In this context, Markov decision processes (MDPs) (Puterman,
2014; Sutton and Barto, 2018) frequently serve as a fundamental model for modeling such decision-
making scenarios. Motivated by different feedback structures in real applications, MDPs consist
of three subclasses — finite-horizon MDPs, infinite-horizon discounted MDPs, and infinite-horizon
average-reward MDPs. Each of these MDP variants is of paramount significance and operates
in a parallel fashion, with none being amenable to complete reduction into another. Of these
three MDP subclasses, finite-horizon MDPs have received significant research efforts in understand-
ing their exploration challenge, especially in the presence of large state spaces which necessitates
function approximation tools. Existing works on finite-horizon MDPs have proposed numerous
structural conditions on the MDP model that empower sample-efficient learning. These struc-
tural conditions include but are not limited to linear function approximation (Jin et al., 2020),
Bellman rank (Jiang et al., 2017), eluder dimension (Wang et al., 2020), Bellman eluder dimen-
sion (Jin et al., 2021), bilinear class (Du et al., 2021), decision estimation coefficient (Foster et al.,
2021), and generalized eluder coefficient (Zhong et al., 2022). Moreover, these works have designed
various model-based and value-based algorithms to address finite-horizon MDPs governed by these
structural conditions.
In contrast to the rich literature devoted to finite-horizon MDPs, the study of sample-efficient
exploration in infinite-horizon MDPs has hitherto been relatively limited. Importantly, it remains
elusive how to design in a principled fashion a sample-efficient RL algorithm in the online setting
withgeneralfunctionapproximation. Tothisend,wefocusoninfinite-horizonaverage-rewardMDPs
(AMDPs), which offer a suitable framework for addressing real-world decision-making scenarios
that prioritize long-term returns, such as product delivery (Proper and Tadepalli, 2006). Our work
endeavors to provide a unified theoretical foundation for understanding infinite-horizon average-
reward MDPs from the perspective of general function approximation, akin to the comprehensive
investigationsconductedinthedomainoffinite-horizonMDPs. Topursuethisoverarchingobjective,
we have delineated two subsidiary goals that form the crux of our research endeavor.
- Development of a Novel Structural Condition/Complexity Measure. Existing works
are restricted to tabular AMDPs (Bartlett and Tewari, 2012; Jaksch et al., 2010) and AMDPs
with linear function approximation (Wu et al., 2022; Wei et al., 2021), with Chen et al. (2022a)
as the only exception (to our best knowledge). While Chen et al. (2022b) does extend the eluder
dimension for finite-horizon MDPs (Ayoub et al., 2020) to the infinite-horizon average-reward
context, their complexity measure seems to be only slightly more general than the linear mixture
AMDPs (Wu et al., 2022) and falls short in capturing other fundamental models such as linear
AMDPs (Wei et al., 2021). Hence, our first subgoal is proposing a new structural condition. This
conditionisenvisionedtobesufficientlyversatiletoencompassallknowntractableAMDPs,while
also potentially introducing innovative and tractable models into the framework.
- Algorithmic Framework for Addressing Identified Structural Condition. The second
subgoal is anchored in the development of sample-efficient algorithms for AMDPs characterized
by the structural condition proposed in our work. Our aspiration is to devise an algorithmic
framework that can be flexibly implemented in both model-based and value-based paradigms,
dependingonthenatureoftheproblemathand. Thisadaptabilityguaranteesthatouralgorithms
possess the ability to effectively address a wide range of AMDPs.
Our work attains these two pivotal subgoals through the introduction of (i) a novel complex-
3Linear
Algorithm Assumption Type Tabular Linear Eluder ABE Kernel AGEC
Mixture
Loop (Ours) Bellmanoptimality Model-based X X X X X X X
(finitespan) &Value-based
Sim-to-Real CommunicatingAMDP
Model-based X X ✗ X ✗ ✗ ✗
(Chenetal.,2022a) (finitediameter)
Ucrl2-Vtr CommunicatingAMDP
Model-based X X ✗ ✗ ✗ ✗ ✗
(Wuetal.,2022) (finitediameter)
Fopo Bellmanoptimality
Value-based X ✗ X ✗ ✗ ✗ ✗
(Weietal.,2021) (finitespan)
Ucrl2 CommunicatingAMDP
Model-based X ✗ ✗ ✗ ✗ ✗ ✗
(Jakschetal.,2010) (finitediameter)
Table 1: A comparison with the most related algorithms on AMDPs. We remark that our assump-
tion is weaker since the communicating MDP satisfies the Bellman optimality and the diameter
is bound by the span. Besides, average-reward Bellman eluder dimension (ABE), kernel AMDPs,
and AGEC are new complexity measures proposed by our work. In particular, AGEC serves as a
unifying complexity measure capable of encompassing all other established complexity measures.
ity measure — Average-reward Generalized Eluder Coefficient (AGEC), and (ii) a corresponding
algorithmic framework dubbed as Local-fitted Optimization with OPtimism (Loop). Our primary
contributions and novelties are summarized below:
- AGEC Complexity Measure. Our complexity measure AGEC extends the generalized eluder
coefficient (GEC) in Zhong et al. (2022) to the infinite-horizon average-reward setting. How-
ever, it incorporates significant modifications. AGEC not only establishes a connection between
the Bellman error and the training error, akin to GEC but also imposes certain constraints on
transferability (see Definition 3 for details). This modification proves instrumental in attaining
sample efficiency in the realm of AMDPs (see Section 5 for detailed discussion). We demonstrate
that AGEC not only encompasses all previously recognized tractable AMDPs, including tabular
AMDPs (Bartlett and Tewari, 2012; Jaksch et al., 2010), linear AMDPs (Wei et al., 2021), linear
mixture MDPs (Wu et al., 2022), AMDPs with low eluder dimension (Chen et al., 2022a), but
also captures some new identified models like linear Q∗/V∗ AMDPs (see Definition 14), kernel
AMDPs(seePropositionC.3),andAMDPswithlowBellmaneluderdimension(seeDefinition 8).
- Loop Algorithmic Framework. Our algorithm Loop is based on an optimism principle and
features a novel construction of confidence sets along with a low-switching updating scheme.
Remarkably, Loop offers the flexibility to be implemented either in the model-based or value-
based paradigm, depending on the problem type.
- Unified Theoretical Results. From the theoretical side, we prove that Loop enjoys the regret
of ˜(poly(d,sp(V∗))√Tβ), where d and β correspond to the AGEC and the log-covering number
O
of the hypothesis class respectively, sp(V∗) denotes the span of the optimal state bias function, T
is the number of steps, and ˜ hides logarithmic factors. This result shows that Loop is capable
O
of solving all AMDPs with low AGEC.
In summary, we provide a unified theoretical understanding of infinite-horizon AMDPs with
general function approximation. Further elaboration on our contributions and technical novelties
are provided in Appendix A.
41.1 Related Work
Infinite-horizonAverage-rewardMDPs. PioneeringworksbyAuer et al.(2008)andBartlett and Tewari
(2012) laid foundation for model-based algorithms operating within the online framework with sub-
linear regret. In recent years, the pursuit of improved regret guarantees has led to the emergence of
a multitude of new algorithms. In tabular case, these advancements include numerous model-based
approaches (Ouyang et al., 2017; Fruit et al., 2018; Zhang and Ji, 2019; Ortner, 2020) and model-
free algorithms (Abbasi-Yadkori et al., 2019; Wei et al., 2020; Hao et al., 2021; Lazic et al., 2021;
Zhang and Xie, 2023). In the context of function approximation, Politex (Abbasi-Yadkori et al.,
2019),avariantoftheregularizedpolicyiteration, isthefirstmodel-freealgorithmwithlinearvalue-
function approximation, and achieves
˜(T43
) regret for the ergodic MDP. The work by Hao et al.
O
(2021) followed the same setting and improved the results to
˜(T2
3) with an adaptive approximate
O
policy iteration (Aapi) algorithm. Wei et al. (2021) proposed an optimistic Q-learning algorithm
Fopo for the linear function approximation, and achieve a near-optimal ˜(√T) regret. On another
O
line of research, Wu et al. (2022) delved into the linear function approximation under the frame-
work of linear mixture model, which is mutually uncoverable concerning linear MDPs (Wei et al.,
2021), and proposed Ucrl2-Vtr based on the value-targeted regression (Ayoub et al., 2020). Re-
cent work of Chen et al. (2022a) expanded the scope of research by addressing the general function
approximation problem in average-reward RL and proposed the Sim-to-Real algorithm, which
can be regarded as an extension to Ucrl2-Vtr. In comparison to the works mentioned, our al-
gorithm, Loop, not only addresses all the problems examined in those studies but also extends its
applicability to newly identified models. See Table 1 for a summary.
Function Approximation in Finite-horizon MDPs. In the pursuit of developing sample-
efficient algorithms capable of handling large state spaces, extensive research efforts have converged
onthelinearfunctionapproximationproblemswithinthefinite-horizonsetting. SeeYang and Wang
(2019); Wang et al. (2019); Jin et al. (2020); Ayoub et al. (2020); Cai et al. (2020); Zhou et al.
(2021a,b); Zhou and Gu (2022); Agarwal et al. (2022); He et al. (2022); Zhong and Zhang (2023);
Zhao et al. (2023); Huang et al. (2023); Li and Sun (2023) and references therein. Furthermore,
Wang et al. (2020) studied RL with general function approximation and adopted the eluder dimen-
sion(Russo and Van Roy,2013)asacomplexitymeasure. Beforethis,Jiang et al.(2017)considered
asubstantialsubsetofproblemswithlowBellmanranks. Buildinguponthesefoundations, Jin et al.
(2021)combinedboththeEluderdimensionandBellmanerror,therebybroadeningthescopeofsolv-
ableproblemsundertheconceptoftheBellmanEluder(BE)dimension. Inaparallellineofresearch,
Sun et al. (2019) proposed the witness ranking focusing on the low-rank structures, and Du et al.
(2021) extended it to encompass more scenarios with the bilinear class. Besides, Foster et al. (2021,
2023) provided a unified framework, decision estimation coefficient, for interactive decision making.
The work of Chen et al. (2022b) extended the value-based Golf (Jin et al., 2021) with the intro-
duction of the discrepancy loss function to handle the broader admissible Bellman characterization
(ABC) class. More recently, Zhong et al. (2022); Liu et al. (2023b) proposed a unified framework
measured by generalized eluder coefficient (GEC), an extension to Dann et al. (2021) that captures
almostallknowntractable problems. Alltheseworks arerestricted tothefinite-horizon regime, and
their complexity measure and algorithms are not applicable in the infinite-horizon average-reward
setting.
Low-Switching Cost Algorithms. Addressing low-switching cost problems in bandit and rein-
forcement learning has seen notable progress. Abbasi-Yadkori et al. (2011) first proposed an algo-
rithm for linear bandits with (logT) switching cost. Subsequent research extended this to tabular
O
5MDPs, including works of Bai et al. (2019); Zhang et al. (2020). A significant stride was made by
Kong et al.(2021), whointroduced importance scores tohandle low-switching costscenarios ingen-
eral function approximation with complexity measured by eluder dimension (Russo and Van Roy,
2013). Recently, Xiong et al. (2023) introduced the eluder condition (EC) class, offering a com-
prehensive framework to address all tractable low-switching cost problems above. In the context
of average-reward RL, Wei et al. (2021); Wu et al. (2022); Chen et al. (2022a); Hu et al. (2022) de-
veloped low-switching algorithms to control the regret under linear structure or model-based class,
leaving a unifying framework for both value-based and model-based problems an open problem.
2 Preliminaries
Notations. For any integer n N+, we take the convention to use [n] = 1,...,n . Consider two
∈ { }
non-negative sequences a and b , if limsupa /b < , then we write it as a = (b ).
n n≥0 n n≥0 n n n n
{ } { } ∞ O
Else if limsupa /b = 0, then we write it as a = o(b ). And we use ˜ to omit the logarithmic
n n n n
O
terms. Denote∆( )betheprobabilitysimplexovertheset . Denotebysup v(x) thesupremum
X X x| |
norm of a given function. x y stands for min x,y and x y stands for max x,y . Given any
∧ { } ∨ { }
continuum , let be the cardinality. Given two distributions P,Q ∆( ), the TV distance of
S |S| ∈ X
the two distributions is defined as TV(P,Q) = 1E [dQ(x)/dP(x) 1].
2 x∼P | − |
An infinite-horizon average-reward Markov Dependent Process (AMDPs) is characterized by
= ( , ,r,P), where is a Borel state space with a possibly infinite number of elements, is a
M S A S A
finitesetofactions, r : [ 1,1]isanunknownreward function1 andP( s,a)istheunknown
S×A 7→ − ·|
transition kernel. The learning protocol for infinite-horizon average-reward RL is as follows: the
agent interacts with over a fixed number of T steps, starting from a pre-determined initial state
M
s . At each step t [T], the agent observe a state s and takes an action a , receives
1 t t
∈ S ∈ ∈ S ∈ A
a reward r(s ,a ) and transits to the next state s drawn from P( s ,a ).
t t t+1 t t
·|
Denote ∆( ) be the probability simplex over the action space . Specifically, the stationary
A A
policy π is a mapping π : ∆( ) with π(a s) specifying the probability of taking action a at
S 7→ A |
state s. Given a stationary policy π, the long-term average reward starting is defined as
T
1
Jπ(s) := lim inf E r(s ,a )s = s , s ,
t t 1
T7→∞ T " | # ∀ ∈ S
t=1
X
where the expectation is taken with respect to the policy, i.e., a π( s ) and the transition, i.e.,
t t
∼ ·|
s P( s ,a ). In infinite-horizon average-reward RL, existing works mostly rely on additional
t+1 t t
∼ ·|
assumptions to achieve sample efficiency. The necessity arises from the absence of a natural coun-
terpart to the celebrated Bellman optimality equation in the average-reward RL that is self-evident
and crucial within episodic and discounted settings (Puterman, 2014). To this end, we consider a
broad subclass where a modified Bellman optimality equation holds (Hernández-Lerma, 2012).
Assumption 1 (Bellman optimality equation). There exists bounded measurable function Q∗ :
R, V∗ : R and unique constant J∗ [ 1,1] such that for all (s,a) , it holds
S ×A 7→ S 7→ ∈ − ∈ S ×A
J∗+Q∗(s,a) = r(s,a)+E s′∼P(·|s,a)[V∗(s′)], V∗(s) = max Q∗(s,a). (2.1)
a∈A
1Throughout thispaper, we consider thedeterministic reward for notational simplicity and all results are readily
generalized to thestochastic setting. Also, we assume reward lies in [−1,1] without loss of generality.
6The Bellman optimality equation, adapted for average-reward RL, posits that for any initial
states s , the optimal reward is independent such that J∗(s ) = J∗ under a deterministic
1 1
∈ S
optimal policywith π∗( s) = argmax Q∗(,a). Thejustificationispresented in Wei et al.(2021).
·| a∈A ·
Note that functions V∗(s) and Q∗(s,a) reveal the relative advantage of starting from state s
and state-action pair (s,a) under the optimal policy, and are respectively called the optimal state
and state-action bias function (Wei et al., 2021). Denote sp(V) = sup s,s′∈S|V(s) −V(s′)
|
as the
span of any bounded measurable function. Note that for any solution pair (V∗,Q∗) satisfying the
Bellman optimality equation in (2.1), the shifted pair (V∗ c,Q∗ c) for any constant c is still
− −
a solution. Thus, without loss of generality, we can focus on the unique centralized solution such
that V∗ 1sp(V∗). Following the tradition in the average-reward RL (Wei et al., 2020, 2021;
k k∞ ≤ 2
Wang et al., 2022; Zhang and Xie, 2023), the span sp(V∗) is assumed to be known.
As aforementioned in thepaper, distinct assumptions have beenemployed inaverage-reward RL
researchtoensuretheexplorabilityoftheproblem,whichincludesergodicAMDPs(Wei et al.,2020;
Hao et al., 2021; Zhang and Xie, 2023), communicating AMDPs (Chen et al., 2022a; Wang et al.,
2022; Wu et al., 2022) and the Bellman optimality equation (Wei et al., 2021). Among these widely
adopted assumptions, we remark that the Bellman optimality equation is the least stringent one.
Note that the ergodic MDP suggests the existence of bias functions for each π Π, while the latter
∈
two only require the existence of bias functions for the optimal policy. As for weak communicating
assumption, a weaker form of communicating MDP (Wang et al., 2022), it directly implies the
existence of the Bellman optimality equation and thus is stronger (Hernández-Lerma, 2012). Given
(2.1), we introduce the average-reward Bellman operator below:
( JF)(s,a) := r(s,a)+E s′∼P(·|s,a) max F(s′,a′) J, (s,a) , (2.2)
T a′∈A − ∀ ∈ S ×A
(cid:20) (cid:21)
for any bounded function F : R and constant J [ 1,1]. Then, the Bellman optimality
S ×A 7→ ∈ −
equation in (2.1) can be written as J∗Q∗ = Q∗. Moreover, we define the Bellman error:
T
(F,J)(s,a) := F(s,a) ( F)(s,a), (s,a) . (2.3)
J
E − T ∀ ∈ S ×A
Learning Objective Undertheframework ofonlinelearning forAMDPs,theagent aims tolearn
the optimal policy by interacting with the environment over potentially infinite steps. The regret
measures the cumulative difference between the optimal average-reward and the reward achieved
after interacting for T steps, formally defined as Reg(T) = T (J∗ r(s ,a )).
t=1 − t t
P
3 General Function Approximation
To capture both model-free and model-based problems with function approximation, we consider a
general hypotheses class which contains a class of functions. We consider two kinds of hypothesis
H
classes, targeting at value-based problems and model-based problems respectively.
Definition 1 (Value-basedhypothesis). Wesay isavalue-basedhypothesesclassifallhypothesis
H
f isdefinedoverstate-actionbiasfunctionQandaverage-reward J suchthatf = (Q ,J ) .
f f
∈H ∈ H
Let V () = max Q (,a) and π () = argmax Q (,a) be the greedy bias function and policy
f · a∈A f · f · a∈A f ·
induced from hypothesis f . Denote f∗ be the optimal hypothesis under true model .
∈ H M
Definition 2 (Model-based hypothesis). We say is a model-based hypotheses class if all hypoth-
H
esis f is defined over the transition kernel P and reward function r such that f = (P ,r ) .
f f
∈ H ∈ H
7LetQ ,V ,J ,andπ respectivelybetheoptimalbiasfunctions,average-rewardandpolicyinduced
f f f f
from hypothesis f , which satisfies the Bellman optimality equation such that
∈ H
Q f(s,a)+J f = r f +P fV f (s,a), P f′V f(s,a) := E s′∼P f′(·|s,a)[V f(s′)],
for all s and a . Denot(cid:0)e f∗ as the(cid:1)hypothesis concerning the true model .
∈S ∈A M
The definition of hypotheses class over the value-based (see Definition 1) and the model-
H
based (see Definition 2) problems in AMDP is different from the episodic setting (Du et al., 2021;
Zhong et al.,2022). ThemostsignificantdifferenceisthattheBellmanequationhasadifferentform.
As a result, in the value-based scenario, instead of using a single state-action value function Q in
f
episodic setting, the paired hypothesis (Q ,J ) is introduced to fully capture the average-reward
f f
structure. Besides, we retain the definition of hypothesis over model-based problems, augmenting it
with an additional average-reward term J induced by (P ,r ). Since we do not impose any specific
f f f
structural form to the hypothesis class, we stay in the realm of general function approximation. As
function approximation is challenging without further assumptions (Krishnamurthy et al., 2016),
we introduce the realizability assumption, which is widely adopted (Jin et al., 2021).
Assumption 2 (Realizablity). We assume that f∗ .
∈ H
Moreover, we establish the fundamental distribution families over the state-action pair upon
whichthemetricisbuilt. Consideringthelearninggoaldefinedovertheempiricalregret,throughout
thepaperwefocusonthepoint-wisedistributionfamily = δ ()(s,a) ,whichincludes
∆ s,a
D { · | ∈ S×A}
collections of Dirac probability measure over . Discussions are deferred to Appendix A.
S ×A
3.1 Average-Reward Generalized Eluder Coefficients
In this subsection, we are going to introduce a novel metric—average-reward generalized eluder
coefficients (AGEC), to capture the complexity of hypotheses class for AMDPs. Extended from
H
the generalized Eluder coefficients (GEC, Zhong et al., 2022) for finite-horizon MDPs, AGEC is a
variant tofittheinfinite-horizon learning withaverage reward, andimposes anadditional structural
constraint—transferability, motivated by Eluder condition (EC) (Xiong et al., 2023) and proved
mild (see §3.2) to ensure the tractability of the problems.
Definition 3 (AGEC). Given hypothesis class , discrepancy function set l and constant
f f∈H
H { }
ǫ > 0, the average-reward generalized eluder coefficients AGEC( , l ǫ) is defined as the smallest
f ,
H { }
coefficients κ and d , such that following two conditions hold with absolute constants C , C > 0:
G G 1 2
(i) (Bellman dominance) There exists constant d > 0 such that
G
T T t−1 1/2
(f )(s ,a ) d E [l (f ,f ,ζ )] 2 +C sp(V∗)min d ,T +Tǫ.
E t t t ≤ G · k ζi fi t t i k2 1 · { G }
t=1 (cid:20) t=1 i=1 (cid:21)
X X X Burn-in cost
Bellman error In-sample training error
| {z }
| {z } | {z }
(ii) (Transferability) There exists constant κ > 0 such that for hypotheses f ,...,f , if
G 1 T
t−1 E [l (f ,f ,ζ )] 2 β holds for all t [T], then we have ∈ H
i=1k ζi fi t t i k2 ≤ ∈
P T
E [l (f ,f ,ζ )] 2 κ βlogT +C sp(V∗)2min κ ,T +2Tǫ2.
k ζt ft t t t k2 ≤ G · 2 · { G }
t=1
X Burn-in cost
Out-sample training error
| {z }
| {z }
8In the definition above, ζ is a subset of trajectory with varying meaning concerning the specific
i
choice of discrepancy function, and the expectation is taken over it; C ,C are absolute constants
1 2
related to span sp(V∗). To simplify the notation, we denote (f )(s,a) := (Q ,J )(s,a) for all
E
t
E
ft ft
(s,a) . Besides, the Burn-in cost is taken at the worst case and it varies across different set-
∈ S×A
tings butusuallynon-dominating. Theintuition behindthemetric isthat, onaverage, ifhypotheses
have small in-sample training error on the well-explored dataset, then the prediction error on a dif-
ferent trajectory is expected to maintain a consistently low level (Zhong et al., 2022). In specific,
the dominance coefficient d encapsulates the challenge inherent in assessing the performance of
G
prediction, specifically the Bellman error, given the consistently controlled in-sample training error
within the designated function class . Moreover, due to the unique challenge of infinite-horizon
H
average-reward setting, weintroduce thetransferability coefficient κ toquantify thetransferability
G
from the in-sample training error to the out-of-sample ones. Despite this additional structural con-
dition, we can verify that nearly all tractable AMDPs admit a low AGEC value (see Section 3.2).
Moreover, in Section 5, we will demonstrate the importance of such additional structural conditions
for achieving sample efficiency in AMDPs from the theoretical perspective.
Moreover, to facilitate further theoretical analysis, we make further assumptions on the discrep-
ancy function and hypothesis class as Chen et al. (2022b); Zhong et al. (2022).
Assumption 3 (Boundedness). Given any f , it holds that l C sp(V∗) with C > 0.
f ∞ ℓ ℓ
∈ H k k ≤ ·
The boundedness assumption is reasonable and uniformly satisfied, as in most cases, it takes
the Bellman discrepancy, defined as: for all ζ = s ,a ,r ,s R , we have
t t t t t+1
{ } ∈ S ×A× ×S
l f′(f,g,ζ t)= Q g(s t,a t) r(s t,a t) V f(s t+1)+J g, (3.1)
− −
or other natural derivatives, so that the discrepancy is generally upper bounded by sp(V∗) .
O
Assumption 4 (Generalized completeness). Let be an auxiliary function class and(cid:0)there e(cid:1)xists
G
a functional operator : , we say that satisfies generalized completeness in concerning
P H 7→ G H G
discrepancy function l f′ if for any (f,g) ( ), it holds that
∈ H× H∪G
l f′(f,g,ζ) l f′(f, (f),ζ) = E
ζ
l f′(f,g,ζ) , (3.2)
− P
where the expectation is taken over trajectory ζ. Besides, t(cid:2)he operato(cid:3)r satisfies that (f∗)= f∗.
P
The completeness assumption is an extension of the Bellman completeness for value-based hy-
pothesis (Jin et al., 2021), incorporating the notion of the decomposition loss function (DLF) prop-
erty proposed in Chen et al. (2022b). Our assumption diverges from the one posited in Zhong et al.
(2022), where an auxiliary function class ( ) is introduced to enrich choices, accompanied
P H ⊆ G
with modifications tailored to accommodate the nuances of the average-reward setting.
Example 1 (Bellman completeness Generalized completeness). Let the discrepancy function
⊆
be the Bellman discrepancy in (3.1) with ζ = s ,a ,r ,s , and takes the (hypothesis-scheme)
t t t t t+1
{ }
Bellman operator, defined as (f)= (Q ),J for all f , modified from (2.2). Then,
T
{TJf f f
} ∈ H
l f′(f,g,ζ t) −l f′(f, T(f),ζ t) = Q g(s t,a t) −TJfQ f(s t,a t)
= Q g(s t,a t) −r(s t,a t)+E
ζt
V f(s t+1) +J
f
= E
ζt
l f′(f,g,ζ t) ,
where the expectation is taken over the transition st(cid:2)ate s (cid:3)from P( s ,a(cid:2)). (cid:3)
t+1 t t
·|
The preceding example illustrates that the Bellman discrepancy, a frequently employed dis-
crepancy function across problems, satisfies both assumptions. More examples and choices of the
discrepancy function for MLE-based algorithms are respectively provided in Appendix B and C.
93.2 Relation with Tractable Complexity Metric
To bridge the gap between concrete function approximation instances and the relatively abstract
measureAGEC,thissectionintroducestwointermediatemetrics: Eluderdimension(Russo and Van Roy,
2013)andAverage-reward BellmanEluder (ABE)dimension. Inparticular, Chen et al.(2022a)em-
ploys the Eluder dimension to gauge the complexity of model-based hypothesis classes for infinite-
horizon learning. To provide an intuitive complexity of value-based hypothesis classes, we addition-
ally proposetheABEdimension, which is ageneralization ofthe standard BEdimension (Jin et al.,
2021). These two metrics provide valuable insights into the nature of AGEC.
Eluder Dimension We start with ǫ-independence notation (Russo and Van Roy, 2013).
Definition 4 (Point-wise ǫ-independence). Let be a function class defined on and consider
H X
sequence z,x ,...,x . We say z is ǫ-independent of x ,...,x with respect to if there
1 n 1 n
exists f,f{ ′ such th} at∈ X n (f(x ) f′(x ))2 ǫ, but{ f(z) f′(z} ) ǫ. H
∈ H i=1 i − i ≤ | − | ≥
Based on ǫ-independenpceP, the Eluder dimension can be efficiently defined as below.
Definition 5 (Eluder dimension). Let be a function class defined on . The Eluder dimension
H X
dim ( ,ǫ) is the length of the longest sequence x ,...,x such that there exists ǫ′ ǫ
E 1 n
H { } ⊂ X ≥
where x is ǫ′-independent of x ,...,x for all i [n].
i 1 i−1
{ } ∈
The following lemma shows that a model-based hypothesis class with a low Eluder dimension
also has low AGEC. Motivated by Ayoub et al. (2020); Chen et al. (2022a), we consider the Eluder
dimension over function class derived from the model-based hypotheses class , defined as
H
H
:= X f,f′(s,a) = r
f
+P f′V
f
(s,a) : f,f′ .
X ∈ H
Note that Chen et al. (2022a)(cid:8) considered fun(cid:0) ction class(cid:1) := P V(s(cid:9) ,a) : f P( ),V ,
H,V f
X { ∈ H ∈ V}
where P( ) denotes the hypotheses class over the transition kernel and denotes the hypotheses
H V
classovertheoptimalbiasfunction. Weremark thatdefinitions overfunctionclassbasedon (P ,V)
f
with (f,V) and (P ,r ) with f is almost equivalent and in this paper we focus on
f f H
∈ H×V ∈ H X
under the latter framework, aligning with the model-based hypothesis (see Definition 2).
Lemma 3.1 (Low Eluder dim Low AGEC). Consider discrepancy function
⊆
l f′(f,g,ζ t) = r g +P gV f′ (s t,a t) r(s t,a t) V f′(s t+1), (3.3)
− −
with (f) = f∗, and the expectatio(cid:0) n is taken(cid:1) over s from P( s ,a ). Let d = dim ( ,ǫ) be
t+1 t t E E H
P ·| X
the ǫ-Eluder dimension defined over , then we have d 2d logT and κ d .
H G E G E
X ≤ · ≤
Average-Reward Bellman Eluder (ABE) Dimension Before delving into details of the
average-rewardBE(ABE)dimension,westartwithtwousefulnotations,distributionalǫ-independence
and distributional Eluder (DE) dimension proposed by Jin et al. (2021), which is a generalization
of point-wise ǫ-independence and Eluder dimension defined above (see Definitions 4 and 5).
Definition 6 (Distributional ǫ-independence). Let beafunctionclassdefinedon andsequence
H X
υ,µ ,...,µ be the probability measures over . We say υ is ǫ-independent of µ ,...,µ with
1 n 1 n
r{
espect to
i}
f there exists f such that
Xn
(E [f])2 ǫ, but E [f]
ǫ{
.
}
H ∈ H i=1 µi ≤ | υ |≥
pP
10Definition 7 (Distributional Eluder dimension). Let be a function class defined on and Γ
H X
be a family of probability measures over . The distributional Eluder dimension dim ( ,Γ,ǫ)
DE
X H
is the length of the longest sequence ρ ,...,ρ Γ such that there exists ǫ′ ǫ where ρ is
1 n i
{ } ⊂ ≥
ǫ′-independent of the remaining distribution sequence ρ ,...,ρ for all i [n].
1 i−1
{ } ∈
Now we are ready to introduce the average-reward Bellman Eluder (ABE) dimension. It is
defined as the distributional Eluder (DE) dimension of average-reward Bellman error in (2.3).
Definition 8 (ABE dimension). Denote = (f)(s,a) : f be the collection of average-
H
E {E ∈ H}
reward Bellman errors defined over . For any constant ǫ > 0, the ǫ-ABE dimension of given
S ×A
hypotheses class is defined as dim ( ,ǫ) := dim , ,ǫ .
ABE DE H ∆
H H E D
The lemma below posits that the value-based hypot(cid:0)hesis prob(cid:1)lem with a low ABE dimension
shall have a low AGEC in terms of the Bellman discrepancy.
Lemma 3.2 (Low ABE dim Low AGEC). Consider the Bellman discrepancy function as defined
⊆
in (3.1) , and the expectation is taken over the transition state s from P( s ,a ). Let d =
t+1 t t ABE
·|
dim ( ,ǫ), then we have d 2d logT and κ d .
ABE G ABE G ABE
H ≤ · ≤
The Eluder dimension and ABE dimension can capture numerous concrete problems, respec-
tively undermodel-basedandvalue-basedscenarios. Specifically, theEluderdimension incorporates
rich model-based problems like linear mixture AMDPs (Wu et al., 2022), and the ABE dimension
can characterize tabular AMDPs (Jaksch et al., 2010), linear AMDPs (Wei et al., 2021), AMDPs
with Bellman Completeness, generalized linear AMDPs, and kernel AMDPs, where the latter three
problems are newly proposed for AMDPs. Details about the concrete examples are deferred to
Appendix C. Combining these facts and Lemmas 3.1 and 3.2, we can conclude that AGEC serves
as a unified complexity measure, as it encompasses all of these tractable AMDPs illustrated above.
4 Local-fitted Optimization with Optimism
To solvetheAMDPswith lowAGEC value(seeDefinition 3), we proposethealgorithm Local-fitted
Optimization with OPtimism (Loop), whose pseudocode is given in Algorithm 1. At a high level,
Loop is a modified version of the classical fitted Q-iteration (Szepesvári, 2010) with optimistic
planning and lazy policy updates. That is, the policies are only updated when a certain criterion is
met (Line 2). When this is the case, Loop performs three main steps:
- Optimisticplanning(Line 4.1): Computethemostoptimisticf within thatmaximizes
t t
∈ H B
the corresponding average-reward J by solving a constrained optimization problem.
t
- Construct confidence set (Line 4.2): Construct the confidence set for optimization using
t
B
, where all f satisfying (f ,f ) inf (f ,g) β is included. Here, β > 0
Dt−1 t
∈H
LDt−1 t t
−
g∈G LDt−1 t
≤
defines the radius, corresponding to the log covering number of the hypothesis class .
H
- Execute Policy and Update Υ (Line 8-10): Choose the greedy policy π = π as the explo-
t t ft
rationpolicy. Executepolicy, collectdata, andupdatetrigger Υ = (f ,f ) inf (f ,g).
t LDt t t
−
g∈G LDt t
Note that both the confidence set and the update condition Υ are constructed upon the (cumu-
t t
B
lative) squared discrepancy, which is crucial to the algorithmic design. It takes the form
(f,f) inf (f,g), where (f,g) = l (f,g,ζ ) 2, (4.1)
LDt −g∈GLDt LD k fi i k2
(fiX,ζi)∈D
11where f
i
and ζ
i
= (s i,a i,s i+1) are drawn from t, and l f′(f,g,ζ) denotes the discrepancy function,
D
which varies across different RL problems. The sum of squared discrepancy serves as an empirical
estimation of the in-sample training error (see Definition 3). Besides, we highlight two key designs:
- Consistent control over discrepancy: The construction of confidence set ensures that the
t
B
cumulative squared discrepancy is controlled at level β in each step. To see this, suppose τ = t,
t
i.e.,policyswitchesatthet-thstep, then(4.2)ensuresthat (f ,f ) inf (f ,g) β.
LDt−1 t t
−
g∈G LDt−1 t
≤
Otherwise, if we do not switch the policy at step t, then we must have Υ = (f ,f )
t−1 LDt−1 t t
−
inf (f ,g) 4β, as hypothesis f = f remains unchanged.
g∈G LDt−1 t
≤
t t−1
- Lazy policy update: Theregretdecompositionin(5.1)elucidates thateachpolicyswitchincurs
an additional cost of V (s ) V (s ) in regret at each step (see (5.3)). This underscores
t+1 t+1 t t+1
| − |
the necessity of implementing lazy updates to achieve sublinear regret. Within the Loop frame-
work, policy updates occur adaptively, triggered only when a substantial increase in cumulative
discrepancy surpassing 3β has occurred since the last update. Intuitively, a policy switch occurs
when there is a notable infusion of new information from newly collected data. Importantly, such
gap is pivotal as it provides the theoretical foundation for the implementation of lazy updates,
leveraging the problem’s transferability structure (see (ii), Definition 3). Here, Loop employs a
threshold of 4β, considering inherent uncertainty and estimation errors between the minimizer g
and (f ), ensuring that the out-of-sample error will exceed β under the updating rule.
t
P
Similar to previous works in general function approximation (Jin et al., 2021; Du et al., 2021), our
algorithm lacks a computationally efficient solution for constrained optimization problems. Instead,
our focus is on the sample efficiency, as guaranteed by the theorem below.
Theorem 4.1 (Regret). UnderAssumptions 1-4, thereexistsconstant csuchthatforany δ (0,1)
∈
and horizon T, with probability at least 1 5δ, the regret of Loop satisfies that
−
Reg(T) sp(V∗) d Tβ ,
≤ O ·
(cid:16) p (cid:17)
whereβ = clog T 2 (1/T)/δ sp(V∗)andd= max √d ,κ . Here,(d ,κ ) = AGEC( , l ,
NH∪G · { G G } G G H { f }
1/√T) are AGEC defined in Definition 3, is the auxiliary function class defined in Definition 4,
(cid:0) (cid:1) G
and () denotes the covering number as defined in Definition 17.
H∪G
N ·
Theorem 4.1 posits that both thr value-based and model-based problems with low AGEC are
tractable. Our algorithm Loop achieves a ˜(√T) regret and the multiplicative factor depends on
O
span sp(V∗), problem complexity max √d ,κ and the log covering number. The proof sketch is
G G
{ }
provided in Section 5 and the detailed proof is deferred to Appendix D.
5 Proof Overview of Regret Analysis
In this section, we present the proof sketch of Theorem 4.1. In Section 4, we elucidated the con-
struction of the confidence set and the circumstances in which updates are performed. Here, we
delve into the theoretical analysis to substantiate the necessity of such designs.
Optimism and Regret Decomposition In Loop, we apply an optimization based method to
ensure the optimism J J∗ at each step t [T]. Based on the optimistic algorithm, we propose a
t
≥ ∈
12Algorithm 1 Local-fitted Optimization with Optimism - Loop( , ,T,δ)
H G
Parameter: Initial s , span sp(V∗), optimistic parameter β = clog T 2 (1/T)/δ sp(V∗)
1 NH∪G ·
Initialize: Draw a Unif( ) and set τ 0, Υ 0, , .
1 0 0 0 0
∼ A ← ← B ← ∅ D (cid:0)← ∅ (cid:1)
1: for t = 1,...,T do
2: if t = 1 or Υ t−1 4β then
≥
3: Set τ t = t.
4: Solve optimization problem f t = argmax ft∈BtJ ft, where
= f : (f,f) inf (f,g) β , (4.2)
Bt
∈ H
LDt−1
−g∈G
LDt−1
≤
(cid:26) (cid:27)
5: Compute Q t = Q ft, V t = V ft and J t = J ft.
6: else
7: Retain (f t,J t,V t,Q t,τ t) = (f t−1,J t−1,V t−1,Q t−1,τ t−1).
8: Execute a t = argmax a∈A Q t(s t,a).
9: Observe r t = r(s t,a t) and transition state s t+1.
10: Update Dt = Dt−1 ∪{(s t,a t,r t,s t+1,f t)
}
and Υ t = LDt(f t,f t) −inf g∈G LDt(f t,g).
new regret decomposition method motivated by the standard performance difference lemma (PDL)
in episodic setting (Jiang et al., 2017), following the form as below:
T T
Reg(T)
≤
E(f t)(s t,a t)+ E st+1∼P(·|st,at)[V t(s t+1)] −V t(s t). (5.1)
t=1 t=1
X X
Bellman error Realization error
| {z } | {z }
Part 1: Bound over Bellman Error The control over the Bellman error is achieved through
the design of a confidence set and update condition that combinely controls the empirical squared
discrepancy. Notethattheconstructionoftheconfidencesetfiltersf withalimitedsumofempirical
t
squared discrepancy. Note that (f ,f ) inf (f ,g) can be regarded as an empirical
LDt−1 t t
−
g∈G LDt−1 t
overestimation of the squared discrepancy, controlled at (β) regardless of updating. Then,
O
t−1
In-sample training error = E [l (f ,f ,ζ )] 2 . β t [T], (5.2)
k ζi fi t t i k2 ∀ ∈
i=1
X
with high probability, and β is pre-determined optimistic parameter depends on horizon T and the
log ρ-covering number. Recall that the dominance coefficient d regulates that Bellman error .
G
1/2
d T t−1 E [l (f ,f ,ζ )] 2 , thus we have Bellman error ( d βT).
G t=1 i=1k ζi fi t t i k2 ≤ O G
h P (cid:16) P (cid:17)i p
Part 2: Bound over Realization error The realization error is small if the switching cost is
low as the concentration arguments indicated that with high probability it holds
Realization error sp(V∗) (T)+ sp(V∗) T log(1/δ) , (5.3)
≤ ·N O ·
Switching cost A(cid:0)zuma-Hopeffding term(cid:1)
| {z } | {z }
where (t) denote switching cost defined as (T) = # t [T] : τ = τ . Motivated by the
t t−1
N N { ∈ 6 }
recent work ofXiong et al. (2023), themainideaoflow-switching control issummarized below. The
13key step is that the minimizer g is a "good" conservative approximator of (f ) in a sense that
t
∈ G P
0 (f , (f )) inf (f ,g) β, t [T] (5.4)
≤
LDt t
P
t
−g∈G
LDt t
≤ ∀ ∈
with high probability based on the minimization and the definition of optimistic parameter β. In
the following analysis, we assume that (5.4) holds. Suppose that an update occurs at step t+1,
then it implies that (f ,f ) inf (f ,g) > 4β and the latest update at step τ ensures that
LDt t t
−
g∈G LDt t t
(f ,f ) inf (f ,g) β. Combine (5.4) with the arguments above, we have
LDτt−1 τt τt
−
g∈G LDτt−1 τt
≤
(i). (f ,f ) f , (f ) > 3β, (ii). (f ,f ) f , (f ) β. (5.5)
LDt t t −LDt t
P
t LDτt−1 τt τt −LDτt−1 τt
P
τt
≤
(cid:0) (cid:1) (cid:0) (cid:1)
Basedontheconcentrationargumentand(i),(ii)in(5.5),theout-sampletrainingerrorbetweentwo
updates is lower bounded by t E [l (f ,f ,ζ )] 2 > β. Let b ,...,b be the sequence of
i=τtk ζi fi i i i k2 1 N(T)+1
updated steps, take summation over the T steps and then we have
P
N(T)bu+1−1 T
(T) β E [l (f ,f ,ζ )] 2 = E [l (f ,f ,ζ )] 2 (κ βlogT), (5.6)
N · ≤ k ζt ft t t t k2 k ζt ft t t t k2≤ O G ·
Xu=1 t X=bu Xt=1
wherethefirstinequality results fromthearguments aboveandthesecondisbasedonthedefinition
of transferability (see Definition 3) given t−1 E [l (f ,f ,ζ )] 2 β for all t [T]. Thus,
i=1k ζi fi t t i k2 ≤ O ∈
we have (T) (κ logT) and the Realization error is bounded by κ sp(V∗)logT . Please
G G
N ≤ O P O(cid:0) (cid:1)·
refer to Lemma D.3 in Appendix D.4 for a formal statement and detailed techniques.
(cid:0) (cid:1)
6 Conclusion
This work studies the infinite-horizon average-reward MDPs under general function approximation.
To address the unique challenges of AMDPs, we introduce a new complexity metric—average-
reward generalized eluder coefficient (AGEC) and a unified algorithm—Local-fitted Optimization
withOPtimism(Loop). Wepositthatourworkpavesthewayforfuturework,includingdeveloping
more general frameworks for AMDPs and new algorithms with sharper regret bounds.
14References
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and Weisz, G. (2019).
Politex: Regret bounds for policy iteration using expert prediction. In International Conference
on Machine Learning, pages 3692–3702. PMLR.
Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. (2011). Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems, 24.
Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R. (2014). Taming the monster:
A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, pages 1638–1646. PMLR.
Agarwal, A., Jin, Y., and Zhang, T. (2022). Vo q l: Towards optimal regret in model-free rl with
nonlinear function approximation. arXiv preprint arXiv:2212.06069.
Auer, P., Jaksch, T., and Ortner, R. (2008). Near-optimal regret bounds for reinforcement learning.
Advances in neural information processing systems, 21.
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L. (2020). Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pages
463–474. PMLR.
Bai, Y.,Xie, T., Jiang, N., andWang, Y.-X.(2019). Provably efficient q-learningwithlowswitching
cost. Advances in Neural Information Processing Systems, 32.
Bartlett, P. L. and Tewari, A. (2012). Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. arXiv preprint arXiv:1205.2661.
Cai,Q.,Yang,Z.,Jin,C.,andWang,Z.(2020). Provablyefficientexplorationinpolicyoptimization.
In International Conference on Machine Learning, pages 1283–1294. PMLR.
Chen, X., Hu, J., Jin, C., Li, L., and Wang, L. (2022a). Understanding domain randomization for
sim-to-real transfer. International Conference on Learning Representations.
Chen, Z., Li, C. J., Yuan, A., Gu, Q., and Jordan, M. I. (2022b). A general framework for sample-
efficient function approximation in reinforcement learning. arXiv preprint arXiv:2209.15634.
Dani, V., Hayes, T. P., and Kakade, S. M. (2008). Stochastic linear optimization under bandit
feedback.
Dann, C., Mohri, M., Zhang, T., and Zimmert, J. (2021). A provably efficient model-free posterior
sampling method for episodic reinforcement learning. Advances in Neural Information Processing
Systems, 34:12040–12051.
Domingues, O. D., Ménard, P., Pirotta, M., Kaufmann, E., and Valko, M. (2021). A kernel-based
approach to non-stationary reinforcement learning in metric spaces. In International Conference
on Artificial Intelligence and Statistics, pages 3538–3546. PMLR.
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R. (2021). Bilinear
classes: A structural framework for provable generalization in rl. In International Conference on
Machine Learning, pages 2826–2836. PMLR.
15Foster, D. J., Golowich, N., and Han, Y. (2023). Tight guarantees for interactive decision making
with the decision-estimation coefficient. arXiv preprint arXiv:2301.08215.
Foster, D. J., Kakade, S. M., Qian, J., and Rakhlin, A. (2021). The statistical complexity of
interactive decision making. arXiv preprint arXiv:2112.13487.
Fruit, R., Pirotta, M., Lazaric, A., and Ortner, R. (2018). Efficient bias-span-constrained
exploration-exploitation in reinforcement learning. In International Conference on Machine
Learning, pages 1578–1586. PMLR.
Hao,B.,Lazic,N.,Abbasi-Yadkori,Y.,Joulani,P.,andSzepesvári,C.(2021).Adaptiveapproximate
policy iteration. In International Conference on Artificial Intelligence and Statistics, pages 523–
531. PMLR.
He, J., Zhao, H., Zhou, D., and Gu, Q. (2022). Nearly minimax optimal reinforcement learning for
linear markov decision processes. arXiv preprint arXiv:2212.06132.
Hernández-Lerma, O. (2012). Adaptive Markov control processes, volume 79. Springer Science &
Business Media.
Hu, J., Zhong, H., Jin, C., and Wang, L. (2022). Provable sim-to-real transfer in continuous domain
with partial observations. arXiv preprint arXiv:2210.15598.
Huang, J., Zhong, H., Wang, L., and Yang, L. F. (2023). Tackling heavy-tailed rewards in rein-
forcementlearningwithfunctionapproximation: Minimaxoptimalandinstance-dependent regret
bounds. arXiv preprint arXiv:2306.06836.
Jaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research, 11(51):1563–1600.
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual
decision processes with low bellman rank are pac-learnable. In International Conference on
Machine Learning, pages 1704–1713. PMLR.
Jin, C., Liu, Q., and Miryoosefi, S. (2021). Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. Advances in neural information processing systems,
34:13406–13418.
Jin, C., Yang, Z., Wang, Z., andJordan, M.I.(2020). Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory, pages 2137–2143. PMLR.
Kong, D., Salakhutdinov, R., Wang, R., and Yang, L. F. (2021). Online sub-sampling for reinforce-
ment learning with general function approximation. arXiv preprint arXiv:2106.07203.
Krishnamurthy, A., Agarwal, A., and Langford, J. (2016). Pac reinforcement learning with rich
observations. Advances in Neural Information Processing Systems, 29.
Lazic, N., Yin, D., Abbasi-Yadkori, Y., and Szepesvari, C. (2021). Improved regret bound and ex-
perience replay in regularized policy iteration. In International Conference on Machine Learning,
pages 6032–6042. PMLR.
Li, X. and Sun, Q. (2023). Variance-aware robust reinforcement learning with linear function
approximation with heavy-tailed rewards. arXiv preprint arXiv:2303.05606.
16Liu, Q., Chung, A., Szepesvári, C., and Jin, C. (2022). When is partially observable reinforcement
learning not scary? In Conference on Learning Theory, pages 5175–5220. PMLR.
Liu, Q., Netrapalli, P., Szepesvari, C., and Jin, C. (2023a). Optimistic mle: A generic model-based
algorithm for partially observable sequential decision making. In Proceedings of the 55th Annual
ACM Symposium on Theory of Computing, pages 363–376.
Liu, Z., Lu, M., Xiong, W., Zhong, H., Hu, H., Zhang, S., Zheng, S., Yang, Z., and Wang, Z.
(2023b). Oneobjectivetorulethemall: Amaximizationobjectivefusingestimationandplanning
for exploration. arXiv preprint arXiv:2305.18258.
Ortner,R.(2020).Regretboundsforreinforcementlearningviamarkovchainconcentration. Journal
of Artificial Intelligence Research, 67:115–128.
Ouyang, Y., Gagrani, M., Nayyar, A., and Jain, R. (2017). Learning unknown markov decision
processes: A thompson sampling approach. Advances in neural information processing systems,
30.
Proper, S. and Tadepalli, P. (2006). Scaling model-based average-reward reinforcement learning for
product delivery. In European Conference on Machine Learning, pages 735–742. Springer.
Puterman, M.L.(2014). Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons.
Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic
exploration. Advances in Neural Information Processing Systems, 26.
Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. (2009). Gaussian process optimization in
the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995.
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. (2019). Model-based rl
in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on learning theory, pages 2898–2933. PMLR.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intel-
ligence and machine learning, 4(1):1–103.
Wang, J., Wang, M., and Yang, L. F. (2022). Near sample-optimal reduction-based policy learning
for average reward mdp. arXiv preprint arXiv:2212.00603.
Wang, R., Salakhutdinov, R. R., and Yang, L. (2020). Reinforcement learning with general value
function approximation: Provably efficient approach via bounded eluder dimension. Advances in
Neural Information Processing Systems, 33:6123–6135.
Wang, Y., Wang, R., Du, S. S., and Krishnamurthy, A. (2019). Optimism in reinforcement learning
with generalized linear function approximation. arXiv preprint arXiv:1912.04136.
Wei, C.-Y., Jahromi, M. J., Luo, H., and Jain, R. (2021). Learning infinite-horizon average-reward
mdps with linear function approximation. In International Conference on Artificial Intelligence
and Statistics, pages 3007–3015. PMLR.
17Wei, C.-Y., Jahromi, M. J., Luo, H., Sharma, H., and Jain, R. (2020). Model-free reinforcement
learning ininfinite-horizon average-reward markov decision processes. InInternational conference
on machine learning, pages 10170–10180. PMLR.
Wu, Y., Zhou, D., and Gu, Q. (2022). Nearly minimax optimal regret for learning infinite-horizon
average-reward mdps with linear function approximation. In International Conference on Artifi-
cial Intelligence and Statistics, pages 3883–3913. PMLR.
Xiong, N., Yang, Z., and Wang, Z. (2023). A general framework for sequential decision-making
under adaptivity constraints. arXiv preprint arXiv:2306.14468.
Yang, L. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive fea-
tures. In International Conference on Machine Learning, pages 6995–7004. PMLR.
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020). Learning near optimal policies
with low inherent bellman error. In International Conference on Machine Learning, pages 10978–
10989. PMLR.
Zhang, Z. and Ji, X. (2019). Regret minimization for reinforcement learning by evaluating the
optimal bias function. Advances in Neural Information Processing Systems, 32.
Zhang, Z.and Xie, Q.(2023). Sharpermodel-free reinforcement learning foraverage-reward markov
decision processes. In The Thirty Sixth Annual Conference on Learning Theory, pages 5476–5477.
PMLR.
Zhang, Z., Zhou, Y., and Ji, X. (2020). Almost optimal model-free reinforcement learn-
ingvia reference-advantage decomposition. Advances in Neural Information Processing Systems,
33:15198–15207.
Zhao, H., He, J., Zhou, D., Zhang, T., and Gu, Q. (2023). Variance-dependent regret bounds
for linear bandits and reinforcement learning: Adaptivity and computational efficiency. arXiv
preprint arXiv:2302.10371.
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T. (2022). Gec: A
unified framework for interactive decision making in mdp, pomdp, and beyond. arXiv preprint
arXiv:2211.01962.
Zhong, H. and Zhang, T. (2023). A theoretical analysis of optimistic proximal policy optimization
in linear markov decision processes. arXiv preprint arXiv:2305.08841.
Zhou, D.andGu,Q.(2022). Computationally efficient horizon-freereinforcement learning forlinear
mixture mdps. arXiv preprint arXiv:2205.11507.
Zhou, D., Gu, Q., and Szepesvari, C. (2021a). Nearly minimax optimal reinforcement learning for
linear mixture markov decision processes. In Conference on Learning Theory, pages 4532–4576.
PMLR.
Zhou, D., He, J., andGu, Q.(2021b). Provably efficient reinforcement learning for discounted mdps
with feature mapping. In International Conference on Machine Learning, pages 12793–12802.
PMLR.
18Appendix for “Sample-efficient Learning of Infinite-horizon
Average-reward MDPs with General Function Approximation”
A Technical Novelties and Further Discussions
FurtherElaborationonOurContributionsandTechnicalNovelties. Comparedtoepisodic
MDPs or discounted MDPs, AMDPs present unique challenges that prevent a straightforward ex-
tensionofexistingalgorithmsandanalysesfromthesewell-studieddomains. Onenotabledistinction
is a different regret notion in average-reward RL due to a different form of the Bellman optimality
equation. Furthermore, such a difference is coupled with the challenge of exploration in the context
of general function approximation. To effectively bound this regret, we introduce a new regret
decomposition approach within the context of general function approximation (refer to (5.1) and
(5.3)). This regret decomposition suggests that the total regret can be controlled by the cumula-
tive Bellman error and the switching cost. Inspired by this, we propose an optimistic algorithm
with lazy updates in the general function approximation setting, which uses the residue of the loss
function as the indicator for deciding when to conduct policy updates. Such a lazy policy update
scheme adaptively divides the total of T steps into (logT) epochs, which is significantly different
O
from (OLSVI.FH; Wei et al., 2021) that reduces the infinite-horizon setting to the finite-horizon
setting by splitting the whole learning procedure into several H-length epoch, where H typically
chosen as Θ(√T) (Wei et al., 2021). We remark that such an adaptive lazy updating design and
corresponding analysis are pivotal in achieving the optimal ˜(√T) rate, as opposed to the ˜(T3/4)
O O
regret in (OLSVI.FH; Wei et al., 2021). Moreover, our approach is an extension to the existing lazy
update approaches for average-reward setting (Wei et al., 2021; Wu et al., 2022) that leverages the
postulated linear structure and is not applicable to problems with general function approximation.
Furthermore, to accommodate the average-reward term, we introduce a new complexity measure
AGEC, which characterizes the exploration challenge in general function approximation. Compared
with Zhong et al. (2022), our additional transferability restriction is tailored for the infinite-horizon
setting and plays a crucial role in analyzing the low-switching error. Despite this additional trans-
ferability restriction, AGEC can still serve as a unifying complexity measure in the infinite-horizon
average-reward setting, like the role of GEC in the finite-horizon setting. Specifically, AGEC cap-
tures a rich class of tractable AMDP models, including all previously recognized AMDPs, including
all known tractable AMDPs, and some newly identified AMDPs. See Table 1 for a summary.
Discussion about distribution families Beyond the singleton distribution family taken in
∆
D
thispaper,thereexistsanotabledistributionfamily = ,proposedinJin et al.(2021),
H H,t t∈[T]
D {D }
where characterizes probability measure over obtained by executing different policies
H,t
D S ×A
induced by f ,...,f , measures the detailed distribution under sequential policies. However,
1 t−1
∈ H
in this paper, we exclude the consideration of for two principal reasons. First, evaluations of
H
D
average-reward RL focus on the difference between observed rewards r(s ,a ) and optimal average
t t
reward J∗ — as opposed to the expected value Vπ (i.e expected sum of reward) under specific
h
policy and optimal value at step h [H] in episodic setting — rendering the introduction of
H
∈ D
unnecessary. Second, in infinite settings, the measure of such distribution becomes highly intricate
and impractical given different policy induced by f ,...,f over a potentially infinite T-steps. As
1 T
a comparison, in the episode setting a fixed policy induced by f is executed over a finite H-step.
t
19Algorithm 2 MLE-based Local-fitted Optimization with Optimism - Mle-Loop( ,T,δ)
H
Parameter: Initial s , span sp(V∗), optimistic parameter β = clog T (1/T)/δ
1 H
N
Initialize: Draw a Unif( ) and set τ 0, Υ 0, , .
1 0 0 0 0
∼ A ← ← B ← ∅ D (cid:0)← ∅ (cid:1)
1: for t = 1,...,T do
2: if t = 1 or Υ t−1 3√βt then
≥
3: Update τ t = t.
4: Solve optimization problem f t = argmax ft∈BtJ ft, where
= f : (f,f) inf (f,g) β , (B.1)
Bt
∈ H
LDt−1
−g∈G
LDt−1
≤
(cid:26) (cid:27)
5: Update Q t = Q ft, V t = V ft, J t = J ft and g t = gin ∈Gf LDt−1(f t,g).
6: else
7: Retain (f t,g t,J t,V t,Q t,τ t) = (f t−1,g t−1,J t−1,V t−1,Q t−1,τ t−1).
8: Execute a t = argmax a∈A Q t(s t,a).
9: Collect reward r t = r(s t,a t) and transition state s t+1.
10: Update Dt = Dt−1 ∪{(s t,a t,r t,s t+1) }, Υ t = s,a∈DtTV P ft( ·|s,a),P gt( ·|s,a) .
P (cid:0) (cid:1)
B Alternative Choices of Discrepancy Function
Note that there is another line of research that addresses model-based problems using Maximum
LikelihoodEstimator(MLE)-basedapproaches (Liu et al.,2023a;Zhong et al.,2022),asopposedto
the value-targeted regression with reward function known. We remark that MLE-based approaches
can be also incorporated within our framework through the use of the discrepancy function:
1
l f′(f,g,ζ t) = P g(s
t+1
s t,a t)/P f∗(s
t+1
s t,a t) 1, (B.2)
2| | | − |
where the trajectory is ζ = (s ,a ,s ) with expectation taken over the next transition state
t t t t+1
s
t+1
from P( ·|s t,a t) such that E ζt[l f′(f,g,ζ t)] = TV P f( ·|s t,a t),P f∗( ·|s t,a t) . To accommodate the
discrepancy function in (B.2), we introduce a natural variant of AGEC defined below.
(cid:0) (cid:1)
Definition 9 (MLE-AGEC). Given hypothesis class , the MLE-discrepancy function l in
f f∈H
H { }
(B.2)andconstantǫ > 0,theMLE-basedaverage-rewardgeneralizedeludercoefficientsMLE-AGEC
( , l ǫ) is defined as the smallest coefficients κ and d such that following two conditions hold
f , G G
H { }
with absolute constants C > 0:
1
(i) (MLE-Bellman dominance) There exists constant d > 0 such that
G
T T
(f )(s ,a ) d sp(V∗) E [l (f ,f ,ζ )] .
E
t t t
≤
G
· k
ζt ft t t i k1
t=1 t=1
X X
(ii) (MLE-Transferability) There exists constant κ > 0 such that for hypotheses f ,...,f ,
G 1 T
if it holds that t−1 E [l (f ,f ,ζ )] √βt for all t [T], then we have ∈ H
i=1k ζi fi t t i k1 ≤ ∈
P
T
E [l (f ,f ,ζ )] poly(logT) κ βT +C sp(V∗)2min κ ,T +2Tǫ2.
k ζt ft t t t k1 ≤ G · 1 · { G }
t=1
X p
20The main difference between the MLE-based variant (see Definition 9) and the original AGEC
(seeDefinition 3)is that the coefficients are defined over ℓ -norm rather than ℓ -norm, and asimilar
1 2
conditionisconsideredinLiu et al.(2023a);Xiong et al.(2023). Now,wearereadytointroducethe
algorithm for the alternative discrepancy function in (B.2) and please see Algorithm 2 for complete
pseudocode. The main modification lies in the construction of confidence set and update condition
Υ . Here, the confidence set now follows
t
= f : (f ,f ) inf (f ,g) β , (f,g) = logP (s′ s,a).
Bt
{
t
∈ H
LDt−1 t t −g∈GLDt−1 t
≤ }
LD
−
g
|
(s,a,s′)∈D
X
In comparison, the update condition follows that Υ = TV P ( s,a),P ( s,a) . Unlike
t (s,a)∈Dt ft ·| gt ·|
the standard Loop algorithm, the theconfidence setand update condition in the MLE-based varint
P (cid:0) (cid:1)
nolongersharesthesameconstruction. Following theliterature ofMLE-basedalgorithms, weadopt
the bracket number to approximation the cardinality of the function class.
Definition 10 (ρ-bracket). Let ρ > 0 and is a set of functions defined over . Under ℓ -norm,
1
F X
a set of functions ( ) is an ρ-bracket of if for any f , there exists a function f′ such
ρ
V F F ∈ F ∈ F
that the following two properties hold: (i) f′(x) f(x) for all x , and (ii) f f′ ρ. The
1
≥ ∈ X k − k ≤
bracketing number (ρ) is the cardinality of the smallest ρ-bracket needed to cover .
F
B F
The theoretical guarantee is provided below.
Theorem B.1 (Cumulative regret). Under Assumptions 1-2 and the discepancy function in (B.2)
with self-completeness such that = , there exists constant c such that for any δ (0,1) and
G H ∈
time horizon T, with probability at least 1 4δ, the regret of Mle-Loop satisfies that
−
Reg(T) sp(V∗) d Tβ ,
≤ O ·
(cid:16) p (cid:17)
whereβ = clog T H(1/T)/δ sp(V∗),d = d G√κ G. Here,(d G,κ G)= MLE-AGEC( , l
f
,1/√T)
B · H { }
denote MLE-AGEC defined in Definition 9 and () denotes the bracketing number.
H
(cid:0) (cid:1) B ·
The proof of Theorem B.1 is similar to that of Theorem 4.1, and can be found in Appendix H.1.
C Concrete Examples
In this section, we present concrete examples of problems for AMDP. We remark that the under-
standing of function approximation problems under the average-reward setting is quite limited, and
to our best knowledge, existing works have primarily focused on linear approximation (Wei et al.,
2021; Wu et al., 2022) and model-based general function approximation (Chen et al., 2022a). Here,
we introduce a variety of function classes with low AGEC. Beyond the examples considered in
existing work, these newly proposed function classes are mostly natural extensions from their coun-
terpart the finite-horizon episode setting (Jin et al., 2020; Zanette et al., 2020; Du et al., 2021;
Domingues et al., 2021), which can be extended to the average-reward problems with moderate
justifications.
C.1 Linear Function Approximation and Variants
Linear function approximation Consider the linear FA, which encompasses a wide range of
concrete problems withstate-action biasfunctionlinear inad-dimensional featuremapping. Specif-
ically, alinear functionclass is defined as = Q(, ) = ω,φ(, ) ,J ω 1sp(V∗)√d ,
H H { · · h · · i ∈ JH |k k2 ≤ 2 }
21where the feature satisfies that φ √2 with first coordinate fixed to 1. We remark that such
2,∞
k k ≤
scaling is without loss of generality as justified in Lemma G.8. To begin with, we first introduce
two specific problems: linear AMDP and AMDP with linear Bellman completion.
Definition 11 (Linear AMDP, Wei et al. (2021)). There exists a known feature mapping φ :
S ×
Rd, an unknown d-dimensional signed measures µ = µ ,...,µ over , and an unknown
1 d
A 7→ S
reward parameter θ Rd, such that the transition kernel the reward function can be written as
∈ (cid:0) (cid:1)
P( s,a) = φ(s,a),µ() , r(s,a)= φ(s,a),θ . (C.1)
·| h · i h i
for all (s,a) . Without loss of generality, we assume that the feature mapping φ satisfies
∈ S ×A
that φ √2 with first coordinate fixed to 1, θ √d and µ( ) √d, where we denote
2,∞ 2 2
k k ≤ k k ≤ k S k ≤
µ( ) = µ ( ),...,µ ( ) and µ ( ) = dµ (s) be the total measure of .
S 1 S d S i S S i S
We r(cid:0)emark that the s(cid:1)caling on the fReature mapping can help in overcoming the gap between
the episodic setting and the average-reward one by ensuring the linear structure of Q- and V-value
function under optimality (Wei et al., 2021). To illustrate the necessity, note that
Q∗(s,a) = r(s,a)+E s′∼P(s,a)[V∗(s′)] J∗ = φ(s,a)⊤ θ J∗e 1+ V∗(s′)dµ(s′) , (C.2)
− −
(cid:18) ZS (cid:19)
wheredenotee = (1,0,...,0) Rd. Next,weprovidetheAMDPswithlinearBellmancompletion,
1
∈
modified from Zanette et al. (2020), which is a more general setting than linear AMDPs.
Definition 12 (LinearBellmancompletion). Thereexistsaknownfeaturemapping φ: Rd
S×A 7→
such that for all (s,a) , ω and J , we have
H H
∈S ×A ∈ W ∈J
hT(ω,J),φ(s,a)
i
:= r(s,a)+E s′∼P(·|s,a) m a′∈a Ax ω⊤φ(s′,a′) −J, (C.3)
(cid:20) (cid:21)
n o
Generalized linear function approximation To introduce the nonlinearity beyond linear FA,
we extend by incorporating a link function. In generalized linear FA, the hypotheses class is defined
as = Q(, ) = σ ω⊤φ(, ) ,J ω √d , where φ(s,a) 1 and σ : R R is an
H 2 2,∞
H { · · · · ∈ J |k k ≤ } k k ≤ 7→
α-bi-Lipschitz function with σ 1sp(V∗). Formally, we say σ is α-bi-Lipschitz continuous if
(cid:0) k(cid:1) k∞ ≤ 2
1
x x′ σ(x) σ(x′) α x x′ , x,x′ R. (C.4)
α ·| − |≤ | − | ≤ ·| − | ∀ ∈
We remark that the generalized linear function class degenerates to the standard linear function
H
class if we choose σ(x) = x. Modified from Wang et al. (2019) for the episodic setting, we define
H
AMDPs with generalized linear Bellman completion as follows.
Definition 13 (Generalized linear Bellman completion). There exists a known feature mapping
φ: Rd such that for all (s,a) , ω and J , we have
H H
S ×A 7→ ∈ S ×A ∈ W ∈ J
σ (ω,J)⊤φ(, ) := r(s,a)+E s′∼P(·|s,a) max σ ω⊤φ(s′,a′) J. (C.5)
T · · a′∈A −
(cid:20) (cid:21)
(cid:16) (cid:17) n (cid:16) (cid:17)o
The proposition below states that (generalized) linear function classes have low AGEC.
Proposition C.1 (Linear FA Low AGEC). Consider linear function class and generalized
Lin
⊂ H
linear function class with a d-dimensional feature mapping φ : Rd, if the problem
Glin
H S ×A 7→
follows one of Definitions 11-13, then it have low AGEC under Bellman discrepancy in (3.1):
d dlog sp(V∗)√dǫ−1 logT , κ dlog sp(V∗)√dǫ−1 .
G G
≤ O ≤ O
(cid:0) (cid:0) (cid:1) (cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
22Linear Q∗/V∗ AMDP Moreover, we consider the linear Q∗/V∗ AMDPs, which is modified from
the one in Du et al. (2021) under the episodic setting.
Definition 14 (Linear Q∗/V∗ AMDP). There exists known feature mappings φ : Rd1,
S ×A 7→
ψ : Rd2, and unknown vectors ω∗ Rd1, θ∗ Rd2 such that optimal value functions follow
S 7→ ∈ ∈
Q∗(s,a) = φ(s,a),ω∗ , V∗(s′) = ψ(s′),θ∗ ,
h i h i
for all (s,a,s′) . Without loss of generality, we assume that features φ √2 and
2,∞
∈S ×A×S k k ≤
ψ √2 with first coordinate fixed to 1, and ω∗ 1sp(V∗)√d and θ∗ 1sp(V∗)√d .
k k2,∞ ≤ k k2 ≤ 2 1 k k2 ≤ 2 2
The proposition below states that linear Q∗/V∗ also has low AGEC.
Proposition C.2 (Linear Q∗/V∗ Low AGEC). Linear Q∗/V∗ AMDPs with coupled (d ,d )-
1 2
⊂
dimensional feature mappings φ: Rd1 and ψ : Rd2 have low AGEC such that
S ×A 7→ S 7→
d d+log sp(V∗)√d+ǫ−1 logT , κ d+log sp(V∗)√d+ǫ−1 ,
G G
≤ O ≤ O
where denote d+ =(cid:0)d 1+d 2(cid:0)as the sum of d(cid:1)imensi(cid:1)ons of featu(cid:0)res. (cid:0) (cid:1)(cid:1)
The proposition above asserts that inlinear Q∗/V∗ AMDPs, with additional structural informa-
tion in state bias function, add ˜(d ) in complexity from the AGEC perspective. We remark that
2
O
linear FA, generalized linear FA, and linear Q∗/V∗ AMDPs are typical value-based problems. The
proof of this proposition relies on ABE dimension as an intermediate, and then uses Lemma 3.2.
C.2 Kernel Function Approximation
In this subsection, we first introduce the notion of effective dimension. With this notion, we prove
a useful proposition that any kernel function class with a low effective dimension has low AGEC.
Consider kernel FA, a natural extension to linear FA from d-dimensional Euclidean space Rd to a
decomposable kernel Hilbert space . Formally, a kernel function class is defined as = Q(, ) =
K H · ·
φ(, ),ω ,J ω sp(V∗)R , where the feature mapping φ : satisfies that
K H K
h · · i ∈ J | k k ≤ S ×A 7→ K (cid:8)
φ 1. To measure the complexity of problems in a Hilbert space with a potentially infinite
K,∞
k k ≤ (cid:9) K
dimension, we introduce the ǫ-effective dimension below.
Definition 15 (ǫ-effective dimension). Consider a set with the possibly infinite elements in a
Z
given separable Hilbert space , the ǫ-effective dimension, denoted by dim ( ,ǫ), is defined as the
eff
K Z
length n of the longest sequence satisfying the condition below:
n
1 1 1
sup logdet I+ z z⊤ .
z 1,...,z n∈Z(n
(cid:16)
ǫ2
Xt=1
i i (cid:17)≤ e )
Here, the concept of ǫ-effective dimension is inspired by the measurement of maximum informa-
tion gain (Srinivas et al., 2009) and is later introduced as a complexity measure of Hilbert space in
Du et al. (2021); Zhong et al. (2022). Similar to Jin et al. (2021), we augment the assumption by
requiring the to be self-complete under average-reward Bellman operator, i.e., = . Next, the
H G H
proposition below demonstrates that kernel FA has low AGEC.
Proposition C.3 (KernelFA LowAGEC). Undertheself-completeness, kernelFAwithfunction
⊂
class concerning a known feature mapping φ: have low AGEC such that
Ker
H S ×A7→ K
d dim ,ǫ/2sp(V∗)R logT, κ dim ,ǫ/2sp(V∗)R ,
G eff G eff
≤ X ≤ X
where denote = φ(s,a) :(cid:0)(s,a) (cid:1)as the collection of fe(cid:0)ature mappings(cid:1).
X { ∈ S ×A}
ThepropositionaboveshowsthatthekernelFAwithalowǫ-effectivedimensionovertheHilbert
space also has low AGEC. As a special case of kernel FA, if we choose = Rd, then we can prove
K
that the RHS in the proposition above is upper bounded by ˜(d).
O
23C.3 Linear Mixture AMDP
In this subsection, we focus on the average-reward linear mixture problem considered in Wu et al.
(2022). In this context, the hypotheses function class is defined as = P(s′ s,a) = θ,φ(s,a,s′) ,
H { | h i
r(s,a) = θ,ψ(s,a) θ 1 with known feature mappings φ: Rd, ψ : Rd,
2
h i|k k ≤ } S×A×S 7→ S×A 7→
and an unknown parameter θ Rd. Specifially, the problem is defined as below.
∈
Definition 16 (Linear mixture AMDPs, Wu et al. (2022)). There exists a known feature mapping
φ: Rd, ψ : Rd, and an unknown vector θ Rd, it holds that
S ×A×S 7→ S ×A 7→ ∈
P(s′ s,a) = θ,φ(s,a,s′) , r(s,a) = θ,ψ(s,a) ,
| h i h i
for all (s,a,s′) . Without loss of generality, we assume φ √d and ψ √d.
2,∞ 2,∞
∈ S×A×S k k ≤ k k ≤
Now we show that the linear mixture problem is tractable under the framework of AGEC.
Proposition C.4 (Linear mixture Low AGEC). Consider linear mixture problem with hypothe-
⊂
ses class and d-dimensional feature mappings (φ,ψ). If we choose discrepancy function as
H
l f′(f,g,ζ t)= θ g⊤ ψ(s t,a t)+ φ(s t,a t,s′)V f′(s′)ds′ −r(s t,a t) −V f′(s t+1), (C.6)
(cid:18) ZS (cid:19)
and takes = with operator following (f)= f∗ for all f , it has low AGEC such that
H G P ∈H
d dlog sp(V∗)T/√dǫ , κ dlog sp(V∗)T/√dǫ .
G G
≤ O ≤ O
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
The proposition posits that AGEC can capture the linear mixture AMDP, based on a modified
version of the Bellman discrepancy function in (2.2). In contrast to the linear FA discussed in
AppendixC.1,thepresenceoftheaverage-reward terminthismodel-basedproblemdoesnotimpose
any additional computational or statistical burden, and there is no need for structural assumptions
on feature mappings, such as a fixed first coordinate, considering discrepancy in (C.6).
D Proof of Main Results for Loop
D.1 Proof of Theorem 4.1
Proof of Theorem 4.1. Note that the regret can be decomposed as
T T
Reg(T)= J∗ r(s ,a ) J r(s ,a ) (optimism)
t t t t t
− ≤ −
Xt=1(cid:16) (cid:17) Xt=1(cid:16) (cid:17)
T T
( =a) E(f t)(s t,a t)
−
E st+1∼P(·|st,at) Q t(s t,a t) −m a∈a Ax Q t(s t+1,a)
Xt=1 Xt=1 h i
T T
( =b) E(f t)(s t,a t)+ E st+1∼P(·|st,at)[V t(s t+1)] −V t(s t) , (D.1)
Xi=1 Xt=1h i
Bellman error Realization error
where step (a) and ste|p (b) fo{lzlow th}e de|finition of the Bel{lzman optimality op}erator and the greedy
policy. Below, we will present the bound of Bellman error and Realization error respectively.
24Step 1: Bound over Bellman error Recall that the of confidence set ensures that (f ,f )
LDt−1 t t
inf (f ,g) (β) across all steps. Using the concentration arguments, we can infer
−
g∈G LDt−1 t
≤ O
t−1
E [l (f ,f ,ζ )] 2 (β), (D.2)
k ζi fi t t i k2 ≤ O
i=1
X
with high probability and the formal statements are deferred to Lemma D.2 in Appendix D.3. In
the following arguments, we assume the above event holds. Take ǫ = 1/√T, recall the definition of
dominance coefficient d in AGEC( , ,l,ǫ) and it directly indicates that
G
H J
T T t−1 1/2
Bellman error = (f )(s ,a ) d E [l (f ,f ,ζ)] 2 + sp(V∗) d T ,
E t t t ≤ G k ζi fi t t k2 O G
" #
Xt=1 Xt=1 Xi=1 (cid:16) p (cid:17)
and thus the Bellman error can be upper bounded by sp(V∗) d βT .
G
O
(cid:16) p (cid:17)
Step 2: Bound over Realization error To bound Realization error, we use the concentration
argument and the upper-boundded switching cost. Note that
T
Realization error ( =c) [V (s ) V (s )]+ sp(V∗) T log(1/δ) ,
t t+1 t t
− O
t=1
X (cid:0) p (cid:1)
T
= V (s ) V (s ) + sp(V∗) T log(1/δ) , (Shift)
τt t+1
−
τt+1 t+1
O
t=1
X(cid:2) (cid:3) (cid:0) p (cid:1)
T
= V (s ) V (s ) 1(τ = τ )+ sp(V∗) T log(1/δ)
τt t+1
−
τt+1 t+1 t
6
t+1
O
t=1
X(cid:2) (cid:3) (cid:0) p (cid:1)
(d)
sp(V∗) (T)+ sp(V∗) T log(1/δ) sp(V∗) κ T log(1/δ) ,
G
≤ ·N O ≤ O ·
(D.3)
(cid:0) p (cid:1) (cid:0) p (cid:1)
where step (c) directly follows the Azuma-Hoeffding inequality and step (d) is based the fact that
V V sp(V∗) and the bounded switching cost such that (T) (κ logT), where κ
k
τt− τt+1k∞
≤ N ≤ O
G G
is the transferability coefficient in AGEC with ǫ = 1/√T. Please refer to Lemma D.3 in Appendix
D.4 for the detailed statement and proof of the bounded switching cost.
Step 3: Combine the bounded erroes Plugging (D.2) and (D.3) back into (D.1), we have
Reg(T) Bellman error+ Realization error
≤
sp(V∗) d βT + sp(V∗)κ T log(1/δ) = sp(V∗) d Tβ ,
G G
≤ O O O ·
(cid:16) p (cid:17) (cid:0) p (cid:1) (cid:16) p (cid:17)
where d= max √d ,κ is a function of (d ,κ ) = AGEC( , l ,1/√T). In the arguments
G G G G f f∈H
{ } H { }
above, the optimistic parameter is chosen as β = clog T 2 (1/T)/δ sp(V∗), which takes the
NH∪G ·
upper bound of the optimistic parameters, aligning with the choice in Lemma D.1, Lemma D.2, and
(cid:0) (cid:1)
Lemma D.3. Then finish the proof of cumulative regret for Loop in Algorithm 1. 2
25D.2 Proof of Lemma D.1
Lemma D.1 (Optimism). Under Assumptions 1-4, Loop is an optimistic algorithm such that it
ensures J J∗ for all t [T] with probability greater than 1 δ.
t
≥ ∈ −
Proof of Lemma D.1. Denote ( ) be the ρ-cover of and (ρ) be the size of ρ-cover ( ).
ρ G ρ
V G G N V G
Consider fixed (i,g) [T] and define the auxiliary function
∈ ×G
X (g) := l (f∗,g,ζ ) 2 l (f∗,f∗,ζ ) 2, (D.4)
i,fi k fi i k2−k fi i k2
where f∗ is the optimal hypothesis in value-based problems and the true hypothesis in model-based
ones. Let F be the filtration induced by s ,a ,...,s ,a and note that f ,...,f is fixed under
t 1 1 t t 1 t
{ }
the filtration, then we have
E[X (g)F ]= E [ l (f∗,g,ζ ) 2 l (f∗, (f∗),ζ ) 2 F ]
i,fi | i ζi k fi i k2−k fi P i k2| i
= E l (f∗,g,ζ ) l (f∗, (f∗),ζ ) l (f∗,g,ζ )+l (f∗, (f∗),ζ ) F
ζi fi i
−
fi
P
i
·
fi i fi
P
i i
h (cid:12) i
= E E(cid:2) l (f∗,g,ζ ) l (f∗,g,ζ )+(cid:3) l(cid:2) (f∗, (f∗),ζ ) F (cid:3)(cid:12)
ζi ζi fi i · fi i fi P i i (cid:12)
= kE ζh
i
l fi(cid:2)(f∗,g,ζ i) k2 2(cid:3), (cid:2) (cid:3)(cid:12) (cid:12)
(cid:12)
i
where the equation follow(cid:2)s the definit(cid:3)ion of generalized completeness (see Assumption 4):
E ζi[l f′(f,g,ζ)] = l f′(f,g,ζ) −l f′(f, P(f),ζ),
(E ζi[l f′(f,g,ζ)] = E
ζi
l f′(f,g,ζ)+l f′(f, P(f),ζ) .
Similarly, we can obtain that the second mom(cid:2)ent of the auxiliary function(cid:3) is bounded by
E[X (g)2 F ] sp(V∗)2 E [l (f∗,g,ζ )] 2 ,
i,fi | i ≤ O k ζi fi i k2
(cid:16) (cid:17)
By Freedman’s inequality (see Lemma G.7), with probability greater than 1 δ it holds that
−
t t
X (g) E [l (f∗,g,ζ )] 2
i,fi − k ζi fi i k2
(cid:12)Xi=1 Xi=1 (cid:12)
(cid:12) (cid:12)
(cid:12) t (cid:12)
log(1/δ) sp(V∗)2 E [l (f∗,g,ζ )] 2+log(1/δ) .
≤ Ov · k ζi fi i k2 
u i=1
u X
t 
Bytakingunionboundover[T] ( ),forany(t,φ) [T] ( )wehave t X (φ) (ζ),
where ζ = sp(V∗)log(T
(ρ)× /δV )ρ aG
nd we use the
f∈
act
t× haV tρ G
E [l
(f∗,g−
,ζ
)]i= 21 isi,f ni on-n≤ egaO
tive.
NG k ζi fi Pi k2
Recall the definition of ρ-cover, it ensures that for any g , there exists φ ( ) such that
ǫ
∈ G ∈ V G
g(s,a) φ(s,a) ρ for all (s,a) . Therefore, for any g we have
1
k − k ≤ ∈ S ×A ∈ G
t
X (g) ζ +tρ . (D.5)
−
i,fi
≤ O
Xi=1 (cid:16) (cid:17)
Combine the (D.5) above and the designed confidence set, then for all t [T] it holds that
∈
t−1
(f∗,f∗) inf (f∗,g) = X (g˜) ζ +tρ < β, (D.6)
LDt−1
−g∈G
LDt−1
−
i,fi
≤ O
Xi=1 (cid:16) (cid:17)
where g˜ is the local minimizer to (f∗,g), and we take the covering coefficient as ρ = 1/T
LDt−1
and optimistic parameter as β = clog T 2 (1/T)/δ sp(V∗). Based on (D.6), with probability
NH∪G ·
greater than 1 δ, f∗ is a candidate of the confidence set such that J J∗ for all t [T]. 2
t
− (cid:0) (cid:1) ≥ ∈
26D.3 Proof of Lemma D.2
Lemma D.2. Forfixed ρ > 0and theoptimistic parameter β = c sp(V∗) log T 2 (ρ)/δ +Tρ
· NH∪G
where c > 0 is constant large enough, then it holds that
(cid:0) (cid:0) (cid:1) (cid:1)
t−1
E l (f ,f ,ζ ) 2 (β), (D.7)
ζik fi t t i
k ≤ O
i=1
X
for all t [T] with probability greater than 1 δ.
∈ −
Proof of Lemma D.2. Denote ( ) be the ρ-cover of and (ρ) be the size of ρ-cover ( ).
ρ H ρ
V H H N V H
Consider fixed (i,f) [T] and define the auxiliary function
∈ ×H
2 2
X (f) := l f,f,ζ l f, (f),ζ ,
i,fi fi i 2− fi P i 2
Let F
t
be the filtration induced by (cid:13) (cid:13)s 1,(cid:0) a 1,...,(cid:1) s(cid:13) (cid:13)t,a
t
(cid:13) (cid:13)an(cid:0) d note tha(cid:1) t(cid:13) (cid:13)f 1,...,f
t
is fixed under the
{ }
filtration, then we have
E[X (f)F ] = E [ l (f,f,ζ ) 2 l (f, (f),ζ ) 2 F ]
i,fi | i ζi k fi i k2−k fi P i k2| i
= E l (f,f,ζ ) l (f, (f),ζ ) l (f,f,ζ )+l (f, (f),ζ ) F
ζi fi i
−
fi
P
i
·
fi i fi
P
i i
= E ζih l(cid:2) fi(f,f,ζ i) ·E ζi l fi(f,f,ζ i)+(cid:3) l f(cid:2) i(f, P(f),ζ i) F i (cid:3)(cid:12) (cid:12)
(cid:12)
i
= E ζ(cid:2)
i
l fi(f,f,ζ i(cid:3)) 2 2,(cid:2) (cid:12)
(cid:12)
(cid:3)
(cid:13) (cid:2) (cid:3)(cid:13)
where the equation gen(cid:13)eralized complete(cid:13)ness (see Lemma D.1). Similarly, we can obtain that the
second moment of the auxiliary function is bounded by
E[X (f)2 F ] sp(V∗)2 E l (f,f,ζ ) 2 ,
i,fi | i ≤ O ζi fi i 2
(cid:16) (cid:13) (cid:2) (cid:3)(cid:13) (cid:17)
By Freedman’s inequality in Lemma G.7, with probab(cid:13)ility greater than(cid:13) 1 δ we have
−
t t
X (f) E l (f,f,ζ ) 2
i,fi − ζi fi i 2
(cid:12) (cid:12)Xi=1 Xi=1
(cid:13) (cid:2) (cid:3)(cid:13)
(cid:12)
(cid:12)
(cid:13) (cid:13)
(cid:12) t (cid:12)
log(1/δ) sp(V∗)2 E l (f,f,ζ ) 2 +log(1/δ) .
≤Ov · ζi fi i 2 
u i=1
u X(cid:13) (cid:2) (cid:3)(cid:13)
t (cid:13) (cid:13) 
Define ζ = sp(V∗)log(T (ρ)/δ), by taking a union bound over ρ-covering of hypothesis set , we
H
N H
can obtain that with probability greater than 1 δ, for all (t,φ) [T] ( ) we have
ρ
− ∈ ×V H
t t
X (φ) E l (φ,φ,ζ ) 2
i,fi − k ζi fi i k2
(cid:12) (cid:12)Xi=1 Xi=1
(cid:2) (cid:3)
(cid:12)
(cid:12)
(cid:12) t (cid:12)
ζ sp(V∗)2 E l (φ,φ,ζ ) 2+ζ . (D.8)
≤ Ov · k ζi fi i k2 
u i=1
u X (cid:2) (cid:3)
t 
27The following analysis assumes that the event above is true. Recall that the Loop ensures that
t−1 t−1 t−1
X (f )= l (f ,f ,ζ ) 2 l f , (f ),ζ 2,
i,fi t k fi t t i k2− k fi t P t i k2
i=1 i=1 i=1
X X X (cid:0) (cid:1)
t−1 t−1
2 2
l f ,f ,ζ inf l f ,g,ζ ,
≤ fi t t i 2−g∈G fi t i 2
i=1 i=1
X(cid:13) (cid:0) (cid:1)(cid:13) X(cid:13) (cid:0) (cid:1)(cid:13)
= (cid:13) (f ,f ) in(cid:13)f (f ,g(cid:13)) (β), (cid:13) (D.9)
LDt−1 t t
−g∈G
LDt−1 t
≤ O
where the last inequality is based on the confidence set and the update condition combined. Note
that if the update is executed at time t, the confidence set ensures that
(f ,f ) inf (f ,g) β,
LDt−1 t t
−g∈G
LDt−1 t
≤
within the update step t. Otherwise, if the update condition is not triggered, we have f = f and
τt t
Υ = (f ,f ) inf (f ,g) 4β.
t−1 LDt−1 t t
−g∈G
LDt−1 t
≤
Recall that based on the definition of ρ-cover for any f , there exists φ ( ) such that
ρ
∈ H ∈ V H
g(s,a) φ(s,a) ρ for all (s,a) , we have the in-sample training error is bounded by
1
k − k ≤ ∈ S ×A
t−1 t−1
E l (f ,f ,ζ ) 2 E l (φ ,φ ,ζ ) 2 + (tρ), (ρ-approximation)
ζi fi t t i 2 ≤ ζi fi t t i 2 O
i=1 i=1
X(cid:13) (cid:2) (cid:3)(cid:13) X(cid:13) (cid:2) (cid:3)(cid:13)
(cid:13) (cid:13) t−1 (cid:13) (cid:13)
= X (φ )+ (tρ+ζ) ( (D.8))
i,fi t
O
i=1
X
t−1
= X (f )+ (tρ+ζ) (Tρ+ζ +β) = (β), (D.10)
i,fi t
O ≤ O O
i=1
X
where the last inequality follows (D.9), and takes β = c (sp(V∗)log T 2 (ρ)/δ +Tρ . 2
NH∪G
(cid:0) (cid:0) (cid:1) (cid:1)
D.4 Proof of Lemma D.3
Lemma D.3. Let (T) be the switching cost with time horizon T, defined as
N
(T) = # t [T]: τ = τ .
t t−1
N { ∈ 6 }
Given fixed ρ > 0 and the optimistic parameter β = c sp(V∗)log T 2 (ρ)/δ +Tρ , where c > 0
NH∪G
is large enough constant, then with probability greater than 1 2δ we have
(cid:0) −(cid:0) (cid:1) (cid:1)
(T) κ logT +β−1Tǫ2 ,
G
N ≤ O
where κ G is the transferability coefficient wi(cid:0)th respect to AGEC(cid:1)( , l f′ ,ǫ).
H { }
Proof of Lemma D.3. Denote ( ) be the ρ-cover of and (ρ) be the size of ρ-cover ( ).
ρ H ρ
V H H N V H
Step 1: Bound the difference of discrepancy between the minimizer and (f).
P
Consider fixed tuple (i,f,g) [T] and define auxiliary function as
∈ ×H×G
2 2
X (f,g) := l f,g,ζ l f, (f),ζ
i,fi fi i 2− fi P i 2
(cid:13) (cid:0) (cid:1)(cid:13) (cid:13) (cid:0) (cid:1)(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
28Let F be the filtration induced by s ,a ,...,s ,a and note that f ,...,f is fixed under the
t 1 1 t t 1 t
{ }
filtration, then we have
E[X (f,g)F ]= E [ l f,g,ζ 2 l f, (f),ζ 2 F ]
i,fi | i ζi fi i 2− fi P i 2| i
= E (cid:13)l (cid:0)(f,g,ζ )(cid:1)(cid:13) l ((cid:13)f, (cid:0)(f),ζ ) (cid:1)l(cid:13)(f,g,ζ )+l (f, (f),ζ ) F
ζi (cid:13) fi i (cid:13)− fi(cid:13) P i · (cid:13)fi i fi P i i
= E ζih l(cid:2) fi(f,g,ζ i) ·E ζi l fi(f,g,ζ i)+(cid:3) l f(cid:2) i(f, P(f),ζ i) F i (cid:3)(cid:12) (cid:12)
(cid:12)
i
= E ζ(cid:2)
i
l fi(f,g,ζ i(cid:3)) 2 2,(cid:2) (cid:12)
(cid:12)
(cid:3)
where the equation gener(cid:13)alize(cid:2)d complete(cid:3)n(cid:13)ess (see Lemma D.1). Similarly, we can obtain that the
(cid:13) (cid:13)
second moment of the auxiliary function is bounded by
E[X (f,g)2 F ] sp(V∗)2 E l (f,g,ζ ) 2 ,
i,fi | i ≤ O ζi fi i 2
(cid:16) (cid:13) (cid:2) (cid:3)(cid:13) (cid:17)
By Freedman’s inequality in Lemma G.7, with probabi(cid:13)lity greater than(cid:13)1 δ
−
t t
X (f,g) E l (f,g,ζ ) 2
i,fi − ζi fi i 2
(cid:12) (cid:12)Xi=1 Xi=1
(cid:13) (cid:2) (cid:3)(cid:13)
(cid:12)
(cid:12)
(cid:13) (cid:13)
(cid:12) t (cid:12)
log(1/δ) sp(V∗)2 E l (f,g,ζ ) 2 +log(1/δ)
≤ Ov · ζi fi i 2 
u i=1
u X(cid:13) (cid:2) (cid:3)(cid:13)
t (cid:13) (cid:13) 
Define ζ = sp(V∗)log(T 2 (ρ)/δ), by taking a union bound over ρ-covering of hypothesis set
NH∪G
, with probability greater than 1 δ, for all (t,φ,ϕ) [T] ( ) ( ) it holds
ρ ρ
H×G − ∈ ×V H ×V G
t t
X (φ,ψ) E l (φ,ψ,ζ ) 2
i,fi − ζi fi i 2
(cid:12) (cid:12)Xi=1 Xi=1
(cid:13) (cid:2) (cid:3)(cid:13)
(cid:12)
(cid:12)
(cid:13) (cid:13)
(cid:12) t (cid:12)
ζ sp(V∗)2 E l (φ,ψ,ζ ) 2 +ζ , (D.11)
≤ Ov · ζi fi i 2 
u i=1
u X(cid:13) (cid:2) (cid:3)(cid:13)
t (cid:13) (cid:13) 
where ζ = sp(V∗)log(T 2 (ρ)/δ). Note that E l (φ,ψ,ζ ) 2 is non-negative, then it holds
NH∪G ζi fi i 2
that t X (φ,ϕ) (ζ) for all t [T]. Based on (D.11) and the ρ-approximation, we have
− i=1 i,fi ≤O ∈ (cid:13) (cid:2) (cid:3)(cid:13)
(cid:13) (cid:13)
P t
X (f,g) ζ +tρ , t [T],
−
i,fi
≤ O ∀ ∈
Xi=1 (cid:16) (cid:17)
for any (f,g) . Recall that β = clog(T 2 (ρ)/δ)sp(V∗), for all t [T] we have
∈ H×G NH∪G ∈
t t
(f , (f )) inf (f ,g) = l (f , (f ),ζ ) 2 inf l (f ,g,ζ ) 2
LDt t P t −g∈G LDt t k fi t P t i k2−g∈G k fi t i k2
i=1 i=1
X X
t
= X (f ,g˜) ζ +tρ β. (D.12)
−
i,fi t
≤ O ≤
Xi=1 (cid:16) (cid:17)
Combine (D.12), and the fact that g is defined as the local minimizer among auxiliary class and
G
(f ) , then for all t [T] we have the difference of discrepancy bounded by
t
P ∈ G ∈
0 (f , (f )) inf (f ,g) β. (D.13)
≤
LDt t PJt t
−g∈G
LDt t
≤
29Step 2: Bound the out-sample training error between updates.
Consider an update is executed at step t+1, it directly implies that (f ,f ) inf (f ,g) >
LDt t t
−
g∈G LDt t
4β, while the latest update atstep τ ensures that (f ,f ) inf (f ,g) β, where
t LDτt−1 τt τt
−
g∈G LDτt−1 τt
≤
τ is the pointer of the lastest update. Combined the results above with (D.13), we have
t
(f ,f ) f , (f ) > 3β, (f ,f ) f , (f ) β. (D.14)
LDt t t −LDt t
P
t LDτt−1 τt τt −LDτt−1 τt
P
τt
≤
(cid:0) (cid:1) (cid:0) (cid:1)
It indicates that the sum of squared empirical discrepancy between two adjacent updates follows
t
l (f ,f ,ζ ) 2 = (f ,f ) (f , (f )) > 2β, (D.15)
k ft t t t k2 LDτt:t t t −LDτt:t t PJt t
i X=τt
where denote = / . Based on the similar concentration arguments as Lemma D.2, we
have the out-saD
mτt p:t
le trD
ait
niD
ngτt
error between updates is bounded by t E [l (f ,f ,ζ )] 2 > β.
i=τtk ζi fi i i i k2
Step 3: Bound the switching cost under the transferabilityPconstraint.
Denote b ,...,b ,b be the sequence of updated steps such that τ b for all t [T],
1 N(T) N(T)+1 t t
∈ { } ∈
and we fix the recorder b = 1 and b = T + 1. Note that based on (D.15), the sum of
1 N(T)+1
out-sample training error shall have a lower bound such that
T N(T)bu+1−1
E [l (f ,f ,ζ )] 2 = E [l (f ,f ,ζ )] 2 (T) β. (D.16)
k ζt ft t t t k2 k ζt ft t t t k2 ≥N ·
Xt=1 Xu=1 t X=bu
Besides, note that the in-sample training error t−1 E [l (f ,f ,ζ )] 2 β for all t [T]
i=1k ζt fi t t t k2 ≤ O ∈
and based on the definition of transferability coefficient κ (see Definition 3), we have
G
P (cid:0) (cid:1)
T
E [l (f ,f ,ζ )] 2 κ βlogT +sp(V∗)2min κ ,T +Tǫ2 (D.17)
k ζt ft t t t k2 ≤ O G · { G }
t=1
X (cid:0) (cid:1)
Combine (D.16) and (D.17), it holds (T) κ logT +β−1Tǫ2 and finish the proof. 2
G
N ≤ O
(cid:0) (cid:1)
E Proof of Results about Complexity Measures
Inthissection,weprovidetheproofofresultsaboutthecomplexity metricsinSection3. Weremark
that the proof highly relies on Lemma G.2 and Lemma G.3, which are natural extentions to original
results in Jin et al. (2020); Zhong et al. (2022) and proofs are provided in Section G.1.
E.1 Proof of Lemma 3.1
Proof of Lemma 3.1. Recall that the eluder dimension is defined over the function class following
H
:= X f,f′(s,a) = r
f
+P f′V
f
(s,a) : f,f′ ,
X ∈ H
(cid:8) (cid:0) (cid:1) (cid:9)
and for model-based problems, the discrepancy function is chosen as
l f′(f,g,ζ t) = r g +P gV f′ (s t,a t) r(s t,a t) V f′(s t+1).
− −
(cid:0) (cid:1)
Step 1: Bound over transferability coefficient.
30Start with the transferability coefficient, the condition can be equivalently written as
t−1 t−1
kE ζi[l fi(f t,f t,ζ i)] k2 2 = k r ft +P ftV ft −r f∗ +P f∗V ft (s i,a i) k2 2
i=1 i=1
X X (cid:0) (cid:1)
t−1
= X ft,ft −X ft,f∗ (s i,a i)2
≤
β, ∀t
∈
[T]. (E.1)
i=1
X(cid:0) (cid:1)
Let ˘ = f f′ : f,f′ , the generalized pigeon-hole principle (see Lemma G.2) indicates
H H
X { − ∈ X }
that if we take Γ = D∆, φ
t
= X
ft,ft
−X ft,f∗, Φ = X˘ H, kφ
t k∞
≤
sp(V∗)+2, then it holds that
t t
kE ζi[l fi(f i,f i,ζ i)] k2 = X fi,fi −X fi,f∗ (s i,a i)2
i=1 i=1
X X(cid:0) (cid:1)
dim ( ˘ , ,ǫ) βlogt+(sp(V∗)+2)2min dim ( ˘ , ,ǫ),t +tǫ2
DE H ∆ DE H ∆
≤ X D · { X D }
= dim ( ,ǫ) βlogt+(sp(V∗)+2)2min dim ( ,ǫ),t +tǫ2, (E.2)
E H E H
X · { X }
given condition that (E.1) holds for all t [T], where the last equation uses dim ( ˘ , ,ǫ) =
DE H ∆
∈ X D
dim ( ,ǫ). Denote d = dim ( ,ǫ), then we have κ d based on (E.2).
E H E E H G E
X X ≤
Step 2: Bound over dominance coefficient.
Based on Lemma G.3 and (f )(s ,a ) = E l (f ,f ,ζ ) based on definition, it holds that
E
t t t ζt ft t t t
(cid:2) (cid:3)
T T
kE ζt l ft(f t,f t,ζ t) k2 2 = X ft,ft −X ft,f∗ (s t,a t) 2
t=1 t=1
X (cid:2) (cid:3) X(cid:2)(cid:0) (cid:1) (cid:3)
1/2
T t−1
≤
2d DElogT X ft,ft −X ft,f∗ (s i,a i) 2 +(sp(V∗)+2)min {d DE,T }+Tǫ
" #
t=1 i=1
XX(cid:2)(cid:0) (cid:1) (cid:3)
1/2
T t−1
= 2d logT E [l (f ,f ,ζ )] 2 +(sp(V∗)+2)min d ,T +Tǫ, (E.3)
E ζi fi t t i 2 { E }
" #
t=1 i=1
XX(cid:13) (cid:13)
(cid:13) (cid:13)
by taking Γ = D∆, φ
t
= X
ft,ft
−X ft,f∗, Φ = X˘ H, kφ
t k∞
≤
sp(V∗)+2, and 1+logT
≤
2logT. 2
E.2 Proof of Lemma 3.2
Proof of Lemma 3.2. Consider the Bellman discrepancy function, defined as
l f′(f,g,ζ t)= Q g(s t,a t) r(s t,a t) V f(s t+1)+J g,
− −
and the expectation is taken over s from P( s ,a ) such that E [l (f ,f ,ζ )] = (f )(s ,a ).
i+1
·|
i i ζi fi t t i
E
t i i
Step 1: Bound over transferability coefficient.
First,we’regoingtodemonstratethetransferability. Notethatthegeneralized pigeon-holeprinciple
(see Lemma G.2) directly indicates that, given
t−1 t−1
(f )(s ,a ) 2 = E [l (f ,f ,ζ )] 2 β, t [T],
kE t i i k2 k ζi fi t t i k2 ≤ ∀ ∈
i=1 i=1
X X
31if we take φ = (f ), Φ = and Γ = , then for all t [T] we have
t t H ∆
E E D ∈
t
(f )(s ,a ) 2 d βlogt+(sp(V∗)+2)2min d ,t +tǫ2, (E.4)
kE i i i k2 ≤ ABE · { ABE }
i=1
X
and thus we upper bound κ d := dim ( ,ǫ).
G ABE ABE
≤ H
Step 2: Bound over dominance coefficient.
Based on Lemma G.3 and E [l (f ,f ,ζ )] = (f )(s ,a ), it holds that
ζi fi t t i
E
t i i
T T t−1 1/2
(f )(s ,a ) 2d logT (f )(s ,a ) 2 +(sp(V∗)+2)min d ,T +Tǫ,
E t t t ≤ ABE kE t i i k2 { ABE }
" #
t=1 t=1 i=1
X XX
where we take φ = (f ), Φ = , Γ = , (f ) sp(V∗)+2, and 1+logT 2logT. 2
t t H ∆ t ∞
E E D kE k ≤ ≤
F Proof of Results for Concrete Examples
In this section, we provide detailed proofs of results for concrete examples in Appendix C.
F.1 Proof of Proposition C.1
Proof of Proposition C.1. To show that linear FA has low AGEC, we first prove that it is captured
byABEdimensiondim ( ,ǫ), andthenapplyLemma3.2(lowABEdim lowAGEC).Suppose
ABE
H ⊆
that there exists ǫ′ ǫ, δ , and f with length m N, such that
≥ {
si,ai}i∈[m]
⊆
D∆
{
i }i∈[m]
⊆ H ∈
t−1
(f )(s ,a ) 2 ǫ′, (f )(s ,a ) > ǫ′, t [m]. (F.1)
v E t i i ≤ E t t t ∀ ∈
ui=1
uX(cid:2) (cid:3) (cid:12) (cid:12)
t (cid:12) (cid:12)
Based on Definitions 7-8, dim ( ,ǫ) is the largest m. Following this, we provide detailed discus-
ABE
H
sion about linear AMDPs, AMDPs with linear Bellmen completion, and AMDPs with generalized
linear completion (see Definition 11-13). Denote Q (s,a) = φ(s,a)⊤ω for all (s,a,t) [m].
t t
∈ S×A×
(i). Linear AMDPs. As defined in Definition 11, for any f , it holds that
t
∈ H
(f )(s ,a ) = φ(s ,a )⊤ω φ(s ,a )⊤θ φ(s ,a )⊤ V (s)dµ(s)+J
t i i i i t i i i i t t
E − −
ZS
= φ(s ,a )⊤ ω θ+ V (s)dµ(s)+J e . (F.2)
i i t t t 1
−
(cid:18) ZS (cid:19)
(ii). AMDPs with linear Bellmen completion. As a natural extension to the linear AMDPs,
linear Bellmen completeness (see Definition 12) suggests that the Bellman error follows
(f )(s ,a )= φ(s ,a )⊤ω φ(s ,a )⊤ (ω ,J )= φ(s ,a )⊤(ω (ω ,J )). (F.3)
t i i i i t i i t t i i t t t
E − T −T
32(iii). AMDPs with generalized linear completion. Moreover, AMDPs with generalized
linear completion further extends the standard linear FA by introducing link functions. Note that
(f )(s ,a ) = σ φ(s ,a )⊤ω σ φ(s ,a )⊤ (ω ,J ) (F.4)
t i i i i t i i t t
E − T
(cid:0) (cid:1) (cid:0) (cid:1)
and based on the α-bi-Lipschitz continuity condition in (C.4), it holds that
1
φ(s ,a )⊤(ω (ω ,J )) (f )(s ,a ) α φ(s ,a )⊤(ω (ω ,J )). (F.5)
i i t t t t i i i i t t t
α · −T ≤ E ≤ · −T
Based on the Lemma G.4 and arguments in (i), (ii) and(iii), we are ready to provide a unified
proof for linear FA. By substituting the arguments (F.2), (F.3) and (F.5) into (F.1), then
t−1 ǫ′
[ φ(s ,a ),ω (ω ,J ) ]2 αǫ′, φ(s ,a ),ω (ω ,J ) > , t [m]. (F.6)
v h i i t −T t t i ≤ |h t t t −T t t i| α ∀ ∈
ui=1
uX
t
Here, we take α = 1 for standard linear FA , and let α be the Lipschitz constant for generalized
linear FA. Based on Lemma G.4, if we take φ = φ(s ,a ), ψ = ω (ω ,J ),B = √2,B =
t t t t t t t φ ψ
− T
sp(V∗)√d, ε = ǫ, c = α, c = α−1, then m dlog(sp(V∗)√d/ǫ) . As the ABE dimension is
1 2
≤ O
defined as the length of the longest sequence satisfying (F.6), thus
(cid:0) (cid:1)
dim ( ,ǫ) dlog sp(V∗)√d/ǫ .
ABE
H ≤ O
Based on Lemma 3.2, d dlog sp(V∗)√dǫ(cid:0) −1 log(cid:0) T and κ (cid:1)(cid:1) dlog sp(V∗)√dǫ−1 . 2
G G
≤ O ≤O
(cid:0) (cid:0) (cid:1) (cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
F.2 Proof of Proposition C.2
Proof of Proposition C.2. For all f , the Bellman error can be written as
t
∈ H
(f )(s ,a )= φ(s ,a )⊤ω E[ψ(s )]⊤θ +J r(s ,a )
t i i i i t i+1 t t i i
E − −
= φ(s ,a )⊤ω E[ψ(s )]⊤θ +J (Q∗(s ,a ) E[V∗(s )] J∗)
i i t i+1 t t i i i+1
− − − −
φ(s ,a ) ⊤ ω ω∗
= E[ψ(i s i+i 1)] θt ∗− θ
t
+(J t −J∗) ·e 1 , (F.7)
(cid:20) (cid:21) (cid:18)(cid:20) − (cid:21) (cid:19)
where the second equation results from Bellman optimality equation in (2.1). Following a similar
argument in the proof of Proposition C.1, we can show that the linear Q∗/V∗ AMDPs have a low
ABE dimension with an (d +d )-dimensional compound feature mapping equivalently based on
1 2
(F.7). Based on Lemma 3.2 and write d+ = d +d , then we have
1 2
d d+log sp(V∗)√d+ǫ−1 logT , κ d+log sp(V∗)√d+ǫ−1 . 2
G G
≤ O ≤ O
(cid:0) (cid:0) (cid:1) (cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
F.3 Proof of Proposition C.3
Proof of Proposition C.3. Similar to linear FA, we will show that kernel FA is captured by by ABE
dimension dim ( ,ǫ), and then apply Lemma 3.2 (low ABE dim low AGEC). Suppose that
ABE
H ⊆
there exists ǫ′ ǫ, δ , and f with length m N, such that
≥ {
si,ai}i∈[m]
⊆
D∆
{
i }i∈[m]
⊆ H ∈
t−1
(f )(s ,a ) 2 ǫ′, (f )(s ,a ) > ǫ′, t [m]. (F.8)
v E t i i ≤ E t t t ∀ ∈
ui=1
uX(cid:2) (cid:3) (cid:12) (cid:12)
t (cid:12) (cid:12)
33Suppose the kernel function class has a finite ǫ-effective dimension concerning the feature mapping
φ. The existence of Bellman error (f ) is equivalent to the one of W ( ):
t t
E ∈ W −W
(f )(, ) = (Q Q )(, ) = φ(, ),ω ω′ := φ(, ),W , (F.9)
E
t
· ·
ft −TJt ft
· · h · ·
t
−
tiK
h · ·
t iK
where the second equation is based on the self-completeness assumption with kernel FA such that
= . Denote X = φ(s ,a ), we can rewrite the condition in (F.8) as
t t t
G H
t−1
(X⊤W )2 ǫ′, X⊤W > ǫ′, t [m]. (F.10)
v i t ≤ | t t | ∀ ∈
ui=1
uX
t
Let Σ t = t i=− 11X iX i⊤+(ǫ′2/4R2 ·sp(V∗)2) ·I, then kW t kΣt
≤
√2ǫ′ and ǫ′ ≤kW t kΣtkX t kΣ− t1 for all
t [m] based on Cauchy-Swartz inequlity and ω sp(V∗)R. Thus, X 2 0.5 and
∈ P k t kK ≤ k t kΣ−1 ≥
t
m detΣ 4R2sp(V∗)2 m
log 1+ X 2 = log m+1 = logdet I+ X X⊤ , (F.11)
Xt=1 (cid:16)
k t kΣ− t1
(cid:17) (cid:18)
detΣ
1 (cid:19) (cid:20)
ǫ′2
Xt=1
t t
(cid:21)
based on the matrix determinant lemma. Therefore, (F.10) directly implies that
1 3 1 4R2sp(V∗)2 m
log logdet I+ X X⊤ ,
e ≤ 2 ≤ m ǫ′2 t t
(cid:20) t=1 (cid:21)
X
and then we have m dim ,ǫ/2sp(V∗)R .Recall that the ǫ-effective dimension is the minimum
eff
≤ X
positive integer satisfying the condition. As ABE dimension is defined as the length of the longest
(cid:0) (cid:1)
sequence satisfying (F.10), thus it holds that
dim ( ,ǫ) dim ,ǫ/2sp(V∗)R .
ABE eff
H ≤ X
Based on Lemma 3.2, d dim ,ǫ/2sp(V∗)R l(cid:0) ogT and κ d(cid:1) im ,ǫ/2sp(V∗)R . 2
G eff G eff
≤ X ≤ X
(cid:0) (cid:1) (cid:0) (cid:1)
F.4 Proof of Proposition C.4
Proof of Proposition C.4. Note that expected discrepancy function follows: for any t [T]
∈
E [l (f ,f ,ζ )] = θ⊤ ψ(s ,a )+ φ(s ,a ,s′)V (s′)ds′ r(s ,a ) E [V (s )]
k ζi fi t t i k2 t i i i i fi − i i − ζi fi i+1
(cid:16)
ZS
(cid:17)
= (θ θ∗)⊤ ψ(s ,a )+ φ(s ,a ,s′)V (s′)ds′ . (F.12)
t
−
i i i i fi
(cid:16)
ZS
(cid:17)
Let W = θ θ∗, X = ψ(s ,a )+ φ(s ,a ,s′)V (s′)ds′, and Σ = ǫI+ t−1X X⊤. Note that
t t − t i i S i i fi t i=1 t t
R P 1/2
t−1
W = θ θ∗ = ǫ θ θ∗ 2+ E [l (f ,f ,ζ )] 2
k t kΣt k t − kΣt k t − k2 k ζi fi t t i k2
" #
i=1
X
1/2
t−1
2√ǫ+ E [l (f ,f ,ζ )] 2 (F.13)
≤ k ζi fi t t i k2
" #
i=1
X
34where we use θ 1. Based on the elliptical potential lemma (see Lemma G.6), we have
t
k k ≤
T T T
1
X t Σ− t1 ∧1 ≤ 2d ·log 1+ d kX t 2
!
t=1 t=1 t=1
X(cid:13) (cid:13) X X (cid:13)
(cid:13) (cid:13) (cid:13) −1
2d log 1+(1+sp(V∗)/2) T √dǫ := d(ǫ), (F.14)
≤ · ·
(cid:18) (cid:19)
(cid:16) (cid:17)
where the last inequality results from φ √d, ψ √d and V 1sp(V∗). Combine
k k2,∞ ≤ k k2,∞ ≤ k f k∞ ≤ 2
(F.13) and 1 kX t Σ− t1 ≥ 1 ≤kX t Σ t−1 ∧1, it holds that
(cid:16) (cid:13) (cid:17) (cid:13)
(cid:13) T (cid:13) T
1 kX t Σ− t1
≥
1
≤
X t Σ− t1 ∧1
≤
d(ǫ). (F.15)
Xt=1 (cid:16) (cid:13) (cid:17) Xt=1 (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)
Step 1: Bound over dominance coefficient.
Note the sum of Bellman errors follows that
T T
E(f t)(s t,a t) = r ft +P ftV ft (s t,a t)
−
r f∗ +P f∗V ft (s t,a t)
t=1 t=1
X X(cid:0)(cid:0) (cid:1) (cid:0) (cid:1) (cid:1)
T
= (θ θ∗)⊤ ψ(s ,a )+ φ(s ,a ,s′)V (s′)ds′
t
−
t t t t ft
Xt=1 (cid:16)
ZS
(cid:17)
T
= W t⊤X t
·
1 kX t Σ− t1
≤
1 +1 kX t Σ− t1 > 1
Xt=1 (cid:16) (cid:16) (cid:13) (cid:17) (cid:16) (cid:13) (cid:17)(cid:17)
T (cid:13) (cid:13)
≤
W t⊤X t ·1 kX t Σ− t1
≤
1 +(sp(V∗)+2) ·min {d(ǫ),T
}
Xt=1 (cid:16) (cid:13) (cid:17)
T (cid:13)
≤
kW t kΣt
·
kX t Σ− t1 ∧1 +(sp(V∗)+2) ·min {d(ǫ),T }, (F.16)
Xt=1 (cid:16) (cid:13) (cid:17)
(cid:13)
where the first inequality results from (F.15) and the last inequality arises from the Cauchy-Swartz
inequality. Combine (F.13) and (F.14), we have
1/2
T T t−1
kW t kΣt · kX t Σ− t1 ∧1 ≤ 2√ǫ+
"
kl fi(f t,f t,ζ i) k2 2
#
· kX t Σ− t1 ∧1
Xt=1 (cid:16) (cid:13) (cid:17) Xt=1 Xi=1 (cid:16) (cid:13) (cid:17)
1/2 (cid:13) 1/2 1/2 (cid:13) 1/2
T T T t−1 T
≤
"
4ǫ
# "
kX t Σ− t1 ∧1
#
+
"
kl fi(f t,f t,ζ i) k2 2
# "
kX t Σ− t1 ∧1
#
t=1 t=1 t=1 i=1 t=1
X X (cid:13) XX X (cid:13)
(cid:13) T t−1 1/2 (cid:13)
2 Tǫ min d(ǫ),T + d(ǫ) l (f ,f ,ζ ) 2 , (F.17)
≤ · { } k fi t t i k2
" #
t=1 i=1
p XX
35where the second inequality results from Cauchy-Swartz inequality and the last inequality follows
(F.14). Plugging the result back into the (F.16), we conclude that
1/2
T T t−1
(f )(s ,a ) d(ǫ) E [l (f ,f ,ζ )] 2
E t t t ≤ k ζi fi t t i k2
" #
t=1 t=1 i=1
X XX
+2 Tǫ min d(ǫ),T +(sp(V∗)+2)min d(ǫ),T
· { } { }
1/2
pT t−1
d(ǫ) E [l (f ,f ,ζ )] 2 +(sp(V∗)+3)min d(ǫ),T +Tǫ, (F.18)
≤ k ζi fi t t i k2 { }
" #
t=1 i=1
XX
where the last inequality follows AM-GM inequality. Thus, d (dlog(sp(V∗)T/√dǫ)).
G
≤ O
Step 2: Bound over transferability coefficient.
Given condition that t−1 E[l (f ,f ,ζ )] 2 β for all t [T], we have
i=1k fi t t i k2 ≤ ∈
T P T 2
E[l (f ,f ,ζ )] 2 = (θ θ∗)⊤ ψ(s ,a )+ φ(s ,a ,s′)V (s′)ds′
k ft t t t k2 i − t t t t fi
Xt=1 Xt=1(cid:20) (cid:16) ZS (cid:17)(cid:21)
T
≤
(W t⊤X t)2 ·1 kX t 2 Σ− t1
≤
1 +(sp(V∗)+2)2min {d(ǫ),T
}
Xt=1 (cid:16) (cid:13) (cid:17)
T (cid:13)
≤
(β +4ǫ)
·
kX t 2 Σ− t1 ∧1 +(sp(V∗)+2)2min {d(ǫ),T
}
Xi=1 (cid:16) (cid:13) (cid:17)
d(ǫ)βlogT +(sp(V(cid:13)∗)2+4sp(V∗)+6)min d(ǫ),T +2Tǫ2, (F.19)
≤ { }
where we use a variant of (F.14) and (F.15), following that
T T T
1 X 2 1 X 2 1 X 1 d(ǫ),
k
t Σ− t1
≥ ≤
t Σ− t1
∧ ≤
t Σ− t1
∧ ≤
Xt=1 (cid:16) (cid:13) (cid:17) Xt=1 (cid:13) (cid:13) Xt=1 (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
and the last inequality results from a similar proof as (F.17) and (F.18) using Cauchy-Swartz and
AM-GM inequality. Thus, we have κ (dlog(sp(V∗)T/√dǫ)). 2
G
≤ O
F.5 Discussion about Performance on Concrete Examples
In this subsection, we show performance of Loop for specific problems. Loop achieves an ˜(√T)
O
regret, which is nearly minimax optimal in T, in linear AMDP and linear mixture AMDP.
LinearAMDP Recallthatlinearfunctionclassisdefinedas = (Q,J) : Q(, ) =ω⊤φ(, ) ω
2
H · · · · k k ≤
1sp(V∗)√d,J . Consider the ρ-covering number, note that
2 ∈ JH (cid:8) (cid:12)
(cid:12)
(cid:9)
Q(s,a) Q′(s,a) (ω ω′)⊤φ(s,a) √2 ω ω′ .
1
| − | ≤ | − | ≤ ·k − k
Based on the Lemma G.9, combine the fact that 2 and its ρ-covering number ( ) is at
H ρ H
|J | ≤ N J
most 2ρ−1, we can get the log covering number of the hypotheses class is upper bounded by
H
log H(ρ) dlog sp(V∗)23 2d3 2ρ−2 , (F.20)
N ≤
(cid:16) (cid:17)
36by taking α = w, P = d, B = sp(V∗)√d/2. Recall that Proposition C.1 indicates that
d dlog sp(V∗)√dρ−1 logT , κ dlog sp(V∗)√dρ−1 . (F.21)
G G
≤ O ≤ O
Combine (F.20), (F.21(cid:0)) and(cid:0)the regret gua(cid:1)rantee(cid:1)in Theorem(cid:0)4.1, w(cid:0)e get (cid:1)(cid:1)
Reg(T)
≤ O
sp(V∗)max {d G,κ
G
}
T log T NH2 ∪G(1/T)/δ sp(V∗)
≤
O˜ sp(V∗)3 2d3 2√T .
(cid:16) q (cid:17) (cid:16) (cid:17)
(cid:0) (cid:1)
For linear AMDPs, our method achieves ˜(sp(V∗)3 2d3 2√T) regret for both linear and generalized
O
linear AMDPs. In comparison, the Fopo algorithm (Wei et al., 2021) achieves the best-known
˜(sp(V∗)d3 2√T) regret. Our method incurs an additional constant sp(V∗)21 in the regret bound.
O
Linear mixture Recallthatthe Proposition C.4positsthat AGEC ofthe linearmixture probelm
satisfies that max √d ,κ (d√logT). Note that the hypotheses class is defined as
G G
{ } ≤ O
= (P,r) : P( s,a) = θ⊤φ(s,a, ), r(s,a) = θ⊤ψ(s,a) θ 1 .
2
H { ·| · | k k ≤ }
Consider the covering number note that both transition function and reward can be written as
(i). (P P′)(s′ s,a) = (θ θ′)⊤φ(s,a,s′) √d θ θ′ ,
1
| − | | | − | ≤ ·k − k
(ii). (r r′)(s,a) = (θ θ′)⊤ψ(s,a) √d θ θ′ .
1
| − | | − | ≤ ·k − k
Based on the Lemma G.9, the log covering number of is upper bounded by
LM
H
log H(ρ) 2dlog d3 2ρ−1 ,
N ≤
(cid:16) (cid:17)
by taking α = θ, P = d, B = 1. Combine results above and Theorem 4.1, we get
Reg(T)
≤ O
sp(V∗)max {d G,κ
G
}
T log T NH2 ∪G(1/T)/δ sp(V∗)
≤
O˜ sp(V∗)3 2d3 2√T .
(cid:16) q (cid:17) (cid:16) (cid:17)
(cid:0) (cid:1)
At our best knowledge, the UCRL2-VTR (Wu et al., 2022) achieves the best ˜(Dd√T) regret for
O
linear mixture AMDP, where D is the diameter under communicating AMDP assumption and it is
provablethatsp(V∗) D(Wang et al.,2022). Weremarkthatthetwoalgorithmsareincomparable
≤
under different assumptions and both achieve a near minimax optimal regret at ˜(√T).
O
G Technical Lemmas
In this section, we provide useful technical lemmas used in later theoretical analysis. Most are
directly borrowed from existing works and proof of modified lemmas is provided in Section G.1.
Lemma G.1. Given function class Φ defined on , and a family of probability measures Γ over .
Suppose sequence φ K Φ and µ K ΓX satisfy that for all k [K], k−1(E [φ ])2 X β.
{ k }k=1 ⊂ { k }k=1 ⊂ ∈ t=1 µt k ≤
Then, for all k [K], we have
∈ P
k
β
1 E [φ ] > ǫ +1 dim (Φ,Π,ǫ).
µt t ≤ ǫ2 DE
Xt=1 (cid:16)(cid:12) (cid:12) (cid:17) (cid:16) (cid:17)
(cid:12) (cid:12)
Proof. See Lemma 43 of Jin et al. (2021) for detailed proof.
37Lemma G.2 (Pigeon-hole principle). Given function class Φ defined on with φ(x) C for all
X | | ≤
φ Φ and x , and a family of probability measure over . Suppose sequence φ K Φ and
∈ ∈ X X { k }k=1 ⊂
µ K Γ satisfy that for all k [K], it holds k−1(E [φ ])2 β. Let d = dim (Φ,Γ,ǫ)
{ k }k=1 ⊂ ∈ t=1 µt k ≤ DE DE
be the DE dimension, then for all k [K] and ǫ > 0, we have
∈ P
k
E [φ ] 2 d βk+min k,d C +kǫ,
µt t
≤
DE
{ }
Xt=1(cid:12) (cid:12) p
(cid:12) (cid:12)
and (cid:12) (cid:12)
k
2
E [φ ] d βlogk+min k,d C2 +kǫ2.
µt t
≤
DE
{ }
Xt=1h i
Proof. See Section G.1.1.
Lemma G.3. Given function class Φ defined on with φ(x) C for all φ Φ and x , and
X | | ≤ ∈ ∈ X
a family of probability measure over . Let d = dim (Φ,Γ,ǫ) be the DE dimension, then for
DE DE
X
all k [K] and ǫ > 0, we have
∈
1/2
k K k−1
E [φ ] d (1+logK) (E [φ ])2 +min d ,k C +kǫ.
µk k
≤
DE µt k
{
DE
}
" #
Xt=1(cid:12) (cid:12) Xk=1 Xt=1
(cid:12) (cid:12)
(cid:12) (cid:12)
Proof. See Section G.1.2.
Lemma G.4 (d-upper bound). Let Φ and Ψ be sets of d-dimensional vectors and φ B ,
2 φ
k k ≤
ψ B for any φ Φ and ψ Ψ. If there exists set (φ ,...,φ ) and (ψ ,...,ψ )such that for
2 ψ 1 m 1 m
k k ≤ ∈ ∈
all t [m], t−1 φ ,ψ 2 c ε and φ ,ψ > c ε, where c c > 0 is a constant and ε > 0,
∈ k=1h t k i ≤ 1 |h t t i| 2 1 ≥ 2
then the numqber of elements in set is bounded by m dlog(B B /ε) .
P ≤ O φ ψ
Proof. See Section G.1.3. (cid:0) (cid:1)
Lemma G.5. For any sequence of positive reals x ,...,x , it holds that
Pm i=1xi
√1+logn.
1 m √Pm ix2 ≤
i=1 i
Proof. See Lemma 6 in Dann et al. (2021) for detailed proof.
Lemma G.6. Let x be a sequence of vectors defined over Hilbert space . Let Λ be a
i i∈[t] 0
{ } X
positive definite matrix and Λ = Λ + t−1x x⊤. It holds that
t 0 i=1 t t
t P detΛ
x 2 1 2log t+1 .
i=1k t kΛ− t1 ∧ ≤
(cid:18)
detΛ
0 (cid:19)
X
Proof. See Elliptical Potential Lemma (EPL) in Dani et al. (2008) for a detailed proof.
Lemma G.7 (Freedman’s inequality). Let X ,...,X be a real-valued martingale difference se-
1 T
quence adapted to filtration F T . Assume for all t [T] X R, then for any η (0,1/R),
{ t }t=1 ∈ t ≤ ∈
with probability greater than 1 δ
−
T T
log(1/δ)
X η E X2 F + ,
t ≤ O t| t η
Xt=1 (cid:16) Xt=1
(cid:2) (cid:3)
(cid:17)
38Proof. See Lemma 7 in Agarwal et al. (2014) for detailed proof.
Lemma G.8 (Scalinglemma). Letφ : Rd bead-dimensionalfeaturemapping, thereexists
S×A 7→
an invertible linear transformation A Rd×d such that for any bounded function f : R
∈ S ×A 7→
and z Rd defined by
∈
f(s,a)= φ(s,a)⊤z,
we have Aφ(s,a) 1 and A−1z sup f √d for all (s,a) .
k k ≤ k k ≤ s,a| | ∈S ×A
Proof. See Lemma 8 in Wei et al. (2021) for detailed proof.
In Theorem 4.1, the proved regret contains the logarithmic term of the 1/T-covering number of the
function classes (1/T), which can be regarded as a surrogate cardinality of the function class .
H
N H
Here, we provide a formal definition of ρ-covering and the upper bound of ρ-covering number.
Definition 17 (ρ-covering). The ρ-covering number of a function class is the minimum integer
F
t satisfying that there exists subset ′ with ′ = t such that for any f we can find a
F ⊆ F |F | ∈ F
correspondence f′ ′ that it holds f f′ ρ.
∞
∈F k − k ≤
Lemma G.9 (ρ-covering number). Let be a function defined over that can be parametrized
F X
by α = (α ,...,α ) RP with α B for all i [P]. Suppose that for any f,f′ it holds
1 P i
∈ | | ≤ ∈ ∈ F
that sup f(x) f′(x) L α α′ and let (ρ) be the ρ-covering number of , then
x∈X| − | ≤ k − k1 NF F
2BLP
log (ρ) P log .
F
N ≤ ρ
(cid:16) (cid:17)
Proof. See Lemma 12 in Wei et al. (2021) for detailed proof.
G.1 Proof of Technical Lemmas
In this subsection, we present the proofs of technical auxiliary lemmas with modifications.
G.1.1 Proof of Lemma G.2
Proof of Lemma G.2. The first statement is directly from Lemma 41 in Jin et al. (2021), and the
second statement follows a similar procedure as below. Note that Lemma G.1 suggests that
k
β
1 E [φ ] 2 > ǫ2 +1 dim (Φ,Γ,ǫ),
µt t ≤ ǫ2 DE
Xt=1 (cid:16)
(cid:2) (cid:3)
(cid:17) (cid:16) (cid:17)
and note that the sum of squared expectation can be decomposed as
k k k
E [φ ] 2 = E [φ ] 21 E [φ ] 2 > ǫ2 + E [φ ] 21 E [φ ] 2 ǫ2
µt t µt t µt t µt t µt t
≤
Xt=1
(cid:2) (cid:3)
Xt=1
(cid:2) (cid:3)
(cid:16)
(cid:2) (cid:3)
(cid:17) Xt=1
(cid:2) (cid:3)
(cid:16)
(cid:2) (cid:3)
(cid:17)
k
E [φ ] 21 E [φ ] 2 > ǫ2 +kǫ2. (G.1)
≤
µt t µt t
Xt=1
(cid:2) (cid:3)
(cid:16)
(cid:2) (cid:3)
(cid:17)
Assume sequence E [φ ] 2 ,..., E [φ ] 2 are sorted in the decreasing order and consider t [k]
µ1 1 µk k
∈
such that [E [φ ] 2 > ǫ2, there exists a constant α (ǫ2,[E [φ ] 2 ) satisfying
µt t (cid:2) (cid:3) (cid:2) (cid:3)
∈
µt t
k (cid:3) (cid:3)
β β
t 1 E [φ ] 2 > α +1 dim (Φ,Γ,√α) +1 dim (Φ,Γ,ǫ),
≤
µi i
≤ α
DE
≤ α
DE
Xi=1 (cid:16)
(cid:2) (cid:3)
(cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
39where the last inequality is based on the fact that the DE dimension is monotonically decreasing
in terms of ǫ as proposed in Jin et al. (2021). Denote d = dim (Φ,Γ,ǫ) and the inequality
DE DE
above implies that α d β/t d. Thus, we have E [φ ] 2 d β/t d. Beside, based on the
≤
DE
−
µt t
≤
DE
−
definition we also have E [φ ] 2 C2 and thus E [φ ] 2 min d β/t d,C2 , then
µt t
≤
µ(cid:2)t t ≤(cid:3)
{
DE
− }
(cid:2) (cid:3) (cid:2) (cid:3)
k k
d β
E [φ ] 21 E [φ ] 2 > ǫ2 min d ,k C2 + DE
µt t µt t
≤ {
DE
} t d
Xt=1
(cid:2) (cid:3)
(cid:16)
(cid:2) (cid:3)
(cid:17) t= Xd+1(cid:18) − DE(cid:19)
k 1
min d ,k C2 +d β dt
DE DE
≤ { } · t
Z0
min d ,k C2 +d βlogk. (G.2)
DE DE
≤ { } ·
Combine (G.1) and (G.2), then finishes the proof. 2
G.1.2 Proof of Lemma G.3
We remark that the proof provided in this subsection follows the almost same procedure as Lemma
3.16 in Zhong et al. (2022) with adjustment, and we preserve it for comprehension.
Proof of Lemma G.3. Denote d = dim (Φ,Γ,ǫ), ǫ = E [φ ] and ǫ = ǫ 1(ǫ > ǫ) for
DE DE t,k
|
µt k
|
t,k t,k t,k
t,k [K], µ Γ and φ Φ. The proof follows the procedure below. Consider K empty buckets
t k
∈ ∈ ∈
B 0,...,B
K−1
as initialization, and we examine ǫ
k,k
onbe by one for all k [K] asbbelowb:
∈
Case 1 If ǫ = 0, i.e., ǫ ǫ, then discard it.
k,k k,k
≤
Case 2 If ǫ > 0, i.e., ǫ > ǫ, at bucket j we add k into B if (ǫ )2 (ǫ )2,
k,k b k,k j t≤k−1,t∈Bj t,k ≤ k,k
otherwise we continue with the next bucket B .
j+1
P
b
Denote by b the index of bucket that at step k the non-zero ǫ falls in, i.e. k B . Based on
k k,k
∈
bk
the rule above, it holds that
K k−1 K K
(ǫ )2 (ǫ )2 b (ǫ )2,
t,k t,k k k,k
≥ ≥ ·
Xk=1 Xt=1 Xk=10≤j≤b Xk−1,bk≥1t≤k− X1,t∈Bj Xk=1
where the first inequality arises from t B : t k 1, 0 j b 1,b 1 [k 1] due tothe
j k k
{ ∈ ≤ − ≤ ≤ − ≥ } ⊆ −
discarding of the b th bucket, and the second equality directly follows the allocation rule such that
k
(ǫ )2 (ǫ )2 for any j b 1. Recall that based on the definition of distributional
t≤k−1,t∈Bj t,k ≥ k,k ≤ k −
eluder (DE) dimension, it is suggested the size B is no larger than d . Then,
P | j | DE
K K−1
b (ǫ )2 = j (ǫ )2 (re-summation)
k k,k t,t
Xk=1 Xj=1 t X∈Bj
2 2
K−1 K−1
j j
ǫ ǫ (B d )
t,t t,t j DE
≥ B   ≥ d   | | ≤
j DE
Xj=1 | | t X∈Bj Xj=1 t X∈Bj
  2  2
K−1
(d (1+logK))−1 ǫ = (d (1+logK))−1 ǫ , (G.3)
DE t,t DE t,t
≥    
Xj=1 t X∈Bj t∈[ XK]\B0
   
40where the second inequality follows Lemma G.5. Combine the (G.1.2) and (G.3) above, we have
K K
ǫ ǫ +Kǫ ǫ +min d ,K φ +Kǫ
k,k k,k t,t DE ∞
≤ ≤ { }k k
Xk=1 Xk=1 t∈[ XK]\B0
b 1/2
K k−1
d (1+logK) (ǫ )2 +min d ,K C +Kǫ
DE t,k DE
≤ { }
" #
k=1t=1
XX
1/2
K k−1
d (1+logK) (ǫ )2 +min d ,K C +Kǫ.
DE t,k DE
≤ { }
" #
k=1t=1
XX
Substitute the definition ǫ = E [φ ] back intob the inequality, then finishes the proof. 2
t,k
|
µt k
|
G.1.3 Proof of Lemmba G.4
Proof of Lemma G.4. For notation simplicity, denote Λ = t−1 ψ ψ⊤ + ε2 I, then for all
t k=1 t t B φ2 ·
t [m] we have φ t−1(φ⊤ψ )2+ ε2 φ 2 = c2+P 1 ε based on the given condition.
∈ k t kΛt ≤ k=1 t k B φ2k t k2 1
r
Using the Cauchy-Swartz ineqPuality and results above, thepn it holds kψ t kΛ− t1
≥
|hφ t,ψ t i|/ kφ t kΛt =
c / c2+1. On one hand, the matrix determinant lemma ensures that
2 1
p m−1 c2 m−1 ε2 d
detΛ = detΛ 1+ ψ 2 1+ 2 . (G.4)
m 0 ·
t=1
k t kΛ− t1 ≥
(cid:18)
1+c2
1(cid:19)
B φ2
!
Y (cid:0) (cid:1)
On the other hand, according to the definition of Λ , we have
t
Tr(Λ ) d t−1 ψ 2 ε2 d B2(m 1) ε2 d
detΛ m k k k2 + ψ − + . (G.5)
m ≤ d ≤ d B2 ≤ d B2
(cid:18) (cid:19) k=1
φ! φ!
X
Combine (G.4) and (G.5), if we take logarithms at both sides, then we have
B2B2(m 1) c2
m 1+dlog φ ψ − +1 log 1+ 2 .
≤ dε2 1+c2
! (cid:30) (cid:18) 1(cid:19)
After simple calculations, we can obtain that m is upper bounded by dlog(B B /ε) . 2
φ ψ
O
(cid:0) (cid:1)
H Supplementary Discussions
H.1 Proof Sketch of MLE-based Results
In this subsection, we provide the proof sketch of Theorem B.1. We first introduce several useful
lemmas, which is the variant of ones in Appendix D for MLE-based problems, and most have been
fully researched in Liu et al. (2022, 2023a); Xiong et al. (2023). As there’s no significant technical
gap between episodic andaverage-reward formodel-based problems, we only provide aproof sketch.
Lemma H.1 (AkintoLemmaD.1). UnderAssumptions1-2,Mle-Loopisanoptimisticalgorithm
such that it ensures J J∗ for all t [T] with probability greater than 1 δ.
t
≥ ∈ −
41Proof Sketch of Lemma H.1. See Proposition 13 in Liu et al. (2022) with slight modifications. 2
Lemma H.2 (Akin to Lemma D.2). For fixed ρ > 0 and a pre-determined optimistic parameter
β = c(log T (ρ)/δ)+Tρ where constant c >0, it holds that
H
B
(cid:0) (cid:1)
t−1 t−1
kE ζi[l fi(f t,f t,ζ i)]
k1
= TV P ft( ·|s i,a i),P f∗( ·|s i,a i)
≤
O( βt), (H.1)
i=1 i=1
X X (cid:0) (cid:1) p
for all t [T] with probability greater than 1 δ.
∈ −
Proof Sketch of Lemma H.2. See Proposition 14 in Liu et al. (2022) with slight modifications. 2
LemmaH.3(AkintoLemmaD.3). Let (T)betheswitchingcostwithtimehorizonT,givenfixed
N
covering coefficient ρ > 0 and pre-determined optimistic parameter β = c log T (ρ)/δ +Tρ
H
B
where c is a large enough constant, with probability greater than 1 2δ we have
− (cid:0) (cid:0) (cid:1) (cid:1)
(T) κ poly(logT)+β−1Tǫ2 ,
G
N ≤ O ·
(cid:0) (cid:1)
where κ G is the transferability coefficient with respect to MLE-AGEC( , l f′ ,ǫ).
H { }
Proof Sketch of Lemma H.3. The proof is almost the same as Lemma D.3.
Step 1: Bound the difference of discrepancy between the minimizer and f∗.
As proposed in Proposition 14, Liu et al. (2022), 0
≤
t i=1TV P f∗( ·|s i,a i),P gi( ·|s i,a i) 2
≤
β holds
with high probability if the update happens at t-th step. Based on the AM-GM inequlaity, we have
P (cid:0) (cid:1)
t
0
≤
TV P f∗( ·|s i,a i),P gi( ·|s i,a i)
≤
βt. (H.2)
i=1
X (cid:0) (cid:1) p
Step 2: Bound the expected discrepancy between updates.
Note that for all t+1 [T], the update happens only if
∈
t
TV P ( s ,a ),P ( s ,a ) > 3 βt. (H.3)
ft
·|
i i gi
·|
i i
i=1
X (cid:0) (cid:1) p
Combine the (H.2) and (H.3) above, and apply the triangle inequality, we have
t
TV P ft( ·|s i,a i),P f∗( ·|s i,a i)
i=1
X (cid:0) (cid:1)
t
≥
TV P ft( ·|s i,a i),P gt( ·|s i,a i) −TV P f∗( ·|s i,a i),P gt( ·|s i,a i)
≥
2 βt.
i=1
X (cid:0) (cid:1) (cid:0) (cid:1) p
and the construction of confidence set ensures that τ i=t 1TV P ft( ·|s i,a i),P f∗( ·|s i,a i)
≤
√βt with
high probability (Liu et al., 2022, Proposition 14). Recall the definition of the MLE-transferability
P (cid:0) (cid:1)
coefficient, then the switching cost can be bounded following the same argument in Lemma D.3.2
42Algorithm 3 Extended Value Iteration (EVI)
Input: hypothesis f = (P ,r ), desired accuracy level ǫ.
f f
Initialize: V(0)(s) = 0 for all s , J(0) = 0 and counter i= 0.
∈ S
1: repeat
2: for s and a do
∈ S ∈ A
3: Set Q(i)(s,a)
←
r f(s,a)+E s′∼P f(s,a)[V(i)(s′)] −J(i)
4: Update V(i+1)(s) max a∈AQ(i)(s,a)
←
5: Update counter i i+1
←
6: until max s∈S V(i+1)(s) V(i)(s) min s∈S V(i+1)(s) V(i)(s) ǫ
{ − }− { − } ≤
Proof Sketch of Theorem B.1. Recall that
T T
Reg(T)
≤
E(f t)(s t,a t)+ E st+1∼P(·|st,at)[V t(s t+1)] −V t(s t) , (H.4)
Xi=1 Xt=1(cid:16) (cid:17)
Bellman error Realization error
| {z } | {z }
where the inequality follows the optimism in Lemma H.1. Combine Lemma H.2, Lemma H.3 and
the definition of MLE-AGEC (see Definition 9), then we can finish the proof. 2
H.2 Extended Value Iteration (EVI) for Model-Based Hypotheses
In model-based problems, the discrepancy function sometimes relies on the optimal state bias func-
tion V and optimal average-reward J (see linear mixture model in Section C). In this section,
f f
we provide an algorithm, extended value iteration (EVI) proposed in Auer et al. (2008), to output
the optimal function and average-reward under given a model-based hypothesis f = (P ,r ). See
f f
Algorithm 3 forcomplete pseudocode. The convergence of EVI is guaranteed by thetheorem below.
Theorem H.4. UnderAssumption 1, there exists a unique centralized solution pair (Q∗,J∗) to the
Bellman optimality equation for any AMDP characterized by hypothesis f . Then, if the
f
M ∈ H
extended value iteration (EVI) is stopped under the condition that
max V(i+1)(s) V(i)(s) min V(i+1)(s) V(i)(s) ǫ,
s∈S { − }− s∈S{ − } ≤
then the achieved greedy policy π(i) is ǫ-optimal such that Jπ(i) J∗ +ǫ.
Mf ≥ Mf
Proof Sketch: See Theorem 12 in Auer et al. (2008).
43