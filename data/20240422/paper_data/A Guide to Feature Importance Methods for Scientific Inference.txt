A Guide to Feature Importance Methods for
Scientific Inference
Fiona Katharina Ewald1,2[0009−0002−6372−3401], Ludwig
Bothmann1,2[0000−0002−1471−6582], Marvin N. Wright4,5,6[0000−0002−8542−6291],
Bernd Bischl1,2[0000−0001−6002−6980], Giuseppe
Casalicchio⋆,1,2[0000−0001−5324−5966](cid:0), and Gunnar
K¨onig⋆,3[0000−0001−6141−4942]
1 Department of Statistics, LMU Munich, Munich, Germany
(cid:0)Giuseppe.Casalicchio@stat.uni-muenchen.de
2 Munich Center for Machine Learning (MCML), Munich, Germany
3 DepartmentofComputerScienceandTu¨bingenAICenter,UniversityofTu¨bingen,
Tu¨bingen, Germany
4 Leibniz Institute for Prevention Research & Epidemiology – BIPS, Bremen,
Germany
5 University of Bremen, Bremen, Germany
6 University of Copenhagen, Copenhagen, Denmark
Abstract. While machine learning (ML) models are increasingly used
due to their high predictive power, their use in understanding the data-
generating process (DGP) is limited. Understanding the DGP requires
insights into feature-target associations, which many ML models can-
not directly provide, due to their opaque internal mechanisms. Feature
importance (FI) methods provide useful insights into the DGP under
certain conditions. Since the results of different FI methods have differ-
ent interpretations, selecting the correct FI method for a concrete use
caseiscrucialandstillrequiresexpertknowledge.Thispaperservesasa
comprehensive guide to help understand the different interpretations of
FI methods. Through an extensive review of FI methods and providing
new proofs regarding their interpretation, we facilitate a thorough un-
derstanding of these methods and formulate concrete recommendations
for scientific inference. We conclude by discussing options for FI uncer-
tainty estimation and point to directions for future research aiming at
full statistical inference from black-box ML models.
Keywords: Feature Importance · Model-agnostic Interpretability · In-
terpretable ML
1 Introduction
Machinelearning(ML)modelshavegainedwidespreadadoption,demonstrating
theirabilitytomodelcomplexdependenciesandmakeaccuratepredictions[32].
⋆ equal contribution as senior authors
4202
rpA
91
]LM.tats[
1v26821.4042:viXra2 F. K. Ewald et al.
Besidesaccuratepredictions,practitionersandscientistsareoftenequallyinter-
ested in understanding the data-generating process (DGP) to gain insights into
the underlying relationships and mechanisms that drive the observed phenom-
ena [53]. Since analytic information regarding the DGP is mostly unavailable,
we usually analyze a predictive model as a surrogate. Although this approach
haspotentialpitfalls,itcanserveasaviablealternativeforgaininginsightsinto
the inherent patterns and relationships within the observed data, particularly
when the generalization error of the ML model is small [43]. Regrettably, the
complexandoftennon-linearnatureofcertainMLmodelsrendersthemopaque,
presenting a significant challenge in understanding them.
A broad range of interpretable ML (IML) methods have been proposed in the
lastdecades[11,25].Theseincludelocal techniquesthatonlyexplainonespecific
prediction as well as global techniques that aim to explain the whole ML model
or the DGP; model-specific techniques that require access to model internals
(e.g., gradients) as well as model-agnostic techniques that can be applied to any
model; and feature effects methods, which reflect the change in the prediction
dependingonthevalueofthefeatureofinterest(FOI),aswellasfeature impor-
tance (FI)methods,whichassignanimportancevaluetoeachfeaturedepending
onitsinfluenceonthepredictionperformance.Wearguethatinmanyscenarios,
analystsareinterestedinreliablestatistical,population-levelinferenceregarding
the underlying DGP [41,61],instead of“simply” explaining themodel’s internal
mechanismsorheuristiccomputationswhoseexactmeaningregardingtheDGP
is at the very least unclear or not explicitly stated at all. If an IML technique is
used for such a purpose, it should ideally be clear, what property of the DGP is
computed, and, as we nearly always compute on stochastic and finite data, how
varianceanduncertaintyarehandled.TherelevanceofIMLinthecontextofsci-
entificinferencehasbeenrecognizedingeneral[53]aswellasinspecificsubfields,
e.g.,inmedicine[8]orlaw[15].Krishnaetal.[34]illustratethedisorientationof
practitioners when choosing an IML method. In their study, practitioners from
both industry and science were asked to choose between different IML methods
and explain their choices. The participants predominantly based their choice on
superficial criteria such as publication year or whether the method’s outputs
align with their prior intuition, highlighting the absence of clear guidelines and
selection criteria for IML techniques.
Motivating Example. The well-known “bike sharing” data set [17] includes 731
observations and 12 features corresponding to, e.g., weather, temperature, wind
speed, season, and day of the week. Suppose a data scientist is not only inter-
estedinachievingaccuratepredictionsofthenumberofbikerentalsperdaybut
also in learning about the DGP to identify how the features are associated with
the target. She trains a default random forest (RF, test-RMSE: 623, test-R2:
0.90), and for analyzing the DGP, she decides to use two FI methods: permuta-
tionfeatureimportance(PFI)andleave-one-covariate-out(LOCO)withL2loss
(details on these follow in Sections 4 and 6). Unfortunately, she obtains some-
what contradictory results – shown in Figure 1. The methods produce results
that agree on using temperature (temp), season (season), the number of daysA Guide to Feature Importance Methods for Scientific Inference 3
days_since_2011 temp
temp hum
yr weathersit
hum season
season windspeed
mnth days_since_2011
0e+003e+056e+059e+05 0e+00 5e+04 1e+05
(a) PFI (b) LOCO
Fig.1: Six most important features following (a) PFI and (b) LOCO.
elapsedsincethestartofdatacollectionin2011(days since 2011),andhumid-
ity(hum)aspartofthetop6mostimportantfeatures,buttherankingsofthese
features differ across different methods. She is unsure which feature in the DGP
is the most important one, what the disagreement of the FI methods means,
and, most importantly, what she can confidently infer from the results about
the underlying DGP. We will address her questions in the following sections.
ContributionsandOutline. ThispaperassessestheusefulnessofseveralFImeth-
odsforgaininginsightintoassociationsbetweenfeaturesandthepredictiontar-
get in the DGP. Here, for now we focus on loss-based, global, model-agnostic FI
methods. Our literature review in Section 2 highlights the current state-of-the-
art and identifies a notable absence of guidelines. Section 3 determines the type
offeature-targetassociationswithintheDGPthatshallbeanalyzedwiththeFI
methods. In Section 4, we discuss methods that remove features by perturbing
them;inSection5methodsthatremovefeaturesbymarginalizingthemout;and
in Section 6 methods that remove features by refitting the model without the
respective features. In each of the three sections, we first briefly introduce the
FImethods,followedbyaninterpretationguidelineaccordingtotheassociation
types introduced in Section 3. At the end of each section, our results are stated
mathematically, with some proofs provided in Appendix A. We illustrate our
theoretical results in a simulation study in Section 7 and formulate recommen-
dations and practical advice in Section 8. We mainly analyze the estimands of
theconsideredFI,butitshouldbenotedthattheinterpretationoftheestimates
comes with additional challenges. Hence, we briefly discuss approaches to mea-
sure and handle their uncertainty in Section 9 and conclude in Section 10 with
open challenges.
2 Notation and Related Work
General Notation. Let D =
(cid:0)(cid:0) x(1),y(1)(cid:1) ,...,(cid:0) x(n),y(n)(cid:1)(cid:1)
be a data set of n
observations, which are sampled i.i.d. from a p-dimensional feature space X =
X ×...×X and a target space Y. The set of all features is denoted by P =
1 p
{1,...,p}. The realized feature vector is x(i) = (x(i),...,x(i))⊤, i ∈ {1,...,n},
1 p4 F. K. Ewald et al.
where
y=(cid:0) y(1),...,y(n)(cid:1)⊤
are the realized labels. The associated random vari-
ablesareX =(X ,...,X )⊤ andY,respectively.Marginalrandomvariablesfor
1 p
a subset of features S ⊆P are denoted by X . The complement of S is denoted
S
by −S = P \S. Single features and their complements are denoted by j and
−j, respectively. Probability distributions are denoted by F, e.g., F (Y) is the
Y
marginaldistributionofY.Iftworandomvectors,e.g.,featuresetsX andX ,
J K
areunconditionallyindependent,wewriteX ⊥⊥X ;iftheyareunconditionally
J K
dependent, which we also call unconditionally associated, we write X ⊥̸⊥X .
J K
We assume an underlying true functional relationship f : X → Y that im-
true
plicitly defines the DGP by Y = f (X)+ϵ. It is approximated by an ML
true
model fˆ: X → Rg, estimated on training data D. In the case of a regression
model, Y = R, and g = 1. If fˆrepresents a classification model, g is greater
or equal to 1: for binary classification (e.g., Y = {0,1}), g is 1; for multi-class
classification,itrepresentstheg decisionvaluesorprobabilitiesforeachpossible
outcomeclass.TheMLmodelfˆisdeterminedbytheso-calledlearnerorinducer
I :D×λ(cid:55)→fˆthatuseshyperparametersλtomapadatasetDtoamodelinthe
hypothesisspacefˆ∈H.Givenalossfunction,definedbyL:Y×Rg →R+,the
0
riskfunctionofamodelfˆisdefinedastheexpectedlossR(fˆ)=E[L(Y,fˆ(X))].
Related Work. SeveralpapersaimtoprovideageneraloverviewofexistingIML
methods [11,12,25,26], but they all have a very broad scope and do not dis-
cuss scientific inference. Our work is the first concrete and encompassing guide
for global, loss-based, model-agnostic FI methods directed toward researchers
who aim to make informed decisions on the choice of FI methods for scientific
inference. Freiesleben et al. [19] propose a general procedure to design interpre-
tationsforscientificinferenceandprovideabroadoverviewofsuitablemethods.
In contrast, we provide concrete interpretation rules for FI methods. Hooker
et al. [30] analyze FI methods whose calculations are based on the reduction in
performanceaccuracywhentheFOIisunknown.WeexamineFItechniquesand
provide usage recommendations depending on different types of feature-target
associations.
This paper builds on a range of work that assesses how FI methods can be in-
terpreted: Strobl et al. [57] extended PFI [7] for random forests by using the
conditional distribution instead of the marginal distribution when permuting
the FOI, resulting in the conditional feature importance (CFI); Molnar et al.
[42] modified CFI to a model-agnostic version where the dependence structure
isestimatedbytrees;K¨onigetal.[33]generalizePFIandCFItoamoregeneral
familyofFItechniquescalledrelativefeatureimportance(RFI)andassesswhat
insightintothedependencestructureofthedatatheyprovide;Covertetal.[10]
derivetheoreticallinksbetweenShapleyadditiveglobalimportance(SAGE)val-
ues and properties of the DGP; Watson and Wright [59] propose a CFI based
conditionalindependencetest;Leietal.[35]introduceLOCOandareamongthe
firsttobaseFIonhypothesistesting;Williamsonetal.[61]presentaframework
for loss-based FI methods based on model refits, including hypothesis testing;
andAuetal.[4]focusonFImethodsforgroupsoffeaturesinsteadofindividualA Guide to Feature Importance Methods for Scientific Inference 5
features, such as leave-one-group-in (LOGI).
In addition to the interpretation methods discussed in this paper, other FI ap-
proaches exist. Another branch of IML deals with variance-based FI methods
aimedattheFIofanMLmodelandnotnecessarilyregardingtheDGP,asthey
onlyusethepredictionfunctionofanMLmodelwithoutconsideringtheground
truth. For example, the feature importance ranking measure (FIRM) [64] uses
a feature effect function and defines the standard deviation as an importance
method. A similar method by [23] uses the standard deviation of the partial
dependence(PD)function[20]asanFImeasure.TheSobolindex[55]isamore
generalvariance-basedmethodbasedonadecompositionofthepredictionfunc-
tionintomaineffectsandhigh-ordereffects(i.e.,interactions)andestimatesthe
variance of each component to quantify their importance [45]. Lundberg et al.
[38]introducedtheSHAPsummaryplotasaglobalFImeasurebasedonaggre-
gating local SHAP values [39], which are defined only regarding the prediction
function without considering the ground truth.
3 Feature-Target Associations
When analyzing the FI methods, we focus on whether they provide insight into
(conditional) (in)dependencies between a feature X and the prediction target
j
Y. More specifically, we are interested in understanding whether they provide
insight into the following relations:
(A1) Unconditional association (X ⊥̸⊥Y).
j
(A2) Conditional association ...
(A2a) ... given all remaining features X (X ⊥̸⊥Y |X ).
−j j −j
(A2b) ... given any user-specified set X , G ⊂P\{j} (X ⊥̸⊥Y |X ).
G j G
An unconditional association (A1) indicates that a feature X provides infor-
j
mation about Y, i.e., knowing the feature on its own allows us to predict Y
better; if X and Y are independent, this is not the case. On the other hand, a
j
conditional association (A2) with respect to (w.r.t.) a set S ⊆ P\{j} indicates
that X provides information about Y, even if we already know X . When an-
j S
alyzing the suitability of the FI methods to gain insight into (A1)-(A2b), it is
important to consider that no FI score can simultaneously provide insight into
more than one type of association. In supervised ML, we are often interested in
theconditionalassociationbetweenX andY,givenX (A2a),i.e.,predicting
j −j
Y better if we are given information regarding all other features.
For example, given measurements of several biomarkers and a disease outcome,
a doctor may not only be interested in a well-performing black-box prediction
model based on all biomarkers but also in understanding which biomarkers are
associated with the disease (A1). Furthermore, the doctor may want to under-
stand whether measuring a biomarker is strictly necessary for achieving optimal
predictiveperformance(A2a)andtounderstandwhetherasetofotherbiomark-
ers G can replace the respective biomarker (A2b).6 F. K. Ewald et al.
Example 1 shows that conditional association does not imply unconditional as-
sociation((A2)̸⇒(A1)).Additionally,unconditionalassociationdoesnotimply
conditional association, as Example 2 demonstrates ((A1) ̸⇒ (A2)).
Example 1. LetX ,X ∼Bern(0.5)beindependentfeaturesandY :=X ⊕X
1 2 1 2
(where ⊕ is the XOR operation). Then, all three features are pairwise indepen-
dent, but X and X together allow us to predict Y perfectly.
1 2
Example 2. Let Y := X with X ∼ N(0,1) and X := X + ϵ with ϵ ∼
1 1 2 1 2 2
N(0,0.1). Although X provides information about Y, all of this information
2
is also contained in X . Thus, X is unconditionally associated with Y but
1 2
conditionally independent from Y given X .
1
Furthermore, conditional (in)dependence w.r.t. one feature set does not imply
(in)dependence w.r.t. another, e.g., (A2a) ̸⇔ (A2b). This is demonstrated by
adding unrelated features to the DGP and the conditioning set, as shown in
Examples 1 and 2.
4 Methods Based on Univariate Perturbations
Methods based on univariate perturbations quantify the importance of an FOI
by comparing the model’s performance before and after replacing the FOI X
j
with a perturbed version X˜ (permuted observations):
j
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
FI =E L Y,fˆ(X˜ ,X ) −E L Y,fˆ(X) . (1)
j j −j
Theideabehindthisapproachisthatifperturbingthefeatureincreasesthepre-
dictionerror,thefeatureshouldbeimportantforY.Below,wediscussthethree
methods PFI (Section 4.1), CFI (Section 4.2), and RFI (Section 4.3) differing
in their perturbation scheme: Permutation in PFI [7,18] preserves the feature’s
marginal distribution while destroying all dependencies with other features X
−j
and the target Y, i.e.,
X˜ ∼F (X ) and X˜ ⊥⊥(X ,Y);
j Xj j j −j
CFI perturbs the FOI while preserving its dependencies with the remaining
features, i.e.,
X˜ ∼F (X |X ) and X˜ ⊥⊥Y|X ;
j Xj|X−j j −j j −j
RFI[33]isageneralizationofPFIandCFI,sincetheperturbationspreservethe
dependencies with any user-specified set G, i.e.,
X˜ ∼F (X |X ) and X˜ ⊥⊥Y,X |X .
j Xj|XG j G j P\(G∪{j}) G
ToindicateonwhichsetGthepermutationofj isconditioned,wedenoteRFIG.
j
We obtain PFI by setting G=∅ and CFI by setting G=−j. As will be shown,
the type of perturbation strongly affects which features are considered relevant.A Guide to Feature Importance Methods for Scientific Inference 7
4.1 Permutation Feature Importance (PFI)
Insight into X ⊥̸⊥ Y (A1): Non-zero PFI does not imply an unconditional
j
associationwithY (NegativeResult4.1.2).IntheproofofNegativeResult4.1.2,
we construct an example where the PFI is non-zero because the permutation
breaks the dependence between the features (and not because of an uncondi-
tionalassociationwithY).Basedonthis,onemayconjecturethatunconditional
featureindependenceisasufficientassumptionfornon-zeroPFItoimplyanun-
conditional association with Y; however, this is not the case, as Negative Result
4.1.3demonstrates.Fornon-zeroPFItoimplyanunconditionalassociationwith
Y, the features must be independent conditional on Y instead (Result 4.1.1).
ZeroPFIdoesnotimplyindependencebetweentheFOIandthetarget(Negative
Result 4.1.4). Suppose the model did not detect the association, e.g., because it
is a suboptimal fit or because the loss does not incentivize the model to learn
the dependence. PFI may be zero in that case, although the FOI is associated
with Y. In the proof of Negative Result 4.1.4, we demonstrate the problem for
L2 loss, where the optimal prediction is the conditional expectation (and thus
neglects dependencies in higher moments). For cross-entropy optimal predictors
andgivenfeatureindependence(bothwithandwithoutconditioningonY),zero
PFI implies unconditional independence with Y (Result 4.1.1).
Insight into X ⊥̸⊥ Y conditional on X or X (A2): PFIrelatestoun-
j G −j
conditional (in)dependence and, thus, is not suitable for insight into conditional
(in)dependence (see Section 3).
Result 4.1.1 (PFI Interpretation). For non-zero PFI, it holds that
(X ⊥⊥X |Y)∧(PFI ̸=0) ⇒ X ⊥̸⊥Y.
−j j j j
For cross-entropy loss and the respective optimal model,
(X ⊥⊥X )∧(X ⊥⊥X |Y)∧(PFI =0) ⇒ X ⊥⊥Y.
j −j j −j j j
Proof. ThefirstimplicationdirectlyfollowsfromTheorem1in[33].Thesecond
follows from the more general Result 4.3.1. ⊔⊓
Negative Result 4.1.2. PFI ̸=0 ̸⇒ X ⊥̸⊥Y.
j j
Proof (Counterexample). Let Y,X ∼N(0,1) be two independent random vari-
1
ables,X :=X ,andthepredictionmodelfˆ(x)=x −x .Itissimpletocalculate
2 1 1 2
that this model has expected L2 loss of 1, as E[L(Y,X −X )] = E[Y2] = 1.
1 2
Now let X˜ bethe permuted version of X (X˜ ∼F (X )), and X˜ ⊥⊥(Y,X ).
1 1 1 Xj 1 1 2
TheexpectedL2lossunderpermutationnowisE[(Y −(X˜ −X ))2]=Var(Y −
1 2
X˜ +X )=3, which implies PFI =2. So PFI is non-zero, but X ⊥⊥Y. ⊔⊓
1 2 1 1 1
Negative Result 4.1.3. (X ⊥⊥X )∧(PFI ̸=0) ̸⇒ X ⊥̸⊥Y.
−j j j j8 F. K. Ewald et al.
Proof (Counterexample). Let X ,X ∼ Bern(0.5) with X ⊥⊥ X , and Y :=
1 2 1 2
X ⊕X ,where⊕isXOR.Consideraperfectpredictionmodelfˆ(X)=x ⊕x ,
1 2 1 2
and fˆencodes the posterior probability for Y =1 (here, Y can be only 0 or 1).
This model has a cross-entropy loss of 0, since Y =fˆ(X). Furthermore, it holds
that X ⊥⊥ Y. Again, let X˜ be the permuted version of X . One can easily
1 1 1
verify that Y =(X ⊕X )⊥⊥(X˜ ⊕X )=Y˜ˆ and Y,Y˜ˆ ∼Bern(0.5). Thus, the
1 2 1 2
prediction Y˜ˆ usingthe perturbed feature X˜ assigns probability1 tothe correct
1
and wrong class with probability 0.5 each. Thus, the cross-entropy loss for the
perturbed prediction is non-zero (actually, positive infinity), and PFI ̸=0. ⊔⊓
j
Negative Result 4.1.4. FI =0̸⇒X ⊥⊥Y |X for any G⊆P\{j}, even if the
j j G
model is L2-optimal.
NB: This result holds not only for PFI but also for any FI method based on
univariate perturbations, including PFI, CFI, and RFI (Equation 1).
Proof (Counterexample). If a model does not rely on a feature X , FI =0. We
j j
constructanexamplewherefˆisL2-optimalbutdoesnotrelyonthefeatureX ,
1
which is dependent with Y conditional on any set G⊆P\{j}. Let Y|X ,X ∼
1 2
N(X ,X )withX ,X ∼N(0,1)andX ⊥⊥X .Then,Y isdependentwithX
2 1 1 2 1 2 1
conditional on any set G ⊆ P\{1}: Here, G could either be G = ∅ or G = {2}.
Now, for small X , extreme values of Y are less likely than for X = 100,
1 1
irrespective of whether we know X . Now consider fˆ(x) = x . fˆis L2-optimal
2 2
since E[Y|X]=X , but fˆdoes not depend on X . ⊔⊓
2 1
4.2 Conditional Feature Importance (CFI)
Insight into X ⊥̸⊥ Y|X (A2a): SinceCFIpreservesassociationsbetween
j −j
features, non-zero CFI implies a conditional dependence on Y, even if the fea-
tures are dependent (Result 4.2.1). The converse generally does not hold, so
Negative Result 4.1.4 also applies to CFI. However, for cross-entropy optimal
models, zero CFI implies conditional independence (Result 4.2.1).
Insight into X ⊥̸⊥ Y (A1) and X ⊥̸⊥ Y|X (A2b): Since CFI provides
j j G
insight into conditional dependence (A2a), it follows from Section 3 that CFI is
not suitable to gain insight into (A1) and (A2b).
Result 4.2.1 (CFI interpretation). For CFI, it holds that
CFI ̸=0 ⇒ X ⊥̸⊥Y |X
j j −j
For cross-entropy optimal models
CFI =0⇒X ⊥⊥Y|X
j j −j
Proof. The first equation follows from Theorem 1 in [33]. The second follows
from the more general Result 4.3.1.A Guide to Feature Importance Methods for Scientific Inference 9
4.3 Relative Feature Importance (RFI)
Insight into X ⊥̸⊥ Y|X (A2b): Result 4.3.1 generalizes Results 4.1.1 and
j G
4.2.1. While PFI and CFI are sensitive to dependencies conditional on no or all
remaining features, RFI is sensitive to conditional dependencies w.r.t. a user-
specified feature set G. Nevertheless, we must be careful with our interpretation
iffeaturesaredependent.RFImaybenon-zeroeveniftheFOIisnotassociated
with the target (Negative Result 4.3.2). In general, zero RFI does not imply
independence(NegativeResult4.1.4).Still,forcross-entropyoptimalmodelsand
underindependenceassumptions,insightintoconditionalindependencew.r.t.G
can be gained (Result 4.3.1).
Insight into X ⊥̸⊥ Y (A1) and X ⊥̸⊥ Y|X (A2a): If features are
j j −j
conditionallyindependentgivenY,settingGto∅(yieldingPFI)enablesinsight
into unconditional dependence. Setting G to −j (yielding CFI) enables insight
into the conditional association given all other features.
Result 4.3.1 (RFI interpretation). For R=P\(G∪{j}), it holds that
(X ⊥⊥X |X ,Y)∧(RFIG ̸=0) ⇒ X ⊥̸⊥Y | X .
j R G j j G
For cross-entropy optimal predictors and G⊆P\{j}, it holds that
(X ⊥⊥X |X ,Y)∧(X ⊥⊥X |X )∧(RFIG =0) ⇒ X ⊥⊥Y |X .
j R G j R G j j G
Proof. The first implication follows directly from Theorem 1 in [33]. The proof
of the second implication can be found in Appendix A.1.
Negative Result 4.3.2. RFIG ̸=0 ̸⇒ X ⊥̸⊥Y |X .
j j G
Proof (Counterexample). Let G=∅. Then, RFIG =PFI and X ⊥̸⊥Y|X ⇔
j j j G
X ⊥̸⊥Y. Thus, the result directly follows from 4.1.2. ⊔⊓
j
5 Methods Based on Marginalization
In this section, we assess SAGE value functions (SAGEvf) and SAGE values
[10]. The methods remove features by marginalizing them out of the prediction
function. The marginalization [39] is performed using either the conditional or
marginal expectation. These so-called reduced models are defined as
(cid:104) (cid:105)
fˆm(x )=E fˆ(x ,X ) , and
S S X−S S −S
(2)
(cid:104) (cid:105)
fˆc(x )=E fˆ(x ,X )|X ,
S S X−S|XS S −S S
where fˆm is the marginal and fˆc is the conditional-sampling-based version and
fˆm = fˆc the average model prediction, e.g., E[Y] for an L2 loss optimal model
∅ ∅10 F. K. Ewald et al.
andP(Y)foracross-entropylossoptimalmodel.Basedonthese,SAGEvfquan-
tify the change in performance that the model restricted to the FOIs achieves
over the average prediction:
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
vm/c(S)=E L Y,fˆm/c −E L Y,fˆm/c(X ) (3)
∅ S S
We abbreviate SAGEvf depending on the distribution used for the restricted
prediction function (i.e., fˆm or fˆc) with mSAGEvf (vm) and cSAGEvf (vc).
SAGE values [10] regard FI quantification as a cooperative game, where the
features are the players, and the overall performance is the payoff. The surplus
performance (surplus payoff) enabled by adding a feature to the model depends
onwhichotherfeaturesthemodelcanalreadyaccess(coalition).Toaccountfor
thecollaborativenatureofFI,SAGEvaluesuseShapleyvalues[52]todividethe
payoff for the collaborative effort (the model’s performance) among the players
(features). SAGE values are calculated as the weighted average of the surplus
evaluations over all possible coalitions S ⊆P \{j}:
ϕm/c(v)= 1 (cid:88) (cid:18) p−1(cid:19)−1(cid:16) vm/c(S∪{j})−vm/c(S)(cid:17) ,
j p |S|
S⊆P\{j}
where the superscript in ϕ denotes whether the marginal vm(S) or conditional
j
vc(S) value function is used.
5.1 Marginal SAGE Value Functions (mSAGEvf)
Insight into X ⊥̸⊥ Y (A1): Like PFI, mSAGE value functions use marginal
j
samplingandbreakfeaturedependencies.mSAGEvfmaybenon-zero(vm({j})̸=
0), although the respective feature is not associated with Y (Negative Re-
sult 5.1.2). While an assumption about feature independence was sufficient for
PFI for insight into pairwise independence, this is generally not the case for
mSAGEvf. The feature marginalization step may lead to non-zero importance
fornon-optimalmodels(NegativeResult5.1.3).Givenfeatureindependenceand
L2orcross-entropyoptimalmodels,anon-zeromSAGEvfimpliesunconditional
association; the converse only holds for CE optimal models (Result 5.1.1).
Insight into X ⊥̸⊥ Y conditional on X or X (A2): The method
j G −j
mSAGEvf does not provide insight into the dependence between the FOI and
Y (Negative Result 5.1.2) unless the features are independent and the model is
optimal w.r.t. L2 or cross-entropy loss (Result 5.1.1). Then, mSAGEvf can be
linked to (A1) and, thus, is not suitable for (A2) (Section 3).
Result 5.1.1 (mSAGEvf interpretation). ForL2lossorcross-entropyloss-
optimal models (and the respective loss) and (X ⊥⊥X ), it holds that
j −j
vm({j})̸=0 ⇒ X ⊥̸⊥Y
j
For cross-entropy optimal predictors, the converse holds as well.A Guide to Feature Importance Methods for Scientific Inference 11
Proof. The proof can be found in Appendix A.2.
Negative Result 5.1.2. vm({j})̸=0̸⇒∃G⊆P\{j}:X ⊥̸⊥Y|X
j G
Proof (Counterexample).LetusassumethesameDGPandmodelasintheproof
ofNegativeResult4.1.2.Inthesetting,boththefullmodelfˆm(x)=x −x =0
1 1 2
andfˆm =0areoptimal,butfˆm(x )=x ̸=0issub-optimal.Thus,vm({1})̸=0
∅ 1 1 1
(although X ⊥⊥Y for any G⊆P\{j}).
1
Negative Result 5.1.3. (v({j})̸=0)∧(X ⊥⊥X )̸⇒X ⊥̸⊥Y
j −j j
Proof (Counterexample).LetX ,Y ∼N(0,1),andletX besome(potentially
1 −1
multivariate)randomvariable,withX ⊥⊥Y andX ⊥⊥X .Letfˆ(x)=x be
−1 −1 1 1
thepredictionmodel.Then,fˆm =fˆc =E [X ]=0andfˆm(x )=fˆc(x )=x .
∅ ∅ X 1 1 1 1 1 1
Since the optimal prediction Yˆ∗ = E[Y|X] = E[Y] = 0, the average prediction
fˆm = fˆc = 0 is loss-optimal and fˆm(x ) = fˆc(x ) = x is not loss-optimal.
∅ ∅ 1 1 1 1 1
Consequently, v({1}) ̸= 0 (although X is independent of target and features).
1
Notably, the example works both for vm and vc.
5.2 Conditional SAGE Value Functions (cSAGEvf)
Insight into X ⊥̸⊥ Y (A1): Like for mSAGEvf, model optimality w.r.t. L2
j
or cross-entropy loss is needed to gain insight into the dependencies in the data
(Negative result 5.1.3). However, since cSAGEvf preserves associations between
features, the assumption of independent features is not required to gain insight
into unconditional dependencies (Result 5.2.1).
Insight into X ⊥̸⊥ Y conditional on X or X (A2): Since cSAGEvf
j G −j
provide insight into (A1), they are unsuitable for gaining insight into (A2) (see
Section 3). However, the difference between cSAGEvf for different sets, called
cSAGEvf surplus (cSAGEvfsG := vc(G∪{j})−vc(G), where G ⊆ P\{j} is
j
user-specified), provides insights into conditional associations (Result 5.2.1).
Result 5.2.1 (cSAGEvf interpretation). For L2 loss or cross-entropy loss
optimal models, it holds that:
vc({j})̸=0 ⇒ X ⊥̸⊥Y
j
cSAGEvfsG ̸=0 ⇒ X ⊥̸⊥Y|X
j j G
For cross-entropy loss, the respective converse holds as well.
Proof. The first implication (and the respective converse) follows from the sec-
ond (and the respective converse) by setting G=∅. The second implication was
proven in Theorem 1 in [40]. For the converse, [10] show that for cross-entropy
optimal models v(G∪{j})−v(G)=I(Y,X |X ); it holds that I(Y,X |X )=
j G j G
0⇔X ⊥⊥Y|X .
j G12 F. K. Ewald et al.
5.3 SAGE Values
Since non-zero cSAGEvf imply (conditional) dependence and cSAGE values are
based on surplus cSAGEvfs of different coalitions, cSAGE values are only non-
zero if a conditional dependence w.r.t. some conditioning set is present (see
Result 5.3.1).
Result 5.3.1. Assuming that the ML model is optimal and using L2 loss or
cross-entropy loss, the following interpretation rule for cSAGE values holds for
a feature X :
j
ϕc(v)̸=0 ⇒ ∃S ⊆P\{j}:X ⊥̸⊥Y |X .
j j S
For cross-entropy optimal models, the converse holds as well.
Proof. The Proof can be found in Appendix A.3.
Given the complexity of the interpretation of mSAGEvf, the interpretation of
mSAGE is even more difficult.
6 Methods Based on Model Refitting
This section addresses FI methods that quantify importance by removing fea-
tures from the data and refitting the ML model. For LOCO [35], the difference
in risk of the original model and a refitted model relying on every feature but
the FOI is computed:
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
LOCO =E L Y,fˆr (X ) −E L Y,fˆ(X) ,
j −j −j
where fˆr is the newly fitted model with removed feature j, keeping the learner
−j
I(D,λ) fixed.7
Williamson et al. [61] generalize LOCO, as they are interested in not only one
FOIbutalsoinafeaturesetS ⊆P.Astheydonotassignanacronym,wefrom
here on call it Williamson’s Variable Importance Measure (WVIM):
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
WVIM =E L Y,fˆr (X ) −E L Y,fˆ(X) .
S −S −S
As [61] also assumes a loss optimal model to compute the FI value, they use
the so-called “super learner”, which is a heterogeneous ensemble approach that
combines ML from very different model classes by stacking, for the learning
algorithm in the FI, so in that sense they do not allow an arbitrarily specified
model by the user. Obviously, WVIM equals LOCO for S = j. Note that for
S =P, the optimal refit reduces to the optimal constant prediction, e.g., for an
L2-optimal model fˆr (X ) = fˆr(x ) = E[Y] and for a cross-entropy optimal
−S −S ∅ ∅
model fˆr(x )=P(Y).
∅ ∅
7 InEq.(2),wetaggedthereducedmodelsfˆmandfˆc,indicatingthetypeofmarginal-
ization. For refitting-based methods, we use the superscript r.A Guide to Feature Importance Methods for Scientific Inference 13
6.1 Leave-One-Covariate-Out (LOCO)
ForL2andcross-entropyoptimalmodels,LOCOissimilartovc(−j∪j)−vc(−j),
withthedifferencethatwedonotobtainthereducedmodelbymarginalizingout
oneofthefeatures,butratherbyrefittingthemodel.Assuch,theinterpretation
is similar to the one of cSAGEvf (Result 6.1.1).
Result 6.1.1. For an L2 or cross-entropy optimal model and the respective op-
timal reduced model fˆr , it holds that LOCO ̸= 0 ⇒ X ⊥̸⊥ Y|X . For cross-
−j j j −j
entropy loss, the converse holds as well.
Proof. For cross-entropy and L2-optimal fits, the reduced model that we obtain
fromconditionalmarginalizationbehavesthesameastheoptimalrefit(forcross-
entropy loss fˆr = fˆc = P(Y|X ), for L2 fˆr = fˆc = E[Y|X ]) [10, Appendix
S S S S S S
B] and thus LOCO =vc(j∪−j)−vc(−j). As such, the result follows directly
j
from Result 5.2.1.
6.2 WVIM as relative FI and Leave-One-Covariate-In (LOCI)
For S = j, the interpretation is the same as for LOCO. Another approach to
analyzing the relative importance of the FOI is investigating the difference in
WVIM for a group G⊆P\{j} and for the group and the FOI (G∪{j}):
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
WVIM −WVIM =E L Y,fˆr(X ) −E L Y,fˆr (X ) .
−G −(G∪{j}) G G G∪{j} G∪{j}
ItholdsthatWVIM −WVIM equalsvc(G∪{j})−vc(G),onlydiffering
−G −(G∪{j})
in the way features are removed from the prediction, so the interpretation is
similartotheoneofsurpluscSAGEvfs.AspecialcaseresultsforG=∅,i.e.,the
differenceinriskbetweentheoptimalconstantpredictionandamodelrelyingon
the FOI only. We refer to this special case (leaving one covariate in) as LOCI .
j
For cross-entropy or L2-optimal models, the interpretation is the same as for
cSAGEvf, since LOCI =vc({j}) (Result 6.2.1).
j
Result 6.2.1. For L2 or cross-entropy optimal learners, it holds that
LOCI ̸=0⇒X ⊥̸⊥Y, and
j j
WVIM −WVIM ̸=0⇒X ⊥̸⊥Y|X .
−G −(G∪{j}) j G
For cross-entropy, the converse holds as well.
Proof. For L2-optimal models, fˆc = E[Y] = fˆr and fˆc = E[Y|X ] = fˆr. For
∅ ∅ G G G
cross-entropy optimal models, fˆc =P(Y)=fˆr and fˆc =P(Y|X )=fˆr. Thus,
∅ ∅ G G G
the interpretation is the same as for cSAGEvf (Result 5.2.1).14 F. K. Ewald et al.
Yˆ modellevel a) LM with pair−wise interactions b) RF (untuned)
PFI
CFI
X1 X2 X3 X4 X5
cSAGEvf
scSAGEvf −j
mSAGEvf
X1 X2 X3 X4 X5
mSAGE
cSAGE
LOCO
Y datalevel
LOCI
−1.0 −0.5 0.0 0.5 1.0 0.00 0.25 0.50 0.75 1.00
importance
feature X1 X2 X3 X4 X5
Fig.2:Left:Graphillustratingthemodelanddatalevelassociations.Right:Re-
sultsofFImethodsfortheLMinpanel(a)andtheRFinpanel(b);importance
values are relative to the most important feature.
7 Illustrative Example
To illustrate our recommendations, we apply the FI methods to a simplified
setting where the DGP and the model’s mechanism are known and intelligi-
ble, including features with different roles. The example includes five features
X ,...,X and a target Y with the following dependence structure (visualized
1 5
in Figure 2, left plot):
– X ,X and X are independent and standard normal: X ∼N(0,1),
1 3 5 j
– X is a noisy copy of X : X :=X +ϵ ,ϵ ∼N(0,0.001),
2 1 2 1 2 2
– X is a (more) noisy copy of X : X :=X +ϵ ,ϵ ∼N(0,0.1),
4 3 4 3 4 4
– Y depends on X and X via linear effects and a bivariate interaction:
4 5
Y :=X +X +X ∗X +ϵ , ϵ ∼N(0,0.1).
4 5 4 5 Y Y
Regarding(A1),featuresX ,X andX areunconditionallyassociatedwithY,
3 4 5
while only X is conditionally associated with Y given all other features (A2a).
5
We sample n = 10,000 observations from the DGP and use 70% of the ob-
servations to train two models: A linear model (LM) with additional pair-wise
interactions between all features (test-MSE = 0.0103, test-R2 = 0.9966), and
a random forest (RF) using default hyperparameters (test-MSE =0.0189, test-
R2 = 0.9937). We apply the FI methods on 30% test data with the L2 loss to
both models using 50 repetitions for methods that marginalize or perturb the
FOI. We present the results in Figure 2.8 The right plot shows each feature’s FI
value relative to the most important feature (which is scaled to 1).
8 AllFImethodsandreproduciblescriptsfortheexperimentsareavailableonlinevia
https://github.com/slds-lmu/paper 2024 guide fi.git. Most FI methods were com-
puted using the Python package fippy (https://github.com/gcskoenig/fippy.git).A Guide to Feature Importance Methods for Scientific Inference 15
(A1):LOCI andcSAGEvf correctlyidentifyX ,X andX asunconditionally
3 4 5
associated.PFI correctlyidentifiesX andX toberelevant,however,itmisses
4 5
X , presumably since the model predominantly relies on X . For the LM, PFI
3 4
additionally considers X and X to be relevant, although they are fully inde-
1 2
pendent of Y; due to correlation in the feature sets, the trained model includes
the term 0.36x −0.36x , which cancels out in the unperturbed, original dis-
1 2
tribution, but causes performance drops when the dependence between X and
1
X is broken viapermutation. For mSAGEvf, similarobservations canbe made,
2
with the difference that X and X receive negative importance. The reason is
1 2
thatformSAGEvf,theperformanceoftheaveragepredictioniscomparedtothe
prediction where all but one feature is marginalized out; we would expect that
introducingavariableimprovestheperformance,butforX andX ,addingthe
1 2
variable worsens the performance since adding the variable breaks the depen-
dence between X and X , which are supposed to cancel each other out.
1 2
(A2): CFI, LOCO, and cSAGEvfs−j correctly identify X as conditionally as-
5
sociated, as expected. cSAGE correctly identifies features that are dependent
with Y conditional on any set S, specifically, X , X and X . The results of
3 4 5
mSAGE for the RF are similar to those for cSAGE; on the LM, the results are
quite inconclusive – most features have a negative importance.
Overall,theexampleempiricallyillustratesthedifferencesbetweenthemethods
as theoretically shown in Sections 4 to 6.
8 Summary and Practical Considerations
In Sections 4 to 6, we presented three different classes of FI techniques: Tech-
niques based on univariate perturbations, techniques based on marginalization,
andtechniquesbasedonmodelrefitting.Inprinciple,eachapproachcanbeused
to gain partial insights into questions (A1) to (A2b). However, the practicality
of the methods depends on the specific application. As follows, we discuss some
aspects that may be relevant to the practitioner.
For(A1),PFI,mSAGEvf,cSAGEvf,andLOCIare–intheory–suitable.How-
ever, PFI and mSAGEvf require assumptions about covariate independence,
which are typically unrealistic. cSAGEvf require marginalizing out variables
through a multivariate conditional distribution P(X |X ), which can be chal-
−j j
lenging since not only the dependencies between X and X but also the de-
j −j
pendencies between the marginalized out variables X have to be considered.
−j
LOCI requires fitting a univariate model, which is computationally much less
demanding than the cSAGEvf computation.
For (A2a), a comparatively more challenging task, CFI, surplus cSAGEvfs and
LOCO are suitable, but it is unclear which of the methods is preferable in prac-
tice. While CFI and cSAGEvfs require a model of the univariate conditional
P(X |X ), LOCO requires fitting a model to predict Y from X . For (A2b),
j −j −j
thepracticalrequirementsdependonthesizeoftheconditioningset.Thecloser
the conditioning set is to −j, the fewer variables have to be marginalized out
for SAGEvfs, and the fewer covariate dependencies may lead to extrapolation16 F. K. Ewald et al.
for RFI. For the difference in WVIM, larger relative feature sets imply more
expensive model fits.
Importantly,allthreequestions(A1)to(A2b)couldalsobeassessedwithdirect
or conditional independence tests, e.g., mutual information [9], partial correla-
tion tests [5], kernel-based measures such as the Hilbert-Schmidt independence
criterion [24,63], or the generalized covariance [51]. This seems particularly ap-
propriate for question (A1), where we simply model the association structure
of a bivariate distribution. Methods like mSAGEvf can arguably be considered
overly complex and computationally expensive for such a task.
Returning to our motivating example, we can now answer the open questions.
Using Result 4.1.1, we know that PFI can assign high FI values to features even
if they are not associated with the target itself but only with other features
that are associated with the target. Conversely, LOCO only assigns non-zero
values to features conditionally associated with the target (here: bike rentals
per day, cf. Result 6.1.1). We can therefore conclude that at least the features
weathersit, season, temp, mnth, windspeedandweekdayareconditionally
associated with the target, and the TOP 5 most important features, according
to PFI, tend to share information with other features or may not be associated
with bike rentals per day at all.
9 Statistical Inference for FI Methods
Sofar,wehavedescribedhowthepresentedFImethodsshouldbehaveintheory
oraspointestimators.However,theestimationofFIvaluesisinherentlysubject
tovarioussourcesofuncertaintyintroducedduringtheFIestimationprocedure,
model training, or model selection [41,61].
ThissectionreviewsavailabletechniquestoaccountforuncertaintyinFIbyap-
plying methods of statistical inference, e.g., statistical tests and the estimation
ofconfidenceintervals(CIs).AllFImethodsinthispapermeasuretheexpected
loss.Topreventbiasedormisleadingestimatesduetooverfitting,itiscrucialto
calculate FI values on independent test data not seen during training, aligning
withbestpracticesinMLperformanceassessment[54,37].ComputingFIvalues
on training data may lead to wrong conclusions. For example, Molnar et al. [43]
demonstrated that even if features are random noise and not associated with
the target, some features are incorrectly deemed important when FI values are
computed using training data instead of test data. If no large dedicated test set
is available, or the data set is not large in general to facilitate simple holdout
splitting, resampling techniques such as cross-validation or bootstrap provide
practical solutions [54].
PFI and CFI. Molnar et al. [41] address the uncertainty of model-specific PFI
and CFI values caused by estimating expected values using Monte Carlo inte-
gration on a fixed test data set and model. They introduce a variance estimator
and develop Wald-type CIs by perturbing the FOI repeatedly and measuring
the variance of the corresponding FI estimate. To address the variance of theA Guide to Feature Importance Methods for Scientific Inference 17
learning algorithm, they introduce the learner-PFI, computed using resampling
techniques such as bootstrapping or subsampling on a held-out test set within
each resampling iteration. They also propose variance-corrected Wald-type CIs
to compensate for the underestimation of variance caused by partially sharing
training data between the models fitted in each resampling iteration. For CFI,
Watson and Wright [59] address sampling uncertainty by comparing instance-
wise loss values. The authors use Fisher’s exact (permutation) tests and paired
t-testsforhypothesistesting.Thelatter,basedonthecentrallimittheorem,can
be applied to all decomposable loss functions that are calculated by averaging
instance-wise losses.
SAGE. The original paper of SAGE [10] introduced an efficient algorithm to
approximate SAGE values, since the exact calculation of SAGE values is com-
putationally expensive. They show that, according to the central limit theorem,
the approximation algorithm convergences to the correct values and that the
variance reduces with the number of iterations at a linear rate. They briefly
mention that the variance of the approximation can be estimated at a specific
iteration and can be used to construct CIs (which corresponds to the same un-
derlyingideaoftheWald-typeCIforthemodel-specificPFImentionedearlier).
LOCO and WVIM. Lei et al. [35] introduced statistical inference for LOCO by
splitting the data into two parts: one for model fitting and one for estimating
LOCO. They further employed hypothesis testing and constructing CIs using
sign tests or the Wilcoxon signed-rank test. The interpretation of the results is
limited to the importance of the FOI to an ML algorithm’s estimated model on
afixedtrainingdataset.Williamsonetal.[61]constructWald-typeCIintervals
for LOCO and its generalization WVIM, based on k-fold cross-validation and
sample-splitting9. Compared to LOCO, it provides a more general interpreta-
tion of the results as it considers the FI of an ML algorithm trained on samples
of a particular size (i.e., due to cross-validation, the results are not tied to a
single training data set). The approach is related to [41] but removes features
via refitting instead of marginal or conditional sampling and does not consider
any variance correction. The authors note that while sample-splitting helps to
address issues related to zero-importance features having an incorrect type I er-
ror or coverage of their CIs, it may not fully leverage all available information
in the data set to train a model.
PIMP. The PIMP heuristic [2] is based on model refits and was initially devel-
oped to address bias in FI measures such as PFI within random forests. How-
ever, PIMP is a general procedure and has broader applicability across various
FI methods [36,43]. PIMP involves repeatedly permuting the target to disrupt
its associations with features while preserving feature dependencies, training a
model on the data with the permuted target, and computing PFI values. This
leads to a collection of PFI values (called null importances) under the assump-
tion of no association between the FOI and the target. The PFI value of the
9 Thisinvolvesdividingthek-foldsintotwopartstoservedistinctpurposes,allowing
for separate estimation and testing procedures.18 F. K. Ewald et al.
model trained on the original data is then compared with the distribution of
null importances to identify significant features.
Methods based on the Rashomon set. The Rashomon set refers to a collection
of models that perform equally well but may differ in how they construct the
prediction function and the features they rely on. Fisher et al. [18] consider the
Rashomon set of a specific model class (e.g. decision trees) defined based on a
performance threshold and propose a method to measure the FI within this set.
ForeachmodelintheRashomonset,theFIofaFOIiscomputed,anditsrange
across all models within the Rashomon set is reported. Other works include the
Variable Importance Cloud (VIC) [13], providing a visual representation of FI
valuesoverdifferentmodeltypes,RashomonImportanceDistribution(RID)[14],
providing the FI distribution across the set and CIs to characterize uncertainty
aroundFIpointestimates,andShapleyVIC[44],extendingVICtoSAGEvalues
and using a variance estimator for constructing CIs. Here, the main idea is to
address uncertainty in model selection by analyzing a Rashomon set of well-
performingmodelsinthehopethatsomeofthesemodelsreflectthebehaviorof
the underlying DGP and assign similar FI values to features.
Multiple Comparisons. Testing multiple FI values simultaneously poses a chal-
lengeknownasmultiplecomparisons,wheretheriskoffalselyrejectingtruenull
hypotheses increases with the number of comparisons. Many of the described
methods that provide p-values or estimate CIs do not warn regarding this is-
sue, hence it is often overlooked in applications as well – but see [56,4] for some
notable exceptions. However, practitioners should always keep this problem in
mind, and mitigate it, e.g., by controlling the family-wise error rate or the false
discovery rate [49,43].
10 Open Challenges and Further Research
Feature Interactions. FI computations are usually complicated by the presence
of strong and higher-order interactions [43]. Such interactions typically have to
bemanuallyspecifiedin(semi-)parametricstatisticalmodels.However,complex
non-parametric ML models, to which we usually apply our model-agnostic IML
techniques, automatically include higher-order interaction effects. While recent
advances have been made in visualizing the effect of feature interactions and
quantifyingtheircontributionregardingthepredictionfunction[3,23,27],wefeel
that this topic is somewhat underexplored in the context of loss-based FI meth-
ods, i.e., how much an interaction contributes to the predictive performance.
A notable exception is SAGE, which, however, does not explicitly quantify the
contribution of interactions towards the predictive performance but rather dis-
tributes interaction importance evenly among all interacting features. In future
work, this could be extended by combining ideas from functional decomposi-
tion [3,27], FI based on those [29] and loss-based methods as in SAGE.A Guide to Feature Importance Methods for Scientific Inference 19
Model Selection and AutoML. As a subtle but important point: it seems some-
whatuncleartowhichmodelclassorlearningalgorithmsthecoveredtechniques
can or should be applied to, if DGP inference is the goal. From a mechanistic
perspective, these model-agnostic FI approaches can be applied to basically any
modelclass,whichseemstobethecaseincurrentapplications.Consideringwhat
Williamsonetal.[61]notedinand,followingourresults,manystatementsinthe
Sections 4 to 6 only hold under a “loss-optimal model”. First of all, in practice,
the construction of a loss-optimal model with certainty is virtually impossible.
Does this imply we should try to squeeze out as much predictive performance
as possible, regardless of the incurred extra model complexity? As described in
Section 6, Williamson et al. [61] use the “super learner” in their definition and
implementation of W-VIM [60]. Modern AutoML systems like AutoGluon [16]
are based on the same principle. While we perfectly understand that choice,
and find the combination of AutoML and IML techniques very exciting, we are
unsure about the trade-off costs. Certainly, this is a computationally expensive
technique. But we rather also worry about the underlying implications for FI
methods(ormoregenerallyIMLtechniques),whenmodelsofbasicallythehigh-
estorderofcomplexityarenowused,whichusuallycontainnearlyunconstrained
higher-order interactions. We think that this issue needs to be more analyzed.
RashomonSetsandModelDiagnosis.Expandingonthepreviousissue:Inclassi-
calstatisticalmodeling,modelsareusuallynotexclusivelyvalidatedbychecking
predictiveperformancemetricsonly.TheRashomoneffecttellsusthatinquitea
fewscenarios,verysimilarlyperformingmodelsexist,whichgiverisetodifferent
responsesurfacesanddifferentIMLinterpretations.Thishintsattheeffectthat
ML researchers and data scientists might likely have to expand their model val-
idation toolbox, in order to have better options to exclude mispecified models.
Empirical Performance Comparisons.Wehavetriedtocompileasuccinctlistof
theoremstodescribewhatcanbederivedfromvariousFImethodsregardingthe
DGP. However, we would also like to note that such theoretical analysis often
considerably simplifies the complexity of real-world scenarios to which we apply
these techniques. For that reason, it is usually a good idea to complement such
mathematicalanalysiswith informative,detailed,andcarefullyconstructedem-
pirical benchmarks. Unfortunately, not a lot of work on empirical benchmarks
exists in this area. Admittedly, this is not easy in FI, as ground truths are often
only available in simulations, which, in turn, lack the complexity found in real-
world data sets. Moreover, even in simulations, concrete “importance ground
truth numbers” might be debatable. So far, there are no extensive benchmarks
intheliteratureonFImethods.Manycomparelocalimportancemethods[1,26],
but few global methods: E.g., Blesch et al. [6] and Covert et al. [10] compare FI
methods for different data sets, metrics, and ML models. However, the compar-
isons are not applied with regard to different association types, as the methods
are not differentiated in this respect as in our paper.
Causality.Beyondassociation,scientificpractitionersareofteninterestedincau-
sation(see,e.g.,[62,58,22,21,50]).InourexamplefromSection3,thedoctormay
notonlywanttopredictthediseasebutmayalsowanttotreatit.Knowingwhich20 F. K. Ewald et al.
features are associated with the disease is insufficient for that purpose – associ-
ation remains on rung 1 of the so-called ladder of causation [47]: Although the
symptoms are associated with the disease, treating the symptoms does not af-
fectY.Togaininsightintotheeffectsofinterventions(rung2),experimentation
and/or causal knowledge as well as specialized tools are required [46,31,48,28].
Acknowledgements
MNWwassupportedbytheGermanResearchFoundation(DFG),GrantNum-
bers: 437611051, 459360854. GK was supported by the German Research Foun-
dationthroughtheClusterofExcellence“MachineLearning-NewPerspectives
for Science” (EXC 2064/1 number 390727645).
Appendix
A Additional proofs
A.1 Proof of Result 4.3.1
Proof. We show X ⊥̸⊥Y|X ⇒RFIG ̸=0. For cross-entropy loss
j G j
RFI =(E [D (p(y|x)||f(y|x ,x˜ ))]−H(Y|X))
j X KL −j j
−(E [D (p(y|x)||f(y|x))]−H(Y|X))
X KL
f=p
= E [D (p(y|x)||f(y|x ,x˜ )]
X KL −j j
It remains to show that KL-divergence for f(y,x ,x˜ ) is non-zero.
−j j
p(x ,y,x ,x )=p(x |y,x ,x )
j G R j G R
=p(x |y,x )p(y,x ,x ) (X ⊥⊥X |X ,Y)
j G G R j R G
̸=p(x |x )p(y,x ,x ) (X ⊥̸⊥Y|X )
j G G R j G
=p(x˜|x )p(y,x ,x ) (def. of X˜ )
j G G R j
=p(x˜,y,x ,x )
j G R
Since X ⊥⊥ X |X it holds that p(x˜ ,x ,x ) = p(x ,x ,x ) and, thus,
j R G j R G j R G
p(y|x˜ ,x ,x ) ̸= p(y|x ,x ,x ). In combination with model optimality, we
j R G j R G
get p(y|x) ̸= f(y|x˜ ,x ). Since KL divergence > 0 for p ̸= f it holds that
j −j
RFI >0. ⊔⊓
j
A.2 Proof of Result 5.1.1: mSAGEvf interpretation
Proof. The implication is shown by proving the counterposition:
X ⊥⊥(Y,X )⇒vm({j})=0.
j −j
X ⊥⊥ (Y,X ) implies that X ⊥⊥ X and X ⊥⊥ Y. Since X ⊥⊥ (Y,X ), it
j −j j −j j j −j
holds that f∗,m(x )=f∗,c(x ) and thus vm({j})=vc({j}).
j j j j
Because X ⊥⊥Y it follows that vc({j})=0 (Result 5.2.1). Thus, vm({j})=0.
j
⊔⊓A Guide to Feature Importance Methods for Scientific Inference 21
A.3 Proof of Result 5.3.1: cSAGE interpretation
Proof. The equation is shown by proving the contraposition
∀S ⊆P\{j}:X ⊥⊥Y |X ⇒ ϕc(v)=0. (4)
j S j
From Result 5.2.1 we know that X ⊥⊥ Y|X ⇒ vc(G∪{j})−vc(G) = 0 for
j G
L2 and cross-entropy optimal predictors. If ∀S ⊆ P\{j} : X ⊥⊥ Y |X , all
j S
summands of the SAGE value are zero, and thus ϕc =0.
j
Converse for cross-entropy loss: We prove the converse by counterposition, i.e.,
by showing that
ϕc(v)=0 ⇒ ∀S ⊆P\{j}:X ⊥⊥Y |X .
j j S
If L is the cross-entropy loss and f∗ the Bayes model, using [10, Appendix C.1]
for the left part results in
1 (cid:88)
(cid:18) p−1(cid:19)−1
ϕc(v)= I(Y;X |X )=0,
j p |S| j S
S⊆P\{j}
wherethemutualinformationandthecoefficientsarealwaysnon-negative.Thus,
we add non-negative terms so the sum can only be zero if
∀S ⊆P\{j}:I(Y;X |X )=0
j S
⇒ ∀S ⊆P\{j}:X ⊥⊥Y |X .
j S
⊔⊓
References
1. Agarwal,C.,Krishna,S.,Saxena,E.,Pawelczyk,M.,Johnson,N.,Puri,I.,Zitnik,
M., Lakkaraju, H.: OpenXAI: Towards a Transparent Evaluation of Model Ex-
planations. Advances in Neural Information Processing Systems 35, 15784–15799
(2022)
2. Altmann, A., Tolo¸si, L., Sander, O., Lengauer, T.: Permutation Importance: A
Corrected Feature Importance Measure. Bioinformatics 26(10), 1340–1347 (2010)
3. Apley, D.W., Zhu, J.: Visualizing the Effects of Predictor Variables in Black Box
Supervised Learning Models. Journal of the Royal Statistical Society Series B:
Statistical Methodology 82(4), 1059–1086 (2020)
4. Au, Q., Herbinger, J., Stachl, C., Bischl, B., Casalicchio, G.: Grouped Feature
ImportanceandCombinedFeaturesEffectPlot.DataMiningandKnowledgeDis-
covery 36(4), 1401–1450 (2022)
5. Baba,K.,Shibata,R.,Sibuya,M.:PartialCorrelationandConditionalCorrelation
as Measures of Conditional Independence. Australian & New Zealand Journal of
Statistics 46(4), 657–664 (2004)
6. Blesch,K.,Watson,D.S.,Wright,M.N.:ConditionalFeatureImportanceforMixed
Data. AStA Advances in Statistical Analysis pp. 1–20 (2023)
7. Breiman, L.: Random Forests. Machine Learning 45, 5–32 (2001)22 F. K. Ewald et al.
8. Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., Elhadad, N.: Intelligible
Models for Healthcare: Predicting Pneumonia Risk and Hospital 30-Day Read-
mission. In: Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. pp. 1721–1730 (2015)
9. Cover, T.M.: Elements of Information Theory. John Wiley & Sons (1999)
10. Covert,I.,Lundberg,S.M.,Lee,S.I.:UnderstandingGlobalFeatureContributions
with Additive Importance Measures. Advances in Neural Information Processing
Systems 33, 17212–17223 (2020)
11. Covert, I.C., Lundberg, S., Lee, S.I.: Explaining by Removing: A Unified Frame-
work for Model Explanation. The Journal of Machine Learning Research 22(1),
9477–9566 (2021)
12. Das, A., Rad, P.: Opportunities and Challenges in Explainable Artificial Intelli-
gence (XAI): A Survey. arXiv preprint arXiv:2006.11371 (2020)
13. Dong, J., Rudin, C.: Variable Importance Clouds: A Way to Explore Variable
Importance for the Set of Good Models. arXiv preprint arXiv:1901.03209 (2019)
14. Donnelly, J., Katta, S., Rudin, C., Browne, E.: The Rashomon Importance Dis-
tribution:GettingRIDofUnstable,SingleModel-basedVariableImportance.Ad-
vances in Neural Information Processing Systems 36 (2024)
15. Doshi-Velez, F., Kortz, M., Budish, R., Bavitz, C., Gershman, S.J., O’Brien, D.,
Scott, K., Shieber, S., Waldo, J., Weinberger, D., et al.: Accountability of AI Un-
der the Law: The Role of Explanation. Berkman Center Research Publication,
Forthcoming (2017)
16. Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., Smola, A.:
Autogluon-tabular: Robust and Accurate AutoML for Structured Data. arXiv
preprint arXiv:2003.06505 (2020)
17. Fanaee-T,H.,Gama,J.:EventLabelingCombiningEnsembleDetectorsandBack-
ground Knowledge. Progress in Artificial Intelligence pp. 1–15 (2013)
18. Fisher, A., Rudin, C., Dominici, F.: All Models are Wrong, but Many are Useful:
LearningaVariable’sImportancebyStudyinganEntireClassofPredictionModels
Simultaneously. Journal of machine learning research: JMLR 20, 177 (2019)
19. Freiesleben, T., K¨onig, G.: Dear XAI Community, We Need to Talk! In: World
Conference on Explainable Artificial Intelligence. pp. 48–65. Springer (2023)
20. Friedman, J.H.: Greedy Function Approximation: A Gradient Boosting Machine.
Annals of statistics pp. 1189–1232 (2001)
21. Gangl,M.:CausalInferenceinSociologicalResearch.AnnualReviewofSociology
36, 21–47 (2010)
22. Glass,T.A.,Goodman,S.N.,Herna´n,M.A.,Samet,J.M.:CausalInferenceinPub-
lic Health. Annual Review of Public Health 34, 61–75 (2013)
23. Greenwell, B.M., Boehmke, B.C., McCarthy, A.J.: A Simple and Effective Model-
Based Variable Importance Measure. arXiv preprint arXiv:1805.04755 (2018)
24. Gretton, A., Bousquet, O., Smola, A., Scho¨lkopf, B.: Measuring Statistical De-
pendence with Hilbert-Schmidt Norms. In: Algorithmic Learning Theory: 16th
International Conference, ALT 2005, Singapore, October 8-11, 2005. Proceedings
16. pp. 63–77. Springer (2005)
25. Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.:
ASurveyofMethodsforExplainingBlackBoxModels.ACMComputingSurveys
(CSUR) 51(5), 1–42 (2018)
26. Han, T., Srinivas, S., Lakkaraju, H.: Which Explanation Should I Choose? A
Function Approximation Perspective to Characterizing Post Hoc Explanations.
Advances in Neural Information Processing Systems 35, 5256–5268 (2022)A Guide to Feature Importance Methods for Scientific Inference 23
27. Herbinger, J., Bischl, B., Casalicchio, G.: Decomposing Global Feature Effects
based on Feature Interactions. arXiv preprint arXiv:2306.00541 (2023)
28. Hernan, M., Robins, J.: Causal Inference: What If. CRC Press (2023)
29. Hiabu, M., Meyer, J.T., Wright, M.N.: Unifying Local and Global Model Expla-
nationsbyFunctionalDecompositionofLowDimensionalStructures.In:Interna-
tional Conference on Artificial Intelligence and Statistics. pp. 7040–7060. PMLR
(2023)
30. Hooker,G.,Mentch,L.,Zhou,S.:UnrestrictedPermutationForcesExtrapolation:
Variable Importance Requires at Least One More Model, or There Is No Free
Variable Importance. Statistics and Computing 31(6), 82 (2021)
31. Imbens,G.W.,Rubin,D.B.:CausalInferenceinStatistics,Social,andBiomedical
Sciences. Cambridge University Press (2015)
32. Jordan, M.I., Mitchell, T.M.: Machine Learning: Trends, Perspectives, and
Prospects. Science 349(6245), 255–260 (2015)
33. K¨onig, G., Molnar, C., Bischl, B., Grosse-Wentrup, M.: Relative Feature Impor-
tance.In:202025thInternationalConferenceonPatternRecognition(ICPR).pp.
9318–9325. IEEE (2021)
34. Krishna, S., Han, T., Gu, A., Pombra, J., Jabbari, S., Wu, S., Lakkaraju, H.:
The Disagreement Problem in Explainable Machine Learning: A Practitioner’s
Perspective. arXiv preprint arXiv:2202.01602 (2022)
35. Lei,J.,G’Sell,M.,Rinaldo,A.,Tibshirani,R.J.,Wasserman,L.:Distribution-Free
PredictiveInferenceforRegression.JournaloftheAmericanStatisticalAssociation
113(523), 1094–1111 (2018)
36. Linardatos, P., Papastefanopoulos, V., Kotsiantis, S.: Explainable AI: A Review
of Machine Learning Interpretability Methods. Entropy 23(1), 18 (2020)
37. Lones, M.A.: How to Avoid Machine Learning Pitfalls: A Guide for Academic
Researchers. arXiv preprint arXiv:2108.02497 (2021)
38. Lundberg,S.M.,Erion,G.G.,Lee,S.I.:ConsistentIndividualizedFeatureAttribu-
tion for Tree Ensembles. arXiv preprint arXiv:1802.03888 (2019)
39. Lundberg,S.M.,Lee,S.I.:AUnifiedApproachtoInterpretingModelPredictions.
Advances in Neural Information Processing Systems 30 (2017)
40. Luther,C.,Ko¨nig,G.,Grosse-Wentrup,M.:EfficientSAGEEstimationviaCausal
Structure Learning. In: International Conference on Artificial Intelligence and
Statistics. pp. 11650–11670. PMLR (2023)
41. Molnar,C.,Freiesleben,T.,Ko¨nig,G.,Herbinger,J.,Reisinger,T.,Casalicchio,G.,
Wright, M.N., Bischl, B.: Relating the Partial Dependence Plot and Permutation
Feature Importance to the Data Generating Process. In: World Conference on
Explainable Artificial Intelligence. pp. 456–479. Springer (2023)
42. Molnar,C.,Ko¨nig,G.,Bischl,B.,Casalicchio,G.:Model-agnosticFeatureImpor-
tance and Effects with Dependent Features – A Conditional Subgroup Approach.
Data Mining and Knowledge Discovery pp. 1–39 (2023)
43. Molnar, C., Ko¨nig, G., Herbinger, J., Freiesleben, T., Dandl, S., Scholbeck, C.A.,
Casalicchio,G.,Grosse-Wentrup,M.,Bischl,B.:GeneralPitfallsofModel-Agnostic
Interpretation Methods for Machine Learning Models. In: Holzinger, A., Goebel,
R.,Fong,R.,Moon,T.,Mu¨ller,K.R.,Samek,W.(eds.)xxAI-BeyondExplainable
AI:InternationalWorkshop,HeldinConjunctionwithICML2020,July18,2020,
Vienna,Austria,RevisedandExtendedPapers,pp.39–68.SpringerInternational
Publishing, Cham (2022)
44. Ning,Y.,Ong,M.E.H.,Chakraborty,B.,Goldstein,B.A.,Ting,D.S.W.,Vaughan,
R., Liu, N.: Shapley Variable Importance Cloud for Interpretable Machine Learn-
ing. Patterns 3(4) (2022)24 F. K. Ewald et al.
45. Owen, A.B.: Variance Components and Generalized Sobol’ Indices. SIAM/ASA
Journal on Uncertainty Quantification 1(1), 19–41 (2013)
46. Pearl, J.: Causality. Cambridge University Press (2009)
47. Pearl,J.,Mackenzie,D.:TheBookofWhy:TheNewScienceofCauseandEffect.
Basic books (2018)
48. Peters, J., Janzing, D., Scho¨lkopf, B.: Elements of Causal Inference: Foundations
and Learning Algorithms. The MIT Press (2017)
49. Romano, J.P., Shaikh, Azeem M. Wolf, M.: Multiple Testing, pp. 1–5. Palgrave
Macmillan UK, London (2016)
50. Rothman, K.J., Greenland, S.: Causation and Causal Inference in Epidemiology.
American Journal of Public Health 95(S1), S144–S150 (2005)
51. Shah, R.D., Peters, J.: The Hardness of Conditional Independence Testing and
the Generalised Covariance Measure. The Annals of Statistics 48(3), 1514 – 1538
(2020)
52. Shapley,L.S.:NotesontheN-PersonGame–II:TheValueofanN-PersonGame.
RAND Corporation, Santa Monica, CA (1951)
53. Shmueli,G.:ToExplainortoPredict?StatisticalScience25(3),289–310(2010)
54. Simon, R.: Resampling Strategies for Model Assessment and Selection. In: Fun-
damentals of Data Mining in Genomics and Proteomics, pp. 173–186. Springer
(2007)
55. Sobo´l,I.:SensitivityEstimatesforNonlinearMathematicalModels.Math.Model.
Comput. Exp. 1 (1993)
56. Stachl,C.,Au,Q.,Schoedel,R.,Gosling,S.D.,Harari,G.M.,Buschek,D.,V¨olkel,
S.T., Schuwerk, T., Oldemeier, M., Ullmann, T., et al.: Predicting personality
frompatternsofbehaviorcollectedwithsmartphones.ProceedingsoftheNational
Academy of Sciences 117(30), 17680–17687 (2020)
57. Strobl,C.,Boulesteix,A.L.,Kneib,T.,Augustin,T.,Zeileis,A.:ConditionalVari-
able Importance for Random Forests. BMC Bioinformatics 9(1), 1–11 (2008)
58. Varian, H.R.: Causal Inference in Economics and Marketing. Proceedings of the
National Academy of Sciences 113(27), 7310–7315 (2016)
59. Watson, D.S., Wright, M.N.: Testing Conditional Independence in Supervised
Learning Algorithms. Machine Learning 110(8), 2107–2129 (2021)
60. Williamson, B.D.: vimp: Perform Inference on Algorithm-Agnostic Variable Im-
portance (2023), R package version 2.3.3
61. Williamson, B.D., Gilbert, P.B., Simon, N.R., Carone, M.: A General Framework
forInferenceonAlgorithm-AgnosticVariableImportance.JournaloftheAmerican
Statistical Association 118(543), 1645–1658 (2023)
62. Yazdani, A., Boerwinkle, E.: Causal Inference in the Age of Decision Medicine.
Journal of Data Mining in Genomics & Proteomics 6(1) (2015)
63. Zhang, K., Peters, J., Janzing, D., Scho¨lkopf, B.: Kernel-based Conditional
Independence Test and Application in Causal Discovery. arXiv preprint
arXiv:1202.3775 (2012)
64. Zien,A.,Kra¨mer,N.,Sonnenburg,S.,Ra¨tsch,G.:TheFeatureImportanceRanking
Measure.In:MachineLearningandKnowledgeDiscoveryinDatabases:European
Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceed-
ings, Part II 20. pp. 694–709. Springer (2009)