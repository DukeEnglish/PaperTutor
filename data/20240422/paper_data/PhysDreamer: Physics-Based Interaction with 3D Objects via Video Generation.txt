PhysDreamer: Physics-Based Interaction with
3D Objects via Video Generation
Tianyuan Zhang1 Hong-Xing Yu2 Rundi Wu3 Brandon Y. Feng1
Changxi Zheng3 Noah Snavely4 Jiajun Wu2 William T. Freeman1
1Massachusetts Institute of Technology 2Stanford University
3Columbia University 4Cornell University
Abstract. Realistic object interactions are crucial for creating immer-
sivevirtualexperiences,yetsynthesizingrealistic3Dobjectdynamicsin
responsetonovelinteractionsremainsasignificantchallenge.Unlikeun-
conditionalortext-conditioneddynamicsgeneration,action-conditioned
dynamics requires perceiving the physical material properties of objects
and grounding the 3D motion prediction on these properties, such as
object stiffness. However, estimating physical material properties is an
openproblemduetothelackofmaterialground-truthdata,asmeasur-
ingthesepropertiesforrealobjectsishighlydifficult.WepresentPhys-
Dreamer, a physics-based approach that endows static 3D objects with
interactive dynamics by leveraging the object dynamics priors learned
byvideogenerationmodels.Bydistillingthesepriors,PhysDreameren-
ables the synthesis of realistic object responses to novel interactions,
such as external forces or agent manipulations. We demonstrate our
approach on diverse examples of elastic objects and evaluate the real-
ism of the synthesized interactions through a user study. PhysDreamer
takes a step towards more engaging and realistic virtual experiences by
enabling static 3D objects to dynamically respond to interactive stim-
uli in a physically plausible manner. See our project page at https:
//physdreamer.github.io/.
Keywords: Physics-based modeling · Interactive 3D dynamics
1 Introduction
Realisticobjectinteractionsplayapivotalroleincreatingimmersivevirtualex-
periences. Recent advances in 3D vision have enabled the capture and creation
ofhigh-qualitystatic3Dassets[36,50],andsomemethodsevenextendto4Das-
sets [46,47,59], generating unconditioned dynamics. However, these approaches
cannot handle interactive action-conditioned dynamics, which is crucial for syn-
thesizing realistic object responses to novel interactions, such as external forces
or agent manipulations.
Action-conditioneddynamicsdiffersfromunconditionalandtext-conditioned
dynamics in that it requires perceiving the physical material properties of ob-
jectsandgroundingthe3Dmotionpredictionontheseproperties,suchasobject
stiffness. Yet, estimating physical material properties is a challenging task due
4202
rpA
91
]VC.sc[
1v62031.4042:viXra2 Zhang et al.
3D Gaussians Material field
Video genera�on model
PhysDreamer Synthsized 3D dynamics under interac�on
Fig.1:(Left)Leveraginganddistillingpriorknowledgeofdynamicsfromapre-trained
video generation model, we estimate a physical material field for the static 3D object.
(Right)Thephysicalmaterialfieldallowssynthesizinginteractive3Ddynamicsunder
arbitrary forces. We show rendered sequences from two viewpoints, with red arrows
indicating force directions. Please see the videos on our project website for better
visualization.
to the lack of ground-truth data, as measuring these properties for real objects
is highly difficult. Real-life objects often exhibit complex, spatially-varying ma-
terial properties, making the estimation problem even more challenging.
Inthiswork,wefocusonsynthesizinginteractive3Ddynamics.Ourgoalisto
transformstatic3Dobjectsintointeractiveonesthatcanrespondtonovelinter-
actions in a physically plausible manner. To address this challenge, we propose
PhysDreamer, a physics-based approach that leverages the object dynamics
priors learned by video generation models. The key idea behind PhysDreamer
is to distill these priors to estimate the physical material properties of static 3D
objects.Wehypothesizethatvideogenerationmodels,trainedonlargeamounts
ofvideodata,implicitlycapturetherelationshipbetweenobjectappearanceand
dynamics. By leveraging this learned prior knowledge, PhysDreamer can infer
thephysicalmaterialpropertiesthatdrivethedynamicbehaviorofobjects,even
in the absence of ground-truth material data (Fig. 1).
PhysDreamer represents 3D objects using 3D Gaussians [36], models the
physical material field with a neural field [71], and simulates 3D dynamics using
the differentiable Material Point Method (MPM) [28]. The differentiable simu-
lation and rendering allow for direct optimization of the physical material field
andinitialvelocityfieldbymatchingpixelspaceobservations.Wefocusonelas-
ticdynamicsandshowcasePhysDreamerthroughdiverserealexamples,suchas
flowers, plants, a beanie hat, and a telephone cord.
Weevaluatetherealismofthesynthesizedinteractivemotionthroughauser
study,comparingPhysDreamertostate-of-the-artmethods.Theresultsdemon-
strate that our approach significantly outperforms existing techniques in terms
ofmotionrealism,validatingtheeffectivenessofleveragingvideogenerationpri-
ors for estimating physical material properties and synthesizing interactive 3D
dynamics.PhysDreamer 3
In summary, PhysDreamer addresses the challenge of synthesizing interac-
tive 3D dynamics by leveraging the object dynamics priors learned by video
generation models. By distilling these priors to estimate the physical material
properties of static 3D objects, our approach enables the creation of immersive
virtual experiences where objects can respond realistically to novel interactions.
The main contributions of our work include enabling static 3D objects to dy-
namically respond to interactive stimuli in a physically plausible manner and
taking a step towards more engaging and realistic virtual experiences. We be-
lieve that PhysDreamer has the potential to greatly enhance the realism and
interactivity of virtual environments, paving the way for more engaging and
lifelike simulations.
2 Related work
2.1 Dynamic 3D reconstruction
Dynamic 3D reconstruction methods aim to reconstruct a representation of a
dynamic scene from inputs such as depth scans [13,41], RGBD videos [51], or
monocular or multi-view videos [1,40,45,47,52,53,57,66,69,74,75]. This task is
especially challenging in the monocular setting with slow-moving cameras and
fast-moving scenes [20]. Novel scene representations are a major driver of recent
progress. One prominent approach is to augment a canonical Neural Radiance
Fields (NeRF) with a deformation field [57]. This approach can be further im-
provedbyincorporatingflowsupervision[23,66]oras-rigid-as-possibleorvolume
preservingregularizationterms[52,53].Time-modulatedNeRFs[7,19,20,43]offer
asimpleralternativerepresentation.DuetoitsLagrangiannature,3DGaussian
Splatting [36] is readily adaptable to the task of efficient dynamic scene recon-
struction [17,31,40,47,72,74]. Data-driven prior, such as from monocular depth
models [39,77] and image diffusion models [67], can also be used to reduce the
inherent ambiguity in dynamic reconstruction from monocular videos.
2.2 Dynamic 3D generation
Our work also relates to efforts to synthesize dynamic 3D scenes. A common
approach is to integrate a 3D generation pipeline with a video generation model
[2,46,59,61]. For instance, Make-A-Video3D begins by creating a static NeRF
as per DreamFusion [56], then extending it temporally using Score Distillation
Sampling (SDS) [56] derived from a video diffusion model. The approach can
be improved with more efficient representations, stronger diffusion priors, and
stable training techniques [2,46]. However, applying SDS with video diffusion
models demands significant computational and memory costs. Compact4D [73]
and DreamGaussian4D [59] used a more efficient approach, synthesizing 3D dy-
namics by aligning a reference video from video generation models while em-
ploying SDS from image diffusion models to reduce novel view artifacts. These
methods are currently limited to producing fixed-length 3D videos. We focus on
synthesizing interactive 3D motions under any new physical interactions.4 Zhang et al.
2.3 Interactive motion generation
Interactive motion generation animates still images or 3D contents according
to user inputs like text [11,76], motion fields [21], motion layers [12,14], or
directmanipulationsuchasdraggingandpulling[15,44].EarlyworkfromDavis
et al. [15,16] demonstrated animating an image using an image-space modal
basisextractedfromavideoofanobjectundergoingsubtlevibrationalmotions.
Buildinguponthisimage-spacerepresentation[15],GenerativeImageDynamics
[44]usedadiffusionmodeltrainedonadatasetwithpairedimageanditsmodal
basistomodelscenemotiondistributions,enablingrealisticinteractionwithstill
input images. We focus on interacting with 3D objects rather than images.
For 3D assets, physics-based approaches enable synthesizing motions under
anyphysicalinteractions.VirtualElasticObjects[10]jointlyreconstructsthege-
ometry, appearances, and physical parameters of elastic objects in a multiview
capture setup with compressed air system, allowing realistic animations under
any new physical interactions. PAC-NeRF [42] and PhysGaussian [18] integrate
physics-based simulations with NeRF and 3D Gaussians, respectively, to gen-
erate physically plausible motions. We use the same physics-based approach to
generaterealisticinteractions,butanovelingredientofourworkistodistillthe
material parameters of the object from pre-trained video generation models.
2.4 Video generation models
Recent progress in video generation is driven by the development of larger au-
toregressive[27,38,65,68]anddiffusionmodels[3–6,22,24,26,60].Thesemodels,
trained on increasingly large datasets, continue to advance the quality and real-
ism of generated video content. The state-of-the-art approach [6] can generate
minute-long videos with realistic motions and viewpoint consistency. However,
current video generation models cannot support physics-based control, like in-
teractions with objects through external forces. Our approach leverages strong
motionpriorfromavideogenerationmodel,andasthefieldofvideogeneration
continues to advance, we expect our method to yield even better results.
3 Problem formulation
Givenastaticobjectrepresentedby3DGaussians{G }P ,G ={x ,α ,Σ ,c }
p p=1 p p p p p
(where x denotes the position, α denotes the opacity, Σ denotes the covari-
p p p
ance matrix, and c denotes the color of the particle), our goal is to estimate
p
physical material property fields for the object to enable realistic interactive
motion synthesis. These properties include mass m, Young’s modulus E, and
Poisson’s ratio ν. Among these physical properties, Young’s modulus E plays
a particularly important role in determining the object’s motion in response to
applied forces. Intuitively, Young’s modulus (Eq. 2) measures the material stiff-
ness. A higher Young’s modulus results in less deformation and more rigid and
higher-frequency motion, while a lower value leads to more flexible and elasticPhysDreamer 5
Young's modulus 1x Young's modulus 10x Young's modulus 100x
3D Gaussians 10.30 seconds 2.30 seconds 0.87 seconds
Fig.2: Effect of Young’s modulus. We depict the motion of a simulated flower under
thesameexternalforcebutwiththreedifferentYoung’smoduli,ameasureofmaterial
stiffness.FlowerswiththehighestYoung’smodulus(100×)exhibitsmalleroscillations
and higher frequencies, while the flower with the lowest Young’s modulus (1×) sways
the most and oscillates at the lowest frequency. Time annotations below each image
indicate the duration of one complete motion path shown in the figure.
behavior. Fig. 2 illustrates the simulated motion of a flower under the same
applied forces but with different Young’s modulus.
Therefore,ourproblemformulationfocusesonestimatingthespatiallyvary-
ing Young’s modulus field E(x) for the 3D object. To allow particle simulation,
we query a particle’s Young’s modulus by E = E(x ). As for other physical
p p
properties, the mass for a particle m can be pre-computed as the product of
p
a constant density (ρ) and particle volume V . The particle volume can be es-
p
timated [70] by dividing a background cell’s volume by the number of particles
that cell contains. As for the Poisson’s ratio ν , we found that it has negligible
p
impact on object motion in our preliminary experiments, and so we assume a
homogeneous constant Poisson’s ratio.
4 PhysDreamer
PhysDreamer estimates a material field for a static 3D object. Our key idea is
to generate a plausible video of the object in motion, and then optimize the
material field E(x) to match this synthesized motion. We begin by rendering
a static image (I ) for the 3D scene {G } from a certain viewpoint. We then
0 p
leverage an image-to-video model to generate a short video clip {I ,I ,...,I }
0 1 T
depicting the object’s realistic motion. This generated video serves as our refer-
encevideo.WethenoptimizethematerialfieldE(x)andaninitialvelocityfield
v (x) (both modeled by implicit neural fields [71]) through differentiable simu-
0
lation and differentiable rendering, such that a rendered video of the simulation
matches (from the same viewpoint as I ) the reference video. Fig. 3 shows an
0
overview of PhysDreamer.
4.1 Preliminaries
3D Gaussians [36] adopts a set of anisotropic 3D Gaussian kernels to represent
the radiance field of a 3D scene. Although introduced primarily as an efficient6 Zhang et al.
Render
Video genera�on model
3D Gaussians Loss Reference video
Render
P2G
Material field Differen�able MPM Rendered video
Fig.3: Overview of PhysDreamer. Given an object represented as 3D Gaussians, we
first render it (with background) from a viewpoint. Next, we use an image-to-video
generationmodeltoproduceareferencevideoofthatobjectinmotion.Usingdifferen-
tiable Material Point Methods (MPM) and differentiable rendering, we optimize both
a spatially-varying material field and an initial velocity field (not shown in the fig-
ureabove).Thisoptimizationaimstominimizethediscrepancybetweentherendered
video and the reference video. The dashed arrows represent gradient flow.
method for 3D novel view synthesis, the Lagrangian nature of 3D Gaussians
alsoenablesthedirectadaptationofparticle-basedphysicssimulators.Following
PhysGaussian[70],weusetheMaterialPointMethod(MPM)tosimulateobject
dynamicsdirectlyontheseGaussianparticles.Since3DGaussiansmainlylieon
object surfaces, an optional internal filling process can be applied for improved
simulation realism [70]. Below, we provide a brief introduction on the underly-
ing physical model and how to integrate MPM into 3D Gaussians. For a more
comprehensiveintroductionofMPM,wereferinterestedreadersto[28,33,34,70].
Continuum mechanics and elastic materials. Continuum mechanics models ma-
terial deformation using a map ϕ that transforms points from the undeformed
material space X to the deformed world space x = ϕ(X,t). The Jacobian of
the map, F = ∇ ϕ(X,t), known as the deformation gradient, measures local
X
rotation and strain. This tensor is crucial in formulating stress-strain relation-
ship. For example, the Cauchy stress in a hyper-elastic material is computed
by: σ = 1 ∂ψFT. Here, ψ(F) represents the strain energy density function,
det(F)∂F
quantifying the extent of non-rigid deformations. This function is typically de-
signed by experts, to follow principles like material symmetry and rotational
invariance while aligning with empirical data. In this work, we use fixed coro-
tated hyperelastic model, whose energy density function can be expressed as:
(cid:32) d (cid:33)
(cid:88) λ
ψ(F)=µ (σ −1)2 + (det(F)−1)2, (1)
i 2
i=1PhysDreamer 7
whereσ denotesasingularvalueofthedeformationgradient.µandλarerelated
i
to Young’s modulus E and Poisson’s ratio ν via:
E Eν
µ= , λ= . (2)
2(1+ν) (1+ν)(1−2ν)
The dynamics of an elastic object are governed by the following equations:
Dv Dρ
ρ =∇·σ+f, +ρ∇·v =0, (3)
Dt Dt
where ρ denotes density, v(x,t) denotes the velocity field in world space, and f
denotes an external force.
MaterialPointMethod(MPM).WeusetheMovingLeastSquaresMaterialPoint
Method (MLS-MPM) [28] to solve the above governing equation. MPM is a hy-
brid Eulerian-Langrangian method widely adopted for simulating dynamics for
a wide range of materials, such as solid, fluid, sand, and cloth [32,37,58,62].
MPM offers several advantages, such as easy GPU parallelization [29], handling
of topology changes, and the availability of well-documented open-source imple-
mentations [30,48,49,70].
Following PhysGaussian [70], we view the Gaussian particles as the spatial
discretization of the object to be simulated, and directly run MPM on these
Gaussian particles. Each particle p represents a small volume of the object, and
itcarriesasetofpropertiesincludingvolumeV ,massm ,positionxt,velocity
p p p
vt, deformation gradient Ft, and local velocity field gradient Ct at time step t.
p p p
MPMoperatesinaparticle-to-grid(P2G)andgrid-to-particle(G2P)transfer
loop. In the P2G stage, we transfer the momentum from particle to grid by:
mtvt =(cid:88) N(x −xt)[m vt+(m Ct− 4 ∆tV ∂ψ FtT )(x −xt)]+ft, (4)
i i i p p p p p (∆x)2 p∂F p i p i
p
where the mass of the grid node i is mt = (cid:80) N(x −xt)m , N(x −xt) is
i p i p p i p
the B-spline kernel, ∆x is the spatial grid resolution, ∆t is the simulation step
size, and vt is the updated velocity on the grid. We then transfer the updated
i
velocity back to the particles and update their positions as:
(cid:88)
vt+1 = N(x −xt)vt, xt+1 =xt +∆tvt+1. (5)
p i p i p p p
i
Meanwhile, the local velocity gradient and deformation gradient is updated as:
4 (cid:88)
Ct+1 = N(x −xt)vt(x −xt)T, Ft+1 =(I+∆tCt+1)Ft. (6)
p (∆x)2 i p i i p p p p
i
4.2 Estimating physical properties
Using MLS-MPM [28,70] as our physics simulator and the Fixed Corotated
hyper-elastic material model for the 3D objects. the simulation process for a
single sub-step is formalized as:
xt+1,vt+1,Ft+1,Ct+1 =S(xt,vt,Ft,Ct,θ,∆t), (7)8 Zhang et al.
where xt = [xt,··· ,xt ] denotes the positions of all particles at time t, and
1 P
similarlyvt =[vt,··· ,vt ]denotesthevelocitiesofallparticlesattimet.Ftand
1 P
Ctdenotethedeformationgradientandthegradientoflocalvelocityfieldsforall
particles, respectively. Both Ft and Ct are tracked for simulation purposes, not
for rendering. θ denotes the collection of the physical properties of all particles:
mass m = [m ,··· ,m ], Young’s modulus E = [E ,··· ,E ], Poisson’s ratio
1 P 1 P
ν =[ν ,··· ,ν ], and volume V =[V ,··· ,V ]. ∆t is the simulation step size.
1 P 1 P
Weuseasub-stepsize∆t≊1×10−4formostofourexperiments.Tosimulate
dynamics between adjacent video frames, we iterate over hundreds of sub-steps
(time interval between frames are typically tens of milliseconds). For simplicity,
we abuse notation to express a simulation step with N sub-steps as:
xt+1,vt+1,Ft+1,Ct+1 =S(xt,vt,Ft,Ct,θ,∆t,N), (8)
where the timestamp t+1 is ahead of timestamp t by N∆t. After simulation,
we render the Gaussians at each frame:
Iˆt =F (xt,α,Rt,Σ,c), (9)
render
where F denotes the differentiable rendering function, and Rt denotes the
render
rotation matrices of all particles obtained from the simulation step.
Using the generated video as reference, we optimize the spatially-varying
Young’s modulus E and an initial velocity v0 by a per-frame loss function:
Lt =λL (Iˆt,It)+(1−λ)L (Iˆt,It), (10)
1 D−SSIM
where we set λ=0.1 in our experiments.
We parameterize the material field and velocity field by two triplanes [9],
eachfollowedbyathree-layerMLP.Additionally,weapplyatotalvariationreg-
ularization for all spatial planes of both fields to encourage spatial smoothness.
Using u to denote one of the 2D spatial planes, and u as a feature vector on
i,j
the 2D plane, we write the total variation regularization term as:
(cid:88)
L = ∥u −u ∥2+∥u −u ∥2. (11)
tv i+1,j i,j 2 i,j+1 i,j 2
i,j
Rather than optimizing the material parameters and initial velocity jointly,
we split the optimization into two stages for better stability and faster con-
vergence. In particular, in the first stage, we randomly initialize the Young’s
modulus for each Gaussian particle and freeze it. We optimize the initial veloc-
ity of each particle using only the first three frames of the reference video. In
thesecondstage,wefreezetheinitialvelocityandoptimizethespatiallyvarying
Young’s modulus. During the second stage, the gradient signal only flows to the
previous frame to prevent gradient explosion/vanishing.
4.3 Accelerating simulation with subsampling
High-fidelityrenderingwith3DGaussianstypicallyrequiresmillionsofparticles
to represent a scene. Running simulations on all the particles poses a signifi-
cant computational burden. To improve efficiency, we introduce a subsampling
procedure for simulation, as illustrated in Fig. 4.PhysDreamer 9
Fig.4: Accelerated MPM with K-
Means downsampling. We employ
K-Meansclusteringtocreateasetof
“driving particles” (in yellow) at the
initialtimestep(t=0).Weonlysim-
K-Means
ulate these driving particles. When downsample Interpola�on
rendering, we obtain each particle’s
positionandrotationbyfittingalo-
Simulate
cal rigid body transformation using
neighboring driving particles.
t = 0 t = 1
Specifically, we apply K-Means clustering to create a set of driving parti-
cles {Q }Q at t = 0, where each driving particle is represented by Q0 =
q q=1 q
{x0,v0,F0,C0,E ,m ,ν ,V }. The initial position of a driving particle x0 is
q q q q q q q q q
computed as the mean of the position x of all cluster members. The number of
p
the driving particles is much smaller than the number of 3D Gaussian particles,
Q≪P. We run simulations only on the driving particles. During rendering, we
compute the position and rotation for each 3D Gaussian particle G by interpo-
p
latingthedrivingparticles.Inparticular,foreach3DGaussianparticle,wefind
itseightnearestdrivingparticlesatt=0,andwefitarigidbodytransformation
T between these eight driving particles at t = 0 and at the current timestamp.
This rigid body transformation T is applied to the initial position and rotation
of the particle G to obtain its current position and rotation. We summarize our
p
algorithm with pseudo-code in Appendix C.
5 Experiments
5.1 Setup
Datasets. We collect eight real-world static scenes by capturing multi-view im-
ages. Each scene includes an object and a background. The objects include five
flowers (a red rose, a carnation, an orange rose, a tulip, and a white rose), an
alocasia plant, a telephone cord, and a beanie hat. For each scene except for the
red rose scene, we capture four interaction videos illustrating its natural mo-
tion after interaction, such as poking or dragging, and we use the real videos as
additional comparison references.
Baselines. We compare our approach to two baselines: PhysGaussian [70] and
DreamGaussian4D [59]. PhysGaussian [70] integrates MPM simulation to static
3D Gaussians to support simulation, but it cannot estimate material properties
andreliesonmanuallysettingmaterialparametervalues.Thus,weusethesame
initialization strategy as ours to assign material properties for PhysGaussian.
DreamGaussian4D [59] generates non-interactive dynamic 3D Gaussians from a
static image. It first obtains a static 3D Gaussians using DreamGaussian [63],
and then animate it by optimizing a deformation field from a generated driving10 Zhang et al.
video. For a fair comparison, we run its deformation field optimization on our
reconstructedstatic3DGaussians,andweloopedtheresultingdeformationfield
when rendering longer videos in later comparison.
Evaluation metrics.Wefocusonthequalityofthesynthesizedobjectmotion,in
particular,visualquality andmotionrealism.Therefore,weconductauserstudy
andadopttheTwo-alternativeForcedChoice(2AFC)protocol:theparticipants
are shown two side-by-side synchronized videos, including one video result from
oursandtheotheronefromthecompetitor’s,witharandomleft-rightordering.
Theparticipantsarethenaskedtochoosetheonewithhighervisualqualityand
the one with higher motion realism.
Werecruited100participants,eachaskedtojudgeall8scenes,formingatotal
of 800 2AFC judgement samples for each baseline comparison. For each scene,
we create 4 sample video pairs and show participants a random one from the 4
pairs.Inparticular,wecreate4five-secondmotionsequencesusingPhysDreamer
withrandomizedinitialconditions(applyinganexternalforcetotheforeground
object or assigning an initial velocity to the object), and render videos from
randomly picked viewpoints. For the baseline method, we apply the same initial
conditions (for PhysGaussian only) and render videos from the same viewpoint
as ours to form the video pairs. Please see Appendix B for human study details,
andAppendixAforquantitativemetricsforvideos(e.g.,FréchetVideoDistance
[64]).
5.2 Implementation details
Neural material fields.Weparameterizebothourmaterialfieldandinitialveloc-
ity field using triplanes [9,55] each followed by a three-layer MLP. The triplanes
has spatial resolution of 83 and 243 for the material field and velocity field,
respectively.
3DGaussianreconstruction.SimilartoPhysGaussian[70],weemployanisotropic
regularizationtoreduceskinnyartifactsinthereconstruction.Eachreconstructed
scenecontains0.5to1.5millionparticles(includingforegroundandbackground).
Simulation details. For computational efficiency, we segment the background
and keep only foreground object particles for simulation. In our experiments,
the foreground object contains around 50 to 300 thousand 3D Gaussian parti-
cles. We then discretize the foreground into a 643 grid. The number of driving
particlesare10to50timesfewerthanthenumberof3DGaussianparticles,de-
terminedbymaintaininganaverageofatleasteightparticlesperoccupiedvoxel.
Foraccuratemotion,weuse768sub-stepsbetweensuccessivevideoframes,cor-
responding to a duration of 4.34×10−5 second for each sub-step. To address
the high memory consumption from large number of steps, we apply simulation
state checkpointing and re-computation during gradient back-propagation. We
add Dirichlet boundary conditions for stationary grid cells. We fill the internal
volumes of certain solid objects to enhance simulation realism [70].PhysDreamer 11
Material result Synthesized 3D dynamics under interac�on
Fig.5: Interactive 3D dynamics synthesis. (Left) Visualization of the material fields.
Brighter color indicates higher Young’s modulus within each example. (Right) We
applyanexternalforce(redarrow)oneachobject,andthefollowingcolumnsdemon-
strate the object dynamics rendered at a static viewpoint.
Generating reference videos. We render a 3D object with its background from
a viewpoint, and then we use Stable Video Diffusion [4] to animate this ren-
dered image and generate fourteen video frames. We use a small motion bucket
number [4] (e.g., 5 or 8) so that the generated video contains mostly object mo-
tion and little camera motion. We specifically use rendered images rather than
Gaussians’ training images for the video generation, because we might not have
access to training images (e.g., the 3D Gaussians are generated from text-to-3D
models). Also, rendering images directly from 3D Gaussians simplifies later op-
timization.Whilenotnecessary,inourexperiments,wechooseaviewpointfrom
the original 3D Gaussian’s training set.
5.3 Results
We show our qualitative results of the spatially-varying Young’s modulus in
Fig. 5 (left), and simulated interactive motion in Fig. 5 (right). Please see our
project website videos for a better motion visualization. Tab. 1 presents the user
study results in comparison to baseline methods and real captured videos.12 Zhang et al.
Table1:Humanstudy2AFCresultsofPhysDreamer(Ours)overrealcapturedvideos
andbaselinemethods(PhysGaussian[70]andDreamGaussian4D[59])onMotionReal-
ismandoverallVisual Quality.“RoseO”,“RoseW”,and“RoseR” denotestheorange,
white, and red roses, respectively.
Motion realism AlocasiaCarnation Hat RoseORoseWRoseRCordTulip Avg.
OursoverRealcapture 86% 61% 55% 63% 47% - 29% 35% 53.7%
OursoverPhysGaussian 96% 89% 57% 91% 93% 73% 61% 86% 80.8%
OursoverDreamGaussian 75% 77% 51% 78% 51% 41% 71% 64% 63.5%
Visual quality
OursoverRealcapture 36% 53% 28% 40% 41% - 29% 34% 37.3%
OursoverPhysGaussian 67% 69% 50% 75% 73% 58% 58% 70% 65.0%
OursoverDreamGaussian 82% 75% 74% 76% 60% 47% 76% 70% 70.0%
Compared to PhysGaussian, 80.8% of the human participant 2AFC samples
prefer PhysDreamer (ours) in motion realism and 65.0% prefer PhysDreamer in
visual quality. Note that since the static scenes are the same, the visual quality
alsodependsonthegeneratedobjectmotion.Fig.6showstemporalslicesofthe
motionpatterns.WeobservethatPhysGaussianproduceslarge,unrealisticslow
motion due to the lack of a principled estimation of material properties.
Compared to DreamGaussian4D, 70.0%/63.5% 2AFC samples prefer ours in
visualqualityandmotionrealism,respectively.FromFig.6,wecanobservethat
DreamGaussian4Dgeneratesperiodicmotionwithaconstant,smallmagnitude,
whilePhysDreamercansimulatethedampinginmotion.ThisisbecauseDream-
Gaussian4Ddoesnotsimulatethephysicaldynamicsbutsimplydistillamotion
sequence from a generative model, so it cannot extrapolate to different motion.
Wefurtherincludeonemoreevaluationdimensionon“motionamount” compar-
ing to DreamGaussian4D, where we ask the participants to judge which video
has higher amount of motion, and 73.6% 2AFC samples prefer PhysDreamer.
Surprisingly, even when compared to real videos, 53.7% 2AFC samples fa-
voredthemotionrealismofmotionsynthesizedbyPhysDreamer.However,view-
ersbiasedwithpriorknowledge,liketheauthors,caneasilydistinguishsimulated
videosfromrealvideos,sincePhysDreamerproducesmotionwithlowerfrequen-
cies in most experiments. This low-frequency motion pattern is evident in the
space-time slice visualizations at Fig. 6. For a more detailed visual comparison,
please see the supplementary materials.
5.4 Ablation: using multi-view reference videos
For objects with self-occlusion, observing salient motion of all object parts from
a single video is challenging (e.g., the alocasia scene where a leaf can occlude
another leaf). We may alleviate this problem by rendering from multiple view-
points to provide comprehensive coverage of the object. Here, we use multiplePhysDreamer 13
Object Real video Ours PhysGaussian DreamGaussian4D
Fig.6: We compare our results with real captured videos, PhysGaussian [70], and
DreamGaussian4D [59] using space-time slices. In these slices, the vertical axis repre-
sent time, and the horizontal axis shows a spatial slice of the object (denoted by red
lines on the “object” column). These slices visualize the magnitude and frequencies of
these oscillating motions. Results for our PhysDreamer (Ours) and PhysGaussian are
simulated with the same initial conditions.
piluT
droc
enohpeleT
aisacolA
esor
etihW
taH
no�anraC
esor
engarO14 Zhang et al.
Ini�al frame Synthesized 3D dynamics under interac�on Space-�me slices
Fig.7: Comparison between single-view (top) and two-view (bottom) supervisions.
The object (alocasia) exhibits self-occluding structures. We can use generated videos
at two views to jointly optimize the material field. Single-view reference provides less
effectivesupervisionsignalsformaterialsofoccludedparts,asshownintheblueboxes.
Inthespace-time(X-t)slices,theverticalaxisrepresentstime,andthehorizontalaxis
shows a spatial slice of the object.
videos in the material estimation, jointly optimizing a video-agnostic, spatially-
varyingYoung’smodulusforeachparticlealongwithvideo-specificinitialveloc-
ities. From the comparison of the alocasia scene in Fig. 7, we can see that using
multi-viewreferencevideos(afrontviewandabackview)helpsinsuchcomplex
self-occluding objects: PhysDreamer benefits significantly from having supervi-
sionfromtwoviews,whileusingonlyasingleviewleadstoartifacts.Inouruser
study, 81.0% 2AFC samples prefer PhysDreamer with two view supervision in
visual quality and 86.0% in motion realism.
6 Conclusion
In this work, we introduced PhysDreamer, a novel approach for synthesizing
interactive 3D dynamics by endowing static 3D objects with physical mate-
rial properties. Our method leverages the object dynamics priors learned by
video generation models to estimate the spatially-varying material properties.
The estimated material properties enable realistic physical interaction with the
3D objects. We demonstrated the effectiveness of PhysDreamer through exper-
iments on a diverse set of elastic objects and a user study, showing that our
approach significantly outperforms state-of-the-art methods in terms of motion
realism. We believe that PhysDreamer takes a significant step towards creating
more engaging and immersive virtual environments by enabling static 3D ob-
jects to respond realistically to novel interactions, opening up a wide range of
applications from realistic simulations to interactive virtual experiences.
Limitations. Our approach requires the user to manually specify the object to
simulate and separate it from the background, and establish boundary condi-
tions for stationary parts, like the pot of flowers. 3D object discovery may help
for simulatable object extraction. In addition, our approach is computationally
demanding. Despite our subsampling strategy, our current algorithm takes ap-
proximately one minute on a NVIDIA V100 GPU to produce a single second
weiv-elgniS
weiv-owTPhysDreamer 15
of video. Further improving efficiency remains an important future problem.
Finally, in this work, we restrict our scope to elastic objects without collisions.
Acknowledgements. ThisworkisinpartsupportedbytheNSFPHY-2019786
(TheNSFAIInstituteforArtificialIntelligenceandFundamentalInteractions),
RI#2211258,#2338203,ONRMURIN00014-22-1-2740,QuantaComputer,and
Samsung.WewouldliketothankPeterYichenChen,ZhengqiLi,PingchuanMa,
Minghao Guo, Ge Yang, and Shai Avidan for help and insightful discussions.
References
1. Attal,B.,Huang,J.B.,Richardt,C.,Zollhoefer,M.,Kopf,J.,O’Toole,M.,Kim,C.:
Hyperreel:High-fidelity6-dofvideowithray-conditionedsampling.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
16610–16620 (2023) 3
2. Bahmani, S., Skorokhodov, I., Rong, V., Wetzstein, G., Guibas, L., Wonka, P.,
Tulyakov, S., Park, J.J., Tagliasacchi, A., Lindell, D.B.: 4d-fy: Text-to-4d gen-
eration using hybrid score distillation sampling. arXiv preprint arXiv:2311.17984
(2023) 3
3. Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A.,
Hur,J.,Li,Y.,Michaeli,T.,etal.:Lumiere:Aspace-timediffusionmodelforvideo
generation. arXiv preprint arXiv:2401.12945 (2024) 4
4. Blattmann,A.,Dockhorn,T.,Kulal,S.,Mendelevitch,D.,Kilian,M.,Lorenz,D.,
Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling
latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127
(2023) 4, 11
5. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 22563–22575 (2023) 4
6. Brooks, T., Peebles, B., Homes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D.,
Taylor,J.,Luhman,T.,Luhman,E.,Ng,C.,Wang,R.,Ramesh,A.:Videogener-
ation models as world simulators (2024), https://openai.com/research/video-
generation-models-as-world-simulators 4
7. Cao,A.,Johnson,J.:Hexplane:Afastrepresentationfordynamicscenes.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion. pp. 130–141 (2023) 3
8. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kineticsdataset.In:proceedingsoftheIEEEConferenceonComputerVisionand
Pattern Recognition. pp. 6299–6308 (2017) 20
9. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo,
O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d
generativeadversarialnetworks.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 16123–16133 (2022) 8, 10
10. Chen, H.y., Tretschk, E., Stuyck, T., Kadlecek, P., Kavan, L., Vouga, E., Lass-
ner, C.: Virtual elastic objects. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 15827–15837 (2022) 416 Zhang et al.
11. Chen,X.,Liu,Z.,Chen,M.,Feng,Y.,Liu,Y.,Shen,Y.,Zhao,H.:Livephoto:Real
imageanimationwithtext-guidedmotioncontrol.arXivpreprintarXiv:2312.02928
(2023) 4
12. Chuang,Y.Y.,Goldman,D.B.,Zheng,K.C.,Curless,B.,Salesin,D.H.,Szeliski,R.:
Animating pictures with stochastic motion textures. In: ACM SIGGRAPH 2005
Papers. pp. 853–860 (2005) 4
13. Curless, B., Levoy, M.: A volumetric method for building complex models from
rangeimages.In:Proceedingsofthe23rdannualconferenceonComputergraphics
and interactive techniques. pp. 303–312 (1996) 3
14. Dai,Z.,Zhang,Z.,Yao,Y.,Qiu,B.,Zhu,S.,Qin,L.,Wang,W.:Animateanything:
Fine-grained open domain image animation with motion guidance. arXiv e-prints
pp. arXiv–2311 (2023) 4
15. Davis, A., Chen, J.G., Durand, F.: Image-space modal bases for plausible manip-
ulation of objects in video. ACM Transactions on Graphics (TOG) 34(6), 1–7
(2015) 4
16. Davis, M.A.: Visual vibration analysis. Ph.D. thesis, Massachusetts Institute of
Technology (2016) 4
17. Duan, Y., Wei, F., Dai, Q., He, Y., Chen, W., Chen, B.: 4d gaussian splat-
ting: Towards efficient novel view synthesis for dynamic scenes. arXiv preprint
arXiv:2402.03307 (2024) 3
18. Feng,Y.,Shang,Y.,Li,X.,Shao,T.,Jiang,C.,Yang,Y.:Pie-nerf:Physics-based
interactive elastodynamics with nerf. arXiv preprint arXiv:2311.13099 (2023) 4
19. Fridovich-Keil,S.,Meanti,G.,Warburg,F.R.,Recht,B.,Kanazawa,A.:K-planes:
Explicit radiance fields in space, time, and appearance. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12479–
12488 (2023) 3
20. Gao, H., Li, R., Tulsiani, S., Russell, B., Kanazawa, A.: Monocular dynamic view
synthesis:Arealitycheck.AdvancesinNeuralInformationProcessingSystems35,
33768–33780 (2022) 3
21. Geng, D., Owens, A.: Motion guidance: Diffusion-based image editing with differ-
entiablemotionestimators.In:TheTwelfthInternationalConferenceonLearning
Representations (2023) 4
22. Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S.S., Shah,
A.,Yin,X.,Parikh,D.,Misra,I.:Emuvideo:Factorizingtext-to-videogeneration
by explicit image conditioning. arXiv preprint arXiv:2311.10709 (2023) 4
23. Guo, X., Sun, J., Dai, Y., Chen, G., Ye, X., Tan, X., Ding, E., Zhang, Y., Wang,
J.:Forwardflowfornovelviewsynthesisofdynamicscenes.In:Proceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision.pp.16022–16033(2023)
3
24. Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L.,
Lezama, J.: Photorealistic video generation with diffusion models. arXiv preprint
arXiv:2312.06662 (2023) 4
25. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017) 20
26. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P.,
Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video
generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022) 4
27. Hong,W.,Ding,M.,Zheng,W.,Liu,X.,Tang,J.:Cogvideo:Large-scalepretrain-
ingfortext-to-videogenerationviatransformers.arXivpreprintarXiv:2205.15868
(2022) 4PhysDreamer 17
28. Hu,Y.,Fang,Y.,Ge,Z.,Qu,Z.,Zhu,Y.,Pradhana,A.,Jiang,C.:Amovingleast
squaresmaterialpointmethodwithdisplacementdiscontinuityandtwo-wayrigid
body coupling. ACM Transactions on Graphics (TOG) 37(4), 1–14 (2018) 2, 6, 7
29. Hu, Y., Li, T.M., Anderson, L., Ragan-Kelley, J., Durand, F.: Taichi: a language
forhigh-performancecomputationonspatiallysparsedatastructures.ACMTrans-
actions on Graphics (TOG) 38(6), 1–16 (2019) 7
30. Hu, Y., Li, T.M., Anderson, L., Ragan-Kelley, J., Durand, F.: Taichi: a language
forhigh-performancecomputationonspatiallysparsedatastructures.ACMTrans-
actions on Graphics (TOG) 38(6), 1–16 (2019) 7
31. Huang, Y.H., Sun, Y.T., Yang, Z., Lyu, X., Cao, Y.P., Qi, X.: Sc-gs:
Sparse-controlled gaussian splatting for editable dynamic scenes. arXiv preprint
arXiv:2312.14937 (2023) 3
32. Jiang, C., Gast, T., Teran, J.: Anisotropic elastoplasticity for cloth, knit and hair
frictional contact. ACM Transactions on Graphics (TOG) 36(4), 1–14 (2017) 7
33. Jiang, C., Schroeder, C., Selle, A., Teran, J., Stomakhin, A.: The affine particle-
in-cell method. ACM Transactions on Graphics (TOG) 34(4), 1–10 (2015) 6
34. Jiang, C., Schroeder, C., Teran, J., Stomakhin, A., Selle, A.: The material point
method for simulating continuum materials. In: ACM SIGGRAPH 2016 courses.
pp. 1–52 (2016) 6
35. Kay,W.,Carreira,J.,Simonyan,K.,Zhang,B.,Hillier,C.,Vijayanarasimhan,S.,
Viola,F.,Green,T.,Back,T.,Natsev,P.,etal.:Thekineticshumanactionvideo
dataset. arXiv preprint arXiv:1705.06950 (2017) 20
36. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-timeradiancefieldrendering.ACMTransactionsonGraphics42(4)(2023) 1,
2, 3, 5
37. Klár, G., Gast, T., Pradhana, A., Fu, C., Schroeder, C., Jiang, C., Teran, J.:
Drucker-pragerelastoplasticityforsandanimation.ACMTransactionsonGraphics
(TOG) 35(4), 1–12 (2016) 7
38. Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H.,
Akbari, H., Alon, Y., Birodkar, V., et al.: Videopoet: A large language model for
zero-shot video generation. arXiv preprint arXiv:2312.14125 (2023) 4
39. Kopf,J.,Rong,X.,Huang,J.B.:Robustconsistentvideodepthestimation.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion. pp. 1611–1621 (2021) 3
40. Kratimenos, A., Lei, J., Daniilidis, K.: Dynmf: Neural motion factorization for
real-time dynamic view synthesis with 3d gaussian splatting. arXiv preprint
arXiv:2312.00112 (2023) 3
41. Li,H.,Sumner,R.W.,Pauly,M.:Globalcorrespondenceoptimizationfornon-rigid
registration of depth scans. In: Computer graphics forum. vol. 27, pp. 1421–1430.
Wiley Online Library (2008) 3
42. Li, X., Qiao, Y.L., Chen, P.Y., Jatavallabhula, K.M., Lin, M., Jiang, C., Gan,
C.: Pac-nerf: Physics augmented continuum neural radiance fields for geometry-
agnostic system identification. arXiv preprint arXiv:2303.05512 (2023) 4
43. Li, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time
view synthesis of dynamic scenes. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 6498–6508 (2021) 3
44. Li, Z., Tucker, R., Snavely, N., Holynski, A.: Generative image dynamics. arXiv
preprint arXiv:2309.07906 (2023) 4
45. Li,Z.,Wang,Q.,Cole,F.,Tucker,R.,Snavely,N.:Dynibar:Neuraldynamicimage-
basedrendering.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 4273–4284 (2023) 318 Zhang et al.
46. Ling,H.,Kim,S.W.,Torralba,A.,Fidler,S.,Kreis,K.:Alignyourgaussians:Text-
to-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint
arXiv:2312.13763 (2023) 1, 3
47. Luiten,J.,Kopanas,G.,Leibe,B.,Ramanan,D.:Dynamic3dgaussians:Tracking
bypersistentdynamicviewsynthesis.arXivpreprintarXiv:2308.09713(2023) 1,3
48. Ma, P., Chen, P.Y., Deng, B., Tenenbaum, J.B., Du, T., Gan, C., Matusik, W.:
Learning neural constitutive laws from motion observations for generalizable pde
dynamics. In: International Conference on Machine Learning. PMLR (2023) 7
49. Macklin, M.: Warp: A high-performance python framework for gpu simulation
and graphics. https://github.com/nvidia/warp (March 2022), nVIDIA GPU
Technology Conference (GTC) 7
50. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.In:ECCV
(2020) 1
51. Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and track-
ing of non-rigid scenes in real-time. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 343–352 (2015) 3
52. Park,K.,Sinha,U.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Seitz,S.M.,Martin-
Brualla, R.: Nerfies: Deformable neural radiance fields. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 5865–5874 (2021)
3
53. Park,K.,Sinha,U.,Hedman,P.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Martin-
Brualla,R.,Seitz,S.M.:Hypernerf:Ahigher-dimensionalrepresentationfortopo-
logically varying neural radiance fields. arXiv preprint arXiv:2106.13228 (2021)
3
54. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in
ganevaluation.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 11410–11420 (2022) 20
55. Peng, S., Niemeyer, M., Mescheder, L., Pollefeys, M., Geiger, A.: Convolutional
occupancynetworks.In:ComputerVision–ECCV2020:16thEuropeanConference,
Glasgow,UK,August23–28,2020,Proceedings,PartIII16.pp.523–540.Springer
(2020) 10
56. Poole,B.,Jain,A.,Barron,J.T.,Mildenhall,B.:Dreamfusion:Text-to-3dusing2d
diffusion. In: The Eleventh International Conference on Learning Representations
(2022) 3
57. Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural ra-
diancefieldsfordynamicscenes.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 10318–10327 (2021) 3
58. Ram,D.,Gast,T.,Jiang,C.,Schroeder,C.,Stomakhin,A.,Teran,J.,Kavehpour,
P.: A material point method for viscoelastic fluids, foams and sponges. In: Pro-
ceedings of the 14th ACM SIGGRAPH/Eurographics Symposium on Computer
Animation. pp. 157–163 (2015) 7
59. Ren,J.,Pan,L.,Tang,J.,Zhang,C.,Cao,A.,Zeng,G.,Liu,Z.:Dreamgaussian4d:
Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 (2023) 1, 3, 9,
12, 13, 20
60. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H.,
Ashual,O.,Gafni,O.,etal.:Make-a-video:Text-to-videogenerationwithouttext-
video data. arXiv preprint arXiv:2209.14792 (2022) 4
61. Singer, U., Sheynin, S., Polyak, A., Ashual, O., Makarov, I., Kokkinos, F., Goyal,
N., Vedaldi, A., Parikh, D., Johnson, J., et al.: Text-to-4d dynamic scene genera-
tion. arXiv preprint arXiv:2301.11280 (2023) 3PhysDreamer 19
62. Stomakhin, A., Schroeder, C., Chai, L., Teran, J., Selle, A.: A material point
method for snow simulation. ACM Transactions on Graphics (TOG) 32(4), 1–10
(2013) 7
63. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)
9
64. Unterthiner,T.,VanSteenkiste,S.,Kurach,K.,Marinier,R.,Michalski,M.,Gelly,
S.:Towardsaccurategenerativemodelsofvideo:Anewmetric&challenges.arXiv
preprint arXiv:1812.01717 (2018) 10, 20
65. Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar,
M.T.,Castro,S.,Kunze,J.,Erhan,D.:Phenaki:Variablelengthvideogeneration
from open domain textual descriptions. In: International Conference on Learning
Representations (2022) 4
66. Wang,C.,MacDonald,L.E.,Jeni,L.A.,Lucey,S.:Flowsupervisionfordeformable
nerf. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition. pp. 21128–21137 (2023) 3
67. Wang, C., Zhuang, P., Siarohin, A., Cao, J., Qian, G., Lee, H.Y., Tulyakov, S.:
Diffusionpriorsfordynamicviewsynthesisfrommonocularvideos.arXivpreprint
arXiv:2401.05583 (2024) 3
68. Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., Duan, N.: Nüwa: Visual
synthesispre-trainingforneuralvisualworldcreation.In:Europeanconferenceon
computer vision. pp. 720–736. Springer (2022) 4
69. Xian, W., Huang, J.B., Kopf, J., Kim, C.: Space-time neural irradiance fields for
free-viewpoint video. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 9421–9431 (2021) 3
70. Xie, T., Zong, Z., Qiu, Y., Li, X., Feng, Y., Yang, Y., Jiang, C.: Physgaus-
sian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint
arXiv:2311.12198 (2023) 5, 6, 7, 9, 10, 12, 13, 20
71. Xie,Y.,Takikawa,T.,Saito,S.,Litany,O.,Yan,S.,Khan,N.,Tombari,F.,Tomp-
kin, J., Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond.
In: Computer Graphics Forum. vol. 41, pp. 641–676. Wiley Online Library (2022)
2, 5
72. Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaus-
sians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint
arXiv:2309.13101 (2023) 3
73. Yin, Y., Xu, D., Wang, Z., Zhao, Y., Wei, Y.: 4dgen: Grounded 4d content gen-
erationwithspatial-temporalconsistency.arXivpreprintarXiv:2312.17225(2023)
3
74. Yu,H.,Julin,J.,Milacski,Z.Á.,Niinuma,K.,Jeni,L.A.:Cogs:Controllablegaus-
sian splatting. arXiv preprint arXiv:2312.05664 (2023) 3
75. Yu,H.,Julin,J.,Milacski,Z.A.,Niinuma,K.,Jeni,L.A.:Dylin:Makinglightfield
networks dynamic. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 12397–12406 (2023) 3
76. Zhang, S., Wang, J., Zhang, Y., Zhao, K., Yuan, H., Qin, Z., Wang, X., Zhao, D.,
Zhou, J.: I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion
models. arXiv preprint arXiv:2311.04145 (2023) 4
77. Zhang, Z., Cole, F., Tucker, R., Freeman, W.T., Dekel, T.: Consistent depth of
movingobjectsinvideo.ACMTransactionsonGraphics(TOG)40(4),1–12(2021)
320 Zhang et al.
Appendix
A Metrics
We compare the visual quality of our method with two baseline methods, Phys-
Gaussian [70] and DreamGaussian4D [59], by computing the Frechet Video Dis-
tance (FVD) [64] against real captured videos. We compute the FVD with a
16-framewindow,2-framestride,basedontheI3D[8]modeltrainedontheHu-
man Kinetics Dataset [35]. All videos are resized (short edge to 144 pixels) and
center-croppedto128×128pixelspriortoFVDcomputation.Wecompareeach
methodagainstrealcapturedvideos,creating272clipspersceneforevaluation.
The results are shown in Table 2.
We further compare methods using the Frechet Inception Distance (FID)
[25,54], as shown in Table 3. FID calculation incorporates all frames across all
objects, totaling 4200 frames per method.
Table2:FrechetVideoDistance(FVD)betweenrealcapturedvideoandPhysDreamer
(Ours) and baseline methods (PhysGaussian [70] and DreamGaussian4D [59])
FVD (↓) AlocasiaCarnationHatRoseO.RoseW.CordTulip Avg.
Ours 272 282 54 231 640 185 228 270.3
PhysGaussian 560 629 50 408 961 184 586 482.6
DreamGaussian 308 359 75 200 1379 210 497 432.6
Table 3: Frechet Inception Distance (FID) between real captured video and Phys-
Dreamer(Ours)andbaselinemethods(PhysGaussian[70]andDreamGaussian4D[59])
Method FID (↓)
Ours 47.7
PhysGaussian 63.2
DreamGaussian 52.8
B User Study
WeuseProlific1 torecruitparticipantsforthehumanpreferenceevaluation.We
useGoogleformstopresentthesurvey.Thesurveyisfullyanonymizedforboth
1 https://www.prolific.com/PhysDreamer 21
the participants and the host. We attach an example anonymous survey link in
thefootnote2 forreference.Reviewercanenteranytextsuchas“test” forProlific
ID.
C Algorithm details
We present python-style pseudo-code for accelerating material point methods
with K-Means downsampling in Algorithm 1.
Algorithm 1 Acclerate material point method with downsampling
# x, alpha, R, Sigma, c: the position, opacity, rotation, covariance and
color of each Gaussian particle. x of shape [N, 3]
# num_drive_pts: int, top_k: int default as 8
clusters = KMeans(x, num_drive_pts)
drive_x = clusters.x # [M, 3]
# pre-compute the index of neighboor points
cdist = -1.0 * torch.cdist(x, drive_x) # [N, M]
_, top_k_index = torch.topk(cdist, top_k, -1)
# query initial velocity and material params, and simulate
drive_v = VeloField(drive_x)
drive_material = MaterialField(drive_x)
drive_x_simulated = Simulate(drive_x, drive_v, drive_material)
neighboor_drive_x = drive_x[top_k_index] # [N, top_k, 3]
neighboor_drive_x_simulated = drive_x_simulated[top_k_index]
# R: [N, 3, 3], t: [N, 3]
R_sim, t_sim = fitRigidTransform(drive_x, drive_x_simulated)
# apply transform to interpolate points
x = x + t_sim
R = R_sim @ R
# render
frame = Render(x, alpha, R @ Sigma @ R.T, c)
2 An example user study survey (comparing to PhysGaussian): https://forms.gle/
CZfwxGHX2LaA7KxGA. Google forms require signing in to participate, but it does not
record any participant’s identity.