PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
MAPPING SOCIAL CHOICE THEORY TO RLHF
JessicaDai*andEveFleisig*
DepartmentofComputerScience
UniversityofCalifornia,Berkeley
{jessicadai,efleisig}@berkeley.edu
ABSTRACT
Recentworkonthelimitationsofusingreinforcementlearningfromhumanfeed-
back(RLHF)toincorporatehumanpreferencesintomodelbehavioroftenraises
social choice theory as a referencepoint. Social choice theory’sanalysis of set-
tingssuchasvotingmechanismsprovidestechnicalinfrastructurethatcaninform
howtoaggregatehumanpreferencesamiddisagreement.Weanalyzetheproblem
settings of social choice and RLHF, identify key differencesbetween them, and
discusshowthesedifferencesmayaffecttheRLHFinterpretationofwell-known
technical results in social choice. We then translate canonical desiderata from
social choice theory for the RLHF context and discuss how they may serve as
analyticaltoolsforopenproblemsinRLHF.
1 INTRODUCTION AND RELATED WORK
Reinforcementlearningfromhumanfeedback(RLHF)hasrecentlyemergedasakeytechniquefor
incorporatinghumanvaluesintoAImodels.1 ThecentralproblemsettingofRLHF,inwhichpeople
providepreferencesoveroptionsthatare then used to determineglobalbehavior,shares keysimi-
laritieswithscenariosstudiedundersocialchoicetheory(SCT).2 Recentworkhasdiscussedsome
of the ways in which SCT can serve as a reference for analysis of RLHF, including direct appli-
cation of social choice axiomsto RLHF (Mishra, 2023) and indicatingthat RLHF implicitly opti-
mizesfortheBordacount(Siththaranjanetal.,2023).OpenproblemsinRLHFidentifiedinsurveys
byCasperetal. (2023)andLambertetal.(2023), amongothers,includethedifficultyofselecting
evaluators,accountingfordisagreementamongevaluatorsandcognitivebiases,non-representative
sampled prompts, challenges of developinga single reward function for diverse users, measuring
downstream impacts, and assumptions regarding the ease of quantifying or aggregating complex
individualpreferences. Both Casperetal. (2023) and Lambertetal. (2023) raise social choice as
onepotentialwaytoanalyzesomeoftheproblemstheyidentify(see alsoAppendixA).However,
differencesin theRLHF settingmeanthatestablishedtechnicalresultsin SCT requireadjustment
tobeapplicabletoRLHF.
Intheremainderofthispaper,weoutlinecoredifferencesandsimilaritiesbetweentheRLHFand
SCT problem settings (§2). We then propose SCT-style axioms for RLHF given the differences
in problem settings and discuss open problems raised by these axioms (§3). Finally, we discuss
implications of our analysis for conceptualization of human preference models in the context of
RLHF(§4). In concurrentwork, Conitzeretal. (2024)also advocateforsocialchoiceapproaches
to handling disagreement in RLHF. We propose different formalizations of the problem setting;
moreover, their work proposesan axiomatic approachto evaluation, with an emphasis on axioms
relatedtovoterbehavior.Bycontrast,weviewtheaxiomaticapproachasonlyoneofseveralrelevant
approaches,notablywithdistortion(§3.3)asakeyperspectivefromthesocialchoiceliterature.
1RLHFisthedominantparadigmforincorporatinghumanpreferencesintolanguagemodels,butthispaper
discusses learning human preferences from pairwiseor k-wise comparisons of outputs more generally. Our
analysesarenotrestrictedonlytousingRLasthemethodofincorporatingthesepreferencesintomodelbehav-
ior.
2SeeBrandtetal.(2016)foratextbooktreatmentofSCTforacomputationalaudience.
1
4202
rpA
91
]IA.sc[
1v83031.4042:viXraPublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
SocialChoice RLHF/PreferenceModeling
Spaceofalternatives Fixedandfinite(e.g.,candidatesin Evaluatorsseeonlysamplesfromastructured
anelection) butinfinitespace(e.g.allpossiblepromptsand
completions)
Inputstoprocess Fixed set of voters; often assume Setofevaluators(notalwaysrepresentativeof
full information over alternatives generalpopulation)givepairwisecomparisons
(e.g. each voter submits a ballot over subset of alternatives; noguarantee over
rankingallcandidates) whichevaluatorsgivefeedbackonwhichalter-
natives
Goalofprocess Asinglewinner(e.g.anelectedof- Arewardfunctionthatmeasuresthequalityof
ficial)orasetoftop-kwinners anyalternative,evenifunseen(intheelection
example,awaytoquantifynotonlythegood-
ness of all the current candidates, but also of
anynewcandidate)
Evaluation Preferences of fixed set of voters Generalization, as measured by accuracy of
over fixed alternatives evaluated the reward model or utility/regret of a policy
based on axioms (analyze proper- usingtherewardmodel,withrespecttoallpos-
ties of voting rule) or distortion sibleprompts/completions
(measureutilityofoutcome)
Table 1: Summary of major differences in problem settings of social choice and preference modeling for
LLMs.
2 LEARNING PROBLEMS: PREFERENCE MODELING AND SOCIAL CHOICE
Wefirstoutlineeachproblemsettingandtheircoredifferences(seeTable1).Wenotatealternatives,
theoptionsthatvotersorevaluatorscanchoose,asa∈A,withA⊆Rdasthespaceofallpossible
alternatives.Eachalternativeashouldbethoughtofasaprompt-responsepair,notjustaresponse.3
Humans—oftencalledevaluators(preferencemodeling)orvoters(socialchoice)—areindexedwith
i,withntotalevaluators.Thesetofactualpreferences(votes)isdenoted{π} i∈[n],withπ i,asetof
pairwisecomparisons,asthepreferencesfromvoteri. Fora,a′ ∈A,a≻a′denotes“aispreferred
toa′.”
Human Preference Modeling The goal of human preference modeling4 is to learn a reward
model r : A → R that quantifies the desirability of a particular alternative a. The standard
data collection method is via pairwise comparisons of text completions for a given prompt, and
the standard noise model assumed is a Bradley-Terry model, where the likelihood of observing
an instance of a being preferred to a′ is proportional to how much “better” a is than a′, i.e.
p(a≻a′)= exp(r(a)) .
exp(r(a))+exp(r(a′))
SocialChoice Inthestandardsocialchoicesetting,givenasetofalternativesandasetofvoters,
we receive a ballot from each voter i quantifying the voter’s preferences over all A alternatives.
Commonly, this takes the form of rankings, where π gives voter i’s rankings of all alternatives.
i
A voting rule aggregates all n ballots to produce a single election winner. Voting rules can be
evaluatedaxiomatically,i.e.withrespecttowhethertheysatisfyparticularaxioms(propertiesofthe
aggregationprocess), or in terms of distortion, i.e. with respectto the (aggregate)benefitderived
fromselectingthewinneroverotheralternatives.
Propertiesandgoalsofthelearningproblem Commonassumptionsofthesocialchoiceprob-
lem settinginclude: thatalternativesarefixed andfinite, i.e. thatAcontainstheonlyoptionsthat
couldpossiblybeconsidered;thatwehavefullinformationfromeachvoterabouteachalternative;
and that voters and their preferences are fixed (see also Appendix A). By contrast, in RLHF, the
spaceofalternativesisstructuredbutinfinite. Theevaluators,whomaynotberepresentativeofthe
fullpopulation,givepairwiseork-wisepreferencesoversubsetsofalternatives,oftenfromahand-
3Wechoosetoconsideralternativesatthelevelofprompt-responsepairs,ratherthanindividualresponses
perprompt.ThisisbecauseeventhoughRLHFannotationsarepairwisecomparisonsforresponsestothesame
prompt,thehigh-levelgoalistolearnaglobalrewardfunctionthatcanscoreanyprompt-responsepair.
4SeeLambertetal.(2023);Casperetal.(2023)fordetailedsurveys,andcitationsthereinformathematical
derivationsoftheRLHFobjective.
2PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
pickedsetofprompts. Eachevaluatoronlyseesasmallsubsetofalternatives,andeachalternative
isonlyseenbyasmallsubsetofevaluators.
In SCT, common goals of the voting process include selecting a single winner (e.g. an elected
official) or a set of top-k winners. By contrast, the goal of RLHF is to produce a reward model
thatcanscorethequalityofnewalternatives. Thatis,alternativesarenotonlyrankedbutassigned
real-valuedrewards,andtheassignmentoftheserewardsmustgeneralizetounseenalternatives.
2.1 DEFINING THE(PREFERENCE MODELING)VOTING RULE
We proposea reformulationof thesocial choiceproblemthatlets usinterpolatebetweenbothset-
tings.5
Consideration 1: Parameterization of alternatives and preferences. First, we assume that
alternativesareparameterizedina d-dimensionalspace asa ∈ Rd, withA ⊆ Rd. Recallthatwe
letarepresentapromptanditscompletion;accordingly,wecontinuetoassumethatallalternatives
arecommensurable,i.e. thatthereisareasonableandwell-definedcomparisonacrossalternatives.
Wealsoparameterizethepreferencesofeachvoteriovertextfeaturesasθ ∈ Rd andmodelvoter
i
i’srewardascribedtoaparticularpieceoftextasr (x) = hθ ,xi. Inourmodel,votershavefixed
θi i
preferencesover features, and rewards for each piece of text are scaled by those preferences; see
alsoAppendixA.
Consideration2: Votersfromapopulation. Insteadofconsideringnfixedvoters(evaluators),
asinsocialchoice,weassumeallvoters’preferencesaredrawnfromsomeunderlyingpopulation
θ ∼ V.6 This allows us to model both the scenario in which there exists some shared societal
i
norm from which individual preferences are drawn, and more complicated distributions (e.g.
mixturesof preferencemodels; Zhaoetal. (2016; 2018);Liu&Moitra(2018) discussmixturesof
Bradley-Terryandrandomutilitymodels).
With these considerations in mind, we present the preference modeling voting rule. Though the
definitioncansimplybeinterpretedasarewardmodel,ourstatementisintendedtoemphasizethe
interplay between how the voting rule aggregates seen preferences and how it evaluates unseen
alternatives.
Definition2.1(PreferenceModelingVotingRule). Apreferencemodelingvotingrulef :Rd →R
isafunctionthatmapssome(parameterized)alternativea ∈ Rd tosomereal-valuedscore,which
representsthepopulation’sassessmentofthequalityofa.
3 EVALUATING THE (PREFERENCE MODELING) VOTING RULE
How can social choice inform preference model evaluation? We argue that preference modeling
can be divided into two subproblems: that of generalizing a particular set of preferences to new
outputs; and that of deciding how to aggregate preferences over outputs. RLHF research has
focused almost exclusively on the generalization problem. This perspective, though reasonable
under the assumption that evaluators do not meaningfullydisagree on their preferences, does not
account for meaningful and widespread disagreement between evaluators, as evidenced by work
such as Aroyoetal. (2023). When evaluators disagree, two perspectives from SCT can help to
analyze problemsthat arise in RLHF: axiomatic approachesand distortion. We discuss the tenets
and potential contributions of these three perspectives–generalization, axiomatic approaches, and
distortion–inthissection.
3.1 PERSPECTIVE 1: GENERALIZATION
Following standard approaches in statistics and machine learning theory, recent work (Zhuetal.,
2023; Wangetal., 2020) studies the problem of estimating θˆunder Bradley-Terry models where
r (x) ∝ hθ,xi. These approaches take the alternatives to be fixed and predetermined, and all
θ
5Siththaranjanetal. (2023) give aninterpretation of themaximum-likelihood estimator (MLE) under the
Bradley-TerrymodelasanimplementationofBordacount,astandardvotingrulefromthesocialchoiceliter-
ature,thoughwithoutexplicitlymakingthemodelingdecisionsweproposehere. Infuturework,wehopeto
giveadeeperanalysisofthispropositionbasedonthediscussioninSec.3.
6SomeworkinSCT,e.g. thelineofworkinDey&Bhattacharyya(2015),alsoconsidersapopulationof
voters;most,however,assumesfixedvoters.
3PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
randomness is due to Bradley-Terry. Even under the goal of estimating θˆ well (or minimizing
the regret of a policy that would use an estimated θˆ, as analyzed in Zhuetal., 2023), natural
extensionscouldconsiderthecomplicationofpsychologicalfactorsthatbiasobservedpreferences,
suchaspreferencesfor“sycophantic”textorlongoutputs(Perezetal.,2023;Singhaletal.,2023);
modelingmorediverseevaluators;oranalyzingthesetofpromptstobeannotated.
3.2 PERSPECTIVE 2: AXIOMATICCHARACTERIZATIONS
A core tenet of social choice is that voting rules can be axiomatically analyzed; i.e., absent the
notion of some “ground truth,” there are particular principles that the final output should follow.
This permits a finer-grained understandingof how aggregationshandle individual pieces of input.
However, to apply axiomatic analysis to the preference modeling setting, we must still consider
“generalization”inasensenotconsideredbystandardSCTapproaches.
Onatechnicallevel,axiomsforthepreferencemodelingsettingmustsatisfytwocriteria: (1)they
mustapplytoscores,ratherthansinglewinners;and(2)theymustdistinguishbetweenrelationships
thatapplytothefullspaceofalternativesAandallvoters,andthosethatapplyonlytoalternatives
and ballots seen at train time. These properties mean that some canonical SCT axioms may be
wholly inapplicable,while othersmayrequire carefulreformulationto applyin the RLHF setting.
Wehighlightexamplesofeachofthesecasesbelow.
Example:unanimity,consistency,andCondorcetconsistency. Inthesingle-winnersetting,an
alternativea ∈ AisaCondorcetwinneronaparticularsetofballotsif,forthemajorityofvoters,
a ≻ a′ foralla′ 6= aanda′ ∈ A. ACondorcet-consistentvotingruleforthesingle-winnersetting
alwaysreturnstheCondorcetwinner,ifitexists. Consistencyissatisfiedifwhen,foreverypartition
ofvoters,thevotingruleselectsthesamewinner,itholdsthatthevotingruleoverallthevotersalso
selectsthatwinner. Unanimityissatisfiedwhen,ifeveryindividualvoterexpressesthepreference
a≻b,thenthevotingrulealsoselectsaoverb.
Recall that, for RLHF, we are interestednotin the finalwinner butin the scoresthatf assignsto
arbitrary(unseen)alternatives,andthatinthissetting,arbitrarilysmallpreferencesmaymatterless
than differences in reward greater than some margin ε. Also recall that voters are sampled from
an underlyingpopulation. To accountfor these differences, we propose definitionsfor unanimity,
consistency,andCondorcetconsistencyfortheRLHFsettingasfollows:
Definition3.1(Unanimityforpreferencemodeling). First,define(a,ε)-unanimityasfollows. For
a fixed a (a potentially unseen alternative) and a population of (potentially unseen) voters V, let
A′ ⊆ A be the set of alternatives such that for all voters, hθ ,(a−a′)i > ε for all a′ 6= a and
a i
a′ ∈ A′. Thena(preferencemodeling)votingrulef is(a,ε)-unanimouswhenf(a)−f(a′) > ε
a
foralla′ ∈A′,andε-unanimouswhenitis(a,ε)-unanimousforalla∈A. Iff isε-unanimousfor
a
ε=0,thenwesayf isunanimous.
Definition3.2(Condorcetconsistencyforpreferencemodeling). We define(a,ε)-Condorcetcon-
sistencyasfollows.ForafixedaandvoterpopulationV,letA′ ⊆Abethesetofalternativessuch
a
thatE θi∼V[hθ i,(a−a′)i]>εforalla′ 6=aanda′ ∈A′ a. Thena(preferencemodeling)votingrule
f is(a,ε)-Condorcetconsistentwhenf(a)−f(a′)>εforalla′ ∈A′,andε-Condorcetconsistent
a
whenitis(a,ε)-Condorcetconsistentforalla ∈ A. Iff isε-Condorcetconsistentforε = 0,then
wesayf isCondorcetconsistent.
Definition3.3(Consistencyforpreferencemodeling). Wedefine(a,ε)-consistencyasfollows.For
afixeda,letA′ ⊆ Abethesetofalternativessuchthatforanysufficiently-largesubsetofvoters
a
V′ ⊆V,fV′(a)−fV′(a′)>εforalla′ ∈A′ a. Thena(preferencemodeling)votingrulef is(a,ε)-
consistentwhenfV(a)−fV(a′) > εforalla′ ∈ A′ a,andε-consistentwhenitis(a,ε)-consistent
foralla∈A. Iff isε-consistentforε=0,thenwesayf isconsistent.
Notethedistinctionbetween3.1,3.2,and3.3liesprimarilyinwhatsliceofvotersisbeingexamined;
the expectationin 3.2 capturesthe idea ofmajoritypreference,while 3.3 is concernedwith agree-
mentacrosssubsetsofvoters. Thefactthattheseaxiomscanbeexpressedintermsofwhichtypes
ofagreementareimportanttorespectsuggeststhatthereisroomformoreexplicitconsiderationof
whichtypesofdisagreementareimportantinpreferencemodeling.
Other axioms of (single-winner) social choice may not apply. Resolvability (that the voting
rule producesno ties) or majority (that any alternative ranked first by a majority of voters should
win) have no clear translation to the preference model setting, since choosing a single winner is
4PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
no longer the main objective. Strategyproofness—therobustness of the final outcome to strategic
behaviorby individualvoters—is a less pressing concernwhen the numberof alternativesis very
large,andwhenthefinaloutcomeisascoringruleratherthanasinglewinner.7 Despitetheseeming
inapplicabilityofsome“classic”SCTaxioms,wearguethatitisstillworthwhiletoconsiderwhat
new axioms for preference modeling might look like: axioms were developed to establish more
fundamentaldesideratafordemocraticprocesses;moreover,theyaredesignedtoapplyincasesthat
lackanobvious“groundtruth”optimum.
3.3 PERSPECTIVE 3: DISTORTION
An alternative approach to evaluating voting rules in social choice is through distortion (see
Anshelevichetal. (2021) for a recent survey). At a high level, distortion is a quantitative notion
ofsuboptimalitywithrespecttosomeinformationthatthevotingrulemaynotbeabletoaccess(e.g.
the “hiddencontext”of Siththaranjanetal. (2023). In an extensionof the standard setting, voters
have utilities over alternatives, and submit ballots—rankings—that are consistent with those true
utilities. 8 Thedistortionofavotingruleistheworst-casedifferenceinutilitybetweentheoptimal
election winner and the winner chosen by the voting rule, where the worst-case is taken over all
possibleutilityfunctionsthatwouldhavestillbeenconsistentwiththeobservedballots.
Aswithaxiomaticcharacterizations,itisnontrivialtoredefinedistortiondirectlyforRLHF.InSCT,
itisreasonabletoanalyze“worst-case”(hidden)utilitiesduetothefinitenessofboththealternatives
andthevoter,theassumptionofnonoiseinreportingutilities, andbecauseballotstendtocontain
informationaboutvoters’preferencesovertheentirespaceofalternatives.
For RLHF, therefore, we might consider an approach that captures the conceptual insight of the
distortionmetricratherthanattemptingtotransliterateitdirectly. Forexample,wemightconsider
someper-featureweightw ∈Rd wherew scalestheimportanceoffeaturej. Thiscouldrepresent
j
phenomenalikecognitivebiasinlabeling,suchthatannotationsarereceivedaccordingtoaproxy
reward function r (a) = P θ w a , but where true utilities depend only on θ and not w.
θ,w j∈[d] j j j
On theotherhand, thiscouldalso reflectthe scenariowhereannotationsare givenwith respectto
r =hθ,aibututilityforhowaccuratelyaparticularfeatureθ islearnedvariesbyfeature(scaledby
θ i
w )—forexample,someonemaypreferbothlongertextcompletionsandtextthatavoidsgendered
i
stereotypes,butwouldcaremoreaboutthefinalθcapturingthelatter. Tooperationalizethe“worst-
case”inconjunctionwiththerandomnessinvotingassumedbyBradley-Terry-Luce,wecouldwork
withexpectedwelfare(wheretheexpectationincludesrandomnessinvoting);assumesomerelation-
shipbetweenθandw;andtaketheworst-caseoverpossibleθandwthatwouldbeconsistentwith
theobservedvotes. Interestingly,forthestatisticalinferenceproblemofestimatingθ,thisisafun-
damentallydistinctperspectivefromthestandardRLHFapproachofmaximumlikelihood;instead,
itismoreanalogoustoestimatingaθthatisrobustto“worstcase”realizationsofhiddencontext.
4 DISCUSSION
SCTprovidesarichinfrastructureforfiner-graineddiscussionofmanydifficultiesinusingRLHFto
representhumanpreferencesbyprovidingtheoreticalunderpinningsforsourcesoftheseproblems.
These includeconceptionsofthe promptspace, includinghow promptsforRLHF are chosenand
whatguaranteeson representationmay be lacking if the sample is handpickedin an unusualway;
and of the evaluator space, including consequences of using an unrepresentative or small set of
evaluators. In addition, they may help to conceptualize evaluator behavior: though potentially
“misaligned” or “strategic” evaluators are often discussed (Casperetal., 2023), SCT provides
a framework for analyzing the perhaps more common issues of evaluators dealing with poor
incentives, incomplete directions, and cognitive biases (Singhaletal., 2023; Huangetal., 2023).
Carefulengagementisnecessarytocombineinsightsfromthesecommunitiesinawaythatisboth
rigorousandfruitful. OnenaiveinterpretationofimpossibilityresultssuchasArrow’sTheoremis
thataversionofdemocracythatreliesondirectinputfromthepublicissimplyuntenable.However,
a key aspect of SCT is that the axioms or properties of a voting mechanism that are actually
desirabledependgreatlyonthecontextofthedecision.Byfocusingonthepropertiesthatmatterin
7Eveninthecontextofbinaryclassification,Hardtetal.(2023)requirecoordinatedactionofaround10%
ofevaluatorstosubstantiallyaffecttheoutputofthelearnedmodel.
8Theseutilitiesarehiddenandperhapsunknowneventothevoter,sotheycannotbeeliciteddirectly.
5PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
theRLHFsetting,andadaptingSCTformulationstodifferencesintheRLHFproblem,wereacha
spaceofproblemsthatarebothcriticalandtractable.
ACKNOWLEDGMENTS
ManythankstoBaileyFlanigan,PaulGo¨lz,NikaHaghtalab,CassidyLaidlaw,andAnandSiththa-
ranjanfortheirhelpfuldiscussionsandfeedback.
REFERENCES
Elliot Anshelevich, Onkar Bhardwaj, Edith Elkind, John Postl, and Piotr Skowron. Ap-
proximating optimal social choice under metric preferences. Artificial Intelligence, 264:
27–51, 2018. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.2018.07.006. URL
https://www.sciencedirect.com/science/article/pii/S0004370218304569.
Elliot Anshelevich, Aris Filos-Ratsikas, Nisarg Shah, and AlexandrosA Voudouris. Distortionin
socialchoiceproblems:Thefirst15yearsandbeyond. arXivpreprintarXiv:2103.00911,2021.
Lora Aroyo, Alex S. Taylor, Mark Diaz, Christopher M. Homan, Alicia Parrish, Greg Serapio-
Garcia, Vinodkumar Prabhakaran, and Ding Wang. Dices dataset: Diversity in conversational
aievaluationforsafety,2023.
Gerdus Benade, Ariel D. Procaccia, and Mingda Qiao. Low-distortion social wel-
fare functions. In AAAI Conference on Artificial Intelligence, 2019. URL
https://api.semanticscholar.org/CorpusID:53078427.
Felix Brandt, VincentConitzer, Ulle Endriss, Je´roˆme Lang, and Ariel D Procaccia. Handbookof
computationalsocialchoice. CambridgeUniversityPress,2016.
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Je´re´my Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems
and fundamental limitations of reinforcement learning from human feedback. arXiv preprint
arXiv:2307.15217,2023.
Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H. Holliday, Bob M. Jacobs, Nathan
Lambert, Milan Mosse´, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and
WilliamS.Zwicker. Socialchoiceforaialignment:Dealingwithdiversehumanfeedback. arXiv
preprintarXiv:2404.10271v1,2024.
Palash Dey and Arnab Bhattacharyya. Sample complexity for winner prediction in elections. In
Proceedingsofthe2015InternationalConferenceonAutonomousAgentsandMultiagentSystems,
pp.1421–1430,2015.
Sara Fish, Paul Go¨lz, David C Parkes, Ariel D Procaccia, Gili Rusak, Itai Shapira, and Manuel
Wu¨thrich. Generativesocialchoice. arXivpreprintarXiv:2309.01291,2023.
Bailey Flanigan, Daniel Halpern, and Alexandros Psomas. Smoothed analysis of social choice
revisited. In InternationalConference on Web and Internet Economics, pp. 290–309.Springer,
2023.
Daniel Halpern, Gregory Kehne, Ariel D Procaccia, Jamie Tucker-Foltz, and Manuel Wu¨thrich.
Representationwithincompletevotes. InProceedingsoftheAAAIConferenceonArtificialIntel-
ligence,volume37,pp.5657–5664,2023.
MoritzHardt,EricMazumdar,CelestineMendler-Du¨nner,andTijanaZrnic. Algorithmiccollective
actioninmachinelearning. arXivpreprintarXiv:2302.04262,2023.
Olivia Huang, Eve Fleisig, and Dan Klein. Incorporatingworker perspectives into mturk annota-
tionpracticesfornlp. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pp.1010–1028,2023.
Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The history and risks of reinforcement
learningandhumanfeedback. arXive-prints,pp.arXiv–2310,2023.
6PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
AllenLiuandAnkurMoitra. Efficientlylearningmixturesofmallowsmodels. In2018IEEE59th
AnnualSymposiumonFoundationsofComputerScience(FOCS),pp.627–638.IEEE,2018.
Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and
Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint
arXiv:2309.06657,2023.
DavidMiller. Deliberativedemocracyandsocialchoice. Politicalstudies,40(1 suppl):54–67,1992.
AbhilashMishra. Aialignmentandsocialchoice:Fundamentallimitationsandpolicyimplications.
arXivpreprintarXiv:2310.16048,2023.
Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig
Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben-
jamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan,
Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khun-
dadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Lan-
dau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Mi-
randa Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova
DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec,
Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan,
Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda
Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer,
and Jared Kaplan. Discovering language model behaviors with model-written evaluations. In
Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association
for Computational Linguistics: ACL 2023, pp. 13387–13434, Toronto, Canada, July 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.847. URL
https://aclanthology.org/2023.findings-acl.847.
ArielD ProcacciaandJeffreySRosenschein. Thedistortionofcardinalpreferencesinvoting. In
InternationalWorkshoponCooperativeInformationAgents,pp.317–331.Springer,2006.
RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherDManning,andChelsea
Finn. Direct preference optimization: Your languagemodel is secretly a reward model. arXiv
preprintarXiv:2305.18290,2023.
Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating
lengthcorrelationsinrlhf. arXivpreprintarXiv:2310.03716,2023.
AnandSiththaranjan,CassidyLaidlaw,andDylanHadfield-Menell. Distributionalpreferencelearn-
ing:Understandingandaccountingforhiddencontextinrlhf.ArXiv,abs/2312.08358,2023.URL
https://api.semanticscholar.org/CorpusID:266191810.
PiotrSkowronandEdithElkind. Socialchoiceundermetricpreferences: Scoringrulesandstv. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume31,2017.
JingyanWang,NiharShah,andRRavi. Stretchingtheeffectivenessofmlefromaccuracytobias
forpairwisecomparisons.InInternationalConferenceonArtificialIntelligenceandStatistics,pp.
66–76.PMLR,2020.
Nils Wilde, Erdem Biyik, Dorsa Sadigh, and Stephen L Smith. Learning reward functions from
scalefeedback. InConferenceonRobotLearning,pp.353–362.PMLR,2022.
ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,AlaneSuhr,PrithvirajAmmanabrolu,NoahASmith,
MariOstendorf,andHannanehHajishirzi. Fine-grainedhumanfeedbackgivesbetterrewardsfor
languagemodeltraining. arXivpreprintarXiv:2306.01693,2023.
ZhibingZhao,PeterPiech,andLirongXia. Learningmixturesofplackett-lucemodels. InInterna-
tionalConferenceonMachineLearning,pp.2906–2914.PMLR,2016.
Zhibing Zhao, Tristan Villamil, and Lirong Xia. Learningmixtures of random utility models. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32,2018.
BanghuaZhu, Jiantao Jiao, andMichaelI.Jordan. Principledreinforcementlearningwith human
feedbackfrompairwiseork-wisecomparisons,2023.
7PublishedatICLRWorkshoponReliableandResponsibleFoundationModels2024
A ADDITIONAL RELATED WORK
Social choice Instead of applying social choice to AI, Fishetal. (2023) show that applying AI
to augment democratic processes can improve canonical social choice results. In line with our
discussion of application-specific reworkings of social choice axioms, Flaniganetal. (2023) uses
smoothed analysis for relaxation of worst-case social choice axioms, which helps to distinguish
votingrulesthatrarelysatisfyaxiomsfromthosethatoftendo.
Halpernetal. (2023) discuss relaxed assumptions on fixed preferences and full information.
Fishetal. (2023) discusses relaxations of assumptions regarding fixed, finite, and commensu-
rable preferences. The literature on metric preferences (see e.g. Skowron&Elkind (2017);
Anshelevichetal.(2018))alsoparameterizesalternativesinEuclideanspace;however,voters’pref-
erencesoverthemaredeterminedbytheirdistancefromeachalternative.
In single-winner elections, the distortion of Borda countis unbounded(Procaccia&Rosenschein,
2006);however,numericalexperimentsfromBenadeetal.(2019)suggestthatBordacountmaybe
near-optimalinasettingwherethevotingruleoutputsarankingoveralternativesratherthanasingle
winner.
RLHF Related empirical RLHF work has raised alternative ways to collect human preferences,
suchasmeasuringthedegreeofpreference(Wildeetal.,2022)orsolicitingfine-grainedpreferences
that compare alternatives along multiple dimensions (Wuetal., 2023). Alternative optimization
methodstoRLHFhavealsobeenproposed(e.g.Rafailovetal.(2023);Liuetal.(2023)).
Political theory For democratic theorists, social choice results over the last few decades have
provided both challenges and clarity to normative discussions of what makes collective decision-
making “legitimate.” For example, types of choices have different normative consequences (e.g.,
choosingthe best way to explain a math problemor the best way to discuss a conspiracytheory),
which links to discussion of what decisions are best suited to different voting rules; for instance,
Miller(1992)arguethatBordacountoftenworkswell,butkeynormativedecisionsmightdemand
majoritarianism.InthecontextofLMs,thisdistinctionalsohelpstodistinguishbetweenpreferences
thatcanbepersonalizedanduniversaljudgmentsthatmustaffectallusers.
8