Stackelberg Game-Theoretic Learning for
Collaborative Assembly Task Planning
Yuhan Zhao1, Lan Shi2, and Quanyan Zhu1
1 New York University, Brooklyn NY 11201
{yhzhao, qz494@nyu.edu}
2 Purdue University, West Lafayette, IN, 47906
shi633@purdue.edu
Abstract. As assembly tasks grow in complexity, collaboration among
multiplerobotsbecomesessentialfortaskcompletion.However,central-
ized task planning has become inadequate for adapting to the increas-
ing intelligence and versatility of robots, along with rising customized
orders. There is a need for efficient and automated planning mecha-
nisms capable of coordinating diverse robots for collaborative assembly.
Tothisend,weproposeaStackelberggame-theoreticlearningapproach.
By leveraging Stackelberg games, we characterize robot collaboration
through leader-follower interaction to enhance strategy seeking and en-
sure task completion. To enhance applicability across tasks, we intro-
duce a novel multi-agent learning algorithm: Stackelberg double deep
Q-learning, which facilitates automated assembly strategy seeking and
multi-robot coordination. Our approach is validated through simulated
assembly tasks. Comparison with three alternative multi-agent learning
methodsshowsthatourapproachachievestheshortesttaskcompletion
time for tasks. Furthermore, our approach exhibits robustness against
both accidental and deliberate environmental perturbations.3
Keywords: Task planning · Stackelberg games · Deep reinforcement
learning · Multi-agent learning · Collaborative assembly · Smart manu-
facturing.
1 Introduction
Task planning has emerged as a pivotal component in smart manufacturing
[5,24,32].Itoptimizesthemanufacturingprocessesandimprovestheadaptability
to meet customized orders, and has been extensively applied to diverse smart
manufacturingscenarios,suchaswarehousepicking[41],inventorymanagement
[45], and human-robot collaboration [7,42]. As a prominent sector within smart
manufacturing, assembly particularly demands effective task planning methods
tocompletemanufacturingtasks.Despitethehugevariationsinassemblytasks,
the fundamental objective of assembly planning is to find an optimal assembly
3 The simulation codes are available at https://github.com/yuhan16/
Stackelberg-Collaborative-Assembly.
4202
rpA
91
]OR.sc[
1v07521.4042:viXra2 Y. Zhao et al.
sequenceamongfeasibleassemblyplanstoassembletheincomingproduct.Many
studies have been conducted to address various assembly planning problems
[15,26,36].
Fig.1:IllustrationofStackelbergLearningframeworkforcollaborativeassemblytasks.
The real assembly tasks (part 1) are abstracted and decomposed using chessboard
representation (part 2) and are then used as virtual tasks for Stackelberg learning
between robots (part 3). Robots leverage the learned strategies to collaborate and
complete assembly tasks (part 4). Assembly task figures are adapted from [14,35].
As more industrial robots are deployed to handle increasingly complex as-
semblytasks,effectivetaskplanningmechanismsmustnowconsidermulti-robot
coordination.However,designingsuchmechanismsforassemblycontinuestode-
mand considerable research effort due to emerging challenges. First, as robots
becomemoreintelligentandtheirnumbersincrease,theygainversatilityandare
capableofbeinggroupedwithotherstotacklevarioustasks,aimingtooptimize
resourceutilization.Centralizedplanningbecomesinadequateforprovidingflex-
ible collaboration plans in this evolving landscape. Therefore, robot-level plan-
ningandcollaborationstrategyseekingbecomescriticaltoadaptingtodynamic
manufacturing environments and maximizing efficiency. Second, as smart man-
ufacturingshiftstowardsmasspersonalization,taskplanningalsoneedstomeet
the increasingly customized demands. Traditional batch production paradigms
areill-suitedforpersonalizedorders,andrelyingonhumanexpertstodesigncol-
laboration schedules for each customized task becomes unmanageable. Hence,
there is a need for effective and automated planning mechanisms to facilitate
multi-robot collaboration and ensure task completion in this dynamic environ-
ment.
Gametheoryemergesasanaturalcandidateformodelingmulti-robotcollab-
oration from agent perspectives. Considering most collaborative assembly tasks
are operated sequentially by two robots, Stackelberg games provide an ideal
framework to capture sequential interactions between heterogeneous robots in
collaborative assembly tasks using a hierarchical interaction structure [38]. For
instance,whentworobotsperformdifferentoperationsinacollaborativeassem-Stackelberg Learning for Collaborative Assembly 3
blytask,onerobottakestheleadershiprole4andchoosesanassemblyactionfirst
by anticipating the follower’s subsequent move. The other robot, called the fol-
lower, operates sequentially after the leader’s action. Such leader-follower inter-
actionshaveprovenmoreeffectiveinensuringtaskcompletion.TheStackelberg
game-theoretic formulation provides advantages for coordinating sequential col-
laborationsbyapplyingaleader-followertypeofinteractiontorobots.Underthe
Stackelberg game-theoretic planning approach, two robots leverage the Stack-
elberg equilibrium strategies as the agent-level collaboration plans to complete
assembly tasks that not only optimize the task performance but also facilitate
task completion, ultimately leading to an optimal task planning scheme.
However, designing analytic game equilibrium strategies, or collaboration
plans, requires substantial computational efforts. Recent advances in DRL pro-
videpromisinglearningsolutionstoseekcollaborationplans,whichsignificantly
alleviate the challenge posed by massive, customized task requirements. DRL
learns the optimal strategy for sequential decision-making problems by inter-
acting with the environment [1] and has emerged as a valuable tool in smart
manufacturing [20,31,33]. By extending the learning paradigm, like Q-learning,
to multi-robot settings, we can develop learning algorithms that facilitate the
generationofeffectivetaskplanswithintheframeworkofgame-theoreticcollab-
oration.
Therefore, in this work, we introduce a Stackelberg game-theoretic learning
framework to achieve collaborative assembly between two robots with heteroge-
neouscapabilities.Specifically,wefirstdecomposeanassemblytaskintodifferent
types of sub-tasks based on robots’ heterogeneous assembly capabilities and or-
ganizethetaskusingaunifiedchessboardrepresentation.Next,weformulatethe
collaborationprocessbetweentherobotsusingstochasticStackelberggamesand
develop Stackelberg double deep Q-learning for the leader and follower robots
to learn the collaborative strategies. Then, based on the learned strategies, two
robots take assembly actions in turn to complete assembly tasks. We use eight
simulated assembly tasks to validate our proposed algorithm and compare it
with three multi-agent learning methods. The results show that our Stackelberg
learningalgorithmnotonlyguaranteestaskcompletionbutalsoprovidesamore
effectivecollaborationplan,asmeasuredbyactionrewards.Additionally,oural-
gorithm also produces robust collaboration under deliberate perturbations.
The contributions of this paper are summarized as follows.
– We propose a Stackelberg game-theoretic framework to enable collaborative
assembly task planning between two robots with different capabilities.
– WedevelopaStackelbergdoubledeepQ-learningalgorithmtoenablelearn-
ing the optimal task planning schemes that ensure task completion.
– We validate the effectiveness and robustness of the proposed framework by
using eight simulated assembly tasks and comparing them with other multi-
robot collaborative algorithms.
The rest of the paper is organized as follows. Section 2 discusses the related
work.Section3introducesthecollaborativeassemblytaskrepresentationfortask
4 In practice, we can choose the more advanced robot as the leader.4 Y. Zhao et al.
visualization and learning. We describe the Stackelberg game-theoretic frame-
work for collaborative assembly and the Stackelberg double deep Q-learning
algorithm in Section 4. Section 5 evaluates the proposed framework with eight
simulated tasks, and Section 6 concludes the work.
2 Related Work
2.1 Stackelberg Games in Collaborative Manufacturing
The hierarchical decision-making structure has popularized Stackelberg games
invariousaspectsofmanufacturing,suchassupplychainoptimization[9,18,40],
resource allocation [6,8], energy management [17], and cyber security [43]. As
we are aware, most Stackelberg game-theoretic works focus on holistic produc-
tion solutions for collaborative manufacturing problems. For example, Chua et
al. in [9] have adopted dynamic Stackelberg games to capture the hierarchical
production flow in supply chains and jointly optimize the supply chain pricing,
production,andorderingdecisions.Caoetal.in[6]havedevelopedaStackelberg
game-based resource allocation strategy to improve idle manufacturing resource
utilization in cloud manufacturing and satisfy the customers’ real-time demand.
Similarly,Lietal.in[21]haveoptimizedmakespansinindustrialjobscheduling
problems using bilevel (Stackelberg) formulations. Only a few works leverage
Stackelberg games to make agent-level collaborative planning for assembly and
related manufacturing tasks. For example, Zhou et al., in in [48] have developed
a Stackelberg game-based approach for human-robot collaboration to jointly re-
move screws in products. The game-theoretic strategy has enabled the robot to
findasafeandefficientassemblyactiontoassistthehumanoperator(leader).In
multi-object rearrangement tasks, Zhao and Zhu in [47] have utilized a stochas-
tic Stackelberg game to coordinate and instruct two heterogeneous robots to
rearrange objects to the target position. In this work, we leverage Stackelberg
games to design effective operation schedules for two manufacturing robots to
collaborate on assembly tasks.
2.2 Reinforcement Learning in Collaborative Manufacturing
Reinforcementlearning(RL)hasfacilitateddecision-makingandproductionpro-
cessesincollaborativemanufacturing.Sincecollaborationrequiresmultipleman-
ufacturing agents (human operators or robots), most work develops multi-agent
reinforcementlearning(MARL)algorithmstolearneffectivecollaborationplans
andachievemanufacturingtaskobjectives.Forexample,Wangetal.in[39]have
studied manufacturing task allocation to multiple robots in a resource preemp-
tion environment using QMIX [34]. Johnson et al. in [12] have leveraged double
deep Q-network [37] based learning algorithms to dynamically schedule arriving
assembly jobs and minimize makespans for multiple robots in an assembly cell.
Besides, the self-organized task allocation mechanism between multiple indus-
trialvehicleshasbeeninvestigatedin[22]usingtheMADDPGalgorithm[25]toStackelberg Learning for Collaborative Assembly 5
reducetransportationcostsandimprovedistributionefficiency.Efficienthuman-
robot task scheduling in collaborative assembly based on Nash Q-learning [11]
has been proposed and studied in [42] to optimize the task completion time.
MARL is not the only learning paradigm in collaborative manufacturing. Some
works focus on learning a high-level scheduling plan for the overall manufactur-
ingprocessusingclassicRLapproaches,suchasDQNandpolicygradient.Then,
manufacturing agents collaborate based on the learned schedule. For example,
Bhatta et al. in [3] have developed a DQN-based algorithm to dynamically as-
sign mobile robots to workstations in a manufacturing system and improve the
overall production performance. Liu et al. in [23] have leveraged DRL to search
foreffectiveapproachesforschedulingdecentralizedrobotservicesincloudman-
ufacturing to achieve on-demand service provisioning. However, the majority of
the aforementioned works rely on the classic RL paradigm without character-
izing the strategic interactions between the manufacturing agents. In our work,
we propose a Stackelberg double deep Q-learning algorithm to learn an effective
and collaborative task scheduling to complete different assembly tasks.
2.3 Learning in Stackelberg Games
Learning is required to learn the equilibrium of a Stackelberg game when the
player’s decision-making models or the environment are unknown. Although ex-
isting learning algorithms, such as best-response learning [4,19,27], can learn
the equilibrium of static Stackelberg games, we focus on learning in dynamic
Stackelberg games since collaborative assembly requires multiple interaction
rounds to assemble a product. Reinforcement learning (RL) is a common learn-
ing paradigm for learning equilibrium in dynamic Stackelberg games. Some RL-
based algorithms have been developed to achieve the objective. For example,
the work [13,29] has proposed an asymmetric Q-learning algorithm to estimate
the Stackelberg equilibrium of a Markov game. Zhang et al. in [44] have devel-
opedabi-levelactor-criticstructuretolearnalocalStackelbergequilibriumofa
Markov game. Notably, some recent learning approaches based on regret learn-
ing [16] and online learning [46] have been introduced to find the Stackelberg
equilibrium with a myopic follower who makes decisions based on a one-step
prediction. However, as observed in [37], classic Q-learning, including deep Q-
learning[30],exhibitsmaximizationbiasthatresultsinoverestimatedvaluesand
sub-optimalpolicies.Moreover,Stackelberggameswithmyopicfollowersaretoo
conservative for collaborative assembly since current intelligent assembly robots
have sufficient prediction capabilities. Therefore, we develop a double deep Q-
learning-based algorithm to capture the diverse assembly tasks and alleviate
potential performance degeneration in the learning process.6 Y. Zhao et al.
3 Collaborative Assembly Task Description
3.1 Task Decomposition
Planning a collaborative assembly task generally consists of two processes: task
decomposition and task allocation5. We decompose a complex assembly task
intoseveralsub-tasks,andeachsub-taskrepresentsthesmallesttaskunitthata
robotcancompleteindividuallywithoutexternalassistance.Wefurtherassociate
sub-tasks with different types to represent their assembly characteristics, which
correspondtotherobots’heterogeneousassemblycapabilities.Wetakeabracket
assembly task in Fig. 2 as an example for illustration. Two collaborative robots
(L and F) aim to install a set of bolts (B1-B8) on the front and upper surface
of the module to connect the bracket parts. A bolt needs to be first placed in
the right position by one robot and then screwed by the other robot. The bolts
B1-B4 are common ones, so the placing and screwing can be done by either
robot. The bolts B5-B8 are specialized ones: only the robot L can screw the
bolts, and the robot F places the bolts in the right position. After installing the
bolts B1-B4 on the front surface, two robots flip the module together to install
B5-B8 on the upper surface.
Fig.2: The bracket assembly task example. Two robots collaboratively install bolts
B1-B8 to connect the bracket parts.
Therefore, we can decompose the bracket assembly task into a group of sub-
tasks and categorize them into four types:
- Type 1: Only robot L can complete.
- Type 2: Only robot F can complete.
- Type 3: Both robots L and F can complete, but one robot is sufficient.
- Type 4: Both robots need to work together to complete.
All decomposed sub-tasks are numbered and summarized in Tab. 1.
5 Mostliteratureusestheterms“planning"and“allocation"interchangeably.Here,we
use task allocation to distinguish the task decomposition.Stackelberg Learning for Collaborative Assembly 7
Task No. Sub-tasks Type
1-4 Place B1-B4 3
5-8 Place B5-B8 2
9-12 Screw B1-B4 3
13-16 Screw B5-B8 1
17 Flip left module 4
18 Flip right module 4
Table 1: The module bracket assembly task is decomposed into 18 sub-tasks and
categorized into 4 types.
3.2 Task Planning and Chessboard Representation
Task planning finds an optimal task execution sequence for robot collaboration
after decomposing a complex assembly task into sub-tasks. Task planning is
subject to temporal constraints, which specify the basic order of execution. For
example,aboltmustbeplacedintheproperpositionbeforeassembled;thefront
surfaceassemblymustbeprocessedbeforetheuppersurfaceassembly.Notethat
temporal constraints do not necessarily apply to all sub-tasks. For example, we
can either tighten a bolt immediately after placing it or place all bolts first
and tighten them. A task with temporal constraints can be characterized by
a directed graph (see Fig. 3 left), where the nodes denote the sub-tasks and
the edges denote the temporal relationship. For large-scale assembly tasks, a
directed graph representation can be messy when visualizing and tracking task
progress. Inspired by [42], we use the chessboard representation to represent an
assemblytask,whichnaturallycapturesthedecomposedsub-tasksandtemporal
relationships.
Thechessboardrepresentationofthebracketassemblyexampleisillustrated
inFig.3(right).Eachblockinthechessboarddenotesasub-task.Thesub-tasks
with temporal constraints are stacked by rows; the sub-tasks at the same hier-
archical level (no temporal relationship) are placed in the same row. Therefore,
the number of rows represents the maximum temporal dependencies (hierarchi-
cal levels); the number of columns is determined by the maximum number of
non-temporal-correlated sub-tasks. For example, sub-tasks 1-4 are not tempo-
rally correlated since they can be completed in any order. So, the chessboard
has 4 columns. The bottom row contains all available sub-tasks for robots to
operate on at the moment. When a sub-task in the bottom row is completed,
a new task may drop down from the top, depending on the task structure. The
same sub-tasks in the same row are combined to better represent the temporal
relationship between adjacent sub-tasks. For example, we combine sub-tasks 17
and 18 in the third row to indicate that sub-task 17 (resp. 18) is available only
after sub-tasks 9 and 10 (resp. 11 and 12) are completed. Hence the size of the
block does not have practical meaning.
Fig. 4 shows a feasible task planning scheme based on the chessboard repre-
sentation. At the interactive round t = 1, robots L and F choose the sub-tasks8 Y. Zhao et al.
Fig.3:[Left]directedgraphrepresentationofthebracketassemblytask.[Right]chess-
board representation of the task. Each node (resp. block) represents a sub-task. Dif-
ferent colors denote sub-task types. The last row of the chessboard contains available
sub-tasks.Thesameadjacentsub-tasksinthechessboardaremergedforcompactrep-
resentation.
T1 and T2, respectively, since both are type 3. Upon completion, sub-tasks 9
and 10 drop down, and so do other successive sub-tasks. At round t = 2, the
available sub-tasks are T9,T10,T3, and T4. Two robots choose T9 and T10 in
turns. At round t = 3, both robots choose T17 since it requires collaborative
force to complete (Type 4). Note T17 fails if only one robot chooses to execute
it.
The chessboard structure provides a concise and unified representation for
assemblytasks.Thedecomposedsub-tasks,theirtemporalrelationship,andthe
overall task completion progress are all presented in a well-structured way. It is
critical for product designers to design and examine the task flow. Besides, it
also facilitates the learning process of robot collaboration. Instead of tracking
the next sub-task in a complicated graph, the robots only need to focus on the
bottom row and select the available sub-task to complete. It reduces robots’
action spaces and leads to a more efficient learning algorithm design. Based
on the assembly interactions using the chessboard representation, we propose a
Stackelberggame-theoreticframeworktocapturethecollaborationbetweentwo
heterogeneous robots, which is discussed in Sec. 4.
4 Stackelberg Game-Theoretic Learning for Collaborative
Assembly
4.1 Stochastic Stackelberg Game Framework
Weconsidertworobotscollaborativelyassemblesomeproductsandassumethat
two robots have heterogeneous capabilities (i.e., good at solving different sub-
tasks).Duringtheassembly,tworobotstaketurnstocompletesub-tasks.Ineach
interactive round, one robot (the leader, she, denoted by L) takes the initiative
toleadthecollaborationandactsfirstbyanticipatingtheotherrobot’sresponse.Stackelberg Learning for Collaborative Assembly 9
(a) Atroundt=1,robotsLandF chooseT1andT2,respectively.Bothsub-tasksare
type3.
(b) Interactiveroundt=2.
(c) Interactiveroundt=3.
Fig.4: Collaborative task planning for the bracket assembly task in three interaction
rounds.10 Y. Zhao et al.
Theotherrobot(thefollower,he,denotedbyF)observestheleader’sactionand
makes his next move.
WeleveragestochasticStackelberggamestomodelthiscollaborativeinterac-
tion.AstochasticStackelberggamecanberepresentedbythetuple⟨S,Ai,ri,p,γ⟩,
i ∈ {L,F}. Let S represent the state space of an assembly task. A state s ∈ S
reflects the assembly task status, such as currently available sub-tasks. Let
ai ∈ Ai be a feasible action to complete a certain sub-task and the action
space for the robot i, i ∈ {L,F}. A special ∅ ∈ Ai, i ∈ {L,F}, means that
the robot does not take action. The reward function of robot i is given by
ri : S ×AL ×AF → R, i ∈ {L,F}. The overall task proceeds based on the
transition kernel p:S×AL×AF →∆(S), where ∆(S) is the set of probability
distributions over S. A strategy is a decision rule to generate actions. In this
work, we focus on Markov strategies πi : S → ∆(Ai) and denote Πi as the
set of all feasible strategies, i∈{L,F}. Both robots maximize the accumulated
rewardtocompletethetask.Then,thecollaborativetaskplanningproblemcan
be formulated as follows:
(cid:34) ∞ (cid:35)
max E
πL,πF
(cid:88) γtrL(s t,aL t,aF
t
)(cid:12) (cid:12)s
0
=s ,
πL,πF
t=0
(cid:34) ∞ (cid:35) (1)
s.t. maxE
πL,πF
(cid:88) γtrF(s t,aL t,aF
t
)(cid:12) (cid:12)s
0
=s ,
πF
t=0
πL ∈ΠL,πF ∈ΠF.
Given any feasible strategy pair ⟨πL,πF⟩, we define the robot i’s value function
in the state s∈S, i∈{L,F}, as the accumulated reward:
(cid:34) ∞ (cid:35)
V πi L,πF(s):=E
πL,πF
(cid:88) γtri(s t,aL t,aF
t
)(cid:12) (cid:12)s
0
=s . (2)
t=0
Then, we define the Stackelberg equilibrium (SE) as the solution concept of the
Stackelberg game (1) as follows.
Definition 1. Thestrategypair⟨πL∗,πF∗⟩constitutesaStackelbergequilibrium
of the game (1) if for all s∈S, there exists a best response mapping T :ΠL →
ΠF such that
VF (s)≥VF (s), ∀πF ∈ΠF,
πL∗,πF∗ πL∗,πF
(3)
VL (s)≥VL (s), ∀πL ∈ΠL.
πL∗,πF∗ πL,T(πL)
Here, the best response mapping T is obtained by argmax VF (s).
πF∈ΠF πL,πF
The resulting SE provides the strategies for robot collaboration. The robots’
interactiveactionsequencesgeneratedbytheSEnaturallyprovideoptimalplan-
ningforcollaborativeassemblytasks.Therefore,theStackelberggame-theoretic
framework can be used as an effective approach to scheduling assembly task
collaborations.Stackelberg Learning for Collaborative Assembly 11
Remark 1. For simplicity, we use the bold notation a = [aL,aF] and π =
[πL,πF] to denote compact vectors. The transition kernel can be written as
p(s′|s,a); the reward and value functions are simplified to ri(s,a) and Vi(s),
π
i∈{L,F}.
GameSpecificationforCollaborativeTasks WemaptheStackelberggame
framework to the collaborative assembly tasks. Given a chessboard (n columns)
representation of an assembly task, we define the state s at round t as the sub-
t
task indices in bottom row, which is an n-dimensional vector. Then, we define
the action space Ai = {∅,1,...,n}, i ∈ {L,F}, where a = j means choosing
t
the sub-task in the j-th column to complete.
Remark 2. Using the entire chessboard as the state can complicate the reward
function design because every task’s progress needs to be considered. Besides, it
is also more practical to assume robots can only observe the currently available
sub-tasks in the bottom row instead of the entire chessboard.
The task proceeds according to the chessboard rule discussed in Sec. 3.2,
which is inherently Markovian. The probabilistic transition kernel p(s′|s,a) al-
lows us to capture the accidents that fail the task completion, such as robot
chattering and environmental disturbance. Specifically, we use probabilities to
characterizethesefailuressincetheyarenotman-madeerrors.Forexample,the
robot L has a probability α > 0 to complete a type 1 sub-task and 1−α to
fail. We also can set probabilities for the rest types of sub-tasks. Deterministic
task dynamics is a special case where both robots can perfectly complete all the
sub-tasks, equivalent to setting zero probability of failure.
4.2 Stackelberg Q-Functions
Similartothevaluefunction(2),wedefinetheQ-functionofajointstate-action
pair (s,a) for robot i as
(cid:34) ∞ (cid:35)
Qi π(s,a)=E
π
(cid:88) γtri(s t,a t)(cid:12) (cid:12)s
0
=s,a
0
=a , i∈{L,F}. (4)
t=0
It is clear that E [Qi (s,a)] = Vi(s), ∀s ∈ S, i ∈ {L,F}. We also have the
π π π
following relationship using a one-step transition:
(cid:88)
Qi (s,a)=ri(s,a)+γ p(s′|s,a)Vi(s′), i∈{L,F}. (5)
π π
s′
Therefore, the SE of the game (1) can also be characterized by the Q-function
based on the following proposition.
Proposition 1. Let π∗ := ⟨πL∗,πF∗⟩ be a SE of the game (1). Then π∗ is
also a SE of the bimatrix game ⟨QL (s,·),QF (s,·)⟩ for all s ∈ S. Conversely,
π∗ π∗
a SE π∗ of the bimatrix game ⟨QL (s,·),QF (s,·)⟩ for all s ∈ S is also a SE
π∗ π∗
of the game (1). Here, we denote Qi (s,·) as an |AL|×|AF| matrix with the
π∗
(m,n)-entry the value of Qi (s,m,n), m∈AL,n∈AF,i∈{L,F}.
π∗12 Y. Zhao et al.
We refer the reader to Appendix A for the definition of bimatrix games.
Proof. For the first part, from (2) we have
(cid:34) (cid:35)
(cid:88)
Vi(s)=E ri(s,a)+γ p(s′|s,a)Vi(s′) , i∈{L,F}.
π π π
s′
It shows that the accumulated reward of robot i can be viewed as the aver-
age value of the utility matrix ri +γ(cid:80) pVi(s′), which is the Q-function of
s′ π
robot i from (5). Therefore, π∗ is automatically a SE of the bimatrix game
⟨QL (s,·),QF (s,·)⟩.
π∗ π∗
Forthesecondpart,sinceπ∗istheSEofthebimatrixgame⟨QL (s,·),QF (s,·)⟩,
π∗ π∗
for every s∈S and for every πF ∈ΠF,πL ∈ΠL, we have
E πL∗,πF∗[QF πL∗,πF∗(s,a)]≥E πL∗,πF[QF πL∗,πF(s,a)],
E [QL (s,a)]≥E [QL (s,a)],
πL∗,T(πL∗) πL∗,T(πL∗) πL,T(πL) πL,T(πL)
where the mapping T is given by argmax E [QF (s,a)]. Since
πL πL,πF πL,πF
E [Qi (s,a)]=Vi(s), ∀s∈S, i∈{L,F}, using the definition (3), we conclude
π π π
that π∗ is the SE of the game (1).
For any finite bimatrix game ⟨QL (s,·),QF (s,·)⟩ (i.e., |AL| and |AF| are
π∗ π∗
finite), s ∈ S, the SE in pure strategy exists [2]. Therefore, we can write the
optimal policy π∗ = a∗ := [aL∗,aF∗], which can be obtained by solving the
bilevel optimization problem:
(cid:18) (cid:19)
a∗ ←argmaxQL s,aL,argmaxQF (cid:0) s,aL,aF(cid:1) . (6)
π∗ π∗
aL aF
Prop. 1 paves the way for Stackelberg Q-learning to learn the SE of the
game (1). At time t, two robots observe the current state s and decide their
t
action a using the SE from the bimatrix game ⟨QL(s ,·),QF(s ,·)⟩. Then,
t t t t t
they receive their rewards r and the new state s ∼ p(·|s ,a ). Finally, two
t t+1 t t
robots update their Q-functions by computing a SE a′ of the bimatrix game
se
⟨QL(s ,·),QF(s ,·)⟩ using (6):
t t+1 t t+1
Qi (s,a)←(1−α)Qi(s,a)
t+1 t
(7)
+α(cid:0) ri(s,a)+γQi(s′,a′ )(cid:1) , i∈{L,F}.
t se
Remark 3. In (7), we use Qi rather than Qi to denote the Q-function at time
t πt
t, i ∈ {L,F}, because Qi is not the real Q-function associated with π . As the
t t
learningproceeds,wecanlearnthepolicyandtheQ-functionthatareconsistent
with each other. The resulting policy is the SE of the game (1).Stackelberg Learning for Collaborative Assembly 13
4.3 Double Deep Q-Network for Stackelberg Learning
For complex assembly tasks, the state space S can be huge, and learning the Q-
value for all state-action pairs becomes intractable. Thus, we parameterize the
Q-function by Q(s,a;θ) for a compact state space representation, resulting in a
deep Q network (DQN). The classic DQN algorithm [30] is designed for a single
agent and cannot be applied to the Stackelberg game (1) directly. Besides, as
observedin[37],DQNlearningcanleadtooverestimatedvaluesandsub-optimal
policies. Therefore, to enable learning in Stackelberg games and overcome the
disadvantages of the DQN, we propose the Stackelberg double deep Q-network
(DDQN) learning algorithm to learn the SE of the game (1).
In Stackelberg DDQN learning, the robot i, i ∈ {L,F}, possess two Q-
networks:anonlinenetworkQi(·;θi)foractiongeneration,andatargetnetwork
Q(cid:98)i(·;θ(cid:98)i) for future reward evaluation. In every learning step with state s, two
robots first use the online networks to generate an ϵ-greedy SE action
(cid:40)
ai with probability 1−ϵ
ai = SE , (8)
random a∈Ai\{ai } with probability ϵ
SE
for collaboration, i ∈ {L,F}, where a := [aL ,aF ] is the SE of the bimatrix
SE SE SE
game ⟨QL(s,·;θL),QF(s,·;θF)⟩. Then, a transition sample h := {s,a,r,s′ ∼
p(·|s,a)} is obtained and added to the replay buffer D as a past trajectory.
Next, the target network evaluates the future reward of transition samples in D
and updates the online network by minimizing the loss function
(cid:20)(cid:16) (cid:17)2(cid:21)
L(θi)=E
s,a,r,s′∼D
Qi(s,a;θi)−ri−γQ(cid:98)i(s′,a′ SE;θ(cid:98)i) (9)
for i ∈ {L,F}. The loss function measures one-step temporal difference (TD)
error resulting from the Bellman equation (5). Its gradient reads as
(cid:104)
∇ L(θi)=E ∇ Qi(s,a;θi)
θi s,a,r,s′∼D θi
(10)
(cid:16) (cid:17)(cid:105)
Qi(s,a;θi)−ri−γQ(cid:98)(s′,a′ ;θ(cid:98)i) .
SE
Here, a′ is the SE of the bimatrix game ⟨QL(s′,·;θL),QF(s′,·;θF)⟩. It is com-
SE
puted by the online network but evaluated by the target network, which aims
to reduce the overestimation in the Q-function. The target network parameter
θ(cid:98)i is periodically updated by θi. We summarize the Stackelberg DDQN learning
algorithm in Alg.1
Comparison with MARL A distinctive feature of Stackelberg DDQN learn-
ing is that robots use the SE strategy generated from their Q-functions to in-
teract in the learning process. This interaction pattern differs from common
MARL paradigms, such as independent Q-learning [28] and MADDPG [25].
Specifically, learning agents in independent Q-learning make interactive action14 Y. Zhao et al.
Algorithm 1: Stackelberg DDQN learning for collaborative assembly
task planning.
1 Initialize: Robot i’s online Q-network Qi(s,a;θi), target Q-network
Q(cid:98)i(s,a;θ(cid:98)i) with θ(cid:98)i =θi, i∈{L,F} ;
2 Initialize: Replay buffer D ;
3 for episode =1,...M do
4 Initialize environment state s 1 for both robots;
5 for t=1,...T do
// Generate transition trajectory
6 a se :=[aL se,aF se]← SE of bimatrix game ⟨QL(s t,·;θ tL),QF(s t,·;θ tF)⟩
using (6) ;
7 a t :=[aL t,aF t ]← ϵ-greedy action from a se using (8) ;
8 Two robots execute a t; observe reward r t and new state
s ∼p(·|s ,a ) ;
t+1 t t
9 Store the transition h t =(s t,a t,r t,s t+1) to D ;
// Update Q networks
Random sample mini-batch of trajectories
10
D :={(s ,a ,r ,s )}B ∼D ;
batch j j j j+1 j=1
11 for each sample in D batch do
12 a′ se ← SE of bimatrix game ⟨QL(s j+1,·;θ tL),QF(s j+1,·;θ tF)⟩ using
(6) ;
13
Compute future reward using target Q(cid:98)i, i∈{L,F}:
(cid:40)
ri if episode terminates at step j+1
ℓi = j
j r ij+γQ(cid:98)i(s j+1,a′ se;θ(cid:98)i) otherwise
;
14 end
15 θ ti +1 ← one-step GD to minimize (cid:80) j(cid:2) Qi(cid:0) s j,a j;θ ti(cid:1) −ℓi j(cid:3)2 using
(9)-(10), i∈{L,F} ;
16 Soft update θ(cid:98)i every C steps: θ(cid:98)i ←τθ(cid:98)i+(1−τ)θ ti, i∈{L,F} ;
17 s t ←s t+1 ;
18 end
19 endStackelberg Learning for Collaborative Assembly 15
decisions solely based on their local Q-functions. In MADDPG, the action gen-
erated by a learning agent is based on its Q-function and an estimate of other
agents’ strategies. Neither of them generates actions during learning based on
game equilibria, which can prohibit the effectiveness of collaboration.
Learning Architecture and Communication Between Agents The cen-
tralized training decentralized execution architecture is widely adopted in many
MARL algorithms (e.g., [10,25]). In this architecture, a centralized server sam-
ples the reply buffer and updates the Q-functions of all agents during the train-
ing stage. Subsequently, each agent chooses an action independently using the
updated Q-functions during the execution stage. Despite the interaction mecha-
nismdifferences,ourStackelbergDDQNlearningfollowsasimilartrainingstage
but requires additional communication during the execution stage. Specifically,
the leader needs the follower’s Q-function to compute her Stackelberg strategy
and action, while the follower needs the leader’s action to generate his Stackel-
berg strategy. This communication is bilateral but asymmetric since the leader
and follower require different information for learning. Therefore, by designing
proper communication protocols, we can achieve a distributed execution stage
in the learning.
5 Simulation and Evaluation
5.1 Simulation Setup
WeevaluateourStackelbergDDQNlearningalgorithmoneightdifferentassem-
blytasks,rangingfromsimpletocomplex.Task1involvesthebracketassembly
task in Sec. 3, and the remaining tasks are designed to assemble hypothetical
products. They can be readily replaced by any real-assembly scenarios. Each
task contains four types of sub-tasks as discussed in Sec. 3.1. We show Task 2-4
in Fig. 5, which consists of 18, 20, and 26 sub-tasks, respectively. The rest tasks
and their training results are listed in Appendix C for clarity.
We set three basic reward quantities r = 2, r = 1 and r = −1 to
cop ind cost
designtherewardfunctions.Iftworobotsjointlyselectthecooperativesub-task
(type 4), they each receive r . A robot receives r if the robot selects the
cop ind
right individual sub-task. For example, the leader robot selects the type 1 or
type 3 sub-task. If either robot selects the wrong type sub-task or both robots
have a conflict sub-task, a negative r will be received. If both robots take no
cost
actions, they receive r /2. Further details regarding the environment settings
cost
and hyperparameters can be found in Appendix B.
We conduct ten training sessions for each task to obtain the mean-variance
training performance. To facilitate comparison, we implement three alternative
learning algorithms, which are independent Q-learning (IND), Nash Q-learning
(NASH), and MADDPG. In independent Q-learning, each robot learns a sepa-
rate Q-function by treating the other as part of the environment. In the Nash
Q-learning, two robots interact by simultaneously playing a Nash game instead16 Y. Zhao et al.
(a) Assemblytask2. (b) Assemblytask3. (c) Assemblytask4.
Fig.5: Plots of Assembly Tasks 2-4.
of a Stackelberg game. In MADDPG, we use the mixed strategy to induce a
continuous action space. Each robot selects an assembly action based on the
mixed strategy.
5.2 Training Results
We use two metrics to measure learning performance: the task completion step
and the leader’s (and follower’s) averaged cumulative reward in each learning
episode. Averaging the cumulative reward is essential because we use a proba-
bilistic transition kernel during training. This probabilistic kernel allows robots
toaccumulatehigher rewardsbyrepetitively attemptingthe samesub-task fails
initially.WetakeTask1asanexample.Theleaderreceivesr ifsheselectsthe
ind
sub-task T1. If T1 is not completed (with 0.1 probability) and the leader keeps
selecting the same sub-task, she will receive an extra r . Therefore, to provide
ind
amoreaccuratereflectionoflearningefficacy,weaveragethecumulativereward
over the completion steps in each episode.
The training results for both the leader and follower are illustrated in Fig-
uresinFig.6and7,respectively.Notably,StackelbergDDQNshowsthehighest
averaged cumulative rewards for both leader and follower across all four tasks.
Thisindicatesthattherobotstakethemosteffectiveactionsduringthelearning
compared with the three alternative methods. Additionally, the lowest comple-
tion step for all tasks in Fig. 8 also suggests that Stackelberg DDQN is more
efficient in learning the collaborative strategy. Furthermore, Stackelberg DDQN
exhibits a smaller variance than Nash Q-learning and independent Q-learning,
indicating a more stable collaboration during learning.
Itisworthnotingthatthreemethods(StackelbergDDQN,NashQ-learning,
and independent Q-learning) show similar learning results for Task 1, primarily
duetoitssimplicity.SimpletaskslikeTask1donotfullydistinguishthelearning
methods. As the task complexity increases, Stackelberg DDQN demonstrates
superior performance in learning and indicates that Stackelberg game-theoretic
planning can effectively assist robots in finding optimal collaboration plans.Stackelberg Learning for Collaborative Assembly 17
Fig.6: Leader’s averaged cumulative rewards for Tasks 1-4.
Fig.7: Follower’s averaged cumulative rewards for Tasks 1-4.
Fig.8: Completion steps for Tasks 1-4. The maximum step length per episode is
(40,50,60,60) in corresponding tasks. Stackelberg DDQN has the lowest completion
step in training, while MADDPG fails to find a feasible assembly sequence in most
training experiments.18 Y. Zhao et al.
Task 1 Task 2 Task 3 Task 4
SG 18.9(3.151) 24.0(5.811) 31.2(9.835) 39.0(2.0)
NASH 27.5(4.631) 41.3(7.708) 33.9(9.728) 50.0(0.0)
IND 32.2(5.793) 39.0(5.865) 52.1(12.086) 60.0(0.0)
MADDPG 35.9(4.414) 45.0(8.074) 53.2(9.786) 60.0(0.0)
(a) Completionsteps.
Task1 Task2 Task3 Task4
reward_l reward_f reward_l reward_f reward_l reward_f reward_l reward_f
SG 0.792(0.198)0.710(0.223)1.357(0.123)1.201(0.134)1.289(0.257)1.062(0.179)1.148(0.138)1.035(0.156)
NASH 0.596(0.214) 0.651(0.164) 0.752(0.177) 0.711(0.247) 0.917(0.309) 0.921(0.215) 0.819(0.242) 0.839(0.186)
IND 0.239(0.397) 0.236(0.348) 1.127(0.281) 0.942(0.392) 0.476(0.503) 0.497(0.593) 0.386(0.431) 0.492(0.342)
MADDPG 0.528(0.123) 0.124(0.035) 0.68(0.161) -0.064(0.152) -0.016(0.099) -0.019(0.112) 0.345(0.114) -0.022(0.108)
(b) Averagereward.
Table 2: Validation results of different methods for Tasks 1-4. The statistics are ob-
tained using the learned model of ten learning experiments for each task in the corre-
sponding task environment.
Incontrast,independentQ-learningproducesthelargestvariance,indicating
instability during the learning process and difficulty in achieving consistent re-
sults.Moreover,italsoneedsmorelearningstepstocompletethetaskcompared
to Stackelberg DDQQN and Nash Q-learning, and hence, it receives smaller av-
eraged cumulative rewards. This phenomenon is more obvious in complex tasks
such as Tasks 3 and 4 (and Tasks 5-8 in Fig. 11 and Tab. 4). This is mainly
because the independent Q-learning does not consider the partner robot’s in-
teraction in the learning. The existence of partners leads to a non-stationary
environment, causing difficulty in learning effective strategies for collaboration.
Nash Q-learning yields results similar to Stackelberg DDQN but remains
inferior. The smaller average rewards in Fig. 6 and 7 are primarily due to the
interaction pattern in the Nash game, where both robots simultaneously take
actions. This can lead to situations where both robots select the same sub-task,
resulting in negative rewards, particularly evident in complex tasks. Besides,
Nash Q-learning needs considerable training time because every learning step
must compute the Nash equilibrium. For example, in learning Task 2, Nash Q-
learning takes nearly four times the time compared with Stackelberg DDQN
(about 25 min). As the task becomes complex, Nash Q-learning requires even
more time, severely limiting its practicality.
Surprisingly, MADDPG performs poorly across all tasks. Although MAD-
DPG occasionally finds effective strategies for simple tasks like Task 1 (as seen
in completion steps below 40 in Figure 8), it consistently requires more training
stepsperepisodecomparedtotheothermethods,whichshowsitislesseffective
in learning the collaboration plan. This underscores the advantage of game-
theoreticplanning.NotethatrobotsmayreceivenegativerewardsinMADDPG
duetosimultaneousaction-taking.SimilartoNashQ-learning,conflictsbetween
robots can lead to negative rewards.Stackelberg Learning for Collaborative Assembly 19
We further validate the trained model using the corresponding task environ-
ments and summarize the validation results in Tab. 2. Our observation reveals
that Stackelberg DDQN outperforms other methods and shows the highest re-
ward, the lowest completion steps, and the smallest variance.
5.3 Strategy Under Disturbances
Stackelberg DDQN with a probabilistic transition kernel enhances the robust-
ness of the learned collaborative strategy against external disturbances. Using
the learned model, the robots can also complete the task in the deterministic
environment, where all types of sub-tasks are completed with probability 1. We
summarize the completion steps using Stackelberg DDQN for Tasks 1-4 under
deterministic environments in Tab. 3 (normal). The learned model successfully
completed all 40 experiments (4 tasks with 10 experiments each). Compared
with Tab. 2, the completion steps drop because all sub-tasks are guaranteed to
be completed once selected.
Task 1 Task 2 Task 3 Task 4
normal 11.4(0.663) 14.4(0.489) 16.2(0.979) 20.1(1.044)
perturbed 14.0(0.632) 16.9(0.7) 20.0(0.774) 22.9(0.830)
Table 3: Completion steps of Tasks 1-4 in the normal and perturbed assembly envi-
ronments.
Moreover, the learned model is robust enough to handle perturbations be-
yond the probabilistic environment. To demonstrate this, we introduce pertur-
bations (indicated by red arrows in Fig. 9) into the deterministic environment.
Specifically, we perturb the robots to take empty actions at certain time steps
regardless of their planned actions based on the learned models. Perturbations
are applied as follows. For Task 1, we perturb the leader robot at steps 1 and 4;
and the follower robot at steps 6 and 8; for Task 2, we perturb the leader robot
atsteps3and7,andthefollowerrobotatsteps5and10;forTask3,weperturb
the leader robot at steps 1, 2, and 9, and the follower robot at steps 2, 7, and
11; and for Task 4, we perturb the leader robot at steps 9, 14, and 15, and the
follower robot at steps 2 and 6.
Usingthelearnedmodels,boththeleaderandfollowersuccessfullycomplete
allperturbedtasks(4taskswith10experimentseach).Thecompletionstepsare
summarized in Tab. 3 (perturbed). For better visualization, we plot the leader
and follower’s cumulative rewards of one experiment out of 10 for Tasks 1-4 in
Fig.9.Asweobserve,itisnotsurprisingthatthereisaperformance(cumulative
reward) drop when a perturbation occurs. Despite the slight performance loss,
robotscanquicklyadjustthestrategyandfindasequenceofsuboptimalactions
tocontinuethetasktillitscompletionusingthelearnedmodels.Althoughmore
time may be required to complete the task after perturbations, the deviation20 Y. Zhao et al.
in performance is insignificant, indicating that robots can still select effective
actions. Eventually, robots complete the task and achieve similar cumulative
rewards. It demonstrates the robustness and effectiveness of Stackelberg DDQN
in the perturbed environment.
Fig.9: Leader and follower’s cumulative rewards under disturbances (red arrow) for
Tasks1-4.Thetrainedstrategiesallcompletedthetaskswithalongercompletionstep
and a smaller cumulative reward.
6 Conclusion
In this work, we have proposed a Stackelberg dynamic game-theoretic learning
approachtoenableeffectivetaskplanningincollaborativeassemblythatrequires
multi-robot coordination. Our approach seeks optimal assembly schedules for
robots through the lens of game theory, where we model robot collaboration as
astochasticStackelberggametocapturethesequentialnatureofassemblytasks.
The resulting Stackelberg equilibrium strategies guide the robots’ actions and
provide a collaboration plan to improve the efficiency of the assembly process.
Buildingongame-theoreticprinciples,wehavefurtherdevelopedtheStackelberg
double deep Q-learning algorithm to automate robot-level collaboration strat-
egyseekingandaccommodateheterogeneousrobotcollaborationacrosstasksofStackelberg Learning for Collaborative Assembly 21
varying complexity. Simulations have corroborated our approach, demonstrat-
ing its efficacy in generating more effective collaboration plans compared with
three alternative multi-agent learning methods. Besides, our learning approach
also exhibits robustness to both accidental and deliberate perturbations in the
assembly tasks.
Our future research endeavors include extending our learning framework to
broader application scenarios, such as collective transportation, to explore ef-
fective task planning for a wider range of collaborative tasks. Furthermore, the
theoretical performance guarantee for Stackelberg learning would provide valu-
ableinsightsintotherobustnessandreliabilityofourapproach,whichisanother
intriguing avenue for future work.
A Bimatrix Game and Stackelberg Equilibrium
Abimatrixgameisatwo-playergamerepresentedbythetuple⟨UL,UF⟩,where
Ui ∈Rm×n is the utility matrix of the player i, i∈{L,F}. The utility matrices
indicate that player L and F have m and n actions, respectively. The jk-entry
of Ui is the player i’s utility when L plays the action j ∈ {1,...,m} and F
plays the action k ∈ {1,...,n}. We define players’ strategies πL ∈ ∆(Rm) :=
ΠL and πn ∈ ∆(Rn) := ΠF as the probability distributions over Rm and Rn,
respectively.
Definition 2. The strategy pair ⟨πL∗,πF∗⟩ constitutes the Stackelberg equilib-
rium of the bimatrix game ⟨UL,UF⟩ if
(πL∗)TUFπF∗ ≥(πL∗)TULπF, ∀πF ∈ΠF,
(πL∗)TULπF∗ ≥(πL)TULT(πL), ∀πL ∈ΠL,
wherethebestresponsemappingT :ΠL →ΠF isgivenbyT(πL)=argmax (πL)TBy.
y∈ΠF
In the stochastic Stackelberg game (1), the leader and follower’s Q-function
atanystatescanconstituteabimatrixgame⟨QL(s,·),QF(s,·)⟩sinceQi(s,·)∈
R|AL|×|AF|, i∈{L,F}.
B Hyperparameter Settings
We use a probabilistic transition kernel to capture accidents that fail task com-
pletion. A robot has a probability of 0.9 to successfully complete individual
sub-tasks, i.e., a type 1 or type 3 sub-task for the leader robot and a type 2 or
type 3 sub-task for the follower robot. Both robots have a probability of 0.7 to
successfully complete a cooperative (type 4) sub-task due to the joint operation
and its complexity.
In the Stackelberg learning and other comparing algorithms, we use a three-
layerneuralnetworktoapproximatetheleaderandfollowerrobots’Q-functions.
We use the Adam optimizer with the learning rate of 10−4 to conduct training.22 Y. Zhao et al.
The total number of episodes is set by 10k (20k for Task 8); the stages in each
episode vary with tasks indicating their complexity. For Tasks 1-4, we set (40,
50,50,60)stages.ForTasks5-8,weset(60,80,80,150)stages.Thesoftupdate
parameters are set by τ =0.1 and C =50.
C Settings and Training Results of Tasks 5-8
We show the task plot of Tasks 5-8 in Fig. 10, from simple to complex.
Fig.10: Plots of Assembly Tasks 5-8.
Forsimplicity,weonlyplotthetrainingresultsofcompletionstepsforTasks
5-8 in Fig. 11 and summarize the validation results in Tab. 4. Our Stackel-
berg DDQN outperforms other methods by having a lower completion step and
a smaller variance. Besides, it also exhibits a higher average reward for both
the leader and the follower, showing it provides a more effective collaboration
plan for robots. In contrast, Nash Q-learning requires significant training time.
Independent Q-learning exhibits a large variance in training that prevents its
practicalityandconsistency.MADDPGfailstofindmeaningfulstrategiesunder
the stage number settings.
References
1. Arulkumaran,K.,Deisenroth,M.P.,Brundage,M.,Bharath,A.A.:Deepreinforce-
ment learning: A brief survey. IEEE Signal Processing Magazine 34(6), 26–38
(2017)
2. Başar, T., Olsder, G.J.: Dynamic noncooperative game theory. SIAM (1998)
3. Bhatta,K.,Huang,J.,Chang,Q.:Dynamicrobotassignmentforflexibleserialpro-
duction systems. IEEE Robotics and Automation Letters 7(3), 7303–7310 (2022)
4. Blum,A.,Haghtalab,N.,Procaccia,A.D.:Learningoptimalcommitmenttoover-
come insecurity. Advances in Neural Information Processing Systems 27 (2014)
5. Bueno,A.,GodinhoFilho,M.,Frank,A.G.:Smartproductionplanningandcontrol
intheindustry4.0context:Asystematicliteraturereview.Computers&Industrial
Engineering 149, 106774 (2020)Stackelberg Learning for Collaborative Assembly 23
Task 5 Task 6 Task 7 Task 8
SG 47.4(5.004) 60.9(8.780) 68.2(8.885) 78.6(11.182)
NASH 49.0(6.481) 65.0(7.629) 74.4(5.783) 136.2(16.928)
IND 54.7(6.753) 80.0(0.0) 80.0(0.0) 150.0(0.0)
MADDPG 60.0 (0.0) 80.0(0.0) 80.0(0.0) 150.0(0.0)
(a) Completionsteps.
Task5 Task6 Task7 Task8
reward_l reward_f reward_l reward_f reward_l reward_f reward_l reward_f
SG 1.206(0.143)0.880(0.121)1.214(0.206)0.960(0.160)1.078(0.262)0.958(0.247)1.217(0.138)1.141(0.115)
NASH 0.937(0.119) 0.843(0.124) 0.924(0.121) 0.847(0.084) 0.845(0.174) 0.835(0.163) 0.251(0.411) 0.145(0.378)
IND 0.653(0.419) 0.525(0.401) -0.254(0.273) -0.373(0.157) 0.303(0.239) 0.331(0.248) -0.366(0.178) -0.292(0.149)
MADDPG -0.243(0.163) -0.155(0.100) 0.223(0.092) 0.087(0.091) -0.148(0.110) -0.173(0.129) 0.145(0.121) -0.256(0.101)
(b) Averagereward.
Table 4: Validation results of different methods for Task 5-8. Each task is validated
using the learned models of 10 experiments.
Fig.11: Completion steps for Tasks 5-8. The maximum step length per episode is
(60,80,80,150) in corresponding tasks. Stackelberg DDQN always learns an assembly
plan and achieves the lowest task completion step. In contrast, the three alternative
learning methods either require more completion steps (Nash Q-learning) or fail to
learn a feasible collaboration plan (Independent Q-learning and MADDPG) as the
task becomes more complex.24 Y. Zhao et al.
6. Cao,X.,Bo,H.,Liu,Y.,Liu,X.:Effectsofdifferentresource-sharingstrategiesin
cloud manufacturing: A stackelberg game-based approach. International Journal
of Production Research 61(2), 520–540 (2023)
7. Casalino, A., Zanchettin, A.M., Piroddi, L., Rocco, P.: Optimal scheduling of
human–robotcollaborativeassemblyoperationswithtimepetrinets.IEEETrans-
actions on Automation Science and Engineering 18(1), 70–84 (2019)
8. Chen, J., Jia, X., He, Q.: A novel bi-level multi-objective genetic algorithm for
integratedassemblylinebalancingandpartfeedingproblem.InternationalJournal
of Production Research 61(2), 580–603 (2023)
9. Chua,F.L.S.,Vasnani,N.N.,Pacio,L.B.M.,Ocampo,L.A.:Astackelberggamein
multi-periodplanningofmake-to-orderproductionsystemacrossthesupplychain.
Journal of Manufacturing Systems 46, 231–246 (2018)
10. Foerster, J., Assael, I.A., De Freitas, N., Whiteson, S.: Learning to communicate
withdeepmulti-agentreinforcementlearning.Advancesinneuralinformationpro-
cessing systems 29 (2016)
11. Hu,J.,Wellman,M.P.:Nashq-learningforgeneral-sumstochasticgames.Journal
of machine learning research 4(Nov), 1039–1069 (2003)
12. Johnson, D., Chen, G., Lu, Y.: Multi-agent reinforcement learning for real-time
dynamic production scheduling in a robot assembly cell. IEEE Robotics and Au-
tomation Letters 7(3), 7684–7691 (2022)
13. Könönen,V.:Asymmetricmultiagentreinforcementlearning.WebIntelligenceand
Agent Systems: An international journal 2(2), 105–121 (2004)
14. Kunic, A., Kramberger, A., Naboni, R.: Cyber-physical robotic process for re-
configurablewoodarchitecture:Closingthecircularloopinwoodarchitecture.In:
39thInternationalConferenceonEducationandResearchinComputerAidedAr-
chitecturalDesigninEurope,eCAADe2021.pp.181–188.Educationandresearch
in Computer Aided Architectural Design in Europe (2021)
15. Lamon, E., De Franco, A., Peternel, L., Ajoudani, A.: A capability-aware role
allocation approach to industrial assembly tasks. IEEE Robotics and Automation
Letters 4(4), 3378–3385 (2019)
16. Lauffer, N., Ghasemi, M., Hashemi, A., Savas, Y., Topcu, U.: No-regret learning
in dynamic stackelberg games. arXiv preprint arXiv:2202.04786 (2022)
17. Lee, C., Park, L., Cho, S.: Light-weight stackelberg game theoretic demand re-
sponse scheme for massive smart manufacturing systems. IEEE Access 6, 23316–
23324 (2018)
18. Leenders, L., Bahl, B., Hennen, M., Bardow, A.: Coordinating scheduling of pro-
ductionandutilitysystemusingastackelberggame.Energy175,1283–1295(2019)
19. Letchford, J., Conitzer, V., Munagala, K.: Learning and approximating the opti-
mal strategy to commit to. In: Algorithmic Game Theory: Second International
Symposium,SAGT2009,Paphos,Cyprus,October18-20,2009.Proceedings2.pp.
250–262. Springer (2009)
20. Li, C., Zheng, P., Yin, Y., Wang, B., Wang, L.: Deep reinforcement learning in
smart manufacturing: A review and prospects. CIRP Journal of Manufacturing
Science and Technology 40, 75–101 (2023)
21. Li, L., Fu, X., Zhen, H.L., Yuan, M., Wang, J., Lu, J., Tong, X., Zeng, J.,
Schnieders, D.: Bilevel learning for large-scale flexible flow shop scheduling. Com-
puters & Industrial Engineering 168, 108140 (2022)
22. Li,M.,Guo,B.,Zhang,J.,Liu,J.,Liu,S.,Yu,Z.,Li,Z.,Xiang,L.:Decentralized
multi-agv task allocation based on multi-agent reinforcement learning with infor-
mation potential field rewards. In: 2021 IEEE 18th International Conference on
Mobile Ad Hoc and Smart Systems (MASS). pp. 482–489. IEEE (2021)Stackelberg Learning for Collaborative Assembly 25
23. Liu, Y., Ping, Y., Zhang, L., Wang, L., Xu, X.: Scheduling of decentralized robot
services in cloud manufacturing with deep reinforcement learning. Robotics and
Computer-Integrated Manufacturing 80, 102454 (2023)
24. Liu,Y.,Wang,L.,Wang,X.V.,Xu,X.,Zhang,L.:Schedulingincloudmanufactur-
ing: state-of-the-art and research challenges. International Journal of Production
Research 57(15-16), 4854–4879 (2019)
25. Lowe, R., Wu, Y.I., Tamar, A., Harb, J., Pieter Abbeel, O., Mordatch, I.: Multi-
agent actor-critic for mixed cooperative-competitive environments. Advances in
neural information processing systems 30 (2017)
26. Malik,A.A.,Bilberg,A.:Complexity-basedtaskallocationinhuman-robotcollab-
orative assembly. Industrial Robot: the international journal of robotics research
and application (2019)
27. Marecki, J., Tesauro, G., Segal, R.: Playing repeated stackelberg games with un-
known opponents. In: Proceedings of the 11th International Conference on Au-
tonomous Agents and Multiagent Systems-Volume 2. pp. 821–828 (2012)
28. Matignon, L., Laurent, G.J., Le Fort-Piat, N.: Independent reinforcement learn-
ers in cooperative markov games: a survey regarding coordination problems. The
Knowledge Engineering Review 27(1), 1–31 (2012)
29. Meng,L.,Ruan,J.,Xing,D.,Xu,B.:Learninginbi-levelmarkovgames.In:2022
InternationalJointConferenceonNeuralNetworks(IJCNN).pp.1–8.IEEE(2022)
30. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. nature 518(7540), 529–533 (2015)
31. Oroojlooyjadid,A.,Nazari,M.,Snyder,L.V.,Takáč,M.:Adeepq-networkforthe
beergame:Deepreinforcementlearningforinventoryoptimization.Manufacturing
& Service Operations Management 24(1), 285–304 (2022)
32. Ouelhadj, D., Petrovic, S.: A survey of dynamic scheduling in manufacturing sys-
tems. Journal of scheduling 12, 417–431 (2009)
33. Panzer, M., Bender, B.: Deep reinforcement learning in production systems: a
systematicliteraturereview.InternationalJournalofProductionResearch60(13),
4316–4341 (2022)
34. Rashid,T.,Samvelyan,M.,DeWitt,C.S.,Farquhar,G.,Foerster,J.,Whiteson,S.:
Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning.
The Journal of Machine Learning Research 21(1), 7234–7284 (2020)
35. Suárez-Ruiz,F.,Zhou,X.,Pham,Q.C.:Canrobotsassembleanikeachair?Science
Robotics 3(17), eaat6385 (2018)
36. Tereshchuk,V.,Stewart,J.,Bykov,N.,Pedigo,S.,Devasia,S.,Banerjee,A.G.:An
efficientschedulingalgorithmformulti-robottaskallocationinassemblingaircraft
structures. IEEE Robotics and Automation Letters 4(4), 3844–3851 (2019)
37. Van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double q-
learning. In: Proceedings of the AAAI conference on artificial intelligence. vol. 30
(2016)
38. Von Stackelberg, H.: Market structure and equilibrium. Springer Science & Busi-
ness Media (2010)
39. Wang,X.,Zhang,L.,Lin,T.,Zhao,C.,Wang,K.,Chen,Z.:Solvingjobscheduling
problems in a resource preemption environment with multi-agent reinforcement
learning. Robotics and Computer-Integrated Manufacturing 77, 102324 (2022)
40. Yang, D., Jiao, J.R., Ji, Y., Du, G., Helo, P., Valente, A.: Joint optimization
for coordinated configuration of product families and supply chains by a leader-
followerstackelberggame.EuropeanJournalofOperationalResearch246(1),263–
280 (2015)26 Y. Zhao et al.
41. Yoshitake,H.,Kamoshida,R.,Nagashima,Y.:Newautomatedguidedvehiclesys-
tem using real-time holonic scheduling for warehouse picking. IEEE Robotics and
Automation Letters 4(2), 1045–1052 (2019)
42. Yu, T., Huang, J., Chang, Q.: Optimizing task scheduling in human-robot collab-
oration with deep multi-agent reinforcement learning. Journal of Manufacturing
Systems60,487–499(Jul2021).https://doi.org/10.1016/j.jmsy.2021.07.015
43. Zarreh,A.,Saygin,C.,Wan,H.,Lee,Y.,Bracho,A.:Agametheorybasedcyber-
security assessment model for advanced manufacturing systems. Procedia Manu-
facturing 26, 1255–1264 (2018)
44. Zhang, H., Chen, W., Huang, Z., Li, M., Yang, Y., Zhang, W., Wang, J.: Bi-level
actor-critic for multi-agent coordination. In: Proceedings of the AAAI Conference
on Artificial Intelligence. vol. 34, pp. 7325–7332 (2020)
45. Zhao,C.,Kang,N.,Li,J.,Horst,J.A.:Productioncontroltoreducestarvationin
apartiallyflexibleproduction-inventorysystem.IEEETransactionsonAutomatic
Control 63(2), 477–491 (2017)
46. Zhao,G.,Zhu,B.,Jiao,J.,Jordan,M.I.:Onlinelearninginstackelberggameswith
an omniscient follower. arXiv preprint arXiv:2301.11518 (2023)
47. Zhao, Y., Huang, B., Yu, J., Zhu, Q.: Stackelberg strategic guidance for hetero-
geneous robots collaboration. In: 2022 International Conference on Robotics and
Automation (ICRA). pp. 4922–4928. IEEE (2022)
48. Zhou, Y., Peng, Y., Li, W., Pham, D.T.: Stackelberg model-based human-robot
collaboration in removing screws for product remanufacturing. Robotics and
Computer-Integrated Manufacturing 77, 102370 (2022)