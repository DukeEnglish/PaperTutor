Neural Flow Diffusion Models: Learnable Forward Process for Improved
Diffusion Modelling
GrigoryBartosh1 DmitryVetrov2 ChristianA.Naesseth1
Abstract theticdatasets(dataaugmentation)anduncoveringhidden
patternsandstructuresinanunsupervisedlearningsetting.
Conventionaldiffusionmodelstypicallyrelieson
afixedforwardprocess,whichimplicitlydefines Diffusion models (Sohl-Dickstein et al., 2015; Ho et al.,
complex marginal distributions over latent vari- 2020) are a class of generative models are constructed by
ables. Thiscanoftencomplicatethereversepro- twokeyprocesses:theforwardprocessandthereversepro-
cess’taskinlearninggenerativetrajectories,and cess. Theforwardprocessgraduallycorruptsthedatadis-
results in costly inference for diffusion models. tribution,transformingitfromitsoriginalformtoanoised
Toaddresstheselimitations,weintroduceNeural state.Thereverseprocesslearnstoinvertcorruptionsofthe
FlowDiffusionModels(NFDM),anovelframe- forwardprocessandrestorethedatadistribution.Thisway,
workthatenhancesdiffusionmodelsbysupport- itlearnstogeneratedatafrompurenoise.Diffusionmodels
ing a broader range of forward processes be- have demonstrated remarkable results in various domains
yondthefixedlinearGaussian. Wealsopropose (Hoetal.,2022;Sahariaetal.,2022;Tanetal.,2024;Wat-
a novel parameterization technique for learning son et al., 2023; Trippe et al., 2023). Nevertheless, most
the forward process. Our framework provides existingdiffusionmodelsfixtheforwardprocesstobepre-
an end-to-end, simulation-free optimization ob- defined Gaussian, which makes it unable to adapt to the
jective, effectively minimizing a variational up- task at hand or simplify the target for the reverse process.
per bound on the negative log-likelihood. Ex- At the same time there are many works that demonstrate
perimental results demonstrate NFDM’s strong howmodificationsoftheforwardprocessimproveperfor-
performance, evidenced by state-of-the-art like- manceintermsofgenerationquality(Nichol&Dhariwal,
lihood estimation. Furthermore, we investigate 2021;Vahdatetal.,2021;Darasetal.,2022),likelihoodes-
NFDM’scapacityforlearninggenerativedynam- timation(Kingmaetal.,2021;Nielsenetal.,2023;Bartosh
ics with specific characteristics, such as deter- etal.,2023)orsamplingspeed(Leeetal.,2023;Pooladian
ministic straight lines trajectories. This explo- etal.,2023;Tongetal.,2023).
ration underscores NFDM’s versatility and its
In this paper, we present Neural Flow Diffusion Models
potentialforawiderangeofapplications.
(NFDM),aframeworkthatallowsforthepre-specification
andlearningoflatentvariabledistributionsdefinedbythe
forward process. Unlike conventional diffusion models
1.Introduction
(Ho et al., 2020), which rely on a conditional Gaussian
Generativemodelsareaversatileclassofprobabilisticma- forward process, NFDM may accommodate any continu-
chinelearningmodels, whichfindsapplicationsacrossdi- ous(andlearnable)distributionthatcanbeexpressedasan
versefieldssuchasart,music,medicine,andphysics(Tom- invertible mapping applied to noise. We also derive, and
czak, 2022; Creswell et al., 2018; Papamakarios et al., leverage, an end-to-end simulation-free optimization pro-
2021;Yangetal.,2022). Generativemodelsexcelincon- cedure, that minimizes a variational upper bound on the
structingprobabilitydistributionsthataccuratelyrepresent negativelog-likelihood(NLL).
datasets,enablingthemtoproducenewsamplesakintothe
Wealsoproposeanefficientneuralnetwork-basedparame-
original data. Such capabilities make them ideal for tasks
terizationfortheforwardprocess,enablingittoadapttothe
likeenhancingtrainingdatathroughthegenerationofsyn-
reverseprocessduringtrainingandsimplifythelearningof
1University of Amsterdam 2Constructor University, Bre- thedatadistribution. TodemonstrateNFDM’scapabilities
men.Correspondenceto:GrigoryBartosh<g.bartosh@uva.nl>, withalearnableforwardprocessweprovideexperimental
Dmitry Vetrov <dvetrov@constructor.university>, Christian A. resultsonCIFAR-10,ImageNet32and64,attainingstate-
Naesseth<c.a.naesseth@uva.nl>.
of-the-art NLL results, which is crucial for many applica-
tions such as data compression (Ho et al., 2021; Yang &
1
4202
rpA
91
]LM.tats[
1v04921.4042:viXraNeuralFlowDiffusionModels
Mandt,2022),anomalydetection(Chenetal.,2018b;Dias andfollowingthereverseSDE(Anderson,1982):
etal.,2020)andout-of-distributiondetection(Serra` etal.,
dz =f˜B(z ,t)dt+g(t)dw¯, where (2)
2019;Xiaoetal.,2020). t t
f˜B(z ,t)=f˜F(z ,t)−g2(t)∇ logq (z ). (3)
Leveraging the flexibility of NFDM, we further explore t t zt φ t
training with constraints on the reverse process to learn
Here, w¯ denotes a standard Wiener process where time
generative dynamics with specific properties. As a case
flows backwards. Diffusion models approximate this re-
study, we discuss a curvature penalty on the determinis-
verseprocessbylearning∇ logq(z ),knownasthescore
tic generative trajectories. Our empirical results indicate
zt t
function,throughaλ -weightedscorematchingloss:
t
improved computational efficiency compared to baselines
o dn ows ny sn at mhe pti lc edd Ia mta as ge ets Na es t.well as MNIST, CIFAR-10, and u(t)E q(zt)(cid:104) λ t(cid:13) (cid:13)s θ(z t,t)−∇ ztlogq φ(z t)(cid:13) (cid:13)2 2(cid:105) , (4)
Wesummarizeourcontributionsasfollows: where u(t) represents a uniform distribution over the in-
terval [0,1], and s : RD × [0,1] (cid:55)→ RD is a learnable
θ
approximation. This loss can be reformulated as a more
1. We introduce Neural Flow Diffusion Models
tractabledenoisingscorematchinglossVincent(2011):
(NFDM), improving diffusion modelling through a
learnableforwardprocess. u(t)qE (x,zt)(cid:104) λ t(cid:13) (cid:13)s θ(z t,t)−∇ ztlogq φ(z t|x)(cid:13) (cid:13)2 2(cid:105) . (5)
2. We develope an end-to-end optimization procedure
With a learned score function s (z ,t), one can generate
that minimizes an upper bound on the negative log- θ t
a sample from the reverse process by first sampling from
likelihoodinasimulation-freemanner.
thepriorz ∼N(z ;0,I),andthensimulatingthereverse
1 1
3. Wedemonstratestate-of-the-artlog-likelihoodresults SDE:
onCIFAR-10,ImageNet32and64. dz =(cid:2) f˜F(z ,t)−g2(t)s (z ,t)(cid:3) dt+g(t)dw¯, (6)
t t θ t
4. WeshowhowNFDMcanbeusedinlearninggenera- resultingasamplez ∼p (z )≈q(z )≈q(x).
0 θ 0 0
tiveprocesseswithspecificproperties,exemplifiedby
Diffusionmodelspossessseveralimportantproperties. For
dynamicswithstraightlinetrajectories,whereNFDM
example, for specific λ , the objective (eq. (5)) can be
leads to significantly faster sampling speeds and en- t
reformulated (Song et al., 2021) as an Evidence Lower
hancedgenerationqualitywithfewersamplingsteps.
Bound (ELBO) on the model’s likelihood. Furthermore,
theminimizationofdenoisingscorematching(eq.(5))isa
2.Background
simulation-free procedure. This means that simulating ei-
thertheforwardorreverseprocessesthroughitsSDEisnot
Diffusionmodelsaregenerativelatentvariablemodelscon-
necessaryforsamplingz ,norisitnecessaryforestimating
sisting of two processes: the forward and the reverse (or t
the gradient of the loss function. Instead, we can directly
generative)process.Theforwardprocessisadynamicpro-
cessthattakesadatapointx∼q(x),x∈RD,andperturbs sample z t ∼ q(z t|x). The simulation-free nature of this
approachisacrucialaspectforefficientoptimization.
itovertimebyinjectingnoise. Thisgeneratesatrajectory
oflatentvariables{z(t)} t∈[0,1],conditionalonthedatax, Another notable property is the existence of an Ordinary
where [0,1] is a fixed time horizon and z t = z(t) ∈ RD. Differential Equation (ODE) corresponding to the same
The(conditional)distributioncanbedescribedbyaninitial marginaldensitiesq(z )astheSDE(eq.(1)):
t
distributionq(z |x)andaStochasticDifferentialEquation
0
(SDE) with a linear drift term f˜F(z t,t) : RD ×[0,1] (cid:55)→ dz t =f(z t,t)dt, where (7)
RD, scalar variance g(t) : [0,1] (cid:55)→ R +, and a standard
f(z ,t)=f˜F(z ,t)−
g2(t)
∇ logq (z ). (8)
Wienerprocessw: t t 2 zt φ t
dz t =f˜F(z t,t)dt+g(t)dw. (1) Thisimpliesthatwecansamplefromdiffusionmodelsde-
terministically,allowingtheuseofoff-the-shelfnumerical
Due to the linearity of f˜F, we can reconstruct the condi- ODE solvers for sampling, which may improve the sam-
tional marginal distribution q(z |x) = N(z ;α x,σ2I). plingspeedcomparedtostochasticsamplingthatrequires
t t t t
Typically, the conditional distributions evolve from some simulating an SDE. Additionally, deterministic sampling
low variance distribution q(z |x) ≈ δ(z − x) to a unit enables us to compute densities by treating the model as
0 0
Gaussian q(z |x) ≈ N(z ;0,I). This forward process is a continuous normalizing flow, as detailed in Chen et al.
1 1
thenreversedbystartingfromthepriorz ∼ N(z ;0,I), (2018a);Grathwohletal.(2018).
1 1
2NeuralFlowDiffusionModels
3.NeuralFlowDiffusionModels specificvaluesofxandεandvaryingtfrom0to1results
inasmoothtrajectoryfromz toz . Differentiatingthese
0 1
Diffusion models can be viewed as a specific type of hi-
trajectoriesovertimeyieldsavelocityfieldcorresponding
erarchical Variational Autoencoders (VAEs) (Kingma &
totheconditionaldistributionq (z |x),therebydefininga
φ t
Welling,2013;Rezendeetal.,2014),wherethelatentvari-
conditionalODE:
ablesareinferredthroughapre-specifiedscalingofthedata
point and injection of Gaussian noise. However, this for- dz =f (z ,t,x)dt, where (10)
t φ t
mulationlimitsdiffusionmodelsintermsoftheflexibility (cid:12)
oftheirlatentspace,andmakeslearningofthereversepro- f φ(z t,t,x)=
∂F φ( ∂ε t,t,x)(cid:12)
(cid:12)
(cid:12)
. (11)
cessmorechallenging. Toaddressthislimitation,wepro- ε=Fφ−1(zt,t,x)
poseageneralizedformofdataandnoisetransformations
that enables the definition and learning of a broad range Therefore,ifwesamplez ∼q (z |x)andsolvetheODE
0 φ 0
ofdistributionsinthelatentspacebeyondtheconventional ineq.(10)untiltimet,wehaveasamplez ∼q (z |x).
t φ t
Gaussian.
The time derivative of F may be calculated using au-
φ
Inthissection, weintroduceNeuralFlowDiffusionMod- tomatic differentiation tools like PyTorch (Paszke et al.,
els (NFDM) – a framework that generalizes conventional 2017) or JAX (Bradbury et al., 2018). Specifically, we
diffusion models. The key idea in NFDM is to define the can compute a Jacobian Vector Product (JVP) with a unit
forward process implicitly via a learnable transformation one-dimensionalvector. Unlikestandardbackpropagation,
F φ(ε,t,x). Thisletstheuserdefineabroadrangeofcon- which corresponds to a Vector Jacobian Product (VJP),
tinuous time and data dependent forward process. Impor- JVP does not necessitate O(D) backpropagation passes,
tantly,NFDMretainscrucialpropertiesofconventionaldif- enhancingscalability.
fusion models, like likelihood-based and simulation-free
ConditionalSDE.ThefunctionF andthedistributionof
training.Previousdiffusionmodelsemergeasspecialcases φ
the noise q(ε) together defines q (z |x). To completely
when the data transformation is linear, time-independent, φ 0
define the distribution of trajectories {z(t)} , we in-
and/oradditiveGaussian.TheconnectionsbetweenNFDM t∈[0,1]
troduce a conditional SDE that starts from sample z and
andexistingmodelsarediscussedinSection6. 0
runsforwardintime.
3.1.Forwardprocess With access to both the ODE and score function
∇ logq (z |x),theSDE(Songetal.,2020b)correspond-
Theforwardprocessdefinesatargetforthereverseprocess.
zt φ t
ingtothedistributionq (z |x),is:
It describes the distribution of trajectories {z(t)} in φ t
t∈[0,1]
thelatentspacethatthereverseprocesstriestomatch. dz =f˜F(z ,t,x)dt+g (t)dw, where (12)
t φ t φ
Weapproachtheforwardprocessconstructively. First,we g2(t)
defineaconditionalmarginaldistributionq φ(z t|x)fort ∈ f˜ φF(z t,t,x)=f φ(z t,t,x)+ φ
2
∇ ztlogq φ(z t|x).
[0,1]. Then, we introduce the corresponding conditional
ODE that together with the initial distribution q (z |x) Here, g : [0,1] (cid:55)→ R is a scalar function, and w
φ 0 φ +
matchestheconditionalmarginaldistributionq (z |x).Fi- represents a standard Wiener process. It is important to
φ t
nally, we define a conditional SDE that implicitly defines note,thatg onlyinfluencesthedistributionoftrajectories
φ
distribution over trajectories {z(t)} , with the same {z(t)} , not the marginal distributions. The Condi-
t∈[0,1] t∈[0,1]
marginaldistributionsq (z |x). tional SDE in eq. (12) has the same conditional marginal
φ t
distribution,q (z |x),foranyg .
Forward Marginal Distribution. We characterize the φ t φ
marginal distribution q (z |x) of the forward process Thescorefunctionofq (z |x)is:
φ t φ t
troughafunctionofinjectednoiseε,timet,andthedatax:
z =F (ε,t,x), (9)
∇ ztlogq φ(z t|x)=∇
zt(cid:2) logq(ε)+log(cid:12)
(cid:12)J
F−1(cid:12) (cid:12)(cid:3)
(13)
t φ ∂F−1(z ,t,x)
where F
φ
: RD ×[0,1]×RD (cid:55)→ RD and ε ∼ q(ε) = ε=F φ−1(z t,t,x) and J F−1 = φ ∂zt
t
. (14)
N(ε;0,I). Thisfunctiontransformsasamplefromadis-
tribution q(ε) into z , conditional on the data point x and
t While the log-determinant of the Jacobian matrix J−1 is
timestept,implicitlydefiningtheconditionaldistribution F
not generally available in explicit form, it can be analyt-
oflatentvariablesq (z |x).Additionally,eq.(9)facilitates
φ t ically evaluated for certain transformations or specific ar-
directandefficientsamplingfromq (z |x)throughF .
φ t φ chitecturesofthefunctionF ,likeRealNVPstylearchitec-
φ
ConditionalODE.AssumingthatF isdifferentiablewith tures (Dinh et al., 2016; Kingma & Dhariwal, 2018). The
φ
respect to ε and t and invertible with respect to ε, fixing parameterizationofF isfurtherdiscussedinSection3.5.
φ
3NeuralFlowDiffusionModels
Given access to the log probability logq(ε) and the log- Algorithm1OptimizationofNFDM
determinantofJ−1,automaticdifferentiationtoolsenable
F Require: q(x),F φ,xˆ θ
the calculation of gradients and the score function. Un-
forlearningiterationsdo
likethetimederivative,regularbackpropagationcanbeuti-
x∼q(x),t∼u(t),{z ,z ,z }∼q(·|x)
0 t 1
lized here, ensuring computationally efficient score func-
L =−logp(x|z )
rec 0
tionevaluation. L
diff
= 2g21 (t)(cid:13) (cid:13)f˜ φB(z t,t,x)−fˆ θ,φ(z t,t)(cid:13) (cid:13)2
2
φ
L =logq (z |x)−logp(z )
3.2.Reverseprocess prior φ 1 1
Gradientsteponθandφw.r.t. L +L +L
rec diff prior
Todefinethereverse(generative)process,wespecifyare- endfor
verseSDEthatstartsfromz ∼p(z )andrunsbackwards
1 1
in time. To do so we first introduce a conditional reverse
Algorithm2StochasticsamplingfromNFDM
SDEthatreversestheconditionalforwardSDE(eq.(12)).
Following(Anderson,1982),wedefine: Require: F φ,xˆ θ,T –numberofsteps
∆t= 1,z ∼p(z )
T 1 1
dz =f˜B(z ,t,x)dt+g (t)dw, where (15) fort=1,..., 2, 1 do
t φ t φ T T
w¯ ∼N(0,I)
g2(t) √
f˜ φB(z t,t,x)=f φ(z t,t,x)− φ
2
∇ ztlogq φ(z t|x). z
t−∆t
=z t−fˆ θ,φ(z t,t)∆t+g φ(t)w¯ ∆t
endfor
x∼p(x|z )
0
Secondly,wereferencethereverseSDE(seeeq.(15))and
incorporatingthepredictionofx:
dz =fˆ (z ,t)dt+g (t)dw¯, where (16) AswedemonstrateinAppendixA.1thisobjectivesprovide
t θ,φ t φ
fˆ (z ,t)=f˜B(cid:0) z ,t,xˆ (z ,t)(cid:1) , (17) avariationalboundonthemodel’slikelihoodp θ,φ(x).
θ,φ t φ t θ t
TheobjectiveLsharessimilaritieswiththestandarddiffu-
and xˆ : RD × [0,1] (cid:55)→ RD is a function that predicts sionmodelobjective(Hoetal.,2020),thoughtheindivid-
θ
the data point x. This SDE defines the dynamics of the ualtermsdiffer.Moreover,iftheforwardprocessisparam-
generativetrajectories{z(t)} . eterizedbyφ,weneedtooptimizetheobjective(eq.(18))
t∈[0,1]
with respect to these parameters. The objective L also
However, to fully specify the reverse process, it is neces-
exhibits strong connections with both the Flow Matching
sarytodefineapriordistributionp(z )andareconstruction
1 (Lipmanetal.,2022)andScoreMatching(Vincent,2011)
distribution p(x|z ). In all of our experiments, we set the
0 objectives. WeexploretheseconnectionsinAppendixB.2,
prior p(z ) to be a unit Gaussian distribution N(z ;0,I)
1 1 wherewealsodiscusstheroleofg .
andletp(x|z )beaGaussiandistributionwithasmallvari- φ
0
anceN(x;z ,δ2I),whereδ2 =10−4. AkeycharacteristicoftheNFDMobjectiveisitsamenabil-
0
itytotrainingwithinasimulation-freeparadigm,whichis
This particular parameterization of the reverse process is
criticalforefficientoptimization. Wesummarizethetrain-
nottheonlypossibility. However,itisaconvenientchoice
ingprocedureinAlgorithm1.
asitallowsforthedefinitionofthereverseprocesssimply
throughthepredictionofx,akintoconventionaldiffusion
3.4.Sampling
models(Hoetal.,2020).
NFDM offers several methods for sampling from the
3.3.Optimization trainedreverseprocess. Thefirstmethodisstochasticsam-
pling, as defined in Section 3.2. This process involves
Weproposetooptimizetheforwardandreverseprocesses
the following steps: 1) sample z ∼ p(z ); 2) numer-
ofNFDMjointly,minimizingthefollowingobjective: 1 1
ically simulate the SDE (eq. (16)) to obtain z ; 3) sam-
0
ple x ∼ p(x|z ). This procedure is summarized in Algo-
L=L +L +L , (18) 0
rec diff prior rithm2.
L = E [−logp(x|z )], (19)
rec 0
qφ(x,z0) Moreover, during the sampling process, we can adjust the
L
diff
= u(t)qφE (x,zt)(cid:20)
2g
φ21 (t)(cid:13) (cid:13)f˜ φB(z t,t,x)−fˆ θ,φ(z t,t)(cid:13) (cid:13)2 2(cid:21) , l ie sv ie ml po of rts at no tch toas nti oc ti ety thb ay
t
cm ho ad ni gf ey sin tg og gφ φ( (t t)
)
( ae lq so. ( i1 n2 fl) u) e. ncI et
(20) f˜ φF (eq.(12))andfˆ θ,φ(eq.(17)).Intheextremecasewhere
L = E (cid:2) D (cid:0) q (z |x)∥p(z )(cid:1)(cid:3) . (21) g φ(t) ≡ 0, the reverse process becomes deterministic, al-
prior KL φ 1 1 lowing us to utilize off-the-shelf numerical ODE solvers
q(x)
4NeuralFlowDiffusionModels
and to estimate densities (Chen et al., 2018a; Grathwohl ingspecificpairsofprocessesdependingonthetask.
etal.,2018).
Suppose our objective is not just to learn any generative
process for the data, but to learn one endowed with spe-
3.5.Parameterization
cificbeneficialproperties. Forinstance,onesuchproperty
The parameterization of the variance function is straight generative ODE trajectories. Learning a gener-
g (t) (eq. (12)) and the data point predictor ative process with straight ODE trajectories can be highly
φ
xˆ (z ,t) (eq. (17)) is straightforward. In our exper- beneficial,asitenablesthegenerationofsampleswithfar
θ t
iments, we parameterize them directly using neural fewerstepsintheODEsolver. Thiscanleadtosignificant
networks. However, the parameterization of the transfor- computationalsavings.
mationF (eq.(9))ismorecomplex. NFDMnecessitates
φ Toachievethis,wemustintroducesomerestrictionsonthe
thatF bedifferentiableandinvertiblewithrespecttoε,as
φ reverse process. One approach is to directly encode this
wellasprovideaccesstothelogarithmofthedeterminant
in the definition of the reverse process. Alternatively, we
of the transformation log|J−1| (eq. (14)). Therefore, we
F canintroducepenaltiesonthecurvatureofitstrajectories.
usethefollowingparameterizationforF :
φ In both cases learnable forward process would then adapt
to align with the reverse process. As a result, we would
F (ε,t,x)=µ (x,t)+εσ (x,t), where (22)
φ φ φ
obtain a generative process that not only corresponds to
µ (x,t)=(1−t)x+t(1−t)µ¯ (x,t), (23)
φ φ theforwardprocess,ensuringaccuratedatageneration,but
(cid:0) (cid:1)
σ (x,t)=δ+tσ¯ (1−t)x,t /σ¯ (0,1). (24) also features the desired property of straight trajectories.
φ φ φ
WediscussNFDMwithrestrictionsinmoredetailsinAp-
In this context, µ¯ φ : RD × [0,1] (cid:55)→ RD and σ¯ φ : pendixA.2.
RD ×[0,1] (cid:55)→ RD, with δ being a constant. In our ex-
+
periments, we set δ2 = 10−4. If µ¯ and σ¯ are differ- Importantly, conventional diffusion models are incapable
φ φ
of handling such constraints or penalization strategies. In
entiable, this parameterization meets the requirements on
diffusionmodelswithafixedforwardprocess,thetargetfor
F andrendersq(z |x)aconditionalGaussiandistribution
φ t
thereverseprocessispredeterminedandthecorresponding
withparameterizedmeananddiagonalcovariancematrix.
ODEtrajectoriesarehighlycurved. Hence,imposingcon-
Furthermore, this reparameterization ensures that straints on the reverse process, such as trajectory straight-
q(z 0|x) = N(z 0;x,δ2I) and q(z 1|x) = N(z 1;0,I). ness, would lead to a mismatch with the forward process
These restrictions are not necessary for F φ, however, it and, consequently, an inability to generate samples with
eliminates the need to optimize the reconstruction loss highdatafidelity.
L (eq.(19))andthepriorlossL (eq.(21)), asthey
rec prior
areindependentoftheparametersφandθ.
4.1.NFDMwithoptimaltransport
ThechosenparameterizationofF ,g (t),andxˆ (z ,t)is
φ φ θ t In this section, we discuss curvature penalization on the
nottheonlypossibleapproachandmaynotbeoptimal.For
ODEtrajectoriesofthereverseprocessinNFDM.Werefer
instance, itispossibletoparameterizeF φ asaflow-based tothisvariantasNFDM-OT1.
model(Dinhetal.,2016;Kingma&Dhariwal,2018),en-
ablingabroaderrangeofdistributionsthanjustconditional Weproposelearningamodelwithanadditionalcurvxature
Gaussians. However, for the purpose of this paper, we penaltyassuggestedbyKellyetal.(2020):
maintain a simple setup and leave the exploration of the
L =L+λL , where (25)
mosteffectiveparameterizationforfutureresearch. OT crv
(cid:13) (cid:13)2
(cid:13)dfˆ (z ,t)(cid:13)
We discuss the parameterization in more detail in Ap- L = E (cid:13) θ,φ t (cid:13) . (26)
pendixC.2. crv u(t)qφ(x,zt)(cid:13) (cid:13) dt (cid:13) (cid:13)
2
L isanadditionalcurvaturelossthatpenalizesthesec-
4.RestrictedNFDM crv
ondtimederivativeofthegenerativeODEtrajectories.Lis
estimatedasinsection3.3,whereaswhencalculatingL
WeintroduceNFDMasapowerfulframeworkthatenables crv
we set g (t) ≡ 0. L ≡ 0 ensures that the generative
bothpre-definitionandlearningoftheforwardprocess,po- φ crv
trajectories are straight lines. Although we parameterize
tentiallysimplifyingthetaskforthereverseprocess. How-
fˆ (eq. (17)) through f˜B (eq. (15)), L does not pe-
ever,thereareingeneralaninfinitenumberofforwardand θ,φ φ crv
nalizethecurvatureoftheconditionalforwardtrajectories.
reverseprocessesthatcorrespondtoeachother. Sohowto
choosebetweenthem? ThestandardNFDM,asdescribed
1OTstandsforoptimaltransport. Thisdesignationisusedfor
in Section 3, learns one such pair of processes. However, convenience.Whilestraighttrajectoriesareanecessarycondition
theflexibilityofNFDMopensupthepossibilityofchoos- fordynamicoptimaltransport,theyarenotsufficient.
5NeuralFlowDiffusionModels
Table1.ComparisonofNFDMresultswithbaselinesondensityestimationtasks. WepresentresultsintermsofBPD,lowerisbetter.
NFDMachievesstate-of-the-artresultsaccrossthethreebenchmarktasks.
Model CIFAR10 ImageNet32 ImageNet64
DDPM(Hoetal.,2020) 3.69
ScoreSDE(Songetal.,2020b) 2.99
ImprovedDDPM(Nichol&Dhariwal,2021) 2.94 3.54
VDM(Kingmaetal.,2021) 2.65 3.72 3.40
ScoreFlow(Songetal.,2021) 2.83 3.76
FlowMatching(Lipmanetal.,2022) 2.99 3.53 3.31
StochasticInterp. (Albergo&Vanden-Eijnden,2022) 2.99 3.48
i-DODE(Zhengetal.,2023b) 2.56 3.43
MuLAN(Sahooetal.,2023) 2.55 3.67
NFDM(thispaper) 2.49 3.36 3.20
ThepenaltyrequiresF tobetwicedifferentiablewithre-
φ
specttotime. Thespecificsofthecurvaturelossareelabo-
rateduponinmoredetailinAppendixA.2.
Even though the parameterization, as discussed in sec-
tion3.5,issimpleanddoesnotguaranteereachingthezero
curvature, we find that in practice it is flexible enough to
successfully learn straight line generative trajectories. In
our experiments, we set λ = 10−2. Empirical evidence (a)ScoreSDE(Songetal.,2020b)
suggeststhathighervaluesofλleadtoslowerconvergence
ofthemodel. WeprovideempiricalresultsforNFDM-OT
inSection5.2.
5.Experiments
We first showcase results demonstrating that NFDM con-
sistentlyachievesbetterlikelihoodcomparedtobaselines, (b)NFDM(thispaper)
obtainingstate-of-the-artdiffusionmodelingresultsonthe
CIFAR-10anddownsampledImageNetdatasets. Then,we Figure1.Comparisonoftrajectoriesbetweenthedatadistribution
explore the NFDM-OT modification, which penalizes the (ontheleft)andthepriordistribution(ontheright).
curvature of the deterministic generative trajectories. The
NFDM-OTreducestrajectorycurvature,significantlylow-
eringthenumberofgenerativestepsrequiredforsampling.
5.1.Likelihoodestimation
We evaluate NFDM on synthetic data, CIFAR-10
We have trained NFDM on the CIFAR-10 and downsam-
(Krizhevskyetal.,2009),andtwodownsampledImageNet
pledImageNetdatasets. TheNLLresultsaresummarized
(Dengetal.,2009;VanDenOordetal.,2016)datasets.For
in Table 1. Notably, NFDM outperforms diffusion-based
parameterization we use multi-layer perceptrons for syn-
baselinesonallthreedatasets. Weconsiderthisenhanced
thetic data and the U-Net architecture from Dhariwal &
performanceoftheNFDManaturalprogressionduetothe
Nichol(2021)forimages.
followingreasons.
WereporttheNLLinBitsPerDimension(BPD)andsam-
First,itiswell-established(Songetal.,2021;Zhengetal.,
plequalitymeasuredbytheFIDscore(Heuseletal.,2017).
2023b) that diffusion models exhibit improved likelihood
The NLL is calculated by integrating the ODEs using the
estimationwhentrainedwiththefullELBOobjective. As
RK45solverDormand&Prince(1980),withallNLLmet-
delineated in Section 3.3, the objective L (eq. (18)) used
rics computed on test data. For the FID, we provide the
for training the NFDM constitutes a variational bound on
averageover50kimages.
thelikelihood.
For a detailed description of parameterizations and other
Second,aspreviouslymentioned,diffusionmodelscanbe
experimentaldetails,pleaserefertoAppendixC.
seenashierarchicalVAEs. Fromthisperspective,thebase-
6NeuralFlowDiffusionModels
Table2.SummaryofFIDresultsforfew-stepgeneration. Thetableisdividedintothreesections,basedondifferenttypesofmethods:
thosethatdonotminimizecurvature, solversforpretrainedmodels, andmodelsthatspecificallyaimtominimizecurvature. Forthe
DDPM,weincluderesultscorrespondingtotwodistinctobjectives:thefullELBO-basedobjectiveandasimplifiedobjective(L ).
simple
NFDM-OToutperformsbaselineswithcomparableNFEvalues.
CIFAR-10 ImageNet32 ImageNet64
Model NFE↓ FID↓ NFE↓ FID↓ NFE↓ FID↓
DDPM(L )(Hoetal.,2020) 1000 3.17
simple
DDPM(ELBO)(Hoetal.,2020) 1000 13.51
FlowMatching(Lipmanetal.,2022) 142 6.35 122 5.02 138 14.14
DDIM(Songetal.,2020a) 10 13.36
12 5.28
DPMSolver(Luetal.,2022)
24 2.75
TrajectoryCurvatureMinimization(Leeetal.,2023) 5 18.74
4 17.28 4 38.45
MultisampleFlowMatching(Pooladianetal.,2023)
12 7.18 12 17.6
2 12.44 2 9.83 2 27.70
NFDM-OT(thispaper) 4 7.76 4 6.13 4 17.28
12 5.20 12 4.11 12 11.58
lines in Table 1 resemble VAEs with either fixed or con- objective (eq. (18)), which is known to yield higher FID
strained variational distributions. In contrast, the NFDM scoresfordiffusionmodels(Songetal.,2021;Zhengetal.,
extends beyond these baselines by providing a more flex- 2023b). In contrast, some of the approaches listed in Ta-
ible variational distribution. This flexibility allows the ble 2 are trained with different objectives, leading to im-
NFDM to better conform to the reverse process, conse- proved FID scores. Even so, for comparable NFE values
quentlyenhancinglikelihoodestimation. NFDM-OTstillachievessuperiorresults.
TheprimaryaimoftheexperimentswithNFDM-OTisnot
5.2.Straighttrajectories
tointroduceanovelmodelthatsurpassesothersinfew-step
We next evaluate NFDM-OT, which is designed to pe- generation,butrathertoshowcaseNFDM’sabilitytolearn
nalize the curvature of deterministic generative trajecto- generativedynamicswithspecificproperties. Thestraight-
ries. Initially, we compare NFDM-OT with a conven- ness of the trajectories is just one example of such prop-
tional continuous-time diffusion model. Figure 1a illus- erties. We leave it for future research to explore different
tratesdeterministictrajectoriesbetweenatwo-dimensional parameterizationsandamodifiedobjectivesforNFDMthat
datadistributionandaunitGaussiandistributionlearntby mayyieldevenbetterresults.
a conventional diffusion model (Song et al., 2020b). Fig-
ure 1b depicts trajectories learnt by NFDM-OT. Conven- 6.Relatedwork
tional diffusion, being constrained in its forward process,
learnshighlycurvedtrajectories,whereasNFDM-OTsuc- Diffusion models, originally proposed by Sohl-Dickstein
cessfullylearnsstraightgenerativetrajectoriesasdesired. etal.(2015),haveevolvedsignificantlythroughsubsequent
developments (Song & Ermon, 2019; Ho et al., 2020).
Subsequently, wepresentevaluationresultsofNFDM-OT
These advancements have resulted in remarkable genera-
on image datasets. Table 2 reports the FID scores for 2,
tivequalityforhigh-dimensionaldatadistributions(Dhari-
4, and 12 Number of Function Evaluations (NFE) with
wal & Nichol, 2021; Saharia et al., 2022). Nevertheless,
respect to the function fˆ (eq. (16)). In this experi-
θ,φ conventional diffusion models, typically relying on a lin-
ment,weemployEuler’smethodforsamplingintegration.
earGaussianforwardprocess, maynotoptimallyfitsome
ForthespecifiedNFEs,NFDM-OTdemonstratessuperior
datadistributions. Toaddressthis,alternativeforwardpro-
sample quality compared to other approaches with simi-
cesses have been explored, such as combining blurring
lar NFE values. Specifically, NFDM-OT outperforms ap-
withGaussiannoiseinjection(Rissanenetal.,2022;Daras
proaches that minimize the curvature of generative trajec-
et al., 2022; Hoogeboom & Salimans, 2022), diffusion in
tories(Pooladianetal.,2023;Leeetal.,2023).
the wavelet spectrum (Phung et al., 2023), and forward
Importantly, NFDM-OT is trained with an ELBO-based processesbasedontheexponentialfamily(Okhotinetal.,
2023).Thesemodelsarelimitedbytheirfixedforwardpro-
7NeuralFlowDiffusionModels
cessesandmaybeseenasspecificinstancesofNFDM. izescurvatureofgenerativetrajectories.
Some studies have focused on making the forward pro- Lee et al. (2023) suggested learning the forward process
cess learnable. Approaches include a learnable noise in- distribution q(z |x) and interpolating linearly between x
1
jectionschedule(Kingmaetal.,2021;Nichol&Dhariwal, andz . Similarly,Shauletal.(2023a)aimedtomodifythe
1
2021; Sahoo et al., 2023) and learning data transforma- forwardprocesstoreducecurvatureinthegenerativepro-
tionsliketime-independenttransformationsbasedonVAEs cess. Bothapproachescanbeviewedasspecificinstances
(Vahdat etal., 2021;Rombach etal., 2022)and normaliz- ofNFDM.
ingflows(Kimetal.,2022),ortime-dependenttransforma-
Lastly,Albergoetal.(2023)suggestedoptimizingthefor-
tions (Gu et al., 2022; Nielsen et al., 2023; Bartosh et al.,
ward process simultaneously to training the reverse pro-
2023). Thesemethodscanalsobeconsideredspecialcases
cess, resultin in a challenging high dimentional min-max
ofNFDMwithspecifictransformationsF (eq.(9)).
φ problem.Moreover,onlythetheoreticalalgorithmwaspre-
Recent studies (Zhang & Chen, 2021; De Bortoli et al., sented,withoutempiricalstudyanddescriptionsofwaysto
2021; Shi et al., 2023) have explored generative models parameterizesuchalearnableforwardprocess.
based on Schro¨dinger Bridge theory and finite-time dif-
WecontinuediscussingrelatedworksinAppendixB.
fusion constructions. While these models offer novel ap-
proaches to learning forward transformations, in contrast
to NFDM, they depart from the simulation-free paradigm 7.DiscussionandLimitations
byrequiringthefullsimulationofthestochasticprocesses
In this paper we introduced NFDM, a novel simulation-
fortraining,increasingtheircomputationalcost.
free framework for improved diffusion modeling through
Despite the simulation-free nature of the training proce- a learnable forward processes. Our approach outperforms
dure, diffusion models still necessitate full reverse pro- baseline diffusion models on standard benchmarks, show-
cesssimulationsforsamplegeneration,leadingtoslowand casingtheeffectivenessofNFDMinlog-likelihoodestima-
computationally expensive inference. To address this, in tion.
an orthogonal line of works alternative sampling methods
We introduce NFDM not merely as a specific model, but
have been studied, such as deterministic sampling in dis-
rather as a versatile framework that facilitates the prede-
crete(Songetal.,2020a)andcontinuoustime(Songetal.,
fined specification and learning of the forward process.
2020b)andnovelnumericalsolvers(Tachibanaetal.,2021;
For the sake of simplicity in this work, we have chosen a
Liuetal.,2022a;Luetal.,2022;Shauletal.,2023b). Ad-
straightforwardGaussianparameterizationfortheforward
ditionally,distillationtechniqueshavebeenappliedtoboth
process (see Section 3.5). However, NFDM is capable
discrete(Salimans&Ho,2022)andcontinuoustimemod-
of supporting far more flexible parameterizations beyond
els(Songetal.,2023;Zhengetal.,2023a;Yinetal.,2023)
standardGaussiandistributions.
toenhancesamplingspeed,albeitatthecostoftrainingad-
ditional models. Most of these approches are compatible Similarly,thepenaltyonthecurvatureofdeterministictra-
withNFDMandmaybecombinedforfurthergains. jectories discussed in Section 4.1 represents just one spe-
cificexampleoftherestrictionsthatNFDMcanaccommo-
The significance of deterministic trajectories in efficient
date. NFDM opens up significant prospects for exploring
sampling was highlighted by Karras et al. (2022). At the
newgenerativedynamics.
same time, building on diffusion model concepts, Lipman
etal.(2022);Liuetal.(2022b);Albergo&Vanden-Eijnden Nonetheless, the advantages of NFDM come with certain
(2022)introducedsimulation-freemethodsforlearningde- trade-offs. Once the forward process is parameterized us-
terministicgenerativedynamics. Basedontheseideas,Liu inganeuralnetwork,thisleadstoincreasedcomputational
et al. (2022b) and Liu (2022) proposed distillation pro- costs compared to conventional diffusion models. In our
cedures to straighten deterministic generative trajectories. experiments,anoptimizationiterationofNFDMtakesap-
Sincethesemethodsarebasedondistillation,weconsider proximately2.2timeslongerthanthatofconventionaldif-
themorthogonaltoNFDM. fusionmodels.
Pooladian et al. (2023) and Tong et al. (2023) proposed Despite these challenges, we are optimistic about the po-
to construct the forward process with optimal data-noise tentialofNFDMinvariousdomainsandpracticalapplica-
couplingstolearnstraightergenerativetrajectories. While tions,givenitsflexibilityinlearninggenerativeprocesses.
thismethodistheoreticallyjustified,itreliesonestimating We also believe that with alternative parameterizations,
anoptimalcouplingoverminibatchesoftheentiredataset, modifications of the objective, and the integration of or-
which,forlargedatasets,maybecomeuninformativeasto thogonal approaches like distillation, NFDM has the po-
the true coupling. In contrast, NFDM-OT directly penal- tentialtoachieveevenbetterresults.
8NeuralFlowDiffusionModels
References conferenceonneuralnetworks(IJCNN),pp.1–8.IEEE,
2020.
Albergo, M. S. and Vanden-Eijnden, E. Building normal-
izing flows with stochastic interpolants. arXiv preprint
Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-
arXiv:2209.15571,2022.
mationusingrealnvp.arXivpreprintarXiv:1605.08803,
2016.
Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E.
Stochasticinterpolants: Aunifyingframeworkforflows
Dormand, J. R. and Prince, P. J. A family of embedded
anddiffusions. arXivpreprintarXiv:2303.08797,2023.
runge-kuttaformulae. Journalofcomputationalandap-
Anderson, B.D. Reverse-timediffusionequationmodels. pliedmathematics,6(1):19–26,1980.
StochasticProcessesandtheirApplications,12(3):313–
326,1982. Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I.,
and Duvenaud, D. Ffjord: Free-form continuous dy-
Bartosh,G.,Vetrov,D.,andNaesseth,C.A. Neuraldiffu- namicsforscalablereversiblegenerativemodels. arXiv
sionmodels. arXivpreprintarXiv:2310.08337,2023. preprintarXiv:1810.01367,2018.
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J.,
Gu,J.,Zhai,S.,Zhang,Y.,Bautista,M.A.,andSusskind,
Leary, C., Maclaurin, D., Necula, G., Paszke, A.,
J. f-dm: A multi-stage diffusion model via progressive
VanderPlas, J., Wanderman-Milne, S., and Zhang, Q.
signaltransformation.arXivpreprintarXiv:2210.04955,
JAX: composable transformations of Python+NumPy
2022.
programs, 2018. URL http://github.com/
google/jax.
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and
Hochreiter, S. Gans trained by a two time-scale update
Chen,R.T.,Rubanova,Y.,Bettencourt,J.,andDuvenaud,
rule converge to a local nash equilibrium. Advances in
D.K. Neuralordinarydifferentialequations. Advances
neuralinformationprocessingsystems,30,2017.
inneuralinformationprocessingsystems,31,2018a.
Chen, Z., Yeo, C. K., Lee, B. S., and Lau, C. T. Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
Autoencoder-basednetworkanomalydetection. In2018 bilisticmodels. arXivpreprintarXiv:2006.11239,2020.
Wireless telecommunications symposium (WTS), pp. 1–
Ho,J.,Saharia,C.,Chan,W.,Fleet,D.J.,Norouzi,M.,and
5.IEEE,2018b.
Salimans,T. Cascadeddiffusionmodelsforhighfidelity
Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., imagegeneration. TheJournalofMachineLearningRe-
Sengupta,B.,andBharath,A.A. Generativeadversarial search,23(1):2249–2281,2022.
networks: Anoverview. IEEESignalProcessingMaga-
zine,35(1):53–65,2018. Ho, Y.-H., Chan, C.-C., Peng, W.-H., Hang, H.-M., and
Doman´ski, M. Anfic: Image compression using aug-
Daras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and mented normalizing flows. IEEE Open Journal of Cir-
Milanfar, P. Softdiffusion: Score matching forgeneral cuitsandSystems,2:613–626,2021.
corruptions. arXivpreprintarXiv:2209.05442,2022.
Hoogeboom,E.andSalimans,T. Blurringdiffusionmod-
De Bortoli, V., Thornton, J., Heng, J., and Doucet, A.
els. arXivpreprintarXiv:2209.05557,2022.
Diffusionschro¨dingerbridgewithapplicationstoscore-
based generative modeling. Advances in Neural Infor- Karras,T.,Aittala,M.,Aila,T.,andLaine,S. Elucidating
mationProcessingSystems,34:17695–17709,2021.
the design space of diffusion-based generative models.
AdvancesinNeuralInformationProcessingSystems,35:
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and
26565–26577,2022.
Fei-Fei, L. Imagenet: A large-scale hierarchical image
database. In2009IEEEconferenceoncomputervision
Kelly, J., Bettencourt, J., Johnson, M. J., and Duvenaud,
andpatternrecognition,pp.248–255.Ieee,2009.
D. K. Learning differential equations that are easy to
Dhariwal,P.andNichol,A. DiffusionmodelsbeatGANs solve. AdvancesinNeuralInformationProcessingSys-
on image synthesis. arXiv preprint arXiv:2105.05233, tems,33:4370–4380,2020.
2021.
Kim,D.,Na,B.,Kwon,S.J.,Lee,D.,Kang,W.,andMoon,
Dias,M.L.,Mattos,C.L.C.,daSilva,T.L.,deMacedo,J. I.-C. Maximum likelihood training of implicit nonlin-
A.F.,andSilva,W.C. Anomalydetectionintrajectory eardiffusionmodels. arXivpreprintarXiv:2205.13699,
datawithnormalizingflows. In2020internationaljoint 2022.
9NeuralFlowDiffusionModels
Kingma, D. P. and Dhariwal, P. Glow: Generative flow Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
withinvertible1x1convolutions. Advancesinneuralin- DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and
formationprocessingsystems,31,2018. Lerer,A. Automaticdifferentiationinpytorch. 2017.
Kingma,D.P.andWelling,M. Auto-encodingvariational Phung,H.,Dao,Q.,andTran,A.Waveletdiffusionmodels
Bayes. arXivpreprintarXiv:1312.6114,2013. are fast and scalable image generators. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Kingma, D. P., Salimans, T., Poole, B., and Ho, PatternRecognition,pp.10199–10208,2023.
J. Variational diffusion models. arXiv preprint
arXiv:2107.00630,2,2021. Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C.,
Amos, B., Lipman, Y., and Chen, R. Multisample
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers flowmatching: Straighteningflowswithminibatchcou-
offeaturesfromtinyimages. 2009. plings. arXivpreprintarXiv:2304.14772,2023.
Lee,S.,Kim,B.,andYe,J.C. Minimizingtrajectorycur- Rezende,D.J.,Mohamed,S.,andWierstra,D. Stochastic
vature of ode-based generative models. arXiv preprint backpropagationandapproximateinferenceindeepgen-
arXiv:2301.12003,2023. erativemodels. InInternationalconferenceonmachine
learning,pp.1278–1286.PMLR,2014.
Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and
Le, M. Flow matching for generative modeling. arXiv Rissanen, S., Heinonen, M., and Solin, A. Generative
preprintarXiv:2210.02747,2022. modelling with inverse heat dissipation. arXiv preprint
arXiv:2206.13397,2022.
Liu, L., Ren, Y., Lin, Z., and Zhao, Z. Pseudo numeri-
cal methods for diffusion models on manifolds. arXiv Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
preprintarXiv:2202.09778,2022a. Ommer,B. High-resolutionimagesynthesiswithlatent
diffusionmodels. InProceedingsoftheIEEE/CVFcon-
Liu, Q. Rectified flow: A marginal preserving approach ferenceoncomputervisionandpatternrecognition,pp.
to optimal transport. arXiv preprint arXiv:2209.14577, 10684–10695,2022.
2022.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J.,
Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Denton, E. L., Ghasemipour, K., Gontijo Lopes, R.,
Learningtogenerateandtransferdatawithrectifiedflow. Karagol Ayan, B., Salimans, T., et al. Photorealistic
arXivpreprintarXiv:2209.03003,2022b. text-to-image diffusion models with deep language un-
derstanding.AdvancesinNeuralInformationProcessing
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
Systems,35:36479–36494,2022.
Dpm-solver:Afastodesolverfordiffusionprobabilistic
modelsamplinginaround10steps. AdvancesinNeural Sahoo, S. S., Gokaslan, A., De Sa, C., and Kuleshov, V.
InformationProcessingSystems,35:5775–5787,2022. Diffusion models with learned adaptive noise. arXiv
preprintarXiv:2312.13236,2023.
Nichol,A.andDhariwal,P. Improveddenoisingdiffusion
probabilisticmodels. arXivpreprintarXiv:2102.09672, Salimans, T. and Ho, J. Progressive distillation for
2021. fast sampling of diffusion models. arXiv preprint
arXiv:2202.00512,2022.
Nielsen, B. M., Christensen, A., Dittadi, A., and Winther,
O.Diffenc:Variationaldiffusionwithalearnedencoder. Serra`, J., A´lvarez, D., Go´mez, V., Slizovskaia, O., Nu´n˜ez,
arXivpreprintarXiv:2310.19789,2023. J. F., and Luque, J. Input complexity and out-of-
distribution detection with likelihood-based generative
Okhotin, A., Molchanov, D., Arkhipkin, V., Bartosh,
models. arXivpreprintarXiv:1909.11480,2019.
G., Alanov, A., and Vetrov, D. Star-shaped denois-
ing diffusion probabilistic models. arXiv preprint Shaul,N.,Chen,R.T.,Nickel,M.,Le,M.,andLipman,Y.
arXiv:2302.05259,2023. Onkineticoptimalprobabilitypathsforgenerativemod-
els. In International Conference on Machine Learning,
Papamakarios, G., Nalisnick, E., Rezende, D. J., Mo- pp.30883–30907.PMLR,2023a.
hamed, S., and Lakshminarayanan, B. Normalizing
flows for probabilistic modeling and inference. The Shaul, N., Perez, J., Chen, R. T., Thabet, A., Pumarola,
Journal of Machine Learning Research, 22(1):2617– A.,andLipman,Y. Bespokesolversforgenerativeflow
2680,2021. models. arXivpreprintarXiv:2310.19075,2023b.
10NeuralFlowDiffusionModels
Shi,Y.,DeBortoli,V.,Campbell,A.,andDoucet,A. Dif- VanDenOord,A.,Kalchbrenner,N.,andKavukcuoglu,K.
fusion schr\” odinger bridge matching. arXiv preprint Pixelrecurrentneuralnetworks. InInternationalconfer-
arXiv:2303.16852,2023. enceonmachinelearning,pp.1747–1756.PMLR,2016.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Vincent,P. Aconnectionbetweenscorematchingandde-
Ganguli,S. Deepunsupervisedlearningusingnonequi- noisingautoencoders. Neuralcomputation,23(7):1661–
librium thermodynamics. In International Conference 1674,2011.
onMachineLearning,pp.2256–2265.PMLR,2015.
Watson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L.,
Song,J.,Meng,C.,andErmon,S. Denoisingdiffusionim- Yim, J., Eisenach, H. E., Ahern, W., Borst, A. J.,
plicitmodels. arXivpreprintarXiv:2010.02502,2020a. Ragotte, R. J., Milles, L. F., et al. De novo design of
protein structure and function with rfdiffusion. Nature,
Song, Y. and Ermon, S. Generative modeling by estimat- 620(7976):1089–1100,2023.
ing gradients of the data distribution. arXiv preprint
arXiv:1907.05600,2019. Xiao, Z., Yan, Q., and Amit, Y. Likelihood regret: An
out-of-distribution detection score for variational auto-
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er- encoder.Advancesinneuralinformationprocessingsys-
mon,S.,andPoole,B. Score-basedgenerativemodeling tems,33:20685–20696,2020.
throughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020b. Yang,L.,Zhang,Z.,Song,Y.,Hong,S.,Xu,R.,Zhao,Y.,
Shao, Y., Zhang, W., Cui, B., and Yang, M.-H. Diffu-
Song,Y.,Durkan,C.,Murray,I.,andErmon,S. Maximum sion models: A comprehensive survey of methods and
likelihoodtrainingofscore-baseddiffusionmodels. Ad- applications. arXivpreprintarXiv:2209.00796,2022.
vances in Neural Information Processing Systems, 34:
1415–1428,2021. Yang, R. and Mandt, S. Lossy image compression
with conditional diffusion models. arXiv preprint
Song,Y.,Dhariwal,P.,Chen,M.,andSutskever,I. Consis- arXiv:2209.06950,2022.
tencymodels. arXivpreprintarXiv:2303.01469,2023.
Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand,
Tachibana, H., Go, M., Inahara, M., Katayama, Y., and F., Freeman, W. T., and Park, T. One-step diffusion
Watanabe, Y. It\ˆ{o}-taylor sampling scheme for de- with distribution matching distillation. arXiv preprint
noisingdiffusionprobabilisticmodelsusingidealderiva- arXiv:2311.18828,2023.
tives. arXivpreprintarXiv:2112.13339,2021.
Zhang,Q.andChen,Y. Diffusionnormalizingflow. arXiv
Tan, X., Chen, J., Liu, H., Cong, J., Zhang, C., Liu, Y., preprintarXiv:2110.07579,2021.
Wang,X.,Leng,Y.,Yi,Y.,He,L.,etal. Naturalspeech:
End-to-end text-to-speech synthesis with human-level Zheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and
quality. IEEETransactionsonPatternAnalysisandMa- Anandkumar,A. Fastsamplingofdiffusionmodelsvia
chineIntelligence,2024. operator learning. In International Conference on Ma-
chineLearning,pp.42390–42402.PMLR,2023a.
Tomczak,J.M.Deepgenerativemodeling.Springer,2022.
Zheng, K., Lu, C., Chen, J., and Zhu, J. Improved tech-
Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector- niquesformaximumlikelihoodestimationfordiffusion
Brooks,J.,Fatras,K.,Wolf,G.,andBengio,Y. Improv- odes. arXivpreprintarXiv:2305.03935,2023b.
ingandgeneralizingflow-basedgenerativemodelswith
minibatchoptimaltransport. InICMLWorkshoponNew
FrontiersinLearning,Control,andDynamicalSystems,
2023.
Trippe,B.L.,Yim,J.,Tischer,D.,Baker,D.,Broderick,T.,
Barzilay, R., and Jaakkola, T. S. Diffusion probabilis-
tic modeling of protein backbones in 3D for the motif-
scaffoldingproblem.InTheEleventhInternationalCon-
ferenceonLearningRepresentations,2023.
Vahdat,A.,Kreis,K.,andKautz,J.Score-basedgenerative
modelinginlatentspace. AdvancesinNeuralInforma-
tionProcessingSystems,34:11287–11302,2021.
11NeuralFlowDiffusionModels
A.Derivations
A.1.Derivationofobjective
For the forward process, we have the conditional marginal distribution q (z |x), which is implicitly defined by the in-
φ t
vertible transformation z = F (ε,t,x) (eq. (9)), where ε ∼ q(ε). Additionally, there’s a SDE (eq. (15)) starting from
t φ
z ∼q(z |x)andflowingbackwardsintime:
1 1
dz =f˜B(z ,t,x)dt+g (t)dw. (27)
t φ t φ
ThisSDEcorrespondstothemarginaldistributionq (z |x).
φ t
Thereverseprocessinvolvesapriordistributionp(z )andareverseSDE(eq.(15)):
1
dz =fˆ (z ,t)dt+g (t)dw¯. (28)
t θ,φ t φ
Ourgoalistoderiveavariationalboundonthenegativelog-likelihoodofthemodel.
We begin by discretizing the conditional reverse SDE in eq. (27) and the reverse SDE in eq. (28), transitioning from
continuous-time trajectories {z(t)} to discrete-time trajectories z¯ ,z¯ ,...,z¯ , where T represents the number of
t∈[0,1] 0 1 1
T
discretesteps. Forconvenience,wealsointroduce∆t= 1.
T
ThediscretizationoftheconditionalreverseSDEisasfollows:
(cid:16) (cid:17)
q¯ (z¯ |x)=q (z¯ |x), q¯ (z¯ |z¯ ,x)=N z¯ ;z¯ −∆tf˜B(z¯ ,t,x),∆tg2(t)I . (29)
φ 1 φ 1 φ t−∆t t t−∆t t φ t φ
WesimilarlydiscretizethereverseSDE:
(cid:16) (cid:17)
p¯(z¯ )=p(z¯ ), p¯(x|z¯ )=p(x|z¯ ), p¯ (z¯ |z¯ )=N z¯ ;z¯ −∆tfˆ (z¯ ,t),∆tg2(t)I . (30)
1 1 0 0 θ,φ t−∆t t t−∆t t θ,φ t φ
It’s important to note that once we discretize the trajectories, q¯ (z¯ |x) ̸= q (z¯ |x) and p¯ (z¯ ) ̸= p (z¯ ) except for
φ t φ t θ,φ t θ,φ t
t = 1. However,thisdiscretizationcorrespondstoanEuler-MaruyamamethodofintegratingSDEs. Thus,weknowthat
as∆t→0,thediscretizedtrajectoriesconvergetothecontinuousones.
Now,consideringthesediscreteprocessesandrecognizingthattheyareMarkovianconditionalandunconditionalprocesses
movingbackwardsintime,wecanleverageakeyresultfromHoetal.(2020):
E [−logp¯ (x)]≤L¯ +L¯ +L¯ , where (31)
q(x) θ,φ recon diff prior
L¯ =E [−logp(x|z¯ )], (32)
recon q¯φ(x,z¯0) 0
T
L¯ = (cid:88) E (cid:2) D (cid:0) q¯ (z¯ |z¯ ,x)∥p¯ (z¯ |z¯ )(cid:1)(cid:3) , (33)
diff q¯φ(x,z¯t) KL φ t−∆t t θ,φ t−∆t t
t=1
T
L¯ =E (cid:2) D (cid:0) q¯ (z¯ |x)∥p¯(z¯ )(cid:1)(cid:3) . (34)
prior q(x) KL φ 1 1
Consequently, eq. (31) establishes a variational upper bound on the negative log-likelihood of discretized reverse pro-
cess. Takingthelimitsofthisupperbound,thereconstructiontermL¯reconandthepriortermL¯priorretaintheirforms.
12NeuralFlowDiffusionModels
However,wecanreformulatethediffusionterm:
L = lim L¯ (35)
diff diff
∆t→0
T
= lim (cid:88) E (cid:2) D (cid:0) q¯ (z¯ |z¯ ,x)∥p¯ (z¯ |z¯ )(cid:1)(cid:3) (36)
∆t→0
q¯φ(x,z¯t) KL φ t−∆t t θ,φ t−∆t t
t=1
T
= ∆l tim
→0
t(cid:88) =T
1
E q¯φ(x,z¯t)(cid:20) 2∆t1
g
φ2(t)(cid:13) (cid:13) (cid:13)(cid:0)(cid:0)z¯ t−∆tfˆ θ,φ(z¯ t,t)−(cid:0)(cid:0)z¯ t+∆tf˜ φB(z¯ t,t,x)(cid:13) (cid:13) (cid:13)2 2(cid:21) (37)
T
= ∆l tim
→0
t(cid:88) =T
1
E q¯φ(x,z¯t)(cid:34) 2(cid:8)∆(cid:8)∆ tgt φ22(cid:3) (t)(cid:13) (cid:13) (cid:13)f˜ φB(z¯ t,t,x)−fˆ θ,φ(z¯ t,t)(cid:13) (cid:13) (cid:13)2 2(cid:35) (38)
T
(cid:20) 1 (cid:13) (cid:13)2(cid:21)
=E (cid:13)f˜B(z¯ ,t,x)−fˆ (z¯ ,t)(cid:13) , (39)
u(t)q¯φ(x,z¯t) 2g φ2(t)(cid:13) φ t θ,φ t (cid:13) 2
whereu(t)isauniformdistributionoveraunitinterval.
Therefore, as the discretized trajectory tends to the continuous one when ∆t → 0, we can use the obtained limits as a
variationalboundonthenegativelog-likelihoodofthecontinuousmodel.
A.2.NFDMwithrestrictions
In Section 4, we discussed training the NFDM with restrictions on the reverse process. This section aims to explore the
limitationsofsuchrestrictionsandtofurtherdiscussthetrainingofNFDMwithcurvaturepenalties.
When training NFDM with certain restrictions, it is crucial that these restrictions are feasible. For example, imposing
overly stringent constraints on the reverse process (such as fixing its drift term fˆ ≡ 0) will render the forward process
incapableofadapting,regardlessofitsflexibility. Similarly,unattainablepenalties,likethoseonthesquarednormofthe
driftterm∥fˆ ∥2,willleadtobiasedsolutions,asit’simpossibletomatchtwoprocesseswhen∥fˆ ∥2 ≡0.
θ,φ 2 θ,φ 2
Hence,inSection4.1,wesuggestpenalizingthecurvatureofdeterministicgenerativetrajectoriesspecificallywithpenalty
L (eq.(26)). Thisisbecausefornon-degeneratedatadistributions,thereexistmappingsthattransformthemintounit
crv
Gaussiannoisealongstraightlinetrajectories.
TocalculatethecurvaturepenaltyL (eq.(26)),weproceedasfollows. First,weusethechainruletorewritethetime
crv
derivative:
dfˆ (z ,t) ∂fˆ (z ,t)∂z ∂fˆ (z ,t)
θ,φ t = θ,φ t t + θ,φ t (40)
dt ∂z ∂t ∂t
t
∂fˆ (z ,t) ∂fˆ (z ,t)
= θ,φ t fˆ (z ,t)+ θ,φ t . (41)
∂z θ,φ t ∂t
t
The second term in eq. (41) is the time derivative of a function. As discussed in section 3.1, the time derivatives can be
determinedasaJacobianVectorProduct(JVP)withaone-dimensionalunitvector.Notably,thefirsttermineq.(41)isalso
aJVP.Therefore,wecancombinethesetwooperations. Forthispurpose,wedefineaRD+1dimensionaladjointvectorv:
(cid:20) fˆ (z ,t)(cid:21)
v = θ,φ t . (42)
1
Consequently,L canbecomputedastheJVPoffˆ (z ,t)withthevectorv.
crv θ,φ t
B.ConnectionsofNFDMwithotherapproaches
B.1.SpecialcasesofNFDM
Many existing works define the forward process in diffusion models as either fixed or simply parameterized processes.
TheseprocessescanbeconsideredspecialcasesofNFDM.Inthissection,wereviewsomeoftheseapproaches.
13NeuralFlowDiffusionModels
SoftDiffusion. Darasetal.(2022)consideramoregeneralcaseoffixedforwardprocesses,q(z |x) = N(z ;C x,σ I),
t t t t
whichcanbeparameterizedasF(ε,t,x)=C x+σ ε. Suchdistributionsinclude,forexample,combinationsofblurring
t t
andtheinjectionofGaussiannoise.
Star-ShapedDiffusion.Okhotinetal.(2023)extendedconventionaldiffusionmodelstoincludedistributionsfromtheex-
ponentialfamily. AlthoughStar-ShapedDiffusionisadiscrete-timeapproachanddoesnotdirectlycorrespondtoNFDM,
the latter can work with exponential family distributions through reparameterization functions. For instance, for some
continuousone-dimensionaldistributionq(z |x),NFDMcoulduseF(ε,t,x) = a(b(ε),t,x),whereaistheinverseCu-
t
mulativeDistributionFunction(CDF)ofq(z |x),andbistheCDFofaunitGaussian.
t
Variational Diffusion Models. Kingma et al. (2021) proposed forward conditional distributions q (z |x) as
φ t
N(z ;α (t)x,σ2(t)I) with learnable parameters φ. In the context of NFDM, this distribution can be parameterized
t φ φ
byF (ε,t,x)=α (t)x+σ (t)ε.
φ φ φ
LSGM.Vahdatetal.(2021)suggestanalternativeapproachforparameterizingtheforwardprocess,proposingdiffusionin
alatentspaceofaVAE.Therefore,theforwardprocessischaracterizedbyadistributionq (z |x)=N(z ;α E (x),σ2I),
φ t t t φ t
whereE istheencoderoftheVAE.ToparameterizethesameforwardprocesswithNFDM,onecoulduseF (ε,t,x)=
φ φ
α E (x)+σ ε. To align the reverse process, the reconstruction distribution should be p(x|z ) = N(x;D (z ),δ2I),
t φ t 0 φ 0
whereD isVAE’sdecoder.
φ
DiffEnc. Nielsen et al. (2023) proposed a more general forward process, q (z |x) = N(z ;α f (x,t),σ2I), which,
φ t t t φ t
unlikeLSGM,transformsxinatime-dependentmanner. ThisforwardprocesscanalsobedescribedintermsofNFDM
asF (ε,t,x)=α f (x,t)+σ ε.
φ t φ t
B.2.ConnectionsofNFDMobjective
Inthissection,wedelveintothedetailsoftheNFDM’sobjectivefunction(eq.(18)),withaparticularfocusonthediffusion
lossL (eq.(20)).
diff
First,let’sunpacktheL bysubstitutingthedefinitionsoff˜B andfˆ :
diff φ θ,φ
(cid:20) 1 (cid:13) (cid:13)2(cid:21)
L = E (cid:13)f˜B(z ,t,x)−fˆ (z ,t)(cid:13) (43)
diff u(t)qφ(x,zt) 2g φ2(t)(cid:13) φ t θ,φ t (cid:13) 2
 (cid:13) (cid:13)2
(cid:13) (cid:13)
= u(t)qφE (x,zt)  2g φ21 (t)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:16) (cid:124)f φ(z t,t,x)−f (cid:123)φ (cid:122)(cid:0) z t,t,xˆ θ(z t,t)(cid:1)(cid:17) (cid:125)+g φ2 2(t) (cid:124)(cid:16) ∇ ztlogq φ(cid:0) z t|xˆ θ(z t, (cid:123)t (cid:122))(cid:1) −∇ ztlogq φ(z t|x)(cid:17) (cid:125)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
  .
(cid:13) FlowMatchingterm ScoreMatchingterm (cid:13) 2
(44)
Thisformulationclearlydelineatestwocomponentsoftheobjective: thefirstcalculatesthedifferencebetweentheODE
driftterms,andthesecondcalculatesthedifferencebetweenthescorefunctions. Moreover,thisexpressionhighlightsthe
role of g (t). When g (t) is small, the forward and reverse processes exhibit smoother trajectories, and the objective is
φ φ
dominated by the first term. Conversely, when g (t) is large, the processes exhibit more stochastic trajectories, and the
φ
objectiveisdominatedbythesecondterm.
Crucially,intheextremescenarioswhereg (t)approacheseither0or∞,thediffusionlossL correspondstoeithera
φ diff
reweightedFlowMatchingloss(Lipmanetal.,2022)orareweightedScoreMatchingloss(Vincent,2011),respectively:
(cid:20) (cid:21)
gφl (i tm )→0L
diff
= gφl (i tm )→0E
u(t)qφ(x,zt) 2g
φ21 (t)(cid:13) (cid:13)f φ(z t,t,x)−f φ(cid:0) z t,t,xˆ θ(z t,t)(cid:1)(cid:13) (cid:13)2
2
, (45)
(cid:34) (cid:35)
g2(t)
gφ(l ti )m →∞L
diff
= gφ(l ti )m →∞E
u(t)qφ(x,zt)
φ
4
(cid:13) (cid:13)∇ ztlogq φ(cid:0) z t|xˆ θ(z t,t)(cid:1) −∇ ztlogq φ(z t|x)(cid:13) (cid:13)2
2
(46)
B.3.ConnectionsofNFDMwithStochasticInterpolants
In this section, we explore the connections between NFDM and Stochastic Interpolants, as proposed by Albergo et al.
(2023). BothNFDMandStochasticInterpolantsintroduceamoregeneralfamilyofforwardprocessesthroughtheuseof
14NeuralFlowDiffusionModels
a reparameterization function. Although we acknowledge the relevance of Stochastic Interpolants to our work, there are
notabledifferencesbetweenthetwoapproaches.
First, our methodology involves parameterizing the reverse process by substituting the prediction of x into the forward
process, whereasStochasticInterpolantsnecessitatelearningtwoseparatefunctionsforthereverseprocess: thevelocity
fieldandthescorefunction.
Second, we present NFDM as a framework that enables both the pre-specification and learning of the forward process,
incontrasttoStochasticInterpolants, whicharederivedundertheassumptionofafixedforwardprocess. Consequently,
theobjectivesutilizedbyStochasticInterpolantsdonotsupporttheincorporationoflearnableparametersfortheforward
process. Furthermore, theseobjectivesaretailoredtowardslearningthevelocityfieldandthescorefunction, ratherthan
optimizinglikelihood,asisthecasewithNFDM.
Intheirpracticalapplications,StochasticInterpolantsaredemonstratedwithonlysimpleparameterizationsoftheforward
process. They propose a theoretical framework for learning the forward process that would result in an reverse process
characterizedbydynamicaloptimaltransport. However,thisapproachiscontingentuponsolvingahigh-dimensionalmin-
max optimization problem, for which they do not provide experimental results. Moreover, their work does not clearly
articulatehowStochasticInterpolantsmightbeappliedtolearningothergenerativedynamics.
In contrast, NFDM introduces a more generalized method for learning generative dynamics. Moreover, when NFDM is
learned with restrictions or penalties (see Section 4), it remains within a min-min optimization paradigm, which can be
addressedmoreefficiently.
C.Implementationdetails
Our evaluation of NFDM includes tests on synthetic data, CIFAR-10 (Krizhevsky et al., 2009), and two downsampled
ImageNet (Deng et al., 2009; Van Den Oord et al., 2016) datasets. To maintain consistency with baselines, we employ
horizontalflippingasadataaugmentationtechniqueintrainingmodelsonCIFAR-10andImageNet(Songetal.,2020b;
2021). Fordensityestimationofdiscretedata,uniformdequantizationisused(seeAppendixC.1).
Weparameterizexˆ (eq.(17))inthereverseprocessusinga5-layerperceptronswith512neuronsineachlayerforsynthetic
θ
dataandtheU-NetarchitecturefromDhariwal&Nichol(2021)forimages. Inallexperiments,a3-layerperceptronswith
64neuronsineachlayerisemployedtoparameterizeg (eq.(12)),andforF (eq.(9)),weuseanidenticalneuralnetwork
φ φ
tothatofxˆ . ThesoledifferenceisthatforF , wedoubletheoutputofthelastlayertoparameterizeµ¯ (eq.(23))and
θ φ φ
σ¯ (eq.(24))withsamemodel(seeSection3.5).
φ
ThemodelsweretrainedusingtheAdamoptimizerwiththefollowingparameters: β =0.9,β =0.999,aweightdecay
1 2
of0.0,andϵ = 10−8. Thetrainingprocesswasfacilitatedbyapolynomialdecaylearningrateschedule,whichincludes
awarm-upphaseforapredefinednumberoftrainingsteps. Duringthisphase,thelearningrateislinearlyincreasedfrom
10−8 toapeakvalue. Afterreachingthepeaklearningrate,itisthenlinearlydecreasedto10−8 bythefinaltrainingstep.
ThespecifichyperparametersaredetailedinTable3. TrainingwascarriedoutonTeslaV100GPUs
C.1.Dequantization
ForreportingtheNLL,weemploystandarduniformdequantization. TheNLLisestimatedusinganimportance-weighted
average,givenby
K
1 (cid:88)
log p (x+u ), where u ∼U(0,1), (47)
K θ k k
k=1
wherex∈[0,...,255].
C.2.Parameterization
WedefinetheparameterizationofthefunctionF asfollowsineq.(22):
φ
F (ε,t,x)=µ (x,t)+εσ (x,t). (48)
φ φ φ
15NeuralFlowDiffusionModels
Table3. Traininghyper-parameters.
CIFAR-10 ImageNet32 ImageNet64
Channels 256 256 192
Depth 2 3 3
Channelsmultipliers 1,2,2,2 1,2,2,2 1,2,3,4
Heads 4 4 4
HeadsChannels 64 64 64
Attentionresolution 16 16,8 32,16,8
Dropout 0.0 0.0 0.0
EffectiveBatchsize 256 1024 2048
GPUs 2 4 16
Epochs 1000 200 250
Iterations 391k 250k 157k
LearningRate 4e-4 1e-4 1e-4
LearningRateScheduler Polynomial Polynomial Constant
WarmupSteps 45k 20k -
In addition to inferring F , we require access to its inverse F−1 and the logarithm of the determinant of the inverse
φ φ
transformationlog|J−1|(eq.(14)). Thecalculationoftheinverseisstraightforward:
F
z −µ (x,t)
F−1(z ,t,x)= t φ . (49)
φ t σ (x,t)
φ
Thelogarithmofthedeterminantoftheinversetransformation,log|J−1|,canalsobereadilydetermined:
F
D
(cid:88)
log|J−1|=−log|J |=− log[σ (x,t)] , (50)
F F φ i
i=1
whereDrepresentsthedimensionalityofthedata,and[σ (x,t)] referstoi-thoutputoftheσ function.
φ i φ
Toensuretheconstraintsg ≥0(eq.(12))andσ¯ ≥0(eq.(24))aremet,weapplythesoftplusfunctiontotheoutputsof
φ φ
theneuralnetworks.
16