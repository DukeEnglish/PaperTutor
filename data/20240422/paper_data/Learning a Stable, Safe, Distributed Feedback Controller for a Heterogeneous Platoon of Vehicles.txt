Learning a Stable, Safe, Distributed Feedback Controller for a
Heterogeneous Platoon of Vehicles
Michael H. Shaham1 and Tas¸kın Padır1,2
Abstract—Platooning of autonomous vehicles has the poten-
tial to increase safety and fuel efficiency on highways. The
goal of platooning is to have each vehicle drive at some speed
(set by the leader) while maintaining a safe distance from its
neighbors.Manypriorworkshaveanalyzedvariouscontrollers
forplatooning,mostcommonlylinearfeedbackanddistributed
model predictive controllers. In this work, we introduce an
algorithm for learning a stable, safe, distributed controller
for a heterogeneous platoon. Our algorithm relies on recent
developments in learning neural network stability and safety
Fig.1. TheplatoonoffourF1Tenthvehiclesinthetestcourse.
certificates. We train a controller for autonomous platooning
in simulation and evaluate its performance on hardware with
problem a good case study for certifiable learning-based
a platoon of four F1Tenth vehicles. We then perform further
analysis in simulation with a platoon of 100 vehicles. Exper- control.
imental results demonstrate the practicality of the algorithm In this work, we focus on learning decentralized feedback
and the learned controller by comparing the performance of
controllersforaheterogeneousplatoonofvehicles.Thefocus
theneuralnetworkcontrollertolinearfeedbackanddistributed
is on developing controllers that are provably stable for the
model predictive controllers.
entire platoon (learn a centralized stability certificate based
on a decentralized controller). The difficulty in learning
I. INTRODUCTION
a stable decentralized controller for a multi-agent system
In safety-critical systems where performance guarantees where we need centralized performance guarantees arises
are of paramount importance, adoption of learning-based from the fact that the computational complexity of verifying
controllers has understandably been slow. This is because stabilitygrowsexponentiallyasthesizeofthesystemgrows.
most learning-based controllers, up until recently, have not This is because verifying stability requires solving mixed-
had the ability to provide performance guarantees over the integer linear programs (MILPs).
rangeofscenariosthesystemcanexpecttoencounter.Recent The contributions of this work are as follows: 1) we
work in learning controllers, safety certificates, and stability introduceanalgorithmforlearningacertifiablystableneural
certificateshasopenedthedoorforlearningsafeandreliable network controller that enables the engineer to “guide” the
controllers for safety-critical systems [1]. As we consider resulting controller to desired behavior; 2) we use a change
using these controllers in real systems, however, developing of variable to reformulate the dynamics of a heterogeneous
algorithmsthatwilllearncontrollersthattranslatewellfrom platoon as that of a homogeneous platoon, enabling us to
the simulated model to the real world is critical. Further, learn and verify a single controller for a heterogeneous
for these methods to work on multi-agent systems, it will platoon; 3) we train controllers in simulation and validate
be necessary to develop algorithms that scale well as the the learned controllers on hardware using four F1Tenth
number of agents increases. vehicles [4], shown in Fig. 1, and in simulation with 100
Autonomous vehicle platooning is an automated driving vehicles.
technology that can improve safety [2] and reduce fuel
II. RELATEDWORK
emissions [3] on our roads. However, there is an inherent
tradeoff between these two objectives as fuel efficiency is A. Autonomous vehicle platooning
maximized when vehicles are closer together while safety Theplatooningproblemfirstbegangainingattentionfrom
is maximized when vehicles are further apart. This tradeoff a theoretical perspective as early as the 1960s when the
between performance (a stable, tightly packed platoon) and authors of [5] analyzed the performance of a 3-vehicle
safety (the vehicles do not crash) makes the platooning platoon using a constant distance headway (CDH) and a
linearfeedbackcontroller.However,itwasn’tuntilthe1990s
Research was sponsored by the DEVCOM Analysis Center and was and the California PATH demonstration [6] when the prob-
accomplishedunderCooperativeAgreementNumberW911NF-22-2-001.
1InstituteforExperientialRobotics,NortheasternUniversity,Boston,MA lem began gaining widespread attention. Much of the prior
02116USA{shaham.m, t.padir} @northeastern.edu work has focused on gaining theoretical insights that guide
2Tas¸kınPadırholdsconcurrentappointmentsasaProfessorofElectrical algorithm development.
and Computer Engineering at Northeastern University and as an Amazon
In [7], the authors showed that using a linear feedback
Scholar. This paper describes work performed at Northeastern University
andisnotassociatedwithAmazon. controllerandaCDHspacingpolicycanleadtostringinsta-
4202
rpA
81
]GL.sc[
1v47421.4042:viXrability, meaning disturbances witnessed earlier in the platoon in certifiable learning-based control was [20], where both a
will propagate and become worse toward the end of the neural Lyapunov function and a controller were learned to
platoon. Further, they showed how the disturbance to error provestabilitywhencontrollingagivennonlinearsystem.In
gain increases as the size of the platoon increases for both a [21],thiswasextendedtoconsideralsosystemswithlearned
predecessor following (PF) and bidirectional (BD) topology. neural network dynamics. Depending on the system being
Building on this, the authors of [8] showed how the stability analyzed, it can be straightforward to extend these earlier
margins of a platoon using a linear feedback controller with works directly to multi-agent systems, but the certification
BD topology decays to zero as O(1/N2) when each vehicle problemquicklybecomesintractableasthenumberofagents
has the same dynamics. This work extended [9], where the (and thus the state size) increases due to the need to solve
authors also showed how a particular nonlinear controller a nonconvex mixed integer program. In [22], the authors get
outperformed linear controllers for a platoon using a PF around this due to the fact that the goal of each agent is
topology. independent of others, and thus stability certificates can be
Since CDH spacing policies lead to string instability, learned in a completely decentralized manner.
researchers switched their focus to constant time headway
III. METHODOLOGY
(CTH) spacing policies where the distance between vehicles
A. Controller design
is selected based on each vehicle’s velocity. In [10], it
was shown that platoons using linear feedback controllers We consider a platoon of N vehicles indexed by i =
with CTH spacing policies are string stable. However, it is 1,...,N where vehicle 1 is the leader. Each vehicle has
common to use a time headway of at least 1.8 seconds [11] dynamics given by
whichleadstolargeinter-vehicledistancesandthusreduced (cid:20) (cid:21) (cid:20) (cid:21)
1 ∆t 0
traffic capacity [12]. s i(k+1)= 0 1− ∆t s i(k)+ ∆t a i(k) (1)
If vehicles can communicate with each other, distributed τi τi
model predictive control (DMPC), which requires vehicles where s i(k) = (p i(k),v i(k)) ∈ R2 and a i(k) ∈ R are
share their planned trajectories with their neighbors, is pos- vehicle i’s state (position and velocity) and control input
sible.OneofthefirstDMPCcontrollerswasanalyzedin[13] (desired velocity), respectively, τ i is a vehicle longitudinal
where it was shown that a DMPC controller is both stable dynamics delay parameter, and ∆t is the discrete timestep.
and string stable for a PF topology when all of the vehicles It is common in the literature to use a three-state linear
in the platoon know the desired velocity of the platoon a dynamics model with position, velocity, and acceleration as
priori. In [14], the stability condition was extended for the thestateanddesiredaccelerationasthecontrolinput,similar
casethatnotallvehiclesknowthedesiredvelocityapriori.In to [8], [14], [23]. However, we use Eq. (1) for two reasons:
[15], two DMPC controllers, both slight deviations from the 1) it models the F1Tenth vehicle’s longitudinal dynamics
controllerproposedin[14],werevalidatedonbothhardware well and 2) it uses desired velocity instead of desired
andinsimulation.Thesimulationresults,whichanalyzedthe acceleration as the control input, similar to the F1Tenth
performanceofDMPCasthesizeoftheplatoonscaledupto vehicles.
100 vehicles, showed that DMPC greatly outperforms linear We assume each vehicle in the platoon can sense the
feedback controllers. relativepositionandvelocityofthevehicleinfrontofit.Our
goal is to learn a distributed control law π : R2 → R such
B. Certifiable learning-based control thateachvehicleidrivesatthesamespeedasitspredecessor
Upon analyzing much of the prior theoretical results in i−1whilemaintainingsomepredefineddistanced i,i−1.The
platooncontrol,wearriveatthequestion:canwelearnbetter controller is distributed in the sense that each vehicle runs
controllers for platooning? Specifically, since many vehicles its controller locally based on information it receives from
ontheroadtodayareequippedwithforward-facingradarand its local sensors. The input to the controller is the error state
adaptive cruise control technology, it would be beneficial to (cid:20) p −p −d (cid:21)
x = i−1 i i,i−1 . (2)
learncontrollersforCDHspacingpoliciesandPFtopologies i v −v
i−1 i
that close the performance gap between linear feedback and
We need to ensure each vehicle is able to use the learned
DMPC, thus enabling tightly-packed platoons and greater
controller in the same way even if individual vehicle dy-
trafficcapacity.Toachievethisgoal,weneedtoensurethese
namics are different due to the τ parameters. To do this, we
learned controllers are safe and stable, which is where the i
define
emerging research areas of learning-based control [16] and
a (k)−v (k)
neural network verification [17], [18] merge into certifiable u (k)= i i , i=1,...,N. (3)
learning-based control. To date, research within certifiable i τ i
learning-based control has generally been related to either With this change of variable, the dynamics for vehicle i
reachability analysis [19] or learning Lyapunov and barrier becomes discrete-time double integrator dynamics given by
certificates [1].
s (k+1)=As (k)+Bu (k), (4)
In this work, we are interested in learning Lyapunov i i i
(cid:20) (cid:21) (cid:20) (cid:21)
1 ∆t 0
functions to guarantee stability of a platoon using a dis- A= B = .
0 1 ∆t
tributed neural network controller. One of the earlier worksWe note that without this change of variable, it would be linearprograms(MILPs)andsolvedtoglobaloptimality.We
considerably more difficult to verify a Lyapunov function do this using CVXPY [24] and Gurobi [25].
for the platoon over the range of possible τ parameters due Next, we need an algorithm to train V and π to achieve
i
tothecouplingbetweenthestatevariablesandthedynamics these objectives. A few different algorithms have been pro-
parameters. Based on Eq. (4) and the definition of the error posed,andtheygenerallyrevolvearoundminimizingtheloss
statex ,itisnothardtoshowthattheerrordynamicsofthe function given by
i
entire platoon is given by
L (x)=λ max(L (x),0)+λ max(L (x),0) (8)
V 1 pos 2 dec
x(k+1)=A¯x(k)+B¯u(k) (5)
and periodically checking if the solutions to Eqs. (7a)
where x∈R2N and u∈RN are the concatenated error and and (7b) are zero. Achieving this will ensure the system
input vectors, respectively, and is exponentially stable when using the learned controller.
However,sincewearecontrollingvehicles,weareinterested
 
 A  −B in more than just stability. For example, it is important to
 B −B 
A¯=  ...  , B¯ = 

... ...   . m pai sn si em ni gz ee rth ce omris fok ro tf (c po ril oli rs ii to yns nu(p mri bo er rity twn ou )m . b Te hr u1 s) ,a on nde ton psu ore f
A
B −B attempting to minimize L V, we also penalize values that
violate a second loss function L given by
π
These dynamics are equivalent to a string of double integra-
tors [9], thus enabling us to use various insights from the (cid:88)H
L (x(t))= (L (x )+L (x )+L (x )) (9)
relevant literature to guide our control design. For instance, π safe k comf k stab k
in [9] it was shown that a nonlinear PF controller that k=0
saturates performs better than a linear controller. Thus, we which attempts to guide the controller toward some desired
can be confident that placing bounds on the control input behavior.InEq.(9),x(t)containsthestate/errorinformation
(which would always be required in practice) can lead to for each vehicle at timestep t, and we forward simulate the
better performance. system’s dynamics over some time horizon H using x =
0
x(k) (the current state) and the platoon forward dynamics
B. Controller learning and verification given by Eq. (5) with u(k) = π(x ). The functions L ,
k safe
Our goal is to learn a neural network controller π : L comf, and L stab are hand-designed cost functions that guide
R2 → R and a Lyapunov function V : R2N → R such the controller towards generating trajectories that are safe,
thattheplatoonisexponentiallystablewithinsomecompact comfortable, and stable, respectively. Though we assume
region X containing the origin. From Lyapunov theory, this linear dynamics in this paper, this loss function is easily
is equivalent to extendedtohandlegeneraldifferentiabledynamicsfunctions,
including neural network dynamics.
V(0)=0, (6a)
V(x)>0 ∀x∈X, (6b) Algorithm 1: Guided learning of a controller and
V(A¯x+B¯π(x))−V(x)≤−ε V(x) ∀x∈X (6c) Lyapunov function.
2
Data: Initial V, π, dynamics f, D =∅, X, U,
where π(x) = (π(x ),...,π(x )). Note that depending on
1 N simulation environment env, small ε
how X is crafted, we can only guarantee stability within
some compact sublevel set of X. 1 while not converged do
Equation(6a)istriviallysatisfiedbysettingthebiasterms
2 x←argmax x∈X{L pos(x) or L dec(x)}
3 while x≥ε do
of the feedforward neural network V to zero. For neural
network verification, we cannot have strict inequalities in
4 l←L V(x)+L π(x)
the conditions we would like to verify. Thus we convert
5 D ←D∪{x} if L V(x)>0
6 V, π ←l.backwardstep()
the condition Eq. (6b) to the condition V(x) ≥ ε ∥x∥ .
1 1 7 x←f(x,π(x))
Now suppose we have learned a controller and a Lyapunov
8 end
functionandwewouldliketoverifytheconditionsEqs.(6b)
and (6c) are true. As described in [21], this can be achieved 9 l 1, l 2 ← solutions to Eqs. (7a) and (7b)
by solving the optimization problems 10 break if l 1 =0 and l 2 =0
11 V, π ←train(D) (optional)
maxL (x)=ε ∥x∥ −V(x) (7a) 12 end
pos 1 1
x∈X
maxL (x)=V(A¯x+B¯π(x))−(1+ε )V(x) (7b)
dec 2
x∈X The method we propose to concurrently learn a Lyapunov
and verifying their solutions are equal to zero. Note that function and a controller is summarized in algorithm 1. In
the two objective functions measure the violations of the this algorithm, we simulate the system starting from some
Lyapunov conditions given by Eqs. (6b) and (6c). These error state x until the system reaches the origin. The system
two optimization problems can be encoded as mixed-integer has two loops that it iterates through until convergence. Theouter loop generates an initial error state for the system by
Maximum Lyapunov condition violation
solving either Eq. (7a) or Eq. (7b) (or randomly selecting
a starting point in X). The inner loop simulates the system 0.005 positivity, N=2
decreasing, N=2
using the controller π starting from the initial error state 0.004
positivity, N=3
and maintains a dataset of points that violate the Lyapunov 0.003 decreasing, N=3
conditions.
0.002
Oncetheinnerloopconverges(whenthesystemstatecon-
0.001
vergestotheorigin),theLyapunovconditionsarechecked.If
satisfied,wehavealearnedcontrollerthatiscertifiablystable 0.000
for the system. If not, the process repeats. Optionally, we 0 25 50 75 100 125 150
can train on the dataset D using stochastic gradient descent episode
(or any other optimizer) to further update V and π. We
Fig. 2. Per episode results for the maximum Lyapunov conditions given
have found that augmenting the dataset with random points
byEq.(7a)andEq.(7b)whenrunningalgorithm1.Resultsareshownonly
within X can help speed up convergence. Another option, forN =2(convergenceatepisode30)andN =3(convergenceatepisode
as described in [21], is to augment D with any points that 150);convergenceoccurredafter8episodesforN =1.
violatetheLyapunovconditionsduringthebranchingprocess
of the MILP solver. Overall, algorithm 1 is very flexible and
It is possible other methods, like the minimax algorithm
easily adjusted to incorporate ideas from other works.
described in [21], are better suited to finding a Lyapunov
IV. EXPERIMENTS function and controller that guarantees stability. However,
A. Controller learning it is not immediately clear how we could incorporate soft
constraints like the slew constraints into this algorithm, and
Algorithm1returnsafeedbackcontrollerπ :R2 →R.For
we found that not including the slew constraints leads to
each vehicle i, the controller π takes in the vehicle’s current
overly reactive controllers that would not be comfortable
error state x , given by Eq. (2), and outputs the vehicle’s
i
(e.g., the controller selects large accelerations leading to
desired acceleration u , i.e., u = π(x ). We are able to
i i i
passenger discomfort [26]). Regardless, in [21], the largest
adjust the characteristics of this controller based on how we
state size they were able to certify was for a 3D quadrotor
design L . However, we were only able to verify a learned
π
model with 12 states (versus 6 states in this work when
controller for up to N = 3. Figure Fig. 2 shows the values
N = 3), and that took 3 days. We are interested in
of the solutions to Eqs. (7a) and (7b) for N =2 and N =3
platoonswhosestatedimensionismuchlarger.Thus,instead
while running algorithm 1.
of potentially waiting days or weeks to learn a certificate
For the experiment shown in Fig. 2, we parameterized V
using our algorithm, we quit early when trying to learn
and π using feedforward neural networks with two hidden
a true Lyapunov function and instead evaluate the learned
layers, eight neurons in each hidden layer, and leaky ReLU
controller.
activation functions with negative slope of 0.1. In general,
we have found leaky ReLU neural networks perform better
B. Hardware experiments
with this algorithm than ReLU neural networks. We see that
N =2 converges in 25 episodes whereas N =3 converges Tovalidateouralgorithmonhardware,weusetheF1Tenth
in150episodes.Eachtimebothconditionsareequaltozero, platform, modified as described in [15]. We test the vehicles
we increase the size of the error bounds X until we reach inaroughly4mby8movalracetrack,showninFig.1.We
theerrorboundsweareinterestedinverifying.Thisexplains model each vehicle’s dynamics using Eq. (1) with τ = 0.3
i
why for both N =2 and N =3, both conditions reach zero (found using a least-squares fit to experimental data). The
multiple times, but each time except the last are actually for lead vehicle uses the quadratic cost function DMPC con-
some X˜ ⊂X. trollerdescribed in[15]to trackasafe velocitybased onthe
After N =3, we were never able to achieve convergence current curvature of the course (because we evaluate against
even when using a very small X = [−0.1,0.1]Nn. We the DMPC method and we want the lead vehicle to act
allowedthealgorithmtotimeoutafter1000episodes,which similarly for all experiments). This leads to the lead vehicle
took roughly 18 hours on a computer with an Intel Core i7 assuming faster trajectories during the straight portions of
andanNvidiaGeForceRTX3080.Mostofthistimeisspent the course and slower trajectories around the curves. Each
trying to solve the MILP given by Eq. (7b) or training on following vehicle uses the neural network controller learned
thedatasetD for300epochs.Despitethefactweareunable using algorithm 1 with a desired distance of 0.75 m.
toverifythecontrollersforlargerN,wehavefoundthatthe To ensure safety of the learned controller, we use L
safe
learnedcontrollerdoesstillperformwell,anditdoesnottake to penalize when a vehicle has a distance of less than 0.25
very many episodes for the controller to begin performing m to its predecessor. To encourage smooth trajectories with
well(inonlyafewepisodesforsmallN andafterafewtens L , we penalize large actions and large changes in speed
slew
of episodes for larger N). Thus, for larger N, we ignore the fromonetimesteptothenext.WealsoaddtwoReLUlayers
verification step and run our algorithm to directly learn a to the neural network to ensure the controller only outputs
controller. commands such that |u | ≤ 3 m/s2. To guide the controller
i
noitaloivPlatoon trajectory: Neural network controller Mean RMSE with 95% confidence intervals
Position [m] Velocity [m/s] Position error Velocity error
4
vehicle Method
50
3 1 0.8 DMPC Neural network
25 2 2 Linear feedback
3 0.6
1
4
0 0
0 10 20 30 0 10 20 30 2 3 4 2 3 4
time [s] time [s] Vehicle index Vehicle index
Fig.3. Platoontrajectorywhenusingthelearnedneuralnetworkcontroller. Fig.4. Averageroot-mean-squareerrorforeachfollowervehicleoverthe
tentrials.Wecalculatetheroot-mean-squareerrorforeachvehicle’sposition
toward closed-loop stability, we use L to penalize a and velocity error over each trial, and then estimate the root-mean-square
stab error’s95%confidenceintervalbasedontheresultsoverthetentrials.
runningsumofthedifferenceinerrorfromthenexttimestep
to the current timestep over the horizon (because we ideally train a controller using a desired distance of 5 m. We use
want the errors to decrease). Other choices for L stab can try an almost identical setup as in the hardware section, except
to guide the controller towards improving the convergence weusetwohiddenlayerswith32neuronseach,wepenalize
rate of the system to the origin. We ran algorithm 1 with a vehicle for getting within 2 m of its predecessor, and we
N = 5, where both V and π are two-layer feedforward use a heterogeneous platoon where the dynamics parameter
neural networks with 8 neurons in each hidden layer and τ ∈ [0.2,0.8] is selected at random. As mentioned in the
i
leaky ReLU activation functions with negative slope 0.1 for previous section, we cannot verify the Lyapunov certificates
around 100 episodes to learn a controller. because 1) the search space becomes very large and 2)
The platoon’s trajectories during a trial run when using the number of binary variables required to solve Eq. (7b)
the learned controller are shown in Fig. 3. Since a platoon increaseslinearlywiththeplatoonsizeleadingtopotentially
is perturbed whenever the lead vehicle’s velocity changes, exponentialincreaseinthetimerequiredtosolvetheMILPs.
wetestascenariowheretheleadvehicledrivesaggressively However, we still find that training for a very small amount
throughthecoursefor30secondsbeforecomingtoasudden of time (around 100 simulated episodes) yields controllers
stop. This results in a trajectory that accelerates to almost 4 that perform well in test scenarios.
m/sduringthestraightsectionsofthecourseanddecelerates As shown in [15], there is a large gap in the performance
toaround1.5m/swhilenavigatingthecurves.Aswecansee, of DMPC controllers and linear feedback controllers as pla-
the follower vehicles do not drive as aggressively, since we toon size scales. Though DMPC should always outperform
attempted to guide the controller towards taking aggressive feedback methods due to the advantage of being able to
actions if the vehicles are too close (i.e., close to crashing), communicateplannedtrajectories,itwouldbeusefulinprac-
but didn’t penalize being too far away in the same manner. tice if a feedback controller can perform similarly well. For
Werepeatedthisexperimenttentimesfortheneuralnetwork example,inthescenariowherecommunicationistemporarily
controller and compared it to the performance for the same lost,itwouldbebeneficialtohaveafeedbackcontrollerthat
quadraticDMPCandlinearfeedbackcontrollersinvestigated can take the place of the DMPC method without disrupting
in [15]. Results comparing the average root-mean-square the platoon’s motion. Thus, we again compare our learned
error (RMSE) for each follower vehicle are shown in Fig. 4. controller to DMPC and linear feedback, with the goal
Based on the results in Figs. 3 and 4, it is difficult of bridging the gap between DMPC methods and linear
to determine which algorithm performs the best. We do feedback methods.
note that when using the neural network controller, zero For the simulation experiments, we have the first vehicle
collisions occurred between vehicles or with the course track a velocity profile that starts at 20 m/s, accelerates to
boundaries. With the other two controllers, however, vehi- 25 m/s, then decelerates back to 20 m/s. We add Gaussian
cles would sometimes collide with the course boundaries. noisetoboththeforwarddynamicsandtothesensingofthe
Further, with the linear feedback controller, vehicles would position/velocity errors to ensure the learned controller still
sometimes collide with one another when braking at the works in a noisy setting. Note that for a vehicle traveling
end of the experiment. We speculate that the collisions with 20 m/s, common practice would suggest a spacing of 40
the boundaries occurred because the controllers attempted m [11], so a 5 m spacing is aggressive for this scenario.
to follow the predecessor’s velocity profile too aggressively The simulation results for the neural network controller
when rounding the curves, leading to lateral slipping. are shown in Fig. 5. As expected when using a feedback
controller with a PF topology, the errors do propagate down
C. Simulation experiments
the platoon, leading to the vehicles further away needing to
For a platoon using a PF topology and a CDH spacing overcompensatebyspeedingupandslowingdownmorethan
policy, ensuring safety becomes more difficult as the pla- vehicles earlier in the platoon. However, the controller does
toon size increases [7]–[9]. To investigate the limits of our stabilizetheplatoontothetruedesiredvelocitiesquicklyand
approach, we simulate a platoon of N = 100 vehicles and without any collisions within the platoon.
ESMRSimulated trajectory: Neural network controller
Position [m] Velocity [m/s]
vehicle
1
1000 30 20
40
60
0 20 80
100
ref
0 20 40 60 0 20 40 60
time [s] time [s]
Fig. 6. RMSE analysis for a simulated platoon of 100 vehicles (blue:
DMPCcontroller,orange:neuralnetworkcontroller,green:linearfeedback
Fig. 5. Simulated trajectories for a platoon of 100 vehicles. The lead controller).Thezoomedinportionofthegraphenablesustoseethetradeoff
vehicle tracks the velocity profile shown by the reference velocity (black betweenthefeedbackcontrollersandaDMPCcontroller.
line)inthevelocityplot.Thefadedlinesinthepositionplotshowthetrue
desiredpositionofeachvehicle.
feedbackcontrollerused.Thelevelsetsoftheneuralnetwork
controllerpaintaclearpictureaboutwhattheneuralnetwork
Similar to the hardware experiments, we compare the
learned to value.
neural network controller to the DMPC controller and a
When a vehicle is too close to and is traveling faster than
linear feedback controller by simulating the same scenario
itspredecessor(i.e.,bothpositionerrorandvelocityerrorare
10 times and evaluating the RMSE for each vehicle over
lessthan0,thebottomleftquadrant),thecontroller’soutputs
the 10 trials. We note that collisions did not occur for the
quickly ramp up to return the maximum deceleration of -3.
DMPCorneuralnetworkcontrollersunderthisexperimental
Similarly, the controller quickly corrects based on velocity
scenario, but the linear feedback controller leads to many
error in the top left and top right quadrants. On the other
collisions for vehicles further down the platoon, and should
hand, when a vehicle is too far away but traveling faster
notbeusedforlargeplatoonsusingCDHinpractice.Results
than the predecessor (the bottom right quadrant), there is
over the 10 trials are shown in Fig. 6. Clearly, the neural
less urgency. This is because this scenario is both conducive
network controller sacrifices performance for small platoons
tostability(positionerrorwilldecrease)andisthemostsafe.
toobtainimprovedperformanceonlargerplatoons.However,
Thus, we see that unless the vehicle’s state is in the bottom
we see that the individual vehicle’s spacing RMSE values
right quadrant, the controller heavily weighs the velocity
were higher for smaller platoon sizes and the average values
error as opposed to the position error.
over the 10 trials is much noisier.
Considering these qualitative observations, the level sets
D. Discussion appear to approximate a piecewise-linear function before
saturating at the control limit. Future research could try
The algorithm proposed in this work and in other prior
to use this information to learn saturating piecewise-linear
works do not work well with regard to learning a Lyapunov
controllers that are provably stable via stability analysis
function for very high-dimensional systems. However, these
similar to [9] instead of via a learned Lyapunov certificate.
algorithms used to concurrently learn a Lyapunov function
and a controller do generate controllers that can perform
betterthanhand-designedcontrollersandtranslatewellfrom Controller level sets
simulation to the real world. Neural network Linear feedback
To try to improve the convergence rate of our algorithm, 1.0 8
2 6
we attempted to “warm-start” the Lyapunov function by 0.5 3 4
2
sharing weights from a previously verified Lyapunov func- 0.0 0 1 0
-1 tion with smaller N. However, we did not find that this 0.5 -2 -4-2
-3
improvedperformance.Instead,convergencerateswerevery
1.0
-8 -6
inconsistent even for small N. For N = 2, the algorithm 1.0 0.50.0 0.5 1.0 1.0 0.50.0 0.5 1.0
would often converge around 50 episodes but could take position error [m] position error [m]
as many as 200 episodes. For N = 3 we have only seen
Fig.7. Levelsetsforthelearnedneuralnetworkcontrollerandthelinear
it converge once within 500 episodes, as shown in Fig. 2.
feedback controller studied. Clearly, the learned controller is a nonlinear
So the actual performance of these algorithms with respect functionofthetwo-dimensionalinput.
to convergence seems to depend more on neural network
initializationortherandomlygenerateddataset,whichisnot
V. CONCLUSIONS
a great property for an algorithm.
One potentially useful application of our algorithm is We present an algorithm to learn a controller for dynamic
the ability to use the resulting controllers to guide control systems. In cases where the state dimension is small, the
design.Figure7showsthelevelsetsofthelearnedcontroller algorithm permits learning a Lyapunov function that guar-
we used in simulation and the level sets of the linear antees stability of the closed-loop system. The benefit of
]m[
rorre
yticolevour algorithm is the option to design an loss function that [9] H.HaoandP.Barooah,“Stabilityandrobustnessoflargeplatoonsof
guidesthecontrollertobehaveinadesiredway.Weapplythe vehicles with double-integrator models and nearest neighbor interac-
tion,”InternationalJournalofRobustandNonlinearControl,vol.23,
algorithm to a platooning problem with an intrinsic tradeoff
no.18,pp.2097–2122,2013.
between traffic throughput/fuel efficiency and vehicle safety. [10] G.J.L.Naus,R.P.A.Vugts,J.Ploeg,M.J.G.vandeMolengraft,
Duetoachangeofvariable,wecanlearnasinglecontroller andM.Steinbuch,“String-stablecaccdesignandexperimentalvalida-
tion:Afrequency-domainapproach,”IEEETransactionsonVehicular
for a platoon of heterogeneous vehicles, and we investigate
Technology,vol.59,no.9,pp.4268–4279,2010.
the performance of a controller learned using our algorithm [11] K.Vogel,“Acomparisonofheadwayandtimetocollisionassafety
on a platoon of four F1Tenth vehicles and in a simulated indicators,” Accident analysis & prevention, vol. 35, no. 3, pp. 427–
433,2003.
environmentwith100vehicles.Weshowthatwecantrainan
[12] D. Swaroop, J. Hedrick, C. Chien, and P. Ioannou, “A comparison
algorithm in simulation and obtain performance that bridges of spacing and headway control laws for automatically controlled
the gap between DMPC (the state-of-the-art when vehicles vehicles,”VehicleSystemDynamics,vol.23,no.8,1994.
[13] W. B. Dunbar and D. S. Caveney, “Distributed receding horizon
can communicate with one another) and classical linear
controlofvehicleplatoons:Stabilityandstringstability,”IEEETrans-
feedback controllers. actionsonAutomaticControl,vol.57,no.3,pp.620–633,2012.
Future work will consider explicit safety certificates (bar- [14] Y.Zheng,S.E.Li,K.Li,F.Borrelli,andJ.K.Hedrick,“Distributed
model predictive control for heterogeneous vehicle platoons under
rier functions) along with the stability certificates. Further,
unidirectional topologies,” IEEE Transactions on Control Systems
it is possible that encoding some different notion of state, Technology,vol.25,no.3,pp.899–910,2017.
either by using a history of past errors as input into the [15] M. H. Shaham, R. Ranjan, E. Kirda, and T. Padir, “Design and
realization of a benchmarking testbed for evaluating autonomous
neural network or using a recurrent neural network, could
platooningalgorithms,” in 2023InternationalSymposium onExperi-
leadtobetterperformance.Wehopetoinvestigatethisusing mentalRobotics. SpringerInternationalPublishing,2024.
either this algorithm or similar (e.g., reinforcement learning) [16] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati,
and A. P. Schoellig, “Safe learning in robotics: From learning-based
to investigate the performance limits of feedback controllers
control to safe reinforcement learning,” Annual Review of Control,
forplatoons usinga CDHspacingpolicy anda PFtopology. Robotics,andAutonomousSystems,vol.5,no.1,pp.411–444,2022.
[17] C.Liu,T.Arnon,C.Lazarus,C.Strong,C.Barrett,andM.J.Kochen-
ACKNOWLEDGMENT
derfer,“Algorithmsforverifyingdeepneuralnetworks,”Foundations
andTrends®inOptimization,vol.4,no.3-4,pp.244–404,2021.
Research was sponsored by the DEVCOM Analysis Cen- [18] M. Everett, “Neural network verification in control,” in 2021 60th
IEEE Conference on Decision and Control (CDC), 2021, pp. 6326–
ter and was accomplished under Cooperative Agreement
6340.
NumberW911NF-22-2-001.Theviewsandconclusionscon- [19] M.Everett,G.Habibi,C.Sun,andJ.P.How,“Reachabilityanalysis
tained in this document are those of the authors and should ofneuralfeedbackloops,”IEEEAccess,vol.9,pp.163938–163953,
2021.
not be interpreted as representing the official policies, either
[20] Y.-C. Chang, N. Roohi, and S. Gao, “Neural lyapunov control,”
expressed or implied, of the Army Research Office or the Advancesinneuralinformationprocessingsystems,vol.32,2019.
U.S. Government. The U.S. Government is authorized to [21] H.Dai,B.Landry,L.Yang,M.Pavone,andR.Tedrake,“Lyapunov-
stable neural-network control,” in Proceedings of Robotics: Science
reproduce and distribute reprints for Government purposes
andSystems,Virtual,July2021.
notwithstanding any copyright notation herein. [22] Z. Qin, K. Zhang, Y. Chen, J. Chen, and C. Fan, “Learning safe
multi-agent control with decentralized neural barrier certificates,” in
InternationalConferenceonLearningRepresentations,2020.
REFERENCES
[23] S.Stankovic,M.Stanojevic,andD.Siljak,“Decentralizedoverlapping
control of a platoon of vehicles,” IEEE Transactions on Control
[1] C.Dawson,S.Gao,andC.Fan,“Safecontrolwithlearnedcertificates:
SystemsTechnology,vol.8,no.5,pp.816–832,2000.
A survey of neural lyapunov, barrier, and contraction methods for
[24] S. Diamond and S. Boyd, “CVXPY: A Python-embedded modeling
roboticsandcontrol,”IEEETransactionsonRobotics,vol.39,no.3,
language for convex optimization,” Journal of Machine Learning
pp.1749–1767,2023.
Research,vol.17,no.83,pp.1–5,2016.
[2] S. Magdici and M. Althoff, “Adaptive cruise control with safety
[25] Gurobi Optimization, LLC, “Gurobi Optimizer Reference Manual,”
guarantees for autonomous vehicles,” IFAC-PapersOnLine, vol. 50,
2023.[Online].Available:https://www.gurobi.com
no.1,pp.5774–5781,2017,20thIFACWorldCongress.
[26] K.N.deWinkel,T.Irmak,R.Happee,andB.Shyrokau,“Standards
[3] K.-Y.Liang,J.Mårtensson,andK.H.Johansson,“Heavy-dutyvehicle
forpassengercomfortinautomatedvehicles:Accelerationandjerk,”
platoonformationforfuelefficiency,”IEEETransactionsonIntelligent
AppliedErgonomics,vol.106,p.103881,2023.
TransportationSystems,vol.17,no.4,pp.1051–1061,2016.
[4] M.O’Kelly,H.Zheng,D.Karthik,andR.Mangharam,“F1tenth:An
open-source evaluation environment for continuous control and rein-
forcementlearning,”inNeurIPS2019CompetitionandDemonstration
Track. PMLR,2020,pp.77–89.
[5] W.LevineandM.Athans,“Ontheoptimalerrorregulationofastring
ofmovingvehicles,”IEEETransactionsonAutomaticControl,vol.11,
no.3,pp.355–361,1966.
[6] H.-S. Tan, R. Rajamani, and W.-B. Zhang, “Demonstration of an
automated highway platoon system,” in Proceedings of the 1998
American Control Conference. ACC (IEEE Cat. No.98CH36207),
vol.3,1998,pp.1823–1827vol.3.
[7] P.Seiler,A.Pant,andK.Hedrick,“Disturbancepropagationinvehicle
strings,”IEEETransactionsonAutomaticControl,vol.49,no.10,pp.
1835–1842,2004.
[8] Y. Zheng, S. Eben Li, J. Wang, D. Cao, and K. Li, “Stability and
scalabilityofhomogeneousvehicularplatoon:Studyontheinfluence
ofinformationflowtopologies,”IEEETrans.Intell.Transport.Syst.,
vol.17,no.1,pp.14–26,2016.