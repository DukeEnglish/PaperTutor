LaPA: Latent Prompt Assist Model For Medical Visual Question Answering
TianchengGu KaichengYang DongnanLiu
UniversityofSydney DeepGlint UniversityofSydney
Sydney,NSW,Australia Beijing,China Sydney,NSW,Australia
tigu8498@uni.sydney.edu.au kaichengyang@deepglint.com dongnan.liu@sydney.edu.au
WeidongCai
UniversityofSydney
Sydney,NSW,Australia
tom.cai@sydney.edu.au
Abstract corresponding questions by physicians is both costly and
error-prone[16]. Toaddressthischallenge, therehasbeen
Medical visual question answering (Med-VQA) aims to a growing interest in the development of automatic Med-
automatethepredictionofcorrectanswersformedicalim- VQA techniques [2,6,10,21,24,33]. While deep learn-
ages and questions, thereby assisting physicians in reduc- ing models have achieved remarkable success in predict-
ingrepetitivetasksandalleviatingtheirworkload. Existing ing accurate answers in standard visual-question answer-
approaches primarily focus on pre-training models using ing (VQA) tasks by given images and questions [11,32],
additional and comprehensive datasets, followed by fine- Med-VQA poses unique challenges [2]. The size of Med-
tuningtoenhanceperformanceindownstreamtasks. How- VQA datasets is relatively small, and medical images are
ever, there is also significant value in exploring existing complex and challenging due to the small region of in-
modelstoextractclinicallyrelevantinformation.Inthispa- terest related to the disease that physicians need to focus
per,weproposetheLatentPromptAssistmodel(LaPA)for on [8,29]. Consequently, extracting clinically relevant in-
medicalvisualquestionanswering. Firstly,wedesignala- formationfrommedicalimagesbecomesadifficulttaskfor
tentpromptgenerationmoduletogeneratethelatentprompt themodel[28].
with the constraint of the target answer. Subsequently, NumerousMed-VQAmethods[2,10,21,33]havebeen
we propose a multi-modal fusion block with latent prompt proposed to address the aforementioned challenges and
fusion module that utilizes the latent prompt to extract have demonstrated impressive performance. For instance,
clinical-relevant information from uni-modal and multi- methodssuchasMEVFmodel[24],MMQmodel[4],and
modal features. Additionally, we introduce a prior knowl- CPCR [19] have proposed pretraining the model using ex-
edge fusion module to integrate the relationship between ternalcomplementarydatasetstoenhancethemodel’sana-
diseasesandorganswiththeclinical-relevantinformation. lyticalcapabilities,followedbyfine-tuningfordownstream
Finally, we combine the final integrated information with tasks. Similarly, M2I2 model [15] and m3ae model [2]
image-language cross-modal information to predict the fi- have utilized self-supervised learning to enable the model
nal answers. Experimental results on three publicly avail- to autonomously learn clinical features from both image
able Med-VQA datasets demonstrate that LaPA outper- and language modalities. Notably, despite their remark-
forms the state-of-the-art model ARL, achieving improve- able achievements, none of these approaches consider the
mentsof1.83%,0.63%,and1.80%onVQA-RAD,SLAKE, latent prompt. However, the latent prompt is a crucial as-
andVQA-2019,respectively. Thecodeispubliclyavailable pect that warrants research attention due to its enhanced
athttps://github.com/GaryGuTC/LaPA model. flexibility in information extraction, as evidenced by its
widespread utilization in the field of natural language pro-
cessing[9,26,35].
1.Introduction
This study presents the LaPA (Latent Prompt Assist)
Medicalvisualquestionanswering(Med-VQA)playsa modelformedicalvisualquestionanswering(Med-VQA),
critical role in disease detection and diagnosis. In clinical asillustratedinFig.1.TheLaPAmodelincorporatesthela-
practice, thereviewofnumerousmedicalimagesandtheir tentprompttofilterdifferentmodalinformationandextract
4202
rpA
91
]VC.sc[
1v93031.4042:viXraImage ×N
Self Attention
Extractor Projection
Probabilities
Image Cross-modal
Attention Yes
Language
Question Self Attention Projection
Extractor
Latent
Prompt
Prior Knowledge Fusion Module
Generation Latent Prompt Fusion Module
Module
Latent Prompt
Organ Disease
Adjacent Matrix
Multi-Modal Fusion Block Features
Total Answers Prior Knowledge
Figure1. TheoverallstructureofourproposedLaPAmodel. Theinputfeatureisdenotedbyablockwithroundedcorners, whilethe
square-angledstructurerepresentsamodule. Thelanguageandimagepipelinesarerepresentedbygreenandbluemodules,respectively.
The final tokens in blue, green, and red correspond to the cross-modal image, language, and integrated information, respectively. For
optimalviewing,itisrecommendedtozoominfordetailedexamination.
clinic-relevant information, aiding in the prediction of the 2.RelatedWorks
finalanswer. Firstly,weintroducethelatentpromptgener-
Prompt Learning. Prompt learning is a research focus
ationmodule,whichgeneratesthelatentprompt.Thelatent
aimed at leveraging prompts to enhance various aspects
prompt interacts with the total answer tokens and is con-
of a model’s performance, such as efficiency, flexibility,
strainedbythetargetanswertokenstofocusontherelevant
and knowledgetransfer [9,37,38]. Recent studies[26,27]
tokensassociatedwiththetargetanswer. Subsequently,the
have explored the utilization of prompts to extract rele-
latent prompt is fed into the multi-modal fusion block to
vant information from pre-trained models for downstream
fuse with uni- and multi-modal information, enabling the
tasks,yieldingpromisingresults. Notably,theChatExtract
filtering of different modal information and extraction of
method proposed by [27] employs engineered prompts to
clinic-relevantdetails.Additionally,thelatentpromptinter-
aid in sentence differentiation and data extraction, thereby
actswiththepriorknowledgederivedfromtherelationship
improving answer accuracy. In contrast, [35] focuses on
between organs and diseases, obtained from a knowledge
using latent prompts, encompassing controlled and uncon-
graph[18],resultinginthegenerationofthefinalinteracted
trolled signals, to extract valuable and highly relevant in-
information to further assist in the prediction of the final
formation, thereby enhancing text summarization quality.
answer. Lastly,thelatentpromptcombineswiththeimage-
Building upon these studies, we introduce the concept of
language cross-modal information to produce the final an-
latentpromptstothedomainofMed-VQA.
swer.
Themaincontributionsofourworkcanbesummarized
asfollows:
Medical Visual Question Answering. The field of au-
• We propose the latent prompt generation model that tomatic prediction of answers for medical visual ques-
generatesalatentpromptandutilizeamulti-modalfu- tions based on medical images has been extensively stud-
sionblocktofilterdifferentmodalinformationandex- ied, yieldingnumerousnotableworks[10,21,24,33]. No-
tractclinic-relevantinformation. tably,someapproacheshavebeenproposedtotrainmodels
based on external knowledge, such as MEVF model [24]
• We leverage prior knowledge regarding the relation-
andMMQmodel[4]. Thesemethodsinitializetheweights
ship between organs and diseases by employing a
of specific modules (e.g., visual encoder or decoders) us-
graphneuralnetworktointeractwiththelatentprompt,
ing pre-trained large language models (LLMs) and subse-
ultimatelyassistinginanswerprediction.
quently fine-tune the overall frameworks for downstream
• OurproposedLaPAmodeldemonstratesitseffective- Med-VQAtasks. Q2ATransformer[21]introducesanovel
ness by achieving exceptional performance on VQA- approach that combines the advantages of both classifica-
RAD[12],SLAKE[18],andVQA-2019[1]datasets. tion and generation techniques, achieving a unified treat-Latent Final
Prompt Integrated Integrated
Information Information
C Adjacent Matrix
Cross Attn MM ou dlt ai- l Cross Attn Compress asthma pneu.
Projection lung
Consistent cancer
Projection
Main
Image Cross Attn Cross Attn
Bone
heart
Self Attn GNN arrhy. Osteo.
Language Cross Attn
Embedding
Organ
Latent Total Target Latent Integrated Disease
Prompt Answers Answer Prompt Information
(a) Latent Prompt Generation Module (b) Latent Prompt Fusion Module (c) Prior Knowledge Fusion Module
Figure 2. The structure of the main modules in LaPA is illustrated as follows: (a), (b), and (c) represent the latent prompt generation
module(Sec.3.1),thelatentpromptfusionmodule(Sec.3.2),andthepriorknowledgefusionmodule(Sec.3.3),respectively.Foroptimal
visualization,itisrecommendedtozoominfordetailedexamination.
mentforclosed-endandopen-endquestions.Byemploying cus on the tokens associated with the answer. To this end,
learnable candidate answer embeddings, Q2ATransformer we treat all the answer tokens in the downstream datasets
queriesthepresenceofeachanswerclassforagivenimage- aspriorknowledge,embeddingthemasfeaturesX using
TA
question pair. Additionally, MeDVInt [34] and LLaVA- RoBERTa [20]. Subsequently, the total answer tokens un-
Med[13]aregenerativemodelsforMed-VQAunderstand- dergoself-attentionfollowedbyaprojectionlayertoobtain
ing that align visual information from a pre-trained vision thetotaltokenfeaturesF asfollows:
TA
encoderwithalargelanguagemodel(LLM)orlargevision
language model such as ChatGPT and LLaVA. In contrast F TA =Proj(SA(X TA)), (1)
to these existing works, our proposed approach utilizes la-
where SA(·) and Proj(·) represent self-attention mecha-
tentpromptstofilteruni-andmulti-modalinformationand
nism and projection layer respectively. After that, we em-
extract clinic-relevant information, thereby enhancing the
ploycross-attentiontointegratethetotalanswertokenswith
finalanswerpredictionprocess.
thelatentprompt:
3.LaPAModel Xˆ =CA(X ,F ,F ), (2)
LP LP TA TA
The architectural overview of our proposed LaPA (La-
whereCA(·)representsthecross-attentionmechanism[30]
tent Prompt Assist) model for medical visual question an-
withthequery,keyandvalueasinput.Tofocusonanswers-
swering is presented in Fig. 1. The model comprises
related tokens, we introduce a consistent loss L to con-
CS
three key components: the latent prompt generation mod-
strain the latent prompt with the target answer, thereby
ule(Sec.3.1),themulti-modalfusionblock(Sec.3.2),and
bringingitclosertothetargetanswerinthesemanticspace.
the prior knowledge fusion module (Sec. 3.3). Further in-
Theprocessisdefinedas:
sightsintothetrainingprocesscanbefoundinSec.3.4.
Xˆ⊤ X
3.1.LatentPromptGenerationModule L (Xˆ ,X )=1− LP A , (3)
CS LP A ||Xˆ ||||X ||
LP A
We first propose a latent prompt generation mod-
ule (Fig. 2 (a)) to generate the learnable latent prompt, whereX Aisthetokenembeddingsofthetargetanswer.
which is initialized using the normal distribution. To im-
3.2.Multi-modalFusionBlock
prove training efficiency and performance, we interact the
generated latent prompt with total answer tokens. Under To make the latent prompt fully extract clinic-relevant
the constraint of answer tokens, the latent prompt can fo- information from uni-modal and multi-modal information,we introduce a multi-modal feature fusion block. As 3.3.PriorKnowledgeFusionModule
shown in Fig. 1, the image features and language fea-
Followingthepreviousworks[31,36],weincorporatea
tures are extracted by the Swin Transformer [22] and the
prior knowledge graph [18] that captures the relationships
RoBERTa [20], and the uni-modal features F and F can
I L betweenorgansanddiseasestoenhancetheaccuracyofan-
beobtainedthroughself-attentionasfollows:
swer prediction in Med-VQA. We employ a graph neural
network (GNN(·)) to analyze the organ-disease relation-
F =SA(E (X )), (4)
I I I ships and improve the performance of answer prediction.
Additionally,weproposeapriorknowledgefusionmodule
that integrates the prior knowledge with the integrated in-
F L =SA(E L(X L)). (5) formationtofacilitatethefinalanswerprediction.
AsdepictedinFig.2(c),theadjacentmatrixX isde-
adj
After that, the image and language features are fused
rivedfromtheaforementionedpriorknowledge[18],repre-
through the cross-attention to get the multi-modal features
sentingtherelationshipbetweenorgansanddiseasesusing
F :
MM binary values (0 and 1). The organ-disease feature F
OD
is tokenized and embedded using RoBERTa [20]. Subse-
F =[Proj(CA(F ,F ,F ));Proj(CA(F ,F ,F ))],
MM I L L L I I quently,itisfedintotheGNNmoduletoextractvaluablein-
(6)
formationregardingtheorgan-diseaserelationshipsdenoted
whereProj(·)representstheprojectionlayer.
asF ,whichcanbesummarizedasfollows:
G
Aftergettingtheuni-modalandmulti-modalfeaturesF ,
I
F , and F , we design the latent prompt fission mod-
L MM F =GNN(F ,X ). (10)
G OD adj
ule(Fig.2(b))tomakethelatentprompttointegrateclinic-
Then, the extracted information is combined with the pre-
relevantinformationthroughcross-attention:
viousintegratedinformationx togetthefinalintegrated
(cid:101)LP
information(Xˆ ),andtheprocessisindicatedbelow:
II
X =CA(Xˆ ,F ,F ), (7)
II LP I I Xˆ =[X˜ ;Proj(CA(F ,X ,X ))], (11)
II II G LP LP
whereCA(·)isthecrossattentionmechanismandProj(·)
Xˇ II =CA(X II,F L,F L), (8) is the projection layer. Finally, the interacted relationship-
based features will concatenate ([;]) with latent prompt as
the final integrated information to assist the final answer
X˜ =CA(Xˇ ,F ,F ), (9) predictedforMed-VQA.
II II MM MM
3.4.TrainingDetails
where CA(·) is the cross-attention mechanism. The X
II
represents the integrated information obtained by combin- Aftertheprocessesmentionedabove,weaddthecross-
ain ng dl ta ht een X˜t IIpr do em nop tt es tw heit ih nti em ga rag te edfe ia nt fu or re ms. atS ioim nri ela sr ul ly t, inth ge frX oˇ mII tm ioo nda inl i tn hf eo lr am sta mtio un ltiF -mFI odan ald fuF sF ioL no bf loth ce kc wro its hs- tm heo fida nl ala it nte tn e--
the fusion of language features and multi-modal features, gratedinformationXˆ topredicttheanswer:
II
respectively. Thefusionprocessfollowsasequentialorder,
X =αXˆ +θF +βF , (12)
wherelanguagefeaturesareintegratedfirst,followedbyim- F II FI FL
ages,andfinallymulti-modalfeatures. Wehaveconducted where α, θ, and β are weight to balance different types of
experimentstoexplorevariousapproachesforinformation information. Thisfinaltotalloss(L )isshownbelow:
T
fusionandextraction,andthecurrentformyieldsthemost
L =L (X ,F )+ηL , (13)
optimalresults. T BCE F T CS
In the multi-modal fusion module, the latent prompt is where the L is the binary cross-entropy loss [7] and
BCE
utilizedtointegratewithlanguagefeaturestoextractclini- L is the consistent loss used to minimize the semantic
CS
callyrelevantinformationwithinthetextualsemanticspace. distancebetweenthelatentpromptandthetargetanswer. η
Subsequently,itiscombinedwithimagefeaturestoextract isalossweighttoadjusttheinfluenceofdifferentlosses.
clinically relevant information within the image semantic
space. Finally,theintegratedinformationundergoesfusion 4.ExperimentsandResults
withthecombinedlanguage-imagecross-modalfeaturesto
4.1.ImplementationDetails
filteroutdiversemodalinformationandconsolidatetheuni-
modalfeaturesofbothlanguageandimage,alongwiththeir For our model, we adopted the Swin-Transformer [22]
multi-modal combination features, resulting in the genera- as the image extractor model, RoBERTa [20] as the lan-
tionofthefinalclinicallyrelevantinformation. guage extractor model, the graph attention network [31]VQA-RAD SLAKE VQA-2019
Method Venue
Open Closed Overall Open Closed Overall Overall
BAN[11] NeurIPS 37.40 72.10 58.30 74.60 79.10 76.30 -
18
CPRD-BAN[17] MICCAI 52.50 77.90 67.80 79.50 83.40 80.10 -
21
MMBERT[10] ISBI 63.10 77.90 72.00 - - - 67.20
21
M3AE∗†[2] MICCAI 64.80 82.72 75.61 79.22 85.10 81.53 78.40
22
M2I2[15] ISBI 61.80 81.60 73.70 74.70 91.10 81.20 -
22
ARL∗[3] MM 65.10 85.96 77.55 79.70 89.30 84.10 79.80
22
PubMedCLIP[5] EACL 60.10 80.00 72.10 78.40 82.50 80.10 -
23
CPCR[19] TMI 60.50 80.40 72.50 80.50 84.10 81.90 -
23
LaPA Ours 68.72 86.40 79.38 82.17 88.70 84.73 81.60
Table1. TheresultsoftheLaPAmodelandothertestedmodelsinVAR-RAD,SLAKEandVQA-2019. ∗ indicatesthatwetestedthe
resultsourselves,whichmaydifferfromthosereportedinthemodels’originalpapers.†denotesthebaselinemodel.Theresultsforother
modelswereobtainedfromtheiroriginalpapers.Thehighest-performingresultineachcategoryishighlightedinboldforclarity.
VQA-RAD SLAKE VQA-2019
# Method
Open Closed Overall Open Closed Overall Overall
1 BL. 64.80 82.72 75.61 79.22 85.10 81.53 78.40
2 +GM. & LF. 68.16 84.93 78.27 80.93 87.74 83.60 80.80
w/ocs
3 +GM.& LF. 69.27 85.29 78.94 81.24 87.50 83.70 81.30
4 +GM.& LF.+PF. 68.72 86.40 79.38 82.17 88.70 84.73 81.60
- ∆ ↑3.92 ↑3.68 ↑3.77 ↑2.95 ↑3.60 ↑3.20 ↑3.20
Table 2. The ablation study for the LaPA model was conducted on the VQA-RAD, SLAKE, and VQA-2019 datasets to ascertain the
contributionofindividualcomponentstotheoverallperformance.Inthiscontext,GM.,LF.,andPF.representthelatentpromptgeneration
module, latentpromptfusionmodule, andpriorknowledgefusionmodule, respectively. Thetermw/ocsdenotestheexclusionofthe
consistencymethodfromthemodelconfiguration. ThefinalrowdelineatestheperformanceenhancementachievedbytheLaPAmodel
relativetotheestablishedbaselinemodel.
witheightheadsastheGNNmodel,andutilizedsixmulti- arecategorizedintotwotypes: open-ended(free-form)and
modal fusion blocks. Training was conducted on a single closed-ended (YES/NO) forms. VQA-RAD dataset con-
NVIDIAGeForceRTX3090GPUwith24GBmemory,em- sists of 315 radiology images with 3064 question-answer
ploying half-precision training. Following the approach in pairs, and a subset of 451 pairs was used for testing pur-
M3AE [2], we utilized the AdamW optimizer [23] with poses. SLAKE dataset is composed of 642 radiology im-
a learning rate of 5e-6 for optimization. The input im- ages, with14028question-answer(QA)pairs. Thedataset
ageswereresizedto384×384,andthefeaturedimension wasdividedintoaratioof70:15:15fortraining,validation,
was set to 768. Furthermore, we utilized the pre-training and testing, respectively. It it worth noting that we only
weightsfromtheM3AEmodel,whichwerepre-trainedon evaluatedtheEnglishsubsetofSLAKE.VQA-2019dataset
theROCO[25]andMedICaT[28]datasets. Forevaluation comprises 3200 medical images, with 12792 QA pairs for
purposes,wereportthematchingaccuracyforbothclosed- training,500imageswith2000QApairsforvalidation,and
set and open-set questions. The overall metrics are calcu- 500imageswith500QApairsfortesting.
latedbycombiningtheresultsfromopen-setandclosed-set
questionsusingcoefficients,asoutlinedinM3AE[2]. 4.3.ComparisonExperiments
Our proposed LaPA model was benchmarked against
4.2.Datasets
eight contemporary state-of-the-art (SOTA) Med-VQA
Inordertocomprehensivelyevaluatetheeffectivenessof methodologies: BAN [11], CPRD [17], MMBERT [10],
our proposed method, we conducted experiments on three M3AE [2], M2I2 [15], ARL [3], PubMedCLIP [5] and
widely-used Med-VQA benchmarks: VQA-RAD [12], CPCR [19]. As delineated in Tab. 1, LaPA consistently
SLAKE [18], and VQA-2019 [1]. The dataset splits pro- surpassed the aforementioned models on all three datasets
vided by existing works, such as M3AE [2], were used in in the majority of evaluative metrics. Notably, for the
ourexperiments. ThequestionsinVQA-RADandSLAKE VQA-RAD dataset, our model demonstrated a consider-(a) VQA-RAD (b) SLAKE (c) VQA-2019
Figure3.Ablationontheθandβ.
VQA-RAD SLAKE VQA-2019
InteractOrder
Open Closed Overall Open Closed Overall Overall
I.⇒L.⇒MM. 55.31 84.56 72.95 81.40 87.74 83.88 78.93
L.⇒I.⇒MM. 68.72 86.40 79.38 82.17 88.70 84.73 81.60
Table3.Theresultsofthechangeinthefusiondirectionbylatentpromptinthelatentpromptfusionmodule.TheI.,L.,andMM.arethe
abbreviationsofimage,language,andmulti-modal.
LatentPromptsize 4 8 16 32 64 128 256
VQA-RAD 77.16 78.27 78.49 79.38 76.94 76.49 75.61
SLAKE 84.17 84.35 83.88 84.73 84.26 84.26 84.45
VQA-2019 80.00 80.53 79.47 81.60 79.47 78.93 80.00
Table4.Ablationonthelatentpromptsize.
ableenhancementinperformanceacrossallquestiontypes, complexity.Consequently,thesemodelswerealsoexcluded
achieving an overall accuracy of 79.38%, an improvement fromourcomparativeanalysis.
of 1.83 percentage points over the second-best model. In
the SLAKE dataset, LaPA achieved an overall accuracy 4.4.AblationStudy
of 84.73%, outperforming the runner-up by approximately
0.63 percentage points. For VQA-2019, our model regis- In this section, we present an ablation study designed
tered a significant overall accuracy of 81.6%, which rep- toevaluatetheimpactofeachmodulewithinourproposed
resents a 1.8 percentage point augmentation compared to methodology.TheresultsaresummarizedinTab.2,encom-
the second-best performing model. The M2I2 model ex- passingthreebenchmarkdatasets. Weutilizethefollowing
hibitedproficiencyinansweringclosed-endedquestionsbut abbreviations: BL.forbaseline, GM.forthelatentprompt
showedlimitationswithopen-endedquestiontypes,poten- generationmodule(detailedinSection3.1),LF.forthela-
tiallyattributabletodisparitiesinpre-trainingdatasets. The tent prompt fusion module (described in Section 3.2), and
Q2ATransformer [21] and MUMC [14] models were pre- PF. for the prior knowledge fusion module (elucidated in
cluded from our comparison due to the unavailability of Section3.3). Thenotationw/ocsspecifiesconfigurations
their source code, checkpoints, and pre-training datasets, thatomittheconsistencymethod, whichallowsfortheas-
which hindered reproducibility of their results. Moreover, sessmentofitseffectiveness.Theconcludinglinequantifies
the MeDVInT [34] and LLaVA-Med [13] models possess theenhancementourLaPAmodeloffersoverthebaseline.
aparametercountexceeding7billion,nearly17timesthat Duetotheindirectinteractionofthelatentpromptgen-
ofourLaPAmodel(0.405B).Despitesomesuperiorresults erationmodulewithimageandlanguagemodalities,wein-
fromthesemodels,wepositthatthecomparisonwouldnot vestigate its influence by conducting an ablation study in
be equitable due to the vast difference in model size and conjunction with the GM. and LF. modules. The compar-
ison between conditions #1 and #2 in Tab. 2 demonstratesη 0.01 0.05 0.1 0.5 1 metrics,isachievedwithalatentpromptsizeof32acrossall
VQA-RAD 71.84 72.28 79.38 72.28 72.95 evaluated datasets. We hypothesize that excessively large
SLAKE 83.69 83.60 84.73 83.22 83.60 latent prompt dimensions may introduce superfluous and
VQA-2019 80.27 78.40 81.60 78.40 80.80 potentially disruptive noise into the information extraction
process, thereby detrimentally impacting the precision of
Table5.Ablationontheη. thefinalanswerpredictionintheMed-VQAcontext.
thattheintegrationofthelatentpromptmarkedlyenhances
Ablationontheη. Thehyperparameterη exertsadirect
themodel’scapabilityinaddressingMed-VQAtasks. Fur-
influence on the weighting of the consistency loss within
ther,weexaminetheefficacyoftheconsistencymethod;the
theaggregatelossfunction. Tab.5illustratestheimpactof
comparative improvement of condition #3 over #2 under-
varyingη from0.01to1ontheoverallperformanceacross
scoresitsutility. Theincorporationofthepriorknowledge
three benchmark datasets. The empirical results indicate
fusionmodulefurtheraugmentsmodelperformance(Com-
thatsettingηto0.01yieldsthemostfavorableoutcomeson
parison#4and#3). Ultimately,theamalgamationofallen-
allthreebenchmarks.
hancementsintothebaselinemodelculminatesinasubstan-
tialperformanceleap,asevidencedincondition#5.Theag- 4.5.QualitativeAnalysis
gregateimprovementacrossallthreebenchmarksisnearly
To further elucidate the efficacy of our Latent Prompt
3% relative to the baseline, as detailed in the concluding
Assist(LaPA)model, aqualitativeanalysiswasconducted
lineofourablationanalysis.
on six Medical visual question answering (Med-VQA) in-
stances, specifically three from the VQA-RAD dataset
Ablation on the θ and β. The hyperparameters θ and β
(casesa,b,c)andthreefromtheSLAKEdataset(casesd,e,
arepivotalinmodulatingtheinteractionofcross-modalin-
f),asdepictedinFig.4. Examinationofcasesa,b,c,d,and
formation, subsequentlyinfluencingtheaccuracyofthefi-
frevealsthattheincorporationoflatentpromptsfacilitates
nalpredictiveresponses.Fig.3employsatriadofheatmaps
the model in accurately responding to both closed-ended
toelucidatetheeffectsofvariousθandβcoefficientsonthe
andopen-endedqueriesacrossthetwobenchmarks. How-
fusion of cross-modal image and language features within
ever, in case e, the model’s integration of solely the latent
three benchmark datasets. With the coefficient for the la-
prompt proved insufficient for distinguishing between two
tent prompt (α) held constant at 1 and the latent prompt
highlysimilarresponses. TheadditionofthepriorKnowl-
sizefixedat32,wesystematicallyvaryθ andβ from0.01,
edgefusionmodule(PF.)wasinstrumentalinrectifyingthe
through 0.1, to 1 to assess their impact on model perfor-
model’sresponse. Thesesixcasescollectivelydemonstrate
mance. ThevisualrepresentationinFig.3indicatesthatthe
that our proposed enhancements substantively bolster the
combination of β = 0.1 and θ = 0.1 is optimal across all
model’s performance in resolving both closed-ended and
threeevaluateddatasets.
open-endedVQAchallenges.
5.Conclusion
Ablation on the interaction order. The sequence of in-
teractions within the latent prompt fusion module exerts a
This study introduces a novel Latent Prompt As-
directinfluenceontheefficacyofinformationextractionvia
sist(LaPA)modeldesignedtoenhancetheaccuracyofre-
thelatentprompts. Tab.3delineatestheimpactofvarious
sponses in the domain of medical visual question answer-
fusion sequences on the accuracy of the resultant outputs.
ing (Med-VQA). It employs the latent prompt to filter dif-
Itisobservedthattheoptimalfusionsequencecommences
ferent modal information and extract clinic-relevant infor-
with language, subsequently incorporates image modality,
mation to assist in predicting the final answer. Our inno-
andconcludeswithamulti-modalfusion, therebyyielding
vative framework entails a latent prompt generation mod-
themostfavorableoutcomes.
ule that synthesizes latent prompts under the constraint of
target answer tokens. These prompts are then integrated
Ablationonthelatentpromptsize. Thesizeofthelatent with both uni-modal and multi-modal information streams
promptcriticallydeterminestheparametercountwithinthe toisolateclinicalinsights. Further, themodelincorporates
latentpromptframework.Tab.4presentsananalysisofhow prior knowledge encapsulated in a knowledge graph, de-
varyingthelatentpromptsizefrom4to256influencesper- tailing disease-organ relationships, to interact with the la-
formanceacrossthethreebenchmarkdatasets. Initially,an tentpromptandrefinethefinalanswerprediction. Empiri-
increaseinlatentpromptsizecorrelateswithenhancedper- calvalidationofourapproachacrossthreewell-established
formanceacrossbenchmarks. However,adeclineinmodel benchmarks demonstrates its superiority in generating ac-
accuracyisobservedwhenthelatentpromptexceedsasize curateanswerswithintheMed-VQAcontext. Lookingfor-
of32. Theoptimalperformance,asevidencedbyaccuracy ward,weaimtodeploythelatentpromptmechanismwithin(a) (b) (c)
Question: Has the midline of the Question: Does this look like a
Question: What structures are involved?
mediastinum shifted? healthy liver?
Baseline : yes Baseline : no Baseline : brain
+ GM.&LF. : no + GM. & LF. : yes + GM. & LF. : caudate putemen left parietal
+ GM. & LF. + PF. : no + GM. & LF. + PF. : yes + GM. & LF. + PF.: caudate putemen left parietal
(d) (e) (f)
Question: Which part of the body does this Question: Where is/are the Question: Is the brain healthy?
image belong to? abnormality located?
Baseline : no
Baseline : abdomen Baseline : left lung right
+ GM. & LF. : yes
+ GM. & LF. : chest + GM. & LF. : left lung right
+ GM. & LF. + PF. : yes
+ GM. & LF. + PF. : chest + GM. & LF. + PF. : right lung right
Figure4. SixexamplesoftheLaPAmodelthatusedifferentmodulestodotheablationstudy. Instancesa,b,andcareextractedfrom
theVQA-RADdataset,whereasinstancesd,e,andforiginatefromtheSLAKEdataset. Withintheprovidedillustrations,responsesare
annotatedwithgreentodenotecorrectnessandwithredtosignifyerroneouspredictionsbythemodel. TheGM., LF., andPF.arethe
abbreviationsofthelatentpromptgenerationmodule,latentpromptfusionmodule,andpriorknowledgefusionmodule.
a large-scale, highly-parameterized model to fully explore France, September 27–October 1, 2021, Proceedings, Part
thepotentialoflatentpromptsincomplexinferencetasks. V24,pages64–74.Springer,2021. 1,2
[5] Sedigheh Eslami, Gerard de Melo, and Christoph Meinel.
References Does clip benefit visual question answering in the medical
domain as much as it does in the general domain? arXiv
[1] Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey preprintarXiv:2112.13906,2021. 5
Liu,DinaDemner-Fushman,andHenningMu¨ller.Vqa-med: [6] Haifan Gong, Guanqi Chen, Mingzhi Mao, Zhen Li, and
Overview of the medical visual question answering task at GuanbinLi. Vqamix:Conditionaltripletmixupformedical
imageclef2019. CLEF(workingnotes),2(6),2019. 2,5 visual question answering. IEEE Transactions on Medical
[2] Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guan- Imaging,41(11):3332–3343,2022. 1
bin Li, Xiang Wan, and Tsung-Hui Chang. Multi-modal [7] IrvingJohnGood. Rationaldecisions. JournaloftheRoyal
masked autoencoders for medical vision-and-language pre- StatisticalSociety:SeriesB,14(1):107–114,1952. 4
training. In International Conference on Medical Image [8] TianchengGu,DongnanLiu,ZhiyuanLi,andWeidongCai.
ComputingandComputer-AssistedIntervention,pages679– Complexorganmaskguidedradiologyreportgeneration. In
689.Springer,2022. 1,5 ProceedingsoftheIEEE/CVFWinterConferenceonAppli-
[3] Zhihong Chen, Guanbin Li, and Xiang Wan. Align, rea- cationsofComputerVision,pages7995–8004,2024. 1
sonandlearn: Enhancingmedicalvision-and-languagepre- [9] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
training with knowledge. In Proceedings of the 30th ACM PPT: Pre-trained prompt tuning for few-shot learning. In
InternationalConferenceonMultimedia,pages5152–5161, SmarandaMuresan,PreslavNakov,andAlineVillavicencio,
2022. 5 editors,Proceedingsofthe60thAnnualMeetingoftheAsso-
[4] Tuong Do, Binh X Nguyen, Erman Tjiputra, Minh Tran, ciationforComputationalLinguistics(Volume1: LongPa-
Quang D Tran, and Anh Nguyen. Multiple meta-model pers),2022. 1,2
quantifyingformedicalvisualquestionanswering. InMed- [10] Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi,
icalImageComputingandComputerAssistedIntervention– U Deva Priyakumar, and CV Jawahar. Mmbert: Multi-
MICCAI 2021: 24th International Conference, Strasbourg, modal bert pretraining for improved medical vqa. In 2021IEEE18thInternationalSymposiumonBiomedicalImaging, Hierarchical vision transformer using shifted windows. In
pages1033–1036.IEEE,2021. 1,2,5 ProceedingsoftheIEEE/CVFInternationalConferenceon
[11] Jin-HwaKim,JaehyunJun,andByoung-TakZhang. Bilin- ComputerVision,2021. 4
earAttentionNetworks. InAdvancesinNeuralInformation [23] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
ProcessingSystems31,pages1571–1581,2018. 1,5 regularization. InICLR,2018. 5
[12] JasonJLau, SoumyaGayen, AsmaBenAbacha, andDina [24] Binh D Nguyen, Thanh-Toan Do, Binh X Nguyen, Tuong
Demner-Fushman. A dataset of clinically generated visual Do, ErmanTjiputra, andQuangDTran. Overcomingdata
questions and answers about radiology images. Scientific limitation in medical visual question answering. In Medi-
data,5(1):1–10,2018. 2,5 calImageComputingandComputerAssistedIntervention–
MICCAI 2019: 22nd International Conference, Shenzhen,
[13] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,
China,October13–17,2019,Proceedings,PartIV22,pages
HaotianLiu,JianweiYang,TristanNaumann,HoifungPoon,
522–530.Springer,2019. 1,2
and Jianfeng Gao. Llava-med: Training a large language-
and-visionassistantforbiomedicineinoneday.Advancesin [25] ObiomaPelka,SvenKoitka,JohannesRu¨ckert,FelixNensa,
NeuralInformationProcessingSystems,36,2024. 3,6 and Christoph M Friedrich. Radiology objects in context
(roco): amultimodalimagedataset. InIntravascularImag-
[14] PengfeiLi,GangLiu,JinlongHe,ZixuZhao,andShenjun
ingandComputerAssistedStentingandLarge-ScaleAnno-
Zhong. Maskedvisionandlanguagepre-trainingwithuni-
tationofBiomedicalDataandExpertLabelSynthesis: 7th
modalandmultimodalcontrastivelossesformedicalvisual
JointInternationalWorkshop,CVII-STENT2018andThird
question answering. In International Conference on Medi-
InternationalWorkshop,LABELS2018,HeldinConjunction
cal Image Computing and Computer-Assisted Intervention,
with MICCAI 2018, Granada, Spain, September 16, 2018,
pages374–383.Springer,2023. 6
Proceedings3,pages180–189.Springer,2018. 5
[15] Pengfei Li, Gang Liu, Lin Tan, Jinying Liao, and Shenjun
[26] Jiaren Peng, Wenzhong Yang, Fuyuan Wei, and Liang He.
Zhong. Self-supervisedvision-languagepretrainingforme-
Promptforextraction: Multipletemplateschoicemodelfor
dialvisualquestionanswering. In2023IEEE20thInterna-
eventextraction. Knowledge-BasedSystems, page111544,
tionalSymposiumonBiomedicalImaging,pages1–5.IEEE,
2024. 1,2
2023. 1,5
[27] Morgan D. Polak, M.P. Extracting accurate materials data
[16] ZhihongLin,DonghaoZhang,QingyiTao,DanliShi,Gho-
from research papers with conversational language models
lamrezaHaffari,QiWu,MingguangHe,andZongyuanGe.
and prompt engineering. Nat Commun 15, 1569 (2024).,
Medicalvisualquestionanswering: Asurvey. ArtificialIn-
2024. 2
telligenceinMedicine,page102611,2023. 1
[28] Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben
[17] Bo Liu, Li-Ming Zhan, and Xiao-Ming Wu. Contrastive
Bogin, Madeleine van Zuylen, Sravanthi Parasa, Sameer
pre-trainingandrepresentationdistillationformedicalvisual
Singh,MattGardner,andHannanehHajishirzi. Medicat: A
question answering based on radiology images. In Medi-
datasetofmedicalimages,captions,andtextualreferences.
calImageComputingandComputerAssistedIntervention–
arXivpreprintarXiv:2010.06000,2020. 1,5
MICCAI 2021: 24th International Conference, Strasbourg,
[29] Tim Tanida, Philip Mu¨ller, Georgios Kaissis, and Daniel
France, September 27–October 1, 2021, Proceedings, Part
Rueckert. Interactiveandexplainableregion-guidedradiol-
II24,pages210–220.Springer,2021. 5
ogyreportgeneration.InProceedingsoftheIEEE/CVFCon-
[18] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and
ferenceonComputerVisionandPatternRecognition,pages
Xiao-MingWu. Slake: Asemantically-labeledknowledge-
7433–7442,2023. 1
enhanceddatasetformedicalvisualquestionanswering. In
[30] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
2021 IEEE 18th International Symposium on Biomedical
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Imaging,pages1650–1654.IEEE,2021. 2,4,5
Polosukhin. Attentionisallyouneed. Advancesinneural
[19] BoLiu, Li-MingZhan, LiXu, andXiao-MingWu. Medi- informationprocessingsystems,30,2017. 3
calvisualquestionansweringviaconditionalreasoningand [31] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova,
contrastivelearning.IEEEtransactionsonmedicalimaging, AdrianaRomero,PietroLio,andYoshuaBengio. Graphat-
42(5):1532–1545,2022. 1,5 tention networks. arXiv preprint arXiv:1710.10903, 2017.
[20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar 4
Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle- [32] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and
moyer,andVeselinStoyanov.Roberta:Arobustlyoptimized AlexSmola. Stackedattentionnetworksforimagequestion
bertpretrainingapproach.arXivpreprintarXiv:1907.11692, answering. InProceedingsoftheIEEEconferenceoncom-
2019. 3,4 putervisionandpatternrecognition,pages21–29,2016. 1
[21] Yunyi Liu, Zhanyu Wang, Dong Xu, and Luping Zhou. [33] Anda Zhang, Wei Tao, Ziyan Li, Haofen Wang, and Wen-
Q2atransformer: Improving medical vqa via an answer qiang Zhang. Type-aware medical visual question answer-
querying decoder. In International Conference on Infor- ing. InICASSP2022-2022IEEEInternationalConference
mation Processing in Medical Imaging, pages 445–456. on Acoustics, Speech and Signal Processing, pages 4838–
Springer,2023. 1,2,6 4842.IEEE,2022. 1,2
[22] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng [34] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin,
Zhang, Stephen Lin, and Baining Guo. Swin transformer: Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Vi-sual instruction tuning for medical visual question answer-
ing. arXivpreprintarXiv:2305.10415,2023. 3,6
[35] Yubo Zhang, Xingxing Zhang, Xun Wang, Si qing Chen,
andFuruWei. Latentprompttuningfortextsummarization,
2022. 1,2
[36] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang,
Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of
methodsandapplications. AIopen,1:57–81,2020. 4
[37] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
Liu. Conditionalpromptlearningforvision-languagemod-
els. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition, pages 16816–16825,
2022. 2
[38] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
Liu. Learning to prompt for vision-language models. In-
ternationalJournalofComputerVision,130(9):2337–2348,
2022. 2