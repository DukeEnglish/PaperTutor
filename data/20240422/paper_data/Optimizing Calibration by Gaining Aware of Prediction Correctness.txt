Optimizing Calibration by Gaining Aware of Prediction Correctness
YuchiLiu1 LeiWang12 YuliZou3 JamesZou4 LiangZheng1
Abstract
cat
Cross-entropy supervised
classifier 𝑙𝑜𝑔𝑖𝑡𝑠 𝑙𝑜𝑔𝑖𝑡𝑠 Calibrator 𝑙𝑜𝑔𝑖𝑡𝑠!
0.538
0.455
Modelcalibrationaimstoalignconfidencewith 0.412 0.399
prediction correctness. The Cross-Entropy CE)
lossiswidelyusedforcalibratortraining,which 0.068 0.065 0.032 0.030
enforcesthemodeltoincreaseconfidenceonthe
ground truth class. However, we find the CE
cat dog horse bird cat dog horse bird
loss has intrinsic limitations. For example, for Mean Square Error supervised Correctness-aware supervised
anarrowmisclassification,acalibratortrainedby 𝑙𝑜𝑔𝑖𝑡𝑠 Calibrator 𝑙𝑜𝑔𝑖𝑡𝑠! 𝑙𝑜𝑔𝑖𝑡𝑠 Calibrator 𝑙𝑜𝑔𝑖𝑡𝑠!
the CE loss often produces high confidence on
0.456
0.413
thewronglypredictedclass(e.g.,atestsampleis 0.292
wrongly classified and its softmax score on the 0.274
0.220 0.214
groundtruthclassisaround0.4),whichisundesir- 0.067 0.064
able.Inthispaper,weproposeanewpost-hoccal-
ibrationobjectivederivedfromtheaimofcalibra-
cat dog horse bird cat dog horse bird
tion. Intuitively,theproposedobjectivefunction
asksthatthecalibratordecreasemodelconfidence Figure1.AfailureexampleforcalibratorstrainedbytheCross-
onwronglypredictedsamplesandincreasecon- Entropy(CE)orMeanSquareError(MSE)loss.Aclassifier
fidenceoncorrectlypredictedsamples. Because makesawrongpredictionofacatimage.Beforecalibration,the
asampleitselfhasinsufficientabilitytoindicate classifiergivesprobabilitiesof0.412and0.455ontheground-truth
andpredictedclasses,respectively.Thecalibratortrainedwiththe
correctness,weuseitstransformedversions(e.g.,
CElossassignsevenhigherconfidence0.538tothewrongclass,
rotated,greyscaledandcolor-jittered)duringcali-
makingthingsworse,andthattrainedbyMSEmaintainsasimi-
bratortraining. Trainedonanin-distributionvali-
larconfidence0.456. Incomparison,calibratortrainedwiththe
dationsetandtestedwithisolated,individualtest
proposedCorrectness-Aware(CA)losseffectivelydecreasesconfi-
samples, our method achieves competitive cali-
denceofthiswrongpredictionto0.292,improvingcalibration.
brationperformanceonbothin-distributionand
out-of-distribution test sets compared with the
stateoftheart. Further, ouranalysispointsout
1.Introduction
thedifferencebetweenourmethodandcommonly
usedobjectivessuchasCElossandmeansquare Model calibration is an important technique to enhance
errorloss,wherethelatterssometimesdeviates the reliability of machine learning systems. Generally, it
fromthecalibrationaim. aimstoalignpredictiveuncertainty(a.k.a.confidence)with
predictionaccuracy. Weareinterestedinpost-hocaccuracy
preservingcalibratorsthatscalethemodeloutputtomake
itcalibrated(Guoetal.,2017;Zhangetal.,2020;Tomani
1School of Computing, The Australian National University etal.,2022;Guptaetal.,2020;Kulletal.,2019).
(ANU), Canberra, Australia 2Data61, The Commonwealth Sci-
entificandIndustrialResearchOrganisation(CSIRO),Canberra, ExistingmethodstypicallyuseMaximumLikelihoodEsti-
Australia3TheHongKongPolytechnicUniversity(PolyU),Hong mation(MLE)totrainacalibratorfortheclassificationtask,
Kong, China 4Department of Computer Science, Stanford Uni- suchastheMeanSquareError(MSE)loss(Tomanietal.,
versity,California,UnitedStates.Correspondenceto:YuchiLiu
2022;2023;Zhangetal.,2020)andtheCross-Entropy(CE)
<yuchi.liu@anu.edu.au>.
loss(Guoetal.,2017;Zouetal.,2023). Althoughtheseap-
Codesareavailableat: github.com/liuyvchi/Correctness-aware- proachesdemonstrateefficacyinreducingcalibrationerrors
calibration suchasexpectedcalibrationerrorandBrierscores,theylack
1
4202
rpA
91
]VC.sc[
1v61031.4042:viXra
detarbilacnu
detarbilacnu
ecnedifnoc
ecnedifnoc
detarbilac ecnedifnoc
detarbilac ecnedifnocOptimizingCalibrationbyGainingPredictionCorrectnessAwareness
theoreticalguaranteethatthecalibrationerrorisminimized onvariousINDandOODdatasets.
whenMLEconverges. InFig.1,foranimagewhichisin-
correctlyclassifiedandhasarelativelyhighprobabilityon • Wediagnosecommonlyusedcalibrationlossfunctions
thegroundtruthclass,calibratorstrainedbytheCEorMSE includingtheCEandMSEloss: theyareoftenlimited
losswouldgivehighconfidenceonthewronglypredicted undertestsamplesofcertaincharacteristics.
class. Itmeansausermaytrustthispredictiontobetrue.
AstoberevealedinSec.3.4,theinherentproblemofCE
2.RelatedWork
andMSElossfunctionslimitsthemincalibratingsuchtest
cases,butourmethodeffectivelyreducestheconfidence. Loss functions used in post-hoc calibration. Existing
post-hoccalibratorstypicallyusetheMaximumLikelihood
Inthispaper,wederiveaconcreteinterpretationofthegoal
Estimation(MLE)foroptimization(Jungetal.,2023;Tao
ofcalibrationwhichisthendirectlytranslatedintoanovel
etal.,2023). Forexample,Mukhotietal.(2020)usetheCE
loss function that aligns with the newly interpreted goal.
loss,whileKumaretal.(2019)usetheMSEloss. Addition-
Specifically,westartfromthegeneraldefinitionofcalibra-
ally,Mukhotietal.(2020)usesthefocalloss,avariantto
tionanditserror(Guoetal.,2017)andthen,underafinite
CE,whichenhanceslearningonwrongpredictions. Guo
testset,representtheerrorinadiscretizedform.Minimizing
etal.(2017)directlyoptimizesECE.Thispaperidentifies
thisdiscretizederrorgivesusaveryinterestingandintuitive
inherentproblemswithMLEforcalibrationandderivesa
calibration goal: A correct prediction should have possi-
newlossfunctionthatbetteralignswithcalibrationgoal.
bly high confidence and a wrong prediction should have
possiblylowconfidence. Theoretically, thisoptimization OODcalibrationdealswithdistributionshiftsintestsets
goalcannaturallyreducetheoverlapofconfidencevalues (Tomanietal.,2021). Ausefulpracticeismodifyingthe
betweencorrectandincorrectpredictions. calibrationsettocoverOODscenarios(Tomanietal.,2021;
Krishnan&Tickoo,2020),butthesemethodsareusually
Wetranslatethisgoalintoalossfunctionthatenforceshigh
designed for specific OOD scenarios which may lead to
confidence(i.e.,1)forcorrectlyclassifiedsamples,andlow
confidence(i.e., 1,whereC isthenumberofclasses)for compromisedin-distribution(IND)calibrationperformance.
C Another line of works adapt to test data distribution that
wrongly classified ones. We name it as the Correctness-
canbeeitherOODorIND,usingdomainadaptation(Wang
Aware (CA) loss. Yet for this method, it is non-trivial to
etal.,2020),calibrationsetsfrommultipledomains(Gong
identifyclassificationcorrectness. Toaddressthis,wepro-
et al., 2021), or improving calibration sets by estimating
posetousetransformedversionsoforiginalimagesasthe
test set difficulty (Zou et al., 2023). In comparison, our
calibratorinput:consistencybetweentheirpredictionresults
calibrator is trained on the IND validation set only, and
suggestspredictioncorrectnessoftheoriginalsample.
doesnotneedtestbatchesforupdate,butstilldemonstrates
Our method allows a calibrator to be trained on the in- improvedandcompetitiveOODcalibrationperformance.
distribution validation set and directly applied to individ-
Predictingclassificationcorrectnesshasnotbeenwidely
ualtestsamplesduringinference,whichisconsistentwith
studiedinthecommunity. Amongthefew,Xia&Bouganis
practical usage. We demonstrate the effectiveness of the
(2023)investigateathree-wayclassificationproblem: clas-
proposedstrategyonvarioustestsets.Inbothin-distribution
sifyatestsampleintocorrectprediction,wrongprediction,
(IND)andout-of-distribution(OOD)testsets,ourmethod
oranout-of-distributionsample(itscategoryisoutsidethe
isclearlysuperiortouncalibratedmodelsandcompetitive
traininglabelspace). Wefindthetaskofpredictingclassifi-
comparedwithstate-of-the-artcalibrators. Moreover,our
cationcorrectnesscloselyconnectedtomodelcalibration.
methoddemonstratesthepotentialtobetterseparatecorrect
andincorrecttestsamplesusingtheircalibratedconfidence.
Belowwesummarizethemaincontributions. 3.Approach
3.1.Definingcalibrationerrorfromitsgoal
• Theoretically,wederivetheconcretegoalofmodelcali-
brationthathasclearsemanticmeaning. Thisallowsusto Notations. Westudycalibrationunderthemulti-wayclassi-
designanewcalibrationlossfunction:correctlyclassified ficationproblem. Regularfontsarescalars,e.g.,τ;vectors
samplesshouldhavehighconfidence,whileincorrectly aredenotedbylowercaseboldfaceletters,e.g.,x;matrices
classifiedoneslowconfidence. bytheuppercaseboldface,e.g.,Xforanimage1. I de-
C
notes an index set of integers {1,...,C}, operator ‘;’ and
• To indicate prediction correctness, we use the softmax ⊕ concatenate vectors, e.g., ⊕ v = [v ;...;v ]. A
i∈IM i 1 M
predictionscoresoftransformedversionsofanoriginal classifierf takesad-dimensionalinputx∈Rdanditscor-
imageascalibratorinputs. respondinglabely ∈I withC classeswhicharesampled
C
• Ourmethodachievescompetitivecalibrationperformance 1Forsimplicity,weomitthreecolorchannels.
2OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
v
transformation 1 v! v![q]
𝐗 𝑓 top-𝑘 𝐗 transformation 2 𝑓 v$ v$[q] ⊕ 𝑔 𝜏 temperature 𝐶𝐴𝑙𝑜𝑠𝑠 indices q scaling
logitsz
transformation 𝑀 v# v#[q]
softmax vector v forward pass backward pass ⊕concatenate Classifier 𝑓 Calibrator 𝑔 𝜏: sample-wise temperature
Figure2.Calibrationpipeline.ForagivenimagesampleX,wefirstobtainitslogitvectorzandsoftmaxvectorv.WethenapplyM
differenttransformations(e.g.,rotation,greyscale,colorjitter,etc.)onXtogetitstransformedversionsaswellastheirrelatedsoftmax
vectorsasv (i∈I ).Indicesq∈Rkofthetop-klargestprobabilities(softmaxscores)invareusedtoacquiretop-kscoresfromv
i M i
toformtheconcatenatedinput⊕ v [q]tothecalibrator.Thecalibratoroutputsatemperatureτ,thenbeingusedtoupdatethelogit
i∈IM i
vectorztoproducethecalibratedsoftmaxvector.WeuseourproposedCorrectness-Aware(CA)loss.
fromthejointdistributionp(x,y) = p(y|x)p(x). Weuse thecalibrationerrorE byreplacingp(cˆ)withanempirical
f
≡ to denote the equivalence. The output of f is denoted distribution, formed by assembling Dirac delta functions
asf(X)=(yˆ,cˆ),whereyˆandcˆdenotethepredictedclass (Dirac,1981)centeredateachpredictedsampleconfidence
labelandmaximumconfidencescore,respectively. cˆcomputedfromagivendatasetD ={(x ,y )}n ,where
i i i=1
nisthenumberofsamples:
Calibrationgoal. AccordingtoGuoetal.(2017),thegoal
ofmodelcalibrationisto‘alignconfidencewiththeaccuracy n
1 (cid:88)
ofsamples’. Basedonthis,existingliteraturedefineperfect dp(cˆ)=
n
δ cˆi(cˆ). (4)
calibrationas: i=1
P(yˆ=y|cˆ=c)=c,∀c∈[0,1]. (1) SubstitutingEq. (2)andEq. (4)intoEq. (3),theempirical
calibrationlosscanbewrittenas:
Ourcalibrationerrorformulation. WeinterpretEq. (1) 1 (cid:88)n (cid:90)
Eemp = ∥cˆ − I{y =yˆ }dp(x |cˆ)∥. (5)
as: foranypredictedconfidencecˆ,theexpectedclassifica- f n i xi xi i i
tionaccuracyEaccofmodelf ontheconditionaldistribution i=1
cˆ
p(x|cˆ)shouldequalcˆ.Basedonthisinterpretation,wewrite However,Eq. (5)isstillhardtocomputebecausetheproba-
thecalibrationerrorofclassifierf asafunctionofcˆ: bilitydensityfunctiondp(x |cˆ)isnotaccessibleinpractice.
i i
Assuch, we furtherdiscretizeEq. (5), wherewe assume
(cid:90)
l f(cˆ)=∥cˆ−Ea cˆcc∥=∥cˆ− I{y
x
=yˆ x}dp(x|cˆ)∥, (2) a finite number of samples {x ij}m
j=1
in each distribution
p(x |cˆ). Consequently,Eempisreformulatedas:
i i f
where∥·∥denotesdiscrepancymeasurementsuchasthe
n m
ℓ g2 iv- ed nis ct oan nc de it. ioT nh (e thi end pi rc ea dto icr tif ou nnc mti ao tn chI e{ s·} thr eet gu rr on us n1 di tf ruth the Ee fmp = n1 m(cid:88)(cid:88) ∥cˆ i−I{y xij =yˆ xij}∥. (6)
i=1j=1
labelaccurately)istrue;otherwise,itreturns0.
Notethatinpractice,m=1,becausethereisonlyonetest
Denoting the distribution of predicted confidence as p(cˆ)
sample for each p(x |cˆ), i.e., a dataset only has one test
anditsprobabilitydensityfunctionasdp(cˆ),theexpectation i i
samplewithconfidence0.528933. Therefore,theempirical
ofcalibrationerror2off onp(cˆ)canbeexpressedas:
calibrationlossofclassifierf onatestsetwithnsamples
(cid:90) shouldbe:
E = l (cˆ)dp(cˆ), (3)
f f
n
1 (cid:88)
Eemp = ∥cˆ −I{y =yˆ }∥. (7)
wherel (cˆ)isdefinedinEq. (2). Amodelf isconsidered f n i xi xi
f i=1
tobeperfectlycalibratedifE =0. Modelcalibrationisto
f
optimizeacalibratorwhichreducesE asmuchaspossible.
f LowerandupperboundsofCAloss. Letusderivethe
CA loss range of Eq. (7) for a given test sample x : (i)
i
3.2.Correctness-awareloss Ifthesampleiscorrectlyclassified,theindicatorfunction
I+{y = yˆ } = 1. Themaximumconfidenceforacor-
Inpractice,thedistributionp(cˆ)isunknown,hence,Eq. (3) xi xi
rectlyclassifiedsampleiscˆ+ ∈ (1,1],andthelossrange
cannotbedirectlycomputed. Tosolvethis,weapproximate i C
would be ∥cˆ+ − I+{y = yˆ }∥ ∈ [0,1 − 1). (ii) If
i xi xi C
2Thisdiffersfromtheexpectedcalibrationerror(ECE)metric.
The ECE metric uses discretized histogram bins, whereas our 3We have a mild assumption that a test set has few, if any,
calibrationgoalemploysthecontinuousform. duplicatedimages,whereEq.(6)willalsoapproximatelyhold.
3
… … …OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
thesampleiswronglyclassified,I−{y = yˆ } = 0and Supposeρ≥50%,wehave:
xi xi
cˆ− ∈(1,1],thelossrangewouldbe(1,1]. Letρ∈[0,1]
i C C
be the ratio of correctly classified samples in D. For the (cid:32)(1−ρ)×n ρ×n
1 (cid:88) (cid:88)
correctlyclassifiedsamples,wehave: Eemp = (1−cˆ+)+ (1−cˆ+)
f n j j
j=1 j=(1−ρ)n+1
ρ×n (cid:18) (cid:19)
0≤ (cid:88) ∥cˆ+
i
−I+{y
xi
=yˆ xi}∥<ρ×n× 1− C1 , (8) +(1− (cid:88)ρ)×n cˆ−(cid:33)
i=1 k
k=1
andforthewronglyclassifiedsamples,wehave: (cid:32)(1−ρ)×n (1−ρ)×n (cid:33)
1 (cid:88) (cid:88)
=1−ρ+ cˆ−− cˆ+
(1−ρ)×n n k j
(1−ρ)×n× 1 < (cid:88) ∥cˆ−−I−{y =yˆ }∥ k=1 j=1
C i xi xi ρ×n
i=1 + 1 (cid:88) (1−cˆ+). (13)
≤(1−ρ)×n. (9) n j
j=(1−ρ)n+1
By combining Eq. (8) and (9) with Equation (7), we can
(cid:16) (cid:17)
nowderivethelowerandupperboundsofourCAloss: NowusingEdiff= 1 (cid:80)(1−ρ)×ncˆ−−(cid:80)(1−ρ)×ncˆ+
(1−ρ)×n k=1 k j=1 j
to denote the expectation of the difference in maximum
n
1 (cid:88)
Eemp = ∥cˆ −I{y =yˆ }∥ confidencevaluesbetweencorrectandincorrectpredictions,
f n i xi xi and E+ = 1 (cid:80)ρ×n (1 − cˆ+) to denote the
i=1 (2ρ−1)n j=(1−ρ)n+1 j
n expectation of the maximum softmax scores for the rest
(cid:88)
n×Ee fmp = ∥cˆ i−I{y xi =yˆ xi}∥ (2ρ−1)ncorrectlyclassifiedsamples,Eq.(13)canbewrit-
i=1 tenas:
(cid:32)ρ×n
(cid:88)
= ∥cˆ+ j −I+{y xj =yˆ xj}∥ Ee fmp =(1−ρ)Ediff+(2ρ−1)E++(1−ρ)
j=1
≡(1−ρ)Ediff+(2ρ−1)E+ (14)
(1−ρ)×n (cid:33)
(cid:88)
+ ∥cˆ−−I−{y =yˆ }∥ (10)
k xk xk We notice that minimizing our CA loss is equivalent to
k=1
(cid:18) (cid:19) (cid:18) (cid:19)
minimizingeitherEdifforE+(omittingtheconstant(1−ρ)):
1−ρ 1
n× ≤n×Eemp ≤n×ρ× 1− (i)minimizingEdiffaimstomaximizetheexpectationofthe
C f C
differenceinmaximumconfidencescoresbetweencorrect
+n×(1−ρ) andincorrectpredictions, and(ii)minimizingE+ aimsto
(cid:18) 1−ρ C−1(cid:19) pushthemaximumconfidencescoreofcorrectlyclassified
=n× +
C C samples to 1. These two objectives align well with the
modelcalibrationgoal. BelowwetakeacloselookatEdiff:
1−ρ 1−ρ C−1
≤Eemp ≤ + , (11)
C f C C
(cid:32)(1−ρ)×n (1−ρ)×n (cid:33)
1 (cid:88) (cid:88)
whereC isthenumberofclasses. Hence,theCAlosshasa Ediff = cˆ−− cˆ+
lowerboundof 1− Cρ andanupperboundof 1− Cρ+C C−1. We (1−ρ)×n k=1 k j=1 j
observethatbothlowerandupperboundsarecloselytied (cid:32) ρ×n (cid:33)
to 1−ρ,representingthefractionofmisclassifiedsamples = 1 Eemp− 1 (cid:88) (1−cˆ+)+ρ−1
C 1−ρ f n j
perclassinwholeD.
j=(1−ρ)n+1
(cid:16) (cid:17)
TheoreticalinsightsofCAloss. First,werewriteEq.(7) 1 −1 ρ
asasumoftwoCAlosscomponents,I{y =yˆ }equals Eemp → 1−ρ ≡Ediff → C <0. (15)
xi xi f C 1−ρ
1 and 0 for correctly and incorrectly classified samples,
respectively,andweuseℓ asdiscrepancymeasurementfor
1 Eq. (15) demonstrates that minimizing our CA loss Eemp
simplicity: f
duringtrainingtowardthelowerbound 1−ρ isequivalent
C
Ee fmp = n1(cid:32)ρ (cid:88)×n (1−cˆ+ j )+(1− (cid:88)ρ)×n cˆ− k(cid:33) , (12) t tho ep au vs eh ri an gg eE md aiff xit mow ua mrd so(cid:0) ftC1 m1− − a1 ρ x(cid:1) ρ sc< ore0 s. oT fh wis rom ne ga lyns clp au ss sh ifiin eg
d
j=1 k=1
samples away from those of correctly classified samples,
wherecˆ+andcˆ−denote,respectively,themaximumconfi- therebyreducingtheoverlapofconfidencevaluesbetween
j k
dencescoresforcorrectlyandincorrectlyclassifiedsamples. correctandincorrectpredictions.
4OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
3.3.Gainingcorrectnessawareness Algorithm 1 Calibrator with our proposed Correctness-
AwareLoss
FromtheintuitionoftheCAlossinEq. (7),itsoptimiza-
Input:aclassificationmodelf tobecalibrated(f′(·)extracts
tionrequiresthepost-hoccalibratortobeawareofthecor-
thelogitvector,andσ(·)denotesthesoftmaxfunction),acali-
rectnessofeachtestsample. Empirically,wefindthetest
bratorgparameterizedbyθ,thetotalnumberoftransformsM,
sampleitselfofferlimitedhelptodistinguishcorrectness, andkforselectingthetop-kmaximumsoftmaxscores.yandyˆ
whichleadstoundesirablecalibrationperformance. denotethegroundtruthlabelandthepredictedlabelforagiven
imagesampleX,respectively.
Oursolutiontogainingcorrectnessawarenessismotivated Step1: Obtainthelogitvector: z = f′(X)andthesoftmax
by (Deng et al., 2022). They find consistency of model vector:v=σ(z).
predictionsoftransformed(e.g.,rotated,gray-scaled,etc.) Step2:ApplyM transformstotheoriginalinputimageXto
obtainitstransformedimagesX(i)(i∈I ),thenobtaintheir
images is highly correlated with model accuracy. While M
correspondingsoftmaxvectors:v =σ(f′(X(i))).
theinsightfrom(Dengetal.,2022)isonthedatasetlevel, i
Step3:Gettheindicesqofthetop-kmaximumsoftmaxscores
our assumption is on the individual sample level: model
fromthelogitvectorvusingEq.(16).
behavior on transformed samples is useful to inform the Step4:Usethegeneratedindicesqtoformnewvectorsv ∈
i
correctnessofmodelpredictions. Rk,concatenatethesenewk-dimensionalvectorsresultingin
⊕ v [q] ∈ RM×k,thenpassthisresultingmatrixtothe
ThepipelineofourcalibratorispresentedinFig. 2,weaim cai l∈ ibI rM atori gtoproducethetemperatureτ viaEq.(17).
tocalibrateaclassification‘model’f.Todoso,wecompute Step5: Applythelearnedtemperatureτ totheoriginallogit
thelogitvectorz,softmaxvectorsofanoriginalimageX vectorandobtainitsmaximumsoftmaxscoreviaEq.(18).
Step6:Plugtheupdatedmaximumsoftmaxpredictionscore
anditstransformedversionsvandv (i∈I ,assumingM
i M fromStep5,thegroundtruthlabely,andthepredictedlabelyˆ
typesoftransforms),respectively. Wedeterminetheindices
intoourproposedCorrectness-AwareLossviaEq.(19).
q∈Rk oftheklargestsoftmaxscoresofv,andusethese Return:Calibratormodelweightsθ.
indicesqtolocateandselectthecorrespondingvaluesfrom
v , forming new vector with k dimensions. These k-dim
i
vectorsv [q]ofthetransformedimagesareconcatenatedas
i
2023). Nevertheless, if we assume such access, we can
⊕ v [q],andusedasthecalibratorinput.
i∈IM i
retrieve from test batches images that are similar to the
Calibrator, g parameterized by θ, which is trained by the originaloneandusesoftmaxvectorsoftheretrievedimages
proposedCAloss. Here,thecalibratorconsistsoftwofully as calibrator inputs. As to be shown in Sec. 4.4, we find
connected layers with a ReLU activation function in be- that grayscale, rotation, and colorjitter are effective ones,
tween. Eachhiddenlayercomprises5nodes. Thecalibrator and that using four transformations give a good trade-off
isoptimizedonthecalibration(a.k.a.,validation)set,and betweencalibrationperformanceandcomputationalcost.
thecalibrationoutputistemperatureτ tobeusedtoscale
Duringinference,usingthek-dimvectorsfromthetrans-
themodellogitvectorzoforiginalimage. Belowweshow
formedimages,weobtainanadjustedtemperaturefromthe
thesestepsinequations.
calibrator. Thistemperatureisusedtoscalethelogitsofthe
q=argmaxv∈Rk (16) originalimage,thesoftmaxvectorofwhichisthenupdated.
q
τ =g θ(⊕ i∈IMv i[q]) (17) 3.4.ComparisonbetweenCAlossandMLE
cˆ=maxσ(z/τ)(c) (18)
MaximumLikelihoodEstimate(MLE)iswidelyusedfor
c
calibrationtraining(Kumaretal.,2019),underconcretefor-
whereσ(·)denotesthesoftmaxfunction. Eq.(18)retrieves matssuchastheCross-Entropy(CE)orMeanSquareError
themaximumsoftmaxpredictionscorecˆ. BasedontheCA (MSE)losses. Thissectiongoesthroughtheirconnections
lossinEq. (7),theoptimizationgoalnowbecomes: anddifferenceswiththeCAloss.
argminEemp(cˆ,y ,yˆ ), (19) Forcorrectpredictions,MLE(e.g.,CEorMSEloss)has
f x x
θ
similareffectwiththeCAloss. MLEenforcesthesoftmax
wherey andyˆ denoterespectivelythegroundtruthand probabilityoftheground-truthclasstobecloseto1.Forcor-
x x
predictedlabels. Algorithm1summarisesthiscalibration rectpredictions,thesoftmaxprobabilityoftheground-truth
process. classequalsthesampleconfidence(maximumprobability
in the softmax vector). Under this scenario, MLE aligns
Inpractice,transformationscanbegrayscale,rotation,color
with both the calibration objective and the CA loss: the
jitter,addingGaussiannoise,randomerasing,etc. Theyare
confidenceofcorrectpredictionsshouldbepossiblyhigh.
appliedtotheoriginalimageduringtrainingandinference.
We do not assume access to test batches, which are used Forwrongpredictions,MLEsometimesdeviatesfrom
in some previous works (Guo et al., 2017; Wang et al., thecalibrationgoalwhileCAistheoreticallyconsistent.
5OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
Figure3.Comparisonofdifferentlossfunctionsw.r.t.temperatureandsoftmaxprobabilityofthegroundtruth(GT)class.Ina
four-wayclassificationtask,weexamineawronglypredictedsamplewithlogitvector[a,2.0,0.1,0.05],wherea<2isthevalueonthe
groundtruthclass. Weusec todonatethesoftmaxscoreoftheGTclass. Top: Thelosssurfaceplotsforvaryingtemperaturesand
gt
c ,withredandbluearrowsrepresentingpositiveandnegativetemperaturegradients,respectively.Bottom:Shows2Dlosscurvesfor
gt
varyingc .Thelinesinthebottomchartscorrespondtothelinesofthesamecolorinthetopcharts.ComparedwithtraditionalMaximum
gt
LikelihoodEstimation(MLE)basedfunctions(e.g.,Cross-Entropy,MeanSquaredError),ourCorrectness-Awarelossminimizationdoes
notfavortemperaturesbelow1forincorrectpredictions,whilesometimesMLEdoes.
InFig. 3,wevisualizethelosssurfaceoftheCE,MSE,and small temperature, meaning a large confidence, which is
CAlossw.r.t.temperatureandsoftmaxprobabilityonthe undesirableforthiswronglypredictedsample. ForMSE,its
groundtruthclass(firstrow),fromwhichweuseexamples minimumisachievedwhentemperatureisaround1.0,which
oftwotypicalcalibrationtrainingsamplesformoreintuitive doesnotchangethetemperatureandconfidencemuch. This
illustration(bothfirstandsecondrows). Particularly,opti- againisundesirable. Incomparison,theCAlosskeepde-
maltemperature(x-axis)isachievedwhentherespective creasingwhentemperatureincreasessowilleventuallygive
loss(yaxis)isminimum(thesecondrowiseasiertoread). alargetemperatureorasmallconfidenceforthistypeof
samples. Thisisconsistentwiththecalibrationobjective.
For anabsolutelywrongsample (green curves in both
rows), whose probability of the wrongly predicted class Empirically,wefindthatsuchnarrowlywrongpredictions
isfargreaterthanthatontheground-truthclass,theopti- takeup2%-8%ofthecalibrationset(ImageNetvalidation)4.
mizationdirectionofMLEissimilartoCA:thelosscurve This would negatively impact training efficacy of MLE.
keepsdecreasingandfinallyalargetemperatureoralow Moreover, during inference, if a test set has many such
confidenceisobtained. Infact,underthisscenario,calibra- narrowlywrongpredictions,MLEwillalsobenegatively
tionobjectiverequirestheprobabilityoftheground-truth impacted because of its unsuitable in dealing with such
classtoincreaseandprobabilityofthewronglypredicted samplesduringtraining.Thiswouldexplainwhyoursystem
classtodecrease. Thisisconsistentwiththeobjectiveof issuperiortoandonparwithstateoftheartonOODand
MLE:toincreaseprobabilityontheground-truthclass. INDtestsets,respectively(refertoSec.4). ButinbothIND
andOODscenarios,ourcalibratedmodelsaremuchbetter
Foranarrowlywrongsample(yellowcurvesinbothrows),
thanuncalibratedones.
whoseprobabilityonthewronglypredictedclassismuch
closer to that on the ground-truth class, the optimization 4Wefirstcomputetheratiooftheprobabilityontheground-
direction of MLE is very different from or even opposite truthclasstothatonthewronglypredictedclass. Wedefinea
sampleisnarrowlywrongpredictionifthisratioishigherthan0.5.
to CA. Take the yellow curves in the second row of Fig.
3asexample. TheCEloss,tobecomesmaller,leadstoa
6OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
Table1.Calibrator comparison under the ImageNet setup. Each reported number is averaged over 10 classifiers, described in
Section4.1.Weusefivetestsets:ImageNet-Val,ImageNet-A,ImageNet-R,ImageNet-S,andObjectNet,andfourmetrics:ECE(bin=25),
BS,KSandAUC(AUROC).Bestresultsineachcolumnareinbold.WhencomparingCAandCE,betterresultsarehighlightedinblue.
ImageNet-Val ImageNet-A ImageNet-R ImageNet-S ObjectNet
Method
ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑
Uncal 8.71 14.01 14.39 85.96 39.44 32.90 43.47 61.87 13.97 16.89 19.90 88.06 20.92 21.67 29.01 82.57 31.21 25.20 36.48 78.05
TS 9.10 13.85 13.26 85.41 29.24 23.24 32.50 62.86 6.28 13.80 14.51 88.27 8.92 16.11 19.18 83.22 19.70 17.97 26.92 78.35
ETS 3.22 12.61 12.43 85.90 32.40 26.21 36.98 62.68 8.29 14.21 17.09 88.22 14.79 17.47 23.72 83.17 25.13 20.61 31.11 78.28
MIR 2.36 12.51 12.93 85.92 34.70 26.92 39.37 61.87 10.33 14.71 19.43 88.01 17.39 18.28 26.41 82.55 27.56 21.29 33.48 78.02
SPL 2.38 12.50 12.75 85.94 33.61 26.71 38.28 61.87 9.41 14.58 18.33 88.02 16.48 18.09 25.45 82.56 26.44 21.09 32.27 78.37
AdaptiveTS 6.35 14.20 11.04 82.94 29.30 23.61 32.99 61.43 5.65 14.29 14.68 86.99 9.65 16.96 20.01 81.01 20.77 18.99 27.79 76.95
TCP 8.30 17.38 17.45 72.15 28.07 19.62 32.05 47.57 8.81 22.59 21.16 62.77 9.79 21.69 25.56 56.95 21.79 18.58 31.25 72.57
ProCal 2.89 12.60 13.33 86.08 38.82 30.99 42.35 61.75 11.84 16.29 19.13 86.22 19.85 19.63 28.10 82.30 25.22 22.61 31.58 75.15
CEonly(PTS) 5.02 12.50 11.40 86.69 41.23 32.02 45.60 60.85 19.14 18.70 26.44 87.05 14.39 17.01 24.82 82.75 32.89 24.85 38.18 77.77
CAonly 2.22 12.25 12.63 86.74 32.14 25.28 36.47 61.08 11.62 15.79 20.44 86.86 5.49 15.29 14.97 82.50 22.09 18.59 28.99 77.89
CE+trans. 3.42 12.87 11.77 85.36 28.06 22.01 32.38 63.47 6.22 13.28 15.30 88.64 11.52 15.94 21.08 83.84 20.89 18.28 27.69 78.50
CA+trans.(ours) 4.63 11.85 11.55 87.44 20.65 16.79 22.50 63.74 4.91 12.21 10.12 90.22 4.00 13.83 13.12 84.87 10.33 14.59 18.72 79.25
CA+CE+trans 4.24 13.15 11.53 84.91 26.54 20.85 30.80 63.85 5.63 12.86 14.57 88.96 9.50 15.15 19.52 84.31 21.42 18.79 28.02 78.14
3.5.Discussion decisionmakingforusers,becauselessmistakesaremade.
ThispaperusesareaundertheROCcurve(AUROC)tomea-
HowcouldtheCAlossimprovetheECEmetric? ECE
suretheperformanceofpredictingclassificationcorrectness.
binsconfidenceandcalculatesthedifferencebetweenconfi-
AsshowninSectionB.1ofAppendix,asmallECEdoes
denceandaccuracyofsamplesineachbin. Intheextreme
notalwaysmeanusermakeslessmistakesduringdecision
casewherebinsizeisinfinitelysmall,eachbinwillcontain
making. Consideringthestrongtiebetweenthis2-wayclas-
onlyonesample(assumingnoimageduplicates),meaning
sificationproblemandmodelcalibration(Eq.(1)),wethink
accuracyofeachbiniseither100%or0%. Inthisscenario,
AUROCcanbeanadditionalevaluationmetricformodel
the CA loss will push correct (wrong) predictions to the
calibration. InSectionB.1ofAppendix,morecomparisons
high(low)confidence,whichalwaysreducesECE.When
betweenAUROCandECEarepresented.
binsizegraduallybecomeslarger,improvementbroughtby
CAlosswillbelessdefinitebutstillvisible.
4.Experiments
Sample-adaptive temperature used in (Joy et al., 2023;
Balanya et al., 2022; Wang et al., 2023) and our method 4.1.Modelsanddatasets
hasdifferentpropertiesfromglobaltemperature(Guoetal.,
ImageNet-1ksetup. 1. Models.Weuse10modelstrained
2017). Because global temperature does not change the
or fine-tuned on the ImageNet-1k training set (Deng
orderofsamplesrankedbytheirconfidence,itcannotim-
et al., 2009). We source these models from the model
provetheabilityofconfidencetoseparatecorrectandwrong
zoo Timm (Wightman, 2019). 2. Calibrationsets. We
predictions. Sample-adaptivetemperatureatleasthassuch
use ImageNet-Val (Deng et al., 2009) to train calibrator.
potential (but under specific design). On the other hand,
3. Testsets.(1)ImageNet-A(dversarial)(Hendrycksetal.,
trainedwiththeCEloss,thesample-adaptivetemperature
2021b) comprises natural adversarial examples that are
isdemonstratedtoproduceacompetitivecalibratorforIND
unmodified and occur in the real world. (2) ImageNet-
testsets(Joyetal.,2023;Balanyaetal.,2022). Butissues
S(ketch)(Wangetal.,2019)containsimageswithasketch-
withCEandthelackofusingadditionalinformationlimit
like style. (3) ImageNet-R(endition) (Hendrycks et al.,
itseffectivenessforOODdata. Incomparison,ourmethod
2021a) comprises of 30,000 images that exhibit diverse
iscompetitiveonbothINDandOODtestsets.
styles. (4) ObjectNet (Barbu et al., 2019) is a real-world
WhytheCAlosssometimesstillhaveempiricalfailures? test set for object recognition where illumination, back-
AcalibratorperfectlyoptimizedbytheCAlosswillgive0 groundsandimagingviewpointsareverychallenging. (5)
ECE,becauseallthecorrectly(wrongly)classifiedsamples ImageNet-Val. WetrainthecalibratoronhalfoftheIma-
willhaveconfidenceof1(0). Inpractice,however,thebot- geNetvalidationsetandtestitontheremaininghalf.
tleneckistotellpredictioncorrectness. Weuseaugmented
CIFAR-10setup. 1. Models.Weuse10differentmodels
imagesbutitmightnotbeanoptimalsolution. Infuturewe
trainedonthetrainingsplitofCIFAR-10(Krizhevskyetal.,
willexplorenewmethodsforcorrectnessprediction.
2009)inthissetup. Wefollowthepracticein(Dengetal.,
Correctnesspredictionperformanceasapotentialcal- 2022)toaccessthemodelweights. 2. Calibrationset.Cali-
ibrationmetric. Givenaconfidencevalue,bettersepara- bratorsaretrainedonthetestsetofCIFAR-10. 3. Testsets
bilitybetweencorrectandwrongpredictionsleadstosafer (1) CINIC-10 (Darlow et al., 2018) is a fusion of both
7OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
Table2.CalibratorcomparisonundertheCIFAR-10setup.Eachnumberisaveragedover10classifiers(seeSection4.1).Weuseone
INDtestset(CIFAR10.1)andthreeOODtestsets(CIFAR-10.1,CINIC,andCIFAR-10-C).OthernotationsarethesameasTable1.
CIFAR-10.1 GaussianBlur DefocusBlur CINIC
Method
ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑ ECE↓ BS↓ KS↓ AUC↑
Uncal 10.22 12.59 13.80 85.08 45.72 43.48 50.01 66.7 34.79 34.79 39.73 71.49 24.25 25.29 28.42 78.76
TS 4.84 11.06 11.78 85.15 35.25 34.54 42.81 66.78 24.99 28.17 33.93 71.40 16.04 20.77 24.17 79.15
ETS 2.77 10.78 10.87 85.08 30.04 30.84 39.41 66.69 19.98 25.69 39.73 71.23 11.48 19.13 22.15 79.25
MIR 2.29 10.83 11.01 84.98 30.39 30.83 39.73 66.67 20.31 25.63 31.51 71.41 12.58 19.59 22.86 78.68
SPL 3.04 10.89 10.61 85.04 29.12 30.57 38.49 66.72 19.64 25.53 30.46 71.48 11.88 19.51 22.03 78.72
AdaptiveTS 4.17 11.29 11.74 83.40 22.07 26.58 34.44 65.97 12.15 23.31 27.27 70.27 10.73 19.48 21.99 77.76
TCP 11.40 13.83 12.78 76.09 12.38 24.64 30.48 54.83 6.71 24.66 24.35 58.25 15.35 23.62 17.47 73.16
ProCal 2.96 11.27 11.60 82.85 37.26 36.83 44.28 64.50 25.68 28.99 34.36 69.39 18.04 22.19 25.14 76.22
CEOnly(PTS) 2.92 10.85 11.26 85.12 31.01 31.24 40.07 67.28 21.01 25.86 31.80 71.95 13.69 19.92 23.27 78.93
CAOnly 2.42 10.87 11.06 84.82 29.41 30.19 39.05 67.11 19.45 25.17 30.99 72.04 12.65 19.68 22.93 78.55
CE+trans. 2.87 10.90 11.22 84.84 18.49 24.51 32.22 67.18 9.09 22.04 25.79 71.52 7.05 18.31 20.65 79.03
CA+trans.(ours) 2.76 10.79 10.72 85.15 11.45 22.18 27.91 67.33 4.65 21.30 22.42 71.57 7.27 18.34 15.90 78.91
CIFAR-10andImageNet-C(Hendrycks&Dietterich,2019) 4.3.Mainobservations
imageclassificationdatasets.Itcontainsthesame10classes
Comparisonofcalibrationperformancewiththestate
as CIFAR-10. (2) CIFAR-10-C(orruptions) contains sub-
of the art. We summarize calibration results under the
setsfromCIFAR-10modifiedbyperturbationssuchasblur,
ImageNet, CIFAR-10, and iWildCam setups in Table 1,
pixelation,andcompressionartifactsatvariousseverities.
Table2,andTable3,respectively.Wehavetwoobservations.
iWildCam setup. 1. Model. We use 10 models First, on OOD test sets, our method is very competitive
trained on the iWildCam(Beery et al., 2020) training set. acrossvariousmetrics. Forexample,whencomparedwith
They are downloaded from the official dataset website. the second-best method on ObjectNet, the ECE, BS, and
2. Calibrationset.WetrainthecalibratorontheiWildCam KSmetricsofourmethodare10.56%,3.69%,and8.97%
validationset. 3. Testset.WeusetheiWildCamtestsetcon- lower,respectively. Second,onnearINDorINDtestsets
taininganimalpicturescapturedinthewild. Furtherdetails suchasImageNet-ValandCIFAR-10.1,ourmethodisless
ofthethreesetupsareprovidedinSec.AofAppendix. advantageous but is still competitive. The reason for our
methodbeingmoreeffectiveonOODtestsetsisthatthere
4.2.Calibrationmethodsandevaluationmetrics aremorenarrowlywrongpredictions,mentionedinthelast
paragraphinSection3.4. Besides,asexplainedinSection
Methods. Wecompareourmethodwithsixpopularcali-
3.3,theuseoftransformedimagesmightnotbeanoptimal
brationmethods. Theyincludescaling-basedmethodssuch
waytoinformclassificationcorrectness.
as temperature scaling (TS) (Guo et al., 2017), ensemble
temperature scaling (ETS) (Zhang et al., 2020), adaptive ComparingCAwithMLE.InTable1,Table2,andTable
temperaturescaling(AdaptiveTS)(Joyetal.,2023),and 3,wecompare‘CAonly’with‘CEonly’,and‘CA+trans’
parameterized temperature scaling (PTS) (Tomani et al., with‘CE+trans’. First,‘CAonly’consistentlyoutperforms
2022). We also compare with a binning method multi- ‘MSEonly’in18outof20scenariosundertheImageNet
isotonicregression(MIR)(Zhangetal.,2020),TrueClass setup, 13 out of 16 scenarios under the CIFAR-10 setup,
Probability (TCP) (Corbiere et al., 2021), a spline-based and4outof4scenariosundertheiWildCamsetup. Second,
re-calibration method (Spline) (Gupta et al., 2020), and inmostcases(e.g.,19outof20scenariosunderImageNet
Proximity-Informed Calibration (ProCal) (Xiong et al., setup), ‘CA+trans’isbetterthan‘CE+trans’. Inaddition,
2024). inTable1,weobservethatthecombinationofCEandCA
doesnotyieldbetterresultscomparedtousingCAalonein
Metrics. Apartfromtheexpectedcalibrationerror(ECE)
theOODtestset. ThesuperiorityoftheCAlossismore
(Guo et al., 2017), we report the Brier score (BS), adap-
evidentonOODdatasetsasdiscussedinSec.3.5.
tive calibration error (ACE) (Nixon et al., 2019), and
Kolmogorov-Smirnov(KS)error(Guptaetal.,2020). Inad- PotentialofCAinallowingconfidencetobetterseparate
dition,weuseareaundertheROCcurve(AUROC)toeval- correctandwrongpredictions. InTables1,2,and3,we
uatehowwellthecalibratorsseparatecorrectpredictions compareseparabilityandhavetwoobservations.First,exist-
fromwrongpredictions,which,asmentionedinSec.3.5, ingmethodstypicallydonothaveimprovementinAUROC.
mightalsobeagoodmetricforcalibration. Allthenumbers Thisisnotsurprising,becausetheirworkingmechanisms
inTable1,2,and3areaveragedoverresultsofcalibrating arenotrelevanttotheseparationofcorrectandwrongpre-
10differentmodelsintroducedinSec. 4.1. dictions. Second,ourmethodimprovesAUROCunderthe
ImageNetandiWildCamsetupsandinonparwithexisting
8OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
4.4.Furtheranalysis
Table3.CalibratorcomparisonunderiWildCamsetup.Each
numberisaveragedover10classifiers(seeSection4.1).Weuse Impactofnarrowlywrongpredictionsintrainingand
theiWildCamtestset.OthernotationsarethesameasTable1. testing. Weconstructvariouscalibration(training)setsand
Method ECE↓ BS↓ KS↓ AUC↑ testsetswithsamplesofcontrolleddegreesofbeingwrongly
Uncal 16.04 17.77 22.11 86.59 predicted.FromFig.4(left),ifatestsetisdominantlyfilled
TS 6.63 14.06 14.90 86.22
withnarrowlywrongpredictions,ourmethodwillhavea
ETS 6.83 14.05 13.84 86.17
MIR 5.01 13.53 12.57 86.61 hugeimprovementoverCEandnocalibration: infact,CE
SPL 5.98 13.70 12.44 86.63
hasthesameperformanceasnocalibrationinthisscenario.
ProCal 7.09 14.68 16.39 84.70
AdaptiveTS 7.17 14.25 12.88 86.33 As more absolutely wrong samples are included, the gap
TCP 13.30 13.57 13.73 90.51
between smaller, but our method is still superior. This is
CEonly(PTS) 8.73 17.90 17.45 78.65
CAonly 8.07 17.54 16.95 79.15 becausethecalibrationsetalsohasvariousdegreesofwrong
CE+trans. 6.78 12.88 12.07 88.70
predictions,soCEisnotaswelltrainedasCAandactually
CA+trans.(ours) 7.21 11.81 10.24 90.52
hassimilarperformanceasnocalibration.
Ontheotherhand, fromFig. 4(right), whentrainingset
containslotsofnarrowlywrongpredictions,CEisverypoor
andevenworsethennocalibration. Whenmoreabsolutely
wrongsamplesareincluded,CEbecomesgraduallybetter
and even close to our method. These results empirically
verifiesouranalysisinSection3.4.
Comparingdifferentimagetransformations. Wetrydif-
ferent combinations of image transformations (including
narrowly wrong absolutely wrong narrowly wrong absolutely wrong retrieval-basedaugmentationintestbatches)ascalibrator
input. ResultsaresummarizedinFig.5. Weobservethatus-
ingrotation,gray-scaling,andcolor-jitteringgenerallygive
Figure4.Impactofnarrowlywrongandabsolutelywrongpre-
dictionsoncalibratorperformance. (Left:) wecrafttestsets goodcalibrationresults. Retrieval-basedaugmentationis
containing500wronglypredictedsampleswithvariousdegreesof alsocompetitive,butitrequiresaccesstotestbatcheswhich
beingwrong.Forexample,theleftmosttestsetcontainsnarrowly mightnotbepractical. Moreover,wefindthatusingonly
wrongsamples,whiletherightmostonecontainsabsolutelywrong one transformation is not ideal. While using more trans-
sample.CalibratoristrainedonImageNet-Val.(Right:)wecraft formationsiseffective,threeisagoodnumbertobalance
trainingsetscontaining1,000wrongpredictionsand1,000correct betweencalibrationperformanceandcomputationalcost.
predictions. Thewronglypredictedsamplesalsohavedifferent
degreesofbeingwrong.WeuseImageNet-Aastestset.Forboth Impact of k. We use the indices of top-k confidences to
subfigures,weuse‘beit base’astheclassifierandcompareCA locateandselectthek-dimSoftmaxvectorsfromthetrans-
withCEandnocalibration. Ourmethodismoresuperiorwhen formedimages. InFig.6, wefindthatforvariousvalues
training/testsetscontainmorenarrowlywrongpredictions. ofkourmethodimprovesoveruncalibratedmodels. More-
over,k>5doesnotbringmuchimprovement. Considering
computationalcost,weusek=4. Notekischosenonthe
methodsonCIFAR-10. Infact,wefindpredictionsoftrans- ImageNet-Atestsetandappliedonalltheothertestsets.
formedimagesoffermuchlessdiversityunderCIFAR-10
Computationalcost.Onaserverwith1GeForceRTX3090
classifiers,losingtheirefficacyintellingpredictioncorrect-
GPU,ittakesourmethod583secondstotrainacalibratoron
ness. This could be addressed with a better method than
ImageNet-Val;incomparison,ittakePTSandtemperature
transformedimages,andweleaveitforfuturework. These
scaling987secondsand32secondsrespectivelyintraining.
results, especially those under the challenging ImageNet
Becausetemperaturescalingonlylearnsasingleparameter
andiWildCamsetups,suggestourmethodhasthepotential
(i.e.,temperature),itisthequickesttotrain. Theinference
tobetterdistinguishbetweencorrectandwrongpredictions
timeforours,temperaturescaling,andPTSissimilar: 2.33,
byconfidencescores,whichcouldleadtoimproveddeci-
1.63,and2.8millisecondsperimage,respectively. Thetime
sionmaking. AcloserlookattheROCcurvesisprovided
complexityofourmethodisthesameasthatofPTS.
inFig.5andSec.B.1ofAppendix.
Effectivenessofusingtransformedimagesascalibrator
5.Conclusion
input. Wecompare‘CA+trans’and‘CAonly’inTable1,
Table2,andTable3. Itisveryclearlythat‘CA+trans’gives Thispaperstartsfromthegeneralgoalofcalibration,math-
consistentlybettercalibrationperformancethan‘CAonly’. ematically interprets it, and derives a concrete loss func-
Itindicatesthenecessityofusingtransformedimages.
9OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
Figure5.(Left:)Comparingvariouscombinationsofimagetransformations,includingrotation(R),grayscale(S),colorjitter(C),random
erasing(E)andGaussiannoise(G).Differentcolorsmeansdifferentnumbersoftransformations.Dashedlinesdenoteperformanceof
nocalibrationandretrieval-basedaugmentationthataccessestestbatches.(Right:)VisualizationofROCcurvesofvariouscalibrators.
ExistingmethodstypicallydonotimproveAUC,whileourmethodeffectivelydoes.AllresultsinthisfigurearereportedforObjectNet
usingthemodel‘beit base patch16 384’,asintroducedinAppendixA.
consequencesofourwork,noneofwhichwefeelmustbe
35 CA specificallyhighlightedhere.
no calibration 79.0
TS
30
References
25 78.5 CA
Balanya,S.A.,Maron˜as,J.,andRamos,D. Adaptivetem-
no calibration
20 TS peraturescalingforrobustcalibrationofdeepneuralnet-
78.0
2 4 6 2 4 6 works. arXivpreprintarXiv:2208.00461,2022.
k k
Barbu,A.,Mayo,D.,Alverio,J.,Luo,W.,Wang,C.,Gut-
Figure6.Impactofkintop-kindexselection(Sec.3.3).Weuse freund, D., Tenenbaum, J., and Katz, B. Objectnet: A
ObjectNettestset.Undervariouskourmethodisbetter(lowerKS large-scale bias-controlled dataset for pushing the lim-
andhigherAUROC)thanuncalibratedmodelsandTS.Wechoose its of object recognition models. Advances in neural
k=4astrade-offbetweenperformanceandcomputationalcost. informationprocessingsystems,32,2019.
Beery, S., Cole, E., and Gjoka, A. The iwildcam 2020
competitiondataset. arXivpreprintarXiv:2004.10340,
tionforcalibration. Nameascorrectness-aware(CA)loss,
2020.
intrainingitrequirescorrect(wrong)predictionstohave
high(low)confidence,wheresuchcorrectnessisinformed Corbiere, C., Thome, N., Saporta, A., Vu, T.-H., Cord,
bytransformedversionsoforiginalimages. Duringinfer- M., and Perez, P. Confidence estimation via auxiliary
ence,ourcalibratoralsotakestransformedimagesasinput models. IEEE Transactions on Pattern Analysis and
andtendstogivehigh(low)confidencetolikelycorrectly MachineIntelligence,44(10):6043–6055,2021.
(wrongly)predictedimages. Weshowourmethodisvery
Darlow,L.N.,Crowley,E.J.,Antoniou,A.,andStorkey,
competitivecomparedwiththestateoftheartandpoten-
A.J. Cinic-10isnotimagenetorcifar-10. arXivpreprint
tially benefits decision making with plausible results on
arXiv:1810.03505,2018.
betterseparabilityofcorrectandwrongpredictions. More-
over,werevealthelimitationsoftheCEandMSElossesfor Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,
certaintypeofsamplesinthecalibrationset. Richinsights L. Imagenet: Alarge-scalehierarchicalimagedatabase.
aregivenw.r.thowourmethoddealswithsuchsamples. In In2009IEEEconferenceoncomputervisionandpattern
futurewewillstudymoreeffectivecorrectnessprediction recognition,pp.248–255.Ieee,2009.
methodstoimproveoursystemandhowourmethodcanbe
Deng, W., Gould, S., and Zheng, L. On the strong cor-
usedfortraininglargevisionlanguagemodels.
relation between model invariance and generalization.
InAdvancesinNeuralInformationProcessingSystems,
Impactstatement 2022.
Thispaperpresentsworkwhosegoalistoadvancethefield Dirac,P.A.M. Theprinciplesofquantummechanics. Num-
of machine learning. There are many potential societal ber27.Oxforduniversitypress,1981.
10
)%(
SK
)%(
CUAOptimizingCalibrationbyGainingPredictionCorrectnessAwareness
Gong, Y., Lin, X., Yao, Y., Dietterich, T. G., Divakaran, Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., and
A.,andGervasio,M. Confidencecalibrationfordomain Tran,D.Measuringcalibrationindeeplearning.InCVPR
generalizationundercovariateshift. InProceedingsof workshops,volume2,2019.
the IEEE/CVF International Conference on Computer
Tao,L.,Dong,M.,andXu,C.Dualfocallossforcalibration.
Vision,pp.8958–8967,2021.
ICML,2023.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On
Tomani, C., Gruber, S., Erdem, M. E., Cremers, D., and
calibrationofmodernneuralnetworks. InInternational
Buettner,F. Post-hocuncertaintycalibrationfordomain
conferenceonmachinelearning,pp.1321–1330.PMLR,
driftscenarios. InProceedingsoftheIEEE/CVFConfer-
2017.
ence on Computer Vision and Pattern Recognition, pp.
Gupta,K.,Rahimi,A.,Ajanthan,T.,Mensink,T.,Sminchis- 10124–10132,2021.
escu,C.,andHartley,R. Calibrationofneuralnetworks
Tomani,C.,Cremers,D.,andBuettner,F. Parameterized
usingsplines. arXivpreprintarXiv:2006.12800,2020.
temperaturescalingforboostingtheexpressivepowerin
Hendrycks, D. and Dietterich, T. Benchmarking neural
post-hocuncertaintycalibration.InEuropeanConference
networkrobustnesstocommoncorruptionsandperturba-
onComputerVision,pp.555–569.Springer,2022.
tions. arXivpreprintarXiv:1903.12261,2019.
Tomani,C.,Waseda,F.K.,Shen,Y.,andCremers,D. Be-
yondin-domainscenarios: robustdensity-awarecalibra-
Hendrycks,D.,Basart,S.,Mu,N.,Kadavath,S.,Wang,F.,
tion. InInternationalConferenceonMachineLearning,
Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M.,
pp.34344–34368.PMLR,2023.
Song,D.,Steinhardt,J.,andGilmer,J. Themanyfaces
ofrobustness: Acriticalanalysisofout-of-distribution Wang,H.,Ge,S.,Lipton,Z.,andXing,E.P. Learningro-
generalization. ICCV,2021a. bustglobalrepresentationsbypenalizinglocalpredictive
power. In Advances inNeural InformationProcessing
Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and
Systems,pp.10506–10518,2019.
Song,D. Naturaladversarialexamples. CVPR,2021b.
Wang, L., Koniusz, P., Gedeon, T., and Zheng, L. Adap-
Joy, T., Pinto, F., Lim, S.-N., Torr, P. H., and Dokania,
tive multi-head contrastive learning. arXiv preprint
P.K. Sample-dependentadaptivetemperaturescalingfor
arXiv:2310.05615,2023.
improvedcalibration. InProceedingsoftheAAAICon-
ferenceonArtificialIntelligence,volume37,pp.14919– Wang, X., Long, M., Wang, J., and Jordan, M. Transfer-
14926,2023. ablecalibrationwithlowerbiasandvarianceindomain
adaptation. AdvancesinNeuralInformationProcessing
Jung,S.,Seo,S.,Jeong,Y.,andChoi,J. Scalingofclass-
Systems,33:19212–19223,2020.
wisetraininglossesforpost-hoccalibration. ICML,2023.
Wightman,R.Pytorchimagemodels.https://github.
Krishnan,R.andTickoo,O. Improvingmodelcalibration
com/rwightman/pytorch-image-models,
withaccuracyversusuncertaintyoptimization. Advances
2019.
in Neural Information Processing Systems, 33:18237–
18248,2020. Xia,G.andBouganis,C.-S. Window-basedearly-exitcas-
cadesforuncertaintyestimation: Whendeepensembles
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers are more efficient than single models. arXiv preprint
offeaturesfromtinyimages. 2009. arXiv:2303.08010,2023.
Kull, M., Perello Nieto, M., Ka¨ngsepp, M., Silva Filho, Xiong,M.,Deng,A.,Koh,P.W.W.,Wu,J.,Li,S.,Xu,J.,
T.,Song,H.,andFlach,P. Beyondtemperaturescaling: and Hooi, B. Proximity-informed calibration for deep
Obtainingwell-calibratedmulti-classprobabilitieswith neuralnetworks. AdvancesinNeuralInformationPro-
dirichlet calibration. Advances in neural information cessingSystems,36,2024.
processingsystems,32,2019.
Zhang, J., Kailkhura,B., andHan,T.Y.-J. Mix-n-match:
Kumar, A., Liang, P. S., and Ma, T. Verified uncertainty Ensembleandcompositionalmethodsforuncertaintycal-
calibration. AdvancesinNeuralInformationProcessing ibrationindeeplearning. InInternationalconferenceon
Systems,32,2019. machinelearning,pp.11117–11128.PMLR,2020.
Mukhoti,J.,Kulharia,V.,Sanyal,A.,Golodetz,S.,Torr,P., Zou,Y.,Deng,W.,andZheng,L.Adaptivecalibratorensem-
andDokania,P. Calibratingdeepneuralnetworksusing ble: Navigatingtestsetdifficultyinout-of-distribution
focalloss. AdvancesinNeuralInformationProcessing scenarios. InProceedingsoftheIEEE/CVFInternational
Systems,33:15288–15299,2020. ConferenceonComputerVision,pp.19333–19342,2023.
11OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
A.ExperimentalSetup
Inthissection,weintroducethebenchmarkingdatasetsandclassificationmodelsusedinourpaper.
A.1.ImageNetSetup
Models. WeemploytheImageNetmodelsfromthePyTorchImageModels(timm)library(Wightman,2019),whichoffers
modelstrainedorfine-tunedontheImageNet-1ktrainingset(Dengetal.,2009). Themodelsutilizedinourpaperarelisted
below:
{ ‘beit base patch16 384’, ‘tv resnet152’, ‘tv resnet50’, ‘tv resnet101’, ‘densenet121’, ‘inception v4’, ‘densenet201’,
‘vit base patch16 384’,‘deit base patch16 224’,‘inception v3’}
Datasets. WepresentthetestsetsemployedinthemainpapertoevaluatetheaforementionedImageNetmodels. Datasets
mentionedbelowcanbeaccessedpubliclyviatheprovidedlinks.
ImageNet-A(dversarial)(Hendrycksetal.,2021b):
https://github.com/hendrycks/natural-adv-examples.
ImageNet-S(ketch)(Wangetal.,2019):
https://github.com/HaohanWang/ImageNet-Sketch.
ImageNet-R(endition)(Hendrycksetal.,2021a):
https://github.com/hendrycks/imagenet-r.
ImageNet-Blur(Hendrycks&Dietterich,2019):
https://github.com/hendrycks/robustness.
ObjectNet(Barbuetal.,2019):
https://objectnet.dev/download.html.
A.2.CIFAR-10Setup
Models. WeemploytheCIFAR-10modelsfromtheopensourcelibrary(https://github.com/kuangliu/pytorch-cifar)which
offersmodelstrainedorfine-tunedontheCIFAR-10trainingset(Krizhevskyetal.,2009). Themodelsutilizedinourpaper
arelistedbelow:
{‘VGG19’,‘DenseNet121’,‘DenseNet201’,‘ResNet18’,‘ResNet50’,‘ShuffleNetV2’,‘MobileNet’,‘PreActResNet101’,
‘RegNetX 200MF’,‘ResNeXt29 2x64d’}
Datasets. DatasetsusedintheCIFAR10setupcanbefoundthroughthefollowinglinks. CINIC(Darlowetal.,2018):
https://github.com/BayesWatch/cinic-10.
CIFAR10-C(Hendrycks&Dietterich,2019)(https://github.com/hendrycks/robustness);
A.3.iWildCamSetup
Models. We employ the iWildCam models from the open source library (https://worksheets.codalab.org/worksheets/
0x52cea64d1d3f4fa89de326b4e31aa50a)whichoffersmodelstrainedorfine-tunedontheiWildCamtrainingset(Beery
etal.,2020). Themodelsutilizedinourpaperarelistedbelow:
{ ‘iwildcam erm seed1’, ‘iwildcam deepCORAL seed0’, ‘iwildcam groupDRO seed0’, ‘iwild-
cam irm seed0’,‘iwildcam erm tune0’, ‘iwildcam ermaugment tune0’, ‘iwildcam ermoracle extraunlabeled tune0’,
‘iwildcam swav30 ermaugment seed0’,‘iwildcam dann coarse extraunlabeled tune0’,‘iwildcam afn extraunlabeled tune0’
}
Dataset. iWildCam-OOD (Beery et al., 2020) can be download from the the official guidence: https://github.com/p-
lambda/wilds/.
12OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
Resnet152, ImageNet-A Resnet152, ImageNet-R Resnet152, ImageNet-S Resnet152, ObjectNet
Beit-Base, ImageNet-A Beit-Base, ImageNet-R Beit-Base, ImageNet-S Beit-Base, ObjectNet
Figure7.ComparisonofReceiverOperatingCharacteristic(ROC)CurvesAcrossDifferentCalibrationMethods.Eachfigure’s
titlespecifiestheclassifierandthetestsetused. Itisevidentthatourmethods(greencurves)yieldahigherareaunderROCcurve
(AUROC)comparedtoothercalibrationmethods,signifyinganenhancedabilityofourmodeltodistinguishbetweencorrectandincorrect
predictionsbasedoncalibratedconfidence.
B.AdditionalVisualisations
B.1.AUROCcurves
Fig.7showsthecomparisonofReceiverOperatingCharacteristic(ROC)curvesacrossdifferentcalibrationmethods.
B.2.Distributionsofpredictions
VisualizationsofthedistributionsforcorrectandincorrectpredictionsonfourdatasetsaregiveninFig.8.
13OptimizingCalibrationbyGainingPredictionCorrectnessAwareness
(a) ImageNet-A
(b) ImageNet-R
(c) ImageNet-S
(d) ObjectNet
Figure8.Visualization of the distributions for correct and incorrect predictions of ‘beit base patch16 384’ on (a) ImageNet-A, (b)
ImageNet-R,(c)ImageNet-S,and(d)ObjectNet.Fromlefttoright,themethodsarenocalibration,temperaturescaling,andourmethod.
Wefindthatourmethodcanbetterdistinguishbetweencorrectandincorrectpredictionsbyincreasingtheconfidencevalueforcorrect
predictionsanddecreasingitforincorrectones.
14