Groma: Localized Visual Tokenization for
Grounding Multimodal Large Language Models
Chuofan Ma1⋆ Yi Jiang2† Jiannan Wu1 Zehuan Yuan2 Xiaojuan Qi1†
1The University of Hong Kong 2ByteDance Inc.
Abstract. We introduce Groma, a Multimodal Large Language Model
(MLLM) with grounded and fine-grained visual perception ability. Be-
yondholisticimageunderstanding,Gromaisadeptatregion-leveltasks
such as region captioning and visual grounding. Such capabilities are
built upon a localized visual tokenization mechanism, where an image
input is decomposed into regions of interest and subsequently encoded
into region tokens. By integrating region tokens into user instructions
and model responses, we seamlessly enable Groma to understand user-
specifiedregioninputsandgrounditstextualoutputtoimages.Besides,
to enhance the grounded chat ability of Groma, we curate a visually
grounded instruction dataset by leveraging the powerful GPT-4V and
visual prompting techniques. Compared with MLLMs that rely on the
language model or external module for localization, Groma consistently
demonstratessuperiorperformancesinstandardreferringandgrounding
benchmarks, highlighting the advantages of embedding localization into
image tokenization. Project page: https://groma-mllm.github.io/.
Locate the the reflect- Please give a brief
ion of the frog. description of <region>.
It’s located at <region>. Alarge dinosaur skeleton.
[grounding] Is it safe to drive like this? [grounding] Can you describe this image
in details?
It'snotadvisable.The manis
Inthisimage,weseea womansittingon
holdinga cupinonehandand
a concrete benchworkingonherlaptop.
talking on the phone, which
She'ssurroundedbya green baganda
means he's not using both
white bicyclethat'sleaningagainstthe
handsonthe steering wheel.
bench.Behindher,there'sa large brick
Thiscouldbeadistractionand
buildinganda tree.
increasetheriskofanaccident.
Fig.1: Groma is a multimodal large language model with exceptional region un-
derstanding and visual grounding capabilities. It can take user-defined region inputs
(boxes) as well as generate long-form responses that are grounded to visual context.
⋆ Work done during Chuofan’s internship at ByteDance.
† Corresponding authors
4202
rpA
91
]VC.sc[
1v31031.4042:viXra2 Ma et al., 2024
1 Introduction
Multimodal Large Language Models (MLLMs) have spread the sparks of artifi-
cialgeneralintelligence[5]fromlanguagetothevisualdomain[12,32,49,56,66].
OwingtothefoundationalcapabilitiesofLargeLanguageModels(LLMs)[11,35,
36,44,45], MLLMs excel in vision-language tasks that require advanced under-
standingandreasoning,suchasimagecaptioningandvisualquestionanswering.
However, despite these achievements, current MLLMs typically fall short of lo-
calization capabilities, thus cannot ground understanding to the visual context.
Such limitations constrains the model from fulfilling its potential in real-world
applications like robotics, autonomous driving, and augmented reality.
In light of the gap, one stream of research attempts to augment the LLM
to directly output quantized object coordinates for localization [3,6,7,38,49,57]
(Fig. 2(a)). While this method is simple in design, the substantial computa-
tional demands of LLMs make it challenging to process high-resolution image
inputs, which are essential for accurate localization. Besides, the nature of se-
quence outputs in LLMs is not well-suited for dense prediction tasks such as
segmentation. These concerns elicit another stream of research, which incorpo-
ratesanexternallocalizationmodule(e.g.,SAM[21])todecodeboundingboxes
or masks [25,39,42,61] (Fig. 2(b)). This approach circumvents aforementioned
issues,butintroducesadditionallatencyininferenceasitrequiresprocessingthe
image input twice with the MLLM and the localization module, respectively.
The above motivates us to explore a new paradigm for grounded MLLMs.
Drawing inspiration from open-vocabulary object detection [65], we decompose
the grounding task into two sub-problems: discovering the object (localization)
and relating the object to texts (recognition). We notice that localization alone
requires little semantic understanding but demands perceptual skills, which is
typically out of the scope of an LLM’s expertise. This inspires us to decouple
localization and recognition within MLLMs. But instead of using external mod-
ules, we propose exploiting the spatial understanding capability in the visual
tokenizer of MLLMs for localization (Fig. 2(c)). This perceive-then-understand
design also resembles human vision process.
Building upon this concept, we introduce Groma1 (Grounded Multimodal
Assistant),anMLLMwithlocalizedandfine-grainedvisualperceptionabilities.
Specifically, Groma incorporates region tokenization alongside standard image
tokenization to identify and encode potential regions of interest (ROIs) into
region tokens. During this process, location information is extracted from the
image and associated with region tokens, with each region token anchored to
the underlying ROI. This allows Groma to ground its textual output by simply
referring to region tokens, alleviating the need for the LLM to meticulously
regress object coordinates. Moreover, the tokenizer of Groma can also encode
user-specified region inputs (i.e., bounding boxes) into region tokens, which are
directly inserted into user instructions to initiate referential dialogue.
1 In Latin, Groma refers to an instrument used for accurate measurement, which
implies our focus on accurate localization for MLLMs.Groma: Localized Visual Tokenization for MLLMs 3
Compared to previous methods that augment LLMs for localization [6,7,38,
57], Groma circumvents the heavy computation of LLMs when handling high-
resolution input by settling localization to the image tokenization process. That
is, Groma can use high-resolution images for tokenizer input and downsampled
image tokens for LLM input, which saves computation without sacrificing local-
izationaccuracy.Besides,unlikemethodsadoptingseparatedesignsformodeling
groundingoutputsandreferringinputs[42,61],Gromaseamlesslyunifiesthetwo
capabilities with the use of region tokens.
Fromthedataperspective,toimprovethelocalizedunderstandingofGroma,
we adopt an extensive collection of datasets with region-level annotations for
training, which encompasses a range of region semantics from objects and re-
lationships to detailed region descriptions. In addition, to remedy the lack of
long-form grounded data, we construct a visually grounded chat dataset called
Groma Instruct for instruction finetuning. Groma Instruct is the first grounded
chat dataset constructed with both visual and textual prompts, leveraging the
powerful GPT-4V for data generation.
Our comprehensive experiments demonstrate the superiority of the design
of Groma, with results showing that it outperforms all comparable MLLMs on
established referring and grounding benchmarks. We also showcase that Groma
maintains strong image-level understanding and reasoning abilities on the con-
versationalVQAbenchmark.Moreover,toassesstheabilitytolocalizemultiple,
diverse,andvariably-sizedobjects,weadapttheLVIS[14]detectionbenchmark
for object grounding evaluation. On this challenging benchmark, Groma sur-
passes alternative methods by a significant margin (over 10% AR), highlighting
its robust and precise localization capabilities.
[54, 32, 87, 48] <loc> <box_2>
LLM LLM Decoder LLM
[54, 32, 87, 48]
Tokenizer Tokenizer Tokenizer
(a) (b) (c)
Fig.2:DifferentparadigmsofgroundedMLLMs.Wemarkthemodulesforlocalization
with . (a) LLM for localization (e.g., Kosmos-2 [38] and Shikra [7]); (b) External
modulesforlocalization(e.g.,Lisa[25]);and(c)Localizedvisualtokenization(Ours).
2 Related Work
Image-level MLLMs. Large language models (LLMs) such as GPT series [1,
52]andLLaMA[44,45]haverecentlyundergonerapiddevelopmentandsparked
a revolution in the field of natural language processing. Such progress inspires
the community to extend the foundational capabilities of LLMs to the visual
Encoder4 Ma et al., 2024
domain, giving birth to multimodal large language models (MLLMs). The pio-
neering works [2,12,26,27,56,61,66] of MLLMs typically follow a tripartite ar-
chitecture,comprisingavisualencoder,avision-languageconnector,andalarge
language model. Specifically, BLIP-2 [27] and Flamingo [2] first propose the Q-
Former/Resamplertobridgevisionandlanguage.LLaVA[61]andMiniGPT4[66]
streamline this vision-language connector to a linear layer, and introduce visual
instruction tuning to enhance the instruction-following ability of MLLMs. Fol-
lowingworks[9,49]furthershowcasetheimmensepotentialofMLLMsbyscaling
up the visual components to the magnitude as LLMs. While these works have
exhibited impressive visual understanding capabilities, they are predominantly
constrainedtoimage-leveltasks,suchasimagecaptioningandimagevisualques-
tion answering. This necessitates the research into region-level MLLMs, which
unlock more nuanced and granular visual-language interactions.
Region-level MLLMs. In pursuit of fine-grained and grounded image under-
standing, recent studies further integrate region-level data into the training of
MLLMs [6,7,38,50,51,59,64]. In particular, to model box inputs and outputs,
Kosmos-2 [38] and Shikra [7] directly quantize bounding boxes into discrete
location tokens or numeric representation of positions. GPT4RoI [63] and Re-
gionGPT[13]useasimplepoolingoperationtoextractthefeatureswithinboxes
ormasksastheregionrepresentations.WhileFerret[57]proposesaspatial-aware
visual sampler to deal with free-form region inputs. Besides, to achieve more
accurate localization, some works [25,42,62] resort to off-the-shelf models for
pixel-levelgrounding.Forinstance,LISA[25]takesthesegmentationtokengen-
erated by the MLLM as the prompts for SAM [21] to produce the segmentation
masks. GLaMM [42] and LLaVA-Ground [61] further advance the concept and
enable grounded conversation generation. Our work shares the same focus with
the aforementioned methods on region-level understanding and grounding. Yet,
we distinguish ourselves from existing studies by proposing a novel perspective
in enhancing the localization ability of MLLMs.
3 Method
Inthissection,wepresentGroma,agroundedmultimodallargelanguagemodel
capable of understanding user-defined region inputs and generating visually
groundedoutputs.WefirstillustratethemodelarchitectureofGromainSec.3.1.
Then we introduce how to format region input and output in Sec. 3.2. Finally,
we detail the learning pipelines Sec. 3.3.
3.1 Model Architecture
As illustrated in Fig. 3, Groma primarily consists of (1) an image encoder for
scene-level image tokenization, (2) a region proposer for discovering regions of
interest,(3)aregionencoderforregion-levelimagetokenization,and(4)alarge
languagemodelformodelingmultimodalinputandoutput.Wedetaileachcom-
ponent in the following paragraphs.Groma: Localized Visual Tokenization for MLLMs 5
A dog <r 10> lying on Large Language Model
a cozy coach <r>.
3
<r1> <r2> ... <rn> <r10>
Region Encoder Language
Tokenizer
Image
Encoder
What is in region
Region Proposer [62, 37, 76, 45]?
Fig.3: Overview of Groma. Groma encodes the image input into both global im-
age tokens and local region tokens. For region tokenization, a general-purpose region
proposer is introduced to discover regions of interest, followed by a light-weight re-
gionencoder.Byintegratingregiontokensintouserinstructionsandmodelresponses,
Groma unlocks the referring and grounding abilities of MLLMs.
ImageEncoder. GromaemploysapretrainedDINOv2[37]modelastheimage
encoder with the input image resolution set to 448×448. Compared with the
commonly adopted CLIP [41] visual encoder, DINOv2 is preferred in this work
for its compatibility with high-resolution inputs and fine-grained features for
localization2. However, the use of higher-resolution images leads to extended
sequencesofvisualinputforthelanguagemodel,e.g.,1024tokensinthiscase.To
savecomputations,wefurtherconcatenateeveryfourneighborpatchtokensinto
asingletokenfollowingMiniGPT-v2[6].Butslightlydifferentfrom[6],wemerge
tokens adjacent in 2D instead of 1D, which yields better results empirically.
Region Proposer. To obtain localized understanding of the image, Groma in-
novatively incorporates a region proposer into the image tokenization process.
Specifically,theregionproposerisimplementedasaclass-agnosticdetectorhead
usingtheDeformableDETR(DDETR)transformer[67].Theoriginalclassifica-
tion head of DDETR is replaced by a binary classifier to score region proposals
based on their localization quality. Inspired by ViTDet [28], we extract feature
mapsfromthelast4layersoftheimageencoder,andrescalethesefeaturemaps
to construct a hierarchical feature pyramid as the input to the region proposer.
For each image, the region proposer generates 300 region proposals, which are
then filtered by NMS and objectness scores before fed into the region encoder.
Region Encoder. Theregionencodertranslatesregionproposals(i.e.,bound-
ing boxes), coming from both user input and the region proposer, into region
tokens. Akin to the previous step, we select feature maps from the last three
layers of the image encoder to create a hierarchical feature pyramid. A multi-
scale ROIAlign [15] module as implemented in [42,63] is utilized to crop and
fuse these hierarchical features into unified region tokens. Compared with al-
ternative ways to represent regional inputs, such as numerical representation of
positions [7] and discrete location tokens [6,38], the region token representation
offers distinct benefits as it is semantically aligned with the underlying region,
which renders it more intuitive for the language model to comprehend.
2 AperformancecomparisonbetweenCLIPandDINOv2onthedetectionbenchmark
is available in our ablation study.6 Ma et al., 2024
LLM. We adopt pretrained Vicuna [11] as the language model of Groma. In
particular, we instantiate Groma with the 7B version of Vicuna. Besides, we
follow LLaVA v1.5 [31] to use an MLP layer to project the image tokens and
region tokens into the feature space of the LLM.
3.2 Input and Output Formatting
Beyond textual only instructions and responses, Groma offers the flexibility to
acceptuser-specifiedregionsasinput(referring)andgeneratevisuallygrounded
answers (grounding). Specifically, although different in task formulations, both
referringandgroundingareunifiedintooneformatwiththeuseofregiontokens.
Grounding Output. Rememberinthetokenizationprocess,eachregiontoken
is inherently anchored to a concrete location in the image, corresponding to its
region proposal. This connection allows the language model to ground its text
output to particular regions in the image by simply referring to the associated
regiontokens.However,asregiontokensarecontinuousembeddings,theycannot
be directly integrated into the codebook of the language model and referenced
inthetextoutput.Tobridgethegap,wefurtherintroduceasetofproxytokens
“<r >, <r >, ..., <r >” to register region tokens. As illustrated below, Groma
1 2 n
can refer to any region in the output by addressing the proxy token.
User: Here is an image with region crops from it. Image: <image>. Regions:
<r ><region>, <r ><region>, ..., <r ><region>. [grounding] Please briefly
1 2 n
describe the image content.
Groma:<p>Adog</p><roi><r ></roi>isjumpingtocatch<p>afrisbee</p>
4
<roi><r ></roi> over <p>a fallen man</p> <roi><r ></roi>.
7 1
<image>and<region>standforplaceholdersofimagetokensandregiontokens,
whicharereplacedbycorrespondingvisualtokensbeforebeingfedintotheLLM.
<p>and</p>marksthestartandendofthegroundedphrase.<roi>and</roi>
are used to enclose the referenced regions. In addition, we use a special token
[grounding] to inform the model to generate grounded responses.
Referring Input. Foraregionpointedoutbytheuser,wetreatitthesameas
region proposals from the region proposer, i.e., encoding it into a region token
and assigning a proxy token to it. This allows us to incorporate user-specified
regions into our instructions by inserting corresponding region tokens. A simple
exampleofreferentialdialogueinGromaisgivenbelow,where<r >comesfrom
10
user-specified region input.
User: Here is an image with region crops from it. Image: <image>. Regions:
<r ><region>, <r ><region>, ..., <r ><region>. What is <r ><region>?
1 2 n 10
Groma: A cute cat sleeping on a wooden bench.Groma: Localized Visual Tokenization for MLLMs 7
3.3 Model Training
The training of Groma is partitioned into three stages: (i) detection pretraining
forlocalizationability,(ii)alignmentpretrainingforimage-levelandregion-level
vision-languagealignment,(iii)instructionfinetuningforenhancedconversation
capability. Tab. 1 enumerates the datasets used at different training stages. Ad-
ditionally, we provide the instruction templates used to convert task-specified
datasets to instruction following format in Appendix A.
Table1:Datasetsusedatthreetrainingstages.RefCOCO/g/+isshortforRef-
COCO,RefCOCO+,andRefCOCOg.RECmeansreferringexpressioncomprehension.
Trainingstage Datatypes Datasets
Detectionpretraining Detection COCO,Objects365,OpenImages,V3Det,SA1B
Imagecaption ShareGPT-4V-PT
Groundedcaption Flickr30kEntities
Alignmentpretraining
Regioncaption VisualGenome,RefCOCOg
REC COCO,RefCOCO/g/+,Grit-20m
Groundedcaption Flickr30kEntities
Regioncaption VisualGenome,RefCOCOg
Instructionfinetuning
REC COCO,RefCOCO/g/+
Instructionfollowing GromaInstruct,LLaVAInstruct,ShareGPT-4V
Detection Pretraining. This training stage only involves the image encoder
and the region proposer, which collectively constitute a DDETR-like detector.
Theimageencoderiskeptfrozenduringtraining.Toendowtheregionproposer
withlocalizationcapability,anextensivecollectionofdetectiondatasets,includ-
ing COCO [29], Objects365 [43], OpenImages [24], and V3Det [46], is utilized
for large-scale pretraining. Notably, category information is omitted from the
training process, with a primary focus on box supervision.
Considering traditional detection data are typically limited to object-level
annotations,wecomplementthetrainingwithatwomillionsubsetofSA1B[22]
datafilteredbyGLEE[18].OriginalmaskannotationsofSA1Baretransformed
into bounding boxes for consistency. The inclusion of this enriched dataset en-
courages the region proposer to produce region proposals across a wide spec-
trum of granularities, encompassing not only object instances but also their
constituent parts and various background stuff.
AlignmentPretraining. ToalignvisionandlanguagefeaturespaceofGroma,
we pretrain the model on a wide range of vision-language tasks. Specifically,
for image-level alignment, we leverage ShareGPT-4V-PT [8] for detailed image
captioning. For region-level alignment, we engage COCO [29], RefCOCO [20],
RefCOCO+ [58], RefCOCOg [34], and Grit-20m [38] for referring expression
comprehension (REC), Visual Genome [23] for region captioning, and Flickr30k
Entities [40] for grounded caption generation. To maintain training efficiency,
we focus finetuning efforts on the MLP projection layer and the region encoder,
while other modules are kept frozen throughout the training.8 Ma et al., 2024
Instruction Finetuning. Basedonalignmentpretraining,werefinethetrain-
ingdatatofocusexclusivelyonhigh-qualitydatasetsandproceedtounfreezethe
language model for finetuning purposes. At this stage, LLaVA Instruct [32] and
ShareGPT-4V[8]areincorporatedtoimprovetheconversationalandinstruction-
followingcapabilitiesofGroma3.Besides,wecurateahigh-qualitygroundedchat
dataset, named Groma Instruct (see next section for more details), to facilitate
synergy of chatting and grounding abilities of Groma.
Discussions. A major difference between the training of Groma and current
MLLMs is the integration of dedicated detection pretraining, which endows
Groma with robust and precise localization ability. Thanks to the decoupled
architecture of location and understanding within Groma, we circumvent the
need to involve the LLM during detection pretraining. Such a strategic design
allows Groma to benefit from pretraining on millions of bounding box annota-
tions — a task that would be computationally prohibitive for classic MLLMs.
4 GPT4V-assisted Grounded Conversation Generation
Visual dialogue data have proven to be crucial in advancing the conversational
capability of the MLLM as a visual chatbot. Previous methods mostly rely on
coarse-grainedimagedescriptionstoderivefree-formvisualdialogues,whichtyp-
ically lack fine-grained region details and precise location information [32,66].
For grounded MLLMs, such free-form dialogue data are shown to be insufficient
to enable the model to generate long-form grounded responses [61] - as the for-
mat of grounded responses significantly deviates from that of normal responses,
it could be challenging for the grounded MLLM to generalize its grounding ca-
pability to long-form conversations.
Tobridgethegap,wehavemeticulouslycuratedadatasetcontaining30kvi-
suallygroundedconversationsforinstructionfinetuning,namedGromaInstruct.
AnillustrativeexamplefromGromaInstructisshowcasedinFig.4.Specifically,
we select images with dense region annotations from Visual Genome [23] (VG),
and take the following steps to construct grounded conversations with the assis-
tance of advanced GPT-4V model:
– First, we remove highly overlapped regions (bounding boxes) from VG an-
notations, normally leaving 3-10 regions of interest for each image. Then we
adapt the visual prompting techniques from SoM [55] to overlay a bright nu-
meric marker at the center of each region. Using this marked image as input
unleashesthegroundingcapabilitiesofGPT-4V-itcaneasilymakereferences
to specific image regions by addressing the corresponding numbers.
– Besides visual input, we supply GPT-4V with rich region descriptions, image
descriptions, and image-based Q&A pairs, coming from COCO [29] and VG
3 LLaVA Instruct contains three types of instruction data, namely conversation, de-
tailed description, and complex reasoning. Since the detailed description part of
LLaVAInstructhasseverehallucinations,wereplaceitwithShareGPT-4Vasin[8].Groma: Localized Visual Tokenization for MLLMs 9
annotations4. While such textual context is optional for GPT-4V input, we
empirically find it useful to reduce hallucinations in generated contents and
resolve potential ambiguities in visual prompts5.
– Inspired by prior studies on visual chat data construction [8,32,47,66], we
further provide GPT-4V with manually designed grounded chat as context
examples.Thisprovokesthein-context-learningabilityofGPT-4Vtogenerate
grounded conversations in a uniform format. We also take a post-processing
stage to filter out conversations not following the pre-defined format.
Region descriptions: Q&A pairs:
[1] dog walking on beach What is the surfer holding?
[2] surfboard under woman’s arm Surf board.
[3] woman walking in wetsuit ===
[4] sand on ocean shore What is the dog doing?
[5] ocean with rolling waves Following its master.
[6] hazy mountains on horizon …
Image Descriptions:
A woman in a wetsuit carries a surfboard and walks with a dog.
A lady in a wet suit walking with a dog on a beach.
A woman carries a surf board as a dog walks beside her.
A woman and her dog are walking away from the ocean with a surfboard.
A person is walking with a dog and a surfboard.
GeneratedGroundedConversations:
Question:Canyoudescribetomewhatyouseeinthisimage?
Answer:Sure,Iseeabeachscenewith<p>awoman</p>[3]wearingawetsuit.She'swalkingnearthewater'sedge,and
there's<p>adog</p>[1]walkingalongsideher.<p>Thewoman</p>[3]iscarrying<p>asurfboard</p>[2]underher
arm.Icanalsosee<p>theocean</p>[5]withsomewavesand<p>faintmountains</p>[6]inthedistance.
===
Question:Whatkindofdogisit,andwhat'sitdoing?
Answer:Itlookslike<p>amedium-sized,short-haireddog</p>[1],quitepossiblyamixedbreed.<p>Thedog</p>[1]
isjuststrollingbeside<p>thewoman</p>[3],seeminglyfollowingherastheybothwalkacross<p>thesand</p>[4].
===
Question:Isthebeachcrowded,ordoesitseemdeserted?
Answer:Itactuallylooksquitedeserted,withnooneelseinsightapartfrom<p>thewoman</p>[3]and<p>herdog
</p>[1].Itgivesoffapeaceful,solitaryvibe-justthemandthe<p>vastocean</p>[5]and<p>sandyshore</p>[4].
===
Question:Whatisthestateoftheocean,andisitsuitableforsurfing?
Answer:<p>Theocean</p>[5]appearsquiteserenebutwithrollingwaves,henceappropriateforsurfing.
Fig.4: A data example from Groma Instruct. The top part demonstrates the
image input with visual prompts and contextual text input to GPT-4V. Notably, all
numberedregionshavecorrespondingboundingboxannotations,whicharenotshown
in the image. The bottom part is the grounded conversations generated by GPT-4V.
4 We select VG images that also have a coco id. Thus, we can retrieve corresponding
image captions from COCO Caption.
5 There are cases where two regions highly overlap with each other and GPT-4V
can hardly tell from the image which region maps to which numeric marker. For
these cases, GPT-4V could rely on the numbered region descriptions to find out
correspondences between regions and markers.10 Ma et al., 2024
5 Experiments
Inthissection,wefirstquantitativelyaccesstheabilitiesofGromaongrounding
(Sec. 5.2), referring (Sec. 5.3), and image-based conversation (Sec. 5.4) tasks.
ThenweprovidequalitativeresultstoexemplifythestrongcapabilitiesofGroma
onawiderangeofregion-leveltasks(Sec.5.5).Finally,weablatethedesignand
training of Groma in Sec. 5.6.
5.1 Implementation Details.
We adopt DINOv2-L/14 [37] as the image encoder and Vicuna-7B v1.5 [11] as
the language model. The region proposer follows an encoder-decoder architec-
turewith6encoderlayersand6decoderlayers.Wefurtheremploymixedquery
selectionandlook-forward-twiceschemeasin[60]toaccelerateconvergence.We
set NMS threshold to 0.6 and filter out region proposals with objectness scores
lower than 0.15. Subsequently, we select the top 100 region proposals if there
aremorethan100proposalsleftafterfiltering.Thisresultsinnomorethan356
visual tokens in total. For training, we sequentially proceed 12 epochs of detec-
tion pretraining, 2 epochs of alignment pretraining, and 1 epoch of instruction
finetuning. More training details can be found in the Appendix C.
5.2 Grounding Benchmark Results
Table 2: Resultsonreferringexpressioncomprehensionbenchmarks.Wereportaccu-
racywiththeIoUthresholdsetto0.5.WemakeQwen-VLgraybecauseitusesamuch
larger visual tokenizer (1.9B ViT-bigG [16]).
RefCOCO RefCOCO+ RefCOCOg
Method Modeltype Average
val testA testB val testA testB val test
MDETR[19] 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89 81.81
G-DINO[33] Specialist 90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02 86.60
UNINEXT-L[54] 91.43 93.73 88.93 83.09 87.90 76.15 86.91 87.48 86.95
VisionLLM[51] – 86.70 – – – – – – –
OFA[48] 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58 72.65
Shikra[7] 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19 82.93
Ferret[57] Generalist 87.49 91.35 82.45 80.78 87.38 73.14 83.93 84.76 83.91
MiniGPT-v2[6] 88.69 91.65 85.33 79.97 85.12 74.45 84.44 84.66 84.29
Qwen-VL[4] 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48 85.83
Groma 89.53 92.09 86.26 83.90 88.91 78.05 86.37 87.01 86.52
We evaluate the localization capability of Groma on visual grounding tasks.
Tab. 2 showcases our performance on three classic referring expression com-
prehension benchmarks: RefCOCO [20], RefCOCO+ [58], and RefCOCOg [34].
Groma notably surpasses other generalist models of similar model size across
all metrics. Even in comparison with Qwen-VL [4], which uses a stronger visual
tokenizer and trains on 10× more grounding data, Groma delivers superior ac-
curacy on average. Moreover, as a generalist model, Groma shows competitiveGroma: Localized Visual Tokenization for MLLMs 11
results with state-of-the-art specialist models [33,54]. These findings underscore
the strong capability of Groma in visual grounding.
However, we notice that traditional REC benchmarks only cover a narrow
range of common objects in their referring expressions, which is insufficient to
thoroughly evaluate the MLLM’s localization capability. Therefore, we further
introduce LVIS-Ground, an object grounding benchmark converted from the
LVIS [14] detection data. LVIS-Ground contains 4299 images covering 1203 cat-
egoriesofobjects,withonaverage3.7targetobjectsperimage.Complementary
to REC benchmarks, LVIS-Ground focuses on testing the model’s ability to
locate multiple, diverse, and variably-sized objects. For more details of LVIS-
Ground, please refer to the Appendix B.
Tab. 3 presents ours results on LVIS-Ground. Notably, Groma demonstrates
clear advantages over other grounded MLLMs, especially on the AR@0.75 met-
ric. This evidences that the specialized design and training indeed bring more
accuratelocalizationforGroma.Moreover,itisnoteworthythatcurrentMLLMs
all fall short of small object localization (AR@s metric). We conjecture this is
mainly because the training data (e.g., RefCOCO/g/+, Flickr30k) lack annota-
tions for small objects. We also notice a common failure mode of these methods
is that, most of the time they only predict one box per image. This is an ex-
pected behavior as the they heavily rely on REC data for grounding training,
which only has one target object per query. These findings call for the necessity
of diversifying grounding data used for training in future MLLMs.
Table 3: Results on the LVIS-Ground benchmark. We report average recall (AR) to
measure performances. For each model, we use the native prompt template recom-
mended by the paper for evaluation.
Method AR AR@0.5 AR@0.75 AR@s AR@m AR@l
Shikra[7] 4.9 14.2 2.0 0.1 3.1 18.5
MiniGPT-v2[6] 11.4 19.8 11.2 0.3 8.0 41.1
Ferret[57] 16.8 29.6 16.3 1.6 16.7 51.1
Groma 28.8 37.9 30.3 8.7 35.6 64.3
5.3 Referring Benchmark Results
We evaluate Groma on the region captioning task to assess its fine-grained re-
gion understanding capability. To prompt the model to generate region-level
descriptions, we use queries like “Please describe <region> in details.”, where
‘<region>’ is replaced by the proxy token and region token corresponding to
thetargetregion.Tab.4presentsourresultsontwoestablishedregioncaptioning
benchmarks, RefCOCOg and Visual Genome. Without task-specific finetuning,
Groma shows comparable or improved performance over GLaMM6 [42], which
has separate designs for input referring and output grounding. This exemplifies
the superiority of unified refer-and-ground formulation in Groma.
6 We re-evaluate the performance of GLaMM using the officially released checkpoint
after fixing the bug in its original evaluation scripts.12 Ma et al., 2024
Table 4: Results on region captioning benchmarks. We report METEOR and CIDEr
scores to measure caption quality. †: with task-specific finetuning.
RefCOCOg VisualGenome
Method
METEOR CIDEr METEOR CIDEr
GRIT[53] 15.2 71.6 17.1 142
Kosmos-2[38] 14.1 62.3 – –
GPT4RoI[63] – – 17.4 145.2
GLaMM† [42] 16.1 101.9 19.0 163.9
Groma 16.8 107.3 19.0 158.4
5.4 Conversational VQA Benchmark Results
Inadditiontoregion-leveltasks,wefurtherevaluateGromaontheconversational
style VQA benchmark, LLaVA Bench (COCO) [32], which contains three types
of questions, namely conversation, detailed description, and complex reasoning.
As shown in Tab. 5, Groma surpasses the strong baseline method LLaVA [32]
andachievescompetitiveperformanceamonggroundedMLLMs,especiallyinde-
tailedimagedescription.ThisdemonstratesthatGromamaintainsdecentimage
understanding and visual chatting abilities. For the underperformance in con-
versation and complex reasoning questions, we speculate this could be resulted
from the DINOv2 features. Recent studies [17,30] have shown that DINOv2
image tokenizer slightly underperforms CLIP tokenizer in image understanding
tasks as DINOv2 features are not inherently aligned with text. But we believe
such gap can be closed by scaling up vision-language alignment pretraining.
Table 5: Results on LLaVA-Bench (COCO).
Method Conversation Description Reasoning Average
LLaVA[32] 85.4 68.3 92.1 81.9
Kosmos-2[38] 71.7 63.4 74.9 70.0
Shikra[7] 80.6 70.7 88.1 79.9
LLaVA-G[61] 79.3 71.2 92.8 81.2
Ferret[57] 84.4 79.4 96.3 86.7
Groma 82.6 84.0 88.8 85.2
5.5 Qualitative Results
Fig.5presentsacomparisonbetweenGromaandothergroundedMLLMsonthe
groundedimagecaptioningtask.Wechooseanexemplarimagethatisinherently
challenging with multiple and occluded instances to ground. Groma manifests
exceptional grounding performance in this case with the highest recall and min-
imum hallucinations. In addition, we provide several visualization examples in
Fig.6foracomplementaryunderstandingofGroma’sabilitiesongroundedchat
and referential dialogue. We show that Groma is capable of generating long-
form, grounded and logically rich answers, which can be mainly attributed to
the introduction of Groma Instruct data in finetuning.Groma: Localized Visual Tokenization for MLLMs 13
Kosmos2 MiniGPT v2 Groma
An image of a herd of giraffes and Aherdofzebrasandgiraffesarestanding Agroupofgiraffesandzebrasare
zebras. inafieldwithtreesinthebackground. walkingthroughafield.
Fig.5: A qualitative comparison on the grounded image captioning task.
[grounding] Where is this city? Can [grounding] Which equipment should What is the functionality of <region>
you identify its landmark? I choose for aerobic exercise? in the image?
This is a view of Sydney, Australia. You For aerobic exercise, you might In the image, the telescope is situated
can identify the landmark by the iconic consider using the treadmillor the on a hill, with a dark blue sky and stars
large white sailboat-shaped building elliptical machine. Both are great in the background. The telescope
which is the Sydney Opera House. for cardiovascular workouts. serves the purpose of observing
celestial objects, such as stars, planets,
and other astronomical phenomena. It
allows people to study and learn about
the universe, as well as to enjoy the
beauty of the night sky. The
telescope's location on the hill
provides an elevated vantage point,
which can enhance its view of the sky
and the celestial objects it is observing.
Fig.6: Examples on visually grounded chat and referential dialogue.
5.6 Ablation
CLIP vs. DINOv2. To quantitatively assess the differences in localization
capabilities between CLIP and DINOv2, we compare the two backbones on the
COCOdetectionbenchmarkinTab.6.Forthiscomparison,weequipeachback-
bonewithaDDETR[67]detectionheadandfinetuneonlythedetectionheadon
COCOdataset.Itcanbeseenthatunderthesameresolution,DINOv2backbone
significantlyoutperformsCLIPbackboneby6.5AP.Furthermore,byscalingthe
resolutionto448×448,DINOv2backboneachievesacommendableperformance
of 43.6 AP. The results consolidate our choice of DINOv2 backbone in Groma.
Frozen LLM. In Tab. 7, we reveal that Groma retains robust localized un-
derstanding even without finetuning the LLM, i.e., it demonstrates a referring
ability on par with GPT4ROI [63] (148.0 vs. 145.2) and grounding ability com-
parable to Ferret [57] (84.02% vs. 83.91%). This finding suggests our design14 Ma et al., 2024
effectively decouples localization and understanding within Groma, such that it
requires minimum ‘new knowledge’ from the LLM for localized understanding.
Token Merge. To save computations, Groma by default concatenates every 4
image tokens into one as LLM inputs. Through control experiments in Tab. 8,
wefindthatsuchdownsamplinghasnegligibleimpactsonthegroundingperfor-
mances (e.g., less than 0.1% average accuracy drop on the REC benchmarks).
The results evidence that the decoupled design is optimal in both efficiency and
localization accuracy.
Table 6: Object detection Table 7: Referring and grounding abilities with
performances on COCO with the frozen LLM. We measure referring ability with
differentbackbonesandimage CIDErscoreonVisualGenomeandgroundingabil-
resolutions. ity with average accuracy on REC benchmarks.
Backbone Resolution AP Stage LLMstatus Referring Grounding
CLIP 336×336 32.4 pretraining frozen – 82.33
DINOv2 336×336 38.9 finetuning frozen 148.0 84.02
DINOv2 448×448 43.6 finetuning unfrozen 158.4 86.52
Table 8: Ablation on image token downsampling on the REC benchmarks.
RefCOCO RefCOCO+ RefCOCOg
Downsampling Average
val testA testB val testA testB val test
✓ 89.32 92.15 85.96 84.11 88.10 78.40 86.33 87.40 86.47
✗ 89.54 92.54 86.18 83.72 88.52 78.96 86.17 86.84 86.55
6 Limitations and Conclusions
In this paper, we introduce a novel paradigm, Groma, to unleash the localized
perceptioncapabilitiesofMLLMs.Wemakethepioneeringattempttoembedlo-
calizationintoimagetokenization.Ourparadigmisbasedonaperception-then-
understand mindset that separates localization from high-level understanding
and reasoning. Without introducing external modules, our approach overcomes
the resolution bottleneck of using LLMs as location decoders and unifies refer-
ring and visual grounding tasks. Extensive experiments showcase the superior
performanceof ourapproachin localizedperception, asevidencedby its success
in referring and visual grounding tasks.
However, the current implementation does not support free-form region in-
putsandpixel-levelgrounding.Apromisingdirectiontoaddresssuchlimitations
istore-implementtheregionencoderwithavisualsamplerasin[57,68]andre-
placetheboxregionproposerbyamaskregionproposerlikeMask2Former[10].
We leave this for future studies.Groma: Localized Visual Tokenization for MLLMs 15
References
1. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,
D.,Altenschmidt,J.,Altman,S.,Anadkat,S.,etal.:Gpt-4technicalreport.arXiv
preprint arXiv:2303.08774 (2023) 3
2. Alayrac,J.B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Hasson,Y.,Lenc,K.,Men-
sch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for
few-shotlearning.AdvancesinNeuralInformationProcessingSystems35,23716–
23736 (2022) 4
3. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,
J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 (2023) 2
4. Bai,J.,Bai,S.,Yang,S.,Wang,S.,Tan,S.,Wang,P.,Lin,J.,Zhou,C.,Zhou,J.:
Qwen-vl: A versatile vision-language model for understanding, localization, text
reading, and beyond (2023) 10
5. Bubeck,S.,Chandrasekaran,V.,Eldan,R.,Gehrke,J.,Horvitz,E.,Kamar,E.,Lee,
P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of artificial general intelligence:
Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023) 2
6. Chen,J.,Zhu,D.,Shen,X.,Li,X.,Liu,Z.,Zhang,P.,Krishnamoorthi,R.,Chan-
dra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified
interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478
(2023) 2, 3, 4, 5, 10, 11
7. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-
ing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195
(2023) 2, 3, 4, 5, 10, 11, 12
8. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:
Sharegpt4v: Improving large multi-modal models with better captions. arXiv
preprint arXiv:2311.12793 (2023) 7, 8, 9
9. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q.,
Zhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning
for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238 (2023) 4
10. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention
mask transformer for universal image segmentation. In: Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.1290–1299
(2022) 14
11. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang,
S.,Zhuang,Y.,Gonzalez,J.E.,et al.:Vicuna:Anopen-sourcechatbotimpressing
gpt-4with90%*chatgptquality.Seehttps://vicuna.lmsys.org(accessed14April
2023) (2023) 2, 6, 10
12. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.:
InstructBLIP: Towards general-purpose vision-language models with instruction
tuning. In: Thirty-seventh Conference on Neural Information Processing Systems
(2023), https://openreview.net/forum?id=vvoWPYqZJA 2, 4
13. Guo, Q., Mello, S.D., Yin, H., Byeon, W., Cheung, K.C., Yu, Y., Luo, P., Liu, S.:
Regiongpt: Towards region understanding vision language model (2024) 4
14. Gupta, A., Dollar, P., Girshick, R.: Lvis: A dataset for large vocabulary instance
segmentation. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 5356–5364 (2019) 3, 11, 20
15. He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: Proceedings of the
IEEE international conference on computer vision. pp. 2961–2969 (2017) 516 Ma et al., 2024
16. Ilharco,G.,Wortsman,M.,Wightman,R.,Gordon,C.,Carlini,N.,Taori,R.,Dave,
A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt,
L.: Openclip (2021), https://doi.org/10.5281/zenodo.5143773 10
17. Jiang, D., Liu, Y., Liu, S., Zhang, X., Li, J., Xiong, H., Tian, Q.: From clip to
dino: Visual encoders shout in multi-modal large language models (2023) 12
18. Junfeng, W., Yi, J., Qihao, L., Zehuan, Y., Xiang, B., Song, B.: General object
foundationmodelforimagesandvideosatscale.arXivpreprint2312.09158(2023)
7
19. Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.: Mdetr-
modulated detection for end-to-end multi-modal understanding. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 1780–1790
(2021) 10, 20
20. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to
objects in photographs of natural scenes. In: Proceedings of the 2014 conference
onempiricalmethodsinnaturallanguageprocessing(EMNLP).pp.787–798(2014)
7, 10
21. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023) 2, 4
22. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023) 7
23. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis,Y.,Li,L.J.,Shamma,D.A.,etal.:Visualgenome:Connectinglanguage
and vision using crowdsourced dense image annotations. International journal of
computer vision 123, 32–73 (2017) 7, 8
24. Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Ka-
mali, S., Popov, S., Malloci, M., Kolesnikov, A., et al.: The open images dataset
v4:Unifiedimageclassification,objectdetection,andvisualrelationshipdetection
at scale. International Journal of Computer Vision 128(7), 1956–1981 (2020) 7
25. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning
segmentationvialargelanguagemodel.arXivpreprintarXiv:2308.00692(2023) 2,
3, 4
26. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
4
27. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023) 4
28. Li,Y.,Mao,H.,Girshick,R.,He,K.:Exploringplainvisiontransformerbackbones
for object detection. In: European Conference on Computer Vision. pp. 280–296.
Springer (2022) 5
29. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part V 13. pp. 740–755. Springer (2014) 7, 8, 21
30. Lin,Z.,Liu,C.,Zhang,R.,Gao,P.,Qiu,L.,Xiao,H.,Qiu,H.,Lin,C.,Shao,W.,
Chen,K.,etal.:Sphinx:Thejointmixingofweights,tasks,andvisualembeddings
formulti-modallargelanguagemodels.arXivpreprintarXiv:2311.07575(2023) 12
31. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.
arXiv preprint arXiv:2310.03744 (2023) 6Groma: Localized Visual Tokenization for MLLMs 17
32. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural
information processing systems 36 (2024) 2, 8, 9, 12
33. Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,Li,C.,Yang,J.,Su,H.,Zhu,
J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set
object detection. arXiv preprint arXiv:2303.05499 (2023) 10, 11
34. Mao,J.,Huang,J.,Toshev,A.,Camburu,O.,Yuille,A.L.,Murphy,K.:Generation
and comprehension of unambiguous object descriptions. In: Proceedings of the
IEEE conferenceon computervision and patternrecognition. pp. 11–20 (2016) 7,
10
35. OpenAI: Chatgpt. https://openai.com/blog/chatgpt/ (2023) 2
36. OpenAI: Gpt-4 technical report (2023) 2
37. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) 5, 10
38. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-
2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824 (2023) 2, 3, 4, 5, 7, 12
39. Pi, R., Gao, J., Diao, S., Pan, R., Dong, H., Zhang, J., Yao, L., Han, J., Xu,
H., Zhang, L.K.T.: Detgpt: Detect what you need via reasoning. arXiv preprint
arXiv:2305.14167 (2023) 2
40. Plummer,B.A.,Wang,L.,Cervantes,C.M.,Caicedo,J.C.,Hockenmaier,J.,Lazeb-
nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentencemodels.In:ProceedingsoftheIEEEinternationalconferenceon
computer vision. pp. 2641–2649 (2015) 7
41. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021) 5
42. Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer,
R.M.,Xing,E.,Yang,M.H.,Khan,F.S.:Glamm:Pixelgroundinglargemultimodal
model. arXiv preprint arXiv:2311.03356 (2023) 2, 3, 4, 5, 11, 12
43. Shao,S.,Li,Z.,Zhang,T.,Peng,C.,Yu,G.,Zhang,X.,Li,J.,Sun,J.:Objects365:
A large-scale, high-quality dataset for object detection. In: Proceedings of the
IEEE/CVF international conference on computer vision. pp. 8430–8439 (2019) 7
44. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023) 2, 3
45. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 2, 3
46. Wang,J.,Zhang,P.,Chu,T.,Cao,Y.,Zhou,Y.,Wu,T.,Wang,B.,He,C.,Lin,D.:
V3det: Vast vocabularyvisual detectiondataset. arXivpreprint arXiv:2304.03752
(2023) 7
47. Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., Jiang, Y.G.: To see is to
believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint
arXiv:2311.07574 (2023) 9
48. Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,
J., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple
sequence-to-sequencelearningframework.In:InternationalConferenceonMachine
Learning. pp. 23318–23340. PMLR (2022) 1018 Ma et al., 2024
49. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,
L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv
preprint arXiv:2311.03079 (2023) 2, 4
50. Wang, W., Shi, M., Li, Q., Wang, W., Huang, Z., Xing, L., Chen, Z., Li, H., Zhu,
X.,Cao,Z.,etal.:Theall-seeingproject:Towardspanopticvisualrecognitionand
understanding of the open world. arXiv preprint arXiv:2308.01907 (2023) 4
51. Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou,
J.,Qiao,Y.,etal.:Visionllm:Largelanguagemodelisalsoanopen-endeddecoder
for vision-centric tasks. Advances in Neural Information Processing Systems 36
(2024) 4, 10
52. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,
D., et al.: Chain-of-thought prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems 35, 24824–24837 (2022) 3
53. Wu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., Wang, L.: Grit: A
generative region-to-text transformer for object understanding. arXiv preprint
arXiv:2212.00280 (2022) 12
54. Yan,B.,Jiang,Y.,Wu,J.,Wang,D.,Luo,P.,Yuan,Z.,Lu,H.:Universalinstance
perception as object discovery and retrieval. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.15325–15336(2023)
10, 11
55. Yang, J., Zhang, H., Li, F., Zou, X., Li, C., Gao, J.: Set-of-mark prompting un-
leashesextraordinaryvisualgroundingingpt-4v.arXivpreprintarXiv:2310.11441
(2023) 8
56. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,
Shi, Y., et al.: mplug-owl: Modularization empowers large language models with
multimodality. arXiv preprint arXiv:2304.14178 (2023) 2, 4
57. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv
preprint arXiv:2310.07704 (2023) 2, 3, 4, 10, 11, 12, 13, 14
58. Yu,L.,Poirson,P.,Yang,S.,Berg,A.C.,Berg,T.L.:Modelingcontextinreferring
expressions. In: Computer Vision–ECCV 2016: 14th European Conference, Ams-
terdam,TheNetherlands,October11-14,2016,Proceedings,PartII14.pp.69–85.
Springer (2016) 7, 10
59. Yuan, Y., Li, W., Liu, J., Tang, D., Luo, X., Qin, C., Zhang, L., Zhu, J.:
Osprey: Pixel understanding with visual instruction tuning. arXiv preprint
arXiv:2312.10032 (2023) 4
60. Zhang,H.,Li,F.,Liu,S.,Zhang,L.,Su,H.,Zhu,J.,Ni,L.M.,Shum,H.Y.:Dino:
Detrwithimproveddenoisinganchorboxesforend-to-endobjectdetection.arXiv
preprint arXiv:2203.03605 (2022) 10
61. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang,
L., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal
models. arXiv preprint arXiv:2312.02949 (2023) 2, 3, 4, 8, 12
62. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang,
L., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal
models. arXiv preprint arXiv:2312.02949 (2023) 4
63. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo,
P.: Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv
preprint arXiv:2307.03601 (2023) 4, 5, 12, 13
64. Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., Kang, B.: Bubogpt: Enabling
visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581 (2023) 4Groma: Localized Visual Tokenization for MLLMs 19
65. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty-
thousandclassesusingimage-levelsupervision.In:EuropeanConferenceonCom-
puter Vision. pp. 350–368. Springer (2022) 2
66. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023) 2, 4, 8, 9
67. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable
transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159
(2020) 5, 13
68. Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., Lee,
Y.J.: Segment everything everywhere all at once. Advances in Neural Information
Processing Systems 36 (2024) 1420 Ma et al., 2024
A Task-Specified Instruction Templates
In complementary to discussion on training datasets in Sec. 3.3, we list a few
instruction templates used to convert task-specified datasets to instruction fol-
lowingformatinTab.9.Specifically,weconvertthedetectiondatasetCOCOto
multiple objects grounding data in a similar format as REC data.
Table9:Instructiontemplates.Werandomlyselectthreetemplatesfromeachtask
for illustration.
Task Template
Whatisthisphotoabout?
Imagecaptioning Describethefollowingimage.
Analyzetheimageinacomprehensiveanddetailedmanner.
Whatis<region>?
Regioncaptioning Pleasebrieflydescribe<region>.
Giveaconcisedescriptionof<region>.
Locate<p>{expression}</p>intheimage.
Referringexpression
Whichregionmatches<p>{expression}</p>?
comprehension
Identifytheregionthatcorrespondsto<p>{expression}<p>.
Locateall<p>{objectclass}</p>inthisimage.
Multipleobjects
Findoutallinstancesof<p>{objectclass}</p>intheimage.
grounding
Detectandlisteach<p>{objectclass}</p>thatappearsinthepicture.
[grounding]Givemeashortdescriptionoftheimage.
Groundedimage
[grounding]Succinctlysummarizewhatyouseeintheimage.
captioning
[grounding]Pleasesummarizethecontentofthisimageinbrief.
Groundedchat [grounding]{Free-formuserinstructions}.
B LVIS-Ground Benchmark
CurrentMLLMstypicallydonotsupportdetectingmultiplecategoriesofobjects
at the same time. Therefore, to customize the LVIS [14] detection benchmark
for MLLM evaluation, each time we only select one object class that is included
in the image to ground. For instance, the grounding query can be formulated as
“Locate all {object class name} in this image”. However, this ‘one-by-one’ eval-
uation strategy unavoidably leads to low efficiency. To save time and maintain
class balance, we randomly sample at most 5 images for each object category7
from the LVIS validation set to construct LVIS-Ground.
There are often multiple ground-truth boxes for a query in LVIS-Ground. In
such cases, traditional methods either adopt the ANY-Protocol or MERGED-
BOXES-Protocoltoevaluateperformance[19].Tobespecific,theANY-Protocol
considers recall to be 100% if the prediction matches any of the ground-truth
boxes (e.g., with IoU > 0.5), which fails to truly reflect the model’s capability
7 Some categories have fewer than 5 samples in the original LVIS validation set.Groma: Localized Visual Tokenization for MLLMs 21
in finding out all object instances. On the other hand, the MERGED-BOXES-
Protocol merges all ground-truth boxes into a smallest enclosing box as the
ultimate ground-truth box. However, this protocol ignores the atomicity of in-
dividual boxes, and is not well-suited for instance-level prediction evaluation.
Tobetterevaluaterecallformultipleground-truths,weproposeanewproto-
col termed AS-MANY-Protocol. This protocol selects the top-k predicted boxes
(where k is the number of ground-truth boxes) and measures recall over all
ground-truth boxes. For example, if there are 3 out of 5 ground-truth boxes hit
bythetop-5predictedboxes,therecallis60%.Besides,wefollowcommonprac-
tice in detection [29] to calculate average recall over 10 IoU thresholds (ranging
from 0.5 to 0.95) as the primary metric on LVIS-Ground.
C More Implementation Details
Table 10 lists the detailed hyper-parameter configuration used for Groma train-
ing. It takes roughly 5/2.5/0.5 days to finish stage 1/2/3 training on 8 A100
GPUs. For some large-scale datasets, we merely sample a subset from them
during training. The total number of training samples in one epoch is given
in Tab. 10.
Table 10: Training details. RP, RE, and VLP stand for region proposer, region
encoder, and vision-language projector (an MLP), respectively.
Configuration Detectionpretrain Alignmentpretrain Instructionfinetune
optimizer AdamW AdamW AdamW
epochs 12 2 1
batchsize 64 128 128
learningrate 2e-4 1e-4 1e-5
weightdecay 1e-4 0 0
resolution 448p 448p 448p
trainingsamples 5.7m 3.2m 857k
trainableparam. RP RE,VLP RE,VLP,LLM