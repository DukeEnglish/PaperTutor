Probabilistic-Numeric SMC Sampling for Bayesian Nonlinear System
Identification in Continuous Time
Joe D. Longbottoma, Max D. Champneysa, Timothy J. Rogersa
aDynamics Research Group, University of Sheffield, Western Bank, Sheffield, S10 2TN, United Kingdom
Abstract
In engineering, accurately modeling nonlinear dynamic systems from data contaminated
by noise is both essential and complex. Established Sequential Monte Carlo (SMC) methods,
usedfortheBayesianidentificationofthesesystems, facilitatethequantificationofuncertainty
in the parameter identification process. A significant challenge in this context is the numerical
integration of continuous-time ordinary differential equations (ODEs), crucial for aligning
theoretical models with discretely sampled data. This integration introduces additional
numerical uncertainty, a factor that is often over looked. To address this issue, the field
of probabilistic numerics combines numerical methods, such as numerical integration, with
probabilistic modeling to offer a more comprehensive analysis of total uncertainty. By
retaining the accuracy of classical deterministic methods, these probabilistic approaches
offer a deeper understanding of the uncertainty inherent in the inference process. This
paper demonstrates the application of a probabilistic numerical method for solving ODEs
in the joint parameter-state identification of nonlinear dynamic systems. The presented
approach efficiently identifies latent states and system parameters from noisy measurements.
SimultaneouslyincorporatingprobabilisticsolutionstotheODEintheidentificationchallenge.
The methodology’s primary advantage lies in its capability to produce posterior distributions
over system parameters, thereby representing the inherent uncertainties in both the data and
the identification process.
1. Introduction
Modeling in engineering serves as a crucial bridge between theoretical concepts and
practical applications. It allows engineers to simulate complex systems, predict outcomes,
and optimise designs, significantly reducing the need for costly and time-consuming physical
prototypes. The development of these models remains a primary task, a challenge the field of
system identification (SI) seeks to address [1].
One of the foremost challenges in SI involves fusion of theoretical knowledge with measured
data to create models that not only embody theoretical understanding but also correspond
with the subtleties of real-world data. When the mathematical form of a system is known and
experimental data is available, a practitioner can use parametric identification as a means to
merge these insights into an optimal model.
Preprint submitted to Mechanical Systems and Signal Processing April 22, 2024
4202
rpA
91
]LM.tats[
1v32921.4042:viXraIn mechanical engineering, structural dynamic systems are often represented mathemati-
cally by second-order differential equations, based on Newton’s second law of motion. These
can be trivially projected into a set of first-order ordinary differential equations (ODEs) by
considering the evolution of the dynamics of the state-space vector. The equations of motion
for a general parameter dependent state-space model (SSM) in continuous time are familiar
as,
x˙(t) = f(x(t),u(t),θ ) (1a)
f
y(t) = g(x(t),u(t),θ ) (1b)
g
where, f and g govern the transitions of the state (x) and observation y respectively,
parameterised by θ and θ . In the parameter identification setting, it is assumed that the
f g
functional form of f and g are known.
In parametric identification, the parameters θ and θ are typically determined through
f g
an optimisation procedure aimed at minimising the discrepancy between the predicted and
observed system states. For linear, time-invariant SSMs, well-established techniques exist for
determining unknown parameters from measured data, as detailed in [1, 2]. Due to the loss of
superposition and challenges in analytical integration, the difficulty of parameter estimation
is significantly increased for nonlinear systems [3–5].
Whether the system is linear or nonlinear, a critical challenge in parameter estimation
arises from noise in experimental data. Such noise introduces uncertainties that complicate
the optimisation process, potentially leading to biased, overconfident, or mis-specified models.
If these uncertainties are not adequately addressed, they can degrade the model’s precision
and predictive accuracy. Bayesian parameter estimation offers a rigorous framework for prob-
abilistic modelling, incorporating uncertainty into the estimation process, thereby enhancing
model reliability in noisy environments.
Bayesian parameter estimation is well-established in structural dynamics, where it is used
to quantify uncertainty by identifying probability distributions over parameters, given the
noise in measurements. In a Bayesian setting, the SSM becomes probabilistic and can be
written in the following Hidden Markov model:
x ∼ p(x ), (2a)
0 1
x ∼ p(x |x ), (2b)
t t t−1
y ∼ p(y |x ). (2c)
t t t
where x ≜ x(t) and p(x ) denotes the prior probability distribution of the initial state,
t 0
encapsulating a practitioners knowledge before any measurements are taken. The transition
distribution p(x | x ) characterises the system dynamics and inherent uncertainties,
t t−1
describing the evolution of the state from x to x . Lastly, the measurement model p(y | x )
t−1 t t t
represents the relationship between the current state x and the measurement y , reflecting
t t
how the observed data arises from the underlying state of the system.
2In the presence of uncertainty in both the system model and observations, accurate state
estimation becomes a pivotal challenge to parameter estimation. Bayesian filtering and
smoothing provides an optimal solution to a linear probabilistic SSM. Filtering involves the
sequential update of the posterior distribution p(x |y ) using incoming observations, while
t 1:t
smoothing refers to the retrospective refinement of the state estimates using all available
observations p(x |y ). For an overview of such techniques see [6] or [7]. For clarity the
t 1:T
notation p(x |y ) represents the posterior distribution of the state x at time t, given all the
t 1:t t
observations y from time 1 to time t and p(x |y ) represents the posterior distribution of
1:t t 1:T
the state x given all observations y from time t to t .
t 1:T 1 T
Bayesian parameter estimation regards the model parameters as random variables. By
assigning a prior distribution p(θ), the model incorporates these parameters as conditional
elements. This approach updates the probabilistic SSM to the following form:
θ ∼ p(θ), (3a)
x ∼ p(x |θ), (3b)
1 1
x ∼ p(x |x ,θ), (3c)
t t t−1
y ∼ p(y |x ,θ). (3d)
t t t
The ‘full’ Bayesian solution requires computing the joint posterior p(x ,θ | y ), a task
1:T 1:T
which is almost always intractable. Recursive algorithms offer a viable alternative, providing
a means to approximate the marginal posterior p(θ | y ). When parameters θ are held
1:T
constant, Bayesian recursive filtering yields the distribution:
T
(cid:89)
p(θ | y ) ∝ p(θ) p(y | y ,θ), (4)
1:T t 1:t−1
t=1
Under this framework, a parameter estimation method was proposed by Chopin in the form
of Iterated Batch Importance Sampling (IBIS) [8]. This approach involves the sequential
sampling and reweighting of θ values, guided by the likelihood increments p(y |y ,θ) as
t 1:t−1
defined in Eq.(4), with the θ-particles being updated via resampling and Markov chain
Monte Carlo (MCMC) techniques [9]. Chopin extends this methodology to the sequential
Monte Carlo squared (SMC2) algorithm, catering to scenarios where likelihood increments
are intractable in closed form [10]. This method propagates multiple particle filters within
the x-space, in tandem with SMC using MCMC moves in the θ-space. These methods lay
the foundation for the parameter estimation techniques employed in this research.
A critical step in all these methodologies is the evaluation of trial parameters through the
unnormalised posterior likelihood, which involves comparing predicted states, contingent on
the parameters, against observed data. For most nonlinear systems, the absence of a closed-
form solution necessitates numerical approximation for for this step. Since the numerical
solution to the system state prediction must be approximate, so must be the evaluation of
the quality of the parameters made from the prediction. This means there can be no unique
solution to the parameter estimation problem when numerical integration is required. Rather,
3‘optimal’ parameters identified will be dependent on the numerical error in the prediction
step. Hence, the identification of parameters for nonlinear systems is intrinsically uncertain,
necessitating a probabilistic approach to capture this uncertainty. This is the foundation of
the probabilistic view of numerical methods, known as Probabilistic Numerics (PN) [11].
PN explores the intersection of numerical analysis and probability theory, advocating for a
statisticalapproachtonumericalproblemsincomputation. PNarguesthatcomputation, often
involves solving numerical problems like linear algebra [12], quadrature [13], optimisation [14]
, ODEs [15] and PDEs [16] which do not have exact solutions and thus introduce uncertainty.
PN addresses this by treating numerical solvers as agents that can quantify uncertainty
with probability measures, allowing for richer outputs than traditional point estimates. This
probabilistic approach enables smarter, uncertainty-aware decisions within algorithms and
allows for the encoding of less-than-certain expectations into solvers.
The focus of this paper is directed towards PN solutions for ODEs, which has been
summarized by Tronarp et al. [17]. In their work, the integration of an ODE is reformulated
as a Bayesian filtering and smoothing problem. Here, the proposal p(x |x ) is modeled
t+1 t
as an integrated Wiener process, and the likelihood is defined by conditioning on a pseudo-
△
measurement Z = z(t) = 0, ∀t that captures the derivative relationship of the ODE.
t
Consequently, the posterior p(x |y ,Z ) represents the probability of the state given the noise
t t t
in the measurements and the uncertainty in the numerical integration, thus incorporating
the integration uncertainty into the state estimation process.
Despite much work by many authors across the fields of uncertainty quantification in
parameter estimation and numerical methods, little work has been done to combine these two
areas. Schmidt et al [18] developed a probabilistic SSM for joint inference from ODES and
data able to infer the system state and latent function as a temporal Gaussian Process (GP)
given the uncertainty in the data and numerical methods in a single filer update. Tronarp et
al [19] developed the Fenrir algorithm for a reframing of the state and parameter estimation
into a Gauss–Markov process. This methodology initiates by refining a standard Gauss-
Markov prior into a physics-informed prior via PN. Subsequently, a posterior distribution is
calculated using Gauss-Markov regression. Parameter estimates for the dynamical system
are then extracted by maximising the marginal likelihood, resulting in a calibrated posterior
distribution.
The aim of this work is to unify Bayesian parameter estimation and PN to establish a
comprehensive probabilistic framework for parameter estimation in nonlinear systems. By
explicitly incorporating the uncertainty in both measurement data and numerical computa-
tions, this unified approach will identify the posterior over the states given the uncertainty
in the measurement and numerical integration as p(θ|y ,Z ). The framework is designed to
t t
facilitate informed decision-making by providing a more complete assessment of uncertainty,
thereby enabling engineers to consider the probabilistic nature of their models and to make
risk-aware choices in complex engineering applications.
These aims will be achieved via the contribution of a new methodology to approximate
the marginal posterior p(θ|y ,Z ). The proposed methodology will be benchmarked against a
t t
set of established nonlinear dynamic datasets.
41.1. Contribution
The contribution of this work is to a nonlinear system identification method which
combines the ideas of PN for solving ODEs — as (nonlinear) filtering problems where the
uncertainty in the numerical integration isquantified— with an SMC scheme which iteratively
approaches the posterior over the model parameters. The strength of this approach is that
uncertainty from both measurement noise and the numerical methods employed is respected in
the inference over the model paramters. Additionally, since SMC is a recursive algorithm, each
intermediate posterior is a valid posterior over the parameters conditioned on an increasing
subset of the time-series data; consequently, this approach provides a route to online Bayesian
estimation of nonlinear system parameters (subject to the computational requirements at
each step being achieveable within one sampling period). Finally, by explicitly including
the numerical integration of the ODEs within the parameter identification procedure the
methodolgy presented seeks to directly identify the continuous time ODE(s) which govern
the nonlinear system.
2. Probabilistic Solutions to Ordinary Differential Equations
2.1. Background
The simulation and analysis of dynamic systems are often rooted in the concept of Initial
Value Problems (IVPs), typically presented as solving Ordinary Differential Equations (ODEs)
with known initial conditions:
v˙ = g(t,v(t)), v(t ) = v , (5)
0 0
where v(t) ∈ Rd is the solution, g(·) is a function that maps the current states to their
respective derivatives, and v denotes the initial states at time t . In a broader context,
0 0
engineers are often interested in dynamic systems under forced excitation. The SSM transition
function f(·) can be used to extend the IVP framework to accommodate forced excitation:
x˙(t) = f(x(t),u(t),θ), x(t ) = x (6)
1 1
where x and u(t ) are known. This continuous time equation allows for the exact evaluation
1 1:T
of the derivatives of the state vector at any instance in time. Therefore, the solution to the
states at any time t is the integral of f(x(t),u(t),θ ) between x(t ) and x(t),
f 1
(cid:90) t
x(t) = f(x(t),u(t),θ)dt, (7)
t0
When the underlying dynamic system is linear and time invariant the solution to this integral
is generally available in closed form. For almost all nonlinear systems this is not the case,
necessitating numerical methods.
One such method is Euler’s method, which discretises time into small increments h for
state progression:
5x(t+h) ≈ x(t)+h·f(x(t),u(t),θ). (8)
Euler’s method is a first order method with local error proportional to the square of h and
global error linearly proportional to h such that as h approaches zero the solution to Euler’s
method approaches the true solution to the integral [20]. However, due to computational
limitations or constraints imposed by the sampled frequency of u(t) it is not always possible to
reduce step size to reduce error to an negligible levels. Whilst, this can in part be combated by
the use of higher order methods it can sometimes not be enough. Under these circumstances,
the resulting drift in state predictions can become significant, leading to uncertainty about
the true solution of the integral Eq.(8).
2.2. From Ordinary to Stochastic Differential Equations
It maybe natural therefore, to consider numerical integration as a linear SDE composed of
a linear ODE to represent local linearisation, augmented with a random variable accounting
for the unknown error in the integration process. The continuous-time SDE equation is
expressed as:
X(1) ∼ N(µ(1),Σ(1)), (9a)
dX(t) = [FX(t)+u(t)]dt+Ldβ(t), (9b)
where µ(1) and Σ(1) are the mean and the covariance that describe the Gaussian distribution
over the initial conditions X(1). F is the state transition matrix, u(t) the force, L is the
diffusion matrix and β(t) is defined as a vector of the standard Wiener processes. X(t) is a
vector of X(1)(t) and q +1 derivatives such that Xq+1(t) is the derivative of Xq(t),
 
X(1)(t)
 X(2)(t) 
X(t) =  .  (10)
 . 
.
 
X(q+1)(t)
It has been established that filtering and smoothing on SDEs of this nature are equivalent
to GP regression, where the SDE effectively forms the GP prior [21]. In this context, any
given realisation of the SDE corresponds to a sample from the GP prior. For foundational
insights into GPs, [22] provides an introductory perspective. The representation of a GP
as an SDE offers a notable advantage by constraining the prior to a Markov process [23],
which significantly reduces the computational complexity. Unlike general GPs that exhibit a
computational complexity of O(t3), this approach reduces it to O(t) [24].
To fit the SDE in a Bayesian filtering framework measurements are required. For solving
IVPs, Tronarp [17] proposes that a measurement can be defined upon the known derivative
relationship between the states i.e. F, u and L must be set such that Xq+1(t) is always be
the derivative of Xq(t). This provides the following pseudo-measurement
6   
X(2) X(1)
 X(3)  X(2)
Z(t) =  . −f  .  = 0 (11)
 .   . 
. .
   
X(q+1) X(q)
In this framework, the SDE is conditioned on the residual relationship, as expressed
in Eq.(11), equating to zero. For example, at any moment, the velocity must equal the
derivative (as calculated from the continuous SSM Eq.(6)) of displacement. Any deviation
from this relationship indicates an integration error. However, conditioning the process X(t)
on z(t) = 0 for t ∈ [1,T] is intractable in continuous time, necessitating the adoption of a
discrete time approach.
2.3. The Discrete Time Solution
△
The discrete formulation only attempts to condition the process X(t) on Z(t) = z(t) = 0
at a set of discrete time-points, {t }. Under this Bayesian filtering framework the inference
1:T
problem becomes
X ∼ N(µ ,Σ ), (12a)
1 1 1
X |X ∼ N(A(h)X +ξ(h),Q(h)), (12b)
t+1 t t
˙
Z |X ∼ N(CX −f(CX ,t ),R), (12c)
t t t t t
△
z = 0, t = 1,...,T, (12d)
t
where h is the step size and R represents the measurement variance. In this work and in
general when solving Eq.(12) with Gaussian filtering R = 0. C is the observation matrix
and C˙ is it’s derivative such that C = (cid:2) I 0 ··· 0(cid:3) and C˙ = (cid:2) 0 I ··· 0(cid:3) . That is,
CX = X(1) and C˙ X = X(2). It is important to observe that the likelihood model p(Z | X )
t t t t t t
exhibits nonlinearity, which adds complexity to the filtering solution. Additionally, z denotes
t
the realization of Z , and the state transition A(h), process noise ξ(h), and process noise
t
covariance Q(h) are defined as follows:
A(h) = exp(Fh), (13a)
(cid:90) h
ξ(h) = exp(F(h−τ))udτ, (13b)
1
(cid:90) h
Q(h) = exp(F(h−τ))LLT exp(FT(h−τ))dτ. (13c)
1
72.4. Building the Model
A commonly adopted prior for ODE solvers is the Integrated Wiener Process (IWP).
Specifically, the q +1 times IWP, denoted as IWP(q +1), is utilized due to its capability to
extrapolate using polynomial splines of degree q +1 [11]. This particular choice, IWP(q +1),
facilitates the computation of integrals in Eq.(13) in closed form, as follows:
A(h) = A(1)(h)⊗I, (14a)
ξ(h) = 0, (14b)
Q(h) = Q(1)(h)⊗Γ, (14c)
where ⊗ is the Kronecker product, I ∈ Rd×d is the identity matrix and Γ ∈ Rd×d is a
hyperparameter that calibrates the covariance in Q(h). A(1)(h) and Q(1)(h) are given by
hj−i
A(h)(1) = I (15a)
ij i≤j (j −i)!
h2q+3−i−j
Q(h)(1) = (15b)
ij (2q +3−i−j)(q +1−i)!(q +1−j)!
where I is the indicator function.
2.5. Filtering Solution
Eq.(12) provides a Bayesian filtering problem that is linear in the prediction and nonlinear
in the observation. The linear prediction step is given as,
µP = A(h)µF +ξ(h), (16a)
t+1 t
ΣP = A(h)ΣFAT(h)+Q(h), (16b)
t+1 t
where µF and ΣF are the first and second filtering moments and µP and ΣP are the first and
t t t t
second predictive moments.
The nonlinear observation model can be approximated using Taylor series methods. The
zeroth order extended Kalman filter (EKF0) used in this work because global uncertainty
quantification can be very changing for higher order methods when the ODE is multidi-
mensional such that d>1 (see 4.1 for an example). The filter update for the EKF0 is given
as;
S ≈
C˙ ΣPC˙T
+R, (17a)
t t t
K ≈
ΣPC˙TS−1,
(17b)
t t t
zˆ ≈ C˙ µP −f(CµP,t ), (17c)
t t t t
µF ≈ µP +K (z −zˆ), (17d)
t t t t t
ΣF ≈ ΣP −K S KT. (17e)
t t t t t
8It is important to note that for enhanced numerical stability, the implementation of these
filters using the square root formulation is recommended. A derivation of the square root
Kalman filter is available in [25].
2.6. Calibration
The validity of the posterior not only depends upon the mean but also the variance σ2.
This posterior variance is calibrated by assuring that the IWP of the GP is calibrated through
the hyperparameter Γ. Since Γ in the EKF0 does not depend on the vector field It is proposed
by Bosch et al. [26] that Γ should be a diagonal matrix Γ = diag(σ2,...,σ2) so that the
1 d
variance can be individually calibrated for each dimension d.
Γ can be optimised by maximising the marginal likelihood, which involves selecting σ2
that maximise the evidence for the observed data. The marginal likelihood is given by:
Y
(cid:89)
p(z |σ2) = p(z |σ2) p(z |z ,σ2). (18)
1:T 1 t 1:t−1
t=2
However, computing this is as costly as solving the ODE, thus necessitating an approximation.
Bosch et al. [26] proposed a quasi-ML estimator for multidimensional ODEs in tha case of
the EKF0, expressed as:
1 (cid:88)T (zˆ) 2
Γˆ = t i , i ∈ {1,...,d}. (19)
ii
T s˘
t
t=1
where S = Γs˘ and S is the innovation covariance matrix from the EKF0 equations 17. This
t t t
estimator, while an approximation, offers efficient calibration for the variance of the posterior
and can be easily Incorporated into the EKF0 for almost no additional cost.
3. Bayesian parameter estimation
A fundamental aspect of Bayesian parameter identification is the ability to incorporate
prior knowledge to constrain the search space of potential parameters. Using a prior enables
practitioners to incorporate engineering insights into the identification process. For instance,
the mass of a bridge might be approximated from its design specifications, with uncertainty
accounted for by considering material property and dimensional tolerances. Even in scenarios
where detailed system specifications are unknown, identification can still be bounded by
known physical constraints, such as the requirement for mass, stiffness, and damping ratios
to be positive. In both situations, Bayesian learning and the application of a prior allow
for the incorporation of engineering knowledge into the identification process, rather than
disregarding this valuable information. Consequently, Bayesian learning enhances efficiency
and reduces uncertainty, while also ensuring that the identification remains confined to
physically meaningful regions of the parameter space.
93.1. Sequential Monte Carlo for parameter estimation
Monte Carlo-based methods are numerical techniques that leverage repeated random
sampling to approximate probability distributions [9]. Among these, the Iterated Batch
Importance Sampling (IBIS) method [8] is designed to estimate posterior distributions of
the form p(θ|y ). The IBIS algorithm comprises two principal stages: firstly, evaluating the
1:T
efficacy of a set of trial parameters θ , and secondly, proposing new parameters based on
1:n
the assessed quality of the previous set. The innovation in IBIS is to use importance sampling
and particle rejuvenation to explore partial distributions p(θ|y ) (where t < T) to efficiently
1:t
explore the parameter space and evaluate the posterior p(θ|y ). This section will outline
1:T
the IBIS methodology and show how it can be adapted to evaluate posteriors of the form
p(θ|y,Z). The algorithm implemented in this work is shown in Algorithm 1.
Algorithm 1 Particle System Algorithm
1: Generate a particle system {θ ,w } that targets the prior distribution p(θ )
n n n
2: for t = 1 to T do
3: for n = 1 to N do
4: Evaluate p(X | Z ,θ ) as for Eq.(12)
t+1,n t n
5: Compute ϕ (θ ) according to the energy function Eq.(20)
n n
6: Update the particle system weights in {θ ,w } as in Eq.(21)
n n
7: end for
8: if ESS Eq.(22) < ϵ then
˜
9: Resample a new particle system {θ ,w˜ } using an Independent Metropolis
m m m=1:N
Hastings proposal, Eq.(23)
10: for s = 1 to t do
11: for m = 1 to N do
12: Evaluate p(X | Z ,θ ) as for Eq.(12)
s+1,m s m
˜ ˜
13: Compute ϕ (θ ) according to the energy function Eq.(20)
m m
˜
14: Update the particle system weights in {θ ,w˜ } as in Eq.(21)
m m
15: end for
˜
16: Accept/reject {θ } with probability given in Eq.(25)
m
17: end for
18: end if
19: end for
The algorithm initiates at t , where the parameter distribution is represented by the
1
prior p(θ), as informed by the practitioner’s engineering knowledge. Direct conditioning
of p(θ) on y is computationally intractable. Therefore, p(θ|y ) will be approximated
1:T 1:T
by conditioning on a discrete set of random samples θ ∼ p(θ) (Algorithm 1.1). Using
1:N
these sampled particles, initial conditions X , and the probabilistic numerical integrator
t1,1:N
(referenced in Section 2), the distribution over the system states at subsequent time steps
(X | Z ,θ ) can be determined (Algorithm 1.3).
t+1,n t n n=1:N
The quality of the trial parameters can be assessed by sequentially comparing states at
10X predicted by f(x(t),u(t),θ ) to the noisy measurements y through the state space
t+1 n t+1
measurement model Eq.(1b). The comparison is made by computing the approximate energy
function [7] (Algorithm 1.4),
1 1
ϕ (θ) = log|2πS (θ)|+ vT (θ)S−1(θ)v (θ) (20)
t+1 2 t+1 2 t+1 t+1 t+1
where ϕ denotes the negative log incremental likelihood or negative log incremental weight
p(y |y ,θ ,Z). The terms S (θ) and v (θ) can be calculated using the intermediate
t+1 t n t+1 t+1
steps of the Kalman filter [7]. The energy function encapsulates the quality of the proposed
parameters, considering the integration uncertainties and the influence of noisy observations.
The unnormalised marginal likelihood p(y |θ ,Z) can be approximated from
t+1 n
logw (θ ) = logw (θ )−ϕ (θ ) (21)
t+1 n t n t+1 n
where w is the approximate the unnormalised weight and w = 1/N such that the update
t=1
transitions the unnormalised weight from p(y |θ ,Z ) to p(y |θ ,Z ) (Algorithm 1.5) and
t n t t+1 n t+1
w (θ ) ∝ p(y |θ ,Z ).
T n 1:T n 1:T
Theparticlesystemrepresentsp(θ|y ,Z )throughaweightedsetofparticles, ratherthan
1:t 1:t
directly yielding a set of particles whose distribution intrinsically approximates p(θ|y ,Z )
1:t 1:t
itself. As the algorithm progresses through its prediction and reweighting steps (as outlined
in Algorithms 1.3 to 1.5), it accumulates more information about the quality of the proposed
particles θ . This process causes the posterior distribution p(θ|y ,Z ) to diverge from the
1:N 1:t 1:t
initial distribution p(θ|y ). A common side effect of this process is that the particles become
t=0
degenerate. This degeneracy primarily arises because, with each sequential observation, the
likelihood for most particles diminishes significantly compared to a few that align closely
with the observed data, resulting in an imbalance where a few particles end up with the
majority of the weight. To counteract this issue, resampling and rejuvenation steps can be
implemented. Resampling effectively duplicates particles with higher weights and eliminating
those with lower weights so that the set of particles approximate the distribution p(θ|y ,Z )
1:t 1:t
its self. Therefore after resampling each particle weight w is set to 1/N.
1:N
Rejuvenation, in contrast, involves the introduction of a move step to the particles post-
resampling to avert the loss of diversity and facilitate the exploration of the parameter space.
This process entails slightly altering each particle based on a move kernel. This move is then
accepted or rejected according to a criterion that ensures the overall particle set continues to
approximate p(θ|y ,Z ) accurately.
1:t 1:t
Efficiency in rejuvenation is critical as it is often computationally demanding, requiring a
complete browsing of all past observations (Algorithm 1.9 to 1.11). Efficiency here comes
from two parts. First, only rejuvenating the particles θ when p(θ|y ,Z ) is not well
1:N 1:t 1:t
represented by the particles. To ensure this a standard degeneracy criterion in used,
((cid:80)N w )2
j=1 j
ESS = (22)
(cid:80)N
w2
j=1 j
11where ESS is the effective sample size. The ESS can be compared to a user determined
threshold ϵ (Algorithm 1.6). If the ESS drops below the threshold the particles are determined
tobefunctionallydegenerateandwillberesampledandrejuvenated. Therefore, thethreshold
needs to be set at some compromise between propagating particles that can continue to infer
useful information about the proposed parameters and not resampling too often and incurring
unnecessary computational costs.
The second factor to consider when managing the computational cost of the algorithm is
the efficiency of the move kernel itself. Since a theoretical guarantee of convergence does not
guarantee that particles will explore the search space efficiently it is key to select a move
kernel with this in mind.
For the move kernel to have a good efficiency it is important that the acceptance rate of
new parameters it high. This could be easily achieved artificially via a random walk where a
small perturbation to current particles is applied. However, this fails to actually rejuvenate
the particles as although particles are distinct they are also very similar and therefore very
highly correlated. This approach leaves degeneracy high but less detectable.
Therefore, a move kernel should be chosen that only proposes new particles that weakly
depend on the previous values. For this independent Metropolis Hasting (IMH) kernel is
chosen [10]. This ensures that the acceptance rate becomes better indicator of rejuvenation.
Since the p(θ|y ,Z ) is approximated by the weighted particles, resampling and rejuve-
1:t 1:t
nation can be performed by sampling from that distribution. A rough approximation of the
ˆ ˆ
location of the mass of p(θ | y ,Z )) is given by the expectation (E) and variance (V) of
1:t 1:t
the particle system,
(cid:80)N w θ (cid:80)N w {θ −Eˆ }{θ −Eˆ }′
ˆ j=1 j j ˆ j=1 j j j
E = , V = , (23)
(cid:80)N
θ w
(cid:80)N
θ w
j=1 j j=1 j
such that samples can be drawn (Algorithm 1.7),
˜ ˆ ˆ
θ ∼ N(E,V) (24)
After new parameters are proposed a complete browsing of observations from the initial time
step up to the current must be performed so that the performance of the newly proposed
parameters can be compared to that of the old parameters (Algorithm 1.9 to 1.11). This
involves calculating the distribution over the parameters of the system given the uncertainty
in the measured data and the numerical integration iteratively from time t to t .
1 t+1
With the weights of the proposed particles calculated the move is accepted with probability
(Algorithm 1.12),
(cid:32) (cid:33)
˜ ˜
p(θ)p(y | θ,Z )
t t
α = min 1, (25)
p(θ)p(y | θ,Z )
t t
The employment of the IMH kernel within SMC methods serves a dual purpose: resam-
pling and rejuvenation of the particle set. This dual functionality stems from the kernel’s
12capability to sample from the posterior distribution p(θ|y Z ) as approximated by the
1:t 1:t
weighted particles. Such sampling not only ensures that the distribution of particles closely
approximates the posterior distribution p(θ|y Z )) itself but also facilitates an effective
1:t 1:t
exploration of the parameter space by the particles. Finally, the newly resampled and
rejuvenated particle system is input back into the outer loop and the particle weights are
reset to 1/N so that the process of evaluation given new observations can continue until
rejuvenation is once again required. This process is repeated for each of the the discrete time
steps in the data set (Algorithm 1.12).
4. Case studies
Inthissection, threecasestudiesarepresentedtoevaluatetheeffectivenessoftheproposed
methodology. The first case study involves a simulation of the Bouc-Wen model of hysteresis.
Here, the parameter estimation by the proposed method is compared against a known ground
truth, facilitating a clear assessment of its accuracy.
The latter two case studies are based on experimental datasets: the Silver Box and
an Electromechanical System (EMS). These present a greater challenge in quantifying
identification accuracy, due to the absence of a predetermined ground truth. However, these
cases can be considered more representative of real-world system identification challenges.
Their inclusion in this study is intended to demonstrate the practical applicability and
adaptability of the proposed methodology in a experimental settings.
4.1. Bouc Wen
Hysteresis can be considered as a nonlinear memory effect and can be used to model
internal friction, deformation of rubber and shape memory alloys to name a few examples.
The equation of motion for the Bouc Wen model of hysteresis [27] is given as,
mx¨+cx˙ +ky +z(x,x˙) = u(t) (26a)
(cid:0) (cid:1)
z˙(x,x˙) = αx˙ −β γ|x˙||z|ν−1z +δx˙|z|ν (26b)
where, x is displacement, x˙ is velocity, m is mass, c is viscous damping, k is linear stiffness
and z encodes the nonlinear hysteresis memory effect. The rate of change of z, z˙ is defined
by α, β, γ, δ and ν which are used to tune the shape and smoothness of the hysteresis loop.
For training data generation, a system based on Eq.(26) is simulated using a fourth-order
Runge-Kutta scheme. The parameters for this simulation are detailed in Table 4.1. A random
phase multisine load, with frequencies ranging from 0.5 Hz to 100 Hz across 2000 uniformly
spaced steps, is applied to the system. The load’s amplitude is initially increased linearly
over the first 10% of the simulation time, and subsequently has maximum amplitude of 208N.
The simulation is conducted for 3 seconds at a sampling frequency of 131072 Hz ensuring
minimal integration error. However, the only data that will be used for the identification will
be acceleration measurements down sampled to 4096 Hz to provide 12288 data points. The
data is then corrupted with measurement noise through the addition of i.i.d. samples from
13Table 1: Prior parameter distributions for the Bouc Wen system.
Prior Distribution
p(m) N(2.1,0.011)
p(c) N(8.8,6.97)
p(k) N(5.9×104,2.18×108)
(α) N(4.4×104,1.74×108)
p(β) N(8.6×102,6.66×104)
p(γ) N(0.93,0.0541)
p(δ) N(1.3,0.1056)
a Gaussian distribution with zero mean and a standard deviation of 5% of the root mean
square of the acceleration.
Using the framework for probabilistic parameter identification outlined in this work the
posterior parameter distribution is identified from a prior distribution over the parameters,
the nonlinear SSM, a known forcing and a noisy measurement of the acceleration state. Figure
1 shows the prior and posterior distributions over the parameters before and after training.
It should be noted that Figure 1 shows the parameters normalised by the ground truth so
that the quality of the identification can be more easily interpreted.
The prior means and variances can be found in Table 1. The prior was defined by
perturbing the true values. Large variances are given to each of the parameters to imitated a
scenario where a practitioner has low confidence in the prior values defined.
Figure 1 illustrates a discernible variation in both the accuracy and uncertainty of the
posterior parameter estimates. Specifically, the parameters m, k, and α exhibit more precise
estimations with lower variance compared to the parameters c, β, γ, and δ. This disparity
is attributed to the differing extents to which each parameter influences the system’s state.
Consequently, it can be inferred that, under the loading conditions presented in the training
data, the parameters m, k, and α play a more substantial role in governing the system’s
dynamics than c, β, γ, and δ. A more detailed discussion on this observation is provided
later in this section.
Figure 2 serves to illustrate the phenomenon of particle degeneracy and the subsequent
rejuvenation process. In this figure, the black plots shows samples from the distribution over
the acceleration given the integration uncertainty and the parameters. The effective sample
size (ESS), depicted in blue, is crucial for monitoring particle degeneracy. A decline in ESS
below a predefined threshold indicates functional degeneracy of parameters, necessitating
their resampling.
It is important to note that the figure selectively displays only a fraction of the total
resampling instances, prioritizing readability. However, the omitted data conforms to the
same pattern as that which is shown. As the sampling instances progress, errors in the
proposed parameters become increasingly evident, leading to a reduction in ESS until the
threshold is reached. Upon resampling, the new parameter set generally exhibits closer
14Figure 1: Prior and posterior hystograms and PDFs normalised by the true parameter values for the Bouc
Wen system (denoted by (·)∗) .
15Figure 2: Acceleration samples from the filtering distribution, EES and threshold for time instances 1600 to
3000.
tracking of the system’s true states, with a reduced variance across the sample set. This
process repeats until a complete browsing of the training data has been completed and a
posterior distribution over the parameters given the entire data set is reached.
Figure 3 presents the true states of the Bouc-Wen system alongside the states generated
from the sampled parameters of the posterior distribution. In the zoomed-in sections, it is
evident that the states resulting from the posterior parameter distribution closely follow the
ground truth. Additionally, it is observed that the variance of the posterior in the acceleration
state is markedly lower than that of the noisy observations. This finding indicates that the
posterior distribution over the parameters accurately captures the dynamics of the Bouc-Wen
system under the specified training load.
Parameter estimation is achieved through the assessment of parameter quality as an
ability to predict the state of the system. Therefore, parameters can only be optimised so
long as a change in a parameter has an influence in the accuracy of the predicted state. This
is limited by the ability of the training data to encapsulate the dynamics of the modelled
system. This is limited by the level of noise in the data, any fundamental uncertainly in
the identification process such as integration uncertainty and the degree to which the data
expresses the systems nonlinearity.
To see a significant further reduction in the variance of the posterior distribution further
training data must be provided. This new training data must be such that a system simulated
with parameters sampled from the current posterior would have a signification increase in
variance of the posterior across the states. Simply put, training data must be provided that
16Figure3: SamplesfromtheposterioroverthestatesfortheBoucWensystemplottedwiththetruestates. An
enlarge view of each state is shown to the left. Observations are plotted only for acceleration, as observations
for the other states were not made available during the analysis.
17forces the states of the system outside of the current noise floor. Two different approaches
may be used to achieve this. The first it to increase the number of time steps. As the
number of time steps increases small inaccuracies in the parameters are more likely to cause
drift in the predicted states from the true states. The second is to used training data with
a greater forcing amplitude as it will make the nonlinear effect on the system dynamics
more dominant. However, both of these approaches must be balanced against increasing
integration uncertainty. For a fixed time step size increasing the number of time steps allows
for integration error to accumulate and increasing forcing magnitudes increases the gradients
of the states and makes the makes linear approximations within the integrator less valid.
Here in lies a particular strength of the proposed methodology. When the uncertainly in
the integration is accounted for in the posterior over the parameters it prevents the estimator
from becoming overly confident in parameters that are biased due to numerical integration
errors.
For additional validation of the identified parameters, the identified parameters will be
used in simulation and compared in performance to the ground truth for a benchmark testing
data set [28]. The benchmark data set contains noise free input and output measurements
for both a sine-sweep and multisine loading condition. The data is simulated at 15000 Hz
and down sampled to 750 Hz and consists of 8192 samples for both loading conditions and is
performed using a Newmark integration method [29]. The sine-sweep data starts with zero
initial conditions so is not steady state. The forcing amplitude is 40 N and the frequency
bands rangers from 20 to 50 Hz with a sweep rate of 10 Hz/min. The random phase multi
sine dataset is at steady state and the frequencies range from 5 - 150 Hz with an RMS input
value of 50 N. Note that this testing data set was at no point using in the train process.
To evaluate the performance of the parameter estimation the RMSE was calculated for
each particle for both the sine-sweep and a multisine loading conditions. For the sine-sweep
the maximum RMSE was 6.6416×10−6 the minimum was 4.6313e×10−6 and the mean RMSE
of all particles was 5.4017×10−6. For the mutisine the maximum RMSE was 6.2220×10−6
the minimum was 7.1967×10−7 and the mean RMSE of all particles was 2.4772×10−6.
4.2. Silverbox
The Silverbox [30] is an electrical circuit designed to emulate the behavior of the Duffing
oscillator, a single degree of freedom (SDOF) mass-spring-damper system characterised by a
cubic spring term. The equation of motion for the Duffing oscillator is expressed as:
mx¨+cx˙ +kx+k x3 = u(t) (27)
3
In this equation, x represents displacement, x˙ denotes velocity, m is the mass, c signifies
viscous damping, k is the linear stiffness, k the cubic stiffness, and u(t) the forcing function.
3
While the Silver Box does not perfectly replicate the theoretical Duffing oscillator, it serves
as a highly accurate approximation.
The Silver Box benchmark [31] encompasses two datasets. The dataset predominantly
analysed, and the one chosen for analysis in this study, features an input time signal that
resembles an arrow, as shown in Figure 4. In the Silver Box, both input and output are in
18Figure 4: Measured input and resonce of the silver box benchmark.
the form of voltage. The input voltage simulates the Duffing oscillator’s forcing function,
while the output voltage’s response to this input mimics the displacement response of the
Duffing oscillator to a forcing function u(t).
The input voltage shown in Figure 4 is comprised of two distinct functions. The first is
a Gaussian white noise of linearly increasing amplitude filtered by a 9th order Butterworth
filter. This forms the head of the arrow and is comprised of 40,000 samples. The remaining
samples shows 10 realisations of an odd random phase multisine samples at fixed amplitude.
All data is recorded with a sample frequency of 610.35 Hz.
The model is trained on data from the odd random phase multisine and tested on the
data form the arrow head. Specifically the model is trained on 3072 observations ranging
from data point 49,278 to 52,350. The prior and posterior distributions over the parameters
can be found in Figure 5. It can be noted that all samples of parameters are taken from a log
distribution to enforce the prior that all the parameters as defined in the Duffing equation
must be positive for the identification to be physically meaningful.
Figure 5 illustrates the evolution of parameter distributions in the Duffing equation,
transitioning from a broad prior to a more defined posterior distribution. This transition
encapsulates the full extent of uncertainty inherent in the identification process. Unlike the
analogous plot for the Bouc-Wen system, the parameters in this instance are not normalised
against a known ground truth, as no such truth exists for this experimental system. However,
an analysis of the variance ranges in the posterior distributions allows for the inference that
19Figure 5: Prior and posterior hystograms and PDFs for the silver box system.
parameters m, c, and k exert a more pronounced influence on the training data compared to
the cubic stiffness parameter k .
3
Figure6displaystheobservedoutputvoltagealongsidethedisplacementstatederivedfrom
parameters sampled from the posterior distribution. The focus is solely on the displacement
state, as no observations for other states are available for comparison. This plot reveals
that the sampled parameters generally align well with the observed displacement, with most
observed data points residing within the sampled distribution. However, a closer examination,
particularly in the zoomed-in sections, reveals instances where the observed data deviates
slightly from the sampled distribution.
This deviation can be attributed to the primary sources of uncertainty in this parameter
estimation. Notably, the signal-to-noise ratio for the Silver Box is sufficiently high, rendering
measurement noise negligible. Consequently, during identification, the measurement noise
is set to an order of magnitude around 10−6. This leaves integration uncertainty as the
dominant source of error. The observed data’s lack of smoothness, as evident in Figure 6,
indicates that integration errors are likely to contribute significantly to the uncertainty in
the states, and consequently, in the parameter quality assessment.
Now consider how the uncertainty in the integration is calibrated. The uncertainty in
the integration is calibrated using Γ a quasi-MLE that approximates the average uncertainty
from t=1 to t=T. This means at some points when the gradient of the states are small Γ
will be an over estimate and when the gradient of the states are at their largest Γ will be
an underestimate of the uncertainty in the integration. As such, it is not unexpected that
at some extreme points the observed state falls outside the sampled states. However, even
considering this it should be observed that the observed states does always fall comfortably
inside 2σ of the second moment of the posterior. The second moment of the posterior is
20Figure 6: Samples from the posterior over the voltage for the silver box plotted with the measured output
voltage. An enlarged view of the voltage is shown beneath.
shown in faded gray in Figure 6.
Since, no ground truth is available to evaluate the quality of the identified parameters an
alternative method must be used. For this the identified parameters will be used in simulation
and compared to the observations for the for the arrowhead section of the dataset. The
first 1000 time steps were excluded from this simulation to remove the transient so that the
simulation was run from time step 1000 to 40,000. For this simulation a RMSE was calculated
for each particle. The maximum RMSE was 2.9516×10−3 the minimum was 1.0567×10−3
and the mean RMSE of all particles was 1.8249×10−3.
4.3. Electro-Mechanical Positioning System
The EMPS (Figure 7) is a standard configuration of drive system for prismatic joints of
robots or machine tools. It is comprised of a DC motor equipped with an incremental encoder
and a high-precision low-friction ball screw drive positioning unit. The EMPS also features
an incremental encoder and accelerometer, but their data is excluded from the benchmark,
mirroring the common absence of such measurements in industrial robots [33]. The nonlinear
equation of motion for the EMPS is given as,
τ (t) = Mx¨(t)+F x˙(t)+F sign(x˙(t))+offset (28)
idm v c
where τ is the joint torque/force, M is the inertia of the arm, F is the viscous friction
idm v
and F is the Coulomb friction.
c
21Figure 7: Image of the EMPS [32]
Table 2: Summary of Case Study Results
Case Study: B.W. Sinesweep B.W. Multisine Silver Box EMPs
Unit: RMS (ms−2) RMS (ms−2) RMS (V) RMS (m)
Minimum Particle 4.6313×10−6 7.1967×10−7 1.0567×10−3 2.9500×10−4
Maximum Particle 6.6416×10−6 6.2220×10−6 2.9516×10−3 8.9900×10−4
Mean Particle 5.4017×10−6 2.4772×10−6 1.8249×10−3 5.2018×10−4
For full details of this benchmark see [32]. The EMPS provides two datasets. The first is
for training and the second is for testing. The training dataset consists of 25s of motor force
and motor position measurements sampled at 1000 Hz. When generating the training data
the EMPS excited with bang-bang accelerations. Training is performed based on 24576 data
points and the prior over the parameters is shown if Figure 8. To ensure that M, F and F
v c
remain positive new proposals of these parameters will be sampled in the log space.
Figure 8 shows the prior and posterior parameter distribution for the EMPS benchmark. It
can be seen that the deffuse prior converges to a posterior with a narrow variance. Excellent
convergence is seen across all four identified parameters. Figure 9 shows the observed
displacement plotted together with the displacement state generated from parameters sampled
from the posterior distribution. Only the displacement states is shown as no observations exist
for the other states. The parameters sampled from the posterior distribution correctly track
the measurements with the measured states always falling within the sampled distribution.
To evaluate the quality of the identified parameters. The identified parameters will be
used in simulation and compared to the observations for the testing dataset. The testing
dataset consists of 25s of motor force and motor position measurements sampled at 1000 Hz.
When generating the testing data the EMPS is again excited with bang-bang accelerations
however this time there is an additional pulse component to the loading.
The identified parameters are evaluated for the for the testing dataset and compared
against the measurements. For this simulation a RMSE was calculated for each particle. The
maximum RMSE was 8.9900x10−4 the minimum was 2.9500x10−4 and the mean RMSE of all
particles was 5.2018x10−4.
22Figure8: PriorandposteriorhistogramsandPDFsfortheEMPSshownseparatelyduetothelargereduction
in variance from prior to posterior.
23Figure 9: Samples from the posterior over the Displacement for the EMPS plotted with the measured output
Displacement. An enlarge view of the displacement is shown beneath.
5. Conclusions
This paper introduces a Bayesian parameter estimation method that unifies SMC and
Probabilistic Numerics to establish a comprehensive probabilistic framework for parameter
estimation in nonlinear systems.
It is the argument of the authers that: parameter estimation in nonlinear dynamic
systems can abstractly be delineated into 3 sequential stages. Step 1 is to select a set of
trial parameters. Step 2 involves using the set of trial parameters, the functional form of the
model, and initial conditions to solve the IVP. Solving the IVP gives access to the state of
the system at some future point in time. Step three is to compare the state pertaining to the
IVP to a measured state at the same instance in time. By minimising the distance between
the proposed and measured state it is possible to optimise the model parameters.
This work adopts a comprehensive Bayesian approach throughout these 3 steps. In Step
1, a prior over the parameter space is applied. This approach reduces the size of the search
space. As a result, the efficiency of the selected trial parameters is enhanced. In Step 2 a
probabilistic ODE solver explicitly incorporates the uncertainty associated with solving the
IVP for nonlinear dynamic systems into the identification process. Finally, Step 3 evaluates
the posterior over the parameters, taking into account the uncertainties in both the integration
process and the measurement.
Through 3 case studies, it was demonstrated that the proposed procedure could be realised
24effectively. Notable results were achieved across both simulated and experimental datasets
with low RMSE.
Nonetheless, further investigation is warranted. Future work should delve into the
quantification of numerical uncertainty in more complex system identification challenges, such
as an extension to multiple degree of freedom systems (MDOF) and identifying nonlinear
systems where the nonlinearity’s form, in addition to parameterisation, remains unknown.
Moreover, extending this method to scenarios with unknown system inputs presents an open
challenge deserving of further exploration.
Acknowledgements
The authors gratefully acknowledge the support of the Engineering and Physical Sciences
Research Council (EPSRC), UK through grant number EP/W002140/1 and EP/L016257/1.
The authors would also like to thank Ramboll Energy for their ongoing support. For the
purpose of open access, the author has applied a Creative Commons Attribution (CC BY)
licence to any Author Accepted Manuscript version arising.
References
[1] L. Ljung. System Identification: Theory for the User. Pearson Education, 1998.
[2] Mats Viberg. Subspace-based methods for the identification of linear time-invariant
systems. Automatica, 31(12):1835–1851, 1995.
[3] Keith Worden, Geoffrey R Tomlinson, and K Yagasaki. Nonlinearity in structural
dynamics: detection, identification and modeling. Appl. Mech. Rev., 55(2):B26–B27,
2002.
[4] Gaetan Kerschen, Keith Worden, Alexander F Vakakis, and Jean-Claude Golinval. Past,
present and future of nonlinear system identification in structural dynamics. Mechanical
systems and signal processing, 20(3):505–592, 2006.
[5] Jean-Philippe No¨el and Ga¨etan Kerschen. Nonlinear system identification in structural
dynamics: 10 more years of progress. Mechanical Systems and Signal Processing, 83:
2–35, 2017.
[6] Arnaud Doucet, Adam M Johansen, et al. A tutorial on particle filtering and smoothing:
Fifteen years later. Handbook of nonlinear filtering, 12(656-704):3, 2009.
[7] S. S¨arkk¨a. Bayesian Filtering and Smoothing. Bayesian Filtering and Smoothing.
Cambridge University Press, 2013.
[8] Nicolas Chopin. A sequential particle filter method for static models. Biometrika, 89(3):
539–552, 2002.
25[9] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine
learning, volume 4. Springer, 2006.
[10] Nicolas Chopin, P Jacob, Omiros Papaspiliopoulos, et al. Smc2: A sequential monte
carlo algorithm with particle markov chain monte carlo updates. arXiv preprint
arXiv:1101.1528, 2011.
[11] P. Hennig, M.A. Osborne, and H.P. Kersting. Probabilistic Numerics. Cambridge
University Press, 2022.
[12] P. Hennig. Probabilistic Interpretation of Linear Solvers. SIAM J on Optimization, 25,
January 2015.
[13] Fran¸cois-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A. Osborne, and Dino
Sejdinovic. Probabilistic integration: A role in statistical computation?, 2017.
[14] Philipp Hennig and Martin Kiefel. Quasi-newton methods: A new direction. The Journal
of Machine Learning Research, 14(1):843–865, 2013.
[15] Michael Schober, Simo S¨arkk¨a, and Philipp Hennig. A probabilistic model for the
numerical solution of initial value problems, 2017.
[16] Junyang Wang, Jon Cockayne, Oksana Chkrebtii, Timothy John Sullivan, and Chris J
Oates. Bayesian numerical methods for nonlinear partial differential equations. Statistics
and Computing, 31:1–20, 2021.
[17] Filip Tronarp, Hans Kersting, Simo Sa¨rkka¨, and Philipp Hennig. Probabilistic solutions
to ordinary differential equations as non-linear bayesian filtering: A new perspective.
2019.
[18] Jonathan Schmidt, Nicholas Kr¨amer, and Philipp Hennig. A probabilistic state space
model for joint inference from differential equations and data. Advances in Neural
Information Processing Systems, 34:12374–12385, 2021.
[19] FilipTronarp, NathanaelBosch, andPhilippHennig. Fenrir: Physics-enhancedregression
for initial value problems, 2023.
[20] Philip J Davis and Philip Rabinowitz. Methods of numerical integration. Courier
Corporation, 2007.
[21] Simo S¨arkk¨a and Arno Solin. Applied stochastic differential equations, volume 10.
Cambridge University Press, 2019.
[22] Christopher K Williams and Carl Edward Rasmussen. Gaussian Processes for Machine
Learning, volume 2. MIT press Cambridge, MA, 2006.
26[23] Bernt Oksendal. Stochastic Sifferential Equations: An Introduction with Applications.
Springer Science & Business Media, 2013.
[24] Jouni Hartikainen and Simo S¨arkk¨a. Kalman filtering and smoothing solutions to
temporal gaussian process regression models. In 2010 IEEE International Workshop on
Machine Learning for signal processing, pages 379–384. IEEE, 2010.
[25] Adrian Wills, Thomas B Sch¨on, Fredrik Lindsten, and Brett Ninness. Estimation of
linear systems using a gibbs sampler. IFAC Proceedings Volumes, 45(16):203–208, 2012.
[26] Nathanael Bosch, Philipp Hennig, and Filip Tronarp. Calibrated adaptive probabilistic
ode solvers. In International Conference on Artificial Intelligence and Statistics, pages
3466–3474. PMLR, 2021.
[27] Yi-Kwei Wen. Method for random vibration of hysteretic systems. Journal of the
engineering mechanics division, 102(2):249–263, 1976.
[28] Jean-Philippe Noel and Maarten Schoukens. Hysteretic benchmark with a dynamic
nonlinearity. In Workshop on nonlinear system identification benchmarks, pages 7–14,
2016.
[29] Nathan M Newmark. A method of computation for structural dynamics. Journal of the
engineering mechanics division, 85(3):67–94, 1959.
[30] Rik Pintelon and Johan Schoukens. System identification: a frequency domain approach.
John Wiley & Sons, 2012.
[31] Torbj¨orn Wigren and Johan Schoukens. Data for benchmarking in nonlinear system
identification. Technical Reports from the department of Information Technology, 6:
2013–006, 2013.
[32] Alexandre Janot, Maxime Gautier, and Mathieu Brunot. Data set and reference models
of emps. In Nonlinear System Identification Benchmarks, 2019.
[33] Wisama Khalil and Etienne Dombre. Modeling identification and control of robots. CRC
Press, 2002.
27