Preprint.
Stronger Random Baselines for In-Context Learning
GregoryYauney&DavidMimno
CornellUniversity
{gyauney@cs.,mimno@}cornell.edu
Abstract
Evaluatingthein-contextlearningclassificationperformanceoflanguage
models poses challenges due to small dataset sizes, extensive prompt-
selectionusingthevalidationset,andintentionallydifficulttasksthatlead
tonear-randomperformance.Thestandardrandombaseline—theexpected
accuracyofguessinglabelsuniformlyatrandom—isstablewhentheevalu-
ationsetisusedonlyonceorwhenthedatasetislarge. Weaccountforthe
commonpracticeofvalidationsetreuseandexistingsmalldatasetswitha
strongerrandombaseline:theexpectedmaximumaccuracyacrossmultiple
randomclassifiers. Whenchoosingthebestpromptdemonstrationsacross
sixquantizedlanguagemodelsappliedto16BIG-benchLitetasks,more
than20%ofthefew-shotresultsthatexceedthestandardbaselinedonot
exceedthisstrongerrandombaseline. Whenheld-outtestsetsareavailable,
thisstrongerbaselineisalsoabetterpredictorofheld-outperformancethan
thestandardbaseline,avoidingunnecessarytestsetevaluations. Thismax-
imumrandombaselineprovidesaneasilycalculateddrop-inreplacement
forthestandardbaseline.
1 Introduction
Oneofthemostexcitingapplicationsofcontemporarylargelanguagemodels(LMs)istheir
abilitytoperformcomplextasksgivenonlyasmallnumberofexamples(Brownetal.,2020).
In-contextlearning(ICL),alsocalledfew-shot,performancehasthereforebecomeacritical
toolinLMevaluation(Liuetal.,2023),butthenatureoffew-shottasksmakesthemhardto
contextualize. Becausetheyareintendedtoevaluatespecificabilities,datasetscanbesmall
andidiosyncratic. ICLperformanceisextremelysensitivetosmallchangesinformatting
and demonstrations (Zhao et al., 2021; Sclar et al., 2024). Finally, the fact that few-shot
datasetsareintendedtoevaluatetheouterboundsofLMperformancemeansthattheyare
intentionallydesignedtobedifficult(Suzgunetal.,2023). Westudytheimplicationsof
thesecharacteristicsandargueforusingaprobabilisticbaselinethatbetterdistinguishes
fromrandomperformanceforsmalldatasetsandaccountsforsearchesoverprompts.
Downstream users want to find and deploy the best prompt (Mizrahi et al., 2023). But
ICLperformanceofLMsvariesgreatlyacrosssemanticallyequivalentpromptfeatureslike
thechoiceofdemonstrationsandtheirorder(Zhaoetal.,2021;Luetal.,2022),instruction
phrasing(Mizrahietal.,2023),andtemplateformatting(Sclaretal.,2024;Voronovetal.,
2024). Because performance is both variable and unpredictable, standard ICL practice
involves searching over large numbers of potential prompts based on a validation set.
Researchersoftenreportmodelperformanceasthemaximumscoreonavalidationsetor
thecorrespondingperformanceofthatpromptonanadditionaltrulyheld-outtestsetto
avoidoverfittingtothevalidationset(Brownetal.,2020;Perezetal.,2021). Inthisworkwe
showthatwecanbetteridentifypromptsthatmaybeoverfittingandavoidunneccessary
evaluationsontestdata.
In searching for the best prompts, the simplest and most common comparison is to a
random baseline. The standard random baseline for classification tasks is the expected
accuracyofguessinglabelsuniformlyatrandom(Mitchell,1997,interalia).Whileuniversally
treatedasapointestimate,theaccuracyofanyspecificrandomclassifierfollowsabinomial
distribution. Forlargerdatasets, thevarianceofthisdistributionistightlyconcentrated
1
4202
rpA
91
]LC.sc[
1v02031.4042:viXraPreprint.
around the expectation, but variance can be considerably higher for the small datasets
typicallyusedintheICLsetting. Weintroduceastrongerrandombaselinethataccounts
forbothvarianceandvalidationsetreusebyaskingafairerquestion: ifwearechoosing
thebestof t differentprompts, whynotcomparethatprompt’saccuracytothebestof t
differentrandomclassifiers? Figure1showshowthesetworandombaselinescanleadto
differentconclusions.
Treating random performance as a distri-
butionhastwokeyadvantages. First, the 0.2 Accuracy from different prompts
strongerrandombaselinecanbecalculated
0.0
inclosedformastheexpectationofthemax-
imumorderstatisticofthebinomialdistri- 0.1 Random classifier
bution. When choosing the best prompt
from even as few as 10 options, this base- 0.0
lineincreasesthethresholdforbeating“ran- 0.2 Best of many random classifiers
dom”performancebymorethan7points
ofaccuracyforabinaryclassificationtask 0.0
with100examples. Second,byusingafam-
ilyofparametricdistributionstorepresent
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Expected maximum
random performance rather than a point Standard random baseline random accuracy
Best accuracy across prompts
estimate, we can also calculate a compa- Accuracy
rable performance metric across multiple
datasetsthattakesintoaccountfactorslike Figure 1: 200 different prompts for the
the number of classification options, the emoji movietaskyieldaspreadofaccuracies
numberofevaluatedprompts,andthenum- for OLMo-7B (4-shot, quantized). The best
berofevaluatedexamples. Wecanthenuse prompt has much higher accuracy than the
thetailprobabilityoftherandomdistribu- expectedperformanceofasinglerandomclas-
tionasa p-valuetoquantifywhatfraction sifier. Butitsperformanceisworsethanthe
ofrandomclassifiersoutperformaprompt. expectedmaximumaccuracyamong200dif-
This contextualization also applies in the ferentrandomclassifiers.
truefew-shotsetting,whereevaluationdata
isusedonlyonce.
Ourcontributionsarethefollowing. First,weproposeamaximumrandombaselinethat
explicitlydependsonvalidationsetsizeandthenumberofevaluatedprompts. Wegivea
simplemethodforcomputingthisvaluebytakingtheexpectationofthemaximumorder
statisticofthebinomialdistributionandshowhowthisquantityvarieswithtaskparameters
andevaluationsetup. Second,weexaminequantizedLMfew-shotperformanceacrossthe
BIG-benchLitebenchmarksuitewhenchoosingthebestpromptdemonstrations,findingin
oursettingthatmorethan20%ofresultsthatexceedthestandardbaselinedonotexceedthe
maximumrandombaseline. Third,weshowthatwhenatrulyheld-outtestsetisavailable,
comparingmaximumvalidationperformancetothemaximumrandombaseline, rather
thanthestandardrandombaseline, isbetterabletopredictwhethercorrespondingtest
performance will also be above random. This can help prevent prematurely evaluating
withaheld-outset,reducingthechanceofoverfittingtothatheld-outtestset. Thestronger
baselinecanalsoallowresearcherstousesmallervalidationsetsmoreconfidently. Finally,
themaximumrandombaselinecanbeeasilycalculatedasadrop-inreplacementforthe
standardrandombaseline.1 High-qualitydatasetsremainrareandexpensive. Theywillbe
reused,andourevaluationbaselinesshouldaccountforthis.
2 Relatedwork
Robustmodelcomparisons. Dodgeetal.(2019)introduceexpectedmaximumvalidation
accuracytocomparemodelaccuracyforagivenbudgetofhyperparameterevaluations.This
valuecanbeestimatedwithlowmean-squarederror(Dodgeetal.,2021).Ourworkcaststhe
numberofvalidationsetreusesastheresourceofinterestandasksamorebasicquestion:
isagivenmodelevenoutperformingrandomguessingforasetbudgetofvalidationset
1Codeisavailableat:https://github.com/gyauney/max-random-baseline
2
sreifissalc
fo
noitroporPPreprint.
reuses? Otherworkalsoseekstocomparedistributionsofmodelperformance(Droretal.,
2019),butitdoesnotdosoagainstarandombaseline. Cardetal.(2020)findthatmany
commonNLPevaluationdatasetsaretoosmalltoreliablycomparemodelsunlessthere
isalargeimprovementinperformance. Braggetal.(2021)advocateforlargerfew-shot
evaluationstoreduceconfidenceintervalsaroundreportedperformance. Ourworkfinds
onemorereasontopreferlargerevaluationsets.
Overfitting to the evaluation set. Data reuse has been approached from many angles.
Somerestrictaccesstothetestset,eitherthroughsubmissionsystems(Wangetal.,2019a;b;
Alex et al., 2021; Bragg et al., 2021) or differential privacy (Dwork et al., 2015). Perez
etal.(2021)advocateforatruefew-shotsetting: modelselectionusingcross-validationand
minimum description length on the training set, with only one held-out set evaluation.
Whiletheycompareselectedpromptstorandomlyselectedprompts,weinsteadcompare
toclassifiersthatguessatrandom. Someworkscreateheld-outsetsforICL(Weietal.,2022;
Norietal.,2023),thoughthisisnotalwaysstandardpractice. Ratherthanstipulatehowto
accessthevalidationset,ourapproachcontextualizesthecurrentpracticeofreusingthe
validation set. Sometimes the prompt template with the best validation performance is
chosenfortestsetevaluation(Weietal.,2022;Mizrahietal.,2023). Thestrongerrandom
baselinedoesnotapplytothetestaccuracy,butitdoesapplytothebestvalidationaccuracy.
Permutationtestsforclassification. Permutationtestshavealonghistoryforevaluating
thestrengthofassociationbetweenfeaturesandlabelsbyretrainingclassifiersondatasets
with shuffled labels (Golland et al., 2000; Ojala & Garriga, 2010), especially in low-data
medical tasks (Golland & Fischl, 2003). In contrast, our approach seeks to determine
improvement over a random classifier and—since the random performance distribution
is known—we do not need to retrain any classifiers. Hypothesis tests have also been
increasinglyusedinNLPtotestwhetheroneclassifierreliablyhasbetterperformancethan
another(Droretal.,2018;Zmigrodetal.,2022;Peyrardetal.,2021).
3 Randombaselines
Consider a dataset with n validation examples and m possible labels.2 Then p = 1/m
is the probability of guessing one example’s label correctly uniformly at random. We
primarilyconsiderhowtocontextualizeaccuracyinasettingwithjustonesetofnexamples
(generalizationisdiscussedinSection5.2). Anexperimentevalutestdifferentclassifiers(or
prompts,orhyperparametersettings)andreportsmaximumaccuracy.
Standardrandombaseline. Lethbeaclassifierthatguesseslabelsuniformlyatrandom
foreachexample. LetB(n,p)bethebinomialdistributionwithnindependenttrialsand
probability pofsuccessoneachtrial. LetXbethenumberofcorrectguessesthathmakes
whenevaluatedonallexamples. X ∼ B(n,p)modelsthenumberofcorrectguesses,and:
1
acc(h) = X
n
Theexpectedaccuracyofarandomclassifierisstraightforward:
(cid:20) (cid:21)
1 1
E[acc(h)] = E X = (np) = p (1)
h X∼B(n,p) n n
Expectedmaximumrandombaseline. Inthissetup,wewantabaselinecomparableto
theclassifierthatachievesthemaximumaccuracyonthevalidationsetoutoftdifferent
classifiers. The idea is to take the expected maximum performance among t random
classifiers. Leth ,...,h beclassifiersthatguessanswersindependentlyanduniformlyat
1 t
random,withcorrespondingindependentnumbersofcorrectguessesX ,...,X . Consider
1 t
theonewiththehighestperformance:
h =argmax{acc(h )}
max i
i∈[t]
2AppendixAextendsthebaselinetodatasetswheremvariesperexample.
3Preprint.
ThenumberofcorrectguessesX byh isthetthorderstatistic(orsamplemaximum)
max max
oftheX,denotedX :
i (t)
X =max{X} = X (2)
max i (t)
i∈[t]
Recalltheprobabilitymassfunction f(k) = P(X = k)anddistributionfunction F(k) =
i
P(X ≤ k)forallbinomialrandomvariablesX:
i i
(cid:18) (cid:19)
n
f(k) = pk(1−p)n−k F(k) = I 1−p(n−k,1+k) (3)
k
whereIistheregularizedincompletebetafunction. Followingthegeneralmethodfororder
statisticsofdiscreterandomvariables(Casella&Berger,2002),theprobabilitymassfunction
forthetthorderstatistic,i.e. thesamplemaximum,is:
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
P X = k = P X ≤ k −P X < k (4)
(t) (t) (t)
= P(X ≤ k ∧...∧ X ≤ k)−P(X < k ∧...∧ X < k) (5)
1 t 1 t
= P(X ≤ k)t−P(X < k)t (6)
1 1
=
F(k)t−(cid:0)
F(k)−
f(k)(cid:1)t
(7)
Wenowhaveaclosedformfortheexpectedmaximumaccuracyoutoftrandomclassifiers:
(cid:20) 1 (cid:21) 1 (cid:104) (cid:105) 1 ∑n (cid:16) (cid:17)
E[acc(h )] = E X = E X = kP X = k (8)
max
X1,...,Xt n
max
n X1,...,Xt
(t)
n
k=0
(t)
=
1 ∑n k(cid:32)
I
1−p(n−k,1+k)t−(cid:18)
I
1−p(n−k,1+k)−(cid:18) n(cid:19) pk(1−p)n−k(cid:19)t(cid:33)
(9)
n k
k=0
p-valuesagainstrandombaselines. Foreitherkindofrandombaseline,itcanbeusefulto
comparetothedistributionofrandomperformancesratherthanjusttheexpectedaccuracy. p-
valuesareageneraltoolforcalculatingtheprobabilitythatavaluedrawnfromadistribution
isthesameasormoreextremethanagivenvalue(Wasserman,2013).3 The p-valuesforthe
accuracyofaclassifierh againstbothbaselinesare(detailsinAppendixA):
0
p = P(acc(h) ≥acc(h )) p = P(acc(h ) ≥acc(h )) (10)
standard 0 max max 0
(cid:0) (cid:1) (cid:0) (cid:1)t
=1−F nacc(h )−1 =1−F nacc(h )−1 (11)
0 0
4 Propertiesoftheexpectedmaximumrandombaseline
Thissectionbuildsintuitionsforthevaluesofmaximumrandombaselinesandhowthey
areaffectedbyexperimentaldesignparameters. Thestandardrandombaselinedepends
onlyonthenumberofpossiblelabels. Incontrast,theexpectedmaximumrandombaseline
dependsonthenumberofvalidationexamplesn,theprobabilityofguessingalabelcorrectly
p,andthenumberofvalidationsetevaluationst. Figure2showstheexpectedmaximum
randomaccuracyonabinaryclassificationtaskasafunctionofvalidationsetevaluationst
anddatasetsizen.
Expectedmaximumrandomaccuracyishigherforsmallernandlargert. Foragiven
datasetsizen,expectedmaximumrandomaccuracyincreasesastincreases. Foragiven
numberofvalidationsetevaluationst,smallerdatasetshavegreaterexpectedmaximum
randomaccuracies. Whentherearefewerthanseveralhundredexamplesinthevalidation
set, even a few evaluations of the validation set yields a maximum random baseline at
least10%higherthanthestandardrandombaseline. Forexample, adatasetof n = 100
examples has an expected max accuracy of 0.575 after only t = 10 evaluations. But it
requiresmorethant =10,000evaluationstoreachthatexpectedmaxaccuracyforadataset
withn =1,000.
3Notethatweusethistermdivorcedfromanyhypothesistestingframework.
4Preprint.
Parameterextremescapturestandardset-
Dataset with 2 choices per example
tings. Whent = 1(thetruefew-shotset- 10,000
ting),theexpectedmaximumissimplythe
expectation of the binomial distribution.
The maximum random baseline therefore
subsumesthestandardbaselineasaspecial 1,000
case. When n is large, the expected maxi-
mumrandomaccuracyisnearlythesameas
thestandardrandombaselinebecausethe
100
binomialdistributionisveryconcentrated
around its expectation with large n. The
implicationofthisresultisthatifanexper-
iment has a large t, the standard random 10
1 10 100 1,000 10,000
baselineisonlyagoodindicatorofrandom t = number of validation set evaluations
guessingifnisalsolarge. Butwhileincreas-
ingnisalwaysagoodideatheoretically,in Figure2: Theexpectedmaximumaccuracy
practiceitmaynotbefeasibleforthekind achievedamongtrandomclassifiersonabi-
of diverse, fast-moving LM tasks that are naryclassificationdatasetdependsontand
thekeyusecaseforICL. thesizeofthedataset.
5 Experiments
Weevaluatetheextenttowhichthemaximumrandombaselinerecontextualizesin-context
learning performance. First, we study a deliberately simple and challenging setting of
prompt demonstration selection using validation data: do heavily quantized language
modelsoutperformrandombaselines? Wethenmovetoasettingwithaheld-outdataset
andfindthatcomparingmaximumvalidationaccuracytothemaximumrandombaseline
ratherthanthestandardrandombaselineismoreindicativeofwhetherheld-outaccuracy
exceeds random chance. Finally, we use the maximum random baseline to re-evaluate
publishedresultsonprompttemplateselectionandinstructionselection.4
5.1 Thestandardrandombaselineoverstatesperformance
Considerthetaskofchoosingwhichdemonstrationstoincludeinafew-shotprompt. Prior
workhasshownthatchoosingdifferentexamplescanleadtodrasticdifferencesinaccuracy
(Zhaoetal.,2021). Inthissetting,wearegoingtoreporttheaccuracyofthepromptwiththe
highestvalidationaccuracy. Ourgoalistogetthebestvalidationaccuracypossible,andwe
haveabudgetoft =200differentpromptstoevaluate. FollowingDodgeetal.(2019),we
reporttheexpectedmaximumvalidationaccuracyachievedbyanypromptasafunctionof
t,thenumberofevaluatedprompts. Wecomparethebestprompt’saccuracytoboththe
standardrandomandmaximumrandombaselines. SeeAppendixBforfulldetails.
Modelsanddatasets. WeevaluatesixLMsatthe7B-parameterscale:Llama-2-7b(Touvron
etal.,2023),OLMo-7B(Groeneveldetal.,2024),Falcon-7b(Almazroueietal.,2023),and
theirinstruction-tunedcounterparts: Alpaca-7b(Taorietal.,2023),OLMo-7B-Instruct,and
Falcon-7b-instruct. Wequantizethemodelsto4-bitforaparticularlychallengingsetting
(Dettmersetal.,2023). Weuse16BIG-benchLitemultiplechoicetaskswiththeirstandard
instructiontemplates(Srivastavaetal.,2023). Ourgoalistocomparedifferencesbetween
standardandmaximumbaselinesduetothenatureofdifferentfew-shottasks. Because
we know analytically that the size of a validation set influences random baselines, we
reducesizeasaconfoundingfactorbysubsamplingtaskstohaveamaximumsizeof200
examples.5 Allbaselinesarecalculatedwithrespecttothesizeofthesubsampleddatasets.6
4Reproductioncodeisavailableat:https://github.com/gyauney/stronger-random-baselines
5Usingsmallervalidationsetsalsoresultsinsubstantialsavingsincomputationwhenevaluating
hundredsofexamplecombinations,animportantconsiderationforpractitioners.
6Forthesevendatasetswithn<200,thereareonlynpossibledemonstrationsinthe1-shotsetting.
Inthesecaseswecomparetothepropermaximumrandombaselinewitht=ninsteadoft=200.
5
tesatad
fo
ezis
=
nPreprint.
OLMo-7B OLMo-7B OLMo-7B OLMo-7B
emoji_movie known_unknowns bbq_lite_json hindu_knowledge
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
Standard random baseline Max random baseline Best 1-shot across t prompts Best 2-shot across t prompts Best 4-shot across t prompts
Figure3: OLMo-7B1-,2-,and4-shotbeatsthestandardrandombaseline(dashedline)on
fourtasksinexpectedmaximumvalidationaccuracy.Butaccountingforvalidationsetreuse
withthemaximumrandombaseline(solidblackline),thebestaccuraciesacrossprompts
onthelefttwodatasetsareinfactnobetterthanrandom.
Llama-2-7b / Alpaca-7b OLMo-7B falcon-7b
emoji_movie emoji_movie emoji_movie
0.4 0.4 0.4
0.3 0.3 0.3 Standard random baseline
Max random baseline
0.2 0.2 0.2 Best 1-shot across t
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 prompts
1.0 1.0 1.0 B pre os mt 2 p- ts shot across t
0.5 0.5 0.5 Best 1-shot instruction-
tuned across t prompts
0.0 0 50 100 150 200 0.0 0 50 100 150 200 0.0 0 50 100 150 200 B tue ns et d2 - as ch ro ot s sin ts t pr ru oc mti pon ts- 1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
Figure4: Expectedmaximumvalidationaccuracycomparedtothestandardrandomand
maximumrandombaselineforbaseandinstruction-tunedmodelsonasingleharddataset.
Resultsforindividualdatasets. Figure3showsOLMo-7Bperformanceforselectedtasks.
Thex-axisshowsthenumberofpromptstevaluatedonthevalidationset. Astincreases,
the maximum random baseline also increases, sharply at first and then more gradually.
Foremoji movieandknown unknowns,theexpectedmaximum1-,2-,and4-shotaccuracies
acrossmultiplepromptsareabovethestandardrandombaselinebutbelowthemaximum
random baseline. To further illustrate how the baselines disagree, we can use their dis-
tributionstogenerate p-valuesfortheobservedaccuracies. Inthemiddlerowofpanels,
p-valueswithrespecttothestandardrandomdistributionarenear-zeroinalmostallcases.
Thebottomrowshows p-valueswithrespecttothemaximumbaseline: here1-, 2-, and
4-shotallhavehigh p-values. Contextualizingperformanceagainstthemaximumrandom
baselineshowsthatthesedatasetsaremorechallengingthantheyappear. Performance
ishighenoughontaskslikebbq lite jsonandhindu knowledgetobeaboverandomno
matterwhichbaselineisused. Inthesecases, p-valuesforbothbaselinesarenear-zero. The
maximumrandombaselinegivesusadditionalconfidencethatperformanceisinfactgood.
Figure4comparesbaseandinstruction-tunedmodelsontheemoji moviedataset. These
resultsshowthatthestandardrandombaselinesubstantiallyunderestimatesthedifficultyof
thisdataset. ComparinginsteadtothemaximumrandombaselinerevealsthatonlyLlama-
2-7boutperformsrandomguessing. Whiletheinstruction-tunedfalcon-7bmodelsslightly
outperformthestandardrandombaseline,thestandard p-valuesshowthatasignificant
percentageofrandomclassifiersstillhavehigheraccuracy.
Aggregateresults. Table1showsthatoutof288totalexperiments,maximumvalidation
accuracyexceededthestandardrandombaselinein255,butmaximumvalidationaccuracy
exceededthemaximumrandombaselinein199. Thebaselinesdisagreedin56experiments.
6
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(Preprint.
Basemodel Instruction-tuned
1-shot 2-shot 4-shot 1-shot 2-shot 4-shot
BIG-benchLitedataset n L O F L O F L O F A O F A O F A O F Total
novelconcepts 32 0
knownunknowns 46 9
codelinedescription 60 0
emojimovie 100 11
conceptualcombinations 103 0
strangestories 174 0
hinduknowledge 175 0
bbqlitejson 200 0
formalfallaciessyllogismsnegation 200 12
languageidentification 200 12
logicaldeduction 200 3
playdialogsameordifferent 200 1
strategyqa 200 0
symbolinterpretation 200 7
vitamincfactverification 200 1
winowhy 200 0
Baselinedisagreementspermodel 2 5 5 1 4 3 1 1 3 4 5 4 2 4 3 3 2 4
Totalbaselinedisagreements 12 8 5 13 9 9 56
Totalpercentageflipped /( + ) 26% 19% 15% 28% 20% 23% 22%
Table1: Agreementbetweenthestandardrandombaselineandmaximumrandombaseline
whenevaluatingthemaximumperformanceovert =200choicesofpromptdemonstrations
onBIG-benchLite. : bestpromptperformedworsethanbothbaselines. : bestprompt
performedbetterthanthestandardrandombaselinebutworsethanthemaximumrandom
baseline. : bestpromptperformedbetterthanbothbaselines. n: numberofexamples. L:
Llama-2-7b,O:OLMo-7B,F:Falcon-7B,A:Alpaca-7b.
This means that 22.0% of results that are above the standard baseline are not above the
maximumbaseline. Amongbasemodels, therearefewerbaselinedisagreementsasthe
numberofshotsincreases. Surprisingly,therearenotfewerdisagreementswhenmoving
frombasemodelstotheirinstruction-tunedcounterparts. Specificdatasetsareresponsible
for many disagreements, the highest number of which come from formal fallacies and
language identification. Formanydatasets,bothbaselinesagreeforallevaluations. Full
per-datasetresultsareinFigures11,12,and13inAppendixD.
5.2 Maximumrandombaselinepredictsheld-outaccuracy
Intheprevioussectionwefocusedoncontextualizingmaximumvalidationaccuracy,which
iscommonlyreportedforICLtasks(Perezetal.,2021). Validationaccuracyisoftenused
asafirststeptoselectamodel,followedbyreportingtheselectedmodel’sperformance
onanadditionalheld-outtestset. Butthemorewelookatthetestset, thelessusefulit
becomes. Hereweshowthatthemaximumrandombaselineisabetterpredictorofwhether
testperformancewillexceedrandomguessingthanthestandardrandombaselineandthus
canavoidwastedtestsetevaluations.
Setup. Just as before, we choose the prompt with best performance on the validation
dataandreport: themaximumvalidationaccuracy,thestandardrandombaseline,andthe
maximumrandombaseline. Givenjustthevalidationaccuracy,wehavetomakeadecision:
is this prompt’s test accuracy above or below random? Using the same models and 16
BIG-benchLitetasksasabove,werandomlypartitioneachtaskintoavalidationset(75%)
andatestset(25%). Foragivenmodelandtask,weselectthepromptfromamongt =200
withthehighestvalidationaccuracy. Thisprompt’svalidationaccuracyiscomparedtothe
standardrandomandthemaximumrandombaselines.
Thepromptissaidtooutperformrandomonthecorrespondingheld-outtestsetwhenits
test accuracy is above the standard random baseline (because there has been no test set
reuse). Foreachof100differentsplitsofeachdataset,weusevalidationaccuracytopredict
7Preprint.
whethertestsetaccuracyisabovestandardrandom. Wearecomparingtwo“classifiers”:
standardpredictstruewhenvalidationaccuracyisabovestandardrandom,andmaxpredicts
truewhenvalidationaccuracyisabovethemaximumrandombaseline. Inbothcasesit
ispossiblethatalowqualitypromptcouldhavehighvalidationaccuracyduetorandom
chance,sowedonotexpectperfectperformance. Anotherreasonwecannotexpectperfect
performance on this task is that just knowing that validation performance is above the
standardrandombaselinemeansthatperformanceisbetterthanthatofatleasthalfofthe
randomclassifiers,notthatitoutperformsallofthem. Butwecanevaluatewhichbaseline
givesusmoreinsightintoheld-outaccuracy.
Results. Evaluatingallpredictionsacross
1 1
allsplitsofdatasets,theheld-outaccuracy
ofthebestpromptexceedsrandom73%of 0.75 0.75
the time. Figure 5 shows that when pre-
0.50 0.50
dictingwhethertestperformanceisabove
randomfromvalidationaccuracy, standard 0.25 0.25
achieves an accuracy of 0.82, AUROC =
0.67,andAUPR=0.81. max hasalowerac- 0 0 0.25 0.50 0.75 1 0 0 0.25 0.50 0.75 1
False positive rate Recall
curacyof0.79,butdoesbetteronAUROC=
0.80 and AUPR = 0.88. standard achieves Standard random baseline Maximum random baseline
higheraccuracyonthistaskattheexpense
ofmanyfalsepositives—itoftenincorrectly Figure 5: ROC and precision-recall curves
predictsthattestperformancewillbeabove whenusingmaximumvalidationaccuracyto
random chance. In fact, the standard ran- predictwhetherheld-outtestaccuracywillbe
dom baseline does only slightly better in aboverandomchance. Blue and red curves
this setting than simply predicting that use binary predictions of above or below a
all test accuracies will be above random. random baseline. The gray curve uses the
Themaximumrandombaselinehashigher distributionfunctionforconfidencescores.
precision—when validation accuracy sur-
passesthemaximumrandombaselinewecanbemoreconfidentthatthecorresponding
held-out performance is above random. Results split by model and dataset are in Ap-
pendixC,asaredifferentvaluesoftforvalidationsetreuse. Asthenumberofpromptst
thatarebeingsearchedoverincreases,themaximumrandombaselinemaintainsalower
falsepositiverateevenasthestandardbaseline’sfalsepositiverateincreases(Figure10,
AppendixC).
Wecanadditionallyuseeachbaseline’sdistributionfunctionasameasureofhowlikely
validation performance is to be above random. Instead of directly predicting above or
below random as compared to each baseline’s expected accuracy, we instead associate
eachpointwiththepercentageofrandomclassifiersthatthevalidationaccuracyisabove.
Bothbaselinesproducethesamerankingsoflikelihoodabovethebaseline. Thisisbecause
thedistributionfunctionofthemaximumorderstatisticofthebinomialdistributionisa
monotonicfunctionofthedistributionfunctionofthebinomialdistribution(Equations3,6).
BothbaselinesthereforeyieldthesameROCandprecision-recall curves (AUROC=0.90,
AUPR=0.96). Thethresholdforpredictingaboveorbelowrandomperformanceiswhat
changes,leadingtothedifferenceshowninFigure5. Inbothcases,usingtheadditional
information provided by treating the random baselines as distributions contextualizes
performancebetterthanpointestimatesalone.
5.3 Choosinginstructionsandtemplateformatting
Themaximumrandombaselinecanrecontextualizeexistingresults. Mizrahietal.(2023)
andSclaretal.(2024)demonstratethatICLperformanceishighlyvariableacrossinstruction
paraphrasesandtemplateformatting,respectively. Astheyreleasemaximumvalidation
accuracies and the number of prompts they evaluated, we are able to analyze the same
experimentsfromtheperspectiveofcomparisontorandombaselines.
Mizrahietal.(2023)reportmaximumvalidationaccuracyacrossdifferentinstructionpara-
phrases for tasks from BIG-bench Lite and BIG-bench Hard. For BIG-bench Hard, they
8
etar
evitisop
eurT
noisicerPPreprint.
evaluate11modelson15datasetsandeachtimereportthemaximumvalidationaccuracy
across t = 10 prompts. Out of 165 experiments, 152 outperform the standard random
baseline,and147outperformthemaximumrandombaseline. 3.3%ofresultsthatexceed
thestandardrandombaselinedonotexceedthemaximumrandombaseline. ForBIG-bench
Lite,9modelsareevaluatedon13datasetswith10promptseach. Outof117experiments,
116outperformthestandardrandombaseline,and109outperformthemaximumrandom
baseline. 6.0%ofresultsthatexceedthestandardrandombaselinedonotexceedthemaxi-
mumrandombaseline. Fromtheperspectiveofrandombaselines,theseexperimentsare
relativelyrobustbecausethemodelsperformverywellandtissmall.
Sclaretal.(2024)useThompsonsamplingtoevaluatet =320differentprompttemplates
forLlama-2-70bon53differenttasksfromSuper-NaturalInstructions(Wangetal.,2022)
thatweresampledtohaven =1,000examples. Usingthemostgenerousparametersettings
thatmakethemaximumrandombaselinecomparabletotheirsetup,wefindthat51results
ofmaximumaccuracyacrossprompttemplatesareabovethestandardrandombaseline,
and46areabovethemaximumrandombaseline. 9.8%ofresultsthatexceedthestandard
randombaselinedonotexceedthemaximumrandombaseline.
Overall,themaximumrandombaselinecanmakeusmoreconfidentwhenmaximumvalida-
tionaccuraciesareaboverandomperformancewhilealsoidentifyingthosemodel/dataset
pairswithespeciallyweakperformance.
6 Discussionandconclusion
If an experiment is calculating the maximum performance across multiple evaluations
on a dataset, the standard random baseline is not necessarily a good representation of
the probability of achieving that performance by random guessing. ICL is particularly
susceptibletothisdangerbecauseofitscombinationofsmallvalidationsets,manyprompt
evaluations,anddifficulttasks. Insuchsettings,theexpectedmaximumaccuracyacross
multiplerandomclassifiersisastrongerandmoreappropriatebaseline. Thismetriciseasily
calculated,andwereleasecodeforadrop-inreplacementbaseline.
Whileitisbestpracticetoreporttestperformanceonatrulyheld-outsetofexamples(Perez
etal.,2021),themaximumrandombaselineoffersadditionalcontextualizationofmaximum
validationaccuracy. Itiscommontoreportvalidationsizen,andreportingthenumberof
promptevaluationstneededforthisbaselineisalsogoodpractice. Ofarandomsampleof
20papersfromEMNLP2023thatstudyICL,sixreportusingmultipleprompts,threestate
thattheyreportthemaximumevaluationaccuracyovermultipleprompts,andonlyoneof
thesereportshowmanytimesthevalidationsetwasused. Wearguethatreportingsuch
settingsallowsforpropercontextualizationofresultsandshouldbeabestpractice.
Wereiteratethemanycallsforlargerevaluationsets(Cardetal.,2020;Braggetal.,2021),
thistimetolimitthevarianceofrandomguessing. ButtherealityofcontemporaryLM
evaluationisthatresearchershavelimitedtimeandbudgetforcomputationanddataset
development. Producingthousandsofhigh-qualityexamplesandevaluatingmanypossible
prompts on them may not always be feasible (Liang et al., 2023; Polo et al., 2024). The
maximumrandombaselineproposedinthispaperenablesresearcherstoreducevalidation
sizeandthereforecomputation,whilestillavoidingfalselypositivepromptsettings.
Weleaveittofutureworktostudythemaximumrandombaselineacrossmoremodels,
tasks,promptvariations,andscoringstrategies,aswellasextensionstometricsbeyond
accuracylikeF1.Futureworkcanalsostudytherigorousspecificationofhypothesistestsfor
decidingwhethergeneralizationperformanceislikelytoexceedrandomguessing. While
welimitouranalysistoin-contextlearningwithlanguagemodels,thestrongerrandom
baselineappliestoanyclassificationsettingwithevaluationsetreuse.
Comparingamodel’sperformanceonanewtasktorandombaselinesisthefirsttestof
themodel’scapabilities. Randombaselinesremainrelevantevenasmodelperformance
increasesacrosstheboardbecauseincreasinglydifficulttasksareregularlyconstructedto
probethelimitsofperformance. Ultimately,validationdatasetswillcontinuetobereused,
especiallygiventhevariabilityofICLperformance. Baselinesshouldaccountforthis.
9Preprint.
References
Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel,
EmmieHine, CarolynAshurst, PaulSedille, AlexisCarlier, etal. RAFT:Areal-world
few-shottextclassificationbenchmark. InNeurIPSDatasetsandBenchmarksTrack,2021.
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,Ruxan-
dra Cojocaru, Me´rouane Debbah, E´tienne Goffinet, Daniel Hesslow, Julien Launay,
Quentin Malartic, et al. The falcon series of open language models. arXiv preprint
arXiv:2311.16867,2023.
JonathanBragg, ArmanCohan, KyleLo, andIzBeltagy. FLEX:Unifyingevaluationfor
few-shotNLP. InNeurIPS,2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Languagemodelsarefew-shotlearners. InNeurIPS,2020.
DallasCard,PeterHenderson,UrvashiKhandelwal,RobinJia,KyleMahowald,andDan
Jurafsky. Withlittlepowercomesgreatresponsibility. InEMNLP,2020.
GeorgeCasellaandRogerL.Berger. StatisticalInference. ThomsonLearning,2ndedition,
2002.
TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. QLoRA:Efficient
finetuningofquantizedllms. InNeurIPS,2023.
JesseDodge,SuchinGururangan,DallasCard,RoySchwartz,andNoahASmith. Show
yourwork: Improvedreportingofexperimentalresults. InEMNLP,2019.
JesseDodge,SuchinGururangan,DallasCard,RoySchwartz,andNoahASmith. Expected
validationperformanceandestimationofarandomvariable’smaximum. InFindingsof
EMNLP,2021.
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. The hitchhiker’s guide to
testingstatisticalsignificanceinnaturallanguageprocessing. InACL,2018.
RotemDror,SegevShlomov,andRoiReichart. Deepdominance-howtoproperlycompare
deepneuralmodels. InACL,2019.
CynthiaDwork,VitalyFeldman,MoritzHardt,ToniPitassi,OmerReingold,andAaron
Roth. Generalizationinadaptivedataanalysisandholdoutreuse. InNeurIPS,2015.
PolinaGollandandBruceFischl. Permutationtestsforclassification: towardsstatistical
significance in image-based studies. In Biennial international conference on information
processinginmedicalimaging,pp.330–341.Springer,2003.
PolinaGolland,FengLiang,SayanMukherjee,andDmitryPanchenko. Permutationtests
forclassification. JMLR,2000.
DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,
AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,etal. OLMo: Acceler-
atingthescienceoflanguagemodels. arXivpreprintarXiv:2402.00838,2024.
CharlesR.Harris,K.JarrodMillman,Ste´fanJ.vanderWalt,RalfGommers,PauliVirtanen,
DavidCournapeau,EricWieser,JulianTaylor,SebastianBerg,NathanielJ.Smith,Robert
Kern,MattiPicus,StephanHoyer,MartenH.vanKerkwijk,MatthewBrett,AllanHaldane,
JaimeFerna´ndezdelR´ıo,MarkWiebe,PearuPeterson,PierreGe´rard-Marchant,Kevin
Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and
TravisE.Oliphant. ArrayprogrammingwithNumPy. Nature,September2020.
AriHoltzman,PeterWest,VeredShwartz,YejinChoi,andLukeZettlemoyer. Surfaceform
competition: Whythehighestprobabilityanswerisn’talwaysright. InEMNLP,2021.
10Preprint.
YiliHong. OncomputingthedistributionfunctionforthePoissonbinomialdistribution.
ComputationalStatistics&DataAnalysis,59:41–51,2013.
PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,
YianZhang,DeepakNarayanan,YuhuaiWu,AnanyaKumar,etal. Holisticevaluationof
languagemodels. TMLR,2023.
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.
Pre-train,prompt,andpredict: Asystematicsurveyofpromptingmethodsinnatural
languageprocessing. ACMComputingSurveys,55(9):1–35,2023.
YaoLu,MaxBartolo,AlastairMoore,SebastianRiedel,andPontusStenetorp. Fantastically
orderedpromptsandwheretofindthem: Overcomingfew-shotpromptordersensitivity.
InACL,2022.
TomMitchell. MachineLearning. McGrawHill,1997.
MoranMizrahi,GuyKaplan,DanMalkin,RotemDror,DafnaShahaf,andGabrielStanovsky.
Stateofwhatart? acallformulti-promptLLMevaluation. arXivpreprintarXiv:2401.00595,
2023.
Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi,
NicholasKing,JonathanLarson,YuanzhiLi,WeishungLiu,etal. Cangeneralistfounda-
tionmodelsoutcompetespecial-purposetuning? casestudyinmedicine. arXivpreprint
arXiv:2311.16452,2023.
MarkusOjalaandGemmaCGarriga. Permutationtestsforstudyingclassifierperformance.
JMLR,2010.
EthanPerez,DouweKiela,andKyunghyunCho. Truefew-shotlearningwithlanguage
models. InNeurIPS,2021.
MaximePeyrard, WeiZhao, SteffenEger, andRobertWest. Betterthanaverage: Paired
evaluationofnlpsystems. InACL,2021.
FelipeMaiaPolo,LucasWeber,LeshemChoshen,YuekaiSun,GongjunXu,andMikhail
Yurochkin. tinyBenchmarks: evaluating LLMs with fewer examples. arXiv preprint
arXiv:2402.14992,2024.
MelanieSclar,YejinChoi,YuliaTsvetkov,andAlaneSuhr. Quantifyinglanguagemodels’
sensitivity to spurious features in prompt design or: How i learned to start worrying
aboutpromptformatting. InICLR,2024.
AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,
AdamFisch,AdamRBrown,AdamSantoro,AdityaGupta,Adria` Garriga-Alonso,etal.
Beyondtheimitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguage
models. TMLR,2023.
MiracSuzgun,NathanScales,NathanaelScha¨rli,SebastianGehrmann,YiTay,HyungWon
Chung,AakankshaChowdhery,QuocLe,EdChi,DennyZhou,andJasonWei. Challeng-
ingbig-benchtasksandwhetherchain-of-thoughtcansolvethem. InFindingsofACL,
2023.
RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,
Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-
followingmodel. 2023.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
Anton Voronov, Lena Wolf, and Max Ryabinin. Mind your format: Towards consistent
evaluationofin-contextlearningimprovements. arXivpreprintarXiv:2401.06766,2024.
11Preprint.
AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,Felix
Hill,OmerLevy,andSamuelRBowman. SuperGLUE:Astickierbenchmarkforgeneral-
purposelanguageunderstandingsystems. InNeurIPS,2019a.
AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelRBowman.
GLUE:Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding.
InICLR,2019b.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
Mirzaei,AtharvaNaik,ArjunAshok,ArutSelvanDhanasekaran,AnjanaArunkumar,
DavidStap,etal. Super-NaturalInstructions: Generalizationviadeclarativeinstructions
on1600+NLPtasks. InEMNLP,2022.
LarryWasserman. AllofStatistics: AConciseCourseinStatisticalInference. SpringerScience&
BusinessMedia,2013.
JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,Nan
Du,AndrewMDai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners.
InICLR,2022.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony
Moi,PierricCistac,TimRault,Re´miLouf,MorganFuntowicz,etal. Transformers: State-
of-the-artnaturallanguageprocessing. InEMNLP:SystemDemonstrations,2020.
ZihaoZhao, EricWallace, ShiFeng,DanKlein, andSameerSingh. Calibratebeforeuse:
Improvingfew-shotperformanceoflanguagemodels. InICML,2021.
RanZmigrod,TimVieira,andRyanCotterell. Exactpaired-permutationtestingforstruc-
turedteststatistics. InNAACL,2022.
12Preprint.
A Additionaldetailsoftheexpectedmaximumrandombaseline
Extendingtotaskswithdifferentnumbersoflabelsperexample. Sometasksalloweach
exampletohaveadifferentnumberofpossiblelabels. Forexample,theBIG-benchtask
logic grid puzzle has examples with 2 to 5 possible labels each. The number of correct
guessesismodeledbyaPoissonbinomialdistribution,whereeachindependenttrialcan
haveadifferentprobabilityofsuccess,ratherthanabinomialdistribution(Hong,2013). The
maximumorderstatisticofthePoissonbinomialdistributioncanbefoundbypluggingin
itsdistributionfunctionandprobabilitymassfunctionintoequation4. Thisextensionis
implementedinourcode,thoughweleavefurtherstudytofuturework.
p-valueagainstrandombaselines. The p-valuefortheaccuracyofaclassifierh against
0
thestandardrandombaselineis:
p = P(acc(h) ≥acc(h )) (12)
standard 0
=1−P(acc(h) <acc(h )) (13)
0
=1−P(nacc(h) < nacc(h )) (14)
0
=1−P(X < nacc(h )) (15)
0
=1−(F(nacc(h ))− f(nacc(h ))) (16)
0 0
=1−F(nacc(h )−1) (17)
0
Line 17 follows because the binomial is a discrete distribution over integers, so F(k) =
F(k−1)+ f(k) if we define F(−1) = 0. By similar reasoning, we get the p-value with
respecttothemaximumrandomclassifier:
p = P(acc(h ) ≥acc(h )) (18)
max max 0
(cid:16) (cid:17)
=1−P X < nacc(h ) (19)
(t) 0
(cid:0) (cid:1)t
=1− F(nacc(h )−1) (20)
0
Additionalplots. Figure6showstheexpectedmaximumaccuracyforvariousparameter
settingsofn,thenumberofexamplesintheevaluationdataset,p,theprobabilityofguessing
acorrectansweroneachexample(onedividedbythenumberofchoices),andt,thenumber
ofevaluationsetreuses. Figure7gives p-valuesfortheexpectedmaximumaccuracyfor
variousparametersettings. Thetoprow(t =1)correspondsto p-valuesforthestandard
randombaseline.
Differentdatasetsexperiencedifferentamountsofincreaseinthebaselinewhenmoving
fromstandardtomaximumbaselines. Figure8showsthisrangeforthetasksintheBIG-
benchLiteandBIG-benchHardsuites. Forexample,whenreusingtheevaluationdataset
t = 10times,threeBIG-benchLitedatasetshaveamaximumrandombaselinethatis10
accuracy points above their corresponding standard random baselines. But four other
datasetshaveanincreaseoflessthan1accuracypoint. Usingthe p-valuecorrespondingto
agivenaccuracyisanunambiguouswaytocomparetoarandombaselineacrossdatasets.
Dataset with 2 choices per example Dataset with 3 choices per example Dataset with 4 choices per example Dataset with 5 choices per example
p = 1/2 p = 1/3 p = 1/4 p = 1/5
100,000 100,000 100,000 100,000
10,000 10,000 10,000 10,000
1,000 1,000 1,000 1,000
100 100 100 100
101 10 100 1,000 10,000 100,000 101 10 100 1,000 10,000 100,000 101 10 100 1,000 10,000 100,000 101 10 100 1,000 10,000 100,000
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
Figure6: Theexpectedmaximumaccuracyforvariousparametersettings. Theleftmostplot
isthesameasFigure2butwithlargerparameterranges.
13
tesatad
fo ezis
=
nPreprint.
Dataset with 2 choices per example Dataset with 3 choices per example Dataset with 4 choices per example Dataset with 5 choices per example
p = 1/2 p = 1/3 p = 1/4 p = 1/5
1 1 1 1
0.75 0.75 0.75 0.75
t=1 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000
1 1 1 1
0.75 0.75 0.75 0.75
t=10 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000
1 1 1 1
0.75 0.75 0.75 0.75
t=100 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000
1 1 1 1
0.75 0.75 0.75 0.75
t=200 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25
010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000 010 100 1,000 10,000
n = size of dataset n = size of dataset n = size of dataset n = size of dataset
Figure 7: p-values with respect to the expected maximum random baseline for various
accuracies and parameter settings. The top row (t = 1) corresponds to p-values for the
standardrandombaseline. Contoursareshownatevery0.1interval,withthe0.5,0.1,and
0.01contourslabeled. Plotshavebeensmoothedforeaseofvisualization.
BIG-bench Lite BIG-bench Hard BIG-bench Lite BIG-bench Hard
t = 1 t = 1 t = 1 t = 1 16 16 1 1
12 8 12 8 0.1 0.1
4 4 0.01 0.01
0 0 10 20 0 0 10 20 0.001 0 10 20 30 40 50 60 700.001 0 10 20 30 40 50 60 70 8 t = 10 8 t = 10 1 t = 10 1 t = 10
4 4 0.1 0.1
0 0 0.01 0.01
0 10 20 0 10 20 0.001 0.001 t = 100 t = 100 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70
4 4 t = 100 t = 100 1 1
2 2 0.1 0.1
0 0 0.01 0.01
0 t1 =0 1000 20 0 t1 =0 1000 20 0.001 0 10 20 30 40 50 60 700.001 0 10 20 30 40 50 60 70
4 4
t = 1000 t = 1000 2 2 1 1
0.1 0.1
0 0 10 20 0 0 10 20 0.01 0.01
Accuracy difference between 0.001 0 10 20 30 40 50 60 700.001 0 10 20 30 40 50 60 70
max random baseline and standard random baseline Normalized score Normalized score
Figure8: Histogramsofhowmanydatasets Figure 9: Within a benchmark suite, the
experience a given increase in accuracy sameaccuracycanbeatdifferentdistances
when moving from the standard random fromtherandombaselinedependingonthe
baselinetothemaximumrandombaseline. dataset. Eachlineisadataset,coloredbyn.
Figure9showsthatasingleperformancevaluecanhavewildlydifferentchancesofbeing
achievedbyarandombaseline,dependingonthedatasetwithinabenchmarksuitelike
BIG-benchLiteorBIG-benchHard. Forexample,whenevaluatingBIG-benchLitetasks
(withonlyoneevaluationofthevalidationset),anormalizedscoreof10cancorrespondto
a p-valuefromnear-zeroto0.5. Putanotherway,sometasksachieveanormalizedscore
that is better than the maximum random baseline 99% of the time when they surpass a
normalizedscoreof1. Butittakesanormalizedscoreof35forothertasks.
14
stesatad
fo
rebmuN
ycaruccA
ycaruccA
ycaruccA
ycaruccA
eulav-p
eulav-p
eulav-p
eulav-pPreprint.
B Evaluationdetails
Weevaluatemodelsinthein-contextlearningsetting,wherethemodelissuppliedwitha
naturallanguagepromptfollowedbydemonstrationsandapossibleanswer(Brownetal.,
2020). Thisiscommonlyreferredtoasthefew-shotsetting, thoughfew-shotcanreferto
othersettingsaswell(Braggetal.,2021).
Datasets. We use 16 datasets from BIG-bench Lite (Srivastava et al., 2023):
bbq lite json, code line description, conceptual combinations, emoji movie,
formal fallacies syllogisms negation, hindu knowledge, known unknowns,
language identification,logical deduction,novel concepts,play dialog same or different,
strange stories, strategyqa, symbol interpretation, vitaminc fact verification, winowhy.
Weusethesebecausewithinadataset,theyhavethesameorverysimilarnumbersoflabels
acrossexamples. Webeginwitheachfulldataset, andforthetaskslargerthan n = 200
examples,werandomlysample200examplesforevaluationandkeepthesefixedforall
experiments. Listsofwhichexamplesweresampledcanbefoundinthesupplementary
code.
Promptsanddemonstrations. Apromptconsistsofanaturallanguageinstructionwith
demonstrationsinagivenformat. Eachdemonstrationisanexamplealongwithitsground-
truthlabel. Inourdeliberatelysimplesetting,wekeepthedemonstrationsconstantacross
evaluationexamples—apromptreferstoafixedstringwithfixeddemonstrationsprepended
tovalidationexamples. Weuseeachmultiplechoicetask’sstandardinstructiontemplate
providedwithBIG-bench.Weevaluate200differentpromptsforeachcombinationofmodel,
dataset,andnumberofdemonstrationshots. Demonstrationsarechosenatrandomand
appended to each other with \n. All prompts with their chosen demonstrations can be
foundinthecoderepository. Foragivenexamplewithmultiplechoices,weselectamodel’s
answerasthelabelwiththehighestaveragelog-likelihoodpertoken(Holtzmanetal.,2021).
Fortaskswithmorethan200examples,wesample200asafixedsetacrossallparameter
combinations. Wechoosedemonstrationsfromtheremainingunsampledexamplesinthe
task. Forthesevendatasetswithn <200,thereareonlynpossiblepromptdemonstrations
inthe1-shotsetting. Inthesecaseswecomparetothepropermaximumrandombaseline
witht = ninsteadoft =200. Forthesamedatasets,thereisasubtletyregardingdataset
size. Because the demonstrations come from the pool of available examples, the 1-shot,
2-shot,and4-shotversionsofthedatasetdifferbyafewexamples. Normallythiswouldnot
makemuchofadifference,butthemaximumrandombaselinedependsondatasetsize. For
easeofvisualizationinFigures3,4,11,12,and13,weplotthemaximumrandombaseline
for the dataset’s full number of examples, which is slightly weaker than each setting’s
exactmaxrandombaseline(becausethebaselineincreaseswithsmallern). Allbaseline
judgmentsinTable1arewithrespecttoeachsetting’sexactmaximumrandombaseline.
Implementationdetails. AllevaluationsareimplementedusingNumPy(Harrisetal.,2020)
andHuggingFacetransformers(Wolfetal.,2020). Weusebitsandbytestoquantizemod-
elstoNF44-bitwithnestedquantizationandcomputedtypebfloat16(Dettmersetal.,2023).
Incaseswherewecomparedquantizedmodelstonon-quantizedmodels,performancewas
notconsistentlybetterorworse. WeuseanNVIDIARTXA6000with48GBofRAM.
C Held-outsetevaluations
For the results in Section 5.2, Table 2 shows per-model results. The maximum random
baselineoutperformsthestandardbaselineinAUROCandAUPRacrossallmodelsonthese
datasets. Table3showsper-datasetresults.
15Preprint.
Accuracy Precision Recall AUROC AUPR
Model Standard Max Standard Max Standard Max Standard Max Standard Max
Llama-2-7b,Basemodel(77%) 0.92 0.87 0.91 0.95 0.99 0.88 0.82 0.85 0.91 0.93
OLMo-7B,Basemodel(72%) 0.79 0.73 0.79 0.89 0.97 0.72 0.64 0.74 0.79 0.84
Falcon-7b,Basemodel(67%) 0.79 0.77 0.77 0.90 0.98 0.74 0.69 0.79 0.77 0.84
Alpaca-7b,Instruction-tuned(84%) 0.83 0.87 0.84 0.96 0.99 0.88 0.50 0.84 0.84 0.94
OLMo-7B,Instruction-tuned(72%) 0.78 0.72 0.78 0.88 0.97 0.71 0.63 0.73 0.78 0.83
Falcon-7b,Instruction-tuned(68%) 0.79 0.80 0.77 0.91 0.98 0.77 0.67 0.81 0.77 0.86
Total(73%) 0.82 0.79 0.81 0.92 0.98 0.79 0.67 0.80 0.81 0.88
Table2: Per-modelresultswhenusingwhethermaximumvalidationaccuracyisaboveeach
baselinetopredictwhetherheld-outtestaccuracywillbeaboverandomchance.Percentages
inparenthesesindicatetheproportionoftrialswherethebestprompt’stestaccuracywas
aboverandomchance.
AUROC AUPR
BIG-benchLitedataset Standard Max Standard Max
codelinedescription (100%) 0.50 0.50 1.00 1.00
bbqlitejson (94%) 0.67 0.78 0.96 0.97
hinduknowledge (100%) 0.50 0.50 1.00 1.00
novelconcepts (93%) 0.50 0.46 0.93 0.92
emojimovie (62%) 0.50 0.53 0.62 0.64
vitamincfactverification (83%) 0.96 0.94 0.98 0.98
conceptualcombinations (81%) 0.50 0.55 0.81 0.83
formalfallaciessyllogismsnegation (37%) 0.58 0.51 0.41 0.37
knownunknowns (59%) 0.49 0.57 0.59 0.63
logicaldeduction (81%) 0.50 0.48 0.81 0.80
playdialogsameordifferent (59%) 0.98 0.95 0.97 0.95
strangestories (69%) 0.65 0.80 0.77 0.86
symbolinterpretation (17%) 0.78 0.50 0.33 0.17
winowhy (100%) n/a n/a 1.00 1.00
strategyqa (93%) 0.50 0.50 0.93 0.93
Total(73%) 0.67 0.80 0.81 0.88
Table3: Per-datasetresultswhenusingvalidationaccuracytopredictwhetherheld-outtest
accuracywillbeaboverandomchance. Percentagesinparenthesesindicatetheproportion
oftrialswherethebestprompt’stestaccuracywasaboverandomchance.
t = 1 t = 2 t = 5
1 1 1
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0 0 0
0 0.25 0.50 0.75 1 0 0.25 0.50 0.75 1 0 0.25 0.50 0.75 1
False positive rate False positive rate False positive rate
t = 10 t = 100 t = 200
1 1 1
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0 0 0
0 0.25 0.50 0.75 1 0 0.25 0.50 0.75 1 0 0.25 0.50 0.75 1
False positive rate False positive rate False positive rate
Standard random baseline Maximum random baseline
Figure 10: ROC curves for multiple values of t when comparing maximum validation
accuracytobaselinestopredictwhetherheld-outperformanceisaboverandomguessing.
ThefinalpaneliswhatisshownontheleftsideofFigure5.
16
etar
evitisop
eurT
etar
evitisop
eurT
etar
evitisop
eurT
etar
evitisop
eurT
etar
evitisop
eurT
etar
evitisop
eurTPreprint.
D Fullresults
ThissectionprovidesfullBIG-benchLiteresultsforLlama-2-7bandAlpaca-7binFigure11,
OLMo-7B and OLMo-7B-Instruct in Figure 12, and Falcon-7b and Falcon-7b-instruct in
Figure13. Wereportexpectedmaximumvalidationaccuracy,asinSection5.
code_line_description bbq_lite_json hindu_knowledge conceptual_combinations
0.8 0.7 0.7 0.7
0.6 0.6 0.6 0.6
0.5 0.5 0.5
0.4 0.4 0.4 0.4
0.2 0.3 0.3 0.3
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
formal_fallacies_syllogisms_negation known_unknowns logical_deduction play_dialog_same_or_different
0.6 0.5
0.8 0.4 0.6
0.4 0.7 0.3 0.4
0.2 0.6 0.2 0.2
0.5 0.1
0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
strange_stories symbol_interpretation winowhy strategyqa
0.6
0.6 0.70
0.25 0.5 0.65
0.4 0.20 0.4 0.60
0.15
0.2 0.3 0.55
0.10
0.2 0.50
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
novel_concepts emoji_movie vitaminc_fact_verification language_identification
0.6
0.7 0.40 0.20
0.6 0.35 0.4 0.15
0.5 0.30
0.4 0.25 0.2 0.10
0.3 0.20 0.05
0.2 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
S bt aa sn ed lia nr ed random M baa sx e lr ia nn edom B a pc re ors o mt s 1 ps- tts shot B a pc re ors o mt s 2 ps- tts shot B a pc re ors o mt s 4 ps- tts shot B i t pn u re s ons t met r u d1 pc - a tts si ch o ro n ot - ss t B i t pn u re s ons t met r u d2 pc - a tts si ch o ro n ot - ss t B i t pn u re s ons t met r u d4 pc - a tts si ch o ro n ot - ss t
Figure11: FullBIG-benchLiteresultsforLlama-2-7bandAlpaca-7b.
17
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(Preprint.
code_line_description bbq_lite_json hindu_knowledge conceptual_combinations
0.7 0.6 0.50
0.6 0.5 0.45
0.5 00 .. 45 0.4 00 .. 34 50
0.4 0.3 0.3 0.3 00 .. 23 50
0.2 0.2
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
formal_fallacies_syllogisms_negation known_unknowns logical_deduction play_dialog_same_or_different
0.6 0.8 0.30
0.6 0.4 0.7 0.25
0.20 0.4
0.2 0.6 0.15 0.2
0.0 0.5 0.10 0.0
0.05
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
strange_stories symbol_interpretation winowhy strategyqa
0.5 0.6
0.4 0.25 0.5 0.60
0.3 0.20 0.4
0.55 0.2 0.15 0.3
0.1 0.10 0.2 0.50
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
novel_concepts emoji_movie vitaminc_fact_verification language_identification
0.6 0.15
0.30
0.5 0.4
0.10
0.4 0.25
0.2
0.3 0.20 0.05
0.0
0.2
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
S bt aa sn ed lia nr ed random M baa sx e lr ia nn edom B a pc re ors o mt s 1 ps- tts shot B a pc re ors o mt s 2 ps- tts shot B a pc re ors o mt s 4 ps- tts shot B i t pn u re s ons t met r u d1 pc - a tts si ch o ro n ot - ss t B i t pn u re s ons t met r u d2 pc - a tts si ch o ro n ot - ss t B i t pn u re s ons t met r u d4 pc - a tts si ch o ro n ot - ss t
Figure12: FullBIG-benchLiteresultsforOLMo-7BandOLMo-7B-Instruct.
18
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(Preprint.
code_line_description bbq_lite_json hindu_knowledge conceptual_combinations
0.5 0.5 0.5
0.6
0.4 0.4
0.4
0.4 0.3 0.3
0.3
0.2 0.2 0.2
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
formal_fallacies_syllogisms_negation known_unknowns logical_deduction play_dialog_same_or_different
0.6 0.70 0.30 0.6 0.4 0.65 0.25
0.60 0.20 0.4
0.2 0.55 0.15 0.2
0.0 00 .. 45 50 0.10 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
strange_stories symbol_interpretation winowhy strategyqa
0.5 0.25 0.6 0.70
0.4 0.5 0.65
0.20
0.3 0.4 0.60
0.2 0.15 0.3 0.55
0.1 0.10 0.2 0.50
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
novel_concepts emoji_movie vitaminc_fact_verification language_identification
0.6
0.6 0.30 0.15
0.5 0.25 0.4 0.10
0.4
0.20 0.2 0.05
0.3
0.2 0.15 0.0 0.00
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations t = number of validation set evaluations
S bt aa sn ed lia nr ed random M baa sx e lr ia nn edom B a pc re ors o mt s 1 ps- tts shot B a pc re ors o mt s 2 ps- tts shot B a pc re ors o mt s 4 ps- tts shot B i t pn u re s ons t met r u d1 pc - a tts si ch o ro n ot - ss t B i t pn u re s ons t met r u d2 pc - a tts si ch o ro n ot - ss t B i t pn u re s ons t met r u d4 pc - a tts si ch o ro n ot - ss t
Figure13: FullBIG-benchLiteresultsforfalcon-7bandfalcon-7b-instruct.
19
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
detcepxE
eulav-p
eulav-p
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(
ycarucca
xam
)dradnats(
)mumixam(