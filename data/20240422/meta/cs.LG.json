[
    {
        "title": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI",
        "authors": "Soham GadgilMahtab Bigverdi",
        "links": "http://arxiv.org/abs/2404.13043v1",
        "entry_id": "http://arxiv.org/abs/2404.13043v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13043v1",
        "summary": "AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.",
        "updated": "2024-04-19 17:57:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13043v1"
    },
    {
        "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
        "authors": "Xi WangNicolas DufourNefeli AndreouMarie-Paule CaniVictoria Fernandez AbrevayaDavid PicardVicky Kalogeiton",
        "links": "http://arxiv.org/abs/2404.13040v1",
        "entry_id": "http://arxiv.org/abs/2404.13040v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13040v1",
        "summary": "Classifier-Free Guidance (CFG) enhances the quality and condition adherence\nof text-to-image diffusion models. It operates by combining the conditional and\nunconditional predictions using a fixed weight. However, recent works vary the\nweights throughout the diffusion process, reporting superior results but\nwithout providing any rationale or analysis. By conducting comprehensive\nexperiments, this paper provides insights into CFG weight schedulers. Our\nfindings suggest that simple, monotonically increasing weight schedulers\nconsistently lead to improved performances, requiring merely a single line of\ncode. In addition, more complex parametrized schedulers can be optimized for\nfurther improvement, but do not generalize across different models and tasks.",
        "updated": "2024-04-19 17:53:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13040v1"
    },
    {
        "title": "Stronger Random Baselines for In-Context Learning",
        "authors": "Gregory YauneyDavid Mimno",
        "links": "http://arxiv.org/abs/2404.13020v1",
        "entry_id": "http://arxiv.org/abs/2404.13020v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13020v1",
        "summary": "Evaluating the in-context learning classification performance of language\nmodels poses challenges due to small dataset sizes, extensive prompt-selection\nusing the validation set, and intentionally difficult tasks that lead to\nnear-random performance. The standard random baseline -- the expected accuracy\nof guessing labels uniformly at random -- is stable when the evaluation set is\nused only once or when the dataset is large. We account for the common practice\nof validation set reuse and existing small datasets with a stronger random\nbaseline: the expected maximum accuracy across multiple random classifiers.\nWhen choosing the best prompt demonstrations across six quantized language\nmodels applied to 16 BIG-bench Lite tasks, more than 20\\% of the few-shot\nresults that exceed the standard baseline do not exceed this stronger random\nbaseline. When held-out test sets are available, this stronger baseline is also\na better predictor of held-out performance than the standard baseline, avoiding\nunnecessary test set evaluations. This maximum random baseline provides an\neasily calculated drop-in replacement for the standard baseline.",
        "updated": "2024-04-19 17:30:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13020v1"
    },
    {
        "title": "Optimizing Calibration by Gaining Aware of Prediction Correctness",
        "authors": "Yuchi LiuLei WangYuli ZouJames ZouLiang Zheng",
        "links": "http://arxiv.org/abs/2404.13016v1",
        "entry_id": "http://arxiv.org/abs/2404.13016v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13016v1",
        "summary": "Model calibration aims to align confidence with prediction correctness. The\nCross-Entropy CE) loss is widely used for calibrator training, which enforces\nthe model to increase confidence on the ground truth class. However, we find\nthe CE loss has intrinsic limitations. For example, for a narrow\nmisclassification, a calibrator trained by the CE loss often produces high\nconfidence on the wrongly predicted class (e.g., a test sample is wrongly\nclassified and its softmax score on the ground truth class is around 0.4),\nwhich is undesirable. In this paper, we propose a new post-hoc calibration\nobjective derived from the aim of calibration. Intuitively, the proposed\nobjective function asks that the calibrator decrease model confidence on\nwrongly predicted samples and increase confidence on correctly predicted\nsamples. Because a sample itself has insufficient ability to indicate\ncorrectness, we use its transformed versions (e.g., rotated, greyscaled and\ncolor-jittered) during calibrator training. Trained on an in-distribution\nvalidation set and tested with isolated, individual test samples, our method\nachieves competitive calibration performance on both in-distribution and\nout-of-distribution test sets compared with the state of the art. Further, our\nanalysis points out the difference between our method and commonly used\nobjectives such as CE loss and mean square error loss, where the latters\nsometimes deviates from the calibration aim.",
        "updated": "2024-04-19 17:25:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13016v1"
    },
    {
        "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
        "authors": "Chuofan MaYi JiangJiannan WuZehuan YuanXiaojuan Qi",
        "links": "http://arxiv.org/abs/2404.13013v1",
        "entry_id": "http://arxiv.org/abs/2404.13013v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13013v1",
        "summary": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.",
        "updated": "2024-04-19 17:22:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13013v1"
    }
]