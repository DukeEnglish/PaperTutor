[
    {
        "title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context",
        "authors": "Zhuofan ZongBingqi MaDazhong ShenGuanglu SongHao ShaoDongzhi JiangHongsheng LiYu Liu",
        "links": "http://arxiv.org/abs/2404.13046v1",
        "entry_id": "http://arxiv.org/abs/2404.13046v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13046v1",
        "summary": "As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM) equipped with expert-routing low-rank\nadaptation (LoRA). In the fine-grained stage, we elaborately conduct the\nmixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse\ntask-specific knowledge from various experts. This coarse-to-fine paradigm\neffectively leverages representations from experts based on multimodal context\nand model expertise, further enhancing the generalization ability. We conduct\nextensive experiments to evaluate the effectiveness of the proposed approach.\nWithout any bells and whistles, MoVA can achieve significant performance gains\nover current state-of-the-art methods in a wide range of challenging multimodal\nbenchmarks. Codes and models will be available at\nhttps://github.com/TempleX98/MoVA.",
        "updated": "2024-04-19 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13046v1"
    },
    {
        "title": "Unified Scene Representation and Reconstruction for 3D Large Language Models",
        "authors": "Tao ChuPan ZhangXiaoyi DongYuhang ZangQiong LiuJiaqi Wang",
        "links": "http://arxiv.org/abs/2404.13044v1",
        "entry_id": "http://arxiv.org/abs/2404.13044v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13044v1",
        "summary": "Enabling Large Language Models (LLMs) to interact with 3D environments is\nchallenging. Existing approaches extract point clouds either from ground truth\n(GT) geometry or 3D scenes reconstructed by auxiliary models. Text-image\naligned 2D features from CLIP are then lifted to point clouds, which serve as\ninputs for LLMs. However, this solution lacks the establishment of 3D\npoint-to-point connections, leading to a deficiency of spatial structure\ninformation. Concurrently, the absence of integration and unification between\nthe geometric and semantic representations of the scene culminates in a\ndiminished level of 3D scene understanding. In this paper, we demonstrate the\nimportance of having a unified scene representation and reconstruction\nframework, which is essential for LLMs in 3D scenes. Specifically, we introduce\nUni3DR^2 extracts 3D geometric and semantic aware representation features via\nthe frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and a\nmulti-scale aggregate 3D decoder. Our learned 3D representations not only\ncontribute to the reconstruction process but also provide valuable knowledge\nfor LLMs. Experimental results validate that our Uni3DR^2 yields convincing\ngains over the baseline on the 3D reconstruction dataset ScanNet (increasing\nF-Score by +1.8\\%). When applied to LLMs, our Uni3DR^2-LLM exhibits superior\nperformance over the baseline on the 3D vision-language understanding dataset\nScanQA (increasing BLEU-1 by +4.0\\% and +4.2\\% on the val set and test set,\nrespectively). Furthermore, it outperforms the state-of-the-art method that\nuses additional GT point clouds on both ScanQA and 3DMV-VQA.",
        "updated": "2024-04-19 17:58:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13044v1"
    },
    {
        "title": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI",
        "authors": "Soham GadgilMahtab Bigverdi",
        "links": "http://arxiv.org/abs/2404.13043v1",
        "entry_id": "http://arxiv.org/abs/2404.13043v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13043v1",
        "summary": "AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.",
        "updated": "2024-04-19 17:57:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13043v1"
    },
    {
        "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
        "authors": "Xi WangNicolas DufourNefeli AndreouMarie-Paule CaniVictoria Fernandez AbrevayaDavid PicardVicky Kalogeiton",
        "links": "http://arxiv.org/abs/2404.13040v1",
        "entry_id": "http://arxiv.org/abs/2404.13040v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13040v1",
        "summary": "Classifier-Free Guidance (CFG) enhances the quality and condition adherence\nof text-to-image diffusion models. It operates by combining the conditional and\nunconditional predictions using a fixed weight. However, recent works vary the\nweights throughout the diffusion process, reporting superior results but\nwithout providing any rationale or analysis. By conducting comprehensive\nexperiments, this paper provides insights into CFG weight schedulers. Our\nfindings suggest that simple, monotonically increasing weight schedulers\nconsistently lead to improved performances, requiring merely a single line of\ncode. In addition, more complex parametrized schedulers can be optimized for\nfurther improvement, but do not generalize across different models and tasks.",
        "updated": "2024-04-19 17:53:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13040v1"
    },
    {
        "title": "LaPA: Latent Prompt Assist Model For Medical Visual Question Answering",
        "authors": "Tiancheng GuKaicheng YangDongnan LiuWeidong Cai",
        "links": "http://arxiv.org/abs/2404.13039v1",
        "entry_id": "http://arxiv.org/abs/2404.13039v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13039v1",
        "summary": "Medical visual question answering (Med-VQA) aims to automate the prediction\nof correct answers for medical images and questions, thereby assisting\nphysicians in reducing repetitive tasks and alleviating their workload.\nExisting approaches primarily focus on pre-training models using additional and\ncomprehensive datasets, followed by fine-tuning to enhance performance in\ndownstream tasks. However, there is also significant value in exploring\nexisting models to extract clinically relevant information. In this paper, we\npropose the Latent Prompt Assist model (LaPA) for medical visual question\nanswering. Firstly, we design a latent prompt generation module to generate the\nlatent prompt with the constraint of the target answer. Subsequently, we\npropose a multi-modal fusion block with latent prompt fusion module that\nutilizes the latent prompt to extract clinical-relevant information from\nuni-modal and multi-modal features. Additionally, we introduce a prior\nknowledge fusion module to integrate the relationship between diseases and\norgans with the clinical-relevant information. Finally, we combine the final\nintegrated information with image-language cross-modal information to predict\nthe final answers. Experimental results on three publicly available Med-VQA\ndatasets demonstrate that LaPA outperforms the state-of-the-art model ARL,\nachieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, and\nVQA-2019, respectively. The code is publicly available at\nhttps://github.com/GaryGuTC/LaPA_model.",
        "updated": "2024-04-19 17:51:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13039v1"
    }
]