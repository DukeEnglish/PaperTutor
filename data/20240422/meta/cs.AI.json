[
    {
        "title": "Mapping Social Choice Theory to RLHF",
        "authors": "Jessica DaiEve Fleisig",
        "links": "http://arxiv.org/abs/2404.13038v1",
        "entry_id": "http://arxiv.org/abs/2404.13038v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13038v1",
        "summary": "Recent work on the limitations of using reinforcement learning from human\nfeedback (RLHF) to incorporate human preferences into model behavior often\nraises social choice theory as a reference point. Social choice theory's\nanalysis of settings such as voting mechanisms provides technical\ninfrastructure that can inform how to aggregate human preferences amid\ndisagreement. We analyze the problem settings of social choice and RLHF,\nidentify key differences between them, and discuss how these differences may\naffect the RLHF interpretation of well-known technical results in social\nchoice.",
        "updated": "2024-04-19 17:49:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13038v1"
    },
    {
        "title": "When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering",
        "authors": "Stephen ChoiWilliam Gazeley",
        "links": "http://arxiv.org/abs/2404.13028v1",
        "entry_id": "http://arxiv.org/abs/2404.13028v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13028v1",
        "summary": "This paper presents the LLM-ADE framework, a novel methodology for continued\npre-training of large language models (LLMs) that addresses the challenges of\ncatastrophic forgetting and double descent. LLM-ADE employs dynamic\narchitectural adjustments, including selective block freezing and expansion,\ntailored to specific datasets. This strategy enhances model adaptability to new\ndata while preserving previously acquired knowledge. We demonstrate LLM-ADE's\neffectiveness on the TinyLlama model across various general knowledge\nbenchmarks, showing significant performance improvements without the drawbacks\nof traditional continuous training methods. This approach promises a more\nversatile and robust way to keep LLMs current and efficient in real-world\napplications.",
        "updated": "2024-04-19 17:43:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13028v1"
    },
    {
        "title": "PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation",
        "authors": "Tianyuan ZhangHong-Xing YuRundi WuBrandon Y. FengChangxi ZhengNoah SnavelyJiajun WuWilliam T. Freeman",
        "links": "http://arxiv.org/abs/2404.13026v1",
        "entry_id": "http://arxiv.org/abs/2404.13026v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13026v1",
        "summary": "Realistic object interactions are crucial for creating immersive virtual\nexperiences, yet synthesizing realistic 3D object dynamics in response to novel\ninteractions remains a significant challenge. Unlike unconditional or\ntext-conditioned dynamics generation, action-conditioned dynamics requires\nperceiving the physical material properties of objects and grounding the 3D\nmotion prediction on these properties, such as object stiffness. However,\nestimating physical material properties is an open problem due to the lack of\nmaterial ground-truth data, as measuring these properties for real objects is\nhighly difficult. We present PhysDreamer, a physics-based approach that endows\nstatic 3D objects with interactive dynamics by leveraging the object dynamics\npriors learned by video generation models. By distilling these priors,\nPhysDreamer enables the synthesis of realistic object responses to novel\ninteractions, such as external forces or agent manipulations. We demonstrate\nour approach on diverse examples of elastic objects and evaluate the realism of\nthe synthesized interactions through a user study. PhysDreamer takes a step\ntowards more engaging and realistic virtual experiences by enabling static 3D\nobjects to dynamically respond to interactive stimuli in a physically plausible\nmanner. See our project page at https://physdreamer.github.io/.",
        "updated": "2024-04-19 17:41:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13026v1"
    },
    {
        "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
        "authors": "Chuofan MaYi JiangJiannan WuZehuan YuanXiaojuan Qi",
        "links": "http://arxiv.org/abs/2404.13013v1",
        "entry_id": "http://arxiv.org/abs/2404.13013v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13013v1",
        "summary": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.",
        "updated": "2024-04-19 17:22:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13013v1"
    },
    {
        "title": "FinLangNet: A Novel Deep Learning Framework for Credit Risk Prediction Using Linguistic Analogy in Financial Data",
        "authors": "Yu LeiZixuan WangChu LiuTongyao WangDongyang Lee",
        "links": "http://arxiv.org/abs/2404.13004v1",
        "entry_id": "http://arxiv.org/abs/2404.13004v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13004v1",
        "summary": "Recent industrial applications in risk prediction still heavily rely on\nextensively manually-tuned, statistical learning methods. Real-world financial\ndata, characterized by its high-dimensionality, sparsity, high noise levels,\nand significant imbalance, poses unique challenges for the effective\napplication of deep neural network models. In this work, we introduce a novel\ndeep learning risk prediction framework, FinLangNet, which conceptualizes\ncredit loan trajectories in a structure that mirrors linguistic constructs.\nThis framework is tailored for credit risk prediction using real-world\nfinancial data, drawing on structural similarities to language by adapting\nnatural language processing techniques. It focuses on analyzing the evolution\nand predictability of credit histories through detailed financial event\nsequences. Our research demonstrates that FinLangNet surpasses traditional\nstatistical methods in predicting credit risk and that its integration with\nthese methods enhances credit card fraud prediction models, achieving a\nsignificant improvement of over 1.5 points in the Kolmogorov-Smirnov metric.",
        "updated": "2024-04-19 17:01:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13004v1"
    }
]