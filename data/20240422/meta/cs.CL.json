[
    {
        "title": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI",
        "authors": "Soham GadgilMahtab Bigverdi",
        "links": "http://arxiv.org/abs/2404.13043v1",
        "entry_id": "http://arxiv.org/abs/2404.13043v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13043v1",
        "summary": "AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.",
        "updated": "2024-04-19 17:57:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13043v1"
    },
    {
        "title": "LaPA: Latent Prompt Assist Model For Medical Visual Question Answering",
        "authors": "Tiancheng GuKaicheng YangDongnan LiuWeidong Cai",
        "links": "http://arxiv.org/abs/2404.13039v1",
        "entry_id": "http://arxiv.org/abs/2404.13039v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13039v1",
        "summary": "Medical visual question answering (Med-VQA) aims to automate the prediction\nof correct answers for medical images and questions, thereby assisting\nphysicians in reducing repetitive tasks and alleviating their workload.\nExisting approaches primarily focus on pre-training models using additional and\ncomprehensive datasets, followed by fine-tuning to enhance performance in\ndownstream tasks. However, there is also significant value in exploring\nexisting models to extract clinically relevant information. In this paper, we\npropose the Latent Prompt Assist model (LaPA) for medical visual question\nanswering. Firstly, we design a latent prompt generation module to generate the\nlatent prompt with the constraint of the target answer. Subsequently, we\npropose a multi-modal fusion block with latent prompt fusion module that\nutilizes the latent prompt to extract clinical-relevant information from\nuni-modal and multi-modal features. Additionally, we introduce a prior\nknowledge fusion module to integrate the relationship between diseases and\norgans with the clinical-relevant information. Finally, we combine the final\nintegrated information with image-language cross-modal information to predict\nthe final answers. Experimental results on three publicly available Med-VQA\ndatasets demonstrate that LaPA outperforms the state-of-the-art model ARL,\nachieving improvements of 1.83%, 0.63%, and 1.80% on VQA-RAD, SLAKE, and\nVQA-2019, respectively. The code is publicly available at\nhttps://github.com/GaryGuTC/LaPA_model.",
        "updated": "2024-04-19 17:51:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13039v1"
    },
    {
        "title": "Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs",
        "authors": "Biyang GuoHe WangWenyilin XiaoHong ChenZhuxin LeeSongqiao HanHailiang Huang",
        "links": "http://arxiv.org/abs/2404.13033v1",
        "entry_id": "http://arxiv.org/abs/2404.13033v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13033v1",
        "summary": "In the burgeoning field of Large Language Models (LLMs) like ChatGPT and\nLLaMA, Prompt Engineering (PE) is renowned for boosting zero-shot or in-context\nlearning (ICL) through prompt modifications. Yet, the realm of the sample\ndesign for downstream fine-tuning, crucial for task-specific LLM adaptation, is\nlargely unexplored. This paper introduces Sample Design Engineering (SDE), a\nmethodical approach to enhancing LLMs' post-tuning performance by refining\ninput, output, and reasoning designs. We conduct a series of in-domain (ID) and\nout-of-domain (OOD) experiments to assess the impact of various design options\non LLMs' downstream performance, revealing several intriguing patterns that\nhold consistently across different LLMs. Based on these insights, we propose an\nintegrated SDE strategy, combining the most effective options, and validate its\nconsistent superiority over heuristic sample designs in complex downstream\ntasks like multi-aspect sentiment analysis, event extraction, and nested entity\nrecognition. Additionally, analyses of LLMs' inherent prompt/output perplexity,\nzero-shot, and ICL abilities illustrate that good PE strategies may not always\ntranslate to good SDE strategies. Code available at\nhttps://github.com/beyondguo/LLM-Tuning.",
        "updated": "2024-04-19 17:47:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13033v1"
    },
    {
        "title": "Stronger Random Baselines for In-Context Learning",
        "authors": "Gregory YauneyDavid Mimno",
        "links": "http://arxiv.org/abs/2404.13020v1",
        "entry_id": "http://arxiv.org/abs/2404.13020v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13020v1",
        "summary": "Evaluating the in-context learning classification performance of language\nmodels poses challenges due to small dataset sizes, extensive prompt-selection\nusing the validation set, and intentionally difficult tasks that lead to\nnear-random performance. The standard random baseline -- the expected accuracy\nof guessing labels uniformly at random -- is stable when the evaluation set is\nused only once or when the dataset is large. We account for the common practice\nof validation set reuse and existing small datasets with a stronger random\nbaseline: the expected maximum accuracy across multiple random classifiers.\nWhen choosing the best prompt demonstrations across six quantized language\nmodels applied to 16 BIG-bench Lite tasks, more than 20\\% of the few-shot\nresults that exceed the standard baseline do not exceed this stronger random\nbaseline. When held-out test sets are available, this stronger baseline is also\na better predictor of held-out performance than the standard baseline, avoiding\nunnecessary test set evaluations. This maximum random baseline provides an\neasily calculated drop-in replacement for the standard baseline.",
        "updated": "2024-04-19 17:30:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13020v1"
    },
    {
        "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
        "authors": "Chuofan MaYi JiangJiannan WuZehuan YuanXiaojuan Qi",
        "links": "http://arxiv.org/abs/2404.13013v1",
        "entry_id": "http://arxiv.org/abs/2404.13013v1",
        "pdf_url": "http://arxiv.org/pdf/2404.13013v1",
        "summary": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded\nand fine-grained visual perception ability. Beyond holistic image\nunderstanding, Groma is adept at region-level tasks such as region captioning\nand visual grounding. Such capabilities are built upon a localized visual\ntokenization mechanism, where an image input is decomposed into regions of\ninterest and subsequently encoded into region tokens. By integrating region\ntokens into user instructions and model responses, we seamlessly enable Groma\nto understand user-specified region inputs and ground its textual output to\nimages. Besides, to enhance the grounded chat ability of Groma, we curate a\nvisually grounded instruction dataset by leveraging the powerful GPT-4V and\nvisual prompting techniques. Compared with MLLMs that rely on the language\nmodel or external module for localization, Groma consistently demonstrates\nsuperior performances in standard referring and grounding benchmarks,\nhighlighting the advantages of embedding localization into image tokenization.\nProject page: https://groma-mllm.github.io/.",
        "updated": "2024-04-19 17:22:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.13013v1"
    }
]