[
    {
        "title": "Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases",
        "authors": "Mateusz DaniolDaria HemmerlingJakub SikoraPawel JemioloMarek WodzinskiMagdalena Wojcik-Pedziwiatr",
        "links": "http://arxiv.org/abs/2404.12984v1",
        "entry_id": "http://arxiv.org/abs/2404.12984v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12984v1",
        "summary": "Parkinson's disease ranks as the second most prevalent neurodegenerative\ndisorder globally. This research aims to develop a system leveraging Mixed\nReality capabilities for tracking and assessing eye movements. In this paper,\nwe present a medical scenario and outline the development of an application\ndesigned to capture eye-tracking signals through Mixed Reality technology for\nthe evaluation of neurodegenerative diseases. Additionally, we introduce a\npipeline for extracting clinically relevant features from eye-gaze analysis,\ndescribing the capabilities of the proposed system from a medical perspective.\nThe study involved a cohort of healthy control individuals and patients\nsuffering from Parkinson's disease, showcasing the feasibility and potential of\nthe proposed technology for non-intrusive monitoring of eye movement patterns\nfor the diagnosis of neurodegenerative diseases.\n  Clinical relevance - Developing a non-invasive biomarker for Parkinson's\ndisease is urgently needed to accurately detect the disease's onset. This would\nallow for the timely introduction of neuroprotective treatment at the earliest\nstage and enable the continuous monitoring of intervention outcomes. The\nability to detect subtle changes in eye movements allows for early diagnosis,\noffering a critical window for intervention before more pronounced symptoms\nemerge. Eye tracking provides objective and quantifiable biomarkers, ensuring\nreliable assessments of disease progression and cognitive function. The eye\ngaze analysis using Mixed Reality glasses is wireless, facilitating convenient\nassessments in both home and hospital settings. The approach offers the\nadvantage of utilizing hardware that requires no additional specialized\nattachments, enabling examinations through personal eyewear.",
        "updated": "2024-04-19 16:34:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12984v1"
    },
    {
        "title": "Ring-a-Pose: A Ring for Continuous Hand Pose Tracking",
        "authors": "Tianhong Catherine YuGuilin HuRuidong ZhangHyunchul LimSaif MahmudChi-Jung LeeKe LiDevansh AgarwalShuyang NieJinseok OhFrançois GuimbretièreCheng Zhang",
        "links": "http://arxiv.org/abs/2404.12980v1",
        "entry_id": "http://arxiv.org/abs/2404.12980v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12980v1",
        "summary": "We present Ring-a-Pose, a single untethered ring that tracks continuous 3D\nhand poses. Located in the center of the hand, the ring emits an inaudible\nacoustic signal that each hand pose reflects differently. Ring-a-Pose imposes\nminimal obtrusions on the hand, unlike multi-ring or glove systems. It is not\naffected by the choice of clothing that may cover wrist-worn systems. In a\nseries of three user studies with a total of 30 participants, we evaluate\nRing-a-Pose's performance on pose tracking and micro-finger gesture\nrecognition. Without collecting any training data from a user, Ring-a-Pose\ntracks continuous hand poses with a joint error of 14.1mm. The joint error\ndecreases to 10.3mm for fine-tuned user-dependent models. Ring-a-Pose\nrecognizes 7-class micro-gestures with a 90.60% and 99.27% accuracy for\nuser-independent and user-dependent models, respectively. Furthermore, the ring\nexhibits promising performance when worn on any finger. Ring-a-Pose enables the\nfuture of smart rings to track and recognize hand poses using relatively\nlow-power acoustic sensing.",
        "updated": "2024-04-19 16:18:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12980v1"
    },
    {
        "title": "What We Augment When We Augment Visualizations: A Design Elicitation Study of How We Visually Express Data Relationships",
        "authors": "Grace GuoJohn StaskoAlex Endert",
        "links": "http://dx.doi.org/10.1145/3656650.3656666",
        "entry_id": "http://arxiv.org/abs/2404.12952v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12952v1",
        "summary": "Visual augmentations are commonly added to charts and graphs in order to\nconvey richer and more nuanced information about relationships in the data.\nHowever, many design spaces proposed for categorizing augmentations were\ndefined in a top-down manner, based on expert heuristics or from surveys of\npublished visualizations. Less well understood are user preferences and\nintuitions when designing augmentations. In this paper, we address the gap by\nconducting a design elicitation study, where study participants were asked to\ndraw the different ways they would visually express the meaning of ten\ndifferent prompts. We obtained 364 drawings from the study, and identified the\nemergent categories of augmentations used by participants. The contributions of\nthis paper are: (i) a user-defined design space of visualization augmentations,\n(ii) a repository of hand drawn augmentations made by study participants, and\n(iii) a discussion of insights into participant considerations, and connections\nbetween our study and existing design guidelines.",
        "updated": "2024-04-19 15:33:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12952v1"
    },
    {
        "title": "Visualizing Intelligent Tutor Interactions for Responsive Pedagogy",
        "authors": "Grace GuoAishwarya Mudgal Sunil KumarAdit GuptaAdam CosciaChris MacLellanAlex Endert",
        "links": "http://dx.doi.org/10.1145/3656650.3656667",
        "entry_id": "http://arxiv.org/abs/2404.12944v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12944v1",
        "summary": "Intelligent tutoring systems leverage AI models of expert learning and\nstudent knowledge to deliver personalized tutoring to students. While these\nintelligent tutors have demonstrated improved student learning outcomes, it is\nstill unclear how teachers might integrate them into curriculum and course\nplanning to support responsive pedagogy. In this paper, we conducted a design\nstudy with five teachers who have deployed Apprentice Tutors, an intelligent\ntutoring platform, in their classes. We characterized their challenges around\nanalyzing student interaction data from intelligent tutoring systems and built\nVisTA (Visualizations for Tutor Analytics), a visual analytics system that\nshows detailed provenance data across multiple coordinated views. We evaluated\nVisTA with the same five teachers, and found that the visualizations helped\nthem better interpret intelligent tutor data, gain insights into student\nproblem-solving provenance, and decide on necessary follow-up actions - such as\nproviding students with further support or reviewing skills in the classroom.\nFinally, we discuss potential extensions of VisTA into sequence query and\ndetection, as well as the potential for the visualizations to be useful for\nencouraging self-directed learning in students.",
        "updated": "2024-04-19 15:21:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12944v1"
    },
    {
        "title": "TimelinePTC: Development of a unified interface for pathways to care collection, visualization, and collaboration in first episode psychosis",
        "authors": "Walter S. MathisMaria FerraraJohn CahillSneha KarmaniSümeyra N. TayfurVinod Srihari",
        "links": "http://arxiv.org/abs/2404.12883v1",
        "entry_id": "http://arxiv.org/abs/2404.12883v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12883v1",
        "summary": "This paper presents TimelinePTC, a web-based tool developed to improve the\ncollection and analysis of Pathways to Care (PTC) data in first episode\npsychosis (FEP) research. Accurately measuring the duration of untreated\npsychosis (DUP) is essential for effective FEP treatment, requiring detailed\nunderstanding of the patient's journey to care. However, traditional PTC data\ncollection methods, mainly manual and paper-based, are time-consuming and often\nfail to capture the full complexity of care pathways.\n  TimelinePTC addresses these limitations by providing a digital platform for\ncollaborative, real-time data entry and visualization, thereby enhancing data\naccuracy and collection efficiency. Initially created for the Specialized\nTreatment Early in Psychosis (STEP) program in New Haven, Connecticut, its\ndesign allows for straightforward adaptation to other healthcare contexts,\nfacilitated by its open-source codebase.\n  The tool significantly simplifies the data collection process, making it more\nefficient and user-friendly. It automates the conversion of collected data into\na format ready for analysis, reducing manual transcription errors and saving\ntime. By enabling more detailed and consistent data collection, TimelinePTC has\nthe potential to improve healthcare access research, supporting the development\nof targeted interventions to reduce DUP and improve patient outcomes.",
        "updated": "2024-04-19 13:33:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12883v1"
    }
]