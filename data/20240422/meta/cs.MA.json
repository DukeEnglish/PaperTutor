[
    {
        "title": "Private Agent-Based Modeling",
        "authors": "Ayush ChopraArnau Quera-BofarullNurullah Giray-KuruMichael WooldridgeRamesh Raskar",
        "links": "http://arxiv.org/abs/2404.12983v1",
        "entry_id": "http://arxiv.org/abs/2404.12983v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12983v1",
        "summary": "The practical utility of agent-based models in decision-making relies on\ntheir capacity to accurately replicate populations while seamlessly integrating\nreal-world data streams. Yet, the incorporation of such data poses significant\nchallenges due to privacy concerns. To address this issue, we introduce a\nparadigm for private agent-based modeling wherein the simulation, calibration,\nand analysis of agent-based models can be achieved without centralizing the\nagents attributes or interactions. The key insight is to leverage techniques\nfrom secure multi-party computation to design protocols for decentralized\ncomputation in agent-based models. This ensures the confidentiality of the\nsimulated agents without compromising on simulation accuracy. We showcase our\nprotocols on a case study with an epidemiological simulation comprising over\n150,000 agents. We believe this is a critical step towards deploying\nagent-based models to real-world applications.",
        "updated": "2024-04-19 16:30:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12983v1"
    },
    {
        "title": "MAexp: A Generic Platform for RL-based Multi-Agent Exploration",
        "authors": "Shaohao ZhuJiacheng ZhouAnjun ChenMingming BaiJiming ChenJinming Xu",
        "links": "http://arxiv.org/abs/2404.12824v1",
        "entry_id": "http://arxiv.org/abs/2404.12824v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12824v1",
        "summary": "The sim-to-real gap poses a significant challenge in RL-based multi-agent\nexploration due to scene quantization and action discretization. Existing\nplatforms suffer from the inefficiency in sampling and the lack of diversity in\nMulti-Agent Reinforcement Learning (MARL) algorithms across different\nscenarios, restraining their widespread applications. To fill these gaps, we\npropose MAexp, a generic platform for multi-agent exploration that integrates a\nbroad range of state-of-the-art MARL algorithms and representative scenarios.\nMoreover, we employ point clouds to represent our exploration scenarios,\nleading to high-fidelity environment mapping and a sampling speed approximately\n40 times faster than existing platforms. Furthermore, equipped with an\nattention-based Multi-Agent Target Generator and a Single-Agent Motion Planner,\nMAexp can work with arbitrary numbers of agents and accommodate various types\nof robots. Extensive experiments are conducted to establish the first benchmark\nfeaturing several high-performance MARL algorithms across typical scenarios for\nrobots with continuous actions, which highlights the distinct strengths of each\nalgorithm in different scenarios.",
        "updated": "2024-04-19 12:00:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12824v1"
    },
    {
        "title": "Grasper: A Generalist Pursuer for Pursuit-Evasion Problems",
        "authors": "Pengdeng LiShuxin LiXinrun WangJakub CernyYouzhi ZhangStephen McAleerHau ChanBo An",
        "links": "http://arxiv.org/abs/2404.12626v1",
        "entry_id": "http://arxiv.org/abs/2404.12626v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12626v1",
        "summary": "Pursuit-evasion games (PEGs) model interactions between a team of pursuers\nand an evader in graph-based environments such as urban street networks. Recent\nadvancements have demonstrated the effectiveness of the pre-training and\nfine-tuning paradigm in PSRO to improve scalability in solving large-scale\nPEGs. However, these methods primarily focus on specific PEGs with fixed\ninitial conditions that may vary substantially in real-world scenarios, which\nsignificantly hinders the applicability of the traditional methods. To address\nthis issue, we introduce Grasper, a GeneRAlist purSuer for Pursuit-Evasion\npRoblems, capable of efficiently generating pursuer policies tailored to\nspecific PEGs. Our contributions are threefold: First, we present a novel\narchitecture that offers high-quality solutions for diverse PEGs, comprising\ncritical components such as (i) a graph neural network (GNN) to encode PEGs\ninto hidden vectors, and (ii) a hypernetwork to generate pursuer policies based\non these hidden vectors. As a second contribution, we develop an efficient\nthree-stage training method involving (i) a pre-pretraining stage for learning\nrobust PEG representations through self-supervised graph learning techniques\nlike GraphMAE, (ii) a pre-training stage utilizing heuristic-guided multi-task\npre-training (HMP) where heuristic-derived reference policies (e.g., through\nDijkstra's algorithm) regularize pursuer policies, and (iii) a fine-tuning\nstage that employs PSRO to generate pursuer policies on designated PEGs.\nFinally, we perform extensive experiments on synthetic and real-world maps,\nshowcasing Grasper's significant superiority over baselines in terms of\nsolution quality and generalizability. We demonstrate that Grasper provides a\nversatile approach for solving pursuit-evasion problems across a broad range of\nscenarios, enabling practical deployment in real-world situations.",
        "updated": "2024-04-19 04:54:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12626v1"
    },
    {
        "title": "Stackelberg Game-Theoretic Learning for Collaborative Assembly Task Planning",
        "authors": "Yuhan ZhaoLan ShiQuanyan Zhu",
        "links": "http://arxiv.org/abs/2404.12570v1",
        "entry_id": "http://arxiv.org/abs/2404.12570v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12570v1",
        "summary": "As assembly tasks grow in complexity, collaboration among multiple robots\nbecomes essential for task completion. However, centralized task planning has\nbecome inadequate for adapting to the increasing intelligence and versatility\nof robots, along with rising customized orders. There is a need for efficient\nand automated planning mechanisms capable of coordinating diverse robots for\ncollaborative assembly. To this end, we propose a Stackelberg game-theoretic\nlearning approach. By leveraging Stackelberg games, we characterize robot\ncollaboration through leader-follower interaction to enhance strategy seeking\nand ensure task completion. To enhance applicability across tasks, we introduce\na novel multi-agent learning algorithm: Stackelberg double deep Q-learning,\nwhich facilitates automated assembly strategy seeking and multi-robot\ncoordination. Our approach is validated through simulated assembly tasks.\nComparison with three alternative multi-agent learning methods shows that our\napproach achieves the shortest task completion time for tasks. Furthermore, our\napproach exhibits robustness against both accidental and deliberate\nenvironmental perturbations.",
        "updated": "2024-04-19 01:37:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12570v1"
    },
    {
        "title": "Learning a Stable, Safe, Distributed Feedback Controller for a Heterogeneous Platoon of Vehicles",
        "authors": "Michael H. ShahamTaskin Padir",
        "links": "http://arxiv.org/abs/2404.12474v1",
        "entry_id": "http://arxiv.org/abs/2404.12474v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12474v1",
        "summary": "Platooning of autonomous vehicles has the potential to increase safety and\nfuel efficiency on highways. The goal of platooning is to have each vehicle\ndrive at some speed (set by the leader) while maintaining a safe distance from\nits neighbors. Many prior works have analyzed various controllers for\nplatooning, most commonly linear feedback and distributed model predictive\ncontrollers. In this work, we introduce an algorithm for learning a stable,\nsafe, distributed controller for a heterogeneous platoon. Our algorithm relies\non recent developments in learning neural network stability and safety\ncertificates. We train a controller for autonomous platooning in simulation and\nevaluate its performance on hardware with a platoon of four F1Tenth vehicles.\nWe then perform further analysis in simulation with a platoon of 100 vehicles.\nExperimental results demonstrate the practicality of the algorithm and the\nlearned controller by comparing the performance of the neural network\ncontroller to linear feedback and distributed model predictive controllers.",
        "updated": "2024-04-18 19:11:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12474v1"
    }
]