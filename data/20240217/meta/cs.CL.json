[
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": "Huizhuo YuanZixiang ChenKaixuan JiQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.10210v1",
        "entry_id": "http://arxiv.org/abs/2402.10210v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10210v1",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "updated": "2024-02-15 18:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10210v1"
    },
    {
        "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
        "authors": "Eliahu HorwitzJonathan KahanaYedid Hoshen",
        "links": "http://arxiv.org/abs/2402.10208v1",
        "entry_id": "http://arxiv.org/abs/2402.10208v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10208v1",
        "summary": "The dominant paradigm in generative modeling consists of two steps: i)\npre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained\nmodel with human values via fine-tuning. This practice is considered safe, as\nno current method can recover the unsafe, pre-fine-tuning model weights. In\nthis paper, we demonstrate that this assumption is often false. Concretely, we\npresent Spectral DeTuning, a method that can recover the weights of the\npre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In\ncontrast to previous attacks that attempt to recover pre-fine-tuning\ncapabilities, our method aims to recover the exact pre-fine-tuning weights. Our\napproach exploits this new vulnerability against large-scale models such as a\npersonalized Stable Diffusion and an aligned Mistral.",
        "updated": "2024-02-15 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10208v1"
    },
    {
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
        "authors": "Rui YangXiaoman PanFeng LuoShuang QiuHan ZhongDong YuJianshu Chen",
        "links": "http://arxiv.org/abs/2402.10207v1",
        "entry_id": "http://arxiv.org/abs/2402.10207v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10207v1",
        "summary": "We consider the problem of multi-objective alignment of foundation models\nwith human preferences, which is a critical step towards helpful and harmless\nAI systems. However, it is generally costly and unstable to fine-tune large\nfoundation models using reinforcement learning (RL), and the\nmulti-dimensionality, heterogeneity, and conflicting nature of human\npreferences further complicate the alignment process. In this paper, we\nintroduce Rewards-in-Context (RiC), which conditions the response of a\nfoundation model on multiple rewards in its prompt context and applies\nsupervised fine-tuning for alignment. The salient features of RiC are\nsimplicity and adaptivity, as it only requires supervised fine-tuning of a\nsingle foundation model and supports dynamic adjustment for user preferences\nduring inference time. Inspired by the analytical solution of an abstracted\nconvex optimization problem, our dynamic inference-time adjustment method\napproaches the Pareto-optimal solution for multiple objectives. Empirical\nevidence demonstrates the efficacy of our method in aligning both Large\nLanguage Models (LLMs) and diffusion models to accommodate diverse rewards with\nonly around $10\\%$ GPU hours compared with multi-objective RL baseline.",
        "updated": "2024-02-15 18:58:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10207v1"
    },
    {
        "title": "Chain-of-Thought Reasoning Without Prompting",
        "authors": "Xuezhi WangDenny Zhou",
        "links": "http://arxiv.org/abs/2402.10200v1",
        "entry_id": "http://arxiv.org/abs/2402.10200v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10200v1",
        "summary": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding substantially\noutperforms the standard greedy decoding.",
        "updated": "2024-02-15 18:55:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10200v1"
    },
    {
        "title": "A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents",
        "authors": "Lingbo MoZeyi LiaoBoyuan ZhengYu SuChaowei XiaoHuan Sun",
        "links": "http://arxiv.org/abs/2402.10196v1",
        "entry_id": "http://arxiv.org/abs/2402.10196v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10196v1",
        "summary": "Language agents powered by large language models (LLMs) have seen exploding\ndevelopment. Their capability of using language as a vehicle for thought and\ncommunication lends an incredible level of flexibility and versatility. People\nhave quickly capitalized on this capability to connect LLMs to a wide range of\nexternal components and environments: databases, tools, the Internet, robotic\nembodiment, etc. Many believe an unprecedentedly powerful automation technology\nis emerging. However, new automation technologies come with new safety risks,\nespecially for intricate systems like language agents. There is a surprisingly\nlarge gap between the speed and scale of their development and deployment and\nour understanding of their safety risks. Are we building a house of cards? In\nthis position paper, we present the first systematic effort in mapping\nadversarial attacks against language agents. We first present a unified\nconceptual framework for agents with three major components: Perception, Brain,\nand Action. Under this framework, we present a comprehensive discussion and\npropose 12 potential attack scenarios against different components of an agent,\ncovering different attack strategies (e.g., input manipulation, adversarial\ndemonstrations, jailbreaking, backdoors). We also draw connections to\nsuccessful attack strategies previously applied to LLMs. We emphasize the\nurgency to gain a thorough understanding of language agent risks before their\nwidespread deployment.",
        "updated": "2024-02-15 18:51:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10196v1"
    }
]