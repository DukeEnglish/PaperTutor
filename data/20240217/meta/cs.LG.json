[
    {
        "title": "Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling",
        "authors": "Raunaq BhirangiChenyu WangVenkatesh PattabiramanCarmel MajidiAbhinav GuptaTess HellebrekersLerrel Pinto",
        "links": "http://arxiv.org/abs/2402.10211v1",
        "entry_id": "http://arxiv.org/abs/2402.10211v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10211v1",
        "summary": "Reasoning from sequences of raw sensory data is a ubiquitous problem across\nfields ranging from medical devices to robotics. These problems often involve\nusing long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to\npredict sequences of desirable physical quantities (e.g. force, inertial\nmeasurements). While classical approaches are powerful for locally-linear\nprediction problems, they often fall short when using real-world sensors. These\nsensors are typically non-linear, are affected by extraneous variables (e.g.\nvibration), and exhibit data-dependent drift. For many problems, the prediction\ntask is exacerbated by small labeled datasets since obtaining ground-truth\nlabels requires expensive equipment. In this work, we present Hierarchical\nState-Space Models (HiSS), a conceptually simple, new technique for continuous\nsequential prediction. HiSS stacks structured state-space models on top of each\nother to create a temporal hierarchy. Across six real-world sensor datasets,\nfrom tactile-based state prediction to accelerometer-based inertial\nmeasurement, HiSS outperforms state-of-the-art sequence models such as causal\nTransformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments\nfurther indicate that HiSS demonstrates efficient scaling to smaller datasets\nand is compatible with existing data-filtering techniques. Code, datasets and\nvideos can be found on https://hiss-csp.github.io.",
        "updated": "2024-02-15 18:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10211v1"
    },
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": "Huizhuo YuanZixiang ChenKaixuan JiQuanquan Gu",
        "links": "http://arxiv.org/abs/2402.10210v1",
        "entry_id": "http://arxiv.org/abs/2402.10210v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10210v1",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "updated": "2024-02-15 18:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10210v1"
    },
    {
        "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
        "authors": "Eliahu HorwitzJonathan KahanaYedid Hoshen",
        "links": "http://arxiv.org/abs/2402.10208v1",
        "entry_id": "http://arxiv.org/abs/2402.10208v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10208v1",
        "summary": "The dominant paradigm in generative modeling consists of two steps: i)\npre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained\nmodel with human values via fine-tuning. This practice is considered safe, as\nno current method can recover the unsafe, pre-fine-tuning model weights. In\nthis paper, we demonstrate that this assumption is often false. Concretely, we\npresent Spectral DeTuning, a method that can recover the weights of the\npre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In\ncontrast to previous attacks that attempt to recover pre-fine-tuning\ncapabilities, our method aims to recover the exact pre-fine-tuning weights. Our\napproach exploits this new vulnerability against large-scale models such as a\npersonalized Stable Diffusion and an aligned Mistral.",
        "updated": "2024-02-15 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10208v1"
    },
    {
        "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
        "authors": "Rui YangXiaoman PanFeng LuoShuang QiuHan ZhongDong YuJianshu Chen",
        "links": "http://arxiv.org/abs/2402.10207v1",
        "entry_id": "http://arxiv.org/abs/2402.10207v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10207v1",
        "summary": "We consider the problem of multi-objective alignment of foundation models\nwith human preferences, which is a critical step towards helpful and harmless\nAI systems. However, it is generally costly and unstable to fine-tune large\nfoundation models using reinforcement learning (RL), and the\nmulti-dimensionality, heterogeneity, and conflicting nature of human\npreferences further complicate the alignment process. In this paper, we\nintroduce Rewards-in-Context (RiC), which conditions the response of a\nfoundation model on multiple rewards in its prompt context and applies\nsupervised fine-tuning for alignment. The salient features of RiC are\nsimplicity and adaptivity, as it only requires supervised fine-tuning of a\nsingle foundation model and supports dynamic adjustment for user preferences\nduring inference time. Inspired by the analytical solution of an abstracted\nconvex optimization problem, our dynamic inference-time adjustment method\napproaches the Pareto-optimal solution for multiple objectives. Empirical\nevidence demonstrates the efficacy of our method in aligning both Large\nLanguage Models (LLMs) and diffusion models to accommodate diverse rewards with\nonly around $10\\%$ GPU hours compared with multi-objective RL baseline.",
        "updated": "2024-02-15 18:58:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10207v1"
    },
    {
        "title": "Ising on the Graph: Task-specific Graph Subsampling via the Ising Model",
        "authors": "Maria BånkestadJennifer AnderssonSebastian MairJens Sjölund",
        "links": "http://arxiv.org/abs/2402.10206v1",
        "entry_id": "http://arxiv.org/abs/2402.10206v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10206v1",
        "summary": "Reducing a graph while preserving its overall structure is an important\nproblem with many applications. Typically, the reduction approaches either\nremove edges (sparsification) or merge nodes (coarsening) in an unsupervised\nway with no specific downstream task in mind. In this paper, we present an\napproach for subsampling graph structures using an Ising model defined on\neither the nodes or edges and learning the external magnetic field of the Ising\nmodel using a graph neural network. Our approach is task-specific as it can\nlearn how to reduce a graph for a specific downstream task in an end-to-end\nfashion. The utilized loss function of the task does not even have to be\ndifferentiable. We showcase the versatility of our approach on three distinct\napplications: image segmentation, 3D shape sparsification, and sparse\napproximate matrix inverse determination.",
        "updated": "2024-02-15 18:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10206v1"
    }
]