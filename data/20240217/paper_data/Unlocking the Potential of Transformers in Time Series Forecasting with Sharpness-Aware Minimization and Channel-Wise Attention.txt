Unlocking the Potential of Transformers in Time Series Forecasting
with Sharpness-Aware Minimization and Channel-Wise Attention
Romain Ilbert*12 Ambroise Odonnat*1 Vasilii Feofanov1
Aladin Virmaux1 Giuseppe Paolo1 Themis Palpanas2 Ievgen Redko1
1Huawei Noah’s Ark Lab 2LIPADE, Paris Descartes University
Abstract
Oracle Reparam
Transformer Transformer + SAM
Transformer-based architectures achieved
breakthrough performance in natural lan- 4
guage processing and computer vision, yet
they remain inferior to simpler linear base-
lines in multivariate long-term forecasting. 2
To better understand this phenomenon, we
startbystudyingatoylinearforecastingprob-
lem for which we show that transformers are 0
incapable of converging to their true solu-
0 50 100
tion despite their high expressive power. We Training Epochs
further identify the attention of transform-
Figure 1: Illustration of our approach on synthetic
ers as being responsible for this low gener-
data. Oracle is the optimal solution, Transformer is
alization capacity. Building upon this in-
a base transformer, σReparam is a Transformer with
sight, we propose a shallow lightweight trans-
weight rescaling (Zhai et al., 2023) and Transformer
former model that successfully escapes bad
+ SAM is Transformer trained with sharpness-aware
local minima when optimized with sharpness-
minimization. Transformer overfits, σReparam im-
aware optimization. We empirically demon-
proves slightly but fails to reach Oracle while
strate that this result extends to all com-
Transformer+SAMgeneralizesperfectly. Thismotivates
monlyusedreal-worldmultivariatetimeseries
SAMformer,ashallowtransformercombiningSAMand
datasets. In particular, SAMformer surpasses
best practices in time series forecasting.
thecurrentstate-of-the-artmodelTSMixerby
14.33% on average, while having ∼4 times
fewer parameters. The code is available at
servations are gathered sequentially, such as medical
https://github.com/romilbert/samformer.
data (Cˇepulionis and Lukoˇseviˇciu¯te˙, 2016), electricity
consumption (UCI, nd), temperatures (Max Planck
Institute, nd), or stock prices (Sonkavde et al., 2023).
1 Introduction
A plethora of methods have been developed for this
task, from classical mathematical tools (Chen and Tao,
Multivariate time series forecasting is a classical learn-
2021; Sorjamaa et al., 2007) and statistical approaches
ingproblemthatconsistsofanalyzingtimeseriestopre-
like ARIMA (Box and Jenkins, 1990; Box et al., 1974)
dict future trends based on historical information. In
to more recent deep learning ones (Casolaro et al.,
particular,long-termforecastingisnotoriouslychalleng-
2023), including recurrent and convolutional neural
ing due to feature correlations and long-term temporal
networks (Fan et al., 2019; Lai et al., 2018a; Rangapu-
dependencies in time series. This learning problem is
ram et al., 2018; Salinas et al., 2020; Sen et al., 2019).
prevalent in those real-world applications where ob-
Recently, the transformer architecture (Vaswani et al.,
2017) became ubiquitous in natural language process-
*Equal contribution. ing (NLP) (Devlin et al., 2018; OpenAI, 2023; Radford
Correspondence to: romain.ilbert@hotmail.fr et al., 2018; Touvron et al., 2023) and computer vi-
Preprint. Copyright 2024 by the author(s).
sion (Caron et al., 2021; Dosovitskiy et al., 2021; Tou-
4202
beF
51
]GL.sc[
1v89101.2042:viXra
ssoL
noitadilaVUnlocking the Potential of Transformers
vron et al., 2021), achieving breakthrough performance 2021b) and channel-wise attention Zamir et al.
in both domains. (2022); Zhang et al. (2022) recently introduced in
computer vision community. We show that opti-
Transformers are known to be particularly efficient in
mizing such a simple transformer with sharpness-
dealing with sequential data, a property that naturally
aware minimization (SAM) allows convergence to
callsfortheirapplicationontimeseries. Unsurprisingly,
local minima with better generalization;
many works attempted to propose time series-specific
transformer architectures to benefit from their capac- 3. We empirically demonstrate the superiority of our
ity to capture temporal interactions (Nie et al., 2023; approach on common multivariate long-term fore-
Wu et al., 2021; Zhou et al., 2021, 2022). However, casting datasets. SAMformer improves the current
the current state-of-the-art in multivariate time se- state-of-the-art multivariate model TSMixer by
ries forecasting is achieved with a simpler MLP-based 14.33% on average, while having ∼4 times fewer
model (Chen et al., 2023), which significantly outper- parameters.
forms transformer-based methods. Moreover, Zeng
et al. (2023) have recently found that linear networks
2 Proposed Approach
can be on par or better than transformers for the fore-
casting task, questioning their practical utility. This
Notations. We represent scalar values with regular
curious finding serves as a starting point for our work.
letters (e.g., parameter λ), vectors with bold lower-
case letters (e.g., vector x), and matrices with bold
Limitation of current approaches. Recent works
capital letters (e.g., matrix M). We denote by M⊤
applying transformers to time series data have mainly
the transpose of M and likewise for vectors. The rank
focusedoneither(i)efficientimplementationsreducing
of a matrix M is denoted by rank(M), and its Frobe-
the quadratic cost of attention (Cirstea et al., 2022;
nius norm by ∥M∥ . We let n˜ = min{n,m}, and
Kitaev et al., 2020; Li et al., 2019; Liu et al., 2022; F
Wu et al., 2021; Zhou et al., 2021) or (ii) decomposing denote by ∥M∥ ∗ = (cid:80)n i˜ =1σ i(M) the nuclear norm
timeseriestobettercapturetheunderlyingpatternsin of M with σ i(M) being its singular values, and by
them (Wu et al., 2021; Zhou et al., 2022). Surprisingly, ∥M∥ 2 =σ max(M) its spectral norm. The identity ma-
none of these works have specifically addressed a well- trixofsizen×nisdenotedbyI n. ThenotationM≽0
known issue of transformers related to their training indicates that M is positive semi-definite.
instability, particularly present in the absence of large-
scale data (Dosovitskiy et al., 2021; Liu et al., 2020). 2.1 Problem Setup
We consider the multivariate long-term forecasting
Trainability of transformers. In computer vision
framework: givenaD-dimensionaltimeseriesoflength
and NLP, it has been found that attention matrices
L(look-back window),arrangedinamatrixX∈RD×L
can suffer from entropy or rank collapse (Dong et al.,
to facilitate channel-wise attention, our objective is
2021). Then, several approaches have been proposed
to predict its next H values (prediction horizon), de-
to overcome these issues Chen et al. (2022); Zhai et al.
noted by Y ∈ RD×H. We assume that we have
(2023). However, in the case of time series forecasting,
access to a training set that consists of N observa-
open questions remain about how transformer architec-
tions (X,Y)=({X(i)}N , {Y(i)}N ), and denote by
tures can be trained effectively without a tendency to i=0 i=0
overfit. We aim to show that by eliminating training X(i) ∈R1×L (respectively Y(i) ∈R1×H) the d-th fea-
d d
instability, transformers can excel in multivariate long- ture of the i-th input (respectively target) time series.
term forecasting, contrary to previous beliefs of their We aim to train a predictor f ω : RD×L → RD×H pa-
limitations. rameterized by ω that minimizes the mean squared
error (MSE) on the training set:
Summary of our contributions. Our proposal
N
puts forward the following contributions: 1 (cid:88)
L (ω)= ∥Y(i)−f (X(i))∥2 . (1)
train ND ω F
1. We illustrate that transformers generalize poorly i=0
and converge to sharp local minima even on a
2.2 Motivational Example
simple toy linear forecasting problem. We further
identify that attention is largely responsible for it;
Recently, Zeng et al. (2023) showed that transformers
2. We propose a shallow transformer model, termed perform on par with, or are worse than, simple linear
SAMformer, that incorporates the best practices neural networks trained to directly project the input
proposed in the research community including re- to the output. We use this observation as a starting
versible instance normalization (RevIN, Kim et al. point by considering the following generative modelRomain Ilbert, Ambroise Odonnat et al.
for our toy regression problem mimicking a time series The assumption made above is verified if P is full rank
forecasting setup considered later: and D <H, which is the case in this toy experiment.
Consequently, the optimization problem of fitting a
Y =XW toy+ε. (2) transformer on data generated with Eq. (2) theoreti-
cally admits infinitely many optimal classifiers W.
We let L=512,H=96,D=7 and W ∈ RL×H,ϵ ∈
toy
RD×H having random normal entries and generate We would now like to identify the role of attention
in solving the problem from Eq. (3). To this end,
15000 input-target pairs (X,Y) (10000 for train and
5000 for validation), with X∈RD×L having random we consider a model, termed Random Transformer,
whereonlyWisoptimized,whileself-attentionweights
normal entries.
W ,W ,W ,W are fixed during training and ini-
Q K V O
Given this generative model, we would like to develop tialized following Glorot and Bengio (2010). This ef-
a transformer architecture that can efficiently solve fectively makes the considered transformer act like a
the problem in Eq. (2) without unnecessary complex- linear model. Finally, we compare the local minima
ity. To achieve this, we propose to simplify the usual obtained by these two models after their optimization
transformer encoder by applying attention to X and using Adam with the Oracle model that corresponds
incorporating a residual connection that adds X to the to the least squares solution of Eq. (2).
attention’s output. Instead of adding a feedforward
block on top of this residual connection, we directly
6 6
employ a linear layer for output prediction. Formally,
our model is defined as follows:
3 3
f(X)=[X+A(X)XW W ]W, (3)
V O
0 0
with W ∈ RL×H,W V ∈ RL×dm, W O ∈ Rdm×L and 10 50 90 10 50 90
Training Epochs Training Epochs
A(X) being the attention matrix of an input sequence
X∈RD×L defined as Oracle Transformer Random Transformer
(cid:18)
XW
W⊤X⊤(cid:19) Figure2: Poor generalization. Despiteitssimplicity,
A(X)=softmax Q√ K ∈RD×D (4) Transformersuffersfromsevereoverfitting. Fixingthe
d
m attention weights in Random Transformer improves
the generalization, hinting at the role of attention in
where the softmax is row-wise, W
Q
∈RL×dm,W
K
∈
avoiding convergence to suboptimal local minima.
RL×dm, and d
m
is the dimension of the model. The
softmax makes A(X) right stochastic, with each row
describing a probability distribution. To ease the nota- We present the validation loss for both models in Fig-
tions, in contexts where it is unambiguous, we refer to ure 2. A first surprising finding is that both transform-
the attention matrix simply as A, omitting X. ers fail to recover W toy, vividly highlighting that opti-
mizingevensuchasimplearchitecturewithafavorable
We term this architecture Transformer and briefly
design exhibits a strong lack of generalization. When
comment on it. First, the attention matrix is applied
fixing the self-attention matrices, the problem is alle-
channel-wise, which simplifies the problem and reduces
viated to some extent although Random Transformer
the risk of overparametrization, as the matrix W has
remains suboptimal. This observation remains consis-
the same shape as in Eq. (2) and the attention matrix
tentacrossvariousoptimizers(seeAppendixFigure15),
becomes much smaller due to L > D. In addition,
suggesting that this phenomenon is not attributable to
channel-wise attention is more relevant than temporal
suboptimal optimizer hyperparameters or the specific
attention in this scenario, as data generation follows
choice of the optimizer.
an i.i.d. process according to Eq. (2). We formally
establishtheidentifiabilityofW byourmodelbelow.
toy
2.3 Transformer’s Loss Landscape
The proof is deferred to Appendix E.2.
Intuition. Todevelopourintuitionbehindthefailure
Proposition 2.1 (Existenceofoptimalsolutions). of transformers observed above, we plot in Figure 3a
Assume W ,W ,W and W are fixed and let the attention matrices at different epochs of training.
Q K V O
P = X+A(X)XW W ∈ RD×L. Then, there It’s observed that the attention matrix approaches an
V O
exists a matrix W ∈ RL×H such that PW = identity matrix after the very first epoch and barely
XW if, and only if, rank([P XW ]) = changes afterward, especially with the softmax ampli-
toy toy
rank(P) where [P XW ] ∈ RD×(L+H) is a fying the differences in the matrix values. This patho-
toy
block matrix. logicalpatternsuggeststhattheoptimizedtransformer
ssoL
gniniarT
ssoL
noitadilaVUnlocking the Potential of Transformers
probably falls into a suboptimal local minimum, from performance despite maximizing the entropy of the
which it struggles to exit in subsequent iterations. We attention matrix. One can observe that the entropy
hypothesize that this inability to escape from the local of the attention obtained with SAM remains close to
minimumcanbeexplained by thesharpness of the loss that of a base Transformer with a slight increase in
landscape of the transformer, an issue that we describe the later stages of the training. It shows that entropy
and study below. collapse as introduced in Zhai et al. (2023) may be
benign in this scenario.
Existing solutions. Recent studies have demon-
TounderstandthisfailureofσReparam,itcanbeuseful
strated that the loss landscape of transformers is
to recall how Eq. (5) was derived. Zhai et al. (2023)
sharpercomparedtootherresidualarchitectures(Chen
departed from a tight lower bound on the attention
et al., 2022; Zhai et al., 2023). This may explain train-
entropy and showed that it increases exponentially fast
ing instability and subpar performance of transformers,
when ∥W W⊤∥ is minimized (Zhai et al., 2023, see
especially when trained on small-scale datasets. The Q K 2
Theorem 3.1). Eq. (5) was proposed as a simple way
sharpness of transformers was observed and quantified
to minimize this quantity. In the case of channel-wise
differently: while Chen et al. (2022) computes λ ,
max attention, however, it can be shown that this has a
the largest eigenvalue of the loss function’s Hessian,
detrimental effect on the rank of the attention matrix,
Zhai et al. (2023) gauges the entropy of the attention
whichwouldconsequentlyexcludecertainfeaturesfrom
matrix to demonstrate its collapse with high sharpness.
being considered by the attention mechanism. We
Both these metrics are evaluated, and their results are formalizethisintuitioninthefollowingProposition2.2,
illustrated in Figure 3b. This visualization confirms where we consider the nuclear norm, a sum of the
our hypothesis, revealing both detrimental phenomena singular values, as a smooth proxy of the algebraic
at once. On the one hand, the sharpness of the trans- rank, which is a common practice (Daneshmand et al.,
formerwithfixedattentionisordersofmagnitudelower 2020; Dong et al., 2021). The proof is deferred to
than the sharpness of the transformer that converges Appendix E.3.
to the identity attention matrix. On the other hand,
the entropy of the transformer’s attention matrix is
Proposition 2.2 (Upper bound on the nuclear
dropping sharply along the epochs when compared to
norm). Let X∈RD×L be an input sequence. As-
the initialization.
suming W W⊤ =W W⊤ ≽0, we have
Q K K Q
To identify an appropriate solution allowing a better
generalization performance and training stability, we ∥XW W⊤X⊤∥ ≤∥W W⊤∥ ∥X∥2.
Q K ∗ Q K 2 F
explore both remedies proposed by Chen et al. (2022)
and Zhai et al. (2023). The first approach involves
utilizing the recently proposed sharpness-aware mini- Note that the assumption made above holds when
mization framework (Foret et al., 2021) which replaces W Q=W K and has been previously studied by Kim
the training objective L of Eq. (1) by et al. (2021a). The theorem confirms that employing
train
σReparamtodecrease∥W W⊤∥ reducesthenuclear
LS trA aiM n(ω)= max L train(ω+ϵ), norm of the numerator of Q attenK tio2 n matrix defined by
∥ϵ∥<ρ
Eq. (4). While the direct link between matrix rank
where ρ>0 is an hyper-parameter (see Remark D.1 of and this nuclear norm does not always hold, nuclear
Appendix D), and ω are the parameters of the model. norm regularization is commonly used to encourage a
More details on SAM can be found in Appendix D.2 low-rank structure in compressed sensing (Cand`es and
Recht, 2012; Recht, 2011; Recht et al., 2010).
The second approach involves reparameterizing all
weight matrices with spectral normalization and an ad- Although Proposition 2.2 cannot be directly applied to
ditional learned scalar, a technique termed σReparam the attention matrix A(X), we point out that in the
by Zhai et al. (2023). More formally, we replace each extreme case when σReparam leads to the attention
weight matrix W as follows scoresXW W⊤X⊤toberank-1withidenticalrowsas
Q K
studiedin(Anagnostidisetal.,2022),thattheattention
γ
W(cid:99) = W, (5) matrix stays rank-1 after application of the row-wise
∥W∥
2 softmax. Thus, σReparam may induce a collapse of
where γ ∈R is a learnable parameter initialized at 1. theattentionrankthatweempiricallyobserveinterms
of nuclear norm in Figure 7.
The results depicted in Figure 1 highlight our trans-
former’s successful convergence to the desired solu- With these findings, we present a new simple trans-
tion. Surprisingly, this is only achieved with SAM, as former model with high performance and training sta-
σReparam doesn’t manage to approach the optimal bility for multivariate time series forecasting.Romain Ilbert, Ambroise Odonnat et al.
Epoch 0 Epoch 1 Epoch 15 Epoch 35 1012 1.94
0.8 108
0.06
104
0.5
101 0.01
0 50 110
0.2 Training Epochs
Transformer Transformer + SAM Random Transformer
(a) Attention matrices of Transformer along the training. (b) Sharpness at end of training, Entropy collapse.
Figure 3: Transformer’s loss landscape analysis for linear regression. (a) The attention matrices of
Transformer get stuck to identity from the first epoch. (b, left) Transformer converges to sharper minimum
thanTransformer+SAMwithmuchlargerλ (∼×104),whileRandom Transformerhasasmoothlosslandscape.
max
(b, right) Transformer suffers from entropy collapse during training confirming the high sharpness of its loss
landscape.
2.4 SAMformer: Putting It All Together multivariate long-term time series forecasting on com-
mon benchmarks. We show that SAMformer surpasses
The proposed SAMformer is based on Eq. (3) with thecurrentmultivariatestate-of-the-artTSMixer(Chen
two important modifications. First, we equip it with etal.,2023)by14.33%whilehaving∼4timesfewerpa-
Reversible Instance Normalization (RevIN, Kim et al. rameters. All the implementation details are provided
(2021b))appliedtoXasthistechniquewasshowntobe in Appendix A.1.
efficient in handling the shift between the training and
testingdataintimeseries. Second,assuggestedbyour
Datasets. Weconductourexperimentson8publicly
explorations above, we optimize the model with SAM
available datasets of real-world multivariate time se-
to make it converge to flatter local minima. Overall,
ries, commonly used for long-term forecasting (Chen
thisgivesashallowtransformermodelwithoneencoder
et al., 2023; Nie et al., 2023; Wu et al., 2021; Zeng
as represented in Figure 4.
etal.,2023): the4ElectricityTransformerTemperature
We highlight that SAMformer keeps datasets ETTh1, ETTh2, ETTm1 and ETTm2 Zhou
Output
the channel-wise attention repre- et al. (2021), Electricity (UCI, nd), Exchange (Lai
sentedbyamatrixD×DasinEq.(3), RevIN−1 et al., 2018b), Traffic (California Department of Trans-
contrary to spatial (or temporal) at- portation, nd), and Weather (Max Planck Institute,
Linear
tention given by L×L matrix used nd) datasets. All time series are segmented with
in other models. This brings two im- input length L = 512, prediction horizons H ∈
Add
portantbenefits: (i)itensuresfeature Channel-Wise {96,192,336,720}, and a stride of 1, meaning that
permutation invariance, eliminating Self-Attention eachsubsequentwindowisshiftedbyonestep. Amore
theneedforpositionalencoding,com- detailed description of the datasets and time series
monly preceding the attention layer; RevIN preparation can be found in Appendix A.2.
(ii) it leads to a reduced time and
Input
memory complexity as D ≤ L in Baselines. We compare SAMformer with several
most of the real-world datasets. Our Figure 4: SAM - other methods including the earlier presented
channel-wise attention examines the former Transformer and the current state-of-the-art multi-
average impact of each feature on the others through- variate baseline, TSMixer (Chen et al., 2023), en-
out all timesteps. An ablation study, detailed in Ap- tirely built on MLPs. For a fair comparison, we in-
pendix C.4, validates the effectiveness of this imple- clude TSMixer’s performance when trained with SAM,
mentation. along with results reported by Nie et al. (2023) and
Chen et al. (2023) for other recent SOTA multivari-
We are prepared to evaluate SAMformer on common
ate transformer-based models: Informer (Zhou et al.,
multivariatetimeseriesforecastingbenchmarks,demon-
2021),Autoformer(Wuetal.,2021),FEDformer(Zhou
strating its superior performance.
et al., 2022), Pyraformer (Liu et al., 2022), and
LogTrans (Li et al., 2019). Additionally, we also incor-
3 Experiments
porate RevIN into other models for a more equitable
comparison between SAMformer and its competitors.
In this section, we empirically demonstrate the quan- Further and more detailed information on these base-
titative and qualitative superiority of SAMformer in lines can be found in Appendix A.3.
xam
ssenprahS
yportnE
noitnettAUnlocking the Potential of Transformers
ETTh1 Exchange ETTh1 Exchange
1013 1013
0.57 0.38
1011 1011
0.46 0.25
109 109
96 192 336 720 96 192 336 720 0.35 0.13
Prediction Horizon H Prediction Horizon H Transformer SAMformer Transformer SAMformer
Transformer SAMformer Transformer SAMformer
(a) Sharpness of SAMformer and Transformer. (b)Performanceacrossrunsof SAMformerandTransformer.
Figure5: (a)SAMformerhasasmootherlosslandscapethanTransformer. (b)SAMformerconsistentlygeneralize
well for every initialization while Transformer is unstable and heavily depends on the seed.
Transformer Reparam SAMformer
0.8
0.5
0.2
Figure 6: Attention matrices on Weather dataset.
Figure 7: Nuclear norm of the attention matrix
SAMformer preserves self-correlation among features
for different models: σReparam induces lower nu-
whileσReparamdegradestherank,hinderingtheprop-
clear norm in accordance with Proposition 2.2, while
agation of information.
SAMformer keeps the expressiveness of the attention
over Transformer.
AllmodelsaretrainedtominimizetheMSElossdefined
in Eq. (1). The average MSE on the test set, together
with the standard deviation over 5 runs with differ- Smoother loss landscape. The introduction of
ent seeds is reported. Additional details and results, SAM in the training of SAMformer makes its loss
including the Mean Absolute Error (MAE), can be smoother than that of Transformer. We illustrate
found in Appendix B.1. Except specified otherwise, all this in Figure 5a by comparing the values of λ max
our results are also obtained over 5 runs with different for Transformer and SAMformer after training on
seeds. ETTh1 and Exchange. Our observations reveal that
Transformer exhibits considerably higher sharpness,
while SAMformer has a desired behavior with a loss
3.1 Main Takeaways
landscape sharpness that is an order of magnitude
smaller.
SAMformer improves over state-of-the-art. The
experimental results are detailed in Table 1, with a
Student’s t-test analysis available in Appendix Table 6. Improved robustness. SAMformerdemonstratesro-
SAMformer significantly outperforms its competitors bustness against random initialization. Figure 5b il-
on 7 out of 8 datasets. In particular, it improves lustrates the test MSE distribution of SAMformer and
overitsbestcompetitorTSMixer+SAMby5.25%,sur- Transformeracross5differentseedsonETTh1andEx-
passesthestandaloneTSMixerby14.33%andthebest changewithapredictionhorizonofH =96. SAMformer
transformer-basedmodelFEDformerby12.36%. Inad- consistently maintains performance stability across dif-
dition, it improves over Transformer by 16.96%. For ferent seed choices, while Transformer exhibits signifi-
each dataset and horizon, SAMformer is ranked either cant variance and, thus, a high dependency on weight
first or second. Notably, SAM’s integration improves initialization. Thisobservationholdsacrossalldatasets
the generalization capacity of TSMixer, resulting in an and prediction horizons as shown in Appendix B.4.
average enhancement of 9.58%. A similar study with
the MAE in Table 5 leads to the same conclusions. As 3.2 Qualitative Benefits of Our Approach
TSMixer trained with SAM is the second-best baseline,
it serves as a primary benchmark for further discussion Computational efficiency. SAMformer is compu-
in this section. tationally more efficient than TSMixer and usual
xam
ssenprahS ESM
tseTRomain Ilbert, Ambroise Odonnat et al.
Table 1: Performance comparison between our model (SAMformer) and baselines for multivariate long-term
forecasting with different horizons H. Results marked with “∗” are obtained from Chen et al. (2023) and those
marked with “†” are obtained from Nie et al. (2023). Transformer-based models are abbreviated by removing the
“former” part of their name. We display the average test MSE with standard deviation obtained on 5 runs with
different seeds. Best results are in bold, second best are underlined.
withSAM withoutSAM
Dataset H
SAMformer TSMixer Transformer TSMixer In∗ Auto∗ FED∗ Pyra† LogTrans†
96 0.381 0.388 0.509 0.398 0.941 0.435 0.376 0.664 0.878
±0.003 ±0.001 ±0.031 ±0.001
192 0.409 0.421 0.535 0.426 1.007 0.456 0.423 0.790 1.037
±0.002 ±0.002 ±0.043 ±0.003
336 0.423 0.430 0.570 0.435 1.038 0.486 0.444 0.891 1.238
±0.001 ±0.002 ±0.016 ±0.003
720 0.427 0.440 0.601 0.498 1.144 0.515 0.469 0.963 1.135
±0.002 ±0.005 ±0.036 ±0.076
96 0.295 0.305 0.396 0.308 1.549 0.332 0.332 0.645 2.116
±0.002 ±0.007 ±0.017 ±0.003
192 0.340 0.350 0.413 0.352 3.792 0.426 0.407 0.788 4.315
±0.002 ±0.002 ±0.010 ±0.004
336 0.350 0.360 0.414 0.360 4.215 0.477 0.400 0.907 1.124
±0.000 ±0.002 ±0.002 ±0.002
720 0.391 0.402 0.424 0.409 3.656 0.453 0.412 0.963 3.188
±0.001 ±0.002 ±0.009 ±0.006
96 0.329 0.327 0.384 0.336 0.626 0.510 0.326 0.543 0.600
±0.001 ±0.002 ±0.022 ±0.004
192 0.353 0.356 0.400 0.362 0.725 0.514 0.365 0.557 0.837
±0.006 ±0.004 ±0.026 ±0.006
336 0.382 0.387 0.461 0.391 1.005 0.510 0.392 0.754 1.124
±0.001 ±0.004 ±0.017 ±0.003
720 0.429 0.441 0.463 0.450 1.133 0.527 0.446 0.908 1.153
±0.000 ±0.002 ±0.046 ±0.006
96 0.181 0.190 0.200 0.211 0.355 0.205 0.180 0.435 0.768
±0.005 ±0.003 ±0.036 ±0.014
192 0.233 0.250 0.273 0.252 0.595 0.278 0.252 0.730 0.989
±0.002 ±0.002 ±0.013 ±0.005
336 0.285 0.301 0.310 0.303 1.270 0.343 0.324 1.201 1.334
±0.001 ±0.003 ±0.022 ±0.004
720 0.375 0.389 0.426 0.390 3.001 0.414 0.410 3.625 3.048
±0.001 ±0.002 ±0.025 ±0.003
96 0.155 0.171 0.182 0.173 0.304 0.196 0.186 0.386 0.258 ±0.002 ±0.001 ±0.006 ±0.004
192 0.168 0.191 0.202 0.204 0.327 0.211 0.197 0.386 0.266
±0.001 ±0.010 ±0.041 ±0.027
336 0.183 0.198 0.212 0.217 0.333 0.214 0.213 0.378 0.280
±0.000 ±0.006 ±0.017 ±0.018
720 0.219 0.230 0.238 0.242 0.351 0.236 0.233 0.376 0.283
±0.000 ±0.005 ±0.016 ±0.015
96 0.161 0.233 0.292 0.343 0.847 0.197 0.139 - 0.968 ±0.007 ±0.016 ±0.045 ±0.082
192 0.246 0.342 0.372 0.342 1.204 0.300 0.256 - 1.040
±0.009 ±0.031 ±0.035 ±0.031
336 0.368 0.474 0.494 0.484 1.672 0.509 0.426 - 1.659
±0.006 ±0.014 ±0.033 ±0.062
720 1.003 1.078 1.323 1.204 2.478 1.447 1.090 - 1.941
±0.018 ±0.179 ±0.192 ±0.028
96 0.407 0.409 0.420 0.409 0.733 0.597 0.576 2.085 0.684
±0.001 ±0.016 ±0.041 ±0.016
192 0.415 0.433 0.441 0.637 0.777 0.607 0.610 0.867 0.685
±0.005 ±0.009 ±0.039 ±0.444
336 0.421 0.424 0.501 0.747 0.776 0.623 0.608 0.869 0.734
±0.001 ±0.000 ±0.154 ±0.277
720 0.456 0.488 0.468 0.688 0.827 0.639 0.621 0.881 0.717
±0.003 ±0.028 ±0.021 ±0.287
96 0.197 0.189 0.227 0.214 0.354 0.249 0.238 0.896 0.458
±0.001 ±0.003 ±0.012 ±0.004
192 0.235 0.228 0.256 0.231 0.419 0.325 0.275 0.622 0.658
±0.000 ±0.004 ±0.018 ±0.003
336 0.276 0.271 0.278 0.279 0.583 0.351 0.339 0.739 0.797
±0.001 ±0.001 ±0.001 ±0.007
720 0.334 0.331 0.353 0.343 0.916 0.415 0.389 1.004 0.869
±0.000 ±0.001 ±0.002 ±0.024
Overall MSE improvement 5.25% 16.96% 14.33% 72.20% 22.65% 12.36% 61.88% 70.88%
transformer-based approaches, benefiting from a shal- Fewer hyperparameters and versatility.
lowlightweightimplementation,i.e.,asinglelayerwith SAMformer requires minimal hyperparameters
one attention head. The number of parameters of tuning, contrary to other baselines, including TSMixer
SAMformer and TSMixer is detailed in Appendix Ta- and FEDformer. In particular, SAMformer’s architec-
ble7. Weobservethat,onaverage,SAMformerhas∼4 ture remains the same for all our experiments (see
times fewer parameters than TSMixer, which makes Appendix A.1 for details), while TSMixer varies in
this approach even more remarkable. Importantly, terms of the number of residual blocks and feature
TSMixer itself is recognized as a computationally effi- embedding dimensions, depending on the dataset.
cient architecture compared to the transformer-based This versatility also comes with better robustness
baselines (Chen et al., 2023, Table 6). to the prediction horizon H. In Appendix C.1
Figure 13, we display the evolution forecasting
accuracy on all datasets for H ∈ {96,192,336,720}
1hTTE
2hTTE
1mTTE
2mTTE
yticirtcelE
egnahcxE
cffiarT
rehtaeWUnlocking the Potential of Transformers
for SAMformer and TSMixer (trained with SAM). We sharpness does not change with respect to ρ, given the
observe that SAMformer consistently outperforms its constant nature of the loss function’s Hessian. Con-
best competitor TSMixer (trained with SAM) for all sequently, TSMixer benefits less from changes in ρ
horizons. than SAMformer. Our observations consistently show
that a sufficiently large ρ, generally above 0.7 enables
Better attention. We display the attention ma- SAMformer to achieve lower MSE than TSMixer.
trices after training on Weather with the prediction
horizon H = 96 for Transformer, SAMformer and
Transformer + σReparam in Figure 6. We note that
SAM vs σReparam. We mentioned previously
Transformerexcludesself-correlationbetweenfeatures,
that σReparam doesn’t improve the performance of
having low values on the diagonal, while SAMformer
a transformer on a simple toy example, although it
strongly promotes them. This pattern is reminiscent
makes it comparable to the performance of a trans-
of He et al. (2023) and Trockman and Kolter (2023):
former with fixed random attention. To further
both works demonstrated the importance of diagonal
show that σReparam doesn’t provide an improvement
patterns in attention matrices for signal propagation
on real-world datasets, we show in Figure 8a that
intransformersusedinNLPandcomputervision. Our
on ETTh1 and Exchange, σReparam alone fails to
experiments reveal that these insights also apply to
match SAMformer’s improvements, even underperform-
time-seriesforecasting. Notethatfreezingtheattention
ing Transformer in some cases. A potential improve-
to A(X)=I is largely outperformed by SAMformer
D ment may come from combining SAM and σReparam
as shown in Table 9, Appendix C.4, which confirms
to smooth a rather sparse matrix obtained with SAM.
the importance of learnable attention. The attention
However,asFigure8billustrates,thiscombinationdoes
matrix given by σReparam at Figure 6 has almost
not surpass the performance of using SAM alone. Fur-
equal rows, leading to rank collapse. In Figure 7, we
thermore, combining SAM and σReparam significantly
display the distributions of nuclear norms of attention
increases training time and memory usage, especially
matrices after training Transformer, SAMformer and
for larger datasets and longer horizons (see Appendix
σReparam. We observe that σReparam heavily penal-
Figure 11), indicating its inefficiency as a method.
izesthenuclearnormsoftheattentionmatrix,whichis
coherent with Proposition 2.2. In contrast, SAMformer
maintains it above Transformer, thus improving the
expressiveness of attention. 4 Discussion and Future Work
3.3 Ablation Study and Sensitivity Analysis
In this work, we demonstrated how simple transform-
Choices of implementation. We empirically com- ers can reclaim their place as state-of-the-art models
pared our architecture, which is channel-wise attention in long-term multivariate series forecasting from their
(Eq. (3)), with temporal-wise attention. Table 8 of MLP-based competitors. Rather than concentrating
Appendix C.4 shows the superiority of our approach in on new architectures and attention mechanisms, we
the considered setting. We conducted our experiments analyzed the current pitfalls of transformers in this
with Adam (Kingma and Ba, 2015), the de-facto opti- task and addressed them by carefully designing an ap-
mizers for transformers (Ahn et al., 2023; Chen et al., propriate training strategy. Our findings suggest that
2022; Pan and Li, 2022; Zhou et al., 2021, 2022). We even a simple shallow transformer has a very sharp
provide an in-depth ablation study in Appendix C.3 loss landscape which makes it converge to poor local
that motivates this choice. As expected (Ahn et al., minima. We analyzed popular solutions proposed in
2023; Liu et al., 2020; Pan and Li, 2022; Zhang et al., the literature to address this issue and showed which
2020), SGD (Nesterov, 1983) fails to converge and of them work or fail. Our proposed SAMformer, opti-
AdamW (Loshchilov and Hutter, 2019) leads to similar mized with sharpness-aware minimization, leads to a
performance but is very sensitive to the choice of the substantial performance gain compared to the existing
weight decay strength. forecastingbaselinesandbenefitsfromahighversatility
and robustness across datasets and prediction horizons.
Sensitivity to the neighborhood size ρ. The test Finally, we also showed that channel-wise attention
MSE of SAMformer and TSMixer is depicted in Fig- in time series forecasting can be more efficient – both
ure 14 of Appendix C.2 as a function of the neigh- computationallyandperformance-wise–thantemporal
borhood size ρ. It appears that TSMixer, with its attention commonly used previously. We believe that
quasi-linear architecture, exhibits less sensitivity to this surprising finding may spur many further works
ρ compared to SAMformer. This behavior is consis- building on top of our simple architecture to improve
tent with the understanding that, in linear models, the it even further.Romain Ilbert, Ambroise Odonnat et al.
ETTh1 Exchange ETTh1 Exchange
0.63 1.46 0.43 1.0
0.51 0.82 0.41 0.59
0.39 0.19 0.38 0.18
96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720
Prediction Horizon H Prediction Horizon H Prediction Horizon H Prediction Horizon H
Reparam Transformer SAMformer SAMformer SAMformer + Reparam
(a)Comparisonof Transformer,σReparamandSAMformer.(b) Comparison of SAMformer and SAMformer+σReparam.
Figure 8: Suboptimality of σReparam. (a) σReparam alone does not bring improvement on Transformer
and is clearly outperformed by SAMformer. Combining σReparam with SAMformer does not bring significant
improvement but heavily increases the training time (see Figure 11).
References and Zecchina, R. (2017). Entropy-SGD: Biasing
gradient descent into wide valleys. In International
Ahn, K., Cheng, X., Song, M., Yun, C., Jadbabaie, A.,
Conference on Learning Representations.
and Sra, S. (2023). Linear attention is (maybe) all
Chen, R. and Tao, M. (2021). Data-driven prediction
you need (to understand transformer optimization).
ofgeneralhamiltoniandynamicsvialearningexactly-
Anagnostidis, S., Biggio, L., Noci, L., Orvieto, A.,
symplecticmaps.InMeila,M.andZhang,T.,editors,
Singh, S. P., and Lucchi, A. (2022). Signal propaga-
Proceedings of the 38th International Conference on
tion in transformers: Theoretical perspectives and
Machine Learning, volume 139 of Proceedings of Ma-
the role of rank collapse. In Oh, A. H., Agarwal,
chine Learning Research, pages 1717–1727. PMLR.
A., Belgrave, D., and Cho, K., editors, Advances in
Chen, S.-A., Li, C.-L., Arik, S. O., Yoder, N. C., and
Neural Information Processing Systems.
Pfister, T. (2023). TSMixer: An all-MLP architec-
Box, G. E. P. and Jenkins, G. (1990). Time Series ture for time series forecasting. Transactions on
Analysis, Forecasting and Control. Holden-Day, Inc., Machine Learning Research.
USA.
Chen, X., Hsieh, C.-J., and Gong, B. (2022). When
Box, G. E. P., Jenkins, G. M., and MacGregor, J. F. vision transformers outperform resnets without pre-
(1974). Some Recent Advances in Forecasting and training or strong data augmentations. In Interna-
Control. Journal of the Royal Statistical Society tional Conference on Learning Representations.
Series C, 23(2):158–179.
Cirstea, R.-G., Guo, C., Yang, B., Kieu, T., Dong, X.,
CaliforniaDepartmentofTransportation(n.d.). Traffic and Pan, S. (2022). Triformer: Triangular, variable-
dataset. specificattentionsforlongsequencemultivariatetime
seriesforecasting.InRaedt,L.D.,editor,Proceedings
Cand`es, E. and Recht, B. (2012). Exact matrix com-
oftheThirty-FirstInternationalJointConferenceon
pletion via convex optimization. Commun. ACM,
Artificial Intelligence, IJCAI-22, pages 1994–2001.
55(6):111–119.
International Joint Conferences on Artificial Intelli-
Caron, M., Touvron, H., Misra, I., J´egou, H., Mairal,
gence Organization. Main Track.
J., Bojanowski, P., and Joulin, A. (2021). Emerg-
Daneshmand, H., Kohler, J., Bach, F., Hofmann, T.,
ing properties in self-supervised vision transformers.
and Lucchi, A. (2020). Batch normalization prov-
In Proceedings of the International Conference on
ably avoids rank collapse for randomly initialised
Computer Vision (ICCV).
deep networks. In Proceedings of the 34th Interna-
Casolaro, A., Capone, V., Iannuzzo, G., and Camastra, tional Conference on Neural Information Processing
F. (2023). Deep learning for time series forecasting: Systems, NIPS’20, Red Hook, NY, USA. Curran
Advances and open problems. Information, 14(11). Associates Inc.
Cˇepulionis, P. and Lukoˇseviˇciu¯te˙, K. (2016). Electro- Devlin, J., Chang, M.-W., Lee, K., and Toutanova,
cardiogram time series forecasting and optimization K. (2018). Bert: Pre-training of deep bidirectional
using ant colony optimization algorithm. Mathemat- transformers for language understanding.
ical Models in Engineering, 2(1):69–77.
Dong, Y., Cordonnier, J.-B., and Loukas, A. (2021).
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Attention is not all you need: pure attention loses
Y., Baldassi, C., Borgs, C., Chayes, J., Sagun, L., rank doubly exponentially with depth. In Meila, M.
ESM
tseT
ESM
tseTUnlocking the Potential of Transformers
andZhang,T.,editors,Proceedings of the 38th Inter- of Proceedings of Machine Learning Research, pages
national Conference on Machine Learning, volume 5562–5571. PMLR.
139 of Proceedings of Machine Learning Research,
Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and
pages 2793–2803. PMLR.
Choo, J. (2021b). Reversible instance normalization
Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn, for accurate time-series forecasting against distribu-
D., Zhai, X., Unterthiner, T., Dehghani, M., Min- tion shift. In International Conference on Learning
derer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Representations.
Houlsby, N. (2021). An image is worth 16x16 words:
Kingma, D. and Ba, J. (2015). Adam: A method for
Transformers for image recognition at scale. In In-
stochastic optimization. In International Conference
ternational Conference on Learning Representations.
onLearningRepresentations(ICLR),SanDiega,CA,
Dziugaite, G. K. and Roy, D. M. (2017). Computing USA.
nonvacuous generalization bounds for deep (stochas-
Kitaev, N., Kaiser, L., and Levskaya, A. (2020). Re-
tic) neural networks with many more parameters
former: The efficient transformer. In International
thantrainingdata.InProceedingsofthe33rdAnnual
Conference on Learning Representations.
Conference on Uncertainty in Artificial Intelligence
(UAI). Lai, G., Chang, W.-C., Yang, Y., and Liu, H. (2018a).
Modeling long- and short-term temporal patterns
Fan, C., Zhang, Y., Pan, Y., Li, X., Zhang, C., Yuan,
withdeepneuralnetworks. InThe41stInternational
R.,Wu,D.,Wang,W.,Pei,J.,andHuang,H.(2019).
ACMSIGIRConferenceonResearch&Development
Multi-horizon time series forecasting with temporal
in Information Retrieval, SIGIR ’18, page 95–104,
attention learning. In Proceedings of the 25th ACM
New York, NY, USA. Association for Computing
SIGKDD International Conference on Knowledge
Machinery.
Discovery&DataMining,KDD’19,page2527–2535,
New York, NY, USA. Association for Computing Lai, G., Chang, W.-C., Yang, Y., and Liu, H. (2018b).
Machinery. Modeling long- and short-term temporal patterns
with deep neural networks. In Association for Com-
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur,
puting Machinery, SIGIR ’18, page 95–104, New
B. (2021). Sharpness-aware minimization for effi-
York, NY, USA.
ciently improving generalization. In International
Conference on Learning Representations. Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang,
Y.-X., and Yan, X. (2019). Enhancing the locality
Glorot, X. and Bengio, Y. (2010). Understanding the
and breaking the memory bottleneck of transformer
difficulty of training deep feedforward neural net-
ontimeseriesforecasting. InWallach,H.,Larochelle,
works. In Teh, Y. W. and Titterington, M., ed-
H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and
itors, Proceedings of the Thirteenth International
Garnett,R.,editors,AdvancesinNeuralInformation
Conference on Artificial Intelligence and Statistics,
Processing Systems, volume 32. Curran Associates,
volume 9 of Proceedings of Machine Learning Re-
Inc.
search,pages249–256,ChiaLagunaResort,Sardinia,
Italy. PMLR. Liu, L., Liu, X., Gao, J., Chen, W., andHan, J.(2020).
Understandingthedifficultyoftrainingtransformers.
He, B., Martens, J., Zhang, G., Botev, A., Brock, A.,
In Proceedings of the 2020 Conference on Empirical
Smith, S. L., and Teh, Y. W. (2023). Deep trans-
Methods in Natural Language Processing (EMNLP
formers without shortcuts: Modifying self-attention
2020).
for faithful signal propagation. In The Eleventh In-
ternational Conference on Learning Representations. Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X.,
andDustdar,S.(2022). Pyraformer: Low-complexity
Horn, R. A. and Johnson, C. R. (1991). Topics in
pyramidal attention for long-range time series mod-
Matrix Analysis. Cambridge University Press.
eling and forecasting. In International Conference
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, on Learning Representations.
M., and Tang, P. T. P. (2017). On large-batch train-
Loshchilov,I.andHutter,F.(2017). SGDR:Stochastic
ing for deep learning: Generalization gap and sharp
gradientdescentwithwarmrestarts. InInternational
minima. In International Conference on Learning
Conference on Learning Representations.
Representations.
Loshchilov, I. and Hutter, F. (2019). Decoupled weight
Kim,H.,Papamakarios,G.,andMnih,A.(2021a). The
decay regularization. In International Conference on
lipschitz constant of self-attention. In Meila, M. and
Learning Representations.
Zhang, T., editors, Proceedings of the 38th Interna-
tional Conference on Machine Learning, volume 139 Max Planck Institute (n.d.). Weather dataset.Romain Ilbert, Ambroise Odonnat et al.
Nesterov, Y. (1983). A method for solving the con- Touvron, H., Cord, M., Douze, M., Massa, F., Sablay-
vex programming problem with convergence rate rolles, A., and Jegou, H. (2021). Training data-
o(1/k2). Proceedings of the USSR Academy of Sci- efficient image transformers and distillation through
ences, 269:543–547. attention. In Meila, M. and Zhang, T., editors, Pro-
ceedings of the 38th International Conference on Ma-
Nie, Y., Nguyen, N.H., Sinthong, P., andKalagnanam,
chineLearning,volume139ofProceedingsofMachine
J.(2023). Atimeseriesisworth64words: Long-term
Learning Research, pages 10347–10357. PMLR.
forecasting with transformers. In The Eleventh In-
ternational Conference on Learning Representations. Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
OpenAI (2023). Gpt-4 technical report. Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal,
N., Hambro, E., Azhar, F., Rodriguez, A., Joulin,
Pan, Y. and Li, Y. (2022). Toward understanding why
A., Grave, E., and Lample, G. (2023). Llama:
adam converges faster than SGD for transformers.
Open and efficient foundation language models. cite
In OPT 2022: Optimization for Machine Learning
arxiv:2302.13971.
(NeurIPS 2022 Workshop).
Radford, A., Narasimhan, K., Salimans, T., and Trockman, A.and Kolter, J.Z.(2023). Mimetic initial-
Sutskever,I.(2018). Improvinglanguageunderstand- ization of self-attention layers. In Proceedings of the
ing by generative pre-training. 2018 OpenAI Tech 40th International Conference on Machine Learning,
Report. ICML’23. JMLR.org.
Rangapuram,S.S.,Seeger,M.W.,Gasthaus,J.,Stella, UCI (n.d.). Electricity dataset.
L., Wang, Y., and Januschowski, T. (2018). Deep
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
state space models for time series forecasting. In
Jones, L., Gomez, A. N., Kaiser, L. u., and Polo-
Bengio,S.,Wallach,H.,Larochelle,H.,Grauman,K.,
sukhin,I.(2017). Attentionisallyouneed. InGuyon,
Cesa-Bianchi,N.,andGarnett,R.,editors,Advances
I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus,
inNeuralInformationProcessingSystems,volume31.
R., Vishwanathan, S., and Garnett, R., editors, Ad-
Curran Associates, Inc.
vances in Neural Information Processing Systems,
Recht, B. (2011). A simpler approach to matrix com- volume 30. Curran Associates, Inc.
pletion. J. Mach. Learn. Res., 12(null):3413–3430.
Wu, H., Xu, J., Wang, J., and Long, M. (2021). Aut-
Recht,B.,Fazel,M.,andParrilo,P.A.(2010). Guaran-
oformer: Decomposition transformers with Auto-
teed minimum-rank solutions of linear matrix equa-
Correlation for long-term series forecasting. In Ad-
tions via nuclear norm minimization. SIAM Review,
vances in Neural Information Processing Systems.
52(3):471–501.
Zamir, S. W., Arora, A., Khan, S., Hayat, M., Khan,
Salinas, D., Flunkert, V., Gasthaus, J., and
F. S., and Yang, M.-H. (2022). Restormer: Efficient
Januschowski, T. (2020). Deepar: Probabilistic fore-
transformer for high-resolution image restoration. In
casting with autoregressive recurrent networks. In-
CVPR.
ternational Journal of Forecasting, 36(3):1181–1191.
Zeng, A., Chen, M., Zhang, L., andXu, Q.(2023). Are
Sen, R., Yu, H.-F., and Dhillon, I. (2019). Think glob-
transformers effective for time series forecasting? In
ally, act locally: a deep neural network approach to
Proceedings of the AAAI Conference on Artificial
high-dimensional time series forecasting. In Proceed-
Intelligence.
ings of the 33rd International Conference on Neu-
ral Information Processing Systems, Red Hook, NY, Zhai,S.,Likhomanenko,T.,Littwin,E.,Busbridge,D.,
USA. Curran Associates Inc. Ramapuram, J., Zhang, Y., Gu, J., and Susskind,
Sonkavde, G., Dharrao, D. S., Bongale, A. M., De- J. M. (2023). Stabilizing transformer training by
okate, S. T., Doreswamy, D., and Bhat, S. K. (2023). preventing attention entropy collapse. In Krause, A.,
Forecasting stock market prices using machine learn- Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
ing and deep learning models: A systematic review, and Scarlett, J., editors, Proceedings of the 40th In-
performance analysis and discussion of implications. ternationalConferenceonMachineLearning,volume
International Journal of Financial Studies, 11(3). 202 of Proceedings of Machine Learning Research,
pages 40770–40803. PMLR.
Sorjamaa, A., Hao, J., Reyhani, N., Ji, Y., and
Lendasse, A. (2007). Methodology for long-term pre- Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Lin, H., Zhang,
diction of time series. Neurocomputing, 70(16):2861– Z., Sun, Y., He, T., Mueller, J., Manmatha, R., Li,
2869. Neural Network Applications in Electrical M., and Smola, A. (2022). Resnest: Split-attention
Engineering Selected papers from the 3rd Interna- networks. In Proceedings of the IEEE/CVF Confer-
tional Work-Conference on Artificial Neural Net- ence on Computer Vision and Pattern Recognition
works (IWANN 2005). (CVPR) Workshops, pages 2736–2746.Unlocking the Potential of Transformers
Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi,
S., Kumar, S., and Sra, S. (2020). Why are adaptive
methods good for attention models? In Larochelle,
H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H., editors, Advances in Neural Information Process-
ing Systems, volume 33, pages 15383–15393. Curran
Associates, Inc.
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong,
H.,andZhang,W.(2021). Informer: Beyondefficient
transformer for long sequence time-series forecasting.
In The Thirty-Fifth AAAI Conference on Artificial
Intelligence, AAAI 2021, Virtual Conference, vol-
ume 35, pages 11106–11115. AAAI Press.
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin,
R. (2022). FEDformer: Frequency enhanced decom-
posed transformer for long-term series forecasting.
In Proc. 39th International Conference on Machine
Learning (ICML 2022).Romain Ilbert, Ambroise Odonnat et al.
Unlocking the Potential of Transformers: Supplementary Materials
Roadmap. Inthisappendix,weprovidethedetailedexperimentalsetupinSectionA,additionalexperimentsin
Section B, and a thorough ablation study and sensitivity analysis in Section C. Additional background knowledge
is available in Section D and proofs of the main theoretical results are provided in Section E. We display below
the corresponding table of contents.
Table of Contents
A Experimental Setup 14
A.1 Architecture and Training Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.3 More Details on the Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B Additional Experiments 16
B.1 MAE Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.2 Significance Test for SAMformer and TSMixer with SAM . . . . . . . . . . . . . . . . . . . . . . . 16
B.3 Computational Efficiency of SAMformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.4 Strong Generalization Regardless of the Initialization . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.5 Faithful Signal Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C Ablation Study and Sensitivity Analysis 19
C.1 Sensitivity to the Prediction Horizon H. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.2 Sensitivity to the Neighborhood Size ρ.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.3 Sensitivity to the Change of the Optimizer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.4 Ablation on the Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D Additional Background 22
D.1 Reversible Instance Normalization: RevIN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.2 Sharpness-aware minimization (SAM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E Proofs 25
E.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 Proof of Proposition 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.3 Proof of Proposition 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
E.4 Proof of Proposition D.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.5 Matrix formulation of Yˆ in Eq. (11) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29Unlocking the Potential of Transformers
A Experimental Setup
A.1 Architecture and Training Parameters
Architecture. We follow Chen et al. (2023); Nie et al. (2023), and to ensure a fair comparison of baselines, we
applythereversibleinstancenormalization(RevIN)ofKimetal.(2021b)(seeAppendixD.1formoredetails). The
network used in SAMformer and Transformer is a simplified one-layer transformer with one head of attention and
without feed-forward. Its neural network function follows Eq. (3), while RevIN normalization and denormalization
are applied respectively before and after the neural network function, see Figure 4. We display the inference step
of SAMformer in great details in Algorithm 1. For the sake of clarity, we describe the application of the neural
network function sequentially on each element of the batches but in practice, the operations are parallelized
and performed batch per batch. For SAMformer and Transformer, the dimension of the model is d =16 and
m
remains the same in all our experiments. For TSMixer, we used the official implementation than can be found
on Github. For all of our experiments, we train our baselines (SAMformer, Transformer, TSMixer with SAM,
TSMixer without SAM) with the Adam optimizer (Kingma and Ba, 2015), a batch size of 32, a cosine annealing
scheduler (Loshchilov and Hutter, 2017) and the learning rates summarized in Table 2.
Training parameters. For SAMformer and TSMixer trained with SAM, the values of neighborhood size ρ∗
used are reported in Table 3. The training/validation/test split is 12/4/4 months on the ETT datasets and
70%/20%/10% on the other datasets. We use a look-back window L=512 and use a sliding window with stride
1 to create the sequences. The training loss is the MSE on the multivariate time series (Eq. (1)). Training is
performed during 300 epochs and we use early stopping with a patience of 5 epochs. For each dataset, baselines,
andpredictionhorizonH ∈{96,192,336,720}, eachexperimentisrun5timeswithdifferentseeds, andwedisplay
the average and the standard deviation of the test MSE and MAE over the 5 trials.
Table 2: Learning rates used in our experiments.
Dataset ETTh1/ETTh2 ETTm1/ETTm2 Electricity Exchange Traffic Weather
Learningrate 0.01 0.01 0.0001 0.001 0.0001 0.0001
Table3: Neighborhoodsizeρ∗ atwhichSAMformerandTSMixerachievetheirbestperformanceonthebenchmarks
described in Table 4.
H Model ETTh1 ETTh2 ETTm1 ETTm2 Electricity Exchange Traffic Weather
SAMformer 0.5 0.5 0.6 0.2 0.5 0.7 0.8 0.4
96
TSMixer 1.0 0.9 1.0 1.0 0.9 1.0 0.0 0.5
SAMformer 0.6 0.8 0.9 0.9 0.6 0.8 0.1 0.4
192
TSMixer 0.7 0.1 0.6 1.0 1.0 0.0 0.9 0.4
SAMformer 0.9 0.6 0.9 0.8 0.5 0.5 0.5 0.6
336
TSMixer 0.7 0.0 0.7 1.0 0.4 1.0 0.6 0.6
SAMformer 0.9 0.8 0.9 0.9 1.0 0.9 0.7 0.5
720
TSMixer 0.3 0.4 0.5 1.0 0.9 0.1 0.9 0.3
A.2 Datasets
We conduct our experiments on 8 publicly available datasets of real-world time series, widely used for multivariate
long-term forecasting (Chen et al., 2023; Nie et al., 2023; Wu et al., 2021). The 4 Electricity Transformer
Temperature datasets ETTm1, ETTm2, ETTh1, and ETTh2 (Zhou et al., 2021) contain the time series collected
by electricity transformers from July 2016 to July 2018. Whenever possible, we refer to this set of 4 datasets
as ETT. Electricity (UCI, nd) contains the time series of electricity consumption from 321 clients from 2012 to
2014. Exchange (Lai et al., 2018b) contains the time series of daily exchange rates between 8 countries from
1990 to 2016. Traffic (California Department of Transportation, nd) contains the time series of road occupancy
rates captured by 862 sensors from January 2015 to December 2016. Last but not least, Weather (Max Planck
Institute, nd) contains the time series of meteorological information recorded by 21 weather indicators in 2020.
It should be noted that Electricity, Traffic, and Weather are large-scale datasets. The ETT datasets can be
downloaded here while the 4 other datasets can be downloaded here. Table 4 sums up the characteristics of the
datasets used in our experiments.Romain Ilbert, Ambroise Odonnat et al.
Table 4: Characteristics of the multivariate time series datasets used in our experiments.
Dataset ETTh1/ETTh2 ETTm1/ETTm2 Electricity Exchange Traffic Weather
#features 7 7 321 9 862 21
#timesteps 17420 699680 26304 7588 17544 52696
Granularity 1hour 15minutes 1hour 1day 1hour 10minutes
Algorithm 1: Architecture of the network used in SAMformer and Transformer
Parameters: Batch size bs, input length L, prediction horizon H, dimension of the model d .
m
Network trainable parameters: W
Q
∈RL×dm,W
K
∈RL×dm, W
V
∈RL×dm, W
O
∈Rdm×L, W∈RL×H.
RevIN trainable parameters: β,γ.
Input: Batch of bs input sequences X∈RD×L arranged in a tensor B of dimension bs×L×D.
in
RevIN normalization: X←X˜ following Eq. (7). The output is a tensor B˜ of dimension bs×L×D.
in
Transposition of the batch: B˜ is reshaped in dimension bs×D×L.
in
Applying the neural network of Eq. (3):
for each X˜ ∈B˜ do
in
1. Attention layer
Rescale the input with the attention matrix (Eq. (4)).
The output A(X˜)X˜W W is of dimension D×L
V O
2. Skip connection
Sum the input X˜ and the output of the attention layer.
The output X˜ +A(X˜)X˜W W is of dimension D×L.
V O
3. Linear layer
Apply a linear layer on the output of the skip connection.
(cid:104) (cid:105)
The output Y˜ = X˜ +A(X˜)X˜W W W is of dimension D×H.
V O
Unnormalized predictions are arranged in a tensor B˜ of dimension bs×D×H.
out
end
Transposition of the batch: B˜ is reshaped in dimension bs×H ×D.
out
RevIN denormalization: Y˜ ←Yˆ following Eq. (8).
Output: Batch of bs prediction sequences Yˆ ∈RD×H arranged in a tensor Bˆ of dimension bs×H ×D.
out
A.3 More Details on the Baselines
As stated above, we conducted all our experiments with a look-back window L=512 and prediction horizons
H ∈{96,192,336,720}. Results reported in Table 1 from SAMformer, TSMixer, and Transformer come from
our own experiments, conducted over 5 runs with 5 different seeds. The reader might notice that the results
of TSMixer without SAM slightly differ from the ones reported in the original paper (Chen et al., 2023). It
comes from the fact that the authors reported results from a single seed, while we report average performance
with standard deviation on multiple runs for a better comparison of methods. We perform a Student’s t-test in
Table 6 for a more thorough comparison of SAMformer and TSMixer with SAM. It should be noted that, unlike
our competitors including TSMixer, the architecture of SAMformer remains the same for all the datasets. This
highlights the robustness of our method and its advantage as no heavy hyperparameter tuning is required. For
a fair comparison of models, we also report results from other baselines in the literature that we did not run
ourselves. For Informer Zhou et al. (2021), Autoformer (Wu et al., 2021), and Fedformer (Zhou et al., 2022),
the results on all datasets, except Exchange, are reported from Chen et al. (2023). Similarly, for Pyraformer (Liu
et al., 2022) and LogTrans (Li et al., 2019), results on all datasets except Exchange are reported from Nie et al.
(2023). It is important to note that these two baselines were not implemented with RevIN (Kim et al., 2021b).
Results on the Exchange dataset for those 5 baselines come from the original corresponding papers and hence
refer to the models without RevIN. This approach ensures a comprehensive and comparative analysis across
various established models in multivariate long-term time series forecasting.Unlocking the Potential of Transformers
Table 5: Performance comparison between our model (SAMformer) and baselines for multivariate long-term
forecasting with different horizons H. Results marked with “∗” are obtained from Chen et al. (2023) and those
marked with “†” are obtained from Nie et al. (2023). Transformer-based models are abbreviated by removing the
“former” part of their name. We display the average test MAE with standard deviation obtained on 5 runs with
different seeds. Best results are in bold, second best are underlined.
withSAM withoutSAM
Dataset H
SAMformer TSMixer Transformer TSMixer In∗ Auto∗ FED∗ Pyra† LogTrans†
96 0.402 0.408 0.619 0.414 0.769 0.446 0.415 0.612 0.740
±0.001 ±0.001 ±0.203 ±0.004
192 0.418 0.426 0.513 0.428 0.786 0.457 0.446 0.681 0.824
±0.001 ±0.002 ±0.024 ±0.001
336 0.425 0.434 0.529 0.434 0.784 0.487 0.462 0.738 0.932
±0.000 ±0.001 ±0.008 ±0.001
720 0.449 0.459 0.553 0.506 0.857 0.517 0.492 0.782 0.852
±0.002 ±0.004 ±0.021 ±0.064
96 0.358 0.367 0.416 0.367 0.952 0.368 0.374 0.597 1.197
±0.002 ±0.002 ±0.025 ±0.003
192 0.386 0.393 0.435 0.395 1.542 0.434 0.446 0.683 1.635
±0.003 ±0.001 ±0.019 ±0.003
336 0.395 0.404 0.434 0.404 1.642 0.479 0.447 0.747 1.604
±0.002 ±0.004 ±0.014 ±0.002
720 0.428 0.435 0.448 0.441 1.619 0.490 0.469 0.783 1.540
±0.001 ±0.002 ±0.006 ±0.005
96 0.363 0.363 0.395 0.371 0.560 0.492 0.390 0.510 0.546
±0.001 ±0.001 ±0.024 ±0.002
192 0.378 0.381 0.414 0.384 0.619 0.495 0.415 0.537 0.700
±0.003 ±0.002 ±0.027 ±0.003
336 0.394 0.397 0.445 0.399 0.741 0.492 0.425 0.655 0.832
±0.001 ±0.002 ±0.009 ±0.003
720 0.418 0.425 0.456 0.429 0.845 0.493 0.458 0.724 0.820
±0.000 ±0.001 ±0.035 ±0.002
96 0.274 0.284 0.290 0.302 0.462 0.293 0.271 0.507 0.642
±0.010 ±0.004 ±0.026 ±0.013
192 0.306 0.320 0.347 0.323 0.586 0.336 0.318 0.673 0.757
±0.001 ±0.001 ±0.025 ±0.005
336 0.338 0.350 0.360 0.352 0.871 0.379 0.364 0.845 0.872
±0.001 ±0.001 ±0.017 ±0.003
720 0.390 0.402 0.424 0.402 1.267 0.419 0.420 1.451 1.328
±0.001 ±0.002 ±0.014 ±0.003
96 0.252 0.273 0.288 0.277 0.393 0.313 0.302 0.449 0.357 ±0.002 ±0.001 ±0.013 ±0.003
192 0.263 0.292 0.304 0.304 0.417 0.324 0.311 0.443 0.368
±0.001 ±0.011 ±0.033 ±0.027
336 0.277 0.297 0.315 0.317 0.422 0.327 0.328 0.443 0.380
±0.000 ±0.007 ±0.018 ±0.018
720 0.306 0.321 0.330 0.333 0.427 0.342 0.344 0.445 0.376
±0.000 ±0.006 ±0.014 ±0.015
96 0.306 0.363 0.369 0.436 0.752 0.323 0.276 - 0.812 ±0.006 ±0.013 ±0.049 ±0.054
192 0.371 0.437 0.416 0.437 0.895 0.369 0.369 - 0.851
±0.008 ±0.021 ±0.041 ±0.021
336 0.453 0.515 0.491 0.523 1.036 0.524 0.464 - 1.081
±0.004 ±0.006 ±0.036 ±0.029
720 0.750 0.777 0.823 0.818 1.310 0.941 0.800 - 1.127
±0.006 ±0.064 ±0.040 ±0.007
96 0.292 0.300 0.306 0.300 0.410 0.371 0.359 0.468 0.384
±0.001 ±0.020 ±0.033 ±0.020
192 0.294 0.317 0.321 0.419 0.435 0.382 0.380 0.467 0.390
±0.005 ±0.012 ±0.034 ±0.218
336 0.292 0.299 0.348 0.501 0.434 0.387 0.375 0.469 0.408
±0.000 ±0.000 ±0.093 ±0.163
720 0.311 0.344 0.325 0.458 0.466 0.395 0.375 0.473 0.396
±0.003 ±0.026 ±0.023 ±0.159
96 0.249 0.242 0.281 0.271 0.405 0.329 0.314 0.556 0.490
±0.001 ±0.002 ±0.018 ±0.009
192 0.277 0.272 0.302 0.275 0.434 0.370 0.329 0.624 0.589
±0.000 ±0.003 ±0.020 ±0.003
336 0.304 0.299 0.310 0.307 0.543 0.391 0.377 0.753 0.652
±0.001 ±0.001 ±0.012 ±0.009
720 0.342 0.341 0.363 0.351 0.705 0.426 0.409 0.934 0.675
±0.000 ±0.002 ±0.002 ±0.021
OverallMAEimprovement 3.99% 11.63% 9.60% 53.00% 15.67% 9.93% 44.44% 54.44%
B Additional Experiments
In this section, we provide additional experiments to showcase, quantitatively and qualitatively, the superiority of
our approach.
B.1 MAE Results
In this section, we provide the performance comparison of the different baselines with the Mean Absolute Error
(MAE). We display the results in Table 5. The conclusion are similar to the one made in the main paper and
confirms the superiority of SAMformer.
B.2 Significance Test for SAMformer and TSMixer with SAM
In this section, we perform a Student t-test between SAMformer and TSMixer trained with SAM. It should be
noted that TSMixer with SAM significantly outperforms vanilla TSMixer. We report the results in Table 6. We
observe that the SAMformer significantly improves upon TSMixer trained with SAM on 7 out of 8 datasets.
1hTTE
2hTTE
1mTTE
2mTTE
yticirtcelE
egnahcxE
cffiarT
rehtaeWRomain Ilbert, Ambroise Odonnat et al.
Table 6: Significance test with Student’s t-test and performance comparison between SAMformer and TSMixer
trainedwith SAMacrossvariousdatasets andpredictionhorizons. Wedisplaytheaverageand standard deviation
of the test MSE obtained on 5 runs (mean ). The performance of the best model is in bold when the
±std
improvement is statistically significant at the level 0.05 (p-value<0.05).
H Model ETTh1 ETTh2 ETTm1 ETTm2 Electricity Exchange Traffic Weather
96 SAMformer 0.381 ±0.003 0.295 ±0.002 0.329 ±0.001 0.181 ±0.005 0.155 ±0.002 0.161 ±0.007 0.407 ±0.001 0.197 ±0.001
TSMixer 0.388 ±0.001 0.305 ±0.007 0.327 ±0.002 0.190 ±0.003 0.171 ±0.001 0.233 ±0.016 0.409 ±0.016 0.189 ±0.003
192 SAMformer 0.409 ±0.002 0.340 ±0.002 0.353 ±0.006 0.233 ±0.002 0.168 ±0.001 0.246 ±0.009 0.415 ±0.005 0.235 ±0.000
TSMixer 0.421 ±0.002 0.350 ±0.002 0.356 ±0.004 0.250 ±0.002 0.191 ±0.010 0.342 ±0.031 0.433 ±0.009 0.228 ±0.004
336 SAMformer 0.423 ±0.001 0.350 ±0.000 0.382 ±0.001 0.285 ±0.001 0.183 ±0.000 0.368 ±0.006 0.421 ±0.001 0.276 ±0.001
TSMixer 0.430 ±0.002 0.360 ±0.002 0.387 ±0.004 0.301 ±0.003 0.198 ±0.006 0.474 ±0.014 0.424 ±0.000 0.271 ±0.001
720 SAMformer 0.427 ±0.002 0.391 ±0.001 0.429 ±0.000 0.375 ±0.001 0.219 ±0.000 1.003 ±0.018 0.456 ±0.003 0.334 ±0.000
TSMixer 0.440 ±0.005 0.402 ±0.002 0.441 ±0.002 0.389 ±0.002 0.230 ±0.005 1.078 ±0.179 0.488 ±0.028 0.331 ±0.001
Table 7: Comparison of the number of parameters between SAMformer and TSMixer on the datasets described
in Table 4 for prediction horizons H ∈ {96,192,336,720}. We also compute the ratio between the number of
parameters of TSMixer and the number of parameters of SAMformer. A ratio of 10 means that TSMixer has 10
times more parameters than SAMformer. For each dataset, we display in the last cell of the corresponding row
the ratio averaged over all the horizons H. The overall ratio over all datasets and horizons is displayed in bold in
the bottom right-hand cell.
H=96 H=192 H=336 H=720
Dataset Total
SAMformer TSMixer SAMformer TSMixer SAMformer TSMixer SAMformer TSMixer
ETT 50272 124142 99520 173390 173392 247262 369904 444254 -
Exchange 50272 349344 99520 398592 173392 472464 369904 669456 -
Weather 50272 121908 99520 171156 173392 245028 369904 442020 -
Electricity 50272 280676 99520 329924 173392 403796 369904 600788 -
Traffic 50272 793424 99520 842672 173392 916544 369904 1113536 -
Avg. Ratio 6.64 3.85 2.64 1.77 3.73
B.3 Computational Efficiency of SAMformer
In this section, we showcase the computational efficiency of our approach. We compare in Table 7 the number
of parameters of SAMformer and TSMixer on the several benchmarks used in our experiments. We also display
the ratio between the number of parameters of TSMixer and the number of parameters of SAMformer. Overall,
SAMformer has ∼4 times fewer parameters than TSMixer while outperforming it by 14.33% on average.
B.4 Strong Generalization Regardless of the Initialization
In this section, we demonstrate that SAMformer has a strong generalization capacity. In particular, Transformer
heavilydependsontheinitialization, whichmightbeduetobadlocalminimaasitslosslandscapeissharperthan
the one of SAMformer. We display in Figure 10 the distribution of the test MSE on 5 runs on the datasets used
in our experiments (Table 4) and various prediction horizons H ∈{96,192,336,720}. We can see that SAMformer
has strong and stable performance across the datasets and horizons, regardless of the seed. On the contrary, the
performance Transformer is unstable with a large generalization gap depending on the seed.
B.5 Faithful Signal Propagation
In this section, we consider Transformer, SAMformer, σReparam, which corresponds to Transformer with the
rescaling proposed by Zhai et al. (2023) and SAMformer + σReparam which is SAMformer with the rescaling
proposedbyZhaietal.(2023). Forillustration,weplotabatchofattentionmatricesaftertrainingwithprediction
horizon H =96 (our primary study does not identify significant changes with the value of horizon) on Weather in
Figure 12. While Transformer tends to ignore the importance of a feature on itself by having low values on the
diagonal, we can see in the bottom left of Figure 12 that SAMformer strongly encourages these feature-to-feature
correlations. A very distinctive pattern is observable, a near-identity attention reminiscent of He et al. (2023) andUnlocking the Potential of Transformers
ETTh1 ETTh2 ETTm1 ETTm2
0.57 0.44 0.43 0.28
0.46 0.36 0.37 0.22
0.35 0.27 0.31 0.16
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Electricity Exchange Traffic Weather
0.19 0.38 0.5 0.24
0.17 0.25 0.44 0.22
0.15 0.13 0.38 0.19
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Transformer SAMformer
(a) Prediction horizon H =96.
ETTh1 ETTh2 ETTm1 ETTm2
0.59 0.44 0.44 0.3
0.49 0.38 0.38 0.26
0.38 0.32 0.33 0.22
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Electricity Exchange Traffic Weather
0.29 0.42 0.53 0.29
0.22 0.32 0.46 0.26
0.15 0.21 0.4 0.23
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Transformer SAMformer
(b) Prediction horizon H =192.
Trockman and Kolter (2023). The former showcased that pre-trained vision models present similar patterns and
both identified the benefits of such attention matrices for the propagation of information along the layers of deep
transformers in NLP and computer vision. While in our setting, we have a single-layer transformer, this figure
indicates that at the end of the training, self-information from features to themselves is not lost. In contrast,
we see that σReparam leads to almost rank-1 matrices with identical columns. This confirms the theoretical
insightsfromTheorem2.2thatshowedhowrescalingthetrainableweightswithσReparamtolimitthemagnitude
of ∥W W⊤∥ could hamper the rank of XW W⊤X⊤ and of the attention matrix. Finally, we observe that
Q K 2 Q K
naively combining SAMformer with σReparam does not solve the issues caused by the latter: while some diagonal
patterns remain, most of the information has been lost. Moreover, combining both σReparam and SAMformer
heavily increases the training time, as shown in Figure 11.
ESM
tseT
ESM
tseT
ESM
tseT
ESM
tseTRomain Ilbert, Ambroise Odonnat et al.
ETTh1 ETTh2 ETTm1 ETTm2
0.62 0.43 0.5 0.34
0.51 0.38 0.43 0.31
0.4 0.34 0.36 0.28
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Electricity Exchange Traffic Weather
0.24 0.56 0.83 0.28
0.21 0.44 0.6 0.28
0.17 0.33 0.36 0.27
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Transformer SAMformer
(a) Prediction horizon H =336.
ETTh1 ETTh2 ETTm1 ETTm2
0.68 0.44 0.53 0.47
0.53 0.41 0.47 0.41
0.39 0.38 0.41 0.36
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Electricity Exchange Traffic Weather
0.27 1.77 0.51 0.36
0.24 1.32 0.48 0.34
0.21 0.88 0.44 0.33
Transformer SAMformer Transformer SAMformer Transformer SAMformer Transformer SAMformer
Transformer SAMformer
(b) Prediction horizon H =720.
Figure 10: Test Mean Squared error on all datasets for a prediction horizon H ∈ {96,192,336,720} across
five different seed values for Transformer and SAMformer. This plot reveals a significant variance for the
Transformer, as opposed to the minimal variance of SAMformer, showing the high impact of weight initialization
on Transformer and the high resilience of SAMformer.
C Ablation Study and Sensitivity Analysis
C.1 Sensitivity to the Prediction Horizon H.
In Figure 13, we show that SAMformer outperforms its best competitor, TSMixer trained with SAM, on 7 out of
8 datasets for all values of prediction horizon H. This demonstrates the robustness of SAMformer.
ESM
tseT
ESM
tseT
ESM
tseT
ESM
tseTUnlocking the Potential of Transformers
ETTh1 Exchange
1000
100
500 50
0 0
96 192 336 720 96 192 336 720
Prediction Horizon H Prediction Horizon H
SAMformer SAMformer + Reparam
Figure 11: Using σReparam on top of SAMformer heavily increases the training time.
11111111111111111111111111111111................................00000000000000000000000000000000 11111111111111111111111111111111................................00000000000000000000000000000000
00000000000000000000000000000000................................88888888888888888888888888888888 00000000000000000000000000000000................................88888888888888888888888888888888
00000000000000000000000000000000................................66666666666666666666666666666666 00000000000000000000000000000000................................66666666666666666666666666666666
00000000000000000000000000000000................................44444444444444444444444444444444 00000000000000000000000000000000................................44444444444444444444444444444444
00000000000000000000000000000000................................22222222222222222222222222222222 00000000000000000000000000000000................................22222222222222222222222222222222
00000000000000000000000000000000................................00000000000000000000000000000000 00000000000000000000000000000000................................00000000000000000000000000000000
(a) Transformer (b) σReparam
11111111111111111111111111111111................................00000000000000000000000000000000 11111111111111111111111111111111................................00000000000000000000000000000000
00000000000000000000000000000000................................88888888888888888888888888888888 00000000000000000000000000000000................................88888888888888888888888888888888
00000000000000000000000000000000................................66666666666666666666666666666666 00000000000000000000000000000000................................66666666666666666666666666666666
00000000000000000000000000000000................................44444444444444444444444444444444 00000000000000000000000000000000................................44444444444444444444444444444444
00000000000000000000000000000000................................22222222222222222222222222222222 00000000000000000000000000000000................................22222222222222222222222222222222
00000000000000000000000000000000................................00000000000000000000000000000000 00000000000000000000000000000000................................00000000000000000000000000000000
(c) SAMformer (d) SAMformer + σReparam
Figure 12: Batch of 32 attention matrices on Weather with horizon H =96 after training different models. (a)
Transformer. (b) σReparam (c) SAMformer. (d) SAMformer + σReparam.
C.2 Sensitivity to the Neighborhood Size ρ.
In Figure 14, we display the evolution of test MSE of SAMformer and TSMixer with the values of neighborhood
size ρ for SAM. Overall, SAMformer has a smooth behavior with ρ, with a decreasing MSE and less variance. On
the contrary, TSMixer is less stable and fluctuates more. On most of the datasets, the range of neighborhood
seizes ρ such that SAMformer is below TSMixer is large. The first value ρ=0 amounts to the usual minimization
with Adam, which confirms that SAM always improves the performance of SAMformer. In addition, and despite
its lightweight (Table 7), SAMformer achieves the lowest MSE on 7 out of 8 datasets, as shown in Table 1 and
Table 6. It should be noted that compared to similar studies in computer vision (Chen et al., 2022), values of ρ
must be higher to effectively improve the generalization and flatten the loss landscapes. This follows from the
high sharpness λ observed in time series forecasting (Figure 5a) compared to computer vision models (Chen
max
et al., 2022).
C.3 Sensitivity to the Change of the Optimizer.
In our work, we considered the Adam optimizer (Kingma and Ba, 2015) as it is the de-facto optimizer for
transformer-based models (Ahn et al., 2023; Chen et al., 2022; Pan and Li, 2022; Zhou et al., 2021, 2022). The
superiority of Adam to optimize networks with attention has been empirically and theoretically studied, where
)s(
emiT
gniniarTRomain Ilbert, Ambroise Odonnat et al.
ETTh1 ETTh2 ETTm1 ETTm2
0.44 0.4 0.44 0.39
0.41 0.35 0.38 0.28
0.38 0.3 0.33 0.18
96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720
Electricity Exchange Traffic Weather
0.23 1.21 0.51 0.33
0.19 0.7 0.45 0.26
0.16 0.18 0.4 0.19
96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720
Prediction Horizon H Prediction Horizon H Prediction Horizon H Prediction Horizon H
TSMixer SAMformer
Figure 13: Evolution of the test MSE on all datasets for a prediction horizon H ∈{96,192,336,720}. We display
the average test MSE with a 95% confidence interval. We see that SAMformer consistently performs well with a
low variance. Despite its lightweight (Table 7), SAMformer surpasses TSMixer (trained with SAM) on 7 out of 8
datasets as shown in Table 1 and Table 6.
ETTh1 ETTh2 ETTm1 ETTm2 Electricity Exchange Traffic Weather
0.53 0.41 0.4 0.23 0.21 0.69 0.98 0.28
0.45 0.35 0.36 0.2 0.18 0.43 0.64 0.22
0.38 0.29 0.32 0.17 0.15 0.16 0.3 0.17
0.57 0.42 0.43 0.28 0.26 0.54 1.01 0.31
0.49 0.38 0.39 0.26 0.21 0.4 0.64 0.26
0.41 0.34 0.35 0.23 0.16 0.25 0.27 0.22
0.58 0.41 0.47 0.33 0.33 0.56 1.05 0.31
0.5 0.38 0.43 0.31 0.25 0.46 0.71 0.29
0.43 0.35 0.38 0.28 0.18 0.36 0.37 0.26
0.63 0.43 0.5 0.45 0.3 1.48 1.46 0.37
0.53 0.41 0.46 0.41 0.26 1.21 0.87 0.35
0.43 0.39 0.41 0.38 0.22 0.94 0.27 0.32
0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1
Neighbourhood Size Neighbourhood Size Neighbourhood Size Neighbourhood Size Neighbourhood Size Neighbourhood Size Neighbourhood Size Neighbourhood Size
TSMixer SAMformer
Figure14: EvolutionofthetestMSEwiththeneighborhoodsizeρofSAM(RemarkD.1). Wedisplaytheaverage
test MSE with a 95% confidence interval. Overall, SAMformer has a smooth behavior with ρ, with a decreasing
MSE and less variance. On the contrary, TSMixer is less stable and fluctuates more. On most of the datasets, the
range of neighborhood seizes ρ such that SAMformer is below TSMixer is large. The first value ρ=0 amounts to
the usual minimization with Adam, which confirms that SAM always improves the performance of SAMformer.
In addition, and despite its lightweight (Table 7), SAMformer achieves the lowest MSE on 7 out of 8 datasets, as
shown in Table 1 and Table 6. It should be noted that compared to similar studies in computer vision (Chen
et al., 2022), values of ρ must be higher to effectively improve the generalization and flatten the loss landscapes.
recent works show that the SGD (Nesterov, 1983) was not suitable for attention-based models (Ahn et al., 2023;
Liu et al., 2020; Pan and Li, 2022; Zhang et al., 2020). To ensure the thoroughness of our investigation, we
conducted experiments on the synthetic dataset introduced in Eq. (2) and reported the results in Figure 15a. As
expected, we see that using SGD leads to high-magnitude losses and divergence. We also conducted the same
experiments with the AdamW (Loshchilov and Hutter, 2019) that incorporates the weight decay scheme in the
adaptive optimizer Adam (Kingma and Ba, 2015). We display the results obtained with weight decay factors
69=H
291=H
633=H
027=H
ESM
tseT
ESM
tseT
ESM
tseT
ESM
tseT
ESM
tseT
ESM
tseTUnlocking the Potential of Transformers
wd=1e−3 in Figure 15a and with wd∈{1e−5,1e−4} in Figure 15b. When wd=1e−3, we observe that it does
not converge. However, with wd∈{1e−5,1e−4}, we observe a similar behavior for Transformer than when it is
trained with Adam (Figure 2). Hence, using AdamW does not lead to the significant benefits brought by SAM
(Figure 1. As the optimization is very sensitive to the value of weight decay wd, it motivates us to conduct our
experiments with Adam.
493 490 6 6
254 252 3 3
14 14 0 0
10 50 90 10 50 90 10 50 90 10 50 90
Training Epochs Training Epochs Training Epochs Training Epochs
Oracle AdamW (wd=1e-3) SGD Oracle AdamW (wd=1e-5) AdamW (wd=1e-4)
(a) SGD and AdamW with wd=1e−3 (b) AdamW with wd∈{1e−5,1e−4}.
Figure 15: Illustration of different optimizers on synthetic data generated with Eq. (2) where Oracle is the
least-square solution. We saw in Figure 1 that with Adam, Transformer overfits and has poor performance while
SAMformer smoothly reaches the oracle. (a) We can see that using SGD and Adam with weight decay wd=1e−5
leads to huge loss magnitudes and fails to converge. (b) With well-chosen weight decays (wd∈{1e−3,1e−4}),
training Transformer with AdamW leads to similar performance than Adam. The overfitting is noticeable and
the training is unstable. AdamW does not bring more stabilization and is very sensitive to the hyperparameters.
Hence, this toy example motivates us to conduct our thorough experiments with the optimizer Adam.
C.4 Ablation on the Implementation.
This ablation study contrasts two variants of our model to showcase the effectiveness of Sharpness-Aware
Minimization (SAM) and our attention approach. Identity Attention represents SAMformer with an attention
weight matrix constrained to identity, illustrating that SAM does not simply reduce the attention weight matrix
to identity, as performance surpasses this configuration. Temporal Attention is compared to our Transformer
without SAM, highlighting our focus on treating feature correlations in the attention mechanism rather than
temporal correlations.
Table 8: The Temporal Attention model is benchmarked against our Transformer model, which employs
feature-based attention rather than time-step-based attention. We report in the last column the Overall
improvement in MSE and MAE of Transformer over the Temporal Attention. This comparison reveals that
channel-wise attention, i.e., focusing on features pairwise correlations, significantly boosts the performance, with
a 12.97% improvement in MSE and 18.09% in MAE across all considered datasets.
Model Metrics H ETTh1 ETTh2 ETTm1 ETTm2 Electricity Exchange Traffic Weather OverallImprovement
96 0.496±0.009 0.401±0.011 0.542±0.063 0.330±0.034 0.291±0.025 0.684±0.218 0.933±0.188 0.225±0.005
MSE 192 0.510±0.014 0.414±0.020 0.615±0.056 0.394±0.033 0.294±0.024 0.434±0.063 0.647±0.131 0.254±0.001 12.97%
336 0.549±0.017 0.396±0.014 0.620±0.046 0.436±0.081 0.290±0.016 0.473±0.014 0.656±0.113 0.292±0.000
720 0.604±0.017 0.396±0.010 0.694±0.055 0.469±0.005 0.307±0.014 1.097±0.084 - 0.346±0.000
96 0.488±0.007 0.434±0.006 0.525±0.040 0.393±0.020 0.386±0.014 0.589±0.096 0.598±0.072 0.277±0.004
MAE
192 0.492±0.010 0.443±0.015 0.566±0.032 0.421±0.019 0.385±0.014 0.498±0.033 0.467±0.072 0.294±0.001
18.09%
336 0.517±0.012 0.440±0.012 0.550±0.024 0.443±0.039 0.383±0.009 0.517±0.008 0.469±0.070 0.320±0.000
720 0.556±0.009 0.442±0.006 0.584±0.027 0.459±0.004 0.396±0.012 0.782±0.041 - 0.356±0.000
D Additional Background
D.1 Reversible Instance Normalization: RevIN
Overview. Kim et al. (2021b) recently proposed RevIN, a reversible instance normalization to reduce the
discrepancy between the distributions of training and test data. Indeed, statistical properties of real-world
time series, e.g. mean and variance, can change over time, leading to non-stationary sequences. This causes a
distribution shift between training and test sets for the forecasting task. The RevIN normalization scheme is
now widespread in deep learning approaches for time series forecasting (Chen et al., 2023; Nie et al., 2023). The
RevIN normalization involves trainable parameters (β,γ)∈RK ×RK and consists of two parts: a normalization
ssoL
gniniarT
noitnettAlaropmeT
ssoL
noitadilaV
ssoL
gniniarT
ssoL
noitadilaVRomain Ilbert, Ambroise Odonnat et al.
Table 9: Identity Attention represents our SAMformer with the attention weight matrix constrained to an
identity matrix. We report in the last column the Overall improvement in MSE and MAE of SAMformer over
the Identity Attention. This setup demonstrates that naively fixing the attention matrix to the identity does
not enable to match the performance of SAM, despite the near-identity attention matrices SAM showcases (see
Appendix B.5 for more details). In particular, we observe an overall improvement of 11.93% in MSE and 4.18%
in MAE across all the datasets.
Model Metrics H ETTh1 ETTh2 ETTm1 ETTm2 Electricity Exchange Traffic Weather OverallImprovement
96 0.477±0.059 0.346±0.055 0.345±0.027 0.201±0.035 0.175±0.015 0.179±0.031 0.416±0.037 0.206±0.019
MSE
192 0.467±0.074 0.374±0.031 0.384±0.042 0.248±0.016 0.189±0.022 0.320±0.070 0.437±0.041 0.236±0.002
11.93%
336 0.512±0.070 0.372±0.024 0.408±0.032 0.303±0.022 0.211±0.019 0.443±0.071 0.500±0.155 0.277±0.003
720 0.505±0.107 0.405±0.012 0.466±0.043 0.397±0.029 0.233±0.019 1.123±0.076 0.468±0.021 0.338±0.009
96 0.473±0.041 0.395±0.033 0.376±0.019 0.294±0.027 0.283±0.023 0.320±0.023 0.301±0.039 0.259±0.021
MAE
192 0.463±0.055 0.413±0.022 0.399±0.030 0.321±0.012 0.291±0.029 0.418±0.043 0.314±0.042 0.278±0.002
4.18%
336 0.490±0.049 0.413±0.015 0.411±0.019 0.354±0.018 0.309±0.021 0.498±0.041 0.350±0.106 0.305±0.003
720 0.496±0.066 0.438±0.008 0.444±0.030 0.406±0.017 0.322±0.021 0.788±0.021 0.325±0.023 0.347±0.009
step and a symmetric denormalization step. Before presenting them, we introduce for a given input time series
X(i) ∈X the empirical mean µˆ[X(i)] and empirical standard deviation σˆ2[X(i)] of its k-th feature X(i) ∈R1×L as
k k k
follows:
 (cid:104) (cid:105)
 µˆ X(i) = 1 (cid:80)L X(i)
k L t=1 kj
(cid:104) (cid:105) (6)
 σˆ2 X(i) = 1 (cid:80)L (X(i)−µˆ[X(i)])2.
k L t=1 kj k
The first one acts on the input sequence X(i) and outputs the corresponding normalized sequence X˜(i) ∈RK×L
such that for all k,t,
 
(cid:104) (cid:105)
X(i)−µˆ X(i)
X˜( ki t) =γ k  (cid:114)kt
(cid:104)
(cid:105)k   +β k, (7)
σˆ2 X(i) +ε
k
where ε>0 is a small constant to avoid dividing by 0. The neural network’s input is then X˜(i), instead of X(i).
The second step is applied to the output of the neural network Y˜(i), such that the final output considered for the
forecasting is the denormalized sequence Yˆ(i) ∈RK×H such that for all k,t,
(cid:114) (cid:104) (cid:105) (cid:32) Y˜(i)−β (cid:33) (cid:104) (cid:105)
Yˆ(i) = σˆ2 X(i) +ε· kt k +µˆ X(i) . (8)
kt k γ k
k
As stated in Kim et al. (2021b), µˆ,σˆ2,β and γ contain the non-stationary information of the input sequences
X(i).
End-to-end closed form with linear model and RevIN. We consider a simple linear neural network.
Formally, for any input sequence X∈RD×L, the prediction of f : RD×L →RD×H simply writes
lin
f (X)=XW. (9)
lin
When combined with RevIN, the neural network f is not directly applied to the input sequence but after the
lin
first normalization step of RevIN (Eq. (7)). An interesting benefit of the simplicity of f is that it enables us to
lin
write its prediction in closed form, even when with RevIN. The proof is deferred to Appendix E.4.
Proposition D.1 (Closed-form formulation). For any input sequence X∈RK×L, the output of the linear
model Yˆ =f (X)∈RK×H has entries
lin
 
L L
Yˆ
kt
=µˆ[X k]+(cid:88) (X
kj
−µˆ[X k])W jt− β γk(cid:112) σˆ2[X k]+ε1−(cid:88) W jt, (10)
k
j=1 j=1
noitnettA
ytitnedIUnlocking the Potential of Transformers
Proposition D.1 highlights the fact that the k-th variable of the outputs Yˆ only depends on k-th variable of the
input sequence X. It leads to channel-independent forecasting, although we did not explicitly enforce it. (10) can
be seen as a linear interpolation around the mean µˆ with a regularization term on the network parameters W
involving the non-stationary information σˆ2,β,γ. Moreover, the output sequence Yˆ can be written in a more
compact and convenient matrix formulation as follows
Yˆ =XW+ξ(X,W,β,γ), (11)
where ξ(X,W,β,γ) ∈ RK×H with entry (cid:16) µˆ[X ]− βk(cid:112) σˆ2[X ]+ε(cid:17)(cid:16) 1−(cid:80)L W (cid:17) in the k-th row and t-th
k γk k j=1 jt
column. The proof is deferred to Appendix E.5. With this formulation, the predicted sequence can be seen as a
sum of a linear term XW and a residual term ξ(X,W,β,γ) that takes into account the first and second moments
of each variable X , which is reminiscent of the linear regression model.
k
D.2 Sharpness-aware minimization (SAM)
Regularizing with the sharpness. Standard approaches consider a parametric family of models f and
ω
aim to find parameters ω that minimize a training objective L (ω), used as a tractable proxy to the true
train
generalization error L (ω). Most deep learning pipelines rely on first-order optimizers, e.g. SGD (Nesterov,
test
1983) or Adam (Kingma and Ba, 2015), that disregard higher-order information such as the curvature, despite its
connection to generalization (Chaudhari et al., 2017; Dziugaite and Roy, 2017; Keskar et al., 2017). As L
train
is usually non-convex in ω, with multiple local or global minima, solving min L (ω) may still lead to high
ω train
generalizationerrorL (ω). Toalleviatethisissue,Foretetal.(2021)proposetoregularizethetrainingobjective
test
with the sharpness, defined as follows
Definition D.2 (Sharpness, Foret et al. (2021)). For a given ρ≥0, the sharpness of L at ω writes
train
s(ω,ρ):= max L (ω+ϵ)−L (ω). (12)
train train
∥ϵ∥2≤ρ
Remark D.1 (Interpretation of ρ). Instead of simply minimizing the training objective L , SAM searches for
train
parameters ω achieving both low training loss and low curvature in a ball B(ω,ρ). The hyperparameter ρ ≥ 0
corresponds to the size of the neighborhood on which the parameters search is done. In particular, taking ρ=0 is
equivalent to the usual minimization of L .
train
In particular, SAM incorporates sharpness in the learning objective, resulting in the problem of minimizing w.r.t
ω
LSAM(ω):= max L (ω+ϵ) . (13)
train train
∥ϵ∥2≤ρ
(cid:124) (cid:123)(cid:122) (cid:125)
=Ltrain(ω)+s(ω,ρ)
Gradient updates. As the exact solution to the inner maximization in Eq. (13) is hard to compute, the
authors of (Foret et al., 2021) approximate it with the following first-order Taylor expansion
ϵ∗(ω):=argmaxL (ω+ϵ)
train
∥ϵ∥2≤ρ
≈argmaxL (ω)+ϵ⊤∇L (ω)
train train
∥ϵ∥2≤ρ
=argmaxϵ⊤∇L (ω), (14)
train
∥ϵ∥2≤ρ
where the solution of (14) writes ϵˆ(ω)=ρ ∇Ltrain(ω) . It leads to the following gradient update
∥∇Ltrain(ω)∥2
(cid:18) (cid:19)
∇L (ω)
ω =ω −η∇L ω +ρ train ,
t+1 t train t ∥∇L (ω)∥
train 2
where η is the learning rate.Romain Ilbert, Ambroise Odonnat et al.
E Proofs
E.1 Notations
To ease the readability of the proofs, we recall the following notations. We denote scalar values by regular letters
(e.g., parameter λ), vectors by bold lowercase letters (e.g., vector x), and matrices by bold capital letters (e.g.,
matrix M). For a matrix M∈Rn×m, we denote by M its i-th row, by M its j-th column, by m its entries
i ·,j ij
and by M⊤ its transpose. We denote the trace of a matrix M by Tr(M), its rank by rank(M) and its Frobenius
norm by ∥M∥ . We denote σ(M):=(σ (M),...,σ (M)) the vector of singular values of M in non-decreasing
F 1 n˜
order, with n˜ =min{n,m} and the specific notation σ (M),σ (M) for the minimum and maximum singular
min max
values, respectively. We denote by ∥M∥
=(cid:80)n˜
σ (M) its nuclear norm and by ∥M∥ =σ (M) its spectral
∗ i=1 i 2 max
norm. When M is square with n=m, we denote λ(M):=(λ (M),...,λ (M)) the vector of singular values of
1 n
M in non-decreasing order and the specific notation λ (M),λ (M) for the minimum and maximum singular
min max
values, respectively. For a vector x, its transpose writes x⊤ and its usual Euclidean norm writes ∥x∥. The identity
matrix of size n×n is denoted by I . The vector of size n with each entry equal to 1 is denoted by 1 . The
n n
notation M≽0 indicates that M is positive semi-definite.
E.2 Proof of Proposition 2.1
We first recall the following technical lemmas.
Lemma E.1. Let S∈Rn×m and B∈Rm×m. If B has full rank, then
rank(SB)=rank(BS)=rank(S).
Proof. Let F :={Su|u∈Rm}⊂Rn and F :={(SB)u|u∈Rm}⊂Rn be the vector spaces generated by the
1 2
columns of S and SB respectively. By definition, the rank of a matrix is the dimension of the vector space
generated by its columns (equivalently by its rows). We will show that F and F coincides. Let v ∈F , i.e.,
1 2 1
there exists u∈Rm such that v=Su. As B is full rank, the operator x→Bx is bijective. It follows that there
always exists some z∈Rm such that u=Bz. Then, we have
v=Su=S(Bz)=(SB)z,
which means that v∈F . As v was taken arbitrarily in F , we have proved that F ⊂F . Conversely, consider
2 1 1 2
y∈F , i.e., we can write y=(SB)z for some z∈Rm. It can then be seen that
2
y=(SB)z=S(Bz),
which means that y ∈ F . Again, as y was taken arbitrarily, we have proved that F ⊂ F . In the end, we
1 1 2
demonstrated that F and F coincide, hence they have the same dimension. By definition of the rank, S and SB
1 2
have the same rank. Similar arguments can be used to show that S and BS have the same rank, which concludes
the proof.
The next lemma is a well-known result in matrix analysis and can be found in Horn and Johnson (1991, Theorem
4.4.5). For the sake of self-consistency, we recall it below along with a sketch of the original proof.
Lemma E.2. (see Horn and Johnson, 1991, Theorem 4.4.5, p. 281). Let S ∈ Rn×m,B = Rp×q and
C∈Rn×q. There exist matrices Y ∈Rm×q and Z∈Rn×p such that SY−ZB=C if, and only if,
(cid:18)(cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21)(cid:19)
S C S 0
rank =rank .
0 B 0 B
Proof. Assume that there exists Y ∈Rm×q and Z∈Rn×p such that SY−ZB=C. We recall that the following
equality holds
(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)
S SY−ZB I −Y S 0 I Z
= m n . (15)
0 B 0 I 0 B 0 I
q pUnlocking the Potential of Transformers
Using Lemma E.1 twice on the right-hand-side of Eq. (15), we obtain
(cid:18)(cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21)(cid:19)
S SY−ZB S 0
rank =rank .
0 B 0 B
Using SY−ZB=C concludes the proof for the direct sense of the equivalence.
Conversely, assume that
(cid:18)(cid:20) (cid:21)(cid:19) (cid:18)(cid:20) (cid:21)(cid:19)
S C S 0
rank =rank .
0 B 0 B
Since two matrices have the same rank if, and only if, they are equivalent, we know that there exists Q ∈
R(n+p)×(n+p),U∈R(m+q)×(m+q) non-singular such that
(cid:20) (cid:21) (cid:20) (cid:21)
S C S 0
=Q U. (16)
0 B 0 B
The rest of the proof in Horn and Johnson (1991) is constructive and relies on Eq. (16) to exhibit Y ∈Rm×q and
Z∈Rn×p such that SY−ZB=C. This concludes the proof of the equivalence.
We now proceed to the proof of Proposition 2.1.
Proof. Applying Lemma E.2 with S=P, B=0, C=XW and W in the role of Y ensures that there exists
toy
W∈RL×H suchthatPW=XW ifandonlyifrank([P XW ])=rank(P),whichconcludestheproof.
toy toy
E.3 Proof of Proposition 2.2
We first prove the following technical lemmas. While these lemmas are commonly used and, for most of them,
straightforward to prove, they are very useful to demonstrate Proposition 2.2.
Lemma E.3 (Trace of a product of matrix). Let S,B ∈ Rn×n be symmetric matrices with B positive
semi-definite. We have
λ (S)Tr(B)≤Tr(SB)≤λ (S)Tr(B).
min max
Proof. The spectral theorem ensures the existence of P ∈ Rn×n orthogonal, i.e., P⊤P = PP⊤ = I , and
n
Λ∈Rn×n diagonal with the eigenvalues of S as entries such that S=PΛP⊤. Benefiting from the properties of
the trace operator, we have
Tr(SB)=Tr(I SB)
n
 
=TrPP⊤SB (orthogonality of P)
(cid:124)(cid:123)(cid:122)(cid:125)
=In
=Tr(cid:0) P⊤SBP(cid:1)
(cyclic property of trace)
=Tr(cid:0) P⊤PΛP⊤BP(cid:1)
(Spectral theorem)
 
=TrP⊤PΛP⊤BP (orthogonality of P)
(cid:124)(cid:123)(cid:122)(cid:125)
=In
=Tr(cid:0) ΛP⊤BP(cid:1)
.
We introduce B˜ =P⊤BP=[˜b ] . It follows from the definition of Λ that
ij ij
Tr(SB)=Tr(cid:0) ΛP⊤BP(cid:1) =Tr(cid:16) ΛB˜(cid:17) =(cid:88) λ (S)˜b . (17)
i ii
iRomain Ilbert, Ambroise Odonnat et al.
We would like to write the˜b with respect to the p ,b the elements of P,B, respectively. As P is orthogonal,
ij ij ij
we know that its columns (e )n form an orthonormal basis of Rn. Hence, the entry (i,j) of ΛP⊤BP, writes as
i i=0
follows:
˜b =(cid:88) p b p
ij ki ij jk
kl
(cid:32) (cid:33)
(cid:88) (cid:88)
= p b p
ki ij jk
k l
(cid:124) (cid:123)(cid:122) (cid:125)
[Bej]
k
(cid:88)
= p [Be ]
ki j k
k
=e⊤Be ≥0. (B≽0)
i j
Hence, as B is positive semi-definite, the˜b are nonnegative. It follows that
ij
λ (S)(cid:88)˜b ≤(cid:88) λ (S) ˜b ≤λ (S)(cid:88)˜b . (18)
min ii i ii max ii
(cid:124)(cid:123)(cid:122)(cid:125)
i i ≥0 i
Moreover, usingthedefinition ofB˜, theorthogonality ofP andthecyclic propertyofthetraceoperation, wehave
 
(cid:88)˜b
ii
=Tr(cid:16) B˜(cid:17) =Tr(cid:0) P⊤BP(cid:1) =TrPP⊤B=Tr(B).
(cid:124)(cid:123)(cid:122)(cid:125)
i =In
Combining this last equality with Eq. (17) and Eq. (18) concludes the proof, i.e.,
λ (S)Tr(B)≤Tr(SB)≤λ (S)Tr(B). (19)
min max
Lemma E.4 (Power of symmetric matrices). Let S∈Rn×n be symmetric. The spectral theorem ensures the
existence of P∈Rn×n orthogonal, i.e., P⊤P=PP⊤ =I , and Λ∈Rn×n diagonal with the eigenvalues of
n
S as entries such that S=PΛP⊤. For any integer n≥1, we have
Sn =PΛnP⊤.
In particular, the eigenvalues of Sn are equal to the eigenvalues of S to the power of n.
Proof. Let n≥1 be an integer. We have
Sn =(cid:0) PΛP⊤(cid:1)n
=PΛP⊤×PΛP⊤×···×PΛP⊤×PΛP⊤
(cid:124) (cid:123)(cid:122) (cid:125)
×n
=PΛ×ΛP⊤...PΛ×ΛP⊤ (orthogonality of P)
(cid:124) (cid:123)(cid:122) (cid:125)
×n
=PΛ×Λ×···×Λ×ΛP⊤ (orthogonality of P)
(cid:124) (cid:123)(cid:122) (cid:125)
×n
=PΛnP⊤.
The diagonality of Λ suffices to deduct the remark on the eigenvalues of Sn.Unlocking the Potential of Transformers
Lemma E.5 (Case of equality between eigenvalues and singular values). Let S ∈ Rn×n be symmetric
and positive semi-definite. Then the i-th eigenvalue and the i-th singular value of S are equal, i.e., for all
i∈ 1,n , we have
(cid:74) (cid:75) λ (S)=σ (S).
i i
Proof. Let i∈ 1,n . By definition of singular value, we have
(cid:74) (cid:75)
(cid:113)
σ (S):= λ (S⊤S)
i i
(cid:112)
= λ (S2) (S is symmetric)
i
(cid:113)
= λ (S)2 (Lemma E.4)
i
=|λ (S)|
i
=λ (S). (S≽0)
i
Lemma E.6. Let X∈RD×L be an input sequence and S∈RL×L be a positive semi-definite matrix. Then,
XSX⊤ is positive semi-definite.
Proof. It is clear that XSX⊤ ∈RL×L is symmetric. Let u∈RL. We have:
u⊤XSX⊤u=(cid:0) X⊤u(cid:1)⊤ S(cid:0) X⊤u(cid:1) ≥0. (S≽0)
As u was arbitrarily chosen, we have proved that XSX⊤ is positive semi-definite.
We now proceed to the proof of Theorem 2.2.
Proof. We recall that W W⊤ is symmetric and positive semi-definite, we have
Q K
(cid:18)(cid:113) (cid:19)
∥XW W⊤X⊤∥ =Tr (cid:0) XW W⊤X⊤(cid:1)⊤ XW W⊤X⊤
Q K ∗ Q K Q K
(cid:16)(cid:113) (cid:17)
=Tr XW W⊤X⊤XW W⊤X⊤
K Q Q K
(cid:18)(cid:113) (cid:19)
=Tr XW W⊤X⊤XW W⊤X⊤ (symmetry)
Q K Q K
(cid:18)(cid:113) (cid:19)
=Tr
(cid:0)
XW
W⊤X⊤(cid:1)2
Q K
=Tr(cid:0) XW W⊤X⊤(cid:1) (Lemma E.6 with S=W W⊤)
Q K Q K
=Tr(cid:0) X⊤XW W⊤(cid:1) . (cyclic property of the trace)
Q K
Using the fact that X⊤X is positive semi-definite (Lemma E.6 with S=I ), and that W W⊤ is symmetric,
L Q K
Lemma E.3 can be applied with M=W W⊤ and B=X⊤X. It leads to:
Q K
∥XW W⊤X⊤∥ =Tr(cid:0) X⊤XW W⊤(cid:1) ≤λ (cid:0) W W⊤(cid:1) Tr(cid:0) X⊤X(cid:1) . (Lemma E.3)
Q K ∗ Q K max Q K
As W W⊤ is positive semi-definite, Lemma E.5 ensure
Q K
λ (cid:0) W W⊤(cid:1) =σ (cid:0) W W⊤(cid:1) =∥W W⊤∥
max Q K max Q K Q K 2
by definition of the spectral norm ∥·∥ . Recalling that by definition, Tr(cid:0) X⊤X(cid:1) =∥X∥2 concludes the proof, i.e.,
2 F
∥XW W⊤X⊤∥ ≤∥W W⊤∥ ∥X∥2.
Q K ∗ Q K 2 FRomain Ilbert, Ambroise Odonnat et al.
E.4 Proof of Proposition D.1
Proof. Let k ∈ 1,K and t∈ 1,H . We have
(cid:74) (cid:75) (cid:74) (cid:75)
(cid:18) (cid:19)
Yˆ =(cid:112) σˆ2[X ]+ε· y˜ kt−β k +µˆ[X ], (from (8))
kt k γ k
k
(cid:32)(cid:80)L X˜ W −β (cid:33)
=(cid:112) σˆ2[x ]+ε· j=1 kj jt k +µˆ[X ], (from (9))
k γ k
k
= (cid:112) σˆ2[X k]+ε ·(cid:88)L X˜ W − β k(cid:112) σˆ2[X ]+ε+µˆ[X ]
γ kj jt γ k k
k k
j=1
= (cid:112) σˆ2[X k]+ε ·(cid:88)L (cid:32) γ (cid:32) X kj −µˆ[x k] (cid:33) +β (cid:33) W − β k(cid:112) σˆ2[x ]+ε+µˆ[X ], (from (7))
γ k
j=1
k (cid:112) σˆ2[X k]+ε k jt γ k k k
 
L L
=(cid:88)
(X
kj
−µˆ[X k])W jt+
β γk(cid:112)
σˆ2[X
k]+ε(cid:88)
W jt−1+µˆ[X k]
k
j=1 j=1
 
L L
=µˆ[X
k]+(cid:88)
(X
kj
−µˆ[X k])W jt−
β γk(cid:112)
σˆ2[X
k]+ε1−(cid:88)
W jt.
k
j=1 j=1
E.5 Matrix formulation of Yˆ in Eq. (11)
Let k ∈ 1,K and t∈ 1,H . From Proposition D.1, we have
(cid:74) (cid:75) (cid:74) (cid:75)
 
L L
Yˆ kt =µˆ[X k]+(cid:88) (X kj −µˆ[X k])W jt− β γk(cid:112) σˆ2[X k]+ε1−(cid:88) W jt
k
j=1 j=1
 
L (cid:18) (cid:19) L
=(cid:88)
X kjW jt+ µˆ[X k]−
β γk(cid:112)
σˆ2[X k]+ε
·1−(cid:88)
W jt.
k
j=1 j=1
Gathering in matrix formulation concludes the proof.