AN ACCELERATED DISTRIBUTED STOCHASTIC GRADIENT
METHOD WITH MOMENTUM
APREPRINT
KunHuang ShiPu
TheChineseUniversityofHongKong,Shenzhen TheChineseUniversityofHongKong,Shenzhen
SchoolofDataScience(SDS) SchoolofDataScience(SDS)
Shenzhen,Guangdong,China Shenzhen,Guangdong,China
kunhuang@link.cuhk.edu.cn pushi@cuhk.edu.cn
AngeliaNedic´
ArizonaStateUniversity
SchoolofElectrical,ComputerandEnergyEngineering
Tempe,AZ,UnitedStates
angelia.nedich@asu.edu
February16,2024
ABSTRACT
Inthispaper,weintroduceanaccelerateddistributedstochasticgradientmethodwithmomentumfor
solvingthedistributedoptimizationproblem,whereagroupofnagentscollaborativelyminimizethe
averageofthelocalobjectivefunctionsoveraconnectednetwork. Themethod,termed“Distributed
StochasticMomentumTracking(DSMT)”,isasingle-loopalgorithmthatutilizesthemomentum
tracking technique as well as the Loopless Chebyshev Acceleration (LCA) method. We show
that DSMT can asymptotically achieve comparable convergence rates as centralized stochastic
gradientdescent(SGD)methodunderageneralvarianceconditionregardingthestochasticgradients.
Moreover,thenumberofiterations(transienttimes)requiredforDSMTtoachievesuchratesbehaves
(cid:112)
asO(n5/3/(1−λ))forminimizinggeneralsmoothobjectivefunctions,andO( n/(1−λ))under
thePolyak-Łojasiewicz(PL)condition. Here,theterm1−λdenotesthespectralgapofthemixing
matrixrelatedtotheunderlyingnetworktopology.Notably,theobtainedresultsdonotrelyonmultiple
inter-nodecommunicationsorstochasticgradientaccumulationperiteration,andthetransienttimes
aretheshortestunderthesettingtothebestofourknowledge.
1 Introduction
WeinvestigatehowagroupofnetworkedagentsN := {1,2,...,n}collaboratetosolvethefollowingdistributed
optimizationproblem:
n
1 (cid:88)
minf(x):= f (x), (1)
x∈Rp n i
i=1
whereeachagentihasaccessonlytoitslocalobjectivefunctionf : Rp → R. Problem(1)arisesinvariousfields,
i
includingsignalprocessing,distributedestimation,andmachinelearning. Specifically,solvinglarge-scaleproblems
suchastrainingfoundationmodels[3,6,54,30]inspecializedandcentralizedclusterscanbecostly[50,44],while
decentralizedtrainingoffersapromisingalternativethatrelievestherobustnessbottleneckandreducesthehighlatency
atthecentralserver[28]. Inthispaper,weconsiderthetypicalsettingwhereeachagentcanqueryunbiasednoisy
gradientsoff (x)intheformofg (x;ξ)thatsatisfiesacertainvariancecondition,whereξrepresentsarandomsample
i i
orabatchofrandomsamples.
4202
beF
51
]CO.htam[
1v41790.2042:viXraDistributedStochasticMomentumTracking APREPRINT
Decentralization may slow down the optimization process due to the partial communication over sparse networks.
Forinstance,thenumberofiterationsrequiredforthedistributedsubgradientdescent(DGD)method[29]toreach
certainaccuracythresholdsignificantlygrowswiththenetworksize[36],downgradingitsperformancecomparedto
thecentralizedsubgradientmethod. Whenstochasticgradientsareused,however,severaldistributedmethodssuchas
thosein[21,37,11,42,40,34,1,51,47,12,17]haveexhibitedtheso-called“asymptoticnetworkindependent(ANI)”
property[36]. Inotherwords,thesemethodsachievethesameconvergenceratesastheircentralizedcounterpartsaftera
finitetransienttimehaspassed. Suchapropertyensuresthattherequirednumberofiterationstoachievehighaccuracy
donotincreasesignificantlywiththenetworksize. Nevertheless,itisstillcriticaltodevelopalgorithmswithshorter
transienttimesthattypicallydependonthenetworksizeandtopology. Currently,thebestknowntransienttimesfor
minimizinggeneralsmoothobjectivefunctionsareO(n3/(1−λ)2)[1,12]andO(n/(1−λ))[11,1,51]withand
withoutthestrongconvexity(orthePolyak-Łojasiewiczcondition)ontheobjectivefunctions,respectively.
ChebyshevAcceleration(CA)hasbeenapopulartechniqueforachievingfasterconsensusamongnetworkedagents
duringthedistributedoptimizationprocess[38]. WhenCAiscombinedwithstochasticgradientaccumulationusing
a large batch size at every iteration, the optimal convergence guarantee can be achieved [24, 53]. However, CA
requiresinnerloopsofmultiplecommunicationstepswhichmayaffectthepracticalperformanceofthealgorithms,
and large batches are not always available. Inspired by the recently developed Loopless Chebyshev Acceleration
(LCA)techniquethatworkswithoutinnerloops[39],weconsiderinthispaperanewmethodtermed“distributed
stochasticmomentumtracking(DSMT)”thatincorporatestwokeyfeatures. First,theLCAtechniqueisutilizedto
acceleratethecommunicationprocessandavoidsmultiplecommunicationstepsbetweentwosuccessivestochastic
gradientcomputations. Second,eachagentemploysanauxiliaryvariabletotracktheaveragemomentumparameter
overthenetwork. WeshowthatDSMTimprovesthetransienttimesovertheexistingworksforminimizingsmooth
objectivefunctions,withorwithoutthePolyak-Łojasiewicz(PL)condition. Inparticular,theconvergenceratesof
DSMTarederivedunderthemostgeneralconditiononthestochasticgradientsunderthedistributedsetting[15,20,19],
enhancingtheapplicabilityofthemethod.
Itisworthnotingthat,theconvergenceresultofDSMTimpliesthemomentumparametercanbenefitthealgorithmic
convergenceofdistributedstochasticgradientmethods,particularlyrelatedtothenetworktopology.Suchanobservation
canpotentiallyinspirefuturedevelopmentofdistributedoptimizationalgorithms.
1.1 RelatedWorks
There exists a rich literature on the distributed implementation of stochastic gradient methods over networks. For
example,earlyworksincluding[4,27,5,32,33]suggestthatdistributedstochasticgradientmethodsmayachieve
comparableperformancewithcentralizedSGDunderspecificsettings. Morerecently,thepaper[21]firstdemonstrates
theANIpropertyenjoyedbythedistributedstochasticgradientdescent(DSGD)method,followedbythestudyin
[35,17,36]. Subsequentresearchthatconsidersgradienttrackingbasedmethods[47,34,16,46,49]andprimal-dual
likemethods[11,51,43,12]alsodemonstratetheANIpropertywhilerelievingfromthedataheterogeneitychallenge
encounteredbyDSGD.Bothtypesofmethodsareunifiedin[1].
Recently,thereexistsalineofworksthatachievetheoptimaliterationcomplexityindistributedstochasticoptimization
byintegratingstochasticgradientaccumulationandacceleratedcommunicationtechniquessuchasChebyshevAccel-
eration(CA)[53,24]. Inparticular,theworksin[53]and[24]obtaintheoptimaliterationcomplexityundersmooth
objectivefunctionswithandwithoutthePLcondition,respectively. Forfinitesumproblems,theworkin[25]achieves
theoptimalguaranteebyfurtherincorporatingavariancereductiontechnique.1 Nevertheless,CAthatreliesoninner
loopsofmultiplecommunicationstepsmaynotbecommunication-efficient,andsamplinglargebatchesforcomputing
thestochasticgradientsisnotalwayspractical[46]. Therefore,wearemotivatedtodevelopanefficientdistributed
stochasticgradientmethodthatsamplesconstantbatchesforcomputingthestochasticgradientsandperformsone
roundofcommunicationateveryiteration.
Themomentummethod[31]ispopularforacceleratingtheconvergenceoffirst-ordermethods. Recently,ithasbeen
applied to the distributed setting [7, 52, 45, 22], resulting in practical improvements. However, it remains unclear
whethermomentumparameterscanbenefittheconvergenceofdistributedstochasticgradientmethodstheoretically.
1.2 MainContribution
Themaincontributionofthispaperistwo-fold.
1Itisworthmentioningthatwhentheobjectivefunctionstakeafinitesumform,distributedrandomreshuffling(RR)methods
canfurtherimprovetheiterationcomplexity[9,13].
2DistributedStochasticMomentumTracking APREPRINT
Additional TransientTime TransientTime
Method g
Assumption Nonconvex PLcondition
(cid:16) (cid:17) (cid:16) (cid:17)
DSGD[21] (σ2,0) (ζ2,0)-BGD O n3 O n
(1−λ)4 (1−λ)2
(cid:16) (cid:17)
DSGD[37] (σ2,η2) (ζ2,0)-BGD / O n
(1−λ)2
(cid:16) (cid:17) (cid:16) (cid:17)
ED[1] (σ2,0) / O n3 O n
(1−λ)2 1−λ
(cid:16) (cid:17)
EDAS[11] (σ2,η2) / / O n
1−λ
(cid:16) (cid:110) (cid:111)(cid:17) (cid:16) (cid:110) (cid:111)(cid:17)
DSGT[1] (σ2,0) / O max n3 , n O max n , n1/3
(1−λ)2 (1−λ)8/3 1−λ (1−λ)4/3
(cid:16) (cid:17)
DSGT-HB[7] (σ2,0) / O n3 /
(1−λ)4
(cid:16) (cid:17)
QG-DSGDm[22] (σ2,0) (ζ2,0)-BGD O n3 /
(1−λ)4
Momentum (σ2,0) / O(cid:16) n3 (cid:17) /
Tracking[41] (1−λ)8
DSMT
ABC f lowerbounded
O(cid:16) n5/3(cid:17) O(cid:16)(cid:113)
n
(cid:17)
(Thispaper) i 1−λ 1−λ
Table1: Comparisonofdifferentmethodsregardingtheassumptionsandthetransienttimesforminimizingsmooth
objectivefunctions. The“g”columndescribestheconditionsassumedonthestochasticgradients,wherethenotation
(σ2,η2)representstherelaxedgrowthcondition,i.e.,E[∥g (x;ξ )−∇f (x)∥2|x]≤σ2+η2∥∇f (x)∥2,∀i∈N,and
i i i i
ABCstandsforAssumption1.2. Thenotation(ζ2,ψ2)-BGDstandsfortheboundedgradientdissimilarityassumption
that 1 (cid:80)n ∥∇f (x)−f(x)∥2 ≤ζ2+ψ2∥∇f(x)∥2.
n i=1 i
Firstly,theproposedDSMTmethodachievesimprovedconvergenceratesandbettertransienttimesovertheexisting
distributedstochasticgradientmethods(seeTable1). Specifically, forgeneralsmoothobjectivefunctions, DSMT
shortensthestate-of-the-arttransienttimefromO(n3/(1−λ)2)toO(n5/3/(1−λ)),where1−λdenotesthespectral
gapofthemixingmatrixthatmayscaleasO(1/n2)forsparsenetworks. Whentheobjectivefunctionsfurthersatisfy
(cid:112)
thePLcondition,DSMTshortensthestate-of-the-arttransienttimefromO(n/(1−λ))toO( n/(1−λ)).
Secondly,theconvergenceresultsofDSMTonlyassumestheso-calledABCcondition[15,20,19]onthestochastic
gradients,whichisthemostgeneralvarianceconditionunderthedistributedsettingstoourknowledge. Bycomparison,
existingworksdependontherelaxedgrowthcondition(RGC)ortheboundedvariance[2]regardingthestochastic
gradientsortheboundedgradientdissimilarity(BGD)assumptionconcerningthedataheterogeneityamongtheagents.
SuchageneralizationexpandsthepracticalapplicabilityofDSMTgiventhattheABCconditioncoverscounterexamples
thatdonotsatisfyRGC[10,15].
1.3 NotationandAssumptions
Throughoutthispaper,columnvectorsareconsideredbydefaultunlessspecifiedotherwise. Weusex ∈Rptodenote
i,k
theiterateofagentiatthek-thiteration. Forthesakeofclarityandpresentation,weintroducethestackedvariablesas
follows:
x :=(x ,x ,...,x )⊺ ∈Rn×p,
k 1,k 2,k n,k
∇F(x ):=(∇f (x ),∇f (x ),...,∇f (x ))⊺ ∈Rn×p,
k 1 1,k 2 2,k n n,k
(cid:18) (cid:19)
A
A := ∈R2n×p.
# A
Weusex¯∈Rptodenotetheaveragedvariablesofx amongtheagents.Forinstance,thevariablex¯ :=1/n(cid:80)n x
i k i=1 i,k
standsfortheaverageofalltheagents’iteratesatthek-thiteration. Weuse∥·∥todenotetheFrobeniusnormfora
matrixbydefaultandtheℓ normforavector. Theterm⟨a,b⟩standsfortheinnerproductoftwovectorsa,b∈Rp. For
2
twomatricesA,B
∈Rn×p,⟨A,B⟩isdefinedas⟨A,B⟩:=(cid:80)n
⟨A ,B ⟩,whereA (andB )representsthei-rowof
i=1 i i i i
A(andB).
Wenextintroducethestandingassumptions. Assumption1.1iscommonthatrequirestheobjectivefunctionstobe
smoothandlowerbounded.
3DistributedStochasticMomentumTracking APREPRINT
Assumption1.1. Eachf (x):Rp →RisL-smooth,i.e.,
i
∥∇f (x)−∇f (x′)∥≤L∥x−x′∥, ∀x,x′ ∈Rp.
i i
In addition, each f (x) is bounded from below, i.e., f (x) ≥ f∗ := inf f (x) > −∞,∀x ∈ Rp. We also denote
i i i x i
f∗ :=inf f(x).
x
Let the filtration {F } be generated by {ξ |i ∈ N,ℓ = 0,1,...,k −1}. We consider the ABC condition in
k k≥0 i,ℓ
Assumption1.2regardingthestochasticgradients.
Assumption1.2. Assumeeachagenthasaccesstoanconditionallyunbiasedstochasticgradientg (x;ξ )of∇f (x),
i i i
i.e.,E[g (x;ξ )|F ]=∇f (x),andthereexistconstantsC,σ ≥0suchthatforanyk ∈N,i∈[n],
i i,k k i
(cid:104) (cid:12) (cid:105)
E ∥g (x;ξ )−∇f (x)∥2(cid:12)F ≤C[f (x)−f∗]+σ2, (2)
i i,k i (cid:12) k i i
Inaddition,thestochasticgradientsareindependentacrossdifferentagentsateachk ≥0.
It is noteworthy that the relaxed growth condition (RGC) implies (2) given that each f has Lipschitz continuous
i
gradientsandislowerbounded[15],however,thereexistexamplesthatonlysatisfytheABCconditionbutnotRGC
[15,10]. Moreover,Assumption1.2generallyholdsfortheempiricalriskminimizationproblemsifthestochastic
gradientsarequeriedthroughuniformlysamplingwithreplacement[10].
WeassumetheagentsinthenetworkareconnectedviaagraphG = (N,E)withE ⊆ N ×N representingtheset
ofedgesconnectingtheagents. Inparticular, (i,i) ∈ E foralli ∈ N. Thesetofneighborsforagentiisdenoted
byN = {j ∈N :(i,j) ∈E}. Theelementw intheweightmatrixW ∈Rn×n representstheweightoftheedge
i ij
betweenagentsiandj. Regardingthenetworktopology,weconsiderAssumption1.3thatisstandardinthedistributed
optimizationliterature. Theconditionguaranteesthatthespectralnormλofthematrix(W −11⊺/n)isstrictlyless
thanone.
Assumption1.3. ThegraphG isundirectedandconnected,i.e.,thereexistsapathbetweenanytwonodesinG. There
isadirectlinkbetweeniandj (i̸=j)inG ifandonlyifw >0andw >0;otherwise,w =w =0. Themixing
ij ji ij ji
matrixW isnonnegativeanddoublystochastic,i.e.,1⊺W =1⊺andW1=1. Inaddition,weassumew >0forany
ii
i∈N.
1.4 Organization
Therestofthispaperisorganizedasfollows. InSection2,weintroducetheproposedDSMTalgorithmstartingwith
its motivation. We then proceed to conduct a preliminary analysis in Section 3. The main convergence results of
DSMTundersmoothobjectivefunctionswithorwithoutthePLconditionarepresentedinSection4. Wethenprovide
numericalexperimentsinSection5andconcludethepaperinSection6.
2 ADistributedStochasticMomentumTrackingMethod
Inthissection,weintroducetheproposedalgorithm,DistributedStochasticMomentumTracking(DSMT).Westart
withthemotivationandtheintuitionbehindDSMT.
2.1 Motivation
Theproposedalgorithmreliesheavilyontheso-calledLooplessChebyshevAcceleration(LCA)techniquestatedin
Lemma2.1below.
Lemma 2.1 (Lemma 11 in [39]). Suppose the mixing matrix W is symmetric and positive semidefinite. Define
√ √ √
η :=1/(1+ 1−λ2),thenρ˜ := η ∼O(1− 1−λ2),andforanyA∈Rn×pandk ≥0,
w w w
(cid:13) (cid:13)2
(cid:13)Π˜W˜kΠ˜A (cid:13) ≤c ρ˜2k∥ΠA∥2, c =14,
(cid:13) #(cid:13) 0 w 0
where
(cid:18)
Π
0(cid:19) 11⊺ (cid:18) A(cid:19) (cid:18)
(1+η )W −η
I(cid:19)
Π˜ := , Π:=I− , A := , W˜ := w w .
0 Π n # A I 0
4DistributedStochasticMomentumTracking APREPRINT
Theabovelemmaimpliesthat,byemployinganaugmentedmixingmatrixW˜,theconsensusrateamongmultiple
agents can be accelerated compared to applying the basic consensus procedure using the matrix W. In addition,
whenLCAisemployedindistributedoptimizationalgorithms,thereisnoneedtoperforminnerloopsofmultiple
communicationrounds. Forinstance,theOptimalGradientTracking(OGT)methodproposedin[39]achievesthe
optimalcommunicationandcomputationcomplexitiessimultaneouslywithoutinnerloopsofmultiplecommunication
roundsforminimizingsmoothstronglyconvexobjectivefunctionsusingfullgradients.
In the following discussion, we adhere to the notations introduced in Lemma 2.1. Additionally, we use [x˜ ] to
k 1:n
representthefirstnrowsofx˜ ∈R2n×p.
k
ItisnoteworthythatapplyingtheLCAtechniquetodistributedstochasticgradientmethodswhileachievingimproved
transient times is a nontrivial task. A recent attempt in [49] realized the same transient time O(n/(1 − λ)) as
those presented in [11, 1, 51]. We discuss the reasons of this result as follows. In [49], the LCA technique is
directlycombinedwiththedistributedstochasticgradienttracking(DSGT)methoddescribedin(3), whereg :=
k
(g (x ;ξ ),...,g (x ;ξ ))⊺ ∈Rn×pdenotesthestackedstochasticgradientatthek-thiteration.
1 1,k 1,k n n,k n,k
x =W (x −αy ), (3a)
k+1 k k
y =W (y +g −g ), y =g . (3b)
k+1 k k+1 k 0 0
Here, the auxiliary variables y track the averaged stochastic gradients among the agents since y¯ = g¯ based on
k k k
Assumption1.3andinductionfrom(3b). OnecommonapproachforanalyzingtheconvergenceofDSGTinvolves
estimatingthefollowingterm(see,e.g.,[34,49]or(29)):
(cid:104) (cid:105) α2C (cid:104) (cid:105)
E ∥x −1x¯⊺ ∥2 + 0 E ∥y −1y¯⊺ ∥2 , (4)
k k (1−λ)2 k k
forsomeconstantC >0thatisindependentofnand(1−λ). However,thisleadstoanunsatisfactoryperformanceof
0
thealgorithm,primarilyduetothebiasiny ,i.e.,E[y −g |F ]̸=0.
k k k k
Toalleviatethebiasiny , weintroducealternativevariables, z , fory totrack. Specifically, theupdatey isas
k k k k
follows:
y =W (y +z −z ). (5)
k+1 k k+1 k
Simultaneously,thevariablesz followtheupdatedefinedin(6):
k
z −z =−(1−β)(z −∇F(x ))+(1−β)[g −∇F(x )], (6)
k+1 k k k+1 k+1 k+1
whereβ ∈ (0,1). Withthehelpoftheadditionalterm(1−β)in(6),theimpactofthestochasticgradientvariance
isbettercontrolled,asdetailedinLemma3.7. Suchastrategicadjustmentenablesthecancellationoftheextraterm
1/(1−λ)introducedby(4).
Remark2.1. Substitutingtheupdate(5)and(6)intotheDSGTmethod(3)leadstothefollowingrelations.
x =W (x −αy ),
k+1 k k
z =βz +(1−β)g , (7)
k+1 k k+1
y =W (y +z −z ), y =z =(1−β)g .
k+1 k k+1 k 0 0 0
InvokingAssumption1.3,wecandeducethaty¯ =z¯ byinduction. Consequently,thebehaviorofx¯ asdescribedin
k k k
(8)resemblestheupdateoftheStochasticGradientDescentwithMomentum(SGDM)method[31]. Itturnsoutthaty
k
tracksthemomentumvariablez ,andthisiswhytheproposedmethodisnamed“distributedstochasticmomentum
k
tracking”.
x¯ =x¯ −αz¯ ,
k+1 k k
(8)
z¯ =βz¯ +(1−β)g¯ .
k+1 k k
Theabovediscussionsmotivatetheproposedmethodinthenextpart.
5DistributedStochasticMomentumTracking APREPRINT
2.2 TheProposedMethod
WebeginbypresentingthecompactformoftheDistributedStochasticMomentumTracking(DSMT)methodin(9). In
essence,DSMTleveragestheLooplessChebyshevAcceleration(LCA)techniquefortheupdate(7).
x˜ =W˜ (x˜ −α(y ) ) (9a)
k+1 k k #
x =[x˜ ] (9b)
k+1 k+1 1:n
z =βz +(1−β)g (9c)
k+1 k k+1
y˜ =W˜ (y˜ +(z ) −(z ) ) (9d)
k+1 k k+1 # k #
y =[y˜ ] , (9e)
k+1 k+1 1:n
whereW˜ definedinLemma2.1signifiestheapplicationoftheLCAtechnique,and(y ) := (y⊺ ,y⊺ )⊺ ∈ R2n×p.
k # k k
Consequently, the updates in (9b) and (9e) are required to obtain x and (y ) in the subsequent iterations.
k+1 k+1 #
Theupdatesin(9c)and(9d)alignwiththerationaleexplainedfor(7),representingacriticalstepinimprovingthe
convergenceratecomparedtothemethodoutlinedin[49].
TheformaldescriptionoftheDSMTmethodisoutlinedinAlgorithm1. Ateachiteration,agentifirstperformsan
approximategradientdescentstepinLine4toobtainx andxl . Then,aftercommunicatingwithneighboring
i,k+ 21 i,k+ 21
agentstheupdatedx inLine5,thenewiteratesx andxl areobtained. Subsequently,agentiqueriesa
i,k+1 i,k+1 i,k+1
2
noisygradientbasedonx andcompletestheupdateofz inLine7. Theremainingproceduresaimtotrack
i,k+1 i,k+1
theaveragedvariablez¯ giventheinitializationy =z =(1−β)g .
k 0 0 0
Algorithm1DistributedStochasticMomentumTracking(DSMT)
1: Initialize x i,0 = xl i,0 ∈ Rp for all agent i ∈ N, determine W = [w ij] ∈ Rn×n, stepsize α and parameter β.
Initializey =yl =z =(1−β)g . Inputη .
i,0 i,0 i,0 i,0 w
2: fork =0,1,2,...,K−1do
3: forAgenti=1,2,...,ninparalleldo
5
64
:
::
C
AU
o
cp qmd ua
m
it re
u
ex sni
i
a, ck
a
s+
t te
o1 2
ca
h=
n ad
sx
tu
ii cp,k
d
ga−
rt ae
dα
x
iy eii n,, kk
t+
ga
1
in ,k=d +x 1(l i 1, =k ++ g1 2
η
i(w=
x)
i,(cid:80)x kl i +, jk
1∈
;−
N
ξ
iiα ,kwy +ii 1j,k
x
).
j ∈,k+ R1 2 p.−η wxl i,k+ 21 andxl i,k+1 =x i,k+ 21.
7: Updatez i,k+1 =βz i,k+(1−β)g i,k+1.
98 :: CU op mda mte uy ni i, ck a+ te21 a= ndy ui, pk d+ atez i y,k i,+ k+1 1− =z i (,k 1a +nd
η
wy il ),k (cid:80)+ j1
2
∈= Niy wil , ik jy+ j,kz +i,k 21+ −1−
η
wz yi, ilk ,k.
+1
2
andy il
,k+1
=y
i,k+
21.
10: endfor
11: endfor
12: Outputx i,T forallagenti∈N.
Remark2.2. WecompareDSMTmethodwiththeDSGTmethod. ItisworthnotingthatDSMTnecessitatesadditional
informationofλcomparedtoDSGT.Thisiscommonforconstructingacceleratedoroptimalmethodsincludingthose
in [24, 53, 38]. Regarding the communication cost per iteration, each agent in the DSMT method communicates
variables(x ,y ) ,whichalignswiththecommunicationloadofDSGT.Theadditionalcostprimarilystemsfrom
i,k i,k k≥0
thestorageofvariables(xl ,yl ,z ) .
i,k i,k i,k k≥0
Remark 2.3. In the analysis, we introduce the definition z := 0. It is worth noting that the relation z =
−1 0
βz +(1−β)g remains valid due to the initialization z = (1−β)g in Algorithm 1. As a consequence, the
−1 0 0 0
subsequentanalysisinvolvingz isvalid.
−1
3 PreliminaryAnalysis
Inthissection,wepresentseveralpreliminaryresults. Westartwithestablishingtherecursionsinvolvingtheaveraged
variablesx¯ ,y¯ ,andz¯ .
k k k
Lemma3.1. LetAssumption1.3holdandy =z =(1−β)g ,wehaveforanyk ≥0that
0 0 0
x¯ =x¯ −αy¯ ,
k+1 k k
z¯ =βz¯ +(1−β)g¯ ,
k+1 k k+1
y¯ =z¯ .
k+1 k+1
6DistributedStochasticMomentumTracking APREPRINT
Proof. SeeAppendixA.1.
AshighlightedinRemark2.1,theupdateofx¯ canbeviewedasanapproximateversionoftheSGDMmethod. Such
k
anobservationpromptsustoinvestigatethebehaviorofaseriesofauxiliaryvariables{d¯ }definedasfollows:
k
(cid:40)
x¯ , k =0
d¯ := k (10)
k 1 x¯ − β x¯ , k ≥1.
1−β k 1−β k−1
Thebehaviorofd¯ ismoreliketheupdateofSGDcomparedtothatofx¯ ;seeLemma3.2below. Similararguments
k k
havebeenexploredinpriorworks,including[48,8,23].
Lemma3.2. LetAssumption1.3hold. Defined¯ asin(10),wehave
k
d¯ =d¯ −αg¯ , k ≥0,
k+1 k k
and
αβ
d¯ −x¯ =− z¯ , k ≥0. (11)
k k 1−β k−1
Proof. SeeAppendixA.2.
Next, we consider the relation between E[f(d¯ ) − f∗] and E[f(x¯ ) − f∗] in light of (11) in Lemma 3.2. As a
k k
consequence,thefollow-upanalysisneedsnotinvolvethetermE[f(x¯ )−f∗]whichmayappearduetoAssumption
k
1.2.
Lemma3.3. LetAssumptions1.1-1.3hold. Setthestepsizeαtosatisfyα≤(1−β)/(3βL). Wehave
E[f(x¯ )−f∗]≤E(cid:2) f(d¯ )−f∗(cid:3) + αβ E(cid:104) ∥∇f(x¯ )∥2(cid:105) + αβ E(cid:104) ∥z¯ ∥2(cid:105) .
k k 1−β k 1−β k−1
Proof. SeeAppendixA.3.
OurprimarygoalinthissectionistolocateandanalyzeaLyapunovfunctiontofacilitatetheconvergenceanalysis. We
startwithLemma3.4thatpresentstherecursionofE[f(d¯ )−f∗]inlightofAssumption1.1andLemma3.2.
k
Lemma3.4. LetAssumptions1.1-1.3hold. Setthestepsizeαtosatisfyα≤min{1/(4L),1/(2C)}. Wehave
E(cid:2) f(d¯ )−f∗(cid:3) ≤(cid:18) 1+ 2α2CL(cid:19) E(cid:2) f(d¯ )−f∗(cid:3) − α(cid:18) 1− 4α2CL (cid:19) E(cid:104) ∥∇f(x¯ )∥2(cid:105)
k+1 n k 2 n(1−β) k
αL2 (cid:104) (cid:105) α3β2L(L+2C) (cid:104) (cid:105) α2L(2Cσ∗+σ2)
+ E ∥Πx ∥2 + E ∥z¯ ∥2 + f ,
n k (1−β)2 k−1 n
whereσ∗ :=f∗− 1 (cid:80)n f∗.
f n i=1 i
Proof. SeeAppendixA.4.
Lemma3.4inspiresustoconsidertherecursionofE[∥z¯ ∥2]asfollows.
k
Lemma3.5. LetAssumptions1.1-1.3hold. Setthestepsizeαtosatisfyα≤min{(1−β)/(2βL),1/(4Cβ)}. Wehave
E(cid:104) ∥z¯ ∥2(cid:105) ≤ 1+β E(cid:104) ∥z¯ ∥2(cid:105) + 2C(1−β)2 E(cid:2) f(d¯ )−f∗(cid:3) +3(1−β)E(cid:104) ∥∇f(x¯ )∥2(cid:105)
k 2 k−1 n k k
(1−β)2(2Cσ∗+σ2) 2(1−β)L(cid:18) C(1−β)(cid:19) (cid:104) (cid:105)
+ f + L+ E ∥Πx ∥2 .
n n n k
Proof. SeeAppendixA.5.
WenextconsidertherecursionofE[∥z −∇F(1x¯⊺ )∥2].
k k
7DistributedStochasticMomentumTracking APREPRINT
Lemma3.6. LetAssumptions1.1-1.3hold. Setthestepsizeαtosatisfyα≤(1−β)/(3L). Wehaveforanyk ≥0that
E(cid:104)
∥z
k−∇F(1x¯⊺ k)∥2(cid:105) ≤βE(cid:104)(cid:13)
(cid:13)z
k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) +2αn(L+C)E(cid:104)
∥z¯
k−1∥2(cid:105)
+2(1−β)L(L+C)E(cid:104) ∥Πx ∥2(cid:105) +2n(1−β)2CE(cid:2) f(d¯ )−f∗(cid:3) +2αn(1−β)CE(cid:104) ∥∇f(x¯ )∥2(cid:105)
k k k
+n(1−β)2(2Cσ∗+σ2),
f
wherex :=x andz =0.
−1 0 −1
Proof. SeeAppendixA.6.
Next,weconsidertheconsensuserrortermsE[∥Πx ∥2]andE[∥Πy ∥2]similartothosein[39]. Oneofthekeysteps
k k
inthederivationofLemma3.7involvesidentifyingtheerrortermE[∥z −∇F(1x¯⊺ )∥2],asdiscussedinSubsection
k k
2.1. Thisparticularstepisessentialforpreservingthebenefitsofthegradienttrackingtechnique,whichreducesthe
impactofdataheterogeneity.
Lemma3.7. LetAssumptions1.1-1.3hold. Setthestepsizeαandtheparameterβ tosatisfy
(cid:40) (cid:115) (cid:114) (cid:41)
1 1−ρ˜ 1−β
α≤min , w , , 1>β ≥ρ˜ .
2(L+C) 6c2L(L+C) 4CL w
0
Then,thereexistsequences{Rx}and{Ry}suchthat
k k
(cid:104) (cid:105) (cid:20)(cid:13) (cid:13)2(cid:21)
E ∥Πx ∥2 ≤E (cid:13)Π˜x˜ (cid:13) ≤Rx,
k (cid:13) k(cid:13) k
(cid:104) (cid:105) (cid:20)(cid:13) (cid:13)2(cid:21)
E ∥Πy ∥2 ≤E (cid:13)Π˜y˜ (cid:13) ≤Ry,
k (cid:13) k(cid:13) k
and
α2c ρ˜
Rx ≤ρ˜ Rx+ 0 wRy, (12)
k+1 w k 1−ρ˜ k
w
Ry
k+1
≤ 1+ 2ρ˜ wRy k+ 15c 0(1− 1β −)2 ρ˜L(L+C) Rx k+ 3c 0 1(1 −− ρ˜β)2β E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
w w
+9αnc (1−β)(C+L)E(cid:104) ∥z¯ ∥2(cid:105) +12nc (1−β)2CE(cid:2) f(d¯ )−f∗(cid:3)
0 k−1 0 k
(cid:104) (cid:105)
+20αnc (1−β)(C+L)E ∥∇f(x¯ )∥2 +6nc (1−β)2(2Cσ∗+σ2), (13)
0 k 0 f
wherec =14isdefinedinLemma2.1and
0
α2c ρ˜ (cid:104) (cid:105)
Rx ≤c ∥Πx ∥2+ 0 wE ∥Πy ∥2 ,
0 0 0 1−ρ˜ 0
w (14)
(cid:104) (cid:105)
Ry :=c E ∥Πy ∥2 +nc ρ˜ (1−β)2(2Cσ∗+σ2).
0 0 0 0 w f
Proof. SeeAppendixA.7.
WearenowreadytointroducetheLyapunovfunctionL definedin(15):
k
L :=E(cid:2) f(d¯ )−f∗(cid:3) + 3αL2 Rx+ 12α3c 0ρ˜ wL2 Ry
k k n(1−ρ˜ ) k n(1−ρ˜ )3 k
w w
(15)
+
4α3 (L 1( −L β+ )32C) E(cid:104)
∥z¯
k−1∥2(cid:105)
+
4 n8 (α 13 −c2 0ρ ρ˜ ˜wL )32 E(cid:104)(cid:13)
(cid:13)z
k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
.
w
Lemma3.8belowpresentstherecursionforL bycombiningtheresultsinLemmas3.4-3.7.
k
8DistributedStochasticMomentumTracking APREPRINT
Lemma3.8. LetAssumptions1.1-1.3hold. Setthestepsizeαandtheparameterβ tosatisfy
(cid:40)(cid:115) (1−ρ˜ )2 1−β (cid:18) (1−ρ˜ )3 (cid:19)1/3(cid:41)
α≤min w , , w ,
240c2L(L+C) 24(L+C) 4032c2(1−β)L2(C+L)
0 0
1>β ≥ρ˜ .
w
Wehave
(cid:18) 4α2CL 240α3c2ρ˜ CL2(1−β)2(cid:19) α (cid:104) (cid:105) α2L(2Cσ∗+σ2)
L ≤ 1+ + 0 w L − E ∥∇f(x¯ )∥2 + f
k+1 n (1−ρ˜ )3 k 4 k n
w
(cid:20) 72c2ρ˜ L2(1−β)2 4L(L+2C)(cid:21)
+ 0 w + α3(2Cσ∗+σ2).
(1−ρ˜ )3 n(1−β) f
w
Moreover,denote∆ :=f(x¯ )−f∗,thetermL canbeupperboundedby
0 0 0
3αL2 12α3c ρ˜ L2 48α3c2ρ˜ L2
L =∆ + Rx+ 0 w Ry+ 0 w ∥∇F(x )∥2
0 0 n(1−ρ˜ ) 0 n(1−ρ˜ )3 0 n(1−ρ˜ )3 0
w w w
(16)
9αL2∥Πx ∥2 27α3c2L2(1−β)2(2Cσ∗+σ2) 72α3c2L2(cid:80)n ∥∇f (x¯ )∥2
≤2∆ + 0 + 0 f + 0 i=1 i 0 .
0 n(1−ρ˜ ) (1−ρ˜ )2 n(1−ρ˜ )3
w w w
Proof. SeeAppendixA.8.
Insummary,Lemma3.4inspiresustoderivetherecursionsofthecorrespondingerrorterms,outlinedinLemmas
3.5-3.7,andtoconsidertheLyapunovfunctionin(15). Moreover,duetothefactthat∥Π˜W˜∥ >1,theconsensuserror
1
termsE[∥Πx ∥2]andE[∥Πy ∥2]areprovedtobeR-linearsequenceswith“additionalerrors”similarasthosein[39].
k k
Notably,theanalysisofDSMTdiffersfrom[39]mainlyduetotheneedfordealingwiththestochasticgradientsunder
thegeneralvariancecondition(2)andthemomentumvariablez .
k
4 MainResults
Inthissection,wepresentthemainresults,whichoutlinetheconvergenceratesandthetransienttimesoftheDSMT
methodforminimizingsmoothobjectivefunctionswithorwithoutthePLcondition. Theseresultsaredetailedin
Subsections4.1and4.2,respectively. TheformaldefinitionsoftransienttimesK(NCVX)andK(PL) undergeneral
Transient Transient
smoothobjectivefunctionsandsmoothobjectivefunctionssatisfyingthePLconditionaredefined,asfollows:
(cid:26) (cid:104) (cid:105) (cid:18) 1 (cid:19) (cid:27)
K(NCVX) :=inf min E ∥∇f(x¯ )∥2 ≤O √ , ∀k ≥K (17a)
Transient K t=0,1,...,k−1 t nk
(cid:40) n (cid:18) (cid:19) (cid:41)
K(PL) :=inf 1 (cid:88) E[f(x )−f∗]≤O 1 , ∀k ≥K . (17b)
Transient K n i,k nk
i=1
√
NotethatO(1/ nk)andO(1/(nk))representtheconvergenceratesofthecentralizedstochasticgradientmethod
underthetwoconsideredconditions,respectively.
4.1 GeneralNonconvexCase
WebeginbypresentingtheconvergenceresultofDSMTforminimizingsmoothnonconvexobjectivefunctions,as
demonstratedinTheorem4.1below.
Theorem4.1. LetAssumptions1.1-1.3hold. ForagivenK ≥1,letthestepsizeαandtheparameterβ tosatisfy
(cid:40)(cid:115)
(1−ρ˜ )2 1−β (cid:18) (1−ρ˜ )3 (cid:19)1/3
α≤min w , , w ,
240c2L(L+C) 24(L+C) 4032c2(1−β)L2(C+L)
0 0
(cid:114) n (cid:18) (1−ρ˜ )3 (cid:19)1/3(cid:41)
, w , 1>β ≥ρ˜ .
8CLK 480c2CL2(1−β)2K w
0
9DistributedStochasticMomentumTracking APREPRINT
Then,wehave
(cid:104) (cid:105) 24∆ 108L2∥Πx ∥2 324c2α2L2(1−β)2(2Cσ∗+σ2)
min E ∥∇f(x¯ )∥2 ≤ 0 + 0 + 0 f
k=0,1,···,K−1 k αK n(1−ρ˜ w)K (1−ρ˜ w)2K
4αL(2Cσ∗+σ2) 864c2α2L2(cid:80)n ∥∇f (x¯ )∥2
+ f + 0 i=1 i 0 (18)
n n(1−ρ˜ )3K
w
(cid:20) L(L+2C) 18c2ρ˜ L2(1−β)2(cid:21)
+ + 0 w 16α2(2Cσ∗+σ2).
n(1−β) (1−ρ˜ )3 f
w
Inaddition,ifweset
1 1
α= , β =1− (1−ρ˜ ),
(cid:113) L(2Cσ∗+σ2)K n1/3 w (19)
f +γ
3n∆0
where
(cid:18) 4032c2L2(C+L)(cid:19)1/3 24c n1/3(L+C) (cid:114) 8CLK (cid:18) 480c2CL2K (cid:19)1/3
γ := 0 + 0 + + 0 ,
n1/3(1−ρ˜ )2 1−ρ˜ n n2/3(1−ρ˜ )
w w w
then
(cid:115)
(cid:114)
k=0,1m ,··i ·n
,K−1E(cid:104)
∥∇f(x¯
k)∥2(cid:105)
=O
∆ 0L(2C nKσ f∗+σ2)
+
∆ n2
0
KCL
+
n2/9(∆
10
−(C ρ˜L w2 )) 11 // 33
K2/3
(20)
n1/3∆ (L+C)+∥Πx ∥2L2/n n1/3∆ L ∆ L(cid:80)n ∥∇f (x¯ )∥2 (cid:33)
+ 0 0 + 0 + 0 i=1 i 0 ,
(1−ρ˜ )K K2 (1−ρ˜ )3(2Cσ∗+σ2)K2
w w f
whereO(·)hidesthenumericalconstants.
Proof. WefirststateLemma4.1from[26,Lemma6]thatprovidesadirectlinkconnectingLemma3.8tothedesired
results.
Lemma4.1. Ifthereexistconstantsa,b,c≥0andnon-negativesequences{s },{q }suchthatforanyt,wehave
t t
s ≤(1+a)s −bq +c,
t+1 t t
thenitholdsthat
(1+a)T c
min q ≤ s + .
t=0,1,...,T−1 t bT 0 b
ApplyingLemma4.1toLemma3.8leadsto
min
E(cid:104)
∥∇f(x¯
)∥2(cid:105)
≤
4(cid:16) 1+ 4α2 nCL + 240α3c (2 0 1ρ˜ −w ρC ˜wL )2 3(1−β)2(cid:17)K
L +
4αL(2Cσ f∗+σ2)
k=0,1,···,K−1 k αK 0 n (21)
(cid:20) L(L+2C) 18c2ρ˜ L2(1−β)2(cid:21)
+ + 0 w 16α2(2Cσ∗+σ2).
n(1−β) (1−ρ˜ )3 f
w
Letting
(cid:40)(cid:114) n (cid:18) (1−ρ˜ )3 (cid:19)1/3(cid:41)
α≤min , w
8CLK 480c2CL2(1−β)2K
0
yields
(cid:18) 4α2CL 240α3c2ρ˜ CL2(1−β)2(cid:19)K (cid:26)(cid:20) 4α2CL 240α3c2ρ˜ CL2(1−β)2(cid:21) (cid:27)
1+ + 0 w ≤exp + 0 w K ≤exp(1).
n (1−ρ˜ )3 n (1−ρ˜ )3
w w
(22)
10DistributedStochasticMomentumTracking APREPRINT
Substituting(22)andtheexpressionofL in(16)into(21)yields(18).
0
Setαasin(19),then
(cid:115)
∆
0 =
∆ 0L(2Cσ f∗+σ2)
+∆
(cid:114) 8CL
+
∆ 0(480c2 0CL2)1/3
αK 3nK 0 nK n2/9(1−ρ˜ )1/3K2/3
w
∆ [4032c2L2(C+L)]1/3 24c ∆ n1/3(L+C)
+ 0 0 + 0 0 , (23)
n1/9(1−ρ˜ )2/3K (1−ρ˜ )K
w w
(cid:115)
4αL(2Cσ f∗+σ2) ≤4 3L(2Cσ f∗+σ2)∆ 0 , α2 ≤ 3n∆ 0 .
n nK L(2Cσ∗+σ2)K
f
Substituting(23)into(18)yieldsthedesiredresult(20).
Subsequently,wecomputethetransienttimerequiredforDSMTtoachievecomparableperformancetothecentralized
SGDmethodwhenminimizingsmoothobjectivefunctions,asdetailedinCorollary4.2below.
Corollary4.2. LetAssumptions1.1-1.3hold. Setthestepsizeαandtheparameterβ tosatisfy(19). Thenthetransient
timeK(NCVX) requiredforDSMTmethodtoachievecomparableperformanceasthecentralizedSGDmethodbehaves
Transient
as
  (cid:16) (cid:17)2/3
K(NCVX)
=max O(cid:18) n5/3 (cid:19) ,O(cid:32) ∥Πx 0∥4 (cid:33) ,On1/3 (cid:80)n i=1∥∇f i(x¯ 0)∥2 
. (24)
Transient 1−λ (1−λ)n  1−λ 
 
Inaddition,ifweinitializeallagentsatthesamesolutionandassumethat(cid:80)n ∥∇f (x¯ )∥2 =O(n2),thetransient
i=1 i 0
timeK(NCVX) statedin(24)furtherreducesto
Transient
(cid:18) n5/3 (cid:19)
K(NCVX) =O .
Transient 1−λ
Remark4.1. Itisworthnotingthatinitializingalltheagentsatthesamesolutionandassuming(cid:80)n ∥∇f (x¯ )∥2 =
i=1 i 0
O(n2)aremildconditions. Inpractice,theterm(cid:80)n ∥∇f (x¯ )∥2oftenbehavesasO(n)assumingx¯ isfixed.
i=1 i 0 0
Proof. Wehidetheconstantsthatareindependentofnand(1−λ)inthefollowing. From(20),wehave
(cid:104) (cid:105) (cid:18) 1 (cid:19) (cid:18) n5/(18) n5/6
min E ∥∇f(x¯ )∥2 =O √ O 1+ + √
k=0,1,···,K−1 k nK (1−ρ˜ w)1/3K1/6 (1−ρ˜ w) K
∥Πx ∥2 n5/6 √ n(cid:80)n ∥∇f (x¯ )∥2(cid:33) (25)
+ 0√ + + i=1 i 0 .
(1−ρ˜ w) nK K3/2 (1−ρ˜ w)3K3/2
Notingthat
(cid:115) √
1 1−λ2 (cid:16)√ (cid:17)
1−ρ˜ w =1− √ = (cid:112) √ (cid:16) (cid:112) √ (cid:17) ∼O 1−λ . (26)
1+ 1−λ2 1+ 1−λ2 1+ 1+ 1−λ2
Accordingtothedefinitionfortransienttimesin(17a),weobtainthedesiredresult(24)byinvoking(25)and(26).
4.2 PLCondition
In this part, we will consider a specific nonconvex condition known as the Polyak-Łojasiewicz (PL) condition in
Assumption4.3. Overparameterizedmodelsoftensatisfythiscondition. Notably,thestrongconvexityconditionimplies
thePLcondition[14].
11DistributedStochasticMomentumTracking APREPRINT
Assumption4.3. Thereexistsµ>0,suchthattheaggregatefunctionf(x)= 1 (cid:80)n f (x)satisfies
n i=1 i
2µ(f(x)−f∗)≤∥∇f(x)∥2, (27)
forallx∈Rp,wheref∗ :=inf x∈Rpf(x).
InlightofAssumption4.3,weconstructa“contractive”recursionforthequantityE[f(d¯ )−f∗].
k
Lemma4.4. LetAssumptions1.1-1.3and4.3hold,andletK >0. Setd¯ asin(10)andletthestepsizeαtosatisfy
k
α≤min{µ/(4LC),(1−β)/(2L)}. Wehave
E(cid:2) f(d¯ )−f∗(cid:3) ≤(cid:16) 1− αµ(cid:17) E(cid:2) f(d¯ )−f∗(cid:3) + αL2 E(cid:104) ∥Πx ∥2(cid:105) + α2L(2Cσ f∗+σ2)
k+1 2 k n k n
α3βL(L+C) (cid:104) (cid:105)
+ E ∥z¯ ∥2 .
(1−β)2 k−1
Proof. SeeAppendixB.1.
ThankstoAssumption4.3,wearenowabletoshowthatL isaQ-linearsequencewith“additionalerrors”inLemma
k
4.5. ThederivationissimilartothoseintheproofofLemma3.8.
Lemma4.5. LetAssumptions1.1-1.3and4.3hold,andletk ≥0. Setthestepsizeαandtheparameterβ tosatisfy
(cid:40) (cid:115) (cid:115) (cid:115) (cid:41)
1−β 1−β (1−ρ˜ )2 1−ρ˜ µ(1−β)2
α≤min , , w , w , ,
3µ 486c2(L+C) 240c2L(L+C) 324c2(L+C)2 960c2L(L+C)2
0 0 0 0
11(1−ρ˜ )
1≥β ≥1− w .
12
Wehave
(cid:16) αµ(cid:17) (cid:20) L 72αc2ρ˜ L2(1−β)2 4αL(L+2C)(cid:21)
L ≤ 1− L + + 0 w + α2(2Cσ∗+σ2). (28)
k+1 4 k n (1−ρ˜ )3 n(1−β) f
w
Proof. SeeAppendixB.2.
WecanobtaintheconvergencerateofDSMTinTheorem4.2bynotinganotherLyapunovfunctionH :
k
H
k
:=Rx k+ (3 1α −2c ρ˜0ρ˜ w )2Ry k+ 405α (13n −c2 0 ρ˜(C )3+L) E(cid:104) ∥z¯ k−1∥2(cid:105) + 5 (14α −2c ρ˜2 0ρ˜ )w 2E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) . (29)
w w w
Theorem4.2. LetAssumptions1.1-1.3and4.3hold. Setthestepsizeαandtheparameterβ tosatisfy
(cid:40) (cid:115) (cid:115) (cid:115) (cid:41)
1−β 1−β (1−ρ˜ )2 1−ρ˜ µ(1−β)2
α≤min , , w , w , , (30)
3µ 486c2(L+C) 240c2L(L+C) 324c2(L+C)2 960c2L(L+C)2
0 0 0 0
11(1−ρ˜ )
1≥β ≥1− w . (31)
12
Wehave
1 (cid:88)n
E[f(x )−f∗]≤33L
(cid:16)
1−
αµ(cid:17)k
+
5LH
0
(cid:18) 5+β(cid:19)k
+
20αL(2Cσ f∗+σ2)
n i,k 0 4 n 6 nµ
i=1
(cid:20) 48LS 504c2(1+S )(1−β) 32(6L+C)(1−ρ˜ )(cid:21)4α2L(2Cσ∗+σ2)
+ 1 + 0 2 + w f ,
µ 1−ρ˜ nµ(1−β) 1−ρ˜
w w
where
18c2(1−β)2 (L+2C)(1−ρ˜ ) L LS
S := 0 + w , S := + 1,
1 (1−ρ˜ )2 nL(1−β) 2 nµ µ
w
and
3α2c ρ˜ 54α2c2ρ˜
H =Rx+ 0 w Ry+ 0 w ∥∇F(1x¯⊺ )∥2
0 0 (1−ρ˜ )2 0 (1−ρ˜ )2 0
w w
(32)
≤2c ∥Πx ∥2+
12α2nc2 0(1−β)2(C∆ 0+2Cσ f∗+σ2)
+
72α2c2 0(cid:80)n i=1∥∇f i(x¯ 0)∥2
.
0 0 (1−ρ˜ )2 (1−ρ˜ )2
w w
12DistributedStochasticMomentumTracking APREPRINT
Proof. SeeAppendixB.3.
BasedonTheorem4.2,wecanfirstcomparethenon-exponentiallydecreasingtermsintheerrorboundsfordifferent
distributedstochasticgradientmethodsunderaconstantstepsizeα;seeTable2. ItcanbeseenthattheDSMTmethod
(cid:16) (cid:112) (cid:17)
exhibitstheloweststaticerrorwhencomparedtothestate-of-artmethodsbychoosingβ = O 1− (1−λ)/n ,
n≥2.
Method StaticError
(cid:16) (cid:17)
DSGD[37] O α + α3/2 + α2
n 1−λ (1−λ)2
(cid:16) (cid:17)
ED[1] O α + α2 + α4
n 1−λ n(1−λ)3
(cid:16) (cid:17)
DSGT[1] O α + α2 + α4
n 1−λ n(1−λ)4
(cid:16) (cid:104) (cid:105) (cid:17)
DSMT O α + 1−β + 1 α2
n 1−λ n(1−β)
(cid:18) (cid:19)
DSMT
(cid:112) O α + √ α2
β =O(1− (1−λ)/n) n n(1−λ)
Table2: Comparisonofdifferentmethodsregardingthenon-exponentiallydecreasingtermsforminimizingsmooth
objectivefunctionssatisfyingthePLcondition.
Table2alsoshowsthatproperchoiceofβ improvesthefinalerrorofDSMT,demonstratingthebenefitsofmomentum
accelerationindecentralizedoptimizationmethods.
Byconsideringspecificstepsizeαandparameterβ,wecanwritedownthefollowingconvergenceresultofDSMT
basedonTheorem4.2.
Corollary4.6. LettheconditionsinTheorem4.2hold. Furthermore,letthestepsizeαandparameterβ satisfy
(cid:32) (cid:33)
4 33nµ2KL (1−ρ˜ )
α=α = ln 0 , β =1− √ w ,n≥2, (33)
1 µK (2Cσ∗+σ2)L n
f
forsomegivenK ≥1. Then,thefinaliteratesx foralli∈N satisfy
i,K
n (cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:88) E[f(x )−f∗]≤33L exp −αµK + 5LH 0 exp −(1− √ρ˜ w)K
n i,K 0 4 n n
i=1
(34)
(cid:32) (cid:33)
(2Cσ∗+σ2) (2Cσ∗+σ2)
+O˜ f + √ f ,
nK 1−λK2
(cid:16) (cid:17)
whereO˜(·)hidesthetermln 33nµ2KL0 andsomeconstantsthatareindependentofnand(1−λ). Inparticular,
(2Cσ∗+σ2)L
f
wehave
1 (cid:88)n
E[f(x
)−f∗]=O˜(cid:40)(cid:34)
∥Πx ∥2+
(∆ 0+Cσ f∗+σ2)
+
(cid:80)n i=1∥∇f i(x¯ 0)∥2L2(cid:35) exp(cid:18) −(1− √ρ˜ w)K(cid:19)
n i,K 0 µ2K2 µ2(1−ρ˜ )2K2 6 n
w
i=1
(cid:34) L2∥Πx ∥2 L2(2Cσ∗+σ2) L2(cid:80)n ∥∇f (x¯ )∥2(cid:35) (cid:18) (1−ρ˜ )µK(cid:19)
+ ∆ + 0 + f + i=1 i 0 exp − w
0 nµ(1−ρ˜ )K nµ3K3 µ3(1−ρ˜ )3K3 4L
w w
(cid:41)
L(Cσ∗+σ2) L(Cσ∗+σ2)
+ f + √ f .
nµ2K n(1−ρ˜ )µ2K2
w
(35)
√
Proof. Sinceβ =1−(1−ρ˜ )/ n,theconstantsS andS become
w 1 2
18c2 L+2C L 18c2L L+2C
S = 0 + √ , S = + 0 + √ .
1 n nL 2 nµ nµ nµ
13DistributedStochasticMomentumTracking APREPRINT
Denote
48L(cid:18) 18c2 L+2C(cid:19) 32(6L+C)
S := √ 0 + +504c2(1+S )+ ∼O(1),
3 µ n L 0 1 µ
wehave
n (cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:88) E[f(x )−f∗]≤33L exp −αµK + 5LH 0 exp −(1− √ρ˜ w)K
n i,K 0 4 n 6 n
i=1 (36)
20αL(2Cσ∗+σ2) 4α2S L(2Cσ∗+σ2)
f + √3 f .
nµ n(1−ρ˜ )
w
Letα¯satisfy(30). Itsufficestodiscussthefollowingtwocases:
CaseI:Ifα¯ <α ,thenwesubstituteα¯into(36)andobtain
1
n (cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:88) E[f(x )−f∗]≤33L exp −α¯µK + 5LH 0 exp −(1− √ρ˜ w)K
n i,K 0 4 n 6 n
i=1
(cid:16) (cid:17) (cid:16) (cid:17) (37)
80L(2Cσ∗+σ2)ln 33nµ2KL0 64LS (2Cσ∗+σ2)ln2 33nµ2KL0
f (2Cσ∗+σ2)L 3 f (2Cσ∗+σ2)L
+ f + √ f .
nµ2K n(1−ρ˜ )µ2K2
w
√
Inthiscase,wehaveα¯ ∼O((1−ρ˜ )/[ n(L+C)]). Then,
w
1 (cid:88)n
E[f(x
)−f∗]=O˜(cid:40)(cid:34)
∥Πx ∥2+
(C∆ 0+Cσ f∗+σ2)
+
(cid:80)n i=1∥∇f i(x¯ 0)∥2(cid:35) exp(cid:18) −(1− √ρ˜ w)K(cid:19)
n i,K 0 µ2K2 µ2(1−ρ˜ )2K2 6 n
w
i=1
(cid:34) L2∥Πx ∥2 L2(2Cσ∗+σ2) L2(cid:80)n ∥∇f (x¯ )∥2(cid:35) (cid:18) (1−ρ˜ )µK(cid:19)
+ ∆ + 0 + f + i=1 i 0 exp − √ w
0 nµ(1−ρ˜ )K nµ3K3 µ3(1−ρ˜ )3K3 4 n(L+C)
w w
(cid:41)
L(Cσ∗+σ2) L(Cσ∗+σ2)
+ f + √ f .
nµ2K n(1−ρ˜ )µ2K2
w
(38)
CaseII:Ifα¯ ≥α ,thenwesubstituteα into(36)andobtain
1 1
1 (cid:88)n
E[f(x )−f∗]≤
(2Cσ f∗+σ2)L
+
5LH
0
exp(cid:18) −(1− √ρ˜ w)K(cid:19)
n i,K nµ2K n 6 n
i=1
(cid:16) (cid:17) (cid:16) (cid:17) (39)
80L(2Cσ∗+σ2)ln 33nµ2KL0 64LS (2Cσ∗+σ2)ln2 33nµ2KL0
f (2Cσ∗+σ2)L 3 f (2Cσ∗+σ2)L
+ f + √ f .
nµ2K n(1−ρ˜ )µ2K2
w
Inthiscase,wehavefrom(39)that
1 (cid:88)n
E[f(x
)−f∗]=O˜(cid:40)(cid:34)
∥Πx ∥2+
(∆ 0+Cσ f∗+σ2)
+
(cid:80)n i=1∥∇f i(x¯ 0)∥2L2(cid:35) exp(cid:18) −(1− √ρ˜ w)K(cid:19)
n i,K 0 µ2K2 µ2(1−ρ˜ )2K2 6 n
w
i=1
(cid:41)
L(Cσ∗+σ2) L(Cσ∗+σ2)
+ f + √ f .
nµ2K n(1−ρ˜ )µ2K2
w
(40)
Combining(37)and(39)yieldsthedesiredresult(34). Combining(38)and(40)leadstothedesiredresult(35).
Finally,weareabletoderivethetransienttimeofDSMTundersmoothobjectivefunctionssatisfyingthePLcondition.
Corollary4.7. LettheconditionsinCorollary4.6hold. ThenthetransienttimeK(PL) requiredforDSMTtoachieve
Transient
comparableperformanceascentralizedSGDmethodwhenminimizingsmoothobjectivefunctionssatisfyingthePL
conditionbehavesas
(cid:18)(cid:114) (cid:19)
n
K(PL) =O˜ , (41)
Transient 1−λ
14DistributedStochasticMomentumTracking APREPRINT
Figure1: Illustrationofringgraphtopologywithn=16.
whereO˜(·)hidesthefollowinglogarithmtermsandotherconstantsthatareindependentofnand(1−λ):
(cid:40) (cid:16) (cid:17) (cid:32) 33nµ2KL (cid:33) (cid:16)n(cid:17) (cid:32) n(cid:80)n ∥∇f (x¯ )∥2(cid:33) (cid:32) ∥Πx ∥2(cid:33)(cid:41)
max ln n∥Πx ∥2K ,ln 0 ,ln ,ln i=1 i 0 ,ln √ 0 .
0 (2Cσ∗+σ2)L K (1−λ)3/2K 1−λ
f
Proof. Wehidetheconstantsthatareindependentofnand(1−λ)inthefollowing. Basedon(35),wehave
1
(cid:88)n
E[f(x
)−f∗]=O(cid:18)
1
(cid:19) O˜(cid:40)(cid:34)
nK∥Πx ∥2+
n
+
n(cid:80)n
i=1∥∇f i(x¯
0)∥2(cid:35) exp(cid:18)
−(1− √ρ˜
w)K(cid:19)
n i,K nK 0 K (1−ρ˜ )2K n
w
i=1
(cid:34) ∥Πx ∥2 1 n(cid:80)n ∥∇f (x¯ )∥2(cid:35) (cid:18) (1−ρ˜ )K(cid:19) √ n (cid:41) (42)
+ nK+ 0 + + i=1 i 0 exp − √w +1+ .
(1−ρ˜ ) K2 (1−ρ˜ )3K2 n (1−ρ˜ )K
w w w
√
InvokingthedefinitionofK(PL) andtherelation(1−ρ˜ )∼O( 1−λ)in(26)yieldsthedesiredresult(41).
Transient w
5 Simulations
ThissectionpresentstwonumericalexamplesthatillustratetheperformanceofDSMTcomparedwiththeexisting
methodsoveraringgraph(Figure1). Weevaluatethesealgorithmsonastronglyconvexproblem(43)(satisfyingthe
PLcondition)andageneralnonconvexproblem(44)toverifythetheoreticalfindings. Overall,theresultsdemonstrate
thattheincorporationofeitherthemomentumtrackingtechniqueorLooplessChebyshevAcceleration(LCA)enhances
theperformanceofdistributedstochasticgradientmethods. Furthermore,theDSMTmethodoutperformsexisting
methodsduetotheeffectivecombinationofthesetwotechniques.
5.1 LogisticRegression
Weconsiderabinaryclassificationproblemusinglogisticregression(43)ontheCIFAR-10[18]dataset. Eachagent
possessesadistinctlocaldatasetS ={(u ,v )}selectedfromthewholedatasetS. Here,thevariableu ∈Rpdenotes
i j j j
theimageinputandv ∈Rrepresentsthelabel. Inparticular,weconsideraheterogeneousdatasetting,wheredata
j
samplesaresortedbasedontheirlabelsandpartitionedamongtheagents. Theclassifiercanthenbeobtainedbysolving
thefollowingoptimizationproblemusingalltheagents’localdatasetsS ,i=1,2,...,n:
i
n
1 (cid:88)
minf(x)= f (x),
x∈Rp n i
i=1 (43)
f (x):= 1 (cid:88) log[1+exp(−x⊺ u v )]+ ρ ∥x∥2,
i |S | j j 2
i
j∈Si
15DistributedStochasticMomentumTracking APREPRINT
whereρissetas0.2.
Weevaluatetheperformanceofseveralmethods,includingDSMT,DSGT[37],EDAS[11],DSGD[37],DSGT-HB
[7],centralizedSGD(CSGD),andcentralizedSGDM(CSGDM),inthecontextofclassifyingairplanesandtrucks
ontheCIFAR-10datasetusingaconstantstepsize. TheresultsarepresentedinFigure2. Additionally,tofacilitate
comparison,weimplementtheDSMTmethodwithouttheLCAtechnique,denotedasDSMT_noLCAinsubsequent
discussions.
102 102
100 100
CSGDM CSGDM
10 2 C DS SG GD D 10 2 C DS SG GD D
EDAS EDAS
DSGT DSGT
10 4 D DS SM GTT -HB 10 4 D DS SM GTT -HB
DSMT_noLCA DSMT_noLCA
0 1000 2000 3000 4000 5000 6000 7000 8000 0 1000 2000 3000 4000 5000 6000 7000 8000
# Iterations # Iterations
(a)Ringgraph,n=100,1−λ=6.6×10−4. (b)Ringgraph,n=50,1−λ=2.6×10−3.
Figure2: ComparisonamongDSMT,DSGT,EDAS,DSGD,CSGD,andCSGDMforsolvingProblem(43)onthe
CIFAR-10 dataset using a constant stepsize. The stepsize is set as α = 0.01 for all the methods. The momentum
parameter is set as β = ρ˜ for DSMT, DSMT_noLCA, and SGDM. The results are averaged over 10 repeated
w
experiments.
TheperformanceofdecentralizedmethodsshowninFigures2aand2brevealsthat, asthenetworksizedecreases
(resultinginalargervalueof(1−λ)),themethodstendtoexhibitmorecomparableperformancetocentralizedSGD.
Amongthesedecentralizedmethods, DSMTachievesthemostsimilaraccuracycomparedtocentralizedmethods,
especiallywhenthenetworktopologyisnotwell-connected(Figure2a).
Comparingthosedecentralizedmethodsincorporatingmomentumacceleration,theyallbehavemoresimilartoCSGDM
than CSGD, aligning with the arguments in Subsection 2.1. However, a comparison between DSMT_noLCA and
DSGT-HB reveals the importance of appropriately employing the momentum technique to enhance the practical
performance. Straightforwardcombinationmaynotyieldsignificantimprovements,asseenfromtheperformance
comparison of DSGT and DSGT-HB in Figure 2. Such a comparison implies the effectiveness of the momentum
trackingtechnique.
ComparingtheperformanceofDSMTandDSMT_noLCAinFigure2,itcanbeconcludedthattheLCAtechnique
indeedacceleratestheconvergenceprocess. Suchaneffectbecomesmorepronouncedwhenthenetworktopology
degrades,asevidencedinthetransitionfromFigure2btoFigure2a.
Insummary,theincorporationofeitherthemomentumtrackingtechniqueorLCAmethodproveseffectiveinenhancing
thepracticalperformanceofdistributedstochasticgradientmethods. Notably, theDSMTmethodoutperformsthe
aforementionedalternativesbycombiningthesetwotechniques.Suchacomparisoncorroboratesthetheoreticalfindings
inSection4.
5.2 NonconvexLogisticRegression
Inthispart,weconsiderabinaryclassificationproblem(44)classifyingairplanesandtrucksontheCIFAR-10dataset.
Theparameterωissetasω =0.05and[x] denotestheq−elementofx∈Rp. Theothersettingsfollowthesameas
q
thoseinSubsection5.1. Wecomparedtheaforementionedmethodsoverringgraphswithn = 100(Figure3a)and
16
n
2||*x
t0 ,ix||
1 n 1=i n
2||*x
t0 ,ix||
1 n 1=iDistributedStochasticMomentumTracking APREPRINT
n=50(Figure3b),respectively.
n
1 (cid:88)
minf(x)= f (x),
x∈Rp n i
i=1
(44)
f (x):= 1 (cid:88) log[1+exp(−x⊺ u v )]+ ω (cid:88)p [x]2 q .
i |S | j j 2 1+[x]2
i j∈Si q=1 q
ItcanbeseenfromFigure3thatsolelyapplyingmomentumtechniquetoDSGTdoesnotenhanceitsperformance.
In Figure 3b, DSMT_noLCA demonstrates a slight improvement over DSGT-HB. However, DSMT consistently
outperformsothermethods,capitalizingonthecombinedefficacyofmomentumtrackingtechniqueandLCAmethod.
Notably,thedifferencebetweenDSMTandEDASbecomesnegligibleasthenumberofiterationincreases,suggesting
thattheapplicationofLCAtoEDAScouldpotentiallyenhanceitsconvergence. Suchaphenomenoninspiresfurther
researchofinterest.
CSGDM
10 1 10 1 CSGD
DSGD
10 2 10 2 E DD SA GS T
DSMT
10 3 DSGT-HB 10 3 DSMT_noLCA
10 4 CSGDM 10 4
CSGD
10 5 DSGD
EDAS 10 5
DSGT
10 6 DSMT
DSGT-HB 10 6
DSMT_noLCA
10 7
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
# Iterations # Iterations
(a)Ringgraph,n=100,1−λ=6.6×10−4. (b)Ringgraph,n=50,1−λ=2.6×10−3.
Figure3: ComparisonamongDSMT,DSGT,EDAS,DSGD,CSGD,andCSGDMforsolvingProblem(44)onthe
CIFAR-10datasetusingaconstantstepsize. Thestepsizesaresetas0.02forallmethods. Themomentumparameter
issetasβ =1−(1−ρ˜ )/n1/3forDSMT,DSMT_noLCA,andSGDM.Theresultsareaveragedover10repeated
w
experiments.
6 Conclusion
Thispaperfocusesonaddressingthedistributedstochasticoptimizationproblemovernetworkedagents. Theproposed
algorithm,DistributedStochasticMomentumTracking(DSMT),leveragesmomentumtrackingtechniqueaswellasthe
LooplessChebyshevAcceleration(LCA)methodtoenhancetheperformanceofdistributedstochasticgradientmethods
overnetworks. Inparticular,DSMTshortensthetransienttimesfordecentralizedstochasticgradientmethodswithout
requiringmultiplecommunicationperiterationforsmoothobjectivefunctionswithorwithoutthePolyak-Łojasiewicz
(PL)conditionunderthemostgeneralvarianceconditionindistributedsettings.Suchaconditionforstochasticgradients
enablesthewiderangeofapplicationofDSMT.Experimentalresultsalsocorroboratetosuchtheoreticalfindings.
Themomentumtrackingtechniqueisalsoofindependentinterestandcanpotentiallyinspirefuturedevelopmentof
distributedoptimizationalgorithms.
17
2 )kx(f 2 )kx(fDistributedStochasticMomentumTracking APREPRINT
A ProofsfortheGeneralNonconvexCase
A.1 ProofofLemma3.1
Definex⊺ := 11⊺[x˜ ] andx¯⊺ := 1 1⊺x˜ . From(9a),wehave
k n k n+1:2n k 2n k
1
x¯ = [(2+η )x¯ −η x ]−αy¯ ,
k+1 2 w k w k k
(45)
x =(1+η )W (x −αy )−η ([x˜ ] −αy ),
k+1 w k k w k n+1:2n k
[x˜ ] =x −αy .
k+1 n+1:2n k k
Therefore,itsufficestoshowx¯ =x =x¯ foranyk ≥0,whichweprovebyinduction. Whenk =0,x¯ =x =x¯
k k k 0 0 0
sincex˜ =(x ) . Supposex¯ =x =x¯ forsomek ≥0,then(45)leadsto
0 0 # k k k
x¯ =x¯ −αy¯ ,
k+1 k k
x¯ =(1+η )x¯ −η x −αy¯ =x¯ −αy¯ ,
k+1 w k w k k k k
x =x¯ −αy¯ .
k+1 k k
Hence, we have x¯ = x = x¯ for any k ≥ 0. Similar line of analysis can show that y¯ = y¯ = y =
k k k k+1 k+1 k+1
y¯ +z¯ −z¯ . Theupdateformfory¯ andtheinitializationy =z implyy¯ =z¯ foranyk ≥0.
k k+1 k k 0 0 k k
A.2 ProofofLemma3.2
RecallfromLemma3.1thatx¯ =x¯ −αz¯ . Wefirstshowd¯ =d¯ −αg¯ . Whenk =0,wehave
k+1 k k k+1 k k
1 β 1 1
d¯ −d¯ = x¯ − x¯ −x¯ = (x¯ −αz¯ )− x¯ =−αg¯ ,
1 0 1−β 1 1−β 0 0 1−β 0 0 1−β 0 0
wherethelastequlityholdsbyinitializingz =(1−β)g .
0 0
Forthecasek ≥1,wehave
1 1
d¯ −d¯ = (x¯ −βx¯ −x¯ +βx¯ )= (−αz¯ +βαz¯ )=−αg¯ .
k+1 k 1−β k+1 k k k−1 1−β k k−1 k
Nextweshowd¯ −x¯ =− αβ z¯ ,whichisbecausefork ≥1,
k k 1−β k−1
1 β β αβ
d¯ −x¯ = x¯ − x¯ −x¯ = (x¯ −x¯ )=− z¯ . (46)
k k 1−β k 1−β k−1 k 1−β k k−1 1−β k−1
Duetoz =0andd¯ =x¯ ,therelation(46)alsoholdsfork =0.
−1 0 0
A.3 ProofofLemma3.3
Giventhatx¯ =d¯ +αβz¯ /(1−β),k ≥0fromLemma3.2,wehave
k k k−1
f(x¯ )−f∗ ≤f(d¯ )−f∗+ αβ (cid:10) ∇f(d¯ )−∇f(x¯ )+∇f(x¯ ),z¯ (cid:11) + α2β2L ∥z¯ ∥2 (47)
k k 1−β k k k k−1 2(1−β)2 k−1
αβ αβ[1+3αβL/(1−β)]
≤f(d¯ )−f∗+ ∥∇f(x¯ )∥2+ ∥z¯ ∥2, (48)
k 2(1−β) k 2(1−β) k−1
whereweinvoke∥∇f(x¯ )−∇f(d¯ )∥≤αβL∥z¯ ∥/(1−β)fork ≥0. Takingthefullexpectationonbothsides
k k k−1
yieldsthedesiredresult.
18DistributedStochasticMomentumTracking APREPRINT
A.4 ProofofLemma3.4
InlightofLemma3.2thatd¯ =d¯ −αg¯ fork ≥0,wehavefromthedescentlemmaandAssumption1.2that
k+1 k k
(cid:42) n (cid:43)
E(cid:2) f(d¯ k+1)(cid:12) (cid:12)F k(cid:3) ≤f(d¯ k)−α ∇f(d¯ k)−∇f(x¯ k), n1 (cid:88) ∇f i(x i,k)
i=1
−α(cid:42)
∇f(x¯ ),
1 (cid:88)n
∇f (x
)(cid:43)
+
L E(cid:104)
∥αg¯
∥2(cid:12)
(cid:12)F
(cid:105)
k n i i,k 2 k (cid:12) k
i=1
≤f(d¯ k)+α(cid:13) (cid:13)∇f(d¯ k)−∇f(x¯ k)(cid:13) (cid:13)2 + α
4
(cid:13) (cid:13) (cid:13) (cid:13)n1 (cid:88)n ∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13)2 − α(1− 2αL)(cid:13) (cid:13) (cid:13) (cid:13)n1 (cid:88)n ∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1
− α 2 ∥∇f(x¯ k)∥2+ α 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∇f(x¯ k)− n1 (cid:88) i=n 1∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 + α 22L E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)g¯ k− n1 (cid:88) i=n 1∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)F k  (49)
≤f(d¯ k)+αL2(cid:13) (cid:13)d¯ k−x¯ k(cid:13) (cid:13)2 − α
2
∥∇f(x¯ k)∥2− α
8
(cid:13) (cid:13) (cid:13) (cid:13)n1 (cid:88)n ∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13)2 + α 2L n2 (cid:88)n ∥x¯ k−x i,k∥2
(cid:13) (cid:13)
i=1 i=1
α2L(cid:34)
C
(cid:88)n (cid:35)
+ (f (x )−f∗)+σ2 ,
n n i i,k i
i=1
whereweletα≤1/(4L)andinvokeAssumption1.2forthelastinequality.
Notingthat
L
f (x )−f∗ ≤f (x¯ )−f∗+⟨∇f (x¯ ),x −x¯ ⟩+ ∥x −x¯ ∥2
i i,k i i k i i k i,k k 2 i,k k
≤f (x¯ )−f∗+ 1 ∥∇f (x¯ )∥2+L∥x −x¯ ∥2 (50)
i k i 2L i k i,k k
≤2(f (x¯ )−f∗)+L∥x −x¯ ∥2,
i k i i,k k
wehave
n
1 (cid:88) (f (x )−f∗)≤2(f(x¯ )−f∗)+2σ∗+ L ∥x −1x¯⊺ ∥2. (51)
n i i,k i k f n k k
i=1
Inlightoftherelationd¯ −x¯ =−αβz¯ /(1−β)andz =0,wehave
k k k−1 −1
(cid:13) (cid:13)d¯ k−x¯ k(cid:13) (cid:13)2 = (1α −2β β2
)2
∥z¯ k−1∥2,k ≥0. (52)
Combing(49)-(52),takingthefullexpectation,lettingα≤1/(2C),andinvokingLemma3.3yieldthedesiredresult
fork ≥0.
A.5 ProofofLemma3.5
AccordingtoLemma3.1,wehave
(cid:32) n (cid:33) (cid:34) n (cid:35)
1 (cid:88) 1 (cid:88)
z¯ =βz¯ +(1−β) g¯ − ∇f (x ) +(1−β) ∇f (x )−∇f(x¯ ) +(1−β)∇f(x¯).
k k−1 k n i i,k n i i,k k
i=1 i=1
(53)
19DistributedStochasticMomentumTracking APREPRINT
Then,byAssumption1.2andJensen’sinequality,wehave
E(cid:104)
∥z¯
∥2(cid:12)
(cid:12)F
(cid:105)
≤β∥z¯ ∥2+
(1−β)2 (cid:34) C (cid:88)n
(f (x
)−f∗)+σ2(cid:35)
+2(1−β)∥∇f(x¯ )∥2
k (cid:12) k k−1 n n i i,k i k
i=1
2(1−β)L2
+ ∥x −1x¯⊺ ∥2
n k k (54)
2C(1−β)2 (1−β)2(2Cσ∗+σ2)
≤β∥z¯ ∥2+ [f(x¯ )−f∗]+2(1−β)∥∇f(x¯ )∥2+ f
k−1 n k k n
(cid:18) (cid:19)
2(1−β)L C(1−β)
+ L+ ∥x −1x¯⊺ ∥2.
n n k k
TakingthefullexpectationonbothsidesandinvokingLemma3.3yieldthedesiredresult.
A.6 ProofofLemma3.6
Firstconsider
⊺ ⊺
z −∇F(1x¯ )=βz +(1−β)g −∇F(1x¯ )
k k k−1 k k
(cid:2) ⊺ (cid:3) ⊺
=β z k−1−∇F(1x¯ k−1) +(1−β)[g k−∇F(x k)]+(1−β)[∇F(x k)−∇F(1x¯ k)] (55)
(cid:2) ⊺ ⊺ (cid:3)
−β ∇F(1x¯ )−∇F(1x¯ ) , k ≥1.
k k−1
From(55)andJensen’sinequality,wehave
E(cid:104) ∥z k−∇F(1x¯⊺ k)∥2(cid:12) (cid:12) (cid:12)F k(cid:105) ≤β(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2 +2(1−β)∥∇F(x k)−∇F(1x¯⊺ k)∥2
+ 12 −β2
β
(cid:13) (cid:13)∇F(1x¯⊺ k)−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2 +(1−β)2E(cid:104) ∥g k−∇F(x k)∥2(cid:12) (cid:12) (cid:12)F k(cid:105)
(56)
≤β(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2 + 2α 12β −2n βL2 ∥z¯ k−1∥2+2(1−β)L2∥Πx k∥2
(cid:26) (cid:27)
L
+n(1−β)2(2Cσ∗+σ2)+n(1−β)2C 2[f(x¯ )−f∗]+ ∥Πx ∥2 ,
f k n k
whereAssumption1.2andtheupperbound(57)areinvokedforthelastinequality. Theinequality(57)holdsdueto
Lemma3.1thatx¯ =x¯ −αz¯
k k−1 k−1
E(cid:104)(cid:13) (cid:13)∇F(1x¯⊺ k)−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) ≤nL2E(cid:104)
∥x¯ k−x¯
k−1∥2(cid:105) ≤α2nL2E(cid:104)
∥z¯
k−1∥2(cid:105)
. (57)
Takingfullexpectationonbothsidesof(56),andsubstitutingtheresultofLemmas3.3yieldthat
E(cid:104)
∥z
k−∇F(1x¯⊺ k)∥2(cid:105) ≤βE(cid:104)(cid:13)
(cid:13)z
k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
+
2α 12β −2n βL2 E(cid:104)
∥z¯
k−1∥2(cid:105) +2(1−β)L2E(cid:104)
∥Πx
k∥2(cid:105)
+n(1−β)2C(cid:26) 2E(cid:2) f(d¯ )−f∗(cid:3) + 2αβ E(cid:104) ∥∇f(x¯ )∥2(cid:105) + 2αβ E(cid:104) ∥z¯ ∥2(cid:105) + L E(cid:104) ∥Πx ∥2(cid:105)(cid:27)
k 1−β k 1−β k−1 n k
+n(1−β)2(2Cσ∗+σ2)
f
≤βE(cid:104)(cid:13)
(cid:13)z
k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) +2αn(cid:18) 1α −L2
β
+(1−β)C(cid:19) E(cid:104)
∥z¯
k−1∥2(cid:105) +2(1−β)L(L+C)E(cid:104)
∥Πx
k∥2(cid:105)
+n(1−β)2(2Cσ∗+σ2)+2n(1−β)2CE(cid:2) f(d¯ )−f∗(cid:3) +2αn(1−β)CE(cid:104) ∥∇f(x¯ )∥2(cid:105) , k ≥1.
f k k
Lettingα≤(1−β)/Lleadstothedesiredresultfork ≥1. Forthecasek =0,recallthatx =x ,z =0and
−1 0 −1
z =(1−β)g ,then
0 0
⊺ (cid:2) ⊺ (cid:3) ⊺
z −∇F(1x¯ )=β z −∇F(1x¯ ) +(1−β)[g −∇F(x )]+(1−β)[∇F(x )−∇F(1x¯ )]
0 0 −1 −1 0 0 0 0
(58)
(cid:2) ⊺ ⊺ (cid:3)
−β ∇F(1x¯ )−∇F(1x¯ ) .
0 −1
20DistributedStochasticMomentumTracking APREPRINT
⊺ ⊺
Since[∇F(1x¯ )−∇F(1x¯ )]=0duetox =x ,similartothederivationsof(56),wehave
0 −1 −1 0
E(cid:104) ∥z 0−∇F(1x¯⊺ 0)∥2(cid:105) ≤βE(cid:104)(cid:13) (cid:13)z −1−∇F(1x¯⊺ −1)(cid:13) (cid:13)2(cid:105) +(1−β)L2∥Πx 0∥2
(cid:26) (cid:27)
L
+n(1−β)2(2Cσ∗+σ2)+n(1−β)2C 2E[f(x¯ )−f∗]+ ∥Πx ∥2
f 0 n 0 (59)
≤βE(cid:104)(cid:13) (cid:13)z −1−∇F(1x¯⊺ −1)(cid:13) (cid:13)2(cid:105) +(1−β)L(L+C)∥Πx −1∥2
+n(1−β)2(2Cσ∗+σ2)+2n(1−β)2CE[f(x¯ )−f∗],
f −1
whereweinvokethatx¯ = x¯ = d¯. Asaconsequence,combiningtherecursionsfork = 0andk ≥ 1yieldsthe
−1 0 0
desiredresult.
A.7 ProofofLemma3.7
NotethatΠ˜W˜ =Π˜W˜Π˜. Then,
E(cid:20)(cid:13) (cid:13) (cid:13)Π˜x˜ k+1(cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12)F k(cid:21) =E(cid:20)(cid:13) (cid:13) (cid:13)Π˜W˜Π˜x˜ k−αΠ˜W˜Π˜(y k) #(cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12)F k(cid:21)
(60)
1(cid:13) (cid:13)2 α2c ρ˜2 (cid:104) (cid:12) (cid:105)
≤ (cid:13)Π˜W˜Π˜x˜ (cid:13) + 0 wE ∥Πy ∥2(cid:12)F ,
q (cid:13) k(cid:13) 1−q k (cid:12) k
whereweinvokeJensen’sinequalityforsomeq >0(tobedeterminedlater)andLemma2.1forthelastinequality.
Fortheterm∥Π˜W˜Π˜x˜ ∥2,wehave
k
1 qE(cid:20)(cid:13) (cid:13) (cid:13)Π˜W˜Π˜x˜ k(cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12)F k−1(cid:21) ≤ q1
2
(cid:13) (cid:13) (cid:13)Π˜W˜2Π˜x˜ k−1(cid:13) (cid:13) (cid:13)2 + qα (2 1c −0ρ˜ q4 w )E(cid:104) ∥Πy k−1∥2(cid:12) (cid:12) (cid:12)F k−1(cid:105) . (61)
Choosingq =ρ˜ in(60)and(61)andrepeatingsuchproceduresleadto
w
E(cid:20)(cid:13)
(cid:13)Π˜x˜
(cid:13) (cid:13)2(cid:21)
≤
1 (cid:13)
(cid:13)Π˜W˜kΠ˜x˜
(cid:13) (cid:13)2
+
α2c 0ρ˜
w
k (cid:88)−1 ρ˜k−tE(cid:104)
∥Πy
∥2(cid:105)
(cid:13) k(cid:13) ρ˜k (cid:13) 0(cid:13) 1−ρ˜ w t
w w t=0
(62)
≤c ρ˜k ∥Πx ∥2+
α2c 0ρ˜
w
k (cid:88)−1 ρ˜k−tE(cid:104)
∥Πy
∥2(cid:105)
:=Rx,
0 w 0 1−ρ˜ w t k
w
t=0
whereweapplyLemma2.1forthelastinequality. ForthetermRx,wehave
k
α2c ρ˜2 (cid:104) (cid:105) α2c ρ˜ (cid:20)(cid:13) (cid:13)2(cid:21)
Rx =ρ˜ Rx+ 0 wE ∥Πy ∥2 ≤ρ˜ Rx+ 0 wE (cid:13)Π˜y˜ (cid:13) . (63)
k+1 w k 1−ρ˜ k w k 1−ρ˜ (cid:13) k(cid:13)
w w
Similarly,wecanconstructasequence{Ry}thatboundsthetermE[∥Π˜y˜∥2]asfollows.
k
Relation(55)guidesustorewrite(z −z )asin(64),sothatwecantakeadvantageoftheadditionalcoefficient
k+1 k
(1−β)andreducetheimpactofthedataheterogeneitydueto(cid:80)n ∥∇f (x )∥2asmentionedinSubsection2.1.
i=1 i i,k
⊺
z −z =−(1−β)(z −∇F(1x¯ ))+(1−β)[g −∇F(x )]
k+1 k k k k+1 k+1
(64)
(cid:2) ⊺ (cid:3) (cid:2) ⊺ ⊺ (cid:3)
+(1−β) ∇F(x )−∇F(1x¯ ) +(1−β) ∇F(1x¯ )−∇F(1x¯ ) .
k+1 k+1 k+1 k
Theaboverelationfollowsfrom(9c). Therefore,wecanrewritetheupdateofy˜ asfollows.
y˜ =W˜y˜ −(1−β)W˜ [z −∇F(1x¯⊺ )] +(1−β)W˜ [g −∇F(x )]
k+1 k k k # k+1 k+1 #
(65)
+(1−β)W˜ (cid:2) ∇F(x )−∇F(1x¯⊺ )(cid:3) +(1−β)W˜ (cid:2) ∇F(1x¯⊺ )−∇F(1x¯⊺ )(cid:3) .
k+1 k+1 # k+1 k #
21DistributedStochasticMomentumTracking APREPRINT
⊺ ⊺ ⊺ ⊺
DenoteA:=y˜ −(1−β)[z −∇F(1x¯ )] +(1−β)[∇F(x )−∇F(1x¯ )] +(1−β)[∇F(1x¯ )−∇F(1x¯ )] .
k k k # k+1 k+1 # k+1 k #
Wethenhave
E(cid:20)(cid:13) (cid:13) (cid:13)Π˜y˜ k+1(cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12)F k+1(cid:21) =(cid:13) (cid:13) (cid:13)Π˜W˜Π˜A(cid:13) (cid:13) (cid:13)2 +(1−β)2E(cid:20)(cid:13) (cid:13) (cid:13)Π˜W˜Π˜[g k+1−∇F(x k+1)] #(cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12)F k+1(cid:21)
(cid:104)(cid:68) (cid:69)(cid:12) (cid:105)
+2E Π˜W˜Π˜A,(1−β)Π˜W˜Π˜[g −∇F(x )] (cid:12)F
k+1 k+1 # (cid:12) k+1
(66)
≤ 1 (cid:13) (cid:13)Π˜W˜Π˜y˜ (cid:13) (cid:13)2 + 3(1−β)2c 0ρ˜2 w (cid:104) ∥z −∇F(1x¯⊺ )∥2+L2∥Πx ∥2+α2nL2∥z¯ ∥2(cid:105)
ρ˜ (cid:13) k(cid:13) 1−ρ˜ k k k+1 k
w w
(cid:26) (cid:27)
+nc ρ˜2(1−β)2(cid:0) 2Cσ∗+σ2(cid:1) +nc ρ˜2C(1−β)2 2[f(x¯ )−f∗]+ L ∥Πx ∥2 ,
0 w f 0 w k+1 n k+1
whereweinvokeLemma2.1,Assumptions1.1and1.2,and(51)forthelastinequality. Similarly,wecanobtainthe
recursionforE[∥Π˜W˜Π˜y˜ ∥2|F ]:
k k
ρ˜1 E(cid:20)(cid:13) (cid:13) (cid:13)Π˜W˜Π˜y˜ k(cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12)F k(cid:21) ≤ ρ˜1
2
(cid:13) (cid:13) (cid:13)Π˜W˜2Π˜y˜ k−1(cid:13) (cid:13) (cid:13)2 +nc 0ρ˜3 w(1−β)2(2Cσ f∗+σ2)
w w
+ 3(1− 1−β) ρ˜2c 0ρ˜3 w (cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2 +L2∥Πx k∥2+α2nL2∥z¯ k−1∥2(cid:105) (67)
w
(cid:26) (cid:27)
L
+nc ρ˜3(1−β)2C 2[f(x¯ )−f∗]+ ∥Πx ∥2 .
0 w k n k
Takingthefullexpectationin(66)and(67)andusingthesimilarstepsasintheprecedinganalysisleadto
E(cid:20)(cid:13)
(cid:13)Π˜y˜
(cid:13) (cid:13)2(cid:21)
≤
1 E(cid:20)(cid:13)
(cid:13)Π˜W˜kΠ˜y˜
(cid:13) (cid:13)2(cid:21)
+nc
ρ˜2(1−β)2(2Cσ∗+σ2)k (cid:88)−1
ρ˜k−1−t
(cid:13) k(cid:13) ρ˜k (cid:13) 0(cid:13) 0 w f w
w t=0
+
3(1−β)2c 0ρ˜2
w
k (cid:88)−1 ρ˜k−1−t(cid:110) E(cid:104)
∥z −∇F(1x¯⊺
)∥2(cid:105) +L2E(cid:104)
∥Πx
∥2(cid:105) +α2L2nE(cid:104) ∥z¯∥2(cid:105)(cid:111)
1−ρ˜ w t t t+1 t
w
t=0
+nc
ρ˜2(1−β)2C(cid:88)k ρ˜k−t(cid:26)
2E[f(x¯ )−f∗]+
L E(cid:104)
∥Πx
∥2(cid:105)(cid:27)
0 w w t n t
t=1
≤c
ρ˜kE(cid:104)
∥Πy
∥2(cid:105)
+
3(1−β)2c 0ρ˜2
w
k (cid:88)−1 ρ˜k−1−t(cid:110) E(cid:104)
∥z −∇F(1x¯⊺
)∥2(cid:105) +L2E(cid:104)
∥Πx
∥2(cid:105) +α2L2nE(cid:104) ∥z¯∥2(cid:105)(cid:111)
0 w 0 1−ρ˜ w t t t+1 t
w
t=0
+nc
ρ˜2(1−β)2(2Cσ∗+σ2)k (cid:88)−1
ρ˜k−1−t+nc
ρ˜2(1−β)2C(cid:88)k ρ˜k−t(cid:26)
2E[f(x¯ )−f∗]+
L E(cid:104)
∥Πx
∥2(cid:105)(cid:27)
0 w f w 0 w w t n t
t=0 t=1
:=Ry, k ≥1,
k
(68)
whereweinvokeLemma2.1forthelastinequality. ThenwecanobtaintherecursionforRy accordingto(68):
k
3(1−β)2c ρ˜2 (cid:110) (cid:104) (cid:105) (cid:104) (cid:105)(cid:111)
Ry =ρ˜ Ry + 0 w E ∥z −∇F(1x¯⊺ )∥2 +L2∥Πx ∥2+α2L2nE ∥z¯ ∥2
k+1 w k 1−ρ˜ k k k+1 k
w
(cid:26) L (cid:104) (cid:105)(cid:27)
+nc ρ˜2(1−β)2(2Cσ∗+σ2)+nc ρ˜2(1−β)2C 2E[f(x¯ )−f∗]+ E ∥Πx ∥2 , k ≥1.
0 w f 0 w k+1 n k+1
(69)
Notethat(68)definesRy asfollows
1
(cid:104) (cid:105) 3(1−β)2c ρ˜2 (cid:110) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)(cid:111)
Ry =c ρ˜ E ∥Πy ∥2 + 0 w E ∥z −∇F(1x¯⊺ )∥2 +L2E ∥Πx ∥2 +α2L2nE ∥z¯ ∥2
1 0 w 0 1−ρ˜ 0 0 1 0
w
(cid:26) L (cid:104) (cid:105)(cid:27)
+nc ρ˜2(1−β)2(2Cσ∗+σ2)+nc ρ˜2(1−β)2C 2E[f(x¯ )−f∗]+ E ∥Πx ∥2 .
0 w f 0 w 1 n 1
Hence,wecandefineRy := c E[∥Πy ∥2]+nc ρ˜ (1−β)2(2Cσ∗ +σ2)sothattherecursion(69)alsoholdsfor
0 0 0 0 w f
k =0.
22DistributedStochasticMomentumTracking APREPRINT
NotingthatE[∥Πx ∥2]≤E[∥Π˜x˜ ∥2]≤Rx andE[∥Π˜y˜ ∥2]≤Ry,thedesiredresultfollowsbyinvoking(63)
k+1 k+1 k+1 k k
andLemmas3.3-3.6:
Ry
k+1
≤ρ˜ wRy k+ 3c 0(1− 1β −)2 ρ˜L(L+C)(cid:18) ρ˜ wRx k+ α 12 −c 0 ρ˜ρ˜ wRy k(cid:19) + 3c 0 1( −1− ρ˜β)2 (cid:110) βE(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
w w w
+2αn(L+C)E(cid:104) ∥z¯ ∥2(cid:105) +2(1−β)L(L+C)E(cid:104) ∥Πx ∥2(cid:105) +2n(1−β)2CE(cid:2) f(d¯ )−f∗(cid:3)
k−1 k k
(cid:104) (cid:105) (cid:111) (cid:18) 3αL2(1−β)(cid:19)(cid:110) (cid:104) (cid:105)
+2αn(1−β)CE ∥∇f(x¯ )∥2 +n(1−β)2(2Cσ∗+σ2) +α(1−β)nc 2C+ E ∥z¯ ∥2
k f 0 1−ρ˜ k−1
w
(cid:41)
+2C(1−β)2 E(cid:2) f(d¯ )−f∗(cid:3) +3(1−β)E(cid:104) ∥∇f(x¯ )∥2(cid:105) + (1−β)2(2Cσ f∗+σ2) + 2(1−β)L(L+C) Rx
n k k n n k
+2nc (1−β)2C(cid:26)(cid:18) 1+ 2α2CL(cid:19) E(cid:2) f(d¯ )−f∗(cid:3) − α(cid:18) 1− 4α2CL (cid:19) E(cid:104) ∥∇f(x¯ )∥2(cid:105)
0 n k 2 n(1−β) k
(cid:41)
αL2 (cid:104) (cid:105) α3β2L(L+2C) (cid:104) (cid:105) α2L(2Cσ∗+σ2)
+ E ∥Πx ∥2 + E ∥z¯ ∥2 + f +nc ρ˜2(1−β)2(2Cσ∗+σ2)
n k (1−β)2 k−1 n 0 w f
2nc (1−β)2Cα(cid:110) (cid:104) (cid:105) (cid:104) (cid:105)(cid:111)
+ 0 2E ∥∇f(x¯ )∥2 +2α2L2E ∥z¯ ∥2
1−β k k
≤(cid:20) ρ˜ + 3α2c2 0(1−β)2L(L+C)(cid:21) Ry + 3c 0(1−β)2L(cid:2) L+C+2(L+C)+2α(L+C)2+αCL(cid:3) Rx
w (1−ρ˜ )2 k 1−ρ˜ k
w w
+ 3c 0 1(1 −− ρ˜β)2β E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) +3αn(1−β)c 0(L+C)(cid:20) 2 1(1 −− ρ˜β) +1+ 34 (α 12 −C βL )(cid:21) E(cid:104) ∥z¯ k−1∥2(cid:105)
w w
+2nc (1−β)2C(cid:20) 3(1−β)2 + 3α(1−β)(C+L) +2(cid:21) E(cid:2) f(d¯ )−f∗(cid:3)
0 1−ρ˜ n k
w
(cid:20) 6(1−β)2C 9(C+L)(1−β)(cid:21) (cid:104) (cid:105)
+4αnc (1−β) C+ + E ∥∇f(x¯ )∥2
0 4(1−ρ˜ ) 4 k
w
(cid:20) 3(1−β)2 3α(1−β)(L+C) 2α2CL(cid:21)
+nc (1−β)2 1+ + + (2Cσ∗+σ2)
0 1−ρ˜ n n f
w
≤ 1+ 2ρ˜ wRy k+ 15c 0(1− 1β −)2 ρ˜L(L+C) Rx k+ 3c 0 1(1 −− ρ˜β)2β E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
w w
+9αnc (1−β)(C+L)E(cid:104) ∥z¯ ∥2(cid:105) +12nc (1−β)2CE(cid:2) f(d¯ )−f∗(cid:3)
0 k−1 0 k
(cid:104) (cid:105)
+20αnc (1−β)(C+L)E ∥∇f(x¯ )∥2 +6nc (1−β)2(2Cσ∗+σ2),
0 k 0 f
wherewelet1−β ≤1−ρ˜ andthestepsizesatisfy
w
(cid:40) (cid:115) (cid:114) (cid:41)
1 1−ρ˜ 1−β
α≤min , w , .
2(L+C) 6c2L(L+C) 4CL
0
A.8 ProofofLemma3.8
WedefinetheLyapunovfunctionL as
k
L
k
:=E(cid:2) f(d¯ k)−f∗(cid:3) +αC 1Rx k+α3C 2Ry k+α3C 3E(cid:104) ∥z¯ k−1∥2(cid:105) +α3C 4E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) , (70)
whereC -C arepositiveconstantstobedeterminedlater.
1 4
23DistributedStochasticMomentumTracking APREPRINT
CombingtheresultsofLemmas3.4-3.7and(70)leadsto
L
≤(cid:20)
1+
2α2CL
+12α3C nC(1−β)2c +
2α3C(1−β)2C
3 +2nα3C
(1−β)2C(cid:21)
E(cid:2) f(d¯ )−f∗(cid:3)
k+1 n 2 0 n 4 k
(cid:20) αL2 15α3C (1−β)2c L(L+C) 2α3C (1−β)L(L+C) (cid:21)
+ +αC ρ˜ + 2 0 + 3 +2α3C (1−β)(L+C)L Rx
n 1 w 1−ρ˜ n 4 k
w
(cid:20) α3c ρ˜ C (1+ρ˜ )α3C (cid:21)
+ 0 w 1 + w 2 Ry
1−ρ˜ 2 k
w
(cid:20) α3β2L(L+2C) (1+β)α3C (cid:21) (cid:104) (cid:105)
+ +9α4C n(C+L)(1−β)c + 3 +2α4C n(C+L) E ∥z¯ ∥2
(1−β)2 2 0 2 4 k−1
+(cid:20) 3(1− 1β −)2α ρ˜3C 2c 0β +α3C 4β(cid:21) E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
w
α(cid:20) 4α2CL (cid:21) (cid:104) (cid:105)
− 1− −40α3nc (1−β)(C+L)C −6(1−β)α2C −4n(1−β)α3C C E ∥∇f(x¯ )∥2
2 n(1−β) 0 2 3 4 k
(cid:20) L αC (1−β)2 (cid:21)
+ +6αC nc (1−β)2+ 3 +nαC (1−β)2 α2(2Cσ∗+σ2).
n 2 0 n 4 f
(71)
Therefore,itsufficestodetermineC -C andtheparametersαandβ suchthat
1 4
αL2 15α3C (1−β)2c L(L+C) 2α3C (1−β)L(L+C)
+ 2 0 + 3 +2α3C (1−β)(L+C)L≤(1−ρ˜ )αC ,
n 1−ρ˜ n 4 w 1
w
(72a)
α3c ρ˜ C (1+ρ˜ )α3C
0 w 1 + w 2 ≤α3C , (72b)
1−ρ˜ 2 2
w
α3β2L(L+2C) (1+β)α3C
+9α4C n(C+L)(1−β)c + 3 +2α4C n(C+L)≤α3C , (72c)
(1−β)2 2 0 2 4 3
3(1−β)2α3C c β
2 0 +α3C β ≤α3C . (72d)
1−ρ˜ 4 4
w
From (72d), it is sufficient to choose C = 4c C given that β ≥ ρ˜ . According to (72b), we can choose C =
4 0 2 w 2
4c ρ˜ C /(1−ρ˜ )2. SubstitutingtheaboveC andC into(72a)and(72c)leadsto
0 w 1 w 2 4
L2 2α2C (1−β)L(L+C) (cid:18) 60α2(1−β)2c2ρ˜ L(L+C) 32α2c2(1−β)(L+C)Lρ˜ (cid:19)
+ 3 ≤ 1−ρ˜ − 0 w − 0 w C ,
n n w (1−ρ˜ )3 (1−ρ˜ )2 1
w w
(73a)
β2L(L+2C) 36αn(1−β)c2CC ρ˜ 8αc2n(C+L)C ρ˜ (1−β)C
+ 0 1 w + 0 1 w ≤ 3. (73b)
(1−β)2 (1−ρ˜ )2 (1−ρ˜ )2 2
w w
Notethat(1−β)≤(1−ρ˜ ). Lettingthestepsizeαsatisfy
w
(cid:40)(cid:115) (cid:115) (cid:41)
(1−ρ˜ )2 (1−ρ˜ )2
α≤min w , w (74)
240c2L(L+C) 128c2L(L+C)
0 0
leadsto
2L2 4α2C (1−β)L(L+C)
C ≥ + 3 . (75)
1 n(1−ρ˜ ) n(1−ρ˜ )
w w
Accordingto(73b),(74),and(75),wecanchooseC andC asfollows:
1 3
3L2 4L(L+2C)
C = , C = . (76)
1 n(1−ρ˜ ) 3 (1−β)3
w
Tomakesuchachoicefeasible,itissufficienttoletthestepsizeαandtheparameterβ satisfy
(cid:26) (cid:27)
1−ρ˜ 1
α≤min w, , β ≥ρ˜ ,
48L 216(C+L) w
24DistributedStochasticMomentumTracking APREPRINT
sothatthefollowinginequality,whichisderivedfrom(73b),holds
108α(1−β)c CL2 24αc (C+L)L2 L(L+2C)
0 + 0 ≤ .
(1−ρ˜ )3 (1−ρ˜ )3 (1−β)2
w w
Therefore,wehavedeterminedtheconstantsC -C :
1 4
3L2 12c ρ˜ L2 4L(L+2C) 48c2ρ˜ L2
C = , C = 0 w , C = , C = 0 w . (77)
1 n(1−ρ˜ ) 2 n(1−ρ˜ )3 3 (1−β)3 4 n(1−ρ˜ )3
w w w
Hence,theinequality(71)becomes
(cid:20) 2α2CL 240α3c2ρ˜ CL2(1−β)2 8α3CL(L+2C)(cid:21)
L ≤ 1+ + 0 w + L
k+1 n (1−ρ˜ )3 n(1−β) k
w
α(cid:20) 4α2CL 672α3c2ρ˜ (1−β)L2(C+L) 24α2L(L+2C)(cid:21) (cid:104) (cid:105)
− 1− − 0 w − E ∥∇f(x¯ )∥2 (78)
2 n(1−β) (1−ρ˜ )3 (1−β)2 k
w
(cid:20) L 72αc2ρ˜ L2(1−β)2 4αL(L+2C)(cid:21)
+ + 0 w + α2(2Cσ∗+σ2).
n (1−ρ˜ )3 n(1−β) f
w
Lettingthestepsizesatisfy
(cid:40) 1−β (cid:18) (1−ρ˜ )3 (cid:19)1/3 (cid:115) (1−β)2 (cid:114) 1−β(cid:41)
α≤min , w , ,
4(L+2C) 4032c2(1−β)L2(C+L) 144L(L+2C) 24CL
0
yieldsthedesiredresultfortherecursionofL .
k
WenextconsiderthetermL inthefollowing. NotingthedefinitionofRx,Ry in(14)andthat
0 0 0
(cid:104) (cid:105) (cid:104) (cid:105)
E ∥Πy ∥2 ≤(1−β)2E ∥g ∥2
0 0
(cid:40) n (cid:41)
≤(1−β)2 2nC[f(x¯ )−f∗]+2nCσ∗+nσ2+2L(L+C)∥Πx ∥2+2(cid:88) ∥∇f (x¯ )∥2 ,
0 f 0 i 0 (79)
i=1
n
(cid:13) (cid:13)z −1−∇F(1x¯⊺ −1)(cid:13) (cid:13)2 =∥∇F(1x¯⊺ 0)∥2 =(cid:88) ∥∇f i(x¯ 0)∥2,
i=1
wehave
L
0
=f(x¯ 0)−f∗+ n(13α −L ρ˜2 )Rx
0
+ 1 n2 (α 13 −c 0ρ ρ˜ ˜wL )32 Ry
0
+ 4 n8 (α 13 −c2 0ρ ρ˜ ˜wL )32 (cid:13) (cid:13)z −1−∇F(1x¯⊺ −1)(cid:13) (cid:13)2
w w w
(cid:20) 6α3c2L2C(1−β)2 24α3c2L2C(1−β)2(cid:21)
≤ 1+ 0 + 0 ∆
(1−ρ˜ )2 (1−ρ˜ )3 0
w w
(cid:20) 2α2c (1−β)2L(L+C) 8α2c2L(L+C)(1−β)2(cid:21) 3αL2∥Πx ∥2
+ c + 0 + 0 0 (80)
0 1−ρ˜ (1−ρ˜ )2 n(1−ρ˜ )
w w w
27α3c2L2(1−β)2(2Cσ∗+σ2) 72α3c2L2(cid:80)n ∥∇f (x¯ )∥2
+ 0 f + 0 i=1 i 0
(1−ρ˜ )3 n(1−ρ˜ )3
w w
9αL2∥Πx ∥2 27α3c2L2(1−β)2(2Cσ∗+σ2) 72α3c2L2(cid:80)n ∥∇f (x¯ )∥2
≤2∆ + 0 + 0 f + 0 i=1 i 0 .
0 n(1−ρ˜ ) (1−ρ˜ )2 n(1−ρ˜ )3
w w w
25DistributedStochasticMomentumTracking APREPRINT
B ProofsforthePLConditionCase
B.1 ProofofLemma4.4
Similartothederivationof(49),wehave
E(cid:2) f(d¯ k+1)−f∗(cid:12) (cid:12)F k(cid:3) ≤f(d¯ k)−f∗−α(cid:42) ∇f(d¯ k), n1 (cid:88)n ∇f i(x i,k)(cid:43) + L 2E(cid:104) ∥αg¯ k∥2(cid:12) (cid:12) (cid:12)F k(cid:105)
i=1
≤f(d¯ k)−f∗− α
2
(cid:13) (cid:13)∇f(d¯ k)(cid:13) (cid:13)2 − α(1− 2αL)(cid:13) (cid:13) (cid:13) (cid:13)n1 (cid:88)n ∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) (cid:13)
i=1
+ α 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∇f(d¯ k)− n1 (cid:88) i=n 1∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 + α 22L E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)g¯ k− n1 (cid:88) i=n 1∇f i(x i,k)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)F k  (81)
≤(1−αµ)(cid:2) f(d¯ k)−f∗(cid:3) + α 2L n2 (cid:88)n (cid:13) (cid:13)d¯ k−x i,k(cid:13) (cid:13)2 + α2L(2C nσ f∗+σ2) + α2 2L n2 2C ∥Πx k∥2
i=1
+ α2LC (cid:26)(cid:18) 1+ αβL (cid:19) (cid:2) f(d¯ )−f∗(cid:3) + αβ(1+αβL/(1−β)) ∥z¯ ∥2(cid:27) ,
n (1−β) k 2(1−β) k−1
whereweinvokeL-smoothnessoff ,Assumptions1.2and4.3,and(47)forthelastinequality.
i
Inlightoftherelationd¯ −x¯ =−αβz¯ /(1−β),wehave
k k k−1
n1 (cid:88)n (cid:13) (cid:13)d¯ k−x i,k(cid:13) (cid:13)2 = (1α −2β β2
)2
∥z¯ k−1∥2+ n1 ∥Πx k∥2. (82)
i=1
Combing(81)-(82),lettingα≤(1−β)/L,andtakingthefullexpectationleadto
E(cid:2) f(d¯ )−f∗(cid:3) ≤(cid:18) 1−αµ+ 2α2LC(cid:19) E(cid:2) f(d¯ )−f∗(cid:3) + αL2 E(cid:104) ∥Πx ∥2(cid:105) + α2L(2Cσ f∗+σ2)
k+1 n k n k n
α3βL[C(1−β)+Lβ] (cid:104) (cid:105)
+ E ∥z¯ ∥2 .
(1−β)2 k−1
Lettingα≤µ/(4LC)yieldsthedesiredresult.
B.2 ProofofLemma4.5
Similartothederivationof(71),wedeterminethenewrecursionofL inlightofAssumption4.3asfollows:
k
L ≤(cid:20) 1− αµ + 240α3c2 0ρ˜ wCL2 + 8α3CL(L+2C)(cid:21) E(cid:2) f(d¯ )−f∗(cid:3)
k+1 2 1−ρ˜ n(1−β) k
w
(cid:20) αL2 3αL2ρ˜ 180α3c2ρ˜ L3(L+C) 8α3L2(L+2C)(L+C) 96α3c2ρ˜ (L+C)L3(cid:21)
+ + w + 0 w + + 0 w Rx
n n(1−ρ˜ ) n(1−ρ˜ ) n(1−β)2 n(1−ρ˜ )2 k
w w w
(cid:20) 3α3c L2ρ˜ 12(1+ρ˜ )α3c ρ˜ L2(cid:21)
+ 0 w + w 0 w Ry
n(1−ρ˜ )2 2n(1−ρ˜ )3 k
w w
(cid:20) α3βL(L+2C) 108α4L2(C+L)c2ρ˜ 4(1+β)α3L(L+2C) 96α4c2ρ˜ L2(C+L)(cid:21) (cid:104) (cid:105)
+ + 0 w + + 0 w E ∥z¯ ∥2
(1−β)2 (1−ρ˜ )2 2(1−β)3 (1−ρ˜ )3 k−1
w w
+(cid:20) 36α3c n2 0 (ρ˜ 1w −βL ρ˜2( )1 4−β)2 + 48 nα (13c −2 0ρ˜ ρ˜wL )2 3β(cid:21) E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105)
w w
+ 48α3L(L+C)(cid:26) 4LE(cid:2) f(d¯ )−f∗(cid:3) + 2α2β2L2 E(cid:104) ∥z¯ ∥2(cid:105)(cid:27)
(1−β)2 k (1−β)2 k−1
(cid:20) L 72αc2ρ˜ L2(1−β)2 4αL(L+2C)(cid:21)
+ + 0 w + α2(2Cσ∗+σ2),
n (1−ρ˜ )3 n(1−β) f
w
(83)
26DistributedStochasticMomentumTracking APREPRINT
whereweconsiderα≤1/(28c2L),β ≥ρ˜ ,andtheestimatein(84). Theestimate(84)holdsdueto(52)
0 w
∥∇f(x¯ k)∥2 ≤2(cid:13) (cid:13)∇f(d¯ k)(cid:13) (cid:13)2 + 2 (1α2 −β2 βL )22 ∥z¯ k−1∥2 ≤4L(cid:2) f(d¯ k)−f∗(cid:3) + 2 (1α2 −β2 βL )22 ∥z¯ k−1∥2. (84)
Toderivethedesiredresultin(28),itisthensufficienttodetermineαandβ suchthat
240α3c2ρ˜ CL2 8α3CL(L+2C) 192α3L2(L+C) αµ
0 w + + ≤ , (85a)
1−ρ˜ n(1−β) (1−β)2 4
w
8α2(L+2C)(L+C)(1−ρ˜ ) 32α2c2ρ˜ (L+C)L 2(1−ρ˜ ) αµ
12α2c2ρ˜3LC+ w + 0 w ≤ w − , (85b)
0 w 3(1−β)2 1−ρ˜ 3 4
w
1−ρ˜ αµ
0≤ w − , (85c)
4 4
27αL(C+L)c2ρ˜ (1−β) 24αc2ρ˜ L(C+L) 24α2(L+C)L2β2 1−β αµ
0 w + 0 w + ≤ − , (85d)
L+2C L+2C (L+2C)(1−β) 4 4
3(1−β)2 αµ
≤1−β− . (85e)
4(1−ρ˜ ) 4
w
Toensure(85a)holds,itsufficestohave
240α3c2L(L+C)2 αµ 11
0 ≤ , β ≥1− (1−ρ˜ ).
(1−β)2 4 12 w
(cid:112)
Then,weletα≤ µ(1−β)2/[960c2L(L+C)2].
0
For(85b)tohold,itsufficestoletthestepsizeαsatisfy
(cid:40) (cid:115) (cid:115) (cid:115) (cid:41)
4(1−ρ˜ ) 1−ρ˜ (1−ρ˜ )2 (1−β)2
α≤min w , w , w , .
3µ 108c2LC 288c L(L+C) 48(L+C)2
0 0
For(85c),itissufficienttoletα≤(1−ρ˜ )/µ.
w
For(85d),itsufficestohave
(cid:40) (cid:114) (cid:41)
1−β 1 1−β (1−β)2
α≤min , , , .
3µ 486c2L 432c L 432L2
0 0
Finally,wechooseβ ≥(1+11ρ˜ )/12suchthat(85e)holdsandobtainthedesiredresult.
w
B.3 ProofofTheorem4.2
WefirstderiveanupperboundforE[f(d¯ )−f∗]. Unrollingtherecursion(28)inLemma4.5andrearrangingtheterms
k
yield
E(cid:2) f(d¯ )−f∗(cid:3) ≤L ≤(cid:16) 1− αµ(cid:17)k L +(cid:20) L + 72αc2 0ρ˜ wL2(1−β)2 + 4αL(L+2C)(cid:21)4α(2Cσ f∗+σ2)
k k 4 0 n (1−ρ˜ )3 n(1−β) µ
w
(86)
≤(cid:16)
1−
αµ(cid:17)k
L +
4αL(2Cσ f∗+σ2)
+
16α2L2S 1(2Cσ f∗+σ2)
,
4 0 nµ (1−ρ˜ )µ
w
where
18c2(1−β)2 (L+2C)(1−ρ˜ )
S := 0 + w .
1 (1−ρ˜ )2 nL(1−β)
w
WenextconstructanewLyapunovfunctionH in(87)suchthattheconsensuserrorE[∥Πx ∥2]canbebounded.
k k
H
k
:=Rx k+α2C 5Ry k+α3C 6E(cid:104) ∥z¯ k−1∥2(cid:105) +α2C 7E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) . (87)
27DistributedStochasticMomentumTracking APREPRINT
CombingtheresultsofLemmas3.5-3.7,invoking(84),andrearrangingthetermsyield
(cid:20) 15α2C (1−β)2c L(C+L) 2α3(1−β)L(L+C)C (cid:21)
H ≤ ρ˜ + 5 0 + 6 +2α2C (1−β)L(L+C) Rx
k+1 w 1−ρ˜ n 7 k
w
(cid:20) α2c ρ˜ α2C (1+ρ˜ )(cid:21)
+ 0 w + 5 w Ry
1−ρ˜ 2 k
w
(cid:20) (1+β)α3C (cid:21) (cid:104) (cid:105)
+ 9α3C nc (1−β)(C+L)+ 6 +2α3C n(C+L) E ∥z¯ ∥2
5 0 2 7 k−1
+(cid:20) 3α2C 5 1(1 −− ρ˜β)2c 0β +βα2C 7(cid:21) E(cid:104)(cid:13) (cid:13)z k−1−∇F(1x¯⊺ k−1)(cid:13) (cid:13)2(cid:105) (88)
w
(cid:20) 2α3C C(1−β)2 (cid:21)
+ 12α2C nc (1−β)2C+ 6 +2α2C n(1−β)2C L
5 0 n 7 k
+(cid:2) 20α3C nc (1−β)(C+L)+3α3C (1−β)+2α3C n(1−β)C(cid:3)(cid:26) 4LL + 2α2β2L2 E(cid:104) ∥z¯ ∥2(cid:105)(cid:27)
5 0 6 7 k (1−β)2 k−1
(cid:20) αC (1−β)2 (cid:21)
+ 6C nc (1−β)2+ 6 +C n(1−β)2 α2(2Cσ∗+σ2).
5 0 n 7 f
Lettingα≤(1−β)/(5L),itissufficienttodetermineC -C ,andβ suchthat
5 7
15α2C (1−β)2c L(C+L) 2α3(1−β)L(L+C)C 5+β
ρ˜ + 5 0 + 6 +2α2C (1−β)L(L+C)≤ , (89a)
w 1−ρ˜ n 7 6
w
α2c ρ˜ α2C (1+ρ˜ ) 5+β
0 w + 5 w ≤ α2C , (89b)
1−ρ˜ 2 6 5
w
(1+β)α3C 5+β
9α3C nc (1−β)(C+L)+ 6 +2α3C n(C+L)≤ α3C , (89c)
5 0 2 7 6 6
3α2C (1−β)2c β 5+β
5 0 +α2C β ≤ α2C . (89d)
1−ρ˜ 7 6 7
w
For(89d),wecanchooseC =18c C . For(89b)itsufficesthat
7 0 5
(cid:18) (cid:19)
c ρ˜ 5+β 1+ρ˜
0 w ≤ − w C .
1−ρ˜ 6 2 5
w
Wecanthenchoose
3c ρ˜ 54c2ρ˜
C := 0 w , C := 0 w , β ≥ρ˜ .
5 (1−ρ˜ )2 7 (1−ρ˜ )2 w
w w
SubstitutingC andC into(89c)yields
5 7
27c2ρ˜ nC(1−β) 108c2ρ˜ n(C+L) (1−β)C
0 w + 0 w ≤ 6. (90)
(1−ρ˜ )2 (1−ρ˜ )2 3
w w
Therefore,wecanchoose
405nc2(C+L)
C := 0 , β ≥ρ˜ . (91)
6 (1−ρ˜ )3 w
w
SubstitutingC −C into(89a)andnotingβ ≥ρ˜ leadto
5 7 w
45α2(1−β)2c2L(C+L)ρ˜ 910c2α3(1−β)L(L+C)2 108c2ρ˜ α2(1−β)L(L+C) 5(1−ρ˜ )
0 w + 0 + 0 w ≤ w . (92)
(1−ρ˜ )3 (1−ρ˜ )3 (1−ρ˜ )2 6
w w w
28DistributedStochasticMomentumTracking APREPRINT
Itsufficestoletα≤(1−ρ˜ )/[160c (L+C)]for(92)tohold. Then,theinequality(88)becomes
w 0
5+β
72nc2α2(1−β)2(2Cσ∗+σ2)(cid:20) 45α(C+L)(cid:21)
H ≤ H + 0 f 1+
k+1 6 k (1−ρ˜ )2 8(1−ρ˜ )
w w
(cid:20) 144α2nc2(1−β)2(C+L)(1+5αL/(1−β)) 6370α3nc2(1−β)C(C+L)(cid:21)
+ 0 + 0 L (93)
(1−ρ˜ )2 (1−ρ˜ )3 k
w w
5+β 168α2(1−β)2nc2(2Cσ∗+σ2) 288α2c2(1−β)2n(C+L)L
≤ H + 0 f + 0 k,
6 k (1−ρ˜ )2 (1−ρ˜ )2
w w
wherethelastinequalityfollowsfromα≤(1−β)/[234(C+L)].
Unrolling(93)andsubstituting(86)yield
(cid:18) 5+β(cid:19)k 1008α2(1−β)nc2(2Cσ∗+σ2)
H ≤ H + 0 f
k 6 0 (1−ρ˜ )2
w
+
288α2c2 0(1−β)2n(C+L)L
0
(cid:16)
1−
αµ(cid:17)k(cid:88)k (cid:32) 1− 1− 6β(cid:33)j
(1−ρ˜ )2 4 1− αµ
w j=0 4 (94)
(cid:40) (cid:41)
+
1728α2(1−β)c2 0n(C+L) 4αL(2Cσ f∗+σ2)
+
16α2L2S 1(2Cσ f∗+σ2)
(1−ρ˜ )2 nµ (1−ρ˜ )µ
w w
≤(cid:18) 5+β(cid:19)k
H +
25nL
0
(cid:16)
1−
αµ(cid:17)k
+
1008α2(1+S 2)(1−β)nc2 0(2Cσ f∗+σ2)
,
6 0 L+C 4 (1−ρ˜ )2
w
wherethefollowingrelationsarenoted:
(cid:88)k (cid:32) 1− 1− 6β(cid:33)j ≤(cid:88)k (cid:18) 10+2β(cid:19)j
≤
12
, S :=
L
+
S 1L
,
1− αµ 11+β 1−β 2 nµ µ
j=0 4 j=0 (95)
(cid:26) (cid:27)
1−β 1
α≤min , .
4L 12(C+L)
√
Substituting(86)and(94)intotheresultofLemma3.5andinvoking(84)yieldthat,forα≤(1−β)/(2L 3),there
holds
(cid:104) (cid:105) 3+β (cid:104) (cid:105) (1−β)2(2Cσ∗+σ2)
E ∥z¯ ∥2 ≤ E ∥z¯ ∥2 + f
k 4 k−1 n
(cid:40) (cid:41)
+2(1−β)(C+6L)
(cid:16)
1−
αµ(cid:17)k
L +
4αL(2Cσ f∗+σ2)
+
16α2L2S 1(2Cσ f∗+σ2)
4 0 nµ (1−ρ˜ )µ
w
+
2(1−β)L(L+C)(cid:40)(cid:18) 5+β(cid:19)k
H +
25nL
0
(cid:16)
1−
αµ(cid:17)k
+
1008α2(1+S 2)(1−β)nc2 0(2Cσ f∗+σ2)(cid:41)
n 6 0 L 4 (1−ρ˜ )2
w
24L(L+C)H (cid:18) 5+β(cid:19)k (cid:16) αµ(cid:17)k 4(1−β)(2Cσ∗+σ2)
≤ 0 +24L (C+L) 1− + f
n 6 0 4 n
(cid:20) (cid:21)
6L+C 4αLS (C+6L) 252α(1+S )(1−β)(L+C)
+ + 1 + 2 32αL(2Cσ∗+σ2).
nµ (1−ρ˜ )µ (1−ρ˜ )2 f
w w
(96)
FromLemma3.3,wehave
(cid:18) 1− 2αβL(cid:19) E[f(x¯ )−f∗]≤E(cid:2) f(d¯ )−f∗(cid:3) + 2αβ E(cid:104) ∥z¯ ∥2(cid:105) .
1−β k k 1−β k−1
29DistributedStochasticMomentumTracking APREPRINT
Therefore,wehaveforα≤(1−β)/(4βL)that
n
1 (cid:88)
E[f(x )−f∗]≤2E[f(x¯ )−f∗]+
L E(cid:104)
∥Πx
∥2(cid:105)
n i,k k n k
i=1 (97)
4αβ (cid:104) (cid:105) L
≤4L + E ∥z¯ ∥2 + H .
k 1−β k−1 n k
Substituting(86),(94),and(96)into(97)andsetting
(cid:26) (cid:27)
1−β 1−β
α≤min ,
32(C+L) 4(C+6L)
yieldsthedesiredresult.
References
[1] S. A. ALGHUNAIM AND K. YUAN,Aunifiedandrefinedconvergenceanalysisfornon-convexdecentralized
learning,IEEETransactionsonSignalProcessing,70(2022),pp.3264–3279.
[2] L.BOTTOU,F.E.CURTIS,ANDJ.NOCEDAL,Optimizationmethodsforlarge-scalemachinelearning,Siam
Review,60(2018),pp.223–311.
[3] T.BROWN,B.MANN,N.RYDER,M.SUBBIAH,J.D.KAPLAN,P.DHARIWAL,A.NEELAKANTAN,P.SHYAM,
G. SASTRY, A. ASKELL, ET AL., Language models are few-shot learners, Advances in neural information
processingsystems,33(2020),pp.1877–1901.
[4] J.CHENANDA.H.SAYED,Onthelimitingbehaviorofdistributedoptimizationstrategies,in201250thAnnual
AllertonConferenceonCommunication,Control,andComputing(Allerton),IEEE,2012,pp.1535–1542.
[5] J.CHENANDA.H.SAYED,Onthelearningbehaviorofadaptivenetworks‚Äîparti: Transientanalysis,IEEE
TransactionsonInformationTheory,61(2015),pp.3487–3517.
[6] A. CHOWDHERY, S. NARANG, J. DEVLIN, M. BOSMA, G. MISHRA, A. ROBERTS, P. BARHAM, H. W.
CHUNG,C.SUTTON,S.GEHRMANN,ETAL.,Palm: Scalinglanguagemodelingwithpathways,arXivpreprint
arXiv:2204.02311,(2022).
[7] J.GAO,X.-W.LIU,Y.-H.DAI,Y.HUANG,ANDJ.GU,Distributedstochasticgradienttrackingmethodswith
momentumaccelerationfornon-convexoptimization,ComputationalOptimizationandApplications,84(2023),
pp.531–572.
[8] E.GHADIMI,H.R.FEYZMAHDAVIAN,ANDM.JOHANSSON,Globalconvergenceoftheheavy-ballmethodfor
convexoptimization,in2015Europeancontrolconference(ECC),IEEE,2015,pp.310–315.
[9] K. HUANG, X. LI, A. MILZAREK, S. PU, AND J. QIU,Distributedrandomreshufflingovernetworks,IEEE
TransactionsonSignalProcessing,71(2023),pp.1143–1158.
[10] K.HUANG,X.LI,ANDS.PU,Distributedstochasticoptimizationunderageneralvariancecondition,arXiv
preprintarXiv:2301.12677,(2023).
[11] K.HUANGANDS.PU,Improvingthetransienttimesfordistributedstochasticgradientmethods,IEEETransac-
tionsonAutomaticControl,(2022).
[12] K. HUANGANDS. PU,Cedas: Acompresseddecentralizedstochasticgradientmethodwithimprovedconver-
gence,2023,https://arxiv.org/abs/2301.05872.
[13] K.HUANG,L.ZHOU,ANDS.PU,Distributedrandomreshufflingmethodswithimprovedconvergence,2023,
https://arxiv.org/abs/2306.12037.
[14] H.KARIMI,J.NUTINI,ANDM.SCHMIDT,Linearconvergenceofgradientandproximal-gradientmethodsunder
thepolyak-łojasiewiczcondition,inJointEuropeanconferenceonmachinelearningandknowledgediscoveryin
databases,Springer,2016,pp.795–811.
[15] A.KHALEDANDP.RICHTÁRIK,Bettertheoryforsgdinthenonconvexworld,arXivpreprintarXiv:2002.03329,
(2020).
[16] A.KOLOSKOVA,T.LIN,ANDS.U.STICH,Animprovedanalysisofgradienttrackingfordecentralizedmachine
learning,AdvancesinNeuralInformationProcessingSystems,34(2021),pp.11422–11435.
[17] A. KOLOSKOVA, S. U. STICH, AND M. JAGGI,Decentralizedstochasticoptimizationandgossipalgorithms
withcompressedcommunication,arXivpreprintarXiv:1902.00340,(2019).
30DistributedStochasticMomentumTracking APREPRINT
[18] A.KRIZHEVSKY,G.HINTON,ETAL.,Learningmultiplelayersoffeaturesfromtinyimages,(2009).
[19] Y. LEI, T. HU, G. LI, AND K. TANG, Stochastic gradient descent for nonconvex learning without bounded
gradientassumptions,IEEEtransactionsonneuralnetworksandlearningsystems,31(2019),pp.4394–4400.
[20] X.LIANDA. MILZAREK,Aunifiedconvergencetheoremforstochasticoptimizationmethods,arXivpreprint
arXiv:2206.03907,(2022).
[21] X. LIAN, C. ZHANG, H. ZHANG, C.-J. HSIEH, W. ZHANG, AND J. LIU, Can decentralized algorithms
outperformcentralizedalgorithms? acasestudyfordecentralizedparallelstochasticgradientdescent,inNIPS,
2017,pp.5336–5346.
[22] T.LIN,S.P.KARIMIREDDY,S.U.STICH,ANDM.JAGGI,Quasi-globalmomentum:Acceleratingdecentralized
deep learning on heterogeneous data, in International Conference On Machine Learning, Vol 139, vol. 139,
JMLR-JOURNALMACHINELEARNINGRESEARCH,2021.
[23] Y.LIU,Y.GAO,ANDW.YIN,Animprovedanalysisofstochasticgradientdescentwithmomentum,Advancesin
NeuralInformationProcessingSystems,33(2020),pp.18261–18271.
[24] Y. LU AND C. DE SA,Optimalcomplexityindecentralizedtraining,inInternationalConferenceonMachine
Learning,PMLR,2021,pp.7111–7123.
[25] L. LUOANDH. YE,Anoptimalstochasticalgorithmfordecentralizednonconvexfinite-sumoptimization,arXiv
preprintarXiv:2210.13931,(2022).
[26] K.MISHCHENKO,A.KHALEDRAGABBAYOUMI,ANDP.RICHTÁRIK,Randomreshuffling: Simpleanalysis
withvastimprovements,AdvancesinNeuralInformationProcessingSystems,33(2020).
[27] G.MORRAL,P.BIANCHI,ANDG.FORT,Successandfailureofadaptation-diffusionalgorithmswithdecaying
stepsizeinmultiagentnetworks,IEEETransactionsonSignalProcessing,65(2017),pp.2798–2813.
[28] A.NEDIC´,A.OLSHEVSKY,ANDM.G.RABBAT,Networktopologyandcommunication-computationtradeoffs
indecentralizedoptimization,ProceedingsoftheIEEE,106(2018),pp.953–976.
[29] A.NEDICANDA.OZDAGLAR,Distributedsubgradientmethodsformulti-agentoptimization,IEEETransactions
onAutomaticControl,54(2009),pp.48–61.
[30] OPENAI,Gpt-4technicalreport,2023,https://arxiv.org/abs/2303.08774.
[31] B.T. POLYAK,Somemethodsofspeedinguptheconvergenceofiterationmethods,Ussrcomputationalmathe-
maticsandmathematicalphysics,4(1964),pp.1–17.
[32] S.PUANDA.GARCIA,Aflocking-basedapproachfordistributedstochasticoptimization,OperationsResearch,
1(2018),pp.267–281.
[33] S.PUANDA.GARCIA,Swarmingforfasterconvergenceinstochasticoptimization,SIAMJournalonControl
andOptimization,56(2018),pp.2997–3020.
[34] S.PUANDA.NEDIC´,Distributedstochasticgradienttrackingmethods,MathematicalProgramming,187(2021),
pp.409–457.
[35] S.PU,A.OLSHEVSKY,ANDI.C.PASCHALIDIS,Asharpestimateonthetransienttimeofdistributedstochastic
gradientdescent,2019,https://arxiv.org/abs/1906.02702.
[36] S.PU,A.OLSHEVSKY,ANDI.C.PASCHALIDIS,Asymptoticnetworkindependenceindistributedstochastic
optimizationformachinelearning: Examiningdistributedandcentralizedstochasticgradientdescent, IEEE
signalprocessingmagazine,37(2020),pp.114–122.
[37] S.PU,A.OLSHEVSKY,ANDI.C.PASCHALIDIS,Asharpestimateonthetransienttimeofdistributedstochastic
gradientdescent,IEEETransactionsonAutomaticControl,(2021).
[38] K. SCAMAN, F. BACH, S. BUBECK, Y. T. LEE, AND L. MASSOULIÉ, Optimal algorithms for smooth and
stronglyconvexdistributedoptimizationinnetworks,ininternationalconferenceonmachinelearning,PMLR,
2017,pp.3027–3036.
[39] Z.SONG,L.SHI,S.PU,ANDM.YAN,Optimalgradienttrackingfordecentralizedoptimization,Mathematical
Programming,(2023),pp.1–53.
[40] A. SPIRIDONOFF, A. OLSHEVSKY, AND I. C. PASCHALIDIS,Robustasynchronousstochasticgradient-push:
Asymptoticallyoptimalandnetwork-independentperformanceforstronglyconvexfunctions.,JournalofMachine
LearningResearch,21(2020),pp.1–47.
[41] Y.TAKEZAWA,H.BAO,K.NIWA,R.SATO,ANDM.YAMADA,Momentumtracking: Momentumacceleration
fordecentralizeddeeplearningonheterogeneousdata,arXivpreprintarXiv:2209.15505,(2022).
31DistributedStochasticMomentumTracking APREPRINT
[42] H. TANG, X. LIAN, M. YAN, C. ZHANG, ANDJ. LIU,d2: Decentralizedtrainingoverdecentralizeddata,in
Proceedingsofthe35thInternationalConferenceonMachineLearning,J.DyandA.Krause,eds.,vol.80of
ProceedingsofMachineLearningResearch,PMLR,10–15Jul2018,pp.4848–4856,http://proceedings.
mlr.press/v80/tang18a.html.
[43] H.TANG,X.LIAN,M.YAN,C.ZHANG,ANDJ.LIU,D2: Decentralizedtrainingoverdecentralizeddata,in
InternationalConferenceonMachineLearning,2018,pp.4848–4856.
[44] J.WANG,Y.LU,B.YUAN,B.CHEN,P.LIANG,C.DESA,C.RE,ANDC.ZHANG,Cocktailsgd: Fine-tuning
foundationmodels over500mbpsnetworks, inInternationalConference onMachineLearning, PMLR,2023,
pp.36058–36076.
[45] Z.WANG,J.ZHANG,T.-H.CHANG,J.LI,ANDZ.-Q.LUO,Distributedstochasticconsensusoptimizationwith
momentumfornonconvexnonsmoothproblems,IEEETransactionsonSignalProcessing,69(2021),pp.4486–
4501.
[46] T.XIAO,X.CHEN,K.BALASUBRAMANIAN,ANDS.GHADIMI,Aone-sampledecentralizedproximalalgorithm
fornon-convexstochasticcompositeoptimization,arXivpreprintarXiv:2302.09766,(2023).
[47] R. XIN, U. A. KHAN, AND S. KAR, An improved convergence analysis for decentralized online stochastic
non-convexoptimization,IEEETransactionsonSignalProcessing,69(2021),pp.1842–1858,https://doi.
org/10.1109/TSP.2021.3062553.
[48] Y.YAN,T.YANG,Z.LI,Q.LIN,ANDY.YANG,Aunifiedanalysisofstochasticmomentummethodsfordeep
learning,inIJCAIInternationalJointConferenceonArtificialIntelligence,2018.
[49] H. YE AND X. CHANG, Snap-shot decentralized stochastic gradient tracking methods, arXiv preprint
arXiv:2212.05273,(2022).
[50] B.YUAN,Y.HE,J.DAVIS,T.ZHANG,T.DAO,B.CHEN,P.S.LIANG,C.RE,ANDC.ZHANG,Decentralized
trainingoffoundationmodelsinheterogeneousenvironments,AdvancesinNeuralInformationProcessingSystems,
35(2022),pp.25464–25477.
[51] K.YUANANDS.A.ALGHUNAIM,Removingdataheterogeneityinfluenceenhancesnetworktopologydepen-
denceofdecentralizedsgd,2021,https://arxiv.org/abs/2105.08023.
[52] K.YUAN,Y.CHEN,X.HUANG,Y.ZHANG,P.PAN,Y.XU,ANDW.YIN,Decentlam:Decentralizedmomentum
sgdforlarge-batchdeeptraining,inProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
2021,pp.3029–3039.
[53] K. YUAN, X. HUANG, Y. CHEN, X. ZHANG, Y. ZHANG, AND P. PAN,Revisitingoptimalconvergencerate
forsmoothandnon-convexstochasticdecentralizedoptimization,AdvancesinNeuralInformationProcessing
Systems,35(2022),pp.36382–36395.
[54] S.ZHANG,S.ROLLER,N.GOYAL,M.ARTETXE,M.CHEN,S.CHEN,C.DEWAN,M.DIAB,X.LI,X.V.
LIN,ETAL.,Opt: Openpre-trainedtransformerlanguagemodels,arXivpreprintarXiv:2205.01068,(2022).
32