How to validate average calibration for machine learning
regression tasks ?
Pascal PERNOT 1
InstitutdeChimiePhysique,UMR8000CNRS,
UniversitéParis-Saclay,91405Orsay,Francea)
Average calibration of the uncertainties of machine learning regression tasks can be
testedintwoways. Onewayistoestimatethecalibrationerror(CE)asthedifference
betweenthemeanabsoluteerror(MSE)andthemeanvariance(MV)ormeansquared
uncertainty. The alternative is to compare the mean squared z-scores or scaled errors
(ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an
ensemble of datasets from the recent machine learning uncertainty quantification lit-
erature. ItisshownherethattheCEisverysensitivetothedistributionofuncertain-
ties, and notably to the presence of outlying uncertainties, and that it cannot be used
reliably for calibration testing. By contrast, the ZMS statistic does not present this
sensitivity issue and offers the most reliable approach in this context. Implications
forthevalidationofconditionalcalibrationarediscussed.
a)Electronicmail: pascal.pernot@cnrs.fr
1
4202
beF
51
]LM.tats[
1v34001.2042:viXraI. INTRODUCTION
The assessment of prediction uncertainty calibration for machine learning (ML) regres-
sion tasks is based on two main statistics: (1) the calibration errors (CE: UCE, ENCE1...)
which are based on the comparison of the mean squared errors (MSE) to mean squared un-
certainties or mean variance (MV); and (2) the Negative Log-Likelihood (NLL2–4) which is
basedonthemeanofsquaredz-scoresorscalederrors(ZMS5).
Averagecalibrationhasbeenshowntobeinsufficienttoguaranteethereliabilityofuncer-
tainties across data space6, but it remains a necessary condition that is too often overlooked
incalibrationstudies.
The MSE=MV equation has been used to test or establish average calibration7,8, but it
mostly occurs in ML through a bin-based setup, meaning that it measures conditional cal-
ibration. As the binning variable is generally the uncertainty1,9, the UCE and ENCE are
typically used to measure consistency, as defined by Pernot5. In post hoc calibration, scaling
factors for the uncertainties can be derived from the ZMS (σ scaling10, BVS9,11) or from the
UCE9.
ThisshortstudyfocusesonthecomparisonofCE-andZMS-basedapproachestovalidate
averagecalibration. Theinterestoftheseaveragecalibrationstatisticsistohaveapredefined
referencevalue,enablingdirectstatisticaltesting. Thisisnotthecaseforbin-basedstatistics
such as the UCE and ENCE, for which validation is much more complex12. De facto, the
latterarepracticallyusedonlyincomparativestudies,withoutvalidation.
The next section defines the calibration statistics and the validation approach. Sect.III
illustrates the comparison of these statistics over an ensemble of datasets issued from the
ML-UQliteratureandproposesananalysisoftheresults. Themainconclusionsarereported
inSect.IV.
II. AVERAGECALIBRATIONSTATISTICS
(cid:8) (cid:9)M
Let us consider a dataset composed of paired errors and uncertainties E ,u to be
i E i i=1
tested for average calibration. The variance-based UQ validation statistics are built on a
probabilisticmodellinkingerrorstouncertainties
E ∼ u D(0,1) (1)
i E
i
where D(µ,σ) is an unspecified probability density function with mean µ and standard
deviation σ. This model states that errors are expected to be unbiased (µ = 0) and that
2uncertaintyquantifiesthedispersionoferrors,accordingtothemetrologicaldefinition13.
A. Thecalibrationerrorandrelatedstatistics
Let us assume that the errors are drawn from a distribution D(0,σ) with unknown scale
parameter σ, itself distributed according to a distribution G. The distribution of errors is
thenascalemixturedistribution H,withprobabilitydensityfunction
(cid:90) ∞
p (E) = p (E|σ)p (σ)dσ (2)
H D G
0
andthevarianceofthecompounddistributionoferrorsisobtainedbythelawoftotalvariance
Var(E) = ⟨Var (E|σ)⟩ +Var (⟨E|σ⟩ ) (3)
D G G D
=< u2 > +Var (⟨E|σ⟩ ) (4)
E G D
The first term of the RHS is the mean squared uncertainty < u2 >. This expression can be
E
comparedtothestandardexpressionforthevariance
Var(E) =< E2 > − < E >2 (5)
Foranunbiasederrordistribution,onegetsVar (⟨E|σ⟩ ) = 0and < E >= 0,leadingto
G D
< E2 >=< u2 > (6)
E
Basedonthisequation,theRelativeCalibrationErrorisaimedtotestaveragecalibration,
andisdefinedas
RMV −RMSE
RCE = (7)
RMV
√
where RMSE is the root mean squared error < E2 > and RMV is the root mean variance
(cid:113)
( < u2 >). The RCE statistic does not depend on the shape of D and its reference value is
E
0.
The RCE occurs in a bin-based statistic of conditional calibration,5 the Expected Normal-
izedCalibrationError1
1 ∑N
ENCE = |RCE | (8)
i
N
i=1
where RCE is estimated over the data in bin i. Depending on the variable chosen to design
i
the bins, the ENCE might be used to test consistency (binning on u ) or adaptivity (binning
E
oninputfeatures).5 TheENCEhasnopredefinedreferencevalue(itdependsonthedataset
and the binning scheme)12, which complicates its use for statistical testing of conditional
calibration.
3B. ZMS andrelatedstatistics
AnotherapproachtocalibrationbasedonEq.1usesscalederrorsorz-scores
E
Z = i ∼ D(0,1) (9)
i
u
E
i
withtheproperty
Var(Z) = 1 (10)
assessing average calibration for unbiased errors14,15. If one accepts that the uncertainties
havebeentailoredtocoverbiasederrors,thecalibrationequationbecomes
ZMS =< Z2 >= 1 (11)
which is the preferred form for testing5, notably when a dataset is split into subsets for the
assessment of conditional calibration. The ZMS does not depend on the shape of D and
its target value is 1. Note that for homoscedastic datasets (u = const.), one gets RCE =
E
1−ZMS.
Thenegativelog-likelihood(NLL)scoreforanormallikelihoodislinkedtotheZMSby16
1 (cid:16) (cid:17)
NLL = < Z2 > + < lnu2 > +ln2π (12)
E
2
It combines the ZMS as an average calibration term17 to a sharpness term driving the uncer-
tainties towards small values18 when the NLL is used as a loss function, hence preventing
theminimizationof< Z2 >byarbitrarylargeuncertainties. Foragivensetofuncertainties,
testingtheNLLvalueisequivalenttotestingtheZMSvalue.
C. Validation
Foragivendataset(E,u )andastatisticϑ,oneestimatesthestatisticoverthedatasetϑ ,
E est
andabootstrappedsamplefromwhichonegetsthebiasofthebootstrappeddistributionb
BS
and a 95% confidence interval I = (cid:2) I− ,I+ (cid:3) . Note that it is generally not recommended
BS BS BS
to correct ϑ from the bootstrapping bias b , but it is important to check that the bias is
est BS
negligible. Considering that errors and uncertainties have generally non-normal distribu-
tions, and that it is not reliable to invoke the Central Limit Theorem to use normality-based
testing approaches (see Pernot14), one has to infer confidence intervals on the statistics by
bootstrapping (BS). The most reliable approach in these conditions is considered to be the
BiasCorrectedAccelerated(BC )method19,whichisusedthroughoutthisstudy.
a
4The most straightforward validation approach is then to check that the target value for
thestatistic, ϑ ,lieswithin I ,i.e.
ref BS
ϑ ∈ (cid:2) I− ,I+ (cid:3) (13)
ref BS BS
Togobeyondthisbinaryresult,itisinterestingtohaveacontinuousmeasureofagreement,
andonecandefineastandardizedscoreζ astheratioofthesigneddistanceoftheestimated
value ϑ to its reference ϑ , over the absolute value of the distance between the ϑ and
est ref est
thelimitoftheconfidenceintervalclosestto ϑ . Moreconcretely
ref

ζ(ϑ ,ϑ ,I ) =
 ϑ
I
Be +s St− −ϑ ϑr ee stf if (ϑ
est
−ϑ ref) ≤ 0
(14)
est ref BS

ϑest−ϑ
ref if (ϑ −ϑ ) > 0
ϑest−I B−
S
est ref
which considers explicitly the possible asymmetry of I around ϑ . The compatibility of
BS est
thestatisticwithitsreferencevaluecanthenbetestedby
|ζ| ≤ 1 (15)
which is strictly equivalent to the interval test (Eq.13). In addition to testing, ζ provides
valuable information about the sign and amplitude of the mismatch between the statistic
anditsreferencevalue.
III. EXPERIMENTS
The validation approach presented above is applied to nine datasets extracted from the
ML-UQliterature,andtheresultsareanalyzed.
A. Thedatasets
Nine test sets, including errors and calibrated uncertainties, have been taken from the re-
centML-UQliteratureforthepredictionofvariousphysico-chemicalpropertiesbyadiverse
panelofMLmethods. Thisselectionrejectedsmalldatasetsandthosepresentingduplicated
properties. Note that for all the datasets, the uncertainties have been calibrated by a palette
of methods with various levels of success. The datasets names, sizes and bibliographic ref-
erences are gathered in TableI, and the reader is referred to the original articles for further
details. In the following, a short notation is used, e.g. ’Set 7’ corresponds to the QM9_E
dataset.
5Set# Name Size(M) Reference
1 Diffusion_RF 2040 Palmeretal.20
2 Perovskite_RF 3834 Palmeretal.20
3 Diffusion_LR 2040 Palmeretal.20
4 Perovskite_LR 3836 Palmeretal.20
5 Diffusion_GPR_Bayesian 2040 Palmeretal.20
6 Perovskite_GPR_Bayesian 3818 Palmeretal.20
7 QM9_E 13885 Busketal.21
8 logP_10k_a_LS-GCN 5000 Rasmussenetal.4
9 logP_150k_LS-GCN 5000 Rasmussenetal.4
TableI.Theninedatasetsusedinthisstudy: number,name,sizeandreference.
Set RCE bias 95%CI ζ Set ZMS bias 95%CI ζ
RCE ZMS
1 0.01860 6.1e-04 [-0.0209,0.0542] 0.47 1 0.960 -1.0e-03 [0.867,1.1] -0.28
2 -0.03870 5.2e-04 [-0.107,0.0193] -0.67 2 0.885 -1.8e-05 [0.803,0.995] -1.05
3 -0.00748 -8.2e-06 [-0.0524,0.04] -0.16 3 1.120 -1.5e-04 [1.05,1.2] 1.67
4 0.05450 -1.2e-03 [0.000718,0.126] 1.01 4 1.230 1.3e-04 [1.16,1.3] 3.48
5 0.09860 4.9e-04 [0.0574,0.135] 2.39 5 0.846 -6.3e-04 [0.777,0.929] -1.85
6 0.09240 1.2e-03 [0.00335,0.16] 1.04 6 0.984 -2.0e-04 [0.857,1.15] -0.10
7 -0.26400 8.6e-03 [-0.685,-0.0028] -1.01 7 0.972 -1.9e-04 [0.936,1.01] -0.71
8 0.04590 1.3e-04 [0.00676,0.0777] 1.17 8 0.926 3.7e-05 [0.869,0.993] -1.10
9 -0.01310 1.2e-04 [-0.0715,0.0263] -0.33 9 0.971 1.7e-04 [0.901,1.08] -0.27
Table II. RCE and ZMS statistics and their validation results. The bold characters signal sets where
|ζ| ≤ 1.
B. Comparisonofvalidationresults
The statistics and confidence intervals have been estimated for the RCE and ZMS for all
datasets with 104 bootstrap replicates. The reference values ϑ are 0 for the RCE and 1
ref
for the ZMS. The results are reported in TableII. It is clear from these results that average
calibrationissatisfiedbyalldatasetsand,moreproblematically,thatthediagnosticdepends
onthechoiceofstatistic.
Comparison of the absolute ζ-scores for ZMS and RCE across the nine datasets shows a
contrastedsituation(Fig.1):
6Figure 1. Comparison of the absolute ζ-scores for ZMS and RCE. The symbols represent the set
numbersinTableII.Thecoloredareascontainthedisagreeingvalidationresults.
• points close to the identity line and more globally in uncolored areas, are the datasets
for which both statistics agree on the calibration diagnostic, i.e. positive for Sets 1and
9,andnegativeforSet5and8.
• two sets have ambiguous validation results for RCE, as they lie on top or are very
close to the validation limit (Sets 4 and 7). For Set 4 the RCE and ZMS values are very
different,whichislessmarkedforSet7.
• Set6isvalidatedbytheZMSandrejectedbytheRCEandbothscoreserverydifferent,
• finally,Sets2and3arevalidatedbyRCEandrejectedbyZMS.
Globally, the statistics disagree on more than half of the datasets, which is somewhat sur-
prisingfortwostatisticsderivinganalyticallyfromthesamegenerativemodel(Eq.1).
7C. Analysis
In order to understand the discrepancy of the validation results by RCE and ZMS, one
needs to consider the sensitivity of these statistics to the uncertainty distributions, and no-
tably to the large, sometimes outlying, values. In the z-scores, these values are likely to
contribute to small absolute values of Z having a small impact on the ZMS, while they
are likely to affect significantly the estimation of the RMV. This hypothesis is tested on the
nine datasets by a decimating experiment, where both statistics are evaluated on datasets
iteratively pruned from their largest uncertainties, and by a simulation experiment with
syntheticuncertaintysetsofvaryingskewness.
1. Sensitivitytotheuppertailoftheuncertaintydistributions
The deviations of the ZMS and RCE scores from their value for the full dataset are esti-
mated for aniterative pruning of thedatasets from their largestuncertainties, as performed
in confidence curves22. The values of ∆ = RCE − RCE and ∆ = ZMS − ZMS
RCE k 0 ZMS k 0
for a percentage k of discarded data varying between 0 and 10% are shown in Fig.2, where
zero-centered bootstrapped 95%CIs for both statistics are displayed as vertical bars to help
toassesstheamplitudeofthedeviations.
It appears that in all cases the ZMS is less or as sensitive as the RCE and that its curve
always strays within the limits of the corresponding CI. For the RCE, one can find cases
wheretheRCEismoresensitivethantheZMSbutlieswithinthelimitsoftheCI(Sets2and
6)andcaseswhereitstraysbeyondthelimitsoftheCI(Sets3,4and7). Thefivelattercases
arepreciselythosewheretheRCEdiagnosticdiffersthemostfromtheZMS.
This sensitivity test by dataset decimation confirms the hypothesis that the RCE is more
sensitive than the ZMS to the upper tail of the uncertainty distribution, to a point where its
estimationmightbecomeunreliable.
2. Reliabilityforskeweduncertaintydistributions
TocharacterizetheimpactofthesensitivityoftheRCEandZMSstatisticstotheuppertail
of the uncertainty distribution in a validation context, a comparison of the rates of failure
of validation tests for both statistics is also performed on synthetic datasets: N = 103
MC
synthetic datasets of size M = 5000 are generated using Eq.1 with a normal generative
distribution [D = N(0,1)] and Inverse Gamma distributions for u2 [u2 ∼ IG(ν/2,ν/2)],
E E
8Figure2. VariationoftheRCEandZMSstatisticsaccordingtothepercentagekoflargestuncertainties
removed from the datasets. The vertical colored bars represent 95% confidence intervals on the
statisticsforthefulldatasets.
with degrees of freedom (ν) varying between 2 and 20. The skewness of the IG distribution
decreasesasνincreases,butisnotdefinedforν ≤ 6. Thiscombinationofprobabilitydensity
functions generates errors with a Student’s-t distribution with ν degrees of freedom (E ∼
t(ν)). t(2) is a distribution with infinite variance, and it is assumed that t(20) is close to a
normaldistribution. Thisisasub-caseoftheNIGdistributionusedinevidentialinference.23
For each sample, the calibration was tested using |ζ| ≤ 1 and a probability of validity
9Figure3. ValidationprobabilityoftheZMSandRCEstatisticsforcalibrateddatasetsgeneratedfrom
theNIGmodelwithνdegreesoffreedom.
wasestimatedas
1 ∑N
p = 1(|ζ| ≤ 1) (16)
val i
N
i=1
where1(x) istheindicatorfunctionwithvalues0when x isfalse,and1when x istrue. The
valuesof p withtheir95%CIsobtainedbyabinomialmodel14,areplottedinFig.3forthe
val
RCEandZMSstatistics.
This test shows that the ZMS is not sensitive to ν, even for extreme uncertainty distri-
butions, and that it provides intervals that consistently validate 95% of the calibrated syn-
theticdatasets. FortheRCE,thevalidationerrorisstronglysensitivetoνforvaluesbelow4,
reaching more than 20% for ν = 2. This confirms the conclusions obtained on the literature
datasets.
IV. CONCLUSION
This study focused on the reliability of RCE and ZMS as average calibration statistics.
Discrepancies of diagnostic between both statistics was observed for an ensemble of nine
10datasets extracted from the recent ML-UQ literature for regression tasks. This anomaly has
been elucidated by showing that the RCE is much more sensitive than the ZMS to the up-
per tail of the uncertainty distribution, and notably to the presence of outliers. A conse-
quence is that average calibration statistics based on the comparison of MSE =< E2 > to
MV =< u2 > should not be relied upon for the kind of datasets found in ML-UQ regres-
E
sion problems. In contrast, the ZMS statistic, which deals globally with the better-behaved
distributionsofscalederrors(Z = E/u )hasnoreliabilityissueandshouldthereforebethe
E
statisticofchoiceforaveragecalibrationtesting.
This finding has consequences for the validation of conditional calibration. The popular
UCE and ENCE statistics implement the comparison of MV to MSE statistics, but, being
bin-based, they are less susceptible to be affected by this sensitivity issue when the binning
variableis u (consistencytesting5). Foralargeenoughnumberofbins,theproblemoflarge
E
outlying u values should vanish, or at least be confined to a single bin. Unfortunately, this
E
is not the case if the UCE or ENCE are based on another binning variable (such as an input
feature), as is requested to test adaptivity.5 In this case, each bin might contain a large range
ofuncertaintyvalues,leadingtothereliabilityproblem. Itmightthereforebebettertoassess
conditionalcalibrationbybinnedZMSstatistics.
AUTHORDECLARATIONS
ConflictofInterest
Theauthorhasnoconflictstodisclose.
CODEANDDATAAVAILABILITY
The code and data to reproduce the results of this article are available at https://github.
com/ppernot/2024_RCE/releases/tag/v1.0 and at Zenodo (https://doi.org/10.5281/
zenodo.8300692).
REFERENCES
1D. Levi, L. Gispan, N. Giladi, and E. Fetaya. Evaluating and Calibrating Uncertainty Pre-
dictioninRegressionTasks. Sensors,22:5540,2022.
112T. Gneiting and A. E. Raftery. Strictly Proper Scoring Rules, Prediction, and Estimation. J.
Am.Stat.Assoc.,pages359–378,2007.
3K. Tran, W. Neiswanger, J. Yoon, Q. Zhang, E. Xing, and Z. W. Ulissi. Methods for com-
paring uncertainty quantifications for material property predictions. Mach. Learn.: Sci.
Technol.,1:025006,2020.
4M. H. Rasmussen, C. Duan, H. J. Kulik, and J. H. Jensen. Uncertain of uncertainties? A
comparison of uncertainty quantification metrics for chemical data sets. J. Cheminf., 15:1–
17,December2023.
5P.Pernot. Calibrationinmachinelearninguncertaintyquantification: Beyondconsistency
totargetadaptivity. APLMach.Learn.,1:046121,2023.
6V. Kuleshov, N. Fenner, and S. Ermon. Accurate uncertainties for deep learning using cal-
ibrated regression. In J. Dy and A. Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,
pages 2796–2804. PMLR, 10–15 Jul 2018. URL: https://proceedings.mlr.press/v80/
kuleshov18a.html.
7J. Wellendorff, K. T. Lundgaard, K. W. Jacobsen, and T. Bligaard. mBEEF: An accurate
semi-localbayesianerrorestimationdensityfunctional. J.Chem.Phys.,140:144107,2014.
8P.PernotandF.Cailliez. Acriticalreviewofstatisticalcalibration/predictionmodelshan-
dlingdatainconsistencyandmodelinadequacy. AIChEJ.,63:4642–4665,2017.
9L. Frenkel and J. Goldberger. Calibration of a regression network based on the predictive
variance with applications to medical images. In 2023 IEEE 20th International Symposium
onBiomedicalImaging(ISBI),pages1–5.IEEE,2023.
10M.-H. Laves, S. Ihler, J. F. Fast, L. A. Kahrs, and T. Ortmaier. Well-calibrated regression
uncertainty in medical imaging with deep learning. In T. Arbel, I. Ben Ayed, M. de Brui-
jne, M. Descoteaux, H. Lombaert, and C. Pal, editors, Proceedings of the Third Conference
on Medical Imaging with Deep Learning, volume 121 of Proceedings of Machine Learning Re-
search, pages 393–412. PMLR, 06–08 Jul 2020. URL: https://proceedings.mlr.press/
v121/laves20a.html.
11P. Pernot. Can bin-wise scaling improve consistency and adaptivity of prediction uncer-
taintyformachinelearningregression? arXiv:2310.11978,October2023.
12P. Pernot. Properties of the ENCE and other MAD-based calibration metrics.
arXiv:2305.11905,May2023.
13BIPM, IEC, IFCC, ILAC, ISO, IUPAC, IUPAP, and OIML. Evaluation of measurement
data - Guide to the expression of uncertainty in measurement (GUM). Technical Report
12100:2008, Joint Committee for Guides in Metrology, JCGM, 2008. URL: http://www.bipm.
org/utils/common/documents/jcgm/JCGM_100_2008_F.pdf.
14P. Pernot. The long road to calibrated prediction uncertainty in computational chemistry.
J.Chem.Phys.,156:114109,2022.
15P. Pernot. Prediction uncertainty validation for computational chemists. J. Chem. Phys.,
157:144103,2022.
16J. Busk, M. N. Schmidt, O. Winther, T. Vegge, and P. B. Jørgensen. Graph neural network
interatomic potential ensembles with calibrated aleatoric and epistemic uncertainty on
energyandforces. Phys.Chem.Chem.Phys.,25:25828–25837,2023.
17W.Zhang,Z.Ma,S.Das,T.-W.Weng,A.Megretski,L.Daniel,andL.M.Nguyen. Onestep
closertounbiasedaleatoricuncertaintyestimation. arXiv:2312.10469,December2023.
18T.Gneiting,F.Balabdaoui,andA.E.Raftery. Probabilisticforecasts,calibrationandsharp-
ness. J.R.Statist.Soc.B,69:243–268,2007.
19T. J. DiCiccio and B. Efron. Bootstrap confidence intervals. Statist. Sci., 11:189–212, 1996.
URL:https://www.jstor.org/stable/2246110.
20G. Palmer, S. Du, A. Politowicz, J. P. Emory, X. Yang, A. Gautam, G. Gupta, Z. Li, R. Ja-
cobs, and D. Morgan. Calibration after bootstrap for accurate uncertainty quantification
inregressionmodels. npjComput.Mater.,8:115,2022.
21J.Busk,P.B.Jørgensen,A.Bhowmik,M.N.Schmidt,O.Winther,andT.Vegge. Calibrated
uncertaintyformolecularpropertypredictionusingensemblesofmessagepassingneural
networks. Mach.Learn.: Sci.Technol.,3:015012,2022.
22P. Pernot. Confidence curves for UQ validation: probabilistic reference vs. oracle.
arXiv:2206.15272,June2022.
23A. Amini, W. Schwarting, A. Soleimany, and D. Rus. Deep Evidential Regression.
arXiv:1910.02600,October2019.
13