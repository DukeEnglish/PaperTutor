1
Inadequacies of Large Language Model Benchmarks
in the Era of Generative Artificial Intelligence
Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Senior Member, IEEE, and Malka N. Halgamuge,
Senior Member, IEEE
Abstract—The rapid rise in popularity of Large Language of Experts (MoE) for specialized domain knowledge process-
Models (LLMs) with emerging capabilities has spurred public ing, have showcased their strengths in reasoning, language
curiositytoevaluateandcomparedifferentLLMs,leadingmany
tasks, and multimodal capabilities, reflecting efforts to align
researcherstoproposetheirLLMbenchmarks.Noticingprelimi-
thesemodelswithdiversescientificandsocietalneeds[1]–[3].
naryinadequaciesinthosebenchmarks,weembarkedonastudy
tocriticallyassess23state-of-the-artLLMbenchmarks,usingour Withover10,000NaturalLanguageProcessing(NLP)models
novelunifiedevaluationframeworkthroughthelensesofpeople, available on platforms like Hugging Face, and many com-
process, and technology, under the pillars of functionality and mercial LLMs available publicly, competition among LLM
security.Ourresearchuncoveredsignificantlimitations,including
developers has intensified. This “race to the top” has driven
biases, difficulties in measuring genuine reasoning, adaptability,
efforts to evaluate and compare LLMs, using challenge sets
implementation inconsistencies, prompt engineering complexity,
evaluator diversity, and the overlooking of cultural and ideolog- and leaderboards focused on tasks like question-answering,
ical norms in one comprehensive assessment. Our discussions reading comprehension, and common sense reasoning, and
emphasized the urgent need for standardized methodologies, ofteninvolvesleveragingbenchmarkstoassertsuperiorityover
regulatory certainties,andethical guidelinesin lightof Artificial
othermodels[3],[4].Inthiscontext,a“benchmark”referstoa
Intelligence (AI) advancements, including advocating for an
standardized set of tasks or datasets designed to evaluate and
evolutionfromstaticbenchmarkstodynamicbehavioralprofiling
to accurately capture LLMs’ complex behaviors and potential compare the performance of different LLMs across various
risks. Our study highlighted the necessity for a paradigm shift dimensions such as accuracy, robustness, and reasoning [3].
inLLMevaluationmethodologies,underliningtheimportanceof Unlike the automobile and aviation industries, where clear
collaborative efforts for the development of universally accepted
regulations and well-defined public consensus guide bench-
benchmarksandtheenhancementofAIsystems’integrationinto
marking practices, the advanced AI field lacks such univer-
society.
sally accepted standards, leading many researchers to devise
Index Terms—Artificial Intelligence (AI), AI Evaluation,
their own benchmarks. Benchmarks of LLMs, such as BIG-
Benchmark,Cybersecurity,EvaluationFrameworks,LargeLan-
bench [4] by Google DeepMind and PromptBench [5] by
guage Model (LLM).
MicrosoftResearch,haveencompasseddiversemethods,each
withuniqueapproachesandcriteria,focusingmainlyonexam-
I. INTRODUCTION style or task-based assessments. Such methods, evaluating
models’ abilities to perform specific functions or solve prob-
LARGE Language Models (LLMs), as an advanced form
lems, typically emphasized tasks with predefined answers
of generative Artificial Intelligence (AI), have signif-
or scenarios with limited variability [5], [6]. Models were
icantly evolved recently, transitioning from narrow, task-
evaluatedusingmetricslikeaccuracy,perplexity,andF1-score
specific systems to versatile, multi-purposemodelscapableof
on fixed datasets (e.g., [7], [8]), or using human evaluators
few-shotlearningacrossdiverse tasks [1], [2]. LLMs are usu-
to measure “human-level” performance (e.g., [9], [10]). The
allyevaluatedusingautomaticevaluationmethods,whichrely
common approach of LLM benchmarking has often assumed
on standard metrics and tools for efficiency and standardiza-
a standardized “correct” answer or a marking rubric for each
tion, and human evaluation, which offers insights into quality
question, which LLMs are expected to reproduce, and the
andaccuracythroughexpertanduserassessments,reflectinga
samebenchmarksetsarerepeatedlyusedtocompareandrank
balancebetweencomputationalprecisionandhumanjudgment
LLMs, ignoring their broader implications and real-world ap-
[3].RecentadvancementsofLLMs,suchasGPT-4andGoogle
plicability.While providinginsights, we believethis approach
Geminiwith their multimodalityfor enhancedcomprehension
often fails to capture the subtleties and complexities of real-
andreasoning,alongsideMistral8x7B’sintegrationofMixture
world use, as previous studies mainly focused on function-
ality within the technological context, neglecting processual
Manuscript received February16,2024.
TimothyMcIntoshiswithAcademiesAustralasiaPolytechnic, Melbourne, and human aspects, especially the diversity of human values
VIC3000,Australia (e-mail: t.mcintosh@aapoly.edu.au). and cultures [11]. Additionally, LLM benchmarks have often
Tong Liu and Teo Susnjak are with Massey University, Auckland 0632,
lackedathoroughexamination,especiallyinevaluatingLLMs’
NewZealand(e-mail: t.liu@massey.ac.nzandt.susnjak@massey.ac.nz).
PaulWattersiswithCyberstronomyPtyLtd,Ballarat,VIC3350,Australia fullcapabilities,limitations,andsafetyissues,highlightingthe
(e-mail: ceo@cyberstronomy.com). need for a more comprehensive and sophisticated approach.
Malka N. Halgamuge is with RMIT University, Melbourne, VIC 3000,
Evaluating in LLMs and generative AI should require a
Australia (e-mail:malka.halgamuge@rmit.edu.au).
ThisparagraphwillincludetheAssociateEditorwhohandledyourpaper. method that assesses not only standardized task performance,
4202
beF
51
]IA.sc[
1v08890.2042:viXra2
but also their real-world applicability and safety, which calls LLM benchmarking. Section VII provides a discussion on
for a shift from conventional methodologies to more holistic the overarching themes and implications of our findings.
evaluations, reflecting the complexities of modern LLMs and Finally, Section VIII concludes the paper, summarizing the
their diverse roles in society and technology [2], [3]. key findings and contributions.
This study has been motivated by our preliminary observa-
tionsofinadequaciesincurrentLLMbenchmarks,specifically II. BACKGROUND AND RELATEDWORK
theirsystemicgapsthatfailtoaccuratelyandcomprehensively
This section analyzes traditional computer science bench-
evaluate LLM capabilities and risks across technological,
marksandthosespecifictogenerativeAIandLLMs,likeBIG-
processual, and human dimensions. For example, a notable
benchandPromptBench,highlightingtheirmethodologiesand
issueisculturallydependentorvaguebenchmarkingquestions,
inherent limitations.
raisingconcernsabouttheiruniversalapplicabilityandlackof
clear “groundtruth”. Humanvalues, often used as benchmark
A. Traditional Benchmarking in Computer Science
standardanswersorguidelines,exhibitwidediversityandcon-
tradictions, challenging the concept of standardized “human In computer science, traditional benchmarking tests differ-
values”inLLMassessments[11].Manybenchmarksprimarily ent hardware or software systems under identical tasks or
focus on LLM functionality, neglecting their security and ro- conditions to assess performance and quality [12]. Perfor-
bustness,essentialrequirementsincybersecurity,whereLLMs mance benchmarks, exemplified by 3DMark and Cinebench,
canfaceadversarialattacksthatcouldpotentiallycompromise evaluatethespeed andefficiencyof varioushardwaresystems
their functionality, thereby necessitating a reevaluation of in executing specific tasks. Such benchmarks are crucial for
benchmarking criteria. Therefore, this study aims to analyze understanding the capabilities of systems in processing com-
the inadequacies of exam-style benchmarking in multimodal plex computations and graphics rendering, rather than tasks
LLMs. Our research questions include: (1) How to identify, likefiletransferspeedordisplaycalibration[12],[13].Onthe
categorize, and explain the common inadequacies of state-of- other hand, quality benchmarks focus on the effectiveness of
the-art LLM benchmarks; (2) Whether such inadequacies are outcomes, such as assessing the compression ratio of lossless
evident in the popular benchmarks we have identified; and compressionalgorithms[14],evaluatingthedetectionaccuracy
(3) What a comprehensive evaluation of LLM benchmarks of anti-malware tools by VB100 or AV-Comparatives, and
should include, considering both functionality and security measuringtheprecisionofbiometricrecognitionsystemssuch
for a complete understanding of risks and societal impacts as fingerprintscannersin smartphonesor facialrecognitionin
of LLMs. We propose a unified evaluation framework for security systems [15].
LLM benchmarks, aligned with the cybersecurity domains The primary aim of benchmarking in computer science is
of people, process, and technology, and it will facilitate a to compare systems against idealized standards or establish
comprehensive examination of benchmarks, assessing both relative performancerankings, which has significant commer-
functionality and security in each domain. The framework is cial implications[12], [13]. For example,in high-stakesareas
designed to enhance LLM benchmark evaluations, revealing like Central Processing Unit (CPU) performance,benchmark-
where current practices are deficient and providing a route to ing metrics like Instructions Per Cycle (IPC) can influence
betterassess LLMsandgenerativeAI,asimprovedevaluation major purchasing decisions, potentially involving millions of
methods are vital for guiding more effective and secure LLM dollars. However, the integrity of benchmarking is at times
and generative AI development and deployment. compromisedbypracticessuch as benchmarkcheating.Some
The major contributions of this study include: System on a Chip (SOC) manufacturers, for instance, have
beenfoundtoartificiallyboostchipfrequenciesupondetecting
1) Developing a unified evaluation framework for LLM
benchmarking code, achieving higher performance scores 1,
benchmarks that integrates the cybersecurity domains
while software developers might use algorithms optimized
of people, process, and technology, thereby facilitating
solely for benchmark tests2, distorting results from actual
comprehensiveassessmentsoffunctionalityandsecurity.
performance scenarios.
2) Criticallyanalyzing23state-of-the-artLLMbenchmarks
Thereliabilityofbenchmarkingresultsincomputerscience
tohighlighttheprevalenceandspecificsofcurrentinad-
and AI depends critically on their reproducibility, generaliz-
equacies,emphasizingthe needforenhancedevaluation
ability across different contexts, consistency over time, and
methods.
objectivity in measurement, but at the same time, it raises the
3) Proposing an extension of LLM benchmarkswith LLM
questionofwhoholdstheauthoritytosetbenchmarkstandards
behavioral profiling and audits, aiming to improve the
[16], [17]. Consider that the same anti-malware products
inclusivity and security insights of LLM evaluations.
can score differently in tests like VB100, AV-Comparatives,
The rest of this paper is organized as follows: Section II and AV-Test, even when tests are administered around the
offersacomprehensiveanalysisofprevalentbenchmarks.Sec- same time, indicating variations in benchmarkingcriteria and
tion III introducesthe proposed unified evaluation framework evaluation methodologies [18]. Moreover, within the context
for assessing LLMs. Section IV examines the technological
challenges in current LLM benchmarking practices. Section
1https://www.anandtech.com/show/15703/mobile-benchmark-cheating-
mediatek
V discusses the processual elements affecting LLM evalua-
2https://www.mensxp.com/technology/smartphones/86880-smartphone-
tions. Section VI explores the human dynamics influencing companies-caught-cheating-on-benchmark-apps.html3
of cybersecurity,ourprevioussurveyon ransomwarerevealed be exploited to generate and propagate bias, misinformation
concerns about the self-benchmarkingpractices in many anti- or disinformation, thereby undermining LLM reliability and
ransomware studies, which often used diverse ransomware trustworthiness,andcansignificantlyimpacttheirusability[2],
samples and methodologies and based their claims of su- [11], [21].
periority merely on higher detection rates [19]. This situa- Given our preliminaryobservationsand the challengesout-
tion underscores the need for rigorous, comprehensive, and lined byAnthropicin [20], includingmeasuringsocialbiases,
scientifically sound benchmarking practices that can provide the subjectivity of human evaluations, and red teaming com-
reliable, meaningful and universally accepted comparisons. plexities,itbecomesevidentthatexaminingLLMbenchmarks
for potential inadequacies is valuable, to identify areas for
B. Benchmarking in Generative AI and LLMs significantimprovementsandenhancements,particularlyinin-
Generative AI, particularly LLMs, has introduced distinct corporating diverse languages, ensuring response consistency,
benchmarkingchallenges unlike those in traditional computer and addressing both functionality and security aspects. Such
science [4]–[6], [20]. As highlighted by the LLM company an examination is vital to align with the rapidly evolving AI
Anthropic, developing robust and reliable model evaluations technology landscape and its broad impact on societies.
is a complex task, especially considering the limited ability
III. UNIFIED EVALUATION FRAMEWORK FORLLM
of existing evaluation suites to accurately indicate model
BENCHMARKS
capabilities or safety [20]. LLMs generate a wide range of
outputs and often operate in a black-box manner, assessed The evaluation of LLMs, given their nature as advanced
through methods that differ significantly from those used software systems, sometimes augmented with specialized
for conventional systems, whereas humans typically generate hardware or sensors, necessitates an approach rooted in ro-
questionsand standardizedanswers for LLMs, expectingspe- bust cybersecurity risk assessment principles. This approach
cific outcomes [1], [3], [21]. The evaluation process varies, extends beyond traditional assessments to accommodate the
involving text string comparisons for straightforward answers novel capabilities and complexities of AI technologies.
or relying on human judgment for more complex and subtle
A. Cybersecurity Principles in LLM Benchmarking
responses [3]. Many research organizations and groups have
attemptedtointroducetheirownLLMbenchmarks,whichwe Incybersecurity,thecomprehensiveandholisticassessment
have listed and analyzed in Table I. of systems typically involves the three pillars of “people”,
A notable observation in the state-of-the-art LLM bench- “process”,and“technology”,apracticeconsistentlyadvocated
marks, which we selected based on the comprehensivesurvey across major cybersecurity frameworks like COBIT and ISO
by Zhao et al. [38] that self-claimed to be the standardized 27001, to highlight the importance of evaluating risks related
benchmarksfor LLMs, was the predominanceof benchmarks to human factors, process inefficiencies, and technological
inEnglish,occasionallyinSimplifiedChinese(althoughoften vulnerabilities [39]. We believe such a holistic approach is
simply referred to as “Chinese”, overlooking differences in equally effective and crucial when applied to LLMs, where
Traditional Chinese). Those benchmarks typically adopted an a narrow focus on technology might overlook crucial aspects
exam-style or task-based format of questions and answers, related to human interaction with those LLMs, and the pro-
including Multiple Choice Questions (MCQs) and conversa- cesses governing their deployment and maintenance, due to
tional dialogues. However, many of those benchmarks did theiroperationalintricaciesandpotentialrisks, thusproviding
not consider alternative answers that might have been correct athoroughunderstandingoftheirstrengthsandvulnerabilities,
in different contexts, including divergent cultures, religions, and ensuring their reliable and secure application.
andlanguages.Theirbenchmarkingprocessesfrequentlyover- • people:encompassingdevelopment,deployment,and us-
looked the consistency of LLM responses, relying usually age by individuals and teams, which involves evaluating
on the first attempt in a non-conversational setting, which the expertise and skills of those involved in creating and
contrasted with the more typical public use of LLMs in managing LLMs, and the end-users’ understanding and
ongoing dialogues, where conversations evolved over several interaction with these models.
exchanges rather than isolated question-answer pairs. Among • process: focusing on the methods and procedures em-
the 23 benchmark studies we examined, only 5 studies ( [8], ployedinthedevelopment,deployment,andmaintenance
[22], [24], [31], [37]) were foundto havebeenpeer-reviewed, of LLMs, including the robustness of developmentprac-
a fact that did not diminish the scientific value of preprints, tices, effectiveness of deployment strategies, and effi-
but highlighted the evolving and dynamic nature of research ciency of ongoing maintenance and updates.
in this field. Additionally,while most benchmarksfocused on • technology:assessing thetechnicalinfrastructure,includ-
thequalitativeresultsofLLM-generatedanswers,aspectssuch ing hardware and software, that supports LLMs, by ex-
as the speed of result generation were often not measured, an amining the architecture, scalability, and interoperability
oversight critical in applications where fast response time or of systems underpinning LLMs.
a good user experience was preferential. More importantly,
most LLM benchmarks evaluated only the functionality, not B. Functionality and Security in LLM Benchmarks
the cybersecurity aspects, such as susceptibility to syntactic Incybersecurity,theevaluationofsystemstypicallyempha-
attacksorideologicalmanipulation,andwe believethisomis- sizestwopivotalaspects:functionalityandsecurity.Function-
sion was particularlyconcerning.Vulnerabilitiesin LLMscan ality pertains to the system’s capacity to perform its intended4
TABLE I: Comparison of Various Generative AI and LLM Benchmarks (ranked by chronologicalpublication dates)
Name MainAffiliation DataSource FocusArea Language BenchmarkFormat EvaluationMethodology
MMLU[22] UCBerkeley Humanexams Languageunderstanding English MCQs Zero-shotandfew-shotevaluation
CodeSynthesisfrom Programmingproblems
HumanEval[6] OpenAI GitHubcode English Functionalcorrectnessviaunittests
Docstrings withunittests
Stanford Manualtask IRAC&classification Taskperformanceevaluation,
LegalBench[23] Legalreasoning English
University construction tasks comparativeanalysisacrossmodels
FinancialPhraseBank, Financialsentiment
GeorgiaInstitute Sentimentanalysis
FLUE[24] FiQA2018,Goldnews analysis,news English Evaluationondatasets
ofTechnology (regression,classification)
headlinedataset headlineclassification
MedQA,MedMCQA, Medicalquestion Few-shotprompting,self
MultiMedQA[8] Google English MCQs
PubMedQA,etc. answering -consistency,humanevaluation
Tianjin Officialexamsand Multilevel,multisubject Zero-shotandfew-shotevaluation,
M3KE[7] SimplifiedChinese MCQs
University educationalmaterials knowledgeevaluation comparativeanalysisacrossmodels
SambaNova Empiricalanalysisof Softwaretool Comparativeanalysisacrossmodels,
T-Bench[25] English Diversesoftwaretools
SystemsInc. open-sourceLLMs manipulationcapability performanceevaluation
Chain-of-Thought Universityof Curatedreasoning English, Few-shotchain-of-thoughtprompting,
Reasoningperformance Diversereasoningtasks
Hub[26] Edinburgh benchmarks SimplifiedChinese Finalansweraccuracyevaluation
Knowledgememorization, Standardizedoverallscoring,
Tsinghua Knownandevolving
KoLA[9] Worldknowledge English understanding,applying, self-contrastmetricfor
University datasources
andcreatingtasks knowledgecreation
College-level Comparisonwithcorrectanswers;
SciBench[27] UCLA textbooksand Scientificproblems English Open-endedquestions gradedbyinstructors’rubrics;
courseexams human-verifiedsolutions
DuckAI& Standardizedtests, MCQs,shortanswer, Task-specificinstructions,
ARB[28] Advancedreasoning English
GeorgiaTech problembooks openresponsequestions automaticandmanualevaluation
Zero-shotandfew-shotevaluation,
Comprehensivedomain Multidisciplinary, English,
Xiezhi[29] FudanUniversity MCQs accuracymeasurement,
knowledgeevaluation 516disciplines SimplifiedChinese
automatedtestaccuracyreporting
BIG-bench[4] Google Humanannotation Generalknowledge English Tasks Algorithmandhumanraters
Zero-shotandfew-shotevaluation,
Officialpublicand Human-centricreasoning English, MCQs,
AGIEval[10] Microsoft chain-of-thoughtreasoning,
high-standardexams tasks SimplifiedChinese fill-in-the-blank
quantitativeandqualitativeanalysis
Chinese Simulatedtool-use
Generalizedtoollearning Performanceevaluationon
ToolAlpaca[30] Academy Multi-agentsimulation English instancesin
forlanguagemodels unseentools,diversityintoolset
ofSciences multiplecategories
Stanford Multi-metricmeasurement,Dense
HELM[31] Benchmarkcollection Metrics-basedevaluation English 16Scenarios,7metrics
University evaluationofscenariosandmetrics
AssessmentofAPIretrievalaccuracy,
Curatedfrom16,000+
Tsinghua APIandtool-augmented API-basedtasksin taskperformance,andgeneralization
ToolBench[32] real-worldAPIsusing English
University LLMperformance variousscenarios capabilityusingbothground-truth
ChatGPT
andgeneratedAPIs
Quickperformanceassessment,
PromptBench[5] Microsoft Benchmarkcollection Generalknowledge English APIs,datasets,models dynamicevaluation,semantic
evaluation
Tsinghua Code,game,andweb Agentperformance English, Diverseinteractivetasks Evaluationacrossmultipleenvironments,
AgentBench[33]
University environments ininteractivetasks SimplifiedChinese acrossenvironments comparativeanalysisofLLMsasagents
Tool-augmentedLLM AccuracyofAPIcallsandresponse
APIBank[34] AlibabaGroup Syntheticdialogues English Conversationaldialogues
performance qualitymeasuredbyROUGE-Lmetric
Shanghai Zero-shotandfew-shotevaluation,
Mockexams,high Multilevel,multidiscipline
C-Eval[35] JiaoTong SimplifiedChinese MCQs Accuracymeasurement,
-standardexams knowledgeevaluation
University Automatedtestaccuracyreporting
Performancecomparisonoforchestrated
Salesforce Simulated Autonomousagent
BOLAA[36] English Decision-making agentarchitectures,quantitative
Research(USA) environments orchestration
analysisofagentinteractions
Renmin ChatGPT-generated
QA,dialogue,and API-basedautomaticevaluation
HaluEval[37] University andhuman-annotated LLMhallucinations English
summarization accordingtoprovidedanswers
ofChina samples
tasks under standard operating conditions, while security as- such gaming strategies.
sesses the system’s resilience against malicious attempts that
aimtocompromiseitsintegrity,confidentiality,andavailability
C. Applying the Framework to LLM Benchmarks
[39]. We hereby adaptively extended those principles in the
critique of LLM benchmarks to encompass: Inthisresearch,weemployeda“reversethinking”approach
(specifically, identifying counter-examples to challenge the
• FunctionalityAssessment:evaluatingtheaccuracyandef- assumptions that a benchmark is fully adequate in LLM
fectivenessof benchmarksin measuringLLMs’ capabili- evaluation), pivoting from our initial observations of cultural
tiestodeliverprecise,dependable,andefficientresponses inadequacies and the lack of inclusivity in LLM benchmarks.
under benign conditions, ensuring they meet predefined Contrary to traditional evaluations that often relied on com-
performance standards without being influenced by the pleting checklists of requirements, our methodology sought
benchmarks themselves. counter-examples to challenge the LLM benchmarks, which
• Security Considerations: scrutinizing benchmarks for presupposedthatidentifyingasinglecounter-examplesufficed
their vulnerabilityto being gamed or bypassed by LLMs todemonstrateaninadequacywithinabenchmarkstudy.After
optimized to exploit evaluation criteria without truly pinpointing the most prevalent and systemic inadequacies,
possessing the evaluated capabilities. It addresses the ignoring minor study flaws, we devised a unified evaluation
robustness of benchmarks against attempts by LLMs to frameworktore-examineallLLMbenchmarks,systematically
produce favorable outcomes through optimization tricks assessing theidentifiedinadequaciesandcategorizingthemas
thatdonotreflecttheirtrueperformanceorsafetylevels, either: (1) present and unacknowledged (marked as ×), (2)
ensuring the integrity of the evaluation process against acknowledgedbutunresolved(markedas△),or(3)considered5
Response Variability in
Start evaluation 22
Standardized Evaluations
Genuine Reasoning vs
22
Technical Optimization
Evaluate
benchmark Tension - Helpfulness
19
inadequacies and Harmlessness
Linguistic Variability and
17
Embedded Logic Diversity
Does each
no or unclear Benchmark Installation 16
inadequacy
and Scalability
apply?
Biases in LLM
9
-Generated Evaluations
yes
0 10 20
Prevalence (max 23)
yes
Efforts made Considered
Fig. 2: Technological Inadequacies in LLM Benchmarking
to addressit? addressed
no
IV. TECHNOLOGICAL ASPECTS
Inthissection,weexaminethetechnologicalintricaciesand
limitations inherent in current LLM benchmarks (Fig. 2).
Acknowledged
Acknowledged yes
but unresolved
inadequacy?
(markedas △) A. Response Variability in Standardized Evaluations
Prevalence: 22/23
no AmajorinadequacyidentifiedinLLMbenchmarkswasthe
Present and variabilityin modelresponses,particularlywhenstandardized
unacknowledged evaluationsareappliedtomodelsdesignedforspecificformats
(marked as ×) or use cases. Frameworks such as [4]–[10], [22]–[28], [30]–
[37], though broad and comprehensive, may not effectively
captureorreflectthetrueperformanceandsubtletiesofmodels
End evaluation tailored for particular scenarios or formats (Appendix A-A).
This discrepancy arose due to the generic nature of bench-
Fig. 1: Evaluation Flowchart for LLM Benchmarks marks which often overlooks the specialized, context-driven
response behavior inherent in certain models. Surprisingly,
simple formatting alterations in benchmarks, like switching
from (A) to (1), from (A) to [A], or inserting an extra space
addressed(unmarked).Ourstructuredmanualevaluationscru- between the option and answer, could shift LLM response
tinizedtechnological,processual,andhumandimensions,with accuracy by about 5%, demonstrating LLMs’ sensitivity to
aspecialemphasisontheextensivelyresearchedtechnological minor input changes and emphasizing the importance of
aspects and critical culturaland linguistic dimensions, aiming standardizing benchmark presentation [20].
to uncover shortcomings in current practices and to suggest The variability in LLM responses to standardized evalu-
future research directions. The evaluation flowchart in Fig. ations has highlighted critical concerns for both the utility
1 illustrates our methodology, which includes assessing each and safety of these models. Functionally, this variability can
identified inadequacy against a given LLM benchmark to misrepresentamodel’struecapabilities,potentiallymisleading
determine its presence and the benchmark’s response to it. developers and users about its practical utility and affecting
deployment strategies [3]. On the security front, LLMs could
The analysis, as summarized in Table II, revealed the
unpredictably fail or be exploited in unanticipated scenarios,
prevalence of various inadequacies across LLM benchmark
underminingtheirreliabilityandsafety[40].Enhancedbench-
studies.Despitethepotentialforsubjectivebiasinourmanual
mark designs that consider model specificity, architecture,
review, our critique was justified by published opinions in
and application contexts shall be essential to address these
existing studies, and our careful evaluative rationale detailed
issues,ensuringaccuraterepresentationandevaluationofLLM
in Appendices A, B, and C. Our aim is not to assert the
capabilities in diverse operational environments.
superiority of one LLM benchmarkover another, but to high-
light common concerns that pervade many such benchmarks,
B. Genuine Reasoning vs Technical Optimization
thereby fostering a broader understanding of their potential
limitations. Prevalence: 22/236
TABLE II: Prevalence of LLM Benchmark Inadequacies(×: present and unacknowledged;△: acknowledgedbut unresolved)
Technological Aspects Processual Elements Human Dynamics
Benchmark
Research
MMLU [22] × × △ △ × × × △ △ △
HumanEval [6] × × × × △ ×
LegalBench [23] × × × × × △ △ △ ×
FLUE [24] × × × × × × × ×
MultiMedQA [8] × △ × △ △ × △ △ △
M3KE [7] × × × × × × × × × △
T-Bench [25] × × × × × × △ △ ×
Chain-of-Thought Hub [26] △ × × × × × △ × ×
KoLA [9] × × △ × △ × × △
SciBench [27] × × × × △ △ △ ×
ARB [28] △ × × × × △ △ △ ×
Xiezhi [29] × × × × × △ △
BIG-Bench [4] × × × × × △ △ ×
AGIEval [10] × × × △ × × × △ ×
ToolAlpaca [30] × × × × × × × △
HELM [31] △ × × △ △ △ △ △
ToolBench [32] × × × × △ ×
PromptBench [5] × △ × × × × × ×
AgentBench [33] × × × × × × × △ × ×
APIBank [34] × × △ × × × × ×
C-Eval [35] △ × × × × × × △
BOLAA [36] × × × × × △
HaluEval [37] × × × × × × △ △ △ △ ×
Another notable inadequacy, as seen in [4]–[8], [10], lenge is crucial, given the complex nature of LLMs and the
[22]–[37] (Appendix A-B), was the difficulty in determining varied approachesto their training,it may be difficult to fully
whetherresponsesgeneratedby LLMsare a resultof genuine resolve. Continuous research and refinement of benchmark
reasoning, or merely due to technical optimization, such as methodologies are necessary to more effectively probe and
overtrainingtomatchbenchmarkanswers.Thisissueemerged assess the genuine reasoning capabilities of LLMs.
fromtheopaque,‘blackbox’natureofLLMs,whichobscured
the understanding of how they generated their outputs. As C. Tension Between Helpfulness and Harmlessness
a result, there existed a potential for deceptive practices, Prevalence: 19/23
where LLMs might appear proficient by exploiting certain There is an inherenttension between a model’s helpfulness
characteristics of benchmarks without truly comprehending and harmlessness, particularly evident in human evaluation
or reasoning through the content. Accurately distinguishing methods, such as A/B tests, where models are assessed on
between LLMs’ genuinereasoningand technicaloptimization their ability to provide helpful and harmless responses in
is essential to ensure their intellectual integrity and prevent open-ended dialogues, as in [5]–[10], [22]–[28], [31]–[33],
themfrommerelycheating,akintosomestudentshavingprior [35]–[37] (Appendix A-C). Offering multiple options without
access to test questions and reciting the answers. judgment, for instance, suggesting various perspectives on
The significance of this inadequacy extended to both the climate change actions, represents one solution. However, it
practicalutility andsecuritymeasuresofLLMbenchmarking. is not always feasible or appropriate, such as in addressing
Functionally, it cast doubt on the benchmarks’ ability to direct inquiries on human rights issues or providing guidance
accurately and comprehensively assess the intended qualities onmedicaltreatments,whereclear,authoritativeresponsesare
of LLMs, especially given that LLMs were more likely to crucial.Thischallengeisfurthercompoundedwhennavigating
encounter the contents of well-known benchmarks during sensitive topics, requiring LLMs to tread carefully between
training,and could simply be optimizedto fit the benchmarks being informative and maintaining neutrality and political
without real comprehension or reasoning [20], [41]. From correctness. Many commercial LLMs have even opted to
a security perspective, this opens possibilities for LLMs to refusetorespondtocertaintopicsaltogether,particularlythose
bypass or manipulate benchmarks, leading to falsified or related to geopolitical or religious sensitivities, to maintain
influenced results [41], [42]. Although addressing this chal- neutrality and avoid controversy [11], [20].
ni ytilibairaV
esnopseR
snoitaulavE
dezidradnatS
sv gninosaeR
eniuneG
noitazimitpO
lacinhceT
neewteB
noisneT
dna
ssenlufpleH
ssensselmraH ytilibairaV
citsiugniL
deddebmE
dna
ytisreviD
cigoL
noitallatsnI
kramhcneB
ytilibalacS
dna
detareneG-MLL
ni
sesaiB
snoitaulavE
MLL
kramhcneB
tnetsisnocnI
noitatnemelpmI
tseT
wolS
emit
noitaretI
reporP
fo
egnellahC
gnireenignE
tpmorP
namuH
ni ytisreviD
srotaulavE
dna
srotaruC
,laicoS
,larutluC
esreviD
dna suoigileR
,lacitiloP
smroN
lacigoloedI7
Thistensionposedsignificantconcernsinbothfunctionality E. Benchmark Installation and Scalability
and cybersecurity. Functionally, benchmarks might struggle
Prevalence: 16/23
to determine a standardized criterion that accurately gauges
A notable inadequacy in LLM benchmarks is the difficulty
the balance between helpfulness and harmlessness, leading to
ininstallingandscalingframeworks,whichislikelytoatleast
potential misinterpretation of LLM capabilities [3], [20]. In
applyto[4],[5],[7],[9],[10],[22],[23],[25],[27]–[30],[33],
security, prioritizing harmlessness may cause LLMs to with-
[34], [36], [37] (Appendix A-E). These benchmarks, crucial
hold information critical for user inquiries, such as malware
for comprehensive evaluations, often posed significant tech-
analysisorsafe bombremovaltechniques,outofconcernthat
nical challenges in terms of installation on different systems
thisinformationcouldbemisused,thusnavigatingthedelicate
andefficientoperationatscale.Theinstallationprocesseswere
balance between being informative and preventing potential
notstraightforwardandrequiredsubstantialengineeringeffort,
harm [40]. Addressing this inadequacy would require a com-
while efficiently running these benchmarks across various
plexapproachthatencompassesbroaderhigh-levelnormsand
scenariosand modelswas equally demanding,often requiring
values, yet the complexity of human language and diverse
extensive resource allocation and infrastructure adjustments.
societal expectations make it a challenging task to resolve.
Researchers at Anthropic found that some benchmarks were
inefficient in scaling, requiring a potential overhaul to align
with their test infrastructure,and an unexpectedinvestmentof
D. Linguistic Variability and Embedded Logic Diversity timeandresourcesto rectifybugsandimplementbenchmarks
effectively [20].
Prevalence: 17/23
This inadequacy impacts both the functionality and cy-
ThesignificantinadequacyincurrentLLMbenchmarkslies
bersecurity aspects of LLM benchmarks. From a functional
in their disregard for linguistic differences and the embedded
perspective,thecumbersomeinstallationandscalingprocesses
logics in different languages, an issue illuminated by the
hinder timely and accurate assessments of LLMs, particularly
Sapir-Whorf hypothesis, which posits that language structure
for larger models that necessitate more extensive evaluations
profoundly influences human cognition and worldviews, sug-
[20]. In security, the complexity of implementing and scal-
gestingthatlanguagesmayfosteruniquesystemsofreasoning
ing benchmarks could lead to gaps in coverage or delayed
[43].Benchmarksthatevaluatedlinguisticcomprehensionand
evaluations, potentially leaving new or complex models less
reasoningfrequentlyfollowedapredominantlyEnglish-centric
scrutinized than necessary [3]. Although improvements in
or Simplified-Chinese-centric approach,often treating all lan-
benchmark design and infrastructure could mitigate these
guagesashavingsimilarthinkingpatterns,asseenin[6]–[10],
challenges,theinherentcomplexitiesinbenchmarkinstallation
[22]–[26], [29]–[32], [34], [35], [37] (Appendix A-D). This
and scalability remain significant hurdles in the path towards
inadequacywashighlightedasmostLLMbenchmarks,primar-
robust and efficient LLM evaluation.
ily conducted in English, assumed or implied their reasoning
capabilities were universally applicable across all languages
and cultures without accounting for the unique cognitive F. Biases in LLM-Generated LLM Evaluations
frameworks inherent to each language’s structure. Moreover, Prevalence: 9/23 LLMs have been utilized by some LLM
in our own evaluation of two unnamed commercial LLMs, benchmarks to generate test questions and challenges or to
we received answers in English for our prompts constructed aid in evaluations of other LLMs, as observed in [4], [7],
entirelyinSimplifiedChinese.WhilesomecommercialLLMs [25], [28]–[30], [33], [34], [37] (Appendix A-F), akin to
offered multilingualsupport, it remained unclear whether this being both the player and the judge simultaneously. These
support was inherently native, or relied on translation to LLM-powered benchmarks were susceptible to inheriting the
English for processing, and current benchmarks could not biasesandinaccuraciesoftheLLMsusedtopowerthem,thus
verify the nature of this multilingual capability. potentially compromisingthe objectivity and reliability of the
From a functionality standpoint, this inadequacy leads to evaluations.
benchmarks that do not accurately, adequately, or sufficiently The implications of this inadequacy were twofold. From a
evaluate LLMs, especially in multilingual contexts. Such functionalityperspective,theinheritedbiasesinLLM-powered
benchmarks, without catering to the diverse cognitive and evaluationscouldleadtoskewedormisleadingassessmentsof
logical frameworks of different languages, risk being biased LLMcapabilities,eitheroverestimatingorunderestimatingthe
or skewed, thereby misrepresenting the true capabilities of trueperformanceofthemodelsbeingevaluated[1],[3].Inthe
LLMsinhandlinglinguisticdiversity[11].Intermsofsecurity, securitydomain,suchbiasesinevaluationscouldposesignifi-
unevenmoderationstrengthacrosslanguagescouldenablethe cantrisks,whenbiasedevaluationsmightnoteffectivelydetect
circumvention of content moderation mechanisms, allowing vulnerabilitiesorpotentialmisuse scenarios,therebyexposing
prohibited or malicious content to be generated in less mod- systems to security threats [21], [42]. While we respectfully
erated languages [44]. Addressing this inadequacy in LLM speculate that the use of LLMs to power LLM benchmarks
benchmarkscan be challenging,due to the profounddiversity might have been intended to automate and accelerate the
and often contradictory nature of diverse human languages, processof creating such benchmarks,relyingsolely on LLMs
thought processes, and the values they embody, making the risks perpetuating LLM biases, and we believe a balanced,
standardization of benchmarks to account for this complexity human-expert-ledapproachcanhelpensureevaluationsremain
a daunting task. rigorous.8
Inconsistent Benchmark [24]–[28], [30], [31],[33]–[37](AppendixB-B).Theinvolve-
18
Implementation ment of external parties in evaluations, while beneficial for
objectiveanalysisby leveragingspecialized domainexpertise,
Slow Test
18 can further contribute to delays due to the complex coordina-
Iteration time
tion, significant engineering support required, and slow com-
Challenge of Proper munication channels, as seen in benchmarks like BIG-bench
14
Prompt Engineering [4] and HELM [31]. Furthermore, the comprehensive nature
0 10 20 ofthesebenchmarks,requiringextensivetestingacrossvarious
Prevalence (max 23) scenarios, inherently extends the time frame for completion..
The implications of slow iteration time are multifaceted.
Fig. 3: Processual Inadequacies in LLM Benchmarking
Functionally, this delay undermines the benchmarks’ ability
to provide timely insights into the evolving capabilities of
LLMs. Given the rapid pace of advancementsin AI, an LLM
V. PROCESSUAL ELEMENTS
might undergo updates or improvements within the period
This section looks into the various process-related chal- of its evaluation, rendering the results less relevant or even
lenges and intricacies inherent in the implementation and obsolete,andcompromisingbenchmarkingreproducibility[2],
evaluation of LLM benchmarks (Fig. 3). [3]. From a security perspective, this delay could potentially
be risky if models are not promptly evaluated for emerging
threatsorvulnerabilities,andalsolimitstheabilitytopromptly
A. Inconsistent Benchmark Implementation
rectify or improvemodels based on evaluation feedback [40],
Prevalence: 18/23 [45]. Although efforts to streamline the evaluation process
The implementation of benchmarks across different LLM andenhancecollaborationwithexternalpartiescouldmitigate
developers has exposed a significant inadequacy of the in- some of those issues, the challenge of aligning rapid AI de-
consistency in execution, which we believe applies to [5]– velopmentwith thoroughandeffectivebenchmarkingremains
[10],[22]–[24],[26]–[33],[37](AppendixB-A).Thesebench- a significant concern in LLM evaluation practices.
marks, which are useful in evaluating language models, often
yield diverse results owing to the differing methodologies
employed by research teams. The variation in benchmarking C. Challenge of Proper Prompt Engineering
resultscouldbeattributedtofactorslikedifferentialinterpreta-
tionofthebenchmarks,distinctimplementationstrategies,and Prevalence: 14/23
varyinglevelsof expertiseacrossteams. Notably,benchmarks The challenge of proper promptengineeringinvolvescraft-
like MMLU were susceptible to such inconsistencies, when ing prompts that accurately and effectively elicit the capa-
implementation variations like few-shot learning or chain-of- bilities of language models. Such prompts are fundamental
thought reasoning across research teams could substantially in evaluations like multiple-choice tests (e.g., MMLU [22])
increase their scores [20]. and more complex assessments (e.g., FLUE [24], HaluEval
Theimplicationsofsuchinconsistenciesaretwofold.Func- [37]). The difficulty arises in formulating prompts that truly
tionally, they challenge the benchmarks’ capacity to offer reflect a model’s capabilities without introducing biases or
uniform and objective assessments of LLMs. The variance misinterpretations, which can skew the assessment results,
in implementation methods can skew benchmarking results, whichwe believeappliesto[4],[7],[8],[22]–[26],[28],[31],
rendering them unreliable for comparative analysis across [33], [35]–[37] (Appendix B-C).
differentLLMs or LLM developers[3], [20]. From a security The impact of this inadequacyspans both functionalityand
perspective,thisinconsistencycanunderminethebenchmarks’ cybersecurity domains. Functionally, inadequate promptengi-
integrity, when differing approaches might leave room for neering can lead to assessments that do not accurately reflect
loopholes or biased interpretations, potentially leading to a model’s true capabilities, resulting in either overestimation
manipulated outcomes [41], [42]. Given such concerns, the or underestimation of performance [46]. In terms of security,
solution may lie in establishing more standardized protocols poorly engineered prompts could be exploited to manipulate
and guidelines for benchmark implementation, fostering a benchmark outcomes, thereby compromising the integrity of
uniform approach across different research teams. the evaluation process [40], [44]. While advancements in
understanding model behaviors and refining assessment tech-
niques continue, the resolution of this challenge is complex,
B. Slow Test Iteration Time
consideringtheintricatenatureoflanguageandthecontinuous
Prevalence: 18/23 evolution of language models.
Slow iteration time present a notable inadequacy, which is
predominantly observed in third-party evaluation frameworks
like BIG-bench and HELM, where the process of evaluating VI. HUMAN DYNAMICS
new models can take weeks or months, and is often hindered
by prolonged processing periods or delays in communication This section explores the subtle complexities of human
with external parties, as evident in [4], [5], [8]–[10], [22], factors influencing LLM benchmarks (Fig. 4).9
Diversity in Human single standardized approach could not possibly capture the
19
Curators and Evaluators full spectrum of human diversity.
This challenge is a significant inadequacy for several rea-
Diverse Cultural, Social,
sons.Infunctionality,thesebenchmarksmayfailtoaccurately
Political, Religious and 18
and sufficiently evaluate LLMs in a way that is respectful
Ideological Norms 0 10 20 and representative of diverse perspectives, and could lead
Prevalence (max 23) to biases or skewed results that do not reflect the varied
andsometimesconflictingvaluespresentin differentsocieties
Fig. 4: Human Dynamics Inadequacies in LLM [11]. In terms of security, while this inadequacy might not
Benchmarking directlyleadtosystemmanipulationorgaming,itunderscores
concernsregardingthe benchmarks’capacity to ensure LLMs
are evaluated for resilience against ideological manipulation,
A. Diversity in Human Curators and Evaluators
specifically in terms of generating misinformation or disin-
Prevalence: 19/23
formation across diverse cultural and ideological landscapes
TheconstructionandanalysisofLLMbenchmarkscouldbe
[2]. Resolving this inadequacy poses a complex challenge,
significantly shaped by the heterogeneity of human curators
as it involves reconciling fundamentally divergentbeliefs and
and evaluators. Even with the presence of clear evaluation
values, a task that often goes beyond the scope of technical
guidelines or rubrics, the variability in cultural, religious,
solutions, and enters the domain of ethical decision-making
political,andacademicorcommercialbackgroundsofhumans
and cultural sensitivity.
couldlead to inconsistenciesin the benchmarks’development
and their evaluation, especially in situations where responses
VII. DISCUSSIONS
were not unequivocal or quantifiable, due to the diverse
Thissectionprovidesacomprehensivediscussionoftheun-
linguistic expressions used to convey comparable meanings.
derlyingcausesthathave resultedinthe previouslyaforemen-
This challenge was evident across various benchmark types,
tioned inadequacies in current LLM benchmarking methods.
including those depending on human evaluators, red teaming
for national security, and other sophisticated evaluative meth-
ods,asin[4]–[10],[22],[23],[25]–[27],[29],[30],[32]–[35], A. Challenges in Inclusive Benchmarking
[37] (Appendix C-A).
Standardizingbenchmarksfor LLMs faces significant chal-
This diversity presents a notable inadequacy for two main
lenges due to the diversity of human values and perspectives,
reasons. Functionally, it can lead to benchmarks that do not
which extend beyond linguistic, cultural, religious, and polit-
uniformly or accurately assess LLMs, as the subjectivity
ical realms to encompass a broad spectrum of societal norms
introduced by human curators and evaluators may result in
and ethical considerations, and are sometimes conflicting and
benchmarksthat are skewed or biased [1], [3], [20]. Security-
irreconcilable. The predominance of English and Simplified
wise, the inconsistency in the application and interpretation
Chinese(overlookingTraditionalChinesedifferences)inthose
of benchmarks by diverse human evaluators raises concerns
benchmarksfrequentlyreflected the defaultvaluesand beliefs
about the potential manipulation or circumvention of results
of their creators, overlooking the pluralistic nature of global
[45]. The resolution of this issue will be complex, due to the
beliefsandpractices.Forexample,the EnglishandSimplified
inherent variability in human values and judgments, particu-
Chinese centrism in benchmarks like BIG-Bench [4], and the
larlyin benchmarksthatheavilyrelyonhumaninterpretation.
focusoncrystallizedknowledgeinMultiMedQA[8]andARB
[28], highlighted such challenges, potentially disadvantaging
B. Diverse Cultural, Social, Political, Religious and Ideolog- LLMs not trained in those more prevalent languages and
ical Norms limiting assessments of their broader reasoning capabilities.
Prevalence: 18/23 Such a narrower focus would compromise the benchmarks’
LLM benchmarks often face the challenge of integrating a ability to comprehensively and holistically evaluate LLMs,
broad spectrum of religious, ideological, legal, political, and potentially favoring models that aligned with specific cultural
cultural viewpoints, and are beyond the control of curators or ideological norms at the expense of broader applicability
and evaluators, as seen in [4], [5], [7]–[10], [22]–[24], [26]– and acceptance, and raising concerns about the trustworthi-
[29], [31], [33]–[35], [37] (Appendix C-B). The frequent ness of LLMs in the absence of cultural and ideological
reliance on standardizedanswers or rubrics for benchmarking inclusivity. The reliance on standardized answers or rubrics
might conflict with values like diversity, inclusivity, cultural exacerbated this issue, especially in assessments designed to
sensitivity,religiousfreedom,freedomofspeech,andpolitical gauge fairness, societal appropriateness, or ethical decision-
correctness, which were deeply valued in many countries. making,wherethe richtapestryof humandiversitydemanded
This inadequacy became pronouncedwhen some benchmarks amoresophisticatedapproach.Addressingtheaforementioned
were designed to measure traits such as fairness or societal challenges would require moving beyond technical fixes to
appropriateness,where the pluralistic nature of human beliefs incorporate ethical decision-making and cultural sensitivity,
and values came into play. The benchmarks that particularly highlighting the need for benchmarks that facilitate the de-
fellintothiscomplexitywerethoseinvolvingethicaldecision- velopment of globally aware LLMs, acknowledging the vast
making, societal norms, and cultural interpretations, where a array of human values. As the development of future LLM10
benchmarks progresses, it is anticipated that many will en- publishing, reflecting the developing stage of AI regulatory
counter similar challenges, particularly when the diversity compliance and an evolving public consensus on AI’s roles
of benchmark creators, evaluators, or the content itself does and impacts, which has led to a wide array of diverse tasks,
not fully embrace the broad spectrum of human diversity. questions, and answers being labeled as benchmarks. This
However, different jurisdictions, religions, or cultures may situation,markedbyaself-endorsedapproachcomparedtothe
have varying interpretations of inclusivity, ethics, or societal rigorousgovernanceinsectorssuchasaviationorautomotive,
fairness that could lead to disagreements on what constitutes requires a critical distinction between liberal evaluations of
a superior LLM, which extendsbeyondtechnical benchmarks LLMsandgenuinestandardizedbenchmarks,whentheformer
into broader considerations of values and politics. often emerged as assessments rooted in the researchers’ own
understanding of LLMs, and their personal perspectives and
predispositions. Despite these challenges, the contributionsof
B. Assessment Limitations for Reasoning and Multimodality
authors in advancing the field of LLM and AI evaluations
Current benchmarks have demonstrated a marked insuffi-
shouldbeacknowledgedandappreciated,astheireffortscould
ciency in evaluating the comprehensive reasoning and multi-
further contribute to shaping the public’s expectations and
modalcapabilitiesof LLMs. Thisshortfallis twofold,encom-
understandingof AI, and advancing AI regulations. However,
passing both the subtle distinction between crystal (recalled
the proliferation of studies claiming to establish benchmarks,
knowledge) and fluid (creative problem-solving) intelligence,
eachwithuniquemethodologiesandsetsoftasksorquestions,
andthechallengesinherentinassessingmultimodalreasoning
adds to an increasingly complex landscape that demands
that spans beyond text to include visual and auditory data.
cautious and critical scrutiny to avoid misinterpreting these
Benchmarks predominantly focus on text-based tasks, over-
evaluations as definitive standards. It is anticipated that au-
looking the integration of visual and auditory inputs, thereby
thors will continue to create different combinations of tasks
limiting their applicability to truly multimodal LLMs. This
and questions, labeling them as benchmarks in their LLM
oversight not only restricts the evaluation to task-specific
evaluations, further contributing to the existing complexity
performance,neglectingthedepthofunderstandingandadapt-
until regulatory bodies and public consensus provide clearer
ability, but also fails to safeguard against “cheating” through
guidance. Such complexity underlines the urgent need for a
modelsovertrainedonbenchmarkdatasets.Consequently,such
morestructuredanduniversallyacceptedbenchmarkingframe-
benchmarks inadequately reflect LLMs’ actual capabilities to
work in advanced AI, aligning with both emergingregulatory
engage in creative reasoning or to interpret and respond to
mandates and societal needs to navigate the risks associated
complex multimodal information, underscoring the need for
with AI’s misuse as it becomesmore entwined with everyday
more technically robust frameworks that encompass the full
life.
spectrum of intelligence and multimodal interaction expected
of next-generation AI systems.
E. Knowledge Boundaries in AI Benchmarking
C. Unpredictability and Non-Repeatability Benchmarksfor LLMs and advancedAI are fundamentally
limitedbythecurrentexpanseofhumanknowledge,impeding
The unpredictability and non-repeatability of results with
their capacity to fully evaluate and nurture the development
commercialLLMs and GPT modelsunderscorethe inadequa-
of groundbreaking AI capabilities. This inherent constraint
cies of current benchmarks in offering consistent and reliable
is particularly pronounced in the evaluation of emergent AI
evaluations. Unlike regulated industries such as automobile
functionalitiesthattranscendconventionalhumancomprehen-
and aviation, where stringent regulations, clear frameworks,
sion,potentiallystalling progressin disciplinespoisedfor AI-
and a reasonably well-defined public consensus guide safety
driven breakthroughs. Moreover, the scarcity of specialized
and functionality evaluations to meet regulatory needs and
domain knowledge among evaluators further compromises
societal expectations, the generative AI sector lacks these
the efficacy and security of those benchmarks. In critical
comprehensivestandards.Thisabsenceisfurthercompounded
sectors that demand deep, domain-specific expertise—such as
bytherapidpaceofAIdevelopment,whichnotonlyoutstrips
national security—the prevalent generalist approach among
the establishment of universally accepted guidelines and reg-
evaluatorsleadstobenchmarksthatareinadequatelyequipped
ulations, but also challenges the time it takes to develop and
to address the sophisticated requirements of these fields. This
evolve public consensus expectations. Consequently, the field
gap not only introducesbiases into the evaluationprocess but
has seen a proliferation of researcher-proposed benchmarks,
also risks the integrity of the benchmarks, opening avenues
many of which suffer from the observed inadequacies. This
for manipulation. The dual challenge of expanding beyond
scenario highlights the critical need for developing effective,
current human knowledge while ensuring evaluators possess
universally accepted guidelines that can adapt to the evolving
the necessary domain-specific expertise calls for a concerted
landscape of AI technology, ensuring responsible advance-
efforttoenhancethesophisticationofbenchmarks.Itiscrucial
ments and deployments.
to both broaden the knowledge base informing benchmark
development and to cultivate a cadre of evaluators with deep
D. Possible Misuse of the Term “Benchmark”
domainexpertise.Suchmeasuresareessentialto forgebench-
Theterm“benchmark”mayhavefoundbroaderapplication marks that can accurately reflect the advanced capabilities
thanwarrantedinthelargelyself-regulatedsphereofacademic of LLMs and AGIs, ensuring they are poised to meet the11
complex demands of specialized domains and are resilient themselvescould becomeoutdatedas technologyand modern
against evolving AI threats. The urgency of addressing these civilization evolves, and the study in [20] found mislabeled
knowledge boundaries is underscored by the rapid advance- or unanswerable examples within MMLU [22]), we opted to
mentofAI technologies,necessitating benchmarksthatare as give these studies the benefit of the doubt,acknowledgingthe
dynamic and innovative as the systems they aim to evaluate. potentialfor possible oversight.Secondly,while only 5 outof
the 23 surveyed benchmark studies were peer-reviewed, our
F. Future Research Direction: Extending Benchmarks with criticalinclusionofpreprintsshouldnotbeseenasdiminishing
Behavioral Profiling and Regular Audits their scientific contribution,although it highlighted the evolv-
ing and dynamic nature of research in this field. Thirdly, our
To effectively evaluate and utilize LLMs amid the swift
decision not to attempt reproducingresults from the surveyed
advancements in generative AI, their evaluation methods can
benchmark studies, was informed by the substantial time and
be extended beyond traditional benchmarks to include both
effort required, coupled with the challenge of LLM updates
initialscreeningsandin-depth,ongoingassessmentsthatalign
potentially altering outcomes during the lengthy evaluation
with changingtechnologicalandsocietalneeds.Similarto the
process. Furthermore, while we addressed linguistic differ-
initial candidate selection and aptitude tests in the human re-
encesbetweenSimplifiedChineseandTraditionalChinese,we
cruitmentprocess,traditionalbenchmarkscanserveasthefirst
did not explore the subtle distinctions among various English
filter to screen out LLMs that fail to meet basic competence
dialects (e.g., American, British) and their inherent logical
levels,akintoidentifyingcandidateswhomightbefraudulent,
differencesrelatedtoculturalattitudes,therebyacknowledging
incompetent,non-compliantorotherwiseunsuitable.Thisstep
another layer of complexity in language-based evaluations.
ensures that only LLMs with a foundational level of profi-
Lastly, the absence of specific regulations and guidelines for
ciency and regulatory compliance proceed to more rigorous
generative AI and LLMs, and a lack of public consensus
evaluations, optimizing resource allocation for subsequent
on acceptable AI behaviors, constrained our ability to defini-
stagesoftheassessmentprocess.Mirroringtheinterviewstage
tively enumerate all benchmark inadequacies. The evolving
of candidate selection, behavioral profiling explores deeper
capabilities of advanced AI systems would require ongoing
into the subtleties of LLM performance by assessing models
amendments to our critique, reflecting the dynamic nature of
beyond mere task completion, examining adaptability, ethical
the field and the continuous emergence of new AI features.
reasoning, and creativity in scenarios reflecting real-world
complexities. Such AI behavioral analysis and profiling, a
topic for future research, will prioritize LLMs most likely to VIII. CONCLUSION
fulfill specific roles or tasks, providing a sophisticated under-
This study has performed a rigorous examination of the
standing of model capabilities and informing better selection
prevailing methodologies in LLM benchmarks, uncovering
decisions for deployment in sensitive or critical applications.
significant inadequacies, spanning technological, processual,
Echoing the probation period in employment, regular audits
and human dynamics, which could undermine the accuracy,
post-deployment serve to continuously assess LLM perfor-
comprehensiveness,and security of these benchmarks.Unlike
mance against evolving standards and expectations. This step
the stringent, consensus-driven benchmarkingpractices in the
iscrucialforcatchinganydiscrepanciesbetweeninitialevalu-
automobile and aviation industries, the field of advanced AI
ationsandactualperformance,ensuringLLMsremainaligned
lacked such standardized frameworks, leading to a multitude
with the requirements and ethical standards of evaluators
ofresearcher-proposedLLMbenchmarksthatfailedtoaddress
and regulators over time. Implementing this comprehensive
the complexities inherent in modern LLMs, and highlighting
evaluation frameworkwill require collaborationacross acade-
the pressing necessity for enhanced and universally recog-
mia, industry, and regulatory bodies to develop standardized
nized evaluation protocols in the face of generative AI’s
methodologiesand ethical guidelines for each stage, ensuring
swift evolution. Our contributions to this discussion include:
assessments are rigorous and reflective of societal values and
(i) developing a unified evaluation framework grounded in
technological advancements. This will facilitate the creation
cybersecurityprinciples,designedtoidentifyandaddressinad-
of a dynamicevaluationecosystem capable of adapting to the
equacies across technological, processual, and human factors,
rapid pace of AI innovationand ensuring deployedLLMs are
enhancing both functionality and security assessments; (ii)
both effective and secure.
performing a thorough critique on 23 state-of-the-art LLM
benchmarks, using counterexamples to determine and show-
G. Limitations of This Study casetheextentandcharacterofexistinginadequacies;and(iii)
This study, while aiming to provide a thorough critique advocating for the extension of LLM benchmarks to include
of LLM benchmarks, encounters its own set of limitations. LLM behavioral profiling and audits, supplemented by the
Firstly, our analysis inherently carries our subjective bias, creation of standardized evaluation guidelines.
a challenge we have attempted to mitigate by transparently As we edge closer to the realization of AGI, it becomes
listing our evaluative rationale in the Appendices. Due to crucialtoaddressthecurrentbenchmarkinginadequacies.The
the varying level of detail in benchmark descriptions, when adoptionof behavioralprofiling and distinct functionalityand
we could not conclusively determine whether a study had securityauditstoextendsuchbenchmarkscouldensureLLMs
acknowledgedor addressed a specific benchmark inadequacy, are both effective in their applications and fortified against
or evaluate the correctness of questions and answers (which securitythreats.Apracticalnextstepcanbetheestablishment12
of new benchmarks that embody the behavioral profiling and [17] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang,
security audit frameworks discussed herein. Additionally, the “Benchmarkingsingle-imagedehazingandbeyond,”IEEETransactions
onImageProcessing,vol.28,no.1,pp.492–505,2018.
formulation of standardized guidelines for LLM evaluation
[18] S. Zhu, J. Shi, L. Yang, B. Qin, Z. Zhang, L. Song, and G. Wang,
will be crucial. To facilitate these advancements as future “Measuringandmodelingthelabeldynamicsofonline{Anti-Malware}
researchdirections,weproposetheinitiationofacollaborative engines,” in 29th USENIX Security Symposium (USENIX Security 20),
2020,pp.2361–2378.
internationalinitiative,tofocusonthecontinuousdevelopment
[19] T. McIntosh, A. Kayes, Y.-P. P. Chen, A. Ng, and P. Watters, “Ran-
and refinement of LLM benchmarks, ensuring they remain somware mitigation in the modern era: A comprehensive review, re-
relevant and effective amidst rapid technological progress. search challenges, and future directions,” ACM Computing Surveys
(CSUR),vol.54,no.9,pp.1–36,2021.
Such an initiative would marshal the collective expertise
[20] D. Ganguli, N. Schiefer, M. Favaro, and J. Clark,
of academia, industry, and regulatory bodies, fostering the “Challenges in evaluating AI systems,” 2023. [Online]. Available:
creation of AI systems that are innovative, trustworthy, and https://www.anthropic.com/index/evaluating-ai-systems
[21] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
beneficially integrated into society.
A.Madotto, and P.Fung, “Survey ofhallucination innatural language
generation,” ACMComputing Surveys,vol.55,no.12,pp.1–38,2023.
REFERENCES [22] D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika, D.Song,and
J. Steinhardt, “Measuring massive multitask language understanding,”
[1] B. Min, H.Ross, E.Sulem, A.P.B. Veyseh, T.H.Nguyen, O.Sainz, arXivpreprintarXiv:2009.03300, 2020.
E.Agirre,I.Heintz,andD.Roth,“Recentadvancesinnaturallanguage [23] N. Guha, D. E. Ho, J. Nyarko, and C. Re´, “Legalbench: Prototyp-
processing via large pre-trained language models: A survey,” ACM ing a collaborative benchmark for legal reasoning,” arXiv preprint
Computing Surveys,vol.56,no.2,pp.1–40,2023. arXiv:2209.06120, 2022.
[2] T.R.McIntosh, T.Susnjak, T.Liu, P.Watters, and M.N.Halgamuge, [24] R.Shah,K.Chawla,D.Eidnani,A.Shah,W.Du,S.Chava,N.Raman,
“From google gemini to openai q*(q-star): A survey of reshaping the C.Smiley,J.Chen,andD.Yang,“Whenfluemeetsflang:Benchmarks
generative artificial intelligence (ai)researchlandscape,”arXivpreprint andlargepretrainedlanguagemodelforfinancialdomain,”inProceed-
arXiv:2312.10868, 2023. ingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguage
[3] Y.Chang,X.Wang,J.Wang,Y.Wu,L.Yang,K.Zhu,H.Chen,X.Yi, Processing,2022,pp.2322–2335.
C. Wang, Y. Wang et al., “A survey on evaluation of large language [25] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, and J. Zhang, “On the tool
models,” ACM Transactions on Intelligent Systems and Technology, manipulation capability of open-source large language models,” arXiv
2023. preprintarXiv:2305.16504, 2023.
[4] A.Srivastava, A.Rastogi,A.Rao,A.A.M.Shoeb,A.Abid,A.Fisch, [26] Y.Fu,L.Ou,M.Chen,Y.Wan,H.Peng,andT.Khot,“Chain-of-thought
A.R.Brown,A.Santoro,A.Gupta,A.Garriga-Alonsoetal.,“Beyond hub: Acontinuous effort tomeasure large language models’ reasoning
the imitation game: Quantifying and extrapolating the capabilities of performance,” arXivpreprintarXiv:2305.17306, 2023.
language models,”arXivpreprintarXiv:2206.04615, 2022. [27] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R.
[5] K. Zhu, Q. Zhao, H. Chen, J. Wang, and X. Xie, “Promptbench: A Loomba,S.Zhang,Y.Sun,andW.Wang,“Scibench:Evaluatingcollege-
unifiedlibraryforevaluation oflargelanguage models,”arXivpreprint level scientific problem-solving abilities of large language models,”
arXiv:2312.07910, 2023. arXivpreprintarXiv:2307.10635, 2023.
[6] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, [28] T.Sawada,D.Paleka,A.Havrilla,P.Tadepalli,P.Vidas,A.Kranias,J.J.
H.Edwards,Y.Burda,N.Joseph,G.Brockmanetal.,“Evaluatinglarge Nay,K.Gupta,andA.Komatsuzaki,“Arb:Advancedreasoningbench-
language models trained on code,” arXiv preprint arXiv:2107.03374, mark for large language models,” arXiv preprint arXiv:2307.13692,
2021. 2023.
[7] C. Liu, R. Jin, Y. Ren, L. Yu, T. Dong, X. Peng, S. Zhang, J. Peng, [29] Z. Gu, X. Zhu, H. Ye, L. Zhang, J. Wang, S. Jiang, Z. Xiong, Z. Li,
P. Zhang, Q. Lyu et al., “M3ke: A massive multi-level multi-subject Q.He,R.Xuetal.,“Xiezhi: Anever-updating benchmark forholistic
knowledge evaluation benchmark for chinese large language models,” domainknowledgeevaluation,”arXivpreprintarXiv:2306.05783,2023.
arXivpreprintarXiv:2305.10263, 2023. [30] Q.Tang,Z.Deng,H.Lin,X.Han,Q.Liang,andL.Sun,“Toolalpaca:
[8] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, Generalized tool learning for language models with 3000 simulated
N.Scales,A.Tanwani,H.Cole-Lewis,S.Pfohletal.,“Largelanguage cases,”arXivpreprintarXiv:2306.05301, 2023.
modelsencodeclinicalknowledge,”Nature,vol.620,no.7972,pp.172– [31] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,
180,2023. Y.Zhang, D.Narayanan, Y. Wu,A.Kumaret al.,“Holistic evaluation
[9] J. Yu, X. Wang, S. Tu,S. Cao, D. Zhang-Li, X. Lv,H. Peng, Z. Yao, oflanguage models,”arXivpreprintarXiv:2211.09110, 2022.
X.Zhang,H.Lietal.,“Kola:Carefullybenchmarkingworldknowledge [32] Y.Qin,S.Liang,Y.Ye,K.Zhu,L.Yan,Y.Lu,Y.Lin,X.Cong,X.Tang,
oflargelanguage models,”arXivpreprintarXiv:2306.09296, 2023. B. Qian et al., “Toolllm: Facilitating large language models to master
[10] W.Zhong,R.Cui,Y.Guo,Y.Liang,S.Lu,Y.Wang,A.Saied,W.Chen, 16000+real-world apis,”arXivpreprintarXiv:2307.16789, 2023.
and N. Duan, “Agieval: A human-centric benchmark for evaluating [33] X.Liu,H.Yu,H.Zhang,Y.Xu,X.Lei,H.Lai,Y.Gu,H.Ding,K.Men,
foundation models,”arXivpreprintarXiv:2304.06364, 2023. K.Yangetal.,“Agentbench:Evaluatingllmsasagents,”arXivpreprint
[11] T.R.McIntosh,T.Liu,T.Susnjak,P.Watters,A.Ng,andM.N.Halga- arXiv:2308.03688, 2023.
muge,“Aculturallysensitivetesttoevaluatenuancedgpthallucination,” [34] M.Li,F.Song,B.Yu,H.Yu,Z.Li,F.Huang,andY.Li,“Api-bank:A
IEEETransactions onArtificial Intelligence, 2023. benchmarkfortool-augmentedllms,”arXivpreprintarXiv:2304.08244,
[12] A.V.Papadopoulos,L.Versluis,A.Bauer,N.Herbst,J.VonKistowski, 2023.
A. Ali-Eldin, C. L. Abad, J. N. Amaral, P. Tuma, and A. Iosup, [35] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu,
“Methodological principles for reproducible performance evaluation in C. Lv, Y. Zhang, J. Lei et al., “C-eval: A multi-level multi-
cloudcomputing,”IEEETransactionsonSoftwareEngineering,vol.47, disciplinechineseevaluationsuiteforfoundationmodels,”arXivpreprint
no.8,pp.1528–1543, 2019. arXiv:2305.08322, 2023.
[13] M.S.Aslanpour, S.S.Gill, andA.N.Toosi,“Performance evaluation [36] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng,
metricsforcloud,fogandedgecomputing:Areview,taxonomy,bench- Z. Chen, J. C. Niebles, D. Arpit et al., “Bolaa: Benchmarking
marksandstandardsforfutureresearch,”InternetofThings,vol.12,p. and orchestrating llm-augmented autonomous agents,” arXiv preprint
100273,2020. arXiv:2308.05960, 2023.
[14] J.Yan,J.Yuan,P.H.Leong,W.Luk,andL.Wang,“Losslesscompres- [37] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:
sion decoders forbitstreams and software binaries based on high-level A large-scale hallucination evaluation benchmark for large language
synthesis,” IEEE Transactions on Very Large Scale Integration (VLSI) models,”inProceedingsofthe2023ConferenceonEmpiricalMethods
Systems,vol.25,no.10,pp.2842–2855, 2017. inNaturalLanguageProcessing,2023,pp.6449–6464.
[15] S. Dargan and M. Kumar, “A comprehensive survey on the biometric [38] W.X.Zhao,K.Zhou,J.Li,T.Tang,X.Wang,Y.Hou,Y.Min,B.Zhang,
recognition systemsbasedonphysiological andbehavioral modalities,” J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv
ExpertSystemswithApplications, vol.143,p.113114,2020. preprintarXiv:2303.18223, 2023.
[16] E. Davis, “Benchmarks for automated commonsense reasoning: A [39] T. McIntosh, T. Liu, T. Susnjak, H. Alavizadeh, A. Ng, R. Nowrozy,
survey,”ACMComputing Surveys,vol.56,no.4,p.41,2023. and P. Watters, “Harnessing gpt-4 for generation of cybersecurity grc13
policies: A focus on ransomware attack mitigation,” Computers & Malka N. Halgamuge obtained her Ph.D. from the University of Mel-
security, vol.134,p.103424,2023. bourne, Australia, and worked there as a researcher during 2007-2021. She
[40] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How does llm was awarded prestigious fellowships at the University of California, Los
safetytrainingfail?”inThirty-seventhConferenceonNeuralInformation Angeles(USA),LundUniversity,Lund(Sweden),andtheChineseAcademy
ProcessingSystems,2023. of Sciences (CAS) in Beijing (PRC), among others. She is currently a
[41] S. Balloccu, P. Schmidtova´, M. Lango, and O. Dusˇek, “Leak, cheat, Senior Lecturer in cybersecurity at RMIT University, Melbourne, Australia.
repeat:Datacontaminationandevaluationmalpracticesinclosed-source Her research includes emerging technology, cybersecurity, security in IoT,
llms,”arXivpreprintarXiv:2402.03927, 2024. edge, power grid, supply chain, blockchain, and developing solutions using
[42] K. Zhou, Y. Zhu, Z. Chen, W. Chen, W. X. Zhao, X. Chen, Y. Lin, deep/machine learning.
J.-R.Wen,andJ.Han,“Don’tmakeyourllmanevaluation benchmark
cheater,” arXivpreprintarXiv:2311.01964, 2023.
[43] L.Perlovsky, “Language andemotions: emotional sapir–whorfhypoth-
esis,”NeuralNetworks, vol.22,no.5-6,pp.518–526,2009.
[44] Y. Deng, W. Zhang, S. J. Pan, and L. Bing, “Multilingual jailbreak
challengesinlargelanguagemodels,”arXivpreprintarXiv:2310.06474,
2023.
[45] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and
M.Fritz,“Notwhatyou’vesignedupfor:Compromisingreal-worldllm-
integrated applications with indirect prompt injection,” in Proceedings
ofthe16thACMWorkshoponArtificialIntelligenceandSecurity,2023,
pp.79–90.
[46] J. Zamfirescu-Pereira, R. Y. Wong, B. Hartmann, and Q. Yang, “Why
johnny can’t prompt: how non-ai experts try (and fail) to design llm
prompts,”inProceedingsofthe2023CHIConferenceonHumanFactors
inComputing Systems,2023,pp.1–21.
Timothy R. McIntosh received his Ph.D. in cybersecurity from La Trobe
University, Melbourne (Australia) in 2022. He is currently the course coor-
dinatorofcybersecurity atAcademiesAustralasiaPolytechnicinMelbourne,
andanadjunctlecturerofcybersecurityatLaTrobeUniversity,afterworking
as a Security Operations Centre (SOC)analyst inthe industry. His research
focusesonLLMsincybersecurity,andransomwaremitigation.HeisanISC2
CISSP and CSSLP, an IAPP CIPP/E and CIPT, a CompTIA CASP+ and
CySA+,aMicrosoftCertifiedCybersecurityArchitectExpert,andaMicrosoft
Certified SolutionDeveloper (MCSD).
Teo Susnjak is a Senior Lecturer in Computer Science and Information
Technology at Massey University, Auckland (New Zealand), where he re-
ceivedhisPh.D.inmachinelearning.HeistheSubjectLeadofDataScience
andtheCoordinatorfortheMasterofAnalyticsdegreeatMasseyUniversity.
He brings his extensive industry experience as a Machine Learning Analyst
tosolvereal-worldproblems.HisrecentresearchfocusedonLargeLanguage
Models (LLMs)for tasks such as generating data visualizations, fine-tuning
data visualizations, sentiment analysis of scientific literature, and exploring
theimplications ofgenerative AItechnologies inEducation.
Tong Liu is a lecturer in computer science and Information Technology
at Massey University, Auckland (New Zealand). She received her Ph.D. in
Computer Science with a specialization in machine learning. Her research
journeylookedintotherealmsofmachinelearningandArtificialIntelligence,
during which she pioneered several innovative machine learning algorithms.
She is the guest editor for a special issue of the pattern recognition letter
journal. Herprofessional affiliations extend beyondheracademic roles.
Paul Watters serves as the Academic Dean at Academies Australasia
Polytechnic, Melbourne (Australia), and as the CEO of Cyberstronomy Pty
Ltd. He has held notable academic positions, being an Honorary Professor
at Macquarie University, Sydney(Australia), and an Adjunct Professor at La
TrobeUniversity. Heis aChartered ITProfessional, aFellow oftheBritish
Computer Society, a Senior Member of the IEEE, a Member of the ACM,
andaMemberoftheAustralian Psychological Society.14
APPENDIX A However, the benchmark’s design inherently might not
EXAMPLESOF BENCHMARK INADEQUACIES IN fully capture the specialized, context-driven response
TECHNOLOGICAL ASPECTS behavior of models tailored for specific scenarios, re-
flecting the inadequacy related to response variability in
A. Response Variability in Standardized Evaluations
standardized evaluations.
Prevalence: 22/23 • Chain-of-Thought Hub [26]: The study introduced the
• MMLU [22]: The study designed a benchmark that Chain-of-Thought Hub for evaluating LLMs’ reasoning
focused on a wide range of tasks across different fields, capabilities,butacknowledgedthechallengeofaccurately
likeProfessionalLawandCollegeMedicine.However,it representing models’ performance due to variability in
reported performance variations among different LLMs, response behavior across different reasoning tasks and
particularly GPT-3, which struggled with tasks requiring datasets.
detailed procedural knowledge or calculations, like El- • KoLA [9]: The study demonstrated this inadequacy by
ementary Mathematics. This indicates the benchmark’s evaluating LLMs across a range of tasks designed to
potential inadequacy in capturing the specialized perfor- measure different types of knowledge (memorization,
mance of LLMs in specific scenarios or formats. understanding, applying, creating), suggesting that the
• HumanEval [6]: The study demonstrated inadequacy in benchmark may not accurately capture model perfor-
response variability as it evaluated Codex’s performance mance in specialized or intended contexts, particularly
mainly through functional correctness, using a unique highlighted in its design of evolving and known data
setofhand-writtenprogrammingproblems(HumanEval), sources to assess models’ handling of unseen data.
which might not fully capture the model’s behavior in • SciBench [27]: The study’s benchmarks demonstrated
diverse real-world scenarios. the inadequacy of capturing response variability, when
• LegalBench [23]: The study demonstrated inadequacies applied to models designed for specific contexts, as
in standardized evaluations by applying generic bench- seen in their detailed evaluation of LLMs using varied
marksto specializedlegalreasoningtasks, whichdid not prompting strategies and external tools, which failed to
accurately reflect the models’ capabilities in the specific uniformly enhance model performance across different
context of legal reasoning. This is exemplified in their scientific problem-solvingtasks.
developmentof the IRAC framework-basedtasks, which • ARB [28]: The ARB benchmark was designed to eval-
sought to evaluate the application of legal reasoning but uate advanced reasoning capabilities in LLMs across
mightnotalignwiththespecificitiesofthemodelstested. various domains, including mathematics, physics, and
• FLUE [24]: The study developed a novel Financial Lan- law, with a focus on expert-level problem solving that
guage Model (FLANG) and Financial Language Under- requires specific formats or use cases. However, the
standing Evaluation (FLUE) benchmarks, but the bench- study acknowledged the challenge in accurately repre-
marksmightnoteffectivelyrepresentmodelperformance senting model performance in specialized contexts, par-
in specific financial contexts, as they focused on generic ticularly in symbolic reasoning and proof-like problems,
linguistic capabilities rather than specialized financial which required sophisticated understanding and evalu-
scenarios. ation methodologies beyond standard metrics. This re-
• MultiMedQA[8]:Thestudyappliedstandardizedbench- flected the inadequacy in capturing the true performance
marks (MultiMedQA) to Flan-PaLM, a model not ex- and subtleties of models tailored for particular scenarios
clusively trained on medical data. This approach failed or formats.
to fully capture Flan-PaLM’s specialized performance in • BIG-Bench [4]: The study demonstrated the benchmark
themedicalcontext,asevidencedbyitslimitedalignment inadequacy of response variability when standardized
with clinical consensus compared to clinician-generated evaluations were applied to models designed for specific
answers. formats,asevidencedbyitsextensiveevaluationacrossa
• M3KE[7]:ThestudyevaluatedChineseLargeLanguage widerangeoftasksandmodelscaleswithoutspecifically
Models using the M3KE benchmark, which consists of tailoringbenchmarksto individualmodelcontextsor use
multiple-choice questions across diverse subjects and cases.
educationlevels. However,this approachdid notaccount • AGIEval [10]: The study designed AGIEval, a novel
for the specialized contexts and formats that specific benchmark focused on human-centric tasks, but encoun-
models may be tailored for, thus not fully capturing the tered challenges in accurately evaluating models across
subtle performanceof these modelsin their intended use varied tasks, such as complex reasoning and domain-
cases. specific knowledge requirements, indicating a potential
• T-Bench[25]:Thestudy evaluatedLLMsontoolmanip- underestimationofmodelcapabilitiesinspecificcontexts.
ulation tasks with diverse software tools, incorporating • ToolAlpaca [30]: The study introduced ToolAlpaca, a
bothexisting datasetsand newly collectedones. Itaimed frameworkaimingtoenhancecompactlanguagemodels’
to address the variability in model responses by enhanc- ability to generalize tool use by training on a diverse
ingopen-sourceLLMswith techniqueslike modelalign- and simulated tool-use corpus. However, the evaluation
ment with programmatic data curation, demonstration primarily focused on the models’ capability to handle
retrieval,and generationregulationwith system prompts. unseen tools based on the generated dataset, without15
explicitlyaddressingormitigatingthevariabilityinmodel val, which focused on evaluating LLMs’ ability to rec-
responses to standardized evaluations for specific sce- ognize hallucinated content without tailoring to the spe-
narios or formats, which could impact the accuracy of cific contexts or formats of the evaluated models. This
assessing a model’s performance in its intended context. approach inherently overlooked the specialized response
• HELM [31]: The study acknowledged the challenge of behaviorsofmodelsinparticularscenarios,leadingtopo-
standardizing language model evaluation across diverse tential inaccuracies in reflecting true model performance
scenarios, indicating an understanding of response vari- across diverse applications.
abilitybutnotfullyaddressingitthroughitsmethodology.
• ToolBench [32]: The study designed and utilized the
B. Genuine Reasoning vs Technical Optimization
ToolBench framework, which, although comprehensive,
might not fully account for the variability in responses Prevalence: 22/23
when applied to specialized, context-driven models, as • MMLU [22]: The study highlighted the challenge of
evidenced by its broad approach encompassing 16,000+ discerninggenuinereasoningfromtechnicaloptimization
real-world APIs without specific adjustments for models in LLMs, as evidencedby the varied performanceacross
designed for niche applications. tasks and the reliance on pretraining, without a clear
• PromptBench [5]: The study demonstrated that LLMs mechanismtoensuregenuineunderstandingorreasoning
were not robust to adversarial prompts, highlighting a capabilities.
significant gap in evaluating models’ performance in • HumanEval[6]: The study relied on automatic unit tests
real-world, context-specific scenarios. This inadequacy to evaluate the functional correctness of code generated
was evident through the extensive evaluation of LLMs’ by LLMs, which did not directly address whether the
vulnerabilitytovariousadversarialpromptattacks,which model’s solutions stemmed from genuine understanding,
was not specifically addressed or resolved within the or were merely optimized for the test scenarios, thus not
framework of the benchmark study itself. fully assessing genuine reasoning capabilities.
• AgentBench [33]: The study introduced AgentBench to • LegalBench[23]:Thestudy’sfocusondevelopingbench-
evaluate LLMs as agents across diverse real-world sce- marksforlegalreasoninginLLMshighlightedchallenges
narios, but it inherently faced the challenge of response in ensuring that the models genuinely understood legal
variability due to standardized evaluations not fully cap- concepts, rather than merely optimizing to fit bench-
turing the specialized, context-driven response behavior mark requirements, as evidenced by the varied perfor-
of LLMs tailored for specific scenarios, as evidenced by manceacrosstasksandtheutilizationofchain-of-thought
thesignificantperformancedisparitybetweencommercial prompts to improve reasoning capabilities.
and open-source LLMs across different tasks. • FLUE [24]: The study introduced a novel pre-training
• APIBank [34]: The study developed API-Bank, which, methodology and evaluation benchmarks, without ex-
despite its comprehensive design to evaluate tool- plicitly addressing the challenge of discerning genuine
augmentedLLMsthroughavarietyofAPIsanddialogue reasoningfromtechnicaloptimizationinLLMs,focusing
scenarios, inherently faces challenges in accurately rep- primarilyonperformanceimprovementsinfinancialNLP
resentingmodelperformanceacrossspecialized,context- tasks.
driven scenarios, as it aims to generalize across a thou- • MultiMedQA[8]:Thestudyacknowledgedthechallenge
sanddomainsandmultipletoolusageabilities,potentially inassessingwhetherLLMresponsestomedicalquestions
overlooking the complex performance of LLMs in spe- are derived from genuine understanding or technical
cific, real-world applications. optimization, particularly highlighting the reliance on
• C-Eval[35]:ThestudyintroducedC-EVALasacompre- benchmark performances that might not fully capture
hensiveChinese evaluationsuite, butacknowledgedvari- the complex reasoning capabilities required in clinical
ability in model responses, especially highlighting that contexts. This inadequacy remained unaddressed, as the
specialized models might not be accurately assessed by study focused on improving accuracy through technical
generic benchmarks, as seen in their varied performance meanssuchasinstructionprompttuning,withoutdirectly
across different subjects and levels. resolving the underlying issue of discerning genuine
• BOLAA [36]: The study demonstrated that benchmarks reasoning from optimization in LLM outputs.
like BOLAA, designed to evaluate LLM-augmented au- • M3KE [7]: The study utilized a unified promptapproach
tonomous agents across various scenarios, did not fully for zero-shot and few-shot settings without explicitly
capture the specialized response behavior of models tai- addressing the differentiation between models’ genuine
lored for specific tasks, leading to variability in model comprehension and their ability to technically optimize
responses.Thiswasevidentintheexperimentsconducted for benchmark performance.
across different LLMs and task complexities, where • T-Bench[25]:Thestudyfocusedonboostingopen-source
the performance of orchestrated multi-agent strategies LLMs’ tool manipulationcapabilities using methodslike
(BOLAA) in the WebShop and HotPotQA environments modelalignment,demonstrationretrieval,and generation
highlighted the challenge of accurately assessing the regulation, without directly addressing the challenge of
sophisticated capabilities of specialized models. distinguishing genuine reasoning from technical opti-
• HaluEval[37]: The study designeda benchmark,HaluE- mization in LLM responses.16
• Chain-of-ThoughtHub[26]:Thestudyusedfinalanswer todiscerngenuinereasoningfromtechnicaloptimization.
accuracyasaproxyforreasoningcapabilitywithoutcon- However, the primary focus was on improving LLMs’
sidering the correctnessof intermediatesteps, potentially ability to interact with APIs rather than directly address-
failing to differentiate genuine reasoning from technical ing the benchmark’spotential inadequacy in distinguish-
optimization. ing genuine reasoning capabilities from mere technical
• SciBench [27]: The study demonstrated challenges in optimization.
evaluatingthetruereasoningcapabilitiesofLLMsdueto • PromptBench [5]: The study acknowledged the chal-
reliance on technical optimization, as seen when LLMs, lenge of discerning genuine reasoning from technical
prompted with external tools, incorrectly derived equa- optimization in LLMs, indicating that it primarily eval-
tions,highlightingtheirstruggletounderstandmathemat- uated models based on their ability to follow instruc-
ical relationships beyond mere optimization. tions and generate outputs without deeply investigating
• ARB [28]: The study utilized a rubric-based evaluation, whether these outputs resulted from true understanding
which, despite its innovative approach, could not defini- ormerelyoptimizationstrategies.Thissuggestedthatthe
tively resolve whether LLMs’ responses stemmed from benchmarks could not fully distinguish between genuine
true understandingor were merely optimized for the test reasoning and technical optimization, reflecting the de-
conditions. scribed inadequacy.
• Xiezhi[29]:Thestudydevelopedacomprehensive,multi- • AgentBench [33]: The study did not provide explicit
disciplinary, auto-updating benchmark named Xiezhi, methods to differentiate whether LLMs’ responses were
aimed at evaluating domain knowledge. However, it fo- the result of genuine reasoning or merely technical opti-
cused on creating a broad and balanced dataset without mizations, focusing instead on evaluating LLMs’ ability
specifically addressing the challenge of discerning gen- toactasagentsinvariousenvironmentswithoutaddress-
uinereasoningfromtechnicaloptimizationinLLMs.The ing this specific concern.
benchmark’sdesignemphasizesthevarietyandfreshness • APIBank [34]: The study detailed an evaluation system
of questions but does not detail mechanisms for eval- fortool-augmentedLLMs,withoutdirectlyaddressingthe
uating the depth of understanding versus surface-level challenge of determining whether LLM responses stem
pattern recognition. from genuinereasoningor merely technicaloptimization
• BIG-Bench[4]:The studyintroducedBIG-Bench,which to match benchmark answers.
includes tasks beyond current LLM capabilities, aiming • C-Eval [35]: The study emphasized evaluating advanced
tobetterunderstandandpredictLLMbehavior.However, reasoningabilitiesthroughC-EVALHARDwithcomplex
it inherently struggles to differentiate between genuine questions, but it did not specifically address or provide
reasoning and technical optimization due to the diverse mechanisms to distinguish between genuine understand-
and complex nature of tasks, some of which may allow ingandtechnicaloptimizationbyLLMs,focusinginstead
models to optimize for benchmark performance without on the broad assessment of reasoning abilities without
demonstrating true comprehension. delving into the nature of the reasoning process itself.
• AGIEval [10]: The study utilized the Chain-of-Thought • BOLAA [36]: The study did not specifically address the
prompting technique to assess models’ reasoning capa- challengeofdiscerninggenuinereasoningfromtechnical
bilities, but it noted variability in performance improve- optimization in LLMs, focusing instead on evaluating
mentsacrosstasks,suggestingachallengeinconclusively LAAs across different architectures and tasks, without
determining genuine reasoning abilities versus technical explicitlyexaminingtheunderlyingreasoningcapabilities
optimization. of the LLMs involved.
• ToolAlpaca[30]:Thestudycreatedandevaluatedmodels • HaluEval [37]: The study’s methodology, focused on
using simulated tool-use scenarios, without explicitly evaluating LLMs’ ability to recognize hallucinated con-
addressingthedistinctionbetweengenuinereasoningand tent, indirectly involves assessing models based on their
optimizationto benchmarkspecifications,exemplifiedby output, without thoroughly investigating the underlying
the training and evaluation of models on a constructed reasoningprocesses,orthe extentto whichresponsesare
corpus without mechanisms to differentiate between true generatedthroughcomprehensionversusoptimizationfor
comprehension and mere performance optimization. benchmark performance.
• HELM[31]:TheHELMevaluationframeworkexplicitly
aimed to address variousfacets of language modelcapa-
bilities,includingreasoning,buttheinherentchallengeof C. Tension Between Helpfulness and Harmlessness
distinguishing between genuine reasoning and technical
Prevalence: 19/23
optimization in responses remains a significant concern
duetothemodels’“blackbox”natureandthecomplexity • MMLU [22]: The study acknowledged the challenge in
of interpreting LLM outputs beyond mere performance balancing helpfulness with harmlessness across various
metrics. tasks, including those with significant societal relevance
• ToolBench [32]: The study implemented a depth-first likelawandmorality,withoutofferingaresolvedmethod-
search-baseddecisiontree(DFSDT)toenhancetheplan- ology to accurately gauge this balance within its bench-
ning and reasoning ability of LLMs, indicating an effort marks.17
• HumanEval [6]: The study focused on evaluating LLMs benchmarks. It focused on evaluating LLMs’ advanced
trainedoncode,specificallyassessingtheirabilitytogen- reasoning capabilities across various domains, without
erate standalonePythonfunctionsfromdocstrings.How- explicitly discussing the balance between providing use-
ever, it did not address the tension between helpfulness fulinformationandavoidingharmfuloutputsinitsbench-
andharmlessnessinLLMbenchmarks.Themethodology marking approach.
primarilycenteredonfunctionalcorrectnessthroughunit • AGIEval[10]:Thestudyutilizedahuman-centricbench-
tests and did not incorporate a framework to evaluate mark based on standardized exams, without explicitly
or balance the helpfulness versus harmlessness of the addressing the balance between generating helpful and
generated code, which is essential in diverse real-world non-harmfuloutputs in diverse real-world contexts.
scenarios. • HELM [31]: The HELM evaluation framework included
• LegalBench [23]: The study did not provide a standard- targetedevaluationsandmulti-metricmeasurementacross
izedcriterionforbalancinghelpfulnessandharmlessness variousscenarios,butdidnotspecificallyaddresstheten-
in evaluating LLMs, focusing instead on legal reasoning sion between helpfulness and harmlessness in its criteria
tasks without addressing this specific tension. or methodologies, indicating an unresolved challenge in
• FLUE [24]:The studyutilizedA/B tests forhumaneval- balancing these aspects within the
uation without explicitly addressing the balance between • ToolBench [32]: The study introduced ToolBench and
helpfulness and harmlessness, potentially leading to an DFSDT to enhance LLMs’ tool-use capabilities, but did
overemphasis on one aspect over the other in model not specifically address the balance between helpfulness
evaluation. andharmlessness,focusinginsteadontoolintegrationand
• MultiMedQA [8]: The study introduced a sophisticated reasoning efficiency without detailing measures to avoid
approach for evaluating LLMs in medical question an- harmful outputs.
swering, incorporating human evaluation frameworks to • PromptBench [5]: The study did not establish a clear
assessthebalancebetweenhelpfulnessandharmlessness; criterion for balancing helpfulness and harmlessness in
however, it recognized the complexity in achieving this adversarial prompt evaluation, focusing instead on the
balance due to the intricate nature of medical knowl- robustness of LLMs against adversarial prompts without
edge and societal values, without presenting a resolved directlyaddressinghowthesemodelsbalancebeinghelp-
methodology to adequately quantify or standardize this fulandavoidingharmfuloutputsin real-worldscenarios.
balance. • AgentBench[33]:ThestudyextensivelyevaluatedLLMs
• M3KE [7]: The study did not establish a clear criterion across various tasks and environments, but did not es-
for balancing helpfulnessand harmlessness in evaluating tablish a clear methodology or criteria for balancing the
LLMs, focusing instead on assessing knowledge across provision of useful information with the prevention of
various subjects and education levels without addressing harmfuloutputs,asseenintheirevaluationsetupandper-
the subtle balance between generating useful and non- formanceanalysissections.Thisgapindicatedapotential
harmful responses. oversight in considering the complex and subtle balance
• T-Bench [25]: The study improved open-source LLMs’ betweenthoseaspectsinreal-worldapplications,directly
tool manipulation capabilities using techniques like impacting the benchmark’s ability to comprehensively
model alignment, system prompts, and demonstration evaluate LLMs for both functionality and cybersecurity.
retrievers, without specifically addressing the balance • C-Eval [35]: The study implemented a comprehensive
between helpfulness and harmlessness. evaluation across diverse disciplines, but it did not
• Chain-of-ThoughtHub[26]:Thestudydidnotestablisha specifically address the balance between helpfulness and
clearcriterionforbalancinghelpfulnessandharmlessness harmlessness, particularlyin the contextof cybersecurity
in its evaluation of LLMs, focusing instead on reasoning threats,indicatingagapinensuringLLMs’responsesare
capabilities without addressing the potential for harmful both useful and non-harmful.
outputs in diverse real-world scenarios. • BOLAA [36]: The experiment focused on orchestrating
• KoLA [9]: The study introduced a contrastive evaluation multiple LAAs to enhance decision-making and knowl-
system focusing on knowledge hallucination and overall edge reasoning capabilities, without specifically address-
performance, without explicitly addressing the balance inghowthesemodelsmanagethetrade-offbetweengen-
betweenhelpfulnessandharmlessnessinitsmethodology. erating helpful responses and avoiding harmful outputs
• SciBench[27]:Thestudydemonstratedatensionbetween in diverse real-world scenarios.
helpfulness and harmlessness in LLM benchmarks by • HaluEval [37]: The study introduced HaluEval, a bench-
emphasizing benchmarks’ struggle to quantify a bal- mark for evaluating LLMs’ performance in recognizing
ancebetweenprovidinghelpfulinformationandavoiding hallucinations, which inherently did not address the ten-
harmful outputs, particularly through the use of A/B sion between helpfulness and harmlessness. It focused
testsandtheincorporationofhumanevaluationmethods, on generating and annotating hallucinated responses to
without a clear criterion for this balance. assessLLMs’abilitytorecognizehallucinations,without
• ARB [28]: The study did not specifically address or directlytacklingthebalancebetweenprovidingusefulin-
demonstrate unresolved inadequacies regarding the ten- formationandavoidingharmfuloutputsinthebenchmark
sion between helpfulness and harmlessness in LLM design.18
D. Linguistic Variability and Embedded Logic Diversity • T-Bench [25]: The study focused on enhancing open-
source LLMs for tool manipulation without addressing
Prevalence: 17/23
the incorporation or evaluation of multilingual capa-
• MMLU [22]: The study introduced a benchmark cov- bilities or considering linguistic diversity in its bench-
ering a wide range of subjects but did not specifically marking process. The benchmark study, ToolBench, did
address or incorporate the diversity of cognitive and not acknowledge or attempt to address this particular
logical frameworks across different languages, focusing inadequacy of LLM benchmarks.
instead on assessing language understanding primarily • Chain-of-ThoughtHub[26]:Thestudyincludedbilingual
throughEnglish-basedtasks.Thestudyacknowledgedthe reasoning capabilities in English and Simplified Chinese
challenge of evaluating LLMs across diverse knowledge (erroneously labeling them as “Chinese” without differ-
domains, but did not explicitly attempt to address the entiation with Traditional Chinese), but did not specif-
linguistic diversity inadequacy. ically address or consider the linguistic differences and
• HumanEval [6]: The study focused exclusively on code embedded logics across different languages beyond this,
generation from English docstrings, without considering indicating a potential oversight of the depth of cognitive
or addressing the subtleties of linguistic diversity or the and logical frameworks unique to each language. The
embedded logic differences in various languages. This benchmark study did not acknowledge this inadequacy
approach inherently ignored the benchmark inadequacy or make attempts to address it.
related to linguistic differences and embedded logics, as • KoLA[9]:Thestudyacknowledgedtheinadequacybyin-
it evaluated the LLM’s code generation capability solely corporatingevolvingdata sources, includingnon-English
based on English, without any mention of adaptation or contentlikenewsarticlesandnovels,toevaluatemodels’
evaluation across different languages or addressing the understandingand generation abilities in diverse linguis-
inherentcognitiveframeworksshapedbyeachlanguage’s tic contexts. However, it did not explicitly address or
unique structure. The benchmarkstudy did not acknowl- attempttoresolvethe benchmarkinadequacyof ignoring
edge this inadequacy or make attempts to address it. linguistic differences and embedded logics in different
• LegalBench [23]: The study focused on developing and languages.
evaluating benchmarks within the context of American • Xiezhi [29]: The study constructed the Xiezhi bench-
legalreasoning and did notaddress linguistic differences mark focusing on domain knowledge evaluation across
or embedded logics in different languages. It utilized a wide array of disciplines without explicitly addressing
tasksprimarilyin“NaturalEnglish”,withoutconsidering linguistic diversity or the embedded logics in different
multilingualcontextsorthediversecognitiveframeworks languages.ItpredominantlyutilizeddatafromSimplified
shaped by each language’s unique structure. The bench- Chinese (erroneously labeling them as “Chinese” with-
markstudydidnotacknowledgethis specificinadequacy out differentiation with Traditional Chinese) educational
or make attempts to address it. systems and academic surveys, with no clear mention of
• FLUE[24]:Thestudyfocusedondevelopingandevaluat- adjustments or considerations for linguistic differences
inglanguagemodelsspecificallyforthefinancialdomain, beyondChinese andEnglish,nordid itacknowledgethis
utilizingprimarilyEnglishdatasetswithoutaddressingor as a limitation or attempt to address it.
incorporating multilingual contexts or the diverse cogni- • AGIEval [10]: The study utilized a bilingual (English
tive frameworks shaped by different languages’ unique and Simplified Chinese, erroneously labeling them as
structures. There was no mention of efforts to include “Chinese”) approach without specifically addressing or
or evaluate the models’ performance across various lan- accounting for the unique cognitive and logical frame-
guages or to consider linguistic diversity explicitly. works shaped by each language’s structure, thereby not
• MultiMedQA [8]: The study’s benchmarks primarily fo- fully addressing the benchmark inadequacy regarding
cused on English-language datasets and did not address linguistic diversity and embedded logic. While the study
the need for multilingual evaluations, indicating a dis- acknowledgedthe inclusion of tasks in both English and
regard for linguistic differences and embedded logics in Chinese, it did not discuss efforts to address the unique
different languages. While the benchmark study, Multi- systemsofreasoningfosteredbydifferentlinguisticstruc-
MedQA, was praised for its diversity and inclusion of tures, suggesting a potential overlook of the linguistic
various medical exam, research, and consumer sources, diversity and embedded logic inadequacy.
the study acknowledged its limitation by not incorporat- • ToolAlpaca [30]: The study predominantly utilized En-
ing a broader variety of languages and cultural contexts, glish for instructions and API documentation generation
thereby not addressing the mentioned benchmark inade- without explicitly addressing or incorporating linguistic
quacy. differences or the unique logics embedded in various
• M3KE [7]: The benchmarkstudy focused exclusivelyon languages. The study did not acknowledge or attempt
Simplified Chinese LLMs, erroneously labeling them as to address the benchmark inadequacyrelated to ignoring
“Chinese” without differentiation with Traditional Chi- linguistic differences and embedded logics in different
nese, and did not address or recognize the diversity of languages.
cognitive and logical frameworks across languages, thus • HELM [31]: The study acknowledged the challenge of
overlooking linguistic differences and embedded logics. accounting for linguistic diversity in benchmarks, but19
did not provide a solution, exemplified by its focus on • T-Bench [25]: The study encountered substantial diffi-
English and limited consideration of other languages, culty in effectively scaling benchmarks for open-source
which indicates an unaddressed and unresolved inad- LLMs. For instance, they noted significant performance
equacy regarding linguistic differences and embedded disparities between open-source models and proprietary
logics. oneslike GPT-4 in tool manipulationtasks, necessitating
• ToolBench [32]: The study focused on integratingLLMs enhancedtechniquesandinfrastructuretobridgethisgap.
with a vastnumberof real-worldAPIs withoutexplicitly • KoLA [9]: The study required significanteffort to install
addressing the challenge of linguistic diversity or the and scale the KoLA benchmark, as indicated by the
embedded logics in different languages. The methodol- detailed description of the evolving and known data
ogy was primarily based on automatic construction us- sources and complex evaluation criteria, demonstrating
ing ChatGPT, which inherently follows a predominantly the technological challenges inherent in this process.
English-centric approach due to its training data and • SciBench [27]: The study demonstrated complexity in
design, without adapting to or considering the unique implementing and scaling benchmarks, as it involved
cognitiveandlogicalframeworksthatdifferentlanguages intricateprocesseslikemanualdataextractionfromPDFs
might embody. The study did not acknowledge this to LaTeX, complex problem formulations, and the use
specific inadequacy or attempt to address it. of external tools for evaluation, which aligned with the
• APIBank [34]: The study’s benchmark, API-Bank, was stated concerns of installation and scalability challenges.
primarilyfocusedonEnglishanddidnotaddressthecon- • ARB [28]: The study faced significant difficulties in
struction and evaluation of models for other languages, efficiently scaling and implementing the ARB bench-
indicating a disregard for linguistic differences and em- mark, evident in the extensive manual grading required
bedded logics in different languages. The benchmark forcomplexsymbolicandproof-likeproblems,reflecting
study acknowledged this limitation and mentioned plans challenges in installation and scalability.
to address data construction and model evaluation for • Xiezhi [29]: The study utilized the Xiezhi benchmark,
other languages as future work. which requiredconsiderable effort to adapt and scale for
• C-Eval [35]: The study explicitly focused on evaluating evaluating various LLMs, as evidenced by the detailed
LLMsinaSimplifiedChinese(erroneouslylabelingthem descriptionofitscomplexsetupandevaluationprocesses.
as“Chinese”withoutdifferentiationwithTraditionalChi- • BIG-Bench [4]: The study encountered challenges in ef-
nese)context,withoutaddressingorattemptingtoinclude ficientlyscalingtheBIG-benchframeworkacrossvarious
diverse linguistic backgrounds or cognitive frameworks, models, as evidencedby their focuson analyzingperfor-
thus overlooking the benchmark inadequacy related to mance differences across model sizes and architectures.
ignoring linguistic differences and embedded logics in • AGIEval [10]: The study encountered challenges in in-
differentlanguages.Thebenchmarkdidnotacknowledge stalling and scaling benchmarks for evaluating LLMs,
or address this inadequacy. as evidenced by the extensive use of different models
• HaluEval [37]: The study focused on evaluating LLMs’ (GPT-4, ChatGPT, Text-Davinci-003) and varied testing
ability to recognize hallucinations across different tasks, methodologies (zero-shot, few-shot, Chain-of-Thought),
without addressing the linguistic diversity or embedded indicatingsignificantengineeringeffortsforimplementa-
logic specific to non-English languages. The benchmark tion.
did not acknowledge or attempt to address this inade- • ToolAlpaca[30]:Thestudyfacedchallengesininstalling
quacy, relying instead on predominantly English-centric and scaling the framework for evaluating language mod-
data and evaluation criteria. els,asevidencedbytheirrelianceoncomplexmulti-agent
simulation environments and sophisticated infrastructure
E. Benchmark Installation and Scalability
for ToolAlpaca’s implementation.
Prevalence: 16/23 • PromptBench [5]: The study encountered difficulties in
• MMLU [22]: The study demonstrated challenges in efficiently scaling their benchmark framework, Prompt-
scalability and installation complexity for benchmarks, Bench, across various large language models, indicating
as evidenced by the extensive engineering efforts and substantial technical challenges in installation and scala-
resource allocation required to implement and scale the bility.
benchmark across various scenarios and models. • AgentBench[33]:Thestudydemonstratedthecomplexity
• LegalBench [23]: The study encountered challenges in of setting up and operating the AgentBench benchmark,
efficientlyscalingits benchmarkacrossvariousscenarios which required extensive configurationand adaptation to
and models, as evidenced by the need for manually various models and environments, indicating the chal-
generating and annotating samples for specific tasks like lenges in installation and usage.
the “Hearsay” or “Personal Jurisdiction” sections. • APIBank [34]: The study faced significant challenges
• M3KE [7]: The study introduced M3KE, requiring sig- in installing and scaling the benchmark framework, ev-
nificant engineering efforts for setting up and assessing idenced by the complex implementation of 73 APIs,
a wide range of Chinese LLMs, without specifically requiring substantial engineering efforts and extensive
addressing the complexity and scalability challenges of annotation processes.
the benchmark’s installation and operational efficiency. • BOLAA [36]: The study encounteredchallenges in scal-20
ing and installing the benchmark framework for eval- benchmark to biases and inaccuracies of the generative
uating LLM-augmented Autonomous Agents (LAAs), model used, as described in the study’s methodologyfor
necessitating considerable adjustments in infrastructure generating and evaluating hallucinated content.
and resource allocation.
• HaluEval [37]: The study faced significant challenges in
implementingandscalingtheirHaluEval,whichrequired
extensive use of resources and intricate engineering ef-
forts, exemplified by their complex two-step generation
process for hallucinated samples.
F. Biases in LLM-Generated LLM Evaluations
Prevalence: 9/23
• M3KE [7]: The M3KE benchmark utilized model-
generated multiple-choice questions to assess the capa-
bilities of Chinese Large Language Models, inherently
incorporating the biases and inaccuracies of the models
used to generate these evaluations, as evidenced by their
methodologyofcollectingandorganizingquestionsfrom
public websites and ensuring a standardized assessment
process without explicitly addressing the mitigation of
suchbiasesinthecreationorselectionofthesequestions.
• T-Bench [25]: The study utilized generative AI models
to enhance open-source LLMs for tool manipulation,
explicitly involvingmodel-generatedevaluationsthrough
programmatic data generation and in-context demonstra-
tionretrievers,whichcouldinheritbiasesfromthemodels
used to generate them.
• ARB [28]: The study utilized GPT-4 to generate rubrics
and evaluate the reasoning of symbolic math and proof-
like problems,inheritingpotentialbiasesofGPT-4 in the
evaluation process.
• Xiezhi [29]: The benchmark utilized ChatGPT for tag-
ging questionswith disciplinarylabels, demonstratingan
instance where biases inherent to the ChatGPT model
generating evaluations could have influenced the objec-
tivity and reliability of the evaluations.
• BIG-Bench [4]: The benchmark utilized generative AI
models to assess other LLMs, inheriting the biases and
inaccuracies from the models used to generate evalua-
tions, thus potentially compromising the objectivity and
reliability of these assessments.
• ToolAlpaca[30]:ThestudyutilizedgenerativeAImodels
to automatically generate structured documentation for
APIs,whichwerethenusedtosimulatetool-useinstances
for training LLMs, inherently incorporating biases from
the generative models used.
• AgentBench[33]: The study utilized generativeAI mod-
elstocreatenoveltasksandevaluateLLMs,inheritingbi-
asesfromthemodelsusedforgeneration,asevidencedby
theuseofmodel-generatedtasksinvariousenvironments
without addressing the potential for inherited biases.
• APIBank [34]: The study utilized generative AI models
toautomaticallymass-producetrainingdata,whichlikely
inheritedbiasesfromthemodelsused,compromisingthe
objectivity and reliability of evaluations.
• HaluEval [37]: The study utilized generative AI models,
specifically ChatGPT, to automatically generate halluci-
nated samples for evaluation, inherently subjecting the21
APPENDIX B performancecomparison.Therewasnomentionofefforts
EXAMPLESOF BENCHMARK INADEQUACIES IN to standardize benchmark execution or address potential
PROCESSUAL ELEMENTS inconsistencies across different research teams.
• KoLA [9]: The study established a new benchmark,
A. Inconsistent Benchmark Implementation
KoLA, designed to evaluate LLMs across various
Prevalence: 18/23 knowledge-intensivetasksusingbothknownandevolving
• MMLU [22]: The study developed a new benchmark data sources, and introduced a contrastive evaluation
to evaluate LLMs across a diverse set of subjects and system aimed at ensuring fairness and applicability in
tested themin zero-shotand few-shotsettings, indicating LLMassessment.However,itdidnotspecificallyaddress
an innovative approach yet not directly addressing the theinconsistencyinbenchmarkexecutionacrossdifferent
uniformity and objectivity in benchmark implementation laboratories,focusinginstead on creating a more reliable
across different laboratories. and fair evaluation framework through innovative data
• HumanEval[6]:Thestudyacknowledgedthepotentialfor sources and evaluation metrics. The benchmark study
diverse outcomesfrom varyingimplementationmethods, acknowledgedtheneedforbetterfairnessandapplicabil-
but did not specifically address or propose solutions ityinLLMevaluationsbutdidnotmakeexplicitattempts
to standardize benchmark execution across different re- to address the inconsistencyin executionacrossdifferent
search teams, focusing instead on the development and research teams.
evaluation of models using their own benchmarks and • SciBench [27]: The study introduced SciBench to sys-
methodologies. tematically examine complex scientific problem-solving
• LegalBench[23]:Thestudy introducedLegalBenchwith capabilities of LLMs, but it encountered inconsistencies
an emphasis on diverse legal reasoning tasks and con- in benchmark implementation. The benchmarks, derived
ducted preliminary evaluations on different models, yet from collegiate-level textbooks and exams, required ad-
it did not address the inconsistency in benchmarkimple- vanced computations like integration and differentiation,
mentation directly. Although the study aimed to foster which varied in execution complexity. Despite efforts
standardized evaluations through its collaborative and to minimize data leakage and use detailed solutions for
open design, it implicitly acknowledgedthe challenge of error analysis, the study acknowledged the challenge
ensuring uniformity across implementations by inviting of ensuring uniform assessment across different LLM
communitycontributionstorefineandexpandthebench- configurationsand the potentialfor variedinterpretations
mark suite. and implementations of benchmarks, without detailing
• FLUE[24]:ThestudyintroducedtheFinancialLanguage explicit measures to address these inconsistencies di-
Understanding Evaluation (FLUE) and a new financial rectly.
language model (FLANG), focusing on domain-specific • ARB [28]: The study introduced a novel benchmark,
pre-training and evaluation without explicitly addressing ARB, to evaluate LLMs using advanced reasoning prob-
or attempting to standardize the implementation process lems, yet acknowledged the challenge of ensuring con-
across different teams or laboratories, which could lead sistent and reliable evaluation methods, particularly for
to inconsistent benchmark applications and results. The symbolic and proof-like problems, without directly ad-
study did not mention any measures to address this dressing the inconsistency in benchmark implementation
benchmark inadequacy directly. across different labs.
• MultiMedQA [8]: The study introduced MultiMedQA, • Xiezhi[29]:ThestudyintroducedtheXiezhibenchmark,
aiming to overcome limitations of existing benchmarks which aimed to establish a comprehensive and multi-
by combining six medical question answering datasets disciplinary auto-updatingbenchmark.Despite its efforts
and a new dataset, HealthSearchQA, for comprehensive toaddressinconsistenciesbyincorporatingabroadrange
LLMevaluation.However,itrecognizedthechallengeof of disciplines and updating content, the study does not
ensuringconsistentbenchmarkapplicationacrossdiverse explicitly discuss measures taken to standardize bench-
medicaldomains,acknowledgingthedifficultyinuniform mark implementation across different research teams or
implementation without directly addressing a solution to laboratoriesto ensureuniformexecutionandassessment.
this inconsistency. • AGIEval [10]: The study employed a novel benchmark,
• M3KE[7]:ThestudydevelopedM3KE,abenchmarkfor AGIEval, which was specifically designed to evaluate
Simplified Chinese (erroneously labeling them as “Chi- foundation models on human-centric tasks derived from
nese” without differentiation with Traditional Chinese) high-standard admission and qualification exams. The
LLMs, without addressing the inadequacy of inconsis- benchmarks aimed to assess models’ general abilities in
tent benchmark implementation; it did not mention any tasks related to human cognition and problem-solving.
standardizedprotocolsor guidelinestoensureuniformity However, the study did not address the issue of im-
across different research teams or laboratories. plementation inconsistency across different laboratories.
• Chain-of-Thought Hub [26]: The study integrated di- While the study made a significant effort to create a
verse benchmarks (e.g., GSM8k, MATH, MMLU) for standardized and objective assessment through AGIEval,
evaluating LLM reasoning, but did not address imple- it did not explicitly mention measures to ensure uniform
mentation inconsistencies, focusing instead on model implementation especially of human-centric tasks across22
different research teams, which could potentially lead • MMLU[22]:Thestudyfacedslowtestiterationtime,evi-
to variability in results similar to those observed in dentfromitsrelianceoncomprehensiveevaluationacross
benchmarks like MMLU and BBQ. a diverse set of subjects and the significant processing
• ToolAlpaca [30]: The study developed a novel frame- periods required, without specific mention of efforts to
work, ToolAlpaca, to automatically generate a diverse address this benchmark inadequacy.
tool-use corpus and enhance compact language mod- • FLUE [24]: The study introducedbenchmarksfor evalu-
els’ generalized tool-use abilities, without specifically atingLargeLanguageModelsinthefinancialdomain,ne-
addressing or resolving the inconsistency in benchmark cessitating extensive testing and iterations across various
implementation across different laboratories. The bench- scenarios,likelyextendingthetimeframeforbenchmark
markstudydidnotacknowledgethisinadequacyofLLM completion.Thestudydidnotexplicitlyacknowledgethis
benchmarks nor made attempts to address it. inadequacy or mention attempts to address it.
• HELM [31]: The study acknowledged the challenge of • MultiMedQA [8]: The study utilized a benchmark com-
ensuring uniform evaluation methods across all models, bining multiple datasets and evaluated models with hu-
highlighting ongoing efforts to refine and adapt bench- man and automated methods, which inherently extends
marking practices to maintain relevancy and accuracy the evaluation timeframe, but did not explicitly mention
in assessments, without explicitly stating that this inade- efforts to address or mitigate slow iteration time.
quacy has been fully resolved. • T-Bench [25]: The study demonstrated that the pro-
• ToolBench [32]: While the study introduced innovative cess of enhancing open-source Large Language Models
methods like DFSDT to improve LLM’s planning and (LLMs) with techniques like model alignment, system
reasoningabilities anddevelopedan automaticevaluator, prompts,andin-contextdemonstrationretrieversrequired
ToolEval,toassessLLM’stool-usecapabilities,itprimar- a practical level of human supervision, indicating that
ily focused on enhancing model performance and gener- the evaluation of LLMs using the ToolBench benchmark
alizationratherthan standardizingbenchmarkimplemen- could not be fully automated and might be subject to
tation procedures. The benchmark study acknowledged prolonged iteration time. The study acknowledged the
the complexity of evaluating LLMs in tool-use scenarios challenge of slow iteration time implicitly by emphasiz-
but has not made specific attempts to address the incon- ingthepracticalamountofhumansupervisionneededfor
sistency in benchmark execution methodologies. enhancing LLMs, although it did not explicitly address
• PromptBench[5]:Thestudydidnotaddressthepotential measures to reduce evaluation time.
variabilityinimplementingitsadversarialpromptattacks • Chain-of-Thought Hub [26]: The study introduced the
across different models or laboratories, and it did not Chain-of-ThoughtHub, which aimed to evaluate LLMs’
attemptto standardizeor ensure uniformimplementation reasoning capabilities, but did not specifically address
methodologies for these prompts. the slow test iteration time. This benchmark required
• AgentBench [33]: The study introduced AgentBench, a evaluating new models across multiple complex reason-
new benchmark designed to evaluate LLMs across a ing tasks, which likely extended the evaluation period
variety of environments, and extensively evaluated 27 significantly, similar to the slow iteration time observed
LLMs, including both API-based commercial models inbenchmarkslikeBIG-benchandHELM.However,the
and open-sourced LLMs. However, the study did not study did not acknowledge this inadequacy or mention
directly address the issue of inconsistency in benchmark attempts to address it.
implementation across different laboratories. Instead, it • KoLA [9]: The study introduced a benchmark that re-
focusedon establishing a new benchmarkand evaluating quires manual annotation for evolving tasks, which was
LLMs within that framework without mentioning efforts time-intensive and cannot be automated, leading to po-
tostandardizeimplementationprotocolstoensureconsis- tential delays in evaluating LLMs akin to those seen in
tent execution across different research teams. frameworkslike BIG-bench and HELM. The benchmark
• HaluEval [37]: The study utilized ChatGPT to automat- didnotaddresstheneedforquickeriterationtime,despite
ically generate hallucinated samples, which is subject mentioning the evolving nature of data and tasks.
to the model’s capacity to follow complex instructions • SciBench [27]: Thestudy introducedSciBench, a bench-
for hallucination sampling. Their approach introduced markrequiringdetailedsolutionsandcomplexreasoning,
inconsistency in benchmark implementation due to the involvingmanualLaTeXformattingfromPDFstoensure
reliance on a single LLM’s capacity, potentially leading dataintegrityandminimizetrainingdataleakage.Despite
to variability in the quality of generated samples. The efforts to mitigate slow iteration time through detailed
study acknowledgedthis limitation, noting the quality of problem and solution documentation, the study’s exten-
hallucinated samples is bounded by ChatGPT’s under- sive and manualdata processing indicated potentialslow
standing of the instructions, but did not propose specific test iteration time for evaluating LLMs, acknowledging
measures to address this inconsistency directly. but not explicitly addressing this benchmark inadequacy.
• ARB[28]:Thestudyintroducedarubric-basedevaluation
method for advanced reasoning tasks and tested LLMs,
B. Slow Test Iteration Time
including GPT-4, on a diverse set of problems, requir-
Prevalence: 18/23 ing human evaluators for complex symbolic answers23
and proof-likequestions,indicatingprolongedevaluation withtheinadequacymentioned.Thestudydidnotspecif-
time.Whileitacknowledgedthechallengesinautomating ically address the speed of iteration time as a limitation
the evaluation of advanced reasoning tasks, it did not or concern in its evaluation framework.
specifically address measures to significantly reduce test • APIBank[34]:ThestudyintroducedAPI-Bank,abench-
iteration time. mark that requires manual annotation and extensive API
• BIG-Bench [4]: The benchmark study emphasized the integration,whichinherentlyextendedtheevaluationtime
computationalexpense of full evaluation,especially with frame, echoing the inadequacy of slow test iteration
programmatictasks, indicatinga lengthyprocesswithout time. Although they employed a novel method to reduce
addressing the efficiency of iteration time directly. annotationcosts,thebenchmark’sdesignnecessitatedde-
• AGIEval[10]:Thestudyutilizedahuman-centricbench- tailedandmanualinterventions,whichlikelyprolongthe
mark, AGIEval, specifically designed for evaluating overall process. The study did not mention any specific
LLMs,focusingontasksderivedfromofficialpublicand strategies to address or mitigate the slow iteration time
high-standardexams.Despiteitsinnovativeapproach,the directly.
study did not explicitly address the potential for slow • C-Eval [35]: The study outlined the creation and eval-
test iteration time due to the comprehensive and man- uation of the C-EVAL suite, emphasizing rapid under-
ual nature of its evaluation process, involving extensive standing and improvement of LLM capabilities, but did
human comparison and analysis. This indicates that the not specifically address or propose solutions for slow
processofevaluatingnewmodelswithAGIEvalcouldbe test iteration time inherent in comprehensive evaluations
time-consuming, aligning with the described benchmark like those mentioned for BIG-bench and HELM. This
inadequacyofslow iterationtime.Therewas nomention indicated that the process of using C-EVAL for LLM
ofattemptstoexpeditetheevaluationprocessorautomate evaluation could indeed be lengthy and possibly lacked
the benchmarking to mitigate this issue. automation,aligningwiththementionedinadequacy.The
• ToolAlpaca[30]:Thestudyutilizedamulti-agentsimula- document did not explicitly acknowledge or attempt to
tion to generate a corpusfor evaluating LLMs, a process address the slow iteration time challenge.
that cannot be automated and likely extends over weeks • BOLAA [36]: The study’s benchmark, BOLAA, was
or months due to manual intervention and complexity, designed to orchestrate multiple LLM-augmented au-
directly mirroring the inadequacy of slow test iteration tonomous agents, and required extensive testing across
time.Thestudydidnotmentionaddressingthisparticular variousscenarios,inherentlyextendingthe timeframefor
benchmark inadequacy. completion. The benchmark did not explicitly address
• HELM [31]: The study implemented a comprehensive or attempt to reduce the slow iteration time associated
and systematic evaluation approach to benchmarking with evaluatingnew models,makingit susceptible to the
LLMs, incorporating extensive metrics and scenarios, described inadequacy.
which inherently necessitated significant computational • HaluEval [37]: The study utilized a two-step process,
resources and time for thorough evaluation, thus likely sampling-then-filtering, for generating hallucinated sam-
extending iteration time for model testing. The study, ples, which inherently involved a time-consuming, man-
while acknowledging the broad and intricate scope of ual annotation component by human labelers. The study
its benchmark, did not specifically address or propose acknowledged the limitation of slow iteration time, due
solutions to reduce the slow test iteration time inherent totherelianceonmanualhumanannotationandcomplex
to its extensive evaluation methodology. instruction following by LLMs for quality control, but
• PromptBench [5]: The study required iterating over the it did not present a solution to expedite the evaluation
entire dataset 100 times on average to generate one process significantly.
adversarial prompt, indicating a potentially prolonged
evaluation process, which aligned with the mentioned
C. Challenge of Proper Prompt Engineering
inadequacy of slow iteration time in benchmarks. The
study did not specifically addressor propose solutionsto Prevalence: 14/23
reduce these iteration time, suggesting that the challenge • MMLU[22]:Thestudydesignedthebenchmarktoassess
of aligning rapid AI development with thorough and models in zero-shot and few-shot settings across a wide
effective benchmarking remains unaddressed. range of subjects, focusing on evaluating knowledge ac-
• AgentBench [33]: The study developed a comprehen- quired during pretraining without directly addressing the
sive and systematic benchmark, AgentBench, to evalu- intricaciesofpromptengineering.Itmentionedtheuseof
ate LLMs as agents across a wide array of real-world various prompts, but did not scrutinize the optimization
challenges,requiringmulti-turninteractionswithdetailed ofsuchpromptstoavoidbiasesormisinterpretationsthat
and complex environments. This inherently suggested a could skew assessment results. The study acknowledged
potentially time-consumingevaluation process, given the thelimitationofcurrentbenchmarksinaccuratelyreflect-
variety and depth of the tasks involved. Although the ing models’ capabilities, but did not specify efforts to
study provided an integrated toolkit to streamline LLM address prompt engineering challenges specifically.
evaluation,the complexityandbreadthof the benchmark • LegalBench [23]: The study outlined challenges in
likely extended the time frame for completion, aligning prompt engineering, notably in crafting prompts that ac-24
curately assess LLMs’ legal reasoning capabilities with- However, it did not specifically address or propose solu-
outintroducingbias,thusimpactingbenchmarkintegrity. tions to overcome those challenges in prompt engineer-
Thestudyacknowledgedthecomplexityoflegallanguage ing, highlighting an ongoing inadequacy in effectively
and the ongoing effort to refine evaluation techniques, evaluating LLMs through benchmarks.
indicatingan attemptto addressbenchmarkinadequacies • HELM [31]: The study utilized a standardized few-
but also highlighting the unresolved nature of prompt shot prompting adaptation for all models, indicating an
engineering challenges. awarenessofpromptengineeringchallenges.However,it
• FLUE [24]: The benchmark development focused on also highlighted the sensitivity of model performance to
creating assessments across various NLP tasks in the prompt formatting and adaptation methods, revealing an
financial domain, but did not explicitly mention efforts ongoing struggle with crafting prompts that accurately
to mitigate or acknowledge the intricacies involved in assess model capabilities without introducing biases or
crafting unbiased and effective prompts to accurately inaccuracies. The benchmark acknowledged this inade-
gauge LLM capabilities. quacy by discussing the variation in model performance
• MultiMedQA [8]: The study acknowledgedthe difficulty based on different prompting strategies.
in creating promptsthat accurately assess LLMs without • AgentBench [33]: The study designed and implemented
introducing biases or misinterpretations, affecting both AgentBench to evaluate LLMs as agents across various
functionality and cybersecurity domains. It attempted environments,including code-grounded,game-grounded,
to address this through instruction prompt tuning, aim- and web-grounded scenarios, without explicitly address-
ing to align LLMs more closely with medical domain ingthechallengesofcraftingunbiasedandrepresentative
requirements, but the challenge remains complex and promptsthataccuratelyassessamodel’scapabilities.This
unresolved, indicating ongoing issues with prompt en- omissionsuggestedthatthestudymightnotfullyaccount
gineering adequacy in benchmark assessments. for the complexities of prompt engineering, potentially
• M3KE [7]: The study utilized a unified prompt for all affecting the accuracy of its evaluations. The benchmark
models across different settings without detailing efforts study acknowledged the need for systematic evaluation
to mitigate biases or inaccuracies in promptformulation, of LLMs as agents, but did not specifically address
impacting the models’ evaluation accuracy and poten- or attempt to mitigate the inadequacies associated with
tiallyoverlookingthecomplexityofassessingLLMs’true prompt engineering.
capabilities. The study did not explicitly acknowledgeor • C-Eval [35]: The study implemented a comprehensive
address this benchmark inadequacy. evaluationof LLMs on C-EVAL, includingboth answer-
• T-Bench [25]: The study developedToolBench, a bench- only and chain-of-thought settings, without explicitly
mark to evaluate open-source LLMs for tool manipula- addressing or mitigating the intricacies of prompt engi-
tion tasks, employing techniques like model alignment, neering.Althoughthe studydetailedthe creationandap-
demonstration retrieval, and generation regulation with plicationof C-EVALforevaluatingLLMsacrossvarious
system prompts. However, it did not explicitly address disciplinesanddifficultylevels,itdidnotdiscussspecific
the intricacies of crafting unbiasedand effectiveprompts measurestoensurethepromptsaccuratelyandeffectively
to accurately measure a model’s capabilities. The study elicit the models’ capabilities without introducing biases
acknowledged the importance of prompt engineering by or misinterpretations. The benchmark study recognized
incorporating system prompts designed to guide model the importanceof evaluatingadvancedabilities of LLMs
generation, yet it did not detail efforts to address or in a Simplified Chinese context but did not explicitly
mitigate the potential biases and limitations inherent in mention efforts to address the prompt engineering chal-
prompt design. lenge.
• Chain-of-Thought Hub [26]: The study introduced the • BOLAA [36]: The study did not specifically address or
Chain-of-ThoughtHub to measure reasoning capabilities attempt to mitigate the intricacies of prompt engineering
of LLMs using a suite of reasoning benchmarkswithout for accurately and effectively eliciting the capabilities
addressing the intricacy of crafting prompts that accu- of language models. Instead, it focused on evaluating
rately assess these capabilities without bias, which was the performance of various LAA architectures and their
crucial for fair evaluation. The study acknowledged the orchestration without delving into the effects of prompt
challenge of evaluating complex reasoning capabilities designonbenchmarkoutcomes.Thestudyacknowledged
in LLMs, but did not specifically mention attempts to the importance of prompt engineering indirectly by ex-
address the prompt engineering perimenting with different LAA architectures and their
• ARB [28]: The study introduced a novelbenchmarkthat interaction strategies, but did not offer a solution to the
included advanced reasoning problems and proposed a inadequacy of prompt engineering itself.
rubric-basedself-evaluation method for assessing LLMs, • HaluEval[37]:The studyacknowledgedthechallengeof
acknowledging the difficulty in prompt engineering, but prompt engineering by designing a two-step generation
did not provide a conclusive solution to this challenge. and evaluation process to generate and evaluate hallu-
• BIG-Bench[4]:Thestudyacknowledgedthedifficultyin cinated responses, but it did not explicitly address the
craftingpromptsthataccuratelyreflectedamodel’scapa- inadequacy of ensuring prompts accurately reflect LLM
bilities without introducing biases or misinterpretations. capabilities without introducing biases.25
APPENDIX C duce inconsistenciesand subjectivity in LLM evaluation.
EXAMPLES OF BENCHMARK INADEQUACIES IN HUMAN Thestudydidnotacknowledgeorattempttoaddressthis
DYNAMICS inadequacy.
• KoLA [9]: The study introduced the KoLA benchmark,
A. Diversity in Human Curators and Evaluators
focusing on evaluating LLMs’ world knowledge across
Prevalence: 19/23 variouscognitiveabilitieswithoutspecifically addressing
• MMLU[22]:Thestudydesignedabenchmarkthatrelied the diversity of human curators and evaluators. While it
on the aggregation of questions from various sources emphasized unbiased and fair evaluation through known
without explicitly addressing the potential biases intro- and evolving data sources, it did not explicitly mention
duced by the diversity of human curators and evaluators. effortstomitigateoracknowledgetheinfluenceofhuman
It acknowledged the importance of diverse evaluative diversity in its creation or evaluation processes, poten-
perspectives, but did not detail measures to address this tially leaving room for subjectivity and inconsistency in
benchmark inadequacy directly. benchmark developmentand interpretation.
• HumanEval [6]: The study utilized an evaluation set • SciBench [27]: The study involved human annotators in
called HumanEval, composed of hand-written program- datacollectionandverification,emphasizingtheinfluence
ming problems assessed through automated unit tests, of human diversity on the benchmark’sdevelopmentand
without explicit mention of human evaluators in the evaluation. It acknowledged the challenges of diverse
benchmarking process. The study did not address the humaninterpretation,butdidnotdetaileffortsto address
potential variability and subjectivity that human eval- this specific benchmark inadequacy.
uators could introduce, focusing instead on automated • Xiezhi [29]: The study employed manual annotations
correctness checks. and used ChatGPT for tagging, introducing subjectivity
• LegalBench [23]: The study developed LegalBench with and potential inconsistency due to the reliance on hu-
tasks contributed by legal professionals, which did not man interpretation and machine understanding, without
explicitly address the diversity in backgrounds of these addressing the diversity of evaluators’ backgrounds. The
contributorsor evaluators,possibly leading to the bench- study acknowledged the importance of diverse domain
mark not uniformly assessing LLMs. The benchmark knowledge, but did not explicitly address the potential
studyacknowledgedthe collaborativenatureoftask con- biases introduced by the human element in the creation
tributions but did not specifically address the diversity and evaluation of the benchmark.
aspect of human curators and evaluators. • BIG-Bench[4]:Thestudyengagedateamofexpertraters
• MultiMedQA [8]: The study engaged clinicians and to complete tasks submitted to BIG-bench, employing
laypeople from diverse locations (USA, UK, India) for those human evaluators without addressing the potential
the evaluation of LLM outputs, highlighting variability variability and bias introduced by their diverse cultural,
in human judgmentsdue to their backgrounds.However, religious, political, and academic or commercial back-
it acknowledged this diversity and attempted to address grounds.Whileaimingforastronghumanraterbaseline,
it by using a panel of clinicians and lay users for the study acknowledged the challenge of representing
evaluations, aiming for a comprehensive understanding “human performance” due to the wide-ranging content
across different demographics. within BIG-bench,but did not providespecific strategies
• M3KE [7]: The study involved human-generated to address the diversity-related inadequacies in bench-
multiple-choice questions covering a broad range of mark evaluation.
subjects and educational levels without explicitly ad- • AGIEval [10]: The study acknowledged the challenge of
dressing the variability in cultural, religious, political, subjectivity in human evaluation, but did not provide a
and academic backgrounds. The benchmark study did specific solution to address the inconsistency in bench-
not acknowledge or attempt to address this aspect of marks due to diverse human interpretations, especially
benchmark inadequacy. in evaluating LLMs using a human-centric benchmark
• T-Bench [25]: The study involved human curators in the derived from various high-standard exams.
creationof demonstrationexamplesandthe alignmentof • ToolAlpaca [30]: The study utilized ChatGPT as a user
modelswithprogrammaticdata,demonstratingareliance agenttogenerateinstructionsandGPT-3.5asanassistant
on human judgment for benchmark development. How- agentforstructuredoutputgeneration,relyingonhuman-
ever, the study did not explicitly address the potential curated inputs and annotations for evaluating model per-
biases or inconsistencies introduced by this human in- formance,whichcouldbeinfluencedbytheheterogeneity
volvement, nor did it outline specific measures taken to of human backgrounds and subjective interpretations.
mitigate such issues. The study acknowledged the importance of diversity in
• Chain-of-Thought Hub [26]: The study curated bench- its dataset construction and aimed to mitigate this by
markswithoutaddressingthediversityinhumancurators generating a diverse corpus. However, it did not specif-
and evaluators, leading to potential biases in benchmark ically address the potential biases introduced by human
development and evaluation. The benchmarks did not evaluators’ diversity in the development and evaluation
accountfor the variability in cultural, religious, political, of benchmarks.
andacademicbackgroundsofhumans,whichcouldintro- • ToolBench [32]: The study used ChatGPT for the con-26
struction of the ToolBench benchmark, involving the norms, indicating a potential inadequacy in respecting
generation of diverse instructions and solution paths for diverse perspectives through standardized answers or
real-world APIs, inherently relying on the human-like rubrics.Thestudyacknowledgedthecomplexityofevalu-
reasoning capabilities of ChatGPT for curating and eval- atingLLMsonsociallyrelevantsubjectslikemoralityand
uating benchmarks. The study did not explicitly address law, but did not explicitly address attempts to overcome
the potential biases or inconsistencies that might arise this inadequacy.
from the diverse backgroundsof human curators (in this • LegalBench [23]: The study developed benchmarks for
context, the ChatGPT model’s training data), nor did it legal reasoning without explicitly addressing or adapt-
attempt to mitigate the influence of such diversity on the ing to diverse cultural and legal norms across different
benchmark’s development and evaluation. jurisdictions, which could lead to standardized answers
• PromptBench[5]:Thestudydidnotaddressthediversity that do not accommodate the variety of legal, cultural,
of human curators and evaluators, focusing instead on and ideological perspectives worldwide. The study did
generating adversarial prompts across multiple levels to not acknowledge or attempt to address this form of
test LLMs’ robustness, which did not directly relate to benchmark inadequacy.
or account for the variability in human interpretation • FLUE [24]: The study introduced benchmarks in the fi-
and judgment based on cultural, religious, political, and nancialdomainwithoutdetailedconsiderationofcultural
academic or commercial backgrounds. and ideological diversity in its standardized evaluation
• AgentBench [33]: The study implemented AgentBench, metrics, which could conflict with values like diversity
a benchmark that evaluated LLMs across various envi- and inclusivity, particularly in tasks such as sentiment
ronments without explicitly addressing or attempting to analysis and news classification that inherently required
mitigate the diversity in human curators and evaluators, culturalsensitivity.The studydidnotexplicitlyacknowl-
which could lead to inconsistencies in benchmark devel- edge or address this benchmark inadequacy.
opmentand evaluationdue to the subjectivity introduced • MultiMedQA [8]: The study acknowledged the com-
by human involvement. plexity of accurately representing diverse cultural and
• APIBank [34]: The study utilized human annotators for ideologicalnormsinbenchmarkingLLMsforclinicalap-
dialogueannotation,whichinherentlyintroducedthevari- plications,butdidnotprovidearesolvedmethodologyfor
ability in cultural, religious, political, and academic or integratingtheseaspectsintothebenchmarks.Thestudy’s
commercial backgrounds of these humans. This process focus on creating a diverse benchmark, MultiMedQA,
led to inconsistencies in the benchmarks’ development aimedtoevaluateLLMsacrossvariousmedicalquestion-
and their evaluation.Althoughthe study aimed to ensure answering datasets, including those involving consumer
the quality of annotations through discussion and review medicalquestions,yetit didnotspecificallyaddresshow
by multiple annotators,it did not specifically address the to incorporateorevaluatethe pluralistic natureof human
potential bias and inconsistency arising from the diverse beliefsandvaluesdirectlywithinthebenchmark’sdesign
backgroundsof these human evaluators. or evaluation criteria.
• C-Eval [35]: The study involved human validation in • M3KE [7]: The study acknowledged the challenge of
the development of C-EVAL, indicating subjectivity and integrating a broad spectrum of viewpoints by cover-
potential inconsistency due to the diverse backgrounds ing a wide variety of subjects including humanities,
of the validators, yet did not address this aspect of politics, law, and religion, aiming for inclusivity and
benchmark inadequacy directly. cultural sensitivity in its benchmarks. However, it did
• HaluEval [37]: The study involved human labelers in notexplicitlyaddressthemethodofreconcilingdivergent
annotatinghallucinationsin LLM responses, indicatinga beliefs and values, nor did it mention any attempts to
relianceondiversehumanevaluatorswhichcanintroduce adjustthestandardizedanswersorrubricsforculturaland
variability in benchmark outcomes due to their different ideologicaldiversity,leaving this inadequacyunresolved.
backgrounds.Thestudyacknowledgedtheroleofhuman • Chain-of-ThoughtHub [26]: The study focused on eval-
annotators,butdidnotaddressthepotentialinconsistency uating the reasoningcapabilities of LLMs across various
their diversity might introduce in evaluating LLMs. benchmarks,withnospecificmentionoremphasisonad-
dressingtheintegrationofdiverseculturalandideological
norms in the benchmarks used. The benchmarks aimed
B. Diverse Cultural, Social, Political, Religious and Ideolog-
to assess LLMs’ reasoning abilities without adequately
ical Norms
considering the diversity of cultural, religious, and ideo-
Prevalence: 18/23 logicalperspectivesthatcouldinfluencetheinterpretation
• MMLU[22]:Thebenchmark,bydesign,evaluatedLLMs of questions or the appropriateness of answers. This
acrossabroadspectrumofsubjectsincludingethics,law, oversight implied that the benchmarks might not fully
and societal norms, without specifically addressing or capture or respect the pluralistic nature of human values
compensatingforthediversityofculturalandideological andbeliefs,potentiallyleadingtobiasedornon-inclusive
perspectives. Their approach might not fully capture the evaluations of LLM capabilities. The study did not ex-
pluralistic nature of humanbeliefs and values, especially plicitly acknowledge this limitation nor did it describe
in tasks involving ethical decision-making and societal efforts to address the integration of diverse perspectives27
into the benchmarking process. broad spectrum of human beliefs and values. However,
• KoLA [9]: The KoLA benchmark, designed to evaluate it did not explicitly mention efforts to address or resolve
the knowledge-oriented capabilities of LLMs, incorpo- the benchmarkinadequacyof failing to integrate a broad
rates both known and evolving data sources, including spectrum of cultural and ideological viewpoints through
Wikipediaandnewlypublishedarticles,to assess LLMs’ standardized answers or rubrics.
understanding and application of knowledge. However, • HELM[31]:The study’sextensiveevaluationframework
it did not explicitly address the integration of diverse incorporated metrics such as fairness, bias, and toxi-
cultural and ideological norms in its evaluation criteria, city across a diverse set of scenarios, including mul-
which could lead to standardized answers or rubrics tiple languages and cultural contexts, aiming to mea-
that may not fully represent the pluralistic nature of sureLLMs’performanceholistically.However,thestudy
human beliefs and values. The study acknowledged the acknowledgeditslimitationsinfullycapturingtheplural-
challenges in ensuring fairness and reducing biases in istic nature of human beliefs and values, due to inherent
LLM evaluations,butdidnotspecifyattemptsto address challengesinstandardizingbenchmarksthatcouldrespect
thecomplexitiesofdiverseculturalandideologicalnorms andrepresentabroadspectrumofculturalandideological
directly. norms.
• SciBench [27]: The study demonstrated a reliance • PromptBench [5]: The study focused on creating adver-
on standardized approaches for benchmarking LLMs sarialpromptstotestLLMrobustnessbutdidnotaddress
throughcomplexscientificproblem-solvingtaskswithout the integration of diverse cultural and ideologicalnorms,
explicitlyaddressingorincorporatingdiverseculturaland which could affect standardized answers’ validity across
ideological considerations into the evaluation process or different cultures and ideologies. The benchmarks were
the construction of benchmarks. The benchmarks were designed without consideration for diverse cultural and
designed withoutacknowledgingthe pluralistic nature of ideological perspectives, which could lead to biases or
human beliefs and values, especially in scenarios requir- skewed results not reflective of varied human values.
ing ethical decision-making or societal appropriateness, • AgentBench[33]:The AgentBenchframework,designed
which could lead to biases or skewed results that did not to evaluateLLMs asagentsacrossvariousenvironments,
reflectthevariedvaluespresentindifferentsocieties.The didnotexplicitlyaddresstheintegrationofabroadspec-
benchmarkstudydidnotacknowledgethisinadequacyof trumofreligious,ideological,legal,political,andcultural
LLM benchmarks, nor made attempts to address it. viewpoints in its benchmarking process. This omission
• ARB [28]: The ARB benchmark prioritized problems suggested a potential for standardizedanswers or rubrics
from domains requiring high-level reasoning without that might not fully respect or represent the pluralistic
specifically addressing the diverse cultural and ideolog- nature of human beliefs and values, especially in tasks
ical norms challenge; it focused on evaluating LLMs’ requiring ethical decision-making, societal norms, and
expertreasoninginquantitativesubjectsandlaw,without cultural interpretations. The study did not mention any
mention of integrating diverse perspectives or the poten- acknowledgment or attempts to address this form of
tial for standard answers to conflict with varied societal inadequacy in its benchmark design.
values.Thestudydidnotdiscussattemptstoaddressthis • APIBank [34]: The study designed a benchmark that
benchmark inadequacy. relies on standardized answers or rubrics for evaluating
• Xiezhi [29]: The study acknowledged this benchmark LLMs using API calls without explicitly addressing the
inadequacybycreatingsubsetsofquestionslesssensitive incorporation of diverse cultural and ideological view-
and less China-centric, aiming for a more balanced and points, which could lead to biases or skewed results not
culturallyinclusiveassessment,yetitdidnotfullyresolve reflective of varied human values. The benchmark did
the challenge of integrating a broad spectrum of cultural not acknowledge or attempt to address this particular
and ideological norms. inadequacy.
• BIG-Bench[4]:Thebenchmarkstudyincludedtaskssuch • C-Eval[35]:Thestudyexplicitlyaddressedthechallenge
as “social reasoning”, “emotional understanding”, and of incorporating a wide range of cultural and ideologi-
“figurativelanguage”,whichinherentlyrequiredinterpre- cal perspectives within its benchmarks, recognizing the
tations that could vary significantly across cultures, yet diversity inherent in Chinese society and the subjects
the study did not explicitly address how these diverse covered, which included humanities and social sciences
interpretations were considered or integrated into the among others. The benchmark’s design reflected an un-
benchmarking process. This omission indicated that the derstanding that standardized answers might not capture
benchmarkmightnothave fullyaccountedfor or respect the fullspectrum of human diversity,evidentin its effort
the broad spectrum of human diversity in cultural and toincludediversedisciplinesanddifficultylevelstailored
ideological norms. to the Simplified Chinese context, thus acknowledging
• AGIEval [10]: The study addressed the challenge of di- thecomplexityofculturalandideologicalnorms,without
verseculturalandideologicalnormsbyevaluatingLLMs directly stating attempts to resolve this inadequacy.
using benchmarks derived from high-standard, official • HaluEval[37]: The studyimplementeda benchmarkthat
human-centricexamsacrossvariousdomains,andinboth did not adequately consider the diverse cultural and ide-
English and Simplified Chinese, aiming to capture a ological norms in its evaluation of LLM hallucinations,28
which could inherently reflect biases or cultural insensi-
tivities due to the standardized nature of its evaluation
criteria. The study did not indicate any acknowledgment
of this benchmark inadequacynor attempts to address it.