GES : Generalized Exponential Splatting for Efficient Radiance Field Rendering
AbdullahHamdi1 LukeMelas-Kyriazi1 GuochengQian2,4 JinjieMai2
RuoshiLiu3 CarlVondrick3 BernardGhanem2 AndreaVedaldi1
1VisualGeometryGroup,UniversityofOxford
2KingAbdullahUniversityofScienceandTechnology(KAUST)
3ColumbiaUniversity 4SnapInc.
abdullah.hamdi@eng.ox.ac.uk
Abstract
PSNR Memory Speed
Advancements in 3D Gaussian Splatting have signifi-
cantlyaccelerated3Dreconstructionandgeneration.How- Gaussian 676MB
ever, it may require a large number of Gaussians, which Splatting
creates a substantial memory footprint. This paper intro- 29.41db
137 FPS
duces GES (Generalized Exponential Splatting), a novel
representationthatemploysGeneralizedExponentialFunc-
tion (GEF) to model 3D scenes, requiring far fewer parti- GES 399MB
clestorepresentasceneandthussignificantlyoutperform-
ing Gaussian Splatting methods in efficiency with a plug- 29.68db 160 FPS
and-play replacement ability for Gaussian-based utilities.
GESisvalidatedtheoreticallyandempiricallyinbothprin- Figure1. GES:GeneralizedExponentialSplattingWepropose
cipled1Dsetupandrealistic3Dscenes. Itisshowntorep- afasterandmorememory-efficientalternativetoGaussianSplat-
ting [27] that relies on Generalized exponential Functions (with
resentsignalswithsharpedgesmoreaccurately,whichare
additionallearnableshapeparameters)insteadofGaussians.
typically challenging for Gaussians due to their inherent
low-pass characteristics. Our empirical analysis demon-
strates that GEF outperforms Gaussians in fitting natural-
occurring signals (e.g. squares, triangles, parabolic sig- mixtureofsmall,colouredGaussians. Itskeyadvantageis
nals), thereby reducing the need for extensive splitting op- the existence of a very fast differentiable renderer, which
erations that increase the memory footprint of Gaussian makesthisrepresentationideallysuitedforreal-timeappli-
Splatting. With the aid of a frequency-modulated loss, cationsandsignificantlyreducesthelearningcost. Specif-
GES achieves competitive performance in novel-view syn- ically, fast rendering of learnable 3D representations is of
thesis benchmarks while requiring less than half the mem- key importance for applications like gaming, where high-
ory storage of Gaussian Splatting and increasing the ren- quality,fluid,andresponsivegraphicsareessential.
dering speed by up to 39%. The code is available on the However,GSisnotwithoutshortcomings. Wenoticein
project website https://abdullahamdi.com/ges. particular that GS implicitly makes an assumption on the
natureofthemodeledsignals,whichissuboptimal. Specif-
ically, Gaussians correspond to low-pass filters, but most
3Dscenesarefarfromlow-passastheycontainabruptdis-
1.Introduction continuities in shape and appearance. Fig.2 demosntrates
Thepursuitofmoreengagingandimmersivevirtualex- this inherent low-pass limitation of Gaussian-based meth-
periences across gaming, cinema, and the metaverse de- ods. As a result, GS needs to use a huge number of very
mandsadvancementsin3Dtechnologiesthatbalancevisual smallGaussianstorepresentsuch3Dscenes,farmorethan
richness with computational efficiency. In this regard, 3D if a more appropriate basis was selected, which negatively
GaussianSplatting(GS)[27]isarecentalternativetoneural impactsmemoryutilization.
radiancefields[17,40,44,45,51,81]forlearningandrender- To address this shortcoming, in this work, we intro-
ing3Dobjectsandscenes. GSrepresentsasceneasalarge duce GES (Generalized Exponential Splatting), a new ap-
1
4202
beF
51
]VC.sc[
1v82101.2042:viXraTime-Domain Signals Frequency-Domain (Fourier Transforms)
1.0
Square Signal 1.5 Fourier of Square
0.8 Triangle Signal Fourier of Triangle
Gaussian Signal Fourier of Gaussian
0.6 1.0
0.4
0.5
0.2
0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Figure2. TheInherentLow-PassLimitationofGaussians. WeillustratethebandwidthconstraintofGaussianfunctionscomparedto
squareandtrianglesignals.TheGaussianfunctions’low-passpropertyrestrictstheirabilitytofitsignalswithsharpedgesthathaveinfinite
bandwidth.Thislimitationconstitutesachallengefor3DGaussianSplatting[27]inaccuratelyfittinghigh-bandwidth3Dspatialdata.
proach that utilizes the Generalized Exponential Function its 2D RGB images captured from different camera po-
(GEF) for modeling 3D scenes (Fig.1). Our method is sitions [1, 16]. Classical approaches usually recover a
designed to effectively represent signals, especially those scene’s geometry as a point cloud using SIFT-based [39]
withsharpfeatures,whichpreviousGaussiansplattingtech- point matching [61,63]. More recent methods enhance
niques often smooth out or require extensive splitting to them by relying on neural networks for feature extraction
model [27]. Demonstrated in Fig.3, we show that while (e.g. [22,75,76,83]). The development of Neural Radi-
N = 5 randomly initialized Gaussians are required to fit ance Fields (NeRF) [37,44] has prompted a shift towards
asquare,only2GEFsareneededforthesamesignal. This reconstructing 3D as volume radiance [66], enabling the
stemsfromthefactthatGaussianmixtureshavealow-pass synthesis of photo-realistic novel views [4,5,69]. Subse-
frequency domain, while many common signals, like the quent works have also explored the optimization of NeRF
square,arenotband-limited.Thishigh-bandmodelingcon- in few-shot (e.g. [15,23,28]) and one-shot (e.g. [7,82])
stitutes a fundamental challenge to Gaussian-based meth- settings. NeRF does not store any 3D geometry explic-
ods. TohelpGEStotraingraduallyfromlow-frequencyto itly (only the density field), and several works propose to
high-frequencydetails,weproposeaspecializedfrequency- use a signed distance function to recover a scene’s sur-
modulated image loss. This allows GES to achieve more face [12,33,34,71,72,77,78], including in the few-shot
than50%reductioninthememoryrequirementofGaussian settingaswell(e.g.[84,85]).
splatting and up to 39% increase in rendering speed while Differentiable rendering. Gaussian Splatting is a point-
maintaining a competitive performance on standard novel based rendering [2,19] algorithm that parameterizes 3D
viewsynthesisbenchmarks. pointsasGaussianfunctions(mean,variance,opacity)with
Wesummarizeourcontributionsasfollows: spherical harmonic coefficients for the angular radiance
• We present principled numerical simulations motivating component[80]. Priorworkshaveextensivelystudieddif-
theuseoftheGeneralizedExponentialFunctions(GEF) ferentiablerasterization,withaseriesofworks[26,36,38]
insteadofGaussiansforscenemodeling. proposingtechniquestodefineadifferentiablefunctionbe-
tweentrianglesinatrianglemeshandpixels,whichallows
• We propose Generalized Exponential Splatting (GES), foradjustingparametersoftrianglemeshfromobservation.
a novel 3D representation that leverages GEF to de- Theseworksrangefromproposingadifferentiablerenderer
velop a splatting-based method for realistic, real-time, formeshprocessingwithimagefilters [32],andproposing
andmemory-efficientnovelviewsynthesis. toblendschemesofnearbytriangles [48],toextendingdif-
ferentiable rasterization to large-scale indoor scenes [79].
• Equipped with a specialized frequency-modulated im-
Onthepoint-basedrendering[19]side, neuralpoint-based
age loss and through extensive experiments on standard
rendering[26]allowsfeaturestobelearnedandstoredin3D
benchmarksonnovelviewsynthesis,GESshowsa50%
pointsforgeometricalandtexturalinformation. Wilesetal.
reductioninmemoryrequirementandupto39%increase
combine neural point-based rendering with an adversarial
inrenderingspeedforreal-timeradiancefieldrendering
loss for better photorealism [73], whereas later works use
basedonGaussianSplatting. GEScanactasaplug-and-
points to represent a radiance field, combining NeRF and
playreplacementforanyGaussian-basedutilities.
point-based rendering [74,86]. Our GES is a point-based
rasterizerinwhicheverypointrepresentsageneralizedex-
2.Relatedwork
ponential with scale, opacity, and shape, affecting the ras-
Multi-view 3D reconstruction. Multi-view 3D recon- terizationaccordingly.
struction aims to recover the 3D structure of a scene from Prior-based3Dreconstruction. Modernzero-shottext-to-
2
edutilpmA edutingaM1.0 = 0.5
= 1 True square 1.0 True square
0.8 = 1.5 1.0 Gaussian Mixture GEF Mixture
= 2 0.8
= 3 0.8
0.6 = 10
0.6 0.6
0.4
0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
4 2 0 2 4 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x
(a)AfamilyofGEFsf (x) (b)FiveGaussiansfittingasquare (c)TwoGEFsfittingasquare
β
Figure3.GeneralizedExponentialFunction(GEF).(a):WeshowafamilyofGEFsf
β(x)=Ae−(cid:16)|x− αµ|(cid:17)β
withdifferentβvaluesfor
α=1,µ=0. Whenβ =2,thefunctionreducestotheGaussianfunctionfollowedin3Dgaussiansplatting[27]. InourGES,welearn
βasanotherparameterofeachsplattingcomponent.(b,c):TheproposedGEFmixture,withlearnableβ,fitsthesamesignal(square)with
fewercomponentscomparedtoGaussianfunctionsusinggradient-basedoptimizations. (b): Weshowanexampleofthefittedmixture
withN =5componentswhenGaussiansareusedvs.(c)whenGEFisusedwithN =2components.GEFachieveslesserrorloss(0.44)
andapproximatessharpedgesbetterthantheGaussiancounterpart(0.48error)withlessnumberofcomponents.Theoptimizedindividual
components(initializedwithrandomparameters)areshowningreenafterconvergence.
imagegenerators[3,18,55,56,59,60]haveimprovedthere- GEF becomes a scaled Gaussian f(x|µ,α,β = 2,A) =
sultsbyprovidingstrongersynthesispriors[8,11,42,50,70]. Ae− 21(cid:16) αx /−√µ 2(cid:17)2
. The GEF, therefore, provides a versatile
DreamFusion [50] is a seminal work that proposed to dis-
framework for modeling a wide range of data by varying
tillanoff-the-shelfdiffusionmodel[60]intoaNeRF[5,44]
β, unlike the Gaussian mixtures, which have a low-pass
for a given text query. It sparked numerous follow-up ap-
frequency domain. Many common signals, like the square
proaches for text-to-3D synthesis (e.g. [9,30]) and image-
ortriangle, areband-unlimited, constitutingafundamental
to-3D reconstruction (e.g. [13,35,41,64]). The latter is
challengetoGaussian-basedmethods. Inthispaper,wetry
achievedviaadditionalreconstructionlossesonthefrontal
to learn a positive β for every component of the Gaussian
camera position [35] and/or subject-driven diffusion guid-
splattingtoallowforageneralized3Drepresentation.
ance[30,54]. Thedevelopedmethodsimprovedtheunder-
TheoreticalResults. Despiteitsgeneralizablecapabilities,
lying 3D representation [9,30,67] and 3D consistency of
the behavior of the GEF cannot be easily studied analyti-
the supervision [35,65]; explored task-specific priors [21,
cally,asitinvolvescomplexintegralsofexponentialswith-
24,58]andadditionalcontrols[43]. Lately,Gaussian-based
outclosedformthatdependontheshapeparameterβ. We
methods [68] improved the speed of optimization of 3D
demonstrateinTheorem1intheAppendixthatforspecific
generation,utilizingthefastrasterizationofGaussianSplat-
cases, such as for a square signal, the GEF can achieve a
ting. WeshowcasehowourGEScanactasaplug-and-play
strictlysmallerapproximationerrorthanthecorresponding
replacement for Gaussian Splatting in this application and
Gaussian function by properly choosing β. The proof ex-
otherutilities.
ploits the symmetry of the square wave signal to simplify
3.PropertiesofGeneralizedExponentials the error calculations. Theorem 1 provides a theoretical
foundation for preferring the GEF over standard Gaussian
3.1.GeneralizedExponentialFunction
functionsinourGESrepresentationinsteadof3DGaussian
Preliminaries. The Generalized Exponential Function Splatting[27].
(GEF) is similar to the probability density function (PDF)
of the Generalized Normal Distribution (GND) [14]. This 3.2.Assessing1DGEFMixturesinSimulation
function allows for a more flexible adaptation to various
We evaluate the effectiveness of a mixture of GEFs
data shapes by adjusting the shape parameter β ∈ (0,∞).
inrepresentingvariousone-dimensional(1D)signaltypes.
TheGEFisgivenby:
This evaluation is conducted by fitting the model to syn-
(cid:32) (cid:18) |x−µ|(cid:19)β(cid:33) thetic signals that replicate characteristics properties of
f(x|µ,α,β,A)=Aexp − (1) common real-world signals. More details and additional
α
simulationresultsareprovidedintheAppendix.
whereµ ∈ Risthelocationparameter, α ∈ Risthescale SimulationSetup.Theexperimentalframeworkwasbased
parameter, A ∈ R+ defines a positive amplitude. The be- on a series of parametric models implemented in PyTorch
haviorofthisfunctionisillustratedinFig.3. Forβ =2,the [47],designedtoapproximate1Dsignalsusingmixturesof
3
)
| x(f y y(a)Squaresignal (b)Parabolicsignal (c)Exponentialsignal
101
Gaussian Gaussian Gaussian
DoG 101 DoG DoG
LoG LoG 103 LoG
102 GEF 102 GEF GEF
103
103 104
104
2 5 8 10 15 20 2 5 8 10 15 20 2 5 8 10 15 20
Number of Components (N) Number of Components (N) Number of Components (N)
(d)Trianglesignal (e)Gaussiansignal (f)Halfsinusoidsignal
102
Gaussian 102 Gaussian Gaussian
102 D Loo GG 103 D Loo GG
103
D Loo GG
GEF 104 GEF GEF
103
105
104
104 106
107
105
2 5 8 10 15 20 2 5 8 10 15 20 2 5 8 10 15 20
Number of Components (N) Number of Components (N) Number of Components (N)
Figure4. NumericalSimulationResultsofDifferentMixtures. Weshowacomparisonofaveragelossfordifferentmixturemodels
optimizedwithgradient-basedoptimizersacrossvaryingnumbersofcomponentsonvarioussignaltypes(a-f).Inthecaseof‘NaN‘loss(
gradientexplosion),theresultsarenotshownontheplots.FullsimulationresultsareprovidedintheAppendix
differentfunctionssuchasGaussian(low-pass),Difference Model Configuration. The models were configured with
of Gaussians (DoG), Laplacian of Gaussian (LoG), and a a varying number of components N, with tests conducted
GEFmixturemodel. Eachmodelcomprisedparametersfor using N = {2,5,8,10,15,20}. The weights of the com-
means, variances (or scales), and weights, with the gener- ponentsarechosentobepositive. Alltheparametersofall
alized model incorporating an additional parameter, β, to the N components were learned. Each model was trained
controltheexponentiationoftheGEFfunction. using the Adam optimizer with a mean squared error loss
Models. In this section, we briefly overview the mixture function. The input x was a linearly spaced tensor repre-
modelsemployedtoapproximatetruesignals. Detailedfor- sentingthedomainofthesyntheticsignal,andthetargety
mulationsareprovidedintheAppendix. wasthevalueofthesignalateachpointinx. Trainingpro-
Gaussian Mixture: This model uses a combination of ceededforapredeterminednumberofepochs,andtheloss
multiple Gaussian functions. Each Gaussian is character- wasrecordedattheendoftraining.
ized by its own mean, variance, and weight. The overall DataGeneration. Synthetic1Dsignalsweregeneratedfor
modelisaweightedsumoftheseGaussianfunctions,which various signal types over a specified range, with a given
isalow-passfilter. data size and signal width. The signals were used as the
Difference of Gaussians (DoG) Mixture: The DoG ground truth for training the mixture models. The ground
model is a variation of the Gaussian mixture. It is formed truth signals used in the experiment are one-dimensional
by taking the difference between pairs of Gaussian func- (1D)functionsthatserveasbenchmarksforevaluatingsig-
tionswithapredefinedvarianceratio. Thismodelispartic- nalprocessingalgorithms.Thesignaltypesunderstudyare:
ularlyeffectiveinhighlightingcontrastsinthesignalandis square, triangle, parabolic, halfsinusoidal, Gaussian, and
consideredaband-passfilter. exponentialfunctions. WeshowFig.3anexampleoffitting
Laplacian of Gaussian (LoG) Mixture: This model aGaussianwhenN = 5andaGeneralizedmixtureonthe
combines the characteristics of a Laplacian of Gaussian squaresignalwhenN = 2. Notehowsharpedgesconsti-
function. Each component in the mixture has specific pa- tuteachallengeforGaussiansthathavelowpassbandwidth
rametersthatcontrolitsshapeandscale. JustliketheDoG, while a square signal has an infinite bandwidth known by
theLoGmodelisadeptatcapturingfinedetailsinthesignal thesincfunction[25].
andisaband-passfilter. SimulationResults. Themodels’performancewasevalu-
Generalized Exponential (GEF) Mixture: A more atedbasedonthelossvalueaftertraining. Additionally,the
flexible version of the Gaussian mixture, this model intro- model’sabilitytorepresenttheinputsignalwasvisuallyin-
duces an additional shape parameter β. By adjusting this spectedthroughgeneratedplots. Multiplerunsperconfigu-
parameter,wecanfine-tunethemodeltobetterfitthechar- rationwereexecutedtoaccountforvarianceintheresults.
acteristics of the signal. The GEF Mixture frequency re- Foracomprehensiveevaluation,eachconfigurationwasrun
sponsedependsontheshapeparameterβ. multiple times (20 runs per configuration) to account for
4
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL
egarevAGroundTruth GES(Ours) Gaussians Mip-NeRF360 InstantNGP
Figure5. VisualComparisononNovelViewSynthesis.Wedisplaycomparisonsbetweenourproposedmethodandestablishedbaselines
alongsidetheirrespectivegroundtruthimages. Thedepictedscenesareorderedasfollows: GARDENandROOMfromtheMip-NeRF360
dataset; DRJOHNSON from the Deep Blending dataset; and TRAIN from Tanks&Temples. Subtle differences in rendering quality are
accentuatedthroughzoomed-indetails.ThesespecificsceneswerepickedsimilarlytoGaussinSplatting[27]forafaircomparison.Itmight
bedifficultingeneraltoseedifferencesbetweenGESandGaussiansbecausetheyhavealmostthesamePSNR(despiteGESrequiring
50%lessmemory).
variability in the training process. During these runs, the establishedbenchmarksfornovelviewsynthesis.
number of instances where the training resulted in a ’nan’
4.1.DifferentiableGESFormulation
losswasremovedfromthelossplots,andhencesomeplots
inFig.4donothavelossvaluesatsomeN. Asdepictedin Ourobjectiveistoenhancenovelviewsynthesiswitha
Fig.4,theGEFMixtureconsistentlyyieldedthelowestloss refinedscenerepresentation. Weleverageageneralizedex-
across the number of components, indicating its effective ponentialform,heretermedGeneralizedExponentialSplat-
approximation of many common signals, especially band- ting,whichforlocationxin3Dspaceandapositivedefinite
unlimitedsignalslikethesquareandtriangle. Theonlyex- matrixΣ,isdefinedby:
ception is the Gaussian signal, which is (obviously) fitted
(cid:26) (cid:27)
betterwithaGaussianMixture.
L(x;µ,Σ,β)=exp
−1(cid:0) (x−µ)⊺ Σ−1(x−µ)(cid:1)β
2 ,
2
(2)
4.GeneralizedExponentialSplatting(GES)
whereµisthelocationparameterandΣisthecovariance
Having established the benefits of GEF of Eq.(1) over matrixequivalanceinGaussianSplatting[27]. β isashape
Gaussian functions, we will now demonstrate how to ex- parameter that controls the sharpness of the splat. When
tendGEFintotheGeneralizedExponentialSplatting(GES) β = 2, this formulation is equivalent to Gaussian splat-
framework,offeringaplug-and-playreplacementforGaus- ting [27]. Our approach maintains an opacity measure κ
sianSplatting. Wealsostartwithacollectionofstaticim- for blending and utilizes spherical harmonics for coloring,
agesofasceneandtheircorrespondingcameracalibrations similartoGaussiansplatting[27].
obtainedthroughStructurefromMotion(SfM)[62],which For 2D image projection, we adapt the technique by
additionallyprovidesasparsepointcloud. Movingbeyond Zwickeretal.[88],butkeeptrackofourvariableexponent
Gaussianmodels[27], GESadoptsanexponentβ totailor β. The camera-space covariance matrix Σ′ is transformed
the focus of the splats, thus sharpening the delineation of asfollows: Σ′ = JWΣW⊺J⊺,whereJistheJacobianof
scene edges. This technique is not only more efficient in the transformation from world to camera space, and W is
memory usage but also can surpass Gaussian splatting in adiagonalmatrixcontainingtheinversesquarerootofthe
5eigenvalues of Σ. We ensure Σ remains positively semi-
r
definite throughout the optimization by formulating it as a
product of a scaling matrix S (modified by some positive
modification function ϕ(β) > 0 as we show later) and a
rotation matrix R, with optimization of these components
facilitated through separate 3D scale vectors s and quater-
Center
nionrotationsq.
4.2. Fast Differentiable Rasterizer for Generalized
ExponentialSplats
Intuition from Volume Rendering. The concept of vol-
umerenderinginthecontextofneuralradiancefields[44] Original (𝛽=2) Modified (𝛽>2)
involves the integration of emitted radiance along a ray
∧
Effective projected variance (𝒂)
passing through a scene. The integral equation for the ex-
pectedcolorC(r)ofacamerarayr(t)=o+td,withnear
Figure6. EffectiveVarianceofGEScomponents. Wedemon-
andfarboundst andt ,respectively,isgivenby:
n f stratetheconceptofeffectivevarianceprojectionα(β)foranin-
(cid:98)
dividual splatting component intersecting a camera ray r under
(cid:90) tf
shapemodification(β >2).Notethatα(β)isascaledversionof
C(r)= T(t)κ(r(t))c(r(t),d)dt, (cid:98)
theoriginalsplatprojectedvarianceα.
tn
(3)
(cid:18) (cid:90) t (cid:19)
where T(t)=exp − κ(r(s))ds .
thevarianceofthegeneralizedexponentialdistributionand
tn
thevarianceoftheGaussiandistributionisgivenby[14]as
Here,T(t)representsthetransmittancealongtherayfrom
Γ(3/β)
t n tot, κ(r(t))isthevolumedensity, andc(r(t),d)isthe ϕ(β)= (5)
Γ(1/β)
emittedradianceatpointr(t)inthedirectiond. Thetotal
distance[t ,t ]crossedbytherayacrossnon-emptyspace ,whereΓistheGammafunction. ThisconversioninEq.(5)
n f
dictatestheamountoflostenergyandhencethereduction ensures the PDF integrates to 1. In a similar manner, the
of the intensity of the rendered colors. In the Gaussian integralsinEq.(3)underEq.(4)canbeshowntobeequiv-
Splatting world [27], this distance [t ,t ] is composed of alent for Gaussians and GES using the same modification
n f
theprojectedvariancesαofeachcomponentalongtheray ofEq.(5).Themodificationwillaffecttherasterizationasif
directiono+td. InourGESofEq.(2),iftheshapeparam- wedidperformtheexponentchange.Itisatrickthatallows
eterβ ofsomeindividualcomponentchanges,theeffective using generalized exponential rasterization without taking
impact on Eq.(3) will be determined by the effective vari- the β exponent. Similarly, the Gaussian splatting [27] is
ance projection α of the same component modified by the not learning rigid Gaussians, it learns properties of point
(cid:98)
modifcationfunctionϕ(β)asfollows: cloudsthatactasif thereareGaussiansplacedtherewhen
they splat on the image plane. Both our GES and Gaus-
α (cid:98)(β)=ϕ(β)α . (4) siansareinthesamespiritofsplatting,andrepresenting3D
with splat properties. Fig.6 demonstrates this concept for
Note that the modification function ϕ we chose does not anindividualsplattingcomponentintersectingarayrfrom
dependontheraydirectionsincetheshapeparameterβisa thecameraandtheideaofeffectivevarianceprojectionα.
(cid:98)
globalpropertyofthesplattingcomponent,andweassume However, as can be in Fig.6, this scaler modification ϕ(β)
thescenetocomprisemanycomponents.Wetacklenextthe introducessomeview-dependentboundaryeffecterror(e.g.
choiceofthemodificationfunctionϕandhowitfitsintothe if the ray r passed on the diagonal). We provide an upper
rasterizationframeworkofGaussianSplatting[27]. boundestimateonthiserrorintheAppendix.
ApproximateRasterization. Themainquestionishowto DuetotheinstabilityoftheΓfunctioninEq.(5),wecan
representtheGESintherasterizationframework. Ineffect, approximateϕ(β)withthefollowingsmoothfunction.
the rasterization in Gaussian Splatting [27] only relies on
2
thevariancesplatsofeachcomponent. So,weonlyneedto ϕ¯ (β)= . (6)
ρ 1+e−(ρβ−2ρ)
simulatetheeffectoftheshapeparameterβ onthecovari-
anceofeachcomponenttogettherasterizationofGES.To The difference between the exact modification ϕ(β) and
do that, we modify the scales matrix of the covariance in the approximate ϕ¯ (β) ( controlled by the hyperparameter
ρ
eachcomponentbythescalerfunctionϕ(β)ofthatcompo- shape strength ρ ) is shown in Fig.7. At β = 2 (Gaus-
nent.Fromprobabilitytheory,theexactconversionbetween sian shape), the modifications ϕ and ϕ¯are exactly 1. This
6normalizedfrequenciesω (ω = 0%forlowfrequenciesto
2.00 Exact ω =100%forhighfrequencies).Wechoseσ =0.1+10ω
2
1.75 Approximate ( =0.1)
to ensure the stability of the filter and reasonable result- Approximate ( =0.5)
1.50 Approximate ( =1.0) ing masks. The filtered image is then used to generate an
1.25 Approximate ( =10) edge-awaremaskM throughapixel-wisecomparisontoa
ω
1.00 thresholdvalue(afternormalization)asfollows.
0.75
M
=1(cid:0)
DoG (I ) >ϵ
(cid:1)
,
0.50 ω ω gt normalized ω
(7)
0.25 DoG ω(I)=G(I,0.2+20ω)−G(I,0.1+10ω)
0.00
,where0≤ϵ ≤1isthethreshold(wepick0.5)foranor-
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 ω
values malizedresponseofthefilterDoG ,I isthegroundtruth
ω gt
image, and 1 is the indicator function. See Fig.8 for ex-
Figure7.TheModificationFunctionϕ(β).Weshowdifferentρ
shapestrengthvaluesoftheapproximatefunctionsϕ¯ (β)inEq.(6) amplesofthemasks.Theedge-awarefrequency-modulated
ρ
lossL isdefinedas:
andtheexactmodificationfunctionϕ(β)inEq.(5). Atβ = 2( ω
gaussiansplats), allfunctionshaveavariancemodificationof1,
L =∥(I−I )·M ∥ , (8)
and GES reduces to Gaussian Splatting. In the extreme case of ω gt ω 1
ρ=0,GESreducestoGaussianSplattingforanyβ.
whereIisthereconstructedimage,and∥·∥ denotestheL1
1
norm. Thistermisintegratedintotheoverallloss,asshown
parameterization ϕ¯ (β) ensures that the variance of each later. Themaskistargetedforthespecifiedfrequenciesω.
ρ
Weusealinearscheduletodeterminethesetargetωvalues
componentremainspositive.
inEq.(8)andEq.(7)duringtheoptimizationofGES,ω =
4.3.Frequency-ModulatedImageLoss currentiteration. ThelossL aimstohelpintuningtheshape
totaliterations ω
β based on the nature of the scene. It does so by focusing
To effectively utilize the broad-spectrum capabilities of
the GES components on low pass signals first during the
GES,ithasbeenenhancedwithafrequency-modulatedim-
training before focusing on high frequency with tuning β
ageloss,denotedasL . Thislossisgroundedintheratio-
ω
fromtheirinitialvalues. ThishelpstheefficiencyofGESas
nalethatGES,initiallyconfiguredwithGaussianlow-pass
can be seen later in Table 6 (almost free 9% reduction in
bandsplats,shouldprimarilyconcentrateonlow-frequency
memory).
details during the initial stages of training. As training
Due to DoG filter sensitivity for high-frequencies, the
advances, with the splat formations adapting to encapsu-
maskfor0% < ω ≤ 50%isdefinedas1−M of50% <
latehigherfrequencies,theoptimization’semphasisshould ω
ω ≤ 100%. This ensures that all parts of the image will
graduallyshifttowardsthesehigherfrequencybandswithin
becoveredbyoneofthemasksM ,whilefocusingonthe
the image. This concept bears a technical resemblance to ω
detailsmoreastheoptimizationprogresses.
the frequency modulation approach used in BARF [31],
albeit applied within the image domain rather than the 4.4. Optimization of the Generalized Exponential
3D coordinate space. The loss is guided by a frequency- Splats
conditioned mask implemented via a Difference of Gaus-
sians (DoG) filter to enhance edge-aware optimization in We detail a novel approach for controlling shape den-
image reconstruction tasks modulated by the normalized sity,whichselectivelyprunesGESaccordingtotheirshape
frequencyω. TheDoGfilteractsasaband-passfilter,em- attributes, thus eliminating the need for a variable density
phasizingtheedgesbysubtractingablurredversionofthe mechanism. Thisoptimizationstrategyencompassestheβ
image from another less blurred version, thus approximat- parameter as well as the splat’s position x, opacity κ, co-
ingthesecondspatialderivativeoftheimage. Thisopera- variancematrixΣ,andcolorrepresentationthroughspher-
tionismathematicallyrepresentedas: icalharmonicscoefficients[27]. Optimizationoftheseele-
ments is conducted using stochastic gradient descent, with
DoG(I)=G(I,σ )−G(I,σ ), 0<σ <σ theprocessacceleratedbyGPU-poweredcomputationand
1 2 2 1
specializedCUDAkernels.
where G(I,σ) denotes the Gaussian blur operation on im-
StartingestimatesforΣandxarededucedfromtheSfM
ageIwithstandarddeviationσ.Thechoiceofσvaluesdic-
points, while all β values are initialized with β = 2 (pure
tatesthescaleofedgestobehighlighted,effectivelydeter-
Gaussianspalts). ThelossfunctionintegratesanL metric
1
miningthefrequencybandofthefilter. Wechoseσ =2σ
1 2 combined with a structural similarity loss (SSIM), and the
to ensure the validity of the band-pass filter, where the
frequency-modulatedlossL :
ω
choice of σ will determine the target frequency band of
2
the filter. In our formulation, we use predetermined target L=λ L +λ L +λ L , (9)
L1 1 ssim ssim ω ω
7
ecnairaV
dezilareneGFigure8.Frequency-ModulatedImageMasks.Fortheinputexampleimageontheleft,Weshowexamplesofthefrequencylossmasks
M used in Sec.4.3 for different numbers of target normalized frequencies ω ( ω = 0% for low frequencies to ω = 100% for high
ω
frequencies). This masked loss helps our GES learn specific bands of frequencies. We use a linear schedule to determine these target
ω valuesduringtheoptimizationofGES,ω = currentiteration. NotethatduetoDoGfiltersensitivityforhigh-frequencies, themaskfor
totaliterations
0<ω≤50%isdefinedas1−M of50<ω≤100%.ThisensuresthatallpartsoftheimagewillbecoveredbyoneofthemasksM ,
ω ω
whilefocusingonthedetailsmoreastheoptimizationprogresses.
where λ = 0.2 is applied uniformly in all evaluations, parameter was set at 0.0015, with a shape reset interval of
ssim
andλ =1−λ −λ . Expandeddetailsonthelearning 1000 iterations and a shape pruning interval of 100 itera-
L1 ssim ω
algorithmandotherspecificproceduralelementsareavail- tions. The threshold for pruning based on shape was set
ableintheAppendix. at 0.5, while the shape strength parameter was determined
to be 0.1, offering a balance between accuracy and com-
5.Experiments putational load. Additionally, the Image Laplacian scale
5.1.DatasetsandMetrics factorwassetat0.2, withthecorrespondingλ ω frequency
losscoefficientmarkedat0.5,ensuringedge-enhancedop-
In our experiments, we utilized a diverse range of
timizationinourimagereconstructiontasks. Theotherhy-
datasets to test the effectiveness of our algorithm in ren-
perparametersanddesignchoices(likeopacitysplittingand
deringreal-worldscenes. Thisevaluationencompassed13
pruning)sharedwithGaussiansplitting[27]werekeptthe
real scenes from various sources. We particularly focused
same. MoredetailsareprovidedintheAppendix.
onscenesfromtheMip-Nerf360dataset[5],renownedfor
itssuperiorNeRFrenderingquality,alongsideselectscenes
6.Results
fromtheTanks&Templesdataset[29],andinstancespro-
6.1.NovelViewSynthesisResults
videdbyHedmanetal.[20]fortheirworkinDeepBlend-
ing. Thesescenespresentedawidearrayofcapturestyles, WeevaluatedGES againstseveralstate-of-the-arttech-
ranging from bounded indoor settings to expansive un- niques in both novel view synthesis tasks. Table 1 encap-
boundedoutdoorenvironments. sulatethecomparativeresultsinadditiontoFig.5. Table1
ThequalitybenchmarkinourstudywassetbytheMip- demonstratesthatGES achievesabalancebetweenhighfi-
Nerf360 [4], which we compared against other contem- delity and efficiency in novel view synthesis. Although it
porary fast NeRF methods, such as InstantNGP [45] and does not always surpass other methods in SSIM or PSNR,
Plenoxels. Our train/test split followed the methodology it significantly excels in memory usage and speed. With
recommendedbyMip-NeRF360,usingevery8thphotofor only 377MB of memory and a processing speed of 2 min-
testing.Thisapproachfacilitatedconsistentandmeaningful utes,GES standsoutasahighlyefficientmethod,particu-
errormetriccomparisons,includingstandardmeasuressuch larlywhencomparedtothe3DGaussians-30KandInstant
asPSNR,L-PIPS,andSSIM,asfrequentlyemployedinex- NGP, which require substantially more memory or longer
istingliterature(seeTable1). Ourresultsencompassedvar- processing times. Overall, the results underscore GES ’s
ious configurations and iterations, highlighting differences capabilitytodeliverbalancedperformancewithremarkable
in training time, rendering speeds, and memory require- efficiency, making it a viable option for real-time applica-
mentsforoptimizedparameters. tionsthatdemandbothhigh-qualityoutputandoperational
speedandmemoryefficiency.
5.2.ImplementationDetailsofGES
Notethatitisdifficulttoseethedifferencesinvisualef-
Our methodology maintained consistent hyperparame- fectsbetweenGESandGaussiansinFig.5sincetheyhave
ter settings across all scenes, ensuring uniformity in our almostthesamePSNRbutadifferentfilesize(Table1).For
evaluations. We deployed an A6000 GPU for most of our afairvisualcomparison,werestrictthenumberofcompo-
tests. Our Generalized Exponential Splatting (GES ) was nentstoberoughlythesame(bycontrollingthesplittingof
implementedover40,000iterations, andthedensitygradi- Gaussians) and show the results in Fig.9. It clearly shows
entthresholdissetto0.0003.Thelearningratefortheshape thatGEScanmodeltinyandsharpedgesforthatscenebet-
8Dataset Mip-NeRF360Dataset Tanks&Temples DeepBlending
Method—Metric SSIM↑ PSNR↑ LPIPS↓ Train↓ FPS↑ Mem↓ SSIM↑ PSNR↑ LPIPS↓ Train↓ FPS↑ Mem↓ SSIM↑ PSNR↑ LPIPS↓ Train↓ FPS↑ Mem↓
Plenoxels 0.626 23.08 0.463 26m 6.79 2.1GB 0.719 21.08 0.379 25m 13.0 2.3GB 0.795 23.06 0.510 28m 11.2 2.7GB
INGP 0.699 25.59 0.331 7.5m 9.43 48MB 0.745 21.92 0.305 7m 14.4 48MB 0.817 24.96 0.390 8m 2.79 48MB
Mip-NeRF360 0.792 27.69 0.237 48h 0.06 8.6MB 0.759 22.22 0.257 48h 0.14 8.6MB 0.901 29.40 0.245 48h 0.09 8.6MB
3DGaussians-7K 0.770 25.60 0.279 6.5m 160 523MB 0.767 21.20 0.280 7m 197 270MB 0.875 27.78 0.317 4.5m 172 386MB
3DGaussians-30K 0.815 27.21 0.214 42m 134 734MB 0.841 23.14 0.183 26m 154 411MB 0.903 29.41 0.243 36m 137 676MB
GES(ours) 0.794 26.91 0.250 32m 186 377MB 0.836 23.35 0.198 21m 210 222MB 0.901 29.68 0.252 30m 160 399MB
Table1. ComparativeAnalysisofNovelViewSynthesisTechniques. Thistablepresentsacomprehensivecomparisonofourapproach
withestablishedmethodsacrossvariousdatasets. Themetrics,inclusiveofSSIM,PSNR,andLPIPS,alongsidetrainingduration,frames
persecond,andmemoryusage,provideamultidimensionalperspectiveofperformanceefficacy. Notethatourtrainingtimenumbersof
the differentmethods may becomputed on differentGPUs; they are not necessarily perfectly comparablebut are stillvalid. Notethat
non-explicitrepresentations(INGP,Mip-NeRF360)havelowmemorybecausetheyrelyonadditionalslowneuralnetworksfordecoding.
Red-coloredresultsarethebest.
GroundTruth GES(ours) Gaussians AblationSetup PSNR↑ SSIM↑ LPIPS↓ Size(MB)↓
Gaussians 27.21 0.815 0.214 734
GESw/oapprox.ϕ¯ 11.60 0.345 0.684 364
ρ
GESw/oshapereset 26.57 0.788 0.257 374
GESw/oL loss 27.07 0.800 0.250 411
ω
FullGES 26.91 0.794 0.250 377
Figure 9. Fair Visual Comparison. We show an example of
Gaussians [27] and GES when constrained to the same number
of splatting components for a fair visual comparison. It clearly Table2.AblationStudyonNovelViewSynthesis.Westudythe
showsthatGEScanmodeltinyandsharpedgesforthatscenebet- impactofseveralcomponentsinGESonthereconstructionquality
terthanGaussians. andfilesizeintheMip-NeRF360dataset.
pipelines such as DreamGaussian [68] and Text-to-3D us-
terthanGaussians.
ing Gaussian Splatting [10]. Integrating GES into these
6.2.Ablationandanalysis Gaussian-based 3D generation pipelines has yielded fast
andcompellingresultswithaplug-and-playabilityofGES
Shapeparameters. InTable2,weexploretheeffectofim-
inplaceofGaussianSplatting(seeFig.11).
portanthyperparametersassociatedwiththenewshapepa-
rameteronnovelviewsynthesisperformance. Weseethat
7.Conclusionanddiscussion
properapproximationϕ¯ inEq.(6)isnecessary, becauseif
ρ
wesetρ=10forϕ¯ tobeasclosetotheexactϕ(β)(Fig.7), This paper introduced GES (Generalized Exponential
ρ
thePSNRwoulddropto11.6. Additionaldetailedanalysis Splatting),anewtechniquefor3Dscenemodelingthatim-
isprovidedintheAppendix. proves upon Gaussian Splatting in memory efficiency and
Effectoffrequency-modulatedimageloss. Westudythe signal representation, particularly for high-frequency sig-
effectofthefrequencylossL introducedinSec.4.3onthe nals. Ourempiricalresultsdemonstrateitsefficacyinnovel
ω
performance by varying λ . In table 2 and in Fig.10 we viewsynthesisand3Dgenerationtasks.
ω
demonstratehowaddingthisL improvestheoptimization Limitation. Oneobviouslimitationinourapproachisthat
ω
in areas where large contrast exists or where the smooth performancetypicallydropstryingtomaketherepresenta-
backgroundisrenderedandalsoimprovestheefficiencyof tionasmemor-efficientandascompactaspossible. Thisis
GES.Wenoticethatincreasingλ inGESindeedreduces morenoticeableformorecomplexscenesduetotheprun-
ω
thesizeofthefile,butcanaffecttheperformance.Wechose ingoperationsthatdependonβ-tuning. Removingmanyof
λ = 0.5 as a middle ground between improved perfor- thecomponentscaneventuallydropthePSNRperformance
ω
manceandreducedfilesize. (Table 1 last 2 rows). Future research could focus on en-
Analyzingmemoryreduction. Wefindthatthereduction hancingGES’sperformanceinmorecomplexanddynamic
in memory after learning β is indeed attributed to the re- environmentsandexploringitsintegrationwithothertech-
duction of the number of components needed. For exam- nologiesin3Dmodeling.
ple, inthe“Train”sequence, thenumberofcomponentsis
References
1,087,264and548,064forGaussiansplattingandGESre-
spectively. This translates into the reduction of file size
[1] SameerAgarwal,YasutakaFurukawa,NoahSnavely,IanSi-
from275MBto129.5MBwhenutilizingGES. mon, Brian Curless, Steven M Seitz, and Richard Szeliski.
Applying GES in fast 3D generation. Recent works Building rome in a day. Communications of the ACM,
haveproposedtouseGaussianSplattingfor3Dgeneration 54(10):105–112,2011. 2
9GroundTruth GES(full) GES(w/oL ) GaussianSplatting[27]
ω
Figure10. Frequency-ModulatedLossEffect. Weshowtheeffectofthefrequency-modulatedimagelossL ontheperformanceon
ω
novelviewssynthesis. NotehowaddingthisL improvestheoptimizationinareaswherealargecontrastexistsorasmoothbackground
ω
isrendered.
Realfusion15 NeRF4 StableDiffusion-XL
[7] EricRChan,ConnorZLin,MatthewAChan,KokiNagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Reference
Guibas,JonathanTremblay,SamehKhamis,etal. Efficient
geometry-aware3dgenerativeadversarialnetworks. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVision
Sourceview andPatternRecognition(CVPR),pages16123–16133,2022.
2
[8] DaveZhenyuChen,YawarSiddiqui,Hsin-YingLee,Sergey
Tulyakov, and Matthias Nießner. Text2tex: Text-driven
Novelview1 texture synthesis via diffusion models. arXiv preprint
arXiv:2303.11396,2023. 3
[9] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Fantasia3d: Disentangling geometry and appearance for
Figure 11. GES Application: Fast Image-to-3D Generation.
Novel view2 high-quality text-to-3d content creation. arXiv preprint
We show selected 3D generated examples from Co3D images
arXiv:2303.13873,2023. 3
[57] by combining GES with the Gaussian-based 3D generation
[10] ZilongChen,FengWang,andHuapingLiu.Text-to-3dusing
pipeline [68], highlighting the plug-and-play benefits of GES to
gaussiansplatting. arXivpreprintarXiv:2309.16585,2023.
replaceGaussianSplatting[27].
9
[11] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-
derGSchwing,andLiang-YanGui. Sdfusion: Multimodal
[2] Kara-AliAliev,ArtemSevastopolsky,MariaKolos,Dmitry
3dshapecompletion,reconstruction,andgeneration.InPro-
Ulyanov,andVictorLempitsky. Neuralpoint-basedgraph-
ceedingsoftheIEEE/CVFConferenceonComputerVision
ics. InComputerVision–ECCV2020: 16thEuropeanCon-
andPatternRecognition(CVPR),2023. 3
ference, Glasgow, UK, August 23–28, 2020, Proceedings,
[12] Franc¸oisDarmon,Be´ne´dicteBascle,Jean-Cle´mentDevaux,
PartXXII16,pages696–712.Springer,2020. 2
PascalMonasse,andMathieuAubry. Improvingneuralim-
[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
plicitsurfacesgeometrywithpatchwarping. InProceedings
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
oftheIEEE/CVFConferenceonComputerVisionandPat-
SamuliLaine,BryanCatanzaro,etal. ediffi: Text-to-image
ternRecognition(CVPR),pages6260–6269,2022. 2
diffusionmodelswithanensembleofexpertdenoisers.arXiv
[13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong
preprintarXiv:2211.01324,2022. 3
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-
[4] JonathanTBarron,BenMildenhall,MatthewTancik,Peter tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Objaverse-xl:Auniverseof10m+3dobjects.arXivpreprint
Mip-nerf: Amultiscalerepresentationforanti-aliasingneu- arXiv:2307.05663,2023. 3
ralradiancefields. InProceedingsoftheIEEE/CVFConfer-
[14] J Armando Dominguez-Molina, Graciela Gonza´lez-Far´ıas,
enceonComputerVisionandPatternRecognition(CVPR),
Ramo´nMRodr´ıguez-Dagnino,andITESMCampusMonter-
pages5855–5864,2021. 2,8
rey.Apracticalproceduretoestimatetheshapeparameterin
[5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. thegeneralizedgaussiandistribution. availablethroughlink
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded https://www.cimat.mx/BiblioAdmin/RTAdmin/reportes/enlinea/I-
anti-aliased neural radiance fields. In Proceedings of the 01-18 eng.pdf,1,2003. 3,6,14,31
IEEE/CVF Conference on Computer Vision and Pattern [15] YilunDu,CameronSmith,AyushTewari,andVincentSitz-
Recognition(CVPR),pages5470–5479,June2022. 2,3,8, mann. Learning to render novel views from wide-baseline
32,38 stereopairs.InProceedingsoftheIEEE/CVFConferenceon
[6] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. ComputerVisionandPatternRecognition(CVPR),2023. 2
Srinivasan,andPeterHedman. Zip-nerf: Anti-aliasedgrid- [16] Olivier D Faugeras. What can be seen in three dimen-
basedneuralradiancefields. ICCV,2023. 32 sions with an uncalibrated stereo rig? In Computer Vi-
10sion—ECCV’92: Second European Conference on Com- [30] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
puter Vision Santa Margherita Ligure, Italy, May 19–22, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
1992Proceedings2,pages563–578.Springer,1992. 2 Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolution
[17] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong text-to-3dcontentcreation.InProceedingsoftheIEEE/CVF
Chen, BenjaminRecht, andAngjooKanazawa. Plenoxels: Conference on Computer Vision and Pattern Recognition
Radiancefieldswithoutneuralnetworks. InProceedingsof (CVPR),2023. 3
theIEEE/CVFConferenceonComputerVisionandPattern [31] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
Recognition(CVPR),pages5501–5510,June2022. 1 monLucey.Barf:Bundle-adjustingneuralradiancefields.In
[18] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, IEEEInternationalConferenceonComputerVision(ICCV),
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene- 2021. 7
basedtext-to-imagegenerationwithhumanpriors. InPro- [32] Hsueh-TiDerekLiu,MichaelTao,andAlecJacobson. Pa-
ceedings of the European Conference on Computer Vision parazzi:surfaceeditingbywayofmulti-viewimageprocess-
(ECCV),pages89–106,2022. 3 ing. ACMTrans.Graph.,37(6):221–1,2018. 2
[19] MarkusGrossandHanspeterPfister. Point-basedgraphics. [33] RuoshiLiu,SachitMenon,ChengzhiMao,DennisPark,Si-
Elsevier,2011. 2 mon Stent, and Carl Vondrick. What you can reconstruct
[20] PeterHedman,JulienPhilip,TruePrice,Jan-MichaelFrahm, fromashadow.InProceedingsoftheIEEE/CVFConference
George Drettakis, and Gabriel Brostow. Deep blending onComputerVisionandPatternRecognition,pages17059–
for free-viewpoint image-based rendering. ACM Trans. on 17068,2023. 2
Graphics(TOG),37(6),2018. 8
[34] RuoshiLiuandCarlVondrick. Humansaslightbulbs: 3d
[21] Lukas Ho¨llein, Ang Cao, Andrew Owens, Justin Johnson,
humanreconstructionfromthermalreflection. InProceed-
and Matthias Nießner. Text2room: Extracting textured
ingsoftheIEEE/CVFConferenceonComputerVisionand
3d meshes from 2d text-to-image models. arXiv preprint
PatternRecognition,pages12531–12542,2023. 2
arXiv:2303.11989,2023. 3
[35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
[22] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra
makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-
Ahuja,andJia-BinHuang. Deepmvs: Learningmulti-view
3: Zero-shot one image to 3d object. arXiv preprint
stereopsis. In Proceedings of the IEEE/CVF Conference
arXiv:2303.11328,2023. 3
onComputerVisionandPatternRecognition(CVPR),pages
[36] ShichenLiu,TianyeLi,WeikaiChen,andHaoLi. Softras-
2821–2830,2018. 2
terizer:Adifferentiablerendererforimage-based3dreason-
[23] AjayJain,MatthewTancik,andPieterAbbeel. Puttingnerf
ing. InProceedingsoftheIEEE/CVFInternationalConfer-
onadiet: Semanticallyconsistentfew-shotviewsynthesis.
enceonComputerVision,pages7708–7717,2019. 2
In Proceedings of the IEEE/CVF Conference on Computer
[37] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
VisionandPatternRecognition(CVPR),pages5885–5894,
Schwartz,AndreasLehrmann,andYaserSheikh.Neuralvol-
2021. 2
umes: Learningdynamicrenderablevolumesfromimages.
[24] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rup-
arXivpreprintarXiv:1906.07751,2019. 2
precht, and Andrea Vedaldi. Farm3d: Learning articu-
[38] Matthew M Loper and Michael J Black. Opendr: An
lated 3d animals by distilling 2d diffusion. arXiv preprint
approximate differentiable renderer. In Computer Vision–
arXiv:2304.10535,2023. 3
ECCV 2014: 13th European Conference, Zurich, Switzer-
[25] A.J.Jerri. Theshannonsamplingtheorem—itsvariousex-
land,September6-12,2014,Proceedings,PartVII13,pages
tensionsandapplications: Atutorialreview. Proceedingsof
154–169.Springer,2014. 2
theIEEE,65(11):1565–1596,1977. 4,17
[26] HiroharuKato,YoshitakaUshiku,andTatsuyaHarada.Neu- [39] David G Lowe. Distinctive image features from scale-
ral3dmeshrenderer.InProceedingsoftheIEEEconference invariantkeypoints. InternationalJournalofComputerVi-
on computer vision and pattern recognition, pages 3907– sion(IJCV),60:91–110,2004. 2
3916,2018. 2 [40] RicardoMartin-Brualla, NohaRadwan, MehdiSMSajjadi,
[27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
and George Drettakis. 3d gaussian splatting for real-time worth. Nerfinthewild: Neuralradiancefields foruncon-
radiance field rendering. ACM Transactions on Graphics, strainedphotocollections. InProceedingsoftheIEEE/CVF
42(4),July2023. 1,2,3,5,6,7,8,9,10,14,16,17,32,33, Conference on Computer Vision and Pattern Recognition
38,39 (CVPR),pages7210–7219,2021. 1
[28] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: [41] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
Ray entropy minimization for few-shot neural volume ren- Andrea Vedaldi. Realfusion: 360{\deg} reconstruction
dering. In Proceedings of the IEEE/CVF Conference on of any object from a single image. In Proceedings of
Computer Vision and Pattern Recognition (CVPR), pages theIEEE/CVFConferenceonComputerVisionandPattern
12912–12921,2022. 2 Recognition(CVPR),2023. 3,32
[29] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen [42] GalMetzer,EladRichardson,OrPatashnik,RajaGiryes,and
Koltun.Tanksandtemples:Benchmarkinglarge-scalescene Daniel Cohen-Or. Latent-nerf for shape-guided generation
reconstruction.ACMTransactionsonGraphics,36(4),2017. of3dshapesandtextures. arXivpreprintarXiv:2211.07600,
8 2022. 3
11[43] Aryan Mikaeili, Or Perel, Daniel Cohen-Or, and Ali [55] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
Mahdavi-Amiri. Sked:Sketch-guidedtext-based3dediting. and Mark Chen. Hierarchical text-conditional image gen-
arXivpreprintarXiv:2303.10735,2023. 3 erationwithcliplatents. arXivpreprintarXiv:2204.06125,
[44] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, 2022. 3
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf: [56] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Representingscenesasneuralradiancefieldsforviewsyn- ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.
thesis. InProceedingsoftheEuropeanConferenceonCom- Zero-shottext-to-imagegeneration.InProceedingsoftheIn-
puterVision(ECCV),pages405–421.Springer,2020. 1,2, ternationalConferenceonMachineLearning(ICML),pages
3,6 8821–8831.PMLR,2021. 3
[45] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan- [57] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
derKeller.Instantneuralgraphicsprimitiveswithamultires- LucaSbordone,PatrickLabatut,andDavidNovotny. Com-
olution hash encoding. ACM Trans. Graph., 41(4):102:1– mon objects in 3d: Large-scale learning and evaluation of
102:15,July2022. 1,8 real-life 3d category reconstruction. In Proceedings of the
[46] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela IEEE/CVF International Conference on Computer Vision
Mishkin, and Mark Chen. Point-e: A system for generat- (ICCV),pages10901–10911,October2021. 10
ing3dpointcloudsfromcomplexprompts. arXivpreprint [58] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,
arXiv:2212.08751,2022. 32 andDanielCohen-Or. Texture: Text-guidedtexturingof3d
[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, shapes. arXivpreprintarXiv:2302.01721,2023. 3
James Bradbury, Gregory Chanan, Trevor Killeen, Zem- [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
ing Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: Patrick Esser, and Bjo¨rn Ommer. High-resolution image
An imperative style, high-performance deep learning li- synthesis with latent diffusion models. In Proceedings of
brary. In Advances in Neural Information Processing Sys- theIEEE/CVFConferenceonComputerVisionandPattern
tems(NeurIPS),2019. 3 Recognition(CVPR),pages10684–10695,2022. 3
[48] Felix Petersen, Amit H Bermano, Oliver Deussen, and [60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
DanielCohen-Or. Pix2vex: Image-to-geometryreconstruc- Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
tion using a smooth differentiable renderer. arXiv preprint RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
arXiv:1903.11149,2019. 2 etal.Photorealistictext-to-imagediffusionmodelswithdeep
[49] Dustin Podell, Zion English, Kyle Lacey, Andreas language understanding. Advances in Neural Information
Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and ProcessingSystems(NeurIPS),35:36479–36494,2022. 3
Robin Rombach. Sdxl: Improving latent diffusion models [61] Johannes Lutz Scho¨nberger and Jan-Michael Frahm.
forhigh-resolutionimagesynthesis,2023. 33 Structure-from-motion revisited. In Conference on Com-
[50] BenPoole,AjayJain,JonathanTBarron,andBenMilden- puterVisionandPatternRecognition(CVPR),2016. 2
hall. Dreamfusion: Text-to-3d using 2d diffusion. Inter- [62] Johannes Lutz Scho¨nberger and Jan-Michael Frahm.
national Conference on Learning Representations (ICLR), Structure-from-motion revisited. In Conference on Com-
2022. 3,32 puterVisionandPatternRecognition(CVPR),2016. 5
[51] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and [63] JohannesLutzScho¨nberger,EnliangZheng,MarcPollefeys,
FrancescMoreno-Noguer.D-nerf:Neuralradiancefieldsfor and Jan-Michael Frahm. Pixelwise view selection for un-
dynamicscenes. InProceedingsoftheIEEE/CVFConfer- structured multi-view stereo. In European Conference on
enceonComputerVisionandPatternRecognition(CVPR), ComputerVision(ECCV),2016. 2
pages10318–10327,2021. 1 [64] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young
[52] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Chun. Ditto-nerf: Diffusion-based iterative text to omni-
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko- directional 3d model. arXiv preprint arXiv:2304.02827,
rokhodov, Peter Wonka, Sergey Tulyakov, and Bernard 2023. 3
Ghanem. Magic123: One image to high-quality 3d object [65] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon
generation using both 2d and 3d diffusion priors. arXiv Ko,HyeonsuKim,JunhoKim,Jin-HwaKim,JiyoungLee,
preprintarXiv:2306.17843,2023. 32 and Seungryong Kim. Let 2d diffusion model know 3d-
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya consistencyforrobusttext-to-3dgeneration. InProceedings
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, oftheIEEE/CVFConferenceonComputerVisionandPat-
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- ternRecognition(CVPR),2023. 3
ingtransferablevisualmodelsfromnaturallanguagesuper- [66] AndreaTagliasacchiandBenMildenhall. Volumerendering
vision. In Proceedings of the International Conference on digest(fornerf). arXivpreprintarXiv:2209.02417,2022. 2
MachineLearning(ICML),pages8748–8763.PMLR,2021. [67] Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with
32 stable-diffusion, 2022. https://github.com/ashawkey/stable-
[54] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, dreamfusion. 3
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber- [68] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGang
man, Michael Rubinstein, Jonathan Barron, et al. Dream- Zeng.Dreamgaussian:Generativegaussiansplattingforeffi-
booth3d: Subject-driven text-to-3d generation. arXiv cient3dcontentcreation. arXivpreprintarXiv:2309.16653,
preprintarXiv:2303.13508,2023. 3 2023. 3,9,10,32
12[69] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, neuralradiancefields. InProceedingsoftheIEEE/CVFIn-
JonathanTBarron,andPratulPSrinivasan.Ref-nerf:Struc- ternational Conference on Computer Vision (ICCV), 2021.
turedview-dependentappearanceforneuralradiancefields. 1
In Proceedings of the IEEE/CVF Conference on Computer [82] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
VisionandPatternRecognition(CVPR),pages5481–5490. pixelnerf: Neural radiance fields from one or few images.
IEEE,2022. 2 In Proceedings of the IEEE/CVF Conference on Computer
[70] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, VisionandPatternRecognition(CVPR),pages4578–4587,
andGregShakhnarovich. Scorejacobianchaining: Lifting 2021. 2
pretrained 2d diffusion models for 3d generation. In Pro- [83] Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-to-
ceedingsoftheIEEE/CVFConferenceonComputerVision densemulti-viewstereowithlearnedpropagationandgauss-
andPatternRecognition(CVPR),2023. 3 newtonrefinement.InProceedingsoftheIEEE/CVFConfer-
[71] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku enceonComputerVisionandPatternRecognition(CVPR),
Komura, and Wenping Wang. Neus: Learning neural im- pages1949–1958,2020. 2
plicit surfaces by volume rendering for multi-view recon- [84] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
struction. In Advances in Neural Information Processing tler, and Andreas Geiger. Monosdf: Exploring monocu-
Systems(NeurIPS),2021. 2 lar geometric cues for neural implicit surface reconstruc-
[72] YiqunWang,IvanSkorokhodov,andPeterWonka.Hf-neus: tion. InAdvancesinNeuralInformationProcessingSystems
Improved surface reconstruction using high-frequency de- (NeurIPS),2022. 2
tails. Advances in Neural Information Processing Systems [85] JasonZhang,GengshanYang,ShubhamTulsiani,andDeva
(NeurIPS),35:1966–1978,2022. 2 Ramanan. Ners: neuralreflectancesurfacesforsparse-view
[73] OliviaWiles,GeorgiaGkioxari,RichardSzeliski,andJustin 3dreconstructioninthewild. AdvancesinNeuralInforma-
Johnson. Synsin: End-to-end view synthesis from a sin- tionProcessingSystems(NeurIPS),34:29835–29847,2021.
gle image. In Proceedings of the IEEE/CVF Conference 2
on Computer Vision and Pattern Recognition, pages 7467–
[86] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz,
7477,2020. 2
andFelixHeide. Differentiablepoint-basedradiancefields
[74] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin forefficientviewsynthesis. InSIGGRAPHAsia2022Con-
Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: ferencePapers,pages1–12,2022. 2
Point-based neural radiance fields. In Proceedings of the
[87] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
IEEE/CVF Conference on Computer Vision and Pattern
man, and Oliver Wang. The unreasonable effectiveness of
Recognition,pages5438–5448,2022. 2
deep features as a perceptual metric. In Proceedings of
[75] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan.
theIEEE/CVFConferenceonComputerVisionandPattern
Mvsnet:Depthinferenceforunstructuredmulti-viewstereo.
Recognition(CVPR),2018. 32
InProceedingsoftheEuropeanConferenceonComputerVi-
[88] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and
sion(ECCV),pages767–783,2018. 2
MarkusGross. Ewavolumesplatting. ProceedingsVisual-
[76] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,
ization,2001.VIS’01.,pages29–538,2001. 5
and Long Quan. Recurrent mvsnet for high-resolution
multi-view stereo depth inference. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition(CVPR),pages5525–5534,2019. 2
[77] LiorYariv,JiataoGu,YoniKasten,andYaronLipman. Vol-
umerenderingofneuralimplicitsurfaces. AdvancesinNeu-
ral Information Processing Systems (NeurIPS), 34:4805–
4815,2021. 2
[78] LiorYariv,YoniKasten,DrorMoran,MeiravGalun,Matan
Atzmon, BasriRonen, andYaronLipman. Multiviewneu-
ralsurfacereconstructionbydisentanglinggeometryandap-
pearance. AdvancesinNeuralInformationProcessingSys-
tems(NeurIPS),33:2492–2502,2020. 2
[79] Wang Yifan, Felice Serena, Shihao Wu, Cengiz O¨ztireli,
andOlgaSorkine-Hornung. Differentiablesurfacesplatting
forpoint-basedgeometryprocessing. ACMTransactionson
Graphics(TOG),38(6):1–14,2019. 2
[80] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenox-
els:Radiancefieldswithoutneuralnetworks. arXivpreprint
arXiv:2112.05131,2(3):6,2021. 2
[81] AlexYu,RuilongLi,MatthewTancik,HaoLi,RenNg,and
Angjoo Kanazawa. PlenOctrees for real-time rendering of
13A.TheoryBehindGeneralizedExponentials Theorem 1 (Superiority of GEF Approximation Over
Gaussian for Square Wave Signals). Let S(t) represent a
A.1.GeneralizedExponentialFunction
squarewavesignalwithamplitudeA>0andwidthL>0
TheGeneralizedExponentialFunction(GEF)issimilar centered at t = 0. Define two functions: a scaled Gaus-
totheprobabilitydensityfunction(PDF)oftheGeneralized sianG(t;α,A) = Ae− αt2 2,andaGeneralizedExponential
NormalDistribution(GND)[14]withanadditionalampli- Function GEF(t;α,β,A) = Ae−(|t|/α)β. For any given
tude parameter A ∈ R. This function allows for a more
scale parameter α, there exists a shape parameter β such
flexible adaptation to various data shapes by adjusting the (cid:82)∞
that the approximation error E = |S(t)−f(t)|dt.
shapeparameterβ ∈ (0,∞). TheGEFisgivenbythefol- f −∞
ofthesquaresignalS(t)usingGEFisstrictlysmallerthan
lowing.
thatusingtheGaussianG.
(cid:32) (cid:18) |x−µ|(cid:19)β(cid:33)
Proof. The error metric E for the square signal S(t) ap-
f(x|µ,α,β,A)=Aexp − (10) f
α proximationusingf functionasE =(cid:82)∞ |S(t)−f(t)|dt.
f −∞
UtilizingsymmetryanddefinitionofS(t),andthefactthat
whereµ ∈ Risthelocationparameter, α ∈ Risthescale
S(t) > G(t;α,A), the error for the Gaussian approxima-
parameter,Adefinestheamplitude,andβ >0istheshape
tionsimplifiesto:
parameter. Forβ =2,theGEFbecomesascaledGaussian
distribution: (cid:90) L/2 (cid:90) ∞
A
(cid:32) 1(cid:18) x−µ(cid:19)2(cid:33) E G =2
0
A(1−e−
αt2
2)dt+2 L/2Ae−
αt2
2dt.
f(x|µ,α,β =2,A)= √ exp − √
α 2π 2 α/ 2
FortheGEFapproximation,theerroris:
(11)
Andforβ =1,Eq. 10reducestoascaledLaplacedistribu- (cid:90) L/2 (cid:90) ∞
E =2
A(1−e−(t/α)β
)dt+2
Ae−(t/α)β
dt.
tion: GEF
0 L/2
(cid:18) (cid:19)
A |x−µ|
f(x|µ,α,β =1,A)= exp − (12) The goal is to show the difference in errors ∆E = E −
2α α G
E to be strictly positive, by picking β appropriately.
GEF
The GEF, therefore, provides a versatile framework for Theerrordifferencecanbedescribedasfollows.
modeling a wide range of data by varying β, unlike the
Gaussian mixtures, which have a low-pass frequency do- ∆E =∆E middle+∆E tail
main. Many common signals, like the square or triangle,
(cid:90) L/2 (cid:90) L/2
areband-unlimited,constitutingafundamentalchallengeto ∆E
middle
=2 A(1−e− αt2 2)dt − 2 A(1−e−(t/α)β )dt
Gaussian-basedmethods(seeFig.12). Inthispaper,wetry 0 0
t so pll ae ta tir nn ga top ao ls li ot wive foβ rafo gr ee nv ee rr ay lizc eo dm 3p Don re en pt reo sf enth tae tiG oa nu .ssian ∆E
tail
=2(cid:90) ∞ Ae− αt2 2dt−2(cid:90) ∞ Ae−(t/α)β dt
L/2 L/2
A.2.TheoreticalResults LetusDefineerr(t)asthedifferencebetweentheexponen-
tialterms:
fixeD des bp ei hte avi it os rg ie nne ter ra mliz sa ob fle frc ea qp ua eb nil ci ytie ds o, mth ae inG . E TF heha es rrn oo r err(t)=e− αt2 2 −e−(t/α)β .
functionsoftheGEFanditsFourierdomaincannotbestud- The difference in the middle error terms for the Gaussian
iedanalytically,astheyinvolvecomplexintegralsofexpo- andGEFapproximations, ∆E , canbeexpressedus-
middle
nentials without closed form that depend on the shape pa- ingerr(t)as:
rameterβ. Forexample,theFourierofGEFisgivenby
(cid:90) L/2
(cid:90) ∞ (cid:32) (cid:18) |x−µ|(cid:19)β(cid:33) ∆E middle =2A err(t)dt.
F(f)(ξ)= Aexp − e−2πixξdx 0
α
−∞
Usingthetrapezoidalapproximationoftheintegral,this
which does not have a closed-form solution for a general simplifiesto:
β. We demonstrate that for specific cases, such as for a
squaresignal,theGEFcanachieveasmallerapproximation ∆E middle ≈LAerr(L/2)=LA(cid:16) e− 4L α2 2 −e−(L/2α)β(cid:17) .
errorthanthecorrespondingGaussianfunctionbyproperly
choosing β. Theorem 1 provides a theoretical foundation Basedonthefactthatthenegativeexponentialismono-
forpreferringtheGEFoverstandardGaussianfunctionsin tonicallydecreasingandtoensure∆E isalwayspos-
middle
our GES representation instead of 3D Gaussian Splatting itive, we choose β based on the relationship between L/2
[27]. andα:
14• If L > α (i.e., L > 1), choosing β > 2 ensures componentsisgivenby:
2 2α
e−(L/2α)β <e− 4L α2 2. N
(cid:88)
d(x)= w D
i i
• If L < α(i.e., L < 1), choosing0 < β < 2results
2 2α i=1
ine−(L/2α)β <e− 4L α2 2.
D
=(cid:18) exp(cid:18) −(x−µ i)2(cid:19) −exp(cid:18)
−
(x−µ i)2 (cid:19)(cid:19)
,
i 2σ2+ϵ 2(σ2/ν)+ϵ
i i
Thus, ∆E middle canalwaysbemadepositivebychoosing (14)
β appropriately,implyingthattheerrorintheGEFapprox- whereσ isascaleparameter,andthevarianceratioν
i
imationintheinterval[−L/2,L/2]isalwayslessthanthat isfixedtobe4.
oftheGaussianapproximation. Similarly,thedifferenceof
• Laplacian of Gaussian (LoG) Mixture Model: The
tail errors ∆E can be made positive by an appropriate
tail
LoGmixturemodelisformedbyaseriesofLaplacian
choiceofβ,concludingthatthetotalerrorE isstrictly
GEF
of Gaussian functions, each defined by a mean (µ ),
lessthanE . Thisconcludestheproof. i
G
scale (γ ), and weight (w ). The mixture model l(x)
i i
is:
A.3. Numerical Simulation of Gradient-Based 1D
Mixtures l(x)=(cid:88)N
w
(cid:18) −(x−µ i)2 +1(cid:19) exp(cid:18) −(x−µ i)2(cid:19)
,
Objective. Theprimaryobjectiveofthisnumericalsimula- i γ2 2γ2+ϵ
i=1 i i
tionistoevaluatetheeffectivenessofthegeneralizedexpo- (15)
nentialmodelinrepresentingvariousone-dimensional(1D)
• GeneralizedMixtureModel: Thismodelgeneralizes
signal types. This evaluation was conducted by fitting the
theGaussianmixturebyintroducingashapeparameter
model to synthetic signals generated to embody character-
β. Eachcomponentofthemodelh(x)isexpressedas:
istics of square, triangle, parabolic, half sinusoidal, Gaus-
sian,andexponentialfunctions,whichcanconstituteanon-
exclusivelistofbasictopologiesavailableintherealworld.
h(x)=(cid:88)N
w
exp(cid:18)
−|x−µ
i|β(cid:19)
, (16)
SimulationSetup.Theexperimentalframeworkwasbased
i=1
i 2σ i2+ϵ
onaseriesofparametricmodelsimplementedinPyTorch,
where β is a learnable parameter that is optimized
designed to approximate 1D signals using mixtures of dif-
alongsideotherparameters. Whenβ = 2isfixed,the
ferent functions such as Gaussian, Difference of Gaus-
equationinEq.(16)reducestotheoneinEq.(13).
sians (DoG), Laplacian of Gaussian (LoG), and a Gener-
alized mixture model. Each model comprised parameters
Model Configuration. The models were configured with
formeans,variances(orscales),andweights,withthegen-
a varying number of components N, with tests conducted
eralizedmodelincorporatinganadditionalparameter,β,to
usingN ={2,5,8,10,15,20,50,100}. Theweightsofthe
controltheexponentiationoftheGaussianfunction.
componentscouldbeeitherpositiveorunrestricted. Forthe
Models. Here,wedescribethemixturemodelsusedtoap-
generalizedmodel,theβ parameterwaslearnable.
proximatethetruesignalforms.
Training Procedure. Each model was trained using the
Adam optimizer with a mean squared error loss function.
• GaussianMixtureModel(GMM):TheGMMcom-
The input x was a linearly spaced tensor representing the
bines several Gaussian functions, each defined by its
domain of the synthetic signal, and the target y was the
mean(µ ),variance(σ2),andweight(w ). Forasetof
i i i value of the signal at each point in x. Training proceeded
N Gaussianfunctions,themixturemodelg(x)canbe
for a predetermined number of epochs, and the loss was
expressedas:
recordedattheendoftraining.
DataGeneration. Synthetic1Dsignalsweregeneratedfor
g(x)=(cid:88)N
w
exp(cid:18)
−(x−µ
i)2(cid:19)
, (13)
varioussignaltypesoveraspecifiedrange,withagivendata
i 2σ2+ϵ sizeandsignalwidth. Thesignalswereusedastheground
i=1 i
truthfortrainingthemixturemodels. Thegroundtruthsig-
where ϵ is a small constant to avoid division by zero, nalsusedintheexperimentareone-dimensional(1D)func-
withϵ=1e−8. tions that serve as benchmarks for evaluating signal pro-
cessing algorithms. Each signal type is defined within a
• DifferenceofGaussians(DoG)MixtureModel:The specifiedwidtharoundtheorigin,andthevalueoutsidethis
DoGmixturemodeliscomprisedofelementsthatrep- intervaliszero(seeFig.12). Theparameterwidthσ dic-
resent the difference between two Gaussian functions tatestheeffectivespanofthenon-zeroportionofthesignal.
with a fixed variance ratio ν. The model d(x) for N Wedefinesixdistinctsignaltypesasfollows:
15Square Signal Fourier Transform of Square
1 1
0
0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Triangle Signal Fourier Transform of Triangle
1 1
0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Parabolic Signal Fourier Transform of Parabolic
1
2000
0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Half_sinusoid Signal Fourier Transform of Half_sinusoid
1 100
0
0 100
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Exponential Signal Fourier Transform of Exponential
1 2
0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Gaussian Signal Fourier Transform of Gaussian
1
1
0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
Time Frequency
Figure 12. Commong Signals Used and Their Fourier Transforms. Note that the Gaussian function is low-pass bandwidth, while
commonsignalslikethesquareandtrianglewithsharpedgeshaveinfinitebandwidth,makingthemchallengingtobefittedwithmixtures
thathavelow-passfrequencybandwidth(e.g.Gaussianmixtures,representedbyGaussianSplatting[27]).
1. SquareSignal: Thesquaresignalisabinaryfunction 3. Parabolic Signal: This signal forms a downward-
wherethevalueis1withintheinterval(−σ,σ)and0 facingparabolawithintheinterval, anditsexpression
2 2
elsewhere. Mathematically,itisrepresentedas is
(cid:40)
(cid:40) 1 if − σ <x< σ, f (x)= (σ 2)2−x2 if − σ 2 <x< σ 2, (21)
f square(x)= 2 2 (17) parabolic 0 otherwise.
0 otherwise.
TheFourierTransformoftheparabolicsignalis
ItsFourierTransformisgivenby
(cid:16) (cid:16) (cid:17)(cid:17)2
(cid:18) (cid:19) 3· sinc f·σ
f ·σ 2π
FT{SquareWave}(f)=sinc (18) FT{ParabolicWave}(f)= (22)
π π2·f2
4. Half Sinusoid Signal: A half-cycle of a sine wave is
2. Triangle Signal: This signal increases linearly from
containedwithintheinterval,startingandendingwith
theleftedgeoftheintervaltothecenteranddecreases
zeroamplitude. Itsformulais
symmetrically to the right edge, forming a triangular
shape. Itisdefinedas (cid:40) sin(cid:0) (x+ σ)π(cid:1) if − σ <x< σ,
f (x)= 2 σ 2 2
(cid:40) σ −|x| if − σ <x< σ, halfsinusoid 0 otherwise.
f triangle(x)= 2 2 2 (19) (23)
0 otherwise.
ItsFourierTransformisdescribedby
ItsFourierTransformis (cid:40)
σ iff =0
FT{HalfSinusoid}(f)= 2
(cid:18) (cid:18) f ·σ(cid:19)(cid:19)2 σ·sin(π·f·σ) otherwise
FT{TriangleWave}(f)= sinc (20) π2·f2
2π (24)
16
edutilpmA
edutilpmA
edutilpmA
edutilpmA
edutilpmA
edutilpmA
edutingaM
edutingaM
edutingaM
edutingaM
edutingaM
edutingaM5. ExponentialSignal: Exhibitinganexponentialdecay mean squared error (MSE) loss between the model output
centeredattheorigin,thissignalisrepresentedby and the ground truth signal. The number of components
in the mixture models (N) varied among a set of values,
(cid:40)
exp(−|x|) if − σ <x< σ, andmodelswerealsodifferentiatedbasedonwhetherthey
f (x)= 2 2
exponential 0 otherwise. wereconstrainedtopositiveweights. Foracomprehensive
(25) evaluation, each configuration was run multiple times (20
TheFourierTransformfortheexponentialsignalis runsperconfiguration)toaccountforvariabilityinthetrain-
ing process. During these runs, the number of instances
σ
FT{Exponential}(f)= (26) where the training resulted in a NaN loss was recorded
f2+(cid:0)σ(cid:1)2
as an indicator of stability issues. The stability of each
2
modelwasquantifiedbythepercentageofsuccessfultrain-
6. GaussianSignal: Unliketheothers,theGaussiansig- ingruns(TotalRuns−NaNLossCounts×100%). Theexperiments
TotalRuns
nalisnotboundedwithinaspecificintervalbutinstead thatfailedfailedbecausethelosshasdivergedtoNaN.This
extends over the entire range of x, with its amplitude typicalnumericalinstabilityinoptimizationistheresultof
governedbyaGaussiandistribution. Itisgivenby learningthevariancewhichcangoclosetozero, resulting
intheexponentialformula(inEq.(10))todividebyanex-
(cid:18) x2 (cid:19)
tremelysmallnumber.
f (x)=exp − . (27)
Gaussian 2σ2 The average MSE loss from successful runs was calcu-
latedto provideameasure ofmodelperformance. The re-
TheFourierTransformoftheGaussiansignalisalsoa
sults of these experiments were plotted, showing the rela-
Gaussian,whichinthecontextofstandarddeviationσ
tionshipbetweenthenumberofcomponentsandthestabil-
isrepresentedas
ityandlossofthemodelsforeachsignaltype.
√
FT{Gaussian}(f)=
2π·σ·exp(cid:0) −2π2σ2f2(cid:1)
(28)
Simulation Results. In the conducted analysis, both the
loss and stability of various mixture models with positive
and non-positive weights were evaluated on signals with
As shown in Fig.12, the Gaussian function has a low-
different shapes. As depicted in Figure 13, the Gaussian
pass band, while signals like the square and triangle with
Mixture Model with positive weights consistently yielded
sharpedgeshaveinfinitebandwidth,makingthemchalleng-
the lowest loss across the number of components, indicat-
ing for mixtures that have low-pass frequency bandwidth
ing its effective approximation of the square signal. Con-
(e.g.Gaussianmixtures,representedbyGaussianSplatting
versely, non-positive weights in the Gaussian and General
[27]).
models showed a higher loss, emphasizing the importance
Each signal is sampled at discrete points using a Py-
ofweightsign-onmodelperformance.Thesefindingshigh-
Torch tensor to facilitate computational manipulation and
light the intricate balance between model complexity and
analysis within the experiment’s framework. We show in
weightconstraintsinachievingbothlowlossandhighsta-
Fig.14,15,16,17,18,19,20,21,22,23,24, and 25 examples of
bility. Note that GEF is very efficient in fitting the square
fitting all the mixture on all different signal types of in-
withfewcomponents,whileLoGandDoGaremorestable
terest when positive weighting is used in the mixture vs.
foralargernumberofcomponents. Also,notethatpositive
when allowing real weighting in the combinations in the
weight mixtures tend to achieve lower loss with a smaller
above equations. Note how sharp edges constitute a chal-
numberofcomponentsbutarelessstableforalargernum-
lenge for Gaussians that have low pass bandwidth while a
berofcomponents.
square signal has an infinite bandwidth known by the sinc
function[25].
Loss Evaluation. The models’ performance was evalu-
ated based on the loss value after training. Additionally,
the model’s ability to represent the input signal was visu-
ally inspected through generated plots. Multiple runs per
configuration were executed to account for variance in the
results.
Stability Evaluation. Model stability and performance
were assessed using a series of experiments involving var-
ious signal types and mixture models. Each model was
trained on a 1D signal generated according to predefined
signal types (square, triangle, parabolic, half sinusoid,
Gaussian,andexponential),withthegoalofminimizingthe
17Loss Value vs Number of Components on square signal Stability vs Number of Components on square signal Loss Value vs Number of Components on parabolic signal Stability vs Number of Components on parabolic signal
101 Gaussian (Positive) 100 100 Gaussian (Positive) 100
Gaussian (Real) Gaussian (Real)
D Do oG G ( (P Ro es ai lt )ive) 80 Gaussian (Positive) 101 D Do oG G ( (P Ro es ai lt )ive) 80 Gaussian (Positive)
102 LoG (Positive) Gaussian (Real) LoG (Positive) Gaussian (Real) LoG (Real) 60 DoG (Positive) LoG (Real) 60 DoG (Positive)
GEF (Positive) DoG (Real) 102 GEF (Positive) DoG (Real) GEF (Real) LoG (Positive) GEF (Real) LoG (Positive)
103 40 LoG (Real) 40 LoG (Real)
GEF (Positive) 103 GEF (Positive)
GEF (Real) GEF (Real)
20 20
104 104
0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Number of Components (N) Number of Components (N) Number of Components (N) Number of Components (N)
(a)Squaresignal (b)Parabolicsignal
Loss Value vs Number of Components on exponential signal Stability vs Number of Components on exponential signal Loss Value vs Number of Components on triangle signal Stability vs Number of Components on triangle signal
102 G Ga au us ss si ia an n ( (P Ro es ai lt )ive) 100 101 G Ga au us ss si ia an n ( (P Ro es ai lt )ive) 100
D Do oG G ( (P Ro es ai lt )ive) 80 Gaussian (Positive) D Do oG G ( (P Ro es ai lt )ive) 80 Gaussian (Positive)
103 L Lo oG G ( (P Ro es ai lt )ive) 60 G Doa Gus (s Pia on si t( iR ve ea )l) 102 L Lo oG G ( (P Ro es ai lt )ive) 60 G Doa Gus (s Pia on si t( iR ve ea )l)
GEF (Positive) DoG (Real) GEF (Positive) DoG (Real) GEF (Real) 40 L Lo oG G ( (P Ro es ai lt )ive) 103 GEF (Real) 40 L Lo oG G ( (P Ro es ai lt )ive)
104 G GE EF F ( (P Ro es ai lt )ive) G GE EF F ( (P Ro es ai lt )ive)
20 104 20
0 20 40 60 80 100 0 0 20 40 60 80 100 105 0 20 40 60 80 100 0 0 20 40 60 80 100
Number of Components (N) Number of Components (N) Number of Components (N) Number of Components (N)
(c)Exponentialsignal (d)Trianglesignal
Loss Value vs Number of Components on gaussian signal Stability vs Number of Components on gaussian signal Loss Value vs Number of Components on half_sinusoid signal Stability vs Number of Components on half_sinusoid signal
11 00 32 10 80 0 Gaussian (Positive) 102 G G D Do oa a G Gu us s ( (s s P Ri ia a o en n s ai lt( ( )iP R vo e es a )i lt )ive) 10 80 0 Gaussian (Positive)
Gaussian (Real) LoG (Positive) Gaussian (Real) 104 60 DoG (Positive) 103 LoG (Real) 60 DoG (Positive)
Gaussian (Positive) DoG (Real) GEF (Positive) DoG (Real) 105 G Doa Gus (s Pia on si t( iR ve ea )l) 40 L Lo oG G ( (P Ro es ai lt )ive) GEF (Real) 40 L Lo oG G ( (P Ro es ai lt )ive)
106 D L Lo oo G GG ( (( P RR o ee s aa i ltl )i) ve) 20 G GE EF F ( (P Ro es ai lt )ive) 104 20 G GE EF F ( (P Ro es ai lt )ive)
107 G GE EF F ( (P Ro es ai lt )ive) 0 105 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Number of Components (N) Number of Components (N) Number of Components (N) Number of Components (N)
(e)Gaussiansignal (f)Halfsinusoidsignal
Figure13. NumericalSimulationResultsofDifferentMixtures. Weshowacomparisonofaveragelossandstability(percentageof
successfulruns)fordifferentmixturemodelsoptimizedwithgradient-basedoptimizersacrossvaryingnumbersofcomponentsandweight
configurations(positivevs.realweights)onvarioussignaltypes(a-f).
18
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL egarevA
)elacs
gol(
ssoL egarevA
)%(
ytilibatS
)%(
ytilibatS
)%(
ytilibatS
)elacs
gol(
ssoL
egarevA
)elacs
gol(
ssoL egarevA
)elacs
gol(
ssoL egarevA
)%(
ytilibatS
)%(
ytilibatS
)%(
ytilibatSGaussianMixture LoGMixture DoGMixture GEFMixture
1O .0verfitting Gaussian Mixture to a square Funct T Gi ro aun ue,
s
s sN q ia= u na2 r M, e l io xts us r= e1.82 01 .. 80Overfitting LoG Mixture to a square Function, N= T L2 r o, u
G
elo Mss q is xu= ta u8 r re. e06 11 .. 02Overfitting DoG Mixture to a square Function, N= T Dr2 ou, Ge l o s Ms q is u x= ta ur1 re e.73 1O .0verfitting General Mixture to a square Function, N T G= r Eu2 Fe ,
M
sl qo ixus tas ur= re e0.44
0.8
0.8 0.6 0.8
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.0
0.2 0.2 0.2
0.2
0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.4 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a square Function, N=5, loss=0.48 Overfitting LoG Mixture to a square Function, N=5, loss=1.45 Overfitting DoG Mixture to a square Function, N=5, loss=0.86 Overfitting General Mixture to a square Function, N=5, loss=0.09
1.0 T Gr au ue s s sq iau na r Me ixture 1.0 T Lr ou Ge Msq ixu ta ur re e 1.0 T Dr ou Ge s Mq iu xta ur re e 1.0 T Gr Eu Fe Msq ixu ta ur re e
0.8 0.8 0.8 0.8
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.2
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a square Function, N=8, loss=nan Overfitting LoG Mixture to a square Function, N=8, loss=0.92 Overfitting DoG Mixture to a square Function, N=8, loss=0.29 Overfitting General Mixture to a square Function, N=8, loss=nan
1.0 T Gr au ue s s sq iau na r Me ixture 1.5 T Lr ou Ge Msq ixu ta ur re e 1.0 T Dr ou Ge s Mq iu xta ur re e 1.0 T Gr Eu Fe Msq ixu ta ur re e
0.8 1.0 0.8 0.8
0.6 0.5 0.6 0.6
0.4 0.4 0.4
0.0
0.2 0.2 0.2
0.5
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a square Function, N=10, loss=nan Overfitting LoG Mixture to a square Function, N=10, loss=0.87 Overfitting DoG Mixture to a square Function, N=10, loss=0.26 Overfitting General Mixture to a square Function, N=10, loss=nan
1.0 T Gr au ue s s sq iau na r Me ixture 1.5 T Lr ou Ge Msq ixu ta ur re e 1.0 T Dr ou Ge s Mq iu xta ur re e 1.0 T Gr Eu Fe Msq ixu ta ur re e
0.8 0.8
1.0 0.8
0.6 0.5 0.6 0.6
0.4 0.4 0.4
0.0
0.2 0.2 0.2
0.5
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a square Function, N=20, loss=nan Overfitting LoG Mixture to a square Function, N=20, loss=0.39 Overfitting DoG Mixture to a square Function, N=20, loss=0.03 Overfitting General Mixture to a square Function, N=20, loss=nan
1.0 T Gr au ue s s sq iau na r Me ixture 1.25 T Lr ou Ge Msq ixu ta ur re e 1.0 T Dr ou Ge s Mq iu xta ur re e 1.0 T Gr Eu Fe Msq ixu ta ur re e
0.8 1.00 0.8 0.8
0.75
0.6 0.50 0.6 0.6
0.4 0.25 0.4 0.4
0.00
0.2 0.25 0.2 0.2
0.0 0.50 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Figure14.NumericalSimulationExamplesofFittingSquareswithPositiveWeightsMixtures(N=2,5,8,and10).Weshowsome
fittingexamplesforSquaresignalswithpositiveweightsmixtures. ThefourmixturesusedfromlefttorightareGaussians,LoG,DoG,
andGeneralmixtures.Fromtoptobottom:N=2,8,and10components.Theoptimizedindividualcomponentsareshowningreen.Some
examplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures. NotethatGEFisveryefficientinfittingthe
SquarewithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
19
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Overfitting Gaussian Mixture to a square Function, N=2, loss=1.91 Overfitting LoG Mixture to a square Function, N=2, loss=8.06 Overfitting DoG Mixture to a square Function, N=2, loss=6.39 Overfitting General Mixture to a square Function, N=2, loss=0.83
6 True square 1.0 True square True square 8 True square
Gaussian Mixture 0.8 LoG Mixture 1.25 DoG Mixture 6 GEF Mixture
4 0.6 1.00 4
2 0.4 0.75 2 0
2
00 .. 02 000 ... 025 050 0
42
4 0.2 0.25 6
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.4 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 46verfitting Gaussian Mixture to a square Funct
T
Gi ro aun ue,
s
s
sN
q
ia=
u
na5
r
M,
e
l io xts us r= e0.57 1.0Overfitting LoG Mixture to a square Function, N=
T
L5
r
o,
u
G
elo Mss
q
is xu=
ta
u1
r
re. e12 1.0Overfitting DoG Mixture to a square Function, N=
T
Dr5 ou,
Ge
l o
s
Ms
q
is
u
x=
ta
ur0
re
e.86 O 68verfitting General Mixture to a square Function, N
T
G=
r
Eu5
Fe
,
M
sl qo ixus tas ur=
re
e0.16
0.5 0.8 4
2
0.0 0.6 2
0 0.4 0
0.5
2 2
0.2
4 1.0 4
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 6 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 3verfitting Gaussian Mixture to a square Funct
T
Gi ro aun ue,
s s
sN
q
ia=
u
na8
r
M,
e
l io xts us r= e0.24 4Overfitting LoG Mixture to a square Function, N=
T
L8
r
o,
u G
elo Mss
q
is xu=
ta
u0
r
re. e87 56Overfitting DoG Mixture to a square Function, N=
T
Dr8 ou,
Ge
l o
s
Ms
q
is
u
x=
ta
ur0
re
e.94 O 3verfitting General Mixture to a square Function, N
T
G=
r
Eu8
Fe
,
M
sl qo ixus tas ur=
re
e0.10
2 2 4 2
3
1 0 2 1
0 2 1 0
1 4 0 1 1
2 6 2 2
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a square Function, N=10, loss=nan Overfitting LoG Mixture to a square Function, N=10, loss=0.40 Overfitting DoG Mixture to a square Function, N=10, loss=0.20 Overfitting General Mixture to a square Function, N=10, loss=0.09
1.0 T Gr au ue s s sq iau na r Me ixture 2 T Lr ou Ge Msq ixu ta ur re e 1.5 T Dr ou Ge s Mq iu xta ur re e 3 T Gr Eu Fe Msq ixu ta ur re e
0.8 1 1.0 2
0.6 0.5 1 0
0.4 0.0 0
0.2 1 0.5 1
0.0 2 1.0 2
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a square Function, N=20, loss=nan Overfitting LoG Mixture to a square Function, N=20, loss=0.12 Overfitting DoG Mixture to a square Function, N=20, loss=0.01 Overfitting General Mixture to a square Function, N=20, loss=nan
1.0 T Gr au ue s s sq iau na r Me ixture 3 T Lr ou Ge Msq ixu ta ur re e 1.5 T Dr ou Ge s Mq iu xta ur re e 1.0 T Gr Eu Fe Msq ixu ta ur re e
0.8 2 1.0 0.8
0.6 1 0.5 0.6
0.4 0 0.0 0.4
0.2 1 0.2
0.5
2
0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Figure15. NumericalSimulationExamplesofFittingSquareswithRealWeightsMixtures(N=2,5,8,and10). Weshowsome
fittingexamplesforSquaresignalswithRealweightsmixtures(canbenegative). ThefourmixturesusedfromlefttorightareGaussians,
LoG,DoG,andGeneralmixtures. Fromtoptobottom:N=2,8,and10components. Theoptimizedindividualcomponentsareshownin
green.SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientin
fittingtheSquarewithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
20
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Ov1e0rfitting Gaussian Mixture to a parabolic Func T Gt ri ao u un e s , p sN ia ar= na 2 b M, o ill xio c ts us re=18.75 1O0 8verfitting LoG Mixture to a parabolic Function, N T Lr o= u Ge2 M, p l a io xrs ta us b r= o el1 ic3.26 O 8verfitting DoG Mixture to a parabolic Function, N T Dr= ou Ge2 , p M al io xrs a tus b= ro el3 ic7.52 Ov 8erfitting General Mixture to a parabolic Function T Gr, Eu N Fe = Mp2 a ixr, t al ubo ros els ic=2.95
8
6
6 6
6 4 4 2 4 4
2 0 2 2
2
0 0 0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Overfitting Gaussian Mixture to a parabolic Function, N=5, loss=1.98 Overfitting LoG Mixture to a parabolic Function, N=5, loss=6.95 Overfitting DoG Mixture to a parabolic Function, N=5, loss=1.25 Overfitting General Mixture to a parabolic Function, N=5, loss=0.08
8
T Gr au ue
s
p sia ar na b Mo il xic
ture 8
T Lr ou Ge Mpa ixr ta ub ro elic
8
T Dr ou Ge p Ma ixra tub ro elic
8
T Gr Eu Fe Mpa ixr ta ub ro elic
6
6 4 6 6
4 2 4 4
0
2 2 2
2
0 4 0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a parabolic Function, N=8, loss=0.14 Overfitting LoG Mixture to a parabolic Function, N=8, loss=1.55 Overfitting DoG Mixture to a parabolic Function, N=8, loss=0.83 Overfitting General Mixture to a parabolic Function, N=8, loss=0.03
8 T Gr au ue s p sia ar na b Mo il xic ture 8 T Lr ou Ge Mpa ixr ta ub ro elic 8 T Dr ou Ge p Ma ixra tub ro elic 8 T Gr Eu Fe Mpa ixr ta ub ro elic
6
6 6 6
4 4 2 4 4
2 0 2 2
0 2 0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Ove 8rfitting Gaussian Mixture to a parabolic Func T Gt ri ao u un e s , p sN ia ar= na 1 b M0 o il, xi c tlo urs es=0.08 O 8verfitting LoG Mixture to a parabolic Function, N T Lr o= u Ge1 M0 p, a i xl ro ta us b rs o e= lic0.42 O 8verfitting DoG Mixture to a parabolic Function, N T Dr= ou Ge1 0 p Ma, i xrlo a tubs rs o e= lic0.42 Ov 8erfitting General Mixture to a parabolic Function T G, r EuN Fe= Mp1 a i0 xr t, a ubl ro o es lis c=0.02
6 6 6 6
4 4 4 4
2
2 0 2 2
0 2 0 0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 8rfitting Gaussian Mixture to a parabolic Func T Gt ri ao u un e s , p sN ia ar= na 2 b M0 o il, xi c tlo urs es=0.02 O 8verfitting LoG Mixture to a parabolic Function, N T Lr o= u Ge2 M0 p, a i xl ro ta us b rs o e= lic0.41 O 8verfitting DoG Mixture to a parabolic Function, N T Dr= ou Ge2 0 p Ma, i xrlo a tubs rs o e= lic0.05 Ov 8erfitting General Mixture to a parabolic Function T G, r EuN Fe= Mp2 a i0 xr t, a ubl ro o es lis c=0.01
6 6 6 6
4 4 4 4
2
2 0 2 2
0 2 0 0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure16. NumericalSimulationExamplesofFittingparabolicswithPositiveWeightsMixtures(N=2,5,8,and10). Weshow
somefittingexamplesforparabolicsignalswithpositiveweightsmixtures. ThefourmixturesusedfromlefttorightareGaussians,LoG,
DoG,andGeneralmixtures.Fromtoptobottom:N=2,8,and10components.Theoptimizedindividualcomponentsareshowningreen.
SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientinfitting
theparabolicwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
21
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Overfitting Gaussian Mixture to a parabolic Function, N=2, loss=nan 1O0verfitting LoG Mixture to a parabolic Function, N=2, loss=13.26 Overfitting DoG Mixture to a parabolic Function, N=2, loss=289.49 Overfitting General Mixture to a parabolic Function, N=2, loss=nan
8 T Gr au ue s p sia ar na b Mo il xic ture 8 T Lr ou Ge Mpa ixr ta ub ro elic 10 T Dr ou Ge p Ma ixra tub ro elic 8 T Gr Eu Fe Mpa ixr ta ub ro elic
8
6 6 6 6
4 4
4 4
2 2
2 0 0 2
2 2
0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 4 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a parabolic Function, N=5, loss=nan Overfitting LoG Mixture to a parabolic Function, N=5, loss=1.23 Overfitting DoG Mixture to a parabolic Function, N=5, loss=1.99 Overfitting General Mixture to a parabolic Function, N=5, loss=nan
8
T Gr au ue
s
p sia ar na b Mo il xic
ture
10.0 T Lr ou Ge Mpa ixr ta ub ro elic
8
T Dr ou Ge p Ma ixra tub ro elic
8
T Gr Eu Fe Mpa ixr ta ub ro elic
7.5
6 5.0 6 6
2.5 4 0.0 4 4
2 2.5 2 2
5.0
0 7.5 0 0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a parabolic Function, N=8, loss=nan Overfitting LoG Mixture to a parabolic Function, N=8, loss=0.68 Overfitting DoG Mixture to a parabolic Function, N=8, loss=0.86 Overfitting General Mixture to a parabolic Function, N=8, loss=0.02
8
T Gr au ue
s
p sia ar na b Mo il xic
ture 8
T Lr ou Ge Mpa ixr ta ub ro elic
8
T Dr ou Ge p Ma ixra tub ro elic
8
T Gr Eu Fe Mpa ixr ta ub ro elic
6 6 6 6
4
4 2 4 4
2 0 2 2
2 0 0
0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Ove 8rfitting Gaussian Mixture to a parabolic Func T Gt ri ao u un e
s
, p sN ia ar= na 1 b M0 o il, xi c tlo urs es=0.02 10 7. .O0 5verfitting LoG Mixture to a parabolic Function, N T Lr o= u Ge1 M0 p, a
i
xl ro ta us b rs o e= lic1.20 O 8verfitting DoG Mixture to a parabolic Function, N T Dr= ou Ge1 0 p Ma, i xrlo a tubs rs o e= lic0.37 Ov 8erfitting General Mixture to a parabolic Function T G, r EuN Fe= Mp1 a i0 xr t, a ubl ro o es lis c=0.01
5.0
6 2.5 6 6
4 0.0 4 4
2.5
2 5.0 2 2
0 107 .. 05 0 0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 8rfitting Gaussian Mixture to a parabolic Func T Gt ri auo uen s , p s iN a ar= na b2 Mo0 il x, ic tl uo rs es=nan O 8verfitting LoG Mixture to a parabolic Function, N T Lr o= u Ge2 M0 p, a i xl ro ta us b rs o e= lic0.02 O 8verfitting DoG Mixture to a parabolic Function, N T Dr= ou Ge2 0 p Ma, i xrlo a tubs rs o e= lic0.08 Ov 8erfitting General Mixture to a parabolic Function T G, r EuN Fe= Mp2 a i0 xr t, a ubl ro o es lis c=0.01
6 6 6 6
4 4 4
4
2 2
2
2 0 0
0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 2 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 2 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure17. NumericalSimulationExamplesofFittingParabolicswithRealWeightsMixtures(N=2,5,8,and10). Weshowsome
fittingexamplesforparabolicsignalswithRealweightsmixtures(canbenegative).ThefourmixturesusedfromlefttorightareGaussians,
LoG,DoG,andGeneralmixtures. Fromtoptobottom:N=2,8,and10components. Theoptimizedindividualcomponentsareshownin
green.SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientin
fittingtheparabolicwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
22
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Ov 1e .0rfitting Gaussian Mixture to a exponential Fun
T
Grc aut ueio
s
en
sx
i,
a
pN
no
=
n
Me2 ixn,
t
t
ul io
a
rs
l
es=0.01 1O .0verfitting LoG Mixture to a exponential Functi
T
Lo
r
on
u
G,
e
N Me=
x
ixp2
to
u,
n
rl eeo ns ts i= al0.35 1O .0verfitting DoG Mixture to a exponential Functi
T
Do
r
on
u
Ge, N
e
Mx= ixp2
to
u,
n
rl
e
eo ns ts ia= l1.68 O 1v .0erfitting General Mixture to a exponential Func
T
Grt Euio
Fe
n Me,
x
iN
xp
t=
o un
r2 ee,
n
l to ias ls=nan
0.8 0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4
0.4 0.4 0.4
0.2
0.2 0.0 0.2 0.2
0.0 0.2 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ov 01e .. 80rfitting Gaussian Mixture to a exponential Fun T Grc aut uei so e sn x ia, p nN o n M= e i5 xn, tt uil ao rl ess=nan 01O .. 80verfitting LoG Mixture to a exponential Functi T Lo r on u G, e N Me= x ixp5 to u, n rl eeo ns ts i= al0.02 01O .. 80verfitting DoG Mixture to a exponential Functi T Do r on u Ge, N e Mx= ixp5 to u, n rl e eo ns ts ia= l0.10 O 01v .. 80erfitting General Mixture to a exponential Func T Grt Euio Fe n Me, x iN xp t= o un r5 ee, n l to ias ls=nan
0.6 0.6 0.6 0.6
0.4
0.4 0.4 0.4
0.2
0.2 0.2 0.2
0.0
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ov 1e .0rfitting Gaussian Mixture to a exponential Fun
T
Grc aut uei so
e
sn
x
ia,
p
nN
o n
M=
e
i8 xn,
tt
uil ao
rl
ess=nan 1O .0verfitting LoG Mixture to a exponential Functi
T
Lo
r
on
u
G,
e
N Me=
x
ixp8
to
u,
n
rl eeo ns ts i= al0.01 1O .0verfitting DoG Mixture to a exponential Functi
T
Do
r
on
u
Ge, N
e
Mx= ixp8
to
u,
n
rl
e
eo ns ts ia= l0.02 O 1v .0erfitting General Mixture to a exponential Func
T
Grt Euio
Fe
n Me,
x
iN
xp
t=
o un
r8 ee,
n
l to ias ls=nan
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4
0.4 0.4 0.4
0.2
0.2 0.2 0.2
0.0
0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.2 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Ov 01e ..r 80fitting Gaussian Mixture to a exponential Fun T Gc r auti ueo s n e sx, ia pN no= n M1 e ixn0 tt, ui al ro l ess=nan O 01 ..v 80erfitting LoG Mixture to a exponential Functio T Ln r ou, G eN Me= x ix1 p t0 o un, r eelo ns ts ia= l0.00 O 01 ..v 80erfitting DoG Mixture to a exponential Functio T Drn ou, Ge N e M= x ix1 p to0 un, r e elo ns tis a= l0.00 Ov 01e .. 80rfitting General Mixture to a exponential Func T Gt ri Euo Fen M, e xN ixp= to u1 n re0 en, tl io as ls=nan
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.2
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ov 1e .r 0fitting Gaussian Mixture to a exponential Fun
T
Gc
r
auti ueo
s
n
e
sx,
ia
pN no=
n
M2
e
ixn0 tt,
ui
al ro
l
ess=nan O 1.v 0erfitting LoG Mixture to a exponential Functio
T
Ln
r
ou,
G
eN Me=
x
ix2
p
t0
o
un,
r
eelo ns ts ia= l0.00 O 1.v 0erfitting DoG Mixture to a exponential Functio
T
Drn ou,
Ge
N
e
M=
x
ix2
p
to0 un,
r e
elo ns tis a= l0.00 Ov 1e .0rfitting General Mixture to a exponential Func
T
Gt ri Euo Fen M,
e
xN ixp=
to
u2
n
re0 en, tl io as ls=nan
0.8 0.8 0.8 0.8
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.0
0.2 0.2 0.2 0.2
0.0 0.4 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure18. NumericalSimulationExamplesofFittingExponentialswithPositiveWeightsMixtures(N=2,5,8,and10). Weshow
somefittingexamplesforexponentialsignalswithpositiveweightmixtures.ThefourmixturesusedfromlefttorightareGaussians,LoG,
DoG,andGeneralmixtures.Fromtoptobottom:N=2,8,and10components.Theoptimizedindividualcomponentsareshowningreen.
SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientinfitting
theexponentialwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
23
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Ov 01e .. 80rfitting Gaussian Mixture to a exponential Fun T Grc aut uei so e sn x ia, p nN o n M= e i2 xn, tt uil ao rl ess=nan 01O .. 80verfitting LoG Mixture to a exponential Functi T Lo r on u G, e N Me= x ixp2 to u, n rl eeo ns ts i= al1.39 01 ..O 70 50verfitting DoG Mixture to a exponential Functi T Do r on u Ge, N e Mx= ixp2 to u, n rl e eo ns ts ia= l1.01 O 11 ..v 02e 05rfitting General Mixture to a exponential Func T Gt r Ei uo Fen Me, x iN xp t= o un2 ree, nlo tis as l=0.01
0.6 0.50 0.75
0.6 0.4 0.25 0.50 0.25
0.4 0.2 0.00 0.00
0.2 0.0 0.25 0.25
0.2 0.50 0.50
0.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 0.75 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 0.75 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ov 01e .. 80rfitting Gaussian Mixture to a exponential Fun T Grc aut uei so e sn x ia, p nN o n M= e i5 xn, tt uil ao rl ess=nan 01 ..O 70 50verfitting LoG Mixture to a exponential Functi T Lo r on u G, e N Me= x ixp5 to u, n rl eeo ns ts i= al0.02 01O .. 80verfitting DoG Mixture to a exponential Functi T Do r on u Ge, N e Mx= ixp5 to u, n rl e eo ns ts ia= l0.04 O 1v .e 0rfitting General Mixture to a exponential Func T Gt r Ei uo Fen Me, x iN xp t= o un5 ree, nlo tis as l=0.00
0.50 0.6 0.5
0.6 0.25 0.4 0.0
0.4 0.00 0.2
0.2 0.25 0.0 0.5
0.50 0.2 1.0
0.0 0.4
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Overfitting Gaussian Mixture to a exponential Function, N=8, loss=0.01 Overfitting LoG Mixture to a exponential Function, N=8, loss=0.01 1O.5verfitting DoG Mixture to a exponential Function, N=8, loss=0.01 Overfitting General Mixture to a exponential Function, N=8, loss=0.00
2.5 T Gr au ue s e sx iap no n Me ixn tt uia rl e 1.0 T Lr ou Ge Mex ixp to un reential 1.0 T Dr ou Ge e Mx ixp to un re ential 1.0 T Gr Eu Fe Mex ixp to un reential
2.0
1.5 0.5 0.5 0.5
1.0 0.0 0.0 0.0 0.5 0.5
0.0 1.0 0.5 0.5
0.5 1.0
1.0 1.5 1.5 1.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 1.r 5fitting Gaussian Mixture to a exponential Fun T Gc rt aui uo e s n e s, x i aN p no= n M1 e i0 xn t, t u il a ro l ess=0.00 O 11 ..v 05erfitting LoG Mixture to a exponential Functio T Ln r ou, G eN Me= x ix1 p t0 o un, r eelo ns ts ia= l0.00 01O .. 70v 50erfitting DoG Mixture to a exponential Functio T Drn ou, Ge N e M= x ix1 p to0 un, r e elo ns tis a= l0.00 Ov 1e .0rfitting General Mixture to a exponential Function, N=10, loss=0.00
1.0 0.5 0.50 0.5
0.5 0.25 0.0
0.0 0.0 0.00 0.5
0.5 0.25
0.5 0.50 1.0
1.0 True exponential
1.0 0.75 1.5 GEF Mixture
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 11 ..r 05fitting Gaussian Mixture to a exponential Fun T Gc rt aui uo e s n e s, x i aN p no= n M2 e i0 xn t, t u il a ro l ess=0.01 O 12 ..v 50erfitting T L r oL u Go e G Me x iM xp ti o ux n rt eu enre ti at lo a exponential Function, N=20, loss=0.00 O 1.v 0erfitting DoG Mixture to a exponential Functio T Drn ou, Ge N e M= x ix2 p to0 un, r e elo ns tis a= l0.00 Ov 1e .5rfitting General Mixture to a exponential Func T Gt ri Eo u Fn e , Me N x ixp= to u2 n r0 ee, n l to ias ls=0.00
0.5 1.0 0.5 1.0
0.0 0.5 0.5
0.5 0.0 0.0 0.0
11 .. 50 10 .. 05 0.5 10 .. 05
2.0 1.5 1.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 1.5 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure19. NumericalSimulationExamplesofFittingExponentialswithRealWeightsMixtures(N=2, 5, 8, and10). Weshow
somefittingexamplesforexponentialsignalswithRealweightsmixtures(canbenegative). Thefourmixturesusedfromlefttorightare
Gaussians,LoG,DoG,andGeneralmixtures. Fromtoptobottom: N=2,8,and10components. Theoptimizedindividualcomponents
areshowningreen. SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures. NotethatGEFis
veryefficientinfittingtheexponentialwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
24
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
O 23 ..v 50erfitting Gaussian Mixture to a triangle Funct T Gi ro au un e s, t sN r ii aa= nn2 g M, l e il xo ts us re=0.34 23 .. 50Overfitting LoG Mixture to a triangle Function, N= T Lr o2 u G, e l Mo trs ii xas tn= ug r0 ele.90 23 .. 50Overfitting DoG Mixture to a triangle Function, N= T Dr o2 u Ge, l t Mo rs i ia xs n t= ug r4 l ee.43 23O .. 50verfitting General Mixture to a triangle Function, T GN r Eu= Fe2 Mt, r il i xao tns ugs r= l ee0.05
2.0
2.0 1.5 2.0 2.0
1.5 1.0 1.5 1.5
1.0 0.5 1.0 1.0
0.5 0.0 0.5 0.5
0.5
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 23 ..v 50erfitting Gaussian Mixture to a triangle Funct T Gi ro au un e s, t sN r ii aa= nn5 g M, l e il xo ts us re=0.02 23 .. 50Overfitting LoG Mixture to a triangle Function, N= T Lr o5 u G, e l Mo trs ii xas tn= ug r0 ele.27 23 .. 50Overfitting DoG Mixture to a triangle Function, N= T Dr o5 u Ge, l t Mo rs i ia xs n t= ug r0 l ee.06 23O .. 50verfitting General Mixture to a triangle Function, T GN r Eu= Fe5 Mt, r il i xao tns ugs r= l ee0.00
2.0 2.0 2.0 2.0
1.5
1.5 1.0 1.5 1.5
1.0 0.5 1.0 1.0
0.5 0.0 0.5 0.5
0.0 0.5 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 23 ..v 50erfitting Gaussian Mixture to a triangle Funct T Gi ro au un e s, t sN r ii aa= nn8 g M, l e il xo ts us re=0.01 23 .. 50Overfitting LoG Mixture to a triangle Function, N= T Lr o8 u G, e l Mo trs ii xas tn= ug r0 ele.05 23 .. 50Overfitting DoG Mixture to a triangle Function, N= T Dr o8 u Ge, l t Mo rs i ia xs n t= ug r0 l ee.01 23O .. 50verfitting General Mixture to a triangle Function, T GN r Eu= Fe8 Mt, r il i xao tns ugs r= l ee0.00
2.0 2.0 2.0 2.0
1.5
1.5 1.0 1.5 1.5
1.0 0.5 1.0 1.0
0.5 0.0 0.5 0.5
0.0 0.5 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 23v .. 50erfitting Gaussian Mixture to a triangle Funct T Gio r aun ue,
s
tN sr ii= aa nn1 g0 Ml, e
i
xlo tus rs e=0.01 23 .. 50Overfitting LoG Mixture to a triangle Function, N= T L1 r ou0 Ge, Mtlo r ii xs a ts n u= g rel0 e.04 23 .. 50Overfitting DoG Mixture to a triangle Function, N= T Dr1 ou0 Ge, t Ml ro i ias xns tu= g rl e0 e.01 O 23 ..v 50erfitting General Mixture to a triangle Function, N T Gr= Eu F1 e 0 Mtr,
i
i xalo tn us g rs l ee=0.00
2.0 2.0 2.0 2.0
1.5
1.5 1.0 1.5 1.5
1.0 0.5 1.0 1.0
0.5 0.0 0.5 0.5
0.0 0.5 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 23 ..v 50erfitting Gaussian Mixture to a triangle Funct T Gi ro aun ue,
s
t sN r ii a= a nn2 g M0 le i,
x
l to us rs e=nan 23 .. 50Overfitting LoG Mixture to a triangle Function, N= T L2 r ou0 Ge, Mtlo r ii xs a ts n u= g rel0 e.01 23 .. 50Overfitting DoG Mixture to a triangle Function, N= T Dr2 ou0 Ge, t Ml ro i ias xns tu= g rl e0 e.00 23O .. 50verfitting General Mixture to a triangle Function, N T Gr= Eu Fe2 M0 tr, ii xal to n us g rs l ee=nan
2.0
2.0 2.0 2.0
1.5
1.5 1.0 1.5 1.5
1.0 0.5 1.0 1.0
0.5 0.0 0.5 0.5
0.5
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Figure20.NumericalSimulationExamplesofFittingTriangleswithPositiveWeightsMixtures(N=2,5,8,and10).Weshowsome
fittingexamplesfortrianglesignalswithpositiveweightmixtures.ThefourmixturesusedfromlefttorightareGaussians,LoG,DoG,and
Generalmixtures. Fromtoptobottom: N=2,8,and10components. Theoptimizedindividualcomponentsareshowningreen. Some
examplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures. NotethatGEFisveryefficientinfittingthe
trianglewithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
25
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Overfitting Gaussian Mixture to a triangle Function, N=2, loss=nan Overfitting LoG Mixture to a triangle Function, N=2, loss=0.90 Overfitting DoG Mixture to a triangle Function, N=2, loss=89.91 Overfitting General Mixture to a triangle Function, N=2, loss=nan
23 .. 50 T Gr au ue
s
t sr ii aa nn g Mle
ixture
23 .. 50 T Lr ou Ge Mtr ii xa tn ug rele 23 .. 50 T Dr ou Ge t Mri ia xn tug rl ee 23 .. 50 T Gr Eu Fe Mtr ii xa tn ug rl ee
2.0
2.0 2.0 2.0
1.5
1.5 1.0 1.5 1.5
1.0 0.5 1.0 1.0
0.5 0.0 0.5 0.5
0.5
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Ov 3erfitting Gaussian Mixture to a triangle Funct T Gi ro au un e s, t sN r ii aa= nn5 g M, l e il xo ts us re=0.04 3Overfitting LoG Mixture to a triangle Function, N= T Lr o5 u G, e l Mo trs ii xas tn= ug r3 ele.52 3Overfitting DoG Mixture to a triangle Function, N= T Dr o5 u Ge, l t Mo rs i ia xs n t= ug r0 l ee.03 3O .0verfitting General Mixture to a triangle Function, T GN r Eu= Fe5 Mt, r il i xao tns ugs r= l ee0.01
2 2 2.5
2 2.0
1 1 1 1.5
0 0 0 01 .. 50
1 1 0.0
1
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 0.5 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
O 23 ..v 50erfitting Gaussian Mixture to a triangle Funct T Gi ro au un e s, t sN r ii aa= nn8 g M, l e il xo ts us re=0.00 23Overfitting LoG Mixture to a triangle Function, N= T Lr o8 u G, e l Mo trs ii xas tn= ug r0 ele.27 23 .. 50Overfitting DoG Mixture to a triangle Function, N= T Dr o8 u Ge, l t Mo rs i ia xs n t= ug r0 l ee.35 23O .. 50verfitting General Mixture to a triangle Function, T GN r Eu= Fe8 Mt, r il i xao tns ugs r= l ee0.00
2.0 2.0 2.0
1.5 1 1.5 1.5 001 ... 050 0 01 .. 50 01 .. 50
0.5 1 0.0 0.0
1.0 0.5 0.5
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ov 3erfitting Gaussian Mixture to a triangle Funct
T
Gio
r
aun ue,
s
tN
sr
ii=
aa
nn1 g0 Ml,
e i
xlo tus rs e=0.05 3Overfitti
T
Ln
r
og
u G
eL Mo trG
ii xa
tnM
ug
ri elx eture to a triangle Function, N=10, loss=0.27
23 ..
50Overfitting DoG Mixture to a triangle Function, N=
T
Dr1 ou0 Ge,
t
Ml ro
i
ias xns tu=
g rl
e0 e.01 Ov 3erfitting General Mixture to a triangle Function, N
T
Gr=
Eu
F1
e
0 Mtr,
i i
xalo
tn
us
g
rs
l
ee=0.00
2 2 2.0 2
1 1 1.5 1 1.0
0 0 0.5 0
1 0.0
1
0.5 1
2 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Ov 3erfitting Gaussian Mixture to a triangle Funct
T
Gio
r
aun ue,
s
tN
sr
ii=
aa
nn2 g0 Ml,
e
i
xlo tus rs e=0.00 3Overfitting LoG Mixture to a triangle Function, N=
T
L2
r
ou0 Ge, Mtlo
r ii
xs
a
ts
n
u=
g
rel0 e.01
23 ..
50Overfitting DoG Mixture to a triangle Function, N=
T
Dr2 ou0 Ge,
t
Ml ro
i
ias xns tu=
g rl
e0 e.00 Ov 3erfitting General Mixture to a triangle Function, N
T
Gr=
Eu
F2
e
0 Mtr,
i
i
xalo
tn
us
g
rs
l
ee=0.00
2 2 2.0 2
1 1.5
1 1 1.0
0 0 0.5 0
0.0
1
1 0.5 1
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Figure21. NumericalSimulationExamplesofFittingTriangleswithRealWeightsMixtures(N=2,5,8,and10). Weshowsome
fittingexamplesfortrianglesignalswithRealweightsmixtures(canbenegative).ThefourmixturesusedfromlefttorightareGaussians,
LoG,DoG,andGeneralmixtures. Fromtoptobottom:N=2,8,and10components. Theoptimizedindividualcomponentsareshownin
green.SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientin
fittingthetrianglewithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
26
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Overfitting Gaussian Mixture to a gaussian Function, N=2, loss=0.00 Overfitting LoG Mixture to a gaussian Function, N=2, loss=0.02 Overfitting DoG Mixture to a gaussian Function, N=2, loss=0.92 Overfitting General Mixture to a gaussian Function, N=2, loss=0.00
1.0 T Gr au ue
s
g sia au ns s Mia ixn
ture
1.0 T Lr ou Ge Mga ixu ts us ri ean 1.0 T Dr ou Ge g Ma ixu ts us ri ean 1.0 T Gr Eu Fe Mga ixu ts us ri ean
0.8 0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.0 0.2 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a gaussian Function, N=5, loss=0.00 Overfitting LoG Mixture to a gaussian Function, N=5, loss=0.00 Overfitting DoG Mixture to a gaussian Function, N=5, loss=0.01 Overfitting General Mixture to a gaussian Function, N=5, loss=0.00
1.0 T Gr au ue
s
g sia au ns s Mia ixn
ture
1.0 T Lr ou Ge Mga ixu ts us ri ean 1.0 T Dr ou Ge g Ma ixu ts us ri ean 1.0 T Gr Eu Fe Mga ixu ts us ri ean
0.8 0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.0 0.2 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 1.v 0erfitting Gaussian Mixture to a gaussian Func
T
Grt auio uen
s g
s, iaN
au
n=
s s
M8 ia,
ix
nl to us rs e=nan 1.0Overfitting LoG Mixture to a gaussian Function, N
T Lr
o=
u
Ge8 M,
g
l
a
io
xu
ts
s
us
s
r=
i
ea0 n.00 1.0Overfitting DoG Mixture to a gaussian Function,
T
DN
r
o=
u
Ge8 g,
M
al io xus
ts
us
s
r=
i
ea0 n.00 1O .0verfitting General Mixture to a gaussian Function
T
Gr,
E
uN
Fe
= Mg8
a
ix,
u
tsl uo
s
rs
i
eas n=nan
0.8 0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.0 0.2 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 01v .. 80erfitting Gaussian Mixture to a gaussian Func T Gt ri auo uen
s
, g
s
N ia au= ns 1 s Mi0 a ix, n tl uo rs es=nan 01 ..O 80verfitting LoG Mixture to a gaussian Function, N T L= r ou G1 e 0 Mg, a il xo u tss uss ri e= an0.00 01O .. 80verfitting DoG Mixture to a gaussian Function, N T Dr= ou G1 e0 g M, a il xuo tss uss ri e= an0.00 O 01 ..v 80erfitting General Mixture to a gaussian Function T G, r EN u Fe= Mg1 a i0 xu, ts ul so ri eass n=nan
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.2
0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.4 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 0.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Overfitting Gaussian Mixture to a gaussian Function, N=20, loss=nan Overfitting LoG Mixture to a gaussian Function, N=20, loss=0.00 Overfitting DoG Mixture to a gaussian Function, N=20, loss=0.00 Overfitting General Mixture to a gaussian Function, N=20, loss=nan
1.0 T Gr au ue
s
g sia au ns s Mia ixn
ture
1.5 T Lr ou Ge Mga ixu ts us ri ean 1.0 T Dr ou Ge g Ma ixu ts us ri ean 1.0 T Gr Eu Fe Mga ixu ts us ri ean
0.8 1.0 0.8 0.8
0.6 0.5 0.6 0.6
0.4 0.4 0.4
0.0
0.2 0.2 0.2
0.5
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
Figure22. NumericalSimulationExamplesofFittingGaussianswithPositiveWeightsMixtures(N=2,5,8,and10). Weshow
somefittingexamplesforGaussiansignalswithpositiveweightmixtures. ThefourmixturesusedfromlefttorightareGaussians,LoG,
DoG,andGeneralmixtures.Fromtoptobottom:N=2,8,and10components.Theoptimizedindividualcomponentsareshowningreen.
SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientinfitting
theGaussianwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
27
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
O 01v .. 80erfitting Gaussian Mixture to a gaussian Func T Gt ri auo uen
s
, g
s
iaN au= ns s2 Mi, a
i
xnlo tus rs e=0.00 01 .. 80Overfitting LoG Mixture to a gaussian Function, N T Lr o= u Ge2 M, g l a io xu ts s us s r= i ea3 n.28 011 ... 802Overfitting DoG Mixture to a gaussian Function, T DN r o= u Ge2 g,
M
al io xus ts us s r= i ea4 n.55 O 01 ..v 80erfitting General Mixture to a gaussian Function T G, r EuN Fe= Mg2 a i, xu tl s uo s rs i eas n=0.00
0.6 0.6 0.6 0.6
0.4 0.4
0.4 0.2 0.2 0.4
0.0
0.2 0.0 0.2
0.2
0.0 0.2 0.4 0.0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
x x x x
O 01v .. 80erfitting Gaussian Mixture to a gaussian Func T Gt ri auo uen s , g s iaN au= ns s5 Mi, a i xnlo tus rs e=0.00 2Overfitti T Ln r og u G eL Mo gaG ixu tM s us ri i ex at nure to a gaussian Function, N=5, loss=0.00 01 .. 80Overfitting DoG Mixture to a gaussian Function, T DN r o= u Ge5 g, M al io xus ts us s r= i ea0 n.28 11O .. 02v 05erfitting General Mixture to a gaussian Function T G, r EuN Fe= Mg5 a i, xu tl s uo s rs i eas n=0.00
0.6 1 0.6 0.75
0.4 0.4 0.50 0.2 0 0.2 0.25
0.0 0.0 0.00
0.2 1 0.2 0.25
0.4 0.4 0.50
0.6 2 0.75
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
O 1v .0erfitting Gaussian Mixture to a gaussian Func T Gt ri auo uen s , g s iaN au= ns s8 Mi, a i xnlo tus rs e=0.00 11 .. 05Overfitting LoG Mixture to a gaussian Function, N T Lr o= u Ge8 M, g l a io xu ts s us s r= i ea0 n.00 01 .. 80Overfitting DoG Mixture to a gaussian Function, T DN r o= u Ge8 g, M al io xus ts us s r= i ea0 n.00 O 1.v 0erfitting General Mixture to a gaussian Function T G, r EuN Fe= Mg8 a i, xu tl s uo s rs i eas n=0.00
0.5 0.6 0.5
0.5 0.0
0.0 0.4 0.5
0.5 0.0 0.2 1.0
1.0 0.5 0.0 1.5
1.5 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 0.2 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 2.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 2rfitting Gaussian Mixture to a gaussian Funct T Gi ro aun ue s, g sN ia a= u ns1 s M0 ia i, xn l to us res=0.00 O 1verfitting LoG Mixture to a gaussian Function, N T L= r ou G1 e 0 Mg, a il xo u tss uss ri e= an0.00 01O .. 80verfitting DoG Mixture to a gaussian Function, N T Dr= ou G1 e0 g M, a il xuo tss uss ri e= an0.00 O 12v .. 50erfitting General Mixture to a gaussian Function, T
G
rN Eu F= e M1 ga0 ixu,
t
s ulo s ri es as n=0.00
1 0.6 1.0
0 0.4 0.5 0 0.2 0.0
1 0.0 0.5
1 0.2 1.0
2 0.4 1.5
2 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 0.6 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 2.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
O 1v .e 0rfitting Gaussian Mixture to a gaussian Funct T Gi ro aun ue s, g sN ia a= u ns2 s M0 ia i, xn l to us res=0.00 11 ..O 05verfitting LoG Mixture to a gaussian Function, N T L= r ou G2 e 0 Mg, a il xo u tss uss ri e= an0.00 1O .0verfitting DoG Mixture to a gaussian Function, N T Dr= ou G2 e0 g M, a il xuo tss uss ri e= an0.00 O 1v .0erfitting General Mixture to a gaussian Function, T
G
rN Eu F= e M2 ga0 ixu,
t
s ulo s ri es as n=0.00
0.5 00 .. 05 0.5 0.5
0.0 0.5 0.0 0.0
1.0 0.5
0.5 1.5 0.5
2.0 1.0
1.0 2.5 1.0 1.5
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure23. NumericalSimulationExamplesofFittingGaussianswithRealWeightsMixtures(N=2,5,8,and10). Weshowsome
fittingexamplesforGaussiansignalswithRealweightsmixtures(canbenegative).ThefourmixturesusedfromlefttorightareGaussians,
LoG,DoG,andGeneralmixtures. Fromtoptobottom:N=2,8,and10components. Theoptimizedindividualcomponentsareshownin
green.SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientin
fittingtheGaussianwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
28
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Ove 1.r 0fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc uet s i ho sian al, nf _N s M= in ixu2 ts, u ol ro i edss=0.04 O 1.v 0erfitting LoG Mixture to a half_sinusoid Func T Lt r oi uo Gen M, h N a ixl= f t_ u2 s ri e, n l uo ss os id=0.12 O 1.v 0erfitting DoG Mixture to a half_sinusoid Func T Dt r oi uo Gen h M, aN ixlf= t_ us2 rin, e ulo ss os id=0.48 Ov 1e .0rfitting General Mixture to a half_sinusoid Fu T Gn rc Eut Fei o Mhn a ix, l f tN _ us= ri en2 u, s l oo is ds=0.02
0.8 0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.2
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 1.r 0fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc uet s i ho sian al, nf _N s M= in ixu5 ts, u ol ro i edss=0.00 O 1.v 0erfitting LoG Mixture to a half_sinusoid Func T Lt r oi uo Gen M, h N a ixl= f t_ u5 s ri e, n l uo ss os id=0.09 O 1.v 0erfitting DoG Mixture to a half_sinusoid Func T Dt r oi uo Gen h M, aN ixlf= t_ us5 rin, e ulo ss os id=0.01 Ov 1e .0rfitting General Mixture to a half_sinusoid Fu T Gn rc Eut Fei o Mhn a ix, l f tN _ us= ri en5 u, s l oo is ds=0.00
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.0 0.2 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 01 ..r 80fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc uet s i ho sian al, nf _N s M= in ixu8 ts, u ol ro i edss=0.00 O 01 ..v 80erfitting LoG Mixture to a half_sinusoid Func T Lt r oi uo Gen M, h N a ixl= f t_ u8 s ri e, n l uo ss os id=0.01 O 01 ..v 80erfitting DoG Mixture to a half_sinusoid Func T Dt r oi uo Gen h M, aN ixlf= t_ us8 rin, e ulo ss os id=0.01 Ov 01e .. 80rfitting General Mixture to a half_sinusoid Fu T Gn r Euc Feti o Mhn a ix, lf t_N us r= i en8 u, s olo idss=nan
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.0 0.2 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 01r .. 80fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc ueti s o h sn ia a, l nf _N s M= in ix1 u t0 s uo, r i el doss=nan O 01v .. 80erfitting LoG Mixture to a half_sinusoid Funct T Li r oo u Gn e , M hN a ix= lf t_ u1 s r0 i en, ulo sos is d=0.01 O 01v .. 80erfitting DoG Mixture to a half_sinusoid Func T Dti ro ou Gn e , h M N a ix= lf t_1 us r0 in e, u l so os is d=0.00 Ov 01e ..r 80fitting General Mixture to a half_sinusoid Fun T Grc Eut Feio Mhn a i, x lfN t_ us= ri en1 u0 s, o l io dss=nan
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.0
0.2 0.2 0.2
0.2
0.0 0.4 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 1r .0fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc ueti s o h sn ia a, l nf _N s M= in ix2 u t0 s uo, r i el doss=nan O 1v .0erfitting LoG Mixture to a half_sinusoid Funct T Li r oo u Gn e , M hN a ix= lf t_ u2 s r0 i en, ulo sos is d=0.00 O 1v .0erfitting DoG Mixture to a half_sinusoid Func T Dti ro ou Gn e , h M N a ix= lf t_2 us r0 in e, u l so os is d=0.00 Ov 1e .r 0fitting General Mixture to a half_sinusoid Fun T Grc Eut Feio Mhn a i, x lfN t_ us= ri en2 u0 s, o l io dss=nan
0.8 0.8 0.8 0.8
0.6
0.6 0.4 0.6 0.6
0.4 0.2 0.4 0.4
0.0
0.2 0.2 0.2 0.2
0.0 0.4 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure24.NumericalSimulationExamplesofFittingHalfsinusoidswithPositiveWeightsMixtures(N=2,5,8,and10).Weshow
somefittingexamplesforhalfsinusoidsignalswithpositiveweightsmixtures. ThefourmixturesusedfromlefttorightareGaussians,
LoG,DoG,andGeneralmixtures. Fromtoptobottom:N=2,8,and10components. Theoptimizedindividualcomponentsareshownin
green.SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures.NotethatGEFisveryefficientin
fittingthehalfsinusoidwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
29
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yGaussianMixture LoGMixture DoGMixture GEFMixture
Ove 1.r 0fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc uet s i ho sian al, nf _N s M= in ixu2 ts, u ol ro i edss=0.04 O 1.v 0erfitting LoG Mixture to a half_sinusoid Func T Lt r oi uo Gen M, h N a ixl= f t_ u2 s ri e, n l uo ss os id=0.12 O 1.v 0erfitting DoG Mixture to a half_sinusoid Func T Dt r oi uo Gen h M, aN ixlf= t_ us2 rin, e ulo ss os id=0.48 Ov 1e .0rfitting General Mixture to a half_sinusoid Fu T Gn rc Eut Fei o Mhn a ix, l f tN _ us= ri en2 u, s l oo is ds=0.02
0.8 0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4
0.4 0.2 0.4 0.4
0.2 0.0 0.2 0.2
0.2
0.0 0.0 0.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 1.r 5fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc uet s i ho sian al, nf _N s M= in ixu5 ts, u ol ro i edss=0.01 O 22 ..v 05erfitting LoG Mixture to a half_sinusoid Func T Lt r oi uo Gen M, h N a ixl= f t_ u5 s ri e, n l uo ss os id=0.08 O 1.v 5erfitting DoG Mixture to a half_sinusoid Func T Dt r oi uo Gen h M, aN ixlf= t_ us5 rin, e ulo ss os id=0.01 Ov 2e .0rfitting General Mixture to a half_sinusoid Fu T Gn rc Eut Fei o Mhn a ix, l f tN _ us= ri en5 u, s l oo is ds=0.00
1.0 1.5 1.0 1.5
0.5 1.0 0.5 1.0
0.5 0.0 0.0 0.0 0.5
0.5 0.5 0.5 0.0
1.0 1.0 1.0 0.5
1.5 1.5 1.5 1.0
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 1.r 5fitting Gaussian Mixture to a half_sinusoid Fu T Gn r auc uet s i ho sian al, nf _N s M= in ixu8 ts, u ol ro i edss=0.00 O 11 ..v 05erfitting LoG Mixture to a half_sinusoid Func T Lt r oi uo Gen M, h N a ixl= f t_ u8 s ri e, n l uo ss os id=0.02 O 01 ..v 80erfitting DoG Mixture to a half_sinusoid Func T Dt r oi uo Gen h M, aN ixlf= t_ us8 rin, e ulo ss os id=0.00 Ov 11e .. 05rfitting General Mixture to a half_sinusoid Fu T Gn rc Eut Fei o Mhn a ix, l f tN _ us= ri en8 u, s l oo is ds=0.00
1.0 0.5 0.6 0.5
0.5 0.0 0.4 0.0
0.2
0.5 0.5
0.0 0.0
1.0 1.0
0.2
0.5 1.5 0.4 1.5
10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 1r .f 5itting Gaussian Mixture to a half_sinusoid Fu T Gn rc aut uei so h sn ia a, l nfN _ s M= in i1 xu0 ts u, o rl i edoss=0.00 O 12v .. 50erfitting LoG Mixture to a half_sinusoid Funct T Li r oo u Gn e , M hN a ix= lf t_ u1 s r0 i en, ulo sos is d=0.01 O 01v .. 80erfitting DoG Mixture to a half_sinusoid Func T Dti ro ou Gn e , h M N a ix= lf t_1 us r0 in e, u l so os is d=0.00 Ove 1.r 5fitting General Mixture to a half_sinusoid Fun T Gc r Eut Fi eo Mn ha, ix lN f t_ u= s ri e1 n0 us, olo idss=0.00
1.0
1.0 0.6 1.0
0.5
0.5 0.0 0.4 0.5 0.0 0.5 0.2 0.0
0.5 1.0 0.0 0.5
1.5 0.2
1.0 2.0 0.4 1.0
10.0 7.5 5.0 2.5 0x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Ove 22r ..f 05itting Gaussian Mixture to a half_sinusoid Fu T Gn rc aut uei so h sn ia a, l nfN _ s M= in i2 xu0 ts u, o rl i edoss=0.00 O 12v .. 50erfitting T LL r ouo GeG Mh M a ixli f tx _ ut s ru i enr ue s t oo id a half_sinusoid Function, N=20, loss=0.00 01O .. 70v 50erfitting DoG Mixture to a half_sinusoid Func T Dti ro ou Gn e , h M N a ix= lf t_2 us r0 in e, u l so os is d=0.00 Ove 2.r 0fitting General Mixture to a half_sinusoid Fun T Gc r Eut Fi eo Mn ha, ix lN f t_ u= s ri e2 n0 us, olo idss=0.00
1.5 1.0 0.50 1.5
1.0 0.5 0.25 1.0
0.5 0.0 0.00 0.5
0.0 0.5 0.25 0.0
0.5 1.0 0.50 0.5
11 .. 50 1.5 0.75 1.0
10.0 7.5 5.0 2.5 0x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0 10.0 7.5 5.0 2.5 0 x.0 2.5 5.0 7.5 10.0
Figure25. NumericalSimulationExamplesofFittingHalfsinusoidswithRealWeightsMixtures(N=2,5,8,and10). Weshow
somefittingexamplesforhalfsinusoidsignalswithRealweightsmixtures(canbenegative).Thefourmixturesusedfromlefttorightare
Gaussians,LoG,DoG,andGeneralmixtures. Fromtoptobottom: N=2,8,and10components. Theoptimizedindividualcomponents
areshowningreen. SomeexamplesfailtooptimizeduetonumericalinstabilityinbothGaussiansandGEFmixtures. NotethatGEFis
veryefficientinfittingthehalfsinusoidwithfewcomponentswhileLoGandDoGaremorestableforalargernumberofcomponents.
30
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
y
yB.GenealizedExponentialSplattingDetails B.2.ImplementationDetails
B.1. Upper Bound on the Boundary View- NotethattheDoGinEq.(7)willbeverylargewhenσ is
2
Dependant Error in the Approximate GES large, sowedownsamplethegroundtruthimagebyafac-
Rasterization tor ‘scale ‘ and upsample the mask M similarly be-
im,freq ω
fore calculating the loss in Eq.(8). In the implementation
Given the Generalized Exponential Splatting (GES)
ofourGeneralizedExponentialSplatting(GES)approach,
function defined in Eq.(2) and our approximate rasteriza-
wefine-tunedseveralhyperparameterstooptimizetheper-
tiongivenbyEq.(3),4,and5,weseektoestablishanupper
formance. Thefollowinglistdetailsthespecificvaluesand
boundontheerrorofourapproximationinGESrendering.
purposesofeachparameterinourimplementation:
Since it is very difficult to estimate the error accumulated
in each individual pixel from Eq.(3), we seek to estimate • Iterations: Thealgorithmranforatotalof40,000iter-
theerrordirectlyoneachsplattingcomponentaffectingthe ations.
energyofallpassingrays.
Letusconsiderasimple2Dcasewithsymmetricalcom- • LearningRates:
ponentsasinFig.6. TheerrorbetweenthescaledGaussian
– Initialpositionlearning rate(lr )was setto
component and the original GES component is related to pos,init
0.00016.
theenergylossofraysandcanberepresentedbysimplyes-
timatingtheratioηbetweentheareadifferenceandthearea – Final position learning rate (lr pos,final) was re-
ofthescaledGaussian. Herewewillshowwecanestimate ducedto0.0000016.
anupperboundonηrelativetotheareaofeachcomponent. – Learningratedelaymultiplier(lr )wasset
delaymult
Fortheworst-casescenariowhenβ → ∞,weconsider
to0.01.
twonon-overlappingconditionsfortheapproximation: one
– Maximum steps for position learning rate
wherethesquareistheoutershapeandonewherethecircle
(lr )weresetto30,000.
coversthesquare.Thesidelengthofthesquareis2rforthe pos,maxsteps
√
formercaseand2r/ 2forthelattercase. Theradiusr of • OtherLearningRates:
thecircleisdeterminedbytheeffectiveprojectedvariance
αfromEq.(4). Forasquarewithsidelength2randacircle – Featurelearningrate(lr feature)was0.0025.
withradiusr,wehave: A sq√uare = 4r2,A circle = πr2.Fora – Opacitylearningrate(lr opacity)was0.05.
squarewithsidelength2r/ 2,theareais:A =
square,covered – Shape and rotation learning rates (lr and
2r2. shape
lr )werebothsetto0.001.
Theareadifference∆Ais: rotation
– Scalinglearningrate(lr )was0.005.
∆A =A −A =4r2−πr2, (29) scaling
squarelarger square circle
∆A =A −A =πr2−2r2. • DensityandPruningParameters:
circlelarger circle square,covered
(30)
– Percentage of dense points (percent ) was
dense
Theratioofthedifferenceinareastotheareaoftheinner 0.01.
shape,denotedasη,isboundedby: – Opacityandshapepruningthresholdsweresetto
∆A 4r2−πr2 0.005.
η = squarelarger = ≈0.2732,
squarelarger A πr2
circle • LossWeightsandIntervals:
(31)
η =
∆A
circlelarger =
πr2−2r2
≈0.3634. (32)
– SSIMlossweight(λ ssim)was0.2.
circlelarger A πr2 – Densification, opacity reset, shape reset, and
circle
shape pruning intervals were set to 100, 3000,
Due to the PDF normalization constraint in GND [14],
1000,and100iterations,respectively.
theapproximationfollowedinEq.(4),and5willalwaysen-
sureη ≤ η ≤ η . Thus, ourtargetratioη
squarelarger circlelarger • DensificationDetails:
whenusingourapproximatescalingofvariancebasedonβ
shouldbewithintherange0.2732≤η ≤0.3634. Thisim- – Densificationstartedfromiteration500andcon-
plies in the worst case, our GES approximation will result tinueduntiliteration15,000.
in36.34%energyerrorinthelostenergyofallrayspassing
– Gradient threshold for densification was set to
throughallthesplattingcomponents. Inpractice,theerror
0.0003.
will be much smaller due to the large number of compo-
nentsandthesmallscaleofallthesplattingcomponents. • ImageLaplacianParameters:
31– Image Laplacian scale factor (scale ) was Dataset Metrics\Methods Point-E DreamGaussian GES(Ours)
im,freq
0.2. CLIP-Similarity↑ 0.48 0.56 0.58
NeRF4
PSNR↑ 0.70 13.48 13.33
– WeightforimageLaplacianloss(λ ω)was0.5.
RealFusion15
CLIP-Similarity↑ 0.53 0.70 0.70
PSNR↑ 0.98 12.83 12.91
• Miscellaneous: - AverageRuntime↓ 78secs 2mins 2mins
Table 3. GES Application: Fast Image-to-3D Genera-
– Strengthofshapeρwassetto0.1.
tion pipeline We show quantitative results in terms of CLIP-
Similarity↑/PSNR↑,andRuntime↓,comparedtofastmethods:
These parameters were carefully chosen to balance the
Point-E[46]andDreamGaussian[68]. GESoffersagoodoption
trade-offbetweencomputationalefficiencyandthefidelity forafastandeffectiveimage-to-3Dsolution.
of the synthesized views. The above hyperparameter con-
figuration played a crucial role in the effective implemen-
tationofourGESapproach. Forimplementationpurposes, seethatthethresholdhasasignificantimpactonthetrade-
themodificationfunctionshavebeenshiftedby-2andtheβ off between performance and size, with a higher threshold
initializationissetto0insteadof2(whichshouldnothave decreasingsizeattheexpenseofperformance. Notably,we
anyeffectontheoptimization). see that GES outperforms GS across the range of density
gradientthresholds,yieldingsimilarperformancewhileus-
C.AdditionalResultsandAnalysis inglessmemory.
C.1.AdditionalResults C.4. Analysing the Frequency-Modulated Image
Loss
We show in Fig.32 additional GES results (test views)
and comparisons to the ground truth and baselines. In Westudytheeffectofthefrequency-modulatedlossL ω
Fig.33,showPSNR,LPIPS,SSIM,andfilesizeresultsfor on the performance by varying λ ω and show the results in
every single scene in MIPNeRF 360 dataset [5,6] of our Table4andTable2. Notethatincreasingλ ω inGESindeed
GES and re-running the Gaussian Splatting [27] baseline reducesthesizeofthefile,butcanaffecttheperformance.
with the exact same hyperparameters of our GES and on Wechoseλ ω = 0.5asamiddlegroundbetweenimproved
differentnumberofiterations. performanceandreducedfilesize.
C.2.ApplyingGESinFast3DGeneration C.5.VisualizingtheDistributionofParameters
GESisadaptedtomodern3Dgenerationpipelinesusing We visualize the distribution of shape parameters β in
score distillation sampling from a 2D text-to-image model Fig.28andthesizesofthesplattingcomponentsinFig.29.
[50],replacingGaussianSplattingforimprovedefficiency. Theyclearlyshowasmoothdistributionofthecomponents
WeemploythesamesetupasDreamGaussian[68],altering in the scene, which indicates the importance of initializa-
only the 3D representation to GES . This change demon- tion. This hints a possible future direction in this line of
stratesGES’scapabilityforreal-timerepresentationappli- research.
cationsandmemoryefficiency.
C.6.TypicalConvergencePlots
For evaluation, we use datasets NeRF4 and Real-
Fusion15 with metrics PSNR, LPIPS [87], and CLIP- We show in Fig.30 examples of the convergence plots
similarity[53]followingthebenchmarksinRealfusion[41] of both GES and Gaussians if the training continues up to
and Magic123 [52]. Our GES exhibits swift optimization 50K iterations to inspect the diminishing returns of more
withanaverageruntimeof2minutes,maintainingquality, training. Despite requiring more iterations to converge,
asshowninTable3andFig.26. GEStrainsfasterthanGaussiansduetoitssmallernumber
ofsplattingcomponents.
C.3.ShapeParameters
InTable5,weexploretheeffectofallhyperparameters
associatedwiththenewshapeparameteronnovelviewsyn-
thesis performance. We find that the optimization process
is relatively robust to these changes, as it retains relatively
strongperformanceandyieldsresultswithsimilarsizes.
Density Gradient Threshold. In Fig.27, we visualize the
impactofmodifyingthedensitygradientthresholdforsplit-
ting,usingbothGESandthestandardGaussianSplatting(
aftermodifyingthesetupforafaircomparisontoGES).We
32Realfusion15 NeRF4 StableDiffusion-XL
Reference
Sourceview
λ Method PSNR LPIPS SSIM Size
freq
DeepBlending
Novelview1
GES 29.58 0.252 0.900 431
0.05
GES(fixedβ =2) 29.53 0.251 0.901 433
Novel view2
GES 29.54 0.252 0.901 428
0.10
GES(fixedβ =2) 29.61 0.252 0.901 435
Figure 26. Visulization for 3D generation. We show selected
generatedexamplesbyGESfromRealfusion15(left)andNeRF4 GES 29.66 0.251 0.901 397
0.50
datasets(middle). Additionally,wepicktwotextprompts:”acar GES(fixedβ =2) 29.61 0.252 0.901 437
made out of sushi” and ”Michelangelo style statue of an astro-
GES 27.21 0.259 0.899 366
naut”,andthenuseStableDiffusion-XL[49]togeneratetherefer- 0.90
GES(fixedβ =2) 29.62 0.252 0.901 434
enceimagesbeforeusingGESonthem(right).
MipNeRF
GES 27.08 0.250 0.796 405
0.05
GES(fixedβ =2) 27.05 0.250 0.795 411
GES 27.05 0.250 0.795 403
0.10
GES(fixedβ =2) 27.05 0.250 0.796 412
GES 26.97 0.252 0.794 376
0.50
GES(fixedβ =2) 27.09 0.250 0.796 415
er) 0.26
ett GES 25.82 0.255 0.792 364
er is
b 0.24 0.90
GES(fixedβ =2) 27.08 0.250 0.795 413
w 0.22
o
S (l TanksandTemples
PIP 0.20
L GES 23.49 0.196 0.837 251
0.05
0.0001 0.0001 0.0002 0.0002 0.0003 0.0003 0.0004 GES(fixedβ =2) 23.55 0.196 0.836 255
GES 23.54 0.196 0.837 247
2048 0.10
er) GES(fixedβ =2) 23.53 0.196 0.837 255
ett
File
Size
(lower
is b 1 250 512 624
Metho
G
Od
a uu rsssians
0 0. .5 90
0
G
G
G
GE
E
E
ES
S
S
S( (fi fix xe ed dβ
β
= =2 2)
)
2
22
23
33
2.
..
.6
53
65
05
5
0
00
0.
..
.1
11
29
99
06
77
0
0
00
0.
..
.8
88
83
33
37
66
4
2
22
25
52
16
61
0
0.0001 0.0001 0.0002 0.0002 0.0003 0.0003 0.0004
Densification Threshold
Table4.Ablationofλ .Weshowacomparisonofperformance
freq
Figure27.AblationStudyofDensificationThresholdonNovel (PSNR, LPIPS, SSIM) for various values of λ . Note that in-
freq
ViewSynthesis. Impactofthedensificationthresholdonrecon- creasingλ inGESindeedreducesthesizeofthefile, butcan
freq
struction quality (LPIPS) and file size (MB) for our method and affecttheperformance. Wechoseλ =0.5asamiddleground
freq
Gaussian Splatting [27], averaged across all scenes in the Mip- betweenimprovedperformanceandreducedfilesize.
NeRFdataset. Weseethatthedensificationthresholdhasasig-
nificant impact on both file size and quality. Across the board,
ourmethodproducessmallerscenesthanGaussianSplattingwith
similarorevenslightlyimprovedperformance.
3360000
40000
20000
0
1.0 1.5 2.0 2.5 3.0
Shape Value ( )
Figure28.DistributionofShapeValues.Weshowadistribution
ofβvaluesofaconvergedGESinitializedwithβ =2.Itshowsa
slightbiastoβsmallerthan2.
L 2
1000
L
500
0 AblationSetup PSNR↑ SSIM↑ LPIPS↓ Size(MB)↓
5 10 15 20
Gaussians 23.14 0.841 0.183 411
Sizes
GESw/oL ω 23.54 0.836 0.197 254
GESw/randomβinit. 23.37 0.836 0.198 223
Figure29. DistributionofSizes. Weshowadistributionofsizes GESw/β=2init. 23.35 0.836 0.198 222
(L andL )oftheGEScomponentsofaconvergedscene.
2 ∞
Table6.AblationStudyonNovelViewSynthesis.Westudythe
impactofseveralcomponentsinGESonthereconstructionquality
Shape
PSNR SSIM LPIPS FileSize(MB) andfilesizeintheTanks&Templesdataset.
LearningRate
0.0005 26.83 0.845 0.141 659
0.0010 26.85 0.845 0.141 658
0.0015 26.89 0.846 0.141 651
0.0020 26.82 0.844 0.142 658
Shape
PSNR SSIM LPIPS FileSize(MB)
ResetInterval
200 26.87 0.845 0.141 656
500 26.86 0.845 0.141 658
1000 26.89 0.846 0.141 651
2000 26.84 0.845 0.141 657
5000 26.84 0.845 0.141 661
Shape
PSNR SSIM LPIPS FileSize(MB)
Strength
0.010 26.87 0.845 0.141 661
0.050 26.84 0.845 0.141 653
0.100 26.89 0.846 0.141 651
0.150 26.83 0.844 0.142 656
Table 5. Ablation Study on Novel View Synthesis. Impact of
theshapeparameter’slearningrate,resetinterval,andstrengthon
reconstructionqualityandfilesizeforthegardenscenefromthe
Mip-NeRFdataset.
34
ycneuqerF
ycneuqerFL1Loss TrainLoss TrainPSNR
NumberofComponents TestLoss TestPSNR
Figure30. ConvergencePlotsofGaussiansvs.GES.WeshowanexampleoftheconvergenceplotsofbothGESandGaussiansifthe
trainingcontinuesupto50Kiterationstoinspectthediminishingreturnsofmoretraining. Despiterequiringmoreiterationstoconverge,
GEStrainsfasterthanGaussiansduetoitssmallernumberofsplattingcomponents.
35Figure31. Frequency-ModulatedImageMasks. Fortheinputexampleimageonthetopleft,Weshowexamplesofthefrequencyloss
masksM usedinSec.4.3fordifferentnumbersoftargetnormalizedfrequenciesω (ω = 0%forlowfrequenciestoω = 100%for
ω
highfrequencies). ThismaskedlosshelpsourGESlearnspecificbandsoffrequencies. NotethatduetoLaplacianfiltersensitivityfor
high-frequencies,themaskfor0<ω ≤50%isdefinedas1−M for50<ω ≤100%. Thisensuresthatallpartsoftheimagewillbe
ω
coveredbyoneofthemasksM ,whilefocusingonthedetailsmoreastheoptimizationprogresses.
ω
36GroundTruth GES(Ours) Gaussians Mip-NeRF360 InstantNGP
Figure32. ComparativeVisualizationAcrossMethods. Displayedareside-by-sidecomparisonsbetweenourproposedmethodand
establishedtechniquesalongsidetheirrespectivegroundtruthimagery. Thedepictedscenesareorderedasfollows: BICYCLE,GARDEN,
STUMP, COUNTER, and ROOM from the Mip-NeRF360 dataset; PLAYROOM and DRJOHNSON from the Deep Blending dataset, and
TRUCK and TRAIN from Tanks&Temples. Subtle variances in rendering quality are accentuated through zoomed-in details. It might
be difficult to see differences between GES and Gaussians because they have almost the same PSNR (despite GES requiring 50% less
memory).
37bicycle drjohnson counter bonsai garden playroom room truck kitchen train stump
method
30 Gaussian
GES
25
20
R
N
S
P 15
10
5
0
40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000
value value value value value value value value value value value
bicycle drjohnson counter bonsai garden playroom room truck kitchen train stump
0.4 method
Gaussian
0.35
GES
0.3
0.25
S
P
PI 0.2
L
0.15
0.1
0.05
0
40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000
value value value value value value value value value value value
bicycle drjohnson counter bonsai garden playroom room truck kitchen train stump
method
Gaussian
0.8 GES
0.6
M
SI
S
0.4
0.2
0
40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000
value value value value value value value value value value value
bicycle drjohnson counter bonsai garden playroom room truck kitchen train stump
method
700 Gaussian
GES
600
500
Size
400
File
300
200
100
0
40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000 40000 30000 7000
value value value value value value value value value value value
Figure33.DetailedPerSceneResultsOnMipNeRF360forDifferentIterationNumbers.WeshowPSNR,LPIPS,SSIM,andfilesize
resultsforeverysinglesceneinMIPNeRF360dataset[5]ofourGESandre-runningtheGaussianSplatting[27]baselinewiththeexact
samehyperparametersofourGESandondifferentnumberofiterations.
38GroundTruth GES(full) GES(w/oL ) GaussianSplatting[27]
ω
Figure 34. Frequency-Modulated Loss Effect. We show the effect of the frequency-modulated image loss L on the performance
ω
on novel views synthesis. Note how adding this L improves the optimization in areas where large contrast exists or where a smooth
ω
backgroundisrendered.
39