Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
MariaBa˚nkestad12 JenniferAndersson2 SebastianMair2 JensSjo¨lund2
Abstract
Reducing a graph while preserving its overall
structureisanimportantproblemwithmanyap-
plications. Typically,thereductionapproachesei-
therremoveedges(sparsification)ormergenodes
(coarsening)inanunsupervisedwaywithnospe-
cificdownstreamtaskinmind. Inthispaper,we
presentanapproachforsubsamplinggraphstruc-
turesusinganIsingmodeldefinedoneitherthe
nodesoredgesandlearningtheexternalmagnetic
fieldoftheIsingmodelusingagraphneuralnet-
work. Ourapproachistask-specificasitcanlearn
howtoreduceagraphforaspecificdownstream
taskinanend-to-endfashion. Theutilizedloss
functionofthetaskdoesnotevenhavetobedif-
ferentiable. We showcase the versatility of our
approach on three distinct applications: image Figure1.SampledverticesusingtheIsingmodelwithalearned
segmentation,3Dshapesparsification,andsparse magneticfield.Blackverticesareretainedinthecoarsemeshand
approximatematrixinversedetermination. definethegeneralshape,whilewhiteverticesonlyadddetails.
1.Introduction
sparsificationtoaparticulartask.
Hierarchicalorganizationisarecurringthemeinnatureand We take inspiration from the well-known Ising model in
humanendeavors(Pumain,2006). Partofitsappealliesin physics(Cipra,1987),whichemanatedasananalyticaltool
theinterpretabilityandcomputationalefficiencyitaffords, forstudyingmagnetismbuthassinceundergonemanyex-
especiallyonuniformlydiscretizeddomainssuchasimages tensions (Wu, 1982; Nishimori, 2001). Within the Ising
or time series (Duhamel & Vetterli, 1990; Mallat, 1999; model,eachlocationisassociatedwithabinarystate(inter-
Hackbusch,2013;Lindeberg,2013). Ongraph-structured pretedaspointingupordown),andtheconfigurationofall
data,itis,however,nolongerasclearwhatitmeansto,e.g., statesisassociatedwithanenergy.
“sampleeverysecondpoint”asshowninFigure1.Solidthe-
Thus, the Ising model can be seen as an energy-based
oreticalargumentsfavoraspectralapproach(Shumanetal.,
modelwithanenergyfunctioncomprisingapairwiseterm
2015),whereinasimplifiedgraphisfoundbymaximizing
andapointwise“bias”term. AsillustratedinFigure2,the
thespectralsimilarityoftheoriginalgraph(Jinetal.,2020).
signofthepairwisetermdetermineswhetherneighboring
Ontheotherhand,insteadofretainingasmuchoftheorigi-
statesattractorrepeleachother,whilethepointwiseterm
nalinformationaspossible,itmayoftenbemorevaluable
(corresponding to the local magnetic field) controls the
todistillthecriticalinformationforthetaskathand(Cai
propensityofaparticularalignment.
etal.,2021;Jinetal.,2022a). Thetwomainapproachesfor
graphsimplification,coarsening,andsparsification,involve Inthispaper,weconsidertheIsingmodeldefinedoneither
removingnodesandedges, respectively. However, using the nodes or the edges of a graph and augment it with a
existingmethods,itisnotpossibletotailorcoarseningor graphneuralnetworkthatmodelsthelocalmagneticfield.
There are no particular restrictions on the type of graph
1RISE, Research Institutes of Sweden 2Uppsala University.
neural network, which means that it can, e.g., process
Correspondenceto:MariaBa˚nkestad<maria.bankestad@ri.se>.
multidimensionalnodeandedgefeatures. Sincechoosing
Preprint.Underreview. whether to include a node (or edge) is binary, we use
1
4202
beF
51
]GL.sc[
1v60201.2042:viXraIsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
techniques for gradient estimation in discrete models to messagepassingprocedureatlayerkisdefinedas
trainourmodelforagiventask. Specifically,weshowthat
mk =fm(xk,xk,ek), (3)
thetwo-pointversionoftheREINFORCELeave-One-Out ij i j ij
estimator (e.g., Shi et al. (2022)) results in a straightfor- xˆk+1 =fa (mk), (4)
i j∈N(i) ij
ward expression and allows for using non-differentiable
xk+1 =fu(xk,xˆk+1), (5)
task-specific losses. To showcase the broad applicability i i i
ofourmodel,wedemonstratehowitcanbeusedinseveral where fm is the message function, deriving the message
domains: imagesegmentation,3Dshapesparsification,and from node j to node i, and fa is a function that
j∈N(i)
linearalgebra(sparseapproximateinversedetermination). aggregates the messages from the neighbors of node i,
denotedN(i). Theaggregationfunctionfa isoftenjusta
2.Background simplesumoraverage. Finally,fu istheupdatefunction
that updates the features for each node. A GNN consists
Energy-basedModels. Energy-basedmodelsaredefined ofmessage-passinglayersstackedontoeachother,where
by an energy function E θ: Rd → R, parameterized by θ. thenodeoutputfromonelayeristheinputofthenextlayer.
Thedensitydefinedbyanenergy-basedmodelisgivenby
Torevealthemessage-passingstructureoftheIsingmodel,
theBoltzmanndistribution
notethatwecanrewritetheenergyinEquation(2)as
p (x)=Z−1exp(−βE (x)), (1)  
θ θ θ
(cid:88) (cid:88)
E(x)=− x i·h i+ J ijx j
whereβ >0istheinversetemperatureandthenormaliza-
(cid:82) i∈V j∈N(i) (6)
tionconstantZ = exp(−βE θ(x))dx,alsoknownasthe
(cid:88)
=− x heff(x),
partitionfunction,istypicallyintractable. i i
i∈V
A sample with low energy will have a higher probability
wherewehavedefinedtheeffectivemagneticfield
than a sample with high energy. Many approaches have
beenproposedfortrainingenergy-basedmodels(Song& (cid:88)
heff(x)=h + J x . (7)
Kingma,2021),butmostofthemtargetthegenerativeset- i i ij j
j∈N(i)
tingwhereatrainingdatasetisgivenandtrainingamounts
to(approximate)maximumlikelihoodestimation. Equation (6) can be viewed as a message-passing step
in a graph neural network, where mk = J xk is the
IsingModel. TheIsingmodelcorrespondstoadeceptively ij ij j
message function fm, the aggregation fa function is the
simpleenergy-basedmodelthatisstraightforwardtoexpress
sumxˆk+1 = (cid:80) mk,andtheupdatefunctionfu is
onagraphG =(V,E)consistingofatupleofasetofnodes i j∈N(i) ij
V and a set of edges E ⊆ V × V connecting the nodes. xk i+1 =−xk i (cid:0) h i+xˆk i+1(cid:1) .
Specifically,theIsingenergyisoftheform
GradientEstimationinDiscreteModels. Letp (x)bea
θ
discreteprobabilitydistributionandassumethatwewantto
(cid:88) (cid:88)
E(x)=− J ijx ix j − h ix i, (2) estimatethegradientofalossdefinedastheexpectationof
(i,j)∈E i∈V alossℓ(x)overthisdistribution,i.e.,
where the nodes (indices i,j) are associated with spins L(θ)=E x∼pθ(x)[ℓ(x)]. (8)
x ∈ {±1}, the interactions J ∈ R determine whether
i ij Becauseofthediscreteness,itisnotpossibletocomputea
thebehaviorisferromagnetic(J >0)orantiferromagnetic
gradientdirectlyusingbackpropagation. Throughasimple
(J <0),andh istheexternalmagneticfield.
i algebraicmanipulation,wecanrewritethisexpressionas
ItfollowsfromEquation(2)thatintheabsenceofanexter-
∇ E [ℓ(x)]=E [ℓ(x)∇ logp (x)], (9)
nalmagneticfield,neighboringspinsstrivetobeparallelin θ x∼pθ(x) x∼pθ(x) θ θ
theferromagneticcaseandantiparallelintheantiferromag-
which is called the REINFORCE estimator (Williams,
neticcase. Anexternalmagneticfieldthatcanvaryacross
1992). Thoughgeneral,itsuffersfromhighvariance. Away
nodesinfluencesthesamplingprobabilityofthecorrespond-
toreducethisvarianceistoinsteadusetheREINFORCE
ingnodes. Thesystemisdisorderedathightemperatures
Leave-One-Out(RLOO)estimator(e.g.,Shietal.(2022)),
(smallβ)andorderedatlowtemperatures.
K
GraphNeuralNetworks. Agraphneuralnetwork(GNN) (∇ L)K =
1 (cid:88)(cid:16) ℓ(x(k))−ℓ¯(cid:17)
∇ logp (x(k)),
consistsofmultiplemessage-passinglayers(Gilmeretal., θ LOO K j θ θ
k=1
2017). Givenanodefeaturexk atnodeiandedgefeatures (10)
i
ek between node i and its neighbors {j: j ∈ N(i)}, the whereℓ¯ = 1 (cid:80)K ℓ(x(j)).
ij j K−1 j=1,j̸=k
2IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
External magnetic field h J=1, nit=0 J=1, nit=1 J=1, nit=5 J=1, nit=10
4
2
0 J= 1, nit=0 J= 1, nit=1 J= 1, nit=5 J= 1, nit=10
2
4
Figure2.Theexternalmagneticfieldh(left)canvaryspatiallyandinfluencethesamplingprobabilityrelativetoitsstrength.Thesignof
thecouplingconstantJ determineswhetherneighboringspinsattract(topright)orrepeleachother(bottomright).
3.Method thefractionofnodestosampleor,equivalently,tocontrol
theaveragemagnetizationη,whichisdefinedas
Ingraphneuralnetworkterminology,theinteractionsJ
ij
andtheexternalmagneticfieldsh icouldbeviewedasedge
1
(cid:88)|V|
andnodefeatures,respectively. Inparticular,weuseaGNN η = E [x ], (11)
toparameterizetheexternalmagneticfield,h : G →R|V|. |V| x∼pθ(x) i
θ i=1
Sampling. WeusetheMetropolis-Hastingsalgorithmto wherex∈{±1}|V| denotesthevectorcontainingallstates
samplefromtheprobabilitydistributiondefinedbytheIsing i=1
and x is the ith entry in x. Let x¯ = E [x ] de-
modelaccordingtoEquation(1),seeAlgorithm1. However, i i x∼pθ(x) i
notethelocalmagnetizationatnodei. SincetheMarkov
tomakeitcomputationallyefficient,weparallelizeitbyfirst
blanketofanodeispreciselyitsneighborhood,wecanuse
coloring the graph such that nodes of the same color are
Equation(6)torewritethelocalmagnetizationas
neverneighbors.
Then, we can simultaneously perform the Metropolis- x¯ i =E j∈N(i)[E i[x i |x j]] (12)
Hastingsupdateforallnodesofthesamecolor. Forgrid- =E (cid:2) tanh(βheff(x))(cid:3) . (13)
j∈N(i) i
structuredgraphs,suchasimages,asimplecheckerboard
patterngivesaperfectcoloring. Toextendthistogeneral
Thecorrespondingmean-field(variational)approximation
graphs, we run a greedy graph coloring heuristic (Miller
isanonlinearsystemofequationsinx¯(MacKay,2003):
etal.,1999)thatproducesacoloringwithalimited,butnot
necessarilyminimal,numberofcolorsC. h¯eff(x¯)=h + (cid:88) J x¯ ,
i i ij j
Algorithm1MonteCarloSamplingoftheIsingModel j∈N(i) (14)
Input: Graph{G c}C c=1groupedbycolorc,interactions x¯ i =tanh(βh¯e iff).
termJ,externalmagneticfieldh,numberofiterationsT.
Solving this yields a deterministic way of approximating
the average magnetization η. However, we are primarily
Output: Stateconfigurationx∈{±1}|V|.
interestedintheorderedregime,whereβh¯eff islarge,and
fori=1to|V|do i
itapproximatelyholdsthatx =J x ,whichimpliesthat
x ∼U({−1,1}) {Sampleaninitialstate} j ij i
i x¯ =tanh(βh )and,thus,
fort=1toT do i i
forc=1toC do
|V|
forallnodesx ∈G do 1 (cid:88)
(cid:16)i (cid:80)c (cid:17) η˜ det = |V| tanh(βh i). (15)
∆E =2x J x +h
i i j∈N(i) j i i=1
r ∼U([0,1]) {Sampleruniformlyatrandom}
This can be contrasted with the stochastic estimate we
if∆E <0orexp(−2β∆E)>rthen
i get from using a one-sample Monte Carlo estimate in
x =−1·x {Flipthespinofnodei}
i i Equation(13):
ControllingtheSamplingFraction. Forsometasks,the |V|
obvious way to minimize the loss is to either sample all η˜
sto
= |V1 |(cid:88) tanh(cid:0) βhe iff(x))(cid:1) , x∼p θ(x). (16)
nodesornone. Tocounteractthis,weneedawaytocontrol i=1
3IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
h, Ising+Mag Logits U-net Pred Ising+Mag Pred U-net Target
Figure3.ComparisonofaU-Nettrainedwithacross-entropylossagainstanIsingmodelwithalearnedmagneticfield.Thesequence
fromlefttorightincludestheoriginalimage,themagneticfieldfromtheIsingmodel,theU-Netoutput(thelogits),theIsingmodel
prediction,theU-Netprediction,andtheactualsegmentationmask.
Learning. Incontrasttomostotherenergy-basedmodels thetask-specificlossℓ,oruseadeterministicapproximation
(Song&Kingma,2021),wedonothaveatrainingdataset basedonEquation(15)tomoveitoutsidetheexpectation
wewanttosamplefrom,nordoweconsidertheregression andtherebyenabledirectautomaticdifferentiation. Empiri-
setting(Gustafssonetal.,2020). Instead,weconsiderthe cally,wefoundthelatteroptiontoworkbetter. Specifically,
unsupervisedsettingwherethesubsampledgraphisused weusethetrainingloss
for a downstream task with an associated, possibly non-
differentiable,loss.
L (θ)=E [ℓ(x)]+(η˜ (θ)−η)2, (19)
Forgradient-basedtrainingoftheGNNinourIsingmodel,
tot x∼pθ(x) det
we need the gradient of the log probability. This can be
decomposedasasumoftwoterms: wherethedesiredvalueoftheaveragemagnetizationη is
definedasinEquation(11).
∇ logp (x)=−β∇ E (x)−∇ logZ . (17)
θ θ θ θ θ θ
We are particularly interested in training the model for a 4.Applications
downstreamtaskassociatedwithalossℓ(x)definedinterms
Theproposedschemeforenergy-basedgraphsubsampling
ofsamplesx ∼ p (x)fromtheIsingmodel. Thegoalis
θ
isdemonstratedonthreedistinctapplications: imageseg-
thentominimizetheexpectedlossasdefinedinEquation(8)
mentation,3Dshapesparsification,andsparseapproximate
Sinceweareoperatinginadiscretesetting,weusethegradi- matrixinversedetermination.
entestimationtechniquedescribedinSection2.Specifically,
usingK =2inEquation(10),theproblematiclogZ θ terms 4.1.IllustrativeExampleonImageSegmentation
cancelintheRLOOestimator,andweobtain
WhiletheprimarygoalistolearnIsingmodelsongraphs,
β (cid:16) (cid:17)
(∇ L)2 =− ℓ(x(1))−ℓ(x(2)) weinitiallydelveintoanimagetaskduetoitsvisualand
θ LOO 2 (18) explanatory simplicity. An image can fundamentally be
(cid:16) (cid:17)
·∇ E (x(1))−E (x(2)) . seen as a simple graph, with the pixels as nodes and the
θ θ θ
neighboringpixelsasneighbors(Geman&Geman,1984).
To train the model, we can thus draw two indepen- Specifically,intheferromagneticscenariowithJ =1,Ising
dent samples x(1),x(2) ∼ p (x) from the Ising model modelstendtogroupnearbypixelswiththesamevalues.
θ
and estimate the gradient using Equation (18), where Postprocessingtheoutputofaneuralnetworkwithsucha
∇ (cid:0) E (x(1))−E (x(2))(cid:1) canbecomputedbyautomatic probabilisticgraphicalmodeliswell-knowntoimprovethe
θ θ θ
differentiation. localizationofobjectboundaries(Chenetal.,2017).
Tocontrolthesamplingfraction,asdescribedinSection3, OurApproach. Becauseofthegridstructure,themessage
wecouldeitherincludearegularizationtermthatdependson fromthe8-connectedneighborscanbeimplementedasa
thestochasticsamplingfraction(Equation(16))directlyin convolutionallayerwithkernelsizethreeandzeropadding.
4IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Specifically,weuseconvolutionalfiltersoftheform and using approximation schemes with hierarchical tree
 w∗ w w∗ structureslikeoctrees. Butthesecanonlybeadaptedtothe
extentthatpriorknowledgeofthegeometryofthescanned
W =w 0 w , (20)
objectorapplicationexists(Tenbrincketal.,2019). While
w∗ w w∗
weprimarilyfocusonmeshcoarseningintheexperiments,
wherewandw∗havethesamesignbutcanhavedifferent we emphasize that the same procedure can be applied to
values. Thisgivesustheenergyperpixel, pointclouds,e.g.,byutilizingtherelativedistancesbetween
pointsasedgefeatures. Importantly,weretainthepositions
E =−x (Conv (x )+h (x )). (21)
i i W i θ i oftheoriginalnodesthataresubsampled.
Here,h (x )istheexternalmagneticfieldwiththelearnable
θ i ProblemFormulation. AtriangularmeshMconsistsof
parametersθ. IncontrasttoZhengetal.(2015),whouse
vertices V, edges E and faces F, where the faces define
the mean-field approximation in Equation(14)to learn a
which vertices belong to each triangle in the mesh. To
fully connected conditional random field end-to-end, our
coarsenthemesh,weremoveaselectionofverticesfrom
approachappliestograph-structuredenergyfunctionsand
themesh,andtheedgesconnectedtotheremovedvertices
allowstask-specificlosses.
arecollapsedtotheirclosedvertex. See,e.g.,Figure1.
Sinceweconsiderbinarysegmentation,weuseapixel-wise
Thevertexselectioncandependonanydownstreamtask
misclassificationloss
anddoesnothavetobedifferentiable. Inthisexample,we
|V| definethetaskascreatingacoarsemeshthatisasclosein
1 (cid:88)
ℓ(x)=
|V|
|x i−y i|, (22) spacetotheoriginalmeshaspossible. Wequantifythisas
i=1 thevertex-to-meshdistancebetweentheverticesV1inthe
wherey ∈{−1,1}isthetargetvalue.
originalmeshandthesurfaceofthecoarsemeshM2,
i
Results. WeusetheCaltech-UCSDBirds-200-2011dataset d(V1,M2)= (cid:88) min ||x−y||2. (23)
2
(Wahetal.,2011)andmodelthemagneticfieldoftheIsing y∈M2
x∈V1
model by a U-Net (Ronneberger et al., 2015). See Ap-
pendixB.1fordetails. ForeachvertexinmeshM1,thealgorithmfindsthenearest
pointonthemeshM2andsumsthedistances.
Directlytrainingthemodelforsegmentationusingastan-
dardcross-entropylossyieldsanaverageDicescoreof0.85 OurApproach. Weviewmeshcoarseningasasampling
onthetestset. Incontrast,usingalearnedmagneticfield problem, where the antiferromagnetic Ising model deter-
intheIsingmodel,theaverageDicescoreincreasesto0.87 mines the distribution of the vertices in the coarse mesh.
onthetestset. Figure3comparestwopredictionsfromthe TheantiferromagneticIsingisapromisingchoiceformesh
twomodels. Notably,theoutputfromthemagnetizationnet- sampling because of its distinctive “every other” pattern.
workoftheIsingmodelexhibitsclearerandsharperborders Unlikerandomsampling,whichmayresultinconcentrated
comparedtothelogitspredictedbytheplainsegmentation pointsincertainareaswhileneglectingcrucialpartsofthe
model. Thisdistinctionisalsoevidentinthepredictions, mesh,theIsingmodel’ssamplesaimtobedistributedmore
wherethesegmentationmaskpredictedbytheIsingmodel evenlyacrosstheentiremesh. Furthermore,thelearnedex-
ismoreevenandself-consistent. ternalmagneticfieldh (x)encouragessamplinginregions
θ
ofparticularrelevanceforthetaskathand.
4.2.3DShapeSparsification
We represent the object’s 3D shape with a graph, with
Anobject’s3Dmeshorpointcloudtendstohaveadensely the vertices’ positions as node features and the relative
sampledsurface,resultinginnumerousredundant3Dpoints. distancebetweenthenodesasedgefeatures,e =x −x .
ij i j
These redundant points increase the computational work- To model the magnetic field h (·), we use a three-layer
θ
loadinsubsequentprocessingstages. Consequently,mesh Euclidean graph neural network (Geiger & Smidt, 2022)
and point cloud coarsening (representing the object with since it allows us to learn concise representations that
fewerverticesandedges)arecommonpreprocessingsteps capturecrucialgeometric3Dinformationwhile,atthesame
incomputergraphics(Wrightetal.,2010). Themaintech- time,beingequivarianttotherotationsandtranslationsof
niquesforcoarseningamesharebasedonpreservingthe themesh. WealsomakeuseofthemeshLaplacian(Reuter
spectralpropertiesoftheoriginalmesh(Chenetal.,2022; etal.,2009),whichgivesameasureofthegeometryofthe
Keros&Subr,2022). However,thesemethodscannotbe surfaces(seeAppendixC).
adaptedtospecificdownstreamtasks.
FromthefinemeshM1,wegenerateacoarsemeshM2by
In the case of 3D point clouds, standard approaches for samplingverticesusingtheIsingmodel. Fromtheselected
reducingthenumberofpointsincluderandomsubsampling vertices, we then regenerate the mesh by collapsing the
5IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Table1.Meanandstandarddeviationofthetestvertex-to-meshdistancefromthefive-foldcross-validationofthefourdatasets.
DATASET BUST FOURLEG HUMAN CHAIR
ISING+MAG 0.0221(0.0042) 0.0339(0.0034) 0.0366(0.0031) 0.0369(0.0046)
ISING 0.0288(0.0055) 0.0411(0.0034) 0.0409(0.0033) 0.0548(0.0045)
RANDOM 0.0343(0.0064) 0.0498(0.0032) 0.0498(0.0032) 0.0592(0.0040)
can see that the magnetic field is high in areas where we
havesharpcornersandgenerallymoreintricategeometry.
Tocompensateforthehighmagneticfieldincertainareas,
ourregularizationtermforcesittobelowinotherareasthat
are not as important for the task, i.e., the vertex-to-mesh
distance. Figure 1 provides a closer look at the samples
fromtheIsingmodelwithalearnedmagneticfield. Note
the higher concentration of samples in high curvature
regionssuchasthedog’searandnose.
4.3.SparseApproximateMatrixInverses
Inmanyscientificandtechnicaldomains,solvinglargelin-
ear systems on the form Az = b, where A ∈ Rn×n is
asparsesystemmatrix,remainsafrequentlyencountered
computational challenge. With increasing problem sizes,
directsolutionmethodsarereplacedbyiterativemethods
such as the Krylov subspace methods. If the setting is
ill-conditioned,however,thesemethodsrequireprecondi-
Figure4.Fourexamplesoflearnedmagneticfieldsfromthefour-
tioningoftheoriginalsystem(e.g.,Ha¨usneretal.(2023)).
legdataset(toprow)andthebustdataset(bottomrow).
Sparseapproximateinversesareoftensuitableprecondition-
ersinlarge-scaleapplications.
unused vertices to their closest neighbors. To learn the Problem Formulation. We are interested in finding a
externalmagneticfieldh (·), weusetheRLOOgradient sparseapproximateinverseM ≈A−1 ∈Rn×nconstituting
θ
estimator defined in Equation (18). The loss function is therightpreconditionerforthesparselinearsystemAz =b.
definedinEquation(19),whereℓ(·)isthedistancedefined Givenasparsitypatterns∈S ofthematrixM,wheresis
inEquation(23). acollectionofmapriorideterminedindicescorresponding
to the non-zero matrix entries in M, and S is the space
Results. Weusethemeshsegmentationdataset(Chenetal.,
containingallpossiblesparsitypatterns,wewanttosolve
2009), and learn separate models for each of the classes thefollowingoptimizationproblemforthevaluess′ ∈Rm
bust, four-leg, human, and chair. Each class contains 20
ofthenon-zeroentriesinM
different meshes. We evaluate the model using five-fold
c ter so tss o- nva thli eda rt ei mon a, iw nih ne gre 20w %e .tr Tai hn eo mn e8 s0 h% eso cf ot nh te aid nat ba es te wta en end sm ′∈i Rn m∥AM′(s)−I∥ F. (24)
4712 and 27439 vertices. More details can be found in
Here,∥·∥ denotestheFrobeniusnormandI isthen×
AppendixC.Wetrainthemodelsuntilconvergenceonthe F
n identity matrix. With this notation, M′(s) denotes the
trainingdataset,usingη =0inEquation(19),whichmeans
approximateinversewithsparsitypatternsandnonzeross′.
weaimtoreducethenumberofverticesinthemeshbyhalf.
While elegant, this approach does require an a priori
The results from our experiments are summarized in
assumptiononthesparsitypatterns,whichdirectlyaffects
Table 1. Notably, Ising sampling without an external
the quality of the preconditioner that can be obtained.
magnetic field leads to a reduction in the vertex-to-mesh
Variousmodificationstothedescribedsparseapproximate
distancebetweentheoriginalverticesandthecoarsemesh
inverseapproach(SAI)exist,aimingtorefinethesparsity
compared to random sampling. This is further improved
patterniteratively(Grote&Huckle,1997).
by including a learned magnetic field. Figure 4 provides
avisualrepresentationofthepredictedmagneticfieldfor OurApproach. Weusetheproposedapproachforgraph
four meshes in the test dataset. By visual inspection, we subsamplingwithlearnedIsingmodelstofindpromising
6IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Figure5.Rows show two matrices from the test dataset (Setting 3). Each sparsity pattern shows nonzero elements (white) in the
correspondingmatrix. Fromtheleft: (a)sparsitypattern ofA, (b)sparsity patternoftrue inverseA−1, (c)sparsity patternofthe
approximateinverseusingthe50%ofallmatrixelementspredictedbyourmodel,(d)predictedmagneticfieldforthegivenmatrixA.
sparsitypatternsforsparseapproximateinversedetermina- resentation,andthus,nosuchassumptionsaretheoretically
tion. Todothis,wetakeinspirationfromthegraphrepre- necessary. SeeAppendixD.1andB.3forfurtherdetailson
sentationofsymmetricsparselinearsystems(Mooreetal., themodelspecifics,trainingprocess,andimplementation.
2023),inwhichasymmetricmatrixAisviewedasthead-
Datasets. Wetestourapproachontwodatasetsconsisting
jacencymatrixofaweightedundirectedgraphand,hence,
ofreal,sparse,andsymmetric30×30matrices. Dataset1
thematrixelementscorrespondtoedgefeaturesintheundi-
consistsof1600synthetic,binarysparsematrices.Dataset2
rectedgraph. SincetheIsingmodelisdefinedovernodes,
consists of 1800 sparse 30×30 submatrices constructed
wemovefromthecurrentgraphrepresentationtoitsline
from the SuiteSparse Matrix Collection (Davis & Hu.,
graph(e.g.,Sjo¨lund&Ba˚nkestad(2022)). Withazeromag-
2011),andthematricesinthisdatasetthusresemblesparse
neticfield,theIsingmodelencodesanaprioriassumption
matricesfromreal-worldapplications. Themeansparsity
ofthenonzerosbeingequallydistributedovertherowsand
ofthematricesinDataset1is83%,andthecorresponding
columnswhenappliedtothelinegraph. Here,wesimulate
sparsity level for Dataset 2 is 89%. Independent on the
theIsingmodelonthelinegraphusingamagneticfieldh
dataset,60%ofthedataisusedfortraining,and20%isused
parameterized by a GNN. A sample x from the resulting
forvalidationandtesting,respectively.Furtherdetailsonthe
distributionthencorrespondstotheproposedsparsitypat-
datasetsandpreprocessingcanbefoundinAppendixD.2.
ternsofthepredictedsparseapproximateinverseM ofthe
inputmatrixA. Results. Wetrainthreeseparatemodels,eachinadifferent
setting. InSetting1,weuseDataset1andaddAandA2
Thelearningobjectiveistominimizethetotalexpectedloss
as edge features. In practice, this means that the a priori
withrespecttothetrainableparametersθ,whilereducing
sparsity pattern of the approximate inverse is effectively
the number of elements in an a priori sparsity pattern
the sparsity pattern of A2, and the model is optimized to
(possiblyincludingallelements)by50%. Todothis, we
select50%oftheseelementsforthesparsitypatternofthe
usethetraininglossgiveninEquation(19),with
predicted sparse approximate inverse M. In Setting 2, A
l(x)=∥AM(sθ(x))−I∥ , (25) andA2remainedgefeatures,butthemodelisoptimizedto
F
select50%ofallelementsforthesparsitypatternofM,see
and η = 0. Here, M(sθ(x)) denotes the specific approx- AppendixD.1forimplementationdetails. Setting3isiden-
imateinverseobtainedbysolvingEquation(24)usingthe ticaltoSetting2,butusesDataset2inplaceofDataset1.
sparsitypatternobtainedfromasampleofthelearnedIsing
Table 2 shows the results in terms of the mean loss, cal-
modelgivenitscurrentparameterstate. Tomodeltheex-
culated according to Equation (25), over the test dataset
ternalmagneticfield,weuseagraphconvolutionalnetwork
in each setting, evaluated using a single sample from the
(Chenetal.,2020). Fortraining,weusetheRLOOgradient
learnedIsingmodelforeachtestmatrix. Wecomparethe
estimatoraccordingtoEquation(18),andletAandA2enter
performanceofourmodel(ISING+MAG)tothreecompet-
asedgefeatures. Contrarytoconventionalapproaches,our
ing approaches: a) an Ising model with a small constant
methodisflexibleintermsofapriorisparsitypatterns. Any
magneticfieldtunedtoobtainthesamesamplingfraction
a priori sparsity pattern can be induced by the graph rep-
7IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Markovrandomfield,whereaprobabilisticgraphicalmodel
Table2.Meanandstandarddeviationofthelossonthetestsetfor
(Koller&Friedman,2009)isusedtoencodedependencies
eachmodelandsetting.
between random variables. In our model, we output the
externalmagneticfieldasanintermediatevariablethatwe
MODEL SETTING1 SETTING2 SETTING3
thenconditionontodefineanIsingmodel. Itis,therefore,
ISING+MAG 3.28(0.12) 2.06(0.26) 1.15(1.24) appropriatelydescribedasaconditionalrandomfield(Laf-
ISING 3.86(0.12) 3.69(0.23) 3.78(0.42)
ferty et al., 2001). Conditional random fields are widely
RANDOM 4.04(0.16) 3.86(0.18) 3.86(0.30)
usedforstructuredpredictiontasks,forinstance,innatural
ONLYA 4.12(0.08) 4.12(0.08) 2.00(1.89)
language processing (Lample et al., 2016) and computer
vision(Zhengetal.,2015;Chenetal.,2017).
GraphSparsificationandCoarsening. Thegoalofgraph
astheonerecordedonthecorrespondingtestset(Ising),b)
sparsification and coarsening operations is to generate a
uniformrandomsamplingfromtheallowedsparsitypattern
smallergraphthatpreservestheglobalstructureofalarge
in each setting with the corresponding sampling fraction
graph. A unifying framework that captures both of these
(Random), and c) the quality of the sparse approximate
operationsisconsideredbyBravo-Hermsdorff&Gunderson
inverse obtained using only the sparsity pattern of input
(2019). Incontrasttoourapproach, theirsisnotlearned;
matrixA(OnlyA),whichisacommonaprioriassumption
rather,theypresentanalgorithmforreducingagraphwhile
on the sparsity pattern when determining sparse approxi-
preservingtheLaplacianpseudoinverse.
matematrixinverses. Inallcases,weobserveasignificant
improvementinthemeanlosscomparedtothebaselines. Safroetal.(2015);Loukas&Vandergheynst(2018);Loukas
(2019);Jinetal.(2020)alsoproposealgorithmsforgraph
ResultsfortwosamplesfromthetestdatasetinSetting3
coarsening. Theirapproachistopreservespectralproper-
are visualized in Figure 5, showing in particular how the
ties. Incontrasttothoseapproaches,butbuildingontopof
learnedmagneticfieldhasadaptedtoallowforsamplingof
them,Caietal.(2021)followsanapproachbasedonneural
asparsitypatterncapturingimportantregionswhenthetrue
networksforgraphcoarsening. Fahrbachetal.(2020)com-
inverses have inherently different structures. The results
puteagraphcoarseningbasedonSchurcomplementswith
arediscussedindetailinAppendixD.1.
thegoalofobtainingembeddingsofrelevantnodes. Deng
etal.(2020)pursueasimilargoal. Acoarseningapproach
5.RelatedWork
basedonoptimaltransportisdonebyMa&Chen(2021).
The coarsening of graphs for the scalability of GNNs is
StatisticalPhysics. TheIsingmodelwasoneofthefirst
consideredbyHuangetal.(2021)andJinetal.(2022b;a)
tractableanalyticalmodelsofmagneticsystemsthatexhibit
performdatasetcondensationforgraphdataforthesame
phasetransitions. Interestingly,theclosertheparameteris
scalabilityreasons.ThegoalisthatGNNsperformsimilarly
toitscriticalvalue,thelessthedetailsofthesystemmatter.
onthecondenseddatabutarefastertotrain. Finally,Chen
This observation, referred to as universality in statistical
etal.(2022)providesanoverviewofsuccessfulcoarsening
physics,hasmotivatedthegeneralizationoftheIsingmodel
techniquesforscientificcomputing.
to broader classes of systems. For instance, the standard
PottsmodelisadirectgeneralizationoftheIsingmodelto As for graph sparsification, Spielman & Teng (2011) do
multipleclasses(Wu,1982). Bycontrast,theIsingmodel spectralsparsification,Yuetal.(2022)useaninformation-
withcontinuousstatesx i ∈[−1,1]isknownasaspinglass theoretic formulation for sparsification, and Batson et al.
system(Nishimori,2001). (2013)provideanoverviewonthetopicofsparsification.
Ushijima-Mwesigwaetal.(2017)performgraphpartition-
Modelsfromstatisticalphysicshave,foralongtime,been
ingusingquantumannealingbyminimizinganIsingobjec-
used to inspire and understand neural networks (Carleo
tive.
et al., 2019). Notably, spin glasses are generally known
asHopfieldnetworksinthemachinelearningcommunity
(MacKay,2003). Again,thesehavecontinuousstatesx ∈ 6.DiscussionandConclusion
i
[−1,1],yettheupdateruleforaHopfieldnetworkremains
Wepresentedanovelapproachforgraphsubsamplingby
thesameasforthemean-fieldapproximationoftheIsing
learningtheexternalmagneticfieldintheIsingmodeland
model. Over the past few years, there has been renewed
sampling from the resulting distribution. Moreover, we
interestinHopfieldnetworksasanunsupervisedmodelof
demonstratedthatourapproachexhibitspromisingpotential
associativememory(Krotov&Hopfield,2016;Demircigil
inawiderangeofapplications,withnorestrictiononthe
etal.,2017;Ramsaueretal.,2020).
differentiabilityofthelossfunctioninaspecificapplication.
Probabilistic Graphical Models. The Ising model, or WedidthisusingtheREINFORCELeave-One-Outgradient
spinglassesmoregenerally, gaverisetotheconceptofa
8IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
estimatorandacarefullyselectedregularizationstructure Yuille, A. L. DeepLab: Semantic image segmentation
thatallowsustocontrolthesamplingfraction. Ourmethod with deep convolutional nets, atrous convolution, and
wasdemonstratedonthreedistinctapplications. Inimage fully connected CRFs. IEEE Transactions on Pattern
segmentation,weshowedthatourapproachproducedclear AnalysisandMachineIntelligence,40(4):834–848,2017.
segmentationmaskswithsharpborders.Inmeshcoarsening,
wedemonstratedthatthelearnedmagneticfieldpromotes Chen, M., Wei, Z., Huang, Z., Ding, B., and Li, Y. Sim-
samplinginhighcurvatureregionswhileretainingthechar- pleanddeepgraphconvolutionalnetworks. InInterna-
acteristic“everyother”pattern. Finally,weshowedthatour tionalConferenceonMachineLearning,pp.1725–1735.
approachcanbeusedtolearnpromisingsparsitypatterns PMLR,2020.
for sparse approximate matrix inverse determination via
Chen, X., Golovinskiy, A., and Funkhouser, T. A bench-
Frobeniusnormminimization.
markfor3dmeshsegmentation. ACMTransactionson
LimitationsandFutureWork. Inthecurrentapplications, Graphics(TOG),28(3):1–12,2009.
thecomputationsarerelativelyfast(seeTable5inAppendix
C),butthismaychangeinlarge-scaleapplications,where Cipra, B. A. An introduction to the Ising model. The
thesampling,includingthegraphcoloring,canbecomea AmericanMathematicalMonthly,94(10):937–959,1987.
bottleneck. Thisissomethingwewouldliketoaddressin
futurework. Davis, T. A. and Hu., Y. The university of florida sparse
matrixcollection. ACMTransactionsonMathematical
AnotherlimitationisthattheIsingmodelonlyallowsbinary
Software,38(1):1–25,2011.
states,whereassomeapplications,e.g.,imageclassification,
would require multiple classes. We believe it would be
Demircigil, M., Heusel, J., Lo¨we, M., Upgang, S., and
possible to extend our approach to such cases by instead
Vermet,F. Onamodelofassociativememorywithhuge
using the Potts model or the even more general random
storagecapacity. JournalofStatisticalPhysics,168:288–
clustermodel(Fortuin&Kasteleyn,1972).
299,2017.
Acknowledgements Deng, C., Zhao, Z., Wang, Y., Zhang, Z., and Feng, Z.
Graphzoom: Amulti-levelspectralapproachforaccurate
ThisworkwaspartiallysupportedbytheWallenbergAI,Au- andscalablegraphembedding. InInternationalConfer-
tonomousSystemsandSoftwareProgram(WASP)funded enceonLearningRepresentations,2020.
bytheKnutandAliceWallenbergFoundation.
Duhamel, P. and Vetterli, M. Fast Fourier transforms: a
References tutorialreviewandastateoftheart. SignalProcessing,
19(4):259–299,1990.
Batson,J.,Spielman,D.A.,Srivastava,N.,andTeng,S.-H.
Spectralsparsificationofgraphs: theoryandalgorithms. Fahrbach, M., Goranci, G., Peng, R., Sachdeva, S., and
CommunicationsoftheACM,56(8):87–94,2013. Wang, C. Fastergraphembeddingsviacoarsening. In
InternationalConferenceonMachineLearning,pp.2953–
Bravo-Hermsdorff,G.andGunderson,L. Aunifyingframe-
2963.PMLR,2020.
work for spectrum-preserving graph sparsification and
coarsening. InAdvancesinNeuralInformationProcess- Fortuin,C.M.andKasteleyn,P.W. Ontherandom-cluster
ingSystems,volume32,2019. model: I.introductionandrelationtoothermodels. Phys-
ica,57(4):536–564,1972.
Cai,C.,Wang,D.,andWang,Y.Graphcoarseningwithneu-
ralnetworks. InInternationalConferenceonLearning
Geiger,M.andSmidt,T. e3nn: Euclideanneuralnetworks.
Representations,2021.
arXivpreprintarXiv:2207.09453,2022.
Carleo,G.,Cirac,I.,Cranmer,K.,Daudet,L.,Schuld,M.,
Geman,S.andGeman,D. Stochasticrelaxation,Gibbsdis-
Tishby,N.,Vogt-Maranto,L.,andZdeborova´,L.Machine
tributions,andtheBayesianrestorationofimages. IEEE
learningandthephysicalsciences. ReviewsofModern
Transactions on Pattern Analysis and Machine Intelli-
Physics,91(4):045002,2019.
gence,6:721–741,1984.
Chen,J.,Saad,Y.,andZhang,Z. Graphcoarsening: from
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and
scientificcomputingtomachinelearning. SeMAJournal,
Dahl,G.E. Neuralmessagepassingforquantumchem-
pp.1–37,2022.
istry. InInternationalConferenceonMachineLearning,
Chen,L.-C.,Papandreou,G.,Kokkinos,I.,Murphy,K.,and pp.1263–1272.PMLR,2017.
9IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Grote,M.J.andHuckle,T. Parallelpreconditioningwith Lample,G.,Ballesteros,M.,Subramanian,S.,Kawakami,
sparseapproximateinverses. SIAMJournalonScientific K.,andDyer,C. Neuralarchitecturesfornamedentity
Computing,18(3):838–853,1997. recognition. InProceedingsofthe2016Conferenceof
theNorthAmericanChapteroftheAssociationforCom-
Gustafsson,F.K.,Danelljan,M.,Timofte,R.,andScho¨n, putationalLinguistics: HumanLanguageTechnologies,
T.B. Howtotrainyourenergy-basedmodelforregres- pp.260–270,2016.
sion. InProceedingsoftheBritishMachineVisionCon-
Lindeberg,T. Scale-spacetheoryincomputervision,vol-
ference(BMVC),September2020.
ume256. SpringerScience&BusinessMedia,2013.
Hackbusch,W. Multi-gridmethodsandapplications,vol-
Loukas,A.Graphreductionwithspectralandcutguarantees.
ume4. SpringerScience&BusinessMedia,2013.
Journal of Machine Learning Research, 20(116):1–42,
2019.
Ha¨usner,P.,O¨ktem,O.,andSjo¨lund,J. Neuralincomplete
factorization: learningpreconditionersfortheconjugate Loukas,A.andVandergheynst,P. Spectrallyapproximating
gradientmethod. arXivpreprintarXiv:2305.16368,2023. largegraphswithsmallergraphs. InInternationalCon-
ference on Machine Learning, pp. 3237–3246. PMLR,
Huang,Z.,Zhang,S.,Xi,C.,Liu,T.,andZhou,M. Scaling 2018.
upgraphneuralnetworksviagraphcoarsening. InPro-
ceedingsofthe27thACMSIGKDDConferenceonKnowl- Ma,T.andChen,J. Unsupervisedlearningofgraphhier-
edgeDiscovery&DataMining,pp.675–684,2021. archicalabstractionswithdifferentiablecoarseningand
optimaltransport.InProceedingsoftheAAAIConference
Jin, W., Tang, X., Jiang, H., Li, Z., Zhang, D., Tang, J., onArtificialIntelligence,pp.8856–8864,2021.
and Yin, B. Condensing graphs via one-step gradient
MacKay,D.J. Informationtheory,inferenceandlearning
matching. In Proceedings of the 28th ACM SIGKDD
algorithms. CambridgeUniversityPress,2003.
ConferenceonKnowledgeDiscoveryandDataMining,
pp.720–730,2022a. Mallat, S. A wavelet tour of signal processing. Elsevier,
1999.
Jin, W., Zhao, L., Zhang, S., Liu, Y., Tang, J., and Shah,
N. Graph condensation for graph neural networks. In Miller,G.L.,Talmor,D.,andTeng,S.-H. Optimalcoars-
InternationalConferenceonLearningRepresentations, eningofunstructuredmeshes. JournalofAlgorithms,31
2022b. (1):29–65,1999.
Moore, N. S., Cyr, E. C., Ohm, P., Siefert, C. M., and
Jin, Y., Loukas, A., and JaJa, J. Graph coarsening with
Tuminaro, R. S. Graph neural networks and applied
preservedspectralproperties.InInternationalConference
linearalgebra. arXivpreprintarXiv:2310.14084,2023.
onArtificialIntelligenceandStatistics,pp.4452–4462.
PMLR,2020. Nishimori,H. Statisticalphysicsofspinglassesandinfor-
mation processing: an introduction. Clarendon Press,
Keros,A.D.andSubr,K. Generalizedspectralcoarsening. 2001.
arXivpreprintarXiv:2207.01146,2022.
Pumain,D.(ed.). HierarchyinNaturalandSocialSciences,
Kingma, D. and Ba, J. Adam: A method for stochastic volume3ofMethodosSeries. SpringerDordrecht,2006.
optimization. InInternationalConferenceonLearning
Ramsauer,H.,Scha¨fl,B.,Lehner,J.,Seidl,P.,Widrich,M.,
Representations,2015.
Gruber, L., Holzleitner, M., Adler, T., Kreil, D., Kopp,
M. K., et al. Hopfield networks is all you need. In
Koller,D.andFriedman,N. Probabilisticgraphicalmodels:
InternationalConferenceonLearningRepresentations,
principlesandtechniques. MITPress,2009.
2020.
Krotov,D.andHopfield,J.J. Denseassociativememoryfor
Reuter,M.,Biasotti,S.,Giorgi,D.,Patane`,G.,andSpagn-
patternrecognition. InAdvancesinNeuralInformation
uolo,M. DiscreteLaplace–Beltramioperatorsforshape
ProcessingSystems,volume29,2016.
analysisandsegmentation. Computers&Graphics,33
(3):381–390,2009.
Lafferty,J.D.,McCallum,A.,andPereira,F.C.Conditional
randomfields: Probabilisticmodelsforsegmentingand Ronneberger,O.,Fischer,P.,andBrox,T. U-net: Convolu-
labeling sequence data. In Proceedings of the Interna- tionalnetworksforbiomedicalimagesegmentation. In
tional Conference on Machine Learning, pp. 282–289, MedicalImageComputingandComputer-AssistedInter-
2001. vention–MICCAI2015,pp.234–241.Springer,2015.
10IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Safro,I.,Sanders,P.,andSchulz,C. Advancedcoarsening Zheng,S.,Jayasumana,S.,Romera-Paredes,B.,Vineet,V.,
schemesforgraphpartitioning. JournalofExperimental Su,Z.,Du,D.,Huang,C.,andTorr,P.H. Conditional
Algorithmics,19:1–24,2015. randomfieldsasrecurrentneuralnetworks. InProceed-
ingsoftheIEEEInternationalConferenceonComputer
Shi, J., Zhou, Y., Hwang, J., Titsias, M., and Mackey, L. Vision,pp.1529–1537,2015.
Gradientestimationwithdiscretesteinoperators. InAd-
vancesinNeuralInformationProcessingSystems, vol-
ume35,pp.25829–25841,2022.
Shuman,D.I.,Faraji,M.J.,andVandergheynst,P. Amulti-
scalepyramidtransformforgraphsignals. IEEETrans-
actionsonSignalProcessing,64(8):2119–2134,2015.
Sjo¨lund,J.andBa˚nkestad,M. Graph-basedneuralaccelera-
tionfornonnegativematrixfactorization. arXivpreprint
arXiv:2202.00264,2022.
Song,Y.andKingma,D.P. Howtotrainyourenergy-based
models. arXivpreprintarXiv:2101.03288,2021.
Sorkine, O. Laplacian mesh processing. Eurographics
(StateoftheArtReports),4(4),2005.
Spielman,D.A.andTeng,S.-H. Spectralsparsificationof
graphs. SIAMJournalonComputing,40(4):981–1025,
2011.
Tenbrinck,D.,Gaede,F.,andBurger,M. Variationalgraph
methods for efficient point cloud sparsification. arXiv
preprintarXiv:1903.02858,2019.
Ushijima-Mwesigwa, H., Negre, C. F., and Mniszewski,
S. M. Graph partitioning using quantum annealing on
thed-wavesystem. InProceedingsoftheSecondInter-
nationalWorkshoponPostMooresEraSupercomputing,
pp.22–29,2017.
Wah,C.,Branson,S.,Welinder,P.,Perona,P.,andBelongie,
S. Meshsegmentations. TechnicalReportCNS-TR-2011-
001,CaliforniaInstituteofTechnology,2011.
Williams,R.J. Simplestatisticalgradient-followingalgo-
rithmsforconnectionistreinforcementlearning. Machine
Learning,8:229–256,1992.
Wright, J., Ma, Y., Mairal, J., Sapiro, G., Huang, T. S.,
andYan, S. Sparserepresentationforcomputervision
andpatternrecognition. ProceedingsoftheIEEE,98(6):
1031–1044,2010.
Wu,F.-Y. ThePottsmodel. ReviewsofModernPhysics,54
(1):235,1982.
Yu,S.,Alesiani,F.,Yin,W.,Jenssen,R.,andPrincipe,J.C.
Principleofrelevantinformationforgraphsparsification.
InUncertaintyinArtificialIntelligence,pp.2331–2341.
PMLR,2022.
11IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
A.Isingsampling
ToillustratetheconvergencerateoftheIsingmodel,weplottheenergyagainstthenumberofMonteCarloiterationsin
Figure6. Thesesimulationsaredoneonthe20meshesintheBustdatasetinSection4.2. Weplotthemeanplusthestandard
dedicationofthemodelenergyateachiteration. Weobservethattheferromagnetic(J =1)orantiferromagnetic(J =−1)
Isingmodelconvergesfast,eventhoughtheantiferromagneticoneconvergesfasterinjustahandfulofiterations.
 7 H P S H U D W X U H        -        7 H P S H U D W X U H       -        7 H P S H U D W X U H     -      
   
       
       
   
       
   
       
                       
 0 & 0 &  L W H U D W L R Q V  0 & 0 &  L W H U D W L R Q V  0 & 0 &  L W H U D W L R Q V
 7 H P S H U D W X U H        -       7 H P S H U D W X U H       -       7 H P S H U D W X U H     -     
       
         
       
         
       
                       
 0 & 0 &  L W H U D W L R Q V  0 & 0 &  L W H U D W L R Q V  0 & 0 &  L W H U D W L R Q V
Figure6.ThemeanandstandarddeviationsoftheIsingmodelenergyagainstthenumberofMonteCarloiterationsforthe(J =1)or
antiferromagnetic(J =−1)Isingmodel.Thesimulationisalsodoneforthreedifferenttemperatures:0.01,0.1,and1.
B.Implementationdetails
Here,wegothroughthedetailsofourmodelsandtrainingproceduresfortheapplicationsinSection4. WeusedoneGPU,
NVIDIAGeForceRTX3090,foralltheexperiments,andweperformedalltheexperimentsusinglessthan10GPUhours.
B.1.U-Net
We trained a U-Net of the bird segmentation task in Section 4.1 using a U-Net with three down-sampling blocks, one
middlelayer,andthreeup-samplingblocks. Theimplementationisinspiredbyhttps://github.com/yassouali/
pytorch-segmentation/tree/master. Thedimensionsoftheblocks/layersarepresentedinTable3.
Table3. DimensionsoftheU-Netblocks.
LAYER INPUTDIM OUTPUTDIM
INPUTLAYER 3 16
ENCODER1 16 32
ENCODER2 32 64
ENCODER3 64 128
MIDLAYER 128 128
DECODER1 128 64
DECODER2 64 32
DECODER3 32 16
OUTPUTLAYER 16 1
12
 \ J U H Q H  J Q L V ,
 \ J U H Q H  J Q L V ,
 \ J U H Q H  J Q L V ,
 \ J U H Q H  J Q L V ,
 \ J U H Q H  J Q L V ,
 \ J U H Q H  J Q L V ,IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
WetrainthemodelusingtheAdamoptimizer(Kingma&Ba,2015),abatchsizeof32,andalearningrangeof1e-4. The
datasetcontains6537segmentedimages. Wesplitthedatasetinatrain/val/testsplitusingtheratios0.8/01/0.1. Wetrainthe
modelfor300epochsandtestthemodelwiththelowestevaluationerror. Here,weuseatemperatureofoneandJ =1for
theIsingmodel.
B.2.Equivariantgraphnetwork
TheequivariantgraphneuralnetworkinSection4.2isconstructedusingthee3nnlibrary(Geiger&Smidt,2022). Asthe
nodeinput,wederivethemeshLaplacianaccordingto
cotαij +cotβij ifi,j isanedge
 2
(cid:80)
C = − C ifiisinthediagonal (26)
ij j∈N(i) ij
0
otherwise,
whereandα andβ denotethetwoanglesoppositeofedge(i,j)(Sorkine,2005). Wegetameasureofthecurvatureof
ij ij
thenodeusing
(cid:88)
z0 =|| C (rˆ −rˆ )||2, (27)
i ij i j 2
i=N(i)
whererˆ isthepositionofnodei. Weusez0asthemodelnodeinput. Asedgeattributes,weusethesphericalharmonics
i i
expansion of the relative distance between node i and its neighbors j, Y ((rˆ −rˆ )/||rˆ −rˆ ||)) and the node-to-node
i j i j
distanced =||rˆ −rˆ ||. Weconstructanequivariantconvolutionbyfirstderivingthemessagefromtheneighborhoodas
ij i j
(cid:18) (cid:19)
zˆk+1 = 1 (cid:88) zk⊗ Y rˆ i−rˆ j , (28)
i n j fW(dij) ||rˆ −rˆ ||
d i j
j∈N(i)
wheren isthemeannumberofneighboringnodesofthegraph. Foratriangularmesh,thedegreeiscommonlysix. The
d
formula⊗ definesthetensorproductwithdistantdependentweightsf (d j);thisisatwo-layerneuralnetwork
fW(dij) W i
withlearnableweightswthatderivestheweightsofthetensorproduct. Thenodefeaturesarethenupdatedaccordingto
zk+1 =zk+α·zˆk+1, (29)
i i i
whereαisalearnableresidualparameterdependentonzk. Wetookinspirationforthemodelfromthee3nnhomepage,
i
see https://github.com/e3nn/e3nn/blob/main/e3nn/nn/models/v2106/points_convolution.
py.
Weconstructatwo-layerequivariantnetworkwherethelayerirreduciblerepresentationsare[0e,16x0e+16x1o,16x0e].
Thismeansthattheinput(0e)consistsofavectorof16scalars,andthehiddenlayersconsistofvectorswith16scalars
(16x0e)togetherwith16vectors(16x1o)ofoddparity. This,inturn,mapstotheoutputvectorof16scalars(16x0e). We
subtracttherepresentationwiththemeanofallnodesinthegraphandfinallymapthisrepresentationtoasinglescalar
output. Here,weuseatemperatureofoneandJ =−1fortheIsingmodel.
Foroptimization,weemploytheAdamoptimizer(Kingma&Ba,2015)withalearningrateof0.001andabatchsizeof
one. Wetrainthemodeluntilconvergenceonthetrainingsetbutwithamaximumnumberof50epochs. Wedidnotusea
validationsetastherewerenoindicationsofoverfitting.
B.3.Graphconvolutionalneuralnetwork
Here,weusethesimpleanddeepgraphconvolutionalnetworksfromChenetal.(2020)wherewesetthestrengthofthe
initialresidualconnectionαto0.1andθ,thehyperparametertocomputethestrengthoftheidentityto0.5. Thenetwork
contains4layers,andeachhasahiddendimensionof64. Weutilizeweightsharingacrosstheselayers. Thedimensionof
theoutputofthefinalconvolutionallayeris64. Thisoutputissubtractedwiththemeanofthenodevaluesbeforeafinal
linearlayerreducesthesizetoone.
Foroptimization,weemploytheAdamoptimizer(Kingma&Ba,2015)withalearningrateof0.01. Wetrainthemodel
untilconvergenceonthetrainingsetbutwithamaximumnumberof300epochs,astherewerenoindicationsofoverfitting.
Here,weuseatemperatureofoneandJ =−0.4fortheIsingmodel.
13IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Figure7. Examplesofthemagneticfieldfromthebustdataset(testset).
C.Meshcoarseningdetails
ThissectioncontainsextrainformationonthemeshcoarseningexperimentsinSection4.2. Thedetailsofthemeshdataset
arecompiledinTable4. Thetimeittakestosamplepointsandconstructthecoursemeshforthedifferentmethodsis
presentedinTable5. Additionalpredictedmagneticfieldsfromthetestsplitfromthefourdifferentdatasetsarefoundin
Figures7,8,9,and10. AnillustrationofthecoursemeshesaftersamplingispresentedinFigure11.
Table4. DetailsofthemeshesinthedatasetusedinSection4.2.
DATASET BUST FOURLEG HUMAN CHAIR
NUMBEROFVERTICES 25662(1230) 9778(3671) 8036(4149) 10154(999)
NUMBEROFFACES 51321(2460) 19553(7342) 16070(8302) 20313(2002)
Table5. Meansamplingandcoarseningtimeforthethreedifferentmethods.
BUST FOURLEG HUMAN CHAIR
MODEL SAMP COARS SAMP COARS SAMP COARS SAMP COARS
ISING+MAG 0.19(0.064) 1.30(0.53) 0.13(0.050) 0.37(0.16) 0.13(0.052) 0.45(0.16) 0.33(0.50) 0.46(0.077)
ISING 0.13(0.048) 1.08(0.41) 0.10(0.028) 0.34(0.15) 0.10(0.30) 0.42(0.17) 0.10(0.034) 0.39(0.071)
RANDOM 2.4E-4(4E-4) 2.07(0.83) 1E-4(2E-4) 0.52(0.24) 9.23E-5(3.1E-4) 0.68(0.31) 1.75E-4(2.86E-4) 0.61(0.14)
D.SparseApproximateMatrixInverses
D.1.Resultsandimplementationdetails
Withourinitialmodelarchitecture,thegraphrepresentationinducesanaprioriassumptiononthesparsitypatternofthe
predictedsparseapproximateinverseinwhichweaimtoselect50%oftheelementsinthesparsitypatternofA+A2. Our
approachis,however,notlimitedtothechoiceofanapriorisparsitypattern. Implementation-wise,weliftthisconstraintby
addinganextraedgefeatureconsistingofanall-onesn×nmatrixinthegraphrepresentationoftheinputmatrix,allowing
foraselectionof50%ofallmatrixelementsinthesparseapproximationoftheinverse. Astheapriorisparsitypatternis
alreadyincorporatedintothegraphrepresentationoftheinputmatrix,thisrendersourmethodflexibleintermsofchoiceof
apriorisparsitypattern,whichinourcaseshouldbedenserthanthedesiredsparsitylevel. Thisflexibilityisexpectedtobe
especiallyimportantforcomputationalefficiencyinusecasesinvolvinglargematrixdimensions.
Giventhecurrentstateofthetrainableparameters,twosamplesfromtheIsingmodelarerequiredduringtrainingtoobtain
thegradientestimate. Evaluationiscarriedoutusingasinglesample. Inbothcases,thesamplexfromtheIsingmodel
producedafterT MCMCiterationsispassedtothefinallayer,whichsolvesthethenoptimizationproblems,
s∗ =argmin∥AM (s )−I ∥2,
k k k k 2 (30)
s′∈Rmk
k
14IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Figure8. Examplesofthemagneticfieldfromthefour-legdataset(testset).
Figure9. Examplesofthemagneticfieldfromthehumandataset(testset).
computingthevaluess∗ ∈Rm ofthenon-zeroentriesofeachcolumnM ofM. Thus,thefinaloutputisthepredicted
k k
sparseapproximateinverseM. Empirically,T =3isfoundtobesufficient.
Independentonwhichdatasetisused,60%ofthesamplesareusedfortraining,andtheremaining40%aredividedequally
betweenavalidationsetandatestset. Weusethemagneticfield-dependentregularizationdescribedinSection3tooptimize
forasamplingfractionof50%oftheapriorisparsitypattern. Inallsettings,thesamplingfractionconvergestoanaverage
valuecloseto50%withtheproposedregularizationscheme. Themeanrecordedsamplingfractiononthetestdatasetin
Setting1andSetting2is52.4%and50.9%,respectively. ThecorrespondingsamplingfractioninSetting3is49.8%. The
baselinemethodsaretunedtoallowforthesamesamplingfractionduringevaluation.
Results for two samples from the test dataset in Setting 3 are visualized in Figure 5. The first sample (top) shows the
predictedsparsitypatternwhenthetrueinverseisdense. Forthissample,therecordedlossis2.61(ISING+MAG)compared
to 3.57 (Ising), 4.18 (Random) and 4.12 (Only A). As discussed in Figure 2, the magnitude of the local magnetic field
influencesthesamplingprobabilityinthatregion,whichcanbeseenbycomparingtheoutputmagneticfieldtotheobtained
sparsitypatternforbothsamples. AzeromagneticfieldintheIsingmodelcanbeinterpretedasaprioronthesparsitypattern
inwhich50%oftheelementsineachrowandcolumnofthematrixareselected,resultinginaninverseapproximationin
whichthenonzeroelementsareevenlydistributedacrossthegivenmatrix. Forthesecondsample(bottom),thesparsity
optimizedforislowerthanthesparsityofthetrueinverse. Wechoosetodisplaythissampleastheresultsclearlyillustrate
thatthelearnedmagneticfieldindeedresultsinanIsingmodelfromwhichthesampledsparsitypatternsuccessfullyavoids
themostunimportantregionsinthetrueinversefortheparticularinputmatrix. Forthissample,therecordedlossis1.02
(ISING+MAG)comparedto3.75(Ising),3.76(Random)and4.06(OnlyA).
15IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Figure10. Examplesofthemagneticfieldfromthechairdataset(testset).
Original Ising + mag Ising Random
d(V,M) = 0.038 d(V,M) = 0.050 d(V,M) = 0.060
Figure11.Thecoarsenedmeshusingrandomsubsampling,Isingsubsampling,andlearnedIsingsubsamplingofthenodes.Thepoint-to-
meshdistanceisreducedfrom0.06to0.038whengoingfromtherandomsubsamplingtothelearnedIsinggraph.Thetimeittakesto
sampleandcreatethecoursemeshare:random,1.37sec,Ising0.67sec,andIsing+mag1.31sec.Thereasontherandomsamplingtakes
longertimeisduetoamorecomplicatedcoursemeshcreation.
D.2.Datasets
Dataset1. Thisisasyntheticdatasetcontaining1600binarymatrices.Eachmatrixisreal,symmetric,andofdimensionality
30×30. Thematricesinthedatasetareautomaticallygeneratedbasedontheprinciplethattheprobabilityofanonzero
elementincreasestowardsthediagonal,with100%probabilityofnonzerodiagonalelements. Theupperrightandlowerleft
cornershaveaverylowprobabilityofbeingnonzero,andinbetweentheseelementsandthediagonal,theprobabilityofa
nonzeroelementvariesaccordingtoanonlinearfunction. Themeansparsityis83%zero-elementsinthegenerateddataset.
The maximum sparsity allowed in the data-generating process is 96%, and the determinant of each matrix is bounded
between0.001and50. Figure12showssparsitypatternsoffourmatricesinthefinaldataset.
Thedatasetisavailableuponrequestandwillbereleasedtogetherwiththecodeusedforpreprocessingandgeneration.
Dataset2. Thisisasyntheticdatasetcontaining1800submatricesofsize30×30constructedfromtheSuiteSparseMatrix
Collection (Davis & Hu., 2011). Since the original matrices in the dataset are often denser closer to the diagonal, the
submatricesareconstructedbyiteratingawindowofsize30×30alongthediagonals,suchthatnosubmatrixoverlaps
another submatrix. To ensure that each submatrix is symmetric, only the upper triangular part of the matrix is used to
createeachmatrix. Eachsubmatrixisscaledsuchthattheabsolutevalueofthemaximumelementisone,andsubmatrices
withallvaluesbelow10−6 areremovedfromthedataset. Themeansparsityinthefinaldatasetis89%. Themaximum
sparsityallowedinthedata-generatingprocessis96%,andthedeterminantofeachmatrixisboundedbetween0.001and
50. Figure13showssparsitypatternsoffourmatricesinthefinaldataset.
16IsingontheGraph:Task-specificGraphSubsamplingviatheIsingModel
Figure12. ExampleofsamplesfromDataset1.Nonzeroelementsarerepresentedinwhite.
Figure13. ExampleofsparsitypatternsofsamplesfromDataset2.Nonzeroelementsarerepresentedinwhite.
TheoriginaldatasetisundertheCC-BY4.0License. Ourmodifieddatasetisavailableuponrequestandwillbereleased
withthecodeusedforpreprocessingandgeneration.
17