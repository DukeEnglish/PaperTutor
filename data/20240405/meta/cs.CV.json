[
    {
        "title": "Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning",
        "authors": "Rui LiTobias FischerMattia SeguMarc PollefeysLuc Van GoolFederico Tombari",
        "links": "http://arxiv.org/abs/2404.03658v1",
        "entry_id": "http://arxiv.org/abs/2404.03658v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03658v1",
        "summary": "Recovering the 3D scene geometry from a single view is a fundamental yet\nill-posed problem in computer vision. While classical depth estimation methods\ninfer only a 2.5D scene representation limited to the image plane, recent\napproaches based on radiance fields reconstruct a full 3D representation.\nHowever, these methods still struggle with occluded regions since inferring\ngeometry without visual observation requires (i) semantic knowledge of the\nsurroundings, and (ii) reasoning about spatial context. We propose KYN, a novel\nmethod for single-view scene reconstruction that reasons about semantic and\nspatial context to predict each point's density. We introduce a vision-language\nmodulation module to enrich point features with fine-grained semantic\ninformation. We aggregate point representations across the scene through a\nlanguage-guided spatial attention mechanism to yield per-point density\npredictions aware of the 3D semantic context. We show that KYN improves 3D\nshape recovery compared to predicting density for each 3D point in isolation.\nWe achieve state-of-the-art results in scene and object reconstruction on\nKITTI-360, and show improved zero-shot generalization compared to prior work.\nProject page: https://ruili3.github.io/kyn.",
        "updated": "2024-04-04 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03658v1"
    },
    {
        "title": "OW-VISCap: Open-World Video Instance Segmentation and Captioning",
        "authors": "Anwesa ChoudhuriGirish ChowdharyAlexander G. Schwing",
        "links": "http://arxiv.org/abs/2404.03657v1",
        "entry_id": "http://arxiv.org/abs/2404.03657v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03657v1",
        "summary": "Open-world video instance segmentation is an important video understanding\ntask. Yet most methods either operate in a closed-world setting, require an\nadditional user-input, or use classic region-based proposals to identify never\nbefore seen objects. Further, these methods only assign a one-word label to\ndetected objects, and don't generate rich object-centric descriptions. They\nalso often suffer from highly overlapping predictions. To address these issues,\nwe propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),\nan approach to jointly segment, track, and caption previously seen or unseen\nobjects in a video. For this, we introduce open-world object queries to\ndiscover never before seen objects without additional user-input. We generate\nrich and descriptive object-centric captions for each detected object via a\nmasked attention augmented LLM input. We introduce an inter-query contrastive\nloss to ensure that the object queries differ from one another. Our generalized\napproach matches or surpasses state-of-the-art on three tasks: open-world video\ninstance segmentation on the BURST dataset, dense video object captioning on\nthe VidSTG dataset, and closed-world video instance segmentation on the OVIS\ndataset.",
        "updated": "2024-04-04 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03657v1"
    },
    {
        "title": "MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation",
        "authors": "Hanzhe HuZhizhuo ZhouVarun JampaniShubham Tulsiani",
        "links": "http://arxiv.org/abs/2404.03656v1",
        "entry_id": "http://arxiv.org/abs/2404.03656v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03656v1",
        "summary": "We present MVD-Fusion: a method for single-view 3D inference via generative\nmodeling of multi-view-consistent RGB-D images. While recent methods pursuing\n3D inference advocate learning novel-view generative models, these generations\nare not 3D-consistent and require a distillation process to generate a 3D\noutput. We instead cast the task of 3D inference as directly generating\nmutually-consistent multiple views and build on the insight that additionally\ninferring depth can provide a mechanism for enforcing this consistency.\nSpecifically, we train a denoising diffusion model to generate multi-view RGB-D\nimages given a single RGB input image and leverage the (intermediate noisy)\ndepth estimates to obtain reprojection-based conditioning to maintain\nmulti-view consistency. We train our model using large-scale synthetic dataset\nObajverse as well as the real-world CO3D dataset comprising of generic camera\nviewpoints. We demonstrate that our approach can yield more accurate synthesis\ncompared to recent state-of-the-art, including distillation-based 3D inference\nand prior multi-view generation methods. We also evaluate the geometry induced\nby our multi-view depth prediction and find that it yields a more accurate\nrepresentation than other direct 3D inference approaches.",
        "updated": "2024-04-04 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03656v1"
    },
    {
        "title": "RaFE: Generative Radiance Fields Restoration",
        "authors": "Zhongkai WuZiyu WanJing ZhangJing LiaoDong Xu",
        "links": "http://arxiv.org/abs/2404.03654v1",
        "entry_id": "http://arxiv.org/abs/2404.03654v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03654v1",
        "summary": "NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.",
        "updated": "2024-04-04 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03654v1"
    },
    {
        "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching",
        "authors": "Dongzhi JiangGuanglu SongXiaoshi WuRenrui ZhangDazhong ShenZhuofan ZongYu LiuHongsheng Li",
        "links": "http://arxiv.org/abs/2404.03653v1",
        "entry_id": "http://arxiv.org/abs/2404.03653v1",
        "pdf_url": "http://arxiv.org/pdf/2404.03653v1",
        "summary": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.",
        "updated": "2024-04-04 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.03653v1"
    }
]