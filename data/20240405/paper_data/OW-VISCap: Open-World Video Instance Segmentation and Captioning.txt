OW-VISCap: Open-World Video Instance
Segmentation and Captioning
Anwesa Choudhuri, Girish Chowdhary, and Alexander G. Schwing
University of Illinois at Urbana-Champaign
{anwesac2,girishc,aschwing}@illinois.edu
https://anwesachoudhuri.github.io/OpenWorldVISCap/
Abstract. Open-world video instance segmentation is an important
video understanding task. Yet most methods either operate in a closed-
worldsetting,requireanadditionaluser-input,oruseclassicregion-based
proposals to identify never before seen objects. Further, these methods
onlyassignaone-wordlabeltodetectedobjects,anddon’tgeneraterich
object-centric descriptions. They also often suffer from highly overlap-
pingpredictions.Toaddresstheseissues,weproposeOpen-WorldVideo
Instance Segmentation and Captioning (OW-VISCap), an approach to
jointly segment, track, and caption previously seen or unseen objects
in a video. For this, we introduce open-world object queries to discover
never before seen objects without additional user-input. We generate
rich and descriptive object-centric captions for each detected object via
amaskedattentionaugmentedLLMinput.Weintroduceaninter-query
contrastivelosstoensurethattheobjectqueriesdifferfromoneanother.
Ourgeneralizedapproachmatchesorsurpassesstate-of-the-artonthree
tasks: open-world video instance segmentation on the BURST dataset,
densevideoobjectcaptioningontheVidSTGdataset,andclosed-world
video instance segmentation on the OVIS dataset.
1 Introduction
Open-worldvideoinstancesegmentation(OW-VIS)involvesdetecting,segment-
ing and tracking previously seen or unseen objects in a video. This task is chal-
lengingbecausetheobjectsareoftenneverseenduringtraining,areoccasionally
partlyorentirelyoccluded,theappearanceandpositionoftheseobjectschanges
over time, and because the objects may leave the scene only to re-appear at a
later time. Addressing these challenges to obtain an accurate method for OW-
VISthatworksonlineiscrucialinfieldslikeautonomoussystems,andaugmented
as well as virtual reality, among others.
Some recent methods based on abstract object queries perform remarkably
well for closed-world video instance segmentation [7,13,18,50]. These works
assume a fixed set of object categories during training and evaluation. However,
itisunrealistictoassumethatallobjectcategoriesareseenduringtraining.For
example,inFig.1,thetrailertruck(toprow)highlightedinyellow,andthelawn
mower(bottomrow)highlightedingreen,areneverseenbeforeduringtraining.
4202
rpA
4
]VC.sc[
1v75630.4042:viXra2 A. Choudhuri et al.
a large construction
truck with a trailer on it.
a car is driving in the
rain on a street.
...
a tractor with black and
orange front and rear.
a woman is riding an
orange lawn mower.
a white dog near a
tractor.
Fig.1: OW-VISCapisabletosimultaneouslydetect,trackandcaptionobjectsinthe
given video frames. The first example (top row) shows a road scene with a previously
unseen trailer truck and cars which are seen during training. The second example
(bottomrow)showsapersononalawnmower,andadogonthegrass.Thelawnmower
isn’t part of the training set. We generate meaningful object-centric captions even for
objects never seen during training. The captions for unseen objects are underlined.
Forthisreason,open-worldvideoinstancesegmentation(OW-VIS)hasbeen
proposed[2,10,27,28,39,44].CurrentworksonOW-VISsufferfromthefollowing
threemainissues.Firstly,theyoftenrequireaprompt,i.e.,additionalinputfrom
the user, ground-truth or another network. The prompts can be in the form of
points, bounding boxes or text. These methods only work when the additional
inputs are available, making them less practical in the real-world. Prompt-less
OW-VIS methods [2,10,27,28,39,44] sometimes rely on classic region-based
objectproposals[2,27,28,44],oronlyoperateononekindofobjectqueryforboth
the open- and the closed-world [10,39], which may result in sub-optimal results
(shown later in Tab. 4). Secondly, all methods on video instance segmentation,
closed-oropen-world,assignaone-wordlabeltothedetectedobjects.However,a
onewordlabelisoftennotsufficienttodescribeanobject.Theabilitytogenerate
richobject-centricdescriptionsisimportant,especiallyintheopen-worldsetting.
DVOC-DS [58] jointly addresses the task of closed-world object detection and
object-centric captioning in videos. However, it is not clear how DVOC-DS [58]
can be extended to an open-world setting. Besides, the features from only the
individual object trajectories are used for object-centric captioning in DVOC-
DS [58], so the overall context from the entire video frames may be lost in this
method.DVOC-DS[58]alsostruggleswithverylongvideos,andcannotcaption
multiple action segments within a single object trajectory because the method
produces a single caption for the entire object trajectory. Thirdly, some of the
aforementioned works [7,8,13,18] suffer from multiple similar object queries
resulting in repetitive predictions. Non-maximum suppression, or other post-
processing techniques may be necessary to suppress the repetitions and highly
overlapping false positives.OW-VISCap 3
We address the three aforementioned issues through our Open-World Video
Instance Segmentation and Captioning (OW-VISCap) approach: it simultane-
ously detects, segments and generates object-centric captions for objects in a
video. Fig. 1 shows two examples in which our method successfully detects, seg-
ments and captions both closed- and open-world objects.
To address the first issue, our OW-VISCap combines the advantages of
both prompt-based and prompt-less methods. We introduce open-world object
queries, in addition to closed-world object queries used in prior work [8]. This
encourages discovery of never before seen open-world objects without compro-
misingtheclosed-worldperformancemuch.Notably,wedonotrequireadditional
prompts from the ground truth or separate networks. Instead, we use equally
spaced points distributed across the video frames as prompts and encode them
to form open-world object queries, which enables discovery of new objects. The
equally spaced points incorporate information from different spatial regions of
the given video-frames. We also introduce a specifically tailored open-world loss
to train the open-world object queries to discover new objects.
To address the second issue, OW-VISCap includes a captioning head to pro-
duce an object-centric caption for each object query, both open- and closed-
world. We use masked cross attention in an object-to-text transformer in the
captioning head to generate object-centric text queries, that are then used by a
frozen large language model (LLM) to produce an object-centric caption. Note,
maskedattentionhasbeenusedforclosed-worldobjectsegmentation[7,8].How-
ever, to our best knowledge it has not been used for object captioning before.
The masked cross attention helps focus on the local object features, whereas
the self attention in the object-to-text transformer incorporates overall context
by looking at the video-frame features. Moreover, unlike DVOC-DS [58], we are
able to handle long videos and multiple action segments within a single object
trajectory because we process short video clips sequentially and combine the
clips using CAROQ [13].
To address the third issue, we introduce an inter-query contrastive loss for
both open- and closed-world object queries. It encourages the object queries
to differ from one another. This prevents repetitive predictions and encourages
novel object discovery in the open-world. Note that this contrastive loss also
helps in closed-world video instance segmentation by automatically encouraging
non-maximum suppression, and by removing highly overlapping false positive
predictions.
To demonstrate the efficacy of our OW-VISCap on open-world video in-
stance segmentation and captioning, we evaluate this approach on three diverse
andchallengingtasks:open-worldvideoinstancesegmentation(OW-VIS),dense
video object captioning (Dense VOC), and closed-world video instance segmen-
tation(VIS).Weachieveaperformanceimprovementof∼6%onthepreviously
unseen (uncommon) categories in the BURST [2] dataset for OW-VIS, and a
∼7% improvement on the captioning accuracy for detected objects on the Vid-
STG[57]datasetfortheDenseVOCtask,whileperformingsimilartothestate-4 A. Choudhuri et al.
EP nro com dp et r Object Transformer Class Head Foreground AM ttea ns tk ioe nd
Image
. . . . . Features Det. Head Cap. Head
. . . . . Object-to-Text
LLM *
. . . . . A rabbit sitting Transformer
. . . . . Cap. Head inside a box.
Grid of points A rabbit sitting inside a box.
Fig.2: TheleftfigureshowsanoverviewofourOW-VISCap(Sec.3.1).Weintroduce
open-world object queries q (Sec. 3.2) and a captioning head (Sec. 3.3). The open-
ow
worldobjectqueriesaregeneratedbyencodingagridofpointsalongtheimage-feature
dimensions via a prompt encoder (shown in purple). The right figure details the cap-
tioning head (Sec. 3.3) for object-centric captioning. We use masked attention in the
object-to-text transformer of the captioning head.
of-the-art on the closed-world VIS task on the OVIS data (our AP score is 25.4
as compared to a score of 25.8 for a recent VIS SOTA, CAROQ [13]).
2 Related Work
2.1 Open-world Video Instance Segmentation
Prompt-less methods. Several works have explored the open-world video in-
stance segmentation problem without requiring initial prompts [2,10,28,39,44].
New objects are discovered based on an objectness score. Some works [2,28]
relyonclassicregion-basedobjectproposals.However,recentquery-basedmeth-
ods [10,39] have shown to outperform those methods for object detection and
segmentation. OW-VISFormer [39] proposes a feature enrichment mechanism
and a spatio-temporal objectness module to distinguish the foreground objects
fromthebackground.Differentfromourwork,theyoperateononetypeofobject
query.AlsoorthogonaltoourworkisDEVA[10],whichdevelopsaclass-agnostic
temporalpropagationapproachtotrackobjectsdetectedbyasegmentationnet-
work.UVO[44]andBURST[2]providenoveldatasetsandregion-proposal-based
baselines for the task of open-world video instance segmentation. In this work,
we evaluate our approach on the BURST [2] dataset as it is more diverse and
allows to separately evaluate common and uncommon classes.
Prompt-based methods. Prompt-based methods rely on prompts, i.e., prior
knowledge, to segment objects in videos. Video object segmentation (VOS) [11,
17,19,20,35,40,43,51,56] is a task where the ground truth segmentations of
objects (belonging to any category) in the first frame are available, and act as
prompts to track the objects throughout the video. CLIPSeg [31] uses words
as prompts, by projecting the words onto the image space following CLIP [38],
and calculating a similarity matrix to obtain a segmentation mask for the given
word prompt. Prompt-based methods work when one has some prior knowledge
on what or where to look for. However, in a true open-world setting, such prior
knowledge may not be available. We operate in such a setting.
In this work, we combine the advantages of both prompt-based and prompt-
less methods. Notably, we do not require additional prompts from the groundOW-VISCap 5
truth or separate networks. Instead, we use equally spaced points distributed
overthevideoframesandencodethemaspromptstodiscovernewobjects.Also
note, that all the aforementioned methods, prompt-less or prompt-based, assign
only a one-word label to the detected object. In contrast, we are interested in
generating a rich object-centric caption for the given object in this work.
2.2 Dense Video Object Captioning
Dense video object captioning involves detecting, tracking, and captioning tra-
jectories of objects in a video. For this task, DVOC-DS [58] extends the image-
based GRiT [47] and trains it with a mixture of disjoint tasks. In addition,
existing video grounding datasets VidSTG [57] and VLN [41] are re-purposed
for this task. However, DVOC-DS [58] cannot caption multiple action segments
within a single object trajectory because the method produces a single caption
for the entire object trajectory. In addition, similar to many other video mod-
els [1,45,48], DVOC-DS [58] struggles with very long videos and only processes
upto200frames.Themethodisfurtherconstrainedtohandleonlyknownobject
categories and it is unclear how the method extends to an open-world setting.
UnlikeDVOC-DS[58],weuseopen-worldobjectqueriestotackleopen-world
video instance segmentation. Differently, in this work, we also process video-
frames sequentially using a short temporal context. Hence, our method is able
toprocesslongvideos,aswellashandlemultipleactionsegmentswithinasingle
object trajectory. Finally, we leverage masked attention for dense video object
captioning, which has not been explored before.
2.3 Contrastive Loss for Object Queries
Many recent approaches have used a contrastive loss to help in video instance
segmentation. OWVISFormer [39] uses a contrastive loss in the open-world set-
ting: it ensures that assigned foreground objects are similar to each other while
beingdifferentfromthebackgroundobjects.IDOL[50]worksintheclosed-world
setting, and uses an interframe contrastive loss to ensure object queries belong-
ing to the same object across frames are similar, and object queries of different
instances across frames differ. In contrast, in this work, for both the closed- and
open-worldsetting,weuseacontrastivelosstoensurethatnotwoobjectqueries
in the foreground are similar to each other, even in the same frame.
2.4 Generalized Video Understanding
Recently,therehasbeenprogressinunifyingdifferentvideorelatedtasks.Tube-
Former [23], Unicorn [53], and CAROQ [13] unify different video segmentation
tasks in the closed world. DVOC-DS [58] unifies the tasks of detecting closed-
world objects in videos and captioning those closed-world objects. In this work,
we explore the task of detecting or segmenting both closed- and open-world
objects in videos, and captioning these objects.
Video understanding starts from a strong generalized image understanding.
Several works generalize multiple image related tasks. Some methods [8,9,21]
combinedifferentimagesegmentationmethods,andprovideabaselineformany6 A. Choudhuri et al.
different video understanding tasks [7,13,18,50]. X-Decoder [59] unifies differ-
ent image segmentation tasks along with the task of referring image segmenta-
tion. SAM [24] introduces a vision foundation model, that primarily performs
prompt-basedopen-worldimagesegmentation,andcanbeusedformanydown-
stream tasks. Different from these works, we develop a generalized method for
videos that tackles segmentation and object-centric captioning for both open-
and closed-world objects.
2.5 Closed-World Video Instance Segmentation
Closed-world video instance segmentation involves simultaneously segmenting
and tracking objects from a fixed category-set in a video. Some works [3–5,12,
16,32,36,42,52,54,55] rely on classical region-based proposals. Recent works
[7,13,18,33,46,49,50] rely on query-based proposals and perform significantly
better at discovering closed-world objects. Differently, in this work, we explore
query-based proposals for both the closed- and open-world setting.
3 Approach
Givenavideo,ourgoalistojointlydetect,segmentandcaptionobjectinstances
present in the video. Importantly, note that object instance categories may not
be part of the training set (e.g., the parachutes shown in Fig. 3 (top row)),
placing our goal in an open-world setting. To achieve this goal, a given video is
first broken into short clips, each consisting of T frames. Each clip is processed
using our approach OW-VISCap. We discuss merging of the results of each clip
in Sec. 4.
We provide an overview of OW-VISCap to process each clip in Sec. 3.1. We
then discuss our contributions: (a) introduction of open-world object queries in
Sec. 3.2, (b) use of masked attention for object-centric captioning in Sec. 3.3,
and (c) use of inter-query contrastive loss to ensure that the object qeries are
different from each other in Sec. 3.4. In Sec. 3.5, we discuss the final training
objective.
3.1 Overview
Fig. 2 (left) provides an overview of our approach OW-VISCap. To deal with
the open-world setting, i.e., to ensure that we can detect, segment and caption
neverbeforeseenobjectinstances,weintroduceopen-worldobjectqueries,q ∈
ow
RNobj,ow×C. These open-world object queries are in addition to closed-world ob-
jectqueriesq
cw
∈RNobj,cw×C,commonlyusedinpriorwork[7,8,13,18,50].Here,
N andN arethetotalnumberofopen-worldandclosed-worldqueries,
obj,ow obj,cw
and C refers to the channel dimension for each object query. The open-world
object queries are created by encoding a grid of points, which act as prompts,
throughapromptencoder(showninpurpleinFig.2(left)).Note,thatbyusing
open-world object queries, we aim to discover new open-world objects without
needing additional prompts from the user, ground-truth or a different network.
This allows us to combine the prompt-based and prompt-less methods.OW-VISCap 7
Both open- and closed-world object queries are processed by our specifically
designed captioning head which yields an object-centric caption, a classification
head which yields a category label, and a detection head which yields either a
segmentation mask or a bounding-box.
Wecomputebothsetsofobjectqueries(open-andclosed-worldrespectively)
from initial embeddings e
ow
∈ RNobj,ow×C and e
cw
∈ RNobj,cw×C. Concretely, to
generatemeaningfulopen-andclosed-worldobjectqueriesq andq ,theseini-
ow cw
tialembeddingsarecontinuouslymodulatedintheobjecttransformer(shownin
green) by combining them with multi-level image features via masked attention
following [7,8]. The closed-world embeddings e are trainable. We discuss the
cw
generationofopen-worldembeddingse ,thecomputationofopen-worldobject
ow
queries q from open-world embeddings e and their training in Sec. 3.2.
ow ow
Fig 2 (right) illustrates our captioning head. It consists of an object-to-
text transformer (shown in blue) and a frozen LLM (shown in gray). We use
q =[q ,q ] to denote all the object queries obtained from the object trans-
obj ow cw
former. The ith object query qi ∈ R1×C is concatenated with learnt text em-
obj
beddings e
text
∈RNtext×C, where N
text
is the total number of text queries. The
concatenated object query and text embeddings are continuously modulated in
the object-to-text transformer by combining them with image features so as to
generate meaningful text queries qi ∈ R(Ntext+1)×C for the ith object. Note,
text
qi isusedbythefrozenLLMtogenerateobject-centriccaptions.Thesegmen-
text
tation mask for the ith object generated in the detection head is used to mask
theattentionintheobject-to-texttransformer,asdescribedinSec.3.3.Wefind
this design to enable the LLM to generate more object-centric captions.
Weintroduceaninter-querycontrastivelosstoensurethattheobjectqueries
areencouragedtodifferfromeachother.WeprovidedetailsinSec.3.4.Forclosed
world objects, this loss helps in removing highly overlapping false positives. For
open-world objects, it helps in the discovery of new objects.
Finally, we provide the full training objective in Sec. 3.5.
3.2 Open-World Object Queries
To help discover new objects, we introduce open-world object queries q in ad-
ow
ditiontothecommonlyusedclosedworldobjectqueries.Ouropen-worldobject
queries are generated from open-world embeddings e , which are continuously
ow
modulated in the object transformer by combining them with image features
via masked attention, following [7,8]. In the following, we discuss how we com-
pute open-world embeddings e via a prompt encoder, and how we design the
ow
training loss so as to encourage discovery of new objects.
Promptencoder.Theopen-worldembeddingse aregeneratedintheprompt
ow
encoder. An illustration is provided in Fig. 2 (left). We simply encode a grid of
equallyspacedpointsalongtheheightandwidthoftheframesoftheclip,using
the prompt encoder employed in SAM [24]. The use of equally spaced points
encourages the open-world object queries to focus on different regions of the
video frames, encouraging object discovery throughout the frames. This also
encourages the open-world object queries to be diverse from one another.8 A. Choudhuri et al.
Open-worldloss.Toencouragediscoveryofnewobjects,weintroduceanopen-
world loss L . It differs in one key-aspect from a classic closed-world loss [7]:
ow
wedon’tpenalizefalsepositivepredictions.Weelaborateondetailsandefficacy
of this design next.
We first match the ground truth objects with the open-world predictions by
minimizing a matching cost using the Hungarian algorithm [34]. The optimal
matching is then used to calculate the final open-world loss.
Concretely, consider a set of ground truth objects, padded with a no-object
category ∅ to equal the number of open-world predictions N . Let us use
obj,ow
σˆ to represent the optimal matching between the ground truth objects and
ow
the open-world predictions.
The open-world loss is given by
(cid:88)
L
ow
= 1 ci̸=∅[L cls(i,σˆ oi w)+L det(i,σˆ oi w)], (1)
i
where i is the object index. L (i,σˆi ) is the binary classification loss between
cls ow
the ground truth object with index i and a prediction with index σˆi . This
ow
loss helps in classifying a prediction as a foreground object or as background.
L (i,σˆi ) is the detection loss (mask loss or bounding box loss) between the
det ow
ground truth object with index i and a prediction with index σˆi . ci is the
ow
class prediction, which is foreground (ci ̸= ∅) or background ci = ∅ for the
open-world setting.
Differently, in the classic closed-world setting, the false positive predictions
that are matched with the ground truth no-object category are also penalized.
Concretely,thereisanadditionalcomponent(cid:80) i1 ci=∅[L cls(i,σˆ ci w)]intheclosed-
world loss, where σˆ refers to the optimal matching between the ground truth
cw
objects and the closed-world predictions and L (i,σˆi ) is the binary classifi-
cls cw
cation loss between the ground truth object with index i and a prediction with
index σˆi . This differs from the aforementioned open-world setting, where the
cw
false positives are important as they act as newly discovered objects.
3.3 Captioning Head
To obtain an object-centric caption, object queries q = [q ,q ], concaten-
obj ow cw
tatingtheopen-worldobjectqueriesdiscussedintheprevioussectionandclassic
closed-world object queries, are processed via our captioning head. Fig 2 (right)
illustrates the captioning head. It consists of an object-to-text transformer and
a frozen LLM.
To compute a caption, the ith object query qi is concatenated with learnt
obj
text embeddings e . The concatenated result is continuously modulated in
text
the object-to-text transformer by combining it with image features to generate
meaningful text queries qi for the ith object. Subsequently, qi is used by
text text
the frozen LLM to generate object-centric captions. The segmentation mask for
the ith object generated in the detection head is used to mask the attention in
theobject-to-texttransformer.WefindthistoenabletheLLMtogeneratemore
object-centric captions. We provide details next.OW-VISCap 9
Maskedattentionforobject-centriccaptioning.Maskedattentioninvolves
attending within the foreground region of the predicted mask for each object
query.Concretely,foreachobjecti,werestrictthecrossattentionintheobject-
to-text transformer by using the segmentation mask of the object generated by
the detection head to generate object-centric text queries qi . Intuitively, this
text
enables the model to focus on local object-centric features, which are sufficient
to update the text queries. Importantly, note that context information from the
video frames can be gathered through the self-attention layers. The proposed
design hence doesn’t take away any information. It rather provides the same
information in clearly separated layers.
Formally,fortheithobjectwecomputethequeryfeaturesXcap,i ∈R(Ntext+1)×C
l
obtained from the lth object-to-text transformer layer via
Xcap,i =softmax(Mi+QiKT)V +Xcap,i. (2)
l l l l l−1
Here, Mi ∈ {0,−∞}1×HWT is the attention mask in the object-to-text trans-
former such that at feature location (x,y), Mi(x,y) = 0, if Mi(x,y) = 1 and
Mi(x,y) = −∞, if Mi(x,y) = 0. Mi is the binary mask obtained from the
detectionhead.Moreover,K ,V ∈RHWT×C arethelinearlytransformedimage
l l
features. To initialize, we let Xcap,i = [qi ,e ], where qi ∈ R1×C is the ith
0 obj text obj
object query obtained from the object transformer and e
text
∈RNtext×C are the
learnt text embeddings shown in Fig. 2 and introduced in Sec. 3.1. C is the
channel dimension, H,W are the spatial dimensions of the image features and
T is the clip-length. N is the total number of text queries.
text
We’ll show in Sec. 4.3 that the masked attention helps in generating more
object-centric captions.
3.4 Inter-Query Contrastive Loss
We introduce an inter-query contrastive loss L to ensure that the object
cont
queriesaredifferentfromeachother,asdescribednext.Forclosed-worldobjects,
this loss helps in removing highly overlapping false positives. For open-world
objects, it helps in the discovery of new objects. Formally,
(cid:88)
L =− L(qi ,qj ), (3)
cont obj obj
i,j
where qi and qj are the ith and the jth objects and i̸=j. L refers to the L1
obj obj
distance.Viathisloss,wemaximize theL1distancebetweentheobjectqueries,
i.e., we encourage that the object queries differ from each other.
3.5 Training
Our total training loss is
L =L +L +L +L .
total ow cont cap cw
Here, L is the open-world loss as discussed in Sec. 3.2 and L is the inter-
ow cont
query contrastive loss discussed in Sec. 3.4. L is the standard captioning
cap10 A. Choudhuri et al.
Table 1: Open-world tracking accuracy (OWTA) on the BURST validation and test
sets for all, common (comm.) and uncommon (unc.) categories of objects. Onl. refers
to online frame-by-frame processing. The best scores are highlighted in bold font, and
the second-best scores are underlined.
Method Validation (OWTA) Test (OWTA)
all comm. unc. all comm. unc.
OWTB [28] 55.8 59.8 38.8 56.0 59.9 38.3
Mask2Former [8]+STCN [11] 64.6 71.0 25.0 57.5 62.9 23.9
Mask2Former [8]+DEVA [10] (onl.) 69.5 74.6 42.3 70.1 75.0 44.1
EntitySeg [37]+DEVA [10] (onl.) 68.8 72.7 49.6 69.5 72.9 53.0
OW-VISCap (ours) + DEVA [10] (onl.) 69.0 73.5 55.2 69.2 72.4 57.2
Table 2: DensevideoobjectcaptioningresultsontheVidSTG[57]dataset.Off.indi-
cates offline methods and onl. refers to online methods.
Method Mode CHOTA DetA AssA CapA AP
M
DVOC-DS (joint training) [58] off. 51.6 65.5 56.9 36.8 69.3
DVOC-DS (disjoint training) [58] off. 28.0 45.9 48.0 10.0 39.8
OW-VISCap (ours) onl. 53.0 60.1 54.0 43.9 62.6
loss introduced by DVOC-DS [58]. L is the closed-world loss following prior
cw
work [8]. In order to compute L , the ground truth objects are first matched
cw
withtheclosed-worldobjects;theoptimalmatchingisusedtocomputethefinal
closed-worldlossL .Thisclosed-worldlossinvolvespenalizingthefalsepositive
cw
predictions, which is different from L . Note that in our setting, the ground
ow
truth objects are matched twice, once with the open-world object queries to
compute L , and once with the closed-world object queries to compute L .
ow cw
4 Experiments
We evaluate our proposed approach on the diverse tasks of open-world video
instance segmentation (OW-VIS), dense video object captioning (Dense VOC),
and closed-world video instance segmentation (VIS). Note that there is no ded-
icated dataset for the task of open-world video instance segmentation and cap-
tioning.Henceweusethethreeaforementionedtasksandevaluatethethreedif-
ferent aspects of our approach: open-world capability, video object captioning,
and video instance segmentation. In the following subsections, we first discuss
the datasets and evaluation metrics used in our evaluation in Sec. 4.1. We then
compare our performances to baselines in Sec. 4.2. We demonstrate how each
of our contributions results in better performance through an ablation study in
Sec. 4.3. Finally we show some qualitative results in Sec. 4.4.
4.1 Datasets and Evaluation Metrics
We evaluate our approach on the OW-VIS, Dense VOC and VIS tasks. For
OW-VIS, we use the challenging BURST dataset [2]. For the Dense VOC task,OW-VISCap 11
Table 3: Results on the OVIS [36] validation data. All methods use the ResNet-50
backbone.Thebestperformingmethodsarehighlightedinboldfont.Thesecondbest
methods are underlined.
Method AP AP AP AR AR
50 75 1 10
MaskTrack [54] 10.8 25.3 8.5 7.9 14.9
DeVIS [6] 23.7 47.6 20.8 12.0 28.9
MinVIS [18] 25.0 45.5 24.0 13.9 29.7
VMT [22] 16.9 36.4 13.7 10.4 22.7
InstanceFormer [25] 20.0 40.7 18.1 12.0 27.1
VITA [15] 19.6 41.2 17.4 11.7 26.0
CAROQ [13] 25.8 47.9 25.4 14.2 33.9
OW-VISCap (w/o L ) 23.2 45.2 21.7 13.5 30.1
cont
OW-VISCap (ours) 25.4 48.8 22.8 14.3 32.8
Table 4: Ablation on the Table 5: Ablation on the VidSTG [57] data.
BURST [2] validation data. ‘w/o ‘w/o m.a.’ refers to without masked attention.
q ’ refers to without open-world ‘bb. cap.’ and ‘en. bb. cap.’ refers to bounding
ow
object queries; ‘w/o L ’ refers to boxcaptioningandenlargedboundingboxcap-
cont
without contrastive loss. tioning.
Method OWTA Method CHOTA DetA AssA CapA
all comm. unc. Ours 51.0 56.1 54.0 43.9
Ours 55.5 58.2 43.8 w/o m.a. 39.5 56.1 54.0 20.3
w/o q 54.3 57.9 41.2 bb. cap. 48.9 56.1 54.0 38.6
ow
w/o L 53.5 56.1 41.6 en. bb. cap. 49.4 56.1 54.0 40.4
cont
we use the VidSTG dataset [57]. Note that VidSTG [57] has bounding box
andtrackingidentityforallobjects,butcaptionsarenotexhaustivelyprovided.
However,DVOC-DS[58]usesVidSTG[57]fortheDenseVOCtaskbyremoving
the captioning loss for missing captions during training and not evaluating the
missingcaptionsduringevaluation.Wefollowasimilarsetting.FortheVIStask,
we evaluate on the OVIS dataset [36].
For OW-VIS, we use the standard evaluation metrics of open-world tracking
accuracy (OWTA) [2] for all, common and uncommon categories. For Dense
VOC,weusethecaptionedhigherordertrackingaccuracy(CHOTA)[58],which
depends on the detection accuracy (DetA), association accuracy (AssA), and
captioning accuracy (CapA). We also report the frame-based METEOR score
(AP ). For VIS, we use the standard evaluation metrics of average precision
M
(AP, AP , AP ) and average recall (AR , AR ) [54].
50 75 1 10
4.2 Main Results
Open-world video instance segmentation (BURST). Tab. 1 shows our
results for the OW-VIS task on the BURST dataset [2]. We report the open-
world tracking accuracy for all, common (comm.) and uncommon (unc.) cat-
egories. For the uncommon classes, we achieve the state-of-the-art, improving
upon the next best method (Mask2Former [8]+DEVA [10]) by ∼ 6 points on12 A. Choudhuri et al.
Fig.3: Example from the BURST validation data. The masks are superimposed on
the objects. The top-row shows examples of parachutes in the air and people on the
grass. The parachutes belong to the uncommon object category, i.e., parachutes were
never seen during training. Our approach is able to detect and retain the identity of
theblueandthegreenparachutesasthegreencrossestheblueparachute.Thebottom
rowshowsapersonunboxingaleafblower.Thecartonoftheleafblower(graymask),
the leaf blower (maroon mask) and the plastic wrapper (pink mask) are never seen
during training. We are able to consistently detect and track them along with the
person (common object category during training).
the BURST [2] validation data and by ∼4 points on the BURST test data. For
the common categories, our method ranks 2nd in the BURST validation data.
We use a SwinL [29] backbone, a clip-length of T = 1, and DEVA [10] for the
temporal association of objects.
Dense video object captioning (VidSTG). Tab. 2 shows our results on
the Dense VOC task. We outperform DVOS-DS [58] on the captioning accu-
racy (CapA), demonstrating that our captioning head with masked attention
(Sec. 3.3) is effective in generating object-centric captions. We improve upon
DS-VOC on the overall CHOTA metric, even though we slightly underperform
on DetA and AssA. Note that DVOS-DS is an offline method: the entire object
trajectories are used for generating the captions. Hence DVOS-DS cannot pro-
cessvideoswithmorethan200frames.Thisisincontrasttoouronlinemethod,
where we sequentially process short video clips of length T = 2. This enables
to process very long videos. DVOS-DS uses a ViT [14] backbone, whereas we
use SwinL [29], which leads to a difference in DetA scores. We use a clip-length
T = 2 for this experiment. Note that T = 2 enables the object queries to be
spatio-temporally rich. This helps in generating better object-centric captions.
For tracking, we use CAROQ [13] to propagate object queries across frames.
Video instance segmentation (OVIS). Tab. 3 shows our results for the
closed-world video instance segmentation task on the OVIS [36] dataset. In the
closed-worldsetting,wedisabletheopen-worldobjectqueries.Wenoticethatthe
contrastive loss L (discussed in Sec. 3.4) improves the closed-world results.
cont
Weuseaclip-lengthofT =2inthissettingandCAROQ[13]tocombineresults
from video clips.OW-VISCap 13
there is a screen behind an adult in
black clothes.
there is a brown chair behind an
adult in black clothes.
an adult in black speaks to another
adult.
there is a small screen on the table.
Fig.4: An example from the VidSTG data. Our approach is able to detect and track
objectsinthesceneconsistentlyandtogeneratemeaningfulobject-centriccaptionsfor
each of the detected objects.
4.3 Ablation Studies
Open-world object queries. Tab. 4 (first and second rows) shows how the
open-world object queries q , described in Sec. 3.2, help in discovering new
ow
objects. In Tab. 4, ‘w/o q ’ refers to the setting without open-world object
ow
queries, i.e., we only use the closed-world object queries to find the common
(comm.) and uncommon (unc.) objects. We observe that the performance drops
by 2.6 for uncommon categories and by 0.3 points for common categories in
‘w/o q ’ as compared to ‘Ours’, where both the open- and closed-world object
ow
queries are used.
Masked attention. Tab. 5 shows that masked attention in the object-to-text
transformer of the captioning head, described in Sec. 3.3, helps in object-centric
captioning using the VidSTG [57] data.
The second row ‘w/o m.a.’ of Tab. 5 refers to the setting without masked
attention,i.e.,theentireimage-featureisusedtocalculatethecross-attentionin
the object-to-text-transformer. The object-centric context is only accumulated
by concatenating the ith object query with the learnt text embeddings, as dis-
cussed in Sec. 3.3 and shown in Fig. 2. We observe that the captioning accuracy
CapA drops by 23 points, indicating that concatenating the object query with
the text embeddings is not sufficient for an object-centric focus.
The third row in Tab. 5, ‘bb. cap.’ (bounding box captioning), pursues the
opposite setting. Here, the images are cropped based on the object bounding
box predictions in the detection head. The cropped images are directly used for
captioning, ensuring that both the self and cross attention blocks in the object-
to-text transformer operate on object-centric features. Note, that we don’t use
masked attention in this setting. We see a drop in CapA of 5 points in this
setting.Althoughthecroppinghelpsinretainingtheobject-centricinformation,
the overall context from the entire image is missing.
The fourth row in Tab. 5, ‘en. bb. cap.’ (enlarged bounding box captioning),
showsasimilarsettingasthethirdrow,buttheboundingboxesarefirstenlarged
by10%toprovidemoreoverallimagecontext.Theenlargedboundingboxesare
then used to crop the images for captioning. We observe a drop in CapA of 314 A. Choudhuri et al.
points in this setting, indicating that enlarging the bounding boxes helps but is
not sufficient to provide overall context.
The first row in Tab. 5, ‘Ours’ (our approach), where we retain the overall
context using self attention in the object-to-text transformer and focus on the
object-centric features using masked cross attention, performs the best among
these settings.
Contrastive loss. Tab. 4 (first and last rows) shows how the contrastive loss
L , described in Sec. 3.4, helps in detecting both the common (comm.) and
cont
uncommon (unc.) categories of objects. The performance drops by ∼ 2 points
for both the common and uncommon categories for the setting ‘w/o L ’, i.e.,
cont
when the contrastive loss is not used. The contrastive loss helps in removing
highly overlapping false positives in the closed-world setting and in discovering
newobjectsintheopen-worldsetting.Tab.3furthershowsthatthecontrastive
loss helps in detecting objects in the closed-world setting.
4.4 Qualitative Results
We provide qualitative results generated by OW-VISCap in this section. Fig. 1
shows an example from the BURST [2] dataset. OW-VISCap is able to simulta-
neouslydetect,trackandcaptionobjectsinthegivenvideoframes.Theobjects
belong to both the open- and closed-world. Note that the BURST [2] dataset
doesn’t have object-centric captions available for training or evaluation, hence
our captioning head was not trained on BURST [2]. We train the captioning
head on the Dense VOC task (whose results are shown in Tab. 2). We find this
captioning-head to be effective in generating meaningful object-centric captions
even for objects never seen during training. Fig. 3 shows two examples from
the BURST validation data. We are able to consistently detect, segment, and
track the previously seen and unseen objects. Fig. 4 shows an example from the
VidSTG[57]data.Ourapproachisabletodetectandtrackobjectsinthescene
consistently, and to generate meaningful object-centric captions for each of the
detected objects.
5 Conclusion
In this work, we introduce OW-VISCap to jointly detect, segment, track, and
caption previously seen or unseen objects in videos. We introduce open-world
object queries to encourage discovery of previously unseen objects without the
need of additional user-inputs. Instead of assigning a fixed label to detected ob-
jects, we generate rich object-centric captions for each object by using masked
attention in an object-to-text transformer. Further, we introduce an inter-query
contrastive loss to ensure that the object queries differ from one another. Our
approach mostly matches or surpasses the state-of-the-art on the diverse tasks
of open-world video instance segmentation on the BURST dataset, dense video
object captioning on the VidSTG dataset, and closed-world video instance seg-
mentationontheOVISdataset.Throughanablationstudy,wedemonstratethe
effectiveness of each of our proposed components.OW-VISCap 15
Acknowledgements:ThisworkissupportedinpartybyAgricultureandFood
Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.
1024178fromtheUSDANationalInstituteofFoodandAgriculture:NSF/USDA
National AI Institute: AIFARMS. We also thank the Illinois Center for Digital
Agriculture for seed funding for this project. Work is also supported in part by
NSF under Grants 2008387, 2045586, 2106825, MRI 1725729.
References
1. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., Schmid, C.: Vivit: A
video vision transformer. In: ICCV (2021)
2. Athar,A.,Luiten,J.,Voigtlaender,P.,Khurana,T.,Dave,A.,Leibe,B.,Ramanan,
D.:Burst:Abenchmarkforunifyingobjectrecognition,segmentationandtracking
in video. In: WACV (2023)
3. Athar, A., Mahadevan, S., Osep, A., Leal-Taixé, L., Leibe, B.: Stem-seg: Spatio-
temporal embeddings for instance segmentation in videos. In: ECCV (2020)
4. Bertasius,G.,Torresani,L.:Classifying,segmenting,andtrackingobjectinstances
in video with mask propagation. In: ICCV (2020)
5. Braso,G.,Leal-Taixe,L.:Learninganeuralsolverformultipleobjecttracking.In:
CVPR (June 2020)
6. Caelles, A., Meinhardt, T., Brasó, G., Leal-Taixé, L.: Devis: Making de-
formable transformers work for video instance segmentation. arXiv preprint
arXiv:2207.11103 (2022)
7. Cheng, B., Choudhuri, A., Misra, I., Kirillov, A., Girdhar, R., Schwing, A.G.:
Mask2former for video instance segmentation. arXiv preprint arXiv:2112.10764
(2021)
8. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention
mask transformer for universal image segmentation. In: CVPR (2022)
9. Cheng, B., Schwing, A.G., Kirillov, A.: Per-pixel classification is not all you need
for semantic segmentation. In: NeurIPS (2021)
10. Cheng,H.K.,Oh,S.W.,Price,B.,Schwing,A.,Lee,J.Y.:Trackinganythingwith
decoupled video segmentation. In: ICCV (2023)
11. Cheng, H.K., Tai, Y.W., Tang, C.K.: Rethinking space-time networks with im-
provedmemorycoverageforefficientvideoobjectsegmentation.In:NeurIPS(2021)
12. Choudhuri, A., Chowdhary, G., Schwing, A.G.: Assignment-space-based multi-
object tracking and segmentation. In: ICCV (2021)
13. Choudhuri, A., Chowdhary, G., Schwing, A.G.: Context-aware relative object
queries to unify video instance and panoptic segmentation. In: CVPR (2023)
14. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020)
15. Heo,M.,Hwang,S.,Oh,S.W.,Lee,J.Y.,Kim,S.J.:Vita:Videoinstancesegmen-
tation via object token association. arXiv preprint arXiv:2206.04403 (2022)
16. Hornakova, A., Henschel, R., Rosenhahn, B., Swoboda, P.: Lifted disjoint paths
with application in multiple object tracking. In: ICML (2020)
17. Hu, Y.T., Huang, J.B., Schwing, A.G.: Videomatch: Matching based video object
segmentation. In: ECCV (2018)16 A. Choudhuri et al.
18. Huang,D.A.,Yu,Z.,Anandkumar,A.:Minvis:Aminimalvideoinstancesegmen-
tation framework without video-based training. In: NeurIPS (2022)
19. Huang, X., Xu, J., Tai, Y.W., Tang, C.K.: Fast video object segmentation with
temporal aggregation network and dynamic template matching. In: CVPR (2020)
20. Huang, X., Xu, J., Tai, Y.W., Tang, C.K.: Fast video object segmentation with
temporal aggregation network and dynamic template matching. In: CVPR (2020)
21. Jain,J.,Li,J.,Chiu,M.T.,Hassani,A.,Orlov,N.,Shi,H.:Oneformer:Onetrans-
former to rule universal image segmentation. In: CVPR (2023)
22. Ke,L.,Ding,H.,Danelljan,M.,Tai,Y.W.,Tang,C.K.,Yu,F.:Videomasktrans-
finer for high-quality video instance segmentation. In: ECCV (2022)
23. Kim, D., Xie, J., Wang, H., Qiao, S., Yu, Q., Kim, H.S., Adam, H., Kweon, I.S.,
Chen, L.C.: Tubeformer-deeplab: Video mask transformer. In: ICCV (2022)
24. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023)
25. Koner,R.,Hannan,T.,Shit,S.,Sharifzadeh,S.,Schubert,M.,Seidl,T.,Tresp,V.:
Instanceformer:Anonlinevideoinstancesegmentationframework.arXivpreprint
arXiv:2208.10547 (2022)
26. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023)
27. Liu, Y., Zulfikar, I.E., Luiten, J., Dave, A., Ramanan, D., Leibe, B., Ošep, A.,
Leal-Taixé, L.: Opening up open world tracking. In: CVPR (2022)
28. Liu, Y., Zulfikar, I.E., Luiten, J., Dave, A., Ramanan, D., Leibe, B., Ošep, A.,
Leal-Taixé,L.:Openingupopenworldtracking.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.19045–19055(2022)
29. Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,Guo,B.:Swintrans-
former: Hierarchical vision transformer using shifted windows. In: ICCV (2021)
30. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017)
31. Lüddecke, T., Ecker, A.: Image segmentation using text and image prompts. In:
CVPR (2022)
32. Luiten, J., Fischer, T., Leibe, B.: Track to reconstruct and reconstruct to track.
RAL (2020)
33. Meinhardt, T., Kirillov, A., Leal-Taixe, L., Feichtenhofer, C.: Trackformer: Multi-
object tracking with transformers. In: CVPR (2022)
34. Munkres,J.:Algorithmsfortheassignmentandtransportationproblems.J-SIAM
(1957)
35. Oh,S.W.,Lee,J.Y.,Xu,N.,Kim,S.J.:Videoobjectsegmentationusingspace-time
memory networks. In: ICCV (2019)
36. Qi, J., Gao, Y., Hu, Y., Wang, X., Liu, X., Bai, X., Belongie, S., Yuille,
A., Torr, P.H., Bai, S.: Occluded video instance segmentation. arXiv preprint
arXiv:2102.01558 (2021)
37. Qi,L.,Kuen,J.,Wang,Y.,Gu,J.,Zhao,H.,Torr,P.,Lin,Z.,Jia,J.:Openworld
entity segmentation. PAMI (2022)
38. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021)
39. Thawakar, O., Narayan, S., Cholakkal, H., Anwer, R.M., Khan, S., Laaksonen,
J., Shah, M., Khan, F.S.: Video instance segmentation in an open-world. arXiv
preprint arXiv:2304.01200 (2023)OW-VISCap 17
40. Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos:
Fast end-to-end embedding learning for video object segmentation. In: CVPR
(2019)
41. Voigtlaender,P.,Changpinyo,S.,Pont-Tuset,J.,Soricut,R.,Ferrari,V.:Connect-
ing vision and language with video localized narratives. In: CVPR (2023)
42. Voigtlaender,P.,Krause,M.,O˘sep,A.,Luiten,J.,Sekar,B.B.G.,Geiger,A.,Leibe,
B.: MOTS: Multi-object tracking and segmentation. In: CVPR (2019)
43. Wang,Q.,Zhang,L.,Bertinetto,L.,Hu,W.,Torr,P.H.:Fastonlineobjecttracking
and segmentation: A unifying approach. In: CVPR (2019)
44. Wang, W., Feiszli, M., Wang, H., Tran, D.: Unidentified video objects: A bench-
mark for dense, open-world segmentation. In: ICCV (2021)
45. Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end
video instance segmentation with transformers. In: CVPR (2021)
46. Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end
video instance segmentation with transformers. In: CVPR (2021)
47. Wu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., Wang, L.: Grit: A
generative region-to-text transformer for object understanding. arXiv preprint
arXiv:2212.00280 (2022)
48. Wu, J., Jiang, Y., Sun, P., Yuan, Z., Luo, P.: Language as queries for referring
video object segmentation. In: CVPR (2022)
49. Wu, J., Jiang, Y., Zhang, W., Bai, X., Bai, S.: Seqformer: a frustratingly simple
model for video instance segmentation. arXiv preprint arXiv:2112.08275 (2021)
50. Wu, J., Liu, Q., Jiang, Y., Bai, S., Yuille, A., Bai, X.: In defense of online models
for video instance segmentation. In: ECCV (2022)
51. Xu, N., Yang, L., Fan, Y., Yang, J., Yue, D., Liang, Y., Price, B., Cohen, S.,
Huang, T.: Youtube-vos: Sequence-to-sequence video object segmentation. In:
ECCV (2018)
52. Xu, Z., Zhang, W., Tan, X., Yang, W., Huang, H., Wen, S., Ding, E., Huang, L.:
Segment as points for efficient online multi-object tracking and segmentation. In:
ECCV (2020)
53. Yan, B., Jiang, Y., Sun, P., Wang, D., Yuan, Z., Luo, P., Lu, H.: Towards grand
unification of object tracking. In: ECCV (2022)
54. Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV (2019)
55. Yang, S., Fang, Y., Wang, X., Li, Y., Fang, C., Shan, Y., Feng, B., Liu, W.:
Crossover learning for fast online video instance segmentation. In: ICCV (2021)
56. Zhang, L., Lin, Z., Zhang, J., Lu, H., He, Y.: Fast video object segmentation via
dynamic targeting network. In: ICCV (2019)
57. Zhang, Z., Zhao, Z., Zhao, Y., Wang, Q., Liu, H., Gao, L.: Where does it exist:
Spatio-temporal video grounding for multi-form sentences. In: CVPR (2020)
58. Zhou, X., Arnab, A., Sun, C., Schmid, C.: Dense video object captioning from
disjoint supervision. arXiv preprint arXiv:2306.11729 (2023)
59. Zou, X., Dou, Z.Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang, J.,
Yuan, L., et al.: Generalized decoding for pixel, image, and language. In: CVPR
(2023)18 A. Choudhuri et al.
Supplementary Material — OW-VISCap:
Open-World Video Instance Segmentation and
Captioning
Fig.S1: Results on the BURST dataset. We successfully segment and track closed-
world objects (e.g., persons), and open-world objects (e.g., rackets), throughout the
video.
This is the supplementary material of the OW-VISCap paper. We develop OW-
VISCap, an approach for open-world video instance segmentation and caption-
ing.Wetestourapproachonthetaskofopen-worldvideoinstancesegmentation
(OW-VIS), dense video object captioning (Dense VOC) and closed-world video
instance segmentation (VIS) in Sec. 4.
In this supplementary material, we provide additional analysis (Sec. A) to
supportthecontributionswemadeinSec.3ofthemainpaper.Wethendiscuss
the implementation details (Sec. B) and limitations (Sec. C) of our approach.
Fig. S1 shows additional results on the BURST [2] dataset. We successfully
segmentandtracktheclosed-worldobjects:persons,andtheopen-worldobjects:
rackets,throughoutthevideo.Fig.S2showsresultsontheVidSTG[57]dataset.
Thedetectedobjectsaretrackedthroughoutthevideoandourmethodgenerates
meaningful captions for each object.
A Additional Analysis
In this section we provide additional analysis on the different components dis-
cussed in Sec. 3 of the main paper.
A.1 Open-World Embeddings as Object Proposals
We introduced open-world embeddings e in Sec. 3.2 of the main paper. These
ow
embeddings are modulated in the object transformer (Fig. 2) to generate open-
worldobjectqueries.Weobtaintheopen-worldembeddingsbyencodingagridofOW-VISCap 19
a child in blue speaks to an adult in the room.
an adult in black hugs a baby in the room.
Fig.S2: Results on the VidSTG dataset. Our approach detects, tracks and generates
meaningful object-centric captions for each object throughout the video.
equally spaced points across the feature dimensions through a prompt encoder.
This encourages object discovery throughout the video-frame. The open-world
embeddings act as initial abstract object proposals.
In Fig. S3, we show that the open-world embeddings are strong object pro-
posals,evenbeforetheyaremodulatedbytheobjecttransformerbybeingcom-
binedwithvideoframefeatures.Theperson,thespoon,theplate,andsomefood
on the plate are discovered by the open-world embeddings. Their segmentation
masksareobtainedbythedotproductbetweentheopen-worldembeddingsand
the video frame features. Further, we see a strong spatial correlation between
the grid of points and the segmentation masks generated by the corresponding
open-worldembeddings.Thissuggeststhatencodingagridofpointsthroughout
thefeaturedimensionsencouragesobjectdiscoverythroughoutthevideoframes.
A.2 Masked Attention for Object-Centric Captioning
In Tab. 5, we quantitatively show that masked-attention (Sec. 3.3) helps in
generatingaccurateobject-centriccaptionsforindividualobjects.Fig.S4shows
the high quality of the object-centric captions generated. The black caption (‘a
family sitting on a couch with a child’) is obtained when no mask is provided
in the object-to-text transformer. The entire image features are seen during the
cross attention operation in each layer of the object-to-text transformer. The
caption fails to capture the object-centric details.
The colored captions are generated with masked-attention for the individual
objects. The colored captions on the left clearly highlight the effectiveness of
masked attention to generate object-centric captions. For example, the three
persons(highlightedincyan,grayishblue,andgreen)havedistinctcaptions,each
one describing the individual identities of the corresponding person. The school
bag (light blue) is also described correctly. We want to note that sometimes the
method fails to generate meaningful object-centric captions for small objects
(captions on the right). We discuss this more in Sec. C.20 A. Choudhuri et al.
Open-world embeddings Open-world objects
.
Prompt Encoder
Image
Grid of points Features
Fig.S3:Open-worldembeddingsactasstrongobjectproposals.Further,astrongspa-
tialcorrelationexistsbetweenthegridofpointsandthesegmentationmasksgenerated
by the corresponding open-world embeddings. This is highlighted by the color of the
points in the grid, and the color of the segmentation masks.
a family sitting on a couch with a child.
back of the school bag.
the mother is sitting on the couch. a family sits on the couch.
a child is sitting on a couch. the the the the the the the
a man is sitting on a couch with wife and child.
Fig.S4: Maskedattentionhelpsgenerateobject-centriccaptions(coloredcaptionson
theleft).Providingnomaskduringcrossattentionfailstocaptureobjectcentricdetails
(black caption). However, masked-attention sometimes fails to generate meaningful
object-centric captions for small objects (colored captions on the right).
A.3 Contrastive Loss to Suppress Overlapping Predictions
In Tab. 3 and Tab. 4 of the main paper, we demonstrate the effectiveness of
using the contrastive loss L (discussed in Sec. 3.4) for object detection. This
cont
loss encourages that object queries differ from each other, among others by
suppressing highly overlapping predictions. We highlight this in Fig. S5. The
leftimageshowsaframefromtheOVIS[36]dataset.Thetop-rightandbottom-
rightimagesshowafewpredictionsfromournetworktrainedwithout(top)and
trained with (bottom) the contrastive loss. The repetitive predictions for the
top-right image are highlighted with red and cyan boxes. The contrastive loss
helps in removing these repetitions.OW-VISCap 21
(without contrastive loss)
(with contrastive loss)
Fig.S5: The repetitative predictions without contrastive loss (top-right) are high-
lighted with red and cyan boxes. Contrastive loss (bottom-right) helps in suppressing
these repetitions.
B Implementation Details
In this section, we provide the implementation details of OW-VISCap. We first
describe the architecture of different components discussed in Sec. 3. We then
discuss our choice of hyper-parameters for all experiments discussed in Sec. 4.
We also discuss the resources and the licenses of the code-bases and datasets
used in the paper.
B.1 Architecture
Prompt encoder. Our prompt encoder discussed in Sec. 3.2 is a lightweight
network,followingthearchitectureofthepromptencoderusedinSAM[24].We
initialize our prompt encoder from SAM [24] and fine-tune it on the BURST [2]
dataset.
Captioning head. Our captioning head discussed in Sec. 3.3 consists of an
object-to-texttransformerandafrozenlargelanguagemodel(LLM).Theobject-
to-text transformer decoder has 11 transformer layers. Each layer has a self
attention, masked cross attention (discussed in Sec. 3.3) operations, followed by
afeedforwardnetwork.Theobject-to-texttransformerandthetextembeddings
e are initialized from BLIP-2 [26]. We use a frozen OPT-2.7B model as the
text
LLM. We fine-tune the object-to-text transformer and the text embeddings on
the VidSTG [57] dataset.
B.2 Hyper-Parameters
We now discuss the hyper-parameters used in this work. For experiments on
the BURST [2] dataset, we encode a grid of 7×7 point across the width and
height of the image features to obtain the open-world embeddings e discussed
ow
in Sec. 3.2. Hence the total number of open-world object queries N is 49.
obj,ow
Wealsoexperimentedwithagridof4×4and10×10,butdidn’tseeasignificant
changeinperformance.ForexperimentsontheVidSTG[57]dataset,thenumber22 A. Choudhuri et al.
an adult in gray clothes carries a black bag
there is a black handbag behind an adult woman.
there is a silver train behind an adult in black clothes
an adult in gray clothes
an adult is holding a black dog
Fig.S6: A failure mode, where the object identities aren’t retained after prolonged
occlusion.Afteratraincrossesthescreenforaprolongedperiodoftime(∼30frames),
thepersoninitiallyidentifiedasblueisidentifiedasgreen,andthebagislateridentified
as light green instead of dark blue. The last caption for the bag is also erroneous.
oftextembeddingse is32.Inallexperiments,themaximumnumberofclosed
text
world objects (N ) in a given video for a ResNet-50 backbone is 100, and
obj,cw
for a Swin-L backbone is 200. We use a feature dimension C (Sec. 3.1) of 256 in
all models.
Wetrainedthemodelswithaninitiallearningrateof0.0001andADAMW[30]
optimizer with a weight decay of 0.05. We use a batch size of 8. The networks
were first initialized with weights from Mask2Former [8] trained on the COCO
image instance segmentation dataset. We then fine-tune the models on the re-
spective BURST [2], VidSTG [57], and OVIS [36] datasets for 10,000, 16,000
and 8,000 iterations respectively.
B.3 Resources
Weused8NVIDIAA100GPUstoruntheexperimentspresentedinthispaper.
Each experiment took roughly 10 GPU hours of training on the A100 GPUs for
the BURST experiments, 16 GPU hours for the VidSTG experiments, and 8
GPU hours for the VIS dataset experiments.
B.4 Licenses
Our code is built on Mask2Former [8] which is majorly licensed under the MIT
license, with some portions under the Apache-2.0 License. We also build on
SAM [24], which is released under the Apache 2.0 License and BLIP-2 [26]OW-VISCap 23
which is released under the MIT license. The OVIS [36] dataset is released un-
der the Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) License. The
VidSTG [57] and BURST [2] datasets are released under the MIT license.
C Limitations
Inthissection,weshowsomefailuremodesofourproposedapproachanddiscuss
limitations. Fig. S3 shows that our approach sometimes fails to detect some
open-world objects that a human may find to be of interest. For example, the
grinder on the left, the window at the top-right, etc., are not detected by the
network.ThecoloredcaptionsontherightsideofFig.S4showthatourapproach
sometimesfailstogeneratemeaningfulobject-centriccaptionsforsmallobjects.
For the purple object (cushion on a sofa), the caption (‘the the the ...’) is not
meaningfulsinceitfailstoformacompletesentenceorcapturetheidentityofthe
object it represents. For the red object (other cushions on a sofa), the caption
(‘a family sits on the couch’) is not object-centric since it fails to provide a
descriptionspecifictotheobject.Fig.S6furtherhighlightsafailuremode.After
a train crosses the scene for a prolonged period of time (∼ 30 frames), object
identities may be lost.
These issues can be addressed by stronger strategies for open-world object
discovery, stronger caption-generators, and integrating better object trackers,
which we leave for future work.