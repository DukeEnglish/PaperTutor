LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace
BinGao1 YanYang2 Ya-xiangYuan1
Abstract Inthispaper,weconsiderthefollowingbilevelproblem:
Bilevel optimization, with broad applications in
min φ(x):=f(x,y (x))
∗
m stra uc ch ti un re e.l Gea rr an di in eg n, t-h ba as sea dn mi en tt hr oic da ste hah vi eer ea mrc eh ri gc ea dl x s∈.R td .x
y ∗(x) argming(x,y),
(1)
∈
as a common approach to large-scale bilevel y ∈Rdy
problems. However, the computation of the wheretheupper-levelfunctionf andthelower-levelfunc-
hyper-gradient, which involves a Hessian in- tion g are defined on Rdx Rdy. φ is called the hyper-
versevectorproduct,confinestheefficiencyand objective, and the gradient× of φ(x) is referred to as the
is regarded as a bottleneck. To circumvent hyper-gradient(Pedregosa,2016;Grazzietal.,2020;Chen
the inverse, we construct a sequence of low- et al., 2023; Yang et al., 2023) if it exists. In contrast to
dimensionalapproximateKrylovsubspaceswith standard single-level optimization problems, bilevel opti-
the aid of the Lanczos process. As a result, the mization is inherently challenging due to its intertwined
constructedsubspaceisabletodynamicallyand structure. Specifically,theformulation(1)underscoresthe
incrementally approximate the Hessian inverse crucialroleofthelower-levelsolutiony (x)ineachupdate
∗
vectorproductwithlesseffortandthusleadstoa ofx.
favorable estimate of the hyper-gradient. More-
One of the focal points in recent bilevel methods has
over, we propose a provable subspace-based
shifted towards nonconvex upper-level problems coupled
framework for bilevel problems where one cen-
with strongly convex lower-level problems (Ghadimi &
tralstepistosolveasmall-sizetridiagonallinear
Wang, 2018; Ji et al., 2021; Chen et al., 2022; Dagre´ou
system. Tothebestofourknowledge,thisisthe
et al., 2022; Li et al., 2022; Hong et al., 2023). This con-
first time that subspace techniques are incorpo-
figuration ensures that y (x) is a single-valued function
rated into bilevel optimization. This successful ∗
trialnotonlyenjoys O(ϵ −1)convergenceratebut o thf ex h, yi p. ee r., -gy r∗ a( dx ie) nt= caa nr bg em coin my p∈uR td ey dg f( rx om,y t) h. eS imub ps le icq iu te fun ntl cy -,
alsodemonstratesefficiencyinasyntheticprob-
tiontheoremasfollows(Ghadimi&Wang,2018),
lemandtwodeeplearningtasks.
φ(x)= f(x,y (x)) 2 g(x,y (x))
∇ ∇x ∗ −∇xy ∗ (2)
1.Introduction
×
∇2 yyg(x,y ∗(x)) −1 ∇yf(x,y ∗(x)).
Bileveloptimization,inwhichupper-levelandlower-level Thegradientm(cid:2)ethodsbasedonth(cid:3)ehyper-gradient,x =
k+1
problemsarenestedwitheachother,mirrorsamultitudeof x λ φ(x ),areknownastheapproximateimplicitdif-
k k
− ∇
applications,e.g.,gametheory(Stackelberg,1952),hyper- ferentiation(AID)basedmethods(Jietal.,2021;Arbel&
parameter optimization (Franceschi et al., 2017; 2018), Mairal, 2022; Liu et al., 2023). Nevertheless, the com-
meta-learning (Bertinetto et al., 2018), neural architec- putation of the hyper-gradient (2) suffers from two pains:
ture search (Liu et al., 2018; Wang et al., 2022), adver- 1) solving the lower-level problem to obtain y (x); 2) as-
∗
sarial training (Wang et al., 2021), reinforcement learn- semblingtheHessianinversevectorproduct
ing(Chakrabortyetal.,2023;Hongetal.,2023).
1State Key Laboratory of Scientific and Engineering Com-
v ∗(x):= ∇2 yyg(x,y ∗(x)) −1 ∇yf(x,y ∗(x)), (3)
puting,AcademyofMathematicsandSystemsScience,Chinese orequivalentl(cid:2)y,solvingalarge(cid:3)linearsystemintermsofv,
Academy of Sciences, Beijing, China 2State Key Laboratory of
ScientificandEngineeringComputing,AcademyofMathematics 2 g(x,y (x))v = f(x,y (x)). (4)
andSystemsScience,ChineseAcademyofSciences,andUniver- ∇yy ∗ ∇y ∗
sityofChineseAcademyofSciences,Beijing,China.Correspon-
To this end, it is beneficial to adopt a few inner iterations
denceto:YanYang<yangyan@amss.ac.cn>.
to approximate y (x) and v (x) within each outer itera-
∗ ∗
tion (i.e., the update of x). Note that the approximation
1
4202
rpA
4
]CO.htam[
1v13330.4042:viXraLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
7×10 1
I=1 10 1
I=10
I=20
6×10 1 I=50 10 2 I=150
10 3
5×10 1
10 4 LancBiO
SubBiO
AmIGO-GD
10 5 AmIGO-CG
4×10 1 SOBA
0 1000 2000 3000 4000 5000 20 40 60 80 100 120 140 160
Outer Iteration Time [sec]
Figure2.Estimation error of the Hessian inverse vector product
Figure1.TestlossforthemethodstocBiO(Jietal.,2021)with inhyper-datacleaningtaskwithcorruptionrate0.5fordifferent
different inner iterations I to approximate the Hessian inverse methods:LancBiOandSubBiO(ours),AmIGO(Arbel&Mairal,
vectorproduct. 2022),andSOBA(Dagre´ouetal.,2022).
1.1.Contributions
accuracy of v (x) is crucial for the AID-based methods; Inthispaper,takingadvantageoftheKrylovsubspaceand
∗
see (Ji et al., 2022; Li et al., 2022). Specifically, Fig- the Lanczos process, we develop an innovative subspace-
ure 1 confirms that the more inner iterations, the higher based framework—LancBiO, which features an efficient
quality of the estimate of v , and the more enhanced de- and accurate approximation of the Hessian inverse vector
∗
scentoftheobjectivefunctionwithinthesamenumberof productv inthehyper-gradient—forbileveloptimization.
∗
outeriterations.. Themaincontributionsaresummarizedasfollows.
Approximation: Existingeffortsarededicatedtoapprox- Firstly, we build up a dynamic process for construct-
imating v in different fashions by regulating the number ing low-dimensional subspaces that are tailored from the
∗
of inner iterations, e.g., the Neumann series approxima- Krylov subspace for bilevel optimization. This process
tion (Ghadimi & Wang, 2018; Ji et al., 2021) for the in- effectively reduces the large-scale subproblem (4) to the
verse, gradient descent (Arbel & Mairal, 2022; Dagre´ou small-size tridiagonal linear system, which draws on the
et al., 2022) and conjugate gradient descent (Pedregosa, spirit of the Lanczos process. To the best of our knowl-
2016;Yangetal.,2023)forthelinearsystem. edge, this is the first time that the subspace technique is
leveragedinbileveloptimization.
Amortization: Moreover,therearestudiesaimedatamor-
tizing the cost of approximation through outer iterations. Moreover,theconstructedsubspacesenableustodynami-
These methods include using the inner estimate from the callyandincrementallyapproximatev acrossouteritera-
∗
previousouteriterationasawarmstartforthecurrentouter tions,therebyachievinganenhancedestimateofthehyper-
iteration (Ji et al., 2021; Arbel & Mairal, 2022; Dagre´ou gradient; Figure 2 illustrates that the proposed LancBiO
etal.,2022;Jietal.,2022;Lietal.,2022;Xiaoetal.,2023), reaches the best estimation error for v . Hence, we pro-
∗
oremployingarefinedstepsizecontrol(Hongetal.,2023). vide a new perspective for approximating the Hessian in-
verse vector product in bilevel optimization. Specifically,
Subspace techniques, widely adopted in nonlinear opti-
thenumberofHessian-vectorproductsaveragesat(1+ 1)
mization (Yuan, 2014), approximately solve large-scale m
per outer iteration with the subspace dimension m, which
problemsinlower-dimensionalsubspaces, whichnotonly
isfavorablycomparablewiththeexistingmethods.
reduce the computational cost significantly but also enjoy
favorable theoretical properties as in full space models. Finally, we offer analysis to circumvent the instability in
Takingintoaccounttheabovetwoprinciples, itisreason- the process of approximating subspaces, with the result
able to consider subspace techniques in bilevel optimiza- that LancBiO can profit from the benign properties of the
tion. Specifically,wecanefficientlyamortizetheconstruc- Krylov subspace. We prove that the proposed method
tion of low-dimensional subspaces and sequentially solve LancBiOisgloballyconvergentwiththeconvergencerate
linearsystems(4)inthesesubspacestoapproximatev ac- (ϵ 1).Inaddition,theefficiencyofLancBiOisvalidated
∗ −
O
curately. byasyntheticproblemandtwodeeplearningtasks.
2
ssol
tseT
mron
laudiseRLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
1.2.RelatedWork outeriterationasfollows,
Adetailedintroductiontobileveloptimizationmethodscan x =x λ φ(x ),
k+1 k k
− ∇
befoundinAppendixA.
wherethehyper-gradientisexactlycomputedby
Krylov subspace methods: Subspace techniques have
φ(x )= f(x ,y ) 2 g(x ,y )v
gainedsignificantrecognitionintherealmofnumericallin- ∇ k ∇x k k∗ −∇xy k k∗ k∗
earalgebra(Parlett,1998;Saad,2011;Golub&VanLoan,
with y := y (x ) and v := v (x ) defined in (3).
k∗ ∗ k k∗ ∗ k
2013) and nonlinear optimization (Yuan, 2014; Liu et al.,
In view of the computational intricacy of y and v , it is
k∗ k∗
2021). Specifically, numerous optimization methods uti-
commonly concerned with the following estimator for the
lized subspace techniques to improve efficiency, includ-
hyper-gradient
ing acceleration technique (Li et al., 2020), diagonal pre-
conditioning (Gao et al., 2023), and derivative-free opti- ∇φ(x k,y k,v k):= ∇xf(x k,y k) −∇2 xyg(x k,y k)v k, (5)
mization methods (Cartis & Roberts, 2023). Krylov sub-
wherey isanapproximationofy . DenotetheHessian
space(Krylov,1931),duetoitsspecialstructure, (cid:101) k k∗
A = 2 g(x ,y ) and b = f(x ,y ).
(A,b):=span b,Ab,A2b,...,AN 1b k ∇yy k k k ∇y k k
N −
K
v isthe(approximate)solutionofaquadraticoptimization
k
with the dimension N f(cid:8)or a matrix A and a v(cid:9)ector b,
problem
exhibits advantageous properties in convex quadratic op- 1
min v A v v b , (6)
timization (Nesterov et al., 2018), eigenvalue computa- ⊤ k ⊤ k
tion (Kuczyn´ski & Woz´niakowski, 1992), and regularized
v ∈Sk 2 −
n Ko rn yc loo vnv se ux bsq pu aa cd er hat ai sc bp er eo nbl wem ids el( yC ca or nm so idn er& edD inuc lah ri g, e2 -0 sc1 a8 l) e. w ish Aer −ke 1bS kk
.
i Ss ubth se eqf uu el nl tls yp ,a ic ne oR rdd ey
r
ta ond redth ue ceex tha ect coso mlu pt ui to an
-
tionalcost,itisnaturaltoask:
optimization such as trust region methods (Gould et al.,
1999),tracemaximizationproblems(Liuetal.,2013),and
Canweconstructalow-dimensionalsubspace such
k
cubic Newton methods (Cartis et al., 2011; Jiang et al., S
thatthesolutionof (6)satisfactorilyapproximates
2024). Lanczos process (Lanczos, 1950) is an orthogo- A−k1b k?
nalprojectionmethodontotheKrylovsubspace,whichre-
ducesadensesymmetricmatrixtoatridiagonalform. De- General subspace constructions introduced in the existing
tails of the Krylov subspace and the Lanczos process are subspace methods (Yuan, 2014; Liu et al., 2021) are not
summarizedinAppendixB. straightforwardandnotexploitedinthebilevelsetting,ren-
dering the exploration of appropriate subspaces challeng-
Approximation ofthe Hessianinverse vectorproduct :
ing. In the following subsections, we construct approxi-
It is cumbersome to compute the Hessian inverse vector
mateKrylovsubspacesandproposeanelaboratesubspace-
productinbileveloptimization.Tobypassit,severalstrate-
basedframeworkforbilevelproblems.
gies implemented through inner iterations were proposed,
e.g.,theNeumannseriesapproximation(Ghadimi&Wang,
2.1.WhyKrylovsubspace: theSubBiOalgorithm
2018; Ji et al., 2021), gradient descent (Arbel & Mairal,
2022; Dagre´ou et al., 2022), and conjugate gradient de- InlightoftheNeumannseriesforasuitableη R
scent(Pedregosa,2016;Arbel&Mairal,2022;Yangetal., ∈
2023). Alternatively, the previous information was ex- ∞
A 1b =η (I ηA)ib, (7)
−
ploitedin(Jietal.,2021;Arbel&Mairal,2022)asawarm −
i=0
startforouteriterations;Ramzietal.(2022)suggestedap- (cid:88)
proximating the Hessian inverse in the manner of quasi- it is observed from Appendix B that A −1b belongs to a
Newton; Dagre´ou et al. (2022) and Li et al. (2022) pro- KrylovsubspaceforsomeN >0,i.e.,
posed the frameworks without inner iterations to approxi-
A 1b (A,b)= (I ηA,b).
− N N
matetheHessianinversevectorproduct. ∈K K −
Hence, it is reasonable to consider a Krylov subspace for
theconstructionof .
2.Subspace-basedAlgorithms k
S
Givenaconstantn N,weconsideranapproximationof
Inthissection,wediveintothedevelopmentofbilevelop- ≪
A 1b in a lower-dimensional Krylov subspace (A,b),
timization algorithms for solving (1), which dynamically − Kn
i.e.,v (A,b)= (I ηA,b)and
construct subspaces to approximate the Hessian inverse n ∈Kn Kn −
vectorproduct. n 1
v = − c (I ηA)ib A 1b.
n i −
The (hyper-)gradient descent method carries out the k-th − ≈
i=0
(cid:88)
3LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Note that the approximation v
n
is composed of the set whereS
k
:= [b
k
(I ηA k)v
k
1] Rdy×2. Thesolution
{(I −ηA)ib }n i=−01 inthesenseoftheNeumannseries(7). z
∗
∈
R2 results in v−
k
= S kz− ∗. I∈ t is a two-dimensional
Moreover,weobservethat(I ηA)v (A,b)and subproblem, whereas computing the projection S A S
−
n
∈
Kn+1 k⊤ k k
hencewecanrecursivelychoose requirestwoHessian-vectorproducts,whichdominatethe
cost of the subproblem. Therefore, it is crucial to reduce
v :=span b,(I ηA)v (A,b)
n+1 ∈Sn+1 { − n }⊆Kn+1 oramortizetheprojectioncostwhilepreservingtheadvan-
since the subspace span b,(I ηA)v n includes the in- tages of the Krylov subspace. To this end, we find that
{ − }
formation of the increased set (I ηA)ib n . In sum- the Lanczos process (Appendix B) provides an enlighten-
{ − }i=0
mary,wecanconstructasequenceoftwo-dimensionalsub- ing way (Lanczos, 1950; Saad, 2011; Golub & Van Loan,
spaces that implicitly filters information from the 2013)sinceitallowsfortheconstructionofaKrylovsub-
n
{S }
Krylovsubspaces. Therationaleforthisprocedurecanbe spaceandmaintainingatridiagonalmatrixastheprojection
illustratedinFigure3. matrix,whichsignificantlyreducesthecomputationalcost.
In bilevel optimization, since the quadratic problem (6)
A 1b A 1b v evolves through outer iterations, it is difficult to leverage
− − N
the Lanczos process to amortize the projection cost while
Sn Sn+1 SN A −1b simultaneously updating variables as in SubBiO. Specif-
··· ically, the Lanczos process is inherently unstable (Paige,
v
v n+1 1980), and thus the accumulative difference among A
n { k }
and b willmaketheLanczosprocessinvalid.
k
{ }
In order to address the above difficulties, we propose a
(A,b) (A,b) (A,b) restart mechanism to guarantee the benign behavior of
n n+1 N
K ⊆ K ⊆ ··· ⊆ K
approximating the Krylov subspace and consider solving
residual systems to employ the historical information. In
Figure3.IllustrationofapproximatingA−1b (A,b)byv summary, we propose a dynamic Lanczos-aided Bilevel
N n
inthetwo-dimensionalsubspace (A,∈ b)K . Optimization framework, LancBiO, which is listed in Al-
n n
S ⊆K
gorithm2. TheonlydifferencebetweenLancBiOandSub-
Inthecontextofbileveloptimization,weseekthebestso- Bio is solving the subproblem (line 3-4 in Algorithm 1).
lutionv tothesubproblem(6)inthesubspace
k
=span b ,(I ηA )v . (8)
k k k k 1
S { − − }
Repeatingtheprocedureiscapableofdynamicallyapprox-
imatingtheHessianinversevectorproduct,i.e.,v approx- Algorithm2LancBiO
k
imates A−k1b k. The Krylov Subspace-aided Bilevel Opti- Input: iterationthresholdK,stepsizesθ,λ,initialization
mizationalgorithm(SubBiO)islistedinAlgorithm1. x ,y ,v ,initialcorrection∆v =0,subspacedimen-
1 1 1 0
sionm,initialepochh= 1
−
Algorithm1SubBiO 1: fork =1,2,...,K do
Input: iteration threshold K, step sizes θ,λ,η, initializa- 2: A = 2 g(x ,y ), b = f(x ,y )
k ∇yy k k k ∇y k k
tionx ,y ,v 3: if(kmodm)=1then
1 1 0
1: fork =1,2,...,K do 4: h=h+1
2: A = 2 g(x ,y ),b = f(x ,y ) 5: v¯ =v
k ∇yy k k k ∇y k k h k
3: =span b ,(I ηA )v 6: w =A v¯
k k k k 1 h k h
4 5: : vS xk k+= 1a =rg xm k{ −in v λ∈S ∇k− x1 2 fv ⊤ (xA kk ,v y k−− ) −b} ⊤k ∇v 2 xyg(x k,y k)v
k
87 :: TQ kk −− 11 == E(b mk p− tyw h M) a/ t∥ rb k ix−w h ∥
6: y k+1 =y k θ yg(x k+1,y k) 9: β k =0
− ∇(cid:0) (cid:1)
7: endfor 10: endif
Output: (x K+1,y K+1) 11: (T k,Q k,β k+1)=DLanczos(T k 1,Q k 1,A k,β k)
− −
12: r =b w
k k h
−
13: ∆v =Q (T ) 1Q r
k k k − ⊤k k
2.2.WhydynamicLanczos: theLancBiOframework 14: v =v¯ +∆v
k h k
15: x =x λ f(x ,y ) 2 g(x ,y )v
Notice that the subproblem in SubBiO (Algorithm 1) can k+1 k − ∇x k k −∇xy k k k
16: y =y θ g(x ,y )
beequivalentlyreducedto k+1 k − ∇(cid:0)y k+1 k (cid:1)
17: endfor
1 Output: (x , y )
zmi Rn
2
2z ⊤(S k⊤A kS k)z −b ⊤kS kz, K+1 K+1
∈
4LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
IfweadaptthestandardLanczosprocessfortridiagonaliz- Specifically, we inexactly solve the minimal residual sub-
ingA ,bystartingfromq = b / b , q = 0, β = 0 problem
k 1 1 1 0 1
andusingthedynamicmatricesA ∥ for∥ j =1,2,...,k,the min (b A v¯) A ∆v 2 (10)
j k k k
processisasfollows, ∆v ∈Sk∥ − − ∥
and use the solution ∆v as a correction to v¯ (line 12 to
k
u =A q β q , line14inAlgorithm2),
j j j j j 1
− −
α j =q j⊤u j, v k =v¯+∆v k.
ω =u α q ,
j j − j j Consequently,takingintoaccountthetwostrategiesabove,
β j+1 = ω j , we illuminate the framework LancBiO in Figure 4. It is
∥ ∥
q =ω /β , structured into epochs, with each epoch built by m outer
j+1 j j+1
iterations. Notably, each epoch restarts by incrementally
andT k isatridiagonalmatrixrecursivelycomputedfrom constructing from a one-dimensional space Q 1 to an m-
dimensionalspaceQ ,aimingtoapproximatethesolution
m
to the residual system within these subspaces. The solu-
0 tion ∆v to the subproblem serves as a correction to en-
 T  j
T =
j−1
. hance the hyper-gradient estimation, which facilitates the
j
  β j   (x,y)updating.
 
  0 β j α j  
 
 
Restartmechanism: IncontrasttoSubBiO,weconstruct
thesubspace
Epochs
=span(Q )=span([q ,...,q ]).
k k 1 k
S
Consequently, Q approximates the basis of the true One Epoch
k
Krylovsubspace (A ,b ),andT approximatesthepro-
k k k k
K
jectionofA ontoit,i.e.,
k
m Outer Iterations
T Q A Q .
k
≈
⊤k k k
However, Q willlosetheorthogonalityduetotheevolu- Subspace Construction
k
tion of A , and T will deviate from the true projection.
j k
Basedonthisobservation,werestartthesubspacespanned
··· ··· ··· ···
by the matrix Q for every m outer iterations (line 3 to
line 10 in Algorithm 2). The restart mechanism allows
us to mitigate the accumulation of the difference among Q 1 Q 2 Q j Q m
··· ···
A ,...,A ,andhence,wecanmaintainamorereliable
1 k
{ }
basismatrixtoapproximateKrylovsubspaces. Theabove Residual Minimization
dynamic Lanczos subroutine, DLanczos, is summarized
b A v¯
k k
inAppendixC(line11inAlgorithm2). −
Residualminimization: Theprecedingdiscussionreveals ∆v 1 ∆v 2 ··· ∆v j ··· ∆v m
thatthedimensionofthesubspace shouldbemoderate
k
S
to retain the reliability of the dynamic process. Neverthe- Hyper-gradient Estimation
less,thelimiteddimensionstagnatestheapproximationof
the subproblem (6) to the full space problem. Therefore, φ φ φ φ
∇ 1 ∇ 2 ··· ∇ j ··· ∇ m
insteadofsolvingthequadraticsubproblem(6)inthesub-
space, we intend to find v k = span(Q k) such that e e (x,y) Uepdating e
∈ S
A v b . To this end, after going through m outer iter-
k k
≈
ations, we denote the current approximation by v¯. In the
subsequentouteriterations,weconcentrateonalinearsys-
Figure4. AnoverviewofLancBiO.
temwitharesidualintheform
A ∆v =b A v¯, ∆v . (9) The combination of the two strategies, restart mechanism
k k k k
− ∈S
5LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
and residual minimization, not only controls the dimen- 3.TheoreticalAnalysis
sionofthesubspacebutalsoutilizeshistoricalinformation
to enhance the approximation accuracy. By considering a In this section, we provide a non-asymptotic convergence
simplifiedscenario,wereducethetwostrategiesintosolv- analysis for LancBiO. Firstly, we introduce some appro-
ingastandardlinearsystemproblemAx=bwithAandb
priateassumptions. Subsequently,toaddresstheprincipal
fixed.Notethat,fromtheperspectiveofTheorem1in(Car-
theoretical challenges, we analyze the properties and dy-
mon & Duchi, 2018), the residual associated with solving
a linear system in an m-dimensional Krylov subspace de- namicsofthesubspacesconstructedinSection2. Finally,
caysfasterthanarate 1/m2 aftereachrestart.Inother weprovetheglobalconvergenceofLancBiOandgivethe
words, the estimationO error of the Hessian inverse vector iterationcomplexity;thedetailedproofsareprovidedinthe
productexperiencesade(cid:0)cayrat(cid:1)eof 1/m2 afterevery appendices.
O
restart,i.e.,
Assumption 3.1. The upper-level function f is twice
(cid:0) (cid:1)
(cid:13) (cid:13)b ∥−
b
−Av Am v( mh+
h
∥1)(cid:13) (cid:13) = (cid:13) (cid:13)(b −A ∥v¯ bh) −− AA
v¯
h∆ ∥m(h+1)(cid:13) (cid:13) = O(cid:18) m1 2(cid:19) . c ao nn dti
∇
fnu
(y
xo fu
,(
ys xl ,y
(y
x)d )i )af rf eer Len Cft xia -Lb .l ie p. schiT tzhe andgr Lad fi ye -n Lts ips∇ chx if tz( ,x a, ny d)
y ∗ fy
Remark 2.1. TheclassicLanczosprocessisknownforits ∥∇ ∥≤
Assumption 3.2. The lower-level function g is twice
capability to solve indefinite linear systems (Greenbaum
continuously differentiable. g(x,y) and g(x,y)
etal.,1999). Inthesamefashion,theLancBiOframework ∇x ∇y
are L -Lipschitz and L -Lipschitz. The derivative
can be adapted to the bilevel problems with a nonconvex gx gy
2 g(x,y) and the Hessian matrix 2 g(x,y) are L -
lower-levelproblem. InterestedreadersarereferredtoAp- ∇xy ∇yy gxy
LipschitzandL -Lipschitz.
pendixDformoredetails. gyy
Assumption 3.3. For any x Rdx, the lower-level func-
∈
2.3.Relationtotheexistingalgorithms tiong(x, )isµ g-stronglyconvex.
·
The proposed SubBiO and LancBiO have intrinsic con- The Lipschitz properties of f,g and the strong convex-
nections to the existing algorithms. Generally, in each ity of the lower-level problem revealed by the above as-
outer iteration, methods such as BSA (Ghadimi & Wang, sumptions are standard in bilevel optimization (Ghadimi
2018) and TTSA (Hong et al., 2023) truncate the Neu- & Wang, 2018; Chen et al., 2021; Ji et al., 2021; Khan-
mann series (7) at N, exploiting information from an N- durietal.,2021;Arbel&Mairal,2022;Chenetal.,2022;
dimensional Krylov subspace. On the contrary, both Sub- Dagre´ouetal.,2022;Lietal.,2022;Jietal.,2022;Hong
BiO and LancBiO implicitly gather knowledge from a et al., 2023). These assumptions ensure the smoothness
high-dimensionalKrylovsubspacewithlesseffort. of φ and y ; see the following results (Ghadimi & Wang,
∗
2018).
SubBiO shares similarities with SOBA (Dagre´ou et al.,
Lemma3.4. UndertheAssumptions3.2and3.3,y (x)is
2022)andFSLA(Lietal.,2022). Theupdateruleforthe ∗
estimatorvoftheHessianinversevectorproductinSOBA
L gx/µ
g
-Lipschitzcontinuous,i.e.,foranyx 1,x
2
Rdx,
∈
andFSLAis L
y (x ) y (x ) gx x x . (11)
∗ 1 ∗ 2 1 2
v k =v k 1 η(A kv k 1 b k) ∥ − ∥≤ µ g ∥ − ∥
− − − −
=(I ηA k)v k 1+ηb k, Lemma3.5. UndertheAssumptions3.1, 3.2and3.3, the
− −
hyper-gradient φ() is L -Lipschitz continuous, i.e., for
w suh bi sle pat ch ee ,pr kop =os se pd anSub
b
kB ,i (O Icon ηs Atr ku )c vts ka 1tw ,o d- ed fii nm ee dn is nio (n 8a )l
.
anyx 1,x
2
∈Rd∇
x,
· φ
ItisworthS notingthat{ theupd−
atedv
k
in−S}
OBAandFSLA
φ(x 1) φ(x 2) L φ x 1 x 2 ,
belongs to the subspace . Furthermore, in the sense of ∥∇ −∇ ∥≤ ∥ − ∥
k
solving the two-dimensioS nal subproblem (6), SubBiO se- whereL φ >0isdefinedinAppendixE.
lectstheoptimalsolutionvinthesubspace. Assumption 3.6. There exists a constant C so that
fx
f(x,y) C .
In addition, if the subspace dimension m is set to one, ∥∇x ∥≤ fx
LancBiOissimplifiedtoascenarioinwhichoneconjugate
Assumption3.6,commonlyadoptedin(Ghadimi&Wang,
gradient (CG) step with the warm start mechanism is per-
2018;Jietal.,2021;Liuetal.,2022;Kwonetal.,2023b),
formed in each outer iteration, which exactly recovers the
is helpful in ensuring the stable behavior of the dynamic
algorithm AmIGO-CG (Arbel & Mairal, 2022) with one
Lanczosprocess;seeSection3.1.
inneriterationtotheupdateofv. Alternatively,ifthestep
sizeλinAlgorithm2issetto0withineachm-steps(i.e.,
3.1.SubspacePropertiesinDynamicLanczosProcess
onlyinneriterationsareinvoked),LancBiOreducestothe
algorithmAmIGO-CG(Arbel&Mairal,2022)withmin- In view of the inherent instability of the Lanczos process
neriterations. (Paige,1980;Meurant&Strakosˇ,2006)andtheevolution
6LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
oftheHessian A andthegradient b inLancBiO,the for j = 1,2,...,m, where Q = [q ,q ,...,q ], δQ =
k k j 1 2 j j
{ } { }
analysisoftheconstructedsubspacesisintricate. Basedon [δq ,δq ,...,δq ],
1 2 j
the existing work (Paige, 1976; 1980; Greenbaum, 1997),
this subsection sheds light on the characterization of sub- α 1 β 2
spacesandtheeffectivenessofthesubprobleminapproxi- β 2 α 2 β 3
 
matingthefullspaceprobleminLancBiO.
T = β
... ...
j  3 
A Lan nce zp oo sch pri os cec so sns bti et tu wte ed enof twa oco rem stp al re tt se
,
m na- mst ee lp y,dy afn ta em
r
i hc 
 
... ...
β j

 
epochs, the number of outer iterations is mh. Given the  β α 
 j j 
outer iterations k = mh+j for j = 1,2,...,m, we de-  
note and δq L ε .
j gyy j
∥ ∥≤
(cid:18) (cid:19)
L
εh st := 1+ µg gx ∥x mh+s −x mh+t ∥+ ∥y mh+s −y m∗ h+s∥ 3.2.ConvergenceAnalysis
To guarantee the stable behavior of the dynamic process,
fors,t=1,2,...,mand
threemildassumptionsareneeded.
εh j := max εh st, Assumption 3.9. The initialization of y 1 in Algorithm 2
1 s,t j satisfies
≤ ≤
√3µ
g
serving as the accumulative difference. For brevity, we ∥y 1 −y 1∗ ∥≤ 8(m+1)3L .
omit the superscript where there is no ambiguity, and we gyy
are slightly abusing of notation that at the current epoch,
Similar initialization refinement is used in (Hao et al.,
A and b are simplified by A and b
{ mh+j } { mh+j } { j } { j } 2024), which can be achieved by implementing several
for j = 1,...,m. In addition, the approximations in the
gradientdescentstepsforthesmoothandstronglyconvex
residualsystem(9)aresimplifiedbyv¯and¯b:=b A v¯.
1 − 1 lower-levelproblem.
The following proposition characterizes that the dynamic
Assumption3.10. Foreachepoch,thefollowinginequal-
subspaceconstructedinAlgorithm2withinanepochisin-
ityholds,
deedanapproximateKrylovsubspace.
Proposition 3.7. At the j-th step within an epoch (j =
εh λ 1+
L
gx
m −1
φ(x )
i1 n,2 A, l. g. o. r, itm hm− 21 s), at th ise fis eu sbspacespannedbythematrixQ j+1 m ≤ (cid:18) µ g (cid:19) (cid:88)j=1 (cid:13)∇ mh+j (cid:13) (12)
+ y mh+m −y m∗h+(cid:13) (cid:13) m(cid:101) . (cid:13) (cid:13)
span(Q j+1) ⊆span Aa 11Aa 22 ···Aa jj¯b sa =s = 1,20 ,o .r ..1 ,, j . Assumption 3.11(cid:13) (cid:13). There exists a co(cid:13) (cid:13)nstant C gy > 0 such
(cid:26) (cid:12) (cid:27) that the iterates (x ,y ) generated by Algorithm 2 sat-
(cid:12) k+1 k
Specifically,whenA
1
=A
2
= ···=A j(cid:12) (cid:12)=AandQ
j+1
is isfy ∥∇yg(x k+1,y k) ∥≤C gy.
offullrank,
Assumption 3.10 is mild since the first term on the
span(Q )= A,¯b . right-hand side of (12) contains all upper-level steps
j+1 j+1
K (cid:0) (cid:1) { isλ ∇ reφ as( ox nm abh l+ ej) in}m j= p− r1 a1 cw ticit ehin sino cn ee e wp eoc ih n. itiA als is zu em yption 3.1 y1
Denote
1
≈
1∗
and(cid:101)consider the descent properties of ∥y
k
−y
k∗
∥
outlined
inAppendixH.
A = 2 g(x,y ) and b = f(x,y ).
∗ ∇yy ∗ ∗ ∇y ∗
Denotetheresidual
Notice that the dynamic Lanczos process in Algorithm 2
centers on A j instead of A ∗j. The subsequent lemma in- r¯ 0 :=¯b,
terpretstheperturbationanalysisforthedynamicLanczos r¯ :=¯b A ∆v , j =1,2,...,m.
j
−
∗j j
process in terms of A , which satisfies an approximate 3-
j
termrecurrencewithaperturbationtermδQ. The following lemma reveals that the dynamic process
yieldsanimprovedsolutionforthesubproblem(10).
Lemma 3.8. Suppose Assumptions 3.1 to 3.3 hold. The
dynamic Lanczos process in Algorithm 2 with normalized Lemma3.12. SupposeAssumptions3.1,3.2,3.3,3.6,3.9,
q andα ,β ,q satisfies 3.10, and 3.11 hold. Within each epoch, we set the step
1 j j j
size θ (1/m4) a constant for y and the step size for
∼ O
A Q =Q T +β q e +δQ x as zero in the first m (1) steps, and the others as
∗j j j j j+1 j+1 ⊤j j 0
∼ O
7LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
LancBiO_m10
103
100 LancBiO_m20
101 LancBiO_m50
10 1 LancBiO_m80
10 1 LancBiO_m150
AmIGO-GD_I3
10 2
10 3 AmIGO-GD_I10
AmIGO-CG_I3
10 3 20 40 60 80 100 120 140 10 5 0 20 40 60 80 100 120 140 AmIGO-CG_I10
Time [sec] Time [sec]
Figure5.InfluenceofthesubspacedimensionmonLancBiO.Thepost-fixoflegendrepresentsthesubspacedimensionmortheinner
iterationI.Left:normofthehyper-gradient;Right:residualnormofthelinearsystem, A v b .
k k k
∥ − ∥
an appropriate constant λ (1/m4), then we have the narioinbileveloptimization(1)withd =d =dand
x y
∼ O
followinginequality,
1
f(x,y):=c cos x D y + D x y 2,
1 ⊤ 1 2
2∥ − ∥
j
r¯ j κ˜(j) 1 d (cid:0) (cid:1) d
∥ ∥ 2 κ˜(j) − + jL ε κ˜(j),
∥r¯ 0
∥
≤
(cid:112)
(cid:32)(cid:112)κ˜(j)+1(cid:33)
(cid:112)
gyy j g(x,y):=c 2 i=1sin(x i+y i)+log
(cid:32)
i=1exiyi
(cid:33)
(cid:88) (cid:88)
(cid:112) 1
whereκ˜(j):=
Lgy+2√ 33(j+1)3Lgyyεj. + 2y ⊤(D 3+G)y.
µg−2√ 33(j+1)3Lgyyεj
Theorem 3.13. Suppose Assumptions 3.1, 3.2, 3.3, 3.6, ItcanbeseenfromFigure8thatLancBiOachievesthefi-
3.9, 3.10 and 3.11 hold. Within each epoch, we set the nalaccuracythefastest,whichbenefitsfromthemoreaccu-
step size as θ (1/m4) a constant for y and the step ratev estimation. Figure5illustrateshowvariationsinm
∼ O ∗
sizeforxaszerointhefirstm 0 steps,andtheothersasa andI influencetheperformanceofLancBiOandAmIGO,
constant λ (1/m4), then the iterates x k generated testedacrossarangefrom10to150form,andfrom2to10
∼ O { }
byAlgorithm2satisfy forI.Forclarity,wesettheseedoftheexperimentat4,and
presenttypicalresultstoencapsulatetheobservedtrends.It
m K mλ 1 isobservedthattheincreaseofmacceleratesthedecrease
φ(x k) 2 − , intheresidualnorm, thusachievingbetterconvergenceof
K(m m ) ∥∇ ∥ ≤O K(m m )
− 0 k=0, (cid:18) − 0 (cid:19) thehyper-gradient,whichalignswiththespiritoftheclas-
(kmod(cid:88)
m)>m0
sicLanczosprocess. Underthesameouteriterations,toat-
tainacomparableconvergenceproperty,I forAmIGO-CG
wherem (logm)isaconstantandmisthesubspace
0 ∼O should be set to 10. Furthermore, given that the number
dimension.
of Hessian-vector products averages at (1+ 1) per outer
m
iterationforLancBiO,whereasAmIGOrequiresI 2,it
In other words, we prove that the proposed LancBiO is ≥
followsthatLancBiOismoreefficient.
globally convergent, and the average norm square of the
hyper-gradient φ(x ) 2achievesϵwithin (ϵ 1)outer Datahyper-cleaningonMNIST:Thedatahyper-cleaning
k −
∥∇ ∥ O
iterations. task (Shaban et al., 2019), conducted on the MNIST
dataset (LeCun et al., 1998), aims to train a classifier in a
4.NumericalExperiments corruption scenario, where the labels of the training data
arerandomlyalteredtoincorrectclassificationnumbersata
In this section, we conduct experiments in the determin- certainprobabilityp,referredtoasthecorruptionrate.
istic setting to empirically validate the performance of
The results are presented in Figure 6 and Table 1. Note
the proposed algorithms. We test on a synthetic problem
that LancBiO is crafted for approximating the Hessian
and two deep learning tasks. The selection of parameters
inverse vector product v , while the two solid methods,
and more details of the experiments are deferred to Ap- ∗
TTSAandstocBiOarenot. Consequently,withrespectto
pendix I. We have made the code publicly available on
the residual norm of the linear system, i.e., A v b ,
https://github.com/UCAS-YanYang/LancBiO. ∥ k k − k ∥
we only compare the results with AmIGO-GD, AmIGO-
Synthetic problem: We concentrate on a synthetic sce- CGandSOBA.Observethattheproposedsubspace-based
8
mron
tneidarg-repyH
mron
laudiseRLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
LancBiO SubBiO AmIGO-GD AmIGO-CG SOBA TTSA stocBiO
0.90 100
10 1
9×10 1
0.88
8×10 1 10 2
0.86
7×10 1
0.84 10 3
6×10 1
0.82 10 4
5×10 1
0.80
10 5
0.78 4×10 1
0 20 40 60 80 100 0 20 40 60 80 100 20 40 60 80 100 120 140 160
Time [sec] Time [sec] Time [sec]
Figure6.Comparison of the bilevel algorithms on data hyper-cleaning task when p = 0.8. Left: test accuracy; Center: test loss;
Right:residualnormofthelinearsystem, A v b .
k k k
∥ − ∥
Table1.Comparisonofthebilevelalgorithmsondatahyper-cleaningtaskacrosstwocorruptionratesp=0.5andp=0.8.Theresults
areaveragedover10runsand isfollowedbythestandarddeviation.Theresultsareconductedafter40and60secondsrunningtime.
±
p=0.5 p=0.8
Algorithm
Testaccuracy(%) Testloss Residual Testaccuracy(%) Testloss Residual
LancBiO 90.35 0.1716 0.36 0.0028 1.20e 4 2.52e 5 89.45 0.2470 0.42 0.0038 9.18e 5 2.77e 5
± ± − ± − ± ± − ± −
SubBiO 90.21 0.2159 0.36 0.0035 2.22e 2 2.63e 3 89.22 0.2587 0.42 0.0050 2.49e 2 1.93e 3
± ± − ± − ± ± − ± −
AmIGO-GD 90.16 0.2114 0.37 0.0044 3.66e 2 4.00e 3 89.14 0.2722 0.43 0.0044 1.07e 2 8.25e 4
± ± − ± − ± ± − ± −
AmIGO-CG 90.06 0.2305 0.38 0.0053 5.89e 4 2.52e 4 88.57 0.5839 0.46 0.0176 6.32e 4 3.74e 4
± ± − ± − ± ± − ± −
SOBA 90.00 0.1811 0.37 0.0051 2.74e 2 8.52e 3 88.99 0.2661 0.42 0.0054 1.73e 2 1.70e 3
± ± − ± − ± ± − ± −
TTSA 89.35 0.2747 0.40 0.0103 - 82.91 0.4516 0.74 0.0072 -
± ± ± ±
stocBiO 89.20 0.1824 0.43 0.0033 - 86.44 0.2907 0.54 0.0064 -
± ± ± ±
LancBiO achieves the lowest residual norm and the best 5.ConclusionandPerspective
testaccuracy,andsubBiOiscomparabletotheotheralgo-
This paper presents a novel approach to bilevel optimiza-
rithms. Specifically,inFigure6,theefficiencyofLancBiO
tionthroughthedynamicLanczosprocesstoapproximate
stemsfromitsaccurateapproximationofthelinearsystem.
Krylovsubspaces,toaddressthecomputationalchallenges
Additionally, while AmIGO-CG is also adept at approxi-
inherentincalculatingthehyper-gradient. Thetheoretical
matingv ,thegreenlineinFigure6indicatesthatittends
∗
analysis and empirical validation of LancBiO underscore
toyieldresultswithhighervariance.
its reliability and effectiveness. The proposed framework
Logistic regression on 20Newsgroup: Consider the islimitedtothedeterministicsetting. Theincorporationof
hyper-parameters selection task on the 20Newsgroups stochastic subspace techniques could unlock better poten-
dataset (Grazzi et al., 2020), which contains c = 20 top- tialinwiderapplications.
ics with around 18000 newsgroup posts represented in a
featurespaceofdimensionl = 130107. Thegoalistosi-
References
multaneously train a linear classifier w and determine the
optimalregularizationparameterζ.AsshowninFigure11, Arbel,M.andMairal,J. Amortizedimplicitdifferentiation
AmIGO-CGexhibitsslightlybetterperformanceinreduc- forstochasticbileveloptimization.InTheTenthInterna-
ingtheresidualnorm. Nevertheless, underthesametime, tionalConferenceonLearningRepresentations,2022.
LancBiO implements more outer iterations to update x,
whichoptimizesthehyper-functionmoreefficiently. Bellavia,S.andMorini,B. AgloballyconvergentNewton-
GMRES subspace method for systems of nonlinear
Generally,forstandardlinearsystems,thetraditionalLanc-
equations. SIAM Journal on Scientific Computing, 23
zos process is recognized for its efficiency and versatil-
(3):940–960,2001.
ity over gradient descent methods. LancBiO, in a sense,
reflects this principle within the context of bilevel opti- Bertinetto,L.,Henriques,J.F.,Torr,P.H.,andVedaldi,A.
mization, underscoring the effectiveness of the dynamic Meta-learning with differentiable closed-form solvers.
Lanczos-aidedapproach. arXivpreprintarXiv:1805.08136,2018.
9
ycarucca
tseT
ssol
tseT
mron
laudiseRLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Brown,P.N.andSaad,Y. HybridKrylovmethodsfornon- Franceschi,L.,Frasconi,P.,Salzo,S.,Grazzi,R.,andPon-
linearsystemsofequations. SIAMJournalonScientific til, M. Bilevel programming for hyperparameter opti-
andStatisticalComputing,11(3):450–481,1990. mizationandmeta-learning. InInternationalconference
onmachinelearning,pp.1568–1577.PMLR,2018.
Carmon,Y.andDuchi,J.C. AnalysisofKrylovsubspace
solutionsofregularizednon-convexquadraticproblems. Gao, W., Qu, Z., Udell, M., and Ye, Y. Scalable approx-
AdvancesinNeuralInformationProcessingSystems,31, imateoptimaldiagonalpreconditioning. arXivpreprint
2018. arXiv:2312.15594,2023.
Cartis, C. and Roberts, L. Scalable subspace methods
Ghadimi, S. and Wang, M. Approximation methods for
for derivative-free nonlinear least-squares optimization.
bilevelprogramming. arXivpreprintarXiv:1802.02246,
MathematicalProgramming,199(1-2):461–524,2023.
2018.
Cartis,C.,Gould,N.I.,andToint,P.L.Adaptivecubicreg-
Golub, G. H. and Van Loan, C. F. Matrix computations.
ularisationmethodsforunconstrainedoptimization.part
JHUpress,2013.
I:motivation,convergenceandnumericalresults. Math-
ematicalProgramming,127(2):245–295,2011.
Gould,N.I.,Lucidi,S.,Roma,M.,andToint,P.L.Solving
Chakraborty, S., Bedi, A. S., Koppel, A., Huang, F., and the trust-region subproblem using the Lanczos method.
Wang, M. Principal-driven reward design and agent SIAMJournalonOptimization,9(2):504–525,1999.
policy alignment via bilevel-rl. In Interactive Learning
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S. On
withImplicitHumanFeedbackWorkshop(ILHF),ICML,
the iteration complexity of hypergradient computation.
2023.
In International Conference on Machine Learning, pp.
Chen, L., Xu, J., and Zhang, J. On bilevel optimiza- 3748–3758.PMLR,2020.
tionwithoutlower-levelstrongconvexity.arXivpreprint
arXiv:2301.00712,2023. Greenbaum, A. Iterative methods for solving linear sys-
tems. SIAM,1997.
Chen, T., Sun, Y., and Yin, W. Closing the gap: Tighter
analysis of alternating stochastic gradient methods for Greenbaum, A., Druskin, V., and Knizhnerman, L. On
bilevel problems. In Beygelzimer, A., Dauphin, Y., solvingindefinitesymmetriclinearsystemsbymeansof
Liang,P.,andVaughan,J.W.(eds.),AdvancesinNeural theLanczosmethod. Comput.Math.Math.Phys,39(3):
InformationProcessingSystems,2021. 350–356,1999.
Chen,T.,Sun,Y.,Xiao,Q.,andYin,W.Asingle-timescale Hao,J.,Gong,X.,andLiu,M. Bileveloptimizationunder
method for stochastic bilevel optimization. In Interna- unbounded smoothness: A new algorithm and conver-
tional Conference on Artificial Intelligence and Statis- genceanalysis. InTheTwelfthInternationalConference
tics,pp.2466–2488.PMLR,2022. onLearningRepresentations,2024.
Dagre´ou,M.,Ablin,P.,Vaiter,S.,andMoreau,T.Aframe-
Hestenes, M. R., Stiefel, E., et al. Methods of conju-
workforbileveloptimizationthatenablesstochasticand
gategradientsforsolvinglinearsystems. Journalofre-
globalvariancereductionalgorithms. AdvancesinNeu-
searchoftheNationalBureauofStandards,49(6):409–
ral Information Processing Systems, 35:26698–26710,
436,1952.
2022.
Hong, M., Wai, H.-T., Wang, Z., and Yang, Z. A two-
Dempe, S.andDutta, J. Isbilevelprogrammingaspecial
timescalestochasticalgorithmframeworkforbilevelop-
case of a mathematical program with complementarity
timization:Complexityanalysisandapplicationtoactor-
constraints? Mathematical programming, 131:37–48,
critic. SIAM Journal on Optimization, 33(1):147–180,
2012.
2023.
Dempe,S.andZemkoho,A.B. Thebilevelprogramming
problem: reformulations, constraint qualifications and Hu, X., Xiao, N., Liu, X., and Toh, K.-C. An improved
optimalityconditions.MathematicalProgramming,138: unconstrained approach for bilevel optimization. SIAM
447–473,2013. JournalonOptimization,33(4):2801–2829,2023.
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M. Ji, K., Yang, J., andLiang, Y. Bileveloptimization: Con-
Forwardandreversegradient-basedhyperparameterop- vergenceanalysisandenhanceddesign. InInternational
timization. In International Conference on Machine conferenceonmachinelearning,pp.4882–4892.PMLR,
Learning,pp.1165–1173.PMLR,2017. 2021.
10LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Ji,K.,Liu,M.,Liang,Y.,andYing,L.Willbileveloptimiz- Lin,G.-H.,Xu,M.,andYe,J.J. Onsolvingsimplebilevel
ersbenefitfromloops. AdvancesinNeuralInformation programswithanonconvexlowerlevelprogram. Math-
ProcessingSystems,35:3011–3023,2022. ematicalProgramming,144(1-2):277–305,2014.
Jiang,R.,Raman,P.,Sabach,S.,Mokhtari,A.,Hong,M., Liu, B., Ye, M., Wright, S., Stone, P., andLiu, Q. Bome!
andCevher,V. Krylovcubicregularizednewton: Asub- bileveloptimizationmadeeasy: Asimplefirst-orderap-
spacesecond-ordermethodwithdimension-freeconver- proach.AdvancesinNeuralInformationProcessingSys-
gencerate. arXivpreprintarXiv:2401.03058,2024. tems,35:17248–17262,2022.
Khanduri, P., Zeng, S., Hong, M., Wai, H.-T., Wang, Z., Liu,H.,Simonyan,K.,andYang,Y. Darts: Differentiable
and Yang, Z. A near-optimal algorithm for stochastic architecture search. arXiv preprint arXiv:1806.09055,
bilevel optimization via double-momentum. Advances 2018.
in neural information processing systems, 34:30271–
30283,2021. Liu, R., Mu, P., Yuan, X., Zeng, S., and Zhang, J. A
generic first-order algorithmic framework for bi-level
Krylov, A. N. On the numerical solution of the equation
programmingbeyondlower-levelsingleton. InInterna-
bywhichintechnicalquestionsfrequenciesofsmallos-
tionalConferenceonMachineLearning,pp.6305–6315.
cillations of material systems are determined. Izvestija
PMLR,2020.
AN SSSR (News of Academy of Sciences of the USSR),
Otdel.mat.iestest.nauk,7(4):491–539,1931. Liu,R.,Liu,Y.,Yao,W.,Zeng,S.,andZhang,J. Averaged
method of multipliers for bi-level optimization without
Kuczyn´ski,J.andWoz´niakowski,H.Estimatingthelargest
lower-levelstrongconvexity. InProceedingsofthe40th
eigenvalue by the power and Lanczos algorithms with
InternationalConferenceonMachineLearning,volume
a random start. SIAM journal on matrix analysis and
202ofProceedingsofMachineLearningResearch, pp.
applications,13(4):1094–1122,1992.
21839–21866.PMLR,23–29Jul2023.
Kwon, J., Kwon, D., Wright, S., and Nowak, R. On
Liu, X., Wen, Z., and Zhang, Y. Limited memory block
penaltymethodsfornonconvexbileveloptimizationand
Krylov subspace optimization for computing dominant
first-order stochastic approximation. arXiv preprint
singularvaluedecompositions. SIAMJournalonScien-
arXiv:2309.01753,2023a.
tificComputing,35(3):A1641–A1668,2013.
Kwon, J., Kwon, D., Wright, S., and Nowak, R. D. A
Liu, X., Wen, Z., and Yuan, Y.-X. Subspace methods for
fully first-order method for stochastic bilevel optimiza-
nonlinearoptimization. CSIAMTrans.Appl.Math,2(4):
tion. InInternationalConferenceonMachineLearning,
585–651,2021.
pp.18083–18113.PMLR,2023b.
Lanczos, C. An iteration method for the solution of the Lu,Z.andMei,S. First-orderpenaltymethodsforbilevel
eigenvalueproblemoflineardifferentialandintegralop- optimization. arXivpreprintarXiv:2301.01716,2023.
erators. Journal of Research of the National Bureau of
Maclaurin, D., Duvenaud, D., and Adams, R. Gradient-
Standards,45(4),1950.
based hyperparameter optimization through reversible
LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P.Gradient- learning. InInternationalconferenceonmachinelearn-
based learning applied to document recognition. Pro- ing,pp.2113–2122.PMLR,2015.
ceedingsoftheIEEE,86(11):2278–2324,1998.
Meurant, G. and Strakosˇ, Z. The Lanczos and conjugate
Li, J., Gu, B., and Huang, H. A fully single loop algo- gradient algorithms in finite precision arithmetic. Acta
rithm for bilevel optimization without Hessian inverse. Numerica,15:471–542,2006.
InProceedingsoftheAAAIConferenceonArtificialIn-
telligence,volume36,pp.7426–7434,2022. Nesterov,Y.etal.Lecturesonconvexoptimization,volume
137. Springer,2018.
Li,Y.,Liu,H.,Wen,Z.,andYuan,Y.-x. Low-rankmatrix
iteration using polynomial-filtered subspace extraction. Outrata,J.V.OnthenumericalsolutionofaclassofStack-
SIAM Journal on Scientific Computing, 42(3):A1686– elberg problems. Zeitschrift fu¨r Operations Research,
A1713,2020. doi: 10.1137/19M1259444. 34:255–277,1990.
Li,Y.,Lin,G.-H.,Zhang,J.,andZhu,X.Anovelapproach Paige,C.C. Thecomputationofeigenvaluesandeigenvec-
for bilevel programs based on Wolfe duality. arXiv torsofverylargesparsematrices.PhDthesis,University
preprintarXiv:2302.06838,2023. ofLondon,1971.
11LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Paige, C. C. Error analysis of the lanczos algorithm for Ye,J.J.andZhu,D. Optimalityconditionsforbilevelpro-
tridiagonalizingasymmetricmatrix.IMAJournalofAp- grammingproblems. Optimization,33(1):9–27,1995.
pliedMathematics,18(3):341–349,1976.
Ye,J.J.andZhu,D. Newnecessaryoptimalityconditions
Paige,C.C. AccuracyandeffectivenessoftheLanczosal- forbilevelprogramsbycombiningtheMPECandvalue
gorithmforthesymmetriceigenproblem.Linearalgebra functionapproaches. SIAMJournalonOptimization,20
anditsapplications,34:235–258,1980. (4):1885–1905,2010.
Paige, C. C. and Saunders, M. A. Solution of sparse in- Yuan, Y.-x. A review on subspace methods for nonlin-
definite systems of linear equations. SIAM journal on ear optimization. In Proceedings of the International
numericalanalysis,12(4):617–629,1975. CongressofMathematics,pp.807–827,2014.
Parlett, B.N. The symmetriceigenvalue problem. SIAM,
Zhang, L.-H., Shen, C., and Li, R.-C. On the generalized
1998.
Lanczos trust-region method. SIAM Journal on Opti-
mization,27(3):2110–2142,2017.
Pedregosa, F. Hyperparameteroptimizationwithapproxi-
mate gradient. In International conference on machine
Zhang, Y., Khanduri, P., Tsaknakis, I., Yao, Y., Hong,
learning,pp.737–746.PMLR,2016.
M., and Liu, S. An introduction to bi-level optimiza-
Ramzi,Z.,Mannel,F.,Bai,S.,Starck,J.-L.,Ciuciu,P.,and tion: Foundations and applications in signal processing
Moreau, T. Shine: Sharing the inverse estimate from andmachinelearning.arXivpreprintarXiv:2308.00788,
the forward pass for bi-level optimization and implicit 2023.
models. In International Conference on Learning Rep-
resentations,2022.
Saad,Y.Numericalmethodsforlargeeigenvalueproblems.
SIAM,2011.
Shaban,A.,Cheng,C.-A.,Hatch,N.,andBoots,B. Trun-
catedback-propagationforbileveloptimization. InThe
22ndInternationalConferenceonArtificialIntelligence
andStatistics,pp.1723–1732.PMLR,2019.
Stackelberg,H.v. Thetheoryofthemarketeconomy. Ox-
fordUniversityPress,1952.
Wang, J., Chen, H., Jiang, R., Li, X., and Li, Z. Fast
algorithms for Stackelberg prediction game with least
squares loss. In International Conference on Machine
Learning,pp.10708–10716.PMLR,2021.
Wang,X.,Guo,W.,Su,J.,Yang,X.,andYan,J. Zarts: On
zero-order optimization for neural architecture search.
AdvancesinNeuralInformationProcessingSystems,35:
12868–12880,2022.
Xiao, Q., Lu, S., and Chen, T. An alternating opti-
mizationmethodforbilevelproblemsunderthePolyak-
łojasiewiczcondition. InAnnualConferenceonNeural
InformationProcessingSystems,2023.
Xu, M.andYe, J.J. AsmoothingaugmentedLagrangian
method for solving simple bilevel programs. Com-
putational Optimization and Applications, 59:353–377,
2014.
Yang,H.,Luo,L.,Li,C.J.,Jordan,M.,andFazel,M. Ac-
celerating inexact hypergradient descent for bilevel op-
timization. In OPT 2023: Optimization for Machine
Learning,2023.
12LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
A.RelatedWorkinBilevelOptimization
Avarietyofbileveloptimizationalgorithmsarebasedonreformulation. Thesealgorithmsinvolvetransformingthelower-
levelproblemintoasetofconstraints,suchastheoptimalconditionsofthelower-levelproblem(Dempe&Dutta,2012;
Lietal.,2023),ortheoptimalvaluecondition(Outrata,1990;Ye&Zhu,1995;2010;Dempe&Zemkoho,2013;Linetal.,
2014;Xu&Ye,2014). Furthermore,incorporatingtheconstraintsofthereformulatedproblemasthepenaltyfunctioninto
theupper-levelobjectiveinspiresaseriesofalgorithms(Liuetal.,2022;Huetal.,2023;Kwonetal.,2023a;b;Lu&Mei,
2023). Anothercategoryofmethodsinbileveloptimizationistheiterativedifferentiation(ITD)basedmethod(Maclaurin
etal.,2015;Franceschietal.,2017;Shabanetal.,2019;Grazzietal.,2020;Liuetal.,2020;Jietal.,2021),whichtakes
advantageoftheautomaticdifferentiationtechnique. Centraltothisapproachistheconstructionofacomputationalgraph
during each outer iteration, achieved by solving the lower-level problem. This setup facilitates the approximation of the
hyper-gradientthroughbackpropagation,anditisnotedthatpartsofthesemethodsshareaunifiedstructure,characterized
byrecursiveequations(Jietal.,2021;Lietal.,2022;Zhangetal.,2023). Theapproximateimplicitdifferentiation(AID)
treats the lower-level variable as a function of the upper-level variable. It calculates the hyper-gradient to implement
alternating gradient descent between the two levels (Ghadimi & Wang, 2018; Ji et al., 2021; Chen et al., 2022; Dagre´ou
etal.,2022;Lietal.,2022;Hongetal.,2023).
B.KrylovSubspaceandLanczosProcess
Krylovsubspace(Krylov,1931)isfundamentalinnumericallinearalgebra(Parlett,1998;Saad,2011;Golub&VanLoan,
2013) and nonlinear optimization (Yuan, 2014; Liu et al., 2021), specifically in the context of solving large linear sys-
tems and eigenvalue problems. We will briefly introduce the Krylov subspace and the Lanczos process, and recap some
importantproperties;readersarereferredtoSaad(2011);Golub&VanLoan(2013)formoredetails.
AnN-dimensionalKrylovsubspacegeneratedbyamatrixAandavectorbisdefinedasfollows,
(A,b):=span b,Ab,A2b,...,AN 1b ,
N −
K
andthesequenceofvectors b,Ab,A2b,...,AN 1b for(cid:8)msthebasisforit.TheK(cid:9)rylovsubspaceiswidelyacknowledged
−
foritsfavorablepropertiesinvariousaspects,includingapproximatingeigenvalues(Kuczyn´ski&Woz´niakowski,1992),
(cid:8) (cid:9)
solving the regularized nonconvex quadratic problems (Gould et al., 1999; Zhang et al., 2017; Carmon & Duchi, 2018),
andreducingcomputationcost(Brown&Saad,1990;Bellavia&Morini,2001;Liuetal.,2013;Jiangetal.,2024).
TheLanczosprocess(Lanczos,1950)isanalgorithmthatexploitsthestructureoftheKrylovsubspacewhenAissymmet-
ric. Specifically,inthej-thstepoftheLanczosprocess,wecanefficientlymaintainanorthogonalbasisQ of (A,b),
j j
K
sothatT =Q AQ istridiagonal,whichmeansatridiagonalmatrixT approximatesAintheKrylovsubspace. Conse-
j ⊤j j j
quently,itallowstosolvetheminimalresidualproblemortheeigenvalueproblemefficientlywithintheKrylovsubspace.
ThereareseveralequivalentvariantsoftheLanczosprocess(Paige,1971;1976;Meurant&Strakosˇ,2006),andwefollow
theupdateruleasshowninAlgorithm3. WenowpresentseveralkeypropertiesoftheKrylovsubspaceandtheLanczos
ProcessfromSaad(2011).
Definition B.1. The minimal polynomial of a vector v Rn with respect to a matrix A Rn n is defined as the non-
×
∈ ∈
zero monic polynomial p of the lowest degree such that p(A)v = 0, where a monic polynomial is a non-zero univariate
polynomialwiththecoefficientofhighestdegreeequalto1.
Remark B.2. The degree of the minimal polynomial p does not exceed n because the set of n + 1 vectors
Anv,An 1v,...,A2v,Av,v islinearlydependent.
−
{ }
RemarkB.3. SupposetheminimalpolynomialofavectorvwithrespecttoamatrixAis
p(x)=xm+c xm 1+ +c x2+c x+c ,
m 1 − 2 1 0
− ···
andhasadegreeofm. Ifc =0andAisinvertible,byDefinitionB.1,
0
̸
Amv+c Am 1v+ +c A2v+c Av+c v =0,
m 1 − 2 1 0
− ···
multiplybothsidesoftheequationbyA 1andrearrangetheequation,
−
1
v = Am 1v+c Am 2v+ +c Av+c v .
− m 1 − 2 1
−c 0 − ···
(cid:0) (cid:1)
Inotherwords,A 1vbelongstotheKrylovsubspace (A,v).
− m
K
13LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Algorithm3Lanczosprocess
Input: dimensionm,matrixA Rn n,initialvectorb Rn
×
∈ ∈
1: Initialization: q = b ,q =0,β =0,Q =T =Empty Matrix
1 b 0 1 0 0
2: forj =1,2,...,md∥o∥
3: u =Aq β q
j j j j 1
− −
4: α =q u
j j⊤ j
5: ω =u α q
j j j j
−
6: β = ω
j+1 j
∥ ∥
7: q =ω /β
j+1 j j+1
8: Q =[Q q ]
j j 1 j
−
T
j 1
9: T j = − β 
j
 β α 
 j j
10: endfor 
Output: T ,Q , b e
m m 1
∥ ∥
PropositionB.4. Denotethen j matrixwithcolumnvectorsq ,...,q byQ andthej mtridiagonalmatrixbyT ,
1 j j j
× ×
allofwhicharegeneratedbyAlgorithm3. Thenthefollowing3-termrecurrenceholds.
AQ =Q T +β q e ,
j j j j+1 j+1 ⊤j
Q AQ =T .
⊤j j j
BasedonPropositionB.4,theLanczosprocessisillustratedinFigure7
A Q = Q + β q e
j j j+1 j+1 ⊤j
T
j
Figure7. Classic3-termrecurrence
C.DynamicLanczosSubroutine
ThissectionliststheDLanczossubroutine(Algorithm4)invokedintheLancBiOframework(Algorithm2). Oneofthe
maindifferencesbetweenAlgorithm3andAlgorithm4isthatAlgorithm3representstheentirem-stepLanczosprocess,
whileAlgorithm4servesasaone-stepsubroutine. Specifically,LancBiOinvokesDLanczosonceineachouteriteration
(line 11 in Algorithm 2), expanding both T and Q by one dimension. Consequently, the inputs of Algorithm 4 are not
indexedtoavoidconfusion,withtheircorrespondingvariables(T ,Q ,A ,β )inAlgorithm2evolvingacrossouter
k 1 k 1 k k
iterationsindexedbyk. Anotherdifferenceliesinthedynamicpro−pertyo−ftheDLanczossubroutine,i.e.,thematrixA
k
passedduringeachinvocationinAlgorithm2varies,whiletheclassicLanczosprocess(Algorithm3)employsastaticA.
D.ExtendingLancBiOtoNon-convexLower-levelProblem
The Lanczos process is known for its efficiency of constructing Krylov subspaces and is capable of solving indefinite
linear systems (Greenbaum et al., 1999). In this section, we will briefly demonstrate that the dynamic Lanczos-aided
BilevelOptimizationframework,LancBiO,canalsohandlelower-levelproblemswiththeindefiniteHessian.
14LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Algorithm4DynamicLanczossubroutineforLancBiO(DLanczos)
Input: tridiagonalmatrixT,basismatrixQwithj columns,HessianmatrixA,andβ
1: if j =1then
2: [q ]=Q, q =0
1 1
3: else −
4: [q ,q ,...,q ]=Q
1 2 j
5: endif
6: u =Aq βq
j j j 1
− −
7: α =q u
j j⊤ j
8: ω =u α q
j j j j
−
9: β = ω
j+1 j
∥ ∥
10: q =ω /β
j+1 j j+1
11: Q =[Qq ]
j+1 j+1
T
12: T j+1 = β 
 β α 
 j
Output: T ,Q ,β 
j+1 j+1 j+1
SupposeAisinvertible,andconsidersolvingastandardlinearsystem
Ax=b,
withinitialponitx ,initialresidualr =b Ax andinitialerrore =A 1b x . IfthematrixAispositive-definite,the
0 0 0 0 − 0
− −
classicLanczosalgorithmisequivalenttotheConjugateGradient(CG)algorithm(Hestenesetal.,1952), bothofwhich
minimizetheA-normoftheerrorinanaffinespace(Greenbaum,1997;Meurant&Strakosˇ,2006),i.e.,atthem-thstep,
x = argmin A 1b x .
m − − A
x ∈x0+ Km(A,b)
(cid:13) (cid:13)
(cid:13) (cid:13)
If the matrix A is not positive-definite, MINRES (Paige & Saunders, 1975) is the algorithm recognized to minimize the
2-normoftheresidualinanaffinespace(Greenbaum,1997;Meurant&Strakosˇ,2006),i.e.,atthem-thstep,
x = argmin b Ax . (13)
m
∥ − ∥
x ∈x0+ Km(A,b)
Additionally,basedonQ asthebasisoftheKrylovsubspace (A,b),T astheprojectionofAonto (A,b),and
m m m m
K K
the3-termrecurrence
AQ =Q T +β q e ,
m m m m+1 m+1 ⊤m
wecanrewrite(13)as
x =x +Q c ,
m 0 m m
with
c =argmin r AQ c
m 0 m
∥ − ∥
c
=argmin r Q T c
0 m+1 m+1,m
∥ − ∥
c
=argmin Q ( r e T c)
m+1 0 1 m+1,m
∥ ∥ ∥ − ∥
c
=argmin r e T c ,
0 1 m+1,m
∥∥ ∥ − ∥
c
where
T
T := m .
m+1,m β e
(cid:20)
m+1 ⊤m
(cid:21)
15LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
InthespiritofMINRES,toaddressthebilevelproblemwherethelower-levelproblemexhibitsanindefiniteHessian,the
frameworkLancBiO(Algorithm2)requiresonlyaminormodification. Specifically,line13inAlgorithm2,,whichsolves
asmall-sizetridiagonallinearsystem,willbereplacedbysolvingalow-dimensionalleastsquaresproblem
c =argmin r e T c 2,
k k 1 k+1,k
∥∥ ∥ − ∥
c
andcomputingthecorrection
∆v =Q c ,
k k k
where
T
T := k .
k+1,k β e
(cid:20)
k+1 ⊤k
(cid:21)
E.ProofofSmoothnessofy∗ andφ
To ensure completeness, in this subsection, we provide detailed proofs for the preliminary lemmas that characterize the
smoothnessofthelowerlevelsolutiony andthehyper-objectiveφ.
∗
LemmaE.1. UndertheAssumptions3.2and3.3,y ∗(x)is L µg gx-Lipschitzcontinuous,i.e.,foranyx 1,x
2
∈Rdx,
L
gx
y (x ) y (x ) x x .
∗ 1 ∗ 2 1 2
∥ − ∥≤ µ ∥ − ∥
g
Proof. Theassunptionthat g(x,y)isL -Lipschitzreveals 2 g(x,y) L . Then
∇x gx ∇xy ≤ gx
(cid:13) (cid:13)
∥∇y ∗(x) ∥= ∇2 xyg(x,y) ∇2 yyg(x,y) −1 ≤(cid:13) ∇2 xyg(x,y(cid:13) ) ∇2 yyg(x,y) −1
≤
L µgx ,
g
sinceg(x, )isµ -stronglyco(cid:13) (cid:13) (cid:13)nvex. (cid:2) (cid:3) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13)(cid:2) (cid:3) (cid:13) (cid:13) (cid:13)
g
·
LemmaE.2. UndertheAssumptions3.1,3.2and3.3,thehyper-gradient φ(x)isL -Lipschitzcontinuous,i.e.,forany
φ
x 1,x
2
Rdx, ∇
∈
φ(x ) φ(x ) L x x ,
1 2 φ 1 2
∥∇ −∇ ∥≤ ∥ − ∥
whereL = 1+ Lgy L + LgxLfy+LgxyCfy + LgxCfyLgyy .
φ µg fx µg µ2
g
(cid:16) (cid:17)(cid:16) (cid:17)
Proof. Bycombining
(A ∗1)−1b ∗1−(A ∗2)−1b
∗2
=(A ∗1)−1b ∗1−(A ∗1)−1b ∗2+(A ∗1)−1b ∗2−(A ∗2)−1b
∗2
=(A ∗1)−1(b ∗1−b ∗2)+(A ∗1)−1(A ∗2−A ∗1)(A ∗2)−1b
∗2
withthepropertiesrevealedbyAssumptions3.13.2and3.3,wecanderive
L L C L L
(A ∗1)−1b
1
−(A ∗2)−1b
2
≤
µfy 1+ µgy ∥x
1
−x
2
∥+ fy µ2gyy 1+ µgy ∥x
1
−x
2
∥.
(cid:13) (cid:13) g (cid:18) g (cid:19) g (cid:18) g (cid:19)
(cid:13) (cid:13)
Inasimilarw(cid:13)ay,wecanderive (cid:13)
φ(x ) φ(x )=( f(x ,y (x )) f(x ,y (x )))
1 2 x 1 ∗ 1 x 2 ∗ 2
∇ −∇ ∇ −∇
−∇2 xyg(x 1,y ∗(x 1))(A ∗1)−1b ∗1+ ∇2 xyg(x 1,y ∗(x 1))(A ∗2)−1b
∗2
+ ∇2 xyg(x 2,y ∗(x 2))(A ∗2)−1b ∗2−∇2 xyg(x 1,y ∗(x 1))(A ∗2)−1b ∗2.
16LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Itfollowsthat
L
gy
φ(x ) φ(x ) L 1+ x x
1 2 fx 1 2
∥∇ −∇ ∥≤ µ ∥ − ∥
(cid:18) g (cid:19)
L L C L
gy fy fy gyy
+L 1+ + x x
gx µ µ µ2 ∥ 1 − 2 ∥
(cid:18) g (cid:19)(cid:18) g g (cid:19)
C L
fy gy
+L 1+ x x
gxy 1 2
µ µ ∥ − ∥
g (cid:18) g (cid:19)
L x x ,
φ 1 2
≤ ∥ − ∥
whereL = 1+ Lgy L + LgxLfy+LgxyCfy + LgxCfyLgyy .
φ µg fx µg µ2
g
(cid:16) (cid:17)(cid:16) (cid:17)
F.PropertiesofDynamicSubspaceinSection3.1
Inthissection,wefocusonthepropertiesofthebasismatrixQandthetridiagonalmatrixT constructedwithineachepoch
ofthedynamicLanczosprocess. Denote
A = 2 g(x ,y ) andb = f(x ,y ).
∗k ∇yy k k∗ ∗k ∇y k k∗
Anepochisconstitutedofacompletem-stepdynamicLanczosprocessbetweentworestarts,namely,afterhepochs,the
numberofouteriterationsismh. Giventheouteriterationsk =mh+j forj =1,2,...,m,wedenote
(cid:18) (cid:19)
L
εh := 1+ gx x x + y y∗
st µ g ∥ mh+s − mh+t ∥ ∥ mh+s − mh+s∥
fors,t=1,2,...,mand
εh := max εh,
j st
1 s,t j
≤ ≤
servingastheaccumulativedifference.Forbrevity,weomitthesuperscriptwherethereisnoambiguity,andweareslightly
abusingofnotationthatatthecurrentepoch, A and b aresimplifiedby A and b forj = 1,...,m.
mh+j mh+j j j
Inaddition,theapproximationsintheresidual{ system(} 9)are{ simplifi} edbyv¯and¯b:=b{ A}
v¯.
{ }
1 1
−
WerewritedynamicupdaterulefromSection2.2
u =A q β q , (14)
j j j j j 1
− −
α =q u , (15)
j j⊤ j
ω =u α q , (16)
j j j j
−
β = ω , (17)
j+1 j
∥ ∥
q =ω /β , (18)
j+1 j j+1
forj =1,2,...,mwithq =0,β =0andQ =q =¯b/ ¯b .Thefollowingpropositioncharacterizesthatthedynamic
0 1 1 1
subspaceconstructedinAlgorithm2withinanepochisindeedanapproximateKrylovsubspace.
(cid:13) (cid:13)
Proposition F.1. At the j-th step within an epoch (j = 1(cid:13) ,2(cid:13) ,...,m 1), the subspace spanned by the matrix Q
j+1
−
inAlgorithm2satisfies
span(Q ) span Aa1Aa2 Aaj¯b a s =0or1 . (19)
j+1 ⊆ 1 2 ··· j s=1,2,...,j
(cid:26) (cid:12) ∀ (cid:27)
(cid:12)
Specifically,whenA
1
=A
2
= =A
j
=AandQ j+1isoffullrank,(cid:12)
··· (cid:12)
span(Q )= A,¯b
j+1 j+1
K
(cid:0) (cid:1)
Proof. Note that Q = span q with q = ¯b satisfies (19). We will give a proof by induction. Suppose for i =
1 { 1 } 1 ¯b
1,2,...,j, ∥ ∥
a =0or1
span(Q ) span Aa1Aa2 Aai¯b s .
i+1 ⊆ 1 2 ··· i s=1,2,...,i
(cid:26) (cid:12) ∀ (cid:27)
(cid:12)
(cid:12)
17 (cid:12)LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
BythedynamicLanczosprocess,ityields
1
q = (A q β q α q ). (20)
j+2 j+1 j+1 j+1 j j+1 j+1
β − −
j+2
Since
q span Aa1Aa2 Aaj¯b a s =0or a s =1 ,
j+1 ∈ 1 2 ··· j s=1,2,...,j
(cid:26) (cid:12) ∀ (cid:27)
(cid:12)
thenwehave (cid:12)
A q span Aa1Aa2
Aaj+1(cid:12)
¯b
a
s
=0or a
s
=1
.
j+1 j+1 ∈ 1 2 ··· j+1 s=1,2,...,j+1
(cid:26) (cid:12) ∀ (cid:27)
(cid:12)
Itfollowsfrom(20)that (cid:12)
q span Aa1Aa2 Aaj+1¯b (cid:12) a s =0or a s =1 .
j+2 ∈ 1 2 ··· j+1 s=1,2,...,j+1
(cid:26) (cid:12) ∀ (cid:27)
(cid:12)
Byinduction,wecompletetheproof. (cid:12)
(cid:12)
Althoughwecanestimatethedifferencebetweenthebasisoftheabovetwosubspaces,itisnotedthattheKrylovsubspaces
can be very sensitive to small perturbation (Meurant & Strakosˇ, 2006). Therefore, a more detailed observation of the
dynamic Lanczos process is necessary, together with the development of relevant lemmas adapted from existing work
(Paige,1976;1980;Greenbaum,1997)tosupportouranalysis.
LemmaF.2. SupposeAssumptions3.1to3.3hold. ThedynamicLanczosprocessinAlgorithm2withnormalizedq and
1
α ,β ,q satisfies
j j j
A Q =Q T +β q e +δQ (21)
∗j j j j j+1 j+1 ⊤j j
forj =1,2,...,m,whereQ =[q ,q ,...,q ],δQ =[δq ,δq ,...,δq ],
j 1 2 j j 1 2 j
α β
1 2
β α β
2 2 3
 
T = β
... ...
.
j  3 


... ...
β


 j 
 β α 
 j j 
 
ThecolumnsoftheperturbationδQ satisfy
j
δq L ε , fori=1,2,...,j.
i gyy j
∥ ∥≤
Additionally,ifwedecomposeQ as
j
Q Q =R +R , (22)
⊤j j j⊤ j
withR asastrictlyuppertriangularmatrix,then
j
T R R T =β Q q e +δR , (23)
j j
−
j j j+1 ⊤j j+1 ⊤j j
whereδR isstrictlyuppertriangularwithelements ζ 2L ε ,for1 s<t j.
j st gyy j
| |≤ ≤ ≤
Proof. From
α =q u =q A q β q q
j j⊤ j j⊤ j j
−
j j⊤ j −1
and
1 1 1
q j⊤+1q
j
=
β
j+1ω j⊤q
j
=
β
j+1
(u
j
−α jq j)⊤q
j
=
β
j+1
(A jq
j
−α jq
j
−β jq
j
−1)⊤q j,
wecanderive
q q =0 (24)
j⊤+1 j
byinduction. Then,wecombineequations(14),(15),(16)and(18),andrewritethemintheperturbedform:
β q =A q β q α q =A q β q α q +δq , fori=1,2,...,j, (25)
i+1 i+1 i i
−
i i −1
−
i i ∗j i
−
i i −1
−
i i i
18LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
where δq L ε duetoAssumpstions3.2and3.3. Specifically,(25)canberewritteninacompactform:
i gyy j
∥ ∥≤
A Q =Q T +β q e +δQ .
∗j j j j j+1 j+1 ⊤j j
Then,wewillconsidertheorthogonalityofmatrixQ ,whichisreflectedbyR in(22). MultiplyEquation(21)ontheleft
j j
byQ ,
⊤j
Q A Q =Q Q T +β Q q e +Q δQ .
⊤j ∗j j ⊤j j j j+1 ⊤j j+1 ⊤j ⊤j j
Combineitssymmetrywiththedecomposition(22),then
T R +R R +R T =β Q q e e q Q +Q δQ δQ Q . (26)
j j⊤ j
−
j⊤ j j j+1 ⊤j j+1 ⊤j
−
j j⊤+1 j ⊤j j
−
⊤j j
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
DenoteM =T R R T whichisuppertriangular. Sincetheconsecutiveq isorthogonalasrevealedby(24),wecan
j j j j j i
−
concludethatthediagnoalelementsofM are0. Furthermore,byextractingtheuppertriangularpartoftherighthandside
j
of(26),wecanget
M =T R R T =β Q q e +δR ,
j j j
−
j j j+1 ⊤j j+1 ⊤j j
whereδR isstrictlyuppertriangularwithelementsζ satisfying: fort=2,3,...,j,
j st
ζ =q δq δq q
t ζ−1,t =qt⊤ −δ1
q
t −
δq
t⊤ q−,1 t
s=1,2,...,t 2.
(cid:26)
st s⊤ t
−
s⊤ t
−
Fromtheboundof δq itfollowsthatfor1 s<t j, ζ 2L ε .
j st gyy j
∥ ∥ ≤ ≤ | |≤
LemmaF.2illustratestheinfluenceofthedynamicsinAlgorithm2imposedonthestandard3-termLanczosrecurrence,
andas(22)reveals,RcharacterizesthelossoforthogonalityofthebasisQ. However,thefollowinglemmasdemonstrate
thattherangeofeigenvaluesoftheapproximateprojectionmatrixT iscontrollable.
Tobeginwith,weestablishtheRitzpairsofT as µ(j),y(j) fori=1,2,...,j,suchthat
j i i
(cid:16) (cid:17)
T Y(j) =Y(j)diag µ(j),µ(j),...,µ(j) .
j 1 2 j
(cid:16) (cid:17)
wherethenormalized y(j) j formtheorthogonalmatrixY(j) withtheelementsς(j) for1 s,t j,andwearrange
{ i }i=1 st ≤ ≤
theRitzvaluesinaspecificorder,
µ(j) >µ(j) > >µ(j).
1 2 ··· j
Wedefinethej-thapproximateeigenvectormatrix
Z(j) := z(j),z(j),...,z(j) :=Q Y(j).
1 2 j j
(cid:104) (cid:105)
andthecorrespondingRayleighquotientsofA
∗j
z(j) ⊤A z(j)
ν(j) := i ∗j i , fori=1,2,...,j.
i (cid:16) (cid:17)
z(j) ⊤z(j)
i i
(cid:16) (cid:17)
The subsequent lemma presents a description for the difference of eigenvalues between T and the tridiagonal matrix in
j
theprecedingsteps.
LemmaF.3. SupposeAssumptions3.1to3.3hold. Foranyeigenpair µ(j),y(j) ofT ,thereexistsanintegerpair(s,n)
i i j
where1 s n<j,suchthat (cid:16) (cid:17)
≤ ≤
2j2L ε
µ(j) µ(n) gyy j . (27)
i − s ≤
(cid:12) (cid:12) √3 y i(j) ⊤R jy i(j)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (cid:17) (cid:12) (cid:12)
(cid:12) (cid:12)
19(cid:12) (cid:12)LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Proof. Multiplytheeigenvectory(i)ofT byT ,wherei<j,
r i j
T
y r(i)
=
µ r(i)y r(i)
. (28)
j (cid:20) 0 (cid:21) (cid:34) β r+1ς i( ri)e 1 (cid:35)
Thenmultiply y(j) ⊤onthebothsidesof(28),
s
(cid:16) (cid:17)
µ(j) µ(i) y(j) ⊤ y r(i) =β ς(i)ς(j) , (29)
s − r s 0 i+1 ir i+1,s
(cid:16) (cid:17)(cid:16) (cid:17) (cid:20) (cid:21)
andmultiplytheeigenvectors y(j) ⊤andy(j)ontheleftandrightof(23),respectively,
s t
(cid:16) (cid:17)
µ(j) µ(j) y(j) ⊤R y(j) =ς(j)β z(j) ⊤q +ϵ(j), (30)
s − t s j t jt j+1 s j+1 st
(cid:16) (cid:17)(cid:16) (cid:17) (cid:16) (cid:17)
wherewedifine
ϵ(j) := y(j) ⊤δR y(j). (31)
st s j t
(cid:16) (cid:17)
Notethat ϵ(j) 2jL ε becauseofF.2. Specifically,bytakings=tin(30),
st ≤ gyy j
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) ϵ(j)
z(j) ⊤q = ss , (32)
s j+1 −β ς(j)
(cid:16) (cid:17) j+1 js
andwecanrewrite(32)inthematrixform
Q q =Y(r)c , (33)
⊤r r+1 r
whereforr =1,...,s,
ϵ(r)
ss
e c := .
⊤s r −β ς(j)
r+1 rs
ByobservingthatQ q =Y(r)c isthe(r+1)-thcolumnofR ,wecanderive
⊤r r+1 r j
y(j) ⊤R y(j) = j −1 ς(j) r ϵ( ttr) y(j) ⊤ y t(r) (34)
i j i − r+1,i β ς(r) i 0
(cid:16) (cid:17) (cid:88)r=1 (cid:88)t=1 r+1 rt (cid:16) (cid:17) (cid:20) (cid:21)
=
j −1
ς(j)
2 r ϵ( ttr)
, (35)
− r+1,i µ(j) µ(r)
(cid:88)r=1(cid:16) (cid:17) (cid:88)t=1 i − t
where(34)and(35)followfrom(33)and(29)respectively. Consequently,thedefinition(31)reveals
j
2
ϵ(j) = δR 2 2j2L ε . (36)
st ∥ j ∥F ≤ gyy j
s (cid:88),t=1(cid:16) (cid:17)
Basedon(35)andtheorthogonalityofY(j),
y(j) ⊤R y(j)
j −1
ς(j)
r ϵ( ttr)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:16) i (cid:17) j i (cid:12) (cid:12) (cid:12) (cid:12)≤(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)(cid:88)r=1 r+1,i 1(cid:88)t=1 1 ≤m d ≤in l<j j(cid:12)
(cid:12)
(cid:12)−µ 1( ij) r− ςµ (j( d )l) (cid:12)
(cid:12)
(cid:12)
ϵ(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)(r) ,
≤ min µ(j) µ(l) (cid:12) r+1,i tt (cid:12)
1 ≤d ≤l<j
(cid:12)
i − d (cid:12)(cid:12) (cid:12) (cid:12)(cid:88)r=1 (cid:88)t=1 (cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12)(cid:12) (cid:12)
(cid:12) (cid:12)
20LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
anditfollowsfromtheCauchy–Schwarzinequalityand(36),
2
min µ(j) µ(l)
√j
(cid:114)
j r− =1 1 r t=1 ϵ( ttr) √j j r− =1 1r2(2L gyyε j)2 2j2L gyyε
j .
1 ≤d ≤l<j (cid:12) i − d (cid:12)≤ (cid:80) y i(j) ⊤(cid:80) R jy i((cid:16) j) (cid:17) ≤ (cid:113) (cid:80)y i(j) ⊤R jy i(j) ≤ √3 y i(j) ⊤R jy i(j)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (cid:17) (cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (cid:17) (cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (cid:17) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
Lemma F.4. Suppose Assumptions 3.1 to 3.3 hold. Q and T are the basis matrix and the approximate tridiagonal
j j
matrixinthej-thstepandR isthestrictlyuppertriangularmatrixdefinedin(22)characterizingtheorthogonalityofQ .
j j
Supposeµ(j)isthei-theigenvalueofT ,thenfori=1,...,j,
i j
2√3 2√3
µ (j+1)3L ε µ(j) L + (j+1)3L ε . (37)
g − 3 gyy j ≤ i ≤ gy 3 gyy j
Proof. Byconductingtheleftmultiplicationwith y(j)Q ⊤andtherightmultiplicationwithy(j)onbothsidesofequa-
i j i
tion(21),weobtainthefollowing. (cid:16) (cid:17)
z(j) ⊤A z(j) µ(j) z(j) ⊤z(j) = ϵ(j)+ z(j) ⊤δQ y(j).
i ∗j i − i i i − ii i j i
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
Divideby z(j) ⊤z(j),
i i
(cid:16) (cid:17)
(cid:12) (cid:12)
(cid:12)(cid:16)z i(j) (cid:17)⊤A ∗jz
i
z(j i()
j)−
⊤µ(
i
zj i()
j(cid:16)
)z i(j) (cid:17)⊤z i(j)
(cid:12) (cid:12) (cid:12)≤ (cid:12) (cid:12)
(cid:12)ϵ( iij 1)
+(cid:12) (cid:12)
(cid:12)+
2(cid:12) (cid:12) (cid:12) (cid:12)(cid:16)
yz i(i( jj ))
(cid:17)
⊤⊤ RδQ jyj i(y j)i(j)
(cid:12) (cid:12) (cid:12)
(cid:12)
≤ (cid:12) (cid:12)
(cid:12)ϵ( iij)
(cid:12) (cid:12)
(cid:12)+L gyy 1ε +j
(cid:115)
2j
(cid:18) y i((cid:12) (cid:12) (cid:12)
(cid:12)j1 )+ ⊤2
R(cid:16)
jy yi( i(j j) )(cid:17)⊤R jy i(j)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:19),
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:16) (cid:17) (cid:12) (cid:12) (cid:12) (cid:12) (cid:16) (cid:17) (cid:12) (cid:12) (cid:12) (cid:12) (cid:16) (cid:17) (cid:12) (cid:12) (38)
s(cid:12)ince (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2
z(j) =1+2 y(j) ⊤R y(j). (39)
i i j i
(cid:13) (cid:13) (cid:16) (cid:17)
(cid:13) (cid:13)
CaseI:If (cid:13) (cid:13)
3 ε
y(j) ⊤R y(j) < j (40)
i j i 8 − 2
(cid:16) (cid:17)
holds,thenfrom(39),
1
z(j) .
i ≥ 2
(cid:13) (cid:13)
Furthermore,(38)revealsthatthereexistsaRayleighq(cid:13)uotie(cid:13)ntν(j)ofA thatsatisfies
(cid:13) (cid:13) ∗j
ν(j) µ(j) 2 ϵ(j) + 2jL ε 4j+ 2j L ε .
− i ≤ ii gyy j ≤ gyy j
(cid:12) (cid:12) (cid:12) (cid:12) (cid:112) (cid:16) (cid:112) (cid:17)
(cid:12) (cid:12) (cid:12) (cid:12)
CaseII:Ifthecondition(40)(cid:12)doesnotho(cid:12)ld,by(cid:12)app(cid:12)lyingLemmaF.3,wecanfindanintegerpair(s ,n )with1 s
1 1 1
≤ ≤
n <j suchthat
1
2j2L ε
µ(j) µ(n1) gyy j 2√3j2L ε .
i − s1 ≤ ≤ gyy j
(cid:12) (cid:12) √3 y i(j) ⊤R jy i(j)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)(cid:16) (cid:17) (cid:12) (cid:12)
Byobservingthat µ( sn 11),y s(n 11) and y s(n 11) ⊤R n1y s((cid:12) (cid:12)n 11)canalsobeca(cid:12) (cid:12)tegorizedintoCaseIorCaseII,repeatthisprocess
andconstructaseq(cid:16)uence (s ,n(cid:17)) l+(cid:16)1 with(cid:17)1 n <n < <n <n =j until
{ t t }t=0 ≤ l+1 l ··· 1 0
3 ε
y(nl+1) ⊤R y(nl+1) < nl+1.
sl+1 j sl+1 8 − 2
(cid:16) (cid:17)
21LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Inequality(40)holdswhenthesuperscriptj = 1sinceT = [α ], y(1) = 1andz(1) = q . Therefore, wewillbuildup
1 1 1 1 1
(s ,n ) l+1 infinitesteps,resultinginthefollowingestimation.
{ t t }t=0
l
ν¯(j) µ(j) ν¯(j) µ(nl+1) + µ(nt+1) µ(nl)
− i ≤ − sl+1 st+1 − st
(cid:12) (cid:12) (cid:12) (cid:12) (cid:88)t=0(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) l (cid:12)
4j+ 2j L ε + 2√3n2L ε
≤ gyy j t gyy j
(cid:16) (cid:112) (cid:17) (cid:88)t=0
2√3
(j+1)3L ε ,
gyy j
≤ 3
forsomeν¯(j).
AnyRayleighquotientofA isboundedbyitseigenvalues(Parlett,1998),i.e.,foranyν(j),
∗j
λ(j) ν(j) λ(j) ,
min ≤ ≤ max
where λ(j) and λ(j) are the minimum and maximal eigenvalue of A , respectively. Based on Assumption 3.2 and
min max ∗j
Assumption3.3,wethuscompletetheproof.
G.ProofofLemma3.12
G.1.Proofsketch
TheproofofLemma3.12isstructuredbyfourprincipalsteps.
Step1: extendingε toε˜ withinthelemmasdetailedinAppendixF.
j j
InAppendixF,weadoptA andb asreferencevalueswitheachepochfortheanalysis,i.e.,forj =1,2,...,m,
∗j ∗j
A := 2 g x ,y , b := f x ,y , ¯b:=b A v¯, r¯ :=¯b, r¯ :=¯b A ∆v .
∗j ∇yy j j∗ ∗j ∇y j j∗ 1 − 1 0 j − ∗j j
and (cid:0) (cid:1) (cid:0) (cid:1)
L
gx
ε := max 1+ x x + y y .
j
1 ≤s,t ≤j (cid:18) µ g (cid:19)∥
mh+s
−
mh+t
∥
mh+s
−
m∗h+s
(cid:13) (cid:13)
NotethatwecanalsoviewtheA ,b asreferencevalues. Inthisway,wed(cid:13)enotethesimilarq(cid:13)uantities
1 1
A := 2 g(x ,y ), b := f(x ,y ), ¯b :=b A v¯, r¯ :=¯b, r¯ :=¯b A ∆v . (41)
j ∇yy j j j ∇y j j ′ 1 − 1 0′ ′ j′ ′ − 1 j
and
ε˜ := max x x + y y . (42)
j mh+s mh+t mh+s mh+t
1 s,t j∥ − ∥ ∥ − ∥
≤ ≤
Consequently,weextendthelemmasinAppendixF.TheresultslistedinAppendixG.2aretherecipeofStep2.
Step2: upper-boundingtheresidual r¯ .
j′
InAppendixG.3,wethendemonstratethatifthevalueofε˜ isnottoolarge, r¯ canbebounded,whichisanimportant
(cid:13) (cid:13) j j′
lemmaforStep3. (cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
Step3: controllingε˜ andε byinduction.
j j
Sincetheexpressionofε˜ doesnotinvolvey ,itsmagnitudecanbecontrolledbyadjustingthestepsize,impliedby(42).
j ∗
Withthehelpofthestabilityofε˜ ,wecanprove
j
2√3
µ (j+1)3L ε >0. (43)
g gyy j
− 3
22LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Step4: proofofLemma3.12
Basedontheconclusion(43)revealingthebenignpropertyofthedynamicprocess,weachievetheproofofLemma3.12.
G.2.ExtendedlemmasfromAppendixF
Inthispart,weviewtheA ,b asreferencevalues. Inthisway,wecaninstituteA withA andε withε˜ inthelemmas
1 1 ∗j 1 j j
fromAppendixF,resultingintheextendedversionoflemmas.
Lemma G.1. (Extended version of Lemma F.2) Suppose Assumptions 3.1 to 3.3 hold. The dynamic Lanczos process in
Algorithm2withnormalizedq andα ,β ,q satisfies
1 j j j
A Q =Q T +β q e +δQ (44)
1 j j j j+1 j+1 ⊤j ′j
forj =1,2,...,m,whereQ =[q ,q ,...,q ],δQ = δq ,δq ,...,δq ,
j 1 2 j ′j 1′ 2′ j′
(cid:2) (cid:3)
α β
1 2
β α β
2 2 3
 
T = β
... ...
.
j  3 


... ...
β


 j 
 β α 
 j j 
 
ThecolumnsoftheperturbationδQ satisfy
′j
δq L ε˜ , fori=1,2,...,j. (45)
∥
i′
∥≤
gyy j
LemmaG.2. (ExtendedversionofLemmaF.4)SupposeAssumptions3.1to3.3hold. Q andT arethebasismatrixand
j j
theapproximatetridiagonalmatrixinthej-thstep. Takeµ(j)asthei-theigenvalueofT ,thenfori=1,...,j,
i j
2√3 2√3
µ (j+1)3L ε˜ µ(j) L + (j+1)3L ε˜ . (46)
g − 3 gyy j ≤ i ≤ gy 3 gyy j
G.3.ProofofStep2
Thefollowinglemmademonstratesthatifthevalueofε˜ isnottoolarge, r¯ canbebounded.
j j′
LemmaG.3. SupposeAssumpsions3.1to3.3and
(cid:13) (cid:13)
(cid:13) (cid:13)
2√3
µ (j+1)3L ε˜ >0
g gyy j
− 3
aresatisfiedwithinanepoch. Then,itholdsthat
j
(cid:13)
(cid:13)∥rr ¯¯ 0j ′′
(cid:13)
(cid:13)∥
≤2 (cid:112)κ˜ ′(j) (cid:32)(cid:112)κ κ˜ ˜′ ′( (j j) )+− 11
(cid:33)
+ (cid:112)jL gyyε jκ˜ ′(j),
(cid:112)
whereκ˜ (j):=
Lgy+2√ 33(j+1)3Lgyyε˜j.
′ µg−2√ 33(j+1)3Lgyyε˜j
Proof. Denotethesolutioninthedynamicsubspaceinthej-thstepby
∆ξ
j
=(T j)−1¯b= ∥r¯
0′
∥(T j)−1e 1, (47)
whereT isnonsingularbecauseofLemmaG.2. By(41),(44),and(47),
j
r¯ =¯b A Q ∆ξ = β q e ∆ξ δQ ∆ξ .
j′
−
1 j j
−
j+1 j+1 ⊤j j
−
′j j
Itfollowsthat
r¯
r¯j′
≤
δQ
′j
(T j)−1 + β j+1e
⊤j
(T j)−1e
1
. (48)
(cid:13)
(cid:13)∥
0′(cid:13)
(cid:13)∥ (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)3 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Thefirsttermontherightsideof(48)canbeboundedby(45)and(46):
L
δQ
′j
(T j)−1
≤
jL gyyε˜
j µ
2√3(jg +y
1)3L ε˜ ≤
jL gyyε˜ jκ˜ ′(j). (49)
Recallthat (cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:112) g − 3 gyy j (cid:112)
T
T := j , (50)
j+1,j β e
(cid:20)
j+1 ⊤j
(cid:21)
and that for any symmetric tridiagonal matrix T, where the upper left (j + 1) j block is T , the application of
j+1,j
×
theclassicLanczosalgorithmtoT, startingwiththeinitialvectore willresultinthematrixT atthej-thstep. To
1 j+1,j
constructasuitable(j+1) (j+1)symmetrictridiagonalmatrixT,weconsideravirtualstepwithλ =θ =0,which
⋄ ⋄
×
leadsto(x ,y )=(x ,y ),ε˜ =ε˜ ,and
⋄j+1 j⋄+1 j j ⋄j+1 j
T =T j+1,j 
β
 j+1
 


α˜ j⋄+1

 
ByLemmaG.2,givenanyeigenvalueµofT
2√3 2√3
µ (j+1)3L ε˜ µ L + (j+1)3L ε˜ .
g gyy j gy gyy j
− 3 ≤ ≤ 3
Inthisway, β j+1e
⊤j
(T j)−1e
1
canbeseenastheresidualinthej-thstepoftheclassicLanczosprocesswiththepositive-
definitematr(cid:12)ixT andtheiniitia(cid:12)lvectore 1.SincetheeigenvaluesofT satisfy(50),itfollowsfromthestandardconvergence
(cid:12) (cid:12)
propertyoft(cid:12)heLanczosproces(cid:12)s(Greenbaum,1997)that
j
κ˜ (j) 1
β j+1e ⊤j (T j)−1e 1 ≤2 κ˜ ′(j) (cid:32)(cid:112)κ˜′ ′(j)+−
1(cid:33)
,
(cid:12) (cid:12) (cid:112)
(cid:12) (cid:12)
whichcompletestheproof. (cid:12) (cid:12) (cid:112)
G.4.Proofdetailsofµ 2√3(j+1)3L ε >0
g − 3 gyy j
Inthispart,wewillgivethedetailedproofofµ 2√3(j+1)3L ε >0. Atthesametime,wedemonstratethat,with
g − 3 gyy j
appropriatestepsizes,theauxiliaryvariablev isbounded,therebyensuringthatthehyper-gradientestimator(5)remains
k
bounded,whichensuresthestablebehaviorofthedynamicLanczosprocess,
LemmaG.4. SupposeAssumptions3.1, 3.2, 3.3, 3.6, 3.9, 3.10and3.11hold. Ifwithineachepoch, wesetthestepsize
θ ( 1 )aconstantforyandthestepsizeforxaszerointhefirstm (1)steps,andtheothersasanappropriate
∼O m4 0 ∼O
constantλ ( 1 ),thenforanyepoch,
∼O m4
2√3
µ (j+1)3L ε >0, forj =1,2,...,m+1,
g gyy j
− 3
andthereexistsaconstantC >0sothat v C forv generatedbyAlgorithm2.
v k v k
∥ ∥≤
Proof. Considertheiterateswithinoneepochandtheconstants
L +ε˜
0<ε˜<µ andκ˜ := gy .
g
µ ε˜
g
−
ItfollowsfromLemmaG.3thatifε˜ √3ε˜ ,then
j ≤ 2(m+1)3Lgyy
j
b A v √κ˜ 1 √3ε˜
∥ 1 − 1 j ∥ 2√κ˜ − + κ˜ 3√κ˜. (51)
b
1
A 1v¯ ≤ (cid:32)√κ˜+1(cid:33) 2(m+1)2 ≤
∥ − ∥
24LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Then,wewillprovebyinduction. Atthebeginningofthealgorithm,thecondition(51)reveals
√3µ
g
ε = y y .
1 ∥ 1 − 1∗ ∥≤ 8(m+1)3L
gyy
Combining it with b A v¯ C constructs the start of induction within an epoch and induction between epochs.
1 1 0 r
∥ − ∥ ≤
Withinanepoch,supposethefollowingstatementsholdfori=1,2,...,j,
b A v 3√κ˜C ,
1 1 i r
∥ − ∥≤
1
v 3√κ˜C +C ,
i r fx
∥ ∥≤ µ
g
(cid:16) (cid:17)
1
φ C + 3√κ˜C +C ,
∇ i ≤ fx µ2 r fx
g
(cid:13) (cid:13) (cid:16) (cid:17)
(cid:13)(cid:101) (cid:13) √3ε˜
(cid:13) ε˜(cid:13) ,
i ≤ 2(m+1)3L
gyy
√3µ
g
ε .
i ≤ 4(m+1)3L
gyy
Thenbysettingthestepsize
√3ε˜ 1 −1
λ C + 3√κ˜C +C ,
≤ 4(m+1)4L fx µ2 r fx
gyy (cid:18) g (cid:16) (cid:17)(cid:19)
√3ε˜
θ C 1,
≤ 4(m+1)4L
g−y
gyy
wecanget
i=j i=j √3ε˜
ε˜ λ φ +θ g(x ,y ) . (52)
j+1 ≤ ∇ i ∥∇y i+1 i ∥≤ 2(m+1)3L
gyy
(cid:88)i=1(cid:13) (cid:13) (cid:88)i=1
Itfollowsfrom(51)that (cid:13) (cid:13)(cid:101) (cid:13) (cid:13)
b A v 3√κ˜C , (53)
1 1 j+1 r
∥ − ∥≤
1
∥v
j+1
∥= A−11(b
1
−A 1v
j+1
−b 1)
≤ µ
3√κ˜C r+C
fx
, (54)
g
(cid:13) 1 (cid:13) (cid:16) (cid:17)
φ (cid:13)C + 3√κ˜C +C (cid:13) . (55)
∇ j+1 ≤ fx µ2 r fx
g
(cid:13) (cid:13) (cid:16) (cid:17)
(cid:13)(cid:101) (cid:13)
(cid:13) (cid:13)
Additionally,thedescentpropertyof y y andtheLipschitzcontinuityofy revealthat
∥
s
−
s∗
∥
∗
1
∥y
s
−y
s∗
∥≤(1 −θµ g)2 ∥y
s −1
−y
s∗
∥
(56)
1 1 L gx
≤(1 −θµ g)2 y
s −1
−y
s∗ −1
+(1 −θµ g)2
(cid:18)
µ
g
(cid:19)∥x
s
−x
s −1 ∥
(cid:13) (cid:13) t=s 1
≤(1 −θµ
g)s −2(cid:13)1
∥y 1 −y 1∗
∥+(cid:13)
(1 −θµ
g)1
2
L µgx
λ
−
∇φ t
(cid:18) g (cid:19) (cid:88)t=1 (cid:13) (cid:13)
t=s 1 (cid:13)(cid:101) (cid:13)
≤∥y 1 −y 1∗ ∥+(1 −θµ g)1 2 L µgx λ − ∇φ t . (cid:13) (cid:13) (57)
(cid:18) g (cid:19) (cid:88)t=1 (cid:13) (cid:13)
(cid:13)(cid:101) (cid:13)
Set (cid:13) (cid:13)
λ ≤ 8(m√ +3 1µ )4g L C fx+ µ1 2 3√κ˜C r+C fx −1 1+ L µgx +(1 −θµ g)1 2 L µgx −1 ,
gyy (cid:18) g (cid:16) (cid:17)(cid:19) (cid:18) g (cid:18) g (cid:19)(cid:19)
25LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
ityieldsfrom(11),(54),(57)that
L
gx
ε 1+ max x x + max y y
j+1
≤ (cid:18) µ g (cid:19)1 ≤s,t ≤j+1∥
s
−
t
∥ 1 ≤s ≤j+1∥
s
−
s∗
∥
i=j
L gx 1 L gx
≤∥y
1
−y
1∗
∥+ 1+
µ
+(1 −θµ g)2
µ
λ ∇φ
i
(cid:18) g (cid:18) g (cid:19)(cid:19) (cid:88)i=1(cid:13) (cid:13)
L gx 1 L gx
i=j(cid:13) (cid:13)(cid:101) (cid:13)
(cid:13)
≤∥y
1
−y
1∗
∥+ 1+
µ
+(1 −θµ g)2
µ
λ ∇φ
i
(cid:18) g (cid:18) g (cid:19)(cid:19) (cid:88)i=1(cid:13) (cid:13)
√3µ g . (cid:13) (cid:13)(cid:101) (cid:13) (cid:13) (58)
≤ 4(m+1)3L
gyy
Asforthenextepoch,denotingC = 1 3√κ˜C +C wehave
v µg r fx
(cid:16) (cid:17)
b A v (L +L C )ε˜ + b A v
m+1 m+1 m fx gyy v m+1 1 1 m
∥ − ∥≤ ∥ − ∥
m
√κ˜ 1
(L +L C )ε˜ + 2√κ˜ − +√mL ε˜ κ˜ C
fx gyy v m+1 gyy m+1 r
≤ (cid:32) (cid:32)√κ˜+1(cid:33) (cid:33)
m
L +L C √κ˜ 1
C fx gyy v ε˜ +2√κ˜ − +√mL ε˜ κ˜
r m+1 gyy m+1
≤ (cid:32)(cid:18) C r (cid:19) (cid:32)√κ˜+1(cid:33) (cid:33)
C , (59)
r
≤
bysettingm,ε˜suchthat
m
L +L C √κ˜ 1
fx gyy v ε˜ +2√κ˜ − +√mL ε˜ κ˜ 1.
m+1 gyy m+1
(cid:18)
C
r (cid:19)
(cid:32)√κ˜+1(cid:33) ≤
Moreover,sincethestepsizeforxissetaszeroatthefirstm stepsinthenextepoch,weobtainfori=1,2,...,m ,
0 0
ε
m+i
= y
m+i
−y
m∗+i
≤(1 −θµ g)2i ∥y
m
−y
m∗
∥≤ε
m
≤
4(m√ +3 1µ )3g
L
. (60)
gyy
(cid:13) (cid:13)
Specifically, (cid:13) (cid:13)
y
m+m0
−y
m∗+m0
≤(1 −θµ g)m 20 ∥y
m
−y
m∗
∥≤
8(m√ +3 1µ )3g
L
(61)
gyy
(cid:13) (cid:13)
if we choose m
0
so that(cid:13) (1 −θµ g)m 20
≤
1 2(cid:13) . Therefore, by induction within an epoch (52), (53), (54), (55), (58) and
inductionbetweenepochs(59),(60),(61),weconcludethelemma.
G.5.ProofofLemma3.12
LemmaG.5. SupposeAssumptions3.1, 3.2, 3.3, 3.6, 3.9, 3.10and3.11hold. Ifwithineachepoch, wesetthestepsize
θ ( 1 )aconstantforyandthestepsizeforxaszerointhefirstm (1)steps,andtheothersasanappropriate
∼O m4 0 ∼O
constantλ ( 1 ),thenwehavethefollowing.
∼O m4
j
r¯ κ˜(j) 1
j
∥ ∥ 2 κ˜(j) − + jL ε κ˜(j),
gyy j
∥r¯ 0
∥
≤ (cid:32)(cid:112)κ˜(j)+1(cid:33)
(cid:112) (cid:112)
whereκ˜(j):=
Lgy+2√ 33(j+1)3Lgyyεj. (cid:112)
µg−2√ 33(j+1)3Lgyyεj
Proof. LemmaG.4guaranteesthecondition
2√3
µ (j+1)3L ε >0
g gyy j
− 3
issatisfied. TheremainingproofcanbedirectlyadaptedfromLemmaG.3.
26LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
H.ProofofTheMainTheorems
In this section, we provide proof of the main theorem presented in Section 3.2. Let A = g(x ,y ) and b =
k yy k k k
∇
f(x ,y ),andletthereferencevaluesbeA = 2 g(x ,y ),b = f(x ,y )andv =(A ) 1b .
∇y k k ∗k ∇yy k k∗ ∗k ∇y k k∗ k∗ ∗k − ∗k
Thefollowinglemmadisplaysthedescentpropertiesoftheiterates y .
k
{ }
LemmaH.1. SupposeAssumptions3.2and3.3hold,wehave
2
y y 2 (1+σ)(1 θµ ) y y 2+ 1+ 1 (1 θµ ) L gx x x 2,
k+1
−
k∗+1
≤ −
g
∥
k
−
k∗
∥ σ −
g
µ ∥
k+1
−
k
∥
(cid:18) (cid:19) (cid:18) g (cid:19)
(cid:13) (cid:13)
foranyσ >(cid:13)0. (cid:13)
Proof. Algorithm 2 executes a single-step gradient descent on the strongly convex function g(x , ) during the outer
k+1
·
iteration. Leveragingtheestablishedconvergencepropertiesofstronglyconvexfunctions(Nesterovetal.,2018), weare
thusabletoderivethefollowing.
2 2
y y (1 θµ ) y y .
k+1
−
k∗+1
≤ −
g k
−
k∗+1
ByYoung’sinequalitythat a+b2 (cid:13)(1+σ) a2+(cid:13) 1+ 1 b 2w(cid:13)ithanyσ >(cid:13)0,
| | ≤(cid:13) | | (cid:13) σ ∥ ∥ (cid:13) (cid:13)
y y 2 (1+σ)(1 θµ ) y (cid:0) y 2(cid:1) + 1+ 1 (1 θµ ) L gx 2 x x 2.
k+1
−
k∗+1
≤ −
g
∥
k
−
k∗
∥ σ −
g
µ ∥
k+1
−
k
∥
(cid:18) (cid:19) (cid:18) g (cid:19)
(cid:13) (cid:13)
(cid:13) (cid:13)
Inthecontextofbileveloptimization,wedefinetheinitialresidualinthe(h+1)-thepochas
r :=b A v¯ ,
h+1 mh+1 mh+1 h
−
andtheresidualink-thstep
r :=(b A v¯ ) A ∆v .
k mh+1
−
mh+1 h
−
∗k k
Basedontheboundnessofv inLemmaG.4wecanestimate
k
µ v v (b A v¯ ) A ∆v
g
∥
k
−
k∗
∥≤∥
∗k− ∗k h
−
∗k k
∥
= (b A v¯ ) A ∆v r +r
∥
∗k− ∗k h
−
∗k k
−
k k
∥
b b + A A v¯ + r
≤∥
∗k− mh+1
∥ ∥
∗k− mh+1
∥∥
h
∥ ∥
k
∥
L (x ,y ) (x ,y )
≤
fy
∥
k k∗
−
mh+1 mh+1
∥
+L (x ,y ) (x ,y ) v¯ + r
gyy
∥
k k∗
−
mh+1 mh+1
∥∥
h
∥ ∥
k
∥
=(L +L v )εh+ r
fy gyy ∥ mh ∥ j ∥ k ∥
(L +L C )εh+ r , (62)
≤ fy gyy v j ∥ k ∥
whichcomesfrom (A ) 1 1 ,v =v¯ +∆v ,and v C .
∗j − ≤ µg k h k ∥ k ∥≤ v
Lemma H.2. Supp (cid:13)ose Assu (cid:13)mptions 3.1, 3.2, 3.3, 3.6, 3.10 and 3.11 hold. Within each epoch, we set the step size θ
( 1 ) a constant(cid:13)for y and(cid:13)the step size for x as zero in the first m steps, and the others as an appropriate constan∼ t
O m4 0
λ ( 1 ),thentheiterates
∼O m4
x for k =mh+j, h=0,1,2,..., and j =m +1,m +2,...,m,
k 0 0
{ }
generatedbyAlgorithm2satisfy
j 1
2 −
∇φ(x k) −∇φ(x k) ≤3L2 gxι2(j −m0)+h(m −m0)δ 0+18(1 −γ)−1λ2L2 gxm2ω
φ
∥∇φ(x mh+t) ∥2
(cid:13) (cid:13) (cid:88)t=0
(cid:13) (cid:13)(cid:101) (cid:13) (cid:13)
+18(1 −γ)−1λ2L2 gxm2ω
φh −1
ι2(h −e)(m
−m0)m −1
∥∇φ(x me+t) ∥2,
e=0 t=0
(cid:88) (cid:88)
where0<γ <1,m (logm)areconstants,misthesubspacedimension,andδ isaconstantdefinedintheproof.
0 0
∼O
27LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Proof. By(62),conclusionfromLemmaG.3andtheYoung’sinequality,
2
v v 2 2 L fy+L gyyC v εh 2 +2 1 r 2
∥ k − k∗ ∥ ≤ µ j µ2 ∥ k ∥
(cid:18) g (cid:19) g
2 L fy+L gyyC v
2(cid:0) εh(cid:1)
2 +2 1 2 κ˜(j) κ˜(j) −1
j
+ jL εhκ˜(j)
2
r 2
≤ (cid:18) µ g (cid:19) j µ2 g  (cid:32)(cid:112)κ˜(j)+1(cid:33) gyy j  ∥ k0 ∥
(cid:0) (cid:1) (cid:112) (cid:112)
2 L fy+L gyyC v 2 εh 2 +4 1  4κ˜(j) (cid:112) κ˜(j) −1 2j + jκ˜(j)L 2 εh 2 r 2.
≤ (cid:18) µ g (cid:19) (cid:0) j (cid:1) µ2 g  (cid:32)(cid:112)κ˜(j)+1(cid:33) (cid:16)(cid:112) gyy (cid:17) (cid:0) j (cid:1) ∥ k0 ∥
 (cid:112) 
Anestimatecanbemadefor r ,
h+1
∥ ∥
r = b A v¯
h+1 mh+1 mh+1 h
∥ ∥ ∥ − ∥
= b b +A v A v
∥
mh+1
−
∗mh ∗mh m∗h− mh+1 mh
∥
L (x ,y ) (x ,y )
≤
fy
∥
mh+1 mh+1
−
mh m∗h
∥
C
fy
+ L (x ,y ) (x ,y ) +L v v
µ
gyy
∥
mh+1 mh+1
−
mh m∗h
∥
gy
∥
m∗h− mh
∥
g
µ L +C L L
g fy fy gyy gx
=L v v + 1+ x x ,
gy
∥
mh
−
m∗h∥
µ µ ∥
mh+1
−
mh
∥
g (cid:18) g (cid:19)
thus
2j
∥v k −v k∗ ∥2 ≤ω ε εh j 2 + L µ2 g 2 gy κ˜(j) (cid:32)(cid:112)κ κ˜ ˜( (j j) )+− 11
(cid:33)
∥v mh −v m∗h∥2
(cid:0) (cid:1)
κ˜(j) µ L +C (cid:112)L L 2 κ˜(j) 1 2j
+ g fy fy gyy 1+ gx − x x 2, (63)
µ2 g (cid:18) µ g (cid:18) µ g (cid:19)(cid:19) (cid:32)(cid:112)κ˜(j)+1(cid:33) ∥ mh+1 − mh ∥
(cid:112)
where
2 2
ω =2 L fy+L gyyC v + 4 √mκ˜(m)L 2 L2 C2+ µ gL fy+C fyL gyy 1+ L gx C2 (m).
ε (cid:18) µ g (cid:19) µ2 g gyy (cid:32) gy v (cid:18) µ g (cid:18) µ g (cid:19)(cid:19) s (cid:33)∼O
(cid:0) (cid:1)
Now,wedefine
L2 µ2+L2 C2
δ = fx g gxy fg y y 2+ v v 2,
k (cid:32) L2 gxµ2 g (cid:33)∥ k − k∗ ∥ ∥ k − k∗ ∥
andadd L2 fxµ2 g+L2 gxyC f2 g y y 2onbothsidesof(63). CombineitwithLemmaH.1,
L2 gxµ2 g ∥ k − k∗ ∥
(cid:16) (cid:17)
2j
δ
k
≤ω
ε
εh
j
2 + L µ2 g
2
gy κ˜(j) (cid:32)(cid:112)κ κ˜ ˜( (j j) )+− 11
(cid:33)
∥v
mh
−v m∗h∥2
(cid:0) (cid:1)
κ˜(j) µ L +C (cid:112)L L 2 κ˜(j) 1 2j
+ g fy fy gyy 1+ gx − x x 2
µ2 g (cid:18) µ g (cid:18) µ g (cid:19)(cid:19) (cid:32)(cid:112)κ˜(j)+1(cid:33) ∥ mh+1 − mh ∥
+(1+σ)j(1 θµ )j L2 fxµ2 g+L2 gxyC f2 g y (cid:112) y 2
− g L2 µ2 ∥ mh − m∗h∥
gx g
1 L2 µ2+L2 C2 L 2 j
+ 1+ (1 θµ ) fx g gxy fg gx (1+σ)r(1 θµ )r x x 2. (64)
(cid:18)
σ
(cid:19)
− g L2 gxµ2
g (cid:18)
µ
g (cid:19) r=0
− g ∥ k −r − k −r −1 ∥
(cid:88)
28LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
We set σ,θ to satisfy (1+σ)(1 θµ ) < 1, and since Lemma G.4 reveals that under the appropriate step-size setting
g
−
λ ( 1 )andθ ( 1 ),κ˜(j) κ˜forsomeκ˜ >0,wedefine
∼O m4 ∼O m4 ≤
√κ˜ 1
ι:=max − , (1+σ)(1 θµ ) <1 (65)
g
(cid:40)√κ˜+1 − (cid:41)
(cid:113)
Chooseapositiveintegerm suchthat,for0<γ <1,
0
L2 µ2+L2 C2 −1 L2 µ2+L2 C2 L2 µ L +C L L 2
ι −2m0 (1 γ) −1max fx g gxy fg fx g gxy fg +m , gyκ˜ g fy fy gyy 1+ gx ,
≥ −  (cid:32) L2 gxµ2 g (cid:33) (cid:32) L2 gxµ2 g (cid:33) µ2 g (cid:18) µ g (cid:18) µ g (cid:19)(cid:19)  
whichmeansm (logm). 
0
∼O
Sincewesetthestepsizeforxaszerointhefirstm steps,Assumption3.10revealsthat
0
εh 2 λ2m 1+ L gx 2 j −1 φ(x ) 2 +m y y 2 ,
j ≤ µ ∇ mh+t mh+j − m∗h+j
(cid:0) (cid:1) (cid:18) g (cid:19) t (cid:88)=m0(cid:13) (cid:13) (cid:13)(cid:101) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
then
δ
k
≤λ2mω
ε
(cid:18)1+ L µg gx (cid:19)2
t
(cid:88)=j − m1 0(cid:13)∇φ(x mh+t) (cid:13)2 + L µ2 g
2
gyκ˜(j) (cid:32)(cid:112)κ κ˜ ˜( (j j) )+− 11 (cid:33)2j ∥v
mh
−v m∗h∥2
κ˜(j) µ L +C L (cid:13) (cid:13)(cid:101) L (cid:13) (cid:13) 2 κ˜(j) 1(cid:112)2j
+ g fy fy gyy 1+ gx − x x 2
µ2 g (cid:18) µ g (cid:18) µ g (cid:19)(cid:19) (cid:32)(cid:112)κ˜(j)+1(cid:33) ∥ mh+1 − mh ∥
L2 µ2+L2 C2 (cid:112)
+(1+σ)j(1 θµ )j fx g gxy fg +m y y 2
− g (cid:32) L2 gxµ2 g (cid:33)∥ mh − m∗h∥
1 L2 µ2+L2 C2 L 2 j
+ 1+ (1 θµ ) fx g gxy fg +m gx (1+σ)r(1 θµ )r x x 2. (66)
(cid:18) σ (cid:19) − g (cid:32) L2 gxµ2 g (cid:33)(cid:18) µ g (cid:19) r=0 − g ∥ k −r − k −r −1 ∥
(cid:88)
Thenwecanrearrange(66),
j 1
− 2 2
δ
k
(1 γ)ι2(j −m0)(δ mh)+3m2λ2ω
φ
φ(x mh+t) + φ(x mh) , (67)
≤ − (cid:32) ∇ ∇ (cid:33)
t (cid:88)=m0(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)(cid:101) (cid:13) (cid:13)(cid:101) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
wherewedenote
2 2
ω L κ˜ι µ L +C L L
ε gx g fy fy gyy gx
ω =max 1+ , 1+ ,
φ m µ m2µ2 µ µ
(cid:40) (cid:18) g (cid:19) g (cid:18) g (cid:18) g (cid:19)(cid:19)
1 1 L2 µ2+L2 C2 L 2
1+ (1 θµ ) fx g gxy fg +m gx (1).
m2 (cid:18) σ (cid:19) − g (cid:32) L2 gxµ2 g (cid:33)(cid:18) µ g (cid:19) (cid:41)∼O
Since
φ(x ) φ(x ) 2 3 f(x ,y ) f(x ,y ) 2+3 2 g(x ,y ) 2 v v 2
∇ k −∇ k ≤ ∥∇x k k −∇x k k∗ ∥ ∇xy k k ∥ k − k∗ ∥
(cid:13) (cid:13) (cid:13)(cid:101) (cid:13) (cid:13) (cid:13) +3 ∇2 xyg(x k,y k) −∇2 xyg(x k,y k∗) 2(cid:13) (cid:13) ∥v k∗ ∥2 (cid:13) (cid:13)
2
≤3L2 f(cid:13) (cid:13) x∥y k −y k∗ ∥2+3L2 gx∥v k −v k∗ ∥2(cid:13) (cid:13)+3L2 gxy C µfy ∥y k −y k∗ ∥2
(cid:18) g (cid:19)
=3L2 δ , (68)
gx k
29LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
itfollowsthat
j 1
− 2 2
δ
k
(1 γ)ι2(j −m0)(δ mh)+3m2λ2ω
φ
φ(x mh+t) + φ(x mh)
≤ − (cid:32) ∇ ∇ (cid:33)
t (cid:88)=m0(cid:13) (cid:13) (cid:13) (cid:13)
j −1 (cid:13) (cid:13)(cid:101) (cid:13) (cid:13) (cid:13) (cid:13)(cid:101) 2(cid:13) (cid:13) 2
(1 γ)ι2(j −m0)(δ mh)+6λ2m2ω
φ
φ(x mh+t) φ(x mh+t) + φ(x mh) φ(x mh)
≤ − (cid:32) ∇ −∇ ∇ −∇ (cid:33)
t (cid:88)=m0(cid:13) (cid:13) (cid:13) (cid:13)
j −1 (cid:13) (cid:13)(cid:101) (cid:13) (cid:13) (cid:13) (cid:13)(cid:101) (cid:13) (cid:13)
+6λ2m2ω φ(x ) 2+ φ(x ) 2
φ mh+t mh
(cid:32) ∥∇ ∥ ∥∇ ∥ (cid:33)
t (cid:88)=m0
j 1
−
≤(1 −γ)ι2(j −m0)(δ mh)+18λ2L2 gxm2ω
φ
(cid:32)
δ mh+t+δ
mh
(cid:33)
t (cid:88)=m0
j 1
−
+6(1 γ)(1 γ)−1λ2m2ω
φ
φ(x mh+t) 2+ φ(x mh) 2 . (69)
− − (cid:32) ∥∇ ∥ ∥∇ ∥ (cid:33)
t (cid:88)=m0
Basedon(69),thefollowinginequalitycanbecheckedbyinduction.
j 1
−
δ
k
ι2(j −m0)(δ mh)+6(1 γ)−1λ2m2ω
φ
φ(x mh+t) 2+ φ(x mh) 2 ,
≤ − (cid:32) ∥∇ ∥ ∥∇ ∥ (cid:33)
t (cid:88)=m0
ifλ m 1ιm ,specifically,
−
≤O
(cid:0) (cid:1)
1/2 1/2
γ 1 γ 1
λ min m −3/2, m −1ιm −m0 .
≤ (cid:40)(cid:18)18ω φ(cid:19) L gx (cid:18)18ω φ(cid:19) L gx (cid:41)
Notethatwecanadjustthestepsizeθ in(65)suchthatιisclosedto1,thusforsimplicity,westilltakeλ ( 1 )We
∼ O m4
canboundδ recursivelyby
k
j 1
−
δ
k
ι2(j −m0)(δ mh)+6(1 γ)−1λ2m2ω
φ
φ(x mh+t) 2
≤ − ∥∇ ∥
t=0
(cid:88)
j 1
−
ι2(j −m0)+h(m −m0)δ 0+6(1 γ)−1λ2m2ω
φ
φ(x mh+t) 2
≤ − ∥∇ ∥
t=0
(cid:88)
h 1 m 1
+6(1 γ)−1λ2m2ω
φ
− ι2(h −e)(m −m0) − φ(x me+t) 2,
− ∥∇ ∥
e=0 t=0
(cid:88) (cid:88)
whereweareslightlyabusingofnotationthatδ = L2 fxµ2 g+L2 gxyC f2 g y y 2+ v v 2foranalysisconvenience,
0 L2 gxµ2 g ∥ 0 − 0∗ ∥ ∥ 0 − 0∗ ∥
while we start from (x 1,y 1,v 1) in Algorithm 2 for(cid:16)the statement co(cid:17)nvenience in this paper. Substituting (70) into (68)
completestheproof.
2
φ(x ) φ(x ) 3L2 δ
∇ k −∇ k ≤ gx k
(cid:13) (cid:13) j 1
(cid:13) (cid:13)(cid:101) (cid:13) (cid:13) ≤3L2 gxι2(j −m0)+h(m −m0)δ 0+18(1 −γ)−1λ2L2 gxm2ω
φ
− ∥∇φ(x mh+t) ∥2
t=0
(cid:88)
h 1 m 1
+18(1 −γ)−1λ2L2 gxm2ω
φ
− ι2(h −e)(m −k) − ∥∇φ(x me+t) ∥2.
e=0 t=0
(cid:88) (cid:88)
30LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
Theorem H.3. Suppose Assumptions 3.1, 3.2, 3.3, 3.6, 3.9 3.10, and 3.11 hold. Within each epoch, we set the step size
θ ( 1 )aconstantforyandthestepsizeforxaszerointhefirstm steps,andtheothersasanappropriateconstant
∼O m4 0
λ ( 1 ),thentheiterates x generatedbyAlgorithm2satisfy
∼O m4 { k }
m K mλ 1
φ(x ) 2 − ,
k
K(m m ) ∥∇ ∥ ≤O K(m m )
− 0 k=0, (cid:18) − 0 (cid:19)
(kmod(cid:88)
m)>m0
wherem (logm)isaconstantandmisthesubspacedimension.
0
∼O
Proof. AccordingtoLemmaE.2,agradientdescentstepresultsinthedecreaseinthehyper-gradientφ:
L
φ(x ) φ(x )+ φ(x ),x x + φ x x 2
k+1 k k k+1 k k+1 k
≤ ⟨∇ − ⟩ 2 ∥ − ∥
λ λ 2
φ(x ) λ2L φ(x ) 2+ +λ2L φ(x ) φ(x )
k φ k φ k k
≤ − 2 − ∥∇ ∥ 2 ∇ −∇
(cid:18) (cid:19) (cid:18) (cid:19)(cid:13) (cid:13)
≤φ(x k)
−
λ
2
−λ2L
φ
∥∇φ(x k) ∥2+3 λ
2
+λ2L φ(cid:13) (cid:13)L2 gxι2(j −m0(cid:101))+2h(m −(cid:13) (cid:13)m0)δ
0
(cid:18) (cid:19) (cid:18) (cid:19)
j 1 h 1 m 1
+18 (cid:18)λ
2
+λ2L
φ
(cid:19)(1 −γ)−1λ2L2 gxm2ω
φ
(cid:32)
t=− 0∥∇φ(x mh+t) ∥2+ e=− 0ι2(h −e)(m −m0) t=−
0
∥∇φ(x me+t) ∥2 (cid:33).
(cid:88) (cid:88) (cid:88)
Then,telescopingindexfromk =0tok =K =mH,
K
λ
λ2L φ(x ) 2
φ k
2 − ∥∇ ∥
(cid:18) (cid:19) k=0,
(kmod(cid:88)
m)>m0
H 1 m
λ −
≤φ(x 0) −φ(x ∗)+3
2
+λ2L
φ
L2 gxδ
0
ι2h(m −m0) ι2(j −m0)
(cid:18) (cid:19) h (cid:88)=0 j= (cid:88)m0+1
K H 1
+18λ2 λ
2
+λ2L
φ
(1 −γ)−1L2 gxm2ω φ(m −m 0) ∥∇φ(x k) ∥2 − ι2h(m −m0)
(cid:18) (cid:19) k=0, h=0
(kmod(cid:88)
m)>m0
(cid:88)
≤φ(x 0) −φ(x ∗)+3
λ
2
+λ2L
φ
L2 gxδ
01 −ι 12H(m ι2−m0)
(cid:18) (cid:19) −
+18λ2
λ
2
+λ2L
φ
(1 −γ)−1L2 gxm2ω φ(m −m
0)1 −ι 12H(m ι2−m0) K
∥∇φ(x k) ∥2,
(cid:18) (cid:19) − k=0,
(kmod(cid:88)
m)>m0
wherex ∗ ∈argmin x ∈Rdx φ(x). Byselectingasuitablevalueforλsuchthat
λL φ+18λ
λ
2
+λ2L
φ
(1 −γ)−1L2 gxm2ω φ(m −m
0)1 −ι 12H(m ι2−m0)
≤
1
4,
(cid:18) (cid:19) −
wecanderive
K
φ(x ) 2
4(φ(x 0) −φ(x ∗))
+(6+12λL )L2 δ
1 −ι2H(m −m0)
,
∥∇ k ∥ ≤ λ φ gx 0 1 ι2
k=0, −
(kmod(cid:88)
m)>m0
whichcompletestheproofbydividingbothsidesby m −mm0T.
31LancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
I.DetailsonExperiments
I.1.Generalsettings
Weconductexperimentstoempiricallyvalidatetheperformanceoftheproposedalgorithms. Wetestonasyntheticprob-
lem, ahyper-parametersselectiontask, andadatahyper-cleaningtask. WecomparetheproposedSubBiOandLancBiO
with the existing algorithms in bilevel optimization: stocBiO (Ji et al., 2021), AmIGO-GD and AmIGO-CG (Arbel &
Mairal, 2022), SOBA (Dagre´ou et al., 2022) and TTSA (Hong et al., 2023). The experiments are produced on a server
that consists of two Intel® Xeon® Gold 6330 CPUs (total 2 28 cores), 512GB RAM, and one NVIDIA A800 (80GB
×
memory) GPU. The synthetic problem and the deep learning experiments are carried out on the CPUs and the GPU, re-
spectively.Forwideraccessibilityandapplication,wehavemadethecodepubliclyavailableonhttps://github.com/UCAS-
YanYang/LancBiO.
For the proposed LancBiO, we initiate the subspace dimension at 1, and gradually increase it to m = 10 for the deep
learningexperimentsandtom=80forthesyntheticproblem. Forallthecomparedalgorithms,weemployagridsearch
strategy to optimize the parameters. The optimal parameters yield the lowest loss. The experiment results are averaged
over10runs
Inthispaper,weconsiderthealgorithmsSubBiOandLancBiOinthedeterministicscenario,soweinitiallycomparethem
against the baseline algorithms with a full batch (i.e., deterministic gradient). In this setting, LancBiO yields favorable
numerical results. Moreover, in the data hyper-cleaning task, to facilitate a more effective comparison with algorithms
designedforstochasticapplications,weimplementallcomparedmethodswithasmallbatchsize,findingthattheproposed
methodsshowcompetitiveperformance.
I.2.Syntheticproblem
Weconcentrateonasyntheticscenarioinbileveloptimization:
min f(x,y):=c cos x D y + 1 D x y 2,
x Rd 1 ⊤ 1 2∥ 2 − ∥
∈
(cid:0) (cid:1)
s.t. y =argming(x,y)
∗
y Rd
∈
d d
1
:=c
2
sin(x i+y i)+log exiyi + y ⊤(D 3+G)y,
2
(cid:32) (cid:33)
i=1 i=1
(cid:88) (cid:88)
whereweincorporatethetrigonometricandlog-sum-expfunctionstoenhancethecomplexityoftheobjectivefunctions.
Inaddition,weutilizethepositive-definitematrixGtoensureastronglyconvexlower-levelproblem,anddiagonalmatri-
cesD (i=1,2,3)tocontroltheconditionnumbersofbothlevels.
i
101 103
102
100 LancBiO
101 SubBiO
AmIGO-GD
10 1 100
AmIGO-CG
10 1 SOBA
10 2
TTSA
10 2
stocBiO
10 3
10 3
20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160
Time [sec] Time [sec]
Figure8.Comparisonofthebilevelalgorithmsonthesyntheticproblem.Left:normofthehyper-gradient;Right:residualnormofthe
linearsystem, A v b .
k k k
∥ − ∥
Inthisexperiment,wesettheproblemdimensiond=104andtheconstantsc =0.1, c =0.5.Gisconstructedrandomly
1 2
with d eigenvalues from 1 to 105. We generate entries of D , D and D from uniform distributions over the intervals
1 2 3
32
mron
tneidarg-repyH
mron
laudiseRLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
[ 5,5],[0.1,1.1]and[0,0.5],respectively. TakingintoaccounttheconditionnumbersdominatedbyD (i = 1,2,3)and
i
−
G,wechooseλ=1andθ =10 5forallalgorithmscomparedafteramanualsearch.
−
ItcanbeseenfromFigure8thatLancBiOachievesthefinalaccuracythefastest,whichbenefitsfromthemoreaccurate
v estimation. Figure5illustrateshowvariationsinmandI influencetheperformanceofLancBiOandAmIGO,tested
∗
acrossarangefrom10to150form,andfrom2to10forI. Forclarity,wesettheseedoftheexperimentat4,andpresent
typical results to encapsulate the observed trends. It is observed that the increase of m accelerates the decrease in the
residualnorm,thusachievingbetterconvergenceofthehyper-gradient,whichalignswiththespiritoftheclassicLanczos
process.
When m = 50, the estimation of v is sufficiently accurate to facilitate effective hyper-gradient convergence, which is
∗
demonstratedinFigure5thatform 50, furtherincreasesinmmerelyenhancetheconvergenceoftheresidualnorm.
≥
Under the same outer iterations, to attain a comparable convergence property, I for AmIGO-CG should be set to 10.
Furthermore, given that the number of Hessian-vector products averages at (1 + 1) per outer iteration for LancBiO,
m
whereasAmIGOrequiresI 2,itfollowsthatLancBiOismoreefficient.
≥
I.3.Datahyper-cleaningonMNIST
The data hyper-cleaning task (Shaban et al., 2019), conducted on the MNIST dataset (LeCun et al., 1998), aims to train
a classifier in a corruption scenario, where the labels of the training data are randomly altered to incorrect classification
numbersatacertainprobabilityp,referredtoasthecorruptionrate. Thetaskisformulatedasfollows,
min (λ,w ):= 1 L(w x ,y )
λ Lval ∗ |Dval| (xi,yi) ∈Dval ∗ i i
(cid:80)
s.t. w =argmin (w,λ)
∗ tr
L
w
1
:= σ(λ )L(wx ,y )+C w 2,
i i i r
∥ ∥
tr
|D |(xi,(cid:88)yi)
∈Dtr
where L() is the cross-entropy loss, σ() is the sigmoid function, and C is a regularization parameter. In addition, w
r
· ·
servesasalinearclassifierandσ(λ )canbeviewedastheconfidenceofeachdata.
i
100
0.90
LancBiO_full
0.88
SubBiO_full
0.86 AmIGO-GD_256
6×10 1
AmIGO-CG_full
0.84 SOBA_128
TTSA_256
0.82
4×10 1 stocBiO_256
0.80
0 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 70 80
Time [sec] Time [sec]
Figure9.Comparison of the bilevel algorithms on data hyper-cleaning task with mini-batch when p = 0.5. The training set, the
validationsetandthetestsetcontain5000, 5000and10000samples, respectively. Thepost-fixoflegendrepresentsthebatchsize.
Left:testaccuracy;Right:testloss.
In the deterministic setting, where we implement all compared methods with full-batch, the training set, the valida-
tion set and the test set contain 5000, 5000 and 10000 samples, respectively. For algorithms that incorporate inner
iterations to approximate y or v , we select the inner iteration number from the set 5i i=1,2,3,4 . The step
∗ ∗
{ | }
size of inner iteration is selected from the set 0.01,0.1,1,10 and the step size of outer iteration is chosen from
{ }
5 10i i= 3, 2, 1,0,1,2,3 . TheresultsarepresentedinFigure6. NotethatLancBiOiscraftedforapproximat-
× | − − −
ingtheHessianinversevectorproductv ,whilethesolidmethodsstocBiOandTTSAarenot. Consequently,withrespect
∗
(cid:8) (cid:9)
totheresidualnormofthelinearsystem,i.e., A v b ,weonlycomparetheresultswithAmIGO-GD,AmIGO-CG
k k k
∥ − ∥
andSOBA.Observethattheproposedsubspace-basedLancBiOachievesthelowestresidualnormandthebesttestaccu-
racy, andsubBiOiscomparabletotheotheralgorithms. Specifically, inFigure6, theefficiencyofLancBiOstemsfrom
33
ycarucca
tseT
ssol
tseTLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
itsaccurateapproximationofthelinearsystem. Furthermore,weimplementthesolversdesignedforthestochasticsetting
using mini-batch to enable a broader comparison in Figure 9. It is shown that the stochastic algorithm SOBA tends to
convergefasterinitially,butalgorithmsemployingafull-batchapproachachievehigheraccuracy.
Toexplorethepotentialforextendingourproposedmethodstoastochasticsetting, wealsoconductanexperimentwith
stochastic gradients. In this setting, where we implement all compared methods with mini-batch, the training set, the
validation set and the test set contain 20000, 5000 and 10000 samples, respectively. For algorithms that incorporate
inner iterations to approximate y or v , we select the inner iteration number from the set 3i i=1,2,3,4 . The
∗ ∗
{ | }
step size of inner iteration is selected from the set 0.01,0.1,1,10 , the step size of outer iteration is chosen from
{ }
1 10i i= 3, 2, 1,0,1,2,3 and the batch size is picked from 32 2i i=0,1,2,3 . AmIGO-CG is not
× | − − − × |
presentedsinceitfailsinthisexperimentinoursetting. TheresultsinFigure10demonstratethatLancBiOmaintainsrea-
(cid:8) (cid:9) (cid:8) (cid:9)
sonableperformancewithstochasticgradients, exhibitingfastconvergencerate, althoughthefinalconvergenceaccuracy
isslightlylower.
Generally,forstandardlinearsystems,thetraditionalLanczosprocessisrecognizedforitsefficiencyandversatilityover
gradient descent methods. LancBiO, in a sense, reflects this principle within the context of bilevel optimization, under-
scoringtheeffectivenessofthedynamicLanczos-aidedapproach.
0.900 3×100
0.875
LancBiO_256
2×100
0.850
SubBiO_256
0.825
AmIGO-GD_256
0.800 1×100
SOBA_128
0.775
TTSA_256
0.750
5×10 1 stocBiO_256
0.725
0.700
0 10 20 30 40 50 60 0 10 20 30 40 50 60
Time [sec] Time [sec]
Figure10.Comparison of the bilevel algorithms on data hyper-cleaning task with mini-batch when p = 0.5. The training set, the
validationsetandthetestsetcontain20000,5000and10000samples,respectively. Thepost-fixoflegendrepresentsthebatchsize.
Left:testaccuracy;Right:testloss.
I.4.Logisticregressionon20Newsgroup
Consider the hyper-parameters selection task on the 20Newsgroups dataset (Grazzi et al., 2020), which contains c = 20
topics with around 18000 newsgroups posts represented in a feature space of dimension l = 130107. The goal is to
simultaneouslytrainalinearclassifierw anddeterminetheoptimalregularizationparameterζ. Thetaskisformulatedas
follows,
min (ζ,w ):= 1 L(w x ,y )
λ Lval ∗ |Dval| (xi,yi) ∈Dval ∗ i i
(cid:80)
s.t. w =argmin (w,ζ)
∗ tr
L
w
c l
1 1
:= L(wx ,y )+ ζ2w2.
i i cl j ij
tr
|D |(xi,(cid:88)yi) ∈Dtr (cid:88)i=1 (cid:88)j=1
whereL()isthecross-entropylossand ζ2 arethenon-negativeregularizers.
· { j}
The experiment is implemented in the deterministic setting, where we implement all compared methods with full-batch,
thetrainingset,thevalidationsetandthetestsetcontain5657,5657and7532samples,respectively. Foralgorithmsthat
incorporateinneriterationstoapproximatey orv ,weselecttheinneriterationnumberfromtheset 5i i=1,2,3,4 .
∗ ∗
{ | }
Toguaranteetheoptimalityconditionofthelower-levelproblem,weadoptadecaystrategyfortheouteriterationstepsize,
i.e.,λ = λ/k0.4,forallalgorithms. Theconstantstepsizeθ ofinneriterationisselectedfromtheset 0.01,0.1,1,10
k
{ }
andtheinitialstepsizeλofouteriterationischosenfrom 5 10i i= 3, 2, 1,0,1,2,3 . Theresultsarepresented
× | − − −
inFigure11. Inthissetting,AmIGO-CGexhibitsslightlybetterperformanceinreducingtheresidualnorm. Nevertheless,
(cid:8) (cid:9)
34
ycarucca
tseT
ssol
tseTLancBiO:DynamicLanczos-aidedBilevelOptimizationviaKrylovSubspace
under the same time, LancBiO implements more outer iterations to update x, which optimizes the hyper-function more
efficiently.
LancBiO SubBiO AmIGO-GD AmIGO-CG SOBA TTSA stocBiO
0.950 1×100 10 1
0.925
8×10 1
0.900
0.875 6×10 1 10 2
0.850
0.825
4×10 1
0.800
10 3
0.775
3×10 1
0.750
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 25 50 75 100 125 150 175 200
Time [sec] Time [sec] Time [sec]
Figure11.Comparisonofthebilevelalgorithmshyper-parametersselectiontask. Left: validationaccuracy; Center: validationloss;
Right:residualnormofthelinearsystem, A v b .
k k k
∥ − ∥
35
ycarucca
noitadilaV
ssol
noitadilaV
mron
laudiseR