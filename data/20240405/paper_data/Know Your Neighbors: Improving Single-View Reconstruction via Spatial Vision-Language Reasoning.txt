Know Your Neighbors: Improving Single-View Reconstruction
via Spatial Vision-Language Reasoning
RuiLi1 TobiasFischer1 MattiaSegu1 MarcPollefeys1
LucVanGool1 FedericoTombari2,3
1ETHZu¨rich 2Google 3TechnicalUniversityofMunich
Abstract
Recovering the 3D scene geometry from a single view
isafundamentalyetill-posedproblemincomputervision.
Whileclassicaldepthestimationmethodsinferonlya2.5D
scenerepresentationlimitedtotheimageplane,recentap-
proachesbasedonradiancefieldsreconstructafull3Drep-
resentation. However,thesemethodsstillstrugglewithoc-
cludedregionssinceinferringgeometrywithoutvisualob-
servationrequires(i)semanticknowledgeofthesurround-
ings, and (ii) reasoning about spatial context. We propose
KYN, a novel method for single-view scene reconstruction
that reasons about semantic and spatial context to predict
eachpoint’sdensity. Weintroduceavision-languagemod-
ulation module to enrich point features with fine-grained
semantic information. We aggregate point representations
Figure1. Single-viewscenereconstructionresults. Wepresent
across the scene through a language-guided spatial atten-
thepredicted3Doccupancygridsgivenasingleinputimage.The
tionmechanismtoyieldper-pointdensitypredictionsaware
cameraisatthebottomleftandpointstothetoprightalongthez-
of the 3D semantic context. We show that KYN improves
aixs.PreviousmethodslikeBTS[51]struggletorecoveraccurate
3Dshaperecoverycomparedtopredictingdensityforeach
objectshapes(greenbox)andexhibittrailingeffectsinunobserved
3Dpointinisolation. Weachievestate-of-the-artresultsin
areas(bluebox).Incontrast,KYNrecoversmoreaccuratebound-
scene and object reconstruction on KITTI-360, and show ariesandmitigatesthetrailingeffectsprevalentinpriorart.
improvedzero-shotgeneralizationcomparedtopriorwork.
Projectpage: https://ruili3.github.io/kyn.
visibleparts.
Recently, approaches based on neural radiance
1.Introduction fields [39] have shown great potential in inferring the
true 3D scene representation from a single [51, 59] or
Humans have the extraordinary ability to estimate the ge- multiple views [50]. For instance, Wimbauer et al. [51]
ometry of a 3D scene from a single image, often includ- introduce BTS, a method that estimates a 3D density field
ingitsoccludedparts. Itenablesustoreasonaboutwhere fromasingleviewatinferencewhilebeingsupervisedonly
dynamic actors in the scene might move, and how to best by photometric consistency given multiple posed views at
navigate ourselves to avoid a collision. Hence, estimat- trainingtime.
ing the 3D scene geometry from a single input view is a Intuitively, given only a single image at inference, the
long-standingchallengeincomputervision,fundamentalto modelmustrelyonsemanticknowledgefromtheneighbor-
autonomousnavigation[16]andvirtualrealityapplications ing 3D structure to predict the density of occluded points.
[33]. Sincetheproblemishighlyill-posedduetoscaleam- However, existing approaches lack explicit semantic mod-
biguity, occlusions, andperspectivedistortion, ithastradi- elingand,bymodelingdensitypredictionindependentlyfor
tionally been cast as a 2.5D problem [6, 31, 58], focusing each point, are unaware of the semantic 3D context of the
onareasvisibleintheimageplaneandneglectingthenon- point’s surroundings. This results in the clear limitations
1
4202
rpA
4
]VC.sc[
1v85630.4042:viXra
tupnI
]15[STB
sruOwhich we illustrate in Fig. 1. Specifically, prior work [51] challengeslikevaryingcameraintrinsics[12,20,42,56,58]
struggles with accurate shape recovery (green) and further anddatasetbias[3]. Self-supervisedmethodscasttheprob-
exhibits trailing effects (blue) in the absence of visual ob- lemasaviewsynthesistaskandlearndepthviaphotometric
servation.Wearguethat,whenconsideringasinglepointin consistency on image pairs. Existing works have investi-
3D, its density highly depends on the semantic scene con- gatedhowtohandledynamicobjects[7,13,17,30,31,46],
text,e.g.ifthereisanintersection,aparkinglot,oraside- different network architectures [37, 53, 62, 64] and lever-
walk visible in its proximity. This becomes more critical aging additional constraints [19, 32, 44, 47]. Our method
aswemovefurtherfromthecameraoriginsincethedegree fallsintheself-supervisedcategory. However,weestimate
ofvisualcoveragedecreaseswithdistance,andreconstruct- atrue3Drepresentationfromasingleview, asopposedto
ingtheincreasinglyunobservedscenepartsrequirescontext the2.5Drepresentationproducedbytraditionaldepthesti-
fromtheneighboringpoints. mation.
To this end, we present Know Your Neighbors (KYN), Semantic priors for depth estimation. Previous depth
a novel approach for single-view scene reconstruction that estimation methods use semantic information to enhance
predicts density for each 3D point in a scene by reasoning 2D feature representations with different fusion strategies
aboutitsneighboringsemanticandspatialcontext. Wein- [8,19,22,32],ortoremovedynamicobjects[5,28]during
troducetwokeyinnovations. Wedevelopavision-language training. Thesemethodsutilizesemanticinformationinthe
(VL)modulationschemethatendowstherepresentationof 2D representation space. On the contrary, we use seman-
each3Dpointinspacewithfine-grainedsemanticinforma- ticinformationtoenhance3Dpointrepresentationsandto
tion. To leverage this information, we further introduce a guideour3Dspatialattentionmechanism.
VLspatialattentionmechanismthatutilizeslanguageguid-
Neural radiance fields. Neural radiance fields
ancetoaggregatethevisualsemanticpointrepresentations
(NeRFs) [39, 59] learn a volumetric 3D representa-
across the scene and predict the density of each individual
tion of the scene from a set of posed input views. In
pointasafunctionoftheneighboringsemanticcontext.
particular, they use volumetric rendering in order to
Weshowthat,byinjectingsemanticknowledgeandrea-
synthesize novel views by sampling volume density and
soning about spatial context, our method overcomes the
color along a pixel ray. Recent multi-view reconstruction
limitations that prior art exhibits in unobserved areas, pro-
methods [49, 54, 60] take inspiration from this paradigm,
ducingmoreplausible3Dshapesandmitigatingtheirtrail-
reformulating the volume density function as a signed
ingeffects. Wesummarizeourcontributionsasfollows:
distance function for better surface reconstruction. These
• WeproposeKYN,thefirstsingle-viewscenereconstruc-
methods are focused on single-scene optimization using
tionmethodthatreasonsaboutsemanticandspatialcon-
multi-view constraints, often representing the scene with
texttopredicteachpoint’sdensity.
theweightsofasingleMLP.
• We introduce a VL modulation module to enrich point
To address the issue of generalization across scenes,
featureswithfine-grainedsemanticinformation.
Yu et al. [59] propose PixelNeRF to train a CNN image
• WeproposeaVLspatialattentionmechanismthataggre-
encoderacrossscenesthatisusedtoconditionanMLP,pre-
gatespointrepresentationsacrossthescenetoyieldper-
dicting volume density and color without multi-view opti-
pointdensitypredictionsawareoftheneighboring3Dse-
mizationduringinference. However,theirapproachislim-
manticcontext.
ited to small-scale and synthetic datasets. Recently, Wim-
Our experiments on the KITTI-360 dataset [34] show that
baueretal.[51]proposedBTS,anextensionofPixelNeRF
KYNachievesstate-of-the-artsceneandobjectreconstruc-
to large-scale outdoor scenes. They omit the color predic-
tions. Furthermore,wedemonstratethatKYNexhibitsbet-
tion and, during training, use reference images to query
terzero-shotgeneralizationontheDDADdataset[18]com-
color for a 3D point given a density field that represents
paredtothepriorart.
the 3D scene geometry. While this simplification allows
them to scale to large-scale outdoor scenes, their method
2.RelatedWork
fallsshortinpredictingaccurategeometryforoccludedar-
Monocular depth estimation. Estimating depth from eas(Fig.1). Weaddressthisshortcomingbyinjectingfine-
a single view has been extensively studied over the last grainedsemanticknowledgeandreasoningaboutsemantic
decade [35, 36, 55–58], both in a supervised and a self- 3Dcontextwhenqueryingthedensityfield,leadingtobet-
supervisedmanner. Supervisedmethodsdirectlyminimize tershaperecoveryforoccludedregionsinparticular.
the loss between the predicted and ground truth depths [1, Sceneasoccupancy. Arecentlineofworkinfers3Dscene
11]. For these, varying output representations [1, 2, 14], geometryasvoxelized3Doccupancy[4,38]fromasingle
network architectures [1, 27, 43, 61], and loss functions image. These works predict the occupancy and semantic
[1, 48, 52] have been proposed. Recent methods explore classofeach3Dvoxelbasedonexhaustive3Dannotations.
training unified depth models on large datasets, tackling Therefore, these methods rely heavily on manually anno-
2tateddatasets. Further,thepredefinedvoxelresolutionlim- Point-wisevisualfeatureextraction. Giventheinputim-
its the fidelity of their 3D representation. In contrast, our ageI , weextractimagefeaturesfromstandardimageen-
0
methoddoesnotrelyonlabor-intensivemanualannotations coders[21]andVLimageencoder[29]
andrepresentsthesceneasacontinuousdensityfield.
Semantic priors for NeRFs. Various works integrate se- F app =f(I 0,θ app),
(2)
manticpriorsintoNeRFs. Whilesomeutilize2Dsemantic F =f(I ,θ ),
vis 0 vis
or panoptic semgentation [15, 26, 63], others leverage 2D
VLfeatures[13,24,41]andlifttheseinto3Dspacebydis- whereF andF refertotheappearancefeaturesandVL
app vis
tilling them into the NeRF. This enables the generation of image features, respectively. We freeze the VL image en-
2D segmentation masks from new viewpoints, segmenting coder weights θ to retain the pre-trained semantics. We
vis
objectsin3Dspace,anddiscoveringorremovingparticular thenfusethefeaturesbyconcatenationfollowedby2con-
objectsfroma3Dscene. Whiletheaforementionedmeth- volutionallayers,yieldingF . Toobtainpoint-wisefea-
fused
ods focus on the classical multi-view optimization setting, turesforthe3DpointsX = {x }M ,weextractthefused
i i=1
weinsteadfocusonsingle-viewinputandleverageseman- featureF w.r.t. theprojected2Dcoordinatesp (x )of
fused 0 i
ticpriorstoimprovethe3Drepresentationitself. each 3D point. Then, we combine each 3D point feature
with a positional embedding γ(·) encoding its position in
3.Method normalizeddevicecoordinates(NDC).Weobtainthefused
point-wisevisualfeaturev ∈R1×C fora3Dpointx
Problemsetup. GivenaninputimageI ,itscorresponding i i
0
intrinsics K 0 ∈ R3×4 and pose T 0 ∈ R4×4, we aim to v =Concat(F (p (x )),γ(x0)), (3)
reconstruct the full 3D scene by estimating the density for i fused 0 i i
each3Dpointx∈R3amongpointsetX={x i}M
i=1 where Concat(·,·) is the feature concatenation operation,
andx0isthe3Dpositionw.r.t. I ’scoordinate.
σ i =f(I 0,K 0,T 0,X,θ), (1) i 0
Point-wise text feature extraction. As the text feature
wherethedensityσ iofpointx iisafunctionofthepointset size does not align with the image space, it can not be ex-
X and image I 0 along with its camera intrinsic/extrinsics. tracted directly by querying projected 2D coordinates. To
f denotesthenetworkandθrepresentsitsparameters. The this end, we first derive the semantic category of each 3D
density σ i can be further transformed to the binary occu- point through 2D segmentations. Then we use it to asso-
pancyscoreo i ∈{0,1}withapredefinedthresholdτ. Dur- ciatetextfeatureswitheach3Dpoint. Weutilizethecate-
ing training (Sec. 3.3), additional images I n are incorpo- gorynamesfromoutdoorscenes[9]aspromptstothetext
ratedwiththeircorrespondingintrinsicsK n andextrinsics encoder, yielding the text features t ∈ RQ×C, where Q is
T n,withn∈{1,...,N},providingmultiviewsupervision. thenumberofcategoriesandC isthefeaturechannel. We
Overview. We illustrate our method in Fig. 2. Given an then use the visual feature F ∈ RH×W×C from the VL
vis
inputimageI 0,wefirstextractimageandVLfeaturemaps imageencoder,toobtainthe2Dsemanticmapbycomput-
F appandF vis. Next,wefusetheimageandVLfeaturesinto ing cosine similarity between VL image and text features
asinglefeaturemapF andfurtherutilizecategory-wise
fused
text features to compute a segmentation map S. We then F ⊗t⊤
S = argmax vis , (4)
useintrinsicsK 0toprojectthe3DpointsetXtotheimage q∈{1,...,Q}∥F vis∥∥t∥
planeandqueryF , yieldingpoint-wisevisualfeatures.
fused
Inparallel,weretrievepoint-wisetextfeaturesbyquerying where⊗denotesmatrixmultiplication, S denotestheseg-
thesegmentationmapS andlookingupthecorresponding mentation map by maximizing the similarity scores be-
category-wise text features t for each 3D point. Given the tween VL image and text features along the category di-
point-wise visual and text features, we use our VL modu- mension. Wethencomputethesemanticcategoryforeach
lation layers to augment the features with fine-grained se- 3DpointbyqueryingS withprojected2Dcoordinatesand
mantic information. We aggregate the point-wise features thenleverageittoobtainthetextfeatureofeach3Dpoint.
with VL spatial attention, adding 3D semantic context to
s =S(p (x )),
thepoint-wisefeatures. Weguidetheattentionmechanism i 0 i
(5)
with the VL text features. We finally predict a per-point gt =t(s ),
i i
densitythatisthresholdedtoyieldthe3Doccupancymap.
wheregt ∈RC isthetextfeatureofthe3Dpointx .
3.1.Vision-LanugageModulation i i
VLmodulationlayers. Toaugmentthe3Dpointfeatures
We detail how we extract point-wise visual features and withrichsemanticsfrombothVLimageandtextfeatures,
how we augment the visual features with semantic infor- weproposetheVLmodulationlayers,whichintegrateboth
mationusingourVLmodulationmodule. imagefeaturev andtextrepresentationsgtofa3Dpointx
i i i
3Image
Encoder Vision-Language
Project & Query
Modulation layers
3D points
VL Image Point-wise
Input Encoder visual features
Image
Augmented Occupancies
point-wise features
Threshold
VL Text Seg. Map
Car, truck, … Encoder Vision-Language
Spatial Attention
Category-level Category text Point-wise Density
prompts features text features Estimations
Figure2. Overview. GivenaninputimageI ,weusetwoimageencoderstoobtainfeatures(F ,F ),andfusetheseintofeaturemap
0 app vis
F . We further extract category-level text features and a segmentation map S. For a given 3D point set X, we query the extracted
fused
featuresbyprojectingthemontotheimageplaneyieldingpoint-wisevisualandtextfeatures. Next,theVLmodulationlayersendowthe
pointrepresentationwithfine-grainedsemanticinformation.Finally,theVLspatialattentionaggregatesthesepointrepresentationsacross
the3Dscene,yieldingdensitypredictionsawareofthe3Dsemanticcontext.
for better scene geometry reasoning. The module is com- keys,queries,andvalues
posedofL = 4modulationlayers,eachconductingmodu-
F =f(V,θ ),
lationwithmultiplicationoperations Q Q
F =f(C ,θ ), (7)
K t K
F =f(V,θ ),
vl+1 =ReLU(FC(vl)⊙FC(gt)), (6) V V
i i i
whereallfeaturesareinRM×Cˆ , whereCˆ denotesthefea-
where FC(·) stands for the fully connected layer, ⊙ is the ture dimension before the attention. We then compute the
element-wiseproductbetweenfeatures,FC(gt)denotesthe globalcontextscoreG∈RCˆ×Cˆ byattendingtothekeyand
i
textfeaturesencodedbyasinglefully-connectedlayerand valuefeatures,andthencorrelatingwithqueryfeaturesby
issharedacrossdifferentmodulationlayers.Wesetv1 =v
i i G=Softmax(F⊤)⊗F ,
at the first modulation layer, and iterate through different K V
√ (8)
layers.Weutilizeskipconnectionstoinjecttheinitialvisual F =Softmax(F / D)⊗G,
final Q
informationv1 afterthemodulatedfeaturevl′+1 atlevell′,
i i whereF denotesthespatiallyaggregatedpoint-wisefea-
usingconcatenationfollowedbyonefully-connectedlayer. final
Theoutputfeaturevˆ =vL denotesthe3Dpointfeatureof tures. Thedensityvalueisestimatedbyasinglefullycon-
i i
nectedlayerfollowedbyaSoftplus(·)function
x augmentedwithrichimageandtextsemantics.
i
σ =Softplus(FC(F )). (9)
final
3.2.VLSpatialAttention
Reducingthememoryfootprint. Asthepointfeaturesare
sampledfromtheentire3Dspace,simultaneouslyprocess-
Next, we dive into our VL spatial attention mechanism
ingallpointfeaturescanhitcomputationalbottleneckseven
thataggregatestheextractedpoint-wisefeaturesacrossthe
withlinearattention. Tothisend,werandomlysplittheini-
scene in a global-to-local fashion. First, we combine the
tialpointsintochunks,toensurethateachchunkisidenti-
wholesetofpoint-wisevisualfeatures{vˆ}M andtextfea-
i i=1 cally distributed. Then we conduct the spatial point atten-
tures {gt}M into V ∈ RM×C′ and C ∈ RM×C. Then,
i i=1 t tionseparatelywithineachchunkandcombinethedensity
we aggregate these features using a cross-attention opera-
estimationsafterward. Assuch,theattentioncanaggregate
tion in 3D space. Specifically, we leverage linear atten-
semanticpointrepresentationswithbothspatialawareness
tion [23, 45] over appropriately split point sets to achieve
andefficiency.
memory-efficientspatialcontextreasoning.
3.3.TrainingProcess
Category-informed cross-attention. We take the point-
wise features V as queries and values, and leverage text- Our method achieves self-supervision by computing the
basedfeatureC asthekeysinthelinearattention. Specifi- photometric loss between the reconstructed and target col-
t
cally,weprojectthefeatureswithfullyconnectedlayersto ors. Weextractthe2DfeaturemapofI togetpoint-wise
0
4Figure3.QualitativecomparisonsonKITTI-360dataset.Weillustratethescenereconstructionsasvoxelgrids,wherethecameraison
theleftsideandpointstotherightalongthez-axis.Alightervoxelcolorindicateshighervoxelpositions.Comparedtopreviousmethods
thatstrugglewithcorruptedandtrailingshapes,ourmethodproducesfaithfulscenegeometry,especiallyforoccludedareas.
representations, then partition all images {I } ∪ {I }N in patches. Let Pˆ as the rendered patch from view k in
0 n n=1 k
intoasourcesetN andalosssetN followingprevi- the source N , P as the supervisory patch from N ,
source loss source loss
ouspractice[51]. WerendertheRGBofN fromthecor- andd′ asthepatchdepthofP,thelossfunctionisdefined
loss
responding colors of N similar to the self-supervised followingpreviousmethods[17,51]
source
depth estimation [17]. Instead of resorting to the whole
image, we perform patch-based image supervision [51] to L=L ph+λ eL e, (12)
reduce memory footprints. For each pixel on a patch, we
where λ = 10−3, L and L are photometric loss and
samplethe3Dpointsx onitsback-projectedrayandcon- e ph e
i
edge-awaresmoothnessloss[17]onpatches
duct density estimation. Let x and x be the adjacent
i i+1
sampledpixelsonaray,wecalculatetheRGBinformation (cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
L = min λ L1 P,Pˆ +λ SSIM P,Pˆ ,
byvolumerenderingthesampledcolor[51] ph 1 k 2 k
k∈Nrender
(13)
i−1
(cid:89)
α i =exp(1−σ xiδ i) T i = (1−α j), (10) L
e
=|δ xd′ i|e−|δxP|+|δ yd′ i|e−|δyP|, (14)
j=1
whereλ =0.15andλ =0.85,δ ,δ denotesthegradient
1 1 x y
alongthehorizontalandverticaldirections.
S S
dˆ=(cid:88)
T α d cˆ
=(cid:88)
T α c , (11)
i i i k i i xi,k
4.Experiments
i=1 i=1
where δ denotes the distance between adjacent sampled To demonstrate the effectiveness of our proposed method,
i
pointsx andx alongtheray,andα refertotheproba- wecomparewithexistingworks[17,51,59,62]insingle-
i i+1 i
bilitythattherayendsbetweenx andx . Notethatthe view scene reconstruction, including both depth estima-
i i+1
color c = I (p (x )) is the sampled RGB value from tion [17] and radiance field based methods [51, 59]. We
xi,k k k i
the view k in the source set N , to obtain a better ge- evaluateboththe3Dscene(Sec. 4.4)andobject(Sec. 4.5)
source
ometry. dˆand cˆ represent the terminating depth and the reconstructionresultsontheKITTI-360datasetinshortand
k
renderedcolor. long ranges. We conduct extensive ablations (Sec. 4.6) to
Aswesamplethepixelsinapatch-wisemannerduring verify the effectiveness of each contribution, compare our
training, the rendered RGB and depth are also organized methodwithexistingsemanticfeaturefusiontechniques,as
5
tupnI
]71[2onoM
]95[FReNlexiP
]15[STB
sruOMethod Oacc↑ IEacc↑ IErec↑ Metrics. Weadopttheevaluationmetricsin[51]andmea-
sureoverallreconstruction(O )andoccludedreconstruc-
Monodepth2[17] 0.90 n/a n/a acc
Monodepth2[17]+4m 0.90 0.59 0.66 tion (IE acc, IE rec) accuracies. Specifically, O acc computes
4-20m PixelNeRF[59] 0.89 0.62 0.60 the accuracy between the prediction and the ground truth
BTS[51] 0.92 0.66 0.64 in the full area of the evaluation range, thus reflecting the
Ours 0.92 0.70 0.72
overall performance of the reconstruction. IE computes
acc
Monodepth2[17] 0.82 n/a n/a theaccuracyofthe invisible areasspecifically, i.e.without
Monodepth2[17]+4m 0.81 0.54 0.76
directvisualobservationinI . IE computestherecallof
4-50m PixelNeRF[59] 0.82 0.56 0.68 0 rec
BTS[51] 0.84 0.61 0.53 bothinvisibleandemptyareas, whichevaluatestherecon-
Ours 0.86 0.63 0.73 struction of the occluded empty space. The three metrics
focusondifferentaspectsofthereconstructionquality.
Table 1. Comparison of scene reconstruction on KITTI-360. Scene-andobject-levelevaluation. Inadditiontoevaluat-
Ourmethodachievesthebestoverallperformanceinboththenear
ingtheperformanceofthewholescene,wefocusonobject
andfarevaluationrange.
reconstruction in particular because they are of particular
interest compared to e.g. the road plane. To this end, we
manually annotate the object areas in the ground-truth oc-
wellasevaluateourmethod’sperformancewiththebroader
cupancymapsandcomputetheevaluationmetricsonthese
supervisionrange. Moreover,wedemonstrateourmethod’s
objectareas. Wereferthereadertothesupplementaryma-
zero-shotgeneralizationabilityinSec. 4.7.
terialfordetails.
4.1.Datasets
4.3.ImplementationDetails
We use the KITTI-360 [34] dataset for training and evalu-
ation, as it captures static scenes using cameras deployed We implement our method using Pytorch [40] and train it
withwidebaselines, i.e., twostereocameras, andtwoside on NVIDIA Quadro RTX 6000 GPUs. The appearance
cameras (fisheye cameras) at each timestep, which facili- network is similar to [17] with pre-trained weights on Im-
tatelearningfull3Dshapesbyself-supervision. Duringthe ageNet [10]. We adopt LSeg [29] as the visual-language
trainingphase,weuseallcamerasfromtwotimesteps,i.e. networkandfreezeitsparameters. Themodelistrainedus-
8camerasintotal,totrainourmodel. Weuseaninputres- ing Adam [25] optimizer with a learning rate of 10−4 for
olutionof192×640andchoosetheleftstereocamerafrom 25 epochs, which is reduced to 10−5 after 120k iterations.
the first time step to extract the image features. We split During the training phase, we sample 4096 patches across
all cameras randomly into N
render
and N
loss
for sampling losssetN loss,eachpatchcontains8×8pixels. Wefurther
colors and loss computation, as is illustrated in Sec. 3.3. sample64pointsalongeachrayfollowing[51].
Furthermore,weusetheDDAD[18]datasettoevaluatethe
4.4.SceneReconstruction
zero-shotgeneralizationcapabilityofthemodelstrainedon
KITTI-360. Weselecttestingsequenceswithmorethan50 We compare our method with recent single-view scene re-
imagesanduse384×640imageresolution. construction methods using self-supervision [17, 51, 59].
Specifically, we train Monodepth2 [17] on our benchmark
4.2.Evaluation
as the base depth estimation method. Since the depth net-
We follow the experimental protocol in [51] to evaluate work cannot infer occluded geometry, we use a handcraft
3Doccupancyprediction. Specifically,wesample3Dgrid criteriontosetareas4mbehindthedepthsurfaceasempty
pointson2Dslicesparalleltothegroundplane.ForKITTI- space (Monodepth2 + 4m). We also compare our method
360, we report scores within distance ranges [4,20] and withtheNeRF-basedmethods[51,59]followingthesame
[4,50] meters, where the latter provides a more challeng- trainingprotocol. AsshownintheTab. 1,comparedtopre-
ing evaluation scenario. For ground-truth generation, we vious methods, our method achieves both the best overall
follow[51]andaccumulate3DLiDARpointsacrosstime, performance (O ) and the best occluded area reconstruc-
acc
andset3Dpointslyingoutofalldepthsurfacesasunoccu- tion(IE ,IE ).NotethatMonodepth2(+4m)alsoyields
acc rec
pied, otherwise set to occupied. Unlike [51], which accu- acompetitiveperformanceintermsofinvisibleandempty
mulate only 20 LiDAR sweeps often leading to inaccurate spacereconstruction(IE ),butitreliesonhand-craftedcri-
rec
occludedscenegeometry,weaccumulateupto300LiDAR teriathatcannotlearntrue3Dinthescene.Qualitativecom-
frames. We also provide results with a 20-frame accumu- parisons are shown in Fig. 3. We show the reconstructed
lated ground truth in the supplementary material for refer- occupancy grids, where the camera is on the left side and
ence. FortheDDADdataset,weaccumulateupto100Li- points to the right along the z-axis within [4, 50m] range.
DARframesduetothelimitedsequencelengthandevaluate Ourmethoddemonstratesobviousqualitativesuperiorityin
inthe[4,50]metersrange. reasoning occluded object shapes against the inherent am-
6Method Oacc↑ IEacc↑ IErec↑
Monodepth2[17] 0.69 n/a n/a
Monodepth2[17]+4m 0.70 0.53 0.52
4-20m PixelNeRF[59] 0.67 0.53 0.49
BTS[51] 0.79 0.69 0.60
Ours 0.80 0.69 0.70
Monodepth2[17] 0.65 n/a n/a
Monodepth2[17]+4m 0.68 0.48 0.59
4-50m PixelNeRF[59] 0.66 0.56 0.58
BTS[51] 0.72 0.61 0.48
Ours 0.75 0.64 0.68
Table 2. Comparison of object reconstruction on KITTI-360.
Ourmethodoutperformsothermethodsinallmetricsinboththe
shortandlongevaluationrange.
biguity.Notably,itsubstantiallyreducesthetrailingeffects.
4.5.ObjectReconstruction
Weevaluatetheobjectreconstructionperformancebycom- Figure4.ObjectreconstructionintheKITTI-360dataset[34].
puting the metrics within the annotated object areas. As Fromlefttoright:Reconstructionsofthefence,tree,andcar.Our
shown in Tab. 2, our method achieves competitive or bet- methodproducesmorefaithfulobjectgeometries,inparticularin
occludedareasandforvarioussemanticcategories.
ter results in the [4, 20m] evaluation range. Meanwhile,
it achieves obvious improvements for all metrics in the [4,
50m] range, demonstrating its effectiveness in reasoning SceneRecon. ObjectRecon.
ambiguous geometries away from the camera origin. We Fapp F fused VL-Mod. Oacc IEacc IErec Oacc IEacc IErec
further show reconstructions of different categories in Fig. ✓ 0.84 0.60 0.53 0.72 0.61 0.48
4.Ourmethodgeneratesfaithfulobjectshapeswithreason- ✓ 0.84 0.60 0.55 0.72 0.61 0.48
✓ ✓ 0.85 0.63 0.64 0.73 0.63 0.59
ableestimatesofoccludedgeometryfordifferentcategories
including fences, trees, cars, etc., showing clear improve- Table3. AblationsstudyonVLmodulation. Wereporttheper-
mentsoverexistingmethods. formanceinthe[4,50m]range.NaivelyintroducingtheVLimage
featuredoesnotimproveperformance.OurVLmodulationcorre-
4.6.AblationStudies latingtheimageandtextfeaturesyieldsthebestscores.
Weevaluatetheeffectivenessofeachcontributionbysepa-
SceneRecon. ObjectRecon.
ratelyablatingtheVLmodulation(Sec. 4.6.1)andtheVL Fapp F fused VL-Mod.Attn.VL-Attn. Oacc IEacc IErec Oacc IEacc IErec
spatial attention (Sec. 4.6.2). We further compare with
✓ 0.84 0.60 0.53 0.72 0.61 0.48
existing techniques injecting semantics from related tasks ✓ ✓ 0.85 0.61 0.60 0.73 0.61 0.56
(Sec. 4.6.3). Moreover, we show our method’s improve-
✓ ✓ 0.85 0.61 0.67 0.72 0.60 0.62
mentunderabroadersupervisionrange(Sec. 4.6.4). ✓ ✓ 0.85 0.60 0.66 0.73 0.62 0.61
✓ ✓ ✓ 0.86 0.63 0.75 0.74 0.62 0.73
✓ ✓ ✓ 0.86 0.63 0.73 0.75 0.63 0.68
4.6.1 AblationstudyonVLModulation
Table4. Ablationstudyonspatialattention. Wereporttheper-
WeevaluatetheeffectivenessofVLmodulationbyadding formanceinthe[4,50m]range. Thespatialattentionimprovesin
each variant by enabling the 3D context awareness. Combining
different components upon baseline method [51], which
theVLfeatureswithspatialattentionyieldsthebestperformance.
uses only appearance feature F from the generic back-
app
bone [21]. In Tab. 3, we enhance the baseline by incorpo-
ratingthefusedVLimagefeatures(F )andinjectingim-
fused 4.6.2 AblationStudyonSpatialAttention
age/textsemanticswiththeVLModulation(VL-Mod.).We
findthatmerelyusingthefusedfeatureF fromtheVL InTab.4,weprovideexperimentalsupportfortheeffective-
fused
imageencoderdoesnotcontributetooverallimprovement. nessofusingspatialcontext, bothviaspatialattentionand
Asacomparison,theproposedVLModulationsignificantly VLspatialattention. Wefirstaddspatialattentionoverim-
improvestheperformancesbyproperlyinteractingwithim- ageappearancefeaturesonly(row: 1→2). WithouttheVL
ageandtextfeatures. features,aggregatingspatialcontextstillyieldsnotableim-
7
tupnI
]71[2onoM
]95[FReNlexiP
]15[STB
sruOSemantics Method Oacc IEacc IErec Method Oacc IEacc IErec
– Baseline 0.84 0.60 0.52 PixelNeRF[59] 0.55 0.45 0.23
PlainFusion 0.84 0.61 0.51 BTS[51] 0.48 0.50 0.16
SemanticFeat. 2DFusion-LDLS[32] 0.84 0.60 0.55 Ours 0.59 0.58 0.42
2DFusion-SAFENet[8] 0.84 0.60 0.51
VLFeat. FusingVLimagefeature 0.84 0.60 0.55 Table 7. Generalization to DDAD with KITTI-360 trained
Ours 0.86 0.63 0.73
model. Weevaluateinthe[4, 50m]range. Ourmethoddemon-
stratesbetterzero-shotabilitycomparedtopreviousmethods.
Table5. Comparisonwithsemanticfeaturefusiontechniques.
We compare with 2D feature fusion techniques from semantic-
4.6.4 ImprovementwithBroaderSupervisionRange
guided depth estimation [8, 32]. Our method achieves the best
performancewithvisualandlanguagesemanticenhancementand As single-view reconstruction is supervised by multiple
3Dcontextawareness.
posedimages,theperformancecanbeimprovedbyexpand-
ing the supervisory range during training. To this end, we
SceneRecon. ObjectRecon. investigate if our method can achieve consistent improve-
Supervision Method
Oacc IEacc IErec Oacc IEacc IErec mentusingabroadersupervisoryrange. Instandardsuper-
BTS[51] 0.84 0.61 0.53 0.72 0.61 0.48 vision, we use fisheye views at the next time step (1s in
1sinthefuture
Ours 0.86 0.63 0.73 0.75 0.64 0.68 thefuture). Inthebroadersupervisoryrange,werandomly
BTS[51] 0.86 0.68 0.72 0.74 0.64 0.75
1-4sinthefuture incorporate fisheye views within the next [1-4s] timestep,
Ours 0.87 0.69 0.77 0.78 0.68 0.75
yielding diverse supervisory ranges with a maximum cov-
Table6. Evaluationofdifferentsupervisoryranges. Wereport erage of 40m. We compare with BTS [51] in Tab. 6. Our
the performance in [4, 50m] range. Our method with the stan- methodwiththestandardsupervisoryrangeproducescom-
dard supervision range (1s in the future) yields comparable per-
parable results to [51] with a broader supervision range.
formancewith[51]usingbroadersupervision(1-4sinthefuture).
Enhanced by a broader range of supervision, our method
Meanwhile,itachievesfurtherimprovementwhenusingabroader
achievesfurtherimprovementover[51].
supervisionrange.
4.7.Zero-shotGeneralizationonDDAD
provement.Wefurthershowthattheintroductionofseman-
We evaluate the zero-shot generalization of KITTI-360
ticsviaVLmodulation(VL-Mod)helpsindependentofthe
trainedmodelsontheDDADdataset. Wereportthe[0,50]
spatialaggregationmechanism(row3→5,row: 4→6),un-
meters range scene reconstruction scores in Tab. 7. Our
derpinning that adding VL text features outperforms using
methodoutperformspreviouswork, showingtheeffective-
VLimagefeaturesonly. WhencombiningbothVLmodu-
nessoftheVLguidanceforzero-shotgeneralization.
lationandspatialattentionmechanisms,weachievethebest
performances(rows5,6). Additionally,weobserveasmall
5.Conclusion
albeitnotablegainwhenalsoinjectingVLfeaturesintothe
spatialaggregation(VL-Attn), however, mainlyimproving In this paper, we proposed KYN, a new method for
details are not captured in current metrics. Please refer to single-view reconstruction that estimates the density of
thesupplementarymaterialfordetails. a 3D point by reasoning about its neighboring semantic
and spatial context. To this end, we incorporate a VL
modulation module to enrich 3D point representations
4.6.3 ComparisonwithOtherSemanticGuidances with fine-grained semantic information. We further pro-
pose a VL spatial attention mechanism that makes the
As semantic cues are vital in other tasks such as depth es- per-point density predictions aware of the 3D semantic
timation,weinvestigatewhetherthetechniquesusedinthe context. Our approach overcomes the limitations of prior
relatedliterature[8,32]areusefulforsingle-viewscenere- art [51], which treated the density prediction of each
construction. We use the pre-trained DPT [43] semantic point independently from neighboring points and lacked
network to provide pre-trained semantic features, and in- explicitsemanticmodeling. Extensiveexperimentsdemon-
strate that endowing KYN with semantic and contextual
corporate different feature fusion techniques [8, 32] to our
knowledge improves both scene and object-level recon-
densitypredictionpipeline.AsshowninTab.6,wefindthat
struction. Moreover, we find that KYN better generalizes
conducting 2D feature fusion does not lead to notable im-
out-of-domain, thanks to the proposed modulation of
provementsoverthebaseline,whichisconsistentevenwith
point representations with strong vision-language features.
incorporatingVLimagefeaturefusion.However,byappro- The incorporation of VL features not only enhances the
priately interacting with image and text features with 3D performance of KYN, but also holds the potential to
context awareness, our method outperforms related tech- pave the way towards more general and open-vocabulary
niquesbyanotablemargin. 3D scene reconstruction and segmentation techniques.
8[13] Ziyue Feng, Liang Yang, Longlong Jing, Haiyan Wang,
YingLiTian,andBingLi. Disentanglingobjectmotionand
References occlusion for unsupervised multi-frame monocular depth.
arXivpreprintarXiv:2203.15174,2022. 2,3
[1] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka. [14] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
Adabins:Depthestimationusingadaptivebins. InProceed- manghelich,andDachengTao. Deepordinalregressionnet-
ingsoftheIEEE/CVFConferenceonComputerVisionand workformonoculardepthestimation. InProceedingsofthe
PatternRecognition,pages4009–4018,2021. 2 IEEEConferenceonComputerVisionandPatternRecogni-
[2] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka. tion,pages2002–2011,2018. 2
Localbins:Improvingdepthestimationbylearninglocaldis- [15] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu,
tributions. In European Conference on Computer Vision, LanyunZhu,XiaoweiZhou,AndreasGeiger,andYiyiLiao.
pages480–496.Springer,2022. 2 Panoptic nerf: 3d-to-2d label transfer for panoptic urban
[3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter scene segmentation. In 2022 International Conference on
Wonka, and Matthias Mu¨ller. Zoedepth: Zero-shot trans- 3DVision(3DV),pages1–11.IEEE,2022. 3
ferbycombiningrelativeandmetricdepth. arXivpreprint [16] AndreasGeiger, PhilipLenz, andRaquelUrtasun. Arewe
arXiv:2302.12288,2023. 2 ready for autonomous driving? the kitti vision benchmark
[4] Anh-QuanCaoandRaouldeCharette.Monoscene:Monoc- suite. In 2012 IEEE Conference on Computer Vision and
ular 3d semantic scene completion. In Proceedings of PatternRecognition,pages3354–3361.IEEE,2012. 1
theIEEE/CVFConferenceonComputerVisionandPattern [17] Cle´ment Godard, Oisin Mac Aodha, Michael Firman, and
Recognition,pages3991–4001,2022. 2 GabrielJBrostow. Diggingintoself-supervisedmonocular
[5] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia depthestimation. InProceedingsoftheIEEEInternational
Angelova.Depthpredictionwithoutthesensors:Leveraging ConferenceonComputerVision,pages3828–3838,2019. 2,
structureforunsupervisedlearningfrommonocularvideos. 5,6,7
InProceedingsoftheAAAIConferenceonArtificialIntelli- [18] VitorGuizilini,RaresAmbrus,SudeepPillai,AllanRaven-
gence,pages8001–8008,2019. 2 tos, and Adrien Gaidon. 3d packing for self-supervised
[6] JundaCheng,GangweiXu,PengGuo,andXinYang. Coa- monocular depth estimation. In Proceedings of the
trsnet: Fullyexploitingconvolutionandattentionforstereo IEEE/CVF Conference on Computer Vision and Pattern
matching by region separation. International Journal of Recognition,pages2485–2494,2020. 2,6
ComputerVision,132(1):56–73,2024. 1 [19] VitorGuizilini,RuiHou,JieLi,RaresAmbrus,andAdrien
[7] JunDaCheng,WeiYin,KaixuanWang,XiaozhiChen,Shi- Gaidon. Semantically-guided representation learning for
jieWang,andXinYang. Adaptivefusionofsingle-viewand self-supervised monocular depth. In International Confer-
multi-view depth for autonomous driving. arXiv preprint enceonLearningRepresentations,2020. 2
arXiv:2403.07535,2024. 2 [20] VitorGuizilini, IgorVasiljevic, DianChen, RaresAmbrus,
, ,
[8] Jaehoon Choi, Dongki Jung, Donghwan Lee, and Chang- andAdrienGaidon.Towardszero-shotscale-awaremonocu-
ickKim. Safenet: Self-supervisedmonoculardepthestima- lardepthestimation. InProceedingsoftheIEEE/CVFInter-
tionwithsemantic-awarefeatureextraction. arXivpreprint nationalConferenceonComputerVision,pages9233–9243,
arXiv:2010.02893,2020. 2,8 2023. 2
[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo [21] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Deep residual learning for image recognition. In Proceed-
Franke, Stefan Roth, and Bernt Schiele. The cityscapes ingsoftheIEEEconferenceoncomputervisionandpattern
datasetforsemanticurbansceneunderstanding.InProceed- recognition,pages770–778,2016. 3,7
ingsoftheIEEEconferenceoncomputervisionandpattern [22] HyunyoungJung,EunhyeokPark,andSungjooYoo. Fine-
recognition,pages3213–3223,2016. 3 grained semantics-aware representation enhancement for
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, self-supervisedmonoculardepthestimation. InProceedings
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage oftheIEEE/CVFInternationalConferenceonComputerVi-
database. In2009IEEEconferenceoncomputervisionand sion,pages12642–12652,2021. 2
patternrecognition,pages248–255.Ieee,2009. 6 [23] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,and
[11] DavidEigen,ChristianPuhrsch,andRobFergus.Depthmap Franc¸oisFleuret. Transformersarernns:Fastautoregressive
predictionfromasingleimageusingamulti-scaledeepnet- transformerswithlinearattention. InInternationalconfer-
work.InAdvancesinneuralinformationprocessingsystems, enceonmachinelearning,pages5156–5165.PMLR,2020.
pages2366–2374,2014. 2 4
[12] Jose M Facil, Benjamin Ummenhofer, Huizhong Zhou, [24] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo
Luis Montesano, Thomas Brox, and Javier Civera. Cam- Kanazawa,andMatthewTancik. Lerf:Languageembedded
convs: Camera-aware multi-scale convolutions for single- radiance fields. In Proceedings of the IEEE/CVF Interna-
view depth. In Proceedings of the IEEE/CVF Conference tionalConferenceonComputerVision,pages19729–19739,
onComputerVisionandPatternRecognition,pages11826– 2023. 3
11835,2019. 2 [25] Diederik P Kingma and Jimmy Ba. Adam: A method for
9stochastic optimization. arXiv preprint arXiv:1412.6980, Highresolutionself-supervisedmonoculardepthestimation.
2014. 6 arXivpreprintarXiv:2012.07356,2020. 2
[26] AbhijitKundu,KyleGenova,XiaoqiYin,AlirezaFathi,Car- [38] Ruihang Miao, Weizhou Liu, Mingrui Chen, Zheng Gong,
oline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi, Weixin Xu, Chen Hu, and Shuchang Zhou. Occdepth:
Frank Dellaert, and Thomas Funkhouser. Panoptic neural A depth-aware method for 3d semantic scene completion.
fields:Asemanticobject-awareneuralscenerepresentation. arXivpreprintarXiv:2302.13540,2023. 2
In Proceedings of the IEEE/CVF Conference on Computer [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
VisionandPatternRecognition,pages12871–12881,2022. JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
3 Representingscenesasneuralradiancefieldsforviewsyn-
[27] Minhyeok Lee, Sangwon Hwang, Chaewon Park, and thesis. InECCV,2020. 1,2
SangyounLee. Edgeconvwithattentionmoduleformonoc- [40] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
ulardepthestimation.InProceedingsoftheIEEE/CVFWin- Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ter Conference on Applications of Computer Vision, pages banDesmaison, LucaAntiga, andAdamLerer. Automatic
2858–2867,2022. 2 differentiationinpytorch. 2017. 6
[28] Seokju Lee, Sunghoon Im, Stephen Lin, and In So [41] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea
Kweon. Learning monocular depth in dynamic scenes Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.
via instance-aware projection consistency. arXiv preprint Openscene:3dsceneunderstandingwithopenvocabularies.
arXiv:2102.02629,2021. 2 In Proceedings of the IEEE/CVF Conference on Computer
[29] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen VisionandPatternRecognition,pages815–824,2023. 3
Koltun, and Rene Ranftl. Language-driven semantic seg- [42] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad
mentation. In International Conference on Learning Rep- Schindler, and Vladlen Koltun. Towards robust monocular
resentations,2022. 3,6 depthestimation:Mixingdatasetsforzero-shotcross-dataset
[30] Rui Li, Xiantuo He, Yu Zhu, Xianjun Li, Jinqiu Sun, and transfer.IEEEtransactionsonpatternanalysisandmachine
YanningZhang.Enhancingself-supervisedmonoculardepth intelligence,44(3):1623–1637,2020. 2
estimationviaincorporatingrobustconstraints. InProceed- [43] Rene´ Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi-
ingsofthe28thACMInternationalConferenceonMultime- sion transformers for dense prediction. In Proceedings of
dia,pages3108–3117,2020. 2 theIEEE/CVFinternationalconferenceoncomputervision,
[31] Rui Li, Dong Gong, Wei Yin, Hao Chen, Yu Zhu, Kaix- pages12179–12188,2021. 2,8
uan Wang, Xiaozhi Chen, Jinqiu Sun, and Yanning Zhang. [44] Aron Schmied, Tobias Fischer, Martin Danelljan, Marc
Learningtofusemonocularandmulti-viewcuesformulti- Pollefeys, and Fisher Yu. R3d3: Dense 3d reconstruction
framedepthestimationindynamicscenes.InProceedingsof of dynamic scenes from multiple cameras. In Proceedings
theIEEE/CVFConferenceonComputerVisionandPattern oftheIEEE/CVFInternationalConferenceonComputerVi-
Recognition,pages21539–21548,2023. 1,2 sion,pages3216–3226,2023. 2
[32] RuiLi,DannaXue,ShaolinSu,XiantuoHe,QingMao,Yu [45] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi,
Zhu, Jinqiu Sun, and Yanning Zhang. Learning depth via and Hongsheng Li. Efficient attention: Attention with lin-
leveraging semantics: Self-supervised monocular depth es- ear complexities. In Proceedings of the IEEE/CVF winter
timationwithbothimplicitandexplicitsemanticguidance. conferenceonapplicationsofcomputervision,pages3531–
PatternRecognition,page109297,2023. 2,8 3539,2021. 4
[33] JingyunLiang,YuchenFan,KaiZhang,RaduTimofte,Luc [46] JaimeSpencer,ChrisRussell,SimonHadfield,andRichard
Van Gool, and Rakesh Ranjan. Movideo: Motion-aware Bowden. Kick back & relax: Learning to reconstruct the
video generation with diffusion models. arXiv preprint worldbywatchingslowtv. InProceedingsoftheIEEE/CVF
arXiv:2311.11325,2023. 1 InternationalConferenceonComputerVision,pages15768–
[34] YiyiLiao,JunXie,andAndreasGeiger. Kitti-360: Anovel 15779,2023. 2
datasetandbenchmarksforurbansceneunderstandingin2d [47] Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin,
and3d.IEEETransactionsonPatternAnalysisandMachine Ian Reid, and Chunhua Shen. Sc-depthv3: Robust self-
Intelligence,45(3):3292–3310,2022. 2,6,7 supervisedmonoculardepthestimationfordynamicscenes.
[35] CeLiu, SuryanshKumar, ShuhangGu, RaduTimofte, and IEEETransactionsonPatternAnalysisandMachineIntelli-
LucVanGool.Singleimagedepthpredictionmadebetter:A gence,2023. 2
multivariategaussiantake. InProceedingsoftheIEEE/CVF [48] Zachary Teed and Jia Deng. Deepv2d: Video to depth
Conference on Computer Vision and Pattern Recognition, with differentiable structure from motion. arXiv preprint
pages17346–17356,2023. 2 arXiv:1812.04605,2018. 2
[36] CeLiu, SuryanshKumar, ShuhangGu, RaduTimofte, and [49] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
LucVanGool.Va-depthnet:Avariationalapproachtosingle Komura,andWenpingWang.Neus:Learningneuralimplicit
image depth prediction. arXiv preprint arXiv:2302.06556, surfacesbyvolumerenderingformulti-viewreconstruction.
2023. 2 arXivpreprintarXiv:2106.10689,2021. 2
[37] Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, [50] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
LinaLiu, YongLiu, XinxinChen, andYiYuan. Hr-depth: Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo
10Martin-Brualla,NoahSnavely,andThomasFunkhouser.Ibr- Stefano Mattoccia. Monovit: Self-supervised monocular
net: Learning multi-view image-based rendering. In Pro- depth estimation with a vision transformer. In 2022 Inter-
ceedingsoftheIEEE/CVFConferenceonComputerVision national Conference on 3D Vision (3DV), pages 668–678.
andPatternRecognition,pages4690–4699,2021. 1 IEEE,2022. 2,5
[51] FelixWimbauer,NanYang,ChristianRupprecht,andDaniel [63] ShuaifengZhi,TristanLaidlow,StefanLeutenegger,andAn-
Cremers. Behindthescenes: Densityfieldsforsingleview drewJDavison. In-placescenelabellingandunderstanding
reconstruction.InProceedingsoftheIEEE/CVFConference with implicit scene representation. In Proceedings of the
on Computer Vision and Pattern Recognition, pages 9076– IEEE/CVF International Conference on Computer Vision,
9086,2023. 1,2,5,6,7,8 pages15838–15847,2021. 3
[52] KeXian,JianmingZhang,OliverWang,LongMai,ZheLin, [64] Hang Zhou, David Greenwood, and Sarah Taylor. Self-
andZhiguoCao.Structure-guidedrankinglossforsingleim- supervisedmonoculardepthestimationwithinternalfeature
agedepthprediction. InProceedingsoftheIEEE/CVFCon- fusion. arXivpreprintarXiv:2110.09482,2021. 2
ferenceonComputerVisionandPatternRecognition,pages
611–620,2020. 2
[53] GangweiXu,JundaCheng,PengGuo,andXinYang.Atten-
tion concatenation volume for accurate and efficient stereo
matching. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 12981–
12990,2022. 2
[54] LiorYariv,JiataoGu,YoniKasten,andYaronLipman. Vol-
umerenderingofneuralimplicitsurfaces. AdvancesinNeu-
ralInformationProcessingSystems,34:4805–4815,2021. 2
[55] WeiYin,YifanLiu,ChunhuaShen,andYouliangYan. En-
forcinggeometricconstraintsofvirtualnormalfordepthpre-
diction.InProceedingsoftheIEEE/CVFInternationalCon-
ferenceonComputerVision,pages5684–5693,2019. 2
[56] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
Long Mai, Simon Chen, and Chunhua Shen. Learning to
recover 3d scene shape from a single image. In Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages204–213,2021. 2
[57] WeiYin,JianmingZhang,OliverWang,SimonNiklaus,Si-
monChen,YifanLiu,andChunhuaShen. Towardsaccurate
reconstruction of 3d scene shape from a single monocular
image. IEEETransactionsonPatternAnalysisandMachine
Intelligence,2022.
[58] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu,
KaixuanWang,XiaozhiChen,andChunhuaShen.Metric3d:
Towardszero-shotmetric3dpredictionfromasingleimage.
In Proceedings of the IEEE/CVF International Conference
onComputerVision,pages9043–9053,2023. 1,2
[59] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
pixelnerf: Neural radiance fields from one or few images.
In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pages4578–4587,2021. 1,
2,5,6,7,8
[60] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
tler, and Andreas Geiger. Monosdf: Exploring monocu-
lar geometric cues for neural implicit surface reconstruc-
tion. Advances in neural information processing systems,
35:25018–25032,2022. 2
[61] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan. New crfs: Neural window fully-connected
crfs for monocular depth estimation. arXiv preprint
arXiv:2203.01502,2022. 2
[62] ChaoqiangZhao,YouminZhang,MatteoPoggi,FabioTosi,
Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and
11