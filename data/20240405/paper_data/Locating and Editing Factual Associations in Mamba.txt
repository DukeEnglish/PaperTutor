Preprint. Underreview.
Locating and Editing Factual Associations in Mamba
ArnabSenSharma,∗DavidAtkinson, andDavidBau
KhouryCollegeofComputerSciences,NortheasternUniversity
Abstract
WeinvestigatethemechanismsoffactualrecallintheMambastatespacemodel.
Ourworkisinspiredbypreviousfindingsinautoregressivetransformerlanguage
modelssuggestingthattheirknowledgerecallislocalizedtoparticularmodules
atspecifictokenlocations;wethereforeaskwhetherfactualrecallinMambacan
besimilarlylocalized. Toinvestigatethis,weconductfourlinesofexperiments
onMamba. First,weapplycausaltracingorinterchangeinterventionstolocalize
keycomponentsinsideMambathatareresponsibleforrecallingfacts,revealing
thatspecificcomponentswithinmiddlelayersshowstrongcausaleffectsatthe
lasttokenofthesubject,whilethecausaleffectofinterveningonlaterlayersis
mostpronouncedatthelasttokenoftheprompt,matchingpreviousfindingson
autoregressivetransformers.Second,weshowthatrank-onemodeleditingmethods
can successfully insert facts at specific locations, again resembling findings on
transformermodels. Third,weexaminethelinearityofMamba’srepresentations
of factual relations. Finally we adapt attention-knockout techniques to Mamba
to dissect information flow during factual recall. We compare Mamba directly
toasimilar-sizedtransformerandconcludethatdespitesignificantdifferencesin
architecturalapproach,whenitcomestofactualrecall,thetwoarchitecturesshare
manysimilarities.
1 Introduction
Studiesofautoregressivetransformerlanguagemodels’(LMs)processingoffactualstatementssuch
asTheEiffelTowerislocatedinParis,haveidentifiedalocalizedpatternofinternalcomputations
whenrecallingfacts(Mengetal.,2022a;b;Gevaetal.,2023;Hernandezetal.,2023;Nandaetal.,
2023), and have further found that those LMs can be edited by making single-layer rank-one
changesinmodelparameterstoalteraspecificfact. Althoughtheselocalizedphenomenaappearto
generalizeacrosstransformerLMs,theextenttowhichsimilarlocalitymightappearinverydifferent
architectures—suchasrecurrentnetworks(RNNs)—hasnotyetbeeninvestigated.
In this paper we investigate the internal mechanisms of Mamba (Gu & Dao, 2023), a recently-
proposedstate-spacelanguagemodel,atypeofRNNthatachievesper-parameterperformancethatis
competitivewithtransformers. Specifically,weaskwhetherfactualrecallwithinMambaexhibits
localitysimilartothepatternsobservedinautoregressivetransformerlanguagemodels.
Ourpaperisacasestudyconfrontingakeymethodologicalchallengethatbroadlyfacesinterpretabil-
ityresearchers: asstate-of-the-artneuralnetworkarchitecturesevolve,wemustask,canthedetailed
analyticalmethodsandtoolsdevelopedforoneneuralarchitecture, suchastransformerLMs, be
generalizedandappliedtoadifferentneuralarchitecturesuchasMamba? Inthisweareableto
answerthequestionwithaqualified“yes”: wefindthatmanyofthemethodsusedtoanalyzetrans-
formerscanalsoprovideinsightsonMamba. Wealsodiscussmismatches—thatis,interpretation
methods(suchaspath-dependentattentionpatching)thatdonottransfertoMambaaseasilydueto
architecturaldifferences.
Webeginbystudyingwhetheractivationpatching (Wangetal.,2022)canbesuccessfullyapplied
toMamba. Knownvariouslyascausalmediationanalysis(Vigetal.,2020),causaltracing(Meng
etal.,2022a), andinterchangeinterventions(Geigeretal.,2021), activationpatchingtechniques
cansuccessfullyidentifyspecificmodelcomponentsintransformerLMsthatplaycrucialrolesin
performingatask. WeaskwhetherMambacanbeproductivelystudiedthesameway,eventhough
∗Correspondencetosensharma.a@northeastern.edu,Codeavailableatgithub.com/arnab-api/romba
1
4202
rpA
4
]LC.sc[
1v64630.4042:viXraPreprint. Underreview.
thearchitecturalcomponentsofMambaareverydifferent:forexample,insteadofattentionheadsand
MLPmodules,Mambaiscomposedofconvolutions,gates,andstate-spacemodules. Toanswer,we
adaptactivationpatchingtoMamba,andaskifanysparsitypatternsemergewhichprovideinsights
intotherespectiverolesofitscomponents.
Wealsostudywhetherrank-onemodeleditingcanbeappliedtoMamba. Whilestudiesoftrans-
formers(Mengetal.,2022a;b;Haseetal.,2024)havefoundthattherearearangeofMLPmodules
withinwhichfactualknowledgecanbeinsertedbymakingasinglerank-onechangeinparameters,
MambadoesnothaveMLPmodules,soweaskifthereareanyothermodulesthatcanbesimilarly
editedtoinsertknowledge. Aswithpreviousstudiesoftransformers,thekeyquestioniswhether
factualassociationscanbeeditedwithbothspecificity(withoutinterferingwithunrelatedfacts)and
generalization(whileremainingrobusttorewordingsoftheeditedfact).
Finally,weapplymethodsforunderstandingtheoverallinformationflowsinMamba. Inspiredby
thelinearityfindingsofHernandezetal.(2023),wemeasurethelinearityoftherelationsbetween
subjectandobjectembeddings. AndinspiredbyGevaetal.(2023),weexamineinformationflowby
adaptingattention-blockingmethodstotheattention-freeMambaarchitecture.
InthisworkweconductourexperimentsonMamba-2.8b,thelargestavailableLMinMambafamily,
andforcomparisonweconductthesameexperimentsonthesimilarlysizedPythia-2.8b(Biderman
etal.,2023)autoregressivetransformerLM.
2 BackgroundonMamba
Mamba, introduced in Gu & Dao (2023), is a recent family of language models based on state
spacemodels(SSMs). SSMsaredesignedtomodeltheevolutionofhiddenstateacrosstimewitha
first-orderdifferentialequation(Koopmanetal.,1999;Durbin&Koopman,2012),andwhenthey
areusedastherecurrentstateofanRNN,theycanenablehighlyefficientparallelizedtraining(Gu
et al., 2021). To achieve good performance in language modeling, the Mamba SSM introduces
input-dependentparameterizationorselective-SSMinsteadofthetraditionaltime-invariantSSMs.
MambausesaspecialarchitecturecalledMambaBlock1,whichisstackedhomogeneously,replacing
bothattentionandMLPblocksusedintransformerlayers. Here,wefocusonthedifferentoperations
performedinsideaMambaBlock.
Formally,Mambaisanautoregressivelan- aℓ
i-1
guage model: M : X → Y over a vo- hℓ -1 hℓ
cabularyV thatmapsasequenceoftokens i i
x = [x 1,x 2,...,x T] ∈ X, x i ∈ V to Wℓ aℓ Conv + SSM s ℓ Wℓ oℓ
y ∈ Y ⊂ R|V| whichisaprobabilitydis- a i i o i
tributionoverthenexttokencontinuations
ofx.SimilartootherdeepLMs,inMamba, W gℓ σ g
i
ℓ
a token x is first embedded to a hidden
i
state of size d as h(0) = emb(x ). Then Figure 1: ArchitectureofaMambaBlock. Projectionma-
h(0) istransformedsi equentiallybyi aseries tricesWℓ a andWℓ g havetheshape2d×d,whileWℓ o hasthe
i shaped×2d.h,a,g,s, andoareintermediatestatesofato-
(ℓ)
of MambaBlocks. The hidden state h kenrepresentation.σisSiLUactivationand⊗iselementwise
i
after the ℓth (1-indexed) MambaBlock is multiplication.Conv+SSMoperationabstractstheConv1D
andselective-SSMoperations.
computedasfollows:
h(ℓ)
=
h(ℓ−1) +o(ℓ)
(1)
i i i
whereo(ℓ) istheoutputofℓth MambaBlockfortheith token
i
(cid:16) (cid:17) (cid:16) (cid:17)
o(ℓ) =MambaBlock(ℓ) h(ℓ−1) ,h(ℓ−1) ,...,h(ℓ−1) =W(ℓ) s(ℓ) ⊗g(ℓ) (2)
i 1 2 i o i i
(ℓ)
Here,⊗representselement-wisemultiplicationorHadamardproduct. s iscalculatedas:
i
1Intheirpaper,Gu&Dao(2023)callthiscomponentMamba—thesamenameastheLMfamily.
2Preprint. Underreview.
Average IE of restoring h(l)over 400 prompts
i 0.8
Clean Run,G Patched Run,G*[←h ℓ] prefix
i
subject_[:-2]* 0.6
Michael [PAD]
subject_2nd_last*
Jordan Pelé 0.4
subject_last*
professionally professionally further tokens 0.2
played played last token
0.0
basketball p(ans)= ? 0 5 10 15 20 25 30 35 40 45 50 55 IE
(answer) single restored layer within Mamba-2.8b
(a)Activationpatching (l)
(b)Tracingtheresidualstates,h
i
Figure2: (a)Activationpatching. AstatefromthecleanrunGispatchedintoitscorrespondingposition
inG∗.Thishasadownstreameffectofpotentiallychangingallthestatesthatdependonthepatchedstatein
G∗[←h(ℓ) ].(b)Averageindirecteffectofapplyingcausaltracingonresidualstreamstates(h(ℓ)
inFigure1)
i i
across400differentfacts.
(ℓ) (ℓ) (ℓ)
a =W h (3)
i a i
(cid:16) (cid:16) (cid:17)(cid:17)
(ℓ) (ℓ) (ℓ) (ℓ) (ℓ) (ℓ)
c ,c ,...,c =SiLU Conv1D a ,a ,...,a (4)
1 2 i 1 2 i
(cid:16) (cid:17)
(ℓ) (ℓ) (ℓ) (ℓ)
s =selective-SSM c ,c ,...,c (5)
i 1 2 i
We abstract the operations in Equations 4 and 5 as the Conv + SSM operation in Figure 1. At
a high level, Conv + SSM brings information from the past token representations to the current
tokenrepresentation. ThepurposeissimilartotheattentionblocksintransformerLMs. But,unlike
attentionoperation,Conv+SSMscaleslinearlywiththecontextlengthandtherebyenjoysfaster
inferencespeedandlongercontextlimits. SeeGu&Dao(2023)fordetails.
(ℓ)
Theoutputoftheotherpath g (thatdoesnotpassthroughConv+SSMoperation)isagating
i
mechanismthatregulatestheinformationflow. ThisgatingmechanismresemblepartsofLSTM
(Hochreiter&Schmidhuber,1997)andGRU(Choetal.,2014)networks,wheresimilargatescontrol
selectiveupdatesofrecurrentstate.
(cid:16) (cid:17)
g(ℓ)
=SiLU
W(ℓ) h(ℓ−1)
(6)
i g i
Intheremainderofthepaper,weaimtocharacterizetheroleofthecomponentsofMambainfactual
recallbyadaptingtoolsthathavepreviouslybeenusedtoanalyzetransformers. InSection3,we
applyactivationpatchingtolocalizefactualrecallasinMengetal.(2022a),testingtherolesofstates
s, g,ando atalllayers. InSection4,followingMengetal.(2022a);Haseetal.(2024),wetest
i i i
rank-oneeditsoffactsacrosscomponentsW ,W ,andW ateachlayer. InSection5,wecollect
a g o
Jacobians within Mamba to test the linearity of relational encodings as done by Hernandez et al.
(2023). AndinSection6weaddressthechallengeofapplyingattentionpatchinginMamba,asused
inGevaetal.(2023)toisolateinformationflowinGPTLMs.
3 LocatingKeyStatesforFactualRecall
Webeginwithactivationpatching,seekingtounderstandiftherearespecifichiddenstateswhichplay
importantrolesduringfactualrecall. Weselectafact(s,r,o)thattheLMknows,whererisarelation
thatassociatesasubjectentityswithanobjectentityo. Toestimateeachstate’scontributiontowards
acorrectfactualprediction(s = MichaelJordan, r = professionallyplayed, o = basketball),we
collectmodelactivationsacrossthreedifferentruns:
cleanrun G: Inthecleanrun,wesimplyrunthemodelonapromptspecifyingthefactweare
interestedin. Forexample, x = (s,r) = MichaelJordanprofessionallyplayed. Wecacheallthe
(cid:8) (ℓ) (ℓ) (ℓ) (ℓ) (cid:9)
hiddenstatesduringthecleanruntobeusedlater: h ,a ,s ,g |i ∈ [1,T], ℓ ∈ [1,L] .
i i i i
corruptedrun G∗: Inthecorruptedrun,weswapswithadifferentsubjects∗(Pelé)suchthatthe
LMgivesadifferentanswero∗(soccer)tothemodifiedpromptx∗ = (s∗,r)(i.e.,o∗ ̸= o).
Thissubject-swappingapproachfollowstherecommendationofZhang&Nanda(2023)andhas
theadvantageofusinganaturaltextperturbationstoavoidintroducingout-of-domainstatestothe
3Preprint. Underreview.
(a)Average IE of restoring o(l)over 400 prompts (b)Average IE of restoring g(l) (c) Average IE of restoring s(l)
i 0.5 i 0.5 i 0.5
prefix
0.4 0.4 0.4
subject_[:-2]*
subject_2nd_last* 0.3 0.3 0.3
subject_last* 0.2 0.2 0.2
further tokens
0.1 0.1 0.1
last token
0.0 0.0 0.0
0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE
center of interval of 10 patched layers center of interval of 10 patched layers center of interval of 10 patched layers
Figure3: Averageindirecteffectofdifferentstateso(ℓ) ,g(ℓ) ,ands(ℓ) over400facts.Foreachlayerℓ,states
i i i
forawindowof10layersaroundℓarerestoredfromthecleanrunG.
model’scomputation,asmayhappenwithcorruptingsembeddingswithGaussiannoise(usedin
Mengetal.(2022a)).
patchedrun G∗[← h(ℓ) ]: Inthepatchedrun,werunthemodelonthecorruptedpromptx∗,but
i
(ℓ)
interveneonh byreplacingitsvaluewiththecorrespondingstatecachedfromthecleanrunG.The
i
remainderofthecomputationisrunnormally,meaningthatthepatchedstatecanhaveadownstream
effectofpotentiallychangingallthestatesthatdependonit. SeeFigure2a.
Let p(o), p∗(o),and p∗[← h(ℓ) ](o)denotetheprobabilityassignedtothecorrectanswero in G,
i
G∗,andG∗[← h(ℓ) ]respectively. Tomeasurethecontributionofh(ℓ) inrecallingthefact(s,r,o),
i i
wedefinetheindirecteffect(IE)as:
p∗[← h(ℓ) ](o)−p∗(o)
IE = i (7)
h(ℓ) p(o)−p∗(o)
i
(ℓ)
InFigure2bweplottheaverageindirecteffectofrestoringtheresidualstatesh acrossdifferent
i
layer-tokenpositionsover400facts. ThehighIEinlaterlayersatthelasttokenpositionisnatural,
(ℓ)
asrestoringacleanh theremeansrestoringmostofthemodelcomputationfrom G. However,
i
Mamba also shows high causality in the middle layers at the last subject token position. This is
consistentwithwhatMengetal.(2022a)observedinGPTfamilyoflanguagemodels.
(ℓ) (ℓ)
InFigure3weplottheaverageIEforo ,g ,
i i G*
(ℓ) (ℓ)
and s . The plot for o (Figure 3a) looks
i i
very similar to Figure 2b, confirming that the
outputfromMambaBlockhasstrongcausalef-
fectsatbothearlyandlatesites. Interestingly,
Figure3cshowsthattheselective-SSMoutput h i ℓ from G Blocking s i
(ℓ)
s i hashighIEinlaterlayersatthelasttoken G*[←hℓ ]
of the prompt, resembling the behavior of at- i
tention modules in GPT models (Meng et al.,
2022a). However,thereisnostatethatappears
todotheopposite; inotherwords, thereisno
statewithstrongeffectsattheearlysiteandnot
hℓ gℓ oℓ sℓ
atthelatesite. (Thegateoutputg(ℓ) doeshave i i i i
i
strongerIEattheearlysitethanthelatesite,but
Figure 4: A probe for path-specific effects. h(ℓ) is
theeffectsareveryweak.) Acomparisonwith i
restoredfromthecleanrunGasinFigure2a.Then,to
thecomparablePythia-2.8btransformerLMsis
revealtheroleoftheSSMcomponent,s statesfromthe
showninFigure9inAppendixC.Thiscompar- corruptedrunG∗arealsopatchedtobli
ocktheprevent
isonrevealsakeywaythatMambadiffersfrom
contributionsonthes paths.
i
transformers: whiletransformerMLPoutputs
haveeffectsintheearlysiteandnotthelatesite,inMambathereisnosimilarstatethatspecializes
onlyattheearlysiteatwhichfactualrecallwouldbeexpectedtooccur. Thispresentsthequestion:
whichparametersinMambamediatefactualrecall?
4Preprint. Underreview.
1.00
(a) i = subj last
0.75
0.50
0.25
0.00
(b) i = prompt last
1.00
0.75
0.50
0.25
0.00
0 5 10 15 20 25 30 35 40 45 50 55 60
IE with h(l)restored s blocked g blocked o blocked
i i i i
Figure5: Impactofablatings,g,ando onIE for(a)subjectlastand(b)promptlasttokenpositions.
i i i h(ℓ)
i
Takentogether(a)and(b)showaclearseparationrolesbetweenearly-midandlaterlayersinMamba-2.8b.
(ℓ)
h uptolayer46onlyshowstrongIEatthesubjectlasttokenpositionandhavenegligibleimpactafterthat.
i
(ℓ)
WhereasIEofh jumpsto1.0afterlayer46.(a)alsoshowsthat,atthesubjectlasttoken,beforelayer27−28,
i
(cid:0)
IE issignificantlyreducedbyblockingeithero,g,ors paths sortedindescendingorderofdamaging
h(ℓ) i i i
i (cid:1)
IE .(b)Atthepromptlasttoken,ablatingo ors pathscansignificantlyreduceIE inlayers47−50.
h(ℓ) i i h(ℓ)
i i
To investigate this question, we replicate an experiment from Meng et al. (2022a) to probe path-
specificeffects(Pearl,2022)forpathsthatavoideitherg,s,oro contributions.First,inthecorrupted
i i i
runG∗,attokenpositioni,wecacheallthecontributionsfromthes pathsass∗ = {s∗(ℓ) | ℓ ∈ [1,L]}.
i i i
TheninthepatchedrunG∗[← h(ℓ) ]wepatchh(ℓ) cachedfromthecleanrunGintoitscorresponding
i i
state(asinFigure2a),butwithanadditionalmodification: tounderstandthecontributionfromthes
i
paths,weseverthosepathsbypatchings∗ totheircorrespondinglocations(seeFigure4). Thesame
i
experimentcanbeconductedtounderstandthecontributionsofg ando. Wenotethatblockingthe
i i
o pathswillblockthes andg pathsaswell(seeFigure1).
i i i
InFigure5weplottheaverageresultsofthisexperimentdoneattokenpositions(a)i =subjectlast
and(b)i =promptlastover400examples. Thekeyfindingscanbeunderstoodbyexaminingthe
gapbetweenthepurplebarsandthegreen,red,andbluebars:alargegapindicatesastrongmediating
roleforSSM,W ,orW parameters,respectively. Attheearlysiteatthesubjectlasttoken,both
g o
SSMandW haveastrongrole,butW playsanevenlargerrole. Yetthestrongestmediatoratthe
g o
latesiteisalsoW . ThisexperimenthighlightstheimportanceofW inbothstagesofpredictinga
o o
fact,butitalsosuggeststhatMambadoesnotseparateearly-sitefactualrecallbetweenthesegroups
of parameters as cleanly as transformers. However, Figure 5 reveals a clean separation of roles
betweenearlytomidandlaterlayers,similartowhatHernandezetal.(2023)observedintransformer
LMs. However,wenotethatthisdivisionofresponsibilitiesbetweenlayerscanbemoresharply
noticedinMambawhencomparedtotransformersLMs(comparewithFigure10).
4 EditingFactsWithROME
Havingbeguntocharacterizethelocationsofimportantstatesforfactualrecall,wenowinvestigate
whetherfactualrecallbehaviorcanbeedited. Inparticular,weapplytheROME(RankOneModel
Editing,Mengetal.,2022a)techniquetoMamba. ROMEbeginswiththeobservationthatanylinear
transformations can be considered as an associative memory (Anderson, 1972; Kohonen, 1972),
mappingasetofkeysK = [k |k |...]totheircorrespondingvaluesV = [v |v |...],andusesthis
1 2 1 2
toeditfactualassociationsintransformerLMs. Here,weapplythetechniquetoaparticularsetof
lineartransformationswithinMamba,andreportoureditingsuccessoneach.2
TheinputtoROMEisapromptx = (s,r),wheres(EmmanuelMacron)isasubjectentityandr
(isthePresidentof)isarelation. ROMEalsotakesacounterfactualobjecto∗ (England),meantto
2Furthermotivatingtheseexperiments,previousworkhasshownthatthelocationsidentifiedbyactivation
patchingtechniquesareoftennotthosewhichhavethestrongesteditperformance (Haseetal.,2024).
5
EI
egarevA
EI
egarevAPreprint. Underreview.
(a) ROME performance on Mamba-2.8b W
a(l)
W
g(l)
W
o(l)
Efficacy (ES) Generalization (PS) Specificity (NS) Score (S)
100 100 100 100
75 75 75 75
50 50 50 50
25 25 25 25
0 0 0 0
0 5 10 15 20 25 30 35 40 45 50 55 60 0 5 10 15 20 25 30 35 40 45 50 55 60 0 5 10 15 20 25 30 35 40 45 50 55 60 0 5 10 15 20 25 30 35 40 45 50 55 60
Edit Layer Edit Layer Edit Layer Edit Layer
(b) ROME performance on Pythia-2.8b W
d( ol)
wn
Efficacy (ES) Generalization (PS) Specificity (NS) Score (S)
100 100 100 100
75 75 75 75
50 50 50 50
25 25 25 25
0 0 0 0
0 3 6 9 12 15 18 21 24 27 30 0 3 6 9 12 15 18 21 24 27 30 0 3 6 9 12 15 18 21 24 27 30 0 3 6 9 12 15 18 21 24 27 30
Edit Layer Edit Layer Edit Layer Edit Layer
(ℓ) (ℓ)
Figure6: ROMEperformanceineditingfactsacrossdifferentlayers(a)bymodifyingW ,W ,
a g
(ℓ) (ℓ)
W inMamba-2.8b,and(b)modifyingW inPythia-2.8b. Resultsarereportedonthefirst2000
o down
examplesintheCOUNTERFACTdataset.
replacethecorrectobjecto(France)inthemodel’soutput. Toeffectthatchange,ROMEgenerates
(ℓ)
arank-oneupdatetoW ,thedown-projectionmatrixoftheMLPmoduleforthelasttokenof
down
thesubjectatlayerℓ—whichplaystheroleoftheassociativememory. Ingeneratingtherank-one
(ℓ)
update,ROMEconsiderstheinputtoW
down
asthekey(k∗). Then,withgradientdescentROME
calculatesavalue(v∗)suchthat,whenv∗isinsertedastheoutputofW( dℓ o) wn,themodelwilloutputo∗.
Importantly,whileoptimizingv∗,ROMEattemptstominimizeunrelatedchangesinmodeloutputs
(JoeBiden,forexample,shouldstillbemappedtotheUnitedStatespost-edit). Finally,ROMEadds
arank-1matrix∆toW d(ℓ o) wnsuchthat(cid:0) W( dℓ o) wn+∆(cid:1) k∗ ≈ v∗. (Formathematicalandimplementation
detailsseeMengetal.(2022a).)
4.1 ApplyingROMEinMamba
(ℓ)
WeapplyROMEtothreedifferentprojectionmatrices: W whichaffectsonlytheselective-SSM
a
(ℓ) (ℓ)
path,W whichaffectsonlythegatingpath,andW ,thefinaloutputoftheMambaBlock,which
g o
isaddedtotheresidual. WeplotROMEperformanceacrossdifferentlayersandprojectionmatrices
(cid:0) (ℓ) (ℓ) (ℓ)(cid:1)
W ,W ,andW inFigure6a.
a g o
To evaluate editing performance, we use the COUNTERFACT dataset from Meng et al. (2022a).
COUNTERFACT contains20Kcounterfactualexamplesintheform(s,r,o → o∗),whereo isthe
correctanswertothepromptx = (s,r),ando∗istheobjectwhichistobeinsertedasthenewanswer
totheprompt. (SeeAppendixA.1fordetails). Weselectthefirst2000examplesfromthisdatasetfor
ourmodule-layersweep. WeusetheoriginalevaluationmatricesinMengetal.(2022a)tomeasure
ROMEeditperformance.
Thefinalscore(S)intheROMEevaluationsuiteistheharmonicmeanofthreedifferentscores:
1. Efficacy(ES):Foraneditrequest(s,r,o → o∗),wesaytheeditiseffectiveif,post-edit,theLM
assigns p(o∗) > p(o)inresponsetothepromptx = (s,r). Efficacyreflectstheportionofthe
exampleswheretheeditwaseffective.
2. Generalization(PS):Asuccessfuleditshouldbepersistentacrossdifferentparaphrasesof(s,r).
For each of the request instances (s,r,o → o∗), p(o∗) > p(o) is checked post-edit with a
setofdifferentrephrasings x ∼ P(s,r) oftheprompt x = (s,r), where P denotesasetof
p
paraphrasedtemplatesfortherelationr.
6Preprint. Underreview.
3. Specificity(NS):Finally,theeditshouldbespecifictoP(s,r)andshouldnotadditionallychange
themappingofsomenearbysubjects too∗. Toevaluatethespecificityofaneditwemeasure
n
p(o ) > p(o∗)withP(s ,r)forasetofnearbyfactualassociations{(s ,r,o )|o ̸= o∗}.
n n n n n
Figure6ashowsthatROMEcanachievehighscores(S)forarangeofearlytomiddlelayersby
(ℓ) (ℓ) (ℓ)
modifyinganyoneoftheprojectionmatricesW ,W ,orW ,matchingobservationsmadeby
a g o
Haseetal.(2024)regardingtransformerLMs. However,performancedoesdependonthelocation
(ℓ) (ℓ)
oftheedit. Forexample,inthecaseofW andW ,thescore(S)andgeneralization(PS)drops
g o
afterlayer43. Thisisconsistentwithourfindingsfromthepath-blockingexperimentinFigure5a.
(ℓ)
WealsofindthateditstoW havepoorgeneralization(PS)inearlylayers,whereashighPScan
a
(ℓ) (ℓ)
beachievedatearlylayersbymodifyingeitherW orW ,consistentwiththeirhigherindirect
g o
effectsasseeninFigure5a.
(ℓ)
WhereistherightplacetoapplyROMEonMamba? Figure3couldsuggestW ,sincethecausal
g
effectofg statesismostlyconcentratedatthesubjectlasttoken,similartothebehaviorofMLPs
i
in transformers (Meng et al., 2022a). Consistent with this is the architectural fact that, just as
(ℓ)
transformers’W connectstoattentionmodulesonlythroughtheresidualstream,theoutputof
down
(ℓ)
W doesnotflowthroughtheConv+SSMmodule—amodulethatotherworkhassuggestedmight
g
playrolesimilartothatplayedbyattentionheadsintransformers(Grazzietal.,2024). And,indeed,
(ℓ)
wefindthatROMEcansuccessfullyinsertfactsbymodifyingW . OntheotherhandFigure6a
g
(ℓ)
revealssuddendropsinefficacyandgeneralizationatmiddlelayergates,suggestingthatW may
g
beanunreliablemediatoratsomelayers. Ourexperimentsfurthershowthatthebestperformance
(ℓ)
forROMEisempiricallyachievedbymodifyingW . Thisisconsistentwiththefactthato states
o i
showastrongercausaleffectatthesubjectlasttokenthang statesdo(seeFigure3a). Additionally,
i
ROMEachievesbettergeneralization(PS),competitivespecificity(NS),andanoverallbetterscore
(ℓ) (ℓ)
(S)withW . WehypothesizethatthestrongperformanceofW maybeduetothetheseparation
o o
ofrolesbetweenearly-midandlaterlayersobservedinFigures2b,3a,and5. AlsoseeAppendixB
(ℓ)
whereweisolatethecontributionofW bysubtractingIE +IE fromIE ,whichreveala
o (ℓ) (ℓ) (ℓ)
s g o
i i i
(ℓ)
criticalroleofW inearly-midlayersatsubjectlasttokenpositionwhilemediatingafact.
o
WeplotROMEperformanceforasimilarsizedPythiamodelonFigure6bforcomparison.
5 LINEARITY OF RELATION EMBEDDING (LRE)
WithactivationpatchingwecanidentifywherefactsarelocatedinsideaLM.Wearealsointerested
inunderstandinghowLMextractsthisinformationgivenx = (s,r). Figures2band5showaclear
separation of roles in early-mid and later layers in Mamba. We observe a similar phenomena in
autoregressivetransformerLMs(Mengetal.,2022a;b;Gevaetal.,2023). AccordingtoGevaetal.
(2023),intransformerLMs,thesubjectentityrepresentations,atthesubjectlasttokenposition,goes
throughanenrichmentprocess,mediatedbytheMLPintheearly-midlayers,wheresispopulated
withdifferentfacts/attributesrelevanttothesubjectentity s. And, thenatthelasttokenposition
attentionmodulesperformaqueryintheenrichedstoextracttheanswertothepromptx = (s,r).
Hernandezetal.(2023)approximatethequeryperformedintheenrichedsforaspecificrelationr
bytakingthefirstorderTaylorseriesapproximation(LRE)oftheLMcomputationFas-
F(s,r) ≈ βJ s+b
ρ
(cid:34) (cid:12) (cid:35) (cid:34) (cid:12) (cid:35)
whereJ=E si,r ∂ ∂F s(cid:12) (cid:12)
(cid:12)
, b =E si,r F(s,r)− ∂ ∂F
s
s(cid:12) (cid:12)
(cid:12)
, (8)
(si,r) (si,r)
βisascalar,and ρistherankofJ
Hernandezetal.(2023)showthatforarangeofdifferentrelationsitispossibletoachieveaLRE
thatisfaithfultothemodelcomputationFbyaveragingtheapproximationsofJandbcalculatedon
justn =5examples. WeutilizeLREtounderstandthecomplexityofdecodingfactualrelationsin
7Preprint. Underreview.
LRE faithfulness for factual relations in Mamba-2.8B
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Figure7: LREfaithfulnesswithn=5samplesforallthefactualrelations.Horizontalredlinesindicaterandom
choicebaseline(intheRELATIONSdataset).
Mamba. Wefindthehyperparametersβ,ρandthelayerℓ(wheretoextractenrichedsfrom)using
gridsearch. Formathematicalandimplementationdetails,seeHernandezetal.(2023).
WeplotthefaithfulnessofLREwithn =5samplesonFigure7. faithfulnessrepresentstheportion
offactsinrthatcanbecorrectlyretrievediftheLMcomputationF(s,r)isreplacedwithLRE(s).
We only calculate LRE for the factual relations in the RELATIONS dataset. Figure 7 shows that
onlyfor10outof26factualrelationsalinear LRE canachievemorethan50%faithfulness. For
comparison,inthesamesizedPythia-2.8bLREachives>50%faithfulnessfor11factualrelations
(seeAppendixE).And,inbothMambaandPythia,LREfailstoachievegoodfaithfulnessforthe
relationswheretherange(thenumberofuniqueanswers)islarger.
6 AttentionKnock-outinMamba?
IntransformerLMs,attentionmodulesbringinformationfromtheothertokenpositions. Eachofthe
attentionheadsinanattentionmoduleattn(ℓ) calculatesanattentionmatrixL,whereL quantifies
k,q
howmuchattentionisbeingpaidtotheqth tokenbythekth token. SeeVaswanietal.(2017)for
detailsontheattentionoperation. Wecanblockinformationflowfromqth tokentokth viaaspecific
attentionheadbysimplysettingL := −∞intheforwardpass. Thispatch-dependentattention
k,q
blocking (or attention “knock-out”) is also a form of causal mediation analysis and it has been
effectiveinunderstandingtheinformationflowintransformerLMs(Gevaetal.,2023;Wangetal.,
2022;Toddetal.,2023).
ForMamba,Alietal.(2024)showthattheamountofinformationretainedinthe kth tokenstate
s(ℓ) ,fromtheconvolvedstateatqth tokenc(ℓ) (whereq < k),aftertheselective-SSMoperation(see
k q
Equations4and5)canbevisualizedasanattentionmatrixperchannel(dim). Sincetheselective-
(ℓ) (ℓ)
SSMoperationislinear,theinformationretainedins fromc canbecalculatedaccuratelyas
k q
α˜ =C(ℓ)(cid:16) ∏k A(ℓ)(cid:17) B(ℓ) c(ℓ) ,whereA(ℓ) ,B(ℓ) ,andC(ℓ) areinput-dependentparametersfor
k,q k i=q+1 i q q i i i
the ith token. See Gu & Dao (2023) and Ali et al. (2024) for details. We ask: can we block the
informationflowfromtheqth tokentothekth tokeninMambabysubtractingoutα˜ froms(ℓ) ? If
k,q k
so,attentionknockoutexperimentsinMambabecomefeasible.
Wefindthatblockinginformationflowfromtheqth tokentothekth tokenintheConv+SSM
(ℓ)
operationcanbedifficult. Notethat,sincec isaconvolvedstatewithareceptivefieldofsize4
q
(ℓ) (ℓ) (ℓ) (ℓ)
inMamba-2.8b,thestatesc ,c ,andc retaininformationfroma . And,thesestatescan
q+1 q+2 q+3 q
(ℓ) (ℓ) (ℓ)
“leak”informationabouta tos . Tostopthisleakage,wewouldwanttosubtractfroms all
q k k
8
ssenlufhtiaF
egaugnal
evitan
nosrep
tnenitnoc
no
kramdnal
egaugnal
yrtnuoc
ytic
latipac yrtnuoc
ytisrevinu
nosrep
trops
orp syalp
ytic
tsegral yrtnuoc
dnab
fo regnis
dael
nosrep
noitapucco
nosrep
noitisop
trops
nosrep
yrtnuoc
ni ytic
tnemurtsni
syalp
nosrep
raey
htrib tnediserp
ynapmoc
yb
tcudorp
nosrep
orehrepus
ycnerruc
yrtnuoc
yrtnuoc
morf
doof
qh
ynapmoc
eman
noitalletsnoc
rats
sisemenhcra
orehrepus
noitulove
nomekop
rehtaf
nosrep
yrtnuoc
ni kramdnal
rehtom
nosrep
OEC
ynapmocPreprint. Underreview.
(ℓ)
theinformationretainedabouta . However,calculatingthisischallengingbecauseoftheSiLU
q
non-linearityafterConv1D(seeEquation(4)).
InAppendixD,wepresentaversionofattentionknock-outexperiment,whereweblockinformation
flowoftheqth tokentoallthefuturetokenswithConv+SSMoperation. Werecognizethatthis
interventionisnotassurgicalasblockinginformationfromqth tokentojustkth token. However,
withsomecaveats,thisexperimentleadsustodrawsomeconclusionabouttheinformationflowin
MambathatissimilartowhatGevaetal.(2023)foundforGPTLMs. SeeAppendixDfordetails.
7 RelatedWorks
Mamba. Mamba is a recent family of language models that are based on state space models
(SSMs). NeuralSSM-basedmodelshaveachievedgoodperformanceacrossdifferentmodalities,
includingvision(Nguyenetal.,2022),audio(Goeletal.,2022),andgenomicsequences(Nguyen
etal.,2023). Onlyrecently,however,withMamba,havetheybecomecompetitivewiththelanguage
modelingperformanceoftransformers(Gu&Dao,2023).Liketransformers,Mambacontainsfactual
knowledgeaboutrealworldentities(Grazzietal.,2024). However,knowledgerepresentationin
Mamba(andotherLMsbasedonSSMs)hasuptonowremainedunexplored.
TherearefewworksfocusedoninterpretingMamba. Alietal.(2024)identifyimplicitattention-like
matricesformedbyMamba’sselectivestatespacelayers. Grazzietal.(2024), whilenotstrictly
focusedoninterpretingMamba’sinternals,applylinearprobestoMamba’s(decoded)intermediate
statesduringin-contextregressiontasks. Likeus,theyfindsubstantialsimilaritiesbetweenMamba
andtransformermodels: botharchitecturespursue“iterative”strategies,withthetasklossfalling
moreorlessmonotonicallyasthelayerindexincreases.
LocatingFactualKnowledgeinLanguageModels. Tomakefactuallycorrectstatementsabout
theworldaLMhastostorefactualknowledgeaboutrealworldentitiessomewhereinitsparameters.
Understandinghowandwhereaneuralnetworkstoresknowledgeisacoreproblemforinterpretability
andithasthusbeenstudiedfromseveralperspectives(Jietal.,2021;Wangetal.,2014). Onelineof
worktrainsclassifierstoprobeforpropertiesencodedinmodelrepresentations(Ettingeretal.,2016;
Shietal.,2016;Hupkesetal.,2018;Conneauetal.,2018;Belinkovetal.,2017;Belinkov&Glass,
2019). However,theflexibilityoftheseclassifierscanleadtooverestimatingmodelknowledgeand
capabilites(Belinkov,2022). Causalmediationanalysismethods(Pearl,2022)attempttomeasurethe
causalcontributionofintermediatestatestotaskperformance. Mengetal.(2022a;b)useactivation
patchingtoidentifykeyMLPmodulesforfactualrecall,highlightingthemiddlelayersatparticular
tokenpositionsasbeingespeciallyimportant. Similarly,Gevaetal.(2023)applycausalmediation
analysistoattentionmodules,seekingtounderstandthemechanismofcross-tokenfactualinformation
flowinsidetransformerLMs.
8 Discussion
Inthispaperwehaveexploredwhethertheanalyticalmethodsandtoolsdevelopedfortransformer
LMscanalsobeappliedonMamba,arecurrentstate-spacearchitecture,andwehavecomparedthe
mechanismsoffactualrecallbetweenthetwoclassesofmodels. Ourexperimentshavebeenlimited
toMamba-2.8b,thelargestavailableLMofthatfamily,andcomparisonstoPythia-2.8b(selected
overLlamaorGPTmodelsastheydonotofferpretrainedmodelsofsimilarsize).
Withactivationpatchingwehavefoundthat, similartoautoregressivetransformerLMs, Mamba
showssignsoflocalizationatthelastsubjecttokenandatspecificlayerrangeswhilerecallinga
fact. Wehaveappliedrank-onemodelediting(ROME)torewriteafactinspecificparametersin
Mamba. Although,unliketransformers,MambahasnoMLPmodules,wefindthattheirW weights
o
canreceiveeditswithgoodgeneralizationandspecificityatarangeoflayers. Thenwehavestudied
the linearity of the embeddings of factual relations in Mamba and have found that many can be
wellapproximatedbyLRE,againresemblingtransformerLMs. AlthoughMambahasnoattention
modules,wehavebeenabletopartiallyadaptthetoolsofattentionknock-outusinganSSMblocking
technique,andthisrevealsinformationflowssimilartotransformers. Insummary,wefindthatmany
ofthetoolsusedtointerpretandeditlargetransformerscanbeadaptedtoworkwithMambaaswell,
andthatdespitethearchitecturaldifferences,whenitcomestofactualrecall,Mambasharesmany
similaritieswithtransformerLMs.
9Preprint. Underreview.
Ethics
By exploring the factual recall mechanism in Mamba, we potentially improve its transparency,
enablingoversightandcontrol. However,theabilitytomodifyfactsdirectlyinthemodelbringswith
itthepotentialforabuse,suchasaddingmaliciousmisinformationorbias.
Reproducibility
Weranallexperimentsonworkstationswitheither80GBNVIDIAA100GPUsor48GBA6000
GPUs,usingtheHuggingFaceTransformerslibrary(Wolfetal.,2019)andPyTorch(Paszkeetal.,
2019). WemakeuseofpubliclyavailabledatasetsCOUNTERFACTandRELATIONSinthiswork. We
willmakeoursourcecodeopensourceaftertheanonymityperiodhasended.
References
AmeenAli,ItamarZimerman,andLiorWolf. Thehiddenattentionofmambamodels. arXivpreprint
arXiv:2403.01590,2024.
James A Anderson. A simple neural network generating an interactive memory. Mathematical
biosciences,14(3-4):197–220,1972.
Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational
Linguistics,48(1):207–219,2022.
Yonatan Belinkov and James Glass. Analysis methods in neural language processing: A survey.
TransactionsoftheAssociationforComputationalLinguistics,7:49–72,2019.
YonatanBelinkov,NadirDurrani,FahimDalvi,HassanSajjad,andJamesGlass. Whatdoneural
machinetranslationmodelslearnaboutmorphology? arXivpreprintarXiv:1704.03471,2017.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternational
ConferenceonMachineLearning,pp.2397–2430.PMLR,2023.
KyunghyunCho,BartVanMerriënboer,DzmitryBahdanau,andYoshuaBengio. Onthepropertiesof
neuralmachinetranslation: Encoder-decoderapproaches. arXivpreprintarXiv:1409.1259,2014.
AlexisConneau,GermanKruszewski,GuillaumeLample,LoïcBarrault,andMarcoBaroni. What
youcancramintoasinglevector: Probingsentenceembeddingsforlinguisticproperties. arXiv
preprintarXiv:1805.01070,2018.
JamesDurbinandSiemJanKoopman. Timeseriesanalysisbystatespacemethods,volume38. OUP
Oxford,2012.
YanaiElazar,NoraKassner,ShauliRavfogel,AbhilashaRavichander,EduardHovy,HinrichSchütze,
andYoavGoldberg. Measuringandimprovingconsistencyinpretrainedlanguagemodels. Trans-
actionsoftheAssociationforComputationalLinguistics,9:1012–1031,2021.
AllysonEttinger,AhmedElgohary,andPhilipResnik. Probingforsemanticevidenceofcomposition
bymeansofsimpleclassificationtasks. InProceedingsofthe1stworkshoponevaluatingvector-
spacerepresentationsfornlp,pp.134–139,2016.
Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D.
Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks.
CoRR,abs/2112.00826,2021. URLhttps://arxiv.org/abs/2112.00826.
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual
associationsinauto-regressivelanguagemodels. arXivpreprintarXiv:2304.14767,2023.
10Preprint. Underreview.
KaranGoel,AlbertGu,ChrisDonahue,andChristopherRe. It’sraw! Audiogenerationwithstate-
spacemodels. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,GangNiu,
andSivanSabato(eds.),Proceedingsofthe39thInternationalConferenceonMachineLearning,
volume162ofProceedingsofMachineLearningResearch,pp.7616–7633.PMLR,17–23Jul
2022. URLhttps://proceedings.mlr.press/v162/goel22a.html.
RiccardoGrazzi,JulienSiems,SimonSchrodi,ThomasBrox,andFrankHutter. IsMambaCapable
ofIn-ContextLearning?,2024. URLhttp://arxiv.org/abs/2402.03170.
AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXiv
preprintarXiv:2312.00752,2023.
AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswithstructured
statespaces. arXivpreprintarXiv:2111.00396,2021.
PeterHase,MohitBansal,BeenKim,andAsmaGhandeharioun. Doeslocalizationinformediting?
surprisingdifferencesincausality-basedlocalizationvs.knowledgeeditinginlanguagemodels.
AdvancesinNeuralInformationProcessingSystems,36,2024.
EvanHernandez,ArnabSenSharma,TalHaklay,KevinMeng,MartinWattenberg,JacobAndreas,
YonatanBelinkov,andDavidBau. Linearityofrelationdecodingintransformerlanguagemodels.
arXivpreprintarXiv:2308.09124,2023.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780,1997.
DieuwkeHupkes, SaraVeldhoen, andWillemZuidema. Visualisationand’diagnosticclassifiers’
reveal how recurrent and recursive neural networks process hierarchical structure. Journal of
ArtificialIntelligenceResearch,61:907–926,2018.
ShaoxiongJi,ShiruiPan,ErikCambria,PekkaMarttinen,andSYuPhilip. Asurveyonknowledge
graphs: Representation,acquisition,andapplications. IEEETransactionsonNeuralNetworksand
LearningSystems,33(2):494–514,2021.
TeuvoKohonen. Correlationmatrixmemories. IEEEtransactionsoncomputers,100(4):353–359,
1972.
SiemJanKoopman,NeilShephard,andJurgenADoornik. Statisticalalgorithmsformodelsinstate
spaceusingssfpack2.2. TheEconometricsJournal,2(1):107–160,1999.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
associationsingpt. AdvancesinNeuralInformationProcessingSystems,35:17359–17372,2022a.
KevinMeng,ArnabSenSharma,AlexAndonian,YonatanBelinkov,andDavidBau. Mass-editing
memoryinatransformer. arXivpreprintarXiv:2210.07229,2022b.
Neel Nanda, Senthooran Rajamanoharan, János Kramár, and Rohin Shah. Fact
finding: Attempting to reverse-engineer factual recall on the neuron level,
2023. URL https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/
fact-finding-attempting-to-reverse-engineer-factual-recall.
EricNguyen,KaranGoel,AlbertGu,GordonDowns,PreeyShah,TriDao,StephenBaccus,and
Christopher Ré. S4nd: Modeling images and videos as multidimensional signals with state
spaces. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Ad-
vances in Neural Information Processing Systems, volume 35, pp. 2846–2861. Curran Asso-
ciates,Inc.,2022. URLhttps://proceedings.neurips.cc/paper_files/paper/
2022/file/13388efc819c09564c66ab2dc8463809-Paper-Conference.pdf.
EricNguyen,MichaelPoli,MarjanFaizi,ArminThomas,CallumBirch-Sykes,MichaelWornow,
AmanPatel,ClaytonRabideau,StefanoMassaroli,YoshuaBengio,StefanoErmon,StephenA.
Baccus,andChrisRé. Hyenadna: Long-rangegenomicsequencemodelingatsinglenucleotide
resolution,2023.
11Preprint. Underreview.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,
2019.
JudeaPearl. Directandindirecteffects. InProbabilisticandcausalinference: theworksofJudea
Pearl,pp.373–392.2022.
Xing Shi, Inkit Padhi, and Kevin Knight. Does string-based neural mt learn source syntax? In
Proceedingsofthe2016conferenceonempiricalmethodsinnaturallanguageprocessing, pp.
1526–1534,2016.
EricTodd,MillicentLLi,ArnabSenSharma,AaronMueller,ByronCWallace,andDavidBau.
Functionvectorsinlargelanguagemodels. arXivpreprintarXiv:2310.15213,2023.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessing
systems,30,2017.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,
and Stuart Shieber. Investigating gender bias in language models using causal mediation
analysis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
vancesinNeuralInformationProcessingSystems,volume33,pp.12388–12401.CurranAsso-
ciates,Inc.,2020. URLhttps://proceedings.neurips.cc/paper_files/paper/
2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf.
KevinWang,AlexandreVariengien,ArthurConmy,BuckShlegeris,andJacobSteinhardt. Inter-
pretabilityinthewild: acircuitforindirectobjectidentificationingpt-2small. arXivpreprint
arXiv:2211.00593,2022.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In Proceedings of the AAAI conference on artificial intelligence,
volume28,2014.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers:
State-of-the-artnaturallanguageprocessing. arXivpreprintarXiv:1910.03771,2019.
Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models:
Metricsandmethods. arXivpreprintarXiv:2309.16042,2023.
12Preprint. Underreview.
A Datasets
Weusetwodatasets;COUNTERFACTbyMengetal.(2022a)andRELATIONSbyHernandezetal.
(2023)inthiswork.
A.1 COUNTERFACT
Mengetal.(2022a)developedtheCOUNTERFACTdatasetforevaluatingtheefficacyofcounterfactual
editsinlanguagemodels. ItwaspreparedbyadaptingPARAREL(Elazaretal.(2021))andscraping
Wikidata. The dataset contains 21,919 requests {s,r,o,o∗,π∗} where o is the correct answer to
the prompt x = (s,r), o∗ is the counterfactual edit request, and π∗ ∼ P(s,r) is a paraphrase
of the prompt x = (s,r) to test for generalizability (PS). Each of the records also contain some
neighborhoodpromptsπ totestforspecificity(NS)andsomegenerationpromptsπ totestifLM
N G
generationpost-editisfluentandconsistentwiththeedit. PleaserefertoMengetal.(2022a)for
detailsonthecurationofthisdataset.
We evaluate ROME performance in Mamba-2.8b and Pythia-2.8b on the first 2000 records from
COUNTERFACT.
A.2 RELATIONS
TheRELATIONSdatasetintroducedinHernandezetal.(2023)consistsof47relationsof4types;
factual,linguistic,bias,andcommonsense. Arelationrisanassociationbetweentwoentities. For
example,therelation,r =professionallyplayedthesportconnectsthesubjects =MichaelJordan
withtheobjecto =basketball. Thedatasetcontainsasetof(s,o)foreachrelationr.
Inthescopeofthispaper, weonlyutilizethe26factualrelationsfromthisdataset. Weevaluate
LRE in Mamba and Pythia for all the 26 factual relations. We also use this dataset for locating
keyfact-mediatingstatesinSection3andAppendixC.Werandomlysample400examples(s,r,o)
across6differentfactualrelations-placeincity,countrycapitalcity,personoccupation,playspro
sport,companyhq,andproductbycompany. Foreachoftheseexampleswerandomlyselectanother
examplewithinthesamerelation(s∗,r,o∗)suchthats ̸= s∗ ando ̸= o∗. Theaverageindirecteffect
(IE) of applying activation patching over these 400 examples is depicted on Figures 2b, 3, 5 for
Mamba-2.8b)andonFigure9(forPythia-2.8b).
(ℓ)
B IsolatingTheContributionofW
o
(ℓ) (ℓ) (ℓ)
RecallfromFigure1andEquation(2)thatwheno isrestored,thes and g arerestoredas
i i i
(ℓ)
well. ToisolatethecontributionofonlyW wesubtractoutIE +IE fromIE andplotthe
o (ℓ) (ℓ) (ℓ)
s g o
i i i
resultsonFigure8. Noticethat,subtractingIE cancelsoutthehighindirecteffectatthelatesite
(ℓ)
s
i
shownbylaterlayersatthelasttokenposition. But,togetherIE +IE cannotcancelouthigh
(ℓ) (ℓ)
s g
i i
(ℓ)
IE ofearly-midlayersatthelastsubjecttoken,reconfirmingthemediatingroleofW inthose
(ℓ) o
o
i
layerswhilerecallingafact.
(a)Av pe rr ea fig xe IE of restoring o( il)over 400 prompts
0.5
((b)Average IE of restoring g( il)
0.5
(c)Average IE of restoring si(l)
0.5)
prefix
IEo i(l)- (IEg i(l)+ IE s i(l))
0.5
subject_[:-2]* 0.4 - 0.4 + 0.4 =subj_[:-2]* 0.4
subject_2nd_last* 0.3 0.3 0.3 subj_[-2]* 0.3
subject_last* 0.2 0.2 0.2 sub_last* 0.2
further tokens 0.1 0.1 0.1 other tokens 0.1
last token last token
05 10 15 20 25 30 35 40 45 50 55
IE0.0
05 10 15 20 25 30 35 40 45 50 55
IE0.0
05 10 15 20 25 30 35 40 45 50 55
IE0.0
05 10 15 20 25 30 35 40 45 50 55
IE0.0
center of interval of 10 patched layers center of interval of 10 patched layers center of interval of 10 patched layers
(ℓ)
Figure 8: Isolating the contribution of W . IE +IE subtracted from IE . Notice that
o (ℓ) (ℓ) (ℓ)
s g o
(cid:0) (cid:1) i i (cid:0) i
IE − IE +IE stillshowshighercausaleffectattheearliersite morepronouncedthan
(ℓ) (ℓ) (ℓ)
o s g
i (cid:1) i i
IE whilethehighcausaleffectatthelatersitecancelsout.
(ℓ)
g
i
13Preprint. Underreview.
C LocatingKeyModulesinPythia-2.8b
(a)Average IE of restoring h(l)over 400 prompts (b)Average IE of restoring MLP (b)Average IE of restoring ATTN
i
0.8 0.8 0.8
prefix
subject_[:-2]* 0.6 0.6 0.6
subject_2nd_last*
0.4 0.4 0.4
subject_last*
further tokens 0.2 0.2 0.2
last token
0.0 0.0 0.0
0 5 10 15 20 25 IE 0 5 10 15 20 25 IE 0 5 10 15 20 25 IE
single restored layer within Pythia-2.8b center of interval of 10 patched layers center of interval of 10 patched layers
Figure9: Averageindirecteffectofresidualstate,MLP,andattentionoutputsinPythia-2.8bover
400facts. ForMLPandattentionoutputsawindowof10layersaroundℓisrestoredasrestoringfor
justonelayerbarelyshowvisiblepatterns.
0.75 (a) i = subj last
0.50
0.25
0.00
(b) i = prompt last
1.00
0.75
0.50
0.25
0.00
0 5 10 15 20 25 30
IE with
h(l)restored
ATTN blocked MLP blocked
i i i
Figure10: ImpactofablatingATTN orMLP onIE for(a)subjectlastand(b)promptlasttokenpositions
i i h(ℓ)
i
onPythia-2.8b
D BlockingInformationPropagationWith s Paths
i
In Section 6, we discuss why surgically performing attention knock-out experiments on Mamba
is challenging due to certain architectural choices - we cannot block information flow from the
qth tokentoonlythekth tokenbecauseoftheSiLUnon-linearityinEquation(4). However,fora
layerℓwecanblockinformationpropagationviaConv+SSMfromtheqth tokentoallthefuture
(ℓ)
tokens(k > q)bymeanablatinga . Forawindowof10layersaroundℓweblock-outinformation
q
propagation of the subject, non-subject, and the prompt-last token positions. The results plotted
in Figure 11 show relative change in p(o) by blocking out qth token across different layers as
(cid:16) p(cid:0) o|a( qℓ) :=E(cid:2) a(ℓ)(cid:3)(cid:1) −p(o)(cid:17)
/p(o).
Figure11leadsustodrawsimilarconclusionsaboutinformationflowinMambaasGevaetal.(2023)
observeintransformerLMs.
(a) The purple lines show that blocking out non-subject information flow in early middle layers
canbringdown p(o)byupto50%. Non-subjecttokensareusedtospecifytherelationr. This
observationleadsustobelievethatMambapropagatesrelationspecificinformationtofuture
tokensusingConv+SSMoperationinearly-middlelayers
(b) Interestingly,thegreenlines(blockingthesubjectinformationflow)showstwovalleys-
1. ThefirstvalleyattheearlylayersisnotsurprisingasMambaneedstocollateinformation
fromallthesubjecttokensinearlylayerstorecognizeasubjectentitysconsistingofmultiple
tokens.
2. However,thevalleyatlayers43−48suggestthatMambausesConv+SSMpathsinthose
layerstopropagatecriticalinformationfromthesubjecttolatertokens. Thisalignswith
14
EI
egarevA
EI
egarevAPreprint. Underreview.
0.50
0.25
0.00
0.25
0.50
0.75
1.00
0 5 10 15 20 25 30 35 40 45 50 55 60
Layer
subject non-subject prompt_last
Figure11: Relativechangein p(o)wheninformationflowtofuturetokensvias pathsisblocked
i
fromeithersubject,non-subject,ortheprompt_lasttokenpositions. Foreachlayerℓ,s pathswere
i
blockedforawindowof10layersaroundℓ.
Figures5band3c,wheres statesinthoselayersshowhighindirecteffects,indicatingtheir
i
crucialrolewhilerecallingafact.
Withthisexperiment,wecannotmakestrongclaimsontheroleofthelasttokenposition(prompt-last)
likeGevaetal.(2023). Asweblockoutinformationtoallfuturetokens,theintermediatestatesin
betweentheablatedtokenandthelasttokenaregettingcorruptedaswell.
15
)o(p
ni
egnahc
evitaleRPreprint. Underreview.
E LRE inPythia-2.8b
LRE faithfulness for factual relations in Mamba-3B
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
LRE faithfulness for factual relations in Pythia-3B
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Figure12: Relation-wiseLREfaithfulnesstotheLMdecodingfunctionF. Horizontalredlinesper
relation indicate random-choice baseline. We only present results for the factual relations in the
RELATIONSdataset.
16
ssenlufhtiaF
ssenlufhtiaF
egaugnal
evitan
nosrep
tnenitnoc
no
kramdnal
egaugnal
yrtnuoc
ytic
latipac
yrtnuoc
ytisrevinu
nosrep
trops
orp
syalp
ytic
tsegral
yrtnuoc
dnab
fo
regnis
dael
nosrep
noitapucco
nosrep
noitisop
trops
nosrep
yrtnuoc
ni ytic
tnemurtsni
syalp
nosrep
raey
htrib
tnediserp
ynapmoc
yb
tcudorp
nosrep
orehrepus
ycnerruc
yrtnuoc
yrtnuoc
morf
doof
qh
ynapmoc
eman
noitalletsnoc
rats
sisemenhcra
orehrepus
noitulove
nomekop
rehtaf
nosrep
yrtnuoc
ni kramdnal
rehtom
nosrep
OEC
ynapmocPreprint. Underreview.
F LRE PerformanceOverDifferentRelations
Besides faithfulness Hernandez et al. (2023) introduced another metric causality to measure the
performanceofLRE. SinceLREisalinearfunction,itisinvertible. Assumethatforafact(s,r,o)
LRE canfaithfullyreplaceLMcomputation F(s,r). Thengiventherepresentationo∗ ofanother
object o, J−1(o∗−o) should give us a ∆s, such that when added to s, s˜ := s+∆s, the model
computationF(s˜,r)shouldgenerateo∗. SeeHernandezetal.(2023)fordetailsonthis.
Mamba-2.8b
country capital city landmark in country plays pro sport person occupation
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
person plays instrument company CEO star constellation name person mother
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
Pythia-2.8b
country capital city landmark in country plays pro sport person occupation
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
person plays instrument company CEO star constellation name person mother
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
Faithfulness Causality
Figure13: ForMamba,weonlyperformsweeptilllayer48,asFigure5suggestsnegligibleactivity
forlaterlayersatthesubjectlasttoken
17
erocS
erocS
erocS
erocS
bme
bme
bme
bme
6
6
3
3
6
6
21
21
9
9
81
81
21
21
42
42
51
51
03
03
81
81
12
12
63
63
42
42
24
24
72
72
84
84
03
03
bme
bme
bme
bme
6
6
3
3
6
6
21
21
9
9
81
81
21
21
42
42
51
51
81
81
03
03
12
12
63
63
42
42
24
24
72
72
03
03
84
84
bme
bme
bme
bme
6
6
3
3
6
6
21
21
9
9
81
81
21
21
42
42
51
51
81
81
03
03
12
12
63
63
42
42
24
24
72
72
03
03
84
84
bme
bme
bme
bme
6
6
3
3
6
6
21
21
9
9
81
81
21
21
42
42
51
51
81
81
03
03
12
12
63
63
42
42
24
24
72
72
03
03
84
84Preprint. Underreview.
G ActivationPatchingIndirectEffectsonMamba-2.8b
restoringh(l)of a single layer restoringo(l)of 10 layers around l restoringg(l)of 10 layers around l restorings(l)of 10 layers around l
i i i i
0.5 0.5 0.5 0.5
[PAD]
Michael/ Pe 0.4 0.4 0.4 0.4
Jordan/le
professionally 0.3 0.3 0.3 0.3
played 0.2 0.2 0.2 0.2
the
sport 0.1 0.1 0.1 0.1
of
0.0 0.0 0.0 0.0
0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE
0.5 0.5 0.5 0.5
[PAD]
Harvard/ Oxford 0.4 0.4 0.4 0.4
University/ University
is 0.3 0.3 0.3 0.3
located
in 0.2 0.2 0.2 0.2
the
city 0.1 0.1 0.1 0.1
of 0 5 10 15 20 25 30 35 40 45 50 55 IE 0.0 0 5 10 15 20 25 30 35 40 45 50 55 IE 0.0 0 5 10 15 20 25 30 35 40 45 50 55 IE 0.0 0 5 10 15 20 25 30 35 40 45 50 55 IE 0.0
0.5 0.5 0.5 0.5
[PAD]
By 0.4 0.4 0.4 0.4
profession
0.3 0.3 0.3 0.3
Bruce/ Harper
0.2 0.2 0.2 0.2
Lee/ Lee
was 0.1 0.1 0.1 0.1
a
0.0 0.0 0.0 0.0
0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE
0.5 0.5 0.5 0.5
[PAD]
The 0.4 0.4 0.4 0.4
capital
of 0.3 0.3 0.3 0.3
Canada/ Japan
is 0.2 0.2 0.2 0.2
the
city 0.1 0.1 0.1 0.1
of 0.0 0.0 0.0 0.0
0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE
0.5 0.5 0.5 0.5
[PAD]
The 0.4 0.4 0.4 0.4
head
quarter 0.3 0.3 0.3 0.3
of
Microsoft/ Google 0.2 0.2 0.2 0.2
is
located 0.1 0.1 0.1 0.1
in 0.0 0.0 0.0 0.0
0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE 0 5 10 15 20 25 30 35 40 45 50 55 IE
18