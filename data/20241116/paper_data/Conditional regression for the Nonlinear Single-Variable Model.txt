Conditional regression for the Nonlinear Single-Variable Model
Conditional regression for the Nonlinear Single-Variable
Model
Yantao Wu ywu212@jhu.edu
Department of Mathematics
Johns Hopkins University
Baltimore, MD 21218-2683, USA
Mauro Maggioni mauromaggionijhu@icloud.com
Department of Mathematics and Department of Applied Mathematics & Statistics
Johns Hopkins University
Baltimore, MD 21218-2683, USA
Abstract
Several statistical models for regression of a function F on Rd without the statistical
and computational curse of dimensionality exist, for example by imposing and exploit-
ing geometric assumptions on the distribution of the data (e.g. that its support is low-
dimensional), or strong smoothness assumptions on F, or a special structure F. Among
the latter, compositional models assume F = f ◦g with g mapping to Rr with r ≪ d,
have been studied, and include classical single- and multi-index models and recent works
on neural networks. While the case where g is linear is rather well-understood, much less
isknownwhen g isnonlinear, andinparticularforwhich g’sthecurseofdimensionalityin
estimating F, or both f and g, may be circumvented. In this paper, we consider a model
F(X):=f(Π X)whereΠ :Rd →[0,len ]istheclosest-pointprojectionontotheparam-
γ γ γ
eter of a regular curve γ : [0,len ] → Rd and f : [0,len ] → R1. The input data X is not
γ γ
low-dimensional, far from γ, conditioned on Π (X) being well-defined. The distribution
γ
of the data, γ and f are unknown. This model is a natural nonlinear generalization of
the single-index model, which corresponds to γ being a line. We propose a nonparametric
estimator, based on conditional regression, and show that under suitable assumptions, the
strongest of which being that f is coarsely monotone, it can achieve the one-dimensional
optimalmin-maxratefornon-parametricregression,uptothelevelofnoiseintheobserva-
tions,andbeconstructedintimeO(d2nlogn). Alltheconstantsinthelearningbounds,in
the minimal number of samples required for our bounds to hold, and in the computational
complexity are at most low-order polynomials in d.
Keywords: Nonparametric regression; compositional models; single-index model.
1 Introduction
We consider the standard regression problem of estimating a function F : Rd → R1 from n
samples {(X ,Y )}n , where X ’s are i.i.d. realizations of a predictor variable X ∈ Rd with
i i i=1 i
distribution ρ , and ζ are realizations (independent among themselves and of the X ’s), of
X i i
a random variable ζ modeling observational noise, and
Y = F(X )+ζ .
i i i
In the general nonparametric and distribution-free setup where we only know that F ∈
Cs(Rd) is H¨older continuous with exponent s > 0 and, say, compacted supported, the min-
1
4202
voN
41
]LM.tats[
1v68690.1142:viXraYantao Wu and Mauro Maggioni
max nonparametric rate for estimating F in L2(ρ) is n− 2ss +d, see (Binev et al., 2023; Gy¨orfi
etal.,2002)andreferencestherein. Kernelestimatorsattainthislearningratewithproperly
chosen bandwidth and kernel, as so do a variety of other well-understood estimators based
on Fourier or multiscale decompositions, see (Gy¨orfi et al., 2002; Binev et al., 2005, 2007)
and the numerous references therein. This rate deteriorates dramatically as the dimension
d of the ambient space increases: this is an instance of the curse of dimensionality. As no
estimator can achieve a faster learning rate, in the min-max sense, for all such functions,
and yet in many applications d is very large, it is of interest to consider special classes of
regression problems where the curse of dimensionality may be avoided.
Intrinsically low-dimensional models. In one direction, one may make geometric as-
sumptionsaboutthedistributionρ oftheinputdataX , forexamplethatρ issupported
X i X
on a low-dimensional manifold M, say of dimension r ≪ d, while F is, say, a generic s-
H¨older function on M. In these settings, estimators exist that converge at an optimal rate
concerning the intrinsic dimension r, see (Bickel and Li, 2007; Kpotufe, 2011; Kpotufe and
Garg, 2013; Liao et al., 2016, 2022; Liu et al., 2024a), with at least some of them with
associated fast algorithms, for example the estimators in (Liao et al., 2016, 2022) can be
constructed in time O(Crdsnlogn) and they achieve, adaptively, min-max optimal rate on
large function spaces with unknown regularity possibly varying across locations and scales.
Functions with high degree of regularity. In a different direction, one may con-
sider function classes with smaller complexity (e.g., as measured by metric entropy) than
s-H¨older functions on Rd. A straightforward but highly limiting assumption is that the
H¨older exponent s is proportional to d so that the min-max rate above is independent of
d. Larger function classes are obtained by imposing integrability of the Fourier transform
(Barron,1993), orstrongmixed-smoothness(Str¨omberg,1998), forexamplebyusinghighly
anisotropicBesovspaces. Thesechoiceshoweverstillrequireadegreeofsmoothnessincreas-
ing with d in order to avoid the curse of dimensionality. For example in these anisotropic
Besov spaces, functions need to have O(1) axis-oriented directions in which they have “reg-
ular, O(1) smoothness”, and have “O(d) smoothness” in all other directions. In the context
ofdeepneuralnetworks, SuzukiandNitanda(2024)demonstratethatfunctionsinthisclass
can be well-approximated well, with suitable architectures, in a way that avoids the curse
of dimensionality if the degree of anisotropic smoothness is sufficiently large, while leaving
open the aspects of learning and optimization.
Compositional models. Yet other functional classes are obtained by imposing structural
assumptions, which are often in the form of compositional models. For example assume that
F = f ◦g , with g : Rd → Rr and f : Rr → R1
having some H¨older regularity, and r ≤ d. For general function compositions, (Juditsky
et al., 2009) proves that in some situations, when g is sufficiently smooth, there is an im-
proved min-max nonparametric rate of estimation of F = f◦g, but still subject to the curse
of dimensionality. Recent works combine anisotropic smoothness with composability, espe-
cially in the context of approximators or estimators constructed with deep neural networks.
In this direction, (Schmidt-Hieber, 2020; Shen et al., 2021) consider spaces of functions
that are compositions of low-dimensional functions with anisotropic smoothness conditions,
studying the dependence of approximability and learning risk on the dimensionality of the
spacesinvolvedandthesmoothnessofthecorrespondingfunctions. Unfortunatelytheinner-
2Conditional regression for the Nonlinear Single-Variable Model
most such function is defined on the original high-dimensional space Rd, so in general these
results do not circumvent the curse of dimensionality, unless again the (anisotropic, in gen-
eral) regularity of such function scales with d. Another direction is to design different types
of neural networks that exploit nonlinear compositions, see (Lai and Shen, 2021; Liu et al.,
2024b). Most of these results only address whether a function can well-approximated by a
neural network, and do not cover the learning problem nor address computational aspects,
in particular whether some optimization algorithm can find (efficiently?) the parameters of
the desired estimator. Another aspect that is often left unaddressed is the dependency of
constants on the ambient dimension, which is unfortunately often exponential: see Shamir
(2020) for a discussion of some of these aspects, and Shen et al. (2021) for bounds on such
constants in some of these cases. In the parametric setting, models where g is a polynomial
and f is a function in a known finite-dimensional space have been considered; for example
Wang et al. (2024); Lee et al. (2024) (and related work referenced therein) consider special
classes of polynomials g for which a customized layer-by-layer training of a neural network
leads to estimators of F that are not cursed by the dimensionality.
Single- and multi-index models. Classical examples of structural assumptions based
on function composition for which the curse of dimensionality can provably be avoided
include single- and multi-index models, as well as generalized linear models (Stone, 1982;
Hastie and Tibshirani, 2014; Breiman and Friedman, 1985; Horowitz and Mammen, 2007).
In the case when g = G : Rd → Rr is a linear operator, this is called the multi-index
model, and it implies that the function F ∈ Cs(Rd) only depends on a small number r of
linear features: F(x) = f(Gx) for some link function f ∈ Cs(Rr), matrix G : Rd → Rr,
and the projection of X on the range of G is sufficient for regression. The particular case
r = 1 is called the single-index model, where the function F has the structure F(x) =
f(⟨v,x⟩) for some unknown index vector v ∈ Sd−1 and unknown link function f ∈ Cs(R1).
These models have been intensively studied: Stone (1982) conjectured that the min-max
rate for regression of single-index models is n− 2ss +1 (resp. n− 2ss +r for multi-index models),
coinciding with the one-dimensional min-max nonparametric rate, thereby escaping the
curse of dimensionality. This rate is achieved with kernel estimators in (Hardle and Stoker,
1989, Theorem 3.3) and (Horowitz, 1998, Section 2.5), and the index v can be estimated
at the parametric rate O(n− 21 ) under suitable assumptions. The existence of an estimator
converging at rate n− 2ss +1 is shown in (Gy¨orfi et al., 2002, Corollary 22.1), and (Ga¨ıffas
and Lecu´e, 2007, Theorem 2) demonstrates that n− 2ss +1 is indeed the min-max rate. These
modelshavealsoreceivedattentionrecentlybothfortheirconnectionsto“featurelearning”
in neural networks, see for example Lee et al. (2024) and references therein for learning
single-index models with neural networks (trained in a suitable layer-wise fashion), as well
as Radhakrishnan et al. (2024), and references therein, showing that deep neural networks
appear to exploit low-dimensional linear subspaces for classification and prediction.
Several methods for estimating the index vector v (or the matrix G) and f were de-
veloped over the years, with varying tradeoffs between the assumptions needed to produce
optimal estimators and the computational cost for constructing them. One category in-
cludes semiparametric methods based on maximum likelihood estimation (Ichimura, 1993;
Hardle et al., 1993; Delecroix et al., 1997; Delecroix and Hristache, 1999; Delecroix et al.,
√
2006;R.J.CarrollandWand,1997)andM-estimatorsthatproduce n-consistentindexes-
3Yantao Wu and Mauro Maggioni
timates under general assumptions, but with computationally demanding implementations,
relyingonsensitivebandwidthselectionsforkernelsmoothingandonhigh-dimensionaljoint
optimization of f and v. Another category includes direct methods: for example Average
Derivative Estimation (Stoker, 1986; Hardle and Stoker, 1989) estimates the index vector
v by exploiting its proportionality to ∇F. Early implementations of this idea suffer from
the curse of dimensionality due to the use of kernel estimation for the gradient. The tech-
nique of (Hristache et al., 2001) uses an iterative scheme to gradually adapt an elongated
neighbourhoodwindow. Gradient-basedmethods(stochasticandnon-stochastic)havebeen
studied in the context of neural networks (Lee et al., 2024).
A category of techniques particularly relevant to the present work includes conditional,
or inverse regression methods, which derive their estimators from statistics of the condi-
tional distribution of the explanatory variable X given observations of the (noisy) response
variable Y. Prominent examples include sliced inverse regression (Duan and Li, 1991; Li,
1991), sliced average variance estimation (Cook, 2000), simple contour regression (Li et al.,
2005; Coudret et al., 2013), with its analysis (in the multi-index case) in the work (Li and
Wang, 2007), which yields an estimator with near-optimal rates for the multi-index model.
Conditional methods partition the range of the response variable Y into small intervals and
consider their pre-images that are, when f is monotone, slices with the thinnest side along
the index direction v. They are often straightforward in implementation, consisting of the
computation of conditional empirical moments, or other statistics related to the level sets
of f, and can have only one parameter to tune: the width of slices. Unfortunately, several
of the works above, including (Hristache et al., 2001; Li and Wang, 2007), while obtain-
ing asymptotic learning rates that avoid the curse of dimensionality and are in some cases
min-max optimal, require a minimal number of samples that is exponential in the ambient
dimension d (either explicitly, or through constants exponential in d in the bounds).
Besides the statistical view that the single- and multi-index model do not entail a sta-
tistical cost cursed by the ambient dimension, another critical problem is to find algorithms
with reasonable computational cost to implement a statistically optimal estimator for these
models. For example, even in the case of single-index models, M-estimators typically re-
quired high-dimensional non-convex optimization in v and f; methods based on optimizing
over v, even when combined with random sampling, may scale exponentially in d due to the
need of obtaining points inside a narrow cone around the unknown v. Conditional methods
can often be implemented in a computationally efficient way, running in time O(C n) (up
d
to logn factors, and with the exception of contour regression methods, which scale quadrat-
ically in n), with a constant C a low-order polynomial in d. Lanteri et al. (2022) discuss
d
in detail these techniques for the single-index model, and introduce of a variation, called
Smallest Vector Regression, which enjoys optimal statistical guarantees up to log factors,
provides a theoretically optimal choice for selecting of the slice width parameter (a crucial
parameterwhosechoiceisoftennotdiscussed(Coudretetal.,2013,p. 75)),andisamenable
to a computationally-efficient implementation with cost O(d2nlogn), all without the curse
of dimensionality in the exponents and in the constants, for the rate, the minimum sample
requirements and the computational cost.
In conclusion, while the situation is rather well-understood for compositional models
F = f◦g with g linear, much is open when g is nonlinear, in which case it is not clear when
the compositional structure allows one to circumvent the curse of dimensionality, except
4Conditional regression for the Nonlinear Single-Variable Model
in cases where g has high regularity (global or anisotropic), increasing with d, or belong
to certain families of polynomials. We propose a simple model that, perhaps surprisingly,
without such strong regularity assumptions, still is amenable to nonparametric estimators
that avoid the curse of dimensionality, both statistically and computationally.
1.1 The Nonlinear Single-Variable Model
Our contributions in this work are the following:
(i) we introduce a model, called the Nonlinear Single-Variable Model, that is intermedi-
ate between the semi-parametric single-index model and the nonparametric function
composition model: both the outer and inner functions f and g are nonlinear, but
both the distribution ρ and the inner function g have a special structure related to
X
the geometry of the problem, with the range of g being an unknown curve γ in Rd;
(ii) we construct an efficient estimator that provably defeats the curse of dimensionality
by achieving a dimension-independent optimal (up to log factors) learning rate, with
no constants nor minimum sample requirements cursed by the ambient dimension d;
(iii) an efficient, near linear time algorithm that constructs the estimator given data. All
the constants, in both the learning bounds and in the computational costs, scale as
low-order polynomials in the dimension d, making the estimator practical.
This model generalizes the single-index model by allowing for g nonlinear (but with a
certaingeometricstructure), whilestillbeingamenabletoestimationwithstrongstatistical
and computational guarantees that are not cursed by the ambient dimension, and with no
regularity assumptions that scale with the dimension. To our knowledge, this may be the
first (nontrivial) statistical model that is shown to possess all these properties.
Definition 1 (Nonlinear Single-Variable Model) The regression function F has the
following decomposition:
E[Y|X] = F(X) = f(Π X)
γ
where the underlying curve γ : [0,len ] → Rd is parametrized by arc-length and has length
γ
len , and the link function f : [0,len ] → R1 depends only on one variable. The random
γ γ
vector X is supported in some domain Ω ⊆ Rd containing the image of γ, such that the
γ
closest-point projection Π : Ω → [0,len ], with
γ γ γ
Π (x) := argmin∥x−γ(t)∥ ,
γ
t∈[0,lenγ]
is well-defined on Ω , i.e. the minimizer is unique.
γ
We express the random vector X as its position along the curve and its displacement away
from the curve:
(cid:18) (cid:19)
Z
X = γ(t)+M d−1 (1)
γ′(t) 0
with t = Π X, t a random variable on [0,len ] and Z a centered random vector in Rd−1.
γ γ d−1
For each unit vector v ∈ Sd−1, M ∈ O(d) is an orthogonal matrix that maps the d-th
v
5Yantao Wu and Mauro Maggioni
6 2
1.8
1.8 10
4 1.6
1.6
1.4
2 1.4 5
1.2
1.2
1
0 1 0
0.8
0.8
-2
0.6 -5 0.6
-4 0.4 0.4
0.2 -10 0.2
-6
-6 -4 -2 0 2 4 -10 -5 0 5 10 15
Figure 1: One example of a Nonlinear Single-Variable Model (1): the underlying curve γ,
plotted in black, is a Meyer helix in R36 (details in Appendix C) with σ = 0.5 and the
γ
link function f ∈ C0.7(R1) is strictly monotone, and ζ ≡ 0. We generate n = 5000 samples
X scattered the curve in a tube of radius 6, colored by Y = F(X ) = f(Π X ). Left:
i i i γ i
Random projection of the data onto R3. Right: Orthogonal projection of data onto the
first 3 principal components. The distribution around this curve does not appear to be
linearly embeddable in low dimensions without increasing its complexity, see Appendix C.
canonical basis vector eˆ to the vector v. Therefore, conditioned on t = t ∈ [0,len ], the
d 0 γ
(cid:18) (cid:19)
Z
random vector M d−1 ∈ span{γ′(t )}⊥ is the displacement of X away from γ.
γ′(t0) 0 ∈ 0
The regression problem for the Nonlinear Single-Variable Model: Given pairs
(X ,Y )n with X ’s independent copies of X as above, and Y = F(X )+ζ , with F as in
i i i=1 i i i i
Definition 1 and observational noise ζ , we goal is to estimate the regression function F on
i
the support of X. The ζ ’s are assumed sub-Gaussian, independent among themselves and
i
from the X ’s. Note that the distribution of X is unknown, as are both the link function f
i
and the underlying curve γ. We illustrate one example in Fig.1.
The domain Ω ⊆ Rd needs to be such that the closest-point projection Π is well-
γ γ
defined on it: this condition is connected with the concept of reach of γ (Federer, 1959).
Starting with the distance function dist (x) : Rd → [0,∞), dist (x) = inf{∥x−γ(t )∥ :
γ γ 0
t ∈ [0,len ]}, the domain Unp is defined as the set of all points x ∈ Rd for which there
0 γ γ
is a unique point γ(t ) closest to x. The map Π : Unp → [0,len ] maps x ∈ Unp
0 γ γ γ γ
to the unique t ∈ [0,len ] such that dist (x) = ∥x−γ(t)∥. For t ∈ [0,len ], the local
γ γ 0 γ
reach is reach (t ) := sup{r : B(γ(t ),r) ⊆ Unp }, and the global reach is reach :=
γ 0 0 γ γ
inf{reach (t ) : t ∈ [0,len ]}. Bothreach (t )andreach takevaluesin[0,∞]. Withthese
γ 0 0 γ γ 0 γ
(cid:83)
definitions, Ω could be as large as B(γ(t ),reach (t )). Note that, therefore,
γ t0∈[0,lenγ] 0 γ 0
the distribution ρ is neither supported on γ, nor highly concentrated on γ, unlike the
X
aforementioned models for regression on manifolds, and similarly to single-index models,
where the distribution of the data typically has large variance in the directions normal to
the direction of the index vector: here we are letting this variance to be as large as possible,
6Conditional regression for the Nonlinear Single-Variable Model
constrained on keeping Π , the natural generalization of the orthogonal projection onto the
γ
line spanned by the single index, well-defined.
For any positive integer m ∈ N and any positive real number s ∈ R, we define the
function space Cs(Rm) via the following semi-norm: A function f : Rm → R is called s-
smooth, for s > 0, with s = s + s for nonnegative s ∈ N , chosen to be the largest
1 2 1 0
integer strictly less than s, and positive s ∈ (0,1], if there exists a positive value [f] > 0
2 Cs
such that for every nonnegative m-tuple (α ,...,α ) ∈ Nm with (cid:80) α = s , we have
1 m 0 i i 1
(cid:12) (cid:12)
(cid:12) ∂s1f(x1) − ∂s1f(x2) (cid:12) ⩽ [f] ∥x −x ∥s2 foranyx ,x ∈ Rm,i.e. allthes -thderivatives
(cid:12)∂xα 11···xα mm ∂xα 11···xα mm(cid:12) Cs 1 2 1 2 1
off areH¨oldercontinuouswithexponents . Thesmallestconstant[f] thatcanbechosen
2 Cs
in the above inequalities is called the s-smooth semi-norm of f. The function space Cs(Rm)
consists of functions f ∈ L∞(Rm) with finite semi-norm [f] .
Cs
We ready for an informal version of the main Theorems 3 and 4:
Theorem 2 (Informal) Suppose that f ∈ Cs(R1) for some s ∈ [1,2] and that f is coarsely
2
monotone. With some assumptions on the underlying curve γ, the distribution ρ of the
X
random variable X, and the variance σ2 of the noise ζ, if the number of training data n
ζ
satisfies n ⩾ poly(d,len γ), then the estimator F(cid:98) constructed by Algorithm 1 satisfies
E(cid:20)(cid:12)
(cid:12)
(cid:12)F(cid:98)(X)−F(X)(cid:12)
(cid:12)
(cid:12)2(cid:21)
≲ C 1(f,γ,ρ X,σ ζ,d)n− 2s2 +s 1 logn+C 2(f,γ,ρ X,σ ζ,d)max(σ ζ,ω f)2(s∧1).
If we further suppose that there is no observational noise, i.e., ζ = 0 almost surely, and
that f is strictly monotone, then the estimator F(cid:98) satisfies
(cid:20)(cid:12) (cid:12)2(cid:21) (cid:18) log3n(cid:19)s∧1
E (cid:12) (cid:12)F(cid:98)(X)−F(X)(cid:12)
(cid:12)
≲ C 3(f,γ,ρ X,d)
n2
.
The dependency of the constants C 1,C 2,C
3
on d is a low-order polynomial, and F(cid:98) can be
constructed by an algorithm that runs in time O(d2nlogn).
The bound on the expected L2(ρ ) error of our estimator contains two terms: the first
X
one shows that the learning rate is near optimal (up to logn factors) as it is the min-max
rate of one-dimensional non-parametric regression for f, as if we knew the curve γ; the
second term is a constant at which our estimator saturates, due to the estimator producing
a piecewise linear approximation to γ, but only at scales above σ . The estimator in fact
ζ
produces an estimate of the underlying curve γ, of the (nonlinear) closest-point projection
Π , and of the link function f, thereby providing an interpretable result in terms of all the
γ
terms in the compositional structure to the regression function F. Further remarks may be
found after the formal statement of the main Theorems, in Section 2.
We conclude this section by reporting in Table 1 several math symbols used throughout
this paper.
7Yantao Wu and Mauro Maggioni
symbol definition symbol definition
C,c positiveabsoluteconstants ∥A∥ spectralnormofamatrixA
a≲b a≤CbforsomepositiveabsoluteconstantC a≍b a≲bandb≲a
a∧b minimumof{a,b} |I| LebesguemeasureofanintervalI
λm(A) m-thlargesteigenvaluesofasquarematrixA vm(A) singularvectorcorrespondingtoλm(A)
B(x,r) Euclideanballofcenterxandradiusr 1(E) indicatorfunctionofaneventE
span{S} linearspanofasetS Pu orthogonalprojectionontospan{u}
{S}⊥ orthogonalcomplementofasetS Πγ nearestpointprojectionontothepositionalongγ
lenγ lengthofthecurveγ reachγ reachofthecurveγ
F =f◦Πγ unknownregressionfunctionfromRdtoR1 f unknownlink functionfromR1toR1
[f] semi-normofans-smoothfunction |f| L∞-normforfunctionf
Cs L∞
X randomvectorinRdwithdensityfunctionρX {Xi}n
i=1
samplesofX
Y randomvariabledependentonX {Yi}n
i=1
samplesofY
ζ randomvariablemodelingnoise ζi samplesofζ
R boundedinterval,rangeof{Yi}n
i=1
l totalnumberofpartitionsofintervalR
{Rl,h}l
h=1
partitionintervalsofRindexedbyh R(cid:98)l,h thesetofsampleYisuchthatYi∈Rl,h
Sl,h slice,conditionaldistributionX|Y ∈Rl,h S(cid:98)l,h empiricalslice,thesetofsampleXisuchthatYi∈Rl,h
n numberofsamplepoints{(Xi,Yi)}n
i=1
nl,h numberofsamplesinempiricalsliceS(cid:98)l,h
µl,h centerofsliceSl,h,i.e. E[X|Y ∈Rl,h] µ(cid:98)l,h empiricalmeanofpointsinS(cid:98)l,h
vl,h significantvectorofsliceSl,h v(cid:98)l,h empiricalsignificantvectorofpointsinS(cid:98)l,h
Σl,h covariancematrixforsliceSl,h Σ(cid:98)l,h empiricalcovariancematrixforpointsinS(cid:98)l,h
Hl,h geometricquantityofsliceSl,h H(cid:98)l,h estimatedgeometricquantityforpointsinS(cid:98)l,h
dist(x,h) distancefrompointxtosliceSl,h d(cid:100)ist(x,h) estimateddistancefrompointxtoempiricalsliceS(cid:98)l,h
hx nearestindexforpointx I boundedsubintervalofone-dimensionalprojectionofX
j numberofpartitionofsubintervalI f(cid:98)j|v estimatoroff bylocalpolynomialfitting
Notation
2 An estimator for the Nonlinear Single-Variable Model
We propose an estimator for F, and a corresponding efficient algorithm, for the Nonlinear
Single-Variable Model based on inverse (or conditional) regression, which also produces a
sketch of the curve γ and an estimator of the closest-point projection Π .
γ
Step 1: extract geometric features of the underlying curve γ. Given data {(X ,Y )}n ,
i i i=1
let R be an interval containing all the Y ’s, and {R }l a partition of R, either uniform
i l,h h=1
or based on empirical quantiles (so that all R contain the same number of points). We
l,h
partition the data {(X i,Y i)}n
i=1
into pairs {(S(cid:98)l,h,R(cid:98)l,h)}l
h=1
where R(cid:98)l,h := {Y
i
: Y
i
∈ R l,h}
and S(cid:98)l,h := {X
i
: Y
i
∈ R l,h}. Each empirical slice S(cid:98)l,h is the empirical pre-image of the
interval R in the output variable Y. We perform the Principal Component Analysis
l,h
(PCA) of each S(cid:98)l,h to obtain its mean µ
(cid:98)l,h
and its “significant vector” v (cid:98)l,h: µ
(cid:98)l,h
will be
approximately on γ, and v will be approximately tangent to γ at µ , yielding a local
(cid:98)l,h (cid:98)l,h
first-order approximation to γ.
Step 2: design a distance function dist(x,h) (and its empirical version d(cid:100)ist(x,h)), based
on the geometric shape of S
l,h
(and S(cid:98)l,h respectively), that measure how far the point x
is away from the slice S
l,h
(S(cid:98)l,h, respectively). By assigning each x ∈ Ω
γ
to the “nearest”
slice according to this distance function, we partition the domain Ω into several local
γ
neighborhoods and we use the significant vector v
(cid:98)l,h
to project the points X
i
∈ S(cid:98)l,h onto
a one-dimensional interval I(l,h) in each local neighborhood. Because the significant vector
8Conditional regression for the Nonlinear Single-Variable Model
v is approximately tangential to the curve γ, this linear projection approximates the
(cid:98)l,h
nonlinear projection Π in each local neighborhood.
γ
Step 3: performaone-dimensionalpiecewisepolynomialregressionontheprojectedpoints
in I(l,h) and obtain a local estimator of the regression function F in a local neighborhood.
This, together with the “nearest” slice assignment in the second step, allows us to construct
a global estimator of F in the whole domain Ω .
γ
2.1 Extracting the geometric features of the underlying curve γ
γ′′(t)
For each t ∈ [0,len ], let nˆ(t) := be the unit normal vector to γ, pointing inwards
γ ∥γ′′(t)∥
the circle of curvature. For x ∈ Ω we define the signed projected distance from x to γ to
γ
be d˜(x,γ) := ⟨x−γ(Π x),nˆ(Π x)⟩ , which is zero if x is on the curve γ, positive if x is
γ γ Rd
inside the circle of curvature, and negative otherwise. A direct calculation shows that the
compositional structure F = f ◦Π in Definition 1 implies that for x ∈ Ω
γ γ
f′(Π x)
∇F(x) = γ γ′(Π x). (2)
1−∥γ′′(Π x)∥d˜(x,γ) γ
γ
Therefore, the gradient vectors of points in each level set Π−1(t ) = {x : Π x = t } are
γ 0 γ 0
all parallel to γ′(t ), albeit with magnitude depending on the relative position of x and γ.
0
As a consequence, each Π−1(t ) is contained in a hyperplane. If we could perform singular
γ 0
value decomposition for points on each level set Π−1(t ), the singular vector corresponding
γ 0
to the 0 singular value would be parallel to γ′(t ).
0
The geometry of the level sets will play a prominent role in constructing our estimator
of the Nonlinear Single-Variable Model, as it did in some of the earlier works on the single-
index model including (Stoker, 1986; Hardle and Stoker, 1989), and recent refinements
such as the Smallest Vector Regression estimator of Lanteri et al. (2022). In the single-
index model, where the underlying curve γ is a line segment with direction v, any level set
{x : F(x) = c} is a hyperplane perpendicular tov. As in (Lanteri et al., 2022), we shall take
a uniform partition on the empirical range R := [min(Y ),max(Y )], consisting of suitably
i i
small intervals {R }l where l ∈ N is the total number of partitioning intervals, indexed
l,h h=1
by h = 1,...,l; for each partitioning interval R , we consider the empirical slice
l,h
S(cid:98)l,h := {X
i
: Y
i
∈ R l,h},
which is the pre-image of R at the level of samples, i.e., a sample from the conditional
l,h
distribution X|Y ∈ R l,h. Each empirical slice S(cid:98)l,h is utilized in Lanteri et al. (2022) as an
approximationtoalevelsetΠ− γ1(s 0), andS(cid:98)l,h shouldbe“thin”alongthedirectionofv (and
therefore∇F)and“wide”ondirectionsorthogonaltov,atleastundersuitableassumptions
on F and noise level σ ζ. Lanteri et al. (2022) then perform local PCA on each S(cid:98)l,h and use
the smallest principal component to approximate the direction of v: since γ is a line with
direction v, these smallest principal components should all be independent estimators of
the index vector v, and these per-slice estimates can be suitably averaged across all slices
to obtain an estimator for v. Once v is estimated, the input data is projected onto this line,
and one-dimensional non-parametric regression yields an estimator for f.
In the Nonlinear Single-Variable Model, we are still going to use empirical slices, but
here we cannot perform an aggregation of estimated vectors because the tangent vectors to
9Yantao Wu and Mauro Maggioni
Figure 2: In the same setup as in Fig.1, we
-2.5
partition the range uniformly into l = 800 in-
-2
tervals, and consider two slices. Top: a visu-
-1.5
alization of the two empirical slices, where we
-1
only plot 2000 samples per slice (in green and
-0.5
blue),withγ inblack. Theredcirclesandvec-
0
tors are the sample means and smallest prin-
0.5
1 cipal components of the two empirical slices.
1.5 Bottom: bar plots with the largest, second
2 largest, average, second smallest, and small-
2.5 est singular value of the empirical slices. The
-0.5 0 0.5 1
smallest singular value is significantly smaller
1 1
0.8 0.8
0.6 0.6 than the others.
0.4 0.4
0.2 0.2
0 0
m sa ecx oS nV d max SV me sa en
c
oS nV d min SV min SV m sa ecx oS nV d max SV me sa en
c
oS nV d min SV min SV
γ are not constant due to the curvature of γ. Instead, we can only rely on local information
to estimate the nearest-point projection onto γ, which is now a nonlinear function. Each
S(cid:98)l,h stillapproximatessomelevelsetΠ− γ1(t 0)undertheAssumption(LCV)below: roughly
speaking, when R
l,h
has a sufficiently small diameter, the S(cid:98)l,h is “thin” along the tangential
direction γ′(t ) and “wide” along directions orthogonal to γ′(t ). Consequently, PCA on
0 0
each empirical slice S(cid:98)l,h locally will yield:
(i) the empirical mean µ , which approximates the conditional expectation µ :=
(cid:98)l,h l,h
E[X|Y ∈ R ] and be approximately on the curve γ;
l,h
(ii) the empirical covariance matrix Σ(cid:98)l,h, which should approximate the conditional co-
variance matrix Σ := E[(X −µ )(X −µ )⊺ |Y ∈ R ];
l,h l,h l,h l,h
(iii) the smallest principal component v
(cid:98)l,h
of Σ(cid:98)l,h, which should approximate the smallest
principal component v of Σ and be approximately tangential to the curve.
l,h l,h
These yield a first-order approximation of underlying curve γ in a local neighborhood of
S(cid:98)l,h. Figure 2 depicts an example where the sample mean is approximately on the curve,
and the smallest principal component vector is approximately tangential to the curve.
However, the above argument relies on the Assumption (LCV), which imposes some
restrictions on the underlying curve γ and the “radius” of Ω around γ, the regression
γ
function F, and the noise level σ . Without such an assumption, it might not be the case
ζ
that slices are “thin” in the direction tangent to the curve for various reasons: (i) with an
insufficient amount of samples, choosing very fine partitions of the range R is not optimal:
smaller intervals will contain fewer samples, leading to a high variance in the estimation
of the mean and principal component(s) of the slices; (ii) the observational noise ζ in the
outputs forces a lower bound on the diameter of the partitioning intervals R of the range
l,h
R: subdividing the range into intervals with size smaller than the noise level σ does not
ζ
improve estimation, nor would make the slices thinner; (iii) the reach reach may be small
γ
10Conditional regression for the Nonlinear Single-Variable Model
Figure 3: In the same setup and visual con-
ventions of Fig.1, but with the range R split
14
uniformly into 80 intervals, and a different
12
10 pair of slices. Top: the slices are now elon-
8 gated along the curve, rather than perpendic-
6 ularlytoit. Bottom: thelargestsingularvalue
4
is now significantly larger than the remaining
2
ones.
0
-2
-4
-6
-2 0 2 4 6 8 10
3 3
2 2
1 1
0 0
m sa ecx oS nV d max SV me sa en
c
oS nV d min SV min SV m sa ecx oS nV d max SV me sa en
c
oS nV d min SV min SV
because of the complexity of the underlying curve γ; (iv) the distribution of X might
strongly concentrate around γ, i.e. σ , the standard deviation of Z , is small. In these
γ d−1
situations, we might encounter slices that are “wide”, i.e., much wider in the curve’s local
tangent direction than in all other directions—essentially the opposite of being “thin”. In
this case, the largest principal component can be significantly larger than the remaining
components and is the one that aligns with the tangent direction of γ. Figure 3 illustrates
how,inthesesituations,thesamplemeanandthelargestprincipalcomponentofaslicemay
provide an approximation of the underlying curve γ. In this setting, the largest singular
value is significantly larger than the other ones.
We want our algorithm to adapt to both “thin” and “wide” slice scenarios discussed
above: consider a slice S with conditional mean µ and conditional covariance matrix
l,h l,h
Σ l,h, and their empirical counterparts S(cid:98)l,h, µ (cid:98)l,h, Σ(cid:98)l,h. We define H
l,h
and H(cid:98)l,h as
(cid:32) λ mid(Σ l,h)2 (cid:33) (cid:32) λ mid(Σ(cid:98)l,h)2 (cid:33)
H
l,h
:= log , H(cid:98)l,h := log ,
λ 1(Σ l,h)λ d(Σ l,h) λ 1(Σ(cid:98)l,h)λ d(Σ(cid:98)l,h)
where for any d × d square matrix M, λ (M) ⩾ λ (M) ⩾ ··· ⩾ λ (M) ⩾ λ (M) are
1 2 d−1 d
the eigenvalues of M in descending order, and λ (M) = (λ (M)×···×λ (M))1/(d−2)
mid 2 d−1
is the geometric mean of the eigenvalues excluding the largest and the smallest ones. A
slice S is “thin” when λ (Σ ) ≪ λ (Σ ) ≈ λ (Σ ) ≈ ··· ≈ λ (Σ ), and hence
l,h d l,h 1 l,h 2 l,h d−1 l,h
H > 0; the larger H is, the “thinner” S is. A slice S is “wide” when λ (Σ ) ≫
l,h l,h l,h l,h 1 l,h
λ (Σ ) ≈ ··· ≈ λ (Σ ), and hence H < 0; the more negative H is, the “wider” S
2 l,h d l,h l,h l,h l,h
is. If H is close to zero, then the geometric shape of the slice S is undetermined: it
l,h l,h
may be roughly isotropic or may have both very large λ (Σ ) and very small λ (Σ ). We
1 l,h d l,h
define the significant vector v and the empirical significant vector v as
l,h (cid:98)l,h
(cid:40) (cid:40)
v d(Σ l,h) if H
l,h
> 0 v d(Σ(cid:98)l,h) if H(cid:98)l,h > 0
v := , v := .
l,h (cid:98)l,h
v 1(Σ l,h) if H
l,h
< 0 v 1(Σ(cid:98)l,h) if H(cid:98)l,h < 0
11Yantao Wu and Mauro Maggioni
The significant vector v is used to estimate the tangent vector γ′ in both the “thin” and
l,h
“wide” slice scenarios. When H ≈ 0, we only expect to use the sample mean µ as a
l,h l,h
local 0-th order approximation to γ, as the slice has no preferred direction. Crucially, while
in the “thin” scenario we expect v to be a good approximation of the tangential vector,
(cid:98)l,h
in the “wide” scenario the curvature of γ can have a significant effect in our estimation of
the local direction of the curve.
2.2 Estimating the nonlinear projection Π by assigning points to the
γ
“nearest” slice
Beforeregressingf,weconstructanestimatorforΠ (x),whichisthevalueoftheparameter
γ
in [0,len ] of the curve such that γ(Π (x)) is the closest point on the curve to a point
γ γ
x ∈ Ω . We design a distance function dist(x,h) between x and slice S and assign x to
γ l,h
the “nearest” slice under this distance function. This assignment maps Ω onto {1,...,l}
γ
and thus can be interpreted as a zeroth-order approximation of Π . After this assignment,
γ
we use µ and v to obtain a first-order approximation of Π (·) locally on each slice S .
l,h l,h γ l,h
Our choice of distance function dist(x,h) is dictated by two purposes. First, it should
be “local”, i.e., the distance between x and the center of a slice should play a role. Second,
it should be anisotropic: on any level set {x : Π x = t } we have ⟨x−γ(t ),γ′(t )⟩ = 0, so
γ i i i
the distance to the hyperplane normal to γ′(t ) should play a prominent role, but cannot
i
be too dominant, as there may be multiple slices, even far away from each other, with
⟨x−γ(t ),γ′(t )⟩ ≈ 0. We therefore consider a distance function of the form
i i
|⟨x−µ ,v ⟩|+c∥x−µ ∥
l,h l,h l,h
for some c > 0. The value of c cannot be too small, or this distance function would fail
for highly self-entangled curves, nor too large because we want a small distance dist(x,h)
if the point x is close to the slice S . The optimal choice of c depends on the curve and
l,h
the distribution of data around it. We define the distance function dist(x,h) separately in
the “thin” slice scenario and the “wide” slice scenario:

dist(x,h) =
|⟨x−µ l,h,v l,h⟩|2+ λ λd 1(( ΣΣ
b
ll ,, hh )) ∥x−µ l,h∥2 if H
l,h
> 0
, (3)
∥x−µ ∥2+ λ d(Σ l,h) |⟨x−µb ,v ⟩|2 if H < 0
l,h λ1(Σ l,h) l,h l,h l,h
and similarly for its empirical counterpart d(cid:100)ist(x,h). In the “thin” slice scenario, this dis-
tancefunctionfocusesonmeasuringthedisplacementbetweenxandS alongthedirection
l,h
v , and is less sensitive to the displacement orthogonal to v . In the “wide” scenario,
l,h l,h
dist(x,h) instead pays less attention to the displacement along v and focuses on the dis-
l,h
placementorthogonaltov l,h. NotethatwhentheshapeofthesliceS
l,h
(andS(cid:98)l,h)isroughly
isotropic, λ /λ is roughly one, so the two cases in (3) are consistent with each other and
d 1
the distance above varies regularly as H changes sign. This distance function dist(x,h)
l,h
has the following advantages: (i) it focuses on local slices while incorporating information
aboutthegeometryofeachslice; (ii)insidethelocalneighborhood, itpaysspecialattention
to displacement along the tangential direction; (iii) it is distribution-adaptive, allowing, for
example, to handle in robust fashion heteroscedasticity in the distribution of X around the
curve; (iv) its performance is amenable to mathematical analysis.
12Conditional regression for the Nonlinear Single-Variable Model
We will use equation (3) in the proof of convergence of our algorithm. We remark
that this distance is a simplification of the Mahalanobis distance from x to a slice S
l,h
−1/2
dist(x,h) := ||Σ (x−µ )||, which also puts heavier weight on the displacement along
l,h l,h
the tangential direction in the “thin” slice scenario and lighter weight in the “wide” slice
scenario. As it is a bit harder to analyze mathematically than the distance in (3), we use it
only in numerical simulations.
Slices with little data yield estimators with large variance, and will be disregarded: let
n := #S be the number of samples in S , and define the index set of “heavy” slices
l,h l,h l,h
H := {h ∈ {1,...,l} : n ⩾ n/l}.
l l,h
For x ∈ Ω , we define the “nearest” index for the slice that x belongs to as
γ
h := argmindist(x,h),
x
h∈H
l
and the “correct” index for the slice that x belongs to as the unique index h′ such that
x
F(x) ∈ R . For almost all x ∈ Ω , the minimizer h is unique, since the set of points
l,h′ γ x
x
that cannot be uniquely assigned is a subset of {x ∈ Ω ⊆ Rd : ∃h′,h′′ s.t. dist(x,h′) =
γ
dist(x,h′′)}, which is a finite union of hyper-surfaces in Rd and thus has measure zero. We
will show that in the “thin” slice scenario, the nearest index h is almost the correct index
x
h′ under suitable assumptions, i.e., |h′ −h | ⩽ 1 (Section 3). The possibility of an error
x x x
|h −h′| = 1 stems from the possibility that points near the boundary of a slice can be
x x
misclassifiedtooneofitsadjacentslices. Givensampledata,the“nearest”sliceisestimated
using the empirical counterpart of the distance, yielding
(cid:98)h
x
:= argmind(cid:100)ist(x,h).
h∈H
h
We shall prove that |(cid:98)h x−h x| ⩽ 1 with high probability, with the possibility of |h′ x−h x| = 1
and|(cid:98)h x−h x| = 1bothstemmingfrompointsneartheboundaryofaslicebeingmisclassified
to one of its adjacent slices. Nonetheless, the probability of adjacent misclassification (i.e.,
(cid:98)h
x
= h′
x
±1) is relatively small but does not decrease to zero as the sample size increases:
to avoid artifacts in the regression stage, we include data from adjacent slices to estimate
the regression function in each local neighborhood.
2.3 Local estimator of the link function f and global estimator of the
regression function F
After assigning a point x ∈ Rd to an estimated slice S(cid:98)
l,(cid:98)hx
with index (cid:98)h x, we use piecewise
polynomial estimators to regress the link function f on the corresponding slice. As noted
above, we also need to consider empirical slices adjacent to S(cid:98) . First, we project the
l,(cid:98)hx
(cid:83)
data from |h−(cid:98)hx|⩽1S(cid:98)l,h orthogonally onto the line with direction v (cid:98)l,(cid:98)hx. We consider the
projecteddata{⟨v (cid:98)l,(cid:98)hx,X i⟩ : X
i
∈ (cid:83) |h−(cid:98)hx|⩽1S(cid:98)l,h},coveritwithanintervalI(l,(cid:98)hx),andfurther
partition I(l,(cid:98)hx) uniformly into j smaller intervals {I(l,(cid:98)hx) }j . We use the sample values
j,k k=1
{Y
i
: X
i
∈
(cid:83)
|h−(cid:98)hx|⩽1S(cid:98)l,h} to construct a local polynomial f(cid:98)
j,k|v
(cid:98)l,h(cid:98)x
on each interval I
j( ,l k,(cid:98)hx)
of
13Yantao Wu and Mauro Maggioni
the partition by solving a least squares fitting problem, obtaining a piecewise polynomial
estimator f(cid:98) of the link function. The degree of the local polynomials needed to obtain
j|v
(cid:98)l,h(cid:98)x
optimal estimation rates depends on the regularity of the function. A proper partition (or
scale) is then chosen to minimize the expected mean squared error (MSE) using classical
bias-variance trade-off arguments. Composing this local estimator of f with the “nearest”
slice assignment gives us a global estimator F(x) = f(cid:98)
j|v
(cid:98)l,h(cid:98)x(⟨v (cid:98)l,(cid:98)hx,x⟩).
Algorithm 1: Significant Vector Regression
Input : Samples {(X ,Y )}n ⊆Rd×R, number of partitions l,j ∈N, polynomial
i i i=1
degree m∈N , truncation level M ∈(0,∞].
Output: F(cid:98) estimate of F.
1.a Compute interval R of range of samples {Y }n . Construct {R }l , the uniform partition
i i=1 l,h h=1
of R into l intervals whose preimages are {S(cid:98)l,h}l
h=1
where S(cid:98)l,h ={X
i
:Y
i
∈R l,h};
1.b Denote n =#{Y ∈R }. For each h∈H ={h:n ≥n/l}, compute:
l,h i l,h l l,h
µ
(cid:98)l,h
= nl1
,h
(cid:80) iX i1{X
i
∈S(cid:98)l,h}, the empirical mean
Σ(cid:98)l,h = nl1
,h
(cid:80) i(X i−µ (cid:98)l,h)(X i−µ (cid:98)l,h)⊺1{X
i
∈S(cid:98)l,h}, the empirical covariance matrix
λ(cid:98)l,h,m, the m-th eigenvalue of Σ(cid:98)l,h, in descending order
v (cid:98)l,h,m, the m-th eigenvector of Σ(cid:98)l,h corresponding to eigenvalue λ(cid:98)l,h,m
H
=log(cid:16) (λ(cid:98)l,h,2×···×λ(cid:98)l,h,d−1)2/(d−2)(cid:17)
l,h
λ(cid:98)l,h,1λ(cid:98)l,h,d
the empirical significant vector v , equals v if H >0 and to v otherwise
(cid:98)l,h (cid:98)l,h,d l,h (cid:98)l,h,1
2.a Given x∈Rd, for each h∈H
l
compute the estimated distance between x and S(cid:98)l,h

|⟨x−µ (cid:98)l,h,v (cid:98)l,h⟩|2+ λd(Σ(cid:98)l,h)∥x−µ (cid:98)l,h∥2 if H(cid:98)l,h >0
d(cid:100)ist(x,h)= λ1(Σ(cid:98)l,h)
∥x−µ (cid:98)l,h∥2+ λd(Σ(cid:98)l,h)|⟨x−µ (cid:98)l,h,v (cid:98)l,h⟩|2 if H(cid:98)l,h <0
λ1(Σ(cid:98)l,h)
2.b compute the estimated nearest slice index (cid:98)h
x
=argmin h∈Hld(cid:100)ist(x,h)
3.a For each h∈H
l
compute: the interval I(l,h) containing {⟨v (cid:98)l,h,X i⟩:X
i
∈(cid:83) |h−(cid:98)hx|⩽1S(cid:98)l,h}; the
uniform partition {I(l,h)}j of I(l,h); n(l,h) =#{X :⟨v ,X ⟩∈I(l,h)}.
j,k k=1 j,k i (cid:98)l,h i j,k
3.b For each h∈H and each k ∈K ={k :n(l,h) ⩾n /j} compute
l j j,k l,h
f(cid:98)j,k|v(cid:98)l,h = da er gg (pm )⩽in
m Xi∈(cid:83)
|h(cid:88) −h(cid:98)x|⩽1S(cid:98)l,h|Y i−p(⟨v (cid:98)l,h,X i⟩)|21
I j( ,l
k,h)(⟨v (cid:98)l,h,X i⟩)
3.c For each h∈H l, compute f(cid:98)j|v(cid:98)l,h(r)=(cid:80) k∈Kjf(cid:98)j,k|v(cid:98)l,h(r)1
I j( ,l
k,h)(r) and return the estimator
F(cid:98)(x)=f(cid:98)j|v(cid:98)l,h(cid:98)x(⟨v (cid:98)l,(cid:98)hx,x⟩)
14Conditional regression for the Nonlinear Single-Variable Model
2.4 The main algorithm and guarantees on the estimator it produces
Algorithm 1 summarizes the construction of the proposed estimator of the regression func-
tion F. We prove that the mean squared error of our estimator is the sum of the estimation
error and the curve approximation error. The estimation error decays the one-dimensional
min-max optimal nonparametric learning rate, up to a saturation level dependent on quan-
tities expected to be small, or even zero, as we discuss in detail below.
Theorem 3 (MSE of the Estimator constructed by Algorithm 1) Assumethat(X
subG), (Y subG), (ζ subG), (γ ), (LCV), (ω ), and (P) hold true, and that f ∈
1 f
Cs(R1) with s ∈ (cid:2)1,2(cid:3) . Let C ,M∗,l be the constants specified in (4) below, and C a
2 γ,f max
universal constant. Then, for n such that n ≳ C C flenγ, if we choose l∗,j∗ as
log3/2n γ,f C f′σγ
(cid:16) (cid:17) 2s
 C−1nlog−2n, C if n2s+1 ≲ C M∗
  γ,f log2n γ,f
(cid:16) (cid:17)
(l∗,j∗) :=  

(cid:16) nl m 2a s1x +, 1MC lM ∗m ,a∗ x Cn (cid:17)2s1 +1 i of thn e2 rs w1 +1 ise≳ M∗C mC axY (R σ ζ0 ,ω f) ,
the estimator constructed by Algorithm 1, in running time O(d2nlogn), satisfies
E(cid:20)(cid:12)
(cid:12)
(cid:12)F(cid:98)(X)−F(X)(cid:12)
(cid:12)
(cid:12)2(cid:21)
≲ C 1(f,γ,ρ X,σ ζ,d)n− 2s2 +s 1logn+C 2(f,γ,ρ X,σ ζ,d)
We specify the constants in the Theorem above:
(cid:32) (cid:33) 2s
C 2s+1
C 1(f,γ,ρ X,σ ζ,d):= Cf
′
len γσ ζ2 ([f] Cs+|f| L∞[ρ X] Cs)2s2 +1 , (4)
f
(cid:18) σ C (cid:19)2(s∧1) R3C2 (cid:32) len d3/2 R5C2d4(cid:33)
C (f,γ,ρ ,σ ,d):=[f]2 γ f max(σ ,ω ) , C := 0 f max γ , 0 f ,
2 X ζ Cs∧1 reach ζ f γ,f C′ σ4 C′3σ8
γ f γ f γ
(cid:16) (cid:17) 2 CC R
M∗ := σ−1(C C R )s([f] +|f| [ρ ] ) 2s+1 , l := Y 0 .
ζ f Y 0 Cs L∞ X Cs max max(σ ,ω )
ζ f
The above theorem states that our estimator achieves the min-max optimal rate for
the one-dimensional nonparametric regression, with an additional approximation error of
magnitude O(σ2) for functions that are both Lipschitz and monotone, therefore defeating
ζ
the curse of dimensionality by exploiting the compositional structure of F, even if the inner
functionisnonlinear(unlikethesingle-andmulti-indexmodel)andnotparticularlysmooth
(and its regularity does not scale with the ambient dimension). It is worth mentioning that
the result of Theorem 3 is scaling invariant in X and Y.
This result is satisfactory in the following respects:
(i) it avoids the curse of dimensionality, with d not appearing in the learning rate 2s ;
2s+1
(ii) the learning rate matches the min-max rate for 1-d nonparametric regression;
(iii) the minimal number of samples n required is only a low-order polynomial in d, len ,
γ
and ∥γ′′∥. This is not exponential in ambient dimension d (unless in the extreme case
where γ has length or curvature growing exponentially in d, e.g. a space-filling curve).
15Yantao Wu and Mauro Maggioni
(iv) Algorithm 1 in fact also estimates γ, the nonlinear projection Π , and f. In each
γ
local neighborhood of S(cid:98)l,h, the empirical mean µ
(cid:98)l,h
and significant vector v
(cid:98)l,h
give a
line segment that approximates γ, and Π is estimated, up to a translation, by the
γ
piecewise linear approximation x (cid:55)→ ⟨x,v ⟩. This provides interpretability to our
(cid:98)l,h
estimator, in the sense that both F and its structure are resolved; this is not the case
for neural networks, even in the case of single-index models, where the index can be
identified only by suitably averaging over multiple weights in multiple nodes.
There are two apparent shortcomings in our results:
(i) the assumption that f is coarsely monotone. While similar conditions have appeared
in other works using conditional regression (see Lanteri et al., 2022, for a discussion),
and even in recent approaches based on gradient descent for the single- and multi-
index models (see e.g., Damian et al., 2024; Arous et al., 2021; Bietti et al., 2023, and
references therein) the “amount of oscillation” of f appears prominently and imposes
additional sampling requirements (e.g., scaling as dO(L) where L is the number of
vanishing moments in the Hermite polynomial basis, albeit it is not clear to us that
such assumptions and sampling requirements are sharp);
(ii) the second shortcoming is that we have a second additive term, which we call curve
approximation error, whichdoesnotvanishasntendstoinfinity. Itistypicallysmall,
e.g., if external noise σ or the curvature ∥γ′′∥ is small, and in fact it will vanish when
ζ
externalnoiseσ orcurvature∥γ′′∥vanishes. Inthelimitingcasewheretheunderlying
ζ
curve γ is a straight line segment, we have no curve approximation error, recovering
the results of (Lanteri et al., 2022). Otherwise, the nonzero approximation error
exists because we are using first-order approximations in the estimation of the local
directions of the curve, and with noisy observations our technique for constructing
the estimator does not allow us to consider local pieces below the scale of the noise.
While it seems possible to use approximations of higher order, they would require
significant additional computational complexity and more refined statistical analysis,
possibly with minimal impact in practical applications, and are left to future work.
In the case of a strictly monotone Lipschitz function with zero external noise, we obtain
the rate O(n−2). Here, the curve approximation error vanishes because there is no upper
bound for the number of slice l, contrary to the noisy case in Theorem 3:
Theorem 4 (MSE of Algorithm 1 in the noiseless case) Assumethat(XsubG),(Y
subG), (ζ subG), (γ ), (LCV), (ω ), and (P) hold true. Assume that there is no ob-
1 f
servational noise, i.e., ζ ≡ 0 almost surely, that the link function f is perfectly monotone,
i.e., ω = 0, and that f ∈ Cs with s ∈ (cid:2)1,2(cid:3) . With C in (4), when n ≳ C C flenγ,
f 2 γ,f log3/2n γ,f C f′σγ
if we choose l∗ = 1 n and j∗ = C, then the estimation error of Algorithm 1 satisfies
C γ,f log3/2n
(cid:20)(cid:12) (cid:12)2(cid:21) (cid:18) log3n(cid:19)s∧1
E (cid:12) (cid:12)F(cid:98)(X)−F(X)(cid:12)
(cid:12)
≲ ([f]
Cs∧1
+|f| L∞[ρ X] Cs∧1)2 C γ,f(C fC YR 0)2
n2
.
The better rate O((n−2log3n)(s∧1)) is a consequence of having zero observational noise.
Bauer et al. (2017) proves that, in the case of the L∞-norm, the min-max rate of nonpara-
metric regression for functions in Cs(Rd) with noiseless observations is (logn/n)s/d. The
16Conditional regression for the Nonlinear Single-Variable Model
rate on the mean squared error in Theorem 4 agrees with this when s ⩽ 1 and d = 1, as if γ
(andthereforeΠ )wereknown. Ourrateissuboptimalfors > 1, asweonlyperformafirst-
γ
order approximation of the underlying curve and do not estimate higher-order parameters
such as the curvature. Our estimator avoids the curse of dimensionality by exploiting the
compositional structure of F, even if the inner function is (unknown and) nonlinear (unlike
in the single- and multi-index model) and not particularly smooth (and its regularity does
not scale with the ambient dimension). A key difference between Theorem 3 and Theorem
4 is that there is no curve approximation error in the latter: noiseless observations allow
us to perform an unlimited amount of partitioning, obtaining “thin” slices, provided that
enough samples are available (in order to control the variance of the objects we estimate in
each slice).
When (LCV) is not satisfied, our estimator quickly saturates at the level of the curve
approximation error:
Theorem 5 (NVM without (LCV)) Assume that (X subG), (Y subG), (ζ subG),
(γ ), (ω ), and (SC) hold true. When n ≳ CYR 07d3 , if we choose l∗ = CYR0
1 f log1/2n C f6max(σ ζ,ω f)7 max(σ ζ,ω f)
and j∗ = C, the estimation error of Algorithm 1 satisfies
(cid:20)(cid:12) (cid:12)2(cid:21) (cid:18)
σ C
(cid:19)2(s∧1)
E (cid:12) (cid:12)F(cid:98)(X)−F(X)(cid:12)
(cid:12)
≲ [f]2
Cs∧1
reγ achf max(σ ζ,ω f) .
γ
3 Analysis of the Estimator
We introduce several properties that our model may have and will be assumed, in various
combinations, in our results. We start by collecting a few conditions on the distribution of
X,Y, and ζ that are fairly standard:
(X subG) X has sub-Gaussian distribution with variance proxy R2, and has a density function
0
ρ with [ρ ] < ∞.
X X C2
(Y subG) Y has sub-Gaussian distribution with variance proxy C2R2.
Y 0
(ζ subG) ζ is sub-Gaussian with variance proxy σ2.
ζ
Recall that X can be decomposed as position along the underlying curve and deviation
away from the curve γ as in (1). The following assumption on Z considers how random
d−1
vector X deviates off the underlying curve γ:
(γ ) γ has a Lipschitz derivative γ′ : [0,len ] → Rd. For each t ∈ [0,len ], the conditional
1 γ 0 γ
random vector Z |t = t is mean zero, isotropic with variance σ (t )2I , and
d−1 0 γ 0 d−1
supported in an Euclidean ball B(γ(t ),c reach (t )) ⊆ Rd−1 for some c < 1.
0 γ 0
Assumption (γ ) implies that for any t ∈ [0,len ], the conditional mean is on the curve
1 0 γ
µ := E[X | t = t ] = γ(t ),
t0 0 0
17Yantao Wu and Mauro Maggioni
and the conditional covariance matrix has eigenvalue 0 on the eigenspace span{γ′(t )} and
0
eigenvalue σ (t ) on eigenspace span{γ′(t )}⊥, since
γ 0 0
Σ := E[(X −µ )(X −µ )⊺ | t = t ] = σ (t )2I −σ (t )2γ′(t )γ′(t )⊺ .
t0 t0 t0 0 γ 0 d γ 0 0 0
Because the underlying curve γ is unknown, we cannot condition on t = Π X. Since
γ
we condition over the sample value Y from data (X ,Y ), we need a property that partially
i i i
reveals the “one-to-one” correspondence between t = Π (X) and F(X) = f(Π X):
γ γ
(ω ) There exist constants ω ⩾ 0 and C > C′ > 0 that only depend on the link function
f f f f
f such that, for every interval T with |T| ⩾ ω , we have
f
C f′|T| ⩽ (cid:12) (cid:12)(cid:2) minf−1(T),maxf−1(T)(cid:3)(cid:12) (cid:12) ⩽ C f|T|.
Assumption (ω ) may be regarded as a large-scale sub-Lipschitz property. If f is bi-
f
Lipschitz, and therefore in particular monotone, then (ω ) is satisfied with ω = 0. How-
f f
ever, (ω ) for ω > 0 does not imply that f is monotone: it relaxes monotonicity to
f f
monotonicity “at scales larger than ω ”, and thus we say that f is “coarsely monotone”.
f
The following assumption gives a lower bound on the conditional variance:
(LCV) Define σ := min σ (t ) as the minimum value of σ (t ). We assume that
γ t0∈[0,lenγ] γ 0 γ 0
σ ⩾ 2C max(σ ,ω ).
γ f ζ f
The purpose of Assumption (LCV) is that for any interval T ⊆ [0,len ], it allows us to
γ
compute the conditional mean
µ := E[X | t ∈ T] = E [γ(t) | t ∈ T]
T t
and conditional covariance matrix
Σ := E[(X −µ )(X −µ )⊺ | t ∈ T]
T T T
= E[(γ(t)−µ)(γ(t)−µ)⊺ | t ∈ T]+E[σ (t)2 | t ∈ T]I −E[σ (t)2γ′(t)γ′(t)⊺ | t ∈ T]
γ d−1 γ
Theaboveidentityillustratesthatwhenslicesarethinenough,SignificantVectorRegression
is approximately estimating the tangential vector γ′: if T is small enough such that the
first term has a negligible spectral norm, compared with the second and the third term,
because the second term is a multiple of identity matrix and the third term has negative
sign, the smallest principal component of Σ is roughly the largest principal component of
T
E[σ (t)2γ′(t)γ′(t)⊺ | t ∈ T], yielding an estimate of the direction of E[σ (t)γ′(t) | t ∈ T].
γ γ
Given the above, it is natural that the “thinness” of the slice is desirable: this requires a
small interval T and a lower bound on σ : this motivates Assumption (LCV), since scales
γ
below the noise level σ of the observed Y or below the rough monotonicity scale ω are
ζ i f
not valuable for our inverse regression approach.
Finally, weconsiderthefollowingproperty, whichissatisfiedbyvariousregulardensities
on compact normal domains with no geometrical symmetry required:
(P) The density ρ has compact support. Let ρ denote the push-forward measure of the
X v
distribution of X along the map x (cid:55)→ ⟨x,v⟩. Then there exists constant c > 0 such
that, for every v ∈ Sd−1 and for every interval I ⊆ suppρ , we have ρ (I) ⩾ c|I| and
v v
Var[⟨v,X⟩|⟨v,X⟩ ∈ I] ⩾ c|I|2.
√
Assumption (γ ) implies that for t ∈ [0,len ], σ (t ) < min(R ,reach / d); (LCV)
1 0 γ γ 0 0 γ
implies that σ ⩾ 2C σ ; (ω ) implies that C′C R ⩽ len ⩽ C C R .
γ f ζ f f Y 0 γ f Y 0
18Conditional regression for the Nonlinear Single-Variable Model
3.1 Estimation of slice parameters with Assumption (LCV)
Consider the event of bounded data
(cid:110) √ (cid:111)
B := ∥X∥ ⩽ C dR ,|Y| ⩽ C R
X 0 Y 0
for some C ,C ⩾ 1 fixed constant from now on. We define the following bounded version
X Y
of µ and Σ :
l,h l,h
µb := E[X|Y ∈ R ,B], Σb := Cov[X|Y ∈ R ,B]
l,h l,h l,h l,h
Given the event
√
B := {∥X ∥ ⩽ C dR ,|Y | ⩽ C R }
i i X 0 i Y 0
we define
(cid:88) (cid:88)
nb := 1 (X ), nb := 1({Y ∈ R }∩B ).
Bi i l,h i l,h i
i i
The random variable nb, assuming (X subG) and (Y subG), is larger than a constant
fraction of n with high probability (see Lemma 21). The sample counterparts of µb and
l,h
Σb are
l,h
µ (cid:98)b
l,h
:= n1
b
(cid:88) X i, Σ(cid:98)b
l,h
:= n1
b
(cid:88) (X i−µ (cid:98)b l,h)(X i−µ (cid:98)b l,h)⊺ .
l,h i:{Yi∈R l,h}∩Bi l,h i:{Yi∈R l,h}∩Bi
We denote by v lb
,h
and v (cid:98)lb
,h
the significant vector of Σb
l,h
and Σ(cid:98)b l,h, respectively. Finally, let
Hb := {h : nb ⩾ nb }. Given B, it is natural to pick the interval R in Significant Vector
l l,h l
Regression as
R := [−C R ,C R ]
Y 0 Y 0
Taking slices Y ∈ R is equivalent to conditioning on Y ∈ R . In this procedure, we
l,h l,h
obtaininformationonconditionalrandomvariablessuchastheslicecenterµb := E[X|Y ∈
l,h
R ,B], the slice covariance matrix Σb , and the slice significant vector vb . Moreover, the
l,h l,h l,h
eigenvalues λ (Σb ),...,λ (Σb ) determine features of the geometric shape for the slice
1 l,h d l,h
Σb ; in particular, the smallest eigenvalue λ (Σb ) determines the “width” of slice Sb .
l,h d l,h l,h
We will show that these parameters can be estimated with small errors given a moderate
sample size. Recall that Π : Rd → [0,L] maps points to the one-dimensional interval [0,L],
γ
that encodes the position along curve γ. We start with a proposition on estimating slice
position on curve Π X|Y ∈ T,B.
γ
Proposition 6 Suppose (ζ subG) and (ω ) hold true. Let T ⊆ R be a bounded interval
f
with |T| ⩾ max(σ ,ω ). Then:
ζ f
(a) For every i = 1,...,n and every τ ⩾ 1
(cid:110) (cid:112) (cid:111)
P |Π X −E[Π X|Y ∈ T,B]| ≳ C (|T|+ τ lognσ ) | Y ∈ T,B ⩽ 2n−τ .
γ i γ f ζ i i
(b) Var[Π X|Y ∈ T,B] ≲ C2(|T|2+σ2).
γ f ζ
19Yantao Wu and Mauro Maggioni
We now bound the estimation error for the tangential direction and the smallest eigenvalue
λ (Σb ):
d l,h
Proposition 7 Suppose (ζ subG), (γ ) and (ω ) hold true. Let T ⊆ R be a bounded
1 f
interval with |T| ⩾ max(σ ,ω ). Then:
ζ f
(a) For every i = 1,...,n and every τ ⩾ 1, we have
(cid:110)(cid:12) (cid:12) (cid:112) (cid:111)
P (cid:12)⟨vb ,X ⟩−E[⟨vb ,X⟩|Y ∈ T,B](cid:12) ≳ C (|T|+ τ lognσ ) | Y ∈ T,B ⩽ 2n−τ ;
(cid:12) l,h i l,h (cid:12) f ζ i i
(cid:104) (cid:105)
(b) λ (Σb ) = Var ⟨vb ,X⟩ | Y ∈ T,B ≲ C2(|T|2+σ2).
d l,h l,h f ζ
We now show under which assumptions λ (Σb ) is small compared to λ (Σb ) and the
d l,h d−1 l,h
other eigenvalues, yielding the “thin slice scenario”:
Corollary 8 Suppose (X subG), (Y subG), (ζ subG), (γ ), (LCV), and (ω ) hold
1 f
true. Then, for every l such that l ≳ C C R /σ , and |Rb | ⩾ max(σ ,ω ),h ∈ Hb, we
f Y 0 γ l,h ζ f l
have
λ (Σb )−λ (Σb ) ≳ σ2.
d−1 l,h d l,h γ
In part 1. b) of Algorithm 1, we compute on each slice its sample mean µb , sample
(cid:98)l,h
covariance matrix Σ(cid:98)b l,h, eigenvalues λ(cid:98)l,h,m and eigenvectors v
(cid:98)l,h,m
of the sample covariance
matrix. It is natural to ask how accurately these parameters can be estimated: we address
this in Proposition 17 and Lemma 16. These results are technical and postponed to the
appendix; here, we record that they yield the following corollary that gives the expected
√
near n -consistency in estimating the parameters in each slice, where n is the number
loc loc
of samples per slice:
Corollary 9 Suppose (X subG), (Y subG), (ζ subG), (γ ), (LCV), and (ω ) hold
1 f
true. Then, for every l such that l ≳ C C R /σ , and |Rb | ⩾ max(σ ,ω ) for all h ∈ Hb,
f Y 0 γ l,h ζ f l
(cid:16) (cid:17)2
for every ϵ > 0 and τ ⩾ 1, if n is sufficiently large so that √ n ≳ C fCYR0 dl(t +
τlogn σγ
logd+logl), we have
(cid:40) (cid:114)√ (cid:12) (cid:41)
P ∃h ∈ H lb : (cid:12) (cid:12) (cid:12)⟨v lb ,h,µ (cid:98)b l,h−µb l,h⟩(cid:12) (cid:12)
(cid:12)
≳ C YC fR 0(cid:112) t+logl+logd τ nlo lgn (cid:12) (cid:12)
(cid:12)
nb
l,h
≲ e−t+n−τ
(cid:40) (cid:114) √ (cid:12) (cid:41)
P ∃h ∈ H lb : (cid:13) (cid:13) (cid:13)v (cid:98)lb ,h−v lb ,h(cid:13) (cid:13)
(cid:13)
≳ C YC fR 02σ γ−2(cid:112) t+logl+logd d τ nl logn (cid:12) (cid:12)
(cid:12)
nb
l,h
≲ e−t+n−τ
Moreover, if n ≳ C2C2R4σ−4dl(logd+logl), then for any h ∈ Hb and p ≥ 1,
log3/2n Y f 0 γ l
(cid:20)(cid:13) (cid:13)2p(cid:21) (cid:18) lognlogl(cid:19)p
E (cid:13)vb −vb (cid:13) ≲ C(p)(C C R2σ−2)2p(dlogd)p
(cid:13)(cid:98)l,h l,h(cid:13) Y f 0 γ nl
20Conditional regression for the Nonlinear Single-Variable Model
3.2 Estimation of the distance function and classification accuracy
Here we assume (X subG), (Y subG), (ζ subG), (γ ), (LCV), and (ω ): by Corollary
1 f
8 we are in “thin” slice scenario, i.e. H > 0, and the distance function simplifies to
l,h
d(cid:100)ist(x,h) = |⟨x−µ (cid:98)b l,h,v (cid:98)lb ,h⟩|2 + λ λd 1(( ΣΣ (cid:98)(cid:98)
b
lb l ,, hh )) (cid:13) (cid:13) (cid:13)x−µ (cid:98)b l,h(cid:13) (cid:13) (cid:13)2 . In Algorithm 1, we take the slice (cid:98)h x,
which has the smallest estimated distance, to x, as an estimator of the true correct h′. The
x
following proposition states that the population counterpart h of the nearest index almost
x
equals the correct index h′.
x
Proposition 10 (nearest index is almost correct) Let h = argmin dist(x,h) be
x h∈H
l
the nearest index and define the correct index h′ be the unique h ∈ H such that F(x) ∈ R .
x l l,h
Suppose (ζ subG), (ω ), and (LCV) hold true. Suppose that |R | ⩾ max(σ ,ω ) for all
f l,h ζ f
h ∈ H . Then |h −h′| ⩽ 1. Moreover, the phenomenon of adjacent misclassification (i.e.,
l x x
|h −h′| = 1) only occurs for points near the boundary of some slices.
x x
In Algorithm 1, we use the sample slice with index(cid:98)h x, which has the smallest estimated
distancetox,toestimatethecorrecth′: thisgivesthecorrect/adjacentclassificationw.h.p.:
x
Proposition 11 (classification accuracy) Assume (X subG), (Y subG), (ζ subG),
(γ ), (LCV), and (ω ) hold true. If l ≳ C flenγ, and |R | ⩾ max(σ,ω ) for all h ∈ Hb,
1 f C f′σγ l,h f l
then the probability of misclassification by at least two slices in part 2.b of Algorithm 1
satisfies
(cid:32) (cid:32) (cid:33)(cid:33)
(cid:16)(cid:12) (cid:12) (cid:17) nl−1 C′σ4 σ8C′4
P (cid:12) (cid:12)(cid:98)h x−h x(cid:12)
(cid:12)
⩾ 2 ≲ ldexp −c√
τ logn
min C2R3df 3/γ
2len
, R8γ C4f
d4
+ln−τ .
f 0 γ 0 f
As we see from the above inequality, the probability of misclassification decays at least
linearly in √ n , as long as n and l satisfy
τlogn
(cid:32) (cid:33)
n/l R3C2 d3/2len R5C2d4 C len
≳ C := 0 f max γ , 0 f and l ≳ f γ . (5)
log3/2n γ,f C f′ σ γ4 C f′3σ γ8 C f′σ
γ
We conclude that for n large the estimation error corresponding to misclassification by
at least two slices is negligible compared with the error corresponding to correct classifi-
cation. Similar to Proposition 11, the phenomenon of misclassification to adjacent slices
seems inevitable for points near the boundary of some slices. To prevent this effect from
undermining the performance of the estimator F(cid:98), we will include data from adjacent slices
in the regression of the link function f in each local neighborhood after the projection onto
the local tangent to γ (as in step 3.b of Algorithm 1).
3.3 Function estimation error corresponding to almost correct classification
In this subsection, we work on the event that we classify x ∈ Ω to the empirical slice
γ
S(cid:98)l,h and this classification is almost correct, i.e. |h − h′ x| ⩽ 1. We include data from
(cid:83)
adjacent slices and perform the linear projection of the samples in
|h−(cid:98)h′
x|⩽1S(cid:98)l,h onto the
one-dimensional line with direction v . Let interval I(l,h) denote the range of the projected
(cid:98)l,h
21Yantao Wu and Mauro Maggioni
data {⟨v (cid:98)lb ,h,x⟩ : x ∈ (cid:83) |h−h′|⩽1S(cid:98)l,h}. We use a piecewise polynomial estimator to learn the
x
link function f in a local neighborhood. Consider the mean-squared error of the estimated
function F(cid:98)(x) = f(cid:98)
j|v (cid:98)lb
,hx(⟨v (cid:98)lb ,hx,x⟩) vs. the true function F. There are several sources for
such error: the curve approximation error between the nonlinear projection Π (·) and the
γ
local linear projection ⟨vb ,·⟩, the estimation error for vb , and the bias and variance in the
l,h l,h
nonparametric estimator f(cid:98)
j|vb
. Let f(cid:101)denote the nearest approximation of F(x) = f(Π γx)
l,h
onto the space of one-dimensional functions dependent only ⟨vb ,X⟩):
l,h
f(cid:101):= argminE(cid:20)(cid:12) (cid:12) (cid:12)Y −g(⟨v lb ,h,X⟩)(cid:12) (cid:12) (cid:12)2 1 I(l,h)(⟨v lb ,h,X⟩) (cid:12) (cid:12) X ∈ S l,h(cid:21) .
g:R1→R1
One can show that f(cid:101)has the following explicit formula
f(cid:101)(s) = E(cid:104) F(X) (cid:12) (cid:12) X ∈ S l,h, ⟨v lb ,h,X⟩ = s(cid:105)
(cid:104) (cid:105)
and hence f(cid:101)is H¨older continuous with semi-norm f(cid:101) ⩽ [f]
Cs
+ CY lR0|ρ X| Cs.
Cs
We partition the interval I(l,h) uniformly into j intervals I(l,h) , k = 1,...,j. On each
j,k
(l,h)
I , we let f be the m-order polynomial population estimator of the function f(cid:101), con-
j,k j,k|v
ditioned on projecting the data of slice S onto the unit vector v ∈ Sd−1. Joining the
l,h
f ’s together over k = 1,...,j, we obtain f as a population estimator of f(cid:101)conditioned
j,k|v j|v
on projecting the data of slice S onto the unit vector v ∈ Sd−1. This f is a piecewise
l,h j|v
m-order polynomial with j pieces:
(cid:20) (cid:21)
f j,k|v = argmin E |Y −p(⟨v,X⟩)|21 I(l,h)(⟨v,X⟩) (cid:12) (cid:12) X ∈ S l,h ,
p:R1→R1,deg(p)⩽m j,k
j
(cid:88)
f (s) = f (s)1 (s),
j|v j,k|v I j,k
k=1
Conditionedontheeventofalmostcorrectclassification|(cid:98)h x−h x| ⩽ 1,wedecomposethe
estimation error |F −f(cid:98)
j|vb
| into the following terms, which we call respectively, nonlinear
(cid:98)l,hx
curve approximation error, direction error, bias, variance, and projection error:
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)F(x)−f(cid:98)
j|v (cid:98)lb
,hx(cid:12)
(cid:12)
⩽ (cid:12) (cid:12)f(Π γx)−f(cid:101)(⟨v lb ,hx,x⟩)(cid:12) (cid:12)+(cid:12) (cid:12)f(cid:101)(⟨v lb ,hx,x⟩)−f(cid:101)(⟨v (cid:98)lb ,hx,x⟩)(cid:12)
(cid:12)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(NCA) (Φ)
(cid:12) (cid:12) (cid:12) (cid:12)
+(cid:12) (cid:12)f(cid:101)(⟨v (cid:98)lb ,hx,x⟩)−f
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12) (cid:12)+(cid:12) (cid:12)f
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)−f(cid:98)
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12)
(cid:12)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(B) (V)
(cid:12) (cid:12)
+(cid:12) (cid:12)f(cid:98)
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)−f(cid:98)
j|v (cid:98)lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12)
(cid:12)
(cid:124) (cid:123)(cid:122) (cid:125)
(Ψ)
22Conditional regression for the Nonlinear Single-Variable Model
Proposition 12 (Mean Squared Error conditioned on almost correct classification)
Consider assumptions (X subG), (Y subG), (ζ subG), (γ ), (LCV), (ω ), and (P).
1 f
Suppose f ∈ Cs with s ∈ (cid:2)1,2(cid:3) . Conditioned on almost correct classification in part 2. b)
2
in Algorithm 1, then we have the following estimates for n is sufficiently large,
(cid:32) (cid:33)2(s∧1)
MSE
(NCA)
≲[f]2 Cs∧1(σ γ(cid:13) (cid:13)γ′′(cid:13) (cid:13)C f)2(s∧1)max σ ζ,ω f, l Cen ′γ
l
f
(cid:18) lognlogl(cid:19)2(s∧1)
MSE ≲([f] +C R l−1[ρ ] )2(len R2σ−2)2(s∧1)(d2logd)s∧1
(Φ) Cs∧1 Y 0 X Cs∧1 γ 0 γ nl
(cid:32) (cid:33)2s
len
MSE ≲([f] +C R l−1[ρ ] )2C2smax σ ,ω , γ j−2s
(B) Cs Y 0 X Cs f ζ f C′l
f
ljlogj
MSE ≲σ2
(V) ζ n
(cid:32) (cid:33) 1
C2C2R4 lognlogl 2−(s∧1)
MSE ≲([f] +C R l−1[ρ ] )2 reach2 f Y 0 dlogd
(Ψ) Cs∧1 Y 0 X Cs∧1 γ σ4 nl
γ
(cid:32) (cid:33)
cϵ2n(lj)−1
+ϵ2+jexp − 0
0
([f] +|f| [ρ ]
)2reach2(s∧1)
Cs∧1 L∞ X Cs∧1 γ
+C2R2dexp(cid:32)
−
cσ γ4nl−1j−2(τ logn)− 21 (cid:33) +C2R2dexp(cid:32) −cσ γ4nl−1(cid:33)
+C2R2n−τ
Y 0 C2C2R4d(l−2+l−1j−1) Y 0 R4d2 Y 0
Y f 0 0
where free variable ϵ can be taken arbitrarily.
0
Putting together these bounds and optimizing, yields our main Theorems.
4 Numerical Experiments
We test the performance of Algorithm 1 on synthetic data to demonstrate its performance
and scalability, consistently with the main Theorems. We let the number of samples n
increasesfrom104 to106. Foreachn, werandomlypicknpointsfromtheunderlyingcurve,
and we use 90% for constructing the estimator and 10% for testing in Algorithm 1. The
algorithm requires two key scale parameters, and we will use the values l∗ and j∗ dictated
by the main Theorems. We want to study the mean squared error E[F(cid:98)n(X)−F(X)|2], the
estimation error of the center along the tangential direction E[|⟨µˆ −µ ,γ′ ⟩|], and the
l∗,h l∗,h l∗,h
difference between the significant vector and the tangential direction E[||v −γ′ ||]. For
(cid:98)l∗,h l∗,h
each n, we run the numerical estimation with five independent repetitions.
In each example in this section, we randomly generate n := 2×106 points from the
max
underlying curve γ, for which we will have a complete parametrization. We use all these
2×106 data to compute an approximation to the center µ := E[X | Y ∈ R ] and the
l,h l,h
average tangential vector γ′ :=
E[γ′(ΠγX)|Y∈R l,h]
on each slice. Here, the unit vector γ′
l,h ∥E[γ′(ΠγX)|Y∈R l,h]∥ l,h
is parallel to the average tangential direction E[γ′(Π X) | X ∈ S ] on the slice S .
γ l,h l,h
To obtain a good estimation of the nonlinear curve approximation error MSE ,
(NCA)
which we indicate as responsible for the additive term that does not go to 0 as n increases
23Yantao Wu and Mauro Maggioni
in the bound in Theorem 3, we replace the estimated parameters {(µ ,v )}l by the
(cid:98)l,h (cid:98)l,h h=1
“oracle”parameters{(µ ,γ′ )}l (computedonthen pointsasdescribedabove)when
l,h l,h h=1 max
performing the local linear projection and the local polynomial regression on each sample
slice. Wechoose(l,j)toobtaintheminimumvalueoftheMSEwhenthenumberofsamples
is n , denoted by “MSE at n = 2×106” in Fig.s 4,5,6,7,8. In this way we aim at reducing
max
the effect of any errors originating from the estimation of the parameters (µ ,v )l and
(cid:98)l,h (cid:98)l,h h=1
from the particular choice l = l∗ and j = j∗, thereby imputing the reported “MSE at
n = 2×106” mainly to the nonlinear curve approximation error MSE .
(NCA)
For all figures in this section, we use loglog plots to study how the following quantities
decay with the number of samples n: the mean squared error E[|F(cid:98)n(X) − F(X)|2], the
estimation error of the center along the tangential direction E[|⟨µˆ −µ ,γ′ ⟩|], and the
l∗,h l∗,h l∗,h
difference between the significant vector and the tangential direction E[||v −γ′ ||]. On
(cid:98)l∗,h l∗,h
the interval n ∈ [105,106], we use least squares linear regression to estimate the decaying
rate. We add a dashed vertical line n = 105 for those figures to emphasize that the learning
rate only corresponds to n ∈ [105,106]. Because the mean squared error has a term due
to the curve approximation error, one should keep in mind that the learning rate for mean
squarederrorinnumericaltestsherestillonlyindicativeofthelearningrateforn ∈ [105,106]
and may be smaller than the min-max optimal exponent 2s when the mean squared error
2s+1
is dominated by the curve approximation error and the overall mean squared error starts
to get saturated, which happens for large n. To complicate things further, such curve
approximation error depends on curvature, which in turn can depend on the dimension.
4.1 Example 1: Circular Arcs and verifying Theorem 3
We consider the simplest nonlinear curves as arcs of circles because they have constant
curvatures and can be linearly embedded in R2. We consider a collection of curves where
each is an arc of a circle. These curves are embedded in a fixed ambient dimension 20, with
a fixed length len = 1. Again, we fix σ = 0.5 and the random vector Z follows the
γ γ d−1
normal distribution N(0,σ2I ) with truncation at ∥Z ∥ < 0.9reach . The curvature of
γ d−1 d−1 γ
these curves is varied across realizations, ranging from 0.04 to 0.4. We set our upper bound
for the curvature to be 0.4 because otherwise reach becomes too small and the variance of
γ
Z will no longer be approximately σ2.
d−1 γ
Theorem3claimsthatthecurveapproximationerrorisproportionalto∥γ′′∥2(s∧1),which
is smaller for curve with smaller curvature: we verify that claim by the following numerical
test. We choose f(t) = exp(t) for t ∈ [0,1], which has smoothness exponent s ≥ 2. The
external noise follows the normal distribution N(0,σ2) with σ = 0.03. Figure 4 supports
ζ ζ
the following conclusions, consistent with Theorem 3 and our analysis: (i) when σ > 0,
ζ
then the mean squared error has a nonzero lower bound independent of n; (ii) the curve
approximation error is proportional to ∥γ′′∥2(s∧1); (iii) Corollary 9 states that the decaying
rate is O(n− 21 ) for E[|⟨µˆ l∗,h−µ l∗,h,γ l′ ∗,h⟩|] and E[||µˆ l∗,h−γ l′ ∗,h||].
4.2 Meyer helix
We perform numerical tests for a collection of curves called “Meyer helix” in Rd, which
we construct so that the curve complexity grows with the ambient dimension d. This is
24Conditional regression for the Nonlinear Single-Variable Model
10-5
10-4
10-6
10-5
104 105 106 10-1
105 105
10-3 10-2
10-4
10-3
104 105 106 104 105 106
Figure 4: Numerical Tests in Section 4.1: Circular arcs embedded in Rd, d = 20, with
unit length and curvature varying in [0.04,0.4]. We fix the noise level σ = 0.03. Upper
ζ
row: MSE for F(cid:98) (left) and MSE at n = 2×106 as a function of curvature (right); Bottom
row: estimation error for the center along tangential direction (left) and difference between
estimated significant vector and the tangential direction (right) over 5 runs.
inspired by a curve called the Meyer staircase (named after Y. Meyer), defined by the
map [0,1] → Lp(R), for some p ≥ 1, given by t (cid:55)→ 1 (· − t). We smooth out this
[0,1]
2
original example by considering translations of a Gaussian, and we induce further twists
in the curve to increase its complexity by introducing the Meyer helix as a variation. We
measure the growing complexity of this family of curves as a function of d, and show that
len ≍ d1.5,diam ≍ d0.5,|γ′′| ≍ d−0.5,reach ≍ d0.5, and effective linear dimension ≍ d1:
γ γ γ
see details in Appendix C. This collection of curves allows us to verify the performance of
our algorithm when varying d: we fix σ = 0.5 and let the random vector Z follow the
γ √ d−1
normal distribution N(0,σ2I ) with truncation at |Z | < 0.9 d.
γ d−1 d−1
25Yantao Wu and Mauro Maggioni
105
10-4
10-1
10-2
10-3
10-4 10-5
104 105 106 0.05 0.1 0.15 0.2
105 105
10-2 10-1
10-2
10-3
104 105 106 104 105 106
Figure 5: Numerical Tests in Section 4.2.1: Meyer helix in d = 7 dimensions, with f of
smoothness exponent s = 2, and noise level σ
ζ
varying in [0.05,0.2]. Top row: MSE for F(cid:98)
(left) and MSE at n = 2×106 as a function of σ (right); Bottom row: estimation error of
ζ
center along tangential direction (left) and difference between estimated significant vector
and tangential direction (right), over five independent runs.
4.2.1 Example: Verifying Theorem 3 and Corollary 9
We let the underlying curve γ to be the Meyer helix in d = 7 dimensions, which has
len = 53.78 and reach = 2.65. Consider the link function f(t) = len ·exp(t/len ) for t ∈
γ γ γ
[0,len ]whichhassmoothnessexponents = 2. Theobservationalnoiseζ followsthenormal
γ
distribution N(0,σ2) where the noise level σ varies from 0.05 to 0.2. Figure 5 supports
ζ ζ
the following theoretical conclusions: (i) When σ > 0, then the mean squared error has a
ζ
2(s∧1)
nonzero lower bound. (ii) The curve approximation error is proportional to σ . (iii)
ζ
consistently with Corollary 9, the learning rate is O(n−1 2) for E[|⟨µˆ
l∗,h
−µ l∗,h,γ l′ ∗,h⟩|] and
E[||µˆ −γ′ ||].
l∗,h l∗,h
26Conditional regression for the Nonlinear Single-Variable Model
4.2.2 Example: Verifying Theorem 3 and Corollary 9
We consider a collection of Meyer helix curves with the following ambient dimensions and
geometric parameters:
d 3 4 5 6 7 8 9
len 20.86 33.39 35.35 43.89 53.78 90.20 96.65
γ
reach 1.73 2.00 2.24 2.45 2.65 2.83 3.00
γ
Considerthelinkfunctionf(t) = len ·exp(t/len )fort ∈ [0,len ]whichhassmoothness
γ γ γ
exponent s = 2. The observational noise ζ follows the normal distribution N(0,σ2) where
ζ
the magnitude of the noise is σ = 0.2.
ζ
105 105 2.2 10-4
2.0 10-4
1.8 10-4
10-1 10-4
1.6 10-4
1.4 10-4
1.2 10-4
10-2 10-5
1.0 10-4
10-3
10-6 8.0 10-5
6.0 10-5
104 105 106 104 105 106 4 6 8
Figure 6: Numerical Tests in Section 4.2.2: Meyer helix curves in Rd, d ∈ {3,...,9}, f
with smoothness exponent s = 2, and σ
ζ
= 0.2. Left: Mean Squared Error for F(cid:98). Middle:
L2-relative Mean Squared Error, E[|F(cid:98) − F|2]/E[|F(X) − E(F(X))|2], computed over five
independent runs. Right: Mean Squared Error of F(cid:98) at n = 2×106 as a function of d.
In the upper-left plot of Figure 6, we observe that, for fixed n = 105, the mean squared
error is larger for Meyer helix curves with in ambient dimension d. This is because the
coefficient before n− 2s2 +s 1 in Theorem 3 is larger for curves with larger len γ. In upper right
plot,weobservethat,bycontrast,themeansquarederroratn = 2×106 issmallerforMeyer
helix curves in larger ambient dimension d. This verifies the statement in Theorem 3 that
the curve approximation error is proportional to reach−2s∧1. Some observations about this
γ
example: the Meyer helix in smaller d has smaller len ,reach , and larger curvature, and
γ γ
therefore: (i) the requirement for the number of samples for Theorem 3 is smaller; (ii) the
first term in the mean squared error, which is O(n− 2s2 +s 1 logn), has a smaller coefficient; (iii)
the second term in the bound for the mean squared error in Theorem 3, which is the curve
approximation error, has larger magnitude; (iv) if we denote by n the number of samples
1
27Yantao Wu and Mauro Maggioni
105
10-1
104
102
10-2
100
10-3
10-2
10-4 10-4
10-6
104 105 106 104 105 106
Figure 7: Numerical Tests in Section 4.2.3: the collection “Meyer helix” of curves with
varying dimension. Left: d = 5,...,10. Right: d = 6,12,24,36,48. For all tests, we choose
zero observational noise ζ ≡ 0, and the link function is Lipschitz (i.e., smoothness exponent
s = 1). These errors are computed as the average values in five independent repetitions.
such that first term balances the second term in the upper bound for the mean squared
error, n increases with d. As a consequence, on the particular interval n = [105,106] where
1
the decaying rate is calculated, one need to pay attention to the value of n that determines
1
which term dominates the mean squared error. In particular, the Meyer helix in d = 3
dimensions has n ≊ 105, with the MSE exhibiting good decay for n ≤ n , and saturating
1 1
for n ≥ n . In contrast, the Meyer helix in d = 7 deimsnsions has n ≊ 106, and thus
1 1
we observe a good decay rate on the interval n ∈ [105,106]. For the Meyer helix in higher
dimension, therequirementforthenumberofsamplesbyTheorem3isevenlarger. Thereis
another phenomenon appearing in this numerical test: we notice that for d = 8,9, the error
increases when n increase from 104 to 2×104 and decreases when n ⩾ 4×104. First, this
is not a contradiction with Theorem 3 because the requirement for the minimal number of
samples is not satisfied for the Meyer helix in dimension d = 8,9 when n ⩽ 3×104. Second,
this increase of error for small n is due to the transition from the “wide” slice scenario to
the “thin” slice scenario. Further investigation on the average empirical geometric quantity
l1
∗
(cid:80)l h∗ =1H(cid:98)l∗,h shows that we have the “wide” slice scenario when n ⩽ 3×103 and have the
“thin” slice scenario when n ⩾ 4×104. For n ∈ [3×104,4×104], the average empirical
geometric quantity l1
∗
(cid:80)l h∗ =1H(cid:98)l∗,h ≈ 0 and thus the slices are roughly isotropic which, as
discussed in Section 2.1, prevents an accurate estimate of the significant vector.
Figure 6 supports the following conclusions, in line with our theoretical analysis: (i) the
constants in O(n−2s/(2s+1)) are bigger for curves with bigger len ; (ii) the requirement for
γ
the number of samples so that Theorem 3 holds is larger for more complex curves; (iii) the
significant vector cannot be estimated well when the geometric shape of a slice is roughly
isotropic; (iv) the value of n is larger for curves with larger length and larger reach, where
1
n is the number of samples such that the first term balances the second term in the upper
1
−2(s∧1)
bound for the MSE; (v) the curve approximation error is proportional to reach ;
γ
28Conditional regression for the Nonlinear Single-Variable Model
4.2.3 Example: Verifying Theorem 4
We consider a collection of Meyer helix curves in the following ambient dimensions and
information:
d 5 6 7 8 9 10 12 24 36 48
len 35.35 43.89 53.78 90.20 96.65 93.88 135.76 435.48 730.62 1306.78
γ
reach 2.24 2.45 2.65 2.83 3.00 3.16 3.46 4.90 6.00 6.93
γ
NotethatforMeyerhelixcurveswithd = 5,...,12,thenumberofdataissufficientwhen
n ≥ 104, while for d = 24,36,48, the requirement for the number of data is substantially
larger. Figure 7 supports the following theoretical conclusions: (i) when σ = 0, then the
ζ
mean squared error has no lower bound. The curve approximation error here is zero; (ii)
the decaying rate is O(n−2) if the link function f is monotone and Lipschitz; (iii) The
requirement for the number of data is larger for more complex curves.
5 Robustness: Performance of Algorithm 1 in general setting
Recall that Assumption (LCV) gives a quantitative requirement for the “thin” slice sce-
nario. If we relax this assumption and consider instead the “wide” slice scenario, we expect
that the largest principal component on each slice gives a proper approximation of the
tangential direction under some assumption. Small curvature will do:
(SC) Define σ := min σ (t ) as the minimum value of σ (t ). We assume that
γ t0∈[0,lenγ] γ 0 γ 0
thereexistc ,c > 0suchthatσ < c C max(σ ,ω )andC max(σ ,ω ) ⩽ c reach .
1 2 γ 1 f ζ f f ζ f 2 γ
Proposition 13 ((SC) implies “wide” slices) Suppose(XsubG),(YsubG),(ζ subG),
(γ ), and (ω ) hold true, together with (SC), with c ,c smaller than a small-enough uni-
1 f 1 2
versal constant. Then, for every l such that |R | ≍ max(σ ,ω ),h ∈ H , we have
l,h ζ f l
λ (Σb )−λ (Σb ) ≳ C2|R |2
1 l,h 2 l,h f l,h
Recall that we have the following distance function in this situation of “wide” slices, i.e.
(cid:13) (cid:13)2 λ (Σb )
H < 0, dist(x,h) = (cid:13)x−µb (cid:13) + d l,h |⟨x−µb ,vb ⟩|2, and similarly for its empirical
l,h (cid:13) l,h(cid:13) λ1(Σb l,h) l,h l,h
counterpart. With a proof as in Proposition 11, we conclude that
Proposition 14 (classification accuracy without (LCV)) Assume that (X subG),
(Y subG), (ζ subG), (γ ), (ω ), and (SC) hold true. Fix l = CYR0 . Then, the
1 f max(σ,ω )
f
probability of misclassification by at least two slices in part 2. b) of Algorithm 1 can be
bounded by
(cid:32) (cid:33)
P(cid:16)(cid:12) (cid:12) (cid:12)(cid:98)h x−h x(cid:12) (cid:12)
(cid:12)
⩾ 2(cid:17) ≲ ldexp −cC Cf6m Rax 7d(σ 3√ζ, lω of g) n7n +ln−τ
Y 0
5.1 Example: Verifying Theorem 5
Recall that in the “wide” slice scenario, we have a nontrivial error that does not vanish
as n grows and is at the magnitude of the curve approximation error. We will verify
29Yantao Wu and Mauro Maggioni
105
102
101
10-2
100
10-1
104 105 106 2 3 4 5 6
Figure 8: Numerical Tests in Section 5.1: Meyer helix in d = 21 dimensions. The link
function has smoothness exponent s = 2, and the noise level σ varies from 2 to 6. Left:
ζ
mean squared error; Right: mean squared error at n = 2×106; These errors are computed
as the average values in five independent repetitions.
this in the following numerical test. Let the underlying curve γ to be the Meyer helix
in d = 21 dimension, which has len = 370.63 and reach = 4.58. The link function
γ γ
f(t) = len exp(t/len ) for t ∈ [0,len ] has smoothness exponent s = 2. The external noise
γ γ γ
ζ follows the normal distribution N(0,σ2) where the noise level varies from 2 to 6. Note
ζ
that assumption (LCV) is not satisfied in this case.
Figure 8 verifies that when the assumption (LCV) is not satisfied, then the mean
squared error saturates at the level of the curve approximation error.
6 Conclusion
We introduced the Nonlinear Single-Variable Model, which is a compositional model F =
f ◦g for functions in high-dimensions where both f and g are nonlinear, but g has a one-
dimensional range, and f is therefore a function of only one dimension. Thanks for the
geometric structure inherent in g, using techniques based on inverse regression, at least
when f is roughly monotone, we are able to efficiently estimate the level sets of g and
then learn f, both in a nonparametric fashion, with learning rates and sample requirements
not cursed by the ambient dimension, and with computationally efficient algorithms for
constructing the estimators that scale linearly in the number of samples and with constants
moderately depending on the ambient dimension.
Future directions include the extension to functions f that are not roughly monotone
presents challenges for inverse regression method, but it is also so for other techniques, for
exampleforstochasticgradientmethodsinthesingle-andmulti-indexmodels(Arousetal.,
2021;Biettietal.,2023),andtherearestillgapsintheunderstandingofthesharpstatistical
and computational tradeoffs. Another extension of interest is to a Nonlinear Multi-Variable
Model, where the curve γ is replaced by a higher-dimensional manifold M, also presents
30Conditional regression for the Nonlinear Single-Variable Model
challenges, as the geometry of the level sets of F becomes significantly more complicated.
Both these extensions are subjects of current investigation and left to future work.
Understanding how compositional structure affects the design of estimators, or specific
“general purpose” algorithms for constructing them (such as SGD applied to a suitable
loss function), especially in the case of multiple compositions, beyond just two, and when
and how such structure can help avoid the curse of dimensionality, is an interesting area of
research with many open questions.
Acknowledgments and Disclosure of Funding
The authors are grateful to A. Lanteri, S. Vigogna and T. Klock (during his visit to Johns
Hopkins) for early discussions and experiments on this model, and to Fei Lu and Xiong
Wang and their feedback on an early version of this manuscript. This research was partially
supported by AFOSR FA9550-21-1-0317 and FA9550-23-1-0445. Prisma Analytics Inc.
provided and managed the computing resources for the numerical experiments.
Appendix A. Proof of Propositions
Lemma 15 (Matrix Bernstein Inequality (Vershynin, 2018, Theorem 5.4.1) ) Let
M ,..., M be a sequence of i.i.d. d × d dimensional random matrices with EM =
1 n 1 2 i
c0 o, v∥ aM
rii
a∥ n⩽ ceB no( rb mou σn 2de =d mop ae xra (∥to Er [Xno Xrm
⊺
]) ∥. ,D ∥Een [Xot ⊺e Xsa ]∥m ).pl Te hm enean M(cid:99) = n1 (cid:80) i⩽nM
i
and denote
i i i i
(cid:16)(cid:13) (cid:13) (cid:17) (cid:18) cnt2 (cid:19)
P (cid:13) (cid:13)M(cid:99)(cid:13)
(cid:13)
≳ t ≲ (d 1+d 2)exp −
Bt+σ2
Lemma 16 (Concentration Inequalities for means, covariances and eigenvalues)
√
Suppose that X are iid bounded by ∥X ∥ ⩽ R d, let µb = E[X] and µb = 1 (cid:80) X
i i 0 l,h (cid:98)l,h nb
l,h
i⩽nb
l,h
i
denote the mean and sample mean. Let Σb = E[(X −µb )(X −µb )⊺ ] denote the covari-
l,h l,h l,h
ance matrix, Σ(cid:98)b
l,h
=
nb
l1
,h
(cid:80)
i⩽nb
l,h(X
i
−µ (cid:98)b l,h)(X
i
−µ (cid:98)b l,h)⊺ the sample covariance matrix, and
Σ(cid:101)b
l,h
=
nb
l1
,h
(cid:80)
i⩽nb
l,h(X i−µb l,h)(X i−µb l,h)⊺ be the augmented covariance matrix. Then,
(cid:32) (cid:13) (cid:13) (cid:114) δ(cid:33) (cid:32) cnb δ(cid:33)
P (cid:13)µb −µb (cid:13) ≳ R ≲ dexp − l,h , where δ < αd
(cid:13)(cid:98)l,h l,h(cid:13) 0 α αd
(cid:32) (cid:33)
(cid:18)(cid:13) (cid:13) R2(cid:19) cβ2nb
P (cid:13)Σ(cid:101)b −Σb (cid:13) ≳ β 0 ≲ dexp − l,h , where β < αd
(cid:13) l,h l,h(cid:13) α α2d2
(cid:32) (cid:33)
(cid:18)(cid:12) (cid:12) R2(cid:19) cβ2nb
P (cid:12) (cid:12)λ 1(Σ(cid:98)b l,h)−λ 1(Σb l,h)(cid:12)
(cid:12)
≳ β α0 ≲ dexp − α2dl 2,h , where β < αd
Proof ThefirsttwoinequalitiescanbeshowndirectlybyLemma15. WecansplitΣ(cid:98)b −Σb
l,h l,h
into Σ(cid:98)b
l,h
−Σb
l,h
= Σ(cid:101)b
l,h
−Σb
l,h
+(µ (cid:98)b
l,h
−µb l,h)(µ (cid:98)b
l,h
−µb l,h)⊺ . To bound deviation in λ 1(Σ(cid:98)b l,h),
31Yantao Wu and Mauro Maggioni
one can directly use Weyl’s inequality
(cid:12) (cid:12) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2
(cid:12) (cid:12)λ 1(Σ(cid:98)b l,h)−λ 1(Σb l,h)(cid:12)
(cid:12)
⩽ (cid:13) (cid:13)Σ(cid:98)b l,h−Σb l,h(cid:13)
(cid:13)
⩽ (cid:13) (cid:13)Σ(cid:101)b l,h−Σb l,h(cid:13) (cid:13)+(cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13)
(cid:13)
.
The third inequality follows from the first two inequalities.
A.1 Proof of Proposition 6
√ √
(cid:112) (cid:112)
Proof Define intervals I := (− 2(k+1)σ , 2kσ ] ∪ [ 2kσ , 2(k+1)σ ) for k =
k ζ ζ ζ ζ
(cid:83)
0,1,2,.... We first note that, thanks to (ζ subG), we have ζ ∈ I with
i k⩽τlogn k
probability higher than 1 − 2n−τ, for every i = 1,...,n. Conditioned on this event
and on Y ∈ T, Π X ∈ f−1(T + (cid:83) I ). Meanwhile, E[Π X|Y ∈ T,B,ζ ∈ I ] ∈
i γ i k≤τlogn k γ k
[minf−1(T + I ),maxf−1(T + I )]. It follows from assumption (ω ) that we have an
k k f
(cid:83)
absolute bound upon conditioning on Y ∈ T,B,ζ ∈ I :
i i k≤τlogn k
(cid:112)
|Π X −E[Π X|Y ∈ T,B,ζ ∈ I ]| ⩽ C (|T|+ max(k,τ logn)σ ).
γ i γ k f ζ
By the law of total expectation,
∞
(cid:88)
|Π X −E[Π X|Y ∈ T,B]| ⩽ |Π X −E[Π X|Y ∈ T,B,ζ ∈ I ]|P(ζ ∈ I )
γ i γ γ i γ k k
k=0
√
(cid:112) (cid:88) (cid:112)
≲C (|T|+ τ lognσ +σ ke−k) ≲ C (|T|+ τ lognσ )
f ζ ζ f ζ
k>τlogn
which finishes the proof of (a). For (b), we use the law of total expectation to write
(cid:104) (cid:105)
Var[Π X|Y ∈ T,B] = E (Π X −E[Π X|Y ∈ T,B])2 | Y ∈ T,B
γ γ γ
∞
(cid:88) (cid:104) (cid:105)
= E (Π X −E[Π X|Y ∈ T,B])2 | Y ∈ T,B,ζ ∈ I P{ζ ∈ I } .
γ γ k k
k=0
To bound each term, we follow the same approach as in part (a) to bound conditional
random variable Π X | Y ∈ T,B,ζ ∈ I
γ k
√
|Π X | Y ∈ T,B,ζ ∈ I −E[Π X|Y ∈ T,B]| ≲C (|T|+ kσ ),
γ k γ f ζ
whence
(cid:18) ∞ (cid:19)
(cid:88)
Var[Π X|Y ∈ T,B] ≲ C2 |T|2+σ2 te−t ≲ C2(|T|2+σ2).
γ f ζ f ζ
t=0
32Conditional regression for the Nonlinear Single-Variable Model
A.2 Proof of Proposition 7 and Corollary 8
Proof Recall that that X = γ(t )+W where t = Π X denotes position of X along
i i i i γ i i
(cid:18) W′(cid:19)
curve and W = M i denotes the deviation off the curve. Here each M ∈ O(d) is
i γ′(ti) 0 v
a rotation matrix on Rd that maps d-th canonical unit vector eˆ = (0,...,0,1) to the unit
d
vector v ∈ Sd−1. Observe that ⟨vb ,X ⟩ = ⟨vb ,γ(Π X )⟩+⟨vb ,W ⟩, so we will show a
l,h i l,h γ i l,h i
high probability bound for each term.
Wewillutilizethecontractionpropertyofγ forthefirstterm. Noticethatγ : [0,L] → Rd
isacontractionmap, i.e., ithasLipschitzconstant1: ∥γ(t )−γ(t )∥ ⩽ |t −t |. Recallthat
1 2 1 2
(cid:83)
in Proposition 6 part (a) we show that conditioned on event that ζ ∈ Z and Y ∈
i t⩽τlogn t i
T, we have Π X ∈ f−1(T +(cid:83) Z ). As a consequence, the contraction property of γ
γ i t⩽τlogn t
showsthatconditionedonthesameevent, wealsohaveγ(Π X ) ∈
γ(f−1(T+(cid:83)
Z ))
γ i t⩽τlogn t
whose diameter is bounded by diameter of f−1(T +(cid:83) Z ). Following the proof of
t⩽τlogn t
Proposition 6, we have
(cid:110) (cid:112) (cid:111)
P ∥γ(Π X )−E[γ(Π X ) | Y ∈ T,B]∥ ≳ C (|T|+ τ lognσ ) | Y ∈ T,B ⩽ 2n−τ
γ i γ i f ζ i i
∥Cov[γ(Π X)|Y ∈ T,B]∥ ≲ C2(|T|2+σ2)
γ f ζ
and as a consequence,
(cid:110)(cid:12) (cid:12) (cid:112) (cid:111)
P (cid:12)⟨vb ,γ(Π X )⟩−E[⟨vb ,γ(Π X)⟩ | Y ∈ T,B](cid:12) ≳ C (|T|+ τ lognσ ) | Y ∈ T,B ⩽ 2n−τ
(cid:12) l,h γ i l,h γ (cid:12) f ζ i i
Var[⟨vb ,γ(Π X)⟩|Y ∈ T,B] ≲ C2(|T|2+σ2)
l,h γ f ζ
By construction, W′ ∈ Rd−1 are independent, identical, and centered distribution on
i
B(0,reach ) ⊆ Rd−1 (each W′ may be dependent of t = Π X ). Moreover, conditioned on
γ i i γ i
Y ∈ T and B , the geometric assumption (γ ) implies that
i i 1
(cid:12) (cid:12)
(cid:12)⟨W ,vb ⟩(cid:12) ⩽ 2|Π X −E[Π X|Y ∈ T,B]| .
(cid:12) i l,h (cid:12) γ i γ
Follow the same procedure as in Proposition 6 part (a) we conclude that
(cid:110)(cid:12) (cid:12) (cid:112) (cid:111)
P (cid:12)⟨vb ,W ⟩−E[⟨vb ,W⟩ | Y ∈ T,B](cid:12) ≳ C (|T|+ τ lognσ ) | Y ∈ T,B ⩽ 2n−τ ,
(cid:12) l,h i l,h (cid:12) f ζ i i
Var[⟨vb ,W⟩|Y ∈ T,B] ≲ C2(|T|2+σ2),
l,h f ζ
We combine the above high probability bounds together and conclude that
(cid:110)(cid:12) (cid:12) (cid:112) (cid:111)
P (cid:12)⟨vb ,X ⟩−E[⟨vb ,X⟩ | Y ∈ T,B](cid:12) ≳ C (|T|+ τ lognσ ) | Y ∈ T,B ⩽ 2n−τ ,
(cid:12) l,h i l,h (cid:12) f ζ i i
and
λ (Σb ) := Var[⟨vb ,X⟩|Y ∈ T,B] ≲ C2(|T|2+σ2).
d l,h l,h f ζ
ByassumptionintheCorollary,wehavel ≳ C C R /σ and CYR0 ∼ = |Rb | ≳ max(σ ,ω ).
f Y 0 γ l l,h ζ f
This implies that the above right-hand side is bounded by C2|Rb |2 ≲ σ2, and on the other
f l,h γ
hand, assumption (LCV) implies that λ (Σb ) ≳ σ2.
d−1 l,h γ
33Yantao Wu and Mauro Maggioni
A.3 Proof of Proposition 17 and Corollary 9
Proposition 17 (local NVM) Suppose(XsubG),(YsubG),(ζ subG),(γ ), (LCV),
1
and (ω ) hold true. Let µb be the mean of the h-th slice and vb be the significant vector
f l,h l,h
of the h-th slice. Then, for every l such that l ≳ C C R /σ , |Rb | ⩾ max{σ ,ω } for all
f Y 0 γ l,h ζ f
h ∈ Hb, for every ϵ ∈ (0,1) and τ ⩾ 1, we have:
l
(a) for any h ∈ H and any ϵ > 0, the estimation error of the slice mean along the
l
tangential direction can be bounded as
P(cid:40) (cid:12)
(cid:12) (cid:12)⟨v lb ,h,µ (cid:98)b l,h−µb
l,h⟩(cid:12)
(cid:12)
(cid:12)
>
Rσ γ2 √ϵ
d
(cid:12)
(cid:12) (cid:12)
(cid:12)
nb
l,h(cid:41)
≲
dexp(cid:32)
−
Cc 2σ Cγ4ϵ 22 Rnb
l 4,h
d( (τ l−l 2og +n l) −− 121 ϵ)(cid:33)
+n−τ ;
0 Y f 0
(b) for any h ∈ H and any ϵ > 0, for l ≳ C C R /σ , the estimation error of the
l f Y 0 γ
significant vector can be bounded as,
P(cid:110)(cid:13) (cid:13) (cid:13)v (cid:98)lb ,h−v lb ,h(cid:13) (cid:13)
(cid:13)
> ϵ (cid:12) (cid:12) nb l,h(cid:111) ≲ dexp(cid:32) − Cc 2σ Cγ4ϵ 22 Rnb l 4,h d( (τ l−l 2og +n l) −− 121 ϵ)(cid:33) +dexp(cid:32) −cσ Rγ4 4n db l 2,h(cid:33) +n−τ ;
Y f 0 0
(c) for any h ∈ H , u > 0, for l ≳ C C R /σ , the estimation error of the width of the
l f Y 0 γ
slice can be bounded as,
P(cid:0)(cid:12)
(cid:12) (cid:12)λ d(Σ(cid:98)b l,h)−λ d(Σb
l,h)(cid:12)
(cid:12)
(cid:12)
>C f2C Y2R 02u2/l2
(cid:12)
(cid:12) (cid:12)
(cid:12)
nb l,h(cid:1) ≲
dexp(cid:32) −c Rσ γ4 4C d3Y2 /2u (2 √nb
l d,h
+(τ ulo Cgn C)− )21(cid:33)
0 f Y
(cid:32) (cid:33) (cid:32) (cid:33)
cu4nb cσ4nb
+exp − l,h +dexp − γ l,h +n−τ .
1+u2 R4d2
0
Proof of part (a): By Proposition 7 part (a), we know that conditioned on Y ∈ Rb and
i l,h
(cid:12) (cid:104) (cid:105)(cid:12)
B , withprobabilityhigherthan1−2n−τ, wehave(cid:12)⟨vb ,X ⟩−E ⟨vb ,X⟩ | Y ∈ Rb ,B (cid:12) ≲
i (cid:12) l,h i l,h l,h (cid:12)
√ (cid:104) (cid:105)
C C R τ lognl−1. AlsoProposition7part(b)impliesthatVar ⟨vb ,X⟩ | Y ∈ Rb ,B ≲
f Y 0 l,h l,h
C2C2R2l−2. Therefore, we have the following Bernstein-type inequality for any ϵ > 0:
f Y 0
P(cid:32) (cid:12)
(cid:12)⟨vb ,µb −µb
⟩(cid:12)
(cid:12) >
σ γ2 √ϵ (cid:12)
(cid:12) nb
(cid:33)
≲
dexp(cid:32)
−
cσ γ4ϵ2nb l,h(τ logn)− 21 (cid:33)
+n−τ
(cid:12) l,h (cid:98)l,h l,h (cid:12) R d (cid:12) l,h C2C2R4d(l−2+l−1ϵ)
0 f Y 0
Proof of part (b) The main tool is the following Davis-Kahan type inequality. The
Davis–Kahan Theorem (Bhatia, 1997, Theorem VII.3.1), together with (Stewart and guang
Sun, 1990, Ch. 1, Sec. 5.3, Theorem 5.5) and Corollary 8, gives
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)(Σ(cid:98)b l,h−Σb l,h)v lb ,h(cid:13)
(cid:13)
(cid:13)vb −vb (cid:13) ⩽ (6)
(cid:13)(cid:98)l,h l,h(cid:13)
λ d−1(Σ(cid:98)b l,h)−λ d(Σb l,h)
34Conditional regression for the Nonlinear Single-Variable Model
Step1: We want the denominator of right hand side of (6) to be large. By Corollary 8 and
Weyl’s inequality (Vershynin, 2018, Theorem 4.5.3) we obtain that for l ≳ C C R /σ ,
f Y 0 γ
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:13) (cid:13)
(cid:12) (cid:12)λ d−1(Σ(cid:98)b l,h)−λ d(Σb l,h)(cid:12)
(cid:12)
⩾ (cid:12) (cid:12)λ d−1(Σb l,h)−λ d(Σb l,h)(cid:12) (cid:12)−(cid:12) (cid:12)λ d−1(Σ(cid:98)b l,h)−λ d(Σb l,h)(cid:12)
(cid:12)
≳ σ γ2 −(cid:13) (cid:13)Σ(cid:98)b l,h−Σb l,h(cid:13)
(cid:13)
.
To bound Σ(cid:98)b
l,h
−Σb l,h, we split it as Σ(cid:98)b
l,h
−Σb
l,h
= Σ(cid:101)b
l,h
−Σb
l,h
+(µ (cid:98)b
l,h
−µb l,h)(µ (cid:98)b
l,h
−µb l,h)⊺ ,
where we introduced the intermediate-term
Σ(cid:101)b
l,h
= n1
b
(cid:88) (X i−µb l,h)(X i−µb l,h)⊺1{Y
i
∈ R lb ,h}∩B i.
l,h i
(cid:13) (cid:13) (cid:13) (cid:13)2
We use Lemma 16 to obtain a high probability bound on (cid:13) (cid:13)Σ(cid:101)b l,h−Σb l,h(cid:13)
(cid:13)
and (cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13)
(cid:13)
:
for a fixed constant β < min(1,αd) = 1,
2 2
(cid:32) (cid:33)
P(cid:18) max(cid:18)(cid:13)
(cid:13) (cid:13)Σ(cid:101)b l,h−Σb
l,h(cid:13)
(cid:13)
(cid:13),(cid:13)
(cid:13) (cid:13)µ (cid:98)b l,h−µb
l,h(cid:13)
(cid:13)
(cid:13)2(cid:19)
≳ βσ
γ2(cid:19)
≲ dexp
−cβ R2σ 4γ4 dn 2b
l,h .
0
(cid:12) (cid:12)
Thiswillshowthatthedenominatoroftherighthandsideof (6)is(cid:12) (cid:12)λ d−1(Σ(cid:98)b l,h)−λ d(Σb l,h)(cid:12)
(cid:12)
≳
σ2.
γ
(cid:13) (cid:13)
Step2: WearegoingtoapplyBernsteininequalitytoupperboundthenumerator(cid:13)(Σ(cid:98)b −Σb )vb (cid:13)
(cid:13) l,h l,h l,h(cid:13)
in the right-hand side of (6). Consider the following decomposition:
(cid:13) (cid:13) (cid:13) (cid:13) (cid:12) (cid:12)(cid:13) (cid:13)
(cid:13) (cid:13)(Σ(cid:98)b l,h−Σb l,h)v lb ,h(cid:13)
(cid:13)
⩽ (cid:13) (cid:13)(Σ(cid:101)b l,h−Σb l,h)v lb ,h(cid:13) (cid:13)+(cid:12) (cid:12)⟨v lb ,h,µ (cid:98)b l,h−µb l,h⟩(cid:12) (cid:12)(cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13)
(cid:13)
(cid:13) (cid:13) √ (cid:12) (cid:12)
≲(cid:13) (cid:13)(Σ(cid:101)b l,h−Σb l,h)v lb ,h(cid:13) (cid:13)+R
0
d(cid:12) (cid:12)⟨v lb ,h,µ (cid:98)b l,h−µb l,h⟩(cid:12)
(cid:12)
(cid:12) (cid:12)
Recall that part (a) already gives desired bound for (cid:12)⟨vb ,µb −µb ⟩(cid:12), so we only need
(cid:12) l,h (cid:98)l,h l,h (cid:12)
(cid:13) (cid:13) (cid:13) (cid:13)
to bound (cid:13)(Σ(cid:101)b −Σb )vb (cid:13). Observe that (cid:13)(Σ(cid:101)b −Σb )vb (cid:13) has a priori upper-bound by
(cid:13) l,h l,h l,h(cid:13) (cid:13) l,h l,h l,h(cid:13)
⊺ (cid:13) (cid:13)
|vb (X −µb )|(cid:13)X −µb (cid:13), Moreover, by Proposition 7 part (a), conditioned on Y ∈ S
l,h i l,h (cid:13) i l,h(cid:13) i l,h
⊺ (cid:13) (cid:13)
and B , we have, with probability no lower than 1−2n−τ, |vb (X −µb )|(cid:13)X −µb (cid:13) ≲
i l,h i l,h (cid:13) i l,h(cid:13)
√ (cid:13) (cid:13)
C fC YR 02 dτ lognl−1. This serves as an ℓ∞ bound on (cid:13) (cid:13)(Σ(cid:101)b l,h−Σb l,h)v lb ,h(cid:13) (cid:13).
Next,weconsidertheℓ2 bound(i.e.,variance). Consideringthefollowingdecomposition:
(cid:20)(cid:13) (cid:13)2 (cid:21) (cid:20)(cid:13) (cid:13)2 (cid:21) (cid:13) (cid:13)2
E (cid:13)(Σ(cid:101)b −Σb )vb (cid:13) | nb = E (cid:13)Σ(cid:101)b vb (cid:13) | nb −(cid:13)Σb vb (cid:13) ,
(cid:13) l,h l,h l,h(cid:13) l,h (cid:13) l,h l,h(cid:13) l,h (cid:13) l,h l,h(cid:13)
where
 
E(cid:20)(cid:13)
(cid:13) (cid:13)Σ(cid:101)b l,hv lb
,h(cid:13)
(cid:13)
(cid:13)2
| nb
l,h(cid:21)
=
(nb1
)2v lb
,h⊺
E
(cid:32)
(cid:88)
(X i−µb l,h)(X i−µb
l,h)⊺(cid:33)2
| nb l,hv lb
,h
l,h i
R2d (cid:104) (cid:105) (cid:13) (cid:13)2
⩽ 0 Var ⟨X −µb ,vb ⟩ | nb +(cid:13)Σb vb (cid:13) .
nb i l,h l,h l,h (cid:13) l,h l,h(cid:13)
l,h
35Yantao Wu and Mauro Maggioni
The above inequality, together with Proposition 7 part (b), gives us the following ℓ2 bound:
(cid:20)(cid:13) (cid:13)2 (cid:21) C2C2R4d
E (cid:13)(Σ(cid:101)b −Σb )vb (cid:13) | nb ≲ f Y 0 .
(cid:13) l,h l,h l,h(cid:13) l,h nb l2
l,h
We use the ℓ∞ and ℓ2 bounds above and apply Bernstein inequality 15 to obtain: for
any ϵ > 0,
P(cid:16)(cid:13)
(cid:13)(Σ(cid:101)b −Σb )vb
(cid:13)
(cid:13) > σ2ϵ
(cid:12)
(cid:12) nb
(cid:17)
≲
dexp(cid:32)
−
cσ γ4ϵ2nb l,h(τ logn)−1 2 (cid:33)
+n−τ .
(cid:13) l,h l,h l,h(cid:13) γ (cid:12) l,h C2C2R4d(ϵl−1+l−2)
f Y 0
Combining part (a) and the estimates in Step1-2 finishes the proof.
Proof of part (c) Let V = ⟨vb ,X − µb ⟩2 then E[V | Y ∈ Rb ,B ] = λ (Σb ) ≲
i l,h i l,h i i l,h i d l,h
C2C2R2l−2. Moreover, we can follow the same argument as in Proposition 6 and Propo-
f Y 0
sition 7 to show that E[V i2 | Y
i
∈ R lb ,h,B i] ≲ C f4C Y4R 04l−4. Denote V(cid:98) lb
,h
= nb1 (cid:80) i⟨v lb ,h,X
i
−
l,h
(cid:110) (cid:111)
µb ⟩21 Y ∈ Rb ∩B . Thus, we use Bernstein’s inequality to show that
l,h i l,h i
(cid:32) (cid:33)
(cid:16) (cid:12) (cid:17) cβ2σ2nb
P |V(cid:98) lb ,h−λ d(Σb l,h)| > βσ γ2 (cid:12)
(cid:12)
nb
l,h
≲ exp − αC2C2R2l−4(βl2γ +l, Ch
2C2R2σ−2)
.
f Y 0 f Y 0 γ
We use S (ϵ,β) to denote the event that
h
√
 
(cid:13) (cid:13) R d (cid:12) (cid:12) ϵσ2 (cid:13) (cid:13)
S
h
=   (cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13) (cid:13) < 0 2 ,(cid:12) (cid:12)⟨µ (cid:98)b l,h−µb l,h,v lb ,h⟩(cid:12) (cid:12) ≲ R 0√γ d,(cid:13) (cid:13)v lb ,h−v (cid:98)lb ,h(cid:13) (cid:13) ≲ ϵ,  
(cid:13) (cid:13) (cid:12) (cid:12)
  (cid:13) (cid:13)(Σ(cid:101)b l,h−Σb l,h)v lb ,h(cid:13)
(cid:13)
< ϵσ γ2,(cid:12) (cid:12)V(cid:98) lb ,h−λ d(Σb l,h)(cid:12)
(cid:12)
≲ βσ γ2   
We know from part (a), part (b), and Lemma 16 that, S (ϵ,β) satisfies, for any ϵ,β > 0,
h
P(S (ϵ,β)c)
≲dexp(cid:32)
−
cσ γ4ϵ2nb l,h(τ logn)−1 2 (cid:33)
h C2C2R4d(l−2+ϵl−1)
f Y 0
(cid:32) (cid:33) (cid:32) (cid:33)
cσ2β2nb cσ4nb
+exp − γ l,h +dexp − γ l,h +n−τ
C2C2R2l−4(βl2+C2C2R2σ−2) R4d2
f Y 0 f Y 0 γ 0
Conditioned on the event S (η,ϵ,β), we have the following estimate:
h
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)λ d(Σ(cid:98)b l,h)−λ d(Σb l,h)(cid:12) (cid:12)
(cid:12)
=(cid:12) (cid:12)(cid:12) (cid:12) (cid:12)⟨v (cid:98)lb ,h,µ (cid:98)b l,h−µb l,h⟩(cid:12) (cid:12) (cid:12)2 +⟨v (cid:98)lb ,h,(Σ(cid:101)b l,h−Σb l,h)v (cid:98)lb ,h⟩+⟨v (cid:98)lb ,h,Σb l,hv (cid:98)lb ,h⟩−⟨v lb ,h,Σb l,hv lb ,h⟩(cid:12) (cid:12)
(cid:12) (cid:12)
ϵ2σ4
≲R2dϵ2+ γ +βσ2 +ϵ2σ2 +R2dϵ2+λ (Σb )ϵ2,
0 R2d γ γ 0 d l,h
0
36Conditional regression for the Nonlinear Single-Variable Model
which is bounded by u2C f2C Y2R 02 if one takes ϵ′ = cuC √fCY and β′ = u2C f2C Y2R 02 , for l >
l2 l d σ γ2l2
C C R /σ . This means that we have for any u > 0,
f Y 0 γ
(cid:32) (cid:12) (cid:12) u2C2C2R2 (cid:12) (cid:33)
P (cid:12) (cid:12)λ d(Σ(cid:98)b l,h)−λ d(Σb l,h)(cid:12)
(cid:12)
≳ f l2Y 0 (cid:12) (cid:12)
(cid:12)
nb
l,h
⩽ P(cid:0) S h(ϵ′,β′)c(cid:1)
≲dexp(cid:32) −cu2C Y2σ γ4 √nb l,h(τ logn)−1 2(cid:33) +exp(cid:32) −cnb l,hu4(cid:33) +dexp(cid:32) −cσ γ4nb l,h(cid:33)
+n−τ .
R4d3/2( d+uC C ) 1+u2 R4d2
0 f Y 0
A.3.1 Proof of Corollary 9
(cid:16) √ (cid:17)1
Proof We simply take ϵ = C C R2σ−2 d(t+logl+logd) τlogn 2. The role of logd is
f Y 0 γ nl
to cancel the constant d before the exponential term. Also, since now we consider all slices
h ∈ Hb, there will be an extra constant l before the exponential term, and thus, we include
l
logl to cancel this extra coefficient l before the exponential term. We want ϵ < 1, and this
l
gives us the requirement that √ n ≳ C2C2R4σ−4dl(t+logd+logl). The expectation
τlogn f Y 0 γ
(cid:16) (cid:17)p
estimate can be derived by taking e−t ≲ (C C R2σ−2)2p(dlogd)p lognlogl and use con-
f Y 0 γ nl
ditional expectation formula.
A.4 Proof of Proposition 10
Proof WeuseC(c )todenotesomepositiveconstantthatincreaseswithc andC(c ) → ∞
1 1 1
asc → ∞. ThevalueofC(c )maychangefromlinetolineanddependonotherconstants.
1 1
Fix y ∈ R,t ∈ f−1(y ) and fix x ∈ F−1(y ) then we have Π x = t . Let h′ be the
0 0 0 0 0 γ 0 0 x
unique index h ∈ {1,...,l} such that y ∈ R . Without loss of generality, suppose that
0 l,h
|y −minR | ⩽ |y −maxR |. Then the standing assumption |R | ⩾ 2c max(σ ,ω )
0 l,h′ 0 l,h′ l,h 1 ζ f
x x
implies that for any h ̸∈ {h′,h′ −1}, min{|y −y| : y ∈ R } ⩾ c max(σ ,ω ). Suppose
x x 0 l,h 1 ζ f
that either h′ or h′ −1 ∈ H .
x x l
For each h ∈ H , each slice S is a conditional distribution X|Y ∈ R . For each
l l,h l,h
h ∈ H , we consider the push-forward conditional distribution Π X|Y ∈ R and denote
l γ l,h
its density function ρ (·). We further use ρ (·) to denote the density function of the
t|Y∈R t
l,h
push-forward distribution Π X. By Bayes’ rule, for each h ∈ H
γ l
P(ζ ∈ R −y )
l,h 0
ρ (t ) = ρ (t ).
t|Y∈R l,h 0 P(Y ∈ R ) t 0
l,h
Weclaimthateitherh′ orh′ −1isthemaximizeroftheleft-handsideoverh ∈ H . Indeed,
x x l
if h ̸∈ {h′,h′ −1}, then the property min{|y −y| : y ∈ R } ⩾ c max(σ ,ω ) implies that
x x 0 l,h 1 ζ f
the numerator P(ζ ∈ R −y ) ⩽ 2exp(−Cc2) for some absolute constant C. On the other
l,h 0 1
hand, max P(ζ ∈ R −y ) ⩾ 1P(ζ ∈ R ∪R −y ) ⩾ 1(1−2exp(−Cc2)).
h∈{h′ x,h′ x−1} l,h 0 2 l,h′ x h′ x−1 0 2 1
Therefore, max h∈{h′ x,h′ x−1}P(ζ∈R l,h′ x−y0) ⩾ C(c ). Moreover, the term ρ (t ) is independent of
max h̸∈{h′ x,h′ x−1}P(ζ∈R l,h′ x−y0) 1 t 0
h ∈ H . The a priori probability P(Y ∈ R ) should be comparable among h ∈ H , that
l l,h l
is, sup P(Y ∈ R ) ⩽ cP(Y ∈ R ) for some universal constant c > 0. This is because
h∈H l,h l,h
l
37Yantao Wu and Mauro Maggioni
the construction of H has discarded slices with little probability (or data, in the empirical
l
version) and only consider slices with sufficient probability mass in Algorithm 1. As a
consequence, max h∈{h′ x,h′ x−1}ρ t|Y∈Rl,h(t0) ⩾ C(c ) for some constant C(c ) which increases
max h̸∈{h′ x,h′ x−1}ρ t|Y∈Rl,h(t0) 1 1
with c .
1
Now we introduce a term that is an integral of the density ρ :
t|Y∈R
l,h
Q (t ) := min(P(t ∈ (0,t )|Y ∈ R ), P(t ∈ (t ,len )|Y ∈ R ))
h 0 0 l,h 0 γ l,h
The same argument shows that max h∈{h′ x,h′ x−1}Q h(t0) ⩾ C(c ). Notice that the term Q (t )
max h̸∈{h′ x,h′ x−1}Q h(t0) 1 h 0
takes the smaller conditional probability by comparing two tails (0,t ) and (t ,len ) for the
0 0 γ
conditional distribution Π X|Y ∈ R . If we center the random variable Π X|Y ∈ R
γ l,h γ l,h
and consider W = (Π X −E(Π X|Y ∈ R ))|Y ∈ R instead, we can show that Q (t )
h γ γ l,h l,h h 0
equals
Q (t ) = P(|W | > |t −E(Π X|Y ∈ R )|).
h 0 h 0 γ l,h
Recall that the proof of Proposition 6 also shows that the variances for W are comparable
h
among h ∈ H . That is, C′2(|R |2+σ2) ≲ Var(W ) ≲ C2(|R |2+σ2). It follows similarly
l f l,h ζ h f l,h ζ
that min h̸∈{h′ x,h′ x−1}|t0−E(ΠγX|Y∈R l,h)|2 ⩾ C(c ).
min h∈{h′ x,h′ x−1}|t0−E(ΠγX|Y∈R l,h)|2 1
It is readily that |t −E(Π X|Y ∈ R )|2 ≲ C2(|R |2 +σ2) and min d(x,h) ≳
0 γ l,hx f l,h ζ h̸=hx
C′2(|R |2 +σ2). Moreover, for any h ̸= h , we have |t −E(Π X|Y ∈ R )|2 ≳ d(x,h).
f l,h ζ x 0 γ l,hx
Now, properly choosing constant c , we can show that either h = h′ or h′ −1. Suppose
1 x x x
for a moment that h ̸∈ {h′,h′ −1}, then we have
x x x
C2(|R |2+σ2) ≳ |t −E(Π X|Y ∈ R )|2 ≳ C(c ) min |t −E(Π X|Y ∈ R )|2
f l,h ζ 0 γ l,hx 1
h=h′,h′−1
0 γ l,hx
x x
≳C(c ) min d(x,h) ≳ C(c )C′2(|R |2+σ2)
1 1 f l,h ζ
h=h′,h′−1
x x
which will not hold as long as we properly choose c such that C(c ) is sufficiently large.
1 1
A.5 Proof of Proposition 11
Proof we know that in small neighborhoods, the curve can be viewed as slightly curved,
thatis,thereexistsK > 0dependingonlyonthecurvatureofγ suchthatfor2 ⩽ |k| ⩽ K l,
0 0
we have the following inequality for nearby slices
|k|
(cid:112) dist(x,h +k)−(cid:112) dist(x,h ) ≳ (cid:88) |∆γ | ∼ = |k|len /l
x x hx+k′ γ
k′=1
Note that we do not have such inequality for |k| = 1 because points near the boundary of
one slice may share a very similar distance to the adjacent slice, thus hand to distinct true
slice index h from adjacent one h ±1. This is why we only prove misclassification by at
x x
least two slices.
38Conditional regression for the Nonlinear Single-Variable Model
On the other hand, for far-away slices, we can bound the difference in distance function
by the reach of the curve. Given l ≳ C C R , we deduce that for |k| ⩾ K l, we have the
f Y 0 0
following inequality for far-away slices
(cid:118)
(cid:112) dist(x,h +k)−(cid:112) dist(x,h ) ≳ K (cid:117) (cid:117) (cid:116)λ d(Σb l,h) reach ≳ len γ reach γ ≳ len /l
x x 0 λ (Σb ) γ l σ γ
1 l,h γ
As a consequence, we take all |k| ⩾ 2 and have the following inequality:
(cid:112) (cid:112)
For any h such that |h−h | ⩾ 2, dist(x,h)− dist(x,h ) ≳ len /l
x x γ
In order to obtain correct classification, we want the estimation error of the distance
function to be small, such that for all |h′−h | ⩾ 2,
x
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)d(cid:100)ist(x,h′)−dist(x,h′)(cid:12) (cid:12)+(cid:12) (cid:12)d(cid:100)ist(x,h x)−dist(x,h x)(cid:12)
(cid:12)
< (cid:12) (cid:12)dist(x,h′)−dist(x,h x)(cid:12) (cid:12) (7)
Indeed, this will imply that (cid:98)h
x
= argmin h′∈Hbd(cid:100)ist(x,h′) is the correct or adjacent classifi-
l
cation, i.e. |h x−(cid:98)h x| ⩽ 1.
Consider the event S that we have a small estimation error for information in all slices:
 
(cid:12) (cid:12) ϵσ2 (cid:13) (cid:13) √
    For all h ∈ H lb,(cid:12) (cid:12)⟨µ (cid:98)b l,h−µb l,h,v lb ,h⟩(cid:12) (cid:12) ≲ √γ , (cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13) (cid:13) ≲ σ γ δ,    
R d
S(ϵ,δ,β,u) = 0
(cid:13) (cid:13) (cid:12) (cid:12) (cid:12) (cid:12) u2C2C2R2 
  (cid:13) (cid:13)v lb ,h−v (cid:98)lb ,h(cid:13) (cid:13) ≲ ϵ,(cid:12) (cid:12)λ 1(Σ(cid:98)b l,h)−λ 1(Σb l,h)(cid:12) (cid:12) ≲ βσ γ2,(cid:12) (cid:12)λ d(Σ(cid:98)b l,h)−λ d(Σb l,h)(cid:12) (cid:12) ≲ f l2Y 0  
Notice that Proposition 17 and Lemma 16 state that, with l > C C R /σ , for any δ,β <
f Y 0 γ
R2dσ−2 and any ϵ,u > 0, event S has the following high probability bound:
0 γ
P(Sc)
≲ldexp(cid:32)
−
cσ γ4ϵ2nb l,h(τ logn)− 21 (cid:33) +ldexp(cid:32)
−cnb
min(cid:32) σ γ2δ
,
σ γ4β2
,
σ γ4 (cid:33)(cid:33)
+ln−τ
C2C2R4d(l−2+l−1ϵ) l,h R2d R4d2 R4d2
f Y 0 0 0 0
(cid:32)
cσ γ4C Y2u2nb l,h(τ logn)−
21(cid:33) (cid:32)
cnb
l,hu4(cid:33)
+ldexp − √ +lexp −
R4d3/2( d+uC C ) 1+u2
0 f Y
Wenowinvestigatehowsmalltheseparametersshouldbe. ConditionedoneventS(ϵ,δ,β,u),
we can expand the estimation error in the distance function and estimate its upper-bound
by the following calculation:
(cid:12) (cid:12)
(cid:12)d(cid:100)ist(x,h)−dist(x,h)(cid:12)
(cid:12) (cid:12)
(cid:32) (cid:33)2 (cid:32) (cid:33)
√ √ σ2ϵ √ √ σ2ϵ
≲ R dϵ+σ δϵ+ γ √ + R dϵ+σ δϵ+ γ √ (cid:112) dist(x,h)
0 γ 0 γ
R d R d
0 0
+
λ d(Σb l,h) (cid:16)
σ2δ+R σ
√ d√ δ(cid:17)
+
u2len2
γ
R 02d +βσ2λ d(Σb l,h)R 02d
λ 1(Σb l,h) γ 0 γ l2 λ 1(Σb l,h) γ λ 1(Σb l,h)2
39Yantao Wu and Mauro Maggioni
Thusinaboveinequality,wewantthecoefficientbefore(cid:112) dist(x,h)tobesmallerthanclenγ,
l
len2
and all other terms to be smaller than c γ, so that (7) can be guaranteed. To achieve this,
l2
we let small scales ϵ,δ,β,u to be the following: with l ≳ C C R /σ ,
f Y 0 γ
len σ2 σ2C′2 σ
ϵ′ = c √γ , δ′ = c γ , β′ = c γ f , u′ = c √γ .
lR d R2d R2C2d R d
0 0 0 f 0
√
Hereδ′,β′ < R2dσ−2 automaticallyholdsbecauseofσ ⩽ R ⩽ R dgivenbyAssumption
0 γ γ 0 0
(LCV). Therefore,
(cid:32) (cid:32) (cid:33)(cid:33)
P(cid:0) S(ϵ′,δ′,β′,u′)c(cid:1) ≲ldexp
−c√nl−1
min
C f′σ γ4
,
σ γ8C f′4
+ln−τ
τ logn C2R3d3/2len R8C4d4
f 0 γ 0 f
where we have used the fact that √R α0 < σ
γ
≲ R
0
and C f′C YR
0
⩽ len
γ
⩽ C fC YR 0.
A.6 Proof of Proposition 12
Proof Let s ∧ 1 = s∧1 ∈ [1,1]. Then we have s ∧ 1 ⩾ 2s for all s ⩾ 1. First,
2 2s+1 2
⊺
we decompose X = γ(Π X) + M (Z ,0) . We are going to control the curve
γ γ′(ΠγX) d−1
approximation error
MSE (NCA) := E(cid:20)(cid:12) (cid:12) (cid:12)f(Π γX)−f(cid:101)(⟨v lb ,h,X⟩)(cid:12) (cid:12) (cid:12)2 1 I(l,h)(⟨v lb ,h,X⟩) (cid:12) (cid:12) X ∈ S l,h(cid:21)
Denotet 1 ∈ Π γ(S l,h)tobesuchthatγ′(t 1) = v lb ,h. Recallthatf(cid:101)(s) =
E(cid:104)
F(X)
(cid:12)
(cid:12) X ∈ S l,h, ⟨v lb ,h,X⟩ =
s(cid:105)
.
Asaconsequence,wecancomputethatf(cid:101)(⟨v lb ,h,γ(t 1)⟩) = F(γ(t 1)) = f(t 1) = f(⟨v lb ,h,γ(t 1)⟩+
c(h)) where alignment constant is defined as c(h) = t −⟨γ(t ),vb ⟩.
1 1 l,h
Meanwhile, because f(cid:101)minimizes the population loss
E(cid:20)(cid:12) (cid:12) (cid:12)Y −p(⟨v lb ,h,X⟩)(cid:12) (cid:12) (cid:12)2 1 I(l,h)(⟨v lb ,h,X⟩) (cid:12) (cid:12) X ∈ S l,h(cid:21) ,
we obtain an upper-bound upon replacing f(cid:101)(·) by f(·+c(h)) . By H¨older continuity, the
triangle inequality, and Taylor expansion, we have
MSE (NCA) ⩽[f]2 Cs∧1E(cid:20)(cid:12) (cid:12) (cid:12)Π γX −⟨X,v lb ,h⟩−c(h)(cid:12) (cid:12) (cid:12)2(s∧1) (cid:12) (cid:12) X ∈ S l,h(cid:21)
⩽C(s)[f]2 Cs∧1E(cid:20)(cid:12) (cid:12) (cid:12)⟨γ(Π γX)−γ(t 1),v lb ,h⟩−(Π γX −t 1)(cid:12) (cid:12) (cid:12)2(s∧1) (cid:12) (cid:12) X ∈ S l,h(cid:21)
+C(s)[f]2 Cs∧1E(cid:20)(cid:12) (cid:12) (cid:12)(Z d−1,0)⊺ M γ⊺ ′(t)(v lb ,h−γ′(t))(cid:12) (cid:12) (cid:12)2(s∧1) (cid:12) (cid:12) X ∈ S l,h(cid:21) .(NL)
(cid:32) (cid:33)2(s∧1)
≲[f]2 Cs∧1σ γ2(s∧1)(cid:13) (cid:13)γ′′(cid:13) (cid:13)2(s∧1) C f2(s∧1) max σ ζ,ω f, l Cen ′γ
l
f
40Conditional regression for the Nonlinear Single-Variable Model
We use H¨older continuity property, Cauchy-Schwarz inequality, and Corollary 9 to control
the following error term
MSE (Φ) :=E(cid:20)(cid:12) (cid:12) (cid:12)f(cid:101)(⟨v lb ,h,X⟩)−f(cid:101)(⟨v (cid:98)lb ,h,X⟩)(cid:12) (cid:12) (cid:12)2 1 I(l,h)(⟨v lb ,h,X⟩) (cid:12) (cid:12) X ∈ S l,h(cid:21)
√ (cid:20)(cid:13) (cid:13)2(s∧1)(cid:21)
⩽([f] +C R l−1[ρ ] )2(R d)2(s∧1)E (cid:13)vb −vb (cid:13)
Cs∧1 Y 0 X Cs∧1 0 (cid:13)(cid:98)l,h l,h(cid:13)
(cid:18) lognlogl(cid:19)2(s∧1)
≲([f] +C R l−1[ρ ] )2(len R2σ−2)2(s∧1)(d2logd)s∧1 .
Cs∧1 Y 0 X Cs∧1 γ 0 γ nl
(Φ)
We exploit the H¨older continuity of f(cid:101)(see Liao et al., 2022, Appendix A, Example 1)
to control the bias error term
MSE (B) := E(cid:20)(cid:12) (cid:12) (cid:12)f(cid:101)(⟨v (cid:98)lb ,hx,x⟩)−f j|v lb ,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12) (cid:12) (cid:12)2 1 I(l,h)(⟨v lb ,h,X⟩) (cid:12) (cid:12) X ∈ S l,h(cid:21)
(cid:32)(cid:12) (cid:12)(cid:33)2s (cid:32) (cid:33)2s
≲ (cid:104) f(cid:101)(cid:105)2
Cs
(cid:12)I( jl,h) (cid:12) ≲ ([f]
Cs
+C YR 0l−1[ρ X] Cs)2C f2smax σ ζ,ω f, l Cen ′γ
l
j−2s.
f
(B)
(cid:12) (cid:12)
Thevarianceterm(cid:12) (cid:12)f
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)−f(cid:98)
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12) (cid:12)canbeconcentratedwithknown
calculations, see (Liao et al., 2022, Proposition 2 and Lemma 5):
(cid:20)(cid:12) (cid:12)2(cid:21)
jlogj
E (cid:12) (cid:12)f
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)−f(cid:98)
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12)
(cid:12)
≲ σ ζ2
n/l
. (V)
(cid:20)(cid:12) (cid:12)2(cid:21)
MSE
(Ψ)
:= E (cid:12) (cid:12)f(cid:98)
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)−f(cid:98)
j|v (cid:98)lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12)
(cid:12)
(Ψ)
(cid:12) (cid:12)
Tocontroltheprojectionerrorterm(cid:12) (cid:12)f(cid:98)
j|v lb
,hx(⟨v (cid:98)lb ,hx,x⟩)−f(cid:98)
j|v (cid:98)lb
,hx(⟨v (cid:98)lb ,hx,x⟩)(cid:12) (cid:12),weuseLemma
23. Foreachh ∈ H l, wetakep
1
⩾ 1,s∧1,g = f(cid:101),r = σ γ, andapplyLemma23toeachsam-
plesliceS(cid:98) lb
,h
withdata{(⟨X i,v (cid:98)lb ,h⟩,Y i) : X
i
∈ (cid:83) |h′−h|⩽1S(cid:98) lb ,h′}withv = v lb ,h,v (cid:98)= v (cid:98)lb ,h,n′ ≍ n/l.
A.7 Proof of Theorem 4
Proof Because σ = 0, we take largest l which satisfies (5), that is,
ζ
(cid:32) (cid:33)
1 n R3C2 d3/2len R5C2d4
l∗ = , C := 0 f max γ , 0 f .
C
γ,f
log3/2n γ,f C f′ σ γ4 C f′3σ γ8
In Lemma 23, we take p = 1,s∧1,r =
C fCYR0,
and
1 l
ϵ2 = ([f] +C R l−1[ρ ] )2(C C R )2(s∧1)Cs∧1(logn)3(s∧1)n−2(s∧1).
0 Cs∧1 Y 0 X Cs∧1 f Y 0 γ,f
41Yantao Wu and Mauro Maggioni
Then we have zero variance MSE = 0 and can control the following errors:
(V)
(cid:32)
σ2C2len2C2
log3n(cid:33)s∧1
MSE ≲([f] +|f| [ρ ] )2 γ f γ γ,f
(NCA) Cs∧1 L∞ X Cs∧1 reach2C′2 n2
γ f
(cid:18) len R2(cid:19)2(s∧1) (logn)7(s∧1)
MSE ≲([f] +|f| [ρ ] )2 γ 0 (d2logd)s∧1C2(s∧1)
(Φ) Cs∧1 L∞ X Cs∧1 σ2 γ,f n4(s∧1)
γ
(logn)3s
MSE ≲([f] +|f| [ρ ] )2(C C R )2sC2s
(B) Cs∧1 L∞ X Cs∧1 f Y 0 γ,f n2s
MSE ≲([f] +|f| [ρ ] )2(C C R )2(s∧1)Cs∧1(logn)3(s∧1)n−2(s∧1)
(Ψ) Cs∧1 L∞ X Cs∧1 f Y 0 γ,f
(cid:32) (cid:33) 1
+([f] Cs∧1 +|f| L∞[ρ X] Cs∧1)2(C fC YR 0)2−2 s∧1 (R 0)σ γ2−4 s∧1 dlogdC γ,flog n7/ 22n 1−s∧1 .
1
To disregard those high-order terms, we only need n ≳ C2 R2σ2dlog1/2d, which is a
(logn)2 γ,f 0 γ
consequence of the initial assumption n ≳ C flenγ.
log3/2n C f′σγ
A.8 Proof of Theorem 3
Proof Because σ > 0, we consider bias-variance trade-off between the bias term MSE
ζ (B)
and the variance term MSE . It follows that the optimal way is to let product l∗j∗ grow
(V)
proportional to
(cid:16) (cid:17) 2
n2s1 +1M∗ where M∗ = σ ζ−1(C fC YR 0)s([f]
Cs
+C YR 0l−1[ρ X] Cs) 2s+1
In practice, when the number of samples is not quite sufficient, we take j∗ = C and let l∗
grow with n. On the other hand, when l∗ already has the magnitude of l = CYR0 ,
upper max(σ ,ω )
ζ f
we fix l∗ = l and let j∗ grows with n. Notice that MSE converges to the limit
upper (NCA)
[f]2 Csσ γ2sC f2s
max(σ ,ω )2s. Also, the optimal MSE +MSE is
reach2s ζ f (B) (V)
γ
2s
C ′f2s 2+ s1 len γ2s2 +s 1σ ζ2s4 +s 1([f]
Cs
+C YR 0l−1[ρ X] Cs)2s2 +1n− 2s2 +s 1logn (8)
C 2s+1
f
Therefore, we let ϵ2 to be the maximum of above two errors, that is,
0
ϵ2 0 = max [f]2 Csσ γ2sC rf2 es am cha 2x s(σ ζ,ω f)2s , C f2s2 +s 1len γ2s2 +s 1σ ζ2s4 +s 1([f] C ′s 2+ s C Y 2R s 0l−1[ρ X] Cs)2s2 +1 logn 
γ C 2s+1n2s+1
f
Moreover, when n is sufficiently large the error denoted by ϵ2 dominates all other er-
0
rors: the residual error in the convergence of MSE , as well as the terms MSE and
(NCA) (Φ)
MSE , become negligible high order terms compared with ϵ2, as soon as the standing
(Ψ) 0
assumption n ≳ C C flenγ is satisfied.
log3/2n γ,f C f′σγ
42Conditional regression for the Nonlinear Single-Variable Model
A.9 Proof of Proposition 13
Proof Fix any interval T ⊆ [0,len ] with size |T| = C max(σ ,ω ), and consider the con-
γ f ζ f
ditional mean µ := E[X | t ∈ T] = E [γ(t) | t ∈ T], and its projection onto the underlying
T t
curveγ atlocationt = Π µ . Clearly,γ′(t )isperpendiculartobothγ′′(t )andµ −γ(t ).
1 γ T 1 1 T 1
Recall that ∥γ′′∥ ⩽ reach−1 and hence (SC) implies that C max(σ ,ω )∥γ′′∥ ⩽ c . For
∞ γ f ζ f ∞ 2
any t,t ∈ [0,len ] with |t−t | ⩽ C max(σ ,ω ), we have
1 γ 1 f ζ f
(cid:13) (cid:13)γ(t)−γ(t 1)−γ′(t 1)(t−t 1)(cid:13) (cid:13) ⩽ 21 (cid:13) (cid:13)γ′′(cid:13) (cid:13) ∞|t−t 1|2 ⩽ 1 2c 2|t−t 1|. (9)
This implies that the curve is well-approximated by a straight line on the whole interval T,
implies that t ∈ T, and, combined with the minimizing property of Π , yields
1 γ
1
∥γ(t )−µ ∥ ⩽ ∥γ(E[t|t ∈ T])−µ ∥ ⩽ c |T|.
1 T T 2
2
We have
E[(X −µ )(X −µ )⊺ | t ∈ T]
T T
=(γ(t )−µ )(γ(t )−µ )⊺ +γ′(t )γ′(t )⊺E[|t−t |2|t ∈ T]+(γ(t )−µ )γ′(t )⊺E[t−t |t ∈ T]+U ,
1 T 1 T 1 1 1 1 T 1 1
with ∥U∥ ⩽ ∥γ(t )−µ ∥∥γ′′∥E[(t − t )2|t ∈ T] + ∥γ′′∥2E[(t − t )4|t ∈ T] ⩽ 2c2|T|2. It
1 T 1 1 2
follows that the conditional covariance along γ′(t ) has a lower bound
1
1
γ′(t )⊺ Σ γ′(t ) = E[|t−t |2|t ∈ T]+γ′(t )⊺ Uγ′(t ) ≳ ( −2c2)|T|2,
1 T 1 1 1 1 4 2
which is positive for c small enough, and the conditional covariance along any direction ν
2
that is orthogonal to γ′(t ) has an upper bound
1
ν⊺ Σ ν ⩽ ∥γ(t )−µ ∥2+2c2|T|2+σ2 ⩽ (3c2+c2)|T|2.
T 1 T 2 γ 2 1
Therefore the largest eigenvalue of Σ is significantly larger than others, as long as con-
T
stants c ,c are sufficiently small, i.e. we are in the “wide” slice scenario. Moreover, the
1 2
largest principal component of Σ is roughly tangential to the curve.
T
A.10 Proof of Proposition 14 and Theorem 5
Proof Similar to Proposition 17, we have the following high probability bound on the
(cid:13) (cid:13) (cid:12) (cid:12)
estimation error of parameters such as ⟨v lb ,h,µ (cid:98)b l,h−µb l,h⟩, (cid:13) (cid:13)v (cid:98)lb ,h−v lb ,h(cid:13) (cid:13), (cid:12) (cid:12)λ 1(Σb l,h)−λ 1(Σ(cid:98)b l,h)(cid:12) (cid:12).
The argument is the same, so the proof is omitted.
Proposition 18 (local NVM for “wide” slice) Suppose (X subG), (Y subG), (ζ
subG), (γ ), (SC), and (ω ) hold true. Let µb be the mean of h-th slice and vb be
1 f l,h l,h
the significant vector of h-th slice. Then, for every l such that |Rb | ≍ max(σ ,ω ) for all
l,h ζ f
h ∈ Hb, for every ϵ ∈ (0,1) and τ ⩾ 1, on each slice
l
43Yantao Wu and Mauro Maggioni
(a) For any h ∈ H and any ϵ > 0, the estimation error of the slice mean along the
l
tangential direction can be bounded as
P(cid:40) (cid:12)
(cid:12) (cid:12)⟨v lb ,h,µ (cid:98)b l,h−µb
l,h⟩(cid:12)
(cid:12)
(cid:12)
>
C f2 R|R √lb
,h
d|2ϵ (cid:12)
(cid:12) (cid:12)
(cid:12)
nb
l,h(cid:41)
≲
dexp(cid:32) −cC Rf2| 2R dlb
,
+h|2 ϵϵ C2n |b
l R,h
b(τ |l Rog √n) d− 21(cid:33)
+n−τ
0 0 f l,h 0
(b) For any h ∈ H , the estimation error of the significant vector can be bounded as,
l
P(cid:110)(cid:13) (cid:13) (cid:13)v (cid:98)lb ,h−v lb ,h(cid:13) (cid:13)
(cid:13)
> ϵ (cid:12) (cid:12) nb l,h(cid:111) ≲
dexp(cid:32)
−cC Rf2| 2R dlb , +h|2 ϵϵ C2n |b l R,h b(τ |l Rog √n) d−
21(cid:33) +dexp(cid:32)
−cC f4| RR 4l, dh 2|4nb
l,h(cid:33)
+n−τ
0 f l,h 0 0
√
(c) For any h ∈ H and any 0 < u < R0 d < 1, the estimation error of the first principal
l C |Rb |
f l,h
value of the slice can be bounded as,
(cid:18)(cid:12) (cid:12) (cid:12) (cid:19)
P (cid:12) (cid:12)λ 1(Σ(cid:98)b l,h)−λ 1(Σb l,h)(cid:12)
(cid:12)
> u2C f2|R lb ,h|2 (cid:12) (cid:12) nb
l,h
(cid:12)
≲
dexp(cid:32) −cu2C f4|R lb ,h|4nb l,h(τ logn)−1 2(cid:33) +dexp(cid:32) −cC f4|R l,h|4nb l,h(cid:33)
+n−τ
R4d2+uC2|Rb |2R2d R4d2
0 f l,h 0 0
R2d
Moreover, we have the following probability bound For fixed constant β < 0 ,
C2|R |2
f l,h
(cid:32) (cid:33)
(cid:18) (cid:18)(cid:13) (cid:13) (cid:13) (cid:13)2(cid:19) (cid:19) cβ2C4|Rb |4nb
P max (cid:13) (cid:13)Σ(cid:101)b l,h−Σb l,h(cid:13) (cid:13),(cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13)
(cid:13)
≳ βC f2|R lb ,h|2 ≲ dexp − f R4dl, 2h l,h
0
Consider the event S that we have a small estimation error for information in all slices:
 √ 
(cid:12) (cid:12) ϵ|Rb |2ϵ (cid:13) (cid:13) R d (cid:13) (cid:13)
    For all h ∈ H lb,(cid:12) (cid:12)⟨µ (cid:98)b l,h−µb l,h,v lb ,h⟩(cid:12) (cid:12) ≲ Rl √,h d , (cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13) (cid:13) ≲ 0 2 ,(cid:13) (cid:13)v lb ,h−v (cid:98)lb ,h(cid:13) (cid:13) ≲ ϵ   
S(ϵ,β,u) = 0
   (cid:12) (cid:12) (cid:12)λ 1(Σ(cid:98)b l,h)−λ 1(Σb l,h)(cid:12) (cid:12)
(cid:12)
≲ u2C f2|R lb ,h|2,max(cid:18)(cid:13) (cid:13) (cid:13)µ (cid:98)b l,h−µb l,h(cid:13) (cid:13) (cid:13)2 ,(cid:13) (cid:13) (cid:13)Σ(cid:101)b l,h−Σb l,h(cid:13) (cid:13) (cid:13)(cid:19) ≲ β2C f2|R lb ,h|2  

C2|Rb |2 C |Rb |
We claim that the conditions ϵ′ ≍ β′ ≍ f l,h and u ≍ f √l,h are sufficient to perform
R 02d R0 d
almost correct classification.
Recallthatsimilartothe“thin”slicescenario,inordertoobtaincorrectclassification,we
wanttheestimationerrorofthedistancefunctiontobesmall, suchthatforall|h′−h | ⩾ 2,
x
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)d(cid:100)ist(x,h′)−dist(x,h′)(cid:12) (cid:12)+(cid:12) (cid:12)d(cid:100)ist(x,h x)−dist(x,h x)(cid:12)
(cid:12)
< (cid:12) (cid:12)dist(x,h′)−dist(x,h x)(cid:12) (cid:12)
Indeed, this will imply that (cid:98)h
x
= argmin h′∈Hbd(cid:100)ist(x,h′) is the correct or adjacent classifi-
l
cation, i.e. |h x−(cid:98)h x| ⩽ 1.
Now, we are going to analyze the distance function. Notice that in the “wide” slice
scenario, the difference in distance can be bounded as follows:
(cid:112) (cid:112)
dist(x,h +k)− dist(x,h ) ≳ C |Rb | for any |k| ⩾ 2
x x f l,h
44Conditional regression for the Nonlinear Single-Variable Model
(Recallthatforlargek,wecanboundthisbyreach ≳ C |Rb |byassumption(SC).)Thus
γ f l,h
this gives a lower bound for the right-hand side. We are going to control the estimation
errorontheleft-handside. WeusethesameargumentintheproofofProposition11. Given
event S(ϵ,β,u),
(cid:12) (cid:12)
(cid:12)d(cid:100)ist(x,h)−dist(x,h)(cid:12)
(cid:12) (cid:12)
C4|Rb |4
≲C2|Rb |2β2+C |Rb |β(cid:112) dist(x,h)+u2R2d+ϵ2R2d+C2|Rb |2β2ϵ2+ f l,h ϵ2 ≲ C2|Rb |2
f l,h f l,h 0 0 f l,h R2d f l,h
0
C2|Rb |2 C |Rb |
when ϵ′ ≍ β′ ≍ f l,h and u ≍ f √l,h . Therefore, we derive the conclusion that the
R 02d R0 d
event of misclassification by at least two slices has a small probability:
(cid:32) (cid:33)
P(cid:16)(cid:12) (cid:12) (cid:12)(cid:98)h x−h x(cid:12) (cid:12)
(cid:12)
⩾ 2(cid:17) ≲ ldexp −cC Cf6m Rax 7d(σ 3√ζ, lω of g) n7n +ln−τ
Y 0
ThisfinishestheproofofProposition14. Theorem5followseasilybecausewemainlyutilize
the high accuracy of classification without performing one-dimensional regression.
Appendix B. Technical Results
Lemma 19 Let X be a random variable, and let X ,...,X be independent copies of X.
1 n
Given a measurable set E, defineρ(E) = P{X ∈ E}, andρˆ(E) = n−1(cid:80) 1{X ∈ E}. Then
i i
(cid:26) nt2/2 (cid:27)
P{|ρˆ(E)−ρ(E)| > t} ⩽ 2exp −
ρ(E)+t/3
In particular, for t = ρ(E)/2, we have
(cid:26) (cid:20) (cid:21)(cid:27) (cid:26) (cid:27)
1 3 1 3
P ρˆ(E) ̸∈ ρ(E), ρ(E) ⩽ P |ρˆ(E)−ρ(E)| > ρ(E) ⩽ 2exp(− nρ(E))
2 2 2 28
Lemma 20 Let X ∈ Rd be a sub-Gaussian vector with variance proxy R2. Then for any
0
(cid:16) (cid:17)
t > 0, we have P{∥X∥ > t} ⩽ 2exp − t2 .
2dR2
0
Lemma 21 Let(X ,Y ),...,(X ,Y )beindependentcopiesofasub-Gaussianpair(X,Y) ∈
1 1 n n
Rd+1 with variance proxy R2. Then, for every γ ∈ (0,1) and every a ,a > 0, we have
0 X Y
(cid:110) √ (cid:111) (cid:18) δ (1−γ)2/2 (cid:19)
P #{(X ,Y ) ∈ B(0,a dR )×[−a R ,a R ]} < δ γn ⩽ 2exp − X,Y n ,
i i X 0 Y 0 Y 0 X,Y
1+(1−γ)/3
where δ = δ δ , δ = 1−2exp(cid:0) −a2 /2(cid:1) , and δ = 1−2exp(cid:0) −a2 δ /2(cid:1) .
X,Y X Y|X X X Y|X Y X
Proof See (Lanteri et al., 2022, Lemma B.3).
45Yantao Wu and Mauro Maggioni
Lemma 22 Let ρ be a probability distribution in Rd, v,w ∈ Sd−1, Proj x = ⟨u,x⟩ and
u
I ⊆ R an interval with ρ(Proj−1I) > 0 for u ∈ {v,w}. Suppose ρ has an upper bounded
u
density and ρ(Proj−1I) ≳ |I|, and let ρ denote the pushforward measure under the map
v v
Proj . Then
v
W (ρ (x|Proj x ∈ I),ρ (x|Proj x ∈ I)) ≲ sin(∠(v,w))diam(suppρ).
1 v v v w
Lemma 23 ((Lanteri et al., 2022, Proposition 4 )) Assume (X subG), (ζ subG),
and (P). Suppose function f(cid:101) ∈ Cs with s ∈ [1,1] is defined on interval I with diameter
2
r ⩾ 1. Let integer m = ⌊s⌋ be the largest integer smaller than or equal to s, so m = 0
if s ∈ [1,1) and m = 1 if s = 1. Given j ⩾ 1, construct {I }j the uniform partition
2 j,k k=1
of I into j intervals. Let v ∈ Sd−1 be an estimator of v ∈ Sd−1 such that ⟨v,v⟩ ⩾ 0.
(cid:98) (cid:98)
Given data {(X′,Y′)}n′ . For u ∈ {v,v}, denote n = #{X′ : ⟨u,X′⟩ ∈ I }. For each
i i i=1 (cid:98) j,k|u i i j,k
k ∈ K := {k : n ⩾ n′/j}, compute
j|u j,k|u
f(cid:98)
j,k|u
= argmin (cid:88)(cid:12) (cid:12)Y i−p(⟨u,X i′⟩)(cid:12) (cid:12)21
I
j,k(⟨u,X i′⟩)
deg(p)⩽m
i⩽n′
and compute the piecewise constant (s < 1) or linear (s = 1) estimator of f(cid:101) at scale j
(cid:12) (cid:12)
conditioned on u and truncated at (cid:12)f(cid:101)(cid:12)
(cid:12) (cid:12)
L∞
(cid:88)
f(cid:98) j|u(r) = f(cid:98) j,k|u(r)1
I
j,k(r)
k∈Kj
Conditioned on ∥v−v∥ ⩽ t/j for some t ⩾ 1. For every ϵ > 0, we have
(cid:98) 0
E X(cid:20)(cid:12) (cid:12) (cid:12)f(cid:98) j,k|v(⟨v,X⟩)−f(cid:98) j,k|v (cid:98)(⟨v (cid:98),X⟩)(cid:12) (cid:12) (cid:12)2 1 B(0,r)(X)(cid:21) ≲ (cid:104) f(cid:101)(cid:105)2 Cst2r2−2 s ∥v (cid:98)−v∥2−2 s +ϵ2 0
(cid:18) (cid:19)
cn′ϵ2
with probability higher than 1−C#K exp − 0
j #Kj[f(cid:101)]2
Cst2r2s
Appendix C. Case Analysis: Meyer helix
C.0.1 Background: standard & modified Meyer’s staircase
We consider the standard Y. Meyer’s staircase. Fix constant δ ⩾ 1. Consider the unit
interval I = [0,1] and the set of Gaussians N(t;µ,δ2) where the mean µ takes values in
I, and the density function is truncated to accept arguments t ∈ I only. Varying µ ∈ I
in this manner induces a smooth embedding of the interval I into the infinite-dimensional
HilbertspaceL2(I), i.e. acurve. Explicitly, wetakethesquarerootoftheGaussiandensity
centered at µ ∈ I and truncate it to t ∈ I:
1
(cid:18) |t−µ|2(cid:19)
I → L2(I) : µ (cid:55)→ g (t) := √ √ exp − (10)
µ 4 2π δ 4δ2
BydiscretizingI,wemaysamplethismanifoldandprojectitintoafinite-dimensionalspace.
In particular, for any d ∈ N, a grid Γ ⊆ I of d points may be generated. It is obtained
d
46Conditional regression for the Nonlinear Single-Variable Model
by subdividing I in d equal parts and thus Γ (k) = k/d for k = 1,...,d. Explicitly, the
d
evaluation function is
L2(I) → Rd : g (t) (cid:55)→ (g (1/d),...,g (1))⊺ (11)
µ µ µ
Thus, combining the above two maps (10) and (11) together produces an embedding of
interval I into Rd (which is equivalent to a curve in Rd). We write it explicitly as x(t) =
⊺
(x (t),...,x (t)) wheret ∈ I andforeachk = 1,...,d. ForthestandardMeyer’sstaircase,
1 d
it follows that the expression for x (t) is
k
1
(cid:18) |k/d−t|2(cid:19)
x (t) = √ √ exp − (12)
k 4 2π δ 4δ2
Note that this expression differs from Definition 1 because it does not use the unit-speed
parameterization. However, this expression has two advantages: first, it is uniform over
dimensiond,andthereisnoneedtoworryaboutdifferentlengths;second,itstrengthensthe
fact that these curves are finite-dimensional approximations of the function g (t) ∈ L2(I).
µ
In numerical simulations, we can clearly convert it to the unit-speed parameterization.
Notice that the map (10) describes a curve in L2(I). Here µ ∈ I parameterizes this
curve while t ∈ I is merely the argument for the function g . On the other hand, equation
µ
(12) describes a curve in Rd. Here t ∈ I parameterizes the curve while µ is replaced by a
discrete grid Γ .
d
ThestandardMeyer’sstaircaseisaninterestingexamplebecauseitallowsustoconstruct
a curve in high dimensional Euclidean space Rd. However, because of the construction, as
dimension d → ∞, the standard Meyer’s staircase defined in equation (12) will converge
to a limit that corresponds to the function g ∈ L2(I). This implies that the complexity
µ
of the curve is bounded as d → ∞. Because we are focusing on the regression problem
for general curve classes, we need to consider various curves with different complexity and
different ambient dimensions. Our strategy is to consider analogies of (12).
One direct modification of the standard Meyer’s staircase is to let δ = 1 in equation
d
12. This modified Meyer-staircase is an example of a collection of curves whose complexity
grows with dimension d. Besides parameters such as length, diameter, curvature, and
reach, we also consider the effective linear dimension. One way to measure the effective
linear dimension is to study the singular values of the curve. Suppose we perform Singular
Value Decomposition on a curve in Rd and obtain its singular value λ (k),k = 1,...,d in
γ
descendingorder. Then,wecanconsiderthesumofthesingularvaluesdividedbythelargest
singular value, ∥λ ∥ :=
(cid:80)d
λ (k)/λ (1), or count the number of singular values that are
γ 1 k=1 γ γ
greater than 0.05 times the largest singular value, ∥λ ∥ := #{λ (k) : λ (k) > 0.05λ (1)}.
γ 0 γ γ γ
Both ∥λ ∥ and ∥λ ∥ are scaling invariant and measure the minimal number of linear
γ 1 γ 0
dimensions needed to capture (in the mean squared sense) the underlying curve γ up to a
given relative error. This quantity is commonly used as a stable version of rank for a matrix
(sometimes called numerical, or stable, rank).
Figure 9 illustrates the relationship between the modified Meyer-staircase parameters
and the dimension d. One can observe that the length is roughly proportional to d1.5,
diameter is roughly d0.5, curvature is roughly proportional to d−0.5, and reach is roughly
47Yantao Wu and Mauro Maggioni
101
3
102 2
101 1
100
100
101 102 101 102 101 102
102
1
100
0.995
101
0.99
0.985
100 0.98
101 102 101 102 101 102
Figure 9: Behavior of geometric features of the modified Meyer staircase in Rd as a function
of d.
d0.5. It turns out that both ∥λ ∥ and ∥λ ∥ are roughly proportional to d1. In this sense,
γ 1 γ 0
the standard Meyer-staircase has its complexity growing with d.
However, the modified Meyer-staircase is still special in the following two respects: (i)
√
It approximately stays on the sphere dSd−1: For t ∈ (0,1),
1 1 (cid:88)d (cid:90) 1 1 (cid:18) |s−t|2(cid:19)
∥x(t)∥2 := x (t)2 ≈ √ exp − ds ≍ 1 when d is large ;
d d k 2πδ 2δ2
0
k=1
(ii) the local reach of modified Meyer’s staircase is almost the reciprocal of the magnitude
of local curvature. The above two aspects indicate that the curve traverses the space with
weak self-entanglement. Hence, it suggests the possibility of finding a linear projection
P : Rd → Rd′ with d′ much smaller than d such that the projected image Pγ is a much
simpler curve. For example, suppose we perform the linear projection of modified Meyer’s
staircase onto its first few principal components: the projected image is a simple curve, and
the learning problem can be significantly simplified if we study the regression problem on
the projected curve. Hence, to test the performance algorithm for the regression problem,
we aim to test curves that are so complex that there is no trivial dimension reduction, e.g.
via standard techniques such as Principal Component Analysis.
C.0.2 Meyer helix
To summarize, we want to perform numerical tests of learning problems on curves that
are complex enough. Here are some characteristics of the complexity of the curve: (i)
parameters such that length, diameter, reach, and effective linear dimension ∥λ∥ ,∥λ∥
1 0
growwiththedimensiond; (ii)thecurveγ hasnotrivialdimensionreduction. Inparticular,
consider linear projection P : Rd → Rd′ such as projection onto the first d′ ⩽ d principal
d′
components. Define the “regression complexity”
len
γ
C := (13)
γ
reach
γ
48Conditional regression for the Nonlinear Single-Variable Model
80 10
103 60 68
40
4
102
20
2
101 102 101 102 101 102
100 1
0.8
0.6
10-1 101
0.4
0.2
101 102 101 102 101 102
Figure 10: Behavior of geometric features of the Meyer helix in Rd as a function of d.
of a curve as its length divided by its reach. We consider curves complex enough such that
whenever the projected curve P γ has regression complexity C ≲ C then it implies
d′ P d′γ γ
that the dimension d′ cannot be small, for example d′ ≳ d.
Because the regression problem should be scaling invariant, we can freely rescale the
√
curve, and we choose the normalization such that the reach equals d. This is consistent
with Assumption (γ ) and allows us to take σ , the deviation of data X away from the
1 γ
curve, with order 1. In particular, we do not let curvature grow with d here.
We introduce the following curve, called Meyer helix, as an analogy of the standard and
modified Meyer’s staircase:
(cid:18) (cid:19) (cid:18) (cid:19)
1 t−k/d |k/d−t|
x (t) = √ √ cos ak+ G (14)
k 4 2π δ d δ d′ δ d
where δ = (1+0.3cos(ak))/d and δ′ = (1+0.3sin(ak))/d, with constant a = 10 and
d d
(cid:16) (cid:17)
function G is taken to be Bernstein-type decay G(z) = exp −
z2
. Compared with the
1+z
standard and modified Meyer-staircase, this curve has an extra factor of cosine term. The
effect of this cosine term is to facilitate the traversing of point x(t) around the space Rd
and introduce more self-entanglement. Moreover, the following values vary from one axis to
another: the frequency 1/δ in function G, the frequency 1/δ′ in cosine term, and the phase
d
ak in cosine term. That variation makes the curve less special while keeping the desired
complexity.
√
In Figure 10, we plot the parameters of Meyer helix after scaling the reach to be d.
Similar to the modified Meyer’s staircase, we see that the length is roughly proportional
to d1.5, the diameter is roughly proportional to d0.5, the curvature is roughly proportional
to d−0.5, the reach is roughly proportional to d0.5, and effective linear dimension ∥λ ∥ ,
γ 1
∥λ ∥ is roughly proportional to d. Moreover, we can adopt the regression complexity (13)
γ 0
to measure the effective linear dimension: define d to be the smallest d′ such that
SVD
C ⩽ 1.2C , then this effective linear dimension is roughly proportional to d1.
P d′γ γ
Moreover, the above properties of complexity are pretty robust: we can also choose
Gaussian-type to decay G(z) = exp(−z2) or choose another constant in the variants (e.g.,,
49Yantao Wu and Mauro Maggioni
a,δ , and δ′) and the above properties persist. This indicates that this collection of Meyer
d d
helix curves indeed has its complexity growing with ambient dimension d. Numerical tests
suggest that this collection of curves does not have simple dimension reduction via random
projections that preserves geometric properties such that length and reach. Recall the
following form of Johnson-Lindenstrauss random projection lemma for manifolds:
Lemma 24 ((Baraniuk and Wakin, 2009, Theorem 3.1) ) Let M be a compact K-
dimensional submanifold of RN having condition number 1/τ, volume V, and geodesic cov-
ering regularity R. Fix 0 < ϵ < 1 and 0 < ρ < 1. Let Φ be a random orthoprojector from
RN to RM with M ≳ ϵ−2Klog(NVRτ−1ϵ−1)log(1/ρ). If M ⩽ N, then with probability at
least 1−ρ, the following statement holds: for every pair of points x,y ∈ M,
(cid:114) (cid:114)
M ∥Φx−Φy∥ M
(1−ϵ) ⩽ 2 ⩽ (1+ϵ)
N ∥x−y∥ N
2
If we apply this Johnson-Lindenstrauss lemma to a curve γ on Rd with length len
γ
and reach reach , then we can take ambient dimension N = d, the condition number
γ
1/τ = 1/reach , volume V = len , and geodesic covering regularity R = O(1), and thus
γ γ
(cid:16) (cid:16) (cid:17)(cid:17)
M = O ϵ−2log(1/ρ)log d lenγ , which suggests the possibility of dimension reduction
ϵreachγ
for the curve via linear projection. However, in our context what matters more is whether
the complexity of the curve is simplified, for example len /reach ; furthermore, our samples
γ γ
are not distributed on the curve, but in a tube around the curve of radius as large as a
fraction of the reach. Here are numerical tests in the same setup as Figure 1: for the Meyer
helix in d = 36 dimensions, we consider the projection onto a 12-dimensional subspace,
obtained by PCA or by random projection, and compute the length and the reach of the
image. We consider 10 independent repetitions and, for comparison, we also include the
original curve as well as the projection onto the first 12 principal components. To be
consistent with the scaling in the Johnson-Lindenstrauss lemma, for the PCA and random
√
(cid:112)
projection, we rescale points on the projected image of curve by the factor 36/12 = 3.
projection P original γ PCA 1 2 3 4 5 6 7 8 9 10
len 731 1136 703 720 709 717 639 748 766 709 737 706
Pγ
reach 6.0 5.0 2.2 4.4 2.4 3.9 2.9 4.4 2.0 4.3 2.5 2.3
Pγ
lenPγ 122 226 322 164 295 187 223 171 383 166 291 313
reachPγ
These numerical results support our argument that the Meyer helix cannot be easily
embedded in lower dimensional space without significantly affecting its complexity, which
involves pointwise curvature/reach that are beyond the scope of random projections.
References
Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient de-
scentonnon-convexlossesfromhigh-dimensionalinference. Journal of Machine Learning
Research, 22(106):1–51, 2021. URL http://jmlr.org/papers/v22/20-1288.html.
50Conditional regression for the Nonlinear Single-Variable Model
Richard Baraniuk and Michael Wakin. Random projections of smooth manifolds. Founda-
tions of Computational Mathematics, 9:51–77, 01 2009. doi: 10.1007/s10208-007-9011-z.
A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory,39(3):930–945,1993. doi: 10.1109/18.256500.
Benedikt Bauer, Luc Devroye, Michael Kohler, Adam Krzyz˙ak, and Harro Walk. Nonpara-
metric estimation of a function from noiseless observations at random points. Journal
of Multivariate Analysis, 160:93–104, 2017. ISSN 0047-259X. doi: https://doi.org/10.
1016/j.jmva.2017.05.010. URL https://www.sciencedirect.com/science/article/
pii/S0047259X1730338X.
R. Bhatia. Matrix Analysis, volume volume 169 of Graduate Texts in Mathematics.
Springer, 1997.
Peter J. Bickel and Bo Li. Local polynomial regression on unknown manifolds. Lecture
Notes-Monograph Series, 54:177–186, 2007. ISSN 07492170. URL http://www.jstor.
org/stable/20461468.
Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index
models with gradient flow, 2023. URL https://arxiv.org/abs/2310.19793.
P.Binev, A.Cohen, W.Dahmen, R.A.DeVore, andV.Temlyakov. Universalalgorithmsfor
learning theory part i: piecewise constant functions. J. Mach. Learn. Res., 6:1297–1321,
2005.
P.Binev, A.Cohen, W.Dahmen, R.A.DeVore, andV.Temlyakov. Universalalgorithmsfor
learning theory part ii: piecewise polynomial functions. Constr. Approx., 26(2):127–152,
2007.
P. Binev, A. Bonito, R. DeVore, and G. Petrova. Optimal learning. preprint, 2023. URL
https://arxiv.org/pdf/2203.15994.
Leo Breiman and Jerome H. Friedman. Estimating optimal transformations for multiple
regression and correlation. Journal of the American Statistical Association, 80(391):580–
598,1985. doi: 10.1080/01621459.1985.10478157. URLhttps://www.tandfonline.com/
doi/abs/10.1080/01621459.1985.10478157.
R. Dennis Cook. Save: a method for dimension reduction and graphics in regression.
Communications in Statistics - Theory and Methods, 29(9-10):2109–2121, 2000. doi:
10.1080/03610920008832598. URL https://doi.org/10.1080/03610920008832598.
Rapha¨el Coudret, Benoit Liquet, and J´erˆome Saracco. Comparison of sliced inverse regres-
sion approaches for underdetermined cases. 01 2013.
AlexDamian,LoucasPillaud-Vivien,JasonLee,andJoanBruna. Computational-statistical
gaps in gaussian single-index models (extended abstract). In Shipra Agrawal and Aaron
Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247
of Proceedings of Machine Learning Research, pages 1262–1262. PMLR, 30 Jun–03 Jul
2024. URL https://proceedings.mlr.press/v247/damian24a.html.
51Yantao Wu and Mauro Maggioni
MichelDelecroixandMarianHristache. M-estimateurssemi-param´etriquesdanslesmod`eles
`adirectionr´ev´elatriceunique. Bulletin of the Belgian Mathematical Society Simon Stevin,
6(2):161–186, 1999.
Michel Delecroix, Wolfgang H¨ardle, and Marian Hristache. Efficient estimation in single-
index regression. SFB 373 Discussion Papers 1997,37, Humboldt University of Berlin,
Interdisciplinary Research Project 373: Quantification and Simulation of Economic Pro-
cesses, 1997. URL https://ideas.repec.org/p/zbw/sfb373/199737.html.
MichelDelecroix, MarianHristache, andValentinPatilea. Onsemiparametricm-estimation
in single-index regression. Journal of Statistical Planning and Inference, 136:730–769, 03
2006. doi: 10.1016/j.jspi.2004.09.006.
Naihua Duan and Ker-Chau Li. Slicing regression: A link-free regression method. The
Annals of Statistics, 19(2):505–530, 1991. ISSN 00905364. URL http://www.jstor.
org/stable/2242072.
H. Federer. Curvature measures. Transactions of the American Mathematical Society, 93
(3):418–491, 1959.
St´ephane Ga¨ıffas and Guillaume Lecu´e. Optimal rates and adaptation in the single-index
model using aggregation. Electronic Journal of Statistics, 1, 04 2007. doi: 10.1214/
07-EJS077.
L´aszl´oGy¨orfi,MichaelKohler,AdamKrzyz˙ak,andHarroWalk. ADistribution-FreeTheory
of Nonparametric Regression. Springer New York, 2002. doi: 10.1007/b97848. URL
https://doi.org/10.1007%2Fb97848.
Wolfgang Hardle and Thomas M. Stoker. Investigating smooth multiple regression by the
method of average derivatives. Journal of the American Statistical Association, 84(408):
986–995, 1989. ISSN 01621459. URL http://www.jstor.org/stable/2290074.
Wolfgang Hardle, Peter Hall, and Hidehiko Ichimura. Optimal smoothing in single-index
models. The Annals of Statistics, 21(1):157–178, 1993. ISSN 00905364. URL http:
//www.jstor.org/stable/3035585.
Trevor Hastie and R. Tibshirani. Generalized Additive Models. John Wiley & Sons, Ltd,
2014.ISBN9781118445112.doi: https://doi.org/10.1002/9781118445112.stat03141.URL
https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat03141.
Joel L. Horowitz. Semiparametric Methods in Econometrics. Springer New York, 1998. doi:
10.1007/978-1-4612-0621-7. URL https://doi.org/10.1007%2F978-1-4612-0621-7.
Joel L. Horowitz and Enno Mammen. Rate-optimal estimation for a general class of non-
parametric regression models with unknown link functions. The Annals of Statistics, 35
(6):2589–2619, 2007.
Marian Hristache, Anatoli Juditsky, and Vladimir Spokoiny. Direct estimation of the index
coefficient in a single-index model. The Annals of Statistics, 29(3):595–623, 2001. ISSN
00905364. URL http://www.jstor.org/stable/2673964.
52Conditional regression for the Nonlinear Single-Variable Model
HidehikoIchimura. Semiparametricleastsquares(sls)andweightedslsestimationofsingle-
index models. Journal of Econometrics, 58(1):71–120, 1993. ISSN 0304-4076. doi:
https://doi.org/10.1016/0304-4076(93)90114-K. URL https://www.sciencedirect.
com/science/article/pii/030440769390114K.
AnatoliB.Juditsky,OlegV.Lepski,andAlexandreB.Tsybakov. Nonparametricestimation
of composite functions. The Annals of Statistics, 37(3):1360–1404, 2009. ISSN 00905364,
21688966. URL http://www.jstor.org/stable/30243671.
Samory Kpotufe. K-nn regression adapts to local intrinsic dimension. In Proceedings of the
24th International Conference on Neural Information Processing Systems, NIPS’11, page
729–737, Red Hook, NY, USA, 2011. Curran Associates Inc. ISBN 9781618395993.
Samory Kpotufe and Vikas Garg. Adaptivity to local smoothness and dimension in kernel
regression. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger,
editors, Advances in Neural Information Processing Systems, volume 26. Curran Asso-
ciates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/
file/28fc2782ea7ef51c1104ccf7b9bea13d-Paper.pdf.
Ming-Jun Lai and Zhaiming Shen. The kolmogorov superposition theorem can break
the curse of dimensionality when approximating high dimensional functions. ArXiv,
2112.09963, 2021. URL https://api.semanticscholar.org/CorpusID:245334285.
Alessandro Lanteri, Mauro Maggioni, and Stefano Vigogna. Conditional regression for
single-index models. Bernoulli, 28(4):3051 – 3078, 2022. doi: 10.3150/22-BEJ1482. URL
https://doi.org/10.3150/22-BEJ1482.
Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-
dimensionalpolynomialswithsgdneartheinformation-theoreticlimit,2024. URLhttps:
//arxiv.org/abs/2406.01581.
Bing Li and Shaoli Wang. On directional regression for dimension reduction. Journal of
the American Statistical Association, 102(479):997–1008, 2007. ISSN 01621459. URL
http://www.jstor.org/stable/27639941.
Bing Li, Hongyuan Zha, and Francesca Chiaromonte. Contour regression: A general ap-
proach to dimension reduction. The Annals of Statistics, 33(4):1580–1616, 2005. ISSN
00905364. URL http://www.jstor.org/stable/3448618.
Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American
Statistical Association,86(414):316–327,1991. ISSN01621459. URLhttp://www.jstor.
org/stable/2290563.
WenjingLiao,MauroMaggioni,andStefanoVigogna. Learningadaptivemultiscaleapprox-
imationstodataandfunctionsnearlow-dimensionalsets. In2016IEEEInformationThe-
ory Workshop (ITW), page 226–230. IEEE Press, 2016. doi: 10.1109/ITW.2016.7606829.
URL https://doi.org/10.1109/ITW.2016.7606829.
53Yantao Wu and Mauro Maggioni
Wenjing Liao, Mauro Maggioni, and Stefano Vigogna. Multiscale regression on unknown
manifolds. Mathematics in Engineering, 4(4):1–25, 2022. ISSN 2640-3501. doi: 10.
3934/mine.2022028. URL https://www.aimspress.com/article/doi/10.3934/mine.
2022028.
Hao Liu, Jiahui Cheng, and Wenjing Liao. Deep neural networks are adaptive to function
regularity and data distribution in approximation and estimation, 2024a. URL https:
//arxiv.org/abs/2406.05320.
ZimingLiu, YixuanWang, SachinVaidya, FabianRuehle, JamesHalverson, MarinSoljaˇci´c,
Thomas Y. Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv,
2404.19756, 2024b.
Ir`ene Gijbels R. J. Carroll, Jianqing Fan and M. P. Wand. Generalized partially lin-
ear single-index models. Journal of the American Statistical Association, 92(438):477–
489, 1997. doi: 10.1080/01621459.1997.10474001. URL https://doi.org/10.1080/
01621459.1997.10474001.
Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.
Mechanism for feature learning in neural networks and backpropagation-free machine
learningmodels. Science, 383(6690):1461–1467, 2024. doi: 10.1126/science.adi5639. URL
https://www.science.org/doi/abs/10.1126/science.adi5639.
JohannesSchmidt-Hieber. NonparametricregressionusingdeepneuralnetworkswithReLU
activation function. The Annals of Statistics, 48(4):1875 – 1897, 2020. doi: 10.1214/
19-AOS1875. URL https://doi.org/10.1214/19-AOS1875.
Ohad Shamir. Discussion of: “nonparametric regression using deep neural networks with
ReLUactivationfunction”. The Annals of Statistics,48(4):1911–1915,2020. URLhttps:
//doi.org/10.1214/19-AOS1915.
Guohao Shen, Yuling Jiao, Yuanyuan Lin, Joel L. Horowitz, and Jian Huang. Deep quan-
tile regression: Mitigating the curse of dimensionality through composition, 2021. URL
https://arxiv.org/abs/2107.04907.
G.StewartandJiguangSun. Matrix Perturbation Theory. Computer Science and Scientific
Computing. Academic Press, 1990.
Thomas M. Stoker. Consistent estimation of scaled coefficients. Econometrica, 54(6):1461–
1481, 1986. ISSN 00129682, 14680262. URL http://www.jstor.org/stable/1914309.
Charles J. Stone. Optimal global rates of convergence for nonparametric regression. The
Annals of Statistics, 10(4):1040–1053, 1982. ISSN 00905364. URL http://www.jstor.
org/stable/2240707.
Jan-Olov Str¨omberg. Computation with wavelets in higher dimensions. In Proceedings of
the International Congress of Mathematicians, volume 3, pages 523–532, 1998.
54Conditional regression for the Nonlinear Single-Variable Model
Taiji Suzuki and Atsushi Nitanda. Deep learning is adaptive to intrinsic dimensionality of
model smoothness in anisotropic Besov space. In Proceedings of the 35th International
Conference on Neural Information Processing Systems, NIPS ’21, Red Hook, NY, USA,
2024. Curran Associates Inc. ISBN 9781713845393.
R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge Uni-
versity Press, 2018.
Zihao Wang, Eshaan Nichani, and Jason D. Lee. Learning hierarchical polynomials with
three-layer neural networks. In The Twelfth International Conference on Learning Rep-
resentations, 2024. URL https://openreview.net/forum?id=QgwAYFrh9t.
55