Theory of Mind Enhances Collective Intelligence
Michael S. Harr´e, Catherine Drysdale, Jaime Ruiz-Serra
November 15, 2024
Abstract
Collective Intelligence plays a central role in a large variety of fields, from economics and evolutionary theory
to neural networks and eusocial insects, and it is also core to much of the work on emergence and self-organisation
in complex systems theory. However, in human collective intelligence there is still much more to be understood in
the relationship between specific psychological processes at the individual level and the emergence of self-organised
structures at the social level. Previously psychological factors have played a relatively minor role in the study of
collective intelligence as the principles are often quite general and applicable to humans just as readily as insects
or other agents without sophisticated psychologies. In this article we emphasise, with examples from other complex
adaptivesystems,thebroadapplicabilityofcollectiveintelligenceprincipleswhilethemechanismsandtime-scalesdiffer
significantly between examples. We contend that flexible collective intelligence in human social settings is improved
by our use of a specific cognitive tool: our Theory of Mind. We identify several key characteristics of psychologically
mediated collective intelligence and show that the development of a Theory of Mind is a crucial factor distinguishing
social collective intelligence from general collective intelligence. We then place these capabilities in the context of the
next steps in artificial intelligence embedded in a future that includes an effective human-AI hybrid social ecology.
1 Introduction
All intelligence is collective intelligence. [70]
Collectives are capable of achieving things that individuals alone cannot. Notwithstanding the simplicity or complex-
ity of the individuals, their aggregate behaviour can often be understood as a complex processing of information that
individualsstore, modify, andtransferbetweeneachotherproducing‘useful’collectivebehaviouratthescaleofthewhole
collective. In most instances of Collective Intelligence (CI), where the agents might be ants in an ant colony, bees in a
beehive, or neurons in a neural network, the individual is not aware of the drivers of their behaviour or the behaviour
of other agents. For example, a single neuron is neither aware of its own internal processes nor that of a neuron it is
connected to, nor is it aware of the end goal to which its activity contributes. Despite both this lack of awareness and
the lack of a centralised controller, evolutionary and learning processes have produced an intricate, precise, and highly
adaptive system that is capable of functional behaviour that would be impossible for any single neuron to achieve. In
other instances of CI,such as teamsof humans, or businesses interacting in economic markets, the agents themselvesmay
behighlycomplexandexhibitvaryingdegreesofpurposefulnessandawareness. Withinthiscontext,wedrawattentionto
the role of psychological factors in improving the CI of human social collectives and quantifying the intelligence of social
collectives, both natural and artificial.
Inordertounderstandhowcollectivesprocessinformation,wefirstconsiderthevarietyofwaysinwhichagentsinteract.
Thetopologyofthenetworkdescribingagent-to-agentinteractionsiswellknowntobeimportantfortheproperfunctioning
of social groups [83, 79]. In particular it has been shown that mammalian social groups exhibit patterns of fractal-like
topologies[40,51]thatarearesultofacognitiveabilitytoformdiscretesocialconnectionsbetweenconspicifics[49]. These
links are often both spatially and temporally transient; people meet for a while, go their separate ways, and come back
togetherlater. Despitethistransience,individualconnectionsareoftenthebasisoflongtermsocialrelationshipsbetween
specific individuals as in pair-bonding and friendships. Consequently an important distinction can be made regarding
connectionsbetweenagentsincomplexadaptivesystems: theycanbemorefluid-likeormoresolid-like[101]. Forexample
thelinksbetweenneuronsinthebrainarerelativelyfixedinnaturewhencomparedtothebriefcommunicativeinteractions
between ants, either instantaneous interactions between individual ants or via transient pheromone trails that coordinate
the behaviour of large numbers of ants. Sol´e and colleagues [101, 88] identify a distinction between solid brains, in which
interactions between agents fixed in place are highly persistent in time (e.g. neural networks, spin glasses) and liquid
brains, in which interactions between highly mobile agents are much more short-lived (e.g. ants, immune cells). As Sol´e
et al. noteregardingliquidbrains[101]: “Heretherearenoneural-likeelementsandyetinmanywaysthesesystemssolve
complex problems, exhibit learning and memory, and make decisions in response to environmental conditions.”
All biological agents are composed of sub-units such as organs, cells, and molecular networks [67, 69, 71]. Cells in par-
ticulararethesimplestlivingorganismswithindividualintelligence,orcompetencies[67,33],withintheirnativecontexts.
1
4202
voN
41
]AM.sc[
1v86190.1142:viXraHere, we briefly focus on the archetypal single-cell intelligence, the neural cells. It is well understood that the central
nervoussystemisahighlydeveloped,adaptive,complexsystemthatexhibitsemergentcomputationalcharacteristics[52],
bothinbiologicalandartificialneuralnetworks. Naturallytheartificialmodelsaresimplificationsbuttheextenttowhich
they are simplifications is not so well understood. In a 2021 study, Beniaguev et al. [9] concluded that between five and
eightlayersofanartificialdeepneuralnetworkarerequiredtoapproximatetheinput–outputmappingofa(single)cortical
neuron and that the dendritic branches can be understood as spatiotemporal pattern detectors. This demonstrates that a
single neural cell can be modelled as an artificial agent with highly complex computational capabilities situated within an
adaptive, complex network of other highly complex agents, all signalling to one another. These results can be compared
with earlier studies in which neurons were modelled as a Bayesian agent that is trying to infer the state of a hidden
variable [25]. In each of these interpretations, a single cell can be seen as an agent with computational competencies
situated within the context of a network that is slowly and adaptively changing around it.
We can also compare the competencies of neural cells in networks to the individual competencies of ants in an ant
colony. Inarecentstudy[56]itwasshownthatsocialstructuresofsomeantcoloniesareconservedbetweenspeciesthatare
separated by more than 100 million years of evolution. In the five species studied by Kay et al. [56], they found two social
clusters and similarities in the division of labour that are preserved between the species. In a different study, Richardson
et al. [91] showed that individuals within an ant colony play an important leadership role and that the behaviour of
these individuals significantly improved the collective performance of the ants. Ants are also capable of changing their
social structure in the event of pathogenic infestation of their colony. In a 2021 article, Stockmaier and colleagues [102]
review the research on social distancing and other social restructuring that occurs with conspecifics in order to reduce the
impact of pathogens by changing their social cues, signals, and other behaviours for the collective benefit of the colony.
Thesetwo verydifferentsystems, neuralnetworksandantcolonies, areexamplesof complex collectiveintelligenceswhere
the individuals (neurons, ants) are complex in their own right, but they signal each other in order to restructure their
relationships so as to adapt their collective competencies to external signals. The neural networks are prototypical solid
brains and ant colonies are prototypical liquid brains.
Humansocialinteractionscanalsobeviewedasaformofliquidintelligence. Miglianoetal.[79]discussthe‘fluidity’of
social relations in early human societies: “Quantification and mapping of hunter–gatherers’ social networks has revealed
details of a fluid and multilevel sociality, where friendship links connect unrelated mobile households into camps of
temporary composition”. They describe the key characteristics of early human society, such as egalitarianism, division
of labour, cooperative living with unrelated individuals, multi-locality, fluid social structures, and high mobility between
campsites, which might be thought of as a liquid brain composed of social interactions that both cluster and disperse in
order to store, modify, and transfer information via social networks. The notion that human social interaction might be
a form of computation is not new: Mirowski, Axtell and colleagues [82, 60, 81] have suggested that economic markets are
a form of computation by which prices can be derived, and Harr´e recently hypothesised [45] that this could be measured
using information theory as had been done earlier for financial markets [47, 42]. As Axtell et al. [60] wrote: “There is a
closeconnectionbetweenagentcomputinginthepositivesocialsciencesanddistributedcomputationincomputerscience,
in which individual processors have heterogeneous information that they compute with and then communicate to other
processors.”
The emergence of computation in multi-agent systems is a well-studied area of complex adaptive systems [64, 84]. For
exampleneurosciencehasusedinformationtheorytodescribethestorage,transfer,andmodificationofbitsofinformation
in biological neural processes [117]. More broadly, Integrated Information Theory (IIT) [107, 77] has been put forward as
a measure of the emergence of ‘consciousness’ in generic (non-biological, non-neural) systems. In this case, some forms
of IIT explicitly use information theory [8, 78] to measure the amount of non-trivial computation a system is carrying
out. More generally, there is a move towards understanding complex adaptive systems in computational terms [89, 74] by
empirically measuring the inter-agent flow of information [12].
In this article we use information theory to quantify how much computation in a CI is ‘emergent’ and how much
is simply independent information processing by single agents. In general, we wish to capture the notion of the whole
(computational process) being greater than the sum of the (independent) parts. We translate this to the simple notion
that to the extent to which this inequality holds: Whole - P(Parts) > 0 is the extent to which we will say a system
exhibits non-trivial CI, noting that there are multiple possible implementations of this approach [54]. The Parts is how
much computation a single agent is carrying out from one time step to the next such that the sum is the total of all
agents’ independent computations. The Whole is the totality of computation in the system, it includes all single agent
computations, pairwise computations, and higher order interactions between agents. Our measure will not be unique in
any of its specifics, but it serves to quantify the CI of a system for comparative analysis. This approach also has much
in common with that of Moore et al. [84] in which information theory is used to measure the collective intelligence in
biological systems.
NotonlyistherediversityinthetypesofsystemsthatcanshowpositivemeasuresofCI,butthewaysinwhichagents
manipulate a system’s computations is diverse as well. Take for example Watson and Levin’s discussion of a scientist
manipulating the intercellular signalling in order to change their collective outcome [110]:
2This framework [of collective cellular intelligence] makes a strong prediction: if intercellular signalling (not
genes) is the cognitive medium of a morphogenetic individual, it should be possible to exploit the tools of
behavioural and neuro-science and learn to read, interpret and re-write its information content in a way that
allows predictive control over its behaviour (in this case, growth and form) without genetic changes.
A counter question is: How can single agents, such as human leaders, have predictive control over a social group? Just as
ascientistexternal toacellcollectivecanmanipulateinter-cellularsignallingtocontroltheoutcomesofthecellcollective,
a leader internal to a human collective can manipulate inter-personal behaviours to control the outcomes of the human
collective. In both cases, an agent with a goal-directed psychology is acting on inter-agent relationships, i.e. inter-cellular
or inter-personal, to control outcomes at the next level higher, i.e. organism-scale or societal-scale.
In this work we will ask an analogous question of human agents: What is there in human psychology that allows us
to learn to read, interpret, and re-write our interpersonal information content in a way that allows predictive control over
our collective behaviour? We will not be able to explore all of the possible interpretations of this question here, but we
posit that our Theory of Mind (ToM) is a suite of cognitive skills that allows individuals to have goal directed control
over collective outcomes. Originally ToM was used to describe our ability to infer the unobserved mental states of other
people [34] such as desires and beliefs, an ability humans are particularly good at and other animals much less so [86, 59].
But recently it has been shown that ToM is predictive of group performance as well [121, 31], empirically demonstrating
the role of ToM in going beyond representations of the internal states of others to using that knowledge in a social setting
to improve the collective outcomes for the group. In order to model ToM in a tractable fashion, we will focus on the
narrowergame theory of mind[123],andtheBeliefs, Preferences, and Constraints(BPC)interpretationofgame-theoretic
decisionsputforwardbyGintis[35]. Inthisapproach,whatagentsunderstandofotheragents’hiddenstatesaretheBPC
that structure their observable behaviours.
We will consider this question in the framework of agent interactions that extend agent utilities in a simple but
novel way. We quantify our results using information theory to show the impact that a correctly deployed ToM has to
direct agents’ behaviours in order to increase our CI. The models are simple but they illustrate the central notion that
understanding the “beliefs, preferences, and constraints” [36, 37] of others can be used to improve the CI of a complex
social system.
In Section 2, we describe the liquid–solid dichotomy of interacting agents, review extant models of ToM, and provide
perspective on the interplay between social network structures and ToM. In Section 3, we provide illustrative examples
supportingdifferentaspectsofourargument,introducingourmeasureofcomputationandapplyingittoasimpleempirical
example. In Section 4 we review the psychology of social fluidity and the variety of social outcomes that this fluidity
makespossible. Wealsouseasimplemulti-agentsystemtodescribehowaToMcanbeusedtoimprovethecomputational
processes, i.e. the CI, of interacting agents. Finally, in Section 5, we discuss the broader implications of this approach.
2 Cognitive morphospaces: The network topology of agents’ interactions
The emergence of cognitive networks marked a pivotal moment in our evolutionary history [120]. Earlier, microor-
ganisms had developed collective structures capable of responding to the physical environment, particularly conditions
threatening individual cells [5] and survival became dependent on information exchanges within these groups. Habitua-
tion and the ability to minimise the energy response to danger stimuli is considered one of the simplest forms of learning
and has been extensively studied in simple collective intelligences such as slime moulds [13, 106]. Similarly bacteria use
quorum sensing to coordinate their behaviour based on the density of the population of their community to coordinate
responses, leading to a change in gene expression and function regulation e.g. bioluminescence, release of toxins, and
biofilm formation [72]. Information processing and problem solving capabilities developed using a variety of interaction
types and network topologies long before the appearance of central nervous systems with fixed neuronal structures [5].
But this leads to an interesting question regarding the typologies of agent-to-agent interactions and the intelligence these
structures might enable, a question that can be approached by looking at the morphospace of collective intelligence.
A morphospace is a theoretical framework used to simplify and organize the complex shapes and forms of organisms,
typically focusing on external anatomical features, into a more manageable space representing their potential variations.
For instance, in [3], a three-dimensional morphospace was constructed for organisms with shells, where the diversity of
shell shapes is described by three key parameters: a deviation angle, a translation factor, and a growth factor. This
reducesahigh-dimensionalstructuralspacetoalower-dimensionalone,whereasmallnumberofparameterisedproperties
captures key variations between forms. Morphospaces have found uses in many different fields of research, including the
body shapes of fish [20], network topologies [3], and the structural forms of language [96].
These structures, which represent recurring patterns of trait variation, are of great interest to evolutionary biologists
becausetheymayindicatesharedevolutionaryprocessesandtheirconstraints. Theyarealsoofgreatinteresttoresearchers
investigating the underlying structures of collective intelligence [68], as intelligence in its different forms may also be
subject to shared evolutionary mechanisms and constraints. With this in mind, work has been done in studying a variety
of morphospaces related to collective intelligence and in the next section we review some examples.
32.1 Network topology and the “Solid Brain, Liquid Brain” framework
In the most general of terms, a cognitive network has multiple information processing components that exchange
information with each other and interact with their environmental context. Sol´e et al. [101] identified two key dimensions
for categorising different types of cognitive networks: the system’s physical state—either more liquid or more solid in
nature, differentiated by how freely individual agents (components) can move in space—and the presence or absence of
neurons. The collective dynamics of a large population of agents is influenced by the individuals’ mobility which dictates
how they respond to signals both internal and external to the collective. To help conceptualise this diversity, Sol´e et
al. [101] developed a morphospace and taxonomy in order to compare and contrast the physical states of different types
ofCI.Inthiswaytheywereabletoconsiderthephysicalpropertiesthatformconstraintsonthecomputationsachievable
byasystem. Thisthenposesaninterestingquestion: Istheentirespaceofpossibleintelligencesbeingexploitedbyeither
synthetic biology or abiotic computation?
Liquid brains exhibit cognitive behaviours without neurons. For example, models by Watson and colleagues [112, 111]
illustrated how systems of self-interested agents, driven by a simple mechanism of strengthening beneficial connections,
can lead to robust group-level adaptation and problem-solving. This self-organisation, akin to Hebb’s rule in neural
networks,enablesthesystemtorecallandconsequentlyleveragepastconfigurationsthatweresuccessful. Thisalsoallows
the system to generalise from experience and then predict beneficial states it has not encountered before, highlighting
how decentralised actions can produce a form of CI that guides the system towards greater global utility. The agents
in these models are very simple, but this need not be the case—each agent within a collective may itself have a solid
brain as in human social networks where fluid social interactions allow each solid brain to connect and communicate with
other solid brains for the benefit of the collective [14]. This is extended to a hybrid model by Kao et al. [55], wherein the
modularorganisationofmobileanimalpopulations(asanexampleofaliquidbrain)suggeststhatcertaincommunication
pathways exhibit localised and persistent characteristics, akin to those in solid brains. These pathways enhance collective
decision-making in complex environments and in turn raise important questions regarding the relative strengths of liquid
and solid brains in different contexts. In particular, the conditions under which liquid brains outperform solid brains,
especially in terms of adaptability and scalability, remain an open area of investigation. Which computational problems
are more effectively solved by liquid brains vis-`a-vis solid brains remains to be better understood.
Whole classes of models that describe which biological or artificial structures are capable of some form of compu-
tation, either in potentia or in practice, can often be usefully represented using morphospaces [16]. Computational
morphospaces [101] have proven effective in studying key properties of complex adaptive systems, for example the sta-
tistical mechanics of information processing and structural variations [2, 4]. This sheds light on how energy constraints
influence the evolution and adaptability of neural networks across a variety of biological systems. Arsiwalla et al. [2] have
examined how liquid brains and solid brains fit within such a framework, by comparing the flexibility and adaptability
of different neural architectures [2]. The dimensions of their framework are three different types of complexity that a
system may display: autonomic, computational, and social complexity. We suggest that other axes for consideration are
a system’s solid–liquid dimension as Sol´e et al. [101] and Oll´e-Vila et al. [85] have done, as well as the system’s degree of
ToM (see Section 2.2) and the systems information processing capacity (see Section 3).
We posit that the collective intelligence that emerges from liquid brains (human social networks) is enhanced by our
individual capacity for a ToM, where individuals are aware of the goals of others as well as that of the collective, which
can then be achieved by adaptation at the local level. Rather than collective intelligence arising as an epiphenomenon or
byproduct of agents interacting, our ToM allows agents to causally affect the outcome of the system they are a part of.
2.2 Models of another agent’s internal states
FrithandFrithdefinedTheoryofMind(ToM)ashowweexplainotherpeople’sbehaviouronthebasisoftheirinternal
cognitive states, i.e. their knowledge, beliefs, and desires [34]. There is now a vast literature on this topic in psychology,
sociology,andmorerecentlyartificialintelligence,butforthepurposesofthisarticlewerestrictToMtoapplytothesubset
of the beliefs, preferences, and constraints of other agents in the sense of incentivised decisions. This borrows from the
BPC model [35] put forward as an approach to understanding the socio-cognitive aspects of human decision-making [37].
One way to interpret the BPC model is that it imposes structural constraints on the process by which decisions
are made, and then optimal decisions are discovered within these constraints by parametric variation. Recent work by
Peterson et al. [87] compared more than 20 structurally constrained models of individual decision-making using human
data. That study was extended to human data during strategic interactions by Harr´e and El-Tarifi [48] in order to test
agents’ constrained representations of other agents. This extends Yoshida et al.’s [123] notion of a Game Theory of Mind
to a larger variety of models in which an agent’s strategic reasoning about other agents modulates their behaviour.
In the field of artificial intelligence, Jara-Ettinger [53] proposed the use of Inverse Reinforcement Learning (IRL) as
a model of agential ToM, whereby agents modelling other agents’ mental states is equivalent to inferring an unobserved
world model the other agent uses in their decision-making, as well as their reward function. Jara-Ettinger discusses some
key limitations of IRL, such as the difficulty in recovering an agent’s beliefs and desires even while assuming that all
4agents are identical in their choice-making. IRL has been implemented in many different algorithmic forms, and their
applicability as a basis for ToM was recently reviewed by Ruiz-Serra and Harr´e [93].
What is missing in the Jara-Ettinger perspective is how different internal models, specifically different drivers of
behaviourpredicatedontheBPCofothers, influencehowagentsinteractwithoneanotherinasocialnetwork. Critically,
people use and improve upon these information carrying interactions in their social networks, as discussed next. This
requires agents to have more than a representation of the world model used by other agents, it needs to be a social world
model of how agents are influenced by their interactions with other agents. Beyond inferring internal states, Shteynberg
et al. [97] distinguish between awareness of the self, awareness of the self in relation to others, and an awareness of the
collective,whichismorethanjustthesumoftheselfandothers,atypeofcollectiveawarenesstheyhavecalledTheory of
CollectiveMind. Inasimilarvein,Shumetal.[98]proposedagenerativemodelforunderstandingmulti-agentinteractions
calledComposableTeamHierarchies. Thisapproachusedstochasticgamesinconjunctionwithmulti-agentreinforcement
learning in order to infer relationships between agents and predict future behaviours.
Incognitivescience,recentprogresshasbeenmadeinidentifyinglevelsofToMabilityandplacingtheminahierarchical
structure (see also Yoshida et al.’s Game Theory of Mind [123] and a recent review by Harr´e [46]). A cognitive agent
with zeroth order ToM attributes no cognitive ability to other agents, whereas first order ToM attributes some cognitive
abilities to others, and so on. Here we summarise the orders as described by Lombard and G¨ardenfors [76] and use A and
B to identify two agents that may or may not have any cognitive ability:
• ZeroorderToM:BothAandB’sbehaviourisgovernedbyinstinct,reflexes,andconditioningandsodirectperception
oftheagents’interactionswiththeirenvironmentandeachotherisallthatisneededtounderstandtheirbehaviour.
• First order ToM: A and B can be attributed with emotions, attention, desires, intentions, or beliefs, but neither
agent attributes these properties to any other agent, including themselves.
• Second order ToM: A attributes to B internal cognitive states, and that A uses this knowledge to understand B’s
behaviour. This is the lowest level at which A attributes hidden (cognitive) variables to B in order to explain the
causes of B’s actions, i.e. it abstracts causation away from direct perception of the causes of behaviour.
• ThirdorderToM:AattributestoBanunderstatingofA’sinternalstates. Toborrowanexamplefromgametheory
(stag hunt) and early human society (hunter gatherers), when a hunting party stalks an animal everyone shares a
common goal whereby each person A knows that the others are aware of A’s goal, such that this cognitive state
will causally inform A’s actions. Lombard and G¨ardenfors note that it has not been conclusively demonstrated in
nonhuman primates and indicate there are alternative views [39, 63].
• Fourth order and higher ToM: A has an awareness of at least two mental states, their own and that of B. For
example Happ´e reviewed the evidence and suggested that reflecting on one’s own cognitive state relies on the same
neuro-psychological functions as those we use to attribute thoughts to others [41].
Most interactions between agents across all species will be of the zeroth or first order ToM, where neither agent has
higher order cognitive states, has no sense of being aware of other agents nor any self-awareness. There is no sharp line
that unambiguously distinguishes between stimulus-response, conscious, and self-conscious agents as it is an open area
of research and is a multi-dimensional phenomenon [11], and so the conscious status of agents is likely more incremental
than the discrete levels of this scale would indicate, but is a useful framing device.
This leads to another core finding related to ToM: the interplay between language and ToM in humans. Chomsky has
noted both how well developed this ability is in humans and how communication sits in relation to our mental states [18,
p. 10]:
Communicationisnotamatterofproducingsomemind-externalentitythatthehearerpicksoutoftheworld,
thewayanaturalscientistcould. Rather,communicationisamore-or-lessaffair,inwhichthespeakerproduces
external events and hearers seek to match them as best they can to their own internal resources. Words and
conceptsappeartobesimilarinthisregard,eventhesimplestofthem. Communicationreliesonlargelyshared
cognoscitive powers, and succeeds insofar as similar mental constructs, background, concerns, and presupposi-
tionsallowforsimilarperspectivestobereached. Ifthatistrue—andtheevidenceseemsoverwhelming—then
natural language diverges sharply in these elementary respects from animal communication.
That is to say, effective communication between people requires an encoding-transmission step and a reception-decoding
step, both premised on shared cognitive representations in order to be understood, which we contend is made possible via
ToM, i.e. a (shared) representation of each other’s hidden cognitive variables.
Recent work has informed Chomsky’s view here, linking language and ToM with our ability to carry out causal
reasoning, particularly in social contexts. In Lombard and G¨ardenfors they proposed three key hypotheses relating ToM
to cognitive structures [76] (quoted, emphasis added):
• Theory of mind is an integral element of causal cognition;
5• Generally speaking, the more advanced causal cognition is, the more it is dependent on theory of mind; and
• Theevolutionofcausalcognitiondependsmoreandmoreonmentalrepresentationsofhiddenvariables. [...] [C]ausal
cognition allows us to reason from a network of hidden variables ...
Language use is related to this causal reasoning about hidden variables via the representational view of language due
to their intersection with ToM [32]. In the representational view of language people use specific grammatical structures
to represent complex events and then to reason from them [23]. These structures serve as a cognitive tool, particularly
in representing others’ mental states, enabling the expression of false beliefs, lies, or mistakes. It has also been shown
that these are strong predictors of children’s false belief understanding, a canonical test of ToM, and targeted syntactical
training can improve false belief reasoning [24]. Notably, in developmental learning, language has a stronger influence
on ToM abilities than vice versa [80]. From this we identify causal reasoning about hidden cognitive states with causal
reasoning about the behaviour of others via a shared representation of our social environment, mediated by specific
syntactical structures. We discuss shared representations and their use further in the next section.
2.3 Configurations of our social networks inform individual reasoning
FrithandFrithhavepreviouslyconsideredthebenefitsthataccruetohumansviaourToM[34],butwhatisfunctionally
happening when we use our ToM in social groups? It has been shown that in early hunter-gatherer societies that some
emergent phenomena at the social level, e.g. Dunbar’s Number [26, 27], are a consequence of the layered, fractal topology
of social networks [28, 40, 51], and that these are in turn the product of very specific, discrete, cognitive constraints at
the individual level that shaped the structures of early human societies [49]. At the individual level, a recent review
by Momennejad [83] collected the evidence for different social network topologies and how they integrate interpersonal
knowledgedifferently,showingthattopologyallowssocialnetworkstoservearichvarietyofcollectivegoals. Momennejad
also reviews the neuro-imaging evidence showing that humans neurologically encode these topologies and these encodings
are shared across the members of a social group. This was also demonstrated in the work of Lau et al. [65] showing that
peopleareabletointegrateinformationabouthowagentsrelatetooneanotherinadditiontohowtheyrelatetooneselfin
order to infer social group structures. Lastly, there is evidence for improved collective intelligence when individuals with
higher competencies in ToM are present in the group, as shown in the study by Woolley et al. [121]. It was found that,
just as there is for an individual person a measure of general cognitive ability, usually denoted g, there is an equivalent
measure of collective intelligence, denoted c, for a group of people. They noted a key explanatory factor of a group’s task
performance, as measured by c, was the proportion of group members who ranked highly on the Reading the Mind in the
Eyes cognitive test introduced by Baron-Cohen and colleagues [6, 7], a test used to measure an individual’s capacity for
ToM.
AkeytakeawayfromtheworkofWoolleyandcolleagues[121]isthatthecfactorisnotstronglycorrelatedwitheither
the average or maximum intelligence of the individuals in a group. However, it does correlate well with the average social
sensitivity of its members, the evenness of the distribution of contributions to group discussions, as well as the proportion
of people in the group who rate highly on a ToM test. So there is considerable evidence for the role our ToM plays
in group performance, how this shapes interpersonal interactions between people, and, as a consequence, the emergent
topological properties of our social groups. This provides support for the argument that ToM is (one of) the individual,
bottom-upmechanism(s)throughwhichagentsformhigher-ordersocialstructureswithmeasurablecollectiveintelligence.
This may be how we “learn to read, interpret and re-write our interpersonal information content” as we asked following
the Watson-Levin quote in Section 1. Next, we illustrate these ideas in a dyadic and then a triadic example of agents
interacting with each other and exhibiting non-trivial measure of CI.
3 Topological and cognitive structures in CI: illustrative examples
3.1 A dyadic example of individual learning at short time scales
ToillustratehowwewilluseinformationtheoryasaproxyforWoolleyet al.’scintelligence,weuseadatasetthatwas
previously studied [44] to measure the information flow in an iterated economic game experiment between monkeys and
computers, based on data from an earlier study by Lee et al. [66]. In that study, the experiment had a monkey playing
the matching pennies game against a computer algorithm for a reward, the (Nash) optimal reward for the monkey was
received if it plays 50:50 across its two choices. A simplified description of the three algorithms used by the computer
follows, see Lee et al. [66] for the exact descriptions:
• Algorithm 0: Play uniformly and independently of the monkey’s choices,
• Algorithm 1: The computer stores the history of the choices made by the monkey, then to predict what the monkey
woulddoineachtrialthecomputercalculatestheconditionalprobabilityofthemonkey’schoicegiventhemonkey’s
6choices in the preceding 4 trials, a Null hypothesis was used to test if the monkey played 50:50 and if it was not
rejected the computer plays 50:50, if it was rejected the computer plays probability 1−p against the monkey’s
probability of playing p
• Algorithm 2: Uses the same algorithm as 1) but includes both choices and rewards in the monkey’s history, and
then both algorithm 1) and 2) were tested against the Null of 50:50 and if the Null was not rejected the computer
plays 50:50, otherwise the best reply was played based on the estimated bias of the monkey.
The usefulness of this example is fourfold. First, it is simple enough that computations can be tested and evaluated
in an illustrative way so that the central information theory measures can be applied. Second, it uses game theory as the
foundational mechanism for the interactions that generate the information flow between the agents. Third, it is complex
enough to illustrate key elements of CI with only two agents. Finally, it illustrates the general principles that can be
applied in evolution, psychology, AI collectives, and human-AI hybrid settings. Our definition of CI is based loosely on
the information theory form of Integrated Information Theory (IIT) [107], where we are neutral to the interpretation of
IIT as a measure of consciousness, and will not be carrying out any optimisation over binary partitions of state variables,
sointhatsensethisisnotthesameasIIT,althoughtherearesomesimilarities. Ourdefinitionforasystemwithnagents
is (see Section 2.2.3 of [78] and for a general introduction see Battencourt [10]):
n
X
ϕ(X;τ)≜I(X ;X )− I(Xi;Xi ). (1)
t t−τ t t−τ
i=1
For example we might have X ={X1,X2} as the joint stochastic variable of agents 1 and 2 such as the monkey and the
t t t
computer. The function I(Xi;Xi ) is the time-delayed mutual information (TDMI) with delay τ for any times series of
t t−τ
a stochastic (possibly joint) variable X ∈{X ,X ,...,X }:
t 1 2 T
I(X ;X
)=Xn
p(X ,X
)logh p(X t,X t−τ) i
. (2)
t t−τ t t−τ p(X )p(X )
t t−τ
i=1
Equation 2 encodes the number of bits that variable X is able to predict about its own future state based on its (τ-
t
lagged) past states. For example, in a two-agent system, if X = {X1,X2} then I(X ;X ) is the amount of predictive
t t t t t−τ
information data τ steps in the past is encoded in the current state of the entire joint state of the system. On the
other hand, if X =X1 then I(X ;X ) encodes how much information the past of one agent encodes about its current
t t t t−τ
behaviour. The difference between the whole system TDMI and the sum of the individual’s TDMI is the extent to which
information is being exchanged between the agents, i.e. ϕ(X;τ) in Equation 1 is the excess TDMI.
Table 1 shows the results of these computations. We note the fact that, as the computer algorithm becomes more
sophisticated, from algorithm 0 to 1 to 2, ϕ(X;τ) decreases as the monkey plays closer and closer to the Nash strategy,
but then as the sophistication increases, the monkey looks further into the past data to extract information. This is most
notable in the difference for algorithm 2: at τ =3 (0.0148 bits) is nearly twice that of τ =1 (0.008 bits). This is because,
in this specific case, the monkey decouples itself from the computer in order to earn its highest reward: as the computer
strategy becomes more sophisticated in measuring the monkeys use of the computer’s choices, the less coupled to the
computer and its own past the monkey needs to be.
Time delay: τ Algo Joint TDMI Monkey TDMI Computer TDMI Excess TDMI: ϕ(X;τ)
1 0 0.0999 0 0 0.0999
2 0 0.1197 0.0075 0.0002 0.1120
3 0 0.1237 0.0095 0.0095 0.1047
1 1 0.0693 0.0011 0.0001 0.0681
2 1 0.0748 0.0025 0.0001 0.0722
3 1 0.0766 0.0029 0.0005 0.0732
1 2 0.0105 0.0005 0.0002 0.0080
2 2 0.0155 0.0019 0.0023 0.0113
3 2 0.0216 0.0030 0.0038 0.0148
Table 1: The simplest example of two agents interacting with one another via game theory in which ϕ(X;τ) is non-zero.
In general we cannot know the details of the flow of information, only that there is a net positive flow across all agents.
All values are in bits and computations carried out using JIDT [75].
73.2 A triadic example of evolutionary learning at long time scales
Before we introduce ToM for social interactions, we consider a second example where evolution has found an agent-to-
agent interaction that is similar to the worked example used next in Section 4. Our evolutionary example is a three-agent
system: the larval stage of the fly Liriomyza huidobrensis, the pea plant family Fabaceae that fly larvae predate on, and
the parasitic wasp Opius dissitus that predates on L. huidobrensis. These species interact in the following way [116]: The
larvae of L. huidobrensis infest a pea plant, the pea plant gives off volatiles, called infochemicals, that attract the wasps
to the plant, which in turn feed on the larvae, and this larva–wasp conflict indirectly benefits the pea plant.
In this triadic relationship, the pea plant does not directly respond to the threat from the larvae—for example it has
not evolved a chemical agent that repels the larvae-laying flies. Instead it signals a third party, the wasp, to bring the
waspintocontactwiththelarvae, andthewasptheneatsthelarvae. Thewaspsandthefliesareinadyadicevolutionary
competition that could be modelled using two-agent evolutionary game theory, but the pea plant, having facilitated an
instance of this conflict, benefits indirectly from the wasps success, in a sense plants can employ other species as a kind
of ‘body guard’ [58]. We note that, like the example in Section 4, the plant only signals the wasps to a dyadic interaction
whentheplantdetectsthelarvae,sothattheplantonlysignalswaspswhentheplantperceivesanintermittentinformation
carrying cue from its environment that wasps are needed.
This type of second order interaction occurs in other ecological examples as well [38] where Trait-Mediated Indirect
Interactions (TMIIs) induce hyper-graphs of interactions between species. More complicated interactions have also been
observed in which a plant that is being attacked emits infochemicals that lead to unaffected plants emitting volatiles to
attractpredators[58], toreducetheriskoftheunaffectedplantbeingattackedwhilealsoaidingtheplantbeingattacked.
Kobayashi and Yamamura [58] specifically call this evolutionary development a form of altruism but of course there is no
cognitive aspect to this altruism. These examples illustrate that evolution produces forms of sophisticated, inter-species,
mixed competitive-altruistic interaction networks but without any need for a ToM, individual awareness, or strategic
understanding of the interactions, and so individuals are not knowingly strategic or altruistic as we might interpret a
person to be. But these evolved strategies are limited, by definition, to be fixed within the lifetime of a single agent
and they can only adapt on evolutionary time-scales. This is in contrast to a ToM which allows us to adapt to multiple
strategic and social contexts that may require novel solutions within the lifespan of a single person.
4 Social network modification through ToM: a minimal model
Figure1: Graphsandhyper-graphsofthreeagents: (a)Adisconnectedgraphcontainingtwodyadicallyconnectedagents
and one isolated agent. (b) A connected dyadic graph. (c) A dyadic complete graph. (d) A hyper-graph in which all three
agents are connected by a single hyper-link. (e) A combination of a hyper-graph connecting all agents and a disconnected
dyadic graph. (f) A connected dyadic graph and a single hyper-link.
In this model we introduce a simple example of interacting agents that form a hypergraph [38]. As in the previous
examples, this model is general enough to allow the agents to have any form of biological or artificial psychology, to have
zeroth order ToM or fourth order ToM, and it applies to artificial collectives just as readily as it can to biological and
ecological collectives.
Game theory provides a formal approach to the analysis of incentivised social interactions in which agents attempt to
8optimise the outcome (utility) of their joint actions. The expected utilities can be rewritten as polynomials in the agents’
decision variables, usually interpreted as probabilistic weights, x ∈x for which the co-factors a are derived from a game
i
payoff matrix (as described in Appendix A). These can be generalised to higher order polynomials representing higher
order interactions between the agents: in these expanded utilities the quadratic terms represent the dyadic interactions
between agents, the cubic terms represent the three-way interactions, and so on:
X X X
U (x;a)=a0+ ajx + ajkx x + ajklx x x +h.o.t. (3)
i i i j i j k i j k l
j j,k j,k,l
We note that in general ∂Uj(x;a) describes the impact that i’s choice has on agent j’s utility and that we need not assume
that there exist either
sym∂ mxietrical
or even reciprocal impacts between agents’ utilities and their behavioural choices.
In Figure 1, we illustrate how three agents are connected via dyadic relationships and higher order (cubic) hyper-graph
interactions. Taking the utility for Agent 1 in Figure 1 as an example, the most general description of the interactions in
the dyadic utilities for the top row (a)–(c) are given by:
X X
(a): U (x;a)=a0+a1x (b)and(c): U (x;a)=a0+ ajx + ajkx x (4)
1 1 1 1 1 1 1 j 1 j k
j j,k
In (a), Agent 1 is the only agent that can influence their utility, but for (b) and (c), they are also influenced by other
agents, linearly and possibly quadratically, depending on the payoff structure. The co-factors a of these three utilities are
derivable directly from the utility matrices of dyadic agent-to-agent interactions in conventional game theory [50, 43] (see
Appendix A). For utilities (d)–(f) in Figure 1:
X X
(d): U (x;a)= ajklx x x (e): U (x;a)=a0+a1x + ajklx x x (5)
1 1 j k l 1 1 1 1 1 j k l
j,k,l j,k,l
X X
(f): U (x;a)=a0+a1x + ajkx x + ajklx x x (6)
1 1 1 1 1 j k 1 j k l
j,k j,k,l
With these descriptions of higher order interactions we can now provide an illustrative example of how a ToM may be
used to reconfigure a network to enhance the computational processing of information to improve collective performance.
In the example of the parasitic wasp and the fly larvae eating the leaves of a plant, the plant benefits indirectly from the
direct interaction between two other agents. In this case the plant encourages the wasp to come into contact with the
larvaeinafightforsurvivalthattheplantdoesnotdirectlyparticipatein. Intheexamplethatfollowsnext, asimilarbut
theoretical scenario between three agents is studied where two agents are initially interacting but not producing anything
with a third agent on the sidelines. Then the third agent provides a signal to the other two to selectively change their
behaviour that directly benefits two of the agents and indirectly benefits the third.
4.1 Model scenario
Webeginwiththreeagentsinproximitytooneanothersituatedinanoisyenvironmentinwhichitispossibleforthemto
collectivelydosomethingusefulbuttheyareinitiallyintheunfortunatesituationinwhichthebehaviourofthethreeagents
producesnothingthatisofvalue. Thepossibleactions(x )oftheagents(A )arebinary: x ∈{−1,1},i∈{1,2,3}andso
i i i
inthisexamplethex arenotprobabilities. A israndomlyanduniformlychangingitsstatesduetoa(useful,information
i 1
carrying) signal it receives from the environment at time t: st ∈{−1,1} such that P(st =1)=P(st =−1)=0.5. A and
2
A are initially engaged in the prisoner’s dilemma (PD) game where, as a consequence of selfishly (na¨ıvely) optimising
3
their choice of x , they are in the defect-defect Nash equilibrium (NE), and so their joint action is constant (with zero
i
value). Theyarecapableofasecondoutputwheninthecooperate-cooperateconfiguration,andthisoutputhasapositive
value. However, being stuck in the PD NE, they are not initially able to produce it.
A and A can produce something of value when they cooperate, but it only has value if the external signal st =+1.
2 3
The output value of A and A interacting is initially a sequence of 0s: they are not cooperating with each other, and
2 3
nothing of value is being produced. We assign each agent a state at time t, xt ∈ {−1,1}, such that their dynamic is an
i
ordered sequence of binary states [x1,x2,...,xT] for t ∈ {1,...,T}. The output from A and A interacting at time t
i i i 2 3
is a result of having matched their states: V(ot) = 1 if xt = xt = 1 and V(ot) = 0 if xt = xt = −1, the xt ̸= xt cases
2 3 2 3 2 3
are never achieved as they are not NE and A and A will always choose according to the NE of their interactions. The
2 3
collective behavioural vector is: x={x ,x ,x } or time indexed: xt ={xt,xt,xt}.
1 2 3 1 2 3
The signal st that A receives indicates whether or not A and A should cooperate at time t, but initially no
1 2 3
information is passing from A to {A ,A } and A cannot produce anything by itself. So U (x;a) = 0, and agents A
1 2 3 1 1 2
and A only produce output of zero value: V(ot)=0. The agent utilities in this case are:
3
U (x;a)=0, U (x;a)=a0+a3x +a2x , U (x;a)=a0+a2x +a3x , (7)
1 2 2 2 3 2 2 3 3 3 2 3 3
9Figure 2: The interaction networks for the two scenarios: (a) A perceives signal st but cannot relay it to A and A
1 2 3
and these two agents are not cooperating and so produce 0 value from their output ot. (b) A influences A and A to
1 2 3
cooperate when A receives a signal from the environment that cooperating will result in a positive payoff for A and A ;
1 2 3
as a consequence, A receives a portion of the utility from both A and A .
1 2 3
A →x
3 3 C D
+1≡C −1≡D
C (1,1) (0−c,1+c)
+1≡Cooperate (R, R) (S, T)
A →x D (1+c,0−c) (0,0)
2 2 −1≡Defect (T, S) (P, P)
(b) Prisoner’s Dilemma (c=−1) and Harmony (c= 1)
(a) Generalised payoff matrix 4 4
Figure 3: Payoff matrices for agents A and A , which agent A can strategically influence by setting c←x .
2 3 1 1
and U =U =0 for defect-defect joint strategies. This is equivalent to the configuration in Figure 1(a), and we illustrate
2 3
this in Figure 2, a. In the second scenario, we assume that A and A are still na¨ıvely pursuing their selfish goals, but
2 3
now A is more psychologically aware: it knows the beliefs, preferences, and constraints of the other two agents, and it is
1
able to influence their preferences so that they will cooperate with each other when A receives the signal: st =1. They,
1
in turn, will pay A some portion of their total payoff.
1
To do this numerically, we standardise the symmetric, two-player, two-choice, game theory payoff matrices for A and
2
A by setting R =1 and P =0 (Figure 3, a. and see [43]). We note that the Harmony game (Ha), in which both agents
3
coordinating is the only Nash equilibrium and T > R > P > S, can be transformed into the Prisoner’s Dilemma (PD),
in which both agents defecting is the only Nash equilibrium and R > T > S > P, by introducing a two-state parameter
c ∈ {−1,1} =⇒ {PD,Ha}andreplacingT =R+candS =P−csothatthepayoffmatricesderivablefromFigure3,b.
4 4
result in the utilities:
U (x;a)=R+cx −(R+c)x , U (x;a)=R+cx −(R+c)x . (8)
2 2 3 3 3 2
As agent A is aware of the specific BPC model by which A and A make their decisions, A adjusts their behaviour
1 2 3 1
by manipulating their utility co-factors following st, by setting xt = 1st such that xt ∈{−1,1}. A then modulates the
1 4 1 4 4 1
utility of the other two agents (via an information-carrying signal such as speaking to them) and, in response, receives
10% of the payoffs that result from the interaction between A and A . The resulting payoffs for each agent are (derived
2 3
in detail in Appendix B):
1
U (x;a)= (cid:0) U (x;a)+U (x;a)(cid:1) , U (x;a)=R+x x −(R+x )x , U (x;a)=R+x x −(R+x )x . (9)
1 10 2 3 2 1 2 1 3 3 1 3 1 2
4.2 Model interpretation
We make three observations regarding these two scenarios. First, we note that the interactions in the second scenario
are a hypergraph in the sense that if any one of the nodes were removed in Figure 2, b. no remaining agent receives
any payoff, all agents are necessary contributors to any single agent receiving a utility for their actions [38]. Second,
10it is straightforward to compute the collective intelligence of both scenarios using Equation 1 and the three stochastic
variables {xt,xt,xt} when st ∈ {−1,1} is uniformly distributed: In the first scenario it is 0 bits and for the second
1 2 3
scenario it is 1 bit. This is a simple illustration of how a single agent, knowing the BPC model of two other agents,
who are both na¨ıve to all other agents’ BPC models, can improve the collective intelligence of the group by manipulating
the network of interactions between agents. This is analogous to the results discussed in Woolley [121] in which group
performance, using a measure of collective intelligence, is improved by having agents who have a theory of mind. Third,
unlikeevolutionaryprocesses,theprocessinthesecondscenariohasagoal-directedagentwhoispsychologicallyinformed
about the internal states of the other agents and so is able to make plans contingent on different configurations of the
interaction network. This implies that A understands the preferences implied by the co-factors of Equation 8, (R+c)
1
and Equation 9, (R+x ), influencing the behaviour of the other two agents. These preferences are the hidden (cognitive)
1
variables discussed in Section 2.2, i.e. the Preferences aspect of the BPC framework. In terms of levels of ToM, A only
1
needs to ascribe hidden states to the other two agents but not the ability for A or A to ascribe hidden states to A , for
2 3 1
example the NE can be reached by A and A by observing the other agent’s behaviour. So the ToM of A is of second
2 3 1
order and A and A only need a first order ToM. In this sense a ToM allows A to reconfigure their social networks to
2 3 1
further their goals at much shorter time-scales than the evolutionary time-scale of ecological networks.
4.3 Our perspective in context
We have reviewed some of the recent advances in how people, as complex agents, with a vast space of specialised,
context-dependent behaviours, are able to coordinate their activities. In particular, finding ways to recombine our indi-
vidual competencies to produce, in a short period of time, the appropriate collective competencies is a combinatorically
complex task. The central theme of this article is that having a causal (generative) model of another person’s behaviour
that reflects the current environmental and social context helps us manage this complexity. This is one way in which
our solid brains produce the necessary liquid social structures that quickly build collective solutions with novel collective
competencies. In this section we bring these ideas together and summarise our view.
Wefirstdrawattentiontohow,inSection4,A hasconstructedanicheforitselfinthecontextofthepreexistingdyadic
1
network between A and A . In ecological networks a new agent joins a pre-existing network if it can find a niche within
2 3
which it can fit. This occurs in one of three distinct ways: niche choice, niche conformance, and niche construction [19,
108]. Niche choice occurs when an individual selects environmental conditions that align with its phenotype, while niche
conformance involves adjusting its phenotype to suit the environment. Niche construction is the modification of the
environment to meet individual needs, which may also impact other species. This suggests an analogy between the
formation of ecological networks, where new agents joining may enhance or disrupt the current configuration, and social
networks where membership can be explicitly or implicitly gated according to a prospective agent’s ‘fit’ within the group,
andevenifthegroupcanadjusttoaccommodateanewcomer,justasthenewcomercanadjustinordertobeaccepted. In
human social groups people can be recruited or excluded depending on their contribution to the better functioning of the
collective, which in turn corresponds to changes in individual neural activity related to the social network structure [95].
In our analogy with ecological networks, we suggest that any signalling agents in a network need to encode messages
over an information-carrying medium that receivers are receptive to and that can then be decoded by a specific receiver.
We have argued that humans, as both signallers and receivers, take advantage of our ToM in order to understand what
signals will be correctly interpreted by another person and to adapt their signalling to the cognitive state of the receiver.
Specifically, our psychology allows us to learn how to read, interpret, and re-write our interpersonal communication over
a very short time frame, allowing us rapid, targeted control over our collective behaviour.
In some sense the adage there is nothing new under the sun holds here as evolution and biology have recycled fun-
damental, pre-existing principles in the service of human sociality. But the adaptive speed of our social networks, the
psychological mechanisms involved, the variety of purposes they serve, and the complexity of the communications ap-
pear most highly developed in humans. In ecological networks, for example, some agents can act as an encoder-sender
of semantic information via infochemicals that signal another agent. A second agent then acts as a receiver-decoder
of this signal that in turn changes the behaviour of the receptive agent. The combination of the receiver’s anatomical
configuration and behavioural phenotype elicits an appropriate stimulus-response that benefits the signalling agent, and
these interactions can form vast, complex hypergraphs of competition and cooperation between agents of many different
species. But neither agent needs higher cognitive faculties—their ToM is of the zeroth order. By analogy, humans can
target specific individuals or groups of individuals in order to have them adjust their behaviours to best suit the goals of
the signaller. The competencies that a ToM affords the sender allows them to know that a receiver is capable of both
decoding the signal and acting appropriately in response because they know of the receiver’s receptive and causal states,
both cognitive (hidden) and behavioural (overt). That is to say they take advantage of their ToM to understand how,
when, what, and to whom they need to signal in order to achieve a beneficial outcome, whereas blind evolution would
take much longer to achieve the same results.
The separation of time-scales also plays an important distinction between learning processes that use the same theo-
retical foundation. The mathematical description of the processes that underpin the evolution of species has been shown
11tobeequivalenttothestatisticallearningprocessthatunderpinsBayesianlearninginindividualagents[17,113,114], for
both biological and artificial agents. What Bayesian learning provides at the level of the individual agent is an advantage
in the speed of adaptation that Bayesian learning via evolution cannot achieve. Again, this suggests a universality of
description that only varies qualitatively in terms of time-scales and the specifics of the mechanisms.
5 Discussion: Theory of Mind in the Context of Social AI
Each step in our cultural development has changed the ways in which we combine individual skills to achieve better,
more sophisticated collective outcomes. There is evidence that hunter gatherers participated in the division of labour,
complex social networks, multilevel, and fractal-like social structures long before we settled into villages [22, 29, 49, 40].
This was in part due to our ability to construct our own niches [61] and this practice has persisted from hunter-gathers
to farming [92] through to civilisation building. As Arroyo-Kalin et al. [1] quote in their opening to the special issue
Civilisation and Human Niche Construction:
Itisimpossibletoavoidtheconclusionthatorganismsconstructeveryaspectoftheirenvironmentthemselves.
They are not the passive objects of external forces, but the creators and modulators of these forces. The
metaphor of adaptation must therefore be replaced by one of construction, a metaphor that has implications
for the form of evolutionary theory (Levins and Lewontin 1985: 104).
It has also been argued that through the manipulation of our environmental niche we have brought about the Anthro-
pocene [57, 100, 30, 118].
On the other hand, social niche construction [94] extends this idea to the process whereby agents modify their social
context so as to influence their own social evolution. A social niche is the context in which social behaviour occurs, and
so social niche construction is where agents actively choose to change their social environments, for example, by choosing
who to associate with and how to behave [61]. Ryan et al. [94] describe this in game theoretical terms as the effective
gameagentsareplayingafterallrelevantfactorsareaccountedfor,suchastheunderlyinggameitself(thepayoffmatrices
in conventional games for example) and any social niche modifiers, amongst others. A social niche modifier is a trait that
alterstheeffectivegamebeingplayed,causingittodifferfromtheimmediatepayoffmatricesthatareusuallythecomplete
descriptionoftheincentivisedinteractions,forexamplepopulationstructure,relatedness,punishmentetc. Intheexample
in Section 4, A constructs a social niche for itself by manipulating the structure of the game that A and A are playing,
1 2 3
forthebenefitofA andincidentallyforthebenefitoftheotheragentsaswell. However, socialnicheconstructiontheory
1
pertains to evolution more broadly and is not specific to human social networks as the formal description of the model in
Section 4 could be an evolved network of plants and animals or a more rapidly changing social network.
In moving towards AI that is situated within a hybrid human-AI ecology, the complexities of effective network con-
struction, communication, andcognitivetoolswillneedtobeworkedthrough. Here, wediscussjusttwooftheissues: the
difficulties of building psychologically complex AIs and the social environment AI will need to adapt to. The capacity of
current AI theories to be sufficient to encompass, in principle at least, human levels of cognition is a rich area of research,
from attention mechanisms [109], to reinforcement learning subsuming reasoning [99] and chain of thought reasoning in
language models [73] there are arguments being made for emergent phenomena in AI [115] as well as ToM [103] and even
artificial general intelligence [15]. What is often missed in these examples is that the individual AI’s ability to digest
information and update parameter weights is only a small fraction of what is needed to be effective in a specifically social
context. An important tool in our psychological toolbox is our ability to maintain shared hidden variables that are the
causal basis of our coordinated joint actions in physical and social environments. Even progress on single agents having
effective causal models of the physical environment has been much slower than in other areas of AI [122]. As we have
shown in this article, there is very good evidence that causal models, both physical and social, are necessary for people
to be able to communicate with each other, and communication is inextricably tied to our ToM. This triad of language,
shared causal models of hidden variables, and ToM appears to be a minimum for human social coordination.
Even with this triad established within an AI, the next challenge is where, when, and how an AI should fit into
any given collaborative social context. Simple machine intelligence in collaboration with people, such as auto-correct,
recommender systems, or GPS navigation, are effective because a human decided there was a need for these tools, they
placed them in the appropriate context and then the users adjust their behaviour to any shortcomings the tool might
have. But the more autonomous machine intelligence becomes, the more complex the physical and social environment is,
and the more trust that needs to be placed in the causal models that drive the behaviour of an AI (i.e. their analogue of
a person’s hidden cognitive variables) the more an AI needs to know how to interact with us in a way that resembles how
we interact with each other. They will ultimately need to be able to do this via niche construction, niche adaptation, and
niche choice, all of which, for people, is a negotiated relationship between each other that needs to be satisficing for those
involved. This brings human-AI joint adaptation closer to what Laland et al. have in mind when considering a rethink of
evolutionary theory [62]:
12Weholdthatorganismsareconstructedindevelopment,notsimply‘programmed’todevelopbygenes. Living
things do not evolve to fit into pre-existing environments, but co-construct and coevolve with their environ-
ments, in the process changing the structure of ecosystems.
Consequently,wearguethatforanyAItobeabeneficially adaptive,autonomous,andsociallyawareagent,itwillalso
need to reflect the universal principles we see in many evolutionary transitions [105, 104, 90]. This includes multi-level
selection in economic [118, 119] and biological systems [21, 71] as well as the hierarchical transitions [21] that appear to
be ubiquitous in our biological, social, economic, and technological history.
References
[1] Manuel Arroyo-Kalin, David Wengrow, Dorian Q Fuller, Chris J Stevens, and Mich`ele Wollstonecroft. Civilisation
and human niche construction. Archaeology International, 20(1):106–109, 2017.
[2] Xerxes D Arsiwalla, Ricard Sole, Clement Moulin-Frier, Ivan Herreros, Marti Sanchez-Fibla, and Paul Verschure.
The morphospace of consciousness. arXiv preprint arXiv:1705.11190, 2017.
[3] Andrea Avena-Koenigsberger, Joaqu´ın Gon˜i, Ricard Sol´e, and Olaf Sporns. Network morphospace. Journal of the
Royal Society Interface, 12(103):20140881, 2015.
[4] Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein, and Surya Gan-
guli. Statistical mechanics of deep learning. Annual Review of Condensed Matter Physics, 11(1):501–528, 2020.
[5] Frantiˇsek Baluˇska and Michael Levin. On having no head: Cognition throughout biological systems. 7, 2016.
[6] Simon Baron-Cohen, Therese Jolliffe, Catherine Mortimore, and Mary Robertson. Another advanced test of theory
ofmind: Evidencefromveryhighfunctioningadultswithautismoraspergersyndrome. Journal of Child psychology
and Psychiatry, 38(7):813–822, 1997.
[7] Simon Baron-Cohen, Sally Wheelwright, Jacqueline Hill, Yogini Raste, and Ian Plumb. The “reading the mind in
the eyes” test revised version: a study with normal adults, and adults with asperger syndrome or high-functioning
autism. The Journal of Child Psychology and Psychiatry and Allied Disciplines, 42(2):241–251, 2001.
[8] Adam B Barrett and Anil K Seth. Practical measures of integrated information for time-series data. PLoS compu-
tational biology, 7(1):e1001052, 2011.
[9] David Beniaguev, Idan Segev, and Michael London. Single cortical neurons as deep artificial neural networks.
Neuron, 109(17):2727–2739, 2021.
[10] Lu´ıs MA Bettencourt. The rules of information aggregation and emergence of collective intelligent behavior. Topics
in Cognitive Science, 1(4):598–620, 2009.
[11] JonathanBirch,AlexandraKSchnell,andNicolaSClayton.Dimensionsofanimalconsciousness. Trendsincognitive
sciences, 24(10):789–801, 2020.
[12] Terry Bossomaier, Lionel Barnett, Michael Harr´e, and Joseph T Lizier. An Introduction to Transfer Entropy:
Information Flow in Complex Systems. Springer, 2016.
[13] ABoussard,JDelescluse,AP´erez-Escudero,andADussutour. Memoryinceptionandpreservationinslimemoulds:
the quest for a common mechanism. Philosophical transactions of the Royal Society of London. Series B. Biological
sciences, 374(1774):20180368–20180368, 2019.
[14] Markus Brede and Guillermo Romero-Moreno. Sensing enhancement on social networks: The role of network
topology. Entropy, 24(5):738, 2022.
[15] S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with
gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[16] Graham E Budd. Morphospace. Current Biology, 31(19):R1181–R1185, 2021.
[17] JohnOCampbell. Universaldarwinismasaprocessofbayesianinference. Frontiers in systems neuroscience, 10:49,
2016.
13[18] Noam Chomsky. Biolinguistic explorations: Design, development, evolution. International Journal of Philosophical
Studies, 15(1):1–21, 2007.
[19] Andrew D Clark, Dominik Deffner, Kevin Laland, John Odling-Smee, and John Endler. Niche construction affects
the variability and strength of natural selection. The American Naturalist, 195(1):16–30, 2020.
[20] Thomas Claverie and Peter C Wainwright. A morphospace for reef fishes: elongation is the dominant axis of body
shape evolution. PloS one, 9(11):e112732, 2014.
[21] D´aniel Cz´egel, Istv´an Zachar, and E¨ors Szathm´ary. Multilevel selection as bayesian inference, major transitions in
individuality as structure learning. Royal Society open science, 6(8):190202, 2019.
[22] JavierFern´andez-L´opezdePablo,Val´eriaRomano,MaximeDerex,ErikGjesfjeld,ClaudineGravel-Miguel,MarcusJ
Hamilton, Andrea Bamberg Migliano, Felix Riede, and Sergi Lozano. Understanding hunter–gatherer cultural
evolution needs network thinking. Trends in Ecology & Evolution, 37(8):632–636, 2022.
[23] Jill G de Villiers. The role (s) of language in theory of mind. In The neural basis of mentalizing, pages 423–448.
Springer, 2021.
[24] Jill G De Villiers and Jennie E Pyers. Complements to cognition: A longitudinal study of the relationship between
complex syntax and false-belief-understanding. Cognitive development, 17(1):1037–1060, 2002.
[25] Sophie Deneve. Bayesian spiking neurons i: inference. Neural computation, 20(1):91–117, 2008.
[26] Robin IM Dunbar. Coevolution of neocortical size, group size and language in humans. Behavioral and brain
sciences, 16(4):681–694, 1993.
[27] Robin IM Dunbar and Susanne Shultz. Evolution in the social brain. science, 317(5843):1344–1347, 2007.
[28] RobinIMDunbarandMattSpoors. Socialnetworks, supportcliques, andkinship. Human nature, 6:273–290, 1995.
[29] Mark Dyble, James Thompson, Daniel Smith, Gul Deniz Salali, Nikhil Chaudhary, Abigail E Page, Lucio Vinicuis,
RuthMace,andAndreaBambergMigliano. Networksoffoodsharingrevealthefunctionalsignificanceofmultilevel
sociality in two hunter-gatherer groups. Current Biology, 26(15):2017–2021, 2016.
[30] Erle C Ellis. The anthropocene condition: evolving through social–ecological transformations. Philosophical Trans-
actions of the Royal Society B, 379(1893):20220255, 2024.
[31] David Engel, Anita Williams Woolley, Lisa X Jing, Christopher F Chabris, and Thomas W Malone. Reading the
mindintheeyesorreadingbetweenthelines? theoryofmindpredictscollectiveintelligenceequallywellonlineand
face-to-face. PloS one, 9(12):e115212, 2014.
[32] MJeffreyFarrarandLisaMaag. Earlylanguagedevelopmentandtheemergenceofatheoryofmind. Firstlanguage,
22(2):197–213, 2002.
[33] Chris Fields and Michael Levin. Competency in navigating arbitrary spaces as an invariant for analyzing cognition
in diverse embodiments. Entropy, 24(6):819, 2022.
[34] Uta Frith and Chris Frith. The social brain: allowing humans to boldly go where no other species has been.
Philosophical Transactions of the Royal Society B: Biological Sciences, 365(1537):165–176, 2010.
[35] Herbert Gintis. The foundations of behavior: The beliefs, preferences, and constraints model. Biological Theory,
1:123–127, 2006.
[36] HerbertGintis. Aframeworkfortheunificationofthebehavioralsciences. Behavioralandbrainsciences,30(1):1–16,
2007.
[37] Herbert Gintis. Unifying the behavioral sciences ii. Behavioral and brain sciences, 30(1):45–53, 2007.
[38] Antonio J Golubski, Erik E Westlund, John Vandermeer, and Mercedes Pascual. Ecological networks over the edge:
hypergraph Trait-Mediated Indirect Interaction (TMII) structure. Trends in ecology & evolution, 31(5):344–354,
2016.
[39] Miriam No¨el Haidle. Working-memory capacity and the evolution of modern cognitive potential: implications from
animal and early human tool use. Current anthropology, 51(S1):S149–S166, 2010.
14[40] Marcus J Hamilton, Bruce T Milne, Robert S Walker, Oskar Burger, and James H Brown. The complex structure
of hunter–gatherer social networks. Proceedings of the Royal Society B: Biological Sciences, 274(1622):2195–2203,
2007.
[41] Francesca Happ´e. Theory of mind and the self. Annals of the New York Academy of Sciences, 1001(1):134–144,
2003.
[42] MichaelHarr´e. Entropyandtransferentropy: thedowjonesandthebuilduptothe1997asiancrisis. InProceedings
of the International Conference on Social Modeling and Simulation, plus Econophysics Colloquium 2014, pages 15–
25. Springer International Publishing, 2015.
[43] Michael S Harr´e. Multi-agent economics and the emergence of critical markets. arXiv preprint arXiv:1809.01332,
2018.
[44] MichaelSHarr´e. Strategicinformationprocessingfrombehaviouraldatainiteratedgames. Entropy,20(1):27,2018.
[45] Michael S Harr´e. Entropy, economics, and criticality. Entropy, 24(2):210, 2022.
[46] Michael S Harr´e. What can game theory tell us about an AI ‘Theory of Mind’? Games, 13(3):46, 2022.
[47] Michael S. Harr´e and Terrence Bossomaier. Phase-transition–like behaviour of information measures in financial
markets. Europhysics Letters, 87(1):18009, 2009.
[48] MichaelSHarr´eandHusamEl-Tarifi. Testinggametheoryofmindmodelsforartificialintelligence. Games,15(1):1,
2023.
[49] Michael S Harr´e and Mikhail Prokopenko. The social brain: scale-invariant layering of erd˝os–r´enyi networks in
small-scale human societies. Journal of the Royal Society Interface, 13(118):20160044, 2016.
[50] Adam Harris, Scott McCallum, and Michael S Harr´e. On the smooth unfolding of bifurcations in quantal-response
equilibria. Games and Economic Behavior, 2023.
[51] Russell A Hill, R Alexander Bentley, and Robin IM Dunbar. Network scaling reveals consistent fractal pattern in
hierarchical mammalian societies. Biology letters, 4(6):748–751, 2008.
[52] JohnJHopfield. Neuralnetworksandphysicalsystemswithemergentcollectivecomputationalabilities. Proceedings
of the national academy of sciences, 79(8):2554–2558, 1982.
[53] Julian Jara-Ettinger. Theory of mind as inverse reinforcement learning. Current Opinion in Behavioral Sciences,
29:105–110, 2019.
[54] Maxinder S Kanwal, Joshua A Grochow, and Nihat Ay. Comparing information-theoretic measures of complexity
in boltzmann machines. Entropy, 19(7):310, 2017.
[55] Albert B Kao and Iain D Couzin. Modular structure within groups causes information loss but can improve
decision accuracy. Philosophical transactions of the Royal Society of London. Series B. Biological sciences,
374(1774):20180378–20180378, 2019.
[56] TomasKay,AlbaMotes-Rodrigo,ArthurRoyston,ThomasORichardson,NathalieStroeymeyt,andLaurentKeller.
Ant social network structure is highly conserved across species. Proceedings B, 291(2027):20240898, 2024.
[57] Melissa E Kemp, Alexis M Mychajliw, Jenna Wadman, and Amy Goldberg. 7000 years of turnover: historical
contingency and human niche construction shape the caribbean’s anthropocene biota. Proceedings of the Royal
Society B, 287(1927):20200447, 2020.
[58] Yutaka Kobayashi and Norio Yamamura. Evolution of signal emission by uninfested plants to help nearby infested
relatives. Evolutionary Ecology, 21:281–294, 2007.
[59] Christopher Krupenye and Josep Call. Theory of mind in animals: Current and future directions. Wiley Interdisci-
plinary Reviews: Cognitive Science, 10(6):e1503, 2019.
[60] Robert L. Axtell. Economics as distributed computation. In Meeting the Challenge of Social Problems via Agent-
Based Simulation: Post-Proceedings of the Second International Workshop on Agent-Based Approaches in Economic
and Social Complex Systems, pages 3–23. Springer, 2003.
[61] KevinLaland,BlakeMatthews,andMarcusWFeldman. Anintroductiontonicheconstructiontheory. Evolutionary
ecology, 30:191–202, 2016.
15[62] Kevin Laland, Tobias Uller, Marc Feldman, Kim Sterelny, Gerd B Mu¨ller, Armin Moczek, Eva Jablonka, John
Odling-Smee, Gregory A Wray, Hopi E Hoekstra, et al. Does evolutionary theory need a rethink? Nature,
514(7521):161–164, 2014.
[63] J¨org Lang, Jutta Winsemann, Dominik Steinmetz, Ulrich Polom, Lukas Pollok, Utz B¨ohner, Jordi Serangeli, Chris-
tian Brandes, Andrea Hampel, and Stefan Winghart. The pleistocene of sch¨oningen, germany: a complex tunnel
valley fill revealed from 3d subsurface modelling and shear wave seismics. Quaternary Science Reviews, 39:86–105,
2012.
[64] Chris G Langton. Computation at the edge of chaos: Phase transitions and emergent computation. Physica D:
nonlinear phenomena, 42(1-3):12–37, 1990.
[65] TatianaLau,HillardTPouncy,SamuelJGershman,andMinaCikara. Discoveringsocialgroupsvialatentstructure
learning. Journal of Experimental Psychology: General, 147(12):1881, 2018.
[66] Daeyeol Lee, Michelle L Conroy, Benjamin P McGreevy, and Dominic J Barraclough. Reinforcement learning and
decision making in monkeys during a competitive game. Cognitive brain research, 22(1):45–58, 2004.
[67] Michael Levin. The computational boundary of a “self”: developmental bioelectricity drives multicellularity and
scale-free cognition. Frontiers in psychology, 10:2688, 2019.
[68] Michael Levin. Collective intelligence of morphogenesis as a teleonomic process. 2022.
[69] MichaelLevin. Technologicalapproachtomindeverywhere: anexperimentally-groundedframeworkforunderstand-
ing diverse bodies and minds. Frontiers in systems neuroscience, 16:768201, 2022.
[70] Michael Levin. Bioelectric networks: the cognitive glue enabling evolutionary scaling from physiology to mind.
Animal Cognition, 26(6):1865–1891, 2023.
[71] Michael Levin. Darwin’s agential materials: evolutionary implications of multiscale competency in developmental
biology. Cellular and Molecular Life Sciences, 80(6):142, 2023.
[72] Zhi Li and Satish K. Nair. Quorum sensing: How bacteria can coordinate activity and synchronize their response
to external signals? Protein science, 21(10):1403–1417, 2012.
[73] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently
serial problems. arXiv preprint arXiv:2402.12875, 2024.
[74] Joseph T Lizier. The local information dynamics of distributed computation in complex systems. Springer Science
& Business Media, 2012.
[75] Joseph T Lizier. Jidt: An information-theoretic toolkit for studying the dynamics of complex systems. Frontiers in
Robotics and AI, 1:11, 2014.
[76] MarlizeLombardandPeterG¨ardenfors. Causalcognitionandtheoryofmindinevolutionarycognitivearchaeology.
Biological Theory, 18(4):234–252, 2023.
[77] Pedro AM Mediano, Fernando E Rosas, Juan Carlos Farah, Murray Shanahan, Daniel Bor, and Adam B Barrett.
Integrated information as a common signature of dynamical and information-processing complexity. Chaos: An
Interdisciplinary Journal of Nonlinear Science, 32(1), 2022.
[78] PedroAMMediano,AnilKSeth,andAdamBBarrett. Measuringintegratedinformation: Comparisonofcandidate
measures in theory and simulation. Entropy, 21(1):17, 2018.
[79] Andrea Bamberg Migliano and Lucio Vinicius. The origins of human cumulative culture: from the foraging niche
to collective intelligence. Philosophical Transactions of the Royal Society B, 377(1843):20200317, 2022.
[80] Karen Milligan, Janet Wilde Astington, and Lisa Ain Dack. Language and theory of mind: Meta-analysis of the
relation between language ability and false-belief understanding. Child development, 78(2):622–646, 2007.
[81] Philip Mirowski. Markets come to bits: Evolution, computation and markomata in economic science. Journal of
Economic Behavior & Organization, 63(2):209–242, 2007.
[82] PhilipMirowskiandKoyeSomefun. Marketsasevolvingcomputationalentities. JournalofEvolutionaryEconomics,
8(4):329–356, 1998.
16[83] Ida Momennejad. Collective minds: social network topology shapes collective cognition. Philosophical Transactions
of the Royal Society B, 377(1843):20200315, 2022.
[84] Douglas G Moore, Gabriele Valentini, Sara I Walker, and Michael Levin. Inform: efficient information-theoretic
analysis of collective behaviors. Frontiers in Robotics and AI, 5:60, 2018.
[85] Aina Oll´e-Vila, Salva Duran-Nebreda, Nu´ria Conde-Pueyo, Rau´l Montan˜ez, and Ricard Sol´e. A morphospace for
synthetic organs and organoids: the possible and the actual. Integrative Biology, 8(4):485–503, 2016.
[86] Derek C Penn and Daniel J Povinelli. On the lack of evidence that non-human animals possess anything remotely
resemblinga‘theoryofmind’. PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences,362(1480):731–
744, 2007.
[87] JoshuaCPeterson,DavidDBourgin,MayankAgrawal,DanielReichman,andThomasLGriffiths. Usinglarge-scale
experiments and machine learning to discover theories of human decision-making. Science, 372(6547):1209–1214,
2021.
[88] Jordi Pin˜ero and Ricard Sol´e. Statistical physics of liquid brains. Philosophical Transactions of the Royal Society
B, 374(1774):20180376, 2019.
[89] Mikhail Prokopenko, Fabio Boschetti, and Alex J Ryan. An information-theoretic primer on complexity, self-
organization, and emergence. Complexity, 15(1):11–28, 2009.
[90] MikhailProkopenko,PaulCWDavies,MichaelHarr´e,MarcusHeisler,ZdenkaKuncic,GeraintFLewis,OriLivson,
JosephTLizier,andFernandoERosas. Biologicalarrowoftime: Emergenceoftangledinformationhierarchiesand
self-modelling dynamics. arXiv preprint arXiv:2409.12029, 2024.
[91] Thomas O Richardson, Andrea Coti, Nathalie Stroeymeyt, and Laurent Keller. Leadership–not followership–
determines performance in ant teams. Communications biology, 4(1):535, 2021.
[92] Peter Rowley-Conwy and Robert Layton. Foraging and farming as niche construction: stable and unstable adapta-
tions. Philosophical Transactions of the Royal Society B: Biological Sciences, 366(1566):849–862, 2011.
[93] Jaime Ruiz-Serra and Michael S Harr´e. Inverse reinforcement learning as the algorithmic basis for theory of mind:
current methods and open problems. Algorithms, 16(2):68, 2023.
[94] Paul A Ryan, Simon T Powers, and Richard A Watson. Social niche construction and evolutionary transitions in
individuality. Biology & philosophy, 31:59–79, 2016.
[95] Ralf Schm¨alzle, Matthew Brook O’Donnell, Javier O Garcia, Christopher N Cascio, Joseph Bayer, Danielle S
Bassett, Jean M Vettel, and Emily B Falk. Brain connectivity dynamics during social interaction reflect social
network structure. Proceedings of the National Academy of Sciences, 114(20):5153–5158, 2017.
[96] Lu´ıs F Seoane and Ricard Sol´e. The morphospace of language networks. Scientific reports, 8(1):10465, 2018.
[97] Garriy Shteynberg, Jacob B Hirsh, Wouter Wolf, John A Bargh, Erica J Boothby, Andrew M Colman, Gerald
Echterhoff, and Maya Rossignac-Milon. Theory of collective mind. Trends in Cognitive Sciences, 27(11):1019–1031,
2023.
[98] MichaelShum,MaxKleiman-Weiner,MichaelLLittman,andJoshuaBTenenbaum. Theoryofminds: Understand-
ing behavior in groups through inverse planning. In Proceedings of the AAAI conference on artificial intelligence,
volume 33, pages 6163–6170, 2019.
[99] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artificial Intelligence,
299:103535, 2021.
[100] Bruce D Smith and Melinda A Zeder. The onset of the anthropocene. Anthropocene, 4:8–13, 2013.
[101] Ricard Sol´e, Melanie Moses, and Stephanie Forrest. Liquid brains, solid brains, 2019.
[102] Sebastian Stockmaier, Nathalie Stroeymeyt, Eric C Shattuck, Dana M Hawley, Lauren Ancel Meyers, and Daniel I
Bolnick. Infectious diseases and social distancing in nature. Science, 371(6533):eabc8881, 2021.
[103] James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati
Saxena,AlessandroRufo,StefanoPanzeri,GuidoManzi,etal. Testingtheoryofmindinlargelanguagemodelsand
humans. Nature Human Behaviour, pages 1–11, 2024.
17[104] E¨orsSzathm´ary. Towardmajorevolutionarytransitionstheory2.0. ProceedingsoftheNationalAcademyofSciences,
112(33):10104–10111, 2015.
[105] E¨ors Szathm´ary and John Maynard Smith. The major evolutionary transitions. Nature, 374(6519):227–232, 1995.
[106] Richard F. Thompson. Habituation: A history. Neurobiology of learning and memory, 92(2):127–134, 2009.
[107] Giulio Tononi, Melanie Boly, Marcello Massimini, and Christof Koch. Integrated information theory: from con-
sciousness to its physical substrate. Nature reviews neuroscience, 17(7):450–461, 2016.
[108] Rose Trappes, Behzad Nematipour, Marie I Kaiser, Ulrich Krohs, Koen J Van Benthem, Ulrich R Ernst, Ju¨rgen
Gadau,PeterKorsten,JoachimKurtz,HolgerSchielzeth,etal.Howindividualizednichesarise: Definingmechanisms
of niche construction, niche choice, and niche conformance. BioScience, 72(6):538–548, 2022.
[109] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.
[110] RichardWatsonandMichaelLevin. Thecollectiveintelligenceofevolutionanddevelopment. CollectiveIntelligence,
2(2):26339137231168355, 2023.
[111] Richard A. Watson, C. L. Buckley, and Rob Mills. Optimization in “self-modeling” complex adaptive systems.
Complexity, 16(5):17–26, 2011.
[112] RichardA.Watson,RobMills,andC.L.Buckley. GlobalAdaptationinNetworksofSelfishComponents: Emergent
Associative Memory at the System Scale. Artificial Life, 17(3):147–166, July 2011.
[113] Richard A Watson, Rob Mills, CL Buckley, Kostas Kouvaris, Adam Jackson, Simon T Powers, Chris Cox, Simon
Tudge, Adam Davies, Loizos Kounios, et al. Evolutionary connectionism: algorithmic principles underlying the
evolution of biological organisation in evo-devo, evo-eco and evolutionary transitions. Evolutionary biology, 43:553–
581, 2016.
[114] Richard A Watson and E¨ors Szathm´ary. How can evolution learn? Trends in ecology & evolution, 31(2):147–157,
2016.
[115] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682, 2022.
[116] Jianing Wei, Lizhong Wang, Junwei Zhu, Sufang Zhang, Owi I Nandi, and Le Kang. Plants attract parasitic wasps
to defend themselves against insect pests by releasing hexenol. PLOS one, 2(9):e852, 2007.
[117] Michael Wibral, Raul Vicente, and Joseph T Lizier. Directed information measures in neuroscience, volume 724.
Springer, 2014.
[118] David Sloan Wilson, Guru Madhavan, Michele J Gelfand, Steven C Hayes, Paul WB Atkins, and Rita R Colwell.
Multilevel cultural evolution: From new theory to practical applications. Proceedings of the National Academy of
Sciences, 120(16):e2218222120, 2023.
[119] David Sloan Wilson and Dennis J Snower. Rethinking the theoretical foundation of economics i: The multilevel
paradigm. Economics, 18(1):20220070, 2024.
[120] Rachel Wood, Alexander G Liu, Frederick Bowyer, Philip R Wilby, Frances S Dunn, Charlotte G Kenchington,
Jennifer F Hoyal Cuthill, Emily G Mitchell, and Amelia Penny. Integrated records of environmental change and
evolution challenge the cambrian explosion. Nature ecology and evolution, 3(4):528–538, 2019.
[121] Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, and Thomas W Malone. Evidence
for a collective intelligence factor in the performance of human groups. science, 330(6004):686–688, 2010.
[122] Momiao Xiong. Artificial Intelligence and Causal Inference. Chapman and Hall/CRC, 2022.
[123] WakoYoshida,RayJDolan,andKarlJFriston. Gametheoryofmind. PLoScomputationalbiology,4(12):e1000254,
2008.
18A Utility polynomials
In two-player, m-action normal-form games, the payoffs are encoded in a single m×m matrix G, where each cell
contains the payoffs for the i and j players for a particular combination of actions.
x
j
C D
C (gcc,gcc) (gcd,gdc)
x i j i j
i D (gdc,gcd) (gdd,gdd)
i j i j
Table 2: Two-player, two-action normal-form game payoff matrix G
The payoff matrix G can be decomposed into two standard matrices G ,G containing each player’s respective payoffs.
i j
Utilityfunctionscanbeobtainedinpolynomialformfromtheelementsgl ∈G ,whereeachtermintuitivelyrepresentsthe
i i
contribution of each agent’s action (first-order terms) and their interaction (second-order terms) [43]. For m=2 actions,
we have:
U (x;a )=a0+aix +ajx +aijx x (A1)
i i i i i i j i i j
a0  gcc+gcd+gdc+gdd 
i i i i i
a i =  aa ji i  = 41   ( (g gid cdc+ +g gid dd d) )− −( (g gic cc c+ +g gic dd c) ) 

(A2)
i i i i i
aij (gcc+gdd)−(gcd+gdc)
i i i i i
A.1 General form for n players and m = 2 actions
For any number n of players where each player’s action x ∈{−1,1}, the utility function is
i
X Y
U (x;a )= aS x (A3)
i i i j
S⊆{1,2,...,n} j∈S
where S is any subset of the set {1,2,...,n}, and aS is the co-factor corresponding to the subset S. For example,
i
S = {1,2,3} means the product Q x = x x x , and aS = a1,2,3 represents the co-factor for this interaction. To
j∈S j 1 2 3 i i
determine the co-factors aS, we need to consider the payoff matrix G , which contains 2n entries corresponding to each
i i
combination of n players’ actions. Denote the entry in G for the action combination (x ,x ,...,x ) as gx1x2...xn. The
i 1 2 n i
co-factors aS are:
i 1
X Y
aS = gx1x2...xn x (A4)
i 2n i j
(x1,x2,...,xn)∈{−1,1}n j∈S
This generalization for the utility function for n-player, two-action games can be interpreted through Fourier analysis
on the Boolean cube. This connection arises because we represent the utility function as a sum of basis functions over the
Boolean domain, where the co-factors aS are Fourier co-factors and Q x are parity functions. Thus, we express the
i j∈S j
utilityfunctionU asaFouriertransformofthepayoffmatrixG . Eachco-factoraS capturestheinfluenceofasubsetS of
i i i
players on the overall utility, analogous to how Fourier co-factors capture the influence of different frequency components
inasignal. ThisgeneralizationleveragestheprinciplesofFourieranalysisontheBooleancubetodecomposethecomplex
payoff interactions into simpler, orthogonal components, making it easier to analyze and interpret the contributions of
different action combinations in the game.
19B Derivation of utility polynomials in example
From the provided payoff matrix,
C D
C (1,1) (0−c,1+c)
D (1+c,0−c) (0,0)
we can obtain the polynomial co-factors (see Table 2 and Equation A2 for details)
a0  gcc+gcd+gdc+gdd 
i i i i i
a i =  aa ji i  = 1 4  (( gg i cd dc ++ gg i dd dd )) −− (( gg i cc cc ++ gg i dcd c))  

(B1)
i i i i i
aij (gcc+gdd)−(gcd+gdc)
i i i i i
 1+(0−c)+(1+c)+0   1+1 
=
1 ((1+c)+0)−(1+(0−c))
=
1

1+c−1+c
 (B2)
4 ((0−c)+0)−(1+(1+c))

4 −c−1−1−c

(1+0)−((0−c)+(1+c)) 1+c−1−c
 2   1  a0
i
= 1  2c = 1  c =ai i (B3)
4 −2(1+c)

2 −(1+c)

 aj

i
0 0 aij
i
and substitute into the (second-degree) utility polynomial (with zero quadratic term in this particular case)
1(cid:16) (cid:17)
U (x;a)=a0+aix +ajx +aijx x = 1+cx −(1+c)x (B4)
i i i i i j i i j 2 i j
we can choose to re-scale the utility values by a factor of 2, which results in the following utility polynomials for A and
2
A , respectively
3
U (x;a)=1+cx −(1+c)x , U (x;a)=1+cx −(1+c)x . (B5)
2 2 3 3 3 2
20