Sharp Matrix Empirical Bernstein Inequalities
Hongjian Wang1 and Aaditya Ramdas2
1,2Department of Statistics and Data Science, Carnegie Mellon University
2Machine Learning Department, Carnegie Mellon University
hjnwang,aramdas @cmu.edu
{ }
November 15, 2024
Abstract
We present two sharp empirical Bernstein inequalities for symmetric random matrices
with bounded eigenvalues. By sharp,we meanthat both inequalities adaptto the unknown
variance in a tight manner: the deviation captured by the first-order 1/√n term asymp-
totically matches the matrix Bernstein inequality exactly, including constants, the latter
requiringknowledgeofthe variance. Ourfirstinequality holds for the sample meanofinde-
pendent matrices, and our second inequality holds for a mean estimator under martingale
dependence at stopping times.
1 Introduction
We study the nonasymptotic estimation of the common mean of independent or martingale
dependentboundedrandommatricesinawaythatoptimallyadaptstotheunknownunderlying
variance. We first discuss the scalar case to set some context. The familiar reader can skip
directly to Section 1.2, whereour main results are crisply presented as Propositions 1.1 and 1.2.
1.1 Scalar empirical Bernstein inequalities
The classical Bennett-Bernstein inequality (see Lemma 5 of Audibert et al. [2009]; also Ap-
pendix A.2) states that, for the average X of independent random scalars X ,...,X with
n 1 n
common expected value µ = EX , common almost sure upper bound X 6 B, and second
i i
moment upper bound n EX2 6 nσ2, | |
i=1 i
P
Blog(1/α) 2σ2log(1/α)
P X µ > + 6 α. (1)
n
− 3n n
r !
It is clear that (1) remains true if the assumptions are centered instead: X µ 6 B and
i
n Var(X ) 6 nσ2. A crucial feature of (1) is that if σ2 EX2 . | B2,− the| deviation
i=1 i ≈ 1
is dominated by the “variance term” Θ n 1σ2log(1/α) , tighter than the “boundedness
P −
term” Θ( n −1B2log(1/α)) that domina(cid:16)teps if Hoeffding’s in(cid:17)equality [1963] is applied instead
in the absence of the variance bound σ2.
p
In practice, whereas an almost sure upper bound B of the random variables is often acces-
sible, an explicit variance bound v is rarely known. Thus, such bounds are usually only used
in theoretical analysis, but not to practically construct confidence bounds for the mean. For
the latter task, so-called nonasymptotic empirical Bernstein (EB) inequalities are therefore of
particular interest. These inequalities often only assume the almost sure upper bound B of the
1
4202
voN
41
]RP.htam[
1v61590.1142:viXrarandom variables and are agnostic and adaptive to the true variances Var(X ), to the effect
i
that the final deviation is still dominated by an asymptotically Θ n 1σ2log(1/α) variance
−
term, instead of Θ( n −1B2log(1/α)) from Hoeffding’s inequality(cid:16)upnder the same bo(cid:17)undedness
assumption.
p
Scalar empirical Bernstein inequalities are derived from two very different types of tech-
niques. First, a union bound between a non-empirical (“oracle”) Bernstein inequality and a
concentration inequality onthesamplevariance, whichis employed byearlyempiricalBernstein
results [Audibert et al., 2009, Maurer and Pontil, 2009]. For example, for i.i.d., [0,1]-bounded
X ,...,X , and their Bessel-corrected sample variance σˆ2, Maurer and Pontil [2009, Theorem
1 n n
4] prove the EB inequality
2σˆ2log(2/α) 7log(2/α)
P X µ > n + 6 α. (2)
n
− n 3(n 1)
r !
−
Second, the self-normalization martingale techniques of Howard et al. [2021, Theorem 4] and
Waudby-Smith and Ramdas [2023], which enable sharper rates, stopping time-valid concentra-
tion,martingaledependence,andvarianceproxybypredictableestimates otherthanthesample
variance. For example, Waudby-Smith and Ramdas [2023, Theorem 2, Remark 1]prove the fol-
lowing EB inequality for [0,1]-bounded random variables X ,...,X with common conditional
1 n
mean µ = E[X X ,...,X ]:
i 1 i 1
| −
2log(1/α)V
P µˆ µ > n,α 6 α. (3)
n
− n
r !
Above, µˆ is a particular weighted average of X ,...,X , and V = V(α,X ,...,X ) a
n 1 n n,α 1 n
particular variance estimator. If the observations are i.i.d. with variance σ2,
lim V = σ2, almost surely. (4)
n,α
n
→∞
These exact terms will become clear when we present our matrix result later in Section 4
(takingd= 1),butonecanalreadynoticetheimportantfactthat(3)matches(1)asymptotically
without requiring a known variance bound.
These two methods are inherently different and as argued convincingly by Howard et al.
[2021, Appendix A.8]: the latter’s avoidance of the union bound produces a better concen-
tration. Indeed, (3) is what we call a sharp EB inequality: its first order term, including
constants, asymptotically matches the oracle Bernstein inequality which requires knowledge of
σ2. Waudby-Smith and Ramdas [2023] were the first ones to prove that their EB inequality is
sharp,pointingout thattheunion bound-basedinequalities arenot sharp(butonly slightly so).
We discuss this issue in Appendix B. Other EB inequalities have been proved in the literature
in between the above sets of papers, but they are even looser than the original ones, so we omit
them.
1.2 Matrix empirical Bernstein inequalities
Exponential concentration inequalities for the sum of independentmatrices are in general much
harder to obtain. Tropp [2012, Theorem 6.1] proved a series of Bennett-Bernstein inequalities
for the average X of independent d d symmetric matrices X ,...,X with common mean
n 1 n
EX = M,commoneigenvalueupperb× oundλ (X ) 6 B,and n EX2 nV. Forexample,
i max i i=1 i (cid:22)
the Bennett-type result implies the following ( being the spectral norm),
k·k P
Blog(d/α) 2log(d/α) V
P λ X M > + k k 6 α. (5)
max n
− 3n n
r !
(cid:0) (cid:1)
2The analogy between (1) and (5) is straightforward to notice, including matching constants.
See Appendix A.2 for some remarks on these two non-empirical Bernstein results and a proof of
(5). We shall explore some of the techniques by Tropp [2012] later when developing our results.
To the bestof our knowledge, noexplicit matrix empirical Bernstein inequalities exist in the
literature. ThemaincontributionofthecurrentpaperistwoempiricalBernsteininequalities for
matrices derived via the two methods in the scalar case mentioned earlier. First, we generalize
the union bound and plug-in techniques by Audibert et al. [2009], Maurer and Pontil [2009] to
matrices and obtain:
Proposition 1.1 (Theorem 3.1 of this paper, shortened). Let X ,...,X be i.i.d. symmetric
1 n
matrices with eigenvalues in [0,1], mean M and sample variance V . Then,
n
P λ
max
X
n
−M > s2 kV n klo ng (n −nd 1)α +
O max
nlog V(b n nd/ 1/α 2)
,n3/4 !
6 α. (6)
b { k k }
(cid:0) (cid:1)
 
Second, we provide a faithful generalization of (3) to the mabtrix case which we informally
state as follows.
Proposition 1.2 (Corollary 4.3 of this paper, shortened). Let X ,...,X be symmetric ma-
1 n
trices with eigenvalues in [0,1] and common conditional mean M = E[X X ,...,X ]. For
i 1 i 1
an appropriate weighted average Mˆ of X ,...,X and an appropriate sam| ple varian− ce proxy
n 1 n
v = v(α,X ,...,X ) > 0,
n,α 1 n
2log(d/α)v
P λ (Mˆ M) > n,α 6 α. (7)
max n
− n
r !
Further, for i.i.d. X with variance matrix V,
i
{ }
lim v = V , almost surely. (8)
n,α
n k k
→∞
Again, the detailed description of these weighted average and variance proxy terms will be
furnished in Section 4. From the statements above, it can also be seen that both (6) and (7)
match (5) asymptotically without requiring knowing a bound on the largest eigenvalue of the
variance, with deviation bounds D (the right hand sides of the inequalities) attaining the very
n
same limit
√nD 2log(d/α) V (9)
n
→ k k
as (5). They are thus both sharp matrix emppirical Bernstein inequalities. It is also worth re-
marking that both (3) and our matrix generalization (7) are special fixed-time cases of some
time-uniform concentration inequalities that control the tails of all µˆ or Mˆ simul-
n n>1 n n>1
{ } { }
taneously, enabling application in sequential statistics. This will become clear as we develop
our results.
Besides the work cited above, some other authors have also contributed to the literature
of Bernstein or empirical Bernstein inequalities for random elements. Chugg et al. [2023], for
example, applythePAC-Bayes techniquetotheaforementioned self-normalization methodfrom
Howard et al. [2021] to obtain an empirical Bernstein inequality for bounded random vectors.
Martinez-Taboada and Ramdas [2024] used different techniques to derive a sharp empirical
Bernstein inequality in smooth Banach spaces. Neither implies a satisfactory matrix bound. In
the other direction, Howard et al. [2021] also provide a time-uniform recipe for non-empirical
matrix Bernstein inequality; and Minsker [2017] proves a dimension-free alternative to (5),
replacing d with the smaller “effective rank” tr(V)/ V but incurring a larger constant. Other
k k
matrix Bernstein results in the literature include the one by Mackey et al. [2014]. These will be
discussed more in Section 5.
32 Preliminaries
We adopt the following notational convention throughout the rest of the paper. Let denote
d
S
theset of all d d real-valued symmetric matrices, which is theonly class of matrices considered
×
in this paper. These matrices are denoted by bold upper-case letters A,B, etc. For I R, we
⊆
denote by I the set of all real symmetric matrices whose eigenvalues are all in I. [0, ∞) , the
Sd Sd
(0, )
set of positive semidefinite and ∞ , the set of positive definite matrices are simply denoted
Sd
by + and ++ respectively. The Loewner partial order is denoted , where A B means
Sd Sd (cid:22) (cid:22)
B A is positive semidefinite, and A B means B A is positive definite. We use λ
max
− ≺ −
to denote the largest eigenvalue of a matrix in , and its spectral norm, i.e., the largest
d
S k·k
absolute value of eigenvalues.
As is standard in matrix analysis, a scalar-to-scalar function f :I J is identified canoni-
→
cally with a matrix-to-matrix function f : I J, following the definition
Sd → Sd
T T
f :U diag[λ ,...,λ ]U U diag[f(λ ),...,f(λ )]U. (10)
1 d 1 d
7→
Matrix powers Xk, logarithm logX, and exponential expX are common examples. It is worth
noting that the monotonicity of f : I J is usually not preserved when lifted to f : I J
→ Sd → Sd
in the order. The matrix logarithm, however, is monotone. On the other hand, for any
(cid:22)
monotone f :I J, the function tr f : I R is always monotone.
→ ◦ Sd →
We work on a filtered probability space (Ω, ,P) where := is a filtration, and we
n n>1
F F {F }
assume := ∅,Ω . We say a process X := X is adapted if X is -measurable for all
0 n n n
F { } { } F
integers n > 0 or sometimes n >1; predictable if X is -measurable for all integers n > 1.
n n 1
F −
2.1 Nonnegative Supermartingales and Ville’s Inequality
Many of the classical concentration inequalities for both scalars and matrices are derived via
Markov’s inequality. Howard et al. [2020], pioneered using Ville’s inequality for nonngative su-
permartingales to construct time-uniform concentration inequalities. An adapted scalar-valued
process X iscalled anonnegative supermartingaleifX > 0andE[X ] 6 X forall
n n>0 n n+1 n n
{ } |F
n > 0 (all such inequalities are intended in the P-almost sure sense). Let us state the following
two well-known forms of Ville’s inequality, both generalizing Markov’s inequality.
Lemma 2.1 (Ville’s inequality). Let X be a nonnegative supermartingale and Y be an
n n
{ } { }
adapted process such that Y 6 X for all n. For any α (0,1],
n n
∈
P supY > X /α 6 α. (11)
n 0
(cid:18)n>0
(cid:19)
Equivalently, for any stopping time τ,
P(Y > X /α) 6 α. (12)
τ 0
2.2 Matrix MGFs and Lieb’s Inequality
The Chernoff-Cram´er MGF method cannot be directly applied to the sum of independent
random matrices due to exp(A+B) = (expA)(expB) in general. Tropp [2012] introduced the
6
method of controlling the trace of the matrix CGF via an inequality due to Lieb [1973]. The
Lieb-Troppmethod islater furtheredbyHoward et al.[2020] inturntoconstruct anonnegative
supermartingale for matrix martingale differences. We slightly generalize it as follows.
Lemma 2.2 (Lemma 4 in Howard et al. [2020], rephrased and generalized). Let Z be an
n
{ }
-valued, adapted martingale difference sequence. Let C be an -valued adapted process,
d n d
S { } S
C be an -valued predictable process. If
{
′n} Sd
E(exp(Z C ) ) exp(C ), (13)
n
−
n |Fn −1
(cid:22)
′n
4holds for all n, then the process
n n
L = trexp Z (C +C ) (14)
n i
−
i ′i
!
i=1 i=1
X X
is a nonnegative supermartingale. Further,
n n
L >exp λ Z λ (C +C ) . (15)
n max i
−
max i ′i
! !!
i=1 i=1
X X
We remark that in the supermartingale (14), since the empty sum is the zero matrix, L =
0
trexp0 = trI = d. Thiswilltranslate into thelog(d)-type dimensiondependenceinour bounds.
The above lemma is proved in Appendix A.4.
3 Matrix EB Inequality under Independence
The scalar EB inequality (2) by Maurer and Pontil [2009, Theorem 4] is derived via a union
bound between the non-empirical Bennett-Bernstein inequality (1) and a suitable tail bound
on the sample variance. For matrices, we recall that the Bessel-corrected sample variance for
X ,...,X is
1 n
1
V = (X X )2, (16)
n i j
n(n 1) −
− 16i<j6n
X
b
which,asinthescalarcase,isanunbiasedestimatorfortheircommonvarianceifX ,...,X are
1 n
independent and have common mean and variance. We have the following Maurer-Pontil-style
matrix EB inequality.
[0,1]
Theorem 3.1 (First matrix empirical Bernstein inequality). Let X ,...,X be -valued
1 n Sd
independent random matrices with common mean M and variance V. We denote by X their
n
sample average and V the Bessel-corrected sample variance. Then, for any α (0,1),
n
∈
b P λ X M > Dmeb1 6 α, (17)
max n − n
(cid:16) (cid:17)
(cid:0) (cid:1)
where
Dmeb1 =
2log (n−nd
1)α V 1/2+
2log(2d/α) 2log(2d/α) 1/4
+
log (n−nd
1)α (18)
n s n k n k s n kV n k ∧ (cid:18) n (cid:19) ! 3n
b
Further, if X ,...,X are i.i.d.,
1 n b
meb1
lim √nD = 2log(d/α) V ), almost surely. (19)
n n k k
→∞
p
Proof. By the matrix Bennett-Bernstein inequality (5),
log(d/α) 2log(d/α) V
P λ X M > + k k 6 α. (20)
max n
− 3n n
r !
(cid:0) (cid:1)
[0,1]
Next, we can see that both V and V belong to as well, on which we can employ the
n Sd
matrix Efron-Stein method by Paulin et al. [2016]. Let
Vj
be the sample variance by replacing
n
X with an i.i.d. copy X . Thbe Efron-Stein variance proxy of V satisfies
j ′j n
b
n
1 b1
E[(V Vj)2 X ,...,X ] I, (21)
2 n − n | 1 n (cid:22) 2n
j=1
X
b b
5which can be noted from the fact that each V
n
−V nj
∈
Sd[ −1/n,1/n] . We now invoke the self-
bounding Efron-Stein tail bound, Corollary 5.1 from Paulin et al. [2016] to see that
b b
nt2
P( V V > t)6 P( V V > t)6 2dexp − . (22)
n n
|k k−k k| k − k 2
(cid:18) (cid:19)
Setting the right hand side tbo α, we obtain, with pbrobability at least 1 α,
−
2log(2d/α)
V V < , (23)
n
|k k−k k| n
r
which, due to Lemma A.1, implies that b
1/4
2log(2d/α) 2log(2d/α)
V 1/2 < V 1/2+ . (24)
n
k k k k s n V n ∧ (cid:18) n (cid:19)
k k
b
A union bound with (20) via α= α(n 1)/n+α/n concludes the proof of the bound. The
b
−
asymptotics (19) follows simply from the strong consistency of the sample variance and the
continuity of the spectral norm.
Thefirstordertermofthedeviationradius(18)matchestheoraclematrixBernsteininequal-
ity (5), both being the Θ n 1 V log(d/α) variance term. More importantly, the match is
−
k k
meb1
precise asymptotically, as(cid:16)ispindicated by the li(cid:17)mit (19) of √nD
n
. Indeed, this owes much to
the imbalanced α = α(n 1)/n+α/n split in the union bound in the proof. If a balanced, or
−
more generally n-independent split was employed, the limit would become 2log(Cd/α) V )
k k
for some constant C > 1 instead. A balanced split, however, is exactly what Maurer and Pontil
p
[2009] do in their scalar EB inequality, leading to the intralogarithmic factor 2 as shown in (2).
This too can be avoided by switching to the α = α(n 1)/n+α/n split instead, which we write
−
down formally in Appendix B.
We further remark that a (n 1 V 1/2 n 3/4) dependence exists in the boundedness
− − −
O k k ∧
term. In terms of convergence (i.e., large sample behavior as n ), this is faster than the
→ ∞
(n 1/2) boundedness term with Hoeffding, and the same compared to the (n 1) rate as in
− −
O O
the scalar EB (2). However, in the small sample regime, since Vˆ can be arbitrarily small,
n
k k
(n 3/4) dominates instead and this may be worse than the (n 1) scalar EB (2) rate. This is
− −
O O
largely due to the technique we use: our Efron-Stein argument leads to a slower convergence of
V 1/2 to V 1/2,comparedtotheonefromtheself-boundingtechniqueofMaurer and Pontil
n
k k k k
[2009]. If there is a matrix self-bounding concentration inequality available that leads to
b
nt2
P( V V > t)6 (d) exp − , (25)
n
k − k O · (λ (V))
(cid:18)O max (cid:19)
then we may eliminate the extrabfactor. We leave this to future work.
4 Matrix EB Inequality via Self-Normalized Martingales
Letus,followingHoward et al.[2020,2021],Waudby-Smith and Ramdas[2023],definethefunc-
tion ψ : [0,1) [0, ) as ψ (γ) = log(1 γ) γ. The symbol ψ is from the fact that it is
E E E
→ ∞ − − −
the cumulant generating function (CGF) of a centered standard exponential distribution. The
following lemma is a matrix generalization of Howard et al. [2021, Appendix A.8].
Lemma 4.1. Let X be an adapted sequence of -valued random matrices with conditional
n d
{ } S
means E(X ) = M . Further, suppose there is a predictable and integrable sequence of
n n 1 n
|F −
-valued random matrices X such that λ (X X )> 1. Let
d n min n n
S { } − −
E = exp(γ (X X ) ψ (γ )(X X )2), F = exp(γ (M X )), (26)
n n n b n E n n n b n n n n
− − − −
b b b
6where γ are predictable (0,1)-valued scalars. Then,
n
{ }
E(E ) F . (27)
n n 1 n
|F − (cid:22)
Proof. Recall that ψ (γ) = log(1 γ) γ. An inequality by Fan et al. [2015] quoted by
E
− − −
Howard et al. [2021, Appendix A.8] states that, for all 0 6 γ < 1 and ξ > 1,
−
exp(γξ ψ (γ)ξ2)6 1+γξ. (28)
E
−
[ 1, )
Since X n −X n
∈
Sd− ∞ , we can apply the transfer rule (Lemma A.3), replacing the scalar ξ
above by the matrix X X , and plugging in γ = γ (0,1),
n n n
− ∈
b
exp(γ n(X nb X n) ψ E(γ n)(X n X n)2) 1+γ n(X n X n). (29)
− − − (cid:22) −
Lemma A.4 then guarantees thbe integrability of thebleft hand side, and thbat
E exp(γ (X X ) ψ (γ )(X X )2) E 1+γ (X X ) (30)
n n n E n n n n 1 n n n n 1
− − − F − (cid:22) − F −
(cid:16) (cid:12) (cid:17) (cid:16) (cid:12) (cid:17)
=1+γ n(M n −X n) b(cid:22) exp(γ n(M n −X nb)), (cid:12)
(cid:12)
b (cid:12)
(cid:12)
(31)
where in the final sbtep we use the transfber rule again with 1+x 6 exp(x) for all x R. This
∈
concludes the proof.
We are now ready to state in full our matrix empirical Bernstein inequality based on the
self-normalization technique. The following theorem is stated as a combination of three tools: a
nonnegative supermartingale, a time-uniform concentration inequality, and an equivalent con-
centration inequality at a stopping time.
Theorem 4.2 (Time-uniform and stopped matrix empirical Bernstein inequalities). Let X
n
{ }
be an adapted sequence of -valued random matrices with conditional means E(X ) =
d n n 1
S |F −
M . Let X be a sequence of predictable and integrable -valued random matrices such that
n n d
{ } S
λ (X X ) > 1 almost surely. Then, for any predictable (0,1)-valued sequence γ ,
min n n n
− − { }
b
n n
b Lmeb2 = trexp γ (X M ) ψ (γ )(X X )2 (32)
n i i − i − E i i − i
!
i=1 i=1
X X
b
isa supermartingale. Denote byXγ
n
the weighted average γ1X γ1 1+ +···+ +γ γn nX n w.r.t. the positive weight
sequence γ . Then, for any α (0,1), ···
n
{ } ∈
log(d/α)+λ n ψ (γ )(X X )2
P there exists n > 1, λ Xγ Mγ > max i=1 E i i − i 6α;
 max n − n γ 1(cid:16)+P +γ
n
(cid:17)
··· b
(cid:0) (cid:1)
  (33)
and for any stopping time τ, α (0,1),
∈
log(d/α)+λ τ ψ (γ )(X X )2
P λ Xγ Mγ > max i=1 E i i − i 6 α. (34)
 max τ − τ γ 1(cid:16)+P +γ
τ
(cid:17)
··· b
(cid:0) (cid:1)
 
Proof. Due to Lemma 4.1, we can apply Lemma 2.2 with Z = γ (X M ), C = γ (X
n n n n n n n
− −
M )+ψ (γ )(X X )2, and C = γ (M X ) to see that
n E n n
−
n ′n n n
−
n
b
n n
Lmebb2
= trexp γ (X
Mb
) ψ (γ )(X X )2 (35)
n i i − i − E i i − i
!
i=1 i=1
X X
b
7is a supermartingale, which upper bounds
n n
exp λ γ (X M ) λ ψ (γ )(X X )2 . (36)
max i i i max E i i i
− − −
( ! !)
i=1 i=1
X X
b
Applying Lemma 2.1 to (36), the desired result follows from rearranging.
Before we remark on the uncompromised Theorem 4.2, let us firstwrite down its fixed-time,
fine-tuned special case of (34) with τ = n which shall justify the “empirical Bernstein” name it
bears.
Corollary4.3(SecondmatrixempiricalBernsteininequality). Suppose α (0,1). LetX ,...,X
1 n
∈
be [0,1] -valued i.i.d. random matrices with mean M = EX and variance V = Var(X ). Let
Sd 1 1
X = 1(X + +X ) and X = 0. Define the following variance proxies
i i 1 ··· i 0
k
1 5log(d/α)
V = 0, V = (X X )2, v = λ (V ) , (37)
0 k i k k max k
k − ∨ n
i=1
X
n 2
1 (X X )
s
n
=λ
max
i − i −1 , (38)
n v
i 1 !
Xi=1 −
e
2log(d/α)
and set γ = for i = 1,...,n. Then,
i nvi−1
q
log(d/α)+λ n ψ (γ )(X X )2
P λ
max
Xγ
n
−M > D nmeb2 6 α, whereD nmeb2 = max
γ
+i=1 +E
γ
i i − i −1 .
(cid:16) (cid:0) (cid:1) (cid:17) (cid:0)1P ··· n (39) (cid:1)
Further, nonasymptotically,
log(d/α) 1+2s 9log(d/α)λ (V)
meb2 n max
D 6 ; (40)
n
r
2n 1 n v−1/2 ≈
r
2n
n i=1 i 1
e−
and asymptotically, P
meb2
lim √nD = 2log(d/α) V ) almost surely. (41)
n n k k
→∞
p
meb2
The asymptotic behavior (41) of deviation bound D is satisfying as it adapts fully to,
n
without knowing, the true variance V. In particular, if the assumption on the known spectral
[a,b] [0,1]
bound is X ,...,X as opposed to the stated in Corollary 4.3, one can apply the
result to
X11 a,...,n X∈
n
S ad
to obtain the same
Sd
b −a b −a
− −
log(d/α) V )
Θ k k (42)
n
r !
asymptotic deviation which is free of b a.
−
The three kinds of result states in Theorem 4.2 are for potentially different purposes. The
supermartingale (32) is best as a sequential test for the null
H
0
: E(X
n n
1) = Mnull for all n (43)
|F −
by setting each M
i
to Mnull. The time-uniform concentration inequality (33) can be used to
constructa“confidencesequence”onthecommonconditionalmeanM = E(X );thatis,a
n n 1
sequenceofconfidenceballsB = M : Xγ M 6 ρ suchthatP(M |F − B ) >1 α,
n { ′ ∈Sd k n− ′ k n } ∈∩n n −
leading to the stopped concentration inequality (34) which is a valid confidence ball at a fixed
8stoppingtimeB . We alsoremarkthatitispossibletosharpentheconfidenceballB atafixed
τ τ
stoppingtimebyanapriori randomization,duetoarecentresultbyRamdas and Manole[2024,
Theorem 4.1] called “uniformly randomized Ville’s inequality”. That is, letting U Unif
(0,1)
∼
independent from the filtration , one may replace the log(d/α) term in (34) with the strictly
F
smaller log(Ud/α).
The -measurable term X in Theorem 4.2 is best understood as a “plug-in prediction”
i 1 i
F−
of the next observation X . Indeed, whereas the inequality holds under all choices of X , the
i i
smaller the “prediction error” b(X X )2, the tighter the bound. Thus one may set X to
i i i
−
be the sample average from X to X , which is exactly what is done in Corollary 4b.3. On
1 i 1
−
the other hand, if the sample sizbe n is not fixed in advance and an infinite sequence ofbi.i.d.
(or homoscedastic more generally) observations X ,X ,..., to construct a tight time-uniform
1 2
concentration bound or powerful sequential test, we recommend setting the weights γ to be
n
{ }
a vanishing sequence such that each γ matches the fixed-time near-optimal choice of γ with
n n
2log(d/α)
samplesize n, e.g. one may take γ = . Underthis weight sequence, we seethe choice
n nvn−1
of aweighted average X = XψE(γ) is mq orereasonableas itroughlyminimizes theweighted sum
n n 1
of squares τ ψ (γ )(X X− )2 in (34) into a weighted sample variance. Of course, as long
i=1 E i i − i
as X is any average,b weighted or not, of X ,...,X , the condition λ (X X ) > 1 is
n P
b [0,1]
1 n −1 min n − n −
met when X all take values in .
{ n } Sd
bFinally, as a reprise of the shortened version Proposition 1.2 stated in the obpening, the
“approprioate variance proxy” v = v(α,X ,...,X ) is simply
n,α 1 n
log(d/α)+λ n ψ (γ )(X X )2 2 n
v
n,α
= max i=1 E i i − i −1 (44)
γ + +γ 2log(d/α)
(cid:0)1P
···
n (cid:1)!
which converges almost surely to V under i.i.d. observations due to (41).
k k
5 Comparison to Existing Results
5.1 Self-Normalized EB Inequalities for Scalars and Vectors
OurTheorem4.2andCorollary4.3owemuchtothetechniquesdevelopedbyWaudby-Smith and Ramdas
[2023, Theorem 2 and Remark 1] in the scalar case (who in turn build on the earlier result by
Howard et al. [2021, Theorem 4] via the “predictable mixing” sequence γ ). In particular,
n
{ }
when d = 1, our statements match (including constants) exactly the scalar empirical Bern-
stein inequality counterparts by Waudby-Smith and Ramdas [2023]: Our supermartingale (32)
coincides with Equation (13) in Waudby-Smith and Ramdas [2023]; our time-uniform concen-
tration bound (33) becomes identical to Theorem 2 in Waudby-Smith and Ramdas [2023]; and
our fixed-time asymptotics (41) recovers Equation (17) in Waudby-Smith and Ramdas [2023].
As can be expected, applying a vector bound to matrices (by flattening) will lead to a
very suboptimal result. The self-normalized empirical Bernstein inequality for vectors due to
Chugg et al. [2023, Corollary 5] implies the following for matrices whose Frobenius norm is
bounded by 1/2, for all α 6 0.1,
log(1/α)σ˜2
P Mˆ M > 3.25 n 6 α. (45)
n F
k − k n
r !
Here, σ˜2 converges almost surely to the vectorized variance E X EM 2 with i.i.d. matrices.
n k 1 − kF
Since everything (assumption and result) is in the Frobenius norm, however, translating the
result into the spectral norm will incur a dimensional dependence polynomial in d.
Finally, we note that the self-normalized empirical Bernstein inequality for Banach spaces
dueto Martinez-Taboada and Ramdas [2024] is not applicable as equippedwith the spectral
d
S
norm is not a 2-smooth Banach space.
95.2 Non-Empirical Matrix Bernstein and Hoeffding Inequalities
As we state in the opening (5) and elaborate further in Appendix A.2, Tropp [2012, The-
orem 1.4] proves the following matrix Bennett-Bernstein inequality under the assumptions
max λ (X ) 6 1 and n EX2 nV:
16i6n max i i=1 i (cid:22)
P
Blog(d/α) 2log(d/α) V
P λ X EX > Dtb 6 α, Dtb = + k k. (46)
max n − n n n 3n n
r
(cid:16) (cid:17)
(cid:0) (cid:1)
tb
We can see that with i.i.d. matrices with variance V, √nD converges to 2log(d/α) V
meb1 meb2 n k k
which is the same limit that both √nD and √nD converge to, stated as (19) and (41).
n n p
Therefore, our empirical Bernstein inequalities provide a confidence region fully adaptive to the
unknown variance V and match in asymptotics this oracle Bernstein result which requires V to
be known. Both are thus sharp EB inequalities. Assumption-wise, it is important to note that
[0,1]
it is fair to compare our X assumption to their λ (X ) 6 1 assumption; no constant
i ∈ Sd max i
is glossed over in making this comparison when and two-sided bound is sought. To see this,
the bound by Tropp [2012, Theorem 1.4] can be applied to X M,...,X M, and it takes
1 n
− −
[0,1]
X to ensure both λ (X M) 6 1 and λ ( X +M)6 1 hold.
1 ∈ Sd max 1 − max − 1
Mackey et al. [2014, Corollary 5.2] also obtain a matrix Bernstein inequality. However, as
they acknowledge in the paper, their bound is strictly looser than the bound by Tropp [2012,
Theorem 1.4]. The bound by Minsker [2017, Theorem 3.1] under the same assumption reads
Blog(d′ /α)+ B2log2(d′/α)+18nlog(d′/α) V
P λ X EX >Dmb 6α, Dmb = k k, (47)
max n − n n n q 3n
(cid:0) (cid:0) (cid:1) (cid:1)
where d = 14tr(V)/ V , which decides the dimension-free virtue of their result. This can be
′
k k
tighter than Tropp [2012, Theorem 1.4] only if the largest eigenvalue is at least 14 times greater
than the average eigenvalue of V. It remains an interesting open direction for future work
whether anytime-valid and/or empirical Bernstein inequalities in a similar flavor for matrices
can be derived.
Finally, we quote the tightest known Hoeffding-type inequalities for matrices in the lit-
erature. Mackey et al. [2014, Corollary 4.2] shows that if independent X ,...,X satisfy
1 n
(X EX )2 B almost surely, then
i i
− (cid:22)
2λ (B)log(d/α)
P λ (X EX ) > max 6 α. (48)
max n n
− n
r !
A time-uniform extension can be achieved by applying Lemma 3(h) in Howard et al. [2020],
but its fixed-time corollary remains identical as (48). The squared boundedness assumption
(X
i
−EX i)2
(cid:22)
B implies X
i
−EX
i
∈
Sd[ −kB k1/2, kB k1/2] , so it is a stronger assumption than the
boundedness assumption we make in Corollary 4.3. Further, since (X EX )2 B implies
i i
− (cid:22)
Var(X ) B and in practice this gap can be arbitrarily large, we see that our empirical
i
(cid:22)
Bernstein inequality is asymptotically tighter and the worst that can happen is a degradation
to this, already tightest, matrix Hoeffding bound, when λ (Var(X )) λ (B).
max i max
≈
6 Summary
Weprovidetwonewmatrixconcentrationinequalitiesinthispaper. Thefirstoneisbasedonthe
union bound method, and characterizes, in terms of the sample variance, the concentration of
thesamplemean ofindependentsymmetricmatrices withboundedlargesteigenvalues, common
mean, and common variance. The second one is a self-normalized, time-uniform concentration
inequality for the weighted sum of martingale difference symmetric matrices with bounded
10largest eigenvalues, which when weighted properly, becomes an empirical Bernstein inequality
thatechoesmanyofthepreviousself-normalized-typeempiricalBernsteininequalitiesforscalar,
vectors, and Banach space elements. Our matrix empirical Bernstein inequalities match in
asymptotics the best non-empirical matrix Bernstein inequality in the literature, as they only
depend (in the large sample limit) on the true variance of the matrices which is not required
to be known in our bounds, but required in non-empirical bounds. We expect future work to
address the relatively minor problem of further eliminating the ( V 1/2 n1/4) lower-order
−
O k k ∧
extra dependencein (18), andthe morechallenging problem of unifyingour methodswith those
of the dimension-free matrix Bernstein inequality by Minsker [2017].
Acknowledgement
The authors thank Diego Martinez-Taboada and Arun Kumar Kuchibhotla for helpful discus-
sions.
References
J.-Y. Audibert, R. Munos, and C. Szepesv´ari. Exploration–exploitation tradeoff using variance
estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902, 2009.
B. Chugg, H. Wang, and A. Ramdas. Time-uniform confidence spheres for means of random
vectors. arXiv preprint arXiv:2311.08168, 2023.
X. Fan, I. Grama, and Q. Liu. Exponential inequalities for martingales with applications. 2015.
W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13–30, 1963.
S. R. Howard, A. Ramdas, J. McAuliffe, and J. Sekhon. Time-uniform Chernoff bounds via
nonnegative supermartingales. Probability Surveys, 17:257–317, 2020.
S. R. Howard, A. Ramdas, J. McAuliffe, and J. Sekhon. Time-uniform, nonparametric,
nonasymptotic confidence sequences. The Annals of Statistics, 49(2):1055–1080, 2021.
E. H. Lieb. Convex trace functions and the Wigner-Yanase-Dyson conjecture. Les rencontres
physiciens-math´ematiciens de Strasbourg-RCP25, 19:0–35, 1973.
L. Mackey, M. I. Jordan, R. Y. Chen, B. Farrell, and J. A. Tropp. Matrix concentration
inequalities via the method of exchangeable pairs. The Annals of Probability, 42(3):906 –
945, 2014. doi: 10.1214/13-AOP892. URL https://doi.org/10.1214/13-AOP892.
D. Martinez-Taboada and A. Ramdas. Empirical Bernstein in smooth Banach spaces. arXiv
preprint arXiv:2409.06060, 2024.
A. Maurer and M. Pontil. Empirical bernstein bounds and sample variance penalization. arXiv
preprint arXiv:0907.3740, 2009.
S. Minsker. On some extensions of Bernstein’s inequality for self-
adjoint operators. Statistics & Probability Letters, 127:111–119, 2017.
ISSN 0167-7152. doi: https://doi.org/10.1016/j.spl.2017.03.020. URL
https://www.sciencedirect.com/science/article/pii/S0167715217301207.
D. Paulin, L. Mackey, and J. A. Tropp. Efron–Stein inequalities for random matrices. 2016.
A. Ramdas and T. Manole. Randomized and exchangeable improvements of Markov’s, Cheby-
shev’s and Chernoff’s inequalities. Statistical Science (to appear), 2024.
11J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computa-
tional mathematics, 12:389–434, 2012.
I. Waudby-Smith and A. Ramdas. Estimating means of bounded random variables by betting.
Journal of the Royal Statistical Society: Series B (Methodological), 2023.
12A Additional Proofs
A.1 Technical Lemmas
The following lemma converts bounds on a b to √a √b.
| − | −
Lemma A.1. Let a,b > 0 and D = a b . Then,
| − |
D
√a 6 √b+ √D . (49)
∧ √b
(cid:18) (cid:19)
Proof. Suppose a > b since the bound is trivial otherwise. First, by the subadditivity of the
square root, √a = √b+D 6 √b+√D. Second, D = (√a √b)(√a+√b)> (√a √b)√b so
− −
√a 6 √b+ D . Taking a minimum completes the proof.
√b
The following lemma characterizes the smoothness of ψ (x) = log(1 x) x at 0.
E
− − −
Lemma A.2. When 0 6 x 6 2/5, ψ (x) 6 x2.
E
Proof. Letg(x) = ψ (x) x2. Tpheclaimfollowsfromg (x) = (1 x) 2 2 > 0forx [0, 2/5],
E ′′ −
− − − ∈
and g(0) = 0, g( 2/5) < 0.
p
Thefollowingptransfer rule [Tropp, 2012, Equation 2.2] is commonly usedin derivingmatrix
bounds.
Lemma A.3. Suppose I R and f,g : I R satisfies f(x) 6 g(x), then, f(X) g(X) for
⊆ → (cid:22)
any X I.
∈Sd
It is well-known that if X and Y are scalar random variables such that c 6 X 6 Y almost
surely forsome constant c andthat E Y < , it follows that E X < as well, and EX 6 EY.
| | ∞ | | ∞
This type of “implied integrability” appears frequently in scalar concentration bounds. Let us
prove its symmetric matrix extension for the sake of self-containedness.
[c, )
Lemma A.4 (Dominated integrability). Let X and Y be ∞ -valued random matrices for
Sd
some c R such that X Y almost surely. Further, suppose EY exists. Then, so does EX
∈ (cid:22)
and EX EY.
(cid:22)
Proof. Let us prove that each element X of the random matrix X is integrable. Note that for
ij
any deterministicv Rd, vT Xv 6 vT Yv almost surely. First, taking v = (0,...,0,1,0,...0)T ,
∈
we have
c6 X 6 Y almost surely, (50)
jj jj
concluding that the diagonal element X must be integrable (since Y is). Next, taking v =
jj jj
T
(0,...,0,1,0,...,0,1,0,...,0) , we have
2c 6 2X +X +X 6 2Y +Y +Y almost surely, (51)
ij ii jj ij ii jj
concluding that 2X +X +X must be integrable (since 2Y +Y +Y is). Therefore, the
ij ii jj ij ii jj
off-diagonal element X is integrable since X and X are.
ij ii jj
Now that we have established the existence of EX, it is clear that EX EY since for any
v Rd, vT (EX)v = E(vT Xv) 6 E(vT Yv) = vT (EY)v. (cid:22)
∈
13A.2 Remarks on the Scalar (1) and Matrix (5) Bennett-Bernstein Inequalities
Non-empirical Bernstein inequalities are typically stated in terms of the upperboundof the tail
probability P(S ES > t). These are derived via Bennett-type inequalities via controlling
n n
−
the function
( ) u2
h(u) = (1+u)log(1+u) u >∗ . (52)
− 2(1+u/3)
We, for statistical purposes however, are interested in deviation bounds under a fixed error
probability α. The Bennett-to-Bernstein conversion (*) is looser than the following inequality.
Lemma A.5. For all x > 0, h 1(x) 6 √2x+x/3.
−
A proof of this polynomial upper bound on h 1 can be found from Equation (45) onwards
−
in Audibert et al. [2009]. Tropp [2012, Theorem 6.1] first states a matrix Bennett bound in
terms of the h function, then uses (*) to obtain a closed-formed matrix Bernstein bound, both
controlling the tail probability P(λ (S ES ) > t). Let us use Lemma A.5 to recover a
max n n
−
fixed-error α bound whose tightness is between the matrix Bennett and the matrix Bernstein,
which we already recorded in the paper as (5).
Proof of Ineqaulity (5). Due to Tropp [2012, Equation (i) in Proof of Theorem 6.1],
nλ (V) Bt
P λ X M > t 6 d exp max h . (53)
max n − · − B2 · λ (V)
(cid:18) (cid:18) max (cid:19)(cid:19)
(cid:2) (cid:0) (cid:1) (cid:3)
Setting the right hand side as α, we obtain via Lemma A.5
λ (V) log(d/α)B2 λ (V) 2log(d/α)B2 log(d/α)B2
t = max h 1 6 max + , (54)
−
B
(cid:18)
nλ max(V)
(cid:19)
B s nλ max(V) 3nλ max(V) !
which readily leads to the bound (5)
Blog(d/α) 2log(d/α)λ (V)
P λ X M > + max 6α. (55)
max n
− 3n n
r !
(cid:0) (cid:1)
We also remark that the scalar case (1) is when d= 1.
A.3 Proof of Corollary 4.3
Proof. First, it is straightforward that λ (X X )> 1 for every i= 1,...,n since both
max i i 1
[0,1] − − −
X and X take values in , so Theorem 4.2 is applicable. Let us prove the two claims
i i −1 Sd
EB 2log(d/α)
about the deviation bound D under γ = . Recall that
n i nvi−1
q
k
1 5log(d/α)
V = 0, V = (X X )2, v = λ (V ) , (56)
0 k i k k max k
k − ∨ n
i=1
X
n 2
1 (X X )
s
n
=λ
max
i − i −1 . (57)
n v
i 1 !
Xi=1 −
e
14First, via our definition, v > 5log(d/α) , therefore γ 6 2/5. So Lemma A.2 implies that
i n i
ψ (γ ) 6 γ2. Therefore,
E i i p
log(d/α)+λ n ψ 2log(d/α) (X X )2
DEB
=
max i=1 E nvi−1 i − i −1
(58)
n (cid:16) (cid:16)q (cid:17) (cid:17)
P n 2log(d/α)
i=1 nvi−1
q
log(d/α)+λ Pn 2log(d/α) (X X )2
6
max i=1 nvi−1 i − i −1
(59)
(cid:16) (cid:17)
nP 2log(d/α)
i=1 nvi−1
q
log(d/α) 1P+2s
n
= . (60)
r
2n 1 n v−1/2
n i=1 i 1
e−
EB B
Then, let us show that lim D P/D almost surely. Via the boundedness of X and the
n n n 1
→∞
stronglawoflargenumbers,wecanseethefollowinglimitsholdalmostsurely(cf.Waudby-Smith and Ramdas
[2023, Lemmas 4-6]):
lim X = M, lim V = V = Var(X ), (61)
k k 1
k k
→∞ →∞
n
1
lim v = λ (V), lim (X X )2 = V, (62)
k max i i 1
k n n − −
→∞ →∞ i=1
X
n
1
lim λ (X X )2 = λ (V), (63)
max i i 1 max
n n − − !
→∞ i=1
X
1 n (X X )2
i i 1
lim λ max − − = 1. (64)
n →∞ n Xi=1 v i −1 !
Therefore, we have, via the expansion ψ E(x) = ∞k=2 x kk ,
EB
limsup√nD P (65)
n
n
→∞
log(d/α)+λ n ψ 2log(d/α) (X X )2
=limsup
max i=1 E nvi−1 i − i −1
(66)
(cid:16) (cid:16)q (cid:17) (cid:17)
n P1 n 2log(d/α)
→∞ n i=1 vi−1
q 2
log(d/α)+λ Pn 1 2log(d/α) (X X )2
max i=1 2 nvi−1 i − i −1
6limsup (cid:18) (cid:16)q (cid:17) (cid:19) (67)
n P 1 n 2log(d/α)
→∞ n i=1 vi−1
q k
λ n 1 P2log(d/α) (X X )2
∞ max i=1 k nvi−1 i − i −1
+ limsup (cid:18) (cid:16)q (cid:17) (cid:19) (68)
n P 1 n 2log(d/α)
Xk=3 →∞ n i=1 vi−1
q 2
log(d/α)+λ Pn 1 2log(d/α) (X X )2
max i=1 2 nvi−1 i − i −1
=limsup (cid:18) (cid:16)q (cid:17) (cid:19) (69)
n P 1 n 2log(d/α)
→∞ n i=1 vi−1
=limsup
log(d/α)
1+λ maxP n1 q n
i=1
(X i− viX −i 1−1)2
(70)
n r
2 (cid:16) 1(cid:16) Pn v−1/2 (cid:17)(cid:17)
→∞ n i=1 i 1
−
= 2log(d/α)λ max(V). P (71)
Similarly, one capn show that liminf √nDEB > 2log(d/α)λ (V), concluding the proof.
n n max
→∞
p
15A.4 Proof of Lemma 2.2
Proof. Due to the monotonicity of log, the condition (13) implies
logE(exp(Z C ) ) C . (72)
n
−
n |Fn −1
(cid:22)
′n
Now recall Lieb’s concavity theorem [Lieb, 1973]: for any H , the map X trexp(H+
d
∈ S 7→
logX) ( ++ (0, )) is concave. Therefore,
Sd → ∞
n 1 n 1
E(L )= E trexp − Z − (C +C ) C +logeZ n C n (73)
n |Fn −1 i
−
i ′i
−
′n − !(cid:12)Fn −1
!
Xi=1 Xi=1 (cid:12)
(cid:12)
(Jensen’s inequality) (cid:12)
(cid:12)
n 1 n 1
6 trexp
−
Z
−
(C +C ) C
+logE(eZ
n
C
n ) (74)
i
−
i ′i
−
′n − |Fn −1
!
i=1 i=1
X X
(by (72) and monotonicity of trace)
n 1 n 1
− −
6 trexp Z (C +C ) C +C = L , (75)
i
−
i ′i
−
′n ′n
!
n −1
i=1 i=1
X X
concluding the proof that L is a supermartingale. Finally, observe that
n
{ }
n n
L =trexp Z (C +C ) (76)
n i
−
i ′i
!
i=1 i=1
X X
n n
>trexp Z λ (C +C ) I (77)
i
−
max i ′i
! !
i=1 i=1
X X
n n
>λ exp Z λ (C +C ) I (78)
max i
−
max i ′i
! !
i=1 i=1
X X
n n
=expλ Z λ (C +C ) I (79)
max i
−
max i ′i
! !
i=1 i=1
X X
n n
=exp λ Z λ (C +C ) , (80)
max i
−
max i ′i
! !!
i=1 i=1
X X
concluding the proof.
B Sharp Maurer-Pontil Inequality
Maurer and Pontil [2009, Theorem 4] derived a scalar empirical Bernstein inequality which we
quote as (2), by a union bound between a scalar Bennett-Bernstein inequality and a tail bound
on the sample variance. However, their balanced union bound split α= α/2+α/2 leads to the
looser log(2/α) term. This causes the confidence interval to be 10.9675% longer when α = 0.05
in the large sample limit. We slightly modify their proof below to obtain a sharp EB inequality
for scalars.
Proposition B.1. Let X ,...,X be [0,1]-bounded independent random scalars with common
1 n
mean µ and variance σ2. We denote by X their sample average and σˆ2 the Bessel-corrected
n n
sample variance. Then, for any α (0,1), P X µ > ρ 6 α, where
n n
∈ −
(cid:0) (cid:1)
log n 2σˆ2log n log n log n
(n 1)α n (n 1)α (n 1)α α
ρ n = 3n− + s n − +2v u(cid:16) n− (n (cid:17)1(cid:0)) (cid:1). (81)
u −
t
16Further, with i.i.d. X ,...,X ,
1 n
lim √nρ = 2σ2log(1/α), almost surely. (82)
n
n
→∞
p
Proof. By Bennett-Bernstein inequality (1),
log(1/α) 2σ2log(1/α)
P X µ > + 6 α. (83)
n
− 3n n
r !
Thedeviationofσˆ2 fromσ2iscontrolledbyaself-boundingconcentrationinequality[Maurer and Pontil,
n
2009, Theorem 7],
2log(1/α)
P σ σˆ > 6 α. (84)
n
− n 1
r − !
The desired bound thus follows from an α= α(n 1)/n+α/n union bound.
−
17