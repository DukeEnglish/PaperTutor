SimTube: Generating Simulated Video Comments through
Multimodal AI and User Personas
Yu-KaiHung Yun-ChienHuang Ting-YuSu
b09902040@csie.ntu.edu.tw b09902067@csie.ntu.edu.tw tingyusu1786@gmail.com
NationalTaiwanUniversity NationalTaiwanUniversity NationalTaiwanUniversity
Yen-TingLin Lung-PanCheng BryanWang∗
ytl@ieee.org lung-pan.cheng@csie.ntu.edu.tw bryanw@adobe.com
NationalTaiwanUniversity NationalTaiwanUniversity AdobeResearch
Shao-HuaSun∗
shaohuas@ntu.edu.tw
NationalTaiwanUniversity
Figure1:SimTube(A)generatessyntheticcontent-basedvideocommentsand(B)supportsuser-steerablecommentgeneration
fromusers’respondingordefinedpersona,providingpreliminaryinspirationandinsightforiteratingvideo.
Abstract 1 Introduction
Audiencefeedbackiscrucialforrefiningvideocontent,yetittyp- Audiencefeedbackiscrucialforvideocontentcreatorstoshapeand
icallycomesafterpublication,limitingcreators’abilitytomake refinecontent.Variousmodernplatforms[10,56,59]allowcreators
timelyadjustments.Tobridgethisgap,weintroduceSimTube,a tosharevideosandgathercrowd-sourcedinput[56,70,73,81].For
generativeAIsystemdesignedtosimulateaudiencefeedbackinthe instance,YouTubersandTikTokcreatorsfrequentlyadjusttheir
formofvideocommentsbeforeavideo’srelease.SimTubefeatures contentbasedonaudiencereactionsandcommentsfromprevious
acomputationalpipelinethatintegratesmultimodaldatafromthe episodes[13,25].However,suchaudiencefeedbackisoftendelayed,
video—suchasvisuals,audio,andmetadata—withuserpersonas typicallyprovidedonlyafterthecontenthasbeenpublished,limit-
derivedfromabroadanddiversecorpusofaudiencedemographics, ingcreators’abilitytomaketimelyimprovementsanddynamically
generatingvariedandcontextuallyrelevantfeedback.Furthermore, adjustthecontent.Thatsaid,creatorsmustwaituntiltheircontent
thesystem’sUIallowscreatorstoexploreandcustomizethesimu- ispostedtoreceiveinput,whichcanonlyinfluencefutureepisodes
latedcomments.Throughacomprehensiveevaluation—comprising ratherthanthecurrentone.Fornovicecreators,thischallengeis
quantitativeanalysis,crowd-sourcedassessments,andqualitative evenmorepronouncedduetothelowvisibilityoftheircontent,
userstudies—weshowthatSimTube’sgeneratedcommentsare makingitdifficulttogathersubstantial,actionablefeedback.
notonlyrelevant,believable,anddiversebutoftenmoredetailed Toaddresstheselimitations,weaimtodevelopmethodstoen-
andinformativethanactualaudiencecomments,highlightingits ablevideocontentcreatorstoobtaindiverseandmeaningfulau-
potentialtohelpcreatorsrefinetheircontentbeforerelease. diencefeedbackbeforepublishingtheirvideos.Specifically,we
focusonsimulatingaudiencecomments,aprevalentformoffeed-
∗Equaladvisorycontribution. backonvideo-sharingplatforms.Videocommentsallowviewersto
Correspondenceto:Shao-HuaSun<shaohuas@ntu.edu.tw>
4202
voN
41
]CH.sc[
1v77590.1142:viXradirectlysharetheirthoughtswithcreators,oftensparkingbroader NLGintegratesvisualinput,suchasimagesandvideos,toguide
discussionsasotherusersengagebylikingorreplying[69].These thegenerationprocess.Aprominentexampleisimagecaptioning,
engagementsoffervaluableperspectivesandfosterasenseofcom- whereamodeltakesanimageasinputandoutputsatextualdescrip-
munity[16,69],makingvideocommentsarichsourceoffeedback tionofthevisualcontent[4,26,36,37,49].Thishasmanydown-
forcreators. streamapplications,includingaccessibilityenhancements[14,22].
Tothisend,wepresentSimTube,afull-stackAIsystemcapable Othervision-guidedNLGtasksincludevisualquestion-answering
ofgeneratingdiverse,relevant,andbelievableaudiencecomments (VQA)[1,20],whereamodelreceivesanimageandalanguage-
basedonvideocontent.Weproposedacomputationalpipelinethat basedqueryasinputsandoutputsananswer,orvisualstorytelling,
leveragesgenerativeAImodels,includingvisionlanguagemod- whichinvolvesgeneratingcoherentnarrativesbasedonasequence
els(VLMs)forunderstandingvisuals,speechrecognitionfortran- ofimages[9,17,28,32,63].
scribingaudio,andlargelanguagemodels(LLMs)forgenerating Beyondstaticimages,videosserveasanotherformofvisualcon-
naturallanguagefeedback.Thispipelinefirstintegratesthemulti- tent,extendingimagerywithatemporalaxis.Accordingly,tasks
modaldatainvideos—includingvisuals,audio,andmetadata—to likevideocaptioning[3,6,21,61,64,68],audiovisualcontentrecog-
produceavideosummaryandkeywords.Subsequently,thevideo nition[67],andvideoquestion-answering(VideoQA)[82]have
summaryandkeywordsarecombinedwithvariouspersonade- beenexplored.Thetemporaldimensionintroducesadditionalinfor-
scriptions—representingdifferentaudiencedemographicsandback- mation,whichcanbeexcessiveforapplicationsrequiringconcise
grounds—tosimulatevideocommentsfromdiverseperspectives. understanding.Thus,researchhasbeenconductedtosummarize
Inadditiontothispipeline,wedesignedauserinterfacethatallows individualframesintoconcisevideosummaries[11,72,74].De-
creatorstouploadvideos,obtainsimulatedfeedback,andinter- pendingonthevideotype,differentgenerationtasksarise.For
activelyexploretheresults.Userscanalsocustomizepersonasto instance,inlive-streamedvideos,therehasbeenworkongenerat-
tailorthefeedbacktospecificviewpointsoraudiencebackgrounds ingreal-timecommentary[15,45,78,83].
astheyseefit. TheseworksarecloselyrelatedtoSimTube.However,theypri-
TounderstandtheeffectivenessofSimTubeandthequalityofthe marilyfocusongeneratingviewercommentsforentertainment
commentsitgenerates,weconductedacomprehensivesetofassess- purposesandarelimitedtospecifictypesofvideosbasedontheir
ments,includingquantitativeanalysis,crowd-sourcedratings,and trainingdata.Incontrast,ourworkaimstogeneratediverseperspec-
qualitativestudies.OurresultsindicatethatSimTubeproducesrel- tivesintendedtoassistcreatorsinimprovingtheirvideos.Addition-
evant,believable,andhelpfulcommentsforcreatorsacrossvarious ally,weleverageadvancedvision-languagemodels(VLMs)[11,51],
videogenres.Notably,inmanyinstances,AI-generatedcomments whicharemoregeneralizableandcapableofhandlingdiversevideo
were rated as more informative and beneficial to creators than inputsandgeneratingvariedlanguageoutputs,surpassingtheca-
thoseleftbyactualusers.Additionally,theuserstudyprovided pabilitiesofpriorwork.
insightsintohowSimTubecanintegrateintocreators’videopro-
ductionworkflows,revealinguserperceptionsofgenerativevideo
comments.
2.2 ToolsforProvidingFeedback
Collectively,thispapermakesthefollowingcontributions:
• SimTube,aninteractivesystemthatsupportstheautomatic Feedbackisessentialforimprovingtaskperformances[24,33,35].
Traditionally,feedbackwasgatheredthroughface-to-faceandsyn-
generation of diverse video comments, enabling creators
chronousinteractions.However,withtheadvancementoftheIn-
toreceivevaluablefeedbackbeforepublishingtheirvideo
ternet,feedback-gatheringtoolsnowfacilitateasynchronouscol-
content.
• AmultimodalAIpipelinethatintegratesmultipledatamodal- lectionandtransferofrichfeedbackformsfordocumentation[80],
music [12], image editing [84], and video [56, 59] through text,
itiesinvideo—suchasvisuals,audio,andmetadata—along
audio,peninput,andevengestures.Despitetheseadvancements,
withuserpersonassampledfromalargedataset,toproduce
suchtoolsstillrelyheavilyonhumanfeedback,resultinginhigh
diverse,relevant,believable,andhelpfulvideocommentsfor
wait times and significant human effort. Timely feedback is of-
creators.
• Athoroughevaluationinvolvingautomaticmetrics,crowd- tenonlypossiblethroughpeer-basedcritiques,asseeninMOOC
scenarios[35].
sourcing,andqualitativeuserstudies,demonstratingSim-
To address these limitations, automatic feedback tools have
Tube’seffectivenessandprovidinginsightsforthedevelop-
emerged,providingreal-time,personalizedfeedbackwithouthu-
mentoffutureAI-assistedfeedbacktoolsincontentcreation.
maninterventionacrossvarioustasks,suchasprogramming[39],
2 RelatedWork writing[50,66],andacademicassignments[46],helpingusersit-
erateandrefinetheirwork.Someautomatedassistancetoolsfor
SimTubebuildsuponpriorworkonvision-guidednaturallanguage
content creators, such as AI Insights by YouTube [81], suggest
generation,feedbacktools,andthesimulationofhuman-likebe-
textualvideooutlinesbasedonuserinputandpriorcontent.The-
haviorsusingLLMs.
matic[70]analyzesrealvideocommentstogeneratebulletin-style
reports.SimTubeextendsthisbodyofworkonautomaticfeed-
2.1 Vision-guidedNaturalLanguageGeneration
backtoolsbycontributinganovelinteractivesystemcapableof
NaturalLanguageGeneration(NLG)focusesondevelopingma- generatingvideocommentsfromdiverseperspectives,providing
chinelearningtechniquestogeneratetextualcontent.Asubareaof creatorswithvaluablefeedback.Italsoallowsuserstointeractively
2explorevariousgeneratedcomments,offeringfurtheropportunities Guidedbythesedesigngoals,wedevelopedtheSimTubesystem
foriterativerefinementandcreativeexploration. asacomprehensivefull-stackwebapplication,featuringafrontend
userinterface(UI)andabackendcommentsimulationpipeline.
2.3 SimulatingHuman-LikeBehaviorswith WefirstdescribetheUIanduserexpereinceofusingSimTubein
LLMs Section4,followedbythediscussionofthebackendimplementa-
tionthatempowerstheuserexperience,inSection5.Wewillalso
LLMs,trainedonvastdatasetsandknowledge[7,51],havedemon-
highlighthowourdesigngoalsaretightlyconnectedtospecific
stratedtheabilitytoperformtasksoncethoughttobeexclusiveto
aspectsofthesystem’sdesign.
humans,suchasreasoning[77]andcreativewriting[38,48].Conse-
quently,therehasbeengrowingresearchexploringthepotentialof
4 SimTubeUserInterface
usingLLMstosimulatehumanbehaviors.Oneprominentexample
SimTube’suserinterface(UI)implementsathree-stepworkflow:
isGenerativeAgents[54],whereLLMssimulatedailylifeandin-
VideoUploading,CommentSimulation,andCommentCustomization.
teractionsbetweenvirtualcharactersinagameenvironment.Prior
Intheprocess,usersuploadavideo,afterwhichSimTube’sbackend
workhasalsoexploredsimulatingRedditcommunitycomments
pipelinegeneratesrelatedcomments.Finally,userscanreviewand
[55]andsynthesizinguserstudydata[23].Additionally,numerous
interactwiththecomments.Thefollowingsubsectionswillfurther
studieshaveemployedLLMstocreateagentsservingvariousroles,
discusseachcomponentoftheworkflow.
suchaseducators[57],customerserviceagents[34,53],andeven
historicalfigures1.Thesesimulationsareoftenachievedthrough
4.1 VideoUploading
promptengineeringusingapersonaoradescriptionofahuman
background,enablingthemodelstoperformtasksspecifictothe ThevideouploadprocessbeginswhenusersclicktheCreatebutton,
assignedpersona[5,30,60,71].Inthiswork,webuilduponthis whichopensUploadVideopage,amodalinterfaceforvideosub-
bodyofresearchtofurtherexplorehowLLMs,equippedwithper- mission,asdepictedintheuppersectionofFigure2.Usersprovide
sonas,canbenefitcreatorsbysimulatinghumaninteractionsand essentialvideometadata,suchastitle,description,andthumbnail
feedbackonvideo-sharingplatforms,providinginsightsforcontent image,similartowhattheaudiencewouldperceiveonpopular
refinement.However,weacknowledgethepotentialimplications videostreamingplatformslikeYouTube.Thesystemsupportsmul-
ofsimulatinghuman-likebehaviorsandensurethatusersunder- tiplevideouploads,withallsubmissionsaccessiblethroughthe
standourgeneratedcommentsarepurelysimulated,intendedasa VideoListpage,asdepictedinthelowerleftquadrantofFigure2.
supplementtorealhumanfeedbackratherthanareplacement. Upon successful upload, the frontend sends the video along its
metadatatothebackendpipeline,initiatingthecommentgenera-
3 DesignGoals tionprocess(DG1).Simultaneously,aprogressbarthatmirrorsthe
real-timeprogressionofcommentgenerationisincorporated.
Theobjectiveofoursystem,SimTube,istoautomaticallygenerate
simulatedvideocommentsthatprovidecreatorswithuseful,action-
4.2 CommentSimulation
ablefeedback,whilealsoallowingthemtocustomizeanditeratively
explorethegeneratedcontent.Tothisend,wehaveestablishedthe Uponcompletionofthesimulationprocess,theShowResultbut-
followingdesigngoals(DGs): tonmaterializes,guidinguserstotheSimulatedCommentpage,
illustratedinthelowerrightquadrantofFigure2.Thepagedis-
• DG1:IntegratingandLeveragingMultimodalInforma-
playscontextuallyrelevantcommentsgeneratedbythebackend
tionforVideoCommentGeneration.Videosinherently
pipelinebeneaththevideocontent.Eachsimulatedcommentin-
containrichinformationacrossmultiplemodalities,includ-
cludescomprehensiveparticulars:thecommenter’sname,profile
ingvisuals,audio,andnarration.Tocomprehensivelyinform
picture, and persona — accessible by hovering over the profile
SimTube’scommentgenerationmodule,weaimtodesigna
pictures,asdemonstratedbytagDinFigure2.Eachcommentis
pipelinethateffectivelyintegratesandleveragesmultimodal
generatedbasedondifferentpersonas,showcasingthediversity
informationextractedfromvideos.
ofgeneratedcommentsfromdifferentuserbackgrounds(DG2).
• DG2:GeneratingDiverse,Relevant,andBelievableCom-
Thecommentinterfacedeliberatelymirrorsconventionalvideo-
ments.WeaimtodesignSimTubetogeneratecomments
sharingplatforms,providinguserswithafamiliarenvironment
thatarediverse,relevant,andbelievable,ensuringtheyare
forreceivingfeedback.Thisdesignapproachensuresthatusers
beneficialtocreators.Weintendtosupplement,ratherthan
canfocusonevaluatingthesimulatedcommentswithoutinterface-
replace,genuinehumanfeedbackbyofferinganautomated
relateddistractions,maintaininganintuitiveandseamlessreview
andscalablemeansofgeneratingusefulandassortedfeed-
experience.
backforcreators.
• DG3:SupportingUserstoInteractivelyGenerateCom-
4.3 CommentCustomization
ments.WeplantodesignSimTubeasaninteractivetoolthat
engagesusersinthecommentgenerationpipeline,enabling Whileautomaticcommentsimulationgeneratesdiverseperspec-
themtosteerthegenerationaccordingtotheirpreferences, tives,SimTubeextendsthegenerationbyofferinguser-directed
e.g.,feedbackfromthetargetedaudience,anddeveloping customizationcapabilities.Thiscustomizationframeworkenables
in-depthdiscussionswithcommenters. adjustmentsincommentgenerationinput,allowinguserstoex-
ploreperspectivesbeyondtheinitialoutput(DG3).Thesystem
1https://www.cnn.com/2023/08/21/tech/khan-academy-ai-tutor/index.html implementstwointeractionmechanismsforcomment:
3Figure2:UsersuploadavideoviatheUploadVideoPage,and(A)viewthecommentsgeneratedbySimTubeontheSimulated
CommentPage,andby(B)replyingtoacommentor(C)specifyingacustomizedpersona,userscanaskSimTubetogenerate
newcomments.Inaddition,userscanrefertothepersonadescriptionby(D)hoveringonthecommenter’sicon.
• ThreadExpansion:Userscanextendexistingdiscussions 5.1 VideoUnderstanding
byreplyingtoanysimulatedcomment.Thedialogueisthen Thevideounderstandingpipelinedigestsmultimodalcontentof
deepenedwiththesystemgeneratingafollow-upreplywith thevideo,includingtheclips,thumbnails,andmetadata(DG1),
theoriginalpersona,asillustratedintheleftpanelofFig- subsequentlysynthesizingandproducingacohesivetextualvideo
ure3. summary.Toachievethiswithinthecurrentmodels’capability,we
• PersonaCrafting:Userscanreceivefeedbackfromspe- decomposethevideointovisualandauditorycomponents.The
cificaudience’sperspectivesbydefiningpersonas.Anew pipelinediagramisillustratedintheredblock"VideoUnderstand-
commentwillbegeneratedaccordingtotheuser-defined ing"tagged(a)inFigure4.
personaandthevideocontent,asdemonstratedintheright
panelofFigure3. 5.1.1 AudioTranscription
Manyvideoscontainspeechdialogueornarration,offeringrich
5 SimTubeBackendImplementation informationforunderstandingtheirthemesandkeymessages.To
extractthisinformation,weuseWhisper[58]totranscribethe
IntheSection4,wedescribedtheUIofSimTubeandexplained
dialogueornarrationwithinthevideo.AmongWhisper’smodels,
howuserscouldinteractwithoursystemtoobtainvideofeedback
weselecttheWhisper-mediummodelforitsbalancedefficiency
throughgeneratedcomments.Wenowdetailournovelgenerative
andperformance.Theresultingtranscriptionprovidesboththe
AI-basedbackendpipelinethatpowersthesefeatures.Startingfrom
dialogueandcorrespondingtimestamps.
howtheaudiencewouldperceiveavideo,fromclickingthevideo
thatintereststhem,watchingthevideo,acquiringthecontent,to
5.1.2 FrameCaptioning
leavingtheircomments.Thebackendpipelineconsistsofthree
primarycomponents,asillustratedinFigure4:(a)aVideoUnder- Acomprehensiveunderstandingofavideorequiresintegrating
standingmodule,whichcapturesthesemanticsofvideocontent crucialvisualelementssuchasobjects,scenes,anddynamicinfor-
throughmultimodalsummarization;(b)aPersonaQuerymodule, mation[27].Weutilizeapowerfulvision-languagemodel(VLM),
whichretrievesrelevantuserpersonasforprovidingfeedbackon LLaVA-NeXT 13B,toconvertvisualdataintotextualdescriptions.
thevideo;and(c)CommentGeneration,whichcombinesvideoun- However,mostVLMsaredesignedtoprocesssingleimages,while
derstandinganduserpersonainformationtogenerateandpresent analyzingvideocontentnecessitatescapturingtemporalcontinuity
commentsintheUI,allowinguserinteraction. across sequential frames. To address this, we propose a unique
4Figure3:ThreadExpansion:Uponreceiptof(A)theuser’sreply,Thethreadisexpandedbyboth(B)theuser’sreplyand(C)
thegeneratedresponseofthecommenter.PersonaCrafting:(D)Uponuserspecificationofapersona,(E)anewcommentis
generatedinalignmentwiththevideocontentandtheuser-definedpersona.
approachthatconsidersthetemporalaspectsofvideowhilealign- 5.2 PersonaQuery
ingvisualandaudiotracks.First,weprocessthevisualandaudi- Inourinitialtesting,generatingcommentsdirectlyfromthevideo
torytrackssimultaneouslyinchronologicalorder,matchingthem summaryobtainedinthepreviousstageproducedinformativeand
throughtimestamps.Thisframe-dialoguepairingenablesthemodel helpfulcomments;however,theyoftenlackedsufficientdiversity
tobetterunderstandthesceneandactivitieswithtemporalcontext. inopinionsandvariety.Inspiredbypriorworkleveraginguser
Additionally,weemployacomic-likeassemblyoffourconsecutive personastosimulatediversegeneration[30,47],weincorporated
visualframesasasingleimage,referredtoasa"panel",whichis personainformationintoourgenerationpipelinebydevelopingthe
inputintotheVLM.WedesignaVLMprompttoemphasizekey PersonaQuerysystem.Thissystemqueriesmultiplerelatedper-
visualdetails,suchasobjects,scenes,temporaldynamics,andchar- sonasforcommentgeneration,asshownintheblueblocklabeled
acteremotions,asoutlinedinAppendixB.TheVLMthenprocesses (b)PersonaQueryinFigure4.
eachpanel-dialoguepair,generatingacaptionforaone-second
videosegment.Thiscaption,enrichedwithaudioandtemporal 5.2.1 PersonaDataset
details,integratestheessentialinformation.Examplecaptionsare Apersonaisatextualdescriptionofaperson’spersonality,back-
showninAppendixC. ground,andhobbies,andotherdemographicinformation.Wein-
cludepersonasinoursystemtohelptheLLMgeneratediverse
5.1.3 SummaryGeneration
outputs[30](DG2).Specifically,weusePersonaChat[47],apop-
Onceallvideocaptionsaregenerated,weuseanLLMtosumma- ulardatasetcontainingover8,000personas,toenhancethediversity
rizethemalongwiththetranscript2 tocreateacomprehensive ofuserbackgroundsinourcommentgenerationpipeline.
videosummary.ThecaptionsandtranscriptarefedintotheLLMin
5.2.2 QueryingPersona
chronologicalordertomaintainthetemporalflow.However,videos
cancontainnumerousframes,makingitdifficulttofitallinfor- Inourpreliminarytesting,irrelevantpersonasusuallyleadtolow-
mationintoasinglecontextwindow.Toovercomethis,weutilize qualitycomments;therefore,wefocusonthosemostrelevanttothe
Claude 1.6,anLLMwithanextendedcontextwindowof200K videocontent.Toachievethis,wemeasurethesimilaritybetween
tokens.Basedonourtesting,thiscapacityissufficienttoaccom- personasandthevideokeywordsobtainedinSection5.1.3.Com-
modateallframecaptionsandtheaudiotranscriptofa25-minute paredtothewholevideosummary,keywordsconciselycoverthe
video.TheLLMgeneratesavideosummaryandextractskeywords, videotopicinjustafewrepresentativewords,makingtheprocess
whicharethenusedinthenextstage—personaquerying. fasterandmoreaccurate.Wefirstembedallpersonasandkeywords
intotextembeddingusingthetext-embedding-3-smallmodel
fromOpenAI.Afterthat,wecalcualtethecosinesimilaritybetween
2TheaudiotranscriptisusedinboththeVLMframecaptioningandtheLLMvideo eachpersonaandthekeywords.Wethenrankthepersonasbased
summarization,aswefoundthenarrationinformationbeneficialforbothtasks. ontheirsimilaritytothevideo’skeywords,selectingthetop30most
5Figure4:SimTubeleverageVLMandWhispertoprocessvideosinvisualandaudiocomponentspermodality.Personasare
thenqueriedbasedonthevideocontentforcommentgeneration.Thesystemdiagrams,situatedcentrallyinthefigure,along
withtheintermediateoutputsonbothflanks,exemplifythisprocess.
relevantpersonas.Oncethemostrelevantpersonasareretrieved, backendprocessing.Wedesignaroleinstructionthatguidesthe
thepipelineusesthemincombinationwithotherdata,including modeltoemulatetypicalYouTubecommentingbehaviorthrough
thevideosummaryandmetadata,togeneratevideocomments. few-shotprompting[76],usingrealYouTubecommentsasexam-
ples. Subsequently, we incorporate the retrieved personas from
5.3 CommentGeneration Section5.2toensurediversityinthegeneratedcomments.The
video metadata includes details such as the video title, descrip-
Oursystemsupportsfourtypesofsimulatedcommentsusingdata
tion,author,andthumbnaildescription,alongwiththesummary
fromearlierstages.Thefirsttwotypesareautomaticallygenerated
andkeywordsgeneratedpreviously,encapsulatinginformationa
bythesystem,whilethelasttwoareuser-initiatedandcustomizable,
viewermightperceivewhenwatchingaYouTubevideo.Oncethe
offeringquickandinteractivefeedback(DG3):
commentsaregenerated,arandomnameandiconareassignedfor
(1) Primary Comments: The initial comments that appear
authenticity.NamesareselectedfromanSSAdataset[65]contain-
directlyunderthevideo.
ingover10,000commonU.S.babynames,whileiconsareuniquely
(2) ThreadComments:Thegeneratedresponsestoexisting
generatedusingBoring-avatars[31].
comments,structuredhierarchicallyasthreadedrepliesbe-
neathprimarycomments. 5.3.2 ThreadComments
(3) Response Comments: The comments generated for re-
Tosimulatediscussionthreads,thesystemutilizesdifferentper-
spondingtouser’sreply,contributingtoadiscussionthread
sonastorespondtoprimarycomments,addingdepthtothecon-
undertherepliedcomment.
versationandmimickingtheextendedexchangescommonlyseen
(4) CustomPersonaComments:Thecommentsgenerated
onvideo-sharingplatformswhereviewersengageindiscussions.
basedonauser-definedpersona,tailoredtouserspecifica-
Thisisachievedbyrandomlyselectingaportionoftheprimary
tions.
commentsandperforminganotherLLMinferenceusingaprompt
Afterthevideoispre-processedbytheupstreamcomponents, thatincludesallrelevantvideodata(asusedforprimarycomment
oursystemdirectlygenerates30comments,consistingof70%pri- generation),theselectedprimarycomment,andanewpersona.
marycommentsand30%threadcomments.Additionalgenerations Thisprocessgeneratesthethreadcomments.Anexampleofthread
canbetriggeredtoobtainmorecommentsifneeded.Below,we commentsisshownintherightpanelofFigure2.
discusshoweachtypeofcommentisgenerated.
5.3.3 ReplyComments
5.3.1 PrimaryComments
Ourtoolenablesuserstointeractwithpre-generatedcomments
WeprompttheLLMwithacommentgenerationinstruction,queried (ThreadExpansionfeaturedetailedinSection4.3),similartoen-
personas,andvideometadataobtainedfromtheearlierstagesof gagingwithotherpeopleinanonlineforum.Whenauserreplies
6toacomment,thesystemgeneratesarelevantresponse,providing YouTube based on their meta-ratings, to exclude any of-
immediatefeedback.Themethodforgeneratingtheseresponses fensiveorinappropriateremarks[62].Wealsofilteredout
isidenticaltothatofThreadComment,withthedistinctionthat non-Englishcommentstoexcludetheeffectofdifferentlan-
theuser’sreplyandtherepliedcommentareincluded.Thesystem guages.
processesthesecommentswithinapproximately10seconds.An (2) Persona-basedCommentsweregeneratedbythefullSim-
exampleofreplycommentsislabeledbytagCinFigure3. Tubesystem,includingPrimaryCommentsandThreadCom-
ments.
5.3.4 CustomPersonaComments (3) NoPersonaCommentsweregeneratedbyanablatedver-
Thesystemalsosupportsusersincustomizingpersonasforfurther sionofSimTubewithoutthepersonacomponents.
explorationandexperimentation.Whenauser-definedpersonais
6.1.2 SurveyProtocol
provided,thesystemgeneratescommentsusingthesameapproach
asforprimarycomments,buttailoredtotheuser’sspecifiedper- We recruited 25 participants on Upwork to evaluate comments
sona.Forinspirationonpersonacustomization,userscanhover accordingtothethreeaspects,utilizinga7-PointLikertScale.We
overanexistingcomment’savatartoviewexamplepersonade- collecttheratingsfromtheparticipantsusingGoogleForms.Each
scriptionsfromothercomments(asshownbytagDinFigure2). formincludesoneYouTubevideolink,fivevideoquizzes,andthree
ThistypeofcommentisgeneratedwhenusersutilizethePersona evaluationsectionswithdifferentcriteriaforthesame30comments.
CraftingfeaturedescribedinSection4.3. Toensurethequalityofresponses,westructuredtheformsinto
Thesefourtypesofcommentsarepresentedinthefrontend fivestages:1)WatchtheVideo,2)VideoQuiz(seeAppendixD),3)
UIforuserstoread,engagewith,andprovidefeedbackontheir VideoSummary,4)CommentsEvaluation,and5)FormFeedback.
videos.Inthefollowingtwosections,wewilldiscusscomprehensive Avalidatedreplymustachievean80%correctionrateinthequiz
studiesonthequalityofthegeneratedcomments,aswellasuser instage2andprovideanoverallcorrectvideosummaryinstage3.
perceptionsofoursystemanditscomments,andhowitmight Thevideosummariescollectedinstage3arereusedastheground
integrateintotheirvideocreationworkflow. truthofhuman-likedetailedvideosummariesforlateruse.
6.1.3 Results
6 QuantitativeEvaluation
Theresultsfromourcrowd-sourcedstudy,displayedinthe"Crowd
Weconductedextensivequantitativeevaluationsverifyingtheabil- Study"plotinFigure5,showsignificantdifferencesbetweenthetwo
ityofSimTubetoproducediverseandrelevantcomments.Specifi- configurationsofSimTube-generatedcommentsandrealcomments.
cally,weadoptbothcrowd-sourced,human-annotatedevaluations Specifically,SimTubecommentswiththepersonaconfiguration
as presented in Section 6.1, and automatic metrics widely used wereratedsignificantlyhigherinrelevance,believability,andhelp-
fornaturallanguagegenerationtasks,suchasSelf-BLEU,Distinct- fulnesscomparedtorealcomments(p<0.05).Similarly,comments
Ngram,BERTScore,etc.,aspresentedinSection6.2. generatedwithoutapersona(No-persona)alsooutperformedreal
commentsinrelevance,believability(p<0.05).However,thecom-
6.1 Crowd-SourcedEvaluation parisonbetweenthefullsystemandtheNo-personaconfiguration
Weconductedacrowd-sourcedevaluationtoassesstheeffective- didnotyieldastatisticallysignificantdifference.
nessofSimTubefromhumanperspectives.Giventhewidelyac- WeusedtheWilcoxonSigned-RankTestforpairedcomparisons,
ceptedunderstandingthatLLMscangeneratesentenceswithhuman- withBonferroniCorrectionappliedtocontrolformultiplecom-
level fluency [8], our study focuses on three high-level aspects: parisonsacrossthesethreeconditions.Theseresultshighlightthe
Relevance,Believability,andHelpfulness,toensurealignmentwith effectivenessofSimTube-generatedcomments,withorwithout
ourdesigngoal(DG2). personas,indeliveringfeedbackperceivedasmorevaluablethan
thatfoundinrealcomments.
• Relevance:referstothedegreetowhichcommentsarealigned
Whilesamplingcandidatesfrompopularrealcommentseffec-
withthevideo’scontent.
tivelyeliminatemostnonsensicalcomments,itisnoteworthythat
• Believability:indicatesthelikelihoodthatuserswouldcon-
thesepopularcommentsareoftenbriefandofferlimitedinforma-
siderthecommentcredible.
tivevalueforcreators.Insomeinstances,theyarepurelylauda-
• Helpfulness:assessestheextenttowhichcommentscanpro-
tory,offeringnosubstantialcritiqueorfeedback.Conversely,with
vide valuable feedback to content creators, aiding in the
LLM’soutstandingwritingskillsandinclusiveattitude,generated
enhancementoftheirvideosandpotentiallyinspiringnew
commentsusuallystimulatebroaderdiscussionandamultitudeof
ideas.
potentialinsights.Thisresultalignswithrecentstudiesshowing
thatLLMoutputcanoutperformhuman-writtenones[19].Some
6.1.1 VideosandComments
commentsandtheiraveragescoresofthisevaluationareshownin
WeselectedeightvideosfromYouTube,eachcoveringonedistinct theAppendixG.Therawaveragescoresandlengthsofcomments
videogenrelistedinAppendixA.Foreachvideo,wesampled30 canbefoundinAppendixE.
commentsfromthreecategories:
6.2 AutomaticMetrics
(1) RealCommentsarefromtherealYouTubeuserscomment-
ingonthisvideo.Werandomlysampled30commentsfrom Besidesthecrowd-sourcedevaluation,wealsoconductedexten-
thetop1000popularcommentsoneachvideo,rankedby siveevaluationsusingautomaticmetrics[40–42,85,86]widely
7adoptedinnaturallanguagegenerationtasks[18,43,44,79].Sim- generallybeingshort[69]—therightsideoftheplottitled"Distinct
ilartothecrowdevaluation,weassessthreetypesofcomments: N-gram"inFigure5presentslength-normalizedresults.
FullSystem(withPersona),NoPersona,andRealComments.For SelfBLEU.Self-BLEUinspectsthesimilarityofintra-groupcom-
theautomaticmetrics,wefocusontwowidelyevaluatedaspects ments.AhigherSelf-BLEUscoreimpliesahighersimilarityamong
inNLG:1)Diversityand2)Relevanceofcomments.Tominimize agroupofcommentsandlowerdiversity.Therightsideoftheplot
varianceintheevaluations,wegeneratedalargernumberofcom- titled"Self-BLEU"inFigure5demonstratestheSelf-BLEUscoresof
ments for the automatic metrics assessment. The total number samenumberofcommentsacrossdifferentsources.Thelefthalfof
ofcommentsforFullSystem,NoPersona,andRealCommentsare theplotindicatetheSelfBLEUscoreofalargergroupnumberof
671,1141,18338,respectively,whiletheaveragelengthsofthecom- comments(num=671,1141,18338).Wenoticedthatrealcomments
mentsare286.18,100.1,85.58characters. haveahigherscoreasthesamplenumberincreases,whichmay
implythatapartofrealcommentsareduplicated.
BERTScore.Intra-groupBERTScorechecksthesemanticsim-
ilarityinagroupofcomments.Ahigherintra-groupBERTScore
indicatesalowerdiversity.Therealcommentsscorethehighest
semanticleveldiversity,asshownintheplottitle"BERTScore"in
Figure5.
6.2.2 Relevance
SomeRealCommentscanbeoff-topicanddivergent,causingcon-
fusionandhighsemanticdiversityscores.Arelevantcomment
ison-topicandpertinenttothevideocontent.Wemeasuredthe
wordandsemanticsimilaritybetweencommentsandvideocon-
tent.Theground-truthvideosummarieswerecompiledfrom30
UserStudyparticipantsusingClaude3.5Sonnet.Generally,our
generatedcommentsweremorerelevanttothevideoonbothlevels.
Figure6:Thisfigurecontainsquantitativeresultsfromau-
tomaticmetricsmeasuringrelevancebetweentargetedtext
andvideocontent,suchasLLMEval,ROUGE-n(n=1,2,L),
Figure5:ThisplotincludesquantitativeresultsfromCrowd- andBERTScore.
SourcedStudyandotherautomaticmetricsmeasuringdiver-
sitysuchasBERTScore,DistinctN-grams,andSelf-BLEU.
LLMEvaluation.WeadoptedLLMevaluation,whichhasshown
highcorrelationcoefficientswithhumanratingsamongcommonly
usedautomaticmetrics[42].TomitigatepotentialbiasfromLLMs
6.2.1 Diversity
towardstheirowngeneratedcomments[2],weutilizedtwodif-
Adiversesetofcommentsoffersvariedperspectives,encompassing ferentmodels,Claude3.5SonnetandGPT-4,toassessbothreal
differentwordchoicesandcommenttopics.Typically,generated commentsandthosegeneratedbyoursystem.TheLLMsevalu-
comments,particularlythosefromtheFullSystem(persona-based), atedeachtargetcommentagainstthevideosummaryandratedits
exhibit the highest diversity in wording, while Real Comments relevanceonascalefrom0to100,whereahigherscoreindicates
displaygreaterdiversityatthesemanticlevel.Wealsonotedthat greaterrelevance.Theresultsshowthatthegeneratedcomments
asubsetofRealCommentsappearedtobehighlyrepetitive,as significantlyoutperformtherealcomments,demonstratingcloser
indicatedbythesignificantvariationinthesampledsizeofthe relevancetothevideo,asillustratedintheplottitled"LLMEval
Self-BLEUScore. Figure6.
DistinctN-grams.AgreatercountofdistinctN-gramsindi- ROUGE. ROUGE is a recall-based metric that evaluates the
catesahigherlevelofword-leveldiversity.Generatedcomments, relevanceofacommentatthewordlevel.Theplottitled"ROUGE-
especiallytheFullSystem(Persona-based)comments,achievethe n"inFigure6presentstheprecisionscore,whichistheproportion
highestaveragenumberofdistinctn-gramspercomment,asde- ofwordsinthecommentthatarealsofoundinthevideosummary.
pictedintheplottitled"DistinctN-gram"inFigure5.Considering Mostly,generatedcommentsexhibitgreaterrelevancetothevideo
thelengthvariationsacrosstheconditions—withRealComments acrossallROUGEmetrics.
8BERTScore.BERTScoreevaluatesthesemantic-levelrelevance session,participantswerebriefedonthestudyprotocolandcom-
ofembeddingsbetweencommentsandthegivenvideosummary. pletedapre-studyquestionnaire.Theytheninteractedwiththe
Hence,ahigherBERTScoreindicatesgreatercommentrelevance systemtoexploreandassessthecommentsgeneratedbySimTube.
to the video. To compute the BERTScore, we utilize the model Afterusingthesystem,weconductedsemi-structuredinterviews
microsoft/deberta-xlarge-mnlifromHuggingFace,whichhas togatherinsightsintotheirperceptionsofboththetoolandthe
thehighestcorrelationswithhumanevaluationcurrentlyinthe qualityofthegeneratedcomments.
WMT16benchmark.Asshownintheplottitled"BERTScore"in
Figure6,Thegeneratedcommentsoutperformrealcommentsand 7.3 Findings
demonstrateastatisticallysignificantdifferenceinsemantic-level
Wenowpresentthekeyfindingsidentifiedintheinterviews.
relevance.
7.3.1 Creator’sPerceptionofGeneratedComments
6.3 Conclusion
Participants generally expressed a positive attitude toward the
Accordingtothecrowd-sourcedevaluationandtheevaluations commentsgeneratedbySimTube.P1appreciatedhowthesystem
usingvariousautomatic,thecommentsgeneratedbyoursystem providedinsightsintospecificaudienceviewpoints.Similarly,P2
displaysuperiorword-leveldiversity,whileRealCommentsshow- emphasizedthevalueofdiverseperspectivesofferedbythesystem-
casebettersemanticdiversity.Althoughafewrealcommentscover generatedcomments.P6highlightedthespeedandcost-efficiency
distinctcommontopics,clustersofrealcommentsmaybehighly ofthegeneratedcomments,expressinginterestinuploadingmulti-
similar,asreflectedbytheSelf-BLEUscore.Concerningrelevance plevideoversionsforcomparison:"Imightwanttouploadevery
tovideocontent,generatedcommentsoutperformRealComments versionofmyvideoandmakeanintra-videocomparisontofurther
inword-level,semantic-level,andLLMevaluations.Incomparison observethedifferencesbetweenversions."P4suggestedthatgener-
toRealComments,generatedcommentstendtobemoreon-topic, atedcommentscouldserveassupplementaryfeedbackalongside
authentic,anddifferentiated,offeringapotentsourceofinspiration. realcommentsbutstillpreferredgenuinefeedbackfromhuman
Despitetheirlimitedsemanticdiversity,thescalability,rapidpro- audiencesorfriendsatthisstage,ashistargetaudienceishuman
duction,andpre-publicationavailabilitymakegeneratedcomments ratherthanAI.
anadvantageouspreliminarysourceofinspirationandfeedback Our system provides an advantage over traditional methods
complementingRealComments,particularlybeforetheformalpub- ofgatheringextensivefeedback,allowingcreatorstoreceivein-
licationofvideos. putwithoutdisclosingunpublishedvideos.P6,anoviceYouTuber,
stated,"Iusuallyneedfeedbackfrommyfriend(beforeuploadingmy
7 QualitativeUserStudy video)...(Generatedcomments)arelikehavingmorefriendstoshare
Inadditiontothequantitativeevaluation,weconductedaquali- mynewvideoclipswithandgettheiropinions." Participantsalso
tativeuserstudytogatherfeedbackonthecommentsgenerated commendedthescalabilityofSimTube.Forexample,P4,aYouTu-
bySimTubeandtoassessthesystem’sutility.Wealsoaimedto berwithover460,000subscribers,mentionedtheyonlyreceived
understandcreators’perspectivesonhowSimTubecouldintegrate 126commentsonhislatestvideointwodays,whereasSimTube
intothebroadervideoproductionworkflow. cangenerateover5,000commentsfora20-minutevideowithina
singleday.Thesystemalsoenablesuserstocreatein-depthandin-
teractivediscussionsquickly,whetherbyrespondingtocomments
7.1 Participants
orassigningpersonastothesystem.Lastly,unlikerealcomments,
Werecruitedeightexperiencedcontentcreators,eachwithamin- LLM-generatedtexttendstobemoremoderated,helpingavoid
imumofoneyearofvideofilmingandeditingexperience(M= cyberbullyingandreducingnegativeeffectsonusers.P4notedthat
5.06years,SD=4.76years,range:1–13years).Twoparticipants realhatefulcommentscouldbeharmfultocontentcreators."Gen-
werefull-timecreators.Theagerangewas22to33years(M= eratedcomments,whilegenerallyfreefromharshness,stillpresent
26.38years,SD=4.27years),andthegenderdistributionincluded opposingideas,whichishelpfulforcontentcreators."(P4)
fivemalesandthreefemales.Participantswererecruitedthrough
auniversitycourseforum,aRedditvideoeditingcommunity,and 7.3.2 Persona’sEffectivenessonCommentGeneration
directoutreachonsocialmedia.Thegrouprepresentedavariety
PersonaisakeycomponentweintegratedintoSimTubetosup-
ofvideogenres,includingtravelvlogs(P1,P2,P7),airlinereviews
portdiversefeedbackgeneration,andweaimedtounderstandits
(P3),Instagramreels(P5),streetinterviews(P6),technews(P4),
impactonusers’perceptions.Weobservedmixedreactions:some
andtechreviews(P8).
participantsfoundpersona-basedcommentsengagingandrelevant
(P2,P4,P6),whileothersviewedthemasunrealisticorunnecessary
7.2 StudyProtocol
(P1,P5,P8).Incontrast,nopersonacommentswereconciseand
Eachstudysessionlastedbetween1to1.5hours.Beforethesession, video-focused(P2,P5,P7),buttheysometimeslackedinspiration
participantswereaskedtoprepareeitherafinalizedorrough-cut (P4),detail(P6),andengagement(P4).P4stated,"Persona-based
video, including a thumbnail, title, and description. They could commentslookinformative,justlikeanaudienceresonatingstrongly
optionallyprovideuptothreevideos.Thesevideosservedasma- withmyvideo."Ontheotherhand,P2commented,"Theirreplies
terialforcommentsimulation,enablingparticipantstoevaluate don’tbringnewideas."Wealsoobservedthatsomepersona-based
SimTube’scapabilitiesandlimitations.Atthebeginningofeach commentscontainedirrelevantinformationbecausethesampled
9personaswerelessrelevanttothevideo,whichcouldcauseconfu- andthepersona-basedcommentsgeneratedbySimTubeextended
sion.Overall,wefoundthatwhilepersonaintegrationaddsdepth thediscussionbetweenthehostandinterviewee,introducingnew
anddetailtocommentsandcaninspireusers,itmayalsointroduce topicssuchastimemanagementforuniversitystudents."Itprompted
inconsistencies. metoexplorethisthemefurtherandenrichedmyvideo,"P6shared,
illustratinghowSimTube’sintegrationcanguideandenhancecon-
7.3.3 ThreadsExpansionandPersonaCrafting
tentcreationthroughoutdifferentstagesoftheworkflow.
SimTube allows users to interactively engage in conversations
throughthreadedrepliesandpersonacustomization.Participants 8 DiscussionsandFutureWork
foundthethreadedreplyfeatureuseful,benefitingfromtheoppor- We have presented the design, implementation, and evaluation
tunitytoengageinmultipleroundsofinteraction.Forexample, ofSimTube.Inthissection,wediscussthekeyfindingsfromour
P1initiallyhadtroubleusingfeedbacktoimprovehervideo,but study,thelimitationsofourapproach,andpotentialareasforfuture
afterreplyingtoaskforspecificsuggestions,shegainedvaluable development.
insightsthathelpedhermakeimprovements.Similarly,P3received
acommentonherbudgetairlinereviewvideo,suggestingshetry 8.1 ExpandingSimTube’sPipeline
afull-serviceairline.SherespondedbyaskingtheAIcommenter
SimTubeadvancesautomatedfeedbacktoolswithinthevideocre-
tocomparetheirexperiences,whichledtoadetailedreplyand
ationworkflowbygeneratingpre-publicationcommentsthaten-
theideaforafuturecomparisonvideo.P6alsoaskedforadvice
ablecreatorstoreflectonandrefinetheircontent.However,our
on "broadening horizons" in her video content, and the system
currentfocusislimitedtoaudio/visualsemantics,addressingonly
providedherwithdetailed,personalizedsuggestionsthatmatched
a portion of the video production. Key elements such as visual
hervideo’stheme.
effects,audioenhancements,andeditingtechniques—crucialfor
Thepersonacustomizationfeatureenablesuserstogenerate
maximizingviewerengagement—arenotyetincorporated.Addi-
moretargetedfeedbackusingpersonasthatmaynothavebeen
tionally,whileSimTubecangeneratecommentsforgeneralvideo
includedinourdataset.Forexample,P7appliedpersonassuch
content,itcurrentlydoesnotconsiderinherentvariationsinvideo
asamom,critic,andadvisorforherself-reflectivevideo,finding
likegenre,style,orculturalcontext.Theseareaspresentopportuni-
thatthesetailoredpersonasprovidedfeedbackthatfeltrealistic
tiestoexpandSimTube’scomputationalpipelinetoaccommodate
andalignedwithherneeds.Similarly,P1usedthepersonaofan
additionalcontextualinformationandenablemorecustomizedcom-
English-speakingEastAsiatravelenthusiast.Shefeltthegenerated
mentgeneration.Futureimprovementscouldalsoincludehandling
commentaccuratelycapturedhertargetaudience’sperspective:
longervideoinputsandenhancingtheoverallqualityoflanguage
"Wow,thisvlogseriouslymakesmemissSeoul.Youradventuresat
generationtoprovidemorenuancedandusefulfeedback.
Ewhaandthenightlifeclipsbroughtbacksomanymemories!Can’t
waittoseewhatJapanbrings." 8.2 IntegrationintoVideoProduction
7.3.4 IntegratingSimTubeintoVideoProduction Workflows
Participantsidentifiedmultiplestagesintheirvideoproduction Whilewehaveassessedthequalityofourgeneratedcomments,
workflowswhereSimTubecouldbeeffectivelyintegrated,align- thesystemhasyettobedeployedinreal-worldsettings.Future
ingcloselywithprofessionalfilmproductionprocessessuchas researchshouldexploreintegratingSimTubeintovideoediting
topicbrainstorming,outlining,footagecapture,editing,andfinal- tools[29,75]orproductionenvironmentstoevaluateitsoverallim-
ization[52].Manyparticipants(P4,P6,P8)emphasizedcreating pact.Qualitativestudiescouldfurtherinvestigatehowthesystem
rough-cutversionsorteasersduringtheeditingphase,enabling complementsprofessionalworkflows,providingdeeperinsights
collaborationandfeedbackfromsponsorsorteammembers.AsP4 intoitspracticalutility.Relevantly,thesystemcurrentlygenerates
explained,"Arough-cutletsmyteamandsponsorsgivefeedbackbe- commentsonlybasedonasingleversionofthevideo.However,
foreweproceedfurther".ThissuggeststhatSimTubecouldenhance creatorsoftenproducemultipleiterationstodeterminethebestre-
thisprocessbyprovidingautomatic,diversefeedbackonuploaded sult.Byanalyzingaseriesofvideoedits,thesystemcouldgenerate
roughcuts.P6added,"IcanseamlesslyintegrateSimTubeintomy comparativefeedbackthathighlightsdifferencesbetweenthecur-
workflowandcollectmorefeedbackwithminimaleffortbyuploading rentandpreviousversions,enablinguserstorefinetheirworkmore
therough-cutversionoranysegmentswheneverIcompleteone." effectivelybyleveragingthestrengthsofeachiteration.Expanding
Beyondassistingwithexistingcontent,SimTubecouldalsoin- tohandlemultipleversions,however,introduceschallengesrelated
spirenewvideotopics.Participants(P1,P3,P6)highlightedthat tosystemscalabilityandprocessingefficiencythatwarrantfuture
AI-generatedcommentsledthemtoexplorenewideas.Forinstance, explorations.
P1,atravelvlogger,receivedrecommendationsforfamoustourist
8.3 ImprovingHelpfulnessofComments
spotslikeIkseon-dongHanokVillageafteruploadingaKoreanvlog,
eventhoughtheseplaceswerenotfeaturedinthevideo."SimTube OurautomatedevaluationsindicatethatLLM-generatedcomments
correctlylistedallmyitinerariesbasedonmynarration,whichhelped sometimessurpassonlineuserfeedbackinseveralmetrics,includ-
meplannewvlogs,"P1noted,demonstratingthesystem’sabilityto ing helpfulness. This may be attributed to the nature of online
generatecontextuallyrelevantinsights. comments,whichoftenleantowardopinionatedorhumorousre-
SimTubecouldalsoinfluenceongoingvideoproduction.P6up- marksratherthanconstructivefeedback.Incontrast,oursystem’s
loadedahalf-finishedstreetinterviewvideoonstudentlifestyles, commentsarespecificallydesignedtoprovideactionableinsights.
10Apotentialfuturedirectioninvolvesintegratingprofessionalfeed-
backdataintothepipeline,refiningmodelswithexpertinsightsto
enhanceguidancequality.Nonetheless,generalaudiencefeedback
remainsvaluable;ourstudydemonstratesthatevennon-expert
inputcaninspirecreatorsandpositivelyimpacttheircreativepro-
cesses.
8.4 ImplicationsofAI-GeneratedComments
Lastly,werecognizetheimplicationsofusingAIforhuman-like
commentgeneration,includingpotentialunintendedoutputsand
misuse.Ourgoalistomanagetheserisksresponsibly,ensuringthat
toolslikeSimTuberemainfocusedonsupportingandenhancingcre-
ators’work.Forinstance,P4inourstudyobservedthatthesystem
wasabletogeneratecriticalyetconstructivecommentsinlanguage
thatis“generallyfreefromharshness”,incontrasttorealcomments
thatsometimescontainoffensivelanguageandnegativelyimpact
creators.Movingforward,weplantoadvanceSimTubewithethical
considerationsattheforefront,maintainingitssupportiverolein
thecreativeprocess.
9 Conclusion
WehavepresentedSimTube,anovelsystemthatautomatically
generatesvideocomments,offeringcreatorsvaluablefeedbackto
enhancetheirvideosbeforepublishing.Wedetailedthedesignand
implementationofoursystem,focusingonthecommentgenera-
tionpipelineandtheuserinterface.Thepipelineutilizesadvanced
generativeAImodelstoprocessmultimodalvideodataandincor-
porateuserpersonas,resultingindiverseandtailoredcomments.
Ouruserinterfacesupportsuser-steerablecommentgeneration,
allowinguserstoinfluencetheoutputbasedontheirresponses
anddefinedpersonas.Weconductedacomprehensiveevaluation
usingbothquantitativeandqualitativemethods,includinghuman
crowdsourcedassessments,automatedmetrics,andauserstudy
withexperiencedcontentcreators.Wehopeourworkestablishes
afoundationforfuturedevelopmentofautomaticfeedbackand
critiquetoolsincontentcreationsystems.
11Appendix Frame Caption: In the current frame, a player is
seen holding a blue bucket, standing in front of a
A YouTubeVideoCategoryList
large,glowinglavapit.Theplayerappearstobein
(1) AnimationandFilm theprocessoffillingthebucketwithlava,asindicated
(2) AutosandVehicles bytheaudiocaption.Thelavapitislocatedinthe
(3) MusicVideos centeroftheimage,withtheplayerstandingtoits
(4) PetsandAnimals right.Theplayer’spositionandthedirectionoftheir
(5) Sports gazesuggesttheyarefocusedonthetaskathand.
(6) Travel&Events Theoverallscenesuggestsamomentofgameplayin
(7) Gaming avideogame,possiblyinvolvingresourcegathering
(8) Comedy orcrafting.
(9) PeopleandBlogs
(10) Entertainment
(11) NewsandPolitics
(12) HowtoAnd
(13) Education
(14) ScienceandTechnology
(15) NonProfit&Activism
B Prompts
[Generate Frame Caption]
You are an AI visual assistant that can generate
audio description for a video clip. You receive
a image of 4 frames, which are sampled during 4
seconds of the video clip. You also receive the Figure 8: The frames are captured in a cooking YouTube
audio caption during the 4 seconds. video4between0:57and1:00.
The 4-th frame is the current frame, using the
Frame Caption: A hand is seen holding a silver
provided frames and audio caption, generate an
spoon,whichisscoopingwhiteflourfromacontainer.
audiodescriptionofthecurrentframeinadetailed
thecontainerappearstobeaplasticbag,andtheflour
manner.Includedetailslikeobjectcounts,position
isbeingpouredintoaglassbowl.thebowlisplaced
of the objects, relative position between the
onawoodensurface,andthereareotheringredients
objects,thevisualcomposition,theemotion,what
nearby,includingabottleandajar.thescenesug-
might be going on during the clip, etc. Imagine
geststhatsomeoneisintheprocessofpreparinga
describing the video clip to someone who cannot
recipethatrequiresasignificantamountofflour.the
see the clip.
actionofscoopingtheflourindicatesthattheperson
iscarefullymeasuringtheingredientsfortherecipe.
C SampledFrameCaptions
theoverallcompositionoftheimagesuggestsahome
Pastingfourframesinoneimageandcorrespondingnarrationhelp cookingenvironment.
VLMidentifytemporalinformation,theactivity,andthescene.
D SampledFormQuestions
Whatdidtheyusetosendthegarlicbreadtothesky?
(A)Weatherballoon
(B)Hotairballoon
(C)Heliumballoon
(D)Adrone
Howdidthehostendthevideo?
(A)Expresshislove
(B)Takeaphotowiththecrowd
(C)Talkinhisstudio
(D)Petapuppy
Whatisthefirstmethodintroduced?
(A)BakedChicken
(B)RoastedChicken
Figure 7: The frames are captured in a gaming YouTube
video3between0:45and0:48.
3https://www.YouTube.com/watch?v=dYO9_MoU4Vw
4https://www.YouTube.com/watch?v=djnNkLi_K6E
12(C)BoiledChicken Comments:Isitbird?Isitaplane?Nah...it’sapiece
(D)BroiledChicken ofgarlicbread:DScores:5.36,5.07,4.18.
Comments:STRATOSPHERICBREAD!Wejustlearned
E CommentLengthtoRating aboutthisinsciencelikelastmonthago.Scores:4.79,
4.5,4.18
Table1:AverageLengthandScoreforRelevance,Believabil-
G.2 GeneratedComments
ity,andHelpfulnessAcrossConditions
Video Title: We Sent Garlic Bread to the Edge of
Space,ThenAteIt
Group FullSystem NoPersona Real
Comments:Whoelsegotherejusttoseeifthespace
AverageLength 233.4625 97.8875 107.7625
garlicbreadtastesanydifferent?Scores:5.39,5.75,
Relevance 5.248 5.313 4.295 4.75
Believability 5.293 5.181 4.401 Comments: Nothing like combining the thrill of
Helpfulness 5.213 4.719 4.194 spacewiththecomfortofgarlicbread!Thisisthekind
ofadventurousspiritIlivefor.Greattoseesomething
soout-of-the-box!Makesmeponderwhatotherfoods
F ConfusingPersona-basedCommentExamples
couldtakeastratosphericjourney.#SpaceBreadAd-
Relatedcontentisunderlinedandconfusingcommentsisinitalic. venturesScores:5.64,5.89,5.14.
Video Title: Grad School Visiting and Bar Explo- Comments:Interestingexperiment,buthowabout
rationinNYC. consideringtheenvironmentalimpactofsuchstunts?
Persona:"iliketogotothefarmersmarketevery Launchingobjectsintothestratospherefortastetests
weekend.iamcommittedtotheenvironment.ilive seemsabitfrivolousgiventhebroaderchallengesour
innashville.iplaymusicinmysparetime.iworkin planetfacesScores:4.68,4.89,5.54.
academiaatalocaluniversity.However,ihaveoppo-
siteideatothisvideo.Willleavedifferentcomment Acknowledgments
withdifferentperspective." Shao-HuaSunwassupportedbytheYushanFellowProgramby
Comment:"Lovethediversityofexperiencesyou theMinistryofEducation,Taiwan.
sharedfromtheacademictothesocial,butlet’salso
chataboutsustainabletravelandhowwecanenjoy References
citiesresponsibly.Greetingsfromanotheracademichere
[1] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,
inNashville.Let’skeeppushingforagreenerearthwhile CLawrenceZitnick,andDeviParikh.2015.Vqa:Visualquestionanswering.In
weexplore!" ProceedingsoftheIEEEinternationalconferenceoncomputervision.2425–2433.
[2] ArjunPanicksseryandSamuelR.BowmanandShiFeng.2024.LLMEvaluators
VideoTitle:Review:GalaxyS24Ultra-GalaxyAI RecognizeandFavorTheirOwnGenerations. arXiv:2404.13076[cs.CL] https:
ComprehensiveTestandSmartphonesComparison. //arxiv.org/abs/2404.13076
[3] AndreiBarbu,AlexanderBridge,ZacharyBurchill,DanCoroian,SvenDickinson,
Persona:"ireallyenjoyreadingthenews.ilovenew
SanjaFidler,AaronMichaux,SamMussman,SiddharthNarayanaswamy,Dhaval
technology.itrytofindeasywaystomakemoney.i Salvi,LaraSchmidt,JiangnanShangguan,JeffreyMarkSiskind,JarrellWaggoner,
wishicouldplaythestockmarketwell.iliketocreate SongWang,JinlianWei,YifanYin,andZhiqiZhang.2012.VideoInSentences
Out. arXiv:1204.2742[cs.CV] https://arxiv.org/abs/1204.2742
websites."
[4] KobusBarnard.2016. ComputationalMethodsforIntegratingVisionand
Comments:"Wow,thisreviewblewmymind!The Language. SynthesisLecturesonComputerVision6(042016),1–227. https:
wayAIischanginghowweusesmartphones,espe- //doi.org/10.2200/S00705ED1V01Y201602COV007
[5] KarimBenharrak,TimZindulka,FlorianLehmann,HendrikHeuer,andDaniel
ciallyforsomeonelikemewho’salwaysonthelook- Buschek.2024.Writer-DefinedAIPersonasforOn-DemandFeedbackGeneration.
outfornewtechandeasywaystomaximizeefficiency, InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems
(CHI’24,Vol.11).ACM,1–18. https://doi.org/10.1145/3613904.3642406
isfascinating.Techlikethiscouldreallyshakeupthe
[6] Simion-VladBogolin,IoanaCroitoru,andMariusLeordeanu.2020. Ahierar-
stockmarketfortechcompanies!" chicalapproachtovision-basedlanguagegeneration:fromsimplesentencesto
complexnaturallanguage.InProceedingsofthe28thInternationalConferenceon
G Crowd-SourcedCommentSamples ComputationalLinguistics,DoniaScott,NuriaBel,andChengqingZong(Eds.).In-
ternationalCommitteeonComputationalLinguistics,Barcelona,Spain(Online),
WedemonstratePersona-basedcomments,Realcommentsusedfor 2436–2447. https://doi.org/10.18653/v1/2020.coling-main.220
[7] TomBBrown.2020. Languagemodelsarefew-shotlearners. arXivpreprint
thisevaluationandtheirscores.Threescorearelistedintheorder
arXiv:2005.14165(2020).
of Relevance,BelievabilityandHelpfulness. [8] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,Eric
Horvitz,EceKamar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,Harsha
G.1 RealComments Nori,HamidPalangi,MarcoTulioRibeiro,andYiZhang.2023.SparksofArtificial
GeneralIntelligence:EarlyexperimentswithGPT-4. arXiv:2303.12712[cs.CL]
Video Title: We Sent Garlic Bread to the Edge of https://arxiv.org/abs/2303.12712
[9] KhyathiChandu,ShrimaiPrabhumoye,RuslanSalakhutdinov,andAlanW
Space,ThenAteIt. Black.2019. “MyWayofTellingaStory”:PersonabasedGroundedStory
Comments:That’swhyalienshatepeople.Weact Generation.InProceedingsoftheSecondWorkshoponStorytelling,FrancisFer-
raro,Ting-Hao‘Kenneth’Huang,StephanieM.Lukin,andMargaretMitchell
likewearegivingthemgarlicbreadandtookitback.
(Eds.).AssociationforComputationalLinguistics,Florence,Italy,11–21. https:
Itmakesthemlooklikefool.Scores:4.32,3.86,3.5. //doi.org/10.18653/v1/W19-3402
13[10] SorinaChelaru,ClaudiaOrellana-Rodriguez,andIsmailSengorAltingovde.2014. [29] MinaHuh,SaelyneYang,andYi-HaoPeng.[n.d.].Xiang’Anthony’Chen,Young-
HowusefulissocialfeedbackforlearningtorankYouTubevideos?WorldWide HoKim,andAmyPavel.2023.AVscript:AccessibleVideoEditingwithAudio-
Web17,5(2014),997–1025. https://doi.org/10.1007/s11280-013-0258-9 VisualScripts.InProceedingsofthe2023CHIConferenceonHumanFactorsin
[11] LinChen,XilinWei,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,Zehui ComputingSystems.1–17.
Chen,HaodongDuan,BinLin,ZhenyuTang,LiYuan,YuQiao,DahuaLin,Feng [30] HangJiang,XiajieZhang,XuboCao,CynthiaBreazeal,DebRoy,andJadKabbara.
Zhao,andJiaqiWang.2024.ShareGPT4Video:ImprovingVideoUnderstanding 2024.PersonaLLM:InvestigatingtheAbilityofLargeLanguageModelstoExpress
andGenerationwithBetterCaptions. arXiv:2406.04325[cs.CV] https://arxiv. PersonalityTraits. arXiv:2305.02547[cs.CL] https://arxiv.org/abs/2305.02547
org/abs/2406.04325 [31] josepmartins.[n.d.].boring-avatar. https://github.com/boringdesigners/boring-
[12] AhmedKharrufaColinDodds.2024.Show-and-Tell:AnInterfaceforDelivering avatars
RichFeedbackuponCreativeMediaArtefacts. MultimodalTechnol.Interact [32] TaehyeongKim,Min-OhHeo,SeonilSon,Kyoung-WhaPark,andByoung-Tak
(2024). https://www.mdpi.com/2414-4088/8/3/23 Zhang.2019.GLACNet:GLocalAttentionCascadingNetworksforMulti-image
[13] DigitalMarketingInstitute.2024. 14WaystoGrowYourYouTubeChannel. CuedStoryGeneration. arXiv:1805.10973[cs.CL] https://arxiv.org/abs/1805.
https://digitalmarketinginstitute.com/. https://digitalmarketinginstitute.com/ 10973
blog/10-ways-to-grow-your-youtube-channel-in-2018Accessed:2024-10-01. [33] AvrahamNKlugerandAngeloDeNisi.1996.Theeffectsoffeedbackinterventions
[14] PierreDognin,IgorMelnyk,YoussefMroueh,InkitPadhi,MattiaRigotti,Jarret onperformance:ahistoricalreview,ameta-analysis,andapreliminaryfeedback
Ross,YairSchiff,RichardAYoung,andBrianBelgodere.2022.Imagecaptioning interventiontheory.Psychologicalbulletin119,2(1996),254.
asanassistivetechnology:Lessonslearnedfromvizwiz2020challenge.Journal [34] SayduluKolasani.2023.OptimizingNaturalLanguageProcessing,LargeLan-
ofArtificialIntelligenceResearch73(2022),437–459. guageModels(LLMs)forEfficientCustomerService,andhyper-personalization
[15] ChaoqunDuan,LeiCui,ShumingMa,FuruWei,ConghuiZhu,andTiejun toenablesustainablegrowthandrevenue.TransactionsonLatestTrendsinArtifi-
Zhao. 2020. Multimodal Matching Transformer for Live Commenting. cialIntelligence4,4(2023). https://ijsdcs.com/index.php/TLAI/article/view/476
arXiv:2002.02649[cs.CL] https://arxiv.org/abs/2002.02649 [35] ChinmayE.Kulkarni,MichaelS.Bernstein,andScottR.Klemmer.2015.PeerStu-
[16] IlanaDuboviandIrisTabak.2020. Anempiricalanalysisofknowledgeco- dio:RapidPeerFeedbackEmphasizesRevisionandImprovesPerformance.In
constructioninYouTubecomments.Computers&Education156(2020),103939. ProceedingsoftheSecond(2015)ACMConferenceonLearning@Scale(Vancouver,
https://doi.org/10.1016/j.compedu.2020.103939 BC,Canada)(L@S’15).AssociationforComputingMachinery,NewYork,NY,
[17] AliFarhadi,MohsenHejrati,MohammadAminSadeghi,PeterYoung,Cyrus USA,75–84. https://doi.org/10.1145/2724660.2724670
Rashtchian,JuliaHockenmaier,andDavidForsyth.2010. EveryPictureTells [36] GirishKulkarni,VisruthPremraj,SagnikDhar,SimingLi,YejinChoi,AlexanderC
aStory:GeneratingSentencesfromImages.InComputerVision–ECCV2010, Berg,andTamaraLBerg.2011.Babytalk:Understandingandgeneratingsimple
KostasDaniilidis,PetrosMaragos,andNikosParagios(Eds.).SpringerBerlin imagedescriptions.InCVPR2011.1601–1608. https://doi.org/10.1109/CVPR.2011.
Heidelberg,Berlin,Heidelberg,15–29. 5995466
[18] ShansanGong,MukaiLi,JiangtaoFeng,ZhiyongWu,andLingPengKong.2022. [37] PolinaKuznetsova,VicenteOrdonez,AlexanderBerg,TamaraBerg,andYejin
Diffuseq:Sequencetosequencetextgenerationwithdiffusionmodels. arXiv Choi.2012.CollectiveGenerationofNaturalImageDescriptions.InProceedingsof
preprintarXiv:2210.08933(2022). the50thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume
[19] TanyaGoyal,JunyiJessyLi,andGregDurrett.2022. NewsSummarization 1:LongPapers),HaizhouLi,Chin-YewLin,MilesOsborne,GaryGeunbaeLee,
andEvaluationintheEraofGPT-3.ArXivabs/2209.12356(2022). https://api. andJongC.Park(Eds.).AssociationforComputationalLinguistics,JejuIsland,
semanticscholar.org/CorpusID:252532176 Korea,359–368. https://aclanthology.org/P12-1038
[20] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. [38] MinaLee,PercyLiang,andQianYang.2022.CoAuthor:DesigningaHuman-AI
2017. Makingthevinvqamatter:Elevatingtheroleofimageunderstanding CollaborativeWritingDatasetforExploringLanguageModelCapabilities.In
invisualquestionanswering.InProceedingsoftheIEEEconferenceoncomputer Proceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems
visionandpatternrecognition.6904–6913. (NewOrleans,LA,USA)(CHI’22).AssociationforComputingMachinery,New
[21] SergioGuadarrama,NivedaKrishnamoorthy,GirishMalkarnenkar,Subhashini York,NY,USA,Article388,19pages. https://doi.org/10.1145/3491102.3502030
Venugopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2013. [39] MichaelJ.LeeandAmyJ.Ko.2011.Personifyingprogrammingtoolfeedback
YouTube2Text:RecognizingandDescribingArbitraryActivitiesUsingSemantic improvesnoviceprogrammers’learning.InProceedingsoftheSeventhInterna-
HierarchiesandZero-ShotRecognition.In2013IEEEInternationalConferenceon tionalWorkshoponComputingEducationResearch(Providence,RhodeIsland,
ComputerVision.2712–2719. https://doi.org/10.1109/ICCV.2013.337 USA)(ICER’11).AssociationforComputingMachinery,NewYork,NY,USA,
[22] DannaGurari,YinanZhao,MengZhang,andNilavraBhattacharya.2020.Cap- 109–116. https://doi.org/10.1145/2016911.2016934
tioningimagestakenbypeoplewhoareblind.InComputerVision–ECCV2020: [40] JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan.2016. A
16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXVII Diversity-PromotingObjectiveFunctionforNeuralConversationModels.In
16.Springer,417–434. Proceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociation
[23] PerttuHämäläinen,MikkeTavast,andAntonKunnari.2023.EvaluatingLarge forComputationalLinguistics:HumanLanguageTechnologies,KevinKnight,Ani
LanguageModelsinGeneratingSyntheticHCIResearchData:aCaseStudy.In Nenkova,andOwenRambow(Eds.).AssociationforComputationalLinguistics,
Proceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems SanDiego,California,110–119. https://doi.org/10.18653/v1/N16-1014
(Hamburg,Germany)(CHI’23).AssociationforComputingMachinery,NewYork, [41] Chin-YewLin.2004.Rouge:Apackageforautomaticevaluationofsummaries.
NY,USA,Article433,19pages. https://doi.org/10.1145/3544548.3580688 InTextsummarizationbranchesout.74–81.
[24] JohnHattieandHelenTimperley.2007.ThePowerofFeedback.ReviewofEdu- [42] Yen-TingLinandYun-NungChen.2023.LLM-Eval:UnifiedMulti-Dimensional
cationalResearch77,1(2007),81–112. https://doi.org/10.3102/003465430298487 AutomaticEvaluationforOpen-DomainConversationswithLargeLanguage
arXiv:https://doi.org/10.3102/003465430298487 Models. In Proceedings of the 5th Workshop on NLP for Conversational AI
[25] MariaHolmbom.2015. TheYouTuber:AQualitativeStudyofPopularContent (NLP4ConvAI2023),Yun-NungChenandAbhinavRastogi(Eds.).Associationfor
Creators.Dissertation.UmeåUniversity. https://urn.kb.se/resolve?urn=urn:nbn: ComputationalLinguistics,Toronto,Canada,47–58. https://doi.org/10.18653/v1/
se:umu:diva-105388 2023.nlp4convai-1.5
[26] MD.ZakirHossain,FerdousSohel,MohdFairuzShiratuddin,andHamidLaga. [43] YangLiu,DanIter,YichongXu,ShuohangWang,RuochenXu,andChenguang
2019.AComprehensiveSurveyofDeepLearningforImageCaptioning.ACM Zhu.2023. G-eval:Nlgevaluationusinggpt-4withbetterhumanalignment.
Comput.Surv.51,6,Article118(feb2019),36pages. https://doi.org/10.1145/ arXivpreprintarXiv:2303.16634(2023).
3295748 [44] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,
[27] De-An Huang, Vignesh Ramanathan, Dhruv Mahajan, Lorenzo Torresani, OyvindTafjord,PeterClark,andAshwinKalyan.2022.Learntoexplain:Multi-
ManoharPaluri,LiFei-Fei,andJuanCarlosNiebles.2018.WhatMakesaVideo modalreasoningviathoughtchainsforsciencequestionanswering.Advancesin
aVideo:AnalyzingTemporalInformationinVideoUnderstandingModelsand NeuralInformationProcessingSystems35(2022),2507–2521.
Datasets.InProceedingsoftheIEEEConferenceonComputerVisionandPattern [45] Shuming Ma, Lei Cui, Damai Dai, Furu Wei, and Xu Sun. 2018. LiveBot:
Recognition(CVPR). Generating Live Video Comments Based on Visual and Textual Contexts.
[28] Ting-HaoKennethHuang,FrancisFerraro,NasrinMostafazadeh,IshanMisra, arXiv:1809.04938[cs.CL] https://arxiv.org/abs/1809.04938
AishwaryaAgrawal,JacobDevlin,RossGirshick,XiaodongHe,PushmeetKohli, [46] AliMalik,MikeWu,VrindaVasavada,JinpengSong,MadisonCoots,John
DhruvBatra,C.LawrenceZitnick,DeviParikh,LucyVanderwende,Michel Mitchell,NoahGoodman,andChrisPiech.2021. GenerativeGrading:Near
Galley,andMargaretMitchell.2016.VisualStorytelling.InProceedingsofthe2016 Human-levelAccuracyforAutomatedFeedbackonRichlyStructuredProblems.
ConferenceoftheNorthAmericanChapteroftheAssociationforComputational arXiv:1905.09916[cs.LG] https://arxiv.org/abs/1905.09916
Linguistics:HumanLanguageTechnologies,KevinKnight,AniNenkova,and [47] Meta.2022. PersonaChat. https://www.kaggle.com/datasets/atharvjairath/
OwenRambow(Eds.).AssociationforComputationalLinguistics,SanDiego, personachatAccessed:2024-10-01.
California,1233–1239. https://doi.org/10.18653/v1/N16-1147 [48] PiotrMirowski,KoryW.Mathewson,JaylenPittman,andRichardEvans.2023.
Co-WritingScreenplaysandTheatreScriptswithLanguageModels:Evaluation
byIndustryProfessionals.InProceedingsofthe2023CHIConferenceonHuman
14FactorsinComputingSystems(Hamburg,Germany)(CHI’23).Associationfor [55] JoonSungPark,LindsayPopowski,CarrieCai,MeredithRingelMorris,Percy
ComputingMachinery,NewYork,NY,USA,Article355,34pages. https://doi. Liang,andMichaelS.Bernstein.2022. SocialSimulacra:CreatingPopulated
org/10.1145/3544548.3581225 PrototypesforSocialComputingSystems.InProceedingsofthe35thAnnualACM
[49] MargaretMitchell,XufengHan,JesseDodge,AlyssaMensch,AmitGoyal,Alex SymposiumonUserInterfaceSoftwareandTechnology(Bend,OR,USA)(UIST’22).
Berg,KotaYamaguchi,TamaraBerg,KarlStratos,andHalDaumé.2012.Midge: AssociationforComputingMachinery,NewYork,NY,USA,Article74,18pages.
generatingimagedescriptionsfromcomputervisiondetections.InProceedings https://doi.org/10.1145/3526113.3545616
ofthe13thConferenceoftheEuropeanChapteroftheAssociationforComputa- [56] AmyPavel,DanB.Goldman,BjörnHartmann,andManeeshAgrawala.2016.
tionalLinguistics(Avignon,France)(EACL’12).AssociationforComputational VidCrit:Video-basedAsynchronousVideoReview.InProceedingsofthe29th
Linguistics,USA,747–756. AnnualSymposiumonUserInterfaceSoftwareandTechnology(Tokyo,Japan)
[50] HamedNilforoshanandEugeneWu.2018.LeveragingQualityPredictionModels (UIST’16).AssociationforComputingMachinery,NewYork,NY,USA,517–528.
forAutomaticWritingFeedback.ProceedingsoftheInternationalAAAIConference https://doi.org/10.1145/2984511.2984552
onWebandSocialMedia12,1(Jun.2018). https://doi.org/10.1609/icwsm.v12i1. [57] LudovicaPiro,TommasoBianchi,LucaAlessandrelli,AndreaChizzola,Daniela
14998 Casiraghi,SusannaSancassani,andNicolaGatti.2024. MyLearningTalk:An
[51] OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,Ilge LLM-BasedIntelligentTutoringSystem.InWebEngineering,KostasStefanidis,
Akkaya,FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,Sam KariSystä,MaristellaMatera,SebastianHeil,HaridimosKondylakis,andElisa
Altman,ShyamalAnadkat,RedAvila,IgorBabuschkin,SuchirBalaji,ValerieBal- Quintarelli(Eds.).SpringerNatureSwitzerland,Cham,428–431.
com,PaulBaltescu,HaimingBao,MohammadBavarian,JeffBelgum,IrwanBello, [58] AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,
JakeBerdine,GabrielBernadett-Shapiro,ChristopherBerner,LennyBogdonoff, andIlyaSutskever.2022. RobustSpeechRecognitionviaLarge-ScaleWeak
OlegBoiko,MadelaineBoyd,Anna-LuisaBrakman,GregBrockman,TimBrooks, Supervision. arXiv:2212.04356[eess.AS]
MilesBrundage,KevinButton,TrevorCai,RosieCampbell,AndrewCann,Brit- [59] GonzaloRamosandRavinBalakrishnan.2003.Fluidinteractiontechniquesfor
tanyCarey,ChelseaCarlson,RoryCarmichael,BrookeChan,CheChang,Fotis thecontrolandannotationofdigitalvideo.InProceedingsofthe16thAnnual
Chantzis,DerekChen,SullyChen,RubyChen,JasonChen,MarkChen,Ben ACMSymposiumonUserInterfaceSoftwareandTechnology(Vancouver,Canada)
Chess,ChesterCho,CaseyChu,HyungWonChung,DaveCummings,Jeremiah (UIST’03).AssociationforComputingMachinery,NewYork,NY,USA,105–114.
Currier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,Damien https://doi.org/10.1145/964696.964708
Deville,ArkaDhar,DavidDohan,SteveDowling,SheilaDunning,AdrienEcoffet, [60] YunfanShao,LinyangLi,JunqiDai,andXipengQiu.2023.Character-LLM:A
AttyEleti,TynaEloundou,DavidFarhi,LiamFedus,NikoFelix,SimónPosada TrainableAgentforRole-Playing. arXiv:2310.10158[cs.CL] https://arxiv.org/
Fishman,JustonForte,IsabellaFulford,LeoGao,ElieGeorges,ChristianGibson, abs/2310.10158
VikGoel,TarunGogineni,GabrielGoh,RaphaGontijo-Lopes,JonathanGor- [61] ZhiqiangShen,JianguoLi,ZhouSu,MinjunLi,YurongChen,Yu-GangJiang,and
don,MorganGrafstein,ScottGray,RyanGreene,JoshuaGross,ShixiangShane XiangyangXue.2017.Weaklysuperviseddensevideocaptioning.InProceedings
Gu,YufeiGuo,ChrisHallacy,JesseHan,JeffHarris,YuchenHe,MikeHeaton, oftheIEEEConferenceonComputerVisionandPatternRecognition.1916–1924.
JohannesHeidecke,ChrisHesse,AlanHickey,WadeHickey,PeterHoeschele, [62] StefanSiersdorfer,SergiuChelaru,WolfgangNejdl,andJoseSanPedro.2010.
BrandonHoughton,KennyHsu,ShengliHu,XinHu,JoostHuizinga,Shantanu Howusefulareyourcomments?analyzingandpredictingyoutubecomments
Jain,ShawnJain,JoanneJang,AngelaJiang,RogerJiang,HaozhunJin,Denny andcommentratings.InProceedingsofthe19thInternationalConferenceon
Jin,ShinoJomoto,BillieJonn,HeewooJun,TomerKaftan,ŁukaszKaiser,Ali WorldWideWeb(Raleigh,NorthCarolina,USA)(WWW’10).Associationfor
Kamali,IngmarKanitscheider,NitishShirishKeskar,TabarakKhan,LoganKil- ComputingMachinery,NewYork,NY,USA,891–900. https://doi.org/10.1145/
patrick,JongWookKim,ChristinaKim,YongjikKim,JanHendrikKirchner, 1772690.1772781
JamieKiros,MattKnight,DanielKokotajlo,ŁukaszKondraciuk,AndrewKon- [63] MarkoSmilevski,IlijaLalkovski,andGjorgjiMadjarov.2018.StoriesforImages-
drich,ArisKonstantinidis,KyleKosic,GretchenKrueger,VishalKuo,Michael in-SequencebyUsingVisualandNarrativeComponents.SpringerInternational
Lampe,IkaiLan,TeddyLee,JanLeike,JadeLeung,DanielLevy,ChakMing Publishing,148–159. https://doi.org/10.1007/978-3-030-00825-3_13
Li,RachelLim,MollyLin,StephanieLin,MateuszLitwin,TheresaLopez,Ryan [64] JingkuanSong,YuyuGuo,LianliGao,XuelongLi,AlanHanjalic,andHengTao
Lowe,PatriciaLue,AnnaMakanju,KimMalfacini,SamManning,TodorMarkov, Shen.2018.Fromdeterministictogenerative:MultimodalstochasticRNNsfor
YanivMarkovski,BiancaMartin,KatieMayer,AndrewMayne,BobMcGrew, videocaptioning.IEEEtransactionsonneuralnetworksandlearningsystems30,
ScottMayerMcKinney,ChristineMcLeavey,PaulMcMillan,JakeMcNeil,David 10(2018),3047–3058.
Medina,AalokMehta,JacobMenick,LukeMetz,AndreyMishchenko,Pamela [65] SSA.2023.USABabyNameDataset. https://www.ssa.gov/OACT/babynames/
Mishkin,VinnieMonaco,EvanMorikawa,DanielMossing,TongMu,MiraMurati, limits.html
OlegMurk,DavidMély,AshvinNair,ReiichiroNakano,RajeevNayak,Arvind [66] MarieStevensonandAekPhakiti.2014. Theeffectsofcomputer-generated
Neelakantan,RichardNgo,HyeonwooNoh,LongOuyang,CullenO’Keefe,Jakub feedbackonthequalityofwriting.AssessingWriting19(2014),51–65. https:
Pachocki,AlexPaino,JoePalermo,AshleyPantuliano,GiambattistaParascan- //doi.org/10.1016/j.asw.2013.11.007FeedbackinWriting:IssuesandChallenges.
dolo,JoelParish,EmyParparita,AlexPassos,MikhailPavlov,AndrewPeng, [67] ChunChetTan,Yu-GangJiang,andChong-WahNgo.2011.Towardstextually
AdamPerelman,FilipedeAvilaBelbutePeres,MichaelPetrov,HenriquePonde describingcomplexvideocontentswithaudio-visualconceptclassifiers.InPro-
deOliveiraPinto,Michael,Pokorny,MichellePokrass,VitchyrH.Pong,Tolly ceedingsofthe19thACMInternationalConferenceonMultimedia(Scottsdale,
Powell,AletheaPower,BorisPower,ElizabethProehl,RaulPuri,AlecRadford, Arizona,USA)(MM’11).AssociationforComputingMachinery,NewYork,NY,
JackRae,AdityaRamesh,CameronRaymond,FrancisReal,KendraRimbach, USA,655–658. https://doi.org/10.1145/2072298.2072411
CarlRoss,BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,TedSanders, [68] MingkangTang,ZhanyuWang,ZhenhuaLIU,FengyunRao,DianLi,andXiu
ShibaniSanturkar,GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchul- Li.2021. CLIP4Caption:CLIPforVideoCaption.InProceedingsofthe29th
man,DanielSelsam,KylaSheppard,TokiSherbakov,JessicaShieh,SarahShoker, ACMInternationalConferenceonMultimedia(VirtualEvent,China)(MM’21).
PranavShyam,SzymonSidor,EricSigler,MaddieSimens,JordanSitkin,Katarina AssociationforComputingMachinery,NewYork,NY,USA,4858–4862. https:
Slama,IanSohl,BenjaminSokolowsky,YangSong,NatalieStaudacher,FelipePet- //doi.org/10.1145/3474085.3479207
roskiSuch,NatalieSummers,IlyaSutskever,JieTang,NikolasTezak,MadeleineB. [69] MikeThelwall,PardeepSud,andFaridaVis.2012. CommentingonYouTube
Thompson,PhilTillet,AminTootoonchian,ElizabethTseng,PrestonTuggle, videos:FromGuatemalanrocktoelbigbang.JournaloftheAmericansocietyfor
NickTurley,JerryTworek,JuanFelipeCerónUribe,AndreaVallone,ArunVi- informationscienceandtechnology63,3(2012),616–629.
jayvergiya,ChelseaVoss,CarrollWainwright,JustinJayWang,AlvinWang, [70] ThematicAnalysisInc.2024.ThematicCommentAnalysis. https://getthematic.
BenWang,JonathanWard,JasonWei,CJWeinmann,AkilaWelihinda,Peter com/product/comment-analyzer/Accessed:2024-10-01.
Welinder,JiayiWeng,LilianWeng,MattWiethoff,DaveWillner,ClemensWinter, [71] Yu-MinTseng,Yu-ChaoHuang,Teng-YunHsiao,Wei-LinChen,Chao-WeiHuang,
SamuelWolrich,HannahWong,LaurenWorkman,SherwinWu,JeffWu,Michael YuMeng,andYun-NungChen.2024.TwoTalesofPersonainLLMs:ASurvey
Wu,KaiXiao,TaoXu,SarahYoo,KevinYu,QimingYuan,WojciechZaremba, ofRole-PlayingandPersonalization. arXiv:2406.01171[cs.CL] https://arxiv.org/
RowanZellers,ChongZhang,MarvinZhang,ShengjiaZhao,TianhaoZheng, abs/2406.01171
JuntangZhuang,WilliamZhuk,andBarretZoph.2023.GPT-4TechnicalReport. [72] TessVanDaele,AkhilIyer,YuningZhang,JalynCDerry,MinaHuh,andAmy
arXiv:2303.08774[cs.CL] Pavel.2024.MakingShort-FormVideosAccessiblewithHierarchicalVideoSum-
[52] JimOwens.2023. VideoProductionHandbook(7thed.). Routledge. https: maries.InProceedingsofthe2024CHIConferenceonHumanFactorsinComputing
//doi.org/10.4324/9781003251323 Systems(Honolulu,HI,USA)(CHI’24).AssociationforComputingMachinery,
[53] KeivalyaPandyaandMehfuzaHolia.2023. AutomatingCustomerServiceus- NewYork,NY,USA,Article895,17pages. https://doi.org/10.1145/3613904.
ingLangChain:Buildingcustomopen-sourceGPTChatbotfororganizations. 3642839
arXiv:2310.05421[cs.CL] https://arxiv.org/abs/2310.05421 [73] VEED.2024.VEED. https://www.veed.io/Accessed:2024-10-01.
[54] JoonSungPark,JosephC.O’Brien,CarrieJ.Cai,MeredithRingelMorris,Percy [74] BryanWang,YuliangLi,ZhaoyangLv,HaijunXia,YanXu,andRajSodhi.2024.
Liang,andMichaelS.Bernstein.2023.GenerativeAgents:InteractiveSimulacra LAVE:LLM-PoweredAgentAssistanceandLanguageAugmentationforVideo
ofHumanBehavior. arXiv:2304.03442[cs.HC] Editing. arXiv:2402.10294[cs.HC]
15[75] BryanWang,YuliangLi,ZhaoyangLv,HaijunXia,YanXu,andRajSodhi.2024. [82] Kuo-HaoZeng,Tseng-HungChen,Ching-YaoChuang,Yuan-HongLiao,JuanCar-
LAVE:LLM-PoweredAgentAssistanceandLanguageAugmentationforVideo losNiebles,andMinSun.2017.LeveragingVideoDescriptionstoLearnVideo
Editing. arXiv:2402.10294[cs.HC] https://arxiv.org/abs/2402.10294 QuestionAnswering.ProceedingsoftheAAAIConferenceonArtificialIntelligence
[76] YaqingWang,QuanmingYao,JamesTKwok,andLionelMNi.2020.Generalizing 31,1(Feb.2017). https://doi.org/10.1609/aaai.v31i1.11238
fromafewexamples:Asurveyonfew-shotlearning.ACMcomputingsurveys [83] ZehuaZeng,NengGao,CongXue,andChenyangTu.2021.PLVCG:APretraining
(csur)53,3(2020),1–34. BasedModelforLiveVideoCommentGeneration.InAdvancesinKnowledgeDis-
[77] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi, coveryandDataMining,KamalKarlapalem,HongCheng,NarenRamakrishnan,
QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning R.K.Agrawal,P.KrishnaReddy,JaideepSrivastava,andTanmoyChakraborty
inlargelanguagemodels.Advancesinneuralinformationprocessingsystems35 (Eds.).SpringerInternationalPublishing,Cham,690–702.
(2022),24824–24837. [84] ShuZhang,XinyiYang,YihaoFeng,CanQin,Chia-ChihChen,NingYu,Zeyuan
[78] Hao Wu, Gareth J. F. Jones, and Francois Pitie. 2020. Response to Live- Chen,HuanWang,SilvioSavarese,StefanoErmon,CaimingXiong,andRanXu.
Bot:GeneratingLiveVideoCommentsBasedonVisualandTextualContexts. 2024. HIVE:HarnessingHumanFeedbackforInstructionalVisualEditing.In
arXiv:2006.03022[cs.CL] https://arxiv.org/abs/2006.03022 ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
[79] MichihiroYasunaga,JureLeskovec,andPercyLiang.2022.Linkbert:Pretraining (CVPR).9026–9036.
languagemodelswithdocumentlinks.arXivpreprintarXiv:2203.15827(2022). [85] TianyiZhang*,VarshaKishore*,FelixWu*,KilianQ.Weinberger,andYoavArtzi.
[80] DongwookYoon,NicholasChen,FrançoisGuimbretière,andAbigailSellen. 2020.BERTScore:EvaluatingTextGenerationwithBERT.InInternationalConfer-
2014. RichReview:blendingink,speech,andgesturetosupportcollaborative enceonLearningRepresentations.https://openreview.net/forum?id=SkeHuCVFDr
documentreview.InProceedingsofthe27thAnnualACMSymposiumonUser [86] YaomingZhu,SidiLu,LeiZheng,JiaxianGuo,WeinanZhang,JunWang,and
InterfaceSoftwareandTechnology(Honolulu,Hawaii,USA)(UIST’14).Association YongYu.2018.Texygen:Abenchmarkingplatformfortextgenerationmodels.
forComputingMachinery,NewYork,NY,USA,481–490. https://doi.org/10. InThe41stinternationalACMSIGIRconferenceonresearch&developmentin
1145/2642918.2647390 informationretrieval.1097–1100.
[81] YouTube.2023.MadeonYouTube. https://blog.youtube/news-and-events/made-
on-youtube-2023/
16