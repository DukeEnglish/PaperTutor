Sparse Bayesian Generative Modeling
for Compressive Sensing
BenediktBöck, SadafSyed, WolfgangUtschick
TUMSchoolofComputation,InformationandTechnology
TechnicalUniversityofMunich
{benedikt.boeck,sadaf.syed,utschick}@tum.de
Abstract
Thisworkaddressesthefundamentallinearinverseproblemincompressivesensing
(CS) by introducing a new type of regularizing generative prior. Our proposed
methodutilizesideasfromclassicaldictionary-basedCSand,inparticular,sparse
Bayesianlearning(SBL),tointegrateastrongregularizationtowardssparsesolu-
tions. Atthesametime,byleveragingthenotionofconditionalGaussianity,italso
incorporatestheadaptabilityfromgenerativemodelstotrainingdata. However,
unlikemoststate-of-the-artgenerativemodels,itisabletolearnfromafewcom-
pressedandnoisydatasamplesandrequiresnooptimizationalgorithmforsolving
theinverseproblem. Additionally,similartoDirichletpriornetworks,ourmodel
parameterizesaconjugatepriorenablingitsapplicationforuncertaintyquantifi-
cation. Wesupportourapproachtheoreticallythroughtheconceptofvariational
inferenceandvalidateitempiricallyusingdifferenttypesofcompressiblesignals.
1 Introduction
ResearchinCShasshownthatitispossibletoreducethenumberofmeasurementsfarbelowtheone
determinedbytheNyquistsamplingtheoremwhilestillbeingabletoextracttheinformation-carrying
signalfromtheacquiredobservations. ThefundamentalprobleminCSisanill-posedlinearinverse
problem,i.e.,thegoalistorecoverthesignalx RN ofinterestfromanunder-determinedsetof
∗
measurementsy RM withM N,relatedby∈
∈ ≪
y =Ax ∗+n, (1)
wherex iscompressedbythemeasurementmatrixAandpotentiallycorruptedbyadditivenoise
∗
n (0,σ2I). Sincetheunder-determinedobservationydoesnotcarryenoughinformationalone
∼N n
tofaithfullyreconstructx , additionalprior(ormodel)knowledgeaboutx isrequiredtomake
∗ ∗
theinverseproblem“well-posed”. ClassicalCSalgorithmssuchasLassoregressionororthogonal
matching pursuit (OMP) address this problem by incorporating the model knowledge that x is
∗
sparseorcompressiblewithrespecttosomedictionary[1]. Nowadays,moderndeeplearning(DL)-
basedapproachessuchasunfoldingalgorithms,generativemodel-basedCSandun-trainedneural
networks(NNs)expandthepossibilitiesbylearningpriorknowledgefromatrainingsetordesigning
the network architecture to be biased towards a certain class of signals [2–6]. These approaches
typicallyrequirea(potentiallylarge)trainingsetofground-truthdatasamples,ortheirarchitectureis
specificallybiasedtowardsnaturalimages. However,inmanyapplications,ground-truthtraining
datamightnotbeeasilyaccessible. In,e.g.,electronmicroscopy,theamountofelectrondosehasto
berestrictedtonotinducedamageontheproberesultinginlow-contrastnoisydatasamples[7]. The
sensorsinwearableelectrocardiography(ECG)monitoringdevicesgenerallyprovidenoisysignals
withartifacts[8],whichlimitstheabilitytolearnfrompatient-specificdatainreal-worldsettings.
Anotherexampleisthewireless5Gcommunicationstandard,wheremobileusersreceivecompressed
andnoisyso-calledchannelobservationsonafrequentbasis(cf. [9])whileacquiringground-truth
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
voN
41
]LM.tats[
1v38490.1142:viXrachannelinformationrequirescostlymeasurementcampaigns. Thus,inmanyapplications,itiseither
impossibleorprohibitivelyexpensivetocollectlotsofground-truthtrainingdatawhilecorrupted
dataisreadilyavailable. Thishighlightsthenecessityformethodsthatcanlearnfromcorrupteddata
samples.
Inthispaper,weproposeanewlearnablepriorforsolvingtheinverseproblem(1),whichcanlearn
fromonlyafewcompressedandnoisydatasamplesand,thus,requiresnoground-truthinformation
in its training phase. Additionally, it applies to any type of signal, which is compressible with
respecttosomedictionary. Ourapproachsharessimilaritieswithgenerativemodel-basedCS[4,10].
There,agenerativemodelisfirsttrainedtocapturetheexactunderlyingpriordistributionp(x)of
thesignalx ofinterest. Itthenservesasprobabilisticpriortoregularizetheinverseproblem(1).
∗
Similarly,classicalCSalgorithmslikeLassoregressionalsoimposeaprobabilisticprioronthesparse
representationofx withrespecttosomedictionary[11]. Incontrasttomoderngenerativemodels,
∗
however,thesepriorsdonothaveanygenerationcapabilitiesbutsolelybiastheinverseproblem(1)
towardssparsesolutions. Thisobservationindicatesthataprobabilisticpriordoesnotnecessarily
needtocapturetheexactpriordistributionp(x)toeffectivelyregularize(1). Theproposedmodelin
thisworkbuildsuponthisinsightandformsatrainablebutsimultaneouslysparsity-inducingprior.
Forthat,weaimtocombinetheadaptabilityofgenerativemodelstotrainingdatawiththeproperty
ofmanytypesofsignalsx tobecompressiblewithrespecttosomedictionary. Asaresult, our
∗
modellearnsstatisticalinformationinthesignal’ssparse/compressibledomain. Examplesofsignals
withaspecificstatisticalstructureintheircompressiblerepresentationincludepiecewisesmooth
functionsandnaturalimages,whosewaveletcoefficientsapproximatelybuildaconnectedsub-treeor
wirelesschannels,whichareburst-sparseintheirangulardomain[12,13].
RelatedWork. EarlyworkonCS,whichconsidersstatisticalstructureinthewaveletdomainof
images, isgivenin[14,15]. Basedonthetheoreticalfoundationin[16]andtheconceptofSBL
[17,18],thesepapersintroduceahierarchicalBayesianmodel,whichisusedtoapplyvariational
inferenceandMarkovchainMonteCarlo(MCMC)posteriorsamplingtosolve(1).TrainingGaussian
mixturemodels(GMMs),i.e.,classicalgenerativemodels,fromcompressedimagepatchesofone
or a few images has been analyzed in [19–22]. In this line of research, however, the GMM is
fit directly in the pixel domain. Compressive dictionary learning represents a different line of
researchaimingtolearnthedictionaryfromsolelycompresseddata[23–26],andstronglybaseson
thedictionarylearningmethodK-SVD[27]. Morerecently,variationalautoencoders(VAEs)and
generativeadversarialnetworks(GANs)havebeenstudiedinthecontextofCS[4,10]. There,the
VAE/GANisusedtosolvetheinverseproblem(1)byconstrainingthesignalx ofinteresttolie
∗
intherangeofthegenerativemodelinsteadofbeingsparsewithrespecttosomedictionary. Inthis
context,AmbientGANaswellasCSGANareextensionsthatloosenupthetrainingsetrequirements
andcanlearnfromcorrupteddata[28,29]. Anotherrelatedtopicistheabilityofsomegenerative
models,i.e.,diffusionmodels,VAEsandGMMs,toprovideanapproximationoftheconditional
meanestimator(CME)[30–32]. Inthecaseofthelattertwo,theCMEisrepresentedasatractable
convexcombinationoflinearminimummeansquarederror(MSE)estimatorsbyexploitingtheir
conditionalGaussianityonalatentspacethatdeterminestheseestimators’meansandcovariances.
Ourmaincontributionsareasfollows:
• Weintroduceanewtypeofsparsity-inducinggenerativepriorfortheinverseproblem(1),
whichdiffersfromclassicalCSalgorithmsduetoitsabilitytolearnfromdata. Ontheother
hand,italsodiffersfromothermodernNN-basedapproachesduetoitsabilitytopromote
sparsityinthesignal’scompressibledomain. Moreover,itcanlearnfromafewcorrupted
datasamplesand,thus,requiresnoground-truthinformationinitstrainingphase.
• Wetheoreticallyunderpinourapproachbyprovingthatitstrainingmaximizesavariational
lowerboundofasparsityinducinglog-evidence.
• BuildingonthenotionofconditionalGaussianity,weintroducetwospecificimplementa-
tionsoftheproposedtypeofpriorbasedonVAEsandGMMs, whichdonotrequirean
optimizationalgorithmintheirinferencephaseandcomewithcomputationalbenefits.
• ByexploitingthesharedpropertywithDirichletpriornetworkstoparameterizeaconjugate
prior,wedemonstratehowourapproachcanbeappliedforuncertaintyquantification.
• Wevalidatetheperformanceondatasetscontainingdifferenttypesofcompressiblesignals.1
1Sourcecodeisavailableathttps://github.com/beneboeck/sparse-bayesian-gen-mod.
22 BackgroundandMethod
Notation. Theoperationsb −1,√band b2denotetherespectiveelement-wiseoperationforthe
| |
vector b. Moreover, diag(B) represents the vectorized diagonal of the matrix B, while diag(b)
denotesthediagonalmatrixwiththevectorbonitsdiagonal.
2.1 ProblemFormulation
WeconsiderthetypicalCSsetup,inwhichwemeasureN-dimensionalground-truthsamplesx
i
by a known measurement matrix A RM N (or A RM N) with M N, where each
× i ×
∈ ∈ ≪
observationispotentiallycorruptedbynoisen drawnfromadditivewhiteGaussiannoise(AWGN)
i
n (0,σ2I). Weassumeallx tobecompressiblewithrespecttoaknowndictionarymatrix
D∼ NRN Sn , i.e., y = ADs +i n with x = Ds and s RS being approximately sparse.
× i i i i i i
∈ ∈
All s are assumed to be independent and identically distributed (i.i.d.), i.e., s p(s), where
i i
∼
p(s)isunknownwithsexhibitingnon-triviallydependententries. Typicalsignalswhichfitinto
this category are, e.g., natural images, piecewise smooth functions, and wireless channels (cf.
Section 1). Our approach in this work allows to either solely have access to corrupted training
observations = y Nt ortrainingtupleswithground-truthinformation = (s ,y ) Nt or
Y { i }i=1 G { i i }i=1
= (x ,y ) Nt . Wefirsttraintheproposedmodelusing (or,alternatively, or )toserveas
W { i i }i=1 Y G W
aneffectivepriorfor(1). Ourgoalisthentoestimateaground-truthsignalx ofanewlyobservedy.
∗
Wealsodefine = x Nt and = s Nt .
X { i }i=1 S { i }i=1
2.2 SparseBayesianLearningforCompressiveSensing
InSBLforCS,theideaforsolvingtheinverseproblem(1)istoassignaparameterizedpriortos ,
∗
i.e.,thecompressiblerepresentationofthesignalx ofinterestwithx =Ds [18]. Itassumes
∗ ∗ ∗
y s ∗ p(y s ∗)= (y;ADs ∗,σ2I), s ∗ p γ(s)= (s;0,diag(γ)). (2)
| ∼ | N ∼ N
Givenasingleobservationy,theparametersγ (andsometimesσ2)areestimatedbyanexpectation-
maximization (EM) algorithm maximizing the corresponding log-evidence logp (y) implicitly
γ
defined by (2). After that, it is utilized that p (s) forms a conjugate prior of p(y s ) resulting
γ ∗
|
inaclosed-formposteriorp (xy)providingallnecessaryinformationtoestimatex . In[18],a
γ ∗
|
variationalinterpretationofSBLisgiven,addressingwhythisapproachyieldssparseresults,even
thoughp (s)doesnotinherentlypromotesparsity. ItisshownthatthereexistsaC >0suchthatfor
γ
allγ >0ands
N
(cid:89) 1
p (s) t(s)=C . (3)
γ
≤ · s
i
i=1| |
Thefunctiont(s),however,isawell-knownimproperbutsparsity-inducingprior,usedasanon-
informativepriorforscaleparameters[33]. Letanotherstatisticalmodelbegivenbyp(y s )(cf. (2))
∗
|
andpriort(s)insteadofp (s)withimplicitlydefinedlog-evidencelogπ(s)(y). Duetot(s)being
γ
improper,thelog-evidencelogπ(s)(y)formsanotnormalizedbutvalidlog-likelihood. Moreover,
thismodelissparsity-inducingduetothesparsitypromotingcharacteristicsoft(s). Basedon(3),it
holdsthatlogπ(s)(y) logp (y)and,thus,logp (y)canbeembeddedinvariationalinference,
γ γ
≥
forming a tractable variational lower bound of an intractable log-evidence logπ(s)(y) of actual
interest. Moreover,applyingtheEMalgorithmto(2)is“evidencemaximizationoverthespaceof
variationalapproximationstoamodel(i.e.,p(y s )andt(s)),withasparse,regularizingprior”[18].
∗
|
2.3 GaussianMixtureModelsandVariationalAutoencoders
GMMsandVAEsaregenerativemodelsaimingtolearnanunknowndistributionp(x)fromatraining
set . Bothmodelsrepresentp(x)asthemarginalizationwithconditionallyGaussianp(x )andan
X |·
additionallatentvariablek(orz)[34–36],i.e.,
K K
(cid:88) (cid:88)
p(GMM)(x)= p(k)p(xk)= ρ (x;µ ,C ), (4)
k k k
| N
k=1 k=1
(cid:90) (cid:90)
p(VAE)(x)= p(z)p (xz)dz = (z;0,I) (x;µ (z),C (z))dz. (5)
θ θ θ
| N N
3Thetunableparameters( ρ ,µ ,C K forGMMsandθ forVAEs)arelearnedbyoptimizing
the model’s
log-evidence{(cid:80)k lok gp(fk )} (k x=1
) (f GMM,VAE ) over . In the case of GMMs,
i i ∈ { } X
this is typically done by an EM algorithm alternating between the so-called e- and m-step. The
e-stepdeterminestheclosed-formposteriorsp (k x )(foralli)viatheBayesruleusingthemodel’s
t i
|
parameters in the tth iteration. The m-step then updates the model’s parameters by maximizing
(cid:80) E [logp(x ,k)]. IncaseofVAEs,however,theposteriorp (z x)(andlogp(VAE)(x))are
i pt(k |xi) i θ |
intractableandtheEMalgorithmcannotbeapplied. Therefore,atractabledistributionq (z x)=
ϕ
|
(z;µ (x),diag(σ2(x))withvariationalparametersϕisintroduced,whichapproximatesp (z x),
N ϕ ϕ θ |
andµ (x)andσ2(x)aregeneratedbyaNNencoder. Equivalently,µ (z)andC (z)aregenerally
ϕ ϕ θ θ
realized by a NN decoder. The objective to be maximized is the evidence lower bound (ELBO)
(cid:80)
L(θ,ϕ)servingasatractablelowerboundfortheintractablelog-evidence logp (x ),i.e.,
i θ i
(cid:88) (cid:88)
logp (x ) L(θ,ϕ)= (logp (x ) D (q (z x ) p (z x ))) (6)
θ i θ i KL ϕ i θ i
≥ − | || |
xi∈X xi∈X
withD ()beingtheKullback-Leibler(KL)divergence. Generally,theGMM’sandVAE’straining
KL
·
criticallydependsoniterativelycharacterizingandupdatingtheirposteriorsp (k x )andq (z x ).
t i ϕ i
| |
2.4 ProposedMethod
Thegoalofthissectionistoderiveaclassofgenerativemodels,forwhichwecanguaranteethat,on
theonehand,itissparsityinducing,butontheotherhand,itistrainableandcanlearnfromsolely
compressedandnoisyobservations resultinginaneffectiveprobabilisticpriorfor(1).
Y
Toincorporatethebiastowardssparsity,westartwithSBLdiscussedinSection2.2andcombineit
withtheVAE’sandGMM’smainprinciplefortheiradaptabilitytocomplicateddistributions,i.e.,
introducingalatentvariablez(ork)onwhichweconditionwithaparameterizedGaussian(cf.(4)and
(5)).2 Morespecifically,weexploittheGaussianityofp (s)in(3)andmodifyittoaparameterized
γ
Gaussianconditionedonsomelatentvariablezwitharbitrarilyparameterizedp (z)whileexplicitly
δ
keepingitsmeanzeroanditscovariancematrixdiagonal,i.e.,p (sz)= (s;0,diag(γ (z))). The
θ θ
| N
resultingsetofstatisticalmodelsreferredtoassparseBayesiangenerativemodels,isgivenby
y s p(y s)= (y;ADs,σ2I), sz p (sz)= (s;0,diag(γ (z))), z p (z). (7)
θ θ δ
| ∼ | N | ∼ | N ∼
Trainingprinciple. Ourproposedtrainingschemeisindependentofspecificrealizationsforγ θ(z)
andp (z),whichiswhytheyarekeptgeneralinthissection.Specificparameterizationsarediscussed
δ
(cid:80)
inSection3.2and3.3. Thetraininggoalistomaximizethemodel’slog-evidence logp (y )
i δ,θ i
implicitlydefinedby(7)overatrainingsetofcompressedandpotentiallynoisyobservations . For
Y
that,werelyonthemaintrainingprinciplesforstatisticalmodelsincludinglatentvariables,i.e.,the
EMalgorithmandvariationalinference(cf. Section2.3). Akeyrequirementoftheseprinciplesisthe
abilitytotrackandupdatethemodel’sposterior(i.e.,p (s,z y)for(7))orapproximationsofit
θ,δ
|
overthetrainingiterations(cf. Section2.3). InclassicalSBL,thepriordistributionp (s)formsa
γ
conjugatepriorofp(y s)(cf. (2))and,thus,theposteriorp (sy)istractableinclosedform. We
γ
| |
utilizethispropertybyobservingthatconditionedonsomez,(7)coincideswiththeclassicalSBL
model. Consequently, sz is a conditioned conjugate prior of y s in (7) with tractable Gaussian
| |
posteriorp (sz,y),whoseclosed-formcovariancematrixandmean
θ
|
(cid:18) (cid:19) 1
C θs |y,z(z)= σ1 2DTATAD+diag(γ θ−1(z)) − , µs θ|y,z(z)= σ1 2C θs |y,z(z)DTATy (8)
equal those in SBL [18]. The equivalent formulas for σ2 = 0 are given in Appendix C. By
consideringthedecompositionp (s,z y)=p (sz,y)p (z y),weconcludethatourproposed
θ,δ θ θ,δ
| | |
set of statistical models in (7) exhibits posteriors, which are partially tractable in closed form
independentofanyspecificsofγ (z)andp (z). Theremainingp (z y)resemblesthestandard
θ δ θ,δ
|
GMM’sandVAE’sposterior,andwecaneitherapplytheEMalgorithmorvariationalinferenceto
maximizethecorrespondinglog-evidence. BothwillbediscussedinSection3.2and3.3,respectively.
2Forthesakeofreadability,weexclusivelyusezthroughouttheremainderofthissectiontodenotethe
continuousaswellasthediscreteandfiniterandomvariable,onwhichwecondition.
4Estimationscheme. Thegoalafterthetrainingistousethelearnedstatisticalmodelin(7)for
regularizingtheinverseproblem(1)andestimatingx fromanewlyobservedy. Generally, the
∗
CMEE[xy]isadesirableestimatorduetoitspropertyofminimizingtheMSE.Weutilizeinsights
|
from[32,22],whichderiveapproximateclosedformsoftheCMEusingVAEsandGMMs. More
precisely,byusingthelawoftotalexpectation,weapproximatetheCMEby
E[xy]=E(cid:2)E(cid:2) xy,z(cid:3) y(cid:3) =DE(cid:2)E(cid:2) sy,z(cid:3) y(cid:3) DE (cid:2)E (cid:2) sy,z(cid:3) y(cid:3) . (9)
| | | | | ≈
pθ,δ(z |y) pθ(s |z,y)
| |
The inner expectation is given by the learned
µs |y,z(z)
in (8), while, depending on its specific
θ
characteristics,p (z y)alsoexhibitsaclosedformoravariationalapproximationhasbeenlearned
θ,δ
|
during training (cf. Section 3.2), with which we approximate the outer expectation by a Monte-
Carlo estimation. Based on the perception-distortion trade-off, the CME is not optimal from a
perceptualpoint[37],anddeviatingestimatorsfromtheCMEapproximationmightbebeneficial.
MoredetaileddescriptionsoftheCMEapproximationaswellasfurtherestimatorsbasedonthe
specificimplementationsinSection3.2and3.3aregiveninAppendixG.
3 TheoreticalAnalysisandSpecificParameterizations
3.1 SparsityGuarantees
In Section 2.4, we motivate constraining p (sz) to be a zero mean conditional Gaussian with
θ
|
diagonalcovariancematrixbySBLanditssparsityinducingproperty. Inthefollowing,werigorously
showthatconstrainingp (sz)inthismannerissufficienttomaintainthesparsityinducingproperty
θ
|
for any statistical model following (7) despite the additionally included latent variable z. More
precisely,weshowthatanystatisticalmodelin(7)withsomespecifiedparameterizedp (z)and
δ
γ (z)exhibitsalog-evidencethatisinterpretableasavariationallowerboundofasparsityinducing
θ
log-evidence. Forthat,weestablishthefollowingtheorem.
Theorem3.1. Letp θ(sz)= (s;0,diag(γ θ(z))(i.e.,itisdefinedaccordingto(7)),letzbeeither
| N
continuousordiscreteandfinite,andletγ (z)>0. Then,thereexistsaconstantC >0suchthat
θ
foralls,θ,δwithanyarbitrarydistributionp (z)
δ
(cid:90) N
(cid:89) 1
p (s)= p (z) (s;0,diag(γ (z)))dz t(s)=C . (10)
θ,δ δ θ
N ≤ · s
i
i=1| |
Theintegralcorrespondstoasummationfordiscreteandfinitep (z).
δ
TheproofisgiveninAppendixA.BasedonTheorem3.1,itholdsthat
(cid:16) (cid:17)
logπ(s)(y) logp (y) forallθ,δ (11)
θ,δ
≥
withlogπ(s)(y)beingthelog-evidenceofy s p(y s)andtheimproperbutsparsitypromoting
| ∼ |
prior t(s), whereas logp (y) is implicitly defined by (7). Consequently, we can interpret the
θ,δ
maximizationoflogp (y)over(θ,δ)astheevidencemaximizationoverthespaceofvariational
θ,δ
approximationstoamodelwithasparsityinducingandregularizingprior. However, contraryto
classicalSBL,wehavetheadditionaldegreeoffreedomtochoosethespecificsofγ (z)andp (z)
θ δ
withoutanyconstraintwhilemaintainingtheconnectiontosparsity.
3.2 CompressiveSensingVAE
Theintegralversionofp (s)in(10)stronglyresemblestheVAE’sdecompositionofp(x)in(5).
θ,δ
Infact,byconstrainingp (z)=p(z)= (z;0,I),thedistributionp (s)=p (s)corresponds
δ θ,δ θ
N
toasubsetofallpossibledistributionscoveredbytheVAEdecompositionin(5). Thismotivatesto
choosep (z)inthisexactmannerandrealizeγ (z)astheoutputofaNNdecoder. Generally,we
δ θ
(cid:80)
aimtooptimizethelog-evidence logp (y )in(11)withrespecttoθsolelygivencompressedand
i θ i (cid:80)
noisyobservations . However,equivalenttoVAEs,thelog-evidence logp (y )isintractable
Y i θ i
forp(z) = (z;0,I),whichiswhyatractablelowerboundhastobederived. Atractablelower
N
boundonthelog-evidenceofastatisticalmodelwithlatentvariablesisobtainedbysubtractingthe
KLdivergencebetweentheconditionaldistributionofthelatentvariablesgiventheobservations
5µ ϕ z˜
y qE ϕn (c zo |d ye )r σ ϕ ⊙+ pD θe (c so |d z˜e )r √γ θ
⊙
s AD + y
(0,I) (0,I) (0,σ2I)
N N N
Figure1: AschematicofthesparsityinducingCSVAE.
andatractablevariationaldistributionfromthelog-evidence(cf. (6)). Forthemodelin(7)andi.i.d.
trainingdata,thisresultsin
(cid:88) (cid:88)
logp (y ) L(CSVAE) = logp (y ) D (q (z,sy ) p (z,sy )) (12)
θ i ≥ (θ,ϕ) θ i − KL ϕ | i || θ | i
yi∈Y yi∈Y
(cid:20) (cid:21)
= (cid:88) E logp(y i |s)p θ(s |z)p(z) , (13)
qϕ(z,s |yi) q ϕ(z,sy i)
yi∈Y |
wherethederivationof(13)from(12)isgiveninAppendixB.Basedoninsightsfromthetraining
principle explained in Section 2.4, the variational posterior q (z,sy ) in (13) can be simplified
ϕ i
|
toq (z,sy ) = p (sz,y )q (z y )withclosed-formp (sz,y )andonlyrequiresvariational
ϕ i θ i ϕ i θ i
| | | |
parametersϕforq (z y )tobetractable. Asaresult,theadaptedELBOL(CSVAE) equals
ϕ | i (θ,ϕ)
(cid:88) E (cid:20) log p(y i |s)p θ(s |z)p(z)(cid:21) = (cid:88) (cid:16) E (cid:104) E (cid:2) logp (y s)(cid:3)
qϕ(z,s |yi) p θ(sz,y i)q ϕ(z y i) qϕ(z |yi) pθ(s |z,yi) θ i |
yi∈Y | | yi∈Y (14)
(cid:105) (cid:17)
D (p (sz,y ) p (sz)) D (q (z y ) p(z)) .
KL θ i θ KL ϕ i
− | || | − | ||
ToensurethetrainingofstandardVAEstobetractable,E []isgenerallyapproximatedbya
single-sampleMonte-Carloestimation[36].
Webuildonthqϕ e( sz a|y mi)
e·
strategyandapproximate(14)by
(cid:88) (cid:16) (cid:17)
L(CSVAE) E [logp(y s)] D (q (z y ) p(z)) D (p (sz˜,y ) p (sz˜))
(θ,ϕ) ≈ pθ(s |z˜i,yi) i | − KL ϕ | i || − KL θ | i i || θ | i
yi∈Y
(15)
withz˜ q (z y )andE [logp(y s)]exhibitingaclosed-formsolutiondetailedinAp-
pendixi
D∼
.Inϕ
add|
itii
on,D
(p pθ(s
(|
sz˜i z, ˜yi ,)
y ) p
(si
| z˜))isthetractableKLdivergenceoftwomultivariate
KL θ i i θ i
| || |
Gaussians, and D (q (z y ) p(z)) matches the KL divergence from standard VAEs (cf. [36]).
KL ϕ i
| ||
Both are specified in Appendix E. The adapted ELBO in (15) resembles the classical ELBO of
ordinaryVAEswithamodifiedreconstructionlossandanadditionalKLdivergence. Equivalentto
ordinaryVAEs,wemodelq (z y )= (z;µ (y ),diag(σ2(y )))andrealize(µ (),σ2())by
ϕ | i N ϕ i ϕ i ϕ · ϕ ·
aNNencoder,andγ ()byaNNdecoder. TheadaptedELBOin(15)formsadifferentiableand
θ
· (cid:80)
variationalapproximationofthemodel’slog-evidence logp (y ). BasedonTheorem3.1and
i θ i
(11),itholdsthat
(cid:88) (cid:88)
logπ(s)(y ) logp (y ) L(CSVAE) forallθ,ϕ. (16)
i ≥ θ i ≥ (θ,ϕ)
yi∈Y yi∈Y
Thus,L(CSVAE)
servesasavariationallowerboundofasparsity-inducinglog-evidence. Wedenote
(θ,ϕ)
theresultingVAEasCompressiveSensingVAE(CSVAE),anditsschematicispresentedinFig.1. It
resemblesthevanillaVAEwiththemaindifferencethattheencodertakesacompressedobservation
y asinputandthedecodersolelyoutputsconditionalvariancesγ (z˜)ofsz. Incaseofvarying
θ
|
measurement matrices A for each training, validation and test sample, we use the least-squares
i
estimate xˆ(LS) = AT(A AT) 1y instead of y as encoder input. The CSVAE’s training with
i i i i − i i
ground-truthdataandpseudo-codefortrainingareoutlinedinAppendixHandN(cf. algorithm1).
3.3 CompressiveSensingGMM
Byconsideringdiscreteandfinitep (z)in(7),i.e.,p (z =k)=p (k)=ρ ,weobservethatthe
δ δ δ k
resultingp (s)stronglyresemblestheGMM’sdecompositionofp(x)in(4). Thismotivatesto
θ,δ
choosep (z)inexactlythismannerandp (sz)=p(sk)= (s;0,diag(γ )),i.e.,sismodelled
δ θ k
| | N
asGMMwithzeromeansanddiagonalcovariancematrices. EquivalenttoCSVAEsinSection3.2,
6thedistributionp(sk,y)containedinthemodel’sposteriorp(k,sy)=p(sk,y)p(k y)exhibitsa
| | | |
closed-formsolution(cf. (8)). Moreover,duetothelinearrelationbetweenyandsin(7)andthe
assumptionthatskisGaussian,y kisalsoGaussianwithzeromeanandcovariancematrix
| |
C ky |k =ADdiag(γ k)DTAT+σ2I. (17)
(cid:80)
andp(k y)iscomputableinclosedformbyBayes,i.e.,p(k y)=(p(y k)p(k))/( p(y k)p(k)).
| | | k |
Thus,p(k,sy)exhibitsaclosedform,andanEMalgorithmcanbeapplied,whosee-stepcoincides
|
withcomputingp(k,sy).BasedontheextendedEMalgorithmin[22],wheretheauthorsfitavanilla
|
GMM(4)inthepixeldomainusingcompressedimagepatches,weformulateanm-steptailoredto
ourmodel(7). WedenotetheresultingmodelasCompressiveSensingGMM(CSGMM).Them-step
aims to update the CSGMM’s parameters ρ ,γ by optimizing (cid:80) E [logp(x ,s,k)],
whoseclosed-formsolutionisgiveninthef{ ollk owik n}
glemma.
i pt(k,s |xi) i
Lemma3.2. Letthestatisticalmodelin(7)begivenwithdiscretep δ(z = k) = p(k) = ρ k and
γ (z) = γ ,andlet containi.i.d. noisyandcompressedobservationsy . Givenp (k y )and
θ k i t i
Y |
p t(s |k,y i)= N(s;µs k| ,y ti,k,C ks ,| tyi,k)inthetthiterationofanEMalgorithm,theupdatesof {ρ k,γ k
}
γ
k,(t+1)
=
(cid:80) yi∈Yp t(k |y (cid:80)i)( |µ ks | ,y
t
pi,k
(|
k2 y+ )diag(C ks ,| tyi,k))
, ρ
k,(t+1)
=
(cid:80) yi∈Yp t(k |y i)
(18)
yi∈Y t | i |Y|
formavalidm-stepand,thus,guaranteetoimprovethelog-evidenceineveryiteration.
The proof of Lemma 3.2, the CSGMM’s training with ground-truth data and pseudo-code are
S
outlinedinAppendixF,HandN(cf. algorithm4),respectively.
3.4 Discussion
Fromabroaderperspective,ourproposedalgorithmrepresentstwoconsecutivestages:
1) ChooseadictionaryD,withrespecttowhichageneralsignalset ofinterestiscompress-
C
ible(e.g. awaveletbasisforthesetofnaturalimages).
2) Learn the statistical characteristics of the specific signal subset of interest in its
S ⊂ C
compressibledomain(e.g. thesubsetofhandwrittendigits).
Duetothestrongregularizationofstage1)constrainingthesearchspaceinstage2),ourmodelcan
learnfromafewcorruptedsamples . Toensurethesearchspacetobewithinthesetofcompressible
Y
signalswithrespecttoD,weenforceszin(7)and,thus,stohavezeromean(cf. Theorem3.1).
|
Limitation. Ontheonehand,thisrestrictionregularizestheproblemathand. Ontheotherhand,it
generallyintroducesabias,whichpotentiallypreventsperfectlylearningtheunknowndistributionof
sasitisnotalwayspossibletodecomposeadistributioninthisway. Asaresult,theproposedmodel
isbiasedtowardscapturingthesparsity-specificfeaturesfrom andbeinganeffectivepriorfor(1)
Y
ratherthanlearningacomprehensiverepresentationofthetruep(s).
Distinctionfromgenerativemodel-basedCS. Generativemodel-basedCSbuildsonagenerator
G : z x = G (z)(e.g.,aGANorVAE)withz RF,x RN andF N [4]. Arguably,
θ θ
(cid:55)→ ∈ ∈ ≪
the notion of compressibility is still included in this setup, since it assumes that every x can be
perfectlyencodedbyonlyafewF values. Moreover,thesemodelstypicallyassignasimplisticfixed
distribution,e.g. p(z)= (z;0,I),totheircompressibledomain. Consequently,whilegenerative
N
model-basedCSreplacesthedictionarywithalearnablemappingG andconstrainsthecompressible
θ
domainbyforcingittoberepresentableby,e.g., (0,I),ourapproachdoestheexactopposite. We
N
keepthemappingfromthecompressibleintotheoriginaldomainfixedbyadictionaryDandlearna
non-trivialstatisticalmodelp (s)inthecompressibledomain.
θ,δ
Connectiontouncertaintyquantification. Whilecomingfromdifferentmotivations,ourapproach
sharessimilaritieswithmethodsfromuncertaintyquantification,namelyso-calledpriornetworks
[38]. There,thegeneralideaistolettheNNoutputtheparametersofaconditionedconjugateprior
(conditionedontheNNinput)insteadofdirectlythequantitiesofinterest(cf. [39,40]). Asaresult,
one can use entropy and mutual information measures to quantify different types of uncertainty.
Equivalently,ourproposedmodelsoutputtheparametersγ (z)ofaconditionedconjugatedprior
θ
p (sz),whichiswhytheyenabletoquantifytheiruncertaintybythesamemeasures.
θ
|
7Computationalcomplexity. Itisnoteworthythatouralgorithm’sinferenceaftertrainingrequires
nooptimizationalgorithmbutonlyconsistsof,e.g.,afeed-forwardoperationthroughacomparably
smallVAE.Thus,ourproposedmethod’ssignalreconstructioncomeswithcomputationalbenefits
and differs in this context from other approaches. On the downside, naively implementing the
trainingrequirescomputingandstoringtheposteriorcovariancematricesin(8),whichcanleadtoa
non-negligiblecomputationalandmemory-relatedoverheadforveryhighdimensions. Toovercome
this issue, we derive equivalent reformulations of the CSVAE’s and CSGMM’s update steps for
trainingthatcircumventtheexplicitcomputationofthesematrices. Moreprecisely,thereformulated
updatestepssolelyrequiretheexplicitstoringandinversionoftheobservations’covariancematrices
(cf. (17)),whicharetypicallymuchlowerdimensional. ThereformulationsaregiveninAppendixI.
4 Experiments
4.1 ExperimentalSetup
Datasets. Weevaluatetheperformanceonfourdatasets. WeusetheMNISTdataset(N =784)
forevaluation[41,(CC-BY-SA3.0license)]. Moreover,weuseanartificial1Ddatasetofpiecewise
smoothfunctionsofdimensionN =256.Forthat,wecombineHeaviSinefunctionswithpolynomials
of quadratic degree and randomly placed discontinuities, which are frequently used for CS and
compressibleinthewaveletdomain[42,16]. ThegenerationandexamplesareoutlinedinAppendix
J.Wealsouseadatasetof64 64croppedcelebAimages(N =3 642 =12288)[43]3andevaluate
× ·
ontheFashionMNISTdataset(N =784)inAppendixL[44,(MITlicense)].
Measurementmatrix&evaluationmetric. Forthesimulationsonpiecewisesmoothfunctions,
weuseaseparatemeasurementmatrixA foreachtraining,validationandtestsample,whereallA
i i
containi.i.d. GaussianentriesA (0, 1 ). Forallremainingsimulations,weuseonefixed
i,kl ∼ N M
measurementmatrixAeachwithequallydistributedi.i.d. Gaussianentriesforthewholedataset. We
evaluatethedistortionperformancebyanormalizedMSEnMSE=1/N (cid:80)Ntest( xˆ x 2/N)
test i=1 ∥ i − i ∥2
withxˆ beingtheestimationofx andtheSSIM[45]withN beingsetto5000inanysimulation.
i i test
Baselines&hyperparameters. Asnon-learnablebaselines,weuseLasso[11]andSBL[18],where
weeitheradjustLasso’sshrinkageparameteronaground-truthdatasetorusetheconfigurationsfrom
[4]. Asbaselines,whichcanlearnfromcompresseddata,weuseCSGAN[29]andCKSVD[23]with
OMP,sparsitylevel4and288learnabledictionaryatoms. WesolelyevaluateCKSVDonpiecewise
smoothfunctionsduetoitsrequirementofvaryingmeasurementmatricesforobservingthetraining
samples. ForCSGANonMNIST,weusetheconfigurationspecifiedin[29].4 Inanysimulation,we
useK = 32componentsfortheproposedCSGMMandtheproposedCSVAE’sen-anddecoders
containtwofully-connectedlayerswithReLUactivationandonefollowinglinearlayer,respectively.
WeutilizeAdamforoptimization[46],andonlyconsidertheCMEapproximationoftheestimators
(cf.Section2.4)sincewedidnotobservenotabledifferencestothealternativeestimatorsinAppendix
G.WeutilizeanovercompleteDaubechiesdb4dictionary[47]inallestimatorswithfixeddictionaries.
ForallestimatorsexceptCSGAN,allimagesandestimatesarenormalizedandclippedbetween0
and1,respectively. ForCSGAN,imagesarenormalizedbetween 1and1following[4,29],andfor
−
evaluation,testimagesandtheirestimationsarethenre-normalizedbetween0and1. Foramore
detailedoverviewofhyperparameterconfigurations,seeAppendixK.
4.2 Results
Reconstruction. InFig. 2and3,thereconstructionperformanceofallestimatorsforMNIST,the
piecewisesmoothfunctions,andcelebAisshown. Alltrainablemodelsaretrainedoncompressed
trainingsamples ofdimensionM withoutanyground-truthinformationduringtraining(cf.Section
Y
2.1). Themodelsarethenusedtoestimatetestsignalsx fromobservationsofthesamedimension
i
M (cf. (1))forevaluation. Inthecaseofthepiecewisesmoothfunctions,wealsoaddnoiseofa
10dBsignal-to-noiseratio(SNR)fortrainingandtesting,i.e.,σ2 =E[ Ax 2]/(M 10)in(1).
n ∥ ∥2 ·
3ThecelebAdatasetisreleasedunderacustomlicensefornon-commercialresearchuse.
4TheoriginalworkofCSGAN[29]doesnotprovideresultsforvaryingmeasurementmatricesaswellasfor
celebAtrainedonsolelycompresseddata.Wealsodidnotfindaworkinghyperparameterconfiguration,sowe
leaveoutCSGANforthepiecewisesmoothfunctionsandthecelebAdataset.
8Lasso[11] SBL[18] CKSVD[23] CSGAN[29] CSVAE(ours) CSGMM(ours)
a)MNIST c)MNIST f)piecewisesmooth g)piecewisesmooth
0.1
0.09 0.11 0.11
0.07 0.07
0.08 0.08
0.05
0.04 0.05 0.05
0.03
0.01 0.01 0.02 0.02
80 120 160 200 2 102 2 103 2 104 40 80 120 102 103 104
M · N· t · e) M N t
b)MNIST d)MNIST
0.8
0.8
0.6 0.6
0.4 0.4
0.2
0.2
80 120 160 200 2 102 2 103 2 104
M · N· ·
h) t
Original
Lasso
SBL
i)
CKSVD
Dtype DCT Haar db3 db5 None
CSVAE
(ours) CSVAE 0.0183 0.0170 0.0088 0.0064 0.0513
CSGMM 0.0199 0.0196 0.0100 0.0085 0.0624
Figure2: a)andb)nMSEandSSIMoverM (N = 20000,MNIST),c)andd)nMSEandSSIM
t
overN (M =160,MNIST),e)exemplaryreconstructedMNISTimages(M =200,N =20000),
t t
f) nMSE over M (SNR = 10dB, N = 10000, piece-wise smooth fct.), g) nMSE over N
dB t t
(SNR =10dB,M =100,piece-wisesmoothfct.),h)exemplaryreconstructedpiece-wisesmooth
dB
fct. (M =100,N =1000),i)nMSEcomparisonofdictionaries(MNIST,M =160,N =20000).
t t
In Fig. 2 a)-d), the nMSE and SSIM on MNIST is shown for a) and b) varying observation
dimensions M and the fixed number N = 20000 of training samples, and c) and d) vice versa
t
with fixed M = 160. The error bars represent standard deviations. In terms of both distortion
metricsnMSEandSSIM,CSVAEandCSGMMperformoverallthebest. InFig. 2c),exemplary
reconstructed MNIST images for M = 200 and N = 20000 are shown. Perceptually, CSGAN
t
emphasizesdifferentreconstructionaspectsthantheproposedCSVAEandCSGMM.WhileCSVAE
andCSGMMsuccessfullyrecoverdetailsoftheMNISTimages,CSGANprioritizesthesimilarityin
contrastbetweenbrightanddarkareas. Thiscanbeexplainedbytheirdifferentestimationstrategies.
TheestimationofCSGANsisrestrictedtolieonthelearnedmanifold,whichhasbeenadjustedbased
onthetrainingdataset. Inconsequence,whenthetestsamplex containsdetailsthatcannotbefound
∗
inthetrainingdataset, CSGANscannotreconstructthesebutratheroutputasimilarbutrealistic
representativefromthemanifold. Onthecontrary,ourproposedestimationschemeinSection2.4is
notrestrictedtoamanifoldbutratherinfersconditionaldistributions(cf. (8))overthewholelinear
space.Moreover,Fig.2c)andd)demonstratethattheproposedmodelcaneffectivelylearnfromonly
afewhundredcompressedandnoisysamples. InFig. 2i),CSVAEandCSGMMarecomparedusing
differentdictionariestypes. TheoverallnMSEperformanceremainsgoodfordifferentdictionaries
exceptforapplyingthemodelsdirectlyinthepixeldomain.
In Fig. 2 f), g) and h), the nMSE over varying M (see f)), varying N (see g)) and exemplary
t
reconstructedsamples(seeh))areshownforthesetofpiecewisesmoothfunctions. Here,weonly
evaluatedtheproposedCSVAE.5 ComparedtothebaselinesLasso,SBL,andCKSVD,theCSVAE
performsthebestindistortion(seef)andg))aswellasperception(seeh)). InFig. 3a)-e),thesame
plotsareshownforthecelebAdataset. Despitethesignificantlylargerdimension,theresultsare
consistentwiththoseforMNISTandthesetofpiecewisesmoothfunctions.
5WeleaveouttheCSGMMduetotheneedtocomputeseparateposteriorcovariancematricesforalltraining
samplesbyusingvaryingmeasurementmatrices(cf.(8)),whichleadstoacomputationaloverhead.
9
ESMn
MISS
EAVSC
MMGSC
lanigirO
LBS
NAGSC
)sruo(
)sruo(Lasso[11] SBL[18] CSVAE(ours) CSGMM(ours) labelsofthetestsamplesyare:0 ,1 ,7
a)celebA b)celebA
f)trainedoncompressedzeros
0.008
0.03 0.006 60
0.02 40
0.004 20
0.01 0
0.002 15 17 19 21
900 1800 2700 500 1581 5000 e) h(zy)
|
M N
t
c)celebA d)celebA
0.8
0.8
0.6
0.7
0.4
0.2 0.6
900 1800 2700 500 1581 5000
M N
t
g)MNIST(M =200,N
t
=20000)
Trainingtime Avgreconstruc-
Model
(minutes) tiontime(ms)
Lasso - 73.85
SBL - 294.17
CSGAN 1481 480.81
CSGMM(ours) 45 2.64
CSVAE(ours) 88 2.02
Figure3: a)andb)nMSEandSSIMoverM (N = 5000), c)andd)nMSEandSSIMoverN
t t
(M = 1800), e)exemplaryreconstructedcelebAimages(M = 2700, N = 5000), f)histogram
t
ofh(z y)forcompressedtestMNISTimagesofdigits0,1and7,wheretheCSVAEistrainedon
|
compressedzeros,g)trainingandreconstructiontimeforMNIST(M =200,N =20000).
t
Uncertaintyquantification. Onepossibilitytodeterminethemodel’suncertaintyistodetermine
thedifferentialentropyh(z y)oftheCSVAE’sencoderdistributionq (z y). Sinceq (z y)isa
ϕ ϕ
| | |
Gaussianwithdiagonalcovariancematrix(cf. Section3.2),h(z y)canbecalculatedefficientlyin
|
closedform. InFig. 3f),ahistogramisshownwithvaluesofh(z y). TheCSVAEistrainedon
|
solelycompressedMNISTzeros. Wethenforwardcompressedzeros,onesandsevensandevaluate
h(z y). ItcanbeseenthattheCSVAE’sencoderidentifiestheobservationsthataredistincttothe
|
trainingdatasetbyprovidinglargerentropyvaluesonaverageforthecompressedonesandsevens.
Runtime. Fig. 3g)showsourmeasuredtrainingtimeaswellasaveragereconstructiontimeofthe
MAP-basedestimators(cf. SectionG)forMNIST,M =200,N =20000. CSGMMandCSVAE
t
areconsiderablyfasterthanthebaselinesvalidatingthecorrespondingdiscussioninSection3.4.
Additional results. The results in Fig. 2 and 3 display the performance for CSGAN and the
proposedCSVAEandCSGMMtrainedonsolelycompresseddata. However,allthreemodelscan
alsolearnfromground-truthdata(cf. AppendixH).InAppendixL,weincludethiscomparison
forMNIST,provideresultsonFashionMNIST,analyzetheestimators’robustness,andplotfurther
reconstructionsforalldatasets. Moreover,inAppendixM,weanalyzetheruntimefortrainingand
reconstructioninmoredetailandgiveanoverviewoftheusedcomputeresources.
5 Conclusion
Inthiswork,weintroducedanewtypeoflearnablepriorforregularizingill-conditionedinverse
problemsdenotedbysparseBayesiangenerativemodeling. Ourapproachsharesthepropertyof
classicalCSmethodsofutilizingcompressibility,butatthesametime,itincorporatestheadaptability
totrainingdata.Duetoitsstrongregularizationtowardssparsity,itcanlearnfromafewcorrupteddata
samples. Itappliestoanytypeofcompressiblesignalandcanbeusedforuncertaintyquantification.
WhilethisworkfocusedonsettingupthesparseBayesiangenerativemodelingframework,extensions
tolearnabledictionaries,circumventingtheinversionoftheobservations’covariancematricesduring
training,andperception-emphasizingestimatorsarepartoffuturework.
10
ESMn
MISS
EAVSC
MMGSC
stnuoC
lanigirO
ossaL
LBS
)sruo(
)sruo(AcknowledgmentsandDisclosureofFunding
This research was supported by Rohde & Schwarz GmbH & Co. KG. The authors gratefully
acknowledgethefinancialsupportandresourcesprovidedbythecompany. Theauthorsalsosincerely
appreciatethevaluablediscussionswithDominikSemmlerandBenediktFesl.
References
[1] Y. Eldar and G. Kutyniok, Compressed Sensing: Theory and Applications. Cambridge
UniversityPress,112012.
[2] K. Gregor and Y. LeCun, “Learning fast approximations of sparse coding,” in Proceedings
ofthe27thInternationalConferenceonInternationalConferenceonMachineLearning,ser.
ICML’10. Madison,WI,USA:Omnipress,2010,p.399–406.
[3] J. Liu, X. Chen, Z. Wang, and W. Yin, “ALISTA: Analytic weights are as good as learned
weightsinLISTA,”inInternationalConferenceonLearningRepresentations,2019.
[4] A.Bora,A.Jalal,E.Price,andA.G.Dimakis,“Compressedsensingusinggenerativemodels,”
inProceedingsofthe34thInternationalConferenceonMachineLearning,ser.Proceedingsof
MachineLearningResearch,vol.70. PMLR,Aug2017,pp.537–546.
[5] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep image prior,” International Journal of
ComputerVision,vol.128,no.7,p.1867–1888,Mar.2020.
[6] R.HeckelandP.Hand,“Deepdecoder: Conciseimagerepresentationsfromuntrainednon-
convolutionalnetworks,”in7thInternationalConferenceonLearningRepresentations,ICLR
2019,NewOrleans,LA,USA,May6-9,2019. OpenReview.net,2019.
[7] T.-O.Buchholz,M.Jordan,G.Pigino,andF.Jug,“Cryo-care: Content-awareimagerestoration
forcryo-transmissionelectronmicroscopydata,”in2019IEEE16thInternationalSymposium
onBiomedicalImaging(ISBI2019),2019,pp.502–506.
[8] G.Revach,T.Locher,N.Shlezinger,R.J.G.vanSloun,andR.Vullings,“Hkf: Hierarchical
Kalman filtering with online learned evolution priors for adaptive ECG denoising,” IEEE
TransactionsonSignalProcessing,vol.72,pp.3990–4006,2024.
[9] W.Kim,Y.Ahn,J.Kim,andB.Shim,“Towardsdeeplearning-aidedwirelesschannelestimation
andchannelstateinformationfeedbackfor6G,”JournalofCommunicationsandNetworks,
vol.25,no.1,pp.61–75,2023.
[10] A.Jalal,L.Liu,A.G.Dimakis,andC.Caramanis,“Robustcompressedsensingusinggenerative
models,”inNeuralInformationProcessingSystems,2020.
[11] R.Tibshirani,“RegressionshrinkageandselectionviatheLasso,”JournaloftheRoyalStatisti-
calSociety(SeriesB),vol.58,pp.267–288,1996.
[12] R.Baraniuk,“Optimaltreeapproximationwithwavelets,”ProceedingsofSPIE-TheInterna-
tionalSocietyforOpticalEngineering,vol.3813,pp.196–207,Dec.1999.
[13] A.Liu,V.K.N.Lau,andW.Dai,“Exploitingburst-sparsityinmassiveMIMOwithpartial
channelsupportinformation,”IEEETransactionsonWirelessCommunications,vol.15,no.11,
pp.7820–7830,2016.
[14] L.He,H.Chen,andL.Carin,“Tree-structuredcompressivesensingwithvariationalBayesian
analysis,”IEEESignalProcessingLetters,vol.17,no.3,pp.233–236,2010.
[15] L. He and L. Carin, “Exploiting structure in wavelet-based Bayesian compressive sensing,”
IEEETransactionsonSignalProcessing,vol.57,no.9,pp.3488–3497,2009.
[16] R.G.Baraniuk,V.Cevher,M.F.Duarte,andC.Hegde,“Model-basedcompressivesensing,”
IEEETransactionsonInformationTheory,vol.56,no.4,pp.1982–2001,2010.
11[17] M.E.Tipping,“SparseBayesianlearningandtherelevancevectormachine,”J.Mach.Learn.
Res.,vol.1,p.211–244,sep2001.
[18] D. Wipf and B. Rao, “Sparse Bayesian learning for basis selection,” IEEE Transactions on
SignalProcessing,vol.52,no.8,pp.2153–2164,2004.
[19] G. Yu and G. Sapiro, “Statistical compressed sensing of Gaussian mixture models,” IEEE
TransactionsonSignalProcessing,vol.59,no.12,pp.5842–5858,2011.
[20] F.Renna,R.Calderbank,L.Carin,andM.R.D.Rodrigues,“Reconstructionofsignalsdrawn
fromaGaussianmixturevianoisycompressivemeasurements,”IEEETransactionsonSignal
Processing,vol.62,no.9,pp.2265–2277,2014.
[21] G.Yu,G.Sapiro,andS.Mallat,“Solvinginverseproblemswithpiecewiselinearestimators:
FromGaussianmixturemodelstostructuredsparsity,”IEEETransactionsonImageProcessing,
vol.21,no.5,pp.2481–2499,2012.
[22] J.Yang,X.Liao,X.Yuan,P.Llull,D.J.Brady,G.Sapiro,andL.Carin,“Compressivesensing
by learning a Gaussian mixture model from measurements,” IEEE Transactions on Image
Processing,vol.24,no.1,pp.106–119,2015.
[23] F.PourkamaliAnarakiandS.M.Hughes,“CompressiveK-SVD,”in2013IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing,2013,pp.5469–5473.
[24] F.P.AnarakiandS.Hughes,“MemoryandcomputationefficientPCAviaverysparserandom
projections,”inProceedingsofthe31stInternationalConferenceonMachineLearning,ser.
ProceedingsofMachineLearningResearch,vol.32,no.2. Bejing,China: PMLR,22–24Jun
2014,pp.1341–1349.
[25] F.Pourkamali-Anaraki,S.Becker,andS.M.Hughes,“Efficientdictionarylearningviavery
sparserandomprojections,”in2015InternationalConferenceonSamplingTheoryandApplica-
tions(SampTA),2015,pp.478–482.
[26] T.Chang,B.Tolooshams,andD.Ba,“Randnet: Deeplearningwithcompressedmeasurements
of images,” in 2019 IEEE 29th International Workshop on Machine Learning for Signal
Processing(MLSP),2019,pp.1–6.
[27] M.Aharon,M.Elad,andA.Bruckstein,“K-SVD:Analgorithmfordesigningovercomplete
dictionariesforsparserepresentation,”IEEETransactionsonSignalProcessing,vol.54,no.11,
pp.4311–4322,2006.
[28] A.Bora,E.Price,andA.G.Dimakis,“AmbientGAN:Generativemodelsfromlossymeasure-
ments,”in6thInternationalConferenceonLearningRepresentations,ICLR2018,Vancouver,
BC,Canada,April30-May3,2018,ConferenceTrackProceedings. OpenReview.net,2018.
[29] M.Kabkab,P.Samangouei,andR.Chellappa,“Task-awarecompressedsensingwithgenerative
adversarialnetworks,”inAAAIConferenceonArtificialIntelligence,2018.
[30] B.Fesl,B.Böck,F.Strasser,M.Baur,M.Joham,andW.Utschick,“Ontheasymptoticmean
squareerroroptimalityofdiffusionmodels,”2024,arXiv:2403.02957.
[31] M.Koller, B.Fesl, N. Turan, andW. Utschick, “An asymptoticallyMSE-optimalestimator
based on Gaussian mixture models,” IEEE Transactions on Signal Processing, vol. 70, pp.
4109–4123,2022.
[32] M.Baur,B.Fesl,andW.Utschick,“Leveragingvariationalautoencodersforparameterized
MMSEestimation,”IEEETransactionsonSignalProcessing,vol.72,pp.3731–3744,2024.
[33] J.O.Berger,StatisticaldecisiontheoryandBayesiananalysis. NewYork: Springer-Verlag,
1985.
[34] G.J.McLachlanandD.Peel,Finitemixturemodels. WileySeriesinProbabilityandStatistics,
2000.
12[35] C.M.Bishop,PatternRecognitionandMachineLearning(InformationScienceandStatistics),
1sted. Springer,2007.
[36] D.P.KingmaandM.Welling,“Anintroductiontovariationalautoencoders,”Foundationsand
Trends®inMachineLearning,vol.12,no.4,p.307–392,2019.
[37] Y.BlauandT.Michaeli,“Theperception-distortiontradeoff,”in2018IEEE/CVFConference
onComputerVisionandPatternRecognition. IEEE,Jun.2018.
[38] D.T.Ulmer,C.Hardmeier,andJ.Frellsen,“Priorandposteriornetworks:Asurveyonevidential
deeplearningmethodsforuncertaintyestimation,”TransactionsonMachineLearningResearch,
2023.
[39] A.MalininandM.Gales,“Predictiveuncertaintyestimationviapriornetworks,”inAdvances
inNeuralInformationProcessingSystems,S.Bengio,H.Wallach,H.Larochelle,K.Grauman,
N.Cesa-Bianchi,andR.Garnett,Eds.,vol.31. CurranAssociates,Inc.,2018.
[40] A.Amini,W.Schwarting,A.Soleimany,andD.Rus,“Deepevidentialregression,”Advancesin
NeuralInformationProcessingSystems,vol.33,2020.
[41] Y.Lecun,L.Bottou,Y.Bengio,andP.Haffner,“Gradient-basedlearningappliedtodocument
recognition,”ProceedingsoftheIEEE,vol.86,no.11,pp.2278–2324,1998.
[42] D.L.DonohoandI.M.Johnstone,“Adaptingtounknownsmoothnessviawaveletshrinkage,”
JournaloftheAmericanStatisticalAssociation,vol.90,no.432,pp.1200–1224,1995.
[43] Z.Liu,P.Luo,X.Wang,andX.Tang,“Deeplearningfaceattributesinthewild,”inProceedings
ofInternationalConferenceonComputerVision(ICCV),December2015.
[44] H.Xiao,K.Rasul,andR.Vollgraf,“Fashion-MNIST:anovelimagedatasetforbenchmarking
machinelearningalgorithms,”2017,arXiv:1708.07747.
[45] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality assessment: from error
visibilitytostructuralsimilarity,”IEEETransactionsonImageProcessing,vol.13,no.4,pp.
600–612,2004.
[46] D.KingmaandJ.Ba,“Adam: Amethodforstochasticoptimization,”inInternationalConfer-
enceonLearningRepresentations(ICLR),SanDiega,CA,USA,2015.
[47] G.R.Lee,R.Gommers,F.Waselewski,K.Wohlfahrt,andA.O’Leary,“Pywavelets: Apython
packageforwaveletanalysis,”JournalofOpenSourceSoftware,vol.4,no.36,p.1237,2019.
[48] R.E.Blahut,Principlesandpracticeofinformationtheory. USA:Addison-WesleyLongman
PublishingCo.,Inc.,1987.
[49] P.BickelandK.Doksum,MathematicalStatistics: BasicIdeasandSelectedTopics. Chapman
andHall/CRC,2015.
[50] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P.Prettenhofer,R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,
M.Perrot,andE.Duchesnay,“Scikit-learn: MachinelearninginPython,”JournalofMachine
LearningResearch,vol.12,pp.2825–2830,2011.
13A ProofofTheorem3.1
ToproveTheorem3.1,werestate(3),i.e.,thereexistsaC >0suchthatforallγ >0ands
N
(cid:89) 1
(s;0,diag(γ)) t(s)=C . (19)
N ≤ · s
i
i=1| |
Basedon(19),thereexistsaC >0suchthatforallθ,s,zwithγ (z)>0
θ
N
(cid:89) 1
p (sz)= (s;0,diag(γ (z))) t(s)=C . (20)
θ θ
| N ≤ · s
i
i=1| |
Bymultiplyingbothsidesin(20)withp (z)andintegratingoverz,weconcludethatthereexistsa
δ
C >0suchthatforallθ,s,δwithγ (z)>0
θ
(cid:90) (cid:90)
p (s)= p (z)p (sz)dz p (z)t(s)dz =t(s) (21)
θ,δ δ θ δ
| ≤
independent of the particular choice of p (z). In case of discrete and finite p (z) the integral
δ δ
correspondstoasummation.
B DerivationoftheadaptedELBOforCSVAEs
WewriteouttheKLdivergenceandutilizethatp (y)isindependentoftheexpectation,i.e.,
θ
(cid:88)
L(CSVAE) = logp (y ) D (q (z,sy ) p (z,sy )) (22)
(θ,ϕ) θ i − KL ϕ | i || θ | i
yi∈Y
(cid:20) (cid:21) (cid:20) (cid:21)
= (cid:88) E logp θ(y i)p θ(z,s |y i) = (cid:88) E logp(y i |s)p θ(s |z)p(z) .
qϕ(z,s |yi) q ϕ(z,sy i) qϕ(z,s |yi) q ϕ(z,sy i)
yi∈Y | yi∈Y |
(23)
C ConditionalPosteriorintheNoise-FreeCase
Inthenoise-freecase,theclosed-formcovariancematrixandmeanofp (sz,y)aregivenby
θ
|
C θs |y,z(z)=(cid:18) I −diag((cid:112) γ θ(z))(cid:16) ADdiag((cid:112) γ θ(z))(cid:17) †AD(cid:19) diag(γ θ(z)) (24)
µs θ|y,z(z)=diag((cid:112) γ θ(z))(cid:16) ADdiag((cid:112) γ θ(z))(cid:17) †y. (25)
with() beingtheMoore-Penroseinverse[18].
†
·
D Closed-FormSolutionoftheCSVAEReconstructionLoss
(cid:20) (cid:18) (cid:19)(cid:21)
1 1
E [logp(y s)]=E Mlog(2πσ2)+ y ADs 2 (26)
pθ(s |z˜i,yi) i | pθ(s |z˜i,yi) −2 σ2∥ i − ∥2
1(cid:16) 1 (cid:104)
= Mlog(2πσ2)+ E y 2 yTADs (27)
−2 σ2 pθ(s |z˜i,yi) ∥ i ∥2− i
(cid:105)(cid:17)
sTDTATy +tr(ADssTDTAT)
i
−
1(cid:16) 1 (cid:16)
=
−2
Mlog(2πσ2)+
σ2
∥y
i
∥2 2−y iTADµs θ|yi,z˜i(z˜ i) −µ θs |yi,z˜i(z˜ i)TDTATy
i
(28)
(cid:16) (cid:17) (cid:17)(cid:17)
+tr(AD C θs |yi,z˜i(z˜ i)+µs θ|yi,z˜i(z˜ i)µ θs |yi,z˜i(z˜ i)T DTAT)
1(cid:16) 1 (cid:16) (cid:17)(cid:17)
=
−2
Mlog(2πσ2)+
σ2
∥y
i
−ADµs θ|yi,z˜i(z˜ i) ∥2 2+tr(ADC θs |yi,z˜i(z˜ i)DTAT) (29)
whereµs θ|yi,z˜i(z˜ i)andC θs |yi,z˜i(z˜ i)arethemeanandcovariancematrixofp θ(s |z˜ i,y i).
14E Closed-FormSolutionoftheCSVAEKLDivergences
ByinsertingtheclosedformoftheKLdivergencebetweentwoGaussians(cf. [48]),weget
1(cid:16) (cid:16) (cid:17)
D KL(p θ(s |z˜ i,y i) ||p θ(s |z˜ i))=
2
logdet(diag(γ θ(z˜ i))) −logdet C θs |yi,z˜i(z˜ i) −S
(30)
+tr(cid:16) diag(cid:0) (γ θ(z˜ i) −1(cid:1) C θs |yi,z˜i(z˜ i)(cid:17) +µs θ|yi,z˜i(z˜ i)Tdiag(cid:0) γ θ(z˜ i) −1(cid:1) µs θ|yi,z˜i(z˜ i)(cid:17)
whereµs θ|yi,z˜i(z˜ i)andC θs |yi,z˜i(z˜ i)arethemeanandcovariancematrixofp θ(s |z˜ i,y i). Moreover
[46],
1(cid:88)NL
D (q (z y ) p(z))= (1+logσ2 (y )) µ (y ) σ2 (y )) (31)
KL ϕ | i || −2 j,ϕ i − j,ϕ i − j,ϕ i
j=1
with q (z y ) = (z;µ (y ),diag(σ2(y ))) and µ (y ) and σ2 (y )) being the jth entry of
ϕ | i N ϕ i ϕ i j,ϕ i j,ϕ i
µ (y )andσ2(y ),respectively. Additionally,N denotestheCSVAE’slatentdimension.
ϕ i ϕ i L
F ProofofLemma3.2
Theoptimizationproblemweaimtosolveisgivenby
(cid:88) (cid:88)
ρ ,γ =argmax E [logp(y ,s,k)] s.t. ρ =1. (32)
{
k,(t+1) k,(t+1)
}
{ρk,γk} yi∈Y
pt(k,s |yi) i
k
k
First,wereformulatetheobjectiveas
(cid:88) (cid:88)
E [logp(y ,s,k)]= E [logp(y s)+logp(sk)+logp(k)], (33)
pt(k,s |yi) i pt(k,s |yi) i
| |
yi∈Y yi∈Y
Moreover,weobservethatlogp(y s)doesnotdependon ρ ,γ andwecanleaveitoutfromthe
i k k
| { }
optimizationproblem. Additionally,
(cid:88) E (cid:2)E [logp(sk)+logp(k)](cid:3)
pt(k |yi) pt(s |yi,k)
|
yi∈Y
=
(cid:88) (cid:88)K
p t(k y
i)

1 Slog2π+(cid:88)S
logγ k,j
+(cid:88)S E pt(s |yi,k)(cid:2) |s j |2(cid:3)
+logρ
k
.
(34)
| −2 γ
k,j
yi∈Yk=1 j=1 j=1
whereweinsertedp(sk)= (s;0,diag(γ ))(cf. (7))andγ denotesthejthentryofγ . Inthe
k k,j k
nextstep, letE pt(s |yi,| k)(cid:2) |s jN |2(cid:3) = |µs k| ,y ji,k |2 +C ks ,| jy ,i j,k , whereµs k| ,y ji,k andC ks ,| jy ,i j,k denotethejth
entryofµs |yi,k andthediagonalofCs |yi,k inthetthiteration,respectively. Thus,ouroptimization
k,t k,t
problemofinterestcanberestatedas
argmax
(cid:88) (cid:88)K
p t(k y
i)

1 Slog2π+(cid:88)S (cid:32)
logγ k,j +
|µs k| ,y ji,k |2+C ks ,| jy ,i j,k(cid:33)
+logρ
k

| −2 γ
{ρk,γk} yi∈Yk=1 j=1 k,j
(cid:88)
s.t. ρ =1
k
k
(35)
withLagrangian
=
(cid:88) (cid:88)K
p (k y
)(cid:16) 1(cid:16) Slog2π+(cid:88)S (cid:16)
logγ +
|µs k| ,y ji,k |2+C ks ,| jy ,i j,k (cid:17)(cid:17)
+
t i k,j
L | − 2 γ
k,j
yi∈Yk=1 j=1 (36)
(cid:17) (cid:88)
logρ +ν(1 ρ )
k k
−
k
15andLagrangianmultiplierν. Takingthederivativeof withrespecttoγ andρ andsettingitto
m,q m
L
zeroleadsto
(cid:32) (cid:33)
∂ 1 (cid:88) 1 µs m|y ,qi,k 2+C ms |,y qi ,, qk
= p (my ) | | =0 (37)
∂γ L −2 t | i γ − γ2
m,q yi∈Y m,q m,q
∂ (cid:88) p t(my i)
= | ν =0 (38)
∂ρ L ρ −
m m
yi∈Y
and,thus,
(cid:16) (cid:17)
γ
m,q,(t+1)
=
(cid:80) yi∈Yp t(m (cid:80)|y i)
|
pµs m (| my ,qi,k y|2 )+C ms |,y qi ,, qk
(39)
yi∈Y t (cid:80)| i
p (my )
ρ
m,(t+1)
= yi∈Y νt | i (40)
ν = (41)
|Y|
(cid:80)
where(41)comesfromthenormalizationof ρ =1.
k k
G EstimatorsBasedontheCSVAEandCSGMM
CSVAE. TheCMEapproximationin(9)withtheproposedCSVAEinSection3.2isgivenby
xˆ ∗CSVAE,CME = D (cid:88) E pθ(s |z˜i,y)(cid:2) s |y,z˜ i(cid:3) (42)
|Z|z˜i∈Z
with containing samples z˜ q (z y). Alternatively, one can estimate x based on a newly
i ϕ ∗
Z ∼ |
observedyinthefollowingway. Weusethemeanµ (y)asmaximumaposteriori(MAP)estimate
ϕ
basedonq (z y)and,then,estimatex by
ϕ ∗
|
xˆ ∗CSVAE,MAP =DE pθ(s |µϕ(y),y)(cid:2) s |y,µ ϕ(y)(cid:3) =Dµ θs |y,µϕ(y) (µ ϕ(y)). (43)
Thismethodisappliedin[32]toreducecomputationalcomplexity,butalsodeviatesfromtheCME
approximationandpotentiallyimprovestheperceptualqualityofthereconstruction.
CSGMM. IncaseoftheproposedCSGMMinSection3.3theCMEapproximationin(9)exhibits
aclosedform,i.e.,
(cid:88)
xˆ ∗CSGMM,CME =D p(k |y)E p(s |k,y)[s |k,y]. (44)
k
Analternativeestimatorofx isgivenby
∗
xˆ ∗CSGMM,MAP =DE p(s |kˆ MAP,y)[s |kˆ MAP,y] (45)
withkˆ =argmaxp(k y)[22]. Apseudo-codeforallestimatorsisprovidedinAppendixN(cf.
MAP
|
algorithm2-6).
H CSVAEandCSGMMTrainingonGround-TruthData
TheCSGMMaswellastheCSVAEcanbothbetrainedgivenground-truthdatasets. Fortrainingthe
CSGMM,thisdataseteithercontainsthecompressibleground-truthsignalss ,i.e., ,orthesignals
i
S
x themself,i.e., . Ontheotherhand,fortrainingtheCSVAE,bothdatasetsmustalsocontaintheir
i
X
correspondingobservationsy . Thus,thetrainingdatasetmustbe or (cf. Section2.1).
i
G W
CSGMM. TotraintheCSGMM,thetraininggoalistomaximizethelog-evidenceofthetraining
dataset. Incaseofhavingaccessto ,thevanillaEMalgorithmisemployedwiththemodification
S
ofenforcingtheGMM’smeanstobezeroandcovariancematricestobediagonalineveryupdate
16step. Moreprecisely,aftercomputingtheposteriorsp (k s )bytheBayesruleinthetthiteration,
t i
|
them-stepisgivenby
(cid:80) p (k s )s 2 (cid:80) p (k s )
γ
k,(t+1)
= (cid:80)si∈S t
p
(| ki s| )i | , ρ
k,(t+1)
= si∈S t | i . (46)
si∈S t | i |S|
Incaseofhavingaccessto ,thesamemodifiedEMalgorithmfromSection3.3isemployedby
exchangingADwithDandX settingthenoisevarianceσ2tosomesmallvalue.
CSVAE. TotraintheCSVAEusingthedataset ,wealsoaimoptimizethelog-evidenceofthe
G
trainingdataset
(cid:88) (cid:88)
logp (s ,y )= logp (y s )+logp (s ) (47)
θ i i θ i i θ i
|
(si,yi)
∈G
(si,yi)
∈G
Byconsideringthatlogp (y s )doesnotdependontheCSVAE’sparameters(see(7)),introducing
θ i i
|
additional variational parameters ϕ and approximating p (z y,s) with q (z y), we apply the
θ ϕ
| |
standardreformulationsofVAEs,whichendsupinanELBOresemblingthestandardELBOin(6),
givenby
(cid:88)
L˜(θ,ϕ)= logp (s z˜) D (q (z y ) p(z))
θ i | i − KL ϕ | i || (48)
(si,yi)
∈G
withz˜ q (z y )andp (s )beingdefinedin(10)withp(z)= (z;0,I). Inthecaseoftraining
i ϕ i θ i
∼ | N
theCSVAEwith ,thesametrainingprocedurefromSection3.2isapplied,resultinginthemodified
ELBOL˜ appW roximatedby
(θ,ϕ)
(cid:88) (cid:16) (cid:17)
E [logp(x s)] D (q (z y ) p(z)) D (p (sz˜,x ) p (sz˜))
pθ(s |z˜i,xi) i
| −
KL ϕ
|
i
|| −
KL θ
|
i i
||
θ
|
i
(xi,yi)
∈W
(49)
wherewesetthevariationaldistributionq (s,z x ,y )ofthelatentvariablesgiventheobservations
ϕ i i
|
to q (z y )p (sz,x ). Consequently, instead of computing the posterior p (sz,y ), we use
ϕ i θ i θ i
| | |
p (sz,x ),whichisgivenby(8)withDinsteadofADandsettingthenoisevarianceσ2tosome
θ i
small| value. Additionally,tocomputeE [logp(x s)]wereplacey ,M andADin(29)
withx ,N andD,respectively.
pθ(s |z˜i,xi) i
|
i
i
I ImplementationAspectsforReducingtheComputationalOverhead
In a first step towards a computationally more efficient implementation for training the CSVAE
andCSGMM,wereformulatetheexpressionsoftheconditionalmeanµs |y,z(z)andconditional
θ
covariancematrixCs |y,z(z)from[17,18]in(8). Todoso,weobservethatconditionedonz,sand
θ
yin(7)arejointlyGaussian. Inconsequence,wecanalternativelyapplythestandardformulasfor
computingthemomentsofaconditionaldistributionforajointlyGaussiansetup[49],i.e.,
(cid:16) (cid:17) 1
µs |y,z(z)=Cs,y |z(z) Cy |z(z) − y (50)
θ θ θ
(cid:16) (cid:17) 1
C θs |y,z(z)=diag(γ θ(z)) −C θs,y |z(z) C θy |z(z) − C θs,y |z(z)T (51)
with
C θy |z(z)=ADdiag(γ θ(z))DTAT+σ2I (52)
andC θs,y |z(z)=diag(γ θ(z))DTAT. Importantly,wedonotneedtoexplicitlycomputeC θs |y,z(z)
accordingto(51). ThesubsequentreformulationsdifferbetweenCSGMMandCSVAE.
CSGMM.
FortheCSGMM,onlythediagonalentriesofCs |y,z(z)mustbeexplicitlycomputed,
θ
i.e.,
(cid:16) (cid:17) (cid:18) (cid:16) (cid:17) 1 (cid:19)
diag C θs |y,z(z) =γ θ(z) −diag C θs,y |z(z) C θy |z(z) − C θs,y |z(z)T . (53)
17(cid:16) (cid:17) 1 (cid:16) (cid:17) 1
ThematrixCs,y |z(z) Cy |z(z) − (i.e.,Cs,y |k Cy |k − 6)hasbeenprecomputedforthecon-
θ θ k k
(cid:18) (cid:16) (cid:17) 1(cid:16) (cid:17)T(cid:19)
ditional mean in (50) and subsequently determining diag Cs,y |k Cy |k − Cs,y |k only
k k k
(cid:16) (cid:17) 1
requires (SM)operations. Moreover, Cy |k − hasalreadybeendeterminedforevaluatingthe
O k
posterior distributions p(k y) in the preceding e-step (cf. Section 3.3). The closed-form m-step
(cid:16) | (cid:17)
in(18)onlytakesdiag
Cs |y,k
forwhichwecandirectlyuse(53). Inthisway, wecircumvent
k
explicitlycomputingthefullposteriorcovariancematricesin(8),renderingtrainingtheCSGMM
moreefficient.
CSVAE. TheobjectivefortrainingtheCSVAEisgivenin(15),whichconsistsofthemodified
reconstructionlossin(29)aswellastheKLdivergencesin(30)and(31).
Whileµs |y,z(z)in(29)
θ
canbedirectlycomputedusing(50),wereformulatethetrace-termin(29)tocircumventtheexplicit
computationofCs |y,z(z).
Todoso,weapplythefollowingsteps
θ
(cid:16) (cid:17)
tr(ADC θs |yi,z˜i(z˜ i)DTAT)=tr C θs |yi,z˜i(z˜ i)DTATAD =
(54)
 
σ2tr(cid:18)
C θs |yi,z˜i(z˜
i)(cid:18)(cid:16)
C θs |yi,z˜i(z˜
i)(cid:17) −1
−diag(γ θ−1(z˜
i))(cid:19)(cid:19)
=σ2 S
−(cid:88)S C
θ
γs ,| jy ,i j, (z˜ zi ˜(z )˜ i)

θ,j i
j=1
(55)
with C θs ,| jy ,i j,z˜i(z˜ i) being the jth diagonal entry of C θs |yi,z˜i(z˜ i) and γ θ,j(z˜ i) being the jth entry
ofγ θ(z˜ i). Forthederivation, wemainlyapplytheformulaofC θs |yi,z˜i(z˜ i)in(8). Byobserving
thattheσ2 in(55)cancelsoutwiththe1/σ2 in(29),thetrace-termin(29)cancelsoutwith S+
tr(cid:16) diag(cid:0) (γ θ(z˜ i) −1(cid:1) C θs |yi,z˜i(z˜ i)(cid:17) in (30). We also reformulate logdet(cid:16) C θs |yi,z˜i(z˜ i)(cid:17) in− (30),
renderingtheexplicitcomputationofC θs |yi,z˜i(z˜ i)obsolete,i.e.,
(cid:16) (cid:17) (cid:18) 1 (cid:19)
logdet C θs |yi,z˜i(z˜ i) = −logdet σ2DTATAD+diag(γ θ−1(z˜ i)) = (56)
(cid:18)(cid:18) (cid:19) (cid:19)
1
−logdet σ2DTATAD+diag(γ θ−1(z˜ i)) diag(γ θ(z˜ i))diag(γ θ−1(z˜ i)) = (57)
(cid:18)(cid:18) (cid:19) (cid:19)
1
−logdet σ2DTATAD+diag(γ θ−1(z˜ i)) diag(γ θ(z˜ i)) +logdet(diag(γ θ(z˜ i)))= (58)
(cid:18) (cid:19)
1
logdet DTATADdiag(γ (z˜))+I +logdet(diag(γ (z˜)))= (59)
− σ2 θ i θ i
(cid:18) (cid:19)
1
logdet ADdiag(γ (z˜))DTAT+I +logdet(diag(γ (z˜)))= (60)
− σ2 θ i θ i
1
−logdet(
σ2
I) −logdetC θy |z˜i(z˜ i)+logdet(diag(γ θ(z˜ i)))= (61)
Mlogσ2 −logdetC θy |z˜i(z˜ i)+logdet(diag(γ θ(z˜ i))) (62)
whereweusetheformulaofC θs |yi,z˜i(z˜ i)in(8),C θy |z˜i(z˜ i) = ADdiag(γ θ(z˜ i))DTAT+σ2Ias
wellasSylvester’sdeterminanttheoremforthereformulationfrom(59)to(60).
Ingeneral,equivalentreformulationscanalsobeappliedwhentheCSVAEandCSGMMaretrained
onground-truthdata(cf. AppendixH)byreplacingC θs |yi,z˜i(z˜ i)withC θs |xi,z˜i(z˜ i).
6FortheCSGMMweusethenotationfromSection3.3
180.6 0.5 1
0
0 0
0.5
−
0 100 200 0 100 200 0 100 200
Figure4: Exemplarysignalswithinthe1Ddatasetofpiece-wisesmoothfunctions.
2 1.5
3
1 0
0 1.5 0
−
0 100 200 0 100 200 0 100 200
Figure5: WaveletTransformsoftheexemplarysignalsinFig.4.
J Implementationofthe1DDatasetofPiecewiseSmoothFunctions
Theartificial1Ddatasetofpiecewisesmoothfunctionsisgeneratedinthefollowingway:
(cid:80)2 h(j)tj +a(1)sin(4πt+η(1)), t [0,g )
 j=0 1,i i i ∈ 1
x (t)= (cid:80)2 h(j)tj +a(2)sin(4πt+η(2)), t [g ,g ) (63)
i (cid:80)2j=0 h(2 j,i
)tj
+a(i 2)sin(4πt+ηi
(3)),
t∈ [g1 ,42
)
j=0 3,i i i ∈ 2
whereh(j) Bernoulli(0.5)andeithertakes 0.4or0.4, a(m) (0,0.12), η(m) (0,2π),
m,i ∼ − i ∼ N i ∼ U
g (0,2) and g (2,4). After generation, each x (t) is sampled equidistantly N = 256
1 2 i
∼ U ∼ U
times, anditssamplesarestoredinvectorsx , whichthenconstitutetheground-truthdataset .
i
X
ExemplarysignalsandtheirwaveletdecompositionaregiveninFig.4and5,respectively.
K DetailedOverviewofHyperparameterConfigurations
MNIST&FashionMNIST. Thenon-learnablebaselinesforthesimulationsonMNISTareLasso
aswellSBL.WeapplyLassodirectlyinthepixeldomainwithitsshrinkageparameterλsetto0.1
inlinewith[4]. ForSBL,weuseanovercompletedb4dictionarywithsymmetricextension[47].
Moreover,althoughwedonotincludenoiseinthesimulationsforMNIST,wesetσ2 in(2)toan
incrementcorrespondingto40dBSNRtobeabletoapplythecomputationallyefficientreformulations
fromAppendixI.ForCSGAN,weusetheexacthyperparameterconfigurationsspecifiedin[29]. For
CSGMM,wesetthenumberK ofcomponentsto32anditerateuntiltheincrementsofthetraining
dataset’slog-evidencereachthetoleranceparameterof10 3,astandardstoppingcriterionforGMMs
−
[50]. TheCSVAEencodersanddecoderscontaintwofullyconnectedlayerswithReLUactivation
andonefollowinglinearlayer,respectively. Thewidthsofthelayersaresetinawaysuchthatfor
thefirsttwolayers, thewidthincreaseslinearlyfromtheinputdimensionto256, whilethefinal
linearlayermapsfrom256tothedesireddimension(i.e.,eitherS forthedecoderortwicethelatent
dimensionfortheencoder). Thelatentdimensionissetto16,thelearningrateissetto2 10 5,and
−
·
thebatchsizeissetto64. WeusetheAdamoptimizerforoptimization[46]. Weoncereducethe
learningratebyafactorof2duringtrainingandstopthetraining,whenthemodifiedELBOin(15)
foravalidationsetof5000samplesdoesnotincrease. ForCSGMMaswellasCSVAEwesetσ2
in(7)toanincrementcorrespondingto40dBSNRtobeabletoapplythetrainingreformulation
fromAppendixI.Wealsousethesameovercompletedb4dictionaryasforSBL.Moreover,weuse
N =64samplestoapproximatetheouterexpectationin(9).
s
Piecewise smooth function. For the simulations on the set of piecewise smooth functions, we
adjusttheshrinkageparameterofLassobasedonaground-truthvalidationdatasetof5000samples
onceforeveryM. Moreover,wealsousetheovercompletedb4dictionaryforLassoasforallother
dictionary-basedestimators. Insteadofchoosing256,wechoose128asthemaximumwidthofthe
en-anddecoderlayers. Otherwise,thehyperparametersremainthesameasforthesimulationson
MNIST.
CelebA. For the simulations on celebA, we set the shrinkage parameter λ of Lasso to 0.00001
(cf. [4]). As celebA contains colored images, we choose a block-diagonal dictionary with the
overcompletedb4dictionarythreetimesalongthediagonalandzeromatricesinalloff-diagonals.
19CSGAN[29] CSVAE CSGMM
SBL[18]
(trainedon ) (trainedon ,ours) (trainedon ,ours)
Y Y Y
CSGAN[29] CSVAE CSGMM
(trainedongroundtruth) (trainedongroundtruth,ours) (trainedongroundtruth,ours)
a)MNIST c)MNIST e)MNIST g)MNISTFashion
0.04
0.13
0.05
0.03 0.05
0.09
0.02 0.03 0.03
0.05
0.01
0.01 0.01 0.01
1 4 16 64 80 120 160 200 2 102 2 103 2 104 80 120 160 200
N
s
M · N·
t
· M
b)MNIST d)MNIST f)MNIST h)MNISTFashion
0.8
0.8 0.8 0.8
0.6
0.7 0.6 0.7
0.6 0.6 0.4
0.4
0.5 0.2
0.5
0.2
1 4 16 64 80 120 160 200 2 102 2 103 2 104 80 120 160 200
N
s
M · N·
t
· M
Figure6: a)andb)nMSEandSSIMcomparisonovernumberN ofestimationsperobservation
s
(MNIST, M = 200,N = 20000), c)-f) nMSE and SSIM performance of models trained on
t
compressed data (solid curves) as well as models trained on ground-truth data (dashed curves)
and overc)andd)M (N =20000,MNIST),ande)andf)N (M =160,MNIST),g)andh)
t t
W X
performancecomparisononFashionMNIST.
Eachblockcorrespondstoonecolorchannel. WeusethisdictionaryforallestimatorsoncelebA.
Thebatchsizetosetto32. Otherwise,allhyperparametersarechoseninthesamewayasforMNIST.
Generally,wedidnodetailednetworkarchitecturesearchfortheproposedCSVAEsinceweobserved
noconsiderablechangeinperformancebytestingoutdifferentarchitectures.
L AdditionalResults
Robustness comparison. The proposed CSVAE relies on approximating the outer expectation
in(9)byMonte-Carlosampling. Similartothisapproximation,thebaselineCSGANalsoapplies
severalrandomrestarts,i.e.,itestimatestheground-truthsampleseveraltimesforasingleobservation
andchoosesthebest-performingestimationbycomparingtheirtractablemeasurementerrors[29,4].
Inthisway,bothmethodscanbecomparedintermsofthenumberN ofrepeatedestimationsthey
s
perform for a single observation. In Fig. 6 a) and b), we compare the nMSE and the SSIM for
bothwithrespecttoN . WeevaluatetheirperformanceontheMNISTdatasetwithM =200and
s
N =20000. ItcanbeseenthattheproposedCSVAEachievesalreadygoodperformanceforasingle
t
Monte-Carlosample,i.e.,N =1,whileCSGANissignificantlyworsewhenonlyusingonerestart
s
(i.e.,N =1)comparedtohavingmanyrestarts.
s
MNISTwithtrainingonground-truthdata. InFig. 6c)-f),theresultsfromFig. 2a)-d)are
extendedwiththeperformanceofthecorrespondingmodelstrainedonground-truthdata,whichare
representedbythedashedcurves. Itshouldbenotedthatground-truthinformationherereferstothe
MNISTimagesx themself. Inconsequence,CSGANaswellastheproposedCSGMMaretrained
i
on ,whileCSVAEistrainedon (cf. Section2.1andAppendixH).Forthesakeofreadability,
X W
weleaveouttheerrorbarsintheseplots. Generally,theCSGAN,aswellastheproposedCSVAEand
CSGMM,benefitfromtheadditionalinformationduringtrainingintermsofthedistortionmetrics
nMSEandSSIM. ForthenMSE,theoverallperformancecomparisonremainsthesame,whilefor
SSIMCSGANtrainedon outperformsCSVAEandCSGMMtrainedon and ,respectively.
X W X
InFig. 7,additionalexemplaryreconstructedMNISTimagesareshownforallmodelstrainedon
compressedaswellasground-truthdata incaseoftheCSGANandCSGMMand incaseofthe
X W
CSVAE.Itcanbeseenthatperceptually,theCSGANsignificantlybenefitsfromtheground-truth
20
ESMn
MISSa)reconstructionsfrommodelssolelytrainedoncompresseddata withoutanyground-truthinformation
Y
b)reconstructionsfrommodelstrainedonground-truthdata or
X W
Figure 7: Exemplary reconstructed MNIST images for M = 200, N = 20000 from a) models,
t
whicharesolelytrainedoncompresseddata(withobservationsofdimensionM),andb)models,
whicharetrainedongroundtruthdata.
informationduringtraining,whiletheproposedCSVAEandCSGMMperformsimilarlyinperception
foratrainingsetofcompressedorground-truthsamples. Thishighlightstheireffectiveregularization
effectexplainedinSection3.4.
FashionMNIST. InFig. 6g)andh),theperformanceofSBL,CSGAN,andtheproposedCSVAE
and CSGMM for varying observation dimensions M and fixed number N = 20000 of training
t
samplesisshown. WhileforsmallM,CSGANoutperformstheproposedCSVAEandCSGMM,
itsperformancesaturatesforincreasingM,anditperformsworsethanCSVAEandCSGMM.In
thiscase,CSGAN’sregularizationtoenforcethereconstructiontobeinthegenerator’sdomainis
beneficialforstronglycompressedobservations(i.e.,forsmallM).InFig.8,exemplaryreconstructed
FashionMNISTimagesareshown.
Additionalexemplaryreconstructions. InFig. 9,10and11,additionalexemplaryreconstructions
forthepiecewisesmoothfunctions,MNISTaswellascelebAareshown.
M OverviewofComputeResources
AllmodelshavebeensimulatedonanNVIDIAA40GPUexceptfortheproposedCSGMM,whose
experimentshavebeenconductedonanIntel(R)Xeon(R)Gold6134CPU@3.20GHz. Wereport
thenumberoflearnableparameters,thetimeusedfortrainingaswellastheaveragereconstruction
timeaftertrainingforsimulationswithpiecewisesmoothfunctions,MNISTandcelebAinTable1,2
and3,respectively. Theaveragereconstructiontimeofestimatingx fromanewlyobservedyhas
∗
beenmeasuredfortheMAP-basedestimatorsinAppendixG.Whilethereportednumbersgivean
overviewofthecomparisonbetweenthedifferenttestedmodels,itisimportanttonotethatwedid
notaimtofullyoptimizeoursimulationsforcomputationalefficiency.
N Pseudo-CodefortheTrainingandInferenceoftheCSVAEandCSGMM
Algorithm1summarizesoneiterationofthetrainingprocedurefortheCSVAE.Algorithm4doesthe
samefortheCSGMM.Inalgorithm2and3,thepseudo-codeoftheCMEapproximationandthe
MAP-basedestimatorusingtheCSVAEispresented,respectively(cf. (42)and(43)). Inalgorithm5
and6,thesameisgivenfortheCSGMM(cf. (44)and(45))
21
lanigirO
LBS
NAGSC
E )sA
rV uoS
(CM
)sM
ruG
oS (C
NAGSC
E )sA
rV uoS
(CM
)sM
ruG
oS (CFigure8: ExemplaryreconstructedFashionMNISTimages(M =200,N =20000,Fig. 6)g),h))
t
Figure9: Exemplaryreconstructedpiece-wisesmoothfunctions(M =140,N =10000,Fig. 2d))
t
Figure10: ExemplaryreconstructedMNISTimages(M =160,N =20000,Fig. 2a))
t
Figure11: ExemplaryreconstructedcelebAimages(M =2700,N =5000,Fig. 3a))
t
22
EAVSC
MMGSC
lanigirO
LBS
NAGSC
E )sA
rV uoS
(CM
)sM
ruG
oS (C
lanigirO
ossaL
LBS
DVSKC
E )sA
rV uoS
(C
lanigirO
ossaL
LBS
)sruo(
)sruo(
lanigirO
LBS
NAGSC
E )sA
rV uoS
(CM
)sM
ruG
oS (CTable1: Resourcesforsimulationsonpiecewisesmoothfct. (M =120,N =20000,Fig.2f)).
t
Model #Parameters Trainingtime(hours) Avgreconstructiontimeinms
SBL - - 137.23
CKSVD - 0.43 2.38
CSVAE(ours) 109376 0.24 1.47
Table2: ResourcesforsimulationsonMNIST(M =200,N =20000,Fig. 2a)).
t
Model #Parameters Trainingtime(hours) Avgreconstructiontimeinms
SBL - - 294.17
CSGAN 208817 24.68 480.81
CSGMM(ours) 46208 0.75 2.64
CSVAE(ours) 566467 1.48 2.02
Table3: ResourcesforsimulationsoncelebA(M =1800,N =5000,Fig. 3a)).
t
Model #Parameters Trainingtime(hours) Avgreconstructiontimeinms
SBL - - 6067.70
CSGMM(ours) 555104 5.33 329.34
CSVAE(ours) 5063138 17.88 62.59
Algorithm1UpdateStepintheTrainingPhaseoftheCSVAE
Input:parametersinthetthiterationθ(t)(i.e.,thedecoder)andϕ(t)(i.e.,theencoder),batch batch,meas.
matrixA(orcorrespondingbatchmeas. matrices A i i),dict.
D,noiseσ2,optimizerAdaY
m t inthetth
{ }
iteration,learningrateλ
Output:parametersinthe(t+1)thiterationθ(t+1),ϕ(t+1)
fori=1to batch do
|Y |
1)µ ϕ(t)(y i),σ ϕ(t)(y i) ←E −nc −o −d −er y
i
2)drawz˜
i
∼q ϕ(t)(z |y i)= N(z;µ ϕ(t)(y i),σ ϕ(t)(y i))(viareparameterizationtrick[36])
43 )) (cid:16)γ θ C(t θy) (|( tz˜z )˜ ii () z˜← i)D − ,e −c µo −d s θ−e | (r y t)iz˜ ,zi ˜i(z˜ i)(cid:17) ←(5 −2 −),( −50 −) (cid:0) γ θ(t)(z˜ i),A,D,σ2,y i(cid:1)
(cid:16) (cid:17)
5)E p θ(t)(s |z˜i,yi)[logp(y i |s)] ←A −pp −en −d −ix −D −,I µs θ| (y t)i,z˜i(z˜ i),A,D,σ2,y i
AppendixE (cid:0) (cid:1)
6)D KL(q ϕ(t)(z |y i) ||p(z))
←−−−−−
µ ϕ(t)(y i) (cid:16),σ ϕ(t)(y i)
(cid:17)
7)D KL(p θ(t)(s |z˜ i,y i) ||p θ(t)(s |z˜ i)) ←A −pp −e −nd −ix −E −,I C θy (| tz˜ )i(z˜ i),µs θ| (y t)i,z˜i(z˜ i),γ θ(t)(z˜ i)
endfor
L( (C θS (tV )A ,E ϕ)
(t))
←( −15 −) {5),6),7) }|iY=b 1atch|
(cid:16) (cid:17)
θ(t+1),ϕ(t+1) Adam (L(CSVAE) ,λ,θ(t),ϕ(t))
← t (θ(t),ϕ(t))
Algorithm2CMEApproximationwiththeCSVAEintheInferencePhase(cf. (42))
Input:observationy,encoder(µϕ(),σϕ()),decoderγθ(),meas.matrixA,dict.D,noiseσ2,cardinality
· · ·
in(42)
|Z|
Output:CMEapproximationxˆ∗CME
1)µϕ(y),σϕ(y) Encoder y
←−−−−
fori=1to do
|Z|
2)drawz˜
i
qϕ(zy)= (z;µϕ(y),σϕ(y))
∼ | N
3)γθ(z˜ i) Decoder z˜
i
←−−−−
4)µs θ|y,z˜i(z˜ i) ←( −50 −) (cid:0) γθ(z˜ i),A,D,σ2,y(cid:1)
endfor
5)xˆ∗CME =D/ |Z|(cid:80) |iZ=|1µs θ|y,z˜i(z˜ i)
23Algorithm3MAP-basedEstimatorwiththeCSVAEintheInferencePhase(cf. (43))
Input:observationy,encoder(µϕ(),σϕ()),decoderγθ(),meas.matrixA,dict.D,noiseσ2
· · ·
Output:MAP-basedestimatorxˆ∗MAP
1)µϕ(y) Encoder y
←−−−−
3)γθ(µϕ(y)) Decoder µϕ(y)
←−−−−
4)µ θs |y,µϕ(y) (µϕ(y)) ←( −50 −) (cid:0) γθ(µϕ(y)),A,D,σ2,y(cid:1)
5)xˆ∗MAP
=Dµs θ|y,µϕ(y)
(µϕ(y))
Algorithm4OneEMStepintheTrainingPhaseoftheCSGMMwithonefixedA
Input: parametersinthetthiteration γ(t),ρ(t) K ,trainingset ,meas. matrixA,dict. D,
{ k k }k=1 Y
noiseσ2
Output: parametersinthe(t+1)thiteration γ(t+1),ρ(t+1) K
{ k k }k=1
fork =1toK do
(cid:16) (cid:17)
1)Cy |k (17) γ(t),D,A,σ2
k,t ←−− k
(cid:16) (cid:17) (cid:16) (cid:17)
2)diag
Cs |y,k (53) γ(t),D,A,σ2
k,t ←−− k
fori=1to do
|Y| (cid:16) (cid:17)
3)p t(k |y i) ←( −Ba −y −e −s) C ky ,| tk,ρ( kt)
(cid:16) (cid:17)
4)µs k| ,y ti,k ←(5 −0 −) C ky ,| tk,D,A,σ2,y i
endfor
(cid:16) (cid:17) (cid:18)(cid:110) (cid:111) (cid:16) (cid:17)(cid:19)
5) γ k(t+1),ρ( kt+1) ←L −em −m −a −(3 −.2 −) p t(k |y i),µs k| ,y ti,k | iY =| 1,diag C ks ,| ty,k
endfor
Algorithm5CMEApproximationwiththeCSGMMintheInferencePhase(cf. (44))
Input:observationy,GMM {ρ k,γ
k
}K k=1,meas.matrixA,dict.D,noiseσ2
Output:CMEapproximationxˆ∗CME
fork=1toKdo
1)C ky |k ←( −17 −) (cid:0) γ
k
(cid:16),D,A,σ2 (cid:17)(cid:1)
2)p(k |y) ←(B −ay −e −s) C ky |k,ρ
k
(cid:16) (cid:17)
3)µs k|y,k ←( −50 −) C ky |k,D,A,σ2,y
endfor
4)xˆ∗CME =D(cid:80)K k=1p(k |y)µs k|y,k
Algorithm6MAP-basedEstimatorwiththeCSGMMintheInferencePhase(cf. (45))
Input: observationy,GMM {ρ k,γ k }K k=1,meas. matrixA,dict. D,noiseσ2
Output: MAP-basedestimationxˆ ∗MAP
fork =1toK do
1)C ky |k ←(1 −7 −) (cid:0) γ k, (cid:16)D,A,σ2(cid:1)
(cid:17)
2)p(k |y) ←( −Ba −y −e −s) C ky |k,ρ
k
endfor
3)kˆ =argmaxp(k y)
MAP
4)µs |y,kˆ MAP (50) (cid:16) Cy| |kˆ MAP,D,A,y,σ2(cid:17)
kˆ MAP ←−− k
5)xˆ ∗MAP =Dµ ks ˆ| My A, Pkˆ MAP
24