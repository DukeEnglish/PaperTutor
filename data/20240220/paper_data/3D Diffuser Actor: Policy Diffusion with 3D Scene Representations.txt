3D Diffuser Actor: Policy Diffusion with 3D Scene
Representations
Tsung-Wei Ke†, Nikolaos Gkanatsios†, Katerina Fragkiadaki
Carnegie Mellon University
{tsungwek,ngkanats,katef}@cs.cmu.edu
3d-diffuser-actor.github.io
Abstract—We marry diffusion policies and 3D scene repre-
sentations for robot manipulation. Diffusion policies learn the
actiondistributionconditionedontherobotandenvironmentstate
using conditional diffusion models. They have recently shown to
outperform both deterministic and alternative state-conditioned
action distribution learning methods. 3D robot policies use 3D
scenefeaturerepresentationsaggregatedfromasingleormultiple
camera views using sensed depth. They have shown to generalize
better than their 2D counterparts across camera viewpoints. We
unify these two lines of work and present 3D Diffuser Actor,
a neural policy architecture that, given a language instruction,
builds a 3D representation of the visual scene and conditions
on it to iteratively denoise 3D rotations and translations for
the robot’s end-effector. At each denoising iteration, our model
represents end-effector pose estimates as 3D scene tokens and
Fig. 1: 3D Diffuser Actor sets a new state-of-the-art on
predictsthe3Dtranslationandrotationerrorforeachofthem,by
RLBench [33] on a multi-view setup and on CALVIN [50]
featurizingthemusing3Drelativeattentiontoother3Dvisualand
language tokens. 3D Diffuser Actor sets a new state-of-the-art benchmarksonazero-shotlong-horizonsetup.OnRLBench,
on RLBench with an absolute performance gain of 18.1% over we train and evaluate on the PerAct setup. On CALVIN, we
the current SOTA on a multi-view setup and an absolute gain train on environments A, B, C and test on environment D.
of 13.1% on a single-view setup. On the CALVIN benchmark, it
3D Diffuser Actor achieves state-of-the-art performance on
outperforms the current SOTA in the setting of zero-shot unseen
both benchmarks.
scene generalization by being able to successfully run 0.2 more
tasks,a7%relativeincrease.Italsoworksintherealworldfrom
ahandfulofdemonstrations.Weablateourmodel’sarchitectural
design choices, such as 3D scene featurization and 3D relative
attentions, and show they all help generalization. Our results [10], combination of classification and regression objectives
suggest that 3D scene representations and powerful generative [67], or energy-based objectives [18]. Specifically, they exhibit
modelingarekeystoefficientrobotlearningfromdemonstrations.
better action distribution coverage and higher fidelity (less
mode hallucination) than alternative formulations [55]. They
I. INTRODUCTION have so far been used with low-dimensional engineered state
representations [55] or 2D image encodings [11].
Many robot manipulation tasks are inherently multimodal:
Lifting features from perspective views to a bird’s eye
at any point during task execution, there may be multiple
view (BEV) or 3D robot workspace map has shown strong
actions which yield task-optimal behavior. Indeed, human
results in robot learning from demonstrations [70, 34, 22, 19].
demonstrations often contain diverse ways that a task can be
Policiesthatusesuch2D-to-BEVor2D-to-3Dsceneencodings
accomplished. A natural choice is then to treat policy learning
generalize better than their 2D counterparts across camera
as a distribution learning problem: instead of representing
a policy as a deterministic map π (x), learn the entire viewpoints and can handle novel camera viewpoints at test
θ
time[19,22].Ourconjectureisthatthisimprovedperformance
distribution of actions conditioned on the current robot state
p(y|x) [28, 77, 26, 67]. Recent works use diffusion models comes from the fact that the 3D visual scene content and
the robot’s end-effector poses live in a common 3D space.
for learning such state-conditioned action distributions for
BEV or 3D policy formulations e.g., Transporter Networks
robot manipulation policies from demonstrations [55, 11, 61]
[85], C2F-ARM [34], PerAct [70], Act3D [19] and Robot
and show they outperform deterministic or other alternatives,
View Transformer [22], discretize the robot’s workspace for
such as variational autoencoders [48], mixture of Gaussians
localizing the robot’s end-effector, such as the 2D BEV map
†Equalcontribution for pick and place actions [85], the 3D workspace [19], or
4202
beF
61
]OR.sc[
1v58801.2042:viXraImage
2D feature map
Image
Encoder
Average 3D location
within patch
Depth 3D pixel location Denoising
(a) Encode 2D feature tokens (b) Lift 2D feature tokens to 3D (c) Denoise end-effectorpose estimate
“Put the blocks in a triangle on the plate” “Stack the cups”
Fig. 2: 3D Diffuser Actor is a language and vision conditioned action diffusion policy that learns to predict end-effector
keyposes and trajectories from demonstrations. (a) An image encoder extracts a 2D image feature map from each camera view;
each feature token corresponds to a fixed-size patch in the input image. (b) We lift 2D feature tokens to 3D using sensed depth.
(c) A 3D relative position attention Transformer takes as input the 3D scene tokens, an instruction text encoding, a short history
of end-effector poses and the current noisy estimate of the end-effector future trajectory (or keypose) and outputs the noise in
the form of 3D translations and rotations.
multiview re-projected images [22]. Such 3D policies have not ablative versions of our model that do not use 3D relative
been combined yet with diffusion objectives. attentions.Wefurthershow3D Diffuser Actorcanlearnmulti-
task manipulation in the real world across 12 tasks from a
In this paper, we propose 3D Diffuser Actor, a model that
handful of real world demonstrations. Simulation and real-
marries diffusion policies for handling action multimodal-
world robot execution videos, as well as our code and
ity and 3D scene encodings for effective spatial reasoning.
trained checkpoints are publicly available on our website:
3D Diffuser Actor trains a denoising neural network that takes
3d-diffuser-actor.github.io.
asinputa3Dscenevisualencoding,thecurrentestimateofthe
end-effector’s future trajectory (3D location and orientation),
as well as the diffusion iteration index, and predicts the error
II. RELATEDWORK
in 3D translation and rotation. Our model achieves translation
Learning robot manipulation policies from demonstrations
equivariance in prediction by representing the current estimate
Though state-to-action mappings are typically multimodal, ear-
of the robot’s end-effector trajectory as 3D scene tokens and
lier works on learning from demonstrations train deterministic
featurizing them jointly with the visual tokens using relative-
policies with behavior cloning [57, 3]. To better handle action
position 3D attentions [68, 75], as shown in Figure 2.
multimodality, other approaches discretize action dimensions
We test 3D Diffuser Actor in learning robot manipulation andusecrossentropylosses[46,70,85].However,thenumber
policies from demonstrations on the simulation benchmarks of bins needed to approximate a continuous action space
of RLBench [33] and CALVIN [50], as well as in the real growsexponentiallywithincreasingdimensionality.Generative
world. Our model sets a new state-of-the-art on RLBench, adversarial networks [28, 77, 16], variational autoencoders
outperforming existing 3D policies and 2D diffusion policies [48] and combined Categorical and Gaussian distributions
with a 18.1% absolute gain on multi-view setups and 13.1% [67, 23, 14] have been used to learn from multimodal demon-
on single-view setups (Figure 1). On CALVIN, our model strations. Nevertheless, these models tend to be sensitive to
outperforms the current SOTA in the setting of zero-shot hyperparameters, such as the number of clusters used [67].
unseen scene generalization by a 7% relative gain (Figure 1). Implicit behaviour cloning represents distributions over actions
We additionally show that 3D Diffuser Actor outperforms all by using Energy-Based Models (EBMs) [18, 76]. Optimization
existing policy formulations that either do not use 3D scene in EBMs amounts to searching the energy landscape for
representations or do not use action diffusion, as well as minimal-energy action, given a state. EBMs are inherently
…
3D
Relative
Transformermultimodal, since the learned landscape can have more than by sampling 3D points in the empty workspace and featurizing
one minima. EBMs are also composable, which makes them them using cross-attentions to the 3D physical points. Robotic
suitable for combining action distributions with additional View Transformer (RVT) [22] re-projects the input RGB-D
constraints during inference [20]. Diffusion models [73, 29] image to alternative image views, featurizes those and lifts
are a powerful class of generative models related to EBMs in the predictions to 3D to infer 3D locations for the robot’s
that they model the score of the distribution, else, the gradient end-effector. Both Act3D and RVT show currently the highest
of the energy, as opposed to the energy itself [72, 64]. The performance on RLBench [33]. Our model outperforms them
key idea behind diffusion models is to iteratively transform a by a big margin, as we show in the experimental section.
simple prior distribution into a target distribution by applying Instruction-conditioned long-horizon policies To generate
a sequential denoising process. They have been used for actions based on language instructions, early works [45, 49, 4]
modeling state-conditioned action distributions in imitation employ a text encoder to map language instructions to latent
learning [63, 78, 55, 79, 61, 52] from low-dimensional input, features, then they deploy 2D policies that condition on these
as well as from visual sensory input, and show both better language latents to predict actions. HULC++ [51] alternates
mode coverage and higher fidelity in action prediction than between a free-space policy that reaches subgoals (next object
alternatives. They have not been yet combined with 3D scene tointeractwith)andalocalpolicy[49]forlow-levelinteraction.
representations. However, these models struggle to infer 3D actions precisely
DiffusionmodelsinroboticsBeyondpolicyrepresentationsin from 2D observations and language instructions and do not
imitation learning, diffusion models have been used to model generalize to new environments with different texture. Recent
cross-object and object-part arrangements [44, 71, 52, 17, 20] works [2, 39, 81] explore the use of large-scale pre-training to
and visual image subgoals [37, 15, 1, 2]. They have also been boost their performance. SuSIE [2] proposes to synthesize
used successfully in offline reinforcement learning [7, 83, 25], visual subgoals using InstructPix2Pix [6], an instruction-
where they model the state-conditioned action trajectory distri- conditioned image generative model pre-trained on a subset
bution[25,7]orstate-actiontrajectorydistribution[36,27,80]. of 5 billion images [65]. Actions are then predicted by a low-
ChainedDiffuser [82] proposes to replace motion planners, level diffusion policy conditioned on both current observations
commonly used for keypose to keypose linking, with a and synthesized visual goals. RoboFlamingo [39] fine-tunes
trajectory diffusion model that conditions on the 3D scene existing vision-language models, pre-trained for solving vision
feature cloud and the predicted target 3D keypose to denoise a and language tasks, to predict the robot’s actions. GR-1 [81]
trajectory from the current to the target keypose. 3D Diffuser pre-trains a GPT-style transformer for video generation on
Actor instead predicts the next 3D keypose for the robot’s end- a massive video corpus, then jointly trains the model for
effectoralongsidethelinkingtrajectory,whichisamuchharder predicting robot actions and future observations. We show
task than linking two given keyposes. Lastly, image diffusion that 3D Diffuser Actor can exceed the performance of these
modelshavebeenusedforaugmentingtheconditioningimages policies thanks to effective use of 3D representations.
input to robot policies to help the latter generalize better
[41, 9, 47].
III. METHOD
2D and 3D scene representations for robot manipulation The architecture of 3D Diffuser Actor is shown in Figure 3.
End-to-end image-to-action policy models, such as RT-1 [4], It is a conditional diffusion model that takes as input visual
RT-2[5], GATO [60], BC-Z [35], RT-X [54], Octo [53] and observations, a language instruction, a short history of the
InstructRL[43]leveragetransformerarchitecturesforthedirect robot’s end-effectors and the current estimate for the robot’s
prediction of 6-DoF end-effector poses from 2D video input. future action trajectory, and predicts the error in the end-
However,thisapproachcomesatthecostofrequiringthousands effector’s3Dtranslationsand3Dorientationsforeachpredicted
of demonstrations to implicitly model 3D geometry and adapt timestep. We review Denoising Diffusion Probabilistic models
to variations in the training domains. Another line of research in Section III-A and describe the architecture and training
is centered around Transporter networks [85, 66, 69, 20, 30], details of our model in Section III-B.
demonstrating remarkable few-shot generalization by framing
A. Denoising Diffusion Probabilistic Models
end-effector pose prediction as pixel classification. Never-
theless, these models are usually confined to top-down 2D A diffusion model learns to model a probability distribution
planar environments with simple pick-and-place primitives. p(x) by inverting a process that gradually adds noise to a
Direct extensions to 3D, exemplified by C2F-ARM [34] and sample x. For us, x represents a sequence of 3D translations
PerAct [70], involve voxelizing the robot’s workspace and and 3D rotations for the robot’s end-effector. The diffusion
learning to identify the 3D voxel containing the next end- processisassociatedwithavarianceschedule{β ∈(0,1)}T ,
t t=1
effector keypose. However, this becomes computationally which defines how much noise is added at each time step. The
expensive as resolution requirements increase. Consequently, noisy version of sample x at time t can then be written as
√ √
related approaches resort to either coarse-to-fine voxelization x = α¯ x+ 1−α¯ ϵ where ϵ∼N(0,1), is a sample from
t t t
or efficient attention operations [31] to mitigate computational a Gaussian distribution (with the same dimensionality as x),
costs. Act3D [19] foregoes 3D scene voxelization altogether; it α = 1−β , and α¯ =
(cid:81)t
α . The denoising process is
t t t i=1 i
insteadcomputesa3Dactionmapofvariablespatialresolution modeled by a neural network ϵˆ=ϵ (x ;t) that takes as input
θ tEnd-effector
estimate 𝐚!
Denoising step
𝑡
Image
Encoder 𝐚 &"#
3D visual
feature tokens 𝑜
3D Relative
𝜖!"#$(𝑜,𝑙,𝑐,𝐚%,𝑡) Denoise
Transformer 𝜖!&#%(𝑜,𝑙,𝑐,𝐚%,𝑡)
“Open the middle drawer” Language Language
𝑓!#"’((𝑜,𝑙,𝑐 𝐚%,𝑡)
Encoder feature tokens 𝑙 Proprioception 𝑐
(action history)
(a) 3D Diffuser Actor model architecture
…
𝐚 !~𝑁(0,𝐼)
𝜖"#$%,𝜖"&$!
𝐚 !"# 𝐚 $
𝜖"#$%,𝜖"&$!
𝐚 #
𝜖"#$%,𝜖"&$!
Trajectory prediction 𝐚 %
(b) Denoising process
Fig. 3: 3D Diffuser Actor architecture. Top: 3D Diffuser Actor is a denoising diffusion probabilistic model of the robot
3D trajectory conditioned on sensory input, language goals and proprioceptive information (action history). The model is a
3D relative position denoising transformer that featurizes jointly the scene and the current noisy estimate for robot’s future
action trajectory through 3D relative-position attentions. 3D Diffuser Actor outputs position and rotation residuals for denoising,
as well as the end-effector’s state (open/close). Bottom: During inference, 3D Diffuser Actor iteratively denoises the current
estimate for the robot’s future trajectory, where the initial estimate is initialized from pure noise.
the noisy sample x and the noise level t and tries to predict B. 3D Diffuser Actor
t
the noise component ϵ.
We next describe the architecture of our model that predicts
Diffusion models can be easily extended to draw samples the end-effector’s trajectory error given the current noisy
from a distribution p(x|c) conditioned on input c, which is estimate, a visual scene encoding, a language instruction, the
added as input to the network ϵ θ. For us c is the visual denoising iteration index and the end-effector’s pose history.
scene captured by one or more calibrated RGB-D images, Observation space and action representation 3D Diffuser
a language instruction, as well as a short history of the robot’s Actor is trained on demonstrations of successful trajectories in
end-effector’s poses. Given a collection of D ={(xi,ci)}N i=1 the form of {(o 1,a 1),(o 2,a 2),...}, accompanied with a task
ofend-effectortrajectoriesxi pairedwithobservationandrobot languagedescription,similartopreviousworks[32,70,19,81].
history context ci, the denoising objective becomes: Each observation o comprises a set of posed (multi-view)
RGB-D images. Each action a describes an end-effector
(cid:88) √ √
L (θ;D)= 1 ||ϵ ( α¯ xi+ 1−α¯ ϵ,ci,t)−ϵ||. pose and is decomposed into 3D position, 3D orientation
diff |D| θ t t
xi,ci∈D and a binary open/closed state: a = {apos ∈ R3,arot ∈
(1) R6,aopen ∈ {0,1}}. We represent rotations using the 6D
This loss corresponds to a reweighted form of the variational rotation representation of [86] for all environments in all our
lower bound for logp(x|c) [29]. experiments, to avoid the discontinuities of the quaternion
In order to draw a sample from the learned distribution representation.
p (x|c), we start by drawing a sample x ∼N(0,1). Then, Keyposes 3D Diffuser Actor inherits the temporal abstraction
θ T
we progressively denoise the sample by iterated application of of demonstrations into end-effector keyposes from previous
ϵ T timesaccordingtoaspecifiedsamplingschedule[29,74], works [32, 70, 19, 43]. Keyposes are important intermediate
θ
which terminates with x sampled from p (x): end-effector poses that summarize a demonstration and can
0 θ
be extracted using simple heuristics, such as a change in the
(cid:18) (cid:19)
1 β 1−α¯ open/close end-effector state or local extrema of velocity/accel-
x = √ x − √ t ϵ (x ,t,c) + t+1β z,
t−1 α t 1−α¯ θ t 1−α¯ t eration.Wedescribetheheuristicsweuseforeachdatasetinthe
t t t
(2) Appendix (Section A). During inference, 3D Diffuser Actor
where z∼N(0,1). can either predict and execute the full trajectory of actions upto the next keypose (including the keypose), or just predict estimate of each element of the end-effector’s trajectory:
the next keypose and use a sampling-based motion planner (cid:18) (cid:19)
1 β
to reach it, similar to previous works [70, 24, 19]. In the apos =√ apos− √ t ϵpos(o,l,c,apos,arot,t)
t−1 α t 1−α¯ θ t t
rest of this section we use the term “trajectory" to refer to t t
1−α¯
3D Diffuser Actor’s output. Keypose prediction is a special + t+1β zpos (4)
1−α¯ t
case where the trajectory contains only one future pose. t
(cid:18) (cid:19)
1 β
Scene and language encoder We use a scene and language arot =√ arot− √ t ϵrot(o,l,c,arot,arot,t)
t−1 α t 1−α¯ θ t t
encoder similar to [19, 82]. We describe it here to make the t t
1−α¯
paper self-contained. The input to our scene encoder is a set of + t+1β zrot (5)
1−α¯ t
posedRGB-Dimages.Wefirstextractmulti-scalevisualtokens t
for each camera view using a pre-trained 2D feature extractor wherezpos,zrot ∼N(0,1)variablesofappropriatedimension.
and a feature pyramid network [42]. Next, we associate every We use the following two noise schedulers:
2D feature grid location in the 2D feature maps with a depth 1) a scaled-linear noise scheduler β = (β −β )t+
t max min
value, by averaging the depth values of the image pixels that β , where β ,β are hyperparameters, set to 0.02
min max min
correspond to it. We use camera intrinsics and the pinhole and 0.0001 in our experiments,
camera equation to map a pixel location and depth value 2) a squared cosine noise scheduler β =
t
(x,y,d) to a 3D location (X,Y,Z), and “lift” the 2D feature 1−cos(cid:0) (t+1)/T+0.008∗π(cid:1)2
1.008 2 .
tokens to 3D, to obtain a 3D feature cloud. The language cos(cid:0) t/T+0.008∗π(cid:1)2
encoder maps language task descriptions or instructions into 1.008 2
We found using a scale-linear noise schedule for denoising
languagefeaturetokens.Weusethepre-trainedCLIPResNet50
end-effector’s3Dpositionsandasquaredcosinenoiseschedule
2D image encoder [59] to encode each RGB image into a
for denoising the end-effector’s 3D orientations to converge
2D feature map and the pre-trained CLIP language encoder to
much faster than using squared cosine noise for both.
featurize the language task instruction.
Training 3D Diffuser Actor is trained on a dataset of kines-
3DRelativePositionDenoisingTransformer3DDiffuserAc- thetic demonstrations, which consists of tuples of RGB-D
tor iteratively denoises an estimate of the end-effector’s future observations,proprioceptioninformationofrobot’send-effector
trajectory. Every trajectory step token is an 10-dimensional ac- pose c, action trajectories a = [apos,arot] and correspond-
tion,asdefinedearlier.Weprojecteachsuchactionintoahigh- ing language goals D = {(o 1,c 1,a 1,l 1),(o 2,c 2,a 2,l 2),...}.
dimensional representation through an MLP. Each action token During training, we randomly sample a diffusion step t
comeswiththepositionalembeddingsofitscorrespondingapos. and add noise ϵ = (ϵpos,ϵrot) to the ground-truth target
t t
This enables relative cross-attention with the scene and other action. We supervise 3D Diffuser Actor using a denoising
physical entities. We incorporate proprioceptive information objective: conditioning on the tuple of (o,l,c,t), our model
as a short history of predicted end-effector keyposes. These learns to reconstruct the clean actions by predicting the pose
are represented with learnable feature vectors and their 3D transformation with respect to the current estimate. We adopt
positions are used to compute positional embeddings. the L1 loss for reconstructing the 3D position and 3D rotation
error.Weusebinarycross-entropylosstosuperviseend-effector
We contextualize all tokens, namely visual o, language l, opening fopen. Our objective reformulates Equation 1 into:
proprioceptioncandactiontokens(currentestimate)apos,arot θ
t t
using 3D relative position attention layers. Inspired by recent
work in visual correspondence [40, 21] and 3D manipulation 1 (cid:88)|D|
L = BCE(fopen(o ,l ,c ,apos,arot,t),aopen)
[19, 82], we use rotary positional embeddings [75] with the θ |D| θ i i i t,i t,i i
property that the dot product of two positionally encoded i=1
+w ·∥(ϵpos(o ,l ,c ,apos,arot,t)−ϵpos∥
features x i,x j is: 1 θ i i i t,i t,i t
+w ·∥(ϵrot(o ,l ,c ,apos,arot,t)−ϵrot∥, (6)
2 θ i i i t,i t,i t
where w ,w are hyperparameters estimated using cross-
1 2
PE(p ,x )TPE(p ,x )=xTM(p −p )x (3) validation.
i i j j i j i j
Implementation details We lift 2D image features to 3D by
calculating xyz-coordinates of each image pixel, using the
sensed depth and camera parameters. We augment RGB-D
where M is a matrix function which depends only on the
observations with random rescaling and cropping. Nearest
relative positions of points p and p and thus is translation-
i j neighbor interpolation is used for rescaling RGB-D obser-
invariant.TheupdatedactionfeaturetokensarefedtoMLPsto
vations. To reduce the memory footprint in our 3D Relative
predictthepositionerrorϵpos(o,l,c,apos,arot,t),rotationerror
θ t t Transformer,weuseFarthestPointSamplingtosampleasubset
ϵrot(o,l,c,apos,arot,t) and whether the end-effector should
θ t t of the points in the input 3D feature cloud. We use FiLM [56]
be open or closed fopen(o,l,c,apos,arot,t).
θ t t to inject conditional input, including the diffusion step and
WeuseamodifiedversionofEquation2toupdatethecurrent proprioception history, to every attention layer in the model.We include a detailed architecture diagram of our model and 4) PolarNet [8], a 3D policy that computes dense point
a table of hyper-parameters used in our experiments in the representationsfortherobotworkspaceusingaPointNext
Appendix (Section E and Section F). backbone [58].
5) RVT [22], a 3D policy that deploys a multi-view
IV. EXPERIMENTS
transformer to predict actions and fuses those across
We test 3D Diffuser Actor in learning multi-task manip- views by back-projecting to 3D.
ulation policies on RLBench [33] and CALVIN [50], two 6) Act3D [19], a 3D policy that featurizes the robot’s 3D
established learning from demonstrations benchmarks. We workspace using coarse-to-fine sampling and featuriza-
compare against the current state-of-the-art on single-view tion.
and multi-view setups, as well as against ablative versions of 7) GNFactor [84], a 3D policy that co-optimizes a neural
our model that do not consider 3D scene representations or field for reconstructing the 3D voxels of the input scene
relative attention. We also test 3D Diffuser Actor’s ability to and a PerAct module for predicting actions based on
learn multi-task policies from real-world demonstrations. voxel representations.
A. Evaluation on RLBench We report results for RVT, PolarNet and GNFactor based on
their respective papers. Results for CF2-ARM-BC and PerAct
Setup RLBench is built atop the CoppelaSim [62] simulator,
are presented as reported in RVT [22]. Results for Hiveformer
where a Franka Panda Robot is used to manipulate the scene.
are copied from PolarNet [8].
Our 3D Diffuser Actor is trained to predict the next end-
WeobservedthatAct3D[19]doesnotfollowthesamesetup
effector keypose and we employ the low-level motion planner
asPerActonRLBench.Specifically,Act3Dusesdifferent1)3D
BiRRT [38], native to RLBench, to reach the predicted pose,
object models, 2) success conditions, 3) training/test episodes
followingpreviousworks[70,19].Ourmodeldoesnotperform
and 4) maximum numbers of keyposes during evaluation. For
collision checking for our experiments. We train and evaluate
fair comparison, we retrain and test Act3D on the same setup.
3D Diffuser Actor on two experimental setups of multi-task
We also compare to the following ablative versions of our
manipulation:
model:
1) PerAct setup introduced in [70]: This uses a suite of
1) 2D Diffuser Actor, our implementation of [11]. We
18 manipulation tasks, each task has 2-60 variations,
remove the 3D scene encoding from 3D Diffuser Actor
which concern scene variability across object poses,
andinsteaduseper-image2Drepresentationsbyaverage-
appearance and semantics. The tasks are specified by
pooling features within each view. We add learnable
language descriptions. There are four cameras available
embeddings to distinguish between different views. We
(front, wrist, left shoulder, right shoulder). There are
usestandardattentionlayersforjointencodingtheaction
100 training demonstrations available per task, evenly
estimate and 2D image features.
split across task variations and 25 unseen test episodes
2) 3D Diffuser Actor w/o Rel. Attn., an ablative version of
for each task. Due to the randomness of the sampling-
our model that uses standard (non-relative) attentions
basedmotionplanner,wetestourmodelacross5random
to featurize the current rotation and translation estimate
seeds.Duringevaluation,modelsareallowedtopredicta
with the 3D scene feature cloud. This version of our
maximumof25keyposes,unlesstheyreceiveearliertask-
model is not translation-equivariant.
completion indicators from the simulation environment.
Evaluation metrics Following previous work [19, 70], we
2) GNFactor setup introduced in [84]: This uses a suite
evaluate policies by task completion success rate, which is
of 10 manipulation tasks (a subset of PerAct’s task set).
the proportion of execution trajectories that achieve the goal
OnlyoneRGB-Dcameraviewisavailable(frontcamera).
conditions specified in the language instructions.
There are 20 training demonstrations per task, evenly
Results on the PerAct setup We show quantitative results in
split across variations. The evaluation setup mirrors that
Table I. Our 3D Diffuser Actor outperforms all baselines on
of PerAct, with the exception that we test the final
most tasks by a large margin. It achieves an average 81.3%
checkpoint using 3, rather than 5, random seeds on the
success rate among all 18 tasks, an absolute improvement of
test set.
+18.1% over Act3D, the previous state-of-the-art. In particular,
Baselines We consider the following baselines: 3D Diffuser Actor achieves big leaps on tasks with multiple
1) C2F-ARM-BC[34],a3Dpolicythatiterativelyvoxelizes modes, such as stack blocks, stack cups and place cups,
RGB-D images and predicts actions in a coarse-to-fine which most baselines fail to complete. We obtain substantial
manner. Q-values are estimated within each voxel and improvements of +39.5%, +18.4%, +41.6% and +20.8%
the translation action is determined by the centroid of on stack blocks, put in cupboard, insert peg and stack cups
the voxel with the maximal Q-values. respectively.
2) PerAct [70], a 3D policy that voxelizes the workspace Results on the GNFactor setup We show quantitative
and detects the next voxel action through global self- results in Table II. We train Act3D on this setup using its
attention. publicly available code. 3D Diffuser Actor outperforms both
3) Hiveformer [24], a 3D policy that enables attention GNFactor and Act3D by a significant margin, achieving abso-
between features of different history time steps. lute performance gains of +46.4% and +13.1% respectively.Avg. open slide sweep to meat off turn put in close drag stack
Success ↑ drawer block dustpan grill tap drawer jar stick blocks
C2F-ARM-BC [34] 20.1 20 16 0 20 68 4 24 24 0
PerAct [70] 49.4 88.0±5.7 74.0±13.0 52.0±0.0 70.4±2.0 88.0±4.4 51.2±4.7 55.2±4.7 89.6±4.1 26.4±3.2
HiveFormer [23] 45 52 64 28 100 80 68 52 76 8
PolarNet [8] 46 84 56 52 100 80 32 36 92 4
RVT [22] 62.9 71.2±6.9 81.6±5.4 72.0±0.0 88.0±2.5 93.6±4.1 88.0±5.7 52.0±2.5 99.2±1.6 28.8±3.9
Act3D [19] 63.2 78.4±11.2 96.0±2.5 86.4±6.5 95.2±1.6 94.4±2.0 91.2±6.9 96.8±3.0 80.8±6.4 4.0±3.6
3D Diffuser Actor (ours) 81.3 89.6±4.1 97.6±3.2 84.0±4.4 96.8±1.6 99.2±1.6 96.0±3.6 96.0±2.5 100.0±0.0 68.3±3.3
screw put in place put in sort push insert stack place
bulb safe wine cupboard shape buttons peg cups cups
C2F-ARM-BC [34] 8 12 8 0 8 72 4 0 0
PerAct [70] 17.6±2.0 86.0±3.6 44.8±7.8 28.0±4.4 16.8±4.7 92.8±3.0 5.6±4.1 2.4±2.2 2.4±3.2
HiveFormer [23] 8 76 80 32 8 84 0 0 0
PolarNet [8] 44 84 40 12 12 96 4 8 0
RVT [22] 48.0±5.7 91.2±3.0 91.0±5.2 49.6±3.2 36.0±2.5 100.0±0.0 11.2±3.0 26.4±8.2 4.0±2.5
Act3D [19] 32.8±6.9 95.2±4.0 59.2±9.3 67.2±3.0 29.6±3.2 93.6±2.0 24.0±8.4 9.6±6.0 3.2±3.0
3D Diffuser Actor (ours) 82.4±2.0 97.6±2.0 93.6±4.8 85.6±4.1 44.0±4.4 98.4±2.0 65.6±4.1 47.2±8.5 24.0±7.6
TABLE I: Multi-Task performance on RLBench. We report success rates on 18 RLBench tasks of the same test split as
PerAct [70]. Four camera views are used for all experiments. We evaluate the final checkpoint over 5 random seeds with the
same 3D object models and success conditions of RLBench [33] as PerAct. We show the mean and standard deviation of
success rates average across all random seeds. Our 3D Diffuser Actor outperforms all prior arts among most tasks by a large
margin. Variances are included when available.
Avg. close open sweep to meat off turn slide put in drag push stack
Success. jar drawer dustpan grill tap block drawer stick buttons blocks
GNFactor [84] 31.7 25.3 76.0 28.0 57.3 50.7 20.0 0.0 37.3 18.7 4.0
Act3D [19] 65.3 52.0 84.0 80.0 66.7 64.0 100.0 54.7 86.7 64.0 0.0
3D Diffuser Actor 78.4 82.7 89.3 94.7 88.0 80.0 92.0 77.3 98.7 69.3 12.0
TABLE II: Multi-Task performance on RLBench with single camera view. Following GNFactor [84], we report sucess rates
on 10 RLBench tasks using only the front camera view. All models are trained with 20 demonstrations per task. We evaluate
the final checkpoints across 3 seeds with 25 episodes for each task in the test set. Our 3D Diffuser Actor outperforms prior
state-of-the-art baselines–GNFactor and Act3D–among most tasks by a large margin.
Avg. Success.
Ablations We ablate the use of 3D scene representations
2D Diffuser Actor 47.0 and the use of relative attentions in the PerAct experimental
3D Diffuser Actor w/o Rel. Attn. 71.3 setup in Table III. We draw the following conclusions: 1.
3D Diffuser Actor (ours) 81.3 3D Diffuser Actor largely outperforms its 2D counterpart, 2D
Diffuser Actor, underlining the importance of 3D scene repre-
TABLE III: Ablation study. We follow the same simulation sentations. 2. 3D Diffuser Actor with absolute 3D attentions
setup as [70] and report average success rate on 18 RLBench (3D Diffuser Actor w/o Rel. Attn.) is much worse than
tasks. 3D Diffuser Actor significantly outperforms its counter- 3D Diffuser Actor with relative 3D attentions. This shows
parts that do not use 3D scene representations or translation- that translation equivariance through relative attentions is very
equivariant 3D relative attention. important for generalization. Notably, this baseline already
outperforms all prior arts in Table I, proving the effectiveness
of marrying 3D representations and diffusion policies.
Notably,3D Diffuser ActorandAct3Dutilizesimilar3Dscene
B. Evaluation on CALVIN
representations—sparse 3D feature tokens—while GNFactor
TheCALVINbenchmarkisbuildontopofthePyBullet[13]
featurizes a scene with 3D voxels and learns to reconstruct
simulator and involves a Franka Panda Robot arm that
them. Even with a single camera view, both Act3D and
manipulates the scene. CALVIN consists of 34 tasks and 4
3D Diffuser Actor outperform GNFactor significantly. This
suggests that the choice of 3D scene representation is a more
The definition of “task" varies between different benchmarks: CALVIN
crucial factor than the completion of 3D scenes in developing
considers the manipulation of variations (e.g., same objects with different
efficient 3D manipulation policies. colors)asdifferenttasks,whereas,RLBenchconsidersthosethesametask.Train Task completed in a row
episodes 1 2 3 4 5 Avg. Len
MCIL [45] All 30.4 1.3 0.2 0.0 0.0 0.31
HULC [49] All 41.8 16.5 5.7 1.9 1.1 0.67
RT-1 [4] Lang 53.3 22.2 9.4 3.8 1.3 0.90
RoboFlamingo [39] Lang 82.4 61.9 46.6 33.1 23.5 2.48
SuSIE [2] All 87.0 69.0 49.0 38.0 26.0 2.69
GR-1 [81] Lang 85.4 71.2 59.6 49.7 40.1 3.06
3D Diffuser Actor (ours, 60 keyposes) Lang 92.2 78.7 63.9 51.2 41.2 3.27
3D Diffuser Actor (ours, 360 keyposes) Lang 95.7 88.1 78.6 65.7 55.3 3.83
TABLE IV: Zero-shot long-horizon performance on CALVIN. We report success rates and average length of completed
task sequences. All models are trained on play trajectories on environments A, B, C, and tested on 1000 unique instruction
chains on environment D. MCIL, HULC and SuSIE are trained with all play trajectories in the training scenes, whereas, RT-1,
RoboFlamingo, GR-1 and our 3D Diffuser Actor are trained with the subset of play data annotated with language descriptions.
Our 3D Diffuser Actor achieves competitive performance comparable to the state-of-the-art model, GR-1. For reference, we
include our model’s performance when allowed for more predictions, in which case the performance significantly increases.
different environments (A, B, C and D). All environments observation and task description, then conditions on this
are equipped with a desk, a sliding door, a drawer, a button latent to predict actions.
that turns on/off an LED, a switch that controls a lightbulb 3) RT-1 [4], a 2D transformer-based policy that encodes
and three different colored blocks (red, blue and pink). These the image and language into a sequence of tokens and
environments differ from each other in the texture of the desk employs a Transformer-based architecture that contextu-
andpositionsoftheobjects.CALVINprovides24hoursoftele- alizes these tokens and predicts the arm movement or
operated unstructured play data, 35% of which are annotated terminates the episode.
with language descriptions. 4) RoboFlamingo [39], a 2D policy that adapts existing
We train 3D Diffuser Actor on the subset of play data vision-languagemodels,whicharepre-trainedforsolving
annotated with language descriptions on the environments visionandlanguagetasks,torobotcontrol.Itusesfrozen
A, B and C and evaluate it on 1000 unique instruction vision and language foundational models and learns a
chains on environment D, following prior works [39, 81]. cross-attention between language and visual features, as
Each instruction chain includes five language instructions that well as a recurrent policy that predicts the low-level
need to be executed sequentially. We devise an algorithm actions conditioned on the language latents.
(Appendix, Section A) to extract keyposes on CALVIN, since 5) SuSIE [2], a 2D policy that deploys an large-scale
prior works do not use keyposes on this benchmark. We pre-trained image generative model [6] to synthesize
train our 3D Diffuser Actor to predict both the end-effector visual subgoals based on the current observation and
pose for each keypose and the corresponding trajectory to languageinstruction.Actionsarethenpredictedbyalow-
reach the predicted pose, instead of using a motion planner. level goal-conditioned 2D diffusion policy that models
Prior works [49, 2, 39] predict a maximum of 360 actions inverse dynamics between the current observation and
to complete each instructional task, while, on average, it the predicted subgoal image.
takes only 60 actions to complete each task using ground- 6) GR-1 [81], a 2D policy that first pre-trains an autore-
truth trajectories. Our model predicts both keyposes and gressive Transformer on next frame prediction, using a
correspondingtrajectories,and,onaverage,ittakes10keyposes large-scalevideocorpuswithoutactionannotations.Each
of each demonstration to complete an instructional task. We video frame is encoded into an 1d vector by average-
thusallowourmodeltopredictamaximumof60keyposesfor poolingitsvisual features.Then,thesame architectureis
each task. For reference, we also allow our model to predict fine-tuned in-domain to predict both actions and future
360 times and show the influence of this hyperparameter in observations.
the Appendix (Section B).
We report results for HULC, RoboFlamingo, SuSIE and GR-1
Baselines. We consider the following baselines: from the respective papers. Results from MCIL are borrowed
from [49]. Results from RT-1 are copied from [81].
1) MCIL [45], a multi-modal goal-conditioned 2D policy
that maps three types of goals–goal images, language Evaluation metrics Following previous works [49, 81], we
instructions and task labels–to a shared latent feature report the success rate and the average number of completed
space, and conditions on such latent goals to predict sequential tasks.
actions. Results We present our results in Table IV. 3D Diffuser Actor
2) HULC [49], a 2D policy that uses a variational au- achieves competitive results compared to the state-of-the-art
toencoder to sample a latent plan based on the current models. In comparison to GR-1, our model achieves absoluteclose put insert peg insert peg put open
andweputoneofthreegrapesinthebowl,asshowninFigure
box duck into hole into torus mouse pen
4. 3D Diffuser Actor conditions on language descriptions and
100 100 50 30 80 100 is trained to predict the next end-effector keypose. During
inference, we utilize the BiRRT [38] planner provided by the
press put sort stack stack put block MoveIt! ROS package [12] to reach the predicted poses. We
stapler grapes rectangle blocks cups in triangle evaluate 10 episodes for each task are report the success rate.
We show quantitative results in Table V and video results
90 90 50 20 40 90
on our project webpage.
TABLE V: Multi-Task performance on real-world tasks.
D. Run time
We measure the latency of our 3D Diffuser Actor on
CALVIN in simulation, using an NVIDIA GeForce 2080 Ti
graphic card. The wall-clock time of 3D Diffuser Actor is 600
ms.Notably,ourmodelpredictsend-effectorkeyposessparsely,
resulting in better efficiency than methods that predict actions
densely at each time step. On CALVIN, on average, it takes
10 keyposes / 60 actions to complete each task using ground-
truth trajectories. Our 3D Diffuser Actor can thus perform
manipulation efficiently.
"insert the peg "put grapes "put a mouse
E. Limitations
into a hole" in the bowl" on the pad"
Our framework currently has the following limitations: 1.
Fig. 4: Visualized results of our real-world manipulation.
Our model conditions on 3D scene representations, which
requirecameracalibrationanddepthinformation.2.Alltasksin
RLBench and CALVIN are quasi-static. Extending our method
performance gains of +6.8%, +7.5%, +4.3%,+1.5% and
to dynamic tasks and velocity control is a direct avenue of
+1.1$ for completing the five tasks in the instruction chain.
future work.
On average, 3D Diffuser Actor complete 3.27 tasks in a row,
setting a new state of the art. Note that GR-1’s contribution is
large-scalepre-trainingonvideos,whichisaschemeorthogonal V. CONCLUSION
to our contribution. Lastly, allowing our model to predict
We present 3D Diffuser Actor, a 3D robot manipulation
more times increases the performance significantly. For this
policywithactiondiffusion.Ourmethodsetsanewstate-of-the-
performance, we found stronger conditioning on the language
art on RLBench and CALVIN by a large margin over existing
instructions to be important, as we explain in our appendix
3D policies and 2D diffusion policies. We introduce important
(Figure 6b).
architectural innovations, such as 3D token representations of
C. Evaluation in the real world the robot’s pose estimate and 3D relative position denoising
We validate 3D Diffuser Actor in learning manipulation transformers that endow 3D Diffuser Actor with translation
tasks from real-world demonstrations. We use a Franka Emika equivariance, and empirically verify their contribution to
robot and capture visual observations with a Azure Kinect performance. We further test our model in the real world
RGB-D sensor at a front view. Images are originally captured and show it can learn manipulation tasks from a handful
at 1280×720 resolution and downsampled to a resolution of of demonstrations. Our future work will attempt to train
256×256.Cameraextrinsicsarecalibratedw.r.ttherobotbase. 3D Diffuser Actor policies in domain-randomized simulation
We use 12 tasks: 1) close a box, 2) put ducks in bowls, 3) environments at a large scale, to help them transfer from
insertapegverticallyintothehole,4)insertapeghorizontally simulation to the real world.
into the torus, 5) put a computer mouse on the pad, 6) open
the pen, 7) press the stapler, 8) put grapes in the bowl, 9) VI. ACKNOWLEDGEMENTS
sort the rectangle, 10) stack blocks with the same shape, 11)
stack cups and 12) put block in a triangle on the plate. Details ThisworkissupportedbySonyAI,NSFawardNo1849287,
on the task definitions and success criteria are included in the DARPA Machine Common Sense, an Amazon faculty award,
Appendix (Section C). and an NSF CAREER award. The authors would like to
We collect 15 demonstrations per task where we record the thank Moritz Reuss for useful training tips on CALVIN; Zhou
keyposes, most of which naturally contain noise and multiple Xian for help with the real-robot experiments; Brian Yang for
modes of human behavior. For example, we pick one of two discussions, comments and efforts in the early development of
duckstoputinthebowl,weinsertthepegintooneoftwoholes this paper.REFERENCES based policy learning from demonstration using gaussian
mixture models. In Proceedings of the 6th Interna-
[1] Anurag Ajay, Seungwook Han, Yilun Du, Shaung Li, tional Joint Conference on Autonomous Agents and
Gupta Abhi, Tommi Jaakkola, Josh Tenenbaum, Leslie Multiagent Systems, AAMAS ’07, New York, NY, USA,
Kaelbling, Akash Srivastava, and Pulkit Agrawal. Com- 2007. Association for Computing Machinery. ISBN
positional foundation models for hierarchical planning. 9788190426275. doi: 10.1145/1329125.1329407. URL
arXiv preprint arXiv:2309.08587, 2023. https://doi.org/10.1145/1329125.1329407.
[2] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, [11] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Cousineau, Benjamin Burchfiel, and Shuran Song. Dif-
Levine. Zero-shot robotic manipulation with pre- fusion policy: Visuomotor policy learning via action
trained image-editing diffusion models. arXiv preprint diffusion. arXiv preprint arXiv:2303.04137, 2023.
arXiv:2310.10639, 2023. [12] David Coleman, Ioan Sucan, Sachin Chitta, and Nikolaus
[3] MariuszBojarski,DavideDelTesta,DanielDworakowski, Correll. Reducing the barrier to entry of complex
BernhardFirner,BeatFlepp,PrasoonGoyal,LawrenceD. robotic software: a moveit! case study. arXiv preprint
Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin arXiv:1404.3785, 2014.
Zhang, Jake Zhao, and Karol Zieba. End to end learning [13] Erwin Coumans and Yunfei Bai. Pybullet, a python
for self-driving cars. CoRR, abs/1604.07316, 2016. URL module for physics simulation for games, robotics and
http://arxiv.org/abs/1604.07316. machine learning. 2016.
[4] Anthony Brohan, Noah Brown, Justice Carbajal, Yev- [14] Zichen Jeff Cui, Yibin Wang, Nur Muhammad (Mahi)
gen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Shafiullah, and Lerrel Pinto. From play to policy:
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Conditional behavior generation from uncurated robot
Hsu, et al. Rt-1: Robotics transformer for real-world data. ArXiv, abs/2210.10047, 2022.
control at scale. arXiv preprint arXiv:2212.06817, 2022. [15] Yilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir
[5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Abbeel. Learning universal policies via text-guided video
Danny Driess, Avinava Dubey, Chelsea Finn, Pete Flo- generation. arXiv preprint arXiv:2302.00111, 2023.
rence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana [16] Yiming Ding, Carlos Florensa, Pieter Abbeel, and
Gopalakrishnan,KehangHan,KarolHausman,Alexander Mariano Phielipp. Goal-conditioned imitation learning.
Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil In H. Wallach, H. Larochelle, A. Beygelzimer,
Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,
Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Advances in Neural Information Processing Systems,
Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, volume 32. Curran Associates, Inc., 2019. URL
Karl Pertsch, Kanishka Rao, Krista Reymann, Michael https://proceedings.neurips.cc/paper_files/paper/2019/
Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, file/c8d3a760ebab631565f8509d84b3b3f1-Paper.pdf.
Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, [17] Xiaolin Fang, Caelan Reed Garrett, Clemens Eppner,
Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Tomás Lozano-Pérez, Leslie Pack Kaelbling, and Dieter
Welker,PaulWohlhart,JialinWu,FeiXia,TedXiao,Peng Fox. Dimsam: Diffusion models as samplers for task
Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: and motion planning under partial observability. arXiv
Vision-language-action models transfer web knowledge preprint arXiv:2306.13196, 2023.
to robotic control, 2023. [18] Pete Florence, Corey Lynch, Andy Zeng, Oscar Ramirez,
[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny
Instructpix2pix: Learning to follow image editing instruc- Lee, Igor Mordatch, and Jonathan Tompson. Implicit
tions. In Proceedings of the IEEE/CVF Conference on behavioral cloning. CoRR, abs/2109.00137, 2021. URL
Computer Vision and Pattern Recognition, pages 18392– https://arxiv.org/abs/2109.00137.
18402, 2023. [19] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and
[7] Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Katerina Fragkiadaki. Act3d: Infinite resolution action
Jun Zhu. Offline reinforcement learning via high-fidelity detection transformer for robotic manipulation. arXiv
generative behavior modeling, 2023. preprint arXiv:2306.17817, 2023.
[8] ShizheChen,RicardoGarciaPinel,CordeliaSchmid,and [20] Nikolaos Gkanatsios, Ayush Jain, Zhou Xian, Yunchu
IvanLaptev.Polarnet:3dpointcloudsforlanguage-guided Zhang, Christopher Atkeson, and Katerina Fragki-
robotic manipulation. ArXiv, abs/2309.15596, 2023. adaki. Energy-based models as zero-shot planners for
[9] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash compositional scene rearrangement. arXiv preprint
Kumar. Genaug: Retargeting behaviors to unseen sit- arXiv:2304.14391, 2023.
uations via generative augmentation. arXiv preprint [21] Nikolaos Gkanatsios, Mayank Kumar Singh, Zhaoyuan
arXiv:2302.06671, 2023. Fang, Shubham Tulsiani, and Katerina Fragkiadaki.
[10] Sonia Chernova and Manuela Veloso. Confidence- Analogy-forming transformers for few-shot 3d parsing.ArXiv, abs/2304.14382, 2023. [35] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler,
[22] Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea
Chao, and Dieter Fox. Rvt: Robotic view transformer for Finn. Bc-z: Zero-shot task generalization with robotic
3d object manipulation. arXiv preprint arXiv:2306.14896, imitation learning. In Conference on Robot Learning,
2023. pages 991–1002. PMLR, 2022.
[23] Pierre-Louis Guhur, Shizhe Chen, Ricardo Garcia Pinel, [36] Michael Janner, Yilun Du, Joshua B Tenenbaum, and
Makarand Tapaswi, Ivan Laptev, and Cordelia Schmid. Sergey Levine. Planning with diffusion for flexible
Instruction-driven history-aware policies for robotic ma- behavior synthesis. arXiv preprint arXiv:2205.09991,
nipulations. In6thAnnualConferenceonRobotLearning, 2022.
2022. URL https://openreview.net/forum?id=h0Yb0U_ [37] IvanKapelyukh,VitalisVosylius,andEdwardJohns. Dall-
-Tki. e-bot: Introducing web-scale diffusion models to robotics.
[24] Pierre-Louis Guhur, Shizhe Chen, Ricardo Garcia Pinel, IEEE Robotics and Automation Letters, 2023.
Makarand Tapaswi, Ivan Laptev, and Cordelia Schmid. [38] James J Kuffner and Steven M LaValle. Rrt-connect:
Instruction-driven history-aware policies for robotic ma- An efficient approach to single-query path planning. In
nipulations. In Conference on Robot Learning, pages Proceedings 2000 ICRA. Millennium Conference. IEEE
175–187. PMLR, 2023. International Conference on Robotics and Automation.
[25] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Symposia Proceedings (Cat. No. 00CH37065), volume 2,
JakubGrudzienKuba,andSergeyLevine. Idql:Implicitq- pages 995–1001. IEEE, 2000.
learning as an actor-critic method with diffusion policies. [39] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu,
arXiv preprint arXiv:2304.10573, 2023. Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan
[26] Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gau- Zhang, Huaping Liu, et al. Vision-language foundation
rav S. Sukhatme, and Joseph J. Lim. Multi-modal models as effective robot imitators. arXiv preprint
imitationlearningfromunstructureddemonstrationsusing arXiv:2311.01378, 2023.
generative adversarial nets. CoRR, abs/1705.10479, 2017. [40] Yang Li and Tatsuya Harada. Lepard: Learning partial
URL http://arxiv.org/abs/1705.10479. point cloud matching in rigid and deformable scenes.
[27] Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, 2022 IEEE/CVF Conference on Computer Vision and
Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Pattern Recognition (CVPR), 2022.
Li. Diffusion model is an effective planner and data [41] ZhixuanLiang,YaoMu,MingyuDing,FeiNi,Masayoshi
synthesizer for multi-task reinforcement learning. arXiv Tomizuka, and Ping Luo. Adaptdiffuser: Diffusion
preprint arXiv:2305.18459, 2023. models as adaptive self-evolving planners. arXiv preprint
[28] Jonathan Ho and Stefano Ermon. Generative adversarial arXiv:2302.01877, 2023.
imitation learning. CoRR, abs/1606.03476, 2016. URL [42] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,
http://arxiv.org/abs/1606.03476. Bharath Hariharan, and Serge Belongie. Feature pyramid
[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising networksforobjectdetection. InProceedingsoftheIEEE
diffusion probabilistic models. CoRR, abs/2006.11239, conference on computer vision and pattern recognition,
2020. URL https://arxiv.org/abs/2006.11239. pages 2117–2125, 2017.
[30] Haojie Huang, Owen Howell, Xupeng Zhu, Dian Wang, [43] Hao Liu, Lisa Lee, Kimin Lee, and Pieter Abbeel.
Robin Walters, and Robert Platt. Fourier transporter: Bi- Instruction-following agents with jointly pre-trained
equivariant robotic manipulation in 3d. In ICLR, 2024. vision-languagemodels. arXivpreprintarXiv:2210.13431,
[31] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew 2022.
Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: [44] Weiyu Liu, Tucker Hermans, Sonia Chernova, and Chris
General perception with iterative attention, 2021. Paxton. Structdiffusion: Object-centric diffusion for
[32] Stephen James and Andrew J Davison. Q-attention: semantic rearrangement of novel objects. arXiv preprint
Enabling efficient learning for vision-based robotic ma- arXiv:2211.04604, 2022.
nipulation. IEEE Robotics and Automation Letters, 7(2): [45] Corey Lynch and Pierre Sermanet. Language conditioned
1612–1619, 2022. imitation learning over unstructured data. arXiv preprint
[33] StephenJames,ZicongMa,DavidRovickArrojo,andAn- arXiv:2005.07648, 2020.
drew J Davison. Rlbench: The robot learning benchmark [46] Corey Lynch and Pierre Sermanet. Grounding language
& learning environment. IEEE Robotics and Automation in play. CoRR, abs/2005.07648, 2020. URL https://arxiv.
Letters, 5(2):3019–3026, 2020. org/abs/2005.07648.
[34] Stephen James, Kentaro Wada, Tristan Laidlow, and [47] ZhaoMandi,HomangaBharadhwaj,VincentMoens,Shu-
Andrew J Davison. Coarse-to-fine q-attention: Efficient ran Song, Aravind Rajeswaran, and Vikash Kumar. Cacti:
learning for visual robotic manipulation via discretisation. A framework for scalable multi-task multi-scene visual
InProceedingsoftheIEEE/CVFConferenceonComputer imitation learning. arXiv preprint arXiv:2212.05711,
Vision and Pattern Recognition, pages 13739–13748, 2022.
2022. [48] Ajay Mandlekar, Fabio Ramos, Byron Boots, Li Fei-Fei,Animesh Garg, and Dieter Fox. IRIS: implicit reinforce- Priya Sundaresan, Quan Ho Vuong, Rafael Rafailov,
mentwithoutinteractionatscaleforlearningcontrolfrom Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah,
offline robot manipulation data. CoRR, abs/1911.05321, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean
2019. URL http://arxiv.org/abs/1911.05321. Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl,
[49] Oier Mees, Lukas Hermann, and Wolfram Burgard. What Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar,
mattersinlanguageconditionedroboticimitationlearning S. O. Adebola, Simon Guist, Soroush Nasiriany, Stefan
over unstructured data. IEEE Robotics and Automation Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari,
Letters (RA-L), 7(4):11205–11212, 2022. Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya
[50] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor
Wolfram Burgard. Calvin: A benchmark for language- Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell,
conditioned policy learning for long-horizon robot ma- VidhiJain,VincentVanhoucke,WeiZhan,WenxuanZhou,
nipulation tasks. IEEE Robotics and Automation Letters Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao
(RA-L), 7(3):7327–7334, 2022. Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou,
[51] Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk,
Grounding language with visual affordances over unstruc- Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh hua
tured data. In ICRA, 2023. Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa,
[52] Utkarsh A Mishra and Yongxin Chen. Reorientdiff: Dif- Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open
fusion model based reorientation for object manipulation. x-embodiment: Robotic learning datasets and rt-x models.
arXiv preprint arXiv:2303.12700, 2023. ArXiv, abs/2310.08864, 2023.
[53] Octo Model Team, Dibya Ghosh, Homer Walke, Karl [55] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave
Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Bignell,MingfeiSun,RalucaGeorgescu,SergioValcarcel
Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Macua, Shan Zheng Tan, Ida Momennejad, Katja Hof-
Liang Tan, Dorsa Sadigh, Chelsea Finn, and Sergey mann, and Sam Devlin. Imitating human behaviour with
Levine. Octo: An open-source generalist robot policy. diffusion models, 2023.
https://octo-models.github.io, 2023. [56] Ethan Perez, Florian Strub, Harm De Vries, Vincent
[54] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Dumoulin, and Aaron Courville. Film: Visual reasoning
Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, with a general conditioning layer. In Proceedings of the
AnantRai,AnikaSingh,AnthonyBrohan,AntoninRaffin, AAAI conference on artificial intelligence, volume 32,
Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, 2018.
Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles [57] Dean Pomerleau. Alvinn: An autonomous land vehicle in
Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang aneuralnetwork.InD.S.Touretzky,editor,Proceedingsof
Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline (NeurIPS) Neural Information Processing Systems, pages
Devin,DannyDriess,DeepakPathak,DhruvShah,Dieter 305 – 313. Morgan Kaufmann, December 1989.
Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward [58] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,
Johns,FedericoCeola,FeiXia,FreekStulp,GaoyueZhou, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny,
Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio and Bernard Ghanem. Pointnext: Revisiting pointnet++
Schiavi, Hao Su, Haoshu Fang, Haochen Shi, Heni Ben withimprovedtrainingandscalingstrategies. InNeurIPS,
Amor,HenrikIChristensen,HirokiFuruta,HomerWalke, 2022.
Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Leal,JackyLiang,JaehyungKim,JanSchneider,Jasmine Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Hsu,JeannetteBohg,JeffBingham,JiajunWu,JialinWu, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Learningtransferablevisualmodelsfromnaturallanguage
Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, supervision. In International conference on machine
João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, learning, pages 8748–8763. PMLR, 2021.
Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, [60] Scott Reed, Konrad Zolna, Emilio Parisotto, Ser-
Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento gio Gomez Colmenarejo, Alexander Novikov, Gabriel
Kawaharazuka, Kevin Zhang, Keyvan Majd, Krishan Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
Rana, Krishna Parasuram Srinivasan, Lawrence Yun- Jost Tobias Springenberg, et al. A generalist agent. arXiv
liang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa preprint arXiv:2205.06175, 2022.
Lee, Masayoshi Tomizuka, Maximilian Du, Michael [61] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf
Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Lioutikov. Goal-conditioned imitation learning us-
Srirama,MohitSharma,MooJinKim,NaoakiKanazawa, ing score-based diffusion policies. arXiv preprint
Nicklas Hansen, Nicolas Manfred Otto Heess, Nikhil J. arXiv:2304.02532, 2023.
Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muham- [62] Eric Rohmer, Surya PN Singh, and Marc Freese. V-rep:
mad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pan- A versatile and scalable robot simulation framework. In
nagR.Sanketi,PaulWohlhart,PengXu,PierreSermanet, 2013 IEEE/RSJ international conference on intelligentrobots and systems, pages 1321–1326. IEEE, 2013. 2022.
[63] Hyunwoo Ryu, Jiwoo Kim, Junwoo Chang, Hyun Seok [77] Yoshihisa Tsurumine and Takamitsu Matsubara. Goal-
Ahn, Joohwan Seo, Taehan Kim, Jongeun Choi, and aware generative adversarial imitation learning from
Roberto Horowitz. Diffusion-edfs: Bi-equivariant de- imperfect demonstration for robotic cloth manipulation,
noising generative modeling on se (3) for visual robotic 2022.
manipulation. arXiv preprint arXiv:2309.02685, 2023. [78] Julen Urain, Niklas Funk, Jan Peters, and Georgia
[64] Tim Salimans and Jonathan Ho. Should EBMs model the Chalvatzaki. Se (3)-diffusionfields: Learning smooth cost
energy or the score? In Energy Based Models Workshop functions for joint grasp and motion optimization through
- ICLR 2021, 2021. URL https://openreview.net/forum? diffusion. In 2023 IEEE International Conference on
id=9AS-TF2jRNb. RoboticsandAutomation(ICRA),pages5923–5930.IEEE,
[65] ChristophSchuhmann,RomainBeaumont,RichardVencu, 2023.
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo [79] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou.
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- Diffusion policies asan expressive policy classfor offline
man, et al. Laion-5b: An open large-scale dataset for reinforcement learning. arXiv preprint arXiv:2208.06193,
trainingnextgenerationimage-textmodels. arXivpreprint 2022.
arXiv:2210.08402, 2022. [80] Zidan Wang, Takeru Oba, Takuma Yoneda, Rui Shen,
[66] Daniel Seita, Pete Florence, Jonathan Tompson, Erwin Matthew R. Walter, and Bradly C. Stadie. Cold diffusion
Coumans, Vikas Sindhwani, Ken Goldberg, and Andy on the replay buffer: Learning to plan from known good
Zeng. Learning to rearrange deformable cables, fabrics, states. ArXiv, abs/2310.13914, 2023.
and bags with goal-conditioned transporter networks, [81] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen,
2021. Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and
[67] Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariun- Tao Kong. Unleashing large-scale video generative pre-
tuya Altanzaya, and Lerrel Pinto. Behavior transformers: training for visual robot manipulation. arXiv preprint
Cloning k modes with one stone, 2022. arXiv:2312.13139, 2023.
[68] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self- [82] Zhou Xian, Nikolaos Gkanatsios, Theophile Gervet,
attention with relative position representations, 2018. Tsung-WeiKe,andKaterinaFragkiadaki.Chaineddiffuser:
[69] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: Unifying trajectory diffusion and keypose prediction for
What and where pathways for robotic manipulation. In robotic manipulation. In Conference on Robot Learning,
Conference on Robot Learning, pages 894–906. PMLR, pages 2323–2339. PMLR, 2023.
2022. [83] Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, Tsung-
[70] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Wei Ke, Ayush Jain, Jeff Schneider, and Katerina Fragki-
Perceiver-actor: A multi-task transformer for robotic adaki. Diffusion-es:Gradient-freeplanningwithdiffusion
manipulation. In Conference on Robot Learning, pages for autonomous driving and zero-shot instruction follow-
785–799. PMLR, 2023. ing. ArXiv, abs/2402.06559, 2024.
[71] Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin [84] Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso,
Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li,
Agrawal, and Dieter Fox. Shelving, stacking, hanging: and Xiaolong Wang. Gnfactor: Multi-task real robot
Relational pose diffusion for multi-modal rearrangement. learning with generalizable neural feature fields. arXiv
arXiv preprint arXiv:2307.04751, 2023. preprint arXiv:2308.16891, 2023.
[72] Sumeet Singh, Stephen Tu, and Vikas Sindhwani. Re- [85] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan
visiting energy based models as policies: Ranking noise Welker,JonathanChien,MariaAttarian,TravisArmstrong,
contrastive estimation and interpolating energy models, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Trans-
2023. porter networks: Rearranging the visual world for robotic
[73] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah- manipulation. In Conference on Robot Learning, pages
eswaranathan, and Surya Ganguli. Deep unsupervised 726–747. PMLR, 2021.
learning using nonequilibrium thermodynamics, 2015. [86] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and
[74] Jiaming Song, Chenlin Meng, and Stefano Ermon. Hao Li. On the continuity of rotation representations in
Denoising diffusion implicit models. arXiv preprint neural networks, 2020.
arXiv:2010.02502, 2020.
[75] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-
former with rotary position embedding. arXiv preprint
arXiv:2104.09864, 2021.
[76] Duy-Nguyen Ta, Eric Cousineau, Huihua Zhao, and
Siyuan Feng. Conditional energy-based models for
implicit policies: The gap between theory and practice,APPENDIX 39 # significant motion change of the end effector
We explain our algorithms for keypose discovery on all40 local_max_A = argrelextrema(A, np.greater)[0]
41
benchmarks in Section A. We show the effect of maximum 42 # consider the top 20% of local maximas
number of predicted keyposes on CALVIN in Section B. We43 K = int(A.shape[0] * 0.2)
detailthereal-worldandRLBenchtasksandsuccessconditions44 topK = np.sort(A)[::-1][K]
45 large_A = A[local_max_A] >= topK
in Section C and Section D. We present a detailed version of 46 local_max_A = local_max_A[large_A].tolist()
our architecture in Section E. We list the hyper-parameters47
for each experiment in Section F. Lastly, we visualize the48 # select waypoints sparsely
49 key_poses = [local_max_A.pop(0)]
importance of a square cos variance scheduler for denoising 50 for i in local_max_A:
the rotation estimate in Section G. 51 if i - key_poses[-1] >= buffer_size:
52 key_poses.append(i)
A. Our heuristics for keypose discovery 53
54 return key_poses
For RLBench we use the heuristics from [32, 34]: a pose55
is a keypose if (1) the end-effector state changes (grasp or56 def gripper_state_changed(trajectories):
57 """Select keyposes where the end-effector
release) or (2) the velocity’s magnitude approaches zero (often 58 opens/closes.
atpre-graspposesoranewphaseofatask).Forourreal-world59
experiments we maintain the above heuristics and record pre-60 Args:
61 trajectories: a list of 1D array
graspposesaswellastheposesatthebeginningofeachphase
62
of a task, e.g., when the end-effector is right above an object63 Returns:
of interest. We report the number of keyposes per real-world64 key_poses: a list of integers indicates
65 the time steps when the end-effector
task in this Appendix (Section C). Lastly, for CALVIN we 66 opens/closes.
adapt the above heuristics to devise a more robust algorithm67 """
to discover keyposes. Specifically, we track end-effector state68 trajectories = np.stack(
69 [trajectories[0]] + trajectories, axis=0
changes and significant changes of motion, i.e. both velocity 70 )
and acceleration. For reference, we include our Python code71 openess = trajectories[:, -1]
for discovering keyposes in CALVIN here: 72 changed = openess[:-1] != openess[1:]
73
1 """Utility functions for computing keyposes.""" 74 key_poses = np.where(changed)[0].tolist()
2 import numpy as np 75
3 from scipy.signal import argrelextrem 76 return key_poses
4 77
5 def motion_changed(trajectories, buffer_size): 78 def keypoint_discovery(trajectories, buffer_size=5):
6 """Select keyposes where motion changes 79 """Select keyposes where motion changes
significantly. The chosen poses shall be sparse. significantly. The chosen poses shall be sparse.
7 80
8 Args: 81 Args:
9 trajectories: a list of 1D array 82 trajectories: a list of 1D array
10 buffer_size: an integer indicates the 83 buffer_size: an integer indicates the
11 minimum distance of waypoints 84 minimum distance of waypoints
12 85
13 Returns: 86 Returns:
14 key_poses: a list of integers indicates 87 key_poses: an Integer array indicates the
15 the time steps when motion of the end 88 indices of keyposes
16 effector changes significantly. 89 """
17 """ 90 motion_changed = motion_changed(
18 # compute velocity 91 trajectories, buffer_size
19 trajectories = np.stack( 92 )
20 [trajectories[0]] + trajectories, axis=0 93
21 ) 94 gripper_changed = (
22 velocities = ( 95 gripper_state_changed(trajectories)
23 trajectories[1:] - trajectories[:-1] 96 )
24 ) 97 one_frame_before_gripper_changed = [
25 98 i - 1 for i in gripper_changed if i > 1
26 # compute acceleration 99 ]
27 velocities = np.concatenate( 100
28 [velocities, [velocities[-1]]], 101 last_frame = [len(trajectories) - 1]
29 axis=0 102
30 ) 103 key_pose_inds = (
31 accelerations = velocities[1:] - velocities[:-1]104 moition_changed +
32 105 gripper_changed.tolist() +
33 # compute the magnitude of acceleration 106 one_frame_before_gripper_changed.tolist() +
34 A = np.linalg.norm( 107 last_frame
35 accelerations[:, :3], axis=-1 108 )
36 ) 109 key_pose_inds = np.unique(key_pose_inds)
37 110
38 # local maximas of acceleration indicates 111 return key_pose_inds8) Put grapes in the bowl: The scene contains three vines
of grapes of different color and one bowl. The agent
needs to pick one vine and place it in the bowl. The task
involves four keyposes.
9) Sort the rectangle: Between two rectangle cubes there
is space for one more. The task comprises detecting the
rectangle to be moved and placing it between the others.
It involves four keyposes.
10) Stack blocks with the same shape: The scene contains of
severalblacks,someofwhichhaverectangularandsome
cylindrical shape. The task is to pick the same-shape
blocks and stack them on top of the first one. It involves
eight keyposes.
11) Stack cups: The scene contains three cups of different
colors. The agent needs to successfully stack them in
any order. The task involves eight keyposes.
12) Put block in a triangle on the plate: The agent needs to
Fig. 5: Zero-shot long-horizon performance on CALVIN detect three blocks of the same color and place them
with varying number of keyposes allowed at test time. inside a plate to form an equilateral triangle. The task
involves 12 keyposes.
The above tasks examine different generalization capabili-
B. Evaluation on CALVIN with varying number of maximum ties of 3D Diffuser Actor, for example multimodality in the
keyposes allowed at test time solution space (5, 8), order of execution (10, 11, 12), precision
(3, 4, 6) and high noise/variance in keyposes (1).
We show zero-shot long-horizon performance on CALVIN
with varying number of keyposes allowed at test time in D. RLBench tasks under PerAct’s setup
Figure 5. The performance saturates with more than 300
We provide an explanation of the RLBench tasks and their
keyposes.
successconditionsunderthePerActsetupforself-completeness.
Alltasksvarytheobjectpose,appearanceandsemantics,which
C. Real-world tasks
are not described in the descriptions below. For more details,
We explain the the real-world tasks and their success please refer to the PerAct paper [70].
conditions in more detail. All tasks take place in a cluttered 1) Open a drawer: The cabinet has three drawers (top,
scene with distractors (random objects that do not participate middle and bottom). The agent is successful if the target
in the task) which are not mentioned in the descriptions below. drawer is opened. The task on average involves three
1) Close a box: The end-effector needs to move and hit the keyposes.
lidofanopenboxsothatitcloses.Theagentissuccessful 2) Slide a block to a colored zone: There is one block
if the box closes. The task involves two keyposes. and four zones with different colors (red, blue, pink,
2) Put a duck in a bowl: There are two toy ducks and two and yellow). The end-effector must push the block to
bowls. One of the ducks have to be placed in one of the the zone with the specified color. On average, the task
bowls. The task involves four keyposes. involves approximately 4.7 keyposes
3) Insert a peg vertically into the hole: The agent needs to 3) Sweep the dust into a dustpan: There are two dustpans
detect and grasp a peg, then insert it into a hole that is of different sizes (short and tall). The agent needs to
placed on the ground. The task involves four keyposes. sweep the dirt into the specified dustpan. The task on
4) Insert a peg horizontally into the torus: The agent needs average involves 4.6 keyposes.
to detect and grasp a peg, then insert it into a torus that 4) Take the meat off the grill frame: There is chicken leg or
is placed vertically to the ground. The task involves four steck.Theagentneedstotakethemeatoffthegrillframe
keyposes. and put it on the side. The task involves 5 keyposes.
5) Put a computer mouse on the pad: There two computer 5) Turn on the water tap: The water tap has two sides of
mice and one mousepad. The agent needs to pick one handle. The agent needs to rotate the specified handle
mouse and place it on the pad. The task involves four 90◦. The task involves 2 keyposes.
keyposes. 6) Put a block in the drawer: The cabinet has three drawers
6) Open the pen: The agent needs to detect a pen that is (top,middleandbottom).Thereisablockonthecabinet.
attached vertically to the table, grasp its lid and pull it The agent needs to open and put the block in the target
to open the pen. The task involves three keyposes. drawer. The task on average involves 12 keyposes.
7) Press the stapler: The agent needs to reach and press a 7) Close a jar: There are two colored jars. The jar colors
stapler. The task involves two keyposes. are sampled from a set of 20 colors. The agent needs toMulti-view RGB-D
“Stack the bottle to
the middle of the rack”
2D Visual Encoder
Noisy Noisy Language
position rotation 3D Lifting Encoder
Linear Linear Cross Attention: scene →instructions
Concatenate FPS
FiLM Proprioception 𝑐
Cross Attention: pose estimate →scene
Self Attention: pose estimate ↔scene MLP
SeSlef lAf tAttetnetniotino:n p: opsoes ee setsimtimataet e↔ ↔scsecneene
Denoising step 𝑡
MLP MLP
Position error Rotation
open/close error
Multi-view RGB-D
“Stack the bottle to
the middle of the rack”
2D Visual Encoder
Noisy Noisy Language
position rotation 3D Lifting Encoder
Linear Linear Cross Attention: scene →instructions
Concatenate FPS
Proprioception 𝑐
FiLM
Cross Attention: pose estimate →scene
MLP
Cross Attention: pose estimate, scene →instructions
Self Attention: pose estimate ↔scene
Denoising step 𝑡
Cross Attention: pose estimate, scene →instructions
SeSlef lAf tAttetnetniotino:n p: opsoes ee setsimtimataet e↔ ↔scsecneene
MLP MLP
Position error Rotation
open/close error
Fig. 6: 3D Diffuser Actor architecture in more detail. Top: Standard version: the encoded inputs are fed to attention layers
that predict the position and rotation error for each trajectory timestep. The language information is fused to the visual stream
by allowing the encoded visual feature tokens to attend to language feature tokens. There are two different attention and output
heads for position and rotation error respectively. Bottom: version with enhanced language conditioning: cross-attention layers
from visual and pose estimate tokens to language tokens are interleaved between pose estimate-visual token attention layers.pick up the lid and screw it in the jar with the specified 3D Diffuser Actor with enhanced language conditioning in
color. The task involves six keyposes. Figure 6b, which achieves SOTA results on CALVIN.
8) Drag a block with the stick: There is a block, a stick and The inputs to our network are i) a stream of RGB-D views;
four colored zones. The zone colors are sampled from ii) a language instruction; iii) proprioception in the form of
a set of 20 colors. The agent is successful if the block end-effector’s history poses; iv) the current noisy estimates of
is dragged to the specified colored zone with the stick. position and rotation; v) the denoising step t. The images are
The task involves six keyposes. encoded into visual tokens using a pretrained 2D backbone.
9) Stack blocks: There are 8 colored blocks and 1 green The depth values are used to “lift" the multi-view tokens into a
platform. Each four of the 8 blocks share the same color, 3D feature cloud. The language is encoded into feature tokens
whiledifferfromtheother.Theblockcolorsaresampled using a language backbone. The proprioception is represented
fromasetof20colors.TheagentneedstostackNblocks as learnable tokens with known 3D locations in the scene.
of the specified color on the platform. The task involves The noisy estimates are fed to linear layers that map them to
14.6 keyposes. high-dimensional vectors. The denoising step is fed to an MLP.
10) Screw a light bulb: There are 2 light bulbs, 2 holders, The visual tokens cross-attend to the language tokens and
and 1 lamp stand. The holder colors are sampled from a get residually updated. The proprioception tokens attend to
set of 20 colors. The agent needs to pick up and screw the visual tokens to contextualize with the scene information.
the light bulb in the specified holder. The task involves We subsample a number of visual tokens using Farthest
7 keyposes. Point Sampling (FPS) in order to decrease the computational
11) Put the cash in a safe: There is a stack of cash and a requirements.Thesampledvisualtokens,proprioceptiontokens
safe. The safe has three layers (top, middle and bottom). and noisy position/rotation tokens attend to each other. We
The agent needs to pick up the cast and put it in the modulate the attention using adaptive layer normalization and
specified layer of the safe. The task involves 5 keyposes. FiLM [56]. Lastly, the contextualized noisy estimates are fed
12) Placeawinebottleontherack:Thereisabottleofwine to MLP to predict the error terms as well as the end-effector’s
and a wooden rack. The rack has three slots (left, middle state (open/close).
and right). The agent needs to pick up and place the
F. Hyper-parameters for experiments
wine at the specified location of the wooden rack. The
task involves 5 keyposes. Table VI summarizes the hyper-parameters used for train-
13) Put groceries in the cardboard: There are 9 YCB objects ing/evaluating our model. On CALVIN we observed that our
and a cupboard. The agent needs to grab the specified model overfits the training data, resulting in lower test perfor-
object and place it in the cupboard. The task involves 5 mance. We use higher weight_decay and shorter total_epoch
keyposes. on CALVIN compared to RLBench.
14) Put a block in the shape sorter: There are 5 blocks of
G. The importance of noise scheduler
differentshapesandasorterwiththecorrespondingslots.
We visualize the clean/noised 6D rotation representations
The agent needs to pick up the block with the specified
as two three-dimensional unit-length vectors in Figure 7. We
shapeandinsertitintotheslotwiththesameshape.The
plot each vector as a point in the 3D space. We can observe
task involves 5 keyposes.
that noised rotation vectors generated by the squared linear
15) Push a button: There are 3 buttons, whose colors are
scheduler cover the space more completely than those by the
sampled from a set of 20 colors. The agent needs to
scaled linear scheduler.
push the colored buttons in the specified sequence. The
task involves 3.8 keyposes.
16) Insert a peg: There is 1 square, and 1 spoke platform
with three colored spoke. The spoke colors are sampled
from a set of 20 colors. The agent needs to pick up the
square and put it onto the spoke with the specified color.
The task involves 5 keyposes.
17) Stackcups:Thereare3cups.Thecupcolorsaresampled
from a set of 20 colors. The agent needs to stack all the
other cups on the specified one. The task involves 10
keyposes.
18) Hang cups on the rack: There are 3 mugs and a mug
rack.TheagentneedstopickupNmugsandplacethem
onto the rack. The task involves 11.5 keyposes.
E. Detailed Model Diagram
We present a more detailed architecture diagram of our
3D Diffuser Actor in Figure 6a. We also show a variant ofPerAct GNFactor CALVIN
Model
image_size 256 256 200
embedding_dim 120 120 192
camera_views 4 1 2
action_history_length 3 3 3
FPS : % of sampled tokens 20% 20% 33%
diffusion_timestep 100 100 25
noise_scheduler : position scaled_linear scaled_linear scaled_linear
noise_scheduler : rotation squaredcos squaredcos squaredcos
action_space absolute pose absolute pose relative displacement
Training
batch_size 240 240 1080
learning_rate 1e−4 1e−4 3e−4
weight_decay 5e−4 5e−4 5e−3
total_epochs 1.6e4 8e5 450
optimizer Adam Adam Adam
loss weight : w 30 30 30
1
loss weight : w 10 10 10
2
Evaluation
maximal # of keyposes 25 25 60
TABLE VI: Hyper-parameters of our experiments. We list the hyper-parameters used for training/evaluating our model on
RLBench and CALVIN simulated benchmarks. On RLBench we conduct experiments under two setups: PerAct and GNFactor.
(a) Clean (b) Scaled Linear (c) Square Cosine (d) Rotation Accuracy
Fig. 7: Visualization of noised rotation based on different schedulers. We split the 6 DoF rotation representations into 2
three-dimension unit-length vectors, and plot the first/second vector as a point in 3D. The noised counterparts are colorized in
magenta/cyan. We visualize the rotation of all keyposes in RLBench insert_peg task. From left to right, we visualize the (a)
clean rotation, (b) noisy rotation with a scaled-linear scheduler, and (c) that with a square cosine scheduler. Lastly, we compare
(d) the denoising performance curve of two noise schedulers. Here, accuracy is defined as the percentage of times the absolute
rotation error is lower than a threshold of 0.025. Using the square cosine scheduler helps our model to denoise from the pure
noise better.