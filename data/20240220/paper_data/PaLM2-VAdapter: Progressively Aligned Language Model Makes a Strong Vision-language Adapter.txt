PaLM2-VAdapter: Progressively Aligned Language Model
Makes a Strong Vision-language Adapter
JunfeiXiao12 ZhengXu2 AlanYuille1 ShenYan†2 BoyuWang†2
Abstract
Stronger
PaLM2-VAdapter
This paper demonstrates that a progressively Scalability Higher
Performance
aligned language model can effectively bridge
frozenvisionencodersandlargelanguagemod- FasterConvergence
els(LLMs). Whilethefundamentalarchitecture
andpre-trainingmethodsofvisionencodersand
SOTAAdapter
LLMs have been extensively studied, the archi-
tectureandtrainingstrategyofvision-language
adapters vary significantly across recent works.
Ourresearchundertakesathoroughexploration
ofthestate-of-the-artperceiverresamplerarchi-
tectureandbuildsastrongbaseline. However,we
Figure1: Faster,higher,andstronger. Ourprogressively
observethatthevision-languagealignmentwith
alignedlanguagemodeldemonstratesfasterconvergence,
perceiver resampler exhibits slow convergence
higherperformanceandstrongerscalabilityasanadapter
andlimitedscalabilitywithalackofdirectsuper-
forvision-languagealignment.
vision. Toaddressthisissue,weproposePaLM2-
VAdapter,employingaprogressivelyalignedlan- sophisticated Large Vision-Language Models (LVLMs).
guagemodelasthevision-languageadapter.Com- This is achieved by integrating robust unimodal models,
paredtothestrongbaselinewithperceiverresam- namelyvisionencodersandLLMs,therebycircumventing
pler,ourmethodempiricallyshowsfasterconver- the need to develop these models from scratch (Alayrac
gence,higherperformanceandstrongerscalabil- etal.,2022;Lietal.,2023a;Liuetal.,2023b;Chenetal.,
ity. ExtensiveexperimentsacrossvariousVisual 2023). TheseLVLMshavedemonstratedexceptionalperfor-
QuestionAnswering(VQA)andcaptioningtasks manceacrossavarietyofmulti-modalbenchmarks,show-
onbothimagesandvideosdemonstratethatour casingtheirimpressivecapabilitiesinunderstanding,rea-
modelexhibitsstate-of-the-artvisualunderstand- soning,andgeneralizingacrossdifferentcontexts(Alayrac
ingandmulti-modalreasoningcapabilities. No- etal.,2022;Lietal.,2023a;Moonetal.,2023).
tably, ourmethodachievestheseadvancements
with30∼70%fewerparametersthanthestate-of- Contrasting with traditional full-model finetuning ap-
the-artlargevision-languagemodels,markinga proaches,recentresearchhasshiftedtowardsfreezingboth
significantefficiencyimprovement. visionencoderandLLMduringLVLMtraining(Alayrac
et al., 2022; Li et al., 2023a; Moon et al., 2023). There
aretwomainreasonsforthis. Firstly,visionencodersand
1.Introduction LLMs have learned very strong feature extraction ability
andreasoningabilitythroughthelarge-scalepretrainingon
Withthenotablesuccessesoflargelanguagemodel(LLM)
high-qualitydata,andfinetuningcouldleadtocatastrophic
(Brownetal.,2020;Touvronetal.,2023;Aniletal.,2023),
forgetting. Secondly,asthesebasemodelsaregettingbig-
coupled with advancements in vision-language pretrain-
ger,freezingthemsavestrainingcosts. Therefore,thefocus
ing(Radfordetal.,2021;Jiaetal.,2021;Lietal.,2022;Yu
isontraininganadapterthatconnectsthevisionencoder
etal.,2022),researchersarenowwell-equippedtoconstruct
andtheLLMforcross-modalityalignment.
†EqualAdvising1JohnsHopkinsUniversity2GoogleResearch.
TobuildstrongLVLMsusingpre-trainedandfrozenvision
Correspondenceto: ShenYan<shenyan@google.com>,Boyu
encodersandLLMs,thekeyslieinthedesignandtraining
Wang<boyuwang@google.com>.
strategy of the adapter. Existing methods like Flamingo
Preprint. and AnyMAL (Alayrac et al., 2022; Moon et al., 2023)
1
4202
beF
61
]VC.sc[
1v69801.2042:viXraPaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
employtheperceiverresamplerastheiradapterarchitecture, etal.,2022;Douetal.,2022),image-textcontrastivelearn-
resulting an effective way for cross-modality alignment. ing(Radfordetal.,2021;Jiaetal.,2021;Yangetal.,2022b;
On the other hand, BLIP-2 (Li et al., 2023a) tackles the Duanetal.,2022),andalsoauto-regressiveimagecaption-
adapterpre-trainingissuebyintroducingQ-Former,which ing(Lietal.,2022;Yuetal.,2022;Wangetal.,2021a;b).
takesanadditionalpretrainingstagewithmulti-tasklearning
However,modelspretrainedonimage-textpairsoftenlack
onimage-textpairs. Althoughthesemethodsdemonstrate
the complex reasoning and few-shot learning abilities of
impressive performance, questions regarding the optimal
Large Language Models (LLMs), primarily due to their
architecture for the adapter and the necessity of adapter
focus on image captions (Lin et al., 2014; Radford et al.,
pretrainingstillremainopenforexploration.
2021;Jiaetal.,2021;Schuhmannetal.,2021;Srinivasan
Toaddresstheopenquestionsinthedesignandtrainingof etal.,2021). Toovercomethis,recenteffortshaveshifted
adaptersforLVLMs,weconductanin-depthstudyintothe towardsintegratingpretrainedvisionencodersandLLMs
latest cross-attention based adapter architectures, particu- intolargervision-languagemodels. Thisstrategyaimsto
larlyfocusingontheperceiverresamplerandmakeastrong extend their capabilities to more advanced tasks such as
baseline.However,weobservedthattheperceiverresampler imagecaptioningandVisualQuestionAnswering(VQA),
adapterexhibitsslowconvergenceandlimitedscalability, leveragingLLMsforimprovedperformance.
especiallywhenscalingupthevisionencoder. Toovercome
2.2.LargeLanguageModels(LLMs)
thesechallenges,weproposePaLM2-VAdapter,whichem-
ploysaprogressivealignmentstrategyforbridgingfrozen Armingwithscaled-updataandmodels,LargeLanguage
visionencodersandLLMdecoders. Specifically,theclas- Models(LLMs)havedemonstratedemergentcapabilities
sicalignmentframeworkisusedinaprogressivewaywith likezero-shotgeneralizationandin-contextlearningabil-
two stages and a tiny PaLM-2 model is trained as differ- ity. This has sparked a surge in research and develop-
ent roles (stage 1: LM decoder, stage 2: adapter). Com- ment, leading to significant advancements in models like
paredtothebaselinemodelsusingstate-of-the-artadapters, FlanT5 (Chung et al., 2022), PaLM 2 (Anil et al., 2023),
PaLM2-VAdapterdemonstratesfasterconvergence,higher GPT-4(OpenAI,2023),LLaMA(Touvronetal.,2023)and
performanceandstrongerscalability,asdetailedinFigure1. etc. Given the complex reasoning and remarkable under-
standingability,LLMsareutilizedasa”head”.Inthispaper,
Weevaluateourmodelsonvariousvision-languagebench-
weaimstobridgestrongvisionencoderswiththePaLM2
marksinbothimage-basedandvideo-basedcaptioningand
seriesofLLMs,extendingitscapabilitytounderstandand
QA tasks. Our models consistently show state-of-the-art
doreasoningwithvisualembeddings. ToavoidthePaLM
orcomparableperformance,whileonlyrequiring30∼80%
2modellosinganyknowledgeoritsstronglanguagerea-
fewer parameters than previous models. This efficiency
soningability,ourmethodkeepsthelargePaLM2model
underscores the effectiveness of our proposed PaLM2-
frozenallthetime.
VAdapterinadvancingthefieldofLVLMs.
2.3.LargeVision-languageModels(LVLMs)
Tosumup,ourcontributionslieinthreefolds:
Large Vision-language Models (LVLMs) connect vision
1. Weconductacomprehensivestudyofthestate-of-the-
and language together and extend the reasoning ability
artadapterarchitecture(i.e.,perceiverresampler)and
of LLMs to process with multi modal input. Numer-
buildastrongbaselinewithit.
ousworkshavebeenproposedinthisdirection,including
Flamingo(Alayracetal.,2022),BLIP-2(Lietal.,2023a),
2. We propose PaLM2-VAdapter, a progressive align-
InstructBLIP (Liu et al., 2023a), MiniGPT-4 (Zhu et al.,
mentstrategytotrainatinyPaLM2languagemodel
2023),LLaVA(Liuetal.,2023b)andetc. Mostworksdiffer
asthevision-languageadapter,makingsolidimprove-
basedontheiradapterdesign,trainingprocessandobjec-
mentonconvergence,performanceandscalability.
tives,trainingcorporaandinstructiontuning. Flamingois
thefirstworkinthisline,whichusestheperceiverresampler
3. Our models achieve state-of-the-art performance on
as an adapter to feed visual tokens into language models.
variousvisualcaptioningandQAbenchmarkswhile
However,thenumberoftrainableparametersinFlamingo
use30∼80%lessparametersthanothermodels.
isstillmorethanbillions,makingthealignmentwithlim-
itedefficiency. BLIP-2proposesalightweightQ-Former
2.RelatedWork
as the adapter. However, the Q-Former needs a complex
trainingprocess,includingatwo-stagetrainingwiththree
2.1.Vision-languagePre-training
trainingobjectives(vision-lanaugecontrastiveloss,match-
Vision-languagepre-training aimstolearnuniversalmul- inglossandgenerationloss). InstructBLIPandMiniGPT-4
timodalrepresentationsthroughasetofpretrainingobjec- areextensionsofBLIP-2byusinginstructiontuningdataor
tives,includingimage-textmatching(Lietal.,2021;Bao
2PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Image & Video Captions
Image & video Captions ( i )
Tiny PaLM-2
LLM Decoder
(PaLM-2)
Vision Encoder Cross-Attention
…
Describe the following: <visual tokens>
( ii ) Image & video Captions
Vision Encoder Adapter Large PaLM-2
… … Tiny PaLM-2
Images (duplicated) Learnable Queries
Vision Encoder Perceiver Resampler
…
…
Video frames
(a) Classic visual-language alignment (b) Progressive alignment with PaLM2-VAdapter
Figure2: Methodoverview. (a): Theclassicmodelframeworkforvisual-languagealignment,consistingofthreemajor
parts: avisionencoder,anadapterandaLLMdecoder. (b): OurprogressivealignmentstrategyofourPaLM2-VAdapter.
(i) A tiny PaLM2 language model (∼108M) is trained as the LM decoder in the first stage and (ii) then trained as the
vision-languageadapter(withanaddition1-layerperceiverresampler)foraligningthesamevisionencoderandalarge
PaLM2decoder.
additionalprojectionlayer. LLaVAusesasimpleprojection ment.Specifically,thevisualfeaturesextractedbythevision
layertoconvertvisionrepresentationsintothesamedimen- encoderareservedasthekeysandvalueswhicharecross-
sionasthelanguage.Inthispaper,weproposeaprogressive attentionedtoasetoflearnablequeries,showninFigure2a.
alignmentstrategytouseapre-trainedlanguagemodelas Weconductacomprehensivestudyofthestate-of-the-art
the adapter, which shows faster convergence, higher per- perceiverresamplerarchitectureandestablishaverystrong
formanceandstrongerscalabilitythanthestate-of-the-art baseline model using 6-layer perceiver resampler as the
perceiverresampler. adapter(detailedin§4.2).
3.Method Self-attentionbasedadapter.Self-attentionlayerscanalso
beintroducedinadapterstoimproverepresentationquality.
Ourstudyisbasedonaclassicvisual-languagealignment
Notably,self-attentionbasedadapterscouldusepretrained
pipelinewhichkeepsthevisualencoderandlargelanguage
languagemodelsforinitializationtogetbetterconvergence
model (LLM) frozen all the time. An adapter is inserted
andimprovetheperformance.
between the vision encoder and LLM to project the en-
coded visual embeddings to the language representation
3.2.Visual-languageAlignmentwithAdapter
space. Thissectionfirstlyprovidesapreliminaryoverview
ofvision-languageadapterarchitectures(§3.1)andthenex- AsshowninFigure2a,thevision-languagemodelhasthree
plainsthemodelframeworkofvisual-languagealignment majorparts: visionencoder,visualadapterandLLM.The
withadapter(§3.2). Lastly, wepresentourmethodusing target is to align the visual features with the LLM repre-
progressivevision-languagealignmentstrategyfortraining sentationspace. ThevisualencoderandtheLLMareboth
atinylanguagemodelasadapter(§3.3). frozenallthetime. Thissetupgreatlyreducestrainingcost
andpreservestheirstrongvisualfeatureextractionandrea-
3.1.Preliminary soningabilitywhichisduetothelarge-scalepre-training.
Specifically,thevisionencoderispre-trainedwithimage-
Existinglargevision-languagemodelsadoptvariouskinds
text pairs (Yu et al., 2022) and is used to convert images
of adapter architectures for cross-modality alignment. In
andvideoframesintoasetoffeaturetokens. Thesefeature
thispaper,wepresentanin-depthexplorationofthestate-
tokensareprojectedbyalightweightvisualadaptertobe
of-the-artcross-attentionbasedadaptersandproposetopro-
alignedtotheLLMrepresentationspace. WeadoptPaLM
gressivelyalignedself-attentionbasedlanguagemodel.
2(Aniletal.,2023)seriesmodelsastheLLMdecoderand
thetrainingtaskistogeneratecaptionsbasedonthevisual
Cross-attentionbasedadapter. Theadaptersinthisstyle
embeddedprefix.
adoptthecross-attentionmechanismforvisualfeaturealign-
3PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Query&Key(Value)LN FinalLN COCOCap. VQAv2(Val) FFN TimeEmbedding COCOCap. VQAv2(Val)
✗ ✓ 38.4 32.2
✓ ✗ 34 38.3
Shared ✗ 44.0 46.7
Separate ✗ 46.8 52.5 ✗ ✓ 33.8 45.1
Separate ✓ 36.2 37.6 ✓ ✓ 46.8 52.5
(a)LayerNormoptions. (b)Feed-forwardnetwork(FFN)&timeembedding.
QueryDim COCOCap. VQAv2(Val) HiddenDim COCOCap. VQAv2(Val) #Layers COCOCap. VQAv2(Val)
384 40.9 45.4 384 40.6 46.7 1 37.7 37.5
768 46.8 52.5 768 46.8 52.5 3 40.8 47.6
1536 38.3 45.0 1536 38.5 32.1 6 46.8 52.5
(c)Querydimension. (d)Hiddendimension. (e)Numberoflayers.
Table1:In-depthanalysiswithkeycomponentsofperceiverresampler. ResultsonCOCOcaptioningbenchmark(CIDEr
score)andVQAv2validationset(accuracy)arereported. ModelsaretrainedonWebLI(image-textpaireddataset).
3.3.ProgressiveVisual-languageAlignment pretrainedmodelsastheLLM.Perceiverresampler(Alayrac
etal.,2022)isusedasthebaselineadapterarchitecture,with
As language models emerge strong representation ability
256learnablequeries. Ourproposedadapterconsistsofa
throughthegenerativepre-trainingtaskandusuallyshows
1-layer perceiver resampler and a tiny transformer-based
greatscalability,weproposetointroduceatinyPaLM2lan-
languagemodel(∼110M).
guagemodel,usingaprogressivevision-languagealignment
strategytomakestrongvision-languageadapters. Specifi- Data. Ourmodelsaretrainedonimage-textpaireddataof
cally,ourmethodusesatinyPaLM2languagemodel(TLM) WebLI(Chenetal.,2023)datasetandvideo-textpaireddata
astheadapterandtrainsitinaprogressiveway,whichcon- of VTP (Alayrac et al., 2022) and SMIT (Monfort et al.,
sistsoftwostages: 2021) datasets. The ablations with cross-attention based
adaptersaresolelytrainedonWebLI.
Stage1-TLMtrainedasthedecoder: Inthefirststage,
the language model starts from a pretrained tiny PaLM2 Training. Theimagesandvideosareduplicatedorsampled
model (∼108M) and is finetuned with the classic vision- to 8 frames (Yan et al., 2022) as the visual inputs. The
languagealignmenttask(showninFigure2b(i)). base learning rate is 5e-4 and is scheduled with a warm-
up and linear decay. The training batch size is 2048. By
Stage2-TLMtrainedastheadapter:Inthesecondstage,
default,experimentsaretrainedwith250Ksteps. Weuse
giventhispre-alignedtinyPaLM2model,anadditional1-
a prompt template of ”Describe the following: <visual
layerperceiverresamplerisaddedbeforethealignedtiny
tokens>”fortraining. Fordetailedinformation,pleaserefer
PaLM2modeltobridgethesamevisionencoderandalarger
toAppendixA.
PaLM2model(showninFigure2b(ii)).
Evaluation. Alltheinputresolutionisthesameastrain-
Compared to our strongest model with state-of-the-art
ing (i.e., 288) with a patch size of 18. We evaluate our
adapter (i.e., perceiver resampler), our method is proven
methodoncaptioningtasksandVisualQuestionAnswer-
tohavefasterconvergence,higherperformanceandstronger
ing(VQA)tasksforbothimagesandvideos. Specifically,
scalability (detailed in §4.3). In addition to the effective
COCO (Chen et al., 2015), VQAv2 (Goyal et al., 2017),
architecture, theproposedprogressivealignmentstrategy
TextVQA(Singhetal.,2019),VizWiz(Bighametal.,2010),
greatlyadvancePaLM2-VAdapter,makingremarkableim-
OKVQA(Marinoetal.,2019)areusedforimage-basedeval-
provementsforvision-languagealignment(detailedin§4.4).
uation. MSRVTT(Xuetal.,2016),VATEX(Wangetal.,
Notably,theadditionalperceiverresamplerisverycrucial
2019),MSVD-QA(Xuetal.,2017),andiVQA(Yangetal.,
forefficientcross-modalityfusionbasedonourempirical
2021)areusedforvideo-basedevaluation. Weusedifferent
observation(detailedin§4.5).
prompts for the LLM decoder on different tasks. For de-
tailedpromptsinformation,pleaserefertoAppendixA&B.
4.Experiments
4.2.AStrongBaselinewithPerceiverResampler
4.1.ImplementationDetails
To figure out the effectiveness of different model compo-
Model. WeadoptCoCa(Yuetal.,2022)pretrainedViTs
nentsofcross-attentionbasedadapters,weconductacom-
asourvisionencoders. Theinputresolutionis288andthe
prehensive ablation study based on perceiver resampler,
patchsizeis18x18. WeadoptPaLM2(Aniletal.,2023)
4PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Converg. COCO MSRVTT Cross-attention COCO VQAv2
Method VisionEnc. #Layers
Steps(K) CIDEr CIDEr ModuleType CIDEr Accuracy
PerceiverRes. ViT-B 250 81.4 38.2 AttentionalPooler 1 81.1 53.5
PaLM2-VAdapter ViT-B 60(-76%)83.0(+1.6)42.1(+3.9)
PerceiverResampler 1 85.6 55.1
PerceiverRes. ViT-L 250 82.4 38.2
PerceiverResampler 6 70.3 49.7
PaLM2-VAdapter ViT-L 60(-76%)89.6(+7.2)42.7(+4.5)
Table4: Comparisionofusingdifferenttypesofcross-
Table2: Faster,higherandstronger. Comparedtotheper- attention modules. A lightweight perceiver resampler
ceiverresamplerbaseline,PaLM2-VAdaptershowsfaster cross-attention module is the best cross-modality fusion
convergence,higherperformanceandstrongerscalability. choiceforPaLM2-VAdapter.
PaLM2-1BisusedastheLLMdecoderfortheexperiments.
#Total #TrainableCOCO
Method
LanguageOnly Vision-language COCO VQAv2 Params Params CIDEr
(PaLM2pretraining) (Stage1) CIDEr Accuracy CM3Leon(Yuetal.,2023) 7B 7B 61.6
✗ ✗ 79.2 50.8 Flamingo-3B(Alayracetal.,2022) 3.2B 1.2B 73.0
✓ ✗ 81.3 52.1 Flamingo-9B(Alayracetal.,2022) 9.3B 1.6B 79.4
✓ ✓ 83.0 53.8 Flamingo-80B(Alayracetal.,2022) 80B 10.2B 84.3
Table3: Comparisonofdifferentadapterpre-training IDEFICS-9B(Laurenc¸onetal.,2023) 9B 1.5B 46.0
settings. Both language-only generative pre-training IDEFICS-80B(Laurenc¸onetal.,2023) 80B 14B 91.8
(PaLM2)andvision-languagegenerativepre-training(stage- AnyMAL-15B(Moonetal.,2023) 15B 100M∗ 99.5
1,languagemodelasdecoder)canimprovethefinalaligned PaLM2-VAdapter1B(ViT-B) 1.8B 120M 83.0
largevision-languagemodel’sperformance. PaLM2-VAdapter1B(ViT-L) 2.0B 120M 89.6
PaLM2-VAdapter1B(ViT-g) 2.8B 130M 97.5
whichisthestate-of-the-artadapterarchitecture. Asshown PaLM2-VAdapter8B(ViT-g) 10.8B 130M 95.2
inTable1,ourstudycoversdifferentchoicestoapplyLay- Table 5: Zero-shot Image Captioning. The best result
erNorm, importantmodules(i.e., Feed-ForwardNetwork is bolded and the second-best result is underlined. Com-
FFNandtimeembedding),dimensionofqueriesandcross- paredtopreviousstate-of-the-artvision-languagemodels,
attentionlayersandalsothenumberofperceiverresampler ourmodeldemonstratescomparablezero-shotvisualunder-
layers. standingability. *: Estimatedbygiveninformation.
Basedontheempiricalresults,wegetseveraldesignrules
higherperformancethanthebaselineperceiverresampler
forperceiverresamplerbasedadapter: 1)LayerNormsare
models(ViT-B:83.0vs. 81.4,ViT-L:89.6vs. 82.4)when
importantandshouldbeseparatelyappliedtothequeries
aligningthesamevisionencoderandLLMdecoderpairs.
andthecross-modalityinputs(askeysandvalues). 2)Feed-
Forward Network (FFN) and time embedding make the Strongerscalability. Perceiverresamplershowsmarginal
adapter training stable and effective and can greatly im- improvementwhenthevisionencoderisscaledfromViT-
provetheperformance. 3)Thedimensionofthelearnable BtoViT-L.However,ourPaLM2-VAdaptermakesmuch
queriesandthecross-attentionlayershouldbesetmoder- largerimprovement(COCO:6.6vs1.0,MSRVTT:0.6vs
ate. Followingthisrules, webuildaverystrongbaseline 0.0),showingstrongerscalability.
achieving81.4CIDEronCOCOcaptioning,38.2CIDEron
4.4.ProgressiveTrainingDoesHelp
MSRVTTcaptioningand53.1accuracyonVQAv2.
Weconductacomparisonregardingdifferentpre-training
4.3.Faster,Higher,andStronger strategiesusingthesameadapterarchitecture(1-layerper-
ceiverresampler+PaLM2-108M),detailedinTable3. The
Althoughthebaselineshowsreasonableperformance,we
ablationcomparesthreetrainingstrategiesfortheadapter:
observethatithaslimitedscalabilityandslowconvergence
a) randomly initialized; b) Generative pre-trained on lan-
(showninFigure1). Toaddresstheseissues,weproposeto
guagedata(PaLM2pretraining),initializedfromaPaLM2
introduceatinylanguagemodelasanadapterandtrainit
checkpoint; c) Pretrained with the proposed progressive
progressively(showninFigure2b). Comparedtothestrong
trainingstrategy. ThetinyPaLM2modelisfirstinitialized
baseline based on state-of-the-art architecture (shown in
fromthePaLM2checkpointandthenfine-tunedwithvision-
Table2),ourproposedPaLM2-VAdaptershows:
languagegenerativepre-training(stage1,thetinyPaLM2
Fasterconvergence. Whiletheperceiverresamplerbase- modelistrainedastheLMdecoder). Theresultsprovethe
lines take 250K steps to converge, our PaLM2-VAdapter effectivenessoftheprogressivetrainingstrategyappliedto
onlyneed60Kstepstoconvergewhichis∼3×faster. theadapterincludinglanguage-onlygenerativepre-training
( (Anil et al., 2023)) and vision-language generative pre-
Higher performance. PaLM2-VAdapter achieves much
training(stage1,showninFigure2b(i)).
5PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Caption: Caption: Caption: Caption:
A cow is licking ametal A motorcycle is parked in Cartoon characters are A video of a Minecraft game
pipe front of a house hanging from a rope where a person is building a car.
Caption: Caption: Caption: Caption:
A dog is sleeping on a pillow A surfer is riding a wave in A band is playing on stage A person is drawing a picture
with a teddy bear the ocean in front of a large crowd of a cartoon character
Figure 3: Qualitative examples of Visual Captioning. Left: Image captioning on the COCO dataset. Right: Video
captioningontheMSRVTTdataset. PaLM2-VAdapterdemonstratesstrongvisualunderstandingability.
#Total#TrainableMSRVTTVATEX leadtobetterperformancewithouradapterdesignwhichis
Method
Params Params CIDEr CIDEr contrarytotheobservationwithvanillaperceiverresampler
VideoCoCa(Yanetal.,2022) 2.1B 2.1B 27.1 22.8 adaper. Theempiricalresultsshowthata1-layerperceiver
DeCap(Lietal.,2023b) 140M 50M 34.8 18.7
resampler seems to be the best choice for cross-modality
Flam.-3B(Alayracetal.,2022) 3.2B 1.2B - 40.1
fusioninourproposedPaLM2-VAdapter.
Flam.-9B(Alayracetal.,2022) 9.3B 1.6B - 39.5
Flam.-80B(Alayracetal.,2022) 80B 14B - 46.7 4.6.VisualCaptioning
PaLM2-VAdapter1B(ViT-B) 1.8B 120M 42.1 38.3
Imagecaptioning
PaLM2-VAdapter1B(ViT-L) 2.0B 120M 42.7 45.5
PaLM2-VAdapter1B(ViT-g) 2.8B 130M 45.6 51.2 As detailed in Table 5, we evaluate the zero-shot image
PaLM2-VAdapter8B(ViT-g) 10.8B 130M 47.7 53.0
captioningperformanceontheCOCOdataset(Chenetal.,
Table 6: Zero-shot Video Captioning. The best result 2015). Comparedtothestate-of-the-artAnyMALmodel,
is bolded and the second-best result is underlined. Our ourmethodshowscomparableimagecaptioningcapability,
modeldemonstratesthestate-of-the-artzero-shotvisualun- butonlyrequires70%parameters(10.8Bvs. 15B),prov-
derstandingabilityonvideos. ingtheeffectivenessofourprogressivealignmentstrategy.
Additionally,thescalabilityofourPaLM2-VAdapterisevi-
dencedthroughthevisionencoderscalingexperiment(from
4.5.PerceiverResamplerisStillNeeded
ViT-B to ViT-g), indicating that a more powerful vision
Inourfirstvision-languagealignmentstage(showninFig- encodercorrelateswithenhancedimagecaptioningperfor-
ure2b(i)),wefollowCoCa(Yuetal.,2022)touseanatten- mance. QualitativeexamplesareprovidedinFigure3and
tionalpoolerasthecross-attentionmodule. Thisattentional AppendixC.
poolerconsistsofasimplecross-attentionlayerandaLayer-
Videocaptioning
Normlayerforthefinalqueriedfeatures. Basedonourob-
servationofourin-depthempiricalstudywiththeperceiver As detailed in Table 6, we evaluate the zero-shot video
resamplerarchitecture(detailedinSection4.2),wereplace captioning performance on the MSRVTT and VATEX
theattentionalpoolerwitha1-layerperceiverresamplerto datasets (Xu et al., 2016; Wang et al., 2019). Compared
improvecross-modalalignmentandachievebetterperfor- tothestate-of-the-artFlamingomodels,ourmethodmakes
mance,showninTable4. Ontheotherhand,weobserve solidimprovementontheVATEXbenchmarkbutonlyre-
that adding more layers of perceiver resampler does not
6PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
#Total #Trainable VQAv2 TextVQA VizWiz OKVQA
Method
Params Params Accuracy Accuracy Accuracy Accuracy
Flamingo-3B(Alayracetal.,2022) 3.2B 1.2B 49.2 30.1 28.9 41.2
Flamingo-9B(Alayracetal.,2022) 9.3B 1.6B 51.8 31.8 28.8 44.7
Flamingo-80B(Alayracetal.,2022) 80B 10.2B 56.3 35.0 31.6 50.6
BLIP-2(FlanT5xxL)(Lietal.,2023a) 12.1B 108M 65.0† 44.1∗ 29.4 45.9
InstructBLIP(V-13B)(Liuetal.,2023a) 14.1B 108M - 50.7†∗ 33.4 -
IBELICS-9B(Laurenc¸onetal.,2023) 9B 1.5B 50.9 25.9 35.5 38.4
IBELICS-80B(Laurenc¸onetal.,2023) 80B 14B 60.0 30.9 36.0 45.2
AnyMAL13B(ViT-G)(Moonetal.,2023) 15B 100M 59.6 24.7 24.4 33.1
PaLM2-VAdapter1B(ViT-B) 1.8B 120M 53.8 18.7 28.6 31.0
PaLM2-VAdapter1B(ViT-L) 2.0B 120M 55.0 22.2 37.2 31.7
PaLM2-VAdapter1B(ViT-g) 2.8B 130M 57.9 23.7 44.1 33.6
PaLM2-VAdapter8B(ViT-g) 10.8B 130M 60.6 24.8 43.7 40.9
Table7: Zero-shotImageQuestionAnswering. Thebestresultisboldedandthesecond-bestresultisunderlined. Our
modeldemonstratesstrongzero-shotvision-languagereasoningabilityonthefourclassicbenchmarks,comparabletothe
state-of-the-artmethods. *: withadditionalOCRinputs. †: in-domainimageswereused.
#Total #Trainable MSRVTT-QA MSVD-QA iVQA
Method
Params Params (Top-1Acc.) (Top-1Acc.) (iVQAAcc.)
JustAsk(Yangetal.,2021) 600M 600M 5.6 13.5 13.3
HiTeA(Yeetal.,2023) 297M 297M 8.6 18.2 -
FrozenBiLM(Yangetal.,2022a) 890M 30M 16.9 33.7 26.2
Flamingo-3B(Alayracetal.,2022) 3.2B 1.2B 11.0 27.5 32.7
Flamingo-9B(Alayracetal.,2022) 9.3B 1.6B 13.7 30.2 35.2
Flamingo-80B(Alayracetal.,2022) 80B 14B 17.4 35.6 40.7
PaLM2-VAdapter1B(ViT-B) 1.8B 120M 12.7 26.2 25.8
PaLM2-VAdapter1B(ViT-L) 2.0B 120M 14.0 18.6 28.3
PaLM2-VAdapter1B(ViT-g) 2.8B 130M 15.9 27.7 26.1
PaLM2-VAdapter8B(ViT-g) 10.8B 130M 19.6 40.5 36.7
Table8: Zero-shotVideoQuestionAnswering. Thebestresultisboldedandthesecond-bestresultisunderlined. Our
modeldemonstratesthestate-of-the-artzero-shotmutli-modalreasoningabilityonvideos.
quires14%parameters(10.8Bvs. 80B).Similartoimage Figure4andAppendixC.
captioning,PaLM2-VAdapterstillshowsstrongscalability
Videoquestionanswering
when the vision encoder is scaled up. Moreover, scaling
up language model also improves video captioning per- AsdetailedinTable8,weevaluatethezero-shotvideoques-
formance,indicatingthatalargerlanguagemodelleadto tionansweringperformanceontheMSRVTT-QA,MSVD-
strongerabilitytounderstandsequentialvisualinformation QAandiVQAdatasets(Xuetal.,2016;2017;Yangetal.,
ofvideos. QualitativeexamplesareprovidedinFigure3 2021). Comparedtothestate-of-the-artFlamingomodels,
andAppendixC. ourmethodshowsstate-of-the-artvideoquestionanswer-
ing ability but only requires 14% parameters (10.8B vs.
4.7.VisualQuestionAnswering 80B),provingtheremarkableeffectivenessofourmethod.
The results also justify the strong scalability of PaLM2-
Imagequestionanswering
VAdapter. Qualitative examples are provided in Figure 4
As detailed in Table 7, we evaluate the zero-shot image andAppendixC.
questionansweringperformanceontheVQAv2,TextVQA,
5.Limitation&Discussion
VizWiz,andOKVQAdatasets(Goyaletal.,2017;Singh
etal.,2019;Bighametal.,2010;Marinoetal.,2019). Com- OurPaLM2-VAdaptermakesasignificantimprovementin
paredtothestate-of-the-artIBELICSmodels,ourmethod efficiency, operating with substantially fewer parameters
shows comparable image question answering ability but and much less training cost. However, its alignment pro-
onlyrequires14%parameters(10.8Bvs. 80B),provingthe cessencounterschallengesastheLLMdecoderscales,just
effectivenessofourprogressivealignmentstrategy. PaLM2- like other large vision-language models. The key of this
VAdaptershowsverystrongscalability-alwaysachieving challenge lies in ensuring visual embeddings seamlessly
betterperformancewhenthevisionencoderandLLMde- transition into the scaled-up LLMs’ input representation
coderarescaledup. Qualitativeexamplesareprovidedin space. Apotentialsolutioninvolvesthedirectquantization
7PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Q: Are there ducks on the Q: How many zebras are Q: What did someone cut a Q: What is taking its clothes
shoreline? standing up? rack of? off?
A: Yes A: 3 A: Ribs A: Monkey
Q: Is this a vegetarian pizza? Q: Is the cat watching TV? Q: What is the man doing? Q: Who makes a goal?
A: No A: Yes A: Riding a motorcycle A: Soccer player
Figure4: QualitativeexamplesofVisualQuestionAnswering. Left: ImagequestionansweringontheVQAv2dataset.
Right: videoquestionansweringontheMSVD-QAdataset.
COCO baselinewhosevisualembeddingsisnotquantized,thereis
Setting SoftmaxTemp. Temp.Decay
CIDEr ahugeperformancedropwhenthevisualembeddingsare
Baseline - - 44.1 quantizedtothewordsofLLMcodebook.
Gumbel-Softmax 1.0 - 0
Gumbel-Softmax 2.0 - 13.1 This implies that the visual embeddings might share the
Gumbel-Softmax 2.0 Exponential∗ 15.3 samerepresentationspacewiththeLLMcodebookbutcan-
not be “translated” to words with simple matching. We
Table9: Quantizethevisualembeddingstowords. The
baselineisonlyalignedwithimage-textpairs(WebLI).∗: believethisisaninterestingdirectionforfutureexploration:
maketheencoderandadapterhavezero-shotscalabilityto
thegumbel-softmaxtemperatureisexponentialdecayed.
largerLLMs.
ofvisualembeddingsintolanguagetokens,leveragingthe 6.Conclusion
sharedLLMcodebookacrossmodelsofvaryingsizesfor
zero-shottransferability. So,herecomesthequestion: Inthispaper,weproposePaLM2-VAdapter,whichusesa
tinylanguagemodelwithprogressivetrainingstrategyto
Canthevisualembeddingsbe“translated”towords? effectivelyalignvisionencodersandlargelanguagemod-
els. Demonstrating exceptional zero-shot generalization
To answer this question, we conduct a study to see if the
capabilitiesacrossdiversevision-languagetasks,PaLM2-
visualembeddingsoutputbytheadaptercaneasilybe“trans-
VAdaptermarksasignificantstrideinefficiency,operating
lated”toasequenceofwordsandthenusedastheprefix
withsubstantiallyfewerparametersthanexistingmodels.
for the LLM decoder. Specifically, we introduce a fully-
connected layer (FC layer) after the adapter and use the Ourcontributionsextendbeyondmeretechnicalenhance-
gumel-softmaxoperation(Jangetal.,2017)toquantizethe ments in Large Vision-Language Models (LVLMs). We
visual embeddings. The output logits from the FC layer establish a simple but effective framework for future re-
correspondtothewordsoftheLLMcodebookandtheword search in vision-language alignment, fostering advance-
with highest logit will be assigned to the corresponding mentsinmulti-modalintegration. Morevover,thePaLM2-
visual token. As shown in Table 9, the gumbel-softmax VAdapter’s success in combining vision and language
operationisveryhardtotrain. Weexplorealotofhyper- modalitypavesthewayforfurtherexplorations,potentially
parameterstomakethetrainingstable, however, thebest revolutionizing various applications incorporating more
resultwegotisjust15.3CIDErscoreontheCOCOcaption- modalities(e.g.,audio,pose,...). Ourfindingshighlightthe
ingdataset(showninthelastline),withthesoftmaxtemper- criticalroleandvastpotentialofadaptertrainingstrategyin
aturesetto2.0andexponentiallydecayed. Comparedtothe therapidlyevolvingdomainofmulti-modalalignment.
8PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
ImpactStatement Data collection and evaluation server. arXiv preprint
arXiv:1504.00325,2015.
This work presents a method to build vision language
adapters effectively and efficiently. It fits in the broader Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A.,
contextoflargevisionlanguagemodelsandsharemanyof Padlewski, P., Salz, D., Goodman, S., Grycner, A.,
thebenefitsandissuesofsuchmodels. Theadvancements Mustafa, B., Beyer, L., Kolesnikov, A., Puigcerver, J.,
invisionlanguagemodelsenablemanyusefulapplications Ding, N., Rong, K., Akbari, H., Mishra, G., Xue, L.,
acrossvariousfields. However,itiscrucialtoacknowledge Thapliyal,A.V.,Bradbury,J.,Kuo,W.,Seyedhosseini,
potentialbiasesandethicalimplicationsinthemodels,es- M.,Jia,C.,Ayan,B.K.,Ruiz,C.R.,Steiner,A.P.,An-
peciallybecausethemodelsutilizespre-trainedcheckpoints gelova,A.,Zhai,X.,Houlsby,N.,andSoricut,R. PaLI:
anddatasetsandthusinheritssuchissues. Researchdirec- Ajointly-scaledmultilinguallanguage-imagemodel. In
tionsincludingmitigatingbiasesintrainingdata,improving ICLR,2023.
algorithmicfairnessandprivacy-preservingtechniquesare
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
becoming extremely vital to explore in order to address
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
theseharmfulissuesandbenefitthebroadercommunity.
S.,etal. Scalinginstruction-finetunedlanguagemodels.
arXivpreprintarXiv:2210.11416,2022.
Acknowledgement
Dou,Z.-Y.,Kamath,A.,Gan,Z.,Zhang,P.,Wang,J.,Li,L.,
WethankChenWei,SiyuanQiaoandZhishuaiZhangfor
Liu,Z.,Liu,C.,LeCun,Y.,Peng,N.,etal. Coarse-to-fine
valuablediscussionandsupport.
vision-languagepre-trainingwithfusioninthebackbone.
NeurIPS,2022.
References
Duan,J.,Chen,L.,Tran,S.,Yang,J.,Xu,Y.,Zeng,B.,and
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Chilimbi,T. Multi-modalalignmentusingrepresentation
Hasson,Y.,Lenc,K.,Mensch,A.,Millican,K.,Reynolds, codebook. InCVPR,2022.
M.,etal.Flamingo:avisuallanguagemodelforfew-shot
learning. InNeurIPS,2022. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and
Parikh, D. Making the V in VQA Matter: Elevating
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, theRoleofImageUnderstandinginVisualQuestionAn-
D.,Passos,A.,Shakeri,S.,Taropa,E.,Bailey,P.,Chen, swering. InCVPR,2017.
Z., et al. PaLM 2 technical report. arXiv preprint
arXiv:2305.10403,2023. Jang,E.,Gu,S.,andPoole,B. Categoricalreparameteriza-
tionwithgumbel-softmax. InICLR,2017.
Bao,H.,Wang,W.,Dong,L.,Liu,Q.,Mohammed,O.K.,
Jia,C.,Yang,Y.,Xia,Y.,Chen,Y.-T.,Parekh,Z.,Pham,H.,
Aggarwal, K., Som, S., Piao, S., and Wei, F. Vlmo:
Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up
Unified vision-language pre-training with mixture-of-
visualandvision-languagerepresentationlearningwith
modality-experts. NeurIPS,2022.
noisytextsupervision. InICML,2021.
Bigham,J.P.,Jayant,C.,Ji,H.,Little,G.,Miller,A.,Miller,
Laurenc¸on, H., Saulnier, L., Tronchon, L., Bekman, S.,
R.C., Miller, R., Tatarowicz, A., White, B., White, S.,
Singh,A.,Lozhkov,A.,Wang,T.,Karamcheti,S.,Rush,
etal.Vizwiz:nearlyreal-timeanswerstovisualquestions.
A.M.,Kiela,D.,etal. OBELISC:Anopenweb-scalefil-
InProceedingsofthe23ndannualACMsymposiumon
tereddatasetofinterleavedimage-textdocuments. arXiv
User interface software and technology, pp. 333–342,
preprintarXiv:2306.16527,2023.
2010.
Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C.,
Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
and Hoi, S. C. H. Align before fuse: Vision and lan-
J.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,G.,
guage representation learning with momentum distilla-
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
tion. NeurIPS,2021.
Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.M.,Wu,
J.,Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M., Li,J.,Li,D.,Xiong,C.,andHoi,S. BLIP:Bootstrapping
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, language-imagepre-trainingforunifiedvision-language
S.,Radford,A.,Sutskever,I.,andAmodei,D. Language understandingandgeneration. InICML,2022.
modelsarefew-shotlearners. InNeurIPS,2020.
Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: boot-
Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., strappinglanguage-imagepre-trainingwithfrozenimage
Dolla´r,P.,andZitnick,C.L. MicrosoftCOCOcaptions: encodersandlargelanguagemodels. InICML,2023a.
9PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Li, W., Zhu, L., Wen, L., and Yang, Y. Decap: De- ResearchandDevelopmentinInformationRetrieval,pp.
coding CLIP latents for zero-shot captioning via text- 2443–2449,2021.
only training. In The Eleventh International Confer-
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
enceonLearningRepresentations,2023b. URLhttps:
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
//openreview.net/forum?id=Lt8bMlhiwx2.
Bhosale,S.,etal. Llama2: Openfoundationandfine-
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., tuned chat models. arXiv preprint arXiv:2307.09288,
Ramanan, D., Dolla´r, P., and Zitnick, C. L. Microsoft 2023.
COCO:Commonobjectsincontext. InECCV,2014.
Wang, J., Hu, X., Gan, Z., Yang, Z., Dai, X., Liu, Z.,
Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction Lu, Y., and Wang, L. Ufo: A unified transformer for
tuning. arXivpreprintarXiv:2304.08485,2023a. vision-languagerepresentationlearning. arXivpreprint
arXiv:2111.10023,2021a.
Liu, H., Li, C., Wu, Q., andLee, Y.J. Visualinstruction
tuning. arXivpreprintarXiv:2304.08485,2023b. Wang,X.,Wu,J.,Chen,J.,Li,L.,Wang,Y.-F.,andWang,
W. Y. Vatex: A large-scale, high-quality multilingual
Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.
datasetforvideo-and-languageresearch. InProceedings
OK-VQA:AVisualQuestionAnsweringBenchmarkRe-
oftheIEEE/CVFInternationalConferenceonComputer
quiringExternalKnowledge. InCVPR,2019.
Vision,pp.4581–4591,2019.
Monfort,M.,Jin,S.,Liu,A.,Harwath,D.,Feris,R.,Glass,
Wang,Z.,Yu,J.,Yu,A.W.,Dai,Z.,Tsvetkov,Y.,andCao,
J.,andOliva,A. Spokenmoments: Learningjointaudio-
Y. Simvlm: Simplevisuallanguagemodelpretraining
visualrepresentationsfromvideodescriptions. InPro-
withweaksupervision. ICLR,2021b.
ceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition(CVPR),pp.14871–14881, Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X.,
June2021. andZhuang,Y. Videoquestionansweringviagradually
refinedattentionoverappearanceandmotion. InACM
Moon,S.,Madotto,A.,Lin,Z.,Nagarajan,T.,Smith,M.,
Multimedia,2017.
Jain,S.,Yeh,C.-F.,Murugesan,P.,Heidari,P.,Liu,Y.,
Srinet,K.,Damavandi,B.,andKumar,A. AnyMAL:An Xu,J.,Mei,T.,Yao,T.,andRui,Y. Msr-vtt: Alargevideo
efficientandscalableany-modalityaugmentedlanguage descriptiondatasetforbridgingvideoandlanguage. In
model. arXivpreprintarXiv:2309.16058,2023. ProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pp.5288–5296,2016.
OpenAI. GPT-4 Technical Report. arXiv preprint
arXiv:2303.08774,2023. Yan, S., Zhu, T., Wang, Z., Cao, Y., Zhang, M., Ghosh,
S., Wu, Y., and Yu, J. Video-text modeling with zero-
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
shottransferfromcontrastivecaptioners. arXivpreprint
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
arXiv:2212.04979,2022.
J.,Krueger,G.,andSutskever,I. Learningtransferable
visual models from natural language supervision. In Yang, A., Miech, A., Sivic, J., Laptev, I., andSchmid, C.
ICML,2021. Just ask: Learning to answer questions from millions
of narrated videos. In Proceedings of the IEEE/CVF
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,
internationalconferenceoncomputervision,pp.1686–
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and
1697,2021.
Komatsuzaki, A. Laion-400m: Open dataset of clip-
filtered 400 million image-text pairs. arXiv preprint Yang, A., Miech, A., Sivic, J., Laptev, I., andSchmid, C.
arXiv:2111.02114,2021. Zero-shotvideoquestionansweringviafrozenbidirec-
tionallanguagemodels. AdvancesinNeuralInformation
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,
ProcessingSystems,35:124–141,2022a.
Batra, D., Parikh, D., and Rohrbach, M. Towards vqa
modelsthatcanread. InProceedingsoftheIEEE/CVF Yang,J.,Duan,J.,Tran,S.,Xu,Y.,Chanda,S.,Chen,L.,
conferenceoncomputervisionandpatternrecognition, Zeng,B.,Chilimbi,T.,andHuang,J. Vision-language
pp.8317–8326,2019. pre-training with triple contrastive learning. In CVPR,
2022b.
Srinivasan, K., Raman, K., Chen, J., Bendersky, M., and
Najork,M. Wit: Wikipedia-basedimagetextdatasetfor Ye,Q.,Xu,G.,Yan,M.,Xu,H.,Qian,Q.,Zhang,J.,and
multimodalmultilingualmachinelearning. InProceed- Huang, F. Hitea: Hierarchical temporal-aware video-
ingsofthe44thInternationalACMSIGIRConferenceon languagepre-training. InProceedingsoftheIEEE/CVF
10PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
InternationalConferenceonComputerVision,pp.15405–
15416,2023.
Yu,J.,Wang,Z.,Vasudevan,V.,Yeung,L.,Seyedhosseini,
M.,andWu,Y. Coca: Contrastivecaptionersareimage-
textfoundationmodels. TMLR,2022.
Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O.,
Wang,T.,Babu,A.,Tang,B.,Karrer,B.,Sheynin,S.,etal.
Scalingautoregressivemulti-modalmodels: Pretraining
andinstructiontuning. arXivpreprintarXiv:2309.02591,
2023.
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
Minigpt-4: Enhancing vision-language understanding
with advanced large language models. arXiv preprint
arXiv:2304.10592,2023.
11PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
Appendix
A.ImplementationDetails.
Hyperparameter Setting
Warmupsteps 1000
Learningrate 5e-4
Batchsize 2048
AdamWβ (0.9,0.999)
Weightdecay 0.0001
Imageresolution 288
Patchsize 18
Prompt “Describethefollowing:<visualtokens>:”
LLMdecodemode Greedy
Table10: Trainingsettingsforvision-languagealignment.
Benchmark TaskType PromptTemplate
COCO
MSRVTT Image&VideoCaptioning Describethefollowing<visualtokens>:
VATEX
VQAv2
Answerthequestiongiventheimages.
TextVQA Given<visualtokens>.
ImageQuestionAnswering
Question:<question>?
Vizwiz
Answer:
OKVQA
MSRVTT-QA Answerthequestiongiventheimages.
Given<visualtokens>.
MSVD-QA VideoQuestionAnswering
Question:<question>?
iVQA Answerinexactlyoneword:
Table11: PrompttemplatesusedfortheVisualCaptioningandQuestionAnsweringbenchmarks.
B.Zero-ShotGeneralizationtoVQAtasks
FollowingFlamingo(Alayracetal.,2022),weuseseveralpseudosamplesfromthedownstreamVQA
tasksasthepromptprefixcontext(4examplesareusedinourexperiments). Allthepseudoexamples
inthepromptarerandomlyselectedfromthetrainingsetandjustincludequestionsandanswers
(withoutimage). Anexamplezero-shotVQApromptwithtwopseudosamplesis:
Answer the question given the images.
Given
Question: Is the river clear?
Answer: yes
Given
Question: Is this arctic or desert?
Answer: desert
Given <visual embeddings>
Image question: Where is he cooking? Answer:
12PaLM2-VAdapter:ProgressivelyAlignedLanguageModelMakesaStrongVision-languageAdapter
C.AdditionalQualitativeExamples
Caption: Caption: Caption: Caption:
A baseball game is being Two women are walking A video of a man speaking in a A person is cutting an onion on a
played on a field down the street in the rain crowd cutting board
Caption: Caption: Caption: Caption:
A baby elephant is walking A time lapse of a city at night A man is playing the violin A man is sitting in a car and
through the forest heis talking about the car
Q: How many wheels do you Q: What is sitting atop the Q: What does a person jump Q: What is a woman coating
see? chairs? off? a pork chop in?
A: 4 A: Dog A: Cliff A: Flour
Q: What color is the bird? Q: What color is the train Q: What is a woman holding? Q: What is the woman
A: Yellow engine? A: Cat pencilling on?
A: Red A: Eyebrow
Figure5: Additionalqualitativeexamples. Topleft: ImagecaptioningontheCOCOdataset. Topright: Videocaptioning
ontheMSRVTTdataset. Bottomleft: ImagequestionansweringontheVQAv2dataset. Bottomright: Videoquestion
answeringontheMSVD-QAdataset.
13