JournalofMachineLearningResearch24(2023) 1-43 Submitted10/22;Revised11/23; Published12/23
Double Duality: Variational Primal-Dual Policy
Optimization for Constrained Reinforcement Learning
Zihao Li zihaoli@princeton.edu
Department of Electrical and Computer Engineering
Princeton University
Princeton, NJ 08544, USA
Boyi Liu boyiliu2018@u.northwestern.edu
Department of Industrial Engineering and Management Sciences
Northwestern University
IL 60208, USA
Zhuoran Yang zhuoranyang.work@gmail.com
Department of Statistics and Data Science
Yale University
CT 06511-6814, USA
Zhaoran Wang zhaoranwang@gmail.com
Department of Industrial Engineering and Management Sciences
Northwestern University
IL 60208, USA
Mengdi Wang mengdiw@princeton.edu
Department of Electrical and Computer Engineering
Princeton University
Princeton, NJ 08544, USA
Editor: Mingyuan Zhou
Abstract
We study the Constrained Convex Markov Decision Process (MDP), where the goal is to
minimize a convex functional of the visitation measure, subject to a convex constraint.
Designing algorithms for a constrained convexMDP faces severalchallenges,including (1)
handling the large state space, (2) managing the exploration/exploitation tradeoff, and
(3) solving the constrained optimization where the objective and the constraint are both
nonlinear functions of the visitation measure. In this work, we present a model-based
algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian
and Fenchel duality are implemented to reformulate the original constrained problem into
anunconstrainedprimal-dualoptimization. Moreover,theprimalvariablesareupdatedby
model-basedvalue iterationfollowingthe principle ofOptimism in the Face of Uncertainty
(OFU), while the dual variables are updated by gradient ascent. Moreover,by embedding
the visitation measure into a finite-dimensional space, we can handle large state spaces by
incorporatingfunctionapproximation. Twonotableexamplesare(1)KernelizedNonlinear
Regulatorsand(2)Low-rankMDPs. Weprovethatwithanoptimisticplanningoracle,our
algorithm achieves sublinear regret and constraint violation in both cases and can attain
the globally optimal policy of the original constrained problem.
©2023ZihaoLi,BoyiLiu,ZhuoranYang,ZhaoranWang, MengdiWang.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovided
athttp://jmlr.org/papers/v24/22-1190.html.
4202
beF
61
]GL.sc[
1v01801.2042:viXraLi, Liu, Yang, Wang, Wang
Keywords: Online Learning, Function Approximation, Reinforcement Learning, Con-
strained Optimization, Duality Theory
1 Introduction
In recent years, constrained reinforcement learning (RL) has attracted greater research in-
terest. In contrast to unconstrained RL, in which an agent can freely learn to maximize its
cumulative reward or minimize its cost by interacting with an unknown environment, we
face learning problems with various kinds of constraints in many real-world applications.
For example, in autonomous driving, we want to minimize the time cost while avoiding
speeding or colliding with other cars (Garcıa and Ferna´ndez, 2015). Other applications in-
cludecost-constrainedRLinmedicalapplicationsandbusinessrestrictionsfortaxcollection
optimization (Abe et al., 2010), in which the total budget is restricted.
However, existing works on Markov decision process (MDP) with constraints are still
limited. Currently, most works consider the constrained MDP with both the objectives and
constraints being linear functionals of visitation measures (Efroni et al., 2020; Ding et al.,
2021). However, in many complex scenarios, we encounter problems with certain nonlin-
ear structures. For example, in apprenticeship learning the agent aims to simulate the
performance of an expert in a demonstrated task (Abbeel and Ng, 2004b). It is difficult
to formulate an explicit reward function, and the learning goal is given by the ℓ -norm
2
distance between the visitation measure of the agent and the expert. In multi-objective
MDP, we have to consider nonlinear interaction between different objectives (Wu et al.,
2021; Yu et al., 2021). Other examples include cautious MDP (Zhang et al., 2020a) and
general utility MDP (Zhang et al., 2020b).
In this work, we introduce the Constrained Convex Markov Decision Process (C2MDP),
where we consider a constrained convex optimization over the space of visitation measures.
The agent manipulates her policy over the space of visitation to minimize the objective
while fulfilling the constraints. Compared to previous works, our model allows objectives
and constraints to be nonlinear in visitation measure, thus significantly extending beyond
ConstrainedMDP(Efroni et al.,2020;Ding et al.,2021). Moreover, ourmodelcovers inter-
estingexamplessuchasconvexMDP(Zahavy et al.,2021),generalutilityRL(Zhang et al.,
2020b), and apprenticeship learning (Abbeel and Ng, 2004b) as special cases. Challenges
in designing an efficient online algorithm for constrained convex MDP are threefold:
(i) Most existing theoretical convergence guarantees for convex MDP apply only to the
tabular case (Zhang et al., 2020b; Efroni et al., 2020; Zahavy et al., 2021), where the
visitation measure is a vector of dimension O(H ), making the convex MDP a
|S||A|
convex optimization problem. However, when facing a continuous state space, the
visitation measure becomes a general distribution on the state-action space. Due to
the curse of dimensionality, algorithms designed for tabular MDP fail to tackle the
problem.
(ii) Highly different from simple constrained MDP, which only imposes a linear constraint
in the value function, the objective and constraint of C2MDP can be nonlinear func-
tionals of the visitation measure. Without knowing further structure, finding optimal
2Primal-Dual Policy Optimization for Constrained Reinforcement Learning
solutionsforsuchproblemsismuchharderthanConstrainedMDP,whichisequivalent
to solving a linear programming problem (Efroni et al., 2020).
(iii) In a C2MDP, the transition of the environment is unknown, and can only be learned
through the transition through interacting with the environment. With limited infor-
mation, designing an efficient online exploration strategy is hard.
With these coupled challenges, we ask the following question:
Can we find the globally optimal policy of constrained convex MDP in online learning?
In this work, we give an affirmative answer to this question.
• To handle (i), we incorporate function approximation and formulate the optimization
in the embedded space of the visitation measures. In particular, we consider the fea-
ture map in function approximation and its expectation underthe visitation measure,
which is known as the kernel embedding of visitation measure (Hofmann et al., 2008;
Muandet et al., 2016). We further consider the optimization with the kernel embed-
ding of the visitation being the decision variables, which motivates us to implement
online optimization techniques for solving C2MDP. Such a formulation recovers the
tabular setting as a special case when using the canonical embedding.
• To handle (ii), we use Lagrangian duality to transform the constrained problem to
an unconstrained minimax optimization problem. In presence of Slater’s condition,
it is guaranteed that the original minimization shares the same optimal value with
the unconstrained one. Moreover, to handle nonlinearity in the objective and the
constraint, we apply Fenchel duality to introduce a linear structure. Combining the
above two types of duality, we obtain a primal-dual optimization problem with a
linear dependency on the kernel embedding. This allows us to construct a linear
reward and adopt techniques of previous works in model-based value iteration, such
as Kakade et al. (2020); Ayoub et al. (2020).
• To handle (iii), we apply the principle of Optimism in the Face of Uncertainty (OFU)
(Jin et al., 2020; Yang et al., 2020) by an optimistic planning oracle (Jin et al., 2021;
Kakade et al., 2020; Ayoub et al., 2020) which behaves as if the model parameters
assume their best possible values in accordance to the observations so far.
With the above techniques, our algorithm is provably sample-efficient. In specific, we
prove that our algorithm achieves O(√T) in both the regret and the constraint violation,
whereT isthenumberofthesamplingepisodes. Tothebestofourknowledge,ouralgorithm
is the first provably sample-efficient algorithm for the constrained nonlinear optimization
over visitation measures. As special cases, our method can be widely applied to multi-
objective MDP, and apprenticeship learning, and lead to efficient algorithms.
1.1 Related Works
Optimizationoveroccupancymeasures/ConvexMDP.Severalearlyworks(Tewari and Bartlett,
2007; Chen and Wang, 2016; Wang, 2017, 2020) studied tabular MDP via linear program-
mingreformulation. Zahavy et al.(2021)studiedconvexMDPviaFenchelduality. Zhang et al.
3Li, Liu, Yang, Wang, Wang
(2020a,b,b) studied for convex optimization over occupancy measures. However, while all
these methods are successful in tabular MDP, they cannot (i) avoid the curse of dimensions
in large state space MDP, and (ii) handle constraints.
Constrained MDP. Our work is a generalization of the constrained MDP. Efroni et al.
(2020);Yu et al.(2021);Qiu et al.(2020);Brantley et al.(2021)studiedtabularconstrained
MDP. Ding et al. (2021) studied safe reinforcement learning under function approximation
setting under a linear mixture MDP model and using upper confidence bound (UCB) al-
gorithm for exploration. Wu et al. (2021) further provided a general algorithm for Multi-
objective MDPwithgeneralconstraints andobjective relies onmultiplevaluefunctions. All
of these methods assume that a given reward exists and explores the environment following
the principle of optimism, and achieves great success by providing sublinear regret and con-
straint violation. Vaswani et al. (2022) provides a zero-constrained algorithm and provide
a lower bound under such scenario. However, when there is no given reward function, these
methods are no longer applicable.
Provably efficient online RL. Our work is closely related to a line of provably efficient
online RL algorithms on Low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022) and
kernelized nonlinear regulator (Kakade et al., 2020; Mania et al., 2020), where efficient ex-
ploration of the agent is obtained by choosing an optimistic model in the confidence set.
However, these results are only designed for unconstrained problems that are linearly de-
pendent on the occupancy measure.
1.2 Notations
We denote by [a : b] the set of integers between a and b, i.e., [a : b] = i Z a i b ,
{ ∈ | ≤ ≤ }
and write [n] = [1 : n]. We denote by x = (x ) the column vector obtained by
h h∈[H]
concatenating the elements of x , i.e., x = (x ; ;x ). We write a b as the inner
h h∈[H] 1 H
{ } ··· ·
product of two finite dimensional vectors, and f,g as the inner product of two functions
H
h i
f and g in the reproducing kernel Hilbert space (RKHS) . We also denote by the
2
ℓ -norm in Euclidean space, and d the the unit ball in RdH , i.e., x Rd : x k 1·k . The
2 2
B { ∈ k k ≤ }
set of probability distribution over a space is denoted by ∆( ). We define (s′ s,a) as
X X P |
the probability for the agent transiting to state s′ from s when taking action a.
2 Background
In this section, we briefly introduce the concepts of reinforcement learning, Constrained
Convex MDPs, Low-rank MDPs, and Kernalized nonlinear Regulator (KNR).
2.1 MDP Setting
We consider an episodic Markov decision process problem ( , ,H,c) , where Rd is the
S A S ⊂
state space embedded in the Euclidean space, is a (possibly continuous) action space, H
is the horizon, and c = c H is a collectionA of cost functions where c : R is
{ h }h=1 h S ×A →
the cost of stage h. In each episode, we consider an agent with policy π = π H , where
{ h }h=1
π : ∆( ). At the stage h, the agent takes an action a according to the policy
h h
S → A ∈ A
π ( s ). Thestate then transits to s with probability (s s ,a ) according to the
h h h+1 h h+1 h h
·| P |
4Primal-Dual Policy Optimization for Constrained Reinforcement Learning
underlying transition rule. Since the choice of the initial state does not add complexity to
the problem, for simplicity, we assume that the initial state is fixed, i.e., s = s.
1
We introduce the concepts of action-state value function and state value function from
reinforcement learning. The action-state value function Qπ : R is defined as
h S ×A →
H
Qπ(s,a) = E c (s ,a ) s = s,a = a , (s,a,h) [H].
h π i i i h h ∀ ∈ S ×A×
(cid:20)i=h (cid:12) (cid:21)
X (cid:12)
(cid:12)
Correspondingly, the action-value fun(cid:12)ction Vπ : R is defined as
h S →
H
Vπ(s)= E c (s ,a ) s = s , (s,h) [H]. (1)
h π i i i h ∀ ∈ S ×
(cid:20)i=h (cid:12) (cid:21)
X (cid:12)
(cid:12)
Here the expectation E [] is taken over the(cid:12) trajectory (s ,a ) induced by π
π h h h∈[H] h h∈[H]
· { } { }
and the underlying transition. For notation simplicity, we also write
P hf(s h,a h)= E s′∼P(·|sh,ah)[f(s′)]
for any integrable f : R and conditional probability ( s ,a ).
h h h
S → P ·|
2.2 Constrained Convex MDP
We generalize the problem of convex MDP (Zahavy et al., 2021), which considers a non-
constrainedconvexoptimizationproblemwiththeoccupancymeasured = (d (s,a))
π π,h (s,a,h)∈S×A×[H]
as the variable. The agent manipulates the occupancy measure by properly adjusting its
policy. The aim is to find the optimal policy that minimizes the objective function. A
tabular Constrained Convex MDP (C2MDP) usually takes the form of
min f (d ) s.t. d (s,a) = E 1 , (2)
π∈∆(A|S,H)
π,h h∈[H] π,h π (sh,ah)=(s,a)
(cid:0) (cid:1) (cid:2) (cid:3)
g(d (s,a)) 0,
π,h
≤
where f,g : R|S||A|H R are both convex functions.
→
In the tabular MDP, the set of all x induced by the agent’s policy is represented by a poly-
tope represented by O( ) linear constraints (Efroni et al., 2020; Zahavy et al., 2021).
|S||A|
However,inthecontinuousstatespacecase,duetothecurseofdimensionalityandtheshort-
age of memory, such an LP formulation is generally impossible. Therefore, we incorporate
function approximation to handle the large state space by embedding the information of
the state-action pair with a finite-dimensional feature map ψ : Rd. Such a method
S×A →
is widely used in RL literature (Yang et al., 2020; Jin et al., 2020; Uehara et al., 2022;
Kakade et al., 2020). To describe the visitation of the agent, we apply kernel embedding
of probability distribution to the visitation measure (Muandet et al., 2017; Zahavy et al.,
2021;Efroni et al.,2020). Byembeddingtheprobabilitydistributioninducedbytheagent’s
policy π on into finite dimension linear space, the objective and constraints related
S ×A
to the distribution can be reformulated into a function for the kernel embedding.
5Li, Liu, Yang, Wang, Wang
Definition 1 (Kernel Embedding) For a MDP with kernel feature mapping ψ :
h
Rd , we define its kernel embedding as { S ×
h∈[H]
A → }
Ψπ = Ψπ =E [ψ (s ,a )] , (3)
h π h h h h∈[H]
(cid:0) (cid:1)
where the expectation is taken under the trajectory (s ,a ) induced by policy π and
h h h∈[H]
{ }
the underlying transition.
The kernel embedding in (3) represents the agent’s visitation distribution on every state-
action pair under the policy π. The kernel method is also frequently used in existing MDP
literature, as it can be used to incorporate function approximation when designing learning
targets. For example, The reward function is often regarded as a linear function of a
kernelized feature mapping in RL literature (Yang et al., 2020; Jin et al., 2020; Ding et al.,
2021; Wu et al., 2021). When the reward function is known, we can also take the reward
functionasthekernelfeature(Kakade et al.,2020;Uehara et al.,2022). InanMDPwithan
underlying kernelized structure, we can evaluate the agent’s policy by its initial state value
function Vπ = E [ H c (s ,a )] . The function Vπ can be reformulated to Vπ = θ Ψπ
1 π h=1 h h h 1 1 ·
under the linear function approximation case (Yang et al., 2020; Jin et al., 2020), which is
P
a linear mapping with respect to the kernel embedding. In the general case, when ψ is not
h
given, we can learn it through a supervised learning oracle or a model-free exploration, and
subsequently employ it in our downstream algorithms, e.g. see Algorithm 1 in Modi et al.
(2022). Thus,in an MDP-related optimization, it is reasonable to usethe kernel embedding
as a measure of how a state-action pair (s,a) contributes to the objective. To this end, we
aim to solve the following optimization problem defined as a constrained convex MDP,
min f Ψπ s.t. g(Ψπ) 0. (4)
π∈∆(A|S,H) ≤
(cid:0) (cid:1)
To measure the efficiency of policies in the first T episodes, we introduce the following
performance measures,
Regret(T) =T f
Ψπb
f
Ψπ∗
, Violation(T) = Tg
Ψπb
. (5)
−
Here Ψπb = 1/T T Ψπk is(cid:0) th(cid:0) e av(cid:1) erage(cid:0) ker(cid:1) n(cid:1) el embedding correspo(cid:0) ndin(cid:1) g to the mixed
t=1
policy π = π H of the first episode. By mixed policy we mean the agent rolls out and
{ h }h=P1
performs a random policy of index from 1 to T in equal probability at the beginning state.
The pebrformbance measures in (5) are widely adopted by previous works in RL where a
convex objective function is concerned (see, e.g., Ding et al. (2021); Brantley et al. (2021);
Yu et al. (2021); Wu et al. (2021)).
We remark that our model is more general than the standard RL problem. To see this, we
can reduce the C2MDP to standard RL by setting ψ (s ,a ) = c (s ,a ) , f as the linear
h h h h h h
mapping with a H-dimension one hot feature vector, and removing the constraint.
Example 1 (Multi-objective MDP, (Yu et al., 2021; Wu et al., 2021)) AMulti-objective
MDP considers the following problem,
min h (Vπ) s.t. h (Vπ) 0, (6)
1 2
π∈∆(A|S,H) ≤
6Primal-Dual Policy Optimization for Constrained Reinforcement Learning
where
H
Vπ = E ci(s ,a )
π h h h
(cid:18) (cid:20)h=1 (cid:21)(cid:19)i∈[I]
X
is the initial state value function vector, and h ,h : RI R are 1-Lipschitz convex func-
1 2
→
tions. If we use linear function approximation for the cost function, i.e.
ci(s,a) = ψ(s,a) θi, i [I]
h · h ∀ ∈
the Multi-objective MDP turns into a constrained convex MDP,
min h Ξ Ψπ s.t. h Ξ Ψπ 0.
1 2
π∈∆(A|S,H) · · ≤
(cid:0) (cid:1) (cid:0) (cid:1)
Here Ξ =(θ1,⊤ , θ1,⊤ ; θI,⊤ , ,θI,⊤ ) is a matrix formed by concatenating by θi h∈[H] .
1 ··· H ··· 1 ··· H { h}i∈[I]
Note that when I = 1 and h (x) = h (s) = x, Multi-objective MDP reduces to the con-
1 2
strained MDP in Ding et al. (2021) and Efroni et al. (2020). We also claim that our model
is more general than the one in (Yu et al., 2021; Wu et al., 2021), since they assume h ,h
1 2
to be monotone in all components and h can only take the form d(x, ), with being a
2
W W
convex set.
Example 2 (Feasibility/Apprenticeship Learning, (Abbeel and Ng, 2004a; Syed et al., 2008; Miry
Feasibility learning considers minimizing the distance between the kernel embedding of the
probability induced by the performance policy and a convex set , i.e.,
W
min dist(Ψπ, ). (7)
π∈∆(A|S,H) W
Here dist can be chosen as any sort of discrepancy measure.When reduces to a singleton
W
Ψ = (E [ψ(s ,a )]) , i.e. the kernel embedding of a given probability distribution
P h h h∈[H]
{ }
, the optimization reduced to apprenticeship learning.
h h∈[H]
{P }
2.3 Examples of the Underlying Transition Models
Recall that C2MDP is defined for any decision problem with a given linear kernel in its
objective. With additional assumptions on the underlying transition, we can define dif-
ferent algorithms for solving it. The transition models we discuss here are (1) Kernelized
Nonlinear Regulator (KNR)setting and(2) Low-rank MDP setting, whichcannot besolved
by algorithms design for tabular setting.
Kernelized Nonlinear Regulator. The Kernelized Nonlinear Regulator setting gener-
alizes the linear quadratic regulator (LQR) setting (Kakade et al., 2020) and is especially
helpful in continuous control problems. A KNR is an MDP with the following transition
model,
s = W⋆φ(s ,a )+ǫ, ǫ 0,σ2 , (8)
h+1 h h
∼ N I
for all h [H], where φ : is a given kernel fe(cid:0)ature m(cid:1) apping of a d-dimension
∈ S ×A → H
space Rd (Kakade et al., 2020; Mania et al., 2020; Song and Sun, 2021). The transition pa-
rameterization W∗ characterizes the mapping from the feature φ(s ,a ) to the expectation
h h
7Li, Liu, Yang, Wang, Wang
of the next state s . We also remark that the KNR is a general model in the sense that
h+1
both the state space and the action space can be continuous.
S A
Low-rankMDP.InaLow-rankMDP(Uehara and Sun,2021;Agarwal et al.,2020;Modi et al.,
2022), the underlying transition takes the form
∗(s s ,a )= φ∗(s ,a ),µ∗(s ) , (9)
Ph h+1 | h h h h h h h h+1 i
(1) (d)
for all h [H]. Here the vector µ = (µ ,...,µ ) is the concatenation of d unknown
∈ h h h
(signed) measures over . Unlike KNR , both the feature mapping φ∗ and the measure
S h
µ∗ in Low-rank MDP are unknown to the agent and need to be learned. For Low-rank
h
MDP, it is natural to assume the agent access to two function classes Θ Rd and
⊂ S×A →
Υ Rd for candidate mappings for learning the true embeddings (µ∗,φ∗). Thus we
⊂ S → h h
make the following assumption,
Assumption 2 (Realizability) Themodelclass(Θ,Υ)with µ∗ Θand φ∗
{
h}h∈[H]
⊂ {
h}h∈[H]
⊂
Υ is known, where both Θ and Υ are finite sets.
Uehara et al. (2022) show that the case of finite function class can be easily generalized
to infinite case. When feature φ∗ is known, such a setting degerates to the linear MDP
Yang and Wang(2019,2020)Withoutlossofgenerality,wealsomakethefollowingstandard
assumptions (Kakade et al., 2020; Uehara et al., 2022). The choice of the upper bound will
not add complexity to our analysis.
Assumption 3 We have the following assumptions.
1. For the KNR case, we assume that the feature φ of the underlying RKHS is uniformly
bounded, i.e., φ(s,a) 1 for all (s,a) . For simplicity, we also assume that
2
k k ≤ ∈ S×A
the transition parametrization satisfies W∗ 1, here is the matrix 2-norm.
2 2
k k ≤ k·k
2. For the Low-rank MDPs, we assume that φ (s,a) 1 for all (s,a) , and
h 2
k k ≤ ∈ S ×A
for any function g : [0,1] and µ Υ, µ (s)g(s)dν √d, here ν() is a
S → ∈ k S h k2 ≤ ·
given abstract measure defined on the state space .
R S
3. Forthekernelvectors ψ intheobjectiveandconstraint, weassume ψ (s,a)
h h∈[H] h 2
{ } k k ≤
B.
4. We assume that the objective f and the constraint g in (4) are convex and 1-Lipschitz,
which further implies that ∂f , ∂g 1.
2 2
{k k k k } ≤
For both cases mentioned above, the underlying transition probability is unknown, and
can only be estimated through stochastic interactions with the environment. Thus, directly
representing the set of all kernel embedding, i.e., = Ψπ : any π ∆( ,H) is impos-
V { ∈ A|S }
sible, which makes (4) a challenging problem. As a consequence, we cannot simply regard
(4) as a constrained optimization problem. Instead, we have to learn the optimal policy
by collecting data via interacting the environment. Moreover, with the general constraint
g(Ψπ) 0 on the distribution, the simple dual optimization method for set constraint
≤
(Yu et al., 2021) becomes infeasible. To address these challenges, we introduce a primal-
dual algorithm in the subsequent section.
8Primal-Dual Policy Optimization for Constrained Reinforcement Learning
3 Main Algorithm
Inthissection,weprovideaprimal-dualalgorithmVariationalPrimal-DualPolicyOptimization
(VPDPO) for (4), which achieves sublinear in both regret and constraint violation.
3.1 Reformulation: Double Duality
In this subsection, we reformulate (4) as an unconstrained convex-concave problem, so that
we can utilize the standard MDP method to solve it. Doing so will enable us to design a
provably efficient algorithm.
The convex problem (4) is nontrivial only when its feasible set is none-empty. With the set
of all reachable kernel embedding , we assume that g(Ψπ) 0 is not empty, so that
V V∩{ ≤ }
(4) is well-posed. To verify the convexity of feasible set (4), we first present the following
proposition.
Proposition 4 (Convex Problem) The generalized optimization problem in (4)isacon-
vex problem.
Proof See Appendix E.3 for detailed proof.
Next, we make the following assumption on g, which is standard in convex optimiza-
tion and constrained convex MDP literature (Zahavy et al. (2021), Efroni et al. (2020),
Ding et al. (2021)).
Assumption 5 (Slater Point) There exists a policy π′, such that (4) holds with strict
inequality, i.e.,
g(Ψπ′
) < 0.
Note that in Assumption5, wedonotrequirea pre-knowledgefor π′. From an optimization
perspective, a problem-dependent Slater condition is a measure of the size of the feasible
region and determines the difficulty of solving a constrained optimization. The absence of
such a condition may result in the lack of constraint qualification and cause failure in even
simpleoptimization problems,forexample,seeHijazi and Liberti(2016). WithAssumption
5, we can reformulate (4) to a standard Lagrangian optimization problem (Corollary 28.1.1,
Rockafellar (1970)). The Lagrangian function of (4) takes the form
minmax f(Ψ)+γ g(Ψ) . (10)
Ψ∈V γ≥0 ·
(cid:0) (cid:1)
Slater’s condition not only justifies the application of the Lagrangian duality but also
allows us to bound the optimal value of the Lagrangian dual variable γ∗ from above, which
will further be helpful for our algorithm for the gradient update of the dual variables.
Lemma 6 (Bounded Lagrangian Dual Variable) With Slater’s condition in (5) , we
have
0 γ∗ Γ := f(Ψπ′ ) f(Ψπ∗ ) /g(Ψπ′ ). (11)
≤ ≤ − −
(cid:0) (cid:1)
9Li, Liu, Yang, Wang, Wang
Proof See Appendix E for detailed proof.
Lemma 6 provides an upper bound for the optimal dual variable γ∗. In order to find γ∗, we
only need to focus on the interval [0,Γ]. In practice, we only need to know an upper bound
of Γ, which can be easily achieved through linear search.
Since f, g are 1-Lipschitz continuous and satisfy the closed-proper function condition,
we have
f(Ψπ) = max α⊤Ψπ f∗(α) , γg(Ψπ)= max β⊤Ψπ γ g∗(β/γ) , (12)
α∈BdH − β/γ∈BdH − ·
(cid:0) (cid:1) (cid:0) (cid:1)
for all γ 0 (Corollary 13.3.3, Rockafellar (1970)). Here f∗ and g∗ are the Fenchel duals
≥
of f and g, respectively. With these relations, we linearize the objective functions in (10)
by introducing the variables α, β,
min max (α,β,γ,π) = (α+β)⊤Ψπ f∗(α) γ g∗(β/γ) . (13)
Ψπ∈Vγ≥0,α,β/γ∈BdHD − − ·
(cid:0) (cid:1)
Wenowreformulatetheoriginallynon-linearminimizationproblemintoamin-maxproblem
that is linear in Ψ and concave in (α,β,γ). Note that is a closed convex set due to our
V
setting and Assumption 3. Meanwhile, the feasible set for the dual variables (α,β,γ) is
a convex compact set. Therefore, by the minimax theorem (Rockafellar, 1970), we can
reformulate (13) to
max min (α,β,γ,π) = (α+β)⊤Ψπ f∗(α) γ g∗(β/γ) . (14)
γ≥0,α,β/γ∈BdHΨπ∈VD − − ·
(cid:0) (cid:1)
In the rest of this paper, we denote by α∗, β∗ and γ∗ the optima of the dual variables in
(14), π∗ = π∗ the optimal policy, and Ψ∗ the kernel embedding corresponding to
{
h}h∈[H]
π∗. We can rewrite (14) as max (γ,α,β), where (γ,α,β) = min (α,β,γ,π).
γ,α,β Ψπ∈V
L L D
When the dual variables are fixed, it suffices to implement model-based value iteration for
solving (γ,α,β). By simultaneously updating γ, α and β, we can reach optimality by a
L
primal-dual method.
Remark 7 In (14), the term γg∗(β/γ) is a convex function composed with a perspective
function, so it must be convex in (β,γ). See Boyd et al. (2004) for details.
3.2 Solution: Primal-Dual Method
The minimax structure in (13) implies us to implement a primal-dual method. Such im-
plementation is common when facing nonlinearity in visitation measures (e.g., Wu et al.
(2021) and Efroni et al. (2020)).
Dual Update. We perform an online projected gradient ascent method for a dual update.
In each iteration, we update α by moving αk to a direction of maximizing the dual function
(α,β,γ,π) and then project it to the unit ball. To represent the projection set for (β,γ),
D
we combine the restriction imposed by Fenchel dual and Slater’s condition and define
= (β,γ) : β γ,γ [0,Γ] .
2
G { k k ≤ ∈ }
When the Slater’s condition holds, the optimal solution (β∗,γ∗) always lies in by Lemma
G
6. If we know the underlying transition map W∗ in priori, we can solve the outer iteration
10Primal-Dual Policy Optimization for Constrained Reinforcement Learning
of theminimax problemin (13)by valueiteration andimplement
Ψπt
in thegradient ascent
step. However, sincethetransitionremainsobscuretous,weuseΨt = (Ψ ) asaproxy,
h h∈[H]
where Ψt = E [Ψ (s ,a )]. In the dual update, the step size ηt is set as O(1/√t) (or
h π,Pt h h h
O(1/√T) when T is given). In Algorithm 1, ∂ and ∂ are the subgradient operator with
γ β
γ and β as the variable, respectively.
Algorithm 1 Variational Primal-Dual Policy Optimization
Require: Step size ηt T , α1 dH, γ1 [0,Γ], β1 γ1 dH
{ }t=1 ∈ B ∈ ∈ ·B
1: for t = 1,...,T do
2: αt+1 Π αt+ηt Ψt ∂f∗(αt)
←
BdH
−
3: βt+1 βt+ηt Ψt ∂ g∗(βt/γt)
← (cid:8) − (cid:0)β (cid:1)(cid:9)
4: γt+1 γt+ηt ∂ ( γtg(βt/γt))
γ
← (cid:0) − (cid:1)
5: (bβt,γt)= Π (βt+1,γt+1)
G (cid:0) (cid:1)
6: θbt+1 αt+1+βt+1
←
7: Update the cobst funbction {ct h+1(s,a) = θ ht+1 ·ψ(s,a)
}h∈[H]
8: Update the confidence set t+1 by Algorithm 2 or 3
9: (πt+1, t+1) argmin minC Vt+1,π .
P ← π P∈Ct+1 1,P
10: Calculate Ψt+1 = (E [ψ(s ,a )])
πt+1,Pt+1 h h h∈[H]
11: end for
Primal Update: Construct a cost. Algorithm1furtherrelies ontheagent’s exploration
toestimatethetransition t withexperienceintheprevioust 1episodes. Sinceanexplicit
P −
cost does not necessarily occur in our optimization problem, to implement value iteration,
weconstructacost byintroducingthedualvector θt = αt+βt forallt, andsetatemporary
reward ct = ψ θ . Note that in the minimax problem (13), with fixed (α,β), the objective
h h · h
function turns into
H
min (α+β) Ψπ = E π,P∗[Ψ h(s h,a h) (α
h
+β h)],
π · ·
h=1
(cid:0) (cid:1) X
which can be viewed as an accumulative cost minimization problem. This is essentially an
optimal control problem. Corresponding to ct, we set the value functions
h
H
Vt,π (s)= E ct(s ,a ) s = s , (15)
h,P π,P h i i h
(cid:20)i=h (cid:12) (cid:21)
X (cid:12)
H (cid:12)
Qt,π (s,a) = E ct(s ,a )(cid:12) s = s,a = a . (16)
h,P π,P h i i h h
(cid:20)i=h (cid:12) (cid:21)
X (cid:12)
(cid:12)
for policy π. For simplicity, we denote Vt,πt and Q(cid:12) t,πt as Vt and Qt , respectively.
h,P h,P h,P h,P
Here and in the rest of this paper, we denote by E [] the expectation taken over the
π,P
·
trajectory (s ,a ) induced by π and the underlying transition kernel .
h h h∈[H] h h∈[H]
{ } { } P
With the confidence set t given by Algorithms 2 and 3, Line 9 in Algorithm 1 follows the
C
principle of “Optimism in the Face of Uncertainty”, and chooses the policy and model in
11Li, Liu, Yang, Wang, Wang
the confidence set that can incur the smallest cost. We highlight that Algorithm 1 is a
model-based algorithm, as it explicitly learns the underlying transition probability.
Algorithm 2 VPDPO for KNR case
Require: (si,ai) , λ > 0, C > 0, θt,Λ = λI, π = a
{ h h }i∈[t],h∈[H] 1 0 0 0
1: Execute πt to sample a new trajectory (st,at)
{ h h }h∈[H]
2: Wt argmin t H Wφ(sτ,aτ) sτ 2 +λ W 2.
← W τ=1 h=1 h h − h+1 2 k kF
3: Λt λI + t H φ(sτ,aτ)φ(sτ,aτ)⊤.
4: Uc p← date t τ=1P h=1P W h (cid:13) (cid:13) Wh t Λh t 1/h 2 2 R(cid:13) (cid:13) t, W 1, parametrized by W
C P ← PP | − 2 ≤ k k2 ≤ P
with Rt defined (cid:8)in (1 (cid:13)7)(cid:16). (cid:17) (cid:0) (cid:1) (cid:13) (cid:9)
5: return Confidence s(cid:13) et t c (cid:13)
C
Algorithm 3 VPDPO for Low-rank MDP case
Require: model set = (µ,φ) : µ Υ,φ Θ , = ∅, π = U( )
0,h 0
M { ∈ ∈ } D A
1: Collect a set of tuples (st,at,st ) by rolling out st with policy π and then
{ h h h+1 }h∈[H−1] h t
select at by a uniform distribution on , i.e. at U( ), st ( st,at), .
h A h ∼ A h+1 ∼ P ·| h h
2: Update = (st,at,st ) .
Dh,t Dh,t−1 ∪{ h h h+1 }
3: (µt,φt) argmax E [log(µ(s )⊤φ(s ,a ))] .
h h ← (µ,φ)∈M Dh,t h+1 h h
4: t( s ,a ) µt()⊤φt(s ,a ).
5:
UP bh pd· a|b teh th ← h ·
=
h h h
E t( s ,a ) P ( s ,a ) 2 Rt with Rt
dbefined
iC
n
(1←
8)
bP b{Ph }h∈[H] D ht kPh ·| h h − h ·| h h k1 ≤
(cid:8) (cid:12) (cid:2) (cid:3) (cid:9)
6: return Confidence set t (cid:12) b
C
Algorithms 2 and 3 interact with the environment with policy πt = πt given by
{
h}h∈[H]
Algorithm 1, and then construct confidence set for possible models. In each episode, we
construct a confidence set t, whose center and weighted radius are designed deliberately.
C
The center of the confidence set is chosen by the maximum likelihood estimation (MLE),
and the weighted radius Rt is chosen so that the real transition mapping ∗ lies in t for
P C
every t with a high probability. Specifically, in Algorithm 2 we set
Rt = c(λσ2+σ2(d+log(tdet(Λt)/δdet(Λ0))) (17)
for the KNR case, and in Algorithm 3 for Low-rank MDP we set
Rt = clog(TH Υ Θ /δ)/t (18)
| || |
The difference between Algorithms 2 and 3 is that, in the KNR setting, the agent collects
a full trajectory by performing the same policy π , while in the Low-rank MDP setting,
t
for each epoch t and for h [H], the agent performs πt for the first h step and then
∈
augment the trajectory by a randomly choose an action and then transit to the next state,
i.e., a U( ), s ( s ,a ). Note that this exploration manner only influences
h h+1 h h
∼ A ∼ P ·|
the degree of H in the sampling complexity, and does not affect the sublinear regret and
violation.
12Primal-Dual Policy Optimization for Constrained Reinforcement Learning
We remark on the computation efficiency of Algorithms 1-3. For Algorithm 1, the
projection set for dual variable (β,γ) can be seen as an intersection of a second-order
G
cone (x,t) : x t and a half space (x,t) : t [0,Γ] . Projection to both sets has
2
{ k k ≤ } { ∈ }
a closed-form solution. The projection to can thus be computed via implementing the
G
alternating projection method, which involves a sequence of gradient steps and projection
(Bregman, 1967). The proxy Ψt can be estimated by Monte Carlo method, with Wt as a
knowntransition. WewouldalsoliketoremarkthatthecalculationofLine9ofAlgorithm1,
known as the optimistic planning, is in general NP-hard (Dani et al., 2008), and we assume
there is an oracle to implement it (Kakade et al., 2020; Uehara and Sun, 2021; Jin et al.,
2021; Ayoub et al., 2020). Then we only focus on the statistical complexity. From that, we
make the following assumption.
Assumption 8 (Black-box Computation Oracle) We assume that there is an oracle
that implements Line 9 of Algorithm 1.
In practice, several effective heuristics may be available through gradient-based methods
such as iLQG (Todorov and Li, 2005), and CIO Mordatch et al. (2012), or sampling-based
methods, such as MPPI (Williams et al., 2015) and DMDMPC (Wagener et al., 2019).
In the Low-rank MDP setting, motivated by the estimation of conditional probability
(Uehara et al., 2022; Agarwal et al., 2020), we useMLE for estimating the underlyingtran-
sition. Unlike in the KNR case where the MLE has a closed-form solution, it is hard to
find a general closed-form solution for representation learning by MLE. Correspondingly,
we need an oracle for efficient MLE computation for Line 1 in Algorithm 3.
Assumption 9 (Maximum-Likelihood Estimation) Consider the model class and
M
a dataset in the form of (s,a,s′), the MLE oracle returns the maximum likelihood esti-
D
mator,
(µ,φ) = argmax E log µ s′ ⊤ φ(s,a) ,
(µ,φ)∈M D
which implements Line 3 of Algorithm 3. (cid:2) (cid:0) (cid:0) (cid:1) (cid:1)(cid:3)
b b
We assume there exists practical algorithms that avoid explicitly enumerating over all func-
tions in the model space . In practice, such oracles can be reasonably approximated
M
whenever optimizing over is feasible, such as in neural networks.
M
4 Theoretical Results
In this section, we provide theoretical analysis for Algorithms 1 and 2. For the regret and
the constraint violation, we make the decompositions
T f(Ψπb ) f(Ψ∗) = T f(Ψπb ) f(Ψ) +T f(Ψ) f(Ψ∗) ,
− − −
(cid:0) (cid:1) (cid:0) (R.i) (cid:1) (cid:0) (R.ii) (cid:1)
b b
T g(Ψπb ) = |T g(Ψπb ){z g(Ψ)}+T| g(Ψ),{z }
− ·
(cid:0) (cid:1) (cid:0) (V.i) (cid:1) (V.ii)
b b
| {z } | {z }
where we recall that Ψπb = 1/T T Ψπt, Ψ = 1/T T Ψt. Here (R.i) and (V.i) are
t=1 t=1
the estimation errors incurred by the noise in the regression. With the Lipschitz condition
P P
b
13Li, Liu, Yang, Wang, Wang
imposed on f and g, it suffices to bound T (Ψπt Ψt) . We reformulate it into
k t=1 − k2
bounding a value difference summation T (Vt Vt ). The gradient update for the
t=1P1,Pt − 1,P∗
dual variables allows us to give an upper bound for (R.ii) and (V.ii) in terms of a value
P
difference sequence as well.
However, we first need to handle the non-linearity in (4). By implementing the online
gradient ascent method in Algorithm 1, we can guarantee that the following coupling term
can be bounded by the value difference of two processes and an O(√T) term.
Lemma 10 (Dual Update: Gradient Ascent) For all γ [0,Γ], we have
∈
T
T f(Ψ) f(Ψ∗)+γ g(Ψ) θt (Ψt Ψ∗)+CBΓ√HT, (19)
· − · ≤ · −
t=1
(cid:2) (cid:3) X
b b
where C > 0 is an absolute constant.
Proof See Appendix B for detailed proof.
Lemma 10 displays a coupling between the regret and the constraint violation, which is also
frequentlymetinonlinealgorithmsusingdualupdates,suchasCMDPandMulti-objectives
(Ding et al., 2021; Yu et al., 2021). The proof of Lemma 10 incorporates the standard re-
gret analysis of online gradient ascent and the self-dual property of Fenchel dual, which
is a common technique in analyzing nonlinear function differences with gradient updates.
The occurrence of the coupling term directly comes from the gradient update of the dual
variables in Algorithm 1.
In the following lemma we introduce the difference of a sequence of projected kernel embed-
ding, which can be interpreted as the performance difference of two systems in T episodes.
When ∗ falls in t, by the principle of optimism implemented in Line 9 of Algorithm 1,
P C
the value difference is always negative. In this paper, we denote the event of ∗ t for
P ∈ C
all t [T] by , i.e., = T ∗ t . By the construction of the confidence set, we
∈ Ecb Ecb ∪t=1{P ∈ C }
can further prove that ∗ always lies in t with the probability of at least 1 δ. With the
P C −
construction of confidence set in Algorithms 2 and 3, we choose the transition model and
policy that would incur the highest accumulative reward in expectation. Therefore, as long
as the real dynamic falls in the confidence set, we can obtain optimism in the sense of the
following lemma,
Lemma 11 (Optimism: Value Difference) If the real model ∗ falls in the confidence
P
set t for all t, then we have the following inequality,
C
T
θt (Ψt Ψ∗) 0. (20)
· − ≤
t=1
X
Proof The inequality comes from the construction of the cost function in (15) and the
choice of t and πt in Line 9 in Algorithm 1.
P
Conditioning on the event that Lemma 36 holds, we actually claim that the coupling term
in (19) can be bounded by O(√T). Combining this with the optimization trick of Theorem
33, we can further prove that (R.i) and (V.i) are boundedby O(√T). We leave the detailed
proof in Section B.
14Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Lemma 12 Assume that ∗ t for all t [T]. Then for all γ [0,Γ], we have
P ∈ C ∈ ∈
T(f(Ψ) f(Ψ∗)) CBΓ√HT, (21)
− ≤
T g(Ψ) CB√HT. (22)
b · ≤
We now bound the difference of the couplinbg of the objective and constraint violation by
√T, with the estimated feature embedding Ψ as a self variable. But what can we say
about the difference between the estimated average feature embedding Ψ and the real
average feature embedding
Ψπb
,
Ψπb
Ψ ? Tobtackle this issue, we interpret the difference
2
k − k
of the kernel mean embedding as the supreme of a set of value differencebs. For a fixed
x = (x ) RdH with x 1b, we can consider T (Ψπb Ψ) x as the value
h h∈[H] ∈ k k2 ≤ t=1 − ·
difference of two processes, with cost at stage h defined as c (s ,a ) = ψ(s ,a ) x . For
h h h h h h
P ·
simplicity, we denote x (Ψπt Ψt)= Vπt Vt. As long as we can unifbormly upper bound
· − 1 − 1
Vπt Vt for all x 1, we can give a bound for Ψπb Ψ . The following lemma allows
1 − 1 k k2 ≤ k − k2
us to decompose a value difference and is useful in our analysis.
b
Lemma 13 (Value Difference Lemma) Consider two MDPs , , 1 H , r H
S A {Ph}h=1 { h }h=1
and , , 2 H , r H and a given policy π = π . Then for all h [H] the
S A {Ph}h=1 { h }h=1 { h }h∈[H] (cid:0) ∈ (cid:1)
following relation holds,
(cid:0) (cid:1)
H
Vπ(s) Vπ′ (s)= E (P1Vπ (s ,a ) P2Vπ (s ,a )) s = s . (23)
h − h π,P2 i i+1 i i − i i+1 i i h
(cid:20)i=h (cid:12) (cid:21)
X (cid:12)
(cid:12)
(cid:12)
Proof This lemma is a direct corollary of Lemma 36 in the appendix, as the two MDP
share the same reward.
Next, we directly give the performance guarantees for KNR and low-rank MDP cases, and
give a brief proof under this value difference routine for the two cases respectively. Both
results contain a O(√T logT) scale in the regret and violation, which shows that VODPO
learns in C2MDPs in a statistically efficient manner. As T grows bigger, the mixed policy
π would achieve an suboptimality that decreases in a O(logT/√T) manner. To the best of
our knowledge, this algorithm is the first one that achieves sublinear regret and constraint
vbiolation in C2MDP.
4.1 Analysis of the KNR Case
Theorem 14 Assume that Assumptions 3-5 and 8 hold. Set λ = max σ2,1 . For Algo-
{ }
rithm 1 and 2, with probability at least 1 δ, the regret is bounded by
−
HT
Regret(T) O ΓB√HT +CBHd√T log ,
≤ dδ
(cid:18) (cid:18) (cid:19)(cid:19)
and the constraint violation is bounded by
HT
Violation(T) O CBHd√T log .
≤ dδ
(cid:18) (cid:18) (cid:19)(cid:19)
15Li, Liu, Yang, Wang, Wang
4.2 Proof Sketch of Theorem 14
In this section, we sketch the proof of Theorem 14. The detailed proof is deferred to
Appendix C.
Lemma 15 (Simulation Lemma) For any policy π, feature mapping W, bounded cost c,
and for any initial state s , with the value function defined in (15)(with a upper bound of
1
√H), we have
H
V 1π ,P∗(s 1) −V 1π ,P(s 1)
≤
O B√H ·E π,P∗ W⋆ −W φ(s h,a h)
2
,
(cid:18) (cid:20)h=1 (cid:21)(cid:19)
X(cid:13)(cid:0) (cid:1) (cid:13)
(cid:13) (cid:13)
where the state-value function is defined with underlying cost c. Here ∗ and are the
P P
conditional distribution induced by W∗ and W, respectively.
Proof With Lemma 13 we have
H
V 1π ,P∗(s 1) −V 1π ,P(s 1)
≤
E π,P∗ B√H Ph∗( ·|s h,a h) −Ph( ·|s h,a h)
1
(cid:20) h=1 (cid:21)
X(cid:13) (cid:13)
H (cid:13) (cid:13)
. B√HE π,P∗ (W⋆ −W)φ(s h,a h)
2
,
(cid:20)h=1 (cid:21)
X(cid:13) (cid:13)
(cid:13) (cid:13)
where the second inequality follows from the estimation
∗( s ,a ) ( s ,a ) = O (W⋆ W)φ(s ,a )
kPh ·| h h −Ph ·| h h 1 − h h 2
(cid:13) (cid:16)(cid:13) (cid:13) (cid:17)
from Devroye et al. (2018). Here we drop t(cid:13)he consta(cid:13)nts that only depen(cid:13)d on σ.
By Lemma 15 and the Elliptical Potential Lemma (Uehara and Sun, 2021), following the
value decomposition routine, we give an upper bound for the estimation error in terms of
the maximum information gain in the following lemma.
Lemma 16 (Estimation Error) For Algorithms 1 and 2, with λ = max σ2,1 , we have
{ }
∗ t for all t [T] holds with probability at least 1 δ, and
P ∈ C ∈ −
HT
T Ψπb Ψ CBHd√T log (24)
2
k − k ≤ dδ
(cid:18) (cid:19)
b
holds with probability at least 1 δ, where C > 0 is an absolute constant that only depends
−
on σ.
Proof See Appendix B for detailed proof.
With the 1-Lipschitz assumption for f,g, Lemma 16 in fact gives a uniform upper bound
for (R.i) and (V.i). Combining the results on regret and constraint violation in Lemma 12
with the error estimation in Lemma 16, we finish the proof of Theorem 14.
16Primal-Dual Policy Optimization for Constrained Reinforcement Learning
4.3 Analysis of the Low-rank MDP case
For the Low-rank MDPs, we also prove the sublinear regret and violation underAlgorithms
1 and 3.
Theorem 17 Assume that Assumptions 3-5 and 9 hold. Set Rt as in (18). For Algorithms
2 and 3, with probability 1 δ, the regret is bounded by
−
TH Θ Υ
Regret(T) O ΓB√HT +B TH d2log | || | ,
≤ |A| δ
(cid:18) (cid:18) (cid:19)(cid:19)
p
and the constraint violation is bounded by
TH Θ Υ
Violation(T) O B TH d2log | || | .
≤ |A| δ
(cid:18) (cid:18) (cid:19)(cid:19)
p
We remark that our regret and constraint violation guarantees in Theorem 14 and 17 serve
as Probably Approximately Correct (PAC) bounds: with probability at least 1 δ, we can
obtain a Markov policy π := 1 T πt such that f(Ψπb ) f(Ψπ∗ ) = O(1/√T− ), and the
T t=1 −
constraintviolationg(Ψπb )= O(1/√T). Consequently,withasamplecomplexityofO(1/ǫ2),
the Markov policy π suchbthat
f(P Ψπb
)
f(Ψπ∗
) ǫ and
g(Ψπb
) ǫ hold simultaneously with
hightprobability.
Fromanasymptotic− perspecti≤
ve,withT
tend≤ stoinfinity,f(Ψπb
)converges
to the optimal valube, while the violation of constraint
g(Ψπb
) can be arbitrarily small with
high probability. Our result is different in form from the standard definitions in online
convex optimization due the existence of both optimality gap and constraint violation.
4.4 Proof Sketch of Theorem 17
In this section we briefly sketch the proofs of efficiencies of Algorithm 1 and 3 in the Low-
rank MDP setting. For detailed proof, see Appendix D.
We define the state-action visitation induced by the mixed Markov policy before epoch t
and the one augmented by choosing random action,
1 1
ρt h(s h,a h)=
t 1
d πi,h,P∗(s h,a h), ρt h(s h,a h)=
t 1
d πi,h,P∗(s h)u(a h),
− i∈[t−1] − i∈[t−1]
X X
b
where d (s ,a ) is the visitation probability on the h-th state-action pair induced by
π,h,P h h
policy π and transition kernel , and u(a) is the uniform distribution on the action set .
P A
By implementing MLE in every epoch t, we claim that with high probability, the model
error under the distribution of the previous policy E ρbt[ kPht( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1] is of
O(1/t). With a standard Bernstein-type argument for martingales, we have the following
lemma. b
e
Lemma 18 (Shrinking Confidence Ball) With probability at least 1 δ, we have ∗
− P ∈
t and
C clog(TH Υ Θ /δ)
E ρbt h kPh∗( ·|s h,a h) −Ph( ·|s h,a h) k2 1 ≤ t| || | ,
for all transition (cid:2) t, t [T] and h [H], wher(cid:3) e c is an absolute constant. Here t is
P ∈ C ∈ ∈ Ph
the transition learned by the MLE in Algorithm 3.
b
17Li, Liu, Yang, Wang, Wang
Proof See Appendix D for details.
Lemma 18 implies that with high probability, our choice of the confidence set t is good
C
enough for the real transition to fall in. Moreover, the distance between ∗ and other ele-
P
mentsin t alsodecreasesunderthedistributionofthemixedpolicyρt. Bytheconstruction
C
of the value function of (15), we obtain the following lemma. As in the KNR case, we also
care for the error brought by our insufficient model estimation, Ψt b Ψπt 2. To overcome
k − k
this tissue, the underlying linear structure of low-rank MDPs is crucial. We introduce the
following lemma, which is a modification of Lemma 16 in Uehara et al. (2022):
Lemma 19 Take any h R such that h D. Then,
∞
∈ S ×A → k k ≤
E π[h(s h,a h)]
≤
E π kφ⋆ h−1(s h−1,a h−1) kΣ− ρt1
,φ⋆
h−1qt |A|E ρbt[h2(s,a)]+λdD2,
where Σ ρt,φ⋆
h
= tE (s,a)∼ρt φ⋆ h(s,a)φ⋆ h(s,a)⊤ +λI.
(cid:2) (cid:3)
Proof See Appendix D for details.
Note that here the parameter λ and the matrix Σ ρt,φ∗ do not occur in the actual implemen-
h
tation. This lemma introduces an elliptical potential structure. By then, using the same
method as in the KNR case, we prove the upper bound for the 2-norm estimation error.
Lemma 20 (Estimation Error) With Assumption 3 and Algorithm 3, we have
TH Θ Υ
T Ψ
Ψπb
c TH d2log | || |
2
k − k ≤ |A| δ
(cid:18) (cid:19)
p
b
holds with probability at least 1 δ.
−
Note that f,g are both 1-Lipschitz by Assumption 4, we can thereby control the upper
bound of f(Ψπ) f(Ψ) and g(Ψπ) g(Ψ) can be bounded by the same scale, combine
| − | | − |
this with Lemma 12 concludes our proof.
b b
4.5 Applications to Concrete Examples
With the general results above, we also highlight their applications on concrete examples
rise in RL. In Section 2 we introduced several settings that are well known in MDP liter-
aturewhich can be regarded as examples of C2MDP, with f being their objectives and g
being their constraints. We then implement VODPO to solve them, In this section we use
Multi-objective MDPs and Feasiblity Learning as examples to show the power of VODPO.
First, we have the following corollary for the KNR setting,
Corollary 21 Under Assumptions 2 - 8, we assume that θi √d for all i [I] and
k
hk2
≤ ∈
h [H]. Set λ = max σ2,1 , we have
∈ { }
O ΓH√IT +CH3/2d√IT log HT for Multi-objective MDP,
Regret(T) dδ (25)
≤ (O (cid:0)CHd√T log H dδT
(cid:0) (cid:1)(cid:1)
for Feasibility Learning,
(cid:0) (cid:0) (cid:1)(cid:1)
18Primal-Dual Policy Optimization for Constrained Reinforcement Learning
and
HT
Violation(T) O CH3/2d√IT log for Multi-objective MDP,
≤ dδ
(cid:18) (cid:18) (cid:19)(cid:19)
hold with probability at least 1 δ. Here C > 0 is an absolute constant that only depends
−
on σ.
Under the low-rank MDP setting, we have similar results.
Corollary 22 Under Assumptions 2 - 9, assuming that θi √d for all i [I] and
k
hk2
≤ ∈
h [H], we have
∈
O ΓH√IdT +H Id3T log TH|M| for Multi-objective MDP,
Regret(T) |A| δ (26)
≤ (O
(cid:0)
H |A|d2T log pTH δ|M|
(cid:0) (cid:1)(cid:1)
for Feasibility Learning,
and (cid:0)p (cid:0) (cid:1)(cid:1)
TH
Violation(T) O H Id3T log |M| for Multi-objective MDP,
≤ |A| δ
(cid:18) (cid:18) (cid:19)(cid:19)
p
hold with probability at least 1 δ. Here = Θ Υ is total number of the model classes.
− |M| | || |
We claim that when f degenerates to a linear function, our results recover the regret of
standard KNR in Kakade et al. (2020). Specifically, our results in regret matches Theorem
3.2Kakade et al.(2020)intermsofH andd,wheretheyaccomplisharegretofO(√H3d2T).
When considering a low-rank MDP with a finite horizon, Uehara et al. (2022) achieves a
regret of O( d3H2 T), which is also consistent with our result for low-rank MDP case.
|A|
We also compare our results of low-rank MDP with existing works such as Yu et al. (2021),
p
which focuses on the study of online Multi-objective MDP under the tabular case. Tabular
MDP can be regarded as a special case of low-rank MDP with a known feature, with the
dimension d = SA. By assuming approachability, Yu et al. (2021) propose an algorithm
with regret of O(Γ IH3S2A/T) and a constraint violation of O( IH3S2A/T) , where
S and A are the cardinality of and , respectively. We claim that our results have a
p S A p
higher-order dependence on d = SA due to the error inherited from MLE and the invoke
of one step back inequality. For a technical understanding, we recommend the readers to
Appendix D.
5 Conclusion
In this paper, we have developed a provably efficient online algorithm, Variational Primal-
Dual Policy Optimization (VPDPO) for constrained constrained convex MDP. KNR and
Low-rank MDP are two examples. The algorithm extends the reward-based RL algorithm
to constrained convex MDP where no explicit reward is needed and incorporates the La-
grangian primal-dual method to transform the constrained optimization into a minimax
problem. To handle the balance between exploration and exploitation, we follow the prin-
ciple of optimism in the face of uncertainty. We prove that that our algorithm enjoys a
O(√T) regret and a O(√T) violation with high probability under standard optimization
assumptions, where T is the total number of episodes taken by the algorithm.
e e
19Li, Liu, Yang, Wang, Wang
Acknowledgments and Disclosure of Funding
Mengdi Wang acknowledges the support by National Science Foundation grants DMS-
1953686, IIS-2107304, CMMI1653435, CPS2312093, ONR grant1006977, Google Research
and C3.AI. Zhaoran Wang acknowledges National Science Foundation (Awards 2048075,
2008827, 2015568, 1934931), Simons Institute (Theory of Reinforcement Learning), Ama-
zon, J.P. Morgan, and Two Sigma for their support. Zhuoran Yang acknowledges Simons
Institute (Theory of Reinforcement Learning) for their support. We would like to thank
YufengZhang,SiruiZheng,andRunzheWuforhelpfuldiscussionsaboutpartsofthispaper.
We would also like to thank the associate editor and the reviewers for many constructive
comments that improved the presentation of the paper.
20Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Appendix A. Additional Notations
We write P(A) as the probability of event A. For a KNR, P( W,s ,a ) denotes the
h h
· |
probability distribution over when the agent is in state s and takes action a , with the
h h
S
transition parametrization W. For two series a and b , we write a . b if
n n≥1 n n≥1 n n
{ } { }
a C b holds for constant C and all sufficient large n.
n n
≤ ·
Appendix B. Proof of Lemma 10
Inthedualupdate,thecostisrelatedtothenon-stationaryvariableθ. Withthesummation
of the value difference bounded, we directly prove the following lemma by adding a O(√T)
scale regret which comes from the employment of online gradient ascent.
Recall that Ψ =1/T T Ψt. We present the following lemma, which can be seen as a
t=1
corollary of Theorem 30.
P
b
Lemma 23 Suppose that Assumptions 3 and 4 holds. For all γ [0,Γ], we have
∈
T f(Ψ) f(Ψ∗)+γ g(Ψ) . BΓ√HT,
− ·
(cid:0) (cid:1)
which further implies b b
T(f(Ψ) f(Ψ∗)) . BΓ√HT, (27)
−
T g(Ψ) . √HT. (28)
b ·
Proof We have the following relations holdsbfor all γ [0,Γ],
∈
f(Ψ) f(Ψ∗)+γ g(Ψ) (29)
− ·
= max α⊤Ψ f∗(α) f(Ψ∗)+β⊤Ψ γg∗(β/γ) .
b α∈B,β∈γ·B b− − −
(cid:8) (cid:9)
Thus, the dual update is equivalentbto implementing online gbradient ascent on h , where
t
h (α,β,γ) = α⊤Ψt f∗(α)+β⊤Ψt γg∗(β/γ).
t
− −
By Theorem 30, wesetthestep size η =2Γ/H√t(or 2Γ/H√T whenT is pre-decided), the
t
constants R = 2Γ andG= 2B√H (to verifytheconditions, notethatg∗ isB√H-Lipschitz,
see Dubovitskii and Milyutin (1965)) to get
T f(Ψ) f(Ψ∗)+γ g(Ψ)
− ·
(cid:20) (cid:21)
(30)
T
b b
Ψt (αt +βt) f∗(αt) γtg∗(βt/γt) Tf(Ψ∗)+CBΓ√HT,
≤ · − − −
t=1
X
where C is an absolute constant. With γt 0 and g(Ψ∗) 0, by the definition of Fenchel
≥ ≤
dual, we have
0 γtg(Ψ∗) βt Ψ∗ γtg∗(βt/γt), f(Ψ∗) αt Ψ∗ f(αt). (31)
≥ ≥ · − ≥ · −
21Li, Liu, Yang, Wang, Wang
Recall that θt = αt+βt. Plugging (31) back to (30), we obtain the following relation holds
for all γ [0,Γ],
∈
T
T f(Ψ) f(Ψ∗)+γ g(Ψ) θt (Ψt Ψ∗)+cBΓ√HT
− · ≤ · −
(cid:20) (cid:21) t=1
X
b b cBΓ√HT,
≤
where the second inequality comes from Lemma 36 and c is an absolute constant. With
γ = 0 we obtain (27). With γ = Γ, we have
T f(Ψ) f(Ψ∗)+Γ g(Ψ) . Γ√HT.
− ·
(cid:20) (cid:21)
b b
And with Theorem 33 we obtain (28). Therefore, we conclude the proof.
Appendix C. Proof of Theorem 14
WefirstshowthatWt t withhighprobabilityifRt isproperlychosen,whichensuresthat
∈ C
Algorithm 2 induces sufficient optimism. The following lemma is frequently used to provide
a sufficient trustworthy radius for a confidence set and is first proved by Kakade et al.
(2020). We provide its proof for completeness.
Lemma 24 (Confidence Ball) For all t [T], we set t as the event that W∗ falls in
∈ Ecb
t, i.e.,
C
t = W¯ t W⋆ Λt 1/2 2 Rt ,
Ecb − 2 ≤
(cid:26) (cid:27)
(cid:13)(cid:0) (cid:1)(cid:0) (cid:1) (cid:13)
(cid:13) (cid:13)
and as the event that all Wt falls in t, and = T t . Let
Ecb C Ecb ∩t=1Ecb
Rt = 2λ W⋆ 2+8σ2 dlog(5)+2log(t)+log(4)+log(det(Λt)/det(Λ0)/δ) ,
k k2
(cid:0) (cid:1)
We have
∞ ∞
2
P(¯t ) = P W¯ t W⋆ Λt 1/2 > Rt δ/2.
Ecb − 2 ≤
Xt=0 Xt=0 (cid:18)(cid:13)
(cid:13)(cid:0) (cid:1)(cid:0) (cid:1)
(cid:13)
(cid:13)
(cid:19)
(cid:13) (cid:13)
Proof The center of the confidence ball, W¯ t, is the minimizer of the ridge regression
objective, and its closed-form expression is
t H
W¯ t = sτ φ(sτ,aτ)⊤(Λt)−1,
h+1 h h
τ=1h=1
XX
22Primal-Dual Policy Optimization for Constrained Reinforcement Learning
where Λt = λI + t H φ(sτ,aτ)⊤φ(sτ,aτ)⊤. Since sτ = W⋆φ(sτ,aτ) + ǫτ with
τ=1 h=1 h h h h h+1 h h h
ǫτ 0,σ2 , we have
h ∼ N I P P
(cid:0) (cid:1)t H
W¯ t W⋆ = sτ φ(sτ,aτ)⊤(Λt)−1 W⋆
− h+1 h h −
τ=1h=1
XX
t H
= (W⋆φ(sτ,aτ)+ǫτ)φ(sτ,aτ)⊤ Λt −1 W⋆
h h h h h −
τ=1h=1
XX (cid:0) (cid:1)
t H t H
= W⋆ φ(sτ,aτ)φ(sτ,aτ)⊤ (Λt)−1 W⋆+ ǫτφ(sτ,aτ)⊤(Λt)−1
h h h h − h h h
(cid:18)τ=1h=1 (cid:19) τ=1h=1
XX XX
t H
= λW⋆(Λt)−1+ ǫτφ(sτ,aτ)⊤(Λt)−1.
− h h h
τ=1h=1
XX
For any 0 < δ < 1, using Lemma 38, it holds with probability at least 1 δ ,
t t
−
t H
(W¯ t W⋆)(Λt)1/2 λW⋆(Λt)−1/2 + ǫτφ(sτ,aτ)⊤(Λt)−1/2
− ≤ h h h
(cid:13) (cid:13)2 (cid:13) (cid:13)2 (cid:13)τ=1h=1 (cid:13)2
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)XX (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)√λ W⋆ 2+σ(cid:13) 8dl(cid:13)og(5)+8log det(Λt)det(Λ0)(cid:13)−1/δ
t
,
≤ k k
q
wherethefirstinequalityfollowsfromthetriangleinequality. T(cid:0) herefore,weobtainP (cid:1)t
δ . We seek to bound ∞ P(¯t ). Note that at t = 0 we have initialized 0 to contaiE ncb W≤ ⋆,
t t=0 Ecb C (cid:0) (cid:1)
we have P(¯0) = 0. For t 1, let us assign failure probability δ = (3δ/π2)/t2 for the t-th
Ecb P ≥ t
event.We obtain
∞ ∞
P(¯t ) (δ/t2)(3/π2) = δ/2.
Ecb ≤
t=1 t=1
X X
Therefore, we conclude the proof of Lemma 24.
For all t [T], we set t as the event that W∗ falls in t, i.e.,
∈ Ecb C
t = W¯ t W⋆ Λt 1/2 2 Rt ,
Ecb − 2 ≤
(cid:26) (cid:27)
(cid:13)(cid:0) (cid:1)(cid:0) (cid:1) (cid:13)
and as the event that all Wt fa(cid:13)lls in t, i.e., = (cid:13)T t . We prove in Lemma 24 that
∞Ec Pb
(¯t ) δ/2, where ¯t denotes
thC
e
compE lec mb en∩ tt o= f1Ec tb
. The following lemma shows
t=1 Ecb ≤ Ecb Ecb
that by efficiently implementing the principle of optimism in Algorithm 2, the summation
P
of the expected discrepancy of two projected kernel features is bounded. The main idea is
to cast the projected kernel embedding to an initial state value function. Then by the value
iteration implemented in Algorithm 2, we give a general bound for regret and violation in
O(√T) scales.
Lemma 25 (Optimism for KNR) Suppose that Assumption 3 holds. For Algorithms 1
and 2, the following inequality holds with probability at least 1 δ,
−
T
E θt (Ψt Ψ∗) (1+Γ)√H. (32)
· − ≤
(cid:20)t=1 (cid:21)
X
23Li, Liu, Yang, Wang, Wang
Proof If Wt falls in t for all t, it holds that T θt (Ψt Ψ∗) 0 by optimism induced
C t=1 · − ≤
by line 9 in Algorithm 2. We condition on the event t and the proof is done.
P Ecb
The final step is to bound the estimation error of the visitation
f(Ψπb
) f(Ψ) and
g(Ψπb
)
g(Ψ). With f and g being 1-Lipschitz, it suffices to bound T
Ψπb
Ψ
−
.
−
2
k − k
b
Lemma 26 (Bound for estimation error) Suppose that Assumptions 5-3 and 8 hold.
b b
For Algorithms 1 and 2, we have
HT
T Ψπb Ψ CBHd√T log (33)
2
k − k ≤ dδ
(cid:18) (cid:19)
holds with probability at least 1 δ,bhere C is an absolute constant only depends on σ.
−
Proof For all x =(x ) RdH with x 1, we can consider T (Ψπb Ψ) x as the
h h∈[H] ∈ k k2 ≤ t=1 − ·
value difference of two processes, with cost at stage h defined as c (s ,a )= ψ(s ,a ) x .
h h h h h h
P ·
For simplicity, we denote x (Ψπt Ψt) = Vπt Vt. In the following analysis wbe condition
· − 1 − 1
on the event , further estimate the value difference. With Lemma 15, we have
cb
E
T T H
Vπt(s) Vt(s)) . B√HE W⋆ Wt φ(st,at) , (34)
1 − 1 πt − h h 2 Ht
t=1 t=1 (cid:20)h=1 (cid:21)
X(cid:0) X X(cid:13)(cid:0) (cid:1) (cid:13) (cid:12)
(cid:13) (cid:13) (cid:12)
Here is the history before episode t, and the inequality holds by Lemma 15. For
t t∈[T]
{H }
W∗ t, we have
∈ C
Wt W∗ φ(st,at) Wt W∗ (Λt)1/2 (Λt)−1/2φ(st,at)
− h h 2 ≤ − 2 h h 2
(cid:13) (cid:13)(cid:0) c (cid:1) (cid:13) (cid:13) ≤ (cid:13) (cid:13)(cid:0) cWt −W¯(cid:1)t Λt 1/(cid:13) (cid:13)2(cid:13) (cid:13) 2+ W¯ t −W∗ ((cid:13) (cid:13)Λt)1/2 2 φ(st h,at h) (Λt)−1
(cid:18) (cid:19)
≤
2√(cid:13) (cid:13)(cid:0) Rct φ(st h,a(cid:1)t h(cid:0) ) (Λ(cid:1) t)−1(cid:13) (cid:13). (cid:13) (cid:13)(cid:0) (cid:1) (cid:13) (cid:13) (cid:13) (cid:13) ((cid:13) (cid:13)35)
(cid:13) (cid:13)
Summing up (35) over h [H], w(cid:13)e obtain(cid:13)
∈
H H
W⋆ −Wt φ(st h,at h)
2 ≤
2√Rt kφ(st h,at h) k(Λt)−1. (36)
h=1 h=1
X(cid:13)(cid:0) (cid:1) (cid:13) X
(cid:13) (cid:13)
Plugging (36) back to (34), we have the following holds with probability at least 1 δ,
−
T T H
V 1πt(s) −V 1t(s) . B√H E √RT kφ(st h,at h) k(Λt)−1 Ht
t=1 t=1 (cid:20) h=1 (cid:12) (cid:21)
X(cid:0) (cid:1) X X (cid:12)
T H (cid:12)
. B sHσ2dlog H dδT E kφ(st h,a(cid:12) t h) k(Λt)−1 Ht
(cid:18) (cid:19)t=1 (cid:20)h=1 (cid:12) (cid:21)
X X (cid:12)
T H (cid:12)T
HT (cid:12)
. B sHσ2dlog
dδ
kφ(st h,at h) k(Λt)−1 + M tT,
(cid:18) (cid:19)t=1h=1 t=1
XX X
T H 1/2
HT
CB THdlog φ(st,at) 2 +H 2T log(4/δ),
≤ s dδ k h h k(Λt)−1
(cid:18) (cid:19)(cid:18)t=1h=1 (cid:19)
XX p
24Primal-Dual Policy Optimization for Constrained Reinforcement Learning
holds with probability at least 1 δ. Here C is some absolute constant that only depends
−
on σ. The second inequality comes from the fact that Rt is non-decreasing, and
RT = max 2σ2,2 +8σ2 dlog(5)+2log(T)+log(4)+log det(ΛT)det(Λ0)−1 /δ
{ }
(cid:18) (cid:19)
(cid:0) (cid:1)
C′σ2 d+log(T)+log det(ΛT)det Λ0 −1 /δ . σ2(dlog(TH/dδ))
≤
(cid:18) (cid:19)
(cid:0) (cid:0) (cid:1) (cid:1)
and λ = max σ2,1 . The third inequality decompose the expectation term into a elliptical
{ }
potential summation and a martingale difference series. The last inequality comes from the
martingale difference is bounded by H, and with Hoeffding’s inequality, we have
T s2
P( MT s) 2exp( − ),
| t | ≥ ≤ 2TH2
t=1
X
set s = H 2T log(4/p) and we prove that T MT H T log(4/p) with probability at
t=1 t ≤
least 1 δ/2. Since we condition on , which holds with probability at least 1 δ/2,
− p Ecb P p −
the inequality holds with probability at least 1 δ. Next, we bound the elliptical potential
−
term. By Lemma 42, we have
T H
TH
φ(st,at) 2 2Hlog det(ΛT)det(Λ0)−1 . dHlog( ),
k h h k(Λt)−1 ≤ d
t=1h=1
XX (cid:0) (cid:1)
where the third inequality comes from 41. Combining the results above we have
T
HT
Vπt(s) Vt(s) . CBHd√T log ,
1 − 1 dδ
t=1 (cid:18) (cid:19)
X(cid:0) (cid:1)
hereC is anabsolute constant thatonly relates to σ. Sincetheargumentabove holdsfor all
xwith x 1,setx = (Ψπt Ψt)/ Ψπt Ψt ,andweconcludetheproofofLemma26.
2 2
k k ≤ − k − k
Appendix D. Proof for Theorem 17
In this section we give a detailed proof for Theorem 17. The main tool is the MLE funda-
mental theorem and Bernstein’s inequality for martingales.
Proof First, we prove that the choice of the confidence set t is fully efficient, i.e. ∗ t
C P ∈ C
with high probability.
Lemma 27 With probability at least 1 δ, we have the true underlying transition kernel
−
∗ : R lies in the confidence set t for all t [T] and h [H], i.e.,
Ph S ×S ×A → C ∈ ∈
log(TH Θ Υ /δ)
E
D
ht[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1]
≤
c |
t
|| | ,
where E [f(s,a)] takes the average ofbf on the dataset .
D
D
25Li, Liu, Yang, Wang, Wang
Proof By the construction of t in Algorithm 3, we have st π and at U( ). Recall
Dh h ∼ t h ∼ A
that
1
ρt(s ,a )= d (s )u(a ),
h h h t 1 πi,h h h
− i∈[t−1]
X
Therefore, E [ ∗( s ,ab ) t( s ,a ) 2] is a empirical realization of the visitation
D ht kPh ·| h h − Ph ·| h h k1
measure for (s ,a ) under the Markov policy ρt. For notation simplicity, for each (h,t)
h h
∈
[H] [T], define to be to thbe σ-algebra generated by the trajectories,
t,h
× F
b
= σ( (sτ,aτ) (st,at) ),
Ft,h { i i }(i,τ)∈[H]×[t−1] ∪{ i i }i∈[h−1]
since πτ = πi(s1,a1,...,sτ−1) and is measurable with respect to , we have
1 1 H Ft,h
t(E D ht[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1] −E ρbt h[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1])
= ∗( sτ,aτ) t( sτ,aτ) 2 E ∗( s ,a ) t( s ,a ) 2
kPh ·| h bh −Ph ·| h h k1 − sh∼πτ,ah∼U(A) kbPh ·| h h −Ph ·| h h k1
τ∈[t]
X
b b
being a martingale process with respect to the filtration for all (h,t)
t,h (h,t)∈[H]×[T]
{F } ∈
[H] [T]. Therefore, by applying Freedman inequality (Lemma 43), with probability at
×
least 1 δ that
−
E D ht[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1] −E ρbt h[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1]
(cid:12) (cid:12) 2Var ρbt
h
kPbh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1 log(2TH/bδ)
+
log(2TH(cid:12) (cid:12)/δ)
≤ s t 3t
(cid:2) (cid:3)
b
8E ρbt
h
kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1 log(2TH/δ)
+
log(2TH/δ)
,
≤ s t 3t
(cid:2) (cid:3)
b
wherethesecondinequality follows from ∗( s ,a ) t( s ,a ) 2 4. Recall thatwith
kPh ·| h h −Ph ·| h h k1 ≤
Lemma 39, we have
b
log(TH Θ Υ /δ)
E ρbt h kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1 ≤ c | t || | .
(cid:2)
Therefore, we have b
log(TH Θ Υ /δ)
E D ht[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1] −E ρbt h[ kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1] ≤ c | t || | .
(cid:12) (cid:12)
S(cid:12)umming this and Dband we conclude the proof. b (cid:12)
The next lemma ensures that as we explore and shrink the radius of the confidence set, the
statistical distances between the MLE estimation and all transitions in the confidence set
uniformly decrease in a O(1/t) manner.
Lemma 28 For all = t and all t [T], with probability at least 1 δ, we
P
e{Ph }h∈[H]
∈ C ∈ −
have
log(TH Θ Υ /δ)
E ρbt[ kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1]
≤
c′ |
t
|| | .
26Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Proof By the construction of t, we have the following inequality holds for all =
C P
in t with high probability,
h h∈[H]
{P } C
E [ ( s ,a ) ∗( s ,a ) 2]
D ht kPh ·| h h −Ph ·| h h k1
2(E [ ( s ,a ) t( s ,a ) 2]+E [ t( s ,a ) ∗( s ,a ) 2])
≤ D ht kPh ·| h h −Ph ·| h h k1 D ht kPh ·| h h −Ph ·| h h k1
log(TH Θ Υ /δ)
2c | || | ,
≤ t
The first inequality comes from (a+b)2 2(a2+b2). Define
≤
A( Ph) =E ρbt h[ kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1] −E D ht[ kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1].
We have
log(TH Θ Υ /δ)
E ρbt h[ kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1]
≤
A( Ph)+2c |
t
|| | ,
for all (h,t) [H] [T]. Applying Freedman’s inequality again, for any t and
h h∈[H]
∈ × {P } ∈C
t [T], we have with probability 1 δ that
∈ −
A( ) c 1Var ρbt kPh∗( ·|s h,a h) −Pht( ·|s h,a h) k2 1 log(TH |Θ ||Υ |/δ) + c 2log(TH |Θ ||Υ |/δ)
Ph ≤ s t t
(cid:2) (cid:3)
b
c 1E ρbt
h
kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k4 1 log(TH |Θ ||Υ |/δ)
+
c 2log(TH |Θ ||Υ |/δ)
≤ s t t
(cid:2) (cid:3)
4c 1E ρbt
h
kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1 log(H |Θ ||Υ |/δ)
+
c 2log(H |Θ ||Υ |/δ)
,
≤ s t t
(cid:2) (cid:3)
where the first inequality comes from Bernstein’s inequality, and the second inequality
comes from the fact that ( s,a) ′( s,a) 2 4 for two probability distributions.
kP ·| − P ·| k1 ≤
Denote ξ = log(Θ Υ TH/δ)/t and taking square in both side of the (D), we have
| || |
2
c(A( )+ξ)log(TH Θ Υ /δ) clog(Θ Υ /δ)
A2( ). Ph | || | + | || |
h
P t t
r !
2
(A( )+ξ)log(TH Θ Υ /δ) cln(TH Θ Υ /δ)
h
. P | || | + | || |
t t
(cid:26) (cid:27)
(A( )+ξ)log(TH Θ Υ /δ) c log(TH Θ Υ /δ)
h 2
. P | || | + | || | .
t t
(A( )+1/tlog(TH Θ Υ /δ))log(TH Θ Υ /δ)
h
. P | || | | || | .
t
Then, we have
A2( ) B A( ) B 0, B = clog(TH Θ Υ /δ)/t, B = c(1/t)2log(TH Θ Υ /δ)2
h 1 h 2 1 2
P − P − ≤ | || | | || |
holds for all t for all t [T]. This concludes
h h∈[H]
{P } ∈ C ∈
B + B2+4B log(TH Θ Υ /δ)
0 A( ) 1 1 2 c B + B c | || | . ξ.
h 1 2
≤ P ≤ 2 ≤ ≤ t
p
(cid:16) p (cid:17)
27Li, Liu, Yang, Wang, Wang
Thus, by using the above A( ) . ξ t and , with probability 1 δ, we have
P P ∈ C −
(cid:0) (cid:1)
E ρbt h kPh( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1 ≤ A(P)+cξ . ξ, {Ph }h∈[H] ∈ Ct.
(cid:2) (cid:3)
Toconcludeourproof,thefinalstepistoboundT Ψ Ψπb = T (Ψt Ψπt ) . Tothis
k − k2 k t=1 − k2
end, we still consider to find an uniform upper bound for T (Ψt Ψπt ) θ , θ = (θ )
t=1 P− · h h∈[H]
with θ 1. As in the case of KNR, we define b
2
k k ≤ P
ct(s,a) = ψ (s,a) θ ,
h h · h
H
Ψt θ = E ct(s ,a ) = Vt,
· πt,Pt h h h 1
(cid:20)h=1 (cid:21)
X
H
Ψπt ·θ = E πt,P∗ r ht(s h,a h) = V 1πt ,
(cid:20)h=1 (cid:21)
X
With standard notations in reinforcement learning, we can definevalue function V (s,a) for
h
all stage h [H]. Using the value-decomposition lemma, we decompose the value difference
Vt Vπt , ∈
1 − 1
T T
(Ψt Ψπt ) θ = Vt Vπt
− · 1 − 1
t=1 t=1
X X
H T
= E P∗Vπt (s ,a ) PtVπt (s ,a ) (37)
πt h h+1 h h − h h+1 h h
h=1t=1
XX (cid:2) (cid:3)
H T
B√HE t( s ,a ) ∗( s ,a ) ,
≤ πt kPh ·| h h −Ph ·| h h k1
h=1t=1
XX (cid:2) (cid:3)
here the second equation comes from the value difference lemma, and the third inequality
comes from the fact that Vπ B√H, since θ 1 and ψ (s,a) B. The next
k 1 k∞ ≤ k k2 ≤ k h k2 ≤
lemma shows that we can upper bound E πt[H(s h,a h)] using E πt kφ∗ h(s h,a h) kΣ− ρt1
,φ∗
h−1
once
we can upper bound E ρbt[H2(s h,a h)].
Lemma 29 (One step back inequality) Take any H R such that H
∞
∈ S ×A → k k ≤
B. Then,
E π[H(s h,a h)]
≤
E π kφ⋆ h−1(s h−1,a h−1) kΣ−
ρt
h1
−1,φ⋆
h−1
qt |A|E ρbt h[H2(s,a)]+λdB2,
where Σ = tE φ⋆(s ,a )φ⋆(s ,a )⊤ +λI. Note that here the parameter λ and the
ρt,φ⋆ ρt h h h h h h
h h h
matrix Σ ρt,φ∗
h
doesn’t (cid:2)occur in the actual im (cid:3)plementation.
28Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Proof First, we have
E [H(s ,a )] = E [H(s ,a )]
π h h (sh−1,ah−1)∼π,sh∼P(sh−1,ah−1),ah∼π h h
= E φ∗ (s ,a )⊤ µ∗ (s )π (a s )H(s ,a )dν
π h−1 h−1 h−1 h−1 h h h | h h h
(cid:20) Z Xah (cid:21)
≤
E π (cid:20)kφ∗ h−1(s h−1,a h−1) kΣ−
ρt
h1
,φ⋆
h−1(cid:13) (cid:13)Z Xah
µ∗ h−1(s h)π h(a h |s h)H(s h,a h)dν
(cid:13) (cid:13)Σ ρt h,φ⋆
h−1(cid:21),
(cid:13) (cid:13)
(cid:13) (cid:13)
where the third inequality comes from Cauchy’s inequality. Here, we have
2
µ⋆ (s)π (a s )H(s ,a )dν(s)
(cid:13) h−1 h h | h h h (cid:13)
(cid:13) (cid:13)Z Xah (cid:13) (cid:13)Σ ρt h−1,φ⋆
h−1
(cid:13) (cid:13)
(cid:13) (cid:13) ⊤
µ⋆ (s )π (a s )H(s ,a )dν(s) tE φ⋆ (s ,a ) φ⋆ (s ,a ) ⊤ +λI µ⋆
≤
(
h−1 h h h | h h h
)
ρt h−1 h−1 h−1 h−1 h−1 h h
(
h−
Z Xah n h
(cid:8) (cid:9)
i o Z Xa
2
tE µ⋆ (s )⊤φ⋆ (s ,a )π (a s )H(s ,a )dν(s) +λdB2
≤ ρt h−1
"
h−1 h h−1 h−1 h−1 h h | h h h
#

 Z Xa 
t E H2(s ,a ) +λdB2,
≤
(sh− 1,ah−1)∼ρt h−1,sh∼P⋆(sh−1,ah−1),ah∼π(s) h h 
n o
(cid:2) (cid:3)
where the last inequality comes from Jensen’s inequality. Further, we have that
E H2(s ,a ) E H2(s ,a )
(sh−1,ah−1)∼ρt,sh∼P⋆(sh−1,ah−1),ah∼π(s) h h
≤ |A|
(sh−1,ah−1)∼ρt,sh∼P⋆(sh−1,ah−1),ah∼U(A) h h
(cid:2) (cid:3) = |A|E ρbt h[H2(s h,a h)] (cid:2) (cid:3)
which concludes the proof.
We then condition on the event
log(TH Υ Θ )
E ρbt h[ kPht( ·|s h,a h) −Ph∗( ·|s h,a h) k2 1]
≤
c t| || | , ∀(h,t)
∈
[H] ×[T],
which holds with probability at least 1 δ, and use Lemma 29 on (37) by setting π = πt
−
for t( s ,a ) ∗( s ,a ) , we have
kPh ·| h h −Ph ·| h h k1
H T
E t( s ,a ) ∗( s ,a )
πt kPh ·| h h −Ph ·| h h k1
h=1t=1
XX (cid:2) (cid:3)
H T
≤
E πt kφ∗ h(s h,a h) kΣ− ρt1
,φ∗
t |A|E ρbt h[ kPht −Ph∗ k2 1]+4λd
Xh=1 Xt=1 h hq
H T
. E πt kφ∗ h(s h,a h) kΣ− ρt1
,φ∗
|A|log(TH |Θ ||Υ |/δ)+λd,
h=1t=1 h h
XX p
29Li, Liu, Yang, Wang, Wang
here the first inequality comes from the one-step back inequality and the fact that every
term in the summation is positive, the second inequality comes from our condition event.
We also have
log(TH Θ Υ /δ)+λd . λdlog(TH Θ Υ /δ) = ξ ,
T
|A| | || | |A| | || |
p p
therefore
H T
E t( s ,a ) ∗( s ,a )
πt kPh ·| h h −Ph ·| h h k1
h=1t=1
XX (cid:2) (cid:3)
H T
. ξ T E πt kφ∗ h(s h,a h) kΣ− ρt1
,φ∗
h=1 t=1 h h
X (cid:0)X (cid:1)
H T
ξ T E φ∗(s ,a )⊤Σ−1 φ∗(s ,a ) ,
≤ h=1 T ·v u t=1 πt (cid:20) h h h ρt h,φ∗ h h h h (cid:21)
X u X
t
where the second inequality comes from Jensen’s inequality. By Lemma 40 and Lemma 41,
we have
T T
E φ∗(s ,a )⊤Σ−1 φ∗(s ,a ) = Tr Σ−1 E φ∗(s ,a )φ∗(s ,a )⊤
πt h h h ρt h,φ∗ h h h h ρt h,φ∗ h · πt h h h h h h
t=1 (cid:20) (cid:21) t=1 (cid:18) (cid:19)
X X (cid:2) (cid:3)
≤
2 logdet Σ ρT h,φ∗
h
−2logdet λI
(cid:18) (cid:19)
(cid:0)T (cid:1) (cid:0) (cid:1)
dlog 1+
≤ dλ
(cid:18) (cid:19)
holds for all h [H]. By then we have
∈
H T
T
E t( s ,a ) ∗( s ,a ) T log 1+ 1/2dλ1/2log(TH Θ Υ /δ),
πt kPh ·| h h −Ph ·| h h k1 ≤ s dλ |A| | || |
h=1t=1 (cid:18) (cid:19)
XX (cid:2) (cid:3)
combinewith(37)andsetλ = 1, θ = (Ψ
Ψπb
)/ Ψ
Ψπb
, weconcludetheproofof Lemma
2
− k − k
20
Combine Lemma 12 and 20 we finibsh the probof of Theorem 17.
Appendix E. Lemmas for Optimization
E.1 Online learning
Online learning involves two players: the adversary and the player. The online learning
protocol is shown in Algorithm 4.
30Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Algorithm 4 Protocol of Online Learning
1: for t = 1,...,T do
2: The player chooses an action x .
t
3: The adversary picks a function f .
t
4: The player obtains reward f (x ).
t t
5: The player learns via f .
t
6: end for
Note that there is no assumption on how the adversary will pick the function f , and it may
t
be adversarially chosen. The player aims to minimize the regret:
T T
Regret = max f (x) f (x ), (38)
t t t
x −
t=1 t=1
X X
whichmeasuresthequality oftheplayer’s strategy x ,...,x comparedwiththesinglebest
1 T
decision in hindsight.
Projected Subgradient Method. Theprojectedsubgradientmethodisaparticular case
of mirror descent/ascent with Euclidean distance. Applying this method to online learning
produces a regret bound of the order O(√T).
Supposethat the actions x are required to be contained in some convex set , i.e., x .
t t
X ∈ X
Denote a subgradient of f at x by g ∂f (x ), G and R are two constants such that
t t t t t
∈
max x y R and max ∂f (x ) G. We set the step length η at the
x,y∈X 2 t∈[T] t t 2 t
k − k ≤ k k ≤
t-th iteration to R/G√t if we do not know the number of iterations T in advance and to
R/G√T if we have the knowledge of T. The latter case will leads to an upper bound with
a smaller constant multiplicative factor. With these notations, the update rule of projected
subgradient method can be expressed as
x argmax f (x )+ η g ,x x x x 2/2 .
t+1 ← t t h t t − t i−k − t k2
x∈X
(cid:8) (cid:9)
We describe the complete method in Algorithm 5.
Algorithm 5 projected subgradient method
1: Arbitrarily initialize x .
1
∈ X
2: for t = 1,...,T 1 do
−
3: Update x argmax f (x )+ η g ,x x x x 2/2
t+1 ← x∈X t t h t t − t i−k − t k2
4: end for
(cid:8) (cid:9)
By this method, the regret is guaranteed to increase sublinearly as stated in the following
theorem.
Theorem 30 Using projected subgradient method mentioned in Algorithm 5, it holds that
for all x in the convex set we have
X
T T
f (x) f (x ) CRG√T,
t t t
− ≤
t=1 t=1
X X
where C is an absolute constant.
31Li, Liu, Yang, Wang, Wang
Proof See Zinkevich (2003) for a detailed proof. Note that the choice of x is irrelevant in
the proof.
E.2 Constrained Optimization
In this subsection we consider a general constrained optimization and discuss its properties.
We consider
f = min f(x): g(x) 0,Ax+b = 0 , (39)
opt
x∈X{ ≤ }
where and f,g : R ( , ) are convex real-valued functions, A Rp×n,b Rp. We
→ −∞ ∞ ∈ ∈
define a value function associated with (39),
v(u,t) = min f(x) :g(x) u,Ax+b = t .
x∈X{ ≤ }
Furthermore, we define the dual problem to (39). The dual function is
q(λ,γ) = min L(x,λ,γ) = f(x)+λTg(x)+γT(Ax+b) ,
x∈X
(cid:8) (cid:9)
where λ Rm,γ Rp. The corresponding dual problem is
∈ + ∈
q = max q(λ,γ) : (λ,γ) dom( q) .
opt
λ∈Rm,γ∈Rp{ ∈ − }
+
Where dom( q) = (λ,γ) Rm,γ Rp :q(λ,γ) > . Furthermore, we denote an op-
− ∈ + ∈ −∞
timal solution of (E.2) by λ∗,γ∗.
(cid:8) (cid:9)
We make the following assumption which will be verified to hold. The assumption
implies strong duality, i.e., q =f .
opt opt
Assumption 31 The optimal value of (39) is finite and exists a Slater point x such that
g(x < 0 and exists a point x ri(X) satisfying Ax + b = 0, where ri(X) is the relative
∈
interior of X.
b b
The following theorem is proved in Beck (2017).
Theorem 32 The dual variable (λ∗,γ∗) is an optimal solution of (39) if and only if
(λ∗,γ∗) ∂v(0,0),
− ∈
where ∂f(x) denotes the set of all sub-gradients of f at x.
Proof See Theorem 3.59, Beck (2017).
Using this result we arrive at the following theorem, which is a variant of Beck (2017) ,
Theorem 3.60.
32Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Theorem 33 Let λ∗ be an optimal solution of the dual (39) and assume that 2 λ∗ ρ.
1
k k ≤
Let x satisfy Ax+b = 0 and
f(x) f +ρ [g(x)] δ
e e opt + ∞
− k k ≤
Then we have
e e
δ
[g(x)] .
+ ∞
k k ≤ ρ
Proof Let e
v(u,t) = min f(x) :g(x) u,Ax+b = t .
x∈X{ ≤ }
Since ( λ∗,γ∗) is an optimal solution of the dual problem it follows by Theorem 32 that
−
( λ∗,γ∗) ∂v(0,0). Therefore, for any (u,0) dom(v),
− ∈ ∈
v(u,0) v(0,0) λ∗,u . (40)
− ≥ h− i
Set u= u= [g(x)] . Since u 0, we have
+
≥
v(u,0) v(0,0) = f f(x).
e e opt
≤ ≤
Thus, (40) implies that
e e
f(x) f λ∗,u . (41)
opt
− ≥ h− i
Thus, we obtain
e e
(ρ λ∗ ) u = λ∗ u +ρ u
1 ∞ 1 ∞ ∞
−k k k k −k k k k k k
λ∗,u +ρ u
∞
≤ h− i k k
e e e
= f(x) f +ρ u δ,
opt ∞
− k k ≤
e e
where the last relation follows from (41). Rearranging the terms and using the assumption
e
2 λ∗ ρ, we obtain
1
k k ≤
δ 2
[g(x)] = u δ.
k + k∞ k k∞ ≤ ρ λ∗ ≤ ρ
1
−k k
Therefore, we conclude the preoof of Theorem 32.
For the solution of the dual function, the following lemma is an adjustment of Beck
(2017).
Theorem 34 Let x X be a point satisfying g(x¯) < 0 and Ax + b = 0. Then, for any
λ,γ λ Rm,γ R∈p : q(λ,γ) M , we have
∈ { ∈ + ∈ + ≥ }
f(x¯) M
λ − .
1
k k ≤ min g (x¯)
j∈[m] j
{− }
33Li, Liu, Yang, Wang, Wang
Proof Let
S = λ Rm,γ Rp : q(λ,γ) M .
M { ∈ + ∈ + ≥ }
By the definition of S , for any λ,γ S we have
M M
∈
M q(λ,γ)
≤
= min f(x)+λTg(x)+γT(Ax+b)
x∈X{ }
f(x)+λTg(x)+γT(Ax+b)
≤
m
= f(x)+ λ g (x).
j j
j=1
X
Therefore, we obtain
m
λ g (x) f(x) M,
j j
− ≤ −
j=1
X
which implies that for any (λ,γ) S ,
M
∈
m
f(x) M
λ = λ − .
j 1
k k ≤ min g (x)
j∈[m] j
j=1 {− }
X
Therefore, we conclude the proof of Theorem 34.
A simple corollary gives an estimation of the optimal dual solution λ∗.
Corollary 35 Let x X be a point satisfying g(x¯) < 0 and Ax + b = 0, and λ∗ be an
∈
optimal dual solution. Then, it holds that
f(x¯) M
λ∗ − .
1
k k ≤ min g (x¯)
j∈[m] j
{− }
Proof Since (λ∗,γ∗) S be an optimal solution of the dual problem equation 39, we
∈
fopt
finish the proof by Theorem 34.
E.3 Proof of Proposition 4
Proof To prove the convexity of (4), it suffices to show that is convex. We allow some
V
initial randomizing mechanisms such that the policy π not only rely on h, but also
h h∈[H]
{ }
depends on a randomizing mechanism. We may have a set of policies and a distribution
U
q ∆( ). Then the mixed policy π of , is defined such that we choose some policy π
∈ U U ∈ U
using q and then the agent proceeds executing with only that policy (Altman, 1999). We
have the following equality, b
Ψπb = E [Ψπ],
q
where the expectation is taken with respect to the underlying distribution q and all policy
π . When q is set as the uniform distribution on set π , we have
k k∈[K]
∈ U { }
K
1
Ψπb = Ψπk.
K
k=1
X
34Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Since
Ψπb
with our definition, is a convex set. The optima of (4) over the mixed
∈ V V
policy will remain the same, and is proved to be a convex set. The feasible set for (4) is
V
thus convex and the problem is indeed a convex optimization.
Appendix F. Auxiliary Results
The difference of value functions between two MDPs has the following general decomposi-
tion, which is rather useful in our analysis.
Lemma 36 (Value Difference Lemma) Consider two MDPs , , 1 H , r1 H
S A {Ph}h=1 { h}h=1
and , , 2 H , r2 H and a given policy π = π . Their corresponding value
functiS onA s in{P thh e}h h= -t1 h{ hoh r} izh o= n1 are Vπ and Vπ′ respective{ ly.h } Th∈ h[ eH n] for(cid:0) all h [H] the following(cid:1)
(cid:0) (cid:1) h h ∈
relation holds,
H
Vπ(s) Vπ′ (s) = E [ (r (s ,a ) r′ (s ,a )) s = s] (42)
h − h π,P i i i − i i i | h
i=h
X
H
+E π,P′[ (P iV iπ +1(s i,a i) −P′ iV iπ +1(s i,a i))
|
s
h
= s] (43)
i=h
X
Proof See Lemma E.15 in Dann et al. (2017) for details.
We introduce the following lemma, which gives a self-normalized bound for vector value
martingales(Abbasi-Yadkori et al., 2011).
Lemma 37 (Self-Normalized Bound for Vector-Valued Martingales) Let ε ∞ be
{ i }i=1
a real-valued stochastic process with corresponding filtration ∞ such that ε is mea-
{Fi }i=1 i Fi
surable, E[ε ] = 0, and ε is conditionally σ-sub-Gaussian with σ R+. Let X ∞
i | Fi−1 i ∈ { i }i=1
be a stochastic process with X (some Hilbert space) and X being -measurable. As-
i i t
∈ H F
sume that a linear operator V : is positive definite, i.e., x⊤Vx > 0 for any x .
For any t, define the linear operaH to→ r VH =V + t X X⊤ (here xx⊤ denotes outer-pro∈ duH ct
t i=1 i i
in ). With probability at least 1 δ, we have for all t 1
H − P ≥
t 2 det(V )1/2det(V)−1/2
X ε 2σ2log t .
i i
(cid:13) (cid:13)Xi=1 (cid:13) (cid:13)V
t−1 ≤
(cid:18)
δ
(cid:19)
(cid:13) (cid:13)
Proof For a detaile(cid:13)d proof, s(cid:13)ee Abbasi-Yadkori et al. (2011).
Lemma 38 can be generalized to the case of matrix-valued martingales.
Lemma 38 (Self-Normalized Bound for Matrix-Valued Martingales) Let ε ∞ be
a d-dimensional vector-valued stochastic process with corresponding filtration
∞{ i } si u= c1
h
{Fi }i=1
that ε is measurable, E[ε ] = 0, and ε is conditionally σ-sub-Gaussian with
i i i i−1 i
σ Rd LetF X ∞ be a stochas| tiF c process with X (some Hilbert space) and X being
∈ { i }i=1 i ∈ H i
measurable. Assume that a linear operator V : is positive definite. For any t,
t
F H → H
35Li, Liu, Yang, Wang, Wang
define the linear operator V = V + t X X⊤ Then, with probability at least 1 δ, we
t i=1 i i −
have for all t, we have:
P
t 2 det(V )1/2det(V)−1/2
ǫ X⊤V−1/2 8σ2dlog(5)+8σ2log t .
i i t ≤ δ
(cid:13)i=1 (cid:13)2 (cid:18) (cid:19)
(cid:13)X (cid:13)
Proof Den(cid:13) (cid:13)ote S = t i=1ǫ i(cid:13) (cid:13)X i⊤. Let us form an ǫ-net, in ℓ
2
distance,
C
over the unit ball
w : w 1,w Rd . Via a standard covering argument, we can choose such that
2
{ k k ≤ ∈ P C
log( ) dlog(1+2/ǫ).
C|C o| ns≤ ider a fixed w (cid:9) and w⊤S = t w⊤ǫ XT. Note that w⊤ǫ is a σ-sub Gaussian
∈ C i=1 i i i
due to w 1. Hence, Lemma 38 implies that with probability at least 1 δ, for all t
2
k k ≤ P −
t det(V )1/2det(V)−1/2
V−1/2 X w⊤ǫ √2σ log t .
(cid:13) (cid:13) t Xi=1 i (cid:16) i (cid:17)(cid:13) (cid:13)2 ≤ v u u δ !
(cid:13) (cid:13) t
Now apply a u(cid:13)nion bound over , w(cid:13)e get that with probability at least 1 δ,
(cid:13) C (cid:13) −
t det(V )1/2det(V)−1/2
w : V−1/2 X w⊤ǫ √2σ dlog(1+2/ǫ)+log t .
∀ ∈C (cid:13) (cid:13) t Xi=1 i (cid:16) i (cid:17)(cid:13) (cid:13)2 ≤ v u u δ !
(cid:13) (cid:13) t
For any w(cid:13)with w 1, there(cid:13)exists a w′ such that w w′ ǫ. Hence, for all w
(cid:13) k k2 ≤ (cid:13) ∈ C k − k2 ≤
such that w 1,
2
k k ≤
t det(V )1/2det(V)−1/2
V−1/2 X w⊤ǫ √2σ dlog(1+2/ǫ)+log t
(cid:13) (cid:13) t Xi=1 i (cid:16) i (cid:17)(cid:13) (cid:13)2 ≤ v u u δ !
(cid:13) (cid:13) (cid:13) (cid:13) t t
(cid:13) (cid:13) +ǫ ǫ X⊤V−1/2 .
i i t
(cid:13) (cid:13)
(cid:13)Xi=1 (cid:13)2
(cid:13) (cid:13)
By the definition of the spectral norm, this(cid:13)implies that, (cid:13)
(cid:13) (cid:13)
t 1 det(V )1/2det(V)−1/2
ǫ X⊤V−1/2 √2σ dlog(1+2/ǫ)+log t .
(cid:13) (cid:13)Xi=1 i i t (cid:13) (cid:13)2 ≤ 1 −ǫ v u u δ !
(cid:13) (cid:13) t
Takin(cid:13)g ǫ = 1/2 conclu(cid:13)des the proof.
(cid:13) (cid:13)
Weintroducethefollowinglemma,whichguaranteestheMLEconvergencerefertoAgarwal et al.
(2020).
Lemma 39 (MLE bound, Agarwal et al. (2020)) By Algorithm 3, for a fixed t 0
≥
and h [H], with probability at least 1 δ, we have
∈ −
2log(Θ Υ /δ)
E ρbt[ kPh∗( ·|s,a) −Pt( ·|s,a) k2 1]
≤
| t|| | .
As a straightforward corollary, we have with probability at least 1 δ,
b
−
2log(TH Θ Υ /δ)
E ρbt[ kPh∗( ·|s,a) −Pt( ·|s,a) k2 1]
≤
t| || | ,
for all t [T] and h [H].
b
∈ ∈
36Primal-Dual Policy Optimization for Constrained Reinforcement Learning
The following is a standard inequality to prove regret bounds for online learning in linear
models.
Lemma 40 (Agarwal et al. (2020)) Considerthefollowing process. Fort = 1, ,T,M =
t
···
M + G with M = λ I and G being a positive semidefinite matrix with eigenvalues
t−1 t 0 0 t
upper-bounded by 1. We have that
T
2lndet(M ) 2lndet(λ I) Tr G M−1 .
T − 0 ≥ t t−1
n=1
X (cid:0) (cid:1)
The next lemma provides an upper bound for the potential elliptical lemma and was first
proved in Lemma 20 of Uehara et al. (2022). For completeness, we provide its proof.
Lemma 41 ((Uehara et al., 2022)) Suppose Tr(G ) B2.
n
≤
NB2
2lndet(M ) 2lndet(λ I) dln 1+ .
N 0
− ≤ dλ
(cid:18) 0 (cid:19)
Proof Let σ , ,σ be the set of singular values of M recalling M is a positive
1 d N N
···
semidefinite matrix. Then, by the AM-GM inequality,
d d
1
lndet(M )/det(λ I) = ln (σ /λ ) dln (σ /λ )
N 0 i 0 i 0
≤ d
!
i=1 i=1
Y X
Since we have σ = Tr(M ) dλ +NB2, the statement is concluded.
i i N ≤ 0
The next lemmPa provides an upper bound for the summation of potential function and is
a simple generalization of the elliptical potential lemma(Abbasi-Yadkori et al., 2011). In
fact, it is a special case of Lemma 40.
Lemma 42 (Elliptical Potential Lemma) For any sequence of φ (st,at) ,
{ h h h }t∈[T],h∈[H]
we have
T H
φ (st,at) 2 2Hlog det(ΛT)det(Λ0)−1 .
h h h (Λt)−1 ≤
t=1h=1 (cid:18) (cid:19)
XX(cid:13) (cid:13)
(cid:13) (cid:13)
Proof Denote φ (st,at) by φt. Recall that Λt+1 = Λt + H−1φt φt ⊤ and Λ0 = λI.
h h h h h=0 h h
Since λ
≥
1 and kφ k2
≤
1, kφt hk(Λt)−1
≤
1 for all (t,h)
∈
[T] P×[H]. Us (cid:0)e x
(cid:1)≤
2Hlog(1+x)
for x [0,H], we have
∈
H H
φt 2 2Hlog 1+ φt 2 .
h (Λt)−1 ≤ h (Λt)−1
h=1 (cid:18) h=1 (cid:19)
X(cid:13) (cid:13) X(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
For Λt+1, using its recursive formulation, we have:
H
logdet(Λt+1) =logdet(Λt)+logdet I +(Λt)−1/2 φt(φt)⊤(Λt)−1/2 .
h h
(cid:18) h=1 (cid:19)
X
37Li, Liu, Yang, Wang, Wang
Denote the eigenvalues of (Λt)−1/2 H φt(φt)⊤(Λt)−1/2 as σ for i 1. We have
h=1 h h i ≥
H P
logdet I +(Λt)−1/2 φt(φt)⊤(Λt)−1/2 = log (1+σ ) log 1+ σ ,
h h i ≥ i
(cid:18) h=1 (cid:19) i≥1 i≥1
X Y (cid:0) X (cid:1)
where the last inequality uses that σ 0 for all i. Using the above and the definition of
i
≥
the trace,
H H
logdet I +(Λt)−1/2 φt(φt)⊤(Λt)−1/2 log 1+tr (Λt)−1/2 φt(φt)⊤(Λt)−1/2
h h ≥ h h
(cid:18) h=1 (cid:19) (cid:18) (cid:18) h=1 (cid:19)(cid:19)
X X
H
= log 1+ (φt)⊤(Λt)−1φt .
h h
(cid:18) h=1 (cid:19)
X
(44)
Telescoping over t [T], we have
∈
T H T
2H log 1+ (φt)⊤(Λt)−1φt 2H logdet(Λt+1) logdet(Λt)
h h ≤ −
t=1 (cid:18) h=1 (cid:19) t=1
X X X(cid:0) (cid:1)
= 2Hlog det(ΛT)det(Λ0)−1 ,
Therefore, we conclude the proof of Lemma 42. (cid:0) (cid:1)
The following lemma was proved in Freedman (1975) and generalizes Bernstein’s in-
equality for independent variables to martingale case.
Lemma 43 (Freedman inequality) Suppose X ,...,X is a sequence of random vari-
1 n
ablessuchthat0 X 1. Definethemartingale differencesequence Y = E[X X ,...,X ] X
i n n 1 n−1 n
≤ ≤ { | − }
and note K the sum of the conditional variances
n
n
K = E X2 X ,...,X .
n k | 1 k−1
k=1
X (cid:2) (cid:3)
Let S = n X , then for all ǫ,v 0,
n i=1 i ≥
PP( n E[X X ,...,X ] S ǫ,K k) exp ǫ2
i=1 i | 1 i−1 − n ≥ n ≤ ≤ −2k+2ǫ/3
(cid:16) (cid:17)
P
Lemma 44 (χ2-Distance Between Two Gaussians) ForGaussiandistributions µ ,σ2
1
N I
and µ ,σ2 , the (squared) chi-squared distance between and is,
2 1 2
N I N N (cid:0) (cid:1)
(cid:0) (cid:1) ( (z) (z))2 µ µ 2
1 2 1 2
N −N dz = exp k − k 1.
(z) 2σ2 −
1 !
Z N
Proof Note that,
( (z) (z))2 (z)2 (z)2
1 2 2 2
N −N dz = (z) 2 (z)+ N dz = 1+ N dz.
1 2
(z) N − N (z) − (z)
1 1 1
Z N Z N Z N
38Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Also note that for 2(z)/ (z), we have
N2 N1
1 1
2(z)/ (z) = exp 2 z µ 2 z µ 2 ,
N2 N1 Z −2σ2 k − 2 k2−k − 1 k2
(cid:18) (cid:19)
(cid:16) (cid:17)
where Z is the normalization constant for 0,σ2 , i.e. Z = exp 1 z 2 dz. Thus,
N I −2σ2k k2
for 2 z µ 2 z µ 2, we can verify that
k − 2 k2−k − 1 k2 (cid:0) (cid:1) R (cid:0) (cid:1)
2 z µ 2 z µ 2 = z+(µ 2µ ) 2 2 µ µ 2.
k − 2 k2−k − 1 k2 k 1 − 2 k2− k 1 − 2 k2
which implies,
(z)2 1 1
N2 dz = exp z (2µ µ ) 2 2 µ µ dz
(z) Z −2σ2 k − 2 − 1 k2− k 1 − 2 k
Z N1 Z (cid:18) (cid:16) (cid:17)(cid:19)
1 µ µ 2 1
= exp k 1 − 2 k2 exp z (2µ µ ) 2 dz
Z σ2 −2σ2 k − 2 − 1 k2
!
Z (cid:18) (cid:19)
µ µ 2
= exp k 1 − 2 k2 .
σ2
!
Therefore, we conclude the proof.
Lemma 45 (Expectation Difference Under Two Gaussians) ForGaussiandistribu-
tion (µ ,σ2 ) and (µ ,σ2 ), suppose that µ , µ B, then for any (appropri-
1 2 1 2 2 2
N I N I {k k k k } ≤
ately measurable) positive function g, it holds that:
µ µ
E [g(z)] E [g(z)] C(σ,B) k 1 − 2 k2 E [g(z)2],
z∼N1
−
z∼N2
≤ · σ
z∼N1
q
where C(σ,B)= exp(B2/σ2).
Proof Define m = E [g(z)] for i 0,1 . We have:
i z∼N1
∈{ }
N (z)
m m = E g(z) 1 2
1
−
2 z∼N1
− N (z)
(cid:20) (cid:18) 1 (cid:19)(cid:21)
(N (z) N (z))2
E [g(z)2] 1 − 2 dz
≤
z∼N1
s N 1(z)
q Z
µ µ 2
= E [g(z)2] exp k 1 − 2 k2 1.
z∼N1 s 2σ2 −
q (cid:18) (cid:19)
By convexity we have exp(x) 1+xexp(x) for all x, we have
≤
µ µ 2 µ µ 2 µ µ 2
exp k 1 − 2 k2 1 k 1 − 2 k2 exp k 1 − 2 k2
2σ2 − ≤ 2σ2 · 2σ2
(cid:18) (cid:19) (cid:18) (cid:19)
µ µ 2 2B2
k 1 − 2 k2 exp .
≤ 2σ2 · σ2
(cid:18) (cid:19)
39Li, Liu, Yang, Wang, Wang
Therefore, we have
B2 µ µ
m m exp k 1 − 2 k2 E [g(z)2].
1 − 2 ≤ σ2 · σ z∼N1
(cid:18) (cid:19) q
References
Yasin Abbasi-Yadkori, D´avid Pa´l, and Csaba Szepesv´ari. Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems, 24:2312–2320,
2011.
PieterAbbeelandAndrewYNg.Apprenticeshiplearningviainversereinforcementlearning.
In Proceedings of the twenty-first international conference on Machine learning, page 1,
2004a.
Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforce-
ment learning. In Proceedings of the Twenty-First International Conference on Ma-
chine Learning, ICML ’04, page 1, New York, NY, USA, 2004b. Association for
Computing Machinery. ISBN 1581138385. doi: 10.1145/1015330.1015430. URL
https://doi.org/10.1145/1015330.1015430.
Naoki Abe, Prem Melville, Cezar Pendus, Chandan K. Reddy, David L. Jensen, Vince P.
Thomas, James J. Bennett, Gary F. Anderson, Brent R. Cooley, Melissa Kowalczyk,
Mark Domick, and Timothy Gardinier. Optimizing debt collections using constrained
reinforcement learning. In Proceedings of the 16th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, KDD ’10, page 75–84, New York,
NY, USA, 2010. Association for Computing Machinery. ISBN 9781450300551. doi:
10.1145/1835804.1835817. URL https://doi.org/10.1145/1835804.1835817.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Struc-
tural complexity and representation learning of low rank mdps. arXiv preprint
arXiv:2006.10814, 2020.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based rein-
forcement learning with value-targeted regression. In International Conference on Ma-
chine Learning, pages 463–474. PMLR, 2020.
Amir Beck. First-order methods in optimization. SIAM, 2017.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cam-
bridge university press, 2004.
Kiant´e Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz,
Aleksandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in
concave-convex and knapsack settings, 2021.
40Primal-Dual Policy Optimization for Constrained Reinforcement Learning
L.M. Bregman. The relaxation method of finding the common point of convex sets and its
application to the solution of problems in convex programming. USSR Computational
Mathematics and Mathematical Physics, 7(3):200–217, 1967. ISSN0041-5553. doi: https:
//doi.org/10.1016/0041-5553(67)90040-7.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity
of reinforcement learning. arXiv preprint arXiv:1612.02516, 2016.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under
bandit feedback. arXiv preprint, 2008.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform
pac bounds for episodic reinforcement learning. arXiv preprint arXiv:1703.07710, 2017.
LucDevroye, AbbasMehrabian, and Tommy Reddad. Thetotal variation distance between
high-dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018.
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic.
Provably efficient safe exploration via primal-dual policy optimization. In International
Conference on Artificial Intelligence and Statistics, pages 3304–3312. PMLR, 2021.
A.Ya. Dubovitskii and A.A. Milyutin. Extremum problems in the presence of re-
strictions. USSR Computational Mathematics and Mathematical Physics, 5(3):1–80,
1965. ISSN 0041-5553. doi: https://doi.org/10.1016/0041-5553(65)90148-5. URL
https://www.sciencedirect.com/science/article/pii/0041555365901485.
YonathanEfroni,ShieMannor,andMatteoPirotta. Exploration-exploitation inconstrained
mdps. arXiv preprint arXiv:2003.02189, 2020.
David A. Freedman. On Tail Probabilities for Martingales. The Annals
of Probability, 3(1):100 – 118, 1975. doi: 10.1214/aop/1176996452. URL
https://doi.org/10.1214/aop/1176996452.
Javier Garcıa and Fernando Ferna´ndez. A comprehensive survey on safe reinforcement
learning. Journal of Machine Learning Research, 16(1):1437–1480, 2015.
Hassan Hijazi and Leo Liberti. Constraint qualification failure in action. Operations Re-
search Letters, 44(4):503–506, 2016.
Thomas Hofmann, Bernhard Scho¨lkopf, and Alexander J. Smola. Kernel methods in ma-
chine learning. The Annals of Statistics, 36(3), Jun 2008. ISSN 0090-5364. doi: 10.1214/
009053607000000677. URL http://dx.doi.org/10.1214/009053607000000677.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforce-
ment learning with linear function approximation. In Conference on Learning Theory,
pages 2137–2143. PMLR, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes
of rl problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00815, 2021.
41Li, Liu, Yang, Wang, Wang
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen
Sun. Information theoretic regret bounds for online nonlinear control. arXiv preprint
arXiv:2006.12466, 2020.
Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system
identification with guarantees. arXiv preprint arXiv:2006.10277, 2020.
Sobhan Miryoosefi, Kiant´e Brantley, Hal Daume III, Miro Dudik, and Robert E Schapire.
Reinforcement learning with convex constraints. Advances in Neural Information Pro-
cessing Systems, 32, 2019.
AdityaModi,JinglinChen,AkshayKrishnamurthy,NanJiang,andAlekhAgarwal. Model-
free representation learning and exploration in low-rank mdps, 2022.
Igor Mordatch, Zoran Popovi´c, and Emanuel Todorov. Contact-invariant optimization for
handmanipulation. InProceedings of the ACM SIGGRAPH/Eurographics symposium on
computer animation, pages 137–144, 2012.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard Scho¨lkopf.
Kernel mean embedding of distributions: A review and beyond. arXiv preprint
arXiv:1605.09522, 2016.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Scho¨lkopf, et al.
Kernel mean embedding of distributions: A review and beyond. Foundations and
Trends® in Machine Learning, 10(1-2):1–141, 2017.
ShuangQiu,XiaohanWei, ZhuoranYang,JiepingYe,andZhaoranWang. Upperconfidence
primal-dual reinforcement learning for cmdp with adversarial loss. Advances in Neural
Information Processing Systems, 33:15277–15287, 2020.
R Tyrrell Rockafellar. Convex Analysis. Citeseer, 1970.
Yuda Song and Wen Sun. Pc-mlp: Model-based reinforcement learning with policy cover
guided exploration, 2021.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear
programming. In Proceedings of the 25th international conference on Machine learning,
pages 1032–1039, 2008.
Ambuj Tewari and Peter Bartlett. Optimistic linear programming gives logarithmic regret
for irreducible mdps. Advances in Neural Information Processing Systems, 20, 2007.
Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal
feedback control of constrained nonlinear stochastic systems. In Proceedings of the 2005,
American Control Conference, 2005., pages 300–306. IEEE, 2005.
Masatoshi Uehara and Wen Sun. Pessimistic model-based offline rl: Pac bounds and pos-
terior sampling under partial coverage. arXiv e-prints, pages arXiv–2107, 2021.
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and
offline rl in low-rank mdps, 2022.
42Primal-Dual Policy Optimization for Constrained Reinforcement Learning
Sharan Vaswani, Lin F. Yang, and Csaba Szepesv´ari. Near-optimal sample complexity
bounds for constrained mdps, 2022.
Nolan Wagener, Ching-An Cheng, Jacob Sacks, and Byron Boots. An online learning
approach to model predictive control, 2019.
Mengdi Wang. Primal-dual π learning: Sample complexity and sublinear run time for
ergodic markov decision problems. arXiv preprint arXiv:1710.06100, 2017.
Mengdi Wang. Randomized linear programming solves the markov decision problem in
nearly linear (sometimes sublinear) time. Mathematics of Operations Research, 45(2):
517–546, 2020.
GradyWilliams,AndrewAldrich,andEvangelosTheodorou. Modelpredictivepathintegral
control using covariance variable importance sampling, 2015.
Runzhe Wu, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. Offline constrained multi-
objective reinforcement learning via pessimistic dual value iteration. Advances in Neural
Information Processing Systems, 34:25439–25451, 2021.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive
features. In International Conference on Machine Learning, pages 6995–7004. PMLR,
2019.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit,
kernels, and regret bound. In International Conference on Machine Learning, pages
10746–10756. PMLR, 2020.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael Jordan. Provably
efficientreinforcementlearningwithkernelandneuralfunctionapproximations. Advances
in Neural Information Processing Systems, 33, 2020.
Tiancheng Yu, Yi Tian, Jingzhao Zhang, and Suvrit Sra. Provably efficient algorithms for
multi-objective competitive rl. arXiv preprint arXiv:2102.03192, 2021.
Tom Zahavy, Alon Cohen, Haim Kaplan, and Yishay Mansour. Apprenticeship learning via
frank-wolfe. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pages 6720–6728, 2020.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is
enough for convex mdps. arXiv preprint arXiv:2106.00661, 2021.
Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement
learning via distributional risk in the dual domain. arXiv preprint arXiv:2002.12475,
2020a.
Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Vari-
ational policy gradient method for reinforcement learning with general utilities, 2020b.
MartinZinkevich. Onlineconvexprogrammingandgeneralizedinfinitesimalgradientascent.
In Proceedings of the 20th international conference on machine learning (icml-03), pages
928–936, 2003.
43