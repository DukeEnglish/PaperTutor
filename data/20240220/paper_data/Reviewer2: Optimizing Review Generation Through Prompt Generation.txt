REVIEWER2: Optimizing Review Generation Through Prompt Generation
ZhaolinGao,KiantéBrantley,andThorstenJoachims
DepartmentofComputerScience,CornellUniversity
Ithaca,NY,USA
{zg292, kdb82}@cornell.edu,{tj}@cs.cornell.edu
Abstract The ability of LLMs to reason about complex
tasksgivesthemthepotentialtoprovideautomated
Recent developments in LLMs offer new op- feedback on papers (Liu and Shah, 2023; Liang
portunitiesforassistingauthorsinimproving et al., 2023). A key asset is that we already have
their work. In this paper, we envision a use substantialamountsofsuperviseddatafrompeer
casewhereauthorscanreceiveLLM-generated
reviews(Kangetal.,2018a;Yuanetal.,2021;Shen
reviews that uncover weak points in the cur-
etal.,2022;Dyckeetal.,2023),containingpaper-
rentdraft. Whileinitialmethodsforautomated
review pairs across different years, venues, and
review generation already exist, these meth-
subjects. Prior approaches to review generation
ods tend to produce reviews that lack detail,
and they do not cover the range of opinions (Yuanetal.,2021;Linetal.,2023)focusonfine-
that human reviewers produce. To address tuningapre-trainedlanguagemodelbasedonthese
thisshortcoming,weproposeanefficienttwo- datasets. However, unlike typical instruction fol-
stagereviewgenerationframeworkcalledRE-
lowingtasks(Ouyangetal.,2022;Touvronetal.,
VIEWER2. Unlike prior work, this approach
2023),wearguethatopen-endedreviewgeneration
explicitly models the distribution of possible
is under-specified in a way that makes it difficult
aspectsthatthereviewmayaddress. Weshow
to align language models for instruction follow-
that this leads to more detailed reviews that
better cover the range of aspects that human ing. In particular, asking an LLM to generate a
reviewersidentifyinthedraft.Aspartofthere- reviewwithoutspecifyingwhichaspectsofthepa-
search,wegeneratealarge-scalereviewdataset per to focus on exposes the model to substantial
of27kpapersand99kreviewsthatweannotate uncertainty. Thisleadstoshortcomingsalongthe
withaspectprompts,whichwemakeavailable
followingdimensions:
asaresourceforfutureresearch.
Specificity. Peerreviewsexhibitvaryinglevels
ofspecificityfromgeneral(e.g.,"thepaperistech-
1 Introduction
nically sound.") to precise (e.g., "the paper has a
Askingfellowgroupmemberstocritiqueadraftis good theoretical basis based on the derivation in
widelyregardedasavaluablewayofimprovingsci- section 3."). A good review should provide de-
entificwriting,andthelackofaccesstosuchpeers tailed justifications for its assessment, especially
outsideofwell-resourcedresearchgroupsisakey when stating the weaknesses of the paper (Yuan
source of inequality (Merton, 1968; Nielsen and et al., 2021). In addition, justifications make the
Andersen,2021;Kozlowskietal.,2022). Further- reviewmoreconstructiveastheyprovidedirectin-
more,eveninwell-resourcedgroups,thefrequency structionsonhowtoimprovethepaper(Xiongand
withwhichauthorscanreceivefeedbackislimited. Litman, 2011). However, our experiments reveal
Inthispaper,wethusdeveloptechniquesforgener- thatstandardfine-tuningdiminishesthespecificity
atingautomatedfeedbackviaLLMstoaidauthors of the generated reviews. An example is shown
inenhancingthequalityoftheirworkbeforeiten- inTable1wherewegeneratereviewsbasedona
ters the formal peer review. This helps level the modelthatisfine-tunedoverincreasingnumbersof
playing field, and it promises to reduce pressure trainingsteps. Thegeneratedreviewissignificantly
onthepeerreviewprocess(Leeetal.,2012)after moregenericatstep2000comparedtotheoneat
experiencingexponentialincreasesinsubmissions step500.
(BjörkandSolomon,2013;BornmannandMutz, Coverage and Control. Different human re-
2014;Kellyetal.,2014). viewersarelikelytofocusondifferentaspectsof
4202
beF
61
]LC.sc[
1v68801.2042:viXraTrainingStep500 TrainingStep1000 TrainingStep2000
The paper proposes a simple and The authors have done extensive The paper is well-written and
efficientdifferentiabledatagenera- experiments to validate the effec- straightforward. The method is
tionpipeline. tivenessoftheproposedmethod. technicallysound.
Table1: Generatedreviewsfromdifferentstepsusingagenericprompt(1epoch≈1000steps).
apaper. Anautomatedreview-generationsystem and FlashAttention-2, enabling 32k context
should thus cover the range of issues that human length with low memory requirement for fine-
reviewersmayidentify. Wefindthatstandardfine- tuningandinference.
tuningofLLMsforreviewgenerationoftenleads • Wedesigntwonewmetricsforevaluatingspeci-
toaformofregression-to-the-mean,wherethegen- ficityandcoverability. WecompareREVIEWER2
eratedreviewsdonotcoverthefullrangeofaspects. withvariousbaselinemethodsandfindthatitsub-
Wearguethatanidealsystemshouldactivelycon- stantiallyimprovesreviewgeneration.
trolcoverage,andgiveauthorstheabilitytoaskfor • WeproposePGE,anovelpipelineforaugment-
feedbackonspecificaspects. ingexistingreviewdatasetswithaspectprompts,
Toaddresstheseissues,weproposeanefficient andweconstructthefirstlarge-scalepeerreview
two-stagereviewgenerationframeworkforpapers datasetthatincludesaspectprompts.
called REVIEWER2. REVIEWER2 includes two
2 RelatedWork
fine-tuned language models. The first LLM ana-
lyzesthepaperandproducesasetofaspectsthat
Instruction generation and tuning. Previous
thereviewsshouldfocuson. Eachoftheseaspects
workshavedemonstratedtheefficacyofinstruction
takestheformofapromptthatistheinputforthe
fine-tuninginenhancingbothtaskperformanceand
secondstage. ThesecondLLMgeneratesareview
adaptabilitytounseentasks(Weietal.,2022;Sanh
basedonthepaperandtheaspectprompt. Weim-
etal.,2022;Ouyangetal.,2022). However,these
plementREVIEWER2basedonLongLoRA(Chen
approaches depend heavily on human-written in-
etal.,2023)andFlashAttention-2(Dao,2023)to
structiondata,whichisoftenconstrainedinterms
enable32kcontextlength,avoidingtheuseofex-
of quantity and diversity. Several works have ex-
tractivesummariesofthepaperthatwasnecessary
ploredusinglargelanguagemodels(LLMs)toau-
inpriorworkduetolimitationsincontextlength
tomaticallygenerateinstructions. Honovichetal.
(Gehrmann et al., 2018; Chen and Bansal, 2018;
(2022) prompts a language model with seed ex-
Douetal.,2021;Yuanetal.,2021;Linetal.,2023).
amples of instructions to generate additional in-
Unfortunately,existingpeer-reviewdatasetsdo structions,inputs,andoutputs. Wangetal.(2023)
notincludeaspectprompts,providinginsufficient adoptsasimilarapproachwhilefilteringthegener-
data for training either stage of REVIEWER2. To atedinstructionstoensurediversityandquality.
addressthisissue,wedevelopapromptgeneration Self-alignment. Self-alignmentofLLMsisan
withevaluation(PGE)pipelinetogenerateavari- emerging area of research that utilizes the model
etyofhigh-qualityaspectprompts. PGEgenerates toimproveitselfandalignwithhumanvalueswith
promptsgiventhereviewandusesaself-evaluation minimalhuman supervision. Thisfield primarily
steptoensurethequalityofthegeneratedprompts. consistsoftwoapproaches: unsuperviseddatagen-
BasedonPGE,weconstructalarge-scalereview erationandpost-hocoutputrefinement. InLietal.
datasetof27kpapersand99kreviewsfromsixdif- (2023), prompts and responses are generated ac-
ferentvenueswithcorrespondingaspectprompts. cordingtoasmallsetofhuman-writtenprinciples,
Extensive experiments on multiple venues whileSunetal.(2023)focusesongeneratingsyn-
demonstrate that our REVIEWER2 framework theticpromptsderivedfromhuman-writtendocu-
trainedonPGE-generatedaspectpromptssubstan- ments. On the other hand, Madaan et al. (2023)
tiallyoutperformsexistingmethodsintermsofre- employs an iterative process to refine its output
viewquality,specificity,andcoverage. Themajor throughgeneratedfeedback.
contributionsofthispaperaresummarizedbelow: Automation in peer review. Automated sys-
• Wepropose REVIEWER2,anovelframeworkfor tems have played a significant role in various as-
joint aspect prompt and review generation that pectsofthereviewprocess. Numerousalgorithms
improvescoverageandenablescontrol. (Stelmakhetal.,2019;Kobrenetal.,2019;Cohan
• WeimplementREVIEWER2basedonLongLoRA etal.,2020)havebeendevelopedtoevaluatetheex-tuneanLLM
M : p → {x1,...,xk}
p
to produce a set of aspect prompts x1,...xk for
paperpthatcovertheaspectsthatareviewermay
commentonforthispaper. Forthesecondstageof
REVIEWER2,wefine-tuneanotherLLM
M : (p,x) → y
r
to produce a review y for paper p that addresses
aspect x. When generating a review for a new
paper p′, we first query M for an aspect prompt
p
x. We then query M to produce a review y for
r
the generated aspect prompt. This inference pro-
cess is depicted in Figure 1(b). We will provide
evidencethatthistwo-stagepipelinenotonlypro-
vides explicit control of aspect coverage, it also
avoids a type of regression-to-the-mean (Barnett
etal.,2004)thatmakessingle-stagepipelinespro-
ducegenericreviewswithlittlespecificity.
An illustrative example is shown in Figure 2
whichcontainsthreereviews,{y1,y2,y3},forpa-
i i i
Figure1: IllustrationsofREVIEWER2. a)REVIEWER2 perp . Allthreereviewscommentoneitherorboth
i
fine-tunes two models: M p generates aspect prompts theoreticalandempiricaljustifications,represent-
basedonpaper,andM generatesreviewsbasedonthe
r ingthegeneralaspects. However,thereviewspro-
paperandaprompt.b)REVIEWER2utilizesatwo-stage
videdifferentsuggestionsforimprovement,which
inferencetogenerateanaspectpromptandgeneratethe
are considered as specific parts. We find that a
reviewbasedonthegeneratedprompt.
single-stagepipelinethatistrainedwithoutaspect
prompts tends to only generate the general com-
pertiseofpotentialreviewers,optimizingreviewer-
ponentsofthereview,asillustratedinFigure2(b),
paperassignments. Inaddition,severalalgorithms
since such "mean reviews" align closely with all
havebeenproposedtoensurethesubmissionsad-
three reviews. On the other hand, by adding as-
heretoappropriateguidelines,suchasplagiarism
pectprompts{x1,x2,x3}derivedfromthepaper,
detection(Foltýneketal.,2019)anddeskrejection i i i
the augmentation diversifies the aspects that are
prediction(Ghosaletal.,2019). Recently,efforts
addressed, aligning it more effectively with the
havebeendirectedtowardsthedevelopmentofal-
variabilityseeninthehumanreviews. Notethatthe
gorithmsforreviewgeneration(Yuanetal.,2021;
prompt space now better captures the variability
Lin et al., 2023), leveraging papers as input and
betweenreviewers,whichreducesthenoisewhen
fine-tuningonLLMsforreviewgeneration.
mapping to generated reviews. This reduction in
noise enables the generation of more specific re-
3 REVIEWER2 forReviewGeneration views,yˆ,duringinferenceasshowninFigure2(c).
i
Toenableefficientlongcontextfine-tuningand
In this section, we introduce our REVIEWER2 inference, we adapt LoRA+ and S2-Attn from
pipelineforgeneratingreviews. Thekeyideaisto Chenetal.(2023).
insert explicit control into the pipeline to ensure LoRA+. LoRA (Hu et al., 2021) achieves
thatthegeneratedreviewscoverthefullrangeof efficiency by updating only the low-rank matri-
aspects that human reviewers may comment on. ces. Specifically, for a pre-trained weight matrix
Wedemonstratethatthisimprovesbothcoverage W ∈ Rd×k,LoRAutilizesalow-rankdecomposi-
andspecificityofthegeneratedreviews. tionW = W +BA,whereB ∈ Rd×r,A ∈ Rr×k,
Figure 1(a) illustrates how we train the two and r ≪ min(d,k). During training, W remains
stagesofREVIEWER2. Forthefirststage,wefine- fixed, while A and B are updated. In the contextFigure2: Illustrationsoftheeffectofaspectprompts. a)Generalcontentishighlightedinblue,whilespecific
contentishighlightedinred. b)Fine-tuningwithoutaspectpromptscausesthegeneratedcontentstobegeneral
duringinference. c)Fine-tuningwithaspectpromptsallowsspecificcontentgenerationduringinference.
4 ReviewDatasetwithAspectPrompts
TrainingREVIEWER2requiresadatasetofpapers
andreviewsthatisaugmentedwithaspectprompts.
While there is ample data on papers and their as-
sociatedreviews,thesedatasetscontaingenericre-
viewpromptsthatdonotcapturewhichaspectsthe
human reviewer chose to focus on. We therefore
developedthefollowingmethodologyforaugment-
ingexistingreviewdatasetswithaspectprompts.
The result is the first review dataset that is an-
notated with aspect prompts, and we make this
dataset available as a new resource. It consists
ofup-to-datecrawlsofpubliclyavailablereviews
fromNeurIPSandICLR,andwealsoaugmentthe
datasets from PeerRead (Kang et al., 2018b) and
NLPeer(Dyckeetal.,2023).
Figure3: PGEincludestwosteps: generationandeval-
uation. Thepromptisregeneratedifthescoreisbelow
4.1 PGE:PromptGenerationwithEvaluation
5ona5-pointscale,otherwise,itissavedtoS.
Inordertogeneratethecorrespondingpromptfor
of Transformers, LoRA selectively adapts atten- eachreview,weproposePromptGenerationwith
tion weights W ,W ,W ,W while keeping all Evaluation (PGE) pipeline consisting of a gen-
q k v o
other parameters frozen. LoRA+ extends on top eration step and an evaluation step, as shown in
ofLoRAbyalsomakingtheembeddingandnor- Figure 3. Specifically, given a set of m papers
malizationlayerstrainable. P = {p ,p ,...,p }andcorrespondingreference
1 2 m
S2-Attn. Toaddressthequadraticcomplexityof reviews Y = {y in|1 ≤ i ≤ m,1 ≤ n ≤ n i}
self-attention,S2-Attnpresentsasolutionbygroup- wheren
i
isthenumberofreviewsforpaperi,the
inginputtokens. Forinstance,withaninputlength goalofthepipelineistogenerateasetofprompts
of32,768andgroupsize4,itdividestheinputinto X = {xn i|1 ≤ i ≤ m,1 ≤ n ≤ n i} that one
four groups, each of length 8,192, limiting self- promptcorrespondstoonereview.
attentiontotokenswithinthesamegroup. Toallow For a review yn, the generation step generates
i
informationflowacrossdifferentgroups,S2-Attn a prompt, xn, and the evaluation step evaluates
i
shiftsthegrouppartitionbyhalfgroupsizeinhalf the generated prompt based on a 5-point scale.
attentionheads. Inthisway,thefirstgroupinthe If xn achieves a score of 5, the pair (xn,yn) is
i i i
firsthalfoftheattentionheadisfrom1st to8192th storedinthesetS,S = S∪{(xn,yn)},otherwise
i i
token while the first group in the second half of thepromptisregenerated. Thistwo-stepiterative
theheadisfrom4096th to12288th token. S2-Attn approach resolves the problem of the absence of
couldalsobeusedwithFlashAttention-2,allowing ground-truthpromptsforreviewsandensuresthe
acceleratedcomputations. qualityofpromptgenerationwithouthumansuper-Table2: DatasetStatistics
CONLL-16 ACL-17 COLING-20 ARR-22 ICLR-17-23 NeurIPS-16-22 total
#papers 22 137 89 476 16,327 10,754 27,805
#wordsperpaper 4,325 4,679 4,230 4,850 6,959 5,236 6,229
#reviews 39 275 112 684 58,933 39,684 99,727
#wordsperreview 418 440 414 397 512 482 487
#prompts 37 270 108 676 58,107 38,762 97,960
#wordsperprompt 56 60 45 46 52 51 53
%accepted 50% 67% 93% 100% 32% 98% 55%
domain NLP/CL NLP/CL NLP/CL NLP/CL ML ML multi
Table3: DatasetComparison tions. Inspiredbychain-of-thoughtprompting(Wei
et al., 2023), we prompt the LLM to generate an
#papers #reviews prompts
PeerRead explanationforthescorebeforeproducingthefinal
3,006∗ 10,770 ✗
(Kangetal.,2018b) scoretoencouragemoreaccurateassessments.
ASAP-Review 8,877 28,119 ✗ Regeneration. Toensurethequalityofthegen-
(Yuanetal.,2021)
eratedprompt,thepipelineregeneratestheprompt
MReD
7,894 30,764 ✗ ifthescoreisnot5. Sincethein-contextexamples
(Shenetal.,2022)
for generation are randomly sampled rather than
NLPeer
5,672 11,515 ✗
(Dyckeetal.,2023) a fixed set, the regeneration step is guaranteed to
Ours 27,805 99,727 ✓ generateadifferentpromptcomparedtotheprevi-
*Numberofpapersthathavereviews. ousgenerations,minimizingredundancy. Weusea
limitof5generationsperreview,andthereviewis
vision. The prompts we used for generation and
excludedfromfurthergenerationifitexceedsthe
evaluationareshowninAppendixA.
limit. Morethan90%ofthereviewstakelessthan
Prompt Generation. We initialize S with
orequalto3generationstoreachascoreof5.
human-annotated examples that will be used as
initialin-contextexamplesduringgeneration. To 4.2 DatasetDetails
construct these examples, we use Llama-2-70B-
WeincorporatepartsofthePeerReadandNLPeer
Chat (Touvron et al., 2023) to generate prompts
datasets. CONLL-16 and ACL-17 from Peer-
for a randomly selected subset of 100 reviews in
Read contain papers and reviews from the NLP
azero-shotfashion. Then,wemanuallyrefinethe
domain. The reviewing process is double-blind
promptsbyremovingirrelevantquestions,adding
and the formats of the review are unstructured.
missing questions that are covered in the review,
NLPeer’sCOLING-20andARR-22arecollected
andrefiningtoalignwiththeopen-endedformatof
viaadonation-basedworkflowinNLPdomainwith
reviewquestions. Anexampleofareview-prompt
formatsinfree-formreportsandstandardizedstruc-
pairisshowninAppendixB.
turedreviewforms.
To enhance the performance of prompt gener-
Inadditiontothepriordatasets,wecrawlICLR
ation, we apply in-context learning (ICL) (Dong
papers from 2017 to 2023 through OpenReview1
et al., 2023) in the process. The in-context ex-
and NeurIPS papers from 2016 to 2020 through
amples are randomly sampled from S. As more
NeurIPS Proceedings2 and from 2021 to 2022
prompts are generated and saved to S, the pool
through OpenReview. The resulting datasets are
of available examples also expands, ensuring the
ICLR-17-23 and NeurIPS-16-22. For each pa-
diversity of the prompts. We always sample the
per’sreview,wefollowtheformatoftheprevious
maximumpossiblenumberofin-contextexamples
datasetstokeepasmuchmetadatainformationas
whilesatisfyingthecontextlengthconstraint.
possibleincludingreferenceandmetareviewsfrom
PromptEvaluation. Similartogeneration,we
officialreviewers,andfinaldecisions.
alsoapplyICLduringtheevaluationstep. Weuse
Unification. The diverse sources of datasets
Llama-2-70B-Chattoevaluatethereview-prompt
areconvertedintoaunifiedformattoenhanceac-
pairbasedona5-pointscalewithfivein-contextex-
cessibility and consistency. For each paper, we
amplesforeachscorefrom1to5. Thein-context
examples (shown in Appendix C) are manually 1https://openreview.net/
constructedandremainconsistentacrossallevalua- 2http://papers.neurips.cc/includethefulltextofthepaper,metadata,andcor- Table 4: Results of the model variations using three
respondingreviewsandprompts. Forthecontents
metrics across six venues (SS-E0: SINGLES-E0, SS-
ofthepaper,weuseScienceParse3 fromAllenAI
E:SINGLES-E,SS:SINGLES,R2-E:REVIEWER2-E,
R2: REVIEWER2). Thebest-performingmodelforeach
toparsethePDFsofthepapersintoconstructstruc-
venueandmetricishighlightedinbold.
tured JSON files. Each paper is accompanied by
ROUGE(max)
detailedmetadata,providingessentialinformation Method BLEU BertScore
(max) R-1 R-2 R-L (max)
aboutthepaper. Thedetailedsectionsofpaperand
SS-E0 8.15 29.93 7.14 13.76 68.45
metadata are shown in Appendix E. The reviews SS-E 12.53 39.63 10.19 19.76 79.40
containbothtextualcomponentsandscoresthatare R2-E 13.32 40.06 10.59 20.34 80.11
SS 15.08 40.77 11.78 21.09 81.18 dividedintodifferentsectionsbasedonthevenue-
R2 16.94 44.58 13.56 22.62 83.61
specificformats. Inaddition,weemployourPGE SS-E0 8.29 28.96 6.98 13.63 67.82
pipelinetoconstructapromptforeachreview. For SS-E 11.72 39.54 9.75 19.67 79.17
R2-E 12.91 39.87 10.02 19.81 80.17
simplicity,weonlyusethetextpartofthereview
SS 14.44 40.62 11.22 20.8 81.83
forpromptgenerationandreviewgeneration. R2 16.24 42.15 13.11 22.52 83.23
Analysis. Thestatisticsofourdatasetareshown SS-E0 5.02 30.77 6.28 12.69 68.90
SS-E 4.67 35.23 7.07 16.53 78.15
inTable2. Ourdatasetconsistsofmorethan27k
R2-E 4.82 36.44 7.98 16.73 80.03
papers and 99k reviews in various domains. The SS 5.40 35.73 7.94 16.94 80.25
averagepaperlengthspansfrom4k to7k,demon- R2 6.49 36.88 8.04 17.77 83.65
SS-E0 6.01 32.48 7.89 13.91 69.34
stratingsubstantialvariability. Thereviewlength
SS-E 6.89 38.30 9.67 18.67 79.09
andpromptlengthexhibitsmallervariances,aver- R2-E 6.96 39.17 10.94 19.53 80.69
aging from 400 to 500 and 45 to 60 respectively. SS 6.73 38.93 11.22 19.61 81.03
R2 7.46 40.18 12.04 20.76 82.29
Comparedtootherreviewdatasets(Table3),our
SS-E0 3.66 30.51 6.49 12.83 69.19
dataset has the largest number of papers and re- SS-E 2.65 35.31 6.92 16.5 77.92
R2-E 3.01 35.09 7.34 17.74 78.15
viewsandistheonlydatasetthatincludesaspect
SS 3.34 34.57 8.11 17.14 80.21
prompts. R2 4.37 37.13 9.18 18.91 83.35
LicensingandPersonalDataAlldatasetsare SS-E0 5.18 32.01 6.32 12.75 69.45
SS-E 3.41 35.16 6.89 16.18 78.39
distributed under an open Creative Commons li-
R2-E 3.59 34.28 6.74 16.82 80.15
cense and are compiled with explicit consent or SS 5.09 33.85 6.88 16.52 79.83
sourcedfrommaterialswithanopenlicense. We R2 6.07 35.38 7.40 18.22 83.13
attributeauthorsofthepapersinourdatasetwhile
excluding personalmetadata andrevieweridenti-
This ablation is used to evaluate the difference
fiers.
betweenusingthefullpapercomparedtoanex-
5 Experiments tractivesummary.
• SINGLES:Wefine-tuneasingle-stagemodelto
Inthefollowingsection,weevaluatereviewqual- directlygeneratereviewsfromthefullcontextof
ity,reviewspecificity,andaspectcoverageaskey thepaperwithoutanaspectprompt,MS : p →
r
properties of the generated reviews. We provide y. Prompts are neither used in fine-tuning nor
extensive ablation experiments that identify how inference. Thisablationisdesignedtoevaluate
mucheachnovelcontributionofourapproachcon- theeffectofaspectprompts.
tributestoimprovedperformance. Inparticular,we • SINGLES-E:Thisvariantinvolvesfine-tuninga
compare REVIEWER2 againstthefollowingbase- single model togenerate reviews onlyfrom ex-
lines: tractive summaries of papers, MSE : e → y.
r
• REVIEWER2-E: Following (Yuan et al., 2021), This method aligns with commonly employed
weapplyacross-entropy(CE)extractionmethod pipelinesinpreviouspapersandservesasabase-
toextractadiversesetofsentencesfromthepa- linerepresentingthestate-of-the-art.
per to represent the content of the paper. The • SINGLES-E0: Thiszero-shotapproachprompt
frameworkisthesameas REVIEWER2 whilewe anLLMtogenerateareviewfromtheextracted
onlyusetheextractedpartinsteadofthefullpa- context directly without aspect prompts. This
per: M pE : e → {x1,...,xk}, M rE : (e,x) → y baselineevaluatestheeffectoffine-tuning.
where e is the extracted content from paper p.
WeuseLlama-2-70B-Chat(Touvronetal.,2023)
3https://github.com/allenai/science-parse as the instruction-following model for PGE and
niamod-nI
niamod-ssorC
RLCI
SPIrueN
LCA
RRA
GNILOC
LLNOCFigure4: Specificityplotsoffourmethodsfor2000stepsacrosssixvenues.
Llama-2-7B-ChatforREVIEWER2andthesingle vides an additive benefit on top of using aspect
stage baselines. The hyperparameter details are prompts. On the cross-domain datasets (ACL,
shown in Appendix F. We randomly select 80% ARR,COLING,CONLL)wecanobserveacom-
ofICLRandNeurIPSpapersfortraining,10%for parable BertScorewith ICLRand NeurlPS using
validation,and10%fortestingwhileusingallthe REVIEWER2, demonstrating the semantic adapt-
papersinothervenuesfortesting. Sincetheother abilityofthemethodtodomainsthatthemethods
venues have review formats different from ICLR wasnottrainedon.
andNeurIPS,thisallowsustotestadaptabilityto Tofurtherillustrate REVIEWER2,weincluded
differentreviewformats. aspectpromptsproducedbyM andareviewpro-
p
ducedbyM inAppendixD.
r
5.1 QualityAnalysis
5.2 SpecificityAnalysis
To compare the generated reviews with the refer-
encereviews,weemploythreemetrics: BLEU(Pa- Ahighlyspecificreviewidentifiesspecificissuesof
pineni et al., 2002), ROUGE (Lin and Hovy, thegivenpaper,anditdoesnotlooklikeageneric
2003),andBertScore(Zhangetal.,2020). BLEU review that could apply to other papers. To for-
andROUGEmeasurethen-gramsimilaritywhile malizethisintoaconcisemetric,wemeasurethe
BertScoremeasuresthesemanticsimilarityinthe specificity of the review by calculating the drop
embedding space. Notably, there are several ref- inBertScorewhenpairingthereviewwiththeref-
erence reviews for each paper. When computing erence reviews of a different paper. A generated
BLEU,ROUGE,andBertScore,weusethemaxi- reviewwithhighspecificitywillleadtoalargeav-
mumvalueinsteadofanaveragesincethegener- erage drop, while a generic review will lead to a
atedreviewsdonotneedtobecloselyalignedwith smallerdrop. Formally,givenpapersP,reviewsY,
allreferences,giventhatthereferencereviewsmay and generated reviews Yˆ = {yˆ ,yˆ ,...,yˆ }, we
1 2 m
focusondifferentaspects. definespecificity(SPE↑)as:
Result. Table 4 compares the performance of
m
1 (cid:88)
REVIEWER2 against several ablations and base- SPE =
m
max{sim(yˆ i,y in)|1 ≤ n ≤ n i}
lines. Overall, REVIEWER2 outperformsallmeth-
i=1
ods across all metrics and datasets, demonstrat- 1 (cid:88)
− max{sim(yˆ,yn)|1 ≤ n ≤ n }
ing the effectiveness of leveraging both the full m−1 i j i
j̸=i
context of the paper and the aspect prompt. The
comparisonsbetweenREVIEWER2andSINGLES wheresim(a,b)denotestheBertScorebetweena
as well as REVIEWER2-E and SINGLES-E re- and b and yˆ j. We approximate the inner sum by
vealconsistentperformanceimprovementthrough MonteCarlosamplingj ∼ [1,m]\i.
the two-stage approach. Furthermore, the com- Result. To obtain a reliable measure, we con-
parison between REVIEWER2 and REVIEWER2- ductedtenrandomshufflesandcalculatedtheav-
E shows that avoiding extractive summaries pro- erage. TheresultisshowninFigure4alongwithTable5: EffectofpromptsforSINGLES(SS)andRE- Table6: Coverability(COV↓)forREVIEWER2-E(R2-
VIEWER2(R2)acrosssixvenues. E)andREVIEWER2(R2)acrosssixvenues.
1 (cid:88)m 1 (cid:88)ni Method ICLR NeurIPS ACL ARR COLING CONLL
SS sim(MS(p ),yn)
m n r i i R2-E 13.55 12.66 16.62 15.29 14.84 15.46
i i=1 n=1 R2 4.22 3.99 3.23 2.91 5.09 4.25
1
(cid:88)m
1
(cid:88)ni (cid:88)ni
R2 sim(M (p ,xn),yk)
m i=1 n2 i n=1k=1 r i i i aspects.
m
SS 1 (cid:88) max{sim(MS(p ),yn)| 5.4 CoverageAnalysis
m r i i
i=1 1≤n≤n i} Finally,weevaluatewhetherauthorscanachieve
1
(cid:88)m
1
(cid:88)ni
good coverage through the choice of aspect
R2 max{sim(M (p ,xn),yk)|
m i=1 n i n=1 1≤k ≤r n ii } i i prompts. SinceM r andM rE aretheonlymodels
thatpermitaspectprompts,weevaluatetheeffect
Method ICLR NeurIPS ACL ARR COLING CONLL
ofaspectpromptsoncoverageforthesetwomod-
SS 80.19 80.23 79.85 80.23 79.42 78.41
R2 80.13 80.36 79.14 79.96 79.53 78.28 els. Given papers P, reviews Y, prompts X, we
SS 81.18 81.83 80.25 81.03 80.21 79.83 definecoverability(COV↓)forM r as:
R2 83.63 83.41 83.54 82.51 83.19 83.32
m
1 (cid:88)
COV = g i−h
i
m
the variance. For methods that do not make use i=1
ofaspectprompts, SINGLES and SINGLES-E,the 1 (cid:88)ni (cid:88)ni
h = sim(yn,yk)
specificity drops with more training steps. This i n (n −1) i i
i i
indicatesthatincreasedtrainingwithoutprompts n=1k=1
k̸=n
leads to more generic reviews. For the methods
thatuseprompts,REVIEWER2-EandREVIEWER2, g =
1 (cid:88)ni (cid:88)ni sim(M r(p i,xn i),
thespecificityconsistentlyincreaseswithahigher
i n i(n i−1)
n=1k=1
M r(p i,xk i))
k̸=n
numberofsteps. Notably,thedifferencebetween
REVIEWER2andSINGLESishigherthanthedif- Here,h
i
representsthepairwisesimilarityamong
ferencebetween REVIEWER2-E and SINGLES-E, the reference reviews for paper p
i
while g
i
is the
suggestingthataddingpromptsontopofthefull pairwisesimilarityamonggeneratedreviewsbased
contextleadstohigherimprovementcomparingto onthePGEpromptsinthedataset. Thecoverability
addingtotheextractedcontext. for ME is defined similarly but with e as input
r i
instead of p . We use BertScore to calculate the
i
5.3 ControlAnalysis similarities. Ahighg indicatesthatthegenerated
i
To assess how responsive REVIEWER2 is to the reviews are similar despite being generated from
aspectprompts,weconductexperimentsthatcom- differentprompts.
pare REVIEWER2 and SINGLES. The M
r
model Result. TheresultsareshowninTable6. While
inREVIEWER2isgiventhepromptsgeneratedby perfectly reproducing the coverage of the human
PGE. We compute the average similarity of the reviewswouldimplyavalueof0,M r exhibitssig-
generatedreviewtothereferencereviewsforboth nificantlybettercoveragethanM rE,demonstrating
methodsaswellasthemaximumsimilarity. The its effectiveness in generating tailored responses
detailedequationsforthecomputationsareshown across diverse prompts for a given paper and the
inTable5. BertScoreisusedforcomputingsim. importanceofusingfullcontext.
Result. REVIEWER2 and SINGLES have sim-
6 Conclusion
ilar average similarity while REVIEWER2 has a
higher maximum similarity across all six venues. Weproposeatwo-stagereviewgenerationframe-
This meansthat SINGLES generates reviewsthat work that incorporates aspect prompts. Analyses
areclosetoallthereferencereviews,butthatare ofquality,specificity,andcontrollabilityindicate
notparticularlyclosetoanyoneofthem. Incon- thatourmethodcangeneratehigh-qualityandspe-
trast,REVIEWER2isconsistentlyabletogenerate cificreviewswhilebeingcontrollablebasedonthe
reviews that closely match one of the references. aspect prompt. Furthermore, we develop a new
ThisprovidesevidencethatREVIEWER2isrespon- pipelineforannotatingreviewdatasetswithaspect
sive to aspect prompts and can cover the desired prompts,andwemakethisnewdatasetavailable.
gva
xam
gva
xam7 Limitations datasetswereleasedoffermanypossibilitiesforad-
vancingresearchinNLP,includingbutnotlimited
Inthissection,wediscusssomeofthelimitations
to review generation, instruction following, and
forPGEand REVIEWER2.
self-alignment.
7.1 DisjointProcessesforGeneration
Our current configuration first uses PGE to gen-
erate prompts and subsequently fine-tunes RE-
VIEWER2 with the generated prompts. However,
thisapproachleadstoadisjointedprocess,where
prompt generation operates independently of re-
viewgeneration,reducingtheeffectivenessofthe
generatedprompts. Ideally,thegeneratedprompts
shouldassistalignmentduringfine-tuning. Apos-
sible extension is to integrate the two processes
togetherandrefinethegeneratedpromptsbasedon
thereviewgenerationpipeline.
7.2 InputInconsistency
The input to PGE consists of human-written re-
views, while REVIEWER2 also incorporates pa-
pers. This distinction arises from the limitation
of Llama-2-70B-Chat, which only has a context
lengthof4,096. AlthoughGPT-4(OpenAI,2023)
supports up to 32,000 context length, the associ-
atedcostishighsincetheaveragecontextlengthof
thepapersis6,229. Thepotentialimprovementin
performancemaynotbeworththeincreasedcost.
7.3 LimitedDomainKnowledge
Currently, REVIEWER2 relies on its pre-trained
corpus,assumingthatthelanguagemodelusedhas
adequatedomainknowledge. Thisapproachmight
produceinaccuratereviewsforpapersthatdemand
substantialin-domainexpertise. Apotentialfuture
workcouldinvestigatetheeffectivenessofsecond-
stagepre-trainingordomainadaptationusingthe
papercorpus.
8 Ethics
Automaticreviewgenerationisacomplextaskand
bears a wide range of risks. It is crucial to em-
phasize that the ongoing efforts in this field are
notdesignedtoreplacehumanreviewers;instead,
they function as a valuable tool for authors and
a guiding resource for human reviewers. This re-
searchisanexploratoryworkwithinthisdomain,
anditisimportanttostressthattheoutcomespro-
duced by the models should not be misconstrued
as definitive and authentic reviews of the respec-
tivepapers. Inutilizingdatasets,weadheretothe
intendedpurposesoutlinedinpreviousworks. TheReferences OrHonovich,ThomasScialom,OmerLevy,andTimo
Schick. 2022. Unnatural instructions: Tuning lan-
AdrianGBarnett,JoliekeCvanderPols,andAnnetteJ
guagemodelswith(almost)nohumanlabor.
Dobson. 2004. Regression to the mean: what it is
and how to deal with it. International Journal of
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Epidemiology,34(1):215–220.
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen.2021. Lora: Low-rankadaptationof
Bo-Christer Björk and David Solomon. 2013. The
largelanguagemodels.
publishingdelayinscholarlypeer-reviewedjournals.
JournalofInformetrics,7(4):914–923.
Dongyeop Kang, Waleed Ammar, Bhavana Dalvi,
LutzBornmannandRuedigerMutz.2014. Growthrates MadeleinevanZuylen,SebastianKohlmeier,Eduard
ofmodernscience: Abibliometricanalysisbasedon Hovy,andRoySchwartz.2018a. Adatasetofpeer
thenumberofpublicationsandcitedreferences. reviews (PeerRead): Collection, insights and NLP
applications. InProceedingsofthe2018Conference
Yen-ChunChenandMohitBansal.2018. Fastabstrac- oftheNorthAmericanChapteroftheAssociationfor
tivesummarizationwithreinforce-selectedsentence ComputationalLinguistics: HumanLanguageTech-
rewriting. InProceedingsofthe56thAnnualMeeting nologies,Volume1(LongPapers),pages1647–1661,
oftheAssociationforComputationalLinguistics(Vol- NewOrleans,Louisiana.AssociationforComputa-
ume 1: Long Papers), pages 675–686, Melbourne, tionalLinguistics.
Australia.AssociationforComputationalLinguistics.
Dongyeop Kang, Waleed Ammar, Bhavana Dalvi,
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
MadeleinevanZuylen,SebastianKohlmeier,Eduard
ZhijianLiu,SongHan,andJiayaJia.2023. Longlora:
Hovy,andRoySchwartz.2018b. Adatasetofpeer
Efficientfine-tuningoflong-contextlargelanguage
reviews(peerread): Collection,insightsandnlpap-
models.
plications.
Arman Cohan, Sergey Feldman, Iz Beltagy, Doug
Jacalyn Kelly, Tara Sadeghieh, and Khosrow Adeli.
Downey, and Daniel S. Weld. 2020. Specter:
2014. Peer review in scientific publications: Ben-
Document-level representation learning using
efits, critiques, & a survival guide. EJIFCC, page
citation-informedtransformers.
227–243.
TriDao.2023. Flashattention-2: Fasterattentionwith
betterparallelismandworkpartitioning. AriKobren,BarnaSaha,andAndrewMcCallum.2019.
Papermatchingwithlocalfairnessconstraints.
QingxiuDong,LeiLi,DamaiDai,CeZheng,Zhiyong
Wu,BaobaoChang,XuSun,JingjingXu,LeiLi,and DiegoKozlowski,VincentLarivière,CassidyR.Sug-
ZhifangSui.2023. Asurveyonin-contextlearning. imoto, and Thema Monroe-White. 2022. Intersec-
tional inequalities in science. Proceedings of the
Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao NationalAcademyofSciences,119(2):e2113067119.
Jiang, and Graham Neubig. 2021. GSum: A gen-
eralframeworkforguidedneuralabstractivesumma- CaroleJ.Lee,CassidyR.Sugimoto,GuoZhang,and
rization. InProceedingsofthe2021Conferenceof BlaiseCronin.2012. Biasinpeerreview. ASIS&T.
theNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech- XianLi,PingYu,ChuntingZhou,TimoSchick,Luke
nologies,pages4830–4842,Online.Associationfor Zettlemoyer, Omer Levy, Jason Weston, and Mike
ComputationalLinguistics. Lewis.2023. Self-alignmentwithinstructionback-
translation.
NilsDycke,IliaKuznetsov,andIrynaGurevych.2023.
NLPeer: A unified resource for the computational
Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu
studyofpeerreview. InProceedingsofthe61stAn-
Wang,DaisyDing,XinyuYang,KailasVodrahalli,
nualMeetingoftheAssociationforComputational
SiyuHe,DanielSmith,YianYin,DanielMcFarland,
Linguistics (Volume 1: Long Papers), pages 5049–
and James Zou. 2023. Can large language models
5073, Toronto, Canada. Association for Computa-
provideusefulfeedbackonresearchpapers? alarge-
tionalLinguistics.
scaleempiricalanalysis.
Tomáš Foltýnek, Norman Meuschke, and Bela Gipp.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
2019. Academicplagiarismdetection: Asystematic
evaluationofsummariesusingn-gramco-occurrence
literaturereview. ACMComput.Surv.,52(6).
statistics. InProceedingsofthe2003HumanLan-
SebastianGehrmann,YuntianDeng,andAlexanderM. guageTechnologyConferenceoftheNorthAmerican
Rush.2018. Bottom-upabstractivesummarization. Chapter of the Association for Computational Lin-
guistics,pages150–157.
TirthankarGhosal,RaviSonam,AsifEkbal,Sriparna
Saha,andPushpakBhattacharyya.2019. Isthepa- Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong
perwithinscope? areyoufishingintherightpond? Chen,andXiaodongShi.2023. Moprd:Amultidisci-
In2019ACM/IEEEJointConferenceonDigitalLi- plinaryopenpeerreviewdataset. NeuralComputing
braries(JCDL),pages237–240. andApplications,35(34):24191–24206.RyanLiuandNiharB.Shah.2023. Reviewergpt? an datasetforstructure-controllabletextgeneration. In
exploratorystudyonusinglargelanguagemodelsfor FindingsoftheAssociationforComputationalLin-
paperreviewing. guistics: ACL2022,pages2521–2535,Dublin,Ire-
land.AssociationforComputationalLinguistics.
AmanMadaan, NiketTandon,PrakharGupta,Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, IvanStelmakh,NiharB.Shah,andAartiSingh.2019.
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Peerreview4all: Fair and accurate reviewer assign-
Shashank Gupta, Bodhisattwa Prasad Majumder, mentinpeerreview.
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh,andPeterClark.2023. Self-refine: Iterative
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
refinementwithself-feedback.
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. 2023. Principle-driven self-
RobertK.Merton.1968. Themattheweffectinscience.
alignmentoflanguagemodelsfromscratchwithmin-
Science,159(3810):56–63.
imalhumansupervision.
Mathias Wullum Nielsen and Jens Peter Andersen.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
2021. Global citation inequality is on the rise.
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Proceedings of the National Academy of Sciences,
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
118(7):e2012208118.
Bhosale,DanBikel,LukasBlecher,CristianCanton
OpenAI.2023. Gpt-4technicalreport. Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
LongOuyang,JeffWu,XuJiang,DiogoAlmeida,Car- CynthiaGao,VedanujGoswami,NamanGoyal,An-
rollL.Wainwright,PamelaMishkin,ChongZhang, thonyHartshorn,SagharHosseini,RuiHou,Hakan
SandhiniAgarwal,KatarinaSlama,AlexRay,John Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
Schulman,JacobHilton,FraserKelton,LukeMiller, IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Maddie Simens, Amanda Askell, Peter Welinder, Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Paul Christiano, Jan Leike, and Ryan Lowe. 2022. anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Traininglanguagemodelstofollowinstructionswith tinet,TodorMihaylov,PushkarMishra,IgorMoly-
humanfeedback. bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
KishorePapineni,SalimRoukos,ToddWard,andWei- Ruan Silva, Eric Michael Smith, Ranjan Subrama-
JingZhu.2002. Bleu: Amethodforautomaticevalu- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
ationofmachinetranslation. InProceedingsofthe lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
40th Annual Meeting on Association for Computa- ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
tional Linguistics, ACL ’02, page 311–318, USA. Melanie Kambadur, Sharan Narang, Aurelien Ro-
AssociationforComputationalLinguistics. driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023. Llama2: Openfoundationandfine-
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, tunedchatmodels.
and Yuxiong He. 2020. Deepspeed: System opti-
mizationsenabletrainingdeeplearningmodelswith
YizhongWang,YeganehKordi,SwaroopMishra,Alisa
over100billionparameters. InProceedingsofthe
Liu,NoahA.Smith,DanielKhashabi,andHannaneh
26th ACM SIGKDD International Conference on
Hajishirzi. 2023. Self-instruct: Aligning language
Knowledge Discovery & Data Mining, KDD ’20,
modelswithself-generatedinstructions.
page3505–3506,NewYork,NY,USA.Association
forComputingMachinery.
JasonWei, MaartenBosma, VincentY.Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
VictorSanh,AlbertWebson,ColinRaffel,StephenH.
drew M. Dai, and Quoc V. Le. 2022. Finetuned
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
languagemodelsarezero-shotlearners.
Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Thakker,ShanyaSharmaSharma,ElizaSzczechla,
Bosma,BrianIchter,FeiXia,EdChi,QuocLe,and
TaewoonKim,GunjanChhablani,NihalNayak,De-
DennyZhou.2023. Chain-of-thoughtpromptingelic-
bajyotiDatta,JonathanChang,MikeTian-JianJiang,
itsreasoninginlargelanguagemodels.
HanWang,MatteoManica,ShengShen,ZhengXin
Yong, Harshit Pandey, Rachel Bawden, Thomas
Wang,TrishalaNeeraj,JosRozen,AbheeshtSharma, WentingXiongandDianeLitman.2011. Automatically
Andrea Santilli, Thibault Fevry, Jason Alan Fries, predictingpeer-reviewhelpfulness. InProceedings
RyanTeehan,TaliBers,StellaBiderman,LeoGao, of the 49th Annual Meeting of the Association for
ThomasWolf,andAlexanderM.Rush.2022. Multi- ComputationalLinguistics: HumanLanguageTech-
taskpromptedtrainingenableszero-shottaskgener- nologies, pages 502–507, Portland, Oregon, USA.
alization. AssociationforComputationalLinguistics.
ChenhuiShen,LiyingCheng,RanZhou,LidongBing, WeizheYuan,PengfeiLiu,andGrahamNeubig.2021.
YangYou,andLuoSi.2022. MReD:Ameta-review Canweautomatescientificreviewing?Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Evalu-
atingtextgenerationwithbert.A PromptsforPGE
PromptforGeneration
[INST]«SYS»Youareahelpful,respectfulandhonestassistant. Alwaysanswerashelpfully
aspossible,whilebeingsafe. Youranswersshouldnotincludeanyharmful,unethical,racist,
sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially
unbiased and positive in nature. If a question does not make any sense, or is not factually
coherent, explain why instead of answering something not correct. If you don’t know the
answertoaquestion,pleasedon’tsharefalseinformation. «/SYS»
Analyzing the provided review, identify a set of questions that the reviewer is attempting to
addressregardingthepaperwithoutbeingtoospecific.
Herearesomeexamples:
Review:
[SAMPLEDREVIEWFROMS]
Questionstoaddress:
[SAMPLEDPROMPTFROMS]
Review:
[SAMPLEDREVIEWFROMS]
Questionstoaddress:
[SAMPLEDPROMPTFROMS]
Review:
[SAMPLEDREVIEWFROMS]
Questionstoaddress:
[SAMPLEDPROMPTFROMS]
Review:
[REVIEWFORGENERATION]
Questionstoaddress:[/INST]PromptforEvaluation
[INST]«SYS»Youareahelpful,respectfulandhonestassistant. Alwaysanswerashelpfullyaspossible,
whilebeingsafe. Youranswersshouldnotincludeanyharmful,unethical,racist,sexist,toxic,dangerous,
or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If
a question does not make any sense, or is not factually coherent, explain why instead of answering
somethingnotcorrect. Ifyoudon’tknowtheanswertoaquestion,pleasedon’tsharefalseinformation.
«/SYS»
Below is a set of questions and a candidate answer. Evaluate the quality of the questions. Are the
questionsagoodmatchtothecandidateanswer? Pleaseassignascoreusingthefollowing5-pointscale:
1: Thisscoreindicatesthattheresponsedeviatessignificantlyfromtheinstruction,providinginformation
oraddressingaspectsthatwerenotrequiredorspecified.
2: Thisscoresuggeststhattheresponseislimitedinscope,focusingonasmallsubsetofthequestions
posedintheinstruction. Itdoesnotcomprehensivelycovertheentiresetofquestions.
3: This score indicates that the response covers a substantial portion of the questions outlined in the
instructionbutfallsshortofaddressingallofthem. Itsuggestsamoderatelevelofcompleteness.
4: Thisscoreindicatesthattheresponsecoversmostofthequestions. However,thereissomeirrelevant
informationintheanswerthatisnotaskedbyanyofthequestions.
5: This score indicates that the response is comprehensive, addressing all questions in the instruction
withoutanyirrelevantinformation.
Herearesomeexamples:
Questions:
[EXAMPLEPROMPT]
Answer:
[EXAMPLEREVIEW]
Assessment:
[EXAMPLEASSESSMENT]
Score: [EXAMPLESCORE]
Questions:
[EXAMPLEPROMPT]
Answer:
[EXAMPLEREVIEW]
Assessment:
[EXAMPLEASSESSMENT]
Score: [EXAMPLESCORE]
Questions:
[EXAMPLEPROMPT]
Answer:
[EXAMPLEREVIEW]
Assessment:
[EXAMPLEASSESSMENT]
Score: [EXAMPLESCORE]
Questions:
[PROMPTFOREVALUATION]
Answer:
[REVIEWFOREVALUATION]
Assessment:[/INST]B ExampleReview-PromptPair
Review SummaryOfThePaper
Thispaperintroducesneuralmatchingfieldsintosemanticcorrespondence. Tothebestmy
knowledge, this approach should be the first method to do the task using implicit neural
representation. There are two problems: the computation for 4D matching field and the
inferenceefficiency. Authorsprovideeffectmethodtoaddressthetwoproblems.
StrengthsAndWeaknesses
Thispaperemploysimplicitneuralrepresentationtodosemanticcorrespondence. Thisshould
bethemajorcontribution. Accordingtothestatementofauthors,Icanfollowtheideaeasily
andthisideashouldwork. Thedisadvantageofthisworkistheexperiments. Therearetoo
manyquantitativecomparisons. Accordingtothedata,theperformanceofthismethodseems
OK.However,authorsshouldprovidemorevisualexperimentstoconvincereaders.
Questions
Ionlyhaveoneconcern. TraditionalImplicitNeuralRepresentationmethodsuchasLIIFand
NeRFrecordsimagesintotheweightsofneuralnetwork. Oneneuralnetworkrepresentsone
imageoronescene. DoesNeMFtakeaneuralnetworktorepresentasemanticcorrespondence
oramatchingcost. Ifso,howmuchtimewillyourmethodcosttotrainanetwork? Ifnotso,
whatisthedifferencebetweenyourmethodandothersemanticcorrespondencemethods.
Limitations
Accordingtomyunderstand,NeMFtakesanetworktorepresentamatchingcost. Inpractice,
peopleneedamethodtocomputedifferentmatchingcostfordifferentimagepairs. Howdoes
NeMFtodealwiththissituation.
Questions 1. Whatisthefocusandcontributionofthepaperonsemanticcorrespondence?
toaddress 2. Whatarethestrengthsoftheproposedapproachintermsofneuralrepresentation?
3. Whataretheweaknessesfortheexperimentsection?
4. Doyouhaveanyconcernsonthesemanticcorrespondencerepresentation?
5. WhatarethelimitationsregardingtheNeMFapproachonmatchingcostrepresentation?C In-ContextExampleforEvaluation
Questions 1. Whatisthemaincontributionofthepaperondictionarylearning?
2. Whatarethestrengthsofthepaperinthetheoreticalanalysis?
3. Doyouhaveanyquestionsregardingtheassumptions,theorems,andalgorithmofthepaper?
4. Couldyouaccessthereproducibilityofthepaper?
Answer Thepaperproposesanalternatingminimizationalgorithmfordictionarylearning,andtheoreti-
calguaranteesarealsogiven. Ineachstepthealgorithmfirstusesanl1,l2andl_inftyalgorithm
withthresholdingtogetanestimateofthecoefficients,andthenuseanothergradientstepto
updatethedictionary.
Tometwoshiningpointsofthepaper:
1. Guaranteeholdsfortheovercompletedictionary.
2. Improvedthesparsitylevelrequirementbyafactoroflogd.
ObviouslytheNIPSformatistooshortfortheargumentstheauthorsaremaking,andalotof
detailsaremovedtotheappendix. DuetotimelimitIcannotreadallthedetailsoftheproof.
Belowaresomequestions:
1. InA1youhaveamu-incoherenceassumption,butmuisnotshowninyourtheorem3. Isit
hiddensomewhere?
2. InassumptionB1youmentioned,andIagreethatthereisafastrandominitializationso
thattheconditionholds. Canyougivesomedetailsaboutyourinitializationprocedureand
guarantees?
3. HowdoyouhandlethepermutationinvarianceofA?
4. Inyouralgorithm1,line3,theMUSalgorithmhasareturn,butinyourdefinition(equation
2),thereturnisnotspecified. Actuallythereturnedshouldbethetainsteadof(theta,t,u).
5. “(w_k^tisthek^thcovariateatstept)”? Whyw_k^tiscalledthek^thcovariate?
6. Anysimulationresultverifyingyourconvergencerate?
Assessment Theansweraddressesthefirstquestionbysummarizingthemaincontributionofthepaper.
For the second question, the answer gives two strong points of the paper in its theoretical
justifications. The answer address the third question by providing six different questions
converingtheassumptions,theorems,andthealgorithmofthepaper. However,theanswerfails
toaddressthefourthquestion.
Score Sincetheanswerfailstoaddressallofthequestions,itreceivesascoreof3.D GeneratedAspectPromptsandReviewforthisPaper
Aspect Whatisthefocusandcontributionofthepaperonpeerreviewgeneration?
Prompts Whatarethestrengthsoftheproposedapproach,particularlyintermsofitstwo-stagearchitec-
ture?
Whataretheweaknessesofthepaperregardingitsexperimentalsetupandproposedmetrics?
Doyouhaveanyconcernsorsuggestionsforimprovingthepaper’sclarityandorganization?
Review Summary Of the Paper. This paper proposes a two-stage review generation framework
that incorporates aspect prompts. They design a prompt generation with evaluation (PGE)
pipelinetogenerateavarietyofhigh-qualityaspectprompts,andtheymakethisnewdataset
available. They evaluate review quality, specificity, and aspect coverage as key properties
ofthegeneratedreviews. Theyprovideextensiveablationexperimentsthatidentifyhowfar
eachnovelcontributionsintheirapproachcontributestoimprovedperformance. Finally,they
concludebyhighlightingtheadvantagesofleveragingboththefullcontextofthepaperandthe
aspectprompt.
Strengths. Thepaperclearlystatestheproblemofexistingmethodsthatonlygenerategeneric
reviewsandlacksspecificityandcontrol. Theauthorsproposeatwo-stagearchitecturethat
explicitlymodelsthedistributionofpossibleaspectsthatthereviewmayaddress. Theyshow
that this leads to more detailed reviews that better cover the range of aspects that human
reviewersidentifyinthedraft.
Weaknesses. The authors evaluate the effectiveness of the proposed method with several
baselines. However,thebaselinesarenotveryconvincing. Thepaperlacksclearmotivationfor
whytheyuseatwo-stagearchitecture. Thepaperneedstoprovidemoredetailsonhowthey
evaluatethegeneratedprompts.E DatasetDetails F HyperparameterDetails
E.1 PaperContents REVIEWER2andSINGLEShaveacontextlength
of32,768whileothermodelshavea4,096context
• title: titleofthepaper
length. Allofthemodelsexcluding SINGLES-E0
• authors: listofauthornames
are fine-tuned with 8 A6000 GPUs using Deep-
• emails: listofauthoremails Speed (Rasley et al., 2020) stage 2, batch size
64, gradient accumulation 8, and warm-up steps
• sections: listofsectionsofthepaper
100 for 2 epochs. We use the AdamW opti-
– heading: headingofthesection
mizer with a learning rate 1e−5 searched from
– text: textofthesection [5e−6,1e−5,2e−5,5e−5,1e−4,2e−4].
• references: listofreferencesofthepaper
– title: titleofthereference
– author: listofauthornamesofthereference
– venue: venueofthereference
– citeRegEx: citationexpression
– shortCiteRegEx: shortcitationexpression
– year: publicationyearofthereference
• referenceMentions: thelocationofthereference
inthepaper
– referenceID:numericalreferenceid
– context: contextofthereferenceinthepa-
per
– startOffset: startindexofthecontext
– endOffset: endindexofthecontext
• year: yearofpublication
• abstractText: abstractofthepaper
E.2 MetadataContents
• id: uniqueidofthepaper
• conference: venueforthepaper
• decision: final decision for the paper (ac-
cept/reject)
• url: linktothePDFofthepaper
• review_url: linktothereviewofthepaper
• title: titleofthepaper
• authors: listoftheauthorsofthepaper