The Price of Adaptivity in Stochastic Convex Optimization
Yair Carmon Oliver Hinder
ycarmon@tauex.tau.ac.il ohinder@pitt.edu
Abstract
We prove impossibility results for adaptivity in non-smooth stochastic convex optimization.
Given a set of problem parameters we wish to adapt to, we define a “price of adaptivity” (PoA)
that,roughlyspeaking,measuresthemultiplicativeincreaseinsuboptimalityduetouncertainty
intheseparameters. Whentheinitialdistancetotheoptimumisunknownbutagradientnorm
bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and
double-logarithmic for median suboptimality. When there is uncertainty in both distance and
gradientnorm,weshowthatthePoAmustbepolynomialinthelevelofuncertainty. Ourlower
boundsnearlymatchexistingupperbounds,andestablishthatthereisnoparameter-freelunch.
1 Introduction
Stochasticoptimizationmethodsinmodernmachinelearningoftenrequirecarefullytuningsensitive
algorithmic parameters at significant cost in time, computation, and expertise. This reality has led
to sustained interest in developing adaptive (or parameter-free) algorithms that require minimal or
notuning[5,7,11,20,21,23,24,27,33–37,41,43–45]. However,abasictheoreticalquestionremains
open: Are existing methods “as adaptive as possible,” or is there substantial room for improvement?
Put differently, is there a fundamental price to be paid (in terms of rate of convergence) for not
knowing the problem parameters in advance?
To address these questions, we must formally define what it means for an adaptive algorithm to
be efficient. The standard notion of minimax optimality [1] does not suffice, since it does not con-
strain the algorithm be agnostic to the parameters defining the function class; stochastic gradient
descent (SGD) is in many cases minimax optimal, but its step size requires problem-specific tuning.
To motivate our solution, we observe that guarantees for adaptive algorithms admit the following
interpretation: assuming that the input problem satisfies certain assumptions (e.g., Lipschitz conti-
nuity, smoothness, etc.) the adaptive algorithm attains performance close to the best performance
that is possible to guarantee given only these assumptions.
For example, consider the online optimization algorithm of McMahan and Orabona [25], which
requires an upper bound L on the norm of stochastic gradients but makes only a weak assumption
on how far the initialization x (say, the origin) is from the optimum x⋆: the algorithm has an
0
additional parameter r which may viewed as a very conservative lower bound on R = ∥x⋆ −x ∥.
ε 0
Ther parametermaysimplybesettoaverysmallnumber(say,10−8)andisnotmeanttobetuned.
ε
Applying the algorithm to stochastic convex optimization (SCO) via online-to-batch conversion,
afterT
stepsweareguaranteedanexpectedoptimalitygapboundofO(cid:0)L(R √+rε)(cid:112)
log((R+r ε)/r
ε)(cid:1)
.
T
To equivalently state this guarantee, consider the class IL,R of SCO problems with L-Lipschitz
Lip
sample functions and solution norm at most R. Then, for any problem in IL,R such that r ≤ R,
Lip ε
the algorithm [25] guarantee error at most a factor O((cid:112) log(R/r ε)) larger than √LR, the minimax
T
optimal error for the class IL,R.
Lip
1
4202
beF
61
]CO.htam[
1v89801.2042:viXraWe propose a more concise way for stating adaptivity guarantees such as the above, allowing
us to argue about their optimality. Consider a large collection of problem classes, which we call a
meta-class and denote by M. For each problem class I ∈ M we quantify the competitive ratio
between an adaptive algorithm’s (worst-case) suboptimality, and the best possible suboptimality
whentheclassisknowninadvance. Inspiredbythenotionof“priceofanarchy” inalgorithmicgame
theory [39], we define the price of adaptivity (PoA) to be the highest competitive ratio incurred by
any element in the meta-class.
Intheproposedframework, theadaptivityguaranteeofMcMahanandOrabona[25]fitsintoone
sentence: for any ρ > 2 and the meta-class M1,ρ = (cid:8) I1,r(cid:9) the PoA (with respect to expected
√ Lip Lip r∈[1,ρ]
suboptimality) is O( logρ). Such logarithmic dependence on ρ indicates strong robustness to
uncertainty in the distance between the initial point and the optimum (a PoA of 1 corresponds
to perfect robustness). However, for the larger meta-class Mℓ,ρ = (cid:8) Il,r(cid:9) which also
Lip √Lip l∈[1,ℓ],r∈[1,ρ]
captures uncertainty in the Lipschitz constant, the PoA of [25] is O(ℓ logρ). Such polynomial
dependence on ℓ indicates sensitivity to uncertainty in the problem’s Lipschitz constants.
Table1surveys—intermsofPoA—thestate-of-the-artforadaptivealgorithmsforSCOwithun-
knowninitialdistancetooptimalityand/orstochasticgradient(i.e.,Lipschitz)bound,corresponding
to the meta-class Mℓ,ρ described above. We also consider the meta-class Mℓ,ρ corresponding to
Lip SM-Lip
second-momentLipschitzproblems,wherethestochasticgradientboundappliesonlytotheirsecond
Meta-class Suboptimality Algorithm / Theorem Price of Adaptivity
√
(cid:0) (cid:1)
McMahan and Orabona [25] O logρ
expected √
(cid:0) (cid:1)
Theorem 1 Ω logρ
M1,ρ
Lip (cid:16) (cid:16) (cid:17)(cid:17)
log(ρT)
Carmon and Hinder [5] O log
1−δ quantile δ
(cid:18)(cid:114) (cid:19)
(cid:16) (cid:17)
Theorem 2 Ω log logρ /log 1 †
δ δ
(cid:16)(cid:112) ℓlog(ρ)(cid:17)
Cutkosky and Orabona [11] O log(ρ)+ √
expected T
(cid:16) (cid:17)
Mℓ,ρ Cutkosky [9] O(cid:101) √ρ
Lip T
(cid:16)(cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
Carmon and Hinder [5] O 1+ √ℓ log2 log(ρT)
1−δ quantile T δ
(cid:16) (cid:113) (cid:17)
Theorem 3 Ω min √{ρ,ℓ} log 1
T δ
√
(cid:0) (cid:1)
SGD O ρℓ
expected
Adaptive SGD [18, 24] O(ρ)
Mℓ,ρ
SM-Lip
Zhang and Cutkosky [44] O(cid:101)(ℓ)
1−δ quantile
Theorem 3 Ω(min{ρ,ℓ})
Table 1. A summary of lower and upper bounds on the price of adaptivity. Here ℓ is the uncertainty
in the stochastic gradient bound (probability 1 for Mℓ,ρ or second-moment for Mℓ,ρ ) and ρ is
Lip SM-Lip
the uncertainty in the initial distance to optimality. See Sections 2 and 3 for formal definitions and
Appendix A for translation of published result to PoA bounds. In the setting we consider the PoA
√ √
is at least 1 and at most O( T); for brevity we omit the minimum with T and maximum with
1 in the lower and upper bounds, respectively. The O(cid:101)(·) notation hides poly-logarithmic factors.
† This lower bound holds for stochastic first order algorithms, while the rest hold for all stochastic
optimization algorithms (see Section 5).
2moment (instead of applying with probability 1), and we analyze two error measures: expected and
quantile. Notably, for ρ = O(1) it is possible to be completely adaptive to the Lipschitz constant
√
(PoA independent of ℓ), and for ℓ = O( T) it is possible to obtain PoA logarithmic in ρ. However,
even when ℓ = 1 all known PoA bounds have some dependence on ρ. Moreover, no algorithm has
sub-polynomial PoA in both ℓ and ρ. Are these shortcomings evidence of a fundamental cost of not
knowing the problem parameters in advance?
In this work we answer this question in the affirmative, proving three PoA lower bounds high-
lighted in Table 1. Our first lower bound shows that the expected-suboptimality PoA is always
logarithmic in ρ, establishing that [25] is optimal when the Lipschitz constant is known. Taken
together with the double-logarithmic high-probability PoA upper bound [5], our result has the
counter-intuitive implication that parameter-free high probability bounds are fundamentally better
than in-expectation bounds; see additional discussion in Section 5. Our second lower bound shows
that for constant-probability suboptimality, double-logarithmic dependence on ρ is unavoidable,
establishing near-optimality of [5]. Finally, our third lower bound shows that sub-polyonimal PoA
in both ℓ and ρ is impossible. For second-moment Lipschitz problems we give a lower bound of
Ω(min{ℓ,ρ}). This proves that any robustness to distance comes at a cost of sensitivity to Lips-
chitz constant, and vice versa, and establishes that a combination of adaptive SGD and the method
of Zhang and Cutkosky [44] is nearly optimal. For problems with bounded stochastic gradients
√
our lower bound is smaller a factor of T, but is still nearly tight, with a combination of Carmon
and Hinder [5] and Cutkosky [9] providing nearly-matching upper bounds. Altogether, these re-
sults provide a nearly complete picture of the price of adaptivity in non-smooth stochastic convex
optimization.
As a side result, we prove matching upper and lower bounds on the minimax suboptimality
quantiles for Lipschitz and second-moment Lipschitz problems, respectively (see Appendix B). In
other words, for known problem parameters, whether the noise is light-tailed or heavy-tailed (with
bounded second moment) has virtually no effect on the difficulty of obtaining high-probability
guarantees in non-smooth stochastic convex optimization. Despite being a very basic result, we
could not locate it the literature.
Paper organization. Theremainderoftheintroductionsurveysourprooftechniquesandrelated
work. Section 2 introduces key notation and definitions. Section 3 formally defines the price of
adaptivity. Section 4 proves our lower bounds. Section 5 discusses conclusions and open problems.
1.1 Overview of proof techniques
Formally defining the PoA allows us to leverage a primary technique for proving lower bounds in
stochastic optimization: reduction from optimization to hypothesis testing [1, 4, 13]. That is, we
carefully construct subsets of M such that, on the one hand, it is information-theoretically impos-
sible to reliably distinguish elements of these subsets, but on the other hand failing to distinguish
elements incurs higher suboptimality that what is achievable with known problem parameters. The
PoA perspective adds a new twist on this old technique by altering the correspondence between
optimization error to statistical risk, requiring novel analyses and constructions. Below, we briefly
describehoweachofourthreeconstructions(whichareallone-dimensional)embedstatisticalprob-
lems. We then discuss our technique for establishing information-theoretic hardness.
√
Theorem 1. To prove our Ω( logρ) lower bound on expected-suboptimality PoA, we apply a
reduction to testing the bias of a coin. That is, we construct problems where finding the optimum
to sufficient accuracy requires correctly deciding whether T coin flip results came from a coin
3biased by +ε/2 or −ε/2 away from the uniform distribution, for a parameter ε of our choosing.
When the bias is positive, the minimizer is distance ρ away from the initial point, and when
the bias is negative its distance is below 1. This construction lower bounds the PoA by a term
√
proportional to ε T(ρ·p +p ), where p is the probability of mistaking a negative bias for
+|− −|+ +|−
a positive one, and analogously for p . Using a careful information-theoretic argument, we show
−|+
(cid:112)
that the weighted error ρ·p +p = Ω(1) for ε = Ω( log(ρ)/T), establishing the claimed lower
+|− −|+
bound. By contrast, in the classical SCO lower bound the excess suboptimality is proportional to
√ √
ε T(p +p ), and the unweighted error satisfies p +p = Ω(1) only for ε = O(1/ T).
+|− −|+ +|− −|+
√
Theorem 2. Our proof of the Ω( loglogρ) lower bound on constant-probability PoA is a reduc-
tion to the noisy binary search problem [22]: consider n coins such that coin i has bias −ε/2 for
i ≤ i⋆ and bias ε/2 for i > i⋆, and for T steps we get to choose a coin, flip it, and observe the
outcome. We let n := ⌈logρ⌉ and construct a set of optimization problems such that the PoA is
√
proportional to ε TP(ˆi ̸= i⋆), whereˆi is any estimator of i⋆ given the observed coin flips, and i⋆ is
drawn uniformly from [n]. We provide a short self-contained proof of a lower bound from Karp and
Kleinberg[22]showingthatP(ˆi ̸= i⋆) = Ω(1)forε = Ω((cid:112) log(n)/T)yieldingourdouble-logarithmic
lower bound (as n = Ω(logρ)). To our knowledge, this is the first stochastic optimization lower
bound to leverage the hardness of noisy binary search. Notably, this lower bound holds for stochas-
tic first-order algorithms that only access each sample function by computing a single gradient.
In contrast, the other lower bounds we prove hold for the broader class of stochastic optimization
algorithms, that are allowed unrestricted access to each sample function.
√
Theorem3. Finally,toproveourΩ(min{ρ,ℓ}/ T)andΩ(min{ρ,ℓ})lowerboundsforprobability
1 and second-moment bounded stochastic gradients, respectively, we hide the uncertain parameters
in a rare event. To do so, we construct two problem instances such that every point x has an
√ √
excess suboptimality factor of at least ρ/ T for one function or ℓ/ T for the other function.
Moreover, with constant probability both functions produce exactly the same observations, forcing
√
any algorithm to pay an Ω(min{ρ,ℓ}/ T) PoA. The first problem instance we construct is simply
α|x−ρ| with probability 1, for some α ≤ 1. For the second instance, there is a λ/T probability
of instead observing 2αT|x| where λ = Θ(log 1) guarantees that with probability at least δ we
λ δ
only observe α|x−ρ|. Carefully choosing α to satisfy the Lipschitz or second-moment Lipschitz
constraints yields the claimed lower bounds.
Information-theoretic hardness via bit-counting (Appendix D). To prove Theorem 1, we
need to establish that the weighted error ρ·p +p = Ω(1), with p and p defined above.
+|− −|+ +|− −|+
Writing δ = 1/(1+ρ), this is equivalent to showing p = (1−δ)p +δp = Ω(δ), where p is
err +|− −|+ err
the probability of mis-estimating the coin’s bias when it is drawn from a Bernoulli(δ) distribution.
Our crucial observation is that each observation can reveal at most O(δε2) bits of information on
the bias (formally, this is an elementary mutual information bound). Intuitively, as long as the
accumulated information O(δε2T) does not reach the entropy of the bias, H(Bernoulli(δ)) ≥ δlog 1,
δ
we cannot hope to estimate the bias with nontrivial accuracy; we formulate this intuition using
(cid:16)(cid:113) (cid:17) (cid:16)(cid:113) (cid:17)
H(Bernoulli(δ)) log(1/δ)
Fano’s inequality [14]. Thus, we may take ε = Ω ≥ Ω while still
δT T
ensuring p = Ω(δ) as required. As it happens, the same argument also proves our lower bound
err
for the minimax quantile error for known problem parameters (Theorem 5 in the appendix).
Our proof of the noisy binary search hardness at the heart of Theorem 2 follows along similar
lines. There, each observation yields O(ε2) bits of information, but the entropy of the estimand i⋆
is logn, and therefore we intuitively expect non-trivial error probability as long as ε2T ≤ O(logn);
4(cid:0)(cid:112) (cid:1)
Fano’s inequality readily confirms this intuition. Thus, we make take ε = Ω (logn)/T , as
required.
1.2 Related work
We now discuss the price of adaptivity in two settings adjacent to ours: online convex optimization
and non-stochastic optimization.
Online convex optimization. A long line of work starting from Streeter and McMahan [42]
studies parameter-free online optimization methods that require little or no tuning [see, e.g., 7, 9,
11, 21, 24, 27, 33, 35, 36, 41, 45]. These methods directly imply PoA upper bounds via online-to-
batchconversion,andthisishowmanyoftheupperboundsinTable1arederived(seeAppendixA).
Conversely, our PoA lower bounds imply regret lower bounds for parameter-free online algorithms.
The online parameter-free literature also has a number of lower bounds, which do not have
immediateimplicationsonstochasticconvexoptimization. Nevertheless,Theorem2ofOrabona[32]
√
isanalogoustoourΩ( logρ)lowerbound(Theorem1),showingthatforeveryonlinealgorithmand
ρ > 0 there exists u with ∥u∥ = ρ and a sequence of vectors with norm 1 for which u incurs regret
√
(cid:0) (cid:1)
Ω ∥u∥ T logρ . In terms of proof techniques the two lower bounds appear to be fairly different,
with [32] using the probabilistic method, uniformly drawn Rademacher variables, and a careful tail
bound, whereas we perform a reduction to hypothesis testing and appeal to information-theoretic
hardness. Other online parameter-free lower bounds [10, 26] are not directly comparable to our
results, since it is not clear how to translate them into statements on competitive ratios.
Non-stochastic optimization. In smooth optimization with exact gradients, adaptivity can
come almost for free. In particular, line search techniques such as [2] and [6] converge with optimal
complexity up to an additive logarithmic factor, implying PoA that tends to 1 as T grows. In the
non-smooth setting, Polyak’s method [38] attains optimal rates if we assume function value access
and knowledge of the optimum function value. With only a lower bound on the optimum value, a
variant of Polyak’s method attains logarithmic PoA [19]. Without further assumptions, however,
(cid:16)(cid:112) (cid:17)
the best PoA bound in the non-smooth setting is O loglog(ρT) , which follows from setting
η = ∥g ∥−1T−1 in [5, Theorem 7] and returning the algorithm’s output or the initial point if the
ϵ 0
latter has lower objective value.
We are not aware of general PoA lower bounds in the non-stochastic settings. Mishchenko
and Defazio [28, Section 4] give a lower bound on a restricted class of algorithms (excluding [5]
for example), that only applies for T = O(logρ), where it holds essentially by definition of the
algorithm class.
2 Preliminaries
This section formally defines the basic building blocks of our paper: stochastic optimization prob-
lems, algorithms, error metrics, and minimax rates.
Notation. Throughout the paper, X denotes a closed convex set (our lower bounds use X = R)
and ∥ · ∥ denotes the Euclidean norm of a vector, and the infimum Euclidean norm in a set of
vectors (or infinity if the set is empty). For convex function h : X → R we write (with minor
abuse of notation) ∇h(x) for an arbitrary subgradient of h at x, and let ∂h(x) denote the set of
all subgradients at x. We say that h is L-Lipschitz if |h(x)−h(y)| ≤ L∥x−y∥ for all u,v ∈ X or,
5equivalently, ∥∇h(x)∥ ≤ L for almost-all x ∈ X. We use [n] := {1,...,n} and 1 for the indicator
{·}
function. Throughout, log(·) denotes the natural logarithm and N starts from 1.
Stochastic optimization (SO) problems. A SO problem instance is a tuple (f,P) containing
a distribution P over sample space S and a sample objective f : X ×S → R. We write
F (x) := E f(x;S) and X⋆ := argminF (x)
f,P S∼P f,P f,P
x∈X
forthepopulationobjective(whichwewishtominimize)anditssetofminimizers, respectively. Let
I denote a class of SO problem instances. We consider two fundamental classes of convex functions
with a minimizer at most R away from the origin.1 The first class contains problems with bounded
stochastic gradient norm (i.e., where each sample function is L-Lipschitz),
IL,R := (cid:8) (f,P) | f(·;s) is convex and L-Lipschitz for all s ∈ S, and (cid:13) (cid:13)X⋆ (cid:13) (cid:13) ≤ R(cid:9) .
Lip f,P
The second class contains problems with bounded stochastic gradient second moment,
I SL M,R
-Lip
:= (cid:8) (f,P) | f(·;s) is convex ∀s ∈ S and E S∼P∥∇f(·;S)∥2 ≤ L2, and (cid:13) (cid:13)X f⋆ ,P(cid:13) (cid:13) ≤ R(cid:9) .
Clearly, IL,R ⊂ IL,R , with IL,R also including problems with heavy-tailed gradient noise.
Lip SM-Lip SM-Lip
Optimization algorithms. GeneralSOalgorithmshaveunrestrictedaccesstothefunctionf(·;·)
but observe P only through samples S ,S ,... ∼iid P. The output of general SO algorithms is
1 2
therefore of the form
(cid:0) (cid:1)
x = alg f(·;S ),...,f(·;S );ξ ∈ X,
t t 1 T
where alg is some measurable mapping and ξ ∼ Unif([0,1]) ⊥ S ,...,S allows randomization.
t 1 T
Stochastic first-order (SFO) optimization algorithms are SO algorithm that depend on f only
through gradients observed at past iterates. That is, their output sequence takes the form
x = alg (∇f(x ;S ),...,∇f(x ;S );ξ) ∈ X,
t t 0 1 t−1 t
with ξ ∼ Unif([0,1]) ⊥ S ,...,S as before. We write A and A for the sets of all SO and all
1 T SO SFO
SFO algorithms, respectively, so that A ⊂ A . All the algorithms from Table 1 are in A .
SFO SO SFO
Error metrics. For an algorithm alg, SO instance (f,P), and budget T, we let x denote the
T
algorithm’s output given T samples and define the expected error to be
ErrE (alg,(f,P)) := EF (x )− inf F (x⋆),
T f,P T f,P
x⋆∈X
where the expectation is with respect to x and hence S ,...,S and ξ. We similarly define the
T 1 T
high-probability error at confidence level δ as
Errδ(alg,(f,P)) := Q (F (x ))− inf F (x⋆),
T 1−δ f,P T f,P
x⋆∈X
where Q (Y) = min{y : P(Y ≤ y) ≥ p} denotes the p’th quantile of random variable Y [3, Ch. 3].
p
Throughout, we drop the superscripts E and δ when making statements that apply to both.
1This implicitly, and without loss of generality, assumes that optimization methods are initialized at x =0.
0
6Minimax error. Given an instance class I and an algorithm alg, we overload the notation above
to denote worst-case error over the class,
Err (alg,I) := sup Err (alg,(f,P)).
T T
(f,P)∈I
GivenanalgorithmclassA,wefurtheroverloadournotationtoexpressminimaxoptimalworst-case
error
Err (A,I) := inf Err (alg,I) = inf sup Err (alg,(f,P)). (1)
T T T
alg∈A alg∈A(f,P)∈I
Theminimaxerrorfortheinstanceandalgorithmclassesdefinedaboveisfairlywell-understood.
However, for quantile error we could not locate bounds that completely characterize the optimal
dependence on 1/δ, so we derive them here. See Appendix B for proof of the following.
Propostion 1. For all R,L > 0, instance class I ∈ {IL,R,IL,R } and A ∈ {A ,A }, we have
Lip SM-Lip SO SFO
(cid:32) (cid:33)
(cid:16) (cid:17)
ErrE (A,I) = Θ √LR . Moreover, for any δ ∈ (0, 1) we have Errδ(A,I) = Θ LR .
T T 2 T (cid:113) 1+T/log1
δ
3 Defining the price of adaptivity (PoA)
With the standard components in place, we now define and enumerate basic properties of the novel
part of our framework: the price of adaptivity (PoA). We begin with the notion of a meta-class,
i.e., a class of classes, which we denote by M. For any ℓ,ρ ≥ 1, we define
(cid:110) (cid:111)
Mℓ,ρ := IL,R | L ∈ [1,ℓ] and R ∈ [1,ρ]
Lip Lip
to be the meta-class of Lipschitz SCO problems with uncertainty ℓ in the Lipschitz constant and
uncertainty ρ in the distance to optimality. Taking the lower Lipschitz and distance bound to be
1 does not compromise generality (see Proposition 2.6 below). We similarly define Mℓ,ρ by
SM-Lip
replacing IL,R with IL,R in the above display.
Lip SM-Lip
The PoA of algorithm alg competing against all SO algorithms on meta-class M is
Err (alg,I)
PoA (alg,M) := sup T .
T
Err (A ,I)
I∈M T SO
We use PoAE (alg,M) and PoAδ(alg,M) to denote the price of adaptivity w.r.t. expected and
T T
quantile error, respectively (i.e., with Err standing for ErrE and Errδ, respectively). We may also
T T T
consider the price of adaptivity in competing against algorithms in A ⊂ A , which we denote
SO
Err (alg,I)
PoA (alg,M;A) := sup T . (2)
T
Err (A,I)
I∈M T
Our strategy to lower bound the PoA in this paper is to carefully construct a collection of n ‘hard’
problem instances (f ,P ),...,(f ,P ) such that (f ,P ) ∈ I with I ∈ M and then utilize the
1 1 n n k k k k
following observation
Err (alg,I ) Err (alg,(f ,P ))
PoA (alg,M;A) ≥ max T k ≥ max T k k . (3)
T
k∈[n] Err T(A,I k) k∈[n] Err T(A,I k)
7Remark 1 (Information available to algorithms). Our setup implicitly gives algorithms complete
information about the meta-class. For example, for Mℓ,ρ the algorithm has (potentially very loose)
Lip
upper and lower bounds on both the Lipschitz constant L and distance to optimality R. Existing
parameter-free algorithms typically only require a lower bound on R and an upper bound on L.
Providing algorithms with additional information only strengthens our PoA lower bounds.
We conclude this section with basic properties of the PoA (see Appendix C for proof).
Propostion 2. The price of adaptivity satisfies the following properties:
1. PoA (alg,M;A) ≥ 1 for all M,A and alg ∈ A.
T
√ (cid:18) (cid:19)
2. PoAE (alg ,M) ≤ O( T) and PoAδ(alg ,M) ≤ O (cid:113) T +1 for M ∈ {Mℓ,ρ,Mℓ,ρ } and
T 0 T 0 log1 Lip SM-Lip
δ
alg that trivially returns x = 0.
0 0
3. PoAE (alg,M) ≥ Ω(δ)PoAδ(alg,M) for any M ∈ {Mℓ,ρ,Mℓ,ρ }, and alg ∈ A .
T T Lip SM-Lip SO
4. PoA (cid:0) alg,Mℓ,ρ(cid:1) ≤ O(1)PoA (cid:0) alg,Mℓ,ρ (cid:1) for every alg ∈ A .
T Lip T SM-Lip SO
5. PoA (alg,M) = Θ(1)PoA (alg,M;A ) for every alg ∈ A and M ∈ {Mℓ,ρ,Mℓ,ρ }.
T T SFO SO Lip SM-Lip
(cid:110) (cid:111)
6. Let Ml1,l2,r1,r2 := IL,R | L ∈ [l ,l ] and R ∈ [r ,r ] . Then, for all alg ∈ A , there exists alg′
Lip Lip 1 2 1 2 SO
s.t. PoA
(cid:0) alg,Ml1,l2,r1,r2(cid:1)
= PoA
(cid:0) alg′,Ml2/l1,r2/r1(cid:1)
. The same holds replacing Lip with SM-Lip.
T Lip T Lip
4 Proof of lower bounds on the price of adaptivity
This section proves our lower bounds; see Section 1.1 for proof sketches and Section 5 for discussion
of the results.
4.1 Logarithmic PoA for expected error and unknown distance
√
Theorem 1. For all T ∈ N, ρ ≥ 1, alg ∈ A we have PoAE(cid:0) alg,M1,ρ(cid:1) ≥ Ω(cid:0) min(cid:8)(cid:112) log(ρ), T(cid:9)(cid:1) .
SO T Lip
Proof. We pick two elements from M1,ρ, namely I = I1,1 and I = I1,ρ. We then pick two
Lip 0 Lip 1 Lip
instances (f,P ) and (f,P ) such that (f,P ) ∈ I for v ∈ {0,1}. From eq. (3), the PoA is bounded
0 1 v v
from below by
E E E
(cid:16) (cid:17) Err (alg,(f,P )) 1 Err (alg,(f,P )) 1 Err (alg,(f,P ))
PoAE alg,M1,ρ ≥ max T v ≥ · T 0 + · T 1 .
T Lip v∈{0,1} ErrE T(A SO,I v) 2 ErrE T(A SO,I 0) 2 ErrE T(A SO,I 1)
(cid:16) (cid:17) √
Substituting ErrE A ,IL,R = O(LR/ T) as per Lemma 1, we obtain
T SO Lip
√ √
(cid:16) (cid:17) T T
PoAE alg,M1,ρ ≥ ErrE (alg,(f,P ))+ ErrE (alg,(f,P )). (4)
T Lip 2 T 0 2ρ T 1
It remains to specify f, P and P , argue that (f,P ) ∈ I for v ∈ {0,1} holds, and lower bound
0 1 v v
ErrE (alg,(f,P )). We let X = R, S = {0,1} and f : X ×S → R be
T v
(cid:40)
|x| s = 0
f(x;s) =
|x−ρ| s = 1.
8Furthermore, we let
(cid:40)(cid:114) (cid:41)
(cid:18) (cid:19)
logρ 1 1+(2v−1)ε
ε := min , and P := Bernoulli .
v
8T 2 2
Clearly, f(·;s) is 1-Lipschitz for both s ∈ {0,1} and moreover,
(cid:40)
ε|x| v = 0
F (x)− inf F (x⋆) ≥ (5)
f,Pv
x⋆∈R
f,Pv
ε|x−ρ| v = 1.
Consequently, (f,P ) ∈ I for v ∈ {0,1} as required.
v v
Let S ,...,S ∼ P , yielding the sample functions f(·,S ),...,f(·,S ). To lower bound the
1 T v 1 T
error, let x be the output of the algorithm after T oracle queries. Using the expression (5), we
T
have
ερ (cid:16) ρ(cid:17) ερ (cid:16) ρ(cid:17)
E E
Err (alg,(f,P )) ≥ ·P x ≥ and Err (alg,(f,P )) ≥ ·P x < .
T 0 2 0 T 2 T 1 2 1 T 2
To translate these lower bound into the language of hypothesis testing, let Vˆ := 1 and note
{xT≥ρ 2}
that, for v ∈ {0,1},
ερ
ErrE (alg,(f,P )) ≥ P (Vˆ ̸= v).
T v 2 v
√
Substituting into (4), we have that PoAE(cid:0) alg,M1,ρ(cid:1) ≥ ε T(cid:0) ρP (Vˆ ̸= 0)+P (Vˆ ̸= 1)(cid:1) . Writing
T Lip 4 0 1
p := 1 and letting V ∼ Bernoulli(p), we may rewrite the above PoA as
1+ρ
√ √
(cid:16) (cid:17) ε T(cid:16) (cid:17) ε T
PoAE alg,M1,ρ ≥ (1−p)P (Vˆ ̸= 0)+pP (Vˆ ̸= 1) = P(Vˆ ̸= V).
T Lip 4p 0 1 4p
Noting that p ≤ 1 since ρ ≥ 1 and that ρ = 1−p, we invoke Lemma 2 (in Appendix D.2) to
2 p
(cid:16) (cid:17) √
conclude that P(Vˆ ̸= V) ≥ p and therefore (recalling our choice of ε) PoAE alg,M1,ρ ≥ ε T ≥
√ √ 2 T Lip 8
1 min{ logρ, T} as required.
32
4.2 Double logarithmic PoA when Lipschitz constant is known
Theorem 2. For all T ∈ N, ρ ≥ 1, δ ∈ (0, 1], and alg ∈ A we have
2 SFO
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:32)(cid:115) min(cid:8) log(cid:0)1 logρ(cid:1) ,T(cid:9)(cid:33)
PoAδ alg,M1,ρ ≥ PoAδ alg,M1,ρ;A ≥ Ω δ .
T Lip T Lip SFO log(1)
δ
Proof. Throughout the proof we set n := ⌈logρ⌉ and for k ∈ [n], define r := ek−1, so that r = 1
k 1
and r ≤ ρ. The index k will continue to denote a number in [n], and we will use K for an index k
n
drawn uniformly from [n].
(cid:16) (cid:17) (cid:16)(cid:113) (cid:17)
To begin, note that it suffices to prove PoAδ alg,M1,ρ;A ≥ Ω min{log(logρ),T} , since
T Lip SFO log(1/δ)
by Proposition 2.1 we know that
PoAδ(cid:16)
alg,M1,ρ;A
(cid:17)
≥ 1 and
max(cid:110)
1,
loglogρ(cid:111)
≥
1log(1 δlogρ)
.
T Lip SFO log(1/δ) 2 log(1/δ)
We pick n elements from M1,ρ, namely I = I1,r k. For each element, we construct an in-
Lip k Lip
stance (f ,P) such that (f ,P) ∈ I . Since I ∈ M1,ρ for all k ∈ [n], from eq. (3) we have
k k k k Lip
9PoAδ(cid:16) alg,M1,ρ;A (cid:17) ≥ max Errδ T(alg,(f k,P)) . SubstitutingErrδ(A ,I ) = O(cid:18) r (cid:113) log((1/δ)(cid:19)
T Lip SFO k∈[n] Errδ(A ,I ) T SFO k k T
T SFO k
from Proposition 1, we obtain
(cid:32)(cid:115) (cid:33)
(cid:16) (cid:17) T Errδ(alg,(f ,P))
PoAδ alg,M1,ρ;A ≥ Ω max T k . (6)
T Lip SFO log(1/δ) k∈[n] r k
Let us now construct {f } and P. First, define
k
(cid:40)(cid:114) (cid:41)
logn
ε = min ,1 .
4T
We let the domain X = R, the sample space S = {−1,0,1}, and set:

−x s = −1

 1−ε
f (x,s) := |x−r | s = 0 and P(±1) = , P(0) = ε.
k k 2

x s = 1,
Clearly, f (·,s) is 1-Lipschitz for all s and k. Moreover, we have
k
F (x)− inf F (x⋆) = ε|x−r | (7)
f ,P f ,P k
k x⋆∈R k
and therefore (f ,P) ∈ I as required.
k k
For any x ∈ R, let
Kˆ(x) := argmin|x−r |
k
k∈[n]
denote the index k for which r is nearest to x. Letting r = −∞ and r = ∞, if Kˆ(x) ̸= k, we
k 0 n+1
know that
(cid:26) r −r r −r (cid:27) (cid:26) 1−e−1 e−1(cid:27) r
k k−1 k+1 k k
|x−r | ≥ min , ≥ r min , ≥ .
k k
2 2 2 2 4
Letting xk denote the output of alg after interacting with (f ,P) for T iterations and recalling (7),
T k
weobservethattheeventKˆ(xk T) ̸= k impliesF
f
k,P(xk T)−inf x⋆∈RF
f
k,P(x⋆) ≥ εr k/4. Inotherwords,
Errδ(alg,(f ,P)) ≥ εr k1 . Substituting back into (6) and recalling the definition of ε,
T k 4 {P(Kˆ(xk)̸=k)≥δ}
T
we have
(cid:32) (cid:115) (cid:33)
(cid:16) (cid:17) T
PoAδ alg,M1,ρ;A ≥ Ω ε max1 .
T Lip SFO log(1/δ) k∈[n] {P(Kˆ(xk T)̸=k)≥δ}
Let K ∼ Unif([n]) and, to streamline notation, let Kˆ = Kˆ(xK). Noting that P(Kˆ ̸= K) =
T
1 (cid:80)n P(Kˆ(xk) ̸= k) we observe that if P(Kˆ ̸= K) ≥ δ then also P(Kˆ(xk) ̸= k) ≥ δ for some
n k=1 T T
k ∈ [n], and hence, by definition of n and ε,
(cid:32) (cid:115) (cid:33) (cid:32)(cid:115) (cid:33)
(cid:16) (cid:17) T min{loglogρ,T}
PoAδ alg,M1,ρ;A ≥ Ω ε 1 = Ω 1 .
T Lip SFO log(1/δ) {P(Kˆ̸=K)≥δ} log(1/δ) {P(Kˆ̸=K)≥δ}
Finally, we show that P(Kˆ ̸= K) ≥ δ holds for all alg ∈ A . To that end, let S ,...,S ∼iid P
SFO 1 T
be the sequence of observed samples generating, and note that the algorithm’s output xK is a
T
10randomized function of the stochastic (sub)gradients g ,...,g where g ∈ ∂f (x ;S ). We
0 T−1 i K i i+1
may choose the subgradients of f such that they only take values in {−1,1} and, for all t ≥ 0, if
K
we write S′ := gt+1 then the distribution of S′ is Bernoulli(1−ε) if x < r , and Bernoulli(1+ε)
t+1 2 t 2 t−1 K 2
otherwise. Importantly, when conditioning on S′,...,S′ and K (and even if we further condition
1 t−1
on x ), the probability that S′ = 0 is always at most ε away from 1. Therefore, viewing xK and
t−1 t 2 2 T
hence Kˆ as a (randomized) function of S′,...,S′ , and assuming n > 16 without loss of generality,
1 T
Lemma 3 (in Appendix D.3) yields P(Kˆ ̸= K) > 1 ≥ δ as required.
2
4.3 Polynomial PoA when distance and Lipschitz constant are both unknown
Theorem 3. For all T ∈ N, ℓ ≥ 1, ρ ≥ 1, δ ∈ (0, 1) and alg ∈ A we have
3 SO
  (cid:113) 
PoAδ T(cid:16) alg,Mℓ L, iρ p(cid:17) ≥ Ωmin min{ℓ, √ρ}
T
log 1 δ ,(cid:115) loT
g 1
δ 
and
(cid:32) (cid:40) (cid:115) (cid:41)(cid:33)
(cid:16) (cid:17) T
PoAδ alg,Mℓ,ρ ≥ Ω min ρ,ℓ, .
T SM-Lip log 1
δ
Proof. We begin by describing two instances (f,P ) and (f,P ) that are hard to distinguish. To
0 1
that end, define λ := min(cid:8) log 1 , T(cid:9) and for some α ≤ 1 to be determined, let
4 2δ 2
(cid:40)
α|x−ρ| s = 0
f(x;s) =
2αT|x| s = 1.
λ
For v ∈ {0,1} define P = Bernoulli(cid:0)vλ(cid:1) . That is, P (0) = 1 and P (0) = 1− λ. For all x ∈ R,
v T 0 1 T
F (x)− inf F (x ) = α|x−ρ| and,
f,P0
x⋆∈R
f,P0 ⋆
(cid:18) (cid:19)
λ
F (x)− inf F (x ) = 1− α(|x−ρ|−ρ)+2α|x| ≥ α|x|.
f,P1
x⋆∈R
f,P1 ⋆
T
Using the above suboptimality expression, we bound the error incurred by any algorithm. The
algorithm’s output x satisfies
T
αρ αρ
Errδ(alg,(f,P )) ≥ 1 and Errδ(alg,(f,P )) ≥ 1 . (8)
T 0 2 {P0(xT≤ρ 2)≥δ} T 1 2 {P1(xT>ρ 2)≥δ}
Writing
(cid:16) ρ (cid:12) (cid:17)
q := P x ≤ (cid:12) S = ··· = S = 0 ,
T 2 (cid:12) 1 T
we note that, since P (S = ··· = S = 0) = 1, we have P
(cid:0)
x ≤
ρ(cid:1)
= q. Moreover, using the
0 1 T 0 T 2
choice of λ and 1−x ≥ 4−x for all x ≤ 1, we have P (S = ··· = S = 0) = (cid:0) 1− λ(cid:1)T ≥ 4−λ ≥ 2δ.
2 1 1 T T
Therefore,
(cid:16) ρ(cid:17) (cid:16) ρ (cid:12) (cid:17)
P x > ≥ P x > (cid:12) S = ··· = S = 0 P (S = ··· = S = 0) ≥ 2δ(1−q).
1 T 2 T 2 (cid:12) 1 T 1 1 T
Substituting P
(cid:0)
x ≤
ρ(cid:1)
= q and P
(cid:0)
x >
ρ(cid:1)
≥ 2δ(1−q) into eq. (8) gives
0 T 2 1 T 2
αρ αρ αρ
Errδ(alg,(f,P )) ≥ 1 and Errδ(alg,(f,P )) ≥ 1 = 1 . (9)
T 0 2 {q≥δ} T 1 2 {2δ(1−q)≥δ} 2 {q≤1}
2
11Next, we associate our constructed instances with instance classes and bound the PoA from
below. We note that F is minimized at x⋆ = ρ for v = 0 and x⋆ = 0 for v = 1, and that f(·,0)
f,Pv
and f(·,1) are α and 2αT/λ Lipschitz, respectively. Therefore, by setting
(cid:26) (cid:27)
λ
α = min 1, ℓ∧ρ we get (f,P ) ∈ I1,ρ and (f,P ) ∈ Iℓ∧ρ,1,
2T 0 Lip 1 Lip
where ℓ ∧ ρ := min{ℓ,ρ}. Since I1,ρ and Iℓ∧ρ,1 are both members of Mℓ,ρ, we have by eq. (3),
Lip Lip Lip
(cid:16) (cid:17) (cid:16) (cid:113) (cid:17)
eq. (9) and Errδ A ,IL,R = O √LR log 1 (see Proposition 1) that
T SO Lip T δ
 
(cid:16) (cid:17) Errδ(alg,(f,P )) Errδ(alg,(f,P )) 
PoAδ alg,Mℓ,ρ ≥ max T 0 , T 1
T Lip Errδ(cid:16)
A
,I1,ρ(cid:17) Errδ(cid:16)
A
,Iℓ∧ρ,1(cid:17)

T SO Lip T SO Lip
(cid:40) (cid:115) (cid:115) (cid:41) (cid:32) (cid:115) (cid:33)
T αρ T T
≥ Ω(1)max α 1 , 1 = Ω α , (10)
log 1 {q≥δ} ℓ∧ρ log 1 {q≤1 2} log 1
δ δ δ
with the last transition due to ρ ≥ 1 and the fact that either q ≥ δ or q ≤ 1 holds for all q.
ℓ∧ρ 2
Substituting our choices of α and λ yields the claimed lower bound for the Lipschitz case.
Moving on to Mℓ,ρ , we observe that E ∥∇f(x;S)∥2 = α2 and
SM-Lip S∼P0
λ (cid:18) 2αT(cid:19)2 (cid:18) λ(cid:19) 5α2T
E ∥∇f(x;S)∥2 = · + 1− α2 ≤ .
S∼P1
T λ T λ
(cid:110) (cid:113) (cid:111)
Therefore,settingα = min 1, λ (ℓ∧ρ) weget(f,P ) ∈ I1,ρ and(f,P ) ∈ Iℓ∧ρ,1 . According
5T 0 SM-Lip 1 SM-Lip
to Proposition 1, the bounded second moment assumption leads to the same known-parameter
(cid:16) (cid:17) (cid:16) (cid:113) (cid:17)
minimax rates as the bounded Lipschitz assumption, i.e., Errδ A ,IL,R = O √LR log 1 .
T SO SM-Lip T δ
Therefore, since I1,ρ ,Iℓ∧ρ,1 ∈ Mℓ,ρ , we may repeat the argument in eq. (10) and conclude
SM-Lip SM-Lip SM-Lip
(cid:16) (cid:17) (cid:16) (cid:113) (cid:17) (cid:16) (cid:110)(cid:113) (cid:111)(cid:17)
that PoAδ alg,Mℓ,ρ = Ω α T/log 1 = Ω min T/log 1,ℓ∧ρ , establishing the claimed
T Lip δ δ
lower bound in the bounded second moment setting.
5 Discussion
Below, we describe the main conclusions from our work, and open problems they put into focus.
The PoA in stochastic convex optimization is nearly settled. In the regime where the
Lipschitz constant (i.e., probability 1 stochastic gradient bound) is known (ℓ = O(1)), Theorems 1
and 2 show that [25] and [5] provide optimal and near-optimal adaptivity to uncertainty in the
distance to optimality (ρ), for expected and constant-probability error, respectively. Moreover,
when the distance to optimality is known (ρ = O(1)) it is possible to be completely adaptive to the
Lipschitz constant [18]. Finally, the polynomial PoA lower bounds in Theorem 3 are matched, up to
polylogarithmicfactors,byacombinationofexistingalgorithms[5,9,25]. Thus,ourworkestablishes
that overheads incurred by known adaptive and parameter-free algorithms are for the most part
unavoidable. In particular, when both ℓ and ρ are Ω(T), nontrivial adaptivity is impossible.
12Adaptivity is much harder with heavy-tailed noise. Theorem 3 also shows that the price
of adaptivity becomes much higher when we replace the assumption that all stochastic gradients
are bounded with probability 1 with the assumption that only their second moment is bounded,
allowing for heavy-tailed noise. Under the latter assumption we show that any algorithm must
have PoA at least linear in—and hence be sensitive to—the uncertainty in either gradient bound
or distance to optimality, while under the former assumption the PoA is smaller by a factor of
√
T, allowing for robustness to both parameters in some regimes. Our separation between Lipschitz
assumptions is notable, because when the problem parameters are known the rates of convergence
under both assumptions are essentially the same, both in expectation and with high probability
(Proposition 1). Thus, robustness to heavy-tailed noise must come at the cost of knowing some
problem parameters.
PoA in high-probability is lower (!) than in expectation. Theorem 1 implies that, for
uncertainty ρ in distance to optimality, any adaptive algorithm must have expected suboptimality
√
larger by factor of Ω( logρ) than the minimax optimal rate with known parameters. This may
appear to contradict Carmon and Hinder [5], who give probability 1−δ suboptimality bounds only
a factor of O(log2(log(ρT)/δ)) larger than the optimal rate. The apparent contradiction stems from
the fact that high-probability bounds are typically stronger than in-expectation bounds. Indeed,
for any random variable Z (e.g., the suboptimality of an algorithm’s output) if the 1−δ quantile
Q (Z) is bounded by Cpoly(cid:0) log1(cid:1) for all δ, then the identity EZ = (cid:82)1 Q (Z)dδ implies that
1−δ δ 0 δ
EZ = O(C). The resolution to this apparent contradiction is that the bound in [5] does not
hold simultaneously for all δ; instead, the desired confidence level δ is prespecified and affects the
algorithm. Thus, the best possible probability 1−δ PoA bound holding uniformly for all δ must
be logarithmic in ρ; characterizing the uniform-high-probability PoA is an open problem.
Sample complexity vs. gradient oracle complexity. Theorems1and3aresample complexity
lower bounds: they are valid for algorithms with unrestricted access to the sample functions. In
contrast, Theorem 2 is an oracle complexity lower bound: it holds for algorithms restricted to
evaluatingasinglegradientforeachsamplefunction. Itisstraightforwardtoextendtheconstruction
proving Theorem 2 to algorithms that observe a sample function value in addition to a gradient.
To do so, we may simply add a random constant offset to each sample function and bound the
information that observing the function value can add. The required random offset might be fairly
large, but the natural assumptions for our problem place no limitation on its size. Fortifying the
construction of Theorem 2 against algorithms with complete sample function access appears to be
much more difficult, since for such algorithms the first draw of s = 0 reveals the optimum. Whether
the loglogρ PoA lower bound also holds for sample complexity remains an open problem.
The price of adaptivity beyond non-smooth stochastic convex optimization. This work
considers meta-classes that capture the well-studied setting of non-smooth stochastic convex opti-
mization. However,additionalassumptionssuchassmoothness,strongconvexity,andnoisevariance
impact convergence guarantees and introduce additional problem parameters that are rarely known
in advance. Consequently, to fully understand the PoA in stochastic convex optimization, we must
explore richer meta-classes. Furthermore, it is possible to study the PoA beyond convex optimiza-
tion, e.g., by replacing function-value gap with gradient norm.
13Acknowledgements
We thank Amit Attia, Chi Jin, Ahmed Khaled, and Tomer Koren for helpful discussions. This work
was supported by the NSF-BSF program, under NSF grant #2239527 and BSF grant #2022663.
OH acknowledges support from Pitt Momentum Funds, and AFOSR grant #FA9550-23-1-0242.YC
acknowledges support from the Israeli Science Foundation (ISF) grant no. 2486/21, the Alon Fel-
lowship, and the Adelis Foundation.
References
[1] A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower
bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on
Information Theory, 58(5):3235–3249, 2012.
[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183–202, 2009.
[3] A. A. Borovkov. Probability theory. CRC Press, 1999.
[4] G. Braun, C. Guzmán, and S. Pokutta. Lower bounds on the oracle complexity of nonsmooth
convex optimization via information theory. IEEE Transactions on Information Theory, 63(7):
4709–4724, 2017.
[5] Y. Carmon and O. Hinder. Making SGD parameter-free. In Conference on Learning Theory
(COLT), 2022.
[6] Y. Carmon, D. Hausler, A. Jambulapati, Y. Jin, and A. Sidford. Optimal and adap-
tive monteiro-svaiter acceleration. In Advances in Neural Information Processing Systems
(NeurIPS), 2022.
[7] K. Chen, J. Langford, and F. Orabona. Better parameter-free stochastic optimization with
ODE updates for coin-betting. In AAAI Conference on Artificial Intelligence, 2022.
[8] T. M. Cover and A. J. Thomas. Elements of information theory. John Wiley & Sons, 1991.
[9] A. Cutkosky. Artificial constraints and hints for unbounded online learning. In Conference on
Learning Theory (COLT), 2019.
[10] A. Cutkosky and K. Boahen. Online learning without prior information. In Conference on
Learning Theory (COLT), 2017.
[11] A.CutkoskyandF.Orabona.Black-boxreductionsforparameter-freeonlinelearninginBanach
spaces. In Conference on Learning Theory (COLT), 2018.
[12] D. Davis, D. Drusvyatskiy, L. Xiao, and J. Zhang. From low probability to high confidence in
stochastic convex optimization. The Journal of Machine Learning Research, 22(1):2237–2274,
2021.
[13] J. C. Duchi. Introductory lectures on stochastic optimization. The Mathematics of Data, 25:
99–186, 2018.
[14] R. M. Fano and W. Wintringham. Transmission of information, 1961.
14[15] D.A.Freedman. Ontailprobabilitiesformartingales. the Annals of Probability,pages100–118,
1975.
[16] E. Gorbunov, M. Danilova, and A. Gasnikov. Stochastic optimization with heavy-tailed noise
via accelerated gradient clipping. In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
[17] E. Gorbunov, M. Danilova, I. Shibaev, P. Dvurechensky, and A. Gasnikov. Near-optimal high
probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise.
arXiv:2106.05958, 2021.
[18] V. Gupta, T. Koren, and Y. Singer. A unified approach to adaptive regularization in online
and stochastic optimization. arXiv:1706.06569, 2017.
[19] E. Hazan and S. Kakade. Revisiting the Polyak step size. arXiv:1905.00313, 2019.
[20] M. Ivgi, O. Hinder, and Y. Carmon. DoG is SGD’s best friend: A parameter-free dynamic step
size schedule. In International Conference on Machine Learning (ICML), 2023.
[21] A. Jacobsen and A. Cutkosky. Unconstrained online learning with unbounded losses. In Inter-
national Conference on Machine Learning (ICML), 2023.
[22] R. M. Karp and R. Kleinberg. Noisy binary search and its applications. In Symposium on
Discrete Algorithms (SODA), 2007.
[23] A.Kavis,K.Y.Levy,F.Bach,andV.Cevher. UniXGrad: Auniversal,adaptivealgorithmwith
optimal guarantees for constrained optimization. Advances in Neural Information Processing
Systems (NeurIPS), 2019.
[24] H. B. McMahan. A survey of algorithms and analysis for adaptive online learning. The Journal
of Machine Learning Research, 18(1):3117–3166, 2017.
[25] H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces:
Minimax algorithms and normal approximations. In Conference on Learning Theory (COLT),
2014.
[26] Z.MhammediandW.M.Koolen. Lipschitzandcomparator-normadaptivityinonlinelearning.
In Conference on Learning Theory (COLT), 2020.
[27] Z. Mhammedi, W. M. Koolen, and T. Van Erven. Lipschitz adaptivity with multiple learning
rates in online learning. In Conference on Learning Theory, pages 2490–2511, 2019.
[28] K. Mishchenko and A. Defazio. Prodigy: An expeditiously adaptive parameter-free learner.
arXiv:2306.06101, 2023.
[29] A. V. Nazin, A. S. Nemirovsky, A. B. Tsybakov, and A. B. Juditsky. Algorithms of robust
stochastic optimization based on mirror descent method. Automation and Remote Control, 80:
1607–1627, 2019.
[30] A.NemirovskiandD.Yudin. Problem complexity and method efficiency in optimization. Wiley-
Interscience, 1983.
15[31] T. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen. Improved convergence in high proba-
bility of clipped gradient methods with heavy tailed noise. In Advances in Neural Information
Processing Systems (NeurIPS), 2023.
[32] F. Orabona. Dimension-free exponentiated gradient. Advances in Neural Information Process-
ing Systems (NeurIPS), 2013.
[33] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic
learning. Advances in Neural Information Processing Systems (NeurIPS), 2014.
[34] F. Orabona. A modern introduction to online learning. arXiv:1912.13213, 2021.
[35] F.OrabonaandD.Pál. Coinbettingandparameter-freeonlinelearning. InAdvances in Neural
Information Processing Systems (NeurIPS), 2016.
[36] F. Orabona and D. Pál. Parameter-free stochastic optimization of variationally coherent func-
tions. arXiv:2102.00236, 2021.
[37] C. Paquette and K. Scheinberg. A stochastic line search method with expected complexity
analysis. SIAM Journal on Optimization, 30(1):349–376, 2020.
[38] B. T. Polyak. Introduction to Optimization. Optimization Software, Inc, 1987.
[39] T. Roughgarden. Selfish routing and the price of anarchy. MIT press, 2005.
[40] A.Sadiev,M.Danilova,E.Gorbunov,S.Horváth,G.Gidel,P.Dvurechensky,A.Gasnikov,and
P. Richtárik. High-probability bounds for stochastic optimization and variational inequalities:
the case of unbounded variance. In International Conference on Machine Learning (ICML),
2023.
[41] L.SharrockandC.Nemeth.Coinsampling: Gradient-basedbayesianinferencewithoutlearning
rates. In International Conference on Machine Learning (ICML), 2023.
[42] M. Streeter and H. B. McMahan. No-regret algorithms for unconstrained online convex opti-
mization. In Advances in Neural Information Processing Systems (NeurIPS), 2012.
[43] S. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel, and S. Lacoste-Julien. Painless
stochastic gradient: Interpolation, line-search, and convergence rates. In Advances in Neural
Information Processing Systems (NeurIPS), 2019.
[44] J. Zhang and A. Cutkosky. Parameter-free regret in high probability with heavy tails. In
Advances in Neural Information Processing Systems (NeurIPS), 2022.
[45] Z. Zhang, A. Cutkosky, and I. Paschalidis. PDE-based optimal strategy for unconstrained
online learning. In International Conference on Machine Learning (ICML), 2022.
16A Derivation of PoA upper bounds
InthissectionweexplainhowtogofrompublishedsuboptimalityboundstothePoAboundsstated
in Table 1. For brevity we define g = ∇f(x ;S ) and ∆ = f(x ;S )−f(x ;S ).
t t t+1 t t t+1 ⋆ t+1
SGD. Let SGD denote fixed-steps size SGD with step size η (outputting the iterate average).
η
Then, for (f,P) ∈ IL,R it is well-known the iterates of SGD satisfy [see, e.g., 34, Theorem 2.13]:
SM-Lip
1 T (cid:88)−1
∆ ≤
∥x 0−x ⋆∥2
+
η T (cid:88)−1
∥g ∥2
t t
T 2ηT 2T
t=0 t=0
where x is any optimal solution. By online-to-batch conversion [34, Chapter 3] we get
⋆
(cid:16) (cid:17) R2 ηL2
ErrE SGD ,IL,R ≤ + . (11)
T η SM-Lip 2ηT 2
Therefore, by the PoA definition and Proposition 1
(cid:32) √ (cid:33)
(cid:16) (cid:17) T(cid:18) R2 (cid:19) (cid:18) (cid:26) ρ √ (cid:27)(cid:19)
PoAE SGD ,Mℓ,ρ ≤ O sup +ηL2 ≤ O max √ ,ηℓ T .
T η SM-Lip LR ηT η T
L∈[1,ℓ],R∈[1,ρ]
(cid:113)
We can minimize the LHS of the previous expression by setting η = η := 2ρ giving
⋆ ℓT
PoAE(cid:16)
SGD ,Mℓ,ρ
(cid:17)
=
O(cid:16)(cid:112) ℓρ(cid:17)
.
T η⋆ SM-Lip
Adaptive SGD. LetAdaSGD denoteadaptiveSGD[18,24]withprojectiontoX∩{x | ∥x∥ ≤ ρ}.
ρ
Then, we have by [18, Section 3.3]
(cid:118)
T−1 √ (cid:117)T−1
(cid:88) (cid:117)(cid:88)
∆ ≤ 2ρ(cid:116) ∥g ∥2
t t
t=0 t=0
using (i) online-to-batch conversion [34, Chapter 3], (ii) Jensen’s inequality on the concave function
√
t (cid:55)→ t, and (iii) the bound E ∥∇f(x;S)∥2 ≤ L2 we get
S∼P
ErrE T(cid:16) AdaSGD R,I SL M,R -Lip(cid:17) ( ≤i) E √ T2ρ(cid:118) (cid:117) (cid:117) (cid:116)T (cid:88)−1 ∥g t∥2  ( ≤ii) √ T2ρ(cid:118) (cid:117) (cid:117) (cid:116)E(cid:34)T (cid:88)−1 ∥g t∥2(cid:35) (i ≤ii) √ 2√L Tρ ,
t=0 t=0
which the PoA definition and Proposition 1 implies
(cid:32) √ (cid:33)
(cid:16) (cid:17) T Lρ
PoAE AdaSGD ,M∞,ρ = O sup · √ = O(ρ).
T ρ SM-Lip LR T
L∈[1,ℓ],R∈[1,ρ]
Thus, adaptive SGD iscompletely robust to unknown stochastic gradient secondmoment, but more
sensitive to unknown domain size than SGD.
17McMahan and Orabona [25]. Applying Theorem 11 in [25] with u = x , a = G2π and G = ℓ
⋆
yields the following regret bound for (f,P) ∈ Iℓ,∞,
Lip
T−1  (cid:118) (cid:117) (cid:32) √ (cid:33) 
(cid:88) (cid:117) ℓ T∥x ⋆∥
∆
t
≤ Oℓ∥x ⋆∥(cid:116)T log +1 +ϵ.
ϵ
t=0
√
Setting ϵ = ℓ T and employing online-to-batch conversion for R ≥ 1 gives
(cid:18) (cid:19)
ErrE(cid:16)
McMahan and Orabona
[25],Iℓ,R(cid:17)
≤ O
√ℓR (cid:112)
log(R+1) .
T Lip
T
Hence, for all ℓ ≥ 1, ρ ≥ 2 we have
PoAE(cid:16)
McMahan and Orabona
[25],Mℓ,ρ(cid:17)
=
O(cid:16) ℓ(cid:112) logρ(cid:17)
.
T Lip
Compared to SGD, this algorithm has a much stronger adaptivity to domain size but weaker adap-
tivity to gradient norm, and it also requires the stronger assumption of a probability 1 gradient
norm bound.
Cutkosky and Orabona [11]. For (f,P) ∈ I1,∞, using the result of Cutkosky and Orabona [11,
Lip
Page 6] with the Euclidean norm and λ = 1 gives a regret bound of
T (cid:88)−1 (cid:32) (cid:40) (cid:18) ∥x ⋆∥G T(cid:19) (cid:115) (cid:18) ∥x ⋆∥2G
T
(cid:19)(cid:41)(cid:33)
∆ ≤ O ϵ+∥x ∥max log , G log +1 .
t ⋆ ϵ T ϵ2
t=0
with G := (cid:80)T−1∥g ∥2. If (f,P) ∈ Iℓ,∞ then using this method gradients rescaled by 1/ℓ yields
T t=0 t Lip
T (cid:88)−1 (cid:32) (cid:40) (cid:18) ∥x ⋆∥G T(cid:19) (cid:115) (cid:18) ∥x ⋆∥2G
T
(cid:19)(cid:41)(cid:33)
∆ ≤ O ℓϵ+∥x ∥max ℓlog , G log +1 .
t ⋆ ϵℓ2 T ϵ2ℓ2
t=0
√
Setting ϵ = T and employing online-to-batch conversion for L ∈ [1,ℓ] and R ∈ [1,ρ] gives
(cid:18) (cid:19)
ErrE(cid:16)
McMahan and Orabona
[25],IL,R(cid:17)
≤ O
ℓ
(1+Rlog(ρ))+
√LR(cid:112)
log(ρ+1) .
T Lip T T
Therefore,
(cid:18) (cid:19)
PoAE(cid:16)
Cutkosky and Orabona
[11],Mℓ,ρ(cid:17)
= O
√ℓ log(ρ)+(cid:112)
log(ρ+1) .
T Lip
T
Carmon and Hinder [5]. Applying Theorem 14 in [5] with η = 1 , L = ℓ, δ ∈ (0,1/2) we get
ϵ ℓB
for all Lˆ ∈ [1,ℓ] and R ≥ 1 that for all B ∈ N
 (cid:113) (cid:16) (cid:17)
Errδ B(cid:16) Carmon and Hinder [5],I LLˆ i, pR(cid:17) ≤ OR CLˆ2( BB // MM)+C2ℓ + ℓ1 B CLˆ2(B B/ /M M)+C2ℓ2 
(cid:18) (cid:18) (cid:19)(cid:19)
(C +M)R (C +M)ℓ
≤ O √ Lˆ + √
B B
18log(1+R)
where C = log and M = min{B,loglog(T(1+R))}. Therefore, substituting for C and M,
δ
and using ρ ≥ max{R,2} gives that the previous display is upper bounded by
(cid:32) (cid:32) (cid:33)(cid:33) (cid:32) (cid:32) (cid:33)(cid:33)
(cid:18) log(ρT)(cid:19) LˆR (cid:18) log(ρT)(cid:19) ℓR (cid:18) log(ρT)(cid:19)2 LˆR ℓR
O log √ + log = O log √ + .
δ B δ B δ B B
Therefore,
(cid:32) (cid:33)
(cid:16) (cid:17)
(cid:18) log(ρT)(cid:19)2(cid:18)
ℓ
(cid:19)
PoAδ Carmon and Hinder [5],Mℓ,ρ = O log 1+ √ .
T Lip δ T
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17)
In a similar manner, [5, Section D.2] gives an O log log(ρT) bound on PoAδ ·,M1,ρ .
δ T Lip
Cutkosky [9]. Cutkosky [9, Page 7] provides an algorithm which, for D ≥ 1 and domain X ∩{x :
∥x∥ ≤ D}, enjoys a regret guarantee of
T (cid:88)−1 (cid:16) √ (cid:17)
∆
t
≤ O(cid:101) ∥x ⋆∥G T +DG ,
t=0
whereG = max{1,max ∥∇f(x ,S )∥}. SettingD = ρandapplyingonline-to-batchconversion
t<T t t+1
this gives for all R ∈ [1,ρ] that
(cid:18) (cid:19)
(cid:16) (cid:17) LR Lρ
ErrE Cutkosky [9, Page 7],IL,R ≤ O(cid:101) √ + .
T Lip T T
Thus,
(cid:18) (cid:19)
(cid:16) (cid:17) ρ
PoAE Cutkosky [9, Page 7],Mℓ,ρ = O(cid:101) 1+ √ .
T Lip
T
Notably, unlike the other algorithms in Table 1, to obtain this result we needed to know ρ. When
√
ρ is unknown currently the best bound on the price of adaptivity is O˜(1+ρ2/ T) [9, Corollary 4].
Zhang and Cutkosky [44]. For any (f,P) ∈ Iℓ,∞ we have E ∥∇f(x ,S)−∇F (x )∥2 ≤
SM-Lip S∼P t f,P t
E ∥∇f(x ,S)∥2 ≤ ℓ2 and ∥∇F (x )∥ ≤ ℓ. Therefore, Theorem 4 of Zhang and Cutkosky [44]
S∼P t f,P t
with p = 2 gives that,
(cid:34)T−1 (cid:35)
(cid:88) (cid:16) (cid:17)
E ∆
t
= O(cid:101) ϵ+ℓ∥x 0−x⋆∥T1/2 ,
t=0
√
where we are hiding logarithmic dependence in ϵ and other problem parameters. Taking ϵ = T,
we conclude that for every R ≥ 1 and L ∈ [1,ℓ] we have
(cid:18) (cid:19)
(cid:16) (cid:17) ℓR
Errδ Zhang and Cutkosky [44],IL,R = O(cid:101) √ .
T SM-Lip
T
(cid:16) (cid:17)
Therefore, we have PoAδ Zhang and Cutkosky [44],Mℓ,ρ = O(cid:101)(ℓ).
T SM-Lip
19B Proof of Proposition 1
Propostion 1. For all R,L > 0, instance class I ∈ {IL,R,IL,R } and A ∈ {A ,A }, we have
Lip SM-Lip SO SFO
(cid:32) (cid:33)
(cid:16) (cid:17)
ErrE (A,I) = Θ √LR . Moreover, for any δ ∈ (0, 1) we have Errδ(A,I) = Θ LR .
T T 2 T (cid:113) 1+T/log1
δ
Proof. Since IL,R ⊂ IL,R and A ⊂ A , we only need to prove minimax error lower bounds
Lip SM-Lip SFO SO
on (A ,IL,R) and minimax upper bounds on (A ,IL,R ).
SO Lip SFO SM-Lip
(cid:16) (cid:17)
For expected error these results are well-known. The upper bound on ErrE A ,IL,R
T SFO SM-Lip
(cid:16) (cid:17)
follows, for example, by setting η = √R in eq. (11). For the lower bound on ErrE A ,IL,R ,
L T T SO Lip
originally due to Nemirovski and Yudin [30], see, e.g., Duchi [13, Theorem 5.2.10.].
For error quantiles, however, we could not find direct proof of the required upper and lower
bounds, and we prove them in Appendices B.1 and B.2 below.
B.1 Optimal high-probability bound for second-moment Lipschitz problems
In this section we derive high-probability guarantees for the clipped SGD method and the class
IL,R of second-moment L-Lipchitz stochastic convex optimization problems with initial distance
SM-Lip
to optimality R. Our rate of convergence is the same (up to a constant) as the rate standard SGD
achieves for the smaller class IL,R of probability 1 L-Lipschitz functions.
Lip
A number of prior works [e.g., 12, 16, 17, 29, 31, 40] provide high probability bounds under
heavy-tailed noise and many of them use gradient clipping. However, these works are mostly
concerned with the smooth setting. The only exception is Gorbunov et al. [17], who consider non-
smooth optimization under more general heavy-tailed noise distributions, but obtain rates that are
suboptimal by polylogarithmic terms in the desired confidence level δ.
The only probabilistic tool we require is the following simplification of Freedman’s inequality.
Fact 1 (Simplified form of Freedman’s inequality [15]). Let X ,...,X be a sequence of real-valued
1 T
random variables that satisfy E(cid:2) X t2 (cid:12) (cid:12) X 1,...,X t−1(cid:3) ≤ A2 and |X t| ≤ B, with probability 1 for all
t ≤ T. Then, for any δ ∈ (0,1),
(cid:32) T T (cid:114) (cid:33)
(cid:88) (cid:88) 1 B 1
P X > E[X | X ,...,X ]+A 2T log + log ≤ δ.
t t 1 t−1
δ 3 δ
t=1 t=1
Given stochastic optimization instance (f,P), step size parameter η, clipping parameter G
clip
and initial point x ∈ X, clipped SGD consists of the following recursion:
0
g
x = Π (x −ηg¯), where g¯ := t and g ∈ ∂f(x ;S ). (12)
t+1 X′ t t t t t t+1
max{1,∥g ∥/G }
t clip
Here Π is the Euclidean projection onto X′ = X ∩{x | ∥x−x ∥ ≤ R}.
X′ 0
With these definitions in hand, we state and prove our rate of convergence guarantee.
Theorem 4. For any δ ∈ (0, 1), any L,R > 0, and any instance in IL,R , we have that T steps of
2 SM-Lip
√
clipped SGD (12) with step size η = √R and G
clip
= √L T produce iterates x 0,...,x
T−1
whose
L T log(1/δ)
(cid:16) (cid:113) (cid:17)
average x¯
T
= T1 (cid:80) tT =− 01x
t
has suboptimality O √LR
T
log 1
δ
with probability at least 1−δ.
20Proof. Let F(x) = E f(x;S) be the objective function and let x⋆ be its minimizer in X′ (and
S∼P
hence, by assumption, also in X). The standard SGD regret bound, e.g., [34, Lemma 2.12.], gives
T (cid:88)−1 R2 η T (cid:88)−1
⟨g¯,x −x⋆⟩ ≤ + ∥g¯∥2
t t t
2η 2
t=0 t=0
with probability 1. Writing E Z = E[Z | x ,...,x ] to streamline notation, and using g¯ = E g −
t 0 t t t t
(E g −E g¯)−(E g¯ −g¯) we rearrange the above display to read,
t t t t t t t
T (cid:88)−1 R2 T (cid:88)−1 T (cid:88)−1 η T (cid:88)−1
⟨E g ,x −x⋆⟩ ≤ + ⟨E g −E g¯,x −x⋆⟩+ ⟨E g¯ −g¯,x −x⋆⟩+ ∥g¯∥2. (13)
t t t t t t t t t t t t t
2η 2
t=0 t=0 t=0 t=0
SinceE g ∈ ∂F(x ),wehave⟨E g ,x −x⋆⟩ ≥ F(x )−F(x⋆)andtherefore,byJensen’sinequality
t t t t t t t
we obtain the following lower bound on the LHS of Equation (13)
T−1
(cid:88)
⟨E g ,x −x⋆⟩ ≥ T[F(x¯ )−F(x⋆)]. (14)
t t t T
t=0
√
We have R2/(2η) = O(LR T) by our choice of η, and it remains to bound the three sums in the
RHS of Equation (13).
To bound the first sum, we observe that
(cid:20) (cid:18) (cid:19) (cid:21)
∥E g −E g¯∥ ( ≤i) E ∥g −g¯∥ ( =ii) E ∥g ∥ 1− G clip 1 ≤ E (cid:104) ∥g ∥1 (cid:105)
t t t t t t t t t ∥g ∥ {∥gt∥>G clip} t t {∥gt∥>G clip}
t
1 (iii) L2
≤ E ∥g ∥2 ≤ ,
t t
G G
clip clip
where (i) is from Jensen’s inequality applied to the Euclidean norm, (ii) from the definition of g¯
t
and (iii) is from the definition of IL,R . Therefore, using the triangle inequality, Cauchy-Schwarz,
SM-Lip
the bound ∥x −x⋆∥ ≤ 2R, and our setting of G , we get that
t clip
(cid:12) (cid:12)T (cid:88)−1 (cid:12) (cid:12) T (cid:88)−1 2L2RT (cid:114) 1
(cid:12) ⟨E g¯ −E g ,x −x⋆⟩(cid:12) ≤ ∥E g −E g¯∥∥x −x⋆∥ ≤ = 2LR T log . (15)
(cid:12) t t t t t (cid:12) t t t t t G δ
(cid:12) (cid:12) clip
t=0 t=0
To bound the second sum, we note that E [⟨g¯ −E g¯,x −x⋆⟩] = 0 for all t. Moreover, again
t t t t t
by Cauchy-Schwarz and ∥x −x⋆∥ ≤ 2R, we have that
t
E [⟨g¯ −E g¯,x −x⋆⟩]2 ≤ 4R2E ∥g¯ −E g¯∥2 ≤ 4R2E ∥g¯∥2 ≤ 4R2E ∥g ∥2 ≤ 4L2R2,
t t t t t t t t t t t t t
as well as
|⟨g¯ −E g¯,x −x⋆⟩| ≤ ∥g¯ −E g¯∥∥x −x⋆∥ ≤ 4G R,
t t t t t t t t clip
duetothegradientclipping. Therefore, wemayapplyFact1onX = ⟨E g¯ −g¯,x −x⋆⟩toconclude
t t t t t
that, with probability at least 1− δ,
2
T−1 (cid:114) (cid:32) (cid:114) (cid:33)
(cid:88) 2 4 2 1
⟨E g¯ −g¯,x −x⋆⟩ ≤ LR 8T log + G Rlog = O LR T log . (16)
t t t t clip
δ 3 δ δ
t=0
21For the third and final sum, we observe that E ∥g¯∥4 ≤ G2 E ∥g¯∥2 ≤ (G L)2, and that
t t clip t t clip
∥g¯∥2 ≤ G2 with probability 1. We then apply Fact 1 again, this time with X = ∥g¯∥2 to obtain
t clip t t
that, with probability at least 1− δ,
2
T−1 T−1 (cid:114)
η (cid:88) η (cid:88) η 1 η 1
∥g¯∥2 ≤ E ∥g¯∥2+ G L 2T log + G2 log
2 t 2 t t 2 clip δ 6 clip δ
t=0 t=0
(i) η (cid:18) √ 1(cid:19) (ii) (cid:16) √ (cid:17)
≤ L2T · 1+ 2+ ≤ O LR T , (17)
2 3
due to (i) the choice G and E ∥g¯∥2 ≤ E ∥g ∥2 ≤ L2, and (ii) the choice of η.
clip t t t t
Substituting eqs. (14) to (17) into eq. (13) and using again the choice of η and a union bound
over the two events holding with probability at least 1− δ, we conclude that T[F(x¯ )−F(x⋆)] ≤
2 T
(cid:16) (cid:113) (cid:17)
O LR T log 1 with probability at least 1−δ, leading to the claimed rate of convergence.
δ
B.2 Optimal error quantile lower bound
Theorem 5. For all L,R > 0 and δ ∈ (0, 1) we have
Errδ(cid:16)
A
,IL,R(cid:17)
≥
Ω(cid:18) LRmin(cid:26)(cid:113)
log1
δ,1(cid:27)(cid:19)
.
2 T SO Lip T
Proof. We prove Theorem 5 with a construction almost identical to the one in the proof of Theo-
rem 1. We let X = R, S = {0,1} and f : X ×S → R be
(cid:40)
L|x| s = 0
f(x;s) =
L|x−R| s = 1,
and we set, assuming for now δ ≤ 1,
5
(cid:115) 
 log 1−2δ 1 (cid:18) 1+(2v−1)ε(cid:19)
ε := min 2δ , and P := Bernoulli .
v
8T 2 2
 
Clearly, f(·;s) is L-Lipschitz for both s ∈ {0,1} and moreover,
(cid:40)
εL|x+R| v = 0
F (x)− inf F (x⋆) ≥ (18)
f,Pv
x⋆∈R
f,Pv
εL|x−R| v = 1.
Consequently, (f,P ) ∈ IL,R for v ∈ {0,1}.
v Lip
For any alg ∈ A , let xv be the output of alg when interacting with (f,P ) by observing
SO T v
S ,...,S ∼iid P , and let vˆ(x) := 1 . Substituting into (18), we have
1 T v {x>0}
Errδ(alg,(f,P )) ≥ εLR1 for v ∈ {0,1}.
T v {P(vˆ(xv)̸=v)≥δ}
T
Therefore, since max P(vˆ(xv) ̸= v) ≥ E P(vˆ(cid:0) xV(cid:1) ̸= V) we have
v∈{0,1} T V∼Bernoulli(2δ) T
(cid:16) (cid:17)
Errδ alg,IL,R ≥ εLR1 ≥ εLR1 .
T Lip {max P(vˆ(xv)̸=v)≥δ} {E P(vˆ(xV)̸=V)≥δ}
v∈{0,1} T V∼Bernoulli(2δ) T
22LetVˆ := vˆ(cid:0) xV(cid:1) andnotethatitdependsonV onlythroughxV andhencethroughS ,...,S ∼iid P .
T T 1 T V
By our choice of ε and the current assumption that 2δ < 2/5 ≤ 1/2, we may apply Lemma 2 with
p = 2δ and conclude that P(Vˆ ̸= V) ≥ δ, implying
 (cid:115) 
(cid:16) (cid:17)  log 1 
Errδ T alg,I LL i, pR ≥ εLR = ΩLRmin T δ,1 
 
for all δ ≤ 1.
5
(cid:16) (cid:17)
To extend the lower bound to δ ∈ (1, 1), we only need to show that Err1/2 alg,IL,R =
√ 5 2 T Lip
Ω(LR/ T). We do this with a simple “anti-boosting” argument. Let k = 4 and consider k copies
of the construction above for δ′ = 1, each operating on independent samples and independent coor-
5
dinates, scaled so that the sum of the copies is in IL,R. Concretely, the construction uses X = Rk
Lip
and S = {0,1}k, and given f : R×{0,1} → R described above, defines f˜: X ×S → R such that
√
(cid:88)
f˜(x,s) = k−3/2 f(x k;s ).
i i
i∈[k]
For v = (v ,...,v ) ∈ {0,1}k and P = P ×···×P , this construction has optimality gap
1 k v1 v k
(cid:13) √ (cid:13)
F (x)− inf F (x⋆) ≥ 1 εLR·(cid:13)x k−(2v−1)R(cid:13) .
f,Pv x⋆∈X f,Pv k3/2 (cid:13) (cid:13) 1
Therefore, disagreeing with the sign of x⋆ in even a single coordinate implies optimality gap of
√
at least k−3/2εLR = Ω(LR/ T) by our choice of ε. By the discussion above, the probability of
mistaking at least one of the coordinates is at least 1 − (1 − δ′)k > 1 for δ′ = 1/5 and k = 4,
2
completing the proof.
C Proof of Proposition 2
Propostion 2. The price of adaptivity satisfies the following properties:
1. PoA (alg,M;A) ≥ 1 for all M,A and alg ∈ A.
T
√ (cid:18) (cid:19)
2. PoAE (alg ,M) ≤ O( T) and PoAδ(alg ,M) ≤ O (cid:113) T +1 for M ∈ {Mℓ,ρ,Mℓ,ρ } and
T 0 T 0 log1 Lip SM-Lip
δ
alg that trivially returns x = 0.
0 0
3. PoAE (alg,M) ≥ Ω(δ)PoAδ(alg,M) for any M ∈ {Mℓ,ρ,Mℓ,ρ }, and alg ∈ A .
T T Lip SM-Lip SO
4. PoA (cid:0) alg,Mℓ,ρ(cid:1) ≤ O(1)PoA (cid:0) alg,Mℓ,ρ (cid:1) for every alg ∈ A .
T Lip T SM-Lip SO
5. PoA (alg,M) = Θ(1)PoA (alg,M;A ) for every alg ∈ A and M ∈ {Mℓ,ρ,Mℓ,ρ }.
T T SFO SO Lip SM-Lip
(cid:110) (cid:111)
6. Let Ml1,l2,r1,r2 := IL,R | L ∈ [l ,l ] and R ∈ [r ,r ] . Then, for all alg ∈ A , there exists alg′
Lip Lip 1 2 1 2 SO
s.t. PoA
(cid:0) alg,Ml1,l2,r1,r2(cid:1)
= PoA
(cid:0) alg′,Ml2/l1,r2/r1(cid:1)
. The same holds replacing Lip with SM-Lip.
T Lip T Lip
Proof. Item 1 is a direct consequence of definition (2) of the PoA and the definition (1) of minimax
error, which implies that Err (alg,I) ≥ Err (A,I) for all alg ∈ A.
T T
23Item 2 holds since for any (f,P) ∈ I and I ∈ {IL,R,IL,R } the objective function F is L
Lip SM-Lip f,P
Lipschitz. If x⋆ is the minimizer of F closest to x = 0 then we have F (x ) − F (x⋆) ≤
f,P 0 f,P 0 f,P
L∥x⋆ − x ∥ ≤ LR. Therefore, Err (alg ,I) ≤ LR and the resulting upper bounds follow from
0 T 0
Proposition 1.
To show Item 3, note that for any nonnegative random variable Z and any δ ∈ (0,1) we
have EZ ≥ δQ (Z) by Markov’s inequality and consequently, for any alg and instance class I
1−δ
we have ErrE (alg,I) ≥ δErrδ(alg,I). Moreover, by Proposition 1 we have that ErrE (A ,I) ≤
T T T SO
O(cid:0) Errδ(A ,I)(cid:1) for any I ∈ {IL,R,IL,R } and δ ∈ (0, 1). Therefore, for M ∈ {Mℓ,ρ,Mℓ,ρ }
T SO Lip SM-Lip 2 Lip SM-Lip
we have
(cid:32) (cid:33)
ErrE (alg,I) Errδ(alg,I)
PoAE (alg,M) = sup T = Ω δ sup T = Ω(δ)PoAδ(alg,M).
T I∈M ErrE T(A SO,I) I∈M Errδ T(A SO,I) T
(cid:16) (cid:17) (cid:16) (cid:17)
Item4followsfromthefactthatIL,R ⊂ IL,R andthereforeErr alg,IL,R ≤ Err alg,IL,R
Lip SM-Lip T Lip T SM-Lip
(cid:16) (cid:17) (cid:16) (cid:17)
foreveryalg,combinedwithProposition1whichimpliesErr A ,IL,R = Θ(1)Err A ,IL,R .
T SO Lip T SO SM-Lip
Item 5 is immediate from the fact A has the same minimax complexity as A for all the
SFO SO
instance classes in M ∈ {Mℓ,ρ,Mℓ,ρ }, as shown in Proposition 1.
Lip SM-Lip
Finally, to show Item 6, consider alg′ that computes x¯ by applying alg on the modified sample
T
function f¯(x;s) = l r f(x/r ;s), and then returns x = x¯ /r . Clearly, for any distribution P,
1 1 1 T T 1
(cid:18) (cid:19)
F (x¯ )− inf F (x′) = l r F (x )− inf F (x′)
f¯,P T f¯,P 1 1 f,P T f,P
x′∈r1X x′∈X
and therefore
Err
(cid:0) alg,(f¯,P)(cid:1)
= l r Err
(cid:0) alg′,(f,P)(cid:1)
.
T 1 1 T
Moreover, for any α,β > 0, the instance (f,P) ∈ Iα,β if and only if (f¯,P) ∈ Iαl1,βr1 and therefore,
Lip Lip
(cid:16) (cid:17) (cid:16) (cid:17)
Err alg,Iαl1,βr1 = l r Err alg′,Iα,β .
T Lip 1 1 T Lip
Further minimizing over alg shows that
(cid:16) (cid:17) (cid:16) (cid:17)
Err A ,Iαl1,βr1 = l r Err A ,Iα,β .
T SO Lip 1 1 T SO Lip
Substituting into the definition of the PoA, we have
(cid:16) (cid:17)
Err alg,Iα,β
PoA
(cid:16) alg,Ml1,l2,r1,r2(cid:17)
= sup
T Lip
T Lip α∈[l1,l2],β∈[r1,r2] Err T(cid:16) A SO,I Lα i, pβ(cid:17)
(cid:16) (cid:17)
Err alg,Iαl1,βr1
T Lip
= sup
(cid:16) (cid:17)
α∈(cid:104) 1, ll 12(cid:105) ,β∈(cid:104) 1,r r2 1(cid:105) Err T A SO,I Lα il p1,βr1
(cid:16) (cid:17)
l r Err alg′,Iα,β
= sup
1 1 T Lip
= PoA
(cid:16) alg′,Ml2/l1,r2/r1(cid:17)
.
α∈(cid:104) 1,l l2 1(cid:105) ,β∈(cid:104) 1,r r2 1(cid:105) l 1r 1Err T(cid:16) A SO,I Lα i, pβ(cid:17) T Lip
The same argument applies when we replace Lip with SM-Lip everywhere.
24D Information-theoretic hardness results
In this section we establish two information-theoretic hardness results that facilitate the proofs of
Theorems 1, 2 and 5. In each result, we show it is difficult to estimate a random quantity V from
a sequence of binary observations S ,...,S that only carry little information about V, in the
1 T
following sense: conditional on V and S ,...,S , the distribution of S is at most ε far away from
1 t−1 t
a uniform Bernoulli.
We begin with a general bound on the mutual information between V and any estimator Vˆ
of V based on S ,...,S (Appendix D.1). We then consider the case that V ∼ Bernoulli(p) and
1 T
(cid:0)log(1/p)(cid:1)
show that any estimator has error probability Ω(p) for T = Ω (Appendix D.2). Finally, we
ε2
considerV ∼ Unif([n])andshowthatanyestimatorVˆ (evenarandomizedonecapableofadaptively
influencing the S ,...,S ) has constant error probability for T =
Ω(cid:0)logn(cid:1)
(Appendix D.3).
1 T ε2
The developments in this section use the standard notation
I(X;Y) = H(X)−H(X | Y) = H(Y)−H(Y | X)
for mutual information I(X;Y), entropy H(X) = E log 1 and conditional entropy H(X |
x∼X P(X=x)
Y) = E log 1 . We also write
x,y∼X,Y P(X=x|Y=y)
1 1
h (q) := H(Bernoulli(q)) = qlog +(1−q)log
2
q 1−q
for the binary entropy function. Our bounds hinge on the following classical result.
Fact 2 (Fano’s inequality [14]). Let V be a random variable taking n values, and let Vˆ be some
estimator of V. Then
h (P(V ̸= Vˆ))+P(V ̸= Vˆ)log(n−1) ≥ H(V | Vˆ).
2
We remark that prior work [1, 4] also provide optimization lower bounds by controlling mutual
information and applying Fano’s inequality, but in different settings.
D.1 A general mutual information bound
Consistentwiththerestofthispaper,thefollowinglemmameasuresentropyandmutualinformation
in nats (i.e., the logarithm basis is e).
Lemma 1. For any T ∈ N and ε ∈ [0,1] and any random variable V, let S ,...,S be a sequence
1 T
of binary random variables such that, for all t ≤ T
(cid:12) (cid:12)
(cid:12) (cid:12)P(S t = 0 | S 1,...,S t−1,V)− 1(cid:12) (cid:12) ≤ ε
(cid:12) 2(cid:12) 2
with probability 1 w.r.t. S ,...,S and V. Then, for every randomized estimator Vˆ that depends
1 t−1
on V only through S ,...,S , we have
1 T
I(V;Vˆ) ≤ ε2T. (19)
If in addition ε ≤ 1 and, for some p ∈ [0, 1], we have that
2 2
(cid:12) (cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)P(S t = 0)− 1(cid:12) (cid:12) ≥ ε 1 −p
(cid:12) 2(cid:12) 2
for every t ≤ T, then
I(V;Vˆ) ≤ 4pε2T. (20)
25Proof. By the data processing inequality [8, Theorem 2.8.1] and chain rule for mutual information
[8, Theorem 2.5.2],
T
(cid:88)(cid:104) (cid:105)
I(Vˆ;V) ≤ I(S ,...,S ;V) = H(S | S ,...,S )−H(S | S ,...,S ,V) . (21)
1 T t 1 t−1 t 1 t−1
t=1
Using the facts conditioning decreases entropy and that binary random variables have entropy at
most log2, we get
H(S | S ,...,S ) ≤ H(S ) ≤ log2.
t 1 t−1 t
Next, we have
(cid:18) (cid:19)
(cid:104) (cid:16) (cid:17)(cid:105) 1−ε
H(S | S ,...,S ,V) = E h P(S = 0 | S ,...,S ,V) ≥ h ,
t 1 t−1 S1,...,St−1,V 2 t 1 t−1 2
2
by the assumption that (cid:12) (cid:12)P(S t = 0 | S 1,...,S t−1,V)− 1 2(cid:12) (cid:12) ≤ 2ε with probability 1 and the fact that
h (x) has a single maximum at x = 1.
2 2
Combining the last two displays and using log(1+x) ≤ x, we obtain
H(S | S ,...,S )−H(S | S ,...,S ,V)
t 1 t−1 t 1 t−1
(cid:18) (cid:19)
1−ε
≤ log2−h
2
2
1+ε 1−ε
= log(1+ε)+ log(1−ε) ≤ ε2,
2 2
where the last inequality is due to log(1+x) ≤ x. Substituting back into eq. (21) yields the first
claimed bound, I(Vˆ;V) ≤ ε2T.
To obtain the second claimed bound we use the assumption that (cid:12) (cid:12)P(S t = 0)− 1 2(cid:12) (cid:12) ≥ ε(cid:0) 21 −p(cid:1) to
write
(cid:18) (cid:19)
1−(1−2p)ε
H(S | S ,...,S ) ≤ H(S ) ≤ h ,
t 1 t−1 t 2
2
improving the bound on H(S | S ,...,S )−H(S | S ,...,S ,V) to
t 1 t−1 t 1 t−1
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1−(1−2p)ε 1−ε (i) 1−(1−2p)ε 1−ε 1−ε
h −h ≤ − h′
2 2 2 2 2 2 2 2
( =ii) pεlog(cid:18) 1+ 2ε (cid:19) (i ≤ii) 2pε2 ( ≤iv) 4pε2,
1−ε 1−ε
due to (i) concavity of binary entropy, (ii) the fact that h′(x) = log(cid:0)1−x(cid:1) , (iii) the inequality
2 x
log(1 + x) ≤ x and (iv) our assumption that ε ≤ 1. Substituting back into (21), we get that
2
I(Vˆ;V) ≤ 4pε2T, as required.
D.2 Hardness of sharpening a biased Bernoulli prior
Lemma 2. For any p ∈ (0, 1] and T ∈ N, let V ∼ Bernoulli(p) and
2
(cid:115) 
 log 1−p 1
p
ε = min , .
8T 2
 
26Let P = Bernoulli(1−ε) and P = Bernoulli(1+ε). Then, for S ,...,S i∼id P and (potentially
0 2 1 2 1 T V
randomized) estimator Vˆ that depends on V only through S ,...,S , we have
1 T
p
P(Vˆ ̸= V) ≥ .
2
Proof. To streamline notation, we write
q := P(Vˆ ̸= V).
By Fano’s inequality (Fact 2) with n = 2,
h (q) ≥ H(V | Vˆ) = H(V)−I(V;Vˆ). (22)
2
by eq. (20) in Lemma 1,
I(Vˆ;V) ≤ 4pε2T ( ≤i) p log 1−p ( =ii) (cid:16) p− p(cid:17) h′(p) (i ≤ii) h (p)−h (p/2),
2 p 2 2 2 2
(cid:114)
log1−p
due to (i) our choice of ε ≤ p , (ii) the fact that h′(x) = log 1−x and (iii) concavity of
8T 2 x
entropy. Noting that H(V) = h (p) and substituting back to (22), we get that h (q) ≥ h (p/2),
2 2 2
which implies q ≥ p/2 since h is monotone in [0, 1].
2 2
D.3 Hardness of estimating a uniform index
Lemma 3. For any n > 16 and T ∈ N, let V ∼ Unif([n]) and
(cid:40)(cid:114) (cid:41)
logn
ε = min ,1 .
4T
Let S 1,...,S T be binary random variables such that (cid:12) (cid:12)P(S t = 0 | S 1,...,S t−1,V)− 21(cid:12) (cid:12) ≤ 2ε for all
t ≤ T with probability 1. Then, for any (potentially randomized) estimator Vˆ that depends on V
only through S ,...,S , where have
1 T
1
P(Vˆ ̸= V) > .
2
Proof. We have
H(V | Vˆ) = H(V)−I(V;Vˆ) = logn−I(V;Vˆ),
with the second equality due to the uniform distribution of V. Substituting into Fano’s inequality
(Fact 2), rearranging, and using h (q) ≤ log2 for all q, gives the following well-known corollary:
2
(cid:16) (cid:17)
log(n)−I(V;Vˆ)−h 2 P(Vˆ ̸= V) I(V;Vˆ)+log(2)
P(Vˆ ̸= V) ≥ ≥ 1− . (23)
log(n−1) logn
The bound (19) in Lemma 1 and our choice of ε show that
1
I(Vˆ;V) ≤ ε2T ≤ logn.
4
Substituting back into eq. (23) and using n > 16 we obtain
1
P(Vˆ ̸= V) >
2
as required.
27