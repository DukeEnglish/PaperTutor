MULTI-MODAL PREFERENCE ALIGNMENT REMEDIES
REGRESSION OF VISUAL INSTRUCTION TUNING ON LANGUAGE
MODEL
APREPRINT
ShengzhiLi RongyuLin
TIFINalex.li@tifin.com KAUSTrongyu.lin@kaust.edu.sa
ShichaoPei
UniversityofMassachusettsBostonshichao.pei@umb.edu
February19,2024
ABSTRACT
In production, multi-modal large language models (MLLMs) are expected to support multi-turn
queries of interchanging image and text modalitie. However, the current MLLMs trained with
visual-question-answering(VQA)datasetscouldsufferfromdegradation,asVQAdatasetslackthe
diversityandcomplexityoftheoriginaltextinstructiondatasetswhichtheunderlyinglanguagemodel
hadbeentrainedwith. Toaddressthischallengingdegradation,wefirstcollectalightweight(6k
entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics
inagranularfashion,andinvestigatestandardSupervisedFine-tuning,rejectionsampling,Direct
Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are
abletosurpassinstruction-followingcapabilitiesofthelanguagemodel,achievinga6.73scoreon
MT-Bench,comparedtoVicuna’s6.57andLLaVA’s5.99despitesmalldatascale. Thisenhancement
intextualinstructionproficiencycorrelateswithboostedvisualinstructionperformance(+4.9%on
MM-Vet,+6%onLLaVA-Bench),withminimalalignmenttaxonvisualknowledgebenchmarks
comparedtopreviousRLHFapproach. Inconclusion,weproposeadistillation-basedmulti-modal
alignment model with fine-grained annotations on a small dataset that reconciles the textual and
visualperformanceofMLLMs,restoringandboostinglanguagecapabilityaftervisualinstruction
tuning.
1 Introduction
Recentadvancementsinartificialintelligencehaveledtotheriseofmulti-modallargelanguagemodels(MLLMs),
which combine textual and visual interpretation capabilities in a single model [1]. However, effectively blending
multi-modality in one system has proven non-trivial. The integration of diverse data forms often creates internal
representationconflicts,givingrisetotheissueknownas"catastrophicforgetting"[2]. Thediversityconstraintinvisual
questionanswering(VQA)datasetscouldbeattributedasasourceoftheissue.VQAtaskstypicallyfocusondescriptive
queriesaboutimagecontents,whereastextualdatasetsencompassabroaderrangeofcomplexcognitivetasks,including
reasoning,writing,summarization,andcoding. Thisdiscrepancyindatasetcomplexityisakeyfactorcontributing
totheobservedperformancedegradationinMLLMs. OurevaluationofmodelssuchasBLIP-2,InstructBLIP,and
LLaVAagainstlanguageinstruction-followingbenchmarkslikeMT-Bench[3]andAlpacaEval[4]revealeddiminished
languagecapabilitiesincomparisontotheirlinguisticbackbones. Forinstance,LLaVA,builtontheVicuna-13bLLM,
demonstratedadeclineinMT-Benchperformancefrom6.57to5.92,evenunderperformingtheVicuna-7Bmodel.
Drivenbythelimitationsobservedindistillation-basedinstructiontuning,particularlyitsconstrainedgeneralizability
andthenarrowperformanceimprovementsontasksoutsidethetrainingdistribution,thisstudyinvestigatestheefficacy
ofdistillation-basedpreferencealignmentinaddressingmodalityconflictinMLLMs. Thedecisiontoexplorethis
4202
beF
61
]LC.sc[
1v48801.2042:viXraMulti-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
avenueispredicatedonthehypothesisthatintegratingAI-generatedpreferencedatacanprovideamoregranularand
nuancedalignmentwithhumanexpectations,potentiallymitigatingtheadverseeffectsofmodalityconflict.
Thisstudyrigorouslyevaluatesthreebaselinemethodologies—DirectPreferenceOptimization(DPO),SteerLM,and
RejectionSampling—aspotentialsolutionstoutilizethedistilledpreferencedataandenhancetheinstruction-following
capabilitiesandaddressthemodalityconflictinherentinMLLMs. Eachofthesemethodsoffersauniqueapproachto
modelalignment,fromthedirectoptimizationofpreferencesinDPOtotheconditionalsupervisioninSteerLMandthe
selectiveacceptanceinRejectionSampling. OurempiricalanalysisrevealsthatDPO,inparticular,demonstratesa
pronouncedefficacyinreconcilingtheperformancedisparitiesobservedbetweentextualandvisualmodalities. By
leveraging a refined preference dataset, fine-tuned with the DPO objective and supplemented with comprehensive
annotationsfromadvancedAImodels,DPOnotonlyaddressesthemodalityconflictbutalsosignificantlyenhances
themodel’sperformanceacrossaspectrumofbenchmarks. Theresultsindicatethat,throughtheapplicationofDPO,
MLLMscanachieveamorerobustalignmentwithhuman-likepreferences,therebymitigatingtheadverseeffectsof
catastrophicforgettingandmodalityconflict,andelevatingthemodels’capabilitiestoalevelthatsurpassestraditional
instructiontuningmethods.
Ourmaincontributionsare:
1. ExplorationofModalityDegradation: Thisworkisattheforefrontofidentifyingandaddressingmodality
degradation in MLLMs, a phenomenon where visual instruction tuning detrimentally impacts language
instructioncapabilities. Oursystematicinvestigationintothisissuecontributesnovelinsightstothefield,
layingthegroundworkforfurtherresearchinmitigatingsuchdegradation.
2. InnovativePreferenceAlignmentMethodology: Weproposeagroundbreakingpreferencealignmentframe-
work that not only mitigates the negative effects of visual instruction tuning on text-based tasks but also
enhancestheMLLM’sperformancebeyonditsoriginallanguagemodelbackbone. Thismethodalsodemon-
stratessignificantimprovementsinvisualinstructionbenchmarks,underscoringtheefficacyofpreference
alignmentinaugmentingMLLMfunctionalities.
3. EfficientDataAnnotationScheme:Ourdatacollectionstrategyemploysagranularqualitymetricannotation
format,leveragingcost-effectivecommercialAPIs. Thisscalableapproachenablestheefficientproductionof
high-qualitydatasets,addressingacriticalchallengeinMLLMdevelopmentandfacilitatingextensivemodel
trainingandrefinement.
2 RelatedWork
2.1 MLLMsandVisualInstructionTuning
Incorporatinganothermodalityintolargelanguagemodelsrepresentsanaturalevolutionforthesesystems. Modality
expansioncanbeachievedthroughsystem-levelenhancementsatinferencetime,withapproachessuchasMm-react
[5],VisualChatGPT[6],andHuggingGPT[1]enablingtheLLMtoinvokeoff-the-shelfvisionmodelsandAPIs. An
alternativestrandofresearchinvolvesthetrainingofend-to-endMLLMs. Toavoidtheprohibitivecostsassociated
withpre-trainingfromscratch,thesemodelsoftenintegratepre-trainedvisionmodelswithlargelanguagemodels,
applyingvariousdegreesofmodalityadaptation. Mini-GPT4[7]focusessolelyontrainingalinearprojectionmatrix
toconnectCLIP-based[8]visionrepresentationswiththeLLaMAmodel[9]; BLIP-2introducesacross-attention
moduletoextractvisiontokensrelevanttothequery. BothLLaVA[10]andmPlug-OWL[11]featurecross-modality
connectorsbetweenthevisionandlanguagedomains,buttheyalsofine-tunetheLLMandvisionencoder,respectively.
Flamingo[12],incontrast,incorporatesnewcross-attentionlayersdirectlyintotheLLM.
Inthelanguagedomain,Weietal.[13]discoveredthatfine-tuningabaseLLMwithinstructionsdescribedinnatural
languageenhancesthemodel’sabilitytofollowthoseinstructions. Inasimilarvein,MLLMsaretypicallyfine-tuned
withinstructions;Mini-GPT4[7]utilizedtemplateinstructionsbasedonimage-textpairs,whileInstructBLIP[14],Otter
[15],andLLaVA[10]employedhuman-writtenvisualquestion-answersorsyntheticallygeneratedquestion-answer
pairsbypromptingGPT-4withCOCOcaptionsandboundingboxes[10]. However,consideringthatbothLLaVA
andInstruct-BLIPutilizeVicuna[16]—aninstruction-tunedLLaMA—itremainsatopicofdebatewhethertheirsteps
ofvisualinstructiontuninggenuinelyaddtothemodel’sinstruction-followingcapabilitiesormerelyconformtothe
instruction-followingformatusedinVicuna’straining.
2.2 MitigatingModalityConflictinMLLMs
Topreservetheabilitytofollowlanguageinstructions,mPLUG-OWL[17]andLLaVA1.5[10]incorporatelanguage-
onlyinstructiondatabackintotheirmixedvisual-languageinstructiondatasets,specificallyShareGPT.Itisnoteworthy
2Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
thattheLLMbackboneofLLaVA1.5,Vicuna,hadbeenpreviouslytrainedonthisidenticalShareGPTdataset. Further
investigation reveals that, despite the integrated dataset, LLaVA 1.5 exhibits degradation in language instruction-
followingcapabilities;theMT-BenchscoreforLLaVA-1.5-13bisnotablylowerthanthatforVicuna-V1.5-7b.
WhilemPLUG-OWL-2[17]presentspromisingsolutionstothechallengesofmodalityconflict,andhasshownsuperior
performanceontext-basedbenchmarks,italsointroducesincreasedparametercountandmorecompleximplementation,
whichposespracticalchallenges. Specifically,mPLUG-OWL-2implementsmodality-adaptivemodulesthatinclude
distinctlayernormalization,aswellasseparatekeyandvalueprojectionmatricesfortextandvisualtokens,whilst
maintainingasharedstructureforqueryprojectionmatrices. Themodelisfine-tunedfromthenon-instruction-tuned
LLaMA-2-7Bbasemodel,incorporating548Ktextualinstructiondatasamplesfromatotalof1.2M,derivedfromboth
SlimOrca[18]andShareGPT.ThisapproachhasenabledmPLUG-OWL-2toexcelinlanguageandvisual-language
tasks,outperformingRLHF-augmentedLLaMA-2Chatontext-centricevaluationssuchasMMLUandBBH[17].
However,theintroductionofmodality-specificmoduleshasledtoanincreaseinthemodel’sparametercountfrom
7.2billionto8.2billion. Additionally,theseparateprocessingpathsforvisualandlanguagetokenshaveresultedina
moreintricatecomputegraph,complicatingtheutilizationoffusedGPUkernelstoachieveefficientinference. The
methodologiesdiscussedsofararereliantonstandardfine-tuningpractices,necessitatingasignificantaugmentationof
computationalresourcestointegratetextinstructiondataeffectively,withtheaimofresolvingthechallengesposedby
modalityconflict.
2.3 Distillation-basedInstructionTuning
Leveragingtheoutputoflargeproprietarymodels,smalleropen-sourcemodelssuchasVicuna[16],Alpaca,andmore
recentlyShareGPT4V[19],havebeenfine-tuned,althoughthisapproachhaslimitationsintermsofgeneralization
capabilities. Gudibandeetal.[20]observedthatmodelsfine-tunedthroughinstructiontuningbyimitationbarelybridge
theperformancegapintasksbeyondthescopeofthetrainingdata. Theycontendthatimitationasastrategyisafalse
promise,assertingthatonlyasignificantvolumeofimitationdataoralargerbasemodelcanclosethedisparitybetween
openandclosed-sourcemodels[20]. WhilerecognizingtheutilityoftheexpansiveGPT4VdatasetlikeShareGPT4V
[19],itispositedthatsuchscalingofdistillation-basedinstructiontuningprimarilyextendsthemodel’scompetency
withinthedistributionitwastrainedonratherthanitsout-of-distributiongeneralizability. Furtherresearchindicated
thatdistillationinstructiontuningonasmallerscaletendstoskewthemodel’sperformancetowardsanichesubsetand
significantlyimpairitsbroaderapplicability. Thiswasevidencedbyabaselineexperimentinwhichfine-tuningLLaVA
witha6kVQAdataset,sourcedfromGeminiPro-generatedanswers,resultedinpronouncedperformancedeclines
acrossbothtextualandvisualbenchmarks.
2.4 PreferenceAlignment
TheInstruct-GPTseries[21]hasshownthatmerelyemployingsupervisedfine-tuning(SFT)onLargeLanguageModels
(LLMs)isinsufficientforaligningthemwithhumanpreferences. ThetechniqueofReinforcementLearningfrom
HumanFeedback(RLHF)[21]addressesthisbyconstructingarewardmodelthatencapsulateshumanpreferencesand
thenapplyingreinforcementlearningtomaximizethisreward. TheDirectPreferenceOptimization(DPO)approach
positsthatdirectlytuningthepreferencedatasetcanserveasaneffectivesubstituteforrewardmodeling,offeringthe
addedbenefitofreducedcomputationalcomplexity. Anothernovelmethod,knownasrejectionsamplingSteerLM,has
recentlybeenidentifiedtoachieveperformanceakintoRLHFbyincorporatinghuman-annotatedqualitymetricsbefore
generation,servingasaconditionalSFT-basedstrategyforalignment[22]. OurexperimentswithDPO,SteerLM,and
rejectionsamplingreferencethepriorworkonLLaVA-RLHF[23],usingitasabenchmarkforRLHFperformance.
2.5 DistillingAIFeedbackforPreferenceAlignment
Intherealmofalignmentmethods,relianceonhuman-annotatedpreferenceannotationsiscommon. Whileeffective
onalargescale,thisapproachincurssubstantialcostsandoperationalcomplexities[24]. Theeffectivenessofreward
modelsbasedonpairwiserankingisconstrainedbytheinherentsubjectivityofhumanpreferences,withLLaMA’s
rewardmodelachievinganaccuracyrangeof64.3-70.6%,andtheLLaVA-RLHFmodelreaching67%. Inresponseto
theselimitations,Zephyr[25]andUltraFeedback[26]haveutilizedpreferenceannotationsdistilledfromGPT-4totrain
modelswith7Bparameters,achievingperformancelevelscomparabletothoseof70Bparametermodels. Motivatedto
adaptthisdistillation-preferencealignmentapproachforMLLMs,ourworkintroducesanuancedchain-of-thought
promptingtechnique,coupledwithadetailedannotationguide,spanningfiveassessmentmetrics.
3Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
Metric Description
Helpfulness Measuresiftheresponsefullyaddressestheprompt’srequest.
Correctness Assessestheaccuracyandrelevanceoftheinformation,ensuringit’sfreefromerrorsor
misinformation.
Coherence Evaluatestheclarityandlogicalconsistencyoftheresponse.
Complexity Looksatthelevelofsophisticationinthelanguageused,fromsimpletoadvanced.
Verbosity Considersthebrevityorlengthinessoftheresponseinrelationtotheprompt’sneeds.
Table1: GranularannotationformatandlabellingguideproposedinHelpSteer[27]usedinVQAannotationcollection.
LLaVA-1.5-13bgenerates4candidates. UsingtheHelpSteerlabelingguide,images,andquestions,Geminirateseach
completionwithascoreof0-4ineachmetric.
Figure1: StartingfromanSFT-edcheckpoint,wegenerate4completionsforagivenimage-questionprompt. These
answersarethenpresentedtoGeminitoobtaingranularannotationsgivenalabelingguide. Weconstructapreference
datasetof(image-textprompt,preferredcompletion)and(image-textprompt,rejectedcompletion). Webenchmarked
DPO,Rejectionsampling,andSteerLMalignmentmethods,inadditiontoapureSFTbaselineusingGeminiprovided
answerdirectly
3 Method
ThefocusofthisstudyistoinvestigateimprovingthetextinstructionfollowingofMLLMswhileretainingorpotentially
enhancingtheirmulti-modalreasoning. Forthispurpose,weproposetoharnessalignmentmethodsthatutilizeself-
sampledresponsesandpreferenceannotations. Therefore,PureSupervisedFine-Tuning(SFT)isdeemedunsuitable
andthusexcludedfromourmethodology.
3.1 DataCollection
SourcesofMulti-modalPrompts. Wehavecurated3,000samplesfromSciGraphQA[28]andanequalnumberfrom
LRV-Instruct[29]toassembletheimage-textpromptsforourmulti-modalpreferencedataset. TheLRV-Instructdataset
isavisualinstructiondatasetaimedatmitigatinghallucinationsbyincorporatingbothpositiveprompts(inquiringabout
objectspresentintheimage)andnegativeprompts(requestinginformationaboutabsentobjects)[29]. Theinclusionof
negativeexamplesencouragesthemulti-modalLLMtocriticallyevaluatethepromptsandidentifyinstanceswherethe
requestedobjectsarenotpresent. TheLRV-Instructdatasetenhanceditsperformanceinreducinghallucinations,as
evidencedbyitsapplicationinthePopeframework[29].
GranularAnnotation: Wegenerate4completionsforthedatasetaboveusingLLaVA-1.5-13Bwithatemperatureof
0.7andthenpromptGemini-Vision[30]withthelabelingguideofHelpSteergavetoScaleAIworkers[27],images,
questions,andthe4completions,obtaininggranularannotationsforvariousqualitymetrics,includinghelpfulness,
correctness,andcoherence,providingmultifacetedinsightsintothemodel’soutputs. WeleveragedZero-ShotChain-
ofThoughtprompting[31]suchthattheGeminigivesthereasoningforratingeachresponse, aninnercalibration
monologue,andratingsasshownin1. AppendixprovidestwoannotationexamplesontheLLaVA-RLHFdataset[23]
4Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
suchthatwecanvisualizehowGeminireasonsandrateeachmetriccomparedtoabinarycrow-sourcedworker-provided
preference.
WeselectedGeminiPro(dated01/01/2024)foritsperformance,costefficiency,andbiasmitigation[30]. Leadingthe
OpenCompassmulti-modalleaderboardwithanaveragerankof1.89,outperformingGPT-4V’s2.89[32],itsgenerous
freetiersupportsextensivedatacollection. Crucially,usingGPT-4(V)forbothbenchmarkjudginganddatalabeling
couldintroducebias,potentiallyskewingourmodelstowardsGPT-4’spreferences. Thisconsiderationledustooptfor
Geminitoensureamoreobjectiveevaluationofourfine-tuningefforts.
3.2 AlignmentMethods
Self-sampledSteerLM:SteerLM,aconditionalSupervisedFine-Tuning(SFT)technique,alignsLLMsbyaugmenting
promptswithadescriptionofthedesiredresponsequality, asintroducedby[22]. ThismethodconditionsSFTon
granular annotation generated by Gemini and surpasses traditional SFT and alignment strategies. We construct a
conditionalprompttemplateforthistechniquebyincorporatingaconditionalpromptfollowingHelpSteerguidelines.
Forinstance,
RejectionSampling: FollowingConstitutionalAI[33]andLLaMA-2[24],weadoptasimplifiedrejectionsampling
approach. Specifically, we select the top-scoring response from the four completions described above, based on
Gemini’saggregatedscoresforhelpfulness,correctness,andcoherence. AndweapplystandardSFTbasedonthe
selectedresponseswithoutanyadditionalpromptconditions,followingastraightforwardformat: (image,prompt,best
response).
Direct Preference Optimization (DPO): We convert our dataset of granular annotation into a preference set by
selecting the highest score and the worst response. Specifically, we generate an aggregated score per response by
summingHelpfulness,andCorrectness. WhenpromptingGeminiforannotations,Geminireviewsfourresponsesand
providesabsolutequalitymetrics,whichweconvertedintoapreferencedataset. Weform(preferred,rejected)pairsby
selectingthebestresponseandrandomlyselectinganotherfromthefourresponses,withafilterspecifyingthatthe
preferredresponseisatleast2pointslowerinsummedscoresacrosshelpfulness,correctness,andcoherencethanthe
rejectedresponse.
SFTfromGeminiResponsesThisisanimportantbaselinenotusingself-sampling,butusinganswersfromGemini
directlyforpureSFT.Geminiwaspromptedwiththequestionsandimagesfromourdataset. Wegathertheanswer
directlyfromGeminiansweringthequestionandusethesamehyper-parametersinSteerLMandRejectionSampling.
4 Experiments
4.1 ExperimentalSetup
Fortraining,weutilizeAzureCloud(NC-A100series)with4A100-80GGPUs. Inallexperiments,LoRAisemployed
forparameter-efficienttuning. FortheSFTexperiments,includingstandardSFTwithGeminiresponses,rejection
sampling,andself-sampledSteerLM,weadheredtothesamehyperparametersusedinLLaVA-v1.5’sinstructiontuning.
InourDPOexperiments,weperformedahyperparametersearchbasedon1000samplesonLLaVABench,exploring
variousvaluesforbeta(0.1,0.2,and0.3,averagedandnon-averagedlogprobabilities,andlearningratesof5e-5and
5e-6. Wepre-computedthelogprobabilitiesofthereferencemodel(LLaVA-v1.5-13b)forourpreferencedatasetbefore
training. CompletelistisprovidedintheAppendix.
Table2: Performancecomparisonamongalignmentstrategies. TheresultsdemonstrateDPO-13B’ssuperiorperfor-
mance,particularlyinreconcilinglanguagecapabilitieswhileenhancingvisualtaskperformance,validatingtheDPO
methodology’sefficacyinmulti-modalalignment.
ModelName VisualInstructionBenchmark VisualMulti-ChoiceBenchmark TextInstructionBenchmark
MM-Vet LLaVA-bench PoPe MM-Bench MT-bench AlpacaEval
Vicuna-1.5-13b[16] - - - - 6.57 81.4
LLaVA-1.5-13b[10] 36.3 73.1 0.859 67.4 5.99 79.3
LLaVA-RLHF-13b[23] 37.2 76.8 0.869 60.1 6.18 81.0
Alignmentmethodwebenchmarked,finetuningLLaVA-1.5-13b
StandardSFT 36.5 63.7 0.850 65.4 5.01 50.2
SteerLM 35.2 67.0 0.878 65.1 5.70 68.8
Rejection-sampling 38.0 70.6 0.883 67.6 6.22 74.9
DPO 41.2 79.1 0.870 66.8 6.73 86.4
5Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
4.2 Benchmarks
Wehavethefollowingthreesetsofbenchmarks.
VisualInstructionBenchmarks
• MM-Vet:MM-Vetmeasuressixcorevisual-languagecapabilitiesacross128tasks,providingacomprehensive
evaluationofmulti-modalunderstanding. Itblendsmath,reasoning,andvisualknowledge. [34].
• LLaVA-Bench: LLaVA-Bench(inthewild)isadevbenchmarkforLLaVAcomprising60tasks,designedto
testvisualinstruction-followingandquestion-answeringcapabilitiesinnaturalsettings[10].
VisualMulti-ChoiceBenchmarks
• PoPE:Anobjecthallucinationbenchmarkwith8,440samples, aimedatevaluatingthemodel’sabilityto
discernanddescribevisualcontentaccurately. Themulti-modalLLMispromptedtoansweryesornoto
objectsthatcouldappearintheimages[35].
• MM-Bench: Serving as a comprehensive multi-modal benchmark, MM-Bench is a multi-choice visual
knowledgeandreasoningbenchmarkwith4.7Ksamples[36].
LanguageInstruction-FollowingBenchmarks
• MT-Bench: UtilizingLLMtoapproximatehumanpreferenceswithover80%agreement,MT-Benchfocuses
onmeasuringthehelpfulnessofresponsesacross160samplesinsingle-turnandmulti-turnsettings. [3].
• Noisy-image-contextMT-Bench: Inspiredby[37]wherethelanguagecommonsensecapabilityofMLLMs
wasevaluatedbysendingnon-informativeimagesincludingtheblankandrandomimagestotheMulti-modal
LLMtoassesstheirlanguagecommonsensecapability,weproposeMT-Benchwithnon-informativeimage
context to assess the language instruction-following capability of MLLM. We evaluate LLaVA, Blip-2,
InstructBLIP.Withinthethemeofthispaper,weemphasizemulti-modalLLMinproduction.
• AlpacaEval: AlpacaEval leveraged GPT-4 to assess the percentage of cases where the candidate LLM
outperformsGPT-3API(text-davinci-003)across160evaluations[4].
Werunbenchmarkcodeonceontargetmodel,withgreedydecodingunlessabenchmarkuseadifferenttemperature
setting.
4.3 Results
Table2illustratesacomparativeanalysisofvariousalignmentmethods,includingRLHF,StandardSFT,SteerLM,
RejectionSampling,andDPO,aimedatenhancingthelanguagecapabilitiesoftheLLaVAmodelthatwerecompromised
post-visualinstructiontuning. Vicuna,thebaselanguagemodelofLLaVAistestedonlanguagetasks. Aftervisual
instructiontuning,LLaVAexperiencesadeclineinlanguagebenchmarkscoresfromVicuna(from6.57to5.99onMT-
Bench,andfrom81.4to79.3onAlpacaEval). However,DPO,amongthealignmentstrategiesonthe5kmulti-modal
dataset,notonlymitigatesthedegradationproblembutalsosurpassesVicuna’sperformanceonbothbenchmarks.
Inopen-endedvisualinstructiontasks,DPOsignificantlyoutperformsthebaselineLLaVAandLLaVA-RLHFmodels
onboththeMM-VetandLLaVA-bench. Theseopen-endedbenchmarks, structuredsimilarlytoMT-Bench where
GPT-4assessestheresponsesagainstagoldstandard,revealedarelativelyhigh0.73PearsonCorrelationbetween
MM-VetandMT-Bench(n=7,p=0.1).
Invisualmulti-choicebenchmarks,PoPe,whichevaluatesforhallucination,andMM-Bench,whichassessesworld
knowledge and reasoning, rejection sampling emerged as the most effective method, whereas DPO showed lesser
efficacy. WhileDPOimprovedopen-endedbenchmarks,itslightlydecreasedLLaVA’sMM-Benchscorefrom67.4
to66.8,contrastingwithLLaVA-RLHF’snotabledropfrom67.4to60.1,indicatingalesssignificantalignmenttax.
Figure2illustratestheeffectofscalingtheDPOdatasetontheMT-Benchscores,signifyingtheefficacyofDPOin
advancingtheperformanceoftheLLaVA-v1.5-13bmodelaftervisualinstructiontuning. Thebaselineat0%DPOdata,
markedatascoreof5.99,representstheinitialperformanceofLLaVA-v1.5-13b. AstheDPOdataset’ssizeincreases,a
notableriseinMT-Benchscoresisobserved,peakingat6.73with100%datautilization. Remarkably,theperformance
surpassesthatoftheVicuna-v1.5-13bbenchmarkusinglessthan75%or4.2KoftheDPOdata,underliningDPO’s
dataefficiency. ThisdatascalingtrendemphasizesDPO’spotentialasaneffectivealignmentstrategyforMLLMs,
addressingthechallengeofperformancedegradationduetovisualinstructiontuning.
6Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
Figure2: AdvancesinMT-BenchscoresviaDPOdatascaling
5 Discussion
Thescarcity,highcost,andinconsistencyofexistingmulti-modalpreferencedatasetspresentsignificantchallengesto
modelalignment,asevidencedbyoursubjectiveassessmentsandobjectiveevaluationsusingGemini. Ourmanual
labelingofaselectedsubsetfromtheLLaVA-RLHFhumanpreferencedatasetunderscoredthedifficultyinachieving
consensusoridentifyingclearpreferences,sheddinglightontheinherentsubjectivityofthesedatasets. Toquantify
theseobservations, weembarkedonatargetedexperimentinvolving500samples, whereinGeminiannotatedtwo
distinctresponsesfromthedatasetemployedintheLLaVa-RLHFrewardmodel’sdatacollection[23]. Thisfacilitated
a direct comparison between Gemini’s annotations and the human preferences. The correlation heatmap depicted
inFigure3revealsanotablyweakcorrelationbetweenhumanpreferencesfromLLaVA-RLHFandGeminiscores,
with correlation coefficients falling below 0.1. This observation was corroborated by further manual inspections,
whichfrequentlyresultedinaninabilitytodefinitivelydetermineclearpreferences. Thisfindingunderscoresthehigh
subjectivityandindividualbiaswithintheLLaVA-RLHFpreferencedata. Contrarily,thedetailedannotationframework
employedbyGeminirepresentsascalableandmoreobjectivemethodforcollectingpreferencedata,offeringaviable
solutiontothelimitationsobservedincurrentdatasets.
Thesuperficialalignmenthypothesisstatesthatamodel’sknowledgecapabilitiesarelargelyacquiredduringitsinitial
pre-trainingstage[38]. Acorollaryofthishypothesisisthatalignmenttuningrefinesthemodeloutputgenerationwitha
preferredresponseformatratherthanknowledgeacquisition.Asaresult,modelscanbeeffectivelyrealignedpost-visual
instructionusingarelativelysmallsetofexamples[39]. ThisprincipleappliestoMLLMsaswell, whichacquire
multi-modalknowledgerepresentationviavisualinstructiontuning[10]. However,existingworkmixedlarge-scaletext
instructiondata(518Koutof1.23millionincaseofmPlug-OWL2,and40KincaseofLLaVA-1.5). Wehypothesize
thatthedata-inefficiencyaboveisattributedtotheunderlyingalignmentstrategyanddemonstratethatonewouldneed
onlyasmallalignmentdatasetsolongasaproperalignmentstrategysuchasDPOisutilized.
AssuggestedbyTable2,DirectPreferenceOptimization(DPO)emergesasacomputationallyefficientsolutionfor
enhancingmodelperformanceinthemixed-modalalignmentspace. Unlikethemixingtextinstructionasdescribed
aboveorLLaVA-RLHF,whichusedalarge82Kdatasetandcomplextrainingpipelineinvolvingrewardmodelingand
PPO,DPOachievessignificantimprovementsinlanguagecapabilitieswithasmallerdatasetandone-stoptraining
setup. AnotableadvantageofDPOisitsminimalalignmenttax,whichcurtailsthedegradationofexistingknowledge,
asevidencedbyitsperformanceonbenchmarkslikeMM-Bench,whereDPOshowsminimalimpact. Thismethodnot
onlyenableseffectivealignmentofmulti-modalmodelspost-visualinstructiontuningbutalsoensuresthepreservation
ofmodelperformance. Wealsonotethatas3showed,ourDPOmodelismostmorerobustthanotherbaselinesina
real-worldconditionwhereusermayhaveirrelevantimageinthecontext. Ourmethodologyexhibitsnotableproficiency
invaluealignmentanddataefficiency,yetitisimperativetoacknowledgecertainlimitationsandpotentialrisks.Onekey
considerationisthescalabilityofourapproach. Whileourdatascalinganalysissuggestssignificantimprovementsup
toa6Kpreferencedataset,thefullextentofscalabilitybeyondthisthresholdremainsunexplored. Asthefoundational
7Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
Figure3: PearsonCorrelationHeatmapamongthediffer-
enceinGemini-AnnotateddataattributesandLLaVA-RLHF
humanannotatedpreference(n=500).
Table3: ThisbenchmarksaddsirreverentimageincontextwhenbenchmarkingMT-Bench,testingforrobustnessina
real-worldcondition. Vicuna-7BandVicuna-13Bareasbaselinereference.
Model LLM Noisy-imageMT-Bench
Vicuna13Bv1.5 Vicuna 6.57
13B
Vicuna7Bv1.5 Vicuna 6.17
7B
BLIP-2 FlanT5 1.93
InstructBLIP Vicuna 4.73
7B
LLaVA-v1.5-13b Vicuna 5.92
13B
DPO(ours) Vicuna 6.63
13B
open-sourcedmodelslikeLLaVAevolveincomplexityandsize,theeffectivenessofourdistillation-basedapproach
mightencounterdiminishingreturns.
Addressingtruehumanpreferenceaccuratelyisanotherpivotalconcern. Thepreferencedatasetdistilledfromartificial
annotationsmaynotfullyencapsulatethenuancedspectrumofhumanvalues,raisingethicalconsiderationsregarding
modelalignmentwithsocietalnormsandindividualexpectations. Moreover,theinherentrisksassociatedwithsafety
andbiaspropagationaremagnifiedwhenmodelsarefine-tunedonartificially-labeleddata,potentiallyreinforcing
existingprejudices.
6 Conclusion
Inthispaper,weaddressedtheperformancedeclineofthewidely-usedmulti-modalLLM,LLaVA-1.5,onlanguage
instruction-following benchmarks. We investigated various alignment strategies to enhance the model’s language
instructioncapabilities,includingstandardSFTwithrejectionsampling,usinggoldresponsesfromGemini,SteerLM,
andDPO.OurcomprehensivebenchmarkevaluationsrevealedthatDPOstandsoutasthemosteffectivemethod,not
onlyrestoringbutalsosurpassingthelanguagecapabilitiesofLLaVA’sfoundationalVicunamodelonbothMT-Bench
andAlpacaEval. Specifically,LLaVA’sperformanceonMT-BenchhaddippedfromVicuna’s6.57to5.99,whereas
ourDPOmodeladvancedto6.73. Beyondlanguageproficiency,DPOalsobolsteredperformanceonmulti-modal
benchmarks,enhancingaccuracyby4.9%onMM-Vetand6%onLLaVA-Bench. Remarkably,DPOachievedthiswith
just5kpreferenceexamples,provingmoredata-efficientthanotherstrategieslikeRLHF,whichtypicallydemandover
80ktrainingexamples. OurfindingshighlightDPO’sminimal‘alignmenttax,’effectivelyboostingoneperformance
area without substantially detracting from another. This aspect is particularly valuable for practitioners aiming to
maintainthepre-trainingknowledgethroughoutmulti-modalalignment.
8Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
7 References
References
[1] YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang. Hugginggpt: Solvingai
taskswithchatgptanditsfriendsinhuggingface,2023. URLhttps://arxiv.org/abs/2303.17580.
[2] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiARusu,Kieran
Milan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal. Overcomingcatastrophicforgettingin
neuralnetworks. Proceedingsofthenationalacademyofsciences,114(13):3521–3526,2017.
[3] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,Zhuohan
Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with
mt-benchandchatbotarena,2023.
[4] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
TatsunoriB.Hashimoto. Alpacaeval: Anautomaticevaluatorofinstruction-followingmodels. https://github.
com/tatsu-lab/alpaca_eval,2023.
[5] ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,EhsanAzarnasab,FaisalAhmed,ZichengLiu,CeLiu,
MichaelZeng, andLijuanWang. Mm-react: Promptingchatgptformultimodalreasoningandaction. ArXiv
preprint,abs/2303.11381,2023. URLhttps://arxiv.org/abs/2303.11381.
[6] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:
Talking, drawing and editing with visual foundation models. ArXiv preprint, abs/2303.04671, 2023. URL
https://arxiv.org/abs/2303.04671.
[7] TaoGong,ChengqiLyu,ShilongZhang,YudongWang,MiaoZheng,QianZhao,KuikunLiu,WenweiZhang,
PingLuo,andKaiChen. Multimodal-gpt:Avisionandlanguagemodelfordialoguewithhumans. ArXivpreprint,
abs/2305.04790,2023. URLhttps://arxiv.org/abs/2305.04790.
[8] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InMarinaMeilaandTongZhang,editors,Proceedingsofthe38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
ProceedingsofMachineLearningResearch,pages8748–8763.PMLR,2021. URLhttp://proceedings.mlr.
press/v139/radford21a.html.
[9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundationlanguage
models. ArXivpreprint,abs/2302.13971,2023. URLhttps://arxiv.org/abs/2302.13971.
[10] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning,2023.
[11] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,Pengcheng
Shi,YayaShi,etal. mplug-owl: Modularizationempowerslargelanguagemodelswithmultimodality. ArXiv
preprint,abs/2304.14178,2023. URLhttps://arxiv.org/abs/2304.14178.
[12] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Mensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelforfew-shotlearning.
AdvancesinNeuralInformationProcessingSystems,35:23716–23736,2022.
[13] JasonWei,MaartenBosma,VincentY.Zhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM.
Dai,andQuocV.Le. Finetunedlanguagemodelsarezero-shotlearners. InTheTenthInternationalConference
on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL
https://openreview.net/forum?id=gEZrGCozdqR.
[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,
PascaleFung,andStevenHoi. Instructblip: Towardsgeneral-purposevision-languagemodelswithinstruction
tuning,2023.
[15] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: Amulti-modal
modelwithin-contextinstructiontuning. ArXivpreprint,abs/2305.03726,2023. URLhttps://arxiv.org/
abs/2305.03726.
[16] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,
YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: Anopen-sourcechatbotimpressing
gpt-4with90%*chatgptquality,2023. URLhttps://lmsys.org/blog/2023-03-30-vicuna/.
9Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
[17] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and
JingrenZhou. mplug-owl2: Revolutionizingmulti-modallargelanguagemodelwithmodalitycollaboration,2023.
[18] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium".
Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. URL https:
//https://huggingface.co/Open-Orca/SlimOrca.
[19] LinChen,JinsongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin. Sharegpt4v:
Improvinglargemulti-modalmodelswithbettercaptions,2023.
[20] ArnavGudibande,EricWallace,CharlieSnell,XinyangGeng,HaoLiu,PieterAbbeel,SergeyLevine,andDawn
Song. Thefalsepromiseofimitatingproprietaryllms,2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswithhuman
feedback. AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022.
[22] YiDong,ZhilinWang,MakeshNarsimhanSreedhar,XianchaoWu,andOleksiiKuchaiev. Steerlm: Attribute
conditionedsftasan(user-steerable)alternativetorlhf,2023.
[23] ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,ChunyuanLi,YikangShen,ChuangGan,Liang-Yan
Gui,Yu-XiongWang,YimingYang,KurtKeutzer,andTrevorDarrell. Aligninglargemultimodalmodelswith
factuallyaugmentedrlhf,2023.
[24] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels.
ArXivpreprint,abs/2307.09288,2023. URLhttps://arxiv.org/abs/2307.09288.
[25] LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,YounesBelkada,Shengyi
Huang,LeandrovonWerra,ClémentineFourrier,NathanHabib,NathanSarrazin,OmarSanseviero,AlexanderM.
Rush,andThomasWolf. Zephyr: Directdistillationoflmalignment,2023.
[26] GanquCui,LifanYuan,NingDing,GuanmingYao,WeiZhu,YuanNi,GuotongXie,ZhiyuanLiu,andMaosong
Sun. Ultrafeedback: Boostinglanguagemodelswithhigh-qualityfeedback,2023.
[27] ZhilinWang,YiDong,JiaqiZeng,VirginiaAdams,MakeshNarsimhanSreedhar,DanielEgert,OlivierDelalleau,
JanePolakScowcroft,NeelKant,AidanSwope,andOleksiiKuchaiev. HelpSteer: Multi-attributeHelpfulness
DatasetforSteerLM. arXiv,2023. doi:10.48550/arXiv.2311.09528.
[28] ShengzhiLiandNimaTajbakhsh.SciGraphQA:ALarge-ScaleSyntheticMulti-TurnQuestion-AnsweringDataset
forScientificGraphs. arXiv,2023. doi:10.48550/arXiv.2308.03349.
[29] FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. Aligninglargemulti-modal
modelwithrobustinstructiontuning. ArXivpreprint,abs/2306.14565,2023. URLhttps://arxiv.org/abs/
2306.14565.
[30] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,
JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapablemultimodalmodels.
ArXivpreprint,abs/2312.11805,2023. URLhttps://arxiv.org/abs/2312.11805.
[31] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Largelanguagemodels
arezero-shotreasoners. Advancesinneuralinformationprocessingsystems,35:22199–22213,2022.
[32] OpenCompass,2023. URLhttps://opencompass.org.cn/leaderboard-multimodal. [Online;accessed
24.Jan.2024].
[33] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,
AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,CarolChen,CatherineOlsson,ChristopherOlah,Danny
Hernandez,DawnDrain,DeepGanguli,DustinLi,EliTran-Johnson,EthanPerez,JamieKerr,JaredMueller,
JeffreyLadish,JoshuaLandau,KamalNdousse,KamileLukosuite,LianeLovitt,MichaelSellitto,NelsonElhage,
NicholasSchiefer,NoemiMercado,NovaDasSarma,RobertLasenby,RobinLarson,SamRinger,ScottJohnston,
ShaunaKravec,SheerElShowk,StanislavFort,TameraLanham,TimothyTelleen-Lawton,TomConerly,Tom
Henighan,TristanHume,SamuelR.Bowman,ZacHatfield-Dodds,BenMann,DarioAmodei,NicholasJoseph,
SamMcCandlish,TomBrown,andJaredKaplan. Constitutionalai: Harmlessnessfromaifeedback,2022.
[34] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,andLijuan
Wang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities,2023.
[35] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobjecthallucination
inlargevision-languagemodels,2023.
10Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
[36] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,
ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modalmodelanall-aroundplayer?,
2023.
[37] KankanZhou,EasonLai,WeiBinAuYeong,KyriakosMouratidis,andJingJiang. Rome: Evaluatingpre-trained
vision-languagemodelsonreasoningbeyondvisualcommonsense. ArXivpreprint,abs/2310.19301,2023. URL
https://arxiv.org/abs/2310.19301.
[38] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
LiliYu,SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,andOmerLevy. LIMA:LessIsMorefor
Alignment. arXiv,2023. doi:10.48550/arXiv.2305.11206.
[39] YuvalKirstain,PatrickLewis,SebastianRiedel,andOmerLevy. Afewmoreexamplesmaybeworthbillions
of parameters. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1017–
1029, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https:
//aclanthology.org/2022.findings-emnlp.72.
[40] FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. MitigatingHallucinationin
LargeMulti-ModalModelsviaRobustInstructionTuning. arXiv,2023. doi:10.48550/arXiv.2306.14565.
A Appendix
A.1 dataandmodeldetails
Table4: DPOpreferencedatamixture
DataType DataName Size
VQA LRV-Insturct[40] 2562
SciGraphQA[28] 2522
Total 5084
11Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
Model DataName Size
Modelarchitecture Pretrainedmodelcheckpoint llava-v1.5-13b
LanguageBackboneArchitecture LLaMA-13B(finetunedtoVicunaandLLaVa)
VisionTower openai/clip-vit-large-patch14-336
Multi-modalAdapter 2-layerMulti-layerperception
LoraSetting LoraRank 128
LoraAlphra 256
DPOSetting UseAverageLogProbablity False
UseFine-grainedDPO False
Beta 0.1
LearningRate 5e-5
GradientAcculumulationSteps 4
Perdevicetrainbatchsize 8
ModelMaxlength 300
SFTSetting LearningRate 4e-4
Perdevicetrainbatchsize 16
ModelMaxLength 2048
Commonsettings Harwdware 4XA10080G
DistributedLearning Zero-3
UseBF-16 True
LearningRateScheduler Cosine
LearningRateWarmup 0.003
WeightDecay False
Table5: Modelparameters
12Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
Gemini-Annotationexample
Question Whataspectsoftheroommighthave Whatdoyouseehappeninginthisimage?
artisticorcreativesignificance?
Response1 Inthisimage,theroomhassomeartis- Theimagefeaturesapersonrunningalongabeach
ticorcreativesignificanceduetothe nearthewater’sedge. Behindtheperson, multi-
presenceofseveralsculpturesplaced plebirdsareflyinginthesky,creatingadynamic
onalongwoodentable.Thesculptures andlivelyscene. Themaniswearingaswimsuit
appeartobemadefromvariousmate- andappearstobeenjoyinghistimeatthebeach.
rials, suchasstoneandwood. Addi- Thebirds’positionsvaryacrossthesky,withsome
tionally,theroomfeaturesachairand flyingclosertothepersonandothersfurtheraway,
a light fixture that resembles a boot. spreadingoutacrossthebackground.
Theseelements,combinedwiththeas-
sortmentofsculptures,givetheroom
aneclecticandartisticatmosphere,sug-
gesting that the space might be used
fordisplayingart,inspiration,orasa
workspaceforanartistordesigner.The
presenceoftheboot-shapedlightalso
addsauniqueandimaginativetouchto
theroom’soverallaesthetic.
Gemini-RatingforReponse1 Helpfulness:4, Correctness 4, Coher- Helpfulness:3,Correctness4,Coherence:4,Com-
ence:4,Complexity:3,Verbosity:3. plexity:2,Verbose:2
Response2 In the room, a collection of wooden Theimagecapturesabeachscenewhereamanis
shoes on a long table next to a chair runningalongtheshorenearthewater,withwaves
canbeconsideredashavingartisticor crashingbehindhim.Themanisinmotion,asheis
creative significance. The display of probablyenjoyinganinvigoratingrunonthebeach.
shoesmayrepresentauniqueformof Intheskyabove,severalbirdsflyinaline,addinga
artisticexpressionoradesignconcept. senseofmovementandlivelinesstothescene.The
Additionally,thewhite-walledsetting birds’positionsvary,withsomebeingclosertothe
withadimlightallowsforaminimalist waterandothershigherupinthesky.Theoverall
andvisuallystrikingatmospherethat atmosphereoftheimagesuggestsapleasant,active
enhancestheshoes’displayandcreates dayatthebeach.
asenseofdepthandartisticinterestin
thespace.
Gemini-RatingforReponse2 Helpfulness:3, Correctness 3, Coher- Helpfulness:4,Correctness4,Coherence:4,Com-
ence:3,ComTpalbelxeit6y::2E,xVaemrbpolseitsy:o3f.Annpoletaxtiityo:n3,Verbosity:3.
13Multi-modalpreferencealignmentefficientlyremediesregressionofvisualinstructiontuningonlanguagemodel
Gemini-Annotationexample
Gemini-Reasoning Bothresponsesarerelevanttotheques- IthinkthatResponse2isthebestresponsebecause
tionandprovidegooddescriptionsof itisthemostdetailed,engaging,andaccurate. It
theroom.However,thefirstresponseis providesthemostinformationabouttheimageand
moredetailedandprovidesmoreanal- usesthemostvividlanguage. Response1isalso
ysisofthepotentialartisticorcreative good,butitisnotasdetailedorengagingasRe-
significanceoftheroom.Iwillgivethe sponse2.
firstresponseaslightlyhigherrating
forHelpfulness,Correctness,andCom-
plexity.
LLaVA-RLHFratedpreference Reponse2 Response2
Ourpreference Response1 Table7: * Response2
Table6: ExamplesofAnnotation(continued)
14