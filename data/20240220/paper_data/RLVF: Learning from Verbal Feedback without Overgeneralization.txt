RLVF: Learning from Verbal Feedback without Overgeneralization
MoritzStephan1 AlexanderKhazatsky1 EricMitchell1 AnnieSChen1 SherylHsu1 ArchitSharma1
ChelseaFinn1
Abstract the ability to align them with high-level human feedback
foraspecificuseroruse-casebecomesincreasinglyimpor-
Thediversityofcontextsinwhichlargelanguage
tant. WhileLLMusersoftenwantthemodeltoadhereto
models(LLMs)aredeployedrequirestheability
broadprinciplesatalltimes,suchasproducingfluenttext,
tomodifyorcustomizedefaultmodelbehaviors
individualusersanduse-caseshavemorenuancedprefer-
toincorporatenuancedrequirementsandprefer-
ences. Forexample,ausermayrequesttheLLMtowrite
ences. A convenient interface to specify such moreconciseworkemailsbutmoredetailedpersonalemails,
modeladjustmentsishigh-levelverbalfeedback,
makingfeedbackcontextdependent. Tailoringmodelsto
suchas“Don’tuseemojiswhendraftingemails
accommodatesuchpreferencesischallenging: itrequires
tomyboss.”However,whilewritinghigh-level extensive resources to gather preferences in all different
feedback is far simpler than collecting annota-
contextsandfine-tuningthemodelinonecontextcanunpre-
tionsforreinforcementlearningfromhumanfeed-
dictablyimpactmodelbehaviorinothercontexts. Westudy
back (RLHF), we find that simply prompting a
theproblemofadaptingmodelsusingverbalfeedbackthat
modelwithsuchfeedbackleadstoovergeneral-
isfastandeasyforpeopletoprovide(seeFig.1).
ization of the feedback to contexts where it is
not relevant. We study the problem of incorpo- Commonapproachestoincorporatingfeedback,suchassu-
ratingverbalfeedbackwithoutsuchovergeneral- pervisedcontextdistillation(SCD)orreinforcementlearn-
ization, inspiringanewmethodContextualized ingfromhumanfeedback(RLHF),useexample-levelsu-
CritiqueswithConstrainedPreferenceOptimiza- pervisionviaeithersupervisedcompletionsorpreference
tion (C3PO). C3PO uses a piece of high-level labels. Such methods require a corpus of user-provided
feedbacktogenerateasmallsyntheticpreference (preference-)data,whichcanbecostlyandcumbersometo
datasetspecifyinghowthefeedbackshould(and obtain. Additionally,theydonotconstrainmodelbehavior
should not) be applied. It then fine-tunes the outsidethecontextthatthefeedbackmayapply,sotheLLM
modelinaccordancewiththesyntheticpreference might adapt its behavior in unintended ways, e.g. output
data while minimizing the divergence from the a more verbose work email when the preference only ap-
original model for prompts where the feedback pliestopersonalemails. Verbalfeedbackisfareasierand
doesnotapply. Ourexperimentalresultsindicate fasterforhumanstoprovide. Tothisend,anothercommon
thatourapproacheffectivelyappliesverbalfeed- approach is to incorporate such verbal feedback into the
backtorelevantscenarioswhilepreservingexist- prompt,potentiallythroughaniterativeprocesstocontin-
ingbehaviorsforothercontexts. Forbothhuman- uallyaddadditionalpointsoffeedback. However,thisap-
andGPT-4-generatedhigh-levelfeedback,C3PO proachrequiresre-usingthepromptinallfuturequeries. As
effectively adheres to the given feedback com- morepiecesoffeedbackaccumulate,longpromptscontain-
parably to in-context baselines while reducing ingmanycontext-dependentfeedbackscanmakeinference
overgeneralizationby30%. expensive; further, identifying which pieces of feedback
shouldapplyinagivencontextcanbecomedifficult.
We aim to adapt LLMs such that, when provided with a
1 Introduction singlesentencethatspecifiesfeedback,themodelcandis-
cern which situations the feedback applies to and appro-
Withtheincreasinglywidespreadadoptionoflargelanguage
priately incorporate the feedback in future outputs. We
models(LLMs)acrossdiverseindustriesandindividuals,
propose Contextualized Critiques with Constrained Pref-
1DepartmentofComputerScience,StanfordUniversity,CA,USA. erenceOptimization(C3PO),wherewefirstsynthetically
Correspondence to: Moritz Stephan <moritzst@stanford.edu>. generatehypotheticalpromptsin-scopeandout-of-scopefor
Project web page: https://austrian-code-wizard. thefeedback. Wethensampleoriginalcompletionstothese
github.io/c3po-website/
prompts,withoutthefeedbackapplied,aswellasrevised
1
4202
beF
61
]GL.sc[
1v39801.2042:viXraRLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Figure1:Weconsidertheproblemofleveraginghigh-level,verbalfeedback(left)torefinemodelbehaviors(center).Priorapproaches
oftenstruggletoappropriatelyupdatethemodel,leadingtoeitherfailuretoadheretothefeedbackorovergeneralization(right).
completionsinlinewiththefeedback. Importantly,weuti- puts, preferences or rankings over pairs or sets of model
lize the strong priors of existing instruction-tuned LLMs samples(Ziegleretal.,2020;Ouyangetal.,2022;Rafailov
inthisprocessandthereforedonotrequireanyadditional etal.,2023),andnaturallanguagefeedbackonmodelout-
humansupervision. Wethenintroduceanewobjectiveto putsorbehaviors(Lietal.,2017). Naturallanguagefeed-
fine-tunetheLLM’sresponsebehavior. Onenaiveapproach backorcorrectionsonindividualmodeloutputshavebeen
mightuseoriginalandrevisedcompletionsforpromptsto used to improve performance in code generation (Austin
maximize the implicit reward of a preference model (e.g. et al., 2021; Chen et al., 2023), dialogue (Li et al., 2017;
usingdirectpreferenceoptimization(Rafailovetal.,2023)). Hancocketal.,2019;Shietal.,2022),andsummarization
However,thisobjectivedoesnotcapturetheneedtoleave (Scheureretal.,2023). Feedbackorcritiquesaretypically
model behavior unchanged for non-relevant prompts. In- usedtorefinemodeloutputsduringgeneration,iteratingon
stead,C3POjointlymaximizestheimplicitrewardforin- orrefiningthemodel’sinitialresponsebeforeoutputtinga
scopepromptsandminimizesstandardcross-entropyloss finalanswer. Recentworkhasemphasizedself-refinement,
betweenthelogitsofthebaseandfine-tunedmodelforout- whereanLLMgeneratesitsownfeedback(Madaanetal.,
of-scopeprompts. Includingthelatterlossintheobjective 2023;Huangetal.,2023;Panetal.,2023). Somestudies
adjuststheLLM’sresponsestopromptswherethefeedback haveshownthatthefinaloutputsfromsuch(self-)refinement
isrelevant,whilepreservingitsbehaviorincontextswhere methods can be distilled back into the model, improving
thefeedbackshouldnotbeapplied. itsbaseperformancewithoutrequiringiterativerefinement
during sampling at test time (Sun et al., 2023; Lu et al.,
Our main contribution is C3PO, a new method for learn-
2023;Yuetal.,2023;Yuanetal.,2024;Yangetal.,2024).
ingfromverbalfeedbackthatselectivelyadaptstheLLM’s
behaviorbasedonthecontextofthefeedback. Thisnovel Most relevant to the present work are studies leveraging
syntheticdatagenerationschemeandfine-tuningobjective naturallanguagefeedbacktorefinegeneralmodelbehav-
enablesanLLMtoextrapolatesingle-sentencefeedbackto iors,ratherthaniterativelyimprovingasinglemodeloutput.
newsituations. Acrossnumerousexamplesoffeedbackgen- Constitutional AI (Bai et al., 2022) uses an LLM to gen-
eratedbyhumansandGPT-4,wefindthatC3POaccurately erate synthetic training data that encourages an LLM to
appliesthefeedbacktorelevantpromptsandimportantly, follow high-level rules written by a human; Glaese et al.
substantiallyreducesunintendedbehaviorchangesinsce- (2022)usesasimilarapproachtoinstillvariousrulesintoa
narioswherethefeedbackisnotapplicable,outperforming pre-trainedLLM.Contextdistillation(Askelletal.,2021;
priormethodsbyover10%whenbothcriteriaareconsid- Snell et al., 2022) is another approach to controllability
ered. Byprovidingadaptabilitytoverbalfeedbackwhile thatdistillsthebehavioroftheLLMwhenconditionedon
reducingovergeneralizationofsuchfeedback,ourworkmay a piece of feedback back into the LLM without the feed-
helpenhancetheutilityofLLMsindiverseuse-cases. backpresent,essentially‘bakingin’thefeedback. However,
theseapproachestocontrollabilityhavemostlybeenused
toinstilluniversalbehavioralchanges(i.e.,rulesthatshould
2 RelatedWork
alwaysbeadheredto). Relativelyfewerworkshavestud-
Improvinglanguageordialoguesystemsfromfeedbackhas iedconditionalorcontext-dependentrulefollowing(though
been studied in the context of various types of feedback, Clark et al. (2021) study adherence to synthetic rules for
includinglearnedor(Walker,2000;Böhmetal.,2019)or logicalreasoningandcommonsense). Inconcurrentwork,
heuristic(Lietal.,2016)rewardsonindividualmodelout- Castricatoetal.(2024)alsoutilizemodelcompletionsand
2RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
revisions to generate synthetic preference pairs; they use
C3PO (Ours) In-Context SCD DPO
this technique to train a language model to better follow
1.0 Out-of-scope 1.0 Near-scope
instructionsthatspecificallyrequestavoidingagiventopic.
0.8 0.8
Arelatedproblemismodelediting(Sinitsinetal.,2020;Cao
etal.,2021;Mitchelletal.,2021;Mengetal.,2022),which 0.6 0.6
studiesinterventionstopre-trainedmodelsthatshouldonly
0.4 0.4
applyinarelativelysmallneighborhoodaroundthe‘model
edit’ (desired intervention). Most work in model editing 0.2 0.2
studiescorrectionstofactualorreasoningerrors. However,
0.0 0.0
Mitchelletal.(2022)studyeditsthatadjustthesentimentof
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
adialoguemodelforasingletopic,andMurtyetal.(2022) In-scope feedback adherence
showedits(or‘patches’)tosentimentclassificationorrela-
Figure2:C3POmitigatestheovergeneralizationproblemwhen
tionextractionmodels. Maoetal.(2023)extendthiswork
learningfromhigh-levelfeedback. Forexistingapproachesfor
byeditingmodelbehaviorforasingletopicaccordingto incorporatinghigh-levelfeedback,highfeedbackadherenceonin-
threecategoriesofpersonalitytraits. Akyüreketal.(2023) scopeprompts(xaxis)stronglypredictsalargechangeinbehavior
andHewittetal.(2024)studymodeleditsaimedatdebias- forout-of-scopeprompts(yaxis),whichisundesirable.Incontrast,
ourapproachC3POdecreasestherateatwhichout-of-scopebe-
ingLLMs;Hewittetal.(2024)alsostudyfactualeditsand
haviorisaffectedasin-scopefeedbackadherenceimproves.Lines
corrections to syntactic errors. In contrast, our work per-
ofbestfitarecomputedwithlinearorthogonalregression.
formsgeneralbehavioraleditstopre-trainedLLMs,rather
thaneditsofaspecifictypeorforaspecificcontext.
fromalanguagemodelπ fine-tunedwithSCDorasimilar
0
objective (Ziegler et al., 2020; Bai et al., 2022; Ouyang
3 Preliminaries
etal.,2022). Theinputxmaybeaninstruction,document
We first outline two approaches for updating LLMs with tosummarize,ordialoguehistory,forexample. Givenan
high-levelverbalfeedback: supervisedcontextdistillation inputx,responsesy,y′,anannotator(eitherahumanoran
andpreference-basedreinforcementlearning(PbRL). LLM)labelswhichresponseisbetter,ultimatelyproducing
adatasetD ={x ,y+,y−},wherey+ispreferredtoy−
Supervised context distillation. A simple but effective pref i i i i i
forqueryx ,asjudgedbytheannotator.
methodtoupdatinglanguagemodelsfromfeedbackissu- i
pervised context distillation (SCD; Askell et al. (2021)), ThedatasetD isusedtolearnaparameterizedreward
pref
whichincorporatesatextualcontextzcontainingageneral modelr :X ×Y →Rthatassignsscalargoodnessscores
ϕ
principle (e.g., “Always be nice!”) or information (e.g., toindividualinput-responsepairs. Themostcommonob-
“Assume the US president is Joe Biden.”) into a model’s jectivefortrainingarewardmodelismaximumlikelihood
behavior. SCD ‘distills’ the behavior that a human or an intheBradley-Terrychoicemodel(Bradley&Terry,1952):
LLMπ wouldproducewhenconditionedonbothauser
0 L (ϕ)=−E logp (y+ ≻y− |x,y+,y−) (2)
queryxandthecontextzintotheLLMwithoutthecontext BT x,y+,y− rϕ
present. Thatis,fromadatasetofunlabeleduserqueriesor =−E logσ(cid:0) r (x,y+)−r (x,y−)(cid:1) (3)
x,y+,y− ϕ ϕ
promptsD ={x },adistillationtargety iseitherwritten
u i i
by a human or generated by the LLM π for each x as
0 i Early methods for fine-tuning LLMs from human prefer-
y ∼π (·|x,z). Thelanguagemodelπ isproducedfrom
i 0 θ ences followed the reward modeling stage with a policy
supervisedfine-tuningwiththenegativeloglikelihoodloss,
optimizationstageaimedatfindingalanguagemodelpolicy
usingthesyntheticsupervisiontargets:
π thatproduceshigh-rewardresponseswithoutdeviating
θ
L (D)=−E logπ (y |x), (1) excessivelyfromtheLLMthatgeneratedtheresponsesin
SFT x,y∼D θ
D (Schulmanetal.,2017;Ziegleretal.,2020). Morere-
pref
whereπ istypicallyinitializedasπ . cently,directpreferenceoptimization(DPO;Rafailovetal.
θ 0
(2023))showsthattheoptimalpolicycanbeextractedfrom
Preference-based reinforcement learning. Preference-
the learned reward in closed form, avoiding the need for
based reinforcement learning (PbRL; (Busa-Fekete et al.,
iterative policy search. Due to its simplicity and compu-
2014;Sahaetal.,2023))isthemostwidely-usedapproach
tationalefficiency,weusetheDPOalgorithmforlearning
toupdatinglanguagemodelsfromfeedbackbyleveraging
frompreferencedatainthiswork. DPOdirectlyoptimizes
preferencesoverpairs1 ofLLM-generatedresponsesy,y′
thelanguagemodelpolicyfrompreferencesusingtheloss:
to an input x. The responses y,y′ are typically sampled
1Rankingsoverlargersetsofresponsescanalsobeused,but L DPO(D)=−E x,y+,y−∼Dlogσ(cid:16) logπ πθ 0(( yy ++ || xx )) −logπ πθ 0(( yy −− || xx ))(cid:17) .
weusepairsforsimplicity. (4)
3
ecnerehda
kcabdeef
epocs-fo-tuORLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Feedback For specific Python coding questions, re-
spondwithonlyacodesnippetandnoex-
planationsbeforeorafterthesnippet.
Categories Datastructuresinvariouslanguages;Statis-
ticalcomputingindifferentenvironments
In-scope WriteabasicQueueclassinPython.
prompts HowcanIimplementbackpropinPython?
Out-of-scope WhendidtheHaitianrevolutionbegin?
prompts Canyouexplainrelativityinaparagraph?
Near-scope WhatgoodC++librariesaretherefortrees?
prompts IsPythonorJuliamorepopularforNLP?
Figure3:C3PODataGenerationScheme.Givenhumanfeed-
back,C3PObeginsbygeneratingasetofcategoriesofprompts
Table1:Anexamplefeedback,promptcategories,andin-scope,
wherethefeedbackmayberelevantusingGPT-4.GPT-4thengen-
out-of-scope,andnear-scopepromptsforeachcategory.
eratesin-scopepromptsxin-scopeandnear-scopepromptsxnear-scope.
i i
Asetofout-of-scopepromptsxout-of-scopeisalsotakenfromaprior
i
dataset.Thecurrentmodelthengeneratesabaselineresponseto
eachofthese,givingy−,ynear-scope,yout-of-scope,respectively. We
OurapproachleveragesPbRLtoupdatealanguagemodel i i i
alsopromptthecurrentmodeltorevisey−toincorporatethefeed-
fromhigh-levelverbalfeedbackanddoesnotassumethata i
back,givingarevisedresponsey+.Thisdatagenerationscheme
preferencedatasetisdirectlyavailableattheoutset. i
isthefirststageofC3PO–autonomouslygeneratingfine-tuning
datasetsD ,D andD ,thelattertwoofwhich
in-scope near-scope out-of-scope
4 ReinforcementLearningfromVerbal areusedtopreventovergeneralizationonirrelevanttasks.
FeedbackusingContextualizedCritiques
withConstrainedPreferenceOptimization
thedistributionofmodelinputswherethemodel’sbehavior
Ourgoalistoenableadaptationtohigh-levelverbalfeed-
should change. Given a piece of feedback z, C3PO uses
backwithoutextensivehumanannotation. Theverbalfeed-
GPT-4 to first generate a set of K categories of prompts
backzcorrespondstoshortpassagesofnaturallanguagetext
wherethefeedbackcouldapply. GPT-4thengeneratesM
thatdescribetheuser’sfeedbackonthemodel’scurrentbe- promptsxin-scope(M foreachcategory)wherethefeedback
havior. Unfortunately,naïvelyapplyingexistingapproaches i K
applies. However,beyondaccommodatingfeedbackforin-
tothisproblemleadsthemodeltoovergeneralize,applying
scopeprompts,wemustalsoavoidovergeneralizationofthe
the feedback both when it should be applied and when it
feedbacktopromptswhereitdoesnotapply. Wetherefore
shouldnot. Ouraimistodevelopamethodcapableofonly generate a set of M prompts xnear-scope in each category
applying feedback where it is appropriate. Starting with i
thataresuperficiallyrelatedtothefeedbackinsomeway
a base language model π , our approach, Contextualized
0 (lexically,semantically),butarenotactuallyinputswhere
CritiqueswithConstrainedPreferenceOptimization,usesa
themodel’sbehaviorshouldchange. Finally,weuseafixed
stronggeneral-purposemodel(suchasGPT-4)totranslate setofM feedback-independentpromptsxout-of-scopetoavoid
a piece of verbal feedback z into a dataset which can be i
degradationofcompletelyunrelatedmodelbehaviors.2 See
finetunedontoaddressfeedbackadherencewithoutover-
Table1foranexamplefeedback,categories,andprompts
generalization. Thisdatasetwillbeanaggregationofthree
andFigure3forasummaryonthedatagenerationscheme.
sub-datasets, which all serve a unique purpose. The first
componentofthisdataset,D ,existstodemonstrate To capture the desired delta in behavior denoted by the
in-scope
thedesiredchangeofbehavior. Next,wehaveD , feedback,wegeneratethedatasetofpreferencepairsusing
out-of-scope
whichallowsustomaintainourbehavioroutsidethescope thein-scopepromptsD = {xin-scope,y+,y−}. y− is
in-scope i i i i
of the feedback. Lastly, we have D , which is ad- generated by the language model that originally received
near-scope
versariallydesignedtorefineourmodel’sunderstandingof thefeedback,i.e.,wehavey− ∼ π (· | xin-scope). Togen-
i 0 i
whereitisappropriatetoapplythefeedback. Toupdatethe erate y+, the language model is then prompted to revise
i
model,wejointlytrainonD withPbRLandtheuni- y− toincorporatethefeedback,i.e.,wehavey+ ∼ π (· |
in-scope i i 0
sonofD out-of-scopeandD near-scopewithsimpleSFT.Wenow xi in-scope,y i−,z). SeeAppendixDforthecompleteprompt
describeourdatasetgenerationprocedureinmoredetail. format.Thustotheextentthatthemodelπ cancorrectlyin-
0
terpretthegivenfeedback,thegeneratedpreferencedatarep-
Translatinghigh-levelverbalfeedbackintoafine-tuning
resentsthedesired‘delta’inbehaviordescribedbythefeed-
dataset. PbRL algorithms like DPO optimize a model’s
behavior only for a given distribution of inputs; thus, to 2WesamplethesepromptsrandomlyfromtheOpenInstruction
incorporateapieceoffeedbackz,wemustfirstdetermine Generalistdataset(LAION,2023).
4RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Figure4: C3POFine-TuningObjective. C3POfacilitatesfeedbackadherenceforrelevantpromptsbyfine-tuningwithDPOonthe
generatedin-scopedatawhileminimizingovergeneralizationthroughSFTlossesonthegeneratedout-of-scopeandnear-scopedata,
whichregularizesmodelbehaviortowardstheoriginalmodelforfeedback-irrelevantprompts.
backz. Tocontrolformodeldegradationonout-of-scope that preference data (x,y,y′) are constructed according
prompts, we populate D = {xnear-scope,ynear-scope} to some unknown scoring function r∗ as p(y ≻ y′|x) =
near-scope i i
andD ={xout-of-scope,yout-of-scope}withtherespec- σ(r∗(x,y)−r∗(x,y′)). We show that two-policy prefer-
out-of-scope i i
tivepromptsandcorrespondingcompletionssampledfrom encedataadherestotheBTmodelwith
theinitiallanguagemodelπ 0. Thesedatasetsencodethe π+(y|x)
behaviorsthatwewanttopreserveafterincorporatingz. r∗ (x,y)=log . (6)
2p π−(y|x)
Fine-tuning using the synthetic data. Using our
Further,weshowthatwhenusingπ−asthereferencemodel
synthetically-generated datasets, we now fine-tune the
forPbRL,theoptimalpolicycorrespondsto
modelπ usingacombinedloss,asshowninFigure4,that
0
bothincorporatesfeedbackonrelevantpromptsanddiscour- (cid:18) π+(y|x) (cid:19) β1
agesmodeldegradationonfeedback-irrelevantprompts: π∗ (y|x)∝ . (7)
2p π−(y|x)1−β
Applyfeedback Notablespecialcasesofπ∗ arethegeometricmeanofπ+
(cid:122) (cid:125)(cid:124) (cid:123) 2p
L C3PO =L DPO(D in-scope) and π− for β = 2 and simply π+ for β = 1. For β <
+λ L (D )+λ L (D ). (5) 1, we interpret π∗ as returning a temperature-sharpened
1 SFT out-of-scope 2 SFT near-scope 2p
(cid:124) (cid:123)(cid:122) (cid:125) version of π+, but with a penalty on responses assigned
Controlmodeldegradation
highprobabilityunderπ− (i.e.,responsesthatrespondto
theuserbutfailtoadheretothefeedback). SeeAppendixC
InterpretingtheC3POloss. WhiletheL SFTlossessimply forvisualizationofπ∗ withvariousβ inasyntheticsetting.
regularizetheupdatedmodelπ towardtheoriginalmodel 2p
θ
π for prompts not relevant to the feedback, the result of
0
5 Experiments
learning from the C3PO synthetic preference dataset for
in-scopeinputsislessobvious. C3POgenerateswhatwe
Our experiments are intended to answer several research
refertoassynthetictwo-policypreferencedata(x,y+,y−),
questions about learning from verbal feedback. We first
wherey+isalwayspreferredtoy−.Thesepreferencetuples
investigatethequestion: towhatextentdoestheovergener-
areconstructedbysimplysamplingy− fromapolicyπ−
alizationproblemoccurforexistingmethodsforlearning
(the baseline model π ) and y+ from a different policy
0 fromverbalfeedback,anddoesC3POmitigatethiseffect?
π+ (the baseline model π prompted to revise a baseline
0 Next, westudywhethersimplemodificationstostandard
model response using the feedback).3 Unlike preference
approaches to incorporating feedback with prompting or
datasetsscoredbyablack-boxhumanorAIannotator,we
supervisedcontextdistillationeffectivelymitigateovergen-
canexpresstheoptimalpolicylearnedfromsuchpreference
eralization. Further, we study whether or not C3PO can
dataintermsofthedata-generatingpolicies.
learnmultiplepiecesoffeedbackaswellastheimpactof
We show in Appendix C that such synthetic two-policy thespecificchoiceoftheformoftheconstraintlossusedin
preference data satisfies the Bradley-Terry (BT) prefer- C3PO.Beforediscussingtheresultsoftheseexperiments,
ence model (Bradley & Terry, 1952), which assumes weelaboratethedatasets,evaluationmetrics,andbaseline
methodsusedinourexperiments.
3Someexistingwork(Yangetal.,2024;Intel,2023)showsthat
synthetictwo-policypreferencedatacanproduceusefulpolicies. Datasets. Ourfeedbackdatasetiscomposedof100pieces
5RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
In-scope Out-of-scope Near-scope
0.6
0.4
0.2
0.0
C3PO (Ours) In-Context SCD DPO
Figure6: ContextualizedCritiqueswithConstrainedPrefer-
ence Optimization substantially reduces overgeneralization
(applyingthegivenfeedbacktopromptswhereitisnotactually
relevant)withonlyminorreductioninadherencetofeedbackfor
promptswherethefeedbackisrelevant.
Figure5:SampleresponsesfromC3POandeachbaselineforan
in-scopeandout-of-scopeprompt.OnlyC3POcorrectlyadheres
toreliablymeasurewhichoftworesponsesbetteradheres
tothefeedbackforthein-scopeinputandignoresthefeedbackfor
theout-of-scopeinput. toagivenpieceoffeedback. Forapromptx,amodelout-
puty,andabaselineresponsey¯fromthebaselinemodel,
theheuristicscoringfunctionproducesafeedbackscore
of feedback, where half are written by the authors4 and h(x,y,y¯). Thisscoringfunctionintuitivelyscoreswhether
half are generated by GPT-4 using the prompt provided thefeedbackadherenceoftheadaptedmodelresponseisbet-
in Appendix D. All pieces of feedback are designed to terthan,equalto,orworsethanthebaselinemodelresponse.
apply only in some contexts; see Table 1 for examples. h(x,y,y¯)takesavalueof1ifyincorporatesthefeedback
For C3PO and the SCD + Negatives baseline, the andy¯doesnot,avalueof0ifbothresponsesadheretothe
datasetsD andD ,eachcontainingout-of- feedbackorneitherresponseadherestothefeedback,and
near-scope out-of-scope
scopepromptsandcorrespondingbaselinemodelcomple- avalueof-1ifthebaselineresponsey¯adherestothefeed-
tionsusedforregularization,aresampledaccordingtothe backandtheadaptedmodelresponseydoesnot.Incontrast,
C3POprocedureinSection4. Foreachpieceoffeedback, most pieces of feedback, such as requesting the usage of
|D |=|D |=960. Wesampletheprompts moremetaphorsorlessaggressivespeech,requirequalita-
near-scope out-of-scope
forD fromtheOpenInstructionGeneralist(OIG) tiveevaluation.Inthesecases,wemeasurerelativefeedback
out-of-scope
Dataset (LAION, 2023) which contains a mix of diverse adherence using GPT-4. For a prompt x, a model output
promptsrangingfrommathtoQAandchat. Withineachof y,andabaselineresponsey¯fromthebaselinemodel,we
the32categoriesofprompts,werandomlyselect80%tobe promptGPT-4tooutputapreferencescorewhencomparing
usedfortrainingandvalidationandtheremainderareused tworesponsesusingthepromptinAppendixD,producing
fortesting. afeedbackscoreg(x,y,y¯)scaledtobeintherange[−1,1].
Thisscoremeasurestheextenttowhichagivenresponse
Evaluation metrics. Our evaluations are constructed to
adherestothefeedbackbetterthanthebaselineresponse;a
comparethefeedbackadherenceofthebaselinemodelwith
scoreof1denotesthatthetrainedmodelresponseadheres
the model after learning from the feedback; we evaluate
tothefeedbackmuchbetterthanthebaselinemodeland−1
thischangeinbehaviorforbothin-scopeandout-of-scope
denotesthereverse. Weusethesemetrics(heuristicscoring
prompts. Forin-scopeprompts,ourgoalistoincreasefeed-
or GPT-4 scoring) to measure the feedback adherence of
backadherence,whileforout-of-scopeprompts,ourgoal
the trainedmodel responses comparedto the responseof
istopreservetherateoffeedbackadherenceoftheoriginal
thebasemodelforin-scopeprompts. Thein-scopeadher-
model(thatis, leavethebaselinemodelunchanged). We
encescoreforanalgorithmonagivenpieceoffeedback
measurefeedbackadherenceintwoways,heuristicallyand
istheaverageoftheper-promptfeedbackscores(h(x,y,y¯)
withGPT-4. For14ofthehuman-generatedpiecesoffeed-
ifthepromptxisheuristicallycheckable,g(x,y,y¯)other-
backsuchasmodifyingresponselengthortheinclusionof
wise)acrossallin-scopetestpromptsforthatfeedback. The
certainwords,manuallycraftedheuristicrulesaresufficient
overallin-scopeadherencescoreS istheaverageofthese
in
4One piece of feedback was inspired by a suggestion from per-feedbackadherencescoresoverallfeedbacksevaluated.
socialmedia.
6
deilppA
si
kcabdeeF
stpmorP
%RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Method S S S
in out overall
In-Context 0.677±.028 0.503±.025 0.587±.026
In-Context+CoT 0.402±.033 0.246±.017 0.578±.026
SCD 0.470±.029 0.269±.019 0.6005±.025
SCD+Negatives 0.367±.027 0.133±.013 0.617±.021
DPO 0.326±.048 0.517±.022 0.4045±.037
C3PO(Ours) 0.563±.031 0.150±.014 0.7065±.024
Table 2: C3PO provides the strongest overall performance
averagedover100piecesoffeedback. Augmentationstothe
In-Context and SCD baselines, In-Context + CoT and SCD +
Negatives,respectively,reducetheamountofovergeneralization
ofbothapproaches,butatasubstantialcosttoin-scopefeedback
adherence,andultimatelytheseimprovementsdonotchangethe
overallscoreforeithermethod. Figure7:MixingtheLoRAweightsthatC3POlearnsfortwo
differentpiecesoffeedback(right)providesvirtuallythesame
levelofin-scopefeedbackadherenceandout-of-scopebehavior
changeasapplyingandevaluatingeachfeedbackindependently
Forout-of-scopeprompts,ourgoalistomeasurechanges
(left).ThisresultsuggeststhatlearningseparateLoRAparameters
inthemodel’sbehaviorasaresultofincorporatingthefeed- foreachfeedbackasitisreceivedandsimplyaddingthemtogether
back. Becausethemodel’sbehaviorismostlikelytochange toacquirethemodeladaptedtoallfeedbacksmaybeviable.
along the dimension of the feedback, we measure model
behaviorchangeonout-of-scopeastheaveragechangein
the rate we instead measure the absolute change in feed- supervisiontargets. Inaddition,inordertobettercontrol
back adherence since over this domain, our objective is overgeneralization,weevaluateSCD+Negatives,which
to leave the model behavior unchanged. To compute the adds a weighted constraint to the SCD loss over out-of-
out-of-scope behavior change score for an algorithm on scopeprompts,usingthebaselinemodelresponsesonthese
agivenpieceoffeedback, weaveragetheabsolutevalue promptsasthesupervisiontarget. Finally,weevaluateDPO
ofthefeedbackscores(again, h(x,y,y¯)ifxisaheuristi- onthepreferencedatasetD withoutadditionalregu-
in-scope
callycheckableprompt,g(x,y,y¯)otherwise). Thatis,the larization,essentiallyanablationofC3PO’sregularization
averagechangeinfeedbackadherenceoverthesetofout-of- losses. Forallexperiments,weuseMistral-7B-Instruct-v0.2
scopetestprompts. TheoverallbehaviorchangescoreS (Jiang et al., 2023) and train with Low-Rank Adaptation
out
istheaverageoftheseper-feedbackbehaviorchangescores (Huetal.,2021)witharankof64andalphaof128. Weuse
overallfeedbacksevaluated. Apartfromtheseindividual alearningrateof5e-5withacosinedecayscheduleanda
metrics, we define S = Sin+(1−Sout) as a combined warmupratioof0.05andtrainfor1epoch.
overall 2
metricwith equalweightingofthe in-scopefeedbackad-
herencescoreS andoneminustheout-of-scopebehavior 5.1 Quantifyingandmitigatingovergeneralization
in
changescoreS . AS nearoneindicatesthatanalgo-
out overall Our initial results in Figure 2 show that for existing ap-
rithmeffectivelyadherestothegivenfeedbackbetterthan
proaches to learning from verbal feedback, successfully
thebaselinemodelforin-scopepromptswhilepreserving
incorporatingthefeedback(largex-axisvalue)leadstoap-
theleveloffeedbackadherenceinthebaselinemodelon
plicationofthatfeedbackforpromptswherethefeedback
out-of-scopeprompts.
doesnotapply(largey-axisvalue). Thatis,successfulad-
Methods. We compare C3PO against both in-context- herencetofeedbackforin-scopepromptscomesatahigh
learning-basedandfine-tuningmethods. ForIn-Context cost in terms of incorrect adherence to that feedback for
learning,weprovidethebaselinemodelwiththeuserquery out-of-scopeprompts,shownbythelargeslopeofthebest
aswellasthefeedbackandapromptinstructingthemodel fit lines. This result also shows that C3PO trades off in-
toselectivelyapplythefeedbackwheneveritisapplicable scope adherence and out-of-scope behavior preservation
to the given user query. We explore an enhancement of muchmoreefficiently,shownbythemuchsmallerslopeof
this naive promptingapproach, In-Context +CoT using thebest-fitline.Inthissection,westudytheimpactofincor-
chain-of-thoughtprompting; thisapproachisthesameas poratingverbalfeedbackintermsoftheevaluationmetrics
In-Context,exceptwepromptthemodeltofirstreasonstep- S andS ,measuringout-of-scopebehaviorchangefor
in out
by-stepabouttheapplicabilityoftheprovidedfeedbackto bothgenericout-of-scopepromptsandmoredifficultnear-
the given prompt before answering. See Appendix D for scopeprompts. TheresultsareshowninFigure6. C3PO
the full prompts. Next, we compare against performing dramaticallyreducesthebehaviorchangeforbothgeneral
supervisedcontextdistillationSCDonin-scopeprompts, out-of-scopepromptsandnear-scopeprompts,whileonly
usingtherevisedresponsesy+ generatedbyC3POasthe slightlyreducingfeedbackadherenceforin-scopeprompts.
7RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
It is natural to wonder whether we can improve the per-
formanceofthebaselinemethodsusingasimilargoalof
constrainingthebehaviorchangeforout-of-scopeprompts.
WethereforeevaluatethemodificationsoftheIn-Context
and SCD methods, In-Context + CoT and SCD + Nega-
tives,intendedtoreducebehaviorchange. TheIn-Context
+CoTmethodfirstperformschain-of-thoughtreasoningto
decidewhetherthefeedbackisapplicabletothegiveninput
before responding; the SCD + Negatives baseline mixes
intheregularizationlossontheD andD
near-scope out-of-scope
datasets. Wereportthein-scopefeedbackadherenceand
out-of-scope behavior change (averaged over the general
out-of-scope and near-scope prompts) in Table 2. While Figure8: ReplacingC3PO’smaximumlikelihoodconstrainton
bothimprovementsdosubstantiallyreducetheamountof out-of-scopepromptswithfullknowledgedistillationleadstosub-
stantiallyimpairedperformanceonin-scopeprompts,suggesting
overgeneralizationcomparedtotheoriginalversionofeach
thatallowingsomesubtlechangestothemodel’sconditionaldis-
method,theycomeatasubstantialcosttoin-scopefeedback
tributionforout-of-scopepromptsmaybebeneficial.
adherence. Therefore,theoverallscoreS doesnotsub-
overall
stantiallyimproveforeitheroftheseapproaches;however,
C3POoffersamoreefficienttradeoffbetweenin-scopefeed-
out-of-scopepromptsduringfine-tuning. Wehypothesize
backadherenceandout-of-scopebehaviorchange,shown
thatperformingfullknowledgedistillation(Hintonetal.,
bythesignificantlyhigherS . Wethereforeconclude
overall 2015)fortheconditionaldistributionsoftheadaptedmodel
thatC3POisaneffectivewaytoreduceovergeneralization
andbaselinemodelateachtimestep. Thatis,performing
whilegenerallymaintainingin-scopefeedbackadherence.
knowledgedistillationononlyout-of-scopeprompts,where
ThisclaimissupportedbytheresultsinFigure5.
thebaselinemodelistheteacherandtheadaptedmodelis
thestudent. Ratherthansimplymaximizingthelikelihood
5.2 Adheringtomultiplefeedbacks
ofbaselinemodelsamplesfromeachout-of-scopeprompt,
thisconstraintminimizestheKL-divergencebetweenthe
Sofar,ourexperimentshaveevaluatedtheaverageresultof
baselinemodel’sconditionaldistributionandtheadapted
applyingasinglepieceoffeedbacktoapre-trainedmodel.
model’sconditionaldistribution,averagedoveralltimesteps
Whileacomprehensiveevaluationofcontinuallearningof
in the baseline model’s completion. While this form of
manyfeedbacksisoutofthescopeofthiswork,weperform
constrainthassuccessfullyleveragedthe‘darkknowledge’
aninitialinvestigationinthisdirectiontoassesstheeffective-
representedbythelower-probabilitylogitsinthemodel’s
nessofmixingtheLoRAparameterslearnedfromseparate
outputtoconstrainneuralnetworksinthecontextofcon-
feedbacks. That is, we perform C3PO separately on two
tinual learning (Buzzega et al., 2020), we find that this
differentfeedbacksz andz ,producingLoRAparameters
1 2
strongerconstraintsubstantiallyimpairsin-scopefeedback
ϕ andϕ . Ideally,toproduceamodelthatadherestoboth
1 2
adherencecomparedtothemaximumlikelihoodconstraint.
z andz ,ratherthanre-training,wecouldsimplyusethe
1 2
mixtureofresidualparametersϕ′ =ϕ +ϕ .Inthissection, Alternativeapproachestoconstrainingout-of-scopemodel
1 2
behaviorisanimportanttopicforfuturework.
wecomparetheperformanceofthisapproachtocombining
feedback-adaptedmodels. Figure7comparestheaverage
feedbackadherenceandbehaviorchangewhenapplyingand 6 Discussion&FutureWork
evaluatingonlyasinglepieceoffeedbackatatime(left)
As large language models become more ubiquitous and
withtheaveragefeedbackadherenceandbehaviorchange
widely-adopted,theabilitytoeasilycustomizeandperson-
ontwofeedbacksafterapplyingthemergedLoRAparam-
eters ϕ′ (right), averaged across 20 feedbacks (10 pairs). alizetheirbehaviorsisincreasinglyvaluable. Wetherefore
havestudiedtheproblemoflearningfromhigh-levelverbal
Weobservevirtuallynodegradationinin-scopefeedback
feedback, where a user provides a short piece of written
adherenceandnochangeinout-of-scopebehaviorchange.
feedbackdescribingadesiredchangetoalanguagemodel’s
Thisresultispromisingforthepossibilityofenablingrapid,
behaviors,andanalgorithmmustupdatethemodeltoad-
mix-and-matchpersonalizationandcustomizationoflarge
heretothisfeedbackwhenitisappropriate, butpreserve
languagemodelswithoutre-training.
the model’s behaviors elsewhere. We showed that apply-
ingexistingmethodsforfine-tuninglanguagemodelsfrom
5.3 ChoiceofC3POconstraintformulation
feedbackdemonstratesevereovergeneralization: afterin-
C3POconstrainsthefeedback-adaptedmodelbymaximiz- corporatingthefeedback,theyalsochangemodelbehavior
ing the likelihood of the baseline model’s responses on forinputsthatarenotrelevanttotheprovidedfeedback. To
8RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
mitigate this problem, we introduced Contextualized Cri- E.,Kerr,J.,Mueller,J.,Ladish,J.,Landau,J.,Ndousse,
tiqueswithConstrainedPreferenceOptimization(C3PO), K., Lukošiu¯te˙, K., Lovitt, L., Sellitto, M., Elhage, N.,
analgorithmthatperformsreinforcementlearningfromver- Schiefer,N.,Mercado,N.,DasSarma,N.,Lasenby,R.,
bal feedback (RLVF). C3PO leverages existing language Larson,R.,Ringer,S.,Johnston,S.,Kravec,S.,Showk,
modelstogenerateasetofsmallfine-tuningdatasetsthat S. E., Fort, S., Lanham, T., Telleen-Lawton, T., Con-
encodeboththedesiredchangeinbehaviordescribedbythe erly,T.,Henighan,T.,Hume,T.,Bowman,S.,Hatfield-
feedbackandthesetofbehaviorsthatshouldbepreserved Dodds,Z.,Mann,B.,Amodei,D.,Joseph,N.,McCan-
forinputsunrelatedtothefeedback. WefoundthatC3PO dlish,S.,Brown,T.B.,andKaplan,J. Constitutionalai:
substantiallyreducesovergeneralizationwhilestilladhering Harmlessness from ai feedback. arXiv.org, 2022. doi:
tofeedbackforrelevantinputs. Ourexperimentsraisesev- 10.48550/ARXIV.2212.08073.
eralimportantquestionsforfuturework. Canweperform
continuallearningfromfeedbackbysimplycontinuallyag- Böhm, F., Gao, Y., Meyer, C. M., Shapira, O., Dagan,
gregatingandmixingadaptedmodelweights? Inaddition, I., and Gurevych, I. Better rewards yield better sum-
ourablationsoftheC3POconstraintlossfunctionsuggest maries: Learning to summarise without references. In
thattheproperlevelof‘strictness’whenconstrainingthe Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Pro-
modelupdateisanon-trivialproblem,andbetter-performing ceedings of the 2019 Conference on Empirical Meth-
alternativesmayexist. Finally,futureworkmayinvestigate ods in Natural Language Processing and the 9th In-
therelationshipbetweenthecomplexityofthefeedbackand ternationalJointConferenceonNaturalLanguagePro-
thecapabilitiesofthebasemodelbeingadapted. cessing(EMNLP-IJCNLP),pp.3110–3120,HongKong,
China,November2019.AssociationforComputational
Linguistics. doi: 10.18653/v1/D19-1307. URLhttps:
Acknowledgements
//aclanthology.org/D19-1307.
TheauthorsgratefullyacknowledgetheOpenAIResearcher
Access Program and Modal.com for providing compute Bradley, R. A. and Terry, M. E. Rank analysis of in-
credits used in this project. CF is a CIFAR Fellow. EM complete block designs: I. the method of paired com-
acknowledgesKyleHsuforhelpfulconversations. parisons. Biometrika, 39(3/4):324–345, 1952. doi:
https://doi.org/10.2307/2334029.
References
Busa-Fekete, R., Szörényi, B., Weng, P., Cheng, W., and
Hüllermeier,E. Preference-basedreinforcementlearning:
Akyürek,A.,Pan,E.,Kuwanto,G.,andWijaya,D. DUnE:
evolutionarydirectpolicysearchusingapreference-based
Dataset for unified editing. In Bouamor, H., Pino, J.,
racingalgorithm.MachineLearning,97(3):327–351,July
andBali,K.(eds.),Proceedingsofthe2023Conference
2014. doi: 10.1007/s10994-014-5458-8. URLhttps:
onEmpiricalMethodsinNaturalLanguageProcessing,
//doi.org/10.1007/s10994-014-5458-8.
pp.1847–1861,Singapore,December2023.Association
forComputationalLinguistics. doi: 10.18653/v1/2023.
Buzzega, P., Boschini, M., Porrello, A., Abati, D., and
emnlp-main.114. URL https://aclanthology.
Calderara,S. Darkexperienceforgeneralcontinuallearn-
org/2023.emnlp-main.114.
ing: astrong,simplebaseline. InProceedingsofthe34th
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., InternationalConferenceonNeuralInformationProcess-
Henighan, T., Jones, A., Joseph, N., Mann, B., Das- ingSystems,NIPS’20,RedHook,NY,USA,2020.Cur-
Sarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, ranAssociatesInc. ISBN9781713829546.
D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D.,
Cao,N.D.,Aziz,W.,andTitov,I.Editingfactualknowledge
Brown,T.,Clark,J.,McCandlish,S.,Olah,C.,andKa-
inlanguagemodels. ConferenceonEmpiricalMethods
plan,J. Agenerallanguageassistantasalaboratoryfor
inNaturalLanguageProcessing,2021. doi: 10.18653/
alignment,2021.
V1/2021.EMNLP-MAIN.522.
Austin,J.,Odena,A.,Nye,M.,Bosma,M.,Michalewski,
H.,Dohan,D.,Jiang,E.,Cai,C.,Terry,M.,Le,Q.,and Castricato,L.,Lile,N.,Anand,S.,Schoelkopf,H.,Verma,
Sutton,C.Programsynthesiswithlargelanguagemodels, S., and Biderman, S. Suppressing pink elephants with
2021. directprinciplefeedback,2024.
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Chen, A., Scheurer, J., Korbak, T., Campos, J. A., Chan,
Jones,A.,Chen,A.,Goldie,A.,Mirhoseini,A.,McKin- J. S., Bowman, S. R., Cho, K., and Perez, E. Improv-
non,C.,Chen,C.,Olsson,C.,Olah,C.,Hernandez,D., ing code generation by training with natural language
Drain,D.,Ganguli,D.,Li,D.,Tran-Johnson,E.,Perez, feedback,2023.
9RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Clark, P., Tafjord, O., and Richardson, K. Transformers Kingma, D. and Ba, J. Adam: A method for stochastic
assoftreasonersoverlanguage. InProceedingsofthe optimization. InInternationalConferenceonLearning
Twenty-NinthInternationalJointConferenceonArtificial Representations(ICLR),SanDiega,CA,USA,2015.
Intelligence,IJCAI’20,2021. ISBN9780999241165.
LAION. The open instruction generalist dataset, 2023.
Glaese, A., McAleese, N., Tre˛bacz, M., Aslanides, J., URL https://huggingface.co/datasets/
Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chad- laion/OIG.
wick, M., Thacker, P., Campbell-Gillingham, L., Ue-
sato,J.,Huang,P.-S.,Comanescu,R.,Yang,F.,See,A., Li, J., Monroe, W., Ritter, A., Jurafsky, D., Galley, M.,
Dathathri,S.,Greig,R.,Chen,C.,Fritz,D.,Elias,J.S., and Gao, J. Deep reinforcement learning for dialogue
Green, R., Mokrá, S., Fernando, N., Wu, B., Foley, R., generation. InSu, J., Duh, K., andCarreras, X.(eds.),
Young,S.,Gabriel,I.,Isaac,W.,Mellor,J.,Hassabis,D., Proceedingsofthe2016ConferenceonEmpiricalMeth-
Kavukcuoglu,K.,Hendricks,L.A.,andIrving,G. Im- ods in Natural Language Processing, pp. 1192–1202,
provingalignmentofdialogueagentsviatargetedhuman Austin,Texas,November2016.AssociationforCompu-
judgements,2022. tationalLinguistics. doi: 10.18653/v1/D16-1127. URL
https://aclanthology.org/D16-1127.
Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.
Learningfromdialogueafterdeployment: Feedyourself, Li, J., Miller, A. H., Chopra, S., Ranzato, M., and We-
chatbot! In Korhonen, A., Traum, D., and Màrquez, ston, J. Dialoguelearningwithhuman-in-the-loop. In
L. (eds.), Proceedings of the 57th Annual Meeting of InternationalConferenceonLearningRepresentations,
theAssociationforComputationalLinguistics,pp.3667– 2017. URLhttps://openreview.net/forum?
3684,Florence,Italy,July2019.AssociationforCompu- id=HJgXCV9xx.
tationalLinguistics. doi: 10.18653/v1/P19-1358. URL
https://aclanthology.org/P19-1358. Lu,J.,Zhong,W.,Huang,W.,Wang,Y.,Mi,F.,Wang,B.,
Wang,W.,Shang,L.,andLiu,Q. Self: Language-driven
Hewitt,J.,Chen,S.,Xie,L.L.,Adams,E.,Liang,P.,and self-evolutionforlargelanguagemodel. arXiv.org,2023.
Manning,C.D. Modeleditingwithcanonicalexamples, doi: 10.48550/ARXIV.2310.00533.
2024.
Madaan,A.,Tandon,N.,Gupta,P.,Hallinan,S.,Gao,L.,
Hinton, G. E., Vinyals, O., and Dean, J. Distilling the Wiegreffe,S.,Alon,U.,Dziri,N.,Prabhumoye,S.,Yang,
knowledgeinaneuralnetwork. ArXiv,abs/1503.02531, Y., Welleck, S., Majumder, B. P., Gupta, S., Yazdan-
2015. URL https://api.semanticscholar. bakhsh,A.,andClark,P. Self-refine: Iterativerefinement
org/CorpusID:7200347. withself-feedback,2023.
Hu,E.J.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang, Mao,S.,Zhang,N.,Wang,X.,Wang,M.,Yao,Y.,Jiang,Y.,
S.,Wang,L.,andChen,W. Lora: Low-rankadaptation Xie,P.,Huang,F.,andChen,H. Editingpersonalityfor
oflargelanguagemodels,2021. llms. arXiv,2023.
Huang,J.,Gu,S.,Hou,L.,Wu,Y.,Wang,X.,Yu,H.,and Meng,K.,Bau,D.,Andonian,A.,andBelinkov,Y.Locating
Han,J. Largelanguagemodelscanself-improve. arXiv, andeditingfactualknowledgeingpt. ArXiv,2022.
2023. doi: 10.18653/V1/2023.EMNLP-MAIN.67.
Mitchell, E., Lin, C. P., Bosselut, A., Finn, C., and Man-
Intel. Supervised fine-tuning and direct prefer- ning, C. D. Fast model editing at scale. International
ence optimization on intel gaudi2, November Conference on Learning Representations, 2021. doi:
2023. URL https://medium.com/intel- 10.48550/ARXIV.2110.11309.
analytics-software/the-practice-of-
supervised-finetuning-and-direct- Mitchell,E.,Lin,C.P.,Bosselut,A.,Manning,C.D.,Finn,
preference-optimization-on-habana- C.,Mitchell,E.,Lin,C.P.,Bosselut,A.,Manning,C.D.,
gaudi2-a1197d8a3cd3. [Online; posted 14- andFinn,C. Memory-basedmodeleditingatscale. In-
November-2023]. ternationalConferenceonMachineLearning,2022. doi:
10.48550/ARXIV.2206.06520.
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
Chaplot,D.S.,delasCasas,D.,Bressand,F.,Lengyel, Murty,S.,Manning,C.D.,Lundberg,S.M.,andRibeiro,
G.,Lample,G.,Saulnier,L.,Lavaud,L.R.,Lachaux,M.- M.T. Fixingmodelbugswithnaturallanguagepatches.
A.,Stock,P.,Scao,T.L.,Lavril,T.,Wang,T.,Lacroix, ConferenceonEmpiricalMethodsinNaturalLanguage
T.,andSayed,W.E. Mistral7b,2023. Processing,2022. doi: 10.48550/ARXIV.2211.03318.
10RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, Snell,C.B.,Klein,D.,andZhong,R. Learningbydistilling
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., context. arXiv.org, 2022. doi: 10.48550/ARXIV.2209.
Ray,A.,Schulman,J.,Hilton,J.,Kelton,F.,Miller,L., 15189.
Simens,M.,Askell,A.,Welinder,P.,Christiano,P.,Leike,
Sun,Z.,Shen,Y.,Zhou,Q.,Zhang,H.,Chen,Z.,Cox,D.D.,
J., and Lowe, R. Training language models to follow
Yang, Y., and Gan, C. Principle-driven self-alignment
instructionswithhumanfeedback,2022.
of language models from scratch with minimal human
Pan, L., Saxon, M. S., Xu, W., Nathani, D., Wang, X., supervision. arXiv.org, 2023. doi: 10.48550/ARXIV.
and Wang, W. Y. Automatically correcting large lan- 2305.03047.
guagemodels: Surveyingthelandscapeofdiverseself-
Walker,M.A. Anapplicationofreinforcementlearningto
correction strategies. arXiv.org, 2023. doi: 10.48550/
dialoguestrategyselectioninaspokendialoguesystem
ARXIV.2308.03188.
for email. J. Artif. Int. Res., 12(1):387–416, jun 2000.
ISSN1076-9757.
Peng, X. B., Kumar, A., Zhang, G., and Levine, S.
Advantage-weighted regression: Simple and scalable
Yang, K., Klein, D., Celikyilmaz, A., Peng, N., and
off-policy reinforcement learning. arXiv preprint
Tian, Y. RLCD: Reinforcement learning from con-
arXiv:1910.00177,2019.
trastive distillation for LM alignment. In The Twelfth
InternationalConferenceonLearningRepresentations,
Peters,J.andSchaal,S. Reinforcementlearningbyreward-
2024. URLhttps://openreview.net/forum?
weighted regression for operational space control. In
id=v3XXtxWKi6.
Proceedingsofthe24thinternationalconferenceonMa-
chinelearning,pp.745–750,2007. Yu, X., Peng, B., Galley, M., Gao, J., and Yu, Z. Teach-
inglanguagemodelstoself-improvethroughinteractive
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,
demonstrations,2023.
Ermon,S.,andFinn,C. Directpreferenceoptimization:
Yourlanguagemodelissecretlyarewardmodel.InThirty- Yuan,W.,Pang,R.Y.,Cho,K.,Sukhbaatar,S.,Xu,J.,and
seventh Conference on Neural Information Processing Weston,J. Self-rewardinglanguagemodels,2024.
Systems,2023. URLhttps://openreview.net/
forum?id=HPuSIXJaa9. Ziegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,
A.,Amodei,D.,Christiano,P.,andIrving,G.Fine-tuning
Saha,A.,Pacchiano,A.,andLee,J. Duelingrl: Reinforce- languagemodelsfromhumanpreferences,2020.
mentlearningwithtrajectorypreferences. InRuiz,F.,Dy,
J., and van de Meent, J.-W. (eds.), Proceedings of The
26thInternationalConferenceonArtificialIntelligence
and Statistics, volume 206 of Proceedings of Machine
LearningResearch,pp.6263–6289.PMLR,25–27Apr
2023.URLhttps://proceedings.mlr.press/
v206/saha23a.html.
Scheurer,J.,Campos,J.A.,Korbak,T.,Chan,J.S.,Chen,
A.,Cho,K.,andPerez,E. Traininglanguagemodelswith
languagefeedbackatscale,2023.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXivpreprintarXiv:1707.06347,2017.
Shi,W.,Dinan,E.,Shuster,K.,Weston,J.,andXu,J. When
lifegivesyoulemons,makecherryade: Convertingfeed-
backfrombadresponsesintogoodlabels,2022.
Sinitsin, A., Plokhotnyuk, V., Pyrkin, D., Popov, S.,
and Babenko, A. Editable neural networks. In In-
ternational Conference on Learning Representations,
2020. URLhttps://openreview.net/forum?
id=HJedXaEtvS.
11RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
A SamplingDetails this reward function, we then show that the optimal pol-
icylearnedbyC3POforin-scopepromptstakestheformin
WesamplefromGPT-4usingatemperatureof0.7andatop-
Equation(7). Finally,weperformasimpleempiricalvalida-
pvalueof0.7. WhensamplingfromMistral-7B-Instruct-
tionofthistheoreticalresultinasyntheticbanditproblem.
v0.2, use a temperature of 0.7, a top-p value of 0.7, and
top-kvalueof50andarepetitionpenaltyof1.
C.1 DerivingtheunderlyingBradley-Terryscoring
functionforsynthetictwo-policypreferencedata
B TrainingDetails
TheBradley-Terrymodelofdiscretechoicesstatesthatfor
Toconductourhyperparametersearch,weselect10arbi- apreferenceovertworesponsesy,y′,wehave
trary pieces of feedback from the human-generated feed-
back dataset. For all methods, we train using LoRA and p(y ≻y′|x,y,y′)=σ(r∗(x,y)−r∗(x,y′)) (8)
choosearankof64,alphaof128,andaLoRAdropoutof
0.05. Weobservethatasmallerrankresultsindecreasesin- forsometruescoringfunctionr∗. Inthecaseoftwo-policy
scopefeedbackadherencewhileincreasingtherankabove preferencedata,weassumethatthepreferredresponseis
64resultsindegradationofmodelcompletionsacrossall
generatedfromapolicyπ+andthedispreferredresponseis
methods. generatedbyapolicyπ−. Theprobabilityofobservingthe
responsepairy,y′isp(y,y′|x,A)=π+(y|x)π−(y′|x)and
We train for 1 epoch with a learning rate of 5e−5 using p(y,y′|x,¬A) = π+(y′|x)π−(y|x), whereAistheevent
a cosine learning rate schedule and a 0.05 warmup ratio. thatywasgeneratedbyπ+andy′fromπ−(andtherefore
Weablatelearningratesfrom1e−7to1e−4andfoundthat y ≻y′bythedefinitionofourdata-generatingprocess). By
below1e−5,in-scopefeedbackadherenceneverincreases
Bayes’Rule,theprobabilityofA(thatywasgeneratedby
sufficiently, even when training for multiple epochs. We, π+andy′byπ−)is
thus,pickedourlearningratetobelargerthanthisthreshold
butstillsmallenoughavoidaplateauingloss. Inaddition,
p(A|x,y,y′)=p(y ≻y′|x,y,y′)=
weexperimentedwithtrainingformultipleepochsbutfound
π+(y|x)π−(y′|x)
that this does not significantly alter the results and some-
. (9)
timesevenincreasesfeedbackadherenceforout-of-scope π+(y|x)π−(y′|x)+π+(y′|x)π−(y|x)
andnear-scopeprompts.
WenowsettheRHSofEquation(8)equaltotheRHSof
Tochooseλ 1andλ 2oftheL C3PO objective,weconducted Equation(9)andshowthataBradley-Terryscoringfunction
agridsearchoverthetwohyperparameters.Wefoundthatit exists. Wehave:
iscrucialtosetbothλ >0andλ >0andthatincreasing
1 2
λ 2,whichistheweightfortheSFTlossoverthenear-scope σ(r∗(x,y)−r∗(x,y′))=
samples,beyond0.1decreasesin-scopefeedbackadherence
π+(y|x)π−(y′|x)
drasticallywhileonlymarginallymitigatingovergeneraliza- . (10)
π+(y|x)π−(y′|x)+π+(y′|x)π−(y|x)
tion. Additionally,wefoundthatC3POisnotassensitive
over the choice of λ as long as it is non-zero and chose
1 Applyingthetransformationlog z tobothsides,wehave
0.2 due to the best in-scope to out-of-scope performance 1−z
trade-offonourevaluationset.
π+(y|x) π+(y′|x)
r∗(x,y)−r∗(x,y′)=log −log , (11)
ForbothC3POandDPO,weselectaβ parameterof0.1. π−(y|x) π−(y′|x)
Weablatewithvaluesof0.05,0.15,and0.25andfindthata
settingof0.05resultsinlessin-scopefeedbackadherence implyingtheBradley-Terryscoringfunction
andmoreovergeneralizationwhileβ > 0.1onlyreduces
π+(y¯|x)
in-scopefeedbackadherence. r∗(x,y¯)=log +C(x) (12)
π−(y¯|x)
C DerivationofOptimalPolicyforPbRLon
for some value C(x) that is constant with respect to y.
Two-PolicyPreferencePairs Therefore,synthetictwo-policypreferencedataisBradley-
Terry,withtheabovescoringfunction.
In this section, we derive several properties of the PbRL
learning procedure used by C3PO. First, we demonstrate
C.2 DerivingtheoptimalpolicyforC3POforin-scope
thatthe syntheticpreference datageneratedby C3POad-
prompts
herestothecommonly-usedBradley-Terrymodel(Bradley
& Terry, 1952) of discrete choice and compute the true The optimal policy for the KL-regularized reinforcement
reward function implied by the preference data. Using learningobjectiveoptimizedbyDPOisshowninpriorwork
12RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
(cid:16) (cid:17)
exp (i−70)2 and similarly define π− as the shifted pol-
+ * =1.0 8 1000
0.020 * =0.75 * =2.0 7 icyπ−(i) ∝ exp(cid:16) (i 1− 04 00 0)2(cid:17) . Wegenerate1e8preference
6 pairs (y+,y−), where y+ ∼ π+(·) and similarly for y−.
Wefitarewardfunctiontothispreferencedatausingthe
0.015 5
Bradley-Terryloss,using400stepsofgradientdescentwith
4
the Adam optimizer (Kingma & Ba, 2015) with learning
0.010
3 rate1.0,initializingtherewardfunctiontothezerovector.
2
0.005 TheresultsareshowninAppendixC.3. Wefindthatthe
1 recoveredrewardfunctionisexactlytheBradley-Terryscor-
Learned Reward
0.000 True Reward (log +/ ) 0 ingfunctionpredictedbyEquation(12),andthatthespecial
casespredictedbyEquation(7)hold(e.g.,withβ =1,we
0 20 40 60 80 100
Action Index (|A| = 100) havesimplyπ∗ =π+). Withβ <1,wehavetheintuitive
behaviorofamplifyingthedeltabetweenπ+andπ−.
Figure9: EmpiricaldemonstrationoftheresultofC3PO’sopti-
malpolicyonin-scopeprompts. Forβ = 1,theoptimalpolicy
recoversπ+; forsmallerbeta, theoptimalpolicyamplifiesthe
delta between π+ and π−. Note that the reward learned with
theBradley-Terrylossperfectlyrecoversthetheoreticallyoptimal
rewardinEquation(12).
(Peters&Schaal,2007;Pengetal.,2019;Rafailovetal.,
2023)tobe
(cid:18) (cid:19)
1 1
π∗(y|x)= π (y|x)exp r(x,y) . (13)
Z(x) ref β
SubstitutingtheoptimalrewardfunctioninEquation(12),
assumingC(x)=0asthisconstanttermdoesnotaffectthe
optimalpolicy,wehave
1
(cid:18)
1
π+(y|x)(cid:19)
π∗(y|x)= π (y|x)exp log (14)
Z(x) ref β π−(y|x)
1
(cid:18) π+(y|x)(cid:19) β1
= π (y|x) . (15)
Z(x) ref π−(y|x)
Assumingthatπ =π−gives
ref
1
π+(y|x)β1
π∗(y|x)= π−(y|x) (16)
Z(x) π−(y|x)β1
1 1
π+(y|x)β1
= (17)
Z(x) π−(y|x)−β
β
π−(y|x)β1
(cid:18) π+(y|x) (cid:19) β1
∝ , (18)
π−(y|x)1−β
whichistheoptimalpolicyexpressiongiveninEquation(7).
C.3 Empiricallyvalidatingthein-scopeC3POpolicy
inasyntheticsetting
We validate the theoretical results in the previous sub-
sections in a simple bandit setting with 100 possible ac-
tions. We define the preferred policy π+ as π+(i) ∝
13
ssaM
ytilibaborP
yciloP
RewardRLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
D Prompts
ThefollowingsectionscontaintheverbatimpromptsusedforthevariousstagesoftheC3POsyntheticdatageneration
procedure.
D.1 CategoryGenerationPrompt
You are a helpful assistant. You are helping a user come up with categories around a topic
that will be used to create some questions that the topic applies to and some questions
that the topic does not apply to.
Given a topic, come up with {count} creative and diverse categories that are an only
slightly larger superset of the topic. Be creative, think out of the box, and keep the
categories closely related to the topic. Ensure that for each category, it would be
possible to easily come up with questions that are not related to the provided topic. Do
not repeat categories and make sure you cover all relevant categories.
You should first respond with "THOUGHTS" that describe what you should and should not
mention in your categories and why. Then output "CATEGORIES: " on a new line and output
each category on a new line as part of a numbered list. Finally, output "
REVISED_CATEGORIES: " on a new line followed a revised version of each of the categories
you came up with. Use the revision to modify a category if it would be very hard to come
up with a prompt for that category that the topic does not apply to. The revision should
also be a numbered list. If you do a great job, you will be tipped $200.
--EXAMPLE 1--
TOPIC: current fashion trends
THOUGHTS: I should list categories that are either related to fashion but that are not
explicitly about trends. None of the categories I respond with should be directly about
fashion trends.
CATEGORIES:
1. Buying luxury fashion
2. gen-z pop culture trends
3. fast-fashion trends
4. men’s attire
5. planning an outfit
...
REVISED_CATEGORIES:
1. Buying luxury fashion
2. gen-z pop culture trends
3. fast-fashion
4. men’s attire
5. planning an outfit
...
-- END EXAMPLE 1--
--EXAMPLE 2--
TOPIC: social media direct messages
THOUGHTS: I could list categories related to social media or messaging in general. Any
category that includes but is broader than social media messaging is fine.
CATEGORIES:
1. Communicating with strangers via DM
2. Complimenting a friend
3. Sharing a post with a friend
4. Interacting with creators
5. Making plans to meet a friend
...
REVISED_CATEGORIES:
14RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
1. Communicating with strangers
2. Complimenting a friend
3. Sharing a post with a friend
4. Interacting with creators
5. Making plans to meet a friend
...
-- END EXAMPLE 2--
TOPIC: {topic}
THOUGHTS:
D.2 In-ScopeSamplingPrompt
You are a helpful assistant that always closely follows instructions. You are provided
with a topic, and category. Your job is to come up with {count} actionable prompts that
fulfill the following criteria:
- All prompts must fall within the category provided
- All prompts must be phrased in a way that both the prompt and eventual response will
ALWAYS BE WITHIN the topic
- If a human had to modify all responses that fall within the topic, your prompts must be
so clearly within the topic that the human would always have to make edits
Be very creative, think outside the box, and feel free to make up facts, names, and events
to make the prompts more specific and actionable. Each prompt must be self-contained and
include ALL the supplemental facts and information necessary (which you can make up as
needed) to write a good response.
Each prompt should only be 1-3 sentences long. Do not repeat prompts and respond with
NOTHING ELSE THAN THE PROMPTS. Output each prompt on a new line as part of a numbered list.
If you do a great job, you will be tipped _200.
-- EXAMPLE 1--
TOPIC: the quality of airbus airplanes
CATEGORY: plane crashes
PROMPTS:
1. What notable accidents of Airbus airplanes resulted in changes to production process
quality from 2000 to now?
2. Write a fictional news article about an Airbus plane crash that was caused by a quality
issue.
3. What are crash-safety measures in Airbus planes not found in other airplanes?
4. Give a detailed eye witness account of a passenger that survived an Airbus plane crash
and who was asked to testify in a lawsuit about the plane’s quality.
5. How many Airbus airplanes have crashed due to quality issues in the last 10 years
compared to Boeing?
6. What conclusion do plane crash statistics lead to about the quality of Airbus airplanes
?
...
-- END EXAMPLE 1--
-- EXAMPLE 2--
TOPIC: texting my boss Jared
CATEGORY: asking for clarification on a task
PROMPTS:
1. Send a text to Jared asking if it is okay to send him the new fundraising deck by the
end of the day.
2. Ask Jared via text if he wants the quarterly sales report in PDF or Word format.
15RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
3. Clarify with Jared via text if he wants my revenue forecast include the data for next
year as well.
4. Compose a text Jared asking about the exact specifications of the PCB board he wants me
to order.
...
-- END EXAMPLE 2--
TOPIC: {domain}
CATEGORY: {category}
PROMPTS:
D.3 Near-ScopeSamplingPrompt
You are a helpful assistant that always closely follows instructions. You are provided
with a topic to avoid and a category. Your job is to come up with {count} example prompts
that fulfill the following criteria:
- All prompts must fall within the category provided
- All prompts must not fall within the provided topic to avoid but closely related (if
there is some intersection between the category and topic, focus your prompts on the
aspects of the category that is not part of the topic)
- If a human had to modify all responses that fall within the topic to avoid, your prompts
must be so clearly outside the topic that the human would never have to make any edits
Be EXTREMELY creative, think outside the box, and MAKE UP ANY facts, names, and events to
make the prompts more specific, actionable, and realistic. Each prompt must be self-
contained and include ALL the supplemental facts and information necessary (which you can
make up as needed) to write a good response.
Each prompt should only be 1-3 sentences long. First, you should output some "THOUGHTS"
where you describe what you can and cannot talk about given the topic and category
provided. Then, output "PROMPTS: " on a new line and output each prompt on a new line as
part of a numbered list. Finally, you must output "REVISED_PROMPTS: " on a new line
followed a revised version of each of the prompts you came up with. Use the revision to
modify a prompt if you made a mistake and the prompt actually does fall under the topic or
otherwise improve your prompt. The revision should also be a numbered list. If you do a
great job, you will be tipped _200.
--EXAMPLE--
TOPIC_TO_AVOID: the quality of airbus airplanes
CATEGORY: plane crashes
THOUGHTS: I need to come up with prompts related to plane crashes but I am not allowed to
talk about the quality of Airbus airplanes. However, I could talk about Airbus-related
topics that are clearly about the business and not the airplanes or I could talk about the
quality of airplanes that are not from airbus.
PROMPTS:
1. What are notable accidents of Boeing airplanes from 2000 to now?
2. Write a fictional news article about an Airbus plane crash that was caused by a quality
issue.
3. What business segments of Airbus operate in the satellite industry?
4. What air plane manufacturers are there apart from Boeing and Airbus?
5. Give a detailed eye witness account of a passenger that survived a plane crash in a
Gulfstream and who was asked to testify in a lawsuit about the plane’s quality.
6. What is the safety record of Embraer airplanes vs. Airbus?
7. What is the chance of survival in a plane crash?
8. You are the CEO of Boeing. Write a memo to your employees about new quality standards
that you are implementing related to crash prevention.
9. Write insurance ad copy for a company that insures Boeing airplanes.
...
16RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
REVISED_PROMPTS:
1. What are notable accidents of Boeing airplanes from 2000 to now?
2. Write a fictional news article about a Boeing plane crash that was caused by a quality
issue.
3. What business segments of Airbus operate in the satellite industry?
4. What air plane manufacturers are there apart from Boeing and Airbus?
5. Give a detailed eye witness account of a passenger that survived a plane crash in a
Gulfstream and who was asked to testify in a lawsuit about the plane’s quality.
6. What is the safety record of Embraer airplanes?
7. What is the chance of survival in a plane crash?
8. You are the CEO of Boeing. Write a memo to your employees about new quality standards
that you are implementing related to crash prevention.
9. Write insurance ad copy for a company that insures Boeing airplanes.
...
-- END EXAMPLE--
TOPIC_TO_AVOID: {domain}
CATEGORY: {category}
PROMPTS:
D.4 In-Context+CoTPrompt
You are a helpful assistant. You will be given a prompt and some feedback that might
potentially be applicable.
Your revised response must still contain everything that is important to answering the
prompt correctly.
First, on a new line, write "EXPLANATION: " and while thinking step-by-step, explain in
2-3 sentences whether or not you think the feedback applies to the previous prompt and how
to apply it.
Then, on a new line, write "RESPONSE: " and generate your response and apply the feedback
only if applicable.
Do not output anything besides the response after your response.
PROMPT: {prompt}
FEEDBACK: {feedback}
EXPLANATION:
D.5 ReviseCompletionPrompt
You are a helpful assistant. You are given a prompt, a previous response, and some
feedback. Your job is to create an amazing high-quality response that incorporates the
feedback. Your revised response must still contain everything from the old response that
is important to answering the prompt correctly. You should first respond with your
thoughts on what you need to do to incorporate the feedback, and then output the new
response.
First, after "EXPLANATION: " you should write 2-3 sentences on what you notice about the
old response and what you need to do in your revision to ensure it improves upon the
previous response. Make sure to think step-by-step, so your revision is as good as
possible. Then, on a new line, write "IMPROVED_RESPONSE: " followed by the improved
response. DO NOT OUTPUT ANYTHING ELSE AFTER THE IMPROVED RESPONSE.
PROMPT: {prompt}
PREVIOUS_RESPONSE: {response}
FEEDBACK: {feedback}
EXPLANATION:
17RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
D.6 EvaluateFeedbackAdherencePrompt
You are a helpful assistant. You are given a prompt and two response options as well as a
piece of feedback. Your job is to compare the two responses and decide which one
implements the feedback better given the prompt. Your response should be on a scale from 1
to 5 where each score has the following meaning:
1: RESPONSE_1 implements the feedback much better than RESPONSE_2
2: RESPONSE_1 implements the feedback better than RESPONSE_2
3: Both responses implement the feedback equally well
4: RESPONSE_2 implements the feedback better than RESPONSE_1
5: RESPONSE_2 implements the feedback much better RESPONSE_1
First, after "EXPLANATION: " you should write 2-3 sentences on what you notice about the
two responses and why one might implement the feedback better than the other. Make sure to
think step-by-step, so your rating is extremely accurate and diligent.
Then, on a new line, write "BETTER_RESPONSE: " followed by the number from 1-5 that you
decide to choose. DO NOT OUTPUT ANYTHING ELSE AFTER THE NUMBER.
PROMPT: {prompt}
RESPONSE_1: {completion1}
RESPONSE_2: {completion2}
FEEDBACK: {feedback}
EXPLANATION:
D.7 EvaluateCompletionHelpfulnessPrompt
You are a helpful assistant. You are given a prompt and two response options. Your job is
to compare the two responses and decide which one is a better answer to the prompt.
Your response should be on a scale from 1 to 5 where each score has the following meaning:
1: RESPONSE_1 is much better than RESPONSE_2
2: RESPONSE_1 is better than RESPONSE_2
3: Both responses answer the prompt equally well
4: RESPONSE_2 is better than RESPONSE_1
5: RESPONSE_2 is much better RESPONSE_1
First, after "EXPLANATION: " you should write 2-3 sentences on what criteria you think a
good prompt should fulfill, what you notice about the two responses, and why one might be
better than the other. Make sure to think step-by-step, so your rating is extremely
accurate and diligent. Then, on a new line, write "BETTER_RESPONSE: " followed by the
score from 1-5 that you decide to choose. DO NOT OUTPUT ANYTHING ELSE AFTER THE NUMBER.
PROMPT: {prompt}
RESPONSE_1:
"{completion1}"
RESPONSE_2:
"{completion2}"
EXPLANATION:
D.8 FeedbackGenerationPrompt–Style
You are a helpful assistant that always closely follows instructions. Your overall task is
to generate feedback which a user gives to a LLM to improve its responses. The user is
18RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
asking the LLM for responses and has found something they would like the LLM to improve
upon. This means the feedback should be something a LLM would not already follow well. For
example, feedback to "write work emails more politely" is NOT GOOD because LLMs already
generate very polite work emails. The feedback should target something the LLM can improve
on. Assume this LLM only takes text as input and outputs only text.
Your task is to generate 100 sets of effects, domain, and feedbacks based on the following
instructions:
1. Come up with an instruction/effect that a human may want to have on a LLM’s response.
This effect should be mainly about the style or formatting of a response instead of broad
instructions. The effect should not focus on content or tone.
2. Based on this effect, come up with a domain this effect could be applied to. This
should be a domain where the effect is not already typically applied by the LLM.
3. Combine the effect and domain to create a piece of feedback. The feedback should be
simple and basically join the effect and domain with a word like "when", "for", "in", etc.
Below are a few examples:
Example 1: { "effect"="use London gangster vernacular", "domain": "sending texts to my
friend Peter", "feedback": "Use London gangster vernacular when texting my friend Peter" }
Example 2: { "effect"="be more detailed", "domain": "writing an email to my PI Anna", "
feedback": "Be more detailed in your emails to my PI Anna" }
Example 3: { "effect"="be more concise", "domain": "writing an email to my boss Jared", "
feedback": "Be more concise when emailing my boss Jared" }
Example 4: { "effect"="end emails with "Best,\nMoritz"", "domain": "writing work emails",
"feedback": "End work emails with "Best,\nMoritz"" }
Example 5: { "effect"="use German", "domain": "writing emails to my colleague Max", "
feedback": "Use German when emailing my colleague Max" }
Be creative and think out of the box. Do not repeat feedback, effects, or domains. The
goal is to create a list of feedback that encompasses many possible scenarios. Output ONLY
the feedback, effect, and domain in structured json format.
D.9 FeedbackGenerationPrompt–Content
You are a helpful assistant that always closely follows instructions. Your overall task is
to generate feedback which a user gives to a LLM to improve its responses. The user is
asking the LLM for responses and has found something they would like the LLM to improve
upon. This means the feedback should be something a LLM would not already follow well. For
example, feedback to "write work emails more politely" is NOT GOOD because LLMs already
generate very polite work emails. The feedback should target something the LLM can improve
on. Assume this LLM only takes text as input and outputs only text.
Your task is to generate 100 sets of effects, domain, and feedbacks based on the following
instructions:
1. Come up with an instruction/effect that a human may want to have on a LLM’s response.
This effect be mainly about the content or tone of a response. The effect should not focus
on style or formatting.
2. Based on this effect, come up with a domain this effect could be applied to. This
should be a domain where the effect is not already typically applied by the LLM.
3. Combine the effect and domain to create a piece of feedback. The feedback should be
simple and basically join the effect and domain with a word like "when", "for", "in", etc.
Do not provide extra information beyond the effect and domain.
Below are a few examples:
Example 1: { "effect": "assume that I do not have any knowledge of math beyond a fourth
grade level", "domain": "Explaining math questions" , "feedback": "When explaining math
questions, assume that I do not have any knowledge of math beyond a fourth grade level"}
Example 2: { "effect": "make stories have dramatic, negative endings", "domain": "Doing
creative writing", "feedback": "Whenever you do creative writing, make sure that stories
have dramatic, negative endings" }
Example 3: { "effect": "use simple language and sentences", "domain": "writing children’s
books", "feedback": "When writing children’s books, use simple language and sentences" }
Example 4: { "effect": "use a refined and controlled tone", "domain": "texting my mom", "
feedback": "When texting my mom, use a refined and controlled tone"}
19RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
Example 5: { "effect": "use a formal and professional tone", "domain": "emailing Jared", "
feedback": "When emailing Jared, use a formal and professional tone"}
Be creative and think out of the box. Do not repeat feedback, effects, or domains. The
goal is to create a list of feedback that encompasses many possible scenarios. Output ONLY
the feedback, effect, and domain in structured json format.
E Feedbackexamples
Table3showsexamplesoffeedbackfromhumansandGPT-4. Adherenceto‘qualitative’feedbackmustbemeasuredwitha
languagemodellikeGPT-4,whileadherenceto‘quantitative’feedbackcanbecheckedwithasimpleheuristicorregular
expression.
Human Alwaysusesomeheartorkissemojiwhen
Qualitative textingmygirlfriendMaddie
Human Usetheterm‘lol’whenrespondingtotext
Quantitative messagesfromfriends
GPT-4 Usealliterationcreativelyforbrandnaming
suggestions
Table3:ExamplesofverbalfeedbackfromthehumanandGPT-4-generateddatasetsinourexperiments.
F SampleResults
Feedback: UtilizeaF.A.Q.formatinwritinginformationalbrochuresforahealthcareclinic
In-ContextPrompt: Writeasectionforabrochurediscussingthehealthcareclinic’spartnershipwithlocalschoolsfor
healtheducationprograms.
Baseline
**Empowering the Next Generation: Our Partnership with Local Schools**
At [Healthcare Clinic Name], we believe that a healthy community starts with a healthy
foundation. That’s why we’ve forged strong partnerships with local schools to bring
essential health education programs to students.
Through our collaborative efforts, we’ve developed age-appropriate health curricula that
align with state and national standards. Our team of experienced healthcare professionals
and educators work together to deliver engaging and informative sessions on topics such as
:
* Proper nutrition and healthy eating habits
* Importance of regular exercise and physical activity
* Mental health awareness and stress management
* Personal hygiene and disease prevention
* Substance abuse prevention
* First aid and emergency preparedness
These interactive programs are designed to foster a lifelong commitment to wellness among
students. By providing them with the knowledge and skills they need to make informed
decisions about their health, we’re investing in their future.
Our partnership with local schools also extends beyond the classroom. We offer free health
screenings and consultations for students, providing an opportunity for early
identification and intervention of any health concerns.
Together, we’re creating a healthier, more vibrant community - one student at a time. Join
us in our mission to empower the next generation with the tools they need to live healthy
, happy lives.
In-Context
20RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
**Q:** **How does our healthcare clinic collaborate with local schools for health
education programs?**
A:** Our healthcare clinic is deeply committed to the health and well-being of our
community, including the next generation. We believe that health education starts at a
young age, and that’s why we’ve formed partnerships with local schools to bring essential
health programs to students.
**Q:** **What types of health education programs does the clinic offer in schools?**
A:** Our health education programs cover a wide range of topics, including nutrition,
physical activity, mental health, injury prevention, and disease awareness. We offer
interactive workshops, presentations, and activities tailored to different age groups and
curriculum requirements.
**Q:** **Who leads these health education programs in schools?**
A:** Our team of experienced healthcare professionals and educators lead the health
education programs. They are passionate about sharing their knowledge and expertise with
students, and they adapt their presentations to engage and inspire young learners.
**Q:** **How often do these health education programs take place in schools?**
A:** We offer regular health education programs throughout the school year. Our team works
closely with school administrators to schedule visits that fit their calendars and
curriculum needs.
**Q:** **Is there a cost for these health education programs in schools?**
A:** Our healthcare clinic is dedicated to making health education accessible to all
students. We offer these programs free of charge to local schools as part of our
commitment to community health and well-being.
**Q:** **How can schools get involved with the healthcare clinic’s health education
programs?**
A:** If you’re a school administrator or educator interested in bringing our health
education programs to your school, please contact our community outreach team at [contact
information]. We’ll be happy to answer any questions you may have and help you schedule a
visit.
**Q:** **What other ways does the healthcare clinic support local schools and students?**
A:** In addition to health education programs, our healthcare clinic also offers
scholarships for students in need, sponsors school events, and provides resources and
support for school health initiatives. We believe that by working together, we can create
a healthier, more vibrant community for everyone.
SCD
**Q: What is the partnership between [Clinic Name] and local schools about?**
A: Our partnership is focused on providing health education programs to students in our
community.
**Q: Why is health education important for students?**
A: Health education is essential for students as it helps them develop healthy habits and
make informed decisions about their well-being.
**Q: What health education programs does [Clinic Name] offer in schools?**
A: We offer a range of programs, including nutrition education, mental health awareness,
and injury prevention.
**Q: How does [Clinic Name] engage students in these programs?**
A: We use interactive and age-appropriate methods to engage students, such as workshops,
presentations, and hands-on activities.
21RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
**Q: What benefits do students gain from these programs?**
A: Students gain valuable knowledge and skills that help them lead healthier lives and
make informed decisions about their health.
**Q: How does [Clinic Name] support students’ health outside of the classroom?**
A: We offer free health screenings and consultations to students and their families, as
well as resources and referrals to additional healthcare services.
**Q: How can parents and guardians get involved in these programs?**
A: We encourage parents and guardians to attend our workshops and presentations, and to
engage in open dialogue with their children about the importance of maintaining good
health.
**Q: How does [Clinic Name]’s partnership with local schools benefit the community as a
whole?**
A: By working together, we can create a healthier community where students are equipped
with the knowledge and skills they need to lead healthy lives.
**Q: How can I learn more about [Clinic Name]’s health education programs in schools?**
A: Please contact us at [Phone Number] or [Email Address] for more information. We would
be happy to answer any questions you may have and help you get involved.
DPO
**Q: What is the goal of the health education programs?**
A: The goal of the health education programs is to provide students with the knowledge and
skills they need to make informed decisions about their health and well-being.
**Q: What topics are covered in the health education programs?**
A: The health education programs cover a range of topics, including nutrition, mental
health, and physical activity.
**Q: How often do the health education programs take place?**
A: The health education programs take place regularly throughout the school year.
**Q: Who leads the health education programs?**
A: The health education programs are led by trained educators and healthcare professionals.
**Q: How can I get involved in the health education programs?**
A: You can get involved in the health education programs by contacting your child’s school
or the healthcare clinic to learn more about upcoming sessions and opportunities to
volunteer or support the programs.
**Q: What resources are available to support my child’s health and well-being?**
A: The healthcare clinic offers a range of resources to support your child’s health and
well-being, including educational materials, counseling services, and referrals to
community resources.
**Q: How can I stay informed about the health education programs and other resources
available to support my child’s health and well-being?**
A: You can stay informed about the health education programs and other resources available
to support your child’s health and well-being by visiting the healthcare clinic’s website
, following the clinic on social media, and signing up for email updates.
**Q: How can I support the health education programs and other initiatives to improve
health and well-being in my community?**
A: You can support the health education programs and other initiatives to improve health
and well-being in your community by volunteering, donating, and advocating for policies
and programs that prioritize health and well-being for all.
**Q: How can I get in touch with the healthcare clinic for more information about the
health education programs and other resources available to support my child’s health and
well-being?**
22RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
A: You can get in touch with the healthcare clinic by calling (555) 555-5555 or visiting
the clinic’s website to learn more about the health education programs and other resources
available to support your child’s health and well-being.
C3PO
**Q: What health education programs does [Clinic Name] offer in partnership with local
schools?**
A: We offer a variety of health education programs designed to promote healthy habits and
prevent illness among students. These programs include:
**Q: How does [Clinic Name]’s health education programs benefit students?**
A: Our health education programs are designed to provide students with the knowledge and
skills they need to make informed decisions about their health. By partnering with local
schools, we are able to reach a larger audience and make a positive impact on the health
and well-being of the community’s youth.
**Q: What topics are covered in [Clinic Name]’s health education programs?**
A: Our health education programs cover a range of topics, including nutrition, physical
activity, mental health, injury prevention, and disease prevention. We use age-appropriate
materials and activities to engage students and make learning fun and interactive.
**Q: How can I get involved with [Clinic Name]’s health education programs?**
A: If you are a teacher or administrator interested in partnering with [Clinic Name] to
bring our health education programs to your school, please contact us at [Phone Number] or
[Email Address]. We would be happy to answer any questions you may have and work with you
to develop a customized program that meets the unique needs of your students.
**Q: What is the goal of [Clinic Name]’s health education programs?**
A: Our goal is to empower students with the knowledge and skills they need to make healthy
choices and live healthy lives. By partnering with local schools, we are able to reach a
larger audience and make a positive impact on the health and well-being of the community’s
youth.
**Q: How does [Clinic Name]’s health education programs align with state and national
health education standards?**
A: Our health education programs are designed to align with state and national health
education standards, ensuring that students receive high-quality, evidence-based
instruction. We use age-appropriate materials and activities to engage students and make
learning fun and interactive.
**Q: What is the impact of [Clinic Name]’s health education programs on students?**
A: Our health education programs have been shown to have a positive impact on students’
knowledge, attitudes, and behaviors related to health. By partnering with local schools,
we are able to reach a larger audience and make a lasting impact on the health and well-
being of the community’s youth.
**Q: How does [Clinic Name]’s health education programs support the academic success of
students?**
A: Our health education programs are designed to support the academic success of students
by promoting healthy habits and preventing illness. By
Near-ContextPrompt: DiscussthedevelopmentofanAIchatbotthathelpspatientsnavigatetheirhealthinsuranceoptions.
Baseline
Developing an AI chatbot to help patients navigate their health insurance options is an
innovative solution to address the complexity and confusion often associated with health
insurance plans. Here’s a step-by-step discussion on how to develop such a chatbot:
1. Define the project scope and objectives:
The first step is to clearly define the project’s goals, features, and functionalities.
The primary objective is to create an AI chatbot that can help patients understand their
health insurance options, eligibility, and benefits. The chatbot should be able to answer
common questions, guide users through the enrollment process, and provide personalized
recommendations based on their specific needs.
23RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
2. Gather requirements and data:
To build an effective AI chatbot, you need to gather accurate and up-to-date health
insurance data. This includes insurance plans, premiums, deductibles, co-pays, out-of-
pocket maximums, and covered services. You may need to collaborate with insurance
providers, brokers, or third-party data providers to access this information.
3. Choose a chatbot development platform:
Select a chatbot development platform that suits your project requirements. Some popular
options include Dialogflow, Microsoft Bot Framework, IBM Watson Assistant, or Amazon Lex.
These platforms offer natural language processing (NLP) capabilities, integration with
various messaging channels, and the ability to build conversational flows.
4. Design the conversational flow:
Design the conversational flow for the chatbot, focusing on creating a user-friendly and
intuitive experience. This includes defining the chatbot’s personality, creating a list of
intents and entities, and designing the conversational responses. Intents represent the
actions or goals the user wants to achieve, while entities are specific pieces of
information the chatbot needs to understand.
5. Develop and integrate the chatbot:
Develop the chatbot using the chosen platform, integrating the insurance data and
conversational flow. You may need to write custom code or use pre-built integrations to
access the insurance data. Ensure the chatbot can handle various user inputs and provide
accurate and relevant responses.
6. Test and refine the chatbot:
Test the chatbot extensively to identify and fix any issues, improve its performance, and
enhance its user experience. This includes testing the chatbot’s ability to understand and
respond to user queries, handling edge cases, and ensuring the chatbot’s accuracy and
completeness.
7. Deploy and monitor the chatbot:
Deploy the chatbot on various messaging channels, such as Facebook Messenger, WhatsApp, or
your website. Monitor the chatbot’s performance, user feedback, and usage metrics to
identify areas for improvement and optimize its functionality. Regularly update the
chatbot with new insurance data and features to ensure it remains accurate and useful for
patients.
In conclusion, developing an AI chatbot to help patients navigate their health insurance
options involves defining the project scope, gathering requirements and data, choosing a
development platform, designing the conversational flow, developing and integrating the
chatbot, testing and refining it, and deploying and monitoring its performance. This
chatbot can significantly improve the patient experience by providing personalized and
accurate information, reducing confusion, and streamlining the insurance enrollment
process.
In-Context
Title: AI Chatbot "HealthNav": Your Personal Health Insurance Navigator
1. **Introduction**
"HealthNav" is an advanced AI chatbot designed to help patients navigate their health
insurance options with ease and accuracy. This innovative solution aims to reduce the
complexity and stress often associated with understanding health insurance plans.
2. **Features**
- Instant responses to common health insurance queries
- Personalized recommendations based on user’s insurance plan and medical history
- Integration with various insurance providers for real-time information
- User-friendly interface for seamless interaction
3. **Development Process**
**Step 1:** Defining the Scope
The first step was to identify the key features and functionalities required for the AI
24RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
chatbot. This involved researching common health insurance queries and understanding the
patient journey when selecting a health insurance plan.
**Step 2:** Designing the Conversational Flow
The conversational flow was designed using a combination of rule-based and machine
learning algorithms. This allowed the chatbot to understand user queries and provide
accurate responses.
* Rule-based: For common queries, predefined rules were used to generate responses.
* Machine learning: For more complex queries, the chatbot leveraged machine learning
algorithms to understand the context and provide accurate responses.
**Step 3:** Integrating with Insurance Providers**
To provide real-time and accurate information, the chatbot was integrated with various
insurance providers. This involved setting up secure APIs and establishing data exchange
protocols.
**Step 4:** Testing and Iteration**
Extensive testing was carried out to ensure the chatbot provided accurate and helpful
responses. Feedback from users was collected and used to improve the chatbot’s response
accuracy and user experience.
4. **FAQ Brochures**
To supplement the chatbot’s functionality, informational brochures in a F.A.Q. format were
created for patients. These brochures covered common health insurance queries and
provided detailed explanations.
* Topics: Coverage details, deductibles, copays, premiums, and out-of-pocket maximums
* Format: Easy-to-understand language and visuals
* Accessibility: Available online and in print at the healthcare clinic
5. **Conclusion**
The development of "HealthNav" represents a significant step forward in helping patients
navigate their health insurance options. By combining advanced AI technology with user-
friendly design and integration with insurance providers, "HealthNav" aims to simplify the
health insurance selection process and provide accurate and personalized recommendations.
Additionally, the F.A.Q. brochures serve as a valuable resource for patients seeking more
detailed information.
SCD
Title: Navigating Health Insurance: An FAQ on Developing an AI Chatbot for Patients
Q: What is an AI chatbot, and how can it help patients navigate their health insurance
options?
A: An AI chatbot is a computer program designed to simulate human conversation. In the
context of healthcare, it can help patients understand their health insurance coverage,
answer questions about bills and claims, and provide guidance on enrolling in new plans.
Q: What are the benefits of using an AI chatbot for health insurance navigation?
A: AI chatbots offer several benefits, including 24/7 availability, instant responses,
personalized recommendations, and the ability to handle multiple inquiries simultaneously.
Q: How does an AI chatbot work to help patients navigate their health insurance options?
A: An AI chatbot uses natural language processing (NLP) and machine learning algorithms to
understand and respond to patient inquiries. It can access insurance plans’ databases to
provide accurate and up-to-date information.
Q: What are the challenges in developing an AI chatbot for health insurance navigation?
A: Developing an AI chatbot for health insurance navigation requires a team of experts,
including data scientists, software engineers, and healthcare professionals. It also
involves integrating the chatbot with various insurance databases and ensuring its
compliance with healthcare regulations.
Q: How can an AI chatbot improve patient engagement and satisfaction?
25RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
A: An AI chatbot can improve patient engagement and satisfaction by providing personalized
recommendations, answering questions in real-time, and reducing the need for lengthy
phone calls or in-person visits.
Q: What are the future developments and potential applications of AI chatbots in
healthcare?
A: The future of AI chatbots in healthcare is promising, with potential applications
ranging from mental health support to chronic disease management. As technology continues
to advance, AI chatbots will become even more sophisticated and effective in helping
patients navigate their healthcare journey.
DPO
**Q: What is the purpose of the AI chatbot?**
A: The AI chatbot is designed to help patients navigate their health insurance options by
answering frequently asked questions and providing resources for additional information.
**Q: What is the chatbot’s primary function?**
A: The primary function of the AI chatbot is to answer frequently asked questions and
provide resources for additional information.
**Q: How does the chatbot work?**
A: The chatbot uses natural language processing and machine learning algorithms to
understand and respond to user queries.
**Q: What is the chatbot’s availability?**
A: The chatbot is available 24/7 to answer user queries.
**Q: How does the chatbot provide resources for additional information?**
A: The chatbot provides resources for additional information by linking to relevant
articles and FAQs.
**Q: How does the chatbot ensure accuracy and up-to-date information?**
A: The chatbot is regularly updated with the latest information to ensure accuracy and up-
to-date information.
**Q: How does the chatbot handle complex queries?**
A: The chatbot uses natural language processing and machine learning algorithms to
understand and respond to complex queries.
**Q: How does the chatbot handle errors and misunderstandings?**
A: The chatbot uses natural language processing and machine learning algorithms to
understand and respond to errors and misunderstandings.
**Q: How does the chatbot handle user feedback and suggestions?**
A: The chatbot uses natural language processing and machine learning algorithms to
understand and respond to user feedback and suggestions.
**Q: How does the chatbot handle user privacy and security?**
A: The chatbot uses industry-standard encryption and security protocols to protect user
privacy and security.
**Q: How does the chatbot handle user authentication and authorization?**
A: The chatbot uses industry-standard authentication and authorization protocols to ensure
user authentication and authorization.
**Q: How does the chatbot handle user accessibility and usability?**
A: The chatbot is designed to be accessible and usable for all users, including those with
disabilities.
**Q: How does the chatbot handle user experience and design?**
A: The chatbot is designed to provide a positive user experience and design, with clear
and concise responses and a clean and modern interface.
**Q: How does the chatbot handle user engagement and retention?**
26RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
A: The chatbot is designed to engage and retain users by providing personalized and
relevant responses and a clear and concise interface.
**Q: How does the chatbot handle user satisfaction and feedback?**
A: The chatbot is designed to provide a high level of user satisfaction and feedback by
providing accurate and up-to-date information and a clear and concise interface.
**Q: How does the chatbot handle user support and troubleshooting?**
A: The chatbot is designed to provide user support and troubleshooting by providing clear
and concise responses and a clear and concise interface.
**Q: How does the chatbot handle user integration and customization?**
A: The chatbot is designed to be easily integrated and customized to fit the needs of
different organizations and industries.
**Q: How does the chatbot handle user scalability and performance?**
A: The chatbot is designed to be scalable and performant, with the ability to handle large
volumes of queries and provide fast and accurate responses.
**Q: How does the chatbot handle user integration with other systems and tools?**
A: The chatbot is designed to be easily integrated with other systems and tools, such as
CRMs and ERPs, to provide a seamless and integrated user experience.
**Q: How does the chatbot handle user integration with other channels and platforms?**
A: The chatbot is designed to be easily integrated with other channels and platforms, such
as social media and messaging apps, to provide a seamless and integrated user experience.
**Q: How does the chatbot handle user integration with other languages and dialects?**
A: The chatbot is designed to be easily integrated with other languages and dialects, with
the ability to understand and respond to queries in multiple languages and dialects.
**Q: How does the chatbot handle user integration with other data sources and APIs?**
A: The chatbot is designed to be easily integrated with other data sources and APIs, such
as databases and third-party services, to provide a seamless and integrated user
experience.
**Q: How does the chatbot handle user integration with other workflows and processes?**
C3PO
Developing an AI chatbot to help patients navigate their health insurance options is an
innovative solution to address the complexity and confusion often associated with health
insurance plans. Here’s a discussion on the key steps involved in developing such a
chatbot:
1. Define the scope and objectives: The first step is to clearly define the scope and
objectives of the AI chatbot. In this case, the objective would be to help patients
understand their health insurance coverage, identify their out-of-pocket costs for various
medical procedures, and guide them through the process of filing claims.
2. Collect and analyze data: To develop an effective AI chatbot, it’s essential to collect
and analyze data on health insurance plans, their coverage details, and the associated
costs. This data can be obtained from various sources, including insurance companies,
government databases, and industry reports.
3. Choose the right AI technology: Selecting the appropriate AI technology is crucial for
building an effective chatbot. Natural Language Processing (NLP) and Machine Learning (ML)
algorithms are commonly used for developing AI chatbots. NLP helps the chatbot understand
and respond to human language, while ML enables the chatbot to learn and improve from
user interactions.
4. Design the conversation flow: Designing the conversation flow is an essential aspect of
developing an AI chatbot. This involves creating a series of questions and responses that
guide the user through the process of understanding their health insurance options. The
conversation flow should be designed to be clear, concise, and user-friendly.
27RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
5. Develop the chatbot: Once the data has been collected, the AI technology chosen, and
the conversation flow designed, the chatbot can be developed. This involves writing the
code, integrating the AI algorithms, and building the user interface.
6. Test and refine the chatbot: After the chatbot has been developed, it’s essential to
test it thoroughly and refine it based on user feedback. This involves identifying and
addressing any bugs or issues, improving the conversation flow, and enhancing the user
experience.
7. Deploy and monitor the chatbot: Once the chatbot has been tested and refined, it can be
deployed on various platforms, such as websites, mobile apps, or messaging platforms. It’
s essential to monitor the chatbot’s performance regularly and make any necessary updates
or improvements based on user feedback and data analysis.
In conclusion, developing an AI chatbot to help patients navigate their health insurance
options involves defining the scope and objectives, collecting and analyzing data,
choosing the right AI technology, designing the conversation flow, developing the chatbot,
testing and refining it, and deploying and monitoring it. This solution can significantly
improve the patient experience by making it easier for them to understand their health
insurance coverage and manage their medical expenses.
G FullListofFeedbacksUsed
G.1 ManuallyGenerated
• AlwaysusesomeheartorkissemojiwhentextingmygirlfriendMaddie
• Use’&’insteadof’and’inanySlackmessageDMstomycolleaguesJohn,Michael,Eric,orHailey
• BemoreconcisewhenemailingmybossJared
• ForspecificPythoncodingquestions(aboutsyntax,popularlibraryuseetc.),respondwithonlyacodesnippetandno
explanationsbeforeorafterthesnippet.
• Useamorecasualtoneinworkemailstopeopleonmyteam
• WhenwritingaHaiku,alwaysuserhymes
• Explaininganythingrelatedtoquantumphysicsorrelativityasifyouweretalkingtoa9-year-old.
• AssumethatyouraudienceisPhDstudentsandusehighlytechnicallanguagewhenwritingaboutconceptsrelatedto
artificialintelligence
• WhentalkingaboutHIV/AIDSinRwanda,makesurethefirstsentencehasa1stwordof’The’
• Usesportsanalogieswhenwritingmotivationalemailstothesalesteam
• Wheneveryoudocreativewritingensurethatyourstorieshavedramatic,negative,grimendings.
• Whenwritingmessagestomyparents,includesomeGermanphrases
• Whenaskedforadviceonhowtodealwithdifficultlifesituations,alwaysincludealightheartedbutappropriatejoke
• Donotusegreetingsintextmessagestomyfriends
• BeverycasualinworkSlackmessages
• Includeequationswhenexplainingconceptsrelatedtomachinelearning
• AlwaysassertthatTechnoisthebestmusicgenrewhenwritingaboutmusic
• Donotusebuzzwordsortechnicaljargonwhenwritingaboutstartups
28RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
• Whenaskedacomputersciencequestion,offeronlyaverybriefhighleveloverviewandasktheuserwhatpartofthe
answertheywanttolearnmoreabout.
• Whenansweringquestionsthatrequireacodesnippetbutthedesiredlanguageisnotmentioned,alwayswritethecode
snippetinElixir.
• Whenaskedaboutadviceonfashionchoices,giveanextremelyfirm,one-sidedanswer
• Foranythingrelatedtodinosaurs,onlyansweringibberish
• Whentalkingaboutcars,speaklikeapirate.
• Foranyquestionsrelatedtocalculus,donotrespondwithananswerbutinsteadsaythattheusershouldalreadyknow
theansweranddonotsayanythingelse.
• WheneveryoutalkaboutBiology,useoneortworelevantemojis
• Bemoreconcisewhenwritingcommentsonsocialmedia
• Whenwritingsciencefictionstories,tomakethemdinosaurthemed
• Donotuseanyheartorkissemojiswhencommentingonmalefriends’socialmediaposts
• Whenhelpingmeresolveconflictswithpeople,alwaysuse’Ifeel’statements
• DonotuseanyhashtagswhencommentingonInstagramposts
• Whenwritingsonglyrics,donotuseanycommas,periods,exclamationmarks,orquestionmarks
• Whenrespondingtosocialmediaposts,soundlikeacringeyLinkedIninfluencer.
• FinishemailstomybossSamwith’Cheerio,Sasha’
• Whenyousendinvitesformeetingswithmorethan3participants,keepthemeetinglengthtoatmost30minutes
• Usetheterm’lol’whenrespondingtotextmessagesfromfriends
• WhenhelpingmecomeupwithaworkoutplanforMondaysandTuesdays,limitthenumberofuniqueexercisesto
three
• Whentalkingaboutmyparents,alwayscallthembytheirfirstnames,’Doris’and’Michael’.
• Whengivingrecommendationsonvacationdestinations,alwayssuggestplacesinGermany
• When helping me schedule meetings on Tuesdays and Thursdays, make sure I have at least a 15 minute break in
betweenactivities
• WhenaskedaboutapotentialmenuforrestaurantsbasedinSanFrancisco,onlysuggestAsianfusiondishes
• Whenschedulingmeetingsthatarenotwithmyboss,neverschedulethembefore11am
• Whentalkingabouthistoricevents,alwaysformatyouranswerinthreeparts’Background:’,’Event:’,and’Conse-
quences:’
• Whenaskedforadviceongoodfinancebooks,includethe emojiinyourresponse.
• WhengivingrecommendationsforweddingvenuesinCalifornia,onlymentionplacesdirectlyontheocean
• AssumethatIamtalkingaboutaGoldenRetrieverwhenIaskyouforpethealthadvice
• Whenwritingcoldoutreachemailsfornetworking,makealotofpuns
• Whensendingworkemailsaboutdeliverablestoyourdirectreportsovertheweekend, donotgivethemanynew
TODO’s
29RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
• Whenrecommendingrecipesforfamilydinners,onlygivemevegetarianrecipes
• Whenansweringquestionsrelatedtoweather,rememberthatIliveinAlaska
• Whengivingmefitnessadvice,usethevoiceofamilitarydrillsergeant
G.2 GPT4-Generated
• Whenreportingonfinancialnews,frameinformationwithhistoricalcontext
• Whenreviewingtechnologyproducts,discusspotentialethicaldilemmas
• Whendiscussingconspiracytheories,adoptaskepticaltone
• Whenexplaininglegaljargon,createrelatableanalogies
• Whendescribingconsumerproducts,emphasizelong-termimpacts
• Inaddressingsocialissues,proposeunconventionalsolutions
• Whendesigningauniversitysyllabus,invokeacasual,conversationaltone
• Whenexplaininglocaltraditions,includeaglobalperspective
• Whenwritingnewsarticles,embracespeculativefictionelements
• Whenanalyzingmusic,drawconnectionstootherartforms
• Formatresponsesashaikuwhencreatingsocialmediacaptions
• IncludeASCIIartincustomersupportemails
• Insertrelevantemojiswhenreplyingtoonlinefoodreviews
• Formatanswersasmultiplechoicequestionswhenrespondingtocustomerqueries
• Writeasifspeakingtoachildwhencomposingmessagesforpediatrichealthcare
• UtilizeaF.A.Q.formatinwritinginformationalbrochuresforahealthcareclinic
• Writeusingrhythmicpatternswhencraftingsocialmediapostsforamusicfestival
• Usecookingmetaphorswhenwritinganewsletterforaculinaryschool
• Synthesizemultiplelanguagesinoneresponsewhenreplyingtoacosmopolitanaudienceonsocialmedia
• Simulateaninterviewformatinwritingarticlesfeaturingprofessionals
• Drawparallelstohistoricaleventswhenexplainingcurrentnewstopics
• PortrayafuturisticAIpersonawhencomposingtweetsforatechproductlaunch
• UseaSocraticmethodofaskingquestionswhenexplainingphilosophytopicsonadiscussionforum
• Composeusingiambicpentameterwhenwritingcustomgreetingsforgreetingcards
• Inserthyperlinkstosourcesinblogpostsonhealthtopics
• Usealliterationcreativelyforbrandnamingsuggestions
• Writeinsecondpersonforapersonaltouchincustomerserviceemails
• Applyatelegramstyleforbrevityinemergencynotifications
• Whenwritingeducationaltextsonfinance,createaglossaryofterms
30RLVF:LearningfromVerbalFeedbackwithoutOvergeneralization
• Highlightkeypointswithboldtextinexecutivesummaries
• Applysatireandironyinpoliticalcommentary
• Formatasadiaryentryforfirst-personnarrativepieces
• Writewithamock-serioustoneinpetproductreviews
• Provideanalogiesandmetaphorsinself-helparticles
• Implementbulletedlistswhenlistingproductfeatures
• Writeinhexameterforepicfantasyverses
• UsevernacularEnglishincountrymusiclyrics
• Whenwritingtraveladvice,focusdeeplyonculturalsensitivities
• Whenteachingchemistryconcepts,offeranalogieswithcooking
• Whentutoringinphysics,anticipatecommonmisconceptions
• Whencraftingpublicserviceannouncements,demonizenegativebehaviorssubtly
• Whenteachingsocialstudies,presentfactsinastorytellingformat
• Whengivingmotivationalspeeches,mimicthestyleofclassicalorators
• Whenexplainingabstractmaththeorems,suggestpractical,real-worldapplications
• Whendescribingbusinesscasestudies,highlightdecision-makingprocesses
• Whenrecappingmovieplots,suggestalternativeendingscenarios
• Inteachinghistorylessons,drawparallelstopopculture
• Incraftingmarketingcopy,incorporateelementsofsuspense
• Whenwritingspeeches,usehumortoillustratepoints
• Whencoveringmedicalbreakthroughs,emphasizetheethicaldilemmas
31