Weak-Mamba-UNet:
Visual Mamba Makes CNN and ViT Work Better
for Scribble-based Medical Image Segmentation
Ziyang Wang1 and Chao Ma2
1 Department of Computer Science, University of Oxford, UK
2 Mianyang Visual Object Detection and Recognition Engineering Center, China
ziyang.wang@cs.ox.ac.uk
https://github.com/ziyangwang007/Mamba-UNet
Abstract. Medical image segmentation is increasingly reliant on deep
learningtechniques,yetthepromisingperformanceoftencomewithhigh
annotationcosts.ThispaperintroducesWeak-Mamba-UNet,aninnova-
tiveweakly-supervisedlearning(WSL)frameworkthatleveragestheca-
pabilities of Convolutional Neural Network (CNN), Vision Transformer
(ViT), and the cutting-edge Visual Mamba (VMamba) architecture for
medicalimagesegmentation,especiallywhendealingwithscribble-based
annotations. The proposed WSL strategy incorporates three distinct
architecture but same symmetrical encoder-decoder networks: a CNN-
based UNet for detailed local feature extraction, a Swin Transformer-
basedSwinUNetforcomprehensiveglobalcontextunderstanding,anda
VMamba-based Mamba-UNet for efficient long-range dependency mod-
eling. The key concept of this framework is a collaborative and cross-
supervisorymechanismthatemployspseudolabelstofacilitateiterative
learningandrefinementacrossthenetworks.TheeffectivenessofWeak-
Mamba-UNetisvalidatedonapubliclyavailableMRIcardiacsegmenta-
tion dataset with processed scribble annotations, where it surpasses the
performance of a similar WSL framework utilizing only UNet or Swin-
UNet. This highlights its potential in scenarios with sparse or imprecise
annotations. The source code is made publicly accessible.
Keywords: MedicalImageSegmentation,MambaUNet,Weakly-Supervised
Learning, Scribble.
1 Introduction
Medicalimagesegmentationisimportantformedicalimageanalysisandeffective
treatment planning for healthcare purpose, with deep learning-based networks
i.e.UNet[21].TheUNetknownforitssymmetricalU-shapeencoder-decoderar-
chitectureandintegralskipconnections,hasbeenthefoundationalsegmentation
backbone network. These skip connections effectively preserve essential spatial
information, merging features across the encoder and decoder layers to enhance
the network’s performance. The encoder reduces the input to extract high-level
4202
beF
61
]VI.ssee[
1v78801.2042:viXra2 Z Wang, C Ma
Fig.1. The Example Images of MRI Cardiac Scans, with the Corresponding Ground
Truth, and Scribble-based Annotations.
features, which the decoder then uses to reconstruct the image, thereby im-
proving segmentation performance. Advancements in UNet have led to various
enhanced networks designed to tackle the segmentation of complex anatomical
structures in CT and MRI scans[43,39,19,36,41].
Recent advancements have introduced innovative architectures such as the
Transformer and Mamba, both of which excel in capturing global contextual
information[26,8].TheTransformerachievesthisthroughamulti-headself-attention
mechanism, while Mamba is noted for its computational efficiency, grounded in
the State Space Model (SSM) [29,8,7]. These architectures have been applied to
arangeofcomputervisiontasks,leadingtodevelopmentsliketheVisionTrans-
former [6], Swin Transformer [14], nnFormer [42], ScribFormer [12], and UNetr
[9] for Transformers, and Vision Mamba [44], UMamba [18], Segmamba [38],
MambaUNet [37], VM-UNet [22], and Semi-MambaUNet [30] for Mamba-based
networks.
The effectiveness of deep learning methods often hinges on the availability
of large, accurately labeled datasets, which can be challenging to acquire in the
medical image analysis domain. To address the high costs and time associated
withobtainingdetailedannotationslikepixel-levelsegmentationmasks,research
hasshiftedtowardsSemi-SupervisedLearning(SSL)[4,16,33,10,31]andWeakly-
Supervised Learning (WSL) [15,35,20,13,34]. SSL focuses on training networks
withasmallsetofpixel-levellabeleddata,whereasWSLemployssimplerforms
of annotations such as bounding boxes, checkmarks, and points to provide a
feasibleapproachfortrainingsegmentationnetworksunderlimited-signalsuper-
vision. Among these, scribble-based annotation is particularly noted for its effi-
ciencyandconvenienceforexperts,streamliningtheannotationprocesswithoutWeak-Mamba-UNet for Medical Image Segmentation 3
significantly compromising the quality of supervision. Examples of MRI scans,
conventional dense annotations, and scribble-based annotations are illustrated
in Figure 1.
Following the recent success of the Transformer and Mamba architectures
in computer vision tasks, and concern with limited annotated data, this pa-
per introduces Weak-Mamba-UNet. The proposed WSL framework integrates
Convolution, Transformer, and Mamba architectures within a multi-view cross-
supervised learning scheme tailored for scribble-based supervised medical image
segmentation.Tothebestofourknowledge,thisisthefirstefforttoleveragethe
Mamba architecture for medical image segmentation with scribble annotations.
The contributions of Weak-Mamba-UNet are threefold:
1. TheintegrationofaMamba-basedsegmentationnetworkwithWSLformed-
ical image segmentation using scribble-based annotations.
2. The development of a novel multi-view cross-supervised framework that en-
ables the collaborative operation of three distinct architectures: CNN, ViT,
and Mamba, under conditions of limited-signal supervision.
3. DemonstrationsofWeak-Mamba-UNetonapubliclyavailablepre-processed
dataset for scribble-based experiments demonstrating the Mamba architec-
ture’scapabilitytoenhancetheperformanceofCNNandViTinWSLtasks.
Fig.2. Semi-Mamba-UNet: The Framework of Contrastive Cross-Supervised Visual
Mamba-based UNet for Semi-Supervised Medical Image Segmentation.
2 Methodology
TheframeworkofWeak-Mamba-UNetisillustratedinFigure2.Inthisstudy,the
pair (X,Y ) represents the scribble-based labeled training dataset, whereas
scrib4 Z Wang, C Ma
the pair (X ,Y ) denotes the dense labeled testing dataset. Here, X ∈ Rh×w
t t
corresponds to a 2D grayscale image of height h and width w. The scribble
annotations Y ∈ {0,1,2,3,None} indicate the regions corresponding to the
scrib
right ventricle (RVC), left ventricle (LVC), myocardium (MYO), background,
and unlabeled pixels, respectively.
Threesegmentationnetworksaredenotedasf (X;θ),f (X;θ),andf (X;θ),
cnn vit mamba
andarehighlightedingreen,blue,andyellowinFigure2,respectively.Thepre-
diction of a segmentation network for an input X is denoted as Y = f(X;θ),
p
where θ represents the network parameters. The predictions from the three net-
works can be combined to form a dense pseudo label Y .
pseudo
The overall loss comprises the scribble-based partial cross-entropy loss L
pce
and the dense-signal pseudo label dice-coefficient loss L . The total training
dice
objective aims to minimize the combined loss L , which is formulated as:
total
3
(cid:88)
L = (Li +Li ) (1)
total pce dice
i=1
where i indicates each of three networks. All mathematical symbols are defined
in Figure 2. The final evaluation assesses the agreement between the predicted
labels Y and the true dense labels Y on the test set.
p t
2.1 Scribble-Supervised Learning
To address the challenges posed by sparse-signal scribble supervision, we utilize
a modified CrossEntropy (CE) function that concentrates solely on the anno-
tated pixels while ignoring the unlabeled ones. This approach leads to a form
of partially supervised segmentation loss. Specifically, we introduce the Partial
Cross-Entropy (pCE) [23], which leverages only the scribble annotations during
the training of the networks, denoted as L . This is expressed in Equation 2
pce
as follows:
(cid:88) (cid:88)
L =− y [i,k]log(y [i,k]), (2)
pce s p
i∈ΩL k
where i denotes the index of a given pixel, and Ω represents the set of
L
pixelsannotatedwithscribbles.Thevariablek indicatestheclassindex(4inthis
study),andy [i,k]andy [i,k]denotethegroundtruthandpredictedprobability
s p
ofa network, respectively, ofthe i-thpixel belongingto thek-thclass.The L
pce
is utilized for all three networks f (X;θ), f (X;θ), and f (X;θ), and
cnn vit mamba
denoted as Li where i∈[1,2,3],
pce
2.2 Multi-View Cross-Supervised Learning
InspiredbyCrossPseudoSupervision(CPS)[4],CrossTeaching[17],andMulti-
view Learning [32], which are designed to facilitate consistency regularization
underdifferentnetworkperturbations,ourproposedmulti-viewcross-supervisedWeak-Mamba-UNet for Medical Image Segmentation 5
learningframeworkintegratesMamba-UNet[37]withtheoriginalUNet[21]and
Swin UNet [3]. Each network follows a U-shaped encoder-decoder architecture.
Specifically,UNetemploysa2-layerCNNwith3×3kernels[21]andperforms4
levelsofdownsamplingandupsampling.Swin-UNetutilizes2SwinTransformer
blocks [3], and Mamba-UNet incorporates 2 Visual Mamba blocks [22,37]. Both
SwinUNet and MambaUNet perform 3 levels of downsampling and upsampling
andarepretrainedonImageNet[5].Thissetupintroducesthreedistinctarchitec-
tural perspectives, each initialized separately to ensure diversity in viewpoints.
To foster mutual enhancement among the networks, a composite pseudo label
Y isformulatedtoconvertsparse-labelinformationintodensesignallabels,
pseudo
as shown in the equation below:
Y =α×f (X;θ)+β×f (X;θ)+γ×f (X;θ), (3)
pseudo cnn vit mamba
where α, β, and γ are weighting factors that balance the contributions from
the CNN-based UNet, ViT-based SwinUNet, and Mamba-based MambaUNet,
respectively. These factors are randomly generated in each iteration, and fol-
lowing α+β+γ = 1, introducing an element of data perturbation inspired by
[27,15]. This approach ensures a diverse integration of perspectives from each
network, enhancing the robustness and generalizability of the generated pseudo
labels.Oncepseudolabelprovided,thedense-signalsupervisioncanbeachieved
by Dice-Coefficient-based loss L illustrated as
dice
(cid:0) (cid:1)
L =Dice argmax(f(X;θ),Y ) (4)
dice pseudo
TheL isutilizedforallthreenetworksf (X;θ),f (X;θ),andf (X;θ),
dice cnn vit mamba
and denoted as Li where i∈[1,2,3].
dice
3 Experiments
Datasets: The performance of Weak-Mamba-UNet, as well as various baseline
methods, were evaluated using a publicly available MRI cardiac segmentation
dataset [1]. Scribble annotations were derived from the original dense annota-
tions, in line with previous studies [25]. All images were resized to a uniform
resolution of 224×224 pixels for consistency in the evaluation process. The ex-
periments were conducted on an Ubuntu 20.04 system equipped with an Nvidia
GeForceRTX3090GPUandanIntelCorei9-10900KCPU,usingPyTorch.The
entire experimental run took an average of 4 hours. We trained Weak-Mamba-
UNet with all other baseline methods for 30,000 iterations with a batch size of
24. Optimization was performed using Stochastic Gradient Descent (SGD) [2],
with an initial learning rate of 0.01, momentum set to 0.9, and weight decay at
0.0001. The networks were evaluated on the validation set every 200 iterations,
saving the network weights only when the validation performance improved.
Baseline Segmentation Networks and WSL Frameworks:Theframe-
work of Weak-Mamba-UNet is depicted in Figure 2 with three segmentation
backbone networks. To ensure equitable comparisons, we also employed the6 Z Wang, C Ma
Fig.3.TheExampleSegmentationResultswhen5%ofDataareAssumedasLabeled
Data.Weak-Mamba-UNet for Medical Image Segmentation 7
CNN-based UNet [21] and the Swin ViT-based SwinUNet [3] as segmentation
backbonenetworksfordifferentWSLframeworks.TheWSLbaselineframeworks
evaluated includes partial Cross Entropy(pCE) [24], Uncertainty-aware Self-
ensembling and Transformation-consistent Mean Teacher Model(USTM) [40],
Mumford [27], Gated Conditional Random Field(Gated CRF) [28]. Both Swin-
UNet [3] and UNet [21] were employed as the segmentation backbone networks
across these frameworks.
Table 1. Direct Comparison of Weak-supervised Frameworks on MRI Cardiac Test
Set.
Framework+Network Dice↑ Acc↑ Pre↑ Sen↑ Spe↑ HD↓ ASD↓
pCE [23] + UNet 0.76200.98070.67990.91740.9823151.059354.6531
USTM [13] + UNet 0.85920.99170.81280.92570.9888 99.8293 26.0185
Mumford [11] + UNet 0.89930.99500.88440.92000.9874 28.0604 7.3907
Gated CRF [20] + UNet 0.90460.99550.88900.93040.9922 7.4340 2.0753
pCE [23] + SwinUNet 0.89350.99500.88080.91290.9884 24.4750 6.9108
USTM [13] + SwinUNet 0.90440.99570.89520.91870.9898 6.5172 2.2319
Mumford [11] + SwinUNet 0.90510.99580.89960.91570.9889 6.0653 1.6482
Gated CRF [20] + SwinUNet0.89950.99550.89200.91750.9904 6.6559 1.6222
Weak-Mamba-UNet 0.91710.99630.90950.93090.9920 3.9597 0.8810
Results: To evaluate the performance of Weak-Mamba-UNet relative to
other WSL baseline methods, we employed a set of comprehensive evaluation
metrics.Forsimilaritymeasures,wherehighervaluesindicatebetterperformance
(↑), we included the Dice Coefficient (Dice), Accuracy (Acc), Precision (Pre),
Sensitivity(Sen),andSpecificity(Spe).Fordifferencemeasures,wherelowerval-
ues are preferable (↓), we considered the 95% Hausdorff Distance (HD) and Av-
erageSurfaceDistance(ASD).Giventhedataset’sfocuson4-classsegmentation
tasks,wereportthemeanvaluesofthesemetricsacrossallclasses.Theresultsof
ourquantitativecomparisonontheACDCdatasetaredetailedinTable1,high-
lighting several key observations with the best-performing results underscored.
Notably, WSL methods employing the SwinUNet architecture (pCE-SwinUNet
and USTM-SwinUNet) generally surpass those based on the UNet framework
(pCE-UNetandUSTM-UNet).Forinstance,pCE-SwinUNetexceedspCE-UNet
in DSC and HD with scores of 0.7620 and 54.6531, respectively, underscoring
the significance of employing advanced algorithms within the WSL framework.
However, an optimized integration of multiple independent algorithms, as ex-
hibitedbyWeak-Mamba-UNet,canyieldevenmoreimpressiveresults.Figure3
showcasestheefficacyofourproposedmethodthroughthreeillustrativesample
slices alongside their actual labels. These examples demonstrate how conven-
tional pCE and USTM frameworks may lead to erroneous predictions, whereas
our novel multi-model combination approach effectively addresses these issues,
achieving superior segmentation outcomes.8 Z Wang, C Ma
Table 2.AblationStudiesonDifferentCombinationsofSegmentationBackboneNet-
works with the Same WSL Framework.
Network Dice↑ Acc↑ Pre↑ Sen↑ Spe↑ HD↓ ASD↓
3×UNet 0.91410.99590.89580.93830.9927 8.0566 2.8806
3×SwinUNet 0.74460.97910.65550.91420.9815121.422451.4317
3×MambaUNet 0.91280.99580.89310.93950.9932 8.3386 2.7928
UNet+SwinUNet+MambaUNet(Ours)0.91710.99630.90950.93090.9920 3.9597 0.8810
Ablation Study: The ablation studies presented in Table 2 illustrates the
contributionsoftheproposedWSLframeworkwithdifferentcombinationsofseg-
mentationbackbonenetworks.AscanbeseenfromTable2,theWSLframework
consistingofSwinUNetperformslesswell,whichindicatesthatalthoughtheper-
formance of the independent SwinUNet algorithm is able to outperform that of
UNet,thereisalackofdifferentiationbetweentheMulti-SwinUNetmodels.Itis
worthnotingthatMamba-UNetcanenhancethefeaturediversityamongmulti-
ple Mamba-UNet models by learning feature dependencies over longer distances
to show excellent performance. Finally, our proposed WSL framework achieves
optimal results on most of the segmentation metrics, which demonstrates that
multiple independent algorithms of different types can complement each other
with different levels of feature information to enhance the segmentation perfor-
mance of the networks.
4 Conclusion
Weak-Mamba-UNet, by integrating the feature learning capabilities of CNN,
ViT,andVMambawithinascribble-supervisedlearningframework,significantly
reduces the costs and resources required for annotations. The multi-view cross-
superviselearningapproachemployedenhancestheadaptabilityofdifferentnet-
workarchitectures,enablingthemtomutuallybenefitfromeachother.Crucially,
this study demonstrates the effectiveness of the novel Visual Mamba network
architecture in medical image segmentation under limited signal supervision.
The promising outcomes of this research not only highlight the network’s high
accuracy in segmentation tasks but also underscore the potential for broader
applications in medical image analysis, particularly in settings where resources
are limited.
References
1. Bernard, O., et al.: Deep learning techniques for automatic mri cardiac multi-
structures segmentation and diagnosis: is the problem solved? IEEE transactions
on medical imaging 37(11), 2514–2525 (2018)
2. Bottou, L.: Stochastic gradient learning in neural networks. In: Proceedings of
Neuro-Nîmes 91. EC2, Nimes, France (1991)Weak-Mamba-UNet for Medical Image Segmentation 9
3. Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swin-
unet: Unet-like pure transformer for medical image segmentation. In: European
conference on computer vision. pp. 205–218. Springer (2022)
4. Chen, X., et al.: Semi-supervised semantic segmentation with cross pseudo super-
vision. In: CVPR (2021)
5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009)
6. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020)
7. Gu, A.: Modeling Sequences with Structured State Spaces. Ph.D. thesis, Stanford
University (2023)
8. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. arXiv preprint arXiv:2312.00752 (2023)
9. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,
Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In:
Proceedings of the IEEE/CVF winter conference on applications of computer vi-
sion. pp. 574–584 (2022)
10. Jiao, R., Zhang, Y., Ding, L., Xue, B., Zhang, J., Cai, R., Jin, C.: Learning with
limitedannotations:Asurveyondeepsemi-supervisedlearningformedicalimage
segmentation. Computers in Biology and Medicine (2023)
11. Kim,B.,Ye,J.C.:Mumford–shahlossfunctionalforimagesegmentationwithdeep
learning. IEEE Transactions on Image Processing 29, 1856–1866 (2019)
12. Li,Z.,Zheng,Y.,Shan,D.,Yang,S.,Li,Q.,Wang,B.,Zhang,Y.,Hong,Q.,Shen,
D.: Scribformer: Transformer makes cnn work better for scribble-based medical
image segmentation. IEEE Transactions on Medical Imaging (2024)
13. Liu,X.,Yuan,Q.,Gao,Y.,He,K.,Wang,S.,Tang,X.,Tang,J.,Shen,D.:Weakly
supervisedsegmentationofcovid19infectionwithscribbleannotationonctimages.
Pattern recognition 122, 108341 (2022)
14. Liu, Z., Lin, Y., et al.: Swin transformer: Hierarchical vision transformer using
shifted windows. arXiv preprint arXiv:2103.14030 (2021)
15. Luo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., Zhang, S.: Scribble-
supervised medical image segmentation via dual-branch network and dynamically
mixed pseudo labels supervision. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. pp. 528–538. Springer (2022)
16. Luo, X., et al.: Semi-supervised medical image segmentation via cross teaching
between cnn and transformer. arXiv preprint arXiv:2112.04894 (2021)
17. Luo, X., et al.: Semi-supervised medical image segmentation via cross teaching
between cnn and transformer. In: MIDL (2022)
18. Ma,J.,Li,F.,Wang,B.:U-mamba:Enhancinglong-rangedependencyforbiomed-
ical image segmentation. arXiv preprint arXiv:2401.04722 (2024)
19. Milletari,F.,Navab,N.,Ahmadi,S.A.:V-net:Fullyconvolutionalneuralnetworks
for volumetric medical image segmentation. In: 2016 fourth international confer-
ence on 3D vision (3DV). pp. 565–571. IEEE (2016)
20. Obukhov, A., et al.: Gated crf loss for weakly supervised semantic image segmen-
tation. arXiv preprint arXiv:1906.04651 (2019)
21. Ronneberger, O., et al.: U-Net: Convolutional networks for biomedical image seg-
mentation.In:IntConfMedImComp&Comp-AssistedIntervention.pp.234–241.
Springer (2015)10 Z Wang, C Ma
22. Ruan,J.,Xiang,S.:Vm-unet:Visionmambaunetformedicalimagesegmentation.
arXiv preprint arXiv:2402.02491 (2024)
23. Tang,M.,Djelouah,A.,Perazzi,F.,Boykov,Y.,Schroers,C.:Normalizedcutloss
for weakly-supervised cnn segmentation. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 1818–1827 (2018)
24. Tarvainen,A.,Valpola,H.:Meanteachersarebetterrolemodels:Weight-averaged
consistencytargetsimprovesemi-superviseddeeplearningresults.In:Proceedings
of the 31st International Conference on Neural Information Processing Systems.
pp. 1195–1204 (2017)
25. Valvano, G., Leo, A., Tsaftaris, S.A.: Learning to segment from scribbles using
multi-scale adversarial attention gates. IEEE Transactions on Medical Imaging
40(8), 1990–2001 (2021)
26. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. In: Advances in neural information
processing systems. pp. 5998–6008 (2017)
27. Verma, V., Lamb, A., Kannala, J., Bengio, Y., Lopez-Paz, D.: Interpolation con-
sistency training for semi-supervised learning. In: International Joint Conference
on Artificial Intelligence. pp. 3635–3641 (2019)
28. Vu, T.H., Jain, H., Bucher, M., Cord, M., Pérez, P.: Advent: Adversarial entropy
minimization for domain adaptation in semantic segmentation. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2517–2526 (2019)
29. Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., Hamid, R.: Selective
structured state-spaces for long-form video understanding. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6387–
6397 (2023)
30. Wang,Z.,Ma,C.:Semi-mamba-unet:Pixel-levelcontrastivecross-supervisedvisual
mamba-basedunetforsemi-supervisedmedicalimagesegmentation.arXivpreprint
arXiv:2402.07245 (2024)
31. Wang, Z., Ma, C.: Dual-contrastive dual-consistency dual-transformer: A semi-
supervised approach to medical image segmentation. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 870–879 (2023)
32. Wang, Z., Voiculescu, I.: Triple-view feature learning for medical image segmen-
tation. In: MICCAI Workshop on Resource-Efficient Medical Image Analysis. pp.
42–54. Springer (2022)
33. Wang, Z., Voiculescu, I.: Exigent examiner and mean teacher: An advanced 3d
cnn-based semi-supervised brain tumor segmentation framework. In: Workshop
on Medical Image Learning with Limited and Noisy Data. pp. 181–190. Springer
(2023)
34. Wang, Z., Voiculescu, I.: Weakly supervised medical image segmentation through
dense combinations of dense pseudo-labels. In: MICCAI Workshop on Data Engi-
neering in Medical Imaging. pp. 1–10. Springer (2023)
35. Wang,Z.,Zhang,H.,Liu,Y.:Weakly-supervisedself-ensemblingvisiontransformer
for mri cardiac segmentation. In: 2023 IEEE Conference on Artificial Intelligence
(CAI). pp. 101–102. IEEE (2023)
36. Wang, Z., et al.: Rar-u-net: a residual encoder to attention decoder by residual
connections framework for spine segmentation under noisy labels. In: 2021 IEEE
International Conference on Image Processing (ICIP). IEEE (2021)
37. Wang, Z., et al.: Mamba-unet: Unet-like pure visual mamba for medical image
segmentation. arXiv preprint arXiv:2402.05079 (2024)Weak-Mamba-UNet for Medical Image Segmentation 11
38. Xing,Z.,Ye,T.,Yang,Y.,Liu,G.,Zhu,L.:Segmamba:Long-rangesequentialmod-
elingmambafor3dmedicalimagesegmentation.arXivpreprintarXiv:2401.13560
(2024)
39. Yan, X., et al.: After-unet: Axial fusion transformer unet for medical image seg-
mentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. pp. 3971–3981 (2022)
40. Zhang, Y., Yang, L., Chen, J., Fredericksen, M., Hughes, D.P., Chen, D.Z.: Deep
adversarialnetworksforbiomedicalimagesegmentationutilizingunannotatedim-
ages. In: International conference on medical image computing and computer-
assisted intervention. pp. 408–416. Springer (2017)
41. Zhang,Z.,Li,S.,Wang,Z.,Lu,Y.:Anovelandefficienttumordetectionframework
forpancreaticcancerviactimages.In:202042ndAnnualInternationalConference
of the IEEE Engineering in Medicine & Biology Society (EMBC). pp. 1160–1164.
IEEE (2020)
42. Zhou, H.Y., Guo, J., Zhang, Y., Han, X., Yu, L., Wang, L., Yu, Y.: nnformer:
Volumetric medical image segmentation via a 3d transformer. IEEE Transactions
on Image Processing (2023)
43. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net
architecture for medical image segmentation. In: Deep Learning in Medical Im-
age Analysis and Multimodal Learning for Clinical Decision Support, pp. 3–11.
Springer (2018)
44. Zhu,L.,Liao,B.,Zhang,Q.,Wang,X.,Liu,W.,Wang,X.:Visionmamba:Efficient
visualrepresentationlearningwithbidirectionalstatespacemodel.arXivpreprint
arXiv:2401.09417 (2024)