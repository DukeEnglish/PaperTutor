TRADING OFF CONSISTENCY AND DIMENSIONALITY OF
CONVEX SURROGATES FOR THE MODE
EnriqueNueveennu6440@colorado.edu,UniversityofColoradoBoulder
BoWaggonerbwag@colorado.edu,UniversityofColoradoBoulder
DhammaKimparadhamma.kimpara@colorado.edu,UniversityofColoradoBoulder
JessieFinocchiarojessie@seas.harvard.edu,CRCS,HarvardUniversity
ABSTRACT
In multiclass classification over n outcomes, the outcomes must be embedded into the reals with
dimension at least n−1 in order to design a consistent surrogate loss that leads to the “correct”
classification, regardless of the data distribution. For large n, such as in information retrieval and
structured prediction tasks, optimizing a surrogate in n − 1 dimensions is often intractable. We
investigate ways to trade off surrogate loss dimension, the number of problem instances, and re-
strictingtheregionofconsistencyinthesimplexformulticlassclassification. Followingpastwork,
weexamineanintuitiveembeddingprocedurethatmapsoutcomesintotheverticesofconvexpoly-
topesinalow-dimensionalsurrogatespace. Weshowthatfull-dimensionalsubsetsofthesimplex
existaroundeachpointmassdistributionforwhichconsistencyholds,butalso,withlessthann−1
dimensions,thereexistdistributionsforwhichaphenomenoncalledhallucinationoccurs,whichis
whentheoptimalreportunderthesurrogatelossisanoutcomewithzeroprobability. Lookingto-
wardsapplication,wederivearesulttocheckifconsistencyholdsunderagivenpolytopeembedding
andlow-noiseassumption,providinginsightintowhentouseaparticularembedding. Weprovide
examples of embedding n = 2d outcomes into the d-dimensional unit cube and n = d! outcomes
into the d-dimensional permutahedron under low-noise assumptions. Finally, we demonstrate that
withmultipleprobleminstances,wecanlearnthemodewith n dimensionsoverthewholesimplex.
2
1 Introduction
Forgeneralpredictiontasksinmachinelearning,wecantypicallyeasilymodelthepredictiontaskwithsomediscrete
targetloss. However,minimizingthisdiscretetargetisalmostalwayscomputationallyintractable. Instead,weopti-
mizeasurrogateloss,whichhasnicepropertiessuchascontinuityandconvexity,resultinginaneasiercomputational
problem. However, to ensure these surrogates properly “correspond” to the original prediction problem, we seek to
designconsistent surrogates. Consistencyofasurrogateensuresthatwelearnthesamemodelorestimatethesame
statisticbyminimizingthesurrogateaswewouldifwecouldhavedirectlyminimizedthediscreteloss.
However, an easier computational problem may still not be a feasible one, as a large dimension d of the surrogate
loss L : Rd ×Y → R can make gradient-based optimization intractable. Universal upper bounds show that any
prediction task has a convex and consistent surrogate with dimension n − 1, where n is the number of outcomes
[7,13]. Futhermore,thereareknownlowerboundsonthesurrogatedimension[8,9,13]. Notably,Ramaswamyand
Agarwal[13]showthatforcommonpredictiontaskssuchasmulticlassclassificationandranking,theuniversaln−1
dimensional bound on surrogate dimension is tight. If n is large, then optimizing a consistent and convex surrogate
lossmaystillbeinfeasible. Weexaminecertainpracticalcompromisesthatreducethedimensionofthesurrogateloss
formulticlassclassification.
4202
beF
61
]GL.sc[
1v81801.2042:viXraTradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
We relax the requirement that a surrogate loss must be consistent for every distribution over the labels. Agarwal
andAgarwal[1]showthatconsistentsurrogatesformulticlassclassificationinlog(n)dimensionscanbeconstructed
underlow-noiseassumptions. However,theirworkdoesnotstudywhetherthisreductionisnecessary,orhowitmight
be mitigated, nor do they show results for convex surrogates. Using different techniques, we seek to understand the
tradeoffsofconsistency,surrogatepredictiondimension,andnumberofprobleminstancesthroughtheuseofpolytope
embeddings.
We first show that regardless of the number of outcomes embedded into a lower-dimensional polytope, a full-
dimensional set of distributions always exists for which consistency holds. However, we also show there always
existsasetofdistributionswherehallucinationsoccur: wheretheminimizingreportisanoutcomeinwhichtheun-
derlyingtruedistributionhasnoweightonsaidreport. Followingthis,weshowthateverypolytopeembeddingleads
to consistency if one assumes a low-noise assumption. Finally, we demonstrate through leveraging the embedding
structureandmultipleprobleminstancesthatthemode(inparticular,afullrankordering)overnoutcomesembedded
intoa n dimensionalsurrogatespaceiselicitableoveralldistributionsviaO(n2)probleminstances.
2
2 Background
2.1 Notation
Let Y be a finite label space, and throughout let n = |Y|. Define RY to be the nonnegative orthant. Let ∆ =
+ Y
{p ∈ RY | ||p|| = 1}bethesetofprobabilitydistributionsonY,representedasvectors. Wedenotethepointmass
+ 1
distribution of an outcome y ∈ Y by δ ∈ ∆ . Let [d] := {1,...,d}. For any S ⊆ [d], we let 1 ∈ {0,1}d with
y Y S
(1 ) = 1 ⇐⇒ i ∈ S be the 0-1 indicator for S. For ϵ > 0, we define an epsilon ball via B (u) = {u ∈ Rd |
S i ϵ
||u−x|| <ϵ}andB :=B (⃗0). ForA⊆Rdandu∈Rd,letA+u:={a+u|a∈A}betheMinkowskisum. For
2 ϵ ϵ
A⊆Rd,theinteriorisint(A):={u|∃ϵ>0,B (u)⊂A}andtheclosureiscl(A):=∩{A+B (u)|ϵ>0,u∈A}.
ϵ ϵ
TheboundaryofasetA⊂Rdisbd(A):=cl(A)\int(A). GivenaclosedconvexsetC ⊂Rd,wedefineaprojection
operationontoC viaProj (u) := argmin ||u−x|| . Ingeneral,wedenoteadiscretelossbyℓ : R×Y → R
C x∈C 2 +
with reports denoted by r ∈ R and outcomes by y ∈ Y and a surrogate loss by L : Rd ×Y → R with surrogate
reportsu∈Rd andoutcomesy ∈Y. Thesurrogatemustbeaccompaniedbyalinkψ :Rd →Rmappingtheconvex
surrogate model’s predictions back into the discrete target space, and we discuss consistency of a pair (L,ψ) with
respecttothetargetℓ.
2.2 Polytopes
AConvexPolytopeP ⊂ Rd,orsimplyapolytope,istheconvexhullofafinitenumberofpointsu ,...,u ∈ Rd.
1 n
AnextremepointofaconvexsetA,isapointu∈A,withthepropertythatifu=λy+(1−λ)zwithy,z ∈Aand
λ ∈ [0,1],theny = uand/orz = u. Weshalldenotebyvert(P)asapolytope’ssetofextremepoints. Conversely,a
polytopecanbeexpressedbytheconvexhullofitsextremepoints,i.e. P =conv(vert(P))[6,Theorem7.2].
We define the dimension of P via dim(P) := dim(affhull(P)) where affhull(P) denotes the smallest affine set
containingP. AsetF ⊆ P isafaceofP isthereexistsahyperplaneH(y,α) := {u ∈ Rd | ⟨u,y⟩ = α}suchthat
F =P ∩H andP ⊆H+ suchthatH+(y,α):={u∈Rd |⟨u,y⟩≤α}. LetF (P)wherei∈[d−1]denotesetof
i
facesofdimiofapolytopeP.Afaceofdimensionzeroiscalledavertexandafaceofdimensiononeiscalledanedge.
WedefinetheedgesetofapolytopeP byE(P):={conv((v ,v ))|(v ,v
)⊆(cid:0)vert(P)(cid:1)
,conv((v ,v ))∈F (P)}.
i j i j 2 i j 1
We define the neighbors of a vertex v by ne(v;P) := {vˆ ∈ vert(P) | conv((v,vˆ)) ∈ E(P)}. We will denote
conv((v,vˆ))∈E(P)byase andne(v;P)byne(v).
v,vˆ
2.3 PropertyElicitation,Calibration,andPredictionDimension
Throughout,weexaminetheconsistencyofconvexsurrogatelossesthroughthesufficientconditionofconvexproperty
elicitation. Foramorethoroughdiscussionontherelationshipbetweenpropertyelicitationandconsistency,werefer
thereaderto[8]. Propertyelicitationcharacterizestheminimizingreportsofaprovidedlossfunction,ratherthanthe
infimumoftheexpectedloss,asincalibrationandconsistency.
Definition 1 (Property, Elicits, Level Set). For P ⊆ ∆ , a property is a set-valued function Γ : P → 2R \{∅},
Y
whichwedenoteΓ:P ⇒R. AlossL:R×Y →RelicitsthepropertyΓonP if
∀p∈P, Γ(p)=argminE [L(u,Y)].
Y∼p
u∈R
2TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
IfLelicitsaproperty,itisuniqueandwedenoteitprop[L]. ThelevelsetofΓforreportristhesetΓ := {p ∈ P |
r
r =Γ(p)}. Ifprop[L]=Γand|Γ(p)|=1forallp∈P,wesaythatLisstrictlyproperforΓ.
Toconnectpropertyelicitationtostatisticalconsistency,weworkthroughthenotionofcalibration,whichisequivalent
toconsistencyinthefiniteoutcomesetting[3,13,19]. Onedesirablecharacteristicofcalibrationoverconsistencyis
theabilitytoabstractfeaturesx∈X.Meaning,givendatadrawnfromadistributionDoverX×Y,aspaceoffeatures
andlabels,wecanevaluateforconsistencyviacalibrationwhichonlyrequiresexaminingp:=D =Pr[Y|X =x]∈
x
∆ .
Y
Definition2(ℓ-CalibratedLoss). Letdiscretelossℓ:R×Y →R ,proposedsurrogatelossL:Rd×Y →R,and
+
linkfunctionψ :Rd →Rbegiven. Wesaythat(L,ψ)isℓ-calibratedoverP if,forallp∈P ⊆∆ ,
Y
inf E [L(u,Y)]> inf E [L(u,Y)].
Y∼p Y∼p
u∈Rd:ψ(u)∈/prop[ℓ](p) u∈Rd
IfP isnotspecified,thenwearediscussingcalibrationover∆ .
Y
Thedefinitionsofconsistencyandcalibrationrelycruciallyontheexistenceofalinkfunctionψ :Rd →Rthatmaps
surrogate reports to the discrete target prediction space. Proving a pair (L,ψ) is ℓ-calibrated is often challenging.
Fortunately,[1],showthatconvergingtoapropertyvalueimpliesshowcalibrationforthetargetlossitself.
Definition3(ℓ-CalibratedProperty). LetP ⊆ ∆ ,Γ : P ⇒ Rd,discretelossℓ : R×Y → R ,andψ : Rd → R.
Y +
Wewillsay(Γ,ψ)isℓ-calibratedforallp∈P andallsequencesin{u }inRd,
m
u →Γ(p)⇒E [ℓ(ψ(u ),Y)]→minE [ℓ(r,Y)].
m Y∼p m Y∼p
r∈R
Therefore,if(Γ,ψ)isℓ-calibrated,thenitholdsforallp∈P,ψ(Γ(p))∈prop[ℓ](p).
Theorem 1 (ℓ-Calibrated loss via Elicitable ℓ-Calibrated properties). [1, Theorem 3] Let ℓ : R × Y → R and
+
P ⊆∆ . LetΓ:P ⇒Rdandψ :Rd →RbesuchthatΓisdirectlyelicitableand(Γ,ψ)isℓ-calibratedoverP. Let
Y
L:Rd×Y →RbestrictlyproperforΓi.e. prop[L]=Γand|Γ(p)|=1forallp∈P. Then,(L,ψ)isℓ-calibrated
overP.
Finally,wepresentthe0-1lossthatweanalyze,whichisthetargetlossformulticlassclassification.
Definition4(0-1Loss). Wedenotethe0-1lossbyℓ :Y ×Y →{0,1}suchthatℓ (y,yˆ):=1 .
0−1 0−1 y̸=yˆ
Observeγmode(p):=prop[ℓ ](p)={y ∈Y|y ∈mode(p)}wheremode(p)=argmax p .
0−1 y y
3 PolytopeEmbeddingandExistenceofCalibratedRegions
Often,discreteoutcomesareembeddedintopointsincontinuousspaceviaone-hotencodingontounitvectorsorthe
verticesoftheunitcubeviabinaryencoding. Inthissection,wefirstintroducethepolytopeembeddingsφinspiredby
Blondeletal.[5],Wainwrightetal.[17],whichnaturallyinduceaclassoflossfunctionsLG andlinkfunctionsψP.
φ
With these natural surrogate and link pairs, we introduce the notion of hallucination which is in a sense the ‘worst
case’behaviorofasurrogatepair(§3.2). Finally,weintroducecalibrationregions(§3.3),whicharesetsP ⊆ ∆
Y
suchthatoursurrogateandlinkpair(LG,ψP)areℓ-calibratedoverP.
φ
3.1 PolytopeEmbedding
FollowinginspirationfromworkpertainingtoMarginalPolytopes[5,17],weproposethefollowingembeddingpro-
cedure.
Construction1(PolytopeEmbedding). GivenY outcomessuchthat|Y| = n,chooseapolytopeP ⊂ Rd suchthat
|vert(P)| = nandd < n−1. ChooseabijectionbetweenY andvert(P). Accordingtothisbijection,assigneach
vertexauniqueoutcome: {v |y ∈Y}=vert(P). Thenthepolytopeembeddingφ:∆ →P is
y Y
(cid:88)
φ(p):= p v .
y y
y∈Y
Weproposethefollowingapproachtotieouranalysisofpolytopeembeddingstominimizinglossfunctions.Following
theworkof[4]andtheirproposedProjection-basedlosses,weuseaBregmandivergences(Definition5)andapolytope
embeddingφtodefineaninducedlossLGasshowninDefinition6.
φ
3TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Definition5(BregmanDivergence). GivenastrictlyconvexfunctionG:Rd →R,
D (u,v):=G(v)−[G(u)+⟨dG ,u−v⟩]
G v
is a Bregman divergence where dG denotes the gradient of G at v. For this work, we shall always assume that
v
dom(G)=Rd.
Definition6((D ,φ)InducedLoss). GivenaBregmandivergenceD andapolytopeembeddingφ,wesay(D ,φ)
G G G
inducesalossLG :Rd×Y →Rdefinedas
φ
LG(u,y):=D (u,v )=G(v )−[G(u)+⟨dG ,u−v ⟩].
φ G y y vy y
Weshowthatforanyp∈∆ ,thereportthatuniquelyminimizestheexpectationofthelossLGisφ(p),theembedding
Y φ
pointofp. Furthermore,thepolytopeP containsstrictlyalloftheminimizingreportsinexpectationunderLG. We
φ
referthereadertotheAppendixAforaproofofProposition2.
Proposition2. ForagiveninducedlossLG,theuniquereportwhichminimizestheexpectedlossis
φ
u∗ :=argminE [LG(u,Y)]=φ(p)
Y∼p φ
u∈Rd
suchthatu∗ ∈P. Furthermore,everyuˆ∈P isaminimizerofE [LG(u,Y)]forsomepˆ∈∆ .
Y∼pˆ φ Y
WenowdefinetheMAPLink,whichwillbeusedinconjunctionwithaninducedlossLGtoformasurrogatepairfor
φ
the0-1loss.TheMAPLinkisamaximumaposterioriinferencetaskwhichisseenoftenthroughoutmachinelearning
[4,16,18].
Definition7(MAPLink). Letφbeapolytopeembedding. TheMAPlinkψP :Rd →Y isdefinedas
ψP(u)=argmin||Proj (u)−v || . (1)
P y 2
y∈Y
ThelevelsetofthelinkforyisψP ={u∈Rd|y =ψP(u)}. Webreaktiesarbitrarilybyapre-defineddeterministic
y
policy.
3.2 HallucinationRegions
Since our polytope embedding violates surrogate dimension bounds, calibration for 0-1 loss will not hold for all
distributions. In particular, we show there always exists some adversarial distributions for which LG is minimized
φ
at u and ψP(u) = y but, the true underlying distribution has no weight at all on outcome y. These cases yields
inconsistency in a “worst” case manner where the reported outcome could never actually occur with respect to our
embeddingofneventsviaφintovert(P).AsLargeLanguageModelsoftengeneratedishonestresponsestoquestions
attimes[12],wenamethisactofreportinganoutcomewithnoprobabilitytobeahallucination.
Definition8(Hallucination). Givenadiscretetargetlossℓ:Y ×Y →R over|Y|=noutcomeswhichelicitsγ =
+
prop[ℓ],(L,ψ)whereψissomelinkfunction,Lisind-dimensionswhered<n,andanembeddingfunctionφ:Y →
Rd,wesaythatahallucinationoccursatasurrogatereportu ∈ Rd if,forsomep,u ∈ argmin E [L(uˆ,Y)]
uˆ∈Rd Y∼p
and ψ(u) = r but p = 0. We denote by H ⊆ P ⊂ Rd as the hallucination region as the elements of P at which
r
hallucinationscanoccur.
Weexpressthesubspaceofthesurrogatespacewherehallucinationscanoccurasthehallucinationregiondenotedby
H. InTheorem3,wecharacterizethehallucinationregionforanypolytopeembeddingwhileusingthesurrogatepair
(LG,ψP)andshowthatHisneverempty.
φ
Theorem3. Withrespecttoℓ ,foranygivenpair(LG,ψP)suchthatn−1>d;itholdsthat
0−1 φ
H=∪ conv(vert(P)\{v })∩ψP
y∈Y y y
andfurthermoreH̸=∅.
Wesketchtheproof,leavingacompleteproofinAppendixA.
Proof. Fixy ∈Y.Weabusenotationandwritevert(P)\v :=vert(P)\{v }.Observeconv(vert(P)\v )∩ψP ⊆H
y y y y
since any point in this set can be expressed as a convex combination without needing vertex v implying there is a
y
4TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Figure 1: (Left) Mode level sets of ∆ where Y = {a,b,c,d} embedded into a two dimensional unit cube. The
Y
center red point denotes the origin (0,0) which is the hallucination region. (Right) An embedding of ∆ where
Y
Y ={a,b,c,d,e,f}intoathree-dimensionalpermutahedron:thebeigeregionexpressesstrictcalibrationregions,the
lightpinkregionsexpressesregionswithinconsistency,andtheauburnregionexpressesregionswithhallucinations.
distributionembeddedbyφtosaidpointwhichhasnoweightony.ToshowthatH⊆∪ conv(vert(P)\v )∩ψP.
y∈Y y y
Assumethereexistsapointu∈/ conv(vert(P)\v )∩ψP suchthatthereexistssomep∈∆ whereφ(p)=u,p =0,
y y Y y
andψP(u)=y.SinceψP(u)=yandu∈/ conv(vert(P)\v ))∩ψP,itmustbethecasethatu∈/ conv(vert(P)\v ).
y y y
However,thatimpliesthatuisstrictlyinthevertexfigureandthusmusthaveweightonthecoefficientfory. Thus,
formingacontradictionthatp = 0whichimpliesthatH ⊆ ∪ conv(vert(P)\v )∩ψP. Finally,usingHelly’s
y y∈Y y y
Theorem[14,Corollary21.3.2],weareabletoshowthenon-emptinessofH.
Thepossibilityforhallucinationsmayexistfortargetlossessolelybyhowtheyaredefined,regardlessifacalibrated
surrogatelinkpairisused. Weconjecturethathallucinationsarepossibleformanystructuredpredictionlosses,and
referthereadertoAppendixBforanexampleofthiswiththeHammingloss.
3.3 CalibrationRegions
Observethelevelsetsofγmode arepolytopesandarefinitelymany. Weobservethatthemode’sembeddedlevelsets
inthepolytopeoverlap,whichisunsurprisinggiventhatweareviolatingthelowerboundsonsurrogatepredictionfor
themode. Since|2Y\{∅}|isafiniteset,weknowthatthenumberofuniquemodelevelsetsembeddedtoanyu∈P
is finite. Although every point in the polytope is a minimizing report for some distribution, if multiple distributions
withnon-intersectingmodesetsareembeddedtothesamepoint,thereisnowaytodefinealinkfunctionthatiscorrect
in all cases. However, if the union1 of mode sets for the p’s mapped to any u ∈ P is a singleton, regardless of the
underlyingdistribution,alinkψwouldbecalibratedovertheunionifitmappedutothementionedsingleton. Given
(L,ψ)andφ,andatargetlossℓ,wedefinestrictcalibratedregionsasthepointsforwhichcalibrationholdsregardless
oftheactualdistributionrealized,whicharepossibleatsaidpoints.
Definition9(StrictCalibratedRegion). Given(L,ψ)andφ,fixatargetlossℓ. WesayR ⊆ P isastrictcalibrated
regionvia(L,ψ)withrespecttoℓif(L,ψ)isℓ-calibratedforallp ∈ φ−1(R). Foranyy ∈ Y, weshalldenoteby
R ⊆R∩ψ . WeletR :=∪ R .
y y Y y∈Y y
In Theorem 4, we show that the strict calibration region is never empty and that (LG,ψ) is calibrated for the 0-1
φ
loss over all distributions embedded into the strict calibration region. We show for each v ∈ vert(P) there exists
y
an epsilon ball B (v ), such that B (v )∩P ⊆ R . Hence, we show (LG,ψP) is ℓ -calibrated for at least the
ϵ y ϵ y y φ 0−1
distributionsP =∪ φ−1(B (v )∩P),thereforeφ(P)⊆R .
vy∈vert(P) ϵ y Y
Theorem4. LetD beaBregmandivergence,φbeanypolytopeembedding,ψbetheMAPlink,andLGbetheloss
G φ
inducedby(D ,φ). ThereexistsaP ⊆∆ suchthatP ≠ ∅andφ(P)⊆R via(LG,ψP)withrespecttoℓ .
G Y Y φ 0−1
Proof. Recall that γmode(p) := prop[ℓ ](p) = mode(p). Fix y ∈ Y. For contradiction, assume for any yˆ ∈ Y
0−1
wherey ̸= yˆ,itholdsthatB (v )∩φ(γmode) ̸= ∅forallϵ > 0. ByLemma3,itholdsthatconv({v }∪m ) ⊆
ϵ y yˆ y vy,α
1Weleavethemoregeneralcaseoflinkinguwhen(cid:84) γ(p)̸=∅tofuturework.
p∈φ−1(u)
5TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
φ(γmode)wherem := {(1−α)v +αv | v ∈ ne(v )}definedbyanyα ∈ (0,.5). Furthermore,theelementsof
y vy,α y y
∪ conv({v }∪{m})haveonedistributionembeddedontoitwherey istheonlyvalidmodethus,weknow
m∈mvy,α y
thatφ(mode )∩∪ conv({v }∪{m})=∅.Sinceφ(γmode)⊂P isclosedandconvex,theremustexistsome
yˆ m∈mvy,α y yˆ
non-negativemindistancebetweenφ(γmode)andv whichweshalldenotebyd . Foranyϵ∈(0,d ),wecandefine
yˆ y v vy
B (v )suchthatB (v )∩φ(γmode)=∅,formingacontradiction.
ϵ y ϵ y yˆ
Foreachv ∈vert(P)definead andletϵ′ ∈∩ (0,d ). BytheconstructionofP andthedefinitionofψP,
y vy vy∈vert(P) vy
thereexistsaϵ′′ > 0suchthatforallu ∈ B (v )itholdsthatψ(u) = y andB (v ) ⊂ ψP . Foranyy ∈ Y,we
ϵ′′ y ϵ′′ y y
knowthatB (v ))∩P ⊆R bytheconstructionofourepsilonball. Weclaimφ−1(B (v )∩P)is
min{ϵ′,ϵ′′} y y min{ϵ′,ϵ′′} y
asetofdistributionsforwhichcalibrationholds.
Forp ∈ ∆ suchthatφ(p) ∈ B (v )∩P forsomev ∈ vert(P), supposeasequence{u }convergesto
Y min{ϵ′,ϵ′′} y y m
prop[LG](p)=φ(p)(equalitybyProposition2). ByconstructionofB (v )∩P,ψP(φ(p))=y ∈mode(p)
φ min{ϵ′,ϵ′′} y
andhence, aminimizingreportforℓ (y;p). Furthermore, sinceB (v ) ⊂ ψP , allelementswithin
0−1 min{ϵ′,ϵ′′} y φ−1(vy)
B (v ) link to y. Since {u } converges to prop[LG](p), there exists some N ∈ N and n ≥ N, such that
min{ϵ′,ϵ′′} y m φ
∥u −φ(p)∥ <min{ϵ′,ϵ′′},meaningthatE [ℓ (ψP(u ),Y)]→min E [ℓ (y,Y)]. Hence,forany
n 2 Y∼p 0−1 m y∈Y Y∼p 0−1
v ∈vert(P),(prop[LG],ψP)isℓ -calibratedpropertywithrespecttoφ−1(B (v )∩P). Furthermore,by
y φ 0−1 min{ϵ′,ϵ′′} y
theconstructionofB (v )foreachv ∈vert(P),wehavethatLG isstrictlyforprop[LG]. Thus,byTheorem
minϵ′,ϵ′′ y y φ φ
1, (LG,ψP) is ℓ -calibrated for at least the distributions P = ∪ φ−1(B (v ) ∩ P) as well as
φ 0−1 vy∈vert(P) min{ϵ′,ϵ′′} y
φ(P)⊆R . Furthermore,sinceB foreachv ∈vert(P)isnon-empty,wehavethatP ≠ ∅.
Y min{ϵ′,ϵ′′} y
AlthoughstrictcalibrationregionsR existforeachoutcomey ∈Y viathepolytopeembedding,tightlycharacteriz-
y
ingstrictcalibrationregionsisnon-trivial.Sincethelevelsetsofelicitablepropertiesareconvexwithintheunderlying
simplex,characterizingthestrictcalibrationregionsbecomesacollisiondetectionproblem,whichisoftencomputa-
tionallyhard.
4 RestoringInconsistentSurrogatesviaLow-NoiseAssumptions
Lookingtowardsapplication,werefineourresultsregardingtheexistenceofdistributionsforwhichcalibrationholds
in low dimensions to provide an interpretable description of said distributions via low-noise assumptions. We show
examplesofdistributionsforwhichcalibrationholdsunderlow-noiseassumptionswhenembedding2doutcomesinto
ddimensionsandd!outcomesintoddimensions.
4.1 CalibrationviaLow-NoiseAssumptions
In the following theorem, we demonstrate every polytope embedding leads to calibration under some low-noise as-
sumption. Followingpreviouswork[1],wedefinealownoiseassumptiontobeasubsetoftheprobabilitysimplex:
Θ = {p ∈ ∆ | max p ≥ 1 − αˆ} where αˆ ∈ [0,1]. Given α ∈ (0,1] and y ∈ Y, we define the set
αˆ Y y∈Y y
Ψy ={(1−α)δ +αδ |yˆ∈Y}. WithanembeddingφontoP,wedefinethesetPy :=φ(conv(Ψy)).
α y yˆ α α
Theorem5. LetD beaBregmandivergence,φbeanypolytopeembedding,andLGbethelossinducedby(D ,φ).
G φ G
Thereexistsanα∈[0,.5)suchthatforthelink
ψP(u)=argmin∥u−Py∥ ,
α α 2
y∈Y
(LG,ψP)isℓ -calibratedoverthedistributionsΘ :={p∈∆ |max p ≥1−α}.
φ α 0−1 α Y y∈Y y
Proof. Part1(Choosingα∈[0,.5)): ByTheorem4,thereexistsanϵ>0suchthatB (v )∩P ⊆R forally ∈Y.
ϵ y y
Given that vert(P) are unique points, there exists a sufficiently small ϵ′ > 0 such that B (v)∩B (vˆ) = ∅ for all
ϵ′ ϵ′
v,vˆ ∈ vert(P) where v ̸= vˆ. Let ϵ′′ = min(ϵ,ϵ′). For any y ∈ Y, observe the set conv(Ψy), defined using any
α
α∈[0,.5),isascaled-downtranslatedunitsimplexandthatforallp∈conv(Ψy)⊂∆ itholdsthaty =mode(p).
α Y
6TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Weshallshowthatforsomesufficientlysmallα∈[0,.5),PyisascaleddownversionofP positionedattherespective
α
vertexv . Furthermore,weshallshowthatPy ⊂B (v )∩P ⊆R forally ∈Y. Observethatbylinearityofφ,
y α ϵ′′ y y
Py :=φ(conv(Ψy))
α α
=conv(φ(Ψy))
α
=conv(φ({(1−α)δ +αδ |yˆ∈Y}))
y yˆ
=conv({(1−α)v +αv |yˆ∈Y})
y yˆ
and hence, Py is a scaled version of P positioned at v . Hence for some sufficiently small α,
α y
(1 − α)v + αv ∈ B (v ) for all yˆ and hence Py ⊆ B (v ) ⊆ R . With said sufficiently small α, de-
y yˆ ϵ′′ y α ϵ′′ y y
fineψP andtherespectivesetsconv(Ψy)foreachy ∈Y. Usingthepreviousα,definethesetΘ aswell.
α α α
Part2(ShowingCalibration):Recall,byProposition2,foranyp∈∆ ,u=φ(p)minimizestheexpectedsurrogate
Y
lossE [LG(u,Y)]. Foranyfixedy ∈Y,observethatconv{(1−α)δ +αδ |yˆ∈Y}={p:p ≥1−α}⊂∆
Y∼p φ y yˆ y Y
and hence, by Proposition 2, ∪ Py contains all of the minimizing surrogate reports with respect to Θ . By our
y∈Y α α
choice of α and the construction of ψP, every u ∈ ∪ Py is linked to the proper unique mode outcome since
α y∈Y α
∪ Py ⊆R . Assumingalow-noiseconditionwherep∈Θ ,anyu∈/ ∪ Py isneveroptimalforanylow-noise
y∈Y α Y α y∈Y α
distribution. Insuchcases,weprojectthepointtothenearestPy asamatterofconvention. Giventhatcalibrationisa
α
resultpertainingtominimizingreports,thisdesignchoiceisnon-influential. Finally,sinceevery∪ Py ⊆ R ,by
y∈Y α Y
thedefinitionofstrictcalibrationregion,itholdsthat(LG,ψP)isℓ -calibratedforΘ .
φ α 0−1 α
4.2 EmbeddingintotheUnitCubeandPermutahedronunderLow-Noise
Inthissection,wedemonstrateembeddingoutcomesintotheunitcubeandthepermutahedron. Weshowthatbyem-
bedding2doutcomesintoaddimensionalunitcubeP□,(LG,ψP□
)iscalibratedforΘ forα∈[0,.5).Furthermore,
φ α α
we foundthat byembedding d! outcomesinto a d dimensionalpermutahedron Pw, (LG,ψPw) iscalibrated for Θ
φ α α
forα∈(0,1). WeproveTheorem6tosubsequentlyanalyzetheaforementionedembeddings.
d
Theorem6. LetD beaBregmandivergence,φbeanypolytopeembedding,andLGbethelossinducedby(D ,φ).
G φ G
Fixα∈[0,.5)andwithitdefineΘ . Ifforally,yˆ∈Y suchthaty ̸=yˆitholdsthatPy∩Pyˆ=∅,then(LG,ψP)is
α α α φ α
ℓ -calibratedforΘ .
0−1 α
Proof. Pickanαsuchthatforally,yˆ ∈ Y,Py ∩Pyˆ = ∅. DefineΘ andψP accordingly. Forp ∈ Θ andsome
α α α α α
y ∈ Y,sayasequence{u }convergestoprop[LG](p) = φ(p) ∈ Py,wheretheequalityisbyProposition2. Given
m φ α
thateachPy isclosedandpairwisedisjoint,thereexistssomeϵˆ> 0suchthatforally,yˆ ∈ Y wherey ̸= yˆ,italso
α
holdsthat(Py +B )∩(Pyˆ+B ) = ∅. Since{u }convergestoφ(p),thereexistssomeN ∈ Nsuchthatforall
α ϵˆ α ϵˆ m
n ≥ N,∥u −φ(p)∥ < ϵˆ. BythedefinitionofψP,anyu wheren ≥ N willbemappedtoy,thecorrectunique
n 2 α n
reportgiventhatprop[LG](p) ∈ Py. Hence,(prop[LG],ψP)isℓ -calibratedpropertywithrespecttoΘ . Finally,
φ α φ α 0−1 α
sinceLGisstrictlyproperforprop[LG],byTheorem1,wehavethat(LG,ψP)isℓ -calibratedforΘ .
φ φ φ α 0−1 α
Define a unit cube in d-dimensions by P□ := conv({−1,1}d). Binary encoding outcomes into the elements of
{−1,1}d (the vertices of a unit cube) is a commonly used method in practice [15]. We show that calibration holds
underalownoiseassumptionofΘ ,α<.5
α
Corollary7. Letφbeanembeddingfrom2doutcomesintotheverticesofP□ind-dimensionsanddefineaninduced
lossLG. Fixα∈[0,.5)anddefineΘ . (LG,ψP□ )isℓ -calibratedforΘ .
φ α φ α 0−1 α
LetS expressthesetofpermutationson[d]. Thepermutahedronassociatedwithavectorw ∈Rdisdefinedtobethe
d
convexhullofthepermutationsoftheindicesofw,i.e.,
Pw :=conv{π(w)|π ∈S }⊂Rd.
d
Thepermutahedronmayserveasanembeddingfromd!outcomesintod-dimensions;itisanaturalchoiceforembed-
dingfullrankingsoverditems.
Corollary 8. Let φ be an embedding from d! outcomes into the vertices of Pw in d-dimensions such that w =
(0, 1 , 2 ,...,d−1) ∈ Rd where β = d−1 and define an induced loss LG. Fix α ∈ (0,1) and define Θ . Then
βd βd βd 2 φ d α
(LG,ψPw)isℓ -calibratedforΘ .
φ α 0−1 α
7TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Figure2: (Left)CornersrepresentthestrictcalibrationregionsforΘ whereY ={a,b,c,d}isembeddedintoatwo
α
dimensional unit cube such that α = .25. (Right) Auburn regions show that strict calibration holds for Θ where
α
Y ={a,b,c,d,e,f}isembeddedintoathree-dimensionalpermutahedronsuchthatα=sup(0,1).
3
5 ElicitationinLow-DimensionswithMultipleProblemInstances
Til now, we have analyzed surrogates for which there was some restriction of the calibrated region in the simplex
for the mode. In this section we utilize multiple problem instances to enable elicitability over the whole simplex.
This approach is well-motivated when working with limited computational resources since it allows for distributed
computingofseparatemodels.
Definition10. ExtendingDefinition1,wesayalossandlinkpair(L,ψ),whereL:Rd×Y →Randψ :Rd →R,
elicitsapropertyΓ:P ⇒RonP ⊆∆ if
Y
∀p∈P, Γ(p)=ψ(argminE [L(u,Y)]).
Y∼p
u∈Rd
Definition11((n,d,m)-PolytopeElicitable). Saywehaveapropertyγ : P ⇒ RsuchthatP ⊆ ∆ and|Y| = n
Y
finite outcomes. Say we have m unique polytope embeddings {φj : ∆ → Rd}m where n > d+1, a Bregman
Y j=1
divergence D , and a set of induced losses {LG }m and links ψj : Rd → B defined wrt. φj, where B is an
G φj j=1 j j
arbitraryreportset. Foreachj ∈ [m],assumethepair(LG ,ψj)elicitsthepropertyΓ : P ⇒ B . Ifthereexistsa
φj j j
functionΥ:B ×···×B ⇒Rsuchthatforanyp∈∆ itholdsthatΥ(Γ (p),...,Γ (p))=γ(p),wesaythatγ
1 m Y 1 m
is(n,d,m)-PolytopeElicitablewithrespecttoP.
Equivalently,wewillalsosaythatthepair({(LG ,ψj)}m ,Υ)(n,d,m)-Polytopeelicitsthepropertyγwithrespect
φj j=1
toP.
We shall express a d-cross polytope by P⊕ := conv({π((±1,0,...,0)) | π ∈ S }) where (±1,0,...,0) ∈ Rd.
d
Observethatad-crosspolytopehas2dvertices. Foranyvertexofad-crosspolytopev ∈vert(P⊕),weshallsaythat
(v,−v)formsadiagonalvertexpair.
Lemma9. Saywearegivenapolytopeembeddingφ:∆ →P⊕,aBregmandivergenceD ,andaninducedloss
2d G
LG. Withrespecttoφlet(v ,v ),betheithdiagonalpair(i.e. φ(δ ) = v ). DefinethepropertyΓφ : ∆ → B
φ ai bi ai ai 2d
element-wiseby
(cid:40) (<,a ,b ) ifp <p
i i ai bi
Γφ(p) := (>,a ,b ) ifp >p
i i i ai bi
(=,a ,b ) ifp =p .
i i ai bi
Furthermoredefinethelinkψ :Rd →Bwithrespecttoeachdiagonalpairas
φ
(cid:40) (<,a ,b ) if||u−v || >||u−v ||
i i ai 2 bi 2
ψ(u;v ,v ) := (>,a ,b ) if||u−v || <||u−v || (2)
ai bi φ,i i i ai 2 bi 2
(=,a ,b ) o.w.
i i
Then(LG,ψ )elicitsΓφ.
φ φ
WenowdemonstratethroughusingmultipleprobleminstancesbasedonLemma9,wecanPolytope-elicitthemode.
Theorem10. Letd≥2. Themodeis(2d,d,m)-PolytopeElicitableforsomem∈[2d−1,d(2d−1)].
Wesketchtheproof,leavingdetailstoAppendixA.
8TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Figure3: FouroutcomesembeddedinR2 intwodifferentways,withtheminimizingreports•foradistributionp.”
(Left)Configurationφ1 with•at(−.5,.3)implyingp > p andp > p . (Right)Configurationφ2 with•at(0,0)
a d b c
implyingp =p andp =p . Thisimpliesthetruedistributionisp=(0.4,0.4,0.1,0.1).”
a b c d
Proof. Wewillelicitthemodeviatheintermediateproperties,Γφj,definedinLemma9. Eachintermediateproperty
givesasetofrankingsoftheprobabilityofoutcomes,p ,p ,correspondingtodiagonalpairsofthecrosspolytope.
Intheworstcase,weneedeverycomparisonbetweenevea ri yob ui tcome,(cid:0)2d(cid:1)
=d(2d−1)comparisonpairs.Na¨ıvely,we
2
needatmostd(2d−1)probleminstances,oneforeachuniquepair. But,bychoosingeachψj andthusΓφj carefully,
wecouldreducethenumberofprobleminstancesto(2d−1). Withallcomparisonsinhand, wethenuseasorting
algorithmtocomputethesetofr ∈Rsuchthatp ismaximum.
r
Algorithm1Elicitmodeviacomparisonsandthed-CrossPolytopes
Require: M ={(LG ,ψj)}m
φj φ j=1
Learnamodelh :X →Rdforeachinstance(LG ,ψj)∈M
j φj φ
Forsomefixedx∈X,collectallB ←ψj(h (x))whereB ∈B
j φ j j j
ReportR←FindMaxes(B ,...,B )
1 m
In practice using real data, given that these are asymptotic results, we may have conflicting logic for the provided
individualreports. InAppendixC,wediscussanapproachofhowtoaddressthisinpracticewheretheremaybethe
resultsoftheseparateprobleminstancesmayconflictintheelicitedreports.
6 DiscussionandConclusion
Thisworkexaminedvarioustradeoffsbetweensurrogatelossdimension,numberofprobleminstances,andrestricting
theregionofconsistencyinthesimplexwhenusingthe0-1loss.Givenouranalysisisbasedonanembeddingapproach
commonlyusedinpractice,ourworkprovidestheoreticalguidanceforpractitionerschoosinganembedding. Wesee
severalpossiblefuturedirections.
Thefirstisadeeperinvestigationintohallucinations.Futureworkcouldinvestigatethesizeofthehallucinationregion
intheory,andthefrequencyofreportsinthehallucinationregioninpractice. Anotherdirectionwouldbetoconstruct
amethodthatefficientlyidentifiesthestrictcalibrationregionsandthedistributionsembeddedintothem. Thiswould
provide better guidance on whether or not a particular polytope embedding aligns with one’s prior over the data.
Finally, anotherdirectionistoidentifyotherpropertiesthatcanbeelicitedviamultipleprobleminstancewhilealso
minimizingthetotaldimension.
Acknowledgments
WethankRafaelFrongillofordiscussionsabouthallucinations,whichledtotheexplorationofmanyoftheideasin
thisworkandAmziJeffsfordiscussionsregardingconvexgeometry. Thismaterialisbaseduponworksupportedby
theNationalScienceFoundationunderAwardNo. 2202898(JF).
9TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
References
[1] Arpit Agarwal and Shivani Agarwal. On consistent surrogate risk minimization and property elicitation. In
ConferenceonLearningTheory,pages4–22.PMLR,2015.
[2] ArindamBanerjee,XinGuo,andHuiWang.Ontheoptimalityofconditionalexpectationasabregmanpredictor.
IEEETransactionsonInformationTheory,51(7):2664–2669,2005.
[3] PeterLBartlett,MichaelIJordan,andJonDMcAuliffe. Convexity,classification,andriskbounds. Journalof
theAmericanStatisticalAssociation,101(473):138–156,2006.
[4] Mathieu Blondel. Structured prediction with projection oracles. Advances in neural information processing
systems,32,2019.
[5] Mathieu Blondel, Andre´ FT Martins, and Vlad Niculae. Learning with fenchel-young losses. The Journal of
MachineLearningResearch,21(1):1314–1382,2020.
[6] ArneBrondsted. Anintroductiontoconvexpolytopes,volume90. SpringerScience&BusinessMedia,2012.
[7] Jessica Finocchiaro, Rafael Frongillo, and Bo Waggoner. An embedding framework for consis-
tent polyhedral surrogates. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/
9ec51f6eb240fb631a35864e13737bca-Paper.pdf.
[8] Jessie Finocchiaro, Rafael Frongillo, and Bo Waggoner. Unifying lower bounds on prediction dimension of
consistentconvexsurrogates. arXivpreprintarXiv:2102.08218,2021.
[9] RafaelMFrongilloandIanAKash.Elicitationcomplexityofstatisticalproperties.Biometrika,108(4):857–879,
2021.
[10] PeterMGruber. Convexanddiscretegeometry,volume336. Springer,2007.
[11] Jean-Baptiste Hiriart-Urruty and Claude Lemare´chal. Fundamentals of convex analysis. Springer Science &
BusinessMedia,2004.
[12] ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,DanSu,YanXu,EtsukoIshii,YeJinBang,AndreaMadotto,
and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):
1–38,2023.
[13] Harish G Ramaswamy and Shivani Agarwal. Convex calibration dimension for multiclass loss matrices. The
JournalofMachineLearningResearch,17(1):397–441,2016.
[14] RTyrrellRockafellar. Convexanalysis,volume11. Princetonuniversitypress,1997.
[15] CedricSeger. Aninvestigationofcategoricalvariableencodingtechniquesinmachinelearning: binaryversus
one-hotandfeaturehashing,2018.
[16] IoannisTsochantaridis,ThorstenJoachims,ThomasHofmann,YaseminAltun,andYoramSinger. Largemargin
methodsforstructuredandinterdependentoutputvariables. Journalofmachinelearningresearch,6(9),2005.
[17] MartinJWainwright,MichaelIJordan,etal. Graphicalmodels,exponentialfamilies,andvariationalinference.
FoundationsandTrends®inMachineLearning,1(1–2):1–305,2008.
[18] YexiangXue, ZhiyuanLi, StefanoErmon, CarlaPGomes, andBartSelman. Solvingmarginalmapproblems
withnporaclesandparityconstraints. AdvancesinNeuralInformationProcessingSystems,29,2016.
[19] TongZhang.Statisticalanalysisofsomemulti-categorylargemarginclassificationmethods.JournalofMachine
LearningResearch,5(Oct):1225–1251,2004.
10TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Notation Explanation
r ∈R Predictionspace
y ∈Y Labelspace
∆ SimplexoverY
Y
[d]:={1,...,d} Indexset
1 ∈{0,1}ds.t. (1 ) =1⇔i∈S 0-1IndicatoronsetS ⊆[d]
S S i
C ⊂Rd Closedconvexset
u∈Rd Surrogatepredictionspace
Proj (u):=argmin ||u−x|| Projectionontoclosedconvexset
C x∈C 2
π ∈S Permutationsof[d]
d
ℓ:R×Y →R Discreteloss
+
L:Rd×Y →R Surrogateloss
E [ℓ(r,Y)] Expecteddiscreteloss
Y∼p
E [L(u,Y)] Expectedsurrogateloss
Y∼p
Table1: Tableofgeneralnotation
Notation Explanation
P ⊂Rd Polytope
P□ :=conv({−1,1}d) Unitcube
Pw :=conv{π·w |π ∈S }⊂Rds.t. w ∈Rd Permutahedron
d
P⊕ :=conv({π((±1,0,...,0))|π ∈S }) Crosspolytope
d
vert(P) VerticesofP
E(P) SetofedgesofP
ne(v;P) Setofneighborvertexofv
Table2: Tableofpolytopenotation
11TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
A OmittedProofsandResults
A.1 OmittedProofsfrom§3
Proposition2. ForagiveninducedlossLG,theuniquereportwhichminimizestheexpectedlossis
φ
u∗ :=argminE [LG(u,Y)]=φ(p)
Y∼p φ
u∈Rd
suchthatu∗ ∈P. Furthermore,everyuˆ∈P isaminimizerofE [LG(u,Y)]forsomepˆ∈∆ .
Y∼pˆ φ Y
Proof. By[2,Theorem1],theminimizerofE [LG(u,Y)]is(cid:80) p v =φ(p).Thus,bytheconstructionofthe
Y∼p φ y∈Y y y
polytopeembedding,itholdsthatu∗ = φ(p). SinceBregmandivergencesaredefinedwithrespecttostrictlyconvex
functions,u∗uniquelyminimizesE [LG(u,Y)].
Y∼p φ
Conversely,everyuˆ ∈ P isexpressibleasaconvexcombinationofvertices; hence,bythedefinitionofφ,forsome
distribution,saypˆ∈∆ ,itholdsuˆ=φ(pˆ). Therefore,itholdsthatuˆminimizesE [LG(u,Y)].
Y Y∼pˆ φ
Theorem3. Withrespecttoℓ ,foranygivenpair(LG,ψP)suchthatn−1>d;itholdsthat
0−1 φ
H=∪ conv(vert(P)\{v })∩ψP
y∈Y y y
andfurthermoreH̸=∅.
Proof. Chooseay ∈Y.Weabusenotationandwritevert(P)\v :=vert(P)\{v }.Observeallu∈conv(vert(P)\
y y
v )∩ψP can be expressed as a convex combination of vertices without needing vertex v . The coefficients of said
y y y
convexcombinationexpressap∈∆ thatisembeddedtothepointu∈P wherep =0. Yet,byProposition2,said
Y y
uisanexpectedminimizerofLG withrespecttop. GiventheintersectionwithψP andbyDefinition8,itholdsthat
φ y
∪ conv(vert(P)\v )∩ψP ⊆H.
y∈Y y y
We now shall show that H ⊆ ∪ conv(vert(P) \ v ) ∩ ψP. Fix y ∈ Y. Assume there exists a point u ∈/
y∈Y y y
conv(vert(P)\v )∩ψP such that there exists some p ∈ ∆ where φ(p) = u, p = 0, and ψP(u) = y. Since
y y Y y
ψP(u) = y and u ∈/ conv(vert(P)\v ))∩ψP, it must be the case that u ∈/ conv(vert(P)\v ). However, that
y y y
implies that u is strictly in the vertex figure and thus must have weight on the coefficient for y. Thus, forming a
contradictionthatp =0whichimpliesthatH=∪ conv(vert(P)\v )∩ψP.
y y∈Y y y
Toshownon-emptinessofH,weshalluseHelly’sTheorem(Rockafellar[14],Corollary21.3.2). W.l.o.g,assignan
index such that Y = {y ,...,y ,y ,...,y }. Observe the elements of the set {Y \y }n each differ by one
1 d d+1 n i i=1
element. W.l.o.g,pickthefirstd+1elementsofthepreviousset. Observe|∩d+1Y\y |=|Y\{y ,...,y ,y }|=
i=1 i 1 d d+1
n−(d+1)>0. Hence,byHelly’stheoremanduniquenessofy ’s,∩ conv(vert(P)\v )̸=∅.
i y∈Y y
Pickapointu′ ∈∩ conv(vert(P)\v ). SinceψP iswell-defined,u′ willbelinkedtosomeoutcomey′ ∈Y and
y∈Y y
thusu′ ∈conv(vert(P)\v )∩ψP ⊂H. Yet,u′ canbeexpressedasaconvexcombinationwhichdoesnotusev
y′ y′ y′
sinceitliesin∩ conv(vert(P)\v ). Thus,byusingProposition2andbythedefinitionofHallucination(Def. 8),
y∈Y y
wehavethatH̸=∅.
Lemma1(Proposition1.2.4). [11]IfφisanaffinetransformationofRnandA⊂Rnisconvex,thenthentheimage
φ(A)isalsoconvex. Inparticular,ifthesetAisaconvexpolytope,theimageisalsoaconvexpolytope.
Lemma2. LetD beaBregmandivergence,φbeanypolytopeembedding,ψ betheMAPlink,andLG betheloss
G φ
induced by (D ,φ). Assume the target loss is ℓ . If a point is in a strict calibrated region such that u ∈ R for
G 0−1 y
somey ∈Y,itisnecessarythatu∈conv({v }∪ne(v ))\conv(ne(v )).
y y y
(cid:0) (cid:1)
Proof. If u ∈ R and u ∈ P \ conv({v } ∪ ne(v )) \ conv(ne(v )) , then u can be expressed as a convex
y y y y
combinationwhichhasnoweightonthecoefficientforv . Hence,thereexistsadistributionembeddedintouwhere
y
ywouldnotbethemode,thusviolatingtheinitialclaimthatu∈R .
y
Lemma3. LetD beaBregmandivergence,φbeanypolytopeembedding,ψ betheMAPlink,andLG betheloss
G φ
inducedby(D ,φ). Foranyu∈e ∈E(P),itholdsthat|φ−1(u)|=1.
G (vi,vj)
12TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
Proof. Observe,thetwoverticesofanedgedefinetheconvexhullmakinguptheedgeandhence,by([10],Theorem
2.3)thetwoverticesareaffinelyindependent. Therefore,allelementsoftheedgehaveauniqueconvexcombination
which are expressed by the convex combinations of the edge’s vertices. Given the relation of the embedding φ and
convexcombinationsofverticesexpressingdistributions,itholdsthat|φ−1(u)|=1.
Lemma4. LetD beabregmandivergence, φbeapolytopeembedding, andLG betheinducedlossby(D ,φ).
G φ G
Forally ∈Y,itholdsthatdim(φ(mode ))=dim(P)≥2.
y
Proof. By the construction of φ, we know that dim(P) ≥ 2. Fix y ∈ Y. By Lemma 3, we know that any edge
connectedfromv andvˆ ∈ ne(v ),thedistributionsembeddedintothehalfofthelinesegmentclosertov ,y isin
y y y
themode. ByLemma1,weknowthatφ(γmode)isaconvexset. Thus,theconvexhullofthehalflinesegmentsispart
y
ofφ(γmode). Sinceeachvertexhasatleastdim(P)neighbors,itholdsthatdim(φ(γmode))=dim(P).
y y
A.2 OmittedProofsfrom§4
Corollary7. Letφbeanembeddingfrom2doutcomesintotheverticesofP□ind-dimensionsanddefineaninduced
lossLG. Fixα∈[0,.5)anddefineΘ . (LG,ψP□ )isℓ -calibratedforΘ .
φ α φ α 0−1 α
Proof. W.l.o.g,saytheoutcomey ∈Y isembeddedinto1 ∈vert(P□). Sayα=.5. Observethat
1 [d]
1 1−α 1−α 1−α 1−α
Ψy α1
=

  
0
0
0. .
.
  
,
  

α
0
0. . .

  
,
  

α0
0. .
.

  
,...,
  

α0
0. . .

  
,
  

α0
0. . .

  


and that 1 ≥ (1−α)±α ≥ 0 for any α ∈ (0,.5). Hence, for any α ∈ (0,.5) it holds that Py1 = conv({0,1}d)
0.5
andfurthermorePy1 ⊂ Py1 ⊂ Rd . BysymmetryofP□ andthelinearityofφ,foranyα ∈ (0,.5)andy ∈ Y,we
α 0.5 >0
havethatPy isastrictsubsetoftheorthantthatcontainsv . Hence, forally,yˆ ∈ Y suchthaty ̸= yˆ, itholdsthat
α y
Py∩Pyˆ=∅. ThusbyTheorem6,(LG,ψP□ )isℓ -calibratedforΘ whereα∈(0,.5).
α α φ α 0−1 α
Corollary 8. Let φ be an embedding from d! outcomes into the vertices of Pw in d-dimensions such that w =
(0, 1 , 2 ,...,d−1) ∈ Rd where β = d−1 and define an induced loss LG. Fix α ∈ (0,1) and define Θ . Then
βd βd βd 2 φ d α
(LG,ψPw)isℓ -calibratedforΘ .
φ α 0−1 α
Proof. Let∆ :=conv({1 ∈Rd |i∈[d]})andobservePw ⊂∆ sinceforallπ,∥π·w∥ =∥w∥ =1. Observe
d i d 1 1
thatPw canbesymmetricallypartitionedintod!regionswithdisjointinteriors,oneforeachpermutationπ ∈ S via
d
∆π := {u ∈ ∆ | u ≤ ··· ≤ u }. Fixπ ∈ S andw.l.o.gassumeπ isassociatedwiththeconstraints∆π := {u ∈
d d 1 d d w
∆ | u ≤ ··· ≤ u }implyingthatπ(w) = ( 0 , 1 ,...,d−1). Letα = 1 anddefineΘ . WithrespecttoΘ ,let
w 1 d βd βd βd d α α
y :=φ−1(π(w))∈Yandyˆ:=φ−1(πˆ(w))∈Ysuchthatπˆ ∈S .ThusthesetΨy :={(1−1)δ +(1)δ |y,yˆ∈Y}
d α d y d yˆ
ismappedviaφtothefollowingpoints
1 1
φ(Ψy)={(1− )(π(w))+( )(πˆ(w))|πˆ ∈S }
α d d d
withinthepermutahedron.
WeshallshowthatPy ⊆ ∆π. Ifthiswerenottrue,therewouldexistsanelementofwπ,πˆ ∈ φ(Ψy)suchsuchthat
α d α
forsomepairofadjacentindices,sayi,i+1 ∈ [d−1],wπ,πˆ > wπ,πˆ. Forsakeofcontradiction,fixi ∈ [d−1]and
i i+1
assumethereexistsaπˆ ∈S suchthatwπ,πˆ >wπ,πˆ. Observethatanyelementofπˆ(w)canbeexpressedby j using
d i i+1 βd
13TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
somej ∈{0,1,...,d−1}. Thus,
wπ,πˆ >wπ,πˆ
i i+1
1 i−1 1 1 i 1
⇔(1− )( )+( )(πˆ(w)) >(1− )( )+( )(πˆ(w))
d βd d j d βd d ˆj
1 i−1 1 j 1 i 1 ˆj
⇒ (1− )( )+( )( )>(1− )( )+( )( ) Multiplybyβd
d βd d βd d βd d βd
1 1 1 1
⇒ (i−1)(1− )+j( )>i(1− )+ˆj( )
d d d d
⇒ 1−d>ˆj−j
forsomej,ˆj ∈{0,1,...,d−1}wherej ̸=ˆj.
Case1: (j <ˆj): Thesmallestvaluepossibleforˆj−j is0−(d−1)however,1−d≯1−d.
Case2:(j >ˆj): Thesmallestvaluepossibleforˆj−j is1however,1−d≯1.
Hence,Py ⊆ ∆π andspecifically,therecanexistsanextremepointofPy thatliesontheboundaryof∆π asshown
α d α d
inCase1. However,ifα ∈ (0,1),everyextremepointofPy movesclosertoπ(w)(besidestheextremepointitself
d α
already on π(w)) and therefore Py lies strictly within ∆π. By symmetry of Pw and the linearity of φ, this would
α d
imply that for all y′,y′′ ∈ Y such that y′ ̸= y′′ it holds that Py′ ∩Py′′ = ∅. Thus by Corollary 6, (LG,ψPw) is
α α φ α
ℓ -calibratedforΘ whereα∈(0,1).
0−1 α d
A.3 OmittedProofsfrom§5
Lemma9. Saywearegivenapolytopeembeddingφ:∆ →P⊕,aBregmandivergenceD ,andaninducedloss
2d G
LG. Withrespecttoφlet(v ,v ),betheithdiagonalpair(i.e. φ(δ ) = v ). DefinethepropertyΓφ : ∆ → B
φ ai bi ai ai 2d
element-wiseby
(cid:40) (<,a ,b ) ifp <p
i i ai bi
Γφ(p) := (>,a ,b ) ifp >p
i i i ai bi
(=,a ,b ) ifp =p .
i i ai bi
Furthermoredefinethelinkψ :Rd →Bwithrespecttoeachdiagonalpairas
φ
(cid:40) (<,a ,b ) if||u−v || >||u−v ||
i i ai 2 bi 2
ψ(u;v ,v ) := (>,a ,b ) if||u−v || <||u−v || (2)
ai bi φ,i i i ai 2 bi 2
(=,a ,b ) o.w.
i i
Then(LG,ψ )elicitsΓφ.
φ φ
Proof. W.l.o.g,fixadiagonalpair(v ,v )andletv :=1 andv :=−1 . Definetheembeddingφaccordingly. We
a b a 1 b 1
willshowthatthefollowingistrueforalldistributionsmappedviaφtou∈P⊕.
||u−v || >||u−v || ⇐⇒ p <p
a 2 b 2 a b
OR||u−v || <||u−v || ⇐⇒ p >p
a 2 b 2 a b
OR||u−v || =||u−v || ⇐⇒ p =p .
a 2 b 2 a b
First, fix p ∈ ∆ . Recall, by Proposition 2, the minimizing report for LG in expectation is u = φ(p) ∈ P ⊂ Rd.
2d φ
We will prove the forward direction of the first and second lines. Then the reverse directions follow from the
contrapositives.
Case1, =⇒: Assumeforcontradictionthatp <p and||φ(p)−v || <||φ(p)−v || . Then
a b a 2 b 2
⟨φ(p)−1 ,φ(p)−1 ⟩<⟨φ(p)+1 ,φ(p)+1 ⟩
1 1 1 1
(cid:88) (cid:88)
(u −1)2+ u2 <(u +1)2+ u2
1 i 1 i
i=1 i=1
−u <u .
1 1
14TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
By the definition of a d-cross polytope P⊕ := conv({π((±1,0,...,0)) | π ∈ S }) and the orthogonal relation
d
between vertices, to express a u ∈ P⊕ as a convex combination of vertices, each diagonal pair of vertices
coefficientssolelyinfluencethepositionalongasingleunitbasisvector. Hence, duetothedefinitionofφ, wehave
u =1 ·p −1 ·p <0sincewehaveassumedthatp <p . Hence−u <u <0,acontradiction.
1 1 a 1 b a b 1 1
Case2, =⇒ : Assumep > p and||φ(p)−v || < ||φ(p)−v || . Bysymmetrywithcase1,alltheinequalities
a b a 2 b 2
arereversed,leadingtothecontradictionthat−u >u >0.
1 1
Case3: (p =p ): Followsfromtheifandonlyifsofcases1and2.
a b
Hence(LG,ψ )elicitsΓφ.
φ φ
Theorem10. Letd≥2. Themodeis(2d,d,m)-PolytopeElicitableforsomem∈[2d−1,d(2d−1)].
Proof. Wewillelicitthemodeviatheintermediateproperties,Γφj,definedinLemma9. Firstweconstructasetof
embeddingssothatweguaranteethatalltheφj’sallowcomparisonbetweenanypairofoutcomeprobabilities. For
example, for each unique pair (a,b) ∈ (cid:0)Y(cid:1) define an embedding: φj(δ ) = 1 and φj(δ ) = −1 , and embed
j 2 a 1 b 1
every other remaining report r ∈ Y \ {a,b} arbitrarily. Since (LG,ψ ) elicits Γφ, minimizing each LG with a
φ φ φj
separate model yields us comparisons via the link ψ . To find the set r ∈ R such that p is maximum, we use a
φ r
sorting algorithm that uses pairwise comparisons, such as bubble sort. Hence with Υ as Algorithm 1, we have that
Υ({LG ,ψ })=mode(p).
φj φ
Assuming there exist φjs such that there is no redundancy in comparison pairs between each Γφj, we would need
only d(2d−1) = 2d−1probleminstances. Hence, weestablishourlowerboundontheneedednumberofproblem
d
instances.
B HammingLossHallucinationExample
Hamminglossℓ:Y ×Y →R isdefinedbyℓ(y,yˆ)=(cid:80)d 1 whereY ={−1,1,}d. Supposed=3andwe
+ i=1 yi̸=yˆi
havethefollowingindexingoveroutcomes
Y :={y ≡(1,1,1),y ≡(1,1,−1),y ≡(1,−1,1),y ≡(−1,1,1),
1 2 3 4
y ≡(−1,−1,1),y ≡(1,−1,−1),y ≡(−1,1,−1),y ≡(−1,−1,−1)}.
5 6 7 8
Letusdefinethefollowingdistribution
1 1 1
p =(0, −ϵ, −ϵ, −ϵ,0,0,0,3ϵ)∈∆
ϵ 3 3 3 Y
suchthatϵ>0.
• E [ℓ(y ,Y)]=1+6ϵ
Y∼p 1
• E [ℓ(y ,Y)]=E [ℓ(y ,Y)]=E [ℓ(y ,Y)]= 4 +2ϵ
Y∼p 2 Y∼p 3 Y∼p 4 3
• E [ℓ(y ,Y)]=E [ℓ(y ,Y)]=E [ℓ(y ,Y)]= 7 −4ϵ
Y∼p 5 Y∼p 6 Y∼p 7 3
• E [ℓ(y ,Y)]=2−6ϵ
Y∼p 8
Forallϵ ∈ [0, 1 ),theminimizingreportinexpectationisy = (1,1,1). However,p = 0andthus,ahallucination
12 1 1
wouldoccurunderacalibratedsurrogateandlinkpair.
C LinkingunderMultipleProblemInstances
Asstatedin§5,whenusingrealdata,giventhattheseareasymptoticresults,wemayhaveconflictinglogicforthe
providedindividualreports. Inthissection,weprovideanapproachsuchthatthealgorithmstillreportsinformation
intheaforementionedscenarioandwillreducetoAlgorithm1asymptotically. WebuildabinaryrelationtableM ∈
15TradingoffConsistencyandDimensionalityofConvexSurrogatesfortheMode
{0,1}n×nwiththeprovidedreports. BasedonM,weselectalargestsubsetofS ⊆Y suchthatwhenM isrestricted
torowsandcolumnscorrespondingtotheelementsofS,denotedbyM ,wehavethatM isreflexive,antisymmetric,
S S
transitive, and strongly connected implying M has a total-order relation defined over its elements. Having a total-
S
orderrelationinfersthemodecanbefoundviacomparisons. Thealgorithmreturns(R,S),whereRisthemodeset
withrespecttotheelementsofS.
Algorithm2Elicitmodeviacomparisonsandthed-CrossPolytopesoverwell-definedpartialorderings
Require: M ={(LG ,ψj)}m
φj φ j=1
Learnamodelh :X →Rdforeachinstance(LG ,ψj)∈M
j φj φ
Forsomefixedx∈X,collectallB ←ψj(h (x))whereB ∈B
j φ j j j
BuildM ∈{0,1}n×nbinaryrelationtablewithprovided{B }m assuch
j j=1
• Labelrowstoptobottombyy ,...,y andcolumnslefttorightbyy ,...,y .
1 n 1 n
• Forall(·,p ,p )∈B ,ifp ≤p setM[i,k]=1and0otherwise.
yi yk j yi yk
SelectlargestsubsetS ⊆Y suchthatM isreflexive,antisymmetric,transitive,andstronglyconnected.
S
Report(R,S)←FindMaxElements-of-S(M;S)
16