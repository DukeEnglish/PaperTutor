ERROR FEEDBACK RELOADED: FROM QUADRATIC TO
ARITHMETIC MEAN OF SMOOTHNESS CONSTANTS
PeterRichta´rik ElnurGasanov KonstantinBurlachenko
AIInitiative AIInitiative AIInitiative
KAUST∗,SaudiArabia KAUST,SaudiArabia KAUST,SaudiArabia
ABSTRACT
Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing
convergence issues which arise in distributed training methods (such as distributed GD
or SGD) when these are enhanced with greedy communication compression techniques
suchasTopK.WhileEFwasproposedalmostadecadeago(Seideetal.,2014), andde-
spite concentrated effort by the community to advance the theoretical understanding of
this mechanism, there is still a lot to explore. In this work we study a modern form of
errorfeedbackcalledEF21(Richta´riketal.,2021)whichoffersthecurrentlybest-known
theoretical guarantees, under the weakest assumptions, and also works well in practice.
In particular, while the theoretical communication complexity of EF21 depends on the
quadratic mean of certain smoothness parameters, we improve this dependence to their
arithmeticmean,whichisalwayssmaller,andcanbesubstantiallysmaller,especiallyin
heterogeneous data regimes. We take the reader on a journey of our discovery process.
StartingwiththeideaofapplyingEF21toanequivalentreformulationoftheunderlying
problemwhich(unfortunately)requires(oftenimpractical)machinecloning,wecontinue
tothediscoveryofanewweightedversionofEF21whichcan(fortunately)beexecuted
withoutanycloning,andfinallycirclebacktoanimprovedanalysisoftheoriginalEF21
method. While this development applies to the simplest form of EF21, our approach
naturallyextendstomoreelaboratevariantsinvolvingstochasticgradientsandpartialpar-
ticipation. Further, our technique improves the best-known theory of EF21 in the rare
featuresregime(Richta´riketal.,2023). Finally,wevalidateourtheoreticalfindingswith
suitableexperiments.
CONTENTS
1 Introduction 4
1.1 Communicationcompression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 Summaryofcontributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 EF21Reloaded: OurDiscoveryStory 6
2.1 Step1: Cloningtheclientwiththeworsesmoothnessconstant . . . . . . . . . . . . . . . . 6
2.2 Step2: Generalizingthecloningidea. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
∗KingAbdullahUniversityofScienceandTechnology
1
4202
beF
61
]GL.sc[
1v47701.2042:viXra2.3 Step3: Fromclientcloningtoupdateweighting . . . . . . . . . . . . . . . . . . . . . . . . 8
2.4 Step4: Fromweightsinthealgorithmtoweightsintheanalysis . . . . . . . . . . . . . . . 9
3 Experiments 10
3.1 Non-convexlogisticregressiononbenchmarkdatasets . . . . . . . . . . . . . . . . . . . . 10
3.2 Non-convexlinearmodelonsyntheticdatasets. . . . . . . . . . . . . . . . . . . . . . . . . 11
A BasicResultsandLemmas 18
A.1 Optimalclientcloningfrequencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Descentlemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Young’sinequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.4 2-Suboptimalbutsimplestepsizerule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.5 OptimalcoefficientinYoung’sinequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B CloningreformulationforPolyak-Łojaschewitzfunctions 20
C ProofofTheorem3(TheoryforEF21-W) 21
C.1 Alemma. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 Mainresult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 MainresultforPolyak-Łojasiewiczfunctions . . . . . . . . . . . . . . . . . . . . . . . . . 24
D ProofofTheorem4(ImprovedTheoryforEF21) 25
D.1 Twolemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 Mainresult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D.3 MainresultforPolyak-Łojasiewiczfunctions . . . . . . . . . . . . . . . . . . . . . . . . . 27
E EF21-W-SGD:WeightedErrorFeedback2021withStochasticSubsampledGradients 29
E.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.2 Alemma. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E.3 Mainresult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
F EF21-W-SGD:WeightedErrorFeedback2021withStochasticGradientsundertheABCAs-
sumption 37
F.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.2 Alemma. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
F.3 Mainresult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2G EF21-W-PP:WeightedErrorFeedback2021withPartialParticipation 46
G.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
G.2 Alemma. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
G.3 Mainresult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
H ImprovedTheoryforEF21intheRareFeaturesRegime 52
H.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
H.2 Newsparsitymeasure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
H.3 Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
H.4 Mainresult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
I Experiments: FurtherDetails 58
I.1 Computingandsoftwareenvironment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
I.2 Commentsontheimprovement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
I.3 Whenimprovedanalysisleadstomoreaggressivesteps . . . . . . . . . . . . . . . . . . . . 58
I.4 Datasetgenerationforsyntheticexperiment . . . . . . . . . . . . . . . . . . . . . . . . . . 59
I.5 DatasetshufflingstrategyforLIBSVMdataset. . . . . . . . . . . . . . . . . . . . . . . . . 60
J AdditionalExperiments 61
J.1 AdditionalexperimentsforEF21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
J.2 AdditionalexperimentsforEF21-W-PP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
J.3 AdditionalexperimentsforEF21-W-SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
K ReproducibilityStatement 69
31 INTRODUCTION
Duetotheirabilitytoharnessthecomputationalcapabilitiesofmoderndevicesandtheircapacitytoextract
valuefromtheenormousdatageneratedbyorganizations,individuals,andvariousdigitaldevicesandsen-
sors, Machine Learning (ML) methods (Bishop, 2016; Shalev-Shwartz & Ben-David, 2014) have become
indispensableinnumerouspracticalapplications(Krizhevskyetal.,2012;Linetal.,2022;Vaswanietal.,
2017;Onay&O¨ztu¨rk,2018;Poplinetal.,2017;Gavrilu¸tetal.,2009;Sunetal.,2017).
Thenecessitytohandlelargedatasetshasdrivenapplicationentitiestostoreandprocesstheirdatainpower-
fulcomputingcenters(Yangetal.,2019;Deanetal.,2012;Verbraekenetal.,2020)viadistributedtraining
algorithms. Besidethisindustry-standardcentralizedapproach,decentralizedformsofdistributedlearning
arebecomingincreasinglypopular. Forexample,FederatedLearning(FL)facilitatesacollaborativelearn-
ingprocessinwhichvariousclients,suchashospitalsorownersofedgedevices,collectivelytrainamodel
on their devices while retaining their data locally, without uploading it to a centralized location (Konecˇny´
etal.,2016b;a;McMahanetal.,2017;Lietal.,2020a;Kairouzetal.,2021;Wangetal.,2021).
Distributedtrainingproblemsaretypicallyformulatedasoptimizationproblemsoftheform
(cid:26) n (cid:27)
min f(x):= 1 (cid:80) f (x) , (1)
x∈Rd n
i=1
i
wherenisthenumberofclients/workers/nodes, vectorx ∈ Rd representsthedtrainableparameters, and
f (x)isthelossofthemodelparameterizedbyxonthetrainingdatastoredonclienti∈[n]:={1,...,n}.
i
One of the key issues in distributed training in general, and FL in particular, is the communication bottle-
neck (Konecˇny´ et al., 2016b; Kairouz et al., 2021). The overall efficiency of a distributed algorithm for
solving(1)canbecharacterizedbymultiplyingthenumberofcommunicationroundsneededtofindasolu-
tionofacceptableaccuracybythecostofeachcommunicationround:
communicationcomplexity=#communicationrounds×costof1communicationround. (2)
Thissimpleformulaclarifiestherationalebehindtwoorthogonalapproachestoalleviatingthecommunica-
tionbottleneck. i)Thefirstapproachaimstominimizethefirstfactorin(2). Thisisdonebycarefullyde-
cidingonwhatworkshouldbedoneontheclientsineachcommunicationroundinorderforittoreducethe
totalnumberofcommunicationroundsneeded,andincludesmethodsbasedonlocaltraining(Stich,2018;
Lin et al., 2018; Mishchenko et al., 2022; Condat et al., 2023; Li et al., 2019) and momentum (Nesterov,
1983; 2004; d’Aspremont et al., 2021). Methods in this class communicate dense d-dimensional vectors.
ii)Thesecondapproachaimstominimizethesecondfactorin(2). Methodsinthiscategorycompressthe
information(typicallyd-dimensionalvectors)transmittedbetweentheclientsandtheserver(Alistarhetal.,
2017;Khiriratetal.,2018;Bernsteinetal.,2018;Safaryanetal.,2021).
1.1 COMMUNICATIONCOMPRESSION
Vectorcompressioncanbeachievedthroughtheapplicationofacompressionoperator. Below,weoutline
twoprimaryclassesoftheseoperators: unbiased(withconicallyboundedvariance)andcontractive.
Definition1(Compressors). ArandomizedmappingC : Rd → Rd iscalledi)anunbiasedcompressorif
forsomeω >0itsatisfies
E[C(x)]=x, E(cid:2) ∥C(x)−x∥2(cid:3) ≤ω∥x∥2, ∀x∈Rd, (3)
andii)acontractivecompressorifforsomeα∈(0,1]itsatisfies
E(cid:2) ∥C(x)−x∥2(cid:3) ≤(1−α)∥x∥2, ∀x∈Rd. (4)
ItiswellknownthatwheneveracompressorC satisfies(3),thenthescaledcompressorC/(ω+1)satisfies
(4) with α = 1 − (ω + 1)−1. In this sense, the class of contractive compressors includes all unbiased
compressorsaswell. However,itisalsostrictlylarger. Forexample,theTopK compressor,whichretains
the K largest elements in absolute value of the vector it is applied to and replaces the rest by zeros, and
4Algorithm1EF21: ErrorFeedback2021
1: Input: initialmodelx0 ∈Rd;initialgradientestimatesg0,g0,...,g0 ∈Rdstoredattheserverandthe
1 2 n
clients;stepsizeγ >0;numberofiterationsT >0
2: Initialize: g0 = 1 (cid:80)n g0ontheserver
n i=1 i
3: fort=0,1,2,...,T −1do
4: Servercomputesxt+1 =xt−γgtandbroadcastsxt+1toallnclients
5: fori=1,...,nontheclientsinparalleldo
6: Computeut
i
=C it(∇f i(xt+1)−g it)andupdateg it+1 =g it+ut
i
7: Sendthecompressedmessageuttotheserver
i
8: endfor
9: Serverupdatesgt+1 =gt+utforalli∈[n],andcomputesgt+1 = 1 (cid:80)n gt+1
i i i n i=1 i
10: endfor
11: Output: PointxˆT chosenfromtheset{x0,...,xT−1}uniformlyatrandom
happenstobeverypowerfulinpractice(Alistarhetal.,2018),satisfies(4)withα= K,butdoesnotsatisfy
d
(3). Fromnowon,wewriteC(α)todenotetheclassofcompressorssatisfying(4).
Itwillbeconvenienttodefinethefollowingfunctionsofthecontractionparameter
√ (cid:113) √
θ =θ(α):=1− 1−α; β =β(α):= 1√−α ; ξ =ξ(α):= β(α) = 1+ 1−α −1. (5)
1− 1−α θ(α) α
Note that 0 ≤ ξ(α) < 2 −1. The behavior of distributed algorithms utilizing unbiased compressors for
α
solving (1) is relatively well-understood from a theoretical standpoint (Khirirat et al., 2018; Mishchenko
et al., 2019; Li et al., 2020b; Gorbunov et al., 2021; Tyurin & Richta´rik, 2023). By now, the community
possessesarobusttheoreticalunderstandingoftheadvantagessuchmethodscanofferandthemechanisms
behindtheirefficacy(Gorbunovetal.,2020;Khaledetal.,2023;Tyurin&Richta´rik,2023). However,itis
wellknownthattheclassofcontractivecompressorsincludessomepracticallyverypowerfuloperators,such
asthegreedysparsifierTopK(Stichetal.,2018;Alistarhetal.,2018)andthelow-rankapproximatorRankK
(Vogels et al., 2019; Safaryan et al., 2022), which are biased, and hence their behavior is not explainable
by the above developments. These compressors have demonstrated surprisingly effective performance in
practice(Seideetal.,2014;Alistarhetal.,2018), evenwhencomparedtothebestresultswecangetwith
unbiasedcompressors(Szlendaketal.,2022),andareindispensableondifficulttaskssuchasthefine-tuning
offoundationmodelsinageographicallydistributedmanneroverslownetworks(Wangetal.,2023).
However,ourtheoreticalunderstandingofalgorithmsbasedoncontractivecompressorsingeneral,andthese
powerful biased compressors in particular, is very weak. Indeed, while the SOTA theory involving unbi-
asedcompressorsofferssignificantandoftenseveral-degrees-of-magnitudeimprovementsoverthebaseline
methodsthatdonotusecompression(Mishchenkoetal.,2019;Horva´thetal.,2019b;Lietal.,2020b;Gor-
bunovetal.,2020;2021;Tyurin&Richta´rik,2023),thebesttheorywecurrentlyhaveformethodsthatcan
provablyworkwithcontractivecompressors,i.e.,thetheorybehindtheerrorfeedbackmethodcalledEF21
developedbyRichta´riketal.(2021)(seeAlgorithm1)anditsvariants(Fatkhullinetal.,2021;Condatetal.,
2022; Fatkhullin et al., 2023), merely matches the communication complexity of the underlying methods
thatdonotuseanycompression(Szlendaketal.,2022).
To the best of our knowledge, the only exception to this is the very recent work of Richta´rik et al. (2023)
showingthatinararefeaturesregime,theEF21method(Richta´riketal.,2021)outperformsgradientdescent
(whichisaspecialcaseofEF21whenCt(x) ≡ xforalli ∈ [n]andt ≥ 0)intheory. However,Richta´rik
i
et al. (2023) obtain no improvements upon the current best theoretical result for vanilla EF21 (Richta´rik
etal.,2021)inthegeneralsmoothnonconvexregime,outlinedinSection1.2,weinvestigateinthiswork.
51.2 ASSUMPTIONS
WeadoptthesameveryweakassumptionsasthoseusedbyRichta´riketal.(2021)intheiranalysisofEF21.
Assumption1. Thefunctionf isL-smooth,i.e.,thereexistsL>0suchthat
∥∇f(x)−∇f(y)∥≤L∥x−y∥, ∀x,y ∈Rd. (6)
Assumption2. Thefunctionsf areL -smooth,i.e.,foralli∈[n]thereexistsL >0suchthat
i i i
∥∇f (x)−∇f (y)∥≤L ∥x−y∥, ∀x,y ∈Rd. (7)
i i i
Notethatif(7)holds,then(6)holds,andL≤L := 1 (cid:80)n L . So,Assumption1doesnotfurtherlimit
AM n i=1 i
theclassoffunctionsalreadycoveredbyAssumption2.Indeed,itmerelyprovidesanewparameterLbetter
characterizingthesmoothnessoff thantheestimateL obtainablefromAssumption2could.
AM
Sinceourgoalin(1)istominimizef,thebelowassumptionisnecessaryfortheproblemtobemeaningful.
Assumption3. Thereexistsf∗ ∈Rsuchthatinff ≥f∗.
1.3 SUMMARYOFCONTRIBUTIONS
InourworkweimprovethecurrentSOTAtheoreticalcommunicationcomplexityguaranteesfordistributed
algorithmsthatworkwithcontractivecompressorsingeneral,andempiricallypowerfulbiasedcompressors
suchasTopK andRankK inparticular(Richta´riketal.,2021;Fatkhullinetal.,2021).
In particular, under Assumptions 1–3, the best known guarantees were obtained by Richta´rik et al. (2021)
fortheEF21method: tofinda(random)vectorxˆT
satisfyingE(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105)
≤ε,Algorithm1requires
T
=O(cid:0)
(L+L
ξ(α))ε−1(cid:1)
QM
(cid:113)
iterations, where L := 1 (cid:80)n L2 is the Quadratic Mean of the smoothness constants L ,...,L .
QM n i=1 i 1 n
Ourmainfindingisanimprovementofthisresultto
T
=O(cid:0)
(L+L
ξ(α))ε−1(cid:1)
, (8)
AM
whereL := 1 (cid:80)n L istheArithmeticMeanofthesmoothnessconstantsL ,...,L . Weobtainthis
AM n i=1 i 1 n
improvementinthreedifferentways:
i) byclientcloning(seeSections2.1and2.2andTheorem2),
ii) by proposing a new smootness-weighted variant of EF21 which we call EF21-W (see Section 2.3
andTheorem3),and
iii) byanewsmoothness-weightedanalysisofclassicalEF21(seeSection2.4andTheorem4).
We obtain refined linear convergence results cases under the Polyak-Łojasiewicz condition. Further, our
analysistechniqueextendstomanyvariantsofEF21,includingEF21-SGDwhichusesstochasticgradients
insteadofgradients(SectionE),andEF21-PPwhichenablespartialparticipationofclients(SectionG).Our
analysisalsoimprovesupontheresultsofRichta´riketal.(2023)whostudyEF21intherarefeaturesregime
(SectionH).Finally,wevalidateourtheorywithsuitablecomputationalexperiments(Sections3,IandJ).
2 EF21 RELOADED: OUR DISCOVERY STORY
Wenowtakethereaderalongonarideofourdiscoveryprocess.
2.1 STEP1: CLONINGTHECLIENTWITHTHEWORSESMOOTHNESSCONSTANT
Thestartingpointofourjourneyisasimpleobservationdescribedinthefollowingexample.
6Example1. Letn=4andf(x)= 1(f (x)+f (x)+f (x)+f (x)). Assumethesmoothnessconstants
4 1 2 3 4
L 1,L 2,L 3off 1,f 2,f 3areequalto1,andL 4isequalto100. Int √hiscase,EF21needstorunfor
T
:=O(cid:0)
(L+L
ξ(α))ε−1(cid:1) =O(cid:0)(cid:0)
L+
2501.5ξ(α)(cid:1) ε−1(cid:1)
1 QM
iterations. Now,envisiontheexistenceofanadditionalmachinecapableofdownloadingthedatafromthe
fourth“problematic”machine. Byrescalinglocallossfunctions,wemaintaintheoveralllossfunctionas:
f(x)= 1(f (x)+f (x)+f (x)+f (x))= 1(cid:0)5f (x)+ 5f (x)+ 5f (x)+ 5f (x)+ 5f (x)(cid:1) :=f˜(x).
4 1 2 3 4 5 4 1 4 2 4 3 8 4 8 4
RescalingofthefunctionsmodifiesthesmoothnessconstantstoLˆ = 5L fori=1,2,3,andLˆ = 5L for
i 4 i i 8 4
i=4,5. EF21,launchedonthissettingoffivenodes,requires
T :=O(cid:16)(cid:16) L+L˜ ξ(α)(cid:17) ε−1(cid:17) ≈O(cid:0)(cid:0) L+√ 1564ξ(α)(cid:1) ε−1(cid:1)
2 QM
iterations,whereL˜ isthequadraticmeanofthenewsmoothnessconstantsLˆ ,...,Lˆ .
QM 1 5
Thissimpleobservationhighlightsthattheadditionofjustonemoreclientsignificantlyenhancesthecon-
√ √
vergence rate. Indeed, EF21 requires approximately ξ(α)( 2501.5− 1564) ≈ 10ξ(α) fewer iterations.
ε ε
Wewillgeneralizethisclientcloningideainthenextsection.
2.2 STEP2: GENERALIZINGTHECLONINGIDEA
Wewillnowtaketheabovemotivatingexamplefurther,allowingeachclientitobeclonedarbitrarilymany
(N ) times. Let us see where this gets us. For each i ∈ [n], let N denote a positive integer. We define
i i
N
:=(cid:80)n
N (thetotalnumberofclientsaftercloning),andobservethatf canbeequivalentlywrittenas
i=1 i
f(x)( =1) 1
(cid:80)n
f (x)= 1
(cid:80)n (cid:80)Ni
1 f (x)= 1
(cid:80)n (cid:80)Ni
N f (x)= 1
(cid:80)n (cid:80)Ni
f (x), (9)
n
i=1
i n i=1j=1Ni i N i=1j=1nNi i N
i=1j=1
ij
wheref (x) := N f (x)foralli ∈ [n]andj ∈ [N ].Noticethatwescaledthefunctionsasbefore,and
ij nNi i i
thatf isL -smooth,whereL := N L .
ij ij ij nNi i
Analysisoftheconvergencerate. TheperformanceofEF21,whenappliedtotheproblem(9)involvingN
clients,dependsonthequadraticmeanofthenewsmoothnessconstants:
(cid:115) (cid:115) (cid:115)
M(N 1,...,N n):= N1 i(cid:80) =n 1j(cid:80)N =i 1L2
ij
= i(cid:80) =n 1n2N NiL2
i
= n1 i(cid:80) =n 1NL i/2 i N. (10)
NotethatifN =1foralli∈[n],thenM(1,...,1)=L .
i QM
Optimalchoiceofcloningfrequencies.OurgoalistofindintegervaluesN ∈N,...,N ∈Nminimizing
1 n
thefunctionM(N ,...,M )definedin(10). Whilewedonothaveaclosed-formformulafortheglobal
1 n √
minimizer, weareabletoexplicitlyfindasolutionthatisatmost 2timesworsethantheoptimalonein
termsoftheobjectivevalue. Inparticular,ifweletN i⋆ =⌈Li/LAM⌉foralli∈[n √],then
L ≤ min M(N ,...,N )≤M(N⋆,...,N⋆)≤ 2L ,
AM N1∈N,...,Nn∈N 1 n 1 n AM
andmoreover,n≤N⋆ :=(cid:80) N⋆ ≤2n. Thatis,weneedatmostdoublethenumberofclientsinourclient
i i
cloningconstruction. SeeLemma2intheAppendixfordetails.
By directly applying EF21 theory from (Richta´rik et al., 2021) to problem (9) involving N⋆ clients, we
obtaintheadvertisedimprovementfromL toL .
QM AM
Theorem 2 (Convergence of EF21 applied to problem (9) with N⋆ machines). Consider Algorithm 1
(EF21)appliedtothe“cloningreformulation”(9)ofthedistributedoptimizationproblem(1),whereN⋆ =
i
⌈Li/LAM⌉foralli ∈ [n]. LetAssumptions1–3hold,assumethatC it
j
∈ C(α)foralli ∈ [n],j ∈ [N i]and
7t≥0,set
Gt := N1 (cid:80)N (cid:80)Ni (cid:13) (cid:13)g it
j
−∇f ij(xt)(cid:13) (cid:13)2 ,
i=1j=1
and let the stepsize satisfy 0 < γ ≤ √ 1 . If for T ≥ 1 we define xˆT as an element of the set
L+ 2LAMξ(α)
{x0,x1,...,xT−1}chosenuniformlyatrandom,then
E(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105) ≤ 2(f(x0)−f∗) + G0 .
γT θ(α)T
When we choose the largest allowed stepsize and g0 = ∇f (x0) for all i,j, this leads to the complexity
ij ij √
(8); thatis, bycloningclientmachines, wecanreplaceL inthestandardratewith 2L . Asimilar
QM AM
resultcanbeobtainedevenifwedonotignoretheintegralityconstraint,butwedonotincludeitforbrevity
reasons.However,itisimportanttonotethatthecloningapproachhasseveralstraightforwardshortcomings,
whichwewilladdressinthenextsection.1
2.3 STEP3: FROMCLIENTCLONINGTOUPDATEWEIGHTING
It is evident that employing client cloning improves the convergence rate. Nevertheless, there are obvious
drawbacksassociatedwiththisapproach. Firstly,itnecessitatesalargernumberofcomputationaldevices,
rendering its implementation less appealing from a resource allocation perspective. Secondly, the utiliza-
tion of EF21 with cloned machines results in a departure from the principles of Federated Learning, as it
inherentlycompromisesuserprivacy–transferringdatafromonedevicetoanotherisprohibitedinFL.
However,asimplerapproachtoimplementingthecloningideaemergeswhenweassumethecompressors
usedtobedeterministic. Toillustratethis,letusinitiallyexaminehowwewouldtypicallyimplementEF21
withclonedmachines:
xt+1 =xt−γ 1
(cid:80)n (cid:80)Ni
gt , (11)
N ij
i=1j=1
gt+1 =gt +Ct (∇f (xt+1)−gt ), i∈[n], j ∈[N ]. (12)
ij ij ij ij ij i
We will now rewrite the same method in a different way. Assume we choose g0 = g0 for all j ∈ [N ].
ij i i
We show by induction that gt is the same for all j ∈ [N ]. We have just seen that this holds for t = 0.
ij i
Assumethisholdsforsomet. Thensince∇f (xt+1)= N ∇f (xt+1)forallj ∈[N ]combinedwiththe
ij nNi i i
induction hypothesis, (12) and the determinism of Ct , we see that gt+1 is the same for all j ∈ [N ]. Let
ij ij i
us define gt ≡ gt for all t. This is a valid definition since we have shown that gt does not depend on j.
i ij ij
1Inourwork,weaddressanoptimizationproblemoftheformmin wj≥0∀j∈[n];(cid:80)n i=1wi=1(cid:80)n
i=1
wa2 i i,wherea irep-
resentcertainconstants.Thisformulationbearsaresemblancetothemetaproblemintheimportancesamplingstrategy
discussedin(Zhao&Zhang,2015). Despitetheapparentsimilaritiesintheabstractformulation,ourapproachandthe
oneinthereferencedworkdivergesignificantlyinbothmotivationandimplementation.WhileZhao&Zhang(2015)ap-
pliesimportancesamplingtoreducethevarianceofastochasticgradientestimatorbyadjustingsamplingprobabilities,
ourmethodinvolvesadjustingclientcloningweightswithoutsampling. Furthermore,ourgradientestimatorisbiased,
unliketheunbiasedestimatorinthereferencedpaper,andweaimtominimizethequadraticmeanofthesmoothness
constants,whichisinherentlydifferentfromtheobjectivesinZhao&Zhang(2015). Althoughbothapproachescanbe
expressedthroughasimilarmathematicalframework,theyareemployedinvastlydifferentcontexts,andanyparallelism
maybecoincidentalratherthanindicativeofadirectconnection.
8Algorithm2EF21-W:WeightedErrorFeedback2021
1: Input: initial model parameters x0 ∈ Rd; initial gradient estimates g0,g0,...,g0 ∈ Rd stored at the
1 2 n
2:
Is ner iv tie ar lia zn ed :t gh 0e =cli (cid:80)en
n
it =s; 1w we ii gg i0ht os nw ti he= seL ri v/ e(cid:80) rjLj;stepsizeγ >0;numberofiterationsT >0
3: fort=0,1,2,...,T −1do
4: Servercomputesxt+1 =xt−γgtandbroadcastsxt+1toallnclients
5: fori=1,...,nontheclientsinparalleldo
6: Computeut
i
=C it( nw1 i∇f i(xt+1)−g it)andupdateg it+1 =g it+ut
i
7: Sendthecompressedmessageuttotheserver
i
8: endfor
9: Serverupdatesg it+1 =g it+ut
i
foralli∈[n],andcomputesgt+1 =(cid:80)n i=1w ig it+1
10: endfor
11: Output: PointxˆT chosenfromtheset{x0,...,xT−1}uniformlyatrandom
Becauseofalloftheabove,iterations(11)–(12)canbeequivalentlywrittenintheform
n
xt+1 =xt−γ (cid:80) w gt, (13)
i i
i=1
(cid:16) (cid:17)
gt+1 =gt+Ct 1 ∇f (xt+1)−gt , i∈[n], (14)
i i i nwi i i
where w i = (cid:80)L ji Lj. This transformation effectively enables us to operate the method on the original n
clients,eliminatingtheneedforN clients! Thisrefinementhasledtothecreationofanewalgorithmthat
outperformsEF21intermsofconvergencerate,whichwecallEF21-W(Algorithm2). Whilewereliedon
assuming that the compressors are deterministic in order to motivate the transition from N to n clients, it
turnsoutthatEF21-Wconvergeswithouttheneedtoinvokethisassumption.
Theorem3(TheoryforEF21-W). ConsiderAlgorithm2(EF21-W)appliedtothedistributedoptimization
problem(1). LetAssumptions1–3hold,assumethatCt ∈C(α)foralli∈[n]andt≥0,set
i
n (cid:13) (cid:13)2
Gt := (cid:80) w (cid:13)gt− 1 ∇f (xt)(cid:13) ,
i=1
i(cid:13) i nwi i (cid:13)
wherew i = (cid:80)L ji Lj foralli ∈ [n],andletthestepsizesatisfy0 < γ ≤ L+LA1 Mξ(α).IfforT > 1wedefine
xˆT asanelementoftheset{x0,x1,...,xT−1}chosenuniformlyatrandom,then
E(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105) ≤ 2(f(x0)−f∗) + G0 . (15)
γT θ(α)T
2.4 STEP4: FROMWEIGHTSINTHEALGORITHMTOWEIGHTSINTHEANALYSIS
Intheprecedingsection,weintroducedanovelalgorithm: EF21-W.Whileitbearssomeresemblancetothe
vanillaEF21algorithm(Richta´riketal.,2021)(werecoveritforuniformweights),therelianceonparticular
non-uniformweightsenablesittoachieveafasterconvergencerate. However,thisisnottheendofthestory
asanotherinsightrevealsyetanothersurprise.
LetusconsiderthescenariowhenthecompressorsinAlgorithm2arepositivelyhomogeneous2.Introducing
thenewvariableht =nw gt,wecanreformulatethegradientupdateinAlgorithm2to
i i i
(cid:104) (cid:16) (cid:17)(cid:105)
ht+1 =nw gt+1 ( =14) nw gt+Ct ∇fi(xt+1) −gt =ht+Ct(∇f (xt)−ht),
i i i i i i nwi i i i i i
2AcompressorC :Rd →RdispositivelyhomogeneousifC(tx)=tC(x)forallt>0andx∈Rd.
9indicatingthatht adherestotheupdateruleofthevanillaEF21method! Furthermore, theiteratesxt also
i
followthesameruleasEF21:
n n n
xt+1 ( =13) xt−γ (cid:80) w gt =xt−γ (cid:80) w 1 ht =xt−γ1 (cid:80) ht.
i=1
i
i=1
inwi i n
i=1
i
So,whatdoesthismean? Oneinterpretationsuggeststhatforpositivelyhomogeneouscontractivecompres-
sors,thevanillaEF21algorithmisequivalenttoEF21-W,andhenceinheritsitsfasterconvergenceratethat
depends on L rather than on L . However, it turns out that we can establish the same result with-
AM QM
outhavingtoresorttopositivehomogeneityaltogether. Forexample,the”naturalcompression”quantizer,
whichroundstooneofthetwonearestpowersoftwo,isnotpositivelyhomogeneous(Horva´thetal.,2019a).
Theorem4(NewtheoryforEF21). ConsiderAlgorithm1(EF21)appliedtothedistributedoptimization
problem(1). LetAssumptions1–3hold,assumethatCt ∈C(α)foralli∈[n]andt≥0,set
i
n
Gt := 1 (cid:80) 1 ∥gt−∇f (xt)∥2 ,
n i=1nwi i i
wherew i = (cid:80)L ji Lj foralli ∈ [n],andletthestepsizesatisfy0 < γ ≤ L+LA1 Mξ(α).IfforT > 1wedefine
xˆT asanelementoftheset{x0,x1,...,xT−1}chosenuniformlyatrandom,then
E(cid:2) ∥∇f(xˆT)∥2(cid:3) ≤ 2(f(x0)−f∗) + G0 . (16)
γT θ(α)T
ThislastresulteffectivelypushestheweightsfromthealgorithminEF21-Wtotheproof,whichenabledus
toshowthattheoriginalEF21methodalsoenjoysthesameimprovement: fromL QMtoL AM.
3 EXPERIMENTS
3.1 NON-CONVEXLOGISTICREGRESSIONONBENCHMARKDATASETS
Inourfirstexperiment,weemployedalogisticregressionmodelwithanon-convexregularizer,i.e.,
f (x):= 1 (cid:80)ni log(cid:0) 1+exp(−y ·a⊤x)(cid:1) +λ (cid:80)d x2 j ,
i ni
j=1
ij ij j=1x2 j+1
where (a ,y ) ∈ Rd ×{−1,1} represents the j-th data point out from a set of n data points stored at
ij ij i
client i, and λ > 0 denotes a regularization coefficient. We utilized six datasets from LIBSVM (Chang &
Lin,2011).Thedatasetshufflingstrategy,detailedinAppendixI.5,wasemployedtoemulateheterogeneous
datadistribution. Eachclientwasassignedthesamenumberofdatapoints. Figure1providesacomparison
betweenEF21employingtheoriginalstepsize(Richta´riketal.,2021)andEF21-Wwiththebetterstepsize.
Theinitialgradientestimatorswerechosenasg0 = ∇f (x0)foralli ∈ [n]. Asevidencedempirically,the
i i
EF21-Walgorithmemergesasapracticalchoicewhenutilizedinsituationscharacterizedbyhighvariance
in smoothness constants. As evident from the plots, the algorithm employing the new step size exhibits
superiorperformancecomparedtoitspredecessor.
Next, we conducted a comparative analysis of the performance of EF21-W-PP and EF21-W-SGD, as elu-
cidated in the appendix, compared to their non-weighted counterparts. In the EF21-PP/EF21-W-PP algo-
rithms, each client participated independently in each round with probability p = 0.5. Moreover, in the
i
caseofEF21-SGD/EF21-W-SGDalgorithms,asingledatapointwasstochasticallysampledfromauniform
distribution at each client during each iteration of the algorithm. As observed in Figure 2, the algorithms
employingthenewlearningratesdemonstratefasterconvergence. Notably,Figure2(c)depictsmorepro-
nouncedoscillationswithupdatedstepsizes,asthenewanalysispermitslargerstepsizes,whichcaninduce
oscillationsinstochasticmethods.
10106 E EF F2 21 1-W E EF F2 21 1-W
101 101
6×105
102 102
EF21-W
4×105 EF21
0 10000 R2o0u0n0d0s 30000 40000 0 5000 R1o0u0n0d0s 15000 20000 0 5000 R1o0u0n0d0s 15000 20000
(a)AUSTRALIAN,L ≈1016 (b)W1A,L ≈3.28 (c)W2A,L ≈2.04
var var var
EF21-W
E EF F2 21 1-W 101 EF21 102 E EF F2 21 1-W
101
102 102 103
1030 5000 R1o0u0n0d0s 15000 20000 0 5000 R1o0u0n0d0s 15000 20000 0 5000 R1o0u0n0d0s 15000 20000
(e)MUSHROOMS,L ≈5×10−1
(d)W3A,L ≈1.58 var (f)PHISHING,L =9×10−4
var var
Figure1: ComparisonofEF21versusournewEF21-WwiththeTop1compressoronthenon-convexlogisticregression
problem. Thenumberofclientsnis1,000. ThestepsizeforEF21issetaccordingto(Richta´riketal.,2021),andthe
stepsizeforEF21-WissetaccordingtoTheorem3.Thecoefficientλfor(b)–(f)issetto0.001,andfor(a)issetto1,000
fornumericalstability.WeletL :=L2 −L2 = 1 (cid:80)n L2−(cid:0)1 (cid:80)n L (cid:1)2.
var QM AM n i=1 i n i=1 i
EF21-W-SGD EF21-W-PP 106 106
EF21-SGD EF21-PP
101 101
105 105
102 102
EF21-W-SGD EF21-W-PP
EF21-SGD EF21-PP
0 5000 10000 15000 20000 0 10000 20000 30000 40000 0 20000 40000 60000 80000 0 20000 40000 60000 80000
Rounds Rounds Rounds Rounds
(a)W1A,SGD (b)W1A,PP (c)AUSTRALIAN,SGD (d)AUSTRALIAN,PP
Figure 2: Comparison of EF21-W with partial partial participation (EF21-W-PP) or stochastic gradients (EF21-W-SGD)
versusEF21withpartialpartialparticipation(EF21-PP)orstochasticgradients(EF21-SGD)(Fatkhullinetal.,2021)).The
Top1compressorwasemployedinallexperiments.Thenumberofclientsn=1,000.Allstepsizesaretheoretical.The
coefficientλwassetto0.001for(a),(b)andto1,000for(c),(d).
3.2 NON-CONVEXLINEARMODELONSYNTHETICDATASETS
Inoursecondsetofexperiments,wetrainedalinearregressionmodelwithanon-convexregularizer. The
functionf forthelinearregressionproblemisdefinedasfollows:f (x):= 1 ∥A x−b ∥2+λ(cid:80)d x2 j .
i i ni i i j=1 x2 j+1
Here, A
i
∈ Rni×d and b
i
∈ Rni represent the feature matrix and labels stored on client i encompassing
n datapoints. Thedataemployedinfourexperiments, asillustratedinFigure3, wasgeneratedinsucha
i
mannerthatthesmoothnessconstantLremainedfixed,whileL variedsothatthedifferencebetweentwo
i
crucial to analysis terms L and L changed from a relatively large value to negligible. As evident
QM AM
from Figure 3, the performance of EF21-W consistently matches or surpasses that of the original EF21,
particularlyinscenarioscharacterizedbysignificantvariationsinthesmoothnessconstants. Foradditional
detailsandsupplementaryexperiments,wereferthereadertoSectionsIandJ.
11
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||104 102 E F 2 1 - W 103 EF21-W 103 E F 21-W
102 104 EF21 103 EF21 103 EF21
108 1010 109 109
11 00 21 04 E
E
F
F
2
2
1
1
- W 11 00 21 26 11 00 21 15 11 00 21 15
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 0 2000 4000 6000 8000 10000
Rounds Rounds Rounds Rounds
(a)L ≈4.4×106 (b)L ≈1.9×106 (c)L ≈1.0×105 (d)L ≈5.4×103
var var var var
LQM≈2126,LAM≈252 LQM≈1431,LAM≈263 LQM≈433,LAM≈280 LQM≈294,LAM≈285
Figure3: ComparisonofEF21vs.EF21-WwiththeTop1compressoronthenon-convexlinearproblem. Thenumberof
clientsnis2,000. Thecoefficientλhasbeensetto100. ThestepsizeforEF21issetaccordingto(Richta´riketal.,
2021),andthestepsizeforEF21-WissetaccordingtoTheorem3.Inallcases,thesmoothnessconstantLequals50.
12
2||)x(f|| 2||)x(f|| 2||)x(f|| 2||)x(f||ACKNOWLEDGEMENTS
ThisworkofallauthorswassupportedbytheKAUSTBaselineResearchScheme(KAUSTBRF).Thework
PeterRichta´rikandKonstantinBurlachenkowasalsosupportedbytheSDAIA-KAUSTCenterofExcellence
inDataScienceandArtificialIntelligence(SDAIA-KAUSTAI).WewishtothankBabisKostopoulos—a
VSRPinternatKAUSTwhospentsometimeworkingonthisprojectinSummer2023—forhelpingwith
somepartsoftheproject. WeofferedBabisco-authorship,buthedeclined.
REFERENCES
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-
efficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing
Systems,2017.
DanAlistarh,TorstenHoefler,MikaelJohansson,SaritKhirirat,NikolaKonstantinov,andCe´dricRenggli.
Theconvergenceofsparsifiedgradientmethods. InAdvancesinNeuralInformationProcessingSystems,
2018.
JeremyBernstein,Yu-XiangWang,KamyarAzizzadenesheli,andAnimashreeAnandkumar.signsgd:Com-
pressed optimisation for non-convex problems. In International Conference on Machine Learning, pp.
560–569.PMLR,2018.
C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer
New York, 2016. ISBN 9781493938438. URL https://books.google.com.sa/books?id=
kOXDtAEACAAJ.
KonstantinBurlachenko,SamuelHorva´th,andPeterRichta´rik. Fl pytorch: optimizationresearchsimulator
forfederatedlearning. InProceedingsofthe2ndACMInternationalWorkshoponDistributedMachine
Learning,pp.1–7,2021.
Chih-ChungChangandChih-JenLin. LIBSVM:alibraryforsupportvectormachines. ACMTransactions
onIntelligentSystemsandTechnology(TIST),2(3):1–27,2011.
Laurent Condat, Kai Yi, and Peter Richta´rik. EF-BV: A unified theory of error feedback and variance
reductionmechanismsforbiasedandunbiasedcompressionindistributedoptimization. InAdvancesin
NeuralInformationProcessingSystems,volume35,2022.
Laurent Condat, Grigory Malinovsky, and Peter Richta´rik. Tamuna: Accelerated federated learning with
localtrainingandpartialparticipation. arXivpreprintarXiv:2302.09832,2023.
Alexandred’Aspremont,DamienScieur,andAdrienTaylor. Accelerationmethods. FoundationandTrends
inOptimization,5(1–2):1–245,2021.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato,
AndrewSenior,PaulTucker,KeYang,etal. Largescaledistributeddeepnetworks. AdvancesinNeural
InformationProcessingSystems,25,2012.
IlyasFatkhullin,IgorSokolov,EduardGorbunov,ZhizeLi,andPeterRichta´rik.EF21withbells&whistles:
Practicalalgorithmicextensionsofmodernerrorfeedback. arXivpreprintarXiv:2110.03294,2021.
IlyasFatkhullin,AlexanderTyurin,andPeterRichta´rik. Momentumprovablyimproveserrorfeedback! In
AdvancesinNeuralInformationProcessingSystems,volume36,2023.
13Dragos¸Gavrilu¸t,MihaiCimpoes¸u,DanAnton,andLiviuCiortuz. Malwaredetectionusingmachinelearn-
ing. In2009InternationalMulticonferenceonComputerScienceandInformationTechnology,pp.735–
741.IEEE,2009.
EduardGorbunov, FilipHanzely, andPeterRichta´rik. AunifiedtheoryofSGD:Variancereduction, sam-
pling,quantizationandcoordinatedescent.InThe23rdInternationalConferenceonArtificialIntelligence
andStatistics,2020.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richta´rik. MARINA: Faster non-convex
distributedlearningwithcompression. In38thInternationalConferenceonMachineLearning,2021.
Samuel Horva´th, Chen-Yu Ho, Lˇudov´ıt Horva´th, Atal Narayan Sahu, Marco Canini, and Peter Richta´rik.
Naturalcompressionfordistributeddeeplearning. arXivpreprintarXiv:1905.10988,2019a.
SamuelHorva´th,DmitryKovalev,KonstantinMishchenko,SebastianStich,andPeterRichta´rik. Stochastic
distributedlearningwithgradientquantizationandvariancereduction. arXivpreprintarXiv:1904.05115,
2019b.
Samuel Horva´th, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter Richta´rik.
Naturalcompressionfordistributeddeeplearning. InMathematicalandScientificMachineLearning,pp.
129–141.PMLR,2022.
IEEEComputerSociety. IEEEStandardforFloating-PointArithmetic. Technicalreport,IEEE,2008. URL
https://ieeexplore.ieee.org/document/4610935. IEEEStd754-2008.
PeterKairouz,HBrendanMcMahan,BrendanAvent,Aure´lienBellet,MehdiBennis,ArjunNitinBhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problemsinfederatedlearning. FoundationsandTrends®inMachineLearning,14(1–2):1–210,2021.
AhmedKhaledandPeterRichta´rik.BettertheoryforSGDinthenonconvexworld.TransactionsonMachine
LearningResearch,2022.
AhmedKhaled,OthmaneSebbouh,NicolasLoizou,RobertM.Gower,andPeterRichta´rik.Unifiedanalysis
ofstochasticgradientmethodsforcompositeconvexandsmoothoptimization. JournalofOptimization
TheoryandApplications,2023.
SaritKhirirat, HamidRezaFeyzmahdavian, andMikaelJohansson. Distributedlearningwithcompressed
gradients. arXivpreprintarXiv:1806.06573,2018.
JakubKonecˇny´,H.BrendanMcMahan,DanielRamage,andPeterRichta´rik. Federatedoptimization: dis-
tributedmachinelearningforon-deviceintelligence. arXiv:1610.02527,2016a.
JakubKonecˇny´,H.BrendanMcMahan,FelixYu,PeterRichta´rik,AnandaTheerthaSuresh,andDaveBa-
con. Federatedlearning: strategiesforimprovingcommunicationefficiency. InNIPSPrivateMulti-Party
MachineLearningWorkshop,2016b.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neuralnetworks. InAdvancesinNeuralInformationProcessingSystems,volume25,2012.
TianLi,AnitKumarSahu,AmeetTalwalkar,andVirginiaSmith. Federatedlearning: Challenges,methods,
andfuturedirections. IEEESignalProcessingMagazine,37:50–60,2020a.
XiangLi,KaixuanHuang,WenhaoYang,ShusenWang,andZhihuaZhang. OntheconvergenceofFedAvg
onnon-iiddata. arXivpreprintarXiv:1907.02189,2019.
14ZhizeLi,DmitryKovalev,XunQian,andPeterRichta´rik. Accelerationforcompressedgradientdescentin
distributedandfederatedoptimization. InInternationalConferenceonMachineLearning,2020b.
ZhizeLi, HongyanBao, XiangliangZhang, andPeterRichta´rik. PAGE:Asimpleandoptimalprobabilis-
tic gradient estimator for nonconvex optimization. In International Conference on Machine Learning
(ICML),2021.
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human false-
hoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 3214–3252, Dublin, Ireland, May 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.
acl-long.229.
TaoLin,SebastianUStich,KumarKshitijPatel,andMartinJaggi. Don’tuselargemini-batches,uselocal
SGD. arXivpreprintarXiv:1808.07217,2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence
andstatistics,pp.1273–1282.PMLR,2017.
Konstantin Mishchenko, Eduard Gorbunov, Martin Taka´cˇ, and Peter Richta´rik. Distributed learning with
compressedgradientdifferences. arXivpreprintarXiv:1901.09269,2019.
KonstantinMishchenko, GrigoryMalinovsky, SebastianStich, andPeterRichta´rik. Proxskip: Yes! Local
gradient steps provably lead to communication acceleration! Finally! In International Conference on
MachineLearning,pp.15750–15769.PMLR,2022.
YuriiNesterov. Amethodforunconstrainedconvexminimizationproblemwiththerateofconvergenceo
(1/kˆ2),1983. URLhttps://cir.nii.ac.jp/crid/1370576118744597902.
YuriiNesterov.Introductorylecturesonconvexoptimization:abasiccourse(AppliedOptimization).Kluwer
AcademicPublishers,2004.
CeylanOnayandElifO¨ztu¨rk.Areviewofcreditscoringresearchintheageofbigdata.JournalofFinancial
RegulationandCompliance,26(3):382–405,2018.
RyanPoplin,AvinashV.Varadarajan,KatyBlumer,YunLiu,MichaelV.McConnell,GregoryS.Corrado,
LilyPeng, andDaleR.Webster. Predictingcardiovascularriskfactorsfromretinalfundusphotographs
usingdeeplearning. arXivpreprintarXiv:1708.09843,2017.
PeterRichta´rik,IgorSokolov,andIlyasFatkhullin. EF21: Anew,simpler,theoreticallybetter,andpracti-
callyfastererrorfeedback. InAdvancesinNeuralInformationProcessingSystems,volume34,2021.
PeterRichta´rik,ElnurGasanov,andKonstantinBurlachenko. Errorfeedbackshineswhenfeaturesarerare.
arXivpreprintarXiv:2305.15264,2023.
MherSafaryan,EgorShulgin,andPeterRichta´rik. Uncertaintyprincipleforcommunicationcompressionin
distributedandfederatedlearningandthesearchforanoptimalcompressor. InformationandInference:
A Journal of the IMA, 11(2):557–580, 04 2021. ISSN 2049-8772. doi: 10.1093/imaiai/iaab006. URL
https://doi.org/10.1093/imaiai/iaab006.
Mher Safaryan, Rustem Islamov, Xun Qian, and Peter Richta´rik. FedNL: Making Newton-type methods
applicabletofederatedlearning. InInternatioanlConferenceonMachineLearning,2022.
15Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the
InternationalSpeechCommunicationAssociation,2014.
S.Shalev-ShwartzandS.Ben-David. UnderstandingMachineLearning: FromTheorytoAlgorithms. Un-
derstanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. ISBN
9781107057135. URLhttps://books.google.com.sa/books?id=ttJkAwAAQBAJ.
Sebastian U Stich. Local SGD converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018.
SebastianU.Stich,J.-B.Cordonnier,andMartinJaggi.SparsifiedSGDwithmemory.InAdvancesinNeural
InformationProcessingSystems,2018.
Yu Sun, Yuan Liu, Guan Wang, Haiyan Zhang, et al. Deep learning for plant identification in natural
environment. ComputationalIntelligenceandNeuroscience,2017.
Rafał Szlendak, Alexander Tyurin, and Peter Richta´rik. Permutation compressors for provably faster dis-
tributednonconvexoptimization. InInternationalConferenceonLearningRepresentations,2022.
AlexanderTyurinandPeterRichta´rik. DASHA:Distributednonconvexoptimizationwithcommunication
compression and optimal oracle complexity. In International Conference on Learning Representations,
2023.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,LukaszKaiser,
andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInformationProcessingSystems,
volume30,2017.
JoostVerbraeken,MatthijsWolting,JonathanKatzy,JeroenKloppenburg,TimVerbelen,andJanSReller-
meyer. Asurveyondistributedmachinelearning. ACMComputingSurveys,53(2):1–33,2020.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient com-
pressionfordistributedoptimization. InAdvancesinNeuralInformationProcessingSystems,2019.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y Arcas,
Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, Suhas Dig-
gavi, Hubert Eichner, Advait Gadhikar, Zachary Garrett, Antonious M. Girgis, Filip Hanzely, Andrew
Hard, Chaoyang He, Samuel Horvath, Zhouyuan Huo, Alex Ingerman, Martin Jaggi, Tara Javidi, Pe-
ter Kairouz, Satyen Kale, Sai Praneeth Karimireddy, Jakub Konecny, Sanmi Koyejo, Tian Li, Luyang
Liu,MehryarMohri,HangQi,SashankJ.Reddi,PeterRichtarik,KaranSinghal,VirginiaSmith,Mahdi
Soltanolkotabi, Weikang Song, Ananda Theertha Suresh, Sebastian U. Stich, Ameet Talwalkar, Hongyi
Wang, Blake Woodworth, Shanshan Wu, Felix X. Yu, Honglin Yuan, Manzil Zaheer, Mi Zhang, Tong
Zhang, Chunxiang Zheng, Chen Zhu, and Wennan Zhu. A field guide to federated optimization. arXiv
preprintarXiv:2107.06917,2021.
JueWang, YuchengLu, BinhangYuan, BeidiChen, PercyLiang, ChristopherDeSa, ChristopherRe, and
CeZhang. Cocktailsgd: Fine-tuningfoundationmodelsover500mbpsnetworks. InInternationalCon-
ferenceonMachineLearning,pp.36058–36076.PMLR,2023.
Tao Yang, Xinlei Yi, Junfeng Wu, Ye Yuan, Di Wu, Ziyang Meng, Yiguang Hong, Hong Wang, Zongli
Lin, and Karl H. Johansson. A survey of distributed optimization. Annual Reviews in Control, 47:
278–305, 2019. ISSN 1367-5788. doi: https://doi.org/10.1016/j.arcontrol.2019.05.006. URL https:
//www.sciencedirect.com/science/article/pii/S1367578819300082.
16PeilinZhaoandTongZhang. Stochasticoptimizationwithimportancesamplingforregularizedlossmin-
imization. InFrancisBachandDavidBlei(eds.), Proceedingsofthe32ndInternationalConferenceon
Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1–9, Lille, France,
07–09Jul2015.PMLR. URLhttps://proceedings.mlr.press/v37/zhaoa15.html.
17A BASIC RESULTS AND LEMMAS
Inthissection,weofferafewresultsthatserveasessentialprerequisitesforestablishingthemainfindings
inthepaper.
A.1 OPTIMALCLIENTCLONINGFREQUENCIES
Lemma1(Optimalweights). Leta >0fori∈[n]. Then
i
(cid:88)n a2 (cid:32) (cid:88)n (cid:33)2
min i = a , (17)
w1(cid:80)>
n
i0 =, 1... w,w i=n> 10
i=1
w i
i=1
i
whichisachievedwhenw i∗ = (cid:80)a ji aj. Thismeansthat
(cid:118)
1(cid:117) (cid:117)(cid:88)n a2 1 (cid:88)n
w1(cid:80)>
n
i0
=m
,
1..i
.
wn
,w i=n>
10n(cid:116)
i=1
wi
i
=
n
i=1a i. (18)
(cid:108) (cid:109) √
WenowshowthatthecloningfrequenciesgivenbyN⋆ = Li forma 2-approximationfortheopti-
i LAM
mizationproblemoffindingtheoptimalintegerclientfrequencies.
√ (cid:108) (cid:109)
Lemma2( 2-approximation). IfweletN⋆ = Li foralli∈[n],then
i LAM
√
L ≤ min M(N ,...,N )≤M(N⋆,...,N⋆)≤ 2L .
AM N1∈N,...,Nn∈N 1 n 1 n AM
Proof. Recallthat
(cid:118)
1(cid:117) (cid:117)(cid:88)n L2
M(N 1,...,N n):= n(cid:116)
N
/i N.
i
i=1
Thefirstinequalityintheclaimfollowsbyrelaxingtheintegralityconstraints,whichgivesusthebound
(cid:118) (cid:118)
1(cid:117) (cid:117)(cid:88)n L2 1(cid:117) (cid:117)(cid:88)n L2
min (cid:116) i ≤ min (cid:116) i ,
w1(cid:80)>
n
i0 =, 1... w,w i=n> 10n
i=1
w i N1∈N,...,Nn∈Nn
i=1
N i/N
andsubsequentlyapplyingLemma17.
Next,wearguethatthequantityN⋆ :=(cid:80) N⋆isatmost2n. Indeed,
i i
n n (cid:24) (cid:25) n (cid:18) (cid:19)
N⋆ =(cid:88) N⋆ =(cid:88) L i ≤(cid:88) L i +1 =2n. (19)
i L L
AM AM
i=1 i=1 i=1
WewillnowusethistoboundM(N⋆,...,N⋆)fromabove:
1 n
(cid:118) √ (cid:118) √ (cid:118)
M(N 1⋆,...,N n⋆)= n1(cid:117) (cid:117) (cid:116)(cid:88)n N⋆L /2 i
N⋆
( =19) √ n2(cid:117) (cid:117) (cid:116)(cid:88)n NL2 i
⋆
= √ n2(cid:117) (cid:117) (cid:116)(cid:88)n L NL Ai M
⋆
L iL AM.
i=1 i i=1 i i=1 i
Li
Since LAM ≤1foralli∈[n],theproofisfinishedasfollows:
N⋆
i
√ (cid:118) √ (cid:118)
2(cid:117) (cid:117)(cid:88)n 2(cid:112) (cid:117) (cid:117)(cid:88)n √
M(N 1⋆,...,N n⋆)≤ √ n(cid:116) L iL AM = √ n L AM(cid:116) L i = 2L AM.
i=1 i=1
18A.2 DESCENTLEMMA
Lemma3(Lietal.(2021)). LetAssumption1holdandxt+1 =xt−γgt,wheregt ∈Rdisanyvector,and
γ >0isanyscalar. Then,wehave
(cid:18) (cid:19)
γ 1 L γ
f(xt+1)≤f(xt)− ∥∇f(xt)∥2− − ∥xt+1−xt∥2+ ∥gt−∇f(xt)∥2. (20)
2 2γ 2 2
A.3 YOUNG’SINEQUALITY
Lemma4(Young’sinequality). Foranya,b∈Rdandanypositivescalars>0itholdsthat
∥a+b∥2 ≤(1+s)∥a∥2+(1+s−1)∥b∥2. (21)
A.4 2-SUBOPTIMALBUTSIMPLESTEPSIZERULE
Lemma5(Lemma5,Richta´riketal.(2021)). Leta,b>0.If0<γ ≤ √1 ,thenaγ2+bγ ≤1.Moreover,
a+b
theboundistightuptothefactorof2since
(cid:26) (cid:27)
1 1 1 2
√ ≤min √ , ≤ √ .
a+b a b a+b
A.5 OPTIMALCOEFFICIENTINYOUNG’SINEQUALITY
Lemma6(Lemma3,Richta´riketal.(2021)). Let0<α≤1andfors>0,letθ(α,s):=1−(1−α)(1+s)
andβ(α,s):=(1−α)(cid:0) 1+s−1(cid:1)
. Then,thesolutionoftheoptimizationproblem
(cid:26) (cid:27)
β(α,s) α
min :0<s< (22)
s θ(α,s) 1−α
√
isgivenbys∗ = √1 −1. Furthermore,θ(α,s∗)=1− 1−α,andβ(α,s∗)= 1√−α .
1−α 1− 1−α
19B CLONING REFORMULATION FOR POLYAK-ŁOJASCHEWITZ FUNCTIONS
Forcompleteness,wealsoprovideaseriesofconvergenceresultsunderPolyak-Łojasiewiczcondition. We
commenceourexpositionwiththesubsequentdefinition.
Assumption4(Polyak-Łojasiewicz). Thereexistsapositivescalarµ > 0suchthatforallpointsx ∈ Rd,
thefollowinginequalityissatisfied:
1
f(x)−f(x∗)≤ ∥∇f(x)∥2, (23)
2µ
wherex∗ :=argminf(x).
Theorem5. LetAssumptions1,2,and4hold. AssumethatCt ∈ C(α)foralli ∈ [n]andt ≥ 0. Consider
i
Algorithm 1 (EF21) applied to the “cloning” reformulation 9 of the distributed optimization problem (1),
whereN∗ =⌈ Li ⌉foralli∈[n]. Letthestepsizebesetas
i LAM
 
(cid:32) (cid:114) (cid:33)−1
 √ 2β θ 
0≤γ ≤min L+ 2L , ,
AM θ 2µ
 
√
whereθ =1− 1−αandβ = 1√−α . Let
1− 1−α
γ
Ψt :=f(xt)−f(x∗)+ Gt.
θ
Then,foranyT ≥0,wehave
E(cid:2) ΨT(cid:3) ≤(1−γµ)TΨ0.
Proof. ThistheoremisacorollaryofTheorem2in(Richta´riketal.,2021)andLemma2.
20C PROOF OF THEOREM 3 (THEORY FOR EF21-W)
Inthissection,wepresentaproofforTheorem3. Tostartthisproof,weestablishacorrespondingcontrac-
tionlemma. Wedefinethefollowingquantities:
Gt
i
:=(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
; Gt
:=(cid:88)n
w iGt i, (24)
i
i=1
wheretheweightsw aredefinedasspecifiedinAlgorithm2,thatis,
i
L
w = i . (25)
i (cid:80)n
L
j=1 j
C.1 ALEMMA
Withthesedefinitionsinplace,wearenowpreparedtoproceedtothelemma.
Lemma7. LetCt ∈C(α)foralli∈[n]andt≥0.LetWt :={gt,gt,...,gt,xt,xt+1}.Then,foriterates
i 1 2 n
ofAlgorithm2wehave
E(cid:2) Gt i+1 |Wt(cid:3) ≤(1−θ(α,s))Gt i+β(α,s) n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2 , (26)
i
and
E(cid:2) Gt+1(cid:3) ≤(1−θ(α,s))E(cid:2) Gt(cid:3) +β(α,s)L2 E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) , (27)
AM
wheres>0isanarbitrarypositivescalar,and
θ(α,s):=1−(1−α)(1+s), and
β(α,s):=(1−α)(cid:0) 1+s−1(cid:1)
. (28)
Proof. Theproofisstraightforwardandbearsresemblancetoasimilarprooffoundinapriorwork(Richta´rik
etal.,2021).
E(cid:2) Gt+1 |Wt(cid:3) ( =24)
E(cid:34)(cid:13)
(cid:13) (cid:13)gt+1− ∇f
i(xt+1)(cid:13)
(cid:13)
(cid:13)2 |Wt(cid:35)
i (cid:13) i nw (cid:13)
i
=
E(cid:34)(cid:13) (cid:13) (cid:13)gt+Ct(cid:18) ∇f i(xt+1) −gt(cid:19)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i i nw i nw (cid:13)
i i
( ≤4) (1−α)(cid:13) (cid:13) (cid:13)∇f i(xt+1) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw i(cid:13)
i
=
(1−α)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
∇f i(xt)
+
∇f i(xt) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw nw nw i(cid:13)
i i i
( ≤21) (1−α)(1+s)(cid:13) (cid:13) (cid:13)∇f i(xt) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw i(cid:13)
i
+(1−α)(cid:0) 1+s−1(cid:1) n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2 ,
i
withthefinalinequality holdingforanypositivescalar s > 0. Consequently, we havesuccessfullyestab-
lishedthefirstpartofthelemma.
21By employing (24) and the preceding inequality, we can derive the subsequent bound for the conditional
expectationofGt+1:
(cid:34) n (cid:35)
E(cid:2) Gt+1 |Wt(cid:3) ( =24) E (cid:88) w Gt+1 |Wt
i i
i=1
( =24)
(cid:88)n
w
iE(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)g it+1− ∇f i n(
wxt+1)(cid:13)
(cid:13) (cid:13)
(cid:13)2 |Wt(cid:35)
i
i=1
( ≤26)
(1−θ(α,s))(cid:88)n
w
i(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
i
i=1
n
+β(α,s)(cid:88) ww 2ni
2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2 . (29)
i=1 i
ApplyingAssumption2and(25),wefurtherproceedto:
E(cid:2) Gt+1 |Wt(cid:3) ( ≤29)
(1−θ(α,s))(cid:88)n
w
i(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2 +β(α,s)(cid:88)n
ww 2ni
2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i=1 i i=1 i
n
( =24) (1−θ(α,s))Gt+β(α,s)(cid:88) w1
n2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
i=1
( ≤7) (1−θ(α,s))Gt+β(α,s)(cid:32) (cid:88)n L2 i (cid:33) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2
w n2
i
i=1
 
( =25)
(1−θ(α,s))Gt+β(α,s)(cid:88)n L2
i (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2
i=1 (cid:80)n jL =1i Ljn2
=
(1−θ(α,s))Gt+β(α,s)(cid:32) (cid:88)n L i(cid:80)n j=1L j(cid:33)
(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2
n2
i=1
= (1−θ(α,s))Gt+β(α,s)L2 (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 . (30)
AM
Usingthetowerproperty,weget
E(cid:2) Gt+1(cid:3) =E(cid:2)E(cid:2) Gt+1 |Wt(cid:3)(cid:3)( ≤30) (1−θ(α,s))E(cid:2) Gt(cid:3) +β(α,s)L2 E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) ,
AM
andthisfinalizestheproof.
C.2 MAINRESULT
WearenowpreparedtoestablishtheproofforTheorem3.
Proof. Notethat,accordingto(13),thegradientestimateforAlgorithm2getsthefollowingform:
n
(cid:88)
gt = w gt. (31)
i i
i=1
22UsingLemma3andJensen’sinequalityappliedtothefunctionx(cid:55)→∥x∥2 (since(cid:80)n w =1),weobtain
i=1 i
thefollowingbound:
f(xt+1) ( ≤20) f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 −(cid:18) 21
γ
− L 2(cid:19) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:13) (cid:13) (cid:13) (cid:13)gt−(cid:88)n ∇f i(xt)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) (cid:13)
i=1
( =31) f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 −(cid:18) 21
γ
− L 2(cid:19) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)n w i(cid:18) g it− ∇f ni w(xt)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) i (cid:13)
i=1
≤ f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2
−(cid:18)
21
γ
− L
2(cid:19)
(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:88)n
w
i(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
i
i=1
(cid:18) (cid:19)
( =24) f(xt)− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt. (32)
2 2γ 2 2
Subtractingf∗frombothsidesandtakingexpectation,weget
E(cid:2) f(xt+1)−f∗(cid:3) ≤E(cid:2) f(xt)−f∗(cid:3)
−
γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
2
−(cid:18) 1
−
L(cid:19) E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
+
γ E(cid:2) Gt(cid:3)
. (33)
2γ 2 2
Letδt := E[f(xt)−f∗],st := E[Gt]andrt := E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) .Subsequently,byadding(27)witha
γ multiplier,weobtain
2θ(α,s)
(cid:18) (cid:19)
δt+1+ γ st+1 ( ≤33) δt− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L rt+ γ st+ γ st+1
2θ(α,s) 2 2γ 2 2 2θ
(cid:18) (cid:19)
( ≤27) δt− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L rt+ γ st
2 2γ 2 2
+ γ (cid:0) β(α,s)L2 rt+(1−θ(α,s))st(cid:1)
2θ(α,s) AM
(cid:18) (cid:19)
= δt+ γ st− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L − γ β(α,s)L2 rt
2θ(α,s) 2 2γ 2 2θ(α,s) AM
≤ δt+ γ st− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 .
2θ(α,s) 2
Thelastinequalityisaresultoftheboundγ2β(α,s)L2
AM +Lγ ≤1,whichissatisfiedforthestepsize
θ(α,s)
1
γ ≤ ,
L+L ξ(α,s)
AM
(cid:113)
whereξ(α,s) := β(α,s). MaximizingthestepsizeboundoverthechoiceofsusingLemma6,weobtain
θ(α,s)
thefinalstepsize. Bysummingupinequalitiesfort=0,...,T −1,weget
T−1
0≤δT + γ sT ≤δ0+ γ s0− γ (cid:88) E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) .
2θ 2θ 2
t=0
Multiplyingbothsidesby 2 ,afterrearrangingweget
γT
T (cid:88)−1 1 E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
≤
2δ0
+
s0
.
T γT θT
t=0
23ItremainstonoticethatthelefthandsidecanbeinterpretedasE(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105)
,wherexˆT ischosenfrom
{x0,x1,...,xT−1}uniformlyatrandom.
C.3 MAINRESULTFORPOLYAK-ŁOJASIEWICZFUNCTIONS
Themainresultispresentednext.
Theorem6. LetAssumptions1,2, and4hold. AssumethatCt ∈ C(α)foralli ∈ [n]andt ≥ 0. Letthe
i
stepsizeinAlgorithm2besetas
(cid:26) (cid:27)
1 θ(α)
0<γ ≤min √ , .
L+ 2L ξ(α) 2µ
AM
Let
γ
Ψt :=f(xt)−f(x∗)+ Gt.
θ
Then,foranyT >0thefollowinginequalityholds:
E(cid:2) ΨT(cid:3) ≤(1−γµ)TΨ0. (34)
Proof. Weproceedasinthepreviousproof,startingfromthedescentlemmawiththesamevectorbutusing
thePLinequalityandsubtractingf(x⋆)frombothsides:
(cid:18) (cid:19)
E(cid:2) f(xt+1)−f(x⋆)(cid:3) ( ≤20) E(cid:2) f(xt)−f(x⋆)(cid:3) − γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt
2 2γ 2 2
(cid:18) (cid:19)
( ≤23) (1−γµ)E(cid:2) f(xt)−f(x⋆)(cid:3) − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt. (35)
2γ 2 2
Letδt :=E[f(xt)−f(x⋆)],st :=E[Gt]andrt :=E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) . Thus,
(cid:18) (cid:19)
γ (44) 1 L γ γ
δt+1+ st+1 ≤ (1−γµ)δt− − rt+ st+ st+1
θ 2γ 2 2 θ
(cid:18) (cid:19)  (cid:32) n (cid:33)2 
(27) 1 L γ γ 1 (cid:88)
≤ (1−γµ)δt− 2γ − 2 rt+ 2st+ θ (1−θ)st+β n L i rt 
i=1
γ (cid:18) θ(cid:19) (cid:18) 1 L βL2 γ(cid:19)
= (1−γµ)δt+ 1− st− − − AM rt,
θ 2 2γ 2 θ
whereθ andβ aresetasinLemma6. Notethatourextraassumptiononthestepsizeimpliesthat1− θ ≤
2
1−γµand
1 L βL2 γ
− − AM ≥0.
2γ 2 θ
Thelastinequalityfollowsfromtheboundγ22βL2
AM +γL≤1. Thus,
θ
γ (cid:16) γ (cid:17)
δt+1+ st+1 ≤(1−γµ) δt+ st .
θ θ
Itremainstounrolltherecurrence.
24D PROOF OF THEOREM 4 (IMPROVED THEORY FOR EF21)
Wecommencebyredefininggradientdistortionasfollows:
n
1 (cid:88) 1
Gt := ∥∇f (xt)−gt∥2. (36)
n2 w i i
i
i=1
WerecallthatthegradientupdatestepforstandardEF21(Algorithm1)takesthefollowingform:
gt+1 =gt+Ct(∇f (xt+1)−gt), (37)
i i i i i
n
1 (cid:88)
gt+1 = gt+1. (38)
n i
i=1
D.1 TWOLEMMAS
Oncemore,westartourproofwiththecontractionlemma.
Lemma 8. Let Ct ∈ C(α) for all i ∈ [n] and t ≥ 0. Define Wt := {gt,gt,...,gt,xt,xt+1}. Let
i 1 2 n
Assumption2hold. Then
E(cid:2) Gt+1 |Wt(cid:3) ≤(1−θ(α,s))Gt+β(α,s)L2 ∥xt+1−xt∥2, (39)
AM
whereθ(α,s):=1−(1−α)(1+s)andβ(α,s):=(1−α)(1+s−1)foranys>0.
Proof. TheproofofthislemmastartsasthesimilarlemmainthestandardanalysisofEF21:
n
E(cid:2) Gt+1 |Wt(cid:3) ( =36) 1 (cid:88) 1 E(cid:2) ∥∇f (xt+1)−gt+1∥2 |Wt(cid:3)
n2 w i i
i
i=1
n
( =37) 1 (cid:88) 1 E(cid:2) ∥gt+Ct(∇f (xt+1)−gt)−∇f (xt+1)∥2 |Wt(cid:3)
n2 w i i i i i
i
i=1
n
(4) 1 (cid:88)1−α
≤ ∥∇f (xt+1)−gt)∥2
n2 w i i
i
i=1
n
1 (cid:88)1−α
= ∥∇f (xt+1)−∇f (xt)+∇f (xt)−gt)∥2
n2 w i i i i
i
i=1
n
( ≤21) 1 (cid:88)1−α(cid:0) (1+s−1)∥∇f (xt+1)−∇f (xt))∥2+(1+s)∥gt−∇f (xt)∥2(cid:1)
n2 w i i i i
i
i=1
(40)
25foralls>0. Weproceedtheproofasfollows:
n
E(cid:2) Gt+1 |Wt(cid:3) ( ≤40) 1 (cid:88)1−α(cid:0) (1+s−1)∥∇f (xt+1)−∇f (xt))∥2+(1+s)∥gt−∇f (xt)∥2(cid:1)
n2 w i i i i
i
i=1
n n
1 (cid:88) 1 β(α,s)(cid:88) 1
= (1−θ(α,s)) ∥gt−∇f (xt)∥2+ ∥∇f (xt+1)−∇f (xt))∥2
n2 w i i n2 w i i
i i
i=1 i=1
n
( =36) (1−θ(α,s))Gt+
β(α,s)(cid:88) 1
∥∇f (xt+1)−∇f (xt)∥2
n2 w i i
i
i=1
(7) β(α,s)(cid:88)n L2
≤ (1−θ(α,s))Gt+ i∥xt+1−xt∥2. (41)
n2 w
i
i=1
Notethatthisistheexactplacewherethecurrentanalysisdiffersfromthestandardone. Itfullycoincides
with it when w = 1, i.e., when we assign the same weight for each individual gradient distortion ∥gt −
i n i
∇f (xt)∥2. However,applyingweightsaccordingto“importance”ofeachfunction,weproceedasfollows:
i
E(cid:2) Gt+1 |Wt(cid:3) ( ≤41) (1−θ(α,s))Gt+ β(α,s)(cid:88)n L2 i∥xt+1−xt∥2
n2 w
i
i=1
( =25) (1−θ(α,s))Gt+
β(α,s)(cid:88)n L2
i
(cid:32) (cid:88)n
L
(cid:33)
∥xt+1−xt∥2
n2 L i
i
i=1 i=1
(cid:32) n (cid:33)
β(α,s)(cid:88) (cid:88)
= (1−θ(α,s))Gt+ L L ∥xt+1−xt∥2
n2 j i
j i=1
= (1−θ(α,s))Gt+β(α,s)L2 ∥xt+1−xt∥2,
AM
whatfinishestheproof.
Toprovethemainconvergencetheorem,wealsoneedthefollowinglemma.
Lemma9. ForthevariablegtfromAlgorithm1,thefollowinginequalityholds:
∥gt−∇f(xt)∥2 ≤Gt. (42)
Proof. Theproofisstraightforward:
∥gt−∇f(xt)∥2 ( =38) (cid:13) (cid:13) (cid:13)(cid:88)n 1 (cid:0) gt−∇f (xt)(cid:1)(cid:13) (cid:13) (cid:13)2
(cid:13) n i i (cid:13)
(cid:13) (cid:13)
i=1
= (cid:13) (cid:13) (cid:13)(cid:88)n w 1 (cid:0) gt−∇f (xt)(cid:1)(cid:13) (cid:13) (cid:13)2
(cid:13) iw n i i (cid:13)
(cid:13) i (cid:13)
i=1
≤ (cid:88)n w i(cid:13) (cid:13) (cid:13) (cid:13)w1 n(cid:0) g it−∇f i(xt)(cid:1)(cid:13) (cid:13) (cid:13) (cid:13)2
i
i=1
n
= (cid:88) 1 ∥gt−∇f (xt)∥2 ( =36) Gt,
w n2 i
i
i=1
wheretheonlyinequalityinthisseriesofequationsisderivedusingJensen’sinequality.
26D.2 MAINRESULT
Wearenowequippedwithallthenecessarytoolstoestablishtheconvergencetheorem.
Proof. LetusdefinetheLyapunovfunction
γ
Φt :=f(xt)−f∗+ Gt.
2θ(α,s)
LetusalsodefineWt :={gt,gt,...,gt,xt,xt+1}. Westartasfollows:
1 2 n
E(cid:2) Φt+1 |Wt(cid:3)
= E(cid:2) f(xt+1)−f∗ |Wt(cid:3) + γ E(cid:2) Gt+1 |Wt(cid:3)
2θ(α,s)
(cid:18) (cid:19)
(20) γ 1 L γ
≤ f(xt)−f∗− ∥∇f(xt)∥2− − ∥xt+1−xt∥2+ ∥gt−∇f(xt)∥2
2 2γ 2 2
+ γ E(cid:2) Gt+1 |Wt(cid:3)
2θ(α,s)
(cid:18) (cid:19)
(42) γ 1 L γ
≤ f(xt)−f∗− ∥∇f(xt)∥2− − ∥xt+1−xt∥2+ Gt
2 2γ 2 2
+ γ E(cid:2) Gt+1 |Wt(cid:3)
2θ(α,s)
(cid:18) (cid:19)
(39) γ 1 L γ
≤ f(xt)−f∗− ∥∇f(xt)∥2− − ∥xt+1−xt∥2+ Gt
2 2γ 2 2
+ γ (cid:0) (1−θ(α,s))Gt+β(α,s)L2 ∥xt+1−xt∥2(cid:1)
2θ(α,s) AM
(cid:18) (cid:19)
γ γ 1 L γβ(α,s)
= f(xt)−f∗+ Gt− ∥∇f(xt)∥2− − − L2 ∥xt+1−xt∥2
2θ(α,s) 2 2γ 2 2θ(α,s) AM
(cid:124) (cid:123)(cid:122) (cid:125)
≥0
γ γ
≤ f(xt)−f∗+ Gt− ∥∇f(xt)∥2
2θ(α,s) 2
γ
= Φt− ∥∇f(xt)∥2.
2
Theinequalityinthelastbutonelineisvalidif
1
γ ≤ ,
(cid:113)
L+L β(α,s)
AM θ(α,s)
according to Lemma 5. By optimizing the stepsize bound through the selection of s in accordance with
Lemma6,wederivethefinalstepsizeandestablishtheoptimalvalueforθindefiningtheLyapunovfunction.
Applyingthetowerpropertyandunrollingtherecurrence,wefinishtheproof.
D.3 MAINRESULTFORPOLYAK-ŁOJASIEWICZFUNCTIONS
Forcompleteness,wealsoprovideaconvergenceresultunderPolyak-Łojasiewiczcondition(Assumption4).
Themainresultispresentednext.
27Theorem7. LetAssumptions1,2, and4hold. AssumethatCt ∈ C(α)foralli ∈ [n]andt ≥ 0. Letthe
i
stepsizeinAlgorithm2besetas
(cid:26) (cid:27)
1 θ(α,s)
0<γ ≤min √ , .
L+ 2L ξ(α) 2µ
AM
Let
γ
Ψt :=f(xt)−f(x∗)+ Gt.
θ(α,s)
Then,foranyT >0thefollowinginequalityholds:
E(cid:2) ΨT(cid:3) ≤(1−γµ)TΨ0. (43)
Proof. Weproceedasinthepreviousproof,startingfromthedescentlemmawiththesamevectorbutusing
thePLinequalityandsubtractingf(x⋆)frombothsides:
(cid:18) (cid:19)
E(cid:2) f(xt+1)−f(x⋆)(cid:3) ( ≤20) E(cid:2) f(xt)−f(x⋆)(cid:3) − γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt
2 2γ 2 2
(cid:18) (cid:19)
( ≤23) (1−γµ)E(cid:2) f(xt)−f(x⋆)(cid:3) − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt. (44)
2γ 2 2
Letδt :=E[f(xt)−f(x⋆)],st :=E[Gt]andrt :=E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) . Thus,
(cid:18) (cid:19)
γ (44) 1 L γ γ
δt+1+ st+1 ≤ (1−γµ)δt− − rt+ st+ st+1
θ(α,s) 2γ 2 2 θ(α,s)
(cid:18) (cid:19)
(39) 1 L γ
≤ (1−γµ)δt− − rt+ st
2γ 2 2
 (cid:32) n (cid:33)2 
γ 1 (cid:88)
+ θ(α,s)(1−θ(α,s))st+β n L i rt 
i=1
γ (cid:18) θ(α,s)(cid:19) (cid:18) 1 L βL2 γ(cid:19)
= (1−γµ)δt+ 1− st− − − AM rt.
θ(α,s) 2 2γ 2 θ(α,s)
Notethatourextraassumptiononthestepsizeimpliesthat1− θ(α,s) ≤1−γµand
2
1 L βL2 γ
− − AM ≥0.
2γ 2 θ(α,s)
Thelastinequalityfollowsfromtheboundγ22βL2
AM +γL≤1. Thus,
θ(α,s)
(cid:18) (cid:19)
γ γ
δt+1+ st+1 ≤(1−γµ) δt+ st .
θ(α,s) θ(α,s)
Itremainstounrolltherecurrencewhichfinishestheprove.
28E EF21-W-SGD: WEIGHTED ERROR FEEDBACK 2021 WITH STOCHASTIC
SUBSAMPLED GRADIENTS
The EF21-W algorithm assumes that all clients can compute the exact gradient in each round. In some
scenarios,theexactgradientsmaybeunavailableortoocostlytocompute,andonlyapproximategradient
estimatorscanbeobtained.Inthissection,wepresenttheconvergenceresultforEF21-Winthesettingwhere
thegradientcomputationontheclients, ∇f (xt+1), isreplacedbyaspecificstochasticgradientestimator.
i
ForavariationofEF21-W-SGDwhichisworkingunderamoregeneralsettingpleaseseeAppendixF.
E.1 ALGORITHM
Inthissection,weextendEF21-Wtohandlestochasticgradients,andwecalltheresultingalgorithmEF21-
W-SGD (Algorithm 3). Our analysis of this extension follows a similar approach as the one used by
Fatkhullin et al. (2021) for studying the stochastic gradient version of the vanilla EF21 algorithm, which
theycalledEF21-SGD.AnalysisofEF21-W-SGDhastwoimportantdifferenceswithvanillaEF21-SGD:
(cid:18) (cid:113) (cid:19)−1
1. Vanilla EF21-SGD provides maximum theoretically possible γ = L+L QM β θ1 , where
(cid:18) (cid:113) (cid:19)−1
EF21-W-SGDhasγ = L+L
AM
β θ1
2. VanillaEF21-SGDandEF21-W-SGDformallydiffersinawayhowitreportsiteratexT whichmin-
imizesE(cid:104)(cid:13) (cid:13)∇f(xT)(cid:13) (cid:13)2(cid:105)
duetoaslightlydifferentdefinitionofA(cid:101).TheEF21-W-SGD(Algorithm3)
requiresoutputiteratexˆT randomlyaccordingtotheprobabilitymassfunctiondescribedby(49).
Algorithm3EF21-W-SGD:WeightedErrorFeedback2021withStochasticGradients
1: Input: initialmodelx0 ∈Rd;initialgradientestimatesg0,g0,...,g0 ∈Rdstoredattheserverandthe
1 2 n
2:
c Inli ie tn iats l; izs et :ep gs 0iz =eγ (cid:80)>
n
i=0 1; wnu igm i0b oe nr to hf eit se er ra vt eio rnsT >0;weightsw i = (cid:80)L ji Lj fori∈[n]
3: fort=0,1,2,...,T −1do
4: Servercomputesxt+1 =xt−γgtandbroadcastsxt+1toallnclients
5: fori=1,...,nontheclientsinparalleldo
6: Computeastochas (cid:16)ticestimatorgˆ i(xt+ (cid:17)1)= τ1
i
(cid:80)τ ji =1∇f ξ it j(xt+1)ofthegradient∇f i(xt+1)
7: Computeut
i
=C it nw1 igˆ i(xt+1)−g it andupdateg it+1 =g it+ut
i
8: Sendthecompressedmessageuttotheserver
i
9: endfor
10: Serverupdatesg it+1 =g it+ut
i
foralli∈[n],andcomputesgt+1 =(cid:80)n i=1w ig it+1
11: endfor
12: Output: PointxˆT chosenfromtheset{x0,...,xT−1}randomlyaccordingtothelaw(49)
Assumption5(Generalassumptionforstochasticgradientestimators). Weassumethatforalli∈[n]there
existparametersA ,C ≥0,B ≥1suchthat
i i i
E(cid:104) ∥∇f (x)∥2(cid:105) ≤2A (cid:0) f (x)−finf(cid:1) +B ∥∇f (x)∥2+C , (45)
ξt i i i i i i
ij
29holdsforallx∈Rd,where3finf =inf f (x)>−∞.
i x∈Rd i
WestudyEF21-W-SGDunderthesameassumptionaswasusedforanalyzingVanillaEF21-SGD,whichwe
denote as Assumption 5. To the best of our knowledge, this assumption, which was originally presented
as Assumption 2 by Khaled & Richta´rik (2022), is the most general assumption for a stochastic gradient
estimatorinanon-convexsetting.
Next, tobealignedwithoriginalVanillaEF21-SGD(Fatkhullinetal.,2021)wehaveconsideredaspecific
form of gradient estimator. This specific form of gradient estimator from Vanilla EF21-SGD presented in
Section4.1.2. ofFatkhullinetal.(2021)wherethestochasticgradientgˆ hasbeencomputedasfollows:
i
1
(cid:88)τi
gˆ(xt+1)= ∇f (xt+1),
i τ i ξ it j
j=1
Here τ is a minibatch size of sampled datapoint indexed by ξt of client i in iteration t. And ξt are
i ij ij
independentrandomvariables.ForaversionofEF21-W-SGDwhichisworkingunderamoregeneralsetting
pleaseseeAppendixF.
E.2 ALEMMA
Thecontractionlemmainthiscasegetsthefollowingform:
Lemma10. LetCt ∈C(α)foralli∈[n]andt≥0. Define
i
Gt
i
:=(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
, Gt
:=(cid:88)n
w iGt i.
i
i=1
LetAssumptions 2and 5hold. Then,foranys,ν >0wehave
E(cid:2) Gt+1(cid:3) ≤(1−θˆ)E(cid:2) Gt(cid:3) +βˆ 1L2 AME(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) +A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ 2, (46)
where
L
w := i ,
i (cid:80) L
j j
θˆ := 1−(1−α)(1+s)(1+ν),
βˆ := 2(1−α)(1+s)(cid:0) s+ν−1(cid:1) ,
1
βˆ := 2(1−α)(1+s)(1+ν−1)+(1+s−1),
2
(cid:18) (cid:19)
2(A +L (B −1)) 1
A(cid:101) := max i i i ,
i=1,...,n τ i nw i
(cid:18) (cid:19)
C 1
C(cid:101) := max i .
i=1,...,n τ i nw i
3WhenA =0onecanignorethefirsttermintheright-handsideof(45),i.e.,assumptioninf f (x)>−∞is
i x∈Rd i
notrequiredinthiscase.
30Proof. DefineWt :={gt,...,gt,xt,xt+1}. Theproofstartsasfollows:
1 n
E(cid:2) Gt+1 |Wt(cid:3) ( =24)
E(cid:34)(cid:13)
(cid:13) (cid:13)gt+1− ∇f
i(xt+1)(cid:13)
(cid:13)
(cid:13)2 |Wt(cid:35)
i (cid:13) i nw (cid:13)
i
lin =e7 E(cid:34)(cid:13) (cid:13) (cid:13)gt+Ct(cid:18) gˆ i(xt+1) −gt(cid:19)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i i nw i nw (cid:13)
i i
=
E(cid:34)(cid:13) (cid:13) (cid:13)Ct(cid:18) gˆ i(xt+1) −gt(cid:19) −(cid:18) gˆ i(xt+1) −gt(cid:19)
+
gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i nw i nw i nw nw (cid:13)
i i i i
( ≤21) (1+s)E(cid:34)(cid:13) (cid:13) (cid:13)Ct(cid:18) gˆ i(xt+1) −gt(cid:19) −(cid:18) gˆ i(xt+1) −gt(cid:19)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i nw i nw i (cid:13)
i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
( ≤4) (1−α)(1+s)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt)
+
∇f i(xt) −gt(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw nw i(cid:13)
i i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
( ≤21)
(1−α)(1+s)(1+ν)E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
i
+(1−α)(1+s)(1+ν−1)E(cid:34)(cid:13) (cid:13) (cid:13)∇f i(xt)
−
gˆ i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
( ≤21)
(1−α)(1+s)(1+ν)E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2 |Wt(cid:35)
i
+2(1−α)(1+s)(1+ν−1)E(cid:34)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
gˆ i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
+2(1−α)(1+s)(1+ν−1)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
∇f i(xt)(cid:13) (cid:13) (cid:13)2
(cid:13) nw nw (cid:13)
i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
.
(cid:13) nw nw (cid:13)
i i
To further bound the last term, which contains multiple (1+s−1) factors, we leverage the property that
gˆ(xt+1)isarandomvariableservingasanunbiasedestimatorof∇f (xt+1),takingtheform
i i
1
(cid:88)τi
gˆ(xt+1)= ∇f (xt+1),
i τ i ξ it j
j=1
31whereξt areindependentrandomvariables. Next,wecancontinueasfollows:
ij
E(cid:2) Gt i+1 |Wt(cid:3) ≤ (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
 (cid:13) (cid:13)2 
+ (nβ wˆ 2 i)2  E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)τ1
i
(cid:88) jτ =i 1∇f ξ it j(xt+1)− τ1
i
(cid:88) jτ =i 1∇f i(xt+1)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
|Wt  
= (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
 (cid:13) (cid:13)2 
+ (nwβˆ i2 )2τ2  E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:88) jτ =i 1(cid:16) ∇f ξ it j(xt+1)−∇f i(xt+1)(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
|Wt  
= (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+
βˆ
2
(cid:88)τi (cid:18) E(cid:20)(cid:13)
(cid:13)∇f
(xt+1)(cid:13) (cid:13)2 |Wt(cid:21) −(cid:13) (cid:13)E(cid:104)
∇f
(xt+1)|Wt(cid:105)(cid:13) (cid:13)2(cid:19)
(nw i)2τ i2 (cid:13) ξ it j (cid:13) (cid:13) ξ it j (cid:13)
j=1
≤ (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+ (nwβˆ )2
2τ 2
(cid:88)τi (cid:16) 2A i(cid:0) f i(xt+1)−f iinf(cid:1) +B i∥∇f i(xt+1)∥2+C i−(cid:13) (cid:13)∇f i(xt+1(cid:13) (cid:13)2 )(cid:17)
i i
j=1
= (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13) (cid:13)∇f i(xt+1)−(cid:13) (cid:13)∇f i(xt)(cid:13) (cid:13)2(cid:13) (cid:13) (cid:13)2
i
+ 2A iβˆ 2 (cid:0) f (xt+1)−finf(cid:1) + 2(B i−1)βˆ 2 (cid:18) 1 ∥∇f (xt+1)∥2(cid:19) + C iβˆ 2
(nw )2τ i i (nw )2τ 2 i (nw )2τ
i i i i i i
≤ (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+ 2A iβˆ 2 (cid:0) f (xt+1)−finf(cid:1) + 2(B i−1)βˆ 2L (cid:0) f (xt+1)−finf(cid:1) + C iβˆ 2
(nw )2τ i i (nw )2τ i i i (nw )2τ
i i i i i i
= (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+2(A i+L i(B i−1))βˆ 2 (cid:0) f (xt+1)−finf(cid:1) + C iβˆ 2 .
(nw )2τ i i (nw )2τ
i i i i
Furthermore,asaresultofleveragingAssumption2,wecanderivethesubsequentbound:
E(cid:2) Gt+1 |Wt(cid:3) ≤ (1−θˆ)Gt+
βˆ 1L2
i (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2
i i n2w2
i
+2(A i+L i(B i−1))βˆ 2 (cid:0) f (xt+1)−finf(cid:1) + C iβˆ 2 .
(nw )2τ i i (nw )2τ
i i i i
32Applyingthetowerpropertyandsubsequentlytakingtheexpectation,weobtain:
E(cid:2) Gt i+1(cid:3) ≤(1−θˆ)E(cid:2) Gt i(cid:3) +βˆ 1n21 w2L2 iE(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
(47)
+ 2(A i+L i(B i−1))βˆ 2E(cid:2) f (xt+1)−finf(cid:3) + C iβˆ 2 .
(nw )2τ i i (nw )2τ
i i i i
RegardingtheexpectationofGt+1,wederivethesubsequentbound:
(cid:34) n (cid:35)
E(cid:2) Gt+1(cid:3) = E (cid:88) w Gt+1
i i
i=1
n
=
(cid:88)
w
E(cid:2) Gt+1(cid:3)
i i
i=1
n n
( ≤47) (1−θˆ)(cid:88) w iE(cid:2) Gt i(cid:3) +(cid:88) w iβˆ 1n21 w2L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i=1 i=1 i
+(cid:88)n w 2(A i+L i(B i−1))βˆ 2 ·E(cid:2) f (xt+1)−finf(cid:3) +(cid:88)n w C iβˆ 2
i (nw )2τ i i i(nw )2τ
i i i i
i=1 i=1
n
= (1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
+(cid:88)n 2(A i+L i(B i−1))βˆ 2 ·E(cid:2) f (xt+1)−finf(cid:3) +(cid:88)n C iβˆ 2 .
n2w τ i i n2w τ
i i i i
i=1 i=1
EmployingquantitiesA˜andC˜,thefinalboundcanbereformulatedasfollows:
n
E(cid:2) Gt+1(cid:3) ≤ (1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
n
+ n1 (cid:88) A(cid:101)βˆ 2·E(cid:2) f i(xt+1)−f iinf(cid:3) +C(cid:101)βˆ
2
i=1
n
≤ (1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
n
+ n1 (cid:88) A(cid:101)βˆ 2·E(cid:2) f i(xt+1)−finf(cid:3) +C(cid:101)βˆ
2
i=1
n
≤ (1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
+A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ 2.
33Giventhatw i = (cid:80)L ji Lj,wehave:
E(cid:2) Gt+1(cid:3) ≤ (1−θˆ)E(cid:2) Gt(cid:3) + n1 (cid:88)n βˆ 1(cid:80) j nL j L iE(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i=1
+A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ
2
(cid:32) n (cid:33)2
= (1−θˆ)E(cid:2) Gt(cid:3) +βˆ
1
n1 (cid:88) L
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i=1
+A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ 2,
whatcompletestheproof.
E.3 MAINRESULT
Nowwearereadytoprovethemainconvergencetheorem.
Theorem8. LetCt ∈C(α)forall∈[n]andt≥0inAlgorithm3. Setthefollowingquantities:
i
θˆ := 1−(1−α)(1+s)(1+ν),
βˆ := 2(1−α)(1+s)(cid:0) s+ν−1(cid:1) ,
1
βˆ := 2(1−α)(1+s)(1+ν−1)+(1+s−1),
2
L
w := i ,
i (cid:80) L
j j
(cid:18) (cid:19)
2(A +L (B −1)) 1
A(cid:101) := max i i i ,
i=1,...,n τ i nw i
(cid:18) (cid:19)
C 1
C(cid:101) := max i .
i=1,...,n τ i nw i
Under Assumptions 1, 2, and 5, and selection of s > 0, µ > 0 such that (1+s)(1+µ) < 1 set the
1−α
stepsizeinthefollowingway:
1
γ ≤ . (48)
(cid:113)
L+L
βˆ
1
AM θˆ
ChooseaniteratexˆT from{x0,x1,...,xT−1}withprobability
v
Prob(xˆT =xt)= t , (49)
V
T
where
v
:=(cid:32)
1−
γA˜β˜ 2(cid:33)t
; V
:=T (cid:88)−1
v .
t 2θ T t
t=0
Then,
E(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105) ≤ 2(f(x0)−finf) + G0 + C(cid:101)β 2, (50)
γT (cid:16) 1− γA(cid:101)βˆ 2(cid:17)T θˆT (cid:16) 1− γA(cid:101)βˆ 2(cid:17)T θˆ
2θ 2θ
whereG0 :=(cid:80)n w ∥g0− 1 ∇f (x0)∥2.
i=1 i i nwi i
34Proof. Inthederivationbelow,weuseLemma3for
n
(cid:88)
gt = w gt. (51)
i i
i=1
Westartasfollows:
f(xt+1) ( ≤20) f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 −(cid:18) 21
γ
− L 2(cid:19) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:13) (cid:13) (cid:13) (cid:13)gt−(cid:88)n ∇f i(xt)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) (cid:13)
i=1
( =51) f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 −(cid:18) 21
γ
− L 2(cid:19) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)n w i(cid:18) g it− ∇f ni w(xt)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) i (cid:13)
i=1
≤ f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2
−(cid:18)
21
γ
− L
2(cid:19)
(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:88)n
w
i(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
i
i=1
(cid:18) (cid:19)
= f(xt)− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt. (52)
2 2γ 2 2
Subtractingf∗frombothsidesandtakingexpectation,weget
E(cid:2) f(xt+1)−f∗(cid:3)
≤
E(cid:2) f(xt)−f∗(cid:3)
−
γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
2
−(cid:18) 1
−
L(cid:19) E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
+
γ E(cid:2) Gt(cid:3)
. (53)
2γ 2 2
Letδt :=E[f(xt)−f∗],st :=E[Gt]andrt :=E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) .Thenbyadding γ st+1andemploying
2θ
(46),weobtain:
δt+1+ γ st+1 ≤δt− γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 1 − L(cid:19) rt+ γ st
2θˆ 2 2γ 2 2
γ (cid:16) (cid:17)
+
2θˆ
βˆ 1L2 AMrt+(1−θˆ)st+A(cid:101)βˆ 2δt+1+C(cid:101)βˆ
2
=δt+ 2γ θˆst− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 21
γ
− L
2
− 2γ θˆβˆ 1L2 AM(cid:19) rt+ γA 2(cid:101) θˆβ 2δt+1+ γ 2C θˆ(cid:101) β
2
≤δt+ 2γ θˆst− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) + γA 2(cid:101) θˆβ 2δt+1+ γ 2C θˆ(cid:101) β 2.
The last inequality follows from the bound
γ2βˆ 1L2
AM +Lγ ≤ 1, which holds due to Lemma 5 for γ ≤
θˆ
1 . Subsequently, we will reconfigure the final inequality and perform algebraic manipulations,
(cid:114)
L+LAM βˆ θˆ1
takingintoaccountthat 2 >0. Inthefinalstepofthesealgebraictransformations,wewillleveragethefact
γ
thatst ≥0:
δt+1+ 2γ θˆst+1 ≤ δt+ 2γ θˆst− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) + γA 2(cid:101) θˆβ 2δt+1+ γ 2C θˆ(cid:101) β 2.
Therefore,
γ2 δt+1+ γ2 2γ θˆst+1 ≤ γ2 δt+ γ2 2γ θˆst−E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) + γ2γA 2(cid:101) θˆβ 2δt+1+ γ2γ 2C θˆ(cid:101) β 2.
35Further,
E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) ≤ − γ2 δt+1− γ2 2γ θˆst+1+ γ2 δt+ γ2 2γ θˆst+ γ2γA 2(cid:101) θˆβ 2δt+1+ γ2γ 2C θˆ(cid:101) β
2
(cid:18) (cid:19)
≤
−2
δt+1−
2 γ
st+1+
2
δt+
γ
st +
2γA(cid:101)β
2δt+1+
C(cid:101)β
2
γ γ2θˆ γ 2θˆ γ 2θˆ θˆ
(cid:32)(cid:18) (cid:19) (cid:32) (cid:33) (cid:18) (cid:19)(cid:33)
≤
2
δt+
γ
st −1 1−
γA(cid:101)β
2 δt+1−
γ
st+1 +
C(cid:101)β
2
γ 2θˆ 2θˆ 2θˆ θˆ
(cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33)
≤
2
δt+
γ
st − 1−
γA(cid:101)β
2 δt+1+
γ
st+1 +
C(cid:101)β
2.
γ 2θˆ 2θˆ 2θˆ θˆ
Wesumupinequalitiesabovewithweightsv /V ,wherev :=(1− γA(cid:101)βˆ 2)tandV :=(cid:80)T v :
t T t 2θ T i=1 i
T
E(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105) = (cid:88) v t E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
V
T
t=0
T
=
V1 (cid:88)
v
tE(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
T
t=0
T (cid:32) (cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33) (cid:33)
≤ 1 (cid:88) v 2 δt+ γ st − 1− γA(cid:101)β 2 δt+1+ γ st+1 + C(cid:101)β 2
V
T
t γ 2θˆ 2θˆ 2θˆ θˆ
t=0
T (cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33) T
= 2 (cid:88) w δt+ γ st − 1− γA(cid:101)β 2 δt+1+ γ st+1 +(cid:88) w t · C(cid:101)β 2
γV
T
t 2θˆ 2θˆ 2θˆ W
T
θˆ
t=0 t=0
T (cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33)
= 2 (cid:88) w δt+ γ st − 1− γA(cid:101)β 2 δt+1+ γ st+1 + C(cid:101)β 2
γV
T
t 2θˆ 2θˆ 2θˆ θˆ
t=0
T (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
= 2 (cid:88) w δt+ γ st −w δt+1+ γ st+1 + C(cid:101)β 2
γV
T
t 2θˆ t+1 2θˆ θˆ
t=0
≤
2δ0
+
s0
+
C(cid:101)β
2.
γV
T
θˆV
T
θˆ
T
Finally,wenoticethatV = (cid:80) (1− γA(cid:101)βˆ 2)t ≥T ·(1− γA(cid:101)βˆ 2)T,whatconcludestheproof.
T 2θ 2θ
t=1
36F EF21-W-SGD: WEIGHTED ERROR FEEDBACK 2021 WITH STOCHASTIC
GRADIENTS UNDER THE ABC ASSUMPTION
Inthissection,wepresenttheconvergenceresultforWeightedEF21inthesettingwherethegradientcom-
putationontheclientsisreplacedwithaprettygeneralunbiasedstochasticgradientestimator.
F.1 ALGORITHM
The EF21-W algorithm assumes that all clients can compute the exact gradient in each round. In some
scenarios,theexactgradientsmaybeunavailableortoocostlytocompute,andonlyapproximategradient
estimators can be obtained. To have the ability for EF21-W to work in such circumstances we extended
EF21-Wtohandlestochasticgradients. WecalledtheresultingalgorithmEF21-W-SGD(Algorithm4).
Algorithm4EF21-W-SGD:WeightedEF-21withStochasticGradientsunderABCassumption
1: Input: initialmodelx0 ∈Rd;initialgradientestimatesg0,g0,...,g0 ∈Rdstoredattheserverandthe
1 2 n
2:
c Inli ie tn iats l; izs et :ep gs 0iz =eγ (cid:80)>
n
i=0 1; wnu igm i0b oe nr to hf eit se er ra vt eio rnsT >0;weightsw i = (cid:80)L ji Lj fori∈[n]
3: fort=0,1,2,...,T −1do
4: Servercomputesxt+1 =xt−γgtandbroadcastsxt+1toallnclients
5: fori=1,...,nontheclientsinparalleldo
6: Computeastochasticgradientgˆ i(xt+1)estimatorofthegradient∇f i(xt+1)
(cid:16) (cid:17)
7: Computeut
i
=C it nw1 igˆ i(xt+1)−g it andupdateg it+1 =g it+ut
i
8: Sendthecompressedmessageuttotheserver
i
9: endfor
10: Serverupdatesg it+1 =g it+ut
i
foralli∈[n],andcomputesgt+1 =(cid:80)n i=1w ig it+1
11: endfor
12: Output: PointxˆT chosenfromtheset{x0,...,xT−1}randomlyaccordingtothelaw(59)
OuranalysisofthisextensionfollowsasimilarapproachastheoneusedbyFatkhullinetal.(2021)forstudy-
ingthestochasticgradientversionunderthenameEF21-SGD.However, EF21-W-SGDhasfourimportant
differenceswithvanillaEF21-SGD:
1. Vanilla EF21-SGD algorithm analyzed by Fatkhullin et al. (2021) worked under a specific sam-
pling schema for a stochastic gradient estimator. Our analysis works under a more general ABC
Assumption6.
(cid:18) (cid:113) (cid:19)−1
2. Vanilla EF21-SGD provides maximum theoretically possible γ = L+L QM β θ1 , where
(cid:18) (cid:113) (cid:19)−1
EF21-W-SGDhasγ = L+L
AM
β θ1 .
3. In contrast to the original analysis Vanilla EF21-SGD our analysis provides a more aggressive β 1
parameterwhichissmallerbyafactorof2.
4. Vanilla EF21-SGD and EF21-W-SGD formally differs in a way how it reports iterate xT which
minimizes
E(cid:104)(cid:13) (cid:13)∇f(xT)(cid:13) (cid:13)2(cid:105)
due to a slightly different definition of A(cid:101). The EF21-W-SGD (Algo-
rithm4)requiresoutputiteratexˆT randomlyaccordingtotheprobabilitymassfunctiondescribed
byEquation(59).
37Assumption6(Generalassumptionforstochasticgradientestimators). Weassumethatforalli∈[n]there
existparametersA ,C ≥0,B ≥1suchthat
i i i
E(cid:2) ∥∇gˆ(x)∥2(cid:3) ≤2A (cid:0) f (x)−finf(cid:1) +B ∥∇f (x)∥2+C , (54)
i i i i i i i
holdsforallx∈Rd,where4finf =inf f (x)>−∞.
i x∈Rd i
Assumption 7 (Unbiased assumption for stochastic gradient estimators). We assume that for all i ∈ [n]
therefollowingholdsforallx∈Rd:
E[gˆ(x)]=∇f (x).
i i
WestudyEF21-W-SGDunderAssumption6andAssumption7.Tothebestofourknowledge,thisAssump-
tion6,whichwasoriginallypresentedasAssumption2byKhaled&Richta´rik(2022),isthemostgeneral
assumption for a stochastic gradient estimator in a non-convex setting. For a detailed explanation of the
generalityofthisassumptionseeFigure1ofKhaled&Richta´rik(2022).
F.2 ALEMMA
Thecontractionlemmainthiscasegetsthefollowingform:
Lemma11. LetCt ∈C(α)foralli∈[n]andt≥0. Define
i
Gt
i
:=(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
, Gt
:=(cid:88)n
w iGt i.
i
i=1
LetAssumptions 2, 6, 7hold. Then,foranys>0,ν >0duringexecutionoftheAlgorithm4thefollowing
holds:
E(cid:2) Gt+1(cid:3) ≤(1−θˆ)E(cid:2) Gt(cid:3) +βˆ 1L2 AME(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) +A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ 2, (55)
where
L
w := i ,
i (cid:80) L
j j
θˆ := 1−(1−α)(1+s)(1+ν)
βˆ := (1−α)(1+s)(cid:0) s+ν−1(cid:1) ,
1
βˆ := (1−α)(1+s)+(1+s−1),
2
(cid:18) (cid:19)
1
A(cid:101) := i=m 1,a ..x
.,n
2(A i+L i(B i−1))
nw i
,
(cid:18) (cid:19)
1
C(cid:101) := i=m 1,a ..x
.,n
C
inw
i
.
4WhenA =0onecanignorethefirsttermintheright-handsideof(54),i.e.,assumptioninf f (x)>−∞is
i x∈Rd i
notrequiredinthiscase.
38Proof. DefineWt :={gt,...,gt,xt,xt+1}. Theproofstartsasfollows:
1 n
E(cid:2) Gt+1 |Wt(cid:3) ( =24)
E(cid:34)(cid:13)
(cid:13) (cid:13)gt+1− ∇f
i(xt+1)(cid:13)
(cid:13)
(cid:13)2 |Wt(cid:35)
i (cid:13) i nw (cid:13)
i
lin =e7 E(cid:34)(cid:13) (cid:13) (cid:13)gt+Ct(cid:18) gˆ i(xt+1) −gt(cid:19)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i i nw i nw (cid:13)
i i
=
E(cid:34)(cid:13) (cid:13) (cid:13)Ct(cid:18) gˆ i(xt+1) −gt(cid:19) −(cid:18) gˆ i(xt+1) −gt(cid:19)
+
gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i nw i nw i nw nw (cid:13)
i i i i
( ≤21) (1+s)E(cid:34)(cid:13) (cid:13) (cid:13)Ct(cid:18) gˆ i(xt+1) −gt(cid:19) −(cid:18) gˆ i(xt+1) −gt(cid:19)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) i nw i nw i (cid:13)
i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
( ≤4) (1−α)(1+s)E(cid:34)(cid:13) (cid:13) (cid:13)(cid:18) gˆ i(xt+1)
−
∇f i(xt+1)(cid:19) +(cid:18) ∇f i(xt+1) −gt(cid:19)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw nw i (cid:13)
i i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
=
(1−α)(1+s)E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f i n(
wxt+1)(cid:13)
(cid:13) (cid:13)
(cid:13)2 |Wt(cid:35)
i
+(1−α)(1+s)E(cid:34)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
gˆ i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
= (1−α)(1+s)E(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)g it− ∇f ni w(xt) + ∇f ni w(xt) − ∇f i n( wxt+1)(cid:13) (cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
i i i
+(1−α)(1+s)E(cid:34)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
gˆ i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
(cid:13) nw nw (cid:13)
i i
+(1+s−1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
.
(cid:13) nw nw (cid:13)
i i
39Further,wecontinueasfollows
E(cid:2) Gt i+1 |Wt(cid:3) ( ≤21)
(1−α)(1+s)(1+ν)E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2 |Wt(cid:35)
i
+(1−α)(1+s)(1+ν−1)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
∇f i(xt)(cid:13) (cid:13) (cid:13)2
(cid:13) nw nw (cid:13)
i i
+(cid:0) (1+s−1)+(1−α)(1+s)(cid:1)E(cid:34)(cid:13) (cid:13) (cid:13)gˆ i(xt+1)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt(cid:35)
.
(cid:13) nw nw (cid:13)
i i
To further bound the last term, which contains multiple (1+s−1) factors, we leverage the property that
gˆ(xt+1)isarandomvariableservingasanunbiasedestimatorof∇f (xt+1). Ourapproachisasfollows:
i i
E(cid:2) Gt i+1 |Wt(cid:3) ≤ (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+ (nβ wˆ 2 )2E(cid:104)(cid:13) (cid:13)gˆ i(xt+1)−∇f i(xt+1)(cid:13) (cid:13)2 |Wt(cid:105) .
i
NowduetotherequirementofunbiasednessofgradientestimatorsexpressedintheformofAssumption7
wehavethefollowing:
E(cid:104)(cid:13) (cid:13)gˆ i(xt+1)−∇f i(xt+1)(cid:13) (cid:13)2 |Wt(cid:105) = E(cid:104)(cid:13) (cid:13)gˆ i(xt+1)(cid:13) (cid:13)2 |Wt(cid:105) −(cid:13) (cid:13)∇f i(xt+1)(cid:13) (cid:13)2 (56)
Usingthisvariancedecomposition,wecanproceedasfollows.
40E(cid:2) Gt i+1 |Wt(cid:3) ( ≤56) (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+ (nβ wˆ 2
)2
(cid:16) E(cid:104)(cid:13) (cid:13)gˆ i(xt+1)(cid:13) (cid:13)2 |Wt(cid:105) −(cid:13) (cid:13)∇f i(xt+1)(cid:13) (cid:13)2(cid:17)
i
( ≤54) (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+ (nβ wˆ 2
)2
(cid:16) 2A i(cid:0) f i(xt+1)−f iinf(cid:1) +B i∥∇f i(xt+1)∥2+C i−(cid:13) (cid:13)∇f i(xt+1(cid:13) (cid:13)2 )(cid:17)
i
= (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+2A iβˆ 2 (cid:0) f (xt+1)−finf(cid:1) + 2(B i−1)βˆ 2 (cid:18) 1 ∥∇f (xt+1)∥2(cid:19) + C iβˆ 2
(nw )2 i i (nw )2 2 i (nw )2
i i i
≤ (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+2A iβˆ 2 (cid:0) f (xt+1)−finf(cid:1) + 2(B i−1)βˆ 2L (cid:0) f (xt+1)−finf(cid:1) + C iβˆ 2
(nw )2 i i (nw )2 i i i (nw )2
i i i
= (1−θˆ)E(cid:2) Gt
i
|Wt(cid:3) +βˆ 1n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
+2(A i+L i(B i−1))βˆ 2 (cid:0) f (xt+1)−finf(cid:1) + C iβˆ 2 .
(nw )2 i i (nw )2
i i
Next leveraging Assumption 2 we replace the second term in the last expression, and we can derive the
subsequentbound:
E(cid:2) Gt+1 |Wt(cid:3) ( ≤7) (1−θˆ)Gt+ βˆ 1L2 i (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2
i i n2w2
i
+2(A i+L i(B i−1))βˆ 2 (cid:0) f (xt+1)−finf(cid:1) + C iβˆ 2 .
(nw )2 i i (nw )2
i i
Applyingthetowerpropertyandsubsequentlytakingtheexpectation,weobtain:
E(cid:2) Gt i+1(cid:3) ≤(1−θˆ)E(cid:2) Gt i(cid:3) +βˆ 1n21 w2L2 iE(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
(57)
+ 2(A i+L i(B i−1))βˆ 2E(cid:2) f (xt+1)−finf(cid:3) + C iβˆ 2 .
(nw )2 i i (nw )2
i i
41NextfortheexpectationofthemainquantityofourinterestGt+1,wederivethesubsequentbound:
(cid:34) n (cid:35)
E(cid:2) Gt+1(cid:3) = E (cid:88) w Gt+1
i i
i=1
n
=
(cid:88)
w
E(cid:2) Gt+1(cid:3)
i i
i=1
n n
( ≤57) (1−θˆ)(cid:88) w iE(cid:2) Gt i(cid:3) +(cid:88) w iβˆ 1n21 w2L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i=1 i=1 i
+(cid:88)n w 2(A i+L i(B i−1))βˆ 2 ·E(cid:2) f (xt+1)−finf(cid:3) +(cid:88)n w C iβˆ 2
i (nw )2 i i i(nw )2
i i
i=1 i=1
n
= (1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
+(cid:88)n 2(A i+L i(B i−1))βˆ 2 ·E(cid:2) f (xt+1)−finf(cid:3) +(cid:88)n C iβˆ 2
(n)2w i i n2w
i i
i=1 i=1
EmployingquantitiesA˜andC˜,thefinalboundcanbereformulatedasfollows:
n
E(cid:2) Gt+1(cid:3) ≤(1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
n
+ n1 (cid:88) A(cid:101)βˆ 2·E(cid:2) f i(xt+1)−f iinf(cid:3) +C(cid:101)βˆ
2
i=1
n
≤(1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
n
+ n1 (cid:88) A(cid:101)βˆ 2·E(cid:2) f i(xt+1)−finf(cid:3) +C(cid:101)βˆ
2
i=1
n
≤(1−θˆ)E(cid:2) Gt(cid:3) +(cid:88) βˆ 1n21
w
L2
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
+A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ 2.
Giventhatw i = (cid:80)L ji Lj,wehave:
E(cid:2) Gt+1(cid:3) ≤(1−θˆ)E(cid:2) Gt(cid:3) + n1 (cid:88)n βˆ 1(cid:80) j nL j L iE(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i=1
+A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ
2
(cid:32) n (cid:33)2
=(1−θˆ)E(cid:2) Gt(cid:3) +βˆ
1
n1 (cid:88) L
i
·E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i=1
+A(cid:101)βˆ 2E(cid:2) f(xt+1)−finf(cid:3) +C(cid:101)βˆ 2,
whatcompletestheproof.
42F.3 MAINRESULT
Nowwearereadytoprovethemainconvergencetheorem.
Theorem9. LetCt ∈C(α)forall∈[n]andt≥0inAlgorithm4. setthefollowingquantities:
i
θˆ := 1−(1−α)(1+s)(1+ν),
βˆ := (1−α)(1+s)(cid:0) s+ν−1(cid:1) ,
1
βˆ := (1−α)(1+s)+(1+s−1),
2
L
w := i ,
i (cid:80)n
L
j=1 j
2(A +L (B −1))
A(cid:101) := max i i i ,
i=1,...,n nw i
C
C(cid:101) := max i .
i=1,...,nnw
i
UnderAssumptions1,2, 6, 7,andselectionofs>0,ν >0smallenoughsuchthat(1+s)(1+ν)< 1
1−α
holds,setthestepsizeinthefollowingway:
1
γ ≤ . (58)
(cid:113)
L+L
βˆ
1
AM θˆ
ChooseaniteratexˆT from{x0,x1,...,xT−1}withprobability
v
Prob(xˆT =xt)= t , (59)
V
T
where
v
:=(cid:32)
1−
γA˜β˜ 2(cid:33)t
; V
:=T (cid:88)−1
v .
t 2θ T t
t=0
Then,
E(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105) ≤ 2(f(x0)−finf) + G0 + C(cid:101)β 2, (60)
γT (cid:16) 1− γA(cid:101)βˆ 2(cid:17)T θˆT (cid:16) 1− γA(cid:101)βˆ 2(cid:17)T θˆ
2θ 2θ
(cid:13) (cid:13)2
whereG0 :=(cid:80)n w (cid:13)g0− 1 ∇f (x0)(cid:13) .
i=1 i(cid:13) i nwi i (cid:13)
Proof. Inthederivationbelow,weuseLemma3for
n
(cid:88)
gt = w gt. (61)
i i
i=1
43Westartasfollows:
f(xt+1) ( ≤20) f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 −(cid:18) 21
γ
− L 2(cid:19) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:13) (cid:13) (cid:13) (cid:13)gt− n1 (cid:88)n ∇f i(xt)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) (cid:13)
i=1
( =51) f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 −(cid:18) 21
γ
− L 2(cid:19) (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)n w i(cid:18) g it− ∇f ni w(xt)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) i (cid:13)
i=1
≤ f(xt)− γ
2
(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2
−(cid:18)
21
γ
− L
2(cid:19)
(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ
2
(cid:88)n
w
i(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
i
i=1
(cid:18) (cid:19)
= f(xt)− γ (cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2 − 1 − L (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 + γ Gt.
2 2γ 2 2
Subtractingf∗frombothsidesandtakingexpectation,weget
E(cid:2) f(xt+1)−f∗(cid:3) ≤E(cid:2) f(xt)−f∗(cid:3)
−
γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 1
−
L(cid:19) E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
+
γ E(cid:2) Gt(cid:3)
.
2 2γ 2 2
Letδt :=E[f(xt)−f∗],st :=E[Gt]andrt :=E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) .Thenbyadding γ st+1andemploying
2θ
inequality(46),weobtain:
δt+1+ γ st+1 ≤δt− γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 1 − L(cid:19) rt+ γ st
2θˆ 2 2γ 2 2
γ (cid:16) (cid:17)
+
2θˆ
βˆ 1L2 AMrt+(1−θˆ)st+A(cid:101)βˆ 2δt+1+C(cid:101)βˆ
2
=δt+ 2γ θˆst− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 21
γ
− L
2
− 2γ θˆβˆ 1L2 AM(cid:19) rt+ γA 2(cid:101) θˆβ 2δt+1+ γ 2C θˆ(cid:101) β
2
≤δt+ 2γ θˆst− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) + γA 2(cid:101) θˆβ 2δt+1+ γ 2C θˆ(cid:101) β 2.
Thelastinequalityfollowsfromtheboundγ2βˆ 1L2
AM +Lγ ≤1,whichholdsduetoLemma5for
θˆ
1
γ ≤ .
(cid:113)
L+L
βˆ
1
AM θˆ
Subsequently, we will reconfigure the final inequality and perform algebraic manipulations, taking into
accountthat 2 >0.Inthefinalstepofthesealgebraictransformations,wewillleveragethefactthatst ≥0:
γ
δt+1+ 2γ θˆst+1 ≤ δt+ 2γ θˆst− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) + γA 2(cid:101) θˆβ 2δt+1+ γ 2C θˆ(cid:101) β 2.
Therefore,
γ2 δt+1+ γ2 2γ θˆst+1 ≤ γ2 δt+ γ2 2γ θˆst−E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) + γ2γA 2(cid:101) θˆβ 2δt+1+ γ2γ 2C θˆ(cid:101) β 2.
44Further,
E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) ≤ − γ2 δt+1− γ2 2γ θˆst+1+ γ2 δt+ γ2 2γ θˆst+ γ2γA 2(cid:101) θˆβ 2δt+1+ γ2γ 2C θˆ(cid:101) β
2
(cid:18) (cid:19)
≤
−2
δt+1−
2 γ
st+1+
2
δt+
γ
st +
2γA(cid:101)β
2δt+1+
C(cid:101)β
2
γ γ2θˆ γ 2θˆ γ 2θˆ θˆ
(cid:32)(cid:18) (cid:19) (cid:32) (cid:33) (cid:18) (cid:19)(cid:33)
≤
2
δt+
γ
st −1 1−
γA(cid:101)β
2 δt+1−
γ
st+1 +
C(cid:101)β
2
γ 2θˆ 2θˆ 2θˆ θˆ
(cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33)
≤
2
δt+
γ
st − 1−
γA(cid:101)β
2 δt+1+
γ
st+1 +
C(cid:101)β
2.
γ 2θˆ 2θˆ 2θˆ θˆ
Wesumupinequalitiesabovewithweightsv /V ,wherev :=(1− γA(cid:101)βˆ 2)tandV :=(cid:80)T v :
t T t 2θ T i=1 i
T
E(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105) = (cid:88) v t E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
V
T
t=0
T
=
V1 (cid:88)
v
tE(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
T
t=0
T (cid:32) (cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33) (cid:33)
≤ 1 (cid:88) v 2 δt+ γ st − 1− γA(cid:101)β 2 δt+1+ γ st+1 + C(cid:101)β 2
V
T
t γ 2θˆ 2θˆ 2θˆ θˆ
t=0
T (cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33) T
= 2 (cid:88) w δt+ γ st − 1− γA(cid:101)β 2 δt+1+ γ st+1 +(cid:88) w t · C(cid:101)β 2
γV
T
t 2θˆ 2θˆ 2θˆ W
T
θˆ
t=0 t=0
T (cid:32)(cid:18) (cid:19) (cid:32) (cid:33)(cid:18) (cid:19)(cid:33)
= 2 (cid:88) w δt+ γ st − 1− γA(cid:101)β 2 δt+1+ γ st+1 + C(cid:101)β 2
γV
T
t 2θˆ 2θˆ 2θˆ θˆ
t=0
T (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
= 2 (cid:88) w δt+ γ st −w δt+1+ γ st+1 + C(cid:101)β 2
γV
T
t 2θˆ t+1 2θˆ θˆ
t=0
≤
2δ0
+
s0
+
C(cid:101)β
2.
γV
T
θˆV
T
θˆ
T
Finally,wenoticethatV = (cid:80) (1− γA(cid:101)βˆ 2)t ≥T ·(1− γA(cid:101)βˆ 2)T,whatconcludestheproof.
T 2θ 2θ
t=1
45G EF21-W-PP: WEIGHTED ERROR FEEDBACK 2021 WITH PARTIAL
PARTICIPATION
In this section, we present another extension of error feedback. Again, to maintain brevity, we show our
resultsforEF21-W,however,webelievegettinganenhancedrateforstandardEF21shouldbestraightfor-
ward.
G.1 ALGORITHM
Building upon the delineation of EF21-W in Algorithm 2, we turn our attention to its partial participation
variant,EF21-W-PP,andseektohighlighttheprimarydistinctionsbetweenthem. Onesalientdifferenceis
theintroductionofadistribution,denotedasD,acrosstheclients. Forclarity,considerthepowersetP of
the set [n] := {1,2,...,n}, representing all possible subsets of [n]. Then, the distribution D serves as a
discretedistributionoverP.
WhileEF21-W-PPruns,atthestartofeachcommunicationroundt,themaster,havingcomputedadescent
stepasxt+1 = xt−γgt,samplesaclientsubsetSt fromthedistributionD. ContrastingwithAlgorithm2
wherethenewiterationxt+1issenttoallclients,inthisvariant,itissentexclusivelytothoseinSt.
Anyclienti∈St adherestoproceduresakinto EF21-W:itcompressesthequantity nw1 i∇f i(xt)−g it and
transmits this to the master. Conversely, client j omitted in St, i.e., j ∈/ St, is excluded from the training
for that iteration. Concluding the round, the master updates gt+1 by integrating the averaged compressed
variancesreceivedfromclientsinthesetSt.
Algorithm5EF21-W-PP:WeightedErrorFeedback2021withPartialParticipation
1: Input: initial model parameters x0 ∈ Rd; initial gradient estimates g0,g0,...,g0 ∈ Rd stored at the
1 2 n
2:
Ic nli ie tn iats l; izw e:ei gg 0ht =sw (cid:80)i
n
i= =1L wi/ i(cid:80)
g
i0jL oj n; ts hte ep ss ei rz ve eγ
r
>0;numberofiterationsT >0;distributionDoverclients
3: fort=0,1,2,...,T −1do
4: Servercomputesxt+1 =xt−γgt
5: ServersamplesasubsetSt ∼Dofclients
6: Serverbroadcastsxt+1toclientsinSt
7: fori=1,...,nontheclientsinparalleldo
8: ifi∈Stthen
9: Computeut
i
=C it( nw1 i∇f i(xt+1)−g it)andupdateg it+1 =g it+ut
i
10: Sendthecompressedmessageuttotheserver
i
11: endif
12: ifi∈/ Stthen
13: Setut =0fortheclientandtheserver
i
14: Donotchangelocalstategt+1 =gt
i i
15: endif
16: endfor
17: Serverupdatesg it+1 =g it+ut
i
foralli∈[n],andcomputesgt+1 =(cid:80)n i=1w ig it+1
18: endfor
19: Output: PointxˆT chosenfromtheset{x0,...,xT−1}uniformlyatrandom
AssumeS isdrawnfromthedistributionD. Letusdenote
p :=Prob(i∈St). (62)
i
46Inotherwords,p representstheprobabilityofclientibeingselectedinanyiteration. Forgivenparameters
i
p such that p ∈ (0,1] for i ∈ [n], we introduce the notations p := min p and p := max p ,
i i min i i max i i
respectively.
G.2 ALEMMA
Havingestablishedthenecessarydefinitions,wecannowproceedtoformulatethelemma.
Lemma12. LetCt ∈C(α)foralli∈[n]andt≥0. LetAssumption2hold. Define
i
Gt
i
:=(cid:13)
(cid:13) (cid:13) (cid:13)g it− ∇f ni
w(xt)(cid:13)
(cid:13) (cid:13)
(cid:13)2
, Gt
:=(cid:88)n
w iGt i. (63)
i
i=1
Foranys>0andρ>0,letusdefinethefollowingquantities:
θ(α,s) := 1−(1−α)(1+s)
β(α,s) := β(α,s)=(1−α)(1+s−1)
θ := p ρ+θ(α,s)p −ρ−(p −p )
p min max max min
B˜ := (cid:0) β(α,s)p +(1−p )(1+ρ−1)(cid:1) L2 .
max min AM
Additionally,assumethat
1+ρ(1−p )+(p −p ) ρ(1−p )+(p −p )
min max min ≥θ(α,s)> min max min .
p p
max max
Then,wehave
E(cid:2) Gt+1(cid:3)
≤(1−θ
p)E(cid:2) Gt(cid:3) +B˜E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
. (64)
Proof. LetusdefineWt :={gt,...,gt,xt,xt+1}. Ifclientiparticipatesinthetrainingatiterationt,then
1 n
E(cid:2) Gt+1 |Wt,i∈St(cid:3) ( =63)
E(cid:34)(cid:13)
(cid:13) (cid:13)gt+1− ∇f
i(xt+1)(cid:13)
(cid:13)
(cid:13)2 |Wt,i∈St(cid:35)
i (cid:13) i nw (cid:13)
i
line9ofA =lgorithm5 E(cid:34)(cid:13) (cid:13) (cid:13)gt+Ct(cid:18) ∇f i(xt+1) −gt(cid:19)
−
∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt,i∈St(cid:35)
(cid:13) i i nw i nw (cid:13)
i i
( ≤4) (1−α)(cid:13) (cid:13) (cid:13)∇f i(xt+1) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw i(cid:13)
i
=
(1−α)(cid:13) (cid:13) (cid:13)∇f i(xt+1)
−
∇f i(xt)
+
∇f i(xt) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw nw nw i(cid:13)
i i i
( ≤21) (1−α)(1+s)(cid:13) (cid:13) (cid:13)∇f i(xt) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw i(cid:13)
i
+(1−α)(cid:0) 1+s−1(cid:1) n21
w2
(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2
i
( ≤7) (1−α)(1+s)(cid:13) (cid:13) (cid:13)∇f i(xt) −gt(cid:13) (cid:13) (cid:13)2
(cid:13) nw i(cid:13)
i
+(1−α)(cid:0) 1+s−1(cid:1)
L2
i (cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2 .
n2w2
i
47UtilizingthetowerpropertyandtakingtheexpectationwithrespecttoWt,wederive:
E(cid:2) Gt+1 |i∈St(cid:3) ≤(1−θ(α,s))E(cid:2) Gt(cid:3) +β(α,s) L2 i E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) , (65)
i i n2w2
i
whereθ(α,s)=1−(1−α)(1+s),andβ(α,s)=(1−α)(1+s−1). Wenowaimtoboundthequantity
E(cid:2) Gt+1 |i∈/ St(cid:3) ,startingwithanapplicationofthetowerproperty:
i
E(cid:2) Gt+1 |i∈/ St(cid:3) = E(cid:2)E(cid:2) Gt+1 |Wt,i∈/ St(cid:3)(cid:3)
i i
( =63)
E(cid:34) E(cid:34)(cid:13)
(cid:13) (cid:13)gt+1− ∇f
i(xt+1)(cid:13)
(cid:13)
(cid:13)2
|Wt,i∈/
St(cid:35)(cid:35)
(cid:13) i nw (cid:13)
i
= E(cid:34) E(cid:34)(cid:13) (cid:13) (cid:13)gt− ∇f i(xt+1) + ∇f i(xt) − ∇f i(xt)(cid:13) (cid:13) (cid:13)2 |Wt,i∈/ St(cid:35)(cid:35)
(cid:13) i nw nw nw (cid:13)
i i i
( ≤21) E(cid:34) E(cid:34) (1+ρ)(cid:13) (cid:13) (cid:13)gt− ∇f i(xt)(cid:13) (cid:13) (cid:13)2 +(1+ρ−1)(cid:13) (cid:13) (cid:13)∇f i(xt) − ∇f i(xt+1)(cid:13) (cid:13) (cid:13)2 |Wt,i∈/ St(cid:35)(cid:35)
(cid:13) i nw (cid:13) (cid:13) nw nw (cid:13)
i i i
= (1+ρ)E(cid:2) Gt i(cid:3) + (1 n+ 2wρ− 21) E(cid:104)(cid:13) (cid:13)∇f i(xt+1)−∇f i(xt)(cid:13) (cid:13)2 (cid:105) .
i
GiventhatAssumption2issatisfied,byapplying(7)tothesecondterm,weobtain:
E(cid:2) Gt+1 |i∈/ St(cid:3) ≤(1+ρ)E(cid:2) Gt(cid:3) + L2 i(1+ρ−1) E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) . (66)
i i n2w2
i
Wecombinethetwoprecedingbounds:
E(cid:2) Gt+1(cid:3) = Prob(i∈St)E(cid:2) Gt+1 |i∈St(cid:3) +Prob(i∈/ St)E(cid:2) Gt+1 |i∈/ St(cid:3)
i i i
( =62) p E(cid:2) Gt+1|i∈St(cid:3) +(1−p )E(cid:2) Gt+1|i∈/ St(cid:3)
i i i i
(65) ≤+(66) p i(cid:20) (1−θ(α,s))E(cid:2) Gt i(cid:3) +β(α,s) nL 2w2 i 2E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)(cid:21)
i
+(1−p i)(cid:20) (1+ρ)E(cid:2) Gt i(cid:3) + L2 i( n1 2+ wρ 2−1) E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)(cid:21)
i
= ((1−θ(α,s))p +(1−p
)(1+ρ))E(cid:2) Gt(cid:3)
i i i
+(cid:0) β(α,s)p i+(1−p i)(1+ρ−1)(cid:1) nL 2w2 i 2E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) .
i
48Consequently,forE(cid:2) Gt+1(cid:3)
,wederivethesubsequentbound:
(cid:34) n (cid:35)
E(cid:2) Gt+1(cid:3) ( =63) E (cid:88) w Gt+1
i i
i=1
n
=
(cid:88)
w
E(cid:2) Gt+1(cid:3)
i i
i=1
n
≤
(cid:88)
w ((1−θ(α,s))p +(1−p
)(1+ρ))E(cid:2) Gt(cid:3)
i i i i
i=1
+(cid:88)n w i(cid:0) β(α,s)p i+(1−p i)(1+ρ−1)(cid:1) nL 2w2 i 2E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) ,
i=1 i
where we applied the preceding inequality. Remembering the definitions p := min p and p :=
min i i max
max p ,wesubsequentlyobtain:
i i
n
E(cid:2) Gt+1(cid:3)
≤
(cid:88)
w ((1−θ(α,s))p +(1−p
)(1+ρ))E(cid:2) Gt(cid:3)
i max min i
i=1
+(cid:88)n (cid:0) β(α,s)p max+(1−p min)(1+ρ−1)(cid:1) nL 2w2 i E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
i
i=1
n
= ((1−θ(α,s))p +(1−p
)(1+ρ))(cid:88)
w
E(cid:2) Gt(cid:3)
max min i i
i=1
+(cid:0) β(α,s)p max+(1−p min)(1+ρ−1)(cid:1)(cid:88)n nL 2w2 i E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) .
i
i=1
Applying(63)and(25),weobtain:
E(cid:2) Gt+1(cid:3)
= ((1−θ(α,s))p +(1−p
)(1+ρ))E(cid:2) Gt(cid:3)
max min
+(cid:0) β(α,s)p max+(1−p min)(1+ρ−1)(cid:1)(cid:88) i=n
1
n2
(cid:80)L
n
jL2 i
=1i
Lj
E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
= ((1−θ(α,s))p +(1−p
)(1+ρ))E(cid:2) Gt(cid:3)
max min
n n
+(cid:0) β(α,s)p max+(1−p min)(1+ρ−1)(cid:1)(cid:88)L nj (cid:88)L niE(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
j=1 i=1
= ((1−θ(α,s))p +(1−p
)(1+ρ))E(cid:2) Gt(cid:3)
max min
+(cid:0) β(α,s)p max+(1−p min)(1+ρ−1)(cid:1) L2 AME(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) .
Subsequently,inordertosimplifythelastinequality,weintroducethevariables1−θ andB˜:
p
1−θ := (1−θ(α,s))p +(1−p )(1+ρ)
p max min
= p −p θ(α,s)+1−p +ρ−p ρ
max max min min
= 1−(−p +p θ(α,s)+p −ρ+p ρ)
max max min min
= 1−(p ρ+p θ(α,s)−ρ−(p −p )).
min max max min
49Therefore,
θ = (p θ(α,s)−ρ(1−p )−(p −p ))
p max min max min
B˜ := (cid:0) β(α,s)p +(1−p )(1+ρ−1)(cid:1) L2 .
max min AM
Expressedintermsofthesevariables,thefinalinequalitycanbereformulatedas:
E(cid:2) Gt+1(cid:3)
≤(1−θ
p)E(cid:2) Gt(cid:3) +B˜E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
.
SinceweneedthecontractionpropertyoverthegradientdistortionE(cid:2) Gt+1(cid:3)
,werequire0 < θ ≤ 1. We
p
rewritetheseconditionsasfollows:
1+ρ(1−p )+(p −p ) ρ(1−p )+(p −p )
min max min ≥θ(α,s)> min max min .
p p
max max
G.3 MAINRESULT
Wearereadytoprovethemainconvergencetheorem.
Theorem10. ConsiderAlgorithm5(EF21-W-PP)appliedtothedistributedoptimizationproblem(1). Let
Assumptions1,2,3hold,assumethatCt ∈C(α)foralli∈[n]andt>0,set
i
Gt :=(cid:88)n w i(cid:13) (cid:13) (cid:13) (cid:13)g it− n1
w
∇f i(xt)(cid:13) (cid:13) (cid:13) (cid:13)2 ,
i
i=1
wherew i = (cid:80)L ji Lj foralli∈[n],andletthestepsizesatisfy
 (cid:115) −1
B˜
0<γ ≤L+  , (67)
θ
p
wheres>0,ρ>0,and
θ(α,s) := 1−(1−α)(1+s)
β(α,s) := (1−α)(1+s−1)
θ := p ρ+θ(α,s)p −ρ−(p −p )
p min max max min
B˜ := (cid:0) β(α,s)p +(1−p )(1+ρ−1)(cid:1) L2 .
max min AM
Additionally,assumethat
1+ρ(1−p )+(p −p ) ρ(1−p )+(p −p )
min max min ≥θ(α,s)> min max min .
p p
max max
IfforT >1wedefinexˆT asanelementoftheset{x0,x1,...,xT−1}chosenuniformlyatrandom,then
E(cid:2) ∥∇f(xˆT)∥2(cid:3)
≤
2(f(x0)−f∗)
+
G0
. (68)
γT θ T
p
Proof. FollowingthesameapproachemployedintheprooffortheSGDcase,weobtain
E(cid:2) f(xt+1)−f∗(cid:3)
≤
E(cid:2) f(xt)−f∗(cid:3)
−
γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
2
−(cid:18) 1
−
L(cid:19) E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105)
+
γ E(cid:2) Gt(cid:3)
.
2γ 2 2
50Let δt := E[f(xt)−f∗], st := E[Gt] and rt := E(cid:104)(cid:13) (cid:13)xt+1−xt(cid:13) (cid:13)2(cid:105) . Applying the previous lemma, we
obtain:
δt+1+ 2γ
θ
st+1 ≤ δt− γ 2E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 21
γ
− L 2(cid:19) rt+ γ 2st+ 2γ
θ
(cid:16) B˜rt+(1−θ p)st(cid:17)
p p
= δt+ γ st− γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) −(cid:18) 1 − L − γ B˜(cid:19) rt
2θ 2 2γ 2 2θ
p
(cid:124) (cid:123)(cid:122) (cid:125)
≥0
≤ δt+ γ st− γ E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) .
2θ 2
p
Bysummingupinequalitiesfort=0,...,T −1,weget
T−1
0≤δT + γ sT ≤δ0+ γ s0− γ (cid:88) E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105) .
2θ 2θ 2
p p
t=0
Finally,viamultiplyingbothsidesby 2 ,afterrearrangingweget:
γT
T (cid:88)−1 1 E(cid:104)(cid:13) (cid:13)∇f(xt)(cid:13) (cid:13)2(cid:105)
≤
2δ0
+
s0
.
T γT θ T
p
t=0
Itremainstonoticethattheleft-handsidecanbeinterpretedasE(cid:104)(cid:13) (cid:13)∇f(xˆT)(cid:13) (cid:13)2(cid:105)
,wherexˆT ischosenfrom
theset{x0,x1,...,xT−1}uniformlyatrandom.
Our analysis of this extension follows a similar approach as the one used by Fatkhullin et al. (2021), for
algorithmtheycalledEF21-PP.PresentedanalysisofEF21-W-PPhasabetterprovidesabettermultiplicative
factorindefinitionofB(cid:101) ∝L2 ,whereinvanillaEF21-PPhadB(cid:101) ∝L2 . Thisfactimprovesupperbound
AM QM
onallowablestepsizein(67).
51H IMPROVED THEORY FOR EF21 IN THE RARE FEATURES REGIME
Inthissection,weadaptournewresultstotherarefeaturesregimeproposedandstudiedbyRichta´riketal.
(2023).
H.1 ALGORITHM
Inthissection,wefocusonAlgorithm6,whichisanadaptationofEF21(asdelineatedinAlgorithm1)that
specificallyemploysTopK operators. Thisvariantistailoredfortherarefeaturesscenario, enhancingthe
convergenceratebyshiftingfromtheaverageofsquaredLipschitzconstantstothesquareoftheiraverage.
ThemodificationsintroducedinAlgorithm6,comparedtothestandardEF21,aretwofoldandsignificant.
Primarily, the algorithm exclusively engages TopK compressors, leveraging the inherent sparsity present
in the data. Additionally, the initial gradient estimates g0 are confined to the respective subspaces Rd,
i i
as characterized by equation (71). With the exception of these distinct aspects, the algorithm’s execution
parallelsthatoftheoriginalEF21.
Algorithm6EF21: ErrorFeedback2021withTopK compressors
1: Input: initial model x0 ∈ Rd; initial gradient estimates g0 ∈ Rd,...,g0 ∈ Rd (as defined in equa-
1 1 n n
tion (71)) stored at the server and the clients; stepsize γ > 0; sparsification levels K ,...,K ∈ [d];
1 n
numberofiterationsT >0
2: Initialize: g0 = 1 (cid:80)n g0ontheserver
n i=1 i
3: fort=0,1,2,...,T −1do
4: Servercomputesxt+1 =xt−γgtandbroadcastsxt+1toallnclients
5: fori=1,...,nontheclientsinparalleldo
6: Computeut
i
=TopK i(∇f i(xt+1)−g it)andupdateg it+1 =g it+ut
i
7: Sendthecompressedmessageuttotheserver
i
8: endfor
9: Serverupdatesgt+1 =gt+utforalli∈[n],andcomputesgt+1 = 1 (cid:80)n gt+1
i i i n i=1 i
10: endfor
11: Output: PointxˆT chosenfromtheset{x0,...,xT−1}uniformlyatrandom
H.2 NEWSPARSITYMEASURE
Toextendourresultstotherarefeaturesregime,weneedtoslightlychangethedefinitionoftheparameter
c in the original paper. The way we do it is unrolled as follows. First, we recall the following definitions
from(Richta´riketal.,2023):
Z :={(i,j)∈[n]×[d]|[∇f (x)] =0∀x∈Rd}, (69)
i j
and
I :={i∈[n]|(i,j)∈/ Z}, J :={j ∈[d]|(i,j)∈/ Z}. (70)
j i
WealsoneedthefollowingdefinitionofRd:
i
Rd :={u=(u ,...,u )∈Rd :u =0whenever(i,j)∈Z}. (71)
i 1 d j
Nowwearereadyforanewdefinitionofthesparsityparameterc:
(cid:88)
c:=n·max w , (72)
i
j∈[d]
i∈Ij
52wherew isdefinedasin(25). Wenotethatcrecoversthestandarddefinitionfrom(Richta´riketal.,2023)
i
whenw = 1 foralli∈[n].
i n
H.3 LEMMAS
Wewillproceedthroughseverallemmas.
Lemma13. Letu ∈Rdforalli∈[n]. Then,thefollowinginequalityholds:
i i
(cid:13) (cid:13)(cid:88)n (cid:13) (cid:13)2
c
(cid:88)n
(cid:13) w u (cid:13) ≤ w ∥u ∥2. (73)
(cid:13) i i(cid:13) n i i
(cid:13) (cid:13)
i=1 i=1
Proof. Initially,weobservethat
(cid:13) (cid:13)(cid:88)n (cid:13) (cid:13)2 (cid:88)d (cid:32) (cid:88)n (cid:33)2
(cid:13) w u (cid:13) = w u . (74)
(cid:13) i i(cid:13) i ij
(cid:13) (cid:13)
i=1 j=1 i=1
Wenotethatforanyj ∈[d]itholdsthat
(cid:32) n (cid:33)2  2
(cid:88) (cid:88)
w iu ij = w iu ij
i=1 i∈Ij
 2 2  2 (75)
=(cid:88) w i  (cid:88) (cid:80)w i w u ij  ≤(cid:88) w i (cid:88) (cid:80)w i w u2 ij,
i′ i′
i∈Ij i∈Ij
i′∈Ij
i∈Ij i∈Ij
i′∈Ij
whereonthelastlineweusedtheJensen’sinequality. Subsequentarithmeticmanipulationsandtheincor-
porationofdefinition(72)yield:
(cid:32) n (cid:33)2  
(cid:88) (75) (cid:88) (cid:88)
w iu ij ≤  w i· w iu2 ij
i=1 i∈Ij i∈Ij
 
n
(cid:88) (cid:88)
=  w i· w iu2 ij
i∈Ij i=1
  
n
(cid:88) (cid:88)
≤ max w i· w iu2
ij
j∈[d]
i∈Ij i=1
n
( =72) c (cid:88) w u2 . (76)
n i ij
i=1
SubstitutingEquation(76)intoEquation(74)completestheproof.
Lemma14. Assumethatg0 ∈Rdforalli∈[n]. Then,itholdsforallt>0that
i i
c
∥gt−∇f(xt)∥2 ≤ Gt. (77)
n
53Proof. By Lemma 8 in Richta´rik et al. (2023), for EF21 the update gt stays in Rd if g0 ∈ Rd. We then
i i i i
proceedasfollows:
∥gt−∇f(xt)∥2 (
=9)(cid:13) (cid:13) (cid:13)1 (cid:88)n
gt−∇f
(xt)(cid:13) (cid:13) (cid:13)2 =(cid:13) (cid:13) (cid:13)(cid:88)n
w
(cid:20) 1
(gt−∇f
(xt))(cid:21)(cid:13) (cid:13) (cid:13)2
. (78)
(cid:13)n i i (cid:13) (cid:13) i nw i i (cid:13)
(cid:13) (cid:13) (cid:13) i (cid:13)
i=1 i=1
Since gt ∈ Rd, as was noted, and ∇f (xt) ∈ Rd, by the definition of Rd, then 1 (gt −∇f (xt)) also
i i i i i nwi i i
belongstoRd. ByLemma13,wefurtherproceed:
i
∥gt−∇f(xt)∥2 ( =78)
(cid:13) (cid:13) (cid:13)(cid:88)n
w
(cid:20) 1
(gt−∇f
(xt))(cid:21)(cid:13) (cid:13) (cid:13)2
(cid:13) i nw i i (cid:13)
(cid:13) i (cid:13)
i=1
( ≤73) nc (cid:88)n w i(cid:13) (cid:13) (cid:13) (cid:13)n1
w
(g it−∇f i(xt))(cid:13) (cid:13) (cid:13) (cid:13)2
i
i=1
n
c (cid:88) 1 c
= ∥gt−∇f (xt)∥2 = Gt,
n n2w i i n
i
i=1
whichcompletestheproof.
Fortheconvenienceofthereader,webrieflyrevisitLemma6from(Richta´riketal.,2023).
Lemma15(Lemma6fromRichta´riketal.(2023)). IfAssumption2holds,thenfori∈[n],wehave
(cid:88) (cid:88)
((∇f (x)) −(∇f (y)) )2 ≤L2 (x −y )2 ∀x,y ∈Rd. (79)
i j i j i j j
j:(i,j)∈/Z j:(i,j)∈/Z
Now, we proceed to the following lemma, which aims to provide a tighter bound for the quantity
(cid:80)n 1 ∥∇f (x)−∇f (y)∥2.
i=1 Li i i
Lemma16. IfAssumption2holds,then
n
(cid:88) 1
∥∇f (x)−∇f (y)∥2 ≤cL ∥x−y∥2. (80)
L i i AM
i
i=1
Proof. Theproofcommencesasfollows:
n n
(cid:88) 1 (cid:88) 1 (cid:88)
∥∇f (x)−∇f (y)∥2 = ((∇f (x)) −(∇f (y)) )2
L i i L i j i j
i i
i=1 i=1 j:(i,j)∈/Z
n
(79) (cid:88) 1 (cid:88)
≤ L2 (x −y )2
L i j j
i
i=1 j:(i,j)∈/Z
n
(cid:88) (cid:88)
= L (x −y )2
i j j
i=1j:(i,j)∈/Z
 
d d
(cid:88) (cid:88) (cid:88) (cid:88)
= L i(x
j
−y j)2 = (x
j
−y j)2 L i(.81)
j=1i:(i,j)∈/Z j=1 i:(i,j)∈/Z
54(cid:80)
Toadvanceourderivations,weconsiderthemaximumvalueover L :
i
i:(i,j)∈/Z
 
n d
(cid:88) 1 (81) (cid:88) (cid:88)
L ∥∇f i(x)−∇f i(y)∥2 ≤ (x j −y j)2 L i
i
i=1 j=1 i:(i,j)∈/Z
 
d
(cid:88) (cid:88)
≤ (x j −y j)2max L i
j∈[d]
j=1 i:(i,j)∈/Z
 
d
(cid:88) (cid:88)
= max L i (x j −y j)2
j∈[d]
i:(i,j)∈/Z j=1
 
(cid:88)
= max L i∥x−y∥2
j∈[d]
i:(i,j)∈/Z
 
= max(cid:88) L i∥x−y∥2 ( =72) cL AM∥x−y∥2,
j∈[d]
i∈Ij
whatcompletestheproof.
Forclarityandeasyreference,werecapitulateLemma10fromRichta´riketal.(2023).
Lemma17(Lemma10fromRichta´riketal.(2023)). TheiteratesofAlgorithm6methodsatisfy
(cid:13) (cid:13)g it+1−∇f i(xt+1)(cid:13) (cid:13)2 ≤(1−θ(α))(cid:13) (cid:13)g it−∇f i(xt)(cid:13) (cid:13)2 +β(α)∥∇f i(xt+1)−∇f i(xt)∥2, (82)
(cid:26) (cid:27)
whereα=min min Ki ,1 .
i∈[n]|Ji|
Lemma18. UnderAssumption2,iteratesofAlgorithm6satisfies
c
Gt+1 ≤(1−θ(α))Gt+β(α) L2 ∥x−y∥2. (83)
n AM
55Proof. TheproofisacombinationofLemmas16and17:
n
Gt+1 ( =36) n1
2
(cid:88) w1 (cid:13) (cid:13)g it+1−∇f i(xt+1)(cid:13) (cid:13)2
i
i=1
n
( ≤82) 1 (cid:88) 1 (cid:2) (1−θ(α))∥gt−∇f (xt)∥2+β(α)∥∇f (xt+1)−∇f (xt)∥2(cid:3)
n2 w i i i i
i
i=1
n
β(α)(cid:88) 1
= (1−θ(α))Gt+ ∥∇f (xt+1)−∇f (xt)∥2
n2 w i i
i
i=1
 
n n
( =25) (1−θ(α))Gt+ β n(α 2) (cid:88) L j(cid:88) L1 ∥∇f i(xt+1)−∇f i(xt)∥2
i
j=1 i=1
 2
n
(80) β(α) c (cid:88)
≤ (1−θ(α))Gt+
n2
n L j ∥x−y∥2
j=1
c
= (1−θ(α))Gt+β(α) L2 ∥x−y∥2.
n AM
H.4 MAINRESULT
Andnowwearereadytoformulatethemainresult.
Theorem11. LetAssumptions1,2and3hold. Letg0 ∈Rdforalli∈[n],
i i
(cid:26) (cid:27)
K 1
α=min min i ,1 , 0<γ ≤ .
i∈[n]|J i| L+ ncL AMξ(α)
Undertheseconditions,theiteratesofAlgorithm6satisfy
1 T (cid:88)−1 2(f(x0)−f∗) c G0
∥∇f(xt)∥2 ≤ + . (84)
T γT nθ(α)T
t=0
Proof. LetusdefinetheLyapunovfunction:
γc
Ψt :=f(xt)−f∗+ Gt. (85)
2θn
56Westarttheproofasfollows:
γc
Ψt+1 ( =85) f(xt+1)−f∗+ Gt+1
2θn
(cid:18) (cid:19)
(20) γ 1 L γ γc
≤ f(xt)−f∗− ∥∇f(xt)∥2− − ∥xt+1−xt∥2+ ∥gt−∇f(xt)∥2+ Gt+1
2 2γ 2 2 2θn
(cid:18) (cid:19)
(77) γ 1 L γ c γc
≤ f(xt)−f∗− ∥∇f(xt)∥2− − ∥xt+1−xt∥2+ Gt+ Gt+1
2 2γ 2 2n 2θn
(cid:18) (cid:19)
(82) γ 1 L
≤ f(xt)−f∗− ∥∇f(xt)∥2− − ∥xt+1−xt∥2
2 2γ 2
γ c γc (cid:16) c (cid:17)
+ Gt+ (1−θ)Gt+β L2 ∥xt+1−xt∥2
2n 2θn n AM
γc γ (cid:18) 1 L γβ c2 (cid:19)
= f(xt)−f∗+ Gt− ∥∇f(xt)∥2− − − ·L2 ∥xt+1−xt∥2
2θn 2 2γ 2 2 θ n2 AM
γ (cid:18) 1 L γβ c2 (cid:19)
= Ψt− ∥∇f(xt)∥2− − − ·L2 ∥xt+1−xt∥2
2 2γ 2 2 θ n2 AM
(cid:124) (cid:123)(cid:122) (cid:125)
≥0
γ
≤ Ψt− ∥∇f(xt)∥2.
2
Unrollingtheinequalityabove,weget
T−1
γ γ (cid:88)
0≤ΨT ≤ΨT−1− ∥∇f(xT−1)∥2 ≤Ψ0− ∥∇f(xt)∥2,
2 2
t=0
fromwhatthemainresultfollows.
57I EXPERIMENTS: FURTHER DETAILS
2e+03
Top1
Top5
1.8e+03 Top10
Top50
1.5e+03 Top100
1.2e+03
1e+03
7.5e+02
5e+02
2.5e+02
0
0 200 400 600 800 1000
Dimension d
(cid:112)
Figure4: Thefactorξ = β/θasafunctionofoptimizationvariabledimensiondforseveralTopKcompressors. The
behaviorisindependentofpropertiesof{f (x),...,f (x)}andf(x).
1 n
I.1 COMPUTINGANDSOFTWAREENVIRONMENT
WeusedthePythonsoftwaresuiteFL PyTorch(Burlachenkoetal.,2021)tosimulatethedistributeden-
vironment for training. We carried out experiments on a compute node with Ubuntu 18.04 LTS, 256
GBytes of DRAM DDR4 memory at 2.9GHz, and 48 cores (2 sockets with 24 cores per socket) of In-
tel(R)Xeon(R)Gold6246CPUat3.3GHz. Weuseddouble-precisionarithmeticduringcomputinggradient
oracles. AllourcomputationswerecarriedonCPU.
I.2 COMMENTSONTHEIMPROVEMENT
ThestandardEF21analysis(Richta´riketal.,2021)allowstoutilizeEF21withmaximumallowablestepsize
γ equalto:
(cid:32) (cid:115) (cid:33)−1
β(α) √ 1−α
γ = L+L , θ(α)=1− 1−α, β(α)= √ .
QM θ(α) 1− 1−α
Our analysis allows us to replace the quantity L with L . This improvement has an important con-
QM AM
(cid:113)
sequence. The replaced quantity affects the step size by a factor of ξ(α) = β(α). This factor can be
θ(α)
arbitrarilylargeasdincreases, asshowninFigure4. Ifdisincreasingandtheparameterk ofTopKcom-
pressor is fixed, then even a small improvement in the constant term can have a significant impact in an
absolutesensetothecomputedstepsizeifξ(α)≫L.
I.3 WHENIMPROVEDANALYSISLEADSTOMOREAGGRESSIVESTEPS
(cid:113)
ThequantityL
QM
:= n1 (cid:80)n i=1L2
i
playsessentialroleinEF21analysis. Aswesawwithspecialconsider-
ationthisquantityforEF21anditsextensionsisimprovable. Theimprovedanalysisallowsustoreplaceit
withL := 1 (cid:80)n L .Clearly,bythearithmetic-quadraticmeaninequality,
AM n i=1 i
L :=L2 −L2 ≥0.
var QM AM
58
β θrThedifferenceL −L canbeexpressedasfollows:
QM AM
(cid:18) (cid:19)
L +L
L −L = (L −L ) QM AM
QM AM QM AM L +L
QM AM
=
L2 QM−L2
AM =
1
·
1 (cid:88)n (cid:32)
L −
1 (cid:88)n
L
(cid:33)2
.
L +L L +L n i n i
QM AM QM AM
i=1 i=1
Thecoefficient 1 inthelastequationcanbeboundedfrombelowandaboveasfollows:
LQM+LAM
1 1 1 1 1
= ≤ ≤ ≤ .
2L 2·max(L ,L ) L +L 2·min(L ,L ) 2L
QM QM AM QM AM QM AM AM
As a consequence, difference L − L is bound above by the estimated variance of L divided by
QM AM i
the mean of L , also known as Index of Dispersion in statistics. From this consideration, we can more
i
easilyobservethatEF21-WcanhaveanarbitrarilybetterstepsizethanvanillaEF21ifthevarianceofL
i
is
increasingfasterthanthemeanofL .
i
I.4 DATASETGENERATIONFORSYNTHETICEXPERIMENT
First, we assume that the user provides two parameters: µ ∈ R and L ∈ R . These parameters define
+ +
theconstructionofstronglyconvexfunctionf (x),whicharemodifiedbymeta-parametersq ∈[−1,1]and
i
z >0,describednext.
1. Eachclientinitiallyhas
1
f (x):= ∥A x−b ∥2,
i n i i
i
whereA isinitializedinsuchwaythatf isL smoothandµ stronglyconvex. Parametersare
i i i fi
definedinthefollowingway:
i
L = ·(L−µ)+µ, µ =µ.
i n fi
2. Thescalarvalueq ∈[−1,+1]informallyplaystheroleofmetaparametertochangethedistribution
ofL andmakevaluesofL closetooneofthefollowing: (i)µ;(ii)L;(iii)(L+µ)/2. Theexact
i i
modificationofL dependsonthesignofmetaparameterq.
i
• Case q ∈ [0,1]. In this case for first n/2 (i.e., i ∈ [0,n/2]) compute the value L =
i,q
lerp(L ,µ,q),wherelerp(a,b,t):Rd×Rd×[0,1]→Rdisstandardlinearinterpolation
i
lerp(a,b,t)=a(1−t)+bt.
Thelastn/2(i∈[n/2+1,n])computethevalueL =lerp(L ,L,q).Forexample,ifq =0
i,q i
thenL =L ,∀i∈[n],andifq =1thenL =µforfirstn/2clientsandL =Lforlast
i,q i i,q i,q
n/2clients.
• Caseq ∈ [−1,0]. InthisforallnclientsthenewvalueL = lerp(L ,(L+µ)/2,−q). In
i,q i
thiscaseforexampleifq =0thenL =L andifq =−1thenL =(L+µ)/2.
i,q i i,q
The process firstly fills the A in such form that L forms a uniform spectrum in [µ,L] with the
i i
centerofthisspectrumequaltoa= L+µ. Andthenasq →1,thevarianceofL isincreasing.
2 i,q
3. WeusethesenewvaluesL foralli∈[n]clientsasafinaltargetLnew values. Duetonumerical
i,q i
issues, we found that it’s worthwhile for the first and last client to add extra scaling. First client
scalesL byfactor1/z,andlastn-thclientscalesL byfactorz. Herez ≥ 0isanadditional
1,q n,q
meta-parameter.
594. Next obtained values are used to generate A in such way that ∇2f (x) has uniform spectrum in
i i
[µ ,L ].
fi,q,z i,q,z
5. Asalaststeptheobjectivefunctionf(x)isscaledinsuchawaythatitisLsmoothwithconstant
valueL. Theb foreachclientisinitializedasb :=A ·x ,wherex isfixedsolution.
i i i solution solution
I.5 DATASETSHUFFLINGSTRATEGYFORLIBSVMDATASET
Our dataset shuffling strategy heuristically splits data points so that L is maximized. It consists of the
var
followingsteps:
1. SortdatapointsfromthewholedatasetaccordingtoLconstants.Sortalldatapointsaccording
tothesmoothnessconstantsofthelossfunctionforeachsingledatapoint.
2. AssignasingledatapointtoeachclientAssumethattherearetotalmdatapointsinthedatasets,
andthetotalnumberofclientsisn. Atthebeginningeachclientiholdsasingledatapoint⌊(i−
1+1/2)· m⌋.
n
3. Pass through all points. Initialize set F = {}. Next, we pass through all points except those
assigned from the previous step. For each point we find the best client i′ ∈ [n]\F to assign the
pointinawaythatassignmentofpointtoclienti′maximizeL :=L2 −L2 . Oncetheclient
var QM AM
alreadyhas⌈m⌉datapointsassignedtoit,theclientisaddedtothesetF.
n
ThesetF inthelaststepguaranteeseachclientwillhave m datapoints.Ingeneral,thisisaheuristicgreedy
n
strategy that approximately maximizes L under the constraint that each client has the same amount of
var
datapointsequalto⌊m⌋. Duetoitsheuristicnature,theAlgorithmdoesnotprovidedeepguarantees,butit
n
wasgoodenoughforourexperiments.
60105 EE FF 22 11 - W 103
100
103 103
106
101
109
E
E
F F2 21 1- W
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Rounds Rounds
(a)L ≈4.45×106 (b)L ≈1.97×106
var var
104 102 E F 2 1 -W
101 103 EF21
106 108
1011 1013
1016 E E F F 2 21 1- W 1018
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Rounds Rounds
(c)L ≈1.08×105 (d)L ≈5.42×103
var var
Figure 5: Convex smooth optimization. EF21 and EF21-W with Top1 client compressor, n = 2000, d = 10. The
objectivefunctionisconstituteoff (x)definedinEq.(86).
Regularizationtermλ∥x∥2
,whereλ = 0.01. Theoretical
i 2
stepsize.Fullparticipation.ExtradetailsareinTable1.
J ADDITIONAL EXPERIMENTS
Inthissection,wepresentadditionalexperimentsforcomparisonEF21-W,EF21-W-PP,EF21-W-SGDwith
their vanilla versions. We applied these algorithms in a series of synthetically generated convex and non-
convex optimization problems and for training logistic regression with non-convex regularized with using
severalLIBSVMdatasets(Chang&Lin,2011). Whilecarryingoutadditionalexperimentswewillusethree
quantities. Thesequantitieshavealreadybeenmentionedinthemainpart,butwewillrepeatthemhere:
(cid:118)
(cid:117) n n n (cid:32) n (cid:33)2
(cid:117)1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
L
QM
:=(cid:116)
n
L2 i, L
AM
:=
n
L i, L
var
:=L2 QM−L2
AM
=
n
L i−
n
L
i
.
i=1 i=1 i=1 i=1
TherelationshipbetweenthesequantitieswasdiscussedinAppendixI.3. InourexperimentsweusedTopK
compressor. TheTopKcompressorreturnssparsevectorsfilledwithzeros,exceptK positions,whichcor-
respondtoK maximumvaluesinabsolutevalueandwhichareunchangedbythecompressor. Evenifthis
compressorbreakstiesarbitrarily,itispossibletoshowthatα= K. Thecompressorparameterαisdefined
d
withoutconsideringpropertiesoff . Thequantitiesβ,θ, β arederivedfromα,andtheydonotdependon
i θ
L .
i
J.1 ADDITIONALEXPERIMENTSFOREF21
Convex case with synthetic datasets. We aim to solve optimization problem (1) in the case when the
functionsf ,...,f arestronglyconvex. Inparticular,weworkwith
1 n
1 λ
f (x):= ∥A x−b ∥2+ ∥x∥2, (86)
i n i i 2
i
whereλ = 0.01. ItcanbeshownthatL = 2 λ (A ⊤A )+λ. Theresultofexperimentsfortraining
i ni max i i
linearregressionmodelwithaconvexregularizedispresentedinFigure5. Thetotalnumberofroundsfor
61
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||104 102 E F 2 1 - W
102 104 EF21
108 1010
1014 1016
1020
E
E
F
F
2
2
1
1
- W
1022
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Rounds Rounds
(a)L ≈4.45×106 (b)L ≈1.97×106
var var
103 EF21-W 103 E F 21-W
103 EF21 103 EF21
109 109
1015 1015
1021 1021
0 2000 4000 6000 8000 0 2000 4000 6000 8000 10000
Rounds Rounds
(c)L ≈1.08×105 (d)L ≈5.42×103
var var
Figure 6: Non-Convex smooth optimization. EF21 and EF21-W with Top1 client compressor, n = 2,000, d = 10.
Theobjectivefunctionisconstituteoff (x)definedinEq. (87). Regularizationtermλ(cid:80)d x2 j , withλ = 100.
i j=1 x2+1
j
Theoreticalstepsize.Fullclientparticipation.ExtradetailsareinTable2.
simulationisr =10,000.InstancesofoptimizationproblemsweregeneratedforvaluesL=50,µ=1with
several values of q,z with using the dataset generation schema described in Appendix I.4. The summary
of derived quantities is presented in Table 1. We present several optimization problems to demonstrate
the possible different relationships between L QM and L AM. As we see from experiments, the EF21-W is
superior as the variance of L tends to increase. The plots in Figure 5 (a)–(d) correspond to decreasing
i
varianceofL i. Aswesee,asthevarianceofL
i
decreases,thedifferencebetweenEF21-WandEF21also
tendstodecrease. Finally,EF21-WisalwaysatleastasbestasEF21.
Table1: ConvexOptimizationexperimentinFigure5.Quantitieswhichdefinetheoreticalstepsize.
(cid:113)
Tag L q z L ξ = β L L γ γ
var θ QM AM EF21 EF21−W
(a) 50 1 104 4.45×106 18.486 2111.90 52.04 2.55×10−5 9.87×10−4
(b) 50 1 103 1.97×106 18.486 1408.49 63.56 3.83×10−5 8.16×10−4
(c) 50 1 102 1.08×105 18.486 339.34 80.97 1.58×10−4 6.46×10−4
(d) 50 0.8 1 5.42×103 18.486 112.51 85.03 4.69×10−4 6.16×10−4
Non-convexcasewithsyntheticdatasets. Weaimtosolveoptimizationproblem(1)inthecasewhenthe
functionsf ,...,f arenon-convex. Inparticular,weworkwith
1 n
f (x):=
1
∥A x−b
∥2+λ·(cid:88)d x2
j . (87)
i n i i x2+1
i j=1 j
Theresultofexperimentsfortraininglinearregressionmodelwithanon-convexregularizationispresented
inFigure6. Theregularizationcoefficientλ=100. Instancesofoptimizationproblemsweregeneratedfor
valuesL=50,µ=1andseveralvaluesofq,zforemployeddatasetgenerationschemafromAppendixI.4.
62
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||The summary of derived quantities is presented in Table 2. We present various instances of optimization
problemstodemonstratethedifferentrelationshipsbetweenL andL . Asweseeinthecaseofsmall
QM AM
varianceofL ialgorithmEF21-WisatleastasbestasstandardEF21.
Table2: Non-convexoptimizationexperimentinFigure6.Quantitieswhichdefinetheoreticalstepsize.
(cid:113)
Tag L q z L ξ = β L L γ γ
var θ QM AM EF21 EF21−W
(a) 50 1 104 4.45×106 18.486 2126.25 252.035 2.52×10−5 2.03×10−4
(b) 50 1 103 1.97×106 18.486 1431.53 263.55 3.74×10−5 1.95×10−4
(c) 50 1 102 1.08×105 18.486 433.05 280.958 1.21×10−4 1.83×10−4
(d) 50 0.8 1 5.42×103 18.486 294.39 285.022 1.17×10−4 1.81×10−4
Non-convex logistic regression on benchmark datasets. We aim to solve optimization problem (1) in
thecasewhenthefunctionsf ,...,f arenon-convex. Inparticular,weworkwithlogisticregressionwith
1 n
anon-convexrobustifyingregularizationterm:
f (x):= 1 (cid:88)ni log(cid:0) 1+exp(−y ·a⊤x)(cid:1) +λ·(cid:88)d x2 j , (88)
i n ij ij x2+1
i j=1 j=1 j
where(a ,y )∈Rd×{−1,1}.
ij ij
We used several LIBSVM datasets (Chang & Lin, 2011) for our benchmarking purposes. The results are
presentedinFigure7andFigure8. Theimportantquantitiesfortheseinstancesofoptimizationproblems
aresummarizedinTable3.FromFigures7(a),(b),(c),wecanobservethatforthesedatasets,theEF21-Wis
better,andthiseffectisobservableinpractice.Forexample,fromtheseexamples,wecanobservethat12.5K
rounds of EF21-W corresponds to only 10K rounds of EF21. This improvement is essential for Federated
Learning,inwhichbothcommunicationroundsandcommunicateinformationduringaroundrepresentthe
mainbottlenecksandarethesubjectofoptimization. Figures7(d),(e),(f)demonstratethatsometimesthe
EF21-WcanhavepracticalbehaviorclosetoEF21,evenifthereisanimprovementinstep-size(Forexact
valuesofstepsizeseeTable3). TheexperimentonAUSTRALIANdatasetsarepresentedinFigure8. This
exampledemonstratesthatinthisLIBSVMbenchmarkdatasets,therelativeimprovementinthenumberof
roundsforEF21-WcomparedtoEF21isconsiderable. Forexample40KroundsofEF21correspondsto5K
roundsofEF21-Wintermsofattainable∥∇f(xt)∥2.
Table3: Non-convexoptimizationexperimentsinFigures7,8.Derivedquantitieswhichdefinetheoreticalstepsize.
Tag L Lvar
ξ=(cid:113)β
θ
LQM LAM γEF21 γEF21−W
(a)W1A 0.781 3.283 602.49 2.921 2.291 5.678×10−4 7.237×10−4
(b)W2A 0.784 2.041 602.49 2.402 1.931 6.905×10−4 8.589×10−4
(c)W3A 0.801 1.579 602.49 2.147 1.741 7.772×10−4 9.523×10−4
(d)MUSHROOMS 2.913 5.05×10−1 226.498 3.771 3.704 1.166×10−3 1.187×10−3
(e)SPLICE 96.082 2.23×102 122.497 114.43 113.45 7.084×10−5 7.14×10−5
(f)PHISHING 0.412 9.2×10−4 138.498 0.429 0.428 1.670×10−2 1.674×10−2
(g)AUSTRALIAN 3.96×106 1.1×1016 18.486 3.35×107 3.96×106 9.733×10−10 8.007×10−9
Non-convexlogisticregressionwithnon-homogeneouscompressor. Inthissupplementaryexperiment,
weleveragedtheAUSTRALIANLIBSVMdatasets(Chang&Lin,2011)totrainlogisticregression,incorpo-
63EF21-W EF21-W EF21-W
EF21 EF21 EF21
101 101 101
102 102 102
103
0 5000 10000 15000 20000 0 5000 10000 15000 20000 0 5000 10000 15000 20000
Rounds Rounds Rounds
(a)W1A (b)W2A (c)W3A
EF21-W EF21-W 102 EF21-W
EF21 EF21 EF21
101
100
103
102
101
0 5000 10000 15000 20000 0 5000 10000 15000 20000 0 5000 10000 15000 20000
Rounds Rounds Rounds
(d)MUSHROOMS (e)SPLICE (f)PHISHING
Figure 7: Non-Convex Logistic Regression: comparison of EF21 and EF21-W. The used compressor is Top1. The
numberofclientsn = 1,000. Regularizationtermλ(cid:80)d x2 j ,withλ = 0.001. Theoreticalstepsize. Fullclient
j=1 x2+1
j
participation.Theobjectivefunctionisconstituteoff (x)definedinEq.(88).ExtradetailsareinTable3.
i
106
6×105
EF21-W
4×105 EF21
0 10000 20000 30000 40000
Rounds
(g)AUSTRALIAN
Figure 8: Non-ConvexLogisticRegression: comparisonoftheperformanceofstandardEF21andEF21-W.Theused
compressorisTop1. Thenumberofclientsn=200. Regularizationtermλ(cid:80)d x2 j ,withλ=1,000. Theoretical
j=1 x2+1
j
stepsize.Theobjectivefunctionisconstituteoff (x)definedinEq.(88).ExtradetailsareinTable3.
i
ratinganon-convexsparsity-enhancedregularizationtermdefinedinEq. (88). Theexperimentfeaturedthe
utilizationofanon-homogeneouscompressorknownasNaturalbyHorva´thetal.(2022),belongingtothe
familyofunbiasedcompressorsandadheringtoDefinition3withw =1/8. Thiscompressor,inarandom-
ized manner, rounds the exponential part and zeros out the transferred mantissa part when employing the
standard IEEE 754 Standard for Floating-Point Arithmetic IEEE Computer Society (2008) representation
forfloating-pointnumbers. Consequently,whenusingasinglefloat-pointformat(FP32)duringcommuni-
cation,only9bitsofpayloadperscalarneedtobesenttothemaster,andtheremaining23bitsofmantissa
canbeentirelydropped.
TheexperimentresultsaredepictedinFigure9. Inthisexperiment,wefine-tunedthetheoreticalstepsize
bymultiplyingitwithaspecificconstant. AswecanseetheEF21-WconsistentlyoutperformsEF21across
allcorrespondingstep-sizemultipliers. Aswesee EF21-Woperateseffectivelyby utilizing unbiasednon-
64
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||
2||)x(f||105 106
100 EF21-W Nat: x1 101 EF21-W Nat: x1
EF21-W Nat: x6 EF21-W Nat: x6
10 5 EF21-W Nat: x20 10 4 EF21-W Nat: x20
10 10 EF21-W Nat: x40 10 9 EF21-W Nat: x40
EF21 Nat: x1 EF21 Nat: x1
10 15 EF21 Nat: x6 10 14 EF21 Nat: x6
10 20 E EF F2 21 1 N Na at t: : x x2 40 0 10 19 E EF F2 21 1 N Na at t: : x x2 40 0
0 10000 20000 30000 0 1 2 3 4 5
Rounds #bits/n 1e6
(a) (b)
Figure 9: Non-ConvexLogisticRegression: comparisonoftheperformanceofstandardEF21andEF21-W.Theused
compressorforEF21andEF21-WisNaturalcompressorHorva´thetal.(2022).Thenumberofclientsn=200.Theobjec-
tivefunctionisconstituteoff (x)definedinEq.(88).Regularizationtermλ(cid:80)d x2 j ,withλ=1,000.Multipliers
i j=1 x2+1
j
oftheoreticalstepsize.Fullparticipation.Computationformatsingleprecision(FP32).Dataset:AUSTRALIAN.
homogeneous compressors, and the advantages over EF21 extend beyond the scope of applying EF21-W
solely to homogeneous compressors. Finally, it is worth noting that the increased theoretical step size in
EF21-Wdoesnotentirelycapturethepracticalscenarioofpotentiallyenhancingthestepsizebyasignifi-
cantlylargemultiplicativefactor(e.g.,×40),whichremainsasubjectforfutureresearch.
65
2||)x(f
||
2||)x(f
||106
106 EF21-W-PP EF21-W-PP EF21-W-PP
105 EF21-PP 104 EF21-PP 103 EF21-PP
104
103 102 101
11 00 12 100 101
0 5000 10000 15000 20000 0 5000 10000 15000 20000 0 5000 10000 15000 20000
Rounds Rounds Rounds
(a)L =4.45×106 (b)L =1.97×106 (c)L =1.08×105
var var var
Figure 10: Convexsmoothoptimization. EF21-PPandEF21-W-PPwithTop1clientcompressor, n = 2000, d = 10.
Theobjectivefunctionisconstituteoff (x)definedinEq. (89).
Regularizationtermλ∥x∥2
,λ = 0.01. Theoretical
i 2
stepsize. Theobjectivefunctionisconstituteoff (x)definedinEq.(89). Eachclientparticipatesineachroundwith
i
probabilityp =0.5.ExtradetailsareinTable4.
i
J.2 ADDITIONALEXPERIMENTSFOREF21-W-PP
Convexcasewithsyntheticdatasets.
Table4: ConvexoptimizationexperimentinFigure10.Derivedquantitieswhichdefinetheoreticalstepsize.
(cid:113)
Tag L q z L β L L γ γ
var θ QM AM EF21−PP EF21−W−PP
(a) 50 1 104 4.45×106 18.486 2111.90 52.04 2.55×10−5 9.87×10−4
(b) 50 1 103 1.97×106 18.486 1408.49 63.56 3.83×10−5 8.16×10−4
(c) 50 1 102 1.08×105 18.486 339.34 80.97 1.58×10−4 6.46×10−4
Weaimtosolveoptimizationproblem(1)inthecasewhenthefunctionsf ,...,f arestronglyconvex. In
1 n
particular,wechoose:
1 λ
f (x):= ∥A x−b ∥2+ ∥x∥2. (89)
i n i i 2
i
In this synthetic experiment, we have used the maximum allowable step size suggested by the theory of
EF21-PPandfortheproposedEF21-W-PPalgorithm. Theinitialgradientestimatorshavebeeninitialized
asg0 =∇f (x0)foralli.Thenumberofclientsinsimulationn=2000,dimensionofoptimizationproblem
i i
d = 10,numberofsamplesperclientn = 10,andnumberofcommunicationroundsisr = 10,000. For
i
bothEF21-PPandEF21-W-PPclientsweusedTop1biasedcontractilecompressor. Inourexperiment,each
client’s participation in each communication round is governed by an independent Bernoulli trial which
takesp = 0.5. Theresultofexperimentsfortraininglinearregressionmodelwithaconvexregularizeris
i
presentedinFigure10. Theregularizationconstantwaschosentobeλ = 0.01. Instancesofoptimization
problemsweregeneratedforvaluesL=50,µ=1withseveralvaluesofqandz. Thesummaryofderived
quantities is presented in Table 4. We present several optimization problems to demonstrate the possible
differentrelationshipsbetweenL
QM
andL AM. Asweseefromexperiments,theEF21-W-PPissuperioras
thevarianceofL itendstoincrease. AswecanobserveEF21-W-PPisalwaysatleastasbestasEF21-PP.
Non-convex logistic regression on benchmark datasets. We provide additional numerical experiments
in which we compare EF21-PP and EF21-W-PP for solving (1). We address the problem of training a
binary classifier via a logistic model on several LIBSVM datasets (Chang & Lin, 2011) with non-convex
66
2||)x(f|| 2||)x(f|| 2||)x(f||EF21-W-PP EF21-W-PP
EF21-PP EF21-PP
101 101
102 102
0 10000 20000 30000 40000 0 10000 20000 30000 40000
Rounds Rounds
(a)W1A (b)W2A
EF21-W-PP 102 EF21-W-PP
EF21-PP EF21-PP
101
102 103
0 10000 20000 30000 40000 0 10000 20000 30000 40000
Rounds Rounds
(c)W3A (d)PHISHING
Figure 11: Non-ConvexLogisticRegression: comparisonofEF21-PPandEF21-W-PP.TheusedcompressorisTop1.
Thenumberofclientsn = 1,000. Regularizationtermλ(cid:80)d x2 j ,λ = 0.001. Theoreticalstepsize. Eachclient
j=1 x2+1
j
participatesineachroundwithprobabilityp = 0.5. Theobjectivefunctionisconstituteoff (x)definedinEq.(90).
i i
ExtradetailsareinTable5.
regularization. We consider the case when the functions f ,...,f are non-convex; in particular, we set
1 n
f (x)asfollows:
i
f (x):= 1 (cid:88)ni log(cid:0) 1+exp(−y ·a⊤x)(cid:1) +λ·(cid:88)d x2 j , (90)
i n ij ij x2+1
i j=1 j=1 j
where(a ,y )∈Rd×{−1,1}.
ij ij
We conducted distributed training of a logistic regression model on W1A, W2A, W3A, PHISHING, and
AUSTRALIANdatasetswithnon-convexregularization.Theinitialgradientestimatorsaresetg0 =∇f (x0)
i i
foralli∈[n]. ForcomparisonofEF21-PPandEF21-W-PP,weusedthelargeststepsizeallowedbytheory.
WeusedthedatasetshufflingstrategydescribedinAppendixI.5. TheresultsarepresentedinFigure11and
Figure12. TheimportantquantitiesfortheseinstancesofoptimizationproblemsaresummarizedinTable5.
Table5: Non-ConvexoptimizationexperimentsinFigures11,12.Quantitieswhichdefinetheoreticalstepsize.
Tag L L L L γ γ
var QM AM EF21−PP EF21−W−PP
0.781 3.283 2.921 2.291 2.315×10−4 2.95×10−4
(a)W1A
0.784 2.041 2.402 1.931 2.816×10−4 3.503×10−4
(b)W2A
0.801 1.579 2.147 1.741 3.149×10−4 3.884×10−4
(c)W3A
0.412 9.2×10−4 0.429 0.428 6.806×10−3 6.823×10−3
(d)PHISHING
3.96×106 1.1×1016 3.35×107 3.96×106 3.876×10−10 3.243×10−9
(e)AUSTRALIAN
67
2||)x(f
||
2||)x(f
||
2||)x(f
||
2||)x(f
||106
105
EF21-W-PP
EF21-PP
0 20000 40000 60000 80000
Rounds
(e)AUSTRALIAN
Figure12: Non-ConvexLogisticRegression:comparisonofEF21-PPandEF21-W-PP.TheusedcompressorisTop1.The
numberofclientsn = 200. Regularizationtermλ(cid:80)d x2 j ,withλ = 1,000. Theoreticalstepsize. Eachclient
j=1 x2+1
j
participatesineachroundwithprobabilityp = 0.5. Theobjectivefunctionisconstituteoff (x)definedinEq.(90).
i i
ExtradetailsareinTable5.
FromFigure11(a),(b),(c),wecanobservethatforthesedatasets,theEF21-W-PPisbetter,andthiseffectis
observableinpracticeandisnotnegligible.Figures11(d),demonstratethatsometimesEF21-W-PPinterms
ofthefullgradientatlastiteratecanhaveslightlyworsebehaviorcomparedtoEF21-PP,eventhoughtheory
allowmoreaggressivestep-size(ForexactvaluesofstepsizeseeTable5.TheexperimentonAUSTRALIAN
datasetispresentedinFigure12. ThisexampledemonstratesthatinthisLIBSVMbenchmarkdatasets,the
relativeimprovementinthenumberofroundsforEF21-W-PPcomparedtoEF21-PPisconsiderable. The
EF21-W-PPexhibitsmoreoscillationbehaviorintermsof∥∇f(xt)∥2 forAUSTRALIANdataset,however
aswecanseeobserveinexpectation∥∇f(xt)∥2tendstodecreasefastercomparetoEF21-PP.
J.3 ADDITIONALEXPERIMENTSFOREF21-W-SGD
ThestandardEF21-SGDwiththeanalysisdescribedinCorollary4(Fatkhullinetal.,2021)allowsperform-
ingtheoptimizationprocedurewithmaximumallowablestepsizeuptothefactorof2equalto:
 (cid:115) −1
βˆ
γ EF21-SGD =L+ θˆ1L QM .
Inlastexpressionquantitiesθˆ=1−(1−α)(1+s)(1+ν),andβˆ =2(1−α)(1+s)(cid:0) s+ν−1(cid:1) .Improved
1
analysisforEF21-W-SGDallowstoapplystepsize:
 (cid:115) −1
βˆ
γ EF21-W-SGD =L+ θˆ1L AM .
Thereforeintermsofstepsize
γ ≥γ
EF21-W-SGD EF21-SGD
68
2||)x(f
||andEF21-W-SGDexhibitsamoreaggressivestep-size.
We conducted distributed training of a logistic regression model on W1A, W2A, W3A, PHISHING,
AUSTRALIANdatasetswithnon-convexregularization. Foralldatasets,weconsidertheoptimizationprob-
lem(1),where
f (x):= 1 (cid:88)ni log(cid:0) 1+exp(−y ·a⊤x)(cid:1) +λ(cid:88)d x2 j , (91)
i n ij ij x2+1
i j=1 j=1 j
and(a ,y )∈Rd×{−1,1}.
ij ij
Theinitialgradientestimatorsaresettog i0 = ∇f i(x0)foralli ∈ [n]. ForcomparisonofEF21-SGDand
EF21-W-SGD, we used the largest step size allowed by theory. The dataset shuffling strategy repeats the
strategythatwehaveusedforEF21-W-PPandEF21-WanditisdescribedinAppendixI.5. Thealgorithms
EF21-SGDand/EF21-W-SGDemployedanunbiasedgradientestimator,whichwasestimatedbysampling
asingletrainingpointuniformlyatrandomandindependentlyateachclient.
Table6: Non-ConvexoptimizationexperimentsinFigures13,14.Quantitieswhichdefinetheoreticalstepsize.
Tag L L L L γ γ
var QM AM EF21-SGD EF21-W-SGD
0.781 3.283 2.921 2.291 4.014×10−4 5.118×10−4
(a)W1A
0.784 2.041 2.402 1.931 4.882×10−4 6.072×10−4
(b)W2A
0.801 1.579 2.147 1.741 5.460×10−4 6.733×10−4
(c)W3A
0.412 9.2×10−4 0.429 0.428 1.183×10−2 1.186×10−2
(f)PHISHING
3.96×106 1.1×1016 3.35×107 3.96×106 3.876×10−10 3.243×10−9
(g)AUSTRALIAN
The results are presented in Figure 13 and Figure 14. The important quantities for these instances of op-
timization problems are summarized in Table 6. In all Figures 13 (a), (b), (c), (d) we can observe that
for these datasets, the EF21-W-SGD is better, and this effect is observable in practice. The experiment on
AUSTRALIANdatasetsarepresentedinFigure14. ThisexampledemonstratesthatinthisLIBSVMbench-
markdatasets,therelativeimprovementinthenumberofroundsforEF21-W-SGDcomparedtoEF21-SGD
isconsiderable. Finally,weaddressoscillationbehaviortothefactthatemployedstepsizeforEF21-SGDis
toopessimistic,anditsemployedstepsizeremovesoscillationof∥∇f(xt)∥2.
K REPRODUCIBILITY STATEMENT
To ensure reproducibility, we use the following FL PyTorch simulator features: (i) random seeds were
fixedfordatasynthesis;(ii)randomseedswerefixedfortheruntimepseudo-randomgeneratorsinvolvedin
EF21-PPandEF21-SGDacrossclientsandtheserver;(iii)thethreadpoolsizewasturnedofftoavoidthe
non-deterministicorderofclientupdatesintheserver.
Ifyouareinterestedinthesourcecodeforallexperiments,pleasecontacttheauthors.
69EF21-W-SGD EF21-W-SGD
EF21-SGD EF21-SGD
101 101
102
102
0 5000 10000 15000 20000 0 5000 10000 15000 20000
Rounds Rounds
(a)W1A (b)W2A
EF21-W-SGD 102 EF21-W-SGD
EF21-SGD EF21-SGD
101
102
103
103
0 5000 10000 15000 20000 0 5000 10000 15000 20000
Rounds Rounds
(c)W3A (f)PHISHING
Figure 13: Non-Convexlogisticregression: comparisonofEF21-SGDandEF21-W-SGD.TheusedcompressorisTop1.
TheSGDgradientestimatorisSGD-US,τ =1.Thenumberofclientsn=1,000.Theobjectivefunctionisconstituteof
f (x)definedinEq.(91).Regularizationtermλ(cid:80)d x2 j ,λ=0.001.Theoreticalstepsize.SeealsoTable6.
i j=1 x2+1
j
106
105
EF21-W-SGD
EF21-SGD
0 20000 40000 60000 80000
Rounds
AUSTRALIAN
Figure 14: Non-Convexlogisticregression: comparisonofEF21-SGDandEF21-W-SGD.TheusedcompressorisTop1.
TheSGDgradientestimatorisSGD-US,τ = 1. Thenumberofclientsn = 200. Theobjectivefunctionisconstituteof
f (x)definedinEq.(91).Regularizationtermλ(cid:80)d x2 j ,withλ=1,000.Theoreticalstepsize.Fullparticipation.
i j=1 x2+1
j
ExtradetailsareinTable6.
70
2||)x(f
||
2||)x(f
||
2||)x(f
||
2||)x(f
||
2||)x(f
||