Nash Equilibrium and Learning Dynamics
in Three-Player Matching m-Action Games
YumaFujimoto1,2,3 KaitoAriu3,4 KenshiAbe3,5
1SOKENDAI
2TheUniversityofTokyo
3CyberAgent
4KTH
5TheUniversityofElectro-Communications
Abstract
Nowé,2005,Tuylsetal.,2006,Bloembergenetal.,2015].
A representative method to analyze the dynamics is the
learningalgorithmnamedFollowtheRegularizedLeader
Learningingamesdiscussestheprocesseswhere (FTRL)[Shalev-ShwartzandSinger,2006,Mertikopoulos
multiple players learn their optimal strategies andSandholm,2016,Mertikopoulosetal.,2018],including
throughtherepetitionofgameplays.Thedynam- severalwell-knownalgorithms,suchasreplicatordynam-
icsoflearningbetweentwoplayersinzero-sum ics [Taylor and Jonker, 1978, Friedman, 1991, Hofbauer
games,suchasmatchingpennies,wheretheirben- etal.,1998,BörgersandSarin,1997,Hofbaueretal.,1998,
efitsarecompetitive,havealreadybeenwellana- Sato et al., 2002] and gradient ascent [Hofbauer and Sig-
lyzed.However,itisstillunexploredandchalleng- mund,1990,DieckmannandLaw,1996,Singhetal.,2000,
ing to analyze the dynamics of learning among Zinkevich,2003].Theabovematchingpenniesgameattracts
three players. In this study, we formulate a min- alotofattentiontodiscussthedynamics,wheretheirstrate-
imalistic game where three players compete to giesdrawacyclearoundtheNashequilibrium[Börgersand
matchtheiractionswithoneanother.Althoughin- Sarin,1997,Bloembergenetal.,2015].Suchcyclingdynam-
teractionamongthreeplayersdiversifiesandcom- icsarealsoobservedinsomevariationsofgames[Hofbauer
plicatestheNashequilibria,wefullyanalyzethe etal.,1998,Singhetal.,2000,BaileyandPiliouras,2019]
equilibria.Wealsodiscussthedynamicsoflearn- derivedfromthematchingpenniesandareunderstoodby
ingbasedonsomefamousalgorithmscategorized aconservedquantity[Piliourasetal.,2014,Mertikopoulos
into Follow the Regularized Leader. From both etal.,2018,BaileyandPiliouras,2019].Asatopicdevelop-
theoretical and experimental aspects, we charac- ingrecently,variousalgorithmsareproposedandachieve
terizethedynamicsbycategorizingthree-player theconvergencetotheNashequilibrium(calledlast-iterate
interactionsintothreeforcestosynchronizetheir convergence),especiallyingamesthatusuallyresultincy-
actions,switchtheiractionsrotationally,andseek cles[Bowling,2000,Anagnostidesetal.,2022,Fujimoto
competition. etal.,2024],wheretheirpayoffmatricescanbeseparated
intointeractionsbetweenpairsofplayers.Tosummarize,
analyzingthecyclingdynamicsofagents’strategiesmatters
1 INTRODUCTION
inlearningingames,wherethematchingpenniesgameisa
keystone.
Learningingamesconsidersthatmultipleagentsindepen-
dentlylearntheirstrategiestoincreasetheirutility[Fuden- Despitesuchathoroughunderstandingoftwo-playergames,
bergandLevine,1998].HowtoachievetheNashequilib- three-playergamesarestilllittleexplored.Jordan’sgame
rium[NashJr,1950],inwhichallagentsarenotmotivated isproposedasathree-playerversionofthematchingpen-
tochangetheirstrategies,isakeyissueinlearningingames. nies[Jordan,1993].Severalstudiesdiscussedlearningdy-
However,thisissueiscriticalwhentheirutilityfunctions namics (mainly based on fictitious play [Brown, 1951])
conflictwitheachother.Theminimumexampleofsucha in this game [Gaunersdorfer and Hofbauer, 1995, Mc-
conflict is matching pennies, where two agents have two Cabeetal.,2000,ShammaandArslan,2005,Mealingand
actions,andtheiroptimalactionsareinterdependentoneach Shapiro,2015],wheredivergencefromtheNashequilibrium
other,i.e.,conflict. isobserved.Consideringtheobservedcyclingdynamicsin
thematchingpenniesgameanditsvariants,itcanbeantici-
To resolve this issue, the dynamics of agents’ strategies
patedthatcertainthree-playerinteractionsmayqualitatively
havebeenenthusiasticallystudiedinrecentyears[Tuylsand
4202
beF
61
]TG.sc[
1v52801.2042:viXraalterlearningdynamics.However,howthree-playerinterac- are a and b (see the left panel of Fig. 1-C), respectively.
tioncanchangethestructureofthegame(e.g.,theregion Playerswhochoseadifferentactionfromtheothersreceive
ofNashequilibriumandtheglobalbehavioroflearningdy- thedefaultpayoffofc(seethecenter).Ifallthreeplayers
namics)isstillunclearingeneral.Indeed,fewbutsomestud- takethesameaction,theycommonlyreceivethescoresofϵ
iessupportthatsuchthree-playerinteractioncomplicates (seetheright).Here,weassumethatthewinner’sandloser’s
dynamics in three-player rock-paper-scissors [Sato et al., scoresarehighestandlowest,respectively,i.e.,b<c<a
2005]andsocialdilemma[AkiyamaandKaneko,2000]and andb<ϵ<a.
alsomakesithardtocalculateequilibriumstrategiesinrock-
paper-scissors[Grant,2023],socialdilemma[Muraseand
Baek,2018],andKuhnpoker[Szafronetal.,2013].Italso
shouldberemarkedthatgameAIsaredevelopingrapidlyin
severalmulti-playertablegames,includingPoker[Brown
andSandholm,2019],Mahjong[Lietal.,2020],andDiplo-
macy[Paquetteetal.,2019].Tosummarize,understanding
how three-player interactions change the game structure
iscrucialforgainingbetterinsightintoavarietyofthree-
playergames.
Thisstudyextendstheordinarymatchingpenniesgameto
athree-playergeneralm-actionversionandnameitThree-
PlayerMatchingm-Action(m-3MA)game.Wefirstderive
alltheNashequilibriainm-3MAandshowthattheequilib-
riaarediverseandcomplicateddependingontheparameters
ofthree-playerinteraction.Furthermore,weintroducethe Figure 1: A. Three players, X, Y, and Z, independently
continuous-time FTRL algorithm. Next, we analyze the choosetheiractions.Playerswhochoosethesameaction
dynamics of this algorithm in m-3MA for several repre- playthegametogether.B.Inthegame,playershaveathree-
sentativeregularizers,i.e.,theentropicandEuclideanones. waydeadlockrelationship.X,Y,andZareadvantageousto
Weobservethatthedynamicsprovidevariousbehaviorsde- Y,Z,andX,respectively.C.Thethreeplayersreceivetheir
pendingontheparametersofthree-playerinteraction:cycle ownscoresasaresultoftheiractionchoices.Whentwoof
aroundtheNashequilibria,convergencetothere,anddiver- thethreeplayers(XandYintheleftpanel)choosethesame
gencetoaheterocliniccycle.Wefurtherintroduceanovel action,thewinner’sscoreisa,whiletheloser’sscoreisb,
quantitywhichisthedegreeofsynchronizationamongthree followingthethree-waydeadlockrelationship.Anisolated
players’actions,andprovethatthisquantitycapturesthe player(Zinthecenterpanel)whochoosesadifferentaction
globalbehaviorofthedynamicsinsomecases. fromothersreceivesascoreofc.Ifallthreeplayerschoose
thesameaction(intherightpanel),theyreceivescoresofϵ.
Here,weassumeb<c<aandb<ϵ<a.
2 PRELIMINARY
2.1 THREE-PLAYERMATCHINGm-ACTION Wealsoformulatetheirmixedstrategiesandpayoffs.Let
GAMES 0≤x i ≤1denotetheprobabilitythatXchoosesactiona i.
X’sstrategyisdefinedasx:=(x ,··· ,x )∈∆m−1(the
1 m
We now assume a situation where one’s action is advan- m−1dimensionalsimplex).Similarly,Y’sandZ’sstrategies
tageous under a certain pair of the others’ actions but is are denoted by y ∈ ∆m−1 and z ∈ ∆m−1, respectively.
disadvantageousunderanotherpair.Thus,thepropertyof Whenplayersfollowsuchstrategies,X’sexpectedpayoffis
thematchingpenniesgameisinherited.Basedonthissitua- givenby
tion,wenowformulateThree-PlayerMatchingm-Action
(m-3MA)games. (cid:88) (cid:88)
u(x,y,z)=ϵ x y z +a x y z¯
i i i i i i
LetX,Y,andZdenotethreeplayers.Everyround,theyinde- i i
pendentlydeterminetheiractionsfromthesamem-action +b(cid:88) x y¯z +c(cid:88) x y¯z¯, (1)
i i i i i i
set,A={a ,··· ,a }(seeFig.1-A).Playerswhochoose
1 m i i
the same action interact with each other. This interaction
followsathree-waydeadlockrelationshipamongthem(see
wherewedefinedX¯ :=1−X forarbitraryvariableX.Y’s
Fig.1-B):XwinsY,YwinsZ,butZwinsX.Theyreceive
and Z’s expected payoffs are alsodescribed as u(y,z,x)
theirscores(seeFig.1-C).Whenonlytwooftheminteract,
andu(z,x,y),respectively.
thewinnerandloseraredeterminedfollowingthethree-way
deadlockrelationship,andthewinner’sandloser’sscores In learning in games, the gradient of this payoff is often
2important,calculatedas where we used the permutation function σ for m-
dimensionalsymmetrygroupS ,andalsoused
m
∂u(x,y,z)
γ
∂x i x ext := 2(γ−α), (5)
=ϵy z +ay z¯ +by¯z +cy¯z¯,
i i i i i i i i mx −1
δ := ext , (6)
=(ϵ−a−b+c)y iz i+(a−c)y i+(b−c)z i+c, m−2k
β+γ −β+γ x :=x +δ, (7)
=(α−γ)y z + y + z +c, + ext
i i 2 i 2 i x :=x −δ, (8)
− ext
=:f(y ,z ), (2)
i i x :=(x ,··· ,x ,x ,··· ,x ). (9)
DR;k + + − −
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
where we define α := ϵ−c, β := a−b > 0, and γ := k m−k
a+b−2c.Inthispayoffgradient,cistheoffsetandthus These equilibria are very complex because the player
negligible.Thus,wecancharacterizethem-3MAgamesby chooseseachactionwithtwodifferentprobabilitiesx and
+
thethreeparametersofα,β,andγ. x .
−
Inaddition,fortherangeofA={2,··· ,m−1},wedefine
3 NASHEQUILIBRIUM (cid:32) (cid:33)
(cid:91)
N′(m):=N (m)∪ Proj−1(N (m′)) , (10)
U U U
The Nash equilibrium of m-3MA is defined as the set of m′∈A
strategies(x∗,y∗,z∗)thatsatisfy
where Proj−1 denotes an inverse projection map from
 m′(< m)- to m-dimensional vector set with arbitrary
x∗ ∈argmax u(x,y∗,z∗)
 x permutations. Thus, N′(m) contains all the uniform-
y∗ ∈argmax yu(y,z∗,x∗) . (3) choice equilibria of m′U (< m)-action games. Here, we
z∗ ∈argmax u(z,x∗,y∗) split A into A(−) := {2,··· ,⌊1/(2x )⌋} and A(+) :=
z ext
{⌈1/(2x )⌉,··· ,m − 1} as A = A(−) ∪ A(+). Then,
ext
N′(−)(m) (resp. N′(+)(m)) denotes that the union set in
It is difficult to derive equilibrium in three-player U U
therangeofm′ ∈A(−)(resp.A(+)).N′ (m)issimilarly
games[DaskalakisandPapadimitriou,2005],andindeed, DR
defined.
therearefewsuccessfulstudies[Szafronetal.,2013,Grant,
2023].Nevertheless,wecanfullyanalyzealltheNashequi- Usingtheabovedefinition,wederivetheNashequilibriaas
libriainThm.1inSec.3.1.Becausethetheoremisbased follows.
onrigorousbutcomplicatedcalculations,itsvisualization
Theorem 1 (Nash equilibrium solution). For any Nash
andbriefinterpretationareprovidedinSec.3.2.
equilibrium(x∗,y∗,z∗),x∗ =y∗ =z∗holds,andtheset
ofone’sstrategiesisgivenby
3.1 FULLANALYSIS 
α=γ =0 ⇒∆m−1
We first introduce three types of strategies consisting of
0≥α,0≥γ
⇒N U(m)
theNashequilibrium.Thefirstistheuniform-choiceequi- α>0>γ ⇒N (m)∪N′(−)(m)∪N (m).
P U DR
l di ib mri eu nm sioN nU al(m all) -o: n= es{ v1 e/ cm to} r., Iw nh the ir se e1 quis ilid be rfi iun med ,ta hs et ph le aym er- α
γ
>≥ 00, >γ α≥0 ⇒ ⇒N NP ′(( +m )() m∪ )N ∪U′ N(m
′
)
(m)
choosesalltheactionsatperfectlyrandom.Thesecondisthe U DR
(11)
pure-strategyequilibriaN (m) := {e ,··· ,e },where
P 1 m
e istheunitvectorforthei-thaxisinthem-dimensional
i
space.Theseequilibriameanthattheplayerdeterministi- SeeAppendix.A.1-A.4foritscompleteproof.
callychoosesanaction(usesapurestrategy).Thethirdis ProofSketch.Step1(Lemma4):Weprovex∗ =y∗ =z∗
thedoublerootsequilibria;
bycontradiction.Inotherwords,wederiveacontradiction
from the assumption of x∗ ̸= y∗ ̸= z∗ ̸= x∗ or x∗ =

{σ(x )|σ ∈S ,k ≤1/(2x )} y∗ ̸=z∗.Thiscontradictionisprovedbecausethereisno
 DR;k m
(x
>1/mex )t
orderingamongf(y∗,z∗),f(z∗,x∗),andf(x∗,y∗).Step2
N DR(m):= ext , (Lemma5and6):Ui ndi ertheci ondi itionofx∗i =i y∗ = z∗,
{σ(x DR;k)|σ ∈S m, (xk ≥1 </( 12 /x
mex
)t)}
wefindthesetofone’sequilibriumstrategies.Boththecases
ext whenitsstrategyexistsintheinteriororontheboundaryof
(4)
thestrategyspacesareconsidered.
3Figure2:TheNashequilibriuminm-3MAwithm=3.ThisNashequilibriumcruciallychangesdependingonαandγ.
Eachpanelshowsthesimplex∆2ofone’sstrategy.Theedgesoftrianglesindicatethepurestrategies,i.e.,x=y =z =e ,
i
wherealltheplayerschooseonlyactioni.ThebluestarsrepresentN (3)(andProj−1(N (2))).Thegreenstarsrepresent
U U
N (3).Finally,theredstarsrepresentN (3).Here,notethatsinceweconsiderthecaseofm=3,Proj−1(N (2))=∅
P DR DR
alwayshold.Here,thepositionsofN changedependingonx ,whichisdeterminedbyαandγ.
DR ext
3.2 VISUALIZATIONANDINTERPRETATION classified as a zero-sum game which satisfies monotonic-
ity. Thus, no three-body effect works, and a continuous
TovisualizeThm.1,Fig.2illustratesallthecasesofthe regioncanbeequilibrium.(Pure-strategyequilibria)The
Nash equilibria. As this figure shows, m-3MA provides gameswithα>0haveapositivethree-bodyeffect,where
diverse and complicated Nash equilibrium structures de- positivepayoffsaregeneratedwhenthreeplayersinteract.
pending on α and γ. The Nash equilibria are especially In such games, the players are motivated to synchronize
complicated when α and γ conflict (i.e., α > 0 > γ or their actions. Thus, the states in which the three players
γ >0>α).ForfurtherinterpretationoftheNashequilib- perfectly synchronize their action choices, i.e., the pure-
ria,thefollowingcorollarysummarizesthemainproperties strategy equilibria, can be equilibrium. (Uniform-choice
oftheequilibria. equilibrium)Whentheothertwoplayerstakeallactions
atperfectlyrandom,i.e.,use1/m,alltheactionsareequiv-
Corollary1(MainpropertiesoftheNashequilibria). First,
alent for a player. Thus, the uniform-choice equilibrium
thefollowingpropertyalwaysholds.
x∗ = y∗ = z∗ = N (m)isalwaystheNashequilibrium
U
• (Player symmetry) For any Nash equilibrium, all independentofα.
threeplayerstakethesamestrategy,i.e.,x∗ =y∗ =
z∗.
4 CONTINUOUS-TIMEFOLLOWTHE
Furthermore,theregionoftheNashequilibriahasthefol- REGULARIZEDLEADER
lowingproperties.
Thisstudyconsidersthecontinuous-timeFollowtheRegu-
• (Neutralequilibria)Whenα=γ =0,allthestrate-
larizedLeader(FTRL),whichisformulatedas
giesinthesimplex∆m−1canbetheNashequilibria.
x=q(x†), (12)
• (Pure-strategyequilibria)N (m)={e ,··· ,e }
P 1 m
∂u
aretheNashequilibriumstrategies,ifandonlyifα≥ x˙† = , (13)
0. ∂x
q(x†)=argmax(cid:8) x†·x−h(x)(cid:9)
. (14)
• (Uniform-choiceequilibrium)N (m)={1/m}is
U x
alwaystheNashequilibriumstrategy.
Here,h(x)iscalledthe“regularizer”.Severalrepresentative
examples of this regularizer are the entropic regularizer
Proof.ThesepropertiesareimmediatelyderivedbyThm.1.
h(x) = x·logx and the Euclidean regularizer h(x) =
∥x∥2/2.Thefollowinglemmashowsthattheseregularizers
We now interpret these properties of the Nash equilibria. providethereplicatordynamicsandgradientascent,based
(Neutral equilibrium) The games with α = γ = 0 are ontheresultin[Mertikopoulosetal.,2018].
4Lemma 1 (Replicator dynamics and gradient ascent). x = y = z ∈ N (m).Fortheminimumvaluecondition,
P
(Replicator dynamics) In m-3MA, the continuous-time 0 ≤ Σ x y z triviallyholds.Here,thisequationholdsif
i i i i
FTRLwithh(x)=x·logxarecalculatedas andonlyifx y z = 0 ⇔ (x = 0)∨(y = 0)∨(z = 0)
i i i i i i
foralli∈{1,··· ,m}.
(cid:32) (cid:33)
(cid:88)
x˙ =x f(y ,z )− x f(y ,z ) . (15)
i i i i i i i
5.2 ANALYSISANDEXPERIMENTSOF
i
TWO-ACTIONGAMES
(Gradientascent)Intheinterioroftheplayers’strategy
spaces,thecontinuous-timeFTRLwithh(x)=∥x∥2/2are 5.2.1 TheoreticalAnalysis
calculatedas
Wenowconsiderm-3MAwiththecaseofm = 2.Since
x˙ i =f(y i,z i)− m1 (cid:88) f(y i,z i). (16) m hol= ds2 oh tho ald ts w, ex 2 ca= nd1 e− scx ri1 b, ey t2 he= le1 ar− niy n1 g, da yn nd az m2 i= cs1 by− thz e1
i
threevariablesof(x ,y ,z ).Thedynamicsgivenbythe
1 1 1
FTRLalgorithminthegameareindependentofγ,asproved
5 LEARNINGDYNAMICS
inthefollowing.
Let us consider learning dynamics by continuous-time Lemma3(Simplifieddynamicsform=2). Whenm=2,
FTRL.InSec.5.1,twoquantities,GandV,areintroduced thedynamicsinLem.1arecalculatedas
tocharacterizethelearningdynamics.InSec.5.2,wethe-
x˙ =w(x ){α(y +z −1)+β(y −z )}, (19)
oretically analyze the global behavior of the learning dy- 1 1 1 1 1 1
(cid:40)
namicsbyusingGandV.Finally,inSecs.5.2and5.3,we x (1−x ) (h(x)=x·logx)
simulatethelearningdynamicsform=2andm>2and w(x 1)= 1/1
2
1
(h(x)=∥x∥2/2)
, (20)
experimentallydemonstratehowgameparameters,i.e.,α,
β,andγ,contributetothedynamics. independentofγ.
5.1 CHARACTERIZATIONOFm-3MA SeeAppendix.A.5foritscompleteproof.
ProofSketch.Sincex =x¯ ,y =y¯ ,andz =z¯ holdin
2 1 2 1 2 1
Toinvestigatelearningdynamicsgivenbythecontinuous-
two-actiongames,thedynamicsinLem.1arerewrittenina
timeFTRL,weintroduceGandV as
simplerformbydirectcalculation.
G(x†,y†,z†):=Σ max{x†·x−h(x)}−x†·1/m, Under the entropic and Euclidean regularizers, the time
cyc
x
changesoftheseV andGarecharacterizedbythefollowing
(17)
twotheorems.
V(x,y,z):=Σ x y z −1/m2. (18)
i i i i
Theorem2(MonotonicityofV). Inm-3MAwithm=2,
Here, Σ cyc indicates the cyclic sum for three players. In theFTRLalgorithmwiththeentropicregularizerh(x) =
other words, Σ
cyc
F(x) = F(x)+F(y)+F(z) holds x·logxandEuclideanreguralizerh(x)=∥x∥2/2gives
forarbitraryfunctionF.Wenowexplainthemeaningsof
G and V. First, G is known to be conserved under zero- sign(V˙)=sign(α), (21)
sumgames[Mertikopoulosetal.,2018]andcorresponds
intheinteriorofstrategyspaceotherthanx = y = z =
to Kullback-Leibler divergence from the uniform-choice
1/m.
equilibrium. Next, V means the probability that all three
playerschoosethesameaction,inotherwords,thedegree
SeeAppendix.A.6foritscompleteproof.
ofsynchronizationoftheiractionchoices.Thefollowing
lemmagivesthebasicsofthisV; Proof Sketch. We calculate the dynamics of V by using
Eq.(19).Thetwotermsofαandβcontributetothedynam-
Lemma2(PropertiesofV). V takesitsmaximumvalueif
ics.Here,thetermofβ disappearsbythecyclicsymmetry
andonlyifx = y = z ∈ N (m)anditsminimumvalue
P amongthethreeplayers.Thetermofαisalwayspositive,
if and only if x = 0, y = 0, or z = 0 hold for each
i i i meaningthatthesignsofV˙ andαarethesame.
i∈{1,··· ,m}.
Theorem3(ConnectionbetweenGandV). Inm-3MAwith
Proof. For the maximum value condition, Σ x y z ≤ m = 2,theFTRLalgorithmwiththeentropicregularizer
i i i i
(cid:80) x y ≤ 1 trivially holds. The second equation holds h(x)=x·logxandEuclideanreguralizerh(x)=∥x∥2/2
i i i
if and only if x = y ∈ N (m) by the definition of an givesG˙ =2αV intheinteriorofstrategyspaceotherthan
P
innerproduct.Thefirstequationfurtherholdsifandonlyif x=y =z =1/m.
5Figure3:A.ThedynamicsofFTRLwiththeentropicregularizerinm-3MAwithm=2.Thedynamicsareoutputbythe
fourth-orderRunge-Kuttamethodwiththestep-sizeof2×10−2inallthepanels.Wealsocommonlyset(a,b,c)=(1,−1,0),
inotherwords,β =2.Intheleft,center,andrightpanels,wesetϵ(=α)=0.1,0,−0.1,respectively.Thered,green,and
bluelinesindicatethetimeseriesofx ,y ,andz ,respectively,whilethesolid,broken,anddottedlinesindicatei=1,2,
i i i
and3,respectively.ThesolidblacklineindicatesthetimeseriesofV.Theinitialstrategiesineachpanelarerandomly
sampledfromthestrategysimplexes.Intheleftandrightpanels,theNashequilibriaareplottedbytheblackstar.Inthe
centerpanel,allthepointsontheblacksolidlinearetheNashequilibria.Thebrokenlineintherightpanelisthesetof
statessatisfyingtheminimumV condition.B.ThedynamicsofFTRLwiththeEuclideanregularizer.Themethodand
simulationparametersarethesameaspanelA.
SeeAppendix.A.7foritscompleteproof. 5.2.2 ExperimentalUnderstanding
ProofSketch.ThedynamicsofGarecalculatedbyusing
Wenumericallydemonstratethelearningtrajectoriesinm-
Eq.(19)anddependonbothαandβ.Here,thetermofβ
3MAwithm=2.Fig.3-AandBshowthelearningdynam-
disappearsagainbythecyclicsymmetryamongthethree
icsbycontinuous-timeFTRLwiththeentropic(replicator
players.Thetermofαcorrespondsto2V.
dynamics)andEuclidean(gradientascent)regularizers,re-
Thesetwotheoremsexplaintheglobalbehaviorofthelearn- spectively.Eachfigureplotsthedynamicsinthethreecases
ingdynamicsasfollows. ofα>0(left),α=0(center),andα<0(right).Onecan
seesomedifferencesbetweenFig.3-AandB.Thetrajectory
Corollary 2 (Global behavior of dynamics). In m-3MA
ofthereplicatordynamicshasadistortedshapeandalways
withm=2,thecontinuous-timeFTRLgivesthefollowing
staysintheinteriorofthestrategyspace.Ontheotherhand,
propertiesexceptforwhenthetrajectoryconvergestothe
thetrajectoryofthegradientascenthasacircularshapeand
uniform-choiceequilibrium.
oftenstaysontheboundaryofthestrategyspace.Regard-
lessofsuchadifference,thetrajectorycommonlyshows
• When α = 0, both G and V are conserved in the
thepropertiesgivenbyCor.2asfollows.
trajectory.
• Whenα>0,thetrajectoryasymptoticallyconverges
Cycling behavior (α = 0): First, we see the case of
to the states of maximum V, i.e., either of the fixed
α = 0. Thm. 1 shows that the Nash equilibria are given
points.
by the diagonal line of x∗ = y∗ = z∗ ∈ ∆1. Then, the
• Whenα<0,thetrajectoryasymptoticallyconverges learningdynamicsalwaysgivecyclingbehavioraroundthis
tothestatesofminimumV,i.e.,theheterocliniccycle. diagonalline.AsCor.2proves,bothGandV areinvariant.
Indeed, we can see that V is constant in each trajectory
Proof.ThisisimmediatelyprovedbyLem.2,Thm.2,and (plottedbythesamecolor),whileG,thedistancefromthe
Thm.3. uniform-choiceequilibriuminthecenterpoint,alsoseems
6constantineachtrajectory. 0,playerssynchronizetheiractionsthroughoutthelearning:
convergence to the pure-strategy equilibria, i.e., N (m),
P
Convergence to the pure-strategy equilibria (α > 0): dependingontheirinitialstrategies.Whenα<0,players
In α > 0, the Nash equilibria are given by x∗ = y∗ = learntodesynchronizetheiractionsandcannotreachthe
z∗ ∈ N (2)∪N (2) = {e ,e ,1/2}.AsCor.2proves, onlyuniform-choiceequilibrium.Thesevariousdynamics
P U 1 2
thelearningdynamicsconvergetoeitheroneofthepure- are characterized by V = (cid:80) x y z − 1/m2, again. In
i i i i
strategyequilibriae ore dependingontheirinitialcondi- mathematics,weobtainthefollowingtheorem.
1 2
tion.Indeed,wecanseethatV monotonicallyincreasesin
Theorem4(MonotonicityofV form-3MA). Intherepli-
eachtrajectory(theplottedcolorchangesintoredmonoton-
cator dynamics in Lem. 1, except for the uniform-choice
ically). equilibrium x = y = z ∈ N (m), sign(V˙) = sign(α)
U
holds.
Heterocliniccycles(α<0): Inα<0,theNashequilib-
riumisonlyatx∗ =y∗ =z∗ ∈N (2)={1/2}.Interest-
U SeeAppendix.A.8foritscompleteproof.
ingly,thelearningdynamicsdonotreachthisNashequilib-
riumbutconvergetotheheterocliniccycleof(x ,y ,z )= ProofSketch.WecalculateV˙ bysubstitutingEq.(15).Be-
1 1 1
(0,0,1) → (0,1,1) → (0,1,0) → (1,1,0) → (1,0,0) → causeγ isignored,onlythetermsofαandβ contributeto
(1,0,1)→(0,0,1)→···.Thismeansthataseachplayer thedynamics.Here,thetermofβisnegligiblebythecyclic
cyclically changes his/her action, he/she more biasedly symmetryamongthethreeplayers.Furthermore,theterm
chooses the action. In this heteroclinic cycle, V takes its ofαisdescribedasthevarianceofy iz i,whichispositive
minimumvalue,andthusitisobservedthatCor.2holds. everywhereexceptforx=y =z ∈N U(m).
Theuniform-choiceequilibrium1/2isunstableandcannot
bereachedfromalmostalltheinitialconditions. Convergence to two-action games (γ > 0): We next
see the case of γ > 0 (see the three panels in the upper
α is the force of synchronization: Let us explain the row of Fig. 4). The dynamics approach those in the two-
convergenceinα>0andthedivergencetotheheteroclinic action games. As a simple example, see the dynamics in
cycleinα < 0.Considerwhetheraplayershouldchoose α=0.Neartheinitialtime,allplayerstakeallthreeactions
thesameactionasthetwoothers.Ifchoosing,theplayer stochastically. Throughout learning, however, one of the
obtains ϵ. Otherwise, c. In α > 0 ⇔ ϵ > c, the three three actions (a 2 in the panel) becomes not played at all.
playersprefertosynchronizetheiractionsmore,eventually Eventually,allplayerslearntouseonlytheothertwoactions
concentratingonchoosingeitheraction.Ontheotherhand, oscillatory.Inotherwords,thethree-actiongameconverges
inα<0⇔ϵ<c,theplayersprefertodesynchronizetheir toatwo-actiongameafterasufficientlylongtime.Thecases
actions. ofα̸=0canbeinterpretedsimilarly.Inα>0,theplayers
firstlearnnottouseoneofthreeactions(a inthepanel)
1
andsynchronizetheiractionsafterthat.Inα <0,players
β istheforceofrotation: Wealsointerpretβ = a−b.
firstlearnnottouseoneaction(a inthepanel)andfinally
Here,a(>0)isthescoreforawinner.Thus,thelargerais, 2
reachaheterocliniccycle.
themorequicklyplayersaremotivatedtolearnanadvan-
tageousaction.Ontheotherhand,b(< 0)isthescorefor
Divergencetom-actionsgames(γ <0): Wefinallysee
aloser.Thesmallerbmeansthatplayersquicklylearnto
the case of γ < 0 (see the three panels in the lower row
escapefrombeingexploited.Becausethreeplayershavea
ofFig.4).Thethreeplayersbasicallylearntodesynchro-
three-waydeadlockrelationship,theirstrategiesshowrota-
nizetheiractions.Indeed,thelower-middleandlower-right
tion.β indicatestheforceofrotation.
panelsaresimilartothemiddle-rightpanel(γ =0,α<0).
Thecaseofγ <0,α>0isanexception.Inthiscase,three
5.3 EXPERIMENTSOFMORETHAN playersfirstdesynchronizetheiractions(i.e.,V decreases
TWO-ACTIONGAMES in the lower-left panel), but synchronize their actions in
theend(i.e.,V converselyincreases).Becauseα(>0)and
Next,weconsiderm-3MAwithm>2,wherethelearning γ(<0)conflictinthiscase,non-monotonicdynamicsofV
dynamics are more complicated than the case of m = 2. areclearlyobserved.Whethertheiractionseventuallysyn-
Now,Fig.4showsthatthesedynamicsareclassifiedbynot chronizeordesynchronizedependsontheinitialcondition.
onlyαbutγ.
γ is the force to seek competition: We now interpret
Conservationofactionnumber(γ =0) :Letusconsider anotherthree-playerinteraction,γ =a+b−2c.First,note
thecaseofγ =0(seethethreepanelsinthemiddlerowof thatingameswithmorethantwoactions,threeplayersdo
Fig.4).Thiscaseisviewedastheextensionoftheabove not have to interact, i.e., they may choose different three
dynamicstogeneralm > 2.Inotherwords,whenα = 0, actionsseparately.Whentwooftheplayerscompetewith
thelearningdynamicsshowacyclingbehavior.Whenα> eachother,i.e.,choosethesameaction,thethreeplayers
7Figure4:ThedynamicsofFTRLwiththeentropicregularizer.Thedynamicsareoutputbythefourth-orderRunge-Kutta
method with the step-size of 2 × 10−2 in all the panels. For α > 0, = 0, and < 0, we set (ϵ,c) = (0.1,0), (0,0),
and (−0.1,0), respectively. For γ > 0, = 0, and < 0, we set (a,b,c) = (1.1,−0.9,0), (1,−1,0), and (0.9,−1.1,0),
respectively.
totallyobtainthepayoffofa+b+c.Ontheotherhand, furtherdemonstratedbysimulationthatlearningdynamics
whentheychoosedifferentthreeactions,theirtotalpayoff canbeclassifiedbyα,theforcetosynchronizethechoices
is3c.Thus,γ >0⇔a+b+c>3cmeansthattheplayers ofthreeplayers,andγ,thepreferenceforcompetitionwith
expectedlyobtainhigherpayoffswhentheycompete.Inthis others.Inconclusion,thisstudyshowedtheoreticalanalyses
meaning,they,overtime,learntocompetemoreoftenand andexperimentalfindingsregardingthree-playerinteraction
to cyclically choose only two of m actions. On the other tothemaximum.
hand,inγ <0,theylearntoavoidcompetitionasmuchas
How to achieve convergence to the Nash equilibrium,
possible,leadingtothedispersionoftheiractionchoices.
called last-iterate convergence, in three-player games is
a topic of great interest. Recent literature shows that the
FTRL achieves convergence to the Nash equilibrium in
6 CONCLUSION
two-playerzero-sumgameswhenitoptimisticallyforesees
itsopponent’sfuturestrategy[Mertikopoulosetal.,2019,
Three-playergamesarestillmuchunexplored,comparedto
DaskalakisandPanageas,2019]orincorporatesmutations
two-playergames.Thisstudyfocusedonhowthree-player
intoitslearning[Abeetal.,2022,2023].Inaddition,mem-
interactions affect the games. Surprisingly, we fully ana-
ory, i.e., the ability to change one’s action depending on
lyzed the Nash equilibria, even though solving the Nash
the past games, is known to change game structures. For
equilibriaofthree-playergamesissufficientlydifficultin
example,thismemoryextendstheregionoftheNashequi-
general[ChenandDeng,2006,Daskalakisetal.,2009].We
librium[FujimotoandKaneko,2019,2021,UsuiandUeda,
alsofoundthattheNashequilibriaarevariousandcomplex
2021, Ueda, 2023], induces divergence [Fujimoto et al.,
Nashequilibriawhenthree-playerinteractions,i.e.,αandγ,
2023],andachievesconvergence[Fujimotoetal.,2024].It
conflict.Suchthree-playerinteractionsalsocomplicatethe
wouldbeinterestingfutureworkstoseehowtheselearning
learningdynamics:Theconservedquantitiesinaconstant-
algorithmsperforminthree-playergames.Thisstudywill
sumgamearenolongerrobustforsuchthree-playerinterac-
providebothatheoreticalandexperimentalbasisforsuch
tion.WefoundtheLyapunovfunctionV andprovedthatV
futureworks.
monotonicallyincreasesordecreasesdependingonα.We
8References UlfDieckmannandRichard Law. Thedynamical theory
of coevolution: a derivation from stochastic ecological
Kenshi Abe, Mitsuki Sakamoto, and Atsushi Iwasaki. processes. Journalofmathematicalbiology,34:579–612,
Mutation-driven follow the regularized leader for last- 1996.
iterateconvergenceinzero-sumgames. InUAI,pages
1–10,2022.
DanielFriedman.Evolutionarygamesineconomics.Econo-
metrica:JournaloftheEconometricSociety,pages637–
Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro 666,1991.
Toyoshima, and Atsushi Iwasaki. Last-iterate conver-
gencewithfullandnoisyfeedbackintwo-playerzero- DrewFudenbergandDavidKLevine. Thetheoryoflearn-
sumgames. InAISTATS,pages7999–8028,2023. ingingames,volume2. MITpress,1998.
EizoAkiyamaandKunihikoKaneko. Dynamicalsystems YumaFujimotoandKunihikoKaneko. Emergenceofex-
gametheoryanddynamicsofgames. PhysicaD:Nonlin- ploitation as symmetry breaking in iterated prisoner’s
earPhenomena,147(3-4):221–258,2000. dilemma. PhysicalReviewResearch,1(3):033077,2019.
IoannisAnagnostides,IoannisPanageas,GabrieleFarina, Yuma Fujimoto and Kunihiko Kaneko. Exploitation by
andTuomasSandholm. Onlast-iterateconvergencebe- asymmetry of information reference in coevolutionary
yondzero-sumgames. InICML,pages536–581,2022. learninginprisoner’sdilemmagame. JournalofPhysics:
Complexity,2(4):045007,2021.
JamesPBaileyandGeorgiosPiliouras.Multi-agentlearning
innetworkzero-sumgamesisahamiltoniansystem. In YumaFujimoto,KaitoAriu,andKenshiAbe. Learningin
AAMAS,pages233–241,2019. multi-memorygamestriggerscomplexdynamicsdiverg-
ingfromnashequilibrium. InIJCAI,2023.
Daan Bloembergen, Karl Tuyls, Daniel Hennes, and
MichaelKaisers. Evolutionarydynamicsofmulti-agent Yuma Fujimoto, Kaito Ariu, and Kenshi Abe. Memory
learning:Asurvey. JournalofArtificialIntelligenceRe- asymmetrycreatesheteroclinicorbitstonashequilibrium
search,53:659–697,2015. inlearninginzero-sumgames. InAAAI,2024.
Tilman Börgers and Rajiv Sarin. Learning through rein- AndreaGaunersdorferandJosefHofbauer. Fictitiousplay,
forcementandreplicatordynamics. JournalofEconomic shapley polygons, and the replicator equation. Games
Theory,77(1):1–14,1997. andEconomicBehavior,11(2):279–303,1995.
MichaelBowling. Convergenceproblemsofgeneral-sum
WilliamCGrant. Correlatedequilibriumandevolutionary
multiagentreinforcementlearning. InICML,pages89–
stabilityin3-playerrock-paper-scissors. Games,14(3):
94,2000.
45,2023.
GeorgeWBrown. Iterativesolutionofgamesbyfictitious
JosefHofbauerandKarlSigmund. Adaptivedynamicsand
play. Act.Anal.ProdAllocation,13(1):374,1951.
evolutionarystability. AppliedMathematicsLetters,3(4):
75–79,1990.
NoamBrownandTuomasSandholm. Superhumanaifor
multiplayerpoker. Science,365(6456):885–890,2019.
JosefHofbauer,KarlSigmund,etal. Evolutionarygames
Xi Chen and Xiaotie Deng. Settling the complexity of andpopulationdynamics. Cambridgeuniversitypress,
two-playernashequilibrium. InFOCS,volume6,pages 1998.
261–272,2006.
JamesSJordan. Threeproblemsinlearningmixed-strategy
ConstantinosDaskalakisandIoannisPanageas. Last-iterate nash equilibria. Games and Economic Behavior, 5(3):
convergence:Zero-sumgamesandconstrainedmin-max 368–386,1993.
optimization. InITCS,pages27:1–27:18,2019.
JunjieLi,SotetsuKoyamada,QiweiYe,GuoqingLiu,Chao
Constantinos Daskalakis and Christos H Papadimitriou. Wang,RuihanYang,LiZhao,TaoQin,Tie-YanLiu,and
Three-playergamesarehard. InElectroniccolloquium Hsiao-WuenHon. Suphx:Masteringmahjongwithdeep
oncomputationalcomplexity,volume139,pages81–87. reinforcementlearning.arXivpreprintarXiv:2003.13590,
Citeseer,2005. 2020.
ConstantinosDaskalakis,PaulWGoldberg,andChristosH Kevin A McCabe, Arijit Mukherji, and David E Runkle.
Papadimitriou. The complexity of computing a nash Anexperimentalstudyofinformationandmixed-strategy
equilibrium. CommunicationsoftheACM,52(2):89–97, play in the three-person matching-pennies game. Eco-
2009. nomicTheory,15:421–462,2000.
9RichardMealingandJonathanLShapiro. Convergenceof PeterDTaylorandLeoBJonker. Evolutionarystablestrate-
strategiesinsimpleco-adaptinggames. InFOGA,pages giesandgamedynamics. Mathematicalbiosciences,40
176–190,2015. (1-2):145–156,1978.
PanayotisMertikopoulosandWilliamHSandholm. Learn- KarlTuylsandAnnNowé. Evolutionarygametheoryand
ingingamesviareinforcementandregularization. Math- multi-agentreinforcementlearning. TheKnowledgeEn-
ematicsofOperationsResearch,41(4):1297–1324,2016. gineeringReview,20(1):63–90,2005.
PanayotisMertikopoulos,ChristosPapadimitriou,andGeor- KarlTuyls,PieterJan’THoen,andBramVanschoenwinkel.
giosPiliouras. Cyclesinadversarialregularizedlearning. Anevolutionarydynamicalanalysisofmulti-agentlearn-
InSODA,pages2703–2717,2018. ing in iterated games. In AAMAS, volume 12, pages
115–153.Springer,2006.
PanayotisMertikopoulos,BrunoLecouat,HoussamZenati,
Chuan-ShengFoo,VijayChandrasekhar,andGeorgios MasahikoUeda.Memory-twostrategiesformingsymmetric
Piliouras. Optimisticmirrordescentinsaddle-pointprob- mutual reinforcement learning equilibrium in repeated
lems:Goingtheextra(-gradient)mile. InICLR,2019. prisoners’ dilemma game. Applied Mathematics and
Computation,444:127819,2023.
YohsukeMuraseandSeungKiBaek. Sevenrulestoavoid
thetragedyofthecommons. Journaloftheoreticalbiol- YukiUsuiandMasahikoUeda. Symmetricequilibriumof
ogy,449:94–102,2018. multi-agentreinforcementlearninginrepeatedprisoner’s
dilemma. AppliedMathematicsandComputation,409:
John F Nash Jr. Equilibrium points in n-person games.
126370,2021.
ProceedingsoftheNationalAcademyofSciences,36(1):
48–49,1950. MartinZinkevich. Onlineconvexprogramming andgen-
eralized infinitesimal gradient ascent. In ICML, pages
Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max 928–936,2003.
Smith, Satya O-G, Jonathan K Kummerfeld, Joelle
Pineau,SatinderSingh,andAaronCCourville. No-press
diplomacy:Modelingmulti-agentgameplay. InNeurIPS,
volume32,2019.
GeorgiosPiliouras,CarlosNieto-Granda,HenrikIChris-
tensen, and JeffS Shamma. Persistent patterns: Multi-
agentlearningbeyondequilibriumandutility. InAAMAS,
pages181–188,2014.
YuzuruSato,EizoAkiyama,andJDoyneFarmer. Chaosin
learningasimpletwo-persongame. Proceedingsofthe
NationalAcademyofSciences,99(7):4748–4751,2002.
YuzuruSato,EizoAkiyama,andJamesPCrutchfield. Sta-
bilityanddiversityincollectiveadaptation. PhysicaD:
NonlinearPhenomena,210(1-2):21–57,2005.
ShaiShalev-ShwartzandYoramSinger. Convexrepeated
gamesandfenchelduality. InNeurIPS,volume19,2006.
JeffSShammaandGürdalArslan. Dynamicfictitiousplay,
dynamic gradient play, and distributed convergence to
nashequilibria.IEEETransactionsonAutomaticControl,
50(3):312–327,2005.
Satinder Singh, Michael J Kearns, and Yishay Mansour.
Nashconvergenceofgradientdynamicsingeneral-sum
games. InUAI,pages541–548,2000.
DuaneSzafron,RichardGGibson,andNathanRSturtevant.
Aparameterizedfamilyofequilibriumprofilesforthree-
playerkuhnpoker.InAAMAS,volume13,pages247–254,
2013.
10Appendix of: Nash Equilibrium and Learning Dynamics
in Three-player Matching m-Action Games
YumaFujimoto1,2,3 KaitoAriu3,4 KenshiAbe3,5
1SOKENDAI
2TheUniversityofTokyo
3CyberAgent
4KTH
5TheUniversityofElectro-Communications
A PROOFS
A.1 PROOFOFTHEOREM1
Proof. Fromthedefinition,theNashequilibria(x∗,y∗,z∗)shouldsatisfy,withsomeconstantsC ,C ,C ∈Rforall
X Y Z
i∈{1,··· ,m},theconditionsof(Eq.-X),(Eq.-Y),and(Eq.-Z);
(cid:40)
x∗ >0 ⇒f(y∗,z∗)=C
i i i X , (Eq.-X)
x∗ =0 ⇒f(y∗,z∗)≤C
i i i X
(cid:40)
y∗ >0 ⇒f(z∗,x∗)=C
i i i Y , (Eq.-Y)
y∗ =0 ⇒f(z∗,x∗)≤C
i i i Y
(cid:40)
z∗ >0 ⇒f(x∗,y∗)=C
i i i Z . (Eq.-Z)
z∗ =0 ⇒f(x∗,y∗)≤C
i i i Z
First,bythefollowinglemma(seeAppendix.A.2foritsproof),alltheplayerstakethesamestrategiesintheNashequilibria.
Lemma4(SymmetryamongplayersintheNashequilibria). ForanyNashequilibrium,x∗ =y∗ =z∗issatisfied.
ByLemma4,f(x∗,y∗)=f(y∗,z∗)=f(z∗,x∗)triviallyholds.Thus,wenewlydefineafunction
i i i i i i
f˜(x∗):=f(x∗,x∗)=(α−γ)x∗2+γx∗. (22)
i i i i i
TheNashequilibriumconditionsof(Eq.-X),(Eq.-Y),and(Eq.-Z)arethatthereisC =C =C =:C suchthat,with
X Y Z
someconstantC ∈Rforalli∈{1,··· ,m},
(cid:40)
x∗ >0 ⇒f˜(x∗)=C
i i . (23)
x∗ =0 ⇒f˜(x∗)≤C
i i
Thisconditionissolvedbythefollowinglemmas(seeAppendixA.3andA.4fortheirproofs).Combiningtheselemmas,
wehaveprovedThm.1
Lemma5(TheinteriorNashequilibria). Intheinteriorofthestrategyspaces,i.e,x,y,z ∈int(∆m−1),thesetofx∗is
givenby

α=γ =0 ⇒int(∆m−1)

0≥α,0≥γ ⇒N (m) (24)
U
α>0>γ
⇒N (m)∪N (m)
U DRLemma6(TheboundaryNashequilibria). Ontheboundaryofthestrategyspaces,i.e,x,y,z ∈∂∆m−1,thesetofx∗is
givenby

0α ≥= αγ ,= 0≥0
γ
⇒⇒∂ ∅∆m−1
α≥0,γ ≥0 ⇒N (m)∪(∪ Proj−1(N (m′))) . (25)
P m′∈A U
α
γ
>> 00 >> αγ ⇒ ⇒(N ∪P(m)∪( P∪
rm o′ j∈ −A 1((−
N)P (r moj ′− )1 ))(N
∪U
(( ∪m′)))
Proj−1(N (m′)))
m′∈A(+) U m′∈A DR
A.2 PROOFOFLEMMA4
Proof. Weprovex∗ =y∗ =z∗byacontradictionmethod.First,wederiveacontradictionfromx∗ ̸=y∗ ̸=z∗ ̸=x∗ ⇔
(x∗ ̸= y∗)∧(y∗ ̸= z∗)∧(z∗ ̸= x∗). Second, we derive a contradiction from x∗ = y∗ ̸= z∗, which is equivalent to
y∗ =z∗ ̸=x∗andz∗ =x∗ ̸=y∗bythecyclicsymmetryofthethreeplayers.Thus,wecanderivex∗ =y∗ =z∗.Before
suchacontradictionmethod,wemakesomepreparationsforit.
Preparationforcontradictionmethod: f(y∗,z∗)iswrittenas
i i
f(y∗,z∗)=k y∗z∗+k y∗+k z∗, (26)
i i 0 i i + i − i
wherewedefinedk :=α−γ,k :=a−c=(β+γ)/2,andk :=b−c=(−β+γ)/2.Here,rememberb<c<a
0 + −
andb<ϵ<a.Thus,k >0,k <0,andk >k >k hold.Inthefollowing,weprovethefollowingfourconditions
+ − + 0 −
andextendtheseconditions;
(x∗ ≥y∗ >z∗ >0)∨(x∗ >y∗ ≥z∗ >0)⇒(C >C )∧(C >C ), (A1)
i i i i i i Z Y X Y
x∗ ≥y∗ >z∗ =0⇒(C >C )∧(C >C ), (B1)
i i i Z Y X Y
y∗ ≥x∗ >z∗ =0⇒C >C , (B6)
i i i X Y
x∗ >y∗ =z∗ =0⇒C >C . (C1)
i i i Z X
Proofof(A1): Sincex∗ ̸=0,y∗ ̸=0,andz∗ ̸=0,theNashequilibriumconditionis
i i i

C =f(y∗,z∗)=k y∗z∗+k y∗+k z∗
 X i i 0 i i + i − i
C =f(z∗,x∗)=k z∗x∗+k z∗+k x∗ . (27)
Y i i 0 i i + i − i
C =f(x∗,y∗)=k x∗y∗+k x∗+k y∗
Z i i 0 i i + i − i
Then,C >C isbecause
Z Y
 k ≥0 ⇒C =k x∗ y∗ +k x∗ + k y∗ >k z∗x∗+k z∗+k x∗ =C
 0 Z 0 i
(cid:124) ≥(cid:123)
zi
(cid:122) i∗(cid:125)
+
(cid:124) >(cid:123)
z(cid:122)i
i∗(cid:125) ≥(cid:124)
k−
(cid:123) −(cid:122)
xi
(cid:125)
∗
i
0 i i + i − i Y
(28)
k ≤0 ⇒C =(k y∗+k ) x∗ + k y∗ >(k x∗+k )z∗+k x∗ =C
 0 Z
(cid:124)
0 i
(cid:123)(cid:122)
+ (cid:125)(cid:124)(cid:123)(cid:122)i
(cid:125)
(cid:124)− (cid:123)(cid:122)i
(cid:125)
0 i + i − i Y
≥(k0x∗ i+k+) >z i∗ ≥k−x∗
i
Here,weusedk y∗+k ≥k x∗+k ≥k +k >0fork ≤0.Ontheotherhand,C >C isbecause
0 i + 0 i + 0 + 0 X Y
 k ≥0 ⇒C =(k y∗+k )z∗+k y∗ >(k z∗+k )x∗+k z∗ =C
 0 X
(cid:124)
>0 (k0i
z
i∗(cid:123) +(cid:122)
k−−
)x∗
ii
(cid:125)
+
(cid:124) ≥(cid:123)
zi
(cid:122) i∗(cid:125)
0 i − i + i Y
(29)
k
0
≤0 ⇒C
X
= k
(cid:124)0
(cid:123)y (cid:122)i∗
(cid:125)
z i∗+k
+
(cid:124)y
(cid:123)i
(cid:122)∗ (cid:125)+ k
(cid:124)−
(cid:123)(cid:122)z i∗
(cid:125)
>k 0z i∗x∗
i
+k +z i∗+k −x∗
i
=C
Y
≥k0x∗
i
≥z i∗ >k−x∗
i
Here,(k y∗+k )z∗ >(k z∗+k )x∗isbecausek z∗+k ≤k y∗+k ≤k +k <0fork ≥0andz∗ <x∗.
0 i − i 0 i − i 0 i − 0 i − 0 − 0 i i
12Proofof(B1): Sincex∗ ̸=0,y∗ ̸=0,andz∗ =0,theNashequilibriumconditionis
i i i

C =f(y∗,z∗)=k y∗
 X i i + i
C =f(z∗,x∗)=k x∗ . (30)
Y i i − i
C ≥f(x∗,y∗)=k x∗y∗+k x∗+k y∗
Z i i 0 i i + i − i
Here,C >C triviallyholds,whileC >C isbecause
X Y Z Y
C ≥(k y∗+k )x∗+k y∗ >k x∗ =C . (31)
Z 0 i + i − i − i Y
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
>0 >x∗
i
Proofof(B6): TheNashequilibriumconditionisgivenbyEqs.(27),again,andC >C triviallyholds.
X Y
Proofof(C1): Sincex∗ ̸=0,y∗ =z∗ =0,theNashequilibriumconditionis
i i i

C =f(y∗,z∗)=0
 X i i
C ≥f(z∗,x∗)=k x∗ . (32)
Y i i − i
C ≥f(x∗,y∗)=k x∗
Z i i + i
Here,C >C triviallyholds.
Z X
Extensionofalltheobtainedconditions: TheseconditionsofEqs.(A1),(B1),(B6),and(C1)areextendedasfollows;
(x∗ ≥y∗ >z∗ >0)∨(x∗ >y∗ ≥z∗ >0)⇒(C >C )∧(C >C ), (A1)
i i i i i i Z Y X Y
(y∗ ≥z∗ >x∗ >0)∨(y∗ >z∗ ≥x∗ >0)⇒(C >C )∧(C >C ), (A2)
i i i i i i X Z Y Z
(z∗ ≥x∗ >y∗ >0)∨(z∗ >x∗ ≥y∗ >0)⇒(C >C )∧(C >C ), (A3)
i i i i i i Y X Z X
(z∗ ≥y∗ >x∗ >0)∨(z∗ >y∗ ≥x∗ >0)⇒(C >C )∧(C >C ), (A4)
i i i i i i Y Z Y X
(x∗ ≥z∗ >y∗ >0)∨(x∗ >z∗ ≥y∗ >0)⇒(C >C )∧(C >C ), (A5)
i i i i i i Z X Z Y
(y∗ ≥x∗ >z∗ >0)∨(y∗ >x∗ ≥z∗ >0)⇒(C >C )∧(C >C ), (A6)
i i i i i i X Y X Z
x∗ ≥y∗ >z∗ =0⇒(C >C )∧(C >C ), (B1)
i i i Z Y X Y
y∗ ≥z∗ >x∗ =0⇒(C >C )∧(C >C ), (B2)
i i i X Z Y Z
z∗ ≥x∗ >y∗ =0⇒(C >C )∧(C >C ), (B3)
i i i Y X Z X
z∗ ≥y∗ >x∗ =0⇒C >C , (B4)
i i i Y Z
x∗ ≥z∗ >y∗ =0⇒C >C , (B5)
i i i Z X
y∗ ≥x∗ >z∗ =0⇒C >C , (B6)
i i i X Y
x∗ >y∗ =z∗ =0⇒C >C , (C1)
i i i Z X
y∗ >z∗ =x∗ =0⇒C >C , (C2)
i i i X Y
z∗ >x∗ =y∗ =0⇒C >C . (C3)
i i i Y Z
These conditions are obtained by using symmetries. First, the condition of (A4) is obtained by reversing all the equal
signsinEqs.(28)and(29).Letσ denotethecyclicpermutationforalltheparametersofX,Y,andZ.Then,weobtain
c
alltheotherconditionsby(A2) = σ ((A1)),(A3) = σ2((A1)),(A5) = σ ((A4)),(A6) = σ2((A4)),(B2) = σ ((B1)),
c c c c c
(B3)=σ2((B1)),(B4)=σ ((B6)),(B5)=σ2((B6)),(C2)=σ ((C1)),and(C3)=σ2((C1)).
c c c c c
13Contradictionofx∗ ̸=y∗ ̸=z∗ ̸=x∗: Letusassumex∗ ̸=y∗ ̸=z∗ ̸=x∗ ⇔(x∗ ̸=y∗)∧(y∗ ̸=z∗)∧(z∗ ̸=x∗).If
so,therearei ,i ,i ∈{1,··· ,m}suchthat(x∗ >y∗)∧(y∗ >z∗)∧(z∗ >x∗ ).Here,weobtain
1 2 3 i1 i1 i2 i2 i3 i3
x∗ >y∗ ⇒(A1)∨(A3)∨(A5)∨(B1)∨(B3)∨(B5)∨(C1)
i1 i1
⇒(C >C )∨((C >C )∧(C >C ))
Z X Z Y X Y
⇒(C >C )∨(C >C ), (33)
Z X Z Y
y∗ >z∗ ⇒(A2)∨(A1)∨(A6)∨(B2)∨(B1)∨(B6)∨(C2)
i2 i2
⇒(C >C )∨((C >C )∧(C >C ))
X Y X Z Y Z
⇒(C >C )∨(C >C ), (34)
X Y X Z
z∗ >x∗ ⇒(A3)∨(A2)∨(A4)∨(B3)∨(B2)∨(B4)∨(C3)
i3 i3
⇒(C >C )∨((C >C )∧(C >C ))
Y Z Y X Z X
⇒(C >C )∨(C >C ). (35)
Y Z Y X
Here,theconditionsof(33),(34),and(35)cannotbesatisfiedsimultaneously.Thus,suchi ,i ,andi donotexist.Thisis
1 2 3
acontradiction,provingthatx∗ ̸=y∗ ̸=z∗ ̸=x∗cannothold.
Contradictionofx∗ = y∗ ̸= z∗: Second,weassumex∗ = y∗ ̸= z∗.Ifso,therearei ,i ∈ {1,··· ,m}suchthat
1 2
(x =y >z )∧(z >x =y ).Here,weobtain
i1 i1 i1 i2 i2 i2
x∗ =y∗ >z∗ ⇒((A1)∧(A6))∨((B1)∧(B6))
i1 i1 i1
⇒(C >C )∧(C >C )
Z Y X Y
⇒C >C , (36)
Z Y
z∗ >x∗ =y∗ ⇒((A3)∧(A4))∨(C3)
i2 i2 i2
⇒C >C . (37)
Y Z
Here,theconditions(36)and(37)cannotbesatisfiedsimultaneously.Thus,suchi andi donotexist.Thisisacontradiction,
1 2
provingthatx∗ =y∗ ̸=z∗cannothold.Finally,becauseweprovedthatneitherx∗ ̸=y∗ ̸=z∗ ̸=x∗norx∗ =y∗ ̸=z∗
hold,weprovex∗ =y∗ =z∗.
A.3 PROOFOFLEMMA5
Proof. Intheinteriorofthestrategyspace,theconditionforx∗is,withsomeC foralli∈{1,··· ,m},
f˜(x∗)=C. (38)
i
Classificationdependingonparameters: TheNashequilibriumconditiondependsonthefunctionoff˜.Fig.5classifies
thefunctionbyαandγ.Wedefinethecasesfrom1)to9)asfollows.
1).α>γ ≥0, 2).α>0>γ, 3).0≥α>γ, (39)
4).α=γ >0, 5).α=γ =0, 6).0>α=γ, (40)
7).γ >α≥0, 8).γ >0>α, 9).0≥γ >α. (41)
Case5). Inthiscase,f˜(x∗)=0alwaysholds.Thus,alltheinteriorpointsofthestrategyspacearetheNashequilibrium,
i
i.e.,x∗ ∈int(∆m−1).
Case1)3)4)6)7)9). Inthesecases,f˜(x∗) = C hasnomultiplesolutionsin0 < x∗ < 1/2.Thus,theinteriorNash
i i
equilibriumisonlyx∗ ∈{1/m}=:N (m)(U:Uniform-choiceequilibrium).
U
14Figure5:Allpossiblef˜(x∗)dependingonαandγ.Theorangelinesshowf˜(x∗)forthehorizontalaxisofx∗.Theblack
i i i
dotsarethevaluesinx∗ =0and1.Thegreendotsshowtheextremevalueinx :=γ/{2(γ−α)};0<x <1/2in
i ext ext
thecasesof2)and8),while1/2<x <1inthecasesof3)and7).
ext
Case2)8). Inthesecases,f˜(x∗)isparabolicandtakesitsextremevalueatx∗ =γ/{2(γ−α)}=:x ,whichisinthe
i i ext
regionof0<x <1/2.Thus,forsomeC,f˜(x∗)=C hasdoublerootsx∗ =x ,x ,describedas
ext i i + −
x =x +δ, x =x −δ, (42)
+ ext − ext
with 0 < δ < x . Here, a condition for a strategy which composes of k-pieces of x and (m−k)-pieces of x for
ext + −
k ∈{1,··· ,m−1},i.e.,
x :=(x ,··· ,x ,x ,··· ,x ) (43)
DR;k + + − −
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
k m−k
tobetheNashequilibriumis
kx +(m−k)x =1⇔mx −(m−2k)δ =1 (44)
+ − ext
mx −1
⇔δ = ext . (45)
m−2k
Since0<δ ≤x shouldbesatisfied,wesolve
ext
 (cid:18) (cid:19)
1 1
0< m mx e −xt 2− k1 <x
ext
⇔k <
2x 1ext
(cid:18)x
ext
>
m
1
(cid:19). (46)
k >
2x
x
ext
<
m
ext
15Inconclusion,theinteriorNashequilibriainclude
(cid:40)
{σ(x )|σ ∈S ,k =1,··· ,⌊1/(2x )⌋} (x >1/m)
x∗ ∈ DR;k m ext ext (47)
{σ(x )|σ ∈S ,k =⌈1/(2x )⌉,··· ,m−1} (x <1/m)
DR;k m ext ext
=:N (m), (48)
DR
(DR:DoubleRootsequilibria).Here,σ isapermutationfunctioninthem-dimensionalsymmetricgroupS .Sincethe
m
uniform-choiceequilibriumisalsoincluded,alltheinteriorNashequilibriaaregivenbyN (m)∪N (m).
U DR
A.4 PROOFOFLEMMA6
Proof. Intheinteriorofthestrategyspace,theconditionforx∗is,withsomeC foralli∈{1,··· ,m},
(cid:40)
x∗ >0 ⇒f˜(x∗)=C
i i . (49)
x∗ =0 ⇒f˜(x∗)≤C
i i
Thisisequivalentto,withsomeC ≥0(⇔f˜(0)≤C)forallisuchthatx∗ >0,
i
f˜(x∗)=C. (50)
i
Inthefollowing,weconsiderthisconditionundertheclassificationofαandγ (seeFig.5again).
Case5). Inthiscase,f˜(x∗)=0alwaysholds.Thus,alltheboundarypointsofthestrategyspacearetheNashequilibrium,
i
i.e.,x∗ ∈∂∆m−1.
Case 3) 6) 9). In these cases, f˜(x∗) < 0 = f˜(0) always holds for 0 < x∗ ≤ 1. Thus, there is no boundary Nash
i i
equilibrium.
Case 1) 4) 7). In these cases, f˜(x∗) ≥ 0 = f˜(0) is positive for all 0 ≤ x∗ ≤ 1. In other words, f˜(1) ≥ f˜(0) holds,
i i
meaningthattheboundaryNashequilibriainclude{e ,··· ,e }=:N (m)(P:Pure-strategyequilibria).Furthermore,
1 m P
for2≤m′ ≤m−1,f˜(1/m′)≥f˜(0)alsoholdssothattheboundaryNashequilibriaalsoincludeProj−1(N (m′))for
U
allm′ ∈{2,··· ,m−1}=:A.Here,Proj−1 showstheinverseprojectionfromm′ <m-tom-dimensionalspacewith
arbitrarypermutations.Thus,theNashequilibriaaregivenby
(cid:32) (cid:33)
(cid:91)
N (m)∪ Proj−1(N (m′)) . (51)
P U
m′∈A
case 2). In this case, f˜(x∗) ≥ 0 = f˜(0) holds for x ≤ x∗ ≤ 1. Thus, since f˜(1) ≥ f˜(0) holds, the Nash equi-
i ext i
libria include N (m). Furthermore, since f˜(1/m′) ≥ f˜(0) holds for 2 ≤ m′ ≤ 1/x , the Nash equilibria include
P ext
Proj−1(N (m′))forallm′ ∈{2,··· ,min(⌊1/x ⌋,m−1)}=:A(−).Tosummarize,theNashequilibriaaregivenby
U ext
 
(cid:91)
N P(m)∪ Proj−1(N U(m′)). (52)
m′∈A(−)
Case8). Inthiscase,f˜(x∗)≥0=f˜(0)holdsfor0<x∗ ≤x .Thus,theNashequilibriaincludeProj−1(N (m′))
i i ext DR
forallm′ ∈A.Furthermore,theNashequilibriaalsoincludeProj−1(N (m′))forallm′ ∈{⌈1/(2x )⌉,··· ,m−1}=:
U ext
A(+).Tosummarize,theNashequilibriaaregivenbyN (1/(2x )≤m′ ≤m−1)∪N (2≤m′ ≤m−1).
U ext DR
  (cid:32) (cid:33)
(cid:91) (cid:91)
 Proj−1(N U(m′))∪ Proj−1(N DR(m′)) (53)
m′∈A(+) m′∈A
16A.5 PROOFOFLEMMA3
Proof.Two-actiongamesgivex=(x ,x ),sothatx =x¯ holds.SimilarequationsholdforYandZ.First,Eq.(14)in
1 2 2 1
theFTRLalgorithmiscalculatedas
q (x†)=argmax{x†·x−h(x)}
1
x
=argmax{x†x +x†(1−x )−h (x )}
1 1 2 1 two 1
x1
=argmax{(x† −x†)x −h (x )} (54)
1 2 1 two 1
x1 (cid:124) (cid:123)(cid:122) (cid:125)
=:∆x†
Thus,q (x†)isdeterminedonlybythesinglevariableof∆x†,whichdynamicsare
1
∆x˙† =x˙† −x˙†
1 2
∂u ∂u
= −
∂x ∂x
1 2
=f(y ,z )−f(y ,z )
1 1 2 2
=f(y ,z )−f(1−y ,1−z )
1 1 1 1
=(α+β)y +(α−β)z −α=:f (y ,z ), (55)
1 1 two 1 1
independentofγ.
Wefurtherassumex∈int∆m−1,i.e.,0<x <1.Then,theextremeconditioninEq.(54)iscalculatedas
1
h′ (q (x†))=∆x† ⇔q (x†)=h′−1(∆x†). (56)
two 1 1 two
Bythisequation,weobtain
∂u ∂q ∂u ∂q
x˙ = 1 + 1
1 ∂x 1∂x†
1
∂x 2∂x†
2
∂u ∂u
= (h′−1)′(∆x†)− (h′−1)′(∆x†)
∂x two ∂x two
1 2
=(f(y ,z )−f(y ,z ))(h′−1)′(∆x†)
1 1 2 2 two
=f (y ,z )(h′−1)′(∆x†), (57)
two 1 1 two
Letuscalculate(h′−1)′(∆x†)as
two
h (x )=x logx +(1−x )log(1−x )
two 1 1 1 1 1
⇔h′ (x )=logx −log(1−x )(=∆x†)
two 1 1 1
exp(∆x†)
⇔h′−1(∆x†)= (=x )
two 1+exp(∆x†) 1
exp(∆x†) 1
⇔(h′−1)′(∆x†)= =x (1−x ), (58)
two 1+exp(∆x†)1+exp(∆x†) 1 1
fortheentropicregularizerh(x)=x·logxand
x2+(1−x )2
h (x )= 1 1
two 1 2
⇔h′ (x )=2x −1(=∆x†)
two 1 1
∆x†+1
⇔h′−1(∆x†)= (=x )
two 2 1
1
⇔(h′−1)′(∆x†)= , (59)
two 2
17fortheEuclideanregularizerh(x)=∥x∥2/2.Tosummarize,wedenote(h′−1)′(∆x†)by
two
(cid:40)
x (1−x ) (h(x)=x·logx)
w(x ):= 1 1 . (60)
1 1/2 (h(x)=∥x∥2/2)
A.6 PROOFOFTHEOREM2
Proof. Intwo-actiongames,weobtain
V(x,y,z)=x y z +(1−x )(1−y )(1−z ). (61)
1 1 1 1 1 1
WenowdefinethecyclicsumΣ as
cyc
Σ F(x,y,z):=F(x,y,z)+F(y,z,x)+F(z,x,y), (62)
cyc
forarbitraryfunctionF.UsingthisΣ ,V iscalculatedas
cyc
V˙ =Σ x˙ y z −x˙ (1−y )(1−z )
cyc 1 1 1 1 1 1
=Σ w(x ){α(y +z −1)+β(y −z )}(y +z −1)
cyc 1 1 1 1 1 1 1
=αΣ w(x )(y +z −1)2+βw(x )(y −z )(y +z −1)
cyc 1 1 1 1 1 1 1 1
=αΣ w(x )(y +z −1)2, (63)
cyc 1 1 1
Here,becausew(x )>0alwaysholds,sign(V˙)=sign(α)isproved.Inthelastlineofthisequation,weused
1
Σ w(x )(y −z )(y +z −1)
cyc 1 1 1 1 1
=Σ x (1−x )(y −z )(y +z −1)
cyc 1 1 1 1 1 1
=−Σ x (y −z )−Σ x2(y2−z2)+Σ (x y2−z x2)+Σ (x2y −z2x )
cyc 1 1 1 cyc 1 1 1 cyc 1 1 1 1 cyc 1 1 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0 =0 =0 =0
=0, (64)
fortheentropicregularizerw(x )=x (1−x )and
1 1 1
Σ w(x )(y −z )(y +z −1)
cyc 1 1 1 1 1
1
= Σ (y −z )(y +z −1)
2 cyc 1 1 1 1
1 1
= Σ (y2−z2)− Σ (y −z )
2 cyc 1 1 2 cyc 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0 =0
=0, (65)
fortheEuclideanregularizerw(x )=1/2.
1
A.7 PROOFOFTHEOREM3
Proof. First,wecanderive
∂u
G˙ =Σ Σ (x −x∗)
cyc i ∂x i i
i
=Σ Σ f(y ,z )(x −x∗)
cyc i i i i i
=Σ f (y ,z )(x −x∗)
cyc two 1 1 1 1
=Σ {α(y +z −1)+β(y −z )}(x −x∗)
cyc 1 1 1 1 1 1
=αΣ (y +z −1)(x −x∗)
cyc 1 1 1 1
(cid:26) (cid:27)
3
=2α (x y +y z +z x )−(x +y +z )+
1 1 1 1 1 1 1 1 1 4
=2αV. (66)
18Here,inthefifthequalsign,thetermofβ disappearsbecause
Σ (y −z )(x −x∗)
cyc 1 1 1 1
(cid:18) (cid:19)
1
=Σ (y −z ) x −
cyc 1 1 1 2
1
=Σ x (y −z )− Σ (y −z )
cyc 1 1 1 2 cyc 1 1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0 =0
=0. (67)
Thesixthequalsignholdsbecause
1
V =Σ x y z −
i i i i m2
1
=x y z +(1−x )(1−y )(1−z )−
1 1 1 1 1 1 4
3
=(x y +y z +z x )−(x +y +z )+ . (68)
1 1 1 1 1 1 1 1 1 4
A.8 PROOFOFTHEOREM4
Proof.
V˙ =Σ Σ x˙ y z
cyc i i i i
=Σ Σ x y z (f(y ,z )−Σ x f(y ,z ))
cyc i i i i i i i i i i
β
=αΣ Σ x y z (y z −Σ x y z )+ Σ Σ x y z {(y −z )−Σ x (y −z )}
cyc i i i i i i i i i i 2 cyc i i i i i i i i i i
=αΣ Σ x y z (y z −Σ x y z )
cyc i i i i i i i i i i
=αΣ {Σ x (y z )2−(Σ x y z )2}
cyc i i i i i i i i
=αΣ Var[y z ] . (69)
cyc i i xi
Here,Var[y z ] showsthevarianceofy z basedonthedistributionofx fori.Inthefourthequalsign,weused
i i xi i i i
Σ Σ x y z {(y −z )−Σ x (y −z )}
cyc i i i i i i i i i i
=Σ x y z {Σ (y −z )−Σ Σ x (y −z )}
i i i i cyc i i i cyc i i i
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0 =0
=0. (70)
Fromthedefinitionofvariance,Var[y z ] = 0holdsifandonlyify z takesaconstantvalueC foralli.Thus,the
i i xi i i X
conditionforΣ Var[y z ] =0is
cyc i i xi

y z =C
 i i X
z x =C , (71)
i i Y
x
y =C
i i Z
forsomeconstantsC ,C ,andC .InthesamewayastheproofofLem.4,theconditionisequivalenttox=y =z =
X Y Z
1/m,meaningtheuniform-choiceequilibrium.
19