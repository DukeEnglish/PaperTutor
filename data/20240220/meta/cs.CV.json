[
    {
        "title": "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter",
        "authors": "Junfei XiaoZheng XuAlan YuilleShen YanBoyu Wang",
        "links": "http://arxiv.org/abs/2402.10896v1",
        "entry_id": "http://arxiv.org/abs/2402.10896v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10896v1",
        "summary": "This paper demonstrates that a progressively aligned language model can\neffectively bridge frozen vision encoders and large language models (LLMs).\nWhile the fundamental architecture and pre-training methods of vision encoders\nand LLMs have been extensively studied, the architecture and training strategy\nof vision-language adapters vary significantly across recent works. Our\nresearch undertakes a thorough exploration of the state-of-the-art perceiver\nresampler architecture and builds a strong baseline. However, we observe that\nthe vision-language alignment with perceiver resampler exhibits slow\nconvergence and limited scalability with a lack of direct supervision. To\naddress this issue, we propose PaLM2-VAdapter, employing a progressively\naligned language model as the vision-language adapter. Compared to the strong\nbaseline with perceiver resampler, our method empirically shows faster\nconvergence, higher performance, and stronger scalability. Extensive\nexperiments across various Visual Question Answering (VQA) and captioning tasks\non both images and videos demonstrate that our model exhibits state-of-the-art\nvisual understanding and multi-modal reasoning capabilities. Notably, our\nmethod achieves these advancements with 30~70% fewer parameters than the\nstate-of-the-art large vision-language models, marking a significant efficiency\nimprovement.",
        "updated": "2024-02-16 18:54:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10896v1"
    },
    {
        "title": "Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning",
        "authors": "Chia-Ling TsaiHui-Yun SuShen-Feng SungWei-Yang LinYing-Ying SuTzu-Hsien YangMan-Lin Mai",
        "links": "http://arxiv.org/abs/2402.10894v1",
        "entry_id": "http://arxiv.org/abs/2402.10894v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10894v1",
        "summary": "Stroke is a common disabling neurological condition that affects about\none-quarter of the adult population over age 25; more than half of patients\nstill have poor outcomes, such as permanent functional dependence or even\ndeath, after the onset of acute stroke. The aim of this study is to investigate\nthe efficacy of diffusion-weighted MRI modalities combining with structured\nhealth profile on predicting the functional outcome to facilitate early\nintervention. A deep fusion learning network is proposed with two-stage\ntraining: the first stage focuses on cross-modality representation learning and\nthe second stage on classification. Supervised contrastive learning is\nexploited to learn discriminative features that separate the two classes of\npatients from embeddings of individual modalities and from the fused multimodal\nembedding. The network takes as the input DWI and ADC images, and structured\nhealth profile data. The outcome is the prediction of the patient needing\nlong-term care at 3 months after the onset of stroke. Trained and evaluated\nwith a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80\nand 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing\nmodels that consolidate both imaging and structured data in the medical domain.\nIf trained with comprehensive clinical variables, including NIHSS and\ncomorbidities, the gain from images on making accurate prediction is not\nconsidered substantial, but significant. However, diffusion-weighted MRI can\nreplace NIHSS to achieve comparable level of accuracy combining with other\nreadily available clinical variables for better generalization.",
        "updated": "2024-02-16 18:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10894v1"
    },
    {
        "title": "Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation",
        "authors": "Ziyang WangChao Ma",
        "links": "http://arxiv.org/abs/2402.10887v1",
        "entry_id": "http://arxiv.org/abs/2402.10887v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10887v1",
        "summary": "Medical image segmentation is increasingly reliant on deep learning\ntechniques, yet the promising performance often come with high annotation\ncosts. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised\nlearning (WSL) framework that leverages the capabilities of Convolutional\nNeural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual\nMamba (VMamba) architecture for medical image segmentation, especially when\ndealing with scribble-based annotations. The proposed WSL strategy incorporates\nthree distinct architecture but same symmetrical encoder-decoder networks: a\nCNN-based UNet for detailed local feature extraction, a Swin Transformer-based\nSwinUNet for comprehensive global context understanding, and a VMamba-based\nMamba-UNet for efficient long-range dependency modeling. The key concept of\nthis framework is a collaborative and cross-supervisory mechanism that employs\npseudo labels to facilitate iterative learning and refinement across the\nnetworks. The effectiveness of Weak-Mamba-UNet is validated on a publicly\navailable MRI cardiac segmentation dataset with processed scribble annotations,\nwhere it surpasses the performance of a similar WSL framework utilizing only\nUNet or SwinUNet. This highlights its potential in scenarios with sparse or\nimprecise annotations. The source code is made publicly accessible.",
        "updated": "2024-02-16 18:43:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10887v1"
    },
    {
        "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations",
        "authors": "Tsung-Wei KeNikolaos GkanatsiosKaterina Fragkiadaki",
        "links": "http://arxiv.org/abs/2402.10885v1",
        "entry_id": "http://arxiv.org/abs/2402.10885v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10885v1",
        "summary": "We marry diffusion policies and 3D scene representations for robot\nmanipulation. Diffusion policies learn the action distribution conditioned on\nthe robot and environment state using conditional diffusion models. They have\nrecently shown to outperform both deterministic and alternative\nstate-conditioned action distribution learning methods. 3D robot policies use\n3D scene feature representations aggregated from a single or multiple camera\nviews using sensed depth. They have shown to generalize better than their 2D\ncounterparts across camera viewpoints. We unify these two lines of work and\npresent 3D Diffuser Actor, a neural policy architecture that, given a language\ninstruction, builds a 3D representation of the visual scene and conditions on\nit to iteratively denoise 3D rotations and translations for the robot's\nend-effector. At each denoising iteration, our model represents end-effector\npose estimates as 3D scene tokens and predicts the 3D translation and rotation\nerror for each of them, by featurizing them using 3D relative attention to\nother 3D visual and language tokens. 3D Diffuser Actor sets a new\nstate-of-the-art on RLBench with an absolute performance gain of 16.3% over the\ncurrent SOTA on a multi-view setup and an absolute gain of 13.1% on a\nsingle-view setup. On the CALVIN benchmark, it outperforms the current SOTA in\nthe setting of zero-shot unseen scene generalization by being able to\nsuccessfully run 0.2 more tasks, a 7% relative increase. It also works in the\nreal world from a handful of demonstrations. We ablate our model's\narchitectural design choices, such as 3D scene featurization and 3D relative\nattentions, and show they all help generalization. Our results suggest that 3D\nscene representations and powerful generative modeling are keys to efficient\nrobot learning from demonstrations.",
        "updated": "2024-02-16 18:43:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10885v1"
    },
    {
        "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
        "authors": "Shengzhi LiRongyu LinShichao Pei",
        "links": "http://arxiv.org/abs/2402.10884v1",
        "entry_id": "http://arxiv.org/abs/2402.10884v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10884v1",
        "summary": "In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.",
        "updated": "2024-02-16 18:42:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10884v1"
    }
]