[
    {
        "title": "The Price of Adaptivity in Stochastic Convex Optimization",
        "authors": "Yair CarmonOliver Hinder",
        "links": "http://arxiv.org/abs/2402.10898v1",
        "entry_id": "http://arxiv.org/abs/2402.10898v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10898v1",
        "summary": "We prove impossibility results for adaptivity in non-smooth stochastic convex\noptimization. Given a set of problem parameters we wish to adapt to, we define\na \"price of adaptivity\" (PoA) that, roughly speaking, measures the\nmultiplicative increase in suboptimality due to uncertainty in these\nparameters. When the initial distance to the optimum is unknown but a gradient\nnorm bound is known, we show that the PoA is at least logarithmic for expected\nsuboptimality, and double-logarithmic for median suboptimality. When there is\nuncertainty in both distance and gradient norm, we show that the PoA must be\npolynomial in the level of uncertainty. Our lower bounds nearly match existing\nupper bounds, and establish that there is no parameter-free lunch.",
        "updated": "2024-02-16 18:56:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10898v1"
    },
    {
        "title": "Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning",
        "authors": "Chia-Ling TsaiHui-Yun SuShen-Feng SungWei-Yang LinYing-Ying SuTzu-Hsien YangMan-Lin Mai",
        "links": "http://arxiv.org/abs/2402.10894v1",
        "entry_id": "http://arxiv.org/abs/2402.10894v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10894v1",
        "summary": "Stroke is a common disabling neurological condition that affects about\none-quarter of the adult population over age 25; more than half of patients\nstill have poor outcomes, such as permanent functional dependence or even\ndeath, after the onset of acute stroke. The aim of this study is to investigate\nthe efficacy of diffusion-weighted MRI modalities combining with structured\nhealth profile on predicting the functional outcome to facilitate early\nintervention. A deep fusion learning network is proposed with two-stage\ntraining: the first stage focuses on cross-modality representation learning and\nthe second stage on classification. Supervised contrastive learning is\nexploited to learn discriminative features that separate the two classes of\npatients from embeddings of individual modalities and from the fused multimodal\nembedding. The network takes as the input DWI and ADC images, and structured\nhealth profile data. The outcome is the prediction of the patient needing\nlong-term care at 3 months after the onset of stroke. Trained and evaluated\nwith a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80\nand 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing\nmodels that consolidate both imaging and structured data in the medical domain.\nIf trained with comprehensive clinical variables, including NIHSS and\ncomorbidities, the gain from images on making accurate prediction is not\nconsidered substantial, but significant. However, diffusion-weighted MRI can\nreplace NIHSS to achieve comparable level of accuracy combining with other\nreadily available clinical variables for better generalization.",
        "updated": "2024-02-16 18:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10894v1"
    },
    {
        "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
        "authors": "Moritz StephanAlexander KhazatskyEric MitchellAnnie S ChenSheryl HsuArchit SharmaChelsea Finn",
        "links": "http://arxiv.org/abs/2402.10893v1",
        "entry_id": "http://arxiv.org/abs/2402.10893v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10893v1",
        "summary": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.",
        "updated": "2024-02-16 18:50:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10893v1"
    },
    {
        "title": "Proving membership in LLM pretraining data via data watermarks",
        "authors": "Johnny Tian-Zheng WeiRyan Yixiang WangRobin Jia",
        "links": "http://arxiv.org/abs/2402.10892v1",
        "entry_id": "http://arxiv.org/abs/2402.10892v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10892v1",
        "summary": "Detecting whether copyright holders' works were used in LLM pretraining is\npoised to be an important problem. This work proposes using data watermarks to\nenable principled detection with only black-box model access, provided that the\nrightholder contributed multiple training documents and watermarked them before\npublic release. By applying a randomly sampled data watermark, detection can be\nframed as hypothesis testing, which provides guarantees on the false detection\nrate. We study two watermarks: one that inserts random sequences, and another\nthat randomly substitutes characters with Unicode lookalikes. We first show how\nthree aspects of watermark design -- watermark length, number of duplications,\nand interference -- affect the power of the hypothesis test. Next, we study how\na watermark's detection strength changes under model and dataset scaling: while\nincreasing the dataset size decreases the strength of the watermark, watermarks\nremain strong if the model size also increases. Finally, we view SHA hashes as\nnatural watermarks and show that we can robustly detect hashes from\nBLOOM-176B's training data, as long as they occurred at least 90 times.\nTogether, our results point towards a promising future for data watermarks in\nreal world use.",
        "updated": "2024-02-16 18:49:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10892v1"
    },
    {
        "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
        "authors": "Dylan ZhangJustin WangFrancois Charton",
        "links": "http://arxiv.org/abs/2402.10891v1",
        "entry_id": "http://arxiv.org/abs/2402.10891v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10891v1",
        "summary": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.",
        "updated": "2024-02-16 18:47:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10891v1"
    }
]