[
    {
        "title": "Explainability for Machine Learning Models: From Data Adaptability to User Perception",
        "authors": "julien Delaunay",
        "links": "http://arxiv.org/abs/2402.10888v1",
        "entry_id": "http://arxiv.org/abs/2402.10888v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10888v1",
        "summary": "This thesis explores the generation of local explanations for already\ndeployed machine learning models, aiming to identify optimal conditions for\nproducing meaningful explanations considering both data and user requirements.\nThe primary goal is to develop methods for generating explanations for any\nmodel while ensuring that these explanations remain faithful to the underlying\nmodel and comprehensible to the users.\n  The thesis is divided into two parts. The first enhances a widely used\nrule-based explanation method. It then introduces a novel approach for\nevaluating the suitability of linear explanations to approximate a model.\nAdditionally, it conducts a comparative experiment between two families of\ncounterfactual explanation methods to analyze the advantages of one over the\nother. The second part focuses on user experiments to assess the impact of\nthree explanation methods and two distinct representations. These experiments\nmeasure how users perceive their interaction with the model in terms of\nunderstanding and trust, depending on the explanations and representations.\nThis research contributes to a better explanation generation, with potential\nimplications for enhancing the transparency, trustworthiness, and usability of\ndeployed AI systems.",
        "updated": "2024-02-16 18:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10888v1"
    },
    {
        "title": "Insights into mobile health application market via a content analysis of marketplace data with machine learning",
        "authors": "Gokhan AydinGokhan Silahtaroglu",
        "links": "http://dx.doi.org/10.1371/journal.pone.0244302",
        "entry_id": "http://arxiv.org/abs/2402.10789v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10789v1",
        "summary": "Background Despite the benefits offered by an abundance of health\napplications promoted on app marketplaces (e.g., Google Play Store), the wide\nadoption of mobile health and e-health apps is yet to come. Objective This\nstudy aims to investigate the current landscape of smartphone apps that focus\non improving and sustaining health and wellbeing. Understanding the categories\nthat popular apps focus on and the relevant features provided to users, which\nlead to higher user scores and downloads will offer insights to enable higher\nadoption in the general populace. This study on 1,000 mobile health\napplications aims to shed light on the reasons why particular apps are liked\nand adopted while many are not. Methods User-generated data (i.e. review\nscores) and company-generated data (i.e. app descriptions) were collected from\napp marketplaces and manually coded and categorized by two researchers. For\nanalysis, Artificial Neural Networks, Random Forest and Na\\\"ive Bayes\nArtificial Intelligence algorithms were used. Results The analysis led to\nfeatures that attracted more download behavior and higher user scores. The\nfindings suggest that apps that mention a privacy policy or provide videos in\ndescription lead to higher user scores, whereas free apps with in-app purchase\npossibilities, social networking and sharing features and feedback mechanisms\nlead to higher number of downloads. Moreover, differences in user scores and\nthe total number of downloads are detected in distinct subcategories of mobile\nhealth apps. Conclusion This study contributes to the current knowledge of\nm-health application use by reviewing mobile health applications using content\nanalysis and machine learning algorithms. The content analysis adds significant\nvalue by providing classification, keywords and factors that influence download\nbehavior and user scores in a m-health context.",
        "updated": "2024-02-16 16:12:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10789v1"
    },
    {
        "title": "A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers",
        "authors": "Paola BusiaMatteo Antonio ScrugliVictor Jean-Baptiste JungLuca BeniniPaolo Meloni",
        "links": "http://arxiv.org/abs/2402.10748v1",
        "entry_id": "http://arxiv.org/abs/2402.10748v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10748v1",
        "summary": "Wearable systems for the long-term monitoring of cardiovascular diseases are\nbecoming widespread and valuable assets in diagnosis and therapy. A promising\napproach for real-time analysis of the electrocardiographic (ECG) signal and\nthe detection of heart conditions, such as arrhythmia, is represented by the\ntransformer machine learning model. Transformers are powerful models for the\nclassification of time series, although efficient implementation in the\nwearable domain raises significant design challenges, to combine adequate\naccuracy and a suitable complexity. In this work, we present a tiny transformer\nmodel for the analysis of the ECG signal, requiring only 6k parameters and\nreaching 98.97% accuracy in the recognition of the 5 most common arrhythmia\nclasses from the MIT-BIH Arrhythmia database, assessed considering 8-bit\ninteger inference as required for efficient execution on low-power\nmicrocontroller-based devices. We explored an augmentation-based training\napproach for improving the robustness against electrode motion artifacts noise,\nresulting in a worst-case post-deployment performance assessment of 98.36%\naccuracy. Suitability for wearable monitoring solutions is finally demonstrated\nthrough efficient deployment on the parallel ultra-low-power GAP9 processor,\nwhere inference execution requires 4.28ms and 0.09mJ.",
        "updated": "2024-02-16 15:14:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10748v1"
    },
    {
        "title": "Generative AI and Attentive User Interfaces: Five Strategies to Enhance Take-Over Quality in Automated Driving",
        "authors": "Patrick Ebel",
        "links": "http://arxiv.org/abs/2402.10664v1",
        "entry_id": "http://arxiv.org/abs/2402.10664v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10664v1",
        "summary": "As the automotive world moves toward higher levels of driving automation,\nLevel 3 automated driving represents a critical juncture. In Level 3 driving,\nvehicles can drive alone under limited conditions, but drivers are expected to\nbe ready to take over when the system requests. Assisting the driver to\nmaintain an appropriate level of Situation Awareness (SA) in such contexts\nbecomes a critical task. This position paper explores the potential of\nAttentive User Interfaces (AUIs) powered by generative Artificial Intelligence\n(AI) to address this need. Rather than relying on overt notifications, we argue\nthat AUIs based on novel AI technologies such as large language models or\ndiffusion models can be used to improve SA in an unconscious and subtle way\nwithout negative effects on drivers overall workload. Accordingly, we propose 5\nstrategies how generative AI s can be used to improve the quality of takeovers\nand, ultimately, road safety.",
        "updated": "2024-02-16 13:13:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10664v1"
    },
    {
        "title": "Properties and Challenges of LLM-Generated Explanations",
        "authors": "Jenny KunzMarco Kuhlmann",
        "links": "http://arxiv.org/abs/2402.10532v1",
        "entry_id": "http://arxiv.org/abs/2402.10532v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10532v1",
        "summary": "The self-rationalising capabilities of large language models (LLMs) have been\nexplored in restricted settings, using task/specific data sets. However,\ncurrent LLMs do not (only) rely on specifically annotated data; nonetheless,\nthey frequently explain their outputs. The properties of the generated\nexplanations are influenced by the pre-training corpus and by the target data\nused for instruction fine-tuning. As the pre-training corpus includes a large\namount of human-written explanations \"in the wild\", we hypothesise that LLMs\nadopt common properties of human explanations. By analysing the outputs for a\nmulti-domain instruction fine-tuning data set, we find that generated\nexplanations show selectivity and contain illustrative elements, but less\nfrequently are subjective or misleading. We discuss reasons and consequences of\nthe properties' presence or absence. In particular, we outline positive and\nnegative implications depending on the goals and user groups of the\nself-rationalising system.",
        "updated": "2024-02-16 09:37:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10532v1"
    }
]