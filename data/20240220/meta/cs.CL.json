[
    {
        "title": "Proving membership in LLM pretraining data via data watermarks",
        "authors": "Johnny Tian-Zheng WeiRyan Yixiang WangRobin Jia",
        "links": "http://arxiv.org/abs/2402.10892v1",
        "entry_id": "http://arxiv.org/abs/2402.10892v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10892v1",
        "summary": "Detecting whether copyright holders' works were used in LLM pretraining is\npoised to be an important problem. This work proposes using data watermarks to\nenable principled detection with only black-box model access, provided that the\nrightholder contributed multiple training documents and watermarked them before\npublic release. By applying a randomly sampled data watermark, detection can be\nframed as hypothesis testing, which provides guarantees on the false detection\nrate. We study two watermarks: one that inserts random sequences, and another\nthat randomly substitutes characters with Unicode lookalikes. We first show how\nthree aspects of watermark design -- watermark length, number of duplications,\nand interference -- affect the power of the hypothesis test. Next, we study how\na watermark's detection strength changes under model and dataset scaling: while\nincreasing the dataset size decreases the strength of the watermark, watermarks\nremain strong if the model size also increases. Finally, we view SHA hashes as\nnatural watermarks and show that we can robustly detect hashes from\nBLOOM-176B's training data, as long as they occurred at least 90 times.\nTogether, our results point towards a promising future for data watermarks in\nreal world use.",
        "updated": "2024-02-16 18:49:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10892v1"
    },
    {
        "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
        "authors": "Dylan ZhangJustin WangFrancois Charton",
        "links": "http://arxiv.org/abs/2402.10891v1",
        "entry_id": "http://arxiv.org/abs/2402.10891v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10891v1",
        "summary": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.",
        "updated": "2024-02-16 18:47:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10891v1"
    },
    {
        "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
        "authors": "Ziru ChenMichael WhiteRaymond MooneyAli PayaniYu SuHuan Sun",
        "links": "http://arxiv.org/abs/2402.10890v1",
        "entry_id": "http://arxiv.org/abs/2402.10890v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10890v1",
        "summary": "In this paper, we examine how large language models (LLMs) solve multi-step\nproblems under a language agent framework with three components: a generator, a\ndiscriminator, and a planning method. We investigate the practical utility of\ntwo advanced planning methods, iterative correction and tree search. We present\na comprehensive analysis of how discrimination accuracy affects the overall\nperformance of agents when using these two methods or a simpler method,\nre-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical\nreasoning, show that: (1) advanced planning methods demand discriminators with\nat least 90% accuracy to achieve significant improvements over re-ranking; (2)\ncurrent LLMs' discrimination abilities have not met the needs of advanced\nplanning methods to achieve such improvements; (3) with LLM-based\ndiscriminators, advanced planning methods may not adequately balance accuracy\nand efficiency. For example, compared to the other two methods, tree search is\nat least 10--20 times slower but leads to negligible performance gains, which\nhinders its real-world applications. Code and data will be released at\nhttps://github.com/OSU-NLP-Group/llm-planning-eval.",
        "updated": "2024-02-16 18:45:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10890v1"
    },
    {
        "title": "Reviewer2: Optimizing Review Generation Through Prompt Generation",
        "authors": "Zhaolin GaoKianté BrantleyThorsten Joachims",
        "links": "http://arxiv.org/abs/2402.10886v1",
        "entry_id": "http://arxiv.org/abs/2402.10886v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10886v1",
        "summary": "Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research.",
        "updated": "2024-02-16 18:43:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10886v1"
    },
    {
        "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
        "authors": "Shengzhi LiRongyu LinShichao Pei",
        "links": "http://arxiv.org/abs/2402.10884v1",
        "entry_id": "http://arxiv.org/abs/2402.10884v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10884v1",
        "summary": "In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.",
        "updated": "2024-02-16 18:42:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10884v1"
    }
]