[
    {
        "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
        "authors": "Moritz StephanAlexander KhazatskyEric MitchellAnnie S ChenSheryl HsuArchit SharmaChelsea Finn",
        "links": "http://arxiv.org/abs/2402.10893v1",
        "entry_id": "http://arxiv.org/abs/2402.10893v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10893v1",
        "summary": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.",
        "updated": "2024-02-16 18:50:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10893v1"
    },
    {
        "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
        "authors": "Dylan ZhangJustin WangFrancois Charton",
        "links": "http://arxiv.org/abs/2402.10891v1",
        "entry_id": "http://arxiv.org/abs/2402.10891v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10891v1",
        "summary": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.",
        "updated": "2024-02-16 18:47:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10891v1"
    },
    {
        "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
        "authors": "Ziru ChenMichael WhiteRaymond MooneyAli PayaniYu SuHuan Sun",
        "links": "http://arxiv.org/abs/2402.10890v1",
        "entry_id": "http://arxiv.org/abs/2402.10890v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10890v1",
        "summary": "In this paper, we examine how large language models (LLMs) solve multi-step\nproblems under a language agent framework with three components: a generator, a\ndiscriminator, and a planning method. We investigate the practical utility of\ntwo advanced planning methods, iterative correction and tree search. We present\na comprehensive analysis of how discrimination accuracy affects the overall\nperformance of agents when using these two methods or a simpler method,\nre-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical\nreasoning, show that: (1) advanced planning methods demand discriminators with\nat least 90% accuracy to achieve significant improvements over re-ranking; (2)\ncurrent LLMs' discrimination abilities have not met the needs of advanced\nplanning methods to achieve such improvements; (3) with LLM-based\ndiscriminators, advanced planning methods may not adequately balance accuracy\nand efficiency. For example, compared to the other two methods, tree search is\nat least 10--20 times slower but leads to negligible performance gains, which\nhinders its real-world applications. Code and data will be released at\nhttps://github.com/OSU-NLP-Group/llm-planning-eval.",
        "updated": "2024-02-16 18:45:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10890v1"
    },
    {
        "title": "Explainability for Machine Learning Models: From Data Adaptability to User Perception",
        "authors": "julien Delaunay",
        "links": "http://arxiv.org/abs/2402.10888v1",
        "entry_id": "http://arxiv.org/abs/2402.10888v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10888v1",
        "summary": "This thesis explores the generation of local explanations for already\ndeployed machine learning models, aiming to identify optimal conditions for\nproducing meaningful explanations considering both data and user requirements.\nThe primary goal is to develop methods for generating explanations for any\nmodel while ensuring that these explanations remain faithful to the underlying\nmodel and comprehensible to the users.\n  The thesis is divided into two parts. The first enhances a widely used\nrule-based explanation method. It then introduces a novel approach for\nevaluating the suitability of linear explanations to approximate a model.\nAdditionally, it conducts a comparative experiment between two families of\ncounterfactual explanation methods to analyze the advantages of one over the\nother. The second part focuses on user experiments to assess the impact of\nthree explanation methods and two distinct representations. These experiments\nmeasure how users perceive their interaction with the model in terms of\nunderstanding and trust, depending on the explanations and representations.\nThis research contributes to a better explanation generation, with potential\nimplications for enhancing the transparency, trustworthiness, and usability of\ndeployed AI systems.",
        "updated": "2024-02-16 18:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10888v1"
    },
    {
        "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations",
        "authors": "Tsung-Wei KeNikolaos GkanatsiosKaterina Fragkiadaki",
        "links": "http://arxiv.org/abs/2402.10885v1",
        "entry_id": "http://arxiv.org/abs/2402.10885v1",
        "pdf_url": "http://arxiv.org/pdf/2402.10885v1",
        "summary": "We marry diffusion policies and 3D scene representations for robot\nmanipulation. Diffusion policies learn the action distribution conditioned on\nthe robot and environment state using conditional diffusion models. They have\nrecently shown to outperform both deterministic and alternative\nstate-conditioned action distribution learning methods. 3D robot policies use\n3D scene feature representations aggregated from a single or multiple camera\nviews using sensed depth. They have shown to generalize better than their 2D\ncounterparts across camera viewpoints. We unify these two lines of work and\npresent 3D Diffuser Actor, a neural policy architecture that, given a language\ninstruction, builds a 3D representation of the visual scene and conditions on\nit to iteratively denoise 3D rotations and translations for the robot's\nend-effector. At each denoising iteration, our model represents end-effector\npose estimates as 3D scene tokens and predicts the 3D translation and rotation\nerror for each of them, by featurizing them using 3D relative attention to\nother 3D visual and language tokens. 3D Diffuser Actor sets a new\nstate-of-the-art on RLBench with an absolute performance gain of 16.3% over the\ncurrent SOTA on a multi-view setup and an absolute gain of 13.1% on a\nsingle-view setup. On the CALVIN benchmark, it outperforms the current SOTA in\nthe setting of zero-shot unseen scene generalization by being able to\nsuccessfully run 0.2 more tasks, a 7% relative increase. It also works in the\nreal world from a handful of demonstrations. We ablate our model's\narchitectural design choices, such as 3D scene featurization and 3D relative\nattentions, and show they all help generalization. Our results suggest that 3D\nscene representations and powerful generative modeling are keys to efficient\nrobot learning from demonstrations.",
        "updated": "2024-02-16 18:43:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.10885v1"
    }
]