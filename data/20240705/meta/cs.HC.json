[
    {
        "title": "Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent",
        "authors": "Nikhil HulleStéphane Aroca-OuelletteAnthony J. RiesJake BrawerKatharina von der WenseAlessandro Roncone",
        "links": "http://arxiv.org/abs/2407.03298v1",
        "entry_id": "http://arxiv.org/abs/2407.03298v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03298v1",
        "summary": "Effective collaboration between humans and AIs hinges on transparent\ncommunication and alignment of mental models. However, explicit, verbal\ncommunication is not always feasible. Under such circumstances, human-human\nteams often depend on implicit, nonverbal cues to glean important information\nabout their teammates such as intent and expertise, thereby bolstering team\nalignment and adaptability. Among these implicit cues, two of the most salient\nand fundamental are a human's actions in the environment and their visual\nattention. In this paper, we present a novel method to combine eye gaze data\nand behavioral data, and evaluate their respective predictive power for human\nproficiency, trust, and intent. We first collect a dataset of paired eye gaze\nand gameplay data in the fast-paced collaborative \"Overcooked\" environment. We\nthen train models on this dataset to compare how the predictive powers differ\nbetween gaze data, gameplay data, and their combination. We additionally\ncompare our method to prior works that aggregate eye gaze data and demonstrate\nhow these aggregation methods can substantially reduce the predictive ability\nof eye gaze. Our results indicate that, while eye gaze data and gameplay data\nexcel in different situations, a model that integrates both types consistently\noutperforms all baselines. This work paves the way for developing intuitive and\nresponsive agents that can efficiently adapt to new teammates.",
        "updated": "2024-07-03 17:37:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03298v1"
    },
    {
        "title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation",
        "authors": "Yuan SunNavid Salami PargooTaqiya EhsanZhao Zhang Jorge Ortiz",
        "links": "http://arxiv.org/abs/2407.03291v1",
        "entry_id": "http://arxiv.org/abs/2407.03291v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03291v1",
        "summary": "Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.",
        "updated": "2024-07-03 17:24:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03291v1"
    },
    {
        "title": "EDPNet: An Efficient Dual Prototype Network for Motor Imagery EEG Decoding",
        "authors": "Can HanChen LiuCrystal CaiJun WangDahong Qian",
        "links": "http://arxiv.org/abs/2407.03177v1",
        "entry_id": "http://arxiv.org/abs/2407.03177v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03177v1",
        "summary": "Motor imagery electroencephalograph (MI-EEG) decoding plays a crucial role in\ndeveloping motor imagery brain-computer interfaces (MI-BCIs). However, decoding\nintentions from MI remains challenging due to the inherent complexity of EEG\nsignals relative to the small-sample size. In this paper, we propose an\nEfficient Dual Prototype Network (EDPNet) to enable accurate and fast MI\ndecoding. EDPNet employs a lightweight adaptive spatial-spectral fusion module,\nwhich promotes more efficient information fusion between multiple EEG\nelectrodes. Subsequently, a parameter-free multi-scale variance pooling module\nextracts more comprehensive temporal features. Furthermore, we introduce dual\nprototypical learning to optimize the feature space distribution and training\nprocess, thereby improving the model's generalization ability on small-sample\nMI datasets. Our experimental results show that the EDPNet outperforms\nstate-of-the-art models with superior classification accuracy and kappa values\n(84.11% and 0.7881 for dataset BCI competition IV 2a, 86.65% and 0.7330 for\ndataset BCI competition IV 2b). Additionally, we use the BCI competition III\nIVa dataset with fewer training data to further validate the generalization\nability of the proposed EDPNet. We also achieve superior performance with\n82.03% classification accuracy. Benefiting from the lightweight parameters and\nsuperior decoding accuracy, our EDPNet shows great potential for MI-BCI\napplications. The code is publicly available at\nhttps://github.com/hancan16/EDPNet.",
        "updated": "2024-07-03 14:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03177v1"
    },
    {
        "title": "GMM-ResNext: Combining Generative and Discriminative Models for Speaker Verification",
        "authors": "Hui YanZhenchun LeiChanghong LiuYong Zhou",
        "links": "http://dx.doi.org/10.1109/ICASSP48485.2024.10447141",
        "entry_id": "http://arxiv.org/abs/2407.03135v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03135v1",
        "summary": "With the development of deep learning, many different network architectures\nhave been explored in speaker verification. However, most network architectures\nrely on a single deep learning architecture, and hybrid networks combining\ndifferent architectures have been little studied in ASV tasks. In this paper,\nwe propose the GMM-ResNext model for speaker verification. Conventional GMM\ndoes not consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neighboring speech\nframes. So, we extract the log Gaussian probability features based on the raw\nacoustic features and use ResNext-based network as the backbone to extract the\nspeaker embedding. GMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and allows one to\nmore easily specify meaningful priors on model parameters. A two-path\nGMM-ResNext model based on two gender-related GMMs has also been proposed. The\nExperimental results show that the proposed GMM-ResNext achieves relative\nimprovements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN\non VoxCeleb1-O test set.",
        "updated": "2024-07-03 14:14:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03135v1"
    },
    {
        "title": "Design of a UE5-based digital twin platform",
        "authors": "Shaoqiu LyuMuzhi WangSunrui ZhangShengzhi Wang",
        "links": "http://arxiv.org/abs/2407.03107v1",
        "entry_id": "http://arxiv.org/abs/2407.03107v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03107v1",
        "summary": "Aiming at the current mainstream 3D scene engine learning and building cost\nis too high, this thesis proposes a digital twin platform design program based\non Unreal Engine 5 (UE5). It aims to provide a universal platform construction\ndesign process to effectively reduce the learning cost of large-scale scene\nconstruction. Taking an actual project of a unit as an example, the overall\ncycle work of platform building is explained, and the digital twin and data\nvisualization technologies and applications based on UE5 are analyzed. By\nsummarizing the project implementation into a process approach, the\nstandardization and operability of the process pathway is improved.",
        "updated": "2024-07-03 13:46:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03107v1"
    }
]