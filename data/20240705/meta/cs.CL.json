[
    {
        "title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
        "authors": "Max ZuoFrancisco Piedrahita VelezXiaochen LiMichael L. LittmanStephen H. Bach",
        "links": "http://arxiv.org/abs/2407.03321v1",
        "entry_id": "http://arxiv.org/abs/2407.03321v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03321v1",
        "summary": "Many recent works have explored using language models for planning problems.\nOne line of research focuses on translating natural language descriptions of\nplanning tasks into structured planning languages, such as the planning domain\ndefinition language (PDDL). While this approach is promising, accurately\nmeasuring the quality of generated PDDL code continues to pose significant\nchallenges. First, generated PDDL code is typically evaluated using planning\nvalidators that check whether the problem can be solved with a planner. This\nmethod is insufficient because a language model might generate valid PDDL code\nthat does not align with the natural language description of the task. Second,\nexisting evaluation sets often have natural language descriptions of the\nplanning task that closely resemble the ground truth PDDL, reducing the\nchallenge of the task. To bridge this gap, we introduce \\benchmarkName, a\nbenchmark designed to evaluate language models' ability to generate PDDL code\nfrom natural language descriptions of planning tasks. We begin by creating a\nPDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\ncode generated by language models by flexibly comparing it against a ground\ntruth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across\n13 different tasks, with varying levels of difficulty. Finally, we evaluate\nseveral API-access and open-weight language models that reveal this task's\ncomplexity. For example, $87.6\\%$ of the PDDL problem descriptions generated by\nGPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems,\nbut only $35.1\\%$ are semantically correct, highlighting the need for a more\nrigorous benchmark for this problem.",
        "updated": "2024-07-03 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03321v1"
    },
    {
        "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output",
        "authors": "Pan ZhangXiaoyi DongYuhang ZangYuhang CaoRui QianLin ChenQipeng GuoHaodong DuanBin WangLinke OuyangSongyang ZhangWenwei ZhangYining LiYang GaoPeng SunXinyue ZhangWei LiJingwen LiWenhai WangHang YanConghui HeXingcheng ZhangKai ChenJifeng DaiYu QiaoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2407.03320v1",
        "entry_id": "http://arxiv.org/abs/2407.03320v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03320v1",
        "summary": "We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "updated": "2024-07-03 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03320v1"
    },
    {
        "title": "BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations",
        "authors": "Zhantao YangRuili FengKeyu YanHuangji WangZhicai WangShangwen ZhuHan ZhangJie XiaoPingyu WuKai ZhuJixuan ChenChen-Wei XieChaojie MaoYue YangHongyang ZhangYu LiuFan Cheng",
        "links": "http://arxiv.org/abs/2407.03314v1",
        "entry_id": "http://arxiv.org/abs/2407.03314v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03314v1",
        "summary": "This paper presents Bag-of-Concept Graph (BACON) to gift models with limited\nlinguistic abilities to taste the privilege of Vision Language Models (VLMs)\nand boost downstream tasks such as detection, visual question answering (VQA),\nand image generation. Since the visual scenes in physical worlds are structured\nwith complex relations between objects, BACON breaks down annotations into\nbasic minimum elements and presents them in a graph structure. Element-wise\nstyle enables easy understanding, and structural composition liberates\ndifficult locating. Careful prompt design births the BACON captions with the\nhelp of public-available VLMs and segmentation methods. In this way, we gather\na dataset with 100K annotated images, which endow VLMs with remarkable\ncapabilities, such as accurately generating BACON, transforming prompts into\nBACON format, envisioning scenarios in the style of BACONr, and dynamically\nmodifying elements within BACON through interactive dialogue and more. Wide\nrepresentative experiments, including detection, VQA, and image generation\ntasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel\nin their current cutting-edge solutions.",
        "updated": "2024-07-03 17:55:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03314v1"
    },
    {
        "title": "A Review of the Applications of Deep Learning-Based Emergent Communication",
        "authors": "Brendon BoldtDavid Mortensen",
        "links": "http://arxiv.org/abs/2407.03302v1",
        "entry_id": "http://arxiv.org/abs/2407.03302v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03302v1",
        "summary": "Emergent communication, or emergent language, is the field of research which\nstudies how human language-like communication systems emerge de novo in deep\nmulti-agent reinforcement learning environments. The possibilities of\nreplicating the emergence of a complex behavior like language have strong\nintuitive appeal, yet it is necessary to complement this with clear notions of\nhow such research can be applicable to other fields of science, technology, and\nengineering. This paper comprehensively reviews the applications of emergent\ncommunication research across machine learning, natural language processing,\nlinguistics, and cognitive science. Each application is illustrated with a\ndescription of its scope, an explication of emergent communication's unique\nrole in addressing it, a summary of the extant literature working towards the\napplication, and brief recommendations for near-term research directions.",
        "updated": "2024-07-03 17:43:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03302v1"
    },
    {
        "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query",
        "authors": "Ziwei JiDelong ChenEtsuko IshiiSamuel CahyawijayaYejin BangBryan WiliePascale Fung",
        "links": "http://arxiv.org/abs/2407.03282v1",
        "entry_id": "http://arxiv.org/abs/2407.03282v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03282v1",
        "summary": "The hallucination problem of Large Language Models (LLMs) significantly\nlimits their reliability and trustworthiness. Humans have a self-awareness\nprocess that allows us to recognize what we don't know when faced with queries.\nInspired by this, our paper investigates whether LLMs can estimate their own\nhallucination risk before response generation. We analyze the internal\nmechanisms of LLMs broadly both in terms of training data sources and across 15\ndiverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\nOur empirical analysis reveals two key insights: (1) LLM internal states\nindicate whether they have seen the query in training data or not; and (2) LLM\ninternal states show they are likely to hallucinate or not regarding the query.\nOur study explores particular neurons, activation layers, and tokens that play\na crucial role in the LLM perception of uncertainty and hallucination risk. By\na probing estimator, we leverage LLM self-assessment, achieving an average\nhallucination estimation accuracy of 84.32\\% at run time.",
        "updated": "2024-07-03 17:08:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03282v1"
    }
]