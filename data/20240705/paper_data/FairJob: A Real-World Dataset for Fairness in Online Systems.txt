FairJob: A Real-World Dataset
for Fairness in Online Systems
MariiaVladimirova∗, EustacheDiemert FedericoPavone
CriteoAILab UniversitéParisDauphine-PSL
{m.vladimirova,e.diemert}@criteo.com federico.pavone@dauphine.psl.eu
Abstract
We introduce a fairness-aware dataset for job recommendation in advertising,
designed to foster research in algorithmic fairness within real-world scenarios.
It was collected and prepared to comply with privacy standards and business
confidentiality. An additional challenge is the lack of access to protected user
attributes such as gender, for which we propose a solution to obtain a proxy
estimate. Despitebeinganonymizedandincludingaproxyforasensitiveattribute,
ourdatasetpreservespredictivepowerandmaintainsarealisticandchallenging
benchmark. Thisdatasetaddressesasignificantgapintheavailabilityoffairness-
focusedresourcesforhigh-impactdomainslikeadvertising–theactualimpact
beinghavingaccessornottopreciousemploymentopportunities,wherebalancing
fairness and utility is a common industrial challenge. We also explore various
stagesintheadvertisingprocesswhereunfairnesscanoccurandintroduceamethod
to compute a fair utility metric for the job recommendations in online systems
casefromabiaseddataset. Experimentalevaluationsofbiasmitigationtechniques
on the released dataset demonstrate potential improvements in fairness and the
associatedtrade-offswithutility.
1 Introduction
The intersection of technology and human dynamics presents both opportunities and challenges,
particularly in the realm of artificial intelligence (AI). Despite advancements, persistent biases
rooted in historical inequalities permeate our data-driven systems, perpetuating unfairness and
exacerbatingsocietaldivides. Historicalbiasesshapedatacollection,influencingAImodeloutcomes
and often amplifying existing inequalities [Bolukbasi et al., 2016, Zhao et al., 2017, Chen et al.,
2023]. Despiteconcernsregardingprivacy,liability,andpublicrelations,thecollectionofspecial
andsensitivecategorydataiscrucialforbiasassessments[Andrusetal.,2021]. Moreover,evolving
legal frameworks, exemplified by the recent AI Act and General Data Protector Regulation [UK
InformationCommissioner’sOffice,2022], mandatethedetection, prevention, andmitigationof
biases,whileimposingsomerestrictionsontheuseofsensitivedata.
Recentadvancesinfairnessofteninvolvecomputervision,naturallanguageprocessingandspeech
recognitiontasks[Gustafsonetal.,2023,Andrewsetal.,2024,Halletal.,2024,Schumannetal.,
2024,VelicheandFung,2023],whilelackingattentiontoalgorithmicdecision-makingthatinvolves
tabulardata,whereeachrowrepresentsanindividualoranobservation,andeachcolumnrepresents
afeatureorattribute[LeQuyetal.,2022,Zhangetal.,2021],resultinginaveryfewbenchmark
papers[Gorishniyetal.,2021,2022,Grinsztajnetal.,2022,Shwartz-ZivandArmon,2022,Matteucci
etal.,2023]. Tabulardataiscommonlyusedinvarioushigh-riskdomainssuchasfinance,healthcare,
hiring,criminaljustice,andadvertising[vanBreugelandvanderSchaar,2024].
Algorithmic discrimination in advertising can be related to sensitive verticals which highlights
beneficial employment, financial and housing opportunities, or about who sees potentially less
Preprint.Underreview.
4202
luJ
3
]GL.sc[
1v95030.7042:viXradesirable advertising, such as ads for predatory lending services [Lambrecht and Tucker, 2019].
Whileunfairnessinadvertisingisnotpunitivebutratherassistive,i.e. fairnessconsistsinproviding
equal access to precious opportunities, it is essential to ensure fairness in advertising practices.
Insomecontextssuchashousingorlending,suchdiscriminationisexplicitlyprohibitedbylaw2.
Severalstudiesconductedanalysesonthefairnessinadvertisingatdifferentstagesandobserved
discriminatingbehaviorthatwasnotnecessarilyintendedbythead-services[Speicheretal.,2018,
LambrechtandTucker,2019,Andreouetal.,2019,Alietal.,2019]. Thisemphasizestheneedfor
bettermechanismstoauditandpreventbiasinads.
Mostofstudiesondiscriminatingbehaviorinadvertisingwereconductedviacreatingadvertising
campaignsandchoosingtargetedaudiencesandanalysingthedatafromtheuserperspectivewithout
accessingthealgorithmicfeatures[Speicheretal.,2018,LambrechtandTucker,2019,Andreouetal.,
2019,Alietal.,2019].Theabsenceofpubliclyavailable,realisticdatasetsleadsresearcherstopublish
resultsbasedonprivatedata,resultinginnon-reproducibleclaims[Geyiketal.,2019,Andreouetal.,
2019,Timmarajuetal.,2023,TangandYu,2022]. Thisposeschallengesforcriticalevaluationand
buildinguponpreviousworkinthescientificcommunity. TangandYu[2022]highlightsthelackof
publicbenchmarkingdatasetstostudythefairnessrelatedapproachesinadvertising.
Inaddition,mostofthestudiesassumethattheAIsystemshaveanaccesstotheprotectedattributes
whichisoftenunrealisticduetoprivacyconstraintsorlegalrestrictions[Holsteinetal.,2019,Lahoti
etal.,2020,Molinaetal.,2023,Timmarajuetal.,2023]. Inonlineadvertising, decision-makers
usuallyhaveaccesstoalogofuserinteractionswiththesystem,whichtheycanusetoguessthe
attributes. However,thelevelofinaccuracycanbesignificant,makingitdifficulttoensurethatan
adcampaignreachesanon-discriminatoryaudience[Gelauffetal.,2020]. Thismakesithardto
meetfairnessrequirements[Liptonetal.,2018]. Weemphasizetheneedforthoroughresearchin
real-worldsituationswhereaccesstoprotectedattributesislimited.
Contributions. Tofosterresearchinfairnesswithinreal-worldscenarios,wereleasealarge-scale
fairness-awaredatasetforadvertising. Thedatasetcontainspseudononymizedusers’contextand
publisherfeaturesthatwerecollectedfromajobtargetingcampaignranfor5months. Thedatahas
been sub-sampled non-uniformly to avoid disclosing business metrics. Feature names have been
anonymizedforbusinessconfidentiality,andtheirvaluesrandomlyprojectedtopreservepredictive
power whilemakingtherecoveryoftheoriginalfeaturesorusercontext(i.e. re-indentification)
practicallyimpossible,withaccordancetotheprivacy-safetymeasures3. Althoughourdatasetdoes
not contain explicit sensitive attributes such as gender, it includes a gender proxy derived from
non-protectedrelevantattributes,whichwediscussindetailfurther.
This dataset provides a baseline according to the eligible audience generated by an advertiser’s
targeting criteria for a specific ad. This ensures that ads are tailored to individuals whom the
advertisercanfeasiblyserve(suchasthosewithinaspecificgeographicregion)andwhoarelikelyto
beinterestedintheirofferings,apracticealreadygovernedbypoliciesandstandardsinHousing,
Employment,andCreditverticals. Sinceadvertisertargetingadherestopolicyconstraintstoprevent
discriminatorypractices4 (suchasprohibitingtheuseofgendercriteriainemploymentads), the
resultingeligibleaudienceremainsindependentofpredictionalgorithms,servingasareasonable
baselinemetric.
Withthereleaseddatasetweexaminethestagesintheadvertisingprocesswhereunfairnesscanoccur
andexploretechniquestomitigatesuchbiases. Takingintoaccountpossibleinducedbiases, we
proposeanunbiasedutilitymetricthathelptoanalysedifferentbiasmitigationtechniques. Wealso
2AccordingtoArticle6(2)oftheAIAct,targetedjobadvertisingisconsideredashigh-riskAIsystem.The
FairHousingActintheUnitedStatesmakesitillegaltodiscriminatebasedonreligion,color,nationaloriginor
genderforthesale,rentalorfinancingofhousing.
3’Pseudonymisation’ofdata–definedinArticle4(5)ofGeneralDataProtectionRegulation(GDPR)–means
replacinganyinformationwhichcouldbeusedtoidentifyanindividualwithapseudonym,or,inotherwords,a
valuewhichdoesnotallowtheindividualtobedirectlyidentified.Re-indentificationofthepseudomymized
dataisimpossibleduetoadditionalrandomizationtechniquesduringdataanonymization.Theoriginaldatadoes
notcontainany"specialcategories"ofpersonaldatalistedunderArticle9oftheGDPR,processingofwhichis
prohibited,exceptinlimitedcircumstancessetoutinArticle9oftheGDPR.Thepublicationofthedatasetwas
approvedbyaDataProtectorOfficerandLegalprofessionals.
4TheFairHousingActintheUnitedStatesmakesitillegaltodiscriminatebasedonreligion,color,national
originorgenderforthesale,rentalorfinancingofhousing.
2performexperimentsonthereleaseddatasettoverifyhowwecanimprovefairnessandthepossible
trade-offswithutility.
2 Relatedworks
Open-sourcedatasets. Alimitedavailabilityofpubliclyavailablefairness-awaretabulardatasets
challengesresearchadvancementsinalgorithmicfairness[LeQuyetal.,2022,Hortetal.,2023]. In
2022,LeQuyetal.[2022]studieddatasetsusedatleast3timesinresearchpublicationsonfairness,
and found out there were only 15 open-source fairness datasets, most of which are criticized for
beingtoosmallorfarfromreal-worldscenarios,includingthemostfrequentlyusedAdult[Duaand
Graf,2017]andCOMPASdataset[Larsonetal.,2016]. Eventhoughthereisapositivetendency
onaddressingthisissuebyopen-sourcingprivacy-complyingdatasets,suchasBAF[Jesusetal.,
2022] for bank fraud detection where the data was obtained via data generation techniques, or
WCLD[Ashetal.,2024],acuratedlarge-scaledatasetfromcircuitcourtstoaddresscriminaljustice,
thereisstilllackinavailabledatasetsinotherhigh-impactareassuchadvertising. Itisimportantfor
academicresearcherstohaveaccesstolargedatasetstostudytheproblemrigorously[L.Cardoso
et al., 2019, Li et al., 2022, Le Quy et al., 2022]. Large-scale datasets are advantageous as they
increasethelikelihoodofcapturingsignificantperformancedifferencesinexperimentswithnew
methods. Withlargerdatasetsizes,thevarianceofmetricsdecreases,enablingmorereliableand
meaningfulcomparisonsbetweendifferentapproaches.
Biasmitigationmethods. Theinitialsteptoenhancemodelfairnessistoexcludetheprotected
attributeasafeatureduringtraining,astrategyknownasfairnessthroughunawareness[Chenetal.,
2019]. However, this approach alone does not ensure fairness because the model may still learn
correlationsbetweenotherfeaturesandtheprotectedattributes,seeSection3.1andFigure2bfor
details. To achieve a higher level of fairness, AI systems typically employ one of the additional
methods: pre-processing,in-training,orpost-processing. WerefertoHortetal.[2023]forthemost
up-to-dateandthoroughsurvey.
Fairnesswithoutdemographics. Theinformationontheprotectedattributeisoftennotavailable
inpractice[Holsteinetal.,2019,Hortetal.,2023]. Severalworksstudiedlimitedavailabilityofthe
protectedattributesuchasviaaproxy[Guptaetal.,2018]orassumingthereisapartialaccesstothe
information[Hashimotoetal.,2018,Awasthietal.,2020,Molinaetal.,2023]. Lahotietal.[2020]
reliesontheassumptionthatprotectedgroupsarecomputationally-identifiable. However,ifthere
werenosignalaboutprotectedgroupsintheremainingfeaturesandclasslabels,wecannotmakeany
statementsaboutimprovingthemodelforprotectedgroups. Oneofthepossiblesolutionsistoget
datafromsecuremulti-partycomputation[VealeandBinns,2017,Kilbertusetal.,2018,Huetal.,
2019]ordirectlyfromusers[Gkiouzepietal.,2023]. However,thesetoolsarestilltobeadaptedto
real-worldsituations. Inaddition,transferleaningcanbeusefulwhenthereislittleavailabledataon
theprotectedattributes[Costonetal.,2019].
3 Fairnessinadvertising
The aim of ad-tech companies is to deliver the most relevant advertisements to users navigating
publishers’webpages. Bymatchingusers’browsinghistoriesandcontentpreferenceswithproducts
thatalignwiththeirinterests,targetedadvertisingcreatesamutuallybeneficialecosystem[Wang
et al., 2017, Choi et al., 2020]. Advertisers reach relevant audiences, users have access to free
informationandservicesinexchangeofseeingadsrelatedtotheirinterests,andplatformsprofitfrom
sellingtargetedads.
Ad-techcompaniesgrapplewithvastvolumesofnoisydata,whichencapsulateusers’pastactions.
Leveragingthisdata,theypredictpotentialclicksandconversions. However,ifthedataisbiased,the
algorithmscaninadvertentlyperpetuateandevenamplifythesebiases[Bolukbasietal.,2016,Zhao
etal.,2017,Chenetal.,2023]. Itiscrucialtoscrutinizethepredictorsforbiasanddevisesolutions
tomitigateit. Failingtodosocanresultindiscrepanciesbetweenofflineevaluationsandonline
metrics,ultimatelyharmingusersatisfactionandtrustintheserviceofonlinesystems[Chenetal.,
2023]. Whileadvertisingcommonplaceitemscarrieslittlerisk,companiesmustexercisecaution
withhigh-riskverticalslikejoboffers[Speicheretal.,2018,LambrechtandTucker,2019,Andreou
3etal.,2019,Alietal.,2019]. Forinstance,ifmanagerialpositionsaredisproportionatelyshown
tomenoverwomen,moremenmayapply,perpetuatinghistoricalbiasesandexacerbatinggender
disparities.
Biascanbeintroducedatseveralstagesintheadvertisingprocess,seeFigure1. First,whenauser
visitsawebpagewithanadslot,ad-techcompaniesparticipateinareal-timebidding(RTB)auction.
Duringthisauction,companiesselectacampaign(e.g.,joboffersorclothing)basedonattributesof
thepublisherandtheuser,includingtheirlogofpastinteractionssuchasseenads,theircontext,the
factofclicksontheads,seeSection3.2. Thisauctionmustbeorganizedinafairway,respecting
boththecompaniesplacingbidsandthepublishersprovidingadslots, seeSection3.3. Afteran
ad-techcompanywinsthedisplayauction,thereisthechoiceofwhichproducttoshow(e.g.,asenior
positionjoboranassistantjob). Thisselectioncanalsointroducebiaswithrespecttotheuser,see
Section3.4. Ensuringfairnessatthisstageiscriticaltopreventingthereinforcementofexisting
inequalities.
Selection Market Algorithmic
Userenters bias Campaign bias Real-time bias Ad
webpage selection bidding selection
Figure1: Simplifiedschemeofonlineadvertisingprocessofadselection: (i)userentersawebpage
withavailablebannerforanad,(ii)webpagesendsarequesttoparticipateinthereal-timebidding
auctionwhichtriggerscampaignselectionbyanadserviceforagivenuser,(iii)afterthecampaignis
chosen,ad-servicesendsabidproposition,(iv)iftheproposedbidwontheauction,therecommenda-
tionenginechoosesthebestadfromthechosencampaignandshowsitonthewebpage.
3.1 Fairnessdefinition
Webaseourdiscussiononacounterfactualfairnessframeworkthatexplainstheunderlyingconnec-
tionsbetweenthevariablesinthesystem[Kusneretal.,2017]. LetAdenoteaprotectedattribute
(canbeasetofprotectedattributes)ofanindividual,X denotetheotherobservableattributesofany
particularindividual,Y denotetheoutcometobepredicted,andletYˆ beapredictor. Thepredictor
takes into account the available data from logs of user interactions with the system and product
descriptionsandestimatestheprobabilityofapositiveoutcome,i.e. clickoftheuserontheproduct.
The system takes into account the prediction and then shows the best product to the user, which
resultsintopossiblepositiveoutcome. Inouranalysis,weareinterestedinunderstandinghowAand
X influenceY andhowwellourpredictorYˆ capturestheserelationships. Ourgoalisnotjustto
predictoutcomesaccuratelybutalsotoensurefairnessandmitigatebiasesinthepredictionswith
respecttoA. ThererandomvariableshavethecausalrelationshipsthataremodelledonFig.2and
wediscussfurtherindetail.
X A X A X A
Yˆ Y Yˆ Y Yˆ Y
(a)Unfairnesswithdirectdepen- (b)Fairnessthroughunawareness: (c)Fairnessviazeromutualinfor-
dence:Yˆ =f(X,A). Yˆ =f(X(A)). mation:Yˆ ⊥⊥A|X
Figure2: Causalgraphdepictingeffectsofvariablesappearingduringmodeltrainingunderdifferent
constraints.
Itisimportanttounderstandhowtheinformationflowsduringthetrainingproceduretocreatethe
prediction. Ifwegiveaprotectedattributeasafeaturetothepredictionmodelduringtraininglike
inFig.2a,themodelwilllearndirectlythebiasA → Yˆ. Thefirststepistoremovetheprotected
attributefromthetrainingfeatures,inthiscasethemodelmightlearnthebiasindirectlythroughthe
featuresA→X →Yˆ,seeFig.2b. Thus,withoutbiasmitigationtechniques,thepredictionmodel
4learnsthebiasthatexistsinthedata. Fromcausalperspective,thecorrectiontechniquescorrespond
toblockingtheinformationflowfromAtoYˆ byenforcingthezeromutualinformationbetween
thesevariablesconditionedonX,seeFig.2c. WerefertoHortetal.[2023]forthesurveyonbias
mitigationmethods. Theoretically,wecanmitigatetheprotectedattributebiaswhenhavingaccessto
theinformationthatisusedbyanalgorithmduringtrainingandhavingatleastpartialinformation
abouttheprotectedattribute[Lahotietal.,2020,Hortetal.,2023].
Fromacausalperspective,fairoutcomewithrespecttoaprotectedattributemeansthatitwouldnot
havechangediftheother(counterfactual)valuefortheprotectedattributedwasimposed[Kusner
etal.,2017]:
(cid:16) (cid:12) (cid:17) (cid:16) (cid:12) (cid:17)
P Y =y(cid:12)do(A=a),X =x =P Y =y(cid:12)do(A=a′),X =x . (1)
(cid:12) (cid:12)
Sinceinareal-worldscenariositisnotpossibletohavecounterfactualestimation(wecannotimpose
ausertochangetheirgender),weconsideraveragevaluesforgroups,e.g. demographicparity5:
P(Y =y|A=a)=P(Y =y|A=a′) (2)
Thisisinlinewithcurrentlawsthataimtoensurefairnessinhowhousingandjobadsarepresented6.
Theselawsdonotfocusonwhethercertainpeoplearewronglyincludedorexcluded(individual
fairness),ratheronmakingsuretheadsarerepresentative(groupfairness). Thekeymeasurementis
thedifferenceinaveragethatdifferentgroupswillbeshownthead,regardlessofhowlikelyeach
groupistoactuallyrespondtothead.
3.2 Selectionbiasincampaignchoosing
Inoursetting,weareinterestedinassessingthefairnessinspecificcampaign(e.g.,jobcampaign)
with respect to the protected attribute. For instance, we want to ensure that job advertisements
formanagerialrolesarefairwithrespecttoabinaryprotectedattributeA ∈ {0,1}(e.g.,gender).
Typically,thedataconsideredinthisframeworkregardsthejobadvertisementsforuserswhichhave
been assigned to the job campaign c. However, the campaign selection process might introduce
selectionbias, whichshouldbetakeninaccount. Inparticular, P(A = 1)andP(A = 0)arethe
(internet)-populationlevelofabinaryprotectedattribute. Thismightbeapproximatedtothecensus
populationfrequenciesoftheprotectedattribute. LetC bearandomvariableofchoosingacampaign,
thenP(A=1|C =c)andP(A=0|C =c)arethefrequenciesoftheprotectedattributeinthe
jobcampaigndatac. Thesedifferfromthepopulationlevelsduetoselectionbias.
Note that the recommendation engines predict P(Y = 1 | A = a,C = c) for a product in the
campaign c. Thus, if we use prediction bias mitigation techniques while considering data at the
campaignlevel,inthebestcasescenario,weobtainfairpredictionswhilebeingunfairoutsideof
campaing, P(Yˆ = 1 | A = 0,C = c) = P(Yˆ = 1 | A = 1,C = c) and P(Yˆ = 1 | A = 0) ̸=
P(Yˆ = 1 | A = 1). Thus,wehavetotakeintoaccounttheselectionbiastoensuredemographic
parityintroducedinEq.(2):
P(A=1) P(A=0)
P(Yˆ =1|A=1,C =c) =P(Yˆ =1|A=0,C =c) .
P(A=1|C =c) P(A=0|C =c)
(3)
3.3 Marketbias
Lambrecht and Tucker [2019] found that women are a prized demographic, making them more
expensivetoadvertiseto. Thisimpliesthatadsthataremeanttobegender-neutralcanbedelivered
in the way that appears to be discriminatory by RTB algorithms that focus on optimizing cost-
effectiveness. Alietal.[2019]explainedthatthisisnotsolelytheindicationoftheingrainedcultural
biasnoraresultofuserprofilesinputtedintoadsalgorithms,butrathertheproductofcompetitive
spillovers among advertisers. Additionally, the feedback loop mechanism considers imbalanced
5Wedonotuseequalopportunitymetricbecausedespitethenameitactuallypreservesexitingbiasinthe
caseofassistivefairness,seeAppendixfordetails.
6TheFairHousingActintheUnitedStatesmakesitillegaltodiscriminatebasedonreligion,color,national
originorgenderforthesale,rentalorfinancingofhousing.
5information–how recommendation systems expose content influences user behavior, which then
becomes the training data for future predictions. This feedback loop not only introduces biases
but also amplifies them over time, leading to a ’rich get richer’ scenario known as the Matthew
effect. [Chen et al., 2023]. Imbalanced data with respect to a protected attribute also effects the
learningofaprediction,sinceanalgorithmthatreceivesinrealtimelessdataaboutonegroup,will
learnatdifferentspeeds[LambrechtandTucker,2020]. Theseeffectsarehardtoestimateandshould
beaddressedbytheRTBprocess. Apartfromusers,advertiserscanalsobeunfairlytreatedduring
theRBTauctionprocess[Celisetal.,2019a,Chenetal.,2023]butherewefocussolelyontheuser
discrimination.
3.4 Recommendationbias
X A X A
D Yˆ D Yˆ
(a)Indirectunfairness:Yˆ =f(X(A),D(A)). (b)Fairness,zeromutualinformation:Yˆ ⊥⊥A|X,D
Figure 3: Causal graph depicting effects of variables appearing during model training for an ad
recommendationsystemunderdifferentconstraints.
Intheadrecommendationsystem,thegoalistochoosebestproductsforauserforagivenbanner
thatcanhaveseveraldisplaysatthesametime. Thegoalistomaximizethenumberofclicksfora
givenbanner,meaningthattherecanbeseveralproductsclicked. Whenwehaveseveraldisplaysto
showtoauser,thedisplayrankpositionbecomesimportantandcreatespositionbiaswithrespectto
apositiveoutcome. Theinfluenceofthisbiasishardtoestimate,however,itisimportanttotakeit
intoaccount[SinghandJoachims,2018,2019,Moriketal.,2020,Usunieretal.,2022].
LetJ bearandomvariabledenotingthesetofbannertobeshowntoauser,Dbeadisplay(chosen
product, i.e. job offer) shown to a user on a banner J. Let model f(x,d) predicts the following
positiveoutcome: P(Y = 1|X = x,D = d),i.e. theprobabilityofaclickforachosenproductd
givenuserfeaturesx. Asdiscussedabove,wehavetotakeintoaccountthedisplaypositionwhich
expressedviavariablerankR. However,theinfluenceofthepositionontheutilityishardtoestimate.
Further,wesuggestutilitymetricsforadsrecommendationandinordertoavoidthepositionbias,
wesuggesttocomputethemonlyonrandomizeddisplays,wherethepositionoftheproductsonthe
bannerwaschosenrandomly.
Click-rankutility. Theusers’utilityforagivenmodelcanbeexpressedasapositiveengagement
inthefollowingway:
U(f)=E E (cid:2)I(Y =1) rank f(X ,D)(cid:3) , (4)
I X,D|I D I I
whereI(Y =1)istheidentityfunctionofapositiveoutcome(e.g.click)fordisplayD.Thefunction
D
rank computestheascendingorderrankwithinthesetofdisplaysforabannerofimpressionI.
D
Thismetricsisbasedonestimationofthepositiveoutcomebasedonthepassedeventsforchosen
users.
Product-rankutilityforbiaseddata. Wenoticethatthemetricsforthealgorithmcanbebiased
duetotheselectionbiasdiscussedinSection3.2becausethepredictionalgorithmestimatesP(Y =
1|A = a,C = c) instead of P(Y = 1|A = a). Even if we correct the prediction bias in P(Y =
1|A=a,C =c)basedonthedataprovidedforgivencampaignc,itdoesnotcorrectthefinalbias
inP(Y = 1|A = a)duetoselectionbias. Wecanadapttheclick-rankutilitytoincludepossible
selectionbiasintothemetric,byexplicitlyconsideringthattheproductutilitydependsonachosen
campaign. Then,whencorrectingfortheunfairnessintheprediction,wemightimprovetheutility
metrictakenintoaccounttheselectionbiasinthedata:
6(cid:20) P(A=a ) (cid:21)
U˜(f)=E E I(Y =1) XI rank f(X ,D) , (5)
D I|D D P(A=a |C =c) I I
XI
wherea standsforagenderofagivenuserX. Intuitively,ifthepredictionisbiasedwithrespectto
X
protectedattributeA,thefinalpredictionP(Y =1|A=a)isevenmorebiasedduetoselectionbiasof
withrespecttotheprotectedattributeofchoosingacampaignC =c:P(C =c|A=a). Inthiscase,
thepredictionmodelamplifiestheexistinghistoricalbias. However,wecanremovetheselectionbias
byaddingweightsthatcorrespondtothepresenceoftheprotectedattributeinthewholepopulation
andgiventhecampaign. IftheuserwithprotectedattributeA=ahaslowerprobabilityofclick,and
thisgroupwasunderrepresentedinthecampaignC = c,i.e. P(A = a) > P(A = a|C = c),then
intheutilityfunction,themodel’spredictionwillbehigher,byaddressingthepossiblebiasdueto
under-representationinthedata. Thisisoursuggestedmetrictoevaluatetherecommendationsystem
whentheselectionbiasispresentandknownsuchasintheFairJobdataset.
4 FairJobsdataset
We introduce FairJobs7 dataset that contains fairness-aware data from a real-world scenario of
advertising. Thisdatasetisintendedtolearnclickpredictionsmodelsandevaluatebyhowmuch
their predictions are biased between different gender groups. The dataset consists of 1,072,226
rowsthatwerecollectedduring5monthsofatargetedjobcampaign8,eachrowrepresentsajob
adanduserfeatures: 20categoricaland39numericalfeatures;labelclick(binary,iftheadwas
clicked), protected_attribute (binary, proxy for user gender, see below for more thorough
explanation),senior(binary,ifthejobofferwasforaseniorposition),[user_id,impression_id,
product_id]areunqiueidentifiersofuser,impressionandproduct(jobad).Moredetailsanddataset
statisticsarereferredtoAppendix.
Detailsongenderproxy. Sincewedonotdirectlyaccessuserdemographics,wehavetofindaway
togetaproxyofrelevantattribute9. Mostofrecentworksleveragetheuseofexternaldataorprior
knowledgeoncorrelationstoobtainproxiestorelevantattributes[Guptaetal.,2018,Hashimotoetal.,
2018,Awasthietal.,2020,Lahotietal.,2020]. Wedefineaproductgender,eithergivenbyaclient,
eitherbyacategoryoftheproduct. Thisgivesusapproximately40%ofproductsgenderidentified.
Then,wefollowtheavailablestatisticsandchoosethegenderproxybasedonthedominantgenderof
productstheuserinteractswith. Thisgenderproxyidentitiesabehaviorofauser,i.e. ifausertends
tobuyfemaleormaleproducts. Thegenderproxydoesnotnecessarilycorrelatewiththegender,
asitoftenhappenswiththeproxyvariables[Gelauffetal.,2020]. Verificationoftheaccuracyof
theseapproximationsischallenging. Additionally,iftherearenosignalaboutprotectedgroupsinthe
remainingfeaturesandclasslabels,wecannotmakeanystatementsaboutimprovingthemodelfor
protectedgroups[Lahotietal.,2020].
Limitations and interpretation. We remark that the proposed gender proxy does not give a
definitionofthegender. Sincewedonothaveaccesstothesensitiveinformation,thisisthebest
solutionwehaveidentifiedatthisstagetoidenitifybiasonpseudonymiseddata,andweencourage
any discussion on better approximations. This proxy is reported as binary for simplicity yet we
acknowledgegenderisnotnecessarilybinary. Althoughourresearchfocusesongender,thisshould
notdiminishtheimportanceofinvestigatingothertypesofalgorithmicdiscrimination. Whilethis
datasetprovidesimportantapplicationoffairness-awarealgorithmsinahigh-riskdomain,thereare
severalfundamentallimitationthatcannotbeaddressedeasilythroughdatacollectionorcuration
processes. Theselimitationsincludehistoricalbiasthataffectapositiveoutcomeforagivenuser,as
wellastheimpossibilitytoverifyhowclosethegender-proxyistotherealgendervalue. Additionally,
theremightbebiasduetothemarketunfairnessthatweexplainedinSection3.3. Suchlimitations
7https://huggingface.co/datasets/criteo/FairJob
8Weleavethedetailsonthedatacollection,featureengineeringandprivacy-preservingstepsinthesupple-
mentalmaterial.
9ThisgenderproxyisusedwiththeAdTechcompanytocreatein-marketaudiences: whenaclientcan
choosetoshowadvertisingoftheirproductsto“peoplethatbuymorefemaleproducts”or“peoplethatbuy
moremaleproducts”.Wenotethatthisproxyisnotusedinthepredictionengineandforcampaigncreationon
high-riskverticals.ThecampaigncreationisgovernedbypoliciesandstandardsinHousing,Employment,and
CreditverticalsthatcomplieswiththeFairHousingActintheUSA.
7andpossibleethicalconcernsaboutthetaskshouldbetakenintoaccountwhiledrawingconclusions
fromtheresearchusingthisdataset. Readersshouldnotinterpretsummarystatisticsofthisdatasetas
groundtruthbutratherascharacteristicsofthedatasetonly.
5 Empiricalobservations
Challenges. Thefirstchallengecomesfromhandlingthedifferenttypesofdatathatarecommon
in tables, the mixed-type columns: there are both numerical and categorical features that have to
beembedded[Gorishniyetal.,2021,2022,Grinsztajnetal.,2022,Shwartz-ZivandArmon,2022,
Matteuccietal.,2023]. Inaddition,someofthefeatureshavelong-tailphenomenonandproducts
havepopularitybias,seeFigure4. Ourdatasetscontainsmorethan1,000,000lines,whilecurrent
high-performing models are under-explored in scale, e.g. the largest datasets in Grinsztajn et al.
[2022] are only 50,000 lines, while in Gorishniy et al. [2021, 2022] only one dataset surpasses
1,000,000 lines. Additional challenge comes from strongly imbalanced data: the positive class
proportioninourdataislessthan0.007thatleadstochallengesintrainingrobustandfairmachine
learningmodels[Jesusetal.,2022,Yangetal.,2024].Inourdatasetthereisnosignificantimbalances
indemographicgroupsusersregardingtheprotectedattribute(bothgendersaresub-sampledwith0.5
proportion,femaleprofileuserswereshownlessjobadwith0.4proportionandslightlylesssenior
positionjobswith0.48proportion),however,therecouldbeahiddeneffectofabiasthatwediscussed
inSection3. Thisposesaprobleminaccuratelyassessingmodelperformance[vanBreugeletal.,
2024]. Moredetailedstatisticsandexploratoryanalysisarereferredtothesupplementalmaterial.
50
60000 Job seniority
9000 40 0
1
40000 30
6000
20
20000
3000
10
0 0 0
1 10 20 30+ 1 5 10 15 20+ 0 1 2 3
Number of impressions per user Banner size Log of product frequency
Figure4: ExamplesofsomefeaturestatisticsinFairJobdataset: numberofimpressionsperuserand
bannersizehavelongtailphenomenon(twoplotsontheleft). Theproductshavepopularitybias
(rightplot),i.e. someproductshavemuchhigherorlowerthanaveragenumberofclickswithsenior
jobadshavingmoreclicksonaverage.
Results. Wechoosetwobaselinemethods: (i)unfair,thatusesallattributesfortraining,including
theprotectedone;and(ii)unaware,thatcorrespondstofairnessthroughunawareness,i.e. usingall
attributesduringtrainingexcepttheprotectedone. Wecomparetheresultsofthesealgorithmswitha
(iii)fairmodelthatistrainedwithoutprotectedattributewithadditional"fairness"penalty[Kamishima
etal.,2011,BechavodandLigett,2017]. Thethreemodelscorrespondtothesituationsdescribed
inFigure2. Wereferallthereproducibilitydetailsandadditionalexperimentstothesupplemental
material. TheresultedpredictioncanbevisuallycomparedinFigure5.
These findings suggest that the trade-off relationship between accuracy and fairness is context-
dependent. Ithighlightstheneedforfurtherresearchtobetterunderstandtheconditionsunderwhich
theaccuracyfairnesstrade-offarisesandidentifystrategiestomitigateorovercomeit.
Additionally,weproposetorestraintanaccesstotheprotectedattributeandstudythetrade-offwhen
wetrainthemodelonthewholetrainsetbutaddfairnesspenaltyonlyforsomepercentageoftrain
set. Insomescenarios,weseeimprovementsinfairnesswithoutsacrificingtheoverallperformance.
Thelossinaccuracyduetotheimposedfairnessconstraintsisoftensmallasalsonotedinother
works[Celisetal.,2019b]. Weexplorebiascorrectiontechniquestailoredtoaddressdatalimitations
andpreserveutilityinlarge-scale. WedemonstratehowprioritizingfairnessinAInotonlybenefits
usersbyfosteringinclusivitybutalsocontributestothelong-termsuccessandethicalintegrityof
companies. Detailsandresultsoftheexperimentsarereportedinthesupplementalmaterial.
8
skcilc
fo
rebmuNUnfair Unaware With fairness penalty
Protected attribute
1.5 0
1
1.0
0.5
0.0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Probability of a click
Figure5: Probabilitydensitydistributionsofclickfordifferentvaluesoftheprotectedattributeof
threemodelstrainedindifferentways: (i)unfair–withaprotectedattributeincludedasafeature
duringtraining,(ii)unaware–correspondstofairnessthroughunawareness,(iii)trainedwithfairness
penaltyasabiasmitigationtechnique.
6 Conclusion
Addressing bias in AI goes beyond mere compliance with legal frameworks like the AI Act; it
necessitatesproactivemeasurestodetect,prevent,andmitigatebiases. Drawingfromreal-world
challenges faced by industries, we highlight the limitations of existing bias mitigation strategies,
particularlyinenvironmentswhereaccesstosensitiveuserattributesisrestricted. Weencourage
otherauthorsandpractitionerstoexperimentwithdifferentAIorFairAIalgorithmsonthisdataset.
Weexpectthatwiththiswork,thequalityofevaluationofnovelAImethodsincreases,potentiating
thedevelopmentofthearea. Additionally,wehopeitencouragesothersimilarrelevantdatasetstobe
publishedfromotherauthorsandinstitutions.
References
TakuyaAkiba,ShotaroSano,ToshihikoYanase,TakeruOhta,andMasanoriKoyama.Optuna:Anext-
generationhyperparameteroptimizationframework. InACMSIGKDDinternationalconference
onknowledgediscovery&datamining,2019.
MuhammadAli,PiotrSapiezynski,MirandaBogen,AleksandraKorolova,AlanMislove,andAaron
Rieke. Discrimination through optimization: How Facebook’s Ad delivery can lead to biased
outcomes. ACMonHuman-ComputerInteraction,2019.
AthanasiosAndreou,MárcioSilva,FabrícioBenevenuto,OanaGoga,PatrickLoiseau,andAlan
Mislove. MeasuringtheFacebookadvertisingecosystem. InNetworkandDistributedSystem
SecuritySymposium,2019.
JeroneAndrews, DoraZhao, WilliamThong, ApostolosModas, OrestisPapakyriakopoulos, and
AliceXiang. Ethicalconsiderationsforresponsibledatacuration. AdvancesinNeuralInformation
ProcessingSystems,2024.
McKaneAndrus,ElenaSpitzer,JeffreyBrown,andAliceXiang. Whatwecan’tmeasure,wecan’t
understand: Challenges to demographic data procurement in the pursuit of fairness. In ACM
ConferenceonFairness,Accountability,andTransparency,2021.
Elliott Ash, Naman Goel, Nianyun Li, Claudia Marangon, and Peiyao Sun. WCLD: Curated
largedatasetofcriminalcasesfromWisconsinCircuitCourts. AdvancesinNeuralInformation
ProcessingSystems,2024.
PranjalAwasthi,MatthäusKleindessner,andJamieMorgenstern. Equalizedoddspostprocessing
underimperfectgroupinformation. InInternationalConferenceonArtificialIntelligenceand
Statistics,2020.
YahavBechavodandKatrinaLigett. Penalizingunfairnessinbinaryclassification. arXivpreprint
arXiv:1707.00044,2017.
9TolgaBolukbasi,Kai-WeiChang,JamesYZou,VenkateshSaligrama,andAdamTKalai. Manisto
computerprogrammeraswomanistohomemaker? Debiasingwordembeddings. Advancesin
NeuralInformationProcessingSystems,2016.
ElisaCelis,AnayMehrotra,andNisheethVishnoi. Towardcontrollingdiscriminationinonlinead
auctions. InInternationalConferenceonMachineLearning,2019a.
LElisaCelis,LingxiaoHuang,VijayKeswani,andNisheethKVishnoi. Classificationwithfairness
constraints:Ameta-algorithmwithprovableguarantees.InConferenceonFairness,Accountability,
andTransparency,2019b.
JiahaoChen,NathanKallus,XiaojieMao,GeoffrySvacha,andMadeleineUdell. Fairnessunder
unawareness: Assessingdisparitywhenprotectedclassisunobserved. InConferenceonFairness,
Accountability,andTransparency,2019.
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and
debiasinrecommendersystem: Asurveyandfuturedirections. ACMTransactionsonInformation
Systems,2023.
TianqiChenandCarlosGuestrin. Xgboost: Ascalabletreeboostingsystem. InProceedingsofthe
22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,pages
785–794,2016.
HanaChoi,CarlF.Mela,SantiagoR.Balseiro,andA.Leary. Onlinedisplayadvertisingmarkets: A
literaturereviewandfuturedirections. InformationSystemsResearch,31(2):556–575,2020.
AmandaCoston,KarthikeyanNatesanRamamurthy,DennisWei,KushRVarshney,SkylerSpeakman,
ZairahMustahsan,andSupriyoChakraborty.Fairtransferlearningwithmissingprotectedattributes.
InAAAI/ACMConferenceonAI,Ethics,andSociety,2019.
KonstantinDonhauser,JavierAbad,NehaHulkund,andFannyYang. Privacy-preservingdatarelease
leveragingoptimaltransportandparticlegradientdescent. InternationalConferenceonMachine
Learning,2024.
DheeruDuaandCaseyGraf. UCIMachineLearningRepository. 2017. URLhttp://archive.
ics.uci.edu/ml.
LodewijkGelauff,AshishGoel,KameshMunagala,andSravyaYandamuri. Advertisingfordemo-
graphicallyfairoutcomes. arXivpreprintarXiv:2006.03983,2020.
SahinCemGeyik,StuartAmbler,andKrishnaramKenthapadi. Fairness-awarerankinginsearchand
recommendationsystemswithapplicationtolinkedintalentsearch.InACMSIGKDDInternational
ConferenceonKnowledgeDiscoveryandDataMining,2019.
Eleni Gkiouzepi, Athanasios Andreou, Oana Goga, and Patrick Loiseau. Collaborative ad trans-
parency: Promisesandlimitations. InIEEESymposiumonSecurityandPrivacy,2023.
YuryGorishniy,IvanRubachev,ValentinKhrulkov,andArtemBabenko. Revisitingdeeplearning
modelsfortabulardata. InAdvancesinNeuralInformationProcessingSystems.CurranAssociates,
Inc.,2021.
Yury Gorishniy, Ivan Rubachev, and Artem Babenko. On embeddings for numerical features in
tabulardeeplearning. AdvancesinNeuralInformationProcessingSystems,2022.
LeoGrinsztajn,EdouardOyallon,andGaelVaroquaux. Whydotree-basedmodelsstilloutperform
deeplearningontypicaltabulardata? InAdvancesinNeuralInformationProcessingSystems,
2022.
MayaRGupta,AndrewCotter,MahdiMilaniFard,andSerenaWang. Proxyfairness. arXivpreprint
arXiv:1806.11212,2018.
Laura Gustafson, Chloe Rolland, Nikhila Ravi, Quentin Duval, Aaron Adcock, Cheng-Yang Fu,
MelissaHall,andCandaceRoss. FACET:Fairnessincomputervisionevaluationbenchmark. In
InternationalConferenceonComputerVision,2023.
10SiobhanMackenzieHall,FernandaGonçalvesAbrantes,HanwenZhu,GraceSodunke,Aleksandar
Shtedritski, and Hannah Rose Kirk. VisoGender: A dataset for benchmarking gender bias in
image-textpronounresolution. AdvancesinNeuralInformationProcessingSystems,2024.
MoritzHardt,EricPrice,andNatiSrebro. Equalityofopportunityinsupervisedlearning. Advances
inNeuralInformationProcessingSystems,2016.
TatsunoriHashimoto,MeghaSrivastava,HongseokNamkoong,andPercyLiang. Fairnesswithout
demographicsinrepeatedlossminimization. InInternationalConferenceonMachineLearning,
2018.
Kenneth Holstein, JenniferWortmanVaughan, HalDauméIII,MiroDudik, andHannaWallach.
Improvingfairnessinmachinelearningsystems: Whatdoindustrypractitionersneed? InCHI
ConferenceonHumanFactorsinComputingSystems,2019.
MaxHort,ZhenpengChen,JieMZhang,MarkHarman,andFedericaSarro. Biasmitigationfor
machinelearningclassifiers: Acomprehensivesurvey. ACMJournalonResponsibleComputing,
2023.
HuiHu,YijunLiu,ZhenWang,andChaoLan. Adistributedfairmachinelearningframeworkwith
privatedemographicdataprotection. InInternationalConferenceonDataMining,2019.
SérgioJesus,JoséPombal,DuarteAlves,AndréCruz,PedroSaleiro,RitaRibeiro,JoãoGama,and
PedroBizarro.Turningthetables:Biased,imbalanced,dynamictabulardatasetsforMLevaluation.
AdvancesinNeuralInformationProcessingSystems,2022.
ToshihiroKamishima,ShotaroAkaho,andJunSakuma. Fairness-awarelearningthroughregulariza-
tionapproach. InInternationalConferenceonDataMiningWorkshops,2011.
NikiKilbertus,AdriàGascón,MattKusner,MichaelVeale,KrishnaGummadi,andAdrianWeller.
Blindjustice: Fairnesswithencryptedsensitiveattributes. InInternationalConferenceonMachine
Learning,2018.
MattJKusner,JoshuaLoftus,ChrisRussell,andRicardoSilva. Counterfactualfairness. Advancesin
NeuralInformationProcessingSystems,2017.
RodrigoL.Cardoso,WagnerMeiraJr,VirgilioAlmeida,andMohammedJ.Zaki. Aframeworkfor
benchmarkingdiscrimination-awaremodelsinmachinelearning. InAAAI/ACMConferenceonAI,
Ethics,andSociety,2019.
PreethiLahoti,AlexBeutel,JilinChen,KangLee,FlavienProst,NithumThain,XuezhiWang,and
EdChi. Fairnesswithoutdemographicsthroughadversariallyreweightedlearning. Advancesin
NeuralInformationProcessingSystems,2020.
AnjaLambrechtandCatherineTucker. Algorithmicbias? Anempiricalstudyofapparentgender-
baseddiscriminationinthedisplayofSTEMcareerads. Managementscience,2019.
AnjaLambrechtandCatherineETucker. Apparentalgorithmicdiscriminationandreal-timealgorith-
miclearning. 2020.
JeffLarson,SuryaMattu,LaurenKirchner,andJuliaAngwin. COMPASBrowardcountydataset.
2016. URLhttps://github.com/propublica/compas-analysis.
TaiLeQuy,ArjunRoy,VasileiosIosifidis,WenbinZhang,andEiriniNtoutsi.Asurveyondatasetsfor
fairness-awaremachinelearning. WileyInterdisciplinaryReviews: DataMiningandKnowledge
Discovery,12(3),2022.
NianyunLi,NamanGoel,andElliottAsh. Data-centricfactorsinalgorithmicfairness. InAAAI/ACM
ConferenceonAI,Ethics,andSociety,2022.
Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. Does mitigating ML’s impact
disparityrequiretreatmentdisparity? AdvancesinNeuralInformationProcessingSystems,2018.
11FrancescoLocatello,GabrieleAbbati,ThomasRainforth,StefanBauer,BernhardSchölkopf,and
OlivierBachem. Onthefairnessofdisentangledrepresentations. AdvancesinNeuralInformation
ProcessingSystems,2019.
JérémieMary,ClémentCalauzenes,andNoureddineElKaroui. Fairness-awarelearningforcontinu-
ousattributesandtreatments. InInternationalConferenceonMachineLearning,2019.
FedericoMatteucci,VadimArzamasov,andKlemensBöhm. Abenchmarkofcategoricalencoders
forbinaryclassification. AdvancesinNeuralInformationProcessingSystems,2023.
DanieleMicci-Barreca. Apreprocessingschemeforhigh-cardinalitycategoricalattributesinclassifi-
cationandpredictionproblems. ACMSIGKDDExplorationsNewsletter,2001.
MathieuMolina, NicolasGast, PatrickLoiseau, andVianneyPerchet. Trading-offpricefordata
qualitytoachievefaironlineallocation. InAdvancesinNeuralInformationProcessingSystems,
2023.
Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. Controlling fairness and
biasindynamiclearning-to-rank. InACMSIGIRConferenceonResearchandDevelopmentin
InformationRetrieval,2020.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems,
2019.
F.Pedregosa,G.Varoquaux,A.Gramfort,V.Michel,B.Thirion,O.Grisel,M.Blondel,P.Pretten-
hofer,R.Weiss,V.Dubourg,J.Vanderplas,A.Passos,D.Cournapeau,M.Brucher,M.Perrot,and
E.Duchesnay. Scikit-learn: MachinelearninginPython. JournalofMachineLearningResearch,
12:2825–2830,2011.
Candice Schumann, Femi Olanubi, Auriel Wright, Ellis Monk, Courtney Heldreth, and Susanna
Ricco. ConsensusandsubjectivityofskintoneannotationforMLfairness. AdvancesinNeural
InformationProcessingSystems,2024.
RavidShwartz-ZivandAmitaiArmon. Tabulardata: Deeplearningisnotallyouneed. Information
Fusion,81:84–90,2022.
Ashudeep Singh and Thorsten Joachims. Fairness of exposure in rankings. In ACM SIGKDD
InternationalConferenceonKnowledgeDiscovery&DataMining,2018.
AshudeepSinghandThorstenJoachims. Policylearningforfairnessinranking. AdvancesinNeural
InformationProcessingSystems,2019.
Till Speicher, Muhammad Ali, Giridhari Venkatadri, Filipe Nunes Ribeiro, George Arvanitakis,
Fabrício Benevenuto, Krishna P Gummadi, Patrick Loiseau, and Alan Mislove. Potential for
discrimination in online targeted advertising. In Conference on Fairness, Accountability and
Transparency,2018.
XiaoliTangandHanYu. Towardstrustworthyai-empoweredreal-timebiddingforonlineadvertise-
mentauctioning. arXivpreprintarXiv:2210.07770,2022.
AdityaSrinivasTimmaraju,MehdiMashayekhi,MingliangChen,QiZeng,QuintinFettes,Wesley
Cheung,YihanXiao,ManojkumarRangasamyKannadasan,PushkarTripathi,SeanGahagan,etal.
Towardsfairnessinpersonalizedadsusingimpressionvarianceawarereinforcementlearning. In
ACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,2023.
RyanTurner,DavidEriksson,MichaelMcCourt,JuhaKiili,EeroLaaksonen,ZhenXu,andIsabelle
Guyon. Bayesianoptimizationissuperiortorandomsearchformachinelearninghyperparameter
tuning: Analysisoftheblack-boxoptimizationchallenge2020. InNeurIPS2020Competitionand
DemonstrationTrack,2021.
12UK Information Commissioner’s Office. What do we need to do to ensure lawful-
ness, fairness, and transparency in AI systems? 2022. URL https://ico.org.uk/
for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/
guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/.
NicolasUsunier,VirginieDo,andElvisDohmatob. Fastonlinerankingwithfairnessofexposure. In
ACMConferenceonFairness,Accountability,andTransparency,2022.
BorisvanBreugelandMihaelavanderSchaar. Whytabularfoundationmodelsshouldbearesearch
priority. InternationalConferenceonMachineLearning,2024.
Boris van Breugel, Nabeel Seedat, Fergus Imrie, and Mihaela van der Schaar. Can you rely on
yourmodelevaluation? improvingmodelevaluationwithsynthetictestdata. AdvancesinNeural
InformationProcessingSystems,2024.
MichaelVealeandReubenBinns.Fairermachinelearningintherealworld:Mitigatingdiscrimination
withoutcollectingsensitivedata. BigData&Society,2017.
Irina-Elena Veliche and Pascale Fung. Improving fairness and robustness in end-to-end speech
recognitionthroughunsupervisedclustering. InInternationalConferenceonAcoustics,Speech
andSignalProcessing,2023.
JunWang,WeinanZhang,andShuaiYuan. Displayadvertisingwithreal-timebidding(RTB)and
behaviouraltargeting. FoundationsandTrendsinInformationRetrieval,11(4-5):297–435,2017.
ZeyuYang,PeikunGuo,KhadijaZanna,andAkaneSano.Balancedmixed-typetabulardatasynthesis
withdiffusionmodels. InternationalConferenceonMachineLearning,2024.
MuhammadBilalZafar,IsabelValera,ManuelGomezRodriguez,andKrishnaPGummadi. Fair-
ness beyonddisparate treatment& disparateimpact: Learning classificationwithout disparate
mistreatment. InInternationalConferenceonWorldWideWeb,2017.
RichZemel,YuWu,KevinSwersky,ToniPitassi,andCynthiaDwork. Learningfairrepresentations.
InInternationalConferenceonMachineLearning,2013.
DanielZhang,SaurabhMishra,ErikBrynjolfsson,JohnEtchemendy,DeepGanguli,BarbaraGrosz,
TerahLyons,JamesManyika,JuanCarlosNiebles,MichaelSellitto,etal. TheAIindex2021
annualreport. arXivpreprintarXiv:2103.06312,2021.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like
shopping: Reducing gender bias amplification using corpus-level constraints. Conference on
EmpiricalMethodsinNaturalLanguageProcessing,2017.
13Appendix
WefirstlydescribeindetailhowthedatasetFairJobwascollected,thenprovidealltheinformationon
thecontextandfeatureswithitsstatistics. Further,weperformexperimentsandprovideallthesteps
forthesakeofreproducubility. Thedatasetishostedathttps://huggingface.co/datasets/criteo/FairJob.
Sourcecodefortheexperimentsishostedathttps://github.com/criteo-research/FairJob-dataset/.
Authorstatementofresponsibility. AuthorsandCriteobearallresponsibilityincaseofviolation
ofrightsandconfirmationofthedatalicense.
A Datasetinformation
A.1 Datacollectionanduse
AsillustratedinFigure6,theprocessstartswithusersnavigatingPublisherandAdvertiserwebsites
(typically newspapers and retailer shops respectively). Upon user consent10, user information
abouttheeventssuchasvisitsorproductviews11arecollectedandidentifiedbymeansofbrowser
cookies. Users are subject to personalized advertising (if the job campaign was chosen and the
displayopportunitywaswon)untiltheendofthedatacollectionperiod. Subsequently,onlywon
displayscomingfromPublisherandAdvertiserpartnersarejoinedbycookieidentifierontheAdTech
platformtoformtherawdataset,droppingcookieidswhentheyarenotneededanymore. Toensure
confidentiality,thedatahasbeensub-samplednon-uniformlytoavoiddisclosingbusinessmetrics.
Featurenameshavebeenanonymized,andtheirvaluesrandomlyprojectedtopreservepredictive
powerwhilerenderingtherecoveryoftheoriginalfeaturesorusercontextpracticallyimpossible12.
Thedatasetdoesnotcontaintherelevantattributessuchasgender; however,itincludesagender
proxywhichwediscussindetailinthemaintext.
Figure6: Datacollectionandprocessingoverview.
Licenseandintendeduse. ThedataisreleasedundertheCC-BY-NC-SA4.0licensewhichgives
libertytoShareandAdaptthisdataprovidedthattherespectoftheAttribution,NonCommercial
and ShareAlike conditions. We focus in this paper on algorithmic fairness analysis as a specific
use-case,butitdoesnothavetorestrictedtothis. Forexample,thedatasetcanbeusedasabaseline
forimprovingdeeplearningmodelsontabulardataandtostudymethodsonembeddingcreation
for numerical and categorical features as done by Gorishniy et al. [2021, 2022], Grinsztajn et al.
[2022],Shwartz-ZivandArmon[2022],Matteuccietal.[2023]. Anotherpossibleusageistoexplore
privacy-preservingtechniquesfortabulardataasinDonhauseretal.[2024]. Additionally,thisdataset
canbeusedtoimprovethegenerationtechniquesontabulardata,asdescribedin[Jesusetal.,2022,
vanBreugelandvanderSchaar,2024]. WecompareFairJobdatasetdetailstoothercommonlyused
10InaccordancewithGDPRandArticle82oftheDataProtectionActthatrequiretheprovidertoaskconsent
ofdatasubjectsifitwasreading/writinginformationtotheuser’sdevice.
11Theoriginaldatadoesnotcontainany"specialcategories"ofpersonaldatalistedunderArticle9ofthe
GDPR,processingofwhichisprohibited,exceptinlimitedcircumstancessetoutinArticle9oftheGDPR.
12TheorginaldataisstillusedintheAdTechcompany,however,re-indentificationofthepseudomymized
dataisimpossibleduetoadditionalrandomizationtechniquesduringdataanonymizationtocomplywithArticle
4(5)ofGDPR.
14open-sourcetabulardatasetsfurtherinSectionD.Andlastly,aswediscussedinthemaintext,FairJob
datasetspresentsachallengeoftrainingfairandrobustmodelsonstronglyimbalanceddata[Jesus
etal.,2022,Yangetal.,2024].
A.2 Datasetdetaileddescription
Thedatasetcontainspseudononymizedusers’contextandpublisherfeaturesthatwascollectedfrom
ajobtargetingcampaignranfor5monthsbyCriteoAdTechcompany. Eachlinerepresentsaproduct
thatwasshowntoauser. Eachuserhasanimpressionsessionwheretheycanseeseveralproducts
atthesametime. Eachproductcanbeclickedornotclickedbytheuser. Thedatasetconsistsof
1072226rowsand55columns:
• user_idisauniqueidentifierassignedtoeachuser. Thisidentifierhasbeenanonymized
anddoesnotcontainanyinformationrelatedtotherealusers.
• product_idisauniqueidentifierassignedtoeachproduct,i.e. joboffer.
• impression_idisauniqueidentifierassignedtoeachimpression,i.e. onlinesessionthat
canhaveseveralproductsatthesametime.
• cat0,...,cat5areanonymizedcategoricaluserfeatures.
• cat6,...,cat12areanonymizedcategoricalproductfeatures.
• num13,...,num47areanonymizednumericaluserfeatures.
• protected_attributeisabinaryfeaturethatdescribesusergenderproxy,i.e. female
is0,maleis1. Thedetaileddescriptiononthemeaningcanbefoundinthemainpaper.
• seniorisabinaryfeaturethatdescribestheseniorityofthejobposition,i.e. anassistant
roleis0,amanagerialroleis1. Thisfeaturewascreatedduringdataprocessingstepfrom
theproducttitlefeature: iftheproducttitlecontainswordsdescribingmanagerialrole(e.g.
’president’,’ceo’,andothers),itisassignedto1,otherwiseto0.
• rankisanumericalfeaturethatcorrespondstothepositionalrankoftheproductonthe
displayforgivenimpression_id. Usually,thepositiononthedisplaycreatesthebiaswith
respecttotheclick: lowerrankmeanshigherpositionoftheproductonthedisplay.
• displayrandomisabinaryfeaturethatequals1ifthedisplaypositiononthebannerofthe
productsassociatedwiththesameimpression_idwasrandomized. Theclick-rankmetric
shouldbecomputedondisplayrandom = 1toavoidpositionalbias.
• click is a binary feature that equals 1 if the product product_id in the impression
impression_idwasclickedbytheuseruser_id.
Figure8illustratesdistributionsofsomefeatures:numberofimpressionsperuser,numberofproducts
peruserandbannersizehavelongtailphenomenon(plotsoftheupperrow). Theproductshave
popularitybias(lowerplot),i.e. someproductshavemuchhigherorlowerthanaveragenumberof
clickswithseniorjobadshavingmoreclicksonaverage,andpositionbias,i.e. increasednumberof
clicksperlowerrankandalmostnoclicksinthehighestranks(rightplotonthelowerrow).
Figure7representsthefeatureimportanceaccordingtoanimportancegainofXGBoosttrainedin
two different ways: (i) unaware without protected attribute as a feature and (ii) unfair way with
protectedattributeasafeature. TheirperformanceisreportedinTable8. Wenoticethatthefeature
rankhasahighimportance,however,itgetsreplacedbytheprotected_attributefeatureinthe
unfairregime. Theremostimpactfulfeaturesforbothmodelsarenum23,num33,num43,num18and
num19,apartfromrankfortheunawaremodelandprotected_attributefortheunfairone.
InFigures9and10weplotcorrelationmatrixbetweennumericalfeaturesintheoriginaldatasetand
open-sourceddatasetFairJob,whereweaddednoiseforanonymization. Weobservethatcorrelations
aregenerallyslightlyweakerformostfeatures,whenobservingtheanonymizeddata,thatcanbe
betterseeninFigure11,whereweplotdifferencebetweenthecorrelations. Thisfactcomesnaturally
fromtheaddednoise,however,lowercorrelationvaluesmighttranslateintohigherclassification
difficulty.
15Table1: Categoricalfeaturescardinalities.
feature cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10 cat11 cat12
cardinality 9 9 1025 98 122 1296 2492 3183 3541 2879 2314 1436 912
Table2: Statisticsforindexfeatures.
feature user_id impression_id product_id
cardinality 30361 224898 57355
Table3: Statisticsforbinaryfeatures(outof1072226rows).
feature protected_attribute senior displayrandom click
positive 536113(50%) 713659(66.6%) 105869(9.9%) 7489(0.7%)
400
cat
300 num
200
100
0
400
300
200
100
0
d s r u i p 0 1 2 3 4 5 6 7 8 91011121314151617181920212223242526272829303132333435363738394041424344454647a
Feature labels
Figure7: XGBoostimportancegainperfeaturewhentrainedinunawareway(withoutprotectedat-
tributeasafeature–upperplot)andunfairway(withprotectedattributeasafeature–lowerplot).La-
beldcorrespondstodisplayrandom,stosenior,rtorank,utouser_id,itoimpression_id,
ptoproduct_id,labels0to47tocategorical(cat0,...,cat12)andnumerical(num13,...,num47)
features,respectively,andonlyonthelowerplotatoprotected_attribute.
A.3 Datasetstatistics
Table 1, 2 and 3 demonstrate the available statistics for categorical features cardinalities, index
featuresandbinaryfeaturesrespectively. Additionally,weprovidestatisticsforselectionbiasofthe
jobcampaignthatcomesfromtheCriteoAdTechcompany(outsideofFairJobdataset)inTable4,
whichwetakeintoaccountfurthertocomputeutilitymetrics.
Table4: Statisticsforjobcampaignselectionbias: weseethatwhilefemale(internet)-population
isslightlylargerthanmale,thefemalepopulationinthejobcampaignismuchsmaller,leadingto
selectionbiasofthejobcampaign–maleusersarealmosttwicemorepickedthanfemaleusers.
population advertising jobcampaign ratio
female 53.6% 39.2% 0.73
male 46.4% 60.7% 1.31
Weprovidethedetailedstatisticsonclicks,jobseniorityandtheprotectedattributeinthedatasetin
Table5. Fromthistablewecancomputeprobabilitiesofevents. Forexample,Table6showsthat
ifajobadisaboutaseniorposition,thereisslightlyhigherchancethatitwillbeshowntoamale
16
niag
ecnatropmi
BGX
Unaware
Unfair60000
9000
4000
40000
6000
2000 20000
3000
0 0 0
1 10 20 30+ 1 10 20 30 50+ 1 5 10 15 20+
Number of impressions per user Number of products per user Banner size
50
Job seniority
40 0
1 2000
30
20
1000
10
0 0
0 1 2 3 0 10 20
Log of product frequency Number of clicks per rank
Figure8: ExamplesofsomefeaturestatisticsinFairJobdataset: numberofimpressionsperuser,
numberofproductsperuserandbannersizehavelongtailphenomenon(plotsoftheupperrow). The
productshavepopularitybias(leftplotonthelowerrow),i.e. someproductshavemuchhigheror
lowerthanaveragenumberofclickswithseniorjobadshavingmoreclicksonaverage. Weobserve
apositionbiasduetoincreasednumberofclicksperrankandalmostnoclicksinthehighestranks
(rightplotonthelowerrow).
num16
num17
num18
num19
num20
num21
num22
num23
num24
num25
num26
num27 1.00
num28
num29
num30 0.75
num31
num32
num33 0.50
num34
num35
0.25
num36
num37
num38 0.00
num39
num40
num41
num42
num43
num44
num45
num46
num47
num48
num49
num50
Figure9: CorrelationsbetweennumericalfeaturesintheFairJobdataset.
userthanfemale(by3.36%),whileanassistantjobadismorelikelytobeshowntoafemaleuser
thanmale(by6.68%). Incontrast,ascanbeseeninTable7,thereisalmostnodifferenceinclicks
forsenioradsgiventheprotectedattribute(both0.49%)andajobaddisslightlymorelikelytobe
clickedifshowntoafemaleuser(by0.06%).
17
61mun 71mun 81mun
skcilc
fo
rebmuN
91mun 02mun 12mun 22mun 32mun 42mun 52mun 62mun 72mun 82mun 92mun 03mun 13mun 23mun 33mun 43mun 53mun 63mun 73mun 83mun 93mun 04mun 14mun 24mun 34mun 44mun 54mun 64mun 74mun 84mun 94mun 05munnum16
num17
num18
num19
num20
num21
num22
num23
num24
num25
num26
num27 1.00
num28
num29
num30 0.75
num31
num32
num33 0.50
num34
num35
0.25
num36
num37
num38 0.00
num39
num40
num41
num42
num43
num44
num45
num46
num47
num48
num49
num50
Figure10: Correlationsbetweennumericalfeaturesintheoriginaldataset.
num16
num17
num18
num19
num20
num21
num22
num23
num24
num25
num26
num27
num28
num29
num30
num31 0.02
num32
num33
num34
num35 0.01
num36
num37
num38
num39
num40 0.00
num41
num42
num43
num44
num45
num46
num47
num48
num49
num50
Figure11:DifferenceincorrelationsbetweennumericalfeaturesintheoriginalandFairJobdataset.
B Experiments
B.1 Metrics
Fairnessmetrics. Tomeasurefairnessofthemodel,weconsiderdemographicparityonsenior
jobopportunitiesthatcomputestheaveragedifferenceofpredictionsgiventheprotectedattributeand
18
61mun
61mun
71mun
71mun
81mun
81mun
91mun
91mun
02mun
02mun
12mun
12mun
22mun
22mun
32mun
32mun
42mun
42mun
52mun
52mun
62mun
62mun
72mun
72mun
82mun
82mun
92mun
92mun
03mun
03mun
13mun
13mun
23mun
23mun
33mun
33mun
43mun
43mun
53mun
53mun
63mun
63mun
73mun
73mun
83mun
83mun
93mun
93mun
04mun
04mun
14mun
14mun
24mun
24mun
34mun
34mun
44mun
44mun
54mun
54mun
64mun
64mun
74mun
74mun
84mun
84mun
94mun
94mun
05mun
05munTable5: Observedfrequenciesforclicks,seniorjobads,andprotectedattribute
notclicked clicked
non-senior senior non-senior senior all
female 189982 342221 1274 2636 536113
male 166394 366140 917 2662 536113
all 356376 708361 2191 5298 1072226
Table6: Observedfrenquenciesforuser’sprotectedattributegiventheseniorityoftheshownjobad.
non-senior senior
female 0.533390 0.483224
male 0.466610 0.516776
Table7: Observedjointfrenquenciesforclicks,seniorjobads,conditionalontheprotectedattribute.
notclicked clicked
non-senior senior non-senior senior
female 0.354369 0.638337 0.002376 0.004917
male 0.310371 0.682953 0.001710 0.004965
thejobseniority:
(cid:16) (cid:17) (cid:16) (cid:17)
DP(Yˆ|A)=E Yˆ|A=1,S =1 −E Yˆ|A=0,S =1 . (6)
Demographicparityrequiresequalproportionofpositivepredictionsineachgroup("NoDisparate
Impact")whichinourcasecanbetranslatedassameproportionofshownseniorjobadsineach
groupoftheprotectedattribute. DemographicparitycanbethoughtofasastrongerversionoftheUS
EqualEmploymentOpportunityCommission’s"four-fifthsrule",whichrequiresthatthe“selection
rateforanyrace,sex,orethnicgroup[mustbeatleast]four-fifths(4/5)(oreightypercent)oftherate
forthegroupwiththehighestrate13.
Equalizedodds[Zafaretal.,2017]ensuresthatamachinelearningmodelworksequallywellfor
differentgroups. Incaseofjobads,equalizedoddswouldforcethepredictionofbothpositiveand
negativeoutcomestobethesamewhichmightgeneratemorefalsepositivepredictionsforonegroup
versusothers,resultinginworseoutcome. Equalofopportunity[Hardtetal.,2016]ensuresthata
machinelearningmodelworksequallywellonlyonpositiveoutcomesfordifferentgroupswhich
resultsincapturingthecostsofmissclassificationdisparities.
Performancemetrics. Thelossfunctionislog-lossandreportitasNLLH(negativelog-likelihood).
WealsoreportAUC(AreaundertheROCCurve)asadescriptionofpredictionpoweronstrongly
imbalanceddatainbinaryclassificationproblems. Additionally,weconsiderclick-rankutilityU and
product-rankutilityforbiaseddataU˜,proposedinthemaintextSection3.4.
B.2 Models.
Trainingregimes. Wetrainmodelsinthefollowingways:
• unfair–themodelusesprotectedattributeasafeatureinthedata,
• unaware–themodeldoesnotusetheprotectedattributeinthedata,correspondstofairness
throughunawareness,
• fair–themodeldoesnotusetheprotectedattributeinthedataandtrainedwithanadditional
fairness-enforcingpenaltyintheloss.
13SeetheUniformGuidelinesonEmploymentSelectionProcedures,29C.F.R.§1607.4(D)(2015).
19Algorithm. Asbaselineresults,weperformthreemethodsindifferentregimes:
• Dummy–classifierbasedonasinglethresholdforpositiveclassprobability(unaware),
• XGB–XGBoostalgorithm(unfairandunaware),
• LR–LogisticRegression(unfair,unaware,andfair).
Fairnessmethods. Tostudypossibletrade-offbetweenutilityandfairness,weusein-processing
fairnessmethodsofaddingfairness-inducingpenaltytoalossfunctionduringtraining[Kamishima
etal.,2011]:
Yˆ =argmin L(Yˆ,Y)+λ·Penalty(Yˆ,Y,A). (7)
Parameter λ, or fairness_multiplier, controls the trade-off between the model’s predictive
accuracy L(Yˆ,Y) and fairness. Adjustments on λ allows to control the importance of fairness
relativetoaccuracy. Thesemethodsremovetheinfluenceofprotectedattributeonthemodel’soutput
withoutrestrictionsonthedata[Kamishimaetal.,2011,BechavodandLigett,2017,Maryetal.,
2019]. WeimplementapenaltybasedontheapproachdescribedinBechavodandLigett[2017].
B.3 Results
Baselinemodels. Wetrain(i)aDummyintheunawareregimetoobtainthefirstbaselineand(ii)
XGBintworegimes(unawareandunfair)toachieveamorereasonablebaselineperformance,see
Table8. WenoticethattheDummyclassifierisperfectlyfairwithrespecttoDPwhichisreasonable
sinceitdidnotlearnatallascanbeseenfromtheNLLHresults. However,theutilitymetricsU andU˜
ofDummydonotdiffermuchfromXGBwhichisduetoverystrongimbalanceinthedata. Weremark
thatU˜ isexpectedlyhigherforfairer(unaware)models,whileU isbetterfortheunfairmodel. There
isaslightdifferenceintermsofNLLHandAUC-ROCforXGBmodels,andhigherU correspondsto
higherAUC-ROC.
Table8: PerformancecomparisonforsinglesimulationofDummyclassifier(unaware)andXGBoost
(unawareandunfair)with100trialsfortuning.
NLLH↓ AUC↑ DP↓ U ↑ U˜ ↑
Dummyunaware 0.69239 0.50000 0.00000 0.01009 0.01245
XGBunaware 0.05491 0.75787 0.00278 0.01017 0.01276
XGBunfair 0.05736 0.76201 0.00323 0.01037 0.01236
TheXGBmodelswereusedforfeatureimportancemeasurementreportedinFigure7. Weobserved
thattheresultsreportedinFigure7forbothunfairandunawarecaseswerestablewhenchangingthe
seed.
Fairness-utilitytrade-off. Weillustratethepossibletrade-offbetweenperformanceandfairness
metrics for the logistic regression model when varying fairness_multiplier, see Figure 12.
We notice that for positive fairness_multiplier, DP improves, while NLLH degrades. For
fairness_multiplier = 0.5 and 1.0 we notice slight improvements in utility metrics, espe-
ciallyU˜,withrespecttotheunfairmodelrepresentedasadashedline.
B.4 Possibleimprovements.
Base methods. We considered XGBoost and Logistic Regression models, but in some cases
deep learning models such as Multi-Layer Perceptron, ResNets or Transformers might perform
better, especially when focusing on improving mixed embeddings of categorical and numerical
features[Gorishniyetal.,2022]. Wealsostressthechallengeofstronglyimbalancedclassification,
whichcanbefurtherinvestigated.
Fairness methods. Another way to enforce fairness during training is to use an adversarial al-
gorithm, e.g.Lahotietal.[2020]. Alternatively, thepre-processingcorrectionsarerelatedtothe
direct modifications of the training set before training. The examples of the methods include
200.0123
0.0117 U
0.0111
0.757
0.659 AUC
0.561
0.0099
0.0056 DP
0.0013
0.56
0.33 NLLH
0.10
0.0104
0.0101 U
0.0098
0.1 0.316 0.5 1 5
Fairness multiplier (log-scale)
Figure12: Thetrade-offbetweenperformanceandfairnessmetricsforthelogisticregressionmodel
whenvaryingthefairnessmultiplierfrom0.1to5.0,thevarianceisreportedbasedon10iterations.
Thedashedlinerepresentsunawarelogisticregression.
separationofobservativesX intotwosubsetsX andX [Zemeletal.,2013];trans-
desc(A) non-desc(A)
formationofX tosomeX˜ sothatitsfactualandcounterfactualdistributionsbecomesthesame,i.e.
P |do(A = a) = P |do(A = a′)foralla,a′;learning"disentangled"representations[Locatello
X˜ X˜
etal.,2019]. Thepre-processingmethodsmightbehardtoapplytolargeandcomplexdataasFairJob
andmightleadtolosingalotofdataand,therefore,performance. However,thepre-processingand
in-processingtechniquesmightbecombinedtogethertoachievebetterresults. WerefertoHortetal.
[2023]forarecentsurveyonfairnessinducingmethods.
C Reproducibility
Sourcecodefortheexperimentsishostedathttps://github.com/criteo-research/FairJob-dataset/.
Resources. ExperimentswereconductedonCriteointernalclusteroninstanceswithaRAMof
500Goand46CPUsavailableand6GPUsV100.
Tuning. Wetuneeachmodel’shyperparametersfollowingtheprocedureandthehyperparameters
search spaces from Gorishniy et al. [2022]. Namely, the best hyperparameters are the ones that
perform best on the validation set, so the test set is never used for tuning. For most algorithms,
weusetheOptunalibrary[Akibaetal.,2019]torunBayesianoptimization(theTree-Structured
ParzenEstimatoralgorithm)[Turneretal.,2021]. Wesetthenumberoftuningtrailsto50,weuse
pruning and upper bound the number of data examples for hyperparameter search to 50000 as a
compromisebetweenoptimalityandtrainingtimeconstraint,however,wetrainthemodelswiththe
optimalparametersonthewholetrainingset. Theamountofdataandnumberoftrailsfortuningcan
beincreasedforthebestperformance.
21C.1 Logisticregressiondetails
Feature embeddings. We use a PyTorch [Paszke et al., 2019] module Embedding to compute
embeddingsforcategoricalfeaturesasapartofmodeltraining. Then,wecreatedamixedembedding
layerwhereweconcatenatecategoricalfeaturesembeddingswithnumericalfeatures. Theresulted
mixedembeddinglayerisusedasamodelinput. Theembeddingsforbothcategoricalandnumerical
featurescanbeimproved,forexample,byfollowingthebenchmarkpaperofGorishniyetal.[2022].
Fixed hyperparameters. We tested different batch sizes from [1024, 4000, 10000] and fixed
batch=1024asthebestperforming. Duetostrongclassimbalance,weoversamplepositiveclass
examplesforbatchgenerationwithsamplingweightsinverselyproportionaltotheobservedclass
frequenciesinthedata,usingPyTorch[Paszkeetal.,2019]utilityWeightedRandomSampler. We
fixnumberofepochston_epochs=50.
Tunedhyperparameters. Wetunethefollowinghyperparameters:
• embedding_size = UniformInt[4, 8],
• weight_decay = LogUniform[1e-6, 1e-4],
• scheduler_step_size = UniformInt[20, n_epochs],
• scheduler_gamma = LogUniform[1e-2,1]
Fairnessparameters. Wereporttheresultsfor
• fairness_multiplierλ∈[0.0,0.1,0.316,1.0,3.0,5.0].
Foreachconfigurationweperformedhyperparametertuningdescribedabove.
Evaluation. Foreachtunedconfiguration,werun10experimentswithdifferentrandomseedsand
reporttheaverageperformanceonthetestset.
Experimentfordensityplots. Thedensityplotsillustratedinthemainpaper(Figure5)correspond
to the positive predictions densities for logistic regression model trained in unfair, unaware and
fairways. Foreasyandfastreproducibility,weupper-boundthenumberoftrainingexamplesfor
hyperparameters tuning to 100000, batch_size = 10000 and number_of_epochs = 15. The
resultsareoutputsofonesimulationoflogisticregressionmodelswiththefollowingparametersand
tunedonthefollowinghyperspaces:
• embedding_size = UniformInt[4, 5],
• learning_rate = LogUniform[1e-4, 1e-2],
• weight_decay = LogUniform[1e-6, 1e-4]
Forthemodelwithfairnesspenalty,weusepenaltycoefficientequalto3.0. Wewanttostressthat
theaimofthissimulationwastoprovideadescriptiveexampleratherthantheoptimalmodelfor
clickclassification. Intherepository,wereportthebestparametersandpredictions,aswellasthe
codefortheplotgeneration.
C.2 XGBoostdetails
Featureembeddings. WeusethePythonpackagefortheXGBoostlibrary[ChenandGuestrin,
2016],throughtheScikit-LearnAPI[Pedregosaetal.,2011]. Weencodethecategoricalfeatures
usingtheTargetEncoderinScikit-Learn,learningtheencodingonthetrainingset[Micci-Barreca,
2001].
Fixedhyperparameters. Weusethehistogram(hist)treecontructionalgorithminordertospeed
upthemodelfit. Inordertodealwiththeimbalanceoftheclasses,wesetthescale_pos_weight
parameter to the ratio between negative and positive class occurrences in the training data, as
suggestedinthelibrarydocumentation. Wefixnumberoftrialsfortunington_trials=100.
22Tunedhyperparameters. Wetunethefollowinghyperparameters:
• max_depth = UniformInt[3,10],
• min_child_weight = LogUniform[0.0001,100],
• subsample = Uniform[0.5,1],
• learning_rate = LogUniform[0.001,1],
• colsample_bytree = Uniform[0.5,1],
• reg_lambda = LogUniform[0.1,10],
• gamma = LogUniform[0.001,100]
D Comparisontoothertabulardatasets
Theshortcomingsofexistingfairness-awaredatasetsinclude:ageofthedataitself,measurementbias,
missingvalues,labelleakage,useofdatasetsforapurposetheywerenotintendedforinitially[LeQuy
etal.,2022,Hortetal.,2023]. FairJobdatasetiscollectedin2024,doesnothavemissingvaluesand
representsthereal-worldapplicationforFairAImethods.
Frombenchmarkstudiesonencodingnumericalandcategoricalfeatures[Gorishniyetal.,2021,
2022,Grinsztajnetal.,2022,Matteuccietal.,2023]andsurveysonfairness-awaretabulardatasets
[LeQuyetal.,2022,Hortetal.,2023],weextractthemostuseddatasetandcomparetoFairJobin
Table9.
Table9: ComparisonofFairJobtoothermostfrequentlyusedtabulardatasetsandmostfrequently
usedtabularfairness-awaredatasets.
name #rows #num #cat tasktype protectedattribute
COMPAS 7214 14 37 binclass sex,race
GesturePhase 9873 32 0 multiclass -
ChurnModelling 10000 10 1 binclass -
CaliforniaHousing 20640 8 0 regression income
House16H 22784 16 0 regression -
Adult 48842 6 8 binclass sex,race,gender
OttoGroupProducts 61878 93 0 multiclass -
HiggsSmall 98049 28 0 binclass -
Diabetes 101766 10 40 binclass gender
FacebookCommentsVolume 197080 50 1 regression -
SantanderCustomerTransactions 200000 200 0 binclass -
KDDCencus-Income 299285 34 7 binclass sex,race
Covertype 581012 54 0 multiclass -
FairJob 1072226 36 18 binclass gender
MSLR-WEB10K(Fold1) 1200192 136 0 regression -
E Broaderimpact
Anymachinelearningsystemthatlearnsfromdatarunstheriskofintroducingunfairnessindecision
making.Recentresearch[Speicheretal.,2018,LambrechtandTucker,2019,Andreouetal.,2019,Ali
etal.,2019]hasidentifiedfairnessconcernsinseveralAIsystems,especiallytowardprotectedgroups
thatareunder-representedinthedata. Thus,alongsidethetechnicaladvancementsinimprovingAI
systemsitcrucialthatwealsofocusonensuringthattheyworkforeveryone.
One of the key practical challenges in addressing unfairness in AI systems is that most methods
requireaccesstoprotecteddemographicfeatures,placingfairnessandprivacyintension. Wework
towardaddressingtheseimportantchallengesbyproposinganewbenchmarkingdatasettoimprove
worst-caseperformanceofprotectedgroups,intheabsenceofprotectedgroupinformationbutwitha
proxyvariable.
23Onelimitationofmethodsinthisspaceisthedifficultyofevaluatingtheireffectivenesswhenwedo
nothavedemographicsinarealapplication. Therefore,whilewethinkdevelopingbetterdebiasing
methodsiscrucial,thereremainsfurtherchallengesinevaluatingthem.
24