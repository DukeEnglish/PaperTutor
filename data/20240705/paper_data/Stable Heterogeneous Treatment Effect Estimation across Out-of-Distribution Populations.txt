Stable Heterogeneous Treatment Effect Estimation
across Out-of-Distribution Populations
Yuling Zhang∗, Anpeng Wu‡§, Kun Kuang‡, Liang Du¶, Zixun Sun¶, and Zhi Wang∗†
∗Tsinghua Shenzhen International Graduate School, Tsinghua University
†Tsinghua-Berkeley Shenzhen Institute, Tsinghua University
‡College of Computer Science and Technology, Zhejiang University
§Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence
¶Interactive Entertainment Group, Tencent
Email: zhangyl21@mails.tsinghua.edu.cn, {anpwu,kunkuang}@zju.edu.cn,
{lucasdu,zixunsun}@tencent.com, wangzhi@sz.tsinghua.edu.cn
Abstract—Heterogeneous treatment effect (HTE) estimation is Observational Data Imbalanced Treatment Assignment
vital for understanding the change of treatment effect across
individuals or subgroups. Most existing HTE estimation meth-
ods focus on addressing selection bias induced by imbalanced
distributions of confounders between treated and control units,
but ignore distribution shifts across populations. Thereby, their
C1. Selection Bias
applicability has been limited to the in-distribution (ID) popula-
tion,whichsharesasimilardistributionwiththetrainingdataset.
In real-world applications, where population distributions are
subjecttocontinuouschanges,thereisanurgentneedforstable Weekend Village
HTE estimation across out-of-distribution (OOD) populations,
which,however,remainsanopenproblem.Aspioneersinresolv-
ing this problem, we propose a novel Stable Balanced Represen-
tation Learning with Hierarchical-Attention Paradigm (SBRL-
HAP) framework, which consists of 1) Balancing Regularizer Cloudy Urban Coastal
for eliminating selection bias, 2) Independence Regularizer for
addressing the distribution shift issue, 3) Hierarchical-Attention
Paradigm for coordination between balance and independence.
Summer Large-scale
Inthisway,SBRL-HAPregressescounterfactualoutcomesusing
C2. Distribution Shifts
ID data, while ensuring the resulting HTE estimation can be
across Various Populations
successfully generalized to out-of-distribution scenarios, thereby
enhancingthemodel’sapplicabilityinreal-worldsettings.Exten-
Fig. 1. Two main challenges in stable HTE estimation across OOD popu-
sive experiments conducted on synthetic and real-world datasets lations:(C1)selectionbiasfromimbalancedtreatmentassignment,and(C2)
demonstrate the effectiveness of our SBRL-HAP in achieving distributionshiftacrossvariouspopulations.Theformerismanifestedasim-
stableHTEestimationacrossOODpopulations,withanaverage balanceddistributionsofcovariates(e.g.,age)betweentreated(i.e.,T=1)and
10% reduction in the error metric PEHE and 11% decrease in control(i.e.,T=0)unitsinaspecificpopulation.Thelatteroccursfrequentlyin
the ATE bias, compared to the SOTA methods. real-worldapplications,resultinginout-of-distributionpopulationsthathave
distinctcovariatedistributionsfromthetrainingdataset.Thisworkisamong
Index Terms—Heterogeneous Treatment Effect; Out-of-
thefirsttosynergisticallyresolvebothselectionbiasanddistributionshift.
Distribution; Balanced Representation Learning; Hierarchical-
Attention Optimization
I. INTRODUCTION distributions between treated and control units (Top panel in
Fig. 1). Taking healthcare as an example, in the study of
Estimating Heterogeneous Treatment Effects (HTE) from
the effect of treatment on outcomes, physicians may assign
observational data has gained increasing importance across
different treatment recommendations (e.g., taking the drug or
various fields [1], including medicine, economics, and mar-
not)basedonthepatient’sindividualcircumstances(e.g.,age).
keting[2]–[5].Thiscanprovidepractitionersvaluableinsights
Typically, physicians recommend young individuals to take
into understanding how treatment effects vary among differ-
the medication more often while advising older individuals
ent subpopulations, ultimately achieving personalized health-
not to take it. Such imbalanced treatment assignment can
care and explainable decision-making. However, reliable and
result in selection bias [6], manifested as differences in age
robust estimation of HTE still faces significant challenges.
distribution between the treated group and the control group.
One primary challenge in observational data is non-random
As selection bias has been taken seriously by academia and
treatment assignment, which can lead to imbalanced covariate
industry [7]–[10], various methods such as propensity score
KunKuangandZhiWangarethecorrespondingauthors. matching, doubly robust, stratification, inverse probability of
4202
luJ
3
]GL.sc[
1v28030.7042:viXra
selpmaS
fo
rebmuNtreatment weighting (IPTW) [11]–[14], and representation relation fundamentally stems from the statistical dependence
learning methods [15]–[22] have been developed to reduce between relevant and irrelevant features [38]–[40]. Therefore,
selection bias and estimate treatment effects more accurately. to address distribution shift and maintain performance across
However, one limitation is that these methods have only OOD data, SL methods propose to decorrelate all features by
beentestedandvalidatedondatathatissimilartothetraining samplereweighting,facilitatingmodelstorecognizestableand
data,knownasin-distribution(ID)data.Inreal-worldapplica- invariant relationships between features and outcomes.
tions, where data or population distributions, specifically the Building upon these methods, we propose a novel frame-
covariatedistributions,aresubjecttocontinuouschanges[23]– work called Stable Balanced Representation Learning with
[27], there is a concern regarding the performance of these Hierarchical-Attention Paradigm (SBRL-HAP), which com-
methods when applied to populations with different covariate prises three core components: (a) Balancing Regularizer (BR)
distributions compared to the training dataset [28]–[32]. This to eliminate selection bias and obtain balanced representa-
issue, referred to as distribution shift [33], [34], has posed an- tions; (b) Independence Regularizer (IR) to reweight samples
othersignificantchallengetoachievingstableHTEestimation and enforce independencies between features, addressing the
forout-of-distribution(OOD)populations.AsshowninFig.1, distribution shift issue; (c) Hierarchical-Attention Paradigm
the distribution of patients’ circumstances may change over (HAP) to assign distinct priorities to each neural network
time,seasons,holidays,urbanandruralareas,etc.,resultingin layers for comprehensive feature decorrelation throughout the
the emergence of various populations. These populations may learning process. Notably, in the training process of BR and
have different data distributions and characteristics compared IR, the learning of balanced representation and independence-
tothetrainingdata,andtheymayevenincludeindividualsthat driven weights can be interdependent. For instance, when
werenotpresentinthetrainingdata.Duetoinductionbias,the representations change, the learning of weights would also
causal relations learnt from training data (e.g., data collected adaptaccordingly.Insuchcases,optimizingoneobjectivemay
during weekdays) are typically not applicable to testing data come at the expense of the other. Consequently, we design
(e.g., data collected during weekends). If we directly use a Hierarchical-Attention Paradigm to synergistically facilitate
the above causal models trained on one specific dataset, it the learning of balanced representations and independence-
may lead to unstable and unreliable HTE estimation for other driven weights, thereby alleviating conflicts. To differentiate,
populations. Such unreliability of HTE estimation can lead we refer to the model without the Hierarchical-Attention
to inappropriate treatment choices, posing a huge threat to Paradigm as SBRL.
patients’ health and even resulting in catastrophic medical The primary contributions of this paper are threefold:
events.Therefore,thereisanurgentdemandtodevelopstable
• In this paper, we first investigate the problem of stable
HTE estimation methods that can effectively generalize to
heterogeneous treatment effect estimation across out-of-
unseen samples or different populations.
distribution populations and pioneer the integration of
In this paper, we first study the problem of stable HTE
representation balancing and stable training techniques.
estimation across OOD populations, and systemically review
• We propose a novel SBRL-HAP framework in which
the two main challenges (Fig. 1): (C1) selection bias from
the Hierarchical-Attention Paradigm eliminates selection
imbalanced treatment assignment, and (C2) distribution shift
biasandaddressesdistributionshiftsthroughcomprehen-
acrossvariouspopulations.Selectionbiasinobservationaldata
sive decorrelation in a hierarchical manner. This flexible
can lead to unreliable and biased HTE estimation. Although
framework enables the extension of any existing repre-
many previous causal methods have been proposed to elimi-
sentation balancing method to various OOD populations.
nate selection bias, they still suffer from the distribution shift
• Extensive experiments conducted on synthetic and real-
issue,resultinginahighererrorandunstableestimatesofHTE
world data demonstrate the effectiveness of our SBRL-
on out-of-distribution populations.
HAP in achieving stable HTE estimation across OOD
To address the selection bias, Balanced Representation populations, compared to the SOTA methods. On the
Learning (BRL) has been developed to map the original OOD datasets, our SBRL-HAP reduces the error met-
covariates to a representation space and narrow the represen- ric PEHE by 10% on average compared with the best
tation discrepancies across different treatment arms [15]–[17]. baseline, and reduces the ATE bias by up to 14%.
This approach enables accurate HTE estimation within the
in-distribution data. Nevertheless, in the presence of distribu- II. RELATEDWORK
tion shifts across various populations, the problem of stable
HTE estimation across OOD populations remains relatively
Representation Balancing to Mitigate Selection Bias.
unexplored.AmongthemanyOODgeneralizationalgorithms, Many prior works have concentrated on addressing the chal-
StableLearning(SL)standsoutasapromisingapproach[35]– lenges of estimating heterogeneous treatment effects from
[37] based on the following observation. For general machine observational data while mitigating selection bias, with a
learning models, model crashes under distribution shifts are promisingmethodbeingbalancedrepresentationlearning[14],
mainly caused by the unstable correlation between irrelevant [41].Thismethodminimizesthedistributiondistancebetween
features and the target outcome. This kind of unstable cor- treated and control groups, effectively balancing confounders
andproducingsimilardistributionsintherepresentationspace,ultimately improving prediction accuracy for heterogeneous weights and representations can interfere with each other,
treatment effects. Specifically, representation balancing meth- which is the reason why few works have discussed Stable
ods can be broadly categorized into five groups: 1) Funda- Estimation in HTE across data. Considering the increasing
mentalmethods,suchasCFR[15],[42],whichlearnbalanced importance of stable HTE estimation, this work pioneers to
representationbydirectlyminimizingdistributiondistancebe- propose a novel framework named SBRL-HAP, in which
tweenthetreatedandcontrolgroups;2)Reweightingmethods, theHierarchical-AttentionParadigmcoordinatestheBalancing
such as RCFR [43] and CFR-ISW [16], which incorporate Regularizer and Independence Regularizer to extract balanced
information from treatments and use importance sampling and stable representations, thus bridging these two topics.
techniques to further mitigate the negative impact of selection
III. PROBLEMSETUPANDASSUMPTIONS
bias; 3) Similarity-based methods, such as SITE [22] and
ACE [21], which focus on learning balanced representations A. Problem Setup
while preserving similarity information among data points; In this paper, we study the heterogeneous treatment effect
4) Subgroup methods, such as HNN [44] and SCI [20], estimation across multiple populations. For simplicity, we
which enhance the model’s predictive ability by identifying consider that the population used for training the model is
and partitioning sub-spaces within the representation; and 5) drawn from environment e ∈ E and the target population
Decomposition methods, such as DR-CFR [18] and DeR- is from environment e′ ∈ E. Taking healthcare as an ex-
CFR [17], which separate confounders from pre-treatment ample, as illustrated in Fig. 1, we gather an observational
variables to achieve precise balancing of covariates. These De ={Xe,Te,Ye}={xe,te,yti,e}n from urban hospitals
i i i i=1
methodshaveprovensuccessfulinestimatingtreatmenteffects represented by the environment e, where xe ∈X denotes the
i
without taking distribution shifts into account, but they may covariates (e.g., patients circumstances), te ∈ {0,1} denotes
i
be prone to performance degradation in OOD scenarios. the received treatment (e.g., take drug or not), and yti,e ∈ Y
i
Stable Learning to Eliminate Distribution Shifts. Dis- is the observed outcome corresponding to the treatment te.
i
tribution shifts across distinct populations in HTE estimation Then, in the target environment e′ ∈E different from e, such
are not as well explored, and stable learning is a promising as a remote village, we have a potential population denoted
approach to address the distribution shift issue [35]. Tak- as De′ = {Xe′}. This dataset only includes the covariates
ing inspiration from variable balancing strategies in causal x of the individuals in the target population, without the
inference [45]–[47], stable learning eliminates dependence corresponding treatment or outcome information. Our goal is
among covariates via sample reweighting to manifest cau- to learn a causal model from the dataset De which enables
sation, thus utilizing the stability of causation to achieve accurate HTE estimations for the target populations from
generalization. Recently, several studies, including [39], [48]– different environments e′ ∈ E. We refer to this problem
[50],haveaimedtotacklethediscrepancybetweenthetraining as Heterogeneous Treatment Effect Estimation across Out-of-
and testing distribution stemming from datasets collected at Distribution Populations.
different time periods or platforms. These approaches have Our work focuses on the Heterogeneous Treatment Effect
the potential to handle distribution shifts in HTE estimation. at the individual level, i.e., Individual Treatment Effect (ITE),
Among them, CRLR [48] addresses distribution shifts by and Average Treatment Effect (ATE) at the population level.
simultaneously optimizing global confounder balancing and
Definition 3.1 (Individual Treatment Effect). Given any envi-
weighted logistic regression to estimate the causal effect of
ronment e∈E, the Individual Treatment Effect of unit i is:
eachvariableontheoutcome.However,CRLRrequiresthatall
the features and labels be binary, which is impractical in real- ITEe =y1,e−y0,e, (1)
i i i
world applications. To overcome this limitation, DWR [49]
where y1,e and y0,e are potential outcomes.
proposes to utilize the statistical independence condition to i i
force that variables are independent of each other, thereby Definition 3.2 (Average Treatment Effect). Given any envi-
relaxing the binary restriction. Furthermore, SRDO [50] con- ronment e∈E, the Average Treatment Effect of De is:
structs an uncorrelated design matrix from original covariates n
1 (cid:88)
to alleviate the issue of co-linearity among variables. On the ATEe =E[Y1,e−Y0,e]= (y1,e−y0,e). (2)
n i i
otherhand,StableNet[39]goesbeyondthelinearcaseandad- i=1
dresses both linear and non-linear dependencies between vari- B. Assumptions
ablesusingRandomFourierFeaturesandtheHilbert-Schmidt
Given the training data De = {Xe,Te,Ye} from environ-
Independence Criterion. Overall, stable learning techniques
mente,ourgoalistofindaregressorf(·):X×T →Y capa-
aim to realize model generalization across any distribution by
ble of precisely predicting potential outcomes across different
excavating stable relationships through feature decorrelation.
OOD environments e′ ∈E. To eliminate the selection bias in
Although Representation Balancing [15] and Stable Learn- De,existingcausalmodelsrelyonstandardassumptions[51].
ing[39]canaddressSelectionBiasfromimbalancedtreatment
assignmentanddistributionshiftacrossdatarespectively,their Assumption 3.1 (Stable Unit Treatment Value). The distri-
optimization objectives are not orthogonal. The learning of bution of the potential outcome of one unit is assumed to be
independent of the treatment assignment of another unit.Assumption 3.2 (Unconfoundedness). The distribution of tlestheconflictbetweenbalanceandindependenceinaholistic
treatment is independent of the potential outcome when given and hierarchical manner.
covariates. Formally, T⊥(Y0,Y1)|X.
IV. METHODOLOGY
Assumption 3.3 (Overlap). Everyunitshouldhaveanonzero
In this section, we propose SBRL-HAP to stably esti-
probability to receive either treatment status. Formally, 0 <
mateheterogeneoustreatmenteffectsacrossOODpopulations.
p(T =1|X)<1.
Firstly, we will present the overall framework of our SBRL-
Additionally, without any prior knowledge or structural HAP. Subsequently, we will offer a thorough description of
assumptions, it is impossible to figure out the distribution threecomponentsofSBRL-HAP.Finally,wewilldemonstrate
shiftproblem,sinceonecannotcharacterizetherareorunseen the end-to-end optimization and training strategies.
latent environments [52]. Thereby, we follow the assumption Fig. 2 depicts the overall architecture of our SBRL-HAP
commonlyusedinstudiesofdistributionshift[49],[52],[53]. whichconsistsofthreecomponentsforstableHTEestimation:
Assumption3.4(StableRepresentation). Thereexistsastable • Balancing Regularizer (BR) employs Integral Probabil-
representation Ψs(X) of covariates X such that for any ity Metrics (IPM) [54], [55] to measure the distribution
environment e∈E, E[Y,T|Xe]=E[Y,T|Ψs(Xe)] holds. discrepancy between the treated and control group, and
proposes to adopt a model-free method to narrow the
This assumption implies covariates X include two parts: distribution discrepancy, so as to eliminate selection bias
relevant features having causal effects on outcome Y, known and obtain balanced representations.
as stable features X S; And irrelevant features (i.e., unstable • Independence Regularizer (IR) learns sample weights
features X V) that have Pe(Y|X V) ̸= Pe′(Y|X V) and cre- to remove non-linear dependencies between features by
ate instability for prediction. The existence of X S provides utilizingtheHilbert-SchmidtIndependenceCriterion[56]
the possibility of precisely predicting the outcome Y using with Random Fourier Features [57], thereby facilitating
Ψs(X), which is known as the stable representation with the identification of the stable relationships between fea-
invariant relationships to the outcome Y across different tures and potential outcomes.
environments e∈E [52], [53]. • Hierarchical-Attention Paradigm (HAP) emphasizes
Challenges. Overall, we formally discuss challenges in assigning distinct priorities to each neural network layer,
stable HTE estimation across OOD populations. (C1) Selec- in order to achieve comprehensive feature decorrelation
tion bias refers to the inconsistent distribution of covariates withhierarchicalattention.Therefore,HAPharmoniously
between different treatment arms in a specific environment integrates the Balancing Regularizer and Independence
e, i.e., Pe(Xt) ̸= Pe(Xc), where Xt = {x } and Regularizer, effectively resolving the conflict between
i:ti=1
Xc = {x }. (C2) Distribution shift indicates that the balance and independence.
i:ti=0
marginaldistributionofXshiftsacrossenvironmentswhilethe Next, we will describe each component of our SBRL-
conditional distribution P(T,Y|X) remains unchanged. That HAP model in detail, and then demonstrate the end-to-end
is, ∀e,e′ ∈ E, Pe(T,Y|X) = Pe′(T,Y|X) and Pe(X) ̸= optimization and training strategy.
Pe′(X). One naive method to address selection bias and
the issue of distribution shift is to combine representation- A. Balancing Regularizer
basedmethodsandstablelearningtechniques.Tothisend,we TheBalancingRegularizerisdesignedtoeliminateselection
propose a Stable Balanced Representation Learning (SBRL) bias and obtain a balanced representation by reducing the
toestimateHTEacrossvariouspopulations.However,anovel distributiondiscrepancybetweendifferenttreatmentarmswith
challenge arises, i.e., (C3) the learning of balanced represen- a model-free method. A typical metric used for measuring
tationandindependence-drivenweightsinSBRLcanbeinter- the distribution discrepancy is the Integral Probability Metric
dependent,hencerestrictingthegeneralizationperformanceof (IPM) [54], [55], which is formally defined as
stableHTEestimation.Itshouldbenoticedthatcurrentstable
dist(P ,P )= sup|E [f(x)]−E [f(x)]|, (3)
learningtechniques,designedfortypicalpredictiontasks,learn Φc Φt
f∈F
x∼PΦc x∼PΦt
sample weights by decorrelating the last layer of the network,
where P = {Φ(x )} and P = {Φ(x )} de-
while balanced representations are required in the first half of Φc i i:ti=0 Φt i i:ti=1
note the covariate distribution of the control group and the
the network. Once the balanced representations are updated,
treated group in the representation space Φ, respectively. For
adaptive weight modification is necessary, which, however,
rich enough function families F, dist(P ,P ) = 0 ⇒
cannot guarantee feature independence for generalization. As Φc Φt
P = P holds [15]. Most previous works constrain the
a result, prioritizing the optimization of one objective may Φc Φt
IPM dist(P ,P ) by directly optimizing network param-
entail expenses in achieving the other, making it difficult to Φc Φt
eters [15], [42], thereby getting rid of selection bias. This
achieve stable HTE estimation across environments.
practicemayleadtoanoverbalancedrepresentationdiscarding
To overcome the above challenges, we propose a novel
predictive information [17].
framework named Stable Balanced Representation Learning
Therefore, we propose to adopt the sample reweighting
withHierarchical-AttentionParadigm(SBRL-HAP)whichset-
technique to reduce network dependence. Specifically, ourⅱ. Independence Regularizer
HHSSIICC
Z RFF
Prediction Loss Sample Weights
PPrreeddiiccttiivvee NNeettwwoorrkk hh
00
HH--AA
PPrreeddiiccttiivvee NNeettwwoorrkk hh 11
ⅲ. Hierarchical-Attention
Paradigm
X
Representation Network Final Loss
ⅰ. Balancing Regularizer
ΦΦ
𝑐𝑐
T IIPPMM
ΦΦ
𝑡𝑡
Fig.2. TheframeworkofStableBalancedRepresentationLearningwithHierarchical-AttentionParadigm(SBRL-HAP).SBRL-HAPconsistsofthreemodules:
i. Balancing Regularizer restricts IPM for balanced representation, ii. Independence Regularizer eliminates feature dependence measured by HSIC-RFF for
generalization,andiii.Hierarchical-AttentionParadigmdecorrelatesfeaturescomprehensivelywithahierarchyfordispellingtheinteractionbetweenbalance
andindependence.Withhighflexibility,SBRL-HAPcanbepluggedintomostbalancedrepresentationmethodsbyreplacingtheneuralnetworkbackbone.
Balancing Regularizer strives to mitigate selection bias by Therefore, HSIC with Random Fourier Features (HSIC-
learningasetofsampleweightsw=(w ,w ,...,w )∈Rn RFF) is developed as an approximation technique for HSIC,
1 2 n +
with minimizing the following balance loss L : leadingtoanotablereductionintimecomplexity.Thefunction
B
space of Random Fourier Features is:
minL B = sup|E x∼Pw [f(x)]−E x∼Pw[f(x)]|, (4)
w f∈F Φc Φt √
H ={h:x→ 2cos(wx+φ)}, (6)
where Pw ={w ·Φ(x )} and Pw ={w ·Φ(x )} RFF
Φc i i i:ti=0 Φt i i i:ti=1
denote the weighted probability distributions of covariates in
where w ∼ N(0,1) and φ ∼ U(0,2π) from normal distribu-
the representation space Φ with t=0 and t=1, respectively.
tion and the uniform distribution. Then, the statistics of HSIC
B. Independence Regularizer can be approximated as HSIC RFF:
The Independence Regularizer aims to eliminate feature (cid:13) (cid:13)2
dependencies,soastorecognizestablerepresentationsagainst
HSIC RFF(A,B)=(cid:13)C u(A),v(B)(cid:13)
F
distribution shifts. As stated in previous studies [35], [39], (cid:88)nA (cid:88)nB (cid:12) (cid:12)2 (7)
[53],thestatisticalcorrelationbetweenstablefeaturesX and
= (cid:12)Cov(u i(A),v j(B))(cid:12) ,
S
i=1j=1
unstable features X is a major cause of model failure under
V
distribution shifts, and thus, independence between variables where∥·∥
F
istheFrobeniusnorm,andC
u(A),v(B)
∈RnA×nB
can lead to more reliable and stable models. When variables isthecross-covariancematrixofrandomFourierfeaturesu(A)
are independent, alterations in one variable do not exert any and u(B) containing entries:
influence on the other variables. Thereby, the relationships
between variables and outcomes can be regarded as stable u(A)=(u (A),u (A),...,u (A)),u (A)∈H ,∀i,
1 2 nA i RFF
causation, facilitating the superior performance of models
v(B)=(v (B),v (B),...,v (B)),v (B)∈H ,∀j,
across different OOD populations.
1 2 nB j RFF
(8)
TheIndependenceRegularizeremploystheHilbert-Schmidt wheren andn denotethenumberoffunctionsfromH .
A B RFF
Independence Criterion with Random Fourier Features to The accuracy of the statistics HSIC increases as the values
RFF
measure the non-linear correlation between two variables. of n and n , defaulting to 5, become larger.
A B
HSIC is widely utilized to measure the dependency between Motivated by [38], [39], our Independence Regularizer
two random variables by comparing their representations in a coherently optimizes sample weights w by decorrelating all
Hilbert space [20], [39], [44]: features in covariates (or its representations) X ∈ Rn×m.
HSIC(A,B)=∥K −K ∥2 , (5) That is, for any two features X :,a,X :,b ∈ X, the weighted
A B HS statistics HSIC , denoted by HSICw , should be close to
RFF RFF
where K A = k A(A,A) and K B = k B(B,B) are RBF zero. Formally, for ∀X ,X ∈X,
:,a :,b
kernel matrices, and ∥·∥ is the Hilbert-Schmidt norm. If
HS
ath ne dp Ero [kduc (t Bk ,BAk )]B <is ∞cha hra oc ldte ,ri ts hti ec n, a And ⊥E[ Bk A i( fA a, nA d)] on< ly∞
if
HSICw RFF(X :,a,X :,b,w)=(cid:13) (cid:13)C uw (X:,a),v(X:,b)(cid:13) (cid:13)2
F
HSIC(AB
,B)=0.However,HSICinvolvinglarge-scalekernel
=(cid:88)nA (cid:88)nB
(cid:12) (cid:12)Cov(u i(w⊤X :,a),v j(w⊤X :,b))(cid:12) (cid:12)2 →0.
(9)
matrices is computationally expensive.
i=1j=1
𝑍 𝑜 𝑍 𝑜
Φ
𝑍 𝑟 𝑍
𝑍
𝑜
𝑜
𝑍
𝑍
𝑜
𝑜
𝑍
𝑍
𝑝
𝑝
𝑍
𝑍
𝑍
Φ
𝑝
𝑟
𝑜The corresponding loss term can be denoted as: Algorithm 1 Stable Balanced Representation Learning with
Hierarchical-Attention Paradigm
(cid:88)
L D(X,w)= HSICw RFF(X :,a,X :,b,w). (10) Input: Observational dataset De = {xe,te,yti,e}n from
i i i
1≤a≤b≤m environment e
Following prior work [38], [39], we apply the loss term Output: yˆ0, yˆ1
L D(·,·) to the last layer of the neural network Zp, and thus 1: Initialize network parameters W,b
obtaintheindependencelossofourIndependenceRegularizer 2: Initialize sample weights w←{1}n
L I =L D(Zp,w). This is done to ensure that stable represen- 3: for i=0 to I do
tations can establish the most direct mapping to the outcome. 4: CalculatelossfunctionLw withparametersW,band
Y
NotethatourBalancingRegularizerandIndependenceReg- sample weights w
ularizer are designed to handle the issue of selection bias and 5: Update W,b with gradient descent by fixing w
distribution shifts separately, which are both based on sample 6: Calculate loss function L w with sample weights w
reweighting. Therefore, we propose to directly integrate the 7: Update w with gradient descent by fixing W,b
Balancing Regularizer and the Independence Regularizer to 8: end for
achieve stable and reliable HTE estimation. This approach is
named Stable Balanced Representation Learning (SBRL).
where R = 1 (cid:80)n (w −1)2 avoids all the sample weights
w n i=1 i
C. Hierarchical-Attention Paradigm (HAP) to be zero or model only focuses on some samples and
ignores others. Besides, the value of hyper-parameters α and
Although Balancing Regularizer and Independence Regu-
{γ ,γ ,γ }allowsustoadjustthesensitivitytoselectionbias
larizer are able to solve selection bias and distribution shift 1 2 3
and distribution shift with hierarchical attention.
separately, distribution shift in HTE estimation triggers an
extra challenge, i.e., the contradiction between balance and D. Optimization and Training Procedure
dependence as stated in Section III. This challenge poses a
By optimizing the loss function L , we can acquire the
significant obstacle to reconciling the Balancing Regularizer w
optimalsampleweightsw∗ toguidethedeepneuralnetworks
andIndependenceRegularizermethods,therebymakingitdif-
to achieve stable HTE estimation across OOD data. Note that
ficult to achieve stable HTE estimates in OOD environments.
ourSBRL-HAPlearnssampleweightsregardlessofthemodel
To address this issue, we propose a Hierarchical-Attention
structure; hence, it is applicable to the backbone of nearly all
Paradigmtoformacoordinatedandunifiedobjectivefunction.
balancedrepresentationmethods.Wetakethebackboneofthe
The design of HAP stems from the following insight:
most classic balanced representation algorithm, i.e., Counter-
applying decorrelation solely to the last layer of models,
factual Regressor (CFR) [15], as an example, to illustrate our
as traditional works suggest [38], [39], would induce inter-
end-to-end training process.
action between the learning of balanced representation and
The backbone of CFR contains two sub-modules, i.e.,
independence-driven weights; one intuitive approach is to
a shared representation network (Φ(x )) for representation
uniformly enforce decorrelation for each layer throughout the i
extraction and multi-head predictive networks (h (Φ(x )) for
entire network. However, such indiscriminate constraints may ti i
potential outcome prediction.
leadtoalargevaluefortheindependencelossandarelatively
The representation network is expected to provide a bal-
smallvalueforthebalancelossterm,resultinginthedisregard
anced representation Φ(X), so as to remove distribution
of the covariate balancing objective.
discrepancies between the treated group {Φ(x )} and
Consequently, we propose to divide the entire neural net- i i:ti=1
the control group {Φ(x )} . Then, in HTE estimation,
work into three priorities: the model’s last layer Zp ∈Rn×dp i i:ti=0
to avoid the treatment information being dominated by the
with the first priority, the layer Zr ∈ Rn×dr for balanced
high-dimensional covariates, the two-head networks h (Φ)
representations Φ with the second priority and other fully t=0
and h (Φ) are adopted to predict outcomes in control and
connected layers {Zo ∈ Rn×do}l with the third priority. t=1
i i=1 treated groups, with the prediction loss L :
Then, besides the loss term L for Zp, we emphasize the Y
I
necessity of the loss terms L D(Zr,w) and L D(Zo,w) with
min L =
1 (cid:88)n
l(h (Φ(x )),yti)+R , (12)
h imie pra ar cc th oic fa ul na stt ate bn letio fn eatf uo rr est .horough removal of the negative h0,h1 Y n
i=1
ti i i l2
where R is l regularization for h, and l(·,·) encodes the
By integrating Balancing Regularizer and Independence l2 2
loss function, i.e., mean squared loss (MSE) for continuous
Regularizer with hierarchical attention, the Hierarchical-
outcome, and cross-entropy error for binary outcome.
Attention Paradigm optimizes sample weights w with the
following loss function L : To guide the above neural networks to achieve stable and
w
unbiased prediction, we propose to plug our SBRL-HAP
minL =α·L +γ ·L +γ ·L (Zr,w)+
w B 1 I 2 D module in Equation (12) by
w
(cid:88)l (11) 1 (cid:88)n
γ · L (Zo,w)+R , min Lw = w ·l(h (Φ(x )),yti)+R , (13)
3
i=1
D i w h0,h1 Y n
i=1
i ti i i l2where {w }n ∈ w∗ are the optimal sample weights learnt balanced representation methods, such as RCFR [43], CFR-
i i=1
with the loss function L . ISW [16], SITE [22], and DR-CFR [18], are built upon these
w
Ultimately, we adopt an alternating training strategy to baselines, and these methods have not exceeded the perfor-
iteratively optimize the loss function Lw for heterogeneous mance of DeR-CFR [17]. Consequently, we only combine
Y
outcome prediction and the loss function L for stable and SBRL and SBRL-HAP with TARNet, CFR, and DeR-CFR
w
balanced representations. Algorithm 1 illustrates the details of to study the performance of our methods in estimating HTE
the pseudo-code of our SBRL-HAP. across OOD populations.
V. EXPERIMENTS B. Metrics
A. Baselines Following previous work [15], [17], we adopt the Precision
in Estimation of Heterogeneous Effect (PEHE) [58] and the
In this paper, we propose two model-agnostic frameworks,
bias of ATE prediction (ϵ ) to evaluate the individual-
SBRL and SBRL-HAP1, for estimating heterogeneous treat- ATE
level and population-level performance respectively, where
ment effects across out-of-distribution environments. SBRL (cid:113)
PEHE = 1 (cid:80)n ((yˆ1−yˆ0)−(y1−y0))2 and ϵ =
can be regarded as an ablation study of SBRL-HAP with- n i=1 i i i i ATE
out HAP. In these frameworks, most existing representation |ATE−ATˆE|. Smaller values of these two metrics indicate
balancingmethodscanbeincorporatedasbackbones,because better model performance.
our methods only introduce BR, IR, and HAP as additional Besides, popular evaluation metrics for prediction tasks,
regularizerstoconstrainrepresentationlearning,withoutbeing such as F Score [59], are also adopted to assist in eval-
1
tiedtospecificmodels.Below,todemonstratetheperformance uating the model performance. We utilize the average and
of our SBRL and SBRL-HAP in improving heterogeneous stability of error [49] to evaluate the generalization perfor-
treatment effect estimation across OOD populations, we com- mance. For example, the average of F Score is defined
1
pare them to baselines and describe how we can combine as F¯ = 1 (cid:80) Fe, and the stability of F Score is
1 |E| e∈E 1 1
SBRL and SBRL-HAP with each method: Fstd = 1 (cid:80) (Fe − F¯)2. Lower values of these two
1 |E| e∈E 1 1
• TARNet [15] is a treatment-agnostic representation net- indicators mean better model stability.
work algorithm with a shared representation network,
C. Experimental Settings
which uses a two-head predictive network to predict the
factual treated outcome and control outcome, separately. In the experiment, we utilize ELU as the non-linear ac-
Since TARNet does not include balance regularization, tivation function and adopt the Adam optimizer to train
we only incorporate Independence Regularize into TAR- all methods. We set the maximum number of iterations to
Net as TARNet+SBRL. TARNet+SBRL-HAP achieves 3000. Besides, we apply an exponentially decaying learning
comprehensive feature decorrelation with hierarchical at- rate [60] and report the best-evaluated iterate with early
tention by Hierarchical-Attention Paradigm. stopping. We first identify the optimal hyper-parameters for
• CFR [15], [42] employs IPM to measure the distribution all baseline algorithms by optimizing hyper-parameters with
distance between the treated and control groups, and trails on random search [61]. Then, with the fixed ba-
learns a balanced representation by minimizing IPM to sic hyper-parameters, we conduct a random search for the
eliminateselectionbias.ByincorporatingBalancingReg- hyper-parameters {γ 1,γ 2,γ 3} of HSIC losses in the scope
ularization and Independence Regularization into CFR, {0.0001,0.001,0.01,0.1,1,10,100} to optimize our model.
werefertoitasCFR+SBRL.Furthermore,CFR+SBRL-
D. Experiments on Synthetic Data
HAP employs the Hierarchical-Attention Paradigm for
comprehensive feature decorrelation through hierarchical 1) Datasets: To simulate complex real-world scenarios,
attention mechanisms. the synthetic data used in our study incorporates several
• DeR-CFR [17] further considers confounder separation key factors: 1) The observed covariates include not only
by learning representations for instrumental variables, confounding variables but also other relevant factors; 2) The
confounding variables, and adjustment variables respec- imbalanced treatment assignment would introduce selection
tively. This enables a more precise evaluation of hetero- bias, reflecting the inherent biases that exist in observational
geneoustreatmenteffects.WhenincorporatingtheSBRL studies;and3)Thesyntheticdataalsoincorporatesdistribution
framework,werefertoitasDeR-CFR+SBRL.Addition- shifts that occur across different environments or populations.
ally, when incorporating it into SBRL-HAP framework, We generate synthetic data using the following process.
we call it DeR-CFR+SBRL-HAP. Covariates generation. We generate covariates from a
The aforementioned three baselines are the most classic multi-variable normal distribution, i.e., X 1,X 2,...,X m ∼
solutions to the traditional HTE estimation problem within N(0,1), where m = m I +m C +m A+m V denotes the di-
in-distribution populations, we use them as Vanilla models to mensionofcovariates,and{m I,m C,m A,m V}denotethedi-
compare them with +SBRL and +SBRL-HAP models. Other mensionsofinstrumentsI,confoundersC,adjustmentsAand
noise V, respectively. For generality, we design two settings
1https://github.com/superpig99/SBRL-HAP of variable dimensions {m I,m C,m A,m V} = {8,8,8,2} orTABLEI
THERESULTS(MEAN±STD)OFTREATMENTEFFECTESTIMATIONONSYNTHETICDATASYN 8 8 8 2WITHDIFFERENTBIASRATEρ.
Metric PEHE(Mean±Std)
BiasRate ρ=−3 ρ=−2.5 ρ=−1.5 ρ=−1.3 ρ=1.3 ρ=1.5 ρ=2.5∗ ρ=3
TARNet 0.565±0.009 0.558±0.007 0.567±0.003 0.559±0.006 0.461±0.003 0.420±0.005 0.363±0.003 0.358±0.003
+SBRL 0.474±0.008 0.459±0.008 0.492±0.007 0.489±0.009 0.410±0.007 0.377±0.004 0.341±0.004 0.332±0.004
+SBRL-HAP 0.440±0.005 0.435±0.007 0.442±0.008 0.462±0.004 0.444±0.005 0.421±0.006 0.404±0.006 0.407±0.005
CFR 0.559±0.009 0.552±0.007 0.563±0.003 0.555±0.006 0.459±0.003 0.418±0.005 0.363±0.003 0.357±0.003
+SBRL 0.475±0.008 0.460±0.008 0.492±0.007 0.490±0.009 0.410±0.007 0.378±0.004 0.341±0.004 0.332±0.004
+SBRL+HAP 0.419±0.005 0.412±0.005 0.429±0.004 0.433±0.005 0.401±0.007 0.374±0.006 0.354±0.005 0.352±0.005
DeRCFR 0.431±0.007 0.439±0.009 0.449±0.007 0.455±0.008 0.376±0.005 0.338±0.005 0.311±0.004 0.306±0.005
+SBRL 0.431±0.005 0.429±0.007 0.441±0.004 0.446±0.007 0.371±0.006 0.335±0.006 0.301±0.006 0.293±0.002
+SBRL-HAP 0.350±0.006 0.353±0.009 0.373±0.006 0.374±0.009 0.340±0.006 0.312±0.006 0.295±0.006 0.295±0.006
Improvement 25.0%↑ 25.4%↑ 23.8%↑ 22.0%↑ 12.6%↑ 10.5%↑ 5.1%↑ 3.6%↑
Metric ϵATE (Mean±Std)
BiasRate ρ=−3 ρ=−2.5 ρ=−1.5 ρ=−1.3 ρ=1.3 ρ=1.5 ρ=2.5∗ ρ=3
TARNet 0.019±0.006 0.032±0.008 0.012±0.004 0.015±0.005 0.021±0.008 0.021±0.008 0.018±0.006 0.021±0.007
+SBRL 0.029±0.005 0.040±0.006 0.027±0.004 0.026±0.005 0.029±0.011 0.020±0.006 0.026±0.006 0.029±0.008
+SBRL-HAP 0.021±0.006 0.025±0.009 0.012±0.004 0.015±0.005 0.023±0.008 0.019±0.008 0.017±0.007 0.021±0.007
CFR 0.018±0.006 0.032±0.008 0.012±0.004 0.014±0.004 0.021±0.008 0.020±0.008 0.018±0.006 0.021±0.007
+SBRL 0.029±0.005 0.040±0.006 0.028±0.004 0.026±0.005 0.030±0.011 0.021±0.006 0.027±0.006 0.029±0.008
+SBRL-HAP 0.019±0.006 0.024±0.009 0.015±0.005 0.013±0.004 0.024±0.008 0.018±0.006 0.013±0.006 0.015±0.007
DeRCFR 0.017±0.006 0.021±0.007 0.014±0.004 0.020±0.005 0.021±0.008 0.020±0.007 0.019±0.006 0.021±0.006
+SBRL 0.021±0.007 0.033±0.005 0.024±0.005 0.028±0.006 0.027±0.011 0.018±0.005 0.022±0.007 0.029±0.008
+SBRL-HAP 0.013±0.003 0.023±0.008 0.013±0.005 0.015±0.005 0.022±0.009 0.013±0.005 0.019±0.007 0.021±0.008
Improvement 23.5%↑ 25.0%↑ 7.1%↑ 25.0%↑ 4.8%↓ 35.0%↑ 27.8%↑ 28.6%↑
*In this paper, we utilize synthetic data with ρ = 2.5 as the training population. The testing data with ρ = 2.5 can be regarded as the In-
DistributionPopulation.Astheparameterρincreases,thedifferenceindistributionbetweenthetestingandtrainingdatasetsalsoincreases.
{16,16,16,2} with the sample size n = 10000, and denoted and Fig. 3. Table I reveals that both SBRL and SBRL-HAP
different setting as Syn m m m m . effectivelyboostthestabilityofITEestimationsacrossdiverse
I C A V
Treatments generation. We produce treatment t ∼ OOD data, while presenting a comparable performance in
B( 1 ), where z = 1 θ × X + ξ, X denotes the ATEevaluationcomparedtothevanillamethods.Accordingto
1+e−z 10 t IC IC
covariatesthatbelongtoI andC,andθ
t
∼U((8,16)mI+mC). Table I, with the increasing distribution discrepancy between
Outcomes generation. Two potential outcomes are gen- the testing set and the training set, the error metric PEHE of
erated as follows: Y0 = sign(max(0,z0 − z¯0)) and Y1 = all methods gets worse. Our methods, however, show success
sign(max(0,z1 − z¯1)), where z0 = 1 θ y0×XCA, z1 = in counteracting this performance degradation, and exhibit a
10 mC+mA more obvious improvement as the bias rate ρ decreases from
N11 0 (θ 0my ,1 C 1× + )X .mC2 TAA h,
e
oan bd serθ vy e0 d,θ oy u1 tco∼ meU is(( Y8, =16) Tm YC 1+ +mA () 1−an Td )Yξ 0.∼ 2 fr. o5 mto 5.1− %3, tore 2s 5u %lti .ng Toin vat lh ide atm
e
a thx eim rou bm ustr ne ed su sct oio fn ouo rf mP eE thH oE
d
Finally, to simulate the distribution shift, we generate dif- for high-dimensional data, we report the results of effect
ferent covariate distributions by biased sampling. For each estimation on Syn 16 16 16 2 data. Fig. 3 depicts the ex-
sample,weselectitwithprobabilityPr=(cid:81) |ρ|−10∗Di, cellent performance of our method on high-dimensional data.
where D i = |Y1 − Y0 − sign(ρ) ∗ XX ii |∈ .X IV f ρ > 0, From results on Syn 16 16 16 2 data, we have following
sign(ρ) = 1; otherwise, sign(ρ) = −1. We generate observations and analysis:
different data distributions by altering the bias rate ρ ∈
• Three baselines fail to handle the problem of HTE
{−3.0,−2.5,−1.5,−1.3,1.3,1.5,2.5,3.0}, where ρ > 1 im-
estimation accompanied by distribution shifts. On the
pliesthepositivecorrelationbetweenoutcomeY andunstable
testing data with ρ = 2.5, which shares the same
features X , and ρ < −1 implies the negative correla-
V distributionasthetrainingtest,PEHEis0.417,0.418,and
tion. The higher |ρ| is, the stronger correlation between Y
0.422 for TARNet, CFR, and DeR-CFR. However, the
and X . Therefore, different values of ρ refer to different
V performance of the baseline methods degrades gradually
environments. To evaluate the generalization of our SBRL
as the distribution gap between the testing data and the
and SBRL-HAP frameworks, we use the generated data with
trainingdataincreases(i.e.,asρdecreases).Forinstance,
ρ=2.5asdefaulttrainingdata,andusethedatawithdifferent
on the testing data with ρ = −3, PEHE of TARNet,
ρ∈{−3.0,−2.5,−1.5,−1.3,1.3,1.5,2.5,3.0}astestingdata
CFR, and DeR-CFR worsens to 0.740, 0.728, and 0.625,
with different environments. with the performance decrease2 of 77%, 74%, and 56%,
2) Results of treatment effect estimation: Results of treat-
2PerformancedecreaseinOODtestingdatasetsiscalculatedby:
ment effect estimation on synthetic data are shown in Table I Decrease=(PEHE −PEHE )/PEHE .
{ρ=−3} {ρ=2.5} {ρ=2.5}Fig.3. ResultsofPEHEonsyntheticdataSyn 16 16 16 2withdifferentbiasrateρforthetestingset.Allmodelsaretrainedwithρ=2.5.
(a) F1 scoresforfactualoutcomes. (b) F1 scoresforcounterfactualoutcomes.
Fig.4. ResultsofF1 scoresonsyntheticdataSyn 16 16 16 2withdifferentbiasrateρforthetestingset.Allmodelsaretrainedwithρ=2.5.
respectively. Such performance degradation of baseline
n
o
m spe ut rh io od us
s
cis ora ren lt aic tii op nate bd e, twa es et nhe uy nse tarr bo ln ee vo au rs il ay blc ea sp Xtu Vre at nh de italerro
C
the target outcome Y. raen
• Compared to other baselines, DeR-CFR exhibits supe- iln o
N
rior resistance to distribution shift, whose performance
degradation is about 20% less than TARNet and CFR.
Fig.5. Nonlinearcorrelationamongfeaturesinthebalancedrepresentation.
This is attributed to DeR-CFR’s confounder separation, As shown, the feature correlation is reduced by our SBRL, and further
which orthogonalizes confounding, instrumental, and ad- decreasedbyincorporatingHAP.
justmentvariables.Itindicatesthatdecorrelatingvariables
is beneficial in learning genuine and stable relationships.
aligns with prior observations [49], [62]–[67]. It is be-
• Both SBRL and SBRL-HAP achieve more stable HTE
cause unstable features tend to contribute to better infer-
estimation across various OOD data. With distribution
ence in ID data [49]; however, our algorithm mitigates
shifts (i.e., ρ shifts from 2.5 to −3), the PEHE of DeR-
theinfluenceofthesefeaturestopreventinstabilitywhen
CFR+SBRLvariesfrom0.428to0.609,indicatinga42%
estimating in OOD data.
drop.BycombiningSRBL-HAP,thePEHEofDeR-CFR
changes from 0.488 to 0.545, only reduced by 11%. Furthermore, Fig. 4 demonstrates that our method out-
However,thePEHEoforiginDeR-CFRdeclinesby56%. performs the other methods in stably predicting factual and
This percentage demonstrates that our algorithm is more counterfactualoutcomes,asmeasuredbyF scoreswithmean
1
stableandtheresultsaremorerobustintheOODtesting and standard deviation (std) across all test sets. Especially,
data.Besides,ouralgorithmexceedsallbaselinesoneach our SBRL-HAP reduces the std of F scores from 0.058
1
OOD testing data (i.e., ρ ∈ [−3,1.3]). For example, to 0.026 for factual outcomes and from 0.040 to 0.009
by combining our SBRL-HAP, the PEHE of DeR-CFR for counterfactual outcomes, compared to the best baseline
under ρ=−3 reduces from 0.657 to 0.545, with a 21% (i.e., DeR-CFR). Consequently, our method can significantly
performanceimprovement.Itisbecauseouralgorithmre- improve the stability of HTE estimation.
solves the conflict between balance and independence by 3) DecorrelationPerformance: Wedemonstratethenonlin-
hierarchical decorrelation, obtaining stable and balanced earcorrelationbetweenfeaturesinthebalancedrepresentation
representations. Hence, our algorithm can improve the Φtoillustratetheeffectivenessofourmethodinmitigatingthe
stability of HTE estimation. conflict between balance and independence. Specifically, we
• Our approach outperforms baselines on OOD data (ρ < randomly sample 25-dimension variables from the balanced
2.5) but performs worse on ID data (ρ ≥ 2.5), which representationlearnedbyCFR,CFR+SBRL,andCFR+SBRL-HAP on data Syn 16 16 16 2, and compute HSIC RFF be- TABLEII
tweeneachpairofvariables.AsshowninFig.5,thebalanced ABLATIONEXPERIMENTSONTHEPERFORMANCEOFEACHSUB-MODULE.
(✓REFERSTOKEEPINGTHESUB-MODULE.)
representation obtained from CFR exhibits strong correlation
between features, with average HSIC =0.85, while direct PEHE
integration of representation
balanciR nF gF
and stable training
BR(LB) IR(LI) HAP(LH)
ρ=2.5 ρ=−3
techniques (i.e., CFR+SBRL) reduces the average HSIC to ✓ ✓ 0.457±0.006 0.594±0.002
RFF
0.64. Notably, CFR+SBRL-HAP can further decrease the av- ✓ ✓ 0.502±0.007 0.584±0.006
erageHSIC RFF to0.58,with37%reductioncomparedtoCFR. ✓ ✓ 0.439±0.006 0.662±0.015
Since the major difference between CFR+SBRL-HAP and ✓ ✓ ✓ 0.460±0.007 0.591±0.004
CFR is the feature decorrelation with hierarchical attention, *LH=LD(Zr,w)+LD(Zo,w).
we can safely conclude that such feature decorrelation can
promotethemodeltoidentifystablefeaturesandacquiremore
effectiveassociationswithpotentialoutcomes,thusenhancing IHDP4. This is a binary-treatment and continuous-outcome
the generalization ability. dataset, generated from the Randomized Controlled Trial
4) AblationStudies: TableIIreportstheeffectsofeachsub- (RCT) data of the Infant Health and Development Program
moduleofourSBRL-HAPbyconductingablationexperiments (IHDP) [58]. The RCT data of IHDP is collected to evaluate
on Syn 16 16 16 2 dataset. The observations are as follows: the effect of specialist home visits on the cognitive test scores
(1) Each component of our SBRL-HAP is indispensable since of premature infants. Hill induced selection bias by removing
the absence of any one of them would hinder obtaining bal- a biased subset of the treated group, and Shuilte simulated
anced and stable representations and damage the performance outcomes by setting “A” of the NPCI package [69]. This
ofHTEestimationonOODdata.(2)ComparedtoIRandBR, dataset contains 747 units (139 treated, 608 control) with 25
HAP has the greatest impact on the model’s performance on covariates (6 continuous, 19 discrete) related to children and
OOD populations of Syn 16 16 16 2 data. mothers. To introduce distribution shift, we biasedly sample
10% records as the testing set with specific selection prob-
E. Experiments on Real-world Data
a vb aril ii at bie les s,P ar nd= D(cid:81) X =i∈ |YXl −|ρ| Y−10 −∗D si ig, nw (ρh )er ∗e XX |l .a Tr he ec ro en mti an iu no inu gs
i 1 0 i
90% of records are divided randomly into training/validation
1) Datasets: We also conduct experiments on two real-
with a 70/30 proportion. Different from the unstable variables
world datasets, Twins and IHDP, which are widely used in
X ∼ N(0,1) in Twins, we choose the subset of original
V
HTE estimation literature [15], [16], [21].
variablesinIHDPtointroducedistributionshift.Thisapproach
Twins3.TheTwinsdatasetoriginatesfromtwinsbirthinthe
aims to create a more complex scenario to verify the effec-
USAbetween1989and1991[68].Thetreatmentcorresponds
tiveness of our method.
to twins’ weight, where t = 1 indicates the heavier twin and
2) Results: We report the mean and standard deviation
t = 0 indicates the lighter one. The outcome corresponds
(std) of treatment effect over 10 replications on Twins and
to the twins’ mortality after one year. We collect records
100 replications on IHDP datasets in Table III. The results
of same-sex twins weighing less than 2000g and without
show that in comparison with state-of-the-art methods, our
missing features, resulting in a total of 5271 records. The
SBRL achieves significantly better performance on the testing
dataset consists of 43 variables X = {X ,X ,...,X }, of
1 2 43 set, while avoiding model overfitting and maintaining similar
whichX ={X ,X ,...,X }arederivedfromtheoriginal
C 1 2 28 performance to the baseline methods on the training set.
data related to parents, pregnancy, and birth. In addition,
Especially on Twins, our proposed SBRL-HAP reduces the
10 instrumental variables X = {X ,X ,...,X } and 5
I 29 30 38 error metric PEHE by 13.1%, 10.8%, and 5.6% for TARNet,
unstable variables X = {X ,X ,...,X } are generated
V 39 40 43 CFR, and DeR-CFR, as well as minimizes the ATE bias by
with normal distribution N(0,1). To simulate selection bias,
9.6%, 8.8%, and 14.3%.
treatment is assigned as follows: t |x ∼ B( 1 ), where
i i 1+e−z Compared to synthetic datasets, the performance of our
z = wTX + η, w ∼ U(−0.1,0.1) and η ∼ N(0,0.1).
IC method on real-world datasets is enhanced, but the improve-
B denotes the Bernoulli distribution. Besides, to create dis-
ment is not stably significant. According to the characteristics
tribution shift, we generate selection probabilities for each
and experiment results of Twins and IHDP datasets, we have
sampleinthefollowingway:Pr=(cid:81) |ρ|−10∗Di,where
Xi∈XV thefollowingobservations.Duringthetrainingprocess,thehi-
D = |Y − Y − sign(ρ) ∗ X |. Here, we set ρ = −2.5.
i 1 0 i erarchicalindependencemeasureofTwinsdatasetconsistently
Based on the sample probabilities, 20% records are sampled
remains significantly lower compared to the other datasets
as the testing set. Then, the rest data is randomly split into a
employed. Since most parents made similar pregnancy prepa-
training/validation set using a 70/30 ratio. Repeat the above
rations, there is an abundance of similar or identical variables
data partitioning for 10 rounds to form the final dataset.
in Twins dataset, resulting in distribution differences that are
not highly significant in different environments. Although our
3http://www.nber.org/data/linked-birth-infant-death-data-vital-statistics-
data.html. 4http://www.fredjo.com.TABLEIII
THERESULTS(MEAN±STD)OFTREATMENTEFFECTESTIMATIONONREAL-WORLDDATA.OURMETHODSSIGNIFICANTLYIMPROVETHEACCURACYOF
HTEESTIMATIONONTHETESTINGSET,WITHCOMPARABLEPERFORMANCESONTHETRAININGSETCOMPAREDTOTHEBASELINES.
Twins
Metric PEHE(Mean±Std) ϵATE (Mean±Std)
Dataset Training Validation Testing Training Validation Testing
TARNet 0.313±0.010 0.342±0.014 0.630±0.012 0.024±0.005 0.028±0.007 0.355±0.007
+SBRL 0.309±0.011 0.336±0.014 0.621±0.009 0.026±0.004 0.031±0.006 0.348±0.004
+SBRL-HAP 0.236±0.006 0.239±0.007 0.547±0.003 0.057±0.001 0.056±0.002 0.321±0.002
CFR 0.294±0.013 0.313±0.018 0.613±0.012 0.024±0.004 0.025±0.005 0.352±0.005
+SBRL 0.287±0.014 0.307±0.018 0.611±0.013 0.020±0.005 0.023±0.006 0.356±0.006
+SBRL-HAP 0.236±0.005 0.238±0.007 0.547±0.003 0.056±0.001 0.056±0.002 0.321±0.001
DeRCFR 0.229±0.002 0.229±0.003 0.585±0.009 0.041±0.013 0.040±0.013 0.385±0.013
+SBRL 0.229±0.002 0.229±0.003 0.584±0.009 0.040±0.013 0.039±0.013 0.384±0.013
+SBRL-HAP 0.236±0.002 0.236±0.004 0.552±0.006 0.048±0.010 0.047±0.011 0.330±0.011
IHDP
Metric PEHE(Mean±Std) ϵATE (Mean±Std)
Dataset Training Validation Testing Training Validation Testing
TARNet 0.620±0.042 0.677±0.056 0.857±0.098 0.200±0.026 0.199±0.026 0.254±0.037
+SBRL 0.622±0.042 0.683±0.057 0.834±0.093 0.184±0.025 0.183±0.025 0.250±0.037
+SBRL-HAP 0.628±0.041 0.696±0.058 0.827±0.089 0.179±0.023 0.179±0.023 0.226±0.032
CFR 0.628±0.042 0.687±0.057 0.858±0.099 0.197±0.026 0.196±0.026 0.259±0.038
+SBRL 0.622±0.043 0.681±0.059 0.848±0.094 0.196±0.027 0.197±0.027 0.251±0.037
+SBRL-HAP 0.623±0.038 0.688±0.053 0.820±0.087 0.185±0.024 0.184±0.024 0.220±0.031
DeRCFR 0.460±0.024 0.487±0.029 0.607±0.062 0.150±0.022 0.152±0.022 0.183±0.025
+SBRL 0.450±0.022 0.476±0.028 0.592±0.062 0.141±0.019 0.143±0.019 0.181±0.024
+SBRL-HAP 0.449±0.023 0.478±0.029 0.573±0.057 0.151±0.021 0.154±0.021 0.178±0.024
algorithm eliminated the OOD issue, the level of OOD is too modifications in γ can result in significant changes in the
3
low to indicate remarkable improvement. Similarly, due to entireloss.Hyper-parametersanalysisassistsusinidentifying
limited distribution shift, the performance of our algorithm on the most suitable hyper-parameters for experiments.
IHDP dataset only improved by 2.3%∼15.1%. Furthermore,
G. Training Cost Analysis
for IHDP dataset, we introduce a more complex covariate
In our method, the network structure and hierarchical-
shift than the traditional settings [49], [53]: among the six
attentionindependenceconstraintsaretheprimarycontributors
continuousvariablesusedforbiasedsampling,somemayhave
causation with the outcome Y. Artificially introducing unsta- to the increased model complexity and training time. To
investigate the complexity of all methods, we implement 10
blecorrelationonthesepotentiallystablefeatureswouldmake
replications on IHDP dataset to study the average training
itdifficultforthemodeltoidentifyrealstablerepresentations.
time(s) in a single execution, as shown in Table VI. Table VI
indicates that our SBRL results in nearly twice the training
F. Hyper-parameter Analysis
cost than TARNet and CFR. This is due to the additional
Table IV and Table V list all optimal hyper-parameters training process for sample weights compared to TARNet
of our SBRL-HAP used for each dataset. Note that setting and CFR. Besides, our SBRL-HAP leads to over a 3-fold
{γ 1,γ 2,γ 3} to 0 in Table IV and Table V denotes the op- increase in training time of TARNet and CFR, and a 1.5-fold
timal hyper-parameters of our SBRL. Given that the hyper- increase for DeR-CFR. Such an increase is primarily due to
parameters {γ 1,γ 2,γ 3} determine the hierarchical attention the hierarchical-attention optimization strategy. As the model
for variable decorrelation, we investigate the impact of each complexityincreases,bothaccuracyandstabilityofthemodel
hyper-parameter on the model’s performance and stability. As improve. Despite its higher computational time, our proposed
shown in Fig. 6, we report PEHE on data Syn 16 16 16 2 method achieves the most stable and accurate treatment effect
with ρ=2.5 and F 1 scores of factual outcomes with ρ=−3 estimation.Fortunately,themaximumtrainingtimeinasingle
bychanging{γ 1,γ 2,γ 3}inthescope{0,0.01,0.1,1,10,100}. execution is less than 180 seconds, which is still acceptable.
Since PEHE is higher under γ 1 = 0 compared to that under Hardware configuration: CentOS Linux release 7.2 (Final)
γ 1 = 100, and PEHE is lower under γ 2 = 0 than that under operating system with the AMD EPYC 7K62 48-Core CPU
γ 2 = 100, we conclude that it is better to give relatively Processor,1TBofRAM.Softwareconfiguration:Python3.6.8
more attention to the last layer of models and comparatively with TensorFlow 1.15.0, NumPy 1.19.5, Scikit-learn 0.24.2.
less attention to the balanced representation layer. Besides,
compared to γ and γ , the impact of γ on the model’s
VI. CONCLUSIONANDFUTURE
1 2 3
performance and stability is more complex. This is because In this paper, we first study the problem of the Heteroge-
γ controls attention to nearly all hidden layers, so that slight neousTreatmentEffectacrossOut-of-distributionPopulations.
3TABLEIV
OPTIMALHYPER-PARAMETERSOFCFR+SBRL-HAP.
Hyper-parameters Twins IHDP Syn 8 8 8 2 Syn 16 16 16 2
learningrate 1e-5 1e-3 1e-5 1e-4
batchnorm 1 0 1 1
repnormalization 1 1 0 0
{dr,dy} {3,3} {3,3} {3,3} {3,3}
{hr,hy} {128,64} {256,128} {128,64} {128,64}
{α,λ} {1e-4,1e-4} {1,1e-4} {5e-2,1e-4} {1e-3,1e-4}
{γ1,γ2,γ3} {1,1,1e-1} {1e-1,1e-4,1e-4} {1,1,1e-1} {1,1e-3,1e-3}
*Setαto0togettheoptimalhyper-parametersofTARNet+SBRL-HAP.
TABLEV
OPTIMALHYPER-PARAMETERSOFDER-CFR+SBRL-HAP.
Hyper-parameters Twins IHDP Syn 8 8 8 2 Syn 16 16 16 2
learningrate 1e-1 1e-3 1e-4 5e-4
batchnorm 1 0 1 1
repnormalization 1 1 0 0
{dr,dy,dt} {3,3,2} {5,3,1} {2,2,3} {2,2,3}
{hr,hy,ht} {256,128,128} {32,256,128} {256,256,256} {256,256,256}
{α,β,γ,µ,λ} {1e-2,5,1e-4,5,5} {10,5,1e-3,50,10} {1,1e-3,5,1,1} {1,1e-3,5,1,1}
{γ1,γ2,γ3} {1,1,1e-2} {1,1e-1,1e-2} {1,1e-2,1} {1,1e-2,1e-2}
*RefertoDeR-CFR[17]forthemeaningofhyper-parameters{α,β,γ,µ,λ}.
TABLEVI
TRAININGTIME(S)OFVARIOUSMETHODSINASINGLEEXECUTIONON
IHDPDATASET.
Method TARNet +SBRL +SBRL-HAP
Time(s) 22.4 40.6 79.7
(a) ThePEHEerrorwithρ=2.5.
Method CFR +SBRL +SBRL-HAP
Time(s) 25.3 40.8 80.1
Method DeR-CFR +SBRL +SBRL-HAP
Time(s) 96.4 112.1 140.5
(b) TheF1 scoreonfactualoutcomeswithρ=−3.
does not generalize well to OOD populations. One potential
Fig. 6. Hyper-parameter sensitivity analysis on {γ1,γ2,γ3} within the
solution to find a balance between stability and performance
specified range {0,0.01,0.1,1,10,100} on Syn 16 16 16 2 dataset. The
referenceredlineindicatestheoptimalparametersforthesetting. is to incorporate a module that measures the OOD level
between the target domain and the source domain. Based
on the measured OOD level, it would be feasible to use
Previous causal methods have primarily concentrated on ad- interpolation or spline methods to boost our algorithm with
dressingselectionbiaswithinin-distributiondata.However,in conventionalsupervisedlearning,whichislefttofuturework.
real-worldapplications,wheredistributionshiftsarecommon,
these methods may face challenges in effectively handling ACKNOWLEDGMENT
OODdata.ToachievemoreaccurateHTEestimationonOOD
data, we propose a Stable Balanced Representation Learning This work was supported in part by National Key
with Hierarchical-Attention Paradigm (SBRL-HAP) to jointly Research and Development Project of China (Grant No.
address selection bias and distribution shift by synergistically 2023YFF0905502), Shenzhen Science and Technology
optimizing a Balancing Regularizer and an Independence Program (Grant No. RCYX20200714114523079 and
Regularizer in a Hierarchical-Attention Paradigm. One limita- JCYJ20220818101014030) and National Natural Science
tion is that when combining existing balanced representation Foundation of China (Grant No. 62376243 and U20A20387).
methods with SBRL-HAP, the performance on in-distribution We would like to thank Tencent for supporting the research
data may decrease compared to vanilla methods. Because during Yuling Zhang’s internship. We also would like to
vanillamethodswouldrelyontheinductivebiasfromunstable thank Kuaishou for sponsoring the research. Anpeng Wu’s
featurestoimproveperformanceonin-distributiondata,which research was supported by the China Scholarship Council.REFERENCES [20] L. Yao, Y. Li, S. Li, M. Huai, J. Gao, and A. Zhang, “SCI: Subspace
LearningBasedCounterfactualInferenceforIndividualTreatmentEffect
[1] Z.Chu,R.Li,S.Rathbun,andS.Li,“ContinualCausalInferencewith Estimation,”inProceedingsofthe30thACMInternationalConference
IncrementalObservationalData,”Mar.2023. onInformation&KnowledgeManagement,ser.CIKM’21. NewYork,
NY,USA:AssociationforComputingMachinery,Oct.2021,pp.3583–
[2] M.Ai,B.Li,H.Gong,Q.Yu,S.Xue,Y.Zhang,Y.Zhang,andP.Jiang,
3587.
“LBCF: A Large-Scale Budget-Constrained Causal Forest Algorithm,”
inProceedingsoftheACMWebConference2022,ser.WWW’22. New [21] L.Yao,S.Li,Y.Li,M.Huai,J.Gao,andA.Zhang,“ACE:Adaptively
York,NY,USA:AssociationforComputingMachinery,Apr.2022,pp. Similarity-Preserved Representation Learning for Individual Treatment
2310–2319. Effect Estimation,” in 2019 IEEE International Conference on Data
[3] Z.Tan,S.Zhang,N.Hong,K.Kuang,Y.Yu,J.Yu,Z.Zhao,H.Yang,
Mining(ICDM),Nov.2019,pp.1432–1437.
S.Pan,J.Zhou,andF.Wu,“UncoveringCausalEffectsofOnlineShort [22] ——, “Representation Learning for Treatment Effect Estimation from
VideosonConsumerBehaviors,”inProceedingsoftheFifteenthACM Observational Data,” in Advances in Neural Information Processing
InternationalConferenceonWebSearchandDataMining,ser.WSDM Systems,vol.31. CurranAssociates,Inc.,2018.
’22. NewYork,NY,USA:AssociationforComputingMachinery,Feb. [23] Y.Zhang,C.Li,I.W.Tsang,H.Xu,L.Duan,H.Yin,W.Li,andJ.Shao,
2022,pp.997–1006. “Diverse Preference Augmentation with Multiple Domains for Cold-
[4] Y.Meng,S.Zhang,Z.Ye,B.Wang,Z.Wang,Y.Sun,Q.Liu,S.Yang, start Recommendations,” in 2022 IEEE 38th International Conference
andD.Pei,“CausalAnalysisoftheUnsatisfyingExperienceinRealtime onDataEngineering(ICDE),May2022,pp.2942–2955.
Mobile Multiplayer Games in the Wild,” in 2019 IEEE International [24] J. Cao, J. Sheng, X. Cong, T. Liu, and B. Wang, “Cross-Domain
ConferenceonMultimediaandExpo(ICME),Jul.2019,pp.1870–1875. RecommendationtoCold-StartUsersviaVariationalInformationBottle-
[5] S. Wager and S. Athey, “Estimation and Inference of Heterogeneous neck,”in2022IEEE38thInternationalConferenceonDataEngineering
Treatment Effects using Random Forests,” Journal of the American (ICDE),May2022,pp.2209–2223.
StatisticalAssociation,vol.113,no.523,pp.1228–1242,Jul.2018. [25] S. Zhou, L. Wang, S. Zhang, Z. Wang, and W. Zhu, “Active Gradual
[6] J.Pearl,Causality. CambridgeUniversityPress.,2009. Domain Adaptation: Dataset and Approach,” IEEE Transactions on
[7] Z.Wang,X.Chen,R.Zhou,Q.Dai,Z.Dong,andJ.-R.Wen,“Sequential Multimedia,vol.24,pp.1210–1220,2022.
RecommendationwithUserCausalBehaviorDiscovery,”in2023IEEE [26] S.Zhou,H.Zhao,S.Zhang,L.Wang,H.Chang,Z.Wang,andW.Zhu,
39thInternationalConferenceonDataEngineering(ICDE),Apr.2023, “Online Continual Adaptation with Active Self-Training,” in Proceed-
pp.28–40. ingsofThe25thInternationalConferenceonArtificialIntelligenceand
[8] F.Zhu,M.Zhong,X.Yang,L.Li,L.Yu,T.Zhang,J.Zhou,C.Chen, Statistics. PMLR,May2022,pp.8852–8883.
F. Wu, G. Liu, and Y. Wang, “DCMT: A Direct Entire-Space Causal [27] H. Fang, B. Chen, X. Wang, Z. Wang, and S.-T. Xia, “GIFD: A Gen-
Multi-TaskFrameworkforPost-ClickConversionEstimation,”in2023 erativeGradientInversionMethodwithFeatureDomainOptimization,”
IEEE39thInternationalConferenceonDataEngineering(ICDE),Apr. inProceedingsoftheIEEE/CVFInternationalConferenceonComputer
2023,pp.3113–3125. Vision,2023,pp.4967–4976.
[9] B. Youngmann, M. Cafarella, Y. Moskovitch, and B. Salimi, “On [28] H.Wu,Y.Yan,G.Lin,M.Yang,M.K.Ng,andQ.Wu,“IterativeRefine-
Explaining Confounding Bias,” in 2023 IEEE 39th International Con- mentforMulti-SourceVisualDomainAdaptation(Extendedabstract),”
ferenceonDataEngineering(ICDE),Apr.2023,pp.1846–1859. in 2023 IEEE 39th International Conference on Data Engineering
[10] F. Shen, K. Heravi, O. Gomez, S. Galhotra, A. Gilad, S. Roy, and (ICDE),Apr.2023,pp.3829–3830.
B. Salimi, “Causal What-If and How-To Analysis Using HypeR,” in [29] Y. Yan, H. Wu, Y. Ye, C. Bi, M. Lu, D. Liu, Q. Wu, and M. K. Ng,
2023IEEE39thInternationalConferenceonDataEngineering(ICDE), “Transferable Feature Selection for Unsupervised Domain Adaptation
Apr.2023,pp.3663–3666. : Extended Abstract,” in 2023 IEEE 39th International Conference on
[11] P.R.RosenbaumandD.B.Rubin,“Thecentralroleofthepropensity DataEngineering(ICDE),Apr.2023,pp.3855–3856.
score in observational studies for causal effects,” Biometrika, vol. 70, [30] C.Chen,J.Xiao,J.Liu,J.Zhang,J.Jia,andN.Hu,“UnsupervisedIntra-
no.1,pp.41–55,1983. Domain Adaptation for Recommendation via Uncertainty Minimiza-
[12] P. R. Rosenbaum, “Model-Based Direct Adjustment,” Journal of the tion,”in2023IEEE39thInternationalConferenceonDataEngineering
American Statistical Association, vol. 82, no. 398, pp. 387–394, Jun. Workshops(ICDEW),Apr.2023,pp.79–86.
1987. [31] Z.Chen,T.Xiao,andK.Kuang,“BA-GNN:OnLearningBias-Aware
[13] S. Li, N. Vlassis, J. Kawale, and Y. Fu, “Matching via dimensional- Graph Neural Network,” in 2022 IEEE 38th International Conference
ity reduction for estimation of treatment effects in digital marketing onDataEngineering(ICDE),May2022,pp.3012–3024.
campaigns,” in Proceedings of the Twenty-Fifth International Joint [32] J.Yuan,X.Ma,D.Chen,K.Kuang,F.Wu,andL.Lin,“Label-Efficient
Conference on Artificial Intelligence, ser. IJCAI’16. New York, New Domain Generalization via Collaborative Exploration and Generaliza-
York,USA:AAAIPress,Jul.2016,pp.3768–3774. tion,” in Proceedings of the 30th ACM International Conference on
[14] L. Yao, Z. Chu, S. Li, Y. Li, J. Gao, and A. Zhang, “A Survey on Multimedia, ser. MM ’22. New York, NY, USA: Association for
Causal Inference,” ACM Transactions on Knowledge Discovery from ComputingMachinery,Oct.2022,pp.2361–2370.
Data,vol.15,no.5,pp.1–46,Oct.2021. [33] M.Sugiyama,M.Krauledat,andK.-R.Mu¨ller,“CovariateShiftAdapta-
[15] U. Shalit, F. D. Johansson, and D. Sontag, “Estimating individual tionbyImportanceWeightedCrossValidation,”TheJournalofMachine
treatmenteffect:Generalizationboundsandalgorithms,”inProceedings LearningResearch,vol.8,pp.985–1005,Dec.2007.
of the 34th International Conference on Machine Learning. PMLR, [34] H.Shimodaira,“Improvingpredictiveinferenceundercovariateshiftby
Jul.2017,pp.3076–3085. weighting the log-likelihood function,” Journal of Statistical Planning
[16] N. Hassanpour and R. Greiner, “CounterFactual Regression with Im- andInference,vol.90,no.2,pp.227–244,Oct.2000.
portance Sampling Weights,” in Proceedings of the Twenty-Eighth In- [35] P.CuiandS.Athey,“Stablelearningestablishessomecommonground
ternationalJointConferenceonArtificialIntelligence. Macao,China: betweencausalinferenceandmachinelearning,”NatureMachineIntel-
International Joint Conferences on Artificial Intelligence Organization, ligence,vol.4,no.2,pp.110–115,Feb.2022.
Aug.2019,pp.5880–5887. [36] H. Wang, Z. He, Z. C. Lipton, and E. P. Xing, “Learning Robust
[17] A. Wu, J. Yuan, K. Kuang, B. Li, R. Wu, Q. Zhu, Y. T. Zhuang, and RepresentationsbyProjectingSuperficialStatisticsOut,”Mar.2019.
F. Wu, “Learning Decomposed Representations for Treatment Effect [37] K. Muandet, D. Balduzzi, and B. Scho¨lkopf, “Domain Generalization
Estimation,” IEEE Transactions on Knowledge and Data Engineering, via Invariant Feature Representation,” in Proceedings of the 30th In-
pp.1–1,2022. ternationalConferenceonMachineLearning. PMLR,Feb.2013,pp.
[18] N.HassanpourandR.Greiner,“LearningDisentangledRepresentations 10–18.
forCounterFactualRegression,”inInternationalConferenceonLearn- [38] S. Fan, X. Wang, C. Shi, P. Cui, and B. Wang, “Generalizing Graph
ingRepresentations,Mar.2020. NeuralNetworksonOut-Of-DistributionGraphs,”Nov.2021.
[19] P. Schwab, L. Linhardt, S. Bauer, J. M. Buhmann, and W. Karlen, [39] X. Zhang, P. Cui, R. Xu, L. Zhou, Y. He, and Z. Shen, “Deep Stable
“Learning Counterfactual Representations for Estimating Individual LearningforOut-of-DistributionGeneralization,”inProceedingsofthe
Dose-Response Curves,” Proceedings of the AAAI Conference on Ar- IEEE/CVF Conference on Computer Vision and Pattern Recognition,
tificialIntelligence,vol.34,no.04,pp.5612–5619,Apr.2020. 2021,pp.5372–5382.[40] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, [62] Y. Zhang, X. Wang, J. Liang, Z. Zhang, L. Wang, R. Jin, and T. Tan,
“Building machines that learn and think like people,” Behavioral and “Free Lunch for Domain Adversarial Training: Environment Label
BrainSciences,vol.40,p.e253,Jan.2017. Smoothing,”Jan.2023.
[41] A.Wu,K.Kuang,R.Xiong,M.Zhu,Y.Liu,B.Li,F.Liu,Z.Wang,and [63] X. Tan, L. Yong, S. Zhu, C. Qu, X. Qiu, X. Yinghui, P. Cui, and
F.Wu,“LearningInstrumentalVariablefromDataFusionforTreatment Y. Qi, “Provably Invariant Learning without Domain Information,” in
Effect Estimation,” Proceedings of the AAAI Conference on Artificial Proceedingsofthe40thInternationalConferenceonMachineLearning.
Intelligence,vol.37,no.9,pp.10324–10332,Jun.2023. PMLR,Jul.2023,pp.33563–33580.
[42] F. Johansson, U. Shalit, and D. Sontag, “Learning Representations for [64] D.Krueger,E.Caballero,J.-H.Jacobsen,A.Zhang,J.Binas,D.Zhang,
Counterfactual Inference,” in Proceedings of The 33rd International R. L. Priol, and A. Courville, “Out-of-Distribution Generalization via
ConferenceonMachineLearning. PMLR,Jun.2016,pp.3020–3029. Risk Extrapolation (REx),” in Proceedings of the 38th International
[43] F. D. Johansson, N. Kallus, U. Shalit, and D. Sontag, “Learning ConferenceonMachineLearning. PMLR,Jul.2021,pp.5815–5826.
Weighted Representations for Generalization Across Designs,” Feb. [65] Y.-F. Zhang, J. Wang, J. Liang, Z. Zhang, B. Yu, L. Wang, D. Tao,
2018. and X. Xie, “Domain-Specific Risk Minimization for Domain Gener-
alization,” in Proceedings of the 29th ACM SIGKDD Conference on
[44] Y.ChangandJ.Dy,“InformativeSubspaceLearningforCounterfactual
Knowledge Discovery and Data Mining, ser. KDD ’23. New York,
Inference,” Proceedings of the AAAI Conference on Artificial Intelli-
NY,USA:AssociationforComputingMachinery,Aug.2023,pp.3409–
gence,vol.31,no.1,Feb.2017.
3421.
[45] K. Kuang, P. Cui, B. Li, M. Jiang, and S. Yang, “Estimating Treat-
[66] X. Zhou, Y. Lin, W. Zhang, and T. Zhang, “Sparse Invariant Risk
ment Effect in the Wild via Differentiated Confounder Balancing,” in
Minimization,”inProceedingsofthe39thInternationalConferenceon
Proceedings of the 23rd ACM SIGKDD International Conference on
MachineLearning. PMLR,Jun.2022,pp.27222–27244.
Knowledge Discovery and Data Mining, ser. KDD ’17. New York,
[67] M. Zhang, J. Yuan, Y. He, W. Li, Z. Chen, and K. Kuang, “MAP:
NY,USA:AssociationforComputingMachinery,Aug.2017,pp.265–
Towards Balanced Generalization of IID and OOD through Model-
274.
Agnostic Adapters,” in Proceedings of the IEEE/CVF International
[46] S. Athey, G. W. Imbens, and S. Wager, “Approximate Residual Bal- ConferenceonComputerVision,2023,pp.11921–11931.
ancing: De-Biased Inference of Average Treatment Effects in High [68] D. Almond, K. Y. Chay, and D. S. Lee, “The Costs of Low Birth
Dimensions,”Jan.2018. Weight*,” The Quarterly Journal of Economics, vol. 120, no. 3, pp.
[47] J. Hainmueller, “Entropy Balancing for Causal Effects: A Multivariate 1031–1083,Aug.2005.
Reweighting Method to Produce Balanced Samples in Observational [69] V.Dorie,“Vdorie/npci,”May2023.
Studies,”PoliticalAnalysis,vol.20,no.1,pp.25–46,2012/ed.
[48] Z.Shen, P. Cui, K. Kuang,B. Li,and P.Chen, “CausallyRegularized
LearningwithAgnosticDataSelectionBias,”inProceedingsofthe26th
ACMInternationalConferenceonMultimedia,ser.MM’18. NewYork,
NY,USA:AssociationforComputingMachinery,Oct.2018,pp.411–
419.
[49] K.Kuang,R.Xiong,P.Cui,S.Athey,andB.Li,“StablePredictionwith
Model Misspecification and Agnostic Distribution Shift,” Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp.
4485–4492,Apr.2020.
[50] Z. Shen, P. Cui, T. Zhang, and K. Kunag, “Stable Learning via
SampleReweighting,”ProceedingsoftheAAAIConferenceonArtificial
Intelligence,vol.34,no.04,pp.5692–5699,Apr.2020.
[51] Guido W Imbens and Donald B Rubin, Causal Inference in Statistics,
Social,andBiomedicalSciences. CambridgeUniversityPress,2015.
[52] J. Liu, Z. Hu, P. Cui, B. Li, and Z. Shen, “Heterogeneous Risk
Minimization,”inProceedingsofthe38thInternationalConferenceon
MachineLearning. PMLR,Jul.2021,pp.6804–6814.
[53] R. Xu, X. Zhang, Z. Shen, T. Zhang, and P. Cui, “A Theoretical
Analysis on Independence-driven Importance Weighting for Covariate-
shiftGeneralization,”Jul.2022.
[54] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Scho¨lkopf, and
G. R. G. Lanckriet, “On integral probability metrics, \phi-divergences
andbinaryclassification,”Oct.2009.
[55] A. Mu¨ller, “Integral Probability Metrics and Their Generating Classes
ofFunctions,”AdvancesinAppliedProbability,vol.29,no.2,pp.429–
443,Jun.1997.
[56] A.Gretton,K.Fukumizu,C.Teo,L.Song,B.Scho¨lkopf,andA.Smola,
“A Kernel Statistical Test of Independence,” in Advances in Neural
InformationProcessingSystems,vol.20. CurranAssociates,Inc.,2007.
[57] E. V. Strobl, K. Zhang, and S. Visweswaran, “Approximate Kernel-
BasedConditionalIndependenceTestsforFastNon-ParametricCausal
Discovery,”JournalofCausalInference,vol.7,no.1,Mar.2019.
[58] J. L. Hill, “Bayesian Nonparametric Modeling for Causal Inference,”
Journal of Computational and Graphical Statistics, vol. 20, no. 1, pp.
217–240,Jan.2011.
[59] H.R.Kunsch,“TheJackknifeandtheBootstrapforGeneralStationary
Observations,”TheAnnalsofStatistics,vol.17,no.3,pp.1217–1241,
1989.
[60] J.Duchi,E.Hazan,andY.Singer,“AdaptiveSubgradientMethodsfor
OnlineLearningandStochasticOptimization,”TheJournalofMachine
LearningResearch,vol.12,no.null,pp.2121–2159,Jul.2011.
[61] J. Bergstra and Y. Bengio, “Random search for hyper-parameter opti-
mization,”TheJournalofMachineLearningResearch,vol.13,no.null,
pp.281–305,Feb.2012.