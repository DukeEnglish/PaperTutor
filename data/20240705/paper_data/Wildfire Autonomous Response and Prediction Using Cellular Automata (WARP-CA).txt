Wildfire Autonomous Response and Prediction
Using Cellular Automata (WARP-CA)
Abdelrahman Ramadan
July 4, 2024
4202
luJ
2
]IA.sc[
1v31620.7042:viXraAbstract
Wildfiresposeaseverechallengetoecosystemsandhumansettlements, exacerbatedbyclimatechange
andenvironmentalfactors. Traditionalwildfiremodeling, whileuseful, oftenfailstoadapttotherapid
dynamics of such events. This report introduces the (Wildfire Autonomous Response and Prediction
UsingCellularAutomata)WARP-CAmodel,anovelapproachthatintegratesterraingenerationusing
Perlin noise with the dynamism of Cellular Automata (CA) to simulate wildfire spread. We explore
the potential of Multi-Agent Reinforcement Learning (MARL) to manage wildfires by simulating
autonomous agents, such as UAVs and UGVs, within a collaborative framework. Our methodology
combines world simulation techniques and investigates emergent behaviors in MARL, focusing on
efficient wildfire suppression and considering critical environmental factors like wind patterns and
terrain features.
1Contents
1 Introduction 5
1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Existing Techniques and Their Limitations . . . . . . . . . . . . . . . . . . . . . . . . 5
1.3 Need for a Novel Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.4 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Literature Review 7
2.1 Comprehensive Review on Wildfire Simulation: Models, Algorithms, and Terrain Gen-
eration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Autonomous Wildfire Management and Response Systems: A MARL Perspective . . . 8
3 Methodology 10
3.1 World Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.1 Terrain Generation with Perlin Noise . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.2 Wildfires Fire Spread Simulation using CA . . . . . . . . . . . . . . . . . . . . 12
3.2 Single Agent Reinforcement Learning (SARL) Framework . . . . . . . . . . . . . . . . 18
3.2.1 Environment Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2.2 Reinforcement Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2.3 Reward Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2.4 Integration with Fire Dynamics Simulation . . . . . . . . . . . . . . . . . . . . 20
3.3 Multi-Agent Reinforcement Learning (MARL) Framework . . . . . . . . . . . . . . . . 20
3.3.1 Defining the Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3.2 Applying PPO in MARL for Wildfire Management . . . . . . . . . . . . . . . . 21
4 Simulation Results and Discussion 23
4.1 Simulation Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2 Simulation Machine Specifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.3 Hyperparameters Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.4 SARL Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
24.4.1 Environment Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.4.2 Agent and Model Training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.4.3 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.5 MARL Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.5.1 Environment Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4.5.2 Multi-Agent and Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.6 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.7 Performance Metrics Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
5 Conclusion 40
3List of Figures
3.1 Smoothed Perlin Noise Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.2 The Effect of Choosing Different Scales on Perlin Noise Generation. . . . . . . . . . . 13
3.3 Progression of Fire Spread using CA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.4 Progression of the Forest Fire simulation with additional Environmental Complexities 17
3.5 Pipeline from World Simulation to SARL and MARL Training and Deployment. . . . 22
4.1 Training Flow of the Fire Extinguishing Environment with for a Single Agent.. . . . . 25
4.2 Frame rate per second over time for different models. . . . . . . . . . . . . . . . . . . . 26
4.3 Training loss over time for different models. . . . . . . . . . . . . . . . . . . . . . . . . 27
4.4 Approximate KL divergence over time for PPO models. . . . . . . . . . . . . . . . . . 27
4.5 Value loss over time for the PPO and A2C models. . . . . . . . . . . . . . . . . . . . . 28
4.6 Entropy loss over time, indicating the exploration behavior of the models. . . . . . . . 28
4.7 Mean episode length over time, reflecting the consistency of the agents’ performance. . 29
4.8 Mean episode reward over time, an indicator of the agents’ ability to maximize rewards. 29
4.9 ClipfractionovertimeforthePPOmodels, showingtheproportionofclippedgradients
during training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
4.10 Explained variance over time, indicating the predictive accuracy of the value function. 30
4.11 Policy gradient loss over time, showing the optimization trajectory of the policy. . . . 31
4.12 Flowchart of the Multi-Agent Training Process in FireExtinguishingEnvParallel . 33
4.13 Time per update/frame per second (FPS) over-training steps . . . . . . . . . . . . . . 35
4.14 Approximate Kullback-Leibler divergence over training steps . . . . . . . . . . . . . . 35
4.15 Clip fraction over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.16 Clip range over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.17 Entropy loss over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.18 Explained variance over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.19 Loss over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.20 Policy gradient loss over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.21 Value loss over training steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4Chapter 1
Introduction
1.1 Background
Wildfires have become one of the most challenging natural disasters to predict, control, and mitigate.
Driven by a combination of climatic, environmental, and anthropogenic factors, their unpredictable
nature and the devastating impact they have on ecosystems and human settlements necessitate the
development of advanced modeling techniques and control strategies [1–4].
Regions such as Canada have been particularly affected, witnessing a surge in wildfire activity.
The urgency of the situation is further highlighted by the vast amounts of resources required for fire
suppression, loss of habitats, and the significant economic repercussions for affected communities.
1.2 Existing Techniques and Their Limitations
Traditional wildfire simulations have utilized a range of methods, from the well-known Rothermel
model calibrated with genetic algorithms [5], to the more recent techniques that harness the power
of complex network modeling [6]. Machine learning has also seen its application in this realm, with
methods such as the Least-Squares Support-Vector Machines (LSSVM) [7] combined with Cellular
Automata (CA) [8]. However, while these methods have made significant strides in wildfire modeling,
they still fall short in dynamically responding to the fast-paced changes that occur during a wildfire
event.
1.3 Need for a Novel Approach
The integration of adaptive network growth models provides a promising avenue, allowing models
to offer dynamic responses to varying conditions. Furthermore, the potential of incorporating au-
tonomous agents such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs)
within a Reinforcement Learning (RL) framework offers a tangible method for actionable control
5strategies during a wildfire event. However, as models scale up in complexity, they also introduce
emergent behaviors, which, while offering sophisticated strategies, can introduce challenges due to
their unpredictability [9].
Given this context, the objective of this report is to delve into the development and application
of the WARP-CA model, exploring its potential in addressing the aforementioned challenges and
contributing to the global efforts in wildfire management.
1.4 Contributions
This project introduces a novel approach in the realm of wildfire simulation and management, with
a particular focus on the integration of world simulation techniques and the exploration of emergent
behaviors through Multi-Agent Reinforcement Learning (MARL). The key contributions of this work
are as follows:
• Integration of World Simulation Techniques: Combined use of Perlin Noise for terrain
generation and CA for fire spread simulation, providing a more comprehensive and nuanced
approach to modeling wildfire dynamics.
• Exploration of Emergent Behaviors in MARL: Application of MARL to investigate how
interactions among multiple autonomous agents, such as UAVs and UGVs, can lead to emergent
behaviors that enhance wildfire suppression and forest conservation.
• Focus on Efficient Wildfire Suppression: Implementation of SARL and MARL frameworks
todevelopandteststrategiesthatpotentiallyacceleratewildfiresuppression,aimingtominimize
ecological damage and resource expenditure.
• Incorporation of Environmental Factors: Accounting for critical environmental variables
such as wind patterns, vegetation types, and terrain features in the simulation to better under-
stand their impact on fire behavior.
These contributions represent focused advancements in the field of wildfire simulation, specifically
in the areas of integrated world simulation and the application of MARL to study and leverage
emergent behaviors for more effective wildfire management.
6Chapter 2
Literature Review
2.1 Comprehensive Review on Wildfire Simulation: Models, Algo-
rithms, and Terrain Generation
This section delves into the dynamic field of wildfire simulation, CA, Rothermel Model, Genetic
Algorithms, and Perlin Noise-based terrain generation, and their integration in simulating forest fire
dynamics.
Wildfire simulation has matured into a multifaceted domain, amalgamating diverse
models to replicate the intricate behavior of fire spread. The Rothermel Model, pivotal in
predicting fire behavior, underpins many modern simulation methods. It calculates the rate of fire
spread based on fuel properties, topographical features, and meteorological conditions, thus providing
a foundational understanding of fire dynamics [10–12]. Integrating Genetic Algorithms (GAs) into
wildfire simulations has marked a significant leap in enhancing predictive accuracy and adaptability.
By emulating evolutionary processes, GAs refine parameters in simulation models, bolstering their
adaptability to diverse environmental contexts. This technique has shown effectiveness in customizing
models like the Rothermel Model for local conditions, thereby augmenting their forecasting accuracy
[5].
Recent research has highlighted Perlin Noise as a novel terrain generation method
in world simulations, contributing to the creation of realistic and varied landscapes.
This technique, utilizing a gradient noise function, crafts terrains that significantly influence fire
behavior and spreading patterns, aligning with efforts to include intricate environmental elements in
fire dynamics models [13, 14].
Ourmethodology,centeredonPerlinNoiseforworldsimulation,presentsapioneeringstrideinthis
field. SegmentingthegeneratedPerlinnoiseintodistinctenvironmentalcategoriesallowsforanuanced
representation of diverse landscapes, crucial for comprehending fire spread across varied terrains [15,
16]. Ourapproach,integratingsophisticatedterrainmodelinginfiresimulations,significantlyadvances
7the replication of real-world fire scenarios [17–19].
An enhanced approach to wildfire simulation entails incorporating comprehensive en-
vironmental data, such as forest maps, terrain elevation, and historical weather patterns.
These datasets provide critical insights into tree types, fuel content, age, elevation, and weather met-
rics, imperative for precise wind modeling via finite state models and fluid mechanics. Resources
like the Canadian Wildland Fire Information System (CWFIS) Datamart, Global Forest Watch Open
DataPortal,GeographicInformationSystemsfromNovascotia.ca,andothers,offervaluablegeospatial
data instrumental in creating a rich simulation environment. This approach aligns with sophisticated
models and calculation systems for studying wildland fire behavior [20–27].
In conclusion, wildfire simulation is a rapidly evolving domain, with the advent of ad-
vanced models like the Adaptive CA, Rothermel Model, and Genetic Algorithms playing
crucial roles. Ourresearchmethodology,especiallyinterraingenerationusingPerlinNoiseandwild-
fire spread simulation using CA, signifies a substantial contribution to this field. It aligns with and
extends current research by introducing new layers of complexity and realism in simulating wildfire
dynamics [8, 16, 28]. Our literature review uncovered a lack of studies employing these specific world
simulation techniques for wildfire simulation.
2.2 Autonomous Wildfire Management and Response Systems: A
MARL Perspective
MARL extrapolates the principles of single-agent reinforcement learning to scenarios with multiple
interacting agents. In MARL, each agent operates within Markov Decision Processes (MDPs), with
their actions intricately influencing the dynamics of states and rewards. This concept is particularly
pertinent in wildfire management, where a myriad of entities such as firefighters, drones, and envi-
ronmental factors interact in a shared ecosystem [29, 30]. Markov Games, in this context, model
the interplay between multiple agents, whose collective decisions and actions determine state transi-
tions and rewards. In wildfire management, these games can simulate various firefighting teams or
autonomous systems operating in tandem or contention [30, 31].
Cooperative MARL settings, where agents unite towards a common objective like wildfire
suppression, can be conceptualized as multi-agent MDPs (MMDPs) or team Markov games. Here,
the collective effort is channeled towards a unified reward function [32, 33]. In competitive MARL
settings, such as zero-sum games, agents’ goals are diametrically opposed, akin to competing land
management agencies in wildfire scenarios. This setting is reflective of scenarios where each party’s
gain is another’s loss [30, 34].
The mixed setting in MARL, or general-sum games, aptly represents wildfire man-
agement scenarios with overlapping yet distinct objectives among diverse groups. This
8setting accommodates the complexities and nuances of real-world situations [35, 36]. Extensive-form
gamesadeptlyhandlescenarioswithimperfectinformation, acommonhurdleinwildfiremanagement.
Agents’ decision-making is based on limited or uncertain environmental information, mirroring the
unpredictability inherent in real-world wildfires [37, 38].
Several studies exemplify MARL applications in wildfire management:
• Collaborative Auto-Curricula: Siedler (2022) investigates a MARL system with a Graph
Neural Network communication layer for efficient resource distribution in wildfire management,
underscoring the value of agent collaboration [39].
• UAVs and Deep Q-Learning: Viseras et al. (2021) explore using UAVs for wildfire front
monitoring, employingdeepQ-learningtounderscorethepotentialofautonomousaerialvehicles
in fire tracking [40].
• Distributed Deep Reinforcement Learning: Haksar and Schwager (2018) present dis-
tributeddeepreinforcementlearningformaneuveringaerialrobotstocombatforestfires,demon-
strating decentralized control’s effectiveness in intricate environments [41].
• Swarm Navigation and Control: Ali et al. (2023) concentrate on distributed multi-agent
deep reinforcement learning for UAV swarm navigation in wildfire monitoring, showcasing ad-
vanced coordination techniques [42].
Integrating MARL into wildfire management is pivotal in developing robust, autonomous systems
for effective wildfire response. MARL’s complex interaction models lay the theoretical groundwork for
designing advanced simulation algorithms and optimizing strategies for wildfire prevention, control,
and mitigation [29, 30]. Future research in this arena could focus on devising sophisticated algorithms
that bolster collaboration among autonomous agents, enhance decision-making amidst uncertainty,
and support real-time adaptive strategies. Utilizing MARL can pave the way for more efficacious
systems in wildfire prediction, monitoring, and response [37, 38].
9Chapter 3
Methodology
3.1 World Simulation
3.1.1 Terrain Generation with Perlin Noise
Perlinnoise,agradientnoisefunction,iscentraltoourapproachforgeneratingheterogeneousterrainin
forestfiredynamicssimulation. ThemathematicalbasisofPerlinnoise,itsparameters,andapplication
in terrain generation are detailed below.
Mathematical Definition of Perlin Noise
Introduced by Ken Perlin, Perlin noise is a gradient noise function that generates coherent noise ideal
forsimulatingnaturalpatterns. Theprocessinvolvesseveralstepsandparameters, detailedasfollows:
Gradient Vector Generation AteachpointP onalattice, apseudo-randomgradientvectorG⃗ is
i i
generated. The distribution and orientation of these vectors significantly influence the characteristics
of the resulting noise pattern.
Noise Calculation For a point ⃗x in space, the Perlin noise value, N(⃗x), is calculated by:
1. Identifying the unit cube that encloses ⃗x.
2. Computing the dot product G⃗ D⃗ for each of the cube’s corners, where D⃗ = ⃗x P⃗ is the
i i i i
· −
vector from the corner P⃗ to ⃗x.
i
3. Applying a smoothing function to interpolate these dot products, yielding the final noise value.
Smoothing Function The smoothing function is typically a polynomial, such as the quintic func-
tionf(t) = 6t5 15t4+10t3, which ensuresthe continuityandsmoothness ofthe noisepattern. Which
−
is illustrated below in Figure 3.1
10Figure 3.1: Smoothed Perlin Noise Map
Parameters of Perlin Noise Several parameters control the characteristics of the Perlin noise:
• Scale (σ): Controlsthegranularityofthenoise. Smallervaluesleadtofinerdetails, whilelarger
values create broader patterns.
• Octaves (ω): Determines the number of layers of noise. Higher octaves add complexity and
finer details.
• Persistence (ρ): Influences the amplitude of each octave. Higher persistence values emphasize
finer details.
• Lacunarity (λ): Adjusts the frequency of each octave. Higher lacunarity increases the fre-
quency, adding more small-scale details.
The final function form representing the Perlin noise surface is a composite of multiple layers of
noise, each with its own scale, persistence, and lacunarity. Mathematically, it can be described as a
sum of successive noise functions, where each function noise (x,y) is scaled and its amplitude adjusted
i
according to the given parameters. The form can be represented as:
ω
(cid:88) (cid:16) x y (cid:17)
f(x,y) = ρi noise ,
· i σ λi σ λi
i=0 · ·
This formula combines the effects of scale, octaves, persistence, and lacunarity to generate a
complex, layered noise pattern.
11Application in Terrain Generation
TerraingenerationusingPerlinnoiseinvolvesmappingthenoisevaluestodifferentterraintypes. This
process is crucial for simulating realistic and varied terrains, impacting the spread and behavior of
wildfires. The mapping can be described as follows:
• Assign noise value ranges to specific terrain types (e.g., mountains, plains, forests).
• Use these classifications to create a diverse landscape that realistically affects fire dynamics.
Advantages and Suitability for Fire Dynamics Simulation
Perlin noise is particularly suitable for forest fire dynamics simulations due to:
• Natural Appearance: It creates terrains that closely mimic real-world landscapes.
• Control and Predictability: Adjustable parameters allow for controlled variation in terrain
features.
• Efficiency: Generates complex landscapes with relatively low computational overhead.
These characteristics make Perlin noise an ideal choice for generating terrains in simulations aimed
at studying the complex behavior of wildfires, as they provide a realistic and varied environment that
significantly influences fire spread patterns.
1. Environmental Categorization The generated Perlin noise is segmented into four environ-
mental categories, each corresponding to a unique terrain type:
• Lake: Noise values less than the lake threshold.
• Wetland: Noise values between lake and wetland thresholds.
• Grassland: Noise values between wetland and grassland thresholds.
• Forest: Noise values greater than or equal to the grassland threshold.
In Figure 3.2 we can observe how the tuning of one parameter σ can affect the generated map
drastically, changing σ can amount to zooming in and out on a map as can be noticed in Figure 3.2.
3.1.2 Wildfires Fire Spread Simulation using CA
Model Description
Thissub-sectionpresentsadetailedcellularautomatonmodelforsimulatingforestfires, encompassing
the dynamics of forest growth, fire spread, burning, and regeneration.
12(a) Scale 50 (b) Scale 100
(c) Scale 150 (d) Scale 200
Figure 3.2: The Effect of Choosing Different Scales on Perlin Noise Generation.
13Parameters
• N ,N : Dimensions of the forest grid.
x y
• S : Maximum growth state of a tree (typically 10).
max
• F : Number of fire and ash states (typically 5, states 11 to 15).
max
• α: Probability of fire spreading to adjacent cells.
• β: Probability of spontaneous tree ignition.
• τ: Growth rate of trees.
• A : Minimum age for trees to burn.
min
Cellular Automaton Rules
1. Initial Forest State: The initial state of the forest is generated randomly based on the maxi-
mum growth state of trees.
forest = round(S rand(N ,N )) (3.1)
max x y
×
2. Tree Growth: Treesgrowaccordingtoagrowthprobability,whichisafunctionoftheircurrent
state and a growth rate parameter τ.
(cid:18) (cid:19)
forest(treeIdx)
p = exp τ (3.2)
Growth
− S ×
max
3. Fire Dynamics: Firedynamicsaremodeledasamulti-stepprocess, includingignition, burning
stages, and transition to ash.
(a) Ignition: Trees can spontaneously ignite based on a probability β, which is influenced by
their growth state.
(cid:16) (cid:17)
forest(treeIdx)
exp 10
Smax × 1
p = β (3.3)
Ignite
× exp(10) × N N
x y
×
(b) Burning Stages: Trees transition through burning stages (states 11 to 15) before turning
into ash.
(c) Transition to Ash: After burning, trees transition to an ash state (state 0).
4. Regeneration: Ash cells have the potential to grow new trees, representing the regeneration
of the forest.
We can observe how the fire spreads across multiple frames of simulation as shown in Figure 3.3,
where an emergent behavior exhibiting what would amount to a simulation of fire spreading in a gird
world just by defining our grid and dictating some simple rules that govern the interactions between
these grid cells.
14(a) Frame 440 (b) Frame 450
(c) Frame 460 (d) Frame 470
Figure 3.3: Progression of Fire Spread using CA
15Fire Propagation and Environmental Factors
Fire propagation is influenced by multiple environmental factors, including terrain, wind, and vege-
tation type. These factors affect the likelihood of fire spread from one cell to its neighbors. Different
vegetation types (forest, grassland, wetland) have distinct probabilities of catching and propagating
fire. These probabilities are adjusted based on environmental conditions like slope and wind.
Slope Calculation The slope between two cells affects fire propagation. It is calculated based on
the height difference between neighboring cells.
(cid:18) (cid:19)
height difference
slope = arctan (3.4)
distance between cells
Wind Influence in Fire Spread
The wind model in our simulation represents a simplified version of the more complex wind behav-
ior in wildland fire spread, as discussed in Forthofer’s research. This simplification is necessary for
computational efficiency and ease of implementation in our cellular automaton framework.
Wind Model Used in Simulation Our model assigns a basic wind vector across the simulation
grid. The wind vector is defined as:
 
U
W⃗ =   (3.5)
V
where U and V are the wind velocity components in the x and y directions, respectively.
Custom Wind Model The wind velocity components used in the simulation are defined by the
following equations:
U = 1 X2+X +Y (3.6)
− −
1+X2 Y2
V = − (3.7)
10
where X and Y represent the spatial coordinates on the simulation grid. The wind direction is
computed as:
θ = arctan2(V,U) (3.8)
wind
Comparison with Advanced Models In contrast, Forthofer’s study utilizes complex Computa-
tional Fluid Dynamics (CFD) and mass-consistent models for accurate wind simulations in varied
terrains. These models account for microscale wind variations and are more precise but computation-
ally intensive.
16Rationale for Simplification Oursimplifiedmodelprovidesapracticalbalance, incorporatingthe
influence of wind on fire spread while maintaining computational efficiency. It allows us to capture the
directional influence of wind on fire propagation without the computational burden of more complex
models. This approach is effective for simulating general wind effects in fire behavior, particularly
suitable for large-scale simulations where detailed wind patterns are less critical. Finally in Figure
3.4 we can see that we successfully integrated CA with Perlin noise-generated terrain, in a simulation
environment influenced by environmental factors described in this section.
(a) Frame 350 (b) Frame 360
(c) Frame 370 (d) Frame 380
Figure 3.4: Progression of the Forest Fire simulation with additional Environmental Complexities
17Algorithm 1 Forest Fire Dynamics Simulation
1: procedure ForestFireSimulation
2: Initialize forest grid of size N x N y with random tree ages
×
3: for each simulation frame do
4: for each cell in forest do
5: if cell contains a tree then
6: Compute p Growth
7: if random value ¡ p Growth then
8: Increase tree age
9: end if
10: end if
11: if cell is on fire then
12: Propagate fire with probability α
13: Update burning states
14: end if
15: if cell contains a tree and is not on fire then
16: Compute p Ignite
17: if random value ¡ p Ignite then
18: Ignite the tree
19: end if
20: end if
21: if cell is in the final burning state then
22: Transition to ash (state 0)
23: end if
24: if cell is ash then
25: Chance to regenerate a tree
26: end if
27: end for
28: Save and display current state of forest
29: end for
30: end procedure
3.2 Single Agent Reinforcement Learning (SARL) Framework
3.2.1 Environment Dynamics
The RL Environment is defined as follows:
18• State Space (S): The state s S is a flattened array representing the agent’s local view,
t
∈
which includes terrain, tree, and fire states.
• Action Space (A): The action space is discrete with actions denoted as A = 0,1,...,9 ,
{ }
corresponding to specific movement and interaction actions.
• Transition Dynamics: Thedynamicsinvolvefirespreadinfluencedbyneighboringcells, wind,
and terrain, as well as the agent’s movement.
• RewardFunction: DefinedasR(s ,a ,s ),itcombinesimmediateactionrewardswithstrate-
t t t+1
gic considerations.
3.2.2 Reinforcement Learning Algorithms
We employ three distinct RL algorithms:
• Proximal Policy Optimization (PPO):Utilizesapolicyπ (a s), withtheobjectivefunction:
θ
|
(cid:104) (cid:105)
LPPO(θ) = E min(r (θ)Aˆ ,clip(r (θ),1 ϵ,1+ϵ)Aˆ )
t t t t t
−
where r (θ) is the probability ratio and Aˆ is the advantage estimate.
t t
• Advantage Actor-Critic (A2C): Employs separate actor and critic networks, with the loss
function:
LA2C(θ) = logπ (a s )Aˆ +λ(V (s ) R )2
θ t t t θ t t
− | −
• Deep Q-Network (DQN): Uses a Q-network for state-action value estimation, with the loss
function:
(cid:20) (cid:21)
LDQN(θ) = E (r+γmaxQ (s′,a′) Q (s,a))2
(s,a,r,s′)∼replay θ− θ
a′ −
3.2.3 Reward Function
The reward function is structured to encourage strategic actions:
• Base Rewards: Movement (R = 0.2), extinguishing fire, and clearing vegetation.
move
−
• Strategic Actions: Significant reward for creating a fire barrier (R = 50) and potential-
barrier
based rewards.
• Penalties: For failed extinguishing attempts and unnecessary clearing.
• Normalization: Rewards are normalized to mitigate scaling issues.
ThefunctionR(s,a,s′) = R (a,s)+R (s,s′)+R (s,s′)encapsulatestheseelements.
base strategic potential
193.2.4 Integration with Fire Dynamics Simulation
In this environment, the agent’s actions directly impact fire spread and forest preservation. The
learning algorithms are tasked with optimizing strategies to minimize fire damage and maximize the
effectiveness of the agent’s actions in controlling and managing forest fires.
3.3 Multi-Agent Reinforcement Learning (MARL) Framework
3.3.1 Defining the Environment
State Space
Inourwildfiremanagementscenario, thestatespaceforeachagenti(afireextinguishingunit)attime
t is denoted as si. It includes factors like the agent’s location, the status of the fire, and surrounding
t
environmental conditions. The global state S is then:
t
S = s1,s2,...,sM
t t t t
{ }
where M is the number of agents (firefighting units).
Action Space
The action space Ai for each agent might include moving in different directions, extinguishing fire, or
coordinating with other agents. The collective action space A is:
A = A1 A2 ... AM
× × ×
Observation Space
Eachagentobservesoi,reflectingitsimmediatesurroundingsandthestateofthefire. Theobservation
t
function is:
Oi : S A Oi
× →
Reward Function
The reward function is critical in guiding agents towards effective fire management. It is defined as:
M
(cid:88)
R(S ,A ,S ) = Ri(si,ai,si )
t t t+1 t t t+1
i=1
Ri rewards actions like extinguishing fire or saving trees and penalizes harmful actions.
203.3.2 Applying PPO in MARL for Wildfire Management
Centralized Training with Decentralized Execution (CTDE)
TrainingPhase Duringtraining,acentralizedapproachisusedwherethejointpolicyΠ = π1,π2,...,πM
{ }
is optimized using global information.
Execution Phase Each agent acts independently based on its policy πi and local observation oi.
t
PPO Loss Function
The PPO loss function in the context of MARL for our wildfire scenario is adapted to optimize the
joint policy Π. The loss function for PPO in this setting, considering a simplified view, is:
(cid:104) (cid:105)
LPPO(θ) = E min(r (θ)Aˆ ,clip(r (θ),1 ϵ,1+ϵ)Aˆ )
t t t t t
−
where r (θ) is the probability ratio of the new policy to the old policy, Aˆ is the advantage estimate
t t
at time t, and ϵ is a hyperparameter.
InMARLforfirefighting, theadvantageAˆ foreachagentiscalculatedconsideringbothindividual
t
andcollectiveperformanceinmanagingthefireandpreservingtheenvironment. Thepolicyisupdated
to maximize this loss function, guiding agents to learn cooperative strategies for effective wildfire
management.
The overall pipeline from terrain generation through world simulation to both SARL and MARL
traininganddeploymentissummarizedinFigure3.5. Theprocessbeginswiththegenerationofterrain
using Perlin Noise, followed by the initialization of fire spread simulation using Cellular Automata,
and the integration of environmental factors. Two parallel streams then proceed independently: one
for SARL and one for MARL. Each stream involves defining the environment, training the agents
using suitable algorithms (such as PPO, A2C, DQN for SARL, and SB3 with PPO for MARL), and
finally deploying the trained agents after performance evaluation.
21Terrain & Fire Dynamics Simulation
Generate Terrain with Perlin Noise
terrain generated
Initialize Fire Spread Simulation
Using Cellular Automata
CA initialized
Integrate Environmental Factors
(Wind, Vegetation)
environment defined environment defined
Single Agent RL (SARL) Multi-Agent RL (MARL)
Define SARL Environment Define MARL Environment
With Parameters With Agent Collaboration
training started MARL environment ready
Train SARL Agent Centralized Training
(e.g., PPO, A2C, DQN) with SB3 (PPO)
SARL training complete MARL training complete
SARL Deployment MARL Deployment
Evaluate SARL Agent Performance Evaluate MARL Performance
evaluation complete evaluation complete
Deploy Trained SARL Agent Deploy Trained MARL Agents
Figure 3.5: Pipeline from World Simulation to SARL and MARL Training and Deployment.
22Chapter 4
Simulation Results and Discussion
4.1 Simulation Environment
In our setup, we have developed a custom simulation for a fire extinguishing scenario, represented as
a grid where an agent can move and perform actions such as extinguishing fire or clearing vegetation.
Theenvironmentisstochastic,withelementssuchaswindandterraininfluencingthebehavioroffires.
The following subsections provide detailed descriptions of the environment and the hyperparameters
used in the simulations.
4.2 Simulation Machine Specifications
Simulations were conducted on a dedicated machine to ensure consistent and reliable computational
performance. The specifications of the machine used for the simulations are detailed below:
\footnotesize
Operating System: Ubuntu 20.04.6 LTS
CPU: 11th Gen Intel® Core™ i7-11800H @ 2.30GHz × 16
GPU: NVIDIA GeForce RTX 3050
RAM: 32 GiB
Disk: 1 TB
4.3 Hyperparameters Definition
Based on the methodology outlined, the hyperparameters for the terrain generation using Perlin noise
and the cellular automaton model for wildfire spread simulation are defined as follows:
23Hyperparameter Symbol Value
Scale σ 15
Octaves ω 6
Persistence ρ 0.5
Lacunarity λ 2
Lake Threshold Lake 0.01
Th
Wetland Threshold Wetland 0.2
Th
Grassland Threshold GrassL 0.01
Th
Maximum Tree Growth State S 10
max
Number of Fire States F 4
max
Probability of Fire Spread α 0.8, 1, 0.1 (forest, grassland, wetland)
Probability of Tree Ignition β 0.65
Tree Growth Rate τ 5
Minimum Burning Age A 2
min
Table 4.1: Hyperparameters for terrain generation and wildfire spread simulation.
4.4 SARL Results
4.4.1 Environment Description
The environment, termed FireExtinguishingEnv, is a grid world where the agent’s objective is to
control and extinguish fires. The state of each cell in the grid represents different types of terrain
and fire intensity. The dynamics of fire spread are influenced by various factors including the type of
vegetation, wind, and the presence of water bodies.
4.4.2 Agent and Model Training
The agent is trained using various reinforcement learning algorithms including PPO, A2C, DQN,
DDPG, and SAC. The training process involves the agent interacting with the environment, receiving
observations, andtakingactionsthataregovernedbyapolicynetwork. Thepolicynetworkisupdated
using gradient ascent on expected returns. The Training process and agent interactions with the
environment is shown in Figure 4.1.
The following code snippet shows the instantiation and training of the agent using the PPO
algorithm:
model = PPO(’MlpPolicy’, env, verbose=1, tensorboard_log=unique_log_dir,
device=device)
model.learn(total_timesteps=timesteps)
4.4.3 Performance Metrics
This section provides an analysis of the training performance of our SARL environment using various
reinforcement learning models, specifically PPO, A2C, and DQN. Performance metrics such as frame
24Step Method Details
Apply Action Effects (within step)
effects applied
Update Environment State (within step) Update Agent Position (within step)
state updated updates applied action taken
Core Loop
Calculate Reward & Check Done (within step) Agent Takes Action (step)
all episodes complete after each step
Initialization & Closing
Close Environment (close) Initialize (init) Render Environment (render) reset complete
initialization complete Episode Done
Reset Environment (reset)
Figure 4.1: Training Flow of the Fire Extinguishing Environment with for a Single Agent.
rate per second, loss, KL divergence, and others are considered.
Figure 4.2 illustrates the FPS for the models. PPO models show a sharp decrease initially, which
might suggest a computational adjustment period. Following this, all models appear to achieve a
stable FPS, which is indicative of consistent computational load during training. Loss metrics, as
shown in Figure 4.3, are crucial for understanding the learning progress. The PPO models display a
general downward trend in loss, punctuated by spikes that suggest episodic learning or exploration
phases. DQN’s loss also decreases but shows more variability, which may indicate a different learning
dynamic or exploration strategy. KL divergence is a measure of policy change. In Figure 4.4, the
PPO models demonstrate fluctuating KL divergence, with peaks corresponding to significant policy
updates. The overall decline indicates stabilization of policy updates over time. Value loss, depicted
in Figure 4.5, follows a similar pattern to the loss metric, with PPO showing a decrease over time,
interspersed with spikes. A2C shows a less smooth trend, which could reflect a different approach
to value estimation. Entropy loss, presented in Figure 4.6, measures the randomness in the policy.
The declining trend seen with the PPO models suggests a reduction in policy entropy, which typically
indicates increased confidence in action selection over time. Figure 4.7 shows the mean episode length.
Sharp drops, particularly with the PPO and A2C models, suggest episodes where the agent may fail
quickly, possibly due to exploring suboptimal policies. The mean episode reward, shown in Figure
4.8, is a direct indicator of agent performance. The general upward trend across models, with some
variability,indicateslearningandimprovementinmaximizingrewards. Clipfraction,asseeninFigure
254.9, is specific to PPO and indicates the fraction of times the clipping mechanism in the loss function
is active. The general trend is downward, suggesting the policy is moving towards a stable region
where clipping is less necessary. Explained variance, illustrated in Figure 4.10, shows how well the
value function predicts the actual returns. The values hovering around zero for A2C indicate poor
prediction, whereas PPO shows some periods of better prediction. Finally, policy gradient loss, in
Figure 4.11, measures the optimization progress of the policy. PPO models display a stable trend
with minor fluctuations, which suggests a steady optimization process. In summary The analysis
of the training performance metrics indicates that PPO models, on average, stabilize in terms of
computational load, learning progress, and policy development. A2C and DQN exhibit different
patterns, which may reflect their unique approaches to learning and exploration.
time/fpsoverTime
2250
PPO1
PPO2
PPO3
2000
A2C1
DQN1
1750
1500
1250
1000
750
500
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.2: Frame rate per second over time for different models.
26
spf/emittrain/lossoverTime
175 PPO1
PPO2
PPO3
150
DQN1
125
100
75
50
25
0
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.3: Training loss over time for different models.
train/approx kloverTime
PPO1
PPO2
2.0
PPO3
1.5
1.0
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.4: Approximate KL divergence over time for PPO models.
27
ssol/niart
lkxorppa/niarttrain/value lossoverTime
300
PPO1
PPO2
PPO3
250
A2C1
200
150
100
50
0
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.5: Value loss over time for the PPO and A2C models.
train/entropy lossoverTime
0.0
0.5
−
1.0
−
1.5
−
PPO1
2.0 PPO2
−
PPO3
A2C1
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.6: Entropy loss over time, indicating the exploration behavior of the models.
28
ssolyportne/niart
ssoleulav/niartrollout/ep len meanoverTime
1000
995
990
985
980
PPO1
PPO2
975
PPO3
A2C1
970 DQN1
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.7: Mean episode length over time, reflecting the consistency of the agents’ performance.
rollout/ep rew meanoverTime
PPO1
400 PPO2
PPO3
A2C1
300 DQN1
200
100
0
100
−
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.8: Mean episode reward over time, an indicator of the agents’ ability to maximize rewards.
29
naemnelpe/tuollor
naemwerpe/tuollortrain/clip fractionoverTime
PPO1
PPO2
PPO3
0.4
0.3
0.2
0.1
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.9: Clip fraction over time for the PPO models, showing the proportion of clipped gradients
during training.
train/explained varianceoverTime
1
0
1
−
2
−
PPO1
PPO2
3
− PPO3
A2C1
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.10: Explained variance over time, indicating the predictive accuracy of the value function.
30
noitcarfpilc/niart
ecnairavdenialpxe/niarttrain/policy gradient lossoverTime
PPO1
0.25
PPO2
PPO3
0.20
0.15
0.10
0.05
0.00
0.05
−
0.0 0.2 0.4 0.6 0.8 1.0
Steps 106
×
Figure 4.11: Policy gradient loss over time, showing the optimization trajectory of the policy.
4.5 MARL Results
4.5.1 Environment Description
The FireExtinguishingEnvParallel class is a multi-agent reinforcement learning environment that
simulatesascenariowhereagents(representedasbulldozers)workcollaborativelytoextinguishfiresin
a forest. This environment extends the ParallelEnv class from PettingZoo, indicating its suitability
for parallel agents’ interactions. Unlike single-agent environments, FireExtinguishingEnvParallel
requires coordination and collaboration among multiple agents. Each agent operates independently,
making decisions based on its observation space, but their actions collectively impact the overall
environment and the task’s success.
Interaction with Frameworks
Being a PettingZoo environment, it adheres to PettingZoo’s API for multi-agent environments, en-
abling easy integration and interaction with other PettingZoo-compatible tools and libraries. Super-
Suit can be used to wrap and transform this environment for advanced functionalities like vectoriza-
tion, frame-stacking, etc., making it more flexible for different training setups. To integrate with SB3,
a popular reinforcement learning library, the environment needs to be converted into a vectorized
form using SuperSuit. This allows leveraging SB3’s algorithms for training agents in the environ-
ment. While primarily based on PettingZoo’s API, the environment maintains compatibility with
Gym-like interfaces through wrappers, ensuring it can be used with Gym-based tools and libraries.
31
ssoltneidargycilop/niartFireExtinguishingEnvParallelpresentsacomplexanddynamicmulti-agentreinforcementlearning
challenge, ideal for studying collaborative strategies and agent interactions in a shared environment.
4.5.2 Multi-Agent and Model Training
The training process begins with the initialization of the multi-agent environment. This environment
simulatesascenariowheremultipleagentscollaboratetoextinguishfiresinaforestedgrid. Parameters
such as grid size, vision range, and the number of agents are defined at this stage.
Following initialization, the environment is wrapped with the PettingZoo API to standardize the
multi-agent interface. This step ensures compatibility with further processing and vectorization tools.
The environment is then vectorized and concatenated using SuperSuit, preparing it for integration
with Stable Baselines3 (SB3), a reinforcement learning library. Centralized training is conducted
using SB3’s Proximal Policy Optimization (PPO) algorithm. In this phase, a shared policy model is
trainedacrossallagents, enablingthemtolearncooperativestrategies. Themodelundergoesiterative
training, where agent experiences (observations, actions, rewards) are collected and used to improve
the policy.
During training, checkpoints and model states are saved periodically. This allows for the preserva-
tion of learning progress and facilitates model evaluation and deployment. Upon successful training,
the model is deployed for decentralized execution. Each agent, following the shared policy, indepen-
dently decides its actions based on its local observations. The environment responds to these actions,
updating its state and providing new observations and rewards to the agents.
Thisloopofaction-takingandenvironmentresponsecontinuesuntiltheterminationconditionsare
met (e.g., all fires extinguished or maximum steps reached). The training and execution process in the
FireExtinguishingEnvParallel environment demonstrates the efficiency of combining centralized
training with decentralized execution in a multi-agent setting. This approach enables agents to learn
collaborativebehaviorswhileretainingtheabilitytoactindependentlybasedonlocalizedinformation.
The training process can be shown in Figure 4.12, which intricately showcases how the Multi-agent
training process is realized.
32Initialization & Wrapping
Initialize Multi-Agent
Environment with Parameters
environment initialized
Wrap with PettingZoo API
wrapping complete
Vectorize & Concatenate
with SuperSuit
vectorization complete
Centralized Training
Centralized Training
with SB3 (PPO)
training steps
Checkpoint & Model Saving model loaded
model deployment
Decentralized Execution
Decentralized Agent Execution next training step
actions taken
Environment Step
environment update
Collect Rewards, Observations
& Environment Interaction
Figure 4.12: Flowchart of the Multi-Agent Training Process in FireExtinguishingEnvParallel
334.6 Performance Metrics
4.7 Performance Metrics Discussion
During the training of our multi-agent reinforcement learning framework using the PPO algorithm,
we observed several key performance metrics which are vital for understanding the learning dynamics
and effectiveness of the model. Figures 4.13, 4.14, 4.15, 4.16, 4.17, 4.18, 4.19, 4.20, and 4.21 showcase
these metrics over the course of training steps.
Figure 4.13 shows the time and frames per second (FPS) over the training steps. The FPS pro-
vides insight into the computational efficiency of the training process. An overall downward trend or
significant drops in FPS may indicate computational bottlenecks or increased complexity in the simu-
lation as the training progresses. The Approximate KL Divergence, depicted in Figure 4.14, measures
how the policy distribution changes over time. Spikes in KL divergence can suggest significant policy
updates, which could either be beneficial as the agents explore new strategies or detrimental if the
policy diverges too much from the previous one, potentially leading to instability in training. The
Clip Fraction, shown in Figure 4.15, indicates the fraction of the agent’s probability ratio that was
clipped by the PPO algorithm. This metric helps in understanding the extent to which the policy is
being constrained and can also reflect the stability of the training process. Figure 4.16 shows the clip
range, which is the range within which the probability ratio is constrained. A constant clip range, as
observed, suggests a stable constraint over the policy updates throughout the training. Entropy loss,
visualized in Figure 4.17, describes the randomness in the policy distribution. A higher entropy can
encourage exploration by the agents, while a lower entropy indicates a more deterministic policy. Ide-
ally, entropy should decrease over time as the policy converges to an optimal strategy. The explained
variance metric, in Figure 4.18, measures how well the value function predicts the rewards. Values
closer to 1 indicate better predictions, which can lead to more efficient learning by the agents. Figures
4.19, 4.20, and 4.21 provide insights into the loss experienced by the policy and value functions. The
loss metrics are critical in monitoring the convergence of the learning process. Sharp increases could
indicate potential issues that need to be addressed for smoother learning.
34time/fpsoverTime
18000
PPO-Multi1
16000
14000
12000
10000
8000
6000
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Steps 107
×
Figure 4.13: Time per update/frame per second (FPS) over-training steps
train/approx kloverTime
0.200
PPO-Multi1
0.175
0.150
0.125
0.100
0.075
0.050
0.025
0 100000 200000 300000 400000
Steps
Figure 4.14: Approximate Kullback-Leibler divergence over training steps
35
spf/emit
lkxorppa/niarttrain/clip fractionoverTime
PPO-Multi1
0.35
0.30
0.25
0.20
0.15
0.10
0 100000 200000 300000 400000
Steps
Figure 4.15: Clip fraction over training steps
train/clip rangeoverTime
0.2100 PPO-Multi1
0.2075
0.2050
0.2025
0.2000
0.1975
0.1950
0.1925
0.1900
0 100000 200000 300000 400000
Steps
Figure 4.16: Clip range over training steps
36
egnarpilc/niart
noitcarfpilc/niarttrain/entropy lossoverTime
PPO-Multi1
0.5
−
1.0
−
1.5
−
2.0
−
0 100000 200000 300000 400000
Steps
Figure 4.17: Entropy loss over training steps
train/explained varianceoverTime
1.0
PPO-Multi1
0.8
0.6
0.4
0.2
0.0
0 100000 200000 300000 400000
Steps
Figure 4.18: Explained variance over training steps
37
ssolyportne/niart
ecnairavdenialpxe/niarttrain/lossoverTime
30 PPO-Multi1
25
20
15
10
5
0
0 100000 200000 300000 400000
Steps
Figure 4.19: Loss over training steps
train/policy gradient lossoverTime
0.005 PPO-Multi1
0.000
0.005
−
0.010
−
0.015
−
0.020
−
0.025
−
0.030
−
0 100000 200000 300000 400000
Steps
Figure 4.20: Policy gradient loss over training steps
38
ssoltneidargycilop/niart
ssol/niarttrain/value lossoverTime
16
PPO-Multi1
14
12
10
8
6
4
2
0
0 100000 200000 300000 400000
Steps
Figure 4.21: Value loss over training steps
39
ssoleulav/niartChapter 5
Conclusion
In this project, we have explored the application of single and multi-agent reinforcement learning
frameworks to the challenging problem of wildfire management. The sophisticated simulation envi-
ronment we developed integrates CA for fire spread and Perlin noise for terrain generation. This
environment not only captures the complexity of wildfire dynamics but also provides a fertile ground
for RL agents to learn and optimize strategies for fire extinguishing and forest preservation.
Key Findings
The experimentation phase yielded several key insights. Firstly, we found that the Proximal Pol-
icy Optimization (PPO) algorithm adapted well to the intricacies of the fire management scenario,
achieving a balance between exploration and exploitation, as evidenced by the performance metrics
discussed in previous chapters. The training process demonstrated a convergence of policy, which is
indicative of the agents’ increasing proficiency in tackling the wildfire management task. Our MARL
framework showed promise in fostering collaborative behaviors among agents. The decentralized ex-
ecution post-training allowed agents to operate independently yet effectively, considering the global
objectives of minimizing fire damage and preserving the forest, adhering to CTDE training conditions
of independent agents.
Despite these successes, we encountered several challenges. Computational efficiency remained a
concern, particularly in the MARL setting, where the increase in agents led to a more complex state
space and action space. Moreover, the initial instability during training, reflected by the fluctuations
in the frames per second (FPS), suggested that our computational resources were stretched at times.
Additionally, while the PPO algorithm was effective, it was not without shortcomings. The occa-
sional spikes in the KL divergence and loss metrics hinted at moments where the agents’ policies could
potentially deviate from optimal behaviors. Furthermore, the fixed parameters of Perlin noise and the
simplified wind model, although computationally efficient, might have limited the agents’ exposure to
more varied and realistic fire scenarios.
40Final Thoughts
Theresultsofourexperimentationhavedemonstratedthepotentialofapplyingreinforcementlearning
to the domain of wildfire management. The project has laid a foundation for future research to build
upon,withtheultimategoalofdevelopingAIagentscapableofassistinginthecriticaltaskofmanaging
and mitigating wildfires, a growing concern in the face of climate change.
The journey from algorithmic inception to practical application is often fraught with unexpected
challenges and learning opportunities. In our case, while we achieved a significant measure of success,
the path ahead is clear—there is much room for improvement, refinement, and, most importantly,
innovation.
Future Directions
Lookingforward,thereareseveralavenuestoenhanceourwildfiremanagementsimulationandlearning
frameworks. Investigating more complex and dynamic models for wind and weather patterns could
offer a more realistic challenge to the agents. Advanced MARL algorithms that can efficiently handle
the increased complexity of such models would be worth exploring.
In addition, adapting the learning algorithms to leverage parallel computational resources more
effectively could address the computational bottlenecks observed. The inclusion of human-in-the-loop
training could also provide a unique perspective, allowing the agents to learn from human expertise
in wildfire management.
Introducing Conservation Metric
Reward Shaping (Conservation Metric)
Objective The reward system is designed to incentivize agents to preserve as many trees as possi-
ble, efficiently extinguish fires, and collaboratively work without causing unnecessary environmental
damage. The system aims to balance the actions taken against fires with the imperative to conserve
the forest.
Definitions Let’s define the variables used in the reward functions:
• T : Number of trees preserved (untouched by fire or agents).
p
• T : Number of trees successfully extinguished (saved from burning).
e
• T : Number of trees cleared (by agents, potentially to create firebreaks).
c
• T : Number of trees burned (lost to fire).
b
• A : Total number of actions taken by all agents.
total
41• T : Total time taken to control the fires.
total
• r : Reward for protecting each unit area from fire.
protect,area
• α,β,γ,δ,ϵ,ζ,η,θ : Coefficients representing the weight of each component in the reward calcu-
i
lation.
Reward Functions 1. Efficiency-Oriented Reward Function This function rewards agents
for preserving trees and extinguishing fires while penalizing unnecessary clearing and trees burned. It
also considers the total number of actions taken to encourage efficient decision-making.
R = α T +β T γ T δ T ϵ A
p e c b total
· · − · − · − ·
2. Time-BasedRewardFunctionThisfunctionemphasizestheimportanceoftimeinmanaging
fires. It rewards quick and effective action while penalizing the total time taken and trees lost.
R = α T +β T γ T δ T ζ/T
p e c b total
· · − · − · −
3. Spatial Coverage Reward Function This function rewards agents for protecting large areas
of the forest from fire, in addition to the other components of preserving, extinguishing, clearing, and
burned trees.
(cid:88)
R = α T +β T γ T δ T + r
p e c b protect,area
· · − · − ·
area
4. Collaborative Reward FunctionThisfunctionpromotesteamworkbyscalingrewardsbased
on collaborative efforts, encouraging agents to work together efficiently.
R = η (α T +β T γ T δ T )
p e c b
· · · − · − ·
5. Dynamic Adaptive Reward FunctionThisfunctiondynamicallyadjuststherewardsbased
on specific environmental states or scenarios, encouraging adaptive behavior.
N
(cid:88)
R = α T +β T γ T δ T + θ r
p e c b i dynamic,i
· · − · − · ·
i=1
Purpose of the Design The reward design’s purpose is to create a balanced and holistic approach
to managing wildfires in a simulated environment. It encourages preservation, efficient action, and
collaboration, while also allowing for adaptive responses to dynamic situations.
42Bibliography
[1] C. Tymstra, B. J. Stocks, X. Cai, and M. D. Flannigan, “Wildfire management in Canada:
Review, challenges and opportunities,” Progress in Disaster Science, vol. 5, p. 100045, Jan. 2020.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/S2590061719300456
[2] “The “Canadian” Wildfires of 2023,” Jun. 2023. [Online]. Available: https://carleton.ca/
thedisasterlab/2023/the-canadian-wildfires-of-2023/
[3] “International Journal of Wildland Fire (IJWF).” [Online]. Available: https://www.iawfonline.
org/international-journal-wildland-fire-ijwf/
[4] A. B. C. News, “Wildfires in Canada have broken records for area burned, evacuations
and cost, official says.” [Online]. Available: https://abcnews.go.com/International/wireStory/
wildfires-canada-broken-records-area-burned-evacuations-cost-100806230
[5] J. Pereira, J. Mendes, J. S. S. Ju´nior, C. Viegas, and J. R. Paulo, “A Review of Genetic
Algorithm Approaches for Wildfire Spread Prediction Calibration,” Mathematics, vol. 10, no. 3,
p. 300, Jan. 2022, number: 3 Publisher: Multidisciplinary Digital Publishing Institute. [Online].
Available: https://www.mdpi.com/2227-7390/10/3/300
[6] S. Perestrelo, M. C. Gr´acio, N. A. Ribeiro, and L. M. Lopes, “Modelling Forest Fires Using
Complex Networks,” Mathematical and Computational Applications, vol. 26, no. 4, p. 68, Dec.
2021, number: 4 Publisher: Multidisciplinary Digital Publishing Institute. [Online]. Available:
https://www.mdpi.com/2297-8747/26/4/68
[7] Y. Xu, D. Li, H. Ma, R. Lin, and F. Zhang, “Modeling forest fire spread using machine learning-
based cellular automata in a gis environment,” Forests, vol. 13, no. 12, p. 1974, 2022.
[8] M. Byari, A. Bernoussi, O. Jellouli, M. Ouardouz, and M. Amharref, “Multi-Scale 3d
Cellular Automata Modeling: Application to Wildland Fire Spread,” SSRN Electronic
Journal, vol. null, p. null, 2022. [Online]. Available: https://www.semanticscholar.org/paper/
6716d5a3790b3825c3fba326bf017103f1092787
43[9] S.Liu,G.Lever,J.Merel,S.Tunyasuvunakool,N.Heess,andT.Graepel,“Emergentcoordination
through competition,” International Conference on Learning Representations, 2019.
[10] H. Yin, H. Jin, Y. Zhao, Y.-L. Fan, L. Qin, Q. Chen, L. Huang, X. Jia, L. Liu,
Y. Dai, and Y. Xiao, “The simulation of surface fire spread based on Rothermel
model in windthrow area of Changbai Mountain (Jilin, China),” 2018. [Online]. Available:
https://www.semanticscholar.org/paper/1e215cc03da2044d252b7107f91d1bcc36a84d74
[11] S. Zhang, J. Liu, H. Gao, X. Chen, X. Li, and J. Hua, “Study on Forest Fire
spread Model of Multi-dimensional Cellular Automata based on Rothermel Speed Formula,”
CERNE, vol. null, p. null, 2021. [Online]. Available: https://www.semanticscholar.org/paper/
e9f829f0187edaa09259e1233881fed0b55d9d29
[12] I. Wahyuni, A. Shabrina, and A. Latifah, “Investigating Multivariable Factors of the
Southern Borneo Forest and Land Fire based on Random Forest Model,” Proceedings of
the 2021 International Conference on Computer, Control, Informatics and Its Applications,
vol. null, p. null, 2021. [Online]. Available: https://www.semanticscholar.org/paper/
c48c80cf0327f9d0ac55b4a3bc77090aa155b7ce
[13] R. Fischer, P. Dittmann, R. Weller, and G. Zachmann, “AutoBiomes: procedural generation
of multi-biome landscapes,” The Visual Computer, vol. 36, no. 10, pp. 2263–2272, Oct. 2020.
[Online]. Available: https://doi.org/10.1007/s00371-020-01920-7
[14] E. Mastorakos, S. Gkantonas, G. Efstathiou, and A. Giusti, “A hybrid stochastic Lagrangian
– cellular automata framework for modelling fire propagation in inhomogeneous terrains,”
Proceedings of the Combustion Institute, vol. null, p. null, 2022. [Online]. Available:
https://www.semanticscholar.org/paper/8df63a09953ca72e07077f1b533e18278f7af182
[15] T. Le, “Procedural Terrain Generation Using Perlin Noise,” publisher: California State Polytech-
nic University, Pomona.
[16] A. Jain, A. Sharma, and Rajan, “Adaptive & Multi-Resolution Procedural Infinite Terrain
Generation with Diffusion Models and Perlin Noise,” in Proceedings of the Thirteenth Indian
Conference on Computer Vision, Graphics and Image Processing, ser. ICVGIP ’22. New York,
NY, USA: Association for Computing Machinery, May 2023, pp. 1–9. [Online]. Available:
https://doi.org/10.1145/3571600.3571657
[17] D. Rosadi, W. Andriyani, D. Arisanty, and D. Agustina, “Prediction of Forest
Fire using Hybrid SOM-AdaBoost Method,” Journal of Physics: Conference Series,
vol. 2123, p. null, 2021. [Online]. Available: https://www.semanticscholar.org/paper/
788662cc8bb97848476ab0448a15d5f90ba64398
44[18] Y. Zhao and D. Geng, “Simulation of Forest Fire Occurrence and Spread Based on Cellular
Automata Model,” 2021 2nd International Conference on Artificial Intelligence and Information
Systems, vol. null, p. null, 2021. [Online]. Available: https://www.semanticscholar.org/paper/
01a2c3f6864d9ecd8bd05b0bd3e8f480fd201bc6
[19] L. Sun, C. Xu, Y. He, Y. Zhao, Y. Xu, X. Rui, and H. Xu, “Adaptive Forest Fire Spread
Simulation Algorithm Based on Cellular Automata,” Forests, vol. null, p. null, 2021. [Online].
Available: https://www.semanticscholar.org/paper/501958ef42496f1fca3b5bc07346dca2c73b6bf2
[20] N. R. Canada, “Forest cover map,” Jun. 2013, last Modified: 2023-
08-21 Publisher: Natural Resources Canada. [Online]. Available: https:
//natural-resources.canada.ca/our-natural-resources/forests/sustainable-forest-management/
measuring-and-reporting/remote-sensing-forestry/forest-cover-map/13433
[21] “Global Forest Watch Open Data Portal.” [Online]. Available: https://data.globalforestwatch.
org/
[22] “GeographicInformationSystems-ForestryInformationAvailableforDownload novascotia.ca.”
|
[Online]. Available: https://novascotia.ca/natr/forestry/gis/downloads.asp
[23] “Canada’s National Forest Information System.” [Online]. Available: https://ca.nfis.org/
index eng.html
[24] “topoView USGS.” [Online]. Available: https://ngmdb.usgs.gov/maps/topoview/
|
[25] “Canada topographic map, elevation, terrain.” [Online]. Available: https://en-ca.
topographic-map.com/map-kc957/Canada/
[26] E. Pastor, L. Z´arate, E. Planas, and J. Arnaldos, “Mathematical models and calculation systems
for the study of wildland fire behaviour,” Progress in Energy and Combustion Science, vol. 29,
no. 2, pp. 139–153, 2003.
[27] J. M. Forthofer, “Modeling wind in complex terrain for use in fire spread prediction,” Ph.D.
dissertation, Citeseer, 2007.
[28] H. D. Bhakti, H. Ibrahim, F. Fristella, and M. Faisal, “Fire spread simulation
using cellular automata in forest fire,” IOP Conference Series: Materials Science and
Engineering, vol. 821, p. null, 2020. [Online]. Available: https://www.semanticscholar.org/
paper/eb97ff6ccb62e5446e67b67c8e48fdd3dd5101be
[29] L. S. Shapley, “Stochastic games,” Proceedings of the National Academy of Sciences, vol. 39,
no. 10, pp. 1095–1100, 1953.
45[30] M. L. Littman, “Markov games as a framework for multi-agent reinforcement learning,” in Inter-
national Conference on Machine Learning, 1994, pp. 157–163.
[31] T. Ba¸sar and G. J. Olsder, Dynamic Noncooperative Game Theory. SIAM, 1999, vol. 23.
[32] C. Boutilier, “Planning, learning and coordination in multi-agent decision processes,” in Confer-
ence on Theoretical Aspects of Rationality and Knowledge, 1996, pp. 195–210.
[33] H.-T. Wai, Z. Yang, Z. Wang, and M. Hong, “Multi-agent reinforcement learning via double
averaging primal-dual optimization,” in Advances in Neural Information Processing Systems,
2018, pp. 9649–9660.
[34] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton et al., “Mastering the game of Go without human knowledge,” Nature, vol.
550, no. 7676, p. 354, 2017.
[35] J. Hu and M. P. Wellman, “Nash Q-learning for general-sum stochastic games,” Journal of Ma-
chine Learning Research, vol. 4, no. Nov, pp. 1039–1069, 2003.
[36] OpenAI, “Openai five,” https://blog.openai.com/openai-five/, 2018.
[37] M. J. Osborne and A. Rubinstein, A Course in Game Theory. MIT Press, 1994.
[38] J. Heinrich, M. Lanctot, and D. Silver, “Fictitious self-play in extensive-form games,” in Inter-
national Conference on Machine Learning, 2015, pp. 805–813.
[39] P. D. Siedler, “Collaborative Auto-Curricula Multi-Agent Reinforcement Learning with
Graph Neural Network Communication Layer for Open-ended Wildfire-Management Resource
Distribution,” Apr. 2022, arXiv:2204.11350 [cs]. [Online]. Available: http://arxiv.org/abs/2204.
11350
[40] A. Viseras, M. Meissner, and J. Marchal, “Wildfire front monitoring with multiple
uavs using deep q-learning,” IEEE Access, 2021, publisher: IEEE. [Online]. Available:
https://ieeexplore.ieee.org/abstract/document/9340340/
[41] R. N. Haksar and M. Schwager, “Distributed deep reinforcement learning for fighting forest fires
with a network of aerial robots,” in 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). IEEE, 2018, pp. 1067–1074.
[42] A. Ali, R. Ali, and M. F. Baig, “Distributed Multi-Agent Deep Reinforcement Learning based
Navigation and Control of UAV Swarm for Wildfire Monitoring,” in 2023 IEEE 4th Annual
Flagship India Council International Subsections Conference (INDISCON). IEEE, 2023, pp.
1–8. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/10270198/
46