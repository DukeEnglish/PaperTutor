Value-Penalized Auxiliary Control from Examples for
Learning without Rewards or Demonstrations
TrevorAblett1,BryanChan2,JayceHaoranWang1,JonathanKelly1
1UniversityofToronto,2UniversityofAlberta
Abstract: Learning from examples of success is an appealing approach to re-
inforcement learning that eliminates many of the disadvantages of using hand-
crafted reward functions or full expert-demonstration trajectories, both of which
canbedifficulttoacquire,biased,orsuboptimal. However,learningfromexam-
ples alone dramatically increases the exploration challenge, especially for com-
plex tasks. This work introduces value-penalized auxiliary control from exam-
ples(VPACE);wesignificantlyimproveexplorationinexample-basedcontrolby
addingscheduledauxiliarycontrolandexamplesofauxiliarytasks. Furthermore,
weidentifyavalue-calibrationproblem,wherepolicyvalueestimatescanexceed
theirtheoreticallimitsbasedonsuccessfuldata.Weresolvethisproblem,whichis
exacerbatedbylearningauxiliarytasks,throughtheadditionofanabove-success-
level value penalty. Across three simulated and one real robotic manipulation
environment, and 21 different main tasks, we show that our approach substan-
tially improves learning efficiency. Videos, code, and datasets are available at
https://papers.starslab.ca/vpace.
Keywords: InverseReinforcementLearning,MultitaskLearning,Manipulation
1 Introduction
Robotics presents a unique challenge to learning algorithms: feedback is often formulated as a
manually-defineddenserewardfunction,ordemonstrationtrajectoriesofanexpertcompletingthe
task,bothofwhichcanbedifficulttoacquire,suboptimal,orbiased.Ensuringthatexperttrajectories
arenearlyoptimal,or,alternatively,learninganoptimalpolicyfrommixedorsuboptimaldata,are
bothchallenging,openproblemsinimitationlearning[1]. Sparserewardfunctionsarelessbiased
[2],butrequiresignificantexplorationandcanbenon-trivialtoacquire.
We consider another form of feedback—example states of completed tasks. Obtaining example
states can be far less laborious than designing a reward function or gathering trajectories: practi-
tionerscangatherstatesfromadistributionthatrepresentsacompletedtaskwithoutconsideration
ofhowthestatesarereachedbytheexpert. Learningfromexamplestates,equivalentlyreferredto
as example-based control [3], can be inefficient, however. Similar to sparse rewards, the example
statesprovidenoinformationaboutwhichactionsledtothegoalstate(s). Inroboticsapplications,
this inefficiency is particularly undesirable, since executing suboptimal policies is costly both in
termsofpotentiallydestructiveenvironmentaleffectsandtime. Inthiswork,weaimtoaddressthe
followingquestion:
Isitpossibletolearnpoliciesefficientlygivenonlyexamplestatesofcompletedtasks?
Toanswerthisquestion,weproposeanewexample-basedcontrolmethod,value-penalizedauxiliary
control from examples (VPACE). Our approach is inspired by LfGP [4], a method that intro-
duces the use of a scheduler and expert trajectories of auxiliary tasks to improve exploration. We
build upon this idea to leverage only example states rather than expert trajectories. Our contribu-
tions are fourfold: (i) We find that the na¨ıve application of scheduled auxiliary tasks to example-
4202
luJ
3
]OR.sc[
1v11330.7042:viXraFigure1: VPACEinastandardreinforcementlearningloopandmultitaskactor-criticupdate. Valuepenaliza-
tionensuresthatallexamplestatebuffershavehighervaluethanallotherstates,whileauxiliarycontrolfrom
examplesallowsustolearnfromauxiliarytaskexamplesinsteadofmaintasksalone.
based control can result in poorly-calibrated value estimates, and we alleviate this problem by
introducing value penalization based on the current expected value of the provided examples.
Thecombinationandcomplimentarynatureofthesetwoadditions
AllEnvs/Tasks(Average)
resultsinadramaticimprovementinsampleefficiencyandperfor- 1.0
mance across four environments with 19 simulated and two real 0.8
tasks (see Fig. 2) compared with prior state-of-the-art example- 0.6
based control (RCE, [3]) and imitation learning (DAC, [5], SQIL 0.4
0.2
[6]) approaches. (ii) We situate our approach among these base-
0.0
lines, showing the effects of mixing value penalization and auxil- 0.0 0.2 0.4 0.6 0.8 1.0
Steps(normalized)
iarycontrolundervariousreward-modellingassumptions. (iii)We
VPACE RCE
demonstratethatVPACEcanbeassample-efficientaslearningal-
SQIL DAC
gorithmswithotherformsoffeedbackincludingexperttrajectories
and reward functions. (iv) Finally, we theoretically show that our Figure2:Averagenormalizedre-
value-penalized objective does not prevent an agent from learning sults for all tasks studied in this
work.
theoptimalpolicy.
2 RelatedWork
Sparserewardsareadesirableformoffeedbackforlearningunbiased,optimalpoliciesinreinforce-
mentlearning(RL),buttheycanbedifficulttoobtain,andpresentanimmenseexplorationchallenge
onlong-horizontasks[7]. Rewardshaping[8]anddenserewardscanhelpalleviatetheexploration
problem in robotics [9, 10, 11], but designing dense rewards is difficult for practitioners [12], and
canleadtosurprising,biased,andsuboptimalpolicies. Analternativetomanually-definedrewards
istoperforminverseRL(IRL),inwhicharewardfunctionisrecoveredfromdemonstrations, and
apolicyislearnedeithersubsequently[13,14,15]orsimultaneously,inaprocessknownasadver-
sarialimitationlearning(AIL)[5,6,16,17,18]. AILactor-criticapproachescanbefurtherdivided
intomethodsthatlearnbothavaluefunctionandaseparaterewardmodel[16,17]ormethodsthat
learnavaluefunctiononly[5,6,18].
Like dense rewards, full trajectory demonstrations can be hard to acquire, suboptimal, or biased.
UnlikeIRL/AIL,inexample-basedcontrol(EBC),alearningagentisonlyprovideddistributionsof
single successful example states. Previous EBC approaches include using generative AIL (GAIL,
[16])directly(VICE,[19]),softactorcritic(SAC,[20])withanadditionalmechanismforgenerat-
ing extra success examples (VICE-RAQ, [21]), learning the goal distribution as a reward function
(DisCo RL, [22]), performing offline RL with conservative Q learning (CQL, [23]) and a learned
rewardfunction[24],andusingSACwithaclassifier-basedreward(RCE,[3]).
AllEBCmethodscannaturallysufferfrompoorexploration,giventhatsuccessexamplesareakin
to sparse rewards. Hierarchical reinforcement learning (HRL) aims to leverage multiple levels of
abstractioninlong-horizontasks[25], improvingexplorationinRLboththeoretically[26,27,28]
and empirically [29, 30]. Scheduled auxiliary control (SAC-X, [31]) combines a scheduler with
semantically meaningful and simple auxiliary sparse rewards. SAC-X has also been extended to
thedomainofimitationlearningwithfulltrajectories(LfGP,[4,32,33]). Ourapproachbuildson
2
)dezilamron(nruteRaspects of VICE-RAQ [21], SQIL [6], RCE [3], and LfGP [4], in that we use off-policy learning
withoutaseparaterewardmodeltomaximizeefficiency.
3 Example-BasedControlwithValue-PenalizationandAuxiliaryTasks
Ourgoalistogenerateanagent,withasfewenvironmentinteractionsaspossible,thatcancomplete
ataskgivenfinalstateexamplesofboththesuccessfullycompletedtaskandasmallsetofreusable
auxiliary tasks, with no known reward function or full-trajectory demonstrations. We begin by
formallydescribingtheproblemsettingforexample-basedcontrolinSection3.1.InSection3.2,we
describehowscheduledauxiliarytaskscanbeappliedtoexample-basedcontrol. Finally,motivated
bytheincreasedexplorationdiversityofthemultitaskframework,weproposeanewQ-estimation
objectiveinSection3.3thatleveragesvaluepenalizationforimprovedlearningstability.
3.1 ProblemSetting
AMarkovdecisionprocess(MDP)isdefinedas = , ,R, ,ρ ,γ ,wherethesets and
0
M ⟨S A P ⟩ S A
arerespectivelythestateandactionspace, isthestate-transitionenvironmentdynamicsdistribu-
tion,ρ istheinitialstatedistribution,γ istP hediscountfactor,andthetruerewardR: R
0
S ×A→
is unknown. Actions are sampled from a stochastic policy π(as). The policy π interacts with
|
the environment to yield experience (s ,a ,s ), generated by s ρ (),a π( s ), and
t t t+1 0 0 t
∼ · ∼ ·|
s ( s ,a ).Thegatheredexperience(s ,a ,s )isthenstoredinabuffer ,whichmaybe
t+1 t t t t t+1
∼P ·| B
usedthroughoutlearning. Whenreferringtofinite-horizontasks,t=T indicatesthefinaltimestep
ofatrajectory. Foranyvariablesx ,x , wemaydropthesubscriptsandusex,x′ insteadwhen
t t+1
thecontextisclear.
Inthiswork,wefocusonexample-basedcontrol,amoredifficultformofimitationlearningwhere
we are only given a finite set of example states s∗ ∗, where ∗ and ∗ < ,
∈ B B ⊆ S |B | ∞
representing a completed task. The goal is to (i) leverage ∗ and to learn or define a state-
conditional reward function Rˆ : R that satisfies Rˆ(s∗B ) Rˆ(sB ) for all (s∗,s) ∗ ,
S → ≥ (cid:104) ∈ B ×B(cid:105)
and (ii) learn a policy πˆ that maximizes the expected return πˆ = argmax E (cid:80)∞ γtRˆ(s ) .
π π t=0 t
For any policy π, we can define the value function and Q-function respectively to be:
Vπ(s)=E a∼π[Qπ(s,a)], (1) Qπ(s,a)=Rˆ(s)+γE s′∼P[Vπ(s′)], (2)
corresponding to the return-to-go from state s (and action a). Both the value function and the Q-
function for any policy π satisfy the above Bellman equations that can be used for reinforcement
learning(RL),specificallybytemporaldifference(TD)algorithms.
Onewaytolearntherewardfunctionisthroughadversarialimitationlearning(AIL)—thelearned
rewardfunctionRˆisderivedfromtheminimaxobjective[5,16,17]:
(D)=E s∼B[log(1 D(s))]+E s∗∼B∗[log(D(s∗))], (3)
L −
whereDattemptstodifferentiatetheoccupancymeasurebetweenthestatedistributionsinducedby
∗and .TheoutputofD(s)isusedtodefineRˆ(s),whichisthenusedforupdatingtheQ-function
B B
withEq.(2). Inthesingle-taskregime,thisisalgorithmicallyidenticaltostate-onlyAILapproaches
withfulldemonstrations[34,35].
3.2 LearningaMultitaskAgentfromExamples
Example-basedcontrolpresentsachallengingexplorationproblemforcomplextasks. Wealleviate
thisproblembyadaptinglearningfromguidedplay(LfGP)[4], anapproachforimprovingexplo-
rationbylearningfromauxiliary-taskexpertdata,inadditiontomaintaskdata. Auxiliarytasksare
selectedtohavesemanticmeaning(e.g. reach,lift). Individualtaskdefinitions,andsometimesthe
auxiliaryexampledatathemselves,arereusablebetweenmaintasks.
3LfGP augments an MDP to contain auxiliary tasks, where = ,..., are separate
aux 1 K
M T {T T }
MDPs that share , , ,ρ and γ with the main task . Each task has an example
0 main all
S A P T T ∈ T
statebuffer ∗, where = isthesetofalltasksandwerefertoalltask-specific
BT Tall Taux ∪{Tmain }
entitiesinourmodelas() .
T
·
The agent consists of multiple components for each task , specifically Q , π , and optionally
T T
T
D . Eachπ maximizesthejointpolicyobjective
T T
(cid:88)
(π)= E [Q (s,a)], (4)
L
s∼Ball,a∼πT(·|s) T
T∈Tall
andeachQ minimizesthejointBellmanresidual
T
(Q)= (cid:88) E (s,·,s′)∼B(cid:2) (Q T y T(s,s′))2(cid:3) +E s∗∼B∗ (cid:2) (Q T y T(s∗,s∗))2(cid:3) , (5)
L − T −
T∈Tall
wherey areTDtargetsdefinedbasedonEq.(2)withtask-specificrewardRˆ .
T T
Finally, a scheduler selects π to sample actions from during each environment interaction. Fol-
T
lowing [4, 31], the scheduler has a fixed period, allowing for a set number of policy switches in
eachepisode. Weuseaweightedrandomschedulerandasmallsetofsimplehandcraftedhigh-level
trajectories(e.g.,reachthenlift),followingresultsfrom[4],wherethisapproachperformedaswell
asamorecomplexlearnedscheduler.Theprobabilityofchoosingthemaintaskisahyperparameter
p ,andeachauxiliarytaskischosenwithprobabilityp =(1 p )/K. SeeAppendixD.6.2
Tmain Tk
−
Tmain
formoreschedulerdetails,includingthefullsetofhigh-leveltrajectories.
3.3 ValuePenalizationinExample-BasedControl
Ascheduledmultitaskagentwill, bydesign, exhibitfarmorediversebehaviourthanasingle-task
policy [4, 31]. We show in Section 4.2 that the buffer generated by this behavior, consisting of
transitionsresultingfrommultiplepoliciesπ , canresultinhighlyunstableandpoorlycalibrated
T
Q estimates, especially in example-based control. In this section, we extend TD-error minimiza-
tion, a fundamental component of many off-policy RL algorithms, with a penalty that encourages
Q-functionoutputstostaywell-calibratedwithrespecttotherewardmodel.Generally,thechoiceof
therewardmodelandthelossfunctionforestimatingtheQ-functioncangreatlyimpactlearningeffi-
ciency(seeAppendicesAandBfordetails),butourvalue-penaltytermappliestoanyrewardmodel
andcommonlyusedregression-basedlossfunctions.Thispenaltyappliestoboththesingle-taskand
multitaskregime. Forsimplicity,wedescribevaluepenalizationforthesingle-taskframework.
ConsiderarewardmodelRˆ()whereRˆ(s∗),s∗ ∗,indicatesrewardforsuccessfulstates,while
Rˆ(s),s , for all other st· ates. In AIL, Rˆ is∈ a fB unction of D, while in SQIL [6], for example,
Rˆ(s∗) =∈ 1B and Rˆ(s) = 0. Assuming that s∗ transitions to itself, then for policy evaluation with
the mean-squared error (MSE), we can write the TD target, y : R, of Q-updates as
S × S →
y(s,s′)=Rˆ(s)+γE a′[Q(s′,a′)], (6) y(s∗,s∗)=Rˆ(s∗)+γE a′[Q(s∗,a′)], (7)
where(s, ,s′) , a′ π( s′), ands∗ ∗. Eq.(7)canbereplacedwithy(s∗,s∗) = Rˆ(s∗)
· ∼ B ∼ ·| ∼ B
if one considers successful states to be terminal, but this can cause bootstrapping errors when a
task times out or does not terminate upon success [36], both of which are common practice in
roboticsenvironments. RegressingtoTDtargetsEqs.(6)and(7)willeventuallysatisfytheBellman
equation, but in the short term the targets do not satisfy y(s,s′) y(s∗,s∗). This is because the
≤
TD targets leverage bootstrapping of the current Q-estimate, an estimate that may not satisfy the
BellmanequationandcanexceedtheboundsofvalidQ-values,implyingthatapproximationerror
ofQupdatedwiththeMSEcanbeuncontrollable.
WeintroduceasimpleresolutiontothisissuebyaddingapenaltytoourTDupdatesfors based
onthecurrentestimateofE s∗∼B∗[Vπ(s∗)]. ThispenaltytermenforcestheQ-estimateto∈ fB ocuson
outputtingvalidvaluesanddrivestheTDtargetstosatisfytheinequalityy(s,s′) y(s∗,s∗). We
≤
introducebothaminimumandmaximumvalueforQπ(s,a)respectivelyas
4RealDrawer
1.10.00
0.75
0.05.08
0.25
0.00.06
0.00 0.05 0.10 0.15 0.20 0.25 0.30
RealDoor
1.00.04
0.75
0.05.02
0.25
0.00.00
00..00 00..12 00.2.4 00.3.6 0.04.8 0.15.0
Env.Steps( 100k)
×
VPACE-SQIL
SQIL
(a)Samplesofρ ,B∗ ,andB∗ . (b)Realrobotresults.
0 aux main
Figure3: Left: Examplesfromtheinitialstatedistributionρ ,auxiliarytaskexamplebuffersB∗ ,andmain
0 aux
task example buffers B∗ for Unstack-Stack, Insert, sawyer box close, relocate-human-v0(-dp),
main
RealDrawer, andRealDoor(seeAppendixD.4foralltasks). Right: Performanceresultsforourtworeal
robotictasks.
Qπ
min
=Rˆ min/(1 −γ), (8) Qπ
max
=E s∗∼B∗[Vπ(s∗)], (9)
whereRˆ Rˆ(s)foralls . Then,thevaluepenaltyis
min
≤ ∈B
(cid:104) (cid:105)
π (Q)=λE (max(Q(s,a) Qπ ,0))2 +(max(Qπ Q(s,a),0))2 , (10)
Lpen (s,a)∼B − max min−
whereλ 0isahyperparameter.Noticethatwhenλ ,Eq.(10)becomesahardconstraintthat
≥ →∞
is only satisfied by functions with valid outputs. It immediately follows that y(s,s′) y(s∗,s∗)
≤
holdswithTDupdatesEqs.(6)and(7).
We add value penalization π (Q) to the MSE loss as a regularization term for learning the Q-
Lpen
function. In Appendix C, we show that the optimal solution of this regularized loss achieves low
error on the unregularized loss. Consequently, if we use the regularized loss in policy evaluation,
approximatepolicyiteration(API)stillconvergestotheoptimalsolution.Westateheretheinformal
resultthatboundsthevalueerror(seeTheorem1inAppendixCfortheformaltheorem):
InformalTheorem. Supposethefunctionclass hasboundedoutputandcontainsQπ forallπ,
Q
and suppose we use the value-penalization loss for policy evaluation in API with N samples for
eachiteration. Then,afterkiterationsofAPI,wehavethatwithprobabilityatleast1 δ,
−
(cid:32) (cid:114) (cid:33)
1 log(k/δ)
∥Q∗ −Qπk
∥2,ρ0
≤O˜
(1 γ) 2N
,
−
whereO˜ hidestheconstantsandconcentrabilitycoefficients.
4 Experiments
Through our experiments, we seek to understand if VPACE improves stability and efficiency in
example-basedcontrol. Wealsocompleteanablationstudyofvarioushyperparameteroptions,and
finallyanalyzethelearnedvaluesforagentswithandwithoutvaluepenalization.
4.1 ExperimentalSetup
We learn agents in a large variety of tasks and environments, including those originally used in
LfGP [4] and RCE [3]. Specifically, the tasks from [4] involve a simulated Franka Emika Panda
manipulator, ablueandgreenblock, afixed“bring”areaforeachblock, andasmallslotwith<1
mmtoleranceforinsertingeachblock. Theenvironmenthasasingle,sharedobservationspaceand
action space with multiple options for main and auxiliary tasks. We additionally study all tasks
5
etaRsseccuSUnstack-Stack Insert sawyerboxclose relocate-human-v0-dp
1.10.00 1.00
2000
0.07.58 0.75 0.2
1500
0.6
0.50 0.50 1000
0.4 0.1
0. 02 .5 2 0.25 500
0.00.00 0.00 0.0 0
00.0 1 2 3 4 0.25 0 2 4 60.4 8 10 0 1 0.62 3 4 5 00.8 3 6 9 12 151.0
EnvironmentSteps( 100k)
×
PandaMainTasks SawyerMainTasks AdroitMainTasks AdroitDPMainTasks
11..00 1.0 1.0 1.0
00..88 0.8 0.8 0.8
00..66 0.6 0.6 0.6
00..44 0.4 0.4 0.4
00..22 0.2 0.2 0.2
00..00 0.0 0.0 0.0
00..00 0.2 0.4 0.6 0.8 0.21.0 0.0 0.2 0.4 0.60.4 0.8 1.0 0.0 0.2 0.06.4 0.6 0.8 1.0 0.00.8 0.2 0.4 0.6 0.8 11..00
Steps(normalized)
VPACE-SQIL VPACE-DAC ACE-SQIL ACE-RCE
VP-SQIL VP-DAC SQIL RCE
Figure4: Toprow: Performanceresultsforfourofthemostchallengingmaintasksweconsidered. Bottom
row: Averagenormalizedperformanceforallmaintasksweconsidered,separatedbyenvironment. Amore
detailedtaskperformancebreakdowncanbefoundinAppendixE.Performanceonauxiliarytasksisnotshown.
MethodsthatleverageACEareshownwithsolidlines,andtheirsingle-taskcounterpartsareshownwithdashed
lines. The shaded area corresponds to half standard deviation across five seeds (top), or a quarter standard
deviationacrosstasks(bottom).Toimproveclarity,performanceateachstepispresentedasanaverageacross
thefiveclosesttimesteps. Bothvalue-penalizationandauxiliarytaskexplorationarebeneficialforthemost
complicatedtasks.
from [3]: a slightly modified subset of those from [11], involving a simulated Sawyer arm, and
threeoftheAdroithandtasksoriginallypresentedin[37]. WealsogeneratethreemodifiedAdroit
handenvironmentsthatusedelta-positionactionspaces,insteadoftheabsolutepositionsfromthe
originalenvironments,becausewehavefoundthatpolicieslearnedintheoriginalenvironmentsare
verycoarseandexploitsimulatorbugs(seeAppendixD.2fordetails). Finally,westudydrawerand
dooropeningtaskswitharealFrankaEmikaPanda.
Therewardmodelsthatweconsiderarealearneddiscriminator(Eq.(3), [5]),SQIL(Eq.(11)and
Eq. (12), [6]), and RCE (Eq. (13) and Eq. (14), [3]). For each of these reward models, we can
includevaluepenalization(VP),auxiliarycontrolfromexamples(ACE),orboth(VPACE).VPACE-
RCE and VP-RCE are not considered because RCE circumvents the issue of value penalization
throughitsclassification-basedapproachtoupdatingvaluefunctions(seeAppendixA.3).
AllVPACEandACEalgorithmslearnfromexamplesofcompletedauxiliarytasks,inadditiontothe
maintask. InthesimulatedPandaenvironment,allmaintasksuserelease,reach,grasp,andliftas
auxiliarytasks,andeachauxiliarytaskexampledatasetisreusedbetweenmaintasks.IntheSawyer,
Adroit, andrealPandaenvironments, eachmaintaskusesreachandgraspasauxiliarytasks. Our
schedulerapproachforallenvironmentsisthesameweightedrandomschedulerwithasmallsetof
handcraftedtrajectoriesfrom[4]. Allimplementationsarebuiltonsoftactor-critic(SAC)[20]. For
moreenvironment,algorithm,task,andimplementationdetails,seeAppendixD.
4.2 MainTaskPerformanceResults
Ourresultsforallalgorithmsforfourofthemostchallengingmaintasksweexaminedareshown
at the top of Fig. 4, while the bottom shows normalized average results for all tasks, separated by
environment. Our real Panda results are shown in Fig. 3b. Since the Sawyer and Adroit tasks do
not include specific success evaluation metrics, we only report returns. Policies are evaluated at
25k(environmentstep)intervalsfor50episodesforthesimulatedPandatasks,10kintervalsfor30
episodesfortheSawyerandAdroittasks,and5kintervalsfor10episodesfortherealPandatasks.
Our results clearly show the benefits of combining auxiliary task exploration with value penaliza-
tion. Formosttasks,VPACE-SQILlearnsfasterandmorestablythananyothermethod. Notably,
6
nruteRroetaRsseccuS
)dezilamron(nruteRUnstack-Stack
11..00 VPACE-SQIL,noablations
VP-SQIL,noablations
00..88 +FullTrajectories
+FullTrajectories&Actions
00..66
VP-SQIL+FullTrajectories
00..44 SAC-X(SparseRewards)
λ=1
00..22 λ=100
10Examples
00..00
00.0 01.2 02.4 30.6 40.8 51.0 100Examples
EnvironmentSteps( 100k) Max
×
(a)Ablationresults. (b)Q-valuecalibrationwithvaluepenalization.
Figure5: Left: Resultsforchangesindatatype,λvalue,andB∗ size. Right: DifferencebetweenQ-values
T
andmeanQforexamplestatesforagentsat500kstepsforasingleepisoderollout,withoutACEontheleft
andwithACEontheright. Shadedregionsshowshalfastandarddeviationbetweenseeds,whilethedashed
linesshowthemaximumoutputacrosstheseeds.Valuepenalizationensuresthatthemaximumstaysbelow0,
verifyingthaty(s,s′)≤y(s∗,s∗).
VPACE-SQIL can solve all tasks, apart from relocate-human-v0, by the final evaluation step.
WhileVPACE-DACalsotendstoperformquitewell,itislesssampleefficientthanVPACE-SQIL
onaverage,mostlikelyowingtoVPACE-SQIL’sreducedcomplexityandbetterdiscriminationbe-
tween and ∗. For the Adroit non-DP tasks, VP-SQIL slightly outperforms methods including
B B
ACE, while SQIL alone performs comparably to VPACE methods, but we suspect that this is be-
causethesetaskshavenuancesallowingthemtobe“solved”withverycoarsepoliciesthatmaybe
undesirable for a real platform (see Appendix D.2 for details). ACE-RCE is consistently outper-
formed by both VPACE-SQIL and VPACE-DAC, and exhibits higher performance variance. We
furtherexaminethisresult,whichconflictswithresultspresentedin[3]showingverypoorperfor-
mance for both SQIL and DAC, in Appendix B. On our real tasks (RealDrawer and RealDoor),
SQILmakesminorprogressbutisneverabletosolveeithertask,whileVPACE-SQILsolvesboth.
Valuepenalizationleadstoadramaticimprovementinmanyofthemorecomplextasks,butpresents
comparableresultstoitsexclusioninothertasks. Crucially, therearenotaskswhereitsinclusion
harms performance. VPACE-SQIL, VPACE-DAC, and ACE-RCE all outperform their single-task
counterparts, particularlyforthemostcomplextasks. TheimprovementforDACinthesimulated
Panda environment is particularly stark; reflecting results from [4], DAC learns deceptive rewards
forthesetasksand,inturn,neverlearnstocompleteanyofthemexceptforReach.
4.3 Ablations,DataQuantity,andComparisontoFullTrajectoriesandSparseRewards
WecompletedexperimentswithmanyvariationsfromouroriginalimplementationofVPACE-SQIL
inUnstack-Stack(seeFig.5a).Ourmainexperimentsusedλ=10forvaluepenalizationstrength,
butwealsotestedλ= 1,100 ,andfoundanegligibleeffectonperformance,indicatingrobustness
{ }
toλchoice. Wetestedtwodifferentsizesof ∗ (foralltasks in ), ∗ = 10,100 ,while
BT T Tall |BT| { }
themainexperimentsused ∗ =200(following[3]). Wefoundthat ∗ =100hadanegligible
|BT| |BT|
effectonperformance,but ∗ = 10slowedlearningandimpairedfinalperformance. Evenwith
|BT|
∗ =10,VPACE-SQILoutperformedallbaselineswith ∗ =200,apartfromVPACE-DAC.
|BT| |BT|
A natural question regarding our approach is how its performance compares to more traditional
approaches,suchasusingfullexperttrajectoriesandinversereinforcementlearning(IRL),orusing
RL with true sparse rewards. To test the former, we added full trajectories to each ∗ (labelled
BT
+Full Trajectories in Fig. 5a, +Full Trajectories & Actions for learning from actions as well, and
VP-SQIL +Full Trajectories for single-task only), effectively making our approach similar to [4]
but with value-penalization. Intriguingly, peak performance is reduced in this setting (especially
withoutACE),whichwehypothesizeisbecausetheagentnowhastominimizedivergencebetween
and ∗ formanynon-successfulstates, leadingtoaneffect, commonlyseenwithdensereward
B BT
functions,knownasrewardhacking[38]. ThisresultsuggeststhatinverseRL/adversarialILcanbe
significantlyimprovedbyswitchingtoexample-basedcontrol,butfurtherinvestigationisrequired.
TotestRLwithsparserewards,weremoved ∗ entirely,andinsteaduseagroundtruthR(s,a)from
BT
7
etaRsseccuSthe environment (R(s,a) already having been used for evaluating success rate), similar to SAC-X
[31]withvalue-penalization. SAC-X(SparseRewards)doesnotstartaccomplishingthemaintask
at all in 500k environment steps, exhibiting substantially poorer performance than VPACE-SQIL
and all other baselines (apart from DAC), indicating the high potential utility of immediately and
consistentlysamplingexpertsuccessdata.
4.4 ValuePenalizationandCalibrationofQ-Values
While our performance results show that value penalization improves performance, they do not
explicitly show that y(s,s′) y(s∗,s∗) (see Section 3.3). To verify that this goal was met,
≤
we took snapshots of each learned agent at 500k steps and ran each policy for a single episode,
recording per-timestep Q-values. Instead of showing Q-values directly, we show Q(s ,a )
t t
E [Q(s∗,a∗)], which should be close to 0 when an episode is completed succes− s-
s∗∼B∗,a∗∼π(·|s∗)
fully,andshouldneverclimbabove0fory(s,s′) y(s∗,s∗)tohold. Weshowtheresultsofdoing
≤
soinUnstack-StackinFig.5b(seeAppendixE.2forothertasks).
BothVP-SQILandVPACE-SQILclearlydonotviolatey(s,s′) y(s∗,s∗),whilebothSQILand
≤
ACE-SQILdo.ACE-SQIL,inparticular,hasnovalues,onaverage,wherey(s,s′) y(s∗,s∗),indi-
≤
catingthatithaslearnedpoorlycalibratedestimatesforQ.Thisisreflectedinourmainperformance
results,where,inthemostdifficultPandatasksinparticular,theimprovementfromACE-SQILto
VPACE-SQILispronounced. Consequently, scheduledauxiliarycontrolcanproducepoorer poli-
ciesthanitssingle-taskalternativeunlessitiscoupledwithvaluepenalization.
5 Limitations
VPACE suffers from several limitations, though many of them are inherited from the use of rein-
forcementlearningandlearningfromguidedplay. Foranexpansionofthissection,includingthese
inheritedlimitations,seeAppendixF.Inthiswork,weexclusivelylearnfromnumericalstatedata,
ratherthanrawimages,andrawimagesmayberequiredfortasksinvolvingobjectsthatarenotrigid.
Aswell,weclaimthatexampledistributionsareeasiertogeneratethanfullexperttrajectories,but
forcertaintasks,generatingtheseexampledistributionsmayalsobechallenging. Finally,taskswe
investigateinthisworkhaveroughlyunimodalexamplesuccessstatedistributions,andourmethod
maynotgracefullyhandlemultimodality.
6 Conclusion
In this work, we presented VPACE—value-penalized auxiliary control from examples, where we
coupledscheduledauxiliarycontrolwithvaluepenalizationintheexample-basedsettingtosignifi-
cantlyimprovelearningefficiencyandstability. Ourexperimentsrevealedthatscheduledauxiliary
control can exacerbate the learning of poorly-calibrated value estimates, which can significantly
harm performance, and we alleviated this issue with an approach to value penalization based on
the current value estimate of example data. We theoretically showed that our approach to value
penalizationstillaffordsanoptimalpolicy. Weempiricallyshowedthatvaluepenalization,together
withscheduledauxiliarytasks,greatlyimproveslearningfromexamplestatesagainstasetofstate-
of-the-artbaselines,includinglearningalgorithmswithotherformsoffeedback. Opportunitiesfor
future work include the further investigation of learned approaches to scheduling, as well as au-
tonomouslygeneratingauxiliarytaskdefinitions.
8Acknowledgments
We gratefully acknowledge the Digital Research Alliance of Canada and NVIDIA Inc., who pro-
videdtheGPUsusedinthisworkthroughtheirResourcesforResearchGroupsProgramandtheir
HardwareGrantProgram,respectively.
References
[1] S.Levine,A.Kumar,G.Tucker,andJ.Fu. OfflineReinforcementLearning:Tutorial,Review,
andPerspectivesonOpenProblems. arXiv:2005.01643[cs,stat],May2020.
[2] A.Y.NgandM.I.Jordan. ShapingandPolicySearchinReinforcementLearning. PhDthesis,
UniversityofCalifornia,Berkeley,2003.
[3] B.Eysenbach,S.Levine,andR.Salakhutdinov.ReplacingRewardswithExamples:Example-
BasedPolicySearchviaRecursiveClassification,Dec.2021.
[4] T. Ablett, B. Chan, and J. Kelly. Learning From Guided Play: Improving Exploration for
AdversarialImitationLearningWithSimpleAuxiliaryTasks. IEEERoboticsandAutomation
Letters,8(3):1263–1270,Mar.2023. ISSN2377-3766. doi:10.1109/LRA.2023.3236882.
[5] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-Actor-
Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning.
InProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR’19),New
Orleans,LA,USA,May2019.
[6] S.Reddy,A.D.Dragan,andS.Levine.SQIL:ImitationLearningViaReinforcementLearning
with Sparse Rewards. In 8th International Conference on Learning Representations, ICLR
2020,AddisAbaba,Ethiopia,April26-30,2020.OpenReview.net,2020.
[7] A. Gupta, A. Pacchiano, Y. Zhai, S. M. Kakade, and S. Levine. Unpacking Reward Shap-
ing: UnderstandingtheBenefitsofRewardEngineeringonSampleComplexity. InS.Koyejo,
S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInfor-
mationProcessingSystems35:AnnualConferenceonNeuralInformationProcessingSystems
2022,NeurIPS2022,NewOrleans,LA,USA,November28-December9,2022,2022.
[8] A. Y. Ng, D. Harada, and S. J. Russell. Policy Invariance Under Reward Transformations:
Theory and Application to Reward Shaping. In Proceedings of the Sixteenth International
ConferenceonMachineLearning(ICML’99),ICML’99,pages278–287,SanFrancisco,CA,
USA,June1999.MorganKaufmannPublishersInc. ISBN978-1-55860-612-8.
[9] I.Popov,N.Heess,T.Lillicrap,R.Hafner,G.Barth-Maron,M.Vecerik,T.Lampe,Y.Tassa,
T. Erez, and M. Riedmiller. Data-efficient Deep Reinforcement Learning for Dexterous Ma-
nipulation,Apr.2017.
[10] C.Berner, G.Brockman, B.Chan, V.Cheung, P.Debiak, C.Dennison, D.Farhi, Q.Fischer,
S. Hashme, C. Hesse, R. Jo´zefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. de
OliveiraPinto,J.Raiman,T.Salimans,J.Schlatter,J.Schneider,S.Sidor,I.Sutskever,J.Tang,
F. Wolski, and S. Zhang. Dota 2 with Large Scale Deep Reinforcement Learning. CoRR,
abs/1912.06680,2019.
[11] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-World: A
BenchmarkandEvaluationforMulti-TaskandMetaReinforcementLearning. InL.P.Kael-
bling, D. Kragic, and K. Sugiura, editors, 3rd Annual Conference on Robot Learning, CoRL
2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceed-
ingsofMachineLearningResearch,pages1094–1100.PMLR,2019.
9[12] M. Andrychowicz, D. Crow, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. To-
bin,P.Abbeel,andW.Zaremba. HindsightExperienceReplay. InI.Guyon,U.vonLuxburg,
S.Bengio,H.M.Wallach,R.Fergus,S.V.N.Vishwanathan,andR.Garnett,editors,Advances
inNeuralInformationProcessingSystems30:AnnualConferenceonNeuralInformationPro-
cessingSystems2017,December4-9,2017,LongBeach,CA,USA,pages5048–5058,2017.
[13] A. Ng and S. Russell. Algorithms for inverse reinforcement learning. In International Con-
ference on Machine Learning (ICML’00), pages 663–670, July 2000. ISBN 1-55860-707-2.
doi:10.2460/ajvr.67.2.323.
[14] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In In-
ternationalConferenceonMachineLearning(ICML’04),Banff,Alberta,Canada,2004.ACM
Press. doi:10.1145/1015330.1015430.
[15] B.D.Ziebart,A.L.Maas,J.A.Bagnell,andA.K.Dey.MaximumEntropyInverseReinforce-
ment Learning. In D. Fox and C. P. Gomes, editors, Proceedings of the Twenty-Third AAAI
Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008,
pages1433–1438.AAAIPress,2008. ISBN9781577353683(ISBN).
[16] J. Ho and S. Ermon. Generative Adversarial Imitation Learning. In Conference on Neural
InformationProcessingSystems,pages4565–4573,Barcelona,Spain,Dec.2016.
[17] J.Fu, K.Luo, andS.Levine. LearningRobustRewardswithAdverserialinverseReinforce-
mentLearning. InProceedingsoftheInternationalConferenceonLearningRepresentations
(ICLR’18),Vancouver,BC,Canada,Apr.2018.
[18] I. Kostrikov, O. Nachum, and J. Tompson. Imitation Learning via Off-Policy Distribution
Matching. In Proceedings of the International Conference on Learning Representations
(ICLR’20),Virtual,Apr.2020.
[19] J.Fu,A.Singh,D.Ghosh,L.Yang,andS.Levine. VariationalInverseControlwithEvents: A
GeneralFrameworkforData-DrivenRewardDefinition. InConferenceonNeuralInformation
ProcessingSystems,Montreal,Canada,Dec.2018.
[20] T.Haarnoja,A.Zhou,P.Abbeel,andS.Levine. SoftActor-Critic: Off-PolicyMaximumEn-
tropyDeepReinforcementLearningwithaStochasticActor. InProceedingsofthe35thInter-
nationalConferenceonMachineLearning(ICML’18),pages1861–1870,Stockholm,Sweden,
July2018.
[21] A.Singh,L.Yang,K.Hartikainen,C.Finn,andS.Levine.End-to-EndRoboticReinforcement
LearningwithoutRewardEngineering. arXiv:1904.07854[cs,stat],Apr.2019.
[22] S. Nasiriany, V. H. Pong, A. Nair, A. Khazatsky, G. Berseth, and S. Levine. DisCo RL:
Distribution-ConditionedReinforcementLearningforGeneral-PurposePolicies,Apr.2021.
[23] A.Kumar,A.Zhou,G.Tucker,andS.Levine.ConservativeQ-LearningforOfflineReinforce-
mentLearning.InH.Larochelle,M.Ranzato,R.Hadsell,M.-F.Balcan,andH.-T.Lin,editors,
AdvancesinNeuralInformationProcessingSystems33: AnnualConferenceonNeuralInfor-
mationProcessingSystems2020,NeurIPS2020,December6-12,2020,Virtual,2020.
[24] K.B.Hatch,B.Eysenbach,R.Rafailov,T.Yu,R.Salakhutdinov,S.Levine,andC.Finn. Con-
trastiveExample-BasedControl. InN.Matni,M.Morari,andG.J.Pappas,editors,Learning
forDynamicsandControlConference,L4DC2023,15-16June2023,Philadelphia,PA,USA,
volume211ofProceedingsofMachineLearningResearch,pages155–169.PMLR,2023.
[25] R. S. Sutton, D. Precup, and S. Singh. Between MDPs and Semi-MDPs: A Framework for
Temporal Abstraction in Reinforcement Learning. Artificial Intelligence, 112(1-2):181–211,
Aug.1999. ISSN00043702. doi:10.1016/S0004-3702(99)00052-1.
10[26] R.Fruit,M.Pirotta,A.Lazaric,andE.Brunskill. RegretMinimizationinMDPswithOptions
withoutPriorKnowledge. InI.Guyon,U.vonLuxburg,S.Bengio,H.M.Wallach,R.Fergus,
S. V. N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, December
4-9,2017,LongBeach,CA,USA,pages3166–3176,2017.
[27] Z.Wen,D.Precup,M.Ibrahimi,A.Barreto,B.V.Roy,andS.Singh. OnEfficiencyinHierar-
chicalReinforcementLearning. InH.Larochelle,M.Ranzato,R.Hadsell,M.-F.Balcan,and
H.-T. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Confer-
enceonNeuralInformationProcessingSystems2020, NeurIPS2020, December6-12, 2020,
Virtual,2020.
[28] A.Robert,C.Pike-Burke,andA.A.Faisal. SampleComplexityofGoal-ConditionedHierar-
chicalReinforcementLearning. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,
andS.Levine,editors,AdvancesinNeuralInformationProcessingSystems36: AnnualCon-
ference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
USA,December10-16,2023,2023.
[29] A.Nair,B.McGrew,M.Andrychowicz,W.Zaremba,andP.Abbeel. OvercomingExploration
in Reinforcement Learning with Demonstrations. In Proceedings of the 2018 IEEE Interna-
tionalConferenceonRoboticsandAutomation(ICRA’18),pages6292–6299,Brisbane,Aus-
tralia,May2018. doi:10.1109/ICRA.2018.8463162.
[30] O.Nachum,H.Tang,X.Lu,S.Gu,H.Lee,andS.Levine. WhyDoesHierarchy(Sometimes)
WorkSoWellinReinforcementLearning? InProceedingsoftheNeuralInformationProcess-
ingSystems(NeurIPS’19)DeepReinforcementLearningWorkshop,Sept.2019.
[31] M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess,
andJ.T.Springenberg. LearningbyPlayingSolvingSparseRewardTasksfromScratch. In
Proceedings of the 35th International Conference on Machine Learning (ICML’18), pages
4344–4353,Stockholm,Sweden,July2018.
[32] T.Ablett,B.Chan,andJ.Kelly. LearningfromGuidedPlay: AScheduledHierarchicalAp-
proach for Improving Exploration in Adversarial Imitation Learning. In Proceedings of the
Neural Information Processing Systems (NeurIPS’21) Deep Reinforcement Learning Work-
shop,Dec.2021.
[33] G.Xiang,S.Li,F.Shuang,F.Gao,andX.Yuan.SC-AIRL:Share-CriticinAdversarialInverse
ReinforcementLearningforLong-HorizonTask.IEEERoboticsandAutomationLetters,9(4):
3179–3186,Apr.2024. ISSN2377-3766. doi:10.1109/LRA.2024.3366023.
[34] W.-D. Chang, S. Fujimoto, D. Meger, and G. Dudek. Imitation Learning from Observation
throughOptimalTransport,Oct.2023.
[35] F. Torabi. Imitation Learning from Observation. In Proceedings of the AAAI Conference
onArtificialIntelligence, volume33, pages9900–9901, July2019. doi:10.1609/aaai.v33i01.
33019900.
[36] F. Pardo, A. Tavakoli, V. Levdik, and P. Kormushev. Time Limits in Reinforcement Learn-
ing. In J. G. Dy and A. Krause, editors, Proceedings of the 35th International Conference
onMachineLearning,ICML2018,Stockholmsma¨ssan,Stockholm,Sweden,July10-15,2018,
volume80ofProceedingsofMachineLearningResearch,pages4042–4051.PMLR,2018.
[37] A.Rajeswaran*, V.Kumar*, A.Gupta, G.Vezzani, J.Schulman, E.Todorov, andS.Levine.
LearningComplexDexterousManipulationwithDeepReinforcementLearningandDemon-
strations. In Proceedings of Robotics: Science and Systems (RSS’18), Pittsburgh, PA, USA,
June2018.
11[38] J.Skalse, N.Howe, D.Krasheninnikov, andD.Krueger. Definingandcharacterizingreward
gaming. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
Advances in Neural Information Processing Systems, volume 35, pages 9460–9471. Curran
Associates,Inc.,2022.
[39] M.Vecerik,T.Hester,J.Scholz,F.Wang,O.Pietquin,B.Piot,N.Heess,T.Rotho¨rl,T.Lampe,
andM.Riedmiller.LeveragingDemonstrationsforDeepReinforcementLearningonRobotics
ProblemswithSparseRewards,Oct.2018.
[40] B.Scherrer. ImprovedandGeneralizedUpperBoundsontheComplexityofPolicyIteration.
In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in
Neural Information Processing Systems 26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a Meeting Held December 5-8, 2013, Lake Tahoe,
Nevada,UnitedStates,pages386–394,2013.
[41] T. Zhang. Mathematical Analysis of Machine Learning Algorithms. Cambridge University
Press,Cambridge,2023. doi:10.1017/9781009093057.
[42] B.Scherrer, M.Ghavamzadeh, V.Gabillon, B.Lesner, andM.Geist. Approximatemodified
policy iteration and its application to the game of Tetris. Journal of Machine Learning Re-
search,16:1629–1676,2015. doi:10.5555/2789272.2886802.
[43] S.Garrido-Jurado, R.Mun˜oz-Salinas, F.Madrid-Cuevas, andM.Mar´ın-Jime´nez. Automatic
generationanddetectionofhighlyreliablefiducialmarkersunderocclusion. PatternRecogni-
tion,47(6):2280–2292,June2014. ISSN00313203. doi:10.1016/j.patcog.2014.01.005.
[44] Y. Lin, A. S. Wang, G. Sutanto, A. Rai, and F. Meier. Polymetis.
https://facebookresearch.github.io/fairo/polymetis/,2021.
[45] S.K.S.Ghasemipour,R.S.Zemel,andS.S.Gu. ADivergenceMinimizationPerspectiveon
ImitationLearningMethods. InConferenceonRobotLearning(CoRL’19),2019.
[46] B.Chan. RLsandbox. https://github.com/chanb/rl sandbox public,2020.
[47] D.Yarats,R.Fergus,A.Lazaric,andL.Pinto.MasteringVisualContinuousControl:Improved
Data-AugmentedReinforcementLearning.InTheTenthInternationalConferenceonLearning
Representations,ICLR2022,VirtualEvent,April25-29,2022.OpenReview.net,2022.
[48] S.Sinha,A.Mandlekar,andA.Garg. S4RL:SurprisinglySimpleSelf-SupervisionforOffline
Reinforcement Learning. In Conference on Robot Learning (CoRL’21), London, UK, Nov.
2021.
[49] X.Chen,C.Wang,Z.Zhou,andK.Ross.RandomizedEnsembledDoubleQ-Learning:Learn-
ingFastWithoutaModel. arXiv:2101.05982[cs],Mar.2021.
12A RewardModelFormulations
Inthissection,weinvestigateapproachestooff-policyreinforcementlearning(RL)studiedinthis
work,modifiedtoaccommodateanunknownRandtheexistenceofanexamplestatebuffer ∗.
B
A.1 LearningaRewardFunction
Themostpopularapproachtorewardmodelling,knownasinverseRL,tacklesanunknownRbyex-
plicitlylearningarewardmodel. Modernapproachesundertheclassofadversarialimitationlearn-
ing (AIL) algorithm aim to learn both the reward function and the policy simultaneously. In AIL,
the learned reward function, also known as the discriminator, aims to differentiate the occupancy
measurebetweenthestate-actiondistributionsinducedbyexpertandthelearner. Inexample-based
control,thestate-conditionaldiscriminatorlossisEq.(3),whereDattemptstodifferentiatetheoc-
cupancymeasurebetweenthestatedistributionsinducedby ∗ and . TheoutputofD(s)isused
todefineRˆ(s),whichisthenusedforupdatingtheQ-functioB nusingB
Eq.(6).
Inexample-basedcontrol,thediscriminatorDprovidesasmoothedlabelofsuccessforstates,thus
its corresponding reward function can provide more density than a typical sparse reward function,
makingthis approachan appealingchoice. Unfortunately, alearneddiscriminator cansuffer from
the deceptive reward problem, as previously identified in [4], and this problem is exacerbated in
theexample-basedsetting. Inthefollowingsections,wedescribeoptionstoremovetherelianceon
separatelylearneddiscriminators.
A.2 Discriminator-FreeRewardLabelswithMeanSquaredErrorTDUpdates
A simple alternative to using a discriminator as a reward model was initially introduced as soft-Q
imitation learning (SQIL) in [6]. In standard AIL algorithms, D is trained separately from π and
Qπ, where D is trained using data from both and ∗, whereas π and Qπ are trained using data
B B
exclusivelyfrom .However,mostoff-policyalgorithmsdonotrequirethischoice,andapproaches
B
such as [4, 39] train Q, and possibly π, using data from both and ∗. It is unclear why this
B B
choice is often avoided in AIL, but it might be because it can introduce instability due to large
discrepancyinmagnitudesforQtargetsgivendatafrom and ∗. Samplingfrombothbuffers,we
candefineRˆ(s ,a )inEq.(2)tobelabelscorrespondingB towhB etherthedataissampledfrom or
t t
B
∗—inSQIL,thelabelsarerespectively0and1. Thefulltrainingobjectiveresemblestheminimax
B
objectiveinEq.(3)wherewesetD(s∗)=1andD(s)=0.
(cid:104) (cid:105)
TheresultingrewardfunctionistheexpectedlabelRˆ(s,a) = E 1(ˆ= ∗) ,
Bˆ∼Categorical s({B,B∗}) B B
where Categorical is a categorical distribution corresponding to the probability that s belongs to
s
buffers , ∗. Consequently,onlysuccessfulstatess∗yieldspositiverewardandthecorresponding
B B
optimal policy will aim to reach a successful state as soon as possible. If we further assume that
s∗ transitionstoitself,thenforpolicyevaluationwithmean-squarederror(MSE),wecanwritethe
temporaldifference(TD)target,y,ofQ-updateswith:
y(s,s′)=γE a′[Q(s′,a′)], (11) y(s∗,s∗)=1+γE a′[Q(s∗,a′)], (12)
where(s, ,s′) ,a′ π(a′ s′),ands∗ ∗.
· ∼B ∼ | ∼B
Thisapproachreducescomplexityaswenolongerexplicitlytrainarewardmodel,andalsoguaran-
teesdiscriminationbetweendatafrom and ∗.
B B
A.3 Discriminator-FreeRewardLabelswithBinaryCrossEntropyTDUpdates
Eysenbach et al. [3] introduced recursive classification of examples (RCE), a method for learning
fromexamplesofsuccess. RCEmostlyfollowstheapproachoutlinedaboveinAppendixA.2but
13usesaweightedbinarycross-entropy(BCE)losswithweights1+γwfordatafrom and1 γfor
B −
datafrom ∗. TheTDtargetsarealsochangedfromEq.(11)andEq.(12)to
B
y(s,s′)=γw(s′)/(1+γw(s′)), (13) y(s∗,s∗)=1, (14)
wherew(s′)=Vπ(s′)/(1 Vπ(s′)).
−
ThisapproachcanalsobemadeclosertoSQILbyremovingthechangeinweightsandbyleaving
Eq. (13) as Eq. (11). This makes it equivalent to SQIL, apart from removing bootstrapping from
Eq.(12),andusingaBCEinsteadofMSEforTDupdates.
A major benefit of this approach, compared with the MSE TD updates in Appendix A.2, is that
y(s,s′) y(s∗,s∗)isalwaysenforcedateveryupdate,meaningthatourapproachtovaluepenal-
≤
izationwouldprovidenoextrabenefit. Nonetheless,ourresultsfromAppendixBshowthatSQIL,
even without value-penalization, almost always outperforms both RCE and SQIL with BCE loss
(referredtoSQIL-BCEhere).
14sawyerdraweropen sawyerdrawerclose sawyerpush sawyerlift
1.0 0.20
0.06
0.10 0.15 0.2
0.04
0.8 0.10
0.05 0.1 0.02
0.05
0.00
0.00.06 0.00 0.0
0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0 0 1 2 3 4 5 0 1 2 3 4 5
sawyerboxclose sawyerbinpicking door-human-v0 hammer-human-v0
0.4 0.20 3000 10000
0.2 7500
0.15 2000
5000
0.2 0.10
0.1 1000 2500
0.05
00..00 0.00 0 0
0.00 1 2 3 4 0.25 0.0 0.6 1.2 1.80.4 2.4 3.0 0.0 0.6 01.6.2 1.8 2.4 3.0 00.8 1 2 3 4 51.0
EnvironmentSteps( 100k)
×
SQIL RCE(theirs) SQIL-BCE(theirs) SQIL-BCE-no-nstep(theirs)
Figure6: PerformanceresultsontheSawyerandAdroittasksconsideredin[3],usingtheirownimplementa-
tionsbasedonbinarycrossentropy(BCE)loss,butwithanadditionalSQIL-BCE(includingn-step)baseline.
WealsoshowourimplementationofSQIL(withoutvaluepenalizationorauxiliarytasks),withMeanSquared
ErrorTDupdates,whichoutperformsalloftheBCE-basedmethodsonaverage.
B WhyDoesSQILOutperformRCE?
OurresultsshowthatVPACE-SQILoutperformsACE-RCE,andthatVP-SQILandSQILoutper-
formRCEinalmostallcases. Thisresultisinconflictwithresultsfrom[3], whichshowedSQIL
stronglyoutperformedbyRCE.Inthissection,weshowresultsthathelpexplainourfindings.
Eysenbachetal.[3]claimedthatthehighestperformingbaselineagainstRCEwasSQIL,butitis
worthnotingthattheirimplementation1isadeparturefromtheoriginalSQILimplementation,which
usesMSEforTDupdates,describedinAppendixA.2. Furthermore,Eysenbachetal.[3]alsonoted
that it was necessary to add n-step returns to off-policy learning to get reasonable performance.
In their experiments, however, this adjustment was not included for their version of SQIL with
BCE loss. We complete the experiments using their implementation and show the average results
acrossallRCEenvironmentsinFig.6, andalsocomparetousingSQILwithMSE(withoutvalue
penalizationorauxiliarytasks)forTDupdates.
These results empirically show that RCE and SQIL with BCE loss perform nearly identically, in-
dicating that benefits of the changed TD targets and weights described in Appendix A.3 may not
beasclearaspreviouslydescribed. Furthermore,SQILwithMSEclearlyperformsbetteronaver-
age,althoughitstillperformsworsethanVPACE(seeAppendixE).Whiletheseempiricalresults
demonstrate that example-based control with BCE loss for TD updates is outperformed by SQIL
withMSE,Eysenbachetal.[3]hadtheoreticalresultsindicatingthatRCEshouldstilllearnanop-
timalpolicy. Inthefollowingsection,wedescribeaflawfoundinoneoftheirproofsthatmayhelp
tofurtherunderstandwhyRCEisoutperformedbySQILwithMSE.
B.1 Re-ExaminationoftheProofsfromSection4ofRCE[3]
In this section, we outline potential flaws in the proofs of Lemma 4.2 and Corollary 4.2.1 in RCE
[3]. We then provide a lemma that shows RCE can be considered as recovering a specific reward
function,therebyunifyingRCEwithotherIRLalgorithms.
Tobegin,recallthattheRCEobjective(i.e. Eq. (7)ofEysenbachetal.[3])isdefinedasfollows:
π(θ):=p(e =1)E [logCπ(s ,a )]+E [log(1 Cπ(s ,a ))]. (15)
L t+ p(st,at|et+=1) θ t t p(st,at) − θ t t
1Available at https://github.com/google-research/google-research/tree/master/rce at
timeofwriting.
15
nruteRedosipEEysenbach et al. [3] stated the following Lemma and Corollary to demonstrate that optimizing
Eq.(15)isequivalenttovalueiterationunderthetabularsetting:
Lemma1. (Lemma4.2ofRCE)Inthetabularsetting,theexpectedupdatesforEq.(15)areequiv-
alent to performing value iteration with reward function r(s ,a ) = (1 γ)p(e = 1s ) and a
t t t t
Q-functionparameterizedasQπ θ(s t,a t)= 1−C Cθπ θ( πs (t s, ta ,t a) t). − |
Corollary1. (Corollary4.2.1ofRCE)RCEconvergesinthetabularsetting.
In Lemma 1, we find that the proof intends to demonstrate equivalence with policy evaluation. In
particular, the temporal difference (TD) target is defined to be y = r(s,a)+γE s′,a′[Q(s′,a′)],
which is optimizing the Bellman expected equation rather than Bellman optimal equation. Fur-
thermore,theassignmentequationfortheratioshouldinfactbeanexpectationoverboththenext
state-and-actionpairs(i.e. s p( s ,a )anda π( s ))insteadofonlythenextstate,
t+1 t t t+1 t+1
∼ ·| ∼ ·|
evenifweaimtoperformpolicyevaluation. Asaresult,Corollary1alsofailstoholdifwesimply
usetheconvergenceproofofvalueiteration.
WenowprovidealemmaindicatingthattheoptimalsolutionofEq.(15)isequivalenttofindingQπ
withrewardr(s,a)=(1 γ)pπ(e=1s):
− |
Lemma 2. Fix any policy π : ∆( ). Let θ∗ = argmin π(θ) be the optimal solution of
S → A θL
Eq.(15). ConsideranMDPwithrewardfunctionr(s ,a )=(1 γ)p(e =1s ,a ). Then,inthe
t t t t t
tabular setting, Qπ = Cπ satisfies the Bellman expected equ− ation. Furthe| rmore, we have that
1−Cπ
Qπ(s ,a )=pπ(e =1s ,a ). Thatis,thesolutionofEq.(15)isequivalenttofindingQπ.
t t t+ t t
|
Proof. Fixanypolicyπ. ConsideranMDPwithrewardfunctionr(s,a) = (1 γ)p(e = 1s)and
transitionfunctionp(s′ s,a). WedefinetheBellmanexpectedoperator π
:RS−
×A
RS×A|
tobe
| T →
πq(s,a):=r(s,a)+γE [q(s′,a′)].
s′∼p(·|s,a),a′∼π(·|s)
T
Notethatunderthetabularsetting, π isaγ-contractionmappingundermax-norm[40]. Thus,by
Banachfixed-pointtheorem,wehavT ethatforanyq RS×A,
∈
Qπ =( π)∞ q.
T
Inotherwords,performingpolicyevaluationonπ withrewardr(s,a) = (1 γ)p(e = 1s)yields
− |
Qπ. Now,byLemma4.1ofEysenbachetal.[3],wehavethat
Cπ(s ,a ) (cid:20) Cπ(s ,a ) (cid:21)
t t =(1 γ)p(e =1s )+γE t+1 t+1 .
1 Cπ(s ,a ) − t | t st+1∼p(·|st,at),at+1∼π(·|st) 1 Cπ(s ,a )
t t t+1 t+1
− −
(16)
Eq.(16)satisfiestheBellmanexpectedequation,thusbyuniquenessofthefixed-pointtheorem,we
havethat
Cπ(s ,a )
Qπ(s ,a )= t t .
t t 1 Cπ(s ,a )
t t
−
Recallthat
Cπ(st,at)
istheBayes’optimalclassifierforEq.(15). Therefore,supposeweobtain
1−Cπ(st,at)
theminimumofEq.(15),thatis,θ∗ =argmin π(θ),thenwehavethat
θL
Cπ (s ,a ) Cπ(s ,a )
p(e =1s ,a )= θ∗ t t = t t =Qπ(s ,a ),
t+ | t t 1 Cπ (s ,a ) 1 Cπ(s ,a ) t t
− θ∗ t t − t t
meaningthatfindingtheminimumof π(θ)isequivalenttoperformingpolicyevaluationonπ.
L
WithLemma2andLemma4.3of[3], wecanviewAlgorithm1ofEysenbachetal.[3]asaform
ofpolicyiterationundertherewardfunctionr(s,a) = (1 γ)p(e = 1s), where(i)updatingthe
− |
classifierθ correspondstoperformingapolicyevaluationsteponπ,and(ii)updatingthepolicyπ
correspondstoperformingapolicyimprovementstep. Wenotethat,however,theRCEobjectiveis
notnecessarilyequivalenttopolicyevaluationwhenthetrueminimumofRCE,θ∗,isnotobtained.
16C Finite-SampleAnalysisofValue-PenaltyRegularization
In this section, we show a finite-sample complexity bound of learning the Q-function via mean-
squarederror(MSE)withvalue-penaltyregularizationanddemonstratethatwecanuseitssolution
forapproximatepolicyiteration(API).Intuitively, givenafixedsetofsamples, anear-optimalso-
lutionobtainedusingtheregularizedlossisclosedtothetrueoptimalsolutionoftheunregularized
loss with high probability. Then, if we can control the error of the solution from the regularized
loss, we can further bound the value error between the Q-function output by API and the optimal
Q-function.
Moreformally,consideraparameterizedfunctionclass = Q :RS×A Rθ Θ . Wedefine
θ
Q { → | ∈ }
theMSEas
N
ℓ(θ,Z )= 1 (cid:88) (Q (S ,A ) G )2 , (17)
N θ n n n
N −
n=1
where Z = (S ,A ,G ) N is N samples of the state-action pairs and the corresponding
N { n n n }n=1
expectedreturn. Thevalue-penaltyregularizerisdefinedas
N
h(θ,Z )=λ 1 (cid:88) (max(Q (S ,A ) Q ,0))2 +(max(Q Q (S ,A ),0))2 , (18)
n θ n n U L θ n n
N − −
n=1
for some constants Q < Q , corresponding to the lowest and highest possible Q-values respec-
L U
tively,andpositiveconstantλ. Then,theMSEwithvalue-penaltyregularizationisdefinedas
ℓ (θ,Z )=ℓ(θ,Z )+h(θ,Z ). (19)
reg N N N
During training, suppose we are given N i.i.d. samples Z from . We aim to approximate the
N
regularizedempiricalriskminimizer(ERM)toobtainθˆ: D
ℓ(θˆ,Z )+h(θˆ,Z ) min[ℓ(θ,Z )+h(θ,Z )]+ε′. (20)
n n n n
≤ θ∈Θ
OurgoalistofindtheapproximateregularizedERMθˆthatachievesalowtestloss,definedby
(θ, )=E [ℓ(θ,Z)]. (21)
Z∼D
L D
Tobegin,weassumethatthefunctionclasshasboundedoutputs(orboundedparameters)whichis
astandardrequirementforcharacterizinggeneralizationerrors:
Assumption 1. (Bounded function output and realizability.) Let C max(Q , Q ). The
Q L U
parameterizedfunctionclass isbounded: = Q : RS×A [ C≥ ,C ]θ| Θ| | . Mo| reover,
θ Q Q
F Q { → − | ∈ }
theoptimalparameterofthetestlossbelongstothefunctionclass: θ∗ =argmin (θ, ) Θ.
θL D ∈
Note that the regularizer term Eq. (18) only has an effect when the function Q predicts values
θ
that are out-of-range, therefore we set C to be larger than the maximum possible returns for a
Q
moremeaningfulanalysis, otherwisewerecovertheunregularizedlossEq.(17). Furthermore, we
characterizethecomplexityofthefunctionclass usingtheexpectedRademachercomplexitywith
Q
offset.
Definition1. TheexpectedRademachercomplexityisdefinedtobe
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
Rh( , )=E E sup σ (ℓ(θ,Z )+0.5h(θ,Z )) 0.5h(θ,Z ) ,
N Q D ZN∼D σ N n n n − N
θ∈Θ
n=1
whereσ ,...,σ areindependentRademacherrandomvariables.
1 N
Then,byCorollary6.21of[41],wecanshowthattheapproximateERMsatisfiestheoracleinequal-
itywithhighprobability,whichwestatehere:
17Lemma 3. (Corollary 6.21 of [41].) Let ∆ h(θ) sup [h(θ,z) h(θ,z′)]. Assume that for
N
≤
z,z′
−
someC 0:
≥
(cid:20) (cid:21)
sup sup[ℓ(θ,z) ℓ(θ,z′)+∆ h(θ)] C.
N
θ∈Θ z,z′ − ≤
Then the approximate ERM specified by Eq. (20), θˆ, satisfies the following oracle inequality. For
anyδ,δ′ >0,withprobability1 δ δ′,wehavethat
− −
(cid:34) (cid:114) (cid:35)
log(1/δ′)
(θˆ, ) inf (θ, )+E [h(θ,Z )]+∆ h(θ)
L D ≤θ∈Θ L D
Zn N N
2N
(cid:114)
log(1/δ)
+ε′+2Rh( , )+2C .
N Q D 2N
We note that the first term of RHS in Lemma 3 is zero due to Assumption 1. In particular, by
Assumption 1 and uniqueness of Bellman expected operator, we have that (θ∗, ) = 0. The
secondtermE Zn[h(θ,Z N)] = 0andthefirstfactorinthethirdterm∆ Nh(θL ) = 0D sinceQ θ∗ isa
validQ-function,thusnoclippingisapplied. Withoutlossofgenerality,supposethatε′ = 0,then
byLemma3wehavethatwithprobability1 δ δ′,
− −
(cid:114)
log(1/δ)
(θˆ, ) 2Rh( , )+2C = ε .
L D ≤ N Q D 2N ∥ approx ∥2,D
Inotherwords,asweincreasethenumberofsamplesN,theapproximationerrorofθˆ, ε ,
approx 2,D
∥ ∥
approaches zero with high probability. The boundedness in Assumption 1 further ensures that the
Rademachercomplexityiswell-behavedandshrinkstozeroasN increasesunderthetestloss[41].
Now, considertheapproximatepolicyiterationalgorithmforQ-functioncalledAMPI-Q[42]. In-
steadofapproximatingtheQ-functionusingthestandardsquared-lossEq.(17),weinsteadusethe
regularizedlossEq.(19). FromLemma3weknowthat (θˆ, ) ε withhighprobabil-
approx 2,D
L D ≤∥ ∥
ity.Consequently,ateachiterationkofAMPI-Q,wecanreplacethepolicyevaluationerror ε
k 2,µ
∥ ∥
with ε and immediately obtain a finite-sample analysis of AMPI-Q through Theorem 8
approx 2,D
∥ ∥
of[42].
Theorem 1. With Assumption 1 and the assumptions from [42], after k iterations, AMPI-Q with
regularizedlossEq.(19)forpolicyevaluationsatisfiesthefollowingwithprobability1 δ δ′:
− −
(cid:32) (cid:114) (cid:33)
∥Q∗ −Qπk
∥2,ρ0
≤
2(γ
−
(1γk)(
γC
)∞1 2,k,0)21
2Rh N( Q, D)+2C
log 2( Nk/δ)
+g(k),
−
where 1,k,0andg(k)aretheconcentrability-coefficientsrelatedtermsdefinedin[42].
C∞
Proof. WecandirectlyapplyTheorem8of[42](settingp = 2,q′ = 1,q = )andtaketheunion
boundoverthebadevents,thatis,withprobabilityatmost(δ+δ′)/k,
(θˆ,∞
) > ε for
approx 2,D
L D ∥ ∥
eachofkiterations.
Theorem 1 indicates that when we run AMPI-Q for sufficiently large number of iterations, the
learned policy reaches optimal policy performance, even when using our proposed value-penalty
regularizationEq.(18)onthestandardsquaredlossforpolicyevaluation.
18Figure7: LearnedVPACE-SQILpoliciesatthefinaltrainingstepfor,fromtoptobottom,door-human-v0,
door-human-v0-dp,hammer-human-v0,andhammer-human-v0-dp. Althoughtheoriginalversionsofthe
environmentsaresolved,theabsolutepositionactionspaceallowspoliciestoexecuteverycoarseactionsthat
exploitthesimulator(above,hittingthehandlewithoutgraspingitandthrowingthehammer,respectively),and
wouldalmostcertainlycausedamagetoarealenvironment.
D AdditionalEnvironment,Algorithm,andImplementationDetails
Thefollowingsectionscontainfurtherdetailsoftheenvironments,tasks,auxiliarytasks,algorithms,
andimplementationsusedinourexperiments.
D.1 AdditionalEnvironmentDetails
ComparedwiththeoriginalPandatasksfromLfGP[4],weswitchfrom20Hzto5Hzcontrol(find-
ing major improvements in performance for doing so), improve handling of rotation symmetries
in the observations, and remove the force-torque sensor since it turned out to have errors at low
magnitudes. Crucially, these modifications did not require training new expert policies, since the
same final observation states from the full trajectory expert data from [4] remained valid. Com-
paredwiththeoriginalLfGPtasks,wealsoremoveMove-blockasanauxiliarytaskfromStack,
Unstack-Stack,BringandInsert,sincewefoundaslightperformanceimprovementfordoing
so,andaddReach,Lift,andMove-blockasmaintasks. Theenvironmentwasotherwiseidentical
tohowitwasimplementedinLfGP,includingallowingrandomizationoftheblockandend-effector
positions anywhere above the tray, using delta-position actions, and using end-effector pose, end-
effector velocity, object pose, object velocity, and relative positions in the observations. For even
furtherdetailsoftheoriginalenvironment,see[4].
SincetheSawyertasksfrom[3,11]onlycontainend-effectorpositionandobjectpositionbydefault,
theydonotfollowtheMarkovproperty. Tomitigatethis,wetrainallalgorithmsintheSawyertasks
withframe-stacking of3and addingripperposition totheobservations, since wefoundthat this,
at best, improved performance for all algorithms, and at worst, kept performance the same. We
validate thatthis istrue byalso performingexperiments usingthe original codeand environments
from[3],unmodified,wheretheresultsofRCEwithoutthesemodificationsarepresentedinFig.6,
withresultscomparabletoorpoorerthanourownRCEresults.
D.2 Delta-PositionAdroitHandEnvironments
The Adroit hand tasks from [37] use absolute positions for actions. This choice allows even very
coarse policies, with actions that would be unlikely to be successful in the real world, to learn to
complete door-human-v0 and hammer-human-v0, and also makes the intricate exploration re-
19Figure8: ExperimentalsetupforourrealenvironmentandourRealDrawerandRealDoortasks. Therobot
hasaforce-torquesensorattached,butitisnotusedforourexperiments.
quired to solve relocate-human-v0 very difficult. Specifically, VPACE-SQIL and several other
baselinesachievehighreturnindoor-human-v0andhammer-human-v0,butthelearnedpolicies
use unrealistic actions that exploit simulator bugs. As well, no methods are able to achieve any
returninrelocate-human-v0in1.5Menvironmentsteps.
In the interest of solving relocate-human-v0 and learning more skillful policies, we gener-
ated modified versions of these environments with delta-position action spaces. Furthermore, in
relocate-human-v0-najp-dp, the action space rotation frame was changed to be in the palm,
rather than at the elbow, and, since relative positions between the palm, ball, and relocate goal
are included as part of the state, we removed the joint positions from the state. In our experi-
ments,thesemodifiedenvironmentswerecalleddoor-human-v0-dp,hammer-human-v0-dp,and
relocate-human-v0-najp-dp. SeeFig.7foracomparisonoflearnedpoliciesineachversionof
theseenvironments.
D.3 RealWorldEnvironmentDetails
Fig.8showsourexperimentalplatformandsetupforourtworealworldtasks. InbothRealDrawer
andRealDoor,theobservationspacecontainstheend-effectorposition,anArUcotag[43]topro-
videdrawerordoorposition(intheframeoftheRGBcamera;wedonotperformextrinsiccalibra-
tion between the robot and the camera), and the gripper finger position. The action space in both
containsdelta-positionsandabinarygrippercommandforopeningandclosing.Theactionspacefor
RealDrawerisone-dimensional(allowingmotioninaline),whiletheactionspaceforRealDooris
two-dimensional(allowingmotioninaplane). TheinitialstatedistributionforRealDrawerallows
forinitializingtheend-effectoranywherewithina10cmlineapproximately25cmawayfromthe
drawerhandlewhenclosed. ForRealDoor,theinitialstatedistributionisa20cm 20cmsquare,
×
approximately20cmawayfromthedoorhandlewhenclosed. Actionsaresuppliedat5Hz.
Forbothenvironments,forevaluationonly,successisdeterminedbywhetherthedrawerordooris
fullyopened, asdetectedbytheabsolutepositionoftheArUcotagintheframeoftheRGBcam-
era. OurrobotenvironmentcodeisbuiltonPolymetis[44],andusesthedefaulthybridimpedance
controllerthatcomeswiththelibrary. Toreduceenvironmentaldamagefromexcessiveforcesand
torques, we reduced Cartesian translational stiffness in all dimensions from 750 N/m to 250 N/m,
andtheforceandtorquelimitsinalldimensionsfrom40Nand40Nmto20Nand20Nm.
20Algorithm Value Sched.Aux. RewardModel TDErrorLoss Source
Penalization Tasks
VPACE-SQIL ✓ ✓ SQIL MSE Ours
ACE-SQIL ✗ ✓ SQIL MSE Ours
VPACE-DAC ✓ ✓ DAC MSE Ours
ACE-RCE ✗ ✓ RCE BCE Ours
VP-SQIL ✓ ✗ SQIL MSE Ours
VP-DAC ✓ ✗ DAC MSE Ours
SQIL ✗ ✗ SQIL MSE Ours
RCE ✗ ✗ RCE BCE Ours
RCE(theirs) ✗ ✗ RCE BCE [3]
SQIL-BCE ✗ ✗ SQIL BCE [3]
Table1: Majordifferencesbetweenalgorithmsstudiedinthiswork. MSEreferstomeansquarederror,while
BCEreferstobinarycrossentropy.
D.4 AdditionalTaskDetails
Fig.9showsrepresentativeimagesforallenvironmentsandtasksusedinthiswork. Successexam-
plesforthePandaenvironmentsweregatheredbytakings fromtheexistingdatasetsprovidedby
T
[4]. SuccessexamplesformaintasksfromtheSawyerenvironmentsweregeneratedusingthesame
codefrom[3],inwhichthesuccessexamplesweregeneratedmanuallygivenknowledgeofthetask.
Auxiliary task data was generated with a similar approach. Success examples for the Adroit hand
environmentsweregeneratedfromtheoriginalhumandatasetsprovidedby[37]. Successexamples
forourrealworldtasksweregeneratedbymanuallymovingtherobottoasmallsetofsuccessful
positionsforeachauxiliarytaskandmaintask.
AsstatedinSection4.1,allPandamaintasksusethetheauxiliarytasksrelease,reach,grasp,and
lift.
Therearetwospecificnuancesthatwereleftoutofthemaintextforclarityandbrevity:(i)theReach
maintaskonlyusesreleaseasanauxiliarytask(sinceitalsoactsasa“coarse”reach),and(ii)half
ofthereleasedatasetforeachtaskisspecifictothattask(e.g., containinginsertorstackdata), as
wasthecaseintheoriginaldatasetsfrom[4]. FortheSawyer,Hand,andrealPandaenvironments,
becausetheobservationspacesarenotshared,eachtaskhasitsownseparatereachandgraspdata.
D.5 AdditionalAlgorithmDetails
Algorithm 1 shows a summary of VPACE, built on LfGP [4] and SAC-X [31]. Table 1 shows a
breakdownofsomeofthemajordifferencesbetweenallofthealgorithmsstudiedinthiswork.
D.6 AdditionalImplementationDetails
Inthissection,welistsomespecificimplementationdetailsofouralgorithms. Weonlylistparam-
etersorchoicesthatmaybeconsidereduniquetothiswork,butafulllistofallparameterchoices
canbefoundinourcode. WealsoprovidetheVPACEpseudocodeinAlgorithm1, withbluetext
onlyapplyingtolearneddiscriminator-basedrewardfunctions(seeAppendixAforvariousreward
models).
Wheneverpossible,allalgorithmsandbaselinesusethesamemodifications. Table2alsoshowsour
choicesforcommonoff-policyRLhyperparametersaswellaschoicesforthoseintroducedbythis
work.
DACrewardfunction:forVPACE-DACandVP-DAC,althoughtherearemanyoptionsforreward
functionsthatmapDtoRˆ [45],following[4,5,17],wesettherewardtoRˆ (s)=log(D (s))
T T
−
log(1 D (s)).
T
−
21(a)Pandaρ andB∗ examples. (b)SawyerandAdroitρ andB∗ examples.
0 T 0 T
(c)RealPandaρ andB∗ examples.
0 T
Figure 9: Examples of samples from initial state distribution ρ , auxiliary task buffers B∗ , and main task
0 aux
buffersB∗ forallenvironmentsandtasksusedinthiswork. Auxiliarytaskdataisreusedbetweentasksin
main
thePandaenvironment, sincetheobservationspaceandactionspacearesharedforalltasks, whileonlythe
task definitions themselves are reusable in the Panda and Adroit environments. Images are shown here for
illustrativepurposes,butallalgorithmstestedinthisworkusenumericalstatedata.
22Algorithm1Value-PenalizedAuxiliaryControlfromExamples(VPACE)
Input: Examplestatebuffers ∗ , ∗,..., ∗ ,schedulerperiodξ,samplebatchsizeN andN∗,
Bmain B1 BK
anddiscountfactorγ
Parameters: Intentions π with corresponding Q-functions Q (and optionally discriminators
T T
D ),andschedulerπ (e.g. withQ-tableQ )
T S S
1: Initializereplaybuffer
B
2: fort=1,...,do
3: #Interactwithenvironment
4: Foreveryξsteps,selectintentionπ T usingπ S
5: Selectactiona tusingπ T
6: Executeactiona tandobservenextstates′
t
7: Storetransition ⟨s t,a t,s′ t⟩in
B
8:
9: #OptionallyupdatediscriminatorD T′ foreachtask ′
1 10 1:
:
S foa rm ep al ce h{ ts ai s} kN i=1 ′∼ doB T
12: Sample s∗TN∗ ∗
{ i}i=1 ∼Bk
13: UpdateD T′ followingEq.(3)usingGAN+GradientPenalty
14: endfor
15:
16: #Updateintentionsπ T′ andQ-functionsQ T′ foreachtask ′
11 87 :: fS oa rm ep al ce h{ t( as si k,a i ′) } dN i o=1 ∼B T
19: Sample s∗TN∗ ∗
20: Sample{ a∗ ii ∼}i= π1 T′∼ (sB∗ i)T f′ ori=1,...,N∗
21: ComputerewardsRˆ T′(s i)andRˆ T′(s∗ j)fori=1,...,N andj =1,...,N∗
22: #Computevaluepenalizationterms,seeAppendixD.6.1
23: ComputeQπ mT ax′ = N1
∗
(cid:80)N j=∗ 1Q(s∗ j,a∗ j)
24: ComputeQπ mT in′ =min( ∧N i=1Rˆ T′(s i), ∧N j=∗ 1Rˆ T′(s∗ j))/(1 −γ)
25: endfor
26: UpdateπfollowingEq.(4)
27: UpdateQfollowingEq.(5)withvaluepenalizationEq.(10)
28:
29: #OptionalUpdatelearnedschedulerπ S
30: ifattheendofeffectivehorizonthen
31: ComputemaintaskreturnG Tmain usingrewardestimatefromD main
32: Updateπ S (e.g. updateQ-tableQ S usingEMAandrecomputeBoltzmanndistribution)
33: endif
34: endfor
n-step returns and entropy in TD error: following results from [3], we also add n-step returns
andremovetheentropybonusinthecalculationoftheTDerrorforallalgorithmsinallSawyerand
Adroitenvironments,findingasignificantperformancegainfordoingso.
Absorbingstatesandterminalstates: forallalgorithms,wedonotincludeabsorbingstates(in-
troducedin[5])orterminalmarkers(sometimesreferredtoas“done”),sincewefoundthatbothof
theseadditionscausemajorbootstrappingproblemswhenenvironmentsonlyterminateontimeouts,
andtimeoutsdonotnecessarilyindicatefailure. Previousworksupportsbootstrappingonterminal
stateswhentheyarecausedbynon-failuretimeouts[36].
SQIL labels for policy data: The original implementation of SQIL uses labels of 0 and 1 TD
updatesinEq.(11)andEq.(12),respectively. WefoundthatchangingthelabelforEq.(11)from0
to-1improvedperformance.
23General
TotalInteractions Task-specific(seeAppendixE)
BufferSize Sameastotalinteractions
BufferWarmup 5k
InitialExploration 10k
Evaluationspertask 50(Panda),30(Sawyer/Adroit)
Evaluationfrequency 25k(Panda),10k(Sawyer/Adroit)
Learning
γ 0.99
BBatchSize 128
B∗BatchSize 128
QUpdateFreq. 1
TargetQUpdateFreq. 1
πUpdateFreq. 1
PolyakAveraging 1e-3
QLearningRate 3e-4
πLearningRate 3e-4
DLearningRate 3e-4
αLearningRate 3e-4
Initialα 1e-2
TargetEntropy −dim(a)
Max.GradientNorm 10
WeightDecay(π,Q,D) 1e-2
DGradientPenalty 10
RewardScaling 0.1
SQILLabels(AppendixD.6 (−1,1)
ExpertAug.Factor(AppendixD.6.3) 0.1
ValuePenalization(VP)
λ 10
Qπ ,Qπ num.filterpoints(AppendixD.6.1) 50
max min
Auxiliary Control (ACE) Scheduler (Ap-
pendixD.6.2)
Num.Periods 8(Panda),5(Sawyer/Adroit)
MainTaskRate 0.5(Panda),0.0(Sawyer/Adroit)
HandcraftRate 0.5(Panda),1.0(Sawyer/Adroit)
Table2:Hyperparameterssharedbetweenallalgorithms,unlessotherwisenoted.
RewardScalingof.1: weusearewardscalingparameterof.1forallimplementations. Coupled
withadiscountrateγ =0.99(commonformuchworkinRL),thissetstheexpectedminimumand
maximumQvaluesforSQILto −.1 = 10and .1 =10.
1−γ − 1−γ
Nomultitaskweightsharing:intuitively,onemayexpectweightsharingtobehelpfulformultitask
implementations.Wefoundthatitsubstantiallyhurtperformance,soallofourmultitaskmethodsdo
notshareweightsbetweentasksorbetweenactorandcritic.However,themultitaskdiscriminatorin
VPACE-DACdoeshaveaninitialsetofsharedweightsduetoitssignificantlypoorerperformance
withoutthischoice.
∗samplingforQ:inSQIL,DACandRCE,wesamplefromboth and ∗forQupdates,butnot
B B B
forπupdates(whichonlysamplesfrom ). TheoriginalDACimplementationin[5]onlysamples
B
∗forupdatingD,samplingonlyfrom forupdatingQ.
B B
Allotherarchitecturedetails, includingneuralnetworkparameters, arethesameas[4], whichour
ownimplementationsarebuiltontopof. Ourcodeisbuiltontopofthecodefrom[4],whichwas
originallybuiltusing[46].
24D.6.1 MaintainingQπ ,Qπ EstimatesforValuePenalization
max min
OurapproachtovaluepenalizationrequiresmaintainingestimatesfororchoosingQπ andQπ .
max min
In both DAC and SQIL, the estimate of Qπ comes from taking the mini-batch of data from ∗,
max B
passingitthroughtheQfunction, takingthemean, andthenusingamedianmovingaveragefilter
tomaintainanestimate. The“Qπ ,Qπ num. filterpoints”valuefromTable2referstothesizeof
max min
thisfilter. Wechose50anduseditforallofourexperiments. WesetQπ to
min
rew. scale min(Rˆ(s))
Qπ = × , (22)
min 1 γ
−
where in SQIL, min(Rˆ(s)) = Rˆ(s) is set to 0 or -1, and in DAC, we maintain an estimate of the
minimumlearnedrewardmin(Rˆ)usingamedianmovingaveragefilterwiththesamelengthasthe
oneusedforQπ .
max
D.6.2 SchedulerChoices
AsnotedinSection4.1,ourACEalgorithmsusethesameapproachtoschedulingfrom[4]. Specif-
ically,weuseaweightedrandomscheduler(WRS)combinedasmallsetofhandcraftedhigh-level
trajectories. The WRS forms a prior categorical distribution over the set of tasks, with a higher
probabilitymassp Tmain (MainTaskRateinTable2)forthemaintaskand
pT
Kmain forallothertasks.
Additionally, we choose whether to uniformly sample from a small set of handcrafted high-level
trajectories,insteadoffromtheWRS,attheHandcraftRatefromTable2.
Our selections for handcrafted trajectories are quite simple, and reusable between main tasks
within each environment. In the Panda tasks, there are eight scheduler periods per episode
and four auxiliary tasks (reach, grasp, lift, release), and the handcrafted trajectory options are:
Unstack-Stack
1. reach,lift,main,release,reach,lift,main,release 11..00
2. lift,main,release,lift,main,release,lift,main 00..88
3. main, release, main, release, main, release, main, re- 00..66
lease 00..44
00..22
IntheSawyerandAdroitenvironments, weactuallyfoundthat
00..00
theWRSwasunnecessarytoefficientlylearnthemaintask,and 00.0 01.2 02.4 30.6 40.8 51.0
simplyusedtwohandcraftedhigh-leveltrajectories. Intheseen- EnvironmentSteps( 100k)
×
vironments,therearefiveschedulerperiodsperepisodeandtwo VPACE-SQIL,noablations
auxiliarytasks(reach,grasp),andthehandcraftedtrajectoryop- NoExampleAugmentation
10Examples
tionsare:
10Examples,NoEx.Aug.
1. reach,grasp,main,main,main Figure10: Additionalresultsforthe
Unstack-Stack task from Fig. 11,
2. main,main,main,main,main
eitherincludingorexcludingtheuse
of example augmentation described
D.6.3 ExpertDataAugmentation inAppendixD.6.3.
Weaddedamethodforaugmentingourexpertdatatoartificially
increase dataset size. The approach is similar to other approaches that simply add Gaussian or
uniformnoisetodatainthebuffer[47,48]. Inourcase, wegoonestepfurtherthantheapproach
from[48],andfirstcalculatetheper-dimensionstandarddeviationofeachobservationin ∗,scaling
B
the Gaussian noise added to each dimension of each example based on the dimension’s standard
deviation. Forexample,ifadimensionin ∗ haszerostandarddeviation(e.g.,inInsert,thepose
B
of the blue block is always the same), it will have no noise added by our augmentation approach.
The parameter “Expert Aug. Factor” from Table 2 controls the magnitude of this noise, after our
per-dimensionnormalizationscheme.
25
etaRsseccuSIn Fig. 10, we show the results of excluding expert augmentation, where there is a clear, if slight,
performancedecreasewhenitisexcluded,whichisevenmorepronouncedwithasmaller ∗ size.
BT
Allmethodsandbaselinesfromourownimplementationuseexpertdataaugmentation.
D.6.4 OtherAblationDetails
InourablationexperimentsfromSection4.3,weincludedthreebaselineswithfulltrajectorydata,
in addition to success examples. We added 200 (s,a) pairs from full expert trajectories to make
datasets comparable to the datasets from [4], where they used 800 expert (s,a) pairs, but their
environment was run at 20Hz instead of 5Hz, meaning they needed four times more data to have
roughly the same total number of expert trajectories. We generated these trajectories using high-
performing policies from our main experiments, since the raw trajectory data from [4] would not
applygiventhatwechangedthecontrolratefrom20Hzto5Hz.
D.6.5 RealPandaImplementationDetails
WhilemostofthedesignchoicesinAppendixD.6applytoallenvironmentstested,ourrealPanda
environmenthadsomesmallspecificdifferences,mostlyduetothecomplicationsofrunningrein-
forcementlearningintherealworld.Welistthedifferenceshere,butforanexhaustivelist,ouropen
sourcecodecontainsfurtherdetails.
Maximum episode length: The maximum episode length for both RealDrawer and RealDoor
is1000steps,or200secondsinrealtime. Thiswasselectedtoreducehowoftentheenvironment
had to be reset, which is time consuming. Running episodes for this long, and executing actions
at 5 Hz, our environments complete 5000 environment steps in roughly 20 minutes. The extra
time is due to the time to reset the environment after 1000 steps or after a collision. VPACE took
approximately100minutestolearntocompleteRealDrawerconsistently, andabout200minutes
tolearntocompletethemoredifficultRealDoor.
Shorter initial exploration: To attempt to learn the tasks with fewer environment samples, we
reducebufferwarmupto500steps,andinitialrandomexplorationto1000steps.
Framestack: Fortraining,westackedtworegularobservationstoavoidstatealiasing.
Ending on success: We ended episodes early if they were determined to be successful at the
maintaskonly. Althoughthisisnotnecessaryfortaskstobelearned(andthisinformationwasnot
providedtothelearningalgorithm),itgaveusawaytoevaluatetrainingprogress.
Extragradientsteps: Toaddefficiencyduringexecutionandtraining,wecompletedtrainingsteps
duringthegapintimebetweenanactionbeingexecutedandanobservationbeinggathered. Instead
ofcompletingasinglegradientstepatthistime,asisthecaseforstandardSAC(andVPACE),we
completedfourgradientsteps,findinginsimulatedtasksthatthisgaveabenefittolearningefficiency
withoutharmingperformance. Previouswork[4,49]hasfoundthatincreasingthisupdateratecan
reduceperformance,butwehypothesizethatourvaluepenalizationschemehelpsmitigatethisissue.
Collisions: If the robot is detected to have exceeded force or torque limits (20 N and 20 Nm in
our case, respectively), the final observation prior to collision is recorded, and the environment is
immediately reset. There are likely more efficient ways to handle such behaviour, but we did not
investigateanyinthiswork.
26Reach Lift Move-Block Stack
1.10.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75
0.05.08 0.50 0.50 0.50
0.25 0.25 0.25 0.25
0.00.06 0.00 0.00 0.00
0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0 1 2 3 4 5
Unstack-Stack Bring Insert VPACE-SQIL
1.00.04 1.00 1.00
VPACE-DAC
0.75 0.75 0.75 ACE-SQIL
ACE-RCE
0.05.02 0.50 0.50
VP-SQIL
0.25 0.25 0.25 VP-DAC
0.00.00 0.00 0.00 SQIL
00.0 1 2 3 4 0.25 0 1 2 30.4 4 5 0 2 0.64 6 8 10 0.8 RCE 1.0
EnvironmentSteps( 100k)
×
Figure11: PerformanceresultsonsimulatedPandatasksfrom[4,32]. Performanceonauxiliarytasksisnot
shown. Methodsthatleveragethemultitaskframeworkareshownwithsolidlines,andtheirsingle-taskcoun-
terpartsareshownwithdashedlines.Theshadedareacorrespondstohalfstandarddeviationacrossfiveseeds.
Both value-penalization and auxiliary task exploration are consistently beneficial for the most complicated
tasks.
sawyerdraweropen sawyerdrawerclose sawyerpush
1.0 0.20
0.10 0.15 0.2
VPACE-SQIL
0.8 0.10
0.05 0.1 VPACE-DAC
0.05 ACE-SQIL
0.00.06 0.00 0.0 ACE-RCE
0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0 0 1 2 3 4 5 VP-SQIL
sawyerlift sawyerboxclose sawyerbinpicking VP-DAC
0.0 0. 64 0.20 SQIL
0.2 RCE
0.15
0.04
0.2 0.10
0.02 0.1
0.05
0.00
0.0 0.0 0.00
0.00 1 2 3 0.24 5 0 0.41 2 3 40.6 5 0.0 0.6 0.81.2 1.8 2.4 3.10.0
EnvironmentSteps( 100k)
×
Figure12:PerformanceresultsontheSawyertasksconsideredin[3]andintroducedin[11].Formorecomplex
tasks, themultitaskmethodslearnwithfarfewerenvironmentsteps. Theseresultsaremorestochasticthan
thosefromFig.11,butthesameoverallpatternemerges.
E AdditionalPerformanceResults
Inthissection,weexpanduponourperformanceresultsfromSection4.2andourQ-valuecalibration
resultsfromSection4.4. Wealsoshowperformanceresultsforauxiliarytasks.
E.1 ExpandedMainTaskPerformanceResults
Fig.11,Fig.12,andFig.13showexpandedper-main-taskresultsforourPanda(originallypresented
in[4,32]),Sawyer(originallypresentedin[11,3]),andAdroitHand(originallypresentedin[37,3])
tasks,respectively.
In the Panda tasks (Fig. 11), the hardest tasks (Stack, Unstack-Stack, and Insert) benefit the
mostfromVPACE,butVPACE-SQILisalwaysthefastestandhighestperformingmethod.Notably,
ACE-SQIL is always outperformed by both VPACE-SQIL and VPACE-DAC, and RCE is always
outperformedbyVP-SQIL.Thebenefitsofvaluepenalizationarealsoclear, eveninBring, where
ACE-SQIL intially learns the task, but starts to learn very poor Q estimates and have increasingly
poorerandmoreunstableperformance.
27
etaRsseccuS
nruteRedosipEdoor-human-v0-dp hammer-human-v0-dp relocate-human-v0-najp-dp
11..50 4.0
3.0 2.0
1.0
2.0 VPACE-SQIL 0.8
0.5 1.0 1.0 VPACE-DAC
ACE-SQIL
0.0
00..06 -1.0 0.0 ACE-RCE
0 1 2 3 4 5 0 2 4 6 8 10 0 3 6 9 12 15 VP-SQIL
door-human-v0 hammer-human-v0 relocate-human-v0 VP-DAC
30..04 10.0 SQIL
7.5 2.0 RCE
2.0
5.0
0.2 1.0
1.0 2.5
0.0
00..00 0.0
00..00 0.6 1.2 1.80.22.4 3.0 0 0.41 2 3 40.6 5 0 3 0.86 9 12 151.0
EnvironmentSteps( 100k)
×
Figure13:PerformanceresultsontheAdroittasksconsideredin[3]andintroducedin[37],alongwithmodified
versionsoftheenvironmentsthatusedelta-positionbasedcontrol. Theepisodereturndoesn’tfullyrepresent
theresultsfortheseenvironments,becausethepolicieslearnedfortheoriginaltaskscanbequitecoarseand
unrealistic (see Fig. 7). For the original environments, many baselines perform comparably, but for the dp
versions,VPACE-SQIL,VP-SQIL,andVPACE-DACarethemostconsistentlyhighperforming.
In the Sawyer tasks (Fig. 12), the benefit of VPACE is not as prominent, even if VPACE-SQIL is
still the best performing method on average. The clearer pattern is the improvement from adding
auxiliary task exploration, although the improvement is still lower, on average, than the improve-
mentinthePandatasks. ComparedwiththePandatasks,thesetasksdonothaverandomizedinitial
conditions(apartfromsawyer box close),andalsogenerallyrequireshortermovementstocom-
plete,sothebenefitsofVPACEarelesspronounced.
In the Adroit hand tasks (Fig. 13), VPACE-SQIL, VPACE-DAC, and VP-SQIL are always among
thehighestperformingmethods. Inparticular,RCEandACE-RCEarealwaysoutperformedbyall
other methods. As stated in Section 4.1 and further detailed in Appendix D.2, we also generated
delta-position variants of the environments, which (i) generated, qualitatively, far more realistic
policies(seeFig.7),and(ii)allowedmanymethodstosolvetherelocatetask.
E.2 ValuePenalizationCalibrationImprovement–AllEnvironments
This section expands on the value penalization calibration results from Section 4.4 for the Panda
and Sawyer tasks Fig. 14 and for the Adroit hand tasks Fig. 15. Following the results shown
for Unstack-Stack, the results for other difficult tasks also show clear violations of y(s,a)
≤
y(s∗,a∗), and tasks in which this rule is violated also tend to have poorer performance. The vi-
olations are more pronounced ACE-SQIL, which, as previously discussed, is likely because they
generate far more diverse state distributions in . As shown for Unstack-Stack in Section 4.4,
B
VP-SQIL and VPACE-SQIL never violate y(s,a) y(s∗,a∗). Intriguingly, although the rule is
≤
severely violated for many Adroit hand tasks, ACE-SQIL and SQIL still have reasonable perfor-
manceinsomecases. ThisshowsthathighlyuncalibratedQestimatescanstill,sometimes,leadto
adequateperformance. Wehypothesizethatthisoccursbecausethesetasksdonotnecessarilyneed
s tomatchs∗ ∗ toachievehighreturn,butweleaveinvestigatingthispointtofuture
T ∼ B ∼ Bmain
work.
E.3 AuxiliaryTaskPerformanceResults
In our main text, we only presented performance results for main tasks, since our primary goal
wastoefficientlygeneratepoliciesthatperformedwellonaparticulartask. However,ourscheduled
auxiliarytaskapproachinherentlylearnsmultipleauxiliarytaskpoliciesπ inadditiontothemain
Taux
taskpolicyπ . Whileperformanceontheseauxiliarytaskpoliciesisnotnecessarilyexpectedto
Tmain
be high throughout learning, observing auxiliary task performance can potentially provide useful
28
)0001
(nruteRedosipE
×Max Max
Figure14: Additionalresults,fromthePanda(left)andSawyer(right)tasks,showingthedifferencebetween
per-timestep Q values and the average Q values for data from B∗ . The most difficult tasks, with a larger
main
varietyofpossiblestates,tendtohaveviolationsoftheidealmaximumwhenvaluepenalizationisexcluded,
oftenexacerbatedwhenACEisused.
29Max
Figure15:AdditionalresultsfromtheAdroittasksshowingthedifferencebetweenper-timestepQvaluesand
theaverageQvaluesfordatafromB∗ .
main
30Release Reach
1.10.00 1.00
VPACE-SQIL
0.75 0.75
VPACE-DAC
0.50 0.50 ACE-SQIL
0.25 0.25 ACE-RCE
0.00 0.00
0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0
Grasp Lift
1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75
0.05.08 0.50 0.50 0.50
0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00
0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0
Main
1.00 1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25 0.25
0.00.06 0.00 0.00 0.00 0.00
0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0
1.00 1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00 0.00
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
1.00.04 1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00 0.00
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
1.00 1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75 0.75
0.05.02 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00 0.00
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
1.00 1.00 1.00 1.00 1.00
0.75 0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25 0.25
0.00.00 0.00 0.00 0.00 0.00
0.00 2 4 6 8 10 0.20 2 4 6 8 10 0.40 2 4 6 8 10 0.6 0 2 4 6 8 100.8 0 2 4 6 8 101.0
EnvironmentSteps( 100k)
×
Figure 16: Auxiliary task performance for all of our methods that employ auxiliary control from examples.
Maintasksaredividedbyrow,andauxiliarytasksaredividedbycolumn,withtheright-mostcolumnineach
rowshowingmaintaskperformance. Performanceonauxiliarytasksmostlyfollowsperformanceonthemain
task,indicatingthattheseauxiliarytasksmaybeusefulfortransferlearningtoothermaintasks.
insights,suchaswhethertheauxiliarypolicieswouldbeusefulfortransferlearningtolearnanew
maintask.
WeshowallauxiliarytaskperformanceinforourPandaenvironmentsinFig.16. Auxiliarytasks
tendtobelearnedslightlybeforethemaintaskislearned,andotherwiseauxiliarytaskperformance
generally follows performance on the main task (i.e. if the main task is learned slowly or poorly,
auxiliarytaskperformancewillreflectthis). TheperformanceontheReachauxiliarytaskissome-
timeslowerthanonother,ostensiblymoredifficulttaskssuchasLift,butthisislikelybecausethe
successcriteriaforReachislessforgivingthanthatforLift.
We show auxiliary task performance for Sawyer and Adroit Hand environments in Fig. 17 and
Fig.18. Returnsforauxiliarytasksarecalculatedbasedonrewardfunctionsoriginallytakenfrom
[3],inwhichareachingrewardispositiveasanend-effectorreachesclosertoanobject,andnegative
asitmovesaway,andgraspreceivesasimilarbonusformovingorliftingtheobjectofinterest. In
our ACE experiments for Sawyer and Adroit, we only use auxiliary tasks as part of a handcrafted
high-level trajectory (see Appendix D.6.2), meaning that reach and grasp are only learned after a
31
etaRsseccuS
hcaeR
tfiL
kcolB-evoM
kcatS
kcatS-kcatsnU
gnirB
tresnIReach Grasp Main
1.0
0.2 0.3
0.10
0.2
0.1 0.05
0.1
0.0 0.0 0.00
0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0
0.20
00..28 0.3 0.15
0.2 0.10
0.1
0.1 0.05
0.0 0.0 0.00
0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0
0.2 0.3 0.2
0.6 0.2
0.1 0.1
0.1
0.0 0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
0.2 0.3 0.06
0.2 0.04
00..14 0.02
0.1
0.00
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
0.2 0.3 0.2
0.2
0.1 0.1
0.1
0.2
0.0 0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
0.2 0.3 0.20
0.15
0.2
0.1 0.10
0.1 0.05
00..00 0.0 0.00
00..00 0.6 1.2 1.80.22.4 3.0 0.0 0.04.6 1.2 1.8 2.40.63.0 0.0 0.60.81.2 1.8 2.4 3.10.0
EnvironmentSteps( 100k)
×
VPACE-SQIL VPACE-DAC ACE-SQIL ACE-RCE
Figure17:Sawyerauxiliarytaskperformance.IntheSawyertasks,auxiliarytaskvariesbutisoftenreasonable
formethodswithvaluepenalization,andpoorerwhenitisexcluded.
reset, whereas during evaluation, the policy may have an opportunity to guide the end-effector to
partially reach an object, then move away, with no learned strategy for recovery. We suspect that
using a weighted random scheduler throughout learning may alleviate this issue, but we did not
explorethisinthiswork,sincelearninggenerallyeffectiveorreusableauxiliarytasksisbeyondthe
scopeofthiswork.
32
nruteRedosipE
neporewardreywasesolcrewardreywas
hsupreywas
tfilreywas
esolcxobreywas
gnikcipnibreywasReach Grasp Main
1.0 1.5
0.2 1.5
− 1.0
1.0
0.4
− 0.5
0.5
−0.6 0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
3.0
0.04
0.8
0.1 2.0
0.0 0.02 1.0
−0.1 0.00 0.0
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10
−0.1 0.00 2.0
00..26
−0.05 1.5
− 0.10 1.0
−
0.3 0.5
− 0.15
− 0.0
0 3 6 9 12 15 0 3 6 9 12 15 0 3 6 9 12 15
1.5 3.0
0.0
1.0 2.0
−00..24
0.5 1.0
0.4
− 0.0 0.0
0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0 0.0 0.6 1.2 1.8 2.4 3.0
0 0.0 10.0
7.5
0.2
−1 − 5.0
0.4
− 2.5
−0.22
−0.6 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
0.0
−0.5 2.0
−1.0 −0.2
−1.5 −0.4 1.0
2.0
− 0.6
−20..50
0.00 3 6 90.212
15−
0 0.43 6 9 120.615
0.0
0 30.86 9 12 151.0
EnvironmentSteps( 100k)
×
VPACE-SQIL VPACE-DAC ACE-SQIL ACE-RCE
Figure18:Adroithandauxiliarytaskperformance.Particularlyinthehandenvironments,auxiliarytaskperfor-
mancecanbequitepoor,whichweexpectisbecauseweevaluateauxiliarytasksunderdifferentcircumstances
fromhowtheyareusedduringlearning.
33
)niaMrof0001
(nruteRedosipE
×
pd-0v-namuh-rood
pd-0v-namuh-remmah
pd-pjan-0v-namuh-etacoler
0v-namuh-rood
0v-namuh-etacoler
0v-namuh-remmahF ExpandedLimitations
Inthissection,weexpandonsomeofthelimitationsoriginallydiscussedinSection5,wherewepre-
viouslyskippedlimitationsthatareinherenttoreinforcementlearningandtolearningfromguided
play(LfGP,[4]),bothofwhicharecorepartsofourapproach.
Experimental limitation—Numerical state data only All of our tests are done with numerical
data, instead of image-based data. Other work [31, 47] has shown that for some environments,
image-based learning just results in slowing learning compared with numerical state data, and we
assumethatthesamewouldbetrueforourmethodaswell.
Assumption—GeneratingexamplesuccessstatesiseasierWeclaimthatsuccessexampledistri-
butionsareeasiertogeneratethanfulltrajectoryexpertdata,andwhileweexpectthistobetruein
almostallcases,theremaystillbetasksorenvironmentswhereaccomplishingthisisnottrivial. As
well, similar to other imitation learning methods, knowing how much data is required to generate
aneffectivepolicyisunknown,butaddingawaytoappendtotheexistingsuccessstatedistribution
(e.g.,[21])wouldpresumablyhelpmitigatethis.
Assumption/failuremode—UnimodalexampledistributionsAlthoughwedonotexplicitlyclaim
that unimodal example state distributions are required for VPACE to work, all of our tested tasks
haveroughlyunimodalexamplestatedistributions. Itisnotclearwhetherourmethodwouldgrace-
fullyextendtothemultimodalcase,andinvestigatingthisisaninterestingdirectionforfuturework.
Experimentallimitation—Someenvironment-specifichyperparametersWhilethevastmajority
of hyperparameters were transferable between all environments and algorithms, the scheduler pe-
riod, theinclusionofn-steptargets, andtheuseofentropyinTDupdates, weredifferentbetween
environments to maximize performance. Scheduler periods will be different for all environments,
but future work should further investigate why n-step targets and the inclusion of entropy in TD
updatesmakesenvironment-specificdifferences.
F.1 VPACEandLfGPLimitations
VPACEsharesthefollowinglimitationswithLfGP[4].
Assumption—Existence of auxiliary task datasets VPACE and LfGP require the existence of
auxiliary task example datasets ∗ , in addition to a main task dataset ∗ . This places higher
Baux Bmain
initialburdenonthepractitioner. Infuture,choosingenvironmentswherethisdatacanbereusedas
muchaspossiblewillreducethisburden.
Assumption—ClearauxiliarytaskdefinitionsVPACEandLfGPrequireapractitionertomanu-
allydefineauxiliarytasks. Weexpectthistobecomparativelyeasierthangeneratingasimilardense
rewardfunction,sinceitdoesnotrequireevaluatingtherelativecontributionofindividualauxiliary
tasks. Aswell,alltasksstudiedinthisworksharetaskdefinitions,andthepandaenvironmenteven
sharestaskdataitself,leadingustoassumethatthesetaskdefinitionswillextendtoothermanipu-
lationtasksaswell.
Assumption—Clear choices for handcrafted scheduler trajectories VPACE and LfGP use a
combinationofaweightedrandomschedulerwithahandcraftedscheduler,randomlysamplingfrom
pre-defined trajectories of high level tasks. Ablett et al. [4] found that the handcrafted scheduler
added little benefit compared with a weighted random scheduler, and further work should investi-
gatethisclaim,orperhapsattempttousealearnedscheduler,asin[31].
F.2 ReinforcementLearningLimitations
Experimentallimitation—FreeenvironmentexplorationAsiscommoninreinforcementlearn-
ing methods, our method requires exploration of environments for a considerable amount of time
(ontheorderofhours),whichmaybeunacceptablefortaskswith,e.g.,delicateobjects.
34