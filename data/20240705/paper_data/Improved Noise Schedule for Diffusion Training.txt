TechnicalReport
IMPROVED NOISE SCHEDULE FOR DIFFUSION TRAIN-
ING
TiankaiHang ShuyangGu
∗
SoutheastUniversity MicrosoftResearchAsia
tkhang@seu.edu.cn shuyanggu@microsoft.com
ABSTRACT
Diffusion models have emerged as the de facto choice for generating visual sig-
nals. However,trainingasinglemodeltopredictnoiseacrossvariouslevelsposes
significantchallenges,necessitatingnumerousiterationsandincurringsignificant
computationalcosts. Variousapproaches, suchaslossweightingstrategydesign
and architectural refinements, have been introduced to expedite convergence. In
thisstudy,weproposeanovelapproachtodesignthenoisescheduleforenhancing
thetrainingofdiffusionmodels. Ourkeyinsightisthattheimportancesampling
ofthelogarithmoftheSignal-to-Noiseratio(logSNR),theoreticallyequivalentto
a modified noise schedule, is particularly beneficial for training efficiency when
increasing the sample frequency around logSNR = 0. We empirically demon-
strate the superiority of our noise schedule over the standard cosine schedule.
Furthermore, we highlight the advantages of our noise schedule design on the
ImageNet benchmark, showing that the designed schedule consistently benefits
differentpredictiontargets.
1 INTRODUCTION
Diffusion models have emerged as a pivotal technique for generating visual signals across diverse
domains,suchasimagesynthesis(Rameshetal.,2022;Sahariaetal.,2022;Rombachetal.,2022)
and video generation (Brooks et al., 2024). They are particularly adept at approximating complex
distributions,whereGenerativeAdversarialNetworks(GANs)mayencounterdifficulties. Despite
thesubstantialcomputationalresourcesandnumeroustrainingiterationsrequiredforconvergence,
improvingthetrainingefficiencyofdiffusionmodelsisessentialfortheirapplicationinlarge-scale
scenarios,suchashigh-resolutionimagesynthesisandvideogeneration.
Architectural enhancements offer a promising path to improve both the training speed and perfor-
manceofdiffusionmodels. Forinstance,theuseofAdaptiveLayerNormalization(Guetal.,2022),
whencombinedwithzeroinitializationintheTransformerarchitectureasdemonstratedbyPeebles
&Xie(2023), representssuchanimprovement. Similarly, theadoptionofU-shapedskipconnec-
tionswithinTransformers,asoutlinedinpreviousworks(Hoogeboometal.,2023;Baoetal.,2022;
Crowson et al., 2024), also boosts efficiency. In a parallel development, Karras et al. (2024) have
contributedtothisendeavorbyreengineeringthelayersofADMUNet(Dhariwal&Nichol,2021)
topreservethemagnitudesofactivations, weights, andupdates, ensuringamoreefficientlearning
process.
Concurrently,variouslossweightingdesignshavebeenimplementedtoacceleratetheconvergence
oftraining. Previousworks,suchaseDiff-I(Balajietal.,2022)andMin-SNR(Hangetal.,2023),
foundthatthetrainingofdiffusionmodelsmayencounterconflictsamongvariousnoiseintensities.
Choietal.(2022)prioritizespecificnoiselevelsduringtrainingtoenhancelearningofvisualcon-
cepts. Min-SNR(Hangetal.,2023)reducesweightsofnoisytasks,pursuingtheParetoOptimality
in different denoising tasks, validated its effectiveness on multiple datasets and architectures. A
softer version of this approach, aiming to further enhance high-resolution image synthesis within
hourglassdiffusionmodels,wasintroducedbyCrowsonetal.(2024). SD3(Esseretal.,2024)em-
piricallyfoundthatit’scrucialtoincreasetheweightoftheintermediatenoiseintensities,whichhas
demonstratedtheeffectivenessduringtrainingthediffusionmodels.
∗Long-termresearcherinternatMicrosoftResearchAsia.
1
4202
luJ
3
]VC.sc[
1v79230.7042:viXraTechnicalReport
0.5
Cosine: sech(λ/2)/(2π)
Cosine Scaled: s sech(λs/2)/(2π),s=2
·
0.4 Cosine Shifted: sech(λ/2 s)/(2π),s= 3
− −
Laplace: 1 exp( λ µ/b),µ=0,b=1
2b −| − |
Cauchy: 1 γ ,γ =1,λ =3
0.3 πγ2+(λ −λ0)2 0
0.2
0.1
0.0
20 15 10 5 0 5 10
− − − −
λ=log SNR
Figure1: Illustrationoftheprobabilitydensityfunctionsofdifferentnoiseschedules.
In this study, we present a novel method to enhance the training of diffusion models by strategi-
callyredefiningthenoiseschedule,whichisequivalenttoimportancesamplingofthenoiseacross
differentintensities. However,empiricalevidencesuggeststhatallocatingmorecomputationcosts
(FLOPs)tomid-rangenoiselevels(aroundlogSNR=0)yieldssuperiorperformancecomparedto
increasinglossweightsduringthesameperiod, particularlyunderconstrainedcomputationalbud-
gets. We experimentally analyze the performance of several different noise schedules, including
Laplace, Cauchy, and the Cosine Shifted/Scaled, which are visualized in Figure 1. Notably, the
Laplacescheduleexhibitsfavorableperformance. Werecommendtochoosethisnoiseschedulein
thefuture.
WedemonstratetheeffectivenessofthisapproachusingtheImageNetbenchmark,withaconsistent
training budget of 500K iterations. Evaluated using the FID metric, our results reveal that noise
scheduleswithaconcentratedprobabilitydensityaroundlogSNR = 0consistentlysurpassothers,
as evidenced at both 256 256 and 512 512 resolutions with different prediction target. This
× ×
researchcontributestotheadvancementofefficienttrainingtechniquesfordiffusionmodels.
2 METHOD
2.1 PRELIMINARIES
Diffusionmodels(Hoetal.,2020;Yangetal.,2021)learntogeneratedatabyiterativelyreversing
the diffusion process. We denote the distribution of data points as x p (x). The diffusion
data
∼
processprogressivelyaddsnoisetothedata,whichisdefinedas:
x =α x+σ ϵ, where ϵ (0,I), (1)
t t t
∼N
whereα andσ arethecoefficientsoftheaddingnoiseprocess,essentiallyrepresentingthenoise
t t
schedule. For the commonly used prediction target velocity: v = α ϵ σ x (Salimans & Ho,
t t t
−
2022),thediffusionmodelθistrainedthroughtheMeanSquaredError(MSE)loss:
(θ)=E E (cid:2) w(t) v (α x+σ ϵ,t,c) v 2(cid:3) , (2)
L x ∼pdata(x) t ∼p(t) ∥ θ t t − t ∥2
where w(t) is the loss weight, c denotes the condition information. Common practices sample t
fromtheuniformdistribution [0,1]. Kingmaetal.(2021)introducedtheSignal-to-Noiseratioas
U
SNR(t)=
α2
t tomeasurethenoiselevelofdifferentstates. Tosimplify,wedenoteλ=logSNRto
σ2
t
2
)λ(pTechnicalReport
NoiseSchedule p(λ) λ(t)
Cosine sech(λ/2)/2π
2logcot(cid:0)πt(cid:1)
2
|λ−µ|
Laplace e − b /2b µ bsgn(0.5 t)log(1 2t 0.5)
− − − | − |
Cauchy 1 γ µ+γtan(cid:0)π(1 2t)(cid:1)
CosineShifted π 21 π( sλ e− cµ h)2 (cid:16)+ λγ −22 µ(cid:17) µ+2log(cid:0) c2 ot(cid:0)− π 2t(cid:1)(cid:1)
CosineScaled s sech(cid:0)sλ(cid:1) 2log(cid:0) cot(cid:0)πt(cid:1)(cid:1)
2π 2 s 2
Table1: OverviewofvariousNoiseSchedules. Thetablecategorizesthemintofivedistincttypes:
Cosine,Laplace,Cauchy,andtwovariationsofCosineschedules. Thesecondcolumnp(λ)denotes
the sampling probability at different noise intensities λ. The last column λ(t) indicates how to
samplenoiseintensitiesfortraining. WederivedtheirrelationshipinEquation4andEquation6.
indicatethenoiseintensities. IntheVariancePreserving(VP)setting,thecoefficientsinEquation1
canbecalculatedbyα2 = exp(λ) ,σ2 = 1 .
t exp(λ)+1 t exp(λ)+1
2.2 IMPROVEDNOISESCHEDULEDESIGN
Giventhatthetimesteptisarandomvariablesampledfromuniformdistribution,thenoiseschedule
implicitly defines the distribution of importance sampling on various noise levels. The sampling
probabilityofnoiseintensityλis:
(cid:12) (cid:12)
(cid:12)dt(cid:12)
p(λ)=p(t)(cid:12) (cid:12). (3)
(cid:12)dλ(cid:12)
Consideringthattsatisfiesuniformdistribution,andλismonotonicallydecreasingwitht,wehave:
dt
p(λ)= . (4)
−dλ
We take cosine noise schedule (Nichol & Dhariwal, 2021) as an example, where α =
cos(cid:0)πt(cid:1)
,
t 2
σ =
sin(cid:0)πt(cid:1)
. Thenwecandeducethatλ = 2logtan(πt/2)andt = 2/πarctane λ/2. Thus
t 2 − −
thedistributionofλis: p(λ)= dt/dλ=sech(λ/2)/2π. Thisderivationillustratestheprocessof
−
obtainingp(λ)fromanoisescheduleλ(t).Ontheotherhand,wecanderivethenoiseschedulefrom
thesamplingprobabilityofdifferentnoiseintensitiesp(λ). ByintegratingEquation4,wehave:
(cid:90) λ
t=1 p(λ)dλ= (λ), (5)
− P
−∞
λ= 1(t), (6)
−
P
where (λ) represents the cumulative distribution function of λ. Thus we can obtain the noise
schedulP e λ by applying the inverse function 1. In conclusion, during the training process, the
−
P
importancesamplingofvaryingnoiseintensitiesessentiallyequatestothemodificationofthenoise
schedules.
2.3 UNIFIEDFORMULATIONFORDIFFUSIONTRAINING
VDM++(Kingma&Gao,2023)proposesaunifiedformulationthatencompassesrecentprominent
frameworksandlossweightingstrategiesfortrainingdiffusionmodels,asdetailedbelow:
(cid:20) (cid:21)
1 w(λ)
(θ)= E ϵˆ (x ;λ) ϵ 2 , (7)
Lw 2 x ∼D,ϵ ∼N(0,I),λ ∼p(λ) p(λ) ∥ θ λ − ∥2
where signifiesthetrainingdataset,noiseϵisdrawnfromastandardGaussiandistribution,and
D
p(λ)isthedistributionofnoiseintensities. Differentpredictingtargets,suchasx andv,canalso
0
bere-parameterizedtoϵ-prediction. w(λ)denotesthelossweightingstrategy. Althoughadjusting
w(λ) is theoretically equivalent to altering p(λ). In practical training, directly modifying p(λ) to
concentratecomputationalresourcesontrainingspecificnoiselevelsismoreeffectivethanenlarging
thelossweightonspecificnoiselevels. Therefore,wefocusonhowtodesignp(λ).
3TechnicalReport
Method w(λ) p(λ)
Cosine e λ/2 sech(λ/2)
−
Min-SNR e λ/2 min 1,γe λ sech(λ/2)
− −
· { }
Soft-Min-SNR e λ/2 γ/(eλ+γ) sech(λ/2)
−
·
FM-OT (1+e λ)sech2(λ/4) sech2(λ/4)/8
−
EDM (1+e λ)(0.52+e λ) (λ;2.4,2.42) (0.52+e λ) (λ;2.4,2.42)
− − −
N N
Table2: Comparisonofdifferentmethodsandrelatedlossweightingstrategies. Thew(λ)isintro-
ducedinEquation7.
2.4 PRACTICALSETTINGS
StableDiffusion3(Esseretal.,2024),EDM(Karrasetal.,2022),andMin-SNR(Hangetal.,2023;
Crowson et al., 2024) find that the denoising tasks with medium noise intensity is most critical to
theoverallperformanceofdiffusionmodels. Therefore,weincreasetheprobabilityofp(λ)whenλ
isofmoderatesize,andobtainanewnoisescheduleaccordingtoSection2.2.
Specifically, we investigate four novel noise strategies, named Cosine Shifted, Cosine Scaled,
Cauchy, and Laplace respectively. The detailed setting are listed in Table 1. Cosine Shifted use
the hyperparameter µ to explore where the maximum probability should be used. Cosine Scaled
explores how much the noise probability should be increased under the use of Cosine strategy to
achieve better results. The Cauchy distribution, provides another form of function that can adjust
bothamplitudeandoffsetsimultaneously. TheLaplacedistributionischaracterizedbyitsmeanµ
and scale b, controls both the magnitude of the probability and the degree of concentration of the
distribution.Thesestrategiescontainseveralhyperparameters,whichwewillexploreinSection3.5.
Unlessotherwisestated,wereportthebesthyperparameterresults.
Byre-allocatingthecomputationresourcesatdifferentnoiseintensities,wecantrainthecomplete
denoising process. During sampling process, we standardize the sampled SNR to align with the
cosineschedule,therebyfocusingourexplorationsolelyontheimpactofdifferentstrategiesduring
training. Itisimportanttonotethat,fromtheperspectiveofthenoiseschedule,howtoallocatethe
computationresourceduringinferenceisalsoworthreconsideration. Wewillnotexploreitinthis
paperandleavethisasfuturework.
3 EXPERIMENTS
3.1 IMPLEMENTATIONDETAILS
Dataset. WeconductexperimentsonImageNet(Dengetal.,2009)with256 256and512 512
× ×
resolution.Foreachimage,wefollowthepreprocessinginRombachetal.(2022)tocentercropand
encodeimagestolatents. Theshapeofcompressedlatentfeatureis32 32 4for2562 images
and64 64 4for5122images. × ×
× ×
NetworkArchitecture. WeadoptDiT-BfromPeebles&Xie(2023)asourbackbone. Wereplace
thelastAdaLNLinearlayerwithvanillalinear. Othersarekeptthesameastheoriginalimplemen-
tation.
TrainingSettings. WeadopttheAdamoptimizerwithlearningrate1 10 4. Wesetthebatchsize
−
×
to256asPeebles&Xie(2023);Hangetal.(2023). Eachmodelistrainedfor500Kiterationsifnot
specified. OurimplementationismainlybuiltonOpenDiT(Zhaoetal.,2024)andexperimentsare
mainlyconductedon8 16GV100GPUs.
×
Baselines and Metrics. We compare our proposed noise schedule with several baseline settings
in Table 2. For each setting, we sample images using DDIM (Song et al., 2021) with 50 steps.
Despitethenoisestrategyfordifferentsettingsmaybedifferent,weensuretheyarethesameateach
samplingstep. Thisapproachisadoptedtoexclusivelyinvestigatetheimpactofthenoisestrategy
duringthetrainingphase. Moreover,wereportresultswithdifferentclassifier-freeguidancescales,
andtheFIDiscalculatedusing10Kgeneratedimages.
4TechnicalReport
3.2 COMPARISONWITHBASELINESANDLOSSWEIGHTDESIGNS
ThissectiondetailstheprincipalfindingsfromourexperimentsontheImageNet-256dataset,focus-
ingonthecomparativeeffectivenessofvariousnoiseschedulesandlossweightingsinthecontext
ofCFGvalues. Table3illustratesthesecomparisons,showcasingtheperformanceofeachmethod
intermsoftheFID-10Kscore.
The experiments reveal that our proposed noise schedules, particularly Laplace, achieve the most
notable improvements over the traditional cosine schedule, as indicated by the bolded best scores
andthebluenumbersrepresentingthereductionscomparedtobaseline’sbestscoreof10.85.
We also provide a comparison with methods that adjust the loss weight, including Min-SNR and
Soft-Min-SNR. We find that although these methods can achieve better results than the baseline,
they are still not as effective as our method of modifying the noise schedule. This indicates that
deciding where to allocate more computational resources is more efficient than adjusting the loss
weight. Compared with other noise schedules like EDM (Karras et al., 2022) and Flow (Lipman
etal.,2022),wefoundthatnomatterwhichCFGvalue,ourresultssignificantlysurpasstheirsunder
thesametrainingiterations.
Method CFG=1.5 CFG=2.0 CFG=3.0
Cosine(Nichol&Dhariwal,2021) 17.79 10.85 11.06
EDM(Karrasetal.,2022) 26.11 15.09 11.56
FM-OT(Lipmanetal.,2022) 24.49 14.66 11.98
Min-SNR(Hangetal.,2023) 16.06 9.70 10.43
Soft-Min-SNR(Crowsonetal.,2024) 14.89 9.07 10.66
CosineShifted(Hoogeboometal.,2023) 19.34 11.67 11.13
CosineScaled 12.74 8.04 11.02
Cauchy 12.91 8.14 11.02
Laplace 16.69 9.04 7.96(-2.89)
Table 3: Comparison of various noise schedules and loss weightings on ImageNet-256, showing
theperformance(intermsofFID-10K)ofdifferentmethodsunderdifferentCFGvalues. Thebest
results highlighted in bold and the blue numbers represent the improvement when compared with
thebaselineFID10.85. Thelineingrayisoursuggestednoiseschedule.
Furthermore, we investigate the convergence speed of these method, and the results are shown in
Figure 2. It can be seen that adjusting the noise schedule converges faster than adjusting the loss
weight.Additionally,wealsonoticethattheoptimaltrainingmethodmayvarywhenusingdifferent
CFGvaluesforinference,butadjustingthenoiseschedulegenerallyyieldsbetterresults.
3.3 ROBUSTNESSONDIFFERENTPREDICTINGTARGETS
Weevaluatetheeffectivenessofourdesignednoisescheduleacrossthreecommonlyadoptedpre-
dictiontargets: ϵ,x andv. TheresultsareshowninTable4.
0
Weobservedthatregardlessofthepredictiontarget,ourproposedLaplacestrategysignificantlyout-
performstheCosinestrategy. It’snoteworthythatastheLaplacestrategyfocusesthecomputation
onmediumnoiselevelsduringtraining,theextensivenoiselevelsarelesstrained,whichcouldpo-
tentiallyaffecttheoverallperformance. Therefore,wehaveslightlymodifiedtheinferencestrategy
ofDDIMtostartsamplingfromt =0.99.
max
3.4 ROBUSTNESSONHIGHRESOLUTIONIMAGES
Toexploretherobustnessoftheadjustednoisescheduletodifferentresolutions, wealsodesigned
experimentsonImagenet-512. AspointedoutbyChen(2023),theaddingnoisestrategywillcause
more severe signal leakage as the resolution increases. Therefore, we need to adjust the hyperpa-
rametersofthenoisescheduleaccordingtotheresolution.
5TechnicalReport
25.0 Cosine(CFG=3,0)
Min-SNR(CFG=3,0)
22.5 Soft-Min-SNR(CFG=3,0)
Laplace-(0,0.5)(CFG=3,0)
20.0
17.5
15.0
12.5
11.06
10.0
7.96
7.5
100K 200K 300K 400K 500K
TrainingIterations
Figure2: Comparisonbetweenadjustingthenoiseschedule,adjustingthelossweightsandbaseline
setting. TheLaplacenoisescheduleyieldsthebestresultsandthefastestconvergencespeed.
PredictTarget NoiseSchedule 100K 200k 300k 400k 500k
x Cosine 35.20 17.60 13.37 11.84 11.16
0
Laplace(Ours) 21.78 10.86 9.44 8.73 8.48
v Cosine 25.70 14.01 11.78 11.26 11.06
Laplace(Ours) 18.03 9.37 8.31 8.07 7.96
ϵ Cosine 28.63 15.80 12.49 11.14 10.46
Laplace(Ours) 27.98 13.92 11.01 10.00 9.53
Table4: EffectivenessevaluatedusingFID-10Kscoreondifferentpredictingtargets. Theproposed
LaplacescheduleperformsbetterthanthebaselineCosineschedulealongwithtrainingiterations.
Specifically,thebaselineCosinescheduleachievesthebestperformancewhentheCFGvalueequals
to3. SowechoosethisCFGvalueforinference. Throughsystematicexperimentation,weexplored
the appropriate values for the Laplace schedule’s parameter b, testing within the range 0.5, 0.75,
{
1.0 ,anddeterminedthatb = 0.75wasthemosteffective,resultinginanFIDscoreof9.09. This
}
indicatesthatdespitetheneedforhyperparametertuning,adjustingthenoiseschedulecanstillstably
bringperformanceimprovements.
NoiseSchedule Cosine Laplace
FID-10K 11.91 9.09(-2.82)
Table5: FID-10KresultsonImageNet-512. Allmodelsaretrainedfor500Kiterations.
3.5 ABLATIONSTUDY
Weconductanablationstudytoanalyzetheimpactofhyperparametersonvariousdistributionsof
p(λ),whichareenumeratedbelow.
Laplace distribution is easy to implement and we adjust the scale to make the peak at the
middle timestep. We conduct experiments with different Laplace distribution scales b
∈
0.25,0.5,1.0,2.0,3.0 . The results are shown in Figure 3. The baseline with standard cosine
{ }
scheduleachievesFIDscoreof17.79withCFG=1.5,10.85withCFG=2.0,and11.06withCFG=3.0
6
k01-DIFTechnicalReport
Laplace,CFG=1.5
30 Laplace,CFG=2.0
Laplace,CFG=3.0
Baseline,CFG=1.5
25
Baseline,CFG=2.0
Baseline,CFG=3.0
20
17.79
15
12.93
11.06
1010.85
7.96 8.19
0.5 1.0 1.5 2.0 2.5 3.0
b
Figure 3: FID-10K results on ImageNet-256 with different Laplace distribution scales b in
0.25,0.5,1.0,2.0,3.0 . The location parameter µ is fixed to 0. Baseline denotes standard co-
{ }
sineschedule.
after500Kiterations. WecanseethatthemodelwithLaplacedistributionscaleb = 0.5achieves
thebestperformance7.96withCFG=3.0,whichisrelatively 26.6%betterthanthebaseline.
Cauchydistributionisanotherheavy-taileddistributionthatcanbeusedfornoisescheduledesign.
The distribution is not symmetric when the location parameter is not 0. We conduct experiments
withdifferentCauchydistributionparametersandtheresultsareshowninTable6. Cauchy(0,0.5)
means 1 γ with µ = 0,γ = 0.5. We can see that the model with µ = 0 achieve better
π(λ µ)2+γ2
performanc−e than the other two settings when fixing γ to 1. It means that the model with more
probabilitymassaroundλ=0performsbetterthanothersbiasedtonegativeorpositivedirections.
Cauchy(0,0.5) Cauchy(0,1) Cauchy(-1,1) Cauchy(1,1)
CFG=1.5 12.91 14.32 18.12 16.60
CFG=2.0 8.14 8.93 10.38 10.19
CFG=3.0 11.02 11.26 10.81 10.94
Table6: FID-10kresultsonImageNet-256withdifferentCauchydistributionparameters.
Cosine Shifted (Hoogeboom et al., 2023) is the shifted version of the standard cosine schedule.
We evaluate the schedules with both positive and negative µ values. Shifted with µ = 1 achieves
FID-10kscore 19.34,11.67,11.13 withCFG 1.5,2.0,3.0 . Resultswithshiftedvalueµ = 1
{ } { } −
are 19.30,11.48,11.28 . Comparatively,bothscenariosdemonstrateinferiorperformancerelative
{ }
tothebaselinecosineschedule(µ = 0). Additionally,byexaminingthedatapresentedinTable6,
wefindconcentratedonλ=0canbestimprovetheresults.
Cosine Scaled is also a modification of Cosine schedule. When s is equal to 1, it becomes the
standard Cosine version. s > 1 means sampling more heavily around λ = 0 while s < 1 means
samplingmoreuniformlyofallλ. WereportrelatedresultsinTable7. Largervaluesofs(s > 1)
outperformthebaseline;however,sshouldnotbeexcessivelylargeandmustremainwithinavalid
range. Amodeltrainedwiths=2attainsascoreof8.04,representinga25.9%improvementover
thebaseline.
7
k01-DIFTechnicalReport
1/s 1.3 1.1 0.5 0.25
CFG=1.5 39.74 22.60 12.74 15.83
CFG=2.0 23.38 12.98 8.04 8.64
CFG=3.0 13.94 11.16 11.02 8.26
Table7: FID-10kresultsonImageNet-256withdifferentscalesofCosineScaleddistribution.
4 RELATED WORK
EFFICIENTDIFFUSIONTRAINING
Generallyspeaking,thediffusionmodelusesanetworkwithsharedparameterstodenoisedifferent
noiseintensities. However,thedifferentnoiselevelsmayintroduceconflictsduringtraining,which
makes the convergence slow. Min-SNR (Hang et al., 2023) seeks the Pareto optimal direction for
different tasks, achieves better convergence on different predicting targets. HDiT (Crowson et al.,
2024)proposeasoftversionofMin-SNRtofurtherimprovetheefficiencyonhighresolutionimage
synthesis. Stable Diffusion 3 (Esser et al., 2024) puts more weight on the middle timesteps by
multiplyingthedistributionoflogitnormaldistribution.Ontheotherhand,architecturemodification
isalsoexploredtoimprovediffusiontraining. DiT(Peebles&Xie,2023)proposesadaptiveLayer
NormalizationwithzeroinitializationtoimprovethetrainingofTransformerarchitectures. Amore
robust ADM UNet with better training dynamics is proposed in EDM2 (Karras et al., 2024) by
preservingactivation,weight,andupdatemagnitudes.
NOISESCHEDULEDESIGNFORDIFFUSIONMODELS
Thedesignofthenoisescheduleplaysacriticalroleintrainingdiffusionmodels.InDDPM,Hoetal.
(2020)proposelinearscheduleforthenoiselevel,whichislaterusedinStableDiffusion(Rombach
etal.,2022)version1.5and2.0. iDDPM(Nichol&Dhariwal,2021)introducesacosineschedule
aimedatbringingthesamplewiththehighestnoiselevelclosertopureGaussiannoise. EDM(Kar-
ras et al., 2022) proposes a new continuous framework and make the logarithm of noise intensity
sampledfromaGaussiandistribution. Flowmatchingwithoptimaltransport(Lipmanetal.,2022;
Liu et al., 2022) linearly interpolates the noise and data point as the input of flow-based models.
Chen (2023) underscored the need for adapting the noise schedule according to the token length,
andseveralotherworks(Linetal.,2024;Tangetal.,2023)emphasizethatit’simportanttoprevent
signalleakageinthefinalstep.
5 CONCLUSION
In this technical report, we present a novel method for enhancing diffusion model training by re-
defining the noise schedule. We theoretically analyzed that this approach equates to performing
importancesamplingonthenoise. EmpiricalresultsshowthatourproposedLaplacenoisesched-
ule,focusingcomputationalresourcesonmid-rangesteps,yieldssuperiorperformancecomparedto
the adjustment of loss weights under constrained budgets. This study not only contributes signifi-
cantly to developing efficient training techniques for diffusion models but also offers potential for
futurelarge-scaleapplications.
REFERENCES
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika
Aittala,TimoAila,SamuliLaine,BryanCatanzaro,TeroKarras,andMing-YuLiu. ediff-i: Text-
to-imagediffusionmodelswithensembleofexpertdenoisers. arXivpreprintarXiv:2211.01324,
2022.
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth
words: Avitbackbonefordiffusionmodels. arXivpreprintarXiv:2209.12152,2022.
8TechnicalReport
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generation models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint
arXiv:2301.10972,2023.
JooyoungChoi,JungbeomLee,ChaehunShin,SungwonKim,HyunwooKim,andSungrohYoon.
Perceptionprioritizedtrainingofdiffusionmodels. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.11472–11481,2022.
Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z
Kaplan,andEnricoShippole.Scalablehigh-resolutionpixel-spaceimagesynthesiswithhourglass
diffusiontransformers. InForty-firstInternationalConferenceonMachineLearning,2024.
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehi-
erarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pp.248–255.Ieee,2009.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances
inNeuralInformationProcessingSystems,34:8780–8794,2021.
PatrickEsser, SumithKulal, AndreasBlattmann, RahimEntezari, JonasMu¨ller, HarrySaini, Yam
Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor
high-resolutionimagesynthesis. arXivpreprintarXiv:2403.03206,2024.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.10696–10706,2022.
Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining
Guo. Efficientdiffusiontrainingviamin-snrweightingstrategy. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),pp.7441–7451,October2023.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
NeuralInformationProcessingSystems,33:6840–6851,2020.
EmielHoogeboom,JonathanHeek,andTimSalimans. simplediffusion: End-to-enddiffusionfor
high resolution images. In International Conference on Machine Learning, pp. 13213–13232.
PMLR,2023.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=k7FuTOWMOc7.
TeroKarras,MiikaAittala,JaakkoLehtinen,JanneHellsten,TimoAila,andSamuliLaine. Analyz-
ingandimprovingthetrainingdynamicsofdiffusionmodels. InProc.CVPR,2024.
DiederikKingma, TimSalimans, BenPoole, andJonathanHo. Variationaldiffusionmodels. Ad-
vancesinneuralinformationprocessingsystems,34:21696–21707,2021.
Diederik P Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple
data augmentation. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URLhttps://openreview.net/forum?id=NnMEadcdyD.
Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and
samplestepsareflawed. InProceedingsoftheIEEE/CVFwinterconferenceonapplicationsof
computervision,pp.5404–5411,2024.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMatthewLe.Flowmatch-
ingforgenerativemodeling. InTheEleventhInternationalConferenceonLearningRepresenta-
tions,2022.
9TechnicalReport
XingchaoLiu,ChengyueGong,etal. Flowstraightandfast: Learningtogenerateandtransferdata
withrectifiedflow.InTheEleventhInternationalConferenceonLearningRepresentations,2022.
AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels.
InInternationalConferenceonMachineLearning,pp.8162–8171.PMLR,2021.
WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition,pp.10684–10695,2022.
ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,SeyedKam-
yarSeyedGhasemipour, RaphaelGontijo-Lopes, BurcuKaragolAyan, TimSalimans, Jonathan
Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion mod-
els with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=08Yk-n5l2Al.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In
InternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=TIdIXIpzhoI.
JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InInterna-
tionalConferenceonLearningRepresentations,2021.
Zhicong Tang, Shuyang Gu, Chunyu Wang, Ting Zhang, Jianmin Bao, Dong Chen, and Baining
Guo. Volumediffusion: Flexible text-to-3d generation with efficient volumetric encoder. arXiv
preprintarXiv:2312.11459,2023.
S.Yang,J.Sohl-Dickstein,D.P.Kingma,A.Kumar,S.Ermon,andB.Poole. Score-basedgenera-
tivemodelingthroughstochasticdifferentialequations. InInternationalConferenceonLearning
Representations,2021.
Xuanlei Zhao, Zhongkai Zhao, Ziming Liu, Haotian Zhou, Qianli Ma, and Yang You. Opendit:
Aneasy,fastandmemory-efficientsystemfordittrainingandinference. https://github.
com/NUS-HPC-AI-Lab/OpenDiT,2024.
10TechnicalReport
APPENDIX A: DETAILED IMPLEMENTATION FOR NOISE SCHEDULE
WeprovideasimplePyTorchimplementationfortheLaplacenoisescheduleanditsapplicationin
training. This example can be adapted to other noise schedules, such as the Cauchy distribution,
by replacing the laplace noise schedule function. The model accepts noisy samples x ,
t
timestept,andanoptionalconditiontensorcasinputs. Thisimplementationsupportspredictionof
x ,v,ϵ .
0
{ }
1 import torch
2
3
4 def laplace_noise_schedule(mu=0.0, b=0.5):
5 # refer to Table 1
6 lmb = lambda t: mu - b * torch.sign(0.5 - t) * \
7 torch.log(1 - 2 * torch.abs(0.5 - t))
8 snr_func = lambda t: torch.exp(lmb(t))
9 alpha_func = lambda t: torch.sqrt(snr_func(t) / (1 + snr_func(t)))
10 sigma_func = lambda t: torch.sqrt(1 / (1 + snr_func(t)))
11
12 return alpha_func, sigma_func
13
14
15 def training_losses(model, x, timestep, condition, noise=None,
16 predict_target="v", mu=0.0, b=0.5):
17
18 if noise is None:
19 noise = torch.randn_like(x)
20
21 alpha_func, sigma_func = laplace_noise_schedule(mu, b)
22 alphas = alpha_func(timestep)
23 sigmas = sigma_func(timestep)
24
25 # add noise to sample
26 x_t = alphas.view(-1, 1, 1, 1) * x + sigmas.view(-1, 1, 1, 1) * noise
27 # velocity
28 v_t = alphas.view(-1, 1, 1, 1) * noise - sigmas.view(-1, 1, 1, 1) * x
29
30 model_output = model(x_t, timestep, condition)
31 if predict_target == "v":
32 loss = (v_t - model_output) ** 2
33 elif predict_target == "x0":
34 loss = (x - model_output) ** 2
35 else: # predict_target == "noise":
36 loss = (noise - model_output) ** 2
37
38 return loss.mean()
APPENDIX B: DETAILS FOR SAMPLING PROCESS
As we mentioned before, choosing which noise schedule for sampling worth exploration. In this
paper, we focus on exploring what kind of noise schedule is needed for training. Therefore, we
adoptedthesameinferencestrategyasthecosinescheduletoensureafaircomparison. Specifically,
firstwesample t ,t ,...,t fromuniformdistribution [0,1],thengetthecorrespondingSNRs
0 1 s
{ } U
from Cosine schedule:
α2 t0,α2 t1,...,α2
ts . According to Equation 6, we get the corresponding
{σ t2
0
σ t2
1
σ t2 s}
t ,t ,...,t by inverting these SNR values through the respective noise schedules. Finally, we
{
′0 ′1 ′s}
useDDIM(Songetal.,2021)tosamplewiththesenewcalculated t .
′
{ }
11