Universal Length Generalization with Turing
Programs
KaiyingHou DavidBrandfonbrener ShamKakade
HarvardUniversity KempnerInstitute KempnerInstitute
atHarvardUniversity atHarvardUniversity
SamyJelassi∗ EranMalach∗
CenterofMathematicalSciences KempnerInstitute
andApplicationsatHarvardUniversity atHarvardUniversity
Abstract
Length generalization refers to the ability to extrapolate from short training se-
quencestolongtestsequencesandisachallengeforcurrentlargelanguagemodels.
Whilepriorworkhasproposedsomearchitectureordataformatchangestoachieve
length generalization, these proposals typically apply to a limited set of tasks.
BuildingonpriorscratchpadandChain-of-Thought(CoT)techniques,wepropose
TuringPrograms,anovelCoTstrategythatdecomposesanalgorithmictaskinto
stepsmimickingthecomputationofaTuringMachine. Thisframeworkisboth
universal,asitcanaccommodateanyalgorithmictask,andsimple,requiringonly
copyingtextfromthecontextwithsmallmodifications. Weshowthatbyusing
TuringPrograms,weobtainrobustlengthgeneralizationonarangeofalgorithmic
tasks: addition,multiplicationandin-contextSGD.Wethendemonstratethattrans-
formersachievelengthgeneralizationonrandomTuringPrograms,suggestingthat
lengthgeneralizationispossibleforanyalgorithmictask. Finally,wetheoretically
provethattransformerscanimplementTuringPrograms, constructingasimple
RASP(Weissetal. [53])programthatsimulatesanarbitraryTuringmachine.
1 Introduction
Transformer-based language models have shown impressive
abilitiesinnaturallanguagegeneration,readingcomprehension,
code-synthesis,instruction-following,commonsensereasoning,
and many other tasks [6, 7, 10, 30, 19, 48]. Despite these
impressiveabilities,transformersstrugglewithlengthgener-
alization, which refers to the ability to generalize to longer
sequences than seen during training [1, 3, 24, 55]. This lim-
itation raises a central question about transformers: are they
capableofactuallylearninganalgorithmordotheysolveal-
gorithmictasksbyresortingtomemorizationorshortcuts[33]?
Recently,severalworkshavereportedpoorlengthgeneraliza-
tionoftransformersonawiderangeofalgorithmictasks[3,14,
16,54]. Inparallel,amyriadofpapers[24,27,45,55,56]have
optimizedthedataformatschoice(seeSection3fordetails)to
improvethelengthgeneralizationoftransformerswhentrained
*Equalseniorcontribution.
Figure 1: Turing Program example
forsimulatingaTuringMachinewith
Preprint.Underreview. scratchpad.
4202
luJ
3
]GL.sc[
1v01330.7042:viXratoperformmulti-digitadditionoftwonumbers. Whiletherecentprogressisimpressive—Zhouetal.
[56]achievealmostperfectaccuracyonadditionwith100-digitoperandswhiletrainedon40-digit,
allthese“tricks”arespecifictothecaseofadditionandmaynotgeneralizetoothertasks. Incontrast,
our goal is to develop a technique that is general enough to enable length generalization on any
algorithmictask.
Toachievethis,weintroduceTuringPrograms,
Problem Generalization Accuracy
a novel scratchpad technique that may be ap-
plied to general algorithmic tasks. This tech- Addition(n+n) 50→100(2×) 98%
niqueismotivatedbytheoperationsofaTuring Multiplication(n×1) 50→100(2×) 97%
Machine,amathematicalmodelofcomputation Multiplication(n×3) 50→100(2×) 97%
thatiscapableofimplementinganycomputable SGD(nexamples) 50→80(1.6×) 95%
algorithm.ATuringmachineconsistsofa“tape”
Table1: Lengthgeneralizationresultsonvariousprob-
with symbols and a “head” that, at each step,
lemswithTuringPrograms. Weusex → ytodenote
movesleftorrightonthetape,andcanreadand
trainingonn=xandgeneralizington=y.
writesymbolsinasingletapecell. Therefore,
whenaTuringMachineprocessesaninput,thetapeateachstepisacopyofthepreviousoneuptoa
fewchanges. OurTuringProgramsfollowthisphilosophybydecomposinganalgorithmictaskintoa
seriesofsteps. Ateachstepweupdatea“tape”bycopyingtheprevioustapewithafewelementary
changes. WereferthereadertoFigure1forthecorrespondencebetweenTuringMachinesandTuring
ProgramsandtoFigures2and4,forexamplesofTuringPrograms.
UsingtheTuringProgramstechnique,weshowthattransformersenhancedwiththeHard-ALiBi
positionalencoding[23]—arecentencodingthatachievesstate-of-the-artlengthgeneralizationon
copying—arecapableoflengthgeneralizationonawiderangeofalgorithmictasks. Ourmethod
achievesnon-triviallengthgeneralizationonaddition,multiplicationandsimulationofSGDsteps
(see Table 1). Additionally, we show that transformers can be trained to execute random Turing
machines, extrapolating from 50 to over 100 input tokens, suggesting that our method can work
for general algorithmic tasks. To our knowledge, these are the first results showing non-trivial
length generalization on multiplication, and the first attempt to study length generalization on
complexalgorithmslikeSGD.Wehopethatthisrecipewillbefurtherusedtounlocknovellength
generalizationonotheralgorithmictasks.
Ourkeycontributionsaresummarizedasfollows:
– InSection3,wepresentlengthgeneralizationresultsonmulti-digitadditionusingaTuringProgram
andHard-ALiBipositionalencoding.
– InSection4,wepresenttheTuringProgramframeworkinfullgeneralityanditsconnectionsto
Turingmachines. Additionally,wetheoreticallyprovethattransformerscanimplementTuring
Programs,constructingaRASPprogram[53]simulatingTuringmachines.
– InSection5,wedemonstratethatTuringProgramsaregeneralandleadtonovellengthgeneraliza-
tionresultsinunexploredalgorithmictasks: multiplicationby1or3-digitoperand,SGDforlinear
regressionandTuringMachinesimulation.
Relatedwork
Lengthgeneralizationremainsanimportantchallengeforlargelanguagemodelsasunderlinedin
severalworks[14,16,22,43,54]. Despitetheiradvancedreasoningcapabilities,Transformer-based
large language models struggle to process longer sequences than they were trained on [3]. The
mainapproachesforimprovinglengthgeneralizationfocusonchangingthepositionalencodingand
optimizingthedataformat.
Positional encodings for length generalization. Shaw et al. [44] were early to notice that the
weaklengthgeneralizationofTransformerswasduetothechoiceofabsolutepositionalencoding.
Followingthis,manyalternativeswereproposedtoreplacetheabsolutepositionalencoding: relative
positional encodings, which focus on the relative distances between tokens [12]; and weighted
attentionmechanismsinplaceofpositionembeddings[9,24,31,40,41]. Thesealternativesshowed
substantialimprovementsinlengthgeneralizationonnaturallanguageprocessingtasks. Ontheother
hand,Kazemnejadetal. [27]foundthatacausallanguagemodelwithnopositionalencodingcan
lengthgeneralizebetterthansomeofthesespecializedpositionalencodingsonalgorithmictasks. In
2thiswork,weapplytheHard-ALiBipositionalencoding[23],thatachievedstate-of-the-artlength
generalizationonthespecifictaskofcopying,tomoregeneralalgorithmictasks.
Dataformattingforlengthgeneralization. Awiderangeofdataformattingmethodshavebeen
introducedtoachievelengthextrapolationinalgorithmictasks. ScratchpadandChain-of-Thought
formatswereproposedtolearnarithmeticeitherthroughfinetuningorin-contextlearning[3,55].
Whentrainingfromscratch,someotherproposedtechniquestoimprovelengthgeneralizationon
additioninclude: reversedformattingandrandomspaceaugmentation[45],addingpaddingtothe
sequence[24],andsettingindexhintsinfrontofeachdigit[55]. Closertoourwork,severalworks
[3,16,21,27,28]reportthattrainingorfinetuningamodelonscratchpaddatadoesnotyieldany
significantlengthgeneralizationimprovement.Inourwork,wedemonstratethatlengthgeneralization
ispossibleusingacombinationofaparticularscratchpadvariantandafavorablepositionalencoding.
Additionally,wedevelopTuringPrograms,anovelscratchpadstrategythatisgeneralandmaybe
appliedtoachievelengthgeneralizationonanyalgorithmictask.
NeuralnetworksandTuringMachines. Manypriorworksdesignedarchitecturesinspiredby
Turing Machines [13, 18, 26]. From a theoretical perspective, some works proved the Turing
completenessofRNNs[8,46],transformers[4,11,39,51,36]andevenlinearnext-tokenpredictors
[35]underawiderangeofassumptions. Lastly,anotherlineofworkcharacterizesthecomputational
modelthatTransformersexpress: Weissetal.[53]introduceRASP,ahuman-readableprogramming
languagethatcanbeimplementedbytransformers,Lindneretal.[32]showhowhuman-readable
programsarecompiledintotransformermodelsandotherworks[17,25]studyhowtransformerscan
emulatecomputerprograms. Closertoourwork,Zhouetal.[56]hypothesizethatTransformerscan
lengthgeneralizationonanyalgorithmictaskthatmaywrittenasa“simple”RASPprogram. Inthis
work,weconstructasimpleRASPprogramthatgeneratesTuringProgramstosimulatearbitrary
Turingmachines.
2 Setting
Inthissection,wepresentthelengthgeneralizationproblemandsomeinstanceswhereitappears.
Then,wediscussscratchpadprompting[37],atechniquethatletsthemodelgeneratesolutionsteps
beforeproducingthefinalanswer. Finally,weintroducevariouspositionalencodingmethodsand
discusstheirimplicationsonlengthgeneralization.
2.1 Lengthgeneralization
Manysequencemodelingtaskshaveprobleminstancesofdifferentlengths. Shorterinstancesare
ofteneasiertostate,processandhandle,andrequirelesscomputetofindtheanswer. Bycontrast,
longerinstancesaremorechallengingtoparseandrequiremorecomputetosolve. Reasoningtasks
suchasmulti-hopreasoning,programexecution,deductivereasoning,andtheoremprovingfitinthis
category.
Algorithmicreasoningtasksconsistofinputsthataresequencesoftokensdescribingthetask(e.g.
addition, multiplication) and outputs that are the corresponding solutions. We assume that the
language model is allowed to generate (many) intermediate tokens before outputting the answer.
Thenformally,thelengthgeneralizationproblemconsistsoftrainingalanguagemodeloninputsof
length≤Landsolvingproblemsoflength>Lattesttime.
2.2 Scratchpad
IthasbeenshowninpriorworkthattheperformanceofLLMsonalgorithmictaskscanbegreatly
improvedbygeneratingstep-by-stepsolutionsinsteadofimmediatelyoutputtingthefinalanswer
[52]. Amongthemultiplemethodsdescribedintheliterature,wefocusonthescratchpadmethod
[37]. Givenanalgorithmictask,thismethodencodestheintermediatestepsofthealgorithmastext
andtrainsthemodeltoemitthemtoabufferthatisreferredtoasthe“scratchpad”.
Nye et al. [37] showed that scratchpad finetuning can be used to achieve strong in-distribution
performanceonexecutionbasedtaskssuchascodeexecutionandcomputingpolynomials. They
3alsoreportmodestlengthgeneralizationresultsonintegerarithmetic. Thelimitationofscratchpad
trainingforlengthgeneralizationisfurtherhighlightedin[3,16,21,27,28].
Inthispaper,werevisittheuseofscratchpadtrainingtoachievelengthgeneralizationonalgorithmic
tasks. Webeginwiththeobservationthatthescratchpadtechniquecanberealizedasaniterative
sequenceofcopyingoperations,whereateachiterationtheinputisslightlymodified. Buildingon
previousworksshowingthatwiththerightpositionalencoding, transformerscanachievelength
generalizationonthecopyingoperation[23],wehypothesizethatcombiningthescratchpadtechnique
withafavorablepositionalencodingcanunlocklengthgeneralizationcapabilities. Weverifythis
hypothesisinSection3andSection5,butfirstwereviewvariouschoicesofpositionalencoding.
2.3 Positionalencodings
Theinabilityoftransformerstoextrapolatetolongersequenceshasbeenprimarilyattributedtothe
positionalencoding[44,45]. Inthissection,wereviewthedifferentpositionalencodingschemesand
inSection3,wereporttheirlengthgeneralizationperformance. Wereviewherespecificchoicesfor
positionalencodingsthatareknowntoperformwellforlengthgeneralization,anddiscussadditional
encodingschemes(suchasabsoluteandrelativepositionalencodings)inAppendixA.
NoPositionalEncoding(NoPE). Decoder-onlymodelswithcausalattention,asshownbyHaviv
etal.[20],acquirepositionalunderstanding,withoutexplicitpositionalencoding. Kazemnejadetal.
[27]showsthatamodelwithoutpositionalencodingextrapolatebetterthanthosewithspecialized
positionalencodingsonsomealgorithmictasks.
ALiBi. Pressetal.[40]introducesthisadditivepositionalencodingwherethebiasfunctionfollows
b(i,j) = −r|i−j|,wherer > 0issomehyperparameter. Thisschemehasledtostate-of-the-art
lengthgeneralizationonnaturallanguagetasks. However,Jelassietal.[23]noticesthatitstruggles
atlengthgeneralizationonthecopytaskandhypothesizethatitisduetotheslowdecayofr.
Hard-ALiBi. Jelassietal.[23]introduceHard-ALiBi,anadditivepositionalencodingwherethe
biassatisfiesb(i,j)=−∞forj ≤i−mandb(i,j)=0forj >i−m,forsomehyperparameter
m > 0. Intuitively, with this hard thresholding, tokens can only attend to the m closest tokens.
Differentheadsmayhavedifferentvaluesofmandsomeheadsusem=∞whichcorrespondsto
softmaxattentionwithnopositionalembeddingatall(allowingforpropagationofglobalinformation).
TheauthorsdemonstrateempiricallythatmodelsequippedwiththeHard-ALiBipositionalencoding
achieveremarkablelengthgeneralizationonthecopytask. Inthiswork,weusetheHard-ALiBi
positionencodingtoenablelengthgeneralizationonalgorithmictasksasweshowbelow.
3 Lengthgeneralizationonaddition
Inthissection,weaddressthelengthgeneralization
problem for addition. We first review prior results
onthisproblemanddescribethetechniquesusedin
theseworks. WethendemonstratethatTransformers
trainedwithTuringProgramscratchpadsandHard-
ALiBipositionalencodingachievegoodlengthgener-
alizationperformance,extrapolatingfromlength-50
tolength-100addition. Thisisaremarkableimprove-
mentoverpreviouslengthgeneralizationresultsusing
Figure2: TuringProgramforaddition,textin
the“vanilla”scratchpadtechnique(e.g. [37]),which
commentsisnotpartoftheinput.
showedweaklengthgeneralizationperformance. As
mentioned,thereisalonglistofworksthatfocusonlengthgeneralizationonaddition(seeAppendixB
foracompletereview). Notably,Zhouetal. [56]reportsomewhatbetterlengthgeneralizationresults
compared to our results. However, we note that these results rely on particular choices for the
formattingoftheinputandtheoutput,whichare“tailored”forthetaskofmulti-digitaddition.
4100 100
80 80
60 60
40 40
HAlibi + Direct
HAlibi + Turing Program
20 Alibi + Turing Program 20
NoPE + Turing Program
RoPE + Turing Program
0 0
20 40 60 80 100 120 40 50 60 70 80 90 100
Length of Number Length of Number
(a) (b)
Figure 3: (a): Comparisonofdifferentpositionalencodingsanddataformatsforlengthgeneralizationon
addition. WeseesignificantextrapolationtolongersequencelengthswithHard-ALiBiandscratchpad. In
thisfigureandintherestofthepaper,theshadeshowsthe95%confidenceintervals. (b): Hard-ALiBiwith
scratchpad,trainedwith5differentinitializationseeds.Whilethereissignificantvariabilityacrosstrainingruns,
resultsaremorerobustthanpriorwork.
3.1 LengthgeneralizationonadditionwithTuringProgramsandHard-ALiBi
Inthissection,wepresentourTuringProgramscratchpadstrategyforadditionandreportlength
generalizationresults.
3.1.1 Experimentalsetup
Data. We adopt the scratchpad format and write all the steps into one sequence, where steps
are separated by a separator token. Figure 2 shows our scratchpad strategy for getting length
generalizationonaddition1. Ifnotspecifiedotherwise,ourtokenspaceisofsize24andmadeof
V ={0,...,9,+,a,...,j,∧,<|BOS|>,<|EOS|>,<|SEP|>}. Allthedigitsaresampleduniformly
asfollows:wefirstsamplethelengthofeachoperand(between2andL=50)andthenindependently
sampleeachdigit. Finally,we“packthecontext”withi.i.d.sequencesduringtraining,i.e. wefillthe
contextwithmultipleindependentsamplesofthetask(similarlyto[55]). Wesetthetrainingcontext
lengthto500. Attesttime,weevaluateourmodelsusingaslidingwindow: wegeneratetokensuntil
thetrainingcontextlength(500)isfilled,andtheneachadditionaltokenisgeneratedbyfeedingthe
contextofthemostrecent500tokens,effectivelydroppingallpasttokens2. Thisway,weareableto
generateverylongsequencesoftokenswithouttrainingorevaluatingonlongcontextwindows. To
evaluatetheaccuracyatagivenlength,wetestthemodel’soutputon288examples. Wereportthe
accuracyofexactlymatchingthedesiredoutput.
ModelandTraining. Ourbasemodelisa150MparameterTransformerwithL = 12layers,a
D =1024hiddensize,feedforwardlayerwithahiddendimensionof4096andH =16attention
heads. ThebackboneofourmodelisbasedontheGPT-NeoXarchitecture[5]. Wepickacontext
lengthof500tokens. WeusetheAdamWoptimizer[34]totrainthemodelwithaweightdecayvalue
of0.1andnodropout,for200,000steps. Thelearningratescheduleincorporatesaninitial100-step
linearwarm-up,followedbyalineardecay,startingat7e-5.
Hard-ALiBipositionalencoding. Similarlyto[23],weuseM maskedheadsand(H−M)NoPE
heads. Inthemaskedheads,werespectivelysetthehyperparametermto1,2,... andM. Weswept
over{3,4,5,6,7,8}andfoundthatM =6isthebestchoice.
3.1.2 Results
InFigure3aweshowthelengthgeneralizationperformanceoftransformerstrainedtoperformmulti-
digitadditionusingthescratchpaddescribedabove. Wecomparetheperformanceofdifferentchoices
ofpositionalencodings,aswellascomparingtotheperformanceonadditionwithoutscratchpad
(directlyoutputtingtheanswer). WeobservethatbyusingHard-ALiBitogetherwithscratchpad,
transformersareabletogeneralizewellbeyondthelengthofthetrainingexamples. Inparticular,the
Hard-ALiBimodelachievesa98%accuracyatlength100.AsshownbyFigure8intheappendix,the
1Intheexperiments,weuseaslightlymorecompactversionofthescratchpad,whereeachexamplesis
representedas$4324+139|432e+13j(1,3)|43c+1d(0,63)|4d+b(0,463)|e+∧(0,4463)|4463.
2Forefficiencyreasons,oncewereachthecontextlengthweadvancethe“window”by20tokens.
5
)%(
ycaruccA
)%(
ycaruccAmodelalsolengthgeneralizeswellwhentheoperandsareofdifferentlengths. Finally,inFigure3b
weanalyzetherobustnessoflengthgeneralizationperformancetodifferentchoicesofinitialization
seed. Weobservethat, whilethereissignificantvarianceinperformancewhentestingonlonger
sequences,ourmethodismorerobustcomparedtopriorresults(asreportedin[56]).
4 TuringPrograms
InSection3,weshowedthatTransformerswithHard-ALiBitrainedonaspecificchoiceofscratchpad
formatcanlengthgeneralizetosequencesthatare2×longer. Oncloserinspection,eachlineinthe
scratchpadinFigure2isaslightlymodifiedcopyofthepreviousonewhereafewelementarychanges
are applied, e.g. removing one digit for each operand and updating the intermediate result/carry.
SinceHard-ALiBiyieldsrobustlengthgeneralizationoncopying,thismayexplainwhyweachieve
betterextrapolationthanpreviousworksthattrainedtheirmodelswithscratchpad.
Inthissection,wegeneralizethisapproachandclaimthateveryalgorithmictaskcanbewrittenasa
sequenceofmodifiedcopyoperations: i.e. copyoperationswithsmallandlocalizedmodifications.
SuchdecompositionfollowsimmediatelyfromthestandardconstructionofaTuringMachine, a
universalmodelofcomputation. WethereforerefertothisscratchpadstrategyasaTuringProgram.
WestartthissectionintroducingthestandarddefinitionofaTuringMachine,andthenpresentTuring
Programs,ourscratchpadstrategyforachievinglengthgeneralizationonanyalgorithmictask. Lastly,
we present our main theoretical result: Transformers can implement Turing Programs over long
sequencesofinputs.
4.1 Background: TuringMachines
ATuringMachine[49]isacomputationalmodelthatconsistsofaninfinitetape3withcells,ahead
thatcanreadfromacell,writetoacellandmoveleftorrightoverthetape,andasetofruleswhich
directtheheadbasedonthesymbolitreadsandthecurrentstateofthemachine. Moreformally,a
TuringMachineisdefinedasfollows.
Definition4.1 ATuringMachineisspecifiedasaquadrupleT =(Q,Σ,s,δ)where: 1)Qisafinite
setofstates,2)Σisafinitesetofsymbols,3)s∈Qistheinitialstateandf ∈Qisthefinalstate,4)
δisatransitionfunctiondeterminingthenextmove: δ: (Q×Σ)→(Σ×{L,R}×Q).
Atthefirstiteration,themachineissettostates∈Q,theheadisonthefirst(leftmost)cellofthetape,
andtheinputiswrittenonthetapefromlefttoright. Ateachiteration,theheadisonthei-thcellin
thetape,isinstateq ∈Qandreadsthei-thsymbolonthetapeα. Then,ifδ(q,α)=(α′,D,q′),the
headwritesthesymbolα′,movesinthedirectionD ∈{L,R},andthemachinechangesitsstateto
q′. Ifthemachinereachesthestatef,itstops,andits“output”iswrittenonthetape.
Turing Machines are a powerful model for solving algorithmic tasks since (a) the framework is
universal i.e. it is possible to write any algorithmic task in the Turing Machine formalism, (b)
TuringMachinescansolveawiderangeofalgorithmicproblems—rangingfromsimplearithmeticto
determiningwhetheranumberisaprime[2]—inapolynomialnumberofsteps. Inthenextsection,
weshowhowtousetheTuringMachineformalismtoobtainanovelscratchpadstrategythatunlocks
lengthgeneralizationonanyalgorithmictask.
4.2 TuringPrograms: auniversalscratchpadstrategyforlengthgeneralization
TheleftpanelofFigure1representsthesimulationofaTuringMachineandshowshowthestate,the
headandthetapeevolveswithtime. Notethatateachtimestep,thestateofthetapeisacopyofthe
previoustapewithafewelementarychangessuchasamoveofthehead,aneditofasinglesymbol
andachangeofstate.
ThestepsinaTuringMachinesimulationaresimilartoascratchpadstrategywhereeachstringisa
copyofthepreviousonewithafewmodifications. Therefore,weclaimthatforanyalgorithmictask
3Weassumethatthetapeisunboundedfromtherightside,butboundedfromtheleft. Namely,thereare
infinitelymanycellstotherightoftheinput,butnoemptycellstotheleft.Thisiscomputationallyequivalentto
atapethatisinfinitefrombothsides.
6thatcanbesolvedbyaTuring-computablealgorithm,thereisacorrespondingscratchpadstrategy
for solving this problem (as demonstrated in the right panel of Figure 1). We refer to this novel
scratchpadstrategyasTuringPrograms.
TuringProgramsdecomposeanalgorithmictaskintoaseriesofintermediatereasoningsteps. Each
stepisa“tape”thatmaintainsthestateofthemachine,andthenextstepisacopyoftheprevious
tapewithafewelementarychanges,suchastrimmingofdigitsandupdateofcarry/intermediate
resultasinthecaseofadditionandmultiplication(seeFigures2and4)orupdateoftheparametersin
thecaseofSGDonlinearregression(seeSubsection5.2). InSection5,weshowhowtouseTuring
Programstounlocknovellengthgeneralizationresultsonchallengingalgorithmictasks.
4.3 Theory: TuringProgramsinRASP
TofurthermotivatetheuseofTuringProgramstoachievelengthgeneralizationonarbitraryalgo-
rithms,weprovethattransformerscanimplementTuringProgramsoverlongsequencesofinputs. In
particular,weshowthatTuringProgramscanbeimplementedinRASP[53],aprogramminglanguage
thatwassuggestedasanabstractdescriptionoftheoperationsofatransformer. FollowingZhouetal.
[55],weusearestrictedversionofRASPthatdoesnotallowdirectindexoperations,asZhouetal.
[55]hypothesizedthatRASPprogramswithindexarithmeticsmaynotlengthgeneralize4. Therefore,
ourresultshouldbeviewedasalength-generalization-friendlyconstructionofatransformerthatcan
execute(most)TuringPrograms(andhence,cansimulatemostTuringmachines).
Toavoidindexoperations,weleveragethen-gram
hashingmechanismsuggestedbyJelassietal. [24]
asabasisforthecopyingabilityoftransformers. In
their construction, copying a string from the input
wasachievedbystoringasequenceofnpreceding
tokens(n-gram)ateachposition,anditerativelyre-
trieving the next token after the uniquely matched
n-gram. Our Turing Program construction is very
similar,exceptthatinsteadofcopyingastringfrom
theinput, wecopythenextstateoftheTuringma-
chineascomputedfromthepreviousstring.Asinthe
constructionofJelassietal. [24],ourRASPprogram
islimitedtooperatingoninputsthathavenorepeated Figure4: TuringProgramfor3-digitmultipli-
n-grams(i.e.,nosequenceofntokensappearstwice cation. Ateachstep,weupdatethreeinforma-
intheinput),whichcanbeguaranteedwithhighprob- tion:theheadposition,theresultofthe“local”
abilityforuniformlyrandomsequencesoftokensof multiplication,thecarryandtheintermediate
length≤ exp(n). Additionally,werequirethatthe resultofthe“global”multiplication.
Turingmachinedoesnotgeneraterepeatedn-grams
whenprocessingtheinput,andthatalltheoperationsoftheTuringmachineareappliedin-memory5.
Undertheseassumptions,wegetthefollowingresult:
Theorem4.1 LetT beaTuringMachines.t. 1)T doesnotgeneraterepeatedn-gramsand2)T
operatesin-memory. Then,thereexistsaRASPprogramP ofsize(numberoflines)O(n)s.t. for
everyinputxwithoutrepeatedn-grams,P correctlysimulatesT forexp(n)steps.
WegivethefullcodefortheconstructionofsuchRASPprogramsinAppendixD.
5 Lengthgeneralizationonotheralgorithmictasks
Building upon the encouraging length generalization results on addition from Section 3 and the
TuringProgramsframeworkfromSection4,weshowthatTransformersenhancedwithHard-ALiBi
4OurRASPprogramdoesnotfollowalltherestrictionsoftheRASP-Llanguagesuggestedin[55],aswedo
notrestrictthetokenstohaveint8values.
5Namely,weassumethattheheadoftheTuringmachinedoesnotgobeyondtheinputsequence.Webelieve
thatthisrestrictionmayberemovedatthecostofconstructingamorecomplexRASPprogram.Whilethismay
seemlikealimitingrestriction,wenotethatthislimitationcanbeeasilymitigatedbypaddingtheinputwith
randomtokens.
7mayachieverobustlengthgeneralizationoncomplexalgorithmictasks. Weshowthatourframework
achieveslengthgeneralizationonmultiplicationby1-digitand3-digitoperands,onSGDappliedto
linearregression,andfinally,onnext-statepredictionofarandomTuringMachine.
5.1 Multiplicationbyafixed-lengthoperand
Priorwork. Multiplicationisknowntobeachallengingtaskforlengthgeneralizationandvery
fewworksreportpositivelengthgeneralizationresultsonthistask. Onpretrainedmodels, Zhou
etal.[55]showsthatelaboratepromptingtechniquesslightlyimprovethelengthgeneralizationof
Codexon(n≤3)-multiplication. Dzirietal.[16]showthatevenfine-tunedGPT-3struggleswith
performing3-digitmultiplication. Onrandomlyintializednetworks,Leeetal.[29]showthatmodels
canlearnin-distributionthe2-digitmultiplicationinasampleefficientwayusingscratchpad. Shenet
al.[45]showsthatwithpaddingandreversedproductsitispossibletoperfectlylearnin-distribution
12-digit multiplication. Jelassi et al. [24] focuses on 3-digit multiplication and shows that when
trainingon(5×3)-digit-multiplicationandaddingafewexamplesof(35×3)-digit-multiplication,the
modellengthgeneralizesto(35×3)-digit-multiplication. Insummary,priorworkmainlyfocusedon
in-distributionlearningofmultiplicationanddidnotmanagetoobtainlengthgeneralizationresults.
Datasetup. Ourexperimentalsetupissimilartotheone
100
inSection3. Wefocusonmultiplicationbyafixed-length
operand,i.e.(n×k)-digit-multiplicationwherethefirst 80
operand has variable length n and the second operand
60
alwayshasafixedlengthk ∈{1,3}acrossallexamples.
Weadoptthescratchpadformatandwriteallthestepsinto 40
one sequence, where steps are separated by a separator
token. TheTuringProgramformultiplicationisdescribed 20 (n x 1)
(n x 3)
inFigure4.6 Ourtokenspaceissimilartothetokenspace 0
40 60 80 100 120
usedinSection3,usinga∗symbolinsteadof+andusing Length of Number
anadditionalseparatortoken∼. Allthedigitsaresampled Figure5: Lengthgeneralizationonmul-
uniformly as follows: we first sample the length of the tiplicationby1and3digitnumbers.
firstoperand(between2and50)andthenindependently
sampleeachdigit. Theremainingdetailsofthetraining/testprotocolsaresimilartothoseinSection3.
Results. Figure5reportsourlengthgeneralizationresultson(n×1)and(n×3)multiplications
respectively.Weobtainrobustlengthgeneralizationbyafactor×2(from50to100-digitnumbers)on
(n×1)and(n×3)multiplication. Wenotethat,uptolength100,(n×1)and(n×3)multiplication
performroughlythesame((n×1)hasaccuracy97.1%and(n×3)hasaccuracy96.8%),which
demonstrates the generality of our Turing Programs framework. Both results are achieved with
M = 7 masked heads and peak learning rate 0.0003. The head numbers were again chosen by
sweepingovercandidatenumbersasbeforewhilethelearningrateswerechosenfromthecandidate
set{7e-5,e-4,3e-4}.
5.2 SGDonLinearRegression
100
In this section, we train a model to perform SGD and
demonstrateitsabilitytolengthgeneralize. Whileinpre- 80
vious examples we varied the number of digits in the 60
operands,hereweinsteadvarythenumberofexamples.
40
Problem Description. Let D = {(⃗x i,y i)} i=0,...,n−1 20 Direct Answer
with ⃗x ∈ R2 and y ∈ R be a dataset of size n. Turing Program
i i
Given initial weights w⃗ 0 ∈ Rn, we can obtain the final 0 40 50 60 70 80 90
Size of Dataset
weight w⃗ by performing gradient descent: w⃗ =
w⃗ −λ∇n− (1 y −w⃗ ·⃗x )2, where λ is the learnini g+1 rate. Figure6: Lengthgeneralizationonrun-
i wi i i i ning the SGD algorithm, varying the
Forourexperiment,wepickλ=0.5andw⃗ =0.
0 numberofexamples.
6Intheexperiments,weuseaslightlymorecompactversionofthescratchpad,whereeachexamplesisrepre-
sentedas$4324*135|432e*135(0540∼054,0)|43c*135(0270∼032,40)|4d*135(0405∼043,740)|
e*135(0540∼058,3740)|∧*135(0000∼005,83740)|∧*135(0000∼000,583740)|583740.
8
)%(
ycaruccA
)%(
ycaruccATokenizationandData. Wedividetheinterval[−1,1]into200discretetokens{a ,a ,...,a }.
0 1 199
Asaninput,themodelreceivesasequenceofnexamples,eachencodedastwoinputcoordinateand
oneoutput(label)value. ThemodelthenneedstocomputetheiteratesoftheSGDalgorithmwhen
processingthedataexamples,startingfromthelastdatapoint,andoutputtheresultingweightvector
w⃗ . AdetaileddescriptionoftheTuringProgramforsolvingSGDisdetailedinAppendixE.
n−1
Results. Unlikepreviousexperiments,wherewereportaccuracyw.r.t. exactstringmatching,here
weallowthenetworktoerrbytwoquantizationunit,countinganyoutputthatiswithin2/100from
theground-truthoutput(inℓ norm)ascorrect. Inotherwords,wedisregarderrorsthatmayoccurto
∞
differencesinquantizationofthereal-valuediteratesofSGD.AsshownbythebluecurveinFigure
6,trainingthetransformertoperformSGDondatasetofsizesn ≤ 50generalizeswithaccuracy
> 95%todatasetsofsizen = 80. OurHard-ALiBimodelhasM = 7maskedheads, acontext
lengthof600,andwastrainedwithpeaklearningrate7e-5for400,000stepswithabatchsizeof16.
Forcomparison,wealsotrainedamodeltodirectlycomputethefinalanswerasshownbythered
curveinFigure6. Weobservethattrainingthemodeltoimmediatelyoutputtheanswersignificantly
degradesitsperformance.
5.3 Turingsimulations
100
Inthissection,wetestthelengthgeneralizationoftrans- 80 Turing Machine 0
formers trained to predict the next state of an arbitrary, Turing Machine 1
60 Turing Machine 2
randomlygenerated,Turingmachine. Ourexperimental Turing Machine 3
Turing Machine 4
setupissimilartotheoneinSection3exceptforthedata 40 Turing Machine 5
asdetailedbelow. Turing Machine 6
Turing Machine 7
20
Turing Machine 8
Turing Machine 9
Datasetup. WefirstsamplearandomTuringMachine 0
40 60 80 100 120 140
T with5states,15inputsymbolsandarandomtransition Length of Tape
function(i.e.,foreverypairofstateandsymbolweran- Figure7: Lengthgeneralizationperfor-
domlydrawatripletofstate,symbolandmove-direction). manceon10differentrandomlygener-
Duringtraining,eachinputexampleisgeneratedasfol- atedTuringmachines.
lows: we randomly choose an input sequence length L
between2and50,thenrandomlychooseLtokens,arandompositionfortheheadandarandom
stateforthemachine. Ateachstepoftraining,wegenerateinanonlinemannerabatchofsize16of
TuringsimulationsfromT andfocusonlearning1-stepprediction: giventheinputtape,themodel
hastogeneratetheoutputofthetransitionfunctionfollowedbythenextstateofthetape. Attest
time,weevaluatethemodelontapesoflengthL≥50. FurtherdetailsareinAppendixF.
Results. Figure7showsthattransformersenhancedwithHard-ALiBipredictalmostperfectlythe
1-stepTuringMachinetransitionoftapesthatare2×to3×longerthanthoseseenduringtraining.
Trained with a peak learning rate of 7e-5, the models have M = 8 masked heads and a context
lengthof450. ThisexperimentsuggeststhattransformersmaylengthgeneralizeonarbitraryTuring
Programs7. However, this admittedly does not imply that transformers can successfully execute
TuringProgramsformultiplesteps,asaccumulatingerrorsmightcausetheprogramstofail. That
said,wenotethatinmanycaseswegetlengthgeneralizationwithvirtuallyzeroerror,suggesting
thatmultiplestepsofthemachinecanbeexecutewhilestillmaintainingaccurateperformance.
6 DiscussionandLimitations
Studyingandimprovingthelengthgeneralizationabilitiesoftransformersonalgorithmictaskshas
beenthefocusofvariousrecentworks. Inparallel,ithasbeenestablishedexperimentallythatthe
abilityoflanguagemodelstosolvealgorithmictasksisgreatlyenhancedwhenallowingthemtouse
scratchpad/CoTdata. Additionally,recenttheoreticalworksdemonstratethattransformerscanuse
CoTtosimulatearbitraryalgorithms[36],establishingthattheyarecomputationally“universal”.
Theseresultsmotivateustostudywhethertransformersareuniversallearners,abletolearnfrom
examples to execute arbitrary algorithms. Since algorithms are typically defined over arbitrary
7Wenotethat,formally,theexperimentdemonstratestheabilityoftransformerstolearninthe“average
case”,butdoesnotruleoutthepossibilitythatsome“worstcase”TuringProgramshavemuchmorerestricted
lengthgenerlization.
9
)%(
ycaruccAsequencelengths,weuselengthgeneralizationasameasureofwhetherthemodelhaslearnedthe
truealgorithm. Toestablishthis,weusethekeyobservationthattransformerscanlengthgeneralize
onthecopyingoperation. Sinceexecutinganalgorithmcanbeimplementedasasequenceof“smart”
copyoperations,thecopyingabilityoftransformerscanbeleveragedtoachievenon-triviallength
generalizationperformanceonawiderangeofalgorithmictasks.
That said, we acknowledge that our work still falls short of demonstrating that transformers can
robustlylengthgeneralizeonanyalgorithmictask. Insomeofourresults,theextrapolationtolonger
sequence length is not robust, and degradation in performance may appear shortly after moving
out-of-distribution. Additionally,ourresultsrelyonpotentiallyverylongandcumbersomeCoTdata,
inawaythatisnotnecessarilyusefulforreal-worldapplicationsoflanguagemodels. Thus,weview
ourresultsastheoreticalevidencethatlengthgeneralizationispossible,andleavethedevelopmentof
morepracticalandrobustmethodsforreal-worldlengthgeneralizationtofuturework.
Acknowledgments
ThisworkhasbeenmadepossibleinpartbyagiftfromtheChanZuckerbergInitiativeFoundationto
establishtheKempnerInstitutefortheStudyofNaturalandArtificialIntelligence. SJacknowledges
funding supported by the Center of Mathematical Sciences and Applications. SK acknowledges
supportfromtheOfficeofNavalResearchunderawardN00014-22-1-2377andtheNationalScience
FoundationGrantunderaward#IIS2229881.
References
[1] EmmanuelAbbe,SamyBengio,AryoLotfi,andKevinRizk. Generalizationontheunseen,
logic reasoning and degree curriculum. In International Conference on Machine Learning,
pages31–60.PMLR,2023.
[2] ManindraAgrawal,NeerajKayal,andNitinSaxena. Primesisinp. Annalsofmathematics,
pages781–793,2004.
[3] CemAnil,YuhuaiWu,AndersAndreassen,AitorLewkowycz,VedantMisra,VinayRamasesh,
Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length gen-
eralization in large language models. Advances in Neural Information Processing Systems,
35:38546–38556,2022.
[4] SatwikBhattamishra,ArkilPatel,andNavinGoyal.Onthecomputationalpoweroftransformers
anditsimplicationsinsequencemodeling. arXivpreprintarXiv:2006.09286,2020.
[5] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,
HoraceHe,ConnorLeahy,KyleMcDonell,JasonPhang,etal. Gpt-neox-20b: Anopen-source
autoregressivelanguagemodel. arXivpreprintarXiv:2204.06745,2022.
[6] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[7] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,Jared
Kaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,etal. Evaluatinglarge
languagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
[8] YiningChen,SorchaGilroy,AndreasMaletti,JonathanMay,andKevinKnight. Recurrent
neuralnetworksasweightedlanguagerecognizers. arXivpreprintarXiv:1711.05408,2017.
[9] Ta-ChungChi, Ting-Han Fan, Peter JRamadge, andAlexanderRudnicky. Kerple: Kernel-
izedrelativepositionalembeddingforlengthextrapolation. AdvancesinNeuralInformation
ProcessingSystems,35:8386–8399,2022.
[10] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:
Scalinglanguagemodelingwithpathways. JournalofMachineLearningResearch,24(240):1–
113,2023.
10[11] StephenChungandHavaSiegelmann. Turingcompletenessofbounded-precisionrecurrent
neuralnetworks. Advancesinneuralinformationprocessingsystems,34:28431–28441,2021.
[12] ZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocVLe,andRuslanSalakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint
arXiv:1901.02860,2019.
[13] MostafaDehghani,StephanGouws,OriolVinyals,JakobUszkoreit,andŁukaszKaiser. Uni-
versaltransformers. arXivpreprintarXiv:1807.03819,2018.
[14] GrégoireDelétang,AnianRuoss,JordiGrau-Moya,TimGenewein,LiKevinWenliang,Elliot
Catt,ChrisCundy,MarcusHutter,ShaneLegg,JoelVeness,etal. Neuralnetworksandthe
chomskyhierarchy. arXivpreprintarXiv:2207.02098,2022.
[15] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[16] NouhaDziri,XimingLu,MelanieSclar,XiangLorraineLi,LiweiJiang,BillYuchenLin,Sean
Welleck, PeterWest, ChandraBhagavatula, RonanLeBras, etal. Faithandfate: Limitsof
transformersoncompositionality. AdvancesinNeuralInformationProcessingSystems, 36,
2024.
[17] AngelikiGiannou,ShashankRajput,Jy-yongSohn,KangwookLee,JasonDLee,andDimitris
Papailiopoulos. Loopedtransformersasprogrammablecomputers. InInternationalConference
onMachineLearning,pages11398–11442.PMLR,2023.
[18] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401,2014.
[19] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno,
SivakanthGopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa,OlliSaarikivi,etal.
Textbooksareallyouneed. arXivpreprintarXiv:2306.11644,2023.
[20] AdiHaviv,OriRam,OfirPress,PeterIzsak,andOmerLevy. Transformerlanguagemodels
withoutpositionalencodingsstilllearnpositionalinformation.arXivpreprintarXiv:2203.16634,
2022.
[21] YiHu,XiaojuanTang,HaotongYang,andMuhanZhang. Case-basedorrule-based: Howdo
transformersdothemath? arXivpreprintarXiv:2402.17709,2024.
[22] DieuwkeHupkes,VernaDankers,MathijsMul,andEliaBruni. Compositionalitydecomposed:
Howdoneuralnetworksgeneralise? JournalofArtificialIntelligenceResearch,67:757–795,
2020.
[23] SamyJelassi, DavidBrandfonbrener, ShamMKakade, andEranMalach. Repeatafterme:
Transformersarebetterthanstatespacemodelsatcopying. arXivpreprintarXiv:2402.01032,
2024.
[24] SamyJelassi,Stéphaned’Ascoli,CarlesDomingo-Enrich,YuhuaiWu,YuanzhiLi,andFrançois
Charton. Lengthgeneralizationinarithmetictransformers. arXivpreprintarXiv:2306.15400,
2023.
[25] AnaJojic,ZhenWang,andNebojsaJojic. Gptisbecomingaturingmachine: Herearesome
waystoprogramit. arXivpreprintarXiv:2303.14310,2023.
[26] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint
arXiv:1511.08228,2015.
[27] AmirhosseinKazemnejad,InkitPadhi,KarthikeyanNatesanRamamurthy,PayelDas,andSiva
Reddy. Theimpactofpositionalencodingonlengthgeneralizationintransformers. Advances
inNeuralInformationProcessingSystems,36,2024.
11[28] JackLanchantin,ShubhamToshniwal,JasonWeston,SainbayarSukhbaatar,etal. Learningto
reasonandmemorizewithself-notes. AdvancesinNeuralInformationProcessingSystems,36,
2024.
[29] NayoungLee,KartikSreenivasan,JasonDLee,KangwookLee,andDimitrisPapailiopoulos.
Teachingarithmetictosmalltransformers. arXivpreprintarXiv:2307.03381,2023.
[30] AitorLewkowycz,AndersAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,Vinay
Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving
quantitative reasoning problems with language models. Advances in Neural Information
ProcessingSystems,35:3843–3857,2022.
[31] ShandaLi,ChongYou,GuruGuruganesh,JoshuaAinslie,SantiagoOntanon,ManzilZaheer,
SumitSanghai,YimingYang,SanjivKumar,andSrinadhBhojanapalli.Functionalinterpolation
forrelativepositionsimproveslongcontexttransformers. arXivpreprintarXiv:2310.04418,
2023.
[32] DavidLindner,JánosKramár,SebastianFarquhar,MatthewRahtz,TomMcGrath,andVladimir
Mikulik. Tracr: Compiledtransformersasalaboratoryforinterpretability. AdvancesinNeural
InformationProcessingSystems,36,2024.
[33] BingbinLiu,JordanTAsh,SurbhiGoel,AkshayKrishnamurthy,andCyrilZhang.Transformers
learnshortcutstoautomata. arXivpreprintarXiv:2210.10749,2022.
[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[35] Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv preprint
arXiv:2309.06979,2023.
[36] WilliamMerrillandAshishSabharwal. Theexpresssivepoweroftransformerswithchainof
thought. arXivpreprintarXiv:2310.07923,2023.
[37] MaxwellNye,AndersJohanAndreassen,GuyGur-Ari,HenrykMichalewski,JacobAustin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show
yourwork: Scratchpadsforintermediatecomputationwithlanguagemodels. arXivpreprint
arXiv:2112.00114,2021.
[38] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning
andinductionheads. arXivpreprintarXiv:2209.11895,2022.
[39] Jorge Pérez, Javier Marinkovic´, and PabloBarceló. Onthe turingcompleteness ofmodern
neuralnetworkarchitectures. arXivpreprintarXiv:1901.03429,2019.
[40] OfirPress,NoahASmith,andMikeLewis. Trainshort,testlong: Attentionwithlinearbiases
enablesinputlengthextrapolation. arXivpreprintarXiv:2108.12409,2021.
[41] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. Journalofmachinelearningresearch,21(140):1–67,2020.
[42] AnianRuoss,GrégoireDelétang,TimGenewein,JordiGrau-Moya,RóbertCsordás,Mehdi
Bennani,ShaneLegg,andJoelVeness. Randomizedpositionalencodingsboostlengthgeneral-
izationoftransformers. arXivpreprintarXiv:2305.16843,2023.
[43] AviSchwarzschild,EitanBorgnia,ArjunGupta,FurongHuang,UziVishkin,MicahGoldblum,
andTomGoldstein. Canyoulearnanalgorithm? generalizingfromeasytohardproblemswith
recurrentnetworks. AdvancesinNeuralInformationProcessingSystems,34:6695–6706,2021.
[44] PeterShaw,JakobUszkoreit,andAshishVaswani. Self-attentionwithrelativepositionrepre-
sentations. arXivpreprintarXiv:1803.02155,2018.
[45] RuoqiShen,SébastienBubeck,RonenEldan,YinTatLee,YuanzhiLi,andYiZhang.Positional
descriptionmattersfortransformersarithmetic. arXivpreprintarXiv:2311.14737,2023.
12[46] HavaTSiegelmannandEduardoDSontag. Onthecomputationalpowerofneuralnets. In
ProceedingsofthefifthannualworkshoponComputationallearningtheory,pages440–449,
1992.
[47] JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu. Roformer:
Enhancedtransformerwithrotarypositionembedding. Neurocomputing,568:127063,2024.
[48] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[49] A.M.Turing. Computingmachineryandintelligence. Mind,59(236):433–460,1950.
[50] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[51] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case
studyonapproximatingturingmachineswithtransformers. AdvancesinNeuralInformation
ProcessingSystems,35:12071–12083,2022.
[52] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022.
[53] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International
ConferenceonMachineLearning,pages11080–11090.PMLR,2021.
[54] YiZhang,ArtursBackurs,SébastienBubeck,RonenEldan,SuriyaGunasekar,andTalWagner.
Unveilingtransformerswithlego: asyntheticreasoningtask. arXivpreprintarXiv:2206.04301,
2022.
[55] HattieZhou,ArwenBradley,EtaiLittwin,NoamRazin,OmidSaremi,JoshSusskind,Samy
Bengio,andPreetumNakkiran. Whatalgorithmscantransformerslearn? astudyinlength
generalization. arXivpreprintarXiv:2310.16028,2023.
[56] Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny
Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint
arXiv:2402.09371,2024.
13A AdditionalPositionalEncodingsReview
AbsolutePositionalEncoding(APE). APEconsistsinmaintainingapositionalvectorp foreach
i
positioni. Thisvectoriseitherpredefinedviaasinusoidalfunction[50]orlearned[15]. Then,p is
i
addedtothetokenembeddinge beforebeingprocessedbytheTransformer. Priorworkobserved
i
thatthispositionalencodingdoesnotgeneralizewelltolongersequencesinbothnaturallanguage
[40]andalgorithmictasks[24,27].
AdditiveRelativePositionalEncoding(RPE). Shawetal.[44]werethefirsttointegratepositional
encodingsatthelevelofeachattentionlayer(insteadofdoingitattheinputlevel). Raffeletal.[41]
builtuponthisapproachandaddedscalarbiasestopre-softmaxlogitsasfollows:
A=XW (XW )⊤+B, (1)
Q K
whereX,W ,W aretheinputandqueryandkeyweightmatrices. ThebiasmatrixB ∈Rn×nis
Q K
inducedbysomepositionalencodingfunctionb: N∗2 →R. Forinstance,theT5relativepositional
encoding[41]setb(i,j)=f(i−j),wheref issomefunction. Mostofthesubsequentpositional
encodingssuchasALiBi[40],Kerple[9],RandomizedPositionalEncoding[42]andFire[31]rely
onchangingthepre-softmaxlogitsanddifferintheirdefinitionofb.
RotaryPositionalEncoding(RoPE). RoPE[47]encodespositioninformationinattentionlogits
byapplyingarotationtransformationtothequeryandkeyvectorsbasedontheirrelativepositions.
Despitebeingwidelyused,RoPEexhibitslimitedlengthgeneralization[40,27].
B Priorresultsonmulti-digitaddition
In this section, we summarize the methods proposed by prior work to get length generalization
onadditionalongwiththeircorrespondingperformance. Inwhatfollows, weindicateinredthe
positionalencodingandingreenthedataformatusedintheseworks. Wealsotakeasarunning
exampletheaddition576+361=937.
– Lee et al. [29] from 7 to 7-digit (1.0×). APE + Reversed format. They train their models by
reversingeachoperandas675+163=739. Therefore,thecausalmodelthatprocessesinformation
fromlefttorightcanstartwiththeleastsignificantdigitandproceedtothemostsignificantdigit,
whichcorrespondstothealgorithmforaddition. Theydonotachieveanylengthgeneralization.
–Kazemnejadetal.[27]from8to9-digit(1.125×): NoPE+Reversedformat. Theyshowthata
model without positional encoding trained on reversed additions like 675+163=739 outperforms
thosewithspecializedpositionalencodingslikeT5’srelativepositional[41]orRoPE[47].
–Shenetal.[45]from10to11-digit(1.1×):NoPE+Reversedformat+randomspaceaugmentation.
Theyintroducedrandomspacingbetweendigits,aimingtoalleviatethemodel’srelianceonabsolute
positional information. Combining this with the reversed format, the running example becomes
6 75+16 3=739. TheyshowthatNoPETransformerslengthgeneralizefrom10to11digit-addition.
–Zhouetal.[55]from30to45digits(1.5×): NoPE+IndexHints. Theydefine"indexhints",a
formattingthatconsistsinaddingaletterinfrontofeachdigitintheadditiontoindicatetheirposition.
Forinstance,therunningexamplebecomesa5b7c6+a3b6c1=a9b3c7. Thischangeisappliedduring
trainingandinferenceandenablestransformerstoexecuteindexingviainductionheads[38].
–Zhouetal.[56]from40to100digits(2.5×): Fire[31]+Randomizedpositionalencoding[42]
+Reversedformat+IndexHints. Theyuseacombinationoftwopositionalencodings: Fire[31],
a additive relative positional encoding that has obtained strong length generalization on natural
languagebenchmarksandRandomizedpositionalencoding[42]: atechniquethatsamplesencodings
fromarangeexceedingtest-timelengths. ThegoalistoensurethatTransformerscanadapttolarger
positionalencodingsduringtrainingandnotencounteranyOODencodingattest-time. Withreversed
format and index hints, the data format looks like a6b7c5+a1b6c3=a7b3c9. By using all these
modifications,theyreachstate-of-the-artperformanceonlengthgeneralizationforaddition. However,
thesechoicesseemtobespecifictotheadditioncaseandhardtotransfertootheralgorithmictasks.
14Accuracy (%)
100.00
100 100 100 100 100 99
99.75
100 100 100 100 100 100 99.50
99.25
100 100 100 100 100 100
99.00
100 100 100 100 99 100 98.75
98.50 100 100 100 99 99 98
98.25
99 100 100 99 99 99
98.00
17 32 47 62 77 92
Length of Summand 1
Figure8: Griddisplayingtheaccuracyofourmodelonadditionwhenchangingthelengthofeach
operand. Weobservethatourmodelisabletogeneralizeonoperandswithdifferentlengths.
C Additionalexperimentalresults
Tothebestofourknowledge,[56]achievedlengthgeneralizationmainlyforadditionwhenthetwo
summandshadthesamelength. Ourmethodgeneralizesevenwhenthetwosummandshavedifferent
lengths. For L ,L ∈ {17,32,47,62,77,92}, we sampled 96 addition examples where the first
1 2
summandhaslengthL andthesecondsummandhaslength . Theaccuracyforeachcombinationis
1 2
showninFigure8. Weseethatitgeneralizeswellbeyondthetraineddistribution(L ,L ≤50).
1 2
D RASPTuringPrograms
D.1 RASPPythonDefinitions(from[55])
1 import numpy as np
2
3 def full(x, const):
4 return np.full_like(x, const, dtype=int)
5
6 def indices(x):
7 return np.arange(len(x), dtype=int)
8
9 def tok_map(x, func):
10 return np.array([func(xi) for xi in x]).astype(int)
11
12 def seq_map(x, y, func):
13 return np.array([func(xi, yi) for xi, yi in zip(x, y)]).astype(int
)
14
15 def select(k, q, pred, causal=True):
16 s = len(k)
17 A = np.zeros((s, s), dtype=bool)
18 for qi in range(s):
19 for kj in range(qi+1 if causal else s):
20 A[qi, kj] = pred(k[kj], q[qi])
21 return A
22
15
2
dnammuS
fo
htgneL
71
23
74
26
77
2923 def sel_width(A):
24 return np.dot(A, np.ones(len(A))).astype(int)
25
26 def aggr_mean(A, v, default=0):
27 out = np.dot(A, v)
28 norm = sel_width(A)
29 out = np.divide(out, norm, out=np.full_like(v, default, dtype=
float), where=(norm != 0))
30 return out.astype(int)
31
32 def aggr_max(A, v, default=0):
33 out = np.full_like(v, default)
34 for i, row in enumerate(A):
35 idxs = np.flatnonzero(row)
36 if len(idxs) > 0:
37 out[i] = np.max(v[idxs])
38 return out.astype(int)
39
40 def aggr_min(A, v, default=0):
41 return -aggr_max(A, -v, -default)
42
43 def aggr(A, v, default=0, reduction=’mean’):
44 if reduction == ’mean’:
45 return aggr_mean(A, v, default)
46 elif reduction == ’max’:
47 return aggr_max(A, v, default)
48 elif reduction == ’min’:
49 return aggr_min(A, v, default)
50
51 def kqv(k, q, v, pred, default=0, reduction=’mean’):
52 return aggr(select(k, q, pred), v, default=default, reduction=
reduction)
D.2 AdditionalFunctions(from[55])
1 import operator as op
2 import numpy as np
3
4 # Define comparison operators
5 equals, leq, lt, geq, gt = op.eq, op.le, op.lt, op.ge, op.gt
6
7 def shift_right(x, n, default=0):
8 # shifts sequence x to the right by n positions
9 return kqv(indices(x) + n, indices(x), x, equals, default=default)
10
11 def cumsum(bool_array):
12 # returns number of previous True elements in bool_array
13 return sel_width(select(bool_array, bool_array, lambda k, q: k))
14
15 def where(condition, x_if, y_else):
16 # equivalent to np.where(condition, x_if, y_else)
17 x_masked = seq_map(x_if, condition, lambda x, m: x if m else 0)
18 y_masked = seq_map(y_else, condition, lambda y, m: y if not m else
0)
19 return seq_map(x_masked, y_masked, lambda x, y: x if y == 0 else y
)
20
21 def mask(x, bool_mask, mask_val=0):
22 # equivalent to x*bool_mask + default*(~bool_mask)
23 return where(bool_mask, x, full(x, mask_val))
24
25 def maximum(x):
26 return kqv(x, x, x, lambda k, q: True, reduction=’max’)
27
1628 def minimum(x):
29 return -maximum(-x)
30
31 def argmax(x):
32 mm = maximum(x)
33 return kqv(mm, x, indices(x), reduction=’max’)
34
35 def argmin(x):
36 return argmax(-x)
37
38 def num_prev(x, queries):
39 # output[i] = number of previous elements of x equal to queries[i
], inclusive
40 return sel_width(select(x, queries, equals))
41
42 def has_seen(x, queries):
43 return kqv(x, queries, full(x, 1), equals, default=0)
44
45 def firsts(x, queries, default=-1):
46 # find the index of the first occurrence of each query[i] in x
47 # out[i] := np.flatnonzero(x[:i+1] == queries[i]).min()
48 return kqv(x, queries, indices(x), equals, default=default,
reduction=’min’)
49
50 def lasts(x, queries, default=-1):
51 # find the index of the last occurrence of each query[i] in x
52 # out[i] := np.flatnonzero(x[:i+1] == queries[i]).max()
53 return kqv(x, queries, indices(x), equals, default=default,
reduction=’max’)
54
55 def index_select(x, idx, default=0):
56 # indexes into sequence x, via index sequence idx
57 # i.e., return x[idx] if idx[i] <= i else default
58 return kqv(indices(x), idx, x, equals, default=default)
59
60 def first_true(x, default=-1):
61 # returns the index of the first true value in x
62 seen_true = kqv(x, full(x, 1), full(x, 1), equals, default=0)
63 first_occ = seq_map(seen_true, shift_right(seen_true, 1), lambda
curr, prev: curr and not prev)
64 return kqv(first_occ, full(x, 1), indices(x), equals, default=
default)
65
66 def induct_kqv(k, q, v, offset, default=0, null_val=-999):
67 # get value of v at index of: first occurrence of q[i] found in k
(if found) + offset.
68 # (excludes the last OFFSET tokens of k from matching)
69 # null_val is a special token that cannot appear in k or q; used
to prevent accidental matches
70 indices_to_copy = firsts(shift_right(k, offset, default=null_val),
q, default=null_val)
71 copied_values = index_select(v, indices_to_copy, default=default)
72 return copied_values
73
74 def induct(k, q, offset, default=0, null_val=-999):
75 return induct_kqv(k, q, k, offset=offset, default=default,
null_val=null_val)
76
77 def induct_prev(k, q, offset, default=0, null_val=-999):
78 # A version of induct for negative offsets.
79 indices_to_copy = firsts(k, q, default=null_val) + offset
80 copied_values = index_select(k, indices_to_copy, default=default)
81 return copied_values
17D.3 UtilityFunctions
1 def prefix_fill(x, n, value):
2 ones = full(x, 1)
3 no_fill = shift_right(ones, n)
4 return where(no_fill, x, full(x, value))
5
6 def where3(cond, x, y, z):
7 out = where(cond == 0, x, y)
8 return where(cond == 2, z, out)
D.4 TuringMachineTransitionFunction
1 sep = 0
2 bos = 1
3 eos = 2
4 empt = 3
5 alphabet = list(range(4, 16))
6 state_space = list(range(16, 32))
7
8 state_transition = {a: {s: np.random.choice(state_space) for s in
state_space} for a in alphabet + [bos, eos]}
9 symbol_transition = {a: {s: np.random.choice(alphabet) for s in
state_space} for a in alphabet}
10 move_direction = {a: {s: np.random.choice([0, 1]) for s in state_space
} for a in alphabet}
11
12 def next_state(state, token):
13 if token in state_transition.keys() and state in state_space:
14 return state_transition[token][state]
15 else:
16 return 0
17
18 def next_symbol(state, token):
19 if token in alphabet and state in state_space:
20 return symbol_transition[token][state]
21 elif token == bos:
22 return bos
23 elif token == eos:
24 return eos
25 else:
26 return 0
27
28 def move(state, token):
29 if token in alphabet and state in state_space:
30 return move_direction[token][state]
31 elif token == bos:
32 return 1
33 else:
34 return 0
D.5 ComputationofNextTapeState
1 def get_next(x, x_left, x_right):
2 # compute the next state of head and new symbol, without moving
the head
3 x_state = seq_map(x, x_left, next_state)
4 x_symbol = seq_map(x_right, x, next_symbol)
5 x_move_R = seq_map(x, x_left, move)
6 is_head = tok_map(x, lambda z: z in state_space)
7 is_head_right = tok_map(x_right, lambda z: z in state_space)
8 x_next = where(is_head, x_state, x)
9 x_next = where(is_head_right, x_symbol, x_next)
1810 x_move_R = x_move_R & is_head
11 return is_head, x_next, x_move_R
12
13
14 def select_move_token(no_head_around, head_left_move_left,
head_left_move_right, head_right_move_left, head_right_move_right,
is_head_move_left, is_head_move_right):
15 LEFT_TOKEN = full(no_head_around, 0)
16 CUR_TOKEN = full(no_head_around, 1)
17 RIGHT_TOKEN = full(no_head_around, 2)
18 out = CUR_TOKEN
19 out = where(head_left_move_right | is_head_move_left, LEFT_TOKEN,
out)
20 out = where(head_right_move_left | is_head_move_right, RIGHT_TOKEN
, out)
21
22 return out
23
24
25 def move_head(cur_state, right_state):
26 is_head, cur_next, move_R = cur_state
27 right_is_head, right_next, right_move_R = right_state
28 left_is_head, left_next, left_move_R = shift_right(is_head, 1),
shift_right(cur_next, 1), shift_right(move_R, 1)
29
30 no_head_around = (~left_is_head & ~right_is_head & ~is_head)
31 head_left_move_left = left_is_head & ~left_move_R
32 head_left_move_right = left_is_head & left_move_R
33 head_right_move_left = right_is_head & ~right_move_R
34 head_right_move_right = right_is_head & right_move_R
35 is_head_move_left = is_head & ~move_R
36 is_head_move_right = is_head & move_R
37
38 x_sel_move = select_move_token(no_head_around, head_left_move_left
, head_left_move_right, head_right_move_left,
head_right_move_right, is_head_move_left, is_head_move_right)
39 return where3(x_sel_move, left_next, cur_next, right_next)
40
41
42 def next_tape(x, shift):
43 # compute the state of the head, after shifting by some n >= 2
44 x_ = shift_right(x, shift)
45 x_left = shift_right(x, shift+1)
46 x_right = shift_right(x, shift-1)
47 x_right_right = shift_right(x, shift-2)
48
49 # compute the next state (before moving the head) for current tape
and right tape
50 cur_state = get_next(x_, x_left, x_right)
51 right_state = get_next(x_right, x_, x_right_right)
52
53 x_next = move_head(cur_state, right_state)
54
55 return x_next
D.6 HashingFunctions
1 MAX_INT = 32
2 def hash_n_gram(x, n):
3 out = x
4 before_last_sep = tok_map(x, lambda z: z == 0)
5 shifted = shift_right(x, 1)
6 for i in range(n):
7 shifted_is_sep = tok_map(shifted, lambda z: z == 0)
198 before_last_sep = shifted_is_sep | before_last_sep
9 to_add = seq_map(shifted, before_last_sep, lambda a, b: a*(1-b
))
10 # add to hash
11 out = seq_map(out, to_add, lambda a, b: b + MAX_INT * a)
12 shifted = shift_right(shifted, 1)
13 return out
14
15
16 def hash_n_gram_iter(x, n):
17 is_sep = tok_map(x, lambda z: z == 0)
18 sep_cs = cumsum(is_sep)
19 x_hash = hash_n_gram(x, n)
20 return seq_map(sep_cs, x_hash, lambda a, b: a + (MAX_INT**n)*b)
D.7 Next-TokenPredictionforTuringPrograms
1 def next_token_turing(x):
2 x_next_tape_2 = next_tape(x, 2)
3 x_next_tape_3 = next_tape(x, 3)
4 x_next_tape_3 = prefix_fill(x_next_tape_3, 2, empt)
5 k = hash_n_gram_iter(x_next_tape_3, 1)
6 q = hash_n_gram_iter(x, 1)
7 v = x_next_tape_2
8 out = kqv(k, q, v, equals, reduction=’max’)
9 return out[-1]
E SGDTuringProgramDescription
WebrieflydescribeheretheTuringProgramweusedinSubsection5.2. Beyondthenumericaltokens
“a0,a1,a2,... a199",weincludetokens“$,d,yp,g,cur,|"toaidthecalculation. AtypicalCoTfora
gradientdescentthenlookslikethefollowing:
$da179a166,a76da80a145,a102da77a139,a103|
da179a166,a76da80a145,a102da77a139,a103ypa100ga101a99cura99a101|
da179a166,a76da80a145,a102ypa101ga100a99cura99a102|
da179a166,a76ypa100ga120a117cura79a85|
Intheaboveexample,thefirstlineprovidesadatasetofsizethreewhere“da179a166,a76"denotes
the first example (“a179"and “a166" are the coordinates of⃗x, “a76" is the value of y, and “d" is
atokenthatdenotesthestartofanexample). Fromthesecondlineonward,weperformgradient
descentstartingfromthelastdatapoint,workingbackward: Onthesecondline,theoriginaldataset
iscopied,whilethe“a100"following“yp"isthepredictedvalueofy giventheinitialweightand
thelastfeaturevector“a77a139",the“ga101a99"saysthatλ∇ ||y −w⃗ ·⃗x ||hasvalue“a101
wi i i i
a99",and“cura99a101"meansthatthecurrentweightafterupdateis“a99a101". Afteraexample’s
gradientiscalculated,wedeletethatexample.
F TuringProgramsforSimulatingTuringMachines
We use the tokens space a ,a ,...,b ,b ,...,s ,s ,L,R|,(,),∼
1 2 1 2 1 2
,<|BOS|>,<|EOS|>,<|SEP|>}, where the a ’s are input symbols, the b ’s are symbols
j j
substitutingthea ’swhentheheadispointingtothemand(,),|,∼,L,Raresymbolsusedtoencode
j
thetransitions. Forinstance, thetransition(s ,a ,L)meansthattheTuringmachinesmovesto
1 6
states ,editsthetapebywritinga andmovestheheadtotheleft.
1 6
20