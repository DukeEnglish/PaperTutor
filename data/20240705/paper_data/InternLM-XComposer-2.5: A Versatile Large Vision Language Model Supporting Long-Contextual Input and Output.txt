InternLM-XComposer-2.5: A Versatile Large Vision Language Model
Supporting Long-Contextual Input and Output
PanZhang∗1,XiaoyiDong∗1,2,YuhangZang∗1,YuhangCao1,RuiQian1,2,LinChen1,QipengGuo1,
HaodongDuan1,BinWang1,LinkeOuyang1,SongyangZhang1,WenweiZhang1,YiningLi1,
YangGao1,PengSun1,XinyueZhang1,WeiLi1,JingwenLi1,WenhaiWang1,2,HangYan1,
ConghuiHe3,XingchengZhang3,KaiChen1,JifengDai4,1,YuQiao1,DahuaLin1,2,JiaqiWang1,(cid:66)
1ShanghaiArtificialIntelligenceLaboratory,2TheChineseUniversityofHongKong,
3SenseTimeGroup,4TsinghuaUniversity
internlm@pjlab.org.cn
Abstract
We present InternLM-XComposer-2.5 (IXC-2.5), a ver-
satile large-vision language model that supports long-
contextual input and output. IXC-2.5 excels in various
text-image comprehension and composition applications,
achievingGPT-4Vlevelcapabilitieswithmerely7BLLM
backend.Trainedwith24Kinterleavedimage-textcontexts,
itcanseamlesslyextendto96KlongcontextsviaRoPEex-
trapolation. This long-context capability allows IXC-2.5
to excel in tasks requiring extensive input and output con-
texts. Compared to its previous 2.0 version, InternLM-
XComposer-2.5 features three major upgrades in vision-
language comprehension: (1) Ultra-High Resolution Un-
derstanding, (2) Fine-Grained Video Understanding, and
(3) Multi-Turn Multi-Image Dialogue. In addition to com-
prehension,IXC-2.5extendstotwocompellingapplications
using extra LoRA parameters for text-image composition:
(1) Crafting Webpages and (2) Composing High-Quality
Text-Image Articles. IXC-2.5 has been evaluated on 28 Figure1. OverviewofInternLM-XComposer-2.5(IXC-2.5)per-
benchmarks, outperforming existing open-source state-of- formance on benchmarks in different domains, including Video
the-art models on 16 benchmarks. It also surpasses or Benchmarks, Structural High-resolution Benchmarks, Gen-
eralVisualQABenchmarks,Multi-TrueMulti-ImageBench-
competes closely with GPT-4V and Gemini Pro on 16 key
mark, and Webpage Crafting Benchmark. IXC-2.5 based on
tasks. The InternLM-XComposer-2.5 is publicly available
InternLM2-7B[143]matchesorevensurpassesGPT-4V[112]
at https://github.com/InternLM/InternLM-
andGeminiPro[142]in15benchmarks. PleaserefertoTa-
XComposer.
ble3,4,5fordetails.
1.Introduction
paradigms like GPT-4 [112], Gemini Pro 1.5 [41], and
Claude 3 [3] have achieved considerable success and
Recent advancements in Large Language Models
significantlyexpandedtherangeofapplicationsforLLMs.
(LLMs) [29, 55, 111, 121, 146, 147] have sparked in-
Open-sourceLVLMsarealsobeingrapidlydevelopedand
terest in the development of Large Vision Language
cancompetewithproprietaryAPIsinseveralbenchmarks.
Models (LVLMs) [31, 41, 84, 112, 173, 183]. Leading
However,theseopen-sourcemodelsstilllagbehindclosed-
*equalcontribution.(cid:66)correspondingauthor. source leading paradigms in versatility. They lack the
1
4202
luJ
3
]VC.sc[
1v02330.7042:viXraability to perform diverse vision-language comprehension 16 out of 28 benchmarks based on InternLM2-7B [143]
and composition tasks, largely due to limited diversity in backend. As shown in Figure 1, the performance of IXC-
training corpus and challenges in managing long-context 2.5matchesorevensurpassesproprietaryAPIs,e.g.,GPT-
inputandoutput. 4V[112]andGeminiPro [41],in16benchmarks.
TofurtherbridgethegapbetweenproprietaryAPIs[41, IXC-2.5 web demo now supports audio input and
112]andopen-sourcedLargeVisionLanguageModels,we output using open-source tools [123, 179]. You may
areintroducingInternLM-XComposer-2.5(IXC-2.5),aver- try it at https://huggingface.co/spaces/
satile LVLM supporting long-contextual input and output Willow123/InternLM-XComposer.
with diverse comprehension and composition capacities.
2.RelatedWorks
IXC-2.5 excels in existing open-sourced LVLMs with two
advantages. (1)Versatility: IXC-2.5supportsawiderange
LVLMs for Text-Image Conversation. Large Language
of tasks related to comprehension and composition, such
Models(LLMs)[8,12,13,29,55,60,111,115,121,143,
as free-form text-image conversation, OCR, video under-
146, 147, 168] have received considerable attention be-
standing, article composition with illustrations, and web-
cause of their impressive performance in language com-
page crafting. (2) Long-context capabilities in both in-
prehension and generation. Large vision-language mod-
putandoutput: Itisnativelytrainedwith24Kinterleaved
els (LVLMs) [5, 9, 22–24, 31, 34, 35, 41, 68, 78, 112,
image-textdata,whosecontextwindowcanbeextendedto
118, 159, 173, 183] have been developed by integrating
96K through positional encoding extrapolation [94], em-
LLMs with vision encoders [6, 14, 17, 25, 26, 33, 79, 91,
powering the long-term human-AI interaction and content
93, 113, 122, 138, 150, 167, 169, 170, 176] to extend the
creation.
ability to understand vision content, enabling the applica-
Benefitingfromthelongcontextualcapability,compared
tionoftext-imageconversation. MostexistingLVLMsare
toitsprevious2.0version[33],IXC-2.5hasupgradedthree
trained for single-image multi-round conversations, while
comprehension abilities: (1) Ultra-High Resolution Un-
some works [2, 6, 56, 78, 136, 178] have the ability to
derstanding:IXC-2.5enhancesthedynamicresolutionso-
understandmulti-imageinputs. However, IXC-2.5focuses
lution proposed in IXC2-4KHD [34] with a native 560 ×
onprovidingafree-formlong-contextualmulti-turnmulti-
560ViTvisionencoder,supportinghigh-resolutionimages
image interaction experience [86, 92, 103], which has not
with any aspect ratio. (2) Fine-Grained Video Under-
beenaddressedyet.
standing: IXC-2.5 treats videos as a ultra-high-resolution
LVLMs for High Resolution Images Analysis. Under-
compositepictureconsistingoftenstohundredsofframes,
standing high-resolution images has significant potential
allowing it to capture fine details through dense sampling
applications such as OCR and document/chart analysis,
and higher resolution for each frame. (3) Multi-Turn
which is attracting increased attention in the LVLMs area.
Multi-ImageDialogue: IXC-2.5supportsfree-formmulti-
In recent works, there are two main strategies to enable
turn multi-image dialogue, allowing it to naturally interact
high-resolution understanding: (1) High-resolution (HR)
withhumansinmulti-roundconversations.
visual encoders [47, 74, 102, 141, 151, 177] directly sup-
Besides comprehension, IXC-2.5 also supports two no- port higher resolution images. (2) Patchification: A high-
table applications by incorporating extra LoRA parame- resolutionimageiscroppedintopatches[34,49,67,76,79,
ters for text-image composition: (1) Crafting Webpages: 79,85,90,157,158,158]. Eachpatchisprocessedwitha
IXC-2.5canbereadilyappliedtocreatewebpagesbycom- low resolution vision encoder, e.g., CLIP [122] and visual
posing source code (HTML, CSS, and JavaScript) follow- embeddings of patches are further concatenated as inputs
ingtext-imageinstructions. (2)ComposingHigh-Quality for LLM backends. IXC-4KHD [34] scales the supported
Text-ImageArticles: ComparedtoIXC-2, IXC-2.5lever- resolution of open-source LVLMs into 4K and beyond for
agesspeciallydesignedChain-of-Thought(CoT)[153]and the first time. IXC-2.5 combines both solutions with a vi-
DirectPreferenceOptimization(DPO)[124]techniquesto sion encoder trained with a resolution of 560x560 and a
significantlyenhancethequalityofitswrittencontent. dynamicresolutionsolutionproposedinIXC2-4KHD[34],
We evaluated the versatility of InternLM-XComposer- resultinginfurtherimprovements.
2.5 (IXC-2.5) across a range of twenty-eight benchmarks, LVLMs for Video Understanding. In addition to image
including five video benchmarks [38, 42, 71, 88, 181], understanding, theLVLMsareahasalsowitnessedemerg-
nine structural high-resolution benchmarks [20, 89, 106– ingeffortsinvideoanalysis[37,39,70,88,110,134,135].
108, 117, 133, 139, 140], twelve general VQA bench- Tohandlecomplexvideoinputs,existingworksusesparse
marks [18, 40, 44, 61, 66, 87, 100, 155, 164, 166], one samplingortemporalpooling[52,77,101,104,162],com-
multi-true multi-image benchmark [92], and one webpage pressed video tokens [57, 69, 73, 120, 154, 172], memory
crafting benchmark [131]. Compared to previous open- banks[45,134,135],andlanguageasabridge[54,59,171]
sourceLVLMs,IXC-2.5achievedstate-of-the-artresultsin for video understanding. Apart from these video-specific
2Chat on 4K-resolution Image
Analyze this image in detail
The infographic is a visual representation of various facts
about Dubai. It begins with a statement about Palm
Jumeirah, highlighting it as the largest artificial island
visible from space. It then provides a historical context,
noting that in 1968, there were only a few cars in Dubai,
contrasting this with the current figure of more than 1.5
million vehicles. The infographic also points out that
Dubai has the world's largest Gold Chain, with 7 of the
top 10 tallest hotels located there.
Additionally, it mentions that the crime rate is near 0%,
and the income tax rate is also 0%, with 20% of the
world's total cranes operating in Dubai.
Furthermore, it states that 17% of the population is
Emirati, and 83% are immigrants. The Dubai Mall is
highlighted as the largest shopping mall in the world, with
1200 stores. The infographic also notes that Dubai has no
standard address system, with no zip codes, area codes, or
postal services. It mentions that the Burj Khalifa is so tall
that its residents on top floors need to wait longer to break
fast during Ramadan.
The infographic also includes information about Dubai's
climate-controlled City, with the Royal Suite at Burj Al
Arab costing $24,000 per night. Lastly, it notes that the
net worth of the four listed billionaires is roughly equal to
The HD image (1200 x 4679) the GDP of Honduras.
Multi-Turn Multi-Image Dialogue
I want to buy a car from the three given cars, analyze their advantages and weaknesses one by one
When considering the purchase of a car from the three options provided, it's essential to evaluate each vehicle based on its advantages
and weaknesses. The first image shows a Mercedes-Benz SUV, which is known for its luxury, comfort, and reliability. Its strengths
include spacious interiors, advanced safety features, and a reputation for durability. However, it may not offer the same performance as
the other two vehicles, especially in terms of speed and handling.
The second image depicts a Bugatti Veyron, an iconic supercar renowned for its power, speed, and engineering prowess. Its advantages
lie in its exceptional performance capabilities, with a top speed exceeding 250 mph and a powerful engine that can deliver over 1,100
horsepower. However, this comes at a significant price, both financially and in terms of maintenance costs. Additionally, while the
Veyron offers unparalleled driving experience, it might be less practical for everyday use due to its limited seating capacity and high
fuel consumption.
The third image shows a BMW i8, a hybrid sports car that combines electric propulsion with a combustion engine. This vehicle's
benefits include lower emissions, efficient energy usage, and impressive acceleration times. It also boasts a stylish design andadvanced
technology features. However, one potential weakness could be the relatively short range compared to fully electric cars, requiring
frequent charging stops if used extensively for long-distance travel.
How about this car
The car in the image is a red Ferrari, which is known for its high performance and luxury. This particular model appears to be amodern
iteration of the Ferrari 812 Superfast or a similar high-performance vehicle from the brand. The advantages of this car include its powerful
engine, likely delivering over 800 horsepower, making it one of the fastest cars on the road. Its aerodynamic design suggestsexcellent
handling and stability at high speeds. Additionally, as a Ferrari, it comes with a reputation for superior craftsmanship and quality.
However, there are also some potential weaknesses to consider. One might argue that the high price tag associated with owningsuch a
vehicle could be a deterrent for many buyers. Maintenance costs can also be quite significant due to the complex nature of Ferrari engines
and the specialized parts required. Furthermore, while the car's performance is unmatched by most standards, it may not offerthe same
level of comfort and practicality as more conventional vehicles, especially when considering long-distance travel or daily commuting.
Figure2.ThecapabilitiesofIXC-2.5:Chaton4K-resolutionImageandMulti-TurnMulti-ImageDialogue.
3Chat on Video
frames
What is the most likely use of this video?
The most likely use of this video is as an advertisement for Starbucks. The sequence of events depicted in the video, such
as the preparation of a coffee drink, the interaction between customers and baristas, and the closing shot of the Starbucks
logo, are characteristic of a promotional piece designed to showcase the brand's products and services. The emphasis on
the coffee-making process, the use of the Starbucks app, and the friendly interactions between customers and staff all serve
to create a positive image of the brand. Additionally, the final shot of the Starbucks logo with the text "IT STARTS WITH
YOU" suggests a call to action for viewers to engage with the brand, further supporting the idea that this video is an
advertisement aimed at attracting new customers and encouraging existing ones to continue patronizing Starbucks.
Resume-to-Homepage
Generate Code and
Render
Resume Homepage Some Crop Regions
Instruction-aware Webpage Generation
Generate a website for Research institutions.
The name is Shanghai AI lab. Top
Navigation Bar is blue. Below left, an
image shows the logo of the lab. In the
right, there is a passage of text below that
describes the mission of the laboratory.
Generate Code and
There are several images to show the
research projects of Shanghai AI lab. Render
Instruction
Figure3.ThecapabilitiesofIXC-2.5:ChatonVideo,Resume-to-HomepageandInstruction-awareWebpageGeneration.
4Screenshot-to-Code
Generate Code and
Render
Article Composing
Generate an article about the Impact and Measures Regarding Global Climate Change. The outline is as follows:
1. Describe the impact of climate change on the natural environment, including rising sea levels and an increase in extreme weatherevents.
2. Analyze the effects of climate change on industries such as agriculture and tourism.
3. Explore international efforts to address climate change, such as the Paris Climate Agreement.
4. Discuss actions that individuals and communities can take at a personal level.
Impact and Measures Regarding Global Climate Change
The most likely use of this video is as an advertisement for Starbucks. The sequence of events depicted in the video, such asthe preparation of a coffee
drink, the interaction between customers and baristas, and the closing shot of the Starbucks logo, are characteristic of a promotional piece designed to
showcase the brand's products and services. The emphasis on the coffee-making process, the use of the Starbucks app, and the friendly interactions between
customers and staff all serve to create a positive image of the brand. Additionally, the final shot of the Starbucks logo with the text "IT STARTS WITH
YOU" suggests a call to action for viewers to engage with the brand, further supporting the idea that this video is an advertisement aimed at attracting new
customers and encouraging existing ones to continue patronizing Starbucks.
Impact of Climate Change on the Environment
One of the most visible impacts of climate change is the rise in sea levels. As temperatures increase,
glaciers and ice caps melt, contributing to a rise in sea level that threatens coastal communities around the
world. This can lead to saltwater intrusion into freshwater supplies, affecting agriculture and drinking
water quality. Additionally, increased precipitation can cause flooding, while droughts can result in crop
failure and food shortages.
Another significant impact of climate change is the alteration of ecosystems. As temperatures rise, plant
and animal species are forced to adapt to new conditions or face extinction. This can lead to a loss of
biodiversity and the collapse of entire ecosystems. The warming of oceans also poses a threat to marine
life, including coral reefs, which serve as vital habitats for many ocean species.
Impact of Climate Change on Industry
Climate change has a profound impact on various industries, particularly those dependent on natural
resources. Agriculture, for example, is highly vulnerable to changes in temperature and precipitation
patterns. Droughts can reduce crop yields, while floods can destroy crops and livestock. This can lead to
food shortages and higher food prices, affecting both producers and consumers.
The tourism industry is another sector significantly impacted by climate change. Extreme weather events, such as hurricanes and typhoons, can disrupt travel
plans and damage infrastructure, leading to a loss of revenue for businesses and jobs. Furthermore, the warming of oceans andthe melting of glaciers can
alter the aesthetic appeal of beach destinations, reducing their attractiveness to tourists.
International Efforts to Address Climate Change
In response to the global challenge posed by climate change, countries have come together to form international agreements aimedat mitigating its impact.
The Paris Climate Agreement, signed by 195 countries in 2015, is a landmark effort to address climate change. The agreement setsa goal of limiting global
warming to well below 2 degrees Celsius above pre-industrial levels and pursuing efforts to limit the temperature increase to 1.5 degrees Celsius.
The Paris Agreement recognizes the importance of reducing greenhouse gas
emissions and increasing resilience to the impacts of climate change.
Countries are required to submit nationally determined contributions
(NDCs), outlining their efforts to reduce emissions and adapt to the impacts
of climate change. While the implementation of these measures is voluntary,
countries are encouraged to increase their ambition over time to meet the
goals of the agreement.
Personal Level Actions
While international efforts are crucial in addressing climate change, individuals and communities can also take actions at a personal level. Some measures
include:
-Reducing energy consumption: This can be achieved by using energy-efficient appliances, turning off lights when not in use, and reducing overall energy
usage.
-Using alternative transportation: Walking, cycling, carpooling, and using public transportation can help reduce greenhouse gasemissions from vehicles.
-Reducing waste: Properly recycling and composting waste can help reduce methane emissions from landfills.
-Supporting renewable energy: Investing in solar panels, wind turbines, and other forms of renewable energy can help reduce reliance on fossil fuels.
-Planting trees: Trees absorb carbon dioxide and produce oxygen, making them a valuable tool in the fight against climate change.
In conclusion, climate change poses a significant threat to the natural environment, industry, and human populations. However, through international efforts
and personal actions, it is possible to mitigate its impact and create a more sustainable future.
Figure4.ThecapabilitiesofIXC-2.5:Screenshot-to-CodeandArticleComposing.
5designs, video analysis can also be formulated to under- Encode
stand a high-resolution composite picture consisting of <IMAGE1> & Merge
<IMAGE 2>
sampled video frames [63, 156, 174]. Benefiting from
<IMAGE 3>
theabilitytocomprehendultra-high-resolutionimagesand
\n
longcontext,IXC-2.5exhibitsstrongperformanceonvari- <IMAGE N> \n Flatten \n
ousvideobenchmarksforLVLMs.
Resize Resize Global \n
Webpage Generation. Pix2Code [10] presents an end- <IMAGE1> View sp
to-end solution for UI-to-code transformation leveraging
CNNs and RNNs. This approach contends with the chal- <IMAGE 2> \n
lengesposedbyintricatevisualencodingandextensivetext
<IMAGE 3>
decodingwhenappliedtoreal-worldUIs. Inthesphereof Local \n
View
recent advancements, works such as Sightseer [64], DC-
Gen [148], and Design2Code [131] have employed large <IMAGE N> \n \n
\n PLoRA
vision-language models trained on synthetic screenshot-
\n \n
HTML paired datasets like WebSight v0.1 or v0.2 [64] to Dynamic Video Dynamic Image
Partition Partition \n Flatten
facilitate HTML code generation. Nevertheless, the syn- \n \n
thesized web page datasets have been critiqued for their
<IMAGE1> \n
simplicity and lack of diversity. These studies generally <IMAGE1> \n \n
<IMAGE1>
concentrate on the screenshot/sketch-to-code task. In con- <<IMIMAAGGEE 21>> \n
trast, our IXC-2.5 model extends these capabilities to in- <IMAGE 2> \n
<IMAGE 2>
clude screenshot-to-code, instruction-aware webpage gen- <<IMIMAAGGEE 2 >3>
<IMAGE 3> \n
eration,andresume-to-homepagetasks. IXC-2.5istrained <IMAGE 3>
using a combination of high-quality synthesized and real- << II MM AA GG EE N3> >
worldwebdata. Furthermore,IXC-2.5isproficientingen- <IMAGE N>
<IMAGE N>
erating JavaScript code, thereby enabling the development <IMAGE N>
LLM
ofinteractivefront-endwebpages. Tokenize
PreferenceAlignment. ReinforcementLearningfromHu- Encode
& Merge
manFeedback(RLHF)[115]andReinforcementLearning
from AI Feedback (RLAIF) [7] have shown great promise
Pease describe the image / video …………
in aligning LLMs across various domains, including im-
provinglogicalreasoningandgeneratinghelpfulandharm-
Figure5. FrameworkofIXC-2.5thatsupportsthemulti-modal
less outputs. The typical approach involves training a re-
inputs,includingtext,single/multipleimages,andvideos.
ward model using human or AI preference data and fine-
tuningtheLLMtomaximizetheexpectedrewardfunction
with optimization algorithms like Proximal Policy Opti- XComposer2-4KHD[34](IXC2andIXC2-4KHDforsim-
mization(PPO)[126]. Alternatively,DirectPreferenceOp- plicity), including a light-weight Vision Encoder Ope-
timization (DPO) [124] and the following works [36, 116] nAI ViT-L/14 [122], Large Language Model InternLM2-
have emerged as leading methods that implicitly repre- 7B[13],andPartialLoRA[33]forefficientalignment. We
sent the reward score and eliminate the need for a sepa- recommend the readers to the IXC2 and IXC2-4KHD pa-
rate reward model. Building on the success of RLHF and persformoredetails.
RLAIFinLLMs,recentstudieshavesuccessfullyextended
RLHF/RLAIFalgorithmsformultimodalLVLMs[72,119, 3.2.Multi-modalInput
163,180,182]toreducehallucination. Inthiswork,wein-
Our IXC-2.5 supports diverse input modalities, including
vestigatetheapplicationofpreferencealignmenttechniques
text, single/multiple images, and videos. As showin in
to the text-image article composition task, with a focus on
Figure 5, a Unified Dynamic Image Partition strategy is
generatinghigh-qualityandstableresponseresults.
adoptedforbothvideosandmultipleimageswithanyreso-
lutionsandaspectratios.
3.Method
ImageProcessing. WemainlyfollowtheDynamicImage
Partition and Global-Local Format design used in IXC2-
3.1.ModelArchitecture
4KHD [34] with a few modifications. For the vision en-
ThemodelarchitectureofInternLM-XComposer-2.5(IXC- coder, we reuse the ViT of 490 × 490 resolution used in
2.5 in the following for simplicity) mainly follows the IXC2 and further increase its resolution to 560×560, so
design of InternLM-XComposer2 [33] and InternLM- thateachsub-imagehas400tokens.
6For the high-resolution strategy, we unify the different Inpractice,wejointlytrainallthecomponentswithabatch
strategiesusedintheIXC-4KHDintoascaledidentitystrat- size of 2048 over 4000 steps. Data from multiple sources
egy. Given a maximum partition number H, the image x aresampledinaweightedmanner, withtheweightsbased
with size [h,w] is resized and padded to the new image xˆ on the number of data from each source. The maximum
with size [p ×560,p ×560]. This process is subject to learningrateissetto5×10−5,andeachcomponenthasits
h w
thefollowingconstraints: ownuniquelearningstrategy.Forthevisionencoder,weset
theLLDRto0.9,whichalignswiththepretrainingstrategy.
p w1×⌈p w1×h/w⌉≤H; (1) For the LLM, we employ a fixed learning rate scale factor
p =⌈w∗s/560⌉ (2) of0.2. ThisslowsdowntheupdateoftheLLM,achieving
w2
a balance between preserving its original capabilities and
p =min(p ,p ) ;p =⌈p ×h/w⌉ (3)
w w1 w2 h w
aligningitwithvisionknowledge.
wheresisthescalefactor,p andp representthenumber
w h
3.5.WebpageGeneration
ofpatchesineachrowandcolumn,respectively.
Formulti-imageinput,weassignanindextoeachimage We enhance the capabilities of the IXC-2.5 to include au-
like <IMAGE i>, i ∈ {1,2,3,...} and format the image tomated webpage generation. Specifically, the IXC-2.5 is
andtextinaninterleavedformat. nowequippedtoautonomouslyconstructwebpages,utiliz-
VideoProcessing. Wesampleframesfromthegivenvideo ingHTML,CSS,andJavaScript,basedoninputintheform
and concatenate them along the short side of the frame, of a visual screenshot, a set of free-form instructions, or
leadingtoahigh-resolutionimage. Theframeindexisalso a resume document. Current open-source general-purpose
writtenintheimagetoprovidethetemporalrelation. large language models frequently demonstrate suboptimal
AudioProcessing.IXC-2.5webdemosupportsaudioinput performanceingeneratingHTMLandCSSrelativetotheir
and output using open-source tools. For audio input, we proficiency in natural language generation. To address
employ Whisper [123] to transcribe audio into text. For this limitation, we propose training the screenshot-to-code
audiooutput,weutilizeMeloTTS[179]toconvertthetext taskusingextensivedatasetsfromWebSightv0.1/v0.2[64],
backintoaudio. and Stack v2 [95]. Subsequently, we fine-tune the model
with a smaller, meticulously crafted dataset consisting of
3.3.Pre-training
instruction-aware webpage generation and personal page
During the pre-training phase, the LLM (InternLM2- generationexamples.
7B[143])isfrozenwhileboththevisionencoderandPartial Screenshot-to-code. In addition to the WebSight [64]
LoRA[33]arefine-tunedtoalignthevisualtokenswiththe datasets, we preprocess the HTML and CSS code from
LLM.Thedatausedforpre-trainingisshowninTable1. the Stack v2 [95] dataset to facilitate screenshot-to-code
In practice, we employ the CLIP ViT-L-14-490 [122] training. Initially, we combine the CSS and HTML code
from IXC2 as the vision encoder and further increase its into a single file. Subsequently, we remove all comments,
resolution to 560 × 560. For the Unified Dynamic Im- JavaScriptcode,andexternallinks. Furthermore,weelim-
age Partition strategy [34], we set the maximum number inateanyCSSstylesthatarenotreferenced bytheHTML
H = 12forthepertaining. ForthePartialLoRA[33], we code. We convert all files into screenshots, subsequently
set a rank of 256 for all the linear layers in the LLM de- discarding those that did not render successfully. The re-
coder block. Our training process involves a batch size of maining screenshots are then processed using the IXC2-
4096andspansacross2epochs. Thelearningratelinearly 4KHD [34] model to assess the quality of the web pages.
increases to 2 × 10−4 within the first 1% of the training Following the exclusion of low-quality web pages, we re-
steps.Followingthis,itdecreasesto0accordingtoacosine tained a final set of three remaining about 250,000 high-
decay strategy. To preserve the original knowledge of the qualitywebpages.
visionencoder,weapplyalayer-wiselearningrate(LLDR) We conduct training on the LoRA model utilizing the
decaystrategy[33],andthedecayfactorissetto0.90. three aforementioned datasets. The LoRA rank is set to
512. Thetrainingprotocolemploysabatchsizeof512and
3.4.SupervisedFine-tuning
is executed over a single epoch. Initially, the learning rate
We fine-tune the model with data listed in Table 2. The is incremented linearly to 1×10−4 within the first 1% of
maximum number H of the Unified Dynamic Image Par- the training iterations. Subsequently, the learning rate de-
tition strategy is 24 to handle extremely large images and creasesto0followingacosinedecayschedule.
videos. Forvideodatasets,theIXC-2.5istrainedwithlarge Instruction-aware Webpage Generation. A pivotal at-
images concatenated by at most 64 frames. The largest tribute of large language models lies in their capability to
training context is set to a 24,000 context window size, adhere to human instructions. To facilitate web page gen-
wheretheMMDU[92]datasetcanachievethislimitation. eration based on freeform instructions, we propose con-
7Task Dataset
GeneralSemanticAlignment ShareGPT4V-PT[17],COCO[21],Nocaps[1],TextCaps[132],LAION[125],SBU[114],CC3M[129]ALLaVA[15]
WorldKnowledgeAlignment ConceptData[173]
VisionCapabilityEnhancement WanJuan[46],Flicker[160],MMC-Inst[82],RCTW-17[130],CTW[165],LSVT[137],ReCTs[175],ArT[28]
Table1.DatasetsusedforPre-Training.Thedataarecollectedfromdiversesourcesforthethreeobjectives.
Task Dataset sonnet is also utilized to generate JavaScript events based
ontheHTMLcode. Intotal,wehaveconstructedadataset
Caption ShareGPT4V[17],COCO[21],Nocaps[1]
comprising2,000samples.
GeneralQA VQAv2[4],GQA[53],OK-VQA[105]
Uponconstructingthedatasetforinstruction-awareweb-
VD[32],RD[16],VSR[81],ALLaVA-QA[15]
pagegenerationandresume-to-homepage,wesubsequently
Multi-TurnQA MMDU[92]
fine-tuned the LoRA model for 10 epochs. All other ex-
ScienceQA AI2D[61],SQA[98],TQA[62],IconQA[97]
perimental settings were maintained consistent with those
ChartQA DVQA[58],ChartQA[106],ChartQA-AUG[106] employedduringthescreenshot-to-codetrainingphase.
MathQA MathQA[161],Geometry3K[96],TabMWP[99],
CLEVR-MATH[80],Super[75] 3.6.ArticleComposing
WorldKnowledgeQA A-OKVQA[127],KVQA[128],ViQuAE[65] Generating high-quality text-image articles (e.g., poetry,
OCRQA TextVQA[133],OCR-VQA[109],ST-VQA[11] novels, short stories, and essays) is a crucial capability for
HD-OCRQA InfoVQA[108],DocVQA[107],TabFact[20], AIassistants,withvariousapplicationsindailylife,includ-
WTQ[117],DeepForm[139],VisualMRC[140] ing education and entertainment. Building upon the IXC-
Video ShareGPT4Video[19],ActivityNet[37] 2.5SFTmodelπ inSection3.4,weenhancecreativewrit-
ingcapabilitiesforgeneratinghigh-qualitytext-imagearti-
Conversation LLaVA-150k[84],LVIS-Instruct4V[149]
ShareGPT-en&zh[27],InternLM-Chat[143] cles. However,collectinghigh-qualitytext-imagearticlesis
arareandexpensiveendeavor. Directfine-tuningonscarce
Table2. DatasetsusedforSupervisedFine-Tuning. Wecollect
instructiondatacanleadtounstableresponsesfromLVLMs
data from diverse sources to empower the model with different
inmostcases. Toovercomethesechallenges,weproposea
capabilities.
scalable pipeline that integrates supervised fine-tuning, re-
wardmodeling,preferencedatacollection,andDPOalign-
structing data through querying closed-source large lan- mentforhigh-qualityandstablearticlegeneration.
guage models. Specifically, we utilize GPT-4 to generate SupervisedFine-tuning. WebeginwiththeSFTmodelπ
diverseinstructionsandconceptsforwebpagecreation,en-
(Section 3.4) and a collection of 5,000 instruction tuning
compassingelementssuchastype,style,andlayout.Subse- datasamplesDfromIXC2[33],focusedonarticlewriting.
quently,theseinstructionsareharnessedtoqueryClaude-3-
Duetothelimitedscaleoftheinstructiondata, weusethe
sonnet[3]fortheactualwebpagegenerationprocess. This
SFTmodeltorewritetheoriginalpromptsusingtheChain-
approach results in 18,000 high-quality, instruction-aware
of-Thought(CoT)technique[152],generatingstep-by-step
samples. Additionally,weemployTailwindCSSinsteadof
prompts to supplement the instruction tuning data as aug-
traditionalCSS,givenitssuccinctnature. menteddataD∗.WeobservethattheSFTmodelismoreef-
Resume-to-homepage. In addition to instruction-aware fectiveingeneratinglong-formresponseswhenusingthese
webpage generation, we introduce a more practical task. augmented prompts. We then train the initial model π on
Specifically,givenaresume,themodelisdesignedtogen- the augmented instruction tuning data via LoRA [51] with
erate a personal homepage. This homepage not only en- therankof256andgetthemodelπ toestablishastarting
ref
capsulates the information present in the resume but also pointofouralignmentpipeline.
presents it with a well-structured and visually appealing PreferenceDataCollection. Weusethefine-tunedmodel
format, improving both content organization and aesthetic π to generate diverse responses for each prompt in the
ref
layout. Togeneratecorrespondingdatasets,weproposean augmentedinstruction tuningdataD∗, usingdifferent ran-
idea-resume-homepage data generation pipeline. Initially, dom seeds. This yields a collection of 80,000 prompt-
weleverageGPT-4toproduceresumeideastailoredfordi- responsepairs.Next,weemploytheGPT-4omodeltolabel
versepersonas,suchasresearchers,students,andengineers. 2,000responseswithchosenorrejecteddecisionsandgive
GPT-4 is tasked with generating these resumes in mark- thereasons, whichserveasourrewardmodelingdata. We
downformatbasedontheprovidedideas. Uponobtaining thentrainarewardmodelπ ,sharingthesamearchitecture
rm
thegeneratedresumes,wethenpromptClaude-3-sonnet[3] ofπ , ontherewardmodelingdata. Therewardmodelis
ref
to create corresponding homepages from these resumes. used to make the chosen or rejected prediction on the re-
To enhance the interactivity of these webpages, Claude-3- maining prompt-response pairs. These selected responses
8MME MMB∗1 Temp∗2 Doc Chart Info Text OCR Deep Visual Tab
MVBench MLVU WTQ
Video Video Compass VQA QA VQA VQA Bench Form MRC Fact
Open-Source VideoChat InternVL LIVA InternVL Qwen-VL InternVL InternVL InternVL InternVL GLM-4v DocOwl DocOwl DocOwl DocOwl
PreviousSOTA 2-7B[71] 1.5-26B[26]34B[78]1.5-26B[26] 7B[6] 1.5-26B[26]1.5-26B[26]1.5-26B[26]1.5-26B[26] 9B[43] 1.5-8B[50]1.5-8B[50]1.5-8B[50]1.5-8B[50]
Performance 60.4 50.4 59.0 42.0 58.4 90.9 83.8 72.5 80.6 77.6 40.6 68.8 246.4 80.2
Closed-sourceAPI
GPT-4V[112] 43.5 49.2 59.9 56.0 — 88.4 78.5 75.1 78.0 51.6 — — — —
Gemini-Pro[142] — — 75.0 49.3 70.6 88.1 74.1 75.2 74.6 68.0 — — — —
IXC-2.5-7B 69.1 58.8 55.8 46.9 67.1 90.9 82.2 69.9 78.2 69.0 53.6 71.2 307.5 85.2
Table3.Comparisonwithclosed-sourceAPIsandpreviousopen-sourceSOTAsonVideoBenchmarksandStructuralHigh-resolution
Benchmarks.Thebestresultsareboldandthesecond-bestresultsareunderlined.∗1Wescalethescorefrom0∼3to0∼100foreasier
understanding.∗2Wereportthedeterminismpart(MCQA,Y/N,CaptionMatch)ofTempCompassastheevaluationusingGPT-3.5isnot
stable.
MMDU MMStar RealWQAMathVista AI2D MMMU MME MMB MMBCN MMB1.1 SEEDI MM-Vet HallB
Open-Source LLaVa1.6 InternVL WeMM WeMM InternVL 360VL InternVL InternVL1.5InternVL1.5InternVL1.5WeMMGLM-4vWeMM
PreviousSOTA 8B[83] 1.5-26B[26] 8B[145] 8B[145] 1.5-26B[26]70B[144]1.5-26B[26]1.5-26B[26]1.5-26B[26]1.5-26B[26]8B[145]14B[43]8B[145]
Performance 42.8 57.1 68.1 54.9 80.6 53.4 2,189.6 82.3 80.7 79.7 75.9 58.0 47.5
Closed-sourceAPI
GPT-4V[112] 66.3 57.1 68.0 47.8 75.5 56.8 1,926.5 81.3 80.2 79.8 69.1 56.8 46.5
Gemini-Pro[142] — 42.6 64.1 45.8 70.2 47.9 1,933.3 73.9 74.3 73.9 70.7 59.2 45.2
IXC-2.5-7B 56.6 59.9 67.8 63.8 81.5 42.9 2,229.0 82.2 80.8 79.4 75.4 51.7 42.4
Table4.Comparisonwithclosed-sourceAPIsandpreviousopen-sourceSOTAsonMulti-TurnMulti-ImageDialogandGeneralVisual
QABenchmarks.Thebestresultsareboldandthesecond-bestresultsareunderlined.
are then used to construct the pair data Dp = {x,y ,y }, 4.1.LVLMBenchmarkresults.
w l
whilex,y andy refertotheprompt,chosenresponseand
w l In Table 3 and Table 4, we compare our IXC-2.5 on a list
rejectedresponse,respectively. Ultimately,weobtainato-
of benchmarks with both closed-source APIs and SOTA
talof30,000preferencedataDpforDPO[124]alignment.
open-source LVLMs (with comparable model size). Here
DPOAlignment. WeusetheDPOalgorithmtoupdatethe
we report video understanding results on MVBench [71],
SFT model π on target policy from the preference data
ref MLVU [181], MME-Video [42], MMBench-Video [38],
Dp:
TempCompass [88]. For Structural High-resolution
L DPO(π θ,π ref)=E x,yw,yl∼Dp understanding, we report results on DocVQA [107],
π (y |x) π (y |x) (4) ChartQA [106], InfographicVQA [108], TextVQA [133],
[−logσ(βlog( θ w )−βlog( θ l ))].
π (y |x) π (y |x) OCRBench [89], DeepForm [139], WikiTableQues-
ref w ref l
tion (WTQ) [117], Visual MRC [140], and TabFact [20].
In practice, we use LoRA with a rank of 256 to get the
For general visual question answering, we report results
DPO model π . We observe that our model tends to pri-
θ on MMStar [18], RealWorldQA[155], MathVista [100],
oritizeminimizingthelikelihoodofdis-preferredresponses
MMMU [166], AI2D [61], MME [40], MMBench
y overmaximizingthelikelihoodofpreferredresponsesy
l w (MMB) [87], MMBench-Chinese (MMBCN) [87],
toavoidgeneratinginappropriateorlow-qualitycontent.
MMBench-v1.1 (MMBv1.1) [87], SEED-Bench Im-
In summary, our scalable pipeline consists of three pri-
age Part (SEEDI)[66], MM-Vet [164], HallusionBench
mary components. First, we address the challenge of lim-
(HallB) [44]. For Multi-True Multi-Image dialogue, we
ited instruction tuning data by re-writing original prompts
evaluateIXC-2.5onMMDU[92]benchmark.Forwebpage
into augmented prompts. Next, we generate diverse re-
crafting,wereportasubtaskscreenshot-to-code[131]since
sponses using different random seeds, enabling the explo-
benchmarksforothersarenotavailableinthecommunity.
rationofvariouscreativepossibilities.Finally,weapplythe
The evaluation is mainly conducted on the OpenCom-
DPOalgorithmtothechosenandrejectedresponsestore-
pass VLMEvalKit [30] for the unified reproduction of the
fine our model’s performance. Through our pipeline, our
results.
modeliscapableofgeneratinghigh-qualityarticles.
Comparison on Video Understanding Benchmarks. As
demonstratedinTable3,IXC-2.5exhibitscompetitiveper-
4.Experiments
formance on fine-grained video understanding tasks, out-
In this section, we validate the benchmark performance of performing open-source models on 4 of the 5 benchmarks
our InternLM-XComposer-2.5 (IXC-2.5) after supervised and being on par with Closed-Source APIs. For exam-
fine-tuning. ple,IXC-2.5reaches69.1ontheMVBench,+8.7%higher
9Block-Match Text Position Color CLIP Average
Closed-sourceAPI
GPT-4V[112] 85.8 97.4 80.5 73.3 86.9 84.8
Gemini-Pro[142] 80.2 94.6 72.3 66.2 83.9 79.4
Open-source
WebSightVLM-8B[64] 55.9 86.6 77.3 79.4 86.5 77.1
CogAgent-Chat-18B[48] 7.1 18.1 13.3 13.0 75.5 25.4
Design2Code-18B[131] 78.5 96.4 74.3 67.0 85.8 80.4
IXC-2.5-7B 81.9 95.6 80.9 80.8 86.5 85.1
Table5.Screenshot-to-code.Comparisonwithclosed-sourceAPIsandopen-sourcemodelsonDesign2Codebenchmark.Thebestresults
areboldandthesecond-bestresultsareunderlined.
than the previous SOTA method VideoChat2-7B and out- potential of IXC-2.5 to excel in bridging the gap between
performsGPT-4Vwith+25.6%.Fortherecentchallenging visualdesignandcodeimplementation.
MMBench-Video,IXC-2.5reachestheSOTAperformance
onopen-sourcemodelsandperformsclosetoGemini-Pro. 5.Conclusion
Comparison on Structural High-resolution Bench-
WehaveintroducedInternLM-XComposer-2.5(IXC-2.5),a
marks. Benefiting from the unified image partition strat-
cutting-edgeLargeVision-LanguageModel(LVLM)boast-
egy, IXC-2.5 could handle diverse kinds of images. Ta-
ing long-contextual input and output capabilities that en-
ble 3 reports its performance on several structural high-
able advanced features such as ultra-high resolution im-
resolution benchmarks. IXC-2.5 with only 7B parameters
ageunderstanding,fine-grainedvideounderstanding,multi-
performsonparwiththecurrentlargeopen-sourceLVLMs
turnmulti-imagedialogue,webpagegeneration,andarticle
and close-source APIs. For example, IXC-2.5 gets 90.9%
composing. Our comprehensive experiments demonstrate
on the DocVQA test set, the same as InternVL-1.5 which
that IXC-2.5 achieves competitive performance, remark-
has nearly 4× parameters. For the highly structured form
ably, with a relatively modest 7B Large Language Model
and table understanding tasks, IXC-2.5 outperforms Do-
(LLM)backend.
cOwl1.5-8Bwith+13.0%,+2.4%,+5.0%onWikiTable-
Our model sets out a promising research direction that
Question,DeepFormandTableFacerespectively.
can extend to a more contextual multi-modal environ-
Comparison on Multi-Image Multi-Turn Benchmarks.
ment, including long-context video understanding (e.g.,
IXC-2.5 is capable of taking multiple images as input and
longmovies)andlong-contextinteractionhistory,tobetter
conductingmulti-roundfree-formdialoguebasedonthem.
assisthumansinreal-worldapplications.
WeevaluateitquantitativelyonthenewlyproposedMMDU
Acknowledgements: We deeply express our gratitude to
benchmark [92]. As shown in Table 4, the IXC-2.5 model
Prof. Chao Zhang from Tsinghua University for sugges-
demonstratessuperiorperformance,outperformingthepre-
tionsaboutaudiomodelsandtools.
vious SOTA open-source model by a significant margin of
13.8%. Thisnotableimprovementhighlightstheeffective-
nessofourapproachinadvancingthecapabilitiesofmulti-
imageandmulti-turnunderstanding.
Comparison on General Visual QA Benchmarks. IXC-
2.5isdesignedasageneralLVLMtohandlediversemulti-
modal tasks. Here we report its performance on general
visualQAbenchmarks. AsshowninTable4, theIXC-2.5
showssuperbperformanceonthesebenchmarksandonpar
with current large open-source LVLMs and closed-source
APIs. Forexample,IXC-2.5gets59.9%onthechallenging
MMStarandoutperformsGPT-4VandGemini-Pro. Onthe
RealWorldQA, IXC-2.5 also performs better than Gemini-
ProandclosetoGPT-4V.
ComparisononScreenshot-to-codeBenchmark. Table5
presentsthecomparisonresultsontheDesign2Code[131]
benchmark that assesses the ability to translate visual de-
signintocodeimplementation.OurIXC-2.5evensurpasses
the GPT-4v on average performance, which highlights the
10References Languagemodelsarefew-shotlearners. AdvancesinNeu-
ral Information Processing Systems (NeurIPS), 33:1877–
[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,
1901,2020. 2
Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,
[13] ZhengCai,MaosongCao,HaojiongChen,KaiChen,Keyu
Stefan Lee, and Peter Anderson. Nocaps: Novel object
Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei
captioning at scale. In Proceedings of the IEEE/CVF In-
Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,
ternationalConferenceonComputerVision(ICCV),2019.
YangGao,JiayeGe,ChenyaGu,YuzheGu,TaoGui,Aijia
8
Guo,QipengGuo,ConghuiHe,YingfanHu,TingHuang,
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
TaoJiang,PenglongJiao,ZhenjiangJin,ZhikaiLei,Jiax-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
ingLi,JingwenLi,LinyangLi,ShuaibinLi,WeiLi,Yin-
Mensch,KatieMillican,MalcolmReynolds,RomanRing,
ing Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-
ElizaRutherford,SerkanCabi,TengdaHan,ZhitaoGong,
wen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun
SinaSamangooei,MarianneMonteiro,JacobMenick,Se-
Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
Ning,LinkeOuyang,JiantaoQiu,YuanQu,FukaiShang,
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
YunfanShao,DeminSong,ZifanSong,ZhihaoSui,Peng
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,
Flamingo: avisuallanguagemodelforfew-shotlearning,
Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi
2022. 2
Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong
[3] Anthropic. Claude 3 haiku: our fastest model yet,
Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,
2024. Availableat:https://www.anthropic.com/
Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing
news/claude-3-haiku. 1,8
Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,
[4] StanislawAntol,AishwaryaAgrawal,JiasenLu,Margaret
PengZhang,RuijieZhang,ShuoZhang,SongyangZhang,
Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi
WenjianZhang,WenweiZhang,XingchengZhang,Xinyue
Parikh.VQA:Visualquestionanswering.InProceedingsof
Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe
the IEEE/CVF International Conference on Computer Vi-
Zhou, ZaidaZhou, JingmingZhuo, YichengZou, Xipeng
sion(ICCV),2015. 8
Qiu,YuQiao,andDahuaLin. Internlm2technicalreport.
[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, arXivpreprintarXiv:2403.17297,2024. 2,6
Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan
[14] YuhangCao,PanZhang,XiaoyiDong,DahuaLin,andJi-
Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon
aqi Wang. DualFocus: Integrating macro and micro per-
Kornblith,PangWeiKoh,GabrielIlharco,MitchellWorts-
spectives in multi-modal large language models. arXiv
man, and Ludwig Schmidt. Openflamingo: An open-
preprintarXiv:2402.14767,2024. 2
sourceframeworkfortraininglargeautoregressivevision-
[15] GuimingHardyChen,ShunianChen,RuifeiZhang,Juny-
languagemodels. arXiv.org,2023. 2
ingChen,XiangboWu,ZhiyiZhang,ZhihongChen,Jian-
[6] JinzeBai, ShuaiBai, ShushengYang, ShijieWang, Sinan
quanLi,XiangWan,andBenyouWang.ALLaVAharness-
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
inggpt4v-synthesizeddataforalitevision-languagemodel.
Zhou. Qwen-VL: A frontier large vision-language model
arXivpreprintarXiv:2402.11684,2024. 8
withversatileabilities. arXiv.org,2023. 2,9
[16] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
[7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda
FengZhu,andRuiZhao. Shikra: Unleashingmultimodal
Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna
llm’sreferentialdialoguemagic. arXiv.org,2023. 8
Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.
Constitutional AI: Harmlessness from ai feedback. arXiv [17] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui
preprintarXiv:2212.08073,2022. 6 He, JiaqiWang, FengZhao, andDahuaLin. Sharegpt4v:
[8] Baichuan. Baichuan2: Openlarge-scalelanguagemodels. Improvinglargemulti-modalmodelswithbettercaptions.
arXiv.org,2023. 2 arXivpreprintarXiv:2311.12793,2023. 2,8
[9] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell [18] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang
Nye,AugustusOdena,ArushiSomani,andSag˘nakTas¸ırlar. Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,
Introducingourmultimodalmodels,2023. 2 Dahua Lin, and Feng Zhao. Are we on the right way for
[10] TonyBeltramelli.pix2code:Generatingcodefromagraph- evaluating large vision-language models? arXiv preprint
icaluserinterfacescreenshot. InProceedingsoftheACM arXiv:2403.20330,2024. 2,9
SIGCHIsymposiumonengineeringinteractivecomputing [19] LinChen,XilinWei,JinsongLi,XiaoyiDong,PanZhang,
systems,2018. 6 Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin,
[11] AliFurkanBiten,RubenTito,AndresMafla,LluisGomez, Zhenyu Tang, et al. ShareGPT4Video: Improving video
Marc¸alRusinol,ErnestValveny,CVJawahar,andDimos- understanding and generation with better captions. arXiv
thenisKaratzas. Scenetextvisualquestionanswering. In preprintarXiv:2406.04325,2024. 8
ICCV,2019. 8 [20] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan- William Yang Wang. TabFact: A large-scale dataset for
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. table-based fact verification. In Proceedings of the Inter-
11nationalConferenceonLearningRepresentations(ICLR), [28] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet
2020. 2,8,9 Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao
[21] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Zhang,JunyuHan,ErruiDing,etal.Icdar2019robustread-
Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence ingchallengeonarbitrary-shapedtext-rrc-art. InInterna-
Zitnick.Microsoftcococaptions:Datacollectionandeval- tionalConferenceonDocumentAnalysisandRecognition
uationserver,2015. 8 (ICDAR),2019. 8
[22] XiChen, Josip Djolonga, Piotr Padlewski, BasilMustafa, [29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak- Barham, Hyung Won Chung, Charles Sutton, Sebastian
eri,MostafaDehghani,DanielSalz,MarioLucic,Michael Gehrmann, et al. Palm: Scaling language modeling with
Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, pathways. arXiv.org,2022. 1,2
Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin [30] OpenCompass Contributors. Opencompass: A univer-
Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, sal evaluation platform for foundation models. https:
Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas //github.com/open-compass/opencompass,
Beyer, JulienAmelot, KentonLee, AndreasPeterSteiner, 2023. 9
Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, [31] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhos- Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
seini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Fung, and Steven Hoi. Instructblip: Towards general-
RaduSoricut. Pali-x: Onscalingupamultilingualvision purpose vision-language models with instruction tuning,
andlanguagemodel,2023. 2 2023. 1,2
[23] XiChen,XiaoWang,LucasBeyer,AlexanderKolesnikov, [32] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,
Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebas- DeshrajYadav,Jose´ M.F.Moura,DeviParikh,andDhruv
tian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Batra. VisualDialog. InProceedingsoftheIEEEConfer-
DanielSalz,XiXiong,DanielVlasic,FilipPavetic,Keran enceonComputerVisionandPatternRecognition(CVPR),
Rong,TianliYu,DanielKeysers,XiaohuaZhai,andRadu 2017. 8
Soricut. Pali-3 vision language models: Smaller, faster, [33] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,
stronger,2023. Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,
[24] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio- Haodong Duan, Maosong Cao, Wenwei Zhang, Yining
vanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-
Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander wen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu
Kolesnikov,JoanPuigcerver,NanDing,KeranRong,Has- Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2:
sanAkbari,GauravMishra,LintingXue,AshishThapliyal, Mastering free-form text-image composition and compre-
James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, hension in vision-language large model. arXiv preprint
ChaoJia,BurcuKaragolAyan,CarlosRiquelme,Andreas arXiv:2401.16420,2024. 2,6,7,8
Steiner,AneliaAngelova,XiaohuaZhai,NeilHoulsby,and [34] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,Bin
RaduSoricut.Pali:Ajointly-scaledmultilinguallanguage- Wang, Linke Ouyang, Songyang Zhang, Haodong Duan,
imagemodel,2023. 2 Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe
[25] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, XinyueZhang, WeiLi, JingwenLi, WenhaiWang,
Chen, SenXing, MuyanZhong, QinglongZhang, Xizhou KaiChen,ConghuiHe,XingchengZhang,JifengDai,Yu
Zhu,LeweiLu,BinLi,PingLuo,TongLu,YuQiao,and Qiao, DahuaLin, andJiaqiWang. Internlm-xcomposer2-
Jifeng Dai. Internvl: Scaling up vision foundation mod- 4khd: Apioneeringlargevision-languagemodelhandling
els and aligning for generic visual-linguistic tasks. arXiv resolutions from 336 pixels to 4k hd. arXiv preprint
preprintarXiv:2312.14238,2023. 2 arXiv:2404.06512,2024. 2,6,7
[26] ZheChen,WeiyunWang,HaoTian,ShenglongYe,Zhang- [35] DannyDriess,FeiXia,MehdiS.M.Sajjadi,CoreyLynch,
wei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Yan, HeweiGuo, ConghuiHe, BotianShi, ZhenjiangJin, Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
ChaoXu,BinWang,XingjianWei,WeiLi,WenjianZhang, worth,SergeyLevine,VincentVanhoucke,KarolHausman,
BoZhang,PinlongCai,LichengWen,XiangchaoYan,Min MarcToussaint, KlausGreff, AndyZeng, IgorMordatch,
Dou,LeweiLu,XizhouZhu,TongLu,DahuaLin,YuQiao, andPeteFlorence. Palm-e: Anembodiedmultimodallan-
Jifeng Dai, and Wenhai Wang. How far are we to gpt- guage model. In arXiv preprint arXiv:2303.03378, 2023.
4v?closingthegaptocommercialmultimodalmodelswith 2
open-sourcesuites,2024. 2,9 [36] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
[27] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhang- Dan Jurafsky, and Douwe Kiela. KTO: Model align-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, ment as prospect theoretic optimization. arXiv preprint
Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and arXiv:2402.01306,2024. 6
EricP.Xing. Vicuna: Anopen-sourcechatbotimpressing [37] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia
gpt-4with90%*chatgptquality,2023. 8 andJuanCarlosNiebles. ActivityNet: Alarge-scalevideo
12benchmarkforhumanactivityunderstanding. InProceed- Dong, Ming Ding, et al. CogAgent: A visual language
ingsoftheIEEE/CVFConferenceonComputerVisionand model for gui agents. In Proceedings of the IEEE/CVF
PatternRecognition(CVPR),2015. 2,8 Conference on Computer Vision and Pattern Recognition
[38] XinyuFang,KangruiMao,HaodongDuan,XiangyuZhao, (CVPR),2024. 10
YiningLi,DahuaLin,andKaiChen. MMBench-Video:A [49] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang
long-form multi-shot benchmark for holistic video under- Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei
standing. arXivpreprintarXiv:2406.14515,2024. 2,9 Huang, et al. mplug-docowl 1.5: Unified structure learn-
[39] XinyuFang,KangruiMao,HaodongDuan,XiangyuZhao, ing for ocr-free document understanding. arXiv preprint
YiningLi,DahuaLin,andKaiChen. Mmbench-video: A arXiv:2403.12895,2024. 2
long-form multi-shot benchmark for holistic video under- [50] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang
standing,2024. 2 Zhang,BoZhang,ChenLi,JiZhang,QinJin,FeiHuang,
[40] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, et al. mPLUG-DocOwl 1.5: Unified structure learn-
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin- ing for ocr-free document understanding. arXiv preprint
rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron- arXiv:2403.12895,2024. 9
grong Ji. Mme: A comprehensive evaluation benchmark [51] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-
for multimodal large language models. arXiv preprint Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
arXiv:2306.13394,2023. 2,9 LoRA:Low-rankadaptationoflargelanguagemodels. In
[41] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Proceedings of the International Conference on Learning
Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Representations(ICLR),2022. 8
Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-
[52] Suyuan Huang, Haoxin Zhang, Yan Gao, Yao Hu, and
hui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-
ZengchangQin. Fromimagetovideo,whatdoweneedin
sheng Li, and Xing Sun. A challenger to gpt-4v? early
multimodalllms? arXivpreprintarXiv:2404.11865,2024.
explorationsofgeminiinvisualexpertise. arXivpreprint
2
arXiv:2312.12436,2023. 1,2
[53] DrewAHudsonandChristopherDManning. Gqa:Anew
[42] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai
dataset for real-world visual reasoning and compositional
Ren, RenruiZhang, ZihanWang, ChenyuZhou, Yunhang
question answering. Conference on Computer Vision and
Shen, MengdanZhang, etal. Video-MME:Thefirst-ever
PatternRecognition(CVPR),2019. 8
comprehensiveevaluationbenchmarkofmulti-modalllms
[54] MdMohaiminulIslam,NganHo,XitongYang,TusharNa-
invideoanalysis. arXivpreprintarXiv:2405.21075,2024.
garajan, Lorenzo Torresani, and Gedas Bertasius. Video
2,9
recap: Recursive captioning of hour-long videos. arXiv
[43] TeamGLM,AohanZeng,BinXu,BowenWang,Chenhui
preprintarXiv:2402.13250,2024. 2
Zhang,DaYin,DiegoRojas,GuanyuFeng,HanlinZhao,
[55] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
Hanyu Lai, et al. ChatGLM: A family of large language
Chris Bamford, Devendra Singh Chaplot, Diego de las
models from glm-130b to glm-4 all tools. arXiv preprint
Casas,FlorianBressand,GiannaLengyel,GuillaumeLam-
arXiv:2406.12793,2024. 9
ple, Lucile Saulnier, Le´lio Renard Lavaud, Marie-Anne
[44] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,
Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,
Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,
Thomas Wang, Timothe´e Lacroix, and William El Sayed.
FurongHuang,YaserYacoob,DineshManocha,andTianyi
Mistral7b,2023. 1,2
Zhou. Hallusionbench: An advanced diagnostic suite for
[56] DongfuJiang,XuanHe,HuayeZeng,CongWei,MaxKu,
entangledlanguagehallucination&visualillusioninlarge
Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-
vision-languagemodels,2023. 2,9
imageinstructiontuning,2024. 2
[45] BoHe,HengduoLi,YoungKyunJang,MenglinJia,Xue-
feiCao, AshishShah, AbhinavShrivastava, andSer-Nam [57] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun
Lim. Ma-lmm: Memory-augmented large multimodal Cao, andLiYuan. Chat-univi: Unifiedvisualrepresenta-
model for long-term video understanding. arXiv preprint tionempowerslargelanguagemodelswithimageandvideo
arXiv:2404.05726,2024. 2 understanding. arXivpreprintarXiv:2311.08046,2023. 2
[46] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin [58] Kushal Kafle, Brian Price, Scott Cohen, and Christopher
Wang, WeiLi, HangYan, JiaqiWang, andDaLin. Wan- Kanan.DVQA:Understandingdatavisualizationsviaques-
juan: A comprehensive multimodal dataset for advancing tionanswering. InProceedingsoftheIEEE/CVFConfer-
englishandchineselargemodels. ArXiv,abs/2308.10755, enceonComputerVisionandPatternRecognition(CVPR),
2023. 8 2018. 8
[47] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, [59] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo
Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Park, andMichaelSRyoo. Languagerepositoryforlong
Dong, Ming Ding, et al. Cogagent: A visual language video understanding. arXiv preprint arXiv:2403.14622,
model for gui agents. arXiv preprint arXiv:2312.08914, 2024. 2
2023. 2 [60] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
[48] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Radford,JeffreyWu,andDarioAmodei. Scalinglawsfor
13neurallanguagemodels. arXivpreprintarXiv:2001.08361, [74] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng
2020. 2 Zhong,YixinChen,RuihangChu,ShaotengLiu,andJiaya
[61] AniruddhaKembhavi,MikeSalvato,EricKolve,Minjoon Jia. Mini-Gemini: Miningthepotentialofmulti-modality
Seo, HannanehHajishirzi, andAliFarhadi. Adiagramis visionlanguagemodels. arXivpreprintarXiv:2403.18814,
worth a dozen images. In Proceedings of the European 2024. 2
ConferenceonComputerVision(ECCV),2016. 2,8,9 [75] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam
[62] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Kortylewski,WufeiMa,BenjaminVanDurme,andAlanL
JonghyunChoi,AliFarhadi,andHannanehHajishirzi. Are Yuille. Super-clevr: Avirtualbenchmarktodiagnosedo-
yousmarterthanasixthgrader?textbookquestionanswer- main robustness in visual reasoning. In Proceedings of
ing for multimodal machine comprehension. In Proceed- theIEEE/CVFConferenceonComputerVisionandPattern
ingsoftheIEEE/CVFConferenceonComputerVisionand Recognition(CVPR),2023. 8
PatternRecognition(CVPR),2017. 8 [76] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,
[63] WonkyunKim,ChanginChoi,WonseokLee,andWonjong Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
Rhee.Animagegridcanbeworthavideo:Zero-shotvideo Monkey: Image resolution and text label are important
questionansweringusingavlm,2024. 6 things for large multi-modal models. arXiv preprint
[64] HugoLaurenc¸on,Le´oTronchon,andVictorSanh.Unlock- arXiv:2311.06607,2023. 2
ingtheconversionofwebscreenshotsintohtmlcodewith [77] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and
the websight dataset. arXiv preprint arXiv:2403.09029, Li Yuan. Video-llava: Learning united visual represen-
2024. 6,7,10 tation by alignment before projection. arXiv preprint
[65] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herve´ arXiv:2311.10122,2023. 2
LeBorgne,RomaricBesanc¸on,Jose´ GMoreno,andJesu´s [78] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,
Lovo´nMelgarejo. Viquae, adatasetforknowledge-based AndrewTao,HuiziMao,JanKautz,MohammadShoeybi,
visual question answering about named entities. In Pro- and Song Han. Vila: On pre-training for visual language
ceedingsofthe45thInternationalACMSIGIRConference models,2024. 2,9
on Research and Development in Information Retrieval, [79] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian
pages3108–3120,2022. 8 Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin
[66] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix- Chen,etal.Sphinx:Thejointmixingofweights,tasks,and
iaoGe,andYingShan. Seed-bench: Benchmarkingmulti- visualembeddingsformulti-modallargelanguagemodels.
modalllmswithgenerativecomprehension,2023. 2,9 arXivpreprintarXiv:2311.07575,2023. 2
[67] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, [80] Adam Dahlgren Lindstro¨m and Savitha Sam Abra-
FanyiPu,andZiweiLiu.Otterhd:Ahigh-resolutionmulti- ham. Clevr-math: A dataset for compositional lan-
modalitymodel,2023. 2 guage,visualandmathematicalreasoning. arXivpreprint
[68] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, arXiv:2208.05358,2022. 8
JingkangYang,andZiweiLiu.Otter:Amulti-modalmodel [81] FangyuLiu,GuyEdwardTohEmerson,andNigelCollier.
within-contextinstructiontuning. arXiv.org,2023. 2 Visual spatial reasoning. Transactions of the Association
[69] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wen- forComputationalLinguistics(TACL),2023. 8
hai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu [82] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,
Qiao. Videochat: Chat-centricvideounderstanding. arXiv Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong
preprintarXiv:2305.06355,2023. 2 Yu. Mmc: Advancing multimodal chart understand-
[70] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang, ing with large-scale instruction tuning. arXiv preprint
Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. arXiv:2311.10774,2023. 8
Mvbench: A comprehensive multi-modal video under- [83] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.
standing benchmark. arXiv preprint arXiv:2311.17005, Improved baselines with visual instruction tuning. arXiv
2023. 2 preprintarXiv:2310.03744,2023. 9
[71] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang, [84] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Lee. Visualinstructiontuning. arXiv.org,2023. 1,8
Mvbench: A comprehensive multi-modal video under- [85] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
standing benchmark. In Proceedings of the IEEE/CVF Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-
Conference on Computer Vision and Pattern Recognition provedreasoning,ocr,andworldknowledge,2024. 2
(CVPR),2024. 2,9 [86] ShuoLiu,KainingYing,HaoZhang,YueYang,YuqiLin,
[72] LeiLi,ZhihuiXie,MukaiLi,ShunianChen,PeiyiWang, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi
LiangChen,YazhengYang,BenyouWang,andLingpeng Shao, et al. Convbench: A multi-turn conversation eval-
Kong. Silkie: Preference distillation for large visual lan- uation benchmark with hierarchical capability for large
guagemodels. arXivpreprintarXiv:2312.10665,2023. 6 vision-languagemodels. arXivpreprintarXiv:2403.20194,
[73] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid: An 2024. 2
image is worth 2 tokens in large language models. arXiv [87] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
preprintarXiv:2311.17043,2023. 2 Songyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,
14ConghuiHe, ZiweiLiu, KaiChen, andDahuaLin. Mm- [100] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-
bench: Is your multi-modal model an all-around player? yuanLi,HannanehHajishirzi,HaoCheng,Kai-WeiChang,
arXiv:2307.06281,2023. 2,9 Michel Galley, and Jianfeng Gao. Mathvista: Evaluating
[88] YuanxinLiu,ShichengLi,YiLiu,YuxiangWang,Shuhuai mathematicalreasoningoffoundationmodelsinvisualcon-
Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Temp- texts. InInternationalConferenceonLearningRepresen-
Compass: Dovideollmsreallyunderstandvideos? arXiv tations(ICLR),2024. 2,9
preprintarXiv:2403.00476,2024. 2,9 [101] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,
[89] YuliangLiu,ZhangLi,BiaoYang,ChunyuanLi,Xucheng Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu
Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the Wei. Valley: Video assistant with large language model
hiddenmysteryofocrinlargemultimodalmodels,2024.2, enhancedability. arXivpreprintarXiv:2306.07207,2023.
9 2
[90] YuliangLiu,BiaoYang,QiangLiu,ZhangLi,ZhiyinMa, [102] TengchaoLv,YupanHuang,JingyeChen,LeiCui,Shum-
Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free ing Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,
largemultimodalmodelforunderstandingdocument.arXiv LiDong,WeiyaoLuo,ShaoxiangWu,GuoxinWang,Cha
preprintarXiv:2403.04473,2024. 2 Zhang,andFuruWei. Kosmos-2.5: Amultimodalliterate
[91] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- model,2023. 2
enhofer,TrevorDarrell,andSainingXie.Aconvnetforthe [103] YuboMa,YuhangZang,LiangyuChen,MeiqiChen,Yizhu
2020s,2022. 2 Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi
[92] ZiyuLiu,TaoChu,YuhangZang,XilinWei,XiaoyiDong, Dong, Pan Zhang, Jiang Yu-Gang Pan, Liangming, Jiaqi
PanZhang,ZijianLiang,YuanjunXiong,YuQiao,Dahua Wang, Yixin Cao, and Aixin Sun. MMLongBench-Doc:
Lin, et al. MMDU: A multi-turn multi-image dialog un- Benchmarkinglong-contextdocumentunderstandingwith
derstanding benchmark and instruction-tuning dataset for visualizations. arXivpreprintarXiv:2407.01523,2024. 2
lvlms. arXivpreprintarXiv:2406.11833,2024. 2,7,8,9, [104] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and
10 Fahad Shahbaz Khan. Video-chatgpt: Towards detailed
[93] ZiyuLiu,ZeyiSun,YuhangZang,WeiLi,PanZhang,Xi- videounderstandingvialargevisionandlanguagemodels.
aoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. arXivpreprintarXiv:2306.05424,2023. 2
RAR:Retrievingandrankingaugmentedmllmsforvisual [105] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
recognition. arXivpreprintarXiv:2403.13805,2024. 2 RoozbehMottaghi. Ok-vqa: Avisualquestionanswering
benchmarkrequiringexternalknowledge. InProceedings
[94] LocalLLaMA. Dynamically scaled rope further increases
oftheIEEE/CVFConferenceonComputerVisionandPat-
performance of long context llama with zero fine-tuning,
ternRecognition(CVPR),pages3195–3204,2019. 8
2023. 2
[106] AhmedMasry, DoXuanLong, JiaQingTan, ShafiqJoty,
[95] AntonLozhkov,RaymondLi,LoubnaBenAllal,Federico
and Enamul Hoque. Chartqa: A benchmark for question
Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,
answering about charts with visual and logical reasoning.
DmytroPykhtar,JiaweiLiu,YuxiangWei,etal. Starcoder
2 and the stack v2: The next generation. arXiv preprint arXivpreprintarXiv:2203.10244,2022. 2,8,9
arXiv:2402.19173,2024. 7 [107] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
Docvqa:Adatasetforvqaondocumentimages.InProc.of
[96] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan
theIEEEWinterConferenceonApplicationsofComputer
Huang,XiaodanLiang,andSong-ChunZhu. Inter-gps:In-
Vision(WACV),2021. 8,9
terpretablegeometryproblemsolvingwithformallanguage
andsymbolicreasoning.InThe59thAnnualMeetingofthe [108] Minesh Mathew, Viraj Bagal, Rube`n Tito, Dimosthenis
AssociationforComputationalLinguistics(ACL),2021. 8 Karatzas,ErnestValveny,andCVJawahar.Infographicvqa.
InProc.oftheIEEEWinterConferenceonApplicationsof
[97] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,
ComputerVision(WACV),2022. 2,8,9
WeiZhang,ZhouYu,XiaodanLiang,andSong-ChunZhu.
Iconqa: A new benchmark for abstract diagram under- [109] AnandMishra,ShashankShekhar,AjeetKumarSingh,and
standing and visual language reasoning. arXiv preprint AnirbanChakraborty.OCR-VQA:Visualquestionanswer-
arXiv:2110.13214,2021. 8 ingbyreadingtextinimages. InInternationalConference
onDocumentAnalysisandRecognition(ICDAR),2019. 8
[98] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
WeiChang,Song-ChunZhu,OyvindTafjord,PeterClark, [110] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui,
and Ashwin Kalyan. Learn to explain: Multimodal rea- Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench:
soning via thought chains for science question answer- A comprehensive benchmark and toolkit for evaluat-
ing.InAdvancesinNeuralInformationProcessingSystems ing video-based large language models. arXiv preprint
(NeurIPS),2022. 8 arXiv:2311.16103,2023. 2
[99] PanLu,LiangQiu,Kai-WeiChang,YingNianWu,Song- [111] OpenAI. Chatgpt. https://openai.com/blog/
Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin chatgpt,2022. 1,2
Kalyan. Dynamicpromptlearningviapolicygradientfor [112] OpenAI. Gpt-4technicalreport,2023. 1,2,9,10
semi-structured mathematical reasoning. arXiv preprint [113] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,Huy
arXiv:2209.14610,2022. 8 Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
15Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, 400m: Opendatasetofclip-filtered400millionimage-text
MahmoudAssran,NicolasBallas,WojciechGaluba,Rus- pairs. arXivpreprintarXiv:2111.02114,2021. 8
sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, [126] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRad-
Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu ford,andOlegKlimov. Proximalpolicyoptimizationalgo-
Xu, Herve´ Jegou, Julien Mairal, Patrick Labatut, Armand rithms. arXivpreprintarXiv:1707.06347,2017. 6
Joulin,andPiotrBojanowski. Dinov2: Learningrobustvi- [127] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
sualfeatureswithoutsupervision,2024. 2 Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A
[114] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. benchmark for visual question answering using world
Im2text:Describingimagesusing1millioncaptionedpho- knowledge. In Proceedings of the European Conference
tographs. In Advances in Neural Information Processing onComputerVision(ECCV),2022. 8
Systems(NeurIPS),2011. 8 [128] Sanket Shah, Anand Mishra, Naganand Yadati, and
[115] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,Car- Partha Pratim Talukdar. Kvqa: Knowledge-aware visual
roll Wainwright, Pamela Mishkin, Chong Zhang, Sand- questionanswering. InProceedingsoftheConferenceon
hini Agarwal, Katarina Slama, Alex Ray, et al. Training ArtificialIntelligence(AAAI),2019. 8
language models to follow instructions with human feed- [129] PiyushSharma,NanDing,SebastianGoodman,andRadu
back. AdvancesinNeuralInformationProcessingSystems Soricut. Conceptualcaptions:Acleaned,hypernymed,im-
(NeurIPS),2022. 2,6 agealt-textdatasetforautomaticimagecaptioning. InThe
[116] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley AnnualMeetingoftheAssociationforComputationalLin-
Roberts,SiddarthaNaidu,andColinWhite.Smaug:Fixing guistics(ACL),2018. 8
failuremodesofpreferenceoptimisationwithdpo-positive. [130] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,
arXivpreprintarXiv:2402.13228,2024. 6 PeiXu,LinyanCui,SergeBelongie,ShijianLu,andXiang
Bai. Icdar2017competitiononreadingchinesetextinthe
[117] PanupongPasupatandPercyLiang.Compositionalseman-
wild(rctw-17). InInternationalConferenceonDocument
ticparsingonsemi-structuredtables. InTheAnnualMeet-
AnalysisandRecognition(ICDAR),2017. 8
ingoftheAssociationforComputationalLinguistics(ACL),
[131] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo
2015. 2,8,9
Liu, and Diyi Yang. Design2code: How far are we
[118] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan
from automating front-end engineering? arXiv preprint
Huang,ShumingMa,andFuruWei.Kosmos-2:Grounding
arXiv:2403.03163,2024. 2,6,9,10
multimodallargelanguagemodelstotheworld. arXiv.org,
[132] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
2023. 2
AmanpreetSingh.Textcaps:adatasetforimagecaptioning
[119] RenjiePi,TianyangHan,WeiXiong,JipengZhang,Run-
withreadingcomprehension. InProceedingsoftheEuro-
tao Liu, Rui Pan, and Tong Zhang. Strengthening multi-
peanConferenceonComputerVision(ECCV),2020. 8
modallargelanguagemodelwithbootstrappedpreference
[133] AmanpreetSingh, VivekNatarajan, MeetShah, YuJiang,
optimization. arXivpreprintarXiv:2403.08730,2024. 6
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
[120] RuiQian,XiaoyiDong,PanZhang,YuhangZang,Shuan-
Rohrbach. Towardsvqamodelsthatcanread. InProceed-
grui Ding, Dahua Lin, and Jiaqi Wang. Streaming long
ingsoftheIEEE/CVFConferenceonComputerVisionand
videounderstandingwithlargelanguagemodels,2024. 2
PatternRecognition(CVPR),2019. 2,8,9
[121] Qwen. Introducing Qwen-7B: Open foundation and
[134] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng
human-aligned models (of the state-of-the-arts), 2023. 1,
Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye,
2
Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From
[122] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya densetokentosparsememoryforlongvideounderstand-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, ing. arXivpreprintarXiv:2307.16449,2023. 2
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- [135] EnxinSong,WenhaoChai,TianYe,Jenq-NengHwang,Xi
ingtransferablevisualmodelsfromnaturallanguagesuper- Li,andGaoangWang.Moviechat+:Question-awaresparse
vision. InProceedingsoftheInternationalConferenceon memoryforlongvideoquestionanswering. arXivpreprint
Machinelearning(ICML),2021. 2,6,7 arXiv:2404.17176,2024. 2
[123] AlecRadford,JongWookKim,TaoXu,GregBrockman, [136] QuanSun,YufengCui,XiaosongZhang,FanZhang,Qiy-
Christine McLeavey, and Ilya Sutskever. Robust speech ing Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao,
recognition via large-scale weak supervision. arxiv 2022. Jingjing Liu, Tiejun Huang, and Xinlong Wang. Genera-
arXivpreprintarXiv:2212.04356,10,2022. 2,7 tivemultimodalmodelsarein-contextlearners,2024. 2
[124] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo- [137] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,
pherDManning,StefanoErmon,andChelseaFinn. Direct CanjieLuo, ChunChetNg, JunyuHan, ErruiDing, Jing-
preferenceoptimization:Yourlanguagemodelissecretlya tuoLiu,DimosthenisKaratzas,etal. Icdar2019competi-
rewardmodel.InAdvancesinNeuralInformationProcess- tiononlarge-scalestreetviewtextwithpartiallabeling-rrc-
ingSystems(NeurIPS),2024. 2,6,9 lsvt. In International Conference on Document Analysis
[125] ChristophSchuhmann,RichardVencu,RomainBeaumont, andRecognition(ICDAR),2019. 8
RobertKaczmarczyk,ClaytonMullis,AarushKatta,Theo [138] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion- Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.
16Alpha-CLIP:Aclipmodelfocusingonwhereveryouwant. [153] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
InProceedingsoftheIEEE/CVFConferenceonComputer Bosma,BrianIchter,FeiXia,EdChi,QuocLe,andDenny
VisionandPatternRecognition(CVPR),2024. 2 Zhou. Chain-of-thought prompting elicits reasoning in
[139] S.Svetlichnaya. DeepForm: Understandstructureddocu- largelanguagemodels,2023. 2
mentsatscale.,2020. 2,8,9 [154] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang,
[140] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Vi- and Bohan Zhuang. Longvlm: Efficient long video un-
sualMRC: Machine reading comprehension on document derstanding via large language models. arXiv preprint
images. InProceedingsoftheConferenceonArtificialIn- arXiv:2404.03384,2024. 2
telligence(AAAI),2021. 2,8,9 [155] XAI. Grok-1.5visionpreview. 2024. 2,9
[141] Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, [156] LinXu, YilinZhao, DaquanZhou, ZhijieLin, SeeKiong
BinghongWu,QiLiu,HaoFeng,YangLi,SiqiWang,Lei Ng,andJiashiFeng.Pllava:Parameter-freellavaextension
Liao,WeiShi,YuliangLiu,HaoLiu,YuanXie,XiangBai, fromimagestovideosforvideodensecaptioning,2024. 6
andCanHuang. Textsquare: Scalinguptext-centricvisual [157] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin
instructiontuning,2024. 2 Ni,ChunjiangGe,Tat-SengChua,ZhiyuanLiu,Maosong
[142] GeminiTeam. Gemini: Afamilyofhighlycapablemulti- Sun, andGaoHuang. Llava-uhd: anlmmperceivingany
modalmodels,2023. 1,9,10 aspect ratio and high-resolution images. arXiv preprint
arXiv:2403.11703,2024. 2
[143] InternLMTeam. Internlm: Amultilinguallanguagemodel
with progressively enhanced capabilities. https:// [158] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming
github.com/InternLM/InternLM, 2023. 1, 2, 7, Yan, GuohaiXu, ChenliangLi, JunfengTian, QiQian, Ji
Zhang,etal. Ureader: Universalocr-freevisually-situated
8
language understanding with multimodal large language
[144] 360VLTeam. 360vl,2024. 9
model. arXivpreprintarXiv:2310.05126,2023. 2
[145] WeMMTeam. Wemm,2024. 9
[159] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
[146] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
YiyangZhou,JunyangWang,AnwenHu,PengchengShi,
Martinet, Marie-Anne Lachaux, Timothe´e Lacroix, Bap-
Yaya Shi, et al. mplug-owl: Modularization empowers
tiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar,
largelanguagemodelswithmultimodality.arXiv.org,2023.
etal. Llama:Openandefficientfoundationlanguagemod-
2
els. arXiv.org,2023. 1,2
[160] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
[147] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, enmaier. From image descriptions to visual denotations:
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, New similarity metrics for semantic inference over event
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. descriptions. TransactionsoftheAssociationforComputa-
Llama 2: Open foundation and fine-tuned chat models, tionalLinguistics(TACL),2014. 8
2023. 1,2
[161] LonghuiYu,WeisenJiang,HanShi,JinchengYu,Zhengy-
[148] YuxuanWan,ChaozhengWang,YiDong,WenxuanWang, ing Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian
ShuqingLi, YintongHuo, andMichaelRLyu. Automat- Weller,andWeiyangLiu. Metamath: Bootstrapyourown
ically generating ui code from screenshot: A divide-and- mathematical questions for large language models. arXiv
conquer-basedapproach.arXivpreprintarXiv:2406.16386, preprintarXiv:2309.12284,2023. 8
2024. 6
[162] ShoubinYu,JaeminCho,PrateekYadav,andMohitBansal.
[149] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,Zuxuan Self-chainedimage-languagemodelforvideolocalization
Wu, andYu-GangJiang. Toseeistobelieve: Prompting andquestionanswering.InAdvancesinNeuralInformation
gpt-4vfor bettervisual instructiontuning. arXivpreprint ProcessingSystems(NeurIPS),2024. 2
arXiv:2311.07574,2023. 8
[163] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng
[150] WeihanWang,QingsongLv,WenmengYu,WenyiHong,Ji Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,
Qi,YanWang,JunhuiJi,ZhuoyiYang,LeiZhao,Xixuan MaosongSun,etal. RLHF-V:Towardstrustworthymllms
Song,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,Ming via behavior alignment from fine-grained correctional hu-
Ding,andJieTang. Cogvlm: Visualexpertforpretrained man feedback. In Proceedings of the IEEE/CVF Confer-
languagemodels,2023. 2 enceonComputerVisionandPatternRecognition(CVPR),
[151] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, 2024. 6
Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and [164] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Xiangyu Zhang. Vary: Scaling up the vision vocab- KevinLin,ZichengLiu,XinchaoWang,andLijuanWang.
ulary for large vision-language models. arXiv preprint Mm-vet: Evaluating large multimodal models for inte-
arXiv:2312.06109,2023. 2 gratedcapabilities.arXivpreprintarXiv:2308.02490,2023.
[152] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten 2,9
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. [165] Tai-LingYuan,ZheZhu,KunXu,Cheng-JunLi,Tai-Jiang
Chain-of-thoughtpromptingelicitsreasoninginlargelan- Mu, and Shi-Min Hu. A large chinese text dataset in the
guagemodels.InAdvancesinNeuralInformationProcess- wild. JournalofComputerScienceandTechnology,34(3):
ingSystems(NeurIPS),2022. 8 509–521,2019. 8
17[166] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, [178] HaozheZhao,ZefanCai,ShuzhengSi,XiaojianMa,Kaikai
Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, An,LiangChen,ZixuanLiu,ShengWang,WenjuanHan,
Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin andBaobaoChang. Mmicl: Empoweringvision-language
Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu model with multi-modal in-context learning. arXiv.org,
Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and 2023. 2
Wenhu Chen. Mmmu: A massive multi-discipline multi- [179] WenliangZhao,XuminYu,andZengyiQin.Melotts:High-
modal understanding and reasoning benchmark for expert qualitymulti-lingualmulti-accenttext-to-speech,2023. 2,
agi. arXivpreprintarXiv:2311.16502,2023. 2,9 7
[167] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and [180] ZhiyuanZhao,BinWang,LinkeOuyang,XiaoyiDong,Ji-
Chen Change Loy. Contextual object detection with aqi Wang, and Conghui He. Beyond hallucinations: En-
multimodal large language models. arXiv preprint hancing lvlms through hallucination-aware direct prefer-
arXiv:2305.18279,2023. 2 enceoptimization.arXivpreprintarXiv:2311.16839,2023.
[168] AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,Hanyu 6
Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, [181] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao,
XiaoXia,WengLamTam,ZixuanMa,YufeiXue,Jidong Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang,
Zhai, WenguangChen, ZhiyuanLiu, PengZhang, Yuxiao and Zheng Liu. MLVU: A comprehensive benchmark
Dong, and Jie Tang. GLM-130b: An open bilingual pre- for multi-task long video understanding. arXiv preprint
trainedmodel. InProceedingsoftheInternationalConfer- arXiv:2406.04264,2024. 2,9
enceonLearningRepresentations(ICLR),2023. 2 [182] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea
[169] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Finn,andHuaxiuYao. Aligningmodalitiesinvisionlarge
LucasBeyer.Sigmoidlossforlanguageimagepre-training, languagemodelsviapreferencefine-tuning. arXivpreprint
2023. 2 arXiv:2402.11411,2024. 6
[170] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, [183] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
andJiaqiWang.Long-CLIP:Unlockingthelong-textcapa- Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
bilityofclip. arXivpreprintarXiv:2403.15378,2024. 2 language understanding with advanced large language
[171] CeZhang,TaixiLu,MdMohaiminulIslam,ZiyangWang, models. arXiv.org,2023. 1,2
ShoubinYu,MohitBansal,andGedasBertasius. Asimple
llm framework for long-range video question-answering.
arXivpreprintarXiv:2312.17235,2023. 2
[172] HangZhang, XinLi, andLidongBing. Video-llama: An
instruction-tuned audio-visual language model for video
understanding. arXivpreprintarXiv:2306.02858,2023. 2
[173] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,
LinkeOuyang,ZhiyuanZhao,ShuangruiDing,Songyang
Zhang, Haodong Duan, Hang Yan, et al. Internlm-
xcomposer: A vision-language large model for advanced
text-imagecomprehensionandcomposition.arXivpreprint
arXiv:2309.15112,2023. 1,2,8
[174] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng,
JingkangYang,YuanhanZhang,ZiyueWang,HaoranTan,
ChunyuanLi, andZiweiLiu. Longcontexttransferfrom
languagetovision.arXivpreprintarXiv:2406.16852,2024.
6
[175] RuiZhang,YongshengZhou,QianyiJiang,QiSong,Nan
Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,
MingkunYang,etal. Icdar2019robustreadingchallenge
onreadingchinesetextonsignboard.InInternationalCon-
ference on Document Analysis and Recognition (ICDAR),
2019. 8
[176] TaoZhang,XiangtaiLi,HaoFei,HaoboYuan,Shengqiong
Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan.
Omg-llava: Bridgingimage-level, object-level, pixel-level
reasoningandunderstanding,2024. 2
[177] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang,
Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-
hd: Diving into high-resolution large multimodal models,
2024. 2
18