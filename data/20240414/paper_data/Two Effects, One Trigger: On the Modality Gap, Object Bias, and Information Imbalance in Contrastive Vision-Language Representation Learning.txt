Two Effects, One Trigger:
On the Modality Gap, Object Bias, and
Information Imbalance in Contrastive
Vision-Language Representation Learning
Simon Schrodi1* , David T. Hoffmann1,2* , Max Argus1 ,
Volker Fischer2 , and Thomas Brox1
1 University of Freiburg, 2 Bosch Center for Artificial Intelligence
{schrodi,hoffmann,argusm,brox}@cs.uni-freiburg.de,
volker.fischer@de.bosch.com
Abstract. Contrastive vision-language models like CLIP have gained
popularityfortheirversatileapplicablelearnedrepresentationsinvarious
downstream tasks. Despite their successes in some tasks, like zero-shot
imagerecognition,theyalsoperformsurprisinglypooronothertasks,like
attributedetection.Previousworkhasattributedthesechallengestothe
modalitygap,aseparationofimageandtextinthesharedrepresentation
space, and a bias towards objects over other factors, such as attributes.
In this work we investigate both phenomena. We find that only a few
embedding dimensions drive the modality gap. Further, we propose a
measureforobjectbiasandfindthatobjectbiasdoesnotleadtoworse
performanceonotherconcepts,suchasattributes.Butwhatleadstothe
emergenceofthemodalitygapandobjectbias?Toanswerthisquestion
wecarefullydesignedanexperimentalsettingwhichallowsustocontrol
theamountofsharedinformationbetweenthemodalities.Thisrevealed
that the driving factor behind both, the modality gap and the object
bias, is the information imbalance between images and captions.
Keywords: Contrastivevision-languagerepresentationlearning·Infor-
mation imbalance · Modality gap · Object bias
1 Introduction
Vision-language models have become increasingly popular and are successfully
appliedtonumeroustasks.Theybenefitfromtheirabilitytoexploitweaksuper-
vision,whichcanbeacquiredbyscrapingtheinternetforimage-textpairs.Mod-
elsaretrainedwithcontrastive[27,46]orcaptioning-basedpretraining[16,49,58].
Subsequentworksimprovedreproducibility[11]andtrainingefficiency[31,33,34,
55,70,71]. Despite weaker supervision, these models show intriguing properties:
strongzero-shotimagerecognitionperformance[43,46],cross-modalunderstand-
ing [8] and retrieval [40], or robustness [44].
Equal contribution.
∗
4202
rpA
11
]VC.sc[
1v38970.4042:viXra2 S. Schrodi et al.
Regardless of these remarkable improvements and the widespread usage, the
profound understanding of representations learned by vision-language models
is still in its infancy. For instance, a recent work identified a modality gap in
the shared embedding space [36], while other work conjectured about a bias
towards objects [3]. But how bad are these effects? The consequences as well as
the underlying triggers are not fully understood. In this paper, we thoroughly
compare the learned embeddings for both modalities, study the two phenomena
modality gap and object bias, and identify their common trigger.
The modality gap discovered by Liang et al. [36] is defined as image and
text embeddings occupying separate regions of the shared embedding space.
They attributed the phenomenon to the cone effect during model initialization
and the contrastive loss preserving the gap, while subsequent work studied the
influence of the Softmax temperature [51,59]. In this paper, we show that few
embeddingdimensionsdrivethemodalitygapanditiscausedbyaninformation
imbalance between images and their captions. While we find that increases of
the modality gap correlate with improvements in downstream performance, the
effect of the modality gap on downstream performance is small compared to
other components, such as model size or dataset properties. For instance, when
we control for the datasets, we indeed observe that modality gap decreases as
performanceimproves.Further,wefindthatimageandtextembeddingsexhibit
different biases and neighborhood orderings vary between the modalities.
Beyond above cross-modal anal-
yses, recent work hypothesized that a p h o t o o f a r e d c a t
sitting on a tree with
vision-language models exhibit a bias yellow leaves
towards objects [3]. To formalize the image
notion of “a bias towards objects”, we cS ap pa tr is oe nl ey d encoder a red cat
propose a metric, Matching Object
"an image of
Attribute Distance (MOAD), which
a red cat"
assessesthebiastowardsobjectscom- text encoder
pared to other factors, such as at-
Fig.1: Illustration of information im-
tributes. We confirm that contrastive balancebetweenimages(topleft)andcap-
vision-language models are indeed tions (bottom left). This makes it virtually
more biased towards objects than at- impossiblefortheimageencoder(topright)
tributes. However, we observe that to know what a (sparse) caption may con-
performance on attributes positively tain. Consequently, it focuses on the most
salientobjectsduetotheirhighprobability
correlates with performance on ob-
ofbeingpresentinthecaptionandtendsto
jects. This suggests that improve-
neglectothermoreunlikelyfactors,suchas
mentsonobjecttasksalsoleadtoim-
attributes.
provements on attribute tasks. Fur-
ther, we find that the bias does not stem from the global word frequency of the
training dataset but a per-sample caption presence bias, i.e., models are more
biased towards words that are more consistently mentioned in the captions; as
objects typically are in captions.
Finally,weidentifythecommontriggerforboththemodalitygapandobject
bias: information imbalance. Information imbalance describes the availability ofTwo Effects, One Trigger 3
more information for one modality in comparison to the other. For example, for
vision-language models captions are sparse (lossy) descriptions of images and
determine the focal point, whereas images contain much more information. As
a result, image encoders cannot know what information of the image they need
toencodetoaligntheimageencodingwiththetextencodingforsomeunknown
caption;seeFig.1foranillustration.Thebestthattheimageencoderscandois
tofocusonthemost salientpartsoftheimage thataretypicallypresentincap-
tions,e.g.,objects.Consequently,theencoderexhibitsabiastowardstheseparts
of the captions. Similarly, the modality gap emerges as by-product of the con-
trastiveoptimizationundertheinformationimbalancedregime.Here,themodel
trades off alignment, which is limited due to the information imbalance, with
uniformity by making all images and all texts more dissimilar (a.k.a. modal-
ity gap). Besides that, a reduction of information imbalance through caption
enrichment also improves zero-shot downstream performance. We believe that
abovefindingshelptobetterunderstandtheroleofsparsecaptionsincontrastive
vision-language representation learning.
In summary, the contributions of our analysis paper are as follows: 1) The
modality gap is driven by only few embedding dimensions. 2) For off-the-shelf
contrastive vision-language models, the modality gap and downstream perfor-
mancearepositivelycorrelated duetocommonconfounders.Controllingformore
confounders indicates that a lower modality gap indeed correlates with higher
performance. 3) Image and text embeddings have distinct characteristics de-
spite coupling them via the cross-modal loss. 4) Object bias is caused by higher
per-samplecaptionpresencebias.5)Improvementsonobjecttasksyieldimprove-
ments on attribute tasks. 6) An information imbalance between the modalities
leads to both the modality gap and the object bias.
2 Related Work
Contrastive vision-language representation learning has recently emerged as an
effective technique to learn representations with weak supervision that work
for a wide range of tasks and have intriguing properties, such as strong zero-
shot abilities. However, our understanding of the learned representation is in its
infancy. For instance, recent work showed the presence of a modality gap and
attributedittotheconeeffectofmodelinitializationandthecontrastiveloss[36].
Subsequent work explored the influence of the Softmax temperature [51,59].
Other work found that the modality gap is orthogonal to the span of image and
text embeddings [73]. In this work, we find that few dimensions are responsible
for the separation of the modalities, and identify that information imbalance
between images and text is the driving factor for the modality gap.
While some work found that large models, including vision-language mod-
els, close the gap to human perception [19,32], other work found several fail-
ure modes of them [4,69]. Other work studied the importance of data [44,64],
generalization/robustness[14,42],analyzedthelearnedfeatures/representations
[20,41,47], compositionality [13,27,57], or learned abilities and (social) bi-4 S. Schrodi et al.
ases [1,22,52,63,65,72]. It has also been discussed that vision-language models
maybebiasedtowardsobjects[3].Tostudyobjectbias,weintroduceameasure
to assess “bias towards objects”, affirming that they are indeed biased towards
objects. But, we also find that improvements on object tasks correlate with
improvements on attribute tasks. Lastly, we find that the bias stems from a
per-sample caption presence bias caused by information imbalance.
Finally,theoreticalworkdisentangledtheInfoNCE(contrastive)loss[45]into
an alignment and uniformity term [62] and showed the importance of shared
task-relevant information between the modalities [15,35,61]. In this work, we
connect information imbalance of task-relevant information to the two phenom-
ena modality gap and bias towards objects.
3 Experimental Setup
Contrastivevision-languagemodels.Unlessstatedotherwise,weusedCLIP
ViT-B/16 [46] and SigLIP ViT-B/16 [70] for our analyses. For our large-scale
analyses, we used a total of 112 contrastive vision-language models provided
by OpenCLIP [11,25]. We distinguished between medium- (i.e., dataset size of
≤128M) and large-scale datasets.
Downstream evaluation tasks. We conducted our evaluations on ImageNet
[48],MSCOCO[10,37],MIT-States[26],andUT-Zappos[67]usingthestandard
evaluation protocols from the literature. For ImageNet, we used the CLIP-style
prompts "a photo of a {obj}" [46] and computed the zero-shot (object) ac-
curacy.ForMSCOCO,weprependedtheprompt"a photo of"tothedescrip-
tionofeachimagefollowingRadfordet al.[46]andusedR@1toassesszero-shot
image-to-text retrieval performance (text-to-image retrieval yielded similar re-
sults). For MIT-States and UT-Zappos, we used the prompts "an image of a
{attr} object" and computed the zero-shot attribute accuracy. We provide
further evaluation details in Appendix A.
Multi-modal Attributes and Digits (MAD). To understand the influence
ofdata,webuiltamulti-modaldatasetbasedonMorpho-MNIST[6](avariation
of MNIST [30]) with full control over the data-generating process, called Multi-
modalAttributesandDigits(MAD).Weusedthefollowingmorphingorwarping
operations as latent factors (i.e., attributes): altering im-
age thickness (thickening, thinning, no thickthinning),
swelling (swelling, no swelling), fractures (fracture, no
fracture) from Castro et al. [6], and added scaling (large,
small),colors(gray,red,green,blue,cyan,magenta,yellow)
and captions. Visual examples are provided in Fig. 2 and Ap-
pendix A. To generate captions, we mapped the digit class and
Fig.2: MAD
latent factors to words and chained them together in random
examples.
order, e.g., 0-thickening-swelling-fractures-large-blue.
Model and training details are provided in Appendix B.
To study the effect of information imbalance due to missing information in
captions, we varied the number of attributes included in each caption, whileTwo Effects, One Trigger 5
ensuring that the digit remained consistently present. Importantly, the images
remain unchanged, i.e., all latent factors still affect the images. For example, if
we restrict each caption to one attribute (in addition to the digit), above (full)
caption reduces to, e.g., 0-blue or 0-large.
4 Cross-modal Disparities of Embeddings
Most previous work focused on improving downstream performance of con-
trastive vision-language models, while some works discovered intriguing phe-
nomenaorshortcomingsofcontrastivevision-languagemodels.However,westill
lack a thorough understanding on the causes and effects of the learned repre-
sentations. In an effort to enrich our understanding, we first analyze differences
between image and text embeddings. We start by discovering that few embed-
ding dimensions drive the modality gap (Sec. 4-4.1) and discuss the relation
of the modality gap to downstream performance (Sec. 4.2). Finally, we study
further similarities and differences of the embeddings (Sec. 4.3).
Revisiting the Modality Gap
Liang et al. [36] showed that embeddings of the modalities are located in com-
pletely separate regions of the embedding space and coined the phenomenon
modality gap. They defined the modality gap distance as the L2-distance be-
tween the Means (L2M) of the embeddings:
n n
1 (cid:88) 1 (cid:88)
L2M:=|| x − y || , (1)
n i n i
i=1 i=1
where x ,y are the i-th L2-normalized image or text embeddings, respectively.
i i
4.1 Few Embedding Dimensions Make Up the Modality Gap
Toobtainbetterinsightsintothenatureofthemodalitygap,weaskedtwoques-
tions: 1) Is the modality gap present in all dimensions or only a subset thereof?
2)Doespost-hocclosingofthemodalitygapimprovedownstreamperformance?
Wecomparedthedistributions(meansandvariances)perembeddingdimen-
sion between the modalities. Interestingly, there are few embedding dimensions
that have stark differences in their means, while that difference is close to zero
for most other dimensions (Fig. 3a). Moreover, we find that some dimensions
exhibit substantial variance within one modality and negligible variance within
the other, whereas this is reversed for the other dimension. Consequently, two
of these embedding dimensions suffice to perfectly separate the modalities, as
shown in Fig. 3b. Moreover, these dimensions are by far the largest components
oftheimageortextembeddings,respectively.Hence,theysubstantiallyinfluence
the entire embedding and are responsible for the largest part of the measured
modality gap (see the sharp drop of L2M in Fig. 4).6 S. Schrodi et al.
0.6
CLIPViT-B/16onImageNet
0.5 CLIPViT-B/16onMSCOCO
SigLIPViT-B/16onImageNet
SigLIPViT-B/16onMSCOCO
0.4
0.6
0.3
0.4
0.2
0.2
0.1
0 2 4 6 8
0.0
0 100 200 300 400 500 600 700 800
Embeddingdimensions(sorted)
(a)Abs.differenceofmeansofembeddingdims. (b)Somedims.perfectlyseparatethemodalities.
Fig.3: Few embedding dimensions separate the modalities. (a) We plot the
absolute difference in the means of each embedding dimension between the modali-
ties. Most dimensions are comparable but few dimensions have vastly different means
betweenthemodalities.(b)Pairsofthesedimensionscanperfectlyseparatethemodal-
ities (top: CLIP ViT-B/16, bottom: SigLIP ViT-B/16).
CLIPViT-B/16 SigLIPViT-B/16 CLIPViT-B/16 SigLIPViT-B/16
65 0.75 75 1.00 50 0.75 60 1.0
60 0.50 70 0.75 45 0.50 50 0.5
0 20 40 0 20 40 0 20 40 0 20 40
Ablateddimensions Ablateddimensions Ablateddimensions Ablateddimensions
Fig.4: Successive ablation of embedding dimensions based on the sorting of
embeddingdimensionsfromFig.3aleadstoasharpdrop,followedbyapartialrecovery
of downstream performance, while the modality gap gradually closes.
Takeaway 1: Few embedding dimensions drive the modality gap and two
dimensions suffice to separate the modalities.
Does removing these dimensions close the gap and improve perfor-
mance?Onemaysuspectthatablatingthedimensionswithhighcontributions
tothemodalitygapwill(substantially)closethemodalitygapandleadtobetter
downstreamperformance.Totestthis,wesuccessivelyablatedthesedimensions
using the sorting from Fig. 3a. We re-normalized the remaining dimensions and
evaluated their downstream performance. Fig. 4 shows that the modality gap
(L2M) indeed closes, but downstream performance initially decreases sharply
before partially recovering as more dimensions are ablated.
Whatisthemechanismthatexplainsthisobservation?Notethatthefirstem-
bedding dimensions are the largest components of the embeddings (as discussed
above). Hence, ablating and re-normalizing them causes substantial changes in
cosinesimilaritiesandcross-modalneighborhoods.Considerthefollowingexam-
ple:leti=[8,0.6,0.7,0.3]T beanimageembedding,andt=[5,0.13,0.035,0.02]T
andt′ =[5,1.5,0.7,0.45]T bethematchingornon-matchingtextembedding,re-
spectively. We have cosine similarities d of d(i,t) = 0.995 and d(i,t′) = 0.975.
After ablating the first dimension, we have cosine similarities of 0.822 and 0.917
snaemfoecnereffid.sbA
teNegamI ycarucca OCOCSM 1@R M2LTwo Effects, One Trigger 7
50 50 50 50
0 0 0 0
0.50 0.75 1.00 0.4 0.6 0.8 1.0 0.5 0.6 0.5 0.6
L2M L2M RMG RMG
Fig.5: Relation between modality gap and downstream task performance.
Weplotdownstreamperformanceagainstthemodalitygapforatotalof112contrastive
vision-language models. Performance and the modality gap distances show a positive
correlation,butdownstreamperformanceislargelyinfluencedbytrainingdatasetsize.
To factor this out, we split the models into two groups based on dataset size, i.e.,
medium- and large-scale.
and hence the image-text alignment flipped. Finally, it flips back after ablating
also the second dimension.
Butwhydothesemodality-separatingembeddingdimensionsappear?
AswewilldiscussinmoredetailinSec.6,maximizationofthealignmenttermof
thepopular(contrastive)InfoNCEloss[45](refertoAppendixCforbackground)
is limited by an information imbalance between the modalities, i.e., texts are
sparse descriptions of the images. This makes it impossible to minimize the
contrastive loss only by maximizing the similarity of the matching image-text
pairs (the nominator of the loss also called alignment term). To still minimize
theInfoNCEloss,amodelmaximizesalsoitsuniformityterm(denominator).It
can do so by making images and texts as dissimilar as possible.1 After all, the
InfoNCElossmaximizesthesimilarityofthematchingimage-textpairrelativeto
the similarity of the non-matching image-text pairs, not the absolute similarity.
Thus, we hypothesize that models trade off alignment, which is limited due
to information imbalance (cf., Sec. 6), in a subset of dimensions for higher
uniformity to minimize the loss. To still achieve good alignment, the model uses
few dimensions that will have a negligible effect on the alignment term but
significantly increase the uniformity term (overall lower cosine similarity scores
between images and texts). This hypothesis explains our observations in Fig. 3
(fewlargeandmodalityseparatingdimensions),Fig.4(recoveryofperformance
when these dimensions are dropped but alignment is recovered), and Fig. 8
(modality gap closes as information imbalance reduces).
4.2 Does the Modality Gap Harm Downstream Performance?
Theinfluenceofthemodalitygapondownstreamperformanceiscontroversially
discussedintheliterature[36,53,73,74].IntheexperimentsfromFig.4,ablating
dimensions closed the modality gap but did not improve the downstream per-
formance. Similarly, Liang et al. [36] closed the gap by shifting the embeddings
but found that an increase of the modality gap actually improved performance.
1 Note that the repulsive forces of the uniformity term of the InfoNCE loss solely act
across the modalities and not within the modalities.
teNegamI ycarucca OCOCSM 1@R teNegamI ycarucca OCOCSM 1@R8 S. Schrodi et al.
Table 1: Spearman rank correlation between downstream task performance
and various factors for models trained on medium and large datasets.
Downstream Modality Modality Model Embedding Dataset
task gap (L2M) gap (RMG) size size size
ImageNet 46.8 | 46.2 24.3 | 34.1 34.9 | 79.1 6.4 | 77.9 22.4 | -22.6
MS COCO 29.6 | 32.0 24.9 | 13.1 -14.8 | 77.2 54.0 | 75.2 -18.6 | -15.8
This is in contrast to intuition, which suggests that image-text pairs should be
close and a smaller modality gap should improve downstream performance.
Tobringadditionalinsightsintothisdiscussion,weevaluated112contrastive
vision-language models provided by OpenCLIP [11,25] on ImageNet classifi-
cation and MS COCO image-to-text retrieval (text-to-image retrieval yielded
similar results). We computed the modality gap distance with L2M (Eq. (1))
proposedbyLianget al.[36].OurresultsinFig.5andTab.1showthatalarger
L2M distance counter-intuitively correlates with downstream performance im-
provements.Alsonotetheseparationofmodelstrainedonmedium-(i.e.,≤128M
image-text pairs) and large-scale data.
WesuspectthatL2Mmaybethecauseforthiscounter-intuitiveobservation
(refer to Appendix D for a discussion of its limitations). Thus, we propose the
alternative Relative Modality Gap (RMG) measure:
n
1 (cid:80) d(x ,y )
n i j
RMG:= i=1 ,
(cid:16) n n (cid:17) n
0.5 (cid:80) d(x ,x )+ (cid:80) d(y ,y ) + 1 (cid:80) d(x ,y )
n(n−1) i j i j n i j
i,j=1;i̸=1 i,j=1;i̸=1 i=1
(2)
where x ,y are the i-th L2-normalized image or text embeddings, respectively,
i i
and d is some distance function (we used cosine dissimilarity scaled to [0,1]).
Intuitively, the numerator takes a per-sample view, measuring the gap where it
matters, and the denominator accounts for the effectively used space by setting
the numerator in relation to the average distances within the modalities. We
also add the distances of the matching image-text pairs to the denominator to
scale the metric to [0,1]. However, also with this more sophisticated measure for
the modality gap, we still observe a positive correlation, even though it became
weaker (Fig. 5 and Tab. 1).
Our intuition can fool us, when we assume the modality gap causes a cer-
tain downstream performance independent of other factors. Other factors can
overshadow the effect of the modality gap. As Tab. 1 reveals, downstream per-
formanceismuchmoreinfluencedbythemodelandembeddingsize;particularly
formodelstrainedonlargedatasets.Besidesthat,whencomparingonlymodels
trained on the same dataset (removing the dataset as confounder) we observe
the expected negative correlation for LAION-400M (-46.4), LAION-2B (-27.5),
and WebLI (-58.3); see Appendix D for details. OpenAI’s CLIP models still be-
have differently (38.3), possibly due to differences in the training protocols. WeTwo Effects, One Trigger 9
Table 2: Dissimilarity of neighborhood orderings in the embedding space
unsing normalized Kendall-Tau distance ∈ [0,1]. Higher normalized Kendall-Tau dis-
tancevaluesindicatethattherankingofneighborsisalteredmore.ForImageNet-100,
“s. i” indicates the i-th split.
CIFAR-10CIFAR-100ImgNet-100 s. 1ImgNet-100 s. 2ImgNet-100 s. 3
CLIP 0.3399 0.4965 0.4975 0.5046 0.5081
SigLIP 0.5044 0.4981 0.5003 0.4965 0.4987
note that the grouping based on the datasets substantially reduced the number
of available models and, thus, these correlations need to be taken with cau-
tion. However, as we will see in Sec. 6, the modality gap as well as downstream
performance are indeed both affected by common third variables, e.g., dataset
quality(orembeddingsize).Tab.1alsosuggestsmodelsizeasapotentialshared
influential factor but we leave further investigation of it for future work.
Takeaway 2: Alargermodalitygappositivelycorrelateswithdownstream
performance,yetthereisnoindicationthatthisisacausalrelationship,but
there are rather common confounders.
4.3 Further Similarities and Differences of Cross-modal Embeddings
Beyond the modality gap, we find further similarities and differences of the
embeddings by investigating the following aspects: 1) Do the directions have
similar meaning? 2) Are the nearest neighbor relations the same? 3) Are the
biases of the embeddings equally pronounced (refer to Sec. 5)?
To identify meaningful directions (1st question), we followed the ideal words
approach of Trager et al. [57]: We paired all objects with all attributes in the
captions, marginalized the attributes and subtracted the mean text embedding
to get ideal object words, and vice versa. To get ideal image embeddings, we
followed the same procedure but were limited by the available labeled images.
Wefindlowcosinesimilaritiesbetweenidealwordsandimages(CLIPViT-B/16:
0.19 for MIT-States, 0.16 for UT-Zappos; SigLIP ViT-B/16: 0.20, 0.16). How-
ever,whenwecorrectthemwiththemodalitygapvector(meandifferencevector
between matching image and text embeddings) cosine similarities significantly
increase(CLIPViT-B/16:0.56forMIT-States,0.40forUT-Zappos;SigLIPViT-
B/16: 0.68,0.56). Hence, ideal words and imagesare not aligned andit suggests
that the embedding directions of each modality have different meanings when
not corrected by the modality gap vector.
Totestsimilarityofneighborhoodrelationsoftheembeddings(2ndquestion),
we computed the mean embedding for each class in CIFAR-10, CIFAR-100 [28],
and three ImageNet-100 splits [24]. We computed the normalized Kendall-Tau
distance([0,1]),wherethenormalizationaccountsforvaryingnumberofclasses.
Intuitively, it counts the percentage of bubble-sort swaps (w.r.t. all possible
swaps) necessary to transform a nearest neighbor list of modality A to match10 S. Schrodi et al.
the nearest neighbor list of modality B. Tab. 2 reveals that the neighborhood
orderings are dissimilar between the modalities.
Takeaway 3: Directions of image and text embeddings align when cor-
rectedbythemodalitygapvectorandneighborhoodrelationsvarybetween
the modalities.
5 Object Bias Is a Caption Presence Bias
Object bias refers to the observation that contrastive vision-language models
have high performance on downstream tasks mainly linked to objects, while
achievingcomparablyworseperformanceontaskslinkedtootherlatentfactors,
such as attributes [3]. However, solely assessing object bias based on worse per-
formanceonsomeattributebenchmarkmaybemisleading,asthetaskjustcould
be more difficult than an object-based task.
Instead, we propose a measure for object vs. attribute bias, denoted as
Matching Object Attribute Distance (MOAD). MOAD quantifies how well a
model distinguishes matching to non-matching images (or texts) of objects o
compared to attributes a. Matching images (texts) show both the same object
or attribute, whereas non-matching images (texts) show different objects or at-
tributes. We define MOAD for L2-normalized image embeddings x as follows:
 
MOAD img := 2|1 O| (cid:88)  N1 1 (cid:88) xT i x j − N1 2 (cid:88) xT i x j  
o∈O xi,xj∈Xo xi∈Xo,xj∈X¬o
i̸=j
(3)
 
− 2|1 A| (cid:88)  N1 3 (cid:88) xT i x j − N1 4 (cid:88) xT i x j   ,
a∈A xi,xj∈Xa xi∈Xa,xj∈X¬a
i̸=j
where N ,...,N are normalization factors, X ,X ,X ,X are all images x
1 4 o ¬o a ¬a
that (not) entail the object o∈O or attribute a∈A, respectively. We similarly
define MOAD for text embeddings y. Positive values indicate a bias towards
txt
objects, negative values a bias towards attributes, and zero no bias.
We analyzed the relation between MOAD and downstream performance in
Fig. 6a. As expected, the majority of contrastive vision-language models show-
caseabiastowardsobjects(positiveMOADvalues).Notably,modelstrainedon
large-scale data exhibit a less pronounced object bias (smaller positive MOAD
values)comparedtomodelstrainedonmedium-scaledata.However,wefindonly
a very weak to no correlation between MOAD and downstream performance for
models trained on large-scale data. Fig. 6b shows that this can be attributed to
the medium-to-strong correlations between performance on object (ImageNet,
MS COCO) and attribute tasks (MIT-States, UT-Zappos). This suggests that
contrastive vision-language models tend to perform well or poorly in both types
of tasks, rather than excelling in one while underperforming in the other.Two Effects, One Trigger 11
Rankcorr.:9.6 Rankcorr.:93.7 Rankcorr.:90.9 20 Rankcorr.:-32.5 20 20 20
Rankcorr.:9.1
10 10 Rankcorr.:36.1 10 0
0
0 25 50 75 0 20 40 60
0.02 0.04 0.06 0.08 0.02 0.03
80 Rankcorr.:31.5 40 Rankcorr.:-5.0 40 Rankcorr.:58.0 40 Rankcorr.:58.5 60 Rankcorr.:-19.6 Rankcorr.:23.1 20
40 20
20
20 0
0 25 50 75 0 20 40 60
−0.02 0.00 0.02 0.01 0.02 ImageNet MSCOCO
MOADtext MOADimage accuracy R@1
(a) Objectbiasvs.downstreamperformance. (b) Objectvs.attributeperformance.
Fig.6: Object bias and performance on attribute tasks. (a) We find a bias
towards objects (mostly positive MOAD values) but only weak to no correlation with
attribute performance. We attribute this to the observation that (b) performance im-
provementsonobjecttasks(ImageNet,MSCOCO)positivelycorrelatewithattribute
tasks (MIT-States, UT-Zappos).
Takeaway 4: Contrastive vision-language models trained on large-scale
data tend to have a lower object bias than medium-scale models. However,
thereisnoclearrelationbetweenobjectbiasanddownstreamperformance.
This can be attributed to the observation that performance improvements
on object tasks correlate with improvements on attribute tasks.
Where does the object bias come from?Inprinciple,thewordfrequencyof
thetrainingdatasetcouldcausetheobjectbias.However,Fig.7adisprovesthis,
as attributes are more frequent than objects in LAION-2B. Thus, we posit that
the bias arises from the per-sample prevalence of objects in natural language,
e.g., humans tend to describe the most salient object(s) and typically only few
of their attributes in a caption.2 To verify this hypothesis, we used MAD and
redefined the prevalence of the latent factors. Specifically, we trained models in
5 settings. For each setting another factor (e.g. digit, swelling, fracture, etc.)
was always present in the caption while only one of the remaining factors was
randomly sampled for each training sample. Fig. 7b confirms that the caption
presence bias results in a bias towards objects for natural language captions. It
alsoillustratesthatmodelsperformbetterontasksforwhichthe(task-relevant)
information is always present in the captions.
Takeaway 5: Bias towards concepts, e.g., objects, is caused by their high
presence probability in captions if said concept appears in the image.
2 Notallobjectsinascenewillbedescribed,butobjectsaremoreconsistentlypresent
than attributes.
setatS-TIM
soppaZ-TU
ycaruccartta
ycaruccartta
setatS-TIM
soppaZ-TU
ycaruccartta
ycaruccartta12 S. Schrodi et al.
11 00 78 O Atb tj re ibct us tes 0.6 T Imex at ge 1 80 00
106 0.4 60
105 40
0.2
104 20 Always in caption
103 0.0
0
20% in caption
102
0 10 20 30 40 50 60 70 80
Digit Swelli Fn rg actur Se cali Tn hg icknes Cs olors Digit Swelli Fn rg actur Se cali Tn hg icknes Cs olor
Object/Attribute ID Always in caption Classification target
(a) ObjectandattributecountsinLAION-2B. (b) Objectbiasisacaptionpresencebias.
Fig.7:Objectbiasiscausedbyaper-samplecaptionpresencebias.(a)Object
biasisnotcausedbythewordfrequencysinceattributesappearmorefrequentlythan
objects. We used the objects and attributes from Bravo et al. [3] on the LAION-
2B captions [50]. (b) We trained CLIP models on MAD and changed which factor
is always in the caption (i.e., caption presence bias). The resulting models are more
biasedtowardsthefactor(e.g.,color,thickness,...)thatisalwaysinthecaptionduring
training (left) and achieves higher performance on that factor (right). Hence, the bias
towards objects is not caused by salience of the object, but because captioners tend
to name the object but only some of its attributes. The bias is larger for the image
encoder, as it needs to match to the most likely caption, while the text encoder can
encode the entire information less biased, as sketched in Fig. 1 and outlined in Sec. 6.
6 Information Imbalance Triggers Modality Gap and
Object Bias
The previous sections analyzed the differences of image and text embeddings,
particularly the modality gap, and a bias towards objects. However, what is
the underlying cause for their emergence? This section reveals one factor that
creates both phenomena: information imbalance between images and texts due
tosparsecaptions.Thatis,imagescontainalltheinformation,whilecaptionsare
an incomplete description of the images, as captions typically entail the most
salient object(s) and only a handful of other factors, such as attributes. The
information imbalance problem is illustrated in Fig. 1.
As a consequence of information imbalance, the image encoder can hardly
align its embedding of an image to the one of the text encoder of a matching
caption, as it cannot know what latent factors may be encoded in that caption.
To still achieve sufficient alignment, the best both encoders can do is to focus
on the latent factors that are likely present in the caption (e.g., objects), while
neglecting other factors that are unlikely to be present (e.g., attributes). This
resultsintheabovediscussedcaptionpresencebiasand,consequently,leadstoa
biastowardsthemostlikelypresentwords.Innaturallanguagedescriptionsthese
are objects. Moreover, the modelwill maximize theuniformityof the imageand
text embeddings to minimize the total contrastive loss, as there is a shortcut to
quicklyachievethisduringtraining:makeimagesandtextsmaximallydissimilar.
This creates the modality gap (refer to Sec. 4.1 for a thorough explanation).
stnuoC
noitpac
ni
syawlA
)DAOM(
saib
ycaruccATwo Effects, One Trigger 13
(a) Effectofinformationimbalance. (b)UMAPembeddings.
Fig.8: Increasing shared information between modalities improves the rep-
resentations.Tostudytheinfluenceofinformationimbalancebetweenthemodalities,
we control the number of attributes present in the captions (the image is always af-
fected by all attributes) in MAD. As the amount of information shared between the
modalitiesincreases,themodalitygap(I-II)andbiastowardsobjectsreduces(III-IV),
while downstream accuracy improves (V-VI). b: The contrastive loss is able to close
the modality gap given full shared information between modalities, as illustrated by
the UMAP embeddings after model initialization (top) and after training (bottom).
Tovalidateourhypothesis,wevariedtheinformationimbalanceinMADina
controlled setting. Specifically, we varied the number of attributes mentioned in
thecaptionsandensuredthattheobject(i.e.,digitclass)isalwaysincluded;re-
fertoSec.3fordetails.Notethatsettingswithhighinformationimbalance(few
attributesarepresent)arealignedwithnaturallanguagecaptionsthattypically
only entail the most salient object(s) while neglecting most of the attributes.
In contrast, settings with low information imbalance are more aligned with ap-
proaches using enriched captions. Fig. 8 shows the results and indeed validates
our hypothesis.
Information imbalance and modality gap. Fig. 8a(I-II) show that the
modality gap decreases with decreasing information imbalance. Moreover, even
when there is a modality gap after model initialization, the contrastive loss is
capable to substantially reduce it in the full information setting; see Fig. 8b.
Information imbalance and object bias. Fig. 8a(III-IV) show that the bias
towards objects reduces, as information imbalance reduces. The image encoder
isalsomorebiasedtowardsobjectsthanthetextencoder(Fig.6a,7band8a(II-
III)), which is implied by our hypothesis.
Information imbalance and zero-shot performance. Fig. 8a(V-VI) show
that zero-shot digit (object) and attribute performance improve with decreas-
ing information imbalance. This is also supported by the theoretical results of
Daunhaweret al.[15],whoshowedthatlatentfactors,i.e.,objectsorattributes,
can be block-identified if they are shared between the modalities.14 S. Schrodi et al.
Takeaway6: Informationimbalancebetweenthemodalitiesleadstoboth,
modality gap and object bias. Reducing the level of information imbalance
causes a smaller modality gap and a smaller object bias.
Embedding dimensionality. Fig. 8a(V-VI) shows that a higher embedding
dimensionality also improves zero-shot downstream performance, even in the
presence of substantial information imbalance. This suggests that the embed-
ding dimension could also be an effective way to improve zero-shot downstream
performance. We leave further investigation for future work.
7 Discussion
We found that information imbalance between modalities results in poor down-
stream performance for the imbalanced factors, leads to the modality gap as
well as a bias towards the factors that are always present in the captions. In
contrast, when we ensure information balance with all task-relevant informa-
tion, e.g., attributes, contrastive vision-language models achieve strong zero-
shot downstream performance, a negligible modality gap, and less bias towards
objects.
Synthetic vs. real data. We validated our hypothesis on our synthetic MAD
dataset for small contrastive vision-language models. However, there are several
differences between real and synthetic data, e.g., real images have substantially
morelatentfactors(attributes,lighting,relations,etc.).Further,weacknowledge
that models are typically trained in the large-scale setting, i.e., a large model
trained on a large dataset with large amounts of compute. Nonetheless, we are
confident that our findings go beyond the synthetic setting. We find evidence
for that in form of aligned findings of concurrent empirical studies that showed
the positive influence of data quality [44] or caption enrichment [29,66] for per-
formance of contrastive vision-language models. In this work, we studied the
upper bound of data quality and caption enrichment (i.e., all latent factors are
described in the caption) and found it effective to learn better representations.
Beyond contrastive vision-language models. We focused on contrastive
vision-language models due to their popularity. However, our analysis and con-
clusions are not specific to these modalities and generalize to multi-modal mod-
els trained on other input modalities. Another interesting future direction is to
include recent large-scale captioning-based models [16,49,58] to our analysis.
Unfortunately, without publicly accessible weights this is not possible for us.
8 Conclusion
This work investigated contrastive vision-language models to gain a better un-
derstanding of their characteristics. We found that the modality gap and a bias
towardsobjectsarebothtriggeredbyaninformationimbalancebetweenmodal-
ities.Areductionofsuchmitigatesbothandimprovesdownstreamperformance.Two Effects, One Trigger 15
Surprisingly,wealsofoundthatonlyfewembeddingdimensionsdrivethemodal-
ity gap. Besides that, we introduced two novel measures to compare modality
gaps across models as well as quantify the notion of a “bias towards objects”.
Whileweobservedthatalargermodalitygapcorrelateswithbetterdownstream
performance,wefoundthatboththemodalitygapanddownstreamperformance
aresubstantiallymoreinfluencedbyotherfactors,suchasthedatasetquality.Fi-
nally, we confirmed that contrastive vision-language models have a bias towards
objects but also found that improvements on object tasks positively correlate
with improvements on attribute tasks.
Acknowledgments
This research was funded by the Bundesministerium für Umwelt, Naturschutz, nuk-
leareSicherheitundVerbraucherschutz(BMUV,GermanFederalMinistryfortheEn-
vironment, Nature Conservation, Nuclear Safety and Consumer Protection) based on
a resolution of the German Bundestag (67KI2029A), the Deutsche Forschungsgemein-
schaft (DFG, German Research Foundation) under grant number 417962828, and the
Bosch Center for Artificial Intelligence.
References
1. Agarwal, S., Krueger, G., Clark, J., Radford, A., Kim, J.W., Brundage, M.: Eval-
uating CLIP: Towards Characterization of Broader Capabilities and Downstream
Implications. arXiv (2021) 4
2. Alabdulmohsin, I., Zhai, X., Kolesnikov, A., Beyer, L.: Getting ViT in Shape:
Scaling Laws for Compute-Optimal Model Design. In: NeurIPS (2023) 1
3. Bravo,M.A.,Mittal,S.,Ging,S.,Brox,T.:Open-vocabularyAttributeDetection.
In: CVPR (2023) 2, 4, 10, 12
4. Brody,J.:OnthePotentialofCLIPforCompositionalLogicalReasoning.In:ICLP
(2023) 3
5. Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: COYO-700M: Image-
Text Pair Dataset (2022), https://github.com/kakaobrain/coyo-dataset 1
6. Castro, D.C., Tan, J., Kainz, B., Konukoglu, E., Glocker, B.: Morpho-MNIST:
Quantitative Assessment and Diagnostics for Representation Learning. JMLR
(2019) 4, 1
7. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12M: Pushing
Web-ScaleImage-TextPre-TrainingToRecognizeLong-TailVisualConcepts.In:
CVPR (2021) 1
8. Chen, R., Liu, Y., Kong, L., Zhu, X., Ma, Y., Li, Y., Hou, Y., Qiao, Y., Wang,
W.: CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP. In:
CVPR (2023) 1
9. Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D.,
Goodman, S., Grycner, A., Mustafa, B., Beyer, L., Kolesnikov, A., Puigcerver, J.,
Ding, N., Rong, K., Akbari, H., Mishra, G., Xue, L., Thapliyal, A.V., Bradbury,
J.,Kuo,W.,Seyedhosseini,M.,Jia,C.,Ayan,B.K.,Ruiz,C.R.,Steiner,A.P.,An-
gelova,A.,Zhai,X.,Houlsby,N.,Soricut,R.:PaLI:AJointly-ScaledMultilingual
Language-Image Model. In: ICLR (2023) 116 S. Schrodi et al.
10. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.:
Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv (2015)
4, 1
11. Cherti,M.,Beaumont,R.,Wightman,R.,Wortsman,M.,Ilharco,G.,Gordon,C.,
Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive
language-image learning. In: CVPR (2023) 1, 4, 8
12. Chopra,S.,Hadsell,R.,LeCun,Y.:LearningaSimilarityMetricDiscriminatively,
with Application to Face Verification. In: CVPR (2005) 4
13. Couairon, G., Douze, M., Cord, M., Schwenk, H.: Embedding Arithmetic of Mul-
timodal Queries for Image Retrieval. In: CVPR (2022) 3
14. Crabbé,J.,Rodríguez,P.,Shankar,V.,Zappella,L.,Blaas,A.:Robustmultimodal
models have outlier features and encode more concepts. arXiv (2023) 3
15. Daunhawer,I.,Bizeul,A.,Palumbo,E.,Marx,A.,Vogt,J.E.:IdentifiabilityResults
for Multimodal Contrastive Learning. In: ICLR (2023) 4, 13
16. Desai, K., Johnson, J.: VirTex: Learning Visual Representations from Textual
Annotations. In: CVPR (2021) 1, 14
17. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An Image is Worth
16x16 Words: Transformers for Image Recognition at Scale. In: ICLR (2020) 1
18. Gadre, S.Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten,
R., Wortsman, M., Ghosh, D., Zhang, J., et al.: DataComp: In search of the next
generation of multimodal datasets. In: Datasets and Benchmarks Track@NeurIPS
(2023) 1
19. Geirhos,R.,Narayanappa,K.,Mitzkus,B.,Thieringer,T.,Bethge,M.,Wichmann,
F.A.,Brendel,W.:Partialsuccessinclosingthegapbetweenhumanandmachine
vision. In: NeurIPS (2021) 3
20. Goh, G., Cammarata, N., Voss, C., Carter, S., Petrov, M., Schubert, L., Radford,
A., Olah, C.: Multimodal Neurons in Artificial Neural Networks. Distill (2021) 3
21. Gutmann,M.,Hyvärinen,A.:Noise-contrastiveestimation:Anewestimationprin-
ciple for unnormalized statistical models. In: AISTATS (2010) 4
22. Hamidieh,K.,Zhang,H.,Hartvigsen,T.,Ghassemi,M.:IdentifyingImplicitSocial
Biases in Vision-Language Models. Workshop@ICLR (2023) 4
23. He,K.,Zhang,X.,Ren,S.,Sun,J.:DeepResidualLearningforImageRecognition.
In: CVPR (2016) 1
24. Hoffmann,D.T.,Behrmann,N.,Gall,J.,Brox,T.,Noroozi,M.:RankingInfoNoise
Contrastive Estimation: Boosting Contrastive Learning via Ranked Positives. In:
AAAI (2022) 9
25. Ilharco,G.,Wortsman,M.,Wightman,R.,Gordon,C.,Carlini,N.,Taori,R.,Dave,
A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt,
L.: OpenCLIP (2021). https://doi.org/10.5281/zenodo.5143773 4, 8, 1
26. Isola, P., Lim, J.J., Adelson, E.H.: Discovering States and Transformations in Im-
age Collections. In: CVPR (2015) 4, 1
27. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,
Li,Z.,Duerig,T.:ScalingUpVisualandVision-LanguageRepresentationLearning
With Noisy Text Supervision. In: ICML (2021) 1, 3
28. Krizhevsky,A.,Hinton,G.,etal.:LearningMultipleLayersofFeaturesfromTiny
Images (2009) 9
29. Lai,Z.,Zhang,H.,Wu,W.,Bai,H.,Timofeev,A.,Du,X.,Gan,Z.,Shan,J.,Chuah,
C.N., Yang, Y., et al.: From Scarcity to Efficiency: Improving CLIP Training via
Visual-enriched Captions. arXiv (2023) 14Two Effects, One Trigger 17
30. LeCun, Y.: The MNIST database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ (1998) 4, 1
31. Li,X.,Wang,Z.,Xie,C.:CLIPA-v2:ScalingCLIPTrainingwith81.1%Zero-shot
ImageNet Accuracy within a $10,000 Budget; An Extra $4,000 Unlocks 81.8%
Accuracy. Workshop@NeurIPS (2023) 1
32. Li, X., Wang, Z., Xie, C.: Grounding Visual Illusions in Language: Do Vision-
Language Models Perceive Illusions Like Humans? In: EMLNP (2023) 3
33. Li,X.,Wang,Z.,Xie,C.:AnInverseScalingLawforCLIPTraining.In:NeurIPS
(2023) 1
34. Li, Y., Fan, H., Hu, R., Feichtenhofer, C., He, K.: Scaling Language-Image Pre-
training via Masking. In: CVPR (2023) 1
35. Liang,P.P.,Deng,Z.,Ma,M.,Zou,J.,Morency,L.P.,Salakhutdinov,R.:Factorized
Contrastive Learning: Going Beyond Multi-view Redundancy. In: NeurIPS (2023)
4
36. Liang, V.W., Zhang, Y., Kwon, Y., Yeung, S., Zou, J.Y.: Mind the Gap: Under-
standing the Modality Gap in Multi-modal Contrastive Representation Learning.
In: NeurIPS (2022) 2, 3, 5, 7, 8
37. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick,C.L.:MicrosoftCOCO:CommonObjectsinContext.In:ECCV(2014) 4,
1
38. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A ConvNet for
the 2020s. In: CVPR (2022) 1
39. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: ICLR
(2019) 3
40. Ma, Y., Xu, G., Sun, X., Yan, M., Zhang, J., Ji, R.: X-CLIP: End-to-End Multi-
grained Contrastive Learning for Video-Text Retrieval. In: International Confer-
ence on Multimedia (2022) 1
41. Materzyńska,J.,Torralba,A.,Bau,D.:Disentanglingvisualandwrittenconcepts
in CLIP. In: CVPR (2022) 3
42. Mayilvahanan,P.,Wiedemer,T.,Rusak,E.,Bethge,M.,Brendel,W.:DoesCLIP’s
Generalization Performance Mainly Stem from High Train-Test Similarity? In:
ICLR (2024) 3
43. Menon,S.,Vondrick,C.:VisualClassificationviaDescriptionfromLargeLanguage
Models. In: ICLR (2023) 1
44. Nguyen,T.,Ilharco,G.,Wortsman,M.,Oh,S.,Schmidt,L.:QualityNotQuantity:
OntheInteractionbetweenDatasetDesignandRobustnessofCLIP.In:NeurIPS
(2022) 1, 3, 14
45. Oord, A.v.d., Li, Y., Vinyals, O.: Representation Learning with Contrastive Pre-
dictive Coding. arXiv (2018) 4, 7
46. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning Transferable Visual Models
From Natural Language Supervision. In: ICML (2021) 1, 4
47. Rashtchian,C.,Herrmann,C.,Ferng,C.S.,Chakrabarti,A.,Krishnan,D.,Sun,D.,
Juan,D.C.,Tomkins,A.:SubstanceorStyle:WhatDoesYourImageEmbedding
Know? arXiv (2023) 3
48. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy,A.,Khosla,A.,Bernstein,M.,etal.:ImageNetLargeScaleVisualRecog-
nition Challenge. IJCV (2015) 4, 1
49. Sariyildiz,M.B.,Perez,J.,Larlus,D.:LearningVisualRepresentationswithCap-
tion Annotations. In: ECCV (2020) 1, 1418 S. Schrodi et al.
50. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.W., Wightman, R., Cherti,
M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kun-
durthy, S.R., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: LAION-5B:
An open large-scale dataset for training next generation image-text models. In:
Datasets and Benchmarks Track@NeurIPS (2022) 12, 1
51. Shi,P.,Welle,M.C.,Björkman,M.,Kragic,D.:Towardsunderstandingthemodal-
ity gap in CLIP. In: Workshop@ICLR (2023) 2, 3
52. Shtedritski, A., Rupprecht, C., Vedaldi, A.: What does CLIP know about a red
circle? Visual prompt engineering for VLMs. In: ICCV (2023) 4
53. So,J.,Oh,C.,Lim,Y.,Byun,H.,Shin,M.,Song,K.:GeodesicMulti-ModalMixup
for Robust Fine-Tuning. In: NeurIPS (2023) 7
54. Sohn,K.:ImprovedDeepMetricLearningwithMulti-classN-pairLossObjective.
In: NeurIPS (2016) 4
55. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: EVA-CLIP: Improved Training
Techniques for CLIP at Scale. arXiv (2023) 1
56. Thomee,B.,Shamma,D.A.,Friedland,G.,Elizalde,B.,Ni,K.,Poland,D.,Borth,
D.,Li,L.J.:YFCC100M:TheNewDatainMultimediaResearch.Communications
of the ACM (2016) 1
57. Trager, M., Perera, P., Zancato, L., Achille, A., Bhatia, P., Soatto, S.: Linear
Spaces of Meanings: Compositional Structures in Vision-Language Models. In:
ICCV (2023) 3, 9, 1
58. Tschannen, M., Kumar, M., Steiner, A., Zhai, X., Houlsby, N., Beyer, L.: Image
Captioners Are Scalable Vision Learners Too. In: NeurIPS (2023) 1, 14
59. Udandarao, V.: Understanding and Fixing the Modality Gap in Vision-Language
Models (2022), Master’s thesis 2, 3
60. Visheratin, A.: NLLB-CLIP–train performant multilingual image retrieval model
on a budget. arXiv (2023) 1
61. Von Kügelgen, J., Sharma, Y., Gresele, L., Brendel, W., Schölkopf, B., Besserve,
M., Locatello, F.: Self-Supervised Learning with Data Augmentations Provably
Isolates Content from Style. In: NeurIPS (2021) 4
62. Wang, T., Isola, P.: Understanding Contrastive Representation Learning through
Alignment and Uniformity on the Hypersphere. In: ICML (2020) 4
63. Wu, C., Maji, S.: How well does CLIP understand texture? Workshop@ECCV
(2022) 4
64. Xu,H.,Xie,S.,Tan,X.E.,Huang,P.Y.,Howes,R.,Sharma,V.,Li,S.W.,Ghosh,
G., Zettlemoyer, L., Feichtenhofer, C.: Demystifying CLIP Data. In: ICLR (2024)
3, 1
65. Yamada,Y.,Tang,Y.,Yildirim,I.:WhenareLemonsPurple?TheConceptAsso-
ciation Bias of CLIP. In: EMNLP (2023) 4
66. Yao, L., Chen, W., Jin, Q.: CapEnrich: Enriching Caption Semantics for Web
Images via Cross-modal Pre-trained Knowledge. In: WWW (2023) 14
67. Yu, A., Grauman, K.: Fine-Grained Visual Comparisons with Local Learning. In:
CVPR (2014) 4, 1
68. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.: CoCa:
Contrastive Captioners are Image-Text Foundation Models. TMLR (2022) 1
69. Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., Zou, J.: When and Why
Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?
In: ICLR (2022) 3
70. Zhai,X.,Mustafa,B.,Kolesnikov,A.,Beyer,L.:SigmoidLossforLanguageImage
Pre-Training. In: ICCV (2023) 1, 4Two Effects, One Trigger 19
71. Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., Beyer,
L.: LiT: Zero-Shot Transfer with Locked-image text Tuning. In: CVPR (2022) 1
72. Zhang,R.,Zeng,Z.,Guo,Z., Li,Y.:CanLanguageUnderstandDepth?In:Inter-
national Conference on Multimedia (2022) 4
73. Zhang,Y.,HaoChen,J.Z.,Huang,S.C.,Wang,K.C.,Zou,J.,Yeung,S.:Diagnosing
and Rectifying Vision Models using Language. In: ICLR (2023) 3, 7
74. Zhou,C.,Zhong,F.,Öztireli,C.:CLIP-PAE:Projection-AugmentationEmbedding
to Extract Relevant Features for a Disentangled, Interpretable and Controllable
Text-Guided Face Manipulation. In: SIGGRAPH (2023) 7Two Effects, One Trigger 1
baseimage
class thickness swelling fractures scaling color
MADimage
Fig.9: Causal graph of MAD.
A Evaluation Details
We ran our evaluations on ImageNet [48], MS COCO [10,37], MIT-States [26],
and UT-Zappos [67]. The datasets comprise 50000, 25000 (5000 images with
5 captions each), 12995, or 2914 test samples, respectively. ImageNet and MS
COCO are standard datasets for evaluation of object recognition or retrieval
performance,respectively.Weusedthestandardevaluationprotocolstocompute
accuracyorimageretrievalperformance.MIT-Statesconsistsof245objectsand
115 adjectives (attributes), while UT-Zappos consists of 12 shoe types with 16
fine-grained states (∼ attributes). For both datasets, we assume that we do not
knowtheobjectofarespectiveimageandonlywanttofindtheadjectiveorfine-
grained state. We considered this a classification problem, following previous
work [57]. Note that these datasets implicitly assume that the adjectives are
mutually exclusive per image. However, this may not be necessarily true, as
multiple adjectives or fine-grained states may be present in the image.
Contrastive vision-language model details. For our large-scale analyses,
we used a total of 112 contrastive vision-language models trained across var-
ious datasets provided by OpenCLIP [11,25]3. It contains contrastive vision-
language models, such as OpenAI’s CLIP [46], CLIP-A [33], EVA-CLIP [55],
CoCa [68], NLLB-CLIP [60], or SigLIP [70]. Note that these models use vari-
ous backbones, including ResNet [23], ConvNeXt [38], or ViT [17]. The models
weretrainedon,e.g.,OpenAI’sproprietary(400M)WebImageTextdataset[46],
LAION-400M, LAION-2B, LAION-5B [50], Merged-2B (merge of 1.6B sam-
ples from LAION-2B and 0.4B samples from COYO-700M [5]) [55], WebLI [9],
So-400M [2], MetaCLIP (400M) [64], Conceptual 12M [7], YFCC (15M) [56],
CommonPool-s (max. 12.8M; refer to Table 3 of Gadre et al. [18] for the de-
tails of filtering), CommonPool-m (max. 128M), CommonPool-l (max. 1.28B),
CommonPool-xl (max. 12.8B) [18], or DataPool-s (1.4M), DataPool-m (14M),
DataPool-l (140M), DataPool-xl (1B) [18].
Multi-modal Attributes and Digits (MAD). Our dataset Multi-modal
Attributes and Digits (MAD) is based on the MNIST [30] variation Morpho-
MNIST [6]. The causal graph of the data-generating process of MAD is de-
picted in Fig. 9. We used the following words for digits (0, ..., 9), altering im-
3 https://github.com/mlfoundations/open_clip2 S. Schrodi et al.
4 0 5 8 8 9
no thickthinning thickening thickening no thickthinning thickening no thickthinning
no swelling no swelling no swelling swelling swelling no swelling
fracture fracture fracture no fracture no fracture no fracture
small large small small large large
gray magenta gray magenta magenta magenta
1 0 3 7 5 9
thickening thickening thinning thickening no thickthinning thickening
no swelling swelling swelling swelling swelling swelling
fracture fracture fracture fracture no fracture no fracture
small small small small small small
gray green red green red yellow
5 2 2 8 6 8
no thickthinning no thickthinning thickening thinning no thickthinning thickening
no swelling no swelling swelling no swelling swelling no swelling
no fracture fracture no fracture no fracture no fracture no fracture
large large small small small large
magenta blue green green red red
9 6 4 2 0 8
thickening no thickthinning thickening thickening no thickthinning thickening
swelling no swelling swelling swelling no swelling swelling
fracture no fracture fracture fracture no fracture no fracture
small large small large small small
red magenta red blue magenta red
Fig.10: Example images with corresponding caption of our MAD dataset. Note that
thewordsofthecaptionsareshuffledduringtraining.Forexample,theinthefirstrow
and first column shows the digit 4 without altering the thickness, no swelling applied,
with fracture augmentation, scaled down and the color gray.
agethickness(thickening,thinning,no thickthinning),swelling(swelling,
no swelling),fractures(fracture,no fracture),scaling(large,small),and
color (gray, red, green, blue, cyan, magenta, yellow). Thus, we have 16 dif-
ferent attributes. Fig. 10 provides examples of image-text pairs of MAD.Two Effects, One Trigger 3
In our experiments, we investigated information imbalance in the captions
by restricting the number of attributes present within each caption. We provide
examplesbelow,wherewesequentiallyremovetheamountofinformationwithin
the captions, i.e., fewer latent factors (attributes) are present in the caption:
– Full information setting (i.e., digit & all five attributes)
• yellow-swelling-thickening-9-large-fracture
• swelling-thickening-6-red-small-fracture
• 5-large-yellow-no swelling-fracture-thinning
– Partial information setting I (i.e., digit & four attributes)
• yellow-swelling-thickening-9-large
• swelling-thickening-6-red-small
• 5-large-yellow-no swelling-fracture
– Partial information setting II (i.e., digit & three attributes)
• yellow-swelling-thickening-9
• swelling-thickening-6-red
• 5-large-yellow-no swelling
– Partial information setting III (i.e., digit & two attributes)
• yellow-swelling-9
• swelling-thickening-6
• 5-large-yellow
– Partial information setting IV (i.e., digit & one attributes)
• yellow-9
• swelling-6
• 5-large
Notethatwhileallthelatentfactors,i.e.,digitandallfiveattributes,stillaffect
the generated image, the caption may only provide partial information, i.e.,
attributes are missing from the caption.
B Model and Training Details for the Experiments on
Multi-modal Attributes and Digits
Model details. We used small CLIP models. Specifically, the ViT-based vision
backbonecomprises6layers,eachwithadimensionalitydof256and⌊d/64⌋=4
heads. The transformer-based language backbone also comprises 6 layers, each
withadimensionalityof256and8heads.Wesetthepatchsizeto7andcontext
length to 8. The vocabulary consists of 28 words, i.e., all the words for digits
(10) and attributes (16), as well as a start and end symbol (2).
Training details.Wetrainedallmodelswithabatchsizeof128for200epochs
with a learning rate warm-up period of 5 epochs. We used AdamW [39] as opti-
mizer with cosine annealing learning rate schedule [39]. We always selected the
best performing learning rate across 3 learning rates {5·10−4, 5·10−5, 10−5}
each trained with 3 random seeds. The best learning rate was selected by com-
paring average accuracies over the ideal word accuracy and average zero-shot
accuracy on all attributes and the class label. For all of our results, we report
the average over 3 random seeds.4 S. Schrodi et al.
C Alignment and Uniformity Terms in InfoNCE
Contrastiverepresentationlearning[12,21,45,54]leveragespairedinputsasweak
supervisionsignal.Thebasicideaistolearnrepresentationsinasharedrepresen-
tation space that are as similar as possible for “positive/matching” pairs, while
asdissimilaraspossiblefor“negative/non-matching” pairs.Apopularchoicefor
contrastive learning approaches is the InfoNCE objective [45]:
exp(f (x )Tf (y )/τ)
L(f ,f ):= E [−log x i y i ],
x y
(xi,yi) ∼pdata
exp(f (x )Tf (y
)/τ)+N(cid:80)−1
exp(f (x )Tf (y )/τ)
x i y i x i y j
j=1
(4)
wheref ,f aretwoencodersfortheinputsx,y,τ isthescalartemperature,and
x y
p isthedatadistributionRd×Rd.Wang&Isola[62]definetwocomponents
data
of the loss:
– Alignment (nominator): matching pairs should be close, i.e., aligned.
– Uniformity (denominator):representationsshouldberoughlyuniformlydis-
tributed on the unit hypersphere.
For multi-modal contrastive representation learning, it is popular to use a
symmetric version of above InfoNCE objective [46]:
1 1
L = L(f ,f )+ L(f ,f ) . (5)
sym 2 x y 2 y x
Note that the the repulsive forces of the uniformity term only act across the
modalities but not within them.
Mismatches Can Cause Global Optima Exhibiting a Modality Gap
Goingbeyondthediscussioninthemaintext,weillustratebelowthatthemodal-
itygapcanevenbepresentintheglobaloptimumundercertaindataproperties.
Under perfect image-text pairs, it is easy to see that the global minimum of the
contrastive loss does not exhibit a modality gap. I.e., all matching image-text
pairs are perfectly aligned and the pairs are uniformly distributed on the unit
hypersphere. But what happens in the presence of mismatches caused by, e.g.,
miscaptioning?
To illustrate, the effect of such mismatches on the final image and text
embeddings, we designed a 2D toy example. We generated two sets of points
on the unit circle and directly optimized their positions. The points represent
the embeddings of both modalities: {x ,x } = X and {y ,y } = Y, where
1 2 1 2
x ,y ,x ,y ∈{[a,b]T |a2+b2 =1}.Further,wespecifiedwhichpointofsetX
1 1 2 2
matchestowhichpointinsetY.Fortheperfectmatchingsetting,weconsidered
the following matching pairs:
{(x ,y ),(x ,y )} . (6)
1 1 2 2Two Effects, One Trigger 5
Perfectpairs Imperfectpairs
1.0
0.5
x1
0.0
y1
x2
y2
0.5
−
1.0
−
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
− − − −
dim1 dim1
Fig.11:Visualizationoftheglobalminimaofthecontrastivelossunderperfect
or imperfect pairs in our toy 2D example. For perfect pairs (left), matching pairs are
aligned and uniformly distributed. For imperfect pairs (right), the global minimum
exhibits a modality gap. Here, x denotes sample i from modality 1, e.g., images, and
i
y sample i from modality 2, e.g., text.
i
It is easy to see that the global minimum (up to rotations) is: x =y =[1,0]T,
1 1
x =y =[−1,0]T; see Fig. 11 for a visualization.
2 2
However, what happens when we introduce mismatches? For image-text
pairs, this can happen if a human annotator miscaptions an image. It could
also stem from differing focuses among human annotators on distinct aspects of
the image. We considered the following matching pairs:4
{(x ,y ),(x ,y ),(x ,y ),(x ,y ),(x ,y ),(x ,y )} . (7)
1 1 1 1 2 2 2 2 1 2 2 2
(cid:124) (cid:123)(cid:122) (cid:125)
mismatches
Werecognizethatidentificationandremovalofabovemismatchesmaybesimple
in practice. However, our goal is to illustrate the impact of mismatches in a
simplistic setting to provide an intuition on the behavior of the contrastive loss.
TosearchforthegloballyoptimalembeddingsforEq.(7),weranagridsearch
withanangularresolutionof6◦.Wefoundthefollowingglobalminimum(again
up to rotations): x =[1,0]T, y =[cos276◦,sin276◦]T, x =[cos42◦,sin42◦]T,
1 1 2
y =[cos126◦,sin126◦]T; see Fig. 11 for a visualization. It is apparent that the
2
global minimum exhibits a modality gap.
D Extended details for Sec. 4
D.1 Relative Modality Gap
LimitationsofL2M.L2Mhasbeeninitiallyproposedasmodalitygapdistance
byLiangetal.[36].However,ithasseverallimitationsthatwewilldiscussbelow:
4 We need the additional matching pairs of (x ,y ) and (x ,y ) to avoid the degen-
1 1 2 2
erated global minimum x =y =x =y .
1 1 2 2
2mid6 S. Schrodi et al.
1. L2M does not account for difference of the effectively used hyperspaces.
2. L2M takes a distributional instead of a per-sample view.
3. The L2 norm can be sensitive to outlier embedding dimensions.
Regardingthefirstpoint:notethatdifferentmodelscanusedifferentamounts
of the unit hypersphere, as the contrastive loss accounts for relative cosine sim-
ilarities. Here, the similarity is always relative to the similarity to the negative
samples.Consequently,modelscanuse avarying degreeofthe unithypersphere
and, thus, L2 distances can have a different meaning. For example, consider two
models that have the same L2M but the first model uses the entire unit hy-
persphere, while the second model only uses a small fraction of it. While L2M
suggests that the modality gap distance is the same, the actual gap of the first
model is significantly smaller, since the average distances between the samples
are larger.
Regarding the second point: intuition suggests that matching image-text
pairs should be close but non-matching pairs can (and should) be large. L2M
considers the distance of the means of all pairs. This can lead to misleading
results. An illustrative (but unlikely) example is the case of two distributions
that occupy exactly the same region of the hypersphere, but are rotated by n-
degree (i.e., they are very misaligned). Clearly, there exists a gap between the
modalities but L2M does not indicate it.
Last,theL2normcanbesensitivetoembeddingdimensionsthatexhibitvast
differences. For instance, our discovered most modality-separating embedding
dimensions qualify for this.
Relative Modality Gap (RMG).Asaremedytoaboveoutlinedlimitations,
we proposed a Relative Modality Gap (RMG) measure in the main text. RMG
computesthedistancesbetweenmatchingimage-textpairsinsteadofthemeans
to address 2. Since density estimation in high-dimensional spaces is difficult, we
used the mean distances between all samples per modality as rough approxima-
tion to address 1. Finally, we used cosine similarities instead of the L2 norm to
address 3.
D.2 Rank Correlations when Fixing the Datasets
We also computed rank correlation for single datasets. We applied the following
filtering criteria:
1. We removed models that were subsequently finetuned, e.g., on MS COCO.
2. We removed multiple instances of the same model due varying training
epochs,activations(GeLUvs.Quick-GeLU),ortrainingprotocol(e.g.,aug-
mentations).Weusedthemodelsthatweretrainedlonger.Forvaryingacti-
vations or training protocols, we used the model that achieved better image
recognition performance on ImageNet.
We only considered datasets that have at least seven models after filtering
(LAION-400M: 7, LAION-2B: 14, OpenAI’s CLIP dataset: 9, WebLI: 9). Note
thatthesmallnumberofmodelsmakestherankcorrelationssusceptibletonoise
and they need to be interpreted with caution.