Rate-Optimal Non-Asymptotics for the
Quadratic Prediction Error Method
Charis Stamouli∗, Ingvar Ziemann∗, and George J. Pappas∗
Abstract
We study the quadratic prediction error method—i.e., nonlinear least squares—for a class
oftime-varyingparametricpredictormodels satisfyinga certainidentifiability condition. While
this method is known to asymptotically achieve the optimal rate for a wide range of problems,
there have been no non-asymptotic results matching these optimal rates outside of a selectfew,
typically linear, model classes. By leveraging modern tools from learning with dependent data,
we provide the first rate-optimal non-asymptotic analysis of this method for our more general
setting of nonlinearly parametrized model classes. Moreover, we show that our results can be
applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models,
resulting in the first optimal non-asymptotic rates for identification of ARMA models.
1 Introduction
Identifying predictive models from data is of critical importance in a wide range of fields, from
classical control theory and signal processing to modern machine learning. To this end, a significant
line of work in system identification has been devoted to identifying predictor models of the form:
Y = f (X ,θ )+W , (1)
t t t ⋆ t
fromsequentialdata(X ,Y ),...,(X ,Y ). WetypicallyrefertothevariablesX astheinputs
0 0 T 1 T 1 t
− −
and the variables Y as the outputs, with the inputs allowed to have a causal dependence on past
t
outputs. However, we do not restrict attention to input-output models in the sense that (1) may
well be autonomous, cf. (2) below.
Assuming that the regression functions f (, ) are known, a standard approach for estimating
t
· ·
the unknown parameter θ is to minimize the quadratic criterion:
⋆
T 1
1 −
L (θ):= (f (X ,θ) Y )2
T t t t
T −
t=0
X
over a parameter class M which is assumed to contain θ . This approach yields the quadratic
⋆
prediction error method, also referred to as nonlinear least squares.
∗
TheauthorsarewiththeDepartmentofElectricalandSystemsEngineering,UniversityofPennsylvania,Philadel-
phia, PA 19104, USA.Emails: {stamouli,ingvarz,pappasg}@seas.upenn.edu.
1
4202
rpA
11
]TS.htam[
1v73970.4042:viXraAs a motivating example, consider the classical prediction error method for AutoRegressive
Moving Average (ARMA) models of the form:
p q
Y = a⋆Y + b⋆W (2)
t i t i j t j
− −
i=1 j=0
X X
from system identification [1,2]. Such models can be cast in the form (1) with parameter θ :=
⋆
a⋆,...,a⋆,b⋆,...,b⋆ ⊺ and inputs X := Y ,...,Y ⊺ . To convert (2) to the form (1), one selects
1 p 0 q t 0 t 1
−
f (, ) to be the conditional expectation of Y given all the past data Y ,...,Y . We return to this
t t 0 t 1
(cid:2) · · (cid:3) (cid:2) (cid:3) −
example in more detail in Section 5.
While the asymptotic rates of prediction error methods are by now well understood—including
optimal rates of convergence [1] as characterized by the Cramér-Rao Inequality—relatively less is
known about their non-asymptotic counterparts. Some early progress on extending these ideas to
the finite-sample regime was made in [3]. However, the bounds therein are both qualitatively and
quantitatively loose as compared to older asymptotic results.
A few years ago, drawing upon recent advances in high-dimensional statistics and probability
[4,5], non-asymptotic rates nearly as sharp as the older known asymptotics were derived for the
particular case of fully observed ARMA models, given by Y = a⋆Y +W [6,7]. Soon thereafter,
t 1 t 1 t
−
classical subspace methods from system identification, based on higher-order linear autoregressions
[8–10], were also given a refined non-asymptotic analysis [11]. Note that in contrast to the general
prediction error method, the algorithms in [6,7,11] are based on linear least squares. For a broader
overview of recent results on non-asymptotic learning and identification of linear models, refer
to [12,13].
Asforlearningandidentificationofnonlinearmodelsoftheform(1),progressonnon-asymptotic
analysis has proven somewhat slower. The special case of a generalized linear model (i.e., a first-
order linear autoregression composed with a static known nonlinearity) is analyzed in [14,15]. At a
technicallevel,thegoalhasprimarilybeentosidestep—as muchaspossible—theblockingtechnique
[16], which has otherwise been a dominant approach to deriving non-asymptotic guarantees for
learning with dependent data [17–21]. In brief, the blocking technique splits a dependent sample
Z T 1 into independent blocks, say Z k , Z 2k ,..., and then proceeds to treat each block
{ t }t=−0 { t }t=1 { t }k+1
as an independent datapoint. The caveat of this technique is that it reduces the effective sample
size (e.g., here by a factor of k) and thus typically does not yield optimal rates of convergence. To
provide some intuition, k above can be thought of as an analogue to the inverse stability margin of
a linear system, and in fact, the blocking technique cannot be applied to marginally stable linear
autoregressions. By contrast, an optimal asymptotic characterization of the rate of convergence
for such autoregressions has been known since 1943 [22]. Moreover, note that sidestepping this
approach is precisely what allowed [6] to first derive optimal rates for linear system identification.
More recently, [23] showed how to, at least partially, avoid the blocking approach for the time-
invariant version of (1)—with f (, ) = f(, ) for a fixed function f(, ) independent of time. The
t
· · · · · ·
result of [23] is almost sufficient to provide a rate-optimal non-asymptotic analysis of the ARMA
prediction error method. However, it has two shortcomings for this purpose, one of which we have
already hinted at. First, the result does not allow for time-varying regression functions f (, ),
t
· ·
which is crucial, as the conditional expectation function of Y given the past data Y ,...,Y
t 0 t 1
−
generally varies in time. Second, the final bounds in [23] are loose by logarithmic factors in problem
quantities (including dimensional factors and the time horizon T) and hence cannot match known
2asymptotics [1,24] even up to constant factors. For the case of time-invariant regression functions,
the authors in [25] removed these logarithmic factors via a mixed-tail generic chaining argument.
In this paper, we pursue a simpler approach and provide the first rate-optimal non-asymptotic
prediction error bound for a relatively general class of time-varying parametric predictor models.
Our model class is rich enough to allow for ARMA models of the form (2) that satisfy a certain
identifiability condition. Similar to [23], our approach is based on the martingale offset complexity
introduced to the statistical literature by [26]. We arrive at our result by providing a refined
analysis of this complexity notion for models of the form(1). An informal version of ourmain result
is presented next.
Informal Version of Theorem 1. Given data from a sufficiently stable system, for a wide range
of identifiable models f (,θ ), the mean-squared prediction error corresponding to any least-squares
t ⋆
·
estimate θ
∈
argmin
θ
∈ML T(θ) satisfies:
parameter dimension noise level
b
Mean-Squared Prediction Error(θ) × +higher-order terms.
≤ number of samples
b
The above statistical rate matches known asymptotics [1,24] up to constant factors and higher-
order terms that become negligible for a large enough sample size T. The requirement that T is
larger than a so-called burn-in time is necessary to establish several components of our result, such
as persistence of excitation. We note that the stability properties of the model, which are measured
via the stochastic dependency of the input process X T 1, affects only the burn-in time of our
{
t }t=−0
result.
In the next section, we formally present our mathematical assumptions and the problem formu-
lation. In Section 3, we introduce our main result, a proof sketch of which is given in Section 4. In
Section 5, we apply our main result to scalar ARMA models. Full proofs of all components of the
main theorem’s proof can be found in the Appendix.
Notation. The norm is the Euclidean norm whenever it is applied to vectors and the spectral
k·k
norm whenever it is applied to matrices. Moreover, Sd 1 denotes the unit sphere in Rd, and Bd
− r
the Euclidean ball of radius r in Rd. We use I to denote the identity matrix of size d and tr(A)
d
to denote the trace of any square matrix A Rd d. Expectation and probability with respect
×
∈
to all the randomness of the underlying probability space are denoted by E and P, respectively.
Expectation with respect to a random variable X is denoted by E . Conditional expectation of a
X
random variable X with respect to an event and a σ-field is denoted by E[X ] and E[X ],
E F |E |F
respectively. For any event , we define 1 as the indicator function of , which takes value 1 when
E E E
the event occurs and 0 otherwise. If g(), h() are functions defined on some unbounded subset of
· ·
the positive real numbers and h(x) is strictly positive for all large enough values of x, we write
g = (h) if there exists x R such that limsup g(x)/h(x) < .
O 0 ∈ x →x0| | ∞
2 Problem Formulation
Consider the predictor model (1), where the input variables X take values in X Rdx, whereas the
t
⊂
output and noise variables, denoted by Y and W , respectively, take values in R. For each t, the
t t
regression function f
t
: Rdx Rdθ R is known and depends on the input X
t
and the unknown
× →
parameter θ . The parameter θ is assumed to belong to some known and compact parameter class
⋆ ⋆
M Bdθ , where B is a positive constant.
⊆ Bθ θ
3Before formalizing our problem, we introduce a few further assumptions about model (1) and
the parameter class M. We start by characterizing the stochastic dependency of the input process
X T 1, which can be thought of as a measure of the stability of model (1). Let us first state the
{
t }t=−0
main definition we will need for this characterization.
Definition 1 (Dependency Matrix). [27, Section 2] Let Z T 1 be a stochastic process with
{
t }t=−0
joint distribution PZ. For each pair (i,j), let P
Zi:j
denote the joint distribution of {Z
t
}j
t=i
and
:= σ(Z ,...,Z ) the σ-algebra generated by Z j . The dependency matrix of Z T 1 is the
Zij i j { t }t=i { t }t=−0
matrix Γ dep(PZ):= {Γ
ij
}T i,j−=1
0 ∈
RT ×T, where:
Γ = 2 sup P (B A) P (B) ,
ij Zj:T−1
| −
Zj:T−1
s BA ∈Z∈Z j:T0: −i 1(cid:12)
(cid:12)
(cid:12)
(cid:12)
for i < j, Γ = 1, and Γ = 0, for i > j.
ii ij
Let PX denote the joint distribution of the input process {X
t
}T t=−01. We can measure the de-
pendency of {X
t
}T t=−01 via the norm kΓ dep(PX)
k
of its dependency matrix. Notice that Γ dep(PX)
always satisfies 1 Γ dep(PX) cT, for some c> 0. The lower bound of Γ dep(PX) corresponds
≤ k k ≤ k k
to independent input processes, whereas the upper bound corresponds to fully dependent input
processes (i.e., processes with X = X , for all t = 0,...,T 2). Our results apply to processes
t t+1
−
for which Γ dep(PX) 2 grows sublinearly in T, as formalized in the following assumption.
k k
Assumption 1. There exist b
1
> 0 and b
2
[0,1) such that Γ dep(PX) 2 b 1Tb2.
∈ k k ≤
Assumption 1 holds for a large family of input processes X T 1 including, e.g., geometrically
{
t }t=−0
φ-mixing processes [27], processes that satisfy Doeblin’s condition [27,28], and stationary time-
homogeneous Markov chains [23] (see [23] for details). In the context of stable linear dynamical
systems with bounded noise, it has been shown that the spectral norm of the dependency matrix
Γ dep(PX) is uniformly bounded (i.e., b
2
= 0) [23], which implies an intuitive connection between
stability and dependency in the process X T 1.
{
t }t=−0
Assumption 2. For each t, let := σ(X ,...,X , W ,...,W ) be the σ-field generated by
t 0 t+1 0 t
F
the inputs X ,...,X and the noise variables W ,...,W . For every t, the noise variable W is
0 t+1 0 t t
σ2-conditionally sub-Gaussian with respect to , that is:
w Ft −1
E[eλWt
t
1]
eλ2 2σw2
,
|F− ≤
for all λ R, for some σ > 0.
w
∈
Assumption 2 is satisfied if the noise variables W are i.i.d. zero-mean Gaussian with variance
t
σ2 and independent of the inputs X ,...,X . In addition, it is satisfied by a large number of
w 0 t
non-Gaussian random variables W [5]; it is also standard in prior work [23,29,30].
t
Assumption 3. For each t, the regression function f (, ) is twice differentiable with respect to its
t
· ·
second argument. Moreover, there exist L ,L > 0 such that the partial gradients f (, ) and the
1 2 θ t
∇ · ·
partial Hessians 2f (, ) satisfy f (x,θ) L and 2f (x,θ) L , respectively, for all
θ t θ t 1 θ t 2
∇ · · k∇ k ≤ k∇ k ≤
(x,θ) X M. In addition, the partial Hessians 2f (, ) are L -Lipschitz continuous in their
θ t 3
∈ × ∇ · ·
second argument with respect to the norm .
k·k
4Note that for all functions f (, ) that are three times differentiable with respect to their second
t
argument, Assumption3trivially· ho· ldsifXisboundedgiventhatM Bdθ isbounded. Oneexpects
⊆ Bθ
thatour results extendtounbounded inputs viaatruncation argument, seeforinstance[23,Section
5.1]. We leave a thorough analysis of this case for future work.
Assumption 4 (Positive Definite Information Matrix). There exists λ > 0 such that:
0
T 1
1 −
E f (X ,θ ) ⊺f (X ,θ ) λ I .
T
∇θ t t ⋆ ∇θ t t ⋆
(cid:23)
0 dθ
" #
t=0
X
Assumption4imposesaminimalnoiseexcitationcondition, quantifyingthenotionofpersistence
of excitation [1]. Put differently, it asks that the parameter θ is identifiable (in the second-order
⋆
sense). We note in passing that analogous conditions are employed in recent related work (see,
e.g., [15,23,31]).
Assumption 5 (Quadratic Identifiability). There exists a > 0 such that for every θ M:
∈
T 1
1 −
θ θ 2 aE (f (X ,θ) f (X ,θ ))2 . (3)
⋆ t t t t ⋆
k − k ≤ T −
" #
t=0
X
Assumption 5 imposes a regularity condition on the regression functions f (, ) with respect to
t
· ·
theparameterspace. Morespecifically,itquantifiesthegrowthofthepredictionerrorasquadraticin
the parameter error. We point out that condition (3) is weaker than the global positive-definiteness
condition:
T 1
1 −
E f (X ,θ) ⊺f (X ,θ) δI ,
T
∇θ t t ∇θ t t
(cid:23)
dθ
" #
t=0
X
which is often assumed in the asymptotic literature [24], for all θ M, for some δ > 0. Moreover,
∈
note that Assumption 5 always holds for linear dynamical systems as well as generalized linear
models satisfying a certain expansivity condition (see, e.g., [14,15,32]).
Thegoalof systemidentification canoftenbecastastoidentify theparameter θ ,specifyingthe
⋆
data-generating distribution in (1), from sequential input-output data (X ,Y ),...,(X ,Y )
0 0 T 1 T 1
− −
[33]. In this paper, we analyze the finite-sample performance of the regression functions f (,θ),
t
·
where θ satisfies:
T 1
1 − b
θ argmin (f (X ,θ) Y )2. (4)
b ∈ θ M T t t − t
∈ Xt=0
Inparticular, we areinterested inbproviding anupperbound forthe(mean-squared) prediction error
of the models f (,θ), given by:
t
·
T 1
b E 1 − (f (X¯ ,θ) f (X¯ ,θ ))2 . (5)
t t t t ⋆
T −
" #
t=0
X
b
¯
Herein, we use {X
t
}T t=−01 to denote a fresh sample drawn from PX independently of {X
t
}T t=−01. We
formalize the problem in the following statement.
5Problem 1 (Rate-Optimal Non-asymptotic Analysis of the Quadratic Prediction Error Method).
Assume that θ in predictor model (1) is unknown. Consider a finite number T N of sequential
⋆ +
∈
input-output data (X ,Y ),...,(X ,Y ) generated by model (1) and let θ satisfy (4). Provide
0 0 T 1 T 1
− −
bounds T and ε(T) such that if T T , then:
0 0
≥
b
T 1
E 1 − (f (X¯ ,θ) f (X¯ ,θ ))2 ε(T).
t t t t ⋆
T − ≤
" #
t=0
X
b
The bounds T and ε(T) may also depend on the parameters d , σ , B , L , L , L , λ , a, b ,
0 θ w θ 1 2 3 0 1
and b . Moreover, the prediction error bound ε(T) should match known asymptotics up to constant
2
factors in its leading term (see Remark 1 for details).
Remark 1. We refer to non-asymptotic rates for the prediction error (5) as optimal if they match
known asymptotics up to constant factors and higher-order terms. In particular, existing results for
the quadratic prediction error method from the asymptotic literature [1,24] guarantee that √T(θ θ )
⋆
−
converges in distribution to (0, 1(θ )), where:
− ⋆
N I
b
T 1
(θ ) := 1 E 1 − f (X ,θ ) ⊺ f (X ,θ )
I ⋆ σ2 T ∇θ t t ⋆ ∇θ t t ⋆
w " t=0 #
X
is the Fisher information matrix. An informal calculation—ignoring the higher-order terms in
Taylor’s theorem—suggests that the prediction error can be written as follows:
T 1
E 1 − (f (X¯ ,θ) f (X¯ ,θ ))2
t t t t ⋆
T −
" #
t=0
X
E (θ θ )⊺(σb2 (θ ))(θ θ )
≈ − ⋆ wI ⋆ − ⋆
h i
= Etr σ2 (θ )(θ θ )(θ θ )⊺ . (6)
b wI ⋆ − ⋆ b − ⋆
(cid:16) (cid:17)
Undersuitable regularity conditions, wecan deducebthat thebexpectation ofthe trace on theright-hand
side of (6) asymptotically converges to
σ w2dθ,
that is:
T
T 1Etr σ2 (θ )(θ θ )(θ θ )⊺ tr σ2 (θ ) 1(θ ) = σ2d .
− wI ⋆ − ⋆ − ⋆ → wI ⋆ I− ⋆ w θ
(cid:16) (cid:17)
(cid:0) (cid:1)
In light of the above result, our
goab
l is to
ob
btain a rate of convergence that decays as fast as
cσ w2dθ,
T
for some universal constant c > 0.
3 Optimal Non-asymptotic Rates for the Quadratic Prediction Er-
ror Method
In this section, we present our main result, which is a rate-optimal bound for the prediction error
(5) of the models f (,θ), where θ is an estimate of the true parameter θ , satisfying (4). Before we
t ⋆
·
state our main theorem, let us note that herein, poly denotes a polynomial of degree of order ψ in
ψ
its arguments. b b
6Theorem 1 (OptimalNon-asymptoticRatesfortheQuadraticPredictionErrorMethod). Consider
the predictor model (1) and the parameter class M under Assumptions 1-5. Fix any γ (0,1/2)
∈
and let θ satisfy (4). Then, there exist:
T := poly (d ,L ,a,b ,1/(1 b )), (7a)
b 1 1−1 b2 θ 1 1 − 2
T := poly (d ,σ ,B ,L ,1/λ ,b ,1/(1 b )), (7b)
2 1 θ w θ 1 0 1 2
1−b2 −
T := poly (d ,σ ,B ,L ,L ,L ,a,1/(1 2γ)), (7c)
3 1 θ w θ 1 2 3
1−2γ −
and a universal constant c > 0 such that if T max T ,T ,T , we have:
1 2 3
≥ { }
E 1 T −1 (f (X¯ ,θ) f (X¯ ,θ ))2 cd θσ w2 + B , (8)
T t t − t t ⋆ ≤ T T1+γ
" #
t=0
X
b
where B = 2L2B 2+16.
1 θ
Theexactexpressionsofthepolynomials T ,T ,andT ofTheorem1aregivenintheAppendix.
1 2 3
Remark 2 (Result interpretation). Observe in (8) that for sufficiently large sample size T, the
least-squares prediction error decays at a rate of (T 1). In particular, the leading term in (8) is
−
O
determined by the signal-to-noise ratio (SNR)of model (1), which is defined as SNR = σ2/T. Notice
w
that the longer the predictor model is excited by noise and the smaller the sub-Gaussian parameter
σ is, the smaller the prediction error bound becomes. We note that this rate is optimal in the
w
sense that it matches known asymptotics up to constant factors in its leading term (see Remark 1),
after a finite burn-in time T := max T ,T ,T . The burn-in time grows polynomially in: i)
0 1 2 3
{ }
the parameter dimension d , ii) the sub-Gaussian parameter σ , iii) the noise bound B , iv) the
θ w θ
dependency parameter b , v) the smoothness parameters L , L , L , a, and vi) the inverse of the
1 1 2 3
noise excitation constant λ . Notice also the exponential growth of T in the dependency parameter
0 0
b . The parameter b is typically zero for exponentially stable dynamical systems (consider, e.g.,
2 2
exponentially stable ARMA models, cf. [23] for the case of autoregressive models). Nonetheless,
improving this growth rate is an interesting future research direction.
In the following section, we sketch the proof steps of Theorem 1.
4 Proof Sketch of Theorem 1
In this section, we present the main proof steps of Theorem 1, which provides us with optimal
non-asymptotic rates for the quadratic prediction error method.
A key quantity appearing in our analysis is the martingale offset corresponding to a param-
eter θ M, which can be thought of as a measure of the complexity of the corresponding re-
∈
gression functions f (,θ). To formally define the martingale offset, let us first introduce rele-
t
·
vant notation. Let W T 1 denote the noise sequence corresponding to the input-output data
{
t }t=−0
(X ,Y ),...,(X ,Y ), i.e., let W = Y f (X ,θ ), for all t = 0,...,T 1. Moreover, consider
0 0 T 1 T 1 t t t t ⋆
the shifted proces− s g (X− ,θ) T 1, where g (− X ,θ) = f (X ,θ) f (X ,θ ),− for all t = 0,...,T 1.
{
t t }t=−0 t t t t
−
t t ⋆
−
For any θ M, the martingale offset corresponding to the parameter θ is defined as:
∈
T 1
1 −
M (θ)= 4W g (X ,θ) g2(X ,θ) .
T T t t t − t t
t=0
X(cid:0) (cid:1)
7The above definition is motivated by the martingale offset complexity sup
θ
MM T(θ), which is em-
∈
ployed in previous works [23,29,34]. As we will see in the analysis that follows, deriving a bound
on the expected martingale offset EM (θ) of any least-squares estimate θ, instead of the expected
T
martingale offset complexity E[sup
θ
MM T(θ)], is essential for obtaining optimal finite-sample rates
for the quadratic prediction error m∈ ethodb. b
In the theorem below, we present a bound for the prediction error of the models f (,θ),...,
0
·
f (,θ), conditioned on the given sample (X ,Y ) T 1. Note that the following theorem is a
T −1
· {
t t }t=−0
modified version of [23, Corollary 4.2] for time-varying predictor models. We achieve the extbension
tothetibme-varyingcaseby derivingconcentration inequalities forthesumoftime-varyingfunctions
of the input data, leveraging a result from [27] (see Appendix B for details).
Theorem 2. Consider the predictor model (1) and the parameter class M under Assumptions 1, 3
and 5. Fix any γ [0,1) and let θ satisfy (4). Then, there exists T , defined as in (7a), such that
1
∈
if T T , we have:
1
≥
b
E X¯ 0:T−1"T1 T −1 (f t(X¯ t,θ) −f t(X¯ t,θ ⋆))2
#
≤
8M T(θ)+ 2 TL 12 1 +B γθ2 , (9)
t=0
X
b b
¯ ¯ ¯
where X = (X ,...,X ).
0:T 1 0 T 1
− −
Given (4), by taking the expectation over the sample (X ,Y ) T 1, (9) yields:
{
t t }t=−0
E 1 T −1 (f (X¯ ,θ) f (X¯ ,θ ))2 8EM (θ)+ 2L2 1B θ2 , (10)
T t t − t t ⋆ ≤ T T1+γ
" #
t=0
X
b b
for any γ [0,1). Moreover, Assumption 5 can be invoked to obtain the parameter error bound:
∈
2aL2B2
θ θ 2 8aM (θ)+ 1 θ, (11)
⋆ T
k − k ≤ T
since T T1+γ, for all γ [0,1). b b
≤ ∈
Employing the result from [29, Lemma 10] to bound the expected martingale offset complex-
ity E[sup
θ
MM T(θ)], inequality (10) directly provides us with a prediction error bound of order
∈
(logT/T), after a finite burn-in time T . We note that this is the first non-asymptotic result for
1
O
predictor models of the form (1), where the regression functions f (, ) are time-varying.
t
· ·
In the rest of this section, our goal is to improve upon that rate and ensure an optimal con-
vergence rate of (T 1), after a longer but finite burn-in time. Recall that herein optimality of
−
O
rate implies matching existing results from the asymptotic literature, modulo constant factors and
higher-order terms (see Remark 1). Our refinement of the prediction error bound resulting from
Theorem 2 consists of three distinct steps:
i) First, we derive an upper bound for EM (θ) by using the Taylor expansion of the models
T
f (X ,θ)aroundθ . Thisbounddependsona“linearized” termandhigher-order termsrelated
t t ⋆
to the parameter error θ θ . b
⋆
k − k
b
ii) Second, we provide a rate-optimal bound which scales like d σ2T 1 for the “linearized” term,
b θ w −
by leveraging ideas from linear system identification.
8iii) Third, we combine our bound for the “linearized” term with faster decaying bounds for the
higher-order terms to obtain a refined bound for EM (θ). The main idea is to bound the
T
higher-order terms employing the nearly optimal bounds resulting from Theorem 2. Owing
to the higher order of these components, a careful anablysis does not degrade the leading
d σ2T 1-order term of the linearized component.
θ w −
Putting everything together, we provide the firstoptimal non-asymptotic rates forthe quadratic
predictionerrormethod. Forclarityofpresentation, weseparatelyanalyzetheaforementioned proof
steps.
Step I: Bounding EM (θ) via Taylor expansion. By Taylor’s theorem with remainder, for
T
each t, we have:
b
1
f (X ,θ)= f (X ,θ )+Z⊺ (θ θ )+ (θ θ )⊺V (θ θ ), (12)
t t t t ⋆ t − ⋆ 2 − ⋆ t − ⋆
where Z = f (X ,θ ) abnd V = 2f (X ,θ˜)b, with θ˜ =bα θ +(1 bα )θ , for some α [0,1].
t ∇θ t t ⋆ t ∇θ t t t t t − t ⋆ t ∈
Exploiting the Taylor expansion of each f (X ,θ) from (12), we can prove the following lemma.
t t
b
Lemma 1. Consider the predictor model (1) and the parameter class M under Assumption 3.
b
Moreover, let θ satisfy (4), and for each t, consider the Taylor expansion of f (X ,θ) around θ
t t ⋆
given in (12). Then, the martingale offset of θ satisfies:
b b
M (θ)
M¯
(θ)+
2 T −b1
W V θ θ 2+
L2
2 θ θ 4, (13)
T T t t ⋆ ⋆
≤ T k − k 4 k − k
(cid:13) t=0 (cid:13)
(cid:13) X (cid:13)
b b (cid:13) (cid:13) b b
where:
(cid:13) (cid:13)
T 1
M¯ (θ) = 1 − 4W Z⊺ (θ θ ) 1 (Z⊺ (θ θ ))2 . (14)
T T t t − ⋆ − 2 t − ⋆
Xt=0 h i
By taking the expectatiob n over the sample Xb ,Y T 1 in (13b ), we obtain the following bound
{
t t }t=−0
for the expected martingale offset of θ:
EM (θ) EM¯ (θ)+bE 2 T −1 W V θ θ 2 + L2 2E θ θ 4. (15)
T T t t ⋆ ⋆
≤ T k − k 4 k − k
"(cid:13) t=0 (cid:13) #
(cid:13) X (cid:13)
Notice that the
bob
und for
EMb
(θ)
in(cid:13)
(cid:13)(15)
consists(cid:13) (cid:13)ofb
the “linearized”
tb
erm
EM¯
(θ) (note that
T T
thequadratic term ontheright-hand sideof (14)isnegative) andtwo higher-order terms depending
on the parameter error θ θ . In tbhe next step of our proof, we focus on bounding the linearized
⋆
k − k
component.
StepII:Boundingtheb“linearized” termEM¯ (θ). Inthefollowingtheorem,weprovideabound
T
for the “linearized” term
EM¯
(θ) appearing on the right-hand side of (15). Our analysis employs
T
tools for self-normalized martingales, similar to prebvious works in linear system identification (see,
e.g., [12]).
Theorem 3. Consider the predictor model (1) and the parameter class M under Assumptions 1-4.
Fix any γ (0,1) and let θ satisfy (4). Then, there exists T , defined as in (7b), and a universal
2
∈
constant c> 0 such that if T T , we have:
2
≥
b
EM¯
(θ)
cd θσ w2
+
1
. (16)
T ≤ T T1+γ
b
9Notice that the bound in (16) decays at the optimal rate of
σ w2dθ
(cf. Remark 1), up to a
T
constant factor c > 0 and a higher-order term 1/Tγ+1, which becomes negligible in finite time (set
for instance γ = 1/4). Next, we present the final step of our proof, which combines the bound (16)
with faster decaying bounds for the higher-order terms on the right-hand side of (15).
Step III: Bounding EM (θ) using the bound (16) for EM¯ (θ) and the bound (11) for the
T T
higher-order terms in (15). In Step II we provided a bound of order (T 1) for the “linearized”
−
term
EM¯
(θ) appearing on bthe right-hand side of (15). To bounbd
EMO
(θ) at a rate of (T 1),
T T −
O
it suffices to derive faster decaying bounds of order (T1/(1+γ)) for the higher-order terms, where
O
γ (0,1/2). Combining (16) from Theorem 3 and the parameter error bbound given in (11), we
∈
obtain the following corollary.
Corollary 1. Consider the predictor model (1) and the parameter class M under Assumptions 1-5.
Fix any γ (0,1/2) and let θ satisfy (4). Then, there exist T , T , T , defined as in Theorem 1,
1 2 3
∈
and a universal constant c > 0 such that if T max T ,T ,T , we have:
1 2 3
≥ { }
b
cd σ2 2
EM (θ) θ w + . (17)
T ≤ T T1+γ
Notice that the dominant term on the
rb
ight-hand side of (17) decays at a rate of
cdθσ W2
, which is
T
optimal uptoaconstant factor c > 0(seeRemark1). Wenotethattheaboveboundimproves upon
the rate O(logT/T) that can been shown for the martingale offset complexity E[sup
θ
∈MM T(θ)] via
maximal inequalities [29]. The key point here is that the offset process locally, once θ is sufficiently
near θ , behaves like a linear offset process.
⋆
Combining (10) with (17) from Corollary 1, we complete the proof of (8) in Theborem 1. Next,
we instantiate Theorem 1 to provide finite-sample guarantees for the quadratic prediction error
method for AutoRegressive Moving Average (ARMA) models.
5 Case Study: The ARMA Model
In this section, we demonstrate the applicability of our rate-optimal non-asymptotic analysis of the
quadratic prediction error method to scalar ARMA models. Our result relies on astandard analysis
from [2,35] that allows converting any ARMA model into a predictor model of the form (1). For
completeness of presentation, we briefly review the conversion methodology and then present a
rate-optimal non-asymptotic bound for a particular class of ARMA models.
Consider the scalar ARMA(p,q) model given by:
p q
Y = a⋆Y + b⋆W , (18)
t i t i j t j
− −
i=1 j=0
X X
where the noise variables W R are assumed to be independent and zero-mean, and the initial
t
∈
conditions are assumed to be zero, i.e., Y = 0, W = 0, for all t < 0. Suppose that b⋆ = 1 and the
t t 0
parameter θ := a⋆,...,a⋆,b⋆,...,b⋆ ⊺ Rp+q+1 belongs to some known set M Bdθ, where B
⋆ 1 p 0 q ∈ ⊆ Bθ θ
is a positive constant. The assumption that b⋆ = 1 can always be ensured by providing additional
(cid:2) (cid:3) 0
artificial noise components of zero mean and variance, and applying linear transformations to the
noise variables W [35]. Let z 1 denote the backward-shift operator, defined by z 1e := e , for
t − − t t 1
any stochastic process e . Powers of z 1 are defined recursively by z (i+1)e := z 1(z −ie ) so
t ∞ − − t − − t
{ }−∞
10thatz ie = e . Itisstraightforward toshowthat (18)is equivalent to A (z 1)Y = B (z 1)W ,
− t t i θ⋆ − t θ⋆ − t
−
where A () and B () are polynomials given by:
θ⋆
·
θ⋆
·
p q
A (λ) = 1 a⋆λi, B (λ) = b⋆λj,
θ⋆ − i θ⋆ j
i=1 j=0
X X
respectively, forallλ R. Foreacht, let ¯ := σ(Y ,...,Y )betheσ-field generated bytheoutputs
t 0 t
Y ,...,Y . It is known∈ [2, Section 2.6] thF at the conditional expectation Y := E[Y ¯ ] satisfies:
0 t t t t 1
|F−
B θ⋆(z −1)Y t = [B θ⋆(z −1) −A θ⋆(z −1)]Y t, b (19)
for all t = 0,...,T 1. Hence, we canb rewrite model (18) in the predictor model form (1) with
−
regression functions:
f (X ,θ ):= Y , (20)
t t ⋆ t
⊺
whereX = Y ,...,Y . TheconditionalexpectationsY ,...,Y canbecomputedrecursively
t 0 t −1 b 0 T −1
from(19)withzeroinitial condition, i.e., Y = 0, forallt < 0. Wecansimilarly definetheregression
(cid:2) (cid:3) t
functions f (,θ) corresponding to any parameter θ in thebclass Mb. In the corollary that follows,
t
·
we combine Theorem 1 with the predictorbmodel form of the ARMA model (18) derived above and
provide the first rate-optimal non-asymptotic prediction error bounds for ARMA models.
Corollary 2. Consider the predictor model form of the ARMA(p,q) model (18), defined by (1) and
(20), as well as the parameter class M, under Assumptions 1-5. Fix any γ (0,1/2) and let θ
∈
satisfy (4). Then, there exist T , T , T , defined as in Theorem 1, and a universal constant c > 0
1 2 3
such that if T max T ,T ,T , we have: b
1 2 3
≥ { }
1 T −1 c(p+q)σ2 B
E (f (X ,θ) f (X ,θ ))2 w + ,
T t t − t t ⋆ ≤ T Tγ+1
" #
t=0
X
b
where B = 2L2B 2+16.
1 θ
The proof of Corollary 2 follows directly from Theorem 1, given the predictor model form of
model (18).
Note that the above corollary applies to a particular class of ARMA models that satisfy As-
sumptions 1-5. Assumptions 1-4 are relatively benign for this example, and hold as long as the
noise sequence W T 1 is bounded and the system (18) is stable. For Assumption 1, see e.g. [23]
{
t }t=−0
for the case of B (λ) = 1. Assumption 2 is true by construction of the regression functions (20)
θ⋆
corresponding to model (18) as well as the hypothesis of bounded noise. Assumption 3 can be
verified via arguments entirely analogous to those in [36] as long as W T 1 is bounded and the
{
t }t=−0
system(18)isstable. SufficientconditionsforguaranteeingAssumption4,relatedtotherootsofthe
polynomials A (λ) and B (λ), can be found in [37]. Assumption 5 restricts our result to a specific
θ⋆ θ⋆
class of quadratically identifiable ARMA models (see (3)). As previously explained in Section 2,
this assumption is weaker than the corresponding assumption made in the asymptotic literature
for the quadratic prediction error method [24]. Exploring potential relaxations of the identifiability
condition (3) is an interesting problem for future work.
11References
[1] L. Ljung, System Identification: Theory for the User. Pearson Education, 1998.
[2] A. B. Tsybakov, Introduction to Nonparametric Estimation. Springer, 2009.
[3] M. C. Campi and E. Weyer, “Finite sample properties of system identification methods,” IEEE
Transactions on Automatic Control, vol. 47, no. 8, pp. 1329–1334, 2002.
[4] R. Vershynin, High-dimensional probability: An introduction with applications in data science.
Cambridge university press, 2018, vol. 47.
[5] M. J. Wainwright, High-dimensional statistics: A non-asymptotic viewpoint. Cambridge uni-
versity press, 2019, vol. 48.
[6] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht, “Learning without mixing:
Towards a sharp analysis of linear system identification,” in Conference On Learning Theory.
PMLR, 2018, pp. 439–473.
[7] M. K. S. Faradonbeh, A. Tewari, and G. Michailidis, “Finite time identification in unstable
linear systems,” Automatica, vol. 96, pp. 342–353, 2018.
[8] M. Jansson and B. Wahlberg, “On consistency of subspace methods for system identification,”
Automatica, vol. 34, no. 12, pp. 1507–1519, 1998.
[9] A. Chiuso and G. Picci, “The asymptotic variance of subspace estimates,” Journal of Econo-
metrics, vol. 118, no. 1-2, pp. 257–291, 2004.
[10] S. J. Qin, “An overview of subspace identification,” Computers & chemical engineering, vol. 30,
no. 10-12, pp. 1502–1513, 2006.
[11] A. Tsiamis and G. J. Pappas, “Finite sample analysis of stochastic system identification,” in
2019 IEEE 58th Conference on Decision and Control (CDC). IEEE, 2019, pp. 3648–3654.
[12] A. Tsiamis, I. Ziemann, N. Matni, and G. J. Pappas, “Statistical learning theory for control:
A finite-sample perspective,” IEEE Control Systems Magazine, vol. 43, no. 6, pp. 67–97, 2023.
[13] I. Ziemann, A. Tsiamis, B. Lee, Y. Jedra, N. Matni, and G. J. Pappas, “A tutorial on the
non-asymptotic theory of system identification,” in 2023 62nd IEEE Conference on Decision
and Control (CDC). IEEE, 2023, pp. 8921–8939.
[14] Y. Sattar and S. Oymak, “Non-asymptotic and accurate learning of nonlinear dynamical sys-
tems,” Journal of Machine Learning Research, vol. 23, no. 140, pp. 1–49, 2022.
[15] S. Kowshik, D. Nagaraj, P. Jain, and P. Netrapalli, “Near-optimal offline and streaming algo-
rithms for learning non-linear dynamical systems,” Advances in Neural Information Processing
Systems, vol. 34, pp. 8518–8531, 2021.
[16] B. Yu, “Rates of convergence for empirical processes of stationary mixing sequences,” The
Annals of Probability, pp. 94–116, 1994.
12[17] M. Mohri and A. Rostamizadeh, “Rademacher complexity bounds for non-iid processes,” Ad-
vances in Neural Information Processing Systems, vol. 21, 2008.
[18] J. C. Duchi, A. Agarwal, M. Johansson, and M. I. Jordan, “Ergodic mirror descent,” SIAM
Journal on Optimization, vol. 22, no. 4, pp. 1549–1578, 2012.
[19] V. Kuznetsov and M. Mohri, “Generalization bounds for non-stationary mixing processes,”
Machine Learning, vol. 106, no. 1, pp. 93–117, 2017.
[20] A. Roy, K. Balasubramanian, and M. A. Erdogdu, “On empirical risk minimization with de-
pendent and heavy-tailed data,” Advances in Neural Information Processing Systems, vol. 34,
pp. 8913–8926, 2021.
[21] A. Sancetta, “Estimation in reproducing kernel hilbert spaces with dependent data,” IEEE
Transactions on Information Theory, vol. 67, no. 3, pp. 1782–1795, 2020.
[22] H.B.MannandA.Wald,“Onthestatisticaltreatmentoflinearstochasticdifferenceequations,”
Econometrica, Journal of the Econometric Society, pp. 173–220, 1943.
[23] I.ZiemannandS.Tu, “Learningwithlittlemixing,” Advances in Neural InformationProcessing
Systems, vol. 35, pp. 4626–4637, 2022.
[24] L.LjungandP.E.Caines,“Asymptoticnormalityofpredictionerrorestimatorsforapproximate
system models,” Stochastics, vol. 3, no. 1-4, pp. 29–46, 1980.
[25] I. Ziemann, S. Tu, G. J. Pappas, and N. Matni, “Sharp rates in dependent learning theory:
Avoiding sample size deflation for the square loss,” arXiv preprint arXiv:2402.05928, 2024.
[26] T.Liang, A.Rakhlin,andK.Sridharan, “Learning withsquareloss: Localizationthroughoffset
rademacher complexity,” in Conference on Learning Theory. PMLR, 2015, pp. 1260–1285.
[27] P.-M. Samson, “Concentration of measure inequalities for markov chains and φ-mixing pro-
cesses,” The Annals of Probability, vol. 28, no. 1, pp. 416–461, 2000.
[28] S. P. Meyn and R. L. Tweedie, Markov chains and stochastic stability. Springer Science &
Business Media, 2012.
[29] I. M. Ziemann, H. Sandberg, and N. Matni, “Single trajectory nonparametric learning of non-
linear dynamics,” in conference on Learning Theory. PMLR, 2022, pp. 3333–3364.
[30] I. Ziemann, A. Tsiamis, B. Lee, Y. Jedra, N. Matni, and G. J. Pappas, “A tutorial on the
non-asymptotic theory of system identification,” in 2023 62nd IEEE Conference on Decision
and Control (CDC). IEEE, 2023, pp. 8921–8939.
[31] H. Mania, M. I. Jordan, and B. Recht, “Active learning for nonlinear system identification with
guarantees,” Journal of Machine Learning Research, vol. 23, no. 32, pp. 1–30, 2022.
[32] D. Foster, T. Sarkar, and A. Rakhlin, “Learning nonlinear dynamical systems from a single
trajectory,” in Learning for Dynamics and Control. PMLR, 2020, pp. 851–861.
[33] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series analysis: forecasting
and control. John Wiley & Sons, 2015.
13[34] T.Liang, A.Rakhlin,andK.Sridharan, “Learning withsquareloss: Localizationthroughoffset
rademacher complexity,” in Conference on Learning Theory. PMLR, 2015, pp. 1260–1285.
[35] M. Davis, Stochastic modelling and control. Springer Science & Business Media, 2013.
[36] P. Caines, “Prediction error identification methods for stationary stochastic processes,” IEEE
Transactions on Automatic Control, vol. 21, no. 4, pp. 500–505, 1976.
[37] A. Klein and P. Spreij, “On fisher’s information matrix of an armax process and sylvester’s
resultant matrices,” Linear Algebra and its Applications, vol. 237, pp. 579–590, 1996.
[38] S. Mendelson, “Improving the sample complexity using global data,” IEEE transactions on
Information Theory, vol. 48, no. 7, pp. 1977–1991, 2002.
[39] I. Ziemann, “Statistical learning, dynamics and control: Fast rates and fundamental limits for
square loss,” Ph.D. dissertation, KTH Royal Institute of Technology, 2022.
Appendix
A Basic Definitions and Results
In this subsection, we present a few basic lemmas that we will use in the proofs of our theorems in
the subsequent subsections.
We first state a standard result from algebra. We include the proof for completeness.
Lemma A.1. For all α,β R, we have:
∈
β2
(α β)2 α2 2β2+2(α β)2 (21)
2 − − ≤ ≤ −
Proof. Let us prove the right inequality in (21). For all x,y R, we have (x y)2 0, which
∈ − ≥
implies that:
2xy x2+y2.
≤
Setting x = β and y = α β, we get:
−
2β(α β) β2+(α β)2. (22)
− ≤ −
For all α,β R, we can write:
∈
α2 = (β +(α β))2 = β2+(α β)2+2β(α β) 2β2 +2(α β)2,
− − − ≤ −
where the inequality follows from (22). Similarly, we can show the left inequality in (21).
(cid:4)
Next,westatethedefinition ofε-netsandstateastandardupperboundforthesmallestpossible
cardinality of such sets corresponding to the ball Sd 1.
−
14Definition A.1 (ε-netandCovering numbers). [4, Definitions 4.2.1, 4.2.2] Let (X,d) be a compact
metric space and fix ε > 0. A subset of X is called an ε-net of X if every point of X is within
ε
N
radius ε of a point of , that is:
ε
N
sup inf d(x,x ) ε. (23)
′
x
Xx′
ε
≤
∈ ∈N
Moreover, the smallest possible cardinality of for which (23) holds is called the covering number
ε
N
at resolution ε of (X,d) and is denoted by (ε,X,d).
N
We note that herein when we refer toε-nets, we imply ε-nets of the smallestpossible cardinality.
Lemma A.2. [4, Corollary 4.2.13] For any ε> 0, the covering numbers of Sd 1 satisfy:
−
d
2
(ε,Sd 1, ) +1 . (24)
−
N k·k ≤ ε
(cid:18) (cid:19)
We can use Lemma A.2 to derive basic bounds for the covering numbers of the parameter class
M and the function class:
1
:= g : (x ,...,x ) (g (x ,θ),...,g (x ,θ)) x X, t,θ M , (25)
θ 0 T 1 0 0 T 1 T 1 t
G − 7→ √T − − ∈ ∀ ∈
(cid:26) (cid:12) (cid:27)
(cid:12)
where g (x,θ) = f (x,θ) f (x,θ ), for all (x,θ) X M and all t =(cid:12)0,...,T 1. We present these
t t t ⋆
− ∈ × −
bounds in the next two lemmas.
Lemma A.3. For any ε (0,B ], the covering numbers of the parameter class M Bdθ satisfy:
∈ θ ⊆ Bθ
3B
dθ
(ε,M, ) θ . (26)
N k·k ≤ ε
(cid:18) (cid:19)
Proof. Fix any ε ′ (0,1] and let ε′ be an ε ′-net of Sdθ−1 with respect to the norm . From
∈ N k·k
Lemma A.2 we deduce that:
2
dθ
3
dθ
ε′ +1 , (27)
|N | ≤ ε ≤ ε
(cid:18) ′ (cid:19) (cid:18) ′(cid:19)
where the last inequality follows from the fact that ε (0,1]. Given that M Bdθ, we have:
′ ∈ ⊆ Bθ
(ε,M, ) (ε,Bdθ, ),
N k·k ≤ N Bθ k·k
which implies that it suffices to obtain bounds for the covering numbers of Bdθ. Note that for
Bθ
every θ
∈
Bd Bθ
θ
\{0 }, there exists u := 1
θ
θ
∈
Sdθ−1 (in the trivial case where θ = 0 we can set
k k
u = 0). Then, by definition of ε′, there exists u i ε′ such that u u i ε ′. Hence, setting
θ = θ u Bdθ, we obtain: N ∈ N k − k ≤
i k k i ∈ Bθ
θ θ = θ u u B ε. (28)
i i θ ′
k − k k kk − k ≤
For any ε (0,B ], we can set ε = ε/B in (27) and (28), thus completing the proof of (26).
θ ′ θ
∈
(cid:4)
15Lemma A.4. Consider the predictor model (1) and the parameter class M under Assumption 3.
Moreover, let be as in (25). Then, for any ε (0,L B ], the covering numbers of satisfy:
1 θ
G ∈ G
3L B
dθ
1 θ
(ε, , ) . (29)
N G k·k∞ ≤ ε
(cid:18) (cid:19)
Proof. Fix any ε ′ (0,B θ] and let ε′ be an ε ′-net of M with respect to the norm . From
∈ N k · k
Lemma A.3 we deduce that:
3B
dθ
θ
ε′ . (30)
|N | ≤ ε
(cid:18) ′ (cid:19)
By definition of Nε′, for every θ
∈
M there exists θ i
∈
Nε′ such that kθ −θ i
k ≤
ε ′. Let g θ and g θi
denote the functions in corresponding to θ and θ , respectively. Employing Assumption 3, we can
i
G
write:
T 1 2
− 1
g g = sup (g (x ,θ) g (x ,θ ))
k θ − θik∞ x0,...,xT−1∈Xv
u uXt=0
(cid:18)√T t t − t t i
(cid:19)
t
T 1
1 −
= sup (f (x ,θ) f (x ,θ ))2
x0,...,xT−1∈Xv
u
uT
Xt=0
t t − t t i
t
T 1
1 −
L2 θ θ 2 = L θ θ L ε. (31)
≤ vT 1k − i k 1 k − i k ≤ 1 ′
u t=0
u X
t
We conclude that for every g , there exists g such that (31) holds. Hence, setting
θ
∈ G
θi
∈ G
ε = ε/L , from (30) we deduce that (29) holds for any ε (0,L B ], thus completing the proof.
′ 1 1 θ
∈
(cid:4)
B Proof of Theorem 2
We start by presenting the following lemma, which combines a modified version of [23, Theorem
5.1] and [27, Theorem 2] for time-varying parametric functions g (,θ). Although herein we focus
t
·
on parametric functions, note that the following result directly extends to nonparametric functions
g ().
t
·
Lemma B.1. For each t, let g : X M R be such that 0 g (x,θ) C, for all (x,θ) X M,
t t
× → ≤ ≤ ∈ ×
for some C > 0. Then, for any λ > 0 and θ M we have:
∈
Eexp
λT −1
g (X ,θ) exp
λT −1
Eg (X ,θ)+
λ2 kΓ dep(PX) k2 T −1
Eg2(X ,θ) . (32)
− t t ≤ − t t 2 t t
! !
t=0 t=0 t=0
X X X
Moreover, for any s > 0 and θ M we have:
∈
T −1 T −1 s2
P g (X ,θ) Eg (X ,θ)+s exp (33)
Xt=0
t t ≥
Xt=0
t t ! ≤ −2C kΓ dep(PX) k2( T t=−01Eg t(X t,θ)+s)!
P
16and:
T −1 T −1 s2
P g (X ,θ) Eg (X ,θ) s exp . (34)
Xt=0
t t ≤
Xt=0
t t − ! ≤ −2C kΓ dep(PX) k2 T t=−01Eg t(X t,θ)!
Proof. Fix any θ M and define the finite function class: P
∈
= g (,θ),...,g (,θ) .
0 T 1
F { · − · }
Moreover, let:
T 1
−
f (x ,...,x ) = g (x ,θ),
T 0 T 1 t t
−
t=0
X
for all x ,...,x X, and:
0 T 1
− ∈
1, if k = t
α = ,
kt 0, else
(cid:26)
for all k,t 0,...,T 1 . Notice that:
∈ { − }
T 1T 1
− −
f (x ,...,x )= α g (x ,θ),
T 0 T 1 kt k t
−
k=0 t=0
XX
for all x ,...,x X, and:
0 T 1
− ∈ T 1
−
α = 1,
kt
k=0
X
for all t = 0,...,T 1. Fix any x ,...,x ,x ,...,x X. Set x = (x ,...,x )
and x = (x
,..−
.,x ).
Furth0 ermoreT ,− f1
or
′0
each
t,′T l−et1 α∈
= (α
,.0 .: .T ,− α1 ),0
g(x
,θT )−1
=
′0:T 1 ′0 ′T 1 t 0t (T 1)t t
(g (x ,θ)−,..., g (x ,θ)),−and g(x ,θ) = (g (x ,θ),...,g (x ,θ)). Then, we ha− ve:
0 t T 1 t ′t 0 ′t T 1 ′t
− −
T 1T 1
− −
f (x ) f (x )= α (g (x ,θ) g (x ,θ))
T ′0:T −1 − T 0:T −1 kt k ′t − k t
k=0 t=0
XX
T 1
− ⊺
= α (g(x ,θ) g(x ,θ))
t ′t − t
t=0
X
T 1
= − α⊺ t(g(x ′t,θ) −g(x t,θ))1 xt 6=x′
t
t=0
X
T 1
≤
− α⊺ tg(x ′t,θ)1 xt 6=x′ t,
t=0
X
where the last step follows from the fact that the functions g (,θ) and the coefficients α are
t kt
non-negative. Setting f˜ = f , we can similarly show that: ·
T T
−
T 1
f˜ T(x ′0:T −1) −f˜ T(x 0:T −1)
≤
− α⊺ tg(x t,θ)1 xt 6=x′ t.
t=0
X
From this point onwards, the proof of (32), (33), and (34) is the same as that of (3.12), (3.1), and
(3.2), respectively, from [27, Theorem 2] for Z = f (X ,...,X ), and thus is omitted.
T 0 T 1
−
(cid:4)
17Employing (32) from Lemma B.1, we derive a lower isometry result that extends [23, Theorem
5.2] to time-varying parametric regression functions f (,θ).
t
·
Lemma B.2. Fix any r (0,√8L B ]. Consider the predictor model (1) and the parameter
1 θ
∈
class M under Assumptions 1, 3 and 5 . For each t, define g (x,θ) = f (x,θ) f (x,θ ), for all
t t t ⋆
−
(x,θ) ∈X ×M. Moreover, let
Br
= θ
∈
M T1 T t=−01Eg t2(X t,θ)
≤
r2 . Then, we have:
n (cid:12) P o
P inf
1 T −1
g2(X ,θ)
1 T −1 Eg2(cid:12)
(X ,θ) 0
8√8L 1B
θ
2dθ
exp
T1 −b2
.
θ ∈M \Br T t=0 t t − 8T t=0 t t ! ≤ ! ≤ r ! (cid:18)−8b 1L4 1a2 (cid:19)
X X
(35)
Proof. Fix any θ M. Employing Assumptions 3 and 5, we can write:
∈
2
T 1 T 1 T 1
1 − 1 − 1 −
Eg4(X ,θ)= E(f (X ,θ) f (X ,θ ))4 L4 θ θ 4 L4a2 Eg2(X ,θ) .
T t t T t t − t t ⋆ ≤ 1k − ⋆ k ≤ 1 T t t
!
t=0 t=0 t=0
X X X
(36)
Employing (32) from Lemma B.1 and (36), we can show that:
T 1 T 1
1 − 1 − T
P g2(X ,θ) Eg2(X ,θ) exp , (37)
T
t=0
t t ≤ 2T
t=0
t t
!
≤ (cid:18)−8L4 1a2 kΓ dep(PX) k2
(cid:19)
X X
similarly to the proof of [23, Proposition 5.1]. Let be as in (25) and consider its star-hull:
G
1
:= ¯g :(x ,...,x ) (¯g (x ,θ),...,¯g (x ,θ))
⋆ θ 0 T 1 0 0 T 1 T 1
G − 7→ √T − −
(cid:26) (cid:12)
(cid:12)
¯g (x ,θ)= αg (x ,θ),x X, t,α [0,1],θ M . (cid:12)
t t t t t
∈ ∀ ∈ ∈
(cid:27)
Let
G⋆,r
= ¯g
θ
∈
G⋆
T1 T t=−01E¯g t2(X t,θ)
≤
r2 and let
Nr
denote a r/√8-net of ∂
G⋆,r
:=
¯g
θ
∈
G⋆
T1 nT t=−01E¯g t2((cid:12) (cid:12)X t,Pθ)= r2 with respect o to the norm
k ·
k∞. For every ¯g
θ
∈
G⋆, there
nexist α
∈(cid:12)
(cid:12)[0, P1] and g
θ
∈ G
such thaot ¯g
θ
= αg θ. Hence, employing Assumption 3, we can write:
T 1 2
− 1
¯g = sup αg (x ,θ)
k θ k∞ x0,...,xT−1∈Xv
u uXt=0
(cid:18)√T t t
(cid:19)
t
T 1
1 −
sup (f (x ,θ) f (x ,θ ))2
≤ x0,...,xT−1∈Xv
u
uT
Xt=0
t t − t t ⋆
t
T 1
1 −
L2 θ θ 2 = L θ θ 2L B . (38)
≤ vT 1k − ⋆ k 1 k − ⋆ k ≤ 1 θ
u t=0
u X
t
18Moreover, we have:
T 1 T 1 T 1 T 1
1 − 1 − 1 − 1 −
P ¯g2(X ,θ) E¯g2(X ,θ) = P α2g2(X ,θ) E α2g2(X ,θ)
T t t ≤ 2T t t T t t ≤ 2T t t
! !
t=0 t=0 t=0 t=0
X X X X (cid:2) (cid:3)
T 1 T 1
1 − 1 −
= P g2(X ,θ) Eg2(X ,θ)
T t t ≤ 2T t t
!
t=0 t=0
X X
T
exp , (39)
≤ (cid:18)−8L4 1a2 kΓ dep(PX) k2
(cid:19)
where the last step follows from (37). From (38), [38, Lemma 4.5] and Lemma A.4 we have:
r
= ,∂ ,
r ⋆,r
|N | N √8 G k·k∞
(cid:18) (cid:19)
r
, , (∂ )
⋆ ⋆,r ⋆
≤ N √8 G k·k∞ G ⊆ G
(cid:18) (cid:19)
4√8sup θ ∈M k¯g θ k r , , (from [38, Lemma 4.5])
≤ r N 2√8 G k·k∞
(cid:18) (cid:19)
8√8L B r
1 θ
, , (from (38))
≤ r N 2√8 G k·k∞
(cid:18) (cid:19)
8√8L B 6√8L B
dθ
1 θ 1 θ
(from (29))
≤ r r
!
8√8L B
2dθ
1 θ . (d N ) (40)
θ +
≤ r ∈
!
Note that every element of is of the form α g , where α [0,1] and g . Define the event:
Nr i θi i
∈
θi
∈ G
T 1 T 1
1 − 1 −
= α2g2(X ,θ ) E α2g2(X ,θ )
E T i t t i ≤ 2T i t t i
( )
αig [θi∈Nr Xt=0 Xt=0 (cid:2) (cid:3)
and observe, by a union bound, that:
T 1 T 1
1 − 1 −
P( ) P α2g2(X ,θ ) E α2g2(X ,θ )
E ≤ T i t t i ≤ 2T i t t i
!
αig Xθi∈Nr Xt=0 Xt=0 (cid:2) (cid:3)
T 1 T 1
1 − 1 −
max P α2g2(X ,θ ) E α2g2(X ,θ ) . (41)
≤ |Nr |αigθi∈Nr T
t=0
i t t i ≤ 2T
t=0
i t t i
!
X X (cid:2) (cid:3)
Combining (41) with (39) and (40), and invoking Assumption 1, we obtain:
8√8L B
2dθ
T1 b2
P( ) 1 θ exp − . (42)
E ≤ r −8b L4a2
! (cid:18) 1 1 (cid:19)
19Fix any ¯g ∂ . By definition of , there exists α g such that ¯g α g r/√8.
θ
∈
G⋆,r Nr i θi
∈
Nr
k
θ
−
i θik∞
≤
Hence, by Lemma A.1, we have:
T 1 T 1 T 1
1 − 1 − 1 −
¯g2(X ,θ) α2g2(X ,θ ) (¯g (X ,θ) α g (X ,θ ))2
T t t ≥ 2T i t t i − T t t − i t t i
t=0 t=0 t=0
X X X
T 1
1 −
α2g2(X ,θ ) ¯g α g 2
≥ 2T i t t i −k θ − i θik∞
t=0
X
1 T −1 r2
α2g2(X ,θ ) . (43)
≥ 2T i t t i − 8
t=0
X
On the complement of , (43) yields:
E
1 T −1 1 T −1 r2 r2 r2 r2
¯g2(X ,θ) E α2g2(X ,θ ) = = , (44)
T t t ≥ 4T i t t i − 8 4 − 8 8
t=0 t=0
X X (cid:2) (cid:3)
where the second-to-last step follows from the fact that α g ∂ . From (42) and (44) we
i θi
∈
Nr
⊆
G⋆,r
conclude that:
P inf
1 T −1
¯g2(X ,θ)
r2
0
8√8L 1B
θ
2dθ
exp
T1 −b2
. (45)
¯gθ∈∂
G⋆,r
T
t=0
t t − 8
!
≤
!
≤ r
!
(cid:18)−8b 1L4 1a2
(cid:19)
X
For every ¯g ¯ := , there exists r > r such that:
θ ⋆,r ⋆ ⋆,r ′
∈ G G \G
T 1 T 1
1 − 1 − r 2
E¯g2(X ,θ) = r2 = E ¯g (X ,θ) = r2. (46)
T t t ′ ⇒ T r t t
′
Xt=0 Xt=0 (cid:16) (cid:17)
Since r < 1, by definition of , we have r¯g , and from (46) we deduce that r¯g ∂ .
r′ G⋆ r′ θ
∈
G⋆ r′ θ
∈
G⋆,r
Therefore, from (45) we obtain:
P inf
1 T −1
¯g2(X ,θ)
1 T −1
E¯g2(X ,θ) 0
8√8L 1B
θ
2dθ
exp
T1 −b2
.
¯gθ∈G¯
⋆,r
T
Xt=0
t t − 8T
Xt=0
t t
!
≤
!
≤ r
!
(cid:18)−8b 1L4 1a2
(cid:19)
(47)
Given that g
θ
∈ G
T1 T t=−01Eg t2(X t,θ)> r2
⊆
G¯ ⋆,r, (47) implies (35), thus completing the proof.
n (cid:12) P o (cid:4)
(cid:12)
Next, we employ Lemma B.2 to prove the following result, which is a modified version of [23,
Theorem 4.1] for time-varying parametric regression functions f (,θ).
t
·
Lemma B.3. Consider the predictor model (1) and the parameter class M under Assumptions 1,
3 and 5. Fix any r (0,√8L B ] and let θ satisfy (4). Then, we have:
1 θ
∈
T 1
E X¯ 0:T−1"T1 − (f t(X¯ t,θ)b −f t(X¯ t,θ ⋆))2
#
t=0
X
b
8√8L B
2dθ
T1 b2
8M (θ)+r2+4L2B 2 1 θ exp − , (48)
≤ T 1 θ r −8b L4a2
! (cid:18) 1 1 (cid:19)
where
X¯
=
(X¯ ,...,X¯b
).
0:T 1 0 T 1
− −
20Proof. Let and g (, ) be as in Lemma B.2 and define the event:
r t
B · ·
T 1 T 1
1 − 1 −
= inf g2(X ,θ) Eg2(X .θ) 0 .
E (θ ∈M \Br T t=0 t t − 8T t=0 t t ! ≤ )
X X
Moreover, set X = (X ,...,X ) and let:
0:T 1 0 T 1
− −
T 1
1 −
N (X ,θ)= g2(X ,θ),
T 0:T −1 T t t
t=0
X
for all input samples X and parameters θ M. On the complement c of we have:
0:T 1
− ∈ E E
T 1
1 −
Eg2(X ,θ) max 8N (X ,θ),r2 8N (X ,θ)+r2, (49)
T t t ≤ T 0:T −1 ≤ T 0:T −1
t=0
X (cid:8) (cid:9)
forallinputsamplesX andparametersθ M. GivenAssumption3andthefactthatM Bdθ ,
0:T −1 ∈ ⊆ Bθ
we have:
T 1 T 1
1 − 1 −
g2(X ,θ)= (f (X ,θ) f (X ,θ ))2 L2 θ θ 2 4L2B 2, (50)
T t t T t t − t t ⋆ ≤ 1k − ⋆ k ≤ 1 θ
t=0 t=0
X X
for all input samples X and parameters θ M. Employing (49) and (50), we can write:
0:T 1
− ∈
T 1 T 1 T 1
1 − 1 − 1 −
Eg2(X ,θ)= E1 g2(X ,θ)+ E1 g2(X ,θ)
T t t T Ec t t T E t t
t=0 t=0 t=0
X X X
8N (X ,θ)+r2+4L2B 2P( ), (51)
≤ T 0:T −1 1 θ E
for all input samples X and parameters θ M, given that E1 = P( ). As a byproduct of the
0:T 1
− ∈ E E
proof of [29, Lemma 7], we have the basic inequality:
N (X ,θ) M (θ). (52)
T 0:T 1 T
− ≤
Setting θ = θ in (51) and employing (35) from Lemma B.2 and (52), we obtain:
b b
E X¯ 0:T−1b "T1 T t=− 01 g t2(X¯ t,θ)
#≤
8M T(θ)+r2+4L2 1B θ2 8√8L r1B θ !2dθ exp (cid:18)−8T
b
11 L−
4
1b a2
2
(cid:19),
X
¯ ¯ ¯b b
where X = (X ,...,X ), thus completing the proof.
0:T 1 0 T 1
− −
(cid:4)
Now, we can prove Theorem 2. Fix any γ [0,1) and set r = L1Bθ . Then, if the third term
∈ T(1+γ)/2
on the right-hand side of (48) is dominated by r2, inequality (9) trivially follows. We can write this
term as follows:
8√8L B
2dθ
T1 b2 8√8L B T1 b2
4L2B 2 1 θ exp − = 4L2B 2exp 2d log 1 θ −
1 θ r −8b L4a2 1 θ θ r − 8b L4a2
! (cid:18) 1 1 (cid:19) ! 1 1 !
= 4L2 1B θ2exp 2d θlog 8√8T1+ 2γ
−
8T
b
1 L− 4b a2
2
.
(cid:18) 1 1 (cid:19)
(cid:16) (cid:17)
21Now, we choose T large enough so that:
2d θlog
8√8T1+ 2γ
−
8T
b
1 L− 4b a2
2 ≤
−16T b1 − Lb 42
a2
1 1 1 1
(cid:16) (cid:17)
⇐⇒
T1 −b2
≥
32d θb 1L4 1a2log 8√8T1+ 2γ
(cid:16) 1+γ (cid:17)
T1 b2 C log(8√8)+ logT1 b2 ,
− −
⇐⇒ ≥ 2(1 b )
(cid:18) − 2 (cid:19)
where C = 32d b L4a2. Therefore, it suffices to require that:
θ 1 1
C(1+γ)
T1 b2 Clog 8√8 , T1 b2 logT1 b2. (53)
− − −
≥ ≥ 2(1 b )
2
(cid:16) (cid:17) −
By [6, Lemma A.4], the right inequality holds when:
C(1+γ) 2C(1+γ)
T1 b2 log .
−
≥ 1 b 1 b
− 2 (cid:18) − 2 (cid:19)
Ifalltheaboverequirements on T hold, inorderfor r2 todominate thethirdterm ontheright-hand
side of (48), it suffices to have:
2d T1 b2 L2B 2
4L2B 2exp θ − 1 θ
1 θ − C ≤ T1+γ
(cid:18) (cid:19)
C
T1 b2 log(4T1+γ)
−
⇐⇒ ≥ 2d
θ
C 1+γ
T1 b2 log4+ logT1 b2 .
− −
⇐⇒ ≥ 2d 1 b
θ (cid:18) − 2 (cid:19)
Hence, it suffices to require that:
Clog4 C(1+γ)
T1 b2 , T1 b2 logT1 b2,
− − −
≥ 2d ≥ 2d (1 b )
θ θ 2
−
which is ensured by (53), given that d N and log(8√8) log4/2. Combining all of our
θ +
∈ ≥
conditions on T with the fact that γ < 1 (by assumption), we require that T T , where:
1
≥
T =max 32d b L4a2log 8√8
1/(1 −b2)
,
64d θb 1L4 1a2
log
128d θb 1L4 1a2 1/(1 −b2)
, (54)
1 θ 1 1 1 b 1 b
( (cid:16) (cid:16) (cid:17)(cid:17) (cid:18) − 2 (cid:18) − 2 (cid:19)(cid:19) )
which completes the proof.
C Proof of Lemma 1
Employing the Taylor expansion given in (12), the martingale offset of θ can be written as follows:
T 1
M (θ)= 1 − 4W g (X ,θ) g2(X ,θ) b
T T t t t − t t
Xt=0 (cid:16) (cid:17)
bT 1 b b 2
= 1 − 4W Z⊺ (θ θ )+ 1 (θ θ )⊺V (θ θ ) Z⊺ (θ θ )+ 1 (θ θ )⊺V (θ θ ) .
T t t − ⋆ 2 − ⋆ t − ⋆ − t − ⋆ 2 − ⋆ t − ⋆
" #
t=0 (cid:18) (cid:19) (cid:18) (cid:19)
X
b b b b b b (55)
22Invoking (21) from Lemma A.1 and Cauchy-Schwarz inequality, (55) yields:
T 1
M (θ) 1 − 4W Z⊺ (θ θ )+2W (θ θ )⊺V (θ θ )
T ≤ T t t − ⋆ t − ⋆ t − ⋆
"
t=0
X
b b b b 2
1 1
Z⊺ (θ θ ) 2 + (θ θ )⊺V (θ θ )
− 2 t − ⋆ 2 − ⋆ t − ⋆
#
(cid:18) (cid:19)
(cid:0) (cid:1)
T 1b b T 1b
M¯ (θ)+ 2 − W V θ θ 2+ 1 − (θ θ )⊺V (θ θ ) 2 , (56)
T t t ⋆ ⋆ t ⋆
≤ T k − k 4T − −
(cid:13) t=0 (cid:13) t=0
(cid:13) X (cid:13) X(cid:0) (cid:1)
¯ b (cid:13) (cid:13) b b b
where M (θ) is defined as in (1(cid:13)4). For each(cid:13)t, we have:
T
(θ θ )⊺V (θ θ ) V θ θ 2 L θ θ 2, (57)
b ⋆ t ⋆ t ⋆ 2 ⋆
| − − |≤ k kk − k ≤ k − k
where the last inequality follows from Assumption 3. Combining (56) and (57), we obtain:
b b b b
¯ 2 T −1 L2
M (θ) M (θ)+ W V θ θ 2+ 2 θ θ 4,
T T t t ⋆ ⋆
≤ T k − k 4 k − k
(cid:13) t=0 (cid:13)
(cid:13) X (cid:13)
b b (cid:13) (cid:13) b b
which completes the proof. (cid:13) (cid:13)
D Proof of Theorem 3
Let us define:
T 1
1 − ⊺
Σ = Z Z (58)
T T t t
t=0
X
and: b
T 1
Σ¯ = 1 − E(Z Z⊺ ), (59)
T t t
t=0
X
where Z = f (X ,θ ), as in the Taylor expansion given in (12), for all t = 0,...,T 1.
t θ t t ⋆
∇ −
First, we want to derive high-probability inequalities of the form:
¯ ¯
C Σ Σ C Σ,
1 T 2
(cid:22) (cid:22)
where C and C are positive constants. We typically refer to the right inequality as upper isometry
1 2 b
and the left inequality as lower isometry. We present our results in the next two lemmas, which
rely on the concentration inequalities (33) and (34) from Lemma B.1.
Lemma D.1 (Upper Isometry). Consider the predictor model (1) and the parameter class M under
¯
Assumptions 1, 3 and 4. Let Σ and Σ be as in (58) and (59), respectively. Then, if T T ,
T 21
≥
where:
Tb
=
24d θb 1L2
1 log
6L
1
1/(1 −b2)
, (60)
21
λ √λ
(cid:18) 0 (cid:18) 0(cid:19)(cid:19)
we have:
P Σ 8Σ¯ exp λ 0T1 −b2 .
T 6(cid:22) ≤ − 24b L2
(cid:18) 1 1 (cid:19)
(cid:16) (cid:17)
b
23Proof. Fix any u Sdθ−1 and for each t, set g t(X t,θ ⋆) = (u⊺Z t)2/T. By Cauchy-Schwarz inequality
∈
and Assumption 3, notice that g (X ,θ ) L2/T, for all t = 0,...,T 1. Employing (33) from
t t ¯ ⋆ ≤ 1 −
Lemma B.1 with θ = θ and s = 1u⊺Σu, we obtain:
⋆ 2
P 1
T −1
(u⊺Z )2 3 (u⊺Σ¯ 1/2)2 exp
(u⊺Σ¯
u)T . (61)
T
t=0
t ≥ 2
!
≤ (cid:18)−12L2 1kΓ dep(PX) k2
(cid:19)
X
Invoking Assumption 1, (61) yields:
P 1 T −1 (u⊺Z )2 3 (u⊺Σ¯ 1/2)2 exp (u⊺Σ¯ u)T1 −b2 . (62)
T t ≥ 2 ≤ − 12b L2
t=0 ! (cid:18) 1 1 (cid:19)
X
Fix any ε (0,1] and let
ε
be an ε-net of Sdθ−1 with respect to the norm . Moreover, define
∈ N k·k
the event:
T 1
= 1 − (u⊺ Z )2 3 (u⊺ Σ¯ 1/2)2 .
E T i t ≥ 2 i
( )
ui[∈Nε Xt=0
By a union bound and (62), we can write:
T 1
P( ) P 1 − (u⊺ Z )2 3 (u⊺ Σ¯ 1/2)2
E ≤ T i t ≥ 2 i
!
u Xi∈Nε Xt= (u0
⊺ Σ¯ u )T1 b2
exp i i −
≤ − 12b L2
u Xi∈Nε (cid:18) (u1
⊺
Σ¯1
u
)T(cid:19)
1 b2
max exp i i −
≤ |Nε |ui∈Nε (cid:18)− 12b 1L2
1 (cid:19)
¯
(u⊺Σu)T1 b2
−
sup exp
≤ |Nε | u Sdθ−1 (cid:18)− 12b 1L2 1 (cid:19)
∈
¯
= exp
inf
u
Sdθ−1(u⊺Σu)T1 −b2
. (63)
|Nε | − ∈ 12b L2
1 1 !
Combining (63) with (24) from Lemma A.2 and Assumption 4, we get:
3 dθ λ T1 b2
P( ) exp 0 − . (64)
E ≤ ε − 12b L2
(cid:18) (cid:19) (cid:18) 1 1 (cid:19)
By definition of ε, for every u Sdθ−1 there exists u
i ε
such that u u
i
ε, and thus on
N ∈ ∈ N k − k ≤
24the complement c of we have:
E E
T 1 T 1 T 1
1 − (u⊺Z )2 2 − (u⊺ Z )2+ 2 − ((u u )⊺Z )2 (from (21))
T t ≤ T i t T − i t
t=0 t=0 t=0
X X X
T 1
3(u⊺ Σ¯ 1/2)2+ 2 − ((u u )⊺Z )2 ( c)
≤ i T − i t E
t=0
X
T 1
6(u⊺Σ¯ 1/2)2+6((u u )⊺Σ¯ 1/2)2+ 2 − ((u u )⊺Z )2 (from (21))
i i t
≤ − T −
t=0
X
6(u⊺Σ¯
1/2)2+8ε2L2, (65)
≤ 1
where the last step follows from Cauchy-Schwarz inequality, Assumption 3, and u u ε. By
i
Assumptions 3 and 4, we have 0 < λ
u⊺Σ¯
u =
(u⊺Σ¯
1/2)2 L2. Hence,
settingk
ε
=− √λ0k ,≤
we have
0 ≤ ≤ 1 2L1
ε (0,1] and (65) yields:
∈
T 1
1 − (u⊺Z )2 6(u⊺Σ¯ 1/2)2+2λ 8(u⊺Σ¯ 1/2)2 u⊺Σ u 8u⊺Σ¯ u. (66)
t 0 T
T ≤ ≤ ⇐⇒ ≤
t=0
X
b
From (64) and (66) we get:
P ∃u
∈
Sdθ−1 :u⊺(Σ
T
−8Σ¯ )u
≥
0
≤
√6L λ1 dθ exp −λ 10 2T b1 L−b 22 ,
(cid:18) 0(cid:19) (cid:18) 1 1 (cid:19)
(cid:16) (cid:17)
which implies that: b
P Σ 8Σ¯ 6L 1 dθ exp λ 0T1 −b2 .
T 6(cid:22) ≤ √λ − 12b L2
(cid:18) 0(cid:19) (cid:18) 1 1 (cid:19)
(cid:16) (cid:17)
To finish the proof, it suffices tbo show that if T T 21, where T
21
is defined as in (60), we have:
≥
6L dθ λ T1 b2 λ T1 b2
1 0 − 0 −
exp exp
√λ − 12b L2 ≤ − 24b L2
(cid:18) 0(cid:19) (cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
6L λ T1 b2 λ T1 b2
1 0 − 0 −
exp d log exp
⇐⇒ θ √λ − 12b L2 ≤ − 24b L2
(cid:18) (cid:18) 0(cid:19) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
24d b L2 6L
T1 b2 θ 1 1 log 1 ,
−
⇐⇒ ≥ λ √λ
0 (cid:18) 0(cid:19)
which is true by the last inequality.
(cid:4)
Lemma D.2 (Lower Isometry). Consider the predictor model (1) and the parameter class M under
¯
Assumptions 1, 3 and 4. Let Σ and Σ be as in (58) and (59), respectively. Then, if T T ,
T 22
≥
where:
Tb
=
16d θb 1L2
1 log
15L
1
1/(1 −b2)
, (67)
22
λ √λ
(cid:18) 0 (cid:18) 0 (cid:19)(cid:19)
we have:
P Σ 1 Σ¯ exp λ 0T1 −b2 .
T 6(cid:23) 16 ≤ − 16b L2
(cid:18) (cid:19) (cid:18) 1 1 (cid:19)
b
25Proof. Fix any u Sdθ−1 and for each t, set g t(X t,θ ⋆) = (u⊺Z t)2/T. By Cauchy-Schwarz inequality
∈
and Assumption 3, notice that g (X ,θ ) L2/T, for all t = 0,...,T 1. Employing (34) from
t t ¯ ⋆ ≤ 1 −
Lemma B.1 with θ = θ and s = 1u⊺Σu, we obtain:
⋆ 2
P 1
T −1
(u⊺Z )2 1 (u⊺Σ¯ 1/2)2 exp
(u⊺Σ¯
u)T . (68)
T
t=0
t ≤ 2
!
≤ (cid:18)−8L2 1kΓ dep(PX) k2
(cid:19)
X
Invoking Assumption 1, (68) yields:
P 1 T −1 (u⊺Z )2 1 (u⊺Σ¯ 1/2)2 exp (u⊺Σ¯ u)T1 −b2 . (69)
T t ≤ 2 ≤ − 8b L2
t=0 ! (cid:18) 1 1 (cid:19)
X
Fix any ε (0,1] and let
ε
be an ε-net of Sdθ−1 with respect to the norm . Moreover, define
∈ N k·k
the event:
T 1
= 1 − (u⊺ Z )2 1 (u⊺ Σ¯ 1/2)2 .
E T i t ≤ 2 i
( )
ui[∈Nε Xt=0
By a union bound and (69), we can write:
T 1
P( ) P 1 − (u⊺ Z )2 1 (u⊺ Σ¯ 1/2)2
E ≤ T i t ≤ 2 i
!
u Xi∈Nε Xt= (u0
⊺ Σ¯ u )T1 b2
exp i i −
≤ − 8b L2
u Xi∈Nε (cid:18) (u1
⊺
Σ¯1
u
)T(cid:19)
1 b2
max exp i i −
≤ |Nε |ui∈Nε (cid:18)− 8b 1L2
1 (cid:19)
¯
(u⊺Σu)T1 b2
−
sup exp
≤ |Nε | u Sdθ−1 (cid:18)− 8b 1L2 1 (cid:19)
∈
¯
= exp
inf
u
Sdθ−1(u⊺Σu)T1 −b2
. (70)
|Nε | − ∈ 8b L2
1 1 !
Combining (70) with (24) from Lemma A.2 and Assumption 4, we get:
3 dθ λ T1 b2
P( ) exp 0 − . (71)
E ≤ ε − 8b L2
(cid:18) (cid:19) (cid:18) 1 1 (cid:19)
By definition of ε, for every u Sdθ−1 there exists u
i ε
such that u u
i
ε, and thus on
N ∈ ∈ N k − k ≤
26the complement c of we have:
E E
T 1 T 1 T 1
1 − (u⊺Z )2 1 − (u⊺ Z )2 1 − ((u u )⊺Z )2 (from (21))
T t ≥ 2T i t − T − i t
t=0 t=0 t=0
X X X
T 1
1 (u⊺ Σ¯ 1/2)2 1 − ((u u )⊺Z )2 ( c)
≥ 4 i − T − i t E
t=0
X
T 1
1 (u⊺Σ¯ 1/2)2 1 ((u u )⊺Σ¯ 1/2)2 1 − ((u u )⊺Z )2 (from (21))
i i t
≥ 8 − 4 − − T −
t=0
X
1 (u⊺Σ¯
1/2)2
5
ε2L2, (72)
≥ 8 − 4 1
where the last step follows from Cauchy-Schwarz inequality, Assumption 3, and u u ε. By
i
Assumptions 3 and 4, we have 0 < λ u⊺Σ¯ u = (u⊺Σ¯ 1/2)2 L2. Hence, settink g ε− = k √≤ λ0 , we
0 ≤ ≤ 1 √20L1
have ε (0,1] and (72) yields:
∈
T 1
1 − (u⊺Z )2 1 (u⊺Σ¯ 1/2)2 λ 0 1 (u⊺Σ¯ 1/2)2 uTΣ u 1 u⊺Σ¯ u. (73)
t T
T ≥ 8 − 16 ≥ 16 ⇐⇒ ≥ 16
t=0
X
b
From (71) and (73) we get:
P ∃u
∈
Sdθ−1 : u⊺ Σ
T
−
11 6Σ¯ u
≤
0
≤
1 √5 λL 1 dθ exp −λ 80 bT1 L− 2b2 ,
(cid:18) (cid:18) (cid:19) (cid:19) (cid:18) 0 (cid:19) (cid:18) 1 1 (cid:19)
which implies that: b
P Σ 1 Σ¯ 15L 1 dθ exp λ 0T1 −b2 .
T 6(cid:23) 16 ≤ √λ − 8b L2
(cid:18) (cid:19) (cid:18) 0 (cid:19) (cid:18) 1 1 (cid:19)
To finish the proof, it sufficesbto show that if T T 22, where T
22
is defined as in (67), we have:
≥
15L dθ λ T1 b2 λ T1 b2
1 0 − 0 −
exp exp
√λ − 8b L2 ≤ − 16b L2
(cid:18) 0 (cid:19) (cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
15L λ T1 b2 λ T1 b2
1 0 − 0 −
exp d log exp
⇐⇒ θ √λ − 8b L2 ≤ − 16b L2
(cid:18) (cid:18) 0 (cid:19) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
16d b L2 15L
T1 b2 θ 1 1 log 1 ,
−
⇐⇒ ≥ λ √λ
0 (cid:18) 0 (cid:19)
which is true by the last inequality.
(cid:4)
Now, we can prove Theorem 3. We start by showing an upper bound of order (logT/T) for
E[su Fp ixθ ∈ aM nM y¯ εT( >θ) 0], aw nh der lee tM¯ T( bθ) ei as nd εe -fi nn ee td oa fs MM¯ T :=(θ) θin ( θ14) θwith Mθ .re Sp imla ic le ad
r
tb oy [θ 2, 9,fo LO r ea mll mθ a∈ 1M 0],.
we
ε ⋆ ⋆
N { − | ∈ }
can show that: b b
E supM¯ (θ) 16σ w2 log N(ε,M, k·k) +32σ L ε. (74)
T w 1
(cid:20)θ M (cid:21) ≤ T
∈
27Particularly, note that invoking Assumption 2, similar to [29, Lemma 9] we have:
E max M¯ (θ) 16σ w2 log |Nε |. (75)
T
(cid:20)θ −θ⋆ ∈Nε (cid:21) ≤ T
¯
Moreover,observethatbysimplydiscardingthenegativesecond-orderterminM (θ)andemploying
T
Assumption 3 along with a standard sub-Gaussian concentration inequality for E W , we get:
t
| |
T 1
E sup M¯ (θ) E sup 4 − W g (X ,θ)
 T   t t t 
θ M ≤ θ M T
kθ −∈θ⋆ k≤ε

kθ −∈θ⋆ k≤ε Xt=0

   
T 1
4 −
E sup W f (X ,θ) f (X ,θ )
 t t t t t ⋆ 
≤ θ M T | || − |
kθ −∈θ⋆ k≤ε Xt=0

 
T 1
4 −
E sup W L θ θ
 t 1 ⋆ 
≤ θ M T | | k − k
kθ −∈θ⋆ k≤ε Xt=0

 
16σ L ε. (76)
w 1
≤
A standard one-step discretization bound (c.f. the proof of [5, Proposition 5.17]) yields:
E supM¯ (θ) E max M¯ (θ) +2E sup M¯ (θ) . (77)
T T  T 
(cid:20)θ ∈M (cid:21) ≤ (cid:20)θ −θ⋆ ∈Nε (cid:21)
kθ
−θ ∈θ⋆M
k≤ε

 
Combining (77) with (75) and (76), and noting that (ε,M, ) = (by translation invariance
ε
N k·k |N |
of the metric ), we complete the proof of (74).
k·k
Setting ε= Bθ in (74) and invoking (26) from Lemma A.3, we obtain:
T
E supM¯ (θ) 16σ w2d θlog(3T) + 32σ wL 1B θ . (78)
T
(cid:20)θ M (cid:21) ≤ T T
∈
Given that log(3T) log(3) > 1, for all T N , (78) yields:
+
≥ ∈
E supM¯ (θ) 16(σ w2d θ +2σ wL 1B θ)log(3T) . (79)
T
(cid:20)θ M (cid:21) ≤ T
∈
We have:
EM¯ (θ) supEM¯ (θ) E supM¯ (θ) 16(σ w2d θ +2σ wL 1B θ)log(3T) , (80)
T T T
≤ θ M ≤ (cid:20)θ M (cid:21) ≤ T
∈ ∈
b
where the last inequality follows from (79). Let us define the event = , where:
1 2
E E ∪E
¯ 1 ¯
= Σ 8Σ , = Σ Σ .
1 T 2 T
E 6(cid:22) E 6(cid:23) 16
(cid:26) (cid:27)
n o
b b
28By a union bound and Lemmas D.1 and D.2, if T max T ,T , where T , T are given by
21 22 21 22
≥ { }
(60) and (67), respectively, we have:
λ T1 b2 λ T1 b2 λ T1 b2
P( ) P( )+P( ) exp 0 − +exp 0 − 2exp 0 − . (81)
E ≤ E1 E2 ≤ − 24b L2 − 16b L2 ≤ − 24b L2
(cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
By Assumption 4, on the complement c of we have:
E E
Σ
1 Σ¯ λ 0I
0. (82)
T
(cid:23) 16 (cid:23) 16
dθ
≻
Hence, by maximizing the quadratic fubnction M¯ T(θ)over all θ Rdθ on c and performingstraight-
∈ E
forward algebraic manipulations, we obtain:
T 1 2 T 1 2
M¯ (θ) sup M¯ (θ) 8 − W Z⊺ (TΣ ) 1/2 = 16 − W Z⊺ (2TΣ ) 1/2
T ≤ T ≤ T t t T − T t t T −
θ Rdθ (cid:13) t=0 (cid:13) (cid:13) t=0 (cid:13)
∈ (cid:13)X (cid:13) (cid:13)X (cid:13)
b T 1 (cid:13) 2b (cid:13) (cid:13) b (cid:13)
16 − W Z⊺ (Σ+(cid:13) TΣ ) 1/2 , (cid:13) (cid:13) (cid:13) (83)
≤ T t t T −
(cid:13) t=0 (cid:13)
(cid:13)X (cid:13)
(cid:13) b (cid:13)
¯
wherethelastinequa(cid:13)lityfollows from(82)forΣ(cid:13)= T Σ. Fixanyδ (0,1). Employing[30,Theorem
16 ∈
4.1] (by Assumption 2) and (83), we deduce that with probability at least 1 δ, on c we have:
− E
M¯
(θ)
16
4σ2 log
det(Σ+TΣ T)
+8σ2 log
5
T ≤ T w det(Σ) w δ
!
(cid:18) (cid:18) (cid:19)(cid:19)
b
b 16 5
= 4σ2 logdet(I +TΣ Σ 1)+8σ2 log
T w dθ T − w δ
(cid:18) (cid:18) (cid:19)(cid:19)
16 4σ2 logdet I +Tb (8Σ¯ ) 16 Σ¯ 1 +8σ2 log 5 ( c)
≤ T w dθ T − w δ E1
(cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)(cid:19)
16 5
= 4σ2 log129dθ +8σ2 log
T w w δ
(cid:18) (cid:18) (cid:19)(cid:19)
128σ2 5 12dθ
w log · . (84)
≤ T δ
(cid:18) (cid:19)
For any s > 128 Tσ w2 log(5 ·12dθ), we can select δ
∈
(0,1) such that s = 128 Tσ w2 log 5 ·1 δ2dθ , and (84)
yields: (cid:16) (cid:17)
P M¯ (θ) > s c 5 12dθ exp Ts .
T E ≤ · −128σ2
(cid:18) w(cid:19)
(cid:16) (cid:12) (cid:17)
Define the event ˜= M¯ (θ) < 0b . We h(cid:12) ave:
T
E
E M¯ (θ) (cid:8) c =bE 1 (cid:9) M¯ (θ) c +E 1 M¯ (θ) c E 1 M¯ (θ) c ,
T
E
˜c T
E
˜ T
E ≤
˜c T
E
E E E
h (cid:12) i h (cid:12) i h (cid:12) i h (cid:12) i
b (cid:12) b (cid:12) b (cid:12) b (cid:12)
29where the last step follows from the fact that E 1 M¯ (θ) c < 0, by definition of ˜. Therefore,
˜ T
E E
E
we have: h (cid:12) i
b (cid:12)
E M¯ (θ) c E 1 M¯ (θ) c = ∞P M¯ (θ) > s c ds
T
E ≤
˜c T
E
T
|E
h b (cid:12) (cid:12) i h 128
TE
σw2 log(5b ·12(cid:12) (cid:12)dθ) 1i
ds+Z0
∞
(cid:16) b
5
12(cid:17)
dθ exp
Ts
ds
≤ Z0 Z128 Tσw2 log(5 ·12dθ) · (cid:18)−128σ w2 (cid:19)
=
128σ w2 log(5 ·12dθ)
+
∞
exp
Ts
ds
T −128σ2
Z0 (cid:18) w(cid:19)
128σ2(log5+d log12) 128σ2
= w θ + w
T T
128σ2d log60 128σ2d
w θ + w θ (d N )
θ +
≤ T T ∈
653d σ2
θ w. (85)
≤ T
Invoking the law of total expectation along with (80) and (85), we can write:
EM¯ (θ)= E M¯ (θ) c P( c)+E M¯ (θ) P( )
T T T
E E E E
65h3d σ2 (cid:12) i 16(σ2d h+2σ L(cid:12) Bi )log(3T)
b θ wbP(cid:12)( c)+ w θ wb (cid:12)1 θ P( ). (86)
≤ T E T E
Employing the fact that P( c) 1 as well as (81), (86) yields:
E ≤
EM¯
(θ)
653d θσ w2
+
Clog(3T)
exp
λ 0T1 −b2
, (87)
T ≤ T 3T − 24b L2
(cid:18) 1 1 (cid:19)
b
where C = 96(σ2d +2σ L B ). Now we choose T large enough so that:
w θ w 1 θ
λ T1 b2 λ T1 b2
0 − 0 −
Cexp exp
− 24b L2 ≤ − 48b L2
(cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
λ T1 b2 λ T1 b2
0 − 0 −
exp logC exp
⇐⇒ − 24b L2 ≤ − 48b L2
(cid:18) 1 1 (cid:19) (cid:18) 1 1 (cid:19)
48b L2
T1 b2 1 1 logC.
−
⇐⇒ ≥ λ
0
Moreover, for any fixed γ (0,1), we require that:
∈
λ T1 b2 1 48b L2 48b L2(1+γ)
exp 0 − T1 b2 1 1 logT1+γ T1 b2 1 1 logT1 b2.
− 48b L2 ≤ T1+γ ⇐⇒ − ≥ λ ⇐⇒ − ≥ λ (1 b ) −
(cid:18) 1 1 (cid:19) 0 0 − 2
By [6, Lemma A.4], the latter inequality holds when:
96b L2(1+γ) 192b L2(1+γ)
T1 b2 1 1 log 1 1 ,
−
≥ λ (1 b ) λ (1 b )
0 − 2 (cid:18) 0 − 2 (cid:19)
30and given that γ < 1, it suffices to have:
192b L2 384b L2
T1 b2 1 1 log 1 1 .
−
≥ λ (1 b ) λ (1 b )
0 − 2 (cid:18) 0 − 2 (cid:19)
Combining all of our requirements on T, we need to have T max T ,T ,T , where:
21 22 23
≥ { }
1 1
T = max
192b 1L2
1 log
384b 1L2
1
1−b2
,
48b 1L2 1log(96(σ w2d
θ
+2σ wL 1B θ)) 1−b2
.
23
λ (1 b ) λ (1 b ) λ
( (cid:18) 0 − 2 (cid:18) 0 − 2 (cid:19)(cid:19) (cid:18) 0 (cid:19) )
(88)
That is, we set:
T = max T ,T ,T , (89)
2 21 22 23
{ }
where T , T , T are given by (60), (67), and (88), respectively. Then, given that log(3T) 3T,
21 22 23
≤
for all T N , (86) yields:
+
∈
EM¯
(θ)
653d θσ w2
+
1
,
T ≤ T T1+γ
which completes the proof. b
E Proof of Corollary 1
We want to find an upper bound for EM (θ), employing the inequality:
T
EM (θ) EM¯ (θ)+E
2bT −1
W V θ θ 2 +
L2
2E θ θ 4, (90)
T T t t ⋆ ⋆
≤ T k − k 4 k − k
"(cid:13) t=0 (cid:13) #
(cid:13) X (cid:13)
b b (cid:13) (cid:13) b b
which follows directly from Lemma 1 (see(cid:13)(15)). The(cid:13)first term on the right-hand side of (90) can
be bounded using (16) from Theorem 3.
We proceed with bounding the second term on the right-hand side of (90). For every θ M, let
∈
M (θ) be defined as M (θ) with θ replaced by θ. Moreover, set M = θ θ θ M . Invoking
T T ⋆ ⋆
{ − | ∈ }
Assumption 3, similar to [39, Theorem 4.2.2], we can show that for any fixed u,v,w 0, with
≥
probability at least 1 3ebxp( u2 )b exp( v) eexp( w), we have:
− − 2 − −2 − −
σ2 α vσ2
supM (θ) inf 16 wL δσ + w log (ε,M , )dε+ w
T 1 w ⋆
θ ∈M ≤ α>0,δ ∈[0,α] (cid:18) r T Zδ/2
p
N k·k T
σ2 log (α,M , ) uL ασ
+ w N ⋆ k·k + 1 w +L2α2 .
T √T 1
(cid:19)
Note that for any ε > 0, we have (ε,M , ) = (ε,M, ), by translation invariance of
⋆
N k · k N k · k
the metric . Hence, by selecting α = Bθ, δ = 2Bθ, u = 2 log(3T), v = 2log(3T2), and
k · k T T
w = 1+log(3T2), we conclude that with probability at least 1 1 :
− Tp2
vσ2 σ2 log (B /T,M, ) uL B σ L2B 2
supM (θ) 16 wL δσ + w + w N θ k·k + 1 θ w + 1 θ . (91)
θ M T ≤ (cid:18) 1 w T T T3/2 T2 (cid:19)
∈
31Combining(91)with(26)fromLemmaA.3andperformingstraightforwardalgebraicmanipulations,
we obtain:
1
P supM (θ) > r2 , (92)
(cid:18)θ M T (cid:19) ≤ T2
∈
where r2 = C log(3T) with:
1 T
C = 16(d σ2 +4σ2 +10σ L B +L2B 2).
1 θ w w w 1 θ 1 θ
Define the event:
= θ θ > r¯ ,
⋆
E k − k
n o
where: b
2aL2B 2
r¯= 8ar2+ 1 θ .
s T
If T T , where T is defined as in (54), from (11) and (92) we can write:
1 1
≥
P( ) =P θ θ 2 > r¯2
⋆
E k − k
(cid:16) 2a(cid:17) L2B 2 2aL2B 2
P 8baM (θ)+ 1 θ > 8ar2+ 1 θ
T
≤ T T
(cid:18) (cid:19)
=P M (θ) >b r2
T
(cid:16) (cid:17) 1
P supMb (θ)> r2 . (93)
≤ (cid:18)θ M T (cid:19) ≤ T2
∈
We can decompose the second term on the right-hand side of (90) as follows:
T 1
2 −
E W V θ θ 2 = E +E , (94)
t t ⋆ 1 2
T k − k
"(cid:13) t=0 (cid:13) #
(cid:13) X (cid:13)
(cid:13) (cid:13) b
where: (cid:13) (cid:13)
T 1
2 −
E = E 1 W V θ θ 2
1 c t t ⋆
" E (cid:13)T
t=0
(cid:13)k − k #
(cid:13) X (cid:13)
and: (cid:13) (cid:13) b
(cid:13) (cid:13)
T 1
2 −
E = E 1 W V θ θ 2 .
2 t t ⋆
" E (cid:13)T
t=0
(cid:13)k − k #
(cid:13) X (cid:13)
First, we want to bound the term E . Let(cid:13)us define: (cid:13) b
1 (cid:13) (cid:13)
M¯r = θ M θ θ
⋆
r¯ ,
∈ k − k ≤
n (cid:12) o
and (somewhat abusing notation): (cid:12)
(cid:12)
V (θ)= 2f (X ,α (θ θ )+θ ),
t θ t t t ⋆ ⋆
∇ −
32where α are the same as in the definition of V in (12), for all θ M. Then, we have:
t t
∈
T 1 T 1
2 − 2 −
E E sup W V (θ) θ θ 2 ¯r2E sup W V (θ) . (95)
1 t t ⋆ t t
≤ "θ ∈M r¯(cid:13) (cid:13)T Xt=0 (cid:13) (cid:13)k − k # ≤ "θ ∈M r¯(cid:13) (cid:13)T Xt=0 (cid:13) (cid:13)#
(cid:13) (cid:13) (cid:13) (cid:13)
Fix any θ M¯r and any u(cid:13) Sdθ−1, and de(cid:13)fine the random variable:(cid:13) (cid:13)
∈ ∈
T 1
2 −
R (θ)= u⊺ W V (θ) u.
u t t
T
!
t=0
X
By tower property and Assumptions 2, 3, for any λ R, we can write:
∈
T 1
2λ −
Eexp(λR (θ))= Eexp W (u⊺V (θ)u)
u t t
T
!
t=0
X
T 2
2λ − 2λ
= E exp W (u⊺V (θ)u) E exp W (u⊺V (θ)u)
t t T 1 T 1 T 2
T T − − F −
(cid:20) (cid:18) Xt=0 (cid:19) (cid:20) (cid:18) (cid:19) (cid:12) (cid:21)(cid:21)
(cid:12)
2λT −2 2λ2σ2L2 (cid:12)
E exp W (u⊺V (θ)u) exp w 2
≤ T t t T2
(cid:20) (cid:18) t=0 (cid:19) (cid:18) (cid:19)(cid:21)
X
...
≤
2λ2σ2L2
Eexp w 2 ,
≤ T
(cid:18) (cid:19)
which implies that R (θ) is sub-Gaussian with parameter σ = (2σ L )/√T. From standard results
u w 2
for sub-Gaussian random variables we deduce that for any s > 0:
Ts2
P( R (θ) > s) 2exp . (96)
| u | ≤ −8σ2L2
(cid:18) w 2(cid:19)
Fix any ε (0,1/2) and let
ε
be an ε-net of Sdθ−1. Then, from [30, Lemma 2.5] and (96) we
∈ N
conclude that:
2
T −1
3
dθ
P W V (θ) > s maxP( R (θ) > (1 2ε)s)
t t u
(cid:13)T t=0 (cid:13) !≤ (cid:18)ε (cid:19) u ∈Nε | | −
(cid:13) X (cid:13)
(cid:13) (cid:13) 3 dθ T(1 2ε)2s2
(cid:13) (cid:13) 2 exp − . (97)
≤ ε − 8σ2L2
(cid:18) (cid:19) (cid:18) w 2 (cid:19)
Setting ε = 1/4, (97) yields:
2 T −1 Ts2
P W V (θ) > s 2 12dθ exp
T t t ≤ · −32σ2L2
(cid:13) t=0 (cid:13) ! (cid:18) w 2(cid:19)
(cid:13) X (cid:13)
(cid:13) (cid:13) Ts2
(cid:13) (cid:13) 24dθ exp , (d N ) (98)
≤ −32σ2L2 θ ∈ +
(cid:18) w 2(cid:19)
33for all θ
∈
M. Let N¯r′ be a r¯-net of M¯r,⋆ := {θ −θ ⋆ |θ
∈
M¯r }. Then, by a union bound, (98) yields:
2 T −1 Ts2
P θm i∈a Nx r¯′ (cid:13)T
t=0
W tV t(θ i +θ ⋆)
(cid:13)
> s !≤ |N¯r′ |24dθ exp (cid:18)−32σ w2L2 2(cid:19). (99)
(cid:13) X (cid:13)
(cid:13) (cid:13)
Similar to (26) from Lem(cid:13)ma A.3, we can show(cid:13)that:
3r¯ dθ
|N¯r′
|
= N(r¯,M¯r,⋆, k·k)
≤
r¯ = 3dθ.
(cid:18) (cid:19)
Hence, from (99) we deduce that:
2 T −1 Ts2
P max W V (θ +θ ) > s 72dθ exp
θi∈Nr¯′ (cid:13)T
t=0
t t i ⋆
(cid:13) !
≤ (cid:18)−32σ w2L2
2(cid:19)
(cid:13) X (cid:13)
(cid:13) (cid:13)
and thus we have: (cid:13) (cid:13)
T 1 T 1
E max 2 − W V (θ +θ ) = ∞P max 2 − W V (θ +θ ) > s ds
t t i ⋆ t t i ⋆
"θi∈Nr¯′ (cid:13)T
t=0 (cid:13)# Z0
θi∈Nr¯′ (cid:13)T
t=0 (cid:13) !
(cid:13) X (cid:13) (cid:13) X (cid:13)
(cid:13) (cid:13) (cid:13) Ts2 (cid:13)
(cid:13) (cid:13) ∞ min 1,72(cid:13)dθ exp (cid:13)ds
≤ −32σ2L2
Z0 (cid:26) (cid:18) w 2(cid:19)(cid:27)
Ts2
= ∞ min 1,exp log72dθ ds
− 32σ2L2
Z0 (cid:26) (cid:18) w 2(cid:19)(cid:27)
=
32σ w2L2 2log72dθ
+
∞
exp
Ts2
ds
T −32σ2L2
r Z0 (cid:18) w 2(cid:19)
32d σ2L2log72 8πσ2L2
= θ w 2 + w 2
T T
r r
d σ2L2
θ w 2 32log72+√8π (d N )
θ +
≤ T ∈
r
18√d σ L(cid:16)p (cid:17)
θ w 2
. (100)
≤ √T
By definition of N¯r′, for every θ
∈
M¯r there exists θ i
∈
N¯r′ such that k(θ −θ ⋆) −θ i
k≤
r¯. We define
the operator π() that projects each θ M¯r on the corresponding vector θ
i
+θ ⋆. Then, for every
· ∈
θ M¯r, we have θ π(θ) r¯. Then, employing the triangle inequality along with Assumption 3,
∈ k − k≤
we can write:
T 1 T 1 T 1
2 − 2 − 2 −
E sup W V (θ) E sup W (V (θ) V (π(θ))) + W V (π(θ))
t t t t t t t
"θ ∈M r¯(cid:13) (cid:13)T Xt=0 (cid:13) (cid:13)# ≤ "θ ∈M r¯ (cid:13) (cid:13)T Xt=0 − (cid:13)
(cid:13)
(cid:13) (cid:13)T Xt=0 (cid:13) (cid:13)!#
(cid:13) (cid:13) (cid:13) T 1 (cid:13) (cid:13) T 1 (cid:13)
(cid:13) (cid:13) E sup (cid:13)2 − W V (θ) V (π(θ))(cid:13) + (cid:13)2 − W V (π(θ)) (cid:13)
t t t t t
≤ "θ ∈M r¯ T Xt=0 | |k − k (cid:13) (cid:13)T Xt=0 (cid:13) (cid:13)!#
T 1 T 1 (cid:13) (cid:13)
E 2 − W L r¯+E max 2 − W V(cid:13) (θ +θ ) . (cid:13)
t 3 t t i ⋆
≤ "T
t=0
| |
#
"θi∈Nr¯′ (cid:13)T
t=0
(cid:13)#
X (cid:13) X (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
34Invoking(100)andastandardsub-GaussianconcentrationinequalityforE W ,theaboveinequality
t
| |
yields:
T 1
E sup 2 − W V (θ) 8σ L ¯r+ 18√d θσ wL 2 . (101)
t t w 3
"θ ∈M r¯(cid:13) (cid:13)T Xt=0 (cid:13) (cid:13)# ≤ √T
Combining inequalities (95) and(cid:13)(101), we obta(cid:13)in:
(cid:13) (cid:13)
18√d σ L
E r¯2 8σ L ¯r+ θ w 2 . (102)
1 w 3
≤ √T
(cid:18) (cid:19)
Given that 1< log3 log(3T), for all T N , by definition of r¯we have:
+
≤ ∈
(8aC +2aL2B 2)log(3T)
¯r 1 1 θ (103)
≤ s T
and (102) yields:
(8aC +2aL2B 2)log(3T) (8aC +2aL2B 2)log(3T) 18√d σ L
E 1 1 θ 8σ L 1 1 θ + θ w 2
1 w 3
≤ T  s T √T 
(cid:18) (cid:19)
 
(8aC +2aL2B 2)log(3T) log(3T)
1 1 θ 8σ L 8aC +2aL2B 2+18 d σ L
≤ T w 3 1 1 θ θ w 2 T
(cid:18) (cid:19) (cid:18) q (cid:19)r !
p
3/2
log(3T)
= C , (104)
2
T
(cid:18) (cid:19)
where:
C = 8aC +2aL2B 2 8σ L 8aC +2aL2B 2+18 d σ L .
2 1 1 θ w 3 1 1 θ θ w 2
(cid:18) q (cid:19)
(cid:0) (cid:1) p
Now, we focus on bounding the term E appearing on the right-hand side of (94). Invoking the
2
Cauchy-Schwarz inequality, we can write:
T 1 2
2 −
E E W V θ θ 4 E12
2 ≤ v T t t k − ⋆ k
u "(cid:13) t=0 (cid:13) # E
u (cid:13) X (cid:13)
t (cid:13) (cid:13) b
(cid:13) 2 T −1 (cid:13) 2
E sup W V (θ) θ θ 4 P( ) (E12 = E1 = P( ))
≤ v u
u
"θ ∈M (cid:13) (cid:13)T Xt=0 t t (cid:13)
(cid:13)
k − ⋆ k # E E E E
t (cid:13) (cid:13)
(cid:13) 2 T −1 (cid:13) 2
4B 2 E sup W V (θ) P( ). (105)
≤ θ v u
u
"θ ∈M (cid:13) (cid:13)T Xt=0 t t (cid:13)
(cid:13)
# E
t (cid:13) (cid:13)
Let be a B /√T-net of M. B(cid:13)y definition of(cid:13) , for every θ M there exists θ such that
′′ θ ′′ i ′′
N N ∈ ∈ N
θ θ B /√T. Somewhat abusing notation, we define π() as the operator that projects each
i θ
k − k ≤ ·
θ M on the corresponding vector θ . Then, by a union bound, (98) yields:
i ′′
∈ ∈ N
2 T −1 Ts2
P θm i∈a Nx
′′ (cid:13)T
t=0
W tV t(θ i)
(cid:13)
> s
!
≤ |N′′
|24dθ exp
(cid:18)−32σ w2L2
2(cid:19). (106)
(cid:13) X (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
35Employing (26) from Lemma A.3, we get:
′′
= (B θ/√T,M, ) (3√T)dθ,
|N | N k·k ≤
and (106) yields:
2 T −1 Ts2
P max W V (θ ) > s (72√T)dθ exp .
θi∈N′′ (cid:13)T
t=0
t t i
(cid:13) !
≤ (cid:18)−32σ w2L2
2(cid:19)
(cid:13) X (cid:13)
(cid:13) (cid:13)
Therefore, we have: (cid:13) (cid:13)
T 1 2 T 1 2
E max 2 − W V (θ ) = ∞P max 2 − W V (θ ) > s ds
t t i t t i
"θi∈N′′ (cid:13)T
t=0 (cid:13) # Z0
θi∈N′′ (cid:13)T
t=0 (cid:13) !
(cid:13) X (cid:13) (cid:13) X (cid:13)
(cid:13) (cid:13) (cid:13) T 1 (cid:13)
(cid:13) (cid:13)
=
∞P
max
(cid:13)2 −
W V (θ
)(cid:13)
> √s ds
t t i
Z0
θi∈N′′ (cid:13)T
t=0 (cid:13) !
(cid:13) X (cid:13)
(cid:13) (cid:13) Ts
∞ min 1,(7(cid:13)2√T)dθ exp (cid:13) ds
≤ −32σ2L2
Z0 (cid:26) (cid:18) w 2(cid:19)(cid:27)
Ts
= ∞ min 1,exp log(72√T)dθ ds
− 32σ2L2
Z0 (cid:26) (cid:18) w 2(cid:19)(cid:27)
=
32σ w2L2 2log(72√T)dθ
+
∞
exp
Ts
ds
T −32σ2L2
Z0 (cid:18) w 2(cid:19)
32d σ2L2log(72√T) 32σ2L2
= θ w 2 + w 2
T T
32d σ2L2
θ w 2 log(72√T)+1 (d N )
θ +
≤ T ∈
176d
σ2L2(cid:16)
log(3T)
(cid:17)
θ w 2 , (107)
≤ T
where the last inequality follows by using the fact that 1 < log3 log(3T), for all T N , and
+
≤ ∈
performing straightforward algebraic manipulations. Employing (21) from Lemma A.1 along with
Cauchy-Schwarz inequality and Assumption 3, we can write:
T 1 2 T 1 2 T 1 2
2 − 2 − 2 −
W V (θ) 2 W (V (θ) V (π(θ))) +2 W V (π(θ))
t t t t t t t
T ≤ T − T
(cid:13) t=0 (cid:13) (cid:13) t=0 (cid:13) (cid:13) t=0 (cid:13)
(cid:13) X (cid:13) (cid:13) X (cid:13) (cid:13) X (cid:13)
(cid:13) (cid:13) (cid:13) T 1 T 1 (cid:13) (cid:13) T 1 (cid:13) 2
(cid:13) (cid:13) 8(cid:13) − W2 − V (θ) (cid:13) V (π(θ(cid:13) )) 2 +2 2 − W(cid:13) V (π(θ))
≤ T2 t k t − t k T t t
t=0 ! t=0 ! (cid:13) t=0 (cid:13)
X X (cid:13) X (cid:13)
T 1 T 1 (cid:13) 2 (cid:13)
8 − W2 (L θ π(θ) )2+2 2 − W V(cid:13) (π(θ)) (cid:13)
≤ T t 3 k − k T t t
t=0 ! (cid:13) t=0 (cid:13)
X (cid:13) X (cid:13)
T 1 2 T(cid:13)1 2 (cid:13)
8 − W2 L 3B θ +2 2 −(cid:13) W V (π(θ)) , (cid:13)
≤ T
t=0
t
! (cid:18)
√T
(cid:19)
(cid:13)T
t=0
t t
(cid:13)
X (cid:13) X (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
36for all θ M. Hence, we have:
∈
T 1 2 T 1 2 T 1 2
E sup 2 − W V (θ) E 8 − W2 L 3B θ +2E sup 2 − W V (π(θ))
"θ ∈M (cid:13) (cid:13)T Xt=0 t t (cid:13)
(cid:13)
# ≤ "T Xt=0 t # (cid:18) √T (cid:19) "θ ∈M (cid:13) (cid:13)T Xt=0 t t (cid:13)
(cid:13)
#
(cid:13) (cid:13) T 1 2 (cid:13) T 1 (cid:13)2
(cid:13) (cid:13) = E 8 − W2 L 3B θ +2E max(cid:13) 2 − W V (θ ) (cid:13) . (108)
"T
t=0
t
# (cid:18)
√T
(cid:19)
"θi∈N′′ (cid:13)T
t=0
t t i
(cid:13) #
X (cid:13) X (cid:13)
Invoking (107) and a standard sub-Gaussian concentration inequality f(cid:13) (cid:13)or EW2, (108) y(cid:13) (cid:13)ields:
t
E sup
2 T −1
W V (θ)
2 32σ w2L2 3B θ2
+
352d θσ w2L2 2log(3T)
t t
"θ ∈M (cid:13) (cid:13)T Xt=0 (cid:13)
(cid:13)
#≤ T T
(cid:13) (cid:13) 2d σ2(16L2B 2+176L2)log(3T)
(cid:13) (cid:13) θ w 3 θ 2 , (109)
≤ T
where the last step follows from the fact that d N and 1 < log3 log(3T), for all T N .
θ + +
∈ ≤ ∈
Combining (105) with (93) and (109), we obtain:
4B 2 2d σ2(16L2B 2+176L2)log(3T)
θ θ w 3 θ 2
E . (110)
2 ≤ q T3/2
Given that 1< log3 log(3T) log3(3T), for all T N , (110) yields:
+
≤ ≤ ∈
3/2
log(3T)
E C , (111)
2 3
≤ T
(cid:18) (cid:19)
where:
C = 4B 2 2d σ2(16L2B 2+176L2).
3 θ θ w 3 θ 2
q
Employing (104) and (111), (94) yields:
T 1 3/2
2 − log(3T)
E W V θ θ 2 (C +C ) . (112)
t t ⋆ 2 3
T k − k ≤ T
"(cid:13) t=0 (cid:13) # (cid:18) (cid:19)
(cid:13) X (cid:13)
(cid:13) (cid:13) b
Next, we derive a bo(cid:13)und for the t(cid:13)hird term on the right-hand side of (90). Notice that we can
bound E θ θ 4 as follows:
⋆
k − k
E θ θ 4 = E1 θ θ 4+E1 θ θ 4
k −
⋆bk Ec
k −
⋆
k Ek −
⋆
k
b sup bθ θ ⋆ 4 P( c b)+ sup θ θ ⋆ 4 P( ) (E1 = P( ),E1 c = P( c))
≤ (cid:18)θ M r¯k − k (cid:19) E (cid:18)θ Mk − k (cid:19) E E E E E
∈ ∈
16B 4
r¯4+ θ , (113)
≤ T2
where the last inequality follows from the definition of M¯r, the fact that P( c) 1, and (93). From
E ≤
(103) and (113) we conclude that:
(8aC +2aL2B 2)log(3T) 2 16B 4
E θ θ 4 1 1 θ + θ . (114)
k − ⋆ k ≤ T T2
(cid:18) (cid:19)
b
37Given that 1 < log3 log(3T) 3T, for all T N , (114) implies that the third term on the
+
≤ ≤ ∈
right-hand side of (90) can be bounded as follows:
L2 L2 log(3T) 2
2E θ θ 4 2 (8aC +2aL2B 2)2+16B 4
4 k − ⋆ k ≤ 4 1 1 θ θ T
(cid:18) (cid:19)
b
9L2(cid:0) (cid:1)
log(3T)
2
= 2 (8aC +2aL2B 2)2+16B 4
4 1 1 θ θ 3T
(cid:18) (cid:19)
9L2 (cid:0) (cid:1) log(3T) 3/2
2 (8aC +2aL2B 2)2+16B 4
≤ 4 1 1 θ θ 3T
(cid:18) (cid:19)
(cid:0) 3/2 (cid:1)
log(3T)
C , (115)
4
≤ T
(cid:18) (cid:19)
where:
C = L2 (8aC +2aL2B 2)2+16B 4 .
4 2 1 1 θ θ
Fix any γ (0,1/2). Recall that t(cid:0)he first term on the right-han(cid:1)d side of (90) can be bounded
∈
using (16) from Theorem 3, if T T , where T is defined as in (89). From (90), (112), and (115)
2 2
≥
we deduce that we can finish the proof of (17) by choosing T large enough so that:
3/2
log(3T) 1
max (C +C ),C .
{ 2 3 4 } T ≤ 2T1+γ
(cid:18) (cid:19)
Setting C = 2max (C +C ),C , it suffices to have:
5 2 3 4
{ }
T1/2 γ C log3/2(3T)
− 5
≥
T(1 2γ)/3 C2/3 log(3T)
⇐⇒ − ≥ 5
3
T(1 2γ)/3 C2/3 log3+ logT(1 2γ)/3 ,
⇐⇒ − ≥ 5 1 2γ −
(cid:18) − (cid:19)
which is satisfied if:
2/3
3C
T(1 2γ)/3 C2/3 log3, T(1 2γ)/3 5 logT(1 2γ)/3.
− ≥ 5 − ≥ 1 2γ −
−
By [6, Lemma A.4], the latter inequality holds when:
2/3 2/3
6C 12C
T(1 2γ)/3 5 log 5 .
−
≥ 1 2γ 1 2γ
!
− −
Combining all of our requirements on T, we need to have T max T ,T ,T , where T and T
1 2 3 1 2
≥ { }
are given by (54) and (89), respectively, and:
3/(1 2γ)
6C2/3 12C2/3 3/(1 −2γ)
T =max C2/3 log3 − , 5 log 5 ,
3  5 1 2γ 1 2γ 
!!
(cid:16) (cid:17) − − 
which completes the proof. 
38