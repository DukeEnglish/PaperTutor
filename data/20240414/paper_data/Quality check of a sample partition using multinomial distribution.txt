Quality check of a sample partition using multinomial distribution
Dr. Soumita Modak∗
Faculty, Department of Statistics
University of Calcutta, Basanti Devi College
147B, Rash Behari Ave, Kolkata- 700029, India
Email: soumitamodak2013@gmail.com
Orcid id: 0000-0002-4919-143X
Homepage: https://sites.google.com/view/soumitamodak
Abstract: In this paper, we advocate a novel measure for the purpose of
checking the quality of a cluster partition for a sample into several distinct
classes, and thus, determine the unknown value for the true number of clus-
ters prevailing the provided set of data. Our objective leads us to the de-
velopment of an approach through applying the multinomial distribution to
the distances of data members, clustered in a group, from their respective
cluster representatives. This procedure is carried out independently for each
of the clusters, and the concerned statistics are combined together to design
our targeted measure. Individual clusters separately possess the category-
wise probabilities which correspond to different positions of its members in
the cluster with respect to a typical member, in the form of cluster-centroid,
medoid or mode, referred to as the corresponding cluster representative. Our
method is robust in the sense that it is distribution-free, since this is devised
irrespective of the parent distribution of the underlying sample. It fulfills one
of the rare coveted qualities, present in the existing cluster accuracy mea-
sures, of having the capability to investigate whether the assigned sample
owns any inherent clusters other than a single group of all members or not.
Our measure’s simple concept, easy algorithm, fast runtime, good perfor-
mance, and wide usefulness, demonstrated through extensive simulation and
diverse case-studies, make it appealing.
keyword: Cluster Analysis; Checking for clusters; Quality assessment of a
cluster-partition; Evaluation of clusters’ number; Multinomial distribution.
1 Introduction
Scientists driven by the quest for discoveries of unveiled truth are equipped
with the unsupervised classification, where no prior information is known
regarding a new sample of observations and the sources of variation in the
1
4202
rpA
11
]PA.tats[
1v87770.4042:viXradatasetaretriedtobeexploredbyfindingmeaningfulclusters(Ruspini1970;
Hartigan 1975; McLachlan and Peel 2000; Everitt, Landau and Leese 2001;
Kaufman and Rousseeuw 2005; Balcan, Liang and Gupta 2014; Modak 2021;
Modak et al. 2022; Sureja, Chawda and Vasant 2022; Modak 2023a, 2023d,
2023g). We study the situation where each member of data is clustered
in exactly one group (Jain, Murty and Flynn 1999; Johnson and Wichern
2007; Modak et al. 2018; Modak 2019, 2023b, 2023f). Then, it becomes
straightforward to analysis the physical origins behind the resulting clusters.
However, neitheritiseasytofindouttheunknowntruevalueoftheprevailing
clusters (denoted by K) nor to verify whether a considered value of K is
correctinreality. Thisstillremainsaglobalissueinspiteofvariousmeasures,
based ondiverse perspectives, having been proposed to conquer the challenge
(Dunn 1974; Lewis and Gale 1994; Pakhira, Bandyopadhyay and Maulik
2004; Aljarah and Ludwig 2013; Silva, Melton and Wunsch 2020; Modak
2022,2023c,2023e). Moreover, thebulkofthehighlyacclaimedmeasureslike
Calin´ski and Harabasz index, Dunn index, average silhouette width, nearest
neighbor classification error rate or connectivity (Calin´ski & Harabasz 1974,
Dunn 1974, Rousseeuw 1987, Ripley 1996, Handl et al. 2005) are unable to
investigate whether there is at all any heterogeneity in a sample or not, i.e.,
K = 1 or K > 1. Without checking so, all efforts to look for a value of K
(assumed to be ≥ 2) would just lead us to spurious answer in case of a single-
cluster scenario. On the other hand, very few owning this characteristic
are either possessing heavy parametric assumptions and/or involving huge
computations, e.g., the Bayesian Information Criterion, Bayes factor or gap
statistic (Schwarz 1978; Frayley and Raftery 1998; Tibshirani, Walther and
Hastie 2001). In this context, we advise a novel, simple, distribution-free
and fast implemented index that checks the quality of a sample partition to
estimate K(≥ 1).
Our measure owns the robustness with respect to the distributions of the
originaldataorthecluster-populations. Itisbased onthemultinomial distri-
bution (McCullagh and Nelder 1989; Johnson, Kotz and Balakrishnan 1997)
applied to the distances of all members, in a cluster, computed from their
cluster representatives, wherein the category-wise probabilities represent the
different positions of the members in a particular cluster with respect to its
representative. Thus, it is free from the distributional assumptions regarding
the sample population. In the present study, we consider the widely success-
ful cluster indices Calin´ski and Harabasz index (Calin´ski & Harabasz 1974),
Dunn index (Dunn 1974), nearest neighbor classification error rate (Ripley
21996), gapstatistic (Tibshirani, Walther andHastie 2001), connectivity mea-
sure (Handl et al. 2005), and kernel-based index (Modak 2023e), to compare
the performance of our devised metric. The proposed measure uses only
the distances of members from their cluster representatives, instead of the
massive interpoint distance matrix involved throughout in its competitors,
whereas the gap statistic calls for a parametric distribution and concerns an
extremely computationally-extensive algorithm. These make our index quiet
fastercomparedtoitscontenders andhence ispreferredforbigdataanalyses.
Our suggested method assesses the quality of a sample partition and thereby
estimates the unknown number of clusters present in the underlying sample.
Most importantly, our index possesses the rarest quality among the existing
cluster assessment indices, shared by only the gap statistic from the rival
measures under comparison, that is to check whether there is any clustering
pattern prevailing a data set or not, which is extremely useful in practice
when a new sample is looked into for possible grouping. Our designed crite-
rion, falls under the category of internal cluster accuracy measures which use
only the given data and its partition resulted in the performed clustering,
is eligible for finding division in a new data set without any other informa-
tion available on its existing grouping. The proposed method carries out
a thorough exploration of individual clusters independently in our advised
way, and then the corresponding statistics are combined into a final measure
capable of the targeted purpose, even in cases of the challenging separation
of entangled, overlapping or closely-spaced clusters. Its easy lay-out, sim-
ple interpretation and superior performance, studied in terms of extensive
simulation study and real-world data analyses, are discussed as follows.
Section2describestheconstructionofourmeasure, itsproperties, compu-
tation, significance and interpretation. Section 3 demonstrates the measure’s
application throughdifferent interesting case studies and its comparison with
the competitors, with Section 4 concluding.
2 Method
Suppose we are given N members from a sample into K(≥ 1) hard clusters
K
C ,...,C ofrespective sizes N ,...,N ∋ N = N. First, we determine
1 K 1 K k=1 k
a cluster representative (denoted by R for the k-th cluster for k = 1(1)K;
k
P
namely, centroid or medoid) for individual clusters. Then, for every cluster,
we compute the distances of all members, clustered in it, from their represen-
3tative, and normalize them to lie in [0,1] wherein the lower bound is attained
when the cluster-representative coincides a member from the cluster itself.
Next, the interval [0,1] is divided into l(> 1) number of equally spaced sub-
intervals, which are mutually exclusive, exhaustive and increasing in values,
look like:
0 1 1 2 l−2 l−1 l −1 l
= 0, , , ,..., , , , = 1 ,
l l l l l l l l
(cid:20) (cid:19) (cid:20) (cid:19) (cid:20) (cid:19) (cid:20) (cid:21)
where we count the frequencies of the distances falling in the individual sub-
intervals.
Mathematically speaking, for M being the m-th member from the k-th
mk
cluster C , we compute
k
S = {d(M ,R ),m = 1,...,N }, (1)
k mk k k
where drepresenting a distance measure, specified by theuser. Inanattempt
of computational ease and avoidance of spareness, we consider the following
set of distances:
d(M ,R )
SNor = mk k ,m = 1,...,N , (2)
k max d(M ,R ) k
(cid:26) mk k (cid:27)
m=1(1)Nk
through normalization ∋ all distances from SNor ⊆ [0,1] (0 is attained for
k
R ∈ C ), and then it is divided into l sub-intervals as stated above.
k k
2.1 Implementation of multinomial distribution
Let N = the number of members ∈ C and N = the number of members
k k j|k
∈ C falling in the j-th sub-interval from SNor. Obviously, 0 ≤ N ≤ N
k k j|k k
for any l chosen. Now,
N ∼ bin(N ,π ), (3)
j|k k j|k
where “bin” stands for a binomial distribution and π = probability for a
j|k
member from C falling in the j-th sub-interval, for a fixed k ∈ {1,...,K}
k
and j = 1,...,l. Then, for every k,
N = (N ,...,N )′ ∼ MN (N ,l,π ,...,π ), (4)
k 1|k l|k k 1|k l|k
4where “MN” stands for a multinomial distribution (see, McCullagh and
Nelder 1989; Johnson, Kotz and Balakrishnan 1997), here satisfying
(i)
E(N ) = N π (5)
j|k k j|k
l l
with π = 1 and N = N , and
j=1 j|k j=1 j|k k
(ii)
P Disp(N )P = N {diag(π )−π π′} = Σ (say), (6)
k k k k k k
where π = (π ,...,π )′ constitutes the diagonal entries for the diagonal
k 1|k l|k
matrix: diag(π ), i.e. the above equation can be explicitly written as:
k
N π (1−π ) −N π π ... −N π π −N π π
k 1|k 1|k k 1|k 2|k k 1|k l−1|k k 1|k l|k
−N π π N π (1−π ) ... −N π π −N π π
Σl×l =  k .2|k 1|k k 2|k . 2|k k 2 .|k l−1|k k .2|k l|k .
k . . . .
. . ... . .
 
 −N π π −N π π .... −N π π N π (1−π )
 k l|k 1|k k l|k 2|k k l|k l−1|k k l|k l|k 
 
2.2 Our rationale
A cluster representative is the most typical observation (vector) within the
cluster, which can be either an original member from given sample, like the
medoid in K-medoids clustering (Kaufman and Rousseeuw 2005; Modak et
al. 2020), or some function computed over each member of the cluster, e.g.,
centroid in K-means algorithms (MacQueen 1967; Hartigan 1975), revealing
the inherent properties of the cluster best. Therefore, we study how far the
members of a cluster are located from an appropriately determined cluster-
representative. In checking the quality of a partitionfor a designated sample,
we analyze the dispersion of the clusters with respect to their respective rep-
resentatives, which is quantified in terms of the distances computed between
individual members of a cluster and its representative. The more members
of a cluster cloud around its typical member (i.e., cluster-representative) and
the less members lying far from it, the better is the quality for the clustering;
in which situation we expect more information by means of cluster members
or equivalently more dispersion in the cluster near its representative, com-
pared to the areas increasing in distance from the cluster-representative.
To check the above, we divide the interval [0,1] for the normalized dis-
tances into l (user-defined) mutually exclusive and exhaustive sub-intervals
of equal length. For cluster k, the frequency in the j-th sub-interval is a
5random variable as per Eq. (3), where the joint distribution of the frequen-
cies from all sub-intervals is assumed to be a multinomial, as explained by
Eq. (4) in Section 2.1. Clustering is an exploratory data analysis, where we
neither know the values for these random variables in advance while cluster-
ing a sample into a fixed number of partitions, nor the implementation of
an arbitrary clustering algorithm guarantees constant values for them even
with a specified K. Therefore, our approach is justified, and quite robust in
the sense that in spite of adopting a parametric procedure in terms of multi-
nomial distribution, the analysis is distribution-free due to the fact that our
method does not depend on the population-distribution of the sample.
The above-mentioned quality check of cluster k, from an available parti-
tion for the sample, is now interpreted in terms of each individual N , with
j|k
the k specified, as follows:
(a) If
Var(N ) > ... > Var(N ) holds, (7)
1|k l|k
then the quality of the cluster k is a best possible, whereas
(b) when
Var(N ) < ... < Var(N ) happens, (8)
1|k l|k
the opposite occurs indicating the worst quality of the cluster.
This same explanation is valid for every cluster k = 1(1)K, and based on
that we proceed to propose our novel measure in the following section.
2.3 Proposed measure
A plausible measure for quality check of the k-th cluster:
l
C = w Var(N ), (9)
MNk j|k j|k
j=1
X
where w s (“s” letter is added to mean plural) are non-negative weights
j|k
selected for giving relative importance to Var(N ) s ∋ w ↓ in j.
j|k j|k
A simple choice of w s fulfilling the aforesaid requirements, in the forma-
tion of our quality check index, would be:
w = l +1−j,∀j = 1(1)l, (10)
j|k
maintained throughout all clusters k = 1(1)K. We report the results for our
case studies in Section 3 with this selection.
6Alargevalueforourproposedcriterion(Eq.9)isdesired, whoseanalytical
expression using the distributions of N s (Eq. 6) is obtained as:
j|k
l
C = w N π (1−π ), (11)
MNk j|k k j|k j|k
j=1
X
where the unknown parameters π s are replaced with their respective un-
j|k
biased estimators
πˆ = N /N
j|k j|k k
l
with πˆ = 1, and we reach our (estimated) measure (i.e., sample
j=1 j|k
version) for cluster k as:
P
l
Cˆ = w N πˆ (1−πˆ ). (12)
MNk j|k k j|k j|k
j=1
X
Now, we propose our final measure, for checking the quality of the entire
sample partition, as the sum of the statistics Cˆ s computed for each
MNk
cluster k (Eq. 12):
K
CK = Cˆ
MN MNk
k=1
X
K l
= w N πˆ (1−πˆ )
j|k k j|k j|k
k=1 j=1
XX
K l l
N
j|k
= w j|k N j′|k ,
N
k=1 j=1 k j′(6=j)=1 !
XX X
(13)
wherein if for a K, the data own one or more singleton cluster(s), then we
suggest contributing zero to Eq. (13) for each such cluster.
2.3.1 Quality check and determination of the number of clusters
It is easy to check, theoretically, that our proposed measure, empirically
computed based onthedesignated sample-partition, isanunbiased estimator
7of its population version (approximately), because:
K
E(CK ) = {E(Cˆ )}
MN MNk
k=1
X
K l
= w E{Var(N )}
j|k j|k
" #
k=1 j=1
X X
K l d
= w N E πˆ (1−πˆ )
j|k k j|k j|k
k=1 j=1
XX (cid:8) (cid:9)
K l
= w (N −1)π (1−π )
j|k k j|k j|k
k=1 j=1
XX
K l
≃ w N π (1−π )
j|k k j|k j|k
k=1 j=1
XX
= CK ,
MN
(14)
which is a coveted characteristic of our new measure guaranteeing the reli-
ability of its value, computed based on the given partition of a sample, to
check the quality of the same.
The index suggested CK ≥ 0 ∋
MN
CK ↑ <=> partition quality ↑,
MN
wherein K is estimated over a range of considered values {1,2,...,K } as
max
Kˆ = K : max CK = CKˆ . (15)
MN MN
1≤K≤Kmax
(cid:26) (cid:27)
(cid:8) (cid:9)
CK attainsthelowerboundaryvaluezeroonlyifthereexistonesub-interval
MN
containing all observations and the others having zero for all clusters, which
is unlikely to happen. The zero value can occur numerically, as per our
approach, when all the resultant clusters are singletons for K = N; however,
in practice we are concerned with a value of K << N.
2.3.2 Hyperparameters
Our measure has two tuning parameters or hyperparameters as K and l. In
association with the clustering algorithms (like K-means, K-medoids; see,
8Hartigan 1975; Kaufman and Rousseeuw 2005) for which K is provided as
a priori, we vary K = 2,3,...,K (K depends on the case-study; we
max max
select six for all cases in Section 3); whereas the other kind of algorithms
evaluates K during the cluster analysis itself (e.g., DBSCAN, kernel-based
clustering methods; see, Ester et al. 1996; Matioli et al. 2018; Modak
2023a). Whatever be the procedure, the final value of K is estimated by
the quality-check property of our suggested measure, which hints at a best
possible clustering with its highest value.
Moreover, if we need to examine the sample for the presence of more
than one clusters, we can compare the values of {CK ,K = 2(1)K }
MN max
with the value of C1 (see, Eq. 13), in case of the first kind of clustering
MN
algorithms; and for the other kind, we make the comparison between
CK′
,
MN
wherein K = K′ is determined by the algorithm, and C1 . Our index
MN
for a single-group C1 is calculated over all members of the sample, with
MN
an analogously defined sample-representative for the entire data set as the
cluster-representatives are constructed for each individual clusters with K
greater than one in the implemented algorithm. Finally, in the former case,
theverdict ismadebyEq.(15)asdepictedinSection2.3.1; whereasthelatter
case selects Kˆ as either one or K′ whichever produces max{C1 ,CK′ }.
MN MN
Another hyperparameter l isanon-negativeinteger-valuedconstant being
at least 2 (usually much greater than 2 to expose the underlying structure
well) involved in our cluster assessment measure. The numerical value of the
suggested index depends onl, whose value is case-dependent and provided by
the user, should be varied. We go for the usual trial-and-error method, gen-
erally adopted in such exploratory studies with no global objective approach
in existence (Silverman 1986; Bandyopadhyay and Modak 2018; Matioli et
al. 2018; Modak 2022). l is expected to produce robust answers with natural
resultant clusters for all plausible values of it (see, case study (1) in Section
3). On the other hand, for a challenging clustering pattern, different values
of l may lead to distinct answers, where a best value generates the highest
value for our measure corresponding to an optimal partition in accordance
with the index’s intrinsic design (e.g., see, case study (6) from Section 3).
3 Case studies
The efficacy of our measure is to be evaluated in checking the quality of
an available partition for the assigned sample and thereby in estimating the
9true unknown number of clusters present in the data set. To carry out so,
the sample is first clustered using an appropriate cluster algorithm and we
thencompute our cluster accuracy index alongwith itswell-established rivals
Calin´ski and Harabasz index (CH; Calin´ski and Harabasz 1974), Dunn index
(Dunn; see, Dunn 1975), K˜-nearest neighbor classification error rate (K˜NN;
Ripley 1996), gap statistic (Gap; Tibshirani, Walther and Hastie 2001), con-
nectivity (Conn; Handl et al. 2005), and kernel-based index (M ; Modak
clus
2023e). Dunn is defined as a ratio of the smallest distance between members
from separate clusters to the largest within-cluster distance, and CH is de-
signed as a ratio of between-cluster mean of distances to within-cluster mean
of distances, where boththe indices are greater than zero with increasing val-
ues indicating higher-quality partition for a sample. A sophisticated measure
gap statistic, constructed on the concept of the within-cluster-dispersion, is a
real-valued index producing the optimum with its maximum value. Recently
invented kernel density-based cluster validity index, with bandwidth h (a hy-
perparameter, here h = h∗ for the classical Gaussian kernel, for details, see
Modak 2023e) for the involving kernel, can be considered as a robust devel-
oped version of the popular cluster quality measure average silhouette width
(Rousseeuw 1987). It accepts any value from -1 to 1 with a larger one hinting
ata superior classification. Onthe other hand, Connmeasuring the tightness
for the clusters, is a kind of sum of proximity between each member and its
J (a tuning parameter, here J = 10, unless mentioned otherwise, is chosen as
a trade-off between required information and computation burden) nearest
neighbor(s). Another classical nearest neighbor-based measure is K˜NN that
accounts for the agreement between cluster memberships of a member and
its K˜ (a tuning parameter, here K˜ = 9, except stated otherwise, taken is
an odd number in order to avoid the randomness in the decision to be made
in case a tie is occurred in terms of the considered nearest neighbors) near-
est neighbor(s). Both of the last statistics accept non-negative values where
smaller means higher-quality clustering.
All indices are a function of K and depend on a distance measure like
ours, where we consider the commonly used Euclidean metric unless cited
otherwise; nevertheless, none of the rivals, except Gap, can check a sample
forthepresenceofclustering(i.e., whetherK = 1orgreater)butours. Itisto
be noted that, throughout the case studies, we generally consider the cluster
representatives to be the simple mean vectors, with entries as the dimension-
wise arithmetic means computed over all members in an individual cluster;
unless any special case happens when they are mentioned to be different
10according to the compatibility with the corresponding situation.
Comparison of the time complexity among the measures, with specified
hyperparameter in the concerning measure, underlines: for the k-th cluster
consisting of N members, our measure involves N distances or (N − 1)
k k k
distances if the cluster-representative itself is a member of the cluster, on the
other hand, the other measures need Nk interpoint distances; in addition,
2
the gap statistic adopts the technique of bootstrapping, that needs a large
(cid:0) (cid:1)
number of bootstrap samples, which is quite time-consuming even with the
help of well-known in-built function developed by computer languages.
For numerical computation, we use the statistical programming language
‘R’ version 4.2.2 using a 64-bit laptop with core i3 processor. The corre-
sponding code is given in the appendix for the interested readers, where we
explain the computation of our measure with K = 2(1)K (= 6) for a part
max
of the following case study. If C1 is required, we simply find Eq. (13) with
MN
K = 1, i.e. no clustering is performed and the whole set of given obser-
vations is considered as a single cluster, where the cluster-representative is
computed over the entire data set. In order to provide the exact runtime of
our proposed measure, we study so at the end of the first case study under
different values of dimension, size, number of clusters present in the data
set considered for cluster analysis, and the hyperparameter l involved in our
index.
Case study (1): We consider a multivariate normal distribution in a 10-
dimensionalspacewiththecomponentsconnectedthroughat-copula(Nelsen
2006; Modak and Bandyopadhyay 2019), which is explained by a 10-variate
t-distribution with 2 degrees of freedom and the following 10×10 correlation
matrix:
1 0.15 ... 0.15 0.15
0.15 1 ... 0.15 0.15
 
. . . .
. . . . ... . . . . .
 
0.15 0.15 ... 1 0.15
 
0.15 0.15 .... 0.15 1 
 
 
The copula can be analytically expressed as:
C(u) = F[F−1(u ),...,F−1(u )], (16)
1 1 10 10
where
u = (u ,...,u )′,0 < u < 1∀i = 1(1)10,
1 10 i
11and F is the joint cumulative distribution function (cdf) for the multivariate
t-distribution with F−1 as the inverse function of the cdf for the marginal
i
distribution of the i-th component. Our synthetic data set under the above
compoundstructure iscreated withthreeinherent groupsofunequal sizes 45,
50, and 70, with the mean vectors 0×1 ,−3×1 , and 3×1 , respectively
10 10 10
(1 symbolizes a 10-variate vector with all entries equal to 1).
10
In this situation, we apply the widely used partitioning-based clustering
method K-means (“Hartigan-Wong” algorithm; see, Hartigan 1975, Modak
et al. 2018), as it is well-known for exploring the Gaussian clusters. In order
to implement our cluster accuracy measure, we select the cluster represen-
tatives to be the cluster-wise means, since the clustering algorithm aims at
minimizing the within-cluster sum of squares with respect to the means of
the clusters. The computed values of our index (for l = 10 chosen) along
with its competitors, for K-means clustering, are given in Table 1, where the
well-separated clusters are prominently exposed by all the indices.
To check the robustness oftheclustering results, we performanother clas-
sical and more robust clustering method K-medoids (“PAM” algorithm; see,
Kaufman and Rousseeuw 2005; Modak et al. 2017, 2020), as it is specially
designed for spherically shaped groups. Computation of our measure uses
medoids for individual groups as the cluster representatives in accordance
with the present clustering algorithm. Table 1 shows all measures (except
M ) direct us to Kˆ = 3 with desired success.
clus
It proves that our measure is capable enough to disclose the true clus-
tering pattern inherent to a sample, whereas its different values for the two
clustering algorithms indicate distinct outcome by them. Therefore, to com-
pare their performances, we utilize the original class labels available for all
members from this synthetic data set (see, Table 2). From the table it is
clear that a higher value of our index is an evidence for better clustering
(here K-means), as per its intrinsic design. It justifies the construction and
interpretation of our approach empirically.
Now, we consider the impact of the hyperparameter l on our measure
and hence on the output produced by it. As K-means algorithm is better
suitedforthepresent data(fromTable2), we studythevaluesofourmeasure
with varying l, in association with K-means for different K in Table 3. We
conclude the consistency of our measure, with respect to the various values of
its tuning parameter l, in exploring the existent natural clusters (see, Fig. 1).
Next, we consider the most important aspect regarding the clustering
of a sample, which is to investigate whether there is any further clustering
12pattern existing in the data at all other than the set of data as a whole.
Generally, the cluster indices, through which K is determined in association
with a clustering algorithm, are not capable of this task. As a result, if we
assume K ≥ 2 and start looking for Kˆ, we would never be able to explore
the original scenario while true value of K is one. This serious issue is solved
by our method, where we compute the value of the Eq. (13) for K specified
as one. The present data possess: C1 = 788.5697 (for l = 10 along with
MN
K-means), when compared with our index’s counterpart for K greater than
one: CK ,K = 2(1)6 (see, Table 1), prominently establishes the original
MN
three clusters.
(cid:8) (cid:9)
As we have generated the clusters from known probability distributions,
we are in a position to provide the expected probability of correct recovery
by the proposed measure througha Monte Carlo simulation technique, where
we consider 1000 Monte Carlo replications (the largest ever used for cluster
analysis, to our knowledge). For our index, we count the success times (say,
I) over different l as: I = (834,921,944,944,939,950,946,944,939) for l =
(5,7,10,11,12,13,14,15,17), simply by a grid-search method as depicted in
the form of the following matrix. Here a row with l values has the succeeding
row for the corresponding I values, where the ones (marked in bold) are
computed in a particular stage, out of a total of four S1−S4, used to reach
our decision in this case:
S1
l 5 7 10 11 12 13 14 15 17
 
I 834 921 944 944 939 950 946 944 939
 S2 
 
l 5 7 10 11 12 13 14 15 17 
 
I 834 921 944 944 939 950 946 944 939
 .
 S3 
 
l 5 7 10 11 12 13 14 15 17 
 
I 834 921 944 944 939 950 946 944 939
 
 S4 
 
l 5 7 10 11 12 13 14 15 17 
 
I 834 921 944 944 939 950 946 944 939
 
 
This shows the consistency of the answers with respect to l; whereas for l <
10, the loss of information starts affecting the same. We report I = 950 for
l = 13, with corresponding results of its rival measures in Table 4. Our index
becomes either superior or competitive in comparison with its challengers.
13As far as the computation time of our metric is concerned, we consider
multivariate normal data sets under the above-specified t-copula set-up from
the following combinations of different values for: (a) dimension of data,
highest at 30, (b) number of clusters, up to K = 20, where the adjacent
clusters are formed at a difference of 1 in all entries of their mean vectors,
(c) equal size of each cluster (i.e. N = ... = N ), maximum of size 500
1 K
(i.e. largest of data set has the size N = 500 × 20 = 10,000), and (d)
hyperparameter l till 30. Forclustering, weimplement K-medoids algorithm,
for which the R code, provided in the appendix, can be readily used with
changes under (a)-(d). Using this partition, into K (designated under (b) )
groups, when we compute our measure CK , it takes just less than a second,
MN
which shows fast runtime of the suggested index and its preference for big
data analyses.
Case study (2): The second study is made to confirm the usefulness of
our measure in case of a single cluster. For this objective, we simulate a
multivariate sample under the same structure as in case study (1), with only
difference in the number of clusters. Here we study one cluster, instead of
three, where all observations are possessing the mean vector 0 × 1 . In
10
association with K-means clustering, our index (for l = 10) successfully
discovers the single cluster (see, Table 5).
Estimation of the probability for the above by Monte Carlo simulation
leadsto: (I,l) = (892,5),(907,7),(961,10),(968,13),(968,15),(960,17),wherein
we see similar kind of impact of l on I as in the last case study. It manifests
ourindex’seffectiveness toidentifysingle-clustersituationwithagreatdegree
and consistency over different plausible values of l(≥ 10); whereas no other
competitor, except gap statistic, shares this vital property of single-group
exploration, that appears to be competitive to our measure (see, Table 4).
Case study (3): Gaussian-cluster analyses are followed by non-Gaussian
clusters in this study, where 3 groups (at difference of three between the
dimension-wise means of nearest group duo, as in case study 1) with equal
size of 50 are drawn from a four-dimensional non-normal population (Vale &
Maurelli 1983). The multivariate structure is built by the correlation matrix
withall off-diagonalelements 0.6. Deviationfromnormalityis ascertained by
providing dimension-wise skewness (b ) = 1.75 and kurtosis (b ) = 2 (Joanes
1 2
& Gill 1998).
We carry out the cluster analysis using the agglomerative hierarchical
algorithm, with the average linkage which is thought to be producing rea-
sonably robust answer for non-spherical clusters (Kaufman and Rousseeuw
142005, Modak 2023d). Results over 1,000 replications are given in Table 4. As
usual, initially we compute our measure for l = 10 (giving rise to I = 535),
and then, as gathered from the earlier studies, avoiding the loss of informa-
tion, we check for l ∈ {15,20,25,30,35} leading to corresponding results:
I = (633,676,686,703,698). For further possible improvement, we look into
l = 29 and 31 giving respective I values as 694 and 697. It shows an opti-
mal answer is obtained for l = 30 which is reported in the table along with
its competitors. This case study needs larger values of l, compared to the
previous ones, to reach consistently good outcome through our index. It sug-
gests the complexity of the underlying clusters; however, our measure stands
significantly better than most of its contenders.
Case study (4): Next study is carried out under the non-normal set-
up of the last and through the same clustering algorithm, but concerning
a single cluster. Our advised index with l ∈ {10,13,15,20,25,30,35} gives
rise to respective I = (775,880,939,961,970,974,968), which significantly
outperforms the gap statistic (referred to Table 4).
Casestudy(5): Afterdemonstrationwithrespecttotheidenticallyshaped
clusters, now we generate a more realistic and challenging scenario with four
groups, namely G1-G4 each of size 150, having arbitrary shapes with noisy
(by additive Gaussian noise with mean zero and standard deviation 0.075)
and overlapping observations (see, Fig. 2, wherein for the purpose of an ef-
fortless visual depiction, we consider bivariate observations). First group G1
is consisted of data on the arc of a semi-circle and G2 is created using the
equation of a line within a specified region. Next, G3 is utilizing bivariate
points from the entire area of an ellipse, whereas G4 is of a circle.
Here we carry out the popular density-based clustering method DBSCAN
using a kd-tree (Ester et al. 1996; Campello et al. 2013), established for
uncovering such differently shaped clusters contaminated with noises. Unlike
K-means, K-medoids or hierarchical algorithms, it does not need K to be
specified. Because K is estimated by the algorithm itself. It has two tuning
parameters ǫ and Minpts whose values are very crucial to obtain a plausible
clustering outcome. Selection of their values is neither easy nor objective;
however, acommonlyadoptedthumb-ruleistochooseMinpts = 2×p(where
p = dimension of data), for which ǫ is that value where a knee occurs in the
curve of the Minspts-nearest neighbor distance plot (Hahsler, Piekenbrock,
andDoran2019). Nevertheless, inpractice, thevaluesshouldbevariedbefore
the final decision.
Forexample, differentvaluesofthehyperparameterduo,i.e. (Minspts,ǫ) =
15(4,0.15) and (4,0.17) (see, Fig. 3), evaluate Kˆ = 5 and 4. The latter gives
off better partition of the sample, which is manifested in Table 6 by all
the indices, except Gap which is not compatible with such comparison. It
proves the success of our proposed measure in indicating the true number
of arbitrary-shaped clusters with noisy observations. This does it robustly
for both types of cluster-representatives as: (i) the cluster-means (commonly
used dimension-wise mean; see, third col of Table 6), and (ii) the cluster-
modes (i.e., dimension-wise univariate mode, given in fourth col of Table 6,
as the implemented algorithm is density-based).
Case study (6): After simulated samples, it is time to illustrate the ef-
fectiveness of our measure in clustering real-life data. For this, we choose
the “mtcars” 1 data set (Henderson and Velleman 1981), where the data are
collected from the 1974 Motor Trend US magazine for 32 different car models
onthe following four components: (i) am: transmission (automatic/manual),
(ii) cyl: number of cylinders, (iii) hp: gross horsepower, and (iv) wt: weight
(per 1000 lbs). It makes quite an interesting data set having measurements
on mixed scales, where the first component is actually an attribute having
two categories with the rest as numerical variables.
First, the two categories of “am” automatic and manual are converted to
zero and one, respectively, and we obtain am in the form of a binary variable.
Then, wefitthelogisticregressionmodelforitontheothervariables, namely,
cyl, hp, and wt (McCullagh and Nelder 1989; Dobson 1990; Agresti 2002).
The Wald’s test reveals that the only variable affecting am significantly is wt,
which leads to the cluster analysis in the bivariate space of am-wt using K-
medoids algorithm which adopts the Gower’s distance for these mixed-scaled
data (Gower 1971; Kaufman and Rousseeuw 2005).
Clustering results by different indices fromTable 7 shows that the applied
K-medoids algorithm uncovers three clusters only through our measure. The
clusters C1-C3 (Fig. 4) correspond to three clearly separated groups (see,
Table 8), where C1 represents the manual transmission cars which have low
weights compared to the group of the automatic cars, in which C2 includes
the cars with high weights and C3 with the higher ones.
On the other hand, the challengers Dunn and M welcome the cluster
clus
output from 2-medoids algorithm, which possesses the first cluster C1 that
is the very same as resulted in the 3-medoids case, and the second cluster
by combining C2 with C3 of 3-medoids method. Therefore, these indices
1retrieved from ‘mtcars’ data set embedded in ‘R’ programming language
16become unable to distinguish the highest-weighted automatic cars from the
lower weighted.
Here we have no information regarding the existing clusters prior to the
cluster analysis, whereas the carried out clustering method results in some
clusters with much smaller size (e.g. the second cluster of 19 cars splits into
clusters of 16 and 3, when we divide the 2-cluster partition into 3) than our
previously chosen hyperparameters J = 10 and K˜ = 9 in the respective
˜ ˜
contenders Conn and KNN. Therefore, we put J = 2 and K = 1 for the
present case, where both the contenders come out to be indecisive.
The rest of the competitors CH and Gap fail with a strictly increasing
trend of values as we increase K over the considered range. No doubt, this is
a challenging cluster analysis. It has taken its toll on our measure, too. As
a result, Table 7 is showing very close, practically indistinguishable, values
of our measure at K = 2 and at K = 3, when l is equal to seven; however,
a higher value of our index with l = 10 clearly leads to Kˆ = 3, whose
robustness is supported by the value corresponding to l = 13, that increases
its reliability with l = 15.
Case study (7): Another real-life data set is studied in a trivariate space
named “trees” 2, which provides observations on the variables as follows:
(i) girth: tree diameter in inches, (ii) height: height of tree in ft, and (iii)
volume: volume of timber in cubic ft, for 31 felled black cherry trees (Ryan,
Joiner and Ryan 1976; Atkinson 1985).
Here we implement the agglomerative hierarchical cluster analysis using
Ward’s criterion (Murtagh and Legendre 2014), which results in different
number of clusters indicated by various indices (Table 9). Like the previous
case study, to reflect the clustering quality of the smaller resultant clusters
accurately in decision making, we fix both the tuning parameters for the
competitors Conn and K˜NN at 3. While our index, consistently for all l, in-
ˆ
duces K = 2 supported by Conn and M (see, Fig. 5, for the corresponding
clus
dendrogram), Dunn says Kˆ = 3 and K˜NN confuses between two and three,
whereas CH and Gap give rise to six clusters.
Such an outcome is not rare in real-world data analyses. Let us solve
it by exploring the clusters through detailed investigation (see, Table 10).
From the table exhibiting the cluster sizes, the following decision is reached:
we discard the choice of K = 6 for which the resultant partition is actually
affected by the outliers present in data due to the deficiency in the hierarchi-
2built-in data set ‘trees’ in ‘R’
17cal clustering methods, whereas the cluster properties with respect to each
of the study variables are inspected in Table 11 for K = 2 and 3. It is com-
prehensible that the first cluster from the former is divided into two further
sub-clusters in the latter case. Hence to resolve the conflict around the true
number of clusters in this data set, we apply the hierarchical clustering using
another criterion, i.e., the average linkage which is generally considered to
be much more robust (Kaufman and Rousseeuw 2005). It generates exactly
the same results as Ward’s characteristic leading to the robustness of quality
check through our cluster validity index, and we confirm the verdict of two
clusters differentiating trees with lower values for the study variables from
the higher ones (see, Fig. 6).
Summary: The clustering results of our measure are summarized with
respect to the data study performed above. We consider some hypothetical
situations in the first five cases to cluster synthetic data, where the proposed
index does quiet well compared to its well-known challengers, either outper-
forming most of them or standing strongly competitive with the best one(s).
It is evidenced to be preferred in the single-cluster cases over its only eligible
competitor Gap.
Real-life cluster analysis based on the single sample of observations is
much more complex to beexplored. Therefore, the literature hasexperienced
failure of widely used cluster indices, which are established to be performing
good for simulated data sets, while being applied to the challenging data
analysis of thereal-world samples. Hence, thecluster analysts always suggest
thecross-check ofthepartition, givenbytheoptimalvaluesofsuchindices, in
termsofthedetailedexplorationofthecluster properties. Inwhatsituationa
particular measure would supersede the others is unknown, because nothing
is known regarding the true clusters underlying the given sample. In this
context, we see that our measure executes outstandingly the exposition of
the natural groups of data in comparison to its contenders. For example, in
the sixth case study, it is proved to be the sole rescuer of the reality, while
for the last (i.e. the seventh case study) it consistently, with all considered
values of l, resolves the conflict arising in the cluster analysis.
4 Conclusion
We propose a novel distribution-free clustering accuracy measure to deter-
mine the quality of a sample partition. It is based on the multinomial distri-
18bution applied to the distances of all members computed from their cluster
representatives. With easy construction, simple interpretation, fast compu-
tation, good performance, and diverse applicability, it not only estimates the
unknown number of clusters, but also checks for the existence of any clus-
tering structure in the designated sample. The latter quality is extremely
scarce among the existing cluster validity measures and does not possessed
by its considered competitors except the gap statistic. Readers are encour-
aged to explore the performance of our proposed measure with the selection
of different weight functions, and for clustering of high-dimensional data sets
with dimension close to or much higher than the sample size.
19Appendix:
R codes to compute our measure CK
MN
## Codes are illustrated below for clustering the first synthetic data set from
Case-study (1), through K-medoids algorithm, whose output is printed in
Table 1 (i.e. second column in the matrix of results under K-medoids)
## Loading library ‘cluster’ for implementing K-medoids clustering through
‘PAM’ algorithm
library(cluster)
## Loading library ‘copula’ for generating data under multivariate set-up
constructed by a copula
library(copula)
##——–Generation of data set——–
## Dimension of the data set
p<-10
## Specify copula
myCop.t<-ellipCopula("t",param=.15,dim=p,dispstr="ex",df=2)
## Define population distribution
myMvd<-mvdc(copula=myCop.t,margins=rep("norm",p),
paramMargins=sapply(1:p,function(j)list(list(mean=0,sd=1))))
## Assign cluster sizes and thereby compute total sample size
N1<-45;N2<-50;N3<-70;N<-N1+N2+N3
## Fix data for reproducible output
set.seed(12045)
## Draw sample of size N from the population
Z<-rMvdc(N,myMvd)
data<-matrix(NA,N,p)
## Sample from first cluster
data[1:N1,]<-Z[1:N1,]
## Sample from second cluster
data[(N1+1):(N1+N2),]<-Z[(N1+1):(N1+N2),]
+matrix(rep(-3,N2),N2,p,byrow=T)
## Sample from third cluster
data[(N1+N2+1):(N1+N2+N3),]<-Z[(N1+N2+1):(N1+N2+N3),]
+matrix(rep(3,N3),N3,p,byrow=T)
20## Compute interpoint distances for every unique pair of observations from
entire sample, by default it is Euclidean metric
distMatrix<-dist(data)
## Create N ×N distance matrix
M<-as.matrix(distMatrix)
##——Hyperparameters specification——–
## Max no. of clusters considered
K_max<-6
## Value of hyperparameter l in our measure
l<-10
## Sub-intervals for distances to computes counts: N j|k s
interval<-seq(0,1,1/l)
##——Construction of our measure——–
measure<-NULL
## K loop begins
for(K in 2:K_max){
stat<-NULL
## Cluster data by PAM algorithm
kmed<-pam(distMatrix,k=K,diss=T)
## Cluster memberships
cl<-kmed$clustering
## Cluster representatives’ id
Rk_id<-kmed$id.med
N_j<-matrix(NA,K,l)
## k loop begins
for(k in 1:K){
Sk<-NULL
## Compute the set from Eq. (1)
Sk<-M[which(cl==k),Rk_id[k]]
## Compute the set from Eq. (2)
Sk_norm<-Sk/max(Sk)
## Compute variables from Eq. (3)
N_j[k,]<-hist(Sk_norm,breaks =interval,plot=F)$counts
## k loop ends
}
21## Compute vector of variables from Eq. (4)
Nk<-apply(N_j,1,sum)
## Give estimated variance terms from Eq. (12)
term<-function(x,k){return((x/Nk[k])* (Nk[k]-x))}
## k loop begins again
for(k in 1:K){
objective.in <- term(N_j[k,],k)
wt<-rep(NA,l)
for(j in 1:l)
## Assign decreasing weights
wt[j]<-(l+1-j)
## Compute cluster-wise terms in Eq. (13)
stat[k]<-wt%*%objective.in
## k loop ends again
}
## Calculate our measure as per Eq. (13)
measure[K]<-sum(stat)
## K loop ends
}
##——–Output——–
K_hat<-which.max(measure[-1])+1
## Print estimated number of clusters
K_hat
## Report computed values of our measure for different number of clusters
round(measure,5)
22Table 1: Computed values of different indices for a multivariate normal data
set from case-study (1), through K-means and K-medoids algorithms, with
varying K (Original K and optimal values of indices are marked in bold).
K CK Dunn Conn CH K˜ NN Gap M
MN clus
(l = 10) (J = 10) (K˜ = 9) (h = h∗)
K-means
2 741.7756 0.12315 12.57500 349.4184 1.81818 0.4412529 0.5418888
3 828.0040 0.31933 2.06627 450.0298 0 0.6611909 0.5862262
4 801.7292 0.15702 31.05952 329.0211 3.63636 0.6531145 0.4549353
5 778.9145 0.13795 62.54841 258.5651 9.69697 0.6480189 0.3020418
6 767.1396 0.13795 85.91746 210.8898 12.72727 0.6351098 0.2841469
K-medoids
2 738.1775 0.09814 32.10437 333.6501 0.60606 0.4206209 0.6204327
3 820.8229 0.31229 4.33333 446.6430 1.21212 0.6717865 0.5854474
4 801.6123 0.10607 50.06786 313.2806 4.24242 0.6541694 0.4420826
5 791.6566 0.10907 58.15992 252.4601 4.84848 0.6596946 0.3689428
6 784.2477 0.10907 90.00040 211.4078 12.12121 0.6591167 0.2241668
23Table 2: Original misclassification rates for clustering results of a multi-
variate normal data set from case-study (1), having K = 3 clusters, through
K-meansandK-medoidsalgorithms(Resultsfrombetteralgorithmarehigh-
lighted in bold).
Algorithm C3 Misclassification
MN
(l = 10) rate (%)
K-means 828.0040 0.606
K-medoids 820.8229 1.818
24Table 3: Clustering results for a multivariate normal data set from case-
study (1), in terms of CK , for different l, through K-means algorithm with
MN
varying K (Original K and optimal values of CK are written in bold).
MN
K 2 3 4 5 6
CK (l = 5) 310.9756 350.0653 330.2694 330.0142 321.2280
MN
CK (l = 7) 485.3022 554.9200 537.1359 517.5058 512.6104
MN
CK (l = 10) 741.7756 828.0040 801.7292 778.9145 767.1396
MN
25Table 4: Clustering results by various indices, in terms of the number of
correctly identifying K out of 1000 Monte Carlo replications, under different
case studies.
Case CK (l) Dunn Conn CH K˜NN Gap M
MN clus
study (J = 10) (K˜ = 9) (h = h∗)
(1) 950 (13) 942 938 999 834 981 854
(2) 968 (13) – – – – 968 –
(3) 703 (30) 116 0 798 0 636 758
(4) 974 (30) – – – – 724 –
26Table 5: Clustering results through K-means algorithm for a single-cluster
multivariate normal data set from case-study (2), with varying K (Original
K and optimal value of CK are written in bold).
MN
K CK (l = 10)
MN
1 874.5454
2 821.6158
3 753.2995
4 759.0228
5 734.6703
6 698.1634
27Table6: Clustering results throughDBSCANforarbitrary-shapednoisy clusters fromcase-study (5)(Better
outcomes are highlighted in bold).
Parameters Kˆ CKˆ CKˆ Dunn Conn CH K˜NN Gap M
MN MN clus
of DBSCAN (l = 10) (l = 10) (J = 10) (K˜ = 9) (h = h∗)
(mean) (mode)
Minpts = 4,ǫ = 0.15 5 2826.422 2934.361 0.04594 17.83016 24.80163 1.02916 – -0.20706
Minpts = 4,ǫ = 0.17 4 2907.455 2966.377 0.04762 13.12143 32.92533 0.84175 – -0.18839
28Table7: Computed values ofdifferent indices, for“mtcars” data set fromcase-study (6), throughK-medoids
clustering with varying K (Optimal values of indices are marked in bold).
K CK CK CK CK Dunn Conn CH K˜NN Gap M
MN MN MN MN clus
(l = 7) (l = 10) (l = 13) (l = 15) (J = 2) (K˜=1) (h = h∗)
2 114.93117 168.6883 229.0850 262.6073 1.32173 0.0 420.7910 0.000 0.14600 0.88644
3 114.28205 170.2003 230.5401 269.5897 0.57365 0.0 491.0951 0.000 0.33125 0.85514
4 105.61905 161.6875 218.9375 250.9762 0.18692 0.0 657.8667 0.000 0.50062 0.75078
5 96.90996 147.8182 202.8636 232.1398 0.14480 0.0 704.0130 0.000 0.59624 0.65114
6 93.71905 137.0000 174.6000 211.5762 0.16842 1.5 836.9777 3.125 0.65425 0.68104
29Table 8: Clustering results, for “mtcars” data set from case-study (6),
through K-medoids algorithm with K = 3, where the value on wt and the
category on am are given for each member from individual clusters C1-C3.
Cluster C1 Cluster C2 Cluster C3
wt am wt am wt am
2.620 manual 3.215 automatic 5.250 automatic
2.875 manual 3.440 automatic 5.424 automatic
2.320 manual 3.460 automatic 5.345 automatic
2.200 manual 3.570 automatic
1.615 manual 3.190 automatic
1.835 manual 3.150 automatic
1.935 manual 3.440 automatic
2.140 manual 3.440 automatic
1.513 manual 4.070 automatic
3.170 manual 3.730 automatic
2.770 manual 3.780 automatic
3.570 manual 2.465 automatic
2.780 manual 3.520 automatic
3.435 automatic
3.840 automatic
3.845 automatic
30Table 9: Computed values of different indices, for “trees” data set from case-study (7), through hierarchical
clustering (Ward’s criterion) with varying K (Optimal values of indices are marked in bold).
K CK CK CK CK Dunn Conn CH K˜NN Gap M
MN MN MN MN clus
(l = 5) (l = 7) (l = 10) (l = 13) (J = 3) (K˜ = 3) (h = h∗)
2 74.94667 107.3533 155.62 193.4733 0.25164 0.00000 53.71202 0.00000 0.14632 0.63335
3 72.07143 92.16667 139.8175 168.1429 0.29398 0.33333 58.36777 0.00000 0.16495 0.54648
4 69.19194 102.418 144.1747 177.5146 0.13065 3.16667 56.47167 3.22581 0.18346 0.51240
5 63.92527 90.58462 129.6747 165.8813 0.17736 5.00000 66.63073 6.45161 0.19946 0.49513
6 50.17143 68.20000 98.12857 126.5762 0.19692 7.33333 72.57006 12.90323 0.22338 0.46627
31Table 10: Clustering results for “trees” data set form case-study (7) through
hierarchical clustering (Ward’s criterion) with varying K.
K Cluster
sizes
2 (25,6)
3 (18,7,6)
4 (5,13,7,6)
5 (5,13,7,5,1)
6 (5,10,3,7,5,1)
32Table 11: Cluster properties for “trees” data set from case-study (7) through
hierarchical clustering (Ward’s criterion) with K = 2 and 3.
K Cluster Cluster means
no. (standard errors)
Girth Height Volume
2 1 12.056 74.640 23.456
(2.045107) (6.137622) (8.526668)
2 18.21667 81.66667 58.15000
(1.097598) (2.494438) (8.796353)
3 1 11.12222 73.00000 18.98889
(1.466119) (5.792716) (4.679256)
2 14.45714 78.85714 34.94286
(1.184250) (4.852939) (4.482619)
3 18.21667 81.66667 58.15000
(1.097598) (2.494438) (8.796353)
33CLUSPLOT( data )
−6 −4 −2 0 2 4
Component 1
These two components explain 89.94 % of the point variability.
Figure 1: Three clusters, resulted in K-means clustering for a multivariate
normal data set from case-study (1), in terms of the first two linear and
orthogonal principal components with 89.94% variation, are shown (differ-
ent clusters plotted with distinct patterns), wherein clusters are outlined by
different-colored spheres.
34
2
tnenopmoC
5.1
0.1
5.0
0.0
5.0−
0.1−
5.1−1
Group
G1
G2
0 G3
G4
−1
−1 0 1
x
Figure 2: Arbitrary-shaped synthetic groups G1-G4 with noisy observations
from case study (5).
35
yFigure 3: 4-nearest neighbor (NN) distance plot of the sample from case
study (5), considered for selection of the tuning parameters of DBSCAN, for
Minpts = 4 alongwith two different values of ǫ = 0.15 (left) and 0.17(right).
36
4−NN
distance
Points (sample) sorted by distance0100
300 5000.050.150.25
4−NN
distance
Points (sample) sorted by distance0100
300 5000.050.150.254
Cluster
C1
C2
C3
2
0
0 2 4
wt
Figure4: ThreeclustersC1-C3resultedinK-medoidsalgorithmfor“mtcars”
data set from case-study (6).
37
maCluster Dendrogram
distMatrix
hclust (*, "ward.D2")
Figure 5: Dendrogram (for singleton clusters, the serial numbers of the mem-
bersarewrittenatthebottom)resultedinhierarchicalclusteringwithWard’s
criterion for “trees” data set from case-study (7), where two clusters are out-
lined by red-colored rectangles.
38
ecnatsid
tniopretnI
001
08
06
04
02
0
13
92 03 82 62 72 52 32 42 12 22 71 81 4 7 1 2 3 02 41 91 8 01 51 61 21 31 9 11 5 6Figure 6: Two clusters for “trees” data set from case-study (7), obtained
through hierarchical clustering, both using Ward’s criterion and the average
linkage.
39
Volume
81020304050607080 10 12
Girth14
16 18 20 226065707580
85 He90ightReferences
[1] Agresti, A. (2002). Categorical Data Analysis. John Wiley & Sons, Inc.,
New Jersey.
[2] Aljarah, I. and Ludwig, S. A. (2013) A new clustering approach based on
glowworm swarm optimization. IEEE congress on evolutionary computa-
tion.
[3] Atkinson, A. C. (1985). Plots, Transformations and Regression. Oxford
University Press.
[4] Balcan, M.-F., Liang, Y. andGupta, P. (2014).Robust Hierarchical Clus-
tering. Journal of Machine Learning Research, 15 4011–4051.
[5] Bandyopadhyay, U. & Modak, S. (2018). Bivariate density estimation us-
ing normal-gamma kernel with application to astronomy. Journal of Ap-
plied Probability and Statistics. 13, 23–39.
[6] Calin´ski, T. & Harabasz, J. (1974). A Dendrite Method for Cluster Anal-
ysis. Communications in Statistics – Theory and Methods. 3, 1–27.
[7] Dobson, A. J. (1990). An Introduction to Generalized Linear Models.
Chapman and Hall, London.
[8] Dunn, J. C. (1974). Well-separated clusters and optimal fuzzy partitions.
Journal of Cybernetics. 4, 95–104.
[9] Ester, M., Kriegel, H.-P., Sander, J. & Xu, X. (1996). A density-based
algorithmfordiscoveringclustersinlargespatialdatabaseswithnoise.Pro-
ceedings of the Second International Conference on Knowledge Discovery
and Data Mining (KDD-96). AAAI Press, Portland, Oregon, 226–231.
[10] Everitt, B. S., Landau, S. and Leese, M. (2001). Cluster Analysis.
Arnold, London.
[11] Frayley, C. andRaftery, A. E. (1998), How Many Clusters? Which Clus-
tering Method? Answers via Model-Based Cluster Analysis. The Com-
puter Journal. 41, 578–588.
[12] Handl, J., Knowles, K. & Kell, D. (2005). Computational cluster vali-
dation in post-genomic data analysis. Bioinformatics. 21, 3201–3212.
40[13] Hahsler, M., M. Piekenbrock, and D. Doran. (2019). dbscan: Fast
density-based clustering with R. Journal of Statistical Software. 9, 1–30.
[14] Hartigan, J. A. & Wong, M. A. (1979). A K-means clustering algorithm.
Applied Statistics. 28, 100–108.
[15] Jain, A. K., Murty, M. N. and Flynn, P. J. (1999). Data clustering: a
review. ACM Computing Surveys. 31, 264–323.
[16] Joanes, D. N. and Gill, C. A. (1998). Comparing measures of sample
skewness and kurtosis. The Statistician, 47, 183–189.
[17] Johnson, N. L., Kotz, S. and Balakrishnan, N. (1997). Discrete Multi-
variate Distributions. John Wiley & Sons, Inc., New York.
[18] Johnson, R. A. and Wichern, D. W. (2007). Applied Multivariate Sta-
tistical Analysis, Pearson Prentice Hall, New Jersey.
[19] Kaufman, L. and Rousseeuw, P. J. (2005). Finding Groups in Data: An
Introduction to Cluster Analysis. John Wiley and Sons, New Jersey.
[20] Lewis, D. D. and Gale W. A. (1994). A sequential algorithm for training
text classifiers. In: Proceedings of the 17th annual international ACM
SIGIR conference on research and development in information retrieval.
[21] MacQueen, J. (1967). Some methods for classification and analysis of
multivariate observations. In Proceedings of the Fifth Berkeley Sympo-
sium on Mathematical Statistics and Probability, eds L. M. Le Cam & J.
Neyman, 1, pp. 281–297. Berkeley, CA: University of California Press.
[22] Matioli, L. C., Santos, S. R., Kleina, M. & Leite, E. A. (2018). A new
algorithm for clustering based on kernel density estimation. Journal of
Applied Statistics. 45, 347–366.
[23] McLachlan, G. and Peel, D. (2000). Finite Mixture Models. John Wiley
and Sons, New York.
[24] McCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models.
Chapman and Hall, London.
41[25] Modak, S.(2019).Uncovering astrophysical phenomenarelatedtogalax-
ies and other objects through statistical analysis. Ph.D. Thesis, University
of Calcutta, India. URL: http://hdl.handle.net/10603/314773
[26] Modak, S. (2021). Distinction of groups of gamma-ray bursts in the
BATSE catalog through fuzzy clustering. Astronomy and Computing. 34,
Article id 100441, 1–7.
[27] Modak, S. (2022). A new nonparametric interpoint distance-based mea-
sure for assessment of clustering. Journal of Statistical Computation and
Simulation. 9, 1062–1077.
[28] Modak, S.(2023a).Anewinterpointdistance-basedclusteringalgorithm
usingkernel density estimation. Communications inStatistics–Simulation
and Computation. In Press, Doi: 10.1080/03610918.2023.2179071
[29] Modak, S. (2023b), Pointwise norm-based clustering of data in arbi-
trary dimensional space, Communications in Statistics - Case Studies,
Data Analysis and Applications. 9, 121–134.
[30] Modak, S. (2023c), Validity index for clustered data in non-negative
space, Calcutta Statistical Association Bulletin, 75, 60–71.
[31] Modak, S. (2023d). A Book Review on “Finding Groups in
Data: An Introduction to Cluster Analysis by Kaufman, L. and
Rousseeuw, P. J. (2005)”. Journal of Applied Statistics, DOI:
https://doi.org/10.1080/02664763.2023.2220087
[32] Modak, S. (2023e). A new measure for assessment of clustering based
on kernel density estimation. Communications in Statistics – Theory and
Methods. 52, 5942-5951.
[33] Modak, S. (2023f). A Book Review of Astrostatistical Fundamen-
tals: Statistical Methods for Astronomical Data Analysis authored by
Asis Kumar Chattopadhyay and Tanuka Chattopadhyay, 2014. DOI:
https://doi.org/10.1080/02664763.2023.2220087.
[34] Modak, S. (2023g). Determination of the number of clusters
through logistic regression analysis. Journal of Applied Statistics, DOI:
https://doi.org/10.1080/02664763.2023.2283687.
42[35] Modak, S. & Bandyopadhyay, U. (2019). A new nonparametric test for
two sample multivariate location problem with application to astronomy.
Journal of Statistical Theory and Applications. 18, 136–146.
[36] Modak, S., Chattopadhyay, A. K. & Chattopadhyay, T. (2018). Clus-
tering of gamma-ray bursts through kernel principal component analysis.
Communications in Statistics – Simulation and Computation. 47, 1088–
1102.
[37] Modak, S., Chattopadhyay, T. & Chattopadhyay, A. K. (2017). Two
phase formation of massive elliptical galaxies: study through cross–
correlation including spatial effect. Astrophysics and Space Science. 362,
Article id: 206, pages 1–10.
[38] Modak, S., Chattopadhyay, T. & Chattopadhyay, A. K. (2020). Unsu-
pervised classification of eclipsing binary light curves through k-medoids
clustering. Journal of Applied Statistics. 47, 376–392.
[39] Modak, S., Chattopadhyay, T. & Chattopadhyay, A. K. (2022). Cluster-
ing of eclipsing binary light curves throughfunctional principal component
analysis. Astrophysics and Space Science. 367, Article id: 19, pages 1–10
[40] Murtagh, Fionn and Legendre, Pierre (2014). Ward’s hierarchical ag-
glomerative clustering method: which algorithms implement Ward’s crite-
rion? Journal of Classification, 31, 274–295.
[41] Pakhira, M. K., Bandyopadhyay, S. and Maulik, U. (2004). Validity
index for crisp and fuzzy clusters. Pattern Recognition, 37, 487-501.
[42] Ryan, T. A., Joiner, B. L. and Ryan, B. F. (1976). The Minitab Student
Handbook. Duxbury Press.
[43] Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpre-
tation and validation of cluster analysis. Journal of Computational and
Applied Mathematics. 20, 53–65.
[44] Ripley, B. D. (1996). Pattern Recognition and Neural Networks. Cam-
bridge University Press, Cambridge.
[45] Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpre-
tation and validation of cluster analysis. Journal of Computational and
Applied Mathematics. 20, 53–65.
43[46] Ruspini, E. H. (1970). Numerical methods for fuzzy clustering. Informa-
tion Sciences. 2, 319–350.
[47] Schwarz, G. (1978). Estimating the Dimension of a Model. The Annals
of Statistics, 6, 461–464.
[48] Silva, L. E. B. D., Melton, N. M. andWunsch, D. C. (2020). Incremental
ClusterValidityIndicesforOnlineLearningofHardPartitions: Extensions
and Comparative Study. in IEEE Access, 8, 22025-22047.
[49] Silverman, B. W. (1986), Density Estimation for Statistics and Data
Analysis, Chapman and Hall, London.
[50] Sureja, N., Chawda, B. and Vasant, A. (2022). An improved K-medoids
clustering approach based on the crow search algorithm. Journal of Com-
putational Mathematics and Data Science, 3, 100034.
[51] Tibshirani, R., Walther, G. and Hastie, T. (2001). Estimating the num-
ber of data clusters via the Gap statistic. Journal of the Royal Statistical
Society B, 63, 411–423.
[52] Vale, D. C. and Maurelli V. A. (1983). Simulating multivariate nonnor-
mal distributions. Psychometrika, 48, 465–471.
44