OpenBias: Open-set Bias Detection in Text-to-Image Generative Models
MorenoD’Inca`1,EliaPeruzzo1,MassimilianoMancini1,DejiaXu2,ViditGoel3,4,
XingqianXu3,4,ZhangyangWang2,4,HumphreyShi3,4†,NicuSebe1†
1UniversityofTrento,2UTAustin,3SHILabs@GeorgiaTech&UIUC,4PicsartAIResearch(PAIR)
https://github.com/Picsart-AI-Research/OpenBias
Abstract
Closed-set
Pre-defined biases
Text-to-image generative models are becoming increas-
"A person using Bias specific
ingly popular and accessible to the general public. As a laptop" Classifier Male
these models see large-scale deployments, it is necessary
todeeplyinvestigatetheirsafetyandfairnesstonotdissem-
inate and perpetuate any kind of biases. However, exist- OpenBias (ours)
Open-set biases
ingworksfocusondetectingclosedsetsofbiasesdefineda
Gender
priori, limiting the studies to well-known concepts. In this Bias proposals ...
paper, we tackle the challenge of open-set bias detection Laptop brand
"A person using
intext-to-imagegenerativemodelspresentingOpenBias,a
a laptop"
newpipelinethatidentifiesandquantifiestheseverityofbi- Bias assessment
ases agnostically, without access to any precompiled set. Male
...
OpenBias has three stages. In the first phase, we leverage Apple
aLargeLanguageModel(LLM)toproposebiasesgivena
setofcaptions. Secondly, thetargetgenerativemodelpro-
ducesimagesusingthesamesetofcaptions.Lastly,aVision Figure 1. OpenBias discovers biases in T2I models within an
QuestionAnsweringmodelrecognizesthepresenceandex- open-setscenario. Incontrasttopreviousworks[17,33,72],our
tent of the previously proposed biases. We study the be- pipelinedoesnotrequireapredefinedlistofbiasesbutproposesa
setofnoveldomain-specificbiases.
haviorofStableDiffusion1.5,2,andXLemphasizingnew
biases, never investigated before. Via quantitative experi-
Ethicaltopicssuchasfairnessandbiaseshaveseenmany
ments, we demonstrate that OpenBias agrees with current
definitionsandframeworks[64];definingthemcomprehen-
closed-setbiasdetectionmethodsandhumanjudgement.
sivelyposesachallenge,asinterpretationsvaryandaresub-
jective to the individual user. Following previous works
[17, 70], a model is considered unbiased regarding a spe-
1.Introduction
cific concept if, given a context t that is agnostic to class
Text-to-Image (T2I) generation has become increasingly distinctions, the possible classes c ∈ C exhibit a uniform
popular, thanks to its intuitive conditioning and the high distribution. Inpractice,foraT2Imodel,thisreflectstothe
qualityandfidelityofthegeneratedcontent[48,50,52,53, tendency of the generator to produce content of a certain
55]. SeveralworksextendedthebaseT2Imodel,unlocking
classc(e.g.“man”),givenatextualprompttthatdoesnot
additionalusecases,includingpersonalization[18,54],im-
specifytheintendedclass(e.g.“Apictureofadoctor”).
ageediting[8,16,21,24],andvariousformsofcondition- Several works studied bias mitigation in pre-trained
ing[2,28,73].Thisrapidprogressurgestoinvestigateother models, by introducing training-related methods [29, 46,
key aspects beyond image quality improvements, such as 56,67]orusingdataaugmentationtechniques[1,14]. Nev-
theirfairnessandpotentialbiasperpetration[13,17,72]. It ertheless, a notable limitation of these approaches is their
iswidelyacknowledgedthatdeeplearningmodelslearnthe dependence on a predefined set of biases, such as gen-
underlying biases present in their training sets [4, 23, 74], der, age, and race [13, 17], as well as specific face at-
andgenerativemodelsarenoexception[13,17,45,72]. tributes[72]. Whiletheserepresentperhapsthemostsensi-
tivebiases,wearguethattherecouldbebiasesthatremain
† Correspondingauthors. undiscovered and unstudied. Considering the example in
1
4202
rpA
11
]VC.sc[
1v09970.4042:viXraFig.1,theprompt“Apersonusingalaptop”doesnotspec- 15, 49] and multi-modal models [42, 51, 76]. These mod-
ify the person’s appearance and neither the specific laptop els can be fine-tuned on downstream tasks or applied in a
northescenario.Whileclosed-setpipelinescandetectwell- zero-shotmanner,generalizingtounseentasks[9,60,68].
knownbiases(e.g.gender,race),theT2Imodelmayexhibit Lately, several works combined different foundation
biases also for other elements (e.g. laptop brand, office). models to solve complex tasks. [22, 61] use an LLM to
Thus, an open research question is: Can we identify arbi- generatePythoncodethatinvokesvision-languagemodels
trarybiasespresentinT2Imodelsgivenonlypromptsand to produce results. TIFA [27] assesses the faithfulness of
no pre-specified classes? This is challenging as collecting a generated image to a given text prompt, by querying a
annotateddataforallpotentialbiasesisprohibitive. VQAmodelwithquestionsproducedbyanLLMfromthe
Towardthisgoal,weproposeOpenBias,thefirstpipeline original caption. Similarly, [11, 76] enhance image/video
that operates in an open-set scenario, enabling to iden- captioningbyiterativelyqueryinganLLMtoaskquestions
tify,recognize,andquantifybiasesinaspecificT2Imodel to a VQA model. Differently, [35] identify spurious cor-
without constraints (or data collection) for a specific pre- relations in synthetic images via captioning and language
defined set. Specifically, we exploit the multi-modal na- interpretation,butwithoutcategorizingorquantifyingbias.
ture of T2I models and create a knowledge base of possi- We share a similar motivation, i.e., we leverage power-
ble biases given a collection of target textual captions, by ful foundation models to build an automatic pipeline, tai-
queryingaLargeLanguageModel(LLM).Inthisway,we loredtothenoveltaskofopen-setbiasdiscovery.OpenBias
discover specific biases for the given captions. Next, we builds a knowledge base of biases leveraging the domain-
needtorecognizewhetherthesebiasesareactuallypresent specificknowledgefromrealcaptionsandLLMs.
in the images. For this step, we leverage available Visual Bias Mitigation in Generative Models. Bias mitigation
Question Answering (VQA) models, directly using them is a long-studied topic in generative models. A substan-
to assess the bias presence. By doing this, we overcome tial line of work focused on GAN-based methods. Some
thelimitationofusingattributes-specificclassifiersasdone worksimprovefairnessatinferencetimebyalteringthela-
in previous works [17, 59, 72], which is not efficient nor tentspacesemanticdistribution[62]orbygradientclipping
feasible in an open-set scenario. Our pipeline is modular to control the gradient ensuring fairer representations for
andflexible,allowingfortheseamlessreplacementofeach sensitive groups [33]. The advent of T2I generative mod-
componentwithnewerordomain-specificversionsasthey elshasdirectedresearcheffortstowardsfairnesswithinthis
becomeavailable. Moreover,wetreatthegenerativemodel domain. FairDiffusion [17] guides Stable Diffusion [53]
as a black box, querying it with specific prompts to mimic toward fairer generation in job-related contexts. It en-
end-userinteractions(i.e.withoutcontrolovertrainingdata hances classifier-free guidance [25] by adding a fair guid-
and algorithm). We test OpenBias on variants of Stable ance term based on user-provided fair instructions. Simi-
Diffusion[50,53]showinghuman-agreement,model-level larly, [7] demonstrates that (negative) prompt and seman-
comparisons,andthediscoveryofnovelbiases. tic guidance [6] mitigate inappropriateness generation in
Contributions. Tosummarize,ourkeycontributionsare: several T2I models. Given handwritten text as input, ITI-
• Tothebestofourknowledge,wearethefirsttostudythe GEN [72] enhances the fairness of T2I generative models
problemofopen-setbiasdetectionatlargescalewithout through prompt learning. To improve fairness, [59] guide
relyingonapredefinedlistofbiases. Ourmethoddiscov- generationusingthedatamanifoldofthetrainingset,esti-
ersnovelbiasesthathaveneverbeenstudiedbefore. matedviaunsupervisedlearning.
• We propose OpenBias, a modular pipeline, that, given a Whileyieldingnotableresult,thesebiasmitigationmeth-
listofprompts,leveragesaLargeLanguageModeltoex- ods rely on predefined lists of biases. Here, we argue that
tract a knowledge base of possible biases, and a Vision theremayexistotherbiasesnotconsideredbythesemeth-
QuestionAnswermodeltorecognizeandquantifythem. ods. Therefore, our proposed pipeline is orthogonal, pro-
• Wetestourpipelineonmultipletext-to-imagegenerative vidingavaluabletooltoenhancetheirutility.
models: Stable Diffusion XL, 1.5, 2 [50, 53]. We as-
sess our pipeline showing its agreement with closed-set 3.OpenBias
classifier-basedmethodsandwithhumanjudgement.
ThissectionpresentsOpenBias,ourpipelineforproposing,
assessing, and quantifying biases in T2I generative mod-
2.Relatedwork
els. The overview of the proposed framework is outlined
Pipeline with Foundation Models. We broadly refer to in Fig. 2. Starting from a dataset of real textual captions,
foundation models [5] as large-scale deep learning mod- we leverage a Large Language Model (LLM) to build a
els trained on extensive data corpora, usually with a self- knowledge base of possible biases that may occur during
supervised objective [5]. This approach has been used imagegeneration. Thisprocessenablestheidentificationof
acrossdifferentmodalities,suchastext[9,63],vision[10, domain-specific biases unexplored up to now. In the sec-
2Bias Proposals Bias Assessment Bias Quantification
Bias Ranking
A chef in a kitchen standing next to jars
A person that is posing by a plane
A large round cake rests on a glass plate Person gender
... LLM Train color G VQA
A baby eating in a baby seat
A boy with a book sitting on a toilet Vehicle Type
A man is playing tennis on a tennis court ... Class 1Class 2 ... Class ... ...
Person Race
Target Synthetic Images
Generative Model
Person Race
Classes
Captions Questions
Caucasian
A chef in a kitchen standing next to jars What is the race of the person? African American
... ... ...
A kid on a beach throwing a Frisbee What is the race of the person throwing a Frisbee? Hispanic
Figure2. OpenBiaspipeline. Startingwithadatasetofrealtextualcaptions(T)weleverageaLargeLanguageModel(LLM)tobuild
aknowledgebaseBofpossiblebiasesthatmayoccurduringtheimagegenerationprocess. Inthesecondstage,synthesizedimagesare
generated using the target generative model conditioned on captions where a potential bias has been identified. Finally, the biases are
assessedandquantifiedbyqueryingaVQAmodelwithcaption-specificquestionsextractedduringthebiasproposalphase.
ondstage,wesynthesizeimagesusingthetargetgenerative tothebiasbinT.Nevertheless,D doesnotaccountforthe
b
model, conditioned on captions where a potential bias has potentialspecificationoftheclassesofbinthecaption. For
been identified. Lastly, we assess the biases with a VQA instance, ifweaimtogenerate“Animageofalargedog”,
model, querying it with caption-specific questions gener- thedog’ssizeshouldnotbeincludedamongthebiases. To
atedduringthebiasproposalphase. addressthis, weimplementatwo-stagefilteringprocedure
of D . First, given a pair (t,q) ∈ D we ask the LLM
b b
3.1.BiasProposals
tooutputwhethertheanswertothequestionq isexplicitly
presentinthecaptiont. Secondly,weleverageConceptNet
Given a dataset of real captions T, we construct a knowl-
[57] to identify synonyms for the classes C related to the
edge base B of possible biases. For each caption in the b
specificbiasb,andfilteroutthecaptionsincontainingeither
dataset, we task a LLM with providing three outputs: the
a class C or its synonyms. We empirically observe that
potentialbiasname,asetofclassesassociatedwiththebias, b
combiningthesetwostagesproducesmorerobustresults.
andaquestiontoidentifythebias.
Formally,givenacaptiont∈T,letusdenotetheLLM’s By executing the aforementioned steps, we generate
output as a set of triplets L = {(bt,Ct,qt)}nt where the bias proposals in an open-set manner tailored to the given
t i i i i=1
cardinality of the set n is caption dependent, and each dataset. Inthefollowingsections,weelaborateonthepro-
t
triplet (b,C,q) has a proposed bias b, a set of associated cessofbiasquantificationinatargetgenerativemodel.
classesC,andthequestionqassignedtothespecificcaption
3.2.BiasAssessmentandQuantification
t. To obtain this set, we propose to use in-context learn-
ing [9, 58], providing task description and demonstrations LetGbethetargetT2Igenerativemodel. Ourobjectiveis
directly in the textual prompt.1 We build the knowledge toevaluateifGgeneratesimageswiththeidentifiedbiases.
base B by aggregating the per-caption information on the Given a bias b ∈ B and a caption t ∈ T , we generate the
b
wholedataset.Specifically,wecandefinethesetofcaption- setofN imagesItas
b
specific biases B as the union of its potential biases, i.e.
t
B t =
(cid:83)n
i=t 1b i. The dataset-level set of b (cid:83)iases is then the I bt ={G(t,s)|∀s∈S} (2)
union of the caption-level ones, i.e. B = B . Next,
t∈T t
weaggregatethebias-specificinformationacrossthewhole whereS isthesetofsampledrandomnoise,ofcardinality
dataset.Wedefinethedatabaseofcaptionsandquestionsas |S| = N. Sampling multiple noise vectors allows us to
obtainadistributionoftheGoutputonthesamepromptt.
D b ={(t,q)|∀t∈T,(x,C,q)∈L t,x=b}. (1) To assess the bias within It, we propose to leverage a
b
state-of-the-art Vision Question Answering (VQA) model
D collects captions and questions specific to the bias b.
b
VQA mapping images and questions to answers in natural
Moreover, we define T = {t | (t,q) ∈ D } as the set of
b b language. TheVQAprocessestheimagesIt,andtheiras-
captions,andC bistheunionofthesetofclassesassociated b
sociatedquestionq inthepair(t,q) ∈ D ,choosinganan-
b
1WereferthereadertotheSupp.Mat.forsystempromptdetails. swerfromthepossibleclassesC b.Formally,givenanimage
3I ∈Itwedenotethepredictedclassas within the generative model. As mentioned in Sec. 1, we
b
followexistingwork[17,70]andconsiderthemodelGas
cˆ=VQA(I,q,C b). (3) unbiasedwithrespecttoaconceptbwhenthedistributionof
thepossibleclassesc∈C isuniform. Toquantitativelyas-
b
With this score, we gather statistics on the distribution of
sesstheseverityofthebias,wecomputetheentropyofthe
the classes on a set of images, and use them to quantify
probabilitydistributionoftheclassesobtainedusingeither
the severity of the bias. In the following, we investigate
Eq. (4) or Eq. (5). To compare biases with different num-
twodistinctscenarios,namelycontext-aware,wherewean-
bersofclasses,wenormalizetheentropybythemaximum
alyze the bias on caption-specific images It, and context-
b possibleentropy[69]. Additionally,weadjustthescorefor
free,whereweconsiderthewholesetofimagesI associ-
b enhanced human readability. In practice, our bias severity
atedtoonebiasb∈B.
scoreisdefinedasfollows:
(cid:80)
logp(c|C ,D )
3.2.1 Context-AwareBias H¯ =1+ c∈Cb b b (6)
b log(|C |)
b
AsdiscussedinSection1,ourfocusliesinexaminingbias
TheresultingscoreisalwaysboundedH¯ ∈[0,1],where0
exclusively when the classes are not explicitly mentioned b
indicatesanunbiasedconceptwhile1abiasedone.
in the caption. The bias proposals pipeline described in
We note that, while we focused our pipeline on condi-
Sec.3.1filtersoutsuchcases; nevertheless, therecouldbe
tionalgenerativemodels,ourmodelcanbeeasilyextended
additional aspects within the caption that impact the out-
for studying biases in both real-world multimodal datasets
come. For example, the two captions “A military is run-
(e.g.byassumingimagesItareprovidedratherthangener-
ning” and “A person is running” are both agnostic to the b
ated),andtounconditionalgenerativemodels(i.e.byusing
bias “person gender”, but the direction and magnitude of
a captioning system on their outputs as set T). We refer
thebiasmaybeverydifferentinthetwocases. Toconsider
thereadertotheSupp.Mat. fordetailswherewewillalso
the role of the context in the bias assessment, we collect
show an analysis between the unconditional GAN Style-
statisticsatthecaptionlevel,analyzingthesetofimagesIt
b GAN3[32]anditstrainingsetFFHQ[31].
producedfromaspecificcaptiont ∈ T. Givenabiasbwe
computetheprobabilityforaclassc∈C as:
b 4.Experiments
p(c|t,C ,D )= 1 (cid:88) 1(cid:0) cˆ=c(cid:1) (4) In this section, we conduct a series of experiments to as-
b b |It|
b I∈It sess the proposed framework quantitatively. In Sec. 4.1,
b
we provide implementation details and the preprocessing
withcˆ=VQA(I,q,C )thepredictionoftheVQAasdefined
b steps applied to the datasets. In Sec. 4.2, we quantitative
inEq.(3),and1(·)theindicatorfunction.
evaluateOpenBiasontwodirections,(i)comparingitwith
a state-of-the-art classifier-based method on a closed set
3.2.2 Context-FreeBias of well-known social biases, (ii) testing the agreement be-
tweenOpenBiasandhumanjudgmentviaauserstudy.
Differently from the context-aware scenario, our interest
lies in characterizing the overall behavior of the model G. 4.1.PipelineImplementation
Thisiscrucialasitoffersvaluableinsightsintoaspectssuch
Datasets. We study the bias in two multimodal datasets
asthemajorityclass(i.e.thedirectiontowardwhichthebias
Flickr30k[71]andCOCO[40]. Flickr30k[71]comprises
tends) and the overall intensity of the bias. To effectively
30Kimageswith5captionperimage,depictingimagesin
excludetheroleofthecontextinthecaptions, wepropose
the wild. Similarly, COCO [40] is a large-scale dataset
to average the VQA scores for c ∈ C over all captions t
b
containing a diverse range of images that capture every-
relatedtothatbiasb∈B:
day scenes and objects in complex contexts. We filter this
1 (cid:88) dataset, creating a subset of images whose caption con-
p(c|C ,D )= p(c|t,C ,D ) (5)
b b |D | b b tains a single person. This procedure results in roughly
b
(t,q)∈Db
123Kcaptions. Ourchoiceismotivatedbybuildingalarge
Note that the context-aware bias is a special case of this subset of captions specifically tied to people. This focus
scenario,whereD hasasingleinstance,i.e.D ={(t,q)}. on the person-domain is crucial as it represents one of the
b b
mostsensitivescenariosforexploringbias-relatedsettings.
Nevertheless,itisworthnotingthatthebiaseswediscover
3.2.3 BiasQuantificationandRanking
within this context extend beyond person-related biases to
Aftercollectingthescoresforeachindividualattributeclass include objects, animals, and actions associated with peo-
c∈C ,wecanaggregatethemtoranktheseverityofbiases ple. FurtherdetailsarehighlightedinSec.5.
b
41.0
SD-XL
SD-2
SD-1.5
0.8
0.6
0.4
0.2
0.0
child gende cr at co tl ro ar in bc aol bo yr gend ce ar sk ue rt fy bp oe a sr nd ot wy p ce ondit pi ho on ne ty shp noe r os we ba og are wd ae tr era g ce onditio dn og col po ir z pz ea r ssi oz n e acti pv si et kry as to en ba og are md oe tr o ra cg ye cle type pc erat s oa ng se e kam to eti bo on ar sd k iit ny gp le ocati co hn ild pr ea rc soe pn ea rt sti or ne ge pn ed re sr on sr k pia i ec n re sg oa n bi oli ct cy upation dog ha org se e bre de od g bre ple ad y pe lr aya eg re ge sn kid ie nr g ability kit le as pi tz oe p bran wd ave vs ei hz ie cle ty wp ae ve t ty ep ne s niki s t ply ap ye er le sv kiiel ng l ae irv ce sl r naf ot ws biz oe ard t hy orp se e s kb iir ne ge ld ocatio bn e sld o t py e p de ifficult cy at breed
Figure3.Comparisonofcontext-awarediscoveredbiasesonStableDiffusionXL,2and1.5[50,53]withcaptionsfromCOCO[40].
1.0 SD-XL
SD-2
SD-1.5
0.8
0.6
0.4
0.2
0.0
child gen pd ere fr ormer age dog color artist
s
oa cg ce er posi pti ero sn on ge pn ed rfer orme sr kr aa tc ee board typ we orker pr ea rc se on activity water typ pe erson age child rac pe erso pn
e
rr sa oc ne hair col po er rson attir de og ac pti ev ri st oy n emotio vn ehicle type dog bre se kd iing ability worker ag he ors pe
e
rb sr oe ne d occup ma oti to orn cycle type dog age player pa eg re son heig bh it cycle type wave bsi ez ae ch location dog c plo aat yer gender crowd size boat type
Figure4.Comparisonofcontext-awarefoundbiasesonStableDiffusionXL,2and1.5[50,53]oncaptionsfromFlick30k[71].
Implementation Details. Our pipeline is designed to be Gender Age Race
Model
flexibleandmodular,enablingustoreplaceindividualcom- Acc. F1 Acc. F1 Acc. F1
ponents as needed. In this study, we leverage LLama2-7B CLIP-L[51] 91.43 75.46 58.96 45.77 36.02 33.60
[63] as our foundation LLM. This model is exploited to OFA-Large[66] 93.03 83.07 53.79 41.72 24.61 21.22
buildtheknowledgebaseofpossiblebiases,asdescribedin mPLUG-Large[37] 93.03 82.81 61.37 52.74 21.46 23.26
BLIP-Large[38] 92.23 82.18 48.61 31.29 36.22 35.52
Sec.3.1.WereferthereadertotheSupp.Mat.fordetailsre-
Llava1.5-7B[41,42] 92.03 82.33 66.54 62.16 55.71 42.80
gardingthepromptsandexamplesweusetoinstructLLama Llava1.5-13B[41,42] 92.83 83.21 72.27 70.00 55.91 44.33
toperformthedesiredtasks. Toassessthepresenceofthe
Table1. VQAevaluationonthegeneratedimagesusingCOCO
bias,werelyonstate-of-the-artVisualQuestionAnswering
captions.Wehighlightin gray thechosendefaultVQAmodel.
(VQA) models. From our evaluation outlined in Sec. 4.2,
Llava1.5-13B[41,42]emergesasthetop-performing,thus Flickr30k[71] COCO[40]
Model
we adopt it as our default VQA model. Finally, we con- gender age race gender age race
duct our study by randomly selecting 100 captions associ- Real 0 0.032 0.030 0 0.041 0.028
SD-1.5[53] 0.072 0.032 0.052 0.075 0.028 0.092
atedwitheachbiasandgeneratingN =10imagesforeach
SD-2[53] 0.036 0.069 0.047 0.060 0.045 0.105
captionusingadifferentrandomseed. Inthisway,weob- SD-XL[50] 0.006 0.028 0.180 0.002 0.027 0.184
tainasetof1000images,thatweusetostudythecontext-
Table 2. KL divergence (↓) computed over the predictions of
freeandcontext-awarebiasofthetargetgenerativemodel.
Llava1.5-13BandFairFaceongeneratedandrealimages.
4.2.QuantitativeResults
Agreement with FairFace. We compare the predictions
Ouropen-setsettingharnessesthezero-shotperformanceof ofmultipleSoTAVisualQuestionAnsweringmodelswith
each component. As in [17], we evaluate OpenBias using FairFace. Firstly, we assess the zero-shot performance of
FairFace[30],awell-establishedclassifierfairlytrained,as theVQAmodelsonsyntheticimages,performingourcom-
the ground truth on gender, age, and race. While FairFace parisons using images generated by SD XL. The evalua-
treats socially sensitive attributes as closed-set, we uphold tion involves assessing accuracy and F1 scores, which are
ourcommitmenttoinclusivitybyalsoevaluatingOpenBias computedagainstFairFacepredictionstreatedastheground
withself-identifiedones,reportedintheSupp.Mat.. truth. The results are reported in Tab. 1. Llava1.5-13B
5
ytisnetnI
saiB
ytisnetnI
saiBemergesasthetop-performingmodelacrossdifferenttasks,
1.0 Human
consequently,weemployitasourdefaultVQAmodel. OpenBias (ours)
Next,weevaluatetheagreementbetweenLlavaandFair-
0.8
Face [30] on different scenarios. Specifically, we run the
two models on real and synthetic images generated with
StableDiffusion1.5,2,andXL.Wemeasuretheagreement 0.6
betweenthetwoastheKLDivergencebetweentheproba-
bilitydistributionsobtainedusingthepredictionsofthere- 0.4
spective model. We report the results in Tab. 2. We can
observe that the models are highly aligned, obtaining low
0.2
KL scores, proving the VQA model’s robustness in both
generative and real settings. Supp. Mat. provides a more
0.0
c U po osm se erp dr S pe th iu pe d en lys i. niv ee W ae tev ta hcl eou n ca odti nuo tcn et xo a tf -ahth wue m aV ra eQ n leA e vv. ea l,lu toat aio ssn eso sf it th se alp igro n-
-
Cake typ T Pre eai rsn oc no l oo cr cupatio Fn Piz iz ga si uze W ra Pv eee r ssi o 5z ne .em Hoti Po ern uso mn ra Aic are cr na Sft n os ei wz e vcon aPd ei r lti s uo on n aac tti P ivi e ot rsy o nn att ri Pr eee rs son
u
a lge tB sed .ty Vep he icle P t ery sp oe n gender
mentwithhumanjudgment. Thestudypresents10images
generatedfromthesamecaptionforeachbias. Weusepub-
Fig. 3 and 4. Importantly, OpenBias identifies both well-
lic crowdsourcing platforms, without geographical restric-
known (e.g. “person gender”, “person race”) and novel
tions, and randomizing the questions’ order. Each partic-
biases(e.g.“caketype”, “bedtype”and“laptopbrand”).
ipant is asked to identify the direction (majority class) of
Fromthecomparisonofdifferentmodels,weobserveacor-
eachbiasanditsintensityinarangefrom0to10. Theop-
relation between the intensities of the biases across differ-
tion “No bias” is provided to capture the instances where
ent Stable Diffusion versions. We note, however, a subtle
nobiasisperceived,correspondingtoabiasintensityof0.
predominance of SD XL in the amplification of bias com-
We conduct the user study on a subsection of the biases,
paredtoearlierversionsofthemodel. Moreover,thesetof
resultingin15diverseobject-relatedandperson-relatedbi-
proposed biases varies depending on the initial set of cap-
ases and 390 diverse images. We collect answers from 55
tions used for the extraction. Generally, biases extracted
uniqueusers, foratotalof2200validresponses. Theuser
fromFlickraremoreobject-centriccomparedtothosefrom
study results are shown in Fig. 5, where we compare the
COCO, aligning with the filtering operation applied to the
biasintensityascollectedfromthehumanparticipantswith
latter. ThisdifferencehighlightsthepotentialofOpenBias
the severity score computed with OpenBias. We can ob-
toproposeatailoredsetofbiasesaccordingtothecaptions
serve that there is a high alignment on various biases such
itisappliedto,makingthebiasproposalsdomain-specific.
as “Person age”, “Person gender”, “Vehicle type”, “Per-
Context-Free vs Context-Aware. Next, we study the dif-
son emotion” and “Train color”. We compute the Abso-
ferent behavior of a given model, when compared in a
luteMeanError(AME)betweenthebiasintensityproduced
context-free vs context-aware scenario (see Sec. 3 for for-
by the model and the average user score, resulting in an
maldefinition).Thisanalysisassessestheinfluenceofother
AME = 0.15. Furthermore, we compute the agreement
elementswithinthecaptionsontheperpetuationofapartic-
on the majority class, i.e. the direction of the bias. In this
ularbias.InFig.9wereporttheresultsobtainedonSDXL.
case, OpenBias matches the collected human choices 67%
Itisnoteworthytoobservethat,inthiscase,thecorrelation
ofthecases. Weremarkthatconceptsofbiasandfairness
betweenthescoresisnotconsistentlypresent.Forexample,
are highly subjective, and this can introduce further errors
the intensity score for “motorcycle type” is significantly
intheevaluationprocess. Nevertheless, ourresultsshowa
higherwhencomputedwithinthecontext,comparedtothe
correlationbetweenthescores,validatingourpipeline.
sameevaluationfreeofcontext. Thisdiscrepancysuggests
that there is no majority class (i.e. the general direction of
5.Findings
thebias),butratherthemodelgeneratesmotorcyclesofone
In this section, we present our findings from the exam- specifictypeinagivencontext. Viceversa,for“bedtype”
ination of three extensively utilized text-to-image gener- weobserveahighscoreinbothsettings,suggestingthatthe
ative models, specifically Stable Diffusion XL, 2, and modelalwaysgeneratesthesametypeofbed.
1.5 [50, 53]. We use captions from Flickr and COCO, as Qualitative Results. We show examples of biases dis-
detailed in Sec. 4.1. We structure our findings by examin- covered by OpenBias on Stable Diffusion XL. We present
ing the biases of different models and delineating the dis- theresultsinacontext-awarefashionandvisualizeimages
tinctionsbetweencontext-freeandcontext-awarebias. generatedfromthesamecaptionwhereourpipelineidenti-
Rankings. We present here the biases identified by our fiesabias. Weorganizetheresultsinthreesetsandpresent
pipeline on Stable Diffusion XL, 2, and 1.5 [50, 53], in unexploredbiasesonobjectsandanimals, novelbiasesas-
6
ytisnetnI
saiBTraincolor Laptopbrand Horsebreed
“Atrainzipsdowntherailwayinthesun” “Aphotoofapersononalaptopinacoffeeshop” “Acopridingahorsethroughacityneighborhood”
Figure6.NovelbiasesdiscoveredonStableDiffusionXL[50]byOpenBias.
Childgender Childrace Personattire
“Toddlerinabaseballcaponawoodenbench” “Smallchildhurryingtowardabusonadirtroad” “Theladyissittingonthebenchholdingherhandbag”
Figure7.Novelperson-relatedbiasesidentifiedonStableDiffusionXL[50]byOpenBias.
Persongender Personrace Personage
“Atrafficofficerleaningonanoturnsign” “Amanridinganelephantintosomewaterofacreek” “Awomanridingahorseinfrontofacarnexttoafence”
Figure8.Person-relatedbiasesfoundonStableDiffusionXL[50]byOpenBias.
71.0 els,withoutconsideringtheirintrinsiclimitations. Existing
Context Aware
Context Free research[19,47]highlightsthepresenceofbiasesinthese
0.8 models which may be propagated in our pipeline. Never-
theless, the modular nature of our pipeline provides flexi-
bility,allowingustoseamlesslyincorporateimprovedmod-
0.6
els should they become available in the future. Finally, in
this work, we delve into the distinction between context-
0.4
freeandcontext-awarebiases,revealingdifferentbehaviors
exhibited by models in these two scenarios. However, our
0.2
evaluationoftheroleofthecontextisonlyqualitative. We
identify the possibility of systematically studying the con-
0.0
child gender person a mg oe torcycle ty pp ee rson emotion child race person attir pe erson gender person race laptop brand bed type text’sroleasapromisingfuturedirection.
Figure 9. Highlighting the importance of the context aware ap- 7.Conclusions
proachonStableDiffusionXL[50]onthecaptionsfromCOCO.
AI-generatedcontenthasseenrapidgrowthinthelastfew
sociated with persons, and well-known social biases. We years,withthepotentialtobecomeevenmoreubiquitousin
highlightbiasesdiscoveredonobjectsandanimalsinFig.6. society. Whiletheusageofsuchmodelsincreases,charac-
Forexample,themodeltendstogenerate“yellow”trainsor terizing the stereotypes perpetrated by the model becomes
“quarterhorses”evenifnotspecifiedinthecaption. Fur- ofsignificantimportance. Inthiswork,weproposetostudy
thermore, the model generates laptops featuring a distinct thebiasingenerativemodelsinanovelopen-setscenario,
“Apple”logo,showingabiastowardthebrand. paving the way to the discovery of biases previously un-
explored. We propose OpenBias, an automatic bias detec-
Next,wedisplaynovelbiasesrelatedtopersonsdiscov-
tionpipeline,capableofdiscoveringandquantifyingtradi-
ered by OpenBias. For instance, we unveil unexplored bi-
tionalandnovelbiaseswithouttheneedtopre-definethem.
asessuchasthe“personattire”,withthemodeloftengen-
The proposed method builds a domain-specific knowledge
erating people in a formal outfit rather than more casual
base of biases which are then assessed and quantified via
ones. Furthermore, we specifically study “child gender”
Vision Question Answering. We validate OpenBias show-
and “child race” diverging from the typical examination
ingitsagreementwithclassifier-basedmethodsonaclosed
centeredonadults. Forexample, inFig.7secondcolumn,
set of concepts and with human judgement through a user
we observe that the generative model links a black child
study. Our method can be plugged into existing bias mit-
withaneconomicallydisadvantagedenvironmentdescribed
igation works, extending their capabilities to novel biases.
in the caption as “a dirt road”. The association between
OpenBiascanfosterfurtherresearchinopen-setscenarios,
racialidentityandsocioeconomicstatusperpetuatesharm-
moving beyond classical pre-defined biases and assessing
ful stereotypes and proves the need to consider novel bi-
generativemodelsmorecomprehensively.
ases within bias mitigation frameworks. Lastly, we show
Ethical statement and broader impact. This work con-
qualitative results on the well-studied and sensitive biases
of “person gender”, “race”, and “age”. In the first col- tributestofairerandmoreinclusiveAI,bydetectingbiases
inT2Igenerativemodels. Weconductourresearchrespon-
umn of Fig. 8, Stable Diffusion XL exclusively generates
“male” officers, despite the presence of a gender-neutral sibly, transparently, and with a strong commitment to eth-
jobtitle. Moreover,itexplicitlydepictsa“woman”labeled ical principles. Despite this, due to technical constraints,
as “middle-aged” when engaged in horseback riding. Fi- socially sensitive attributes, such as gender, are treated as
nally, we observe a “race” bias, with depictions of solely closed sets for research purposes only. Moreover, Open-
black individuals for “a man riding an elephant”. This Bias entails the biases of the LLM and VQA models, thus
itmaynotdiscoverallpossiblebiases. Wedonotintendto
context-awareapproachensuresathoroughcomprehension
discriminate against any social group but raise awareness
of emerging biases in both novel and socially significant
onthechallengesofdetectingbiasesbeyondclosedsets.
contexts. These results emphasize the necessity for more
inclusive open-set bias detection frameworks. We provide Acknowledgments: ThisworkwassupportedbytheMUR
additionalqualitativesandcomparisonsintheSupp.Mat.. PNRRprojectFAIR(PE00000013)fundedbytheNextGen-
erationEU and by the EU Horizon projects ELIAS (No.
101120237) and AI4Media (No. 951911), NSF CAREER
6.Limitations
Award#2239840, andtheNationalAIInstituteforExcep-
OpenBiasisbasedontwofoundationmodelstoproposeand tional Education (Award #2229873) by National Science
quantifybiasesofagenerativemodel,namelyLLama[63] Foundation and the Institute of Education Sciences, U.S.
and LLava [42]. We rely on the prediction of these mod- DepartmentofEducation,andPicsartAIResearch(PAIR).
8
ytisnetnI
saiBReferences [13] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval:
Probing the reasoning skills and social biases of text-to-
[1] SharatAgarwal,SumanyuMuku,SaketAnand,andChetan
imagegenerationmodels. InICCV,2023. 1
Arora. Doesdatarepairleadtofairmodels? curatingcon-
[14] MorenoD’Inca`,ChristosTzelepis,IoannisPatras,andNicu
textuallyfairdatatoreducemodelbias. InWACV,2022. 1
Sebe. Improvingfairnessusingvision-languagedrivenim-
[2] OmriAvrahami, ThomasHayes, OranGafni, SonalGupta,
ageaugmentation. InWACV,2024. 1
Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
andXiYin. Spatext: Spatio-textualrepresentationforcon-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
trollableimagegeneration. InCVPR,2023. 1
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
[3] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal
vain Gelly, et al. An image is worth 16x16 words: Trans-
Ladhak,MyraCheng,DeboraNozza,TatsunoriHashimoto,
formersforimagerecognitionatscale. InICLR,2021. 2
DanJurafsky,JamesZou,andAylinCaliskan. Easilyacces-
[16] DaveEpstein, AllanJabri, BenPoole, AlexeiAEfros, and
sibletext-to-imagegenerationamplifiesdemographicstereo-
Aleksander Holynski. Diffusion self-guidance for control-
typesatlargescale.InProceedingsofthe2023ACMConfer-
lableimagegeneration. InNeurIPS,2023. 1
ence on Fairness, Accountability, and Transparency, 2023.
[17] Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik
14
Hintersdorf, Patrick Schramowski, Sasha Luccioni, and
[4] TolgaBolukbasi,Kai-WeiChang,JamesYZou,Venkatesh
KristianKersting. Fairdiffusion: Instructingtext-to-image
Saligrama,andAdamTKalai.Manistocomputerprogram-
generationmodelsonfairness. arXivpreprint,2023. 1,2,4,
mer as woman is to homemaker? debiasing word embed-
5,14
dings. InNeurIPS,2016. 1
[18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
[5] RishiBommasani,DrewAHudson,EhsanAdeli,RussAlt-
AmitH.Bermano,GalChechik,andDanielCohen-Or. An
man,SimranArora,SydneyvonArx,MichaelSBernstein,
imageisworthoneword: Personalizingtext-to-imagegen-
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
erationusingtextualinversion. arXivpreprint,2022. 1
Ontheopportunitiesandrisksoffoundationmodels. arXiv
[19] IsabelOGallegos, RyanARossi, JoeBarrow, MdMehrab
preprint,2021. 2
Tanjim,SungchulKim,FranckDernoncourt,TongYu,Ruiyi
[6] ManuelBrack,FelixFriedrich,DominikHintersdorf,Lukas
Zhang, and Nesreen K Ahmed. Bias and fairness in large
Struppek,PatrickSchramowski,andKristianKersting.Sega:
languagemodels:Asurvey. arXivpreprint,2023. 8
Instructing text-to-image models using semantic guidance.
[20] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna
InNeurIPS,2023. 2
Materzyn´ska,andDavidBau.Unifiedconcepteditingindif-
[7] Manuel Brack, Felix Friedrich, Patrick Schramowski, and
fusionmodels. InWACV,2024. 13
Kristian Kersting. Mitigating inappropriateness in image
[21] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian
generation:Cantherebevalueinreflectingtheworld’sugli-
Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, and
ness? arXivpreprint,2023. 2
HumphreyShi.Pair-diffusion:Acomprehensivemultimodal
[8] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
object-levelimageeditor. arXivpreprint,2023. 1
structpix2pix:Learningtofollowimageeditinginstructions.
InCVPR,2023. 1 [22] TanmayGuptaandAniruddhaKembhavi. Visualprogram-
ming: Compositionalvisualreasoningwithouttraining. In
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
CVPR,2023. 2
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- [23] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Darrell,andAnnaRohrbach.Womenalsosnowboard:Over-
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, comingbiasincaptioningmodels. InECCV,2018. 1
JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,Eric [24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack YaelPritch,andDanielCohen-Or. Prompt-to-promptimage
Clark,ChristopherBerner,SamMcCandlish,AlecRadford, editingwithcrossattentioncontrol. InICLR,2022. 1
Ilya Sutskever, and Dario Amodei. Language models are [25] Jonathan Ho and Tim Salimans. Classifier-free diffusion
few-shotlearners. InNeurIPS,2020. 2,3,12 guidance. InNeurIPS2021WorkshoponDeepGenerative
[10] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou, ModelsandDownstreamApplications,2021. 2
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg- [26] YushiHu,HangHua,ZhengyuanYang,WeijiaShi,NoahA.
ing properties in self-supervised vision transformers. In Smith, and Jiebo Luo. Promptcap: Prompt-guided image
ICCV,2021. 2 captioningforvqawithgpt-3. InICCV,2023. 13
[11] Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, and [27] YushiHu,BenlinLiu,JungoKasai,YizhongWang,MariOs-
MohamedElhoseiny. Videochatcaptioner: Towardstheen- tendorf,RanjayKrishna,andNoahA.Smith.Tifa:Accurate
richedspatiotemporaldescriptions. arXivpreprint,2023. 2 and interpretable text-to-image faithfulness evaluation with
[12] MehdiCherti,RomainBeaumont,RossWightman,Mitchell questionanswering. InICCV,2023. 2
Wortsman,GabrielIlharco,CadeGordon,ChristophSchuh- [28] LianghuaHuang,DiChen,YuLiu,YujunShen,DeliZhao,
mann,LudwigSchmidt,andJeniaJitsev.Reproduciblescal- andJingrenZhou. Composer:Creativeandcontrollableim-
inglawsforcontrastivelanguage-imagelearning. InCVPR, agesynthesiswithcomposableconditions. InICML,2023.
2023. 13 1
9[29] SangwonJung,SanghyukChun,andTaesupMoon. Learn- [45] RanjitaNaikandBesmiraNushi. Socialbiasesthroughthe
ingfairclassifierswithpartiallyannotatedgrouplabels. In text-to-image generation lens. In Proceedings of the 2023
CVPR,2022. 1 AAAI/ACMConferenceonAI,Ethics,andSociety,2023. 1,
[30] KimmoKarkkainenandJungseockJoo. Fairface: Faceat- 14
tribute dataset for balanced race, gender, and age for bias [46] JunhyunNam,HyuntakCha,SungsooAhn,JaehoLee,and
measurementandmitigation. InWACV,2021. 5,6,12 Jinwoo Shin. Learning from failure: De-biasing classifier
[31] Tero Karras, Samuli Laine, and Timo Aila. A style-based frombiasedclassifier. NeurIPS,2020. 1
generatorarchitectureforgenerativeadversarialnetworks.In [47] RobertoNavigli,SimoneConia,andBjo¨rnRoss. Biasesin
CVPR,2019. 4,14,16 large language models: Origins, inventory, and discussion.
[32] Tero Karras, Miika Aittala, Samuli Laine, Erik Ha¨rko¨nen, ACMJournalofDataandInformationQuality,2023. 8
JanneHellsten,JaakkoLehtinen,andTimoAila. Alias-free [48] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
generativeadversarialnetworks.InNeurIPS,2021.4,14,16 Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
[33] PatrikJoslinKenfack,KamilSabbagh,Ad´ınRam´ırezRivera, MarkChen. Glide:Towardsphotorealisticimagegeneration
andAdilKhan. Repfair-gan: Mitigatingrepresentationbias and editing with text-guided diffusion models. In Interna-
ingansusinggradientclipping. arXivpreprint,2022. 1,2 tionalConferenceonMachineLearning,2022. 1
[34] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-
[49] Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy
and-languagetransformerwithoutconvolutionorregionsu-
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
pervision. InICML,2021. 13
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.
[35] Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Dinov2:Learningrobustvisualfeatureswithoutsupervision.
Lee, Jaeho Lee, and Jinwoo Shin. Bias-to-text: Debias- InTransactionsonMachineLearningResearch,2023. 2
ingunknownvisualbiasesthroughlanguageinterpretation.
[50] Dustin Podell, Zion English, Kyle Lacey, Andreas
arXivpreprint,2023. 2
Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
[36] Anjana Lakshmi, Bernd Wittenbrink, Joshua Correll, and
Robin Rombach. SDXL: Improving latent diffusion mod-
Debbie S. Ma. The india face set: International and cul-
elsforhigh-resolutionimagesynthesis. InICLR,2024. 1,2,
turalboundariesimpactfaceimpressionsandperceptionsof
5,6,7,8,12,13,14
categorymembership. FrontiersinPsychology,2021. 14
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[37] ChenliangLi,HaiyangXu,JunfengTian,WeiWang,Ming
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
Cao,etal. mPLUG:Effectiveandefficientvision-language
ingtransferablevisualmodelsfromnaturallanguagesuper-
learningbycross-modalskip-connections.InProceedingsof
vision. InICML,2021. 2,5,13,14
the2022ConferenceonEmpiricalMethodsinNaturalLan-
[52] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
guageProcessing,2022. 5,13
andMarkChen. Hierarchicaltext-conditionalimagegener-
[38] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
ationwithcliplatents. arXivpreprint,2022. 1
Blip: Bootstrappinglanguage-imagepre-trainingforunified
[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
vision-language understanding and generation. In ICML,
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
2022. 5,13
thesiswithlatentdiffusionmodels. InCVPR,2022. 1,2,5,
[39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
6,14
BLIP-2: Bootstrapping language-image pre-training with
[54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
frozenimageencodersandlargelanguagemodels. InPro-
MichaelRubinstein,andKfirAberman. Dreambooth: Fine
ceedings of the 40th International Conference on Machine
tuning text-to-image diffusion models for subject-driven
Learning,2023. 13
generation. InCVPR,2023. 1
[40] Tsung-YiLin,MichaelMaire,SergeJ.Belongie,LubomirD.
Bourdev,RossB.Girshick,JamesHays,PietroPerona,Deva [55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Ramanan,PiotrDolla´r,andC.LawrenceZitnick. Microsoft Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
COCO:commonobjectsincontext. InECCV,2014. 4,5, RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
12,13,14,15 etal.Photorealistictext-to-imagediffusionmodelswithdeep
[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae languageunderstanding. NeurIPS,2022. 1
Lee. Improvedbaselineswithvisualinstructiontuning. In [56] Yash Savani, Colin White, and Naveen Sundar Govindara-
NeurIPS2023WorkshoponInstructionTuningandInstruc- julu. Intra-processing methods for debiasing neural net-
tionFollowing,2023. 5,12,13 works. InNeurIPS,2020. 1
[42] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. [57] RobynSpeer,JoshuaChin,andCatherineHavasi. Concept-
Visualinstructiontuning. InNeurIPS,2023. 2,5,8,12,13 net5.5: Anopenmultilingualgraphofgeneralknowledge.
[43] DebbieS.Ma,JoshuaCorrell,andBerndWittenbrink. The InAAAI,2017. 3
chicagofacedatabase:Afreestimulussetoffacesandnorm- [58] Hongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi,
ingdata. 2015. 14 Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke
[44] Debbie S. Ma, Justin Kantner, and Bernd Wittenbrink. Zettlemoyer, Noah A. Smith, and Tao Yu. Selective anno-
Chicagofacedatabase:Multiracialexpansion. BehaviorRe- tation makes language models better few-shot learners. In
searchMethods,2020. 14 ICLR,2023. 3,12
10[59] XingzheSu,YiRen,WenwenQiang,ZeenSong,HangGao, [73] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
FenggeWu,andChangwenZheng. Unbiasedimagesynthe- conditional control to text-to-image diffusion models. In
sisviamanifold-drivensamplingindiffusionmodels. arXiv ICCV,2023. 1
preprint,2023. 2 [74] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,
[60] SanjaySubramanian,WilliamMerrill,TrevorDarrell,Matt and Kai-Wei Chang. Men also like shopping: Reducing
Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A genderbiasamplificationusingcorpus-levelconstraints. In
strong zero-shot baseline for referring expression compre- EMNLP,2017. 1
hension. InProceedingsofthe60thAnnualMeetingofthe [75] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,
AssociationforComputationalLinguistics,2022. 2 andKai-WeiChang. Genderbiasincoreferenceresolution:
[61] D´ıdacSur´ıs,SachitMenon,andCarlVondrick.Vipergpt:Vi- Evaluation and debiasing methods. In Proceedings of the
sualinferenceviapythonexecutionforreasoning. InICCV, 2018ConferenceoftheNorthAmericanChapteroftheAs-
2023. 2 sociationforComputationalLinguistics: HumanLanguage
Technologies,2018. 13
[62] Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving the
fairnessofdeepgenerativemodelswithoutretraining. arXiv [76] DeyaoZhu,JunChen,KilichbekHaydarov,XiaoqianShen,
preprint,2021. 2 Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks,
blip-2answers: Automaticquestioningtowardsenrichedvi-
[63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
sualdescriptions. InTransactionsonMachineLearningRe-
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
search,2023. 2
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXivpreprint,2023. 2,5,8
[64] SahilVermaandJuliaRubin. Fairnessdefinitionsexplained.
In Proceedings of the international workshop on software
fairness,2018. 1
[65] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
KevinLin,ZheGan,ZichengLiu,CeLiu,andLijuanWang.
Git: A generative image-to-text transformer for vision and
language. InTransactionsonMachineLearningResearch,
2022. 13
[66] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. InICML,2022. 5,13
[67] ZeyuWang,KlintQinami,IoannisChristosKarakozis,Kyle
Genova,PremNair,KenjiHata,andOlgaRussakovsky. To-
wardsfairnessinvisualrecognition: Effectivestrategiesfor
biasmitigation. InCVPR,2020. 1
[68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma,brianichter,FeiXia,EdChi,QuocVLe,andDenny
Zhou. Chain-of-thoughtpromptingelicitsreasoninginlarge
languagemodels. InNeurIPS,2022. 2
[69] AllenRWilcox. Indicesofqualitativevariation. Technical
report,OakRidgeNationalLab.,Tenn.,1967. 4
[70] Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu.
Fairgan: Fairness-aware generative adversarial networks.
In 2018 IEEE International Conference on Big Data (Big
Data).IEEE,2018. 1,4
[71] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-
maier. Fromimagedescriptionstovisualdenotations: New
similaritymetricsforsemanticinferenceovereventdescrip-
tions. Transactions of the Association for Computational
Linguistics,2014. 4,5,12,13,14,16
[72] Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu,
DmitryLagun,ThaboBeeler,andFernandoDelaTorre. Iti-
gen: Inclusivetext-to-imagegeneration. InICCV,2023. 1,
2
11OpenBias: Open-set Bias Detection in Text-to-Image Generative Models
Supplementary Material
In this supplementary material we report further im-
SYSTEM PROMPT
plementation details and analyses. Specifically, in Sec- Upon receiving a text prompt that will be used
tion Appendix A, we describe the implementation details as input to a generative model (such as stable
diffusion), your task is to output a list of
for prompting the Large Language Model (LLM) in our
possible biases that may occur during the
bias proposal module. Additional implementation details
generation.
and evaluation of the Vision Question Answering (VQA) − provide a set of specific biases.
module are provided in Appendix B. In Appendix D, we − provide a set of multiple classes for each bias.
− provide one question for each bias that will
discusstheOpenBiasextensionfordetectingbiasesincap-
help to identify the bias in a set of images.
tionlesssettings. Finally, weprovideadditionalqualitative
For example, if the bias is age, the question
results and generative model comparisons in Appendix E may be ”How old is the person in the picture?”.
andinthesupplementarywebsite. − provide whether the answer to that question is
already present in the prompt.
A.LLMprompting The answer must be in JSON format only.
EXAMPLE
AsdescribedinSec. 3.1,givenacaption,wetasktheLLM Prompt: ”A picture of a doctor”
Bias1:
to output (i) possible biases, (ii) the corresponding set of
− name: Person gender
classes, and (iii) the relative questions. For doing so, we − classes: [’Male’, ’Female’]
leverage in-context learning, providing the model with a − question: What is the gender of the doctor?
− present in prompt: false
system prompt and a series of task examples [9, 58]. The
Bias2:
system prompt we use is shown in Fig. 10 alongside one
− name: Person age
example. Thesystempromptisusedtoinstructthemodel − classes: [’Young’, ’Middle−Aged’, ’Old]
withthetaskwhiletheexamplesprovidecontextandabet- − question: What is the age of the doctor?
− present in prompt: false
terspecificationofthetaskitself.
Whenproposingexamples,itiscrucialtoavoidbiasing
Figure10.InformationprovidedtoLLama.
theLLM.Thisriskmayarisewhenalwaysspecificclasses
are provided as examples, potentially causing the LLM to agesand,afterward,toassessthebiases.
consistentlyproducethesamesetofclassesforthatbiasin
future responses. To avoid this behavior, we first task the B.FullVQAevaluationanddetails
LLM to generate bias-related information using a limited
setofcaptions.Subsequently,weusethemodel’sgenerated Evaluation. As described in Sec. 4.2 of the main paper,
outputdirectlyasexamples,withoutintroducingnewdata. weevaluateseveralstate-of-the-artVQAmodelsonimages
This process ensures that no human bias is injected while generatedbyStableDiffusionXL[50]usingcaptionsfrom
providing examples, with the model encountering only in- COCO [40] and Flickr30k [71]. This evaluation compares
formationithaspreviouslygenerated. theVQAmodelswithFairFace[30],amodeltrainedforfair
BiasProposalpost-processing. Thebiasproposalmodule predictions.ThefullevaluationresultsarereportedinTab.3
produces a set of bias-related information given one cap- and 4 where Llava1.5-13B [41, 42] is the best-performing
tion. Sincethisprocessisappliedtoalargesetofcaptions model,andweadoptitasourdefaultVQAmodel.
and each caption is processed independently (i.e., the lan- It is important to note that the effectiveness of bias de-
guage model does not possess any knowledge of the prior tectionmethodsreliesonthegenerativemodel’scapabilities
captions and responses), the output might contain noise. suchasgenerationqualityandtextualcomprehension.Ifthe
Forthisreason, afteraggregatinginformationasdescribed generativemodelfailswithspecifictextualprompts, itcan
in Sec. 3.1, we apply a two-stage post-processing opera- compromisebiasidentification’saccuracyandreliability.
tion. We first merge biases that share a high percentage Additionalimplementationdetail. WhiletheVQAmodel
of classes. Subsequently, we retain the most supported bi- processes the images, as outlined in Sec. 3.2, we add one
ases, considering the number of captions associated with class denoting an unknown option allowing the model to
eachbias. Weempiricallyobservethatsettingthepercent- flaguncertaintyonthespecificbiasclass. Thismayoccur,
ageofequalclassesto75%andtheminimumsupportto30 e.g.ifthegeneratorfailstofollowthetextualpromptduring
captionsprovidesarobustpost-processingoperationavoid- generationaccurately. Thisoptionisremovedfromoursta-
ing the removal of valuable information. After this stage, tisticalanalyseswhilequantifyingthebiasesasitdoesnot
theknowledgebaseofbiasescanbeappliedtogenerateim- representvaluablebias-relatedinformation.
12Gender Age Race Gender Age Race
Model Model
Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1
PromptCap[26] 90.24 79.54 42.14 31.61 52.36 35.64 PromptCap[26] 89.21 71.13 46.46 32.82 50.72 35.19
CLIP-L[51] 91.43 75.46 58.96 45.77 36.02 33.60 CLIP-L[51] 91.61 70.80 65.66 52.11 37.05 36.97
Open-CLIP[12] 78.88 67.63 20.89 20.80 37.20 33.37 Open-CLIP[12] 79.86 63.95 31.31 30.48 43.88 40.35
OFA-Large[66] 93.03 83.07 53.79 41.72 24.61 21.22 OFA-Large[66] 91.37 73.31 61.11 40.56 28.06 24.39
VILT[34] 85.26 73.03 42.70 20.00 44.49 29.01 VILT[34] 82.25 64.48 45.71 23.84 45.68 28.32
mPLUG-Large [37] 93.03 82.81 61.37 52.74 21.46 23.26 mPLUG-Large [37] 91.85 73.49 71.72 58.89 25.90 25.82
BLIP-Large[38] 92.23 82.18 48.61 31.29 36.22 35.52 BLIP-Large[38] 91.61 73.73 47.73 30.72 34.89 31.31
GIT-Large[65] 92.03 81.60 44.55 24.47 43.70 34.21 GIT-Large[65] 91.37 73.31 42.93 22.62 47.84 40.71
BLIP2-FlanT5-XXL[39] 90.64 80.14 62.85 61.46 37.80 37.91 BLIP2-FlanT5-XXL[39] 89.93 71.60 70.71 59.82 35.97 37.55
Llava1.5-7B[41,42] 92.03 82.33 66.54 62.16 55.71 42.80 Llava1.5-7B[41,42] 89.93 72.20 71.46 57.48 57.91 45.00
Llava1.5-13B[41,42] 92.83 83.21 72.27 70.00 55.91 44.33 Llava1.5-13B[41,42] 90.89 73.13 74.75 65.52 58.27 48.05
Table3. VQAevaluationonStableDiffusionXL[50]gener- Table 4. VQA evaluation on Stable Diffusion XL [50] gen-
atedimagesusingCOCO[40]captions.Wehighlightin gray eratedimagesusingFlickr30k[71]captions. Wehighlightin
thechosendefaultVQAmodel. gray thechosendefaultVQAmodel.
C.AdditionalOpenBiasevaluation
Profession OpenBias [20] Diff
WinoBias evaluation. We aim to evaluate the capabil- Attendant 0.30 0.13 0.17
Cashier 0.70 0.67 0.03
ities of OpenBias in discovering well-known biases in a
Teacher 0.85 0.42 0.43
job-related domain. Towards this end, we use 36 profes-
Nurse 0.94 0.99 0.05
sions from WinoBias [75] to build a dataset of job-related
Assistant 0.18 0.19 0.01
prompts with the following template: ”A person working
Secretary 0.99 0.88 0.11
as <profession>.”, ”A person who is a <profession>.”,
Cleaner 0.13 0.38 0.25
”A <profession>.” and ”A human working as <profes- Receptionist 0.90 0.99 0.09
sion>.”. Next, we run OpenBias to propose and quantify Clerk 0.43 0.10 0.33
biases where it detects both gender and race. Afterward, Counselor 0.70 0.06 0.64
wequantifytheagreementwithexistingworkbycomparing Designer 0.30 0.23 0.07
OpenBiaswithTableD.1of[20]ongender.Following[20], Hairdresser 0.92 0.74 0.18
we compute the metric ∆ = |pdesired−pactual| which de- Writer 0.10 0.15 0.05
pdesired Housekeeper 0.93 0.93 0.00
scribes the deviation of the measured distribution p
actual Baker 0.42 0.81 0.39
with a desired distribution p (i.e., uniform distribu-
desired Librarian 0.79 0.86 0.07
tion). The results of this evaluation are shown in Tab. 5
Tailor 0.10 0.30 0.20
where we observe a high alignment of OpenBias on all
Driver 0.62 0.97 0.35
professions with an average discrepancy of 0.20 ± 0.04 Supervisor 0.74 0.50 0.24
and highest alignment in housekeeper, assistant, worker, Janitor 0.82 0.91 0.09
sheriff, laborer, cashier, nurse, writer and developer, with Cook 0.00 0.82 0.82
a discrepancy of only 0.00, 0.01, 0.01, 0.01, 0.02, 0.03, Laborer 0.97 0.99 0.02
0.05,0.05,and0.05. Overall,thisevaluationfurtherproves Worker 0.99 1.00 0.01
OpenBias’abilitytodetectandquantifywell-knownbiases. Developer 0.85 0.90 0.05
Carpenter 0.99 0.92 0.07
Self-identification evaluation. We further evaluate Open-
Manager 0.37 0.54 0.17
Bias by considering a self-identification setting, offering a
Lawyer 0.54 0.46 0.08
deeperunderstandingofitsbehaviorwithinamoreethical
Farmer 0.77 0.97 0.20
context. Inthisscenario,individualsself-identifytheirgen-
Salesperson 0.43 0.60 0.17
derandraceattributes,removingtheneedforexternalanno- Physician 0.07 0.62 0.55
tations from classifiers or human sources and, thus, avoid- Guard 0.94 0.86 0.08
ing assumptions about socially sensitive attributes. The Analyst 0.45 0.58 0.13
evaluation consists of comparing our chosen VQA model Mechanic 0.92 0.99 0.07
(i.e., Llava1.5-13B) with a self-identification aware clas- Sheriff 0.98 0.99 0.01
sifier. This classifier is built by encoding images of self- CEO 0.39 0.87 0.48
Doctor 0.23 0.78 0.55
identified individuals with a vision encoder, effectively
building clusters of image embeddings belonging to the Average 0.20±0.04
same self-identified class. Next, each self-identified class
is represented by its cluster prototype (i.e., the centroid of Table5.ComparingOpenBiaswith[20]ongender.
13the cluster). Finally, we classify a given generated image unconditionalmodelStyleGAN3[32]. Wecomparethebi-
to the class of the nearest prototype. In this experiment, ases from FFHQ and StyleGAN3 in Fig. 14. Similarly to
weuseCLIP-ViT-L[51]asvisionencoderandemploythe thecaseobservedinCOCOandFlickr30k,OpenBiasiden-
Chicago Face Dataset (CFD) [36, 43, 44], which consists tifiesdifferentbiases,predominantlyprunetothefacialdo-
of high-resolution images of 827 unique male and female main(e.g., “nosepiercing”, “personhaircolor”, “person
individualsofdiverseethnicalgroupsandage. Notably,the beard”, “person hair style”). This is directly attributed to
key feature of this dataset is the self-identification of each theuseofFFHQ,afacialdomaindataset. Furthermore,this
individualforthesociallysensitiveattributesofgenderand comparisonprovidestheopportunitytostudythebiasam-
ethnicity,allowingustobuildtheaboveevaluationpipeline. plificationissuebycomparingthedetectedbiasesofStyle-
We observe a high alignment between the two methods GAN3withthoseinherentinitstrainingsetFFHQ.Wemay
with97.87%accuracyand97.92%F1-scoreongenderand observe how the unconditional generative model tends to
77.60% accuracy and 72.98% F1-score on race. We note amplify specific biases (e.g., “person race”, “nose pierc-
that the misalignment in race is partially due to the pres- ing”, “personsmiling”, “personhairlength”), abehavior
ence of the multi-racial class which describes individuals that aligns with existing works [3, 17, 45]. Nevertheless,
with ancestors of diverse ethnicity, making this class hard italsoexhibitscorrelationswithitstrainingsetinotherbi-
toclassify. Nevertheless,thehighalignmentobservedcon- ases(e.g.,“personhaircolor”,“personemotion”,“person
firms the capabilities of OpenBias also for self-identified gender”,“personhairstyle”).
attributes,providingfurtherinsightsintoitsbehavior.
E.Additionalqualitativeresults
D. OpenBias for Real Datasets and Uncondi-
We show additional qualitative results from Fig. 15
tionalGenerators
to Fig. 26. These figures illustrate multiple biases of the
AswedescribedinSec. 3.2.3OpenBias,withsimplemod- threestudiedStableDiffusionmodels[50,53]. Foraneasy
ifications, can be applicable to real datasets with captions, comparison, we show, for each bias, images generated us-
image-only datasets, and unconditional generative models. ingthesamerandomlysampledcaption. Weshowqualita-
In the following, we describe how OpenBias is applicable tive results of multiple biases, ranging from those already
tothesesettingsandshowaccompanyingresults. Notethat outlined in Sec. 5 of the main paper (e.g., “person race”,
inthesecaseswecannotinvestigatethecontext-awarefor- “childrace”,“traincolor”)tonovelones(e.g.,“bedtype”,
mulationsincewepossessasingle-captionperimage. “caketype”,“wavesize”).Notably,themagnitudeofthese
Application to real datasets. In the case of datasets with biases varies across models suggesting that, as expected,
captions,theprocedureremainslargelyunchanged,withthe the models behave differently given the same context/cap-
exceptionthattheassessmentandquantificationmoduleis tion. This behavior is noticeable on the “child race” bias
applied directly to the real images. We test OpenBias on in Fig. 19 where Stable Diffusion 2 and 1.5 consistently
COCO [40] and Flickr30k [71] as real datasets with cap- generate children of lighter skin tones or in Fig. 20 (i.e.,
tions. The results of this experiment are shown in Fig. 12 “personattire”)wheresubjectswearmorecasualattireon
and Fig. 13. In this scenario, we may see how the differ- theimagesgeneratedbyStableDiffusion2and1.5. Thus,
ent nature of the two datasets leads to the identification of overall, these two generative models consistently exhibit
differentbiases, highlightingtheabilitytoextractdomain- lower bias magnitudes compared to the XL version, align-
specific biases from OpenBias. For example, in Flickr30k ingwiththerankingresultspresentedinSec. 5. Neverthe-
OpenBias identifies worker and artist related biases not less, all three models exhibit the identified biases demon-
presentinCOCO.Finally,weobservelow-intensitybiases strating the robustness of the pipeline. This can be seen
(e.g., “beachlocation”and“buildingtype”inCOCOand in Fig. 18 (i.e., “child gender”) where the models gener-
babygenderandpersongenderinFlickr30k). ate more males or in Fig. 24 (i.e., “bed type”) where the
Application to image-only datasets and unconditional majoritygeneratedbedsareofthedoubletype.
generative models. In scenarios where captions are un- We include a supplementary website where we provide
available,suchasinimage-onlydatasetsandunconditional additionalanddiversecontexts(i.e.,captions)foreachbias.
generative models, the pipeline can be readily applied by
integratingacaptioner. Thiscaptionereffectivelygenerates F.Userstudy
the required set of captions for the bias proposal module.
Fig. 11 provides a screenshot of the conducted user study
After leveraging the generated captions to propose biases,
described in Sec. 4.2 of the main paper. The user study
weapplytherestofthepipelinetotherealorgeneratedim-
provides,foreachbias,thegeneratedimagesatthecontext
ages. In our experiments, we employ Llava1.5-13B as the
level (i.e., generated with the same caption). The user has
captioner, the same model we use for VQA. We test this
tochoosethemajorityclassandthemagnitudeofeachbias.
approachontheimage-onlydatasetFFHQ[31]andonthe
14Figure11.UserstudyscreenshotconductedtoassessthecapabilitiesofOpenBias.
1.0
Context Aware
0.8
0.6
0.4
0.2
beac0 h. l b0 o uic la dt i bi n ao g bn t y y ap oce t bi jv e bi ct aty bt yy cp g hie e ln d d tvg re er e ah fin ficd l ce e lr it gy hp t ce c ool w or b d pr ri ee rne skd ot ny p ge e kn i ktd itee cr c ho el no r s tt ry el ee pt h ey rai sp r oe c n ol ao mc tr t r oi tav ii ot rn y cc yo c pl l eo e rr t sy op n e fh oa on hdd ot r fy s rp ie se c b po iel zeo zr c a l o tl aoo pr tp opi pn hg kb o ir tra s cn e h d ec no l sso e nr pt i oti zw wzn e bag a ot t aoh re p pdr p eei rr n sgg oes n f n ued re nmr io tt ui ro sen k it iy np rg i e ds it nyl g p s e es ort csy cl o ee n r a p hg o oe s srit s ki iieo n wn b gr a l te o ee rc d a cti o bo inn cdi yti clo en ft ry uip dte e t viy c hp e oe rt ps ly e ap ye b er re e g dd e on nd ue tr ct ay k tp ie pe et m ry e s p ooe f n d pa toy s eit ni s no iki sn bt coy oap t u se rt t n y p osp e wu re r s bf o oa anc re a ditt nir g e s kt iy tl ee t w ey a lp v ee e p hsi az ne t p ia z gg z ae a s m ks ii iez ne g g e pl s en o k rr ic sie a onti ng
s
o kean aqb til ui i et p by ocm aae rr n dit om n ca g e sk aa ke n ab i tlli eot c by a ot ai ro dn wt ay pvp eee rtt r sy a oip nn e s st h u ky rai f a sp r tb ne ec o opo al wr aro bdr k ot f ay tre tp eda eie t d nnu dr ng iy e sas bb ci e sl oi a nt ur r oy tc wlol boo ocr aa crti hd i to le on r d ota e hg m be rot ui so hn t wiy np e pe lt a sy y kp e ae r tp i eta c bg ohe tat r ey d nap ie ne rr i c sra ag pf le t a ysi ez r e rl c oe hi av l de d l r c loa uc n ge d git ai go en t by e pp d ee rt sy op ne s f lir oea l pc d ee t dy ifp f bie oc oul kt y ge ek ln i sr t e kse e p anh ts oi a ez n w b e t o cs aoi rz n de d ei rt io gen nder
Figure12.RankingofthediscoveredbiasesontherealdatasetCOCO[40].
15
ytisnetnI
saiB1.0
Context Aware
0.8
0.6
0.4
0.2
b0 a. b0 y p eg re sn o sd n oe cr g ce en r d per osit ai pro eti rn s st oa n g ge e pst ec ru o sr a oe t n c fo al pco eir ra sl oh n ai ar ctivit wy e bia ct yh ce lr te r t ey e p se p pe ec rbi foe oa rs t mt ey r pe g ve en hid ce lr e t fy op oe d ty hp aie r c hc ilol d or gende ar c pti erv t fi rt oe ry e mt ey r p be g ue iln dd pie enr rg s t oy n p re eli ig ni so tsn re utt mi en ng t ty cp are flm oa wk ee r pt ey rp se on h o pa r eg s re e s oc no l eor mot fi ro ui an t r tit sy t pe g pe en rsd oer an r tp wo pos r ee rk f t ory pep m re ser o sna k ag p t be o e asi b st oi ka eo r tn d b at lly p p ie o ns bi st toi roo ukn mg ee rnn etr ste pl aa ury ae nd t pt ey f rip sse oh nt y hp a cie r h ilc do sl e nor om wo ti co on nd git ri ao sn ps ec rsol oo nr da ott gi r pe ostur se e wa os ro kn er ra dc oe g h oc ro sa et bre pe od ol p t ery ssp uoe rfn br oa ac pre ed r t sy op n e h de oi mgg oh tat oc rti cv yit cly e t cy php i el re fd or ra mc ee r hr aa ic r e len dgt wo oh g rs k ea r ge ge pn ld ae yr er a wg a ple v ae y s ei r z rie g ve en r d le or c ha ot pri s eo ren sobr n e sed m vo eki sn sg el wty o brp eke e ar c ha lg oe ca ct ri soo cn w ed n s ei lz oe cation
Figure13.RankingofthediscoveredbiasesontherealdatasetFlickr30K[71].
1.0 C Co on nt te ex xt t A Frw ea ere
0.8
0.6
0.4
0.2
0 p. e0 rson smiling person pp eo rs se on occupation hair curliness nose pierc pi en rg son occlu psi eo rsn on mustach se cene locati po en rson emotion person pr ea rc se on hair l pe en rg st oh n accessori pe es rson makeu pp erson activity person pa et rt sir oe n appeara pn ec re son eye col po er rson occasi bo an by expres psi eo rsn on facial ph eai rr son hair color person beard person ag pe erson lip co cl ho ir ld expre ss csi eo nn e backgro pu en rd son hair styl pe erson gender object color child gende pr erson glasses baby gender
Figure14.ComparisonofthediscoveredbiasesongeneratedimagesfromStyleGAN3[32]andrealimagesfromFFHQ[31].
Persongender
SD-XL SD-2 SD-1.5
Figure15.Comparisononimagesgeneratedwiththesamecaption“Atrafficofficerleaningonanoturnsign”.
16
ytisnetnI
saiB
ytisnetnI
saiBPersonrace
SD-XL SD-2 SD-1.5
Figure16.“Amanridinganelephantintosomewaterofacreek”.
Personage
SD-XL SD-2 SD-1.5
Figure17.“Awomanridingahorseinfrontofacarnexttoafence”.
Childgender
SD-XL SD-2 SD-1.5
Figure18.“Toddlerinabaseballcaponawoodenbench”.
17Childrace
SD-XL SD-2 SD-1.5
Figure19.“Smallchildhurryingtowardabusonadirtroad”.
Personattire
SD-XL SD-2 SD-1.5
Figure20.“Theladyissittingonthebenchholdingherhandbag”.
Traincolor
SD-XL SD-2 SD-1.5
Figure21.“Atrainzipsdowntherailwayinthesun”.
18Laptopbrand
SD-XL SD-2 SD-1.5
Figure22.“Aphotoofapersononalaptopinacoffeeshop”.
Horsebreed
SD-XL SD-2 SD-1.5
Figure23.“Awomanridingahorseinfrontofacarnexttoafence”.
Bedtype
SD-XL SD-2 SD-1.5
Figure24.“Apersonstandinginabedroomwithabedandatable”.
19Caketype
SD-XL SD-2 SD-1.5
Figure25.“Aclose-upofapersoncuttingapieceofcake”.
Wavesize
SD-XL SD-2 SD-1.5
Figure26.“Amanridesawaveonasurfboard”.
20