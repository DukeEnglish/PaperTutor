ControlNet++: Improving Conditional Controls
with Efficient Consistency Feedback
Project Page: liming-ai.github.io/ControlNet_Plus_Plus
Ming Li1(cid:66), Taojiannan Yang1, Huafeng Kuang2, Jie Wu2,
Zhaoning Wang1, Xuefeng Xiao2, and Chen Chen1
(cid:66): mingli@ucf.edu
1 Center for Research in Computer Vision, University of Central Florida
2 ByteDance Inc
Abstract. Toenhancethecontrollabilityoftext-to-imagediffusionmod-
els,existingeffortslikeControlNetincorporatedimage-basedconditional
controls. In this paper, we reveal that existing methods still face signif-
icant challenges in generating images that align with the image condi-
tionalcontrols.Tothisend,weproposeControlNet++,anovelapproach
thatimprovescontrollablegenerationbyexplicitlyoptimizingpixel-level
cycle consistency between generated images and conditional controls.
Specifically, for an input conditional control, we use a pre-trained dis-
criminative reward model to extract the corresponding condition of the
generated images, and then optimize the consistency loss between the
inputconditionalcontrolandextractedcondition.Astraightforwardim-
plementation would be generating images from random noises and then
calculating the consistency loss, but such an approach requires storing
gradients for multiple sampling timesteps, leading to considerable time
and memory costs. To address this, we introduce an efficient reward
strategy that deliberately disturbs the input images by adding noise,
and then uses the single-step denoised images for reward fine-tuning.
This avoids the extensive costs associated with image sampling, allow-
ing for more efficient reward fine-tuning. Extensive experiments show
that ControlNet++ significantly improves controllability under various
conditional controls. For example, it achieves improvements over Con-
trolNet by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for
segmentation mask, line-art edge, and depth conditions.
Keywords: Controllable Generation ¬∑ Diffusion Model ¬∑ ControlNet
1 Introduction
The emergence and improvements of diffusion models [12,43,50], along with the
introduction of large-scale image-text datasets [48,49], has catalyzed significant
strides in text-to-image generation. Nonetheless, as the proverb ‚Äúan image is
worthathousandwords‚Äù conveys,it‚Äôschallengingtodepictanimageaccurately
and in detail through language alone, and this dilemma also perplexes existing
text-to-imagediffusionmodels[43,46].Tothisend,manystudiesfocusonincor-
porating conditional controls such as segmentation mask, and depth map into
4202
rpA
11
]VC.sc[
1v78970.4042:viXra2 Ming et al.
1 2 3
LessDetails More Details
1 2 3
Line-ArtImageCondition
Text Prompt: Ryan Reynolds
(a) Inputs Conditions (b)Ours(SSIM:0.8714)
1 2 3 1 2 3
(c) T2I-Adapter-SDXL (SSIM: 0.6840) (d) ControlNet v1.1 (SSIM: 0.7377)
Fig.1: (a)Giventhesameinputimageconditionandtextprompt,(b)theextracted
conditions of our generated images are more consistent with the inputs, (c,d) while
other methods fail to achieve accurate controllable generation. SSIM scores measure
thesimilaritybetweenallinputedgeconditionsandtheextractededgeconditions.All
thelineedgesareextractedbythesamelinedetectionmodelusedbyControlNet[63].
text-to-image diffusion models [22,30,37,62,63] to facilitate controllable gen-
eration. Despite the diversity in these methods, the core objective remains to
facilitate more accurate and controllable image generation with explicit image-
based conditional controls.
Achieving controllable image generation could involve retraining diffusion
models from scratch [37,43], but this comes with high computational demands
and a scarcity of large public datasets [63]. In light of this, a feasible strategy
is fine-tuning pre-trained text-to-image models [23,61] or introducing trainable
modules [30,62,63] like ControlNet [63]. However, despite these studies have
explored the feasibility of controllability [30,62,63] in text-to-image diffusion
models and expanded various applications [22,23,37], a significant gap remains
in achieving precise and fine-grained control. As shown in Fig. 1, existing state-
of-the-art methods of controllable generation (e.g., ControlNet [63] and T2I-
Adapter [30]) still struggle to accurately generate images that are consistent
with the input image condition. For example, T2I-Adapter-SDXL consistently
produced incorrect wrinkles on the forehead in all generated images, while Con-
trolNet v1.1 introduced many wrong details. Regrettably, current efforts lack
specific methods for improving controllability, which impedes progress in this
research area.
To address this issue, we model image-based controllable generation as an
image translation task [17] from input conditional controls to output generated
images. Inspired by CycleGAN [71], we propose to employ pre-trained discrim-ControlNet++ 3
Condition Generated Image Output
Controllable Pre-trained
Diffusions RewardModels
Prompt: heart, mountains, Hed Model
and nature image Depth Model
Canny Model
Segmentation Model
Cycle Consistency ¬∑¬∑¬∑¬∑¬∑¬∑
Fig.2: Illustration of the cycle consistency.Wefirstpromptthediffusionmodel
G to generate an image x‚Ä≤ based on the given image condition c and text prompt c ,
0 v t
then extract the corresponding image condition cÀÜ from the generated image x‚Ä≤ using
v 0
pre-trained discriminative models D. The cycle consistency is defined as the similarity
between the extracted condition cÀÜ and input condition c .
v v
inative models to extract the condition from the generated images and directly
optimize the cycle consistency loss for better controllability. The idea is that if
we translate images from one domain to the other (condition c ‚Üí generated
v
imagex‚Ä≤),andbackagain(generatedimagex‚Ä≤ ‚Üíconditionc‚Ä≤)weshouldarrive
0 0 v
wherewestarted(c‚Ä≤ =c ),asshowninFig.2.Forexample,givenasegmentation
v v
mask as a conditional control, we can employ existing methods such as Control-
Net [63] to generate corresponding images. Then the predicted segmentation
masks of these generated images can be obtained by a pre-trained segmentation
model. Ideally, the predicted segmentation masks and the input segmentation
masks should be consistent. Hence, the cycle consistency loss can be formulated
astheper-pixelclassificationlossbetweentheinputandpredictedsegmentation
masks. Unlike existing related works [27,30,37,63,65] that implicitly achieve
controllability by introducing conditional controls into the latent-space denois-
ing process, our method explicitly optimizes controllability at the pixel-space
for better performance, as demonstrated in Fig. 3.
Toimplementpixel-levellosswithinthecontextofdiffusionmodels,anintu-
itiveapproachinvolvesexecutingthediffusionmodel‚Äôsinferenceprocess,starting
from random Gaussian noise and performing multiple sampling steps to obtain
the final generated images, following recent works focusing on improving image
quality with human feedback [11,36,60]. However, multiple samplings can lead
to efficiency issues, and require the storage of gradients at every timestep and
thussignificanttimeandGPUmemoryconsumption.Wedemonstratethatiniti-
ating sampling from random Gaussian noise is unnecessary. Instead, by directly
adding noise to training images to disturb their consistency with input condi-
tional controls and then using single-step denoised images to reconstruct the
consistency,wecanconductmoreefficientrewardfine-tuning.Ourcontributions
are summarized as:
‚Äì New Insight: We reveal that existing efforts in controllable generation still
performpoorlyintermsofcontrollability,withgeneratedimagessignificantly
deviatingfrominputconditionsandlackingaclearstrategyforimprovement.4 Ming et al.
üî• üî•
Encoder Diffusion Encoder Diffusion Decoder
: A large building Latent-space Pixel-spaceCycle
with a pointed roof Denoising Loss ConsistencyLoss
and several chimneys.
(a)ExistingMethods (b) Our Solution
Fig.3: (a) Existing methods achieve implicit controllability by introducing image-
based conditional control c into the denoising process of diffusion models, with the
v
guidanceoflatent-spacedenoisingloss.(b)WeutilizediscriminativerewardmodelsD
to explicitly optimize the controllability of G via pixel-level cycle consistency loss.
‚Äì Consistency Feedback: We show that pre-trained discriminative models can
serve as powerful visual reward models to improve the controllability of con-
trollable diffusion models in a cycle-consistency manner.
‚Äì Efficient Reward Fine-tuning: We disrupt the consistency between input im-
ages and conditions, and enable the single-step denoising for efficient reward
fine-tuning, avoiding time and memory overheads caused by image sampling.
‚Äì Evaluation and Promising Results:Weprovideaunifiedandpublicevaluation
of controllability under various conditional controls, and demonstrate that
ControlNet++ comprehensively outperforms existing methods.
2 Related Work
2.1 Diffusion-based Generative Models
Thediffusionprobabilisticmodelpresentedin[50]hasundergonesubstantialad-
vancements [12,19,25], thanks to iterative refinements in training and sampling
strategies [18,51,52]. To alleviate the computational demands for training dif-
fusion models, Latent Diffusion [43] maps the pixel space diffusion process into
the latent feature space. In the realm of text-to-image synthesis, diffusion mod-
els [31,35,40,41,43,46] integrate cross-attention mechanisms between UNet [44]
denoisers and text embeddings from pre-trained language models like CLIP [38]
andT5[39]tofacilitatereasonabletext-to-imagegeneration.Furthermore,diffu-
sion models are employed across image editing tasks [3,14,24,29] by manipulat-
inginputs[40],editingcross-attentions[16],andfine-tuningmodels[45].Despite
the astonishing capabilities of diffusion models, language is a sparse and highly
semantic representation, unsuitable for describing dense, low-semantic images.
Furthermore, existing methods [35,43] still struggle to understand detailed text
prompts, posing a severe challenge to the controllable generation [63].ControlNet++ 5
2.2 Controllable Text-to-Image Diffusion Models
To achieve conditional control in pre-trained text-to-image diffusion models,
ControlNet[63]andT2I-Adapter[30]introduceadditionaltrainablemodulesfor
guided image generation. Furthermore, recent research employs various prompt
engineering [27,61,64] and cross-attention constraints [6,23,58] for a more reg-
ulated generation. Some methods also explore multi-condition or multi-modal
generation within a single diffusion model [21,37,65] or focus on the instance-
based controllable generation [54,69]. However, despite these methods exploring
feasibilityandapplications,therestilllacksaclearapproachtoenhancecontrol-
lability under various conditional controls. Furthermore, existing works implic-
itly learn controllability by the denoising process of diffusion models, while our
ControlNet++ achieves this in a cycle-consistency manner, as shown in Fig. 3.
2.3 Linguistic and Visual Reward Models
The reward model is trained to evaluate how well the results of generative mod-
els align with human expectations, and its quantified results will be used to
facilitate generative models for better and more controllable generation. It is
usually trained with reinforcement learning from human feedback (RLHF) in
NLP tasks [10,32,53], and has recently extended into the vision domain to im-
prove the image quality for text-to-image diffusion models [1,11,13,36,56,60].
However, image quality is an exceedingly subjective metric, fraught with indi-
vidualpreferences,andrequiresthecreationofnewdatasetswithhumanprefer-
ences[26,55,56,60]andthetrainingofrewardmodels[36,55,60].Divergingfrom
the pursuit of global image quality with subjective human preference in current
research, we target the more fine-grained and objective goal of controllability.
Also,it‚Äôsmorecost-effectivetoobtainAIfeedbackcomparedtohumanfeedback.
3 Method
Inthissection,wefirstintroducethebackgroundofdiffusionmodelsinSec.3.1.
In Sec. 3.2, we discuss how to design the cycle consistency loss for controllable
diffusion models to enhance the controllability. Finally, in Sec. 3.3, we examine
the efficiency issues with the straightforward solution and correspondingly pro-
poseanefficientrewardstrategythatutilizesthesingle-stepdenoisedimagesfor
consistency loss, instead of sampling images from random noise.
3.1 Preliminary
The diffusion models [18] define a Markovian chain of diffusion forward process
q(x |x ) by gradually adding noise to input data x :
t 0 0
‚àö ‚àö
x = Œ±¬Ø x + 1‚àíŒ±¬Ø œµ, œµ‚àºN(0,I), (1)
t t 0 t
whereœµisanoisemapsampledfromaGaussiandistribution,andŒ±¬Ø
:=(cid:81)t
Œ± .
t s=0 s
Œ± =1‚àíŒ≤ is a differentiable function of timestep t, which is determined by the
t t6 Ming et al.
denoising sampler such as DDPM [18]. To this end, the diffusion training loss
can be represented by:
L(œµ Œ∏)=(cid:88)T E x0‚àºq(x0),œµ‚àºN(0,I)(cid:104)(cid:13) (cid:13)œµ Œ∏(cid:0)‚àö Œ±¬Ø tx 0+‚àö 1‚àíŒ±¬Ø tœµ(cid:1) ‚àíœµ(cid:13) (cid:13)2 2(cid:105) . (2)
t=1
In the context of controllable generation [30,63], with given image condition c
v
and text prompt c , the diffusion training loss at timestep t can be re-written
t
as:
L =E (cid:2) ‚à•œµ (x ,t,c ,c )‚àíœµ‚à•2(cid:3) . (3)
train x0,t,ct,cv,œµ‚àºN(0,1) Œ∏ t t v 2
During the inference, given a random noise x ‚àº N(0,I), we can predict
T
final denoised image x with the step-by-step denoising process [18]:
0
(cid:18) (cid:19)
1 1‚àíŒ±
x = ‚àö x ‚àí ‚àö t œµ (x ,t) +œÉ œµ, (4)
t‚àí1 Œ± t 1‚àíŒ±¬Ø Œ∏ t t
t t
whereœµ referstothepredictednoiseattimesteptbyU-Net[44]withparameters
Œ∏
Œ∏, and œÉ = 1‚àíŒ±¬Øt‚àí1Œ≤ is the variance of posterior Gaussian distribution p (x )
t 1‚àíŒ±¬Øt t Œ∏ 0
3.2 Reward Controllability with Consistency Feedback
Aswemodelcontrollabilityastheconsistencybetweeninputconditionsandthe
generated images, we can naturally quantify this outcome through the discrim-
inative reward models. Once we quantify the results of the generative model,
we can perform further optimization for more controllable generation based on
these quantified results in a unified manner for various conditional controls.
To be more specific, we minimize the consistency loss between the input
condition c and the corresponding output condition cÀÜ of the generated image
v v
x‚Ä≤, as depicted in Fig. 2. The reward consistency loss can be formulated as:
0
L =L(c ,cÀÜ )
reward v v
=L(c ,D(x‚Ä≤)) (5)
v 0
=L(cid:0)
c
,D(cid:2)GT
(c ,c ,x
,t)(cid:3)(cid:1)
,
v t v T
where GT (c ,c ,x ,t) denotes the process that the model performs T denois-
t v T
ing steps to generate the image x‚Ä≤ from random noise x , as shown in the
0 T
Fig. 4 (a). Here, L is an abstract metric function that can take on different con-
crete forms for different visual conditions. For example, in the context of using
segmentation mask as the input conditional control, L could be the per-pixel
cross-entropy loss. The reward model D is also dependent on the condition, and
we use the UperNet [57] for segmentation mask conditions. The details of loss
functions and reward models are summarized in the supplementary material.
Inadditiontotherewardloss,wealsoemploydiffusiontraininglossinEq.3
toensurethattheoriginalimagegenerationcapabilityisnotcompromisedsinceControlNet++ 7
‚Ä¶ ‚Ä¶
Add
Eq. (5) Noise Eq. (8)
‚Ä¶ ‚Ä¶
Eq. (4) Eq.(4) 50x Inference Eq. (1) Eq.(7) 1x Inference
Multi-step Sampling (e.g., 50 steps) Time & Memory DisturbConsistency Single-stepSampling Time & Memory
(a) Default Reward Strategy (b)EfficientRewardStrategy(Ours)
Fig.4:(a)Pipelineofdefaultrewardfine-tuningstrategy.Rewardfine-tuningrequires
sampling all the way to the full image. Such a method needs to keep all gradients for
each timestep and the memory required is unbearable by current GPUs. (b) Pipeline
ofourefficientrewardstrategy.Weaddasmallnoise(t‚â§t )todisturbtheconsis-
thre
tency between input images and conditions, then the single-step denoised image can
be directly used for efficient reward fine-tuning.
they have different optimization goals. Finally, the total loss is the combination
of L and L :
train reward
L =L +Œª¬∑L , (6)
total train reward
where Œª is a hyper-parameter to adjust the weight of the reward loss. Through
this approach, the consistency loss can guide the diffusion model on how to
sample at different timesteps to obtain images more consistent with the input
conditionalcontrols,thereby enhancingcontrollability.Nonetheless,directlyap-
plying such reward consistency still poses challenges in efficiency in real-world
settings.
3.3 Efficient Reward Fine-tuning
To achieve the pixel-space consistency loss L , it requires x , the final dif-
reward 0
fused image, to calculate the reward consistency from the reward models. As
modern diffusion models, such as Stable Diffusion [43], require multiple steps,
e.g.,50steps,torenderafullimage,directlyusingsuchasolutionisimpractical
in realistic settings: (1) multiple time-consuming samplings are required to de-
riveimagesfromrandomnoise.(2)toenablegradientbackpropagation,wehave
to store gradients at each timestep, meaning the GPU memory usage will in-
creaselinearlywiththenumberoftime-steps.TakingControlNetasanexample,
when the batch size is 1 with FP16 mixed precision, the GPU memory required
for a single denoising step and storing all training gradients is approximately
6.8GB. If we use the 50-step inference with the DDIM [51] scheduler, approxi-
mately 340GB of memory is needed to perform reward fine-tuning on a single
sample,whichisnearlyimpossibletoachievewithcurrenthardwarecapabilities.
Although GPU memory consumption can be reduced by employing techniques
such as Low-Rank Adaptation (LoRA) [11,20], gradient checkpointing [7,11], or
stop-gradient [60], the efficiency degradation caused by the number of sampling
steps required to generate images remains significant and cannot be overlooked.
Therefore, an efficient reward fine-tuning approach is necessary.8 Ming et al.
In contrast to diffusing from random noise x to obtain the final image
T
x , as illustrated in Fig. 4 (a), we instead propose an one-step efficient reward
0
strategy. Specifically, instead of randomly sampling from noise, we add noise to
the training images x , thereby explicitly disturbing the consistency between
0
the diffusion inputs x‚Ä≤ and their conditional controls c , by performing diffusion
t v
forward process q(x |x ) in Eq. 1. We demonstrate this process as the Disturb
t 0
ConsistencyinFig.4(b),whichisthesameprocedureasthestandarddiffusion
training process. When the added noise œµ is relatively small, we can predict the
originalimagex‚Ä≤ byperformingsingle-stepsampling3ondisturbedimagex‚Ä≤ [18]:
0 t
‚àö
x‚Ä≤ ‚àí 1‚àíŒ± œµ (x‚Ä≤,c ,c ,t‚àí1)
x ‚âàx‚Ä≤ = t t ‚àöŒ∏ t v t , (7)
0 0 Œ±
t
andthenwedirectlyutilizethedenoisedimagex‚Ä≤ toperformrewardfine-tuning:
0
L =L(c ,cÀÜ )=L(c ,D(x‚Ä≤))=L(c ,D[G(c ,c ,x‚Ä≤,t)]). (8)
reward v v v 0 v t v t
Essentially, the process of adding noise destroys the consistency between the
input image and its condition. Then the reward fine-tuning in Eq. 8 instructs
thediffusionmodeltogenerateimagesthatcanreconstructtheconsistency,thus
enhancing its ability to follow conditions during generation.
Please note that here we avoid the sampling process in Eq. 5. Finally, the
loss is the combination of diffusion training loss and the reward loss:
(cid:40)
L +Œª¬∑L , if t‚â§t
L = train reward thre, (9)
total
L , otherwise,
train
where t denotes the timestep threshold, which is a hyper-parameter used
thre
todeterminewhetheranoisedimagex shouldbeutilizedforrewardfine-tuning.
t
We note that a small noise œµ (i.e., a relatively small timestep t) can disturb the
consistencyandleadtoeffectiverewardfine-tuning.Whenthetimesteptislarge,
x isclosertotherandomnoisex ,andpredictingx‚Ä≤ directlyfromx resultsin
t T 0 t
severe image distortion. The advantage of our efficient rewarding is that x can
t
be employed both to train and reward the diffusion model without the need for
time and GPU memory costs caused by multiple sampling, thereby significantly
improving the efficiency during the reward fine-tuning stage.
Duringtherewardfine-tuningphases,wefreezethepre-traineddiscriminative
rewardmodelandtext-to-imagemodel,andonlyupdatetheControlNetmodule
following its original implementation, which ensures its generative capabilities
arenotcompromised.Inourexperiments,weobservethatusingonlythereward
losswillleadtoimagedistortion,aligningwiththeconclusionsdrawninprevious
studies [60].
3 We provide a more detailed proof in the supplementary material.ControlNet++ 9
4 Experiments
4.1 Experimental Setup
ConditionControlsandDatasets. Giventhatexistingtext-imagepaireddatasets
for generative models are unable to provide accurate conditional control data
pairs [48,49], such as image-segmentation pairs, we endeavor to select specific
datasets for different tasks that can offer more precise image-label data pairs.
More specifically, ADE20K [67,68] and COCOStuff [4] are used for the segmen-
tation mask condition following ControlNet [63]. For the canny edge map, hed
edge map, lineart map, and depth map condition, we utilize the MultiGen-20M
dataset [37], which is a subset of LAION-Aesthetics [48]. For the datasets with-
out text caption such as ADE20K, we utilize MiniGPT-4 [70] to generate the
image caption with the instruction ‚ÄúPlease briefly describe this image in one
sentence‚Äù. The image resolution for both training and inference is 512√ó512 for
all datasets and methods. More details are provided in Sec. 8.
Evaluation and Metrics. We train ControlNet++ on the training set of each
corresponding dataset and evaluate all methods on the validation dataset. All
the experiments are evaluated under 512√ó512 resolution for fair comparison.
For each condition, we evaluate the controllability by measuring the similarity
betweentheinputconditionsandtheextractedconditionsfromgeneratedimages
of diffusion models. For semantic segmentation and depth map controls, we use
mIoUandRMSEasevaluationmetricsrespectively,whichisacommonpractice
in related research fields. For the edge task, we use F1-Score for hard edges
(canny edge) because it can be regarded as a binary classification problem of 0
(non-edge) and 1 (edge) and has a serious long-tail distribution, following the
standard evaluation in edge detection [59]. The threshold used for evaluation is
(100, 200) for OpenCV, and (0.1, 0.2) for Kornia implementation. The SSIM
metric is used for the soft edges conditional controls (i.e., hed edge & lineart
edge) following previous works [65].
Baselines. Our evaluation primarily focuses on T2I-Adapter [30], ControlNet
v1.1[63],GLIGEN[27],Uni-ControlNet[65],andUniControl[37],asthesemeth-
ods are pioneering in the realm of controllable text-to-image diffusion models
and offer public model weights for various image conditions. To ensure fairness
of evaluation, all methods use the same image conditions and text prompts.
For ControlNet++, we use the UniPC [66] sampler with 20 denoising steps to
generate images with the original text prompt following ControlNet v1.1 [63],
withoutanynegativeprompts.Forothermethods,weutilizedtheiropen-source
codetogenerateimagesandconductedfairevaluationsunderthesamesettings,
without changing any configures such as the number of inference steps or de-
noising sampler. While most methods employ the user-friendly SD1.5 as their
text-to-image model for controllable generation, we have observed that recently
there are a few models based on SDXL [35]. Therefore, we also report the con-
trollability results for ControlNet-SDXL and T2I-Adapter-SDXL. Please note10 Ming et al.
Table1:Controllabilitycomparisonwithstate-of-the-artmethodsunderdifferentcon-
ditional controls and datasets. ‚Üë denotes higher result is better, while ‚Üì means lower
isbetter.ControlNet++achievessignificantcontrollabilityimprovements.‚Äò-‚Äôindicates
thatthemethoddoesnotprovideapublicmodelfortesting.Wegeneratefourgroups
of images for each result and report the average result.
Condition Seg.Mask CannyEdge HedEdge LineArtEdge DepthMap
T2I
(Metric) (mIoU‚Üë) (F1Score‚Üë) (SSIM‚Üë) (SSIM‚Üë) (RMSE‚Üì)
Model
Dataset ADE20KCOCO-StuffMultiGen-20MMultiGen-20MMultiGen-20MMultiGen-20M
ControlNet SDXL - - - - - 40.00
T2I-Adapter SDXL - - 28.01 - 0.6394 39.75
T2I-Adapter SD1.5 12.61 - 23.65 - - 48.40
Gligen SD1.4 23.78 - 26.94 0.5634 - 38.83
Uni-ControlNet SD1.5 19.39 - 27.32 0.6910 - 40.65
UniControl SD1.5 25.44 - 30.82 0.7969 - 39.18
ControlNet SD1.5 27.46 22.58 34.65 0.7621 0.7054 35.90
Ours SD1.5 35.36 33.17 37.04 0.8097 0.8399 28.32
that ControlNet-SDXL mentioned here is not an officially released model as in
ControlNet [63]. More details are provided in the Supplementary Material.
4.2 Experimental Results
In this section, we compare our ControlNet++ with several state-of-the-art
methods in terms of controllability and image quality under different image-
based conditional controls. We employ Stable Diffusion v1.5 (SD1.5) following
the previous works [30,37,63,65] for fair comparison.
Comparison of Controllability with State-of-the-art Methods. The experimental
results are shown in Tab. 1, which can be summarized as the following ob-
servations: (1) Existing methods still underperform in terms of controllability,
struggling to achieve precise controlled generation. For instance, current meth-
ods (i.e., ControlNet) achieve only a 27.46 mIoU for images generated under
the condition of segmentation masks, which is far from its performance on real
datasets with a 50.7 mIoU, under the same evaluation from Mask2Former seg-
mentation model [8]. (2) Our ControlNet++ significantly outperforms existing
worksintermsofcontrollabilityacrossvariousconditionalcontrols.Forexample,
itachieves7.6%RMSEimprovementsagainstpreviousstate-of-the-artmethods
for the depth map condition; (3) For controllable diffusion models, the strength
of the text-to-image backbone does not affect its controllability. As shown in
the table, although SDXL-based [35] ControlNet and T2I-Adapter have better
controllability on some specific tasks, the improvement is not large and is not
significantly better than the counterparts with SD 1.5 [43].
QualitativeComparison. Figs.5and6provideaqualitativecomparisonbetween
ourControlNet++andpreviousstate-of-the-artmethodsacrossdifferentcondi-
tional controls. When given the same input text prompts and image-based con-
ditional controls, we observe that existing methods often generate areas incon-
sistentwiththeimageconditions.Forinstance,inthesegmentationmaskgener-
ationtask,othermethodsoftenproduceextraneouspictureframesonthewalls,ControlNet++ 11
Image & Condition Ours Uni-ControlNet UniControl Gligen ControlNet T2I-Adapter
Image & Condition Ours Uni-ControlNet UniControl Gligen ControlNet
Fig.5: Visualization comparison results in different conditional controls.
resulting in a mismatch between the seg-
Image & Condition Ours ControlNet T2I-Adapter
mentation masks extracted from the gen-
erated image and the input segmenta-
tion mask condition. A similar situa-
tionoccursunderdepthconditions,where
other methods fail to accurately repre-
sentthedepthinformationofdifferentfin-
gers.Incontrast,imagesgeneratedbyour
Fig.6: Visualization comparison re-
ControlNet++maintaingoodconsistency
sults on Line-Art Edge.
with the input depth map.
noitatnemgeS
htpeD
ynnaC
deH
trA-eniL12 Ming et al.
Table2:FID(‚Üì)comparisonwithstate-of-the-artmethodsunderdifferentconditional
controlsanddatasets.Alltheresultsareconductedon512√ó512imageresolutionwith
Clean-FIDimplementation[33]forfaircomparisons.‚Äò-‚Äôindicatesthatthemethoddoes
not provide a public model for testing. We generate four groups of images for each
result and report the average result to reduce random errors.
T2I Seg.Mask CannyEdge HedEdge LineArtEdge DepthMap
Method
ModelADE20KCOCOMultiGen-20MMultiGen-20MMultiGen-20MMultiGen-20M
Gligen SD1.4 33.02 - 18.89 - - 18.36
T2I-Adapter SD1.5 39.15 - 15.96 - - 22.52
UniControlNet SD1.5 39.70 - 17.14 17.08 - 20.27
UniControl SD1.5 46.34 - 19.94 15.99 - 18.66
ControlNet SD1.5 33.28 21.33 14.73 15.41 17.44 17.76
Ours SD1.5 29.49 19.29 18.23 15.01 13.88 16.66
Table 3: CLIP-score (‚Üë) comparison with state-of-the-art methods under different
conditional controls and datasets. ‚Äò-‚Äô indicates that the method does not provide a
publicmodelfortesting.Wegeneratefourgroupsofimagesforeachresultandreport
the average result to reduce random errors.
T2I Seg.Mask CannyEdge HedEdge LineArtEdge DepthMap
Method
ModelADE20KCOCOMultiGen-20MMultiGen-20MMultiGen-20MMultiGen-20M
Gligen SD1.4 31.12 - 31.77 - - 31.75
T2I-Adapter SD1.5 30.65 - 31.71 - - 31.46
UniControlNet SD1.5 30.59 - 31.84 31.94 - 31.66
UniControl SD1.5 30.92 - 31.97 32.02 - 32.45
ControlNet SD1.5 31.53 13.31 32.15 32.33 32.46 32.45
Ours SD1.5 31.96 13.13 31.87 32.05 31.95 32.09
Comparison of Image Quality with State-of-the-art Methods. To verify whether
improving controllability leads to a decline in image quality, we reported the
FID (Fr√©chet Inception Distance) metrics of different methods under various
conditionalgenerationtasksinTab.2.Wediscoveredthat,comparedtoexisting
methods, ControlNet++ generally exhibits superior FID values in most cases,
indicating that our approach, while enhancing the controllability of conditional
controls,doesnotresultinadecreaseinimagequality.Thiscanalsobeobserved
in Fig. 5. We provide more visual examples in Figures 12,13,14,15,16.
Comparison of CLIP score with State-of-the-art Methods. Our ControlNet++
aims to improve the controllability of diffusion models using image-based condi-
tions. Concerned about the potential adverse effects on text controllability, we
evaluatedvariousmethodsusingCLIP-Scoremetricsacrossdifferentdatasetsto
measurethesimilaritybetweengeneratedimagesandinputtext.Asindicatedin
Tab.3,ControlNet++achievedcomparableorsuperiorCLIP-Scoreoutcomeson
several datasets relative to existing approaches. This suggests that our method
not only markedly enhances conditional controllability but also preserves the
original model‚Äôs text-to-image generation proficiency.
Effectiveness of Generated Images. To further validate our improvements in
controllability and their impact, we use the generated images along with realControlNet++ 13
Generated Images Ground Truth Images GT + Generated Images
Fig.7: TrainingDeepLabv3(MobileNetv2)fromscratchwithdifferentimages,includ-
ing ground truth images from ADE20K, and the generated images from ControlNet
andours.Allthelabels(i.e.,segmentationmasks)aregroundtruthlabelsinADE20K.
Please note improvements here are non-trivial for semantic segmentation.
human-annotatedlabelstocreateanewdatasetfortrainingdiscriminativemod-
els from scratch. Please note that the only difference from the original dataset
used to train the discriminative model is that we have replaced the images with
those generated by the controllable diffusion model while keeping the labels un-
changed. If the generative model exhibits good controllability, the quality of
the constructed dataset will be higher, thereby enabling the training of a more
powerful discriminative model.
Specifically, we conduct experiments on the ADE20K [67,68] dataset on
DeepLabv3withMobileNetv2backbone[5].Weusethestandardtrainingdataset
(20210 training samples) to train the discriminative model and the validation
dataset (5000 evaluation samples) for evaluation. We show the experimental re-
sults in Fig. 7, the segmentation model trained on our images outperforms the
baseline results (ControlNet) by 1.19 mIoU. Please note that this improve-
ment is significant in segmentation tasks. For instance, Mask2Former [8]
improves previous SOTA MaskFormer [9] with around 1.1 mIoU in semantic
segmentation. In addition to conducting experiments solely on the generated
dataset, we also combined generated data with real data to train the segmenta-
tionmodel.Theexperimentalresultsindicatethataugmentingrealgroundtruth
data with data generated by ControlNet does not yield additional performance
improvements (34.11 v.s. 34.08). In contrast, augmenting real data with our
generated data results in significant performance enhancements (+1.76 mIoU).
Insummary:(1)bothexperiments(trainingthediscriminativemodelongen-
erated images only or on the real and generated images) show that our method
is better than ControlNet, demonstrating better controllability is beneficial for
training discriminative models. (2) Our generated image can help significantly
boost the discriminative model (e.g., segmentation model) performance when
training with real data.14 Ming et al.
50Steps 100Steps 200Steps 400Steps 50Steps 100Steps 200Steps 400Steps
InputCondition
Prompt:A pelican
gracefully takes off
from the calm water. WithoutDiffusionTrainingLoss WithDiffusionTrainingLoss
Fig.8: Ablationstudyondifferentlosssettingsduringtraining.Usingonlypixel-level
consistency loss leads to severe image distortion, affecting both image quality and
controllability. However, when combined with diffusion training loss, it is possible to
gradually improve controllability without compromising image quality.
NoPrompt Conflict Prompt Perfect Prompt
‚Äúdelicious cake‚Äù ‚Äúa house, high-quality, extremely detailed, 4K‚Äù
Fig.9: Whentheinputtextpromptisemptyorconflictswiththeimage-basedcondi-
tional controls (the segmentation map in the top left corner), ControlNet struggles to
generatecorrectcontent(redboxes),whereasourmethodmanagestogenerateitwell.
4.3 Ablation Study
Loss Settings. In our experiments, we find that maintaining the original diffu-
sion training process is crucial for preserving the quality and controllability of
generated images. Relying solely on pixel-level consistency loss leads to severe
image distortion, whereas training the model with both this loss and the dif-
fusion training loss can enhance controllability without affecting image quality.
We demonstrate the visualization results in Fig. 8.
Influence of Text Prompt. FollowingControlNet[63],herewediscusshowdiffer-
enttypesoftextprompts(NoPrompt,ConflictingPrompt,andPerfectPrompt.)
affectthefinalexperimentalresults.AsshowninFig.9,whenthetextpromptis
emptyorthereisasemanticconflictwiththeimageconditionalcontrol,Control-
Netoftenstrugglestogenerateaccuratecontent.Incontrast,ourControlNet++
managestogenerateimagesthatcomplywiththeinputconditionalcontrolsun-
der various text prompt scenarios.
sruO
teNlortnoCControlNet++ 15
Table 4: Stronger reward model (UperNet) leads to better controllability than the
weaker reward model (DeepLabv3). The eval mIoU are measured by Mask2Former
with performance of 56.01 mIoU.
Reward Model (RM) RM mIoU‚Üë Eval mIoU‚Üë
- - 31.38
DeepLabv3-MBv2 34.02 31.96
FCN-R101 39.91 33.62
UperNet-R50 42.05 35.36
ChoiceofDifferentRewardModels. Wedemonstratetheeffectivenessofdifferent
rewardmodelsinTab.4,alltheevaluationresults(i.e.,EvalmIoUinthetable)
are evaluated by the most powerful segmentation model Mask2Former [8] with
56.01 mIoU, on ADE20K dataset. We experiment with three different reward
models, including DeepLabv3 [5] with MobileNetv2 [47] backbone (DeepLabv3-
MBv2),FCN[28]withResNet-101[15]backbone(FCN-R101)andUperNet[57]
withResNet-50backbone.Theresultsdemonstratethatamorepowerfulreward
model leads to better controllability for controllable diffusion models.
5 Conclusion
In this paper, we demonstrate from both quantitative and qualitative perspec-
tives that existing works focusing on controllable generation still fail to achieve
precise conditional control, leading to inconsistency between generated images
and input conditions. To address this issue, we introduce ControlNet++, it ex-
plicitlyoptimizestheconsistencybetweeninputconditionsandgeneratedimages
using a pre-trained discriminative reward model in a cycle consistency manner,
which is different from existing methods that implicitly achieve controllability
through latent diffusion denoising. We also propose a novel and efficient reward
strategy that calculates consistency loss by adding noise to input images fol-
lowed by single-step denoising, thus avoiding the significant computational and
memory costs associated with sampling from random Gaussian noise. Experi-
mentalresultsundermultipleconditionalcontrolsshowthatControlNet++sig-
nificantly improves controllability without compromising image quality, offering
new insights into the field of controllable generation.
6 More Visualization
More visualization results across different conditional controls for our image
generation are shown in Figures 12,13,14,15,16.16 Ming et al.
7 Overview of Supplementary
The supplementary material is organized into the following sections:
‚Äì Section 8: Implementation details for all experiments.
‚Äì Section 9: Proof for Eq.(7) in the main paper.
‚Äì Section 10: Effectiveness of conditioning scale of existing methods.
‚Äì Section 11: Broader impact and limitation.
8 Implementation Details
8.1 Dataset Details
Considering that the training data for ControlNet [63] has not been publicly
released, we need to construct our training dataset. In this paper, we adhere to
the dataset construction principles of ControlNet [63], which endeavor to select
datasets with more accurate conditional conditions wherever possible. Specifi-
cally,forthesegmentationcondition,previousworkshaveprovideddatasetswith
accurately labeled segmentation masks [4,67,68]. Therefore, we opt to train our
modelusingtheseaccuratelylabeleddatasetsfollowingControlNet[63].Forthe
Hed,LineArtedgetasks,itischallengingtofinddatasetswithrealandaccurate
annotations. As a result, following ControlNet [63], we train the model using
the MultiGen20M dataset [37], which is annotated by models, to address this
issue.Regardingthedepthtask,existingdatasetsincludemasksofcertainpixels
as having unknown depth values, making them incompatible with the current
ControlNet pipeline. Therefore, we also adapt the MultiGen20M depth dataset,
which is similar to the dataset constructed by ControlNet [63]. In terms of the
canny edge task, no human labels are required in the process, so we also adapt
the MultiGen20M dataset. We provide details of the datasets in Tab. 5.
8.2 Reward Model and Evaluation Details
In general, we deliberately choose slightly weaker models as the reward model
and opt for stronger models for evaluation. This practice not only ensures the
fairness of the evaluation but also helps to determine whether performance im-
provements result from alignment with the reward model‚Äôs preferences or from
a genuine enhancement in controllability. While such an approach is feasible
for some tasks (Segmentation, Depth), it becomes challenging to implement for
others (Hed, Canny, LineArt Edge) due to the difficulty in finding two distinct
reward models. In such cases, we use the same model as both the reward model
andtheevaluationmodel.Weutilizestandardevaluationschemesfromtheirre-
spectiveresearchfieldstoevaluatetheinputconditionsandextractedconditions
from the generated images, as demonstrated in Sec. 4.1. We use the same Hed
edge detection model and LineArt edge detection model as ControlNet [63]. We
provide details of reward models and evaluation metrics in Tab. 6.ControlNet++ 17
Table 5: Dataset and evaluation details of different conditional controls. ‚Üë denotes
higher is better, while ‚Üì means lower is better.
SegmentationMask CannyEdge HedEdge LineArtEdge DepthMap
Dataset ADE20K[67,68],COCOStuff[4]MultiGen20M[37]MultiGen20M[37]MultiGen20M[37]MultiGen20M[37]
TrainingSamples 20,210&118,287 1,280,000 1,280,000 1,280,000 1,280,000
EvaluationSamples 2,000&5,000 5,000 5,000 5,000 5,000
EvaluationMetric mIoU‚Üë F1Score‚Üë SSIM‚Üë SSIM‚Üë RMSE‚Üì
Table 6: Details of the reward model, evaluation model, and training loss under dif-
ferent conditional controls. ControlNet* denotes we use the same model to extract
conditions as ControlNet [63]
Seg.Mask DepthEdge CannyEdge HedEdge LineArtEdge
RewardModel(RM) UperNet-R50 DPT-Hybrid KorniaCannyControlNet* ControlNet*
RMPerformance ADE20K(mIoU):42.05NYU(AbsRel):8.69 - - -
. EvaluationModel(EM) Mask2Former DPT-Large KorniaCannyControlNet* ControlNet*
EMPerformance ADE20K(mIoU):56.01NYU(AbsRel):8.32 - - -
ConsistencyLoss CrossEntropyLoss MSELoss MSELoss MSELoss MSELoss
LossWeightŒª 0.5 0.5 1.0 1.0 10
8.3 Training Details
The loss weight Œª for reward consistency loss is different for each condition.
Specifically, Œª is 0.5, 0.5, 1.0, 1.0, and 10 for segmentation mask, depth, hed
edge, canny edge, and LineArt edge condition, respectively. For all experiments,
we first fine-tune the pre-trained ControlNet until convergence using a batch
size of 256 and a learning rate of 1e-5. We then employ the same batch size and
learning rate for 5k iterations reward fine-tuning. To this end, the valid training
samples for reward fine-tuning is 256√ó5,000 = 1,280,000. We set threshold
t =200ofEq.8inthemainpaperforallexperiments.Divergingfromexisting
thre
methodsthatuseOpenCV‚Äôs[2]implementationofthecannyalgorithm,wehave
adopted Kornia‚Äôs [42] implementation to make it differentiable. Our codebase is
based on the implementation in HuggingFace‚Äôs Diffusers [34], and we do not use
classifier-free guidance during the reward fine-tuning process following diffusers.
9 Proof of Equation 7 in the Main Paper
The diffusion models define a Markovian chain of diffusion forward process
q(x |x ) by gradually adding noise to input data x :
t 0 0
‚àö ‚àö
x = Œ±¬Ø x + 1‚àíŒ±¬Ø œµ, œµ‚àºN(0,I), (10)
t t 0 t
at any timestep t we can use the predicted œµ(x‚Ä≤,c ,c ,t‚àí1) to estimate the
t v t
real noise œµ in Eq. 10, and the above equation can be transformed through
straightforward algebraic manipulation to the following form:
‚àö ‚àö
x ‚âà Œ±¬Ø x + 1‚àíŒ±¬Ø œµ (x‚Ä≤,c ,c ,t‚àí1),
t t 0 t Œ∏ t v t
‚àö
x‚Ä≤ ‚àí 1‚àíŒ± œµ (x‚Ä≤,c ,c ,t‚àí1) (11)
x ‚âàx‚Ä≤ = t t ‚àöŒ∏ t v t .
0 0 Œ±
t18 Ming et al.
Image Condition 1000 900 800 700 600 500 400 300 200 100
Predicted atdifferenttimestep
Fig.10: Illustrationofpredictedimagex‚Ä≤ atdifferenttimestepst.Asmalltimestept
0
(i.e., small noise œµ ) leads to more precise estimation x‚Ä≤ ‚âàx .
t 0 0
To this end, we can obtain the predicted original image x‚Ä≤ at any denoising
0
timesteptanduseitastheinputforrewardconsistencyloss.However,previous
work demonstrates that this approximation only yields a smaller error when
the time step t is relatively small [18]. Here we find similar results as shown in
Fig. 10, which illustrates the predicted x‚Ä≤ is significantly different at different
0
timesteps. We kindly encourage readers to refer to Section 4.3 and Figure 5 in
the DDPM [18] paper for more experimental results.
10 Conditioning Scale of ControlNet and T2I-Adapter
To simultaneously achieve control based on text prompts and image conditions,
existingcontrollablegenerationmethodsperformanadditionoperationbetween
the image condition features and the text embedding features. The strength of
different conditions can be adjusted through a weight value. Hence, an obvious
questionarises:canbettercontrollabilitybeachievedbyincreasingtheweightof
the image condition features? To answer this question, we conduct experiments
underdifferentcontrolscales(Theweightofimageconditionfeature)inFig.11.
It shows that naively increasing the control ratio of image conditions does not
enhance controllability and may lead to severe image distortion.
11 Broader Impact and Limitation
In this paper, we leverage visual discriminative models to evaluate and improve
the controllability of the text-to-image model. However, there are several inter-
esting directions for further investigation:
Conditions Expansion: While we have achieved notable improvements under
sixcontrolconditions,ourfutureworkaimstobroadenthescopebyincorporat-
ingadditionalcontrolconditionssuchasHumanPoseandScribbles.Ultimately,
our objective is to control everything.
Beyond Controllability: While our current focus lies predominantly on con-
trollability,weacknowledgethesignificanceofqualityandaestheticappealintheControlNet++ 19
Image&Condition Control Scale: 0.5 ControlScale:1.0 ControlScale:2.0 Control Scale: 3.0 Control Scale: 4.0 ControlScale:10.0
Fig.11:Naivelyincreasingtheweightofimageconditionembeddingcomparedtotext
condition embedding in exiting methods (i.e., ControlNet and T2I-Adapter) cannot
improvecontrollabilitywhileensuringimagequality.Theredboxesinthefigureshigh-
lightareaswherethegeneratedimageisinconsistentwiththeinputconditions.Please
notethatweemploythesamelinedetectionmodeltoextractconditionsfromimages.
generated outputs. To address this, we plan to leverage human feedback to an-
notate controllability images. Subsequently, we will optimize the controllability
model to simultaneously enhance both controllability and aesthetics.
Joint Optimization: To further enhance the overall performance, we intend
to employ a larger set of controllable images for joint optimization of the con-
trol network and reward model. This holistic approach would facilitate their
co-evolution, leading to further improvements in the final generated outputs.
Through our research, we aspire to provide insightful contributions to control-
lability in text-to-image diffusion models. We hope that our work inspires and
encourages more researchers to delve into this fascinating area.
teNlortnoC
LXDS-retpadA-I2T
detareneG
detcartxE
detareneG
detcartxE
segamI
snoitidnoC
segamI
snoitidnoC20 Ming et al.
Image & Condition Generated Images & Extracted Conditions
Fig.12: More visualization results of our ControlNet++ (LineArt Edge)ControlNet++ 21
Image & Condition Generated Images & Extracted Conditions
Fig.13: More visualization results of our ControlNet++ (Depth Map)22 Ming et al.
Image & Condition GeneratedImages&ExtractedConditions
Fig.14: More visualization results of our ControlNet++ (Hed Edge)ControlNet++ 23
Image & Condition GeneratedImages&ExtractedConditions
Fig.15: More visualization results of our ControlNet++ (Canny Edge)24 Ming et al.
Image & Condition Generated Images & Extracted Conditions
Fig.16: More visualization results of our ControlNet++ (Segmentation Mask)ControlNet++ 25
References
1. Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models
with reinforcement learning. arXiv preprint arXiv:2305.13301 (2023) 5
2. Bradski, G.: The OpenCV Library. Dr. Dobb‚Äôs Journal of Software Tools (2000)
17
3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: CVPR (2023) 4
4. Caesar,H.,Uijlings,J.,Ferrari,V.:Coco-stuff:Thingandstuffclassesincontext.
In: CVPR (2018) 9, 16, 17
5. Chen,L.C.,Papandreou,G.,Schroff,F.,Adam,H.:Rethinkingatrousconvolution
for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017) 13, 15
6. Chen, M., Laina, I., Vedaldi, A.: Training-free layout control with cross-attention
guidance. arXiv preprint arXiv:2304.03373 (2023) 5
7. Chen,T.,Xu,B.,Zhang,C.,Guestrin,C.:Trainingdeepnetswithsublinearmem-
ory cost. arXiv (2016) 7
8. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention
mask transformer for universal image segmentation. In: CVPR (2022) 10, 13, 15
9. Cheng,B.,Schwing,A.,Kirillov,A.:Per-pixelclassificationisnotallyouneedfor
semantic segmentation. NeurIPS (2021) 13
10. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-
guage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022) 5
11. Clark,K.,Vicol,P.,Swersky,K.,Fleet,D.J.:Directlyfine-tuningdiffusionmodels
on differentiable rewards. arXiv preprint arXiv:2309.17400 (2023) 3, 5, 7
12. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS
(2021) 1, 4
13. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P.,
Ghavamzadeh,M.,Lee,K.,Lee,K.:Dpok:Reinforcementlearningforfine-tuning
text-to-image diffusion models. NeurIPS (2023) 5
14. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
Cohen-or, D.: An image is worth one word: Personalizing text-to-image genera-
tion using textual inversion. In: ICLR (2023) 4
15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016) 15
16. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-or, D.:
Prompt-to-prompt image editing with cross-attention control. In: ICLR (2023) 4
17. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-
gies. In: SIGGRAPH (2001) 2
18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS
(2020) 4, 5, 6, 8, 18
19. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022) 4
20. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,
W.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022) 7
21. Hu, M., Zheng, J., Liu, D., Zheng, C., Wang, C., Tao, D., Cham, T.J.: Cocktail:
Mixing multi-modality controls for text-conditional image generation. NeurIPS
(2023) 5
22. Huang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., Zhou, J.: Composer: Creative
and controllable image synthesis with composable conditions. In: ICML (2015) 226 Ming et al.
23. Ju, X., Zeng, A., Zhao, C., Wang, J., Zhang, L., Xu, Q.: Humansd: A native
skeleton-guided diffusion model for human image generation. In: ICCV (2023) 2,
5
24. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.:Imagic:Text-basedrealimageeditingwithdiffusionmodels.In:CVPR(2023)
4
25. Kingma,D.,Salimans,T.,Poole,B.,Ho,J.:Variationaldiffusionmodels.NeurIPS
(2021) 4
26. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., Levy, O.: Pick-a-pic:
An open dataset of user preferences for text-to-image generation. arXiv preprint
arXiv:2305.01569 (2023) 5
27. Li,Y.,Liu,H.,Wu,Q.,Mu,F.,Yang,J.,Gao,J.,Li,C.,Lee,Y.J.:Gligen:Open-set
grounded text-to-image generation. In: CVPR (2023) 3, 5, 9
28. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: CVPR (2015) 15
29. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:Sdedit:Guided
imagesynthesisandeditingwithstochasticdifferentialequations.In:ICLR(2022)
4
30. Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter:
Learning adapters to dig out more controllable ability for text-to-image diffusion
models. arXiv preprint arXiv:2302.08453 (2023) 2, 3, 5, 6, 9, 10
31. Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models. In: ICML (2022) 4
32. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow
instructions with human feedback. NeurIPS (2022) 5
33. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in
gan evaluation. In: CVPR (2022) 12
34. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K.,
Davaadorj, M., Wolf, T.: Diffusers: State-of-the-art diffusion models. https://
github.com/huggingface/diffusers (2022) 17
35. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,M√ºller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv preprint arXiv:2307.01952 (2023) 4, 9, 10
36. Prabhudesai, M., Goyal, A., Pathak, D., Fragkiadaki, K.: Aligning text-to-image
diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739
(2023) 3, 5
37. Qin,C.,Zhang,S.,Yu,N.,Feng,Y.,Yang,X.,Zhou,Y.,Wang,H.,Niebles,J.C.,
Xiong,C.,Savarese,S.,etal.:Unicontrol:Aunifieddiffusionmodelforcontrollable
visual generation in the wild. NeurIPS (2023) 2, 3, 5, 9, 10, 16, 17
38. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 4
39. Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. JMLR (2020) 4
40. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
(2022) 4ControlNet++ 27
41. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever, I.: Zero-shot text-to-image generation. In: ICML (2021) 4
42. Riba,E.,Mishkin,D.,Ponsa,D.,Rublee,E.,Bradski,G.:Kornia:anopensource
differentiable computer vision library for pytorch. In: CVPR (2020) 17
43. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022) 1, 2, 4, 7, 10
44. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: MICCAI (2015) 4, 6
45. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation.
In: CVPR (2023) 4
46. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. NeurIPS (2022) 1,
4
47. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-
verted residuals and linear bottlenecks. In: CVPR (2018) 15
48. Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.,Wightman,R.,Cherti,M.,
Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open
large-scale dataset for training next generation image-text models. ArXiv (2022)
1, 9
49. Schuhmann,C.,Vencu,R.,Beaumont,R.,Kaczmarczyk,R.,Mullis,C.,Katta,A.,
Coombes,T.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered
400 million image-text pairs. ArXiv (2021) 1, 9
50. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
vised learning using nonequilibrium thermodynamics. In: ICML (2015) 1, 4
51. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:ICLR(2021)
4, 7
52. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. In: ICLR
(2021) 4
53. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 5
54. Wang, X., Darrell, T., Rambhatla, S.S., Girdhar, R., Misra, I.: Instancediffusion:
Instance-level control for image generation (2024) 5
55. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference
score v2: A solid benchmark for evaluating human preferences of text-to-image
synthesis. arXiv preprint arXiv:2306.09341 (2023) 5
56. Wu, X., Sun, K., Zhu, F., Zhao, R., Li, H.: Better aligning text-to-image models
with human preference. In: ICCV (2023) 5
57. Xiao,T.,Liu,Y.,Zhou,B.,Jiang,Y.,Sun,J.:Unifiedperceptualparsingforscene
understanding. In: ECCV (2018) 6, 15
58. Xie,J.,Li,Y.,Huang,Y.,Liu,H.,Zhang,W.,Zheng,Y.,Shou,M.Z.:Boxdiff:Text-
to-image synthesis with training-free box-constrained diffusion. In: ICCV (2023)
5
59. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015) 9
60. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagere-
ward: Learning and evaluating human preferences for text-to-image generation.
NeurIPS (2023) 3, 5, 7, 828 Ming et al.
61. Yang,Z.,Wang,J.,Gan,Z.,Li,L.,Lin,K.,Wu,C.,Duan,N.,Liu,Z.,Liu,C.,Zeng,
M., et al.: Reco: Region-controlled text-to-image generation. In: CVPR (2023) 2,
5
62. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-
ble image prompt adapter for text-to-image diffusion models. arXiv preprint
arXiv:2308.06721 (2023) 2
63. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: ICCV (2023) 2, 3, 4, 5, 6, 9, 10, 14, 16, 17
64. Zhang,T.,Zhang,Y.,Vineet,V.,Joshi,N.,Wang,X.:Controllabletext-to-image
generation with gpt-4. arXiv preprint arXiv:2305.18583 (2023) 5
65. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-
controlnet:All-in-onecontroltotext-to-imagediffusionmodels.NeurIPS(2023) 3,
5, 9, 10
66. Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector
framework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867
(2023) 9
67. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing
through ade20k dataset. In: CVPR (2017) 9, 13, 16, 17
68. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.:
Semanticunderstandingofscenesthroughtheade20kdataset.IJCV(2019) 9,13,
16, 17
69. Zhou, D., Li, Y., Ma, F., Yang, Z., Yang, Y.: Migc: Multi-instance generation
controller for text-to-image synthesis. In: CVPR (2024) 5
70. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023) 9
71. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: ICCV (2017) 2