GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo
JiangWu1* RuiLi1,2* HaofeiXu2,3 WenxunZhao1 YuZhu1✝ JinqiuSun1 YanningZhang1✝
1NorthwesternPolytechnicalUniversity 2ETHZu¨rich 3UniversityofTu¨bingen,Tu¨bingenAICenter
(a)MVSFormer[2] (b)RA-MVSNet[45] (c)ET-MVSNet[14] (d)GeoMVSNet [46] (e)Ours
Figure1. ComparisonofreconstructionerrorsonTanksandTemplebenchmark. Weshowprecisionandrecallerrormapsforthe
“Horse”scan.Ourmethoddemonstratesnotableimprovementsoverexistingmethodsinchallengingareas.
Abstract 1.Introduction
Multi-view stereo (MVS) is a fundamental computer vi-
Matching cost aggregation plays a fundamental role in
sion problem that recovers 3D shapes from posed im-
learning-based multi-view stereo networks. However, di-
agesbymulti-viewcorrespondencematching[21]. Recent
rectly aggregating adjacent costs can lead to suboptimal
learning-basedMVS[11,25,30,38]estimatesscenedepth
resultsduetolocalgeometricinconsistency. Relatedmeth-
from the cost volume computed by geometric matching,
odseitherseekselectiveaggregationorimproveaggregated
which delivers latent geometric cues crucial for the final
depth in the 2D space, both are unable to handle geomet-
depth[7]. However,theinitialcostvolumecansufferfrom
ricinconsistencyin thecostvolumeeffectively. Inthis pa-
challengingmatchingconditions,e.g.,varyingillumination,
per,weproposeGoMVStoaggregategeometricallyconsis-
textless areas, or repetitive patterns, leading to suboptimal
tentcosts,yieldingbetterutilizationofadjacentgeometries.
pixel-wisecoststhathamperaccurateestimations.
More specifically, we correspond and propagate adjacent
To mitigate this issue, cost aggregation plays an impor-
coststothereferencepixelbyleveragingthelocalgeomet-
tantroleinremovingmatchingambiguitiesandimproving
ric smoothness in conjunction with surface normals. We
discriminativeness by using the neighboring information.
achievethisbythegeometricconsistentpropagation(GCP)
However,theadjacentcostsmaydeliverinconsistentdepth
module. Itcomputesthecorrespondencefromtheadjacent
cues due to the gradual changes in local geometry. As a
depth hypothesis space to the reference depth space using
result, the aggregated costs are not geometrically guaran-
surface normals, then uses the correspondence to propa-
teed to have the highest matching score at the real refer-
gate adjacent costs to the reference geometry, followed by
ence depth, leading to suboptimal depth predictions. The
a convolution for aggregation. Our method achieves new
widely adopted cascade framework [7] can potentially ex-
state-of-the-artperformanceonDTU,Tanks&Temple,and
acerbate this issue as the adjacent costs can have more di-
ETH3D datasets. Notably, our method ranks 1st on the
vergentcostsduetotheshifteddepthhypotheses.
Tanks & Temple Advanced benchmark. Code is available
Asthegeometricinconsistencyisacommonchallengein
athttps://github.com/Wuuu3511/GoMVS.
multi-viewstereoand2-viewstereomatching,relatedmeth-
ods either adopt learned aggregation [25, 31] or enforce
*indicatesequalcontributionsand✝indicatescorrespondingauthors. consistency to the aggregated depth [9, 15, 42]. Specif-
1
4202
rpA
11
]VC.sc[
1v29970.4042:viXra
noisicerP
llaceR
mm0
mm5.4
mm9ically, some methods [25, 31] adopt adaptive aggregation 2.RelatedWorks
schemes to allow networks to select pixels that potentially
2.1.Learning-basedMVSMethods
correlatewellandcontributetothereferencepixel’sgeome-
try. However,theyheavilyrelyonnetworkcapabilitiesand Multi-View Stereo (MVS) aims to reconstruct 3D scenes
do not guarantee geometric plausibility from the selected from multiple posed images. In recent years, learning-
costs. Othermethods[9,19]seektorefineorregularizethe based methods have exhibited promising results. MVS-
aggregateddepthvaluesusingjointlyestimatedsurfacenor- Net [38] uses differentiable homography to construct the
mals. However,thesemethodsonlyrefinetheoutputdepth cost volume and employs a 3D U-Net for regularization.
in 2D image space and are inherently unable to handle in- Subsequentworksimprovethisframeworkinseveralways.
consistencies in the cost volume, which is vital for MVS RNN-based methods [29, 35, 39]and coarse-to-fine ap-
methods. proaches [3, 7, 17, 25, 36] reduce memory consumption
through by designing efficient structures. Another group
of methods [4, 12, 14] devises local or global attention
In this paper, we propose GoMVS that aggregates ge- modules to enhance input feature representations. MVS-
ometrically consistent costs, allowing better utilization of Former [2] incorporates an additional pre-trained trans-
adjacent geometries. Considering that the local geometry former network, enhancing the performance of MVS with
is usually smooth and exhibits gradual changes, we lever- a powerful feature extractor. However, it lacks further ex-
age the local smoothness to correspond and propagate ad- ploration in terms of geometry. GeoMVSNet [46] utilizes
jacent costs to the reference cost. We achieve this by the the coarse depth map to extract additional geometric fea-
geometrically consistent aggregation scheme, which oper- tures. In addition, [27, 34, 44] have designed pixel-wise
ates on the local convolution window and propagates ad- visibilitymodulestohandleocclusions.
jacent costs with the geometrically consistent propagation
2.2.CostVolumeAggregation
(GCP)module. TheGCPmodulecomputesthecorrespon-
dences from the adjacent cost’s hypothesized depth space As cost volume is vital for multi-view depth estimation,
to the reference cost’s depth space, using back-projected recent works introduce different cost aggregation methods
depth hypotheses and the surface normal. Then, it prop- to the depth network. NP-CVP-MVSNet [37] introduces
agates the adjacent costs to the reference by interpolating sparseconvolutiontoaggregatematchingcostsatthesame
costscoreswithrespecttothecorrespondence. Afterprop- depthrange. WT-MVSNet[12]employsacosttransformer
agatingadjacentcostswithinalocalwindow,weaggregate togenerateamorecompleteandsmootherprobabilityvol-
themusingstandardconvolutions.Unlikepreviousmethods ume. GeoMVSNet [46] incorporates the coarse probabil-
[9, 15, 19] that refine the predicted depth in the 2D space, ity volume to enhance the matching discriminative ability.
our method incorporates geometric consistency in the cost While these methods improve the capability of regulariza-
space, yielding a better utilization of adjacent geometries. tionnetworks,thelocalgeometricinconsistencyofthecost
As surface normal is crucial for corresponding and propa- volume still remains and poses challenges for the final ag-
gating local costs, we further investigate different choices gregationresults.
ofnormalpredictions. Wefindappropriatelyapplyingoff-
the-shelfmonocularnormalmodelsenablessmoothandro- 2.3.NormalAssistedDepthEstimation
bustaggregationacrossdatasets. Weconductextensiveex-
Surfacenormalprovidesrichgeometricdetailsandhasbeen
periments to evaluate our method’s effectiveness, and our
widely applied in recent years to depth estimation tasks.
methodachievesnewstate-of-the-artonDTU,Tank&Tem-
Traditional MVS methods [20, 32, 33] optimize depth and
ple, as well as the ETH3D dataset. Our contributions are
normal hypotheses simultaneously by constructing a pla-
summarizedasfollows:
nar prior model. Inspired by traditional methods, SP-
Net[28]performsslantedplanecostaggregationbylearn-
ing parameterized local planes. NAPV-MVS [24] uses lo-
• We propose GoMVS to aggregate geometrically consis- cal normal similarity to emphasize the most relevant adja-
tent costs, allowing better utilization of adjacent geome- cent costs. NR-MVSNet [10] utilizes depth-normal con-
tries. sistencytoadaptivelyexpandthehypothesisrange,provid-
• We propose a geometrically consistent propagation ing broader matchings to assist depth inference. However,
(GCP)modulethatallowsgeometricallyplausiblecorre- these methods do not address the local inconsistency is-
spondenceandpropagationincostspace. sue. GeoNet [19] proposes a monocular depth estimation
• We investigate different choices of normal computation method that uses kernel regression to refine output depth
and find that properly applying the monocular surface withnormals. However,itissensitivetonoisyoutputsand
normalmodelperformswellacrossdatasets. isinherentlyincapableofhandlingcostvolumeinputs. An-
2Geometrically Consistent Propagation
Adjacent Cost
...

Depth Hypotheses at Pixel   0 →     →      0 →     → 
Co  st  Propagated Cost
Normal at Pixel Propagation
Geometric Correspondences

Depth Hypothesis at Pixel
... (   .  ) GT   Depth   →    → 
0 

Geometrically Consistent Aggregation
FPN
Aggregation Convolution
W window
...
G
−  C  
W ×
U P
...
R  ef. P  ixel        − 
W ...
Adj. Pixel 
Warping U Unfoldj         −    Aggreg’ated Cost

→ 
Cost Volume

Normal
Src. Images

Ref. Image
Depth Hypothesis Depth Network Depth Map
Figure2. Overviewofourmethod. Given  areferenceimageandasetofsourceimages,weuseFPNtoextractmulti-scalefeaturesfor
costvolumereconstruction.Toconductgeometricallyconsistentaggregationwithinthelocalwindow,wecollectadjacentgeometriccues
andsendthemtotheproposedgeometricallyconsistentpropagation(GCP)module,whichcomputesthecorrespondencefromtheadjacent
depthhypothesisspacetothereferencedepthspace. Theresultingcostsareendowedwithgeometricconsistency,whichfacilitatesbetter
utilizationofadjacentgeometryandcanbeaggregatedbytheconvolution.
other line of works [9, 15, 42] proposes the depth-normal 3.1.CostVolumeConstruction
consistencylosstoenhancethenetwork’sperceptionofge-
We first apply a Feature Pyramid Network [13] to extract
ometriccues. Unlikethesemethods,ourmethodleverages
thenormaltoyieldgeometricallyconsistentcostsinthe3D
multi-scale image features {Fs i}N
i=0
∈ R 2H s×W 2s×M, where
sisthescalefactor. Forsimplicity,weomitthesuperscript
space,yieldingbetterutilizationofadjacentcosts.
ofsbelow. Tobuildthecostvolumeineachstage,wefirst
sample depth hypotheses d for each pixel in a predefined
depth range. Through differentiable homography, we can
3.Methodology
computethecorrespondingpositionp′ ofthereferenceim-
age’spixelpinthesourceimage,
Given a reference image I ∈ RH×W×3 and N source
0
images {I }N , as well as camera intrinsic {K }N and
i i=1 i i=0
extrinsic parameters {[R 0→i;t 0→i]}N i=1, our goal is to es- p′ =K i[R 0→i(K− 01pd)+t 0→i], (1)
timate the depth map of I from multiple posed images.
0
Fig. 2 shows an overview of our method. We first uti- whereRandtdenotetherotationandtranslationparame-
lize multi-scale image features to build the cost volume tersandKaretheintrinsicmatrix. LetF(p)representsthe
(Sec. 3.1). We then introduce the geometrically consistent featurevectoratpixelp,thenthetwo-viewfeaturecorrela-
aggregationscheme(Sec.3.2),whichconsistsoftheblocks tionvolumeVatpixelpcanberepresentedas
inthedepthnetwork. Wetheninvestigatedifferentchoices
forobtainingsurfacenormals(Sec. 3.3). V (p)=F (p)·F (p′), (2)
i 0 i
3
...where · refers to the dot product. To aggregate multiple depth space of the reference pixel. Based on depth corre-
pair-wisecostvolumes,weutilizeashallownetwork[25]to spondence,weperformgeometricallyconsistentcostprop-
learnthepixel-wiseweightmapsW. Theweightcomputa- agation(GCP).Firstly,weintroducethedepthrelationship
tiontakesplaceexclusivelyintheinitialstage,whileweight amongpixelswithinthesameplane. Givenapixel’simage
mapsforsubsequentstagesarederivedthroughupsampling coordinates (u,v) and depth d(u,v), its 3D point X(u,v)
from the previous stage. Then the multi-view aggregated inthecameracoordinatesystemcanberepresentedas
costvolumeCcanberepresentedas:

x
  u−cx 
C= (cid:80)N i (cid:80)=1 NW i W⊙V i. (3) X(u,v)= y
z
=  v−f f 1x ycy  d(u,v), (4)
i=1 i
3.2.GeometricallyConsistentAggregation wherec ,c ,f ,andf aretheparametersofcameraintrin-
x y x y
sicK. Forthegivenreferencepixeliandadjacentpixelj,
An essential idea of cost aggregation is to leverage neigh-
wemodeltherelationshipbetweenX(u ,v )andX(u ,v )
boringinformationtoimprovethediscriminativenessofthe i i j j
byleveraginglocalplanarassumptionandthesurfacenor-
cost volume, where the key is to find the most relevant
maln. Theysatisfytheequationof
neighbors and effectively aggregate their matching costs.
Toachievethis,typicalconvolution-basedmethodsarelim-
n⊤(X(u ,v )−X(u ,v ))=0. (5)
itedtothesizeoftheconvolutionkernel(e.g., 3×3×3), i i j j
andgeometricinconsistencyisverylikelytohappeninthis
According to Eq. (4) and Eq. (5) , the depth relationship
localregionduetonon-constantdepthdistributionswithin
betweenthereferencepixeliandtheadjacentpixelsj can
this kernel. It’s also computationally inefficient to directly
berepresentedas:
increasethekernelsizetogetimprovedperformance.
In this paper, we observe in a small local region, many (cid:104) (cid:105)⊤
n⊤ ui−cx vi−cy 1
scenescanbeapproximatedwithaplane,whichfrequently d(u j,v j)
=
fx fy
. (6)
exists in real-world scenarios. To this end, we propose to d(u i,v i) n⊤(cid:104) uj−cx vj−cy 1 (cid:105)⊤
leveragethislocallyapproximatedplanarstructuretoguide fx fy
the cost aggregation process in a geometrically consistent
manner. There exists an analytic relationship between the We use r ji = d d( (u uj i, ,v vj i)) to denote the depth ratio be-
referencepixel’sdepthanditslocalneighbors,whichcanbe tween j and i, which describes the linear transformation
leveraged to obtain more reliable cost candidates. Specifi- of depth within the plane. Based on this, we can compute
cally,foreachreferencepixel,wefirstcollectthegeometric the depth hypothesis correspondences. Specifically, define
cluesofitsk×kspatialwindowtocomputethecorrespon- [d1,...,dL] as the depth hypothesis in the pixel i’s depth
i i
dences of the depth hypothesis. Depending on the corre- space,whereLreferstothenumberofdepthsamplinglev-
sponding location, we propagate the adjacent costs to the els.Eachdepthhypothesisisthenmappedtopixelj’sdepth
referencepixel’sdepthspace. Finally,weuseaconvolution spacethroughthedepthratior .
ji
layertoaggregatethepropagatedcosts.
[d1 ,...,dL ]=[r ×d1,...,r ×dL], (7)
3.2.1 LocalGeometricCluesCollection i→j i→j ji i ji i
whered representsthemappingdepthofpixeli’sdepth
We first collect local depth hypotheses and normal maps i→j
hypothesisinpixelj’sdepthspace. Wethenpropagatethe
for each pixel within a spatial window. Specifically, given
matching cost of pixel j at the d to d . Let C denote
thedepthhypothesesofshapeL×H ×W andthenormal i→j i j
the cost for pixel j. The propagated matching cost C
mapofshape3×H×W,whereListhedepthhypothesis j→i
canbeexpressedas:
numberandH,W denotesthespatialdimension,weunfold
eachpixelwithak×kspatialwindow,yieldinglocalinter-
C (d0,...,dl)=C (d0 ,...,dl ). (8)
mediatedepthhypothesesvolumeandnormalmapofshape j→i i i j i→j i→j
k2×L×H ×W andk2×3×H ×W,respectively. We
Since depth hypotheses are discretely sampled at regular
then compute the depth hypothesis correspondences based
depthintervalswithinthedepthrange,wecanconveniently
ontheseintermediategeometricclues.
use linear interpolation to implement the above process.
Withthedefinitiondm =dn,C (dm)canbeexpressed
i→j j j→i i
3.2.2 GeometricallyConsistentPropagation as:
To better aggregate the high-quality costs of the adjacent n−⌊n⌋
C (dm)=(C (d⌈n⌉)−C (d⌊n⌋)) . (9)
pixels,wealigntheadjacentpixels’depthhypothesistothe j→i i j j j j ⌈n⌉−⌊n⌋
4We refer to this process as geometrically consistent prop- the surface normal. Since Omnidata is trained on low-
agation from j to i. It can generate geometrically consis- resolutionimages,itsnormalpredictionmightbecomeun-
tent cost candidates for each reference pixel. Due to vary- reliable when the testing input resolution is increased. To
ingdepthrelationshipsbetweeneachpixelanditsadjacent tackle this, we adopt a divide-and-conquer approach fol-
pixels, cost propagation generates an intermediate cost of lowing MonoSDF [43] to generate high-resolution normal
k2M ×L×H ×W,whereM isthechanneldimension. cues. Specifically,wefirstdividethehigh-resolutionimage
into multiple overlapping patches. Surface normal estima-
3.2.3 AggregatingPropagatedCosts tion is then independently conducted for each patch. Sub-
sequently, thesurfacenormalresultsarealignedandfused
Sincetheintermediatecostsincludek×kspatialinforma- togenerateahigh-resolutionnormalmap.
tioninthechanneldimension, wethusaggregatethecosts
3.4.Optimization
using convolutions with a kernel size of 1×1×k and an
expandedchanneldimensionk2M,leadingtothesamepa-
WetreattheMVStaskasaclassificationproblemandem-
rameters as the generic 3D convolutions with kernel size ploy the winner-takes-all strategy to obtain the final depth
k×k×k. map [39]. We use the cross-entropy loss (Eq. 11) in each
We encapsulate GCP and the convolution into one geo- stage,whichisappliedtotheprobabilityvolumeP andthe
metricallyconsistentaggregationoperatorusedtobuildthe groundtruthone-hotvolumeP′. Following [17],alldepth
depthnetwork. Inparticular,westillkeepthe3DU-Netar- out-of-rangewillbemaskedduringthetrainingstage.
chitectureproposedbyMVSNet[38],whilereplacingeach
standard3D convolutionblock withourproposed geomet- (cid:88)d
′
L= −P log(P ). (11)
ricallyconsistentaggregationoperator. Fortheupsampling i i
layerintheU-Netstructure,weusethepixelshuffletore- i=1
organizefeaturesandobtainahigh-resolutioncostvolume.
4.Experiments
3.3.ExtractingNormalCues
In this section, we evaluate our method on the DTU [1],
Since our approach uses the surface normal for cost ag- ETH3D [22], and Tanks and Temple [8] datasets, respec-
gregation, in this section, we study different methods for tively. Furthermore,weconductedmultipleablationexper-
obtaining surface normals. We conduct experiments to iments on the DTU dataset to validate the effectiveness of
demonstratetheeffectivenessofeachmethodinSec. 4.4. ourmethod.
Depthtonormal. Thesurfacenormalcanbedirectlycom-
4.1.Datasets
putedfromtheestimateddepth. Sinceweuseathree-stage
cascade structure, we leverage the depth map from the g DTU[1]datasetcomprises128scenesincontrolledlabora-
stagetogeneratethesurfacenormalfortheg+1stage.The tory environments, with models captured using structured
normalncanbecomputed[19]inclosedformas: light scanners. Each scene was scanned from the same
49 or 64 camera positions under 7 different lighting con-
(ATA)−1AT1 ditions. Theofficialevaluationassessesthepointcloudus-
n= , (10)
∥(ATA)−1AT1∥ ingdistancemetricsofaccuracyandcompleteness. Blend-
edMVS [40] is a large-scale MVS dataset that consists of
whereAisamatrixcomposedofthecoordinatesofallpix- over 17,000 high-resolution images covering a variety of
elswithinthelocalwindow. Inadditiontousingestimated scenes, including urban environments, architecture, sculp-
depthmaps, wealsocomputetheGTnormalfromtheGT tures,andsmallobjects. TanksandTemples(TNT)[8]isa
depthmapsfollowingthesameprotocolanduseittotrain real-worlddataset,dividedintotwosets,including8scenes
ourmethodforevaluatingperformances. in the intermediate set and 6 scenes in the advanced set.
Costtonormal. Inaddition,inspiredby [9],weuseanad- ETH3D [22] dataset consists of multiple indoor and out-
ditionalnetworkbranchtodirectlyregressthenormalmap doorsceneswithlargeviewpointvariations. Thequalityof
fromthecostvolumeineachstage,whichisthenusedasa pointcloudsontheETH3DandTNTdatasetsismeasured
priorforgeometricallyconsistentaggregation. usingthepercentageofprecisionandrecall.
Off-the-shelfmonocularsurfacenormal. Monocularnet-
4.2.ImplementationDetails
works directly perceive surface geometry from deep fea-
turesandcanestimatereasonablesolutionsinregionswith Training FollowingthedatapartitioningofMVSNet,we
multi-view consistency ambiguities, which complements firsttrainthemodelontheDTUtrainingset. Ournetwork
thetaskofMVS.Therefore,weexploreanexistingmonoc- employs a three-stage cascade structure, with depth sam-
ular normal estimation network Omnidata [5] to generate pling at 48, 32, and 8 in each stage and depth intervals of
5(a)TransMVSNet[4] (b)GeoMVSNet[46] (c)Ours (d)GT
Figure3.Comparisonofreconstructionresults.Ourmethodreconstructsmorecompleteresultsinchallengingareas.
4,1,and0.5,respectively. Wetrainourmodelwith5input Method Acc.↓ Comp.↓ Overall↓
images,eachhavingaresolutionof512×640.Themodelis Gipuma[6] 0.283 0.873 0.578
optimized using Adam for 12 epochs, starting with an ini- COLMAP[21] 0.400 0.664 0.532
tiallearningrateof0.001whichisreducedby0.5afterthe NAPV-MVS[24] 0.367 0.375 0.371
AA-RMVSNet[29] 0.376 0.339 0.357
6and8epochs. Wethenfine-tunethemodelontheBlend-
Vis-MVSNet[44] 0.369 0.361 0.365
edMVS dataset with 9 images at a resolution of 576×768 CasMVSNet[7] 0.325 0.385 0.355
forevaluationonTanksandTemplesandETH3Ddatasets. UniMVSNet[18] 0.352 0.278 0.315
MVSTER[27] 0.350 0.276 0.313
During fine-tuning, we reduce the depth sampling interval
TransMVSNet[4] 0.321 0.289 0.305
ofthelaststagebyhalfofitsoriginalvalue. GbiNet*[17] 0.314 0.295 0.305
RA-MVSNet[45] 0.326 0.268 0.297
Evaluation When testing on the DTU dataset, we use 5 GeoMVSNet[46] 0.331 0.259 0.295
images at a resolution of 864×1152 as input and employ ET-MVSNet[14] 0.329 0.253 0.291
MVSformer[2] 0.327 0.251 0.289
the depth map filtering method following [46] to generate
GoMVS 0.347 0.227 0.287
the final point cloud. For the tanks and temple dataset,
we carried out tests using 11 images with a resolution of
Table1. QuantitativeresultsonDTU[1]. Ourmethodachieves
960×1920. In terms of depth map fusion, we employ the
thebestcompletenessandoverallscore. Moreover,thecomplete-
widelyadopteddynamicfusionstrategy[35].Moreover,we
ness of our point cloud outperforms previous methods by large
conducted tests on the ETH3D dataset using images with margins.
a size of 1152×1536 and the depth map fusion strategy is
consistentwithIterMVS[26]. ous SOTA methods. We have more detailed and complete
reconstructionsinthechallengeareas.
4.3.BenchmarkPerformance
Evaluation on Tanks and Temples dataset. We vali-
Evaluation on DTU dataset. We compare both tradi- dated the generalization of our model on the Tanks and
tional methods and deep learning-based approaches. The Temples dataset, and the quantitative results are shown in
quantitative evaluation results for point cloud reconstruc- Table . We achieved the best performance on both the in-
tionareshowninTab1. OurmethodachievesSOTAcom- termediate and advanced sets. Moreover, we ranked 1st
pleteness and overall performance. It is worth noting that among all submitted results on the advanced set of the
our method shows obvious improvement in completeness TNT benchmark, which contains more complex scenes. It
comparedtopreviousmethods. Thisdemonstratesthatour demonstratesthestrongrobustnessandgeneralizationabil-
methodcanbetteruseadjacentcoststopropagatelocalge- ity of our method. Fig. 4 shows point cloud results on
ometries,resultinginamorecompletereconstruction. Fig. intermediate and advanced sets. Our method achieves de-
3showsacomparisonofourpointcloudresultswithprevi- tailedandcompletereconstructionsacrossdifferentindoor
6(a)Family (b)Francis (c)Courtroom (d)Musume
Figure4.QualitativeresultsonTanksandTemples.Ourmethodachievesdetailedandcompletereconstructionsacrossdifferentscenes.
Intermediate Advanced
Methods
Mean↑ Fam. Fra. Hor. Lig. M60 Pan. Pla. Tra. Mean↑ Aud. Bal. Cou. Mus. Pal. Tem.
COLMAP[21] 42.14 50.41 22.25 26.63 56.43 44.83 46.97 48.53 42.04 27.24 16.02 25.23 34.70 41.51 18.05 27.94
CasMVSNet[7] 56.84 76.37 58.45 46.26 55.81 56.11 54.06 58.18 49.51 31.12 19.81 38.46 29.10 43.87 27.36 28.11
Vis-MVSNet[44] 60.03 77.40 60.23 47.07 63.44 62.21 57.28 60.54 52.07 33.78 20.79 38.77 32.45 44.20 28.73 37.70
GBiNet[17] 61.42 79.77 67.69 51.81 61.25 60.37 55.87 60.67 53.89 37.32 29.77 42.12 36.30 47.69 31.11 36.93
EPP-MVSNet[16] 61.68 77.86 60.54 52.96 62.33 61.69 60.34 62.44 55.30 35.72 21.28 39.74 35.34 49.21 30.00 38.75
TransMVSNet[4] 63.52 80.92 65.83 56.94 62.54 63.06 60.00 60.20 58.67 37.00 24.84 44.59 34.77 46.49 34.69 36.62
UniMVSNet[18] 64.36 81.20 66.43 53.11 64.36 66.09 64.84 62.23 57.53 38.96 28.33 44.36 39.74 52.89 33.80 34.63
D-MVSNet[41] 64.66 81.27 67.54 59.10 63.12 64.64 64.80 59.83 56.97 41.17 30.08 46.10 40.65 53.53 35.08 41.60
ET-MVSNet[14] 65.49 81.65 68.79 59.46 65.72 64.22 64.03 61.23 58.79 40.41 28.86 45.18 38.66 51.10 35.39 43.23
RA-MVSNet[45] 65.72 82.44 66.61 58.40 64.78 67.14 65.60 62.74 58.08 39.93 29.17 46.05 40.23 53.22 34.62 36.30
GeoMVSNet[46] 65.89 81.64 67.53 55.78 68.02 65.49 67.19 63.27 58.22 41.52 30.23 46.53 39.98 53.05 35.98 43.34
MVSFormer[2] 66.37 82.06 69.34 60.49 68.61 65.67 64.08 61.23 59.33 40.87 28.22 46.75 39.30 52.88 35.16 42.95
GoMVS 66.44 82.68 69.23 69.19 63.56 65.13 62.10 58.81 60.80 43.07 35.52 47.15 42.52 52.08 36.34 44.82
Table2. QuantitativeresultsofF-scoreonTanksandTemplesbenchmark. OurmethodachievesthebestF-scoreonboththe“Inter-
mediate”andthechallenging“Advanced”set.Notethatourmethodranks1stontheofficialTNTAdvancedBenchmark.
Method Acc.↓ Comp.↓ Overall↓ compare different cost aggregation and depth aggregation
Standard3D-convolution[38] 0.365 0.265 0.315 methods, and the results are shown in Tab. 3. Regarding
Sparseconvolution[37] 0.354 0.268 0.311 the cost aggregation methods, Sparse convolution [37] ag-
Spatialdeformableaggregation[25] 0.363 0.257 0.310 gregatesthecostatthesamedepthwithoutfullyconsider-
Depthkernelregression[19] 0.369 0.262 0.316
ing the depth geometry, resulting in certain improvements
Ours(GCA) 0.347 0.227 0.287
in performance compared with the baseline. PatchMatch-
Net [25] utilizes deformable convolutions to gather spatial
Table3. Comparisonwithdifferentaggregationmethods. Our
method significantly outperforms previous cost volume aggrega- matchingcostsandaggregatethemusingalightweight3D
tionmethods. CNN.Wereplacetheaggregationnetworkwitha3DU-Net
toensureafaircomparisonwiththesameparameterscale.
andoutdoorscenes.
Evaluation on ETH3D dataset. The ETH3D dataset PatchmatchNet heavily relies on network capabilities and
contains many challenging scenes, including scenes with doesnotguaranteegeometricplausibilityfromtheselected
textureless areas and large viewpoint variations. We com- costs. As a result, it brings limited performance improve-
pare our methods with previous methods and results are ments(row#3).
showninTab.4.Ourmethodachievesthebestperformance Additionally, for the depth aggregation method, we re-
onboththevalidationsetandthetestsplit. Inparticular,it finethedepthmaponthebaselinemethodbyincorporating
outperformspreviousSOTAbyasignificantmarginonthe depth kernel regression proposed by GeoNet [19]. Using
testsplit,demonstratingitsgeneralizationabilityoverexist- normal similarity to compute depth aggregation weights is
ingmethods. prone to the influence of normal noise and cannot effec-
tivelyutilizetheabundantgeometricinformationinthecost
4.4.AblationStudy
volume. This leads to a decline in the accuracy of the fi-
Comparison with different aggregation methods. To nalpointcloud(row#4). Weutilizenormalpriorstoguide
verify the effectiveness of utilizing adjacent geometry, we costaggregation,alleviatingthechallengeofgeometricin-
7Training Test
Methods
Acc.↑ Comp.↑ F-score↑ Acc.↑ Comp.↑ F-score↑
COLMAP[21] 91.85 55.13 67.66 91.97 62.98 73.01
ACMM[32] 90.67 70.42 78.86 90.65 74.34 80.78
IterMVS[26] 73.62 61.87 66.36 76.91 72.65 74.29
GBi-Net[17] 73.17 69.21 70.78 80.02 75.65 78.40
MVSTER[27] 76.92 68.08 72.06 77.09 82.47 79.01
PVSNet[34] 83.00 71.76 76.57 81.55 83.97 82.62
EPP-MVSNet[16] 82.76 67.58 74.00 85.47 81.79 83.40
Vis-MVSNet[44] 83.32 65.53 72.77 86.86 80.92 83.46
EPNet[23] 79.36 79.28 79.08 80.37 87.84 83.72
GoMVS 81.22 77.65 79.16 85.50 86.85 85.91
Table4. QuantitativeresultsonETH3Ddataset. Weshowcomparisonsofreconstructedpointcloudsusingpercentagemetric(%)ata
thresholdof2cm.Ourapproachachievesthebestperformancewithnotablemargins.
consistency and achieving the best performance among all Aggregationkernel(d×h×w) Acc.↓ Comp.↓ Overall↓
aggregationmethods.
StandardConv3D(3×3×3) 0.365 0.265 0.315
StandardConv3D(5×3×3) 0.352 0.260 0.306
Comparisonwithdifferentdepthreceptivefields. Intu- StandardConv3D(7×3×3) 0.352 0.258 0.305
ProposedGCA(3×3×3) 0.347 0.227 0.287
itively, 3D convolutions with larger receptive fields in the
depthdimensioncanalleviatethecostinconsistencyinthe
Table 5. Evaluation of aggregation receptive fields. Directly
local range, by resorting to wider areas. Therefore, we
expandingreceptivefieldsalongthedepthdimensionyieldslim-
compareourapproachwithvariantsdirectlyexpandingthe
itedimprovementandiseasilysaturated. Incontrast,ourmethod
depthreceptivefield.Wekeepthe3×3spatialwindowsize
achievesthebestperformancewithakernelsizeof3.
at each 3D convolution layer and experiment with kernel
sizesof3, 5, and7inthedepthdimensiononthebaseline
Method Acc.↓ Comp.↓ Overall↓
method. The quantitative results are shown in Tab. 5, we
Ours+Depth-to-normal[19] 0.352 0.242 0.297
find that increasing the receptive field in the depth dimen-
Ours+Cost-to-normal[9] 0.358 0.241 0.300
sion leads to some certain improvement. However, due to Ours+Mono-normal[5] 0.347 0.227 0.287
the lack of geometric awareness, its performance is satu- Ours+GTnormal 0.275 0.221 0.248
ratedwhenthedimensionexpandstoacertainkernelsize.
In contrast, we use surface normal to geometrically guide Table 6. Evaluation of different normal cues. Our method
thecostaggregationprocess. Withakernelsizeofonly3, with GT normal demonstrates remarkable performance (0.248).
Amongallestimatednormals,theoff-the-shelfmonocularnormal
our method achieves the best performance, outperforming
hasthebestperformance.
otheralternativesbyclearmargins.
Evaluation of different normal cues. Since the surface
5.Conclusion
normal is important for guiding geometrically consistent
aggregation, we further evaluate the effectiveness of dif-
In this paper, we propose GoMVS, which aggregates lo-
ferent normal cues in Tab. 6. We first train and evalu-
callyconsistentgeometriestobetterutilizeadjacentgeom-
ate our method using the GT normal, which sets an upper
etry. By leveraging local smoothness in conjunction with
bound for our method. As shown inthe last row, it signif-
surface normal, we propose geometrically consistent ag-
icantly improves the performance of point clouds, validat-
gregation. it computes the correspondence from the adja-
ingourmethod’seffectivenesswhenusinghigh-qualitynor-
cent depth hypotheses space to the reference depth space
malinputs. Wefurthertrainandevaluateourmethodusing
and propagates cost accordingly. Furthermore, we investi-
depth-computed normals [19] or cost-computed normals
gatedifferentchoicesforgeneratingnormalpriorsandfind
[9],theresultsaresuboptimalastheyessentiallyrelyonthe
thatmonocularcueseffectivelycomplementtheMVSnet-
quality of input depth, which can degrade in challenging
work. Ourmethodachievesstate-of-the-artperformanceon
areas. Though lacking multi-view consistency, monocular
theDTU,TanksandTemples,andETH3Ddatasets.
normals do not collapse in challenging geometric estima-
Acknowledgements Y. Zhang was supported by
tionregionsofthecostvolume.Thisrevealsaniceproperty NSFC (No.U19B2037) and the Natural Science Ba-
formonocularestimations. InadditiontotheDTUdataset, sic Research Program of Shaanxi (No.2021JCW-
wealsoobservenotableimprovementusingmonocularsur- 03). Y. Zhu was supported by NSFC (No.61901384).
facenormalsonotherbenchmarks.
8References [13] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyra-
[1] HenrikAanæs,RasmusRamsbølJensen,GeorgeVogiatzis,
mid networks for object detection. In Proceedings of the
EnginTola,andAndersBjorholmDahl.Large-scaledatafor
IEEE conference on computer vision and pattern recogni-
multiple-viewstereopsis.InternationalJournalofComputer
tion,pages2117–2125,2017. 3
Vision,120:153–168,2016. 5,6
[14] TianqiLiu,XinyiYe,WeiyueZhao,ZhiyuPan,MinShi,and
[2] Chenjie Cao, Xinlin Ren, and Yanwei Fu. Mvsformer:
ZhiguoCao.Whenepipolarconstraintmeetsnon-localoper-
Learningrobustimagerepresentationsviatransformersand
atorsinmulti-viewstereo. InProceedingsoftheIEEE/CVF
temperature-based depth for multi-view stereo. arXiv
InternationalConferenceonComputerVision,pages18088–
preprintarXiv:2208.02541,2022. 1,2,6,7
18097,2023. 1,2,6,7
[3] ShuoCheng,ZexiangXu,ShilinZhu,ZhuwenLi,LiErran
[15] Xiaoxiao Long, Lingjie Liu, Christian Theobalt, and Wen-
Li,RaviRamamoorthi,andHaoSu.Deepstereousingadap-
pingWang. Occlusion-awaredepthestimationwithadaptive
tivethinvolumerepresentationwithuncertaintyawareness.
normalconstraints. InComputerVision–ECCV2020: 16th
In Proceedings of the IEEE/CVF Conference on Computer
European Conference, Glasgow, UK, August 23–28, 2020,
VisionandPatternRecognition,pages2524–2534,2020. 2
Proceedings,PartIX16,pages640–657.Springer,2020. 1,
[4] YikangDing, WentaoYuan, QingtianZhu, HaotianZhang, 2,3
Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvs- [16] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei
net: Global context-aware multi-view stereo network with Chen,andFanYu. Epp-mvsnet: Epipolar-assemblingbased
transformers. InProceedingsoftheIEEE/CVFConference depth prediction for multi-view stereo. In Proceedings of
on Computer Vision and Pattern Recognition, pages 8585– the IEEE/CVF International Conference on Computer Vi-
8594,2022. 2,6,7 sion,pages5732–5740,2021. 7,8
[5] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir [17] Zhenxing Mi, Chang Di, and Dan Xu. Generalized binary
Zamir. Omnidata: A scalable pipeline for making multi- searchnetworkforhighly-efficientmulti-viewstereo.InPro-
taskmid-levelvisiondatasetsfrom3dscans.InProceedings ceedingsoftheIEEE/CVFConferenceonComputerVision
oftheIEEE/CVFInternationalConferenceonComputerVi- andPatternRecognition,pages12991–13000,2022. 2,5,6,
sion,pages10786–10796,2021. 5,8 7,8
[6] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. [18] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and
Massively parallel multiview stereopsis by surface normal Ronggang Wang. Rethinking depth estimation for multi-
diffusion. InProceedingsoftheIEEEInternationalConfer- view stereo: A unified representation. In Proceedings of
enceonComputerVision,pages873–881,2015. 6 theIEEE/CVFConferenceonComputerVisionandPattern
[7] XiaodongGu,ZhiwenFan,SiyuZhu,ZuozhuoDai,Feitong Recognition,pages8645–8654,2022. 6,7
Tan,andPingTan. Cascadecostvolumeforhigh-resolution [19] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun,
multi-view stereo and stereo matching. In Proceedings of andJiaya Jia. Geonet: Geometricneuralnetwork forjoint
the IEEE/CVF conference on computer vision and pattern depthandsurfacenormalestimation. InProceedingsofthe
recognition,pages2495–2504,2020. 1,2,6,7 IEEEConferenceonComputerVisionandPatternRecogni-
[8] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen tion,pages283–291,2018. 2,5,7,8
Koltun.Tanksandtemples:Benchmarkinglarge-scalescene [20] ChunlinRen,QingshanXu,ShikunZhang,andJiaqiYang.
reconstruction. ACM Transactions on Graphics (ToG), 36 Hierarchicalpriorminingfornon-localmulti-viewstereo.In
(4):1–13,2017. 5 ProceedingsoftheIEEE/CVFInternationalConferenceon
[9] UdayKusupati,ShuoCheng,RuiChen,andHaoSu. Nor- ComputerVision,pages3611–3620,2023. 2
mal assisted stereo depth estimation. In Proceedings of [21] Johannes L Scho¨nberger, Enliang Zheng, Jan-Michael
theIEEE/CVFConferenceonComputerVisionandPattern Frahm, and Marc Pollefeys. Pixelwise view selection for
Recognition,pages2189–2199,2020. 1,2,3,5,8 unstructuredmulti-viewstereo. InComputerVision–ECCV
[10] JingliangLi,ZhengdaLu,YiqunWang,JunXiao,andYing 2016: 14thEuropeanConference,Amsterdam,TheNether-
Wang.Nr-mvsnet:Learningmulti-viewstereobasedonnor- lands,October11-14,2016,Proceedings,PartIII14,pages
mal consistency and depth refinement. IEEE Transactions 501–518.Springer,2016. 1,6,7,8
onImageProcessing,2023. 2 [22] ThomasScho¨ps,JohannesL.Scho¨nberger,SilvanoGalliani,
[11] Rui Li, Dong Gong, Wei Yin, Hao Chen, Yu Zhu, Kaix- TorstenSattler,KonradSchindler,MarcPollefeys,andAn-
uan Wang, Xiaozhi Chen, Jinqiu Sun, and Yanning Zhang. dreas Geiger. A multi-view stereo benchmark with high-
Learningtofusemonocularandmulti-viewcuesformulti- resolutionimagesandmulti-cameravideos. InConference
framedepthestimationindynamicscenes.InProceedingsof onComputerVisionandPatternRecognition(CVPR),2017.
theIEEE/CVFConferenceonComputerVisionandPattern 5
Recognition,pages21539–21548,2023. 1 [23] Wanjuan Su and Wenbing Tao. Efficient edge-preserving
[12] Jinli Liao, Yikang Ding, Yoli Shavit, Dihe Huang, Shihao multi-viewstereonetworkfordepthestimation. InProceed-
Ren, Jia Guo, Wensen Feng, and Kai Zhang. Wt-mvsnet: ingsoftheAAAIConferenceonArtificialIntelligence,pages
window-basedtransformersformulti-viewstereo.Advances 2348–2356,2023. 8
in Neural Information Processing Systems, 35:8564–8576, [24] Wei Tong, Xiaorong Guan, Jian Kang, Poly ZH Sun, Rob
2022. 2 Law,PedramGhamisi,andEdmondQWu. Normalassisted
9pixel-visibilitylearningwithcostaggregationformultiview stereo.InProceedingsoftheIEEE/CVFConferenceonCom-
stereo.IEEETransactionsonIntelligentTransportationSys- puter Vision and Pattern Recognition, pages 4877–4886,
tems,23(12):24686–24697,2022. 2,6 2020. 2
[25] FangjinhuaWang,SilvanoGalliani,ChristophVogel,Pablo [37] Jiayu Yang, Jose M Alvarez, and Miaomiao Liu. Non-
Speciale, and Marc Pollefeys. Patchmatchnet: Learned parametric depth distribution modelling based depth infer-
multi-view patchmatch stereo. In Proceedings of the enceformulti-viewstereo. InProceedingsoftheIEEE/CVF
IEEE/CVF Conference on Computer Vision and Pattern Conference on Computer Vision and Pattern Recognition,
Recognition,pages14194–14203,2021. 1,2,4,7 pages8626–8634,2022. 2,7
[26] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and [38] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan.
MarcPollefeys. Itermvs:Iterativeprobabilityestimationfor Mvsnet:Depthinferenceforunstructuredmulti-viewstereo.
efficientmulti-viewstereo.InProceedingsoftheIEEE/CVF InProceedingsoftheEuropeanconferenceoncomputervi-
conference on computer vision and pattern recognition, sion(ECCV),pages767–783,2018. 1,2,5,7
pages8606–8615,2022. 6,8 [39] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,
[27] XiaofengWang,ZhengZhu,GuanHuang,FangboQin,Yun and Long Quan. Recurrent mvsnet for high-resolution
Ye, YijiaHe, XuChi, andXingangWang. Mvster: epipo- multi-view stereo depth inference. In Proceedings of
lartransformerforefficientmulti-viewstereo. InComputer the IEEE/CVF conference on computer vision and pattern
Vision–ECCV2022:17thEuropeanConference,TelAviv,Is- recognition,pages5525–5534,2019. 2,5
rael, October23–27, 2022, Proceedings, PartXXXI,pages [40] YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,
573–591.Springer,2022. 2,6,8 LeiZhou,TianFang,andLongQuan.Blendedmvs:Alarge-
[28] Yun Wang, Longguang Wang, Hanyun Wang, and Yulan scaledatasetforgeneralizedmulti-viewstereonetworks. In
Guo. Spnet: Learning stereo matching with slanted plane ProceedingsoftheIEEE/CVFConferenceonComputerVi-
aggregation. IEEE Robotics and Automation Letters, 7(3): sionandPatternRecognition,pages1790–1799,2020. 5
6258–6265,2022. 2 [41] Xinyi Ye, Weiyue Zhao, Tianqi Liu, Zihao Huang, Zhiguo
[29] ZizhuangWei,QingtianZhu,ChenMin,YisongChen,and Cao, and Xin Li. Constraining depth map geometry for
GuopingWang.Aa-rmvsnet:Adaptiveaggregationrecurrent multi-view stereo: A dual-depth approach with saddle-
multi-viewstereonetwork.InProceedingsoftheIEEE/CVF shaped depth cells. In Proceedings of the IEEE/CVF In-
InternationalConferenceonComputerVision(ICCV),pages ternational Conference on Computer Vision (ICCV), pages
6187–6196,2021. 2,6 17661–17670,2023. 7
[30] Jiang Wu, Rui Li, Yu Zhu, Wenxun Zhao, Jinqiu Sun, and [42] WeiYin,YifanLiu,ChunhuaShen,andYouliangYan. En-
Yanning Zhang. Boosting multi-view stereo with late cost forcinggeometricconstraintsofvirtualnormalfordepthpre-
aggregation. arXivpreprintarXiv:2401.11751,2024. 1 diction.InProceedingsoftheIEEE/CVFInternationalCon-
[31] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggrega- ferenceonComputerVision,pages5684–5693,2019. 1,3
tionnetworkforefficientstereomatching.InProceedingsof [43] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
theIEEE/CVFConferenceonComputerVisionandPattern tler, and Andreas Geiger. Monosdf: Exploring monocu-
Recognition,pages1959–1968,2020. 1,2 lar geometric cues for neural implicit surface reconstruc-
tion. Advances in neural information processing systems,
[32] Qingshan Xu and Wenbing Tao. Multi-scale geometric
35:25018–25032,2022. 5
consistency guided multi-view stereo. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPattern [44] JingyangZhang,ShiweiLi,ZixinLuo,TianFang,andYao
Recognition,pages5483–5492,2019. 2,8 Yao. Vis-mvsnet: Visibility-aware multi-view stereo net-
work. International Journal of Computer Vision, 131(1):
[33] QingshanXuandWenbingTao. Planarpriorassistedpatch-
199–214,2023. 2,6,7,8
matchmulti-viewstereo. InProceedingsoftheAAAICon-
ferenceonArtificialIntelligence,pages12516–12523,2020. [45] YisuZhang,JiankeZhu,andLixiangLin. Multi-viewstereo
2 representationrevist:Region-awaremvsnet. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPat-
[34] Qingshan Xu, Wanjuan Su, Yuhang Qi, Wenbing Tao, and
ternRecognition,pages17376–17385,2023. 1,6,7
MarcPollefeys. Learninginversedepthregressionforpix-
elwisevisibility-awaremulti-viewstereonetworks. Interna- [46] ZheZhang,RuiPeng,YuxiHu,andRonggangWang. Ge-
tionalJournalofComputerVision,130(8):2040–2059,2022. omvsnet:Learningmulti-viewstereowithgeometrypercep-
2,8 tion. InProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition, pages21508–21518,
[35] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding,
2023. 1,2,6,7
Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing
Tai. Densehybridrecurrentmulti-viewstereonetwithdy-
namic consistency checking. In Computer Vision–ECCV
2020:16thEuropeanConference,Glasgow,UK,August23–
28, 2020, Proceedings, Part IV, pages 674–689. Springer,
2020. 2,6
[36] JiayuYang,WeiMao,JoseMAlvarez,andMiaomiaoLiu.
Costvolumepyramidbaseddepthinferenceformulti-view
10