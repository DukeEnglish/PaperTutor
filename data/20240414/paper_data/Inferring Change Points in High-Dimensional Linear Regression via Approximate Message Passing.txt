Inferring Change Points in High-Dimensional Linear Regression via
Approximate Message Passing
GabrielArpino1 XiaoqiLiu1 RamjiVenkataramanan1
Abstract modelshavefoundapplicationinarangeoffieldsincluding
genomics(Braunetal.,2000),neuroscience(Aston&Kirch,
We consider the problem of localizing change
2012),andeconomics(Andreou&Ghysels,2002).
pointsinhigh-dimensionallinearregression. We
proposeanApproximateMessagePassing(AMP) Inthispaper,weconsiderhigh-dimensionallinearregres-
algorithmforestimatingboththesignalsandthe sionwithchangepoints. Wearegivenasequenceofdata
changepointlocations. AssumingGaussianco- (y ,X )∈R×Rp,fori∈[n],fromthemodel
i i
variates,wegiveanexactasymptoticcharacteri-
zationofitsestimationperformanceinthelimit y =(X )⊤β(i)+ε , i=1,...,n. (1)
i i i
wherethenumberofsamplesgrowsproportion-
allytothesignaldimension. Ouralgorithmcan Here,β(i) ∈ Rp istheunknownregressionvectorforthe
be tailored to exploit any prior information on ithsample,X ∈ Rp isthe(known)covariatevector,and
i
the signal, noise, and change points. It also en- ε isadditivenoise. Wedenotetheunknownchangepoints,
i
ablesuncertaintyquantificationintheformofan i.e.,thesampleindiceswheretheregressionvectorchanges,
efficientlycomputableapproximateposteriordis- byη 1,...,η L∗−1. Specifically,wehave
tribution,whoseasymptoticformwecharacterize
exactly. We validate our theory via numerical 1=η 0 <η 1 <···<η L∗ =n,
experiments,anddemonstratethefavorableper-
formanceofourestimatorsonbothsyntheticdata withβ(i) ̸= β(i−1) ifandonlyifi ∈ {η ℓ}L ℓ=∗ 1−1. Wenote
andimages. that L∗ is the number of distinct signals in the sequence
{β(i)}n ,and(L∗−1)isthenumberofchangepoints.The
i=1
numberofchangepointsisnotknown,butanupperbound
1.Introduction LonthevalueofL∗isavailable. Thegoalistoestimatethe
changepointlocationsaswellastheL∗signals. Wewould
Heterogeneity is a common feature of large, high-
alsoliketoquantifytheuncertaintyintheseestimates,e.g.,
dimensionaldatasets. Whenthedataareorderedbytime,
viaconfidencesetsoraposteriordistribution.
asimpleformofheterogeneityisachangeinthedatagen-
eratingmechanismatcertainunknowninstantsoftime. If Linear regression with change points in the high-
these‘changepoints’wereknown,orestimatedaccurately,
dimensionalregime(wherethedimensionpiscomparable
thedatasetcouldbepartitionedintohomogeneoussubsets, to, or exceeds, the number of samples n) has been stud-
eachamenabletoanalysisviastandardstatisticaltechniques ied in a number of recent works, e.g. (Lee et al., 2016;
(Fryzlewicz,2014). Modelswithchangepointshavebeen Leonardi & Bu¨hlmann, 2016; Kaul et al., 2019; Rinaldo
studied in a variety of statistical contexts, such as the de- etal.,2021;Xuetal.,2022;Lietal.,2023;Bai&Safikhani,
tection of changes in: signal means (Wang & Samworth, 2023). Most of these papers consider the setting where
2018;Wangetal.,2020;Liuetal.,2021);covariancestruc- the signals are sparse (the number of non-zero entries in
tures(Cho&Fryzlewicz,2015;Wangetal.,2021b);graphs β(i) ∈ Rp iso(p)), andanalyzeproceduresthatcombine
(Londschien et al., 2023; Bhattacharjee et al., 2020; Fan theLASSOestimator(oravariant)withapartitioningtech-
& Guan, 2018); dynamic networks (Wang et al., 2021a); nique,e.g.,dynamicprogramming. TherecentworkofGao
andfunctionals(MadridPadillaetal.,2022). Changepoint &Wang(2022)assumessparsityonthedifferencebetween
signalsacrossachangepoint,andChoetal.(2024)consider
1Department of Engineering, University of Cambridge. generalnon-sparsesignals. Althoughexistingprocedures
Correspondence to: Gabriel Arpino <ga442@cam.ac.uk>,
forhigh-dimensionalchangepointregressionincorporate
Xiaoqi Liu <xl394@cam.ac.uk>, Ramji Venkataramanan
sparsity-basedconstraints,theycannotbeeasilyadaptedto
<rv285@cam.ac.uk>.
takeadvantageofotherkindsofsignalpriors. Moreover,
theyarenotwell-equippedtoexploitpriorinformationon
1
4202
rpA
11
]LM.tats[
1v46870.4042:viXraInferringChangePointsinHigh-DimensionalLinearRegression
thechangepointlocations. Bayesianapproachestochange et al., 2020). An attractive feature of AMP algorithms is
point detection have been studied in several works, e.g. thatundersuitablemodelassumptions,theirperformance
(Fearnhead,2006;Lunguetal.,2022),howevertheymainly inthehigh-dimensionallimitcanbecharacterizedbyasuc-
focuson(low-dimensional)time-series. cinct deterministic recursion called state evolution. The
stateevolutioncharacterizationhasbeenusedtoshowthat
AMPachievesBayes-optimalperformanceforsomemod-
Maincontributions Weproposean ApproximateMes-
els(Deshpande&Montanari,2014;Donohoetal.,2013;
sage Passing (AMP) algorithm for estimating the signals
{β(i)}n andthechangepointlocations{η }L∗−1. Under Barbieretal.,2019).
i=1 ℓ ℓ=1
theassumptionthatthecovariatesarei.i.d. Gaussian,we An important feature of our AMP algorithm, in contrast
give an exact characterization of the performance of the totheaboveworks,istheuseofnon-separabledenoising
algorithminthelimitasboththesignaldimensionpand functions. (We say a function g : Rm×L → Rm×L is
thenumberofsamplesngrow, withn/pconvergingtoa separable if it acts row-wise identically on the input ma-
constantδ(Theorem3.1). TheAMPalgorithmisiterative, trix.) Evenwithsimplifyingassumptionsonthesignaland
anddefinedviaapairof‘denoising’functionsforeachit- noise distributions, non-separable AMP denoisers are re-
eration. We show how these functions can be tailored to quired to handle the temporal dependence caused by the
takeadvantageofanypriorinformationonthesignalsand change points, and allow for precise uncertainty quantifi-
the change points (Proposition 3.2). We then show how cationaroundpossiblechangepointlocations. Ourmain
thechangepointscanbeestimatedusingtheiteratesofthe stateevolutionresult(Theorem3.1)leveragesrecentresults
AMPalgorithm,andhowtheuncertaintycanbequantified by Berthier et al. (2019) and Gerbelot & Berthier (2023)
viaanapproximateposteriordistribution(Section3.3). Our forAMPwithnon-separabledenoisers. Non-separablede-
theory enables asymptotic guarantees on the accuracy of noisers for AMP have been been studied for linear and
thechangepointestimatorandontheposteriordistribution generalizedlinearmodels,toexploitthedependencewithin
(Propositions3.3,3.4).InSection4,wepresentexperiments the signal (Som & Schniter, 2012; Metzler et al., 2016;
onbothsyntheticdataandimages,demonstratingthesupe- Ma et al., 2019) or between the covariates (Zhang et al.,
riorperformanceofAMPcomparedtootherstate-of-the-art 2023). Herenon-separabledenoisersarerequiredtoexploit
algorithmsforlinearregressionwithchangepoints. thedependenceintheobservations,causedbythechange
points.
Although our results do not explicitly need assumptions
ontheseparationbetweenadjacentchangepoints,theyare Althoughweassumei.i.d. Gaussiancovariates, basedon
most interesting when the separation is of order n, i.e., recent AMP universality results (Wang et al., 2022), we
∆ := min ℓ∈[L](η ℓ − η ℓ−1) = O(n). This separation is expect the results apply to a broad class of i.i.d. designs.
natural in our regime, where the number of samples n is An interesting direction for future work is to generalize
proportionaltopandthenumberofdegreesoffreedomin ourresultstorotationallyinvariantdesigns,amuchbroader
thesignalsalsogrowslinearlyinp. Existingresultsonhigh- classforwhichAMP-likealgorithmshavebeenproposed
dimensionalchangepointregressionusuallyassumesignals for regression without change points (Ma & Ping, 2017;
thatares-sparse,anddemonstratechangepointestimators Ranganetal.,2019;Takeuchi,2020;Panditetal.,2020).
thatareconsistentwhen∆=ω(cid:0) slogp/κ2(cid:1)
,whereκisa
constantdeterminedbytheseparationbetweenthesignals
2.Preliminaries
(Rinaldoetal.,2021;Wangetal.,2021c;Lietal.,2023). In
contrast,wedonotassumesignalsparsitythatissublinear Notation We let [n] = {1,2,...,n}. We use boldface
inn,sothechangepointestimationerrorwillnottendto notationformatricesandvectors. Forvectorsx,y ∈ Rn,
zero unless n/p → ∞. We therefore quantify the AMP wewritex≤yifx ≤y foralli∈[n],andlet[x,y]:=
i i
performanceviapreciseasymptoticsfortheestimationerror {v ∈ Rn : x ≤ v ≤ y ∀i ∈ [n]}. For a matrix A ∈
i i i
andtheapproximateposteriordistribution. Rm×Landi∈[m],j ∈[L],weletA andA denote
[i,:] [:,j]
its ith row and jth column respectively. Similarly, for a
vector ψ ∈ [L]n, we let A ∈ Rm denote the vector
Approximate Message Passing AMP, a family of it- [:,ψ]
whosei-thentryisA .
erative algorithms first proposed for linear regression i,ψi
(Kabashima, 2003; Donoho et al., 2009; Krzakala et al., Fortwosequences(inn)ofrandomvariablesX ,Y ,we
n n
2012),hasbeenappliedtoavarietyofhigh-dimensionales- P
writeX ≃Y whentheirdifferenceconvergesinprobabil-
timationproblemsincludingestimationingeneralizedlinear n n
ityto0,i.e.,lim P(|X −Y |>ϵ)=0foranyϵ>0.
models(Rangan,2011;Schniter&Rangan,2014;Maillard n→∞ n n
Denote the covariance matrix of random vector Z ∈ Rq
etal.,2020;Mondelli&Venkataramanan,2021)andlow-
asCov(Z) ∈ Rq×q. Werefertoallrandomelements,in-
rankmatrixestimation(Fletcher&Rangan,2018;Lesieur
cludingvectorsandmatrices,asrandomvariables. When
etal.,2017;Montanari&Venkataramanan,2021;Barbier
2InferringChangePointsinHigh-DimensionalLinearRegression
referring to probability densities, we include probability whereq actsrow-wiseonmatrixinputs. Wenotethatthe
massfunctions,withintegralsinterpretedassumswhenthe mixedlinearregressionmodel(Yietal.,2014;Zhangetal.,
distributionisdiscrete. 2022; Tan & Venkataramanan, 2023) can also be written
in theform in (2), with a crucial difference. In mixed re-
ModelAssumptions Inmodel(1),weassumeindepen- gression, the components of Ψ are assumed to be drawn
dent Gaussian covariate vectors X
i. ∼i.d
N(0,I /n) for
independentlyfromsomedistributionon[L],i.e.,eachy iis
i p
independentlygeneratedfromoneoftheLsignals. Inthe
i ∈ [n]. Weconsiderthehigh-dimensionalregimewhere
n,p→∞and n convergestoaconstantδ >0. Following changepointsetting,theentriesofΨaredependent,since
p theychangevalueonlyatentriesη ,...,η .
thechangepointliterature,weassumethenumberofchange 1 (L∗−1)
points(L∗−1)isfixedanddoesnotscalewithn,p.
AMPAlgorithm WenowdescribetheAMPalgorithmfor
estimatingBandη. Ineachiterationt≥1,thealgorithm
Pseudo-Lipschitz Functions Our results are
producesanupdatedestimateofthesignalmatrixB,which
stated in terms of uniformly pseudo-Lipschitz func-
we call Bt, and of the linearly transformed signal Θ :=
tions (Berthier et al., 2019). For C > 0 and
XB,whichwecallΘt. Theseestimateshavedistributions
r ∈ [1,∞), let PL (r,C) be the set of func-
n,m,q thatcanbedescribedbyadeterministiclow-dimensional
tions ϕ : Rn×q → Rm×q such that ∥ϕ(x) √−ϕ(x˜)∥F ≤
m matrixrecursioncalledstateevolution. InSection3.3,we
C(cid:18) 1+(cid:16)
∥
√x∥F(cid:17)r−1 +(cid:16)
∥
√x˜∥F(cid:17)r−1(cid:19)
∥x− √x˜∥F for all
showhowtheestimateΘtcanbecombinedwithytoinfer
n n n ηwithpreciselyquantifiableerror.
x,x˜ ∈ Rn×q. Note that ∪ PL (r ,C) ⊆
C>0 n,m,q 1 Starting with an initializer B0 ∈ Rp×L and defining
∪ PL (r ,C) for any 1 ≤ r ≤ r . A function
C>0 n,m,q 2 1 2 Rˆ−1 :=0 ,fort≥0thealgorithmcomputes:
ϕ ∈ PL (r,C) is called pseudo-Lipschitz of order n×L
n,m,q
r. A family of pseudo-Lipschitz functions is said to be Θt =XBˆt−Rˆt−1(Ft)⊤, Rˆt =gt(cid:0) Θt,y(cid:1) ,
uniformlypseudo-Lipschitzifallfunctionsofthefamilyare (3)
Bt+1 =X⊤Rˆt−Bˆt(Ct)⊤, Bˆt =ft(cid:0) Bt(cid:1) ,
pseudo-Lipschitzwiththesameorderrandthesamecon-
stantC.Forfunctionsf :Rn×m×Rn×M →Roftheform
wherethedenoisingfunctionsgt : Rn×L×Rn → Rn×L
f(a,b)=c,wesayf isuniformlypseudo-Lipschitzwith
andft :Rp×L →Rp×LareusedtodefinethematricesFt,
respecttoaifthefamily{f (·,b) : n ∈ N,b ∈ Rn×M}
n Ctasfollows:
isuniformlypseudo-Lipschitz. Forx,y ∈ Rn, themean
n p
squared error ϕ(x,y) = ⟨x − y,x − y⟩/n and the Ct = 1 (cid:88) ∂ gt(cid:0) Θt,y(cid:1) , Ft = 1 (cid:88) d ft(Bt).
normalized squared correlation ϕ(x,y) = |⟨x,y⟩|/n are n i i n j j
i=1 j=1
examplesofuniformlypseudo-Lipschitzfunctions.
Here∂ gt(Θ,y)istheL×LJacobianofgt w.r.t. theith
i i i
3.AMPAlgorithmandMainResults row of Θ. Similarly, d jf jt(Bt) is the L×L Jacobian of
ft withrespecttothej-throwofitsargument. Thetime
j
We stack the feature vectors, observations and noise ele- complexityofeachiterationin(3)isO(npL+r ),where
n
ments,respectively,toformX =[X 1,...,X n]⊤ ∈Rn×p, r nisthetimecomplexityofcomputingft,gt.
y :=[y ,...,y ]∈Rn andε:=[ε ,...,ε ]⊤ ∈Rn. Let
1 n 1 n Crucially, the denoising functions gt and ft are not re-
η :=[η ,...,η ,...,η ]bethevectorcontainingthetrue
1 L∗ L
strictedtobeingseparable. (Recallthataseparablefunction
changepoints,andB :=[β(η1),...,β(ηL∗),...,β(ηL)]∈
Rp×L bethematrixcontainingthetruesignals. Sincethe acts row-wise identically on its input, i.e., g : Rm×L →
Rm×LisseparableifforallU ∈Rm×Landi̸=j,wehave
algorithmassumesnoknowledgeofL∗,otherthanL∗ ≤L,
[g(U)] =g (U )=g (U ).) Hence,ingeneral,wehave
thecolumnsL∗+1,...,LofBcanallbetakentobezero. i i i j i
Similarly,η
L∗
=η
L∗+1
=···=η
L
=n.  gt(Θ,y)  ft(B)
1 1
Recallthatfori∈[n],eachobservationy isgeneratedfrom gt(Θ,y)= . . , ft(B)= . . .
i  .   . 
model(1). LetΨ∈[L]nbethesignalconfigurationvector, gt(Θ,y) ft(B)
n n
whosei-thentrystorestheindexofthesignalunderlying
observationy .Thatis,fori∈[n]andℓ∈[L],letΨ =ℓ Ournon-separableapproachisrequiredtohandlethetempo-
i i
ifandonlyifβ(i)equalstheℓthcolumnofthesignalmatrix raldependencecreatedbythechangepoints. Forexample,
B. Wenotethatthatthereisaone-to-onecorrespondence if there was one change point uniformly in distributed in
between Ψ and the change point vector η. We can then [n],thengt(Θ,y)shouldtakeintoaccountthaty iismore
rewriteyinamoregeneralform: likelytohavecomefromthethefirstsignalforicloseto1,
andfromthesecondsignalforicloseton. Thisisincon-
y =(XB) +ε:=q(XB,Ψ,ε)∈Rn, (2) trasttoexistingAMPalgorithmsformixedregression(Tan
[:,Ψ]
3InferringChangePointsinHigh-DimensionalLinearRegression
& Venkataramanan, 2023), where under standard model ε, andΨ. Thelimitsin(7)–(10)existundersuitablereg-
assumptions,itsufficestoconsiderseparabledenoisers. ularityconditionsonf ,g , andonthelimitingempirical
t t
distributionsonB,ε;seeAppendixB.Thedependenceon
In Appendix G, we review the AMP algorithm and state
B,εcanalsoberemovedundertheseconditions–thisis
evolutionforthegeneralizedlinearmodelwithoutchange
discussedinthenextsubsection.
points,i.e.,themodelin(2)withoutthevectorΨ. Thisis
usefulbackgroundforthestateevolutioncharacterization
3.1.StateEvolutionCharacterizationofAMPIterates
describedbelowforthechangepointsetting.
Recallthatthematrices(6)–(10)areusedtodefinetheran-
State Evolution The memory terms −Rˆt−1(Ft)⊤ and domvariables(Vt,Vt+1)in(4),(5). Throughthesequan-
Θ B
−Bˆt(Ct)⊤ in our AMP algorithm (3) debias the iterates tities,wenowgiveaprecisecharacterizationoftheAMP
Θt and Bt+1, and enable a succinct distributional char- iterates (Θt, Bt+1) in the high-dimensional limit. Theo-
acterization. In the high-dimensional limit as n,p → ∞ rem3.1belowshowsthatanyreal-valuedpseudo-Lipschitz
(withn/p→δ),theempiricaldistributionsofΘtandBt+1 functionof(Θt,Bt+1)convergesinprobabilitytoitsex-
arequantifiedthroughtherandomvariablesVt andVt+1 pectationunderthelimitingrandomvariables(Vt,Vt+1).
Θ B Θ B
respectively,where InadditiontothemodelassumptionsinSection2,wemake
thefollowingassumptions:
Vt :=Zρ−1νt +Gt ∈Rn×L, (4)
Θ Θ Θ
Vt+1 :=Bνt+1+Gt+1 ∈Rp×L. (5)
B B B (A1) The following limits exist and are finite almost
√
The matrices ρ,νt ,νt+1 ∈ RL×L are deterministic and surely: lim p→∞∥B √0∥ F/ p, lim p→∞∥B⊤B∥ F/p,
definedbelow. TheΘ randB ommatricesZ,Gt ,andGt+1are andlim n→∞∥ε∥ 2/ n.
Θ B
independentofX,andhavei.i.d.rowsfollowingaGaussian (A2) For each t ≥ 0, let g˜t : (u,z) (cid:55)→ gt(u,q(z,Ψ,ε)),
distribution. Namely,fori ∈ [n]wehaveZ ii. ∼i.d N(0,ρ). where(u,z)∈Rn×L×Rn×L. Foreachi∈[n],j ∈
For i ∈ [n],j ∈ [p] and s,r ≥ 0, (Gt ) i. ∼i.d N(0,κt,t) [p], the following functions are uniformly pseudo-
Θ i Θ
with Cov((Gr ) ,(Gs ) ) = κr,s. Similarly, (Gt ) i. ∼i.d Lipschitz: ft,g˜t,d jf jt,∂ 1ig˜ it.
Θ i Θ i Θ B j
N(0,κt B,t)withCov((Gr B) j,(Gs B) j) = κr B,s. TheL×L (A3) Fors,t≥0,thelimitsin(6)–(10)existandarefinite.
deterministicmatricesνt ,κr,s,νt ,andκr,s aredefined
Θ Θ B B
belowviathestateevolutionrecursion.
Assumptions(A1)−(A3)arenaturalextensionsofclassical
GiventheinitializerB0forouralgorithm(3),weinitialize AMPresultswhichassumeseparablesignalsanddenoising
stateevolutionbysettingν0 :=0,and functions. They are similar to those required by existing
Θ
worksonnon-separableAMP(Berthieretal.,2019;Gerbe-
1 1 1 1
ρ:= lim B⊤B, κ0,0 := lim f0(B0)⊤f0(B0). lot&Berthier,2023),andgeneralizethesetothemodel(2),
δ p→∞p Θ δ p→∞p withamatrixsignalBandanauxiliaryvectorΨ∈[L]n.
(6)
Theorem 3.1. Consider the AMP in (3) for the Change
Letg˜ it(Z,V Θt,Ψ,ε) := g it(V Θt,q(Z,Ψ,ε))andlet∂ 1ig˜ it PointRegressionmodelin(2),withthemodelassumptions
bethepartialderivative(Jacobian)w.r.t. theithrowofthe in Section 2 as well as (A1) − (A3). Then for t ≥ 0
firstargument.Then,thestateevolutionmatricesaredefined andanysequenceofuniformlypseudo-Lipschitzfunctions
recursivelyasfollows: φ :Rn×(L(t+1)+2) →Randφ :Rp×(L(t+2)) →R,
n p
(cid:34) n (cid:35)
νt+1 := lim 1 E (cid:88) ∂ g˜t(Z,Vt,Ψ,ε) , (7) φ n(Θ0,...,Θt,y,Ψ)
B n→∞n 1i i Θ
P
i=1 ≃E {φ (V0,...,Vt,q(Z,Ψ,ε),Ψ)},
κs+1,t+1 := lim
1 E(cid:104)
gs(Vs,q(Z,Ψ,ε))⊤
V Θ0,...,V Θt,Z n Θ Θ
(11)
B n→∞n Θ
gt(cid:0) Vt,q(Z,Ψ,ε)(cid:1)(cid:3)
, (8)
φ p(B1,...,Bt+1,B)
Θ
νt+1 := 1 lim 1 E(cid:2) B⊤ft+1(Vt+1)(cid:3) , (9) ≃P E V1,...,Vt+1{φ p(V B1,...,V Bt+1,B)}, (12)
Θ δ p→∞p B B B
κs+1,t+1 := 1 lim 1 E(cid:104)(cid:0) fs+1(Vs+1)−Bρ−1νs+1(cid:1)⊤ asn,p → ∞withn/p → δ,wheretherandomvariables
Θ δ p→∞p B Θ Z,Vt andVt+1aredefinedin(4),(5).
Θ B
(cid:0) ft+1(Vt+1)−Bρ−1νt+1(cid:1)(cid:3)
. (10)
B Θ TheproofofthetheoremisgiveninAppendixA.Itinvolves
The expectations above are taken with respect to reducingtheAMPin(3)toavariantofthesymmetricAMP
Z,Vt,Vs,Vt+1 and Vs+1, and depend on gt, ft, B, iterationanalyzedin(Gerbelot&Berthier,2023,Lemma
Θ Θ B B
4InferringChangePointsinHigh-DimensionalLinearRegression
14). Weuseageneralizedversionoftheiriterationwhich (Vt,Vt+1)definedin(4)–(5),wedefinethematrices
Θ B
allowsfortheauxiliaryquantitiesΨ,q(Z,Ψ,ε)tobein-
cluded. Theorem 3.1 implies that any pseudo-Lipschitz
function φ of (Θt,y) will converge in probability to a
n Z˜t :=Vt(ρ−1νt )−1 =Z+Gt (ρ−1νt )−1, (13)
quantityinvolvinganexpectationover(Vt,Z). Ananalo- Θ Θ Θ Θ
Θ
gousstatementholdsfor(Bt+1,B). B˜t+1 :=Vt+1(νt+1)−1 =B+Gt+1(νt+1)−1, (14)
B B B B
Useful choices of φ ,φ Using Theorem 3.1, we can
n p
evaluate performance metrics such as the mean squared wherefori∈[n],j ∈[p]wehaveZ i. ∼i.d N(0,ρ),Gt i. ∼i.d
errorbetweenthesignalmatrixB andtheestimateBˆt = i Θ,i
ft(Bt). Takingφ p(Bt,B)=∥ft(Bt)−B∥2 F/pleadsto N(0,κt Θ,t),andGt B,ji. ∼i.d N(0,κt B,t). (Iftheinversedoesn’t
P exist we post-multiply by the pseudo-inverse). A natural
∥ft(Bt)−B∥2/p ≃ E[∥ft(Vt)−B∥2/p], where the
F B F objectiveistominimizethetraceofthecovarianceofthe
RHScanbepreciselycomputedundersuitableassumptions.
“noise”matricesin(13)and(14),givenby
In Section 3.3, we will choose φ to capture metrics of
n
interestforestimatingchangepoints.
(cid:32) n (cid:33)
SpecialCases Theorem3.1recoverstwoknownspecial Tr 1 (cid:88) Cov(cid:16) Z˜t−Z (cid:17)
n i i
cases of (separable) AMP results, with complete conver-
i=1
gence replaced by convergence in probability: linear re- =Tr(cid:16)(cid:0) (νt )−1ρ(cid:1)⊤ κt,t(νt )−1ρ(cid:17) , (15)
gressionwhenL=1(Fengetal.,2022),andmixedlinear Θ Θ Θ
r the egr re os wsi so on fw Bh ,e Ψre ,εL c> on1 vea rn gd ewth ee akem lyp toiri lc awal sd oi fst wri eb lu l-t dio en fis neo df Tr(cid:32) p1(cid:88)p Cov(cid:16)
B˜ it+1−B
i(cid:17)(cid:33)
randomvariables(Tan&Venkataramanan,2023). j=1
(cid:16) (cid:17)
=Tr [(νt+1)−1]⊤κt+1,t+1(νt+1)−1 , (16)
B B B
DependenceofStateEvolutiononB,ε,Ψ Thedepen-
denceofthestateevolutionparametersin(7)-(10)onB,ε
canberemovedunderreasonableassumptions. Astandard
wherewerecallfrom(7)–(10)thatνt ,κt,taredefinedby
assumptionintheAMPliterature(Fengetal.,2022)is: Θ Θ
ft,andνt+1,κt+1,t+1aredefinedbygt.
Θ Θ
(S0) Asn,p→∞,theempiricaldistributionsof{B } Fort≥1,wewouldliketoiterativelychoosethedenoisers
j j∈[p]
a ren sd pe{ cε ti iv} ei∈ ly[n ,] wc ito hn bv oer ug ne dew de sa ek cl oy ndto mla ow ms enP tsB¯
.
and P ε¯, (f 1t 6, )g .t Hto owm evin ei rm ,ai sze dit sh ce usq su ea dn at bit oie vs eνon Θt ,th κe t Θ,R t,H νS Θt+o 1f ,κ(1 t Θ5 +) 1,a t+nd 1
depend on the unknown Ψ, so any denoiser construction
basedquantitiescannotbeexecutedinpractice.
InAppendixB,wegiveconditionsonf ,g ,whichtogether
t t
with(S0),allowthestateevolutionequationstobesimpli-
fiedandwrittenintermsofB¯ ∼ P B¯ andε¯∼ P ε¯instead
of(B,ε). Webelievethatthedependenceofthestateevo- EnsembleStateEvolution Toremovethedependenceon
lutiononthesignalconfigurationvectorΨisfundamental. Ψinthestateevolutionequations,wecanpostulaterandom-
SincetheentriesofΨchangevalueonlyatafinitenumber ness over this variable and take expectations accordingly.
ofchangepoints,thestateevolutionparameterswilldepend Indeed,assumethat(S0)holds,andpostulateapriordistri-
on the limiting fractional values of these change points; butionπ Ψ¯ over[L]n. Forexample,π Ψ¯ maybetheuniform
see(S1)inAppendixB.Thisisalsoconsistentwithrecent distribution over all the signal configuration vectors with
changepointregressionliterature,wherethelimitingdistri- twochangepointsthatareatleastn/10apart. Weempha-
butionofthechangepointestimatorsin(Xuetal.,2022)is sizethatourtheoreticalresultsdonotassumethatthetrue
showntobeafunctionofthedatageneratingmechanism. Ψisdrawnaccordingtoπ Ψ¯. Rather,thepriorπ Ψ¯ allows
ustoencodeanyknowledgewemayhaveaboutthechange
3.2.ChoosingtheDenoisingFunctionsft,gt pointlocations,anduseittodefineefficientAMPdenoisers
ft,gt. Thisisdoneviathefollowingensemblestateevolu-
TheperformanceoftheAMPalgorithm(3)isdetermined
tionrecursion,definedintermsoftheindependentrandom
by the functions {ft+1,gt} . We now describe how
these functions can be
chost e≥ n0
based on the state evolu-
variablesΨ¯ ∼P Ψ¯,B¯ ∼P B¯,andε¯∼P ε¯.
tionrecursiontomaximizeestimationperformance. Using Starting with initialization ν¯0 := 0, κ¯0,0 :=
Θ Θ
5InferringChangePointsinHigh-DimensionalLinearRegression
lim 1 f0(B0)⊤f0(B0),fort≥0define: The proof of Proposition 3.2, given in Appendix C.1, is
p→∞ δp
similartothederivationoftheoptimaldenoisersformixed
n
ν¯t+1 := lim 1 (cid:88) E(cid:2) ∂ g˜t(Z ,(V¯t) ,Ψ¯ ,ε¯)(cid:3) (17) regressionin(Tan&Venkataramanan,2023),withafewkey
B n→∞n 1 i 1 Θ 1 i differencesinthederivationofg∗t,whichisnotseparable
i=1
inthechangepointsetting. Withaproductdistributionon
n
κ¯t B+1,t+1 := nl →im ∞n1 (cid:88) E(cid:2) g it(cid:0) (V¯ Θt) 1,q(Z 1,Ψ¯ i,ε¯)(cid:1) π thΨ¯ e, ow pte imre ac lo dv ee nr om iseix rsed inre (Tgr ae nss &io Vn ea nn kd at( a2 r5 am)– a(2 n6 an) ,re 2d 0u 2c 3e ).to
i=1
gt(cid:0) (V¯t) ,q(Z ,Ψ¯ ,ε¯)(cid:1)⊤(cid:105) , (18) Thedenoiserf∗t isseparableandcanbeeasilycomputed
i Θ 1 1 i
p for sufficiently regular distributions P B¯ such as discrete,
ν¯t+1 := 1 lim 1(cid:88) E(cid:2) B¯ft+1(V¯t+1)⊤(cid:3) , (19) Gaussian,orBernoulli-Gaussiandistributions. InAppendix
Θ δ p→∞p j B C.2,weshowhowg∗tcanalsobeefficientlycomputedfor
j=1
Gaussianε¯. AsdetailedinAppendixF,f∗tandg∗tcanbe
p
κ¯t+1,t+1 := 1 lim 1(cid:88) computedinO(nL3)time,yieldingatotalcomputational
Θ δ p→∞p complexityofO(npL3)forAMPwiththesedenoisers.
j=1
E(cid:2)(cid:0) ft+1(V¯t+1)−(ν¯t+1)⊤ρ−1B¯(cid:1)
j B Θ 3.3.ChangePointEstimationandInference
(cid:0) ft+1(V¯t+1)−(ν¯t+1)⊤ρ−1B¯(cid:1)⊤(cid:105)
, (20)
j B Θ WenowshowhowtheAMPalgorithmcanbeusedforesti-
mationandinferenceofthechangepoints{η ,...,η }.
where 1 L∗−1
We first define some notation. Let X ⊂ [L]n be the set
V¯ Θt :=Zρ−1ν¯ Θt +G¯t Θ ∈Rn×L, (21) of all piece-wise constant vectors with respect to i ∈ [n]
V¯t+1 :=(ν¯t+1)⊤B¯ +(G¯t+1) ∈RL, (22) withatmost(L−1)jumps. Thissetincludesallpossible
B B B 1
instancesofthesignalconfigurationvectorΨ. Letthefunc-
and for i ∈ [n],j ∈ [p] we have that Z i. ∼i.d N(0,ρ), tionU : η (cid:55)→ Ψdenotetheone-to-onemappingbetween
i
G¯t i. ∼i.d N(0,κ¯t,t)andG¯t i. ∼i.d N(0,κ¯t,t). changepointvectorsηandsignalconfigurationvectorsΨ.
Θ,i Θ B,j B Foravectorηˆ,welet|ηˆ|denoteitsdimension(numberof
Whenπ Ψ¯ isaunitmassonthetrueconfigurationΨ,(17)– elements).
(20) reduce to the simplified state evolution in Appendix
B. The limits in (17)–(20) exist under suitable regularity Change Point Estimation Theorem 3.1 states that
conditionsonft,gt,suchasthoseinAppendixB. (Θt,y) converges in a specific sense to the random vari-
Wenowproposeaconstructionofft,gtbasedonminimiz- ables (V Θt,q(Z,Ψ,ε)) = (Zρ−1ν Θt +Gt Θ,Z [:,Ψ] +ε),
whosedistributioncruciallycapturesinformationaboutΨ.
ingthefollowingalternativeobjectivesto(15)–(16):
Hence,itisnaturaltoconsiderestimatorsforηoftheform
Tr(cid:16)(cid:0) (ν¯t )−1ρ(cid:1)⊤ κ¯t,t(ν¯t )−1ρ(cid:17) , (23) ηˆ(Θt,y),oneexamplebeinganestimatorthatsearchesfor
Θ Θ Θ
asignalconfigurationvectorψ ∈X suchthatΘtindexed
(cid:16) (cid:17)
Tr [(ν¯t+1)−1]⊤κ¯t+1,t+1(ν¯t+1)−1 , (24) alongψhasthelargestcorrelationwiththeobservedvector
B B B
y. Thatis,
where the deterministic matrices ν¯t , κ¯t,t, ν¯t+1,
Θ Θ B n
κ¯t B+1,t+1 ∈RL×Laredefinedin(17)–(20). Ψˆ(Θt,y)=argmax (cid:88) (Θt) ·y , (27)
i,ψi i
Proposition3.2. Assumethelimitsin(17)–(20)exist. Then, ψ∈X
i=1
fort≥1:
and ηˆ(Θt,y) = U−1(Ψˆ(Θt,y)). A common metric for
1. Givenν¯t ,κ¯t,t,thequantity(23)isminimizedwhen evaluating the accuracy of change point estimators is the
B B
Hausdorff distance (Wang & Samworth, 2018; Xu et al.,
ft(U)=f∗t(U):=E[B¯|V¯t =U ], (25)
j j B j 2022;Lietal.,2023). TheHausdorffdistancebetweentwo
forU ∈Rp×L,j ∈[p]. non-emptysubsetsX,Y ofRis
(cid:40) (cid:41)
2. Givenν¯t ,κ¯t,t,thequantity(24)isminimizedwhen
Θ Θ d (X,Y)=max supd(x,Y), supd(X,y) ,
H
gt(V,u)=g∗t(V,u):=(cid:2) Cov(cid:0) Z |V¯t (cid:1)(cid:3)−1 · x∈X y∈Y
i i 1 Θ,1
(cid:0) E[Z |(V¯t) =V ,q(Z ,Ψ¯ ,ε¯)=u ] where d(x,Y) := min ∥x−y∥ . The Hausdorff dis-
1 Θ 1 i 1 i i y∈Y 2
−E[Z |V¯t =V ](cid:1) , (26) tance is a metric, and can be viewed as the largest of all
1 Θ,1 1
distances from a point in X to its closest point in Y and
forV ∈Rn×L,u∈Rn,i∈[n]. viceversa. WeinterprettheHausdorffdistancebetweenη
6InferringChangePointsinHigh-DimensionalLinearRegression
and an estimate ηˆ as the Hausdorff distance between the Theproof, giveninAppendixD.2, isadirectapplication
setsformedbytheirelements. Thefollowingtheoremstates of Theorem 3.1. The state evolution predictions on the
thatanywell-behavedestimatorηˆproducedusingtheAMP RHS of (28) and (31) can be computed under reasonable
iterateΘt admitsapreciseasymptoticcharacterizationin assumptions,asoutlinedinAppendixB.
termsofHausdorffdistanceandsize.
Proposition 3.3. Consider the AMP in (3). Suppose the 4.Experiments
model assumptions in Section 2 as well as (A1)−(A3)
are satisfied. Let ηˆ(Θt,y) be an estimator such that Inthissection,wedemonstratetheestimationandinference
U(ηˆ(Θt,y))isuniformlypseudo-Lipschitz. Then: capabilitiesoftheAMPalgorithminarangeofsettings. Af-
terrunningAMP,thesignalconfigurationisestimatedusing
d H(η,ηˆ(Θt,y)) ≃P
E
d H(η,ηˆ(V Θt,q(Z,U(η),ε))) theapproximateMaximumaPosteriori(MAP)estimate:
n V Θt,Z n
(28)
Ψˆ =argmax p Ψ|V¯t,q(Z,Ψ,ε¯)(ψ|Θt,y).
ψ∈X Θ
Moreover,ifthenthcomponentofU(ηˆ(Θt,y))isuniformly andηˆ(Θt,y) = U−1(Ψˆ(Θt,y)). APythonimplementa-
pseudo-Lipschitz,then: tion of our algorithm and code to run the experiments is
availableat(Arpino&Liu,2024).
P
|(L∗−1)−|ηˆ(Θt,y)||≃
For all experiments, we use i.i.d. Gaussian covariates:
E Vt,Z(cid:12) (cid:12)(L∗−1)−|ηˆ(V Θt,q(Z,U(η),ε))|(cid:12) (cid:12). (29)
X
i. ∼i.d
N(0,I /n)fori∈[n],andstudytheperformance
Θ i p
on both synthetic and image signals. For synthetic data,
The proof is given in Appendix D.1. The proof of (28) thenoisedistributionischosentobeP = N(0,σ2). The
ε¯
involves showing that d H(η,ηˆ(Θt,y))/n is uniformly denoisers{gt,ft+1} t≥0intheAMPalgorithmarechosen
pseudo-Lipschitz. For each η, Proposition 3.3 precisely accordingtoProposition3.2,unlessotherwisestated. We
characterizes the asymptotic Hausdorff distance and size useauniformpriorπ Ψ¯ overallconfigurationswithchange
errorsforalargeclassofestimatorsηˆ(Θt,y). pointsatleast∆apart,forsome∆thatisafractionofn.
Errorbarsrepresent±onestandarddeviation. Fullimple-
UncertaintyQuantification TherandomvariableV¯t = mentationdetailsareprovidedinAppendixE.
Θ
Zρ−1ν¯t + G¯t in (21), combined with an observation
Θ Θ Figure1plotstheHausdorffdistancenormalizedbynfor
of the form q(Z,Ψ,ε¯) = Z +ε¯, yields a recipe for
[:,Ψ] varyingδ,fortwodifferentchangepointconfigurationsΨ.
constructingaposteriordistributionoverΨ. Usingtheprior
π Ψ¯,theposterioris: W fixe tc who oo ts re uep c= ha6 n0 g0 e,P poB¯ in= ts,N w(0 h, oI se), lσ oc= ati0 o. n1 s,∆ are= inn d/ ic5 aa tn edd
p Ψ|V¯ Θt,q(Z,Ψ,ε¯)(ψ|V,u)=
(cid:80)π ψ˜Ψ¯ π( Ψ¯ψ () ψL ˜)( LV (, Vu ,| uψ |)
ψ˜),
i en voth lue til oe nge pn rd ed. iT cth ioe nal og fo Hri ath um sdou rs fe fs dL ist= ancL e∗ c= los3 e. lyT mhe ats ct ha ete
s
theperformanceofAMP,verifying(28)inProposition3.3.
(30)
Figure 2 shows the approximate posterior on the change
w ofh (e Vr ¯e ΘtV ,q∈ (ZR ,n ψ× ,L ε¯, )u )g∈ ivR enn, ψan ∈dL X.re Upr ne ds ee rnt ts ht eh ae sl sik ue mli ph to ioo nd Wpo ein ot bl so ec ra vt eio sn trs o, nc gom agp ru et ee md eu ns tin wg ithp Ψ th|V e¯ Θt s, tq a( tZ e, eΨ v, oε¯ l) u(· t| iΘ ont, py re) -.
thatε¯ i. ∼i.d N(0,σ2)forsomeσ >0,thelikelihoodcanbe diction, validating Proposition 3.4. The experiment uses
i
computedinclosedformusingtheGaussianlikelihoodL 1 p = 400, P B¯ = N(0,I), σ = 0.1, ∆ = n/5, the true
in(91),withL(V,u|ψ)=(cid:81)n L (V ,u |ψ ). changepointsareatn/3and8n/15(i.e.,L∗ =3),andthe
i=1 1 i i i
algorithmusesL=3. Asδincreases,weobservethatthe
SinceTheorem3.1statesthat(Θt,y)convergesinaspe-
approximateposteriorconcentratesaroundthegroundtruth.
cificsenseto(Vt,q(Z,Ψ,ε)),wecanobtainanapproxi-
Θ
mateposteriordensityoverΨbypluggingin(Θt,y)for Figure3showstheapproximateposterioronthenumberof
(V,u)in(30).Ourtheorythenallowsustocharacterizethe changepoints,i.e.,(cid:80) ψ∈Sp Ψ|V¯t,q(Z,Ψ,ε¯)(ψ|Θt,y)where
Θ
asymptoticformofthisapproximateposteriordistribution. S contains all configurations with a specified number of
Proposition3.4. SupposethemodelassumptionsinSection change points. We use p = 200,P B¯ = N(0,I),σ =
0.1,∆ = n/10,L∗ = 3,L = 4,andauniformpriorover
2 as well as (A1)−(A3) are satisfied. Assume that the
posteriordensityp(ψ|Θt,y):=p Ψ|V¯t,q(Z,Ψ,ε¯)(ψ|Θt,y) the numberof change points (zeroto three). We observe
Θ thattheposteriorconcentratesaroundthegroundtruthfor
in(30)isuniformlypseudo-LipschitzwithrespecttoΘt,y,
moderatelylargeδ.
foreachψ ∈X. Then,forψ ∈X,wehavethat:
Figures4and5comparetheperformanceofAMPagainst
p(ψ|Θt,y)≃P
E
(cid:2) p(ψ|Vt,q(Z,Ψ,ε))(cid:3)
. (31) fourstate-of-the-artalgorithms: thedynamicprogramming
Vt,Z Θ
Θ
7InferringChangePointsinHigh-DimensionalLinearRegression
AMP [0.2n, 0.8n]
1.0
Theory
0.1 AMP [0.3n, 0.7n]
Theory 0.5
0.0
AMP, =0.2
0.0
0.5 1.0 Charcoal, =0.2
AMP, =0.3
= n/p 1000 Charcoal, =0.3
AMP, =0.5
Charcoal, =0.5
Figure1.Hausdorfferrorvs.δ=n/pfortwoconfigurations.
0
0.5 1.0 1.5 2.0 2.5 3.0 3.5
= n/p
0.15
=0.5 True change points
Theory
AMP Figure5.Comparisonwithcharcoalforasparsedifferenceprior
withsparsitylevel0.5.L∗ =L=3.
0.00
0.15
=1.2
calledcharcoal(Gao&Wang,2022). Hyperparameters
arechosenusingcrossvalidation(CV),asoutlinedinSec-
tionA.1of(Lietal.,2023). Thefirstthreealgorithms,de-
0.00
0.1n 0.5n 0.9n signedforsparsesignals,combineLASSO-typeestimators
with partitioning techniques based on dynamic program-
Figure2.Approximateposteriorvs.changepointlocations.
ming. Thecharcoalalgorithmisdesignedforthesetting
wherethedifferenceβ(ηℓ)−β(ηℓ+1)betweenadjacentsig-
1.0 nals is sparse. None of these algorithms uses a prior on
AMP
Theory thechangepointlocations,unlikeAMPwhichcanflexibly
0.5 incorporatebothpriorsviaP B¯ andπ Ψ¯.
=1.21 =2.07
0.0 Figure 4 uses p = 200,σ = 0.1,∆ = n/10,L∗ =
0 1 2 3 0 1 2 3
L = 3 and a sparse Bernoulli-Gaussian signal prior
Figure3.Approximateposterioronthenumberofchangepoints
P B¯ = 0.5N(0,δI) + 0.5δ 0. AMP assumes no knowl-
edge of the true sparsity level 0.5 and estimates the spar-
(L∗−1).L∗ =3,L=4.
sity level using CV over a set of values not including
the ground truth (details in Appendix E). Figure 5 uses
p = 300,∆ = n/10,L∗ = L = 3andaGaussiansparse
0.50 differencepriorwithsparsitylevel0.5(describedin(83)-
(85)). AMPisrunassumingamismatchedsparsitylevelof
0.25
0.9andamismatchedmagnitudeforthesparsedifference
0.00 vector(detailsinAppendixE).Figures4and5showthat
AMPconsistentlyachievesthelowestHausdorffdistance
AMP
amongallalgorithmsandoutperformsmostalgorithmsin
DPDU
100 DCDP runtime. Figures7and8inAppendixEshowresultsfrom
DP anadditionalsetofexperimentscomparingAMPwithDCDP
(thefastestalgorithminFigure4). Figure7showstheper-
0
0.5 1.0 1.5 =2. 0
n/p
2.5 3.0 3.5 formance of AMP with different change point priors π Ψ¯
and suboptimal denoising functions, e.g., soft threshold-
ing. Figure8demonstratesthefavourableruntimescaling
Figure4.Comparison with DPDU, DCDP and DP for sparse prior
of AMP over DCDP with respect to p, due to the LASSO
P B¯ =0.5N(0,δI)+0.5δ 0.L∗ =L=3.Runtimeshownisthe
computationsinvolvedinDCDP.
averageruntimepersetofCVparameters.
Compressed Sensing with change points In Figure 6,
(DP)approachin(Rinaldoetal.,2021);dynamicprogram- weconsidernoiselesscompressedsensing,wherethesig-
ming with dynamic updates (DPDU) (Xu et al., 2022); di- nals{β(i)} arerotatedversionsofa(255,255)sparse
i∈[n]
videandconquerdynamicprogramming(DCDP)(Lietal., grayscaleimageusedbySchniter&Rangan(2014). The
2023); and a complementary-sketching-based algorithm fractionofnonzerocomponentsintheimageis8645/50625.
8
ecnatsid
ffrodsuaH
)s(
emitnuR
ecnatsid
ffrodsuaH
)s(
emitnuR
ecnatsid
ffrodsuaHInferringChangePointsinHigh-DimensionalLinearRegression
2024.
Rotated 0 Rotated 30 Rotated 45
Aston, J.A.D.andKirch, C. Evaluatingstationarityvia
change-pointalternativeswithapplicationstofmridata.
TheAnnalsofAppliedStatistics,2012.
Bai,Y.andSafikhani,A. Aunifiedframeworkforchange
pointdetectioninhigh-dimensionallinearmodels. Statis-
ticaSinica,2023.
Barbier,J.,Krzakala,F.,Macris,N.,Miolane,L.,andZde-
borova´,L. Optimalerrorsandphasetransitionsinhigh-
dimensionalgeneralizedlinearmodels. Proceedingsof
theNationalAcademyofSciences,116(12):5451–5460,
=0.25 2019.
True change points
0.1 Approximate posterior
Barbier,J.,Macris,N.,andRush,C. All-or-nothingstatisti-
0.0 calandcomputationalphasetransitionsinsparsespiked
=0.50 matrix estimation. In Neural Information Processing
0.1 Systems(NeurIPS),2020.
0.0 Bayati, M.andMontanari, A. Thedynamicsofmessage
=0.75
passingondensegraphs,withapplicationstocompressed
0.1
sensing. IEEETransactionsonInformationTheory,57:
764–785,2011.
0.0
0.2n 0.5n 0.9n
Berthier,R.,Montanari,A.,andNguyen,P.-M. Stateevolu-
Figure6.Top:Groundtruthimages(firstrow)andreconstruction tionforapproximatemessagepassingwithnon-separable
from AMP (second row) at δ = 0.75. Bottom: Approximate functions. InformationandInference: AJournalofthe
posteriorvs.fractionalchangepointlocationsfordifferentδ. IMA,2019.
Bhattacharjee, M., Banerjee, M., and Michailidis, G.
Wedownsampletheoriginalimagebyafactorofthreeand ChangePointEstimationinaDynamicStochasticBlock
flatten,yieldinganoperationdimensionofp=852 =7225. Model. JournalofMachineLearningResearch,2020.
We set {β(i)}0.3n to be the image, {β(i)}0.7n to be a
i=1 i=0.3n
30◦ rotated version, and {β(i)}n to be a 45◦ rotated Bradbury,J.,Frostig,R.,Hawkins,P.,Johnson,M.J.,Leary,
i=0.7n
version. We run AMP with a Bernoulli-Gaussian prior C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas,
P B¯, ∆ = n/4 and L = L∗ = 3. Figure 6 shows im- J.,Wanderman-Milne,S.,andZhang,Q. JAX:compos-
agereconstructionsalongwiththeapproximateposterior abletransformationsofPython+NumPyprograms,2018.
p Ψ|V¯t,q(Z,Ψ,ε¯)(·|Θt,y). The approximate posterior con- 0.3.13.
Θ
centrates around the true change point locations as δ in-
Braun,J.V.,Braun,R.,andMu¨ller,H.-G. Multiplechange-
creases,evenwhentheimagereconstructionsareapprox-
pointfittingviaquasilikelihood,withapplicationtodna
imate. The experiment took one hour to complete on an
sequencesegmentation. Biometrika,2000.
AppleM1Maxchip,whereascompetingalgorithmsdidnot
returnanoutputwithin2.5hours,duetothelargersignal
Cho,H.andFryzlewicz,P. Multiple-change-pointdetection
dimensioncomparedtoFigure4(p=7225vsp=200).
for high dimensional time series via sparsified binary
segmentation. Journal of the Royal Statistical Society
References SeriesB:StatisticalMethodology,2015.
Andreou,E.andGhysels,E. Detectingmultiplebreaksin
Cho, H., Kley, T., and Li, H. Detection and inference of
financialmarketvolatilitydynamics. JournalofApplied
changesinhigh-dimensionallinearregressionwithnon-
Econometrics,2002.
sparsestructures,2024. arXiv:2402.06915.
Arpino,G.andLiu,X. AMPimplementationforchange Deshpande,Y.andMontanari,A. Information-theoretically
point inference in high-dimensional linear regression. optimalsparsePCA. InIEEEInternationalSymposium
https://github.com/gabrielarpino/AMP chgpt lin reg, onInformationTheory(ISIT),pp.2197–2201,2014.
9InferringChangePointsinHigh-DimensionalLinearRegression
Donoho, D. L., Maleki, A., and Montanari, A. Message Lavergne,P. ACauchy-Schwarzinequalityforexpectation
passingalgorithmsforcompressedsensing. Proceedings ofmatrices. DiscussionPapersdp08-07,Departmentof
oftheNationalAcademyofSciences,106:18914–18919, Economics,SimonFraserUniversity,November2008.
2009.
Lee,S.,Seo,M.H.,andShin,Y. TheLassoforhighdimen-
Donoho, D. L., Javanmard, A., and Montanari, A. sionalregressionwithapossiblechangepoint. Journalof
Information-theoreticallyoptimalcompressedsensingvia theRoyalStatisticalSociety.SeriesB(StatisticalMethod-
spatialcouplingandapproximatemessagepassing. IEEE ology),2016.
TransactionsonInformationTheory,59(11):7434–7464,
Nov.2013. Leonardi,F.andBu¨hlmann,P. Computationallyefficient
changepointdetectionforhigh-dimensionalregression,
Fan,Z.andGuan,L. Approximateℓ 0-penalizedestimation 2016. arXiv:1601.03704.
ofpiecewise-constantsignalsongraphs. TheAnnalsof
Statistics,46(6B):3217–3245,2018. Lesieur,T.,Krzakala,F.,andZdeborova´,L. Constrained
low-rankmatrixestimation: Phasetransitions,approxi-
Fearnhead, P. Exact and efficient bayesian inference for matemessagepassingandapplications. JournalofSta-
multiplechangepointproblems. Statisticsandcomputing, tistical Mechanics: Theory and Experiment, 2017(7):
16:203–213,2006. 073403,2017.
Feng,O.Y.,Venkataramanan,R.,Rush,C.,andSamworth,
Li, W., Wang, D., and Rinaldo, A. Divide and Conquer
R.J. AUnifyingTutorialonApproximateMessagePass-
DynamicProgramming: AnAlmostLinearTimeChange
ing. FoundationsandTrends®inMachineLearning,15
Point Detection Methodology in High Dimensions. In
(4):335–536,2022.
Proceedingsofthe40thInternationalConferenceonMa-
chineLearning,2023.
Fletcher,A.K.andRangan,S. Iterativereconstructionof
rank-onematricesinnoise. InformationandInference: A
Liu, H., Gao, C., and Samworth, R. J. Minimax rates in
JournaloftheIMA,7(3):531–562,2018.
sparse, high-dimensional change point detection. The
Fryzlewicz, P. Wild binary segmentation for multiple AnnalsofStatistics,49(2),2021.
change-pointdetection. TheAnnalsofStatistics,42(6):
Londschien, M., Bu¨hlmann, P., and Kova´cs, S. Random
2243–2281,2014.
ForestsforChangePointDetection. JournalofMachine
Gao, F. and Wang, T. Sparse change detection in high- LearningResearch,2023.
dimensionallinearregression,2022. arXiv:2208.06326.
Lungu,V.,Papageorgiou,I.,andKontoyiannis,I. Change-
Gerbelot, C. and Berthier, R. Graph-based approximate pointdetectionandsegmentationofdiscretedatausing
messagepassingiterations. InformationandInference: A Bayesiancontexttrees,2022. arXiv: 2203.04341.
JournaloftheIMA,2023.
Ma, J. and Ping, L. Orthogonal AMP. IEEE Access, 5:
Javanmard,A.andMontanari,A.Stateevolutionforgeneral 2020–2033,2017.
approximatemessagepassingalgorithms,withapplica-
tionstospatialcoupling. InformationandInference: A Ma, Y., Rush, C., and Baron, D. Analysis of approxi-
JournaloftheIMA,2013. matemessagepassingwithnon-separabledenoisersand
markovrandomfieldpriors. IEEETransactionsonInfor-
Kabashima, Y. A CDMA multiuser detection algorithm mationTheory,65(11):7367–7389,2019.
on the basis of belief propagation. Journal of Physics
A:MathematicalandGeneral,36(43):11111–11121,Oct MadridPadilla, O.H., Yu, Y., Wang, D., andRinaldo, A.
2003. Optimalnonparametricmultivariatechangepointdetec-
tionandlocalization. IEEETransactionsonInformation
Kaul, A., Jandhyala, V.K., andFotopoulos, S.B. Anef- Theory,68(3):1922–1944,2022.
ficienttwostepalgorithmforhighdimensionalchange
point regression models without grid search. J. Mach. Maillard, A., Loureiro, B., Krzakala, F., and Zdeborova´,
Learn.Res.,2019. L. Phase retrieval in high dimensions: Statistical and
computationalphasetransitions. InNeuralInformation
Krzakala, F., Me´zard, M., Sausset, F., Sun, Y., and Zde-
ProcessingSystems(NeurIPS),2020.
borova´, L. Probabilistic reconstruction in compressed
sensing: algorithms, phase diagrams, and threshold Metzler, C. A., Maleki, A., and Baraniuk, R. G. From
achieving matrices. Journal of Statistical Mechanics: denoisingtocompressedsensing. IEEETransactionson
TheoryandExperiment,2012(8),2012. InformationTheory,62(9):5117–5144,2016.
10InferringChangePointsinHigh-DimensionalLinearRegression
Mondelli,M.andVenkataramanan,R. Approximatemes- Wang,D.,Zhao,Z.,Lin,K.Z.,andWillett,R. Statistically
sagepassingwithspectralinitializationforgeneralized andcomputationallyefficientchangepointlocalization
linear models. International Conference on Artificial inregressionsettings. JournalofMachineLearningRe-
IntelligenceandStatistics,pp.397–405,2021. search,22(248):1–46,2021c.
Montanari,A.andVenkataramanan,R. Estimationoflow- Wang,T.andSamworth,R.J. HighDimensionalChange
rankmatricesviaapproximatemessagepassing. Annals PointEstimationviaSparseProjection. Journalofthe
ofStatistics,45(1):321–345,2021. RoyalStatisticalSocietySeriesB:StatisticalMethodol-
ogy,2018.
Pandit,P.,Sahraee-Ardakan,M.,Rangan,S.,Schniter,P.,
Wang,T.,Zhong,X.,andFan,Z. Universalityofapprox-
andFletcher,A.K. Inferencewithdeepgenerativepriors
imatemessagepassingalgorithmsandtensornetworks,
inhighdimensions. IEEEJournalonSelectedAreasin
2022. arXiv:2206.13037.
InformationTheory,1(1):336–347,2020.
Xu, H., Wang, D., Zhao, Z., and Yu, Y. Change point
Rangan,S.Generalizedapproximatemessagepassingfores-
inferenceinhigh-dimensionalregressionmodelsunder
timationwithrandomlinearmixing. IEEEInternational
temporaldependence,2022. arXiv:2207.12453.
SymposiumonInformationTheory,2011.
Yi,X.,Caramanis,C.,andSanghavi,S. Alternatingmini-
Rangan,S.,Schniter,P.,andFletcher,A.K. Vectorapproxi- mizationformixedlinearregression. InternationalCon-
matemessagepassing.IEEETransactionsonInformation ferenceonMachineLearning,pp.613–621,2014.
Theory,65(10):6664–6684,2019.
Zhang,Y.,Mondelli,M.,andVenkataramanan,R. Precise
Rinaldo,A.,Wang,D.,Wen,Q.,Willett,R.,andYu,Y. Lo- asymptoticsforspectralmethodsinmixedgeneralized
calizingChangesinHigh-DimensionalRegressionMod- linearmodels,2022. arXiv:2211.11368.
els. InInternationalConferenceonArtificialIntelligence
Zhang, Y., Ji, H. C., Venkataramanan, R., and Mondelli,
andStatistics,2021.
M. Spectral estimators for structured generalized lin-
Schniter,P.andRangan,S. Compressivephaseretrievalvia ear models via approximate message passing, 2023.
generalizedapproximatemessagepassing. IEEETrans- arXiv:2308.14507.
actionsonSignalProcessing,63(4):1043–1055,2014.
Som,S.andSchniter,P.Compressiveimagingusingapprox-
imatemessagepassingandaMarkov-treeprior. IEEE
Transactions on Signal Processing, 60(7):3439–3448,
2012.
Takeuchi, K. Rigorous dynamics of expectation-
propagation-basedsignalrecoveryfromunitarilyinvari-
ant measurements. IEEE Transactions on Information
Theory,66(1):368–386,2020.
Tan,N.andVenkataramanan,R. MixedRegressionviaAp-
proximateMessagePassing. JournalofMachineLearn-
ingResearch,24(317):1–44,2023.
Wang,D.,Yu,Y.,andRinaldo,A. Univariatemeanchange
point detection: Penalization, CUSUM and optimality.
ElectronicJournalofStatistics,pp.1917–1961,2020.
Wang,D.,Yu,Y.,andRinaldo,A. Optimalchangepoint
detection and localization in sparse dynamic networks.
TheAnnalsofStatistics,2021a.
Wang, D., Yu, Y., and Rinaldo, A. Optimal covariance
changepointlocalizationinhighdimensions. Bernoulli,
2021b.
11InferringChangePointsinHigh-DimensionalLinearRegression
A.ProofofStateEvolution
TheproofofTheorem3.1reliesonageneralizationofLemma14in(Gerbelot&Berthier,2023),presentedasLemma
A.1below. LetW ∈ RN×Q beamatrixsuchthat∥W⊤W ∥ /N convergestoafiniteconstantasN → ∞, andlet
0 0 0 F
A∈RN×N beasymmetricGOE(N)matrix,independentofW . ThenLemma14in(Gerbelot&Berthier,2023)gives
0
astateevolutionresultinvolvinganAMPiterationwhosedenoisingfunctiontakesasinputtheoutputofthegeneralized
linearmodelφ(AW ),whereφ:RN×Q →RN.
0
LemmaA.1generalizesLemma14ofGerbelot&Berthier(2023)intwoways: i)itallowsfortheinclusionofanauxiliary
matrixΞ∈RN×LΞ sothatthegeneralizedlinearmodelinquestionisoftheformφ(AW 0,Ξ),andii)thestateevolution
convergenceresultholdsforpseudo-LipschitztestfunctionstakingboththeauxiliarymatrixΞandAW asinputs.
0
WeextendthislemmabyconsideringanindependentrandommatrixΞ ∈ RN×LΞ,servingasinputtoasetofpseudo-
Lipschitzfunctionsφ:RN×(Q+LΞ) →RN. WethenanalyzeasimilarAMPiterationwhosedenoisingfunctionf˜ttakes
φ(AW ,Ξ)asinputinsteadofφ(AW ),initializedwithanindependentinitializerX0 ∈RN×Q:
0 0
Xt+1 =AMt−Mt−1(bt)⊤ ∈RN×Q, (32)
Mt =f˜t(φ(AW ,Ξ),Xt) ∈RN×Q, (33)
0
1 (cid:88)N ∂f˜t
bt = i (φ(AW ,Ξ),Xt) ∈RQ×Q. (34)
N ∂Xt 0
i=1 i
OurresultinLemmaA.1presentsanasymptoticcharacterizationof(32)–(34)viathefollowingstateevolutionrecursion:
ν0,νˆ0 =0, (35)
1
κ0,0 = lim f˜0(X0)⊤f˜0(X0), (36)
N→∞N
νt+1 = lim 1 E(cid:2) W⊤ f˜t(cid:0) φ(Z ,Ξ),Z ρ−1νt+W νˆt+Zt(cid:1)(cid:105) , (37)
N→∞N 0 W0 W0 W0 0
(cid:34) N (cid:35)
νˆt+1 = lim 1 E (cid:88) ∂ f¯t(cid:0) Z ,Z ρ−1νt+W νˆt+Zt,Ξ(cid:1) , (38)
N→∞N 1i i W0 W0 W0 0
i=1
κt+1,s+1 =κs+1,t+1 = lim 1 E(cid:20)(cid:16) f˜s(cid:0) φ(Z ,Ξ),Z ρ−1νs+W νˆs+Zs(cid:1) −W ρ−1νs+1(cid:17)⊤
N→∞N W0 W0 W0 0 0 W0
(cid:16) f˜t(cid:0) φ(Z ,Ξ),Z ρ−1νt+W νˆt+Zt(cid:1) −W ρ−1νt+1(cid:17)(cid:105) , (39)
W0 W0 W0 0 0 W0
whereρ = lim 1W⊤W ,andfori ∈ [n]wehave(Z ) i. ∼i.d N(0,ρ ). Fori ∈ [n],0 ≤ s,r ≤ t,wehave
W0 N→∞ N 0 0 W0 i W0
that(Z ) isindependentfrom(Zs) i. ∼i.d N(0,κs,s)withCov((Zs) ,(Zr) )=κs,r. In(38),weletf¯t :(z,u,v)(cid:55)→
f˜t(φ(zW ,v0 ),i
u)andwelet∂
f¯tdenotei thepartialderivativeoff¯twithri especti
tothei-throwofitsfirstargument.
1i i i
WelistthenecessaryassumptionsforcharacterizingthisAMPiteration,followedbytheresult:
Assumptions.
(B1) A∈RN×N isaGOE(N)matrix,i.e.,A=G+G⊤forG∈RN×N withi.i.d.entriesG ∼N(0,1/(2N)).
ij
(B2) Foreacht ∈ N ,f¯t : (z,u,Ξ) (cid:55)→ f˜t(φ(z,Ξ),u)isuniformlypseudo-Lipschitz. Foreacht ∈ N andforany
>0 >0
1 ⩽ i ⩽ N, (u,z) (cid:55)→ ∂f˜ it (φ(z,Ξ),u) is uniformly pseudo-Lipschitz. The function f˜0 : RN×Q → RN×Q is
∂Xi
uniformlypseudo-Lipschitz.
√ √
(B3) TheinitializationX0isdeterministic,and∥X0∥ / N,∥W⊤W ∥ /N,∥Ξ∥ / N convergealmostsurelytofinite
F 0 0 2 F
constantsasN →∞.
(B4) Thefollowinglimitsexistandarefinite:
1 1
lim f˜0(X0)⊤f˜0(X0), lim W⊤f˜0(X0).
N→∞N N→∞N 0
12InferringChangePointsinHigh-DimensionalLinearRegression
(B5) Foranyt∈N andanyκ∈S+,thefollowinglimitexistsandisfinite:
>0 Q
1 (cid:104) (cid:105)
lim E f˜0(X0)⊤f˜t(φ(Z,Ξ),Z)
N→∞N
whereZ ∈RN×Q,Z ∼N(0,κ⊗I ).
N
(B6) Foranys,t∈N andanyκ∈S+ ,thefollowinglimitexistsandisfinite:
>0 2Q
1 (cid:104) (cid:105)
lim E f˜s(φ(Zs,Ξ),Zs)⊤f˜t(φ(Zt,Ξ),Zt)
N→∞N
where(Zs,Zt)∈(RN×Q)2,(Zs,Zt)∼N(0,κ⊗I ).
N
Lemma A.1. Consider the AMP iteration (32)–(34) and the state evolution recursion (35)–(39). Assume (B1) −
(B6). Then for any sequence of functions Φ
N
: (RN×Q)⊗(t+3) × RN×LΞ → R such that (X0,...,Xt,V) (cid:55)→
Φ (X0,X1,...,Xt,V,W ,Ξ)isuniformlypseudo-Lipschitz,wehavethat:
N 0
Φ (cid:0) X0,X1,...,Xt,AW ,W ,Ξ(cid:1)
N 0 0
≃P E (cid:2) Φ (cid:0) Z ρ−1ν0+W νˆ0+Z0,...,Z ρ−1νt+W νˆt+Zt,Z ,W ,Ξ(cid:1)(cid:3) . (40)
Z0,...,Zt,ZW0 N W0 W0 0 W0 W0 0 W0 0
Proof. The main differences between the AMP result in (Gerbelot & Berthier, 2023) and our characterization are: 1)
the φ function is allowed to depend on an auxiliary random variable Ξ, 2) the test function Φ is allowed to depend
N
additionallyonAW ,W ,andΞ. Assumption(B2)guaranteesthatf˜tmaintainsthesamerequiredcoverageproperties
0 0
despitemodification1).
Wenowaddressmodification2).SinceΞandW arefixedandΦ˜ :(X0,...,Xt,V)(cid:55)→Φ (X0,X1,...,Xt,V,W ,Ξ)
0 N N 0
isassumedtobeuniformlypseudo-Lipschitz,thesecanbeincludedinΦ andtherighthandsideof(40)isunaffected.
N
Further, note that by Proposition 2 in (Gerbelot & Berthier, 2023) (a standard upper bound on the operator norm of
√
A∼GOE(N))wehavethat∥AW ∥ / N <∞asN →∞. WecanthereforeincludeAW inthedefinitionofΦ as
0 F 0 N
well,andtheuniformpseudo-LipschitznessofΦ˜ guaranteesconvergencetotheresult.
N
Wenextpresentthemainreduction,mappingtheAMPalgorithmproposedinthiswork(3)tothesymmetriconeoutlinedin
(32)–(34).
ProofofTheorem3.1. ConsidertheChangePointLinearRegressionmodel(1),andrecallthatitcanberewrittenas(2). We
reducethealgorithm(3)to(32)–(34),followingthealternatingtechniqueof(Javanmard&Montanari,2013). Theideaisto
defineasymmetricGOEmatrixwithX andX⊤ontheoff-diagonals. Withasuitableinitialization,theiteration(32)–(34)
thenyieldsBt+1intheeveniterationsandforΘtintheodditerations.
Let N = n+p. For a matrix E ∈ RN×L, we use E and E to denote the first n rows and the last p rows of E
[n] [−p]
respectively. Recallfrom(3)thatX ∈Rn×p,B,Bt ∈Rp×LandΘ=XB,Θt ∈Rn×L,andn/p→δasn,p→∞. We
let
(cid:114) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
δ D X 0 0
A= 1 ∼GOE(N), W = n×L ∈RN×L, X0 = n×L (41)
δ+1 X⊤ D 2 0 B B0
√
whereD ∼GOE(n)and δD ∼GOE(p)areindependentofeachotherandofX. LetΞ:=(cid:2) Ψ ε(cid:3) ∈Rn×2anddefine
1 2
(cid:32)(cid:114) (cid:33)
δ+1
φ:(W,Ξ)(cid:55)→q W ,Ψ,ε ∈RN, (42)
δ [n]
foranyW ∈RN×L. Wethereforehaveφ(AW ,Ξ)=q(XB,Ψ,ε). Letf˜t :RN×L×RN →RN×Lsuchthat
0
(cid:114) δ+1(cid:20) gt(U ,q(XB,Ψ,ε))(cid:21) (cid:114) δ+1(cid:20) 0 (cid:21)
f˜2t+1(φ(AW ,Ξ),U)= [n] and f˜2t(φ(AW ,Ξ),U)= n×L ,
0 δ 0 p×L 0 δ ft(U [−p])
(43)
13InferringChangePointsinHigh-DimensionalLinearRegression
foranyU inRN×L.
Next,considertheAMPiteration(32)–(34)withA,W ,X0,Ξ,φ,f˜tdefinedasin(41)–(43). Notethattheassumptions
0
(B1)−(B6)aresatisfiedbyconstruction,andhencethestateevolutionresultinLemmaA.1holdsfortheiteration(32)–(34).
Wewillnowshowthatthestateevolutionequationsdecomposeintothoseofinterest,(7)–(10). First,notethat
 =(Ft)⊤ 
(cid:122) (cid:125)(cid:124) (cid:123)
X2t+1 =  Xft(X2t )−gt−1(X2t−1,q(XB,Ξ))· 1 (cid:32) (cid:88)p ∂ft(X [2 −t p]) i(cid:33)⊤  , (44)
 [−p] [n] n ∂X2t 
 i=1 [−p],i 
D ft(X2t )
2 [−p]
 D gt−1(X2t−1,q(XB,Ξ)) 
1 [n]
X2t
=

X⊤gt−1(X2t−1,q(XB,Ξ))−ft−1(X2t−2)·(cid:32)
1
(cid:88)n ∂gt−1(X [2 nt ]−1,q(XB,Ξ))(cid:33)⊤
 , (45)
 [n] [−p] n ∂X2t−1 
 i=1 [n],i 
(cid:124) (cid:123)(cid:122) (cid:125)
=(Ct−1)⊤
and hence observe that X2t+1 and X2t are equal to Θt and Bt in (3) respectively. Define the main iterate Qt =
[n] [−p]
Z ρ−1νt+W νˆt+Zt ∈RN×L. Fori∈[n],letg˜t :(Z,V,Ψ,ε)(cid:55)→gt(V,q(Z,Ψ,ε))andlet∂ g˜t bethepartial
W0 W0 0 i i 1i i
derivative(Jacobian)w.r.t. theithrowofthefirstargument. Following(35)–(39),wethenhavethat
ν2t+1 = lim 1 (cid:114) δ+1 E(cid:20) (cid:2) 0 B⊤(cid:3)(cid:20) 0 n×L (cid:21)(cid:21) =(cid:114) δ lim 1 E(cid:104) B⊤ft(Q2t )(cid:105) , (46)
N→∞N δ L×n ft(Q2 [−t p]) δ+1n→∞n [−p]
ν2t =(cid:114) δ+1 lim 1 E(cid:20) (cid:2) 0 B⊤(cid:3)(cid:20) gt−1(Q2 [nt− ] 1,Y)(cid:21)(cid:21) =0 , (47)
δ N→∞N L×n 0
p×L
L×L
νˆ2t+1 =0 (48)
L×L
(cid:34) n (cid:32)(cid:114) (cid:33)(cid:35)
1 (cid:88) δ+1
νˆ2t = lim E ∂ g˜t−1 Z ,Q2t−1,Ψ,ε , (49)
n→∞n 1i i δ W0,[n] [n]
i=1
 (cid:32) (cid:32)(cid:114) (cid:33)(cid:33)⊤ (cid:32) (cid:32)(cid:114) (cid:33)(cid:33)
1 δ+1 δ+1
κ2t,2s = nl →im ∞nEgt−1 Q2 [nt−
]
1,q
δ
Z W0,[n],Ψ,ε gs−1 Q [2 ns ]−1,q
δ
Z W0,[n],Ψ,ε  (50)
(cid:32) (cid:114) (cid:33)⊤(cid:32) (cid:114) (cid:33)
1 δ+1 δ+1
κ2t+1,2s+1 = lim E ft(Q2t )− Bρ−1ν2t+1 fs(Q2s )− Bρ−1ν2s+1 , (51)
n→∞n [−p] δ B [−p] δ B
wherein(51)weusedρ = 1B⊤B = δ ρ . Hence,wecanassociate
W0 N δ+1 B
(cid:114)
δ+1
νt = ν2t+1, (52)
Θ δ
νt =νˆ2t, (53)
B
κt,s =κ2t+1,2s+1, (54)
Θ
κt,s =κ2t,2s. (55)
B
(cid:114)
δ+1
d
Z = Z , (56)
B δ W0,[n]
14InferringChangePointsinHigh-DimensionalLinearRegression
Gt =d Z2t+1, (57)
Θ [n]
Gt =d Z2t , (58)
B [−p]
Vt =d Q2t+1, (59)
Θ [n]
Vt =d Q2t . (60)
B [−p]
Substitutingthechangeofvariables(52)–(60)into(46)–(51),weobtain(7)–(10). Moreover,substituting(44)–(45)and
(59)–(60)intoLemmaA.1yieldsTheorem3.1.
B.StateEvolutionLimitsandSimplifications
B.1.StateEvolutionDependenceonB,ε
Inthissection,weoutlinehowthelimitsin(7)–(10)canbechecked,andhowthedependenceof(7)–(10)onB,εcanbe
removed. Wegiveasetofsufficientconditionsfortheexistenceofthelimitsin(7)–(10),whichallowforremovingthe
dependencyofthestateevolution(7)–(10)onB,ε. Assume(S0)onp.5,andalsothatthefollowingassumptionshold:
(S1) As n → ∞, the entries of the normalized change point vector η/n converge to constants α ,...,α such that
0 L
0=α <α <···<α =...=α =1.
0 1 L∗ L
(S2) Fort≥0,ft isseparable,andgt actsrow-wiseonitsinput. (Werecallthataseparablefunctionactsrow-wiseand
identicallyoneachrow.)
(S3) For ℓ ∈ [L∗ − 1], the empirical distributions of {g˜t(Z ,(Vt) ,Ψ ,ε )} and
i 1 Θ 1 ηℓ i i∈[ηℓ,ηℓ+1)
{∂ g˜t(Z ,(Vt) ,Ψ ,ε )} convergeweaklytothelawsofrandomvariablesgˆt (Z ,(Vt) ,Ψ ,,ε¯))and
1 i 1 Θ 1 ηℓ i i∈[ηℓ,ηℓ+1) ηℓ 1 Θ 1 ηℓ
gˇt (Z ,(Vt) ,Ψ ,ε¯),respectively,where∂ denotesJacobianwithrespecttothefirstargument.
ηℓ 1 Θ 1 ηℓ 1
Theassumption(S1)isnaturalintheregimewherethenumberofsamplesnisproportionaltop,andthenumberofdegrees
offreedominthesignalsalsogrowslinearlyinp. Withoutchangepoints,ft,gtcanbothbeassumedseparablewithoutloss
ofoptimality(dueto(S0)). Tohandlethetemporaldependencecreatedbychangepoints,werequiregttodependoni,for
i
i∈[n]. However,Proposition3.2showsthatitcanbechosentoactrow-wise,i.e.,g∗t(Θt,y)=g∗t(Θt,y ). Thisjustifies
i i i i
(S2). Whengt ischosentobeg∗t fori∈[n],thecondition(S3)canbetranslatedintoregularityconditionsontheprior
i i
marginalsπ Ψ¯
i
anddistributionalconvergenceconditionsonthenoiseε i,asthesearetheonlyquantitiesthatdifferalongthe
elementsofthesets{g˜t(Z ,(Vt) ,Ψ ,ε )} and{∂ g˜t(Z ,(Vt) ,Ψ ,ε )} forℓ∈[L∗−1].
i 1 Θ 1 ηℓ i i∈[ηℓ,ηℓ+1) 1 i 1 Θ 1 ηℓ i i∈[ηℓ,ηℓ+1)
Underassumptions(S0)-(S3),thestateevolutionequationsin(7)–(10)reduceto:
L−1
νt+1 = (cid:88) E(cid:2) gˇt (Z ,(Vt) ,Ψ ,ε¯)(cid:3) , (61)
B ηℓ 1 Θ 1 ηℓ
ℓ=0
L−1
κs+1,t+1 = (cid:88) E(cid:2) gˆs ((Vs) ,q (Z ,Ψ ,ε¯))gˆt ((Vt) ,q (Z ,Ψ ,ε¯))⊤(cid:3) , (62)
B ηℓ Θ 1 1 1 ηℓ ηℓ Θ 1 1 1 ηℓ
ℓ=0
νt+1 = 1 E(cid:2) B¯ft+1((νt+1)⊤B¯ +(Gt+1) )⊤(cid:3) , (63)
Θ δ 1 B B 1
κs+1,t+1 = 1 E(cid:2)(cid:0) fs+1((νs+1)⊤B¯ +(Gs+1) )−(νs+1)⊤ρ−1B¯(cid:1)
Θ δ 1 B B 1 Θ
(cid:0) ft+1((νt+1)⊤B¯ +(Gt+1) )−(νt+1)⊤ρ−1B¯(cid:1)⊤(cid:105) , (64)
1 B B 1 Θ
where(S2)hasallowedustoreducethematrixproductsintosumsofvectorouter-products,andassumptions(S0)–(S3)have
removedthedependenceonBandεduetothelawoflargenumbersargumentinLemma4of(Bayati&Montanari,2011).
B.2.RemovingdependenciesonεontheRHSof(28)–(29)
Underassumptions(S0)–(S3)above,thedependenceofVt ontheRHSof(28)–(29)onB,εcanberemoved. Moreover,
Θ
thedependenceoftheRHSonεthroughηˆcanberemovedonacase-by-casebasis. Weexpectthistohold,forexample,
15InferringChangePointsinHigh-DimensionalLinearRegression
under(S0)–(S3)forestimatorswhosedependenceonεisviaanormalizedinnerproductsuchas(27):
⟨(Vt) ,Z ⟩ ⟨(Vt) ,ε⟩
U(ηˆ(Vt,q(Z,U(η),ε)))=argmax Θ [:,ψ] [:,ψ] + Θ [:,ψ] . (65)
Θ n n
ψ∈X
Indeed,thenumericalexperimentsinSection4forchosenestimatorsdemonstrateastrongagreementbetweentheleft-hand
andright-handsidesof(28)andof(31),whenεontheright-handsideissubstitutedbyanindependentcopywiththesame
limitingmoments.
C.ProofandComputationofOptimalDenoisersft,gt
C.1.ProofofProposition3.2
Itissimilartothederivationoftheoptimaldenoisersformixedregressionin(Tan&Venkataramanan,2023),withafew
differencesinthederivationofg∗t. Weincludebothpartshereforcompleteness. ThisproofreliesonLemmaC.1and
LemmaC.2,Cauchy-SchwarzinequalityandStein’sLemmaextendedtovectorormatrixrandomvariables. Recallthatwe
treatvectors,includingrowsofmatrices,ascolumnvectors,andthatfunctionsh:Rq →Rq havecolumnvectorsasinput
andoutput.
LemmaC.1(ExtendedCauchy-Schwarzinequality,Lemma2in(Lavergne,2008)). LetA∈Rn×L andB ∈Rn×L be
randommatricessuchthatE∥A∥2 <∞,E∥B∥2 <∞,andE[A⊤A]isnon-singular,then
F F
E[B⊤B]−E[B⊤A](E[A⊤A])−1E[A⊤B]≽0. (66)
Lemma C.2 (Extended Stein’s Lemma). Let x ∈ RL and h : RL → RL be such that for ℓ ∈ [L], the function
h : x → [h(x)] is absolutely continuous for Lebesgue almost every (x : i ̸= ℓ) ∈ RL−1, with weak derivative
ℓ ℓ ℓ i
∂h (x)/∂x : RL → RsatisfyingE|∂h (x)/∂x | < ∞. Ifx ∼ N(µ,Σ)withµ ∈ RL andΣ ∈ RL×L positivedefinite,
ℓ ℓ ℓ ℓ
then
E(cid:2) (x−µ)h(x)⊤(cid:3) =ΣE[h′(x)]⊤, (67)
whereh′(x)istheJacobianmatrixofh.
TheproofofLemmaC.2followsfromLemma6.20of(Fengetal.,2022).
Proofofpart1(optimalft). Usingthelawoftotalexpectationandapplying(25),wecanrewriteν¯t in(19)as:
Θ
p
ν¯t = 1 lim 1(cid:88) E(cid:104) B¯(cid:0) ft(V¯t)(cid:1)⊤(cid:105) = 1 E(cid:104) B¯(cid:0) ft(V¯t)(cid:1)⊤(cid:105) = 1 E(cid:104) E(cid:104) B¯(cid:0) ft(V¯t)(cid:1)⊤(cid:12) (cid:12)V¯t(cid:105)(cid:105)
Θ δ p→∞p j B δ j B δ j B B
j=1
=
1 E(cid:104) E(cid:2) B¯|V¯t(cid:3)(cid:0) ft(V¯t)(cid:1)⊤(cid:105)
=
1 E(cid:2) f∗t(ft)⊤(cid:3)
, (68)
δ B j B δ j j
whereweusedtheshorthandft ≡ft(V¯t)andf∗t ≡f∗t(V¯t)=E[B¯|V¯t]. ApplyingLemmaC.1yields
j j B j j B B
E(cid:2) f∗t(f∗t)⊤(cid:3) −E(cid:2) f∗t(ft)⊤(cid:3) E(cid:2) ft(ft)⊤(cid:3)−1 E(cid:2) ft(f∗t)⊤(cid:3)≽0.
(69)
j j j j j j j j
Since(20)canbesimplifiedinto
κ¯t,t = 1 E(cid:2) ft(ft)⊤(cid:3) −(ν¯t )⊤ρ−1νt , (70)
Θ δ j j Θ Θ
using(68)and(70)in(69),weobtainthat
∆:= 1 E(cid:2) f∗t(f∗t)⊤(cid:3) −ν¯t (cid:2) κ¯t,t+(ν¯t )⊤ρ−1(νt )(cid:3)−1 (ν¯t )⊤ ≽0.
δ j j Θ Θ Θ Θ Θ
Addingandsubtractingκ¯t,tontheLHSgivesκ¯t,t−κ¯t,t+∆≽0.Leftmultiplyingbyρ⊤(cid:0) (ν¯t )−1(cid:1)⊤ andrightmultiplying
Θ Θ Θ Θ
by(ν¯t )−1ρmaintainsthepositivesemi-definitenessoftheLHSandfurtheryields
Θ
ρ⊤(cid:0) (ν¯t )−1(cid:1)⊤ κ¯t,t(ν¯t )−1ρ−ρ⊤(cid:0) (ν¯t )−1(cid:1)⊤ (κ¯t,t−∆)(ν¯t )−1ρ≽0, (71)
Θ Θ Θ Θ Θ Θ
(cid:124) (cid:123)(cid:122) (cid:125)
=:Γt
Θ
16InferringChangePointsinHigh-DimensionalLinearRegression
whichimplies
Tr(cid:16) ρ⊤(cid:0) (ν¯t )−1(cid:1)⊤ κ¯t,t(ν¯t )−1ρ(cid:17) ≥Tr(cid:0) Γt (cid:1) . (72)
Θ Θ Θ Θ
Recallfrom(23)thattheLHSof(72)istheobjectivewewishtominimiseviaoptimalft. Indeed,bysettingft =f∗t,(72)
issatisfiedwithequality,whichprovespart1ofProposition3.2.
RemarkC.3. Whenft =f∗t,(68)and(70)reducetoκ¯t,t =ν¯t −(ν¯t )⊤ρ−1ν¯t .
Θ Θ Θ Θ
Proof of part 2 (optimal gt) Recall from (17) that ν¯t+1 = lim 1 (cid:80)n E(cid:2) ∂ g˜t(Z ,(V¯t) ,Ψ¯ ,ε¯)(cid:3) , and we can
B n→∞ n i=1 1i i 1 Θ 1 i
rewritethetransposeofeachsummandas:
E(cid:2) ∂ g˜t(Z ,(V¯t) ,Ψ¯ ,ε¯)(cid:3)⊤ (73)
1i i 1 Θ 1 i
(cid:26) (cid:20) (cid:12) (cid:21)(cid:27)⊤
( =a) E (V¯ Θt)1 E Z1,ε¯ ∂ 1ig˜ it(cid:0) Z 1,(V¯ Θt) 1,Ψ¯ i,ε¯(cid:1)(cid:12) (cid:12) (cid:12)(V¯ Θt) 1
(cid:26) (cid:20) (cid:12) (cid:21)(cid:27)
( =b) E (V¯ Θt)1 Cov(cid:0) Z 1|(V¯ Θt) 1(cid:1)−1E Z1,Ψ¯ i,ε¯ (cid:0) Z 1−E[Z 1|(V¯ Θt) 1](cid:1) g˜ it(cid:0) Z 1,(V¯ Θt) 1,Ψ¯ i,ε¯(cid:1)⊤(cid:12) (cid:12) (cid:12)(V¯ Θt) 1
(cid:26) (cid:20) (cid:20) (cid:12) (cid:21)(cid:21)(cid:27)
( =c) E (V¯ Θt)1 Cov(cid:0) Z 1|(V¯ Θt) 1(cid:1)−1E q(Z1,Ψ¯ i,ε¯) E Z1 (cid:0) Z 1−E[Z 1|(V¯ Θt) 1](cid:1) g it(cid:0) (V¯ Θt) 1,q(Z 1,Ψ¯ i,ε¯)(cid:1)⊤(cid:12) (cid:12) (cid:12)(V¯ Θt) 1,q(Z 1,Ψ¯ i,ε¯)
=E (V¯ Θt)1(cid:110) Cov(cid:0) Z 1|(V¯ Θt) 1(cid:1)−1E q(Z1,Ψ¯ i,ε¯)(cid:104)(cid:0) E[Z 1|(V¯ Θt) 1,q(Z 1,Ψ¯ i,ε¯)]−E[Z 1|(V¯ Θt) 1](cid:1) g it(cid:0) (V¯ Θt) 1,q(Z 1,Ψ¯ i,ε¯)(cid:1)⊤(cid:105)(cid:111)
( =d) E(cid:110) Cov(cid:0) Z |(V¯t) (cid:1)−1(cid:0) E(cid:2) Z |(V¯t) ,q(Z ,Ψ¯ ,ε¯)(cid:3) −E[Z |(V¯t) ](cid:1) gt(cid:0) (V¯t) ,q(Z ,Ψ¯ ,ε¯)(cid:1)⊤(cid:111)
1 Θ 1 1 Θ 1 1 i 1 Θ 1 i Θ 1 1 i
(cid:124) (cid:123)(cid:122) (cid:125)
=E(cid:2) g i∗t(g it)⊤(cid:3) , =g i∗t((V¯ Θt)1,q(Z1,Ψ¯ i,ε¯)) (74)
where(a)and(c)followfromthelawoftotalexpectation;(b)usesLemmaC.2;and(d)uses(26). Inthelastlineof(74)
wehaveusedtheshorthandgt ≡gt(cid:0) (V¯t) ,q(Z ,Ψ¯ ,ε¯)(cid:1) andg∗t ≡g∗t(cid:0) (V¯t) ,q(Z ,Ψ¯ ,ε¯)(cid:1) . Substituting(74)in(17)
i i Θ 1 1 i i i Θ 1 1 i
yields
n (cid:20) (cid:21)
1 (cid:88) 1
ν¯t+1 = lim E[gt(g∗t)⊤]= lim E (gt)⊤g∗t . (75)
B n→∞n i i n→∞ n
i=1
Notethatsinceπ Ψ¯
i
differsacrossi,g itdiffersacrossiandthesumin(75)cannotbereduced. LemmaC.1impliesthat
(cid:20)
1
(cid:21) (cid:20)
1
(cid:21) (cid:20)
1
(cid:21)−1 (cid:20)
1
(cid:21)
E (g∗t)⊤g∗t −E (g∗t)⊤(gt) E (gt)⊤gt E (gt)⊤g∗t ≽0. (76)
n n n n
Recallingfrom(17)and(18)thatthelimitsinthestateevolutioniteratesν¯t+1 =lim E(cid:2)1(gt)⊤g∗t(cid:3) andκ¯t+1,t+1 =
B n→∞ n B
lim
E(cid:2)1(gt)⊤gt(cid:3)
exist,wecantakelimitstoobtain
n→∞ n
lim
E(cid:20)
1
(g∗t)⊤g∗t(cid:21)
− lim
E(cid:20)
1 (cid:0) g∗t(cid:1)⊤
gt(cid:21)(cid:18)
lim
E(cid:20)
1
(gt)⊤gt(cid:21)(cid:19)−1
lim
E(cid:20)
1
(gt)⊤g∗t(cid:21)
≽0. (77)
n→∞ n n→∞ n n→∞ n n→∞ n
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
(ν¯ Bt+1)⊤ (κ¯t B+1,t+1)−1 ν¯ Bt+1
Left multiplying
ν¯t+1(cid:0) (ν¯t+1)−1(cid:1)⊤
and right multiplying
(ν¯t+1)−1(cid:0) ν¯t+1(cid:1)⊤
on the LHS maintains the positive semi-
B B B B
definitenessoftheLHStogive
ν¯t+1(cid:0) (ν¯t+1)−1(cid:1)⊤ lim E(cid:20) 1 (g∗t)⊤g∗t(cid:21) (ν¯t+1)−1(cid:0) ν¯t+1(cid:1)⊤ −ν¯t+1(cid:16) κ¯t+1,t+1(cid:17)−1(cid:0) ν¯t+1(cid:1)⊤ ≽0, (78)
B B n→∞ n B B B B B
(cid:124) (cid:123)(cid:122) (cid:125)
=:Γt+1
B
Moreover,sinceforpositivedefinitematricesΓ andΓ ,Γ −Γ ≽0impliesΓ−1−Γ−1 ≼0,wehavethat
1 2 1 2 1 2
[(ν¯t+1)−1]⊤κ¯t+1,t+1(ν¯t+1)−1−Γt+1 ≽0, (79)
B B B B
17InferringChangePointsinHigh-DimensionalLinearRegression
whichimplies
Tr(cid:16) [(ν¯t+1)−1]⊤κ¯t+1,t+1(ν¯t+1)−1(cid:17) ≥Tr(cid:0) Γt+1(cid:1)
.
B B B B
(cid:16) (cid:17)
Recallfrom(24)thatgtisoptimisedbyminimisingTr [(ν¯t+1)−1]⊤κ¯t+1,t+1(ν¯t+1)−1 . Indeed,bychoosinggt =g∗t,
B B B
theobjectiveachievesitslowerbound,whichcompletestheproof.
RemarkC.4. Notesettinggt =g∗tleadstoν¯t+1 =κ¯t+1,t+1.
B B
C.2.Computationoff∗tandg∗t
The computation of the optimal denoisers relies on Lemmas C.5 and C.6, which state the conditioning properties of
multivariateGaussiansormixturesofGaussians. WecomputetheJacobiansofthesedenoisersin(3)usingAutomatic
DifferentiationinPythonJAX(Bradburyetal.,2018).
Lemma C.5 (Conditioning property of multivariate Gaussian). Suppose x ∈ Rn and y ∈ Rm are jointly Gaussian:
(cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
x µ Σ Σ
∼N x , x xy ,then
y µ Σ⊤ Σ
y xy y
E[x|y =y˜]=µ +Σ Σ−1(y˜−µ ), Cov(x|y)=Σ −Σ Σ−1Σ⊤ .
x xy y y x xy y xy
(cid:20) (cid:21)
LemmaC.6(ConditioningpropertyofGaussianmixtures). Letx
a
∈RLa,x
b
∈RLb with x xa ∼(cid:80)K k=1π kN(µ k,Σ k),
b
then
(cid:80)K π P (x =x)E [x |x =x]
E[x |x =x]= k=1 k k b k a b (80)
a b (cid:80)K π P (x =x)
k′=1 k′ k′ b
(cid:20) (cid:21)
x
whereE [x |x ]istheconditionalexpectationgiven a ∼N(µ ,Σ ).
k a b x k k
b
Computationoff∗t Recalling(22),f∗tin(25)canbecomputedbyexpandingtheconditionalexpectation:
j
f∗t(U)=f∗t(U ):=E[B¯|V¯t =U ],
j j j B j
whichpermitseasilycomputableexpressionsforsufficientlyregulardistributionsP B¯ suchasdiscrete,Bernoulli-Gaussian
orGaussiandistributions. Wegivetheformulasforthesebelow.
Computationoff∗tforGaussiansignals WhenB¯ ∼N (E[B¯],δρ),wehave
L
(cid:20) B¯ (cid:21) ∼N (cid:0) µ ,Σt (cid:1) , where µt :=(cid:20) I L×L (cid:21) E[B¯], Σt :=(cid:20) ρ ρν¯ Bt (cid:21) ,
V¯t 2L B B B (ν¯t )⊤ B (ν¯t )⊤ρ (ν¯t )⊤ρν¯t +κ¯t,t/δ
B B B B B B
andapplyingLemmaC.5yields:
ft(U )=E[B¯]+ρν¯t (cid:0) (ν¯t )⊤ρν¯t +κ¯t,t/δ(cid:1)−1 (U −(ν¯t )⊤E[B¯]).
j j B B B B j B
Computationoff∗tforBernoulli-Gaussianprior ThispriorisusedintheexperimentsinFigures4,6and7. Itassumes
thatB¯ ∈{B¯ } isdistributedasfollows:
ℓ ℓ ℓ∈[L]
B¯ i ∼.i.d ζ(α;σ ):=(1−α)δ +αN(cid:0) 0,σ2(cid:1) ,
ℓ ℓ 0 ℓ
withσ2 = δρ .Recallthatfort≥0,V¯t+1 :=(νt+1)⊤B¯ +(G¯t+1) ,whereB¯ and(G¯t+1) areindependent. Then,for
ℓ α ℓ,ℓ B B B 1 B 1
j ∈[p],wehavethat,
(cid:82) B¯P[V¯t+1 =s|B¯](cid:81)L ζ(α;σ )dB¯ (cid:82) B¯ϕ(s;(νˆt+1)⊤B¯,κt+1,t+1)(cid:81)L ζ(α;σ )dB¯
f∗t(s)=E[B¯|V¯t =s]= B ℓ=1 ℓ = B B ℓ=1 ℓ ,
j B (cid:82) P[V¯t+1 =s|B¯](cid:81)L ζ(α;σ )dB¯ (cid:82) ϕ(s;(νˆt+1)⊤B¯,κt+1,t+1)(cid:81)L ζ(α;σ )dB¯
B ℓ=1 ℓ B B ℓ=1 ℓ
18InferringChangePointsinHigh-DimensionalLinearRegression
where ϕ(x;µ,Σt) denotes the density of a zero-mean multivariate Gaussian with mean µ and covariance matrix Σt,
evaluatedatx. Theabovecanbecomputednumerically,oranalyticallyusingpropertiesofGaussianintegrals. Thelatter
approachyieldsf∗t(s)= d1,where:
j d2
d = (cid:88) αL−|S|(1−α)|S|(cid:90) ϕ(cid:0) B¯ ;0,Σ (cid:1) ·ϕ(cid:16) s−(ν¯t+1)⊤B¯;0,κ¯t+1,t+1(cid:17) dB¯, (81)
2 S S B B
S⊆[L]
d = (cid:88) αL−|S|(1−α)|S|(cid:90) B¯ϕ(cid:0) B¯ ;0,Σ (cid:1) ·ϕ(cid:16) s−(ν¯t+1)⊤B¯;0,κ¯t+1,t+1(cid:17) dB¯, (82)
1 S S B B
S⊆[L]
andB¯ isthesize|S|vectorcontainingtheentriesofB¯ indexedbyS. Here,Σ isthe|S|×|S|sub-matrixofρindexed
S S
alongS×S. Wethenhavethat,
(cid:90) ϕ(cid:0) B¯ ;0,Σ (cid:1) ·ϕ(cid:16) s−(ν¯t+1)⊤B¯;0,κ¯t+1,t+1(cid:17) dB¯
S S B B
=(2π)−L 2 (cid:12) (cid:12)Σ S(cid:12) (cid:12)−1 2(cid:12) (cid:12)κ¯ Bt+1,t+1(cid:12) (cid:12)−1 2 ·(cid:12) (cid:12)Σ c(cid:12) (cid:12)21 exp(cid:18) −1 2(cid:104) −µ⊤
c
Σ− c1µ c+s⊤(κ¯ Bt+1,t+1)−1s(cid:105)(cid:19) ,
whereΣ =(cid:2) Σ−1+∆(cid:3)−1 with∆=ν¯t+1(κ¯t+1,t+1)−1(ν¯t+1)⊤andµ =Σ ν¯t+1(κ¯t+1,t+1)−1s. Moreover,
c S B B B c S B B
(cid:90) B¯ϕ(cid:0) B¯ ;0 ,Σ (cid:1) ·ϕ(cid:16) s−(ν¯t+1)⊤B¯;0 ,κ¯t+1,t+1(cid:17) dB¯ =:z ∈RL,
S L S B L B
where
z S =(2π)−L 2 (cid:12) (cid:12)Σ S(cid:12) (cid:12)−1 2(cid:12) (cid:12)κ¯ Bt+1,t+1(cid:12) (cid:12)−1 2 ·(cid:12) (cid:12)Σ c(cid:12) (cid:12)1 2 exp(cid:18) −1 2(cid:104) −µ⊤ c Σ− c1µ c+s⊤(κ¯ Bt+1,t+1)−1s(cid:105)(cid:19) µ c,
z =0.
[L]\S
Pluggingtheaboveexpressionsinto(81)–(82),weobtainaclosed-formexpressionforf∗t.
j
Computationoff∗tforsparsedifferenceprior FortheexperimentsinFigure5,weconsidersignalswithsparsechanges
betweenadjacentsignals. Thepriortakesthefollowingform:
β(η0)i. ∼i.d. N(0,κ2)
j
(cid:40)
β(η0), withprobability1−α
β(η1) = j
j γ(β(η0)+w), withprobabilityα
j
.
.
.
(cid:40)
β(ηL−1), withprobability1−α
β(ηL) = j (83)
j γ(β(ηL−1)+w), withprobabilityα
j
(cid:113)
wherewi. ∼i.d. N(0,σ2)createsthesparsechangebetweenadjacentsignals, andγ := κ2 isarescalingfactorthat
w κ2+σ2
w
ensuresuniformsignalmagnitudeE[(β(η0))2] = ··· = E[(β(ηL))2],i.e.,E[(B¯ )2] = ··· = E[(B¯ )2]. Note(83)canbe
j j 1 L
compactlyexpressedasamixtureofGaussians,forexample,forL=3:
P B¯ =π 00N(cid:0) 0 3,Cov00(cid:1) +π 01N(cid:0) 0 3,Cov01(cid:1) +π 10N(cid:0) 0 3,Cov10(cid:1) +π 11N(cid:0) 0 3,Cov11(cid:1) (84)
where
π =(1−α)2, π =(1−α)α, π =α(1−α), π =α2,
00 01 10 11

1 1
γ 
1 γ
γ 
1 γ
γ2
Cov00 =κ21 3×3, Cov01 =κ2 1 1 γ, Cov10 =κ2 γ 1 1, Cov11 =κ2 γ 1 γ.
γ γ 1 γ 1 1 γ2 γ 1
19InferringChangePointsinHigh-DimensionalLinearRegression
Ingeneral,foranyL,definingtheshorthandυ ≡[υ ,...,υ ]⊤ ∈{0,1}L−1,wehave
1 L−1
(cid:88)
P B¯ = π υN(0 L,Covυ) (85)
υ∈{0,1}L−1
and{π }and{Covυ}canbedefinedanalogouslytothosein(83). UsingLemmaC.6,weobtainaclosed-formexpression
υ
forf∗tinvolving{π }and{Covυ}.
υ
Computationofg∗tforGaussiannoiseε¯∼N(0,σ2) Recalling(21),wehave
(cid:20) Z (cid:21) (cid:20) ρ ν¯t (cid:21)
1 ∼N (0,Σt ), whereΣt := Θ . (86)
(V¯t) 2L Θ Θ (ν¯t )⊤ (ν¯t )⊤ρ−1ν¯t +κ¯t,t
Θ 1 Θ Θ Θ Θ
LemmaC.5thengivestheformulasforE[Z |(V¯t) =V ]andCov(Z |(V¯t) ),thefirstandlasttermsofg∗tin(26),as
1 Θ 1 1 1 Θ 1 i
follows:
Cov(Z |(V¯t) )=ρ−(ν¯t )⊤(cid:0) (ν¯t )⊤ρ−1ν¯t +κ¯t,t(cid:1)−1 ν¯t ,
1 Θ 1 Θ Θ Θ Θ Θ
E[Z |(V¯t) =V ]=νt (cid:0) (ν¯t )⊤ρ−1νt +κt,t(cid:1)−1 V . (87)
1 Θ 1 1 Θ Θ Θ Θ 1
Whenft =f∗t,wecanusethesimplificationsinRemarkC.3toobtain:
Cov(Z |(V¯t) )=ρ−ν¯t and E[Z |(V¯t) =V ]=V .
1 Θ 1 Θ 1 Θ 1 1 1
We now calculate the middle term E[Z |(V¯t) = V ,q(Z ,Ψ¯ ,ε¯) = u ] of g∗t in (26). Recalling from (2) that
1 Θ 1 i 1 i i i
q(Z 1,Ψ¯ i,ε¯)=(Z 1) Ψ¯
i
+ε¯whereZ 1 ∼N(0,ρ),wehave
E[Z q(Z ,Ψ¯ ,ε¯)|Ψ¯ =ℓ]=E[Z (Z ) ]=ρ ,
1 1 i i 1 1 ℓ [:,ℓ]
E[q(Z 1,Ψ¯ i,ε¯)2|Ψ¯ i =ℓ]=E[(Z 1)2 ℓ +ε¯2]=ρ ℓ,ℓ+σ2, (88)
E[(V¯t) q(Z ,Ψ¯ ,ε¯)|Ψ¯ =ℓ]=E[(Zρ−1ν¯t ) (Z ) ]=(ν¯t )⊤ρ−1ρ =(cid:0) (ν¯t ) (cid:1)⊤ ,
Θ 1 1 i i Θ 1 1 ℓ Θ [:,ℓ] Θ [ℓ,:]
whichimpliesthatconditionedonΨ¯ =ℓ,
i
(cid:20) q(Z(V 1¯ ,Θt Ψ¯) i1 ,ε¯)(cid:21) ∼N L+1(cid:0) 0,Σt(cid:1) , whereΣt :=(cid:20) (ν¯ Θt )⊤ (ρ ν¯− Θt1 )ν¯ [ℓΘt ,:]+κ¯t Θ,t (cid:0) ( ρν¯ ℓΘ ,t ℓ) +[ℓ, σ:](cid:1) 2⊤(cid:21) . (89)
ConditionedonΨ =ℓtherandomvariables(Z ,(V¯t) ,q (Z ,Ψ¯ ,ε¯))arejointlyGaussianwithzeromeanandcovariance
i 1 Θ 1 1 1 i
matrixdeterminedby(86)-(89). UsingLemmaC.5onthesejointlyGaussianvariables,weobtain:
E[Z |(V¯t) =V , q(Z ,Ψ¯ ,ε¯)=u ]= (cid:80)L ℓ=1λt(V i,u i,ℓ)π Ψ¯ i(ℓ)L 1(V i,u i|ℓ) , (90)
1 Θ 1 i 1 i i (cid:80)L ℓ˜=1π Ψ¯ i(ℓ˜)L 1(V i,u i|ℓ˜)
whereπ Ψ¯
i
:ℓ(cid:55)→(cid:80) {ψ:ψi=ℓ}π Ψ¯(ψ)denotesthemarginalprobabilityofΨ¯ i,and
(cid:18)(cid:20) (cid:21) (cid:19)
V
L (V ,u |ℓ):=ϕ i ;0,Σt , (91)
1 i i u
i
(cid:20) (cid:21)
λt(V ,u ,ℓ):=E[Z |V¯t =V , q (Z ,Ψ¯ ,ε¯)=u , Ψ¯ =ℓ]=(cid:2) ν¯t ρ (cid:3) [Σt]−1 V i ∈RL. (92)
i i 1 Θ,1 i 1 1 1 i 1 Θ [:,ℓ] u
i
Here,ϕ(x;0,Σt)denotesthedensityofazero-meanmultivariateGaussianwithcovariancematrixΣt,evaluatedatx.
D.ProofofPropositions3.3and3.4
D.1.ProofofProposition3.3
Proof. RecallthatasignalconfigurationvectorΨ ∈ X isavectorthatispiece-wiseconstantwithrespecttoitsindices
i∈[n],withjumpsofsize1occurringattheindices{η
}L∗−1.
Withoutlossofgenerality,weassumeΨisalsomonotone
ℓ ℓ=1
20InferringChangePointsinHigh-DimensionalLinearRegression
(otherwise, anynon-distinctsignalcanbetreatedasanewsignalhavingperfectcorrelationwiththefirst). Recallthat
η ∈[n]L∗−1isthechangepointvectorcorrespondingtoΨ,i.e.,η =U−1(Ψ). Alsorecallthatfori∈[n],thedistanceof
η fromachangepointestimateηˆ ∈ [n]L−1,isdefinedasd(η ,{ηˆ }L−1) := min ∥η −ηˆ ∥ . Then,wehave
i i j j=1 ηˆj∈{ηˆj}L j=− 11 i j 2
thatd(η ,{ηˆ }L−1) ≤ ∥U(η)−U(ηˆ)∥2 foralli ∈ [L∗−1],andsimilarlyd({η }L∗−1,ηˆ ) ≤ ∥U(η)−U(ηˆ)∥2 forall
i j j=1 F i i=1 j F
j ∈[L−1]. Thisimpliesthatd (η,ηˆ)≤∥U(η)−U(ηˆ)∥2.
H F
We first prove that (Θt,y) (cid:55)→ d (η,ηˆ(Θt,y)) is uniformly pseudo-Lipschitz. Consider two inputs, A(1) :=
H
((Θt)(1),y(1))andA(2) :=((Θt)(2),y(2)). Wethenhavethat:
1 (cid:12) (cid:12)
(cid:12)d (η,ηˆ(A(1)))−d (η,ηˆ(A(2)))(cid:12)
n(cid:12) H H (cid:12)
1
≤ d (ηˆ(A(1)),ηˆ(A(2))) (93)
n H
1
≤ ∥U(ηˆ(A(1)))−U(ηˆ(A(2)))∥2 (94)
n F
1
≤L· √ ∥U(ηˆ(A(1)))−U(ηˆ(A(2)))∥ (95)
n F
 (cid:32)(cid:13) (cid:13)(cid:2) Θ(1) y(1)(cid:3)(cid:13) (cid:13) (cid:33)r−1 (cid:32)(cid:13) (cid:13)(cid:2) Θ(2) y(2)(cid:3)(cid:13) (cid:13) (cid:33)r−1 (cid:13) (cid:13)(cid:2) Θ(1) y(1)(cid:3) −(cid:2) Θ(2) y(2)(cid:3)(cid:13) (cid:13)
≤L·C1+ √ F + √ F  √ F, (96)
n n n
forsomeconstantsC >0,r ≥1. Here(93)followsfromthereversetriangleinequalityforthemetricd ,(94)followsfrom
H
theargumentintheparagraphabove,(95)followsfromthefactthatX ⊆[L]n,and(96)followsfromthepseudo-Lipschitz
assumptiononU(ηˆ(Θt,y)). ApplyingTheorem3.1withφ (Θt,y,Ψ):= 1d (U−1(Ψ),ηˆ(Θt,y)),weobtainthefirst
n n H
resultinProposition3.3.
Wenowprovethat(Θt,y)(cid:55)→|ηˆ(Θt,y)|isuniformlypseudo-Lipschitz. DenotingthenthcomponentofU(ηˆ)byU(ηˆ) ,
n
noticethatforachangepointvectorηˆ∈[n]L−1,bythemonotonicityofΨ,wehavethat|ηˆ|=U(ηˆ) . Considertwoinputs
n
A(1) :=((Θt)(1),y(1)),A(2) :=((Θt)(2),y(2)). Wethenhavethat:
(cid:12) (cid:12)
(cid:12)|ηˆ(A(1))|−|ηˆ(A(2))|(cid:12) (97)
(cid:12) (cid:12)
(cid:12) (cid:12)
=(cid:12)|U(ηˆ(A(1)) |−|U(ηˆ(A(2)) |(cid:12)
(cid:12) n n (cid:12)
 (cid:32)(cid:13) (cid:13)(cid:2) Θ(1) y(1)(cid:3)(cid:13) (cid:13) (cid:33)r−1 (cid:32)(cid:13) (cid:13)(cid:2) Θ(2) y(2)(cid:3)(cid:13) (cid:13) (cid:33)r−1 (cid:13) (cid:13)(cid:2) Θ(1) y(1)(cid:3) −(cid:2) Θ(2) y(2)(cid:3)(cid:13) (cid:13)
≤C1+ √ F + √ F  √ F, (98)
n n n
forsomeconstantsC >0,r ≥1,where(98)followsbecauseU(ηˆ(Θt,y)) isassumedtobeuniformlypseudo-Lipschitz.
n
ApplyingTheorem3.1toφ (Θt,y,Ψ):=|U(ηˆ(Θt,y)) |,weobtainthesecondclaimininProposition3.3.
n n
D.2.ProofofProposition3.4
Proof. Fixψ ∈ X,anddefineφ
n
: (Θt,y) (cid:55)→ p Ψ|V¯t,q(Z,Ψ,ε¯)(ψ|Θt,y). ApplyingTheorem3.1toφ n,weobtainthe
Θ
result.
E.FurtherImplementationandExperimentDetails
State Evolution Implementation Our state evolution implementation involves computing (61)–(64). We estimate
νt+1,κt+1,t+1,νt+1,κt+1,t+1in(61)–(64)withfiniten,pviaempiricalaverages,foragivenchangepointconfiguration
Θ Θ B B
{Ψ }L−1. Specifically,assumingνt ,κt,t,νt ,κt,thavebeencomputed,wecomputeνt+1,κt+1,t+1,νt+1,κt+1,t+1as
ηℓ ℓ=0 Θ Θ B B Θ Θ B B
21InferringChangePointsinHigh-DimensionalLinearRegression
follows:
L−1
νt+1 ≈ (cid:88) 1 (cid:88) Eˆ(cid:2) ∂ g˜t(Z ,(Vt) ,Ψ ,ε¯)(cid:3) , (99)
B n 1 i 1 Θ 1 ηℓ
ℓ=0 i∈[ηℓ,ηℓ+1)
L−1
κt+1,t+1 ≈ (cid:88) 1 (cid:88) Eˆ(cid:104) gt(cid:0) (Vt) ,q (Z ,Ψ ,ε¯)(cid:1)⊤ gt(cid:0) (Vt) ,q (Z ,Ψ ,ε¯)(cid:1)(cid:105) , (100)
B n i Θ 1 1 1 ηℓ i Θ 1 1 1 ηℓ
ℓ=0 i∈[ηℓ,ηℓ+1)
νt+1 ≈ 1 Eˆ(cid:2) B¯ft+1(Vt+1)⊤(cid:3) , (101)
Θ δ 1 B
κt+1,t+1 ≈ 1 Eˆ(cid:104)(cid:0) ft+1(Vt+1)−B¯ρ−1νt+1(cid:1)(cid:0) ft+1(Vt+1)−B¯ρ−1νt+1(cid:1)⊤(cid:105) . (102)
Θ δ 1 B Θ B Θ
whereEˆ denotesanexpectationestimateviaMonteCarlo. Forexample,inthecaseof(100),wegenerate300to1000
independentsamplesof(Z ,(Gt ) ,ε¯),withZ ∼N(0,ρ),(Gt ) ∼N(0,κt,t),andε¯∼P . Weform(Vt) according
1 Θ 1 1 Θ 1 Θ ε¯ Θ 1
to the first row of (4), i.e., (Vt) = Z ρ−1νt +(Gt ) . For ℓ ∈ {0,...,L−1}, this yields a set of samples of the
Θ 1 1 Θ Θ 1
randomvariablesgt((Vt) ,q (Z ,Ψ ,ε¯))and∂ g˜t(Z ,(Vt) ,Ψ ,ε¯)(thelatterfunctioniscomputedusingAutomatic
i Θ 1 1 1 ηℓ 1 i 1 Θ 1 ηℓ
Differentiation (AD)). We then compute Eˆ in (99) and (100) by averaging. The Eˆ terms in (101)–(102) are similarly
computed.
Theensemblestateevolutionrecursion(17)–(20)isusedtocomputetheoptimaldenoisers{g∗t,f∗t+1} accordingto
t≥0
Proposition3.2. Theexpectationintheensemblestateevolutioniterates(17)–(20)areestimatedthroughsampleaverages
andAD,similartothesimplifiedstateevolutioniterates.
ExperimentDetails ForthesyntheticdataexperimentsinSection4,weconsidermodel(2)withεi. ∼i.d N(0,σ2I )and
n
underthemodelassumptionsinSection2. Foreachexperiment,werunAMPfort≤15iterations,andaverageover10to
20independenttrials. WeinitializeAMPwithf0(B0)sampledrow-wiseindependentlyfromthepriorP B¯ usedtodefine
theensemblestateevolution(17)–(20). Thedenoisers{gt,ft+1} intheAMPalgorithmarechosentobetheonesgiven
t≥0
byProposition3.2,whosecomputationisdetailedinAppendixC.2.
Figures4,6,7and8useBernoulli-Gaussianpriors,asdefinedinAppendixC.2. Figure6usesα=1/6andσ =2.5for
ℓ
ℓ ∈ [L]. Figures4, 7and8useα = 0.5andσ2 = δ forℓ ∈ [L]toensurethatthesignalpowerE[(X )⊤β(i)]2 isheld
ℓ i
constantforvaryingδ. Therearetwochangepointsatn/3and8n/15intheexperimentsinFigures4,7and8. AMPuses
crossvalidation(CV)over5valuesofαˆ :{0.1,0.3,0.45,0.6,0.9}whichdonotcontainthetrueα=0.5. DCDP,DPDU,and
DPeachhavetwohyperparameters: onecorrespondingtotheℓ penaltyandtheotherpenalizingthenumberofchange
1
points. Weruncross-validationonthesehyperparametersfor12,12,or42pairsofvalues,respectively,usingthesuggested
valuesfromtheoriginalpapers.
Figure5usesthesparsedifferencepriordefinedinAppendixC.2. Werecallthat,forj ∈[p],β(η0)i. ∼i.d. N(0,κ2)andwith
j
probabilityα wehavethatβ(ηℓ) = γ(β(ηℓ−1) +w)wherewi. ∼i.d. N(0,σ2)andγ > 0isanormalizingconstantsothat
j j w
E[(β(ηℓ))2]=κ2forℓ∈[L]. Weruntheexperimentwithsparsitylevelα=0.5,varianceoftheentriesofthefirstsignal
j
κ2 = 8δ,andperturbationtoeachconsecutivesignalofvarianceσ2 = 400δ,forvaryingδ. Thisensuresthatthesignal
w
powerE[(X )⊤β(i)]2 andthesignaldifferenceE[(X )⊤β(i)−(X )⊤β(i+1)]2 fori ∈ {η }L−1 areheldconstantfor
i i i+1 ℓ ℓ=1
varyingδ. Therearetwochangepointsatn/3and8n/15. Withoutperfectknowledgeofthemagnitudeσ2 andsparsity
w
levelαofthesparsedifferencevector,AMPassumesσˆ2 =2500δandαˆ =0.9intheexperimentsinFigure5.
w
Additionalnumericalresults Figure7showsresultsfromanadditionalsetofexperimentscomparingAMPwithDCDP
(thefastestalgorithminFigure4),usingdifferentchangepointpriorsπ Ψ¯ (Figure7a)ordenoisersft(Figure7b). Inboth
figures,thesolidredplotshowsDCDPperformancewithhyperparameterschosenusingCV.SolidblackcorrespondstoAMP
usingthetruesignalprior. SolidgreencorrespondstoAMPusingL=3,theoptimaldenoiserft withthesparsitylevel
estimatedusingCV.InFigure7a,AMPperformsslightlyworsewithL=4insteadofL=3,becausethepriorassigns
non-zeroprobabilitytothechangepointconfigurationswithL =4signals,whichismismatchedfromthegroundtruth
L∗ =3. InFigure7b,AMPperformsslightlyworseatlowerδwhenusingasuboptimalsoftthresholding(ST)denoiser.
Nevertheless,inbothFigures7aand7b,AMPlargelyoutperformsDCDPdespitethesuboptimalchoicesofpriorordenoiser.
Figure8comparesAMPwithDCDPforn=500andvaryingp,bothusinghyperparameterschosenviacross-validation.
22InferringChangePointsinHigh-DimensionalLinearRegression
1.0 1.0
DCDP with CV DCDP with CV
0.8 AMP with CV, L=3 0.8 AMP with CV, L=3
AMP with CV, L=4 AMP with CV, L=3, ST
0.6 0.6
AMP with = =0.5, L=3 AMP with = =0.5, L=3
0.4 0.4
0.2 0.2
0.0 0.0
0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5
= n/p = n/p
(a)Solidpurple: AMPusingL = 3,4andthe (b) Dashed purple: AMP using L = 3 and a
optimaldenoiserftwiththesparsitylevelesti- suboptimalsoftthresholding(ST)denoiserft
matedusingCV. whosethresholdisselectedusingCV.
Figure7.ComparisonbetweenAMPandDCDPinthesamesettingasinFigure4: p = 200,σ = 0.1,L∗ = 3, sparsesignalprior
P B¯ =0.5N(0,δI)+0.5δ 0andAMPuses∆=n/10.Solidred:DCDPwithhyperparameterschosenusingCV.Solidgreen:AMPusing
L=3,theoptimaldenoiserftwiththesparsitylevelestimatedusingCV.Solidblack:AMPusingthetruesignalprior.
1.00
0.75
0.50
0.25
0.00
AMP
200 DCDP
100
0
1000 2000 3000 4000 5000
p
Figure8.Comparison between AMP and DCDP for fixed n = 500 and varying p, with σ = 0.1, L∗ = 3 and sparse signal prior
P B¯ =0.5N(0,δI)+0.5δ 0.AMPuses∆=n/10andL=3.
TheruntimeshownistheaverageruntimepersetofCVparameters. Forafixedsetofhyperparameters,thetimecomplexity
ofDCDPscalesasLASSO(n,p)comparedtoO(np)forAMP.TheruntimeofDCDPthereforegrowsfasterwithincreasingp.
F.ComputationalCost
Computingg∗t,f∗t+1 Fori∈[n],thefunctiong∗tinProposition3.2onlydependsonΨ¯ in(90)throughthemarginal
i
probabilityofΨ¯ i,i.e.,π Ψ¯
i
:ℓ(cid:55)→(cid:80) {ψ:ψi=ℓ}π Ψ¯(ψ). Thismeansg i∗t canbeefficientlycomputed,involvingonlyasum
overΨ¯ ∈[L]. Indeed,fromtheimplementationdetailsinAppendixC.2,bothf∗tandg∗tcanbecomputedinO(L3)time
i j i
foreachi,j. Thusf∗t andg∗t canbecomputedinO(nL3). Theper-iterationcomputationalcostofAMPistherefore
dominatedbythematrixmultiplicationsin(3),whichareO(npL).
Computingchangepointestimators ForestimatorsηˆwithO(np)runtime,thecombinedAMPandestimatorcomputation
canbemadetoruninO(np)timebyselectingdenoisersft,gt asinProposition3.2. Forexample,theargmaxin(27)
canbereplacedwithagreedybest-firstsearch: searchforthelocationofonechangepointatatime,conditioningonpast
estimatesoftheotherchangepoints. ThiswillyieldatmostLroundsofsearchingoverO(n)elements,resultinginO(np)
totalruntime.
Computingtheapproximateposterior Forlow-dimensionallatentrandomvariablesΨ,thedenominatorin(30)only
requiresevaluatingapolynomialnumberofterms,(cid:0)n(cid:1)
inthecaseofchangepointsignalconfigurations.Therefore,choosing
L
ft,gt as in Proposition 3.2, the computational complexity of computing p Ψ|V¯t,q(Z,Ψ,ε¯)(ψ|Θt,y) along with AMP is
Θ
23
ecnatsid
ffrodsuaH
ecnatsid
ffrodsuaH
)s(
emitnuR
ecnatsid
ffrodsuaHInferringChangePointsinHigh-DimensionalLinearRegression
O(npL+nL3+nL)=O(np+nL).
G.BackgroundonAMPforGeneralizedLinearModels(GLMs)
HerewereviewtheAMPalgorithmanditsstateevolutioncharacterizationfortheGLMwithoutchangepoints. Asin
Section3,letB ∈Rp×LbeasignalmatrixandletX ∈Rn×pbeadesignmatrix. Theobservationy ∈Rn×Lisproduced
as
y =q(XB,ε)∈Rn×L, (103)
whereε∈Rnisanoisevector,andq :RL →RLisaknownoutputfunction. Theonlydifferencebetweenthismodeland
theonein(2)istheabsenceofthesignalconfigurationvectorΨ. TheAMPalgorithmfortheGLMin(103)wasderived
byRangan(2011)forthecaseofvectorsignals(L = 1);seealsoSection4of(Fengetal.,2022). Herewediscussthe
algorithmforthegeneralcase(L≥1),whichcanbefoundin(Tan&Venkataramanan,2023). Foreaseofexposition,we
willmakethefollowingstandardassumption(see(S0)onp.5): asn,p→∞,theempiricaldistributionsof{B } and
j j∈[p]
{ε i} i∈[n]convergeweaklytolawsP B¯ andP ε¯,respectively,withboundedsecondmoments. Wealsorecallthatn/p→δas
n,p→∞.
TheAMPalgorithmforthemodel(103)isthesameastheonein(3),butduetotheassumptionabove,wecantakeft,gt
tobeseparable,i.e.,ft :RL →RLandgt :RL×RL →RLactrow-wiseontheirmatrixinputs. Then,thematricesFt
andCt in(3)canbesimplifiedtoCt = 1 (cid:80)n dgt(Θt,y )andFt = 1 (cid:80)p dft(Bt),wheredgt anddft denotethe
n i=1 i i n j=1 j
L×LJacobianswithrespecttothefirstargument.
Stateevolution Thememoryterms−Rˆt−1(Ft)⊤and−Bˆt(Ct)⊤in(3)debiastheiteratesΘtandBt+1andenablea
succinctdistributionalcharacterization,guaranteeingthattheirempiricaldistributionsconvergetowell-definedlimitsas
n,p→∞. Specifically,Theorem1in(Tan&Venkataramanan,2023)showsthatforeacht≥1,theempiricaldistribution
of the rows of Bt converges to the law of a random vector V˜t := B¯ν˜t +G˜t ∈ R1×L, where G˜t+1 ∼ N(0,κ˜t,t)
B B B B B
is independent of B¯ ∼ P B¯. Similarly, recalling that Θ = XB ∈ Rn×L, the empirical distribution of the rows of
(Θ, Θt) converges to the law of the random vectors (Z˜,Z˜ν˜t + G˜t ), where G˜t+1 ∼ N(0,κ˜t,t) is independent of
Θ Θ Θ Θ
Z˜ ∼N(cid:0) 0,δ(E[B¯B¯T])−1(cid:1) . ThedeterministicL×Lmatricesν˜t ,κ˜t,t,ν˜t,t,andκ˜t,tcanberecursivelycomputedfort≥1
B B Θ Θ
viaastateevolutionrecursionthatdependsonft,gtandthelimitinglawsP B¯ andP ε¯.
ThestateevolutioncharacterizationallowsustocomputeasymptoticperformancemeasuressuchastheMSEoftheAMP
algorithm. Indeed,foreacht ≥ 1,wealmostsurelyhavelim 1∥ft(Bt)−B∥2 = E[∥ft(V˜t)−B¯∥2],wherethe
p→∞ p F B 2
expectationontherightcanbecomputedusingthejointlawoftheL-dimensionalrandomvectorsB¯ andV˜t =B¯ν˜t +G˜t .
B B B
Inthemodel(3)withchangepoints,wehaveanadditionalsignalconfigurationvectorΨ,becauseofwhichwecannot
taketheAMPdenoisingfunctiongttobeseparable,evenunderAssumption(S0). Thisleadstoamorecomplicatedstate
evolutioncharacterization,asdescribedinSection3.
24