[
    {
        "title": "Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation with a Time Budget",
        "authors": "Glen NevilleJiazhen LiuSonia ChernovaHarish Ravichandar",
        "links": "http://arxiv.org/abs/2404.07902v1",
        "entry_id": "http://arxiv.org/abs/2404.07902v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07902v1",
        "summary": "Complex multi-objective missions require the coordination of heterogeneous\nrobots at multiple inter-connected levels, such as coalition formation,\nscheduling, and motion planning. The associated challenges are exacerbated when\nsolutions to these interconnected problems need to both maximize task\nperformance and respect practical constraints on time and resources. In this\nwork, we formulate a new class of spatio-temporal heterogeneous task allocation\nproblems that consider these complexities. We contribute a novel framework,\nnamed Quality-Optimized Incremental Task Allocation Graph Search (Q-ITAGS), to\nsolve such problems. Q-ITAGS builds upon our prior work in trait-based\ncoordination and offers a flexible interleaved framework that i) explicitly\nmodels and optimizes the effect of collective capabilities on task performance\nvia learnable trait-quality maps, and ii) respects both resource constraints\nand spatio-temporal constraints, including a user-specified time budget (i.e.,\nmaximum makespan). In addition to algorithmic contributions, we derive\ntheoretical suboptimality bounds in terms of task performance that varies as a\nfunction of a single hyperparameter. Our detailed experiments involving a\nsimulated emergency response task and a real-world video game dataset reveal\nthat i) Q-ITAGS results in superior team performance compared to a\nstate-of-the-art method, while also respecting complex spatio-temporal and\nresource constraints, ii) Q-ITAGS efficiently learns trait-quality maps to\nenable effective trade-off between task performance and resource constraints,\nand iii) Q-ITAGS' suboptimality bounds consistently hold in practice.",
        "updated": "2024-04-11 16:41:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07902v1"
    },
    {
        "title": "The Role of Confidence for Trust-based Resilient Consensus (Extended Version)",
        "authors": "Luca BallottaMichal Yemini",
        "links": "http://arxiv.org/abs/2404.07838v1",
        "entry_id": "http://arxiv.org/abs/2404.07838v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07838v1",
        "summary": "We consider a multi-agent system where agents aim to achieve a consensus\ndespite interactions with malicious agents that communicate misleading\ninformation. Physical channels supporting communication in cyberphysical\nsystems offer attractive opportunities to detect malicious agents,\nnevertheless, trustworthiness indications coming from the channel are subject\nto uncertainty and need to be treated with this in mind. We propose a resilient\nconsensus protocol that incorporates trust observations from the channel and\nweighs them with a parameter that accounts for how confident an agent is\nregarding its understanding of the legitimacy of other agents in the network,\nwith no need for the initial observation window $T_0$ that has been utilized in\nprevious works. Analytical and numerical results show that (i) our protocol\nachieves a resilient consensus in the presence of malicious agents and (ii) the\nsteady-state deviation from nominal consensus can be minimized by a suitable\nchoice of the confidence parameter that depends on the statistics of trust\nobservations.",
        "updated": "2024-04-11 15:27:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07838v1"
    },
    {
        "title": "Differentially Private Reinforcement Learning with Self-Play",
        "authors": "Dan QiaoYu-Xiang Wang",
        "links": "http://arxiv.org/abs/2404.07559v1",
        "entry_id": "http://arxiv.org/abs/2404.07559v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07559v1",
        "summary": "We study the problem of multi-agent reinforcement learning (multi-agent RL)\nwith differential privacy (DP) constraints. This is well-motivated by various\nreal-world applications involving sensitive data, where it is critical to\nprotect users' private information. We first extend the definitions of Joint DP\n(JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where\nboth definitions ensure trajectory-wise privacy protection. Then we design a\nprovably efficient algorithm based on optimistic Nash value iteration and\nprivatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP\nand LDP requirements when instantiated with appropriate privacy mechanisms.\nFurthermore, for both notions of DP, our regret bound generalizes the best\nknown result under the single-agent RL case, while our regret could also reduce\nto the best known result for multi-agent RL without privacy constraints. To the\nbest of our knowledge, these are the first line of results towards\nunderstanding trajectory-wise privacy protection in multi-agent RL.",
        "updated": "2024-04-11 08:42:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07559v1"
    },
    {
        "title": "WESE: Weak Exploration to Strong Exploitation for LLM Agents",
        "authors": "Xu HuangWeiwen LiuXiaolong ChenXingmei WangDefu LianYasheng WangRuiming TangEnhong Chen",
        "links": "http://arxiv.org/abs/2404.07456v1",
        "entry_id": "http://arxiv.org/abs/2404.07456v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07456v1",
        "summary": "Recently, large language models (LLMs) have demonstrated remarkable potential\nas an intelligent agent. However, existing researches mainly focus on enhancing\nthe agent's reasoning or decision-making abilities through well-designed prompt\nengineering or task-specific fine-tuning, ignoring the procedure of exploration\nand exploitation. When addressing complex tasks within open-world interactive\nenvironments, these methods exhibit limitations. Firstly, the lack of global\ninformation of environments leads to greedy decisions, resulting in sub-optimal\nsolutions. On the other hand, irrelevant information acquired from the\nenvironment not only adversely introduces noise, but also incurs additional\ncost. This paper proposes a novel approach, Weak Exploration to Strong\nExploitation (WESE), to enhance LLM agents in solving open-world interactive\ntasks. Concretely, WESE involves decoupling the exploration and exploitation\nprocess, employing a cost-effective weak agent to perform exploration tasks for\nglobal knowledge. A knowledge graph-based strategy is then introduced to store\nthe acquired knowledge and extract task-relevant knowledge, enhancing the\nstronger agent in success rate and efficiency for the exploitation task. Our\napproach is flexible enough to incorporate diverse tasks, and obtains\nsignificant improvements in both success rates and efficiency across four\ninteractive benchmarks.",
        "updated": "2024-04-11 03:31:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07456v1"
    },
    {
        "title": "Multi-Agent Soft Actor-Critic with Global Loss for Autonomous Mobility-on-Demand Fleet Control",
        "authors": "Zeno WoywoodJasper I. WiltfangJulius LuyTobias EndersMaximilian Schiffer",
        "links": "http://arxiv.org/abs/2404.06975v1",
        "entry_id": "http://arxiv.org/abs/2404.06975v1",
        "pdf_url": "http://arxiv.org/pdf/2404.06975v1",
        "summary": "We study a sequential decision-making problem for a profit-maximizing\noperator of an Autonomous Mobility-on-Demand system. Optimizing a central\noperator's vehicle-to-request dispatching policy requires efficient and\neffective fleet control strategies. To this end, we employ a multi-agent Soft\nActor-Critic algorithm combined with weighted bipartite matching. We propose a\nnovel vehicle-based algorithm architecture and adapt the critic's loss function\nto appropriately consider global actions. Furthermore, we extend our algorithm\nto incorporate rebalancing capabilities. Through numerical experiments, we show\nthat our approach outperforms state-of-the-art benchmarks by up to 12.9% for\ndispatching and up to 38.9% with integrated rebalancing.",
        "updated": "2024-04-10 13:49:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.06975v1"
    }
]