[
    {
        "title": "Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos",
        "authors": "Zhengze XuMengting ChenZhao WangLinyu XingZhonghua ZhaiNong SangJinsong LanShuai XiaoChangxin Gao",
        "links": "http://arxiv.org/abs/2404.17571v1",
        "entry_id": "http://arxiv.org/abs/2404.17571v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17571v1",
        "summary": "Video try-on is a challenging task and has not been well tackled in previous\nworks. The main obstacle lies in preserving the details of the clothing and\nmodeling the coherent motions simultaneously. Faced with those difficulties, we\naddress video try-on by proposing a diffusion-based framework named \"Tunnel\nTry-on.\" The core idea is excavating a \"focus tunnel\" in the input video that\ngives close-up shots around the clothing regions. We zoom in on the region in\nthe tunnel to better preserve the fine details of the clothing. To generate\ncoherent motions, we first leverage the Kalman filter to construct smooth crops\nin the focus tunnel and inject the position embedding of the tunnel into\nattention layers to improve the continuity of the generated videos. In\naddition, we develop an environment encoder to extract the context information\noutside the tunnels as supplementary cues. Equipped with these techniques,\nTunnel Try-on keeps the fine details of the clothing and synthesizes stable and\nsmooth videos. Demonstrating significant advancements, Tunnel Try-on could be\nregarded as the first attempt toward the commercial-level application of\nvirtual try-on in videos.",
        "updated": "2024-04-26 17:55:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17571v1"
    },
    {
        "title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes",
        "authors": "Shangzhan ZhangSida PengTao XuYuanbo YangTianrun ChenNan XueYujun ShenHujun BaoRuizhen HuXiaowei Zhou",
        "links": "http://arxiv.org/abs/2404.17569v1",
        "entry_id": "http://arxiv.org/abs/2404.17569v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17569v1",
        "summary": "This paper aims to generate materials for 3D meshes from text descriptions.\nUnlike existing methods that synthesize texture maps, we propose to generate\nsegment-wise procedural material graphs as the appearance representation, which\nsupports high-quality rendering and provides substantial flexibility in\nediting. Instead of relying on extensive paired data, i.e., 3D meshes with\nmaterial graphs and corresponding text descriptions, to train a material graph\ngenerative model, we propose to leverage the pre-trained 2D diffusion model as\na bridge to connect the text and material graphs. Specifically, our approach\ndecomposes a shape into a set of segments and designs a segment-controlled\ndiffusion model to synthesize 2D images that are aligned with mesh parts. Based\non generated images, we initialize parameters of material graphs and fine-tune\nthem through the differentiable rendering module to produce materials in\naccordance with the textual description. Extensive experiments demonstrate the\nsuperior performance of our framework in photorealism, resolution, and\neditability over existing methods. Project page:\nhttps://zhanghe3z.github.io/MaPa/",
        "updated": "2024-04-26 17:54:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17569v1"
    },
    {
        "title": "ChangeBind: A Hybrid Change Encoder for Remote Sensing Change Detection",
        "authors": "Mubashir NomanMustansar FiazHisham Cholakkal",
        "links": "http://arxiv.org/abs/2404.17565v1",
        "entry_id": "http://arxiv.org/abs/2404.17565v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17565v1",
        "summary": "Change detection (CD) is a fundamental task in remote sensing (RS) which aims\nto detect the semantic changes between the same geographical regions at\ndifferent time stamps. Existing convolutional neural networks (CNNs) based\napproaches often struggle to capture long-range dependencies. Whereas recent\ntransformer-based methods are prone to the dominant global representation and\nmay limit their capabilities to capture the subtle change regions due to the\ncomplexity of the objects in the scene. To address these limitations, we\npropose an effective Siamese-based framework to encode the semantic changes\noccurring in the bi-temporal RS images. The main focus of our design is to\nintroduce a change encoder that leverages local and global feature\nrepresentations to capture both subtle and large change feature information\nfrom multi-scale features to precisely estimate the change regions. Our\nexperimental study on two challenging CD datasets reveals the merits of our\napproach and obtains state-of-the-art performance.",
        "updated": "2024-04-26 17:47:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17565v1"
    },
    {
        "title": "Exploring the Distinctiveness and Fidelity of the Descriptions Generated by Large Vision-Language Models",
        "authors": "Yuhang HuangZihan WuChongyang GaoJiawei PengXu Yang",
        "links": "http://arxiv.org/abs/2404.17534v1",
        "entry_id": "http://arxiv.org/abs/2404.17534v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17534v1",
        "summary": "Large Vision-Language Models (LVLMs) are gaining traction for their\nremarkable ability to process and integrate visual and textual data. Despite\ntheir popularity, the capacity of LVLMs to generate precise, fine-grained\ntextual descriptions has not been fully explored. This study addresses this gap\nby focusing on \\textit{distinctiveness} and \\textit{fidelity}, assessing how\nmodels like Open-Flamingo, IDEFICS, and MiniGPT-4 can distinguish between\nsimilar objects and accurately describe visual features. We proposed the\nTextual Retrieval-Augmented Classification (TRAC) framework, which, by\nleveraging its generative capabilities, allows us to delve deeper into\nanalyzing fine-grained visual description generation. This research provides\nvaluable insights into the generation quality of LVLMs, enhancing the\nunderstanding of multimodal language models. Notably, MiniGPT-4 stands out for\nits better ability to generate fine-grained descriptions, outperforming the\nother two models in this aspect. The code is provided at\n\\url{https://anonymous.4open.science/r/Explore_FGVDs-E277}.",
        "updated": "2024-04-26 16:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17534v1"
    },
    {
        "title": "Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields",
        "authors": "Tianqi LiuXinyi YeMin ShiZihao HuangZhiyu PanZhan PengZhiguo Cao",
        "links": "http://arxiv.org/abs/2404.17528v1",
        "entry_id": "http://arxiv.org/abs/2404.17528v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17528v1",
        "summary": "Generalizable NeRF aims to synthesize novel views for unseen scenes. Common\npractices involve constructing variance-based cost volumes for geometry\nreconstruction and encoding 3D descriptors for decoding novel views. However,\nexisting methods show limited generalization ability in challenging conditions\ndue to inaccurate geometry, sub-optimal descriptors, and decoding strategies.\nWe address these issues point by point. First, we find the variance-based cost\nvolume exhibits failure patterns as the features of pixels corresponding to the\nsame point can be inconsistent across different views due to occlusions or\nreflections. We introduce an Adaptive Cost Aggregation (ACA) approach to\namplify the contribution of consistent pixel pairs and suppress inconsistent\nones. Unlike previous methods that solely fuse 2D features into descriptors,\nour approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D\ncontext into descriptors through spatial and inter-view interaction. When\ndecoding the descriptors, we observe the two existing decoding strategies excel\nin different areas, which are complementary. A Consistency-Aware Fusion (CAF)\nstrategy is proposed to leverage the advantages of both. We incorporate the\nabove ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware\nReconstruction and Fusion-refined Rendering (GeFu). GeFu attains\nstate-of-the-art performance across multiple datasets. Code is available at\nhttps://github.com/TQTQliu/GeFu .",
        "updated": "2024-04-26 16:46:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17528v1"
    }
]