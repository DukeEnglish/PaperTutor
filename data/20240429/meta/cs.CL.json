[
    {
        "title": "A Semi-Automatic Approach to Create Large Gender- and Age-Balanced Speaker Corpora: Usefulness of Speaker Diarization & Identification",
        "authors": "Rémi UroDavid DoukhanAlbert RilliardLaëtitia LarcherAnissa-Claire AdgharouamaneMarie TahonAntoine Laurent",
        "links": "http://arxiv.org/abs/2404.17552v1",
        "entry_id": "http://arxiv.org/abs/2404.17552v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17552v1",
        "summary": "This paper presents a semi-automatic approach to create a diachronic corpus\nof voices balanced for speaker's age, gender, and recording period, according\nto 32 categories (2 genders, 4 age ranges and 4 recording periods). Corpora\nwere selected at French National Institute of Audiovisual (INA) to obtain at\nleast 30 speakers per category (a total of 960 speakers; only 874 have be found\nyet). For each speaker, speech excerpts were extracted from audiovisual\ndocuments using an automatic pipeline consisting of speech detection,\nbackground music and overlapped speech removal and speaker diarization, used to\npresent clean speaker segments to human annotators identifying target speakers.\nThis pipeline proved highly effective, cutting down manual processing by a\nfactor of ten. Evaluation of the quality of the automatic processing and of the\nfinal output is provided. It shows the automatic processing compare to\nup-to-date process, and that the output provides high quality speech for most\nof the selected excerpts. This method shows promise for creating large corpora\nof known target speakers.",
        "updated": "2024-04-26 17:30:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17552v1"
    },
    {
        "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
        "authors": "Stephen ZhaoRob BrekelmansAlireza MakhzaniRoger Grosse",
        "links": "http://arxiv.org/abs/2404.17546v1",
        "entry_id": "http://arxiv.org/abs/2404.17546v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17546v1",
        "summary": "Numerous capability and safety techniques of Large Language Models (LLMs),\nincluding RLHF, automated red-teaming, prompt engineering, and infilling, can\nbe cast as sampling from an unnormalized target distribution defined by a given\nreward or potential function over the full sequence. In this work, we leverage\nthe rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic\ninference problems. In particular, we use learned twist functions to estimate\nthe expected future value of the potential at each timestep, which enables us\nto focus inference-time computation on promising partial sequences. We propose\na novel contrastive method for learning the twist functions, and establish\nconnections with the rich literature of soft reinforcement learning. As a\ncomplementary application of our twisted SMC framework, we present methods for\nevaluating the accuracy of language model inference techniques using novel\nbidirectional SMC bounds on the log partition function. These bounds can be\nused to estimate the KL divergence between the inference and target\ndistributions in both directions. We apply our inference evaluation techniques\nto show that twisted SMC is effective for sampling undesirable outputs from a\npretrained model (a useful component of harmlessness training and automated\nred-teaming), generating reviews with varied sentiment, and performing\ninfilling tasks.",
        "updated": "2024-04-26 17:18:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17546v1"
    },
    {
        "title": "Large Language Model Agent as a Mechanical Designer",
        "authors": "Yayati JadhavAmir Barati Farimani",
        "links": "http://arxiv.org/abs/2404.17525v1",
        "entry_id": "http://arxiv.org/abs/2404.17525v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17525v1",
        "summary": "Conventional mechanical design paradigms rely on experts systematically\nrefining concepts through experience-guided modification and FEA to meet\nspecific requirements. However, this approach can be time-consuming and heavily\ndependent on prior knowledge and experience. While numerous machine learning\nmodels have been developed to streamline this intensive and expert-driven\niterative process, these methods typically demand extensive training data and\nconsiderable computational resources. Furthermore, methods based on deep\nlearning are usually restricted to the specific domains and tasks for which\nthey were trained, limiting their applicability across different tasks. This\ncreates a trade-off between the efficiency of automation and the demand for\nresources. In this study, we present a novel approach that integrates\npre-trained LLMs with a FEM module. The FEM module evaluates each design and\nprovides essential feedback, guiding the LLMs to continuously learn, plan,\ngenerate, and optimize designs without the need for domain-specific training.\nWe demonstrate the effectiveness of our proposed framework in managing the\niterative optimization of truss structures, showcasing its capability to reason\nabout and refine designs according to structured feedback and criteria. Our\nresults reveal that these LLM-based agents can successfully generate truss\ndesigns that comply with natural language specifications with a success rate of\nup to 90%, which varies according to the applied constraints. By employing\nprompt-based optimization techniques we show that LLM based agents exhibit\noptimization behavior when provided with solution-score pairs to iteratively\nrefine designs to meet specifications. This ability of LLM agents to produce\nviable designs and optimize them based on their inherent reasoning capabilities\nhighlights their potential to develop and implement effective design strategies\nautonomously.",
        "updated": "2024-04-26 16:41:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17525v1"
    },
    {
        "title": "On the Use of Large Language Models to Generate Capability Ontologies",
        "authors": "Luis Miguel Vieira da SilvaAljosha KöcherFelix GehlhoffAlexander Fay",
        "links": "http://arxiv.org/abs/2404.17524v1",
        "entry_id": "http://arxiv.org/abs/2404.17524v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17524v1",
        "summary": "Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors.",
        "updated": "2024-04-26 16:41:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17524v1"
    },
    {
        "title": "A Comprehensive Evaluation on Event Reasoning of Large Language Models",
        "authors": "Zhengwei TaoZhi JinYifan ZhangXiancai ChenXiaoying BaiYue FangHaiyan ZhaoJia LiChongyang Tao",
        "links": "http://arxiv.org/abs/2404.17513v1",
        "entry_id": "http://arxiv.org/abs/2404.17513v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17513v1",
        "summary": "Event reasoning is a fundamental ability that underlies many applications. It\nrequires event schema knowledge to perform global reasoning and needs to deal\nwith the diversity of the inter-event relations and the reasoning paradigms.\nHow well LLMs accomplish event reasoning on various relations and reasoning\nparadigms remains unknown. To mitigate this disparity, we comprehensively\nevaluate the abilities of event reasoning of LLMs. We introduce a novel\nbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of\nevaluation of schema and instance and is comprehensive in relations and\nreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs\nhave abilities to accomplish event reasoning but their performances are far\nfrom satisfactory. We also notice the imbalance of event reasoning abilities in\nLLMs. Besides, LLMs have event schema knowledge, however, they're not aligned\nwith humans on how to utilize the knowledge. Based on these findings, we\nintroduce two methods to guide the LLMs to utilize the event schema knowledge.\nBoth methods achieve improvements.",
        "updated": "2024-04-26 16:28:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17513v1"
    }
]