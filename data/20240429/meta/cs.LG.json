[
    {
        "title": "An exactly solvable model for emergence and scaling laws",
        "authors": "Yoonsoo NamNayara FonsecaSeok Hyeong LeeArd Louis",
        "links": "http://arxiv.org/abs/2404.17563v1",
        "entry_id": "http://arxiv.org/abs/2404.17563v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17563v1",
        "summary": "Deep learning models can exhibit what appears to be a sudden ability to solve\na new problem as training time ($T$), training data ($D$), or model size ($N$)\nincreases, a phenomenon known as emergence. In this paper, we present a\nframework where each new ability (a skill) is represented as a basis function.\nWe solve a simple multi-linear model in this skill-basis, finding analytic\nexpressions for the emergence of new skills, as well as for scaling laws of the\nloss with training time, data size, model size, and optimal compute ($C$). We\ncompare our detailed calculations to direct simulations of a two-layer neural\nnetwork trained on multitask sparse parity, where the tasks in the dataset are\ndistributed according to a power-law. Our simple model captures, using a single\nfit parameter, the sigmoidal emergence of multiple new skills as training time,\ndata size or model size increases in the neural network.",
        "updated": "2024-04-26 17:45:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17563v1"
    },
    {
        "title": "Federated Transfer Component Analysis Towards Effective VNF Profiling",
        "authors": "Xunzheng ZhangBShadi MoazzeniJuan Marcelo Parra-UllauriReza NejabatiDimitra Simeonidou",
        "links": "http://arxiv.org/abs/2404.17553v1",
        "entry_id": "http://arxiv.org/abs/2404.17553v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17553v1",
        "summary": "The increasing concerns of knowledge transfer and data privacy challenge the\ntraditional gather-and-analyse paradigm in networks. Specifically, the\nintelligent orchestration of Virtual Network Functions (VNFs) requires\nunderstanding and profiling the resource consumption. However, profiling all\nkinds of VNFs is time-consuming. It is important to consider transferring the\nwell-profiled VNF knowledge to other lack-profiled VNF types while keeping data\nprivate. To this end, this paper proposes a Federated Transfer Component\nAnalysis (FTCA) method between the source and target VNFs. FTCA first trains\nGenerative Adversarial Networks (GANs) based on the source VNF profiling data,\nand the trained GANs model is sent to the target VNF domain. Then, FTCA\nrealizes federated domain adaptation by using the generated source VNF data and\nless target VNF profiling data, while keeping the raw data locally. Experiments\nshow that the proposed FTCA can effectively predict the required resources for\nthe target VNF. Specifically, the RMSE index of the regression model decreases\nby 38.5% and the R-squared metric advances up to 68.6%.",
        "updated": "2024-04-26 17:31:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17553v1"
    },
    {
        "title": "A Semi-Automatic Approach to Create Large Gender- and Age-Balanced Speaker Corpora: Usefulness of Speaker Diarization & Identification",
        "authors": "Rémi UroDavid DoukhanAlbert RilliardLaëtitia LarcherAnissa-Claire AdgharouamaneMarie TahonAntoine Laurent",
        "links": "http://arxiv.org/abs/2404.17552v1",
        "entry_id": "http://arxiv.org/abs/2404.17552v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17552v1",
        "summary": "This paper presents a semi-automatic approach to create a diachronic corpus\nof voices balanced for speaker's age, gender, and recording period, according\nto 32 categories (2 genders, 4 age ranges and 4 recording periods). Corpora\nwere selected at French National Institute of Audiovisual (INA) to obtain at\nleast 30 speakers per category (a total of 960 speakers; only 874 have be found\nyet). For each speaker, speech excerpts were extracted from audiovisual\ndocuments using an automatic pipeline consisting of speech detection,\nbackground music and overlapped speech removal and speaker diarization, used to\npresent clean speaker segments to human annotators identifying target speakers.\nThis pipeline proved highly effective, cutting down manual processing by a\nfactor of ten. Evaluation of the quality of the automatic processing and of the\nfinal output is provided. It shows the automatic processing compare to\nup-to-date process, and that the output provides high quality speech for most\nof the selected excerpts. This method shows promise for creating large corpora\nof known target speakers.",
        "updated": "2024-04-26 17:30:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17552v1"
    },
    {
        "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
        "authors": "Stephen ZhaoRob BrekelmansAlireza MakhzaniRoger Grosse",
        "links": "http://arxiv.org/abs/2404.17546v1",
        "entry_id": "http://arxiv.org/abs/2404.17546v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17546v1",
        "summary": "Numerous capability and safety techniques of Large Language Models (LLMs),\nincluding RLHF, automated red-teaming, prompt engineering, and infilling, can\nbe cast as sampling from an unnormalized target distribution defined by a given\nreward or potential function over the full sequence. In this work, we leverage\nthe rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic\ninference problems. In particular, we use learned twist functions to estimate\nthe expected future value of the potential at each timestep, which enables us\nto focus inference-time computation on promising partial sequences. We propose\na novel contrastive method for learning the twist functions, and establish\nconnections with the rich literature of soft reinforcement learning. As a\ncomplementary application of our twisted SMC framework, we present methods for\nevaluating the accuracy of language model inference techniques using novel\nbidirectional SMC bounds on the log partition function. These bounds can be\nused to estimate the KL divergence between the inference and target\ndistributions in both directions. We apply our inference evaluation techniques\nto show that twisted SMC is effective for sampling undesirable outputs from a\npretrained model (a useful component of harmlessness training and automated\nred-teaming), generating reviews with varied sentiment, and performing\ninfilling tasks.",
        "updated": "2024-04-26 17:18:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17546v1"
    },
    {
        "title": "Using Neural Implicit Flow To Represent Latent Dynamics Of Canonical Systems",
        "authors": "Imran NasimJoaõ Lucas de Sousa Almeida",
        "links": "http://arxiv.org/abs/2404.17535v1",
        "entry_id": "http://arxiv.org/abs/2404.17535v1",
        "pdf_url": "http://arxiv.org/pdf/2404.17535v1",
        "summary": "The recently introduced class of architectures known as Neural Operators has\nemerged as highly versatile tools applicable to a wide range of tasks in the\nfield of Scientific Machine Learning (SciML), including data representation and\nforecasting. In this study, we investigate the capabilities of Neural Implicit\nFlow (NIF), a recently developed mesh-agnostic neural operator, for\nrepresenting the latent dynamics of canonical systems such as the\nKuramoto-Sivashinsky (KS), forced Korteweg-de Vries (fKdV), and Sine-Gordon\n(SG) equations, as well as for extracting dynamically relevant information from\nthem. Finally we assess the applicability of NIF as a dimensionality reduction\nalgorithm and conduct a comparative analysis with another widely recognized\nfamily of neural operators, known as Deep Operator Networks (DeepONets).",
        "updated": "2024-04-26 17:01:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.17535v1"
    }
]