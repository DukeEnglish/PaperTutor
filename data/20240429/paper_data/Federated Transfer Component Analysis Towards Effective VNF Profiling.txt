Federated Transfer Component Analysis Towards
Effective VNF Profiling
(cid:66)
Xunzheng Zhang , Shadi Moazzeni, Juan Marcelo Parra-Ullauri, Reza Nejabati, and Dimitra Simeonidou
High Performance Networks Research Group, Smart Internet Lab, Faculty of Engineering,
University of Bristol, Bristol, Clifton BS8 1UB, UK
Email: {xunzheng.zhang, shadi.moazzeni, jm.parraullauri, Reza.Nejabati, Dimitra.Simeonidou}@bristol.ac.uk
Abstract‚ÄîThe increasing concerns of knowledge transfer
Client C
and data privacy challenge the traditional gather-and-analyse
paradigm in networks. Specifically, the intelligent orchestration
of Virtual Network Functions (VNFs) requires understanding
and profiling the resource consumption. However, profiling all Models
kinds of VNFs is time-consuming. It is important to consider Exchange
transferring the well-profiled VNF knowledge to other lack- Client B Client A
profiled VNF types while keeping data private. To this end,
this paper proposes a Federated Transfer Component Analysis
(FTCA) method between the source and target VNFs. FTCA FTL first trains Generative Adversarial Networks (GANs) based on Data from B
the source VNF profiling data, and the trained GANs model is
sent to the target VNF domain. Then, FTCA realizes federated Data from A
domainadaptationbyusingthegeneratedsourceVNFdataand
lesstargetVNFprofilingdata,whilekeepingtherawdatalocally. Features
ExperimentsshowthattheproposedFTCAcaneffectivelypredict
therequiredresourcesforthetargetVNF.Specifically,theRMSE Fig.1. Federatedtransferlearningwiththedecentralizedarchitecture.
index of the regression model decreases by 38.5% and the R-
squared metric advances up to 68.6%.
Index Terms‚ÄîFederated Transfer Learning, VNF Profiling, providing efficient resource allocation and performance plans,
GANs, Domain Adaptation, Network Function Virtualization
toensureserviceperformancerequirementsinthenetwork[4].
However, profiling all possible resource configurations for
I. INTRODUCTION
each VNF takes time, and collecting the profiling data sets
In Virtual Network Functions (VNFs), network functions from all different VNF types is costly and difficult. Many
are decoupled from proprietary hardware and implemented as possible configurations lead to a multiplicative growth rate
software that can run on standard network infrastructure, such of the profiling test time [5]. In this case, if we can transfer
as firewalls, load balancers, and routers [1]. This allows for thealreadyprofiledVNF(sourceVNF)resourceconfiguration
more flexibility and agility in deploying and scaling network knowledge to another less-profiled VNF (target VNF) type
services. Especially for the next generation of networks (6G), and predict its needed resources, the target VNF may save
orchestrators in the Network Function Virtualization (NFV) more profiling time and effort. This motivation will make the
architecture should automatically deploy, configure, and scale profiling of new VNFs easier, and quickly get the needed
different VNFs [2]. In this case, the needed resources are resourcesforthetargetVNFtype.Furthermore,differentVNF
allocated and interconnected to meet the requirements of types are typically considered distinct entities. Each type of
specific network services. This progress needs VNF profiling. VNFrepresentsaspecificnetworkfunction.TheseVNFtypes
VNF profiling [3] refers to the process of analyzing and cannot directly exchange their profiling data because of data
characterizing the resource requirements and performance privacy concerns. Therefore, this paper focuses on how to
characteristics of VNFs. Profiling involves gathering insights transfer the profiling knowledge from the source VNF to the
anddataabouthowaspecificVNFutilizescomputing,storage, target VNF, while keeping the raw profiling data locally.
and network resources, as well as its overall performance To deal with the above-mentioned problems, decentralized
in a virtualization environment. Considering the pros, VNF federated learning (FL) [6] architecture is considered. As
profiling is proposed as a solution to model the observed shown in Fig. 1, each VNF type is considered as one client.
trends. It provides a model to accurately describe how the Firstly,thealreadyprofiledsourceVNFtrainsadatasynthetic
specific VNF reacts under a certain amount of workload and GANs [7]‚Äì[9] model based on its enough profiling data set.
specificresourceconfigurations.VNFprofilingisdedicatedto Then this model (or its parameters) will be sent to the target
VNF client to generate the synthetic data which is similar
ThisworkwassupportedbytheChinaScholarshipCouncilandtheFuture
to the source VNF profiling data. Secondly, transfer learning
Open Network Research Challenge (FONRC) call, the project Realizing
EnablingArchitecturesandSolutionsforOpenNetworks(REASON),UK. (TL) theories, or domain adaptations [10] are applied at the
4202
rpA
62
]CD.sc[
1v35571.4042:viXra
selpmaS
LabelsTarget domain Source domain Next-generation intelligent VIM
Client EM xco hd ae nls ge Client NFV NM FA VN OO Traf Vfic N FGen.
VNF2 VNF1 Profiles
Performance Target VNFs VNFM VNF
G G G D goals Profiling models instance
Future Demand
Assessment Traffic Rec.
VNF
Resource & Network Monitor
Data from Domain adaption of
VNF 2 FTCA FTCA two VNF types
(offline stage) Federated Transfer
Data from Component Analysis Limited Data G of VNF 1 from the
ùëæùëæ target VNF
Features
Fig. 2. FTCA for VNF profiling. G is the data generator model. D is the
Fig.3. IntelligentNFVMANOwithFTCAVNFprofilinginteraction.
discriminator.Wisthemappingmatrix.AfterFTCA,targetdomaindataand
‚Äôsourcedomain‚Äôdatahavesimilardistributions.
Federated
target VNF domain (Fig. 2). After the kernel matrix operation Transfer
Component
in the target VNF, the generated source VNF data and less Analysis
(FTCA)
target VNF profiling data are mapped into a new space where
they have similar distributions. Then in this new space, the
regression machine learning (ML) model can be trained to
predict the labels (resource configurations of VNFs like CPU
cores,memory,etc)thathavebeenprofiledinthesourceVNF
Fig.4. FeaturedistributionafterfederateddomainadaptationFTCA.
butnotyetprofiledinthetargetVNF.Combiningtwolearning
methods, federated transfer component analysis (FTCA) is
proposed in this paper as a novel method under federated
randomly Autonomous Profiler selects a configuration of re-
transfer learning (FTL).
sources and requests the orchestrator to assign them to the
The main contributions of this paper can be summarized as
VNFunderprofiling.Itthenfindsthemaximumtrafficratethat
follows:
the VNF can handle while meeting the Key Performance In-
(1) A novel FTCA method is proposed considering the
dicators (KPIs) and records this performance data. The offline
profiling knowledge transfer among different VNF types. Un-
profiling process and profiling data set generation continue
like the model-based FTL methods [11]‚Äì[13], FTCA directly
until the profiling time is completed. Moreover, in [4], we
adopts the feature-based transfer component analysis (TCA)
introducedaframeworkforanalyzingVNFs,focusingontheir
[10]inafederatedmanner,withgeneratedsourcedomaindata
resource characteristics, their VNF-level service performance,
from GANs (Fig. 2). FTCA not only controls the complexity
as well as the discovery of resource-performance correlations.
of TCA but also keeps the raw profiling data locally. The
Fig. 3 shows the interaction between the intelligent NFV
targetVNFcanmakedomainadaptationbasedonitslocaldata
MANO (Network Functions Virtualization Management and
and the generated source VNF data, then predict the needed
Orchestration), the traffic generator, network and computing
resource configuration.
resources monitoring tools, the VNF under profiling, our
(2) The work involves multiple VNF profiling knowledge
proposed FTCA method.
transfer tasks which include SNORT Inline, SNORT Passive
and virtual firewall (vFW) VNF types. Multiple regression B. Federated Transfer Learning
models are trained after FTCA to predict the resource config- Considering the data collection process of training and test
urations (CPU cores, memory and link capacity) of the target sets are not in the same working place, FTL aims to improve
VNF. Experiment results show the effectiveness of the FTCA. the generalization ability of ML models, with data privacy
Theremainderofthispaperisorganizedasfollows.Section concerns.ThereareprimarilytwomethodsofFTLinexisting
II describes the related work. Section III details the FTCA research. FL first, then TL like [6], [11]. Another is TL
method. The experiments and analysis are given in Section first, followed by FL [12], [13]. However, these works mostly
IV. The conclusions and future work are drawn in Section V. emphasize parameter-based TL (pre-training and fine-tuning
the model). This paper directly focuses on feature-based TL
II. BACKGROUNDANDRELATEDWORK
(TCA) [10] which aims to reduce the difference between the
A. Intelligent VNF Profiling
targetandgeneratedsourcedomain,butinafederatedmanner.
Regarding the contribution to the field of intelligent VNF Thus, to the best of our knowledge, this is the first work
profiling, our Novel Autonomous Profiling (NAP) method combining TCA in TL with decentralized FL as FTCA. Fig.
introduces an autonomous offline method for profiling VNFs 4 shows the data distribution change in our work. The blue
[3]. More precisely, utilizing our NAP method, the weighted and red histograms represent the target VNF domain and the
selpmaS
Labels detareneG morf
seliforp
FNV
ecruos
ehtB. Federated Transfer Component Analysis
Source Domain Machine
Pro Df ail te ad
s
eV tNF ùíÄùíÄùë∫ùë∫ Trans Mf ao tr rm ixation RL ee g Ma r oer dn s ei sn liog n cliF enT tC inA Fc Lon as ni dde dr os eg se Tn Cer Aato atr tm heod tae rl gs ef tr co lm ient th .e Ai fs to el ra gte ed nes ro au tir nc ge
ùëøùëøùë∫ùë∫ ùëæùëæùëøùëøùë∫ùë∫
Offline the ‚Äòsource data‚Äô at the target VNF, TCA transforms the
Federated features into a new feature space by a matrix, where the
CTGAN Transfer
Component difference between source and target data is smaller. The
Target Domùëøùëøùíêùíê ain ùëøùëøùë∫ùë∫ Analysis Maximum Mean Discrepancy (MMD) [10] is used as an
Online
VN DF aP tr ao sf eil ting ùëøùëøùëªùëª
Transformation
indicator to evaluate the distance between the source and
ùëøùëøùëªùëª Matrix ùëæùëæùëøùëøùëªùëª Model target domain. It describes the kernel mapping distance in
Performance Prediction a Reproducing Kernel Hilbert Space (RKHS). The detailed
Verification ùíÄùíÄùëªùëª Evaluation ùíÄùíÄùëªùëª‚Ä≤ expression of MMD is:
Fig.5. ThedetailedFTCAprocessinVNFprofiling. MMD(X ,X
)=(cid:13) (cid:13)
(cid:13)
1 (cid:88)ns
œï(x )‚àí
1 (cid:88)nt
œï(x
)(cid:13) (cid:13) (cid:13)2
(2)
s t (cid:13)n Si n Ti (cid:13)
(cid:13) s t (cid:13)
i=1 i=1 H
generated source VNF domain. After FTCA, the distributions The generated source domain data set that join the FTCA
are more similar. is set as X s = {x Si} = {x 1,...,x ns}. The number of
generated source samples n can be controlled by the target
s
III. FEDERATEDTRANSFERCOMPONENT VNFclient.ThetargetdomaindatasetissetasX ={x }=
t Ti
ANALYSIS {x ,...,x }. X ‚àà D and X ‚àà D . We assume that
1 nt s S t T
A. GANs Model Delivery the conditional probability P(Y s|X s) ‚âà P(Y t|X t). Y s and
Y are resource configuration labels of VNFs, Y has been
t s
In GANs training, two neural networks compete in a two-
profiledinthesourceVNFbutY hasnotbeenprofiledinthe
player min-max game to simultaneously train a generator G t
target VNF. S(s) and T(t) represent the source domain and
and a discriminator D. The goal of the generator G(zzz;Œ∏ )
g target domain respectively, œï function is the feature mapping
is to learn a distribution p (zzz) over data xxx, by mapping
g corresponding the kernel map K(x ,x )=<œï(x ),œï(x )>.
input noise zzz ‚àº p (zzz) to real samples xxx. p (zzz) is usually i j i j
zzz zzz A semi-defined adaptation matrix W can also denote the
an easy-to-sample distribution like the Gaussian distribution
MMD(X ,X ) feature transformation as follows:
with(0,1).Meanwhile,thediscriminatorD(xxx,Œ∏ )istrainedto s t
d
discriminatebetweentherealsamplesxxxandgeneratedsamples (cid:13) (cid:13) 1 (cid:88)ns 1 (cid:88)nt (cid:13) (cid:13)2
G(zzz). The value function can be described as: (cid:13) (cid:13)n WTx Si ‚àí n WTx Ti(cid:13) (cid:13) (3)
(cid:13) s t (cid:13)
i=1 i=1 H
min max V(D,G)=E [logD(xxx)] According to the property of trace in matrix, (2) and (3)
G D
xxx‚àºpdata(xxx)
(1) can be separately simplified as MMD(X ,X )=tr(KL) and
s t
+E zzz‚àºpzzz(zzz)[log(1‚àíD(G(zzz)))] MMD(X s,X t) = tr(cid:0) WTXLXTW(cid:1) . Here X = [X s,X t].
K is Gram matrices defined on the generated source domain,
In general, D strives to discriminate the real samples,
targetdomain,andcross-domaindataintheembeddedspaces.
simultaneously training G to make D cannot discriminate
tr is the trace of a matrix. The K and L are shown below:
between the raw data and the generated data. This training
process will continue until the generated data successfully
(cid:20) (cid:21)
cheats the discriminator. Some typical GANs models have
K=
<œï(x s),œï(x s)> <œï(x s),œï(x t)>
beenproposedtogeneratetabulardatalikeTableGAN[9]and <œï(x t),œï(x s)> <œï(x t),œï(x t)>
(4)
CTGAN [8]. TableGAN uses GANs to synthesize fake tables (cid:20) K K (cid:21)
= s,s s,t ‚ààR(ns+nt)√ó(ns+nt)
that are statistically similar to the original table yet do not K K
t,s t,t
incur raw data leakage. CTGAN designs a conditional gen-
(cid:34) (cid:35)
erator and training-by-sampling to deal with the imbalanced
1 11T ‚àí1 11T
discretecolumns,withfullyconnectedneuralnetworksforthe
L= ‚àín2 s1
11T
ns 1nt
11T
(5)
generator. Here we choose the CTGAN as the data synthetic
nsnt n2
t
model. where L ij = n1 2 if x i,x j ‚ààX s, L ij = n1 2 if x i,x j ‚ààX t,
In the proposed FTCA, the source domain first trains a otherwise, L =s ‚àí 1 . Here 1 means tht e column vector
ij nsnt
CTGANmodelbyusingthealreadyprofiledVNFtabulardata. with all values of 1. To address this computationally complex
After training, the generator can produce similar synthetic problem of calculating high dimensional K (a semi-definite
VNF profiling data. Then the source domain VNF will send program), Principal Component Analysis (PCA) is used [10]
this well-trained generator model (or generator parameters) to to get new representations of X and X . Then the above
s t
the target domain VNF, as shown in Fig. 2. This means that m (m ‚â™ n + n ) leading eigenvectors can be selected to
s t
the target VNF can do the domain adaptation based on the construct low-dimensional representations. In FTCA for the
generated data, without substantial raw data leakage. VNF profiling scenario, features of the VNF profiling data setare not so many, so we directly set the number of features as
m (which also satisfies m ‚â™ n s +n t), to avoid discarding N Me ot nw io tork r (PromeR te hs eo uu sr c &e NM oo dn ei t Eo xr porter) i-Profiler
potentially useful information. Then the final minimize MMD probe
optimization problem can be written as:
VNF iPerf
iPerfClient
min tr(cid:0) WTXLXTW(cid:1) +Œª‚à•W‚à•2 instance Server
F
W (6) Linux Bridge Linux Bridge
s.t. WTXHXTW=I
m
where Œª>0 is the hyper-parameter to control the complexity Fig.6. Experimentsetup[3].
of W, the regularization term is Œª‚à•W‚à•2. I ‚àà R(m√óm)
F m
is the identity matrix and will be simplified as I. Con- Algorithm 1: FTCA Algorithm for VNF profiling
sidering the constraint as maximum projection variance in Input: The source VNF profiling data X o. The target
PCA, we also need to keep the original data properties of VNF profiling data X t.
X = [X ,X ] after mapping W in FTCA, so the maxi- Output: The target VNF resource configuration labels
s t
mum data variance is included as the form of the scatter estimated by FTCA.
matrix S =
(cid:0) WTX(cid:1) H(cid:0) WTX(cid:1)T
. The centering matrix H 1 Collect the source VNF data X o and train the CTGAN
is calculated as H=I ‚àí 1 11T. Here I ‚àà model.
R(ns+nt)√ó(ns+nt) deno(n tes s+n idt) entitn ys+ mn at trix, 1 ‚àà R(( nn ss ++ nn tt )) is 2 Send the well-trained CTGAN.pkl model to the target
the column vector with all 1‚Äôs, and Hn =H. VNF entity.
Lagrangemultipliersareusedtosolveconstrainedoptimiza- 3 Collect the target VNF data X t and generate source
tion problems like (6): VNF data X‚Ä≤
o
(X‚Ä≤
o
={X s,Y s}).
L(W,Œ¶)=tr(cid:0) WT(cid:0) XLXT+ŒªI(cid:1) W(cid:1) 4 Select the VNF resource labels as Y s that exist in X‚Ä≤ o
but missing in X . Normalize data sets X and X .
(7) t s t
‚àítr(cid:0)(cid:0) WTXHXTW‚àíI(cid:1) Œ¶(cid:1)
5 The matrix of L and H are constructed by (3) to (6)
according to the datasets {X ,X }.
Let ‚àÇL =0, the Lagrange function (7) will have: s t
‚àÇW 6 The solution mapping matrix W is calculated by the
(cid:0) XLXT+ŒªI(cid:1) W=XHXTWŒ¶ (8) Lagrange multiplier according to (7) and (8).
7 Train regression models with the data set {WX s,Y s}.
Let A = (cid:0) XLXT+ŒªI(cid:1)‚àí1 XHXT, the (8) will become 8 Normalize the new profiling data X t, estimate the Y t
A‚àí1W=WŒ¶. Here it is clear that the solution of mapping by the WX on the trained regression models.
t
W are the eigenvectors of A or A‚àí1, the reciprocal of Œ¶ is
the eigenvalues of A. By using the above formulas, we can
determinethetransformationmatrixWfordomainadaptation.
mode (I mode), SNORT Passive mode (P mode) and Virtual
Thismatrixmakestwodistributionsclosetoeachother.Inthe
Firewall mode (V mode), the optimum resource configuration
new space constructed by W, new data can be obtained and
is found to meet the pre-defined demands. Fig. 6 depicts
usedtoaddressclassificationorregressionproblems.Thestep-
our setup with a SNORT instance, showing the connection
by-stepFTCAisoutlinedinAlgorithm1,Fig.5alsogivesthe
between the profiled VNF, the traffic generator and server
detailed FTCA process in VNF profiling.
end-point machines (iPerf client and iPerf server) [3]. The
There are two obvious advantages of FTCA. First, the
recordedVNFperformancedatasethaseightfeaturesandone
number of generated source samples n can be controlled
s output. The input variables are CPU utilization (CPUUTP),
by the generator at the target VNF domain. The target FTL
Memoryutilization(MEMUTP),Networklatency(RTT),VNF
clientcangenerateapropernumberofsourcesamplestomake
maximuminputrate(MIR)andPacketloss(In RX,Out Tx).
adaptations and avoid the heavy computation complexity on
The output variables are one of the VNF resource configura-
the above matrices. Second, by transferring the profiled VNF
tions like CPU cores (CPU), Memory (MEM MB) and Link
knowledge, the time to profile a function-related VNF can
Capacity (LINK Mbps). Our goal is to use FTCA to predict
be significantly decreased rather than 45 hours for one VNF
resource configurations of less-profiled target VNFs in which
in [5]. Future network orchestrations require self-generation
CPU, MEM or LINK is unknown. The number of collected
of data, self-training of models and automatic performance
profiling data samples in I mode (1112), P mode (896) and
improvement. FTCA may be an ideal approach for generating
V mode (775) are decreasing, so we illustrate the federated
synthetic data and transferring knowledge without privacy
transfermissionsasImodetoPmode(I2P),ImodetoVmode
concerns.
(I2V), and P mode to V mode (P2V). In order to compare
the advantages of the proposed FTCA, several regression
IV. EXPERIMENTSANDANALYSIS
methods are used in the new feature space after mapping
A. Federated Transfer Tasks and Comparison Methods
to get the prediction results: Polynomial Regression (Poly),
Our previous work [3] has provided autonomous profiling Support Vector Regression (SVR), Random Forest Regression
data sets of three types of different VNFs: SNORT Inline (RF)andMultilayerPerceptron(MLP).TheevaluationmetricsFeature Distributions TABLEI
0.08 PERFORMANCEEVALUATIONOFTARGETVNFCPURESOURCE
0.06 Data Type Data Type CONFIGURATION.
Real Real
0.06 Generated Generated
0.04 Original FTCAOptimization
0.04 Task Method MAE RMSE R2 MAE RMSE R2
0.02 0.02 Poly 0.219 0.273 0.222 0.133 0.168 0.707
SVR 0.174 0.201 0.579 0.137 0.173 0.688
I2P
0.00 0.00 RF 0.158 0.195 0.604 0.151 0.187 0.635
0.4 0.6 0.8 1.0 0.2 0.4 0.6 MLP 0.169 0.198 0.523 0.129 0.158 0.737
CPUUTP MEMUTP
Poly 0.356 0.410 -0.560 0.278 0.315 0.077
Data Type 0.10 Data Type I2V SVR 0.292 0.326 0.013 0.270 0.303 0.150
0.100 Real Real RF 0.289 0.328 0.005 0.287 0.323 0.031
Generated 0.08 Generated MLP 0.308 0.345 -0.105 0.254 0.281 0.265
0.075
0.06
Poly 0.319 0.356 -0.173 0.180 0.213 0.579
0.050 0.04 SVR 0.221 0.246 0.439 0.172 0.192 0.657
P2V
RF 0.206 0.229 0.512 0.178 0.197 0.638
0.025 0.02
MLP 0.197 0.221 0.546 0.183 0.209 0.596
0.000 0.00
0 1 2 3 0.0 0.5 1.0 1.5
RTT MIR 1e9
TABLEII
Fig.7. Comparisonbetweentheillustrated0.f3eaturesfromgenerateddataand
realr0 a.2 w5 data. Data Type Data Type PERFORMANCEEVALUATIONOFTARGETVNFMEMORY
0.20 Generated Generated RESOURCECONFIGURATION.
Real 0.2 Real
0.15
11 .. 460.10 VNF CPU Estimation T F w a Tr Ci gt Ah e - t PF oT GC lr yoA u Prn ed d T icr tu et dh 22 24 00 0 00 .1 VNF MEM Estimation T F a Tw r Ci gt Ah e - t PF oGT lC r yoA u Prn ed d T icr tu et dh Task Method MAE O Rr Mig Sin Eal R2 MAFT ECA RO Mpt Sim Eizatio Rn 2
1.20.05 F FT TC CA A- -S MV LR P P Pr re ed di ic ct te ed d 2000 F FT TC CA A- -S MV LR P P Pr re ed di ic ct te ed d Poly 0.527 0.506 -0.176 0.178 0.220 0.535
1.00.00 FTCA-RF Predicted 18000.0 FTCA-RF Predicted I2P SVR 0.182 0.227 0.502 0.179 0.221 0.529
0.8
0.6 0.4 0.5 0.3 0.9 0.8 0.7
1600
1300 1100 1200 1600 1000 1400 1500 RF 0.149 0.209 0.575 0.189 0.228 0.497
0.6 CPU 1400 MEM MLP 0.194 0.231 0.484 0.169 0.210 0.570
0.4 Poly 0.157 0.208 0.572 0.198 0.244 0.407
0.20.20 Data Type 12 00 .0 08 Data Type SVR 0.178 0.227 0.487 0.205 0.252 0.370
Generated 1000 Real I2V
0.0 RF 0.184 0.238 0.440 0.190 0.239 0.431 00.15 20 40 Samples60 Re8a0l 100 0.060 20 40 Samples60Gene8r0ated100
MLP 0.203 0.251 0.373 0.201 0.244 0.406
0.10 (a) CPUcores 0.04 (b) Memory Poly 0.345 0.455 -1.058 0.208 0.251 0.375
SVR 0.202 0.270 0.277 0.203 0.245 0.400
1.60.05 VNF CPU Estimation
T
w ari gth
e
tF T GC roA
und Truth
240 0. 002 VNF MEM Estimation
T
aw ri gth
e
tF GTC roA
und Truth
P2V MR LF
P
0 0. .1 15 73
1
0 0. .2 20 16
3
0 0. .5 58 20
9
0 0. .2 21 12
3
0 0. .2 25 54
3
0 0. .3 35 68
2
1.40.00 55060045070050F F0T TC C8A A0- -P S0o V6l Ry 5 PP 0rr ee 4dd 0ii cc 0tt ee 7dd 50 2200.000 0 500 1F F0T TC C0A A0- -P So Vl Ry PP rr ee1dd5ii cc0tt ee0dd
11 .. 02 LINF FKT TC CA A- -M RFL P
P
rP er de id ci tc et ded 12 80 00 00 In_RXF FT TC CA A- -M RFL P
P
rP er de id ci tc et ded
1.0
0.8 0.06 1600 data. The received CTGAN model will synthesize the source
0.6 14000.8
0.4 domain data (2000 samples) at the target VNF for FTCA.
0.04 12000.6
0.2 There are three target VNF resources: CPU cores, memory,
1000
0.0 00
.02
20Data4 0TSyamppeles60 80 100 0.40 20 40 Samples60 80 100 and link capacity on three transfer tasks I2P, I2V and P2V, as
(c) CRe Pa Ul cores 0.2 (d) Memory showninTablesI,IIandIII.Originalmeansthattheregression
Generated
models are trained on raw source data and directly tested on
0.00 0.0
Fig.8. Theco20m0paris4o0n0oft6a0rg0etV8N0F0CPUan0d.0mem0.o2ryr0e.4sour0c.e6spr0e.d8icti1o.n0s the target data. The FTCA optimization makes the models
onFTLmissionsI2POu(t(_aT)X,(b))andP2V((c),(d)).
are the Mean Absolute Error (MAE), the Root Mean Square
1200 VNF LINK Estimation with FTCA 1200 VNF LINK Estimation with FTCA
Error (RMSE) and R-squared (R2). CTGAN [8] and YData Target Ground Truth Target Ground Truth
Synthetic Tool 1 are selected to train on the source VNF and 11 01 00 00 F FT TC CA A- -P So Vl Ry PP rr ee dd ii cc tt ee dd 11 01 00 00 F FT TC CA A- -P So Vl Ry PP rr ee dd ii cc tt ee dd
FTCA-MLP Predicted FTCA-MLP Predicted
send the corresponding generator model to the target VNF. 900 FTCA-RF Predicted 900 FTCA-RF Predicted
800 800
B. Comparison Performance on VNF Profiling 700 700
600 600
Fig. 7 illustrates the distribution of certain features in the
500 500
real data (yellow) and data generated by CTGAN (blue), after 400 400
0 20 40 60 80 100 0 20 40 60 80 100
Samples Samples
training on the source VNF profiling data set. The synthetic
(a) LinkCapacity (b) LinkCapacity
data effectively approximates the distribution of the original
Fig. 9. A comparison of target VNF link capacity resources predictions on
1YDataSynthetic:https://github.com/ydataai/ydata-synthetic FTLmissionI2P(a)andP2V(b).
seroc_UPC
seroc_UPC
ytilibaborP
ytilibaborP
noitroporp
noitroporp
ytilibaborP
ytilibaborP
ytilibaborP
BM_MEM
ytilibaborP
BM_MEM
noitroporp
spbM_KNIL spbM_KNILTABLEIII 1.0
PERFORMANCEEVALUATIONOFTARGETVNFLINK
CAPACITYRESOURCECONFIGURATION. 0.5
0.0
Task Method Original FTCAOptimization 1 0.52 0.52 0.47 0.52 0.49 0.097 0.25 0.1
MAE RMSE R2 MAE RMSE R2
0.52 1 0.64 0.63 0.78 0.78 0.0062 0.23 0.044
Poly 0.175 0.215 0.535 0.207 0.254 0.344
SVR 0.207 0.252 0.358 0.207 0.242 0.410 0.52 0.64 1 0.71 0.78 0.77 0.24 0.08 0.02
I2P
RF 0.184 0.223 0.495 0.221 0.265 0.289
MLP 0.178 0.217 0.523 0.211 0.253 0.352 0.47 0.63 0.71 1 0.79 0.79 0.0044 0.17 0.11
Poly 0.754 0.964 -8.163 0.206 0.252 0.375 0.52 0.78 0.78 0.79 1 0.94 0.082 0.17 0.048
SVR 0.126 0.185 0.661 0.203 0.248 0.394
I2V 0.49 0.78 0.77 0.79 0.94 1 0.066 0.16 0.033
RF 0.070 0.130 0.832 0.222 0.265 0.308
MLP 0.189 0.264 0.315 0.212 0.256 0.354 0.0970.0062 0.24 0.00440.082 0.066 1 0.098 0.044
Poly 0.297 0.365 -0.315 0.216 0.263 0.315
0.25 0.23 0.08 0.17 0.17 0.16 0.098 1 0.59
SVR 0.213 0.244 0.413 0.204 0.248 0.391
P2V
RF 0.220 0.244 0.412 0.233 0.278 0.240 0.1 0.044 0.02 0.11 0.048 0.033 0.044 0.59 1
MLP 0.302 0.329 -0.068 0.204 0.241 0.425
CPUUTP RTT Out_TX CPU MIR In_RX LINKMEMUTPMEM
Fig. 10. The hierarchical clustering heat-map from SNORT Inline VNF
profilingdataset.
trainongeneratedsourcedataandtestonthetargetdata,after
mappingW(Fig.4).ItisclearinTableIthattheRMSEerror
can be reduced by 38.5% and the R2 metric advances up to REFERENCES
68.6%.
The target VNF CPU prediction results of FTL task I2P [1] K. Kaur, V. Mangat, and K. Kumar, ‚ÄúA review on virtualized infras-
tructure managers with management and orchestration features in nfv
and P2V are shown in Fig. 8 (a) and (c), where the cyan
architecture,‚ÄùComputerNetworks,vol.217,p.109281,2022.
line is the real needed CPU cores on the target VNF after [2] Y. Yue, X. Tang, W. Yang, X. Zhang, Z. Zhang, C. Gao, and L. Xu,
profiling, the same for memory predictions in Fig. 8 (b) and ‚ÄúDelay-awareandresource-efficientvnfplacementin6gnon-terrestrial
networks,‚Äù in 2023 IEEE Wireless Communications and Networking
(d). The experimentresults show that the proposedFTCA can
Conference(WCNC). IEEE,2023,pp.1‚Äì6.
effectively help the regression models predict the target VNF [3] S.Moazzeni,P.Jaisudthi,A.Bravalheri,N.Uniyal,X.Vasilakos,R.Ne-
CPU cores and memory. jabati, and D. Simeonidou, ‚ÄúA novel autonomous profiling method for
the next-generation nfv orchestrators,‚Äù IEEE Transactions on Network
However,whenpredictingthelinkcapacity,theFTLresults
andServiceManagement,vol.18,no.1,pp.642‚Äì655,2020.
may not be very good (Fig. 9 and Table III). This negative [4] N.Ferdosian,S.Moazzeni,P.Jaisudthi,Y.Ren,H.Agrawal,D.Sime-
transfer phenomenon is caused by the information gain (IG) onidou, and R. Nejabati, ‚ÄúAutonomous intelligent vnf profiling for
futureintelligentnetworkorchestration,‚ÄùIEEETransactionsonMachine
offeaturesorlabelsinthesourceVNFprofilingdataset.From
LearninginCommunicationsandNetworking,2023.
thehierarchicalclusteringcorrelationheat-map[14]oftheraw [5] S.VanRossem,W.Tavernier,D.Colle,M.Pickavet,andP.Demeester,
SNORTInlineVNFdataset(Fig.10),wecanseethatmostof ‚ÄúProfile-based resource allocation for virtualized network functions,‚Äù
IEEETransactionsonNetworkandServiceManagement,vol.16,no.4,
the IG exists in the deep colour features like CPU and MIR.
pp.1374‚Äì1388,2019.
In contrast, link capacity (LINK) does not have enough IG in [6] I.Kevin,K.Wang,X.Zhou,W.Liang,Z.Yan,andJ.She,‚ÄúFederated
the data set. In this case, it is important to consider what kind transfer learning based cross-domain prediction for smart manufactur-
ing,‚Äù IEEE Transactions on Industrial Informatics, vol. 18, no. 6, pp.
of conditions we need federated transfer or not transfer [15].
4088‚Äì4096,2021.
[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù
V. CONCLUSIONANDFUTUREWORK
Advancesinneuralinformationprocessingsystems,vol.27,2014.
[8] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni,
In this paper, a novel feature-based federated transfer ‚ÄúModeling tabular data using conditional gan,‚Äù Advances in neural
learning method is proposed as federated transfer component informationprocessingsystems,vol.32,2019.
[9] N. Park, M. Mohammadi, K. Gorde, S. Jajodia, H. Park, and Y. Kim,
analysis(FTCA).IttransfersknowledgeamongdifferentVNF
‚ÄúData synthesis based on generative adversarial networks,‚Äù arXiv
profiling entities, based on GANs data in federated learning preprintarXiv:1806.03384,2018.
with transfer learning among profiling datasets. First, FTCA [10] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, ‚ÄúDomain adaptation
viatransfercomponentanalysis,‚ÄùIEEEtransactionsonneuralnetworks,
is a feasible approach for improving tabular data estimation,
vol.22,no.2,pp.199‚Äì210,2010.
without substantial data leakage. Second, by transferring the
[11] Y.Chen,X.Qin,J.Wang,C.Yu,andW.Gao,‚ÄúFedhealth:Afederated
profiledVNFknowledge,fasternewVNFsdeploymentcanbe transfer learning framework for wearable healthcare,‚Äù IEEE Intelligent
expected as service function chaining (SFC). FTCA also has Systems,vol.35,no.4,pp.83‚Äì93,2020.
[12] Y. Cheng, J. Lu, D. Niyato, B. Lyu, J. Kang, and S. Zhu, ‚ÄúFederated
reference value for the future 6G network orchestration. The
transferlearningwithclientselectionforintrusiondetectioninmobile
next step will focus on problems like non-independently and edge computing,‚Äù IEEE Communications Letters, vol. 26, no. 3, pp.
identically (non-iid) features in data for FTL, multiple input- 552‚Äì556,2022.
[13] K. Zhang, X. Liu, X. Xie, J. Zhang, B. Niu, and K. Li, ‚ÄúA cross-
output regression models for SFC, and encrypted CTGAN
domainfederatedlearningframeworkforwirelesshumansensing,‚ÄùIEEE
models. Network,vol.36,no.5,pp.122‚Äì128,2022.
PTUUPCTTR
XT_tuOUPC
RIM
XR_nI
KNIL
PTUMEMMEM[14] X.Zhang,A.Mavromatics,A.Vafeas,R.Nejabati,andD.Simeonidou,
‚ÄúFederated feature selection for horizontal federated learning in iot
networks,‚ÄùIEEEInternetofThingsJournal,2023.
[15] W. Zhang, L. Deng, L. Zhang, and D. Wu, ‚ÄúA survey on negative
transfer,‚Äù IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 2, pp.
305‚Äì329,2022.