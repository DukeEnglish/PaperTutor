Tabular Data Contrastive Learning via Class-Conditioned and
Feature-Correlation Based Augmentation
WeiCui1 RasaHosseinzadeh1 JunweiMa1 TongziWu1 YiSui1 KeyvanGolestan1
Abstract to excel in tabular domain tasks Grinsztajn et al. (2022),
Contrastivelearningisamodelpre-trainingtech- whichstressestheneedforalgorithmimprovement. Onthe
niquebyfirstcreatingsimilarviewsoftheoriginal otherhand,justlikeintheotherdatamodalities,withthe
data, andthen encouragingthe dataand itscor- sheervolumeoftabulardatabeingpresent,itisoftentoo
responding views to be close in the embedding timeorresourceconsumingtocollectqualitytargetlabelfor
space. Contrastive learning has witnessed suc- eachdatapoint. Correspondingly,semi-supervisedlearning
cessinimageandnaturallanguagedata,thanks Yangetal.(2022)orself-supervisedlearningtechniquesGui
tothedomain-specificaugmentationtechniques etal.(2023)arerequiredformodelpre-trainingtoobtain
thatarebothintuitiveandeffective. Nonetheless, high-performancemodelsontabulardata.
intabulardomain,thepredominantaugmentation In recent years, contrastive learning Chen et al. (2020)
techniqueforcreatingviewsisthroughcorrupt-
hasemergedasahighlypopularpre-trainingtechniquefre-
ingtabularentriesviaswappingvalues,whichis
quently adapted in the self-supervised learning and semi-
notassoundoreffective. Weproposeasimple
supervisedlearningsettings. Contrastivelearningismainly
yetpowerfulimprovementtothisaugmentation
usedtopre-trainanencoderblock,whichmapstheinput
technique: corruptingtabulardataconditionedon
rawdatatoanintermediateembeddingspace. Ithasbeen
classidentity. Specifically,whencorruptingaspe-
successfullyutilizedinvariousapplicationswithdifferent
cifictabularentryfromananchorrow,insteadof
datamodalitiesChenetal.(2020);Chen&He(2021);Ma
randomlysamplingavalueinthesamefeaturecol-
etal.(2021);Panetal.(2021). Toperformcontrastivelearn-
umnfromtheentiretableuniformly,weonlysam-
ing,asthefirststep,researchersengineerdataaugmentation
plefromrowsthatareidentifiedtobewithinthe techniquestocreatesimilarviewsontopofeachoriginal
sameclassastheanchorrow.Weassumethesemi- datapoint(oftenreferredtoastheanchor). Acontrastive
supervisedlearningsetting,andadoptthepseudo
lossisthenoptimized,whichminimizesadistancemetric
labelingtechniqueforobtainingclassidentities (i.e.,cosinesimilarity)intheembeddingspacebetweenthe
overalltablerows.Wealsoexplorethenovelidea
anchor and its corresponding views. In the classical con-
ofselectingfeaturestobecorruptedbasedonfea-
trastive loss formulation, views generated from different
turecorrelationstructures. Extensiveexperiments
anchorsareexplicitlypushedawayintheembeddingspace.
showthattheproposedapproachconsistentlyout-
However,multiplevariantstothecontrastivelosshavebeen
performstheconventionalcorruptionmethodfor
proposedthateitherdonotexplicitlypushawaydissimilar
tabulardataclassificationtasks. Ourcodeisavail-
pairs Chen & He (2021), or incorporate additional terms
able at https://github.com/willtop/
intothelossfunctionMaetal.(2022).
Tabular-Class-Conditioned-SSL.
Naturally,thedesignofthedataaugmentationtechniquesis
pivotalforthecontrastivepre-trainingperformance. Agood
1.Introduction data augmentation technique often incorporates domain-
specificknowledge. Forimagedata,researchershavesuc-
Tabular data is one of the most fundamental and preva-
cessfully explored various augmentation techniques that
lent data modalities, from which rich information and in-
leadtoimageviewswithrichvarieties,whilepreservingthe
sightscanbeextractedthroughproperlearningalgorithms.
originalsemanticmeanings, includingrotation, cropping,
Nonetheless,thedeeplearningalgorithm,arguablythemost
colordistortions,Gaussianblurring,andsoonChenetal.
widely used learning algorithms, currently still struggles
(2020); Chen & He (2021). Similarly, in the domain of
naturallanguageprocessing(NLP),contrastivelearninghas
1Layer6AI,Toronto,Canada. Correspondenceto: WeiCui
<wei@layer6.ai>. alsowitnessedsuccessthroughemployingdomain-specific
augmentation techniques, including word masking, word
Copyright2024bytheauthor(s).
1
4202
rpA
62
]GL.sc[
1v98471.4042:viXraTabularDataClass-ConditionedAugmentation
replacement,reordering,andsoonShortenetal.(2021). attempttosamplethesubsetoffeaturesbasedonthecor-
relationstructure,whichwerefertoascorrelation-based
Nonetheless,forthedomainoftabulardata,itismorechal-
featuremasking. Weconductedextensiveexperimentson
lenging to design such effective data augmentation tech-
theOpenML-CC18datasetBischletal.(2021)forbothav-
niques. Thereasonsaremultifold:
enues. Resultssuggestthattheclass-conditionedcorruption
improvement boosts the pre-trained model to outperform
• LackofStructure:tabulardataingenerallacksspatial
itscounterpart,whilethecorrelation-basedfeaturemasking
or temporal structures that can be exploited for aug-
techniqueshowsnoconcreteevidenceforimprovement.
mentationtechniquessuchasrotationorre-ordering.
Formathematicalsymbols, weusethesuperscript(·)T to
• DistinctSchemas: thelistoffeaturesineachtableis denote the transpose of a matrix (or a vector regarded as
unique in ordering and semantics. The organization a single-column matrix). We use ||·|| to denote the L2
2
and representation of information is highly distinct norm of a vector. We use [N] to denote the integer set
fromtabletotable. containingnaturalnumbersuptoN;and⌈·⌉todenotethe
ceilingintegerfunction. Lastly,weusetheoperator←to
• HeterogeneousFeatures: withtabularfeaturesofdif-
denotetheassignmenttoaspecifiedvariable.
ferent types, units, or ranges, an augmentation tech-
niqueapplicabletosomefeaturesmightbeinapplicable
2.ProblemFormulation&Background
tootherfeatureswithinthesametable.
2.1.Semi-SupervisedTabularDataClassification
Therearethreemainclassesofaugmentationtechniquesfor
Consider a tabular dataset D = (D ,D ), where
tabulardataintheliterature: 1. Feature-valueCorruption train test
D = {(x ,y )}Nl ∪ {x }Nl+Nu consists of N la-
Yoon et al. (2020); Iida et al. (2021); Bahri et al. (2022); train i i i=1 i i=Nl+1 l
beledsamplesandN unlabeledsamplesfortraining;and
Nam et al. (2023), 2. Inter-sample Weighted Summation u
D ={(x ,y )}Nl+Nu+Nt consists of N labeled sam-
Somepallietal.(2021);Darabietal.(2021),and3. Crop- test i i i=Nl+Nu+1 t
pingSubsetsofFeaturesUcaretal.(2021);Wang&Sun plesfortesting,withthetestinglabels{y i}N i=l N+ lN +u N+ uN +t 1un-
(2022). Amongthethreeclassesofapproaches,thefeature- knowntothemodel. Eachsampleinputx i consistsofM
valuecorruptionisthemostadoptedtechnique. Regardless, features,witheachfeaturebeingeithercategoricalornumer-
duetotheabove-mentionedcharacteristicsoftabulardata, icalintype. Weusex(k) todenotethek-thfeatureofthe
i
theseaugmentationtechniqueshaveyettoshowsignificant i-thsample. Foreachinputinthelabeledset,aclasslabel
improvementswhenbeingusedformodelpre-training. y isavailable∀i ∈ [N ]. Weaimtolearnaparametrized
i l
mappingf (·)thatsolvestheclassificationtaskbymapping
Inthispaper,recognizingthegapintabulardataaugmen- θ
eachinputinthetestingsettoitsclass.
tation techniques, we first propose an easy yet powerful
improvementtothefeature-valuecorruptiontechnique,by Weexploretheuseofaneuralnetworkformodelingf ,with
θ
incorporatingtheclassinformationintothecorruptionpro- θcorrespondingtotheneuralnetworkparameters. Based
cedure. Specifically,whencorruptingafeaturevalueonthe onthecontrastivelearningalgorithm, theneuralnetwork
anchorrow,insteadofsamplinguniformlyacrosstheentire consistsofthreeparts:
tableforthereplacement,werestrictthesamplingwithin
rowsthatbelongtothesameclassastheanchorrow. We 1. Anencoder,withparametersdenotedasθ e.
refertothisimprovementasclass-conditionedcorruption.
2. Apre-trainhead,withparametersdenotedasθ .
p
Through this simple modification, we further ensure that
thegeneratedviewsaremorelikelytobesemanticallysimi- 3. Aclassificationhead,withparametersdenotedasθ .
c
lartotheoriginalanchor,andthereforeshouldbepushed
closertotheanchorintheembeddingspaceviacontrastive We then have θ = {θ e,θ p,θ c} as the model parameters
learning. Aswedonothavethetargetinformationforevery space. Note that we adopt to this three-part architecture
rowunderthesemi-supervisedsetting,weadoptthepseudo- followingtheconventionofcontrastivelearningresearch
labelingtechniquetoobtainestimationsoftargets. During Chenetal.(2020),wherethepre-trainheadθ p isonlyin-
thepre-trainingprocess,weiterativelytrainourencoderand cludedinthepre-trainingstagetoallowforextraflexibility
updateourtargets. in the learned representations. It will be discarded in the
down-streamfine-tuningstage.
Theclass-conditionedcorruptiontechniquecanberegarded
ashowtocorruptwhengeneratingaugmentedtabularrows.
2.2.ContrastiveLearning
We further explore into another dimension of corruption
strategy: wheretocorrupt. Specifically,toimproveupon Contrastivelearningistheprocessofpre-trainingthemodel
thecurrentapproach,weexploitfeaturecorrelations,and tolearnanembeddingspacewhichwillbefurtherusedfor
2TabularDataClass-ConditionedAugmentation
down-streamclassificationorregressiontasks. Below,we process. Intheliterature, themostpopularaugmentation
present the original contrastive learning approach, which techniquefortabulardatahasbeenthefeaturevaluecorrup-
isadaptedbyourapproaches. Wenotethatthecontrastive tion. Randomlysamplinghasbeenusedforbothsampling
learning variants later proposed are equally applicable to thereplacementvaluewithineachfeature,aswellassam-
ourapproachanddiscussions. plingwhichsubsetoffeaturestobecorruptedYoonetal.
(2020); Iida et al. (2021); Bahri et al. (2022); Nam et al.
Letg(·)denotethedataaugmentationfunction. Givenan
(2023).
anchorx fromthetrainingset,aviewisgeneratedasfol-
i
lows: Todescribethecorruptionprocedureindetails,letpbethe
percentage of features to be corrupted, which is a hyper-
x˜ i =g(x i). (1) parameter that can be chosen arbitrarily. We would then
have⌈Mp⌉featurestobecorrupted. LetM denotethe
Withg(·)intendedtobeasemanticpreservingoperation,x˜ crpt
i subsetoffeaturestobecorrupted. Inthecurrentliterature,
shouldbehighlysimilartox . Therefore,theircorrespond-
i theM featuresarerandomlyselectedfromtheM fea-
ingrepresentationsintheembeddingspaceshouldalsobe crpt
turesforeachanchortobecorrupted. Furthermore,foreach
close.
featurewithintheM features,thereplacementvalueis
crpt
Afterobtainingviewsasabove,wecomputetheembeddings alsorandomlyselected fromthefeaturecolumnfromthe
fortheoriginaldatapointsaswellastheviews: entiretable. Theaugmentationfunctiong(·)alongwiththe
contrastivelearningprocedureisdescribedinAlgorithm1.
z i =f θp(f θe(x i)) (2) Notethatmini-batchescanbeusedwithineachepochinthe
z˜ =f (f (x˜ )). (3) algorithm.
i θp θe i
Considerasampledtrainingbatch{x i}N i=1withbatchsize Algorithm1ContrastiveLearningwithExistingCorruption
N. FollowingEquation(2)andEquation(3),wecanobtain Procedure
twosetsofembeddings: {z }N and{z˜}N . Weconcate- 1: Randomlyinitializeθ andθ
i i=1 i i=1 e p
natethetwosetsofembeddings,leadingtothesetof2N 2: Setp:percentageoffeaturescorrupted
embeddings{zˆ}2N ,inwhichthefirstandsecondhalves 3: SetN eps:numberoftrainingepochs
i i=1 4: fore←1toN do
correspondto{z }and{z˜},respectively. Wedenotethe eps
i i 5: fori←1toN +N do
l u
cosinesimilaritybetweenanypairofembeddingsz iandz j 6: Uniformlyselect⌈Mp⌉featurestoformthesetM crpt
asfollows: 7: Initializex˜ =x .
i i
8: fork←1toM do
s = zˆ iTzˆ j . (4) 9: ifk∈M crptthen
i,j ||zˆ i|| 2||zˆ j||
2
10: Samplejuniformlyfrom[N l+N u]
11: x˜(k) ←−x(k)
i j
12: endif
Acontrastivelossisthendefinedonthistrainingbatchas
13: endfor
follows: 14: Computeandcollectz basedonx viaEquation(2)
i i
. 1 (cid:88)2N (cid:32) es i,i′/τ (cid:33) 1 15 6: : enC do fom rputeandcollectz˜ ibasedonx˜ iviaEquation(3)
L= −log , (5)
2N
i=1
(cid:80)2 j=N 11 [j̸=i]esi,j/τ 1 17 8:
:
C Oo pm timpu izte eL
θ
v ai na dE θqua thti ro on ug(5 h)
gradientdescentonL
e p
. 19: endfor
where i′ = (i+N) mod N is the index of the pairing
view(oranchor)embeddingtotheindexi. Byminimizing
Equation(5),wegraduallypushcloserpairsofanchorand
3.Method
view, and push away anchors and views across different
pairstoavoidcollapsingtothetrivialsolution.
Weformallyproposetwoimprovementsovertheexisting
AfteroptimizingthemodelbyminimizingEquation(5),as tabular data augmentation procedure as described in Sec-
mentionedinSection2.1,wewillproceedtodiscardthepre- tion2.3.Specifically,weinvestigateinthesamplingchoices
trainheadθ ,freezetheencoderportionθ ,andtrainthe forboththereplacementvalueoneachfeature,aswellas
p e
classificationheadθ takingtheoutputsfromtheencoder thesubsetoffeaturestobecorrupted.
c
asinputsonthelabeledtrainingset.
3.1.HowtoCorrupt: Class-ConditionedCorruption
2.3.DataAugmentationviaRandomCorruption
Firstly, we focus on how to corrupt, i.e., the selection of
AsdescribedinSection2.2,thedataaugmentationfunction replacementvaluesthatareusedtocorrupteachselected
g(·)servesasanimportantpiecetotheentirepre-training feature. Incontrastivelearning,theviewsgeneratedfrom
3TabularDataClass-ConditionedAugmentation
each anchor are supposed to be semantically close to the Algorithm2ImprovedCorruptionProcedurewithClass-
anchor. Correspondingly, the augmentation function g(·) ConditionedCorruption
shouldpreservethesemanticsoftheanchordatawhilegen- 1: Randomlyinitializeθ ,θ ,andθ
e p c
eratingvariations. Nonetheless,theexistingaugmentation 2: Setp:percentageoffeaturescorrupted
functiondescribedinAlgorithm1isnotnecessarilyseman- 3: SetN eps:numberoftrainingepochs
4: SetN :intervallengthforupdatingθ
ticpreserving,especiallywhenthefeaturebeingcorrupted up c
5: fore←1toN do
eps
islargelyaffectingtheclassidentityoftherow.
6: fori←N +1toN +N do
l l u
7: Obtainpseudolabels:c =f (f (x ))
Toimprovetheaugmentationtechniqueregardingthisper- i θc θe i
8: endfor
spective,weproposetoadopttheclass-conditionedcorrup-
9: fori←1toN +N do
l u
tiontechnique. Inparticular,whencorruptingeachselected 10: Uniformlyselect⌈Mp⌉featurestoformthesetM
crpt
featureintheanchor,weonlysamplethefeaturevaluefrom 11: Initializex˜ i =x i.
rowsthatareunderthesameclassastheanchor. Giventhat 12: fork←1toM do
13: ifk∈M then
the row used for corruption and the anchor belong to the crpt
14: LetN ∈ [N +N ]denoteindicesofsamples
sameclass,thesemanticsoftheirfeaturesaremorelikelyto
ci l u
underclassc
i
besimilar,atleastundertheperspectiveofthedown-stream 15: SamplejuniformlyfromN .
ci
classificationtask. 16: x˜(k) ←−x(k)
i j
17: endif
The main challenge within the class-conditioned corrup-
18: endfor
tion process is to obtain class labels over the entire table. 19: Computeandcollectz basedonx viaEquation(2).
i i
Conventionally, as elaborated in Section 2.2, contrastive 20: Computeandcollectz˜ ibasedonx˜ iviaEquation(3).
learning is a self-supervised learning approach that does 21: endfor
22: ComputeLviaEquation(5)
notrequireanylabel. However,contrastivelearningisonly
23: Optimizeθ andθ throughgradientdescentonL
e p
responsiblefortrainingtheencodermoduleoftheneural
24: ife mod N ==0then
up
networkthatleadstoasemanticallymeaningfulembedding 25: Freezeθ
e
space,whichisrarelyutilizedinastandalonemanner. In 26: Optimizeθ conthelabeleddata(viaconventionalsuper-
mostapplications,thereexistsadown-streamapplication visedtraining)
27: endif
forwhichthemodelwillbefine-tunedon,atwhichpointthe
28: endfor
targetinformationisrequired. Therefore,theassumption
ofsemi-supervisedlearning,whereasmallsetoflabelsis
available,ismorerealisticandmorefrequentlyencountered
inreal-worldapplications.Giventhesmallsetofclasslabels lizeasupervisedcontrastivelearningobjectiveKhoslaetal.
availableundersemi-supervisedlearning,itisawastedop- (2020), which is different from the classical contrastive
portunitytonotincorporatesuchclassinformationintothe learning objective as in Equation (5). In supervised con-
contrastivelearningprocedure,whichisthecaseinexisting trastive learning, views belonging to the same class are
tabularaugmentationapproaches. pushedaltogether,whichwouldleadtolessgranularlatent
representations especially when the number of classes is
Underthesemi-supervisedlearningsetting, weadaptthe
low,whichisoftenthecasefortabulardatasets.
popular pseudo labeling approach Grandvalet & Bengio
(2004) for obtaining labels on the unlabeled training set.
Specifically,amodelisfirsttrainedonthesmalllabeledset, 3.2.WheretoCorrupt: Correlation-BasedFeature
andthenutilizedtoruninferenceontheremainingunlabeled Masking
set.Themodeloutputsareregardedasthepseudotargetsfor
As the second direction of improvement, we explore the
theunlabeleddata. Inthispaper,weintegratethispseudo
problemofwheretocorrupt,i.e.,whichsubsetoffeaturesto
labelingprocessintothecontrastivelearningpre-training
corrupt. Insteadofrandomlyselectingthesubsetoffeatures
stage,throughiterativelyupdatingtheencodertominimize
tocorruptoneachanchor,wehypothesizethatincorporating
thecontrastiveloss;andupdatingtheclassificationheadto
thecorrelationinformationamongfeaturescanimprovethe
minimizethesupervised lossonthesmall labeled setfor
performanceofcontrastivelearning.
pseudolabels. Algorithm2outlinesthisprocess.
Theintuitionbehindcorrelation-basedfeaturemaskingcan
Wenotethattherehavebeenacoupleofotherapproaches
bebetterunderstoodbydrawingtheanalogytoimagecor-
in the literature that explore incorporating class informa-
ruption. Ifweconsiderpixelsofanimageasitsfeatures,
tionintotheaugmentationprocedureDarabietal.(2021);
in general, adjacent pixels are correlated features, while
Wang&Sun(2022). Themaindifferencefromthesetwo
thedisjointpixelsthatarefarawayapartareuncorrelated
comparedtoourmethodproposedinthissub-sectionisthe
features. In the literature, image corruption has been ex-
contrastivelearningobjective: bothoftheseapproachesuti-
plored as a data augmentation technique, where patches
4TabularDataClass-ConditionedAugmentation
ofananchorimagearecorrupted. Thistechniquecanbe Algorithm 3 Improved Corruption Procedure with
regardedasselectingasubsetoffeaturesthatarehighlycor- Correlation-BasedFeatureMasking
relatedforcorruption. Conversely,selectingfeaturesthat 1: Randomlyinitializeθ andθ
e p
arehighlyuncorrelatedcorrespondstocorruptingsparsely 2: Setp:percentageoffeaturescorrupted
locatedpixelsonanimage. Therationalebehindthisstrat- 3: SetN eps:numberoftrainingepochs
4: fork←1toM do
egy follows that with each feature being corrupted, there
5: TrainanXGBoostmodeltopredictfeaturekbasedonthe
existscorrelatedfeaturesleftintact. Throughreconstructing
remainingM−1featuresoverthetrainingset[N +N ].
l u
thecorruptedfeaturesbasedontheircorrelatedfeatures,the 6: Collect q ∈ RM−1: feature importance scores to k-th
k
modelisencouragedtolearnandutilizetheknowledgeof feature.
feature correlations, which will be helpful in performing 7: endfor
8: fore←1toN do
downstreamtasks. eps
9: fori←1toN +N do
l u
The choice of the feature correlation metric is not trivial. 10: Sample ⌈Mp⌉ features to form the set M crpt, as de-
scribedinAlgorithm4basedon{q ,∀k}.
Themoststraight-forwardselectionisbycomputingtheco- k
11: Initializex˜ =x .
variancematrix. Thismetrichoweverhastwoshortcomings: i i
12: fork←1toM do
beingincompatibletocategoricalfeatures;andbeinglimited 13: ifk∈M then
crpt
tomodelingonlylinearrelationships.Inthispaper,weadopt 14: Samplejuniformlyfrom[N +N ].
l u
amoregeneralandflexiblecorrelationmeasure,whichis 15: x˜(k) ←−x(k)
i j
throughfittinganXGBoostmodelChen&Guestrin(2016) 16: endif
17: endfor
andobtainingthefeatureimportancescoresasaproxyfor
18: Computeandcollectz basedonx viaEquation(2)
featurecorrelation. Specifically,giventheentiretrainingset i i
19: Computeandcollectz˜ basedonx˜ viaEquation(3)
i i
oftabulardata,foreachfeature,wefitanXGBoostmodel 20: endfor
topredictthisfeature(classificationforcategoricalfeatures; 21: ComputeLviaEquation(5)
regressionfornumericalfeatures)basedontheremaining 22: Optimizeθ eandθ pthroughgradientdescentonL
23: endfor
features. Wethenutilizethenormalizedfeatureimportance
scoresastheindicatoronhoweachoftheremainingfeature
correlatestothefeaturetobepredicted. Werelegatethede-
• NoPre-train: afterrandomlyinitializingtheencoder,
scriptionforthesamplingprocedurebasedonthesefeature
wefreezeitanddirectlytraintheclassificationheadon
importancescorestoAppendixA.Thedetaileddescription
thelabeleddataset.
oftheentirecorrelation-basedfeaturemaskingapproachis
showninAlgorithm3. NotethatbydirectlyusingtheXG-
• Conventional Corruption: we perform contrastive
Boostmodelfeatureimportancescores,Algorithm3will
learningusingtheconventionaldataaugmentationap-
provideuswithsubsetsincludinghighly-correlatedfeatures.
proachasintroducedinSection2.3.
Forsamplingsetsofleast-correlatedfeatures,wemaysim-
plyusethenegativevaluesofthefeatureimportancescores • OracleCorruption: assumeaccuratetargetsareavail-
inAlgorithm3. ableovertheentiretrainingset.
Wenotethattheproposedclass-conditionedcorruptionand
correlation-basedfeaturemaskingaretwoparallelimprove- Wenotethattheconventionalcorruptionbaselineessentially
mentsthatcanbeimplementedsimultaneously,whichcorre- resemblestheapproachinBahrietal.(2022). Nonetheless,
spondstomergingAlgorithm2andAlgorithm3together. aswecouldnotreplicateresultsinastablemannerfollow-
ingthesettingsspecifiedintheirpaper(mostlyduetotheir
early-stoppingsetup),thehyper-parametersadoptedinour
4.Experiments
simulationsareslightlydifferentfromthoseinBahrietal.
4.1.ExperimentalSetup (2022).Furthermore,wealsonotethattheoraclecorruption
baselineisnotrealisticunderthesemi-supervisedlearning
Wefocusonthesemi-supervisedtabularclassificationtask
assumption. Thisbaselineservestoshowanupper-bound
asdescribedinSection2.1. Weadoptthecontrastivelearn-
ontheperformanceimprovementobtainedbyincorporat-
ingapproachforobtainingthepre-trainedmodelencoderθ ,
e ingclass-conditionedcorruptionintothedataaugmentation
followedbytrainingtheclassificationheadθ . Withinthe
c process.
contrastivelearningprocess,weimplementourproposed
augmentationtechniquesaselaboratedinAlgorithm2and Intermsoftheneuralnetworkarchitecturespecification,we
Algorithm3. Toexaminetheeffectivenessofbothofourap- implementallthreemodules,namelytheencoder,thepre-
proaches,wecomparethemagainstthefollowingmethods: trainhead, andthedecoder, usingfully-connectedlayers,
witheachofthehiddenlayerssizeof256. Weuse4hidden
layersfortheencoder(i.e.,θ ),1hiddenlayerand1output
e
5TabularDataClass-ConditionedAugmentation
Stage ModelParameters Epochs toreducethelearnedembeddingsdownto3dimensions,and
Contrastive-learning plotthembyclassidentitiesinFigure2. Asevidentfrom
θ &θ 500
Pretraining e p thevisualizations,comparedtotheconventionalaugmenta-
Down-stream tiontechnique,theproposedclass-conditionedcorruption
θ 100
Fine-Tuning c approach helps to learn an embedding space where there
Pseudo-labeling ismoreseparationbetweendatafromdifferentclasses,for
θ 10
IterativeUpdate2 c both the training set and the testing set. Therefore, it is
easiertolearnandperformclassificationbasedonsuchan
Table1. TrainingSet-upsatDifferentStages. embeddingspace.
5.450 4.3.ClassificationPerformanceswithClass-Conditioned
Conventional
5.425 Corruption
Class-Conditioned (Ours)
5.400 Oracle
We evaluate the down-stream classification performance
5.375
of each method over a large set of tabular data from the
5.350
OpenML-CC18 dataset Bischl et al. (2021). We follow
5.325
the specifications for training and evaluating the models
5.300
as listed in Section 4.1. For categorical features, we use
5.275
one-hot encoding after the corruption step to ensure the
5.250
representationofthetabulardataalwaysfollowsthevalid
0 100 200 300 400 500
epoch encodingformat.
Figure1.ContrastiveLossLearningCurvesforCompetingAug- WepresentthewinmatrixW onclassificationaccuracies
mentationMethodsinthePre-trainingStage. Comparedtothe amongeachcompetingmethodinFigure3,withthe(i,j)
conventionaltabularaugmentation(corruptionwithrandomlysam-
entry of the matrix being the ratio computed over the 30
pledvalues), ourmethodachievesnoticeablylowercontrastive
OpenML-CC18tablesasfollows:
lossatafasterpace.Notethatourmethodmatchesthelossopti-
mizationresultsbytheoraclecorruptionmethod(whichhasthe
30
knowledgeofalllabels). (cid:80) 1[methodibeatsj ondatasetd]
W = d=1
i,j 30
(cid:80) 1[methodibeatsORlosestoj ondatasetd]
layerforthepre-trainhead(i.e.,θ p),and1hiddenlayerand d=1
1outputlayerfortheclassificationhead(i.e.,θ ).InTable1, (6)
c
welistoutthenumberofepochsforeachtrainingstage.
Weuseat-testwithunequalpopulationvarianceswithasta-
tisticalsignificancelevel(i.e. thep-value)of0.05,andonly
4.2.ContrastiveLossLearningCurve&Embedding
computetheratiosoverthedatasetsfromwhichstatistical
Visualization
significantcomparisonresultscanbeobtained. Werelegate
Wefirstexaminethecontrastivelearningprogressaswellas thefullclassificationaccuracyresults,aswellasfullAU-
theresultantembeddingspacefromthemodelundereach ROC (area under receiver operating characteristic curve)
augmentationtechniqueasintroducedinSection4.1. For results,inTable2andTable3respectivelyinAppendixB.
illustrationsimplicity,inthissub-section,weusethesim-
As evident through the comparison, our proposed class-
pletabularclassificationdataset“breastcancer”provided
conditionedcorruptionapproachshowsnoticeableimprove-
in the sklearn package. We first plot the contrastive loss
mentsovertheconventionalcorruptionmethod,whereour
valueasdefinedinEquation(5)inFigure1. Asillustrated,
approach achieves improvements on 83% of the datasets.
introducingtheclassinformationintothetabularaugmen-
Furthermore, the oracle method shows dominant perfor-
tation process indeed encourages the views created to be
manceovertherestofthemethodsonboththeaccuracyand
moresimilartotheanchors,leadingtoanoticeablylower
AUROCmetrics,furthervalidatingthebenefitsofconsid-
contrastiveloss.
eringclassidentityinformationintothetabularcorruption
Wethenvisualizethelearnedembeddingspaceundereach process.
method. Afterthecontrastivelearningpre-training,wetake
theembeddingscomputedbytheencoder,i.e.,f (x),and 4.4.LimitationsfromCorruption-BasedAugmentation
θe
visualizehowtheseembeddingsaredistributedacrossdiffer-
Oneinterestingobservationwewouldliketonoteisthat,
entclasses. Weapplyprincipalcomponentanalysis(PCA)
tomuchofoursurprise,theclassificationresultsfromno
2N inAlgorithm2. pre-trainareoccasionallyhigherthantheresultsachieved
up
6
ssolTabularDataClass-ConditionedAugmentation
No Pre-train Conventional Class-Conditioned (Ours) Oracle
+cls Train +cls Train +cls Train +cls Train
-cls Train -cls Train -cls Train -cls Train
+cls Test +cls Test +cls Test +cls Test
-cls Test -cls Test -cls Test -cls Test
Figure2.LearnedEmbeddingsfromAugmentationMethods. Theembeddingsarecomputedbytheencoderpre-trainedundereach
method,andarevisualizedin3-DspacethroughdimensionalityreductionwithPCA.Evidently,afterpre-trainingunderourapproach,
theseparationbetweensamples(bothtrainandtestsamples)fromdifferentclassesismoreprominentinthelearnedembeddingspace
comparedtonopre-trainingorpre-trainingundertheconventionalaugmentationapproach.
isalwaysbeneficialinthetabulardomain.
No
Pre-Train 0.0% 41.7% 28.6% 6.7% O
a Bn
an
c
le
ae
nc
S
co
c
em
a
Sp
le
cr ae
t
lh
a
ee bn
tl
aes bib
li
enle
,t
ee
h
ax
e
ca hOm rpp oel we nMf co oLr n-o
sC
iu sCr tsa 1r o8g fu
d
wm
a
ete
a
in gst
e
hti tss s.t ah nIe
n
dB dta
h
isl e-
-
Conventional 58.3% 0.0% 16.7% 0.0% t
c
ca
o
an nrc rue cps
ht
ato
b ny
ga esp
w
aiv bao
p
rut
p
pi in
n tlg
ya ,wt lw
e ei
ao
g
d- he
it
nn
s
gd oe trd ods aic sta gal ene n.
c
eeN
rs
a,a tt
t
eu
h
dr ea rsl ol cy wa, lew thbh aae tln
a cn
ow
c
ne
e
-
Class-Conditioned 71.4% 83.3% 0.0% 0.0% t
t
dhr aa
a
td
t
ai ssc eut tcs
sh
.th ce asl ea sw as reo lf ikp eh lyys ri ac rs e. inW me od reo ch oo mw pe lv exer ane dm rp eh aa lis si tz ie
c
Oracle 93.3% 100.0% 100.0% 0.0%
4.5.ClassificationPerformanceswithCorrelation-Based
No
Pre-Train Conventional Class-Conditioned Oracle
For
F coe ra rt eu lr ae tioM n-a bs ak si en dg
feature masking, we explore both
variants as elaborated in Section 3.2: sampling features
Figure3.ClassificationAccuracyWinMatrixamongcompeting
that are highly correlated, and sampling features that are
augmentationmethodsonHowtoCorrupt.Ourmethodachieves
highlyuncorrelated.Weexperimenteachofthesevariantsin
betterclassificationresultscomparedtotheconventionalapproach
overalargeportionofdatasets. conjunctionwithourclass-conditionedcorruptionmethod.
Specifically, for each feature masking approach with our
class-conditionedcorruptiontechnique,wefollowthepro-
cedurebymergingAlgorithm2andAlgorithm3together.
afterpre-training(evenincludingtheoracle). Thisobser-
vation potentiallyrevealsthat forcertain tabulardatasets, TheclassificationaccuracyresultsaswellastheAUROCre-
conducting contrastive learning via any corruption-based sultsofthecorrespondingmethodsonthedown-streamclas-
augmentationhasitsintrinsiclimitationsingeneral. Webe- sificationtaskovertheOpenML-CC18datasetsareshown
lievethisfindingcouldmotivatefurtherinvestigationsinto in Table 4 and Table 5 in Appendix C, respectively. As
whetherornotperformingcorruptionfordataaugmentation
7TabularDataClass-ConditionedAugmentation
shownbytheresults,therelacksconsistentimprovement resultingsetsoffeaturessharinglittletononecorrelation
fromexploitingfeaturecorrelationswhencorruptingtabular withineachother. Wehopethispapershedslightonfurther
data. Webelievethatthisislargelyduetothefactthatfrom explorationsandimprovementstothechallengingproblem
carefulprocessingandpreparationofthetabulardatasetsin ofeffectivetabulardataaugmentation.
OpenML-CC18,muchoftheredundancyhasbeenremoved
ineachtable,resultinginthesetoffeaturesbeinglargely
Acknowledgements
independentandsharinglittlecorrelationwithineachother.
WerefertotheBalanceScaletableagainasaclearexample, WesincerelythankourcolleagueGabrielLoaiza-Ganemfor
inwhicheachfeatureiscompletelyindependentfromall thevaluablediscussionandideas,whichsparkedoneofthe
otherfeatures. explorationdirectionsinthispaperontheselectivesubset
offeaturesforcorruption.
Inanattempttounderstandthestrengthoffeaturecorrela-
tionswithineachtable,wehavecomputedandpresented
the feature correlation value range for each table in Ta- References
ble4andTable5. Thefeaturecorrelationvaluerangeisthe
Bahri,D.,Jiang,H.,Tay,Y.,andMetzler,D. Scarf: Self-
rangeoffeaturecorrelationvaluescomputedforeachtable
supervisedcontrastivelearningusingrandomfeaturecor-
as described in Section 3.2. However, we do emphasize
ruption. InInternationalConferenceonLearningRepre-
thatthecorrelationvaluerangeonlyprovidesthetipofthe
sentations,2022.
iceberg in terms of describing the full feature correlation
structurewithinagiventable.Apotentialdirectionoffuture Bischl,B.,Casalicchio,G.,Feurer,M.,Gijsbers,P.,Hutter,
researchistobetterquantifythefeaturecorrelationstructure F.,Lang,M.,Mantovani,R.G.,vanRijn,J.N.,andVan-
andstrengthwithintabulardata,whichwebelievewillalso schoren,J. OpenMLbenchmarkingsuites. InAdvances
benefitthedataaugmentationprocesssuchasthisselective inNeuralInformationProcessingSystemsDatasetsand
featuremaskingapproach. BenchmarksTrack,2021.
Essentially,thereisinsignificantfeaturecorrelationstruc-
Chen,T.andGuestrin,C. XGBoost: Ascalabletreeboost-
ture in these benchmark tables to start with that can be ingsystem. InACMSigKDDInternationalConference
exploitedinthedataaugmentationprocess. Wehypothesize onKnowledgeDiscoveryandDataMining,pp.785–794,
thatourcorrelation-basedfeaturemaskingapproachhasthe
2016.
potentialforimprovingthecontrastivelearningoverlarger,
morecomplex,andmorecrudedatasetsthatarefrequently Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
encounteredinreal-worldapplications. simpleframeworkforcontrastivelearningofvisualrep-
resentations. In International Conference on Machine
Learning,2020.
5.Conclusion
Chen,X.andHe,K. Exploringsimplesiameserepresen-
Weexploreddifferentvariationsintabulardatacorruption
tationlearning. InConferenceonComputerVisionand
basedaugmentationapproachusedincontrastivelearning,
PatternRecognition,2021.
under the semi-supervised learning setting. Specifically,
to ensure high similarities between the augmented views
Darabi, S., Fazeli, S., Pazoki, A., Sankararaman, S., and
andtheanchorrow,weproposedboththeclass-conditioned
Sarrafzadeh,M. SAINT:Improvedneuralnetworksfor
corruptionapproachandthecorrelation-basedfeaturemask-
tabulardataviarowattentionandcontrastivepre-training.
ing approach. Compared to the conventional corruption
arXivpreprintarXiv:2108.12296,2021.
approach,weaimtoincorporatemoreinformationintothe
corruption procedure, specifically, the class information Grandvalet,Y.andBengio,Y. Semi-supervisedlearningby
obtained via pseudo-labeling; and the feature correlation entropyminimization.InAdvancesinNeuralInformation
informationobtainedviatrainedXGBoostmodels. Exten- ProcessingSystems,2004.
siveexperimentalresultssuggestthattheclass-conditioned
Grinsztajn, L., Oyallon, E., and Varoquaux, G. Why do
corruption approach supports learned embedding spaces
tree-basedmodelsstilloutperformdeeplearningontyp-
thatbettersegmentbetweenclasses,andthereforeprovides
ical tabular data? In Advances in Neural Information
improvementstodown-streamclassificationaccuracy. On
ProcessingSystems,2022.
the other hand, the correlation-based feature masking ap-
proachfailstoshowconsistentimprovementscomparedto
Gui, J., Chen, T., Zhang, J., Cao, Q., Sun, Z., Luo, H.,
randomselectionofcorruptedfeatures. Wehypothesizethis
and Tao, D. A survey on self-supervised learning: Al-
islargelyduetothefactthepreprocessingstagesinobtain-
gorithms,applications,andfuturetrends. arXivpreprint
ingthebenchmarkdatasetshaveremovedmuchredundancy,
arXiv:2301.05712,2023.
8TabularDataClass-ConditionedAugmentation
Iida,H.,Thai,D.,Manjunatha,V.,andIyyer,M. TABBIE:
Pretrainedrepresentationsoftabulardata. InConference
of the North American Chapter of the Association for
ComputationalLinguistics,2021.
Khosla,P.,Teterwak,P.,Wang,C.,Sarna,A.,Tian,Y.,Isola,
P.,Maschinot,A.,Liu,C.,andKrishnan,D. Supervised
contrastivelearning. InAdvancesinNeuralInformation
ProcessingSystems,2020.
Ma, S., Zeng, Z., McDuff, D., and Song, Y. Active con-
trastivelearningofaudio-visualvideorepresentations. In
InternationalConferenceonLearningRepresentations,
2021.
Ma, S., Zeng, Z., McDuff, D., and Song, Y. VICReg:
Variance-invariance-covariance regularization for self-
supervised learning. In International Conference on
LearningRepresentations,2022.
Nam,J.,Tack,J.,Lee,K.,Lee,H.,andShin,J. STUNT:
Few-shottabularlearningwithself-generatedtasksfrom
unlabeledtables. InInternationalConferenceonLearn-
ingRepresentations,2023.
Pan,T.,Song,Y.,Yang,T.,Jiang,W.,andLiu,W. Video-
MoCo: Contrastive video representation learning with
temporallyadversarialexamples. InConferenceonCom-
puterVisionandPatternRecognition,2021.
Shorten,C.,Khoshgoftaar,T.M.,andFurht,B. Textdata
augmentationfordeeplearning. JournalofBigData,8
(101),2021.
Somepalli, G., Goldblum, M., Schwarzschild, A., Bruss,
C. B., and Goldstein, T. SAINT: Improved neural net-
worksfortabulardataviarowattentionandcontrastive
pre-training. arXivpreprintarXiv:2106.01342,2021.
Ucar, T., Hajiramezanali, E., and Edwards, L. SubTab:
Subsettingfeaturesoftabulardataforself-supervisedrep-
resentationlearning. InAdvancesinNeuralInformation
ProcessingSystems,2021.
Wang, Z. and Sun, J. TransTab: Learning transferable
tabulartransformersacrosstables. InAdvancesinNeural
InformationProcessingSystems,2022.
Yang, X., Song, Z., King, I., and Xu, Z. A survey on
deep semi-supervised learning. IEEE Transactions on
Knowledge and Data Engineering, 35(9):8934 – 8954,
2022.
Yoon,J.,Zhang,Y.,Jordon,J.,andSchaar,M.v.d. VIME:
Extendingthesuccessofself-andsemi-supervisedlearn-
ingtotabulardomain. InAdvancesinNeuralInformation
ProcessingSystems,2020.
9TabularDataClass-ConditionedAugmentation
A.SamplingProcedureforCorrelation-BasedFeatureMasking
Inourproposedcorrelation-basedfeaturemaskingapproachasinSection3.2,thesubsetoffeaturestobecorruptedon
eachanchorrowissampledconditionedonthefeaturecorrelations. Inthisappendixsection,weprovideindetailsforthis
samplingprocedure.
Firstly,asdescribedinAlgorithm3,foratablewithM features,wetrainanXGBoostmodelforpredictingeachfeature
basedontheremainingM−1features. WethencollectM setsoffeatureimportancevalues,whichwedenoteby{q }for
k
k ∈{1,...,M},suchthatq(j)denotesthefeatureimportanceofthej-thfeatureinpredictingthek-thfeature.
k
Basedonthesetoffeatureimportancevalues,wesamplethesubsetoffeaturesforeachanchorindependently,throughthe
procedureasdetailedinAlgorithm4.
Algorithm4SamplingProcedurefortheFeatureSubsetBasedonFeatureCorrelationValues
input {q }fork∈{1,...,M}
k
input M:thenumberoffeatures
input p:thepercentageoffeaturestobecorrupted
1: InitializeC ∈RM×M asfollows:
corr
C [i,i]=0 ∀i∈[M]
corr
C [i,j]=q(j) ∀i∈[M],j ∈[M],i̸=j ifsamplingmostcorrelatedfeatures
corr i
C [i,j]=−q(j) ∀i∈[M],j ∈[M],i̸=j ifsamplingleastcorrelatedfeatures
corr i
2: Uniformlysamplef fromtheindexset[M].
1
3: InitializethesetM ←[f ].
crpt 1
4: fori←2to⌈Mp⌉do
5: Selectthesub-matrixCpart ←C [M ,:] (SelectthesetofrowsfromC indexedbyM )
corr corr crpt corr crpt
6: p ←min(Cpart ,axis=1) (Selectthecolumn-wiseminimumvalues)
i corr
7: p(k) ←0 ∀k∈M (Setprobabilitiesforalreadyselectedfeaturestozero)
i crpt
8: pˆ ← pi
i (cid:80) k(cid:16) p i(k)(cid:17)
9: Samplethefeatureindexf ∼pˆ .
i i
10: Appendf toM .
i crpt
11: endfor
12: ReturnM
crpt
Notethatinobtainingtheunnormalizedsamplingprobabilityvectorp inAlgorithm4,wetaketheminimumamongfeature
i
importancevaluesamongthecolumns(i.e. alongthefeatures)ofthesub-matrixCpart . Thisminimizationstepallowsusto
corr
collectanewfeaturethatismostcorrelated(orleastcorrelated)toeachofthealreadyselectedfeaturesinthesetM .
crpt
10TabularDataClass-ConditionedAugmentation
B.ClassificationAccuraciesonHowtoCorruptunderRandomFeatureSelection
Table2.Semi-SupervisedClassificationAccuracies(inPercentages)onHowtoCorrupt.Foreachmethod(exceptforNo-PreTrain),the
featurescorruptedarerandomlyselected.
Datasets(DID) No-PreTrain Random Class(ours) Oracle
balance-scale(11) 90.00±0.66 90.50±0.93 91.60±0.97 94.50±0.67
mfeat-fourier(14) 68.00±0.88 78.19±0.96 79.22±0.79 81.88±0.58
breast-w(15) 95.98±0.56 96.79±0.40 96.70±0.36 96.61±0.30
mfeat-karhunen(16) 81.75±0.68 95.22±0.43 94.50±0.28 97.19±0.26
mfeat-morphological(18) 72.62±0.48 70.62±0.58 71.19±0.33 73.59±0.73
mfeat-zernike(22) 76.53±0.44 76.22±0.65 78.78±0.46 82.44±0.51
cmc(23) 53.43±0.95 50.51±0.94 51.53±1.40 50.21±1.03
credit-approval(29) 82.25±1.03 80.89±1.30 83.42±1.26 84.51±1.23
credit-g(31) 71.31±0.40 71.25±1.14 72.88±1.35 75.56±0.72
diabetes(37) 74.03±0.74 72.89±1.06 74.76±0.91 73.78±1.09
tic-tac-toe(50) 78.39±0.70 88.22±0.80 85.68±1.31 88.87±0.60
vehicle(54) 72.06±0.91 71.10±1.11 73.24±0.80 74.34±1.09
eucalyptus(188) 54.56±1.27 54.31±1.07 58.36±1.07 61.91±0.97
analcatdata-authorship(458) 94.67±0.47 99.85±0.14 99.70±0.10 99.63±0.15
analcatdata-dmft(469) 20.08±1.06 18.67±0.93 19.45±0.70 19.61±0.86
pc4(1049) 89.43±0.40 85.92±0.76 87.97±0.54 89.00±0.62
pc3(1050) 88.74±0.29 89.26±0.49 88.70±0.34 88.78±0.46
kc2(1063) 83.57±0.62 82.86±0.58 83.45±0.67 83.21±0.77
pc1(1068) 92.51±0.31 92.91±0.21 91.84±0.64 93.13±0.07
banknote-authentication(1462) 99.64±0.09 99.64±0.17 99.05±0.23 99.41±0.16
blood-transfusion-service-center(1464) 78.75±0.36 76.92±0.91 77.67±1.02 77.67±0.93
ilpd(1480) 68.16±1.54 68.06±1.06 68.27±1.07 70.09±1.36
qsar-biodeg(1494) 83.41±1.11 84.06±0.71 83.59±0.62 86.97±0.94
wdbc(1510) 95.61±0.68 96.60±0.61 97.15±0.37 97.92±0.44
cylinder-bands(6332) 63.89±1.39 70.02±1.10 68.63±1.24 74.07±1.01
dresses-sales(23381) 55.12±0.91 51.62±1.36 50.88±1.79 53.00±1.41
MiceProtein(40966) 74.25±1.98 79.11±1.16 89.58±0.73 95.83±0.50
car(40975) 85.51±0.77 93.21±0.25 93.10±0.53 96.17±0.44
steel-plates-fault(40982) 70.73±0.67 68.32±0.67 70.98±0.95 72.78±0.55
climate-model-simulation-crashes(40994) 92.48±0.30 92.59±0.46 91.55±0.62 93.40±0.42
InTable2above,theresultswhereourproposedclass-conditionedcorruptionapproachoutperformstheconventionalrandom
corruptionapproachwithconcretemarginsaremarkedinboldfonts. Inthemajorityofthedatasets,theclass-conditioned
approachshowsimprovementsovertheconventionalapproach,whilethetwomethodsshowclosetoequalperformancesfor
themostoftheremainingdatasets.
11TabularDataClass-ConditionedAugmentation
Table3.Semi-SupervisedClassificationAUROCsonHowtoCorrupt.Foreachmethod(exceptforNo-PreTrain),thefeaturescorrupted
arerandomlyselected.
Datasets(DID) No-PreTrain Random Class Oracle
balance-scale(11) 0.95±0.00 0.95±0.01 0.97±0.01 0.99±0.00
mfeat-fourier(14) 0.94±0.00 0.97±0.00 0.97±0.00 0.98±0.00
breast-w(15) 0.99±0.00 0.99±0.00 0.99±0.00 0.99±0.00
mfeat-karhunen(16) 0.98±0.00 1.00±0.00 1.00±0.00 1.00±0.00
mfeat-morphological(18) 0.96±0.00 0.96±0.00 0.96±0.00 0.96±0.00
mfeat-zernike(22) 0.97±0.00 0.97±0.00 0.97±0.00 0.98±0.00
cmc(23) 0.70±0.01 0.68±0.01 0.69±0.01 0.69±0.01
credit-approval(29) 0.89±0.01 0.88±0.01 0.90±0.01 0.92±0.01
credit-g(31) 0.69±0.01 0.74±0.01 0.74±0.01 0.78±0.01
diabetes(37) 0.80±0.01 0.79±0.01 0.78±0.02 0.81±0.01
tic-tac-toe(50) 0.84±0.01 0.94±0.00 0.92±0.01 0.96±0.00
vehicle(54) 0.91±0.00 0.89±0.01 0.91±0.00 0.92±0.00
eucalyptus(188) 0.84±0.00 0.79±0.01 0.83±0.01 0.86±0.01
analcatdata-authorship(458) 1.00±0.00 1.00±0.00 1.00±0.00 1.00±0.00
analcatdata-dmft(469) 0.54±0.01 0.52±0.01 0.53±0.01 0.53±0.01
pc4(1049) 0.87±0.01 0.82±0.01 0.84±0.01 0.88±0.01
pc3(1050) 0.77±0.02 0.80±0.02 0.79±0.02 0.82±0.02
kc2(1063) 0.80±0.02 0.80±0.02 0.78±0.02 0.79±0.03
pc1(1068) 0.74±0.03 0.67±0.04 0.74±0.02 0.74±0.01
banknote-authentication(1462) 1.00±0.00 1.00±0.00 1.00±0.00 1.00±0.00
blood-transfusion-service-center(1464) 0.74±0.01 0.69±0.02 0.69±0.02 0.69±0.02
ilpd(1480) 0.71±0.02 0.71±0.01 0.70±0.02 0.72±0.02
qsar-biodeg(1494) 0.89±0.00 0.89±0.00 0.89±0.01 0.92±0.01
wdbc(1510) 0.99±0.00 0.99±0.00 0.99±0.00 1.00±0.00
cylinder-bands(6332) 0.67±0.02 0.76±0.01 0.75±0.01 0.81±0.01
dresses-sales(23381) 0.56±0.02 0.49±0.02 0.49±0.02 0.52±0.02
MiceProtein(40966) 0.97±0.00 0.97±0.00 0.98±0.00 1.00±0.00
car(40975) 0.96±0.00 0.99±0.00 0.99±0.00 1.00±0.00
steel-plates-fault(40982) 0.92±0.00 0.92±0.00 0.92±0.01 0.93±0.00
climate-model-simulation-crashes(40994) 0.85±0.02 0.86±0.02 0.85±0.02 0.91±0.01
In Table 3 above, the results where our proposed class-conditioned corruption approach outperforms the conventional
randomcorruptionapproachwithconcretemarginsaremarkedinboldfonts.
12TabularDataClass-ConditionedAugmentation
C.ClassificationPerformancesonWheretoCorruptforCorrelation-BasedFeatureMasking
Table4.Semi-SupervisedClassificationAccuracies(inPercentages)onWheretoCorrupt.Foreachmethod(exceptforNo-PreTrain),the
class-conditionedcorruptionstrategyisusedforsamplingvaluesusedinthecorruption.
FeatureCorrelation
Datasets(DID) RandomFeatures LeastCorrelated MostCorrelated
ValueRange
balance-scale(11) 0.03 91.60±0.97 92.20±0.94 91.70±0.84
mfeat-fourier(14) 0.14 79.22±0.79 79.03±0.69 78.94±0.66
breast-w(15) 0.33 96.70±0.36 96.70±0.44 96.52±0.41
mfeat-karhunen(16) 0.07 94.50±0.28 94.84±0.35 94.97±0.22
mfeat-morphological(18) 0.72 71.19±0.33 70.50±0.54 71.16±0.31
mfeat-zernike(22) 0.51 78.78±0.46 78.56±0.28 78.81±0.29
cmc(23) 0.13 51.53±1.40 50.04±0.72 50.30±0.76
credit-approval(29) 0.29 83.42±1.26 83.51±1.17 83.42±1.22
credit-g(31) 0.11 72.88±1.35 72.88±0.98 74.25±0.56
diabetes(37) 0.22 74.76±0.91 73.46±1.57 73.78±1.05
tic-tac-toe(50) 0.04 85.68±1.31 86.72±0.94 87.30±0.82
vehicle(54) 0.44 73.24±0.80 71.32±1.07 72.21±0.89
eucalyptus(188) 0.38 58.36±1.07 51.69±1.07 52.53±1.02
analcatdata-authorship(458) 0.11 99.70±0.10 99.63±0.10 99.63±0.21
analcatdata-dmft(469) 0.18 19.45±0.70 18.91±0.71 19.38±0.99
pc4(1049) 0.39 87.97±0.54 87.71±0.59 87.33±0.69
pc3(1050) 0.37 88.70±0.34 89.38±0.34 89.42±0.24
kc2(1063) 0.36 83.45±0.67 82.86±0.77 84.17±0.95
pc1(1068) 0.36 91.84±0.64 92.91±0.26 93.13±0.07
banknote-authentication(1462) 0.25 99.05±0.23 99.23±0.19 99.09±0.19
blood-transfusion-service-center(1464) 0.48 77.67±1.02 76.25±0.90 75.92±0.65
ilpd(1480) 0.35 68.27±1.07 68.27±0.67 68.70±0.87
qsar-biodeg(1494) 0.34 83.59±0.62 83.89±0.59 84.06±0.60
wdbc(1510) 0.43 97.15±0.37 97.26±0.42 96.93±0.35
cylinder-bands(6332) 0.19 68.63±1.24 68.63±1.17 69.33±1.17
dresses-sales(23381) 0.05 50.88±1.79 51.00±0.40 51.50±0.79
MiceProtein(40966) 0.26 89.58±0.73 88.95±1.15 89.47±0.90
car(40975) 0.01 93.10±0.53 93.28±0.70 93.28±0.43
steel-plates-fault(40982) 0.55 70.98±0.95 71.18±0.93 71.14±0.76
climate-model-simulation-crashes(40994) 0.07 91.55±0.62 91.44±0.48 90.97±0.46
13TabularDataClass-ConditionedAugmentation
Table5.Semi-SupervisedClassificationAUROCsonWheretoCorrupt.Foreachmethod(exceptforNo-PreTrain),theclass-conditioned
corruptionstrategyisusedforsamplingvaluesusedinthecorruption.
FeatureCorrelation
Datasets(DID) RandomFeatures LeastCorrelated MostCorrelated
ValueRange
balance-scale(11) 0.03 0.97±0.01 0.97±0.00 0.97±0.01
mfeat-fourier(14) 0.14 0.97±0.00 0.97±0.00 0.97±0.00
breast-w(15) 0.33 0.99±0.00 0.99±0.00 0.99±0.00
mfeat-karhunen(16) 0.07 1.00±0.00 1.00±0.00 1.00±0.00
mfeat-morphological(18) 0.72 0.96±0.00 0.96±0.00 0.96±0.00
mfeat-zernike(22) 0.51 0.97±0.00 0.97±0.00 0.97±0.00
cmc(23) 0.13 0.69±0.01 0.68±0.00 0.68±0.00
credit-approval(29) 0.29 0.90±0.01 0.90±0.01 0.90±0.01
credit-g(31) 0.11 0.74±0.01 0.74±0.01 0.75±0.01
diabetes(37) 0.22 0.78±0.02 0.78±0.02 0.78±0.01
tic-tac-toe(50) 0.04 0.92±0.01 0.93±0.01 0.93±0.00
vehicle(54) 0.44 0.91±0.00 0.90±0.00 0.90±0.00
eucalyptus(188) 0.38 0.83±0.01 0.77±0.01 0.78±0.01
analcatdata-authorship(458) 0.11 1.00±0.00 1.00±0.00 1.00±0.00
analcatdata-dmft(469) 0.18 0.53±0.01 0.52±0.01 0.53±0.01
pc4(1049) 0.39 0.84±0.01 0.84±0.01 0.84±0.01
pc3(1050) 0.37 0.79±0.02 0.80±0.02 0.80±0.02
kc2(1063) 0.36 0.78±0.02 0.79±0.02 0.79±0.02
pc1(1068) 0.36 0.74±0.02 0.62±0.05 0.67±0.02
banknote-authentication(1462) 0.25 1.00±0.00 1.00±0.00 1.00±0.00
blood-transfusion-service-center(1464) 0.48 0.69±0.02 0.68±0.02 0.68±0.02
ilpd(1480) 0.35 0.70±0.02 0.70±0.01 0.69±0.01
qsar-biodeg(1494) 0.34 0.89±0.01 0.89±0.01 0.89±0.01
wdbc(1510) 0.43 0.99±0.00 1.00±0.00 1.00±0.00
cylinder-bands(6332) 0.19 0.75±0.01 0.76±0.01 0.76±0.01
dresses-sales(23381) 0.05 0.49±0.02 0.50±0.01 0.50±0.02
MiceProtein(40966) 0.26 0.98±0.00 0.98±0.00 0.98±0.00
car(40975) 0.01 0.99±0.00 0.99±0.00 0.99±0.00
steel-plates-fault(40982) 0.55 0.92±0.01 0.92±0.00 0.92±0.00
climate-model-simulation-crashes(40994) 0.07 0.85±0.02 0.74±0.02 0.73±0.02
14