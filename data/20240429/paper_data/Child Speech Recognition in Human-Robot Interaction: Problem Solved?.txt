Child Speech Recognition in Human-Robot Interaction: Problem
Solved?
RubenJanssens‚àó EvaVerhelst‚àó GiulioAntonioAbbo
ruben.janssens@ugent.be eva.verhelst@ugent.be giulioantonio.abbo@ugent.be
IDLab-AIRO,GhentUniversity-imec IDLab-AIRO,GhentUniversity-imec IDLab-AIRO,GhentUniversity-imec
Ghent,Belgium Ghent,Belgium Ghent,Belgium
QiaoqiaoRen MariaJosePintoBernal TonyBelpaeme
qiaoqiao.ren@ugent.be mariajose.pintobernal@ugent.be tony.belpaeme@ugent.be
IDLab-AIRO,GhentUniversity-imec IDLab-AIRO,GhentUniversity-imec IDLab-AIRO,GhentUniversity-imec
Ghent,Belgium Ghent,Belgium Ghent,Belgium
ABSTRACT continuousacousticsignalsintodiscretesymbolicrepresentations,
AutomatedSpeechRecognitionshowssuperhumanperformance typicallytext‚Äìhasbeenstudiedextensivelyinacademicandin-
for adult English speech on a range of benchmarks, but disap- dustrialresearch.Inrecentdecades,ASRperformancehascome
pointswhenfedchildren‚Äôsspeech.Thishaslongsatinthewayof alonginleapsandbounds,withcompaniesclaiming‚Äúsuper-human
child-robotinteraction.Recentevolutionsindata-drivenspeech performance‚ÄùonconversationalASRbenchmarksin2017[8].On
recognition,includingtheavailabilityofTransformerarchitectures certainbenchmarksandforresource-richlanguages,bothinterms
andunprecedentedvolumesoftrainingdata,mightmeanabreak- oftrainingdataavailabilityandprioritiesimposedbyeconomic
throughforchildspeechrecognitionandsocialrobotapplications returnsoninvestment,speechrecognitionperformanceisonpar
aimedatchildren.Werevisitastudyonchildspeechrecognition orevenbetterthanmeanhumantranscriptionperformance.The
from2017andshowthatindeedperformancehasincreased,with popularmetricforASRperformanceisWordErrorRate(WER),
newcomerOpenAIWhisperdoingmarkedlybetterthanleading calculatedasthetotalnumberoferrors‚Äîsubstitutions,insertions,
commercialcloudservices.Whiletranscriptionisnotperfectyet, anddeletions‚Äîdividedbythetotalnumberofwordsinthetext.
thebestmodelrecognises60.3%ofsentencescorrectlybarringsmall WERwastypicallyreportedtobebelow5%.Thesesystemsrelied
grammaticaldifferences,withsub-secondtranscriptiontimerun- onneuralnetworkssuchasCNNsandLSTMstoextractfeatures
ningonalocalGPU,showingpotentialforusableautonomous fromaudiosignalsandconverttimeseriestotext.Combinedwith
child-robotspeechinteractions. large,annotatedtrainingsetsandunsupervisedlearning,thesesys-
temsimprovedoverearliermodel-basedlearning.However,while
CCSCONCEPTS impressive,thesesystems‚Äôperformancedegradedcatastrophically
onspeechforwhichitwasnotoptimised,includingatypicalvoices
‚Ä¢Computersystemsorganization‚ÜíRobotics;‚Ä¢Computing
suchasthespeechofelderlyoryoungchildren.Thishasrepercus-
methodologies‚ÜíSpeechrecognition;‚Ä¢Socialandprofes-
sionsforHRIandspecificallyforapplicationsinwhichautonomous
sionaltopics‚ÜíChildren.
socialrobotsareexpectedtointeractwithnon-typicalusers,such
asrobotsforeldercareorrobotsforeducation[1].
KEYWORDS
In2017,Kennedyetal.[3]publishedawidelycitedstudyshow-
Child-RobotInteraction; AutomaticSpeech Recognition; Verbal ingthatthenstate-of-the-artASRcouldnotreliablytranscribethe
Interaction;InteractionDesignRecommendations speechofnative5-year-oldEnglishspeakers.Theyrecordedspeech
from11childreninaprimaryschoolintheU.K.Thespeechranged
fromconstrainedutterances‚Äìsuchascountingfrom1to10‚Äìto
unconstrainedtellingofastoryfromapicturebook.Recordings
1 INTRODUCTIONANDBACKGROUND
weremadeusingthreedifferentmicrophones,toevaluatewhether
SpokenlanguageinteractionisformanytheholygrailinHCIand thequalityandhardwareintegrationofthemicrophoneintoarobot
HRI.Itisbuiltuponacollectionoftechnologies,suchasAutomated hadanimpactonASR.TheASRperformancewasevaluatedfor
SpeechRecognition,DialogueManagement,orText-to-Speech,that fourdifferentengines,threecommercialASRsolutions‚ÄîNuance
arechainedtogethertocreateasystemwhichallowstheuserto VoCon4.7,MicrosoftSpeechAPI(2016),GoogleSpeechAPI(2016)‚Äî
interactorconversewithanartificialsystemusingthemostnatu- andCMUPocketSphinx,theleadingopen-sourcesolutionatthe
ralinterfaceknowntoman.Whilethisprocessingchainisbrittle, time.Theresultswerenothingbutdisappointing.WhileWERfor
thepointofentryisAutomatedSpeechRecognition(ASR).The adultspeechwasbelow5%,mostenginescouldnotcorrectlytran-
abilitytoautomaticallytranscribespeechutterances‚Äìconverting scribeasinglechildutterance.OnlyGoogle‚ÄôsASRdidmarginally
better,recognising11.8%ofconstrainedchildspeechandabout6%
‚àóEqualcontributionandjointfirstauthors. ofspontaneouschildspeech.Still,onlycorrectlybeingabletotran-
Citeas:RubenJanssens,EvaVerhelst,GiulioAntonioAbbo,QiaoqiaoRen,Maria
scribe1utteranceoutof10isarecipeforinteractiondisaster,and
JosePintoBernalandTonyBelpaeme.2024.ChildSpeechRecognitioninHuman-
RobotInteraction:ProblemSolved?.In2024InternationalSymposiumonTechnological
AdvancesinHuman-RobotInteraction,March8-9,Boulder,CO,USA.
4202
rpA
62
]LC.sc[
1v49371.4042:viXraTAHRI‚Äô24,March09‚Äì10,2024,Boulder,CO RubenJanssens,EvaVerhelst,GiulioAntonioAbbo,QiaoqiaoRen,MariaJosePintoBernalandTonyBelpaeme
theauthorsofthestudyatthetimerecommendedagainstrelying Whispermodels,whichclaimsatranscriptiontimeofuptofour
onASRforchild-robotinteraction. timesfasterthanOpenAI‚ÄôsoriginalWhisperimplementation.The
Forward6years.Artificialintelligencehasbeenrevolutionised WhispermodelswererunlocallyonanNVIDIAGeForceGTX1080
bytheTransformerarchitectureforsequence-to-sequencetasks, Tiwith11GBofVRAM.WealsoranthemononlyaCPU,toassess
notonlyresultinginaseaofchangeintheperformanceofgener- thenecessityofadedicatedGPUforthesemodels.Weconfigured
ativelanguagemodelsbutalsointheperformanceofASR[5].In themodelstoexpectEnglishlanguagespeech,aspreliminarytest-
September2022,OpenAIreleasedWhisper,anASRenginebuilt ingrevealedthatwithoutthisoptionWhisperLarge-v3correctly
usinganencoder-decoderTransformerarchitecturetrainedonan detectedEnglishinonly84%ofspontaneousspeechsamples.All
unprecedented680,000hoursoflabelledaudiodata[6].Whilethe transcriptionswereperformedinJanuary2024.
specificsofWhisper‚Äôstrainingregimenanditstrainingdataare Theperformanceofthemodelsiscomparedusingthreedifferent
proprietarytoOpenAI,theinferencemodelisreleasedaspublic metrics,asin[3].Primarily,weusetheLevenshteindistanceatthe
open-sourcesoftware.Whisper‚Äôsperformanceonaverageisbetter letterlevel,whichrepresentstheminimumamountofinsertions,
thancompetingsolutions,butwasfoundtostillbesubpartosolu- deletionsandsubstitutionsrequiredtochangeonesequenceinto
tionsthathavebeenspecificallytrainedorfine-tunedonspecific theother.Usingthismetric,smallerrorsarepenalisedlessthan
datasets,suchasLibriSpeech[6]. theywouldbewhenusingametricliketheWordErrorRate.For
NexttothepubliclyavailableWhispermodels,whichstillre- example,whentheword‚Äúrobots"isrecognizedinsteadoftheword
quireonetoinstallandruntheASRonownhardware,thereare ‚Äúrobot",theLevenshteindistancewouldbe1(asonlyoneeditis
severalcloud-basedsolutions.Inthisarealargeplayers‚ÄîAmazon, neededtochangetherecognisedwordintotheoriginalword).We
Google,MicrosoftandTencent‚Äîcompetewithsmaller,sometimes thennormalisethismetricbytheamountoflettersintheground
specialisedvendors,butallofferconvenientonlineservicesthat truthsequence.Ascoreof0meansperfectrecognition,ascoreof1
areeasilyintegratedwithincode. couldreflectarecognisedsequenceofthesamelengthbutwithno
Giventheavailabilityofnewarchitecturestrainedonlargerand singleletterintherightposition.Furthermore,wealsoreportthe
morediversecorpora,thetimeisopportunetorevisittheresults recognitionpercentage,whichrepresentstheamountofutterances
fromKennedyetal.[3]andevaluatewhetherstate-of-the-artASR thatarecompletelycorrectlyrecognised.Finally,alsoarelaxed
cannowhandlechildspeech.WedecidedtocompareOpenAI‚Äôs accuracyisreported:thismeasurecountshowmanyutterancesare
Whisper,asitisopen-sourceandexemplifiesthenewdirectionin correctlyrecognised,alsocountingasaccuratethosewithsmall
data-drivenASR,andtwocommercialcloud-basedsolutions,opting grammatical differences that do not impact the meaning of the
forMicrosoftAzureSpeechtoText,duetoitspopularityandthefact utterance,followingthesamerulesasin[3].
thatweintegrateitintoourrobotsystemsatGhentUniversity,and Toestimatethepossibilityofreal-timeinteractions,weexplore
GoogleCloudSpeech-to-text.Wearefirstandforemostinterested theresponsivenessofthedifferentsystemsbyreportingtheirtran-
intranscriptionaccuracy,butforouraimofintegratingchildspeech scriptiontime.ForallWhispermodels,thetranscriptiontimeis
recognitionintoaninteractiveHRIscenario,wealsowishtoexplore thetimeittakesforthemodeltoreturnaresult,whichvariesdue
howresponsive differentsystemsareand towhichextentthey tothemodelsizeaswellasthehardwareonwhichitruns.Asthe
wouldsupportreal-timespokeninteraction. AzureandGooglesystemsarecloud-based,theirtranscriptiontime
alsoincludesthetransmissiontimeoftheaudiofileandtheresult.
2 METHODOLOGY Inallanalyses,unlessotherwisestated,weuseonlythestudiomi-
ToevaluatetheASRengines,weusethedatafrom[2]whichcon- crophonerecordings,andonlytherecordingsofthesentencesthat
tains audio recordings (44KHz lossless WAV files) of 11 young
thechildrenrepeatfromtheadult(ùëõ=50)andofthespontaneous
children(ageM=4.9yearsold;5females,6males)recordedatan
utterances(splitintosentences,ùëõ=222),becausepreliminaryanal-
English primary school. The recordings consist of spontaneous ysisshowedthatutterancesconsistingofasinglenumberareoften
speech(retellingapicturebook,‚ÄòFrog,WhereAreYou?‚ÄôbyMercer tooshortfortheenginestodetectanyspeech.
Mayer)andspeechinwhichchildrencountfrom1to10orrepeat
shortsentencesspokenbyanadult(suchas‚Äúthehorseisinthe 3 RESULTS
stable‚Äù).Eachsampleisrecordedfrom3sources:astudio-grademi- Wewillfirstcomparethemodels‚Äôtranscriptionaccuracy,followed
crophone(RodeNT1-A),aportablemicrophone(ZoomH1)andthe bytheresponsiveness,theimpactofthemicrophoneused,and
twofrontmicrophonesoftheAldebaranNAOrobot.Allrecordings finallyareflectiononthepowerconsumptionofthemodels.
havebeenmanuallytranscribedandthisisusedasgroundtruth.
WeevaluatedthreeASRengines:MicrosoftAzureSpeechto
3.1 Transcriptionaccuracy
text, Google Cloud Speech-to-text, and OpenAI‚Äôs Whisper. The
AzureandGooglemodelswereusedthroughacloudAPI.Whis- Firstofall,wecomparetheperformanceofGoogle,Azureandthe
per exists in different model sizes: tiny (39M parameters), base, bestWhispermodel(large-v3)withthefourenginesreportedin
small,medium,andlarge(1550Mparameters),withthreeversions the2017paper.TheseresultsareshowninFigure1.Theyshow
ofthelargemodel.Allsevenofthesemodelsarecomparedinthis thattheGooglespeechrecognitiondidnotimprovecomparedto
study:weexpectthesmallermodelstorunfasterbuthavelower
2017(Levenshteindistanceùêøùê∑ =0.38in2017andin2024),butthe
accuracy.Weusedthefaster-whisper1reimplementationofthe performanceofboththeAzuremodel(ùêøùê∑ =0.23),andtheWhisper
model(ùêøùê∑ = 0.14)arebetterthanallmodelstestedinthe2017
1github.com/SYSTRAN/faster-whisper paper,withWhisperperformingbestofall.ChildSpeechRecognitioninHuman-RobotInteraction:ProblemSolved? TAHRI‚Äô24,March09‚Äì10,2024,Boulder,CO
1.0
2017 0.40
2024 0.8
0.35
0.6
0.30
0.4
0.25
0.2
0.20
0.0
Nuance Sphinx Bing Google Google Azure Whisper 0.15
Model
0.10
Figure1:PerformanceofASRenginesin2017and2024,cal-
culatedasmeannormalisedLevenshteindistancebetween 0.05
groundtruthandtranscription(lowerisbetter).
0.00
Table1:ExamplesofSmallTranscriptionMistakes
Groundtruth thedogisinfrontofthehorse
Whisper thedogisthefrontofthehorse
Azure thedogisthefrontofthehorse
Model
Google thesonginthefrontofthehorse
Figure2:PerformanceofallcurrentASRengines(Whisper
modelversionsingreen,GoogleandAzureinred)calculated
This is also reflected in the recognition percentage: in 2017,
asLevenshteindistancebetweengroundtruthandtranscrip-
Googlewasabletorecognise7.5%ofutterancescorrectly,in2024,
tion(lowerisbetter).
thisbecame9.6%,Azurerecognises23.5%,andWhisper36.8%.
Therelaxedaccuracyscoregivesanimpressionoftheusability systemsadelayofbetween700and1000msisdeemedacceptable
ofthemodels:in2017,Googlerecognised20.3%oftheutterances [7].Fromthisdata,itcanbeconcludedthatusingalocalmodelrun
correctlyusingrelaxedcriteria.Thiswasonly14.7%in2024,with onaGPU,insteadofCPUorusinganAPI,cangreatlyimprovethe
Azureachieving43.0%andWhisper60.3%. responsiveness,untilanacceptablelevelforspokendialogue.
Whilethisisnotyetanidealperformancelevel,thisshowsthat TheKruskal-Wallistestshowssignificantdifferencesbetween
Whisperisalreadyratherusable,assmallmistakesthatdonot thetranscriptiontimeofthetestedmodels(ùëù <0.001),andpost-hoc
countasaccuratefortherelaxedaccuracycriteria,couldstillbe DunntestswithBonferronicorrectionshowsignificantdifferences
handledbydialoguemanagementsoftware.Table1showssome betweenallpairsofmodels,exceptforbetweenWhispertiny,Whis-
examplesofsmallmistakesstillmadebyGoogle,Azureandthe perbaseandWhispersmall,betweenWhisperlargev3andWhisper
best-performingWhispermodel. medium,andbetweenWhisperlarge,Whisperlargev2andAzure.
Figure2showsamoredetailedcomparisonbetweenthedifferent Figure4showstherelationbetweenthemodels‚Äôaveragetran-
model sizesof Whisperand the Azureand Google services. As scriptiontimewiththeiraccuracyusingtheLevenshteindistance.
expected,thelargeWhispermodelsperformbest,withWhisper Tochoosewhichmodeltouse,bothresponsivenessandperfor-
largev3performingbestofall. manceshouldbetakenintoaccount.Lowerresultsarepreferredfor
TheKruskal-Wallistestrevealssignificantdifferencesbetween both,somodelsinthelowerleftcornerofthescatterplotareideal.
theLevenshteindistanceforthetestedmodels(ùëù < 0.001),and Asapparentinthefigure,thereisatrade-offbetweentranscription
post-hocDunntestswithBonferronicorrectiondonotshowsignif- timeandtranscriptionperformance,sothechoiceshouldbemade
icantdifferencesbetweenWhisperlargev3andWhispersmall,but basedonthespecificapplication.
doshowasignificantdifferencebetween,amongothers,alllarge
WhispermodelsandAzure(ùëù < 0.005),andbetweenAzureand 3.3 Microphone
Google(ùëù <0.001).
Figure5showstheLevenshteindistancewhenusingaudiorecorded
by the three different microphones. Here, the transcriptions by
3.2 Responsiveness
Google,AzureandWhisperlargev3wereused.Whencompar-
InFigure3,theaveragetranscriptiontimesforshortsentences ingtheresultsoftheinternalNaomicrophonewiththeportable
(spontaneousspeechandrepeatsentences)areshownforGoogle, andstudiomicrophone,theKruskal-Wallistestshowsasignificant
Azure,alloftheWhispermodelsonGPUandthetiny,baseand differencebetweenthegroups,andDunn‚ÄôstestwithBonferroni
smallWhispermodelsonCPU.ThetranscriptiontimefortheWhis- correctionsaspost-hocanalysisshowsasignificantdifferencebe-
permediumandlargemodelsonCPUarerespectively17.5sand tweentheNaoandportablemicrophone(ùëù <0.001)andtheNao
30.5s,andwereleftoutofthegraphforreadability.Wevisually andstudiomicrophone(ùëù < 0.01).Thereisnosignificantdiffer-
markthe1000mslineonthefigurebecause,eventhoughthemean encebetweenthestudioandportablemicrophone(ùëù =0.399).In
responsetimeinhumanconversationis200ms,forspokendialogue conclusion,theworstresultsareobtainedwhenthemicrophone
ecnatsid
niethsneveL
ecnatsid
niethsneveL
3v
egral
repsihW
egral
repsihW
2v
egral
repsihW
muidem
repsihW
llams
repsihW
eruzA esab
repsihW
ynit
repsihW
elgooGTAHRI‚Äô24,March09‚Äì10,2024,Boulder,CO RubenJanssens,EvaVerhelst,GiulioAntonioAbbo,QiaoqiaoRen,MariaJosePintoBernalandTonyBelpaeme
0.30
6
0.25
5
0.20
4
0.15
3
0.10
2
0.05
1 0.00
Portable Studio Nao
Microphone
0
Figure5:PerformanceofAzure,GoogleandbestWhisper
modelwhenusingdifferentmicrophonetypes,calculated
as Levenshtein distance (lower is better). Best results are
obtainedwhenusingamicrophoneexternalfromtherobot.
Model transcribeddata,thelargestandbest-performingmodel(large-v3)
consumes32.3ùëä‚Ñéandproduces7.7ùëîùê∂ùëÇ 2ùëíùëû.
Figure3:Averagetranscriptiontimeforasentence.Whisper
models(ingreen)wererunonGPUunlessCPUisspecified.
4 CONCLUSION
Dashedlineshowsmaximumacceptabledelayof1000ms.
Basedonourevaluation,wecanmakethefollowingrecommenda-
tions,updatingoroverridingthosemadein[3]:
Recognitionperformance. Therecognitionperformancehas
improveddramaticallyforstate-of-the-artASR,withthebest
modelsof2024showingover60%fewertranscriptionerrors
than in 2017. Still, adult-like recognition is not available
yet,butthesemanticcontentofchildren‚Äôsspeechisnow
sufficientlytranscribedtoofferpotentialforrobustspoken
interaction,especiallyifothercomponentswithinthedia-
loguemanagement‚Äìsuchaslargelanguagemodels‚Äìcan
coverforsuboptimalorevenfailingASR.
Responsiveness. Theresponsivenessoflocallyhostedmodels
(inourcaseOpenAI‚ÄôsWhisper)issignificantlybetterthan
thatofcloud-basedsolutions,withsub-secondresultsfor
somemodels.Thenetworkoverheadandsharedservices
ofusingcloud-basedsolutionsarenotoptimalforreal-time
spokeninteraction,andlocalmodelsevenoutperformthe
cloud-basedsolutionsinaccuracy.
Figure4:Transcriptiontimevs.accuracy(lowerisbetter). Impactofmicrophone. Usinganexternalmicrophone,asop-
WhispermodelswererunonGPUunlessCPUisspecified. posedtoamicrophoneembeddedintherobot,leadstoasig-
IdealASRsystemswouldbeinthelowerleftcorner. nificantlyimprovedrecognitionperformance.Performance
improvesregardlessofthequalityofthemicrophone,asthe
intheNaorobotisused,asthereisalotofaddednoiseduetothe robot‚Äôsnoisehasastrongereffectonthespeechrecognition
closenesstotherobot‚Äôsmotorandventilation,butnodifferenceis thanthechoiceofmicrophone.
foundbetweenbothexternalmicrophones.
ACKNOWLEDGMENTS
3.4 Energyconsumption
Thisresearchreceivedfundingfromimec(SmartEducation),the
Asconcernshavebeenraisedovertheenergyconsumptionand FlemishGovernment(AIResearchProgram)andtheHorizonEu-
consequentlycarbonemissionsofstate-of-the-artmachinelearning ropeVALAWAIproject(grantagreementnumber101070930).We
[4],wethinkitisvaluabletoconsidertheseforASRsystems.We areindebtedtotheauthorsof[2]and[3]formakingtherecordings
onlyhavebeenabletomeasureWhisper‚Äôsconsumption.Perhourof andtranscriptionsavailable.
)sdnoces(
emit
noitpircsnarT
ynit
repsihW
esab
repsihW
llams
repsihW
3v
egral
repsihW
muidem
repsihW
egral
repsihW
2v
egral
repsihW
eruzA UPC
ynit
repsihW
UPC
esab
repsihW
elgooG UPC
llams
repsihW
ecnatsid
niethsneveLChildSpeechRecognitioninHuman-RobotInteraction:ProblemSolved? TAHRI‚Äô24,March09‚Äì10,2024,Boulder,CO
REFERENCES
[4] AlexandreLacoste,AlexandraLuccioni,VictorSchmidt,andThomasDandres.
[1] TonyBelpaeme,JamesKennedy,AditiRamachandran,BrianScassellati,andFu- 2019. Quantifyingthecarbonemissionsofmachinelearning. arXivpreprint
mihideTanaka.2018.Socialrobotsforeducation:Areview.Sciencerobotics3,21 arXiv:1910.09700(2019).
(2018),eaat5954. [5] SiddiqueLatif,AunZaidi,HeribertoCuayahuitl,FahadShamshad,Moazzam
[2] JamesKennedy,S√©verinLemaignan,CarolineMontassier,PaulineLavalade,Bahar Shoukat,andJunaidQadir.2023.Transformersinspeechprocessing:Asurvey.
Irfan,FotiosPapadopoulos,EmmanuelSenft,andTonyBelpaeme.2016.Children arXivpreprintarXiv:2303.11607(2023).
speechrecording(English,spontaneousspeech+pre-definedsentences). https: [6] AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,and
//doi.org/10.5281/zenodo.200495 IlyaSutskever.2023.Robustspeechrecognitionvialarge-scaleweaksupervision.
[3] JamesKennedy,S√©verinLemaignan,CarolineMontassier,PaulineLavalade,Bahar InInternationalConferenceonMachineLearning.PMLR,28492‚Äì28518.
Irfan,FotiosPapadopoulos,EmmanuelSenft,andTonyBelpaeme.2017. Child [7] GabrielSkantze.2021.Turn-takinginconversationalsystemsandhuman-robot
speechrecognitioninhuman-robotinteraction:evaluationsandrecommendations. interaction:areview.ComputerSpeech&Language67(2021),101178.
InProceedingsofthe2017ACM/IEEEinternationalconferenceonhuman-robot [8] WayneXiong,LingfengWu,FilAlleva,JashaDroppo,XuedongHuang,andAn-
interaction.82‚Äì90. dreasStolcke.2018.TheMicrosoft2017conversationalspeechrecognitionsystem.
In2018IEEEinternationalconferenceonacoustics,speechandsignalprocessing
(ICASSP).IEEE,5934‚Äì5938.