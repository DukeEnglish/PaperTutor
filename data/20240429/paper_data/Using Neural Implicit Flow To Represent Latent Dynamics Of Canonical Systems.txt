USING NEURAL IMPLICIT FLOW TO REPRESENT LATENT
DYNAMICS OF CANONICAL SYSTEMS
ImranNasim*1,3andJoaõLucasdeSousaAlmeida†2
1IBM,UK
2IBMResearchBrazil
3DepartmentofMathematics,UniversityofSurrey,Guildford,GU27XH,Surrey,UK
ABSTRACT
TherecentlyintroducedclassofarchitecturesknownasNeuralOperatorshasemergedashighly
versatiletoolsapplicabletoawiderangeoftasksinthefieldofScientificMachineLearning(SciML),
includingdatarepresentationandforecasting. Inthisstudy,weinvestigatethecapabilitiesofNeural
ImplicitFlow(NIF),arecentlydevelopedmesh-agnosticneuraloperator,forrepresentingthelatent
dynamicsofcanonicalsystemssuchastheKuramoto-Sivashinsky(KS),forcedKorteweg–deVries
(fKdV),andSine-Gordon(SG)equations,aswellasforextractingdynamicallyrelevantinformation
fromthem. FinallyweassesstheapplicabilityofNIFasadimensionalityreductionalgorithmand
conductacomparativeanalysiswithanotherwidelyrecognizedfamilyofneuraloperators,knownas
DeepOperatorNetworks(DeepONets).
Keywords deepneuralnetworks·dynamicalsystems·latentspace·reducedordermodelling
1 Introduction
Overthelastfewyearstheclassoftheso-calledNeuralOperators[8]haveemergedasapromisingtoolformany
fundamentaltasksinscientificmachinelearning(SciML),asdatarepresentation[15],time-seriesforecasting[19]and
discoveringofoperatorsfromdata[11]bothindata-drivenandPhysics-informeddomains[20,13]. NeuralOperators
firstappearedwiththeintroductionofDeepNeuralOperators(DeepONets)[11],anewclassofarchitecturesdesigned
toextendthecapabilitiesofneuralnetworksinordertobetterperformtasksrelatedtooperatorlearning. ADeepONet
iscomposedbytwosubnetworks,termedtrunkandbranch,andessentiallyemulatesalinearexpansion,inwhichthe
trunklearnsasetofbasisfunctionsforapredeterminedsystemofcoordinates,whilethebranchdiscoverspenalties
forthesefunctionsastheyrelatetotheforcingvariables. Alternatively, itispossibletoseethebranchnetworkas
a hypernetwork aimed at evaluating the last layer for the trunk [15]. Since DeepONets were first proposed, many
derivedandalternativeapproacheshavebeendevelopedtoaddresstheoperatorlearningproblem. Theseapproaches
includenovelwaystocombinedifferentneuralnetworkarchitectureswithintheDeepONetframework[14],suchas
theFourierNeuralOperators(FNO)[10], andmorerecentlytheNeuralImplicitFlow(NIF)[15], ahypernetwork
whichcanbedescribedasanextensionoftheoriginalDeepONetconcept(seeFigure1). Inorderforadeeplearning
model to faithfully represent dynamics of an arbitrary dynamical system, it must be generalizable to systems that
exhibitbothquantitativelyandqualitativelydifferentdynamicswhichnaturallypresentthemselvesinphysicaland
engineeringsystems. Thereexistsadistinctionindynamicalsystemmodelsbetweenintegrableandnon-integrable
systems,bothofwhichexhibitdistinctlydifferentdynamicalbehaviours[16].Ratherlooselydefined,integrablesystems
havemanyconservedquantitiesconfiningmotiontoalower-dimensionalsub-manifoldinthephasespace. Conversely,
non-integrablesystemslackthischaracteristicgivingrisetocomplexandchaoticmotion[4]. Inthiswork,wetest
thecapabilitiesofNIFonbothintegrableandnon-integrablesystemsnamelytheforcedKorteweg–deVries(fKdV),
Sine-Gordon(SG)andKuramoto-Sivashinsky(KS)equations. Weinvestigatewhetherthelatentrepresentationcontains
dynamicallymeaningfulfeaturesandcomparetheresultswithDeepONets,awellknownfamilyofneuraloperators.
∗imran.nasim@ibm.com,i.nasim@surrey.ac.uk
†joao.lucas.sousa.almeida@ibm.com
InternationalConferenceonScientificComputingandMachineLearning2024(SCML2024)
4202
rpA
62
]GL.sc[
1v53571.4042:viXraUsingNeuralImplicitFlowtorepresentlatentdynamicsofcanonicalsystems
(a)NIF (b)DeepONet
Figure1: NIFandDeepONetarchitectures.
2 ModelsandExperiments
DeepONet. DeepONet[11]isaneuraloperatorcomposedoftwosubnetworks,calledtrunkandbranch,designed
touniversallyapproximatenonlinearoperators. Thebranchnetworkapproximatesamanifoldrepresentingthemost
fundamentalsystemdynamicsandthetrunkprojectstheoriginalspatiotemporalcoordinatesontothislatentmanifold.In
thiswork,wehaveemployedDeepONetsforaveryspecifictask,thatistoseparatespatialandtemporalcomponentsin
ordertoemulateaproperorthogonaldecomposition(POD).SuchanapproachmakesitdifferentfromPOD-DeepONet
[12],whichusesPODasinputforthetrunknetwork,butitcomesclosetotheso-calledSVD-DeepONet[18]),in
whichtheneuralnetworkisemployedtomimicaspectralexpansion. Theproposalistousethebranchnetworkofthe
DeepONettoextractlatentseriesfromthesystem,whichcanbeusedforrepresentingfundamentaldynamics.
NIF.NIFiscomposedbytwosubnetworks,termedParameterNetandShapeNet,whichcloselyresemblesthebranch
andtrunkcomponentsofDeepONets. However,differentlyfromthebranchnetwork,ParameterNetevaluatesnotonly
thelastlayerbuttheentiresetofweightsandbiasesforShapeNetenablingthemodeltomimicadecoderforeach
parametricinput. InordertoavoidissueswiththelargenumberofParameterNetoutputsrequiredtoestimateallthe
coefficientsforShapeNet,alinearoperation(seeFigure1)isusedtoprojectthelatentspaceontotheexpectedset
ofweightsandbiases[15]. NIFisintendedtoworkasadimensionalityreductionalgorithm,sincetheParameterNet
createsalow-dimensionalrepresentationofthefull-spacedataset,whichcanbeusedforfurthermodelling,assparse
reconstruction[15],forecastingandevendatageneration. NIFcanalsoserveasarepresentationmodel[17]ableof
compressinginformationaboutlargedatasetsintoarelativelysmallsetofparameters. AlloftheNIFmodelsconsidered
inthisstudywererunfor5000epochswithalearningrateof5e-3,having2layerswith30unitsinboththeShapeNet
and ParameterNet. These hyperparameter values are within the ranges tested [15]. The DeepONet used to create
theexampleinFigure3iscomposedbytwonetworkshavingthreelayerswith20unitsandasineactivation. The
DeepONetwastrainedfor5000epochswithalearningrateof1e-3.
Test cases and datasets. The forced Korteweg–de Vries (fKdV) equation is an integrable non-linear PDE that is
oftenusedtomodeltheweaklynon-linearflowproblem[2,1], whichdescribestheexistenceofmultiplepossible
solutionswhichisdependentonthetypeofdisturbance. Inourstudy,wewillconsiderthefKdVundertheassumption
of no disturbance. Under this assumption, the equation can be written as 6u +u +(9u−6(F −1))u = 0,
t xxx x
where F is the depth-based Froude number. This model has been shown to exhibit both periodic travelling wave
andsolitondynamics[1,13]. ThesecondmodelweconsideristheKuramoto–Sivashinksyequation(KShereafter).
TheKSequationisafourth-ordernon-integrablenonlinearPDEthatisoftenusedtomodeltheevolutionofsurface
wavesandpatternformationforanumberofphysicalsystems[9]. Theviscousformoftheequationcanbewrittenas
u +uu +u +νu =0,whereν isacoefficientofviscosity. TheKSequationishighlycomplex,capturing
t x xx xxxx
thedynamicsofspatio-temporalinstabilitiesseeninanumberoffluidflows. Italsodisplayschaoticmotionanddue
toitsnon-integrabilitygivesrisetoaricharrayofsolutiontypesdependingonthevalueoftheviscosityparameter
ν. Pertinentto this study, theKS equation exhibits bursting wavedynamics for ν = 16 [7]. We also consider the
71
SineGordon(SG)equationwhichgainedpopularityfortheabilitytoexhibitsolitonsolutions[5]andcanbewritten
asu −u +sinx = 0.WesimulatethefKdVandKSequationsonagridof64pointsonaperiodicdomainof
tt xx
−π ≤x≤πandsettheFroudenumberF =1.5. Weconsiderinitialconditionoftheformu(x,0)=Acos(kx+ϕ)
wherewefixA=0.5,k =1andϕ=1. Wecollectthedynamicaldataforallmodelsfromtimet=300,definedasT,
whichiswellaftertheinitialtransientshavediedoutinordertocapturethedynamicsontheattractor. Allintegrations
fortheKSandfKdVwereperformedusinganexplicitRKfinite-differenceschemewithatoleranceof10−6whichwas
comparedtoapsuedo-spectralmethodtoensureaccuracy. TosolvetheSGequation,weusedanadaptive5thorder
exponentialtimedifferencingmethod[21].
2UsingNeuralImplicitFlowtorepresentlatentdynamicsofcanonicalsystems
3 Results
WefirsttestwhetherNIFisabletofaithfullypredictthetravelingwavedatafromthefKdVequation,theresultsfor
NIFandthecomparativeDeepONetmodelispresentedinFigure2. WeobservethatthepredictionsfromNIFareable
toaccuratelypredictthetravellingwavedynamicsformajorityofthetimedomainyieldingasmallerpointwiseerror
comparedtoDeepONets. Thereappearstobedeviationfromthetruedataatlatertimes,whichismostevidentfor
T >90,thoughtheoverallpredictionsaremoreaccuratethantheDeepONetmodel. Usingthesamenumericalsetup,
weproceedtotestNIFonthemorecomplexburstingdynamicsoftheKSequation. Weobtaintheburstingdynamics
dataafterthetransientshavediedoutinthesamewayasinthefKdVcasewhichcapturesfasttransitionsbetweentwo
saddlepointsconnectedbyfourheteroclinicconnections[6,13]. FromFigure2weseethatNIFperformsextremely
wellonpredictingtheburstingdynamicsoftheKSequationwithessentiallynegligibleerrorthroughouttheentiretime
domain3. Weobservethattheminordeviationspresentinthepredictionoccuratthetimesofrapidtransitioninthetrue
datawhichisduetotheaforementionedtransitionbetweensaddlepointsandappeartogetprogressivelystrongerwith
time. Interestingly,wealsoobservethischaracteristicintheprofilesobtainedfromtheDeepONetmodel. Animportant
featureofanydeeplearningbasedsurrogateishavingameaningfulcompactlatentrepresentationwiththeaimto
capturetheunderlyingdynamicsthatgovernthesystem’sbehavior. Toprobethenatureofthelatentrepresentation
inNIFweplotinFigure2theevolutionofthelatentvariableasafunctionoftimealongwiththereconstructionand
pointwiseerrorforallthemodels. ThemiddlepanelshowstheevolutionofthelatentvariableforthecaseoftheKS
burstingdynamics. Weclearlyobservethattransitionsoccurinthelatentprofileatthetimeswherethephasetransitions
occur in the numerical solution suggesting that the latent variable is capturing these transitions that are occurring.
Interestingly,whenweprobethelatentvariableprofileonthelongerburstingdynamicsdata,rightpanelofFigure2,
wenoticethatthestrengthofthesetransitionsappeartodecreasewithtime. Inthecaseofthelatentprofileforthe
fKdVtravellingwavedata,itappearsthatthelatentvariabledecreasessteadilywithtimeonlyhavingdeviationsfrom
thisbehaviouratlatertimeswherethepredictionerrorfromNIFislarger. Thisprofileissignificantlydifferenttothat
obtainedbyDeepONetswhatappearstocaptureaquasi-periodicbehaviourwhichismoreinterpretableandexplainable
thantheNIFrepresentation.
fKdV KS Longer KS
x x x x x x x x x
x x x x x x x x x
Figure 2: Latent variable profile with the pointwise error and predicted dynamics using NIF (upper panels) and
DeepONet(lowerpanels).
Tofurtherprobethenatureofthecapturedlatentrepresentation,weproceedtorunNIFwith3latentvariablesand
comparethisrepresentationwiththatobtainedbyDeepONets.Toprovideareference‘groundtruth’latentrepresentation
3FromtheexperimentsweconductedwefoundthattheSwishactivationfunctionprovidedthemostaccuratepredictionofthe
burstingandtravellingwavedynamics.
3
emiT
emiT
emiT
emiT
emiT
emiTUsingNeuralImplicitFlowtorepresentlatentdynamicsofcanonicalsystems
forcomparisonweusetherepresentationsobtainedbyaFourierprojectionofthefullorderdataonthetwodominant
modeswhichhighlightsi)theskeletalattractorfortheKSdynamicsii)quasi-periodictorusforthefKdVdata[13](see
leftpanelofFigure3). InterestinglyinFigure3weobservethatthelatentrepresentationcapturedbyDeepONetsisin
fullagreementwiththeFourierprojection,bothofwhichcapturethetransitionbetweentwosaddlepointsviafour
heteroclinicconnections[6,13]. WedonotfindtherepresentationobtainedbyNIFtobedynamicallyinterpretable.
SimilarlyinFigure3,wefindagoodlevelofexplainabilityfromtheDeepONetlatentrepresentation,capturingthe
quasi-periodicityandtransitionregionsunliketherepresentationyieldedfromNIFwhichdoesn’tappeartocapturethis
phasetransitionbehaviour. TheresultsfortheSGhighfrequencyplanewavedataispresentedinAppendixA.Againwe
findthatDeepONetsyieldsamoreinterpretablelatentrepresentationbuthasasubstantiallyhigherreconstructionerror
comparedtoNIF,aneffectobservedforallmodels. Apointworthnotingisthatthereisasubstantialeffectofthelatent
spacedimensionforDeepONets,forexamplethereconstructionerrordecreasesfrom32%to6%whenthisdimension
isincreasedfrom3to6. ThismaysignifytheabilityforDeepONetstofindtheintrinsicmanifolddimensionality,
aneffectsthathasbeenobservedwithautoencoders[3]. WhilewehaveevidencethatthelatentvariablesinNIFare
sensitivetotransitionsintherawdata,overallwefindtherepresentationyieldedbyNIFincomparisontoDeepONetsto
besignificantlylessinterpretable. Wefindtheseresultsverysurprisinggiventherelativereconstructionerrors: forthe
KSburstingdynamicsdata,12.3%forDeepONetand1.68%forNIF.ForthefKdVdata,NIFachievesanerrorof2.0%
whereasDeepONetsyields5.0%. AdditionallyfortheSGdata,NIFachievesanerrorof2.68%whereasDeepONets
yields7.48%. ApointworthnotinghereisthatunlikeDeepONets,NIFdoesn’tfullyseparatevariablesbetweenitstwo
KS
fKdV
Figure3: Thethree-dimensionallatentdynamicsforKS(top)andfKdV(bottom)burstingdynamicsdataproducedby
i)Fourierprojection(leftpanel)ii)DeepONet(middlepanel)andiii)NIF(rightpanel).
sub-networks,withShapeNetcoefficientsstillrelyingonParameterNetinputs. Althoughithinderstheanalogywitha
lineardecomposition,asFourier(seeFigure3),itcanallowthemodeltodiscoverthemostpropermodestorepresent
eachsnapshot,whichendowsitwithagreatercapacitytorepresentcomplexdatasets.
4 Impactstatement
InthisstudywehaveexploredthecapabilitiesofNIFforrepresentingqualitativelydistinctdynamicsforbothintegrable
andnon-integrablePDEsystems. WefindthereconstructeddynamicspredictedbyNIFtobeaccurateandfaithfulto
thedynamicspresentinthetruedata. ThissuggeststhatNIFcanbeusedtorepresentdatasetsrelatedtodynamical
systemsinalow-dimensionalway,thusenablingustoextractimportantinformationfromthislatentdynamics. While
ourresultssupporttheideathatthelatentrepresentationinNIFissensitivetothedynamicalcharacteristicspresent
inthetruedata,itsinterpretabilityisnotasintuitivelyclearwhencomparedtorepresentationsyieldedbyastateof
the art DeepONet architecture. We find this result surprising given the fact that NIF yields a significantly smaller
reconstructionerroronalldatasets. AfurtherinvestigationintointerpretabilitybyanalyzingtheNIFarchitectureand
regularizationmethodswillbeinvestigatedinafuturestudy.
4UsingNeuralImplicitFlowtorepresentlatentdynamicsofcanonicalsystems
References
[1] BenjaminJ.Binder. Steadytwo-dimensionalfree-surfaceflowpastdisturbancesinanopenchannel: Solutionsof
thekorteweg–devriesequationandanalysisoftheweaklynonlinearphasespace. Fluids,4(1),2019.
[2] BJBinderandJ-MVanden-Broeck. Freesurfaceflowspastsurfboardsandsluicegates. EuropeanJournalof
AppliedMathematics,16(5):601–619,2005.
[3] DanielFloryanandMichaelDGraham. Data-drivendiscoveryofintrinsicdynamics. NatureMachineIntelligence,
4(12):1113–1120,2022.
[4] AnatolijTFomenko. Integrabilityandnonintegrabilityingeometryandmechanics,volume31. SpringerScience
&BusinessMedia,2012.
[5] Ryogo Hirota. Exact solution of the sine-gordon equation for multiple collisions of solitons. Journal of the
PhysicalSocietyofJapan,33(5):1459–1463,1972.
[6] IoannisGKevrekidis,BasilNicolaenko,andJamesCScovel. Backinthesaddleagain: acomputerassistedstudy
ofthekuramoto–sivashinskyequation. SIAMJournalonAppliedMathematics,50(3):760–790,1990.
[7] Michael Kirby and Dieter Armbruster. Reconstructing phase space from pde simulations. Zeitschrift für
angewandteMathematikundPhysikZAMP,43:999–1022,1992.
[8] NikolaKovachki,ZongyiLi,BurigedeLiu,KamyarAzizzadenesheli,KaushikBhattacharya,AndrewStuart,and
AnimaAnandkumar. Neuraloperator: Learningmapsbetweenfunctionspaces,2023.
[9] YoshikiKuramoto. Diffusion-InducedChaosinReactionSystems. ProgressofTheoreticalPhysicsSupplement,
64:346–367,021978.
[10] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, An-
drewStuart,andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferentialequations. In
InternationalConferenceonLearningRepresentations,2021.
[11] LuLu,PengzhanJin,GuofeiPang,ZhongqiangZhang,andGeorgeEmKarniadakis. Learningnonlinearoperators
viadeeponetbasedontheuniversalapproximationtheoremofoperators. NatureMachineIntelligence,3:218–229,
2021. inpress.
[12] Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em
Karniadakis. Acomprehensiveandfaircomparisonoftwoneuraloperators(withpracticalextensions)basedon
fairdata. ComputerMethodsinAppliedMechanicsandEngineering,2021.
[13] ImranNasimandMichaelEHenderson. Dynamicallymeaningfullatentrepresentationsofdynamicalsystems.
Mathematics,12(3):476,2024.
[14] VivekOommen,KhemrajShukla,SomdattaGoswami,RémiDingreville,andGeorgeEmKarniadakis. Learning
two-phasemicrostructureevolutionusingneuraloperatorsandautoencoderarchitectures. npjComputational
Materials,8(1):190,Sep2022.
[15] Shaowu Pan, Steven L. Brunton, and J. Nathan Kutz. Neural implicit flow: a mesh-agnostic dimensionality
reductionparadigmofspatio-temporaldata. JournalofMachineLearningResearch,24(41):1–60,2023.
[16] Alfred Ramani, Basil Grammaticos, and Tassos Bountis. The painlevé property and singularity analysis of
integrableandnon-integrablesystems. PhysicsReports,180(3):159–245,1989.
[17] VincentSitzmann,JulienMartel,AlexanderBergman,DavidLindell,andGordonWetzstein. Implicitneural
representationswithperiodicactivationfunctions. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,and
H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7462–7473. Curran
Associates,Inc.,2020.
[18] SimoneVenturiandTiernanCasey. Svdperspectivesforaugmentingdeeponetflexibilityandinterpretability.
ComputerMethodsinAppliedMechanicsandEngineering,403:115718,2023.
[19] SifanWangandParisPerdikaris. Long-timeintegrationofparametricevolutionequationswithphysics-informed
deeponets. JournalofComputationalPhysics,475:111855,2023.
[20] SifanWang,HanwenWang,andParisPerdikaris.Improvedarchitecturesandtrainingalgorithmsfordeepoperator
networks. JournalofScientificComputing,92(2):35,Jun2022.
[21] PatrickWhalen,MoyseyBrio,andJeromeVMoloney. Exponentialtime-differencingwithembeddedrunge–kutta
adaptivestepcontrol. JournalofComputationalPhysics,280:579–601,2015.
5UsingNeuralImplicitFlowtorepresentlatentdynamicsofcanonicalsystems
A ResultsforSine-GordonEquations
TheneuralnetworksusedforSine-GordonhavethesameconfigurationsdescribedinSection2. ForDeepONet,the
reconstructionerrorusingthreelatentvariablesis7.48%andforNIFis2.68%.
SG SG
Pointwise Latent Pointwise Latent
u error u space u error u space
100 100
95 95
90 90
85 85
80 80
75 75
70 70
65 65
60 60
55 55
50 50
45 45
40 40
35 35
30 30
25 25
20 20
15 15
10 10
5 5
0 0
2.50.0 2.5 2.50.0 2.5 2.50.0 2.5 0 1 2.50.02.5 2.50.02.5 2.50.02.5 0 1
x x x x x x
Figure4: Latentvariableprofilewiththepointwiseerrorandpredicteddynamicsusingi)DeepONet(leftpanel)andii)
NIF(rightpanel).
Figure 5: The three-dimensional latent dynamics for Sine-Gordon bursting dynamics data produced by i) Fourier
projection(leftpanel)andii)DeepONet(middlepanel)andiii)NIF(rightpanel).
6
emiT emiT