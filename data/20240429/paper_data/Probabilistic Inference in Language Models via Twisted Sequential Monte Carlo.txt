Probabilistic Inference in Language Models
via Twisted Sequential Monte Carlo
StephenZhao12* RobBrekelmans2* AlirezaMakhzani12** RogerGrosse12**
Abstract reward function which reflects human feedback (Korbak
et al., 2022b). Red-teaming techniques such as prompt-
Numerous capability and safety techniques of
engineeringandinfillingmayseektargetoutputswithlow
Large Language Models (LLMs), including
rewardor(highprobabilityof)undesirableresponses(Zou
RLHF,automatedred-teaming,promptengineer-
etal.,2023;Perezetal.,2022). Inreasoningtasks,wemay
ing, andinfilling, canbecastassamplingfrom
seektotargetoutputswhicharelikelytobedeemedvalid anunnormalizedtargetdistributiondefinedbya
bya‘verifier’(Cobbeetal.,2021;Aniletal.,2021;Dohan
givenrewardorpotentialfunctionoverthefullse-
et al., 2022; Hu et al., 2023). Specific properties of the
quence. Inthiswork,weleveragetherichtoolkit
generatedresponsesmightalsobeenforced(Khalifaetal., ofSequentialMonteCarlo(SMC)fortheseprob-
2020;Yang&Klein,2021;Lewetal.,2023).
abilisticinferenceproblems. Inparticular,weuse
learnedtwistfunctionstoestimatetheexpectedfu- We view the above tasks as instances of probabilistic in-
turevalueofthepotentialateachtimestep,which ference: samplingfromatargetunnormalizeddensityand
enablesustofocusinference-timecomputation estimatingitsintractable(log)normalizationconstant. Con-
on promising partial sequences. We propose a siderapretrainedbasemodelp (s |s )whichgenerates
0 1:T 0
novel contrastive method for learning the twist responsess ofmaximumlengthT basedonavariable-
1:T
functions,andestablishconnectionswiththerich length prompt s . We consider defining the target distri-
0
literature of soft reinforcement learning. As a bution of interest using the base model modulated by a
complementaryapplicationofourtwistedSMC potentialfunctionϕ(s )whichevaluatesfullsequences,
1:T
framework,wepresentmethodsforevaluatingthe 1
σ(s |s ):= p (s |s )ϕ(s ), (1)
accuracyoflanguagemodelinferencetechniques 1:T 0 Z (s ) 0 1:T 0 1:T
σ 0
usingnovelbidirectionalSMCboundsonthelog (cid:88) (cid:88)
where Z (s ):= σ˜(s |s )= p (s |s )ϕ(s ),
partitionfunction. Theseboundscanbeusedto σ 0 1:T 0 0 1:T 0 1:T
s1:T s1:T
estimate the KL divergence between the infer-
ence and target distributions in both directions. where σ˜(s 1:T|s 0) denotes the unnormalized density. We
Weapplyourinferenceevaluationtechniquesto
refertoZ σ(s 0)asthenormalizationconstantorpartition
showthattwistedSMCiseffectiveforsampling function, which is intractable due to the summation over
undesirable outputs from a pretrained model (a s 1:T. Wedropdependenceons 0 toavoidclutter,butnote
useful component of harmlessness training and thateachpromptinducesadifferentpartitionfunction. In
automatedred-teaming),generatingreviewswith
thecontextoftheaforementionedapplications,ϕ(s 1:T)may
variedsentiment,andperforminginfillingtasks. bederivedfromahumanpreferencemodel(forRLHF),an
indicationofbadbehavior(forautomatedred-teaming),ora
verifier’spredictionofcorrectness(forreasoningtasks). We
1.Introduction
refertoTable5orKorbaketal.(2022b);Dohanetal.(2022);
Phanetal.(2023);Huetal.(2023)forfurtherexamplesand
A wide range of language model learning and inference
discussionofprobabilisticinferenceinlanguagemodels.
tasks can be viewed as steering a model’s generations to
satisfyaspecifiedproperty. Inparticular,traditionalrein- Twisted Sequential Monte Carlo in Language Models
forcementlearningfromhumanfeedback(RLHF)pipelines
In this work, we leverage tools from (twisted) Sequen-
(Ziegleretal.,2019;Stiennonetal.,2020;Ouyangetal.,
tialMonte Carlo(SMC)(Doucet etal.,2001;Del Moral
2022;Baietal.,2022;Rafailovetal.,2023)maybeviewed
et al., 2006; Briers et al., 2010; Chopin et al., 2020)
astargetinganunnormalizedtargetmodulatedbyaterminal
to perform and evaluate inference in the language mod-
eling setting (Sec. 3). A particular challenge in sam-
* Jointfirstauthorship, ** Jointseniorauthorship. 1University
ofToronto2VectorInstitute. Contact: {stephenzhao,makhzani, pling from Eq. (1) is that the target distribution σ(s 1:T)
rgrosse}@cs.toronto.edu,rob.brekelmans@vectorinstitute.ai. is non-causal. In order to sample tokens sequentially,
1
4202
rpA
62
]GL.sc[
1v64571.4042:viXraProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
one needs to infer the marginal distribution σ(s ) = whichcanbeusedtodiagnosemode-droppingbehav-
1:t
(cid:80) (cid:80)
σ(s )∝ p (s |s )ϕ(s ),which iorofmethodssuchasproximalpolicyoptimization
st+1:T 1:T st+1:T 0 t+1:T 1:t 1:T
involves an intractable marginalization. To address this (PPO)(Schulmanetal.,2017)whichoptimizeamode-
problem,weproposetolearntwistfunctionsψ (s )which seekingdivergence.
t 1:t
modulatethebasemodelsuchthatp (s )ψ (s )matches
0 1:t t 1:t
thetargetmarginalsσ(s ),uptonormalization. Thetwist Weproceedtodescribebackgroundonimportancesampling
1:t
functionscanbeusedtofocuseachstepoflanguagemodel and SMC inSec.2,beforepresentingourframeworkfor
generationonpromisingpartialsequences. twistedSMCinthelanguagemodelingsettinginSec.3.We
proposemethodstolearnthetwistfunctionsinSec.4and
EvaluatingInferenceinLanguageModeling Sampling methodstoevaluateinferenceinSec.5. Ourexperimental
from the target distribution is closely intertwined with resultsinSec.7showcasetheabilityoftwistedSMCtoim-
boundingthelogpartitionfunction. Similarlytovariational provecontrolledgenerationandlendinsightsintoinference
inference or traditional RLHF objectives (Korbak et al., qualityinexistingmethods.
2022b), SMC algorithms yield lower bounds on logZ ,
σ
where tighter bounds typically coincide with more accu- 2.Background
ratetargetsampling. However,upperboundsmayoftenbe
Suppose we are given access to an unnormalized density
obtainedwhenanexacttargetsampleisavailable(Grosse
σ˜(s ) which can be efficiently evaluated. We focus on
etal.,2015;2016;Brekelmansetal.,2022). Thedifference 1:T
estimation of the partition function or normalization con-
betweenupperandlowerboundsonlogZ σ infactyieldsan stantZ :=(cid:80) σ˜(s ),sinceunbiasedestimatorswith
upperboundonthesymmetrizedKLdivergencebetween σ s1:T 1:T
lowvarianceyieldapproximatesamplingtechniqueswhich
inferencesamplesandthetargetdistribution(Grosseetal.,
closely approximate the target distribution (Finke, 2015;
2016). Forthesereasons,weargueinSec.5thatlogpar-
Maddisonetal.,2017). Wereviewsimpleimportancesam-
titionfunctionestimatesareapowerfultoolforevaluating
pling(SIS)andSMCtechniquesinthissection.
languagemodelinferencetechniques.
2.1.SimpleImportanceSampling
Contributions Our probabilistic inference perspective
leadstothefollowingcontributions: Simple importance sampling (SIS) provides an unbiased
estimatorofZ bycalculatingimportanceweightsforany
σ
normalizedproposaldistributionq(s ),
• TwistedSequentialMonteCarloforLanguageModel- 1:T
ing: WeviewtwistedSMCasageneralframeworkfor σ˜(cid:0) si (cid:1)
w(si ):= 1:T , (2)
samplingandevaluationoflanguagemodels. While 1:T q(si )
1:T
twisted SMC is well-known and Lew et al. (2023) whichisunbiasedsinceZ = E [w(s )]. Theim-
considerSMCwithfixed,few-step-aheadtargetinfor- σ q(s1:T) 1:T
portanceweightsalsoyieldananunbiasedK-sampleesti-
mationinthelanguagemodelingsetting,wepropose matorofthepartitionfunction,
tolearnintermediatetwistfunctionsfortargetdistribu-
K
tionsdefinedbyterminalpotentialonly. ZˆSIS := 1 (cid:88) w(si ), si ∼q(s ). (3)
σ K 1:T 1:T 1:T
• Contrastive Twist Learning: We develop probabilis- i=1
ticmethodsforlearningintermediatetwistfunctions,
BynormalizingtheweightsinEq.(2)overK samplesfrom
presenting a novel contrastive twist learning (CTL) q(s ),wecanobtain(biased)estimatorsofexpectations
1:T
methodinspiredbyenergy-basedmodelingandden- underσ(s ),
1:T
sity ratio estimation in Sec. 4.1. Further, we adapt
existingtwistedSMCmethods(Lawsonetal.,2018; E (cid:2) f(s )(cid:3) ≈(cid:88)K w(sk 1:T) f(sk ) (4)
2022;Lioutasetal.,2022)tothelanguagemodeling σ(s1:T) 1:T k=1(cid:80)K j=1w(sj 1:T) 1:T
setting,andhighlightconnectionswithinferencetech- orselectanapproximatetargetsamplesσ fromacategori-
1:T
niquesinspiredby(soft)reinforcementlearning(RL). caldistributionwiththeself-normalizedimportanceweights
• Evaluating Inference in Language Models: Finally, (cid:40) w(cid:0) si (cid:1) (cid:41)K 
wedemonstratethattwistedSMCprovidesarichset sσ 1:T ←sω 1:T, ω∼cat (cid:80)K w1: (cid:0)T sj (cid:1) . (5)
of tools for evaluating language model fine-tuning j=1 1:T i=1
or controlled generation techniques. We propose a The quality of theapproximations in Eq. (3)-(5) depends
novelSMCupperboundonlogZ
σ
whichisapplica- cruciallyonhowwelltheproposalq(s 1:T)(whichmaybe
blewhenanexacttargetsampleisavailableandmay learned, Sec. 3.2) matches the target σ(s ). While we
1:T
beofindependentinterest. Weleveragethesebounds discussevaluationmethodsinSec.5,notethatifinference
toevaluatethequalityofinferencebymeasuringthe isexact(i.e.,q(s ) = σ(s )),thenthevarianceofthe
1:T 1:T
KLdivergencetothetargetσ(s 1:T)inbothdirections, importanceweightsiszero,asw(s 1:T)=Z
σ
foralls 1:T.
2ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Algorithm1(Twisted)SMCSampling(q )
SMC
SMC-PROPOSAL(cid:0)
p ,q,{ψ
}T−1,ϕ,K(cid:1)
:
0 t t=1
fort=1,...,T do
fork =1,...,K do
Samplesk
t
∼q(cid:0) s t(cid:12) (cid:12)sk 1:t−1(cid:1)
sk ←concat(cid:0) sk ,sk(cid:1)
1:t 1:t−1 t
ift<T then
I liked the title but I don't recommend it I liked was amazing! but I don't recommend it
wk ←
p0(sk t|sk 1:t−1) ψt(sk 1:t)
I loved this book Best book ever! I loved this book and I highly ever! t q(sk t|sk 1:t−1) ψt−1(sk 1:t−1)
TThhiiss bbooookk was bad! A disappointing experience This book only the first and I will read it again! else
Th (is abo )ok Simplewas
I
o mk portann cot
e
a Sampw la ist ne o gf time I hate (bw )as Tba wd! istedB Sest Mbook
C
hands down! w tk ← p q0 (( ss k tk t || ss k 1k 1 :t:t −− 11 )) ψt−ϕ 1( (s sk 1 k 1: :t t) −1)
endif
endfor
Figure2: IllustrativeexampleofSISand(Twisted)SMCforsampling
ift<T then
book reviews conditioned on positive sentiment ϕ(s ). SIS only
1:T
fork =1,...,K do
performsresamplingafterobservingtheentiresequence,whileSMC
can kill or clone partial sequences s
1:t
based on incremental impor- ωk ∼cat(cid:16)(cid:110) w ti (cid:111)K (cid:17)
t ha in gc he /lw oweig imht ps oi rn td au nc ce ed wb ey igt hw ti sst atfu en ac ct hio in ns crψ
et
m(s
e1 n: tt
a) l. sG tere pe on f/r Se Mdi Cnd ,i oc ra ate
t
skt
←sω tk
(cid:80)K j=1w tj i=1
1:t 1:t
thefinalstepofSIS.ForSMCwiththebasemodelproposalp andthe endfor
0
optimaltwists,theincrementalweightsψ∗/ψ∗ (Alg.1orEq.(6)) endif
t t−1
aredirectlycorrelatedwithsentiment. endfor
return(cid:8) sk ,wk(cid:9)K
1:T T k=1
ZˆSMC =(cid:81)T 1 (cid:80)K wk
σ t=1 K k=1 t
2.2.SequentialMonteCarlo dex random variables from the sampling procedure S ∼
q (S)inAlg.1. Assumingresamplingateverystep,1
SMC improves inference by decomposing it into easier SMC
subproblemsinvolvingasetofunnormalizedintermediate
targetdistributions{π˜ t(s 1:t)}T t=1. Akeyobservationisthat Z =E(cid:104) ZˆSMC(cid:105) =E (cid:34) (cid:89)T 1 (cid:88)K w (cid:0) sk (cid:1)(cid:35) . (8)
as long as π T(s 1:T) = σ(s 1:T), we obtain an unbiased σ σ q SMC(S) K t 1:t
estimateofthepartitionfunctionZ = Z ,regardlessof t=1 k=1
T σ
theintermediateπ andproposalq.
t To see that ZˆSMC is unbiased, we can view Eq. (8)
σ
Webeginbydefiningtheincrementalimportanceweights as performing simple importance sampling Z =
σ
(cid:104) (cid:105)
w (s ):=
π˜ t(s 1:t)
. (6)
E q SMC(S) σ˜ q SS MM CC (( SS )) intheextendedstatespace,forappropri-
t 1:t π˜ (s )q(s |s ) atedefinitionsofσ (S)andq (S)detailedinApp.F
t−1 1:t−1 t 1:t−1 SMC SMC
or (Andrieu et al., 2010; Maddison et al., 2017). Intu-
whereπ˜ istheunnormalizeddensityofπ =π˜ /Z .
t t t t itively,wemayviewtheaverageincrementalimportance
SMCmaintainsasetofK partialsequences,byfirstsam- weightsateachstepasestimatingthepartitionfunctionratio
plingfromtheproposalq(sk t|sk 1:t−1)ineachindexk. Op- Z t/Z t−1 ≈ K1 (cid:80)K k=1w t(sk 1:t). Eq.(8)composesinterme-
tional resampling steps may be performed to clone se- diatepartitionfunctionratioestimatorstoobtainanestimate
quenceswithhighincrementalimportanceweightsusing
ofthefinalZ
T
=Z
σ
=(cid:81)T
t=1Z t/Z t−1,withZ
0
=1.
sk 1:t ←sω 1:tk t, ω tk ∼cat(cid:32)(cid:26) (cid:80)K jw =1t( wsi 1 t: (t s) j 1:t)(cid:27)K i=1(cid:33) , (7) W σ st( ei s pth 1: STn M)o = Cre wπs Ta em i( gsp h1l : tTi sn ,)g wa, enS d mM p ar yC op eo sre ts id a mu l ac q te ( ess e1:t xTo p) e.S cUI taS s ti in ow g ni st th h oe rta dfir rng aae wlt -
approximatesamplessσ asinEq.(4)-(5).
similarly to Eq. (5). Since resampling is performed with 1:T
replacement,sequenceswithhighweightsmaybecloned 1Thedecisiontoresamplemaybebasedonanadaptivecon-
multipletimes.
Theresultingsω tk
areusedasprefixesfor
ditionsuchasEffectiveSampleSize(ESS)(Chopinetal.,2020).
1:t For R ≤ T, let {t }R index times where resampling oc-
thenextstepofproposalsamplinginindexk(seeAlg.1). r r=1
curs and fix t = 0 and t = T. The estimator becomes
WecanshowthatSMCyieldsanunbiasedestimatorZˆSMC ZˆSMC = (cid:81)R 0 1 (cid:80)K (cid:16) (cid:81)R tr w (cid:0) si (cid:1)(cid:17) , and the final-
σ σ r=1 K i=1 t=tr−1+1 t 1:t
of the normalization constant Z σ, by considering the ex- stepweightsforexpectationsinEq.(4)orsamplinginEq.(5)are
tended state space S := {sk,ωk}K,T of token and in- givenby(cid:81)T w (cid:0) si (cid:1) .
t t k,t=1 t=tR−1+1 t 1:t
3ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Fig.2illustratesthekeyadvantageofSMCresamplingover whicharenoclosertothetargetmarginalσ(s )thanthe
1:t
SIS.Whileasuboptimalq(s )mayproducesequences basemodelp (s )(forexample,inearlystagesoflearn-
1:T 0 1:t
withlowprobabilityunderthetargetσ(s ),SMCresam- ing),thecrucialfactisthatourfinaltargetdistributionin
1:T
plingwithwell-chosenintermediatetargetsπ clonesthe Eq.(9)reflectsthetargetpotentialϕ(s ). AsinSec.2.2,
t 1:T
mostpromisingpartialsequencess atstept. Sincelater thisensuresthat,regardlessoftheintermediatetwists,our
1:t
samplingproceedsfromtheseprefixes,weexpecttoobtain resultingimportancesamplingestimatorswillbeunbiased.
finalsequenceswhichbettercoverthehigh-probabilityre-
Finally,theoptimaltwistsψ∗(s )recovertheintermediate
gionsofthetargetdistribution. Wediscusstechniquesto t 1:t
marginalsπ∗(s )=σ(s )ofthetargetdistribution. We
evaluatethequalityofSMCorSISsamplinginSec.5. t 1:t 1:t
statethesenseinwhichπ∗andψ∗areoptimalinApp.A.1,
t t
andprovethefollowingpropositioninApp.BProp.B.1.
3.TwistedSequentialMonteCarlofor
Proposition3.2(OptimalTwists). Foragiventargetdis-
LanguageModeling
tribution σ(s ) in Eq. (1), the optimal twist functions
1:T
AkeydesignchoiceintheSMCprocedureaboveisthein- ψ∗(s )(inregionswherep (s )>0)correspondto
t 1:t 0 1:t
termediatetargets{π }T−1,whereweassumeπ (s )=
t t=1 T 1:T 1
σ(s 1:T) is always the target distribution. In state-space π t∗(s 1:t)=σ(s 1:t)=
ψ∗
p 0(s 1:t)ψ t∗(s 1:t). (11)
modelswithobservationlikelihoodsorenvironmentswith Z
t
intermediaterewards,filteringSMCconsiderstargetinfor-
Uptoaconstantindependentofs ,theoptimaltwistsare
1:t
mationcollectedfromtimesτ ≤ ttodefineπ . (Chopin
t (cid:88)
etal.,2020). PreviousworkonSMCforlanguagemodels ψ∗(s )∝ p (s |s )ϕ(s ). (12)
t 1:t 0 t+1:T 1:t 1:T
(Lew et al., 2023) has considered per-token or few-step- st+1:T
aheadstatisticstodefinetractableintermediateπ .However,
t andsatisfytherecursion
weareofteninterestedintargetdistributionswhicharede-
(cid:88)
terminedbyaterminalpotentialϕ(s 1:T)only,asinEq.(1). ψ t∗(s 1:t)∝ p 0(s t+1|s 1:t)ψ t∗(s 1:t+1). (13)
Insuchsettings,twisted SMCmethods(Briersetal.,2010; st+1
Whiteley&Lee,2014;Lawsonetal.,2022)considerthe
Sincetheoptimaltwistfunctionsareunavailableduetothe
fulltargetinformation(untiltimeT)todefine{π }T−1. In
t t=1 needtomarginalizeoverfuturetimesteps,weconsiderlearn-
other words, our desired intermediate targets are the true
ingapproximatetwistfunctionsusingmethodsinSec.4.
marginalsσ(s )ofthetargetdistribution. Intuitively,note
1:t
that in order to exactly sample s ∼ σ(s ), we need
1:T 1:T 3.2.ProposalDistribution
toensurepartialsequencesaredistributedaccordingtothe
Foragivensetoftargets{π }T ,theimportanceweights
intermediate marginals s ∼ σ(s ). In Sec. 3.1, we t t=1
1:t 1:t
willrepresenttheintermediatetargets{π }T−1usingtwist inEq.(10)dependcruciallyonthechoiceofproposal.
t t=1
functionsψ : s →Rwhichmodulatethebasemodelto
t 1:t BaseModelasProposal Themoststraightforwardchoice
(approximately)matchthetargetmarginals,therebysum-
ofproposalisthebasepre-trainedmodel,q =p . Whilewe
0
marizingfutureinformationrelevanttosamplingattimet.
demonstrateinSec.7that SMC resamplingwithlearned
twistsandthebasemodelproposalcancloselyapproximate
3.1.TwistFunctions
the target distribution, this may require large K. We can
Werepresenttheintermediatetargetdistributions{π }T−1
t t=1 achievegreaterefficiencyusingbetterchoicesofproposal.
forSMCsamplingusingthefollowinggeneralform.
Definition3.1(Twisted(Intermediate)Targets). Using Twist-InducedProposal Forgiventargets{π }T ,the
t t=1
approximatetwistfunctions{ψ }T−1andthefinaltargetϕ, optimalproposalminimizesthevarianceoftheimportance
t t=1
wedefinethetwistedintermediatetargetdistributions weights(App.A.1).Inthelanguagemodelsettingwithater-
minalpotentialonly,wewillinfactbeabletosamplefrom
(cid:40)
1 p (s )ψ (s ) t̸=T
π t(s 1:t)= Z tψ 0 1:t t 1:t (9) theoptimalproposalfortheone-stepimportanceweights.
Z1
σ
p 0(s 1:T)ϕ(s 1:T) t=T Proposition3.3. (Twist-InducedProposal). Foragiven
setof intermediatetwisted targets π (s ) inEq. (9), the
t 1:t
Foranarbitraryproposalqandtheunnormalizedtargetsin proposalwhichminimizesthevarianceoftheone-stepin-
Eq.(9),theincrementalimportanceweightsaregivenby crementalimportanceweightsw isgivenby
t
p (s |s ) ψ (s ) π (s )
w (s )= 0 t 1:t−1 t 1:t . (10) qπ(s |s )∝ t 1:t (14)
t 1:t q(s |s ) ψ (s ) t t 1:t−1 π (s )
t 1:t−1 t−1 1:t−1 t−1 1:t−1
1
= p (s |s )ψ (s ).
Whileuninformedtwistfunctionsψ tmayresultinπ t(s 1:t) Z tπ(s 1:t−1) 0 t 1:t−1 t 1:t
4ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
SeeApp.A.2forproof. Fort<T,wecanconstructapa- where the partition function Z (o ) = σ(o ) =
σ T T
(cid:80)
rameterizationofψ (s )suchthattheproposalistractable p (s )σ(o |s )isthemarginalofthegiveno .
t 1:t s1:T 0 1:T T 1:T T
tosampleintransformerarchitectures,wherethenormal-
izationZπ(s )=(cid:80) p (s |s )ψ (s )sumsover In this setting, Prop. 3.2 suggests that the optimal twists,
thediscret tev1 o: ct− ab1 ularyofst nex0 ttot ke1 n:t s− s1 ∈t V.1: Ht
owever,for
which match the marginals σ(s 1:t|o T), correspond to the
t conditionallikelihoodofo givens ,
thefinaltimestep,notethatϕ(s )mayrequirecallstoa T 1:t
1:T
differentneuralnetworksuchasarewardmodelorclassifier. ψ∗(s ,o )∝ (cid:88) p (s |s )ϕ(s ,o )
Wethusconsideranapproximateψ (s ) ≈ ϕ(s )for t 1:t T 0 t+1:T 1:t 1:T T
theproposalq (s |s ) ∝ p
(sT |s1:T
)ψ
(s1:T
)in
st+1:T (18)
T T 1:T−1 0 T 1:T−1 T 1:T
=σ(o |s ),
thefinalstep. Withslightabuseofnotation,weletqπ(s ) T 1:t
1:T
denotethistractableproposaloverfullsequences, (cid:80)
since σ(o |s ) = σ(o ,s |s ). We can
T 1:t st+1:T T t+1:T 1:t
proceedtoconstructintermediatetargetdistributionsand
T−1
qπ(s ):=(cid:16) (cid:89) qπ(s |s )(cid:17) q (s |s ). (15) proposalsasintheprevioussections,whereψ t(s 1:t,o T)and
1:T t t 1:t−1 T T 1:T−1 evenq (s |s ,o )maybeconditionedonaparticular
t=1 t t 1:t−1 T
valueofo .
T
Usingthisproposal,theincrementalweightsbecome
To recover the unconditional setting, we can fix a binary
(cid:80) p (s|s )ψ(s ) observationalvariableσ(o T =1|s 1:T):=ϕ(s 1:T)(Levine,
 st 0
ψ
t−t 1(1 s: 1t− :t−1 1)t 1:t t<T 2018)andomitexplicitconditioning,showingthatcondi-
w(s )= , (16) tionaltwistlearninggeneralizesourpreviousexposition.2
t 1:t (cid:80) sTp 0 ψ(s TT −| 1s (1 s:T 1:− T1 −) 1ψ )T(s 1:T) ψϕ T( (s s1 1:T :T)
)
t=T
Exact Target Sampling on Simulated Data Assum-
ing σ(o |s ) is tractable to sample, we may obtain
T 1:T
whichareindependentofs fort<T.
t an exact sample from the target posterior for simulated
o using ancestral sampling. In particular, by sampling
VariationalProposal AsnotedinSec.2.1,SMCwithno T
s ,o ∼ p (s )σ(o |s ), weobtainasamplefrom
resamplingstepsreducestoSISwiththefulltargetdistri- 1:T T 0 1:T T 1:T
thejointdistribution,whichalsofactorizesasσ(o ,s )=
butionσ(s ). Policygradientmethods(Schulmanetal., T 1:T
1:T σ(o )σ(s |o ). Using the latter factorization, we may
2017; Parshakova et al., 2019; Korbak et al., 2022a; Go T 1:T T
interprets asanexactsamplefromthetargetposterior
etal.,2023)whichdirectlylearnatractableapproximation 1:T
forthegiveno .
q(s )tothetargetdistributionmaythusbeviewedasa T
1:T
particularlysimpleinstanceofSMC,orinferencemoregen- WerefertothisastheBidirectionalMonteCarlo(BDMC)
erally(seeKorbaketal.(2022b)). Wemayalsoevaluate trick(Grosseetal.,2015;2016),andwilluseittodrawexact
theseinferencemethodsusingourproposedtoolsinSec.5. samplesfortraininginSec.4.1.2orevaluationinSec.5.
SeeTable1andApp.Efordetailedlossesanddiscussion.
3.4.ConnectionswithReinforcementLearning
Finally,notethataseparateproposalqmightalsobelearned
Twisted SMC shares close connections with (soft) rein-
alongsidethetwistingtargets{π }T−1. Thismaybeuseful
t t=1 forcementlearning(Levine,2018;Piche´ etal.,2018;Law-
toapproximatethevariance-minimizingproposalformulti-
son et al., 2018; Heng et al., 2020), which we develop
steporadaptiveresampling(Prop.A.5)beyondthetractable
with detailed discussion in App. B.3 and App. D. In par-
optimalone-stepproposalinProp.3.3. Wediscusstraining
ticular, we consider language modeling as a Markov De-
lossesbasedonmulti-stepimportanceweightsinApp.C.1.
cision Process (MDP) with states x := s , actions
t 1:t−1
a := s , and deterministic transitions p(x |x ,a ) =
3.3.ConditionalTargetDistributions t t t+1 t t
δ(s = concat(s ,s )). We describe two differ-
Moregenerally,wemayconsiderconditionaltargetdistri- 1:t t 1:t−1
ent definitions of the reward function in relation to the
butions,obtainedbyconditioningonanobservationrandom
potentialfunctionϕ(s )below. InApp.B.1,wefurther
variableo . Thismirrorsthestandardsettingof SMC in 1:T
T extendourSMCframeworktocapturesettingswithinterme-
state-spacemodels(Doucetetal.,2001;Briersetal.,2010;
diatepotentialsϕ (s )orrewardsoverpartialsequences.
Guetal.,2015;Maddisonetal.,2017;Lawsonetal.,2022), t 1:t
withfurtherdiscussioninApp.B.2. 2Toobtainaprobabilisticinterpretationforσ(o T =1|s 1:T)=
ϕ(s ), note we need to ensure ϕ(s ) ∈ [0,1]. As a result,
1:T 1:T
Definingϕ(s ,o )=σ(o |s )asaprobabilisticmodel samplingfromthetargetσ(s |o =1)orjointσ(s ,o =1)
1:T T T 1:T 1:T T 1:T T
ofo ,ourtargetdistributionistheposteriorσ(s |o ), isnoeasierinthisinterpretationthaninEq.(1),whichisintractable
T 1:T T
ingeneral. Forexample, findingϕ = max ϕ(s )and
max s1:T 1:T
1
dividingϕ(s 1:T) ← ϕ(s 1:T)/ϕ
max
torescaleσ(o
T
= 1|s 1:T)is
σ(s |o )= p (s )σ(o |s ), (17) equivalenttobeingabletoperformrejectionsamplingwiththe
1:T T Z (o ) 0 1:T T 1:T
σ T basemodelproposalp 0(s 1:T)(seeSec.4.1.2).
5ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Base Model Policy Evaluation Viewing the final 4.1.ContrastiveTwistLearning
potentialϕ(s 1:T) as the reward function, the optimality Tomatchourapproximateπθ tothetargetmarginals,we
condition ψ t∗(s 1:t) = (cid:80) st+1:T p 0(s t+1:T|s 1:t)ϕ(s 1:T) in proposetominimizeT separat teKLdivergences,
Eq.(12)correspondstoexactpolicyevaluationofthefuture
reward under the fixed base model policy p (s |s ). T
Mudgaletal.(2023)adoptthisperspective0 fort c+ o1 n:T trol1 l: et d minL CTL(θ):=min(cid:88) D KL(cid:0) σ(s 1:t)(cid:13) (cid:13)π tθ(s 1:t)(cid:1) . (21)
θ θ
decoding,andrefertothetwistfunctionsas‘prefixscorers’. t=1
While other divergences could be used to learn πθ(s ),
t 1:t
SoftRLwithKLRegularization Alternatively,wemay wearguethatthemass-coveringbehaviorofEq.(21)isa
considerthesoftorKL-regularizedRLtargetdistributions desirablepropertyfortwistlearning. Sinceweseparately
commonlyusedinlanguagemodeling(Levine,2018;Ko- matcheachσ(s ),ourhopeisthatsuboptimallearningin
1:t
rbak et al., 2022b) as a special case of our twisted SMC earlytimestepsdoesnotleadtoaggressivepruningofpartial
framework. Foraregularizationstrengthβ,definetheter- sequencesthatwouldachievehighfinaltargetlikelihood.
minalpotentialas
UsingEq.(9),thegradientofEq.(21)ateachtbecomes
ϕ(s 1:T)=eβr(s1:T). (19) E (cid:2) ∇ logψθ(s )(cid:3) −E (cid:2) ∇ logψθ(s )(cid:3) , (22)
σ(s1:t) θ t 1:t π tθ(s1:t) θ t 1:t
Inthiscase,theintermediatetwistfunctionsinDef.3.1cor- whichallowsustolearnfromexacttargetsamplesofσ(s )
1:t
respondtostate-actionQ-values,ψ t(s 1:t)=eβQ(st,s1:t−1) inthefirsttermwhentheyareavailable.
(App. B.3). In particular, consider the recursion for the
WenotethesimilarityoftheobjectiveinEq.(21)andgradi-
optimal twists in Eq. (13). Taking the logarithm of both
entinEq.(22)tomaximumlikelihoodtrainingofenergy-
sidesandrecallingthedefinitionofthesoftvaluefunction
V∗(s )(Levine,2018),weobtain based models (EBM)s. Due to the form of the gradient
1:t
update,werefertothismethodascontrastivetwistlearning
Q∗(s t,s 1:t−1)= β1 log(cid:88) p 0(s t+1|s 1:t)eβQ∗(st+1,s1:t), (20) ( pC osT itL iv) e.W sae mp pr lo inc gee (d firt so td tee rs mcr )ib ane dap np egro ax tii vm ea st ae mte pc lih nn giq (su ee cs of no dr
st+1
term)inthenextsubsections.
(cid:124) (cid:123)(cid:122) (cid:125)
V∗(s1:t)
4.1.1.APPROXIMATENEGATIVESAMPLING
which is a soft Bellman recursion with no intermediate
Acommonchallengeinenergy-basedmodelingisthatthe
reward. FromthesoftRLperspective,thetwistfunctions
secondterminEq.(22)involvessamplingfromthetarget
areanalogoustoacritic,whiletheproposalplaystheroleof π withintractablenormalizationconstantZψ. Weproceed
anactor(Levine,2018;Haarnojaetal.,2018). Weprovide t t
toestimatetheexpectationusingSISasinEq.(4),usinga
detailed discussion of the soft RL case in App. B.3, and
proposalq(s )suchasthebasemodelorthetwist-induced
1:t
reviewRL-inspiredlossesfortwistlearninginApp.C.1.
proposalfromSec.3.2. Notethat SMC resamplingwith
learnedintermediatetwistfunctionscouldalsobeused.
BenefitsoftheProbabilisticPerspective WhilesoftRL
isanaturalspecialcaseofourframeworkwhichgivesintu-
4.1.2.(APPROXIMATE)POSITIVESAMPLING
itionfortheroleofthetwistfunctions,ourapproachallows
IncontrasttotraditionalEBMsettings,wedonotnecessar-
forgeneraltargetdistributionswithoutreferencetoRLob-
ilyhaveexactsamplesavailablefroma‘data’distribution.
jectives and suggests principled probabilistic resampling
Wedescribeseveralsettingsrelatedtoavailabilityofpositive
usingSMC.Further,wedeveloptwistlearningtechniques
samples,whichareexploredinourexperimentsinSec.7.
inspired by density ratio estimation, including our novel
CTL method or the SIXO objective from (Lawson et al.,
Exact Target Samples If exact posterior samples are
2022), which are more naturally motivated from a proba-
available, forexampleusingtheBDMCtrickinSec.3.3,
bilisticperspective. Finally,weleverageourprobabilistic
wemayusethemdirectlyinthegradientupdateinEq.(22).
perspective to propose novel language model evaluation
techniquesinspiredbyBidirectionalMonteCarlo(Grosse Rejection Sampling Rejection sampling can yield ex-
etal.(2015;2016))inSec.5. acttargetsamplessσ whenanupperboundonthelike-
1:T
lihood ratio σ˜(s1:T) ≤ M is known. In cases where
4.LearningtheTwistFunctions
q(s1:T)
the target σ˜(s ) is defined by thresholding or an indi-
1:T
Wenextconsidermethodstolearntwistfunctionsψ tθparam- cator function p 0(s 1:T)I(s 1:t ∈ C) or joint distribution
eterizedbyneuralnetworks,presentinganovelcontrastive p 0(s 1:T)σ(o T|s 1:T), we can clearly take M = 1 for the
twistlearning(CTL)approachinSec.4.1. Wesummarize base model proposal p 0(s 1:T). If the base model yields
twistlearningmethodsfromrelatedworkinSec.4.2. posteriorsamplesinreasonabletime,wecanobtainexact
6ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Name Loss LearningPrinciple
CTL (cid:80)T t=1E πs(oT)(cid:104) DKL(cid:0)σ(s1:t|oT)(cid:13) (cid:13)πtθ(s1:t|oT)(cid:1)(cid:105) (Gradient:) E πs(oT)(cid:104)E σ(s1:t|oT)(cid:2)∇θlogψtθ(s1:t,oT)(cid:3)−E πtθ(s1:t|oT)(cid:2)∇θlogψtθ(s1:t,oT)(cid:3)(cid:105) MarginalMatchingwithMLE
RL (cid:80)T t=− 11E πs(s1:t,oT)(cid:104)(cid:16) log(cid:80) st+1p0(st+1|s1:t)sg(cid:0)ψtθ +1(s1:t+1,oT)(cid:1)−logψtθ(s1:t,oT)(cid:17)2(cid:105) +E πs(s1:T,oT)(cid:104)(cid:0)logϕ(s1:T,oT)−logψTθ(s1:T,oT)(cid:1)2(cid:105) TwistConsistency/SoftQ-Learning
SIXO (cid:80)T t=1E πs(oT)σ(s1:t|oT)(cid:2)logsigmoid(cid:0)logψtθ(s1:t,oT)(cid:1)(cid:3)+E p0(s1:t)πs(oT)(cid:2)log(cid:0)1−sigmoid(cid:0)logψtθ(s1:t,oT)(cid:1)(cid:1)(cid:3) NoiseContrastiveEstimation
FUDGE t(cid:80) =T 1−E πs(s1:t,oT)E p0(st+1:T|s1:t)(cid:104) σ(oT|s1:T)logψtθ(s1:t,oT)+(cid:16) 1−σ(oT|s1:T)(cid:17) log(cid:16) 1−ψtθ(s1:t,oT)(cid:17)(cid:17)(cid:105) BinaryClassification
DPG E πs(oT)(cid:104) DKL(cid:0)σ(s1:T|oT)(cid:13) (cid:13)qξ(s1:T|oT)(cid:1)(cid:105) MaximumLikelihood(MLE)
PPO E πs(oT)(cid:104) DKL(cid:0)qξ(s1:T|oT)(cid:13) (cid:13)σ(s1:T|oT)(cid:1)(cid:105) VariationalInference
Table1: Lossesfortwist(top)andproposal(bottom)learning,whereπ (·)indicatesanarbitrarysamplingdistribution.
s
samplesfortrainingusingrejectionsampling,anduseour perspectiveinApp.C.1andE.1.
twistlearningprocedurestogreatlyimprovesamplingeffi-
ciencyatgenerationtime. SIXO TheSIXOlossproposedbyLawsonetal.(2022)
learnstwistfunctionsusingabinaryclassificationtaskto
Whileanimprovedproposalqshouldmoreefficientlydraw
distinguishsamplesfromthetargetmarginalσ(s |o )and
1:t T
samplesmeetingthetargetconditions,exactrejectionsam-
basemodelp (s )ateachstep,whichcorrespondstonoise
0 1:t
plingwouldrequireestimationofM. Approximateorquasi
contrastiveestimation(Gutmann&Hyva¨rinen,2010)for
rejectionsamplingmightbeusedinthiscase,asanalysed
learningenergy-basedmodels. SeeApp.C.3.
inEikemaetal.(2022).
FUDGE Yang&Klein(2021)learntwistsbyconstruct-
ApproximatePositiveSamplingusingSISorSMC In
ing a binary classification task to instead learn the condi-
caseswhereexactsamplesareunavailableandrejectionsam- tionallikelihoodσ(o |s )(Eq.(18)). Thismaybeviewed
T 1:t
plingisinefficientorinexact,weleverageSMCsampling asenforcingtheT−tstepoptimalityequationinEq.(12)or
withtwisttargets{πθ}T andanyproposalq(s )tofirst
t t=1 1:T Eq.(18),whererolloutsshouldbeobtainedusingthebase
drawasetofK fullsequencess1 1: :K T. AsinEq.(4),wecan modelp 0(s t+1:T|s 1:t)(seeTable1orApp.C.4). Mudgal
usethenormalizedSMCweightssincethelastresampling
et al. (2023); Deng & Raffel (2023) similarly propose to
step to estimate the expected gradient in the first term of enforcetheT −tstepoptimalityconditionusingasquared-
Eq.(22). Withoutresampling,werecoverSISestimation. errorloss,(cid:80) E [(ϕ(s )−ψ (s ))2].
t p0(st+1:T|s1:t) 1:T t 1:t
Whilebothourapproximatepositiveandnegativesampling
5.EvaluatingInferenceinLanguageModels
for estimating the expectations in Eq. (22) rely on SMC
orSISweights(oftenwiththesameproposal),thecrucial OurSMCframeworkyieldsarichsetoftoolsforevaluating
distinctionisthatweightsforpositivesamplingarebased inferencetechniquesinlanguagemodels,usingwell-studied
onthetruetargetpotentialϕ(s 1:T)overfullsequences. quantitiessuchasthelogpartitionfunctionlogZ
σ
andKL
divergencetothetargetdistribution. Remarkably,withac-
TruncationtoPartialSequences Foranexactpositive
cesstoasingleexactsamplefromthetargetdistribution,we
sample,weuseitstruncationtoapartialsequenceoflength
showinProp.5.1thatwecanobtainupperboundsonlogZ
σ
t(whichcorrespondstoasamplefromthedesiredmarginal
inadditiontolowerbounds. Theseboundscantightlysand-
σ )toperformthegradientupdateinEq.(22). Forapprox-
t wichlogZ withincreasingK,therebyensuringreliable
σ
imate positive sampling, we use the same set of K final
conclusionsregardinginferencequality.
weightstoestimatetheexpectedgradientateachtimestep.
5.1.ApplicationsoflogZ Estimation
σ
4.2.TwistLearningMethodsfromRelatedWork
EvaluatingFine-TunedModels Tomotivatethissection
Webrieflydescribealternativeapproachesfortwistlearning,
andpresentanimportantapplicationofourSMCmethods,
withdetaileddiscussioninApp.Candasummaryoftheloss
consider evaluating how well a given q(s ) matches a
functionsformethodsusedinourexperimentsinTable1. 1:T
targetdistributionforcontrolledgenerationorfine-tuning.
SoftQ-Learning(RL) EnforcingtherecursioninEq.(13) Assumethatq istractabletosampleandevaluate. Tocal-
usingasquarederrorlossisanalogoustosoftQ-learning culatetheKLdivergencetoσ ineitherdirection,wealso
in the RL literature (see Eq. (20)), and has been used requireanestimateofthelogpartitionfunctionlogZ σ,
for twisted SMC in Lioutas et al. (2022). Mudgal et al.
(2023)deriveasimilarsquared-errorloss,viewingϕ(s ) (cid:20) q(s ) (cid:21)
1:T D (q(s )∥σ(s ))=E log 1:T +logZ
asthereward. Finally,weinterpretpathconsistencylosses KL 1:T 1:T q p (s )ϕ(s ) σ
0 1:T 1:T
(Nachumetal.,2017),whichwerederivedinthesoftRL (cid:20) p (s )ϕ(s )(cid:21)
D (σ(s )∥q(s ))=E log 0 1:T 1:T −logZ
settingandhavebeenusedforlanguagemodelinginGuo KL 1:T 1:T σ q(s ) σ
1:T
etal.(2021);Huetal.(2023),fromanimportancesampling (23)
7ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
ForD (σ∥q),notethatwealsorequiresamplesfromthe resampling,theSIScaserecoverstheImportanceWeighted
KL
targetσ,asmaybereadilyavailableusingtheBDMCtrick Autoencoder(IWAE)lower(Burdaetal.,2015)andupper
whenσisdefinedasaBayesianposterior(Sec.3.3).Insuch (Sobolev&Vetrov,2019;Brekelmansetal.,2022)bounds.
cases,wearguethatSMCcanbeusedtoaccuratelybound
thevalueoflogZ σ andestimateeachKLdivergenceabove. Samplingfromσ SMC forSMCUpperBounds Wenow
EstimationofD KL(σ∥q)maybeparticularlyimportantto discusssamplingfromσ SMC(S)fortheexpectationinthe
diagnose mode-dropping in inference techniques such as upperbound,whichrequiresasingle,exactsamplefromthe
PPOwhichoptimizethemode-seekingD (q∥σ)during targetdistributionσ(s 1:T). Thissamplemaybeobtained,
KL
fine-tuning(Korbaketal.,2022b). forexample,usingtheBDMCtrickinSec.3.3. Notethat
Sec.2.2andAlg.1describesamplingfromq (S),which
SMC
EvaluatingTwistedSMCSampling AfterrunningSIS isusedfortheexpectationinthelowerbound.
or SMC with K samples, we can sample a single index
Sampling from σ (S) differs from sampling from
asinEq.(5)toreturnasingleapproximatetargetsample SMC
q (S) by its treatment of the exact target sample. In
sσ . However, the marginal distribution of this sample, SMC
1:T particular,thepartialsequencecorrespondingtotheexact
whichwedenoteassσ ∼q (s ),isnottractabledue
1:T SMC 1:T target sample is guaranteed to be cloned once at each re-
to the need to sum over all possible sets of K samples.
samplingstep. Inotherindices,resamplingproceedsasin
Nevertheless,wewillshowbelowthatthetightnessofour
Sec.2.2,wheretheexactsamplemaybeclonedadditional
logZ lowerorupperboundsinProp.5.1providesupper
σ timesbasedonitsincrementalimportanceweights. Finally,
bounds on the KL divergences D (q (s )∥σ(s ))
KL SMC 1:T 1:T wesampleK−1nexttokensfromtheproposal,whilethe
orD (σ(s )∥q (s )),respectively.
KL 1:T SMC 1:T value of the remaining chain is fixed by the exact target
Alternatively, we can also use the single-sample KL di- sample. SeeApp.FandAlg.2fordetaileddiscussion.
vergencesinEq.(23)forthetwist-inducedproposalqπ in
Eq.(15)toevaluateasetoftwistfunctionsψ (Sec.7.2).
t TightnessoftheBidirectionalBounds Sincethebounds
in Prop. 5.1 become exact as K → ∞ for any proposal
5.2.BidirectionalSMCBoundsonlogZ
σ (Burda et al., 2015; Maddison et al., 2017), we can use
Given the importance of logZ estimation as motivated SMCorIWAEwithlargeK tosandwichthelogpartition
σ
above, we propose a bidirectional SMC stochastic upper functionwhenσsamplesareavailable.
boundwhichisnovel(tothebestofourknowledge),and
For a given K, the gap in the extended state space
maybeofinterestoutsideofthelanguagemodelingsetting.
logZ bounds in Prop. 5.1 provides further insight
σ
RecallfromSec.2.2thatSMCadmitsaninterpretationas into the quality of twisted SMC sampling via the dis-
SISinanextendedstatespaceS :={sk,ωk}K,T which tribution of the marginal sample sσ (Sec. 5.1). In
t t k=1,t=1 1:T
includesalltokensandresamplingindices. Wederivelower particular, the data processing inequality suggests that
andupperboundsonlogZ
σ
inProp.5.1below,withproof D KL(q SMC(s 1:T)∥σ(s 1:T))≤D KL(q SMC(S)∥σ SMC(S))
anddetaileddescriptionoftheextendedstatespacetarget andD KL(σ(s 1:T)∥q SMC(s 1:T))≤D KL(σ SMC(S)∥q SMC(S))
σ (S)andproposalq (S)distributionsinApp.F. (Grosseetal.,2015;2016). Thus,ifthedifferencebetween
SMC SMC
upper and lower bounds on logZ is small, then we can
Proposition5.1. (BidirectionalSMCBounds) Thelog σ
conclude that the K-sample SMC or SIS procedures in
partition function logZ of a target distribution σ(s )
σ 1:T Sec. 2.2 yield a single approximate sample sσ whose
canbelowerandupperboundedby 1:T
distribution q (s ) is close to the target σ(s ) in
SMC 1:T 1:T
(cid:34) T K (cid:35) symmetrizedKLdivergence.3
E log(cid:89) 1 (cid:88) w (cid:0) si (cid:1) ≤logZ
q SMC(S) K t 1:t σ
t=1 i=1 6.RelatedWork
(24)
(cid:34) T K (cid:35)
logZ ≤E log(cid:89) 1 (cid:88) w (cid:0) si (cid:1) . Intheprevioussections,wehavediscussedrelatedworkasit
σ σ SMC(S) K t 1:t fitwithinourSMCframeworkforlanguagemodeling.Note
t=1 i=1
thatLewetal.(2023)considerSMCsamplingforlanguage
ThegapinthelowerboundisD (q (S)∥σ (S)),and models,butdonotlearntwistfunctionsorproposals.
KL SMC SMC
thegapintheupperboundisD (σ (S)∥q (S)).
KL SMC SMC Decodingfromlanguagemodelstoobtaindiverse(Holtz-
manetal.,2019;Vilnisetal.,2023)orcontrolledgeneration
SeeApp.Fforadetaileddiscussionandderivations. The
(Zhang et al., 2023; Dathathri et al., 2019; Krause et al.,
proofproceedsbyadaptingageneralapproachforextended
2020; Yang & Klein, 2021; Guo et al., 2021; Qin et al.,
statespacelogpartitionfunctionboundsfromBrekelmans
etal.(2022)usingtheprobabilisticinterpretationofSMC 3Notethatthedifferencebetweenupperandlowerboundyields
fromAndrieuetal.(2010);Maddisonetal.(2017). Withno D (σ (S)∥q (S))+D (q (S)∥σ (S)).
KL SMC SMC KL SMC SMC
8ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
2022;Snelletal.,2022;Huetal.,2023)isanactiveareaof
0
research. OurSMCresamplingapproachmaybeviewedas
aprincipledprobabilisticextensionofbest-of-K decoding 5
methods. Mudgaletal.(2023)proposeaK-wayargmax
10
decodingschemebasedon‘prefixscorers’ψ learnedus-
t
ingEq.(13),butalsoconsiderusingthesetwistsaslogits 15 SIS/IWAE UB (q Proposal)
SIS/IWAE LB (q Proposal)
for softmax sampling in the proposal. However, neither SMC UB (q Proposal)
20 SMC LB (q Proposal)
ofthesedecodingschemesarealignedwithourproposed SIS/IWAE UB (p0 Proposal)
SIS/IWAE LB (p0 Proposal)
SMC framework, as we discuss in detail in App. D. For 25 SMC UB (p0 Proposal)
SMC LB (p0 Proposal)
example,greedyargmaxdecodingwithrespecttotheop- SMC ESS UB (p0 Proposal)
30 SMC ESS LB (p0 Proposal)
timal twists in Prop. 3.2 does not yield samples from the
targetdistributionσ(s ). 35
1:T
4 8 16 32 128 512 2048
Finally, RL-basedmethodssuchas PPO maintainbotha Number of Samples
policyorproposalnetworkandvaluenetworkoradvantage Figure3:ComparisonofSIS(IWAE)andSMCboundsonlogZ
σ
estimatorduringtraining. FromthesoftRLperspectivein forbaseproposalp 0 andtwist-inducedproposalqπ,withtwists
learnedwithCTL.Withthetwist-inducedproposal,bothSISand
Sec.3.4andApp.B.3,thesoftvaluesplayasimilarroleas
SMCboundsaretight;withthebaseproposal,resamplingwith
ourtwistfunctionsforSMCresampling. Liuetal.(2023)
learnedtwistsisneeded. ResamplingbasedonESSinsteadof
considerusingMonteCarloTreeSearch(MCTS)basedon every-stepresamplingyieldssimilarresults.
PPO valueestimatestoimprovedecoding,whileChaffin
etal.(2022)considerdiscriminator-drivenMCTS.
samplesforlogZ UBestimation,butcanrequirehundreds
7.Experiments σ
ofthousandsofsamples. Thus,thissettingalsoallowsusto
Wenowillustrateempiricallyhowourframeworkcanbe testtheeffectivenessofapproximatepositivesamplingfor
usedtoevaluateinferencethroughlogZ σ boundsandKL twisttrainingwhentargetsamplesarerare.
divergencesbetweenthesamplingandtargetdistributions,
Fig.3demonstratesthattrainingtwistswithCTLandap-
providingmeaningfulquantitativecomparisonbetweenvari-
proximatepositivesamplingcansignificantlyimprovelog
ouslearningmethods. Weconsiderarangeoftasksthrough-
partitionfunctionestimationandsamplingefficiency. We
out this section, including toxic story generation (as an
first note that both upper and lower bounds tighten as K
example of uncovering rare undesirable behavior), gen-
increases,asexpected,forbothSISandSMC.Usingp as
erating reviews with varied sentiment, and infilling. For 0
proposal,theSISLB(orange)generallyfailstodrawany
the toxicity and infilling tasks, we consider the TinySto-
samplesmeetingthethreshold. Bycontrast, SMCresam-
ries model (Eldan & Li, 2023)4 as a small-scale model
pling(red)withp proposaleventuallyachievestightlogZ
where the generation is coherent, and use the prompt of 0 σ
upperandlowerbounds,yieldingnear-exacttargetsamples
‘Once upon a time, there was a’. For the toxicity task,
(smallKLdivergencebetweenthedistributionoversamples
we elicit responses judged to be toxic by the classifier
andthetargetdistribution)bythereasoninginSec.5.
from Correˆa (2023)5. For the sentiment task, we con-
sider the GPT2-Medium6 model and a classifier trained However,both SMC and SIS withthetwist-inducedpro-
on Amazon reviews.7 Our code is available at https: posalachievetightestimationandnear-exactsamplingof
//github.com/Silent-Zebra/twisted-smc-lm. thetargettoxicoutputswithordersofmagnitudelowerK.
Resamplingdoesnotappeartohelporhurtthesebounds,as
7.1.ComparingSISandSMCforlogZ Estimation theeffectofthetwistshasbeenincorporatedintheproposal
σ
qπ in Eq. (15). Thus, we conclude that using the twist-
We first use our logZ bounds to test how twisted SMC
σ induced proposal can provide significant efficiency gains
canimproveupon SIS andefficientlysamplerareevents.
overbasemodelsampling.
Weconsiderthetaskoftoxicstorygeneration. Thetarget
is defined as σ(s 1:T) ∝ p 0(s 1:T)I[s 1:T ∈ C] where C := 7.2.EvaluatingTwist-InducedorVariationalProposals
{s |r(s )≤η},r(s )isthenon-toxiclogit,andthe
1:T 1:T 1:T
thresholdη =−5correspondstoagreaterthan99%chance We next leverage our logZ σ bounds to evaluate single-
of being toxic. Rejection sampling under p
0
yields exact sample inference using D KL(q∥σ) and D KL(σ∥q), as in
Sec.5.1. Acrosssettings, weconsidertwo SIS proposal-
4https://huggingface.co/roneneldan/TinyStories-33M
learningmethods: PPO(Schulmanetal.,2017)whichmin-
5https://huggingface.co/nicholasKluge/ToxicityModel
imizesD (q∥σ)duringoptimization,anddistributional
6https://huggingface.co/gpt2-medium KL
policygradient(DPG),whichminimizesD (σ∥q)(Par-
7https://huggingface.co/LiYuan/amazon-review-sentiment-analysis KL
shakovaetal.,2019)(seeApp.E).
9
dnuoB
Z
goLProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Proposalq TwistLearning DKL(q∥σ) DKL(σ∥q) Proposalq TwistLearning DKL(q∥σ) DKL(σ∥q) ProposalqoT TwistLearning E oT[DKL(qoT∥σoT)] E oT[DKL(σoT∥qoT)]
Twisted Contrastive 1.11±0.05 1.07±0.02 Twisted Contrastive 0.55±0.03 0.47±0.01 Twisted Contrastive 23.93±0.34 8.87±0.05
Twisted RL 1.52±0.09 1.42±0.03 Twisted RL 0.94±0.04 0.81±0.02 Twisted RL 31.35±2.33 14.96±1.69
Twisted SIXO 1.71±0.06 1.98±0.04 Twisted SIXO 0.73±0.03 0.59±0.02 Twisted SIXO 20.34±0.36 7.43±0.04
Twisted FUDGE 3.24±0.26 2.00±0.13 Twisted FUDGE 1.01±0.07 0.77±0.07 Twisted FUDGE 60.93±2.82 19.85±0.51
DPG – 1.09±0.05 1.12±0.03 DPG – 0.72±0.04 0.57±0.01 DPG – 13.27±0.44 4.90±0.03
PPO – 0.98±0.01 1.32±0.04 PPO – 1.04±0.31 0.87±0.20 PPO – 19.37±0.41 14.07±0.50
Table2: Toxicity(Sec.7.2.1) Table3: Sentiment(Sec.7.2.2) Table4: Infilling(Sec.7.2.3)
ForwardandreverseKLdivergencesbetweentheSMCorvariationalproposaldistributionsandthetruetargetσ.
Weconsider four twist learning methods, including CTL correspond to continuation tokens, and their likelihood
and RL from Sec. 4, SIXO (Lawson et al., 2022), and σ(o |s ) := p (s |s ) is evaluated under the
T 1:T 0 T+1:T+c 1:T
FUDGE (Yang & Klein, 2021) (see App. C). For each, basemodel,givengenerateds . Thetargetdistribution
1:T
we measure KL divergences involving the twist-induced corresponds to the posterior σ(s |o ). Instead of train-
1:T T
proposalqπ. Thus,theseexperimentsshowcasetwocomple- ingseparate{ψθ}foreacho ,weamortizelearningofa
t T
mentaryapplicationsofSMC:asanovelinferencemethod conditionaltwistnetworkψθ(s ,o ).
t 1:t T
yielding a tractable qπ, and as an evaluation method for
Aseconddistinctivefeatureofthissettingisthatwetrain
anyotherinferencemethod(suchasPPO)usingK-sample
from exact posterior or target samples, which are readily
boundsonlogZ toestimatetheKLdivergence.
σ availableusingtheBDMCtrickinSec.3.3. Inparticular,
7.2.1.GENERATINGTOXICSTORIES wemaysamplesequencesoflengthT +cfromthebase
model s ∼ p (s ) = σ(s ,o = s ),
We consider toxic story generation as in Sec. 7.1, but us- 1:T+c 0 1:T+c 1:T T T+1:T+c
andinterprettheprefixs ∼σ(s |o =s )as
ing a target σ(s ) ∝ p (s )p(a = 1|s ), where 1:T 1:T T T+1:T+c
1:T 0 1:T 1:T atargetsample. Notethatwedonotexplicitlycontrolthe
p(a = 1|s ) denotes the probability of the text being
1:T continuationstokenso definingthetasks. Weevaluateav-
judgedastoxicbyaclassifier. Comparedtothethreshold- T
erageKLdivergencesover2000differento =s ,
ingtarget,thistaskprovidesasmoothergradientsignalfor T T+1:T+c
withT =15andc=10,andreportresultsinTable4.
learning(seeApp.G.3)butstillallowsforexactsampling
via rejection sampling. We train using approximate posi- WefindthatDPGperformsbestforbothdirectionsofthe
tivesampling,butprovideanablationwithexactpositive KLdivergenceinthissetting,likelyduetoitsabilitytolever-
samplingresultsinApp.H.3. ageexactpositivesamplesbyminimizingD (σ ∥q ).
KL oT oT
While CTL alsolearnsfromexactpositivesamples,itre-
WereportKLdivergencesinTable2. WeobservethatPPO
quiresapproximatenegativesamplingandonlyperforms
learns the best proposal with respect to D (q∥σ) while
KL comparably to SIXO, which uses exact positive samples
ourCTLmethodperformsbestwithrespecttoD (σ∥q),
KL and performs exact negative sampling under p . Finally,
which is consistent with the divergences minimized dur- 0
PPOtrainsfromq samplesonly,andperformsrelatively
ing training. Finally, in App. H.1 we provide a quali- oT
poorlywithrespecttoD (σ ∥q ). Weshowqualitative
tative example of a toxic story generated with CTL for KL oT oT
resultsinApp.H.1tocorrelateKLdivergenceresultswith
σ(s ) ∝ p (s )p(a = 1|s )β with β = 10, a case
1:T 0 1:T 1:T samplequality.
wherenoexactsamplesareavailable.
UsingourKLdivergenceevaluationmethods,weconclude
7.2.2.GENERATIONWITHVARIEDSENTIMENT
DPGmaybepreferablewhenexacttargetsamplesareavail-
Forthesentimentsettingdescribedearlier,weconsidera able(Sec.7.2.3,App.H.3),whileCTLmaybepreferable
prompt‘Iboughtthis’andtargetσ(s 1:T)∝p 0(s 1:T)p(a= withapproximatepositivesampling(Sec.7.2.1,Sec.7.2.2).
1|s ), where a = 1 indicates a 1-star review and exact
1:T
samplesareavailablebyrejectionsampling. Wetrainusing 8.Conclusion
approximatepositivesampling(seeApp.H.3forcompari-
Inthiswork,wehavepresentedtwistedSMCasaprincipled
sonwithexact). WhileallmethodsachievelowKLdiver-
probabilisticinferenceframeworkforsolvingnumerousca-
gencesinTable3,CTLperformsbestforbothdirections.
pabilityandsafetytasksinLLMs.Afterdiscussingdifferent
7.2.3.INFILLING designchoicesfortwistedSMCandtheirrelationtorelated
work, we proposed a novel contrastive method for twist
Inthissection,wedemonstrateaconditionaltwistfunction
learning. Furthermore, we have proposed novel bidirec-
parameterization,whereψθ(s ,o )takesinputo which
t 1:t T T tionalSMCboundsforevaluatingLLMinferencemethods.
identifiesthetargetdistributionσ(s |o )asinSec.3.3.
1:T T
Wedemonstratedtheeffectivenessofourmethodsquanti-
We consider an infilling task (Lew et al., 2023; Hu et al.,
tativelyandqualitativelyinbothsamplingandevaluation
2023), where the observation variables o := s
T T+1:T+c
acrossavarietyofexperimentalsettings.
10ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Acknowledgments conditional expectation as a bregman predictor. IEEE
TransactionsonInformationTheory,51(7),2005.
AMandRGacknowledgesupportfromtheCanadaCIFAR
AIChairsprogramandfromOpenPhilanthropy. SZthanks
Brekelmans,R.,Huang,S.,Ghassemi,M.,VerSteeg,G.,
Juhan Bae for helping debug memory issues in the code.
Grosse,R.B.,andMakhzani,A. Improvingmutualin-
Resourcesusedinthisresearchwereprovided,inpart,by
formation estimation with annealed and energy-based
the Province of Ontario, the Government of Canada, and
bounds. InInternationalConferenceonLearningRepre-
companies sponsoring the Vector Institute. We thank the
sentations,2022.
anonymousreviewersforhelpfulcommentsonearlierver-
sionsofthispaper. Briers, M., Doucet, A., andMaskell, S. Smoothingalgo-
rithmsforstate–spacemodels. AnnalsoftheInstituteof
ImpactStatement
StatisticalMathematics,62:61–89,2010.
Thispaperismotivatedbythesocialconsequencesofre-
Burda,Y.,Grosse,R.,andSalakhutdinov,R. Importance
centadvancesinthefieldofmachinelearning. Controlled
weightedautoencoders.arXivpreprintarXiv:1509.00519,
generationfromlanguagemodelshasthepotentialtoim-
2015.
provesafetythroughbettersteeringofgenerationtohuman
preferences,moreefficientautomatedred-teaming,andthe
Chaffin, A., Claveau, V., and Kijak, E. Ppl-mcts: Con-
abilitytoestimateorboundprobabilitiesofrarebehaviors.
strainedtextualgenerationthroughdiscriminator-guided
Any such work is inherently a double-edged sword; the
mctsdecoding. InNAACL2022-ConferenceoftheNorth
sametechniquesusedtogeneratesamplesfromaharmless
AmericanChapteroftheAssociationforComputational
distribution of text could, with a single sign change, be
Linguistics: HumanLanguageTechnologies,2022.
repurposed for generating samples from a harmful distri-
bution of text. Thus, better controlled generation (in our Chopin,N.,Papaspiliopoulos,O.,etal. Anintroductionto
framework,bettersamplingfromtargetdistributions)can sequentialMonteCarlo,volume4. Springer,2020.
providebenefitsinthehandsofresponsibleusersbutcan
alsomagnifyharmsinthehandsofmalevolentusers(who Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H.,
haveaccesstomodelweights). Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano,
R.,etal. Trainingverifierstosolvemathwordproblems.
Overall,webelievethepotentialpositivesocialbenefitsof
arXivpreprintarXiv:2110.14168,2021.
ourworkinevaluationandsteeringlanguagemodeloutput
towardsdesiredtargetdistributionsoutweighthepotential Correˆa, N. K. Aira, 2023. URL https://huggingface.
negativesstemmingprimarilyfrommisuse. co/nicholasKluge/ToxicityModel.
References Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E.,
Molino,P.,Yosinski,J.,andLiu,R. Plugandplaylan-
Andrieu,C.,Doucet,A.,andHolenstein,R.Particlemarkov guagemodels: Asimpleapproachtocontrolledtextgen-
chain monte carlo methods. Journal of the Royal Sta- eration. InInternationalConferenceonLearningRepre-
tisticalSocietySeriesB:StatisticalMethodology,72(3): sentations,2019.
269–342,2010.
DelMoral,P.,Doucet,A.,andJasra,A. Sequentialmonte
Anil, C., Zhang, G., Wu, Y., and Grosse, R. Learning to carlosamplers. JournaloftheRoyalStatisticalSociety
givecheckableanswerswithprover-verifiergames. arXiv SeriesB:StatisticalMethodology,68(3):411–436,2006.
preprintarXiv:2108.12099,2021.
Deng,H.andRaffel,C. Reward-augmenteddecoding: Ef-
Bae,J.,Zhang,M.R.,Ruan,M.,Wang,E.,Hasegawa,S., ficient controlled text generation with a unidirectional
Ba,J.,andGrosse,R.B. Multi-ratevae: Trainonce,get reward model. In The 2023 Conference on Empirical
the full rate-distortion curve. In The Eleventh Interna- MethodsinNaturalLanguageProcessing,2023.
tionalConferenceonLearningRepresentations,2022.
Dohan,D.,Xu,W.,Lewkowycz,A.,Austin,J.,Bieber,D.,
Bai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,Das- Lopes,R.G.,Wu,Y.,Michalewski,H.,Saurous,R.A.,
Sarma,N.,Drain,D.,Fort,S.,Ganguli,D.,Henighan,T., Sohl-Dickstein,J.,etal.Languagemodelcascades.arXiv
etal. Trainingahelpfulandharmlessassistantwithrein- preprintarXiv:2207.10342,2022.
forcementlearningfromhumanfeedback. arXivpreprint
arXiv:2204.05862,2022. Domke,J.andSheldon,D.R. Importanceweightingand
variational inference. Advances in neural information
Banerjee,A.,Guo,X.,andWang,H. Ontheoptimalityof processingsystems,31,2018.
11ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Doucet,A.,DeFreitas,N.,Gordon,N.J.,etal. Sequential Khalifa, M., Elsahar, H., and Dymetman, M. A distri-
Monte Carlo methods in practice, volume 1. Springer, butional approach to controlled text generation. arXiv
2001. preprintarXiv:2012.11635,2020.
Eikema, B., Kruszewski, G., Dance, C. R., Elsahar, H., Khanov,M.,Burapacheep,J.,andLi,Y. ARGS:Alignment
andDymetman,M. Anapproximatesamplerforenergy- as reward-guided search. In The Twelfth International
basedmodelswithdivergencediagnostics. Transactions Conference on Learning Representations, 2024. URL
onMachineLearningResearch,2022. https://openreview.net/forum?id=shgx0eqdw6.
Eldan, R. and Li, Y. Tinystories: How small can lan-
Korbak, T., Elsahar, H., Kruszewski, G., and Dymetman,
guagemodelsbeandstillspeakcoherentenglish? arXiv
M. Controlling conditional language models without
preprintarXiv:2305.07759,2023.
catastrophicforgetting. InInternationalConferenceon
Finke,A. Onextendedstate-spaceconstructionsforMonte MachineLearning,pp.11499–11528.PMLR,2022a.
Carlomethods. PhDthesis,UniversityofWarwick,2015.
Korbak,T.,Perez,E.,andBuckley,C.L.Rlwithklpenalties
Go,D.,Korbak,T.,Kruszewski,G.,Rozen,J.,Ryu,N.,and is better viewed as bayesian inference. arXiv preprint
Dymetman,M. Aligningfoundationmodelsforlanguage arXiv:2205.11275,2022b.
withpreferencesthroughf-divergenceminimization. In
InternationalConferenceonMachineLearning,2023. Krause, B., Gotmare, A. D., McCann, B., Keskar, N. S.,
Joty,S.,Socher,R.,andRajani,N.F. Gedi: Generative
Grosse,R.B.,Ghahramani,Z.,andAdams,R.P. Sandwich- discriminatorguidedsequencegeneration. arXivpreprint
ing the marginal likelihood using bidirectional monte arXiv:2009.06367,2020.
carlo. arXivpreprintarXiv:1511.02543,2015.
Lawson, D., Tucker, G., Naesseth, C. A., Maddison, C.,
Grosse,R.B.,Ancha,S.,andRoy,D.Measuringthereliabil-
Adams,R.P.,andTeh,Y.W. Twistedvariationalsequen-
ityofmcmcinferencewithbidirectionalmontecarlo. Ad-
tialmontecarlo. InThirdworkshoponBayesianDeep
vancesinNeuralInformationProcessingSystems,2016.
Learning(NeurIPS),2018.
Gu,S.S.,Ghahramani,Z.,andTurner,R.E.Neuraladaptive
Lawson,D.,Ravento´s,A.,Warrington,A.,andLinderman,
sequentialmontecarlo. Advancesinneuralinformation
S. Sixo: Smoothing inference with twisted objectives,
processingsystems,28,2015.
2022.
Guo,H.,Tan,B.,Liu,Z.,Xing,E.P.,andHu,Z. Efficient
(soft) q-learning for text generation with limited good Levine, S. Reinforcement learning and control as proba-
data. arXivpreprintarXiv:2106.07704,2021. bilistic inference: Tutorial and review. arXiv preprint
arXiv:1805.00909,2018.
Gutmann, M. and Hyva¨rinen, A. Noise-contrastive esti-
mation: A new estimation principle for unnormalized Lew, A. K., Zhi-Xuan, T., Grand, G., and Mansinghka,
statistical models. In International conference on ar- V.K. Sequentialmontecarlosteeringoflargelanguage
tificial intelligence and statistics, pp. 297–304. JMLR models using probabilistic programs. arXiv preprint
WorkshopandConferenceProceedings,2010. arXiv:2306.03081,2023.
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft Lioutas,V.,Lavington,J.W.,Sefas,J.,Niedoba,M.,Liu,
actor-critic: Off-policymaximumentropydeepreinforce- Y.,Zwartsenberg,B.,Dabiri,S.,Wood,F.,andScibior,A.
ment learning with a stochastic actor. In International Criticsequentialmontecarlo. InTheEleventhInterna-
conferenceonmachinelearning.PMLR,2018. tionalConferenceonLearningRepresentations,2022.
Heng, J., Bishop, A., Deligiannidis, G., and Doucet, A.
Liu, A., Sap, M., Lu, X., Swayamdipta, S., Bhagavatula,
Controlledsequentialmontecarlo. AnnalsofStatistics,
C., Smith, N. A., and Choi, Y. Dexperts: Decoding-
48(5),2020.
time controlled text generation with experts and anti-
Holtzman,A.,Buys,J.,Du,L.,Forbes,M.,andChoi,Y.The experts. In59thAnnualMeetingoftheAssociationfor
curiouscaseofneuraltextdegeneration. InInternational Computational Linguistics and the 11th International
ConferenceonLearningRepresentations,2019. JointConferenceonNaturalLanguageProcessing,2021.
Hu, E. J., Jain, M., Elmoznino, E., Kaddar, Y., Lajoie, Liu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi, H.,
G.,Bengio,Y.,andMalkin,N. Amortizingintractable andCelikyilmaz,A. Don’tthrowawayyourvaluemodel!
inference in large language models. arXiv preprint makingppoevenbetterviavalue-guidedmonte-carlotree
arXiv:2310.04363,2023. searchdecoding. arXive-prints,pp.arXiv–2309,2023.
12ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Maddison,C.J.,Lawson,J.,Tucker,G.,Heess,N.,Norouzi, Snell, C. V., Kostrikov, I., Su, Y., Yang, S., and Levine,
M., Mnih, A., Doucet, A., and Teh, Y. Filtering varia- S. Offline rl for natural language generation with im-
tionalobjectives. AdvancesinNeuralInformationPro- plicitlanguageqlearning. InTheEleventhInternational
cessingSystems,30,2017. ConferenceonLearningRepresentations,2022.
Mudgal,S.,Lee,J.,Ganapathy,H.,Li,Y.,Wang,T.,Huang, Sobolev,A.andVetrov,D.P.Importanceweightedhierarchi-
Y., Chen, Z., Cheng, H.-T., Collins, M., Strohman, T., calvariationalinference.AdvancesinNeuralInformation
etal. Controlleddecodingfromlanguagemodels. arXiv ProcessingSystems,32,2019.
preprintarXiv:2310.17022,2023.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Voss, C., Radford, A., Amodei, D., and Christiano,
Bridging the gap between value and policy based rein- P.F. Learningtosummarizewithhumanfeedback. Ad-
forcementlearning. Advancesinneuralinformationpro- vances in Neural Information Processing Systems, 33:
cessingsystems,30,2017. 3008–3021,2020.
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C., Vilnis,L.,Zemlyanskiy,Y.,Murray,P.,Passos,A.T.,and
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A., Sanghai,S. Arithmeticsampling: paralleldiversedecod-
et al. Training language models to follow instructions ingforlargelanguagemodels. InInternationalConfer-
withhumanfeedback. AdvancesinNeuralInformation enceonMachineLearning.PMLR,2023.
ProcessingSystems,35:27730–27744,2022.
Whiteley,N.andLee,A. Twistedparticlefilters. 2014.
Parshakova,T.,Andreoli,J.-M.,andDymetman,M. Distri-
Yang,K.andKlein,D. Fudge: Controlledtextgeneration
butionalreinforcementlearningforenergy-basedsequen-
withfuturediscriminators. InProceedingsofthe2021
tialmodels. arXivpreprintarXiv:1912.08517,2019.
ConferenceoftheNorthAmericanChapteroftheAssoci-
Perez,E.,Huang,S.,Song,F.,Cai,T.,Ring,R.,Aslanides, ationforComputationalLinguistics: HumanLanguage
J.,Glaese,A.,McAleese,N.,andIrving,G. Redteaming Technologies,pp.3511–3535,2021.
languagemodelswithlanguagemodels. InProceedings
Zhang, H., Song, H., Li, S., Zhou, M., and Song, D. A
ofthe2022ConferenceonEmpiricalMethodsinNatural
surveyofcontrollabletextgenerationusingtransformer-
LanguageProcessing,pp.3419–3448,2022.
based pre-trained language models. ACM Computing
Phan, D., Hoffman, M.D., Douglas, S., Le, T.A., Parisi, Surveys,56(3):1–37,2023.
A.T.,Sountsov,P.,Sutton,C.,Vikram,S.,Saurous,R.A.,
Ziegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,
etal. Trainingchain-of-thoughtvialatent-variableinfer-
A.,Amodei,D.,Christiano,P.,andIrving,G.Fine-tuning
ence. InThirty-seventhConferenceonNeuralInforma-
languagemodelsfromhumanpreferences. arXivpreprint
tionProcessingSystems,2023.
arXiv:1909.08593,2019.
Piche´, A., Thomas, V., Ibrahim, C., Bengio, Y., and Pal,
Zou,A.,Wang,Z.,Kolter,J.Z.,andFredrikson,M. Uni-
C. Probabilistic planning with sequential monte carlo
versalandtransferableadversarialattacksonalignedlan-
methods. InInternationalConferenceonLearningRep-
guagemodels. arXivpreprintarXiv:2307.15043,2023.
resentations,2018.
Qin, L., Welleck, S., Khashabi, D., and Choi, Y. Cold
decoding: Energy-basedconstrainedtextgenerationwith
langevin dynamics. Advances in Neural Information
ProcessingSystems,35:9538–9551,2022.
Rafailov,R.,Sharma,A.,Mitchell,E.,Ermon,S.,Manning,
C.D.,andFinn,C. Directpreferenceoptimization: Your
languagemodelissecretlyarewardmodel.arXivpreprint
arXiv:2305.18290,2023.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXivpreprintarXiv:1707.06347,2017.
Shih,A.,Sadigh,D.,andErmon,S. Longhorizontempera-
turescaling. arXivpreprintarXiv:2302.03686,2023.
13ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Appendix
Table of Contents
A Proofs 15
A.1 ProofforOptimalIntermediateTargetDistributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 ProofofTwist-InducedProposal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 DerivationofCTLGradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B SMCwithIntermediatePotentialsandConnectionwithSoftReinforcementLearning 18
B.1 TwistedSMCwithIntermediatePotentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 ConditionalTwistedSMC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.3 ConnectionwithSoftReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.4 RemarksonParameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C TwistLearningLosses 24
C.1 SoftQ-Learning(RL)andPathConsistencyLossesfromLogImportanceWeights . . . . . . . . . . . . 24
C.2 ControlledDecodingLossesviaOptimalTwistIdentities(Mudgaletal.,2023) . . . . . . . . . . . . . . 27
C.3 SIXO:SmoothingInferencewithTwistedObjectives(Lawsonetal.,2022) . . . . . . . . . . . . . . . . 28
C.4 FUDGE:FutureDiscriminators(Yang&Klein,2021) . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D DecodingStrategiesusingLearnedTwistsfromMudgaletal.(2023) 30
D.1 ProposalSamplinginMudgaletal.(2023) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
D.2 BlockwiseGreedyDecodinginMudgaletal.(2023) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E ProposalLearningMethods 32
E.1 PathConsistencyLearningforControlledGeneration. . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.2 PolicyGradientMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
E.3 PolicyGradientwithMass-Covering/MaximumLikelihoodKLDivergence . . . . . . . . . . . . . . . 33
F BidirectionalSMC 36
G AdditionalExperimentDetails 40
G.1 CommonDetailsAcrossExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
G.2 ChoicesofTwistParameterization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
G.3 CommentsonOurChoicesofExperimentSettings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
G.4 Experiment-SpecificDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
H AdditionalExperimentalResults 44
H.1 QualitativeResults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
H.2 InfillingwithFewerTokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
H.3 Approximatevs.ExactPosteriorSampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
14ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Table5: ExamplesofTargetPosteriorsinLanguageModelFinetuningandControlledGeneration
Type Target References/Examples
Reward σ(s 1:T)∝p 0(s 1:T)e±βr(s1:T) RLHF(Ziegleretal.,2019;Ouyangetal.,2022;Korbaketal.,2022b)
Continuation σ(s 1:T)∝p 0(s 1:T)p 0(s T+1:T+c|s 1:T)β Generatestokensbasedonlikelihoodoffuturetokensp(s T+1:T+c|s 1:T)
Forβ=1,thisisin-filling(Lewetal.,2023).
Asβ→∞,disregardp 0(s 1:T),focusonargmaxofcontinuationprob.
-similartoadversarialpromptgeneration(Zouetal.,2023)
Indicator σ(s 1:T)∝p 0(s 1:T)I[s 1:T ∈C] Generationss 1:T fromthistargetmustsatisfythepropertiesofsetC.
whereIisindicatorofsetC: -MeetingrewardthresholdC
r≤η
:={s
1:T
|±r(s 1:T)≤η}
I[s
1:T
∈C]=1if[s
1:T
∈C] -Containingtopicalorspecificwordsins
1:T
I[s 1:T ∈C]=0if[s 1:T ∈/C] -Havingcertainstructureorrhyme(Yang&Klein,2021),
-Validoutputaccordingtoverifier(Cobbeetal.,2021;Dohanetal.,2022))
Classifier σ(s 1:T)∝p 0(s 1:T)p(y|s 1:T)β Classycanbeabinary(e.g.toxicity)ormultinomial(e.g.1-5starreviews)
Bayesianposteriorforβ=1:σ(s 1:T)=p(s 1:T|y)∝p 0(s 1:T)p(y|s 1:T)
(Dathathrietal.,2019;Krauseetal.,2020;Liuetal.,2021)
Global σ(s 1:T)∝p 0(s 1:T)β Temperingonentiresequences(long-horizon)vs.per-token(myopic)
Temperature -yieldshigherqualitygenerationinShihetal.(2023)
Distributional σ(s 1:T)∝p 0(s 1:T)eβ·T(s1:T) KLminimizationsubj.expectationconstraintsonT ={T i}
argminD KL(q(s 1:T)∥p 0(s 1:T))s.t.E q[T]=η
β
(β=optimalLagrangemultipliersforconstraintsη)
e.g.genderroles/references(Khalifaetal.,2020)
Intermediate References/Examples
Indicator ϕ t(s 1:t)=I[s t∈C] or I[s 1:t∈C] wordsofspecificlength,orspecificsetsoftokens
(Khalifaetal.,2020;Lewetal.,2023)
Productof
Experts σ(s 1:T)∝(cid:81)M m=1(cid:81)T t=1p 0(s t|s 1:t−1,s( 0m)) promptintersection(Lewetal.,2023)
A.Proofs
Inthissection, wepresentthesenseinwhichthetargetmarginalscorrespondtotheoptimalintermediatedistributions
intwistedSMC.WedeferproofofProp.3.2fromthemaintexttoslightlymoregeneralversioninApp.B.1Prop.B.1,
althoughProp.A.4providestheanalogousstatementintermsoftheintermediatetargetdistributionsπ∗(s ) = σ(s )
t 1:t 1:t
insteadoftheoptimaltwistsψ∗.
t
WealsoproveProp.3.3fromthemaintextinApp.A.2andderivethegradientoftheCTLloss(Eq.(22))inApp.A.3.
A.1.ProofforOptimalIntermediateTargetDistributions
In order to achieve sampling from the full joint distribution σ(s ), each intermediate target σ(s ) must match the
1:T 1:t
intermediatemarginalσ(s ). Toformalizethisnotion,weprovidethefollowingdefinitionofoptimality,justifiedbythe
1:t
factthatityieldsanexactpartitionfunctionestimator.
Todoso,wewillconsiderthemulti-stepimportanceweights
t+c−1 t+c−1
w (s )=
(cid:89)
w (s )=
(cid:89) π˜ τ(s 1:τ)
=
π˜ t+c−1(s 1:t+c−1)
t:t+c−1 1:t+c−1 τ 1:τ π˜ (s )q(s |s ) π˜ (s )q(s |s )
τ−1 1:τ−1 τ 1:τ−1 t−1 1:t−1 t:t+c−1 1:t−1
τ=t τ=t
(c-StepSMCWeights)
usingatelescopingcancellationinthefinalequality. Theone-stepweightscorrespondtoc=1,denotedsimplyasw .
t
DefinitionA.1(OptimalTwistedSMCSampling). Foragiventargetdistributionσ(s )∝p (s )ϕ(s ),werefer
1:T 0 1:T 1:T
toatwistedSMCprocedure, SMC({π }T ,q,K)orSMC(p ,{ψ }T ,q,K)(withπ = σ orψ = ϕ),asoptimalif
t t=1 0 t t=1 T T
c-stepimportanceweightsw (s )=Zψ /Zψ forall1≤t≤T and0≤c≤T −t+1.
t:t+c−1 1:t+c−1 t+c−1 t−1
Note,thattheroleofψ andZψ isspecifiedinDef.3.1. Weassumeπ =σforthegoalofestimatingZ ,andshowbelow
t t T σ
thatanoptimaltwistedSMCprocedureyieldsanexactpartitionfunctionestimator.
15ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
PropositionA.2(OptimalSMCyieldsExactPartitionFunctionEstimation). ForanyoptimaltwistedSMCprocedure,
theresultingestimatorofthepartitionfunctionZ haszerobiasandzerovariance.
σ
Proof. AsinFootnote1orApp.FAlg.2,consider{t }R indextimestepswhereresamplingoccursandfixt =0and
r r=1 0
t = T. The SMC estimator of Z = Zψ becomes ZˆSMC = (cid:81)R 1 (cid:80)K (cid:16) (cid:81)tr w (cid:0) si (cid:1)(cid:17) for S ∼ q (S).
R σ T σ r=1 K i=1 t=tr−1+1 t 1:t SMC
Using the optimality definition in Def. A.1, we have w (s ) = Zψ/Zψ for all partial sequences s . Noting the
t 1:t t t−1 1:t
telescopingmultiplicativecancellationandthefactthatw (si )=Zψ/Zψ isconstantwithrespecttoindicesi∈[1,K],
t 1:t t t−1
wehavethefollowingestimatorforasinglerunofanoptimalSMCprocedure,
 
Zˆ σSMC =
(cid:89)R K1 (cid:88)K

(cid:89)tr
w
t(cid:16)
si
1:t(cid:17)
=
(cid:89)R ZZ ψtψ
r =
Z Ztψ
ψR =
Z ZTψ
ψ =Z σ (25)
r=1 i=1 t=tr−1+1 r=1 tr−1 t0 0
asdesired,assumingZψ =1. SinceZˆSMC =Z isindependentofS,weconcludeZˆSMC haszerobiasandzerovariance.
0 σ σ σ
NotethatwecouldalsodefineoptimalityinDef.A.1usingtheconditionthatw (s )=constforall1≤t≤T
t:t+c−1 1:t+c−1
and0 ≤ c ≤ T −t+1. FollowingsimilarderivationsasabovewouldyieldZˆSMC = const. AswewillshowinApp.F,
σ
ZˆSMC isunbiasedwithE[ZˆSMC]=Z . WethusconcludethatZˆSMC =Z withzerovariance,andthusProp.A.2holds.
σ σ σ σ σ
Withthisnotionofoptimalityinmind,wedemonstratethefollowingnecessaryandsufficientconditions.
PropositionA.3(OptimalityConditions). ThefollowingconditionsarenecessaryandsufficientfortwistedSMCoptimality,
(i): π∗(s )=σ(s ) ∀ 1≤t≤T
t 1:t 1:t (26)
(ii): q∗(s |s )=σ(s |s ) ∀ 1≤t≤T .
t t 1:t−1 t 1:t−1
Proof. (Necessary)OptimalTwistedSMC =⇒ (i),(ii): Webeginbywritingthemarginalizationoftheunnormalized
densityπ˜∗ overprefixesoflengthtas
t+c
π˜∗ (s )= (cid:88) π˜∗ (s )= (cid:88) p (s )ψ (s )=p (s ) (cid:88) p (s |s )ψ (s )
t+c 1:t t+c 1:t+c 0 1:t+c t+c 1:t+c 0 1:t 0 t+1:t+c 1:t t+c 1:t+c
st+1:t+c st+1:t+c st+1:t+c
The normalization constant of π˜∗ (s ) can easily be seen to be
Zψ∗
after summing over s above, which
t+c 1:t t+c 1:t
yields π∗ (s ) = π˜∗ (s
)/Zψ∗
. We now factorize the c-step incremental importance weights (at step t + 1,
t+c 1:t t+c 1:t t+c
see Eq. (c-Step SMC Weights)) using the above identities, which imply that π˜∗ (s ) =
Zψ∗
π∗ (s ) =
t+c 1:t+c t+c t+c 1:t+c
Zψ∗
π∗ (s )π∗ (s |s )and
t+c t+c 1:t t+c t+1:t+c 1:t
π˜∗ (s )
Zψ∗
π∗ (s )π∗ (s |s )
w (s )= t+c 1:t+c = t+c t+c 1:t t+c t+1:t+c 1:t (27)
t+1:t+c 1:t+c π˜ t∗(s 1:t)q∗(s t+1:t+c|s 1:t) Z tψ∗ π t∗(s 1:t) q∗(s t+1:t+c|s 1:t)
Inordertohavew (s
)=Zψ∗ /Zψ∗
ingeneral,wethusrequireπ∗ (s )=π∗(s )andπ∗ (s |s )=
t+1:t+c 1:t+c t+c t t+c 1:t t 1:t t+c t+1:t+c 1:t
q∗(s |s )foralltandc≤T −t.
t+1:t+c 1:t
(Sufficient)(i),(ii) =⇒ OptimalTwistedSMC:Considertheincrementalimportanceweightsusing(i)and(ii),
π˜∗(s ) Zψσ(s ) Zψ
w (s )= t 1:t = t 1:t = t (28)
t 1:t π˜ t∗ −1(s 1:t−1)q tπ∗(s t|s 1:t−1) Z tψ −1σ(s 1:t−1)σ(s t|s 1:t−1) Z tψ
−1
whichmatchestheoptimalitydefinitioninDef.A.1.
PropositionA.4(OptimalIntermediateTargetDistributions). Foragiventargetdistributionσ(s )(Eq.(31)),the
1:T
followingconditionsareequivalent,andarenecessaryforoptimalityofatwistedSMCprocedureinvolving{π∗}T ,
t t=1
(cid:88)
(i): π∗(s )= π∗ (s ) ∀ 1≤t≤T −1,
t 1:t t+1 1:t+1
st+1
(cid:88)
(ii): π∗(s )= π∗ (s ) ∀ 1≤t≤T −1, 1≤c≤T −t, (29)
t 1:t t+c 1:t+c
st+1:t+c
(iii): π∗(s )=σ(s ) ∀ 1≤t≤T .
t 1:t 1:t
16ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Conditions(i)and(iii)directlycorrespondtotherecursionsfortheoptimaltwistfunctionsgiveninProp.3.2andProp.B.1.
Proof. (i) ⇐⇒ (ii): Itisclearthat(ii) =⇒ (i)asaspecialcaseforc=1. Toshow(i) =⇒ (ii),wehave
π∗(s )= (cid:88) π∗ (s )= (cid:88) (cid:88) π∗ (s )=...= (cid:88) π∗ (s ).
t 1:t t+1 1:t+1 t+2 1:t+2 t+c 1:t+c
st+1 st+1st+2 st+1:t+c
(i) =⇒ (iii):Recursivelyapplying(i)untiltimeT suggeststhat
π∗(s )= (cid:88) π∗ (s )= (cid:88) (cid:88) π∗ (s )=...= (cid:88) π∗(s )= (cid:88) σ(s )=σ(s ).
t 1:t t+1 1:t+1 t+2 1:t+2 T 1:T 1:T 1:t
st+1 st+1st+2 st+1:T st+1:T
(iii) =⇒ (i):Thetargetmarginalsclearlysatisfytherecursion
(cid:88) (cid:88) (cid:88) (cid:88)
σ(s ):= σ(s )= σ(s )= σ(s ).
1:t 1:T 1:T 1:t+1
st+1:T st+1st+2:T st+1
A.2.ProofofTwist-InducedProposal
Proposition3.3. (Twist-InducedProposal). Foragivensetofintermediatetwistedtargetsπ (s )inEq.(9),theproposal
t 1:t
whichminimizesthevarianceoftheone-stepincrementalimportanceweightsw isgivenby
t
π (s )
qπ(s |s )∝ t 1:t (14)
t t 1:t−1 π (s )
t−1 1:t−1
1
= p (s |s )ψ (s ).
Zπ(s ) 0 t 1:t−1 t 1:t
t 1:t−1
Proof. We seek to minimize the variance of the resulting importance weights, subject to a constraint on the proposal
probabilitiessummingto1. IntroducingaLagrangemultiplierλ(s ),wehave
1:t−1
min E
(cid:20)(cid:16)
π˜t(s1:t)
(cid:17)2(cid:21) −(cid:16)
E
(cid:104)(cid:16)
π˜t(s1:t)
(cid:17)(cid:105)(cid:17)2
+λ(s
)(cid:16)
(cid:80) q(s |s
)−1(cid:17)
q(st|s1:t−1) q(st|s1:t−1) π˜t−1(s1:t−1)q(st|s1:t−1) q(st|s1:t−1) π˜t−1(s1:t−1)q(st|s1:t−1) 1:t−1 st t 1:t−1
Taking δ (·)=0implies
δq
(cid:18)
π˜ (s )
(cid:19)2 (cid:18)
π˜ (s )
(cid:19)
π˜ (s )
0= t 1:t −2q(s |s ) t 1:t t 1:t +λ(s )
π˜ (s )q(s |s ) t 1:t−1 π˜ (s )q(s |s ) π˜ (s )q(s |s )2 1:t−1
t−1 1:t−1 t 1:t−1 t−1 1:t−1 t 1:t−1 t−1 1:t−1 t 1:t−1
wherethederivativeinthesecondtermiszerosincetheq(s |s )cancel. Finally,wehave
t 1:t−1
π˜ (s )2
t 1:t =λ(s )
π˜ (s )2q(s |s )2 1:t−1
t−1 1:t−1 t 1:t−1
1 π˜ (s ) 1
q∗(s |s )= t 1:t = p (s |s )ψ (s )
t 1:t−1 (cid:112) λ(s 1:t−1)π˜ t−1(s 1:t−1) Z tπ(s 1:t−1) 0 t 1:t−1 t 1:t
whereZπ(s )(orλ)ischosentoenforcenormalization.
t 1:t−1
Wefocusedontheone-steptwist-inducedproposalinProp.3.3. However,thisproposalisnotoptimalforresamplingevery
csteps(aswouldalsooccur,forexample,withadaptiveresampling).
PropositionA.5(Multi-StepTwistInducedProposal(GeneralizationofProp.3.3)). Forresamplingc-stepsahead,the
optimalproposal(overs )whichminimizesthevarianceoftheimportanceweightsw (s )isgivenby
t+1:t+c−1 t:t+c−1 1:t+c−1
p (s |s )ψ (s )
qπ(s |s )= 0 t:t+c−1 1:t−1 t+c−1 1:t+c−1 .
t:t+c−1 1:t−1 (cid:80) p (s |s )ψ (s )
0 t:t+c−1 1:t−1 t+c−1 1:t+c−1
st:t+c−1
TheprooffollowsthesamereasoningasintheproofofProp.3.3above,usingthemultistepweightsw (s )=
t:t+c−1 1:t+c−1
π˜t+c−1(s1:t+c−1) fromEq.(c-StepSMCWeights).
π˜t−1(s1:t−1)q(st:t+c−1|s1:t−1)
Notethatthedenominatorisnotusuallytractableforc>1inlanguagemodelingapplications.
17ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
A.3.DerivationofCTLGradient
LemmaA.6(DerivationofCTLGradient). FortheCTLlossminL CTL(θ):=min(cid:80)T t=1D KL(cid:0) σ(s 1:t)(cid:13) (cid:13)π tθ(s 1:t)(cid:1) ,the
θ θ
(negative)gradientwithrespecttotheparametersθisgivenby
−∇ L
(θ)=(cid:88)T
E
(cid:104)
∇ logψθ(s
)(cid:105)
−E
(cid:104)
∇ logψθ(s
)(cid:105)
(30)
θ CTL σ(s1:t) θ t 1:t πtθ(s1:t) θ t 1:t
t=1
Proof. Considerexpandingtheformofπθ(s )usingEq.(9),notingthatthenormalizationlogZψ isindependentofs .
t 1:t t 1:t
Takingthegradientwithrespecttoθusingthelogderivativeidentity∇ f(θ)=f(θ)∇ logf(θ),wehave
θ θ
−∇ L (θ)=−∇ (cid:32) (cid:88)T E (cid:104) logσ(s )−logp (s )−logψθ(s )(cid:105) +log(cid:88) p (s )ψθ(s )(cid:33)
θ CTL θ σ(s1:t) 1:t 0 1:t t 1:t 0 1:t t 1:t
t=1 s1:t
=(cid:88)T E (cid:104) ∇ logψθ(s )(cid:105) −(cid:88)T (cid:88) p 0(s 1:t)ψ tθ(s 1:t) ∇ (cid:16) logp (s )+logψθ(s )(cid:17)
σ(s1:t) θ t 1:t (cid:80) p (s )ψθ(s ) θ 0 1:t t 1:t
t=1 t=1s1:t s1:t 0 1:t t 1:t
=(cid:88)T (cid:16)
E
(cid:104)
∇ logψθ(s
)(cid:105)
−E
(cid:104)
∇ logψθ(s
)(cid:105)(cid:17)
σ(s1:t) θ t 1:t πtθ(s1:t) θ t 1:t
t=1
B.SMCwithIntermediatePotentialsandConnectionwithSoftReinforcementLearning
Inthemaintext,wefocusedonsettingswherethetargetdistributionisdefinedbyapotentialϕ(s )dependingonfull
1:T
sequencesonly,asinEq.(1). Thissettinghighlightstheneedfor(learned)twistfunctionstosummarizethefutureexpected
valueofthepotentialintheabsenceofintermediatetargetinformation.
Inthisappendix,wegeneralizeourexpositiontoshowhowourtwistedSMCframeworkcanaccommodatesettingswith
intermediatepotentials,whichisevocativeofconnectionswithsoftreinforcementlearning(Levine,2018). Weleverage
(sRL)
intuitionfromsoftRLwhileintroducingourgeneralprobabilisticinterpretation,byusing = toinstantiatethesoftRL
specialcase. Inparticular,softRLwillcorrespondtotheterminalpotential
ϕ (s )(s =RL) eβrt(s1:t) (softRLϕ Definition)
t 1:t t
whichcorrespondstoϕ(s 1:T)=eβrT(s1:T)ifthepotentialisgivenatthefinalsteponly(asinRLHF,Korbaketal.(2022b)).
However,wedeferdetaileddiscussionofsoftRLtoApp.B.3. SeeTable5forseveralexamplesofintermediatepotentials.
Finally,weformalizeanotionofconditionaltargetdistributionsandtwistfunctionsinApp.B.2,whichgeneralizesthe
expositioninthemaintextandcapturesourconditionaltwistlearningexperimentsinSec.7.2.3.
B.1.TwistedSMCwithIntermediatePotentials
Togeneralizetheexpositioninthemaintext,wemightconsiderdefiningthetargetas
(cid:32) T (cid:33)
σ(s 1:T):= Z1 p 0(s 1:T) (cid:89) ϕ t(s 1:t) (s =RL) Z1 p 0(s 1:T)eβ(cid:80)T t=1rt(s1:t) (31)
σ σ
t=1
whereEq.(1)andthemaintextexpositioncorrespondstoϕ (s )=1fort<T.
t 1:t
(cid:80)
OptimalTwistswithIntermediatePotentials UsingEq.(31),themarginaldistributionσ(s )= σ(s )over
1:t st+1:T 1:T
ttokensbecomes
(cid:32) t (cid:33) T 
1 (cid:89) (cid:88) (cid:89)
σ(s 1:t)=
Z
p 0(s 1:t) ϕ τ(s 1:τ)  p 0(s t+1:T|s 1:t) ϕ τ(s 1:τ) (32)
σ
τ=1 st+1:T τ=t+1
 
(sRL) 1 β
(cid:80)t
rτ(s1:τ) (cid:88) β
(cid:80)T
rτ(s1:τ)
=
Z
p 0(s 1:t)e τ=1  p 0(s t+1:T|s 1:t)e τ=t+1  (softRLspecialcase)
σ
st+1:T
18ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
AsinProp.3.2,thegoaloftheoptimaltwistfunctionsistofacilitatesamplingfromtheintermediatemarginalsσ(s )of
1:t
thetargetdistributionσ(s ).
1:T
Weconsidertwodifferentquantitiesinvolvedindefiningtheoptimaltwists,whichdifferintheirtreatmentoftheintermediate
reward. ForthesoftRLsetting,thiscorrespondstothenaturaldistinctionbetweenQ-valuesand(soft)valuefunctionsV .
t
(cid:32)t−1 (cid:33) T
1 (cid:89) (cid:16) (cid:88) (cid:89) (cid:17)
σ(s )= p (s ) ϕ (s ) ϕ (s ) p (s |s ) ϕ (s )
1:t Z 0 1:t τ 1:τ t 1:t 0 t+1:T 1:t τ 1:τ
σ
τ=1 st+1:T τ=t+1
(cid:124) (cid:123)(cid:122) (cid:125)
Φ∗ t(s1:t):∝
(cid:124) (cid:123)(cid:122) (cid:125)
ψ t∗(s1:t):∝
(33)
 
(s =RL) Z1
p 0(s
1:t)eβ
τt(cid:80)− =1
1rτ(s1:τ) eβrt(s1:t)(cid:16) (cid:88)
p 0(s t+1:T|s
1:t)eβ
τ=(cid:80)T
t+1rτ(s1:τ)(cid:17)
σ
st+1:T
(cid:124) (cid:123)(cid:122) (cid:125)
Φ∗ t(s1:t):∝eβVt∗(s1:t)=
(cid:124) (cid:123)(cid:122) (cid:125)
ψ
t∗(s1:t):∝eβrt(s1:t)+βVt∗(s1:t)=
where:∝means‘definedtobeproportionalto’andQ∗(s ,s )=r (s )+V∗(s )inRLnotation. SeeApp.B.3for
t t 1:t−1 t 1:t t 1:t
detailedderivationsinthesoftRLspecialcase. Ingeneral,Φ capturestheexpectationoffuturepotentialsfromt+1:T,
t
analogoustothe(soft)valuefunction. Thetwistsψ playaroleanalogoustoaQ-value,estimatingboththeimmediateϕ
t t
andfuturevalueΦ . Inparticular,
t
T
(cid:88) (cid:89)
ψ∗(s )∝ϕ (s )Φ∗(s ) where Φ∗(s ):∝ p (s |s ) ϕ (s ) (34)
t 1:t t 1:t t 1:t t 1:t 0 t+1:T 1:t τ 1:τ
st+1:T τ=t+1
Wecontinuetorefertoψ asthetwistfunctionsandfocusonprobabilisticinterpretationsbasedonψ insteadofΦ∗(see
t t t
App.B.4foradditionaldiscussion).
Toshowthatthisnotationisconsistentwiththemaintext,considertheoptimaltwistsψ∗(s )=ϕ (s )Φ∗(s )withno
t 1:t t 1:t t 1:t
intermediatepotentials,ϕ (s ) = 1fort < T. Fort < T,ψ∗(s ) = Φ∗(s )reflectthefutureexpectedpotentialand
t 1:t t 1:t t 1:t
fort=T,theterminalpotentialisψ∗(s )=ϕ (s ),withnofuturepotentialsafterstepT,i.e. Φ =1.
T 1:T T 1:T T
BuildingonEq.(32)-(33)above,thefollowinggeneralizationofProp.3.2definesthe‘optimal’twistssoastoobtainthe
intermediatetargetmarginalsσ(s )(seeProp.A.4).
1:t
PropositionB.1(OptimalTwists). Foragiventargetdistributionσ(s )inEq.(31),theoptimaltwistfunctionsyield
1:T
intermediate{π }T−1whichmatchthetargetmarginals. Inregionswherep (s )>0,theoptimaltwistsaregivenby
t t=1 0 1:t
(cid:32)t−1 (cid:33) (cid:32)t−1 (cid:33)
1 (cid:89) 1 (cid:89)
π∗(s )=σ(s )= p (s ) ϕ (s ) ψ∗(s ) = p (s ) ϕ (s ) ϕ (s )Φ∗(s ). (35)
t 1:t 1:t Zψ∗ 0 1:t τ 1:τ t 1:t ZΦ∗ 0 1:t τ 1:τ t 1:t t 1:t
t τ=1 t τ=1
Uptoaconstantc independentofs ,theoptimaltwistsψ∗aregivenby
t 1:t t
T
(cid:88) (cid:89)
ψ∗(s )=c ϕ (s ) p (s |s ) ϕ (s ) (36)
t 1:t t t 1:t 0 t+1:T 1:t τ 1:τ
st+1:T τ=t+1
ψ∗
wherec isabsorbedintothenormalizationconstantZ . Theoptimaltwistssatisfytherecursion
t t
ψ∗
Z (cid:88)
ψ∗(s )= t ϕ (s ) p (s |s )ψ∗ (s ). (37)
t 1:t ψ∗ t 1:t 0 t+1 1:t t+1 1:t+1
Z
t+1 st+1
RemarkB.2(EquivalenceClassofψ andΦ ). Notethatanyrescalingofψ ←c ψ¯ byaconstantwithrespecttos
t t t t t 1:t
willyieldthesameintermediatemarginalsπ (s ),duetothenormalizationconstantZψ whichscaleswithψ . Thisdefines
t 1:t t t
anequivalentclassinthespaceoffunctions. ThesamestatementholdsforΦ . WeexpressresultssuchasEq.(36)using
t
proportionality∝. Wedefineψ andΦ asthemembersoftheirequivalentclasseswhosenormalizationZψ andZΦ are
t t t t
equal. Thus,wehaveψ (s )=ϕ (s )Φ (s ).
t 1:t t 1:t t 1:t
19ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Proof. SubstitutingEq.(36)intoEq.(35),weobtainthedesiredmarginalEq.(32),
 
t T
π t∗(s 1:t)= Zc ψt
∗
p 0(s 1:t) (cid:89) ϕ τ(s 1:τ) (cid:88) p 0(s t+1:T|s 1:t) (cid:89) ϕ τ(s 1:τ)=σ(s 1:t)
t τ=1 st+1:T τ=t+1
wherethefinalequalityfollowsfromabsorbingtheconstantc tintoZ tψ∗ ,with Z1
σ
= Zc ψt ∗ andZ σ whichnormalizesσ˜(s 1:t).
t
Wewillnowusec
t
= Z Ztψ σ∗ toshowtherecursioninEq.(37). NotethatEq.(36)implies
ψ∗(s )=c ϕ (s ) (cid:88) p (s |s )(cid:16) ϕ (s ) (cid:88) p (s |s ) (cid:89)T ϕ (s )(cid:17)
t 1:t t t 1:t 0 t+1 1:t t+1 1:t+1 0 t+2:T 1:t+1 τ 1:τ
st+1 st+2:T τ=t+2
(cid:124) (cid:123)(cid:122) (cid:125)
ct1 +1ψ t∗ +1(s1:t+1)
= Z
tψ∗
ϕ (s )(cid:88) p (s |s )ψ∗ (s )
Zψ∗ t 1:t 0 t+1 1:t t+1 1:t+1
t+1 st+1
wherethesecondlinefollowsfrom ct = Z
tψ∗
/Zσ. ThisdemonstratesEq.(37).
ct+1 Z tψ +∗ 1/Zσ
Thisleadstothefollowingdefinitionoftheintermediatetwistingtargets(wedeferthesoftRLspecialcasetoApp.B.3).
Definition B.3 (Twisted Intermediate Targets ). Using approximate twist functions {ψ }T−1, we define the twisted
t t=1
intermediatetargetdistributions
 (cid:32)t−1 (cid:33)
Z1
ψ
p 0(s 1:t)
(cid:89)
ϕ τ(s 1:τ) ψ t(s 1:t) (t<T) (TwistTargets(ψ))
t τ=1
π (s )=
t 1:t
Z1
σp 0(s
1:T)(cid:89)T
ϕ t(s 1:t) (t=T)
t=1
One-StepTwist-InducedProposal UsingProp.3.3andDef.B.3andnotingthatϕ (s )isindependentofs ,we
t−1 1:t−1 t
havetheoptimalone-stepproposal
π (s )
Zψ
ϕ (s )ψ (s )
qπ(s |s )∝ t 1:t = t−1p (s |s ) t−1 1:t−1 t 1:t
t t 1:t−1 π t−1(s 1:t−1) Z tψ 0 t 1:t−1 ψ t−1(s 1:t−1)
1
=: Zπ(s )p 0(s t|s 1:t−1)ψ t(s 1:t) (Twist-InducedProposal(ψ))
t 1:t−1
p (s |s )ψ (s )
= 0 t 1:t−1 t 1:t
(cid:80)
p (s |s )ψ (s )
0 t 1:t−1 t 1:t
st
whereinthesecondline,weabsorbtermswhichdependonlyons (andnots )intothenormalization. InthesoftRL
1:t−1 t
specialcase,wehaveq tπ(s t|s 1:t−1)∝p 0(s t|s 1:t−1)eβQt(st,s1:t−1)(seeEq.(Twist-InducedProposal(softRL))below).
B.2.ConditionalTwistedSMC
Toformalizeournotionofconditionaltwistsintheinfillingexperiments(Sec.7.2.3),wegeneralizeouraboveframework
toexplicitlydependon‘observation’randomvariables{o }T . ThismatchesthecommonsettingofSMCinstate-space
t t=1
models(Briersetal.,2010;Guetal.,2015;Lawsonetal.,2022;Chopinetal.,2020). Ourderivationsinthissectionalso
emphasizethattheoptimaltwistfunctionsinProp.B.1learnfunctionsproportionaltoconditionallikelihoodsofthefuture
observationvariablesgiventhecurrentsequence(seeEq.(40)below)). Werecovertheunconditionaltargetsinthemaintext
forfixedo =1.
T
20ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Consideratargetdistributionσ(s |o )conditionedonparticularobservationrandomvariableso :={o }T . We
1:T 1:T 1:T t t=1
defineaprobabilisticmodeloverobservationsσ(o |s ) = ϕ (o ,s ) astheintermediatepotential,8 whichyieldsthe
t 1:t t t 1:t
targetposterior
(cid:18) T (cid:19)
(cid:81)
σ(s 1:T|o 1:T)=
(cid:80)
p 0(s 1:T (cid:18)) (cid:81)Tt=1σ(o t|s 1:t (cid:19))
=
Z
σ(o1
1:T)p 0(s
1:T)(cid:32) (cid:89)T
ϕ t(o t,s
1:t)(cid:33)
=
p 0(s
1:T
σ) (σ o( 1o
:T1:
)T|s 1:T)
(38)
p (s ) σ(o |s ) t=1
0 1:T t 1:t
s1:T t=1
whereweinterpretσ(o |s
)=(cid:81)T
σ(o |s )andZ (o )=σ(s )tomaketheBayesianposteriorexplicitinthe
1:T 1:T t=1 t 1:t σ 1:T 1:T
lastequality. Note,wenowseektoestimateadifferentpartitionfunctionZ (o )foreachsetofobservationvariables.
σ 1:T
UsingourinfillingexperimentsinSec.7.2.3asanexample,consider(asequenceof)subsequenttokenso =s as
T T+1:T+c
observationvariables,wheretheobservationmodelissimplythebaselanguagemodelσ(o |s ):=p (s |s ).
T 1:T 0 T+1:T+c 1:T
UsingEq.(38),theintermediatemarginalsbecome
(cid:88)
σ(s |o )= σ(s |o )
1:t 1:T 1:T 1:T
st+1:T
T
(cid:88) 1 (cid:16)(cid:89) (cid:17)
= p (s )p (s |s ) σ(o |s )
σ(o ) 0 1:t 0 t+1:T 1:t t 1:t
1:T
st+1:T t=1
(cid:32) t (cid:33) T
1 (cid:89) (cid:88) (cid:16) (cid:89) (cid:17)
= p (s ) ϕ (o ,s ) p (s |s ) ϕ (o ,s )
Z (o ) 0 1:t τ τ 1:τ 0 t+1:T 1:t τ τ 1:τ
σ 1:T
τ=1 st+1:T τ=t+1
(cid:32) t (cid:33)
1 (cid:89)
= p (s ) ϕ (o ,s ) σ(o |s ), (39)
Z (o ) 0 1:t τ τ 1:τ t+1:T 1:t
σ 1:T
τ=1
(cid:80)
notingthatσ(o |s )= σ(o ,s |s )matchesthesecondtolastline.
t+1:T 1:t st+1:T t+1:T t+1:T 1:t
The optimal twists take a similar form as Prop. B.1, but now as a function of the future observation or conditioning
information. Further, the optimal twists is proportional to the conditional likelihoods (e.g., σ(o |s )) of future
t+1:T 1:t
observationsgivens ,whichmarginalizeoverfuturetokens(e.g.,s ),
1:t t+1:T
T
Φ∗(s ,o
)ot+∝1:T
σ(o |s )=
(cid:88)
p (s |s
)(cid:16) (cid:89)
ϕ (o ,s
)(cid:17)
,
t 1:t t+1:T t+1:T 1:t 0 t+1:T 1:t τ τ 1:τ
st+1:T τ=t+1
(40)
T
ψ∗(s ,o )o ∝t:T σ(o |s )= (cid:88) p (s |s )(cid:16)(cid:89) ϕ (o ,s )(cid:17) ,
t 1:t t:T t:T 1:t 0 t+1:T 1:t τ τ 1:τ
st+1:T τ=t
o
wheref(x,o)∝g(x,o)denotesproportionalityuptoaconstantwhichdependsonoonly: ∃c(o): f(x,o)=c(o)g(x,o).
TheseequationscanbeconfirmedbycomparingProp.B.1withthelasttwolinesinEq.(39).
Theintermediatemarginalsoverpartialsequencescanfinallyberewrittenaseither
(cid:32) t (cid:33)
σ(s |o )o ∝1:T p (s ) (cid:89) ϕ (o ,s ) Φ∗(s ,o ),
1:t 1:T 0 1:t τ τ 1:τ t 1:t t+1:T
τ=1
(41)
(cid:32)t−1 (cid:33)
(cid:89)
=p (s ) ϕ (o ,s ) ψ∗(s ,o ).
0 1:t τ τ 1:τ t 1:t t:T
t=1
Wediscussthechoiceofparameterizationusingψ versusΦ inApp.B.4.
t t
TheconditionaltwistlearningformulationmatchesthesettingofLawsonetal.(2022),towhichwereferthereaderfor
additionaldiscussion. WeusethisconditionalperspectivetoderiveclassificationlossesfortwistlearninginApp.C.3-C.4.
8Note,rescalingϕ (s ,o =1)byaconstantcwithrespecttoo ,s doesnotaffectthetargetposteriorinEq.(38).Forexample,
t 1:t t t 1:t
withterminalpotentialonly:σ(s |o )= p0(s1:T)ϕT(s1:T,oT)/c = 1 p (s )ϕ (s ,o )aslongasthescalingfactor
1:T T (cid:80) s1:T p0(s1:T)ϕT(s1:T,oT)/c Zσ(oT) 0 1:T T 1:T T
isindependentofo ands .
T 1:T
21ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
UnconditionalTargetsasaSpecialCase Incaseswhereweareonlylearningtwistsforasinglesetofconditioning
informationsuchasasingleclassifierlabelorarewardmodel,notethatwecanomitexplicitconditioninginformationin
ψ (s ,o )andconsidersetting{o =1}T . Withterminalpotentialonlyasinthemaintext,wewriteσ(o =1|s )=
t 1:t t t t=1 T 1:T
ϕ(s )andtheoveralltargetdistributionasσ(s ) = σ(s |o = 1) ∝ p (s )ϕ (s ). Thus,theformulationin
1:T 1:T 1:T T 0 1:T T 1:T
Eq.(38)-Eq.(40)strictlygeneralizesourexpositioninthemaintextandApp.B.1. Withintermediatepotentials,weset
σ(o =1|s
)=(cid:81)T
ϕ (s ).
1:T 1:T t=1 t 1:t
OurnotationalsomatchestheexpositioninLevine(2018)forthesoftRLcasewithabinaryobservationor‘optimality’
randomvariableσ(o
t
=1|s 1:t−1,s t)=eβrt(s1:t−1,st),wheretherewardisafunctionofthestatex
t
=s
1:t−1
andaction
a =s pair(seetheMDPinterpretationinApp.B.3).
t t
B.3.ConnectionwithSoftReinforcementLearning
Inthissection,wemoreexplicitlydescribethesoftreinforcementlearningsetting(Levine,2018)commonlyusedinRLHF
(sRL)
(Korbaketal.,2022b)asaspecialcaseofourprobabilisticframework. Again,weusenotation = toindicatethatthe
expressionsinthissectioncorrespondtoaparticularinstanceofourSMCframeworkwhereϕ(s 1:T)=eβr(s1:T).
SummaryofSoftRLNotation Tosummarizethebelowderivations,westatetherelevantassignmentsforthesoftRL
case. Wefocusontheoptimalcaseforsimplicity,butnotethatapproximateversionsplayidenticalroles,
ϕ t(s 1:t)=eβrt(s1:t) ψ t∗(s 1:t)=eβrt(s1:t)+βV t∗(s1:t) =eβQ∗ t(st,s1:t−1) Φ∗ t(s 1:t)=eβV t∗(s1:t) (TwisttoSoftRL)
whereψ∗(s )=ϕ (s )Φ∗(s )orQ∗(s ,s )=r (s )+V∗(s ). Intheotherdirection,wehave
t 1:t t 1:t t 1:t t t 1:t−1 t 1:t t 1:t
1 1 1
r (s )= logϕ (s ) Q∗(s ,s )= logψ∗(s ) V∗(s )= logΦ∗(s ) (SoftRLtoTwist)
t 1:t β t 1:t t t 1:t−1 β t 1:t t 1:t β t 1:t
MDP Interpretation To draw connections with soft RL, we view language model controlled decoding as a MDP,
where the prompt is drawn from an initial state distribution s ∼ ν , an action policy π(a |x ) = q(s |s ) selects
0 0 t t t 1:t−1
the next token a = s given a partial sequence x = s as the state, and deterministic environment transitions
t t t 1:t−1
P(x = s |a = s ,x = s ) = δ(x = concat(s ,s )) append the selected token to update the state.
t+1 1:t t t t 1:t−1 t t 1:t−1
Discountingmayalsobeincludedwithoutdifficulty. Therewardisgivenbyr (s ).
t 1:t
FinalTargetDistribution Wedefinethetargetdistributionasthesolutiontothefollowingvariationaloptimizationwhich
solvestheregularizedMDPdescribedabove,
σ(s 1:T)(s =RL) Z1 σp 0(s 1:T)eβ t(cid:80) =T 1rt(s1:t) =a qr (g s1m :Ta )xE q(s1:T)(cid:104)(cid:88) t=T 1rt(s1:t)(cid:105) − β1 D KL(q(s1:T)∥p0(s1:T)) (42)
whichcorrespondstothechoiceϕ t(s 1:t)=eβrt(s1:t)asinEq.(TwisttoSoftRL).Thesoftvalueisdefinedasthemaximum
valueoftheaboveoptimizationforoptimalq∗(s ),andcorrespondstothescaledlogpartitionfunction
1:T
V 0∗(s 0):= β1 logZ σ = β1 log s(cid:88)
1:T
p 0(s 1:T)eβ t(cid:80) =T 1rt(s1:t) = q(m s1a :Tx )E q(s1:T)(cid:104)(cid:88) t=T 1rt(s1:t)(cid:105) − β1 D KL(q(s1:T)∥p0(s1:T)) (43)
whichcanbeconfirmedbysubstitutingq(s )=σ(s )fromEq.(42)intothemaximizationontherightsideofEq.(43).
1:T 1:T
AlthoughweomitthedependenceofZ (s )ontheprompts fornotationalsimplicity(seeEq.(1)),notethatV∗ :=V∗(s )
σ 0 0 0 0
naturallycorrespondstothesoftvalueofthepromptastheinitialstateintheMDP.
22ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
OptimalIntermediateMarginalsandSoftValue DecomposingthemaximizationinEq.(43)intooptimizationsover
eachq(s |s ),wedefinetheintermediatesoftvalueV∗(s )asthemaximumoftheexpectedfutureregularizedreward
t+1 1:t t 1:t
V t∗(s 1:t)= β1 logΦ∗ t(s 1:t)(s =RL) β1 log (cid:88) p 0(s t+1:T|s 1:t)eβ
τ=(cid:80)T
t+1rτ(s1:τ) (OptimalIntermediateSoftValue)
st+1:T
= q(st+m 1:a Tx
|s1:t)
E
q(st+1:T|s1:t)(cid:104) τ=(cid:88)T t+1rτ(s1:τ)(cid:105)
−
β1
D KL(q(st+1:T|s1:t)∥p0(st+1:T|s1:t))
(cid:104) (cid:105) 1
= q(stm +1a |x s1:t)E
q(st+1|s1:t)
rt+1(s1:t+1)+V t∗ +1(s1:t+1) − βD KL(q(st+1|s1:t)∥p0(st+1|s1:t))
where,inthethirdline,weisolatetheoptimizationoverq(s |s )by(i)assumingoptimalityatτ <tand(ii)substituting
t 1:t−1
the optimal value V∗ (s ) = max [...] of the maximization over q(s |s ) (i.e. recursively
t+1 1:t+1 q(st+2:T|s1:t+1) t+2:T 1:t+1
applyingthesecondline).
TheoptimalintermediatemarginalcanbewrittenusingeitherV∗(s )orQ∗(s ,s )form(asinEq.(33)above,orby
t 1:t t t 1:t−1
substitutingtheoptimalV∗orQ∗intothetwisttargetsbelow).
t t
TwistedIntermediateTargets WestatetheapproximatetwistingtargetsforbothV orQ parameterizationsinorderto
t t
makeconnectionswithsoftRLlossesinApp.C.ForapproximateV (s )orQ (s ,s ),wehave
t 1:t t t 1:t−1
π t(s 1:t)(s =RL) Z1
V
p 0(s 1:t)eβ
τt(cid:80)− =1
1rτ(s1:τ) eβrt(s1:t)+βVt(s1:t) (t<T) (TwistTargets(SoftRLV))
t
=
Z1
Qp 0(s
1:t)eβ
τt(cid:80)− =1
1rτ(s1:τ)
eβQt(st,s1:t−1) (t<T) (TwistTargets(SoftRLQ))
t
wherethefinaltwistingtargetisgivenbyEq.(42)andtheoptimalQ-valuesaredefinedas
Q∗(s ,s )=r (s )+V∗(s ) (44)
t t 1:t−1 t 1:t t 1:t
One-StepProposal Finally,theoptimalone-stepproposal(e.g. inV form)canbederivedeither(i)asthetwist-induced
t
proposalfromEq.(TwistTargets(SoftRLV))andProp.B.1or(ii)asthesolutiontotheone-stepoptimizationinthethird
lineofEq.(OptimalIntermediateSoftValue). AsinEq.(Twist-InducedProposal(ψ)),
q tπ(s t|s 1:t−1)(s =RL)
(cid:80)
stp p0 0( (s st t| |s s1 1: :t t− −1 1) )e eβ β( (r rt t( (s s1 1: :t t) )+ +V Vt t( (s s1 1: :t t) ))
)
∝p 0(s t|s 1:t−1)eβQt(st,s1:t−1).
(Twist-InducedProposal(softRL))
Wedefinetheone-steplognormalizationconstantinducedbyanapproximateV orQ asV orV ,respectively,
t t Vt Qt
1 (cid:88) 1 (cid:88)
V (s ):= log p (s |s )eβ(rt(s1:t)+Vt(s1:t)) V (s ):= log p (s |s )eβQt(st,s1:t−1)
Vt 1:t−1 β 0 t 1:t−1 Qt 1:t−1 β 0 t 1:t−1
st st
(45)
suchthat,forexample,q tπ(s t|s 1:t−1)=p 0(s t|s 1:t−1)eβQt(st,s1:t−1)−βVQt(s1:t−1).
RLHF Minimizes D (q∥σ) Note that, for a given suboptimal q(s ), the value of the variational optimization in
KL 1:T
Eq.(42)isalowerboundonthe(scaled)logpartitionfunctionV∗ = 1 logZ . SimilarlytothestandardEvidenceLower
0 β σ
Bound,thegapinthislowerboundisgivenbytheKLdivergence
T
1 1 (cid:16) (cid:104)(cid:88) (cid:105) 1 (cid:17)
logZ = D (q(s )∥σ(s ))+ E r (s ) − D (q(s )∥p (s )) (46)
β σ β KL 1:T 1:T q(s1:T) t 1:t β KL 1:T 0 1:T
t=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ELBOgap(≥0) ‘ELBO’:Eq.(42)
Inthissense,weconsidersoftRLorpolicygradientmethodssuchasPPOwhichoptimizeEq.(42)astargetingσ(s )by
1:T
minimizingD (q(s )∥σ(s ))(Korbaketal.,2022b).
KL 1:T 1:T
23ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
B.4. RemarksonParameterization
Whilethetwistingtargets(Eq.(TwistTargets(ψ)))andtwist-inducedproposal(Eq.(Twist-InducedProposal(ψ)))may
equivalently be parameterized using approximate Φ , we focus on the ψ parameterization to match the main text. In
t t
particular,recallthattheoptimaltwistssatisfyψ∗(s )=ϕ (s )Φ∗(s )forallt. Withnointermediatepotential(ϕ =1
t 1:t t 1:t t 1:t t
fort < T),ourapproximatetwistsestimateψ (s ) ≈ Φ∗(s ) ∝ (cid:80) p (s |s )ϕ (s )fort < T. Inthis
t 1:t t 1:t st+1:T 0 t+1:T 1:t T 1:T
section,wedescribehowthepresenceofintermediatepotentialsmayaffectthechoiceoftwistparameterization.
The twist-induced proposal may not be tractable to evaluate at the final timestep, since it may be costly to evaluate
the terminal potentialϕ (s ) for all s ∈ V given a context s (as described in Sec. 3.2). Thus, we learn an
T 1:T T 1:T−1
approximateψ (s )≈ϕ (s )forproposalsampling,whichcanbeeasilyevaluatedover|V|nexttokens. Thefinal
T 1:T T 1:T
π (s )=σ(s )isdefinedusingϕ(s )inordertopreserveunbiasedestimation. However,aftersamplingtheproposal
T 1:T 1:T 1:T
accordingtoψ ,weonlyneedtoevaluateϕ(s )overK fullsequencestocalculatetheimportanceweightsatthefinal
T 1:T
step(Eq.(16)). SeeIntermediatePotentialTractableoverK SequencesOnlyparagraphbelow.
IntermediatePotentialsTractableover|V|Sequences However,insettingswheretheintermediatepotentialsϕ (s )
t 1:t
aretractableto calculatefor alls ∈ V givens (e.g. usingan indicatorfunctionor forwardpass ina transformer
t 1:t−1
architecture,asinTable5),itmaybeusefultouseaΦ parameterizationofthetwisttargetsandtwist-inducedproposal. This
t
allowsustousetheexactimmediatepotentialsϕ (s )alongsideanestimatedΦθ,insteadofanapproximateψθ ≈ϕ Φ∗
t 1:t t t t t
whichestimatesboththeimmediateϕ andfutureexpectedvalueofpotentialsΦ∗. UsingnotationestablishedinEq.(33)
t t
andProp.B.1,thetwistingtargetsinEq.(TwistTargets(ψ))canberewrittenusingaΦθ parameterization
t
(cid:32)t−1 (cid:33)
1 (cid:89)
πθ(s )= p (s ) ϕ (s ) ϕ (s )Φθ(s ) (t<T) (TwistTargets(Φ))
t 1:t Zψ 0 1:t τ 1:τ t 1:t t 1:t
t τ=1
with π (s ) = σ(s ) as before. The twist-induced proposal qπ(s |s ) ∝ p (s |s )ϕ (s )Φθ(s ) and its
T 1:T 1:T t t 1:t−1 0 t 1:t−1 t 1:t t 1:t
normalizationconstantaretractableinthiscase,byevaluatingboththegivenϕ (s )andparameterizedΦθ(s )inasingle
t 1:t t 1:t
forwardpassandnormalizingoverthediscretevocabularyofnexttokens.
IntermediatePotentialsTractableoverK SequencesOnly Incaseswheretheintermediatepotentialsaredifficultto
evaluate,wewouldliketolimitevaluationofϕ (s )toonlyK partialsequences. Inthiscase,parameterizingthetwisted
t 1:t
targetsπ usingψθ orQθ (Eq.(TwistTargets(ψ)),Eq.(TwistTargets(SoftRLQ)))insteadofΦθ orVθ maybepreferable
t t t t t
toensureatractabletwist-inducedproposal. Separateparameterizationsoftheproposal(usingψξ)andtargets(ϕ Φθ)might
t t t
alsobeconsidered.
InthecaseofthefinaltimestepdescribedaboveorinSec.3.2,notethatweusealearnedψξ toparameterizeatractable
T
variationalproposalq (s |s ). Inthiscase, wehavenofuturevalueΦ (s ) = 1andonlyneedtoevaluatethe
T T 1:T−1 T 1:T
terminalpotentialϕ(s )forcalculatingimportanceweightsoverK sequences.
1:T
C.TwistLearningLosses
In this section, we describe various methods for twist learning beyond our proposed contrastive twist learning (CTL)
procedure from Sec. 4. In App. C.1, we first describe several losses from the soft RL literature from a probabilistic
perspective,buildingcloselyonourdevelopmentsinApp.B.1. WethenproceedtodescribeSIXO(Lawsonetal.,2022)and
FUDGE(Yang&Klein,2021)inApp.C.3-C.4.
Weemphasizelossesfoundinrelatedworkorusedasexperimentalbaselinesusingequationtags(e.g. Eq.(SIXO)),where
equationsEq.(RLBaseline),Eq.(SIXO),Eq.(FUDGE)areusedinourexperiments. Weconsidersettingswithintermediate
potentialsinApp.C.1,butfocusonthe(ϕ =1fort<T)settingintheremainderofthesection,asinthemaintext.
t
C.1. SoftQ-Learning(RL)andPathConsistencyLossesfromLogImportanceWeights
FromtheprobabilisticperspectiveoftheSMClogimportanceweights, wecanderiveseverallossesfortwistlearning,
includingsoftQ-learningandpathconsistencylearning(PCL)(Nachumetal.,2017)lossesfromthesoftRLliterature.
Ageneralprincipleforderivinglossfunctionswouldbetominimizethevarianceofthe(log)importanceweightsunder
somesamplingdistributionπ ,whichleadstoconstantimportanceweightsatoptimality. Todrawconnectionswithprevious
s
work,wealsoconsiderminimizingthesquareofthelogweights,whichatoptimality,ensuresthatlogw =0andw =1are
equaltoaparticularconstant. Wewillproceedtoparameterizethetwistfunctionsusingparametersθandconsiderloss
24ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
termswhichminimizethevarianceorsquareofc-steplogweightsattimet,
(cid:20)t+c−1 (cid:21) (cid:20)(cid:18)t+c−1 (cid:19)2(cid:21)
L(t,c) (θ):=Var (cid:88) logw (s ) L(t,c) (θ):=E (cid:88) logw (s ) . (47)
logVar πs τ 1:τ logCons πs τ 1:τ
τ=t τ=t
L(t,c) (θ)indicates‘consistency’inlog-weightspaceforc-step-aheadweightsattimet(seeEq.(c-StepSMCWeights)).
logCons
We will consider various choices of parameterization and proposal in the following subsections. For example, let
L(t,c) (θ;{ψ ,qπ}) denote the log-consistency loss corresponding to twisting targets parameterized by ψθ and the
logCons t t t
twistinducedproposalqπ (note,ournotationfortheone-stepweightsw (s )doesnotmakethesechoicesexplicit).
t t 1:t
Forreference,wederivethelogimportanceweightswithintermediatepotentialsandarbitraryqas
(cid:18)t−1 (cid:19)
(cid:81)
p (s ) ϕ (s ) ψ (s )
0 1:t τ 1:τ t 1:t
π˜ (s )
logw (s )=log t 1:t =log τ=1
t 1:t π˜ (s )q(s |s ) (cid:18)t−2 (cid:19)
t−1 1:t−1 t 1:t−1 p (s ) (cid:81) ϕ (s ) ψ (s )q(s |s )
0 1:t−1 τ 1:τ t−1 1:t−1 t 1:t−1
τ=1
q(s |s )
=⇒ logw (s )=logϕ (s )+logψ (s )−logψ (s )−log t 1:t−1 (48)
t 1:t t−1 1:t−1 t 1:t t−1 1:t−1 p (s |s )
0 t 1:t−1
Variousspecialcasesarisefromchoicesoftwistparameterizationsandproposalsinthefollowingsubsections.
C.1.1.SOFTQ-LEARNINGANDRLBASELINE
Forsingle-steplog-weights,theψ-parameterizationofthetargets(Eq.(TwistTargets(ψ)),Eq.(TwistTargets(SoftRLQ)
)),andthetwist-inducedproposal(Eq.(Twist-InducedProposal(ψ)),Eq.(Twist-InducedProposal(softRL))),wehave
logw t(s
1:t)=logϕt−1(s1:t−1)+logψt(s1:t)−logψt−1(s1:t−1)−(cid:16) (cid:24)log(cid:24)p0(cid:24)(s(cid:24)t|s1(cid:24)
:t−(cid:24)(cid:24)
1) +logψt(s1:t)−log(cid:88) p0(st|s1:t−1)ψt(s1:t)(cid:17)
p0(st|s1:t−1)
st
(cid:88)
=logϕ (s )+log p (s |s )ψ (s )−logψ (s ) (49)
t−1 1:t−1 0 t 1:t−1 t 1:t t−1 1:t−1
st
wherethesecondtermlogZπ(s )=log(cid:80) p (s |s )ψ (s )normalizesthetwist-inducedproposal(Eq.(14)).
t 1:t−1 st 0 t 1:t−1 t 1:t
Minimizingthesumofone-steplogconsistencylosses(i.e. squaredlogweightsinEq.(48))willyieldthefamiliarsoft
Q-learningloss(e.g. Lioutasetal.(2022)Eq. (4)-(5)). AdjustingindexingfromEq.(48)andintroducingastop-gradient
withinlogZπ(s ),wehave
t 1:t−1
T
minL (θ):=min(cid:88) L(t+1,1)(θ;{ψ ,qπ}) (SoftQLearning)
SOFTQ logCons t t
θ θ
t=1
T
=min(cid:88) E (cid:104)(cid:16) logϕ (s )+log(cid:88) p (s |s )sg(cid:0) ψθ (s )(cid:1) −logψθ(s )(cid:17)2(cid:105)
θ
πs(·) t 1:t 0 t+1 1:t t+1 1:t+1 t 1:t
t=1 st+1
(s =RL) m θin(cid:88)T E πs(·)(cid:104)(cid:16) βr t(s 1:t)+log(cid:88) p 0(s t+1|s 1:t)eβsg(cid:0) Qθ t(st+1,s1:t)(cid:1) −βQθ t(s t,s 1:t−1)(cid:17)2(cid:105)
t=1 st+1
In the final line, we rewrite the loss for the soft RL special case, ϕ t(s 1:t) = eβrt(s1:t) using the substitutions in
Eq. (Twist to Soft RL). Note that the log-normalization term is analogous to an induced soft value V (s ) =
Qθ 1:t−1
t
β1 log(cid:80) stp 0(s t|s 1:t−1)eβQθ t(st,s1:t−1),sothateachsquarederrorlosshastheformE[β2(r t+V t−Q t)2]. Hence,werefer
tothislossasSoftQ-learningloss.
Thelog-normalizationterm,whicharisesfromnormalizingthetwist-inducedproposal,isanalogoustothe‘target’valuein
deepQ-learning. Lioutasetal.(2022)considerthesoft-QlearninglosstoSMCsamplinginself-drivingapplicationswhere
interactionwiththeenvironmentisexpensive. Lawsonetal.(2018)adoptasimilarlossfunction(usingaparameterization
ofthevalueVθ)inthesettingofstate-spacemodelswithtractableintermediaterewards.
t
25ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
RLBaselinewithnoIntermediateReward ThesoftQ-learninglossinEq.(SoftQLearning)simplifiesnicelyinthe
caseofnointermediaterewards,asinthemaintext(ϕ (s )=1fort<T andΦ =1).
t 1:t T
Writtenintermsoftwistfunctions,weseparatethetermsatt<T andt=T forpurposesofexposition
T
minL (θ):=min(cid:88) L(t+1,1)(θ;{ψ ,qπ,ϕ =1}) (RLBaseline)
RL logCons t t t
θ θ
t=1
T−1
=min(cid:88) E (cid:104)(cid:16) log(cid:88) p (s |s )sg(cid:0) ψθ (s )(cid:1) −logψθ(s )(cid:17)2(cid:105) +E (cid:104)(cid:16) logϕ(s )−logψθ(s )(cid:17)2(cid:105)
θ
πs(·) 0 t+1 1:t t+1 1:t+1 t 1:t πs(·) 1:T T 1:T
t=1 st+1
Forintermediatetimesteps,notethatEq.(RLBaseline)enforcestherecursionψθ (s )=(cid:80) p (s |s )ψθ(s )
t−1 1:t−1 st 0 t 1:t−1 t 1:t
inEq.(13)ofthemaintext,albeitinlogspace. InApp.C.2below,weconsidertheone-stepsquarederrorlossenforcing
thisrecursiondirectly(withoutlogarithms),i.e. E [(ψθ (s )−(cid:80) p (s |s )ψθ(s ))2],
πs t−1 1:t−1 st 0 t 1:t−1 t 1:t
C.1.2.PATHCONSISTENCYLEARNING(FORTWISTLEARNING)
Usingthevalueparameterizationofthetargets(Φ orV ,seeEq.(TwistTargets(Φ)),Eq.(TwistTargets(SoftRLV))),the
t t
one-steplogconsistencylosswitharbitraryproposalqrecoversthepath-consistencyloss(PCL)fromNachumetal.(2017).
SwitchingtoaΦθparameterizationofthetwistingtargets,wesubstituteψθ(s )=ϕ (s )Φθ(s )intothelogimportance
t t 1:t t 1:t t 1:t
weightsinEq.(48). Thelog-consistencylossbecomes,
T
minL (θ):=min(cid:88) L(t,1) (θ;{Φ ,anyq}) (PCL)
θ
PCL
θ
logCons t
t=1
=min(cid:88)T
E
(cid:34)(cid:18)
logϕ (s )+logΦθ(s )−logΦθ (s )−log
q(s t|s 1:t−1) (cid:19)2(cid:35)
θ πs t 1:t t 1:t t−1 1:t−1 p 0(s t|s 1:t−1)
t=1
(s=RL)min(cid:88)T
E
(cid:34)(cid:18) β(cid:16)
r (s )+Vθ(s )−Vθ (s
)(cid:17)
−log
q(s t|s 1:t−1) (cid:19)2(cid:35)
θ πs t 1:t t 1:t t−1 1:t−1 p 0(s t|s 1:t−1)
t=1
Inparticular,substitutingthesoftRLpotentialtermsfromEq.(TwisttoSoftRL),Eq.(PCL)recoversthepathconsistency
lossfromNachumetal.(2017). NotethatwederivedPCLfromanimportancesamplingperspective,whereasPCLwas
originallyderivedbyenforcingKKTconditionsofthesoftRLproblem.
Wemightalsoconsidermulti-steplossesforvariousc. Minimizingthesquareofthemulti-steplogweightswitharbitraryq
recoversthemulti-stepPCLloss(Nachumetal.,2017),
minL(t,c)(θ):=minL(t,c) (θ;{Φ ,anyq}) (multi-stepPCL)
θ PCL θ logCons t
(cid:32)t+c
t+c
(cid:33)2
=m θinE πs (cid:88) logϕ τ(s 1:τ)+logΦθ t+c(s 1:t+c)−logΦθ t−1(s 1:t−1)−(cid:88) log pq 0( (s sτ τ| |s s1 1:τ :τ− −1 1)
)

τ=t τ=t
(cid:32)t+c−1
t+c
(cid:33)2
=m θinE πs (cid:88) logϕ τ(s 1:τ)+logψ tθ +c(s 1:t+c)−logψ tθ −1(s 1:t−1)−(cid:88) log pq 0( (s sτ τ| |s s1 1:τ :τ− −1 1)
)
 (50)
τ=t−1 τ=t
(cid:32)
t+c t+c
(cid:33)2
(s=RL)m θinE πs β(cid:88) r τ(s 1:τ)+βV tθ +c(s 1:t+c)−βV tθ −1(s 1:t−1)−(cid:88) log pq 0( (s sτ τ| |s s1 1:τ :τ− −1 1)
)

τ=t τ=t
where we write the ψθ parameterization in Eq. (50) explicitly for use in App. D.1. While PCL considers learned a
t
proposalorpolicyqwiththegoalofapproximatingthesolutionofaregularizedMDP,weleavejointlearningofproposals
{qξ(s |s )}T andSMCtargettwists{ψθ(s )}T or{Vθ(s )}T tofuturework.
t 1:t−1 t=1 t 1:t t=1 t 1:t t=1
InApp.E,wedescribeusingPCLtolearntheproposalonly(Guoetal.,2021),withthevaluesV (s )inducedfrom
Qt 1:t
learnedproposaltwistsQξ(s ,s )whichdefine{qξ (s |s )}T−1(insimilarfashiontoEq.(Twist-InducedProposal
t t+1 1:t Qt t+1 1:t t=0
(softRL)),butwithoutreferencetotwistingtargets).
26ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
C.2.ControlledDecodingLossesviaOptimalTwistIdentities(Mudgaletal.,2023)
InProp.B.1(orProp.3.2andEq.(13)inthemaintext),wenotedthattheoptimaltwistssatisfythefollowingrelationships
T
ψ∗(s )=c ϕ (s ) (cid:88) p (s |s ) (cid:89) ϕ (s ) = c t ϕ (s )(cid:88) p (s |s )ψ∗ (s )
t 1:t t t 1:t 0 t+1:T 1:t τ 1:τ c t 1:t 0 t+1 1:t t+1 1:t+1
t+1
st+1:T τ=t+1 st+1
(ϕt==1) c (cid:88) p (s |s )ϕ(s ) (ϕt==1) c t (cid:88) p (s |s )ψ∗ (s ) (51)
t 0 t+1:T 1:t 1:T c 0 t+1 1:t t+1 1:t+1
t+1
st+1:T st+1
Weproceedtodescribetwo‘controlleddecoding’(CD)lossesfromMudgaletal.(2023)asusingasquarederrorlossto
enforcetheoptimalityconditionsinEq.(51),forsettingswithnointermediatepotentials(ϕ (s )=1fort<T). Mudgal
t 1:t
etal.(2023)alsoproposetwowaystousethelearned‘twists’atinferencetime,whichwediscussinrelationtoourproposed
SMCframeworkinApp.D.1.
CD-Q TheCD-QlossfromMudgaletal.(2023)correspondstominimizingtheone-steprecursioninEq.(51)usingthe
expectedsquarederrorundera(possiblyoff-policy)samplingdistributionπ . Assumingnointermediaterewardandan
s
additionalsquarederrorlosstoapproximatetheterminalpotentialψθ(s )≈ϕ(s ),wehave
T 1:T 1:T
T−1
minL
(θ):=min(cid:88)
E
(cid:104)(cid:16)(cid:88)
p (s |s )ψθ (s )−ψθ(s
)(cid:17)2(cid:105)
+E
(cid:104)(cid:0)
ϕ(s )−ψθ(s
)(cid:1)2(cid:105)
θ
CD-Q
θ
πs(·) 0 t+1 1:t t+1 1:t+1 t 1:t πs(·) 1:T T 1:T
t=1 st+1
(CD-Q)
Eq. (CD-Q) enforces the same optimality condition as the Eq. (RL Baseline) loss (i.e. ψθ(s ) =
t 1:t
(cid:80) p (s |s )ψθ (s )), without log scaling of each term inside the squared error. At optimality, we have
st+1 0 t+1 1:t t+1 1:t+1
zero-varianceone-stepimportanceweights(w(s )=1inEq.(10))forthetwist-inducedproposal.
1:t
Atoptimality,weinfactalsohaveψθ(s )= (cid:80) p (s |s )ϕ (s )(asinEq.(51)andtheproofofProp.B.1).
t 1:t 0 t+1:T 1:t T 1:T
st+1:T
CD-FUDGE WhilewemightnaivelyliketoconsiderthelossE (cid:2)(cid:0) ψθ(s )−(cid:80) p (s |s )ϕ(s )(cid:1)2(cid:3) to
πs(·) t 1:t st+1:T 0 t+1:T 1:t 1:T
enforceProp.3.2orEq.(51),notethatmarginalizationovermultiplestepsisnottractableingeneral.
Instead,theCD-FUDGEloss9definedas
(cid:88)T (cid:20) (cid:20)(cid:16) (cid:17)2(cid:21)(cid:21)
minL (θ):=min E E ψθ(s )−ϕ(s ) (CD-FUDGE)
θ
CD-FUDGE
θ
πs(s1:t) p0(st+1:T|s1:t) t 1:t 1:T
t=1
canbeshowntohavethesamegradientasthedesired(butintractable)squarederrorlossabove(Mudgaletal.,2023).
Sincetheminimizeroftheexpectedsquarederror(underp (s |s ))toasinglefunctionψθ(s )(whichisindependent
0 t+1:T 1:t t 1:t
ofs )isgivenbytheconditionalexpectation(Banerjeeetal.,2005),wecanalsoseethatEq.(CD-FUDGE)hasthe
t+1:T
desiredminimumψθ(s )=(cid:80) p (s |s )ϕ(s ). Note,itiscrucialthattheinnerexpectationsamplesrollouts
t 1:t st+1:T 0 t+1:T 1:t 1:T
underthebasemodelp (s |s )toobtainthedesiredconditionalexpectationastheminimizer. Whileitappearsthat
0 t+1:T 1:t
anyprefixsamplingdistributioncanbeused,π =p allowsforlossestobecalculatedatalltinasinglesamplingrun.
s 0
Mudgaletal.(2023)alsoproposetwodecoding-timeusagesforthelearnedtwistfunctionsψθ: stochastictoken-by-token
t
samplingandargmaxdecodingofpartialsequences. WediscusstheirinconsistencieswithourSMCframeworkinApp.D.
CD-FUDGEforlogψθ WecanalsocompareEq.(CD-FUDGE)withthemulti-stepPCLlossinEq.(50),choosingϕ =1
t t
fort<T andtheproposalequaltothebasemodelq =p sothattheproposaltermscancel.Notingthatψ (s )=ϕ(s )
0 T 1:T 1:T
isfixedtotheexactterminalpotentialandchoosingthec=T −t+1-stepPCLlossforeacht,notethatEq.(50)would
reduceto(cid:80) E[(cid:0) logϕ(s )+0−logψθ(s )−0(cid:1)2 ]. Deng&Raffel(2023)optimizethislosswithreweightingofterms
t 1:T t 1:t
basedontimestep(higherweightfort≈T). Eq.(CD-FUDGE)optimizesthesquarederrorofthedifferencewithoutlog
scalingofeachterm,underappropriatesamplingofrollouts. 10
9Note,wereservethenamingconventionFUDGE(Yang&Klein,2021)forabinarycrossentropylossdescribedinApp.C.4,as
opposedtotheCD-FUDGEsquarederrorlossfromMudgaletal.(2023).
10NotethedifferenceinchoiceofproposalbetweenEq.(CD-Q)(twist-inducedq=qπ)andEq.(CD-FUDGE)(baseq=p ).
t 0
27ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
C.3.SIXO:SmoothingInferencewithTwistedObjectives(Lawsonetal.,2022)
Lawson et al. (2022) adopt a noise-contrastive estimation loss (Gutmann & Hyva¨rinen, 2010) to learn the target twist
functions using binary classification. For state space models, Lawson et al. (2022) adopt our setting in App. B.2 with
observationvariableso emittedbasedonthesamplingstates (orsimplys )andaknownlikelihoodϕ (o ,s )=σ(o |s ).
t 1:t t t t t t t
AsdiscussedinApp.B.4,inthesesettingswitheasilyevaluableintermediatepotentials,itmaybepreferabletoparameterize
Φθ(s ,o )asinEq.(TwistTargets(Φ)). Lawsonetal.(2022)indeedusethisparameterization(seetheirEq. 5).
t 1:t t+1:T
RecallfromEq.(39)thattheoptimaltwistsorfuturevaluesamounttoconditionallikelihoods,
Φ∗(s ,o )ot+∝1:T σ(o |s ), ψ∗(s ,o )o ∝t:T σ(o |s ), (52)
t 1:t t+1:T t+1:T 1:t t 1:t t:T t:T 1:t
o
where∝denotesproportionalityuptoaconstantwhichdependsonoonly. UsingBayesrule,wehave
σ(o |s )= σ(s 1:t|o t+1:T)σ(o t+1:T) ot+∝1:T σ(s 1:t|o t+1:T) , σ(o |s )o ∝t:T σ(s 1:t|o t:T) , (53)
t+1:T 1:t p (s ) p (s ) t:T 1:t p (s )
0 1:t 0 1:t 0 1:t
notingthatσ(o )andp (s )aremarginalsofσ(s ,o )bydefinition. Theabovereasoningsuggeststhatwe
t+1:T 0 1:t 1:t t+1:T
maylearnthetwists,orlikelihoodratioΦ∗(s ,o ) ∝ σ(o |s ) ∝ σ(s |o )/p (s ),usingaclassifier
t 1:t t+1:T t+1:T 1:t 1:t t+1:T 0 1:t
whichseekstodistinguishsamplesfromσ(s |o )andp (s )(Gutmann&Hyva¨rinen,2010;Lawsonetal.,2022).
1:t t+1:T 0 1:t
In particular, at each t, we classify the event y = 1, indicating that s ∼ σ(s |o ), or y = 0, indicating that
1:t 1:t t+1:T
s ∼p (s ).
1:t 0 1:t
Consideragiveno ,whichcanbeeithero =1intheunconditionalcaseoro ∼π (o )drawnfroma
t+1:T t+1:T t+1:T s t+1:T
behavioralpolicyasdiscussedbelow. TheSIXOlossbecomes
L (o ;θ)=T (cid:88)−1 E (cid:104) logsigmoid(cid:0) logΦθ(s ,o )(cid:1)(cid:105) +E (cid:104) log(cid:0) 1−sigmoid(cid:0) logΦθ(s ,o )(cid:1)(cid:1)(cid:105)
SIXO 1:T σ(s1:t|ot+1:T) t 1:t t+1:T p0(s1:t) t 1:t t+1:T
t=1
=(cid:88)T E (cid:104) logsigmoid(cid:0) logψθ(s ,o )(cid:1)(cid:105) +E (cid:104) log(cid:0) 1−sigmoid(cid:0) logψθ(s ,o )(cid:1)(cid:1)(cid:105)
σ(s1:t|ot:T) t 1:t t:T p0(s1:t) t 1:t t:T
t=1
=(cid:88)T
E
(cid:20)
log
ψ tθ(s 1:t,o t:T) (cid:21)
+E
(cid:20)
log
1 (cid:21)
(SIXO)
σ(s1:t|ot:T) 1+ψθ(s ,o ) p0(s1:t) 1+ψθ(s ,o )
t=1 t 1:t t:T t 1:t t:T
NotethatwecanperformapproximatepositivesamplingasinSec.4toestimateexpectationsinthefirstterm.
ExactConditionalSampling However,wecanalsousetheBDMCtrickinSec.3.3toobtainexacttargetsamplesfor
generalobservationvariables. Inordertofacilitatetractablesampling,weoptimizetheEq.(SIXO)lossoverasampling
distributionπ (o )=σ(o )forallt,suchthattheobjectivebecomes
s 1:T 1:T
E [L (o
;θ)]=(cid:88)T
E
(cid:20)
log
ψ tθ(s 1:t,o t:T) (cid:21)
+E
(cid:20)
log
1 (cid:21)
σ(o1:T) SIXO 1:T σ(s1:t,ot+1:T) 1+ψθ(s ,o ) p0(s1:t)σ(ot+1:T) 1+ψθ(s ,o )
t=1 t 1:t t:T t 1:t t:T
Withthischoice,notethatwemaysampleoncefromσ(s ,o
)=(cid:81)T
p (s |s )σ(o |s )usingancestralsampling
1:T 1:T t=1 0 t 1:t−1 t 1:t
andusetheappropriatetruncationsforpositivesamplingtermsinvolvingσ(s ,o ). Byshufflingobservationvariables
1:t t+1:T
acrossabatchofK samples,wemayobtainsamplesfromtheproductofmarginalsp (s )σ(o )orp (s )σ(o )
0 1:T 1:T 0 1:t t+1:T
inthenegativesamplingterm.
Inthemaintext,notethatweconditionono =1oro =s (forinfilling).
T T T+1:T+c
GradientandComparisonwithCTL Proceedingwiththeψθ parameterizationforthetargetσ(s |o ) = σ(s )
t 1:T T 1:T
withfixedo andunconditionaltwistsψθ(s ),thegradientofEq.(SIXO)withrespecttoθis
T t 1:t
∇ L
(θ)=(cid:88)T
E
(cid:20)
∇ logψθ(s )−
ψ tθ(s 1:t)
∇ logψθ(s
)(cid:21)
−E
(cid:20) ψ tθ(s 1:t)
∇ logψθ(s
)(cid:21)
θ SIXO σ(s1:t) θ t 1:t 1+ψθ(s ) θ t 1:t p0(s1:t) 1+ψθ(s ) θ t 1:t
t=1 t 1:t t 1:t
=(cid:88)T
E
(cid:20) 1
∇ logψθ(s
)(cid:21)
−E
(cid:20) ψ tθ(s 1:t)
∇ logψθ(s
)(cid:21)
σ(s1:t) 1+ψθ(s ) θ t 1:t p0(s1:t) 1+ψθ(s ) θ t 1:t
t=1 t 1:t t 1:t
(SIXOGradient)
28ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
TheSIXOgradientissuperficiallysimilartoourCTLgradientinSec.4.1,inthatitinvolves∇ logψθ underpositiveand
θ t
negativessamples. However,viewingπ˜θ(s )=p (s )ψθ(s )astheunnormalizeddensityofourintermediatetwisting
t 1:t 0 1:t t 1:t
target,wecanseethatthesecondtermintheSIXOupdateincludesπ˜ tθ(s 1:t). Rewritingtohighlightdifferenceswithour
CTLgradient,wehave
T (cid:32) (cid:33)
(cid:88) (cid:88) 1 (cid:88) 1
∇ L = σ(s ) ∇ logψθ(s )− π˜θ(s ) ∇ logψθ(s )
θ SIXO 1:t 1+ψθ(s ) θ t 1:t t 1:t 1+ψθ(s ) θ t 1:t
t=1 s1:t t 1:t s1:t t 1:t
T (cid:32) (cid:33)
(cid:88) (cid:88) (cid:88) 1
∇ L = σ(s ) ∇ logψθ(s )− π˜θ(s ) ∇ logψθ(s )
θ CTL 1:t θ t 1:t t 1:t Zψ θ t 1:t
t=1 s1:t s1:t t
(SIXOvs. CTL)
Tocomparethetwo,firstnotethatthepositivesamplinggradientinSIXOisscaledbyafactorof 1 factor(which
1+ψ tθ(s1:t)
reflectsthemisclassificationprobabilityunderψθ). Forthenegativesamplingterms,notethatπ˜θ(s )isdividedbyafactor
t t 1:t
of 1 intheSIXOgradient,insteadofthetruenormalizationconstantZψ forthegradientofourCTLlossEq.(22).
1+ψ tθ(s1:t) t
C.4.FUDGE:FutureDiscriminators(Yang&Klein,2021)
IncontrasttoSIXO,theFUDGEmethodfromYang&Klein(2021)seekstodirectlylearnadiscriminativeclassifierto
matchtheconditionallikelihoodψ∗(s ,o )∝σ(o |s )orψ∗(s ,o )∝σ(o |s )(seeApp.B.2).
t 1:t T T 1:t t 1:t t:T t:T 1:t
As before, we define the joint distribution σ(s ,o ) = p (s )σ(o |s ) with ϕ(s ,o ) = σ(o |s ). From
1:T T 0 1:T T 1:T 1:T T T 1:T
Eq.(52)aboveorApp.B.2Eq.(40),wehave
(cid:88)
ψ∗(s ,o )∝σ(o |s ):= p (s |s )σ(o |s ) (54)
t 1:t T T 1:t 0 t+1:T 1:t T 1:T
st+1:T
Yang&Klein(2021)considertraininga‘futurediscriminator’ψθ(s ,o )≈σ(o |s )which,asinEq.(54)marginalizes
t 1:t T T 1:t
overfuturetokenstopredicttheexpectedprobabilitythatafullsequencewithprefixs emitso (e.g.,leto =abethe
1:t T T
probabilityofaclassifierforclassa,ortheprobabilitythats satisfiesadesiredattributeindicatedbyabooleano =1).
1:T T
InsimilarfashiontoSIXOintheprevioussection,wedefineabinaryrandomvariableysuchthat
(cid:40) (cid:40)
σ(o |s ) y=1 ψθ(s ,o ) y=1
σ(y|s ,o )= T 1:t p (y|s ,o )= t 1:t T (55)
1:t T 1−σ(o T|s 1:t) y=0 ψtθ 1:t T 1−ψ tθ(s 1:t,o T) y=0
wherewedirectlyparameterizep (y|s ,o ) = ψθ(s ,o )tobeaprobabilitydistribution(e.g. usingasigmoidor
ψθ 1:t T t 1:t T
t
softmaxactivation). Foragivenobservationrandomvariableo andpartialsequences ,wecandefinetheFUDGEloss
T 1:t
(cid:88)T (cid:88)T (cid:16) (cid:13) (cid:17)
L (s ,o ;θ):= D σ(y|s ,o )(cid:13)p (y|s ,o ) (FUDGE)
FUDGE 1:t T KL 1:t T (cid:13) ψtθ 1:t T
t=1 t=1
(cid:88)T (cid:104) (cid:105)
= − σ(y=1|s ,o )logp (y=1|s ,o )+σ(y=0|s ,o )logp (y=0|s ,o ) +const.
1:t T ψtθ 1:t T 1:t T ψtθ 1:t T
t=1
=(cid:88)T
−E
(cid:20)
σ(o |s )logψθ(s ,o
)+(cid:16)
1−σ(o |s
)(cid:17) log(cid:16)
1−ψθ(s ,o
)(cid:17)(cid:17)(cid:21)
+const.
p0(st+1:T|s1:t) T 1:T t 1:t T T 1:T t 1:t T
t=1
where, in moving from the second to the third line, we have used the fact that σ(y = 1|s ,o ) = σ(o |s ) =
1:t T T 1:t
(cid:80)
p (s |s )σ(o |s )fromEq.(54)andEq.(55). Attheoptimum,p (y =1|s ,o )=σ(y =1|s ,o )
st+1:T 0 t+1:T 1:t T 1:T ψ tθ 1:t T 1:t T
impliesψθ(s ,o )=σ(o |s ),asdesired.
t 1:t T T 1:t
Whilesamplingmaybedoneusinganarbitrarydistributionoverprefixess andobservationo ,Eq.(FUDGE)requires
1:t T
thatrolloutsbesampledunderthebasemodelp (s |s )inordertoensuresamplingfromtheappropriatedistribution
0 t+1:T 1:t
σ(y =1|s ,o ). ThisrestrictionissimilartowhatwerequiredinEq.(CD-FUDGE),althoughthelossinEq.(FUDGE)is
1:t T
basedoncrossentropyclassificationratherthanasquarederror. Wediscussthechoicesmadeinourexperimentsbelow.
Yang&Klein(2021)Setting IntheoriginalFUDGEpaper,Yang&Klein(2021)considerlearningfromadatasetof
labelledexamples(s ,o )or(s ,o )forabinaryobservationvariableo =1whichdefinesthetargetdistribution.
1:T T 1:t T T
29ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
UnconditionalTwistSetting FortheunconditionaltwistexperimentsinSec.7.2.1-7.2.2,wesampleunderthebasemodel
proposalπ (s )=p (s )wherethetargetdistributionconditionsono =1andσ(o =1|s )=ϕ(s )=σ(y =
s 1:t 0 1:t T T 1:T 1:T
1|s ,o =1). Inparticular,weoptimize
1:T T
T
(cid:88)
min E [L (s ,o =1;θ)]
θ
p0(s1:t) FUDGE 1:t T
t=1
ConditionalTwistSetting Forconditionaltwistlearning,wecanconsideramortizinglearningthetwistsψ (s ,o )
t 1:t T
oversomedistributionofobservationvariablesπ (s ,o ). Inparticular,inourinfillingexperimentsinSec.7.2.3,we
s 1:t T
considersamplingunderthefollowingjointdistribution,
π (s ,o )=p (s )σ(o |s ),
s 1:t T 0 1:t T 1:t
whichwecansamplefrombyfirstsamplingfromp (s )σ(o |s )andthendroppings subsequence. Therefore,
0 1:T T 1:T t+1:T
theoverallobjectivebecomes
minE [L (s ,o ;θ)] (56)
θ
πs(s1:t,oT) FUDGE 1:t T
(cid:88)T (cid:20) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)(cid:21)
=min −E σ(o |s )logψθ(s ,o )+ 1−σ(o |s ) log 1−ψθ(s ,o ) ,
θ
p0(s1:T)σ(oT|s1:t) T 1:T t 1:t T T 1:T t 1:t T
t=1
wheretheexpectationp (s )includestheexpectationunderp (s |s )fromEq.(FUDGE).Notethatrolloutof
0 1:T 0 t+1:T 1:t
s |s usedtosamplefromp (s )shouldbeindependentoftherolloutusedtosamplefromσ(o |s ).
t+1:T 1:t 0 1:T T 1:t
D.DecodingStrategiesusingLearnedTwistsfromMudgaletal.(2023)
D.1.ProposalSamplinginMudgaletal.(2023)
AsnotedinApp.C.2(andinL∗(θ)inMudgaletal.(2023)),theCDlossescanbeseenasenforcingtheoptimalityconditions
(cid:88)
ψcd∗(s )= p (s |s )ϕ(s ), ∀t. (57)
t 1:t 0 t+1:T 1:t 1:T
st+1:T
InRLterms,weinterpretthetwistsψcd∗asperformingpolicyevaluationoftheexpectedunregularized‘reward’ϕ(s )
t 1:T
underafixedpolicyp (s ). ThenotationofMudgaletal.(2023)(theirEq. (1),(5),ourEq.(57))indeedcorrespondsto
0 1:T
ϕ(s )=:r (s ). (CDreward)
1:T cd 1:T
However,Mudgaletal.(2023)proposetousethelearnedtwistfunctionsψθ toperformone-stepsamplingas
t
q tcd(s t|s 1:t−1)∝p 0(s t|s 1:t−1)eβψ tθ(s1:t) (CDproposal)
Weproceedtoexplainthatthisschemedoesnotcorrespondtosamplingfromthetwist-inducedproposalundertwodifferent
definitionsofthetargetσ(s )(orpotentialϕ(s ))inourSMCframework.
1:T 1:T
Comparison with Our ϕ(s ) = r (s ) Case: As we have argued above, the CD-Q and CD-FUDGE may be
1:T cd 1:T
viewedaslearningtwistvaluesψθ foraterminalpotentialϕ(s )=r (s ). However,ourtwist-inducedproposalwhich
t 1:T cd 1:T
minimizesthevarianceoftheone-stepimportanceweightswiththeseSMCtargets{πθ}wouldyield
t
qπ(s |s )∝p (s |s )ψθ(s ), (Twist-Ind. proposal(ϕ=r ))
t t 1:t−1 0 t 1:t−1 t 1:t cd
which,comparedtoEq.(CDproposal)doesnotexponentiateorscaleψθ andisdirectlyproportionaltotheexpectedr .
t cd
30ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Comparison with Our ϕ(s 1:T) = eβrcd(s1:T) Case (Soft RL): The stochastic sampling in Eq. (CD proposal) is also
reminiscentofthetwist-inducedproposalinthesoftRLcaseofourframeworkwhere,incontrasttoEq.(CDreward),the
targetisdefinedviaϕ(s 1:T)=eβrcd(s1:T). AsinApp.B.3,
q tπ(s t|s 1:t−1)∝p 0(s t|s 1:t−1)eβV tθ(s1:t) (Twist-Ind. proposal(ϕ=eβrcd))
Weproceedtowritebothqcdandqπ asthesolutiontoavariationaloptimization,highlightingsimilaritiesinblue,butnoting
t t
thedifferentdefinitionsofϕintermsofr . Weassumenointermediatepotentialorreward,andconsidertheoptimaltwists
cd
toemphasizetheroleofr . UsingMudgaletal.(2023)Eq. 2andThm2.1(forCD)andEq.(OptimalIntermediateSoft
cd
Value)(forsoftRL),wehave
qcd∗ (s |s )= argmax E (cid:104) E (cid:2) r (s )(cid:3)(cid:105) −1 D (q(s |s )∥p (s |s ))
t t 1:t−1 q(st|s1:t−1) p0(st+1:T|s1:t) cd 1:T β KL t 1:t−1 0 t 1:t−1
q(st|s1:t−1) (cid:124) (cid:123)(cid:122) (cid:125)
ψ tcd∗(s1:t) (forϕ=rcd)
(CDproposaloptimization)
q tπ∗ (s t|s 1:t−1)= argmax E q(st|s1:t−1)(cid:104) β1 logE p0(st+1:T|s1:t)(cid:2) eβrcd(s1:T)(cid:3)(cid:105) − β1 D KL(q(s t|s 1:t−1)∥p 0(s t|s 1:t−1))
q(st|s1:t−1)
(cid:124) (cid:123)(cid:122) (cid:125)
V t∗(s1:t) (forϕ=eβrcd)
(SoftRLproposaloptimization)
ThesecondtermsofEq.(CDproposaloptimization)andEq.(SoftRLproposaloptimization)matchandcorrespondto
one-stepKLdivergenceregularizationofthepolicyq (s |s ). However,theexpectationtermsdifferaswenowdiscuss.
t t 1:t−1
SoftValuesAccountforFutureRegularization UsingEq.(OptimalIntermediateSoftValue)toexpandthedefinitionof
thesoftvaluefunction,weseethatEq.(SoftRLproposaloptimization)alsoimplicitlycontainsanexpectedterminalreward,
V t∗(s 1:t)= β1 logE p0(st+1:T|s1:t)eβrcd(s1:T) = q(st+m 1:a Tx |s1:t)E q(st+1:T|s1:t)(cid:2) r cd(s 1:T)(cid:3) − β1 D KL(q(s t+1:T|s 1:t)∥p 0(s t+1:T|s 1:t))
(58)
As β → 0 in Eq. (58), this optimization strictly enforces q(s |s ) = p (s |s ), and the soft value function
t+1:T 1:t 0 t+1:T 1:t
recoverstheexpectedrewardunderthebasemodelE [r (s )],whichappearsinfirsttermEq.(CDproposal
p0(st+1:T|s1:t) cd 1:T
optimization). Ontheotherhand,thesecondterminEq.(CDproposaloptimization)usesβ >0foroptimizationofthe
proposalq(s |s )atthecurrentstep. ThisinconsistencyinEq.(CDproposaloptimization)(usingβ =0inthefirstterm
t 1:t−1
andβ >0inthesecondterm)arisesfromthefactthatEq.(CDproposaloptimization)doesnotconsidertheeffectoffuture
regularization,whiletheMDPformulationinEq.(SoftRLproposaloptimization)doessoviatheoptimizationinEq.(58)
andthelog-mean-expformofthesoftvaluefunctionV∗.
t
OnMudgaletal.(2023)’sOne-StepProposalandSMCInterpretation AsnotedinEq.(57),thetwistslearnedby
Mudgal et al. (2023) correspond to policy evaluation for the reward r under the base model p . However, we have
cd 0
arguedthattheone-stepproposalinEq.(CDproposal)(whichconsidersone-stepKLregularizationofqcdtop )doesnot
t 0
immediatelyfitwithinourSMCframework. Inparticular,itisnotapparentthatthecompositionofone-stepproposals
qcd(s )=(cid:81)t qcd(s |s )samplesfromthemarginalsσ(s )ofanaturaltargetdistributionσ(s )atoptimality.
1:T τ=1 τ τ 1:τ−1 1:t 1:T
FlexibleInference-Timeβ Scaling TheexperimentsinMudgaletal.(2023)evaluatetradeoffcurvesbetweenexpected
rewardandD KL(cid:0) qcd(s 1:T)(cid:13) (cid:13)p 0(s 1:T)(cid:1) forvariousvaluesofregularizationstrengthβ. SincethetwistslearnedbyMudgal
etal.(2023)inEq.(57)donotdependonβ,samplingaccordingtoEq.(CDproposal)orEq.(CDproposaloptimization)
hasthebenefitofallowingflexibletemperingorβ-scalingatinferencetimewithoutadditionallearning.
Suchtradeoffcurvesarealsonaturalfromtheperspectiveofsoft-RL(c.f. Eq.(42)andEq.(46)). WhileEq.(58)appears
torequireseparatetwist-learningproceduresforeachβ,flexibleinference-timeβ scalingcouldbeachievedwithasingle
trainingruninourframeworkbylearningaconditionaltwistnetworkψθ(s ,β)whichconsidersβinitsinputandtraining
t 1:t
loss,oradaptingthemethodsof(Baeetal.,2022)proposedinthecontextofrate-distortionoptimization.
Comparison with Khanov et al. (2024) Khanov et al. (2024) consider softmax decoding similar to Eq. (Twist-Ind.
proposal(ϕ=r )). However,insteadofVθ(s )asthelogit,theyusearewardmodelr (s )whichistrainedfromfull
cd t 1:t T 1:T
sequences(ϕ(s 1:T) = eβrT(s1:T)),butappliedtopartialsequenceswithoutmodification,r T(s 1:t). Thisclearlydoesnot
correspondtoatwistorsoftvaluefunctionV t∗(s 1:t)= β1 log(cid:80)
st+1:T
p 0(s t+1:T|s 1:t)eβrT(s1:T) ̸=r T(s 1:t).
31ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
D.2.BlockwiseGreedyDecodinginMudgaletal.(2023)
Asanalternativeuseofthetwistfunctionsatinferencetimeandageneralizationofbest-of-K decodingtopartialsequences,
Mudgaletal.(2023)alsoconsidera‘blockwise’decodingschemeusingthelearnedtwistfunctionsψθ. Inparticular,forK
t
partialcompletionsoflengthM (fromaprefixs ),sampledfromthebasemodel,s(k) ∼p (s |s ),Mudgal
1:t t+1:t+M 0 t+1:t+M 1:t
etal.(2023)proposetochoose
sω =argmaxψθ (sk ) (59)
t+1:t+M t+M 1:t+M
k
andproceedwithsamplingK furthercontinuationswithprefixsω untilthenextresamplingsteporanend-of-string
1:t+M
tokenisreached. TheargmaxselectionstrategymayseemnaturalfromtheunregularizedRL(asβ →∞)orexpected
futurerewardperspectiveinApp.D.1,butdoesnotyieldsamplesfromσ(s )withthecorrespondingoptimaltwists.
1:T
OurSMCframeworkinsteadwouldadvocateprobabilisticresamplingbasedontheapproximatetwistfunctionsusingthe
(c-orM-step)importanceweightsinSec.3inordertomatchthedesiredtargetdistribution.
Finally,Khanovetal.(2024)alsoconsiderargmaxdecodingofnexttokensusingtheunmodifiedr (s )describedabove.
T 1:t
E.ProposalLearningMethods
We next describe methods for learning variational policies or proposals qξ(s ) = (cid:81)T qξ(s |s ) parameterized
1:T t=1 t t 1:t−1
by ξ, which can be used for SMC sampling with intermediate targets πθ(s ) and learned twists ψθ(s ) or Vθ(s )
t 1:t t 1:t t 1:t
parameterizedbyθ. Alternatively,suchproposalsmaybeuseddirectlyinthe IWAE boundsonlogZ ,whichrelyon
σ
simpleimportancesamplingoverfullsequencesasinSec.2.1anddonotrequirethedefinitionofintermediatetargetsπ .
t
InApp.E.3,weprovideadetaileddescriptionoftheDPGpolicygradientmethod,whichcanbeinterpretedasamaximum
likelihoodobjectiveforasequentialenergy-basedmodel. TodistinguishthisEBMapproachfromourCTLmethodfor
twistlearning,weemphasizeissueswhichcanarisefromnaiveuseofaproposal-learningobjectivetodefineintermediate
twistingtargetsforSMCinApp.E.3.1.
E.1.PathConsistencyLearningforControlledGeneration
Guo et al. (2021) consider learning Q-values to obtain a fine-tuned variational policy which can be directly used as a
samplingdistributionforcontrolledgeneration. Buildingonthepathconsistencylearning(PCL)lossinNachumetal.
(2017)andApp.C.1.2,Guoetal.(2021)considerparameterizingtheproposalusingQξ(s ,s ),
t t 1:t−1
q tξ(s t|s 1:t−1)=p 0(s t|s 1:t−1)eβQξ t(st,s1:t−1)−βV Qξ(s1:t−1) (60)
whereV Qξ(s 1:t−1)= β1 log(cid:80) stp 0(s t|s 1:t−1)eβQξ t enforcesnormalization.
Guoetal.(2021)definethetargetsusingQ¯ξ(s ,s ),aslowly-updatedtargetnetworkbasedonQξ. Usingtheimplied
t t 1:t−1 t
formofthesoftvalueV¯(s 1:t−1):= β1 log(cid:80) stp 0(s t|s 1:t−1)eβQ¯ξ t(st,s1:t−1),thesingle-stepPCLlossbecomes
L
(ξ)=min(cid:88)T
E
(cid:20)(cid:16)
r (s )+sg(V¯(s ))−sg(V¯ (s ))−Qξ(s ,s )+V (s
)(cid:17)2(cid:21)
(61)
PCL−Q
ξ
πs(s1:t) t 1:t t 1:t t−1 1:t−1 t t 1:t−1 Qξ 1:t−1
t=1
wheresg(·)indicatesstopgradient. BuildingontheinterpretationinApp.C.1,weviewV¯(s )andV¯ (s )asthe
t 1:t t−1 1:t−1
twisting targets, with a learned proposal parameterized by Qξ as in Eq. (60) (or App. B.4). While the loss in Eq. (61)
t
issimilarinpracticetothesoftQ-learninglossinApp.C.1.1,weemphasizethatthelatterismotivatedfromtheSMC
perspectivewiththetwistingtargetsastheprimaryobjectofinterestandflexibilityinthechoiceofproposal. Bycontrast,
Guoetal.(2021)areinterestedinlearningaproposalpolicyanddonotconsider,forexample,resamplingaccordingtoV¯.
t
Guoetal.(2021);Nachumetal.(2017)alsoconsider‘multi-step’PCLlosses(Eq.(multi-stepPCL))whichuseobserved
rewardduringrolloutsoflengthλtolimitrelianceonestimatedintermediatevaluesV¯(s ). TheobjectiveinHuetal.
t 1:t
(2023)alsocorrespondstoaPCLobjective.
32ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
E.2.PolicyGradientMethods
TraditionalRLHFpipelinesuseapolicygradientmethodsuchasPPOtooptimizetheobjectiveinEq.(42),
L ELBO(ξ)=m ξax E qξ(s1:T)[r T(s 1:T)]− β1 D KL(cid:0) qξ(s 1:T)(cid:13) (cid:13)p 0(s 1:T)(cid:1) =m ξinD KL(cid:0) qξ(s 1:T)(cid:13) (cid:13)σ(s 1:T)(cid:1) (62)
wherer (s ) = 1 logϕ(s )correspondstoourfinaltwist. AsinEq.(46),thegapinthisoptimizationisthemode-
T 1:T β 1:T
seekingKLdivergenceD KL(cid:0) qξ(s 1:T)(cid:13) (cid:13)σ(s 1:T)(cid:1) .
Notably, this objective does not make use of exact target samples from σ(s ) when they are available. Further, the
1:T
mode-seekingbehaviorhasbeenshowntoreducediversityoffine-tunedmodels(Stiennonetal.,2020;Goetal.,2023). To
combatthis,Goetal.(2023)derivepolicygradientmethodstooptimizearbitraryf-divergencesD f(cid:0) qξ(s 1:T)(cid:13) (cid:13)σ(s 1:T)(cid:1)
betweenthelearnedvariationalpolicyqξ andtargetσ.
E.3.PolicyGradientwithMass-Covering/MaximumLikelihoodKLDivergence
Wefocusonthecaseofminimizingthemass-coveringKLdivergenceD KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) totrainq ξ,whichconstitutes
thedistributionalpolicygradients(DPG)methodforlanguagemodelfinetuning(Parshakovaetal.,2019;Khalifaetal.,2020;
Korbaketal.,2022a;Goetal.,2023)andhasbeenusedtolearnSMCproposalsinstate-spacemodelsin(Guetal.,2015).
Inparticular,thegradientofD KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) =E σ(s1:T)[logσ(s 1:T)−logqξ(s 1:T)]is
(cid:20) (cid:21)
∇ ξD KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) =−E σ(s1:T)(cid:2) ∇ ξlogqξ(s 1:T)(cid:3) =−E
qξ(s1:T)
qσ ξ( (s s1:T) )∇ ξlogqξ(s 1:T)
1:T
(63)
(cid:20) (cid:21)
1 σ˜(s )
=−E 1:T ∇ logqξ(s )
qξ(s1:T) Z qξ(s ) ξ 1:T
σ 1:T
Werecognizetheimportanceweightsw(s )= σ˜(s1:T) fromEq.(3). Goetal.(2023)considerestimatingEq.(63)using
1:T qξ(s1:T)
amovingaverageestimateofthepartitionfunctionZˆ
σ
K
∇ ξD KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) ≈ (cid:88) Zˆ1 w(s( 1k :T))∇ ξlogqξ(s( 1k :T)), (DPG(generalZˆ σ))
k=1 σ
Alternatively,theexpectationmaythusbeestimatedusingSISwiththevariationalpolicyqξ(s ). Usingself-normalized
1:T
importancesampling(SNIS)toestimateEq.(63)asinEq.(5)correspondstoZˆ =(cid:80)K w(s(k)),with
σ j=1 1:T
∇ ξD KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) ≈ (cid:88)K (cid:80)Kw(s w( 1k : (T) s) (j))∇ ξlogqξ(s( 1k :T)). (64)
k=1 j=1 1:T
WeusethisgradientforDPGproposallearninginthemaintextexperiments,althoughweusetheparameterizationdescribed
inEq.(DPG)below.
DPGasSequentialMaximumLikelihoodObjective WenowshowEq.(64)isequivalenttoasequentialmaximum
likelihoodEBMobjective. ConsiderminimizingtheKLdivergence,
D KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) =(cid:88)T E σ(s1:t−1)D KL(cid:16) σ(s t|s 1:t−1)(cid:13) (cid:13) (cid:13)q tξ(s t|s 1:t−1)(cid:17) (EBMproposallearning)
t=1
p (s |s )ψξ(s )
where qξ(s |s )= 0 t 1:t−1 t 1:t . (65)
t t 1:t−1 (cid:80) p (s |s )ψξ(s )
st 0 t 1:t−1 t 1:t
While this is reminscent of the twist-induced proposal in Prop. 3.3, we emphasize distinctions between energy-based
learningoftheproposal(DPG)versusenergy-basedlearningoftwistfunctions(CTL)inApp.E.3.1.
ThegradientofEq.(EBMproposallearning)becomes
∇ D (cid:16) σ(s )(cid:13) (cid:13)qξ(s )(cid:17) =(cid:88)T E (cid:104) E (cid:2) ∇ logψξ(s )(cid:3) −E (cid:2) ∇ logψξ(s )(cid:3)(cid:105) . (66)
ξ KL 1:T (cid:13) 1:T σ(s1:t−1) σ(st|s1:t−1) ξ t 1:t qtξ(st|s1:t−1) ξ t 1:t
t=1
33ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
StartingfromEq.(64),wenowseektorecoverEq.(66). UsingEq.(65),wecanwrite
T
logqξ(s(k))=(cid:88)(cid:0) logp (s(k)|s(k) )+logψξ(s(k))−log(cid:88) p (s |s(k) )ψξ(s ,s(k) )(cid:1)
1:T 0 t 1:t−1 t 1:t 0 t 1:t−1 t t 1:t−1
t=1 st
∇
logqξ(s(k))=(cid:88)T (cid:18)
∇ logψξ(s(k))−E
(cid:104)
∇ logψξ(s ,s(k)
)(cid:105)(cid:19)
ξ 1:T ξ t 1:t qtξ(st|s( 1k :t) −1) ξ t t 1:t−1
t=1
SubstitutingintoEq.(64),werecover
∇ ξD KL(cid:0) σ(s 1:T)(cid:13) (cid:13)qξ(s 1:T)(cid:1) ≈ (cid:88)K (cid:80)Kw(s w( 1k : (T) s) (k))(cid:88)T (cid:16) ∇ ξlogψ tξ(s( 1k :t))−E
q tξ(st|s( 1k :t)
−1)(cid:104) ∇ ξlogψ tξ(s t,s( 1k :t) −1)(cid:105)(cid:17)
k=1 j=1 1:T t=1
(DPG)
whichisanSNISestimateofthemaximumlikelihoodEBMgradientinEq.(66),asdesired. Notethattheexpectationover
qξ(s |s(k) )canbecalculatedexactly.
t t 1:t−1
ComparisonwithCTLObjective ThegradientinEq.(DPG)aboveappearssimilartoourCTLobjectiveandgradientin
Sec.4.1. However,theDPGlossinEq.(EBMproposallearning)isasingle(joint)KLdivergenceovertheentiresequence,
whereasCTLoptimizesT separateKLdivergencesforeachintermediatemarginal.
FortheDPGgradientinEq.(66),negativesamplingisperformedusinga‘positive’prefixs(k) ∼σ(s )andanexact
1:t−1 1:t−1
‘negative’samplefromtheone-step-aheadqξ(s |s(k) )(Eq.(65),whichwehaveassumedtobetractable). Inpractice,we
t t 1:t−1
obtaintheprefixesusingthetruncationofexactsamplesorapproximatepositivesamplingwiththefinaltargetweightsasin
Eq.(DPG).Bycontrast,theCTLgradientinEq.(22)involvesapproximatenegativesamplingundereachπ (s ).
t 1:t
E.3.1.NAIVEUSEOFPROPOSALLEARNINGTODEFINETWISTEDSMCTARGETS
WhilewehaveshowninProp.3.3howone-stepproposals{qπ(s |s )}T canbeinducedfromagivensetoftwist
t t 1:t−1 t=1
functions{ψ (s )}T ortargetdistributions{π (s )}T ,wenowemphasizethatmovingtheotherdirection(inducing
t 1:t t=1 t 1:t t=1
intermediate twisting targets from a proposal learning scheme parameterized by {ψξ}T ) does not yield the correct
t t=1
intermediatetargetsforresampling(App.A.1),evenatoptimalityintheproposallearningobjective.
WefocusourargumentsonlearningwiththeEBMmaximumlikelihoodobjectiveinEq.(EBMproposallearning)asan
example. The proposal energies ψξ(s ) appear to play a role analogous to the twist function ψ (s ) in the one-step
t 1:t t 1:t
proposalinducedfromtwisttargets{π }T inSec.3.
t t=1
However,weproceedtoshowinProp.E.2thatnaiveuseofψξ todefinetwistingtargetsusing11
t
(cid:40) 1 p (s )ψξ(s ) t̸=T
π tξ(s 1:t)= Z 1tψ p0 (s1:t )ϕt
(s
1:t
) t=T
(67)
Zσ 0 1:T 1:T
neednotleadtoanSMCprocedureforwhichπξ(s )=σ(s ),evenifqξ(s |s )=σ(s |s )forallt. Wethus
t 1:t 1:t t t 1:t−1 t 1:t−1
arguethatψξ learnedusingEq.(EBMproposallearning)shouldnotbeusedastargettwistsinEq.(67),sincetheydonot
t
yieldtheoptimalinteremdiatetargetdistributionsatoptimality(App.A.1).
Webeginbyshowingasimplelemmafortheone-stepconditionalsinEq.(EBMproposallearning).
Lemma E.1. Any twist induced proposal qξ(s |s ) (induced by ψξ(s )) is invariant to rescaling ψξ(s ) by an
t t 1:t−1 t 1:t t 1:t
arbitraryconstantc(s )withrespecttos ,
1:t−1 1:t−1
ψξc(s ):=c(s )ψξ(s ) (68)
t 1:t 1:t−1 t 1:t
Proof.
p (s |s )ψξc(s ) p (s |s )c(s )ψξ(s ) p (s |s )ψξ(s )
qξc(s |s )= 0 t 1:t−1 t 1:t = 0 t 1:t−1 1:t−1 t 1:t = 0 t 1:t−1 t 1:t =qξ(s |s ).
t t 1:t−1 (cid:80) p (s |s )ψξc(s ) (cid:80) p (s |s )c(s )ψξ(s ) (cid:80) p (s |s )ψξ(s ) t t 1:t−1
st 0 t 1:t−1 t 1:t st 0 t 1:t−1 1:t−1 t 1:t st 0 t 1:t−1 t 1:t
11Weassumenointermediatepotentialsinthissection,asinthemaintext.
34ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
PropositionE.2. Thereexist{ψξ∗}T suchthat(i)qξ∗(s |s )=σ(s |s )and(ii)theSMCtargets{πξ∗(s )}T
t t=1 t t 1:t−1 t 1:t−1 t 1:t t=1
inducedby{ψξ∗}T viaEq.(67)aredifferentfromσ(s ).
t t=1 1:t
Proof. Tosatisfycondition(i)ofthecurrentproposition,wedefine
(cid:40) (cid:80)
p (s |s )ϕ(s ) τ ̸=t
ψξ∗(s ):= sτ+1:T 0 τ+1:T 1:τ 1:T (69)
τ 1:τ c(s ) (cid:80) p (s |s )ϕ(s ) τ =t
1:t−1 st+1:T 0 t+1:T 1:t 1:T
whichforallτ,yieldsoptimalproposals: (i)qξ∗(s |s ) = σ(s |s ) ∝ p (s |s )ψξ∗(s )viaLemmaE.1.
τ 1:τ−1 τ 1:τ−1 0 τ 1:τ−1 τ 1:τ
However,itisclearthatc(s )̸=1canbreakthenecessaryconditionforoptimalityofSMCsamplingthatπ (s )=
1:t−1 t 1:t
σ(s )(Prop.A.4). Inparticular,consider
1:t
1 1 (cid:88)
πξ∗(s )= p (s )ψξ∗(s )= c(s )p (s ) p (s |s )ϕ(s )
t 1:t Zψ 0 1:t t 1:t Zψ 1:t−1 0 1:t 0 t+1:T 1:t 1:T
t t st+1:T
1
= c(s )σ˜(s )̸=σ(s ) (70)
Zψ 1:t−1 1:t 1:t
t
forc(s ) ̸= 1,whichintroducesanadditionalfactorwhichdependsons . Thus,thetwisttargetπξ∗(s )induced
1:t−1 1:t t 1:t
fromψξ∗(s )inEq.(69)isnotequaltothedesiredmarginalσ(s ),despitethefactthatallproposalsareoptimal.
t 1:t 1:t
WeindeedobservedexperimentallythatresamplingbasedonEq.(67)aftertrainingusingEq.(EBMproposallearning)
couldleadtoworseSMClogZ boundsthansimplycalculatingtheSISorIWAEboundwith(cid:81)T qξ(s |s ).
σ t=1 t t 1:t−1
OptimalityinCTLObjectiveimpliesOptimalTwistedSMC IncontrasttoProp.E.2,notethattheglobaloptimumof
(cid:16) (cid:13) (cid:17)
ourCTLobjectivemin(cid:80)T D σ(s )(cid:13)πψ(s ) (whichoccursfortheoptimaltwists{ψ∗}T−1inProp.3.2),results
t=1 KL 1:t (cid:13) t 1:t t t=1
inboththetwist-inducedproposalqπ∗(s |s )=σ(s |s )andthetwistingtargetsπ∗(s )=σ(s )satisfyingthe
t t 1:t−1 t 1:t−1 t 1:t 1:t
necessaryandsufficientconditionsforoptimalityoutlinedinApp.A.1Prop.A.3.
E.3.2. SMCWITHNORMALIZEDTARGETSINDUCEDBYLEARNEDPROPOSALLEADSTOUNIFORMWEIGHTS
TheissueinProp.E.2arisesfromthedegreeoffreedomc(s )inthenormalizationconstantoftheone-stepproposal.
1:t−1
Toavoidthis,wecaninsteaddefinenormalizedtwistedintermediatetargetsusing
π˜ tξ(s
1:t)= p 0(s 1:t) τ(cid:81) =t 1Zψ τξτ (ξ s( 1s :1 τ: −τ)
1)
= τ(cid:81) =t 1q τξ(s τ|s 1:τ−1) t̸=T
(71)
p (s )ϕ(s ) t=T
0 1:T 1:T
whereZξ(s )arisesfromtheproposalqξ(s |s ):= 1 p (s |s )ψξ(s )learnedaccordingtoEq.(EBM
t 1:t−1 t t 1:t−1 Z tξ(s1:t−1) 0 t 1:t−1 t 1:t
proposallearning).
Crucially,π˜ξ inEq.(71)areautomaticallynormalizedfort̸=T,astheproductofnormalizedproposals. Inthiscase,SMC
t
resamplingwithqξ orthetwist-inducedproposalyieldsuniformresamplingweights,
(fort<T): w t(s 1:t)=
π˜ tξ
−1(s1:tπ˜ −tξ 1( )s q1 ξ:t ()
st|s1:t−1)
=
p0(s1:t−1)(cid:32) τt(cid:81)− =1 1Zψ
τξp
τ (ξ
s0
(
1s( :1s τ:1 −τ:t
)
1) )τ (cid:33)(cid:81) =t Z1 tξZ (ψ τξ s1τ (ξ
1
:s( t1s −:1 τ 1: −τ )) 1 p)
0(st|s1:t−1)ψ tξ(s1:t)
=1 (72)
Although we were able to construct well-behaved intermediate twisting targets from a proposal-learning scheme
qξ(s |s )∝p (s |s )ψξ(s ),Eq.(72)showsthatthisdoesnotleadtomeaningfulintermediateSMCresampling.
t t 1:t−1 0 t 1:t−1 t 1:t
Inotherwords,fort<T,themarginaldistributionsofSMCsamplessk withthisschemearesimplyqξ(s ),thesameas
1:t 1:t
wewouldobtainwithnoresampling(SIS/IWAE).
35ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
F.BidirectionalSMC
Inthissection,werecalltheextendedstate-spaceprobabilisticinterpretationofSMCfrom(Maddisonetal.,2017;Andrieu
etal.,2010). Theideaistodefineanunnormalizedtargetdistributionσ (S)andnormalizedproposal q (S)over
SMC SMC
anextendedstatespaceS containingallrandomvariablesrelevanttoSMCsamplingandimportanceweightingwithK
sequencesoflengthT. Definingσ˜ (S)suchthatitsnormalizationconstantmatchesZ ,wecanusesimpleimportance
SMC σ
sampling(SIS)inthisextendedstatespacetoshowthatK-sequenceSMCsamplingyieldsanunbiasedestimatorofZ ,for
σ
exampleZ σ =E q SMC(S)[σ˜ q SS MM CC (( SS ))](asinEq.(8)). Ourendgoalistousethisprobabilisticinterpretationtoderivethelower
andupperboundsonlogZ inProp.5.1,followingBrekelmansetal.(2022)App. A.
σ
Wedefinetheextendedstatespaceproposalandtargetdistributionsbelow,notingthatourboundswillrequiresampling
fromnormalizedσ (S)orq (S),andevaluatingσ˜ (S)andq (S). Wesummarizethealgorithmforsampling
SMC SMC SMC SMC
σ (S)inAlg.2,usingconcatenationnotationforsimplicityinsteadoftheprobabilisticinterpretationusingindexhistories
SMC
inthetext.
Single-SequenceTargetandProposal Weconstructourimportancesamplingboundswiththegoalofestimatingthe(log)
partitionfunctionandsamplingfromatargetdistributionσ(s )=σ˜(s )/Z . Weleverageasequenceofintermediate
1:T 1:T σ
target distributions, {π (s ) = 1 π˜ (s )}T over partial sequences, with the final target π (s ) = σ(s ) and
t 1:t Zt t 1:t t=1 T 1:T 1:T
Z =Z . Weassumeπ˜ (s )=1forallpromptswithZ =1. Finally,ourboundsandsamplingproceduresalsodepend
T σ 0 0 0
onagivensetofproposaldistribution{q(s |s )}T ,asinSec.2.2.
t 1:t−1 t=1
ExtendedStateSpaceRandomVariables ConsideranextendedstatespaceS containingKT tokens{sk}T,K with
t t=1,k=1
sk ∈V andKT indexingrandomvariables{ωk}T,K withωk ∈[1,K],torepresenttheresultsofresampling(Eq.(7)),
t t t=1,k=1 t
S
:=(cid:8) sk,ωk(cid:9)T,K
(73)
t t t=1,k=1
Foreaseofnotation(andsimilarlytoMaddisonetal.(2017);Andrieuetal.(2010)),wecallattentiontoouruseofrecursive
backtrackingindexoperationstocollectsequences{s }basedontheresultsofresampling{ωk}. Weuselistsofindex
1:t t
historiestoconstructsequencesoftokens,withtworecursivedefinitionsofhistories. Letting+indicateappendingoflists,
hk :=[[[]]] ∀k, hk :=hω tk +[[[ωk]]]
0 t t−1 t (IndexNotation)
h¯k :=[[[]]] ∀k, h¯k :=hk +[[[k]]]
0 t t−1
Forexample,thehistoryhk
willbeusedtoconstructprefixsequencesshk
t−1 (i.e. listsoftokens)forsamplinganext
t−1 1:t−1
tokensk. Wedenotesequencesoftokenswiththeindexhistoryinthesuperscriptandalsoexpandthedefinitionforclarity,
t
shk t :=shω t−tk 1 +[[[sω tk ]]] =[[[shω t−tk 1[1] ,...,shω t−tk 1[t−1] ,sω tk ]]]=[[[sω
1···ωtk
,...,sω tω
−tω
−
2tk
1 ,sω tω −tk 1,sω tk ]]]
1:t 1:t−1 t 1 t−1 t 1 t−2 t−1 t
sh 1¯ :k t t :=sh 1:k t t− −1 1+[[[sk t]]] (SequenceNotations)
Inthesecondline,wedefinesh¯k
t
asasequenceoflengthtwhichconcatenatestheprefixshk
t−1 withnexttokensk. The
1:t 1:t t
h¯k hk
notations t representspartialsequencesbeforeresampling. Bycontrast,wewillusethenotations t inthefirstlineof
1:t 1:t
Eq.(SequenceNotations)torefertosequencesafterresampling.
h¯i
Considerthesequences t inaparticularindexi∈[1,K]beforeresampling. Resamplingattimetmayresultinchoosing
1:t
ωk =iforsomek. Usingthefirstline,weseethatshk t =shω t−tk 1 +[[[sω tk ]]]=shi t−1 +[[[si]]]forthoseindicessuchthatωk =i.
t 1:t 1:t−1 t 1:t−1 t t
Indeed,thismatchesthedefinitionofsh¯i
t
=shi
t−1 +[[[si]]]inthesecondline(beforeresampling). Thus,theindexingnotation
1:t 1:t−1 t
inEq.(SequenceNotations)reflectsresamplingorcloningofsequencessh¯i
t intotheindicessuchthatωk =i,whichyields
1:t t
hk
prefixess t forthenextstepofsampling(t+1)ineachindexk ∈[1,K].
1:t
36ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Algorithm2(Twisted)SMCTargetSampling(σ )
SMC
(blue indicates changes from SMC proposal algorithm; s is an exact
1:T
posteriorsample)
(cid:16) (cid:17)
SMC-TARGET p ,q,{ψ }T−1,ϕ,K,{t }R−1,t =0,t =T,s :
0 t t=1 r t=1 0 R 1:T
Initializej ∼uniform({1,...,K})
fort=1,...,T do
fork =1,...,K do
ifk =j then
sk ←s
t t
else
Samplesk
t
∼q(cid:0) s t(cid:12) (cid:12)sk 1:t−1(cid:1)
endif
sk ←concat(cid:0) sk ,sk(cid:1)
1:t 1:t−1 t
ift<T then
wk ←
p0(sk t|sk 1:t−1) ψt(sk 1:t)
t q(sk t|sk 1:t−1) ψt−1(sk 1:t−1)
else
wk ←
p0(sk t|sk 1:t−1) ϕ(sk 1:t)
t q(sk t|sk 1:t−1) ψt−1(sk 1:t−1)
endif
endfor
ift∈{t }R−1then
r r=1
Samplej ∼uniform({1,...,K})
fork =1,...,K do
start interleaved ifk =j then
chain from here!
sk ←s
1:t 1:t
else
(cid:32)(cid:26) (cid:81)tr wi (cid:27)K (cid:33)
Figure4: GraphicalModelsforextendedstate- ωk ∼cat t=tr−1+1 t
t (cid:80)K (cid:81)tr wj
spaceproposalandtargetdistributionswhich j=1 t=tr−1+1 t i=1
result in the bidirectional SMC bounds. We sk ←sω tk
1:t 1:t
show density evaluation in the proposal and endif
targetforafixedsetof{sk t,ω tk}3 k, =2 1,t=1.Welet endfor
thesizeofthecirclesreflectthe(hypothetical) endif
importanceweightsofsequencessh¯k
t andωk endfor
1:t t (cid:110) (cid:111)K
reflectthe(hypothetical)resultsofresampling return sk ,(cid:81)T wk
with these weights. In (b), we assume fixed
1:T t=tR−1+1 t
k=1
j T+1 =j 3 =1asinthetext,withω 21 =2. Zˇ σSMC =(cid:81)R r=1 K1 (cid:80)K k=1(cid:81)t tr =tr−1+1w tk
Extended State Space Proposal Distribution Sampling from the extended state space proposal corresponds to the
proceduredescribedinSec.2.2andAlg. 1,whichwewriteas12
 
q SMC(cid:16) {sk t,ω tk}T t=,K 1,k=1(cid:17) :=(cid:89)T (cid:89)K q(cid:18) sk t (cid:12) (cid:12) (cid:12) (cid:12)sh 1:k t t− −1 1(cid:19) (cid:89)K q(cid:0) ω tk(cid:12) (cid:12)s1 1: :K t (cid:1)  (SMCExtendedProposal)
t=1 k=1 k=1
π˜t(cid:18) sh 1¯ :i t t(cid:19)
where∀k, q(cid:0) ω tk =i(cid:12) (cid:12)s1 1: :K
t
(cid:1) := (cid:80)Kπ˜t−1(cid:18) sh 1:i t t− −1 1 π˜(cid:19) tq (cid:18)(cid:18) ss
h
1¯i t
:κ t
t(cid:12) (cid:12) (cid:12) (cid:12) (cid:19)sh 1:i t t− −1 1(cid:19) =
(cid:80)K
κw =t 1(cid:16) ws th 1¯ (cid:16):i t t s(cid:17)
h 1¯ :κ t t(cid:17)
(74)
κ=1 π˜t−1(cid:18) sh 1:κ t t− −1 1(cid:19) q(cid:18) sκ t (cid:12) (cid:12) (cid:12) (cid:12)sh 1:κ t t− −1 1(cid:19)
12Notethathk,sh¯k t,andshk t aredeterministicallyconstructedfrom{sk,ωk}T,K duringsampling,andsimplytrackthequantities
t 1:t 1:t t t t=1,k=1
tobecalculatedwhenevaluatingdensities.
37ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Torecountthedescriptionabove,notethatthenexttokensi inindexiissampledfromtheproposal,conditionedonthe
t
prefix
shi
t−1 . We concatenate these tokens
sh¯i
t =
shi
t−1 +[[[si]]] ( Eq. (Sequence Notations)) and calculate importance
1:t−1 1:t 1:t−1 t
weights. Weperformresamplingineachindexk accordingtoq(ωk|s1:K), or SNIS withthecalculatedweights(asin
t 1:t
Eq.(7)). Finally,afterresampling,weclonethesequenceinthechosenindexωk intoindexkandproceedtosamplesk
t t+1
withanprefixdefinedbytheindiceshk =hω tk +[[[ωk]]].
t t−1 t
WorkedExample: Tomakethismoreconcrete,weprovideaworkedexampleoftheprocedureinFig.4(a). Atstept=1,
weresamplethetokensk=2twice(forindicesk =1,3),withω1 =ω3 =2(andinindex2,setω2 =3tosamples3). We
t=1 1 1 1 1
recordtheprefixhistoryas,forexample,h1 =h3
=[[[ω1]]]=[[[2]]],whichcorrespondstosh1
1 =s2.
1 1 1 1 1
At step 2 in (a), we proceed to sample s1 ∼ q(s
|sh1
1 = [[[s2]]]) (and similarly s3 ∼ q(s
|sh3
1 = [[[s2]]])), whereas s2 ∼
2 2 1 1 2 2 1 1 2
q(s
|sh1
1 = [[[s3]]]). We next evaluate the importance weights over three concatenated sequences:
sh¯1
1 = [[[s2]]] +[[[s1]]],
2 1 1 1 1 2
sh¯2
1
=[[[s3]]]+[[[s2]]],andsh¯3
1 =[[[s2]]]+[[[s3]]],emphasizingthatsk isthefinaltokenineachindex. Shownintheredcircles,we
1 1 2 1 1 2 2
proceedtoresampleω1 =2,ω2 =3,andω3 =2atstept=2.
2 2 2
Finally,weneedtobacktracktoobtainthehistoryoftheindicesforthesequencetobeclonedinresampling. Namely,for
index1whereωk=1 =2,weconcatenatehω 21 +[[[ω1]]]=h2+[[[2]]]=[[[3,2]]]=:h1(i.e. thehistoryfortime2,index1). This
t=2 1 2 1 2
listofindicesspecifiestheprefixsh1
2 =[[[s3,s2]]]atstept=3,indexk =1. Similarreasoningappliesforotherindices.
1:2 1 2
ExtendedStateSpaceTarget Wearefinallyreadytospecifytheextendedstatespacetargetdistribution. Thecrucial
h1 h1
differenceistoidentifyasinglesequences T oflengthT (thechoiceofindex1isarbitrary). Thissequences T willbe
1:T 1:T
h1
evaluatedundertheunnormalizedtargetdistributionπ˜ (s )=σ˜(s )orexactlysampledfromthetargets T ∼σ(s )
T 1:T 1:T 1:T 1:T
intheextendedstatespacetargetdistribution.
Inparticular,webeginbysamplingafullsequenceofindices{j }T uniformlyatrandomPr(j ,j ,...j ) = (1/K)T.
t t=1 1 2 T
Settingω1 =j ,weletωjt =j forallt. Thisimpliesthefollowing,
T T t−1 t−1
ω1 =j , ωjt =j =⇒ h1 =[[[j ,j ,...j ]]], hjt =[[[j ,j ,...j ]]], (75)
T T t−1 t−1 T 1 2 T t−1 1 2 t−1
and h¯jt =hjt+1 (76)
t t
Toshowtheseidentities,notethatωjt =j andEq.(IndexNotation)implyhjt =hω tj −t 1+[[[ωjt ]]]=hjt−1+[[[j ]]]=
t−1 t−1 t−1 t−2 t−1 t−2 t−1
h¯jt−1, which matches Eq. (76). Applying this recursion again yields hjt = hjt−2 +[[[j ,j ]]]... = [[[j ,j ,...j ]]].
t−1 t−1 t−3 t−2 t−1 1 2 t−1
Takentogether,thesenotationsallowustointerleaveatruetargetsampleinparticularindices{j },guaranteeingthatat
t
leastonetargetsamplesappearsateachstep.
h1
Theextendedstatespacetargetdistributiondiffersfromq initshandlingofthissequence,whichidentifiedass T with
SMC 1:T
prefixesshj t−t
1 usingEq.(75). Notingthatsampling{j }T amountstospecifyingaparticularsetofωk asinEq.(75)-(76),
1:t−1 t t=1 t
 
σ˜ SMC(cid:16) {sk t,ω tk}T t=,K 1,k=1(cid:17) =Pr(j 1,j 2,...j T) π˜ T(cid:16) sh 1:1 T T(cid:17)(cid:89)T 

(cid:89)K q(cid:18) sk
t
(cid:12) (cid:12) (cid:12) (cid:12)sh 1:k t t− −1 1(cid:19) (cid:89)K q(cid:0) ω tk(cid:12) (cid:12)s1 1: :K
t
(cid:1) .
(cid:124) (cid:123)(cid:122) (cid:125)
t=1 k=1 k=1
( K1)T k̸=jt k̸=jt+1
(SMCExtendedTarget)
Note,thenormalizationconstantofσ˜ (S)isequaltoZ sinceonlyπ˜ (s )=σ˜(s )isunnormalized.
SMC σ T 1:T 1:T
TodescribeancestralsamplingfromEq.(SMCExtendedTarget),wefirstsample{j }T uniformlyasabove,andplacean
t t=1
exacttargetsequenceinindicessh1 T (or,equivalently,sequentiallysamplesjt ∼π (s |shj t−t 1 ). Ateachstep,theremaining
1:T t t t 1:t−1
K−1indicesk ̸=j aresampledfromtheproposal. Forresampling,wefixindexj toholdtheexactsampleandresample
t t
theremainingK−1indices.
Notethattheresamplingweightsq(cid:0) ωk(cid:12) (cid:12)s1:K(cid:1)
inEq.(74)includetheexactsample,which
t 1:t
maybeclonedadditionaltimesintoindicesotherthanj ifitsimportanceweightsarehigh. Theprocedureabovesimply
t
ensuresthatatleastoneexactsequenceissampled. SeeAlg.2forthepseudocodeofthealgorithm.
38ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
NotethatMaddisonetal.(2017,Alg. 2)presentsadifferentSMCextendedstatespacetargetdistributionthanours. Intheir
work,j =1andtheysamplej ,whileinoursj =1andwesamplej . However,bothtargetsresultinthesame
1 2:T+1 T+1 1:T
logpartitionfunctionbounds.
WorkedExample: InFig.2(c),weusebluecirclesandarrowstohighlighttheexact-sampleindicesh1 =[[[j ,j ]]]=[[[3,2]]]
T 1 2
andthetargetsequencesh1 T =[[[s3,s2]]]. Usingtherecursionωjt =j withj =j =1fixed,wemayalsoexpress
1:T 1 2 t−1 t−1 T+1 3
h1 =[[[j ,j ]]]=[[[3,2]]]=[[[ω2,ω1]]]. Atstep2,notethetargetsequenceissampled/evaluatedanadditionaltimeinindex3.
T 1 2 1 2
ImportanceWeightsintheExtendedStateSpace Assumewearegivenafixedsetof{sk,ωk}T,K ,whichmaybe
t t t=1,k=1
sampledfromeitherσ (S)orq (S). Weproceedtoshowthattheunnormalizedimportanceweightsintheextended
SMC SMC
statespacesimplifyasfollows.
Lemma F.1. For the extended state space target σ˜ and proposal q above, the simple importance weights in the
SMC SMC
extendedstatespacebecome
(cid:16) h¯k(cid:17)
σ˜ SMC(cid:16) {sk,ωk}T,K (cid:17) =(cid:89)T 1 (cid:88)K π˜ t s 1:t t =(cid:89)T 1 (cid:88)K w (cid:16) sh¯k t(cid:17) =:(cid:89)T 1 (cid:88)K w (sk )
q SMC t t t=1,k=1 t=1K k=1 π˜ t−1(cid:16) sh 1:k t t− −1 1(cid:17) q(cid:16) sk t (cid:12) (cid:12) (cid:12)sh 1:k t t− −1 1(cid:17) t=1K k=1 t 1:t t=1K k=1 t 1:t
(77)
whichcanbeusedtoobtainunbiasedZ estimators(Eq.(8))orboundsonlogZ (Prop.5.1,withproofbelow).
σ σ
Proof. Toevaluatetheimportanceweights(withthegoalofestimatingZ ),weconsider
σ
σ˜ qSMC(cid:16)
{sk t,ω tk}T t=,K
1,k=1(cid:17)
=
(cid:0) K1(cid:1)T π˜ T(cid:16) sh 1:1 T T(cid:17) (cid:34)(cid:81)T t=1(cid:34) (cid:81) (cid:18)K kk ̸== j1 (cid:12)tq(cid:18) sk t (cid:19)(cid:12) (cid:12) (cid:12) (cid:12)sh 1:k t t− −1 1(cid:19) (cid:81)K k̸=k j= t1 +1q(cid:0) (cid:35)ω tk(cid:12) (cid:12)s1 1: :K t (cid:1)(cid:35)
(78)
SMC (cid:81)T (cid:81)K q sk(cid:12) (cid:12)shk t−1 (cid:81)K q(cid:0) ωk(cid:12) (cid:12)s1:K(cid:1)
t=1 k=1 t (cid:12) 1:t−1 k=1 t 1:t
( =1)(cid:18) K1 (cid:19)T π˜ T(cid:16) sh 1:1 T T(cid:17) t(cid:89) =T 1q(cid:18) sj tt(cid:12) (cid:12) (cid:12) (cid:12)sh 1:j t t−t −1 1(cid:19)1 q(cid:16) ω tjt+1(cid:12) (cid:12) (cid:12)s1 1: :K
t
(cid:17) (79)
wherein(1),notethattermsinthedenominatorcancelexceptfortheindices[[[0,j ,...j ]]]=h1. Recallingthatωjt+1 =j
1 T T t t
fromEq.(76),weexpandtheresamplingweightsq(j
|s1:K)forthesequenceindexedbysjt,shj t−t
1
,andsh¯j tt
,
t 1:t t 1:t−1 1:t−1
(cid:80)K
π˜t(cid:32) sh 1¯ :k t t(cid:33)
( =2)(cid:18) 1 (cid:19)T
π˜
(cid:16) sh1 T(cid:17)(cid:89)T k=1 π˜t−1(cid:32) sh 1:k t t− −1 1(cid:33) q(cid:32) sk t(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)sh 1:k t t− −1 1(cid:33)
(80)
K T 1:T t=1 (cid:24)q(cid:18) (cid:24)sj tt(cid:24)(cid:12) (cid:12) (cid:12) (cid:12)s(cid:24)h 1:j t t(cid:24) −t −1 1(cid:24)(cid:24)(cid:19)
π˜t−1 sh 1:j t
t−tπ −˜t
1
1(cid:32)

s (cid:8)qh 1¯

:j t tt (cid:8)s(cid:33)
j tt(cid:8)(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)sh 1(cid:8) :j t t−t
−(cid:8)
1
1(cid:8)


Finally,weobtainatelescopingcancellationofπ˜ termsusingtheindexingidentitiesinEq.(75)-(76). Inparticular,since
t
h¯jt =hjt+1 andh¯jt−1 =hjt withh¯jT =h1,wecansimplifythetermsinEq.(80)as
t t t−1 t−1 T T
(cid:8)(cid:8) (cid:8)(cid:8)
π˜T(cid:18) sh 1:1 T T(cid:19) t(cid:89) =T 1π˜t π− ˜t1 (cid:32)(cid:32) ss h 1¯1h :j t: tj t t t−t − (cid:33)1 1(cid:33) =π˜T(cid:32) sh 1¯ :j T TT(cid:33) t(cid:89) =T 1π˜t− π˜1 t(cid:32) (cid:32)s sh 1 h 1¯ ¯: :j t j tt t−t t−− (cid:33)1 11(cid:33) = (cid:8)π˜T(cid:8)(cid:32) s(cid:8)h 1¯ :j T TT(cid:8)(cid:8)(cid:33) (cid:8)π˜T (cid:8)(cid:8) π− ˜T1 (cid:8)(cid:8)  (cid:32) ss (cid:8)(cid:8) h 1h 1 ¯¯ :: j Tj T TT TT (cid:8)(cid:8) − − (cid:8)− (cid:33)1 11  (cid:8) ππ˜ ˜TT (cid:8)(cid:8) −− 12 (cid:8)(cid:8)    ss (cid:8)(cid:8) hh 11 ¯¯ :: jj TT TT TT (cid:8)(cid:8) −− −− −− 12 12 12 (cid:8)(cid:8)    ... (cid:8)π˜1(cid:32) (cid:8)s1 h 1¯(cid:8) :j 1 11(cid:8)(cid:33) =1
(cid:8)
usingtheassumptionthatπ˜ (·)=1. SimplifyingfromEq.(80),thefinalunnormalizedimportanceweightsbecome
0
σ˜ q SS MM CC(cid:16) {sk t,ω tk}T t=,K 1,k=1(cid:17) = t(cid:89) =T 1K1 k(cid:88)K
=1 π˜
t−1(cid:18)
s
1h :k
t t−
−π˜
1
1t(cid:16) (cid:19)s qh 1¯ (cid:18):k t t s(cid:17)
k
t
(cid:12) (cid:12)
(cid:12)
(cid:12)sh 1:k
t t− −1
1(cid:19) = t(cid:89) =T 1K1 k(cid:88)K =1w t(cid:16) sh 1¯ :k t t(cid:17) =: t(cid:89) =T 1K1 k(cid:88)K =1w t(sk 1:t) (81)
39ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
asdesired,whereweabbreviatetheimportanceweightsasw (sk )forsimplicityofnotation. Notethatwealsoobtainan
t 1:t
unbiasedestimateofthepartitionfunctionvia
Z =E
(cid:20) σ˜ SMC(S)(cid:21)
=E
(cid:34) (cid:89)T 1 (cid:88)K
w
(cid:16)
sk
(cid:17)(cid:35)
σ qSMC(S) q (S) qSMC(S) K t 1:t
SMC t=1 k=1
Proposition5.1. (BidirectionalSMCBounds) ThelogpartitionfunctionlogZ ofatargetdistributionσ(s )canbe
σ 1:T
lowerandupperboundedby
(cid:34) T K (cid:35)
E log(cid:89) 1 (cid:88) w (cid:0) si (cid:1) ≤logZ
q SMC(S) K t 1:t σ
t=1 i=1
(24)
(cid:34) T K (cid:35)
logZ ≤E log(cid:89) 1 (cid:88) w (cid:0) si (cid:1) .
σ σ SMC(S) K t 1:t
t=1 i=1
ThegapinthelowerboundisD (q (S)∥σ (S)),andthegapintheupperboundisD (σ (S)∥q (S)).
KL SMC SMC KL SMC SMC
Proof. TheprooffollowsdirectlyfromBrekelmansetal.(2022)App. A,whereitisshownthatforσ (S),q (S)such
ext ext
thatZ =E [σ˜ext(S)],wecanconstructlowerandupperboundsonlogZ
σ qext(S) qext(S) σ
(cid:20) (cid:21) (cid:20) (cid:21)
σ˜ (S) σ˜ (S)
D (q (S)∥σ (S))+E log ext =logZ =E log ext −D (σ (S)∥q (S)) (82)
KL ext ext qext(S) q (S) σ σext(S) q (S) KL ext ext
ext ext
(cid:20) (cid:21) (cid:20) (cid:21)
σ˜ (S) σ˜ (S)
E log ext ≤logZ ≤E log ext (83)
qext(S) q (S) σ σext(S) q (S)
ext ext
wherethegapinthelowerandupperboundsareD (q (S)∥σ (S))andD (σ (S)∥q (S)),respectively.
KL ext ext KL ext ext
SubstitutingourSMCprobabilisticinterpretationinEq.(SMCExtendedProposal)andEq.(SMCExtendedTarget),along
withtheimportanceweightsinLemmaF.1,intoEq.(83)yieldsthedesiredboundsinEq.(24).
IWAEasaSpecialCaseofourSMCProbabilisticInterpretation NotethatwerecoverIWAE(orSISoverK samples)
from SMC with no intermediate resampling. In particular, this corresponds to ωk = k for all t < T, with importance
t
weightingfromresamplingoccurringatthefinalstep(cid:81)K q(ωk|s1:K). Thisyieldsthe1/K averageinsidetheloginthe
k=1 T 1:T
IWAEbounds(i.e.,SMCwithonlyoneresamplingstepatt=T). Whiletheimportanceweightsarecrucialtoconstructthe
bound,notethat‘resampling’isnotnecessaryatthefinalstepandwemayreturnallK samplesalongwiththeirweights.
ViewingIWAEasaspecialcaseofourSMCprobabilisticinterpretationiscomplementarytotheinterpretationsinDomke
&Sheldon(2018);Brekelmansetal.(2022)andalsoprovidesupperbounds(Sobolev&Vetrov,2019).
G.AdditionalExperimentDetails
G.1.CommonDetailsAcrossExperiments
Forallexperiments,weusetheAdamoptimizerwithβ ,β ={0.9,0.999}. WeusecustomimplementationsofSMC.For
1 2
PPO,weusetheHuggingFaceTRLPPOTrainer(https://github.com/huggingface/trl/blob/main/trl/trainer/
ppo trainer.py), modified slightly to accomodate our custom twist parameterizations, as described below. For other
methods, we use Optax (Flax) and custom loss functions. We use HuggingFace models (https://huggingface.co/
models)forthebasep modelsandbuildcustomlayersontopofthose.
0
Forthetwistψθ(s ),wealwaysparameterizelogψθ(s )fornumericalstability.Wechooserandomnormalinitializations
t 1:t t 1:t
centeredatmean0,withlowvariance,13suchthatlogψθ(s )≈0,ψθ(s )≈1atthebeginningoftraining,whichmeans
t 1:t t 1:t
theinitialsequencesgeneratedbythetwist-inducedproposalapproximatelycomefromthebasemodelp . Allmethods
0
13WespecificallyuseaformofXavierinitialization,takingthevarianceas 2 .
ninputs+noutputs
40ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
areinitializedusingthesamerandomseeds,andthusstartfromthesameparametervalues. SeeApp.G.2foradditional
discussionofchoicesforthetwistparameterization.
Formethodsthatdirectlylearnaproposal(DPGandPPO),wecoulddirectlyfinetunealanguagemodelthatoutputsq(s ).
1:t
However,inordertoensureconsistencyintermsofmodelcapacityandeaseoflearningcomparedtoourtwistedproposals,
we instead have these proposal learning methods output a modifier logψθ(s ) which is added to the base model log
t 1:t
probabilitylogp (s ). Notethatusingrandomnormalinitializationscenteredatmean0withlowvariance,thisscheme
0 1:t
resultsininitialqsamplescomingapproximatelyfromp .
0
Formethodsthatcanmakeuseofexactposteriorsamples,whenwehaveaccesstothem(Sec.7.2.3,App.H.3),weusethem.
ThisisstraightforwardformethodslikeDPG,SIXO,andourCTL(unlesswehaveonlyasinglesample,aswediscussfor
infillinginApp.G.4). ForourRLtwistlearning,wefoundthebestempiricalperformancetrainingonacombinationofq
andexactσsampleswhentheywereavailable(asopposedtojustqotherwise),andusethoseresults. Similarly,forFUDGE,
whenexactσsamplesareavailable,weusethemtogetherwithp samples.
0
ItisnotstraightforwardtocomparePPOversusothermethods,becauseoftheinnerloopinPPOthatrepeatsseveralclipped
gradientstepsonagivensetofsamples. Thismeansthat,foraconstantnumberofsamples,PPOmakesmoregradient
updatesthanothermethods, whileforaconstantnumberofgradientupdates, PPOseesfewersamples. Ultimatelywe
decidedtonormalizebasedonthenumberofsamplesseen;weconsidereachouterstep(includingafullPPOinnerloop,in
ourexperiments,4gradientsteps)asasingle“gradientupdate.” Wemakethischoicesincesamplingisthemainbottleneck
intermsofcomputationalcost,andthenumberofinnerPPOstepsisahyperparameterwhichwedidnottune.
AllofourexperimentswererunonasingleGPU,usuallyonanNVIDIAA40with48Gmemory. Allexperimentstookno
longerthan9wall-clockhourstorunforasinglelearningmethod,withinfilling(Sec.7.2.3)experimentstakinglongest;
mostotherexperimentstooknolongerthan4hours.
G.2.ChoicesofTwistParameterization
Thechoiceofparameterizationforthetwistlogψθ(s )isadesigndecision,independentofouroverallframework. While
t 1:t
onecouldkeepanentirelyseparatemodelforeachlogψθ(s ),thisislikelytobememory-inefficientandlearnslowly.
t 1:t
Instead,weuseasharedparameterizationacrosss ,inthesamewaythatthebaselanguagemodelusesasinglearchitecture
1:t
tooutputprobabilitydistributionsovertokensateachtimestept. Welayoutparameterizationchoicesweconsideredbelow.
G.2.1.LINEARHEAD
Thesimplestchoiceistoreplacethelinearheadofthebaselanguagemodelwithanewlinearhead,keepthebasemodel
fixed,andonlytrainthelinearhead. Thisparameterizationincursverylittleadditionalcomputationcostcomparedtojust
usingthebaselanguagemodel. However,wefoundthistobecapacityconstrainedinourexperiments,achievingworseKL
divergencesthanotherparameterizations.
G.2.2.MLPHEAD
Insteadofalinearhead,weconsidera3-layerfullyconnectedneuralnetwork(MLP)withReLUnon-linearitiesasahead
ontopofthebaselanguagemodel. Thebasemodelisstillkeptfixed; onlytheMLPheadistrained. Thisincursmore
computationalcostthanalinearhead(App.G.2.1),buttheadditionalcostisstillsmallrelativetothecostofaforward
passthroughthebasetransformermodel. Wefoundthistogenerallyperformwellinourexperiments,soweuseitforthe
toxicitythresholdexperimentinSec.7.1andsentimentinSec.7.2.2.
G.2.3.SEPARATETRANSFORMERFORTHETWIST
Wecanalsoconsideranentirelyseparatetransformerthatoutputsonlythetwistvalue. Thatis,wecopythebasemodel,
andrepurposeittooutputatwistvaluelogψθ(s )insteadoflogitsfornext-tokenprobabilities. Wethentraintheentire
t 1:t
networkend-to-end. Thisissignificantlymorecomputationallycostlythantheformerapproaches,anddoesnotalwaysdo
betterthanjustanMLPhead(App.G.2.2),sowegenerallydonotrecommendusingthis. Still,wefoundittoperformwell
intoxicityclassificationinSec.7.2.1,soweuseitthere.
G.2.4.SEPARATETRANSFORMERFORTHETWIST,WITHMLPHEAD
This is similar to App. G.2.3, except we also replace the final linear head with a MLP head as in App. G.2.2. The
model outputs logψθ(s ) and is trained end-to-end. This is the most computationally costly approach outlined here,
t 1:t
and is unnecessary for most of our settings. However, in infilling with 15 generated tokens (Sec. 7.2.3) we found this
41ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
parameterizationtoperformmateriallybetterthanallothers,particularlywithDPG(App.E.3),soweuseitforallinfilling
experiments.
WithboththisparameterizationandApp.G.2.3,weincreasecomputationtimebyafactorofaround2ontheforwardpass,
andsignificantlyincreasememoryandtimeusageonthebackwardspassduringtraining(thoughsamplingisstillthemain
timebottleneck). Whetherthisparameterizationisworththepotentialgaininperformancedependsonthedesiredusecase.
Weemphasizethatouroverallframeworkisindependentofthechoiceofparameterization.
G.3.CommentsonOurChoicesofExperimentSettings
Our settings and evaluation metrics in Sec. 7 are chosen to highlight our scientific findings. In particular, the toxicity
thresholdexperimentinSec.7.1demonstratestheimprovementofSMCoverSISwiththebasemodelwithCTLlearned
twists.Inordertohighlightthisdistinction,wehavechosenasettingwhereitisextremelydifficulttodrawsamplessatisfying
thethresholdusingthebasemodelp (seeSIS/IWAELBlineinFig.3).
0
However, twist-learninginthetoxicitythresholdsettingpresentschallenges. Forapproximatepositivesamplinganda
thresholdedtarget,allimportanceweightswillbe0ifnoneofourK samplesmeetthethreshold. Asnotedabove,sampling
fromp ,ortheSMC/twistedproposalforψθ(s )≈1atinitialization,isextremelyunlikelytodrawsamplesmeetingthe
0 t 1:t
threshold(i.e.,withinthesupportofthetarget)inthesettingofSec.7.1. Asaresult,initialiterationsoftwistlearning
receivenolearningsignaluntilathresholdedpositivesampleisdrawnfromthebasemodel.
To avoid this difficulty for baselines comparisons in Sec. 7.2, we instead focused on settings with ϕ(s ) given by
1:T
probabilities. Nevertheless,wenotethattherearenofundamentaldifferencesbetweenthesettingsconsideredinSec.7.1
andSec.7.2. Thus,wemayalsoevaluatesingle-sampleD (σ∥q)andD (q∥σ)inthesettingofSec.7.1,orplotlogZ
KL KL σ
boundsasafunctionofK inforthesettingsinSec.7.2.
G.4.Experiment-SpecificDetails
DetailsforSISandSMCComparison(Sec.7.1) Wegenerate10outputtokens,andtraintwistsusingSec.4.1with
approximatepositivesamplingasdiscussedinSec.4.1.2.
Notethatusingσ(s ) ∝ p (s )I[s ∈ C]whereC := {s |r(s ) ≤ η}directlyrunsintonumericalissuesfor
1:T 0 1:T 1:T 1:T 1:T
calculatinglogZ whens ∈/ C andI[s ∈C]=0. Weinsteaduseϵ+I[s ∈C]everywhereinsteadofI[s ∈C],
σ 1:T 1:T 1:T 1:T
whereϵ=10−16. InFig.3,thisyieldsaSIS/IWAElogZ LB≈−36whennosamplesaredrawnthatfallinthesetC.
σ
WeuseanMLPheadtoparameterizethetwist,asinApp.G.2.2,with768hiddenunitsperlayer,matchingtheTinyStories
model’sembeddingdimension. Weuseabatchsize(numberofSMCparticles/samples)of1000,withalearningrateof
0.0001,andtrainusingCTLforatotalof5000gradientupdates. Wedidnottunehyperparametersbecausewefoundthis
settingtoworkwell,andwearenotcomparingacrossdifferentlearningmethods.
ForeachpointoneachlineonFig.3,werunSISorSMC20times,eachwithadifferentrandomlyselectedtrueposterior
samplefortheupperbounds. Thelineshowstheaveragevalueacrossthese20runs,whiletheshadedareashows95%
confidenceintervals. SeealsoApp.G.1fordetailscommonacrossexperiments.
DetailsforToxicity(Sec.7.2.1) Wegenerate20outputtokens. Weparameterizethetwistwithaseparatenetworkasin
App.G.2.3. Weuseabatchsize(numberofSMCparticles/samples)of100,andtrainforatotalof2048gradientupdates.
Foreachlearningmethod,weusedacoarsegridsearchoverlearningratesbetween0.000001and0.001,usingthebestone
found,whichwasusually0.00003or0.0001. Weruneachlearningmethodover5differentrandomseeds,reportingthe
averageKLdivergenceand95%confidenceintervalsoverthese5seeds.
ForeachKLdivergenceevaluation,wefirstgetsandwichboundsonlogZ aslaidoutinSec.5,usingthelearnedtwistsfor
σ
thetwistedproposalwithK =500samples. WefindSIS/IWAEandSMCboundstobesimilarlytight,souseSIS/IWAE
forsimplicity. Wedothis4times,providing4upperboundestimatesand4lowerboundestimates,andtaketheaverage
midpointasthelogZ estimateforeachexperiment. Wethentakethemedian(acrossalllearningmethodsandseeds)of
σ
theseestimates,andusethatasourestimateoflogZ . ThisisthenusedasacommonvaluefortheKLdivergenceacross
σ
allmethodsandseeds,whichcontrolsforpossiblenoiseinlogZ boundsandensuresafaircomparisonacrossmethods.
σ
Wegenerallyhavetightbounds(upperbound≈lowerbound),whichsuggestourlogZ estimatesaregenerallyaccurate,
σ
butnotethatanyinaccuraciesinestimatinglogZ wouldonlyaffecttheabsolutevaluesoftheKLdivergences,notthe
σ
relativedifferencesamongdifferentlearningmethods.
42ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
WeestimateexpectationsinEq.(23)with2000samplesfromqand2000exactposteriorsamplesforσ. With2000samples,
ourestimateshave95%confidenceintervalsgenerallybetween0.05and0.10,suggestingthatourestimatesofexpectations
areunlikelytobeoffbymorethan0.10. Theexactposteriorsampleswerecollectedoffline;suchalargenumberofsamples
takesseveralhourstocollect,andinpracticalsettings,wewouldlikelyonlybeabletocollectamuchsmallernumberof
samples. Allourmethodsstillapplywithfewerexactposteriorsamples,butthevarianceinestimateswillbehigher. See
alsoApp.G.1fordetailscommonacrossexperiments.
Details for Sentiment (Sec. 7.2.2) We generate 10 output tokens. We parameterize the twist using an MLP head
(App.G.2.2),with1024hiddenunitsperlayer,matchingtheGPT2Mediummodel’sembeddingdimension. Otherdetails
arethesameasfortoxicityabove. Collectingexactposteriorsamplesislesstimeconsuminginthiscase(lessthananhour).
SeeApp.G.1forcommonexperimentaldetails.
DetailsforInfilling(Sec.7.2.3) WeparameterizethetwistusingaseparatetransformerwithanMLPhead(App.G.2.4),
with 768 hidden units per layer (matching the TinyStories model’s embedding dimension). We make the following
adjustmentstotheforwardpassofthelanguagemodelfortheconditionaltwistsetting. Insteadoftakinginonlys ,the
1:T
modeltakesinboths ands andpasseseachseparatelythroughthebody(everythingexceptthehead). Thus,
1:T T+1:T+c
s canbeseenasasecondprompt. Fors ,wetaketheembeddingsproducedafterthelastconditioning
T+1:T+c T+1:T+c
tokens hasbeenprocessed,broadcastitacrosstimesteps1 : T,andpassthatasadditionalinputtotheMLPhead
T+c
(concatenatedwithembeddingsfors ateacht∈1...T). ThisallowstheMLPheadtoproducedifferentoutputdepending
1:T
ontheconditioningtokens.
Since we are in the conditional target distribution setting (Sec. 3.3), with o = s , to compare across learn-
T T+1:T+c
ing methods using a single quantity, we estimate E [D (q ∥σ )] := E [D (q(s |o )∥σ(s |o ))] and
oT KL oT oT oT KL 1:T T 1:T T
E [D (σ ∥q )]:=E [D (σ(s |o )∥q(s |o ))]whereE [·]:=E [·]forinfilling. Notethat,
oT KL oT oT oT KL 1:T T 1:T T oT p0(sT+1:T+c)
(cid:20) (cid:20) (cid:21)(cid:21)
q(s |o )
E [D (q(s |o )∥σ(s |o ))]=E E log 1:T T +E [logZ (o )]
oT KL 1:T T 1:T T oT q(s1:T|oT) p (s )ϕ(s ,o ) oT σ T
0 1:T 1:T T
(cid:20) (cid:20) (cid:21)(cid:21)
p (s )ϕ(s ,o )
E [D (σ(s |o )∥q(s |o ))]=E E log 0 1:T 1:T T −E [logZ (o )]
oT KL 1:T T 1:T T oT σ(s1:T|oT) q(s |o ) oT σ T
1:T T
(cid:104) (cid:105) (cid:104) (cid:105)
whereforafixedo ,E log q(s1:T|oT) andE logp0(s1:T)ϕ(s1:T,oT) maybeevaluatedasbefore,
T q(s1:T|oT) p0(s1:T)ϕ(s1:T,oT) σ(s1:T|oT) q(s1:T|oT)
similartotheunconditionalsetting. Inparticular,forourexperiments,weuse1-sampleestimatesoftheseexpectations,as
wehaveasingleexactsamplefromσ(s |o )bytheBDMCtrick(Sec.3.3),andwechoosetodrawasinglesamplefrom
1:T T
theconditionalproposalq(s |o ). Weaveragethisover2000o ∼p (s ),approximatingtheouterexpectation,
1:T T T 0 T+1:T+c
givingusa2000-sampleestimateof1-sampleestimatesforthefirsttermintherighthandsideofbothequationsabove.
With2000samples,ourestimateshave95%confidenceintervalsgenerallybetween0.20and0.30.
NotethatE [logZ (o )]isindependentofthelearningmethodorproposalq,unlikethefirsttermwediscussedabove.
oT σ T
Thus,inordertosavecomputationandprovideuswithamoreaccurateestimateofE [logZ (o )],weestimatethisterm
oT σ T
onlyonce. Specifically,weconsideronlythelearningmethodwiththelowestKLdivergence(DPG),anduseSIS/IWAE
bounds. Foreacho ,weestimatelogZ (o )withK = 500samples,whichgivesusrelativelytightsandwichbounds,
T σ T
againtakingthemidpointasourestimate. Weaveragethisover1000o ∼p (s ),givingusa1000-sampleestimate
T 0 T+1:T+c
ofE [logZ (o )],whereeachlogZ (o )isitselfestimatedvia500samples.
oT σ T σ T
Fornegativesamplingwithcontrastivetwistlearning(CTL)inthissetting,weneedatleast2negativesamplespersetof
conditioningtokenso =s toperformSISreweighting;thisisincontrastwithothertwistlearningmethodswhich
T T+1:T+c
cangenerateasinglenegativesamplepero . Forthepositivesample,wecanuseoursingleexactsampledirectly,orwe
T
canruntheSMCupperboundsamplingprocedure(“Samplingfromσ forSMCUpperBounds”sectioninSec.5.2)
SMC
generatemoreapproximateσsamplesusingthegivenexactsample. Wefindthelattertogenerallyperformslightlybetter
thantheformer,soadoptthatforourinfillingexperiments.
Weuseafixedbatchsizeof100acrossallmethodsfortrainingtwists. Toclarifythemeaningofthisbatchsize,formethods
otherthanCTL,wehave100drawsofexactσ samples,eachforadifferentsetofconditioningtokenso = s ,
T T+1:T+c
sowetrainover100differento atatimeusing1negativesamplepero . ForCTL,sinceweneedatleast2negative
T T
samplespero ,wesplitthebatchsizeof100acrossthenumberofdifferento andthenumberofnegativesamplesper
T T
o ,asanadditionalhyperparameter. Weuse25o with4negativesamplespero fortheexperimentsinSec.7.2.3and
T T T
43ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Table6: QualitativeResults-ReviewsVeryLikelytobeofaParticularRating
Class(Rating) MostTextGeneratedUsingTwistedSMC
1-star “Iboughtthissuckerformywifetouseonherpythonthatshesentmelastyear.Itwasterrible!”
2-star “Iboughtthisthroatraiserforcombatingdentalcaries.Ididn’treallylikeit.Ididn’tlike”
3-star “Iboughtthisafewmonthsback,andIenjoyediteverytimeIheldit.I’mgiving3stars”
4-star “Iboughtthisproductafewmonthsagoandhavereallyenjoyedit.OnlyreasonIgaveit4starsisbecause”
5-star “Iboughtthisphonerecently,andI’vebeenlovingit!Gorgeousdesign,outstandingbatterylife,fantasticcamera”
Table7: QualitativeResults-InfillingExamples
Proposal Prompt(s0) GeneratedTokens(s1:T) ConditioningTokens(sT+1:T+c)
DPG Onceuponatime,therewasa littlegirlnamedMia.Shehadabigheart.Mialovedtohelp othersandmakethemfeelsafe.Mialikedto
SIXO Onceuponatime,therewasa girlnamedMia.Miawasverykindandcompassionate.Shealwayshelpedher othersandmakethemfeelsafe.Mialikedto
CTL Onceuponatime,therewasa girlnamedMia.Shehadathin,pinkdress.Mialikedto othersandmakethemfeelsafe.Mialikedto
10o with10negativesamplespero fortheexperimentsinApp.H.2. Controllingforbatchsizeinthiswayisarguably
T T
disadvantageousforCTLcomparedtootherlearningmethods,asitlearnsonasmallernumberofo ,butthiscontrols
T
formemoryrequirements,andwefeelismorefairthancontrollingforthenumberofo seenbutallowingmorenegative
T
samplesforCTLrelativetoothermethods. Wetrainforatotalof5500gradientupdates. Foreachmethod,weusedacoarse
gridsearchoverlearningratesbetween0.000001and0.001,usingthebestonefound,whichwasusually0.0001or0.00003.
Weruneachlearningmethodover5differentrandomseeds,reportingtheaverageKLdivergenceand95%confidence
intervalsoverthese5seeds. SeealsoApp.G.1fordetailscommonacrossexperiments.
H.AdditionalExperimentalResults
H.1.QualitativeResults
ToxicityControlledGenerationwhenNoExactPosteriorSamplesareAvailable InSec.7.2.1wetargetedσ(s )∝
1:T
p 0(s 1:T)eβlogp(a|s1:T)withβ =1. Wecanalsotargetβ >1;higherβ producesamorepeakeddistributionoftextthatis
morelikelytobeofclassa. However,forβ ̸=1wecannolongergenerateexactposteriorsamplesandthuscannotupper
boundlogZ . Ourtwistlearning(Sec.4.1)withapproximatepositivesampling(Sec.4.1.2)canlearnmeaningfultwistsin
σ
thissetting,whichweillustratewithaqualitativeexampleofastory(200tokensupperlimit)andβ =10:
“Onceuponatime,therewasalittlegirlnamedLily.Shehadabigthumbthatshelikedtosuckon.Oneday,Lilywenttotheparktoplay
withherfriends.Shewashavingsomuchfununtilherthumbgotstuckinhershoe.Shetriedtopullitout,butithurttoomuch.
Lilystartedtocryandherfriendstriedtohelpher,buttheycouldn’tgetherthumbouteither.Shewasscaredanddidn’tknowwhattodo.
Herfriendstriedtohelpher,buttheycouldn’tgetitouteither.Sadly,Lilyhadtogotothehospitalandgetabigbandageonherthumb.
Shecouldn’tplaywithherfriendsanymore.Fromthatdayon,Lilyneverwenttotheparkagain.”
ThestoryiscoherentandfollowsthegeneralstyleoftheTinyStoriesbasemodel,whilehavingahighprobability(≈88%)
ofbeingtoxicaccordingtothetoxicityclassifier,likelyduetothepresenceofnegativewordssuchas‘suck’,‘hurt’,‘cry’,
and‘scared’. Thissupportstheabilityofourmethodstocontroloutputsbasedonthechosenposteriordistribution.
Sentiment Controlled Generation when No Exact Posterior Samples are Available As above, we also consider
σ(s 1:T)∝p 0(s 1:T)eβlogp(a|s1:T),whereβ >1,exceptnowp(a|s 1:T)isbasedonthesentimentclassifierinSec.7.2.2. In
Table6weprovidequalitativeexamplesshowing20tokensproducedwithtwistedSMCwith500particles,forβ =100,
usingtwiststrainedwithSec.4.1. Theseillustrateourframework’sabilitytolearnreviewsthatembodyeachratingclass.14
Infilling InTable7wecomparequalitativeresultsonanexamplesetofconditioningtokensforDPG,SIXO,andCTL
(inthatorder,toreflectincreasingKLdivergence). ThequalitativeresultscorrelatewiththequantitativemeasuresofKL
divergence;thelowestKLdivergence(DPG)correspondstoinfilledtokensthatrespectgrammarandthetopic. SIXO,which
hashigherKLdivergence,failstorespectgrammar. CTLgeneratesincorrectgrammarandislesson-topic,correspondingto
thehighestKLdivergenceamongthesemethods.
14Theresultsareslightlyincoherent;thisisaresultofthebaseGPT2-Mediummodeloftenbeingincoherent.Qualitatively,wefindthat
thesegenerationsaremorecoherentthantheuncontrolledonesfromp .
0
44ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Table 8: KL Divergences (averaged over conditioning tokens drawn from the base model) for Infilling Experiments
(Sec.7.2.3)with2OutputTokensand1ConditioningToken
ProposalqoT TwistLearning E oT[D KL(qoT∥σoT)] E oT[D KL(σoT∥qoT)]
Twisted Contrastive 0.47±0.10 0.25±0.01
Twisted RL 0.42±0.10 0.15±0.01
Twisted SIXO 0.47±0.11 0.25±0.02
Twisted FUDGE 2.62±0.33 0.90±0.02
DPG – 0.16±0.07 0.14±0.01
PPO – 0.52±0.04 1.09±0.34
H.2.InfillingwithFewerTokens
WeconsiderthesamesettingasSec.7.2.3butonlygenerating2tokens,conditionedon1token. WeshowKLdivergence
evaluationsinTable8. Ourevaluationrevealsinterestingdifferencesamonglearningmethods,eveninthiseasiersetting
wheremostmethodsachievelowKLdivergenceinbothdirections. DPGandRLlearnsbest,whileFUDGElearnsnotably
slower. PPOsuffersonD (σ∥q),thoughthismaybeunsurprisingsincePPOdoesnotmakeuseofexactσsamples.
KL
H.3.Approximatevs. ExactPosteriorSampling
Inourtoxicityandsentimentexperiments,wetrainusingapproximateσsamplestoreflectthemorecommonreal-world
settingwheretheamountofexactsamplesneededfortrainingarenotavailable. However,herewerunanadditionalablation
experimentforinsightintotheeffectofpositiveversusapproximatesampling. Weuserejectionsampling(Sec.4.1.2)to
generateexactposteriorsamplesfortraining. Thisismuchslowerthangeneratingapproximatesamples,soisnotapractical
strategyfortraining;weinvestigatethissolelyforunderstanding.
WeprovideacomparisonofKLdivergences(evaluatedthesamewayasinthemainpaper)whentrainingusingexactversus
approximateσsamplesforaselectionofmethodsthatperformedwellinourpreviousexperimentsandareabletomakeuse
ofσsamples. Toxicity(Sec.7.2.1)resultsareinTable9andsentiment(Sec.7.2.2)resultsareinTable10. Thefirsttwo
columnsofKLdivergencesareforexactσsamples. Thenexttwoarefortrainingonthesamenumberofsamples,butusing
approximatepositivesampling(Sec.4.1.2). Overall,foraconstantnumberofsamples,havingexactσsamplesimproves
performanceformostmethods. Notehoweverthatthereisanadditionaltimecostrequiredforrejectionsamplingtogenerate
exactsamples,sotheexactσtrainingrequiressignificantlymorewall-clocktimeforanygivennumberofsamples.
Wealsoplotthesingle-sampleKLdivergenceinbothdirectionsasafunctionoftrainingtimeforexactvs. approximate
sampling,ontoxicityandsentimentexperiments,inFig.5. Theapproximatesamplingresultsmatchthoseinthemain
paper(withdifferentcolors). Theexactσsampleresultscutoffearlierbecausethetimecostrequiredforrejectionsampling
reducesthenumberofgradientupdatesthatcanbemadeforagivenamountofwall-clocktime.
Table9: KLDiv. forToxicityExperiments(Sec.7.2.1),comparingexactσsamplesversusapproximatepositivesampling.
ExactσSamples Same#ofApprox.σSamples
Proposalq TypeofTwistLearning D (q∥σ) D (σ∥q) D (q∥σ) D (σ∥q)
KL KL KL KL
Twisted Contrastive 2.54±0.02 2.68±0.09 2.99±0.18 3.22±0.09
Twisted RL 3.23±0.10 3.24±0.04 3.48±0.15 3.49±0.13
Twisted SIXO 2.37±0.06 2.52±0.05 2.70±0.17 3.05±0.22
DPG – 1.51±0.01 1.50±0.01 2.35±0.15 2.48±0.10
Table10:KLDiv.forSentimentExperiments(Sec.7.2.2),comparingexactσsamplesversusapproximatepositivesampling.
ExactσSamples Same#ofApprox.σSamples
Proposalq(s) TypeofTwistLearning D (q∥σ) D (σ∥q) D (q∥σ) D (σ∥q)
KL KL KL KL
Twisted Contrastive 0.71±0.02 0.64±0.02 0.70±0.02 0.60±0.01
Twisted RL 1.28±0.05 0.94±0.02 2.09±0.08 1.76±0.07
Twisted SIXO 0.68±0.02 0.60±0.01 0.86±0.02 0.68±0.01
DPG – 0.70±0.02 0.58±0.01 0.89±0.03 0.69±0.00
45ProbabilisticInferenceinLanguageModelsviaTwistedSequentialMonteCarlo
Twisted Proposal (Contrastive, Exact ) DKL(q||) 8 Twisted Proposal (Contrastive, Exact ) DKL(q||)
4.5 T Tw wi is st te ed
d
P Pr ro op po os sa al
l
( (C Co on nt tr ra as st ti iv ve e,
)
E Dx Ka L(c qt
|
|)
)
DKL(||q) T Tw wi is st te ed
d
P Pr ro op po os sa al
l
( (C Co on nt tr ra as st ti iv ve e,
)
E Dx Ka L(c qt
|
|)
)
DKL(||q)
4.0 T T Tw w wi i is s st t te e ed d d P P Pr r ro o op p po o os s sa a al l l ( ( (C R RL Lo , ,n E Etr x xa a as c cti t tv e ) )) D DD K KK L LL ( (( q ||| ||| qq ) )) 7 T T Tw w wi i is s st t te e ed d d P P Pr r ro o op p po o os s sa a al l l ( ( (C R RL Lo , ,n E Etr x xa a as c cti t tv e ) )) D DD K KK L LL ( (( q ||| ||| qq ) ))
T Tw wi is st te ed d P Pr ro op po os sa al l ( (R RL L) ) D DK KL L( (q || || q) ) 6 T Tw wi is st te ed d P Pr ro op po os sa al l ( (R RL L) ) D DK KL L( (q || || q) )
3.5 Twisted Proposal (SIXO, Exact ) DKL(q||) Twisted Proposal (SIXO, Exact ) DKL(q||)
Twisted Proposal (SIXO, Exact ) DKL(||q) Twisted Proposal (SIXO, Exact ) DKL(||q)
Twisted Proposal (SIXO) DKL(q||) 5 Twisted Proposal (SIXO) DKL(q||)
3.0 T Dw PGis t Pe rd o pP oro sp ao l s (Ea xl a(S cI tX O ) ) D D KK LL (q( ||||q )) T Dw PGis t Pe rd o pP oro sp ao l s (Ea xl a(S cI tX O ) ) D D KK LL (q( ||||q ))
DPG Proposal (Exact ) DKL(||q) 4 DPG Proposal (Exact ) DKL(||q)
DPG Proposal DKL(q||) DPG Proposal DKL(q||)
2.5 DPG Proposal DKL(||q) DPG Proposal DKL(||q)
3
2.0
2
1.5
1
1.0
0 4 16 64 256 1024 0 4 16 64 256 1024
Number of Gradient Updates Number of Gradient Updates
(a)Toxicity(Sec.7.2.1) (b)Sentiment(Sec.7.2.2)
Figure5: TrainingcomparisonforExactversusApproximateσ (positive)sampling,asdescribedinApp.H.3. Having
accesstoexacttargetsamplesmakeslearningleadtolowerKLdivergencesinamorereliablemanner.
46
ecnegreviD
LK
ecnegreviD
LK