An exactly solvable model for emergence and scaling laws
Yoonsoo Nam∗a, Nayara Fonseca∗a, Seok Hyeong Leeb, and Ard Louisa
a
Rudolf Peierls Centre for Theoretical Physics, University of Oxford
b
Center for Quantum Structures in Modules and Spaces, Seoul National University
Abstract
Deeplearningmodelscanexhibitwhatappearstobeasuddenabilitytosolveanewproblem
as training time (T), training data (D), or model size (N) increases, a phenomenon known as
emergence. In this paper, we present a framework where each new ability (a skill) is represented
as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic
expressions for the emergence of new skills, as well as for scaling laws of the loss with training
time, data size, model size, and optimal compute (C). We compare our detailed calculations to
directsimulationsofatwo-layerneuralnetworktrainedonmultitasksparseparity,wherethetasks
inthedatasetaredistributedaccordingtoapower-law. Oursimplemodelcaptures,usingasingle
fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model
size increases in the neural network.
1 Introduction
Large language models (LLMs) can exhibit rapid (on a log scale) transitions where they acquire the
ability (or skill) to solve a new task as the number of parameters, the training dataset size, or the
amount of training time is scaled up. This phenomenon has been dubbed emergence [1–4], and has
attracted a lot of recent attention. It motivates the costly drive to train ever larger models on ever
larger datasets, in the hope that new skills will emerge. It also motivates theoretical studies with the
goalthatLLMscanbemademorepredictableandsaferinthefuture. Whiletheconceptofemergence
has been critiqued on the grounds that the sharpness of the transition to acquiring a new skill may be
sensitivetothemeasurebeingused[5],theobservationthatimportantnewskillsarelearnedforlarger
models that appear not to be present in smaller ones is robustly established. These results raise many
challenging questions such as: What triggers the appearance of complex new skills as models scale
up? Can we predict when new skills will be acquired? These questions are complicated by difficulties
in formally defining skills or capabilities [6], and by our general lack of understanding of the internal
representations of deep neural networks [7].
Another widely observed property of deep learning models is that the loss improves predictably as
a power-law in the number of data points or number of model parameters or simply in the amount of
compute thrown at a problem. Such phenomenon are called neural scaling laws [8, 9], and have been
widely observed across different architectures and datasets [10–15]. While the scaling exponents can
depend on these details, the general phenomenon of scaling appears to be remarkably robust. This
raises many interesting questions such as: What causes the near-universal scaling behaviour? How
does the continuous scaling of the loss relate to the discontinuous emergence of new skills?
Achallengeinansweringthequestionsraisedbythephenomenaofemergenceandscalinglawsarises
from the enormous scale and expense of training cutting-edge modern LLMs, which are optimized
for commercial applications, and not for answering scientific questions about how they work. One
way that progress can be made is to study simpler dataset/architecture combinations that are more
tractable. Thecurrentpaperisinspiredinpartbyrecentworkinthisdirectionthatproposedstudying
emergence in learning the sparse parity problem [16, 17], which is easy to define, but known to be
computationally hard. In particular, Michaud et al. [17] introduce the multiple unique sparse parity
∗Theseauthorscontributedequally;{yoonsoo.nam,nayara.fonsecadesa}@physics.ox.ac.uk.
1
4202
rpA
62
]GL.sc[
1v36571.4042:viXraproblem – where tasks are distributed in the data through a power-law distribution of frequencies –
as a proxy for studying emergence and neural scaling in LLMs. For this data set, the authors were
able to empirically measure and schematically derive scaling laws as a function of training steps (T),
parameters (N), and training samples (D). They also directly observed the emergence of new skills
withincreasingT,showinghowsmoothneuralscalinglawscanarisebyaveragingovermanyindividual
cases of the emergence of new skills.
Here we define a basis of orthogonal functions for the multitask sparse parity problem. Each basis
function corresponds to a skill that can be learned, and their respective frequencies are distributed
following a power law with exponent α+1. We then propose a simple multilinear expansion in these
orthogonal functions that introduces a layered structure reminiscent of neural networks and gives rise
to the stage-like training dynamics [18]. With our simple model, we can analytically calculate full
scalinglaws,includingpre-factors,asafunctionofdataexponentα,T,D,N,andoptimalcomputeC.
Our simple model can, with just one parameter calibrated to the emergence of the first skill, predict
the ordered emergence of multiple skills in a 2-layer neural network. We summarize our contributions:
1. Skills as basis functions. We establish a framework for investigating emergence by representing
skills as orthogonal functions that form a basis in function space (Section 2). We apply our
methods to controlled experiments on the multitask sparse parity dataset.
2. Multilinear model. We propose an analytically tractable model that is expanded in the basis of
skill functions, and is multilinear with respect to its parameters so that it possesses a layerwise
structure (Section 3). The multilinear nature of the model produces non-linear dynamics, and
the orthogonal basis decouples the dynamics of each skill.
3. Scaling laws. We employed both intuitive (Section 4, Section 5, and Appendix D) and rigorous
(Appendix J) derivations of scaling laws for our multilinear model, relating the model’s perfor-
mance to training time (T), dataset size (D), number of parameters (N), and optimal compute
(C =N×T). Weshowthat the scaling exponents forthese factors are −α/(α+1), −α/(α+1),
−α, −α/(α+2), respectively, whereα+1istheexponentofthepower-law(Zipfian)inputdata.
4. Predicting emergence. We demonstrate that our multilinear model captures the skill emergence
of a 2-layer NN for varying training time, dataset size, and number of trainable parameters.
Our results show that the multilinear model, calibrated only on the first skill, can predict the
emergence of subsequent skills in the 2-layer NN, see Fig. 1 and Section 6.
kk==11 kk==22 kk==33 kk==44 kk==55
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
103 104 103 104 100 101
T D N
(a) Time Emergence (b) Data Emergence (c) Parameter Emergence
Figure 1: Predicting Emergence. The skill strength R , defined as the kth coefficient if a model
k
is expanded in the basis of the skill functions (g s), measures how well the kth skill is learned, and is
k
plotted against(a) time T, (b) dataset size D, and (c)number of parameters N. R is normalizedby
k
thetargetscaleS suchthatR /S =1meanszeroskillloss. Thedashedlinesshowtheabruptgrowth
k
–emergence–of5skillsfora2-layerNN(AppendixI)trainedonthemultitasksparseparityproblem
with data power-law exponent α = 0.6. Solid lines are the predictions (Eqs. (37), (40) and (44),
respectively) from our multilinear model calibrated on the first skill (blue) only.
2
S/
kR
htgnerts
llikS
S/
kR
htgnerts
llikS
S/
kR
htgnerts
llikSSkill idx (I) Control bits Skill bits (X) y M(i,x) g (i,x) g (i,x) ... g (i,x)
1 2 ns
1 10000000 110011000001010 S [1,1,0] 1 0 ... 0
1 10000000 100110010100011 −S [0,1,0] −1 0 ... 0
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
2 01000000 001101010110101 −S [0,0,1] 0 −1 ... 0
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
n 00000001 001001001001100 −S [1,1,1] 0 0 ... −1
s
Table1: Multitask sparse parity dataset and skill basis functions. Thecontrolbitsn areone-
s
hot vectors encoding specific parity tasks, indexed in the first column. The frequency of the distinct
parity tasks follows a rank-frequency distribution with an inverse power law relation. The skill bits
arebinarystrings. They columnshowsthetargetscaleS multipliedbytheresultingparitycomputed
from m = 3 bits (highlighted in colors), which form the subset M(i,x). The last columns show the
values of the skill basis functions g (i,x), defined in Eq. (2).
k
Related works. Focusing on data scaling, Hutter [19] develops a model with a discrete set of
features. Under the assumption of a Zipfian distribution of features, this model demonstrates that
the error decreases as a power law with increasing data size. In a related vein, Michaud et al. [17]
proposeamodelofneuralscalinglawsinwhichthelossisdecomposedintoasumover‘quanta’. Their
model aims to reconcile the apparent discrepancy between loss metrics’ regular power-law scaling and
novel capabilities’ abrupt development in large-scale models. Various other models for neural scaling
laws have been proposed in recent research, including connecting neural scaling exponents to the
data manifold’s dimension [20] and their relation with kernels [21], proposing solvable random-feature
models [22, 23], and developing data scaling models using kernel methods [24–26]. Closely related to
the study of neural scaling laws is the understanding of emergent abilities in large language models.
Several studies [1–4] document examples of such emergent abilities.1Arora and Goyal [28] propose a
framework for the emergence of tuples of skills in language models, in which the task of predicting
text requires combining different skills from an underlying set of language abilities. Okawa et al. [29]
demonstrate that a capability composed of smoothly scaling skills will exhibit emergent scaling due
to the multiplicative effect of the underlying skills’ performance. Other works related to the skill
acquisition include Yu et al. [30], who introduce a new evaluation to measure the ability to combine
skills and develop a methodology for grading such evaluations, and Chen et al. [31], who formalize
the notion of skills and their natural acquisition order in language models. Throughout this work, we
consider the infinite data regime, such that the optimizer only sees ‘fresh’ samples at each iteration
step, and there is no distinction between training and test losses. This contrasts with the grokking
phenomenon [32], which also exhibits sigmoid-shape curves but is related to a discrepancy between
the model’s train and test behavior.
2 Setup
In this section, we define the multitask sparse parity problem under the mean-squared error (MSE)
loss. We represent skills as orthogonal functions and measure their strength in a model by calculating
thelinearcorrelationbetweenthemodeloutputandtheskillbasisfunctions. Foracomprehensivelist
of notations used in this work, refer to the glossary in Appendix A.
Multitask sparse parity problem. Inthesparseparityproblem, n skillbitsarepresentedtothe
b
model. The target function is a parity function applied to a fixed subset of the input bits. The model
must detect the relevant m < n sparse bits and return the parity function on this subset (M(i,x),
b
see Table 1). Michaud et al. [17] introduced the multitask sparse parity problem by introducing n
s
unique sparse parity variants – or skills – with different sparse bits (for a representation, see Table 1).
1WenotethatSchaefferetal. [5]havearguedthatmanyoftheseexamplesmaybeartifactsoftheevaluationmetric
(see also [3, 4, 27]). Our work only considers continuously optimized measures (such as MSE loss) instead of hard
thresholdmeasures(likeaccuracy)thatmayartificiallyenhancethesigmoid-shapedcurves.
3Eachskillisrepresentedinthen controlbitsasaone-hotstring,andthemodelmustsolvethespecific
s
sparse parity task indicated by the control bits (for more details, see Appendix B.1).
The n skills (random variable I ∈ {1,2,...,n }) follow a power law (Zipfian2) distribution P ,
s s s
and the skill bits (random variable X ∈ {0,1}nb) are uniformly distributed. Because P
s
and P
b
are
independent, the input distribution P(I,X) follows a product of two distributions:
i−(α+1)
P s(I =i):= (cid:80)nsj−(α+1), P b(X =x):=2−nb, P(I,X):=P s(I)P b(X). (1)
j
(cid:16) (cid:17)−1
We denote A= (cid:80)ns j−(α+1) so that P (i):=Ai−(α+1).
j=1 s
Skill basis functions. We represent the kth skill as a function g
k
: {0,1}ns+nb → {−1,0,1} that
returns the parity ({−1,1}) on the kth skill’s sparse bits if i = k, but returns 0 if the control bit
mismatches that of the kth skill (i̸=k):
(cid:26) (−1)(cid:80) jMj(i,x) if i=k
g (i,x):= , (2)
k 0 otherwise
where M : {0,1}ns+nb → {0,1}m is the map that selects the relevant sparse bits for the ith skill
(Table 1). Note that different skill functions have 0 correlation as the supports of skills functions are
mutually exclusive:
g (i,x)g (i,x)=δ δ . (3)
k k′ i,k k,k′
The target function. The target function is a sum over n skill functions multiplied by a target
s
scale S:
(cid:88)ns
f∗(i,x):=S g (i,x). (4)
k
k=1
The target scale S is the norm of the target function (E [f∗(I,X)f∗(I,X)] = S2). Note that the
I,X
skill functions serve as ‘features’ or countable basis for describing the target function as in [19].
Loss. We use MSE loss For analytic tractability:
1 (cid:104) (cid:105)
L:= E (f∗(I,X)−f(I,X))2 , (5)
2 X,I
where f is the function expressed by a given model. We define the skill loss L as the loss when only
k
the kth skill is given, formulated as
1 (cid:104) (cid:105)
L := E (f∗(I =k,X)−f(I =k,X))2 . (6)
k 2 X
Then the total loss can be expressed as a sum of L weighted by their skill frequencies P (I =k):
k s
L=
1(cid:88)ns
P (I =k)E
(cid:104)
(Sg (I =k,X)−f(I
=k,X))2(cid:105)
(7)
2 s X k
k=1
(cid:88)ns
= P (I =k)L . (8)
s k
k=1
Skill strength. The skill strength or the linear correlation between the kth skill (g ) and a function
k
expressed by the model at time T (f ) is
T
R (T):=E [g (I =k,X)f (I =k,X)]. (9)
k X k T
2The literature on Zipf’s law is vast. In some traditions Zipf is synonymous with a power law. In others Zifp only
referstofrequency-rankplotswithexponent1. Wewillmainlyusethemoregeneralpowerlawterminology.
4The skill strength R is the kth coefficient if a model is expanded on the basis of the skill functions
k
(g s). The skill strength, like the test loss, can be accurately approximated (see Appendix I.2).
k
The skill loss L (Eq. (6)) can be expressed by the skill strength and the norm of the learned function
k
for I =k:
1 (cid:104) (cid:105)
L (T)= E (Sg (I =k,X)−f (I =k,X))2 (10)
k 2 X k T
= 1(cid:0) S2+E (cid:2) f (I =k,X)2(cid:3) −2SR (f )(cid:1) . (11)
2 X T k T
The skill loss becomes 0 if and only if f (I =k,X)=Sg (I =k,X).
T k
Experimental setting. We experiment with a 2-layer fully connected neural network (NN) with
ReLU activations. The NN receives the n s+n
b
bits as inputs and outputs a scalar ({0,1}ns+nb →R).
Inmostoftheexperiments,theNNistrainedwithstochasticgradientdescent(SGD)withwidth1000,
using n =5, m=3, and n =32, unless otherwise stated. See Appendix I for details.
s b
3 Multilinear Model
We propose a simple multilinear model – multilinear with respect to the parameters – with the first
N most frequent skill functions g (i,x) as the basis functions (features):
k
N
(cid:88)
f (i,x;a,b)= a (T)b (T)g (i,x), (12)
T k k k
k=1
where a,b ∈ RN are the parameters. The model has built-in skill functions g – which transform
k
control bits and skill bits into the parity outputs of each skill – so the model only needs to scale the
parameters to a b = S. The multilinear structure (Fig. 2(a)) is similar to the layered structure of
k k
NNsandgivesrisetothestage-liketrainingdynamics(Section4)differentfromthatoflinearmodels.3
A similar model and its dynamics have been studied by Saxe et al. [18] in the context of linear neural
networks; see Appendix B.2 for details.
Note that a (T)b (T) is equivalent to the skill strength R in Eq. (9):
k k k
a (T)b (T)=E [g (I =k,X)f (I =k,X)]=R (T), (13)
k k X k T k
For the multilinear model, we can express the skill loss in Eq. (10) as a function of R :
k
1
L (T)= (S−R (T))2. (14)
k 2 k
Assuming that we are training the model on D samples from P(I,X), the dynamics of each skill
(R ) is decoupled because g s’ supports are mutually exclusive (Eq. (3)). The empirical loss on
k k
D samples can be decomposed into the sum of empirical skill losses, which only depends on their
respective R (see Appendix C.1):
k
L(D)
=(cid:88)ns
L(D) =
1
(cid:88)ns
d (S−R )2, (15)
k 2D k k
k=1 k=1
where d is the number of samples of the kth skill (i.e. number of samples (i,x) with g (i,x) ̸= 0).
k k
Now, we can independently solve each skill’s dynamics under gradient descent.
3Note that if we reparameterize a b as a single parameter, the model becomes a linear model but with different
k k
dynamics. See Fig. 10 and Appendix G for further discussion on the similarities and differences between linear and
multilinearmodels.
5Decoupled dynamics of the multilinear model. Assuming small and positive initialization (0<
R (0)≪S), the kth skill strength (R (T)) follows
k k
R (T) 1
k = , (16)
(cid:16) (cid:17)
S 1+ RkS
(0)
−1 e−2ηd DkST
where η is the learning rate, D is the number of training samples, and d is the number of samples of
k
the kth skill (i.e. number of samples (i,x) with g (i,x)̸=0).
k
Proof See Appendix C.1.
TheskillstrengthinEq.(16)isasigmoidfunctionintimeandsmallerd /D,whichbecomesP (I =k)
k s
for D →∞, results in a delayed growth (Fig. 2(b)). For the connection to linear neural networks [18],
see Fig. 7 in Appendix B.2.
d /D=0.5 d /D=0.2 d /D=0.1
1 2 3
1.0
0.8
0.6
0.4
0.2
0.0
0 25 50 75 100
T
(a)Multilinearmodelillustration (b)Decoupleddynamics
Figure 2: Multilinear model. (a): An illustration of the multilinear model which is multilinear in
terms of parameters, generating a layerwise structure. The model has the skill functions g s as basis
k
functions. (b): The dynamics of the multilinear model are decoupled and each skill strength (R )
k
shows a sigmoidal growth in time. Note that skills with lower frequency have a more delayed growth.
4 Stage-like training: intuitive derivation of the scaling laws
In this section, we define stage-like training – one skill is completely learned before the next skill
initiates learning (Fig. 2(b)) – show under what conditions it occurs, and provide an example of how
stage-like training results in the time scaling law. This section offers intuition on how the layerwise
structure shared by NNs and our model can result in empirically observed scaling laws. Readers may
skip this section as the scaling laws for our model (Section 5) can be shown without the stage-like
training assumption. Still, it provides intuition for the discussion on NN dynamics in Section 6 and
explains how the model in [17] may arise from the NN dynamics.
In Fig. 2(b), we observe the stage-like training in which one skill saturates (reaches R /S ≈ 1)
k
beforethenextskillinitiatesitsemergence. Toquantifythisbehavior,wedefinetwointervalsforeach
skill (see Fig. 3(a)):
• The emergent time τ(e)(ϵ): the time for R /S to reach ϵ;
k k
• The saturation time τ(s)(ϵ): the time for R /S to saturate from ϵ to 1−ϵ.
k k
Using the dynamics equation (Eq. (16)) and that d /D → P (k), the emergent time and saturation
k s
time of the kth skill becomes
1
(cid:32) S −1(cid:33)
1
(cid:18)
1
(cid:19)
τ(e)(ϵ)= ln Rk(0) ∝kα+1, τ(s)(ϵ)= ln −1 ∝kα+1. (17)
k 2ηP (k)S 1 −1 k ηP (k)S ϵ
s ϵ s
6
...
S/
kR1.0 1 †
− 13.0 N
0.8
τ 1(s)(†) k=1 1 2S2 k=1Ps(I=k)
k=2 12.5 X
0.6
12.0
1S2 (1)+ (†) 0.4 2 Ps O
11.5
0.2 †
τ 1(e)(†)
τ 2(e)(†) 11.0
∆τ(e)(†)
0.0
10.5
0 50 100 150 200 0 50 100 150 200
T T
(a)Emergentandsaturationtime (b)Losschangebetweenemergences
Figure 3: Stage-like training. The multilinear model is trained on the multitask sparse parity
problem with α = 0.6. (a): Skill strength of the model as a function of time. The emergent time
τ(e)(ϵ) is the time required for the kth skill to reach R /S =ϵ. The saturation time τ(s)(ϵ) is the time
k k k
required for R /S to saturate from ϵ to 1−ϵ. The model shows stage-like training if the emergent
k
time interval τ(e) (ϵ)−τ(e)(ϵ) is larger than the saturation time τ(s)(ϵ) for sufficiently small ϵ (0.05 in
k+1 k k
the figure). (b): The loss as a function of time for the same system as (a). For stage-like training,
the change in the loss for the kth emergence is L +O(ϵ) and the interval for the next emergence is
k
∆τ(e)(ϵ)=τ(e) (ϵ)−τ(e)(ϵ).
k+1 k
For sufficiently small initialization (R (0)≪S), we get a stage-like training:
k
τ(s)(ϵ)<τ(e) (ϵ)−τ(e)(ϵ), ϵ≪1. (18)
k k+1 k
Inotherwords,themodelfinisheslearning(saturating)thekth skillbeforestartingtolearn(emerging)
thenextskill(Fig.3(a)). Theemergentintervalbetweenthek andk+1skillsrelativetotheτ(e)(ϵ)is
k
∆T τ(e) (ϵ)−τ(e)(ϵ) (k+1)α+1−kα+1
= k+1 k = (19)
T τ(e)(ϵ) kα+1
k
=(α+1)k−1+O(k−2). (20)
Accordingly, at τ(e)(ϵ), all skills with index up to but not including k have saturated (R ≈ S), or
k i<k
equivalentlyL ≈0(Eq.(14)). Thetotalloss,thesumofL weightedbyP (k)∝k−(α+1) (Eq.(8)),
i<k k s
becomes(cid:80)∞ P (I =j)S2/2(seeFig.3(b)). Thesaturationofthekth skillresultsinalossdifference
j=k s
of P (I =k)S2/2. Thus, we obtain
s
∆L P (I =k) k−(α+1) k−(α+1)
≈ s =− ≈− (21)
L (cid:80)∞ P (I =j) (cid:80)∞ j−(α+1) (cid:82)∞ j−(α+1)dj
j=k s j=k k
=−αk−1+O(k−2). (22)
Assuming k ≫ 1 and combining Eq. (22) and Eq. (20) to the largest order, we have the equation for
the power law with exponent −α/(α+1) in Fig. 4(a, i):
∆L α ∆T
=− . (23)
L α+1 T
When the condition k ≫ 1 is no longer met, typically for small T, the model deviates from the
power-law, as evident in the top left region of Fig. 4(a,i). This behavior is further illustrated by the
reversed-sigmoidal shape of L in Fig. 3(b).
Ifthestage-liketrainingholdsforanyresource(e.g.,time,data,orparameters),thescalinglawcan
be derived using the ratio of change in loss per skill (Eq. (22)) and the ratio of change with respect to
7
S/)T(
kR
Lthe resource (given by the emergent time in Eq. (20)). Although the stage-like training dynamics may
notoccurinreal-worldscenarios,studyingthelayerwisedynamicsandthestage-liketrainingbehavior
that emerges from the power-law input distribution (P ) offers insights into the emergence of skills in
s
NNs that possess this layerwise structure.
5 Scaling laws
In this section, we derive the scaling laws of our multilinear model (Section 3) for time (T), data
(D), parameters (N) and optimal compute (C). For analytical tractability, we define compute as
C :=T×N [23]. Table 2 shows a summary of the scaling laws. Note that we achieve the same scaling
laws as in Hutter [19] for D and in Michaud et al. [17] for T,D, and N. Assuming 0 < α < 1, the
exponents are consistent with the small power-law exponents reported in large-scale experiments, see,
e.g., [9, 14, 33].
Using Eqs. (7), (14) and (16), we have the loss as a function of time (T), data (D), parameters
(N), and the number of observations for each skill [d ,··· ,d ]:
1 ns
S2 (cid:88)N 1 S2 (cid:88)ns
L= P (k) + P (k). (24)
2 s (cid:18) (cid:16) (cid:17)−1 (cid:19)2 2 s
k=1 1+ RkS
(0)
−1 e2ηd DkST k=N+1
Under suitable assumptions (for example, we take D,N → ∞ for the T scaling law), we can use
Eq. (24) to derive how L scales respect to T,D,N, and C (Table 2). Figure 4 shows the empirical
scaling laws for T,D, and N, while Fig. 5 shows the empirical scaling for optimal compute C where
model sizes (N) and training times (T) are chosen optimally to minimize the loss for given compute
budget(C). Detailsof the intuitive derivationsfor thescaling lawsare providedin Appendix D,while
a rigorous approach (including error bounds) is presented in Appendix J.
αα==00..33 αα==00..66 αα==00..99
10 1
10 1 10 1
D,N N,T T,D
10 2
L∝T−→α/∞(α+1) L∝D→ −∞α/(α+1) 10 2 L∝N→ −α∞
100 101 102 103 104 100 101 102 103 100 101 102
T D N
(a) Time Scaling (b) Data Scaling (c) Parameter Scaling
Figure 4: Scaling laws. The learning curve (L is the MSE loss) of the multilinear model (solid)
and the respective power-law (dotted) for (a) time T, (b) data D, and (c) parameters N. Lower left
legends show the condition and exponent for each scaling law. For the derivation of the exponents of
the scaling laws, see Appendix D. For a rigorous derivation including the prefactors, see Appendix J.
5.1 Time scaling law
Aspreviouslydiscussed,whentheinitializationissufficientlysmall,wecanderivethetimescalinglaw
in Eq. (23) using the stage-like training condition in Eq. (18). However, for our model, it is possible
to derive the time scaling law without relying on the stage-like assumption. We first assume the time
as the bottleneck and take N,D →∞. By using Eq. (14) and the decoupled dynamics of the skill loss
(Eq. (16)), each skill loss L becomes a function solely dependent on k−(α+1)T. This establishes a
k
relationship between the derivatives of the skill loss with respect to k and T.
8
L L LBottleneck Time Data Parameter Exponent
Time (T) T ∞ ∞ −α/(α+1)
Data (D) ∞ D ∞ −α/(α+1)
Parameter (N) ∞ ∞ N −α
Compute (C) C(α+1)/(α+2) ∞ C1/(α+2) −α/(α+2)
Table 2: Summary of the scaling laws. The leftmost column shows the bottleneck of the scaling
law. The middle three columns show the resource values in terms of the bottleneck (either taken to
infinity or proportional to the bottleneck). The last column shows the scaling exponent for the loss
as power-law of the bottleneck where α+1 is the exponent of the Zipfian input data (Eq. (1)). For a
detailed derivation, see Appendix D.
S2 dL k dL
L = , k =− k, (25)
k (cid:18) (cid:16) (cid:17)−1 (cid:19)2 dT (α+1)T dk
2 1+ S −1 e2ηAk−(α+1)ST
Rk(0)
where d /D →P for D →∞ and A is the normalization constant for the distribution of skills P in
k s s
Eq. (1). For the total loss, we approximate the sum in Eq. (8) by an integral and differentiate with
respect to T, which can be integrated by parts using Eq. (25) to give the scaling law:
dL (cid:90) ∞ dL 1 (cid:90) ∞ dL α L
≈ Ak−(α+1) kdk =− Ak−α kdk ≈− , (26)
dT dT (α+1)T dk (α+1)T
1 1
where the last approximation requires large T. We can rearrange the equation above to obtain the
time scaling law with exponent −α/(α+1) as in Fig. 4(a). For details of the derivation and the finite
correction of the scaling law for small α, see Appendix D.2.
5.2 Data scaling law
ThedatascalinglawassumesT →∞andN →∞withdataasthebottleneck. Fromthedynamicsof
the multilinear model (Eq. (16)) and the relationship between skill loss and skill strength (Eq. (14)),
we can show that our model is a one-shot learner:
One shot learner. Given that N >k, T →∞, and d is the number of samples from the training
k
set with g (i,x)̸=0, the kth skill loss after training is
k
(cid:26)
0 :d >0
L (∞)= k (27)
k (S−R (0))2/2≈S2/2 :d =0.
k k
Proof Follows trivially from Eq. (15), see Appendix C.2.
Our model requires only one sample from the kth skill to learn such a skill, similar to how language
models are few-shot learners at inference.4 The model can one-shot learn a skill since it has g as the
k
basis functions, and the dynamics among different skills are decoupled. A similar one-shot learner has
been studied in Hutter [19] where the error depends on a single ‘observation’ of a feature.
Because the kth skill loss only depends on d (number of observations for the kth skill), we can
k
calculatetheexpectationoftheskilllossforD datapointsfromP (k|D)ortheprobabilitythat
observed
d >0:
k
1
P (k|D)=1−(1−P (k))D, E [L ]= S2(1−P (k|D)), (28)
observed s D k 2 observed
where the expectation E is over all possible training sets of size D. Using Eq. (8) and Eq. (28), the
D
total loss is
E [L]=
1 S2(cid:88)∞
(1−P (k))DP (k)≈
1 S2A(cid:90) ∞(cid:16) 1−Ak−(α+1)(cid:17)D
k−(α+1)dk, (29)
D 2 s s 2
k=1 1
4Few-shotlearningistypicallydiscussedinthecontextofmodelsthathaveundergonepre-training(see,e.g. [1]). We
speculatethatexpandinginthebasisg inourframeworkcanmodelaspectsofthepre-trainingprocess.
k
9where A is the normalization constant for P (k) in Eq. (1). We can express the loss difference for an
s
additional data point ∆L=E [L]−E [L] as an integral over k:
D+1 D
1 (cid:90) ∞(cid:16) (cid:17)D
∆L≈− αS2A 1−Ak−(α+1) k−2(α+1)dk. (30)
2
1
We can calculate ∆L with integration by parts and express the terms with ∆L and E [L] to obtain
D
the following relationship for D ≫1:
∆L α ∆D
≈− . (31)
E [L] (α+1) D
D
The equation above is the data scaling law with exponent −α/(α+1) (Fig. 4(b)), which agrees with
previous works, see, e.g., [17, 19, 26]. For the details of the derivation, see Appendix D.3.
5.3 Parameter scaling law
The parameter scaling law assumes T → ∞ and D → ∞, with the parameters N < n as the
s
bottleneck. Becauseourmodelisaone-shotlearner(Eq.(27)),learningofthekth skillonly depends
ontheexistenceofg inthemodel;themodelwith[g ,··· ,g ]willlearnallk ≤N skillswithL =0.
k 1 N k
Equivalence between a basis function and a skill. Given T,D → ∞ and if the multilinear
model has the N most frequent skill functions as a basis,
(cid:26)
0 :k ≤N
L (∞)= (32)
k S2/2 :k >N.
Proof Follows trivially from Eq. (15), see Appendix C.3.
Using Eq. (32) and Eq. (7), we can express the total loss as function of N:
S2 (cid:90) ∞
L≈ Ak−(α+1)dk ∝(N +1)−α. (33)
2
N+1
By approximating N ≈N +1 for N ≫1, we obtain the power law with exponent −α (Fig. 4(c)).
5.4 Scaling law for compute optimal performance
Assuming D →∞ and ignoring the constant factors, we approximate the loss in Eq. (24) as
L≈T−α/α+1+N−α, (34)
where we approximate the first term in Eq. (24) (first N skills bottlenecked by time) with the time
scaling law and the second term in Eq. (24) (n −N unlearned skills bottlenecked from the lack of
s
basis functions) with the parameter scaling law. By Lagrangian multiplier, we can show that optimal
computeC isachievedwhenT ∝Nα+1. Intuitively,theoptimalallocationT ∝Nα+1 iswhenT isjust
enough to learn the first N skills as the emergent and saturation times of the Nth skill is proportional
to Tα+1 (Eq. (17)). Plugging in the relationship between T and N for optimal C in Eq. (34), we get
L∝C−α/(α+2). (35)
In Fig. 5, we plot the loss as the function of C for various N and observe that optimal C follows the
scalinglaw(Eq.(35)). ThederivationforEq.(35)fromEq.(34)isprovidedinAppendixD.4, butthe
justification of Eq. (34) requires a more formal approach, which is given in Appendix J (for relevant
theorems, see Theorems 1 to 3; for the summary, see Corollaries 3 and 4).
10αα==00..33 αα==00..66 αα==00..99
100
100
10 1
100
10 1 10 2
102 104 106 108 102 104 106 108 102 104 106 108
C C C
Figure5: Scalinglawforoptimalcompute. Thesolidlinesarethelearningcurvesofthemultilinear
model as a function of compute C =T ×N with varying parameters N from 101 (top plateau) to 104
(bottomplateau). AtradeoffbetweenN andT existsforfixedC: smallerN trainsfasterbutplateaus
after learning all N skills while larger N can achieve smaller loss at a slower pace. The dotted lines
show the optimal loss for given compute C, which follows a power law with exponent −α/(α+2).
Note that the solid lines plateau after intersecting with the dotted lines, indicating that the optimal
allocationofT andN forC istohaveT justlargeenoughtofitN skills. Forthediscussiononα=0.3
where the optimal C for the model decays faster than the power-law, see Appendix D.2.
6 Predicting Emergence
In the previous section, we have shown that our simplified model satisfies scaling laws for time, data
points, and parameters, which match the exponents observed in Michaud et al. [17]. In deriving
the scaling law, we used emergence or an abrupt change in L : (i) decoupled sigmoidal saturation
k
(Eq. (25)) for the time scaling law, (ii) one-shot learning (Eq. (27)) for data scaling law, and (iii) the
one-to-one relationship between basis functions and skills for the parameter scaling law (Eq. (32)). In
thissection, weanalyzetheemergenceofa2-layerfullyconnectedNN(Section2)anddiscusstowhat
degree the emergence in NNs can be described with our model.
BecauseNNslackthebuilt-ing ,weextendourmodeltoapproximatetheemergenceinNNs. The
k
extended models will keep their multilinearity and decoupling among the skills, resulting in the same
scalinglaws,butwillrequireanextraparameterthatmustbecalibrated(fit). Wecalibrateourmodel
fromanNNtrainedononeskill(n =1)anduseittopredicttheemergenceofallskillsforthen =5
s s
setup (Fig. 6).
6.1 Time emergence
In our multilinear model, the orthonormal basis g results in decoupled dynamics for each skill
k
(Eq. (15)), and the layerwise structure – the product of parameters a b – leads to abrupt satu-
k k
rationofeachskillstrength(Eq.(16)). NNssharethelayerwisestructurebutlackafixedorthonormal
basisg andthusthedecoupleddynamicsforeachskill;NNsmust‘discover’g (uptoascalingfactor)
k k
during training before/while saturating each skill (i.e., R /S ≈1).
k
Extended model. To address this difference in scaling between our model and NNs, we extend the
model by multiplying our basis g by a constant B >0:
k
N
(cid:88)
f (i,x;a,b)= a (T)b (T)Bg (i,x), B >0. (36)
T k k k
k=1
The calibration constant 0 < B < 1 adjusts the slower dynamimcs resulting from NN ‘discovering’
(feature-learning) g , which rescales the dynamics in T:
k
R (T) 1
k = . (37)
(cid:16) (cid:17)
S 1− S −1 e−2ηPs(I=k)B2ST
Rk(0)
11
L L LNNNN eexxtteennddeedd mmooddeell
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1000 2000 1000 2000 0 5 10
T D N
(i) Time Calibration (ii) Data Calibration (iii) Parameter Calibration
(a)Calibrationonthefirstskill
kk==11 kk==22 kk==33 kk==44 kk==55
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
103 104 103 104 100 101
T D N
(i) Time Emergence (ii) Data Emergence (iii) Parameter Emergence
(b)Emergencepredictionforns=5system(Fig.1repeated)
Figure 6: Calibration and prediction on emergence. (a): The calibration of the extended
multilinearmodel(solid)onthe2-layerNN(dashed)forn =1system. Forthecalibratedparameters,
s
we have B2 = 1/22 for time (Eq. (37)), D = 800 for data (Eq. (40)), and N = 4 for parameters
c c
(Eq.(44)). (b): Theemergencepredictionofthemultilinearmodel(solid)comparedtotheemergence
of 2-layer NN (dashed) for n =5 system (Fig. 1 repeated). For the NN, we repeated the experiments
s
50 times from random initialization to calculate the standard deviation.
12
kR
htgnerts
llikS
S/
kR
htgnerts
llikS
kR
htgnerts
llikS
S/
kR
htgnerts
llikS
kR
htgnerts
llikS
S/
kR
htgnerts
llikSIn Fig. 6(a,i), we observe that the extended model with B2 = 1/22 fits the NN trained on one skill
(i.e., n =1). In Fig. 6(b,i), we observe that the extended model with B2 =1/22 accurately predicts
s
each skill’s time of emergence and reasonably well the time of saturation, suggesting that emergence
in NNs can be described by gradient descent dynamics of a simple layerwise model.
6.2 Data point emergence
In Section 5, we derived the data scaling law with the model’s single-shot learning ability. One-shot
learningisonlypossiblebecausethemodelhasg sasanorthogonalbasis. NNs,withoutg sasafixed
k k
basis, must ‘discover’ g , which requires multiple samples from the kth skill.
k
Extended model. To make our model a D -shot learner, we extend it by replacing g with the e
c k k,l
basis:
(cid:88)N (cid:88)Dc
f (i,x;a,B)= a (T) B (T)e (i,x), (38)
T k k,l k,l
k=1 l=1
where the matrix B ∈ RN×Dc is an extension of b ∈ RN in Eq. (12), D
c
is a fixed scalar, and
e k,l(i,x):{0,1}ns+nb →R are functions with the following properties:
(cid:88)Dc
1
E [e e ]=δ , e (I ̸=k,x)=0, √ e =g . (39)
X|I=k k,l k,l′ ll′ k,l k,l k
D
c
l=1
The first property states that e s, when I = k, are orthonormal in X. The second property asserts
k
that, similar to g (Eq. (2)), e is non-zero only when I = k, and fitting of the kth skill only occurs
k k,l
among e s: the skills are still decoupled. The third property states that g can be expressed using
k,l k
e .
k,l
Because the extended model decouples the learning of each skill, we can express L with d ,
k k
analogoustoEq.(27). Forthekth skill,theextendedmodeloverfitswhentherearefewerobservations
(d ) compared to the dimension of the e basis (D ), and fits g when d ≥D : thus our model is a
k k,l c k k c
D shot learner.
c
D shot learner. If we initialize the extended model in Eq. (38) with sufficiently small initialization
c
and if Eqs.(39) are satisfied, then the skill strength after training (T →∞) on D datapoints is
(cid:40) (cid:16) (cid:112) (cid:17)
S 1− 1−d /D :d <D
R (∞)= k c k c (40)
k
S :d ≥D .
k c
The number d is the number of samples in the training set for the kth skill (i.e. datapoints with
k
g (i,x)̸=0).
k
Proof See Appendix E.3.
UsingEq.(40),wecancalculatetheemergenceofR /S asafunctionofD. NotethatEq.(40)reduces
k
toEq.(27)whenD =1andissimilartothemodelin[17]inthat, tolearnaskill, themodelrequires
c
a certain number of samples from the skill.
ThederivationofEq.(40)followstriviallyfromthedynamicsoftheextendedmodel(Eq.(16))and
well-knownresultsinlinear/kernelregression[26,34–37]. Tobemorespecific,themodelfindsthemin-
imum norm solution as if we performed ridgeless regression on g with basis functions [e ,···e ].
k k,1 k,Dc
See Appendix E.3 for details.
In Fig. 6(a, ii), we observe that the NN saturates at D ≈ 800 (solid line) for n = 1 set up. In
s
Fig. 6(b, ii), we observe that our extended model with D =800 approximates the data emergence for
c
the consecutive skills, suggesting that the NN discovers g when it observes D samples from the kth
k c
skill.
6.3 Parameter emergence
Since our model has g s as basis functions, adding a g (2 parameters) results in learning the Nth
k N
skill (Eq. (32)). A 2-layer NN with N parameters – the width of the hidden layer (number of nodes)
13– cannot express g with a single node (hidden neuron); it requires multiple hidden nodes to express
N
a single skill (Fig. 6(a,iii)). For this experiment, Adam [38] was used, instead of SGD, to increase the
chance of escaping the near-flat saddle points induced by an insufficient number of parameters.5
Extended model. Tocompensatefortheneedformultiplenodesinexpressingoneskill, weextend
ourmodelsimilarlytoEq.(38). Becausethenumberofparametersisnowabottleneck, weensurethe
model has N basis functions (e s).
k,l
(cid:88)q−1 (cid:88)Nc (cid:88)r
f (i,x;a,B)= a (T)B (T)e (i,x)+ a (T)B (T)e (i,x), (41)
T k k,l k,l q q,l′ q,l′
k=1l=1 l′=1
where N is the number of basis functions needed to express a skill, quotient q is ⌊(N −1)/N ⌋+1,6
c c
and remainder r is such that (q−1)N +r =N. In short, the N basis functions are
c
[e , ··· , e , e , ··· , e ]. (42)
1,1 1,Nc 2,1 q,r
Similar to Eq. (39), the basis functions satisfy the following properties
(cid:88)Nc
1
E [e e ]=δ , e (I ̸=k,x)=0, √ e =g . (43)
X|I=k k,l k,l′ ll′ k,l k,l k
N
c
l=1
N basis functions for a skill. For the extended model in Eq. (41), the skill strength at T,D →∞
c
for a given N becomes

0 :k >q

R (∞)= S r :k =q (44)
k

SNc
:k <q.
Proof See Appendix E.4.
WecanderiveEq.(44)becausethebasisfunctions[e ,··· ,e ]fork <q canexpressg (Eq.(43))
k,1 k,Nc k
but [e ,··· ,e ] cannot express g when r <N .
q,1 q,r q c
In Fig. 6(a, iii), the extended model (Eq. (41)) with N =4 fits the n =1 set up where N is the
c s
hidden-layer width of the NN. The N = 4 model leads to good prediction for the n = 5 setup in
c s
Fig. 6(b, iii). The results suggest that an NN, while lacking the ordering of basis functions (Eq. (42)),
prefers to use the hidden neuron in fitting more frequent skills. The ‘preference’ toward frequent skills
is in agreement with Fig. 6(b,i) where NN learns more frequent skills first.
6.4 Limitations of the multilinear model
Our extended multilinear model, with the decoupled dynamics for each skill, predicts the time, data,
and parameter emergence with a single calibration. However, the dynamics of a simple model with
strong assumptions such as in-built g differ from the more complex dynamics of NNs that lack such
k
assumptions.
Time emergence. WenotethattheNNandthemultilinearmodelemergeatsimilarinstances,but
the NN takes longer to saturate fully. This is because, for a given skill, the dynamics of the NN is not
one sigmoidal saturation but a sum of multiple sigmoidal dynamics with different saturation times.
To express the parity function, the NN must use multiple hidden neurons, and the skill strength can
be divided into the skill strength from each neuron7 whose dynamics follow a sigmoidal saturation.
Because of the non-linearity and the function it expresses, each neuron is updated at different rates,
and the slowly saturating neurons result in a longer tail in comparison to our multilinear model. For
an example, see Fig. 9 in Appendix F.
5We are free to use any optimizer as long as it preserves the stage-like training or the order in which the skills are
learned.
6⌊a/b⌋isthequotientorint(a/b).
7This is possible because of the linearity of the last layer. See Appendix F for the definition of skill strength per
neuron.
14Data point emergence. Our extended model (Eq. (40)) deviates from the NN when d ≪ D :
k c
NN shows a more abrupt change in R as a function of D. This is because our model asserts strict
k
decoupling among the skills: even a few d will contribute to learning g from e . This differs from
k k k,l
the NN, which lacks strict decoupling among the samples from different skills. We speculate that
because NNs can perform benign [39] or tempered [40] overfitting, they treat a few data points from
less frequent skills as ‘noise’ from more frequent skills: requiring more samples to learn the infrequent
skills.
Parameter emergence. Note that Fig. 6(b, iii) has high variance compared to other emergence
plots in Fig. 6(b); this is because NN sparsely, over many repeated trials, use the hidden neurons to
learn less frequent skills over more frequent ones (See Fig. 11 in Appendix H for an example of such
outliers). Because the ‘preference’ of NNs toward more frequent skills is not as strict as in our model,
we speculate that initial conditions (ones that ease the learning of less frequent skills) play a role in
creating outliers.
7 Discussion and Conclusion
Thispaperestablishedanexplicitsettingtoinvestigateemergencebyrepresentingskillsasorthogonal
functions. We proposed a tractable multilinear model—with the skill functions g as the basis—that
k
shows emergence and scaling laws. We additionally extended the model to predict emergence in a
2-layer NN.
Inourmultilinearmodel, theskillfunctions(asbasisfunctions)decoupletheloss, soeachskillloss
evolvesindependentlyoftheothers(Eq.(15)). Asaconsequence, fortime, skilllearningdynamicsare
decoupled (Eq. (16)); for data, they depend only on that specific skill’s observation (Eq. (27)); for the
parameter, learning of the Nth skill depends only on g (Eq. (32)); these are the main properties we
N
use to derive the scaling laws.
Given the importance of g in our model, especially in decoupling the skills, it is puzzling why
k
our model can describe emergence in NNs, which lack the g s as the basis. NNs, with the ability to
k
feature-learn [41, 42] (for recent studies on feature learning, see e.g. [43–48]), will eventually fit the
target function, but it need not learn each g in a decoupled manner. For example, an NN is free to
k
learn any linear combination of g in any order independent of the skill frequency. We speculate that
k
the layerwise structure – which leads to a stage-like training, along with the significant differences in
skill frequency (Zipfian), prompts the NN to learn the most frequent g with limited resources (time,
k
data, or parameters): an effective decoupling of skills.
We can interpret the skill functions as features – the functions useful in describing the target
function [41] – and the multilinear model as a ‘feature-learned’ model already equipped with the
g s. In this interpretation, the resemblance between emergence in NNs and the multilinear model
k
(Section 6) may inform us about the relationship between feature learning and emergence [6]; the
feature learning of the kth skill – learning to express g – may have occurred well before R saturates
k k
(and L decreases). Investigating the role of features (and feature learning) in emergence is left for
k
future work.
WhilemuchworkremainstounderstandLLMs,itisencouragingthatasimplemodelcanreplicate
emergenceandthatanNN,byitslayerwisestructure,allowsdecoupledapproximations. Ourproposal
shedslightontherelationshipbetweenmodelscaleandperformanceandlaysthefoundationforfurther
theoretical investigations into the emergence of complex skills in large-scale models.
We, similar to many previous investigations (see e.g., [17, 19]), used a simple data structure with
a power-law distribution. This begs the question of how well these results hold for more realistic
datasets. In future research, we plan to explore setups involving natural language processing tasks
to establish a tighter connection between ‘skills’ exhibited by LLMs and our theoretical framework.
One approach is to design a task where the skill functions g are directly relevant to language tasks,
k
such as translation with Zipfian word frequencies. This extension is natural since our current work
only utilizes the orthogonality and frequency of g , without relying on the fact that they are parity
k
functions. By validating our theoretical findings in language tasks using transformer-like models, we
aim to contribute to a broader understanding of how neural networks acquire and exhibit complex
behaviors.
158 Acknowledgements
NF acknowledges the UKRI support through the Horizon Europe guarantee Marie Sk(cid:32)lodowska-Curie
grant(EP/X036820/1). SLwassupportedbytheNationalResearchFoundationofKorea(NRF)grant
fundedbytheKoreangovernment(MSIT)(No.2020R1A5A1016126). WethankCharlesLondon,Zohar
Ringel, and Shuofeng Zhang for their helpful comments.
References
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[2] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom
Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large
generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and
Transparency, pages 1747–1764, 2022.
[3] AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond
the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv
preprint:2206.04615, 2022.
[4] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
language models. arXiv preprint: 2206.07682, 2022.
[5] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language
models a mirage? Advances in Neural Information Processing Systems, 36, 2023.
[6] Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase,
Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman,
Zhaowei Zhang, Mario Gu¨nther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric
Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se´an O´
hE´igeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards,
Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer,
He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring
alignment and safety of large language models. arXiv preprint: 2404.09932, 2024.
[7] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Con-
erly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu,
Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex
Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter,
Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language
models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-
circuits.pub/2023/monosemantic-features/index.html.
[8] JoelHestness, SharanNarang, NewshaArdalani, GregoryDiamos, HeewooJun, HassanKianine-
jad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint:1712.00409, 2017.
[9] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
ScottGray,AlecRadford,JeffreyWu,andDarioAmodei.Scalinglawsforneurallanguagemodels.
arXiv preprint:2001.08361, 2020.
[10] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive pre-
diction of the generalization error across scales. arXiv preprint: 1909.12673, 2019.
[11] TomHenighan,JaredKaplan,MorKatz,MarkChen,ChristopherHesse,JacobJackson,Heewoo
Jun,TomBBrown,PrafullaDhariwal,ScottGray,etal.Scalinglawsforautoregressivegenerative
modeling. arXiv preprint:2010.14701, 2020.
16[12] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural
machine translation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-
tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 5915–5922, Online and Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics.
[13] XiaohuaZhai,AlexanderKolesnikov,NeilHoulsby,andLucasBeyer. Scalingvisiontransformers.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages
12104–12113, 2022.
[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint:2203.15556, 2022.
[15] GregorBachmann,SotirisAnagnostidis,andThomasHofmann. Scalingmlps: Ataleofinductive
bias. Advances in Neural Information Processing Systems, 36, 2024.
[16] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in
Neural Information Processing Systems, 35:21750–21764, 2022.
[17] Eric Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural
scaling. Advances in Neural Information Processing Systems, 36, 2023.
[18] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. Proceedings of the International Conference
on Learning Representations 2014, 2014. arXiv:1312.6120.
[19] Marcus Hutter. Learning curve theory. arXiv preprint:2102.04074, 2021.
[20] Utkarsh Sharma and Jared Kaplan. Scaling laws from the data manifold dimension. Journal of
Machine Learning Research, 23(9):1–34, 2022. arXiv:2004.10802.
[21] YasamanBahri,EthanDyer,JaredKaplan,JaehoonLee,andUtkarshSharma. Explainingneural
scaling laws. arXiv preprint:2102.06701, 2021.
[22] Alexander Maloney, Daniel A Roberts, and James Sully. A solvable model of neural scaling laws.
arXiv preprint:2210.16859, 2022.
[23] Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling
laws. arXiv preprint:2402.01092, 2024.
[24] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel meth-
ods: empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory
and Experiment, 2020(12):124001, 2020.
[25] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves
inkernelregressionandwideneuralnetworks. InInternational Conference on Machine Learning,
pages 1024–1034. PMLR, 2020.
[26] HugoCui, BrunoLoureiro, FlorentKrzakala, andLenkaZdeborov´a. Generalizationerrorratesin
kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural Informa-
tion Processing Systems, 34:10131–10143, 2021.
[27] Boaz Barak. Windows on theory blog: Emergent abilities and grokking: Fundamental, mirage,
orboth? https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-
mirage-or-both/, 2023.
[28] Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language models.
arXiv preprint:2307.15936, 2023.
17[29] Maya Okawa, Ekdeep S Lubana, Robert Dick, and Hidenori Tanaka. Compositional abilities
emerge multiplicatively: Exploring diffusion models on a synthetic task. Advances in Neural
Information Processing Systems, 36, 2024.
[30] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev
Arora. Skill-mix: A flexible and expandable family of evaluations for ai models. arXiv
preprint:2310.17567, 2023.
[31] MayeeChen,NicholasRoberts,KushBhatia,JueWang,CeZhang,FredericSala,andChristopher
R´e. Skill-it! a data-driven skills framework for understanding and training language models.
Advances in Neural Information Processing Systems, 36, 2023.
[32] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking:
Generalization beyond overfitting on small algorithmic datasets. arXiv:2201.02177, 2022.
[33] Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication
attempt. arXiv preprint:2404.10102, 2024.
[34] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. Nature
communications, 12(1):2914, 2021.
[35] Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cl´ement Hongler, and Franck Gabriel. Kernel
alignment risk estimator: Risk prediction from training data. Advances in Neural Information
Processing Systems, 33:15568–15578, 2020.
[36] OunsElHarzli,BernardoCuencaGrau,GuillermoValle-P´erez,andArdALouis. Double-descent
curvesinneuralnetworks: anewperspectiveusinggaussianprocesses. InProceedingsoftheAAAI
Conference on Artificial Intelligence, pages 11856–11864, 2024.
[37] James B Simon, Madeline Dickens, and Michael R DeWeese. A theory of the inductive bias and
generalization of kernel regression and wide neural networks. arXiv e-prints, pages arXiv–2110,
2021.
[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint:1412.6980, 2014.
[39] Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overfitting in
linear regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.
[40] Neil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum
Nakkiran. Benign,tempered,orcatastrophic: Towardarefinedtaxonomyofoverfitting. Advances
in Neural Information Processing Systems, 35:1182–1195, 2022.
[41] YoshuaBengio,AaronCourville,andPascalVincent. Representationlearning: Areviewandnew
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,
2013.
[42] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.
[43] Greg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural
networks. In International Conference on Machine Learning, pages 11727–11737. PMLR, 2021.
[44] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners:
The silent alignment effect. arXiv preprint arXiv:2111.00034, 2021.
[45] Arthur Jacot, Eugene Golikov, Cl´ement Hongler, and Franck Gabriel. Feature learning in l 2-
regularized dnns: Attraction/repulsion and sparsity. Advances in Neural Information Processing
Systems, 35:6763–6774, 2022.
[46] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution
in wide neural networks. Advances in Neural Information Processing Systems, 35:32240–32256,
2022.
18[47] Inbar Seroussi, Gadi Naveh, and Zohar Ringel. Separation of scales and a thermodynamic de-
scription of feature learning in some cnns. Nature Communications, 14(1):908, 2023.
[48] Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M Lu, Lenka Zdeborov´a, and Bruno
Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step. arXiv
preprint arXiv:2402.04980, 2024.
[49] Irina Gennad’evna Shevtsova. Sharpening of the upper bound of the absolute constant in the
berry–esseen inequality. Theory of Probability and Its Applications, 51(3):549–553, 2007.
[50] Hugh L Montgomery and Robert C Vaughan. Multiplicative Number Theory I: Classical Theory.
Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2007.
19A Glossary
A Normalization constant for P such that P (k)=Ak−(α+1)
s s
T Time or step
D Number of data points
N Number of parameters (skill basis functions in the model)
C The computation cost T ×N
n The number of skills in the multitask sparse parity problem
s
I Random variable of the control bits
X Random variable of the skill bits
P Probability of skills (control bits)
s
P Probability of skill bits
b
S The target scale or the norm of the target function
R Skill strength of the kth skill (Eq. (9))
k
L Total (generalization) loss
L(D) Empirical loss for D samples
L Skill loss of the kth skill (Eq. (10))
k
d Number of observation of the kth skill (i.e. number of training points (i,x) with g (i,x)̸=0)
k k
f∗ Target function f∗ :{0,1}ns+nb →{−S,S} (Eq. (4))
g
k
The kth skill basis function g
k
:{0,1}ns+nb →{−1,0,1} (Eq. (2))
20Control bits Skill bits y
10000000000 110001000001010 1
01000000000 010100100001000 0
00100000000 001101010110101 1
. . .
. . .
. . .
00000000001 100010001001100 1
Table 3: Representation of the multitask sparse parity as presented in [17]. The control bits are
one-hot vectors encoding a specific parity task. The frequency of the different tasks follows a Zipfian
distribution. Inthisexample, therearen =10tasks, andskillbitsarelengthn =15. They column
s b
is the resulting parity computed from m = 3 bits (highlighted in colors). The multitask dataset
provides a controlled experimental setting designed to investigate skills.
B Background
Inthefollowing,wedescribethemultitasksparseparitydatasetasin[17],andthenonlineardynamics
of two-layer linear networks as in [18].
B.1 Multitask sparse parity
The sparse parity task can be stated as follows: for a bit string of length n , the goal is to determine
b
theparity(summod2)ofapredeterminedsubsetofmbitswithinthatstring. Themultitasksparse
parity [17] extends this problem by introducing n unique sparse parity variants in the dataset. The
s
inputbitstringshavealengthofn +n . Thefirstn bitsfunctionasindicatorsbyassigningaspecific
s b s
task. The frequency of the distinct parity tasks follows a rank-frequency distribution with an inverse
powerlawrelation(Zipfiandistribution). Thelastn bitsareuniformlydistributed. Thissetsabinary
b
classification problem {0,1}ns+nb →{0,1} where only a single bit of the initial n
s
bits is nonzero. In
Table 3, the many distinct parity tasks represent different skills.8
The proposal in [17] aims to reconcile the regularity of scaling laws with the emergence of abilities
withscaleusingthreekeyhypotheses: (i)skills,representedasafinitesetofcomputations,aredistinct
and separate; (ii) these skills differ in their effectiveness, leading to a ranking based on their utility
to reduce the loss; and (iii) the pattern of how frequently these skills are used in prediction follows a
Zipfian power-law distribution. Interestingly, the multitask problem has a consistent pattern across
scaling curves: each parity displays a distinct transition, characterized by a sharp decrease in loss
at a specific scale of parameters, data, or training step. Such a sudden shift occurs after an initial
phaseofnonoticeableimprovement,leadingtoreversesigmoid-shapedlearningcurves. Michaudetal.
[17] empirically show that for a one-hidden-layer neural network with ReLU activation, trained using
cross-entropy loss and the Adam optimizer, these transitions happen at different scales for distinct
tasks. This results in a smooth decrease in the overall loss as the number of skill levels increases.
B.2 Nonlinear dynamics of linear neural network
Saxe et al. [18] have solved the exact dynamics for two-layer linear neural networks with gradient
descent9 under MSE loss (Fig. 7(a)). The dynamics decompose into independent modes that show
sigmoidal growth at different timescales (Fig. 7(c)). The setup assumes orthogonal input features
X ∈Rd1 and input-output correlation matrix Σ∈Rd1×d3 for target output f∗(X)∈Rd3:
E [X X ]=δ , Σ=E
(cid:2) Xf∗T(X)(cid:3)
(45)
X i j ij X
By performing SVD (singular value decomposition) on Σ = UΛV, the target function f∗ : Rd1 →
Rd3 becomes:
f∗(x)=(cid:88)d2
v λ uTx, UΛV =E (cid:2) Xf∗(X)T(cid:3) (46)
k k k X
k=1
8Notethatherewefollowtheeven/oddparityconventionusedin[17], i.e., {0,1}, insteadof{1,−1}asusedinthe
maintext.
9Tobespecific,itisundergradientfloworthecontinuouslimitoffullbatchgradientdescent.
21λ 1=1.0 λ 2=0.5 λ 3=0.25
1.0
0.8
0.6
(a)Linearneuralnetwork
0.4
0.2
0.0
0 25 50 75 100
T
(b)Independentmodes (c)Dynamicsofmodes
Figure 7: Nonlinear dynamics of linear neural networks. (a) A two-layer undercomplete neural
network, which is a multiplication of two matrices, where d < d and d < d . (b) The d indepen-
2 1 2 3 2
dent modes of dynamics for linear neural network (Eq. (47)). The product of parameters a b are
k k
learnableparametersandvectorsu ,v areobtainedfromSVDoftheinput-outputcorrelationmatrix
k k
Σ (Eq. (45)). (c) The temporal evolution of a b under gradient descent, which follows a sigmoidal
k k
growth (Eq. (48)). Note that smaller λ – the singular value of Σ – results in delayed saturation of
k
a b .
k k
where u
k
∈ Rd1,v
k
∈ Rd3 are the row vectors of U,V and λ
k
∈ R are the singular values of Λ of the
input-output correlation matrix Σ.
The dynamics of a two-layer undercomplete linear neural network (the width of the hidden layer
is smaller than the width of the input and output) equals that of the following model:
vTf(x;a,b)=a b uTx k ∈{1,2,··· ,d }. (47)
k k k k 2
wherea k,b
k
∈Raretheparameters. NotethatEq.(47)ared
2
decoupledfunctionsv kTf(x):Rd1 →R
(Fig.7(b)). Assumingsmallandpositiveinitialization(0<a (0)b (0)≪λ ),thedynamicsofEq.(47)
k k k
undergradientdescentwithlearningrateη canbesolvedanalytically; theproductofparametersa b
k k
grows sigmoidally with saturation time proportional to λ−1 (Fig. 7(c)):
k
a (T)b (T) 1
k k = . (48)
(cid:16) (cid:17)
λ k 1+ λk −1 e−2ηλkt
ai(0)bi(0)
As this is a linear neural network, the sigmoid growth is the result of the nonlinear dynamics of
learning (non-linear activations are not present). Saxe et al. [18] demonstrate that multilinear models
show delayed sigmoidal growth of different modes and show that Eq. (48) agrees with the empirical
dynamics of a two-layer neural network with tanh activations.
22
...
... ...
...
λ/
b
a
k
k
kC Derivation of the multilinear model
In this section, we provide derivation for our multilinear model. The two corollaries for data and
parameters (Corollaries 1 and 2) follow from the decoupled dynamics of NN (Lemma 1).
C.1 Decoupled dynamics of the multilinear model
Lemma 1. Let the multilinear model Eq. (12) be trained with gradient on D i.i.d samples for the
setup in Section 3 (input distribution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5)). Let
k ≤N be a skill index in the multilinear model and the input distribution (k ≤n ). Then assuming the
s
following initialization a (0) = b (0) and 0 < a (0)b (0) < S, the dynamics of the kth skill strength
k k k k
(R ) is
k
S
R (T)= (49)
k (cid:16) (cid:17)
1+ RkS
(0)
−1 e−2ηSd DkT
and the skill loss is
S2
L (T)= . (50)
k (cid:18) (cid:16) (cid:17)−1 (cid:19)2
2 1+ RkS
(0)
−1 e2ηSd DkT
where η is the learning rate and d
k
is the number of observations with g k(I =k,x(jk))̸=0.
Proof Forj =1,··· ,D,denote(i(j),x(j))bethejthdatapointinthetrainingset. Thentheempirical
loss for D datapoints is given as
D
1 (cid:88)(cid:16) (cid:17)2
L(D) = f∗(i(j),x(j))−f(i(j),x(j)) . (51)
2D
j=1
We note that
(cid:16) (cid:17)2
(cid:32) (cid:88)ns (cid:33)2
f∗(i(j),x(j))−f(i(j),x(j)) = (S−a b )g (i(j),x(j))
k k k
k=1
=(S−a b )2g (i(j),x(j))2
i(j) i(j) i(j)
=(S−a b )2,
i(j) i(j)
as g (i,j) ∈ {1,−1} and g (i,j) = 0 for i ̸= k. So if we denote d the number of data points with
i k k
i(j) =k, then we can conclude
1
(cid:88)D
1
(cid:88)ns
L(D) = (S−a b )2 = d (S−a b )2, (52)
2D i(j) i(j) 2D k k k
j=1 k=1
which is the decoupled loss in the main text (Eq. (15)). Using the gradient descent equation and
Eq. (52), we obtain
da dL
k =−η D (53)
dt da
k
d
=−η kb (a b −S). (54)
D k k k
Likewise, we can obtain the equation for b as
k
db d
k =−η ka (a b −S). (55)
dt D k k k
Because of symmetry between a and b (See Appendix B.2 or [18]), assuming a (0) = b (0), and
k k
a (0)b (0)>0 results in a (T)=b (T) for all T. The equation for R =a b is
k k k k k k k
dR da db d
k =−η kb +a k =−η k(b2 +a2)(a b −S) (56)
dt dt k k dt D k k k k
d
=−2η kR (R −S). (57)
D k k
23Assuming a (0)b (0)<S, we can solve the differential equation to obtain
k k
S
R (T)= . (58)
k (cid:16) (cid:17)
1+ RkS
(0)
−1 e−2ηSd DkT
The equation for L follows from Eq. (14).
k
C.2 One-shot learner
Corollary 1. For the setup in Lemma 1, the kth skill loss (L ) at T,N →∞ is
k
(cid:26)
0 :d >0
L (∞)= k (59)
k (S−R (0))2/2≈S2/2 :d =0,
k k
where d is the number of kth skill’s observation.
k
Proof The corollary follows directly from Lemma 1. By taking T,N →∞,
(cid:26)
d >0: S
R (∞)= k (60)
k d =0: R (0)
k k
We obtain the result by using the relationship between R and L in Eq. (14).
k k
C.3 Equivalence between a basis function and a skill
Corollary 2. Let the multilinear model Eq. (12) be trained with gradient on D i.i.d samples for the
setup in Section 3 (input distribution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5)).
Assume a (0) = b (0), 0 < a (0)b (0) < S, and that the model has N most frequent skills as basis
k k k k
functions. Then R for the kth ≤ns skill at T,D →∞ is
k
(cid:26)
0 :k ≤N
L (∞)= (61)
k S2/2 :k >N
Proof The corollary follows directly from Lemma 1. By taking T,D →∞,
(cid:26)
k ≤N : S
R (∞)= (62)
k k >N : R (0)
k
We obtain the result by using the relationship between R and L in Eq. (14) and R (0)≪S.
k k k
D Detailed derivation of the scaling laws
This section provides a detailed derivation of the scaling laws up to a rigor common in physics and
engineering. For example, we approximate the Riemann sum as integral or treat k, the number of
skills, as a differentiable parameter. For more general and rigorous derivations and discussions, see
Appendix J.
D.1 Summary of the scaling laws
Bottleneck Time Data Parameter Exponent
Time (T) T ∞ ∞ −α/(α+1)
Data (D) ∞ D ∞ −α/(α+1)
Parameter (N) ∞ ∞ N −α
Compute (C) C(α+1)/(α+2) ∞ C1/(α+2) −α/(α+2)
24D.2 Time scaling law
With the dynamics of R solved as Eq. (16), we can calculate the skill loss L = (S −R (T))2/2
k k k
(given as Eq. (14)) as follows:
S2
L = . (63)
k (cid:18) (cid:16) (cid:17)−1 (cid:19)2
2 1+ RkS
(0)
−1 e2ηd DkST
Noting that d /D →P as D →∞, where P =Ak−(α+1), we have
k s s
S2
L = . (64)
k (cid:18) (cid:16) (cid:17)−1 (cid:19)2
2 1+ S −1 e2ηAk−(α+1)ST
Rk(0)
This is a function of k−(α+1)T only, suggesting the decoupling of dynamics for each skill. Thus,
dL k dL
k =− k. (65)
dT (α+1)T dk
Using Eq. (8) and taking N,n → ∞ at the same rate10, we can approximate the loss as integral
s
instead of a sum over k:
(cid:90) N
L≈ lim Ak−(α+1)L dk, (66)
k
N→∞ 1
whereAisthenormalizationconstantforP . WecandifferentiatethelossanduseEq.(65)toexpress
s
the equation in terms of k:
dL (cid:90) N dL 1 (cid:90) N dL
= lim Ak−(α+1) kdk =− lim Ak−α kdk. (67)
dT N→∞ 1 dT N→∞(α+1)T 1 dk
Integrating by parts, we obtain
dL =− lim 1 (cid:2) Ak−αL (cid:3)N − lim α (cid:90) N Ak−(α+1)L dk (68)
dT N→∞(α+1)T k 1 N→∞(α+1)T
1
k
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 α
=− lim O N−α +O − L. (69)
N→∞ T TeT (α+1)T
The first term goes to 0 as N → ∞ and the second term goes to 0 exponentially faster compared to
the last term for T ≫1, which leads to the scaling law:
dL(T) α dT
=− . (70)
L(T) α+1 T
Finite N correction for small α. In Fig. 8, we observe that our model with α = 0.1 deviates
from the expected power law with exponent −α/(α + 1). The deviation can be explained by the
antiderivative term in Eq. (68):
 N
1 S2A k−α (cid:18) (cid:18) 1(cid:19) (cid:18) 1 (cid:19)(cid:19)
lim   = lim O N−α −O . (71)
N→∞2(α+1)(cid:16) 1+ 1 e2ηSAk−(α+1)T(cid:17)2 T  N→∞ T TeT
S/Rk(0)−1
1
The second term (k = 1) goes to 0 faster than O(T−1) for sufficiently larger T but the first term
(k = N) may not decay fast enough for finite N and sufficiently small α. For example, N = 50,000
and α=0.1 leads to N−α ≈0.3, which is not negligibly small.
10WetakeN andns to∞atthesameratebecausewedonotwanttheparameterstobeabottleneckinthissetup.
25α=0.1 α=0.3 α=0.5 α=0.7
101
simulation
100 powerlaw(corrected)
powerlaw
100 101 102 103
T
Figure 8: Scaling law and corrected predictions. A simulation of our multilinear model with
N = 50,000 (solid), a scaling law with exponent −α/(α+1) (dotted), and a corrected scaling law
consideringfiniteN (dashed,Eq.(72)). ThefiniteN correctedscalinglawbetterpredictsthedynamics,
especially for smaller α.
AssumingfiniteN andsmallαsuchthatthefirstterminEq.(71)isnon-negligible,wecanrewrite
Eq. (68) as
dL α L+L
≈− C, L ≈S2AN−α/2α, (72)
dT (α+1) T C
where we assumed a small initialization S/R (0) ≫ 1 and sufficiently large number of parameters
k
Nα+1 ≫ T to approximate L . Because the total loss at initialization is L(0) = S2/2, L is non-
C C
negligible compared to the loss for sufficiently small α. Thus considering L , we obtain the corrected
C
power-law which better approximates the time scaling law (dashed lines in Fig. 8). For a rigorous and
comprehensive analysis of the time scaling law, see Theorem 2 and Theorem 3 in Appendix J.
D.3 Data scaling law
In this section, we show the data scaling law in more detail. Using the one-shot learning property
(Eq. (27)), the probability of observing the kth skill (Eq. (28)), and the decomposition of the loss into
skill losses (Eq. (7)), the expected loss for D datapoints is
∞
(cid:88)
E [L]= S2P (I =k)(1−P (k)) (73)
D s observed
k=1
∞
=S2A(cid:88) k−(α+1)(1−P (I =k))D (74)
s
k=1
(cid:90) ∞ (cid:16) (cid:17)D
≈S2A k−(α+1) 1−Ak−(α+1) dk, (75)
1
where A is the normalization constant such that P(I = k) = Ak−(α+1). The difference in the loss
∆L=E [L]−E [L] is
D+1 D
(cid:90) ∞ (cid:16) (cid:17)D(cid:16)(cid:16) (cid:17) (cid:17)
∆L=S2A k−(α+1) 1−Ak−(α+1) 1−Ak−(α+1) −1 dk (76)
1
(cid:90) ∞ (cid:16) (cid:17)D
=−S2A2 k−2(α+1) 1−Ak−(α+1) dk. (77)
1
26
LWe can integrate ∆L by parts.
(cid:20) S2Ak−α (cid:16) (cid:17)D+1(cid:21)∞
∆L= − 1−Ak−(α+1)
(α+1)(D+1)
1
S2Aα (cid:90) ∞ (cid:16) (cid:17)D+1
− k−(α+1) 1−Ak−(α+1) dk
(α+1)(D+1)
1
≈O(cid:0)
(1−P
(1))D+1(cid:1)
−
S2Aα (cid:90) ∞ k−(α+1)(cid:16) 1−Ak−(α+1)(cid:17)D(cid:16) 1−Ak−(α+1)(cid:17)
dk
s (α+1)(D+1)
1
α α
≈− E [L]+ ∆L.
(α+1)(D+1) D (α+1)(D+1)
In the second line, the first term goes to 0 for D ≫1. In the last line, we used the expression for ∆L
(Eq. (76)) and E [L] (Eq. (73)). Rearranging the equation above and using that D ≫1, we obtain
D
∆L α α 1
=− ≈− (78)
E [L] 1+(α+1)D (α+1)D
D
α ∆D
=− . (79)
(α+1) D
where in the last line, ∆D/D =1/D as the change in the number of data points relative to D is one.
D.4 Optimal compute scaling law
We start from Eq. (24) with D →∞
(cid:90) N S2 (cid:90) ns
L≈ Ak−(α+1)L dk+ lim Ak−(α+1)dk. (80)
1
k ns→∞ 2
N
We can use Eq. (72) to calculate the first term and integrate the last term to get
S2A
L≈(L(0)+L )T−α/(α+1)−L + N−α (81)
C c 2α
≈O(T−α/(α+1))+O(N−α). (82)
where we used that L(0) ≫ L and S2A/(2α)−L > 0. Intuitively, the approximation shows the
C C
tradeoffbetweenT –whenincreased, decreasesthelossofthefirstN skills–andN –whenincreased,
decreases the loss at sufficiently large T – for fixed compute C. For a comprehensive analysis of the
approximation above, see Appendix J.
Removing the irrelevant constant terms,
L=T−α/(α+1)+N−α. (83)
We can use the method of Lagrangian multiplier to obtain
α
− T−α/(α+1)−1+λN =0 (84)
α+1
−αN−(α+1)+λT =0 (85)
NT −C =0, (86)
where λ is the Lagrange multiplier and C is compute. We can solve the above set of equations to
obtain Tα+1 ∝N and plug it in Eq. (83) to get
L∝C−α/(α+2). (87)
The derivation is similar to that of [23]. For a rigorous derivation of the optimal compute scaling law,
see Corollary 4 and Appendix J.
27E Derviation of the extended multilinear model
E.1 Gradient flow in the extended multilinear model
Lemma 2. Let the extended multilinear model Eq. (38) be trained with gradient on D i.i.d samples for
the setup in Section 3 (input distribution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5)).
For the skill index k ≤N be a skill index in the multilinear model, let the feature matrix Φ∈RDC×dk
for the kth skill be
Φ =e (i(j) =k,x(j)), (88)
lj k,l
and SVD on Φ = USV. Assuming that the system is overparametrized (d < D ), the gradient on
k c
B
k
∈RDC is contained in the column space of semi-orthogonal matrix U ∈RDC×dk:
dB dB
UUT k = k. (89)
dt dt
Proof Similar to Lemma 1, the total loss can be decomposed into each skill such that the dynamics
of B relies only on d observations of the kth skill:
k,l k
1
(cid:88)ns (cid:88)D
(cid:16) (cid:17)2
L = f∗(i(j),x(j))−f(i(j),x(j)) (90)
D 2D
k=1j=1
1
(cid:88)ns (cid:88)dk (cid:32) (cid:88)DC (cid:33)2
=
2D
Sg k(k,x(jk))− a kB k,le k,l(k,x(jk)) (91)
k=1jk=1 l=1
1
(cid:88)ns (cid:88)dk (cid:32) (cid:88)DC
S
(cid:33)2
=
2D
(√
D
−a kB k,l)e k,l(k,x(jk)) . (92)
k=1jk=1 l=1 C
In the second line, we used Eq. (39) that e (I ̸= k,x) = 0 and the orthogonality of g (Eq. (3)). In
k,l k
the last line, we used Eq. (39) that g = D
−1/2(cid:80)
e . We can find the gradient descent equation
k C l k,l
of B from Eq. (92):
k,l
dB
k,l
=−η(cid:88)dk
1
(cid:34)
a e
(k,x(j))(cid:88)DC
(a B −
√S
)e
(k,x(j))(cid:35)
, (93)
dt D k k,l k k,l′ D k,l′
j=1 l′=1 C
which in the matrix form is
(cid:32) (cid:33)
dB ηa S⃗
k =− kΦΦT B a − √ , (94)
dt D k k D
C
where D dimensional vectors B and S⃗ are [B ,··· ,B ] and [S,··· ,S] respectively. It illus-
c k k,1 k,DC
trates that dBk is contained in im(Φ), which is contained in im(U) (immediate from Φ = USV). As
dt
UUT(Uz)=U(UTU)z =Uz, UUT acts as identity on image of U, showing that UUTdBk =UTdBk.
dt dt
E.2 Conserved quantity of extended multilinear model
Lemma 3. In the setup of Lemma 2, a2 −|B |2 is conserved over time.
k k
Proof We can use Eq. (92) to find the equation for a :
k
da k
=−η(cid:88)dk
1
(cid:34)
(cid:88) B e
(k,x(j))(cid:88)DC
(a B − √S )e
(k,x(j))(cid:35)
, (95)
dt D k,l k,l k k,l′ D k,l′
j=1 l=1 l′=1 C
28which in the matrix form is
(cid:32) (cid:33)
da η S⃗
k =− BTΦΦT B a − √ . (96)
dt D k k k D
C
Then
(cid:32) (cid:33)
da ηa S⃗
a k =− kBTΦΦT B a − √ (97)
k dt D k k k D
C
dB
=BT k, (98)
k dt
where we used Eq. (94) in the last line. Thus, a2 −|B |2 is conserved during the dynamics.
k k
E.3 D shot learner
C
Proposition 1. Let the setup be as that in Lemma 2. Suppose that a (T) is eventually bounded away
k
from zero, i.e. there exists δ > 0 and M > 0 such that T > M ⇒ |a (T)| ≥ δ. Also assume that
k
U⊥-component of B (0)a (0) and B (0)S is negligible. Then the skill strength R is
k k k k
(cid:40) (cid:16) (cid:112) (cid:17)
d <D : S 1− 1−d /D
R (∞)= k c k c (99)
k
d ≥D : S
k c
Proof First, we show that dL ≤0 with equality only holding when the gradient is 0.
dt
dL
=
dLda
+(cid:88)Dc
dLdb
i (100)
dt da dt db dt
i
i
dLdL
(cid:88)Dc
dL dL
=− − ≤0. (101)
da da db db
i i
i
Its equality holds only when
dL da dL db
= =0 and = i =0 (102)
da dt db dt
i
We first show that both a and B are bounded throughout whole dynamics. As
k k
(cid:12) (cid:32) (cid:33)(cid:12)2 (cid:12) (cid:32) (cid:33)(cid:12)2
(cid:12) S⃗ (cid:12) (cid:12) S⃗ (cid:12)
L =(cid:12)Φ B a − √ (cid:12) ≥σ2(cid:12)UUT B a − √ (cid:12) (103)
k (cid:12) k k D (cid:12) (cid:12) k k D (cid:12)
(cid:12) C (cid:12) (cid:12) C (cid:12)
for σ2 the smallest nonzero eigenvalue of ΦΦT, where Φ=USV. This shows
(cid:32) (cid:33)
S⃗
UUT B a − √ (104)
k k
D
C
is bounded, so UUTB ka k is bounded. Meanwhile, in Lemma 2, we showed (1−UUT)d dB tk = 0, so
(1−UUT)B a is bounded. This shows B a is bounded. As a −|B |2 is constant (Lemma 3) and
k k k k k k
|B a |=|a ||B | is bounded, this shows both a and |B | are bounded.
k k k k k k
Thedynamicsmovinginsomeboundedregionalwayshasatleastoneaccumulationpoint,whichwe
denoteasp. Wewillshowthat d dL tk =0atp. ThefunctionL k(t)intisdecreasingdifferentialfunction
which is positive. We also note
d2Lk(t)
is globally bounded, as it can be expressed in polynomial
dt2
expressionin(a ,B )andweshowed(a (t),B (t))isbounded. FromTaylor’stheoremonecanobtain
k k k k
dL t2
infL (t)≤L (t +t )≤L (t )+t k(t )+ 2M (105)
k k 1 2 k 1 2 dt 1 2
29for M =sup|d2L dtk 2(t)|. Choosing t
2
=−d dL tk(t 1)M−1 shows that
1
(cid:18)
dL
(cid:19)2
L (t )− k(t ) ≥infL (t) (106)
k 1 2M dt 1 k
and letting t →∞ here gives
1
1
(cid:18)
dL
(cid:19)2
lim k(t ) ≤ lim (L (t )−infL (t))=0 (107)
t1→∞2M dt 1 t1→∞ k 1 k
so d dL tk → 0 as t → ∞. Meanwhile, as p is accumulation point of (a k,B k), d dL tk(p) is accumulation
point of d dL t(a k(t),b k(t)). As lim
t→∞
d dL tk(t)=0, the only accumulation point of d dL tk(t) is zero, which
shows that dLk(p)=0.
dt
Wehaveseenthata2−|B |2and(I−UUT)B areconservedinourdynamics. Aquantityconserved
k k k
in dynamics should also be conserved at p, so p=(a,B) should satisfy the following conditions.
• a2−|B|2 =a (0)2−|B (0)|2 (Lemma 3)
k k
• (I−UUT)B =(I−UUT)B (0) (Lemma 2)
k
• dLk(a,B)=0, or equivalently the gradient is 0 at p
dt
We will solve for p satisfying those three conditions. The third condition is equivalent to that
(cid:32) (cid:33)
S⃗
aUUT Ba− √ =0. (108)
D
C
As a (T) is eventually bounded away from zero, we have a̸=0, so
k
(cid:32) (cid:33)
S⃗
UUT Ba− √ =0. (109)
D
C
It follows that
S⃗
B =UUTB+(I−UUT)B =UUT√ a−1+(I−UUT)B (0) (110)
k
D
C
and substituting to first condition gives
(cid:12) (cid:12)2
a2− a1
2
(cid:12) (cid:12) (cid:12)UUT√S D⃗ (cid:12) (cid:12)
(cid:12)
−(cid:12) (cid:12)(I−UUT)B k(0)(cid:12) (cid:12)2 =a k(0)2−|B k(0)|2. (111)
(cid:12) C(cid:12)
This is equivalent to a quadratic equation in a2, and has a following solution of
(cid:118)
(cid:117)(cid:12) (cid:12)2
(cid:117)(cid:12) S⃗ (cid:12) (a (0)2−|UUTB (0)|2)2 a (0)2−|UUTB (0)|2
a2 =(cid:116)(cid:12)UUT√ (cid:12) + k k + k k . (112)
(cid:12) D (cid:12) 4 2
(cid:12) C(cid:12)
This shows there are two candidates for p, with a given as two square roots of Eq. (112) and B
determinedfromabyEq.(110). ItisimpossibleforL (t)tohaveaccumulationpointsbothinregions
k
a > 0 and a < 0, as it would imply a (t) = 0 happens infinitely many often, contradicting that a is
k k
eventually bounded away from zero. Thus it follows that L (t) can only have one accumulation point.
k
As dynamics having unique accumulation point should converge, it follows that
(a,B)=(a (∞),B (∞)). (113)
k k
One can check that the U⊥-component of B (∞)a (∞) is given as
k k
(I−UUT)B (∞)a (∞)=(I−UUT)B (0)a (114)
k k k
30and this is bounded by |(1−UUT)B (0)|(S +a (0)), so by our assumption this is negligible. Thus,
k k
we find that B (∞)a (∞) is the pseudo-inverse solution, which is also found by the linear model with
k k
e as basis functions. Using the result from kernel regression [26, 34–37] we have
k,l
S2 d
L = (1− k ). (115)
k 2 D
C
Applying Eq. (14), we find the result.
E.4 N basis functions for a skill
c
Proposition 2. Let the extended multilinear model Eq. (38) (but we change the notation D to N )
c c
be trained with gradient on D →∞ i.i.d samples for the setup in Section 3 with n →∞ (input distri-
s
bution: Eq. (1), target function: Eq. (4), and MSE loss: Eq. (5), initialization: that of Proposition 1).
For a model with the following finite N basis functions
[e , ··· , e , e , ··· , e ], (116)
1,1 1,Nc 2,1 q,r
where quotient q =⌊(N−1)/N ⌋+1 and remainder r is suchthat (q−1)N +r =N. The skill strength
c c
at T →∞ becomes

k >q : 0

R (∞)= k =q : S r (117)
k k <q : SN .c
Proof Because we have D → ∞ and [e ,···e ] can express g (Eq. (43)), it is trivial to show
k,1 k,Nc k
that R (∞)=S for k <q. For k =q, the gradient descent dynamics (Eq. (94)) leads to
k
(cid:18) (cid:19)
dB ηa S
k =− kΦΦT B a − √ (118)
dt D k k N
c
where the matrix Φ∈Rr×dk and vector B
k
∈Rr are the feature matrix(Eq. (88)) and parameters for
the kth skill. As D →∞, ΦΦT becomes a rank r identity matrix:
1
lim (ΦΦT) =E [e (k,X)e (k,X)]=δ . (119)
D→∞D ll′ X k,l k,l′ l,l′
Plugging the identity matrix in ΦΦT,
(cid:18) (cid:19)
dB S
k,l =−ηa B a − √ . (120)
dt k k,l k N
c
√
AssumingtheinitializationinProposition1,wecanshowthata (∞)B (∞)=S/ N forl≤r. The
k k,l c
skill strength R (∞) is
k
r
(cid:88) S
R (∞)= √ E [e (k,X)g (k,X)], (121)
k X k,l k
N
c
l=1
r
=S , (122)
N
c
where we used Eq. (43) for the linear correlation between e and g .
k,l k
31F The tail of dynamic emergence
Inthissection,wediscussanexampleforthetimeemergencecase(Fig.6(b,i))inwhichthesaturation
of skill in an NN consists of multiple saturating ‘modes’ as in Fig. 9.
1.0
0.8
0.6 mode1
mode2
0.4
mode3
0.2 NN
MulLin
0.0
0 250 500 750 1000
T
(a)neuronmodesforaparityfunction (b)mode/skillstrength
Figure 9: Modes in NN. A 2-layer FCN with ReLU activation with a width of 3 and weight sharing
(Eq. (125)) is trained to fit the parity function. (a): The skill strength R, because of the last layer’s
linearity, can be decomposed into skill strength from each hidden neuron or each ‘mode’ (shown in
different colors, Eq. (130)). (b): The skill strength for each mode follows a near-sigmoidal curve with
different emergent/saturation times (colors) whose sum results in the total skill strength (solid black).
Note that different saturation times of each mode result in a deviation from the prediction of the
multilinear model with B2 =1/3 (dashed black).
Task. We assume an input X ∈ R3×8 (note that we are not using X as a random variable) that is
all 8 possible inputs for dimension 3 bits. The output is the parity function scaled by S.
(cid:0)00001111(cid:1) (cid:0) (cid:1)
X = 00110011 , Y = S −S −S S −S S S −S (123)
01010101
NN. We assume a 2-layer width 3 NN with ReLU activation with the input dimension 3 (Fig. 9(a)).
The NN has 16 parameters, but to simplify the argument, we use weight sharing so NN has only 4
parameters:
f(x;α,β,γ,c)=wTσ(Wx+b)+c (124)
where σ is the ReLU activation and W,b,w are
(cid:0)−α α −α(cid:1) (cid:0) 0 (cid:1) (cid:0)−2α(cid:1)
W = −β β −β , b= β , w = β . (125)
γ −γ γ −γ γ
√
Modes. It is easy to see that α=β =γ = 2S and c=−S leads to the target parity function. We
notethatoneparameterexceptc(i.e. α,β,γ)mapstooneneuronoramode(colorsinFig.9(a)). We
define the first mode f(1) as
f(1)(x)=w σ(WTx+b )=−2α2σ(x −x −x ) (126)
1 1 1 2 1 3
=−2α2h (x), h (x):=σ(x −x −x ), (127)
1 1 2 1 3
where w ,b are the first entry of w,b respectively and W is the first row of W. Note that f(1)(x)
1 1 1
takesaformsimilartothemultilinearmodel(Eq.(12))butwithh astherespectivebasis. Wedefine
1
f(2),f(3) similarly, and the sum of modes becomes the NN:
3
(cid:88)
f(x)= f(i)(x)+c, (128)
q=1
which resembles the multilinear model with different skills.11
11Notethattheparityfunctionorthetargetfunctioncorrespondsto‘one’skill.
32
S/
RMode strength. Analogous to the skill strength in Eq. (9), we define mode q’s strength R(q) as
1
R(q) = YTf(q)(X), (129)
8S2
where f(q)(X) = [f(q)(X ),··· ,f(q)(X )] and X are the jth column of X. By the linearity of the
1 8 j
expectation,
3
(cid:88)
R= R(q). (130)
q=1
Note that constant c always has zero correlation (inner product) to the target function (Y).
Analysis. The dynamics of each mode R(q)(x) differs from that of the multilinear model (Eq. (16))
becauseh (x)oftendependsontheparameter, andthedynamicsarenolongerdecoupledamongeach
q
mode. Nevertheless, each mode follows a sigmoid-like growth (Fig. 9(b)). We note that each mode
has a different saturation time scale or is updated at different frequencies. A mode with a longer time
scale leads to a longer ‘tail’ of saturation as discussed in the main text.
Update frequency. Because of the non-linearity, the amount of gradient each mode receives is
different. We can explicitly calculate the gradient each parameter receives:
dα2
=2ηα2(−S−(−2α2+2β2+c)) (131)
dt
dβ2
=−ηβ2(S−(−2α2+5β2+5c)) (132)
dt
dγ2
=−ηγ2(S−(γ2+c)) (133)
dt
dc
=−η(2α2−5β2−γ2−8c). (134)
dt
We immediately notice that c will grow the fastest for small initialization (α,β,γ,c ≪ 1) because
it saturates exponentially while other parameters saturate sigmoidally. Considering that S is always
the largest term and c saturate to S quickly, we notice that the saturation is in the order of α2
(≈ 2S +2c ≈ 4S), β2 (≈ −S +5c ≈ 4S), and γ2(≈ 2S). We observe that our crude approximation
holdsinFig.9(b): thefirst(α)andthesecond(β)modessaturateatsimilartimescale,whilethethird
mode (γ) requires approximately twice the time for saturation.
33G Connection to a linear model
In this section, we will discuss the role of multilinearity in describing time emergence or stage-like
training.12 A linear model with g as the basis functions is
k
N
(cid:88)
f (i,x;w)= w (T)g (i,x), (135)
T k k
k=1
where we made the model linear by replacing a b with w . The dynamics of the linear model under
k k k
gradient flow is
R k(T)=w k(T)=S(1−e−ηd DkT), (136)
where we assumed w (0)=0. The linear model follows an exponential saturation of the skill strength
k
in contrast to the sigmoidal saturation of the multilinear model (Fig. 10(a)), and cannot describe the
sigmoidal time emergence behavior of a 2-layer NN in Fig. 1(a).
d /D=0.5 d /D=0.2 d /D=0.1
1 2 3
1.0 1.0
0.5 0.5
0.0 0.0
0 50 100 0 50 100
T T
(a) Linear (b) Multi Linear
Figure 10: Dynamics of linear and multilinear model. (a): Skill strength dynamics of the
linear model (Eq. (136)) (b): Skill strength dynamics of the multilinear model (Eq. (16)). For the
linear model, R emerges from T = 0 for all d /D > 0: obstructing the stage-like training. For the
k k
multilinearmodel,R showsadelayedemergencedependingond /D: allowingthestage-liketraining
k k
and describing the sigmoidal time emergence in Fig. 1(a).
However,thelinearmodelinEq.(136)–becauseofithastheskillsfunctionsasthebasisfunctions
– can predict T,D,N scaling laws in Section 5 and data and parameter emergence in Section 6. For
the time scaling law, we recover the relationship between dL /dT and dL /dk in Eq. (25) because
k k
R k(T) is a function of d DkT only (where d k/D = P s(k) for D → ∞). For the data scaling law, we
recover Eq. (27) because each w s is decoupled. For the parameter scaling law, we recover Eq. (32)
k
trivially. The data and parameter emergence in Section 6 can be obtained from the linear model in
Eq. (135) if we extend the model analogous to Eqs. (38) and (41). The equivalence can be shown by
Lemma 2 which states that the multilinear model finds the minimum norm solution: the solution that
the linear model finds in a ridgeless regression setup.
12Foradiscussiononlinearneuralnetworks,seeAppendixB.2instead.
34
S/
k
R
S/
k
RH Samples for parameter emergence
k =1 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98
k =2 4.5 0.95 0.95 0.95 0.96 0.96 0.04 0.96 0.96 0.95
k =3 0.6 0.0 0.72 0.90 0.92 0.64 0.88 0.8 0.58 0.52
k =4 0.0 0.78 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
k =5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Figure 11: Samples of skill strength R /S. The table shows the skill strength at N = 10 for 10
k
different runs of the parameter emergence experiment (Fig. 6(b, iii)). Note that the variance of R /S
k
is amplified by the outliers – shaded columns – that learn a less frequent skill at the cost of a more
frequent skill (second column) or fail to learn a skill (seventh column).
I Methods
I.1 2-Layer neural network
We trained a two-layer (one hidden layer) fully connected neural network with ReLU activation. All
parameters of NN were initialized with a Gaussian distribution with a standard deviation of 0.001.
The input dimension of the model was n +n =5+32 where n is the length of control bits (number
s b s
ofskills)andn isthelengthoftheskillbits. Eachskillhasm=3mutuallyexclusivesparsebitsthat
b
are used to express the skill function. The target scale was S =5. The model was trained with SGD
withoutmomentumandnoweightdecay(theexceptionistheparameteremergenceexperimentwhere
Adam with learning rate 0.001 and weight decay of 5×10−5 was used to escape the local minima).
For data and parameter emergence experiments, the learning rate was halved every 50,000 steps.
Atevery20steps, theskillstrengthR (t)(Eq.(9))weremeasuredusing20,000i.i.dsamplesfrom
k
the kth skill.13 To mimic the infinite parameter N → ∞, we used the model of width 1000 (for the
hidden layer). To mimic the infinite time T → ∞, we trained for 5×105 steps where each step had
the batch size of 4000. To mimic D → ∞, we sampled new data points for every batch. The details
are given in the following table.
Name Values
width 1000
learning rate 0.05
initialization standard deviation 0.001
activation ReLU
batch size 4000
steps 500,000
target scale 5
number of skill bits 32
number of skills 5
I.2 Measurement of skill strength
TheskillstrengthR isasimplelinearcorrelationbetweenthelearnedfunctionf –functionexpressed
k
by NN – and g for P given I =k. We approximate the expectation over X by taking the mean over
k b
20,000 i.i.d samples from P for the kth skill:
b
20000
1 (cid:88)
R =E [f(k,X)g (k,X)]≈ f(k,x(j))g (k,x(j)), (137)
k X k 20000 k
j=1
where the notation x(j) denotes the jth sample.
13Notethatexceptthedatascalinglawexperiment,thetrainingsetsizeisinfinite.
35J Rigorous derivation of the scaling laws
InAppendixD,wediscussedthescalinglawsinsimplifiedsettings,favoringintuitionovermathematical
rigor. BuildingupontheintuitiveunderstandingdevelopedinAppendixD,wenowturnourattention
to a rigorous analysis of the scaling laws. In this section, we will derive general scaling laws by
considering a comprehensive set of parameters and variables. Our goal is to establish the conditions
underwhichthesescalinglawsholdandtoquantifytheassociatederrorterms. Byexplicitlyanalyzing
the error terms, this section aims to provide a rigorous assessment of the validity and limitations of
our scaling law estimates.
Large resource Condition Scaling law Constant Statement
D ≫T3 Nα+1 =o(T) L=N−α Theorem 1 Theorem 1
D ≫NT2,T3 Nα+1 ≫T L=T−α/(α+1) Theorem 4 Theorems 2 and 3
D ≫T3 Nα+1 ≈T L=C−α/(α+2) Corollary 5 Corollary 4
T ≫D(logD)1+ϵ Nα+1 =o(D) L=N−α Theorem 5 Theorem 5
T ≫D(logD)1+ϵ Nα+1 ≫D L=D−α/(α+1) Theorem 5 Theorem 5
Table 4: Summary of scaling law and their conditions. The leftmost column indicates the
condition for the ‘large resource’ – large enough to be treated as infinity, while the second column
is the condition between the other two resources for the scaling law (third column). The last two
columns show where the statement for the prefactor constant (e.g. A for scaling law L=AN−α) and
the scaling law (with the assumptions and explicit error terms) are given.
J.1 General set up, repeated
We go back to most general settings possible. Our starting point is Eq. (50), which describes the
dynamics of R and L valid for k ≤N:
k k
S2
L = (50)
k (cid:18) (cid:16) (cid:17)−1 (cid:19)2
2 1+ RkS
(0)
−1 e2ηd DkST
We do not use skills for indices k >N in our model, but we can still denote
S2
R =0 and L = . (138)
k k 2
For P (k)=Ak−α−1, the total loss is given as
s
(cid:88)ns (cid:88)N (cid:88)ns S2
L= P (k)L = P (k)L + P (k) . (139)
s k s k s 2
k=1 k=1 k=N+1
When n ,N,T are all set, their dependency with the data is only determined by the statistics d ,
s k
the number of data with i(j) = k. We assumed that (i,x) ∈ I ×{0,1}nd was collected as random
samples with i following the Zipfian distribution of size n and exponent α+1, or equivalently P(i=
s
k)=P (k)=Ak−α−1 for 1≤k ≤n . Then (d ,··· ,d ) is a vector denoting number of occurrences
s s 1 ns
in D independent sampling from that distribution. It follows that d follows binomial distribution
i
B(D,P (k)).
s
In this complete perspective, our loss is dependent on all of those parameters and variables
L=L(n ,D,R ,N,T) (140)
S init
where R = (R (0),··· ,R (0)) denotes the vector representing initial condition. We will also
init 1 N
simply denote r = R (0). We will not assume much on r , but we absolutely need 0 < r < S for
k k k k
dynamics to hold, and we also should have
(cid:88)ns
P (k)r2 =E[f(0)2]≪S2. (141)
s k
k=1
36We will not impose any particular distribution on R . Instead, we will try to identify sufficient
init
conditions on r for our desired result to hold, and those conditions will differ by the situation we are
k
considering. Forexample,inTheorems2and3whereweprovetimescalinglawL=Θ(T−α/(α+1))for
largeenoughD andbottleneckT,weonlyrequire0<r <S/2. However,theexactconstantdepends
k
on the distribution of r , and figuring out the explicit constant seems to be only feasible when we fix
k
r =r as in Theorem 4.
k
J.2 Estimates for large D
We will first consider the situation where D becomes the ‘large resource’ so that its effect on the loss
function is negligible. The number of data d follows binomial distribution B(D,P (k)), so d /D
k s k
converges to P (k) for large enough D. So taking the limit of L when we let D → ∞ has effect of
s
replacingd /D byP (k)intheexpressionofL. Wewillestablishanexplicitinequalitycomparingthe
k s
difference between L and this limit.
Lemma 4. For a function F :R→R with its total variation V(F) bounded, we have
(cid:12) (cid:20) (cid:21) (cid:12)
(cid:12) (cid:12) (cid:12)E D F(d Dk) −E z∼N(Ps(k),Ps(k)(1−Ps(k))/D)[F(z)](cid:12) (cid:12) (cid:12)< √ D(cid:112)
P
(V k( )F (1)
−P (k))
(142)
s s
where N(µ,σ2) denotes normal distribution of mean µ and variance σ2.
Proof This is just application of the Berry-Esseen inequality (with constant 1, see [49] for modern
treatment) applied to d following binomial distribution B(D,P (k)).
k s
Lemma 5. Let F :R→R be a C2 function such that F′′ is bounded. Then we have
(cid:12) (cid:12)E z∼N(Ps(k),Ps(k)(1−Ps(k))/D)[F(z)]−F(P s(k))(cid:12) (cid:12) ≤ P s(k)(1 2D−P s(k)) sup|F′′|. (143)
Proof First we apply Taylor’s theorem to show that
(z−P (k))2
|F(z)−F(P (k))−F′(P (k))(z−P (k))|≤ s sup|F′′|. (144)
s s s 2
Taking expectation when z follows normal distribution N(P (k),P (k)(1−P (k))/D gives
s s s
|E [F(z)−F(P (k))]|=|E [F(z)−F(P (k))−F′(P (k))(z−P (k))]| (145)
z s z s s s
≤E [|F(z)−F(P (k))−F′(P (k))(z−P (k))|] (146)
z s s s
(cid:20) (z−P (k))2 (cid:21)
≤E s sup|F′′| (147)
z 2
P (k)(1−P (k))
= s s sup|F′′|. (148)
2D
Proposition 3. We have
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) S2 (cid:12) 2αS2 4S4η2T2P (k)
(cid:12)E [L ]− (cid:12)< + s . (149)
(cid:12) (cid:12)
(cid:12)
D k 2(cid:18) 1+(cid:16)
S
−1(cid:17)−1 e2ηPs(k)ST(cid:19)2(cid:12) (cid:12)
(cid:12)
(cid:112) DP s(k) D
(cid:12) rk (cid:12)
Proof Consider the function F :R→R given as
S2
F(z)= . (150)
(cid:18) (cid:16) (cid:17)−1 (cid:19)2
2 1+ S −1 e2ηSTz
rk
37This function is monotone decreasing and C2 on whole domain, and its supremum and infimum are
given as
S2
supF = lim F(z)= and infF = lim F(z)=0. (151)
z→−∞ 2 z→∞
This implies that
S2
V(F)=supF −infF = . (152)
2
Also, we will show that F′′ is globally bounded. We first calculate
r e2ηSTz(1− rk − 2rke2ηSTz)
F′′(z)=−4S3r (1− k)2η2T2 S S . (153)
k S (cid:0) 1− rk + rke2ηSTz(cid:1)4
S S
We consider the following inequalities
S (cid:16) r r (cid:17)
e2ηSTz ≤ 1− k + ke2ηSTz (154)
r S S
k
(cid:12) (cid:12) (cid:12) (cid:12)1− r Sk − 2 Sr ke2ηSTz(cid:12) (cid:12) (cid:12) (cid:12)≤(cid:12) (cid:12) (cid:12)1− r Sk(cid:12) (cid:12) (cid:12)+ 2 Sr ke2ηSTz <2(cid:16) 1+ r Sk(e2ηSTz −1)(cid:17) (155)
to show that
r 2S (cid:0) 1− rk + rke2ηSTz(cid:1)2
|F′′(z)|<4S3r (1− k)2η2T2 rk S S <8S4η2T2 (156)
k S (cid:0) 1− rk + rke2ηSTz(cid:1)4
S S
for all z. Thus we can apply both Lemma 4 and Lemma 5 to this function F and we have
(cid:12) (cid:20) (cid:21) (cid:12)
(cid:12) (cid:12) (cid:12)E D F(d Dk) −F(P s(k))(cid:12) (cid:12) (cid:12)<√ D(cid:112)
P
(V k( )F (1)
−P (k))
+ P s(k)(1 2D−P s(k)) sup|F′′|
s s
S2 4P (k)S4η2T2
< √ + s
(cid:112)
2 D P (k)(1−P (k)) D
s s
2αS2 4P (k)S4η2T2
< + s (157)
(cid:112)
DP (k) D
s
where the last line follows from that we always have
2−(α+1)+···+n−(α+1) 2−(α+1) 1
s
1−P (k)≥1−P (1)= > > . (158)
s s 1+2−(α+1)+···+n−(α+1) 1+2−(α+1) 22(α+1)
s
Lemma 6. For any integer N and σ ≥1/2 and σ ̸=1, we have
(cid:88)N N1−σ
k−σ =ζ(σ)+ +O(N−σ) (159)
1−σ
k=1
whereζ isRiemannzetafunction(definedoverwholecomplexplaneexcept1viaanalyticcontinuation).
In addition,
N
(cid:88)
k−1 =logN +γ+O(N−1) (160)
k=1
where γ =0.5772156649... is Euler’s constant.
Proof See Corollary 1.15 of [50], or other analytic number theory textbooks.
38Proposition 4. (Large D approximation) We have
(cid:88)N S2 (cid:88)ns S2
E [L]− P (k) − P (k) (161)
D s (cid:18) (cid:16) (cid:17)−1 (cid:19)2 s 2
k=1 2 1+ S −1 e2ηPs(k)ST k=N+1
rk
(cid:16) (cid:17)
=O S2D−1/2f (N)+S4η2T2D−1 (162)
α
where

1 if α>1

f (N)= logN if α=1 (163)
α
N(1−α)/2
if α<1.
The constant on the O term only depends on α.
Proof From the description of L in Eq. (139), we have
(cid:88)N S2 (cid:88)ns S2
E [L]− P (k) − P (k) (164)
D s (cid:18) (cid:16) (cid:17)−1 (cid:19)2 s 2
k=1 2 1+ S −1 e2ηPs(k)ST k=N+1
rk
 
(cid:88)N  S2 
= P (k)E [L ]− . (165)
k=1 s   D k 2(cid:18) 1+(cid:16) S −1(cid:17)−1 e2ηPs(k)ST(cid:19)2 
rk
We apply Proposition 3 to give
 
(cid:88)N
P
(k)
E [L ]−
S2  <(cid:88)N
P
(k)(cid:32) 2αS2
+
4S4η2T2P s(k)(cid:33)
.
k=1 s   D k 2(cid:18) 1+(cid:16) S −1(cid:17)−1 e2ηPs(k)ST(cid:19)2  k=1 s (cid:112) DP s(k) D
rk
(166)
Each of these sum involving P (k) is bounded as
s
N (cid:32) N (cid:33)2
(cid:88) (cid:88)
P (k)2 < P (k) <1 (167)
s s
k=1 k=1
and
N N
(cid:88)(cid:112) (cid:88)
P (k)< k−(α+1)/2 =O(f (N)) (168)
s α
k=1 k=1
which follows from Lemma 6. Combining those two gives
(cid:88)N
P
(k)(cid:32) 2αS2
+
S4η2T2P s(k)(cid:33) =O(cid:16)
S2D−1/2f
(N)+S4η2T2D−1(cid:17)
. (169)
s (cid:112) DP (k) D α
k=1 s
WhileProposition4holdsforanyD,itbecomesonlymeaningfuliftheresultingerrortermsareless
than the main term we desire. We will revisit this when the exact main term is found, and determine
the sufficient size of D for error terms to become small enough.
J.3 Estimates for not too small n
s
We next discuss the effect of n . When n → ∞ heuristically, then intuitively we have P (k) →
s s s
k−(α+1)/ζ(α+1). We will discuss the difference between when we regard n as ∞ and when we do
s
not.
39Proposition 5. The following equations hold:
(cid:88)ns n−α
A−1 = k−(α+1) =ζ(α+1)− s +O(n−α−1) (170)
α s
k=1
k−α−1 (cid:18) n−α (cid:19)
P (k)= 1+ s O(n−α−1) (171)
s ζ(α+1) αζ(α+1) s
(cid:88)ns N−α−n−α
P (k)= s +O(N−min(α+1,2α)) (172)
s αζ(α+1)
k=N+1
All implied constants on O only depend on α.
Proof The first statement Eq. (170) follows from substituting σ = α+1 in Lemma 6. As P (k) =
s
Ak−(α+1),thesecondstatementEq.(171)immediatelyfollows. Ifwesubstituten =N intoEq.(170)
s
and calculate differences between them, we obtain
(cid:88)ns N−α−n−α
k−α−1 = s +O(N−α−1). (173)
α
k=N+1
Thus we have
(cid:88)ns
P (k)=A
(cid:88)ns
k−(α+1) =
N−α−n− sα
+O(cid:0) N−α−1+(N−α−n−α)n−α(cid:1) . (174)
s αζ(α+1) s s
k=N+1 k=N+1
Regardless of the size of n , We always have
s
(cid:18) N−α(cid:19)2 N−2α
(N−α−n−α)n−α ≤ = (175)
s s 2 4
so the third statement Eq. (172) follows.
We go back to description of total loss given in Eq. (139) as
(cid:88)N (cid:88)ns S2
L= P (k)L + P (k) (139)
s k s 2
k=1 k=N+1
and we take its expectation in D. Proposition 4 suggests that its limit when D →∞ is given as
(cid:88)N S2 (cid:88)ns S2
lim E [L]= P (k) + P (k) . (176)
D→∞ D s (cid:18) (cid:16) (cid:17)−1 (cid:19)2 s 2
k=1 2 1+ S −1 e2ηPs(k)ST k=N+1
rk
Denote
(cid:88)N S2
L = P (k) (177)
1 s (cid:18) (cid:16) (cid:17)−1 (cid:19)2
k=1 2 1+ S −1 e2ηPs(k)ST
rk
(cid:88)ns S2
L = P (k) . (178)
2 s 2
k=N+1
We discuss the effect of n in L and L , by comparing limit of L and L when n →∞ and their
s 1 2 1 2 s
original values.
40• ForthetermL ,thechangeoflettingn asfinitevaluefromn →∞haseffectofmultiplyingT
1 s s
by1+n−α/(αζ(α+1)),andmultiplyingwholeL by1+n−α/(αζ(α+1)). Itcanbeequivalently
s 1 s
put as
(cid:18) n−α (cid:19) (cid:18) (cid:18) n−α (cid:19)(cid:19)
L (n ,N,T)= 1+ s +O(n−α−1) L ∞,N,T 1+ s +O(n−α−1) .
1 s αζ(α+1) s 1 αζ(α+1) s
(179)
We always have n >N and N →∞ eventually, so if dependency of L with respect to T is at
s 1
most polynomial order then change of main term of L is negligible. We can’t establish exact
1
statements yet without the descriptions of size of L .
1
• ThetermL onlydependsonN andn ,notonT. ApplyingProposition5(especiallyEq.(172))
2 s
gives
N−α−n−αS2
L (n ,N,T)= s +O(N−min(α+1,2α)S2) (180)
2 s αζ(α+1) 2
When n grows faster than N then n−α part is totally negligible, and when n has same order
s s s
as N then n−α affects the constant for main term of L . Things might get little complicated
s 2
when n =N +o(N), where N−α−n−α =o(N−α) can happen then.
s s
• Comparing size of L and L mainly depends on time. The term L is fixed, and L decreases
1 2 2 1
as T increases. For T = ∞ we have L = 0, so L having order N−α dominates (this proves
1 2
scaling law for N of exponent α), so restriction on n becomes quite substantial. For small T
s
and large N where size of L is small, we can expect restriction on n is less substantial. For
2 s
example, in the extreme case N = ∞, we have L = 0, and n does not matter at all (except
2 s
that of course it should satisfy n ≥N).
s
Forsuchreasons,itishardtoquantifyexactconditionsforn suchthaterrortermsarecontrolled,
s
unlesswespecifyrelativegrowthof(N,T). However,n =ω(N)sufficestoassurethatsettingn =∞
s s
has zero effect on the main term. We will not worry about n in this setting anymore too, and come
s
back to this at the very end to determine enough n .
s
J.4 Estimating main terms
We assume D =∞ and n =∞ – virtually implying that d /D =P (k) and P (k)=k−α−1/ζ(α+1)
s k s s
(calculated by rule of n =∞). We decomposed our main term into
s
lim lim E [L]=L +L (181)
D 1 2
ns→∞D→∞
where
(cid:88)N S2
L = P (k) (182)
1 s (cid:18) (cid:16) (cid:17)−1 (cid:19)2
k=1 2 1+ S −1 e2ηPs(k)ST
rk
and
(cid:88)∞ S2
L = P (k) . (183)
2 s 2
k=N+1
By Proposition 5, L is determined almost completely as
2
S2N−α
L = +O(N−α−1). (184)
2 2αζ(α+1)
Now focus on L . For
1
S2
F(z)= (185)
(cid:18) (cid:16) (cid:17)−1 (cid:19)2
2 1+ S −1 e2ηSTz
rk
(note: it really depends on r so it is correct to write F , but for convenience we will keep using F.)
k k
one can express L as
1
N
(cid:88)
L = P (k)F(P (k)). (186)
1 s s
k=1
41Lemma 7. Let F(z) be defined as Eq. (185).
1. (Estimate for large z) We have
(S−r )2 (cid:18) S2 (cid:19)
0≤F(z)≤ k min 1, e−4ηSTz . (187)
2 r2
k
2. (Estimate for small z) For z ≥0, we have
(S−r )2 8ηS3T (S−r )2
k − z ≤F(z)≤ k . (188)
2 27 2
Proof
1. The left side is obvious. For the right side, F(z)≤(S−r )2/2 follows from noting that F(0)=
k
(S−rk)2 and proving F′(z) ≤ 0, and F(z) ≤ (S−rk)2S2e−4ηSTz follows from just replacing 1+
2 2 r2
k
(cid:16) (cid:17)−1 (cid:16) (cid:17)−1
S −1 e2ηSTz in the denominator of F by S −1 e2ηSTz.
rk rk
2. For the left side, it suffices to show −F′(z)≤ 8ηS3T. One can calculate
27
r e2ηSTz
F′(z)=−2S2r (1− k)2ηT (189)
k S (cid:0) 1+ rk(e2ηSTz−1)(cid:1)3
S
and
r e2ηSTz(1− rk − 2rke2ηSTz)
F′′(z)=−4S3r (1− k)2η2T2 S S (190)
k S (cid:0) 1+ rk(e2ηSTz −1)(cid:1)4
S
so F has unique inflection point at
(cid:18) (cid:19)
r 2r 1 S
1− k − ke2ηSTz =0 ⇒ e2ηSTZ = −1 (191)
S S 2 r
k
and this point is where −F′(z) obtains maximum. Substituting this to the expression of F′(z)
gives −F′(z)= 8ηS3T.
27
Our threshold for distinguishing two approximation methods will be set as z = z = (ζ(α +
0
1)ηST)−1, where both two error terms are bounded by O(S2). The constant ζ(α+1) is set to make
later calculations much easier. Applying Lemma 7 gives
N
(cid:88)
L = P (k)F(P (k)) (192)
1 s s
k=1
=
(cid:88) (S−r k)2
P (k)
2 s
1≤k≤N,Ps(k)<z0
 
(cid:88) (cid:88) (cid:18) S2 (cid:19)
+OηS3T P s(k)2+S2 P s(k)min 1, r2e−4ηSTPs(k) . (193)
1≤k≤N,Ps(k)<z0 1≤k≤N,Ps(k)>z0 k
Denote
M =
(cid:88) (S−r k)2
P (k) (194)
2 s
1≤k≤N,Ps(k)<z0
(cid:88)
E =ηS3T P (k)2 (195)
1 s
1≤k≤N,Ps(k)<z0
(cid:88) (cid:18) S2 (cid:19)
E =S2 P (k)min 1, e−4ηSTPs(k) . (196)
2 s r2
1≤k≤N,Ps(k)>z0 k
42√
Proposition 6. Suppose that there exists 0 < r < S such that r ≤ r < S/2 for all k. In the
k
decomposition of
lim lim E [L]=M +L +O(E +E ) (197)
D 2 1 2
ns→∞D→∞
given as above, we have the following bound.
1. If (ηST)1/(α+1) >N, then
S2N−α
L = +O(S2N−α−1) (198)
2 2αζ(α+1)
M =E =0 (199)
1
(cid:16) (cid:17)
E =O S2(log(S/r))α/(α+1)(ηST)−α/(α+1) (200)
2
2. If (ηST)1/(α+1) <N, then
 
(cid:88)
L 2+M =ΘS2 P s(k)=Θ(S2(ηST)−α/(α+1)) (201)
k>(ηST)1/(α+1)
(cid:16) (cid:17)
E =O S2(ηST)−α/(α+1) (202)
1
(cid:16) (cid:17)
E =O S2(log(S/r))α/(α+1)(ηST)−α/(α+1) (203)
2
Here all constants in O and Θ terms are absolute with respect to η,S,T,N. (They may depend on α.)
Proof We first note that the condition P (k)<z =(ζ(α+1)ηST)−1 is equivalent to
s 0
1
P (k)<z =(ζ(α+1)ηST)−1 ⇔ k−α−1 < ⇔ k >(ηST)1/(α+1). (204)
s 0 ηST
Thus we can rephrase the descriptions of terms as
M =
(cid:88) (S−r k)2
P (k) (205)
2 s
(ηST)1/(α+1)<k≤N
(cid:88)
E =ηS3T P (k)2 (206)
1 s
(ηST)1/(α+1)<k≤N
(cid:88) (cid:18) S2 (cid:19)
E =S2 P (k)min 1, e−4ηSTPs(k) . (207)
2 s r2
k≤min((ηST)1/(α+1),N) k
Applying Proposition 5 easily shows that
S2N−α
L = +O(S2N−α−1). (208)
2 2αζ(α+1)
ForM andE ,wewillconsiderthembydividingtwocasesdependingonwhether(ηST)1/(α+1) >N or
1
(ηST)1/(α+1) <N. If (ηST)1/(α+1) >N, then the condition (ηST)1/(α+1) <k ≤N is never satisfied,
so M =E =0. Now suppose (ηST)1/(α+1) <N. We first note that
1
L +M =
(cid:88) (S−r k)2
P (k)+
(cid:88) S2
P (k). (209)
2 2 s 2 s
(ηST)1/(α+1)<k≤N k>N
As (S−r )2 =Θ(S2), we can let
k
 
(cid:88)
L 2+M =ΘS2 P s(k) (210)
k>(ηST)1/(α+1)
43andusingProposition5givesthedesiredestimateL +M =Θ(S2(ηST)−α/(α+1)). ForE ,estimating
2 1
sum of P (k)2 using Lemma 6 gives
s
 
(cid:88) (cid:16) (cid:17)
E
1
=OηS3T k−2(α+1) =O S2(ηST)−α/(α+1) . (211)
k>(ηST)1/(α+1)
For E we always have
2
(cid:88) (cid:18) S2 (cid:19)
E ≤S2 P (k)min 1, e−4ηSTPs(k) (212)
2 s r2
k≤(ηST)1/(α+1)
regardless of the size of N, so it suffices to bound this sum. If we denote l=(ηST)1/(α+1) and define
(cid:18) S2 (cid:19)
F (z)=min 1, e−4ηSTz , (213)
2 r2
it suffices to show the bound
(cid:88)
P (k)F (P
(k))=O(cid:16) (log(S/r))α/(α+1)(ηST)−α/(α+1)(cid:17)
. (214)
s 2 s
k≤l
We will approximate this sum as
(cid:88)
P (k)F (P
(k))=(cid:88)
(P (k)−P (k+1))
P s(k)
F (P (k)) (215)
s 2 s s s P (k+1)−P (k) 2 s
s s
k≤l k≤l
(cid:88) k−α−1
= (P (k)−P (k+1)) F (P (k)) (216)
s s (α+1)k−α−2(1+O(k−1)) 2 s
k≤l
 
(cid:88)
=O (P s(k)−P s(k+1))P s(k)−1/(α+1)F 2(P s(k)). (217)
k≤l
to obtain the form of Riemann sum approximation for the integral of
(cid:90) ∞
z−1/(α+1)F (z)dz (218)
2
z=Ps(l)
at P (l) < P (l−1) < ··· < P (1). As F (z) is decreasing function, this Riemann sum is always less
s s s 2
than the integral, so we obtain
(cid:32) (cid:33)
(cid:90) ∞
(cid:88)
P (k)F (P (k))=O z−1/(α+1)F (z)dz . (219)
s 2 s 2
k≤l z=Ps(l)
We note that P (l)=(ζ(α+1)ηST)−1. The threshold for F (z) to become 1 is given at
s 2
S2 1 S
e−4ηSTz =1 ⇔ z = log . (220)
r2 2ηST r
√
As r < S, this value is always greater than P (l). Thus we can divide our integral as
s
(cid:90) ∞
z−1/(α+1)F (z)dz (221)
2
(ζ(α+1)ηST)−1
(cid:90) (2ηST)−1log(S/r) (cid:90) ∞ S2
= z−1/(α+1)dz+ z−1/(α+1) e−4ηSTzdz. (222)
r2
(ζ(α+1)ηST)−1 (2ηST)−1log(S/r)
The first part is bounded by
(cid:90) (2ηST)−1log(S/r)
z−1/(α+1)dz
=O(cid:16)(cid:0) (2ηST)−1log(S/r)(cid:1)α/(α+1)(cid:17)
(223)
(ζ(α+1)ηST)−1
44(cid:16) (cid:17)
which can be shown to be O (log(S/r))α/(α+1)(ηST)−α/(α+1) . For the second part, we apply sub-
stitution of w =4ηSTz to show
(cid:90) ∞ S2 S2 (cid:90) ∞
z−1/(α+1) e−4ηSTzdz = (4ηST)−α/(α+1) w−1/(α+1)e−wdw (224)
r2 r2
(2ηST)−1log(S/r) 2log(S/r)
S2 (cid:18) α S(cid:19)
= (4ηST)−α/(α+1)Γ ,2log (225)
r2 α+1 r
and applying the asymptotic Γ(s,x)=O(xs−1e−x) suggests that this is bounded by
S2 (cid:18) S(cid:19)−1/(α+1) (cid:16) (cid:17)
≪ (4ηST)−α/(α+1) log e−2log(S/r) =O (ηST)−α/(α+1) . (226)
r2 r
Theorem 1. (Parameter scaling law) Assume the following conditions: n > N with lim(N/n ) =
√ s s
γ <1 (γ can be zero), and there exists 0<r < S such that r <R (0)<S/2 for all k. If N,T →∞
k
while satisfying Nα+1 =o(T), the expected loss E [L] for all datasets D of size D satisfies
D
S2(1−γα)
E [L]= N−α
D 2αζ(α+1)
(cid:16) (cid:17)
+O S2N−min(α+1,2α)+S2(log(S/r))α/(α+1)(ηST)−α/(α+1)+S2D−1/2f (N)+S4η2T2D−1
α
(227)
where

1 if α>1

f (N)= logN if α=1 (228)
α
N(1−α)/2
if α<1.
The constant on the O term only depends on α. When D ≫T3, then all the error terms involving D
are negligible.
Proof In the situation n =∞ and D =∞, Proposition 6 shows that
s
S2 (cid:16) (cid:17)
E [L]= N−α+O S2N−(α+1)+S2(log(S/r))α/(α+1)(ηST)−α/(α+1) . (229)
D 2αζ(α+1)
We consider the effect of n first. As L becomes error term in this estimation, letting n as finite
s 1 s
value has no effect on overall estimation. The term L accounts for the main term, and letting n as
2 s
finite value changes it to
N−α−n−αS2
s +O(N−min(α+1,2α)S2). (230)
αζ(α+1) 2
This accounts for the factor (1−γα) on the main term and O(N−min(α+1,2α)S2) added to the er-
ror term. The effect of D is exactly described in Proposition 4, contributing the error term of
O(cid:0) S2D−1/2f (N)+S4η2T2D−1(cid:1) . Regarding the sufficient condition for D, if D ≫ T3 then we
α
have
S4ηT2D−1 ≪T−α/(α+1), S2D−1/2f (N)≪T−3/2N1/2 ≪T−1 (231)
α
so all error terms involving D are less than O(T−α/(α+1)).
For the situation T = O(Nα+1) however, the error terms E and E are of same size, so we can
1 2
only say that the main term is of O(S2(ηST)−α/(α+1)).
Theorem 2. (Upper bound for time scaling law) Assume the following conditions: n >N, and there
√ s
exists there exists 0 < r < S such that r < R (0) < S/2 for all k. If N,T → ∞ while satisfying
k
ηST =O(Nα+1), the expected loss E [L] is
D
(cid:16) (cid:17)
E [L]=O S2(log(S/r))α/(α+1)(ηST)−α/(α+1)+S2D−1/2f (N)+S4η2T2D−1 (232)
D α
45withconstantonOonlydependingonαandlimsup((ηST)1/(α+1)/N),withf definedasinTheorem1.
α
If D ≫NT2 and D ≫T3, then all the error terms involving D are negligible.
Proof The error term regarding D can be obtained in the same way as Theorem 1, so we will let
D =∞ for the rest of the proof. Also we can let n =∞, as we observed that it contributes at most
s
to the constant factor of the upper bound and does not change the scaling.
In the decomposition of Proposition 6, we always have
(cid:16) (cid:17)
E =O S2(log(S/r))α/(α+1)(ηST)−α/(α+1) (233)
2
and
(cid:16) (cid:17)
E =O S2(ηST)−α/(α+1) (234)
1
holding regardless of N, so it only remains to consider L +M. If (ηST)1/(α+1) <N, then L +M
2 2
is of size O(cid:0) S2(ηST)−α/(α+1)(cid:1) . If (ηST)1/(α+1) ≥ N, then N and (ηST)1/(α+1) has same order, so
L +M =L =Θ(S2N−α)isO(cid:0) S2(ηST)−α/(α+1)(cid:1) . Thusineithercaseswehavethedesiredbound.
2 2
Theorem 3. (Lower bound for time scaling law) Assume the following conditions: n > N and
s
0 < R (0) < S/2. If N,T → ∞ while satisfying (8ζ(α+1)−1ηST)1/(α+1) < N, the expected loss
k
E [L] is
D
(cid:16) (cid:17)
E [L]≥κS2(ηST)−α/(α+1)+O η−1ST−1+S2D−1/2f (N)+S4η2T2D−1 (235)
D α
for κ and constant on O only depending on α, with f defined as in Theorem 1. If D ≫ NT2 and
α
D ≫T3, then all the error terms involving D are negligible.
Proof The error term regarding D can be obtained in the same way as Theorem 1, so we will let
D =∞ for the rest of the proof. We only show the lower bound for L , holding regardless of N and
1
n . In Lemma 7 (Eq. (188)) we have
s
(S−r )2 8ηS3T S2 8ηS3T
F(z)≥ k − z ≥ − z (236)
2 27 8 27
for z ≥0, so if z ≤(4ηST)−1 then F(z)≥S2/8−2S2/27>S2/20. The condition P (k)≤(4ηST)−1
s
is equivalent to that k ≥(4ζ(α+1)−1ηST)1/(α+1). In evaluating L =(cid:80)N P (k)F(P (k)), we will
1 k=1 s s
only add over k in range of
(4ζ(α+1)−1ηST)1/(α+1) <k <(8ζ(α+1)−1ηST)1/(α+1). (237)
From the assumption this interval sits inside 1 < k < N. For such k we use use upper bound of
F(P (k))>S2/20. Then by using Proposition 5 we can obtain
s
S2 (cid:88)
L ≥ P (k) (238)
1 20 s
(4ζ(α+1)−1ηST)1/(α+1)<k<(8ζ(α+1)−1ηST)1/(α+1)
=
S2 (cid:18) (ζ(α+1)−1ηST)−α/(α+1)
(4−α/(α+1)−8−α/(α+1))+O(cid:0)
(ηST)−1(cid:1)(cid:19)
. (239)
20 αζ(α+1)
The possible effect of n on the main term is to multiply both the main term by and T by (1+n−α),
s s
so it increases the bound.
The condition (8ζ(α+1)−1ηST)1/(α+1) < N is not absolutely necessary for lower bound. The
condition (ηST)1/(α+1) = Θ(N) and n ≥ 2N would suffice and one can formulate similar theorem,
s
although the constant of lower bound might be much smaller if (ηST)1/(α+1)/N is small.
Lastly, we provide a simpler version of those results combined and discuss the special case where
the optimal compute C =NT, or the given engineering budget, is specified.
46Corollary 3. (Summary of large data estimation) Assuming D ≫NT2,T3 and n ≫N1+ϵ such that
s
effects of n and D are negligible, then for N,T →∞ we have
s
(cid:16) (cid:17)
E [L]=Θ max(N−α,T−α/(α+1)) , (240)
D η,S,r
whereΘ denotesthattheimpliedconstantdependsonη,S,αandr =minR (0)>0. Inparticular,
η,S,r k
we have
Nα+1 =O(T) ⇒ E [L]=Θ (N−α) (241)
D η,S,r
and
T =O(Nα+1) ⇒ E [L]=Θ (T−α/(α+1)). (242)
D η,S,r
Proof Apply Theorem 1 if Nα+1 =o(T) and Theorem 2 and Theorem 3 if Nα+1 ≫T.
Corollary 4. (The ‘computationally optimal’ case) Denote C = NT and assume the conditions in
Corollary 3. Then we have
E [L]≫C−α/(α+2). (243)
D
When N = Θ(C1/(α+2)) and T = Θ(C(α+1)/(α+2)), we achieve E [L] = Θ(C−α/(α+2)). (Its implied
D
constant may depend on implied constant for growth of N and T.)
Proof The first part follows from
E [L]≫max(N−α,T−α/(α+1)) (244)
D
and
max(N−α,T−α/(α+1))≥(N−α)1/(α+2)(T−α/(α+1))(α+1)/(α+2) =(NT)−α/(α+2). (245)
The second part can be checked by substituting (N,T) = (C1/(α+2),C(α+1)/(α+2)) (or their constant
multiples) to Corollary 3.
J.5 Computing the constant for time scaling law
While we have found the time scaling law E[L]=O(T−α/(α+1)) holding for T =O(Nα+1), bounds in
Theorem 2 and Theorem 3 were chosen rather lazily and do not depict the correct picture. We will
findtheconstantusingmorerefinedestimation,butwerequireadditionalassumptionsonparameters.
We will focus on the setting where D and n are large enough to be negligible, R (0)=r is fixed, and
s k
T =O(Nα+1) with fixed constant such that time scaling law holds.
Theorem 4. (Constant for time scaling law) Denote L∞ as the loss when D,n → ∞ so that their
s
effect is negligible:
(cid:88)N S2 S2N−α
L∞ =L∞(T,N)= P (k) + . (246)
k=1
s 2(cid:16) 1+(cid:0)S −1(cid:1)−1 e2ηPs(k)ST(cid:17)2 αζ(α+1)
r
When T,N → ∞ and limN/(ηST)1/(α+1) = λ for a fixed constant λ ∈ (0,∞], the following limit
exists:
A(λ)= lim (ηST)α/(α+1)L∞(T,N). (247)
T,N→∞
The prefactor constant A as the a function of λ (when λ=∞ then let λ−α =λ−(α+1) =0) is
ζ(α+1)−1/(α+1) (cid:90) ∞ S2
A(λ)= u−1/(α+1)Φ (u)du+ λ−α, (248)
α S,r 2αζ(α+1)
λ−(α+1)/ζ(α+1)
where
S2
Φ (u)= . (249)
S,r 2(cid:16) 1+(cid:0)S −1(cid:1)−1 e2u(cid:17)2
r
47Proof We first observe
(cid:88)N S2N−α
L∞ = P (k)Φ (ηSTP (k))+ . (250)
s S,r s αζ(α+1)
k=1
We will seek to convert it into Riemann sum form of certain integral. We start from noting that
k
P (k)=(P (k)−P (k+1)) (1+O(k)) (251)
s s s α
ζ(α+1)−1/(α+1)
= (P (k)−P (k+1))P (k)−1/(α+1)(1+O(k)) (252)
α s s s
Denote u =ηSTP (k), then the sum can be approximated to
k s
(cid:88)
P (k)Φ (ηSTP (k)) (253)
s S,r s
k
(cid:88)
≈ (P (k)−P (k+1))P (k)−1/(α+1)Φ (ηSTP (k)) (254)
s s s S,r s
k
=(ηST)−α/(α+1)(cid:88) (u −u )u−1/(α+1)Φ (u ) (255)
k k+1 k S,r k
k
if we ignore small k. As Φ is decreasing, this corresponds to Riemann sum taking minimum in the
S,r
interval [u ,u ]. So integral provides an upper bound for this sum. Similarly we can approximate
k+1 k
it with Riemann sum taking maximum in [u ,u ] if we use
k k−1
ζ(α+1)−1/(α+1)
P (k)= (P (k−1)−P (k))P (k−1)−1/(α+1)(1+O(k)) (256)
s α s s s
instead. As Φ shows exponential decay, we can ignore values at small k, so this shows
S,r
(cid:90) ∞
(ηST)−α/(α+1)(cid:88) (u −u )u−1/(α+1)Φ (u )≈ u−1/(α+1)Φ (u)du (257)
k k+1 k S,r k S,r
k uN
and from that
u =ηSTN−(α+1)ζ(α+1)−1 =λ−(α+1)ζ(α+1)−1 (258)
N
we obtain our desired result.
Theorem 4 basically tells that for N =λ(ηST)1/(α+1) and D,n large enough, we have
s
L∼A(λ)(ηST)−α/(α+1) (259)
with A(λ) given as Eq. (248), thus specifying the constant for time scaling law. For finite λ, this
theorem covers the computationally optimal case of (N,T) = (λ C1/(α+2),λ C(α+1)/(α+2)) for some
1 2
nonzeroconstantλ ,λ . Forλ=∞, itdescribesthecaseT =o(Nα+1)whereeffectofN isnegligible.
1 2
Corollary 5. Denote L∞ as L∞ as the loss when D,n → ∞ same as Eq. (246). Denote C = NT
s
and suppose that
(N,ηST)=(λ(ηSC)1/(α+2),λ−1(ηSC)(α+1)/(α+2)) (260)
for a fixed constant 0<λ<∞. Then as C →∞, we have
(cid:16) (cid:17)
L∞ =A λ(α+2)/(α+1) λα/(α+1)(ηSC)−α/(α+2)(1+o(1)) (261)
where A is given as Eq. (248) of Theorem 4.
48Proof As limN/(ηST)1/(α+1) = λ(α+2)/(α+1) under above conditions, we can apply Theorem 4 and
substituting Eq. (260) into Eq. (259) gives the desired result.
Technically we can optimize L∞ for a given fixed value of C = NT by letting λ as argument of
minimum of A(cid:0) λ(α+2)/(α+1)(cid:1) λ−α/(α+1), although it seems almost impossible to obtain any form of
formula for such λ.
Lastly, we provide the following estimate for the time scale constant (A(λ)) when r is small,
especially the first term in Eq. (248).
Proposition 7. As r →0, we have (Λ>0 fixed)
(cid:90) ∞ (cid:18) S−r(cid:19)α/(α+1) S2(α+1)
u−1/(α+1)Φ (u)du≈ 2log . (262)
S,r r 4α
Λ
Proof Denote M =(S −1), and replace u by (logM)v. Then we have
r
(cid:90) ∞ S2 (cid:90) ∞ v−1/(α+1)dv
u−1/(α+1)Φ (u)du=(logM)α/(α+1) (263)
S,r 2 (1+M2v−1)2
Λ Λ/logM
S2 (cid:90) ∞ v−1/(α+1)dv
=(logM)α/(α+1) 1 . (264)
2 v≥Λ/logM (1+M2v−1)2
0
As M →∞, the integrand converges to
(cid:40)
v−1/(α+1)dv v−1/(α+1) if v ≤1/2
lim 1 = (265)
M→∞ v≥Λ/logM (1+M2v−1)2 0 if v >1/2.
The integrand is bounded by v−1/(α+1) if v ≤ 1/2 and v−1/(α+1)e−2(2v−1) if v > 1/2, those of which
are all integrable. So we can apply Lebesgue’s dominated convergence theorem to show
(cid:32) (cid:33)
(cid:90) ∞ v−1/(α+1)dv (cid:90) ∞ v−1/(α+1)dv (cid:90) 1/2
lim = lim 1 = v−1/(α+1)dv. (266)
M→∞ Λ/logM
(1+M2v−1)2
0 M→∞
v≥Λ/logM (1+M2v−1)2
0
Thus we have
(cid:18) S−r(cid:19)−α/(α+1)(cid:90) ∞ S2 (cid:90) 1/2 21/(α+1)S2(α+1)
lim log u−1/(α+1)Φ (u)du= v−1/(α+1)dv =
r→0 r Λ S,r 2 0 4α
(267)
which can be observed to be equivalent to desired expression of Eq. (262).
J.6 Estimates for large T and threshold between data/parameter scaling
TheestimatesforsmallDrequiresdifferenttechniquesfromestimatesforlargeD. Wewillconsiderthe
situationT growsmuchfasterthanD andN,anddiscusswhendatascalinglawofL=Θ(D−α/(α+1))
happens. We will consider more simpler setting of ’n = ∞’ or equivalently that effects of n is
s s
negligible (n = ω(N) seems to suffice) and R (0) = r < S is fixed, although it won’t be impossible
s k
to discuss their subtle effects.
First we single out effect of T by comparing L(T) and L(∞). We remind
S2
L (T)= (50)
k 2(cid:16) 1+(cid:0)S −1(cid:1)−1 e2ηdkST/D(cid:17)2
r
and its limit when T →∞ is given as
(cid:40) (S−r)2
if d =0
L (∞)= lim L (T)= 2 k (268)
k k
T→∞ 0 if d >0.
k
49Proposition 8. Suppose that R (0)=r <S is fixed. For large T, we have
k
(cid:16) (cid:17)
E [L(T)]−E [L(∞)]=O S4r−2De−4ηST/D . (269)
D D
Proof As L (T) is decreasing in T, we always have L (T)≥L (∞) so therefore
k k k
E [L(T)]−E [L(∞)]≥0. (270)
D D
So we only need to establish an upper bound for L (T)−L (∞). We note that L (T)−L (∞) when
k k k k
d =0, so one can write
k
L (T)−L (∞)=1 L (T) (271)
k k dk>0 k
where 1 denotes the characteristic function
dk>0
(cid:40)
1 if d >0
1 = k (272)
dk>0
0 if d =0.
k
We use simple bound of
S2 S4
L k(T)<
2(cid:16)(cid:0)S −1(cid:1)−1 e2ηdkST/D(cid:17)2
<
2
r−2e−4ηdkST/D. (273)
r
As d follows binomial distribution B(D,P (k)), considering its moment generating function gives
k s
(cid:16) (cid:17)D
E dk[e−4ηdkST/D]= 1−P s(k)+P s(k)e−4ηST/D (274)
so thus
(cid:16) (cid:17)D
E dk[1 dk>0e−4ηdkST/D]= 1−P s(k)+P s(k)e−4ηST/D −(1−P s(k))D. (275)
Meanwhile, for 0≤u,v ≤1 real numbers, we have
|uD−vD|=|u−v||uD−1+uD−2v+···+vD−1|≤D|u−v| (276)
so applying this inequality to above gives
E dk[1 dk>0e−4ηdkST/D]≤DP s(k)e−4ηST/D. (277)
Thus we can deduce
E [L (T)]−E [L (∞)]=E [1 L (T)] (278)
dk k dk k dk dk>0 k
S4r−2
<
2
E dk[1 dk>0e−4ηdkST/D] (279)
S4r−2
≤ De−4ηST/DP (k) (280)
2 s
and thus
S4r−2 (cid:88)∞ (cid:16) (cid:17)
0≤E [L(T)]−E [L(∞)]< De−4ηST/D P (k)2 =O S4r−2De−4ηST/D . (281)
D D 2 s
k=1
This provides an almost complete account for the effect of very large T. We will let T = ∞ from
this point. We have
(S−r)2 (cid:88)N S2 (cid:88)∞
E [L(∞)]= P (k)(1−P (k))D+ P (k). (282)
D 2 s s 2 s
k=1 k=N+1
50Applying Lemma 6 gives
(cid:88)∞ N−α
P (k)= +O(N−α−1) (283)
s αζ(α+1)
k=N+1
so it suffices to focus on the first sum. We will divide the range of k into two 1 ≤ k ≤ M and
M <k ≤N. For the sum over 1≤k ≤M, we will apply the following simple bound (in the last part
we used 1−x≤e−x)
M
(cid:88)
0≤ P (k)(1−P (k))D ≤(1−P (M))D ≤e−Ps(M)D. (284)
s s s
k=1
For the sum over M <k ≤N, we will approximate the sum into some integral, which happens to be
incomplete gamma function.
Proposition 9. For 2<M <N integers, we have
N
(cid:88)
P (k)(1−P (k))D (285)
s s
k=M+1
ζ(α+1)−1/(α+1) (cid:18) (cid:18) α (cid:19) (cid:18) α (cid:19)(cid:19)
=D−α/(α+1) Γ ,DP (N) −Γ ,DP (M) (286)
α α+1 s α+1 s
(cid:16) (cid:17)
+O D−(2α+1)/(α+1)+D−α/(α+1)M−1 . (287)
Here Γ denotes the incomplete gamma function
(cid:90) ∞
Γ(s,x)= ys−1e−ydy. (288)
x
Proof Consider the interval [P (N),P (M)] and its partition P = {P (N) < P (N −1) < ··· <
s s s s
P (M)}. Forafunctionf(x)=x−1/(α+1)(1−x)D,wewillconsideritsupperandlowerDarbouxsums
s
withrespecttoP. Asf isdecreasingin(0,1],itsupperandlowerDarbouxsumsaregivenrespectively
as
N−1
(cid:88)
U(f,P)= (P (k)−P (k+1))P (k+1)−1/(α+1)(1−P (k+1))D (289)
s s s s
k=M
N−1
(cid:88)
L(f,P)= (P (k)−P (k+1))P (k)−1/(α+1)(1−P (k))D. (290)
s s s s
k=M
and those give bound of the integral of f as
(cid:90) Ps(M)
L(f,P)≤ f(x)dx≤U(f,P). (291)
Ps(N)
Meanwhile, by noting that
ζ(α+1)−1/(α+1)
P (k)= (P (k)−P (k+1))P (k)−1/(α+1)(1+O(k−1)) (292)
s α s s s
one can show
N
(cid:88)
P (k)(1−P (k))D (293)
s s
k=M
ζ(α+1)−1/(α+1) (cid:32)N (cid:88)−1 (cid:33)
= (P (k)−P (k+1))P (k)−1/(α+1)(1−P (k))D (1+O(M−1)) (294)
α s s s s
k=M
ζ(α+1)−1/(α+1)
= L(f,P)(1+O(M−1)). (295)
α
51Applying similar argument for upper Darboux sum gives
(cid:88)N ζ(α+1)−1/(α+1)
P (k)(1−P (k))D = U(f,P)(1+O(M−1)) (296)
s s α
k=M
and from Eq. (291) it follows
(cid:88)N ζ(α+1)−1/(α+1) (cid:32) (cid:90) Ps(M) (cid:33)
P (k)(1−P (k))D = x−1/(α+1)(1−x)Ddx (1+O(M−1)). (297)
s s α
k=M Ps(N)
From now we will estimate the integral
(cid:90) Ps(M)
x−1/(α+1)(1−x)Ddx. (298)
Ps(N)
We replace x=y/D in the integral inside, then it becomes
(cid:90) Ps(M) (cid:90) DPs(M) (cid:16) y (cid:17)D
x−1/(α+1)(1−x)Ddx=D−α/(α+1) y−1/(α+1) 1− dy. (299)
D
Ps(N) DPs(N)
We want to approximate (cid:0) 1− y(cid:1)D by e−y, so we will estimate difference between them. We have
D
(cid:88)∞ yk
Dlog(1−y/D)=−y− (300)
kDk−1
k=2
so if D >2y then
1 (cid:88)∞ yk 1 (cid:88)∞ yk y2
−y >Dlog(1−y/D)=−y− >−y− =−y− (301)
D kDk−2 D 2(2y)k−2 D
k=2 k=2
so
(cid:18) y2(cid:19) (cid:16) y (cid:17)D
e−y 1− <e−ye−y2/D < 1− <e−y, (302)
D D
where we used the inequality 1 − x ≤ e−x. As P (M) < 1/2 if M > 2 (obvious from P (M) <
s s
(P (1)+P (2))/2<1/2), any y in the interval [DP (N),DP (M)] satisfies D >2y. So we can apply
s s s s
this approximation in every y. It follows that
(cid:90) DPs(M) (cid:16) y (cid:17)D
y−1/(α+1) 1− dy (303)
D
DPs(N)
(cid:32) (cid:33)
(cid:90) DPs(M) (cid:90) DPs(M) y2
= y−1/(α+1)e−ydy+O y−1/(α+1)e−y dy (304)
D
DPs(N) DPs(N)
(cid:90) DPs(M) (cid:18) (cid:90) ∞ (cid:19)
= y−1/(α+1)e−ydy+O D−1 y−1/(α+1)e−yy2dy (305)
DPs(N) 0
(cid:18) (cid:19) (cid:18) (cid:19)
α α
=Γ ,DP (N) −Γ ,DP (M) +O(D−1). (306)
α+1 s α+1 s
Combining this with Eq. (297) and Eq. (299) gives the desired result.
We combine Proposition 8 and Proposition 9 together to obtain this final estimation result.
Theorem 5. (Scaling laws for large time estimation) Suppose that N,D → ∞ and n ≫ N1+ϵ for
s
some ϵ>0 so that effect of n is negligible. Suppose that R (0)=r for all 1≤k ≤N.
s k
1. (Parameter scaling law) If N =o(D1/(α+1)), then we have
S2 (cid:16) (cid:17)
E [L]= N−α+O S2D−α/(α+1)+S2N−α−1+S4r−2De−4ηST/D . (307)
D 2αζ(α+1)
522. (Data scaling law) If D =O(Nα+1) and µ=lim(D/Nα+1) exists (it can be zero), then
(cid:18) (S−r)2ζ(α+1)−1/(α+1) (cid:18) α D (cid:19) S2(D/Nα+1)α/(α+1)(cid:19)
E [L]=D−α/(α+1) Γ , +
D 2α α+1 Nα+1ζ(α+1) 2αζ(α+1)
(cid:16) (cid:17)
+O S2D−(2α+1)/(2α+2)+S4r−2De−4ηST/D (308)
Here Γ denotes the incomplete gamma function
(cid:90) ∞
Γ(s,x)= ys−1e−ydy. (309)
x
In particular, if D =o(Nα+1) such that µ=0, we have
(S−r)2ζ(α+1)−1/(α+1) (cid:18) α (cid:19) (cid:16) (cid:17)
E [L]=D−α/(α+1) Γ (1+o(1))+O S4r−2De−4ηST/D .
D 2α α+1
(310)
In either cases, T ≫D(logD)1+ϵ for some ϵ>0 implies that error terms involving T are negligible.
Proof Proposition 8 states
(cid:16) (cid:17)
E [L(T)]−E [L(∞)]=O S4r−2De−4ηST/D (269)
D D
and we showed
(S−r)2 (cid:88)N S2 (cid:88)∞
E [L(∞)]= P (k)(1−P (k))D+ P (k) (282)
D 2 s s 2 s
k=1 k=N+1
and
(cid:88)∞ N−α
P (k)= +O(N−α−1). (283)
s αζ(α+1)
k=N+1
For the sum of P (k)(1−P (k))D over 1≤k ≤N, we use the estimate (see Eq. (284)) of
s s
M
(cid:88) (cid:16) (cid:17)
P (k)(1−P (k))D =O e−Ps(M)D (311)
s s
k=1
and the estimate of Proposition 9. Combining all those gives
E [L] (312)
D
S2N−α
= (313)
2αζ(α+1)
(S−r)2ζ(α+1)−1/(α+1) (cid:18) (cid:18) α (cid:19) (cid:18) α (cid:19)(cid:19)
+D−α/(α+1) Γ ,DP (N) −Γ ,DP (M) (314)
2α α+1 s α+1 s
(cid:16) (cid:17)
+O S2(D−(2α+1)/(α+1)+D−α/(α+1)M−1+N−α−1+e−Ps(M)D)+S4r−2e−4ηST/D . (315)
WewillproveourmainstatementbychoosingappropriateM dependingonsizecomparisonbetween
D and N.
1. If N = o(D1/(α+1)), then we let M = 3, and also regard all incomplete gamma function values
as O(1). Then it follows
S2N−α (cid:16) (cid:17)
E [L]= +O S2D−α/(α+1)+S2N−α−1+S4r−2e−4ηST/D (316)
D 2αζ(α+1)
and thus obtaining the parameter scaling law.
532. Suppose D =O(Nα+1) and µ=lim(D/Nα+1) exists. We want
(S−r)2ζ(α+1)−1/(α+1) (cid:18) α (cid:19) S2N−α
D−α/(α+1) Γ ,DP (N) + (317)
2α α+1 s 2αζ(α+1)
to be our main term, and set M <N such that the term
(cid:18) (cid:19)
α
S2D−α/(α+1)Γ ,DP (M) (318)
α+1 s
and error terms not depending on T given as
(cid:16) (cid:17)
O S2(D−(2α+1)/(α+1)+D−α/(α+1)M−1+N−α−1+e−Ps(M)D) (319)
are all bounded by O(D−(2α+1)/(2α+2)). Set M = D1/(2α+2). Then P (M) = D−1/2/ζ(α+1),
s
so applying the asymptotic Γ(s,x)=O(xs−1e−x) gives
(cid:18) α (cid:19) (cid:16) √ (cid:17)
Γ ,DP (M) =O D−1/2(α+1)e− D/ζ(α+1) . (320)
α+1 s
√
This term and e−Ps(M)D = e− D/ζ(α+1) are less than D−α/(α+1)M−1 = O(D−(2α+1)/(2α+2)),
and obviously D−(2α+1)/(α+1) is less than D−(2α+1)/(2α+2). Thus it follows that
(S−r)2ζ(α+1)−1/(α+1) (cid:18) α D (cid:19) S2N−α
E [L]=D−α/(α+1) Γ , +
D 2α α+1 Nα+1ζ(α+1) 2αζ(α+1)
(cid:16) (cid:17)
+O S2D−(2α+1)/(2α+2)+S4r−2De−4ηST/D . (321)
Regarding the final statement regarding sufficient condition for large T, T ≫D(logD)1+ϵ implies
De−4ηST/D <De−4ηS(logD)1+ϵ <D·D−4ηS(logD)ϵ ≪D−K (322)
for any K >0, showing that the error term
O(cid:0) S4r−2De−4ηST/D(cid:1)
is negligible compared to all other
error terms of Eq. (307) and Eq. (308).
We also provide a summary of all large time estimation results.
Corollary 6. (Summary of large time estimation) Assuming T ≫D(logD)1+ϵ and n ≫N1+ϵ such
s
that effects of n and T are negligible, and R (0) = r for all 1 ≤ k ≤ N. Then for D,N → ∞, we
s k
have
(cid:16) (cid:17)
E [L]=Θ max(N−α,D−α/(α+1)) , (323)
D η,S,r
where Θ denotes that the implied constant depends on η,S,r and α. In particular, we have
η,S,r
Nα+1 =O(D) ⇒ E [L]=Θ (N−α) (324)
D η,S,r
and
D =O(Nα+1) ⇒ E [L]=Θ (D−α/(α+1)). (325)
D η,S,r
Proof Just summarize the results of Theorem 5.
54