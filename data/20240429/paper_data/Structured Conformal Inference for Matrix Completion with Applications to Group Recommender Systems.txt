Structured Conformal Inference for Matrix Completion with
Applications to Group Recommender Systems
Ziyi Liang∗, Tianmin Xie†,‡ Xin Tong†, Matteo Sesia†
April 29, 2024
Abstract
We develop a conformal inference method to construct joint confidence regions for struc-
tured groups of missing entries within a sparsely observed matrix. This method is useful to
provide reliable uncertainty estimation for group-level collaborative filtering; for example, it
can be applied to help suggest a movie for a group of friends to watch together. Unlike stan-
dard conformal techniques, which make inferences for one individual at a time, our method
achieves stronger group-level guarantees by carefully assembling a structured calibration data
set mimicking the patterns expected among the test group of interest. We propose a general-
ized weighted conformalization framework to deal with the lack of exchangeability arising from
such structured calibration, and in this process we introduce several innovations to overcome
computational challenges. The practicality and effectiveness of our method are demonstrated
through extensive numerical experiments and an analysis of the MovieLens 100K data set.
Keywords: Collaborative filtering; confidence regions; conformal inference; exchangeability;
Laplace’s method; simultaneous inference.
1 Introduction
1.1 Background and Motivation
Many data-driven decision problems require a simultaneous understanding of several related un-
knowns, motivating the use of joint confidence (or prediction) regions. This explains why the
concept of joint (or simultaneous) inference has a rich history, with roots tracing back to semi-
nal works by Scheff´e [1] and others [2, 3]. Despite its established roots in classical statistics, this
topic remains relatively under-explored in the context of distribution-free predictive inference, or
conformal inference [4, 5]. In fact, conformal methods have so far primarily focused on predicting
individual outcomes one by one, typically under a data exchangeability assumption. As we shall
see, this traditional approach can make it hard to aggregate individual inferences efficiently. This
is especially a challenge in situations where the goal is to simultaneously predict several quantities
exhibiting potentially complicated dependencies, and a straightforward Bonferroni correction [6]
∗Department of Mathematics, University of Southern California, Los Angeles, CA, USA.
†Department of Data Sciences and Operations, University of Southern California, Los Angeles, CA, USA.
‡The first two authors contributed equally to this work.
1
4202
rpA
62
]EM.tats[
1v16571.4042:viXraapplied to individual-level predictions would be too conservative. We begin to fill this gap in the
literaturebydevelopinganovelsimultaneousconformalinference[4]methodformatrixcompletion,
practically motivated by collaborative filtering for group recommendations.
Group recommender algorithms provide suggestions for collective decisions in diverse contexts,
such as movies [7], music [8], travel [9], restaurants [10], and hiring [11]. For example, think of a
group of friends faced with the daunting task of selecting a movie that caters to everyone’s prefer-
ences. By utilizing data on each individual’s past streaming history and previously indicated movie
preferences, a group recommender system can streamline the decision-making process, enhancing
the likelihood of an inclusive and gratifying movie-watching experience for all involved. This ex-
plains why the topic is gaining increased traction [12–14], fueled by increasing usage of mobile
devices and social networks that connect users and gather relevant data.
While previous research on recommender systems concentrated on algorithmic aspects, uncer-
tainty estimation is an essential ingredient in principled decision-making [15–19], the significance
of which is underscored by the inherent limitations of inferences based on sparse data, subjective
ratings, diverse user behaviors, and potential inaccuracies due to noise [20]. In fact, uncertainty
estimation can enhance transparency [21] and plays a crucial role in identifying flawed predictions
and reinforcing user trust [22]. Moreover, it can provide valuable insights for developing innovative
algorithmic strategies [23], potentially entailing the exclusion of less confident recommendations.
It is also beneficial for internal company processes, such as comparing different algorithms [24] and
offering guidance on future data collection requirements [25]. In the context of group recommen-
dations, uncertainty estimation becomes even more important [26–28] because it can play a critical
role in the aggregation of potentially conflicting individual preferences [14, 29, 30].
To illustrate this idea, consider a recommendation algorithm that needs to suggest a movie for
a group of friends, Alice, Ben, and Chris. All three users are predicted to give 5/5 ratings to “2001:
A Space Odyssey”. However, the algorithm is only confident about Alice’s and Ben’s ratings, while
Chris’s predicted rating is close to random guess, due for example to limited data related to his
enjoyment of science-fiction. Moreover, the algorithm predicts all three users to give a 4/5 rating
to “The Godfather”, but these predictions all are highly confident. Understanding the uncertainty
in both cases would facilitate the recommendations process. For instance, it would lead to the
selection of “The Godfather” if the system follows the “least misery” principle—assuming that the
group’s overall satisfaction is influenced by the least satisfied member [30]—and to “2001: A Space
Odyssey” under the opposite “most pleasure” principle.
With this motivation, we consider the problem of jointly predicting (or estimating) multiple
ratingscorrespondingtodifferentusers. Wefocusoncollaborativefilteringviamatrixcompletion—
the task of approximating the missing entries in a sparsely observed ratings matrix [31], where rows
represent users and columns depict products. If the matrix exhibits certain patterns, such as a low-
rank structure, it becomes feasible to impute the missing entries [32–34]. The underlying concept
is quite intuitive. Individuals often seek recommendations from their peers, placing more trust in
suggestions from people whose preferences align closely with their own. Thus, the structure of the
preference matrix captures valuable similarities between diverse users and products.
While conformal inference has already been recognized as providing a useful uncertainty esti-
mation framework in the context of collaborative filtering and matrix completion [18, 35, 36], we
will explain below that the task of simultaneously estimating uncertainty in a group setting is both
novel and particularly challenging, necessitating a creative approach.
21.2 Preview of Our Contributions
We study the problem of constructing a confidence region for K missing entries within the same
column of a partially observed matrix, for different values of K. Focusing on groups of entries
within the same column is helpful for concreteness, but the main elements of our solution could be
adapted to also address different related questions.
Figure 1 previews the performance of our method applied to the MovieLens 100K data [37],
analyzing a ratings matrix with 800 rows (users) and 1000 columns (movies). Approximately 94%
of the entries are missing, and the goal is to construct a joint 90% confidence region (in the shape
of a hyper-cube) for the unobserved ratings assigned to a random movie by a group of K users.
We refer to Section 5.2 for more details on this data analysis. In this context, an unadjusted
application of conformalized matrix completion method of Gui, Barber, and Ma [35] would produce
individual confidence intervals for one user/movie pair at a time but would not achieve group-level
coverage. Therefore,wecompareourmethodtoabenchmarkthatseekssimultaneousgroupvalidity
by applying a straightforward Bonferroni adjustment to the individual confidence intervals. Our
method outperforms this Bonferroni benchmark, producing narrower and hence more informative
confidenceregionsthankstoitsabilitytoautomaticallyadapttothecomplexdependencystructures
arising from these data. We refer to Appendix A1 for a review of the method of Gui, Barber, and
Ma [35] and further details on the undadjusted and Bonferroni baseline approaches.
Guessed rank: 3 Guessed rank: 5 Guessed rank: 7
3.6 Method
3.2 Simultaneous
2.8 Bonferroni
2 4 6 8 2 4 6 8 2 4 6 8
Group size K
Figure 1: Preview of the performance of our conformal method for simultaneous group-level matrix
completion on the MovieLens 100K data, as a function of the group size. The results in different
columnsareobtainedusinga(convexoptimization)matrixcompletionalgorithmbasedondifferent
hypothesizedmatrixranks. ThebaselineapproachappliesaBonferronicorrectiontotheindividual-
level conformalized matrix completion method of Gui, Barber, and Ma [35]. The nominal group-
levelcoveragelevelis90%,andourmethodoutputsnarrower(moreinformative)confidenceregions.
Our method can be intuitively explained as follows. Conformal inference generally aims to
transform the output of a machine learning model into a confidence set with a tunable parameter
that controls the margin of error. This parameter is calibrated, using hold-out data assumed to be
exchangeable with the test point, to minimize the size of the confidence set while guaranteeing a
suitable notion of average coverage. To address our joint estimation problem, we extend the stan-
dard conformal strategy by constructing and leveraging a structured calibration sample consisting
of groups of user/product observations that mimic the patterns expected at test time.
Yet, it is not easy to translate this high-level idea into a concrete method. The main obstacle
is that our structured calibration data are inconsistent with exchangeability, preventing the use
of standard conformal techniques. Our solution draws inspiration from the weighted exchangeabil-
ity framework of Tibshirani et al. [38], which was developed to address a different covariate shift
problem. However, this approach gives rise to significant computational challenges in our setting,
as it involves summing over exponentially many permutations to account for the lack of exchange-
3
Avg.
widthability. We overcome these challenges by fusing the Gumbel-max trick [39] with a new extension
of the classical Laplace method [40]. The Gumbel-max trick converts the intractable sum into a
more manageable although still analytically unfeasible integral, and the Laplace method provides
an accurate approximation of this integral. Although our integration technique is not exact, we
demonstrate its reliability by proving that it is asymptotically consistent in the large-sample limit
and then we verify the empirical validity of our results numerically.
1.3 Related Works
While matrix completion algorithms are well-studied [32–34, 41, 42], the problem of quantifying
their uncertainty has received attention only more recently. Chen et al. [43], Xia and Yuan [44],
andFarias, Li, andPeng[45]obtainedasymptoticconfidenceintervalsformissingentriesestimated
usingconvexoptimizationalgorithms,undersomemodelingassumptionsfortheunderlyingmatrix.
Gui, Barber, and Ma [35] and Shao and Zhang [36] proposed alternative approaches based on
conformal inference [4, 46], requiring fewer assumptions and providing finite-sample inferences.
Our method builds upon the conformal inference framework, but is not in competition with the
parametric techniques of Chen et al. [43], Xia and Yuan [44], and Farias, Li, and Peng [45]; on the
contrary, in the future these opposite points of view could be combined, possibly drawing strength
from the convex optimization analysis to obtain even more informative conformal inferences.
Gui, Barber, and Ma [35] and Shao and Zhang [36] both seek to construct individual confidence
intervals for the unobserved entries in a partially observed matrix, one at a time, although each
tackles that problem from a different angle. The view of Gui, Barber, and Ma [35] is more similar
to ours, as they assume the matrix is fixed while the missingness is random. In Shao and Zhang
[36], thematrixisrandomandthemissingnessisfixed. OurworkdepartsfromthatofGui, Barber,
and Ma [35] as we seek simultaneous confidence regions for structured groups of missing entries;
this is a more challenging problem that requires different modeling choices and involves several
methodological innovations due to lack of exchangeability.
While this paper focuses on matrix completion, many components of our method are likely
to have broader relevance, potentially enabling structured conformal inferences in different con-
texts. Many prior works focused on regression [5, 47–49], classification [50–52], or outlier detection
[53–55], but the applicability of conformal inference extends to numerous other tasks, including
functional data analysis [56], causal inference [57], data sketching [58], and forecasting [59, 60]; see
also Angelopoulos and Bates [61] for a beginner-level review. In general, conformal inference is
easier when dealing with exhangeable data, but there have been many efforts to deal with more
difficult situations [38, 62–65]. Building on the weighted exchangeability framework introduced by
Tibshirani et al. [38], we focus on the challenges arising from structured calibration.
1.4 Outline of the Paper
Section 2 states our assumptions and goals. Section 3 presents our method. Section 4 delves
into essential implementation aspects. The empirical performance of our method is investigated in
Section5. Section6concludeswithadiscussionandsomeideasforfutureresearch. Additionalcon-
tent is in the appendices. Appendix A1 summarizes technical background information and details
the baseline approaches. Appendix A2 explains additional implementation details, Appendix A3
summarizes further empirical results, and Appendix A4 contains all mathematical proofs.
42 Setup and Problem Statement
Let M ∈ Rnr×nc be a fixed matrix with n
r
rows and n
c
columns. One may consider that the rows
of M correspond to users and its columns to products, while each entry M is the rating assigned
r,c
by user r ∈ [n ] := {1,...,n } to product c ∈ [n ] := {1,...,n }. Although it is common to assume
r r c c
M to have a particular (e.g., low-rank) structure, possibly including some independent additive
random noise, we follow a different approach. Inspired by Gui, Barber, and Ma [35], we allow M
to be deterministic and arbitrary, and we model instead the randomness in the data observation
(or missingness) process.
Considerafixednumberofobservations,n ,andletM denotetheobservedportionofM,
obs X obs
indexed by X = (X ,...,X ) with each X ∈ [n ]×[n ]. We assume that X is randomly
obs 1 n obs i r c obs
sampled without replacement from [n ] × [n ]. For increased flexibility, we allow this sampling
r c
process to be weighted according to parameters w = (w ) , which we assume to be
r,c (r,c)∈[nr]×[nc]
known for now. See Appendix A2.4 for details on how these weights may be estimated in practice.
In a compact notation, we write our sampling model as:
X ∼ Ψ(n ,[n ]×[n ],w). (1)
obs obs r c
In general, Ψ(m,X,w) denotes weighted random sampling without replacement of m ≥ 1 distinct
elements (“balls”) from a finite dictionary (“urn”) denoted as X, with |X| ≥ m. More precisely,
X ∼ Ψ(m,X,w) corresponds to saying that, for any x ∈ Xm,
w w w
P[X = x] = x1 · x2 ·...· xm .
(cid:80) x∈X w x (cid:80) x∈X w x−w x1 (cid:80) x∈X w x−(cid:80)m i=− 11w xi
Therefore, the weights w do not need to be normalized. This model is a special case of the
multivariate Wallenius’ noncentral hypergeometric distribution [66, 67], and it reduces to random
sampling without replacement if all w are the same.
r,c
WedenoteasD ⊂ [n ]×[n ]theunorderedcollectionofindicescorrespondingtotheobserved
obs r c
matrix entries, and call its complement D ; i.e.,
miss
D := {X ,...,X }, D := [n ]×[n ]\D .
obs 1 n obs miss r c obs
Our goal is to construct a joint confidence region for a group of K ≥ 1 unobserved matrix entries,
indexed by X∗ = (X∗,X∗,...,X∗ ), where each X∗ ∈ D and the group size K is fixed. The
1 2 K k miss
indices represented by X∗ are assumed to be sampled without replacement from D , subject to
miss
the constraint that they must be in the same matrix column. Moreover, the sampling of X∗ from
D is guided by a distinct set of weights w∗ = (w∗ ) , which are also assumed to be
miss r,c (r,c)∈[nr]×[nc]
known for now. We will discuss at the end of this section the implications of the choice of w∗.
To ensure that the sampling model for X∗ is fully well-defined, we must account for the possi-
bility that some matrix columns may have fewer than K missing entries. Therefore, we consider a
pruned set of missing indices D¯ ⊆ D , defined as:
miss miss
D¯ = {(r,c) ∈ D : nc ≥ K} (2)
miss miss miss
where nc = |{(r′,c′) ∈ D : c′ = c}| is the number of missing entries in column c. Then, we
miss miss
5assume that X∗ is sampled according to
X∗ | D ,D ∼ Ψcol(K,D¯ ,w∗), (3)
obs miss miss
where Ψcol is a constrained version of Ψ that samples a group of indices belonging to the same
column. This distribution is equivalent to the following sequential sampling procedure:
X∗ | D ,D ∼ Ψ(1,D¯ ,w∗)
1 obs miss miss
X∗ | D ,D ,X∗ ∼ Ψ(1,D¯ \{X∗},w∗),
2 obs miss 1 miss 1 (cid:101)
(4)
.
.
.
X∗ | D ,D ,X∗,...,X∗ ∼ Ψ(1,D¯ \{X∗,...,X∗ },w∗),
K obs miss 1 K−1 miss 1 K−1 (cid:101)
where the weights w∗ are given by w∗ = w∗ I(cid:2) c = X∗ (cid:3) and X∗ is the column of X∗.
(cid:101) (cid:101)r,c r,c 1,2 1,2 1
Note that the sequential sampling procedure in (4) requires the existence of one column with
at least K unobserved entries. This is always the case as long as n < n (n −K +1), and this
obs c r
is a reasonable assumption in applications where M is only sparsely observed.
This premise allows us to state our goal formally. For a given coverage level α ∈ (0,1), we seek
a joint confidence region, denoted as C(cid:98)(X∗;M
X
,α) ⊆ RK, for the K missing matrix entries
obs
indexed by X∗. Crucially, this confidence region should be informative (i.e., not too wide) and
guarantee finite-sample simultaneous coverage, in the sense that
(cid:104) (cid:105)
P M
r,c
∈ C(cid:98)r,c(X∗;M
X
obs,α), ∀(r,c) ∈ {X 1∗,...,X K∗ } ≥ 1−α. (5)
Note that the probability in (5) is taken with respect to both X , which is sampled according
obs
to (1), and X∗, which is sampled according to (4), while M is fixed.
We conclude this section by discussing the important distinction between the sampling weights
w in (1) and the test weights w∗ in (3), which have different interpretations and purposes. Intu-
itively,theroleofw istomodelsituationsinwhichthematrixisnotobserveduniformlyatrandom.
For example, in a collaborative filtering context, some types of users may be more engaged and
certain movies tend to receive more ratings. Such patterns can be captured by the model in (1) us-
ing heterogeneous weights. Further, non-uniform missingness patterns are often apparent from the
observed data, making it possible to estimate w empirically [35], as explained in Appendix A2.4.
By contrast, w∗ may be independently fixed by the practitioner, and its role is to control the
interpretation of the coverage guarantee in (5). To understand this, consider the following two
examples. If all w∗ = 1, Equation (5) offers coverage only in a marginal sense, for X∗ drawn
r,c
uniformly at random from the missing portion of the matrix [35]. If w∗ = 1 if and only if
r,c
c ∈ A, for the subset A ⊂ [n ] corresponding to “action movies”, Equation (5) can be interpreted
c
as ensuring coverage specifically for action movies. The latter is a stronger type of conditional
guarantee [68], which may be appealing if one indeed cares especially about action movies. Thus,
w∗ generallyallowsonetoplacemoreorlessemphasisoncertainunobservedportionsofthematrix,
interpolating between marginal and conditional views. Of course, there is a trade-off. As we will
seeempirically, thepriceofstrongertheoreticalguaranteesobtainedwithmoreconcentratedweight
vectors w∗ tends to take the form of wider (less informative) confidence regions [69], which is why
some flexibility in the choice of w∗ is desirable.
63 Methods
This section describes the key components of our method, which we call Structured Conformalized
Matrix Completion (SCMC). Section 3.1 gives a high-level overview of SCMC and outlines it in
Algorithm1. Section3.2detailstheconstructionofthecalibrationsetutilizedbySCMC.Section3.3
presentsageneralizedquantileinflationlemmathatprovidesthemaintheoreticalbuildingblockfor
our simultaneous coverage results. Section 3.4 characterizes precisely the conformalization weights
needed to apply our quantile inflation lemma in the context of SCMC. Section 3.5 establishes our
lower and upper simultaneous coverage bounds. Important computational shortcuts pertaining to
the evaluation of our conformalization weights are postponed to Section 4.
3.1 Method Outline
Having observed the matrix entries indexed by D ⊂ [n ] × [n ], SCMC partitions D into
obs r c obs
two disjoint subsets: a training set D and a calibration set D , so that D = D ∪D .
train cal obs train cal
However, departing from the standard approach in (split) conformal inference, we do not partition
the data completely at random. On the contrary, since we want the calibration set to exhibit a
structuresimilartothatofthetargetgroupX∗, weformD andD usingamoresophisticated
train cal
approach, the details of which are explained later in Section 3.2.
After appropriately partitioning the observations into D and D , SCMC trains a matrix
train cal
completion algorithm using only the data in D train, producing a point estimate M(cid:99)(D train) of the
full matrix M. Any matrix completion algorithm can be applied for this purpose. For example, if
M is suspected to have an underlying low-rank structure, it may be reasonable to follow a classical
convex nuclear norm minimization approach [33], computing
M(cid:99)(D train) = argmin ∥Z∥
∗
subject to P Dtrain(Z) = P Dtrain(M),
Z∈Rnr×nc
where ∥·∥ denotes the nuclear norm and P (M) is the orthogonal projection of M onto the
∗ Dtrain
subspace of matrices that vanish outside the index set D .
train
Beyond convex optimization, our method can be combined with any matrix completion algo-
rithm,includingthosebasedonnon-convexfactorization[70]ordeeplearning[71,72]. WhileSCMC
tends to produce more informative confidence regions if M(cid:99)(D train) estimates M more accurately,
its coverage guarantee will require no assumptions on how M(cid:99) is derived from D train.
Our method translates any black-box estimate M(cid:99)(D train) into confidence regions for the missing
entries as follows. Let C be a pre-specified set-valued function, termed prediction rule, that takes
as input M(cid:99), a list of K target indices x∗ = (x∗,...,x∗ ), and a parameter τ ∈ [0,1], and outputs
1 K
C(x∗,τ,M(cid:99)) ⊆ RK. (We will often make the dependence of M(cid:99)(D train) on D
train
implicit.) Our
method is flexible in the choice of the prediction rule, but we generally require that this function
be monotone increasing in τ, in the sense that
C(x∗,τ 1,M(cid:99)) ⊆ C(x∗,τ 2,M(cid:99)), almost-surely if τ
1
< τ 2, (6)
7and satisfies the following boundary conditions almost-surely:
(cid:110)(cid:16) (cid:17)(cid:111)
C(x∗,0,M(cid:99)) = M(cid:99)x∗,...,M(cid:99)x∗ , lim C(x∗,τ,M(cid:99)) = RK. (7)
1 K τ→1−
Intuitively, τ = 0correspondstoplacingabsoluteconfidenceintheaccuracyofM(cid:99), whileapproach-
ing τ = 1 suggests that the point estimate carries no information about M.
For example, a simple prediction rule that satisfies the aforementioned requirements is
(cid:18) (cid:19)
τ τ
C(x∗,τ,M(cid:99)) = M(cid:99)x∗ ± ,...,M(cid:99)x∗ ± , (8)
1 1−τ K 1−τ
whichproducesregionsintheshapeofahyper-cube. Thisapproachwillbeutilizedinournumerical
experiments due to its ease of interpretation, but it is of course not unique. See Appendix A2.1 for
further details and additional examples of alternative prediction rules.
The purpose of the observations indexed by D cal, which were not used to train M(cid:99), is to find the
smallest possible τ needed to achieve simultaneous coverage (5). As detailed in Section 3.2, SCMC
carefullyconstructsD sothatitgivesusasetofncalibration groups{Xcal,...,Xcal},whereeach
cal 1 n
Xcal consists of K observed matrix entries within the same column; i.e., Xcal = (Xcal,...,Xcal).
i i i,1 i,K
As explained in the next section, n can be fixed arbitrarily, although it should be small compared
to the total number of observed matrix entries and typically at least greater than 100 to avoid
excessivelyhighvarianceintheresults[73, 74]. Intuitively, thesecalibrationgroupsareconstructed
in such a way as to (approximately) simulate the structure of X∗.
Foreachcalibrationgroup,wecomputeaconformity scoreS = S(Xcal),definedasthesmallest
i i
value of τ for which the candidate confidence region covers all K entries of M :
Xcal
i
(cid:110) (cid:111)
S
i
:= inf τ ∈ R : M
Xcal
∈ C(X ical,τ,M(cid:99)) . (9)
i
Then, the calibrated value τ of τ is obtained by evaluating the following weighted quantile [38]
(cid:98)α,K
of the empirical distribution of the calibration scores:
n
(cid:16) (cid:88) (cid:17)
τ = Q 1−α; p δ +p δ . (10)
(cid:98)α,K i Si n+1 ∞
i=1
Above, Q(1 − α;F) denotes the 1 − α quantile of a distribution F on the augmented real line
R∪{∞}; that is, for S ∼ F, Q(β;F) = inf{s ∈ R : P(S ≤ s) ≥ β}. The distribution in (10) places
a point mass p on each observed value of S and an additional point mass p at +∞. The
i i n+1
expression of the weights p and p will be given in Section 3.4. These weights generally depend
i n+1
on X∗ and on all Xcal, although this dependence is kept implicit here for simplicity.
i
Finally, the calibrated parameter τ is utilized to construct a joint confidence region
(cid:98)α,K
C(cid:98)(X∗;M
X
obs,α) = C(X∗,τ (cid:98)α,K,M(cid:99)). (11)
This will be proved in Section 3.5 to have valid simultaneous coverage (5), as long as X is
obs
sampled from (1) and X∗ from (4). The overall procedure is summarized by Algorithm 1, while all
missing details will be carefully explained in the subsequent sections.
8Algorithm 1 Simultaneous Conformalized Matrix Completion (SCMC)
1: Input: partially observed matrix M X obs, with unordered list of observed indices D obs;
2: Input: test group X∗; nominal coverage level α ∈ (0,1);
3: Input: any matrix completion algorithm producing point estimates;
4: Input: any prediction rule C satisfying (6) and (7);
5: Input: desired number n of calibration groups.
6: Apply Algorithm 2 to obtain D train and the calibration groups (X 1cal,...,X ncal) in D cal;
7: Compute a point estimate M(cid:99), looking only the observations in D train.
8: Compute the conformity scores S i, for all i ∈ [n], with Equation (9).
9: Compute τ (cid:98)α,K in (10), based on the weights p i given by (19) in Section 3.4.
10: Output: Joint confidence region C(cid:98)(X∗;M X ,α) given by Equation (11).
obs
3.2 Assembling the Structured Calibration Set
This section explains how to partition D into a training set D and a collection of calibration
obs train
groups Xcal,...,Xcal that approximately mimic the structure of X∗. To begin, we note that the
1 n
number n of calibration groups cannot exceed ⌊n /K⌋ and, further,
obs
(cid:88)nc (cid:22) nc (cid:23)
n ≤ obs =: ξ , (12)
obs
K
c=1
where nc is the number of observed entries in column c ∈ [n ], which is a function of X in (1).
obs c obs
To satisfy these constraints, as a practical rule-of-thumb one may set n = min{1000,⌊ξ /2⌋}. In
obs
the following, we will assume that n is a fixed parameter (e.g., n = 1000) guaranteed to satisfy the
upper bound in (12). This simplification streamlines the analysis of SCMC without much loss of
generality. In principle, it would also be possible to set n in a data-independent way so that (12)
holds with high probability, as long as K is not too large compared to n and n n .
obs r c
For any given n satisfying (12), we partition D into a training set D and a collection of n
obs train
calibration groups Xcal,...,Xcal as detailed in Algorithm 2. After initializing an empty D , we
1 n train
iterate over each column c and assign to D a random subset of mc := nc mod K observations
train obs
fromthatcolumn,wherenc isthetotalnumberofobservationsincolumnc. Thispreliminarystep
obs
ensures that the remaining number of observations in column c is a multiple of K (possibly zero).
Then, for each i ∈ [n], Xcal is obtained by sampling K observations uniformly without replacement
i
from a randomly chosen matrix column. Finally, all remaining observations are assigned to D .
train
Algorithm2intuitivelymimicsthesamplingmodelforX∗ definedin(3),withthekeydifference
that it samples the calibration groups from D instead of D . This unavoidable discrepancy,
obs miss
however, is delicate, as it implies that Xcal,...,Xcal are neither exchangeable nor weighted ex-
1 n
changeable [38] with the test group X∗. Therefore, an innovative approach is needed to translate
thesecalibrationgroupsintovalidsimultaneousconfidenceregions,asexplainedinthenextsection.
9Algorithm 2 Assembling the structured calibration set for Algorithm 1
1: Input: Set D obs of n obs observed entries; number n of calibration groups; group size K.
2: Initialize an empty set of matrix indices, D prune = ∅.
3: for all columns c ∈ [n c] do
4: Define mc := nc mod K.
obs
5: if mc ̸= 0 then
6: Sample mc indices (I 1,...,I mc) ∼ Ψ(mc,D obs∩([n r]×{c}),1).
7: Add the entry indices {I 1,...,I mc} to D prune.
8: end if
9: end for
10: Initialize a set of available observed matrix indices, D avail = D obs\D prune.
11: Initialize an empty set of matrix index groups, D cal = ∅.
12: for i ∈ [n] do
13: Sample X ical = (X ic ,a 1l,...,X ic ,a Kl) ∼ Ψcol(K,D avail,1), with Ψcol defined as in (3).
14: Insert X ical in D cal. Remove {X ic ,a 1l,...,X ic ,a Kl} from D avail.
15: end for
16: Define: D train = D prune∪D avail.
17: Output: Set of calibration groups D cal = {X 1cal,...,X ncal}; training set D train ⊂ D obs;
3.3 A General Quantile Inflation Lemma
Consider a conformity score S∗, defined similarly to the scores S in (9),
i
(cid:110) (cid:111)
S∗ := inf τ ∈ R : M
X∗
∈ C(X∗,τ,M(cid:99)) . (13)
In words, S∗ is the smallest τ for which C(X∗,τ,M(cid:99)) covers all K entries of M X∗. Although this
score cannot be observed because the matrix entries indexed by X∗ are latent, it is a well-defined
and useful quantity. It allows us to write the probability that the confidence region output by
Algorithm 1 simultaneously covers all elements of M as:
X∗
(cid:104) (cid:105)
P M
X∗
∈ C(X∗,τ (cid:98)α,K,M(cid:99)) = P[S∗ ≤ τ (cid:98)α,K]. (14)
To establish that Algorithm 1 achieves simultaneous coverage (5), the right-hand-side of (14)
must be bounded from below by 1−α, for a suitable (and practical) choice of the weights p and
i
p used to compute τ in (19). This is not straightforward because the scores S ,...,S ,S∗
n+1 (cid:98)α,K 1 n
are neither exchangeable nor weighted exchangeable, as they respectively depend on Xcal,...,Xcal
1 n
and X∗. A solution is provided by the following lemma due to Tibshirani et al. [38].
Lemma 1 (from Tibshirani et al. [38]). Let Z ,...,Z be random variables with joint law f.
1 n+1
For any fixed function s and i ∈ [n+1], define V = s(Z ,Z ), where Z = {Z ,...,Z }\{Z }.
i i −i −i 1 n+1 i
Assume that V ,...,V are distinct almost surely. Define also
1 n+1
(cid:80)
f(z ,...,z )
pf(z ,...,z ) := σ∈S:σ(n+1)=i σ(1) σ(n+1) , ∀i ∈ [n+1], (15)
i 1 n+1 (cid:80) f(z ,...,z )
σ∈S σ(1) σ(n+1)
10where S is the set of all permutations of [n+1]. Then, for any β ∈ (0,1),
(cid:34) (cid:18) n (cid:19)(cid:35)
P V ≤ Q β; (cid:88) pf(Z ,...,Z )δ +pf (Z ,...,Z )δ ≥ β.
n+1 i 1 n+1 Vi n+1 1 n+1 ∞
i=1
TranslatingLemma1intoapracticalmethodrequiresevaluatingtheweightspf definedin(15),
i
which generally involves a computationally unfeasible sum over an exponential number of permuta-
tions. If the distribution f satisfies a symmetry condition called “weighted exchangeability”, it was
shown by Tibshirani et al. [38] that the expression in (15) simplifies greatly, but this is not helpful
in our case because (Xcal,...,Xcal,X∗) do not enjoy such a property. Further, it is unclear how
1 n
Algorithm 2 may be modified to achieve weighted exchangeability.
Fortunately, our groups satisfy a “leave-one-out exchangeability” property that still enables an
efficient computation of the conformalization weights in (15). Intuitively, the joint distribution of
Xcal,...,Xcal,X∗ is invariant to the reordering of the first n variables.
1 n
Proposition 1. Let D and D be subsets of observed and missing matrix entries, respectively,
obs miss
sampled according to (1). Let X∗ be a test group sampled according to (3) conditional on D .
obs
Suppose Xcal,...,Xcal are the calibration groups output by Algorithm 2, while D and D
1 n prune train
are the corresponding training and pruned observation sets. Then, for any permutation σ of [n],
(Xcal,Xcal,...,Xcal,X∗) =d (Xcal ,Xcal ,...,Xcal ,X∗) | D ,D .
1 2 n σ(1) σ(2) σ(n) prune train
The proof of Proposition 1 is in Appendix A4.2. Intuitively, this is established by deriving the
joint distribution of (Xcal,...,Xcal,X∗) conditional on D and D . The usefulness of this
1 n prune train
result becomes clear in the light of the following specialized version of Lemma 1.
Lemma 2. Let Z ,...,Z be leave-one-out exchangeable random variables, so that there exists
1 n+1
a permutation-invariant function g such that their joint law f can be factorized as
f(Z ,...,Z ) = g({Z ,...,Z })·h¯({Z ,...,Z },Z ), (16)
1 n+1 1 n+1 1 n n+1
for some function h¯ taking as first input an unordered set of n elements. For any fixed function
s : Rn+1 (cid:55)→ R and any i ∈ [n + 1], define V = s(Z ,Z ), where Z = {Z ,...,Z } \ {Z }.
i i −i −i 1 n+1 i
Assume that V ,...,V are almost-surely distinct. Then, ∀β ∈ (0,1),
1 n+1
(cid:26) (cid:18) n (cid:19)(cid:27)
(cid:88)
P V ≤ Q β; p (Z ,...,Z )δ +p (Z ,...,Z )δ ≥ β, (17)
n+1 i 1 n+1 Vi n+1 1 n+1 ∞
i=1
where
h¯(Z ,Z )
−i i
p (Z ,...,Z ) = . (18)
i 1 n+1 (cid:80)n+1h¯(Z
,Z )
j=1 −j j
The proof of Lemma 2 is in Appendix A4.1. Note that the weights p in (18) are relatively easy
i
to compute because they involve a sum over only n+1 instead of (n+1)! terms.
113.4 Characterization of the Conformalization Weights
We now characterize explicitly the conformalization weights needed to apply Lemma 2 to our
problem. The following notation is useful for this purpose.
Denote by D¯ an augmented version of D that also includes the unordered set of indices
obs obs
corresponding to the test group X∗ = (X∗,X∗,...,X∗ ); i.e.,
1 2 K
D¯ := D ∪(cid:0) ∪ {X∗}(cid:1) .
obs obs k∈[K] k
Similarly, let D and D¯ denote possible realization of D and D¯ , respectively. Then, for
obs obs obs obs
any i ∈ [n], denote by D the imaginary set of observations obtained by replacing the indices
obs;i
correspondingtothecalibrationgroupXcalwiththosecorrespondingtothetestgroupX∗. Further,
i
let D := D denote the original observation set. In summary,
obs;n+1 obs
(cid:40)
D¯ \∪ {x }, for i ∈ [n],
D := obs k∈[K] i,k
obs;i
D , for i = n+1,
obs
where x = (x ,...,x ) is a realization of Xcal and x is a realization of X∗.
i i,1 i,K i n+1
Next, let nc denote the numbers of observations in column c from the sets D . Define also
obs obs
n¯c := nc −(nc mod K), the corresponding numbers of observations remaining in column c
obs obs obs
aftertherandompruningstepofAlgorithm2. Foranyi ∈ [n+1], letc denotethecolumntowhich
i
x belongs; i.e., c := x ,∀k ∈ [K], where x is the column of the k-th entry in x . Further, let
i i i,k,2 i,k,2 i
D¯ denotearealizationofD¯ in(2). Withslightabuseofnotation,wedenotethesetofmissing
miss miss
indices in column c excluding those in the group x as Dcn+1 \x := Dcn+1 \{x }K .
n+1 n+1 miss n+1 miss n+1,k k=1
We are now ready to state how Lemma 2 applies in our setting, with an explicit expression for the
conformalization weights in (18).
Lemma 3. Under the setting of Proposition 1, let Z ,...,Z ,Z denote Xcal,...,Xcal,X∗,
1 n n+1 1 n
and V ,...,V ,V represent the corresponding scores S ,...,S ,S∗ given by (9) and (13), re-
1 n n+1 1 n
spectively, based on a matrix estimate M(cid:99) computed based on the observations in D train. Then,
Equation (17) from Lemma 2 applies conditional on D and D , with weights
train prune
(cid:32) K (cid:33)
(cid:89)
p (x ,...,x ,x ) ∝ P (D = D )· w∗ w∗
i 1 n n+1 w obs obs;i (cid:101)xi,1 (cid:101)x
i,k
k=2
 (cid:16)nci (cid:17) (cid:16)ncn+1(cid:17) I[ci̸=cn+1] (19)
n¯o cibs n¯o cnbs +1 K (cid:89)−1 n¯ci −k
· obs · obs · obs  .
(cid:16)nci −K(cid:17) (cid:16)ncn+1 +K(cid:17) n¯cn+1 +K −k
n¯o cibs
−K
n¯o cnbs
+1 +K
k=1 obs
obs obs
Above, w∗ and w∗ have explicit expressions that depend on the weights w∗ in (3); i.e.,
(cid:101)xi,1 (cid:101)x
i,k
w∗
w∗ = xi,1 , (20)
(cid:101)xi,1
K (cid:16) (cid:17)
(cid:80) w∗ − (cid:80) w∗ −w∗ +u∗
(r,c)∈D¯
miss
r,c x
n+1,k
x
i,k
xi,1
k=1
12with
 
u∗ = I[c ̸= c ]I(cid:2) nci < K(cid:3) (cid:88) w∗ −I(cid:2) ncn+1 < 2K(cid:3) (cid:88) w∗ ,
xi,1 i n+1 miss  r,c miss r,c
(r,c)∈D mci
iss
(r,c)∈D mcn is+ s1\xn+1
and, for all k ∈ {2,...,K},
w∗
w∗ = x i,k . (21)
(cid:101)x
i,k K K
(cid:80) w∗ + (cid:80) w∗ −I[c = c ] (cid:80) w∗
(r,c)∈Dci
r,c
k′=k
x i,k′ i n+1
k′=1
x n+1,k′
miss
The main challenge in the computation of (19) arises from the term P (D = D ), which
w obs obs;i
is the probability of observing the matrix entries in D and depends on the sampling weights
obs;i
w in (1). Although this probability cannot be evaluated analytically, it can be approximated with
an efficient algorithm, which makes it possible to compute the conformalization weights in (19) at
cost O(n n +nK), as explained in Section 4.
r c
3.5 Finite-Sample Coverage Bounds
The following theorem states formally that Algorithm 1 produces joint confidence regions with
simultaneous coverage for random groups X∗ sampled according to the model defined in (3). This
result follows by integrating Proposition 1, Lemma 2, and Equation (19).
Theorem 1. Suppose D and D are sampled according to (1). Let X∗ be a test group sampled
obs miss
according to (3) conditional on D . Then, for any fixed level α ∈ (0,1), the joint confidence region
obs
output by Algorithm 1 satisfies (5) conditional on D ,D :
train prune
(cid:104) (cid:105)
P M
X∗
∈ C(cid:98)(X∗;M
X
,α) | D train,D
prune
≥ 1−α.
obs
Note that the probability in Theorem 1 is taken over the randomness in {Xcal,...,Xcal} and
1 n
X∗, whileD andD canbeconsideredfixed. Therefore, thisresultimpliesthesimultaneous
train prune
coverage property stated earlier in (5). Further, it is also possible to bound our simultaneous
coverage from above.
Theorem 2. Under the same setting of Theorem 1,
(cid:20) (cid:21)
(cid:104) (cid:105)
P M
X∗
∈ C(cid:98)(X∗;M
X
obs,α) ≤ 1−α+E max p i(X 1cal,...,X ncal,X∗) , (22)
i∈[n+1]
where the conformalization weights p are given by (19) and the expectations can also be taken
i
conditional on D and D , as in Theorem 1.
train prune
Theorem 2 is proved in Appendix A4.4. A numerical investigation of the expected value on
the right-hand-side of (22), conducted in Appendix A3.2, demonstrates that in practice the upper
bound in (22) converges to 1−α as n increases. This is consistent with our empirical observations
that Algorithm 1 is not too conservative, as previewed in Figure 1.
134 Computational Shortcuts and Cost Analysis
4.1 Efficient Evaluation of the Conformalization Weights
We now explain how to efficiently approximate the conformalization weights p in (19), for all
i
i ∈ [n + 1]. The main challenge is to evaluate P (D = D ) according to the missingness
w obs obs;i
model defined in (1). In truth, it suffices to relate this probability, which depends on the index
i ∈ [n+1], to P (D = D ), which is constant and can thus be ignored when computing (19).
w obs obs
In this section, we demonstrate that their ratio can be expressed in a much more tractable form,
one whose computational complexity does not increase with the matrix dimensions.
We begin by expressing P (D = D ) and P (D = D ), for any i ∈ [n+1], as closed-
w obs obs w obs obs;i
(cid:80)
form integrals. Let δ = w denote the cumulative weight of all missing indices and,
(r,c)∈Dmiss r,c
for any positive scaling parameter h > 0, define Φ(τ;h) of τ ∈ (0,1] as
(cid:89) (cid:16) (cid:17)
Φ(τ;h) := hδτhδ−1 1−τhwr,c , ϕ(τ;h) := logΦ(τ;h). (23)
(r,c)∈D
obs
Further, define also d :=
(cid:80)K
(w −w ) for all i ∈ [n+1].
i k=1 x i,k x n+1,k
Proposition 2. For any fixed n < n n , scaling parameter h > 0, and i ∈ [n+1],
obs r c
(cid:90) 1
P (D = D ) = Φ(τ;h)·η (τ)dτ, (24)
w obs obs;i i
0
where, for any τ ∈ (0,1),
τhdi(δ+d i) (cid:32) (cid:89)K 1−τhwxn+1,k(cid:33)
η (τ;h) := · . (25)
i δ 1−τhwxi,k
k=1
Note that η (τ) = 1 for all τ if i = n+1, and in that case Proposition 2 recovers a classical
n+1
result by Wallenius [66]. See the proof of Proposition 2 in Appendix A4.5 for further details.
Furthermore, the function η in (25) is a product of only K simple functions of τ, and therefore it
i
is straightforward to evaluate even for large matrices.
Proposition 2 provides the foundation for evaluating the conformalization weights p in (19).
i
The remaining difficulty is that (24) has no analytical solution. Fortunately, the function Φ(τ;h)
satisfies some properties that make it feasible to approximate this integral accurately.
Lemma 4. If h > 1−δ, the function Φ(τ;h) defined in (23) has a unique stationary point with
respect to τ at some value τ ∈ (0,1). Further, τ is a global maximum.
h h
See Figure 2 for a visualization of Φ(τ;h) and η (τ;h), in two examples where the sampling
i
weights w in (1) are independent and uniformly distributed on [0,1]. These results show that
Φ(τ;h)becomesincreasinglyconcentratedarounditsuniquemaximumforlargersamplesizes,while
η (τ;h) remains relatively smooth (or flat) at that point. Therefore, it makes sense to approximate
i
this integral through a careful extension of Laplace’s method [40]. This is explained below.
The first step to approximate the integral in (24) with a generalized Laplace method (justified
later Section 4.2), is to modify the integrand in such a way as to move the peak away from the
141.0 1.00 1.0 1.00
0.8 0.80 0.8 0.80
0.6 0.60 0.6 0.60
0.4 0.40 0.4 0.40
0.2 0.20 0.2 0.20
0.0 0.00 0.0 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
τ τ
(a) n =10, n n =100, K =10. (b) n =200, n n =1000, K =10.
obs r c obs r c
Figure 2: Visualization of the centered and re-scaled functions Φ(τ;h) and η (τ;h), with h chosen
i
so that Φ(τ;h) is maximized at τ = 1/2, for different matrix sizes and numbers of observations.
integration boundary. To this end, define τ as
h
τ := argmaxΦ(τ;h), (26)
h
τ∈(0,1)
and recall that h > 0 is a parameter that we are free to choose. Therefore, we will tune h in such
a way as to center the peak within the integration domain; that is, we pick a value h such that
τ = 1/2. Fortunately, Lemma 4 tells us that the function Φ(τ;h) has a unique global maximum
h
at τ ∈ (0,1) when h > 1/δ, and a suitable value of h > 1/δ such that τ = 1/2 can be found by
h h
applying the Newton-Raphson iterative algorithm; see Appendix A2.2 for further details.
Havingfixedhsuchthatτ = 1/2,aLaplaceapproximationcanbeobtainedasfollows. Thekey
h
intuition is that, as the number of observations grows, the peak of the function Φ(τ;h) increasingly
dominates the integral. In particular, a second-order Taylor expansion shows that the integral is
primarily determined by the value of η (τ;h)·Φ(τ;h) at τ = τ and by the curvature of logΦ(τ;h)
i h
at the peak, namely ϕ′′(τ ;h). This leads to the following approximation,
h
(cid:115)
−2π
P (D = D ) ≈ η (τ ;h)·Φ(τ ;h) ≈ η (τ ;h)·P (D = D ). (27)
w obs obs;i i h h ϕ′′(τ ;h) i h w obs obs
h
As explained below, this approximation becomes very accurate in the large-sample limit, and it
is useful because it allows us to approximate the ratio P (D = D )/P (D = D ) with
w obs obs;i w obs obs
a quantity, η (τ ;h), that is straightforward to calculate. For example, if the sampling weights w
i h
in (1) are uniformly constant, η (τ;h) ≡ 1 for all i ∈ [n+1] and any τ ∈ (0,1).
i
By combining (27) with (19), it follows that, for each i ∈ [n+1], the conformalization weight
p can be approximately rewritten in the large-sample limit as
i
p¯(x ,...,x ,x )
i 1 n n+1
p (x ,...,x ,x ) ≈ , (28)
i 1 n n+1 (cid:80)n+1p¯
(x ,...,x ,x )
j=1 j 1 n n+1
15
)delacser(ϕ )delacser(η )delacser(ϕ )delacser(ηwith the un-normalized weight p¯ given by:
i
(cid:32) K (cid:33)
(cid:89)
p¯(x ,...,x ,x ) = η (τ )· w∗ w∗
i 1 n n+1 i h (cid:101)xi,1 (cid:101)x
i,k
k=2
 (cid:16)nci (cid:17) (cid:16)ncn+1(cid:17) I[ci̸=cn+1] (29)
n¯o cibs n¯o cnbs +1 K (cid:89)−1 n¯ci −k
· obs · obs · obs  .
(cid:16)nci −K(cid:17) (cid:16)ncn+1 +K(cid:17) n¯cn+1 +K −k
n¯o cibs
−K
n¯o cnbs
+1 +K
k=1 obs
obs obs
This finally makes Algorithm 1 practical because evaluating p¯ in (29) only involves simple arith-
i
metic operations and can be carried out very efficiently, as explained in Section 4.3.
4.2 Consistency of the Generalized Laplace Approximation
It is important to emphasize that our approximation in (27) is not obtained from a standard appli-
cation of the Laplace method, since the latter is typically restricted to handling integrals of simpler
functions; see Appendix A1.4. Yet, the Taylor approximation ideas underlying the Laplace method
are versatile enough to be extended to our setting, as demonstrated by the following theorem. This
novel result provides a rigorous justification for the generalized Laplace approximation in (27). For
simplicity, but without much loss of generality, this theorem relies on some additional technical
assumptions, which will be justified in our context towards the end of this section. This result is
presented informally here for simplicity, but a formal statement can be found in Appendix A4.6,
along with its proof.
Theorem 3 (Informal statement of Theorem A4). Let {v }∞ denote a sequence of i.i.d. random
i i=1
variables from some distribution F supported on (0,1), and {x }∞ a sequence of independent
i i=1
Bernoulli random variables, with x ∼ Bernoulli(v ). Define δ =
(cid:80)m
(1−x )v and let
i i m i=1 i i
m
Φ (τ) := h δ
τhmδm−1(cid:89)(cid:16) 1−τhmvi(cid:17)xi
, (30)
m m m
i=1
where h is such that τ := argmax Φ (τ) = 1/2. Define also ϕ (τ) := logΦ (τ). Then,
m h τ∈(0,1) m m m
for a sequence of functions f bounded away from 0 and satisfying certain smoothness conditions,
m
(cid:82)1
f (τ)Φ (τ)dτ
lim 0 m m = 1 almost surely. (31)
(cid:112)
n→∞ f (τ )·Φ (τ ) −2π/ϕ′′ (1/2)
m h m h m
To relate this result to the Laplace approximations described in Section 4.1, let us compare the
function Φ(τ;h) in (23) with the function Φ (τ) in (30). Given a mapping from the sequence to
m
the matrix entries σ : [n n ] (cid:55)→ [n ]×[n ], we can express Φ(τ;h) as:
r c r c
Φ(τ;h) = hδτhδ−1
(cid:89) (cid:16) 1−τhwr,c(cid:17)
=
hδτhδ−1n (cid:89)rnc(cid:16)
1−τhw
σ(i)(cid:17)1{σ(i)∈D obs}
. (32)
(r,c)∈D i=1
obs
Therefore, the discrepancy between Φ(τ;h) and Φ (τ) can be traced to the different sampling
nrnc
models describing the distributions of our matrix observations and the x variables in Theorem 3.
i
In Theorem 3, the observations follow independent Bernoulli distributions, whereas the matrix
16entriesinourmodel(3)aresampledwithoutreplacement. Theseviewscanbereconciledasfollows.
Sampling without replacement is a natural modelling choice for the simultaneous inference problem
studied in this paper, but it would make the proof of Theorem A4 too complicated. Nevertheless,
these two models are qualitatively consistent. Suppose the sampling weights w in (1) are constant;
then, in that special case our model corresponds to that of Theorem 3 with v ≡ v, for some
i
constant v ∈ (0,1), after conditioning on the observed number of entries n .
obs
4.3 Computational Complexity
The SCMC method described in this paper can be implemented efficiently and is able to handle
completion tasks involving large matrices. Its practicality is demonstrated in this section, which
summarizes the results of an analysis of the computational complexity of different components of
Algorithm 1. We refer to Appendix A2.3 for the details behind this analysis and an explanation of
the underling computational shortcuts with which all redundant operations are streamlined.
In summary, the cost of producing a joint confidence region for a test group X∗ of size K using
Algorithm 1 is O(T+n n +n(K+logn)), where T denotes the fixed cost of training the black-box
r c
matrix completion model based on M and n is the number of calibration groups. Further, it
X
obs
is possible to recycle redundant calculations when constructing simultaneous confidence regions for
m distinct test groups X∗, as explained in Appendix A2.3. Therefore, the overall cost of obtaining
m distinct confidence regions for m different groups is only O(T +n n +n(mK +logn)). See
r c
Table 1 for a summary of these results.
Table 1: Computational analysis of different components of SCMC.
Module Cost for one test group Cost for m test groups
Overall (Algorithm 1) O(T +n n +n(K +logn)) O(T +n n +n(mK +logn))
r c r c
Algorithm 2 O(n n +nK) O(n n +nK)
c r c r
5 Empirical Demonstrations
We apply SCMC to simulated and real data, comparing its performance to those of the unadjusted
and Bonferroni baselines. This section is organized as follows. Section 5.1 describes experiments
based on simulated data, with Section 5.1.1 focusing on (known) uniform sampling weights, and
Section 5.1.2 allowing the sampling weights for the observed data to be heterogeneous (although
stillknownexactly). Section5.2describesmorerealisticexperimentsinvolvingtheMovieLensdata,
considering estimated sampling weights. The results of additional experiments are presented in the
Appendices. Appendix A3.1 describes experiments with synthetic data involving heterogeneous
test weights. Appendix A3.2 investigates the tightness of the theoretical coverage upper bounds
derived in Section 3.5.
175.1 Numerical Experiments with Synthetic Data
5.1.1 Uniform Sampling Weights
We begin with a simple scenario in which the observation pattern in (3) is completely random
and the test weights in (4) are uniform: w = w∗ = 1 for all (r,c) ∈ [n ]×[n ]. A matrix M
r,c r,c r c
with n = 200 rows and n = 200 columns is generated based on a “signal plus noise” model that
r c
exhibits both a low-rank structure and column-wise dependencies. (For example, in the Netflix
data set, users may tend to agree on the quality of certain movies, leading to positive dependency
among the columns of the rating matrix.) This design is motivated by the intuition that column-
wise dependencies make our simultaneous inference task especially challenging, helping us better
understand the settings under which our method brings larger practical advantages relative to the
baselines.
The ground truth matrix M is obtained as M = 0.5 · M¯ + 0.5 · N, where M¯ ∈ Rnr×nc is
low-rank while N ∈ Rnr×nc is a noise matrix exhibiting column-wise dependencies whose strength
can be tuned as a control parameter, as detailed below.
1. M¯ ∈ Rnr×nc is given by a random factorization model with rank l = 5; i.e., M¯ = U¯(V¯)⊤,
where U¯ = (U ) and V¯ = (V ) are such that
r,c r∈[nr],c∈[l] r′,c′ r′∈[nc],c′∈[l]
i.i.d. i.i.d.
U ∼ N(0,1), V ∼ N(0,1). (33)
r,c r′,c′
2. N = 0.1·ϵ+0.9·1ϵ⊤, where 1 ∈ Rnr×1 is a vector of ones, ϵ ∈ Rnr×nc has i.i.d. standard
(cid:101)
normal components, and ϵ (cid:101)∈ Rnc×1 is such that, for all c ∈ [n c],
i.i.d.
ϵ ∼ (1−γ)·N(0,1)+γ ·N(µ,0.1), (34)
(cid:101)c
for suitable parameters γ ∈ (0,1) and µ ∈ R. Thus, 1ϵ⊤ ∈ Rnr×nc has constant columns,
(cid:101)
and larger values of µ ∈ R result in stronger column-wise dependencies compared to the
background i.i.d. noise described by the matrix ϵ. In the following, the value of µ is varied
as a control parameter, while we fix γ = α/2.
For a given ground truth matrix M generated as described above, we observe n = 8000
obs
entries, randomly sampled according to model defined in (1) with w = 1 for all (r,c) ∈ [n ]×[n ].
r,c r c
Let D denote the unordered collection of these observed indices. Then, 100 test groups X∗
obs
of size K, where K ≥ 2 is a control parameter, are sampled without replacement from D =
miss
[n ]×[n ]\D , according to the model defined in (3) with w∗ = 1 for all (r,c) ∈ D .
r c obs r,c miss
The simultaneous confidence region for a test group X∗ is constructed by applying Algorithm 1
with n = min{1000,⌊ξ /2⌋} calibration groups, where ξ , defined in (12), denotes the maximum
obs obs
possible number of such groups. Note that the matrix algorithm leveraged by our method can thus
be trained using n = n −Kn observed entries of M, indexed by D .
train obs train
While SCMC can leverage any matrix completion algorithm producing point predictions, here
weemploythealternatingleastsquaresapproachofHu,Koren,andVolinsky[75],whichisdesigned
to recover low-rank signals. For simplicity, we apply this algorithm with an hypothesized rank of 5,
which matches the true rank of M¯ . It is worth repeating, however, that the validity of the SCMC
confidence regions is independent of both the true M and the matrix completion model.
18Our method is compared to the two baselines introduced in Section 1.2. Recall that the first
one is a naive unadjusted heuristic that ignores the multiple testing aspect of our simultaneous
inference problem and essentially applies Algorithm 1 with K = 1 repeatedly for every individual
entry in X∗. This ensures valid coverage for each entry in X∗ separately, but does not guarantee
simultaneous coverage for groups with K ≥ 2. By contrast, the second Bonferroni baseline relies
on a crude and overly conservative multiple testing adjustment to achieve simultaneous coverage,
essentially applying Algorithm 1 with K = 1 at level α/K instead of α. Both baseline approaches
areappliedusingthesamematrixcompletionmodelleveragedbyourmethod,andtheirpredictions
are calibrated using a calibration set containing Kn observed matrix entries.
µ: 0 µ: 15 µ: 30
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8 Method
0.7
0.7
0.6 0.7 Simultaneous
0.6
12.5
40 Bonferroni
10.0 20
7.5 15 30 Unadjusted
20 5.0 10
2.5 5 10
2 4 6 8 2 4 6 8 2 4 6 8
Group size K
Figure 3: Performance of SCMC and two baselines on simulated data, as a function of the group
size K and of the column-wise noise parameter µ. The nominal coverage level is 90%.
Figure 3 summarizes the results of these experiments as a function of K and for different values
of the noise parameter µ. Each method is assessed in terms of the average width of the output
confidence regions, at level α = 10%, and of the empirical simultaneous coverage for the 100 test
groups. All results are averaged over 300 independent experiments. Our method always achieves
the desired 90% simultaneous coverage, as predicted by the theory, while the unadjusted baseline
becomes increasingly anti-conservative for larger values of K. Further, our method leads to more
informative confidence regions compared to the Bonferroni baseline, which becomes increasingly
conservative with larger values of K and µ. See Figure A10 in Appendix A3 for a different view of
these results, highlighting the behavior of all methods as a function of µ, for different values of K.
5.1.2 Heterogeneous Sampling Weights
Moving beyond the setting of data missing completely at random, we now consider similar experi-
ments in which the sampling weights w of the observation model (3) are heterogeneous, while the
matrix M has a simple low-rank structure. In particular, M is generated according to the random
factorization model defined in (33), so that M = U¯(V¯)⊤ with rank l = 8 and n = n = 400. The
c r
sampling weights w are chosen such as to introduce an interesting spatial missingness pattern, with
some rows and columns being more densely observed than others. Precisely, we set
w = (n (c−1)+r)s, ∀r ∈ [n ], c ∈ [n ], (35)
r,c r r c
where s ≥ 0 controls the degree of heterogeneity. If s = 0, the missingness is uniform, whereas
larger values of s result in columns with higher indices to be more densely observed.
19
Group
cov.
Avg.
widthBased on this model, we randomly sample without replacement n = 48,000 matrix entries
obs
(from a total of 160,000) and then apply Algorithm 1 similarly to the previous section, using n =
min{2000,⌊ξ /2⌋}calibrationgroupsandallocatingtheremainingn = n −Knobservations
obs train obs
to train the matrix completion model. For the latter, we rely on the same alternating least squares
algorithm [75] as in the previous section, with hypothesized rank 8. The two baseline approaches
are also applied similarly, following an approach analogous to that described in Section 5.1.1.
All methods are evaluated on a test set of 100 test groups sampled without replacement accord-
ing to the model defined in (3), with uniform weights w∗ = 1 for all (r,c) ∈ [400]×[400]. The α
r,c
level is 10%. All results are averaged over 300 independent experiments.
K: 2 K: 5 K: 8
1.0 1.0 1.0
0.9 0.9 0.9
0.8
0.8 0.8 0.7 Method
0.7 0.7 0.6 Simultaneous
3 5 Bonferroni
0.6 4
0.4 2 3 Unadjusted
2
0.2 1
1
0.0 0 0
0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0 0.6 0.7 0.8 0.9 1.0
Missingness heterogeneity
Figure 4: Performance of SCMC and two baselines on simulated data with a heterogeneous ob-
servation pattern whose strength is controlled by a scale parameter s ≥ 0. The results are shown
separately for different group sizes K. Other details are as in Figure 3.
Figure 4 reports on the results of these experiments as a function of the parameter s in (35), for
different values of K. As predicted by the theory, our method always achieves valid simultaneous
coverage, unlike the unadjusted baseline. Further, our method produces relatively informative
confidence regions compared to the Bonferroni approach, as the latter becomes more conservative
forlargervaluesofs. Thiscanbeunderstoodasfollows. Asthematrixcompletionmodelnaturally
finds it easier to recover more accurately the missing entries belonging to more densely observed
columns, the heterogeneous sampling model tends to introduce spatial dependencies in the residual
matrix M(cid:99) − M. These dependencies, which intuitively become stronger for larger values of s,
make our simultaneous inference task intrinsically more challenging, resulting in wider confidence
regions for all methods, but have a disproportionate adverse effect on the Bonferroni approach
(which implicitly but incorrectly assumes the miscoverage events corresponding to different entries
to be mutually independent). See Figure A11 in Appendix A3 for a different view of these results,
highlighting the behavior of all methods as a function of K, for different values of s.
5.2 Numerical Experiments with MovieLens Data
We now apply our method to the MovieLens 100K [37] data and compare its performance to those
of the unadjusted and Bonferroni baselines. This data set contains 100,000 ratings (on a scale from
1 to 5) provided by 943 users for 1682 movies. Therefore, approximately 94% of all possible ratings
are missing. To reduce the memory requirements of the matrix completion algorithm utilized to
compute M(cid:99), we reduce the matrix size by half, focusing on a smaller rating matrix M ∈ R800×1000,
corresponding to a random subset of 800 users and 1000 movies.
20
Group
cov.
Avg.
widthAsusual,wedenotethesetofindicesfortheobservedmatrixentriesasD anditscomplement
obs
as D = [800]×[1000]\D . Since the true sampling weights w are unknown in this application,
miss obs
we compute estimated weights w with a data-driven approach inspired by Gui, Barber, and Ma
(cid:98)
[35], as described in Appendix A2.4. Algorithm 1 is then applied with w instead of w, to construct
(cid:98)
simultaneous confidence regions for the unobserved ratings of 100 random test groups X∗. We
utilizen = min{1000,⌊ξ /2⌋}calibrationgroupsandvarythegroupsizeK asacontrolparameter.
obs
The test groups are randomly sampled without replacement from D according to the model
miss
defined in (3), with uniform weights w∗. The matrix completion algorithm is trained as described
intheprevioussections,applyingthealternatingleastsquaresapproachofHu,Koren,andVolinsky
[75] based on n = n −Kn observations. The hypothesized rank of M utilized by this model
train obs
to obtain M(cid:99) is varied as an additional control parameter. As before, the baseline approaches are
also applied based on the same matrix completion model, to facilitate the comparison with our
method.
Figure 1, previewed earlier in Section 1.2, reports on the results of these experiments as a func-
tion of the group size K and of the hypothesized rank utilized by the matrix completion model.
The confidence regions are assessed based on their average width alone, since it is impossible to
measure the empirical coverage given that the ground truth is unknown. The results show that
SCMC produces more informative (narrower) confidence regions compared to the Bonferroni ap-
proach, consistently with the results of our previous experiments based on synthetic data. Figure 1
displays only the performance of the Bonferroni baseline because the unadjusted baseline is not
intended to provide valid simultaneous coverage, making it less suitable for comparisons lacking a
verifiable ground truth. Nevertheless, Figure A12 in Appendix A3.5 includes a comparison with
both baselines, demonstrating that our simultaneous confidence regions are not much wider than
those produced by the unadjusted baseline. Further, our method’s higher reliability compared to
the unadjusted baseline is supported by the following additional experiments, conducted using the
same data but under a more artificial setting in which the ground truth is known.
ToevaluatethecoverageontheMovieLensdata,wecarryoutsimilarbutmorecloselycontrolled
experiments in which the test groups are drawn not from D (for which the ground truth is
miss
unknown) but from a hold-out subset D containing 20% of the observed matrix indices in
hout
D . Algorithm 1 is then applied to construct confidence regions for the unobserved ratings of 100
obs
random test groups X∗ sampled from D , proceeding as described before but utilizing only the
hout
observed data in D \D instead of D .
obs hout obs
Since the estimation of w acknowledges the existence of an unobserved set of entries D , in
(cid:98) miss
this setting our method is essentially aiming to achieve simultaneous coverage for test groups X∗
sampled from D \D instead of D . Of course, we can only evaluate the empirical coverage
miss hout hout
for test groups sampled from D , and this is why these experiments are useful to understand the
hout
robustness of our inferences to possible distribution shifts between D \D and D .
miss hout hout
Figure 5 compares the performances of each method under this hybrid setting, focusing on test
groups sampled from the hold-out data in D . These results are reported as a function of K,
hout
for different values of the hypothesized rank in the matrix completion model. Consistently with
the previous results, our method leads to more informative inferences compared to the Bonferroni
approach, anditnearlyachievesthedesired90%simultaneouscoverageforthetestgroupssampled
fromD ,eventhoughintheoryonewouldonlyexpectittohavevalidcoverageonaverageoverall
hout
test groups sampled from D \D . The nearly valid coverage also demonstrates the robustness
miss hout
of our method towards possible misspecification of the sampling weights.
21Guessed rank: 3 Guessed rank: 5 Guessed rank: 7
1.0
0.9
0.8
Method
0.7
0.6 Simultaneous
4.00 Bonferroni
3.75
Unadjusted
3.50
3.25
3.00
2 4 6 8 2 4 6 8 2 4 6 8
Group size K
Figure 5: Performance of SCMC and two baselines on the MovieLens data set. These confidence
regions are evaluated on a subset of hold-out observations, for which the ground truth is known,
but they are calibrated to achieve valid simultaneous coverage on average over all latent matrix
entries. Other details are as in Figure 1.
6 Discussion
This paper introduces a principled and effective method for simultaneous conformal inference in
matrix completion. Although primarily motivated by the challenges of uncertainty estimation for
group recommender systems, our approach is sufficiently modular and flexible to be potentially
relevant beyond our initial focus. In particular, the core idea of leveraging a structured calibration
set to approximately replicate the patterns expected at test time could be adapted to obtain joint
inferences beyond the task of predicting multiple user ratings for the same product. Moreover,
our newly introduced notion of leave-one-out exchangeability and the related conformalization
techniques extend the existing framework for conformal inference under covariate shift proposed by
Tibshirani et al. [38] and these advances may be useful in other applications of conformal inference.
A related direction for future research may involve extending our method to accommodate the
jackknife+ framework of Barber et al. [76]. The data-splitting approach adopted in this paper may
not be fully satisfactory in situations where the observations are very limited. In fact, a scarcity
of training data may result in less accurate point estimates, thereby reducing the informativeness
of our inferences, and a scarcity of calibration data generally leads to more unstable outputs. In
contrast, cross-validation can make a more efficient use of the limited data, although at the price
of increased theoretical challenges and more expensive computations.
Software Availability
A software package implementing the methods and numerical experiments described in this paper
is available at https://github.com/ZiyiLiang/simultaneous-matrix-completion.
Acknowledgements
M. S. was partly supported by NSF grant DMS 2210637.
22
Group
cov.
Avg.
widthReferences
[1] H. Scheff´e. “A method for judging all contrasts in the analysis of variance”. In: Biometrika
40.1-2 (1953), pp. 87–110.
[2] S. Roy and R. C. Bose. “Simultaneous confidence interval estimation”. In: The Annals of
Mathematical Statistics (1953), pp. 513–536.
[3] L. A. Goodman. “On simultaneous confidence intervals for multinomial proportions”. In:
Technometrics 7.2 (1965), pp. 247–254.
[4] V. Vovk, A. Gammerman, and G. Shafer. Algorithmic learning in a random world. Vol. 29.
Springer, 2005.
[5] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and L. Wasserman. “Distribution-free predic-
tive inference for regression”. In: J. Am. Stat. Assoc. 113.523 (2018), pp. 1094–1111.
[6] V. Vovk. “Transductive conformal prediction”. In: International Journal on Artificial Intel-
ligence Tools 24.06 (2015), p. 1560001.
[7] L.Quijano-Sanchez,J.A.Recio-Garcia,B.Diaz-Agudo,andG.Jimenez-Diaz.“Happymovie:
A group recommender application in Facebook”. In: Twenty-Fourth International FLAIRS
Conference. 2011.
[8] J. F. McCarthy and T. D. Anagnost. “MusicFX: an arbiter of group preferences for computer
supportedcollaborativeworkouts”.In:Proceedings of the 1998 ACM conference on Computer
supported cooperative work. 1998, pp. 363–372.
[9] D. Herzog, C. Laß, and W. W¨orndl. “Tourrec: a tourist trip recommender system for indi-
viduals and groups”. In: Proceedings of the 12th ACM Conference on Recommender Systems.
2018, pp. 496–497.
[10] J. F. McCarthy. “Pocket restaurantfinder: A situated recommender system for groups”. In:
Workshop on mobile ad-hoc communication at the 2002 ACM conference on human factors
in computer systems. Vol. 8. 2002.
[11] J.P.BaskinandS.Krishnamurthi.“Preferenceaggregationingrouprecommendersystemsfor
committee decision-making”. In: Proceedings of the third ACM conference on Recommender
systems. 2009, pp. 337–340.
[12] A. Jameson and B. Smyth. “Recommendation to groups”. In: The adaptive web: methods and
strategies of web personalization. Springer, 2007, pp. 596–627.
[13] A. Felfernig, L. Boratto, M. Stettinger, M. Tkalˇciˇc, et al. Group recommender systems: An
introduction. Springer, 2018.
[14] S. Dara, C. R. Chowdary, and C. Kumar. “A survey on group recommender systems”. In:
Journal of Intelligent Information Systems 54.2 (2020), pp. 271–295.
[15] I. Zukerman and D. W. Albrecht. “Predictive statistical models for user modeling”. In: User
Modeling and User-Adapted Interaction 11 (2001), pp. 5–18.
[16] G. Adomavicius, S. Kamireddy, and Y. Kwon. “Towards more confident recommendations:
Improving recommender systems using filtering approach based on rating variance”. In: Proc.
of the 17th Workshop on Information Technology and Systems. Citeseer. 2007, pp. 152–157.
[17] M.Zhang,X.Guo,andG.Chen.“Predictionuncertaintyincollaborativefiltering:Enhancing
personalized online product ranking”. In: Decision Support Systems 83 (2016), pp. 10–21.
23[18] T.V.Himabindu,V.Padmanabhan,andA.K.Pujari.“Conformalmatrixfactorizationbased
recommender system”. In: Information Sciences 467 (2018), pp. 685–707.
[19] V. Coscrato and D. Bridge. “Estimating and evaluating the uncertainty of rating predictions
and top-n recommendations in recommender systems”. In: ACM Transactions on Recom-
mender Systems 1.2 (2023), pp. 1–34.
[20] S. K. Lam, D. Frankowski, and J. Riedl. “Do you trust your recommendations? An explo-
ration of security and privacy issues in recommender systems”. In: International conference
on emerging trends in information and communication security. Springer. 2006, pp. 14–29.
[21] J. L. Herlocker, J. A. Konstan, and J. Riedl. “Explaining collaborative filtering recommenda-
tions”. In: Proceedings of the 2000 ACM conference on Computer supported cooperative work.
2000, pp. 241–250.
[22] S.M.McNee,S.K.Lam,C.Guetzlaff,J.A.Konstan,andJ.Riedl.“ConfidenceDisplaysand
Training in Recommender Systems.” In: INTERACT. Vol. 3. Citeseer. 2003, pp. 176–183.
[23] R. Price and P. R. Messinger. “Optimal recommendation sets: Covering uncertainty over user
preferences”. In: AAAI. Vol. 10. 2005, p. 5.
[24] A. Gunawardana and G. Shani. “A survey of accuracy evaluation metrics of recommendation
tasks.” In: Journal of Machine Learning Research 10.12 (2009).
[25] M. Chen. “Exploration in recommender systems”. In: Proceedings of the 15th ACM Confer-
ence on Recommender Systems. 2021, pp. 551–553.
[26] L. M. de Campos, J. M. Fern´andez-Luna, J. F. Huete, and M. A. Rueda-Morales. “Man-
aging uncertainty in group recommending processes”. In: User Modeling and User-Adapted
Interaction 19 (2009), pp. 207–242.
[27] D. Sacharidis. “Modeling Uncertainty in Group Recommendations”. In: Adjunct Publication
of the 27th Conference on User Modeling, Adaptation and Personalization. 2019, pp. 69–74.
[28] F.Ismailoglu.“Aggregatinguserpreferencesingrouprecommendersystems:Acrowdsourcing
approach”. In: Decision Support Systems 152 (2022), p. 113663.
[29] L. Xiao, Z. Min, Z. Yongfeng, G. Zhaoquan, L. Yiqun, and M. Shaoping. “Fairness-aware
group recommendation with pareto-efficiency”. In: Proceedings of the eleventh ACM confer-
ence on recommender systems. 2017, pp. 107–115.
[30] M. O’connor, D. Cosley, J. A. Konstan, and J. Riedl. “PolyLens: A recommender system
for groups of users”. In: ECSCW 2001: Proceedings of the Seventh European conference on
computer supported cooperative work. Springer. 2001, pp. 199–218.
[31] A. Ramlatchan, M. Yang, Q. Liu, M. Li, J. Wang, and Y. Li. “A survey of matrix comple-
tion methods for recommendation systems”. In: Big Data Mining and Analytics 1.4 (2018),
pp. 308–323.
[32] J. D. M. Rennie and N. Srebro. “Fast maximum margin matrix factorization for collaborative
prediction”.In:Proceedingsofthe22ndInternationalConferenceonMachineLearning.ICML
’05. New York, NY, USA: Association for Computing Machinery, 2005, pp. 713–719.
[33] E.Cand`esandB.Recht.“Exactmatrixcompletionviaconvexoptimization”.In:Foundations
of Computational Mathematics 9.6 (2009), pp. 717–772.
[34] E. Cand`es and Y. Plan. “Matrix completion with noise”. In: Proceedings of the IEEE 98.6
(2010), pp. 925–936.
24[35] Y. Gui, R. Barber, and C. Ma. “Conformalized matrix completion”. In: Advances in Neural
Information Processing Systems 36 (2023), pp. 4820–4844.
[36] M. Shao and Y. Zhang. “Distribution-Free Matrix Prediction Under Arbitrary Missing Pat-
tern”. In: arXiv preprint arXiv:2305.11640 (2023).
[37] F. M. Harper and J. A. Konstan. “The MovieLens Datasets: History and Context”. In: ACM
Trans. Interact. Intell. Syst. 5.4 (Dec. 2015).
[38] R. J. Tibshirani, R. Foygel Barber, E. Cand`es, and A. Ramdas. “Conformal prediction under
covariate shift”. In: Advances in neural information processing systems 32 (2019).
[39] E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series
of lectures. Vol. 33. US Government Printing Office, 1954.
[40] P. S. Laplace. “M´emoire sur la probabilit´e de causes par les ´evenements”. In: M´emoire de
l’acad´emie royale des sciences (1774).
[41] A.MnihandR.R.Salakhutdinov.“Probabilisticmatrixfactorization”.In:Advancesinneural
information processing systems 20 (2007).
[42] R. Salakhutdinov and A. Mnih. “Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo”. In: Proceedings of the 25th International Conference on Machine Learn-
ing. ICML ’08. Helsinki, Finland: Association for Computing Machinery, 2008, pp. 880–887.
[43] Y. Chen, J. Fan, C. Ma, and Y. Yan. “Inference and uncertainty quantification for noisy
matrix completion”. In: Proceedings of the National Academy of Sciences 116.46 (2019),
pp. 22931–22937.
[44] D. Xia and M. Yuan. “Statistical inferences of linear forms for noisy matrix completion”. In:
J. R. Stat. Soc. B 83.1 (2021), pp. 58–77.
[45] V. Farias, A. A. Li, and T. Peng. “Uncertainty quantification for low-rank matrix completion
with heterogeneous and sub-exponential noise”. In: International Conference on Artificial
Intelligence and Statistics. PMLR. 2022, pp. 1179–1189.
[46] V. Vovk, A. Gammerman, and C. Saunders. “Machine-Learning Applications of Algorithmic
Randomness”. In: Proceedings of the Sixteenth International Conference on Machine Learn-
ing. ICML ’99. 1999, pp. 444–453.
[47] J. Lei and L. Wasserman. “Distribution-free prediction bands for non-parametric regression”.
In: J. R. Stat. Soc. B 76.1 (2014), pp. 71–96.
[48] Y. Romano, E. Patterson, and E. Cand`es. “Conformalized quantile regression”. In: Advances
in neural information processing systems 32 (2019), pp. 3538–3548.
[49] M. Sesia and Y. Romano. “Conformal prediction using conditional histograms”. In: Advances
in Neural Information Processing Systems 34 (2021), pp. 6304–6315.
[50] J. Lei, J. Robins, and L. Wasserman. “Distribution-free prediction sets”. In: J. Am. Stat. As-
soc. 108.501 (2013), pp. 278–287.
[51] Y. Romano, M. Sesia, and E. Cand`es. “Classification with valid and adaptive coverage”. In:
Advances in Neural Information Processing Systems 33 (2020), pp. 3581–3591.
[52] Z.Liang,M.Sesia,andW.Sun.“Integrativeconformalp-valuesforout-of-distributiontesting
with labelled outliers”. In: J. R. Stat. Soc. B (Jan. 2024), qkad138.
25[53] J. Smith, I. Nouretdinov, R. Craddock, C. Offer, and A. Gammerman. “Conformal anomaly
detection of trajectories with a multi-class hierarchy”. In: Statistical Learning and Data Sci-
ences: Third International Symposium, SLDS 2015, Egham, UK, April 20-23, 2015, Proceed-
ings 3. Springer. 2015, pp. 281–290.
[54] S. Bates, E. Cand`es, L. Lei, Y. Romano, and M. Sesia. “Testing for outliers with conformal
p-values”. In: Ann. Stat. 51.1 (2023), pp. 149–178.
[55] A. Marandon, L. Lei, D. Mary, and E. Roquain. “Adaptive novelty detection with false
discovery rate guarantee”. In: Ann. Stat. 52.1 (2024), pp. 157–183.
[56] J.Lei,A.Rinaldo,andL.Wasserman.“Aconformalpredictionapproachtoexplorefunctional
data”. In: Annals of Mathematics and Artificial Intelligence 74 (2015), pp. 29–43.
[57] L. Lei and E. Cand`es. “Conformal inference of counterfactuals and individual treatment
effects”. In: J. R. Stat. Soc. B 83.5 (2021), pp. 911–938.
[58] M.Sesia,S.Favaro,andE.Dobriban.“Conformalfrequencyestimationusingdiscretesketched
data with coverage for distinct queries”. In: Journal of Machine Learning Research 24.348
(2023), pp. 1–80.
[59] I.GibbsandE.Cand`es.“Adaptiveconformalinferenceunderdistributionshift”.In:Advances
in Neural Information Processing Systems 34 (2021), pp. 1660–1672.
[60] Y.Zhou,L.Lindemann,andM.Sesia.“ConformalizedAdaptiveForecastingofHeterogeneous
Trajectories”. In: arXiv preprint arXiv:2402.09623 (2024).
[61] A. N. Angelopoulos and S. Bates. “Conformal Prediction: A Gentle Introduction”. In: Foun-
dations and Trends in Machine Learning 16.4 (2023), pp. 494–591.
[62] A. Podkopaev and A. Ramdas. “Distribution-free uncertainty quantification for classification
under label shift”. In: Uncertainty in artificial intelligence. PMLR. 2021, pp. 844–853.
[63] R. F. Barber, E. Cand`es, A. Ramdas, and R. J. Tibshirani. “Conformal prediction beyond
exchangeability”. In: Ann. Stat. 51.2 (2023), pp. 816–845.
[64] B.-S. Einbinder, S. Bates, A. N. Angelopoulos, A. Gendler, and Y. Romano. “Conformal
prediction is robust to label noise”. In: arXiv preprint arXiv:2209.14295 2 (2022).
[65] M. Sesia, Y. Wang, and X. Tong. “Adaptive conformal classification with noisy labels”. In:
arXiv preprint arXiv:2309.05092 (2023).
[66] K. T. Wallenius. “Biased sampling: the noncentral hypergeometric probability distribution”.
In: PhD thesis. Department of Statistics, Stanford University. 1963.
[67] J.Chesson.“Anon-centralmultivariatehypergeometricdistributionarisingfrombiasedsam-
pling with application to selective predation”. In: Journal of Applied Probability 13.4 (1976),
pp. 795–797.
[68] Y. Romano, R. F. Barber, C. Sabatti, and E. Cand`es. “With malice toward none: Assessing
uncertainty via equalized coverage”. In: Harvard Data Science Review 2.2 (2020), p. 4.
[69] R. Foygel Barber, E. Cand`es, A. Ramdas, and R. J. Tibshirani. “The limits of distribution-
free conditional predictive inference”. In: Information and Inference: A Journal of the IMA
10.2 (2021), pp. 455–482.
[70] R. Sun and Z.-Q. Luo. “Guaranteed matrix completion via non-convex factorization”. In:
IEEE Transactions on Information Theory 62.11 (2016), pp. 6535–6579.
26[71] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. “Autorec: Autoencoders meet collaborative
filtering”. In: Proceedings of the 24th international conference on World Wide Web. 2015,
pp. 111–112.
[72] J. Fan and T. Chow. “Deep learning based matrix completion”. In: Neurocomputing 266
(2017), pp. 540–549.
[73] V. Vovk. “Conditional validity of inductive conformal predictors”. In: Asian conference on
machine learning. PMLR. 2012, pp. 475–490.
[74] M. Sesia and E. Cand`es. “A comparison of some conformal quantile regression methods”. In:
Stat 9.1 (2020).
[75] Y. Hu, Y. Koren, and C. Volinsky. “Collaborative filtering for implicit feedback datasets”.
In: 2008 Eighth IEEE international conference on data mining. Ieee. 2008, pp. 263–272.
[76] R. F. Barber, E. . Cand`es, A. Ramdas, and R. J. Tibshirani. “Predictive inference with the
jackknife+”. In: Ann. Stat. 49.1 (2021), pp. 486–507.
[77] R. W. Butler. Saddlepoint Approximations with Applications. Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge University Press, 2007.
[78] A. Fog. “Calculation Methods for Wallenius’ Noncentral Hypergeometric Distribution”. In:
Communications in Statistics - Simulation and Computation 37.2 (2008), pp. 258–273.
[79] K. E. Atkinson. An Introduction to Numerical Analysis. Second. New York: John Wiley &
Sons, 1989.
[80] M. Cauchois, S. Gupta, and J. C. Duchi. “Knowing what you know: valid confidence sets in
multiclass and multilabel prediction”. In: Stat 1050 (2020), p. 24.
27A1 Additional Technical Background
A1.1 Review of Individual-Level Conformalized Matrix Completion
This section reviews the conformalized matrix completion method proposed by Gui, Barber, and
Ma [35], which is designed to produce confidence intervals for one missing entry at a time.
The setup of Gui, Barber, and Ma [35] is similar to ours as they also treat M as fixed and
assume the randomness in the matrix completion problem comes from the observation process or,
equivalently, the missingness mechanism. However, their modeling choices do not match exactly
with ours. Specifically, they assume that each matrix entry in row r and column c is independently
observed with some (known) probability p , which roughly corresponds to our sampling weights
r,c
w in (1); i.e., D := {(r,c) ∈ [n ]×[n ] : Z = 1}, where
r,c obs r c r,c
Z = I[(r,c) is observed] in ∼d. Bernoulli(p ), ∀(r,c) ∈ [n ]×[n ]. (A36)
r,c r,c r c
Therefore, the total number of observed entries is a random variable in Gui, Barber, and Ma [35],
whereas we can allow n to be fixed within the sampling without replacement model defined
obs
in (1). As shown in this paper, our modeling choice is natural when aiming to construct group-level
simultaneous inferences. The model assumed by Gui, Barber, and Ma [35] also differs from ours in
its requirement that all sampling weights must be strictly positive; p > 0 for all (r,c) ∈ [n ]×[n ]
r,c r c
in (A36). Further, the approach of Gui, Barber, and Ma [35] differs from ours in that they assume
the missing matrix index of interest, namely I∗ ∈ [n ]×[n ], to be sampled uniformly at random
r c
from D , that is, I∗ ∼ Uniform(D ), where D := [n ] × [n ] \ D . By contrast, our
miss miss miss r c obs
sampling model for the test groups, defined in (3), can accommodate heterogeneous weights w∗.
ThemethodproposedbyGui, Barber, andMa[35]constructsconformalconfidenceintervalsfor
individual missing entries as follows. First, D = {(r,c) ∈ [n ]×[n ] : Z = 1} is partitioned into
obs r c r,c
a training set D
train
and a disjoint calibration set D
cal
by randomly sampling Z(cid:101)r,c ∼ Bernoulli(q)
independently for all (r,c) ∈ [n ]×[n ], for some fixed parameter q ∈ (0,1), and then defining
r c
D
train
:= {(r,c) ∈ D
obs
: Z(cid:101)r,c = 1}, D
cal
:= {(r,c) ∈ D
obs
: Z(cid:101)r,c = 0}. (A37)
Similar to us, Gui, Barber, and Ma [35] utilize D
train
to compute M(cid:99), leveraging any black-box
algorithm, and then evaluate conformity scores on the calibration data as explained below.
Let C(·,τ,M(cid:99)) denote a pre-specified prediction rule for a single matrix entry, which should be
monotonicallyincreasinginthetuningparameterτ ∈ [0,1]asexplainedinSection3.1; forexample,
this could correspond to the prediction rule defined in (8) in the special case of K = 1. For any
I ∈ [n r]×[n c], let S
i
= S(I i) denote the conformity score corresponding to C(·,τ,M(cid:99)), as in (9).
Imagining that the calibration set contains the indices of n matrix entries—D = {I ,...,I }—
cal 1 n
the method of Gui, Barber, and Ma [35] evaluates S = S(I ) for all i ∈ [n] and then calibrates the
i i
tuning parameter τ by computing
(cid:32) n (cid:33)
(cid:88)
τindiv = Q 1−α; pindivδ +pindivδ , (A38)
(cid:98)α,1 i Si n+1 ∞
i=1
28where the conformalization weights pindiv are given by
i
R 1−p
pindiv(I ,...,I ) = Ii , R = Ii, (A39)
i 1 n+1 (cid:80)n j+1R
Ij
Ii p
Ii
for all i ∈ [n+1], with the convention that I = I∗. Finally, the confidence interval for the latent
n+1
value of M at index I∗ is given by:
C(cid:98)indiv(I∗;M
D
obs,α) = C(I∗,τ (cid:98)αin ,1div,M(cid:99)). (A40)
The following result establishes that the confidence intervals defined in (A40) have guaranteed
marginal coverage at level 1−α.
Proposition A3 (from Gui, Barber, and Ma [35]). Suppose D is sampled according to (A36)
obs
and I∗ ∼ Uniform(D miss). Then, for any α ∈ (0,1), C(cid:98)indiv(I∗;M
D
,α) in (A40) satisfies
obs
(cid:104) (cid:105)
P M
I∗
∈ C(cid:98)indiv(I∗;M
D
,α) | D
train
≥ 1−α.
obs
Proof. This result follows directly from the proof of Theorem 3.2 in Gui, Barber, and Ma [35].
Alternatively, the following proof can be obtained by applying our Lemma 2. Conditioning on n,
such that D = {I ,...,I }, note that the joint distribution of I ,...,I ,I∗ trivially satisfies the
cal 1 n 1 n
leave-one-out exchangeability condition defined in Lemma 2. Specifically, let I ,...,I ,I be a
1 n n+1
realization of I ,...,I ,I∗, so that D := {I ,...,I } is a realization of D sampled according
1 n cal 1 n cal
to (A37). Then,
P(I = I ,...,I = I ,I∗ = I | D ,|D | = n ,|D | = n)
1 1 n n n+1 train obs obs cal
= P(D = D ,I∗ = I | D ,|D | = n ,|D | = n)
cal cal n+1 train obs obs cal
(cid:89) p r,c
= g(D ∪{I })· ,
cal n+1
1−p
r,c
(r,c)∈D
cal
where the second equality follows from Lemma 3.1 in Gui, Barber, and Ma [35], for a suitable
function g that is invariant to any permutation of its input. Further, it follows that
P(I = I ,...,I = I ,I∗ = I | D ,|D | = n ,|D | = n)
1 1 n n n+1 train obs obs cal
 
= g(D cal∪{I n+1})·
(cid:89) p r,c
·
1−p In+1
1−p p
(r,c)∈D cal∪{In+1}
r,c In+1
 
(cid:89) p r,c
= g(D cal∪{I n+1})·
1−p
·R In+1,
r,c
(r,c)∈D cal∪{In+1}
with R defined as in (A39). This proves that I ,...,I ,I∗ are leave-one-out exchangeable
In+1 1 n
random variables by the definition in (16), with h¯(D ,I ) = R . Therefore, the coverage
cal n+1 In+1
guarantee of Proposition A3 follows directly from Lemma 2.
29A1.2 Limitations of the Unadjusted and Bonferroni Baselines
It is not easy to construct informative simultaneous confidence regions satisfying (5) and, to the
best of our knowledge, there are no satisfactory alternatives to the method proposed in this paper.
Infact, standardconformalmethodsaredesignedtodealwithonetestpointatatime, anddirectly
aggregatingseparatepredictionintervalsintoajointconfidenceregionisneitherprecisenorefficient
in our context, as explained in more detail below.
Recall that the conformalized matrix completion method of Gui, Barber, and Ma [35], reviewed
in Appendix A1.1, is designed to construct a confidence interval C(cid:98)indiv(X 1∗;M
X
obs,α) for one
missing entry at a time, denoted as X∗, such that
1
P[M X 1∗ ∈ C(cid:98)indiv(X 1∗;M X obs,α)] ≥ 1−α (A41)
under a suitable sampling model for X and X∗. The model for X and X∗ considered by
obs 1 obs 1
Gui, Barber, and Ma [35] is different from ours, as they treat n as random, rely on independent
obs
Bernoulli observations insteadof sampling withoutreplacement, and donot consider thepossibility
thatthesamplingweightsw∗ in(3)maybenon-uniform. However,asimilarideacanbeadaptedto
constructconfidenceintervalsC(cid:98)indiv(X 1∗;M
X
obs,α)forasinglematrixentryX 1∗ underoursampling
model (1)–(3), as explained in Appendix A1.3. In any case, regardless of these modeling details,
the limitations of the baseline approaches within our simultaneous inference context can already
be understood as follows.
IfthegoalistomakejointpredictionsforagroupofK matrixentries,concatenatingindividual-
level predictions clearly does not guarantee simultaneous coverage in the sense of (5), as the errors
acrossdifferentcoordinatestendtoaccumulate. Thismaybeseenasaninstanceoftheprototypical
multiple testing problem. The unadjusted baseline approach essentially computes:
(cid:16) (cid:17)
C(cid:98)Unadj(X∗;M
X
obs,α) := C(cid:98)indiv(X 1∗;M
X
obs,α),...,C(cid:98)indiv(X K∗ ;M
X
obs,α) . (A42)
As demonstrated by Figure 5 and other synthetic experiments in Section 5.1, this approach often
leads to low simultaneous coverage.
Figure 1 previewed the performance of a second baseline approach that relies on a simple but
inefficient Bonferroni correction to approximately ensure simultaneous coverage. Intuitively, this
tries to (conservatively) account for the multiplicity of the problem by applying (A42) at level α/K
instead of α, computing
(cid:16) (cid:16) α(cid:17) (cid:16) α(cid:17)(cid:17)
C(cid:98)Bonf(X∗;M
X
obs,α) := C(cid:98)indiv X 1∗;M
X
obs,
K
,...,C(cid:98)indiv X K∗ ;M
X
obs,
K
. (A43)
Although a Bonferroni correction may seem reasonable at first sight, it is still unsatisfactory for
at least two reasons. Firstly, it is not rigorous because we know the K missing entries indexed
by X∗ must belong to the same column, but this constraint cannot be easily taken into account
by individual-level predictions. Secondly, and even more crucially, the Bonferroni correction tends
to be overly conservative in practice because the coverage events M X k∗ ∈ C(cid:98)indiv(X k∗;M X obs,α)
for different values of k ∈ [K] are mutually dependent, since they are all affected by the same
observations X . These dependencies, however, are potentially very complex.
obs
30A1.3 Implementation Details for the Baselines
To facilitate the empirical comparison with our method, which relies on the sampling model for
X and X∗ defined (1)–(3), in this paper we apply the unadjusted and Bonferroni baseline
obs
approaches described in Appendix A1.2 based on individual-level conformal prediction intervals
C(cid:98)indiv obtainedasfollows. Insteadofdirectlyapplyingtheconformalizedmatrixcompletionmethod
of Gui, Barber, and Ma [35], we repeatedly apply our own method separately for each element X∗
k
in X∗ = (X∗,X∗,...,X∗ ), imagining each time that we are dealing with a trivial group of size 1.
1 2 K
This provides us with individual-level prediction intervals C(cid:98)indiv that are similar in spirit to those
of Gui, Barber, and Ma [35] but whose construction more faithfully mirrors the sampling model
assumedinthispaper(althoughtheystillignoretheconstraintthatallelementsofX∗ mustbelong
to the same column). In summary, the implementation of the unadjusted and Bonferroni baseline
approaches applied in this paper is outlined by Algorithms A3 and A4, respectively.
Algorithm A3 Unadjusted confidence region for multiple missing matrix entries
1: Input: partially observed matrix M X obs, with unordered list of observed indices D obs;
2: Input: test group X∗; nominal coverage level α ∈ (0,1);
3: Input: any matrix completion algorithm producing point estimates;
4: Input: any prediction rule C satisfying (6) and (7);
5: Input: desired number n of calibration entries.
6: Apply Algorithm 2 with group size K = 1 to obtain D train, D cal;
7: Compute a point estimate M(cid:99), looking only the observations in D train.
8: Compute the conformity scores S i, for all i ∈ [n], with Equation (9).
9: for all k ∈ [K] do
10: Compute τ (cid:98)α,1 in (10), based on the weights p i given by (19) with x n+1 = X k∗ in Section 3.4.
11: Compute C(cid:98)indiv(X k∗;M X obs,α) = C(cid:98)(X k∗;M X obs,α) given by (11).
12: end for
13: Output: Joint confidence region C(cid:98)Unadj(X∗;M X ,α) given by Equation (A42).
obs
Algorithm A4 Bonferroni-style confidence region for multiple missing matrix entries
1: Input: partially observed matrix M X obs, with unordered list of observed indices D obs;
2: Input: test group X∗; nominal coverage level α ∈ (0,1);
3: Input: any matrix completion algorithm producing point estimates;
4: Input: any prediction rule C satisfying (6) and (7);
5: Input: desired number n of calibration entries.
6: Apply Algorithm 2 with group size K = 1 to obtain D train, D cal;
7: Compute a point estimate M(cid:99), looking only the observations in D train.
8: Compute the conformity scores S i, for all i ∈ [n], with Equation (9).
9: for all k ∈ [K] do
10: Compute τ (cid:98)α,1 in (10), based on the weights p i given by (19) with x n+1 = X k∗ in Section 3.4.
K
11: Compute C(cid:98)indiv(X k∗;M X obs, Kα) = C(cid:98)(X k∗;M X obs, Kα) given by (11).
12: end for
13: Output: Joint confidence region C(cid:98)Bonf(X∗;M X ,α) given by Equation (A43).
obs
31A1.4 Review of the Classical Laplace Method
This section provides a concise review of the classical version of Laplace’s method, as detailed for
example in Butler [77]. This method is a powerful tool for approximating analytically intractable
integrals of the form (cid:82)b enf(x)h(x)dx, where the function f is sufficiently well-behaved and smooth,
a
with a unique global maximum at an interior point x ∈ (a,b), the function h is positive and does
0
not vary significantly near x , and n is a relatively large constant. The method hinges on the
0
principle that this integral’s value is predominantly determined by a small region around the point
wheref achievesitsmaximum. Thisideaisexplainedinmoredetailandmotivatedpreciselybelow.
Let f(x) be a twice continuously differentiable function on an interval (a,b), and assume there
exists a unique global maximum at an interior point x ∈ (a,b), such that f(x ) = max f(x)
0 0 x∈(a,b)
and f′′(x ) < 0. Suppose h is a function that varies slowly around x and is such that h(x) > 0 for
0 0
all x ∈ (a,b). Then, Laplace’s approximation involves replacing the integral (cid:82) h(x)enf(x)dx with
(cid:115)
(cid:90) b 2π
enf(x)h(x)dx ≈ enf(x0)h(x ) . (A44)
0 −nf′′(x )
a 0
Astandardmathematicaljustificationforthisapproximationstartsbyprovingthat, undersuitable
technical assumptions on f and h in the spirit of the intuitive conditions outlined above,
(cid:82)b enf(x)h(x)dx
lim a = 1. (A45)
(cid:113)
n→∞ enf(x0)h(x 0) n(−f2 ′π
′(x0))
The classical proof of (A45) consists of three high-level steps:
1. Local second-order approximation: Approximate f(x) near x using a second-order
0
Taylor expansion: f(x) ≈ f(x )+ 1f′′(x )(x−x )2.
0 2 0 0
2. Integral transformation: Standardize the quadratic term in the integral to apply results
from Gaussian integral analysis.
3. Asymptotic evaluation: Assess the integral in the standardized coordinates to achieve the
asymptotic equivalence in (A45).
The proof of Theorem A4, presented in Appendix A4.6, follows a similar high-level strategy,
although its details are significantly more involved due to the fact that our integral of interest
in (24) cannot be directly written as (cid:82)b enf(x)h(x)dx for some functions f,g.
a
A2 Additional Methodological Details
A2.1 Practical Computation of the Conformity Scores
As detailed in Section 3.1, our method allows flexibility in the choice of the prediction rule C, which
uniquely determines the conformity scores. In this section, we explore three practical options for
the prediction rules and their respective conformity scores.
32A2.1.1 Hyper-Cubic Confidence Regions
An intuitive prediction rule, introduced in Section 3.1, is:
(cid:18) (cid:19)
τ τ
C(x∗,τ,M(cid:99)) = M(cid:99)x∗ ± ,...,M(cid:99)x∗ ± ,
1 1−τ K 1−τ
with the parameter τ taking value in [0,1]. Note that this rule leads to hyper-cubic confidence
regions, with constant widths for all users in a group.
The conformity scores corresponding to this rule can be written explicitly, for any i ∈ [n], as:
 

|M(cid:99)
Xcal
−M Xcal| |M(cid:99)
Xcal
−M Xcal|

S = max i,1 i,1 ,..., i,K i,K .
i
1+|M(cid:99)
Xcal
−M Xcal| 1+|M(cid:99)
Xcal
−M Xcal|
i,1 i,1 i,K i,K
Remark. The function x (cid:55)→ x/(1+x) is an strictly increasing function on x ≥ 0. Therefore,
we can equivalently define the prediction set as the following. Let
(cid:110) (cid:111)
S(cid:101)i = max |M(cid:99)
Xcal
−M Xcal|,...,|M(cid:99)
Xcal
−M Xcal| (A46)
i,1 i,1 i,K i,K
and define the alternate confidence set as
(cid:16) (cid:17)
C′(x∗,τ,M(cid:99)) = M(cid:99)x∗ ±τ,...,M(cid:99)x∗ ±τ ,
1 K
with τ taking value in [0,+∞). The expression in (A46) is more closely related to the typical
notation in the conformal inference literature; e.g., see Lei et al. [5].
A2.1.2 Hyper-Rectangular Confidence Regions
An alternative type of prediction rule, yielding intervals of varying lengths for different users,
involves scaling the hyper-cube defined in (A2.1.1). This modification may be particularly useful in
applicationsinvolvingcountdatawithwideranges, wherethevariancemaybeexpectedtoincrease
in proportion to the observed values. We define this linearly-scaled prediction rule as
(cid:16) (cid:17)
C(x∗,τ,M(cid:99)) = M(cid:99)x∗ ±|M(cid:99)x∗|τ,...,M(cid:99)x∗ ±|M(cid:99)x∗ |τ , (A47)
1 1 K K
which leads to confidence regions in the shape of a hyper-rectangle. The corresponding scores are:
 
|M(cid:99)
Xcal
−M Xcal| |M(cid:99)
Xcal
−M Xcal|

S = max i,1 i,1 ,..., i,K i,K .
i
 |M(cid:99) Xcal| |M(cid:99) Xcal| 
i,1 i,K
A2.1.3 Hyper-Spherical Confidence Regions
Thepredictionrulesdescribedaboveallresultinconfidenceregionswithahyper-rectangularshape.
Alternatively,onecanconstructaconfidenceregionwithahyper-sphericalshapeusingthefollowing
33prediction rule, where ∥·∥ represents the Euclidean norm:
2
(cid:110) (cid:111)
C(x∗,τ,M(cid:99)) = x ∈ RK : ∥x−M(cid:99) Xcal∥
2
≤ τ . (A48)
i
The corresponding conformity scores are
(cid:110) (cid:111)
S
i
:= inf τ ∈ R : ∥M
Xcal
−M(cid:99) Xcal∥
2
≤ τ = ∥M
Xcal
−M(cid:99) Xcal∥ 2.
i i i i
Note that replacing the Euclidean norm with the max norm in (A48) recovers the hyper-cubic the
prediction rule.
The concept of a hyper-spherical confidence region is quite rare in the conformal inference
literature, where the majority of existing methods focus on constructing confidence sets for a single
testpointindividually. However,whenaimingtoprovidesimultaneouscoverageformultipleentries,
it becomes possible to develop confidence regions of varying geometric shapes.
A2.2 Efficient Evaluation of the Conformalization Weights
We discuss in more detail here the choice of scaling parameter h for the function Φ(τ;h) defined in
Equation(23)(Section4.1). Thisfreeparametercontrolsthelocationofτ = argmax Φ(τ;h).
h τ∈(0,1)
Since our Laplace approximation hinges on τ being not too close to the integration boundary, an
h
intuitive and effective choice is to set h so that τ = 1/2; e.g., see Fog [78]. We explain below how
h
to achieve this using the Newton-Raphson algorithm.
Recall Lemma 4, which tells us that the function Φ(τ;h) has a unique stationary point with
respecttoτ atsomevalueτ ∈ (0,1),andthatthisstationarypointisaglobalmaximum. Therefore,
h
since the function Φ(τ;h) is smooth, the Newton-Raphson algorithm can be applied as follows to
find a value of h > 0 such that τ = 1/2. Define
h
z(h) :=
ϕ′( 21;h)
= δ−
1
−
(cid:88) w r,c
,
2h h 2hwr,c −1
(r,c)∈D
obs
and note that this function is monotonically increasing in h. Then it suffices to find h such that
z(h) = 0. Note that z(h) is smooth, and z′(h) > 0, z′′(h) < 0 for h ∈ [1/δ,∞). It is also clear that
the solution of z(h) = 0 must be greater than 1/δ. Further, z(h) has a unique root in the interval
[1/δ,∞) because z(1/δ) < 0 and lim z(h) = δ > 0.
h→∞
Thus, it follows from Theorem 2.2 in Atkinson [79] that the Newton-Raphson algorithm will
convergetotherootτ quadratically,foranystartingpointwithintheinterval[1/δ,∞). Inpractice,
h
one can choose 1/δ as the starting point.
The time complexity of the Newton-Raphson iteration depends on the desired precision level.
If the tolerable error is a predetermined small constant, the iteration terminates after a constant
number of updates due to quadratic convergence. Evaluating z(h) andz′(h) at any given hrequires
O(n ). Hence, solving z(h) = 0 takes O(n ).
obs obs
34A2.3 Computational Shortcuts and Complexity Analysis
A2.3.1 Evaluation of the Conformalization Weights
Evaluating the simplified weights p¯ in (29) only involves arithmetic operations and can be carried
i
out for all i ∈ [n+1] at a total computational cost roughly of order O(n n +nK). To understand
r c
this, first note that computing η (τ ) defined in (25), for all i ∈ [n+1] with any given τ and h,
i h h
has cost O(nK); and finding the correct value of τ and h according to (26) has cost O(n ), or
h obs
equivalently no worse than O(n n ), as explained in Section A2.2.
r c
Next, evaluating w∗ ·...·w∗ for all i ∈ [n+1] has cost O(n n +nK), because the constant
(cid:80) w∗ in th(cid:101) ex di,1 enomin(cid:101) ax ti o,K rs of (20) and (21) can be pre-cr omc puted at cost O(n n ), while
(r,c)∈D¯
miss
r,c r c
the remaining terms in (20) and (21) can be evaluated at cost O(K) separately for each i ∈ [n+1].
Thecostofevaluatingthetermwithinthesquarebracketsin(29)foralli ∈ [n+1]isO(n +nK).
r
This is achieved by pre-computing factorials up to n since nc is upper-bounded by n for any
r obs r
c ∈ [n ]. Then for each i ∈ [n + 1], computing binomial coefficients, given the pre-computed
c
factorials, takes constant time, and the remaining term in the brackets requires O(K). Putting
everything together, the conformalization weights in (29) has cost O(n +nK) for all i ∈ [n+1].
r
A2.3.2 Cost Analysis of Algorithm 1
Analysis for a single test group. The cost of computing a confidence region for a single test
group is O(T +n n +n(K +logn)), as shown below.
r c
• Training the black-box matrix completion model has cost O(T).
• After the black-box model is trained, the cost of computing scores S for all i ∈ [n] is O(nK).
i
• The cost of computing p for all i ∈ [n+1] is O(n n +nK), as explained in Section A2.3.1.
i r c
• After the conformalization weights are computed, the cost of computing τ is O(nlogn).
(cid:98)α,K
This is because sorting the scores S for all i ∈ [n] has a worst-time cost of O(nlogn), while
i
it takes O(n) to find the weighted quantile based on p and the sorted scores.
i
Therefore, the overall cost is O(T +n n +n(K +logn)).
r c
Analysis for m distinct test groups. Thecostofcomputingconfidenceregionsformdistinct
test groups is O(T +n n +n(logn+mK)), as shown below.
r c
• Training the black-box matrix completion model has cost O(T), since the model only needs
to be trained once.
• The cost of computing conformity scores S for all i ∈ [n] is O(nK), since the calibration
i
groups are the same for any new test group.
• Thecostofcomputingp foralli ∈ [n+m]isO(n n +mnK), asexplainedinSectionA2.3.1.
i r c
• After the conformalization weights are computed, the cost of computing the confidence sets
for all m test groups is O(nlogn+nm). Sorting the scores S for all i ∈ [n] has a worst-time
i
cost of O(nlogn), which only needs to be performed once. For any j ∈ [m], it takes O(n) to
find the weighted quantile given weights {p ,...,p ,p } and the sorted scores.
1 n n+j
Therefore, the overall cost is O(T +n n +n(logn+mK)).
r c
35A2.3.3 Cost Analysis of Algorithm 2
• For each column, the cost of computing mc := nc −⌊nc /K⌋ < K is O(n ), and the cost
obs obs r
of sampling mc indices uniformly at random is O(K). Hence the cost of sampling the pruned
indices for all columns is O(n (n +K)), which simplifies to O(n n ) by the fact that K < n .
c r c r r
• Initializing D given the pruned indices D has cost of O(n n ).
avail prune c r
• After D is initialized, the cost of sampling the ith calibration group (and updating D
avail cal
and D ) is O(K), for each i ∈ [n]. Hence sampling all n calibration groups takes O(nK).
avail
Therefore, Algorithm2hastimecomplexityofO(n n +nK), anditdoesnotneedtoberepeatedly
c r
applied when dealing with distinct groups involving the same matrix.
A2.4 Estimation of the Sampling Weights
We describe here a method, inspired by Gui, Barber, and Ma [35], to estimate empirically the
sampling weights w for our sampling model in (1), leveraging the available matrix observations
indexedbyD . Ingeneral,thisestimationproblemismadefeasiblebyintroducingtheassumption
obs
that w has some lower-dimensional structure that can be summarized for example by a parametric
model. The approach suggested by Gui, Barber, and Ma [35] assumes that the weight matrix
w ∈ Rnr×nc is low-rank. For simplicity, we follow the same approach here, although our framework
could also accommodate alternative estimation techniques in situations where different modeling
assumptions about w may be justified.
Suppose the sampling weights follow the parametric model
(cid:18) (cid:19)
w
r,c
log = A ,
r,c
1−w
r,c
whereA ∈ Rnr×nc isamatrixwithrankρandboundedinfinitynorm; i.e.,||A||
∞
≤ ν,forsomepre-
defined constant ν ∈ R. Then, if each matrix entry (r,c) is independently observed (i.e., included
in D ) with probability w , i.e.,
obs r,c
I[(r,c) ∈ D ] in ∼d. Bernoulli(w ), (A49)
obs r,c
then the log-likelihood of A can be written as
(cid:88) (cid:88)
L (A) = log(l(A ))+ log(1−l(A )), (A50)
D r,c r,c
obs
(r,c)∈D
obs
(r,c)∈[nr]×[nc]\D
obs
where l(t) = (1+exp(−t))−1. This suggests estimating A by solving
A(cid:98)= argmaxL
D
(A)
obs
A∈Rnr×nc
√
subject to: ∥A∥ ≤ ν ρn n ,
∗ r c
∥A∥ ≤ ν,
∞
where ∥·∥
∗
is the nuclear norm. Finally, having obtained A(cid:98), the estimated sampling weights w
(cid:98)r,c
36for each (r,c) ∈ [n ]×[n ] are given by
r c
w
(cid:98)r,c
= 1/(1+exp(A(cid:98)r,c)). (A51)
In practice, the numerical experiments described in this paper apply this estimation procedure
using the default choices of the parameters ρ and ν suggested by Gui, Barber, and Ma [35].
It is worth remarking that the independent Bernoulli observation model (A49) underlying this
maximum-likelihood estimation approach differs from the weighted sampling without replacement
model (1) that we utilize to calibrate our simultaneous conformal inferences. This discrepancy,
however, is both useful and unlikely to cause issues, as explained next. On the one hand, sampling
without replacement model is essential to capture the structured nature of our group-level test case
X∗ andofthecalibrationgroupsXcal,...,Xcal. Ontheotherhand,samplingwithoutreplacement
1 n
would make the likelihood function in (A50) intractable, unnecessarily hindering the estimation
process. Fortunately, however, the interpretation of the sampling weights w remains largely
r,c
consistent across the models (1) and (A49), which justifies the use of the estimated weights w
(cid:98)r,c
in (A51) for the purpose of calibrating conformal inferences under the model defined in (1).
A3 Additional Empirical Results
A3.1 Additional Experiments with Synthetic Data
A3.1.1 Heterogeneous Test Sampling Weights
This section describes experiments in which the test group X∗ is sampled according to a model (3)
withheterogeneousweightsw∗. AsexplainedinSection2,theheterogeneousnatureoftheseweights
makes it feasible to ensure valid coverage conditional on interesting features of X∗. Therefore,
the following experiments demonstrate the ability of our method to smoothly interpolate between
marginal and conditional coverage guarantees, giving practitioners flexibility to up-weight or down-
weight different types of test cases, as needed.
The ground-truth matrix M ∈ R400×400 is generated according to the random factorization
model defined in Equation (33), with rank l = 4. We observe n = 48,000 entries of this matrix,
obs
sampled based on the model in (1) with uniform weights w; these are indexed by D , whose
obs
complementisD = [n ]×[n ]\D . Algorithm1isthenappliedasinthepreviousexperiments,
miss r c obs
using n = min{2000,⌊ξ /2⌋} calibration groups and allocating the remaining n = n −Kn
obs train obs
observations for training. For the latter purpose, we rely on the usual alternating least square
approach, with hypothesized rank 4, and thus obtain a point estimate M(cid:99) and its corresponding
factor matrices U(cid:98) ∈ Rnr×l and V(cid:98) ∈ Rnc×l, such that M(cid:99) = U(cid:98)(V(cid:98))⊤.
Theweightsw∗ forX∗ in(3)arebasedonanoracleprocedurethatleveragesperfectknowledge
of M and M(cid:99) to construct a sampling process that over-represents portions of the matrix for which
the point estimate is less accurate. This process is controlled by a parameter δ ∈ (0,1], which
determinestheheterogeneityofw∗. Inthespecialcaseofδ = 1,thetestweightsbecomew∗ = 1for
r,c
allmatrixentries,recoveringtheexperimentalsetupconsideredearlierinSection5.1.1. Bycontrast,
smaller values of δ tend to increasingly over-sample portions of the matrix for which the point
estimate M(cid:99) is less accurate. We refer to Appendix A3.1.2 for details about this construction of the
test sampling weights, which gives rise to an interesting and particularly challenging experimental
37setting in which attaining high coverage is intrinsically difficult.
To highlight the importance of correctly accounting for the heterogeneous nature of the test
sampling weights w, in these experiments we compare the performance of joint confidence regions
obtainedwithtwoalternativeapproaches. ThefirstapproachconsistsofapplyingAlgorithm1based
on the correct values of the data-generating weights w and w∗. The second approach consists of
applying Algorithm 1 based on the correct values of the data-generating weights w but incorrectly
specified weights w∗ = 1 for all r ∈ [n ],c ∈ [n ]. In both cases, the nominal significance level
r,c r c
is α = 10%, and the methods are evaluated based on 100 random test groups sampled from
D \D , according to the model in (3) with the weights w∗ defined in Equation (A54) within
miss wse
Appendix A3.1.2. All results are averaged over 300 independent experiments.
FigureA6comparestheperformancesofthetwoaforementionedimplementationsofourmethod
as a function of the group size K, for different values of the parameter δ. The results show that our
methodappliedwiththecorrectweightsw∗ alwaysachievesthedesired90%simultaneouscoverage,
as predicted by the theory. By contrast, using mis-specified uniform test sampling weights w∗ leads
to lower coverage than expected, especially for lower values of the parameter δ. Figure A7 provides
analternativebutqualitativelyconsistentviewofthesefindings,varyingtheparameterδ separately
for different values of the group size K.
δ: 0.12 δ: 0.16 δ: 0.2
0.91
0.90
0.89
0.88 Method
0.87
Conditional
3.0
Marginal
2.5
2.0
1.5
1 2 3 4 1 2 3 4 1 2 3 4
Group size K
FigureA6: PerformanceofSCMConsimulateddatawithheterogeneoustestweights. Twoalterna-
tiveimplementationsofSCMCarecompared: onebasedonthecorrectheterogeneoustestsampling
weights, and one based on mis-specified homogeneous weights. The results are shown as a function
of the group size K, for different values of the parameter δ which controls the heterogeneity of the
sampling weights. The performance is measured in terms of the empirical simultaneous coverage
(top) and the average widthof theconfidence regions (bottom). The nominalcoverage levelis 90%.
It is interesting to note from Figures A6 and A7 that our method is sometimes slightly over-
conservative when applied with highly heterogeneous test sampling weights w∗ (corresponding
to small values of the parameter δ). This phenomenon is due to the unavoidable challenge of
constructing valid confidence regions in the presence of strong distribution shifts, and it can be
understood more precisely as follows. Smaller values of δ result in a stronger distribution shift
between the observed data in D and X∗, increasing the likelihood that the weighted empirical
obs
quantile τ defined in (10) might become infinite, leading to trivially wide confidence regions.
(cid:98)α,K
In those (relatively rare) cases in which τ diverges, to avoid numerical issues we simply set
(cid:98)α,K
τ equal to S , the highest calibration conformity score. Fortunately, as shown explicitly in
(cid:98)α,K (n)
Figure A8, this issue is not very common (it is observed in fewer than 2.5% of the cases), which
38
Group
cov.
Avg.
widthK: 2 K: 3 K: 4
0.91
0.90
0.89
0.88 Method
0.87
Conditional
3.00
2.75 Marginal
2.50
2.25
2.00
Test group homogeneity
Figure A7: Performance of SCMC on simulated data with heterogeneous test weights. The results
are shown as a function of the heterogeneity parameter δ, for different group sizes K. Other details
are as in Figure A6.
explains why our method appears to be only slightly over-conservative in Figures A6 and A7.
δ: 0.12 δ: 0.16 δ: 0.2
0.91
0.90
0.89
0.88
0.87
3.0
Method
2.5
Conditional
2.0
Marginal
1.5
0.025
0.020
0.015
0.010
0.005
0.000
1 2 3 4 1 2 3 4 1 2 3 4
Group size K
Figure A8: Performance of SCMC on simulated data with heterogeneous sampling weights for the
test group. The bottom panel reports the empirical probability that our method produces trivially
wide confidence regions for a random test group. Other details are as in Figure A6.
A3.1.2 Additional Details for Appendix A3.1.1
The sampling weights w∗ for X∗ utilized in the experiments of Appendix A3.1.1 are defined based
on the following oracle procedure, which leverages perfect knowledge of M and M(cid:99) to construct
a sampling process that over-represents portions of the matrix for which the point estimate is
less accurate. This gives rise a particularly challenging experimental setting. For each entry
(r,c) ∈ [n r]×[n c], define the latent feature vector y
r,c
:= (U(cid:98)r◦,V(cid:98)c◦) ∈ R2l, where U(cid:98)r◦ and V(cid:98)c◦ are
39
21.0 41.0 61.0 81.0 02.0 21.0 41.0 61.0 81.0 02.0 21.0 41.0 61.0 81.0 02.0
Group
cov.
Avg.
width
Group
cov.
Avg.
width
Inf
prop.the r-th row of U(cid:98) and the c-th row of V(cid:98)c◦, respectively. Let also D
wse
⊂ [n r]×[n c] denote a subset
containing 25% of the matrix indices in D , chosen uniformly at random.
miss
The values of M and M(cid:99) indexed by D
wse
are utilized by the oracle to construct w∗ with an
approach inspired by Cauchois, Gupta, and Duchi [80] and Romano, Sesia, and Cand`es [51]. For
any fixed δ ∈ (0,1], define the worst-slab estimation error,
 
(cid:80)
WSE(M(cid:99);δ,D wse) = sup

(r,c)∈S
v,a,b|M(cid:99)r,c−M r,c| s.t.|S v,a,b|
≥
δ
, (A52)
|S | |D |
v∈R2l,a<b∈R v,a,b wse 
where, for any v ∈ R2l and a < b ∈ R, the subset S ⊂ D is defined as
v,a,b wse
S = {(r,c) ∈ D : a ≤ v⊤y ≤ b}. (A53)
v,a,b wse r,c
Intuitively, S is a subset (or slab) of the matrix entries in D characterized by a direction v
v,a,b wse
in the latent feature space and two scalar thresholds a < b. Accordingly, WSE(M(cid:99);δ,D wse) is the
average absolute residual between M and M(cid:99) evaluated for the entries within S v,a,b, after selecting
the worst-case subset S containing at least a fraction δ of the observations within D .
v,a,b wse
In practice, the optimal (worst-case) choice of v in (A52) is approximated by fitting an ordinary
least square regression model to predict the absolute residuals {|M(cid:99)r,c −M r,c|}
(r,c)∈Dwse
as a linear
function of the latent features {y } . Then, the corresponding optimal values of a∗,b∗ in
r,c (r,c)∈Dwse
(A52) are approximated through a grid search, for a fixed value of the parameter δ.
Finally, the test sampling weights w∗ = (cid:8) w∗ (cid:9) are given by
r,c (r,c)∈[nr]×[nc]

normpdf(v∗⊤y ,a∗,σ2)
  r,c , v∗⊤y < a∗
   normpdf(a∗,a∗,σ2) r,c

w∗ = 1, (r,c) ∈ S , (A54)
r,c v∗,a∗,b∗
 normpdf(v∗⊤y ,b∗,σ2)
    normpdf(b∗,r b,c ∗,σ2) , v∗⊤y r,c > b∗,
where normpdf(·,a,σ2) denotes the density function of the Gaussian distribution with mean a
and variance σ2. This density function is introduced for smoothing purposes, setting σ = (b∗ −
a∗)/5. These sampling weights enable us to select test groups from indices that predominantly fall
within the worst-slab region for which M(cid:99) estimates M least accurately. Intuitively, attaining valid
coverage for this portion of the matrix should be especially challenging.
A3.2 Investigation of the Coverage Upper Bound
Inthissection,weinvestigateinmoredetailtheuppercoverageboundforourmethodestablishedby
Theorem 2, which is equal to 1−α+E[max p (Xcal,...,Xcal,X∗)]. Ideally, a small expected
i∈[n+1] i 1 n
value in this equation would guarantee that our conformal inferences are not too conservative.
However, given that it would be unfeasible to evaluate this expected value analytically, we rely on
a Monte Carlo numerical study.
WebeginbyfocusingongroupsofsizeK = 2andconsiderforsimplicitymatriceswithanequal
number of rows and columns; i.e., n = n = 200. We simulate the observation process by sampling
r c
40n = 0.2·n n matrix entries without replacement according to the model defined in (1), with
obs r c
w = (n (c−1)+r)s, ∀r ∈ [n ], c ∈ [n ],
r,c r r c
withascalingparameters = 2. Notethatthisisthesamechoiceofsamplingweightsutilizedinthe
experiments of Section 5.1.2. For simplicity, the test group X∗ is sampled from the model defined
in (3) using weights w∗ exhibiting the same patterns as w. Then, the conformalization weights p
i
for all i ∈ [n+1] are computed by applying Algorithm 1, and varying the number n of calibration
groups as a control parameter. Finally, we estimate E[max p (Xcal,...,Xcal,X∗)] by taking
i∈[n+1] i 1 n
the empirical average of max p (Xcal,...,Xcal,X∗) over 10 independent experiments.
i∈[n+1] i 1 n
0.06 Calibration 0.04 Calibration
Structured Structured
Exchangeable Exchangeable
0.03
0.04
0.02
0.02
0.01
50 100 150 200 250 300 2 3 4 5 6 7 8
Number of calibration groups Group size K
Figure A9: Monte Carlo investigation of the theoretical coverage upper bound for the structured
confidence regions computed by Algorithm 1. We plot E[max p (Xcal,...,Xcal,X∗)], which
i∈[n+1] i 1 n
is the difference between the upper bound established by Theorem 2 and the desired coverage level
1−α. The results are compared to the curve 1/n, which corresponds to the standard upper bound
inflation factor corresponding to conformal inferences under exchangeability. Left: the results are
shown as a function of the number of calibration groups n, for K = 2. Right: the results are shown
as a function of the calibration group size K, for a structured calibration set of cardinality n = 400.
Figure A9 [left] reports on the results of these experiments as a function of n. The results show
that our coverage upper bound approaches 1−α roughly at rate 1/n, as one would generally expect
in the case of standard conformal inferences based on exchangeable data [4]. This is consistent with
our empirical observations that Algorithm 1 is typically not too conservative in practice. Figure A9
[right] reports on the results of additional experiments in which the group size K is varied, while
keeping the number of calibration groups fixed to n = 400. The results shows that the coverage
upper bound tends to become more conservative as the group size increases, reflecting the intrinsic
higher difficulty of producing valid simultaneous conformal inferences for larger groups.
A3.3 Additional Results for Section 5.1.1
41
noitalfni
dnuob
reppU
noitalfni
dnuob
reppUK: 2 K: 5 K: 8
1.0 1.0 1.0
0.9
0.9 0.9
0.8
0.8 0.8 0.7 Method
0.7 0.6 Simultaneous
0.7
40 Bonferroni
3.0 30
30 Unadjusted
2.5 20
20
2.0 10 10
0 10 20 30 0 10 20 30 0 10 20 30
Column−wise dependency
Figure A10: Performance of SCMC and two baselines on simulated data, as a function of the
column-wise noise parameter µ and of the group size K. Other details are as in Figure 3.
A3.4 Additional Results for Section 5.1.2
s: 0.6 s: 0.8 s: 1
1.0 1.0 1.0
0.9 0.9 0.9
0.8
0.8 Method
0.7 0.8
0.7
0.6 0.7 Simultaneous
0.5 3 5 Bonferroni
0.4 4
0.3 2 3 Unadjusted
0.2 1 2
0.1 1
0.0 0 0
2 4 6 8 2 4 6 8 2 4 6 8
Group size K
Figure A11: Performance of simultaneous conformal inference and benchmarks on simulated data,
as a function of the group size K, for different scaling parameter s. Other details are as in Figure 4.
A3.5 Additional Results for Section 5.2
Guessed rank: 3 Guessed rank: 5 Guessed rank: 7
Method
3.5
Simultaneous
3.0
Bonferroni
2.5
Unadjusted
2 4 6 8 2 4 6 8 2 4 6 8
Group size K
Figure A12: Performance of simultaneous conformal inference and benchmarks on the MovieLens
100K data. The unadjusted baseline is designed to achieve valid coverage for a single user at a time,
and is generally invalid for a group of K ≥ 2 users. The output confidence regions are assessed in
terms of their average width (lower is better), as a function of the number of users K in the group.
Other details are as in Figure 1.
42
Group
cov.
Avg.
width
Group
cov.
Avg.
width
Avg.
widthA4 Mathematical Proofs
A4.1 A General Quantile Inflation Lemma
Proof of Lemma 1. The proof follows the same strategy as that of Tibshirani et al. [38]. Let E de-
z
notetheeventthat{Z ,...,Z } = {z ,...,z },forsomepossiblerealizationz = (z ,...,z )
1 n+1 1 n+1 1 n+1
ofZ ,...,Z ,andletv = S(z ,z )foralli ∈ [n+1]. Bythedefinitionofconditionalprobability,
1 n+1 i i −i
for each i ∈ [n+1],
(cid:80)
f(z ,...,z )
P{V = v | E } = P{Z = z | E } = σ:σ(n+1)=i σ(1) σ(n+1) = pf(z ,...,z ),
n+1 i z n+1 i z (cid:80) f(z ,...,z ) i 1 n+1
σ σ(1) σ(n+1)
where σ is a permutation of [n+1]. In other words,
n+1
V | E ∼ (cid:88) pf(z ,...,z )δ ,
n+1 z i 1 n+1 vi
i=1
where δ denotes a point mass at v . This implies that
vi i
(cid:26) (cid:18) n+1 (cid:19) (cid:27)
P V ≤ Quantile β; (cid:88) pf(z ,...,z )δ | E ≥ β,
n+1 i 1 n+1 vi z
i=1
which is equivalent to
(cid:26) (cid:18) n+1 (cid:19) (cid:27)
P V ≤ Quantile β; (cid:88) pf(Z ,...,Z )δ | E ≥ β.
n+1 i 1 n+1 Vi z
i=1
Finally, marginalizing over E leads to
z
(cid:26) (cid:18) n+1 (cid:19)(cid:27)
P V ≤ Quantile β; (cid:88) pf(Z ,...,Z )δ ≥ β.
n+1 i 1 n+1 Vi
i=1
This is equivalent to the desired result because, by Lemma A5,
(cid:18) n (cid:19)
V ≤ Quantile β; (cid:88) pf(Z ,...,Z )δ +pf (Z ,...,Z )δ
n+1 i 1 n+1 Vi n+1 1 n+1 Vn+1
i=1
⇐⇒
(cid:18) n (cid:19)
V ≤ Quantile β; (cid:88) pf(Z ,...,Z )δ +pf (Z ,...,Z )δ .
n+1 i 1 n+1 Vi n+1 1 n+1 ∞
i=1
Lemma A5 (also appearing implicitly in Tibshirani et al. [38]). Consider n+1 random variables
V ,...,V and some weights p ,...,p such that p > 0 and
(cid:80)n+1p
= 1. Then, for any
1 n+1 1 n+1 i i=1 i
43β ∈ (0,1),
n+1 n
(cid:0) (cid:88) (cid:1) (cid:0) (cid:88) (cid:1)
V ≤ Q β; p δ ⇐⇒ V ≤ Q β; p δ +p δ .
n+1 i Vi n+1 i Vi n+1 ∞
i=1 i=1
Proof of Lemma A5. This result was previously utilized by Tibshirani et al. [38] and a proof is
included here for completeness. It is straightforward to establish one direction of the result, namely
n+1 n
(cid:0) (cid:88) (cid:1) (cid:0) (cid:88) (cid:1)
V ≤ Q β; p δ =⇒ V ≤ Q β; p δ +p δ ,
n+1 i Vi n+1 i Vi n+1 ∞
i=1 i=1
because, almost surely, V ≤ ∞, and hence
n+1
n n+1
(cid:0) (cid:88) (cid:1) (cid:0) (cid:88) (cid:1)
Q β; p δ +p δ ≥ Q β; p δ .
i Vi n+1 ∞ i Vi
i=1 i=1
To prove the other direction, suppose V >
Q(cid:0) β;(cid:80)n+1p
δ
(cid:1)
. By definition of the quantile
n+1 i=1 i Vi
function, we can write without loss of generality that
Q(cid:0) β;(cid:80)n+1p
δ
(cid:1)
= V , where j ∈ [n+1] is
i=1 i Vi (j)
defined such that
p +...+p ≥ β, p +...+p < β,
(1) (j) (1) (j−1)
where p ≤ ...p are the order statistics of p ,...,p . Therefore, V > V , and re-
(1) (n+1) 1 n+1 n+1 (j)
assigning V → ∞ does not change V . This means that
Q(cid:0) β;(cid:80)n+1p
δ
(cid:1)
=
Q(cid:0) β;(cid:80)n
p δ +
p δ
(cid:1)
,
len a+ d1
ing to V >
Q(cid:0) β;(cid:80)n ( pj)
δ +p δ
(cid:1)
. Thus, we
hi= av1
e
si hV oi
wn that
i=1 i Vi
n+1 ∞ n+1 i=1 i Vi n+1 ∞
n+1 n
(cid:0) (cid:88) (cid:1) (cid:0) (cid:88) (cid:1)
V > Q β; p δ =⇒ V > Q β; p δ +p δ .
n+1 i Vi n+1 i Vi n+1 ∞
i=1 i=1
Proof of Lemma 2. LetE denotetheeventthat{Z ,...,Z } = {z ,...,z },forsomepossible
z 1 n+1 1 n+1
realization z = (z ,...,z ) of Z ,...,Z , and let v = S(z ,z ) for all i ∈ [n+1]. As in the
1 n+1 1 n+1 i i −i
proof of Lemma 1, for each i ∈ [n+1],
(cid:80)
f(z ,...,z )
P{V = v | E } = P{Z = z | E } = σ:σ(n+1)=i σ(1) σ(n+1) .
n+1 i z n+1 i z (cid:80)
f(z ,...,z )
σ σ(1) σ(n+1)
44Further, because Z ,...,Z are also leave-one-out exchangeable,
1 n+1
(cid:80) (cid:80)
f(z ,...,z ) g(z ,...,z )·h(z ,...,z )
σ:σ(n+1)=i σ(1) σ(n+1) σ:σ(n+1)=i σ(1) σ(n+1) σ(1) σ(n+1)
=
(cid:80) (cid:80)
f(z ,...,z ) g(z ,...,z )·h(z ,...,z )
σ σ(1) σ(n+1) σ σ(1) σ(n+1) σ(1) σ(n+1)
(cid:80) g(z ,...,z )·h¯(z ,z )
σ:σ(n+1)=i 1 n+1 −i i
=
(cid:80) g(z ,...,z )·h¯(z ,z )
σ 1 n+1 −σ(n+1) σ(n+1)
(cid:80) h¯(z ,z )
σ:σ(n+1)=i −i i
=
(cid:80)n+1(cid:80) h¯(z ,z )
j=1 σ:σ(n+1)=j −j j
n!h¯(z ,z )
−i i
= = p (z ,...,z ),
n!(cid:80)n+1h¯(z
,z )
i 1 n+1
j=1 −j j
which implies V | E ∼
(cid:80)n+1p
(z ,...,z )δ . The rest of the proof then follows with the
n+1 z i=1 i 1 n+1 vi
same approach as the proof of Lemma 1.
A4.2 Conformal Inference with Structured Calibration
Proof of Proposition 1. This result is a direct consequence of Proposition A4, which characterizes
the joint distribution of (Xcal,...,Xcal,X∗) conditional on D and D . It is easy to see
1 n prune train
from (A56) that this distribution is invariant to permutations of Xcal,...,Xcal.
1 n
Proposition A4. Consider the same setting of Proposition 1. Let D and D ⊆ D denote
1 0 1
arbitrary realizations of D and D , respectively. Let x ,x ,...,x ,x be any sequence of
train prune 1 2 n n+1
n+1 K-groups involving elements of D such that
obs
(cid:104) (cid:105)
P Xcal = x ,Xcal = x ,...,Xcal = x ,X∗ = x | D = D ,D = D > 0.
1 1 2 2 n n n+1 prune 0 train 1
Define also D = ∪ {xk}, the unordered collection of matrix entries indexed by the groups
2 i∈[n],k∈[K] i
x ,...,x , noting that D = D ∪D . Further, define
1 n obs 1 2
D¯c := {(r′,c′) ∈ D¯ | c′ = c}, ∀c ∈ [n ],
miss miss c
n¯ := |D \D |,
obs obs prune
(A55)
n¯c := |{(r′,c′) ∈ D \D : c′ = c}|, ∀c ∈ [n ],
obs obs prune c
Nc := |{i ∈ [n] : c = x = x = ... = x }|, ∀c ∈ [n ].
n i,1,2 i,2,2 i,K,2 c
Intuitively, D¯c represents the pruned missing indices in column c, n¯c is the number of indices
miss obs
in D \D corresponding to entries in column c, while Nc is the number of calibration groups
obs prune n
45in column c. Note that all quantities in (A55) are uniquely determined by D and D . Then,
0 2
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x ,X∗ = x | D = D ,D = D
1 1 n n n+1 prune 0 train 1
 
w∗ K w∗
= (cid:80) (r,c)∈xn D¯+ m1 i, s1
sw r∗
,c
· k(cid:89)
=2
(cid:80)
(r,c)∈D¯
missw r∗ ,cI[c
=x xn n+ +1 1,k
,1,2]−(cid:80)k k− ′=1 1w x∗
n+1,k′
 
(A56)
1 (cid:89) 1
·P(D = D ∪D )· · 
obs 1 2 P(D = D ,D = D ) (cid:16)nc (cid:17)
prune 0 train 1 obs
c∈[nc] n¯c
obs
(cid:32) (cid:89)n
1
(cid:33) (cid:89)nc (cid:89)N nc K (cid:89)−1
1
· · .
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1j=1 k=1 obs
Proof of Proposition A4. First, note that
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x ,X∗ = x | D = D ,D = D
1 1 n n n+1 prune 0 train 1
(cid:16) (cid:17)
= P X∗ = x | D = D ,D = D ,Xcal = x ,...,Xcal = x
n+1 prune 0 train 1 1 1 n n
(cid:16) (cid:17)
·P Xcal = x ,...,Xcal = x | D = D ,D = D
1 1 n n prune 0 train 1
= P(X∗ = x | D = D ∪D )
n+1 obs 1 2
(A57)
(cid:16) (cid:17)
·P Xcal = x ,...,Xcal = x | D = D ,D = D
1 1 n n prune 0 train 1
 
w∗ K w∗
= (cid:80) (r,c)∈xn D¯+ m1 i, s1
sw r∗
,c
· k(cid:89)
=2
(cid:80)
(r,c)∈D¯
missw r∗ ,cI[c
=x xn n+ +1 1,k
,1,2]−(cid:80)k k− ′=1 1w x∗
n+1,k′
(cid:16) (cid:17)
·P Xcal = x ,...,Xcal = x | D = D ,D = D ,
1 1 n n prune 0 train 1
where the first term on the right-hand-side above was written explicitly using the sequential sam-
pling characterization of Ψcol in (4).
46Next, we focus on the second term on the right-hand-side of (A57):
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x | D = D ,D = D
1 1 n n prune 0 train 1
P(cid:0) Xcal = x ,...,Xcal = x ,D = D ,D = D (cid:1)
= 1 1 n n prune 0 train 1
P(D = D ,D = D )
prune 0 train 1
P(cid:0) Xcal = x ,...,Xcal = x ,D = D ,D = D ,D = D ∪D (cid:1)
= 1 1 n n prune 0 train 1 obs 1 2
P(D = D ,D = D )
prune 0 train 1
(cid:16) (cid:17)
= P Xcal = x ,...,Xcal = x ,D = D | D = D ,D = D ∪D
1 1 n n train 1 prune 0 obs 1 2
P(D = D ,D = D ∪D )
prune 0 obs 1 2
=·
P(D prune = D 0,D train = D 1) (A58)
(cid:16) (cid:17)
= P Xcal = x ,...,Xcal = x | D = D ,D = D ∪D
1 1 n n prune 0 obs 1 2
(cid:16) (cid:17)
=·P D = D | Xcal = x ,...,Xcal = x ,D = D ,D = D ∪D
train 1 1 1 n n prune 0 obs 1 2
P(D = D ,D = D ∪D )
prune 0 obs 1 2
=·
P(D = D ,D = D )
prune 0 train 1
(cid:16) (cid:17)
= P Xcal = x ,...,Xcal = x | D = D ,D = D ∪D
1 1 n n prune 0 obs 1 2
P(D = D ,D = D ∪D )
prune 0 obs 1 2
=· ,
P(D = D ,D = D )
prune 0 train 1
wherethelastequalityabovefollowsfromthefactthatD isuniquelydeterminedbyX ,...,X ,
train 1 n
D and D .
prune obs
The first term on the right-hand-side of (A58) is given by Lemma A6:
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x | D ,D
1 1 n n prune obs
(cid:32) (cid:89)n
1
(cid:33) (cid:89)nc (cid:89)N nc K (cid:89)−1
1
(A59)
= · .
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1j=1 k=1 obs
Note that (A59) implies that, conditional on D and D , the distribution of Xcal,...,Xcal
obs prune 1 n
does not depend on the order of these calibration groups.
Next, we focus on the second term on the right-hand-side of (A58), namely
P(D = D ,D = D ∪D )
prune 0 obs 1 2
. (A60)
P(D = D ,D = D )
prune 0 train 1
47The numerator of (A60) is
P(D = D ,D = D ∪D )
prune 0 obs 1 2
= P(D = D ∪D )·P(D = D | D = D ∪D )
obs 1 2 prune 0 obs 1 2
(cid:89) 1
= P(D = D ∪D )·
obs 1 2 (cid:16)nc (cid:17)
(A61)
obs
c∈[nc] mc
(cid:89) 1
= P(D = D ∪D )· .
obs 1 2 (cid:16)nc (cid:17)
obs
c∈[nc] n¯c
obs
where mc := nc mod K denotes the remainder of the integer division nc /K , and n¯c =
obs obs obs
⌊nc /K⌋ = nc − mc. Above, the denominator does not need to be simplified because it only
obs obs
depends on D and D .
0 1
Finally, combining (A57), (A58), (A59), (A60), and (A61), we arrive at:
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x ,X∗ = x | D = D ,D = D
1 1 n n n+1 prune 0 train 1
 
w∗ K w∗
= (cid:80) (r,c)∈xn D¯+ m1 i, s1
sw r∗
,c
· k(cid:89)
=2
(cid:80)
(r,c)∈D¯
missw r∗ ,cI[c
=x xn n+ +1 1,k
,1,2]−(cid:80)k k− ′=1 1w x∗
n+1,k′
 
1 (cid:89) 1
·P(D = D ∪D )· · 
obs 1 2 P(D = D ,D = D ) (cid:16)nc (cid:17)
prune 0 train 1 obs
c∈[nc] n¯c
obs
(cid:32) (cid:89)n
1
(cid:33) (cid:89)nc (cid:89)N nc K (cid:89)−1
1
· · .
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1j=1 k=1 obs
Lemma A6. Under the same setup as in Proposition A4,
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x | D ,D
1 1 n n prune obs
(cid:32) (cid:89)n
1
(cid:33) (cid:89)nc (cid:89)N nc K (cid:89)−1
1
(A62)
= · .
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1j=1 k=1 obs
Proof of Lemma A6. Weprovethisresultbyinductiononthenumberofcalibrationgroups, n. For
ease of notation, we will denote the column of the i-th calibration group as c , for any i ∈ [n]; that
i
48is, c = x for all k ∈ [K]. In the base case where n = 2,
i i,k,2
(cid:16) (cid:17)
P Xcal = x ,Xcal = x | D = D ,D = D
1 1 2 2 prune 0 obs 1
1 1 1 1
= · ·...· ·
n¯ n¯c1 −1 n¯c1 −K +1 n¯ −K
obs obs obs obs
(cid:20)(cid:18) (cid:19) (cid:18) (cid:19) (cid:21)
1 1 1 1
· ·...· 1{c ̸= c }+ ·...· 1{c = c }
n¯c2 −1 n¯c2 −K +1 1 2 n¯c2 −K −1 n¯c2 −2K +1 1 2
obs obs obs obs
(cid:34) (cid:89)2
1
(cid:35) (cid:89)nc (cid:89)N nc K (cid:89)−1
1
= · .
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1j=1 k=1 obs
Now, for the induction step, suppose Equation (A62) holds for n−1. Then,
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x | D = D ,D = D
1 1 n n prune 0 obs 1
(cid:16) (cid:17)
= P Xcal = x ,...,Xcal = x | D = D ,D = D
1 1 n−1 n−1 prune 0 obs 1
1 1 1
· · ·...·
n¯ −K(n−1) n¯cn −K(Ncn −1)−1 n¯cn −KNn +1
obs obs n obs cn
(cid:34)n (cid:89)−1
1
(cid:35) (cid:89)nc N (cid:89)nc −1K (cid:89)−1
1
= ·
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1 j=1 k=1 obs
K−1
1 (cid:89) 1
· ·
n¯ −K(n−1) n¯cn −K(Ncn −1)−k
obs k=1 obs n
(cid:34) (cid:89)n
1
(cid:35) (cid:89)nc (cid:89)N nc K (cid:89)−1
1
= · .
n¯ −K(i−1) n¯c −K(j −1)−k
i=1 obs c=1j=1 k=1 obs
where the last equality above follows because N nc
−1
= N nc for all c ̸= c n, while N ncn = N ncn −1+1.
A4.3 Characterization of the Conformalization Weights
Proof of Lemma 3. Recall from Proposition A4 that
(cid:16) (cid:17)
P Xcal = x ,...,Xcal = x ,X∗ = x | D = D ,D = D
1 1 n n n+1 prune 0 train 1
= g({x ,...,x })·h¯({x ,...,x },x ),
1 n+1 1 n n+1
for some permutation-invariant function g and
(cid:34) K (cid:35)
(cid:89)
h¯({x ,...,x },x ) = P(D = D ∪D )· w∗ · w∗
1 n n+1 obs 1 2 (cid:101)xn+1,1 (cid:101)x
n+1,k
k=2
  (A63)
(cid:89) 1
(cid:89)nc (cid:89)N nc K (cid:89)−1
1
· · ,
 (cid:16)nc (cid:17) n¯c −K(j −1)−k
c∈[nc] n¯o cbs c=1j=1 k=1 obs
obs
49with
w∗
w∗ = xn+1,1 ,
(cid:101)xn+1,1 (cid:80) w∗
(r,c)∈D¯
miss
r,c
and, for all k ∈ {2,...,K}.,
w∗
w∗ = x n+1,k .
(cid:101)x n+1,k (cid:80) w∗ I[c = x ]−(cid:80)k−1 w∗
(r,c)∈D¯ miss r,c n+1,1,2 k′=1 x n+1,k′
Therefore, Lemma 2 can be applied, with weights proportional to
p (x ,...,x ) ∝ h¯({x ,...,x }\{x },x ). (A64)
i 1 n+1 1 n+1 i i
In order to compute the right-hand-side of (A64), one must understand how (A63) changes
when x is swapped with x , for any fixed i ∈ [n]. This can be done easily, one piece at a time.
n+1 i
To begin, it is immediate to see that swapping x with x results in P(D = D ∪D ) be-
n+1 i obs 1 2
ing replaced by P (D = D ). Similarly, w∗ ,...,w∗ are replaced by w∗ ,...,w∗ ,
w obs obs;i (cid:101)xn+1,1 (cid:101)xn+1,K (cid:101)xi,1 (cid:101)xi,K
defined as
w∗
w∗ = xi,1 ,
(cid:101)xi,1 (cid:80) w∗
(r,c)∈D¯
miss;i
r,c
and, for all k ∈ {2,...,K},
w∗
w∗ = x n+1,k
(cid:101)x i,k (cid:80) w∗ I[c = x ]−(cid:80)k−1 w∗
(r,c)∈D¯ miss;i r,c i,1,2 k′=1 x i,k′
w∗
x
= i,k ,
(cid:80) (r,c)∈D¯ mci iss;iw r∗ ,c−(cid:80)k k− ′=1 1w x∗
i;k′
here, for any i ∈ [n+1], c denotes the column to which x belongs; i.e., c := x ,∀k ∈ [K],
i i i i,k,2
where x is the column of the kth entry in x .
i,k,2 i
To understand the notation in the equations above, recall that D¯ = {(r,c) ∈ D | nc ≥
miss miss miss
K} is a realization of the pruned missing set D¯ , and nc = |{(r′,c′) ∈ D | c′ = c}| is the
miss miss miss
number of missing entries in column c. In the parallel universe where x is swapped with x , the
n+1 i
realization of the missing indices is denoted as D , and the realization of the pruned missing set
miss;i
is D¯ := {(r,c) ∈ D | nc ≥ K}, where nc := |{(r′,c′) ∈ D } : c′ = c|. Similarly,
miss;i miss;i miss;i miss;i miss;i
D¯c := {(r′,c′) ∈ D¯ : c′ = c} denotes entries belonging to column c in the imaginary
miss;i miss;i
pruned missing set. Thus, w∗ and w∗ can be interpreted as normalized sampling weights for
(cid:101)xi,1 (cid:101)x
i,k
the imaginary test group x .
i
Next, let nc and nc denote the numbers of observations in column c from the sets D
obs obs;i obs
and D , respectively. Define also n¯c = ⌊nc /K⌋ and n¯c = ⌊nc /K⌋, the corresponding
obs;i obs obs obs;i obs;i
numbers of observations remaining in column c after the random pruning step of Algorithm 2. Let
Nc := |{i ∈ [n] : c = c}| denote the number of calibration groups in column c ∈ [n ]. Similarly,
n i c
let Nc denote the corresponding imaginary quantity obtained by swapping the calibration group
n;i
50Xcal with the test group X∗; i.e.,
i
Nc := |{j ∈ [n+1]\{i} : c = c}|
n;i j
Further, swapping x with x results in nc , n¯c , and Nc being replaced by nc , n¯c ,
n+1 i obs obs n obs;i obs;i
and Nc , respectively. Therefore,
n;i
p (x ,...,x ,x ) ∝ h¯({x ,...,x }\{x },x )
i 1 n n+1 1 n+1 i i
∝
(cid:32)
w (cid:101)x∗
i,1
(cid:89)K
w (cid:101)x∗
i,k(cid:33)(cid:32) (cid:89)nc
(cid:16)n n¯c
o
cbs;i(cid:17)−1(cid:33) (cid:89)nc N (cid:89)nc ;iK (cid:89)−1
n¯c
−K1
(j
−1)−k
·P w(D
obs
= D obs;i).
k=2 c=1 obs;i c=1j=1 k=1 obs;i
(A65)
Now, we will further simplify the expression in Equation (A65) to facilitate the practical com-
putation of these weights.
Consider the first term on the right-hand-side of Equation (A65), namely
(cid:32) K (cid:33)
(cid:89)
w∗ w∗ .
(cid:101)xi,1 (cid:101)x
i,k
k=2
This quantity depends on the pruned set of missing indices D¯ and, by definition,
miss;i
w∗ w∗
w∗ = xi,1 = xi,1 ,
(cid:101)xi,1 (cid:80) w∗ K (cid:16) (cid:17)
(r,c)∈D¯ miss;i r,c (cid:80) w∗ − (cid:80) w∗ −w∗ +u∗
(r,c)∈D¯
miss
r,c x
n+1,k
x
i,k
xi,1
k=1
where
  
 
u∗ xi,1 = I[c i ̸= c n+1] I(cid:2) nc mi iss < K(cid:3)  (cid:88) w r∗ ,c−I(cid:2) nc mn i+ ss1 < 2K(cid:3)  (cid:88) w r∗ ,c  ,
(r,c)∈D mci
iss
(r,c)∈D mcn is+ s1\xn+1
while, for all k ∈ {2,...,K},
w∗
w∗ = x i,k
(cid:101)x i,k (cid:80) (r,c)∈D¯ mci iss;iw r∗ ,c−(cid:80)k k− ′=1 1w x∗
i;k′
w∗
x
= i,k
 
K (cid:18) K (cid:19)
(r,c)(cid:80)
∈D¯ci
w r∗ ,c+ k(cid:80) ′=kw x∗ i,k′ −I[c i = c n+1] k(cid:80) ′=1w x∗ n+1,k′ +I(cid:2) nc mi iss < K(cid:3)  (r,c)(cid:80)
∈Dci
w r∗ ,c
miss miss
w∗
x
= i,k .
K (cid:18) K (cid:19)
(cid:80) w∗ + (cid:80) w∗ −I[c = c ] (cid:80) w∗
(r,c)∈Dci
r,c
k′=k
x i,k′ i n+1
k′=1
x n+1,k′
miss
In the equations above, with a slight abuse of notation, we denoted the set of missing indices in
column c excluding those in the group x as Dcn+1 \x := Dcn+1 \{x }K .
n+1 n+1 miss n+1 miss n+1,k k=1
51Next, let us consider the second term on the right-hand-side of Equation (A65), namely
(cid:32) (cid:89)nc
(cid:16)nc
(cid:17)−1(cid:33)
obs;i .
n¯c
obs;i
c=1
This evaluates the probability of observing a particular realization of D . Since the pruned
prune
indices are chosen uniformly at random, this quantity only depends on the number of observations
within each column before and after pruning, namely, nc and n¯c . By definition, we have
obs;i obs;i

nc −KI[c ̸= c ], c = c ,
  obs i n+1 i
nc = nc +KI[c ̸= c ], c = c , (A66)
obs;i obs i n+1 n+1

nc , otherwise.
obs
The above equivalence from the fact that swapping x with x only affects the observed indices
i n+1
in column c and c , while all other indices remain the same. In particular, upon swapping,
i n+1
column c will contain K fewer observations, because x is treated as the unobserved test group,
i i
and column c will contain K more observations, because x is treated as the calibration
n+1 n+1
group. Similarly,

n¯c −KI[c ̸= c ], c = c ,
  obs i n+1 i
n¯c = n¯c +KI[c ̸= c ], c = c , (A67)
obs;i obs i n+1 n+1

n¯c , otherwise.
obs
Combining (A66) and (A67), we can rewrite the second term in (A65) as:
(cid:16)nci (cid:17)−1 (cid:16)ncn+1(cid:17)−1
(cid:89)nc
(cid:16)nc (cid:17)−1
(cid:34) (cid:89)nc
(cid:16)nc
(cid:17)−1(cid:35) n¯o cibs;i n¯o cnbs +;i
1
obs;i = obs · obs;i · obs;i
n¯c n¯c (cid:16)nci (cid:17)−1 (cid:16)ncn+1(cid:17)−1
c=1 obs;i c=1 obs obs obs
n¯ci n¯cn+1
obs obs (A68)
 (cid:16)nci (cid:17) (cid:16)ncn+1(cid:17) I[ci̸=cn+1]
(cid:34) (cid:89)nc
(cid:16)nc
(cid:17)−1(cid:35) 

n¯o cibs n¯o cnbs
+1


= obs · obs · obs .
n¯c (cid:16)nci −K(cid:17) (cid:16)ncn+1 +K(cid:17)
c=1 obs   n¯o cibs
−K
n¯o cnbs
+1 +K
 
obs obs
Then, the third term on the right-hand-side of (A65) is
 (cid:89)nc N (cid:89)nc ;iK (cid:89)−1
1

 .
n¯c −K(j −1)−k
c=1j=1 k=1 obs;i
This relates to the probability of choosing a specific realization of the calibration groups given the
observed indices remaining after random pruning. To aid the simplification of this term, we point
out the following relation between Nc and Nc, i.e., the number of calibration groups from each
n;i n
52column in the imagined observed set and original observed set respectively:

Nc −I[c ̸= c ]. c = c ,
  n i n+1 i
Nc = Nc +I[c ̸= c ], c = c , (A69)
n;i n i n+1 n+1

Nc otherwise.
n
Then, using (A67) and (A69), we can write:
(cid:89)nc N (cid:89)nc ;iK (cid:89)−1
1
=
n¯c −K(j −1)−k
c=1j=1 k=1 obs;i
 
(cid:89)nc (cid:89)N nc K (cid:89)−1
1
=  
n¯c −K(j −1)−k
c=1j=1 k=1 obs
 N (cid:81)nci ;iK (cid:81)−1
1
N nc (cid:81)n ;i+1 K (cid:81)−1
1
I[ci̸=cn+1] (A70)


n¯ci
−K(j−1)−k
n¯cn+1−K(j−1)−k

=· j=1 k=1 obs;i · j=1 k=1 obs;i 
 

N (cid:81)nciK (cid:81)−1
1
Nnc (cid:81)n+1K (cid:81)−1
1
 

n¯ci −K(j−1)−k n¯cn+1−K(j−1)−k
j=1 k=1 obs j=1 k=1 obs
 (cid:89)nc (cid:89)N nc K (cid:89)−1 1  (cid:32)K (cid:89)−1 n¯ci −k (cid:33)I[ci̸=cn+1]
= 
n¯c −K(j
−1)−k· n¯cn+1ob +s
K −k
.
c=1j=1 k=1 obs k=1 obs
Above, the last equality follows from the following simplification based on (A69) and (A67), as-
suming that c ̸= c :
i n+1
N (cid:81)nci ;iK (cid:81)−1
1
N nc (cid:81)n ;i+1 K (cid:81)−1
1
n¯ci
−K(j−1)−k
n¯cn+1−K(j−1)−k
j=1 k=1 obs;i j=1 k=1 obs;i
= ·
N (cid:81)nciK (cid:81)−1
1
Nnc (cid:81)n+1K (cid:81)−1
1
n¯ci −K(j−1)−k n¯cn+1−K(j−1)−k
j=1 k=1 obs j=1 k=1 obs
N (cid:81)nciK (cid:81)−1
n¯ci −K(j −1)−k
Nnc (cid:81)n+1K (cid:81)−1
n¯cn+1 −K(j −1)−k
obs obs
j=1 k=1 j=1 k=1
= ·
Nn(cid:81)ci−1K (cid:81)−1 (cid:0) n¯ci −K(cid:1) −K(j −1)−k Nncn (cid:81)+1+1K (cid:81)−1 (cid:0) n¯cn+1 +K(cid:1) −K(j −1)−k
obs obs
j=1 k=1 j=1 k=1
N (cid:81)nciK (cid:81)−1
n¯ci −K(j −1)−k
Nnc (cid:81)n+1K (cid:81)−1
n¯cn+1 −K(j −1)−k
obs obs
j=1 k=1 j=1 k=1
= ·
Nn(cid:81)ci−1K (cid:81)−1
n¯ci −Kj −k
Nncn (cid:81)+1+1K (cid:81)−1
n¯cn+1 −K(j −2)−k
obs obs
j=1 k=1 j=1 k=1
K (cid:89)−1 n¯ci −k
= obs .
n¯cn+1
+K −k
k=1 obs
53Finally, combining (A65) with (A68) and (A70), we arrive at
p (x ,...,x ,x )
i 1 n n+1
(cid:32) K (cid:33)
(cid:89)
∝ P (D = D )· w∗ w∗
w obs obs;i (cid:101)xi,1 (cid:101)x
i,k
k=2
 (cid:16)nci (cid:17) (cid:16)ncn+1(cid:17) I[ci̸=cn+1]
(cid:34) (cid:89)nc
(cid:16)nc
(cid:17)−1(cid:35) n¯o cibs n¯o cnbs
+1
· obs · obs · obs 
n¯c (cid:16)nci −K(cid:17) (cid:16)ncn+1 +K(cid:17)
c=1 obs n¯o cibs
−K
n¯o cnbs
+1 +K
obs obs
 (cid:89)nc (cid:89)N nc K (cid:89)−1 1  (cid:32)K (cid:89)−1 n¯ci −k (cid:33)I[ci̸=cn+1]
·
n¯c −K(j
−1)−k· n¯cn+1ob +s
K −k
c=1j=1 k=1 obs k=1 obs
(cid:32) K (cid:33)
(cid:89)
∝ P (D = D )· w∗ w∗
w obs obs;i (cid:101)xi,1 (cid:101)x
i,k
k=2
 (cid:16)nci (cid:17) (cid:16)ncn+1(cid:17) I[ci̸=cn+1]
n¯o cibs n¯o cnbs +1 K (cid:89)−1 n¯ci −k
· obs · obs · obs  .
(cid:16)nci −K(cid:17) (cid:16)ncn+1 +K(cid:17) n¯cn+1 +K −k
n¯o cibs
−K
n¯co nbs
+1 +K
k=1 obs
obs obs
A4.4 Finite-Sample Coverage Bounds
Proof of Theorem 1. Recall that, by construction,
n
(cid:16) (cid:88) (cid:17)
M
X∗
∈ C(X∗,τ (cid:98)α,K,M(cid:99)) ⇐⇒ S∗ ≤ τ
(cid:98)α,K
= Q 1−α; p iδ
Si
+p n+1δ
∞
.
i=1
Therefore, Theorem 1 follows directly by combining Proposition 1, Lemma 2, and the characteri-
zation of the conformalization weights given by Equation (19).
Proof of Theorem 2. Recall that, by construction,
n
(cid:16) (cid:88) (cid:17)
M
X∗
∈ C(X∗,τ (cid:98)α,K,M(cid:99)) ⇐⇒ S∗ ≤ τ
(cid:98)α,K
= Q 1−α; p iδ
Si
+p n+1δ
∞
.
i=1
Therefore, applying Lemma A5, we see that it suffices to prove
(cid:34) n (cid:35) (cid:20) (cid:21)
(cid:16) (cid:88) (cid:17)
P S∗ ≤ Q 1−α; p δ +p δ ≤ 1−α+E max p (Xcal,...,Xcal,X∗) . (A71)
i Si n+1 S∗ i 1 n
i∈[n+1]
i=1
Let E denote the event that {Xcal,...,Xcal,X∗} = {x ,...,x }, D = D , and D =
x 1 n 1 n+1 drop 0 train
D , for some possible realizations x = (x ,...,x ) of Xcal,...,Xcal,X∗, D of D , and D
1 1 n+1 1 n 0 drop 1
54of D . Let also {v ,...,v } indicate the realization of {S ,...,S ,S∗} corresponding to the
train 1 n+1 1 n
event E , for all i ∈ [n+1]. Applying the definition of conditional probability, as in the proof of
x
Lemma 2, we can see that S∗ | E ∼ (cid:80)n+1p (x ,...,x )δ , with the weights p given by (19).
x i=1 i 1 n+1 vi i
This implies that
(cid:34) n+1 (cid:35)
(cid:16) (cid:88) (cid:17)
P S∗ ≤ Q 1−α; p δ | E ≤ 1−α+ max p (x ,...,x ),
i vi x i 1 n+1
i∈[n+1]
i=1
and further, by taking an expectation with respect to the randomness in E ,
x
(cid:34) n (cid:35) (cid:20) (cid:21)
(cid:16) (cid:88) (cid:17)
P S∗ ≤ Q 1−α; p δ +p δ ≤ 1−α+E max p (Xcal,...,Xcal,X∗) .
i Si n+1 S∗ i 1 n
i∈[n+1]
i=1
A4.5 Efficient Evaluation of the Conformalization Weights
Proof of Proposition 2. We begin by focusing on the special case of i = n + 1. In that case,
Equation(24)becomesaspecialcaseoftheresultsderivedforthemultivariateWallenius’noncentral
hypergeometric distribution [66, 67, 78]. While the original problem addresses biased sampling
without replacement from an urn containing colored balls, our model in (1) can be equivalently
interpreted as drawing samples without replacement from an urn comprising n n balls. Each ball
r c
is uniquely labeled with a color represented by (r,c) ∈ [n ]×[n ], and it is drawn with a probability
r c
proportional to w ; e.g., see Section 2. Therefore, from Equation (19) in Fog [78]:
r,c
(cid:90) 1
P (D = D ) = Φ(τ;h)dτ. (A72)
w obs obs
0
Next, we turn to proving Equation (24) for a general i ∈ [n+1].
For any i ∈ [n + 1], imagine an alternative world in which x is swapped with x . Let
n+1 i
(cid:80)
δ := w indicate the cumulative weight of all missing entries analogous to δ in the
i (r,c)∈Dmiss;i r,c
aforementionedimaginaryworld. Itiseasytoseethatδ = δ+d ,whered :=
(cid:80)K
(w −w ).
i i i k=1 x i,k x n+1,k
Therefore, we can express the probability using Equation (A72) in the imaginary world:
(cid:90) 1 (cid:89) (cid:16) (cid:17)
P (D = D ) = hδ τhδi−1 1−τhwr,c dτ
w obs obs;i i
0
(r,c)∈D
obs;i
(cid:90) 1 (cid:89) (cid:16) (cid:17) (cid:89)K (cid:18) 1−τhw n+1,k(cid:19)
= h(δ+d )τh(δ+di)−1 1−τhwr,c · dτ
0
i 1−τhw
i,k
(r,c)∈D k=1
obs
 
=
(cid:90) 1
hδτhδ−1 
(cid:89)
1−τhwr,c·
h(δ+d i)τh(δ+di)−1 ·(cid:32) (cid:89)K 1−τhw n+1,k(cid:33)
dτ
0
hδτhδ−1 1−τhw
i,k
(r,c)∈D k=1
obs
(cid:90) 1
= Φ(τ;h)·η (τ;h)dτ.
i
0
55where, for any τ ∈ (0,1),
τhdi(δ+d i) (cid:32) (cid:89)K 1−τhw n+1,k(cid:33)
η (τ;h) := · . (A73)
i δ 1−τhw i,k
k=1
Proof of Lemma 4. Recall that the logarithm of Φ(τ;h) takes the form
(cid:88)
ϕ(τ;h) := logΦ(τ;h) = log(hδ)+(hδ−1)log(τ)+ log(1−τhwr,c), (A74)
(r,c)∈D
obs
while its first derivative with respect to τ is:
ϕ′(τ;h) =
hδ−1
−
(cid:88) hw r,cτhwr,c−1
.
τ 1−τhwr,c
(r,c)∈D
obs
Consider the function τϕ′(τ;h),
τϕ′(τ;h) = hδ−1−
(cid:88) hw r,cτhwr,c
,
1−τhwr,c
(r,c)∈D
obs
which is strictly decreasing in τ for all τ ∈ (0,1), because w > 0 for all (r,c). Further, if h > 1/δ,
r,c
lim τϕ′(τ;h) = hδ−1 > 0, lim τϕ′(τ;h) = −∞.
τ→0+ τ→1−
Then, by the intermediate value theorem, τϕ′(τ;h) must have exactly one zero for τ ∈ (0,1), as
long as h > 1/δ. In turn, this implies that ϕ′(τ;h) has exactly one zero for τ ∈ (0,1), as long as
h > 1/δ. Further, the unique zero of ϕ′(τ;h) on τ ∈ (0,1) must be the unique maximum of ϕ(τ;h),
because, under h > 1/δ,
lim ϕ(τ;h) = −∞, lim ϕ(τ;h) = −∞.
τ→0+ τ→1−
A4.6 Consistency of the Generalized Laplace Approximation
We begin by stating a formal version of Theorem 3, the result providing the motivation to apply
the Laplace method to Equation (27).
Theorem A4. Let {w }∞ be a sequence of i.i.d. random variables drawn from a distribution F
i i=1
with support on the open interval (0,1). Consider a sequence of mutually independent Bernoulli
random variables {x }∞ , where each x in ∼d. Bernoulli(w ). Define δ = (cid:80)n (1−x )w and
i i=1 i i n i=1 i i
n
Φ (τ) := h δ
τhnδn−1(cid:89)(cid:16) 1−τhnwi(cid:17)xi
. (A75)
n n n
i=1
56Above, h is the unique root of the function
n
z(h) :=
ϕ′ n(cid:0) 21(cid:1)
= δ −
1 −(cid:88)n x iw
i (A76)
2h n h 2hwi −1
i=1
in the interval [δ ,∞), where ϕ (τ) is the logarithm of Φ (τ), namely
n n n
n
(cid:88) (cid:16) (cid:17)
ϕ (τ) := logΦ (τ) = log(h )+log(δ )+(h δ −1)log(τ)+ x log 1−τhnwi . (A77)
n n n n n n i
i=1
Then, argmax Φ (τ) = 1.
τ∈[0,1] n 2
Further, consider a sequence of functions {f }, where each f ∈ C1(0,1) and f (cid:0)1(cid:1) > ϵ , for
n n n 2 0
some constant ϵ > 0 and all n. Suppose there exists some M > 0 such that |f′(x)| ≤ M for all
0 n
x ∈ (0,1) and for all n. Then, it holds that
(cid:90) 1 (cid:18) 1(cid:19) (cid:18) 1(cid:19)(cid:115) −2π
f (τ)Φ (τ)dτ ∼ f ·Φ almost surely as n → ∞, (A78)
n n n
2
n
2
ϕ′′(cid:0)1(cid:1)
0 n 2
or equivalently,
(cid:82)1
f (τ)Φ (τ)dτ
lim 0 n n = 1 almost surely. (A79)
n→∞ f (cid:0)1(cid:1) ·Φ (cid:0)1(cid:1)(cid:113) −2π
n 2 n 2 ϕ′′(1)
n 2
Proof of Theorem A4. The preliminary part of this result is proved in Appendix A2.2, where we
show that selecting the scaling parameter h as the unique root of the function in (A76) leads to
n
τ∗ := argmax Φ (τ) = 1.
n τ∈[0,1] n 2
Our main objective is to approximate the integral
(cid:90) 1 (cid:90) 1
f (τ)Φ (τ)dτ = f (τ)eϕn(τ)dτ,
n n n
0 0
leveraging a suitable extension of the classical Laplace method reviewed in Appendix A1.4. To
this end, we begin by applying a Taylor series expansion around τ∗, including Lagrange remainder
n
terms; this leads to:
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
f (τ) = f +f′(ξ ) τ − ,
n n 2 n 1 2
(cid:18) 1(cid:19) (cid:18) 1(cid:19)(cid:18) 1(cid:19) ϕ′′(cid:0)1(cid:1) (cid:18) 1(cid:19)2 ϕ′′′(ξ ) (cid:18) 1(cid:19)3
ϕ (τ) = ϕ +ϕ′ τ − + n 2 τ − + n 2 τ − ,
n n 2 n 2 2 2 2 6 2
57for some real numbers ξ ,ξ ∈ [1/2,τ], and
1 2
n
(cid:88) (cid:16) (cid:17)
ϕ (τ) = log(h )+log(δ )+(h δ −1)log(τ)+ x log 1−τhnwi
n n n n n i
i=1
ϕ′ (τ) =
h nδ n−1 −(cid:88)n
x
h nw iτhnwi−1
n τ i (1−τhnwi)
i=1
ϕ′′(τ) =
−h nδ n−1 −(cid:88)n
x
h nw iτhnwi−2(τhnwi +h nw i−1)
n τ2 i (1−τhnwi)2
i=1
h δ −1
ϕ′′′(τ) = 2 n n
n τ3
(cid:88)n (h nw i)τ(hnwi−3)(cid:0) 3(h nw i)(τhnwi −1)+2(τhnwi −1)2+(h nw i)2(τhnwi +1)(cid:1)
+ x .
i (τhnwi −1)3
i=1
By definition of h , we know that ϕ′ (cid:0)1(cid:1) = 0. Next, we need to establish a suitable bound
n n 2
for ϕ′′. This task is complicated by the fact that we do not have an explicit expression for h .
n n
Fortunately, however, it is possible to obtain sufficiently tight lower and upper bounds for h .
n
Lemma A7. In the setting of Theorem A4, for any n > 1,
(cid:32) (cid:33)
2n (cid:18) (cid:19)
s 1 1 n 1
δn n + ≤ h ≤ 1+ . (A80)
2n n n δ n log2 δ
2δn −1 n n
Further, in the limit of n → ∞, it holds almost-surely that
J 1
≤ h ≤ , (A81)
n
L log(2)L
2 2
where
2 1
J := L · · , L := E[x], L := E[(1−x)w].
1 L 2 1 2
2 2L2 −1
The bounds on h provided by Lemma A7 in turns allow us to bound ϕ′′ away from 0 almost
n n
surely for large n. This gives us the necessary ingredients to tackle the approximation of the
integral. To this end, note that
(cid:90) 1 f n(τ)eϕn(τ)dτ = (cid:90) 1(cid:26) f n(cid:18) 21(cid:19) +f n′(ξ 1(τ))(cid:18) τ − 21(cid:19)(cid:27) eϕn(1 2)+ϕ′ n′( 21 2) (τ−1 2)2 +ϕ′ n′′(ξ 62(τ))(τ−1 2)3 dτ,
0 0
(cid:113)
since ϕ′ (1) = 0. Applying the change of variables u = −ϕ′′(cid:0)1(cid:1) (τ − 1) and defining K =
n 2 n 2 2 n
58(cid:112)
−ϕ′′(1/2), the integral becomes
n
(cid:90) 1
f (τ)eϕn(τ)dτ (A82)
n
0
=
eϕ Kn(1 2) (cid:90) Kn/2 (cid:18)
f
n(cid:18) 1 2(cid:19)
+
Ku
f n′(ξ
1(u))(cid:19) e−u 22 +ϕ′ n′′( 6ξ2 K( n3u))u3
du
n −Kn/2 n
=
eϕn(1 2) (cid:40)
f
n(cid:18) 1(cid:19)(cid:90) Kn/2 e−u 22
du+f
n(cid:18) 1(cid:19)(cid:90) Kn/2 e−u 22 (cid:32) eϕ′ n′′( 6ξ2 K( n3u))u3 −1(cid:33)
du
K 2 2
n −Kn/2 −Kn/2
(cid:32) (cid:33) (cid:41)
+(cid:90) Kn/2 Ku f n′(ξ 1(u))e−u 22 du+(cid:90) Kn/2 Ku f n′(ξ 1(u))e−u 22 eϕ′ n′′( 6ξ2 K( n3u))u3 −1 du .
−Kn/2 n −Kn/2 n
(A83)
Note that now ξ and ξ depend implicitly on u due to the change of variables (they previously
1 2
depended on τ). We will now separately analyze each term in (A82).
The limit of the first integral on the right-hand-side of (A82) can be found by leveraging the
following lower bound for K :
n
(cid:115) (cid:18) 1(cid:19) √ (cid:115) −ϕ′′(cid:0)1(cid:1) √ (cid:114) h δ −1 √ √
K = −ϕ′′ = n n 2 ≥ n 4 n n ≥ 2 n J,
n n 2 n n
which leads to
lim
(cid:90) Kn/2
e−u 22
du =
√
2π. (A84)
n→∞
−Kn/2
By contrast, the third integral on the right-hand-side of (A82) is asymptotically negligible.
Recall that, by assumption, f satisfies |f′(x)| ≤ M for all x ∈ (0,1) and n; therefore,
n n
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)(cid:90) Kn/2 Ku f n′(ξ 1(u))e−u 22 du(cid:12) (cid:12)
(cid:12)
≤ KM (cid:90) Kn/2 |u|e−u 22 du → 0 as n → ∞. (A85)
(cid:12) −Kn/2 n (cid:12) n −Kn/2
We continue with the analysis of the second and fourth terms on the right-hand-side of (A83),
which are more involved. The remainder of the second-order Taylor expansion, previously denoted
as ϕ′′′(ξ (u))u3/(6K3), can be expanded back into an infinite power series, given the smoothness
n 2 n
of ϕ within the interval (0,1). This expansion is expressed as:
n
y (u) :=
ϕ′ n′′(ξ 2(u))u3
=
(cid:88)∞ ϕ( nj)(cid:0)1 2(cid:1)
uj, (A86)
n 6K3 j!Kj
n j=3 n
where ϕ(j)(cid:0)1(cid:1) represents the j-th derivative of ϕ evaluated at 1, and this series converge for all n
n 2 n 2
and all u ∈ (−K /2,K /2).
n n
59Note that the j-th derivative of ϕ at 1/2 can be written as:
n
ϕ( nj)(cid:0)1 2(cid:1)
=
h nδ n−1 dj (logτ)(cid:12)
(cid:12) +
1 (cid:88)n
x
dj log(cid:16) 1−τhnwi(cid:17)(cid:12)
(cid:12) . (A87)
n n dτj (cid:12) τ=1 n i dτj (cid:12) τ=1
2 i=1 2
To control y (u), we will show that (A87) is bounded for large n. Lemma A7 tells us that the first
n
term in (A87) is bounded by constants almost surely. For the second term, define:
dj (cid:16) (cid:17)(cid:12)
g(h,x,w) := x log 1−τhw (cid:12) , (A88)
dτj (cid:12) τ=1
2
which is continuous for h > 0 and w > 0. Given that the interval C := [J,1/log(2)] is compact, we
can use the maximum value in h to bound the function g. Thus, we obtain:
n n
1 (cid:88) 1 (cid:88)
g(h ,x ,w ) ≤ maxg(h,x ,w ) almost surely as n → ∞. (A89)
n i i i i
n n h∈C
i=1 i=1
Then, by the strong law of large numbers,
n (cid:20) (cid:21)
1 (cid:88) maxg(h,x ,w ) −a −. →s. E maxg(h,x,w) , (A90)
i i
n h∈C h∈C
i=1
leading to:
n (cid:20) (cid:21)
1 (cid:88)
g(h ,x ,w ) ≤ E maxg(h,x,w) almost surely. (A91)
n i i
n h∈C
i=1
Similarly, we can also show:
n (cid:20) (cid:21)
1 (cid:88)
g(h ,x ,w ) ≥ E ming(h,x,w) almost surely. (A92)
n i i
n h∈C
i=1
Therefore, the second term in (A87) is also bounded by constants almost surely. As a result, the
whole expression in (A87) is bounded by constants almost surely for large n.
Combining the above results, we conclude that, for all j ≥ 3,
(cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)ϕ
j( nj !) K(cid:0) nj21(cid:1)(cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
≤
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)n
jϕ !( nj K) n( nj1 2)(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
≤
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)j!(n
2ϕ √n(j J) n( )1
2
j)
n2j
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
= O(cid:16) n−( 2j−1)(cid:17) almost surely. (A93)
We can now proceed to analyze the integral involving the exponential of the remainder term
y (u), which appears in the second term on the right-hand-side of (A82). Specifically,
n
(cid:90) ∞ e−u 22 (cid:16) eyn(u)−1(cid:17)
du =
√ 2πE(cid:104) eyn(Z)−1(cid:105)
=
√ 2π(cid:88)∞ E(cid:2) (y n(Z))k(cid:3)
, (A94)
k!
−∞
k=1
where Z ∼ N(0,1). Below, we show that all terms on the right-hand-side of (A94) are finite and
60converge to 0 as n increases. To this end, let us start from k = 1, noting that
 
E[y n(Z)] = E
(cid:88) j∞
=3
ϕ j( nj !) K(cid:0) nj1 2(cid:1)
Zj  =
j(cid:88)
≥4,
ϕ j( nj !) K(cid:0) nj21(cid:1) 22jj (!
2j)!
=
j(cid:88)
≥4,
Kϕ nj( nj 2) 2j(cid:0) (1
2
2j(cid:1)
)!,
j iseven j iseven
wherethesimplificationarisesbecausealloddmomentsofastandardnormalarezero, andtheeven
moments follow from the moment generating function. Given (A93), it follows that E[y (Z)] → 0
n
as n → ∞. Similarly, it also can be shown that all higher moments of y (Z) are finite and converge
n
to 0 as n increases. Consequently, we conclude that
(cid:104) (cid:105)
E eyn(Z)−1 → 0 as n → ∞.
Therefore, the limit of the second term in the Taylor error expansion (A83) is
lim
(cid:90) K 2n e−u 22 (cid:32) eϕ′ n′′( 6ξ2 K( n3u))u3 −1(cid:33)
du = 0 almost surely. (A95)
n→∞ −Kn
2
The fourth term in the Taylor error expansion (A83) vanishes similarly because f′ is bounded; i.e.,
n
nl →im ∞(cid:90) −K K2n
n
Ku nf n′(ξ 1(u))e−u 22 (cid:32) eϕ′ n′′( 6ξ2 K( n3u))u3 −1(cid:33) du = 0 almost surely. (A96)
2
Combining (A82) with (A84), (A85), (A95), and (A96), we arrive at the desired result.
Proof of Lemma A7. It is immediate from (A76) that h > 1/δ . To obtain an upper bound, recall
n n
that z(h ) = 0, for the function z in (A76). This implies that
n
n
1 (cid:88) x iw i
δ = +
n h
n
2hnwi −1
i=1
n
≤ 1 +(cid:88) x w i (since 2x−1 > log(2)x for all x > 0)
i
h log(2)h w
n n i
i=1
(cid:18) (cid:19) (cid:18) (cid:19)
s 1 n 1
n
= 1+ ≤ 1+ ,
log(2) h log(2) h
n n
where s =
(cid:80)n
x denotes the number of successful Bernoulli trials. Therefore,
n i=1 i
(cid:18) (cid:19)
n 1
h ≤ 1+ . (A97)
n
log(2) δ
n
The upper bound in (A97) also allows us to find a tighter lower bound. Note that x is a
2x−1
61decreasing function of x > 0, and w < 1 for all i. Therefore, z(h ) = 0 implies
i n
n n
(cid:88) x ih nw i (cid:88) x ih n h n
h δ −1 = ≥ = s .
n n 2hnwi −1 2hn −1 2hn −1 n
i=1 i=1
Combining this result with (A97), we obtain the following lower bound:
(cid:16) (cid:17)
1+ n 1 2n
h nδ n−1 ≥ h n s n ≥ log(2) δn s n ≥ δn s n . (A98)
n 2hn −1 n 2(cid:16) 1+ logn (2)(cid:17) δ1
n −1
n 2δ2 nn −1 n
This completes the proof of (A80).
To prove the second part, we apply the strong law of large numbers, by which sn = (cid:80)n i=1xi −a −. →s.
n n
E[x] as n → ∞, and δn = (cid:80)n i=1(1−xi)wi −a −. →s. E[(1−x)w] as n → ∞. Therefore, by the continuous
n n
mapping theorem,
h δ −1 h s 2 1
n n n n
≥ ≥ L · · := J almost surely as n → ∞,
n 2hn −1 n 1 L
2
2L2
2 −1
where the expected values L = E[x] and L = E[(1−x)w] do not depend on n. Therefore, in the
1 2
limit of n → ∞, it holds almost-surely that
(cid:18) (cid:19)
1 n 1
(nJ +1) ≤ h ≤ 1+ ,
n
δ log2 δ
n n
a.s.
Finally, recall that δ /n −−→ L as n → ∞. Consequently, we have, almost surely, that: J/L ≤
n 2 2
h ≤ 1/[log(2)L ] as n → ∞.
n 2
62