A Semi-Automatic Approach to Create Large Gender- and Age-Balanced
Speaker Corpora: Usefulness of Speaker Diarization & Identification.
Re´miUro1,2,DavidDoukhan1,AlbertRilliard2,3,Lae¨titiaLarcher1,
Anissa-ClaireAdgharouamane1,MarieTahon4,AntoineLaurent4
1FrenchNationalInstituteofAudiovisual(INA),Paris,France. 2Universite´ ParisSaclay,CNRS,LISN.
3UniversidadeFederaldoRiodeJaneiro,Brasil. 4LIUM,LeMansUniversite´,France.
{ruro,ddoukhan,llarcher,aadgharouamane}@ina.fr
albert.rilliard@lisn.fr,{marie.tahon,antoine.laurent}@univ-lemans.fr
Abstract
This paper presents a semi-automatic approach to create a diachronic corpus of voices balanced for speaker’s age, gender,
andrecordingperiod,accordingto32categories(2genders,4agerangesand4recordingperiods). Corporawereselectedat
FrenchNationalInstituteofAudiovisual(INA)toobtainatleast30speakerspercategory(atotalof960speakers;only874
havebefoundyet).Foreachspeaker,speechexcerptswereextractedfromaudiovisualdocumentsusinganautomaticpipeline
consisting of speech detection, background music and overlapped speech removal and speaker diarization, used to present
cleanspeakersegmentstohumanannotatorsidentifyingtargetspeakers. Thispipelineprovedhighlyeffective,cuttingdown
manualprocessingbyafactoroften. Evaluationofthequalityoftheautomaticprocessingandofthefinaloutputisprovided.
Itshowstheautomaticprocessingcomparetoup-to-dateprocess,andthattheoutputprovideshighqualityspeechformostof
theselectedexcerpts.Thismethodshowspromiseforcreatinglargecorporaofknowntargetspeakers.
Keywords:semi-automatic processing, corpus creation, diarization, speaker identification, gender-balanced, age-balanced,
speakercorpus,diachrony
1. Introduction tioncomparedtomanualprocessing.
This paper describes a semi-automatic method for
2. Targetedcorpusandexistingresources
speeding-up speaker corpora construction. There is
a growing need for reference speech corpora featur- 2.1. Voices&genderovertime
ing reliable and known speaker characteristics. Man-
For the needs of the GEM project, the capacity to de-
ual annotations of high level features such as speaker scribe the acoustic characteristics of a given speaker’s
segmentation is a time consuming process (Broux et
voice is important, to compare to its socially recog-
al., 2018). We propose to use up-to-date diarization
nized characteristics, that predominantly includes the
and speaker identification to reduce human interven-
speaker’s gender and age. Having a reliable statisti-
tion,andenablethecreationoflargespokencorporaat
calrepresentation(i.e.,basedonrepresentativedatafor
aminimumcost.
severalageandgendercategories)oftheacousticvari-
This work is part of the Gender Equality Monitor ation of voices presented in the public arena allows
(GEM) project, that aims at describing male and fe- sociological analysis of gender representation, gender
malerepresentationdifferencesintheFrenchbroadcast stereotypicalcharacteristics,potentialchangesinvoice
mediaatscale,usingautomaticinformationextraction asadevicetopresentone’sselfpublicimage,etc. The
methods. A main institution involved in the project is voice is an important aspect of the construction of in-
the French National Institute of Audiovisual (INA), a dividual social persona or character (Podesva, 2007;
publicinstitutioninchargeofarchivingFrenchaudio- Sadanobu, 2015); it changes according to our role in
visual heritage (so calledlegal deposit). INA’s collec- society—e.g.,whiletalkingasanemployeetoasupe-
tions include 23 million hours of TV and radio pro- rior,asaprofessortostudents,asaparenttochildren,
gramsbroadcastedsincethe1930’s. asafriendtosportingmatesetc.Ourvoiceisdeveloped
First,thetargetedcharacteristicsofthecorpusarepre- duringchildhoodandadolescence,varyingbecauseof
sented, with a discussion of comparable datasets, and ourphysiologicaldevelopment(Vorperianetal.,2011),
thepotentialuseofthisresource.Aliteraturereviewon butalsoasaculturallyshapedrepresentationofourso-
diarization and speaker identification systems is then cial projection as an individual (Sergeant and Welch,
presented. The rest of the paper presents: (1) the 2009; Guzman et al., 2014; Scott, 2022). Gender is
methodological steps for the creation of this corpus, a key aspect of the construction of voices, and is cul-
(2) evaluations of these steps in terms of performance turallyshaped: speakershavebeenshowntohavedif-
onavailabledatawithcomparablecharacteristics,(3)a ferent mean pitch in different cultures, and to change
subjectiveevaluationofthequalityofvoiceofthefinal theirpitchforreachingculture-matchingexpectations,
output,and(4)anestimationofprocessingtimereduc- notablyongender(vanBezooijen,1995;Ohara,1992;
4202
rpA
62
]SA.ssee[
1v25571.4042:viXraOhara, 2001). Changes in gender-related vocal cues Another set of corpora was developed during evalua-
haveeffectsonmanyaspectsofsocialinteraction,pitch tion campaigns for NLP tools. For example, the ES-
having been related to a series of characteristics like TERcorpusprovidessoundsandtranscriptionsforone
credibility, charisma, cuteness, sexual attractiveness, hundred hours of broadcast news from French media
etc. (Geiselman and Crawley, 1983; Niebuhr et al., (Gallianoetal.,2012). TheETAPEcorpusisafollow-
2016;Jang,2021;Gussenhoven,2016). on development with less prepared speech from other
To better document the aspects of self that our voices types of radio or TV programs (Gravier et al., 2012).
may voluntarily or not display (Scott, 2022), a refer- These corpora contain information on speaker gender,
ence corpus of voices is a great tool, allowing stud- butnotdirectlyonspeakerage. Unfortunately,noneof
iesonvoicepresentationinthepublicarenaviabroad- them present a diachronic dimension. One resource,
cast media in France (the GEM project is centered on the Eurodelphes database (Barras et al., 2002), pro-
thiscountry). Thiscorpuswouldideallycontainvoices poses a set of broadcast documents spread from the
used in comparable dialogue situations – i.e., not ex- 1940’suptothe1990’s;itisnonethelessrelativelylim-
pressivesituationswherevoicemaybeespeciallyloud, ited in size for the oldest decades, and is highly un-
orexpressemotions,twoaspectswhichhaveimportant balanced for gender and age (Boula de Mareu¨il et al.,
effects on vocal characteristics (Titze and Sundberg, 2012),thusnotsuitedforthetargeteduse.
1992; Lie´nard and Di Benedetto, 1999; Traunmu¨ller The corpus presented in Suire and Barkat-Defradas
and Eriksson, 2000; Lie´nard, 2019; Goudbeek and (2020)istheclosesttowhatwearetryingtobuild. It
Scherer,2010). Althoughsuchphenomenamaybein- is based on media programs with about thirty speak-
terestingtodescribe,theyaddvariabilitythatwouldre- ersofeachgenderselectedbyperiodoftenyears,over
quire more data to be controlled for. Thus broadcast sevendecades,withabout5secondsofspeechforeach
programs featuring interviews or discussions with in- speakers;theageofspeakerswasnotinformed. These
vited people were selected, mostly those recorded in two limitations (short extracts, and no information on
studiosituations. Notethataudiovisualprogrammeta- speaker’sage)makeitunsuitableforthestudy.
datadoesnotallowafullcontroloftherecordingset-
tings, especially as it is common to find reporting on 2.3. Existing(semi-)automaticspeaker
a given topic, that may happens to feature the target corpora
speaker. Detailsontheaudiovisualdocumentselection
Buildingaudiovisualspeakerdatabasesiscostly,since
processaregiveninthenextsection.
it requires finding speakers in audiovisual documents
To achieve accurate representation of voices, through
and obtaining the time codes for speakers’ speech
their long-term acoustic qualities, it is mandatory to
turns. To that aim, few fully or partially automated
obtainadurationthatallowsacousticcharacteristicsto
approaches were proposed for building large speaker
stabilize over articulation and other dynamic aspects.
databaseswithminimalhumaninvolvement.
Lo¨fqvist and Mandersson (1987) showed Long Term
INA’s speaker dictionary was prepared using a semi-
AverageSpectrumisstabilizedatabout20secondsof
automatic procedure based on unsupervised speaker
continuously voiced speech – so about twice this du-
segmentation (diarization) and Optical Character
ration for raw speech; see also Arantes and Eriksson
Recognition (OCR) (Salmon and Vallet, 2014; Val-
(2014)onprosody. Wesetalowerlimitofthreemin-
let et al., 2016). OCR decoded embedded text pre-
utesofdiarizedspeechperspeaker. Itwasthoughtim-
senting people in TV news programs, and filtered
portantsotolimittheinfluenceofnoisesandpotential
characters corresponding to the first twenty thousand
backchannels.
mostreferencedpeopleinINA’saudiovisualdatabases.
2.2. Availableresources Speechsegmentscorrespondingtodecodedembedded
textwereonasecondstagepresentedtohumananno-
Asfarasweknow,thereisnoavailablespeechcorpora
tatorsinchargeofstatingifspeechsegmentswerecor-
providingbalanceddistributionofspeakergender,age
respondingtothedecodedpersonname,resultinginan
andrecordingdateforFrenchmedia.
averageinvolvementtimeof22,8secondspersegment.
For resources in French, various corpora are available
viathecocoonwebsite1 –thatmostlyproposesdialec- VoxCelebwasbuiltfromYouTubevideosusingafully
tological, sociological or ethnological resources. The automatic procedure (Nagrani et al., 2017). YouTube
quality,typeofspeech,andinformationavailableabout video queries were applied using target speakers’
eachspeakerishighlyvariable.Oneoftheseresources, names and the word ‘interview‘. The set of target
the ESLO corpus (Baude, 2019), proposes interviews speakerswasasubsetofcelebritiesknownbyapreex-
ofmanyresidentsofOrleanscity,madeattwoperiods: istingfaceverificationclassifier. Activespeakerverifi-
beginning of 1970’s and of 2010’s (Eshkol-Taravella cationmodelswereusedtodetecttheportionsofvideo
etal.,2011). Ifaninterestingresource,itisstrictlyre- withfaciallipmovementssynchronizedwiththeaudio
strictedtoonecityandtwotimeperiods;itisalsobased track. Facecandidateswereinalatterstagepresented
onfieldrecordingsofvariablerecordingqualities. tothefaceverificationclassifierusingahighthreshold.
Whiletheseapproachesallowedtobuildlargespeaker
1https://cocoon.huma-num.fr/exist/crdo collections, they suffer from several limitations. Bothof them require video material, and cannot be used
forprocessingradiocollections. INA’sspeakerdictio-
nary’sstrategyrequiresembeddedtexttobedisplayed
during target’s speech turn, while VoxCeleb’s strat-
egyrequirestargetspeaker’sfacetobealreadyknown
by a face verification classifier. Moreover, YouTube’s
Clean Speech Detection
queriesdonotallowsearchbyspeakers’age.
The corpus-building process aims at creating a large
Voice Activity Detection
corpus (more than 960 speakers) of gender- and age-
balanced speakers that may serve as a reference of
voice qualities (linked to gender representation) pre- Non-Speech Overlapped
background audio
sented in broadcast media, from the 1950’s until now. segments removal Speech removal
Asamanualgatheringisoutofreach,weaimedatlim-
itingtoaminimummanualinterventionsotospeedthe
Intersection
process. Thispaperpresentthedetailsoftheapproach,
with an evaluation of the automatized process, and of
the quality of the obtained dataset, as well as an esti- Deletion of segments
mationofthetimegainedthroughthesemi-automated smaller than 2 seconds
method,comparedtoafullyhandmadeprocess.
3. Methods Unsupervized Speaker
Segmentation (Diarization)
Speaker selection guidelines were defined to obtain a
diachronic speaker corpus with balanced gender, age,
Manual target speaker
and recording period. These guidelines were pro-
identification in ELAN
vided to INA’s archivists to constitute a corpus of au-
diovisual documents containing a balanced amount of
target speakers. Figure 1 show the semi-automatic Optional automatic
Cross-Document speaker Retrieval
pipeline designed to extract target speaker excerpts
from this huge collection of audiovisual documents
with a minimal amount of human involvement. A Figure1:Semi-automaticpipelineproposedfortheex-
CleanSpeechDetectionprocesswasdefinedtodiscard tractionofcleantargetspeakersegments
speechsegmentswithacousticpropertiesthatmayin-
terferewithacousticparameterextraction.Anunsuper-
vised speaker segmentation and clustering procedure gender (Sataloff et al., 2017; Yamauchi et al., 2015).
(diarization) was used on the resulting clean speech Theperiodsoftimespanfromthe1950’stothe2010’s
segmentsinordertoassignnumericidentifierstoeach with20yearsintervals: thisissomewhatarbitrary,but
speakerfoundintherecording. Resultingspeakerseg- these periods were chosen because it is harder to find
mentationswerethenpresentedtohumanannotatorsin archivesbeforethe1950’s,especiallyfeaturingfemale
chargeofthemanualidentificationofthetargetspeak- speakers,andfourperiodswerethoughtsufficient,and
ers. Ifthefoundspeakerexcerptsinthemanuallypro- limittheamountoftargetspeakers. Eachspeakerwas
cesseddocumentsarelessthanthreeminutes,theseex- selected only in one of the 32 categories to avoid sta-
cerptswereusedtoperformautomaticcross-document tisticaldependency;wearethinkingaboutpreparinga
speakerretrieval,tocompletethespeakersampleinthe section of the corpus with longitudinal recordings of
corpus. speakersavailableinthreetofourtimeperiods.
3.1. Balanceddiachronicspeakercorpus 3.2. Audiovisualdocumentselection
definition
Documentselectionbasedonthecorpusdefinitionwas
Toavoidenvironmentalnoiseandvocalchangeslinked realized by INA’s archivists, and required 3 weeks of
tostylisticvariations,broadcastprogramsfeaturingdi- work. Thelistofparticipantsandthedateofdiffusion
alogandinterviews,recordedindoors,areprioritised. was extracted from TV and radio archives meta-data,
A total of 30 distinct speakers is required for each of andlinkedtoINA’spersonalitythesaurustoobtaindate
32 adult speaker categories (adding up to a total of ofbirthandgenderinformation.Thisallowedtoassign
atleast960differentspeakers),basedonthecombina- each unique speaker (our ”target speakers”) to one of
tionof3parameters:gender(maleandfemale),age(4 the 32 gender, age, and period categories. A manual
groups: 20to35,35to50,51to65andover65years selectionof450TVandradioshowswasrealized,that
old),andperiodsoftime(4periods:1955-1956,1975- usually feature reasonably long studio-recorded inter-
1976, 1995-1996, 2015-2016). The age groups were views,well-knownpersonalities,lowamountsofback-
based on known changes in voice linked to age and groundmusicandnoise,andalowamountofconflict-Period #TV #Radio #Unique Duration Level bgvl bg similar fg music
docs docs speakers (days) Recall(%) 65 89.9 97.0 97.2 99
1955-56 133 508 594 11
1975-76 849 642 1220 46 Table 2: Music detection recall obtained on
1995-96 1565 4686 2393 176 OpenBMAT for varying music levels: bgvl (hard-to-
2015-16 933 7991 1845 160 hearbackgroundmusic),bg(backgroundmusic),simi-
lar(musicandothersignalsmixedatsimilarlevels),fg
Table1:Characteristicsofthespeakercorpuscollected (foregroundmusic),music(musiconly)
byINA’sarchivists: numberofdocuments(docs)from
TVandradio,ofspeakers,andtotalduration.
tection (Bredin et al., 2020; Bullock et al., 2020) was
also used. The detected overlapping speech segments
inginteractionsbetweenparticipants. werethencutfromtheinitialVAD.
While the number of male found for each category
turned out to be greater than thirty, several categories 3.3.2. Non-Speechaudioeventdetection
of female had to benefit from additional manual re- Audiovisualdocumentsmaycontainnon-speechaudio
trieval in INA’s collections to reach the minimum eventsoverlappedwithspeech,suchasmusicornoise.
amountofspeakersrequiredtomeetcorpusspecifica- Sucheventsmayinterferewithvocalfeatureestimation
tions. Female stand out more rarely in broadcast me- andexcerptswiththeseeventsshouldbediscarded.
dia(Doukhanetal.,2018b),andtheirnumberwaspar- Anon-speechaudioeventdetectionmodelisproposed,
ticularlyscarcewithintheolderagegroupsandinthe based on spleeter source separation framework
oldestperiodsinINAthesaurus,possiblyreflectingthe (Hennequinetal.,2020). Weusedspleetervocals
gender bias of media presentation and representation and instrumental accompaniment separation model.
(Tuchman,2000). Researchforcomplementaryspeak- Thepotentialpresenceofnon-speechaudioeventswas
ers was necessary and resulted in the enrichment of linked to the energy of the extracted instrumental ac-
INA’s personality thesaurus, with the inclusion of the companiment track, estimated using the root mean
characteristicsof182mediapersonalities. squareofsignalwith200mswindowsizeand100ms
Table1presentsthecharacteristicsofthecorpusconsti- hopsize. Theenergyisfilteredusingamedianfilterof
tuted by INA’s archivists, with 25092 entries for 6051 size11,andathresholdsetat5%wasused.
distinct speakers, distributed between 17307 audiovi- The evaluation of the non-speech audio detection
sualdocuments,foratotaldurationofabout393days modelisdifficult,duetoalackofannotatedresources
of content (9400 hours). This is clearly too large to containingoverlappingspeech,musicandnoiseanno-
consider a complete manual exploration, and requires tations. Table 2 presents the evaluation of our pro-
thesemi-automaticproceduredescribedbelow. posal on OpenBMAT, a database of audio streams
with annotated music levels (Mele´ndez-Catala´n et al.,
3.3. CleanSpeechDetection(CSD) 2019). With respect to our use-case (obtaining clean
ACleanSpeechDetectionprocedurewasproposedto speech, even with a low document coverage rate) and
detect the cleanest speech excerpts, suited to prosodic to the availability of annotated resources (no dataset
parameter extraction. Figure 1 describes the compo- with speech, music and noise annotations), we used
nents of our proposal, which allows to obtain speech sed evalsegment-baseddetectionrecallforestimat-
segmentswithlowamountofoverlappedspeech,back- ingtheperformanceofourproposal, usingtimetoler-
ground music or noise. Speech segments shorter than ance (collar) of 1 second (Mesaros et al., 2016). The
2 seconds were rejected, as they are likely to be only resultsshowdetectionperformanceabove90%forau-
small parts of sentences and hence be of little interest diblemusic,and65%forhard-to-hearbackgroundmu-
toachieveanaccuraterepresentationofthevoices. sic,whichshalllessaffectacousticanalysis.
3.3.1. VoiceActivityDetection(VAD)and 3.3.3. CleanSpeechDetectionpipelinecoverage
OverlappedSpeechDetection(OVL) Wetestedourpre-processingpipelineontheDIHARD
Voice activity detection was performed us- IIDevelopmentset(Ryantetal.,2019). Table3shows
ing InaSpeechSegmenter (Doukhan et al., thedurationofdetectedspeechatdifferentstagesofthe
2018a). This system is based on a CNN architecture pre-processing.DIHARD II focuses on hard diariza-
trained to distinguish speech from music and noise. tion, i.e. with lots of low volume background speech,
InaSpeechSegmenterwasrankedinfirstposition in the wild speech with music, noise and overlapping
for the VAD task of MIREX 2018 challenge, contain- speech. Note that we target clean speech: low level
ing TV and radio corpora which are representative of noisyspeechisnotofinterestforourintendedprosodic
ourtargetmaterial2. analyses.OurCSDsystemeliminatesmorethanhalfof
Inordertoisolatethesegmentscorrespondingtoonly thetotalspeechtime. Sincewevalueabetterprecision
onespeaker,pyannote-audio’soverlapspeechde- thanrecall,consideringthatonlyabout40%ofagiven
corpusisusableseemsenough. Moreover,onecanas-
2https://www.music-ir.org/mirex/wiki/ sume that the TV and radio broadcast documents thatMethod Duration(s) Coverage genderandrole(anchor,participant)providedbyINA’s
Reference 72311 100% archivists (see section 3.2), to be manually identified.
Ref+OVL 63781 88.2% Thecleanspeechdiarizationdescribedabovewasex-
Ref+NSE 32013 44.3% portedtoELANvideoannotationtoolandpresentedto
VAD 69247 89.1% humanannotators,togetherwiththelistoftargetspeak-
VAD+OVL 56953 78.8% ers(SloetjesandWittenburg,2008).
VAD+NSE 30166 41.7% Foreach document, annotatorshadtomap diarization
VAD+OVL+NSE 28360 39.2% clusterid’stotargetspeakers’identities.
CSD 23980 33.2% The complexity of this task varies a lot depending on
the type of document and the role of the target. For
Table 3: Duration of detected speech on the DI-
instance a recent TV interview with only two speak-
HARDIIDevsetforthedifferentpre-processingsteps
erscanbeprocessedinafewseconds,whereasanold
and coverage relatively to the reference. (VAD: only
radio show with multiple characters, mostly unknown
VAD; OVL: overlapped speech removal; NSE: non-
nowadays, may require to listen almost all the docu-
speech events removal; CSD: clean speech detection
ment,andsometimestheuseofinternettofindphotos
–VAD+OVL+NSE + removal of segments less than 2
ordetailstospotthetarget.
seconds)
3.6. Automaticcross-showspeaker
Input DER(%)
identification
Reference 23.8
Thecorpusaimsatpresentingatleastthreeminutesof
VAD 24.5
speech by speaker. For most documents, the manual
VAD+OVL 21.3
identificationdescribedin3.5wasenoughbecausethe
VAD+NSE 16.5
documentswerechosentomaximisethespeakingtime
CSD 14.7
of the target speakers. However, it is not the case for
Table 4: Performance of the diarization system (mea- alldocuments. Then,thesegmentslinkedtothetarget
suredbyDER)onDIHARDIIdevsetforthedifferent speakerinthemanuallyannotateddocumentwereused
stagesofpre-processing. (collar=0.25) asareferencetoautomaticallyidentifythisspeakerin
otherdocuments.
Using VBx x-vector extraction model, we retrieve
this system targets should contain more clean speech one x-vector per segment corresponding to the target
than DIHARD II documents. So the process shall re- speakerintheannotateddocument,givingusamatrix
moveasmallerpartofthecontentonsuchdocuments. x . The non-annotated document was then pre-
known
processedandautomaticallydiarizedinthesameway,
3.4. DiarizationwithVBx
andonex-vectorpersegmentofthisdocumentwasex-
We used our CSD as an input VAD for diarization, tracted, giving us a matrix x . Cosine similarity
target
meaningthatcleanspeechisconsideredasspeechand was computed between all the vectors in x and
known
non-clean speech as non-speech. We use the x-vector x . Foreachvectorinx ,themeansimilarity
target target
based diarization system VBx (Landini et al., 2022) tothex vectorsgivestheprobabilityscoreofthe
known
with the ResNet101 16kHz model, pretrained on vectorcorrespondingtothetargetspeaker. Ifthisscore
VoxCeleb1 (Nagrani et al., 2017), VoxCeleb2 (Chung exceedsagiventhreshold,weconsideredthattheseg-
et al., 2018) and CN-CELEB (Fan et al., 2020). The ment corresponds to the target speaker. If no segment
diarization step outputs clusters id corresponding to a hadascoreabovethethreshold,weconsideredthetar-
uniquespeaker. get speaker absent from the document. We chose to
We have evaluated the VBx model using our CSD on focus on segment-level identification in order to max-
theDIHARD IIDevelopment set(Ryant etal., 2019). imize the precision, even though it may increase the
Table4showstheDiariationErrorRate(DER)forthe falsenegativerate.
different stages of pre-processing. The DER is com- WeevaluatedourspeakeridentificationsystemonINA
puted by removing non-clean segments from the ref- speakerdictionary(Valletetal., 2016)whichcontains
erence. As expected, we observe a better DER when materials extracted from French TV archives and is
non-clean speech segments are removed, the diariza- similar to the contents our system was designed for.
tiontaskbeingeasier. WeobtainedaDERof14.7with This dataset contains about 1300 speakers extracted
a0.25scollarusingourpre-processingpipeline,which fromFrenchTVbroadcasts. Weanalysethesimilarity
iscomparabletotheDERof12.23%obtainedby(Lan- scorebetweenthreedifferenttypesofrecordingpairs:
dinietal.,2022)withoracleVAD. the same speaker in two different recording sessions,
twodifferentspeakersofthesamegenderandtwodif-
3.5. Manualspeakeridentification
ferentspeakersofdifferentgender. Thiswasevaluated
Each audiovisual document of the source corpus was on a gender-balanced subset of 718 speakers allowing
associated toa list of targetspeakers with known age, foreachspeakertobeinatleasttworecordingsessions,Histogram of similarity scores for different types of pairs for the complete corpus, that would amount to about
of speakers from the INA speaker dictionary 50 hours of speech). In order to subjectively evalu-
5 Same person ate the quality of the automatic process, and as it is
Different person - same sex
Different person - different sex hardly feasible to listen to the complete corpus, we
4 opted for applying a perceptual annotation on a sub-
setoftheavailableextracts. Thesubsetwascomposed
3
by a random selection of one segment for each target
speaker; these segments were annotated for the pres-
2
enceofthefollowingpotentialproblems:backchannel,
1 more than one person speaking, musical background,
and audible noise. Backchannel was defined as up to
0 twosyllablesproducedbyanotherspeakerthanthetar-
0.0 0.2 0.4 0.6 0.8 1.0
Similarity score get; if two speakers spoke more than two syllables, it
wasannotatedasmorethanonepersonspeaking. Au-
Figure 2: Histogram of similarity scores for different
dible background music or noise, while listening the
typesofpairsofspeakersfromtheINAspeakerdictio-
extractswithheadphones,wereannotatedassuch.
nary
The available extracts were divided in three parts, as-
signed to three different annotators, with a 309-large
subsetannotatedbythethreeannotators. Theextracts
Precision and Recall over threshold value
ofthecommonpartwereselectedsoastoproposeupto
1.000
10extractspercategoryofperiod,genderandage(let’s
0.975
recallweaimedatcollecting30speakerspercategory);
0.950 thisshallamountto320extractsifasufficientnumber
0.925 ofspeakerswereavailableineachcategory,whichwas
0.900 not the case. The females of 51 to 65 year-old in the
1955-1956 period, and over 65 year-old in the 1975-
0.875
1976periodwereonly5and4inthecorpuscurrently
0.850
Precision available,sothefinalnumberof309(seedetailsinthe
0.825 Recall resultsectionforavailablespeakers).
35 40 45 50 55 60 Apythonnotebookwasusedtorandomlyloadoneex-
Threshold value
tract,playit,andrequesttheannotatortoanswerase-
ries of letters indicating the presence of the potential
Figure3:PrecisionandRecalloverthresholdvalueson
problems. Afieldforfreecommentswasavailable,be-
theINAspeakerdictionary
forehearingthenextextract.
endingupwith359pairsforeachtype.Themeandura- 4. Results
tionoftheusedsegmentswas14s. Figure2showsthe
4.1. Obtainedtargetspeakers
distribution of similarity scores for the different types
ofpairs.Thereisfewoverlapbetweenthescorescorre- Each target speaker was manually identified for one
spondingtothesamespeakerandthosecorresponding archive. This manual identification work took about
to different speakers. There is very little (4%) over- 140 hours along 20 days. The available speakers are
lapbetweenscoresforthesamespeakerandscoresfor detailed in table 5. Note that it corresponds to more
speakersofdifferentgenders. than40hoursoffinalspeech(morethan3minutesfor
Speaker identification was evaluated according to the 874speakers). Atotalof533maleand341femaletar-
Equal Error Rate (EER). We obtain an EER of 3.9%3 getshavebeenidentified–thus874differentspeakers.
ontheINAspeakerdictionaryforasimilaritythreshold Sixteen categories of speakers out of 32 did not reach
equalto0.40. However,becauseprecisionismoreim- the aim of at least thirty speakers. Four groups from
portantthanrecallhere,wedecidedtouseathreshold the1990’sand2010’s(3femalesgroups)werealmost
equal to 0.52, at which we obtain a precision of 0.99 complete, only missing a few speakers. For the other
andarecallof0.91(seeFigure3). groups, ten have between 10 to 20 speakers (four of
them are male groups), and two female groups have
3.7. Subjectivequalityevaluation only4and5speakers. Atotalof211speakers(22%of
At the end of this selection process, for each tar- ourtarget)aremissing,sotocompleteallgroupswith
get speaker, series of speech segments were available at least 30 speakers. The sixteen groups with at least
amounting to at least three minutes by speakers (i.e. 30speakerspresent125extraspeakers.
For each category, more than thirty targets have been
3Previous work on the same corpus(Vallet et al., 2016) searched for, but for a series of reasons, in some
obtainedanEERof7.3%usinganequivalentevaluationpro- cases therequirements werenot met. These problems
tocol. overwhelmingly arise for female targets, known to be20-35 36-50 51-65 >65 of them reported it. The number of each error on 874
1955-56 13/34 17/61 5/19 17/10 utterancesarereportedintherightpartoftable6. The
1975-76 16/14 18/37 11/31 4/11 percentages of these four types of potential problems,
1995-96 30/27 32/47 29/48 29/35 on the complete corpus, and for each category of pe-
2015-16 31/30 29/51 30/48 30/30 riod,gender,andage,arereportedintable7.
Table 5: Number of speakers with sufficient data ex- Commonsub-part Total
tractedforeachperiod(row)andagegroup(columns),
Annotator A1 A2 A3
forgender(Female/Male).Categorieswithlessthan30
Backchannel 57 82 44 148
speakersshowninboldface.
SeveralSpk. 5 15 21 33
Music 19 17 15 33
Noise 30 51 37 72
under-represented in media (Doukhan et al., 2018b),
Table6: Numberofproblems,foreachcategory,spot-
andforarchivesfromthe1950’sand1970’s–forwhich
ted by the three annotators on the 309 common sen-
documentation and quality is worst. These missing
tencesoftheperceptualevaluation(leftpart),oronthe
speakersaremostlylinkedtothefollowingfactors: (i)
completeset(rightpart).
the target appears in the notice, but was the topic of a
programwithoutactuallyappearing, orthetargetmay
not speak (or not sufficiently), or spoke in a foreign
Theamountofobservedbackchannelsisstableacross
language and was interpreted. (ii) the target may be
gender and age (at about 17%), but is higher for the
interviewed in a noisy place (and was not detected as
1970’s, and clearly lower for the 1950’s. Presence of
cleanspeech),orthetargetvoicemayappearduringa
more than one speaker in one extract is much lower
movie trailer (and thus do not fit with our criterion of
(meanbelow4%),anddoesnotseemtobecorrelatedto
conversationalspeech).
gender, while the same pattern on periods is observed
Amainreasonmoremaletargetswerefoundislinked
(clearlyhigherforthe70’s,lowerforthe50’s). Apat-
tothefactthatmostdocumentshaveseveraltargets(fe-
ternforageappears: theolderthetarget, thelesssev-
male and male) so working on a document that fea-
eralspeakerswerefound.
turesafemaletargetgenerallyledtotheidentification
of one or more male targets, while the reverse is not
Bac SSp Mus Noi Any
true. Moreover, female targets are more prone to be
Globally 16.9 3.8 3.8 8.2 29.7
presentedbymalespeakers,withoutactuallyspeaking.
1955-56 9.1 1.7 0.6 9.1 19.3
Atypicalcaseisprogramsaboutcinemainthe1970’s,
1975-76 21.8 9.2 12.7 26.1 55.6
thatinterviewedthedirector(generallyamale),butjust
1995-96 17.7 2.5 3.2 3.2 26.4
presentthefemaleactressduringextractsofthemovie.
2015-16 18.6 3.6 1.8 3.6 26.5
Forthesereasons,themalecategoriesgenerallypresent
Female 18.5 3.5 4.4 10.0 32.6
wellabove30targetspeakers;notethatagealsointro-
Male 15.9 3.9 3.4 7.1 28.0
ducesbiasintermsofpresenceinthemedia.
20-35 17.9 5.1 2.6 9.2 29.7
36-50 15.1 5.1 3.8 8.9 30.5
4.2. Perceptualevaluationofspeechquality
51-65 17.2 2.7 5.4 6.3 28.5
On a subset of 309 extracts, the three annotators did
Over65 18.7 1.2 3.0 8.4 30.1
evaluate the four scales. The number of extracts de-
tected by each annotator with each type of potential Table 7: Percentage of utterance detected with each
problemisreportedontheleftpartoftable6. Thecor- categoryofpotentialproblem(Bac:Backchannel,SSp:
responding inter-annotator agreement, measured with Severalspeakers,Mus: Music,Noi: Noise,Any: pres-
an exact Fleiss’ kappa (Fleiss, 1971; Gamer et al., ence of at least one error in the extract), globally, and
2019), equals 0.629 for backchannel, 0.569 for more foreachcategoryofperiod,gender,andage.
than one speaker, 0.855 for music, 0.448 for noise –
thisamounttoakappaof0.649forfindingapotential
probleminagivenextract. Thesekappavaluesshows The presence of music varies mostly with the period,
the relative reliability of the annotation, especially for the 1970’s having about 13% of its extracts with per-
musicandbackchannel. Thecomparativelylowkappa ceivable musical background, while the other periods
for noise show that noise is a more complex concept havelowerpercentages. Onlyreducedchangesonthe
than music, and that the three annotators have some- presenceofmusicareobservedwithageandgender.
howdifferentviewsonwhatisanoisyextract. Noise is about twice more frequent than music. Like
From this common ground of 309 extracts, the results music, noise is particularly frequent in the 1970’s
onthecompletesetof874utteranceswasgrouped.For (26%) but, unlike music, is also relatively frequent in
the utterance evaluated by three annotators, the pres- the1950’s,whilelowlevelsofnoiseareobservedinthe
ence of a problem was considered only if at least two twomorerecentperiods. Slightchangesofnoisepres-enceareobservedacrossgenderandagecategories. more complex question: we have seen its presence is
Thepercentageofextractsthatdopresentatleastone difficult to annotate. This is certainly linked with the
potential problem of 30% globally. This percentage complexityofdefiningnoise, thatmaybeanyaudible
variesmostlyacrossperiods,morethanhalfoftheex- sound added to the soundtrack but speech and music
tracts from the 1970’s being annotated with potential (e.g.streetnoise,steps,naturalnoises),butalsosounds
problems, while one on five extracts from the 1950’s linked to the recording place (echo from the room),
hasapotentialproblem. from the recording equipment, from the many hard-
ware used to archive the media (disk, tape), or from
5. Discussion&Conclusions compression used to store audio files. The fact noises
are more present in the 1970 cue for high presence of
We proposed a semi-automatic method to help select-
addednoise, becausetheselectedprogramshappento
ing speakers with known characteristics (here in term
have more outdoor recordings, for example (observa-
of age and gender) in large media archives, avoiding
tion made during the manual target spotting). On the
silence,noiseandmusicalbackground.
contrary, the lower levels of noise in the 1950’s com-
About 140 hours of work was invested to manually
pared to the 70’s (somehow counterintuitive) shows
identify915targetspeakers.Amongthesespeakers,41
archive processing at INA shall not introduce major
wereeithernotfoundinthedocumentsordonotspeak
bias–evenifmorerecentmediahavebetterquality.
sufficientlytobuildamodeloftheirvoices(32ofthese
Noiseandmusicpotentialeffectsonthetargetedmea-
speakershadlessthan20secondsofannotatedspeech).
sures will be evaluated once the corpus is completed,
Theworkrequiredfortheapplicationoftheautomatic
but this is outside the scope of this paper. The pre-
processing scripts, and for processing the files (time
sentedmethodologymayapplyfortheconstructionof
spent by human, not by machine processing) was es-
corporadedicatedtoe.g.,sociologicalwork,thatdonot
timatedto20hoursforthecorpuspresentedhere. The
necessarilyrequirehighsoundquality.
complete set of archives used to obtain the current set
Presence of several speakers in one extract is an un-
ofspeakershadatotaldurationof453hours.
wanted feature, and ideally these extracts shall be re-
Bazillon et al. (2008) measured the time required to
moved. It’ll be important to screen the extracts la-
manuallytranscribespontaneousspeechateighttimes
belled as such to estimate the relative ratio of extracts
the duration of the target speech; manual diarization
with completely different speakers, compared to ex-
processismoresimplethanafulltranscription,anesti-
tractsfeaturingspeakerswithcomparablevoices. The
mateoffourtimesthearchivedurationformanualpro-
secondcaseislessproblematicthantheformer.During
cessingseemsreasonable.Duetothecomplexityofthe
informaltestingtosetuptheperceptualevaluation,one
targetspeakeridentificationprocessonly(thatmayre-
extract was labelled as featuring two different speak-
quireanalmostcompletelisteningofthearchive),this
ers only by one of the three annotators: the other two
estimationdoesnotseemunreasonable.
hadn’tnoticed–onlythedialogue’ssemanticsallowed
Giventheseestimations,wecanassumethemanualex-
judging there were two speakers. The fact the kappa
traction of the target speech data from 453 hours of
for this measure was 0.57, shows in a good deal of
archives would have required at least more than the
the cases, the difference in voices was not spotted by
viewingtimeandupto1800hoursofhumanlabor.Us-
allannotators,whichpleadsforsimilaritybetweenthe
ingtheproposedmethod,ittookabout160hours–i.e.
voices.Theevaluationofthespeakeridentificationsys-
fourtotentimeslessthanamanualannotation,which
tem, withhighersimilaritywithinthanacrossgenders
seemstobeafairlyefficientmethod. Bycomparison,
showsitisaprobableoutcome.
butnotonthesametask,thesemi-automatictranscrip-
Identification work will continue until having a com-
tionmethodproposedbyBazillonetal. (2008)cutby
plete set of speakers. Then, evaluations of the output
halfthemanualprocessingtime.
quality will be applied, with estimation of signal-to-
Theperceptualevaluationofpotentialproblemsshows
noise ratios, potential distortion of acoustic measure-
thatoneextractinthreehasatleastonepotentialprob-
ments due to music background, etc. This corpus has
lem. Thismayseemarelativelyhighrate;meanwhile,
avocationtobesharedviaINA’sonlineresourceman-
annotationofbackchannelamountsforabouthalfthis agement system4, once the project will be over. The
number(seetable6andshallnotbeaproblemforthe
software developed to apply the processing described
targeted analyses, as backchannels are very short re-
hereshallalsobemadepublicinthefuture,aftercon-
gardingto theduration ofextracts (about0.1svs. 10s
solidationoftheiruseonothercorpusbuilding.
forthemeandurationofannotatedextracts): thisshall
not affect the voice’s spectral characteristics or mean
6. Acknowledgements
pitch values. Presence of music, thanks to the music
detection process, was limited, compared to the fre- ThisworkhasbeenpartiallyfundedbytheFrenchNa-
quentuseofmixedmusicinradioandTVshows(see tionalResearchAgency(projectGenderEqualityMon-
the evaluation part). Moreover, even if audible wear- itor-ANR-19-CE38-0012).
ing a headset, the levels of music that were annotated
are low compared to the levels of voices. Noise is a 4https://dataset.ina.fr/7. BibliographicalReferences Cn-celeb: A challenging chinese speaker recogni-
tiondataset. InICASSP2020,pages7604–7608.
Arantes, P. and Eriksson, A. (2014). Temporal stabil-
Fleiss, J. L. (1971). Measuring nominal scale agree-
ityoflongtermmeasuresoffundamentalfrequency.
ment among many raters. Psychological Bulletin,
In Speech Prosody 2014, pages 1149–1152. ISCA,
76(5):378–382.
May.
Gamer, M., Lemon, J., and Singh, I. F. P., (2019).
Barras, C., Allauzen, A., Lamel, L., and Gauvain, J.-
irr:VariousCoefficientsofInterraterReliabilityand
L. (2002). Transcribing audio-video archives. In
Agreement. Rpackageversion0.84.1.
ICASSP’02, pages I–13–I–16, Orlando, FL, USA,
Geiselman,R.E.andCrawley,J.M. (1983). Inciden-
May.IEEE.
tal processing of speaker characteristics: voice as
Bazillon,T.,Este`ve,Y.,andLuzzati,D. (2008). Man-
connotative information. Journal of Verbal Learn-
ualvsassistedtranscriptionofpreparedandsponta-
ingandVerbalBehavior,22(1):15–23,February.
neousspeech. InLREC’08.
Goudbeek, M. and Scherer, K. (2010). Beyond
Boula de Mareu¨il, P., Rilliard, A., and Allauzen, A.
arousal: Valenceandpotency/controlcuesinthevo-
(2012). A Diachronic Study of Initial Stress and
calexpressionofemotion. TheJournaloftheAcous-
other Prosodic Features in the French News An-
ticalSocietyofAmerica,128(3):1322.
nouncer Style: Corpus-based Measurements and
Gussenhoven, C. (2016). Foundations of Intonational
Perceptual Experiments. Language and Speech,
Meaning: Anatomical and Physiological Factors.
55(2):263–293,June.
TopicsinCognitiveScience,8(2):425–434,April.
Bredin,H.,Yin,R.,Coria,J.M.,Gelly,G.,Korshunov,
Guzman, M., Mun˜oz, D., Vivero, M., Mar´ın, N.,
P., Lavechin, M., Fustes, D., Titeux, H., Bouaziz,
Ram´ırez, M., Rivera, M. T., Vidal, C., Gerhard, J.,
W., and Gill, M.-P. (2020). pyannote.audio: neural
and Gonza´lez, C. (2014). Acoustic markers to dif-
building blocks for speaker diarization. In ICASSP
ferentiate gender in prepubescent children’s speak-
2020, IEEE International Conference on Acoustics,
ingandsingingvoice. InternationalJournalofPedi-
Speech, andSignalProcessing, pages7124–7128,
atric Otorhinolaryngology, 78(10):1592–1598, Oc-
Barcelona,Spain,May.
tober.
Broux, P.-A., Doukhan, D., Petitrenaud, S., Meignier, Hennequin, R., Khlif, A., Voituret, F., and Moussal-
S., and Carrive, J. (2018). Computer-assisted lam, M. (2020). Spleeter: a fast and efficient mu-
speakerdiarization: Howtoevaluatehumancorrec- sic source separation tool with pre-trained models.
tions. In Proceedings of the Eleventh International JournalofOpenSourceSoftware,5(50):2154.
ConferenceonLanguageResourcesandEvaluation
Jang, H. (2021). How cute do I sound to you?: gen-
(LREC2018).
derandageeffectsintheuseandevaluationofKo-
Bullock, L., Bredin, H., and Garcia-Perera, L. P. reanbaby-talkregister, Aegyo. LanguageSciences,
(2020). Overlap-aware diarization: resegmentation 83:101289,January.
using neural end-to-end overlapped speech detec- Landini, F., Profant, J., Diez, M., and Burget, L.
tion. In ICASSP 2020, IEEE International Confer- (2022). Bayesian hmm clustering of x-vector se-
ence on Acoustics, Speech, and Signal Processing, quences(vbx)inspeakerdiarization: theory,imple-
pages7114–7118,Barcelona,Spain,May. mentationandanalysisonstandardtasks. Computer
Chung, J. S., Nagrani, A., and Zisserman, A. (2018). Speech&Language,71:101254.
Voxceleb2: Deep speaker recognition. In INTER- Lie´nard, J.-S. and Di Benedetto, M.-G. (1999). Ef-
SPEECH,pages1086–1090. fectofvocaleffortonspectralpropertiesofvowels.
Doukhan, D., Carrive, J., Vallet, F., Larcher, A., The Journal of the Acoustical Society of America,
and Meignier, S. (2018a). An open-source speaker 106(1):411–422,July.
gender detection framework for monitoring gender Lie´nard, J.-S. (2019). Quantifying vocal effort from
equality. InICASSP2018,pages5214–5218.IEEE. theshapeoftheone-thirdoctavelong-term-average
Doukhan, D., Poels, G., Rezgui, Z., and Carrive, J. spectrum of speech. The Journal of the Acoustical
(2018b). Describing gender equality in french au- SocietyofAmerica,146(4):EL369–EL375,October.
diovisual streams with a deep learning approach. Lo¨fqvist, A. and Mandersson, B. (1987). Long-Time
VIEW Journal of European Television History and Average Spectrum of Speech and Voice Analysis.
Culture,7(14):103–122. FoliaPhoniatricaetLogopaedica,39(5):221–229.
Eshkol-Taravella,I.,Baude,O.,Maurel,D.,Hriba,L., Mesaros, A., Heittola, T., and Virtanen, T. (2016).
Dugua, C., and Tellier, I. (2011). Un grand corpus Metrics for polyphonic sound event detection. Ap-
oral “ disponible ” : le corpus d’Orle´ans 1 1968- pliedSciences,6(6):162.
2012. RevueTAL,53(2):17–46. Publisher: ATALA Nagrani, A., Chung, J. S., and Zisserman, A.
(Association pour le Traitement Automatique des (2017). Voxceleb: a large-scale speaker identifica-
Langues). tiondataset. InINTERSPEECH,pages2616–2620.
Fan, Y., Kang, J., Li, L., Li, K., Chen, H., Cheng, S., Niebuhr, O., Voße, J., and Brem, A. (2016). What
Zhang, P., Zhou, Z., Cai, Y., and Wang, D. (2020). makes a charismatic speaker? A computer-basedacoustic-prosodic analysis of Steve Jobs tone of Vallet,F.,Uro,J.,Andriamakaoly,J.,Nabi,H.,Derval,
voice. ComputersinHumanBehavior,64:366–382, M.,andCarrive,J. (2016). Speechtrax:Abottomto
November. the top approach for speaker tracking and indexing
Ohara, Y. (1992). Gender-dependent pitch levels: A in an archiving context. In LREC’16, pages 2011–
comparativestudyinJapaneseandEnglish. InKira 2016.
Hall, etal., editors, Locatingpower.Proceedingsof van Bezooijen, R. (1995). Sociocultural Aspects
the Second Berkeley Women and Language Confer- of Pitch Differences between Japanese and Dutch
ence, volume 2, pages 469–477. Berkeley Women Women. Language and Speech, 38(3):253–265,
and Language Group. Backup Publisher: Berkeley July.
WomenandLanguageGroup. Vorperian,H.K.,Wang,S.,Schimek,E.M.,Durtschi,
Ohara, Y. (2001). Finding one’s voice in Japanese: R.B.,Kent,R.D.,Gentry,L.R.,andChung,M.K.
A study of the pitch levels of L2 users. In Aneta (2011). Developmental Sexual Dimorphism of the
Pavlenko, et al., editors, Multilingualism, Second OralandPharyngealPortionsoftheVocalTract: An
Language Learning, and Gender. DE GRUYTER Imaging Study. Journal of Speech, Language, and
MOUTON,Berlin,NewYork,January. HearingResearch,54(4):995–1010,August.
Podesva, R. J. (2007). Phonation type as a stylistic Yamauchi, A., Yokonishi, H., Imagawa, H., Sakak-
variable: The use of falsetto in constructing a per- ibara, K.-I., Nito, T., Tayama, N., and Yama-
sona. Journal of Sociolinguistics, 11(4):478–504, soba, T. (2015). Quantitative Analysis of Dig-
September. ital Videokymography: A Preliminary Study on
Sadanobu, T. (2015). ”Characters” in Japanese Com- Age- and Gender-Related Difference of Vocal Fold
municationandLanguage: AnOverview. ActaLin- Vibration in Normal Speakers. Journal of Voice,
guisticaAsiatica,5(2):9–28,December. 29(1):109–119,January.
Salmon,F.andVallet,F. (2014). Aneffortlesswayto
8. LanguageResourceReferences
create large-scale datasets for famous speakers. In
LREC’14,pages348–352. Baude, Olivier. (2019). Corpus d’Orle´ans. Centre
Sataloff,R.T.,Kost,K.M.,andLinville,S.E. (2017). Orle´anais de Recherche en Anthropologie et Lin-
Chapter 13. The Effects of Age on the Voice. In guistique.
Robert Thayer Sataloff, editor, Clinical assessment Galliano, Sylvain and Gravier, Guillaume and
ofvoice,pages221–240.PluralPublishing,Inc,San Chaubard, Laura. (2012). The ESTER 2 evaluation
Diego,CA,secondeditionedition. campaignfortherichtranscriptionofFrenchradio
Scott, S. K. (2022). The neural control of voli- broadcasts. ELRA.
tional vocal production—from speech to identity, Gravier, Guillaume and Adda, Gilles and Paulson,
from social meaning to song. Philosophical Trans- NiklasandCarre´,MatthieuandGiraudel,Audeand
actionsoftheRoyalSocietyB:BiologicalSciences, Galibert,Olivier. (2012). TheETAPEcorpusforthe
377(1841):20200395,January. evaluationofspeech-basedTVcontentprocessingin
Sergeant, D. C. and Welch, G. F. (2009). Gender theFrenchlanguage. ELRA.
DifferencesinLong-TermAverageSpectraofChil- Mele´ndez-Catala´n, Blai and Molina, Emilio and
dren’sSingingVoices. JournalofVoice,23(3):319– Go´mez,Emilia. (2019). OpenBroadcastMediaAu-
336,May. dio from TV (OpenBMAT). distributed via Zenodo:
Sloetjes, H. and Wittenburg, P. (2008). Annotation 10.5281/zenodo.3381249.
by category-elan and iso dcr. In 6th international Ryant,NevilleandChurch,KennethandCieri,Christo-
ConferenceonLanguageResourcesandEvaluation pherandCristia,AlejandrinaandDu,JunandGana-
(LREC2008). pathy, Sriram and Liberman, Mark. (2019). The
Suire, A. and Barkat-Defradas, M. (2020). Evolution Second DIHARD Diarization Challenge: Dataset,
ofhumanpitch: PreliminaryanalysesintheFrench Task,andBaselines. LDC.
population using INA audiovisual archives of Vox
Pops. In 2020 IASA - FIAT/IFTA Joint Conference,
Dublin,France,October.
Titze,I.R.andSundberg,J. (1992). Vocalintensityin
speakersandsingers. TheJournaloftheAcoustical
SocietyofAmerica,91(5):2936–2946,May.
Traunmu¨ller,H.andEriksson,A. (2000). Acousticef-
fectsofvariationinvocaleffortbymen,women,and
children. The Journal of the Acoustical Society of
America,107(6):3438–3451,June.
Tuchman, G. (2000). The symbolic annihilation of
womenbythemassmedia. InCultureandPolitics,
pages150–174.PalgraveMacmillanUS.