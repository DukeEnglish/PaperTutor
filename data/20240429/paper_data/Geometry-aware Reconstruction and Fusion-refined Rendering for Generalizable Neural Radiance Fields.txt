Geometry-aware Reconstruction and Fusion-refined Rendering
for Generalizable Neural Radiance Fields
TianqiLiu XinyiYe MinShi ZihaoHuang ZhiyuPan ZhanPeng ZhiguoCao*
SchoolofAIA,HuazhongUniversityofScienceandTechnology
{tq liu,xinyiye,min shi,zihaohuang,zhiyupan,peng zhan,zgcao}@hust.edu.cn
Abstract
(a) Generalizable NeRF aims to synthesize novel views for
unseen scenes. Common practices involve constructing
variance-based cost volumes for geometry reconstruction
Source views MVSNeRF ENeRF Ours
and encoding 3D descriptors for decoding novel views. 24.5
However, existing methods show limited generalization PSNR_LLFF ↑ 0.14s IBRNet
24
abilityinchallengingconditionsduetoinaccurategeome- MVSNeRF
23.5
try, sub-optimal descriptors, and decoding strategies. We NeuRay
(b) 23 7.14s ENeRF
address these issues point by point. First, we find the
74.38s 0.04s GNT
variance-basedcostvolumeexhibitsfailurepatternsasthe 22.5
0.96s MatchNeRF
features of pixels corresponding to the same point can 22
be inconsistent across different views due to occlusions 4.61s 2.40s PSNR_DTU ↑ Ours
21.5
or reflections. We introduce an Adaptive Cost Aggrega- 25.5 26.5 27.5 28.5 29.5
tion (ACA) approach to amplify the contribution of con-
Figure1. Comparisonwithexistingmethods. (a)Withthreein-
sistent pixel pairs and suppress inconsistent ones. Un- putsourceviews,ourgeneralizablemodelsynthesizesnovelviews
like previous methods that solely fuse 2D features into withhigherqualitythanexistingmethods[5,20]inthesevereoc-
descriptors, our approach introduces a Spatial-View Ag- cludedarea.(b)Circlearearepresentsinferencetime.TheX-axis
gregator (SVA) to incorporate 3D context into descriptors representsthePSNRontheDTUdataset[1]andtheY-axisrep-
through spatial and inter-view interaction. When decod- resents the PSNR on the Real Forward-facing dataset [24]. Our
ing the descriptors, we observe the two existing decoding methodattainsstate-of-the-artperformance.
strategies excel in different areas, which are complemen-
tary. A Consistency-Aware Fusion (CAF) strategy is pro-
posed to leverage the advantages of both. We incorporate foreachscene,whichlimitsitsapplications.
theaboveACA,SVA,andCAFintoacoarse-to-fineframe-
Toaddressthisissue, somerecentmethods[5,7,9,15,
work,termedGeometry-awareReconstructionandFusion-
19, 20, 23, 30–32, 38, 44] generalize NeRFs to unseen
refined Rendering (GeFu). GeFu attains state-of-the-art
scenes. Instead of overfitting the scene, they extract fea-
performanceacrossmultipledatasets. Codeisavailableat
ture descriptors for 3D points in a scene-agnostic manner,
https://github.com/TQTQliu/GeFu.
which are then decoded for rendering novel views. Pio-
neermethods[32,44]utilize2Dfeatureswarpedfromthe
sourceimages. Asthispracticeavoidsexplicitmodelingof
1.Introduction
3D geometric constraints, its generalization capability for
geometry reasoning and view synthesis in new scenes is
Novel view synthesis (NVS) aims to generate realistic im-
limited. Hence, subsequent methods [5, 20] introduce ex-
agesatnovelviewpointsgivenasetofposedimages.Byen-
plicit geometry-aware cost volume from multi-view stereo
codingthedensityandradiancefieldsofscenesintoimplicit
(MVS)tomodelgeometryatthenovelview.Despitesignif-
representations, Neural Radiance Field (NeRF) [25] has
icantprogressachievedbythesemethods,synthesisresults
shownimpressiveperformanceinNVS.However,NeRFre-
remainunsatisfactory,especiallyinchallengingareas,such
quiresalengthyoptimizationwithdenselycapturedimages
as the occluded area illustrated in Fig. 1 (a). The model
*Correspondingauthor strugglestoaccuratelyinferthegeometryoftheseregions,
4202
rpA
62
]VC.sc[
1v82571.4042:viXraasvariance-basedcostvolumescannotperceiveocclusions. Error: (a) < (b) Error: (a) > (b)
Generalizable NeRFs’ pipeline consists of two phases:
radiancefieldreconstructionandrendering.Thereconstruc-
tionphaseaimstorecoverscenegeometryandencode3D-
aware features for rendering, i.e., creating descriptors for
3Dpoints. Forgeometryreasoning,similartoMVS,ifthe
(a) Blending (b) Regression (c) Comparison
geometry (i.e., depth) is accurate, features across various
viewsaresupposedtobesimilarwithlowvariance. How- Figure2.Comparisonoftworenderingstrategies.(a)Theview
ever,thisvariance-basedcostmetricisnotuniversal,espe- obtainedusingtheblendingapproachthatcombinescolorvalues
cially in occluded and reflective regions. Due to inconsis- from source views. (b) The view obtained using the regression
tent features in these regions, equally considering the con- approach that directly regresses color values from features. (c)
tributionsofdifferentviewsisunreasonable,leadingtomis- Accuracycomparisonbetweentworenderingstrategies. Thefor-
leadingvariancevalues.InspiredbyMVSmethods[33,35], merstrategyperformsbetterinthewhiteregions,whileworsein
thegreenones.
we propose an Adaptive Cost Aggregation (ACA) mod-
ule.ACAadaptivelyreweightsthecontributionsofdifferent
viewsbasedonthesimilaritybetweensourceviewsandthe
ple that if color values are close to the ground truth, the
novelview. Sincethenovelviewisunavailable,weemploy
multi-view features corresponding to the correct depth are
acoarse-to-fineframeworkandtransfertherenderingview
supposedtobesimilar.
from the coarse stage into the fine stage to learn adaptive
By embedding the above ACA, SVA, and CAF into a
weightsforthecostvolumeconstruction.
coarse-to-fine framework, we propose GeFu. To demon-
Withthegeometryderivedfromthecostvolume,wecan
strate the effectiveness, we evaluate GeFu on the widely-
re-sample3Dpointsaroundthesurfaceandencodedescrip-
used DTU [1], Real Forward-facing [24], and NeRF Syn-
tors for sampled points. For descriptors encoding, previ-
thetic [25] datasets. Extensive experiments show that
ous methods [7, 20, 32, 44] directly aggregate inter-view
GeFuoutperformsothergeneralizableNeRFsbylargemar-
features into descriptors for subsequent rendering. These
gins without scene-specific fine-tuning as shown in Fig. 1
descriptorslack3Dcontextawareness,leadingtodisconti-
(a)&(b). After per-scene fine-tuning, GeFu also outper-
nuities in the descriptor space. To this end, we design the
formsothergeneralizableNeRFsandachievesperformance
Spatial-View Aggregator (SVA) to learn 3D context-aware
comparabletoorevenbetterthanNeRF[25]. Additionally,
descriptors.Specifically,weencodespatiallycontext-aware
GeFuiscapableofgeneratingreasonabledepthmaps,sur-
and smooth features by aggregating 3D spatial informa-
passingothergeneralizationmethods.
tion. Meanwhile, to preserve geometric details, we utilize
Ourmaincontributionscanbesummarizedasfollows:
smoothedfeaturesasqueriestoreassemblehigh-frequency
• We propose ACA to improve geometry estimation
informationacrossviewstocreatefinaldescriptors.
and SVA to encode 3D context-aware descriptors for
Withthescenegeometryandpoint-wisedescriptors,the geometry-awarereconstruction.
subsequentrenderingphaseaimstodecodedescriptorsinto • We conduct an analysis of two existing color decoding
volumedensityandradianceforrenderinganovelview.For strategiesandproposeCAFtounifytheiradvantagesfor
the radiance prediction, [20, 32] predict blending weights fusion-refinedrendering.
tocombinecolorvaluesfromsourceviews,while [5,7,44] • GeFuachievesstate-of-the-artperformanceacrossmulti-
directly regress from features. However, the analysis of pledatasets,showingsuperiorgeneralization.
these two approaches has not been conducted in existing
works. Inthispaper,weobservethattheblendingapproach 2.RelatedWork
performsbetterinmostareas(Fig.2(a)), asthecolorval-
uesfromsourceviewsprovidereferentialfactors.However, Multi-View Stereo. Given multiple calibrated images,
asshowninFig.2(b)&(c),inchallengingareassuchasre- multi-view stereo (MVS) aims to reconstruct a dense 3D
flectionsandboundaries,theregressionapproachproduces representation. Traditional MVS methods [11, 12, 28, 29]
superiorresultswithfewerartifacts,whiletheblendingap- primarilyrelyonhand-craftedfeatures andsimilaritymet-
proachleadstosuboptimalrenderingduetounreliableref- rics, which limits their performance, especially in chal-
erentialfactors. Tounifytheadvantagesofbothstrategies, lenging regions such as weak-texture and repetitive ar-
weproposetoseparatelypredicttwointermediateviewsus- eas. Powered by the impressive representation of neu-
ingtwoapproachesanddesignaweightedstructurenamed ral networks, MVSNet [41] first proposes an end-to-end
Consistency-AwareFusion(CAF)todynamicallyfusethem cost volume-based pipeline, which quickly becomes the
intothefinalview.Thefusingweightsarelearnedbycheck- mainstream in the MVS community. Following works ex-
ingmulti-viewconsistency,followinganunderlyingprinci- plore the potential capacity of this pipeline from variousaspects. e.g., reducing memory consumption with recur- Generalizable NeRF. In generalizable NeRFs, each sam-
rentapproaches[35,39,42]orcoarse-to-fineparadigms[8, pled point is assigned a geometry-aware feature descriptor
14,40,45],enhancingfeaturerepresentations[10,22]and f , as (x,d,f ) →− (σ,r), where x and d represent the
p p
modeling output formats [27, 43]. Another important line coordinate and view direction used by NeRF [25]. σ and
is to optimize the cost aggregation [33, 35] by adaptively r denote the volume density and radiance for the sampled
weighting contributions from various views. In this paper, point, respectively. Specifically, the volume density σ can
following the spirit, we introduce the adaptive cost aggre- be obtained from the descriptor via σ = MLP(x,f ). For
p
gationtailoredfortheNVStasktomitigatetheissueofin- theradiancer,oneapproach[20,32]istopredictblending
consistentfeaturescausedbyreflectionsandocclusions. weightstocombinecolorvaluesfromsourceviews,as:
Generalizable NeRF. With implicit continuous represen-
tation and differentiable volume rendering, NeRF [25] r=(cid:88)N exp(w i)c i ,wherew =MLP(x,d,f ,fi), (2)
achieves photo-realistic view synthesis. However, NeRF
i=1
(cid:80)N
j=1exp(w j)
i p s
and its downstream expansion works [2, 3, 6, 26, 36, 37]
where {c }N are color values from source views. These
require an expensive per-scene optimization process. To i i=1
colors provide referential factors, facilitating convergence
addressthisissue,somegeneralizableNeRFmethodshave
and better performance in most areas. However, in oc-
been proposed, following a reconstruction-and-rendering
cluded and reflective regions, these colors introduce mis-
pipeline. In the reconstruction phase, each sampled point
leading bias to the combination, resulting in distorted col-
is assigned a feature descriptor. Specifically, according to
ors. Besides, for pixels on object boundaries, it is hard
thedescriptors,generalizableNeRFmethodscanbecatego-
to accurately locate reference points for blending, result-
rizedintothefollowingtypes: appearancedescriptors[44],
ing in oscillated colors between the foreground and back-
aggregated multi-view descriptors [20, 30, 32], cost vol-
ground. Another approach [5, 7, 44] is to directly regress
ume interpolated descriptors [5, 20, 23], and correspon-
the radiance from the feature per r = MLP(x,d,f ). As
dence matching descriptors [7]. Despite different forms, p
fewerinductivebiasesareimposedontheoutputspace,the
these descriptors only aggregate inter-view information or
modelcanlearntopredictfewerartifactsinchallengingar-
areinterpolatedfromthelow-resolutioncostvolume,lack-
eas(Fig.2). Withthevolumedensityandradianceofsam-
ingtheabilitytoeffectivelyperceive3Dspatialcontext. To
pledpoints,thecolorvaluescofeachpixelcanbecomputed
remedytheissue,weutilizeaproposedaggregatortofacil-
byvolumerendering,givenby:
itatetheinteractionofspatialinformation. Intherendering
phase,volumedensityisobtainedbydecodingdescriptors. k−1
(cid:88) (cid:88)
Forradiance, [20,32]predictblendingweightstocombine c= τ k(1−exp(−σ k))r k,whereτ k =exp(− σ j), (3)
color from source views, while [5, 7, 23, 30, 44] directly k j=1
regress features. In this paper, we observe that these two whereτ representsthevolumetransmittance.
strategies benefit different regions and thus propose a uni-
fiedstructuretointegratetheiradvantages. 4.Method
3.Preliminaries Given a set of source views {I si}N i=1, NVS aims to gener-
ate a target view at a novel camera pose. As illustrated in
Learning-basedMVS.GivenatargetimageandN source Fig.3,ourmethodconsistsofanNVSpipelinewrappedina
images, MVS aims to recover the geometry, such as the coarse-to-fineframework. Inthepipeline,wefirstemploya
depth map of the target image. The key idea of MVS is featurepyramidnetwork[21]toextractmulti-scalefeatures
toconstructthecostvolumefrommulti-viewinputs,aggre- fromthesourceviews. Then,weproposeanAdaptiveCost
gating2Dinformationinto3Dgeometry-awarerepresenta- Aggregationmodule(Sec.4.1)toconstructacostvolume,
tion. Specifically, each voxel-aligned feature vector f c of whichisfurtherprocessedbya3D-CNNtoinferthegeom-
costvolumecanbecomputedas: etry. Guided by the estimated geometry, we re-sample 3D
points around the surface and apply the Spatial-View Ag-
f =Ω(f ,f1,...,fN), (1)
c t s s gregator(Sec.4.2)toencode3D-awarefeaturedescriptors
wheref andfi representthetargetfeaturevectorandthe f forsampledpoints. Finally,wedecodef intotwointer-
t s p p
warpedsourcefeaturevector,respectively.AndΩdenotesa mediateviewsusingtwostrategiesintroducedinSec.3and
consistencymetric,suchasvariance. Theunderlyingprin- fuse them into the final target view through Consistency-
cipleisthatifasampleddepthisclosetotheactualdepth, AwareFusion(Sec.4.3). Thispipelineisiterativelycalled.
the multi-view features of the sampled point are supposed Initially,thelow-resolutiontargetviewisgeneratedandthe
to be similar, which naturally performs multi-view corre- roughscenegeometryiscaptured. Then,inthesubsequent
spondence matching and geometry reasoning, facilitating refiningstage,high-resolutionresultswithfine-grainedge-
thegeneralizationtounseenscenes. ometryareobtained.Geometry-aware Reconstruction Fusion-refined Rendering
Adaptive Cost Aggregation Spatial-View Aggregator Consistency-Aware Fusion
>1 w    …  sm    w
v v v
⋯ … Attention  d      p    p     
…

w Volume Rendering
… w ×     Wr Wb
v   p  (  )
p It
{    }   =1   =1


×
(  )
Ir Ib
w warp v variance p projection | | depth re-sampling
Figure 3. The overview of GeFu. In the reconstruction phase, we first infer the geometry from the constructed cost volume, and the
geometryguidesustofurtherre-sample3Dpointsaroundthesurface. Foreachsampledpoint,thewarpedfeaturesfromsourceimages
areaggregatedandthenfedintoourproposedSpatial-ViewAggregator(SVA)tolearnspatialandinter-viewcontext-awaredescriptorsf .
p
Intherenderingphase,weapplytwodecodingstrategiestoobtaintwointermediateviewsandfusethemintothefinaltargetviewinan
adaptiveway,termedConsistency-AwareFusion(CAF).Ourpipelineadoptsacoarse-to-finearchitecture,thegeometryfromthecoarse
stage(l = 1)guidesthesamplingatthefinestage(l > 1),andthefeaturesfromthecoarsestagearetransferredtothefinestagefor
s s
ACAtoimprovegeometryestimation.Ournetworkistrainedend-to-endusingonlyRGBimages.
4.1.AdaptiveCostAggregation pointsalongtheray,as:
The core process of geometry reasoning is to construct a (cid:88)
f = τ (1−exp(−σ ))fk. (5)
cost volume that encodes the multi-view feature consis- b k k p
k
tency. Previous works [5, 20] treat different views equally
and employ the variance operator to construct a cost vol- Thecoarse-stagefeaturesarethenfedintoa2DU-Netfor
ume. However, due to potential occlusions and varying spatial aggregation to obtain f . We replace f in Eq. (4)
r t
lightingconditionsamongdifferentviews, multi-viewfea- withthefeaturef toconstructarobustcostvolume,bene-
r
turesofthesame3Dpointmayexhibitnotabledisparities, fitingthegeometryestimation.
thereby resulting in misleading variance values. Inspired
by [35], we propose to adaptively weight the contribution 4.2.Spatial-ViewAggregator
ofdifferentviewstothecostvolume,termedAdaptiveCost
Withtheestimatedgeometry,3Dpointsaroundobjects’sur-
Aggregation (ACA). ACA will suppress the cost contribu-
facescanbere-sampled,andthesubsequentstepisencod-
tionfromfeaturesthatareinconsistentwiththenovelviews
ing descriptors for these sampled points. Existing meth-
caused by reflections or occlusions, and enhance the con-
ods [20, 32] aggregate inter-view features with a pooling
tribution of better-matched pixel pairs. The voxel-aligned
network ρ to construct the descriptors f = ρ({fi}N ).
featuref ofcostvolumecanbecomputedas: p s i=1
c However, these descriptors only encode multi-view infor-
mationandlacktheawarenessof3Dspatialcontext, lead-
N
f = 1 (cid:88) (1+α(fi))⊙fi,wherefi =(fi−f )2, (4) ingtodiscontinuitiesinthedescriptorspace. Tointroduce
c N c c c s t
3D spatial information, a feasible approach is to interpo-
i=1
latethelow-resolutionregularizedcostvolume,butitlacks
where f t and f si denote the target feature vector and the fine-grained details. To address these issues, we design a
warped feature vector of source image I si, respectively. ⊙ 3D-awaredescriptorsencodingapproach. Wefirstutilizea
denotes Hadamard multiplication and α(.) represents the 3DU-Nettermedϕ toaggregate3Dspatialcontext,as:
sm
adaptiveweightforeachview. However,animportantchal-
lengeisthatthetargetviewisavailableinMVS,butnotfor f =ϕ (ρ({fi}N )). (6)
sm sm s i=1
NVS.Toremedythis,weadoptacoarse-to-fineframework
where a coarse novel view is first generated and serves as However, this may lead to the smoothing of 3D features
thetargetviewinEq.(4).Specifically,weobtainthecoarse- and cause the loss of some high-frequency geometric de-
stagefeaturef byaccumulatingdescriptorsf ofsampled tails. Therefore,weusethesmoothedfeaturesasqueriesto
b p
3D-CNNre-gatherinter-viewhigh-frequencydetails,as: normalization. The final target view can be represented in
matrixformasI =W I +W I .
t b b r r
f =ϕ (f ,{fi}N ), (7)
p d sm s i=1
4.4.LossFunction
whereϕ isanattentionmodule. f istheinputqueryq,
d sm
Ourmodelistrainedend-to-endonlyusingtheRGBimage
and {fi}N are the key sequences k and value sequences
s i=1 as supervision. Following [20], we use the mean squared
v. The sequence lengths of q, k, and v are 1, N, and N,
errorlossas:
respectively,whichresultsinonlyaslightincreaseincom-
putationalcosts.
1
(cid:88)Np
L = ||cˆ −c ||2, (9)
4.3.Consistency-AwareFusion mse N i i 2
p
i=1
Withthedescriptorf ,thevolumedensityσisfirstacquired
p where N is the number of pixels and cˆ and c are the
through an MLP, and color values can be obtained using p i i
ground-truthandpredictedpixelcolor,respectively. Inad-
two decoding approaches discussed in Sec. 3. We observe
dition, the perceptual loss [46] and ssim loss [34] can be
that these two approaches exhibit advantages in different
applied,as:
areas.Tocombinetheirstrengths,weproposetopredicttwo
L =||h(Iˆ)−h(I)||,
intermediate views using these two approaches separately, perc
(10)
andthenfusethemintothefinaltargetview. L =1−ssim(Iˆ,I),
ssim
Specifically,fortheblendingapproach,theradiancerof
each sampled point can be computed using Eq. (2). And where h is the perceptual function (a VGG16 network). Iˆ
then the pixel color c and feature f are obtained via the andI aretheground-truthandpredictedimagepatches,re-
b b
volume rendering manner per Eq. (3) and Eq. (5), respec- spectively. Thelossatthekthstageisasfollows:
tively. For another regression approach, one practice is
Lk =L +λ L +λ L , (11)
predicting radiance for sampled points and then accumu- mse p perc s ssim
lating it into pixel color. In contrast, to reduce computa-
whereλ andλ refertolossweights. Theoveralllossis:
tionalcosts,weaccumulatepoint-wisedescriptorsintofea- p s
tures and then decode them into pixel color. Specifically,
(cid:88)Ns
we feed the accumulated feature into a 2D U-Net for spa- L= λkLk (12)
tialenhancementtoobtainpixelfeaturef ,followedbyan
r k=1
MLPtoyieldthepixelcolorc .
r
whereN referstothenumberofcoarse-to-finestagesand
Since these two approaches excel in different areas, in- s
λk representsthelossweightofthekthstage.
stead of using a fixed operator, such as average, we pro-
posedynamicallyfusingthemviac =w c +w c ,where
t b b r r
5.Experiments
w and w are predicted fusing weights. As the target
b r
viewisunavailable,comparingthequalityofc andc be-
b r 5.1.Settings
comesachicken-and-eggproblem.Anaivepracticeistodi-
rectlypredictfusingweightsfromfeatures,whichisunder- Datasets.FollowingMVSNeRF[5],wedividetheDTU[1]
constrained(Sec.5.5). Incontrast,weproposetolearnfus- datasetinto88trainingscenesand16testscenes. Wefirst
ingweightsbyusingthemulti-viewfeatureconsistencyas train our generalizable model on the 88 training scenes of
a hint. The underlying motivation is that if the predicted the DTU dataset and then evaluate the trained model on
colors closely resemble the ground-truth colors, the corre- the 16 test scenes. To further demonstrate the general-
sponding features of the predicted view and source views ization capability of our method, we also test the trained
underthecorrectdeptharesupposedtobesimilar. model(withoutanyfine-tuning)on8scenesfromtheReal
Specifically, we first obtain the final predicted depth Forward-facing [24] dataset and 8 scenes from the NeRF
d in a volume rendering-like way per d = (cid:80) τ (1 − Synthetic [25] dataset, both of which have significant dif-
f f k k
exp(−σ ))d , where d represents the depth of sampled ferencesinviewdistributionandscenecontentcomparedto
k k k
point. Withthedepthd ,wecanobtainthewarpedfeatures the DTU dataset. The image resolutions of the DTU, the
f
{fi}N fromsourceviews,andthemulti-viewconsistency Real Forward-facing, and the NeRF Synthetic datasets are
s i=1
canbecomputedusingvariance: 512×640, 640×960, and 800×800, respectively. The
quality of synthesized novel views is measured by PSNR,
f[b,r] =var(f ,f1,...,fN). (8) SSIM[34],andLPIPS[46]metrics.
w [b,r] s s
Baselines. We compare our methods with state-of-the-art
We then feed the consistency f[b,r] into an MLP to obtain generalizableNeRFmethods[5,7,20,23,30,32,44]. For
w
thefusingweightw ,followedbyasoftmaxoperatorfor generalizationwiththreeviewsandper-sceneoptimization,
[b,r]DTU[1] RealForward-facing[24] NeRFSynthetic[25]
Method Settings
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
PixelNeRF[44] 19.31 0.789 0.382 11.24 0.486 0.671 7.39 0.658 0.411
IBRNet[32] 26.04 0.917 0.191 21.79 0.786 0.279 22.44 0.874 0.195
MVSNeRF[5] 26.63 0.931 0.168 21.93 0.795 0.252 23.62 0.897 0.176
NeuRay[23] 25.81 0.868 0.160 23.39 0.744 0.217 24.58 0.892 0.163
3-view
ENeRF†[20] 27.61 0.956 0.091 22.78 0.808 0.209 26.65 0.947 0.072
ENeRF[20] 27.61 0.957 0.089 23.63 0.843 0.182 26.17 0.943 0.085
GNT[30] 26.39 0.923 0.156 22.98 0.761 0.221 25.80 0.905 0.104
MatchNeRF[7] 26.91 0.934 0.159 22.43 0.805 0.244 23.20 0.897 0.164
Ours 29.36 0.969 0.064 24.28 0.863 0.162 26.99 0.952 0.070
MVSNeRF[5] 24.03 0.914 0.192 20.22 0.763 0.287 20.56 0.856 0.243
NeuRay[23] 24.51 0.825 0.203 22.73 0.720 0.236 22.42 0.865 0.228
ENeRF[20] 25.48 0.942 0.107 22.78 0.821 0.191 24.83 0.931 0.117
2-view
GNT[30] 24.32 0.903 0.201 20.91 0.683 0.293 23.47 0.877 0.151
MatchNeRF[7] 25.03 0.919 0.181 20.59 0.775 0.276 20.57 0.864 0.200
Ours 26.98 0.955 0.081 23.39 0.839 0.176 25.30 0.939 0.082
Table1. Quantitativeresultsunderthegeneralizationsetting. WeshowtheaverageresultsofPSNRs,SSIMs,andLPIPSsonthree
datasetsundertwosettingsforthenumberofinputviews. ENeRF†representsresultsborrowedfromtheoriginalpaper. Thecomparison
methodsareorganizedbasedontheyearofpublication.
DTU[1] RealForward-facing[24] NeRFSynthetic[25]
Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
NeRF [25] 27.01 0.902 0.263 25.97 0.870 0.236 30.63 0.962 0.093
10.2h
IBRNet [32] 31.35 0.956 0.131 24.88 0.861 0.189 25.62 0.939 0.111
ft−1.0h
MVSNeRF [5] 28.51 0.933 0.179 25.45 0.877 0.192 27.07 0.931 0.168
ft−15min
NeuRay [23] 26.96 0.847 0.174 24.34 0.781 0.205 25.91 0.896 0.115
ft−1.0h
ENeRF [20] 28.87 0.957 0.090 24.89 0.865 0.159 27.57 0.954 0.063
ft−1.0h
Ours 30.10 0.966 0.069 26.62 0.903 0.110 28.57 0.958 0.060
ft−15min
Ours 30.18 0.966 0.068 26.76 0.905 0.106 28.81 0.960 0.058
ft−1.0h
Table2.Quantitativeresultsundertheper-sceneoptimizationsetting.Thebestresultisinbold,andthesecond-bestisunderlined.
wefollowthesamesettingas[5,7,20]andborrowthere- λ = 0.1 in Eq. (11), while λ1 = 0.5 and λ2 = 1 in
s
sults of [5, 7, 25, 32, 44] from [5, 7]. We evaluate [20] Eq.(12). WetrainourmodelonfourRTX3090GPUsus-
usingtheofficialcodeandtrainedmodels. Tokeepconsis- ing the Adam [18] optimizer. Refer to the supplementary
tent with the same settings for a fair comparison, such as materialformoreimplementationandnetworkdetails.
thenumberofinputviews,datasetsplitting,viewselection,
andimageresolution,weusethereleasedcodeandtrained 5.2.GeneralizationResults
model of [23] and retrain [30] with the released code, and
We report quantitative results on DTU, Real Forward-
evaluate them under our test settings. For generalization
facing, andNeRFSyntheticdatasetsinTable1undergen-
with two views, we borrow the results of [5, 7] from [7].
eralization settings. PixelNeRF [44], which applies ap-
Forotherbaselines[20,23,30],weevaluatethemusingthe
pearance descriptors, has reasonable results on the DTU
releasedmodel[20,23]ortheretrainedmodel[30].
test set, but insufficient generalization on the other two
Implementation Details. Following [20], the number of datasets. Other methods [5, 7, 20, 23, 30, 32] that model
coarse-to-fine stages N is set to 2. In our coarse-to-fine the scene geometry implicitly or explicitly by aggregat-
s
framework,wesample64and8depthplanesforthecoarse- ing multi-view features can maintain relatively good gen-
level and fine-level cost volumes, respectively. And we eralization. Thanks to our proposed modules tailored for
sample8and2pointsperrayforthecoarse-levelandfine- both the reconstruction and rendering phases, our method
level view rendering, respectively. We set λ = 0.1 and achieves significantly better generalizability. As shown in
pMVSNeRF ENeRF MatchNeRF Ours Ground Truth
Figure4.Qualitativecomparisonofrenderingqualitywithstate-of-the-artmethods[5,7,20]undergeneralizationandthreeinput
viewssettings.
Referenceview Novelview
Method
Abserr↓Acc(2)↑Acc(10)↑Abserr↓Acc(2)↑Acc(10)↑
MVSNet[41] 3.60 0.603 0.955 - - -
PixelNeRF[44] 49 0.037 0.176 47.8 0.039 0.187
IBRNet[32] 338 0.000 0.913 324 0.000 0.866
MVSNeRF[5] 4.60 0.746 0.913 7.00 0.717 0.866
ENeRF[20] 3.80 0.837 0.939 4.60 0.792 0.917
Ours 2.47 0.900 0.971 2.83 0.879 0.961
Table 3. Quantitative results of depth reconstruction on the
DTU test set. MVSNet is trained with depth supervision while
Image ENeRF Ours
othermethodsaretrainedwithonlyRGBimagesupervision.“Abs
Figure5.Qualitativecomparisonofdepthmapswith [20]. err”representstheaverageabsoluteerrorand“Acc(X)”meansthe
percentageofpixelswithanerrorlessthanXmm.
Fig. 4, the views produced by our method preserve more
scenedetailsandcontainfewerartifacts. Inchallengingar-
forsubstantiallylongertime(10.2hours),andalsooutper-
eas, such as occluded regions, object boundaries, and re-
formtheresultsofothergeneralizationmethodsafterfine-
gionswithcomplexgeometry,ourmethodsignificantlyout-
tuning. Withalongerfine-tuningduration,suchas1hour,
performsothermethods. Refertothesupplementarymate-
the rendering quality can be further improved. Qualitative
rialformorequalitativecomparisons.
resultscanbefoundinthesupplementarymaterial.
5.3.Per-sceneFine-tuningResults
5.4.DepthReconstructionResults
The quantitative results after per-scene optimization are
shown in Table 2 and we report the results of our method Following [5, 20], we report the performance of depth re-
after 15 minutes and 1 hour of fine-tuning. Due to the ex- construction in Table 3. Our method can achieve higher
cellentinitializationprovidedbyourgeneralizationmodel, depth accuracy than other methods, even including the
onlyashortperiodoffine-tuningisneededtoachievegood MVS method MVSNet [41] that is trained with depth su-
results. Ourresultsafter15minutesoffine-tuningarecom- pervision. AsshowninFig.5,thedepthmapproducedby
parabletoorevensuperiortothoseofNeRF[25]optimized ourmethodismorerefined,suchassharperobjectedges.
UTD
gnicaf-drawroF
laeR
citehtnyS
FReNACA SVA SVA CAF PSNR↑ Abserr↓
sm d
✗ ✗ ✗ ✗ 27.73 4.32
(cid:34) ✗ ✗ ✗ 28.35 3.85
✗ (cid:34) ✗ ✗ 28.30 3.53
✗ ✗ (cid:34) ✗ 27.89 3.94
✗ ✗ ✗ (cid:34) 28.85 3.36
(cid:34) (cid:34) ✗ ✗ 28.53 3.54
(cid:34) (cid:34) (cid:34) ✗ 28.64 3.06
Image (cid:34) (cid:34) (cid:34) (cid:34) 29.36 2.83
Figure 6. Visualization of F  us  ion Weights.  W  and W rep-
b r
resent the weight maps of the blending approach and regression Table 4. Ablation studies on the DTU dataset. We report the
approach,respectively. imagequality(PSNR)anddepthaccuracy(Abserr)metricswith-
outper-scenefine-tuningunderthreeinputviewssettings.SVA
sm
andSVA representϕ andϕ ofSVA,respectively.
30 18 d sm d
28 baseline 16 baseline
26 ours 14 ours 24 12 22 02 10 BlendingRegressionFusionPSNR↑SSIM↑LPIPS↓
18 8
16 6 No.1 (cid:34) ✗ ✗ 27.73 0.956 0.088
14 4
12 5 10 15 20 25 30 35 40 45 50 2 5 10 15 20 25 30 35 40 45 50 No.2 ✗ (cid:34) ✗ 26.83 0.955 0.091
Difficult areas (X%) Difficult areas (X%) No.3 (cid:34) ✗ AE 27.89 0.959 0.081
Figure 7. Quantitative analysis of difficult areas. The X-axis No.4 (cid:34) (cid:34) DWF 28.10 0.963 0.075
difficultareas(X%)representsconsideringtheareawiththetop No.5 (cid:34) (cid:34) CAF 28.85 0.966 0.070
X% of values in W as a difficult area. A smaller threshold X
r
indicatesamorechallengingarea. Table 5. Comparison of different fusion strategies. The AE
representstherefinementbyanautoencoder.TheDWFrepresents
direct weighted fusion. The CAF is our proposed Consistency-
5.5.AblationsandAnalysis AwareFusion.
Ablation studies. As shown in Table 4, we conduct abla-
tionstudiestoinvestigatethecontributionofeachproposed
and No.2, the result of using the blending approach alone
module. Each individual component can benefit the base-
is better than that of using the regression approach alone.
line model in both view quality and depth accuracy, with
For No.4, the DWF represents fusion weights derived di-
CAFhavingthehighestgain.Aninterestingphenomenonis
rectly from features of the two intermediate views, which
thatCAFgreatlyimprovesdepthaccuracy,indicatingthata
greatly degrades performance compared to our proposed
well-designedviewdecodingapproachalsofacilitatesdepth
way of checking the multi-view consistency (No.5). Our
prediction. Combiningallcomponentsresultsinthegreat-
fusionapproachutilizestheadvantagesofthetwodecoding
est gain, with a 5.9% increase in PSNR and a 34.5% im-
approachestorefinethesynthesizedview. InNo.3, were-
provementindeptherrorcomparedtothebaselinemodel.
finethesynthesizedviewdecodedinasingleway,wherethe
CAF working mechanism. As shown in Fig. 6, we visu-
synthesizedviewisfedintoanauto-encoderforrefinement,
alize the fusion weights of two decoding approaches. The
whichhaslimitedimprovement.
regressionapproachexhibitshigherconfidenceinchalleng-
ing areas such as object edges and reflections, while the
6.Conclusion
blending approach shows higher confidence in most other
areas, which is consistent with the observation in Fig. 2. Inthispaper,wepresentageneralizableNeRFmethodca-
As shown in Fig. 7, we define challenging areas as those pable of achieving high-fidelity view synthesis. Specifi-
withhighconfidenceinW anddividethembyaseriesof cally,duringthereconstructionphase,weproposeAdaptive
r
thresholds. A smaller threshold X indicates a more diffi- Cost Aggregation (ACA) to improve geometry estimation
cult region. When X = 5%, our method improves PSNR andSpatial-ViewAggregator(SVA)toencode3Dcontext-
by 2.55db and “Abs err” by 3.47mm. When the threshold awaredescriptors. Intherenderingphase,weintroducethe
increases, suchasX = 50%, ourmethodimprovesPSNR Consistency-AwareFusion(CAF)moduletounifytheirad-
by1.97dband“Abserr”by1.54mm,whichfurtherdemon- vantages to refine the synthesized view quality. We inte-
stratesthesuperiorityofourmethodinchallengingareas. gratethesemodulesintoacoarse-to-fineframework,termed
Fusion strategy. As shown in Table 5, we investigate the GeFu. Extensiveevaluationsandablationsdemonstratethe
performanceofdifferentfusionstrategies. ComparingNo.1 effectivenessofourproposedmodules.
RNSP )mm( rre
sbAReferences multi-viewstereoandstereomatching. InProc.IEEEConf.
Comput.Vis.PatternRecogn.,pages2495–2504,2020.3,13
[1] HenrikAanaes,RasmusRamsbolJensen,GeorgeVogiatzis,
[15] M. Johari, Y. Lepoittevin, and F. Fleuret. Geonerf: Gen-
EnginTola,andAndersBjorholmDahl.Large-scaledatafor
eralizing nerf with geometry priors. In Proc. IEEE Conf.
multiple-viewstereopsis. Int.J.Comput.Vis.,120:153–168,
Comput.Vis.PatternRecogn.,2022. 1
2016. 1,2,5,6,11,14,15,18
[16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler,
[2] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall,
and George Drettakis. 3d gaussian splatting for real-time
Kalyan Sunkavalli, Milosˇ Hasˇan, Yannick Hold-Geoffroy,
radiancefieldrendering.ACMTransactionsonGraphics,42
David Kriegman, and Ravi Ramamoorthi. Neural re-
(4),2023. 15
flectance fields for appearance acquisition. arXiv preprint
[17] Tejas Khot, Shubham Agrawal, Shubham Tulsiani,
arXiv:2008.03824,2020. 3
Christoph Mertz, Simon Lucey, and Martial Hebert.
[3] MarkBoss,RaphaelBraun,VarunJampani,JonathanTBar-
Learning unsupervised multi-view stereopsis via robust
ron,CeLiu,andHendrikLensch. Nerd: Neuralreflectance
photometricconsistency. arXivpreprintarXiv:1905.02706,
decomposition from image collections. In Proc. IEEE Int.
2019. 13
Conf.Comput.Vis.,pages12684–12694,2021. 3
[18] Diederik P Kingma and Jimmy Ba. Adam: A method for
[4] DiChang, Aljazˇ Bozˇicˇ, TongZhang, QingsongYan, Ying-
stochastic optimization. arXiv preprint arXiv:1412.6980,
cong Chen, Sabine Su¨sstrunk, and Matthias Nießner. Rc-
2014. 6,11
mvsnet:Unsupervisedmulti-viewstereowithneuralrender-
[19] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu
ing. InProc.Eur.Conf.Comput.Vis.,2022. 13
Wang,andGimHeeLee. Mine: Towardscontinuousdepth
[5] AnpeiChen,ZexiangXu,FuqiangZhao,XiaoshuaiZhang,
mpiwithnerffornovelviewsynthesis. InProc.IEEEInt.
FanboXiang,JingyiYu,andHaoSu.Mvsnerf:Fastgeneral-
Conf.Comput.Vis.,2021. 1
izableradiancefieldreconstructionfrommulti-viewstereo.
[20] HaotongLin,SidaPeng,ZhenXu,YunzhiYan,QingShuai,
InProc.IEEEInt.Conf.Comput.Vis.,pages14124–14133,
Hujun Bao, and Xiaowei Zhou. Efficient neural radiance
2021. 1,2,3,4,5,6,7,11,12,15,16,17,18,19,21,22
fields for interactive free-viewpoint video. In SIGGRAPH
[6] AnpeiChen,RuiyangLiu,LingXie,ZhangChen,HaoSu,
AsiaConferenceProceedings,2022. 1,2,3,4,5,6,7,11,
andJingyiYu. Sofgan: Aportraitimagegeneratorwithdy-
12,15,16,17,18,19,21,22
namicstyling. ACMTrans.Graph.,41(1):1–26,2022. 3
[21] Tsung-Yi Lin, Piotr Dolla´r, Ross Girshick, Kaiming He,
[7] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng,
Bharath Hariharan, and Serge Belongie. Feature pyra-
Tat-Jen Cham, and Jianfei Cai. Explicit correspondence
mid networks for object detection. In Proceedings of the
matching for generalizable neural radiance fields. arXiv
IEEE conference on computer vision and pattern recogni-
preprint arXiv:2304.12294, 2023. 1, 2, 3, 5, 6, 7, 16, 17,
tion,pages2117–2125,2017. 3
18,19
[22] TianqiLiu,XinyiYe,WeiyueZhao,ZhiyuPan,MinShi,and
[8] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Er-
ZhiguoCao.Whenepipolarconstraintmeetsnon-localoper-
ranLi,RaviRamamoorthi,andHaoSu. Deepstereousing atorsinmulti-viewstereo. InProc.IEEEInt.Conf.Comput.
adaptivethinvolumerepresentationwithuncertaintyaware- Vis.,pages18088–18097,2023. 3
ness. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn.,
[23] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng
pages2524–2534,2020. 3
Wang, Christian Theobalt, Xiaowei Zhou, and Wenping
[9] JulianChibane,AayushBansal,VericaLazova,andGerard Wang. Neural rays for occlusion-aware image-based ren-
Pons-Moll. Stereoradiancefields(srf): Learningviewsyn- dering. InProc.IEEEConf.Comput.Vis.PatternRecogn.,
thesisforsparseviewsofnovelscenes. InProc.IEEEConf. 2022. 1,3,5,6,13,15,16,21,22
Comput.Vis.PatternRecogn.,pages7911–7920,2021. 1
[24] BenMildenhall, PratulPSrinivasan, RodrigoOrtiz-Cayon,
[10] YikangDing, WentaoYuan, QingtianZhu, HaotianZhang, NimaKhademiKalantari,RaviRamamoorthi,RenNg,and
XiangyueLiu,YuanjiangWang,andXiaoLiu. Transmvsnet AbhishekKar. Locallightfieldfusion: Practicalviewsyn-
globalcontext-awaremulti-viewstereonetworkwithtrans- thesis with prescriptive sampling guidelines. ACM Trans.
formers. InProc.IEEEConf.Comput.Vis.PatternRecogn., Graph.,38(4):1–14,2019. 1,2,5,6,11,12,13,14,15,18
pages8585–8594,2022. 3
[25] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
[11] PascalFuaandYvanGLeclerc. Object-centeredsurfacere- JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
constructioncombiningmulti-imagestereoandshading.Int. Representingscenesasneuralradiancefieldsforviewsyn-
J.Comput.Vis.,16(ARTICLE):35–56,1995. 2 thesis. InProc.Eur.Conf.Comput.Vis.,2020. 1,2,3,5,6,
[12] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. 7,11,12,13,14,15,18,21,22
Massively parallel multiview stereopsis by surface normal [26] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
diffusion.InProc.IEEEInt.Conf.Comput.Vis.,pages873– Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
881,2015. 2 Martin-Brualla. Nerfies: Deformableneuralradiancefields.
[13] RossGirshick. Fastr-cnn. InProc.IEEEInt.Conf.Comput. In Proc. IEEE Int. Conf. Comput. Vis., pages 5865–5874,
Vis.,2015. 13 2021. 3
[14] XiaodongGu,ZhiwenFan,SiyuZhu,ZuozhuoDai,Feitong [27] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and
Tan,andPingTan. Cascadecostvolumeforhigh-resolution Ronggang Wang. Rethinking depth estimation for multi-view stereo a unified representation. In Proc. IEEE Conf. [41] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan.
Comput.Vis.PatternRecogn.,pages8645–8654,2022. 3 Mvsnet depth inference for unstructured multi-view stereo.
[28] JohannesLSchonbergerandJan-MichaelFrahm. Structure- InProc.Eur.Conf.Comput.Vis.,pages767–783,2018. 2,7
from-motion revisited. In Proc. IEEE Conf. Comput. Vis. [42] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,
PatternRecogn.,pages4104–4113,2016. 2 andLongQuan.Recurrentmvsnetforhigh-resolutionmulti-
viewstereodepthinference. InProc.IEEEConf.Comput.
[29] Johannes L Schonberger, Enliang Zheng, Jan-Michael
Vis.PatternRecogn.,pages5525–5534,2019. 3
Frahm, and Marc Pollefeys. Pixelwise view selection for
unstructuredmulti-viewstereo. InProc.Eur.Conf.Comput. [43] Xinyi Ye, Weiyue Zhao, Tianqi Liu, Zihao Huang, Zhiguo
Vis.,pages501–518.Springer,2016. 2 Cao, and Xin Li. Constraining depth map geometry for
multi-view stereo: A dual-depth approach with saddle-
[30] MukundVarmaT,PeihaoWang,XuxiChen,TianlongChen,
shapeddepthcells. InProc.IEEEInt.Conf.Comput.Vis.,
SubhashiniVenugopalan,andZhangyangWang.Isattention
pages17661–17670,2023. 3
allthatneRFneeds? InProc.Int.Conf.Learn.Repr.,2023.
[44] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
1,3,5,6,15,16,21,22
pixelNeRF:Neuralradiancefieldsfromoneorfewimages.
[31] AlexTrevithickandBoYang. Grf: Learningageneralradi-
InProc.IEEEConf.Comput.Vis.PatternRecogn.,2021. 1,
ancefieldfor3drepresentationandrendering.InProc.IEEE
2,3,5,6,7,15,21,22
Int.Conf.Comput.Vis.,pages15182–15192,2021.
[45] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian
[32] QianqianWang,ZhichengWang,KyleGenova,PratulSrini-
Fang. Visibility-awaremulti-viewstereonetwork. Proc.Br.
vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-
Mach.Vis.Conf.,2020. 3
Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet:
[46] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Learningmulti-viewimage-basedrendering. InProc.IEEE
and Oliver Wang. The unreasonable effectiveness of deep
Conf.Comput.Vis.PatternRecogn.,2021. 1,2,3,4,5,6,7,
featuresasaperceptualmetric.InProc.IEEEConf.Comput.
11,15,21,22
Vis.PatternRecogn.,pages586–595,2018. 5,11
[33] XiaofengWang,ZhengZhu,GuanHuang,FangboQin,Yun
Ye, Yijia He, Xu Chi, and Xingang Wang. Mvster epipo-
lartransformerforefficientmulti-viewstereo. InProc.Eur.
Conf.Comput.Vis.,pages573–591.Springer,2022. 2,3,14
[34] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
moncelli. Imagequalityassessment: fromerrorvisibilityto
structuralsimilarity.IEEEtransactionsonimageprocessing,
13(4):600–612,2004. 5,11
[35] ZizhuangWei,QingtianZhu,ChenMin,YisongChen,and
GuopingWang. Aa-rmvsnetadaptiveaggregationrecurrent
multi-viewstereonetwork.InProc.IEEEInt.Conf.Comput.
Vis.,pages6187–6196,2021. 2,3,4,14
[36] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil
Kim. Space-timeneuralirradiancefieldsforfree-viewpoint
video. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn.,
pages9421–9431,2021. 3
[37] Fanbo Xiang, Zexiang Xu, Milos Hasan, Yannick Hold-
Geoffroy, KalyanSunkavalli, andHaoSu. Neutex: Neural
texture mapping for volumetric neural rendering. In Proc.
IEEEConf.Comput.Vis.PatternRecogn.,pages7119–7128,
2021. 3
[38] QiangengXu,ZexiangXu,JulienPhilip,SaiBi,ZhixinShu,
KalyanSunkavalli,andUlrichNeumann. Point-nerf: Point-
basedneuralradiancefields. InProc.IEEEConf.Comput.
Vis.PatternRecogn.,pages5438–5448,2022. 1
[39] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding,
Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing
Tai. Densehybridrecurrentmulti-viewstereonetwithdy-
namic consistency checking. In Proc. Eur. Conf. Comput.
Vis.,pages674–689.Springer,2020. 3
[40] JiayuYang,WeiMao,JoseMAlvarez,andMiaomiaoLiu.
Costvolumepyramidbaseddepthinferenceformulti-view
stereo. InProc.IEEEConf.Comput.Vis.PatternRecogn.,
pages4877–4886,2020. 3Geometry-aware Reconstruction and Fusion-refined Rendering
for Generalizable Neural Radiance Fields
Supplementary Material
7.Appendix
Input Operation Output
Inthesupplementarymaterial,wepresentmoredetailsthat (C,D,H,W) permute (D,C,H,W)
(D,C,H,W) cgr(C,C ) (D,C ,H,W)
arenotincludedinthemaintext,including: h h
(D,C ,H,W) cgr(C ,C ) (D,C ,H,W)
h h h h
• Moreimplementationandnetworkdetails. (D,C ,H,W) conv2d(C ,1) (D,1,H,W)
h h
• Additional ablation experiments, including different (D,1,H,W) sigmoid (D,1,H,W)
numbersofviews,featurediscussionsfortheCAFmod- (D,1,H,W) permute (1,D,H,W)
ule, intermediate view supervision, depth supervisions,
anddifferentimplementationsofACAnetworks. Table I. The network architecture of Adaptive Cost Aggre-
• More qualitative results, including novel view, depth gation (ACA). The cgr represents a block composed of conv2d,
maps,fusionweights,anderrormaps. groupnorm,andrelu.Inourimplementation,C =16andC h =4.
• Per-scenebreakdownresults.
• Limitations.
input image resolution of 512×640. The number of our
model’sparametersis3.15M.
I.ImplementationandNetworkDetails
Implementation Details. Our generalizable model is NetworkDetails. Here,wewillintroducethenetworkde-
trained on four RTX 3090 GPUs using the Adam [18] tailsofthepoolingnetwork(Sec.4.2)andACA(Sec.4.1)
optimizer, with an initial learning rate of 5e − 4. The mentionedinthemaintext.
learning rate is halved every 50k iterations. As shown in
Pooling network. [20, 32] apply a pooling network ρ
Fig.I,wepresentthemetricsoftheDTUtestset[1]vary-
toaggregatemulti-viewfeaturestoobtainthedescriptorvia
ing with the number of iterations. The model tends to f = ρ({fi}N ). The implementation details are as fol-
p s i=1
convergeafterapproximately150kiterations, takingabout lows: initially, the mean µ and variance v of {fi}N are
s i=1
25 hours. It is worth noting that, with only 8k itera-
computed. Subsequently, µ and v are concatenated with
tions,ourmodelcanachievemetricsof27.68/0.961/0.080, each fi and an MLP is applied to generate a weight. The
s
surpassing the metrics of the SOTA method [20], which
f is blended via a soft-argmax operator using obtained
p
is27.61/0.956/0.091(PSNR/SSIM[34]/LPIPS[46]). Fol- weightsandmulti-viewfeatures({fi}N ).
s i=1
lowing ENeRF [20], during training, we select 2, 3, and 4
ACA.PertheEq.(4)inthemaintext,α(.)representsthe
source views as inputs with probabilities of 0.1, 0.8, and
adaptiveweightforeachview,andthenetworkarchitecture
0.1. To save computational costs, the Consistency-aware
thatlearnstheseweightsisshowninTableI.
Fusion (CAF) is exclusively employed in the fine stage,
while in the coarse stage, a blending approach is used to
InferenceSpeed. Foranimagewith512×640resolution,
synthesize a low-resolution target view, supervised by the
theinferencetimeofourmethodis143ms. Wedecompose
ground-truth target view. During training, the final tar-
the inference time in Table II and results demonstrate that
get view is generated by fusing two intermediate views as
the inference time of our modules is only 71ms, with the
I = W I +W I . However, duringevaluationforother
t b b r r
remaining72msspentsavingresults.
datasets[24,25],whichhaveadifferentdomaincompared
withDTU,increasingtheweightsofI canleadtoslightly
b II.Additionalablationexperiments
betterperformance. Therefore, weobtainthefinalviewas
I =(I +(W I +W I ))/2.Ourevaluationsetupiscon- Numbers of Views. As shown in Table III, we evaluate
t b b b r r
sistentwithENeRF[20]andMVSNeRF[5]. Theresultsof the performance of our trained generalization model and
the DTU test set are evaluated using segmentation masks. ENeRF [20] with different numbers of input views on the
Thesegmentationmaskisdefinedbasedontheavailability DTU test set [1]. With an increase in the number of input
ofground-truthdepthateachpixel. Sincethemarginalre- views,theperformanceimprovesasthemodelcanleverage
gionofimagesistypicallyinvisibletoinputimagesonthe moremulti-viewinformation. Intermsofbothoverallper-
RealForward-facingdataset[24],weevaluatethe80%area formanceandthemagnitudeofperformanceimprovement,
in the center of the images. Incidentally, all the inference ourmethodoutperformsENeRF,indicatingitssuperiorca-
timepresentedinFig.1ofthemaintextismeasuredatan pability in leveraging multi-view information for recon-30 psnr 0.975 ssim 0.15 lpips
0.13
28 0.96
0.11
26 0.945
0.09
24 0.93
0.07
22 0.915 0.05
0 50 100 150 200 250 0 50 100 150 200 250 0 50 100 150 200 250
iterations(k) iterations(k) iterations(k)
FigureI.ThemetricsoftheDTUtestsetvarywiththenumberofiterations.
Modules coarsestage finestage Anothermoreintuitivealternativeistouseafeatureextrac-
featureextractor 1.32 tortoextractfeaturesforbothintermediateviews,as:
buildrays 9.11 15.94
geometry costvolume 7.48 8.24 F [b,r] =ϕ eI [b,r], (II)
regularization 1.91 1.97
poolingρ 0.57 0.50 whereϕ representsafeatureextractor,instantiatedasa2D
e
descriptor SVAsm 1.74 1.37
U-Net. f andf arethepixel-wisefeaturesofF andF ,
SVAd 0.73 1.01 b r b r
viewdecoding 1.62 17.80 respectively. As shown in Table V, the strategy employed
saveindictionary 13.27 58.31 in the main text is slightly superior to the other two alter-
native strategies. For the first alternative Eq. (I), f is ob-
b
TableII.Timeoverheadforeachmodule(inmilliseconds).The tainedbyblendingfeaturesfromsourceviews. f lacks3D
b
term“saveindictionary”referstostoringtensorresultsindictio- contextawareness, leadingtosomeinformationlossinthe
naryformforsubsequentevaluationofvariousmetrics. subsequentlyaccumulatedpixelfeatures.Forthesecondal-
ternativeEq.(II),f andf areextractedfromscratchatthe
b r
RGBlevel. Thispracticeisdisadvantageousforthesubse-
structing scene geometry and rendering novel views. Ad-
quent learning of 3D consistency weights, due to the lack
ditionally, we also present the performance of our method
ofutilizationof3Dinformation. Additionally,theintroduc-
on the Real Forward-facing [24] and NeRF Synthetic [25]
tion of a feature extractor also increases the burden on the
datasets under different numbers of input views, as shown
model. However, the strategy in the main text maximally
in Table IV. The results demonstrate the same trend, indi-
utilizestheobtained3D-awaredescriptors,whilealsohav-
catingthecapabilityofourmodelinleveragingmulti-view
ing the smallest computational cost compared to the other
informationisgeneralizable.
twoalternativeapproaches.
FeaturesforIntermediateViews. InSec.4.3ofthemain
text,f andf areutilizedasthefeaturerepresentationsfor Intermediate View Supervision. In the main text, we
b r
thetwointermediateviewsI andI , respectively. Subse- only supervise the images fused through CAF. However,
b r
quently,theirconsistencywithsourceviewsisindividually simultaneously supervising intermediate results is also a
computedtolearnfusionweights.Thechoiceofusingf as commonpractice,whosefinalresultis29.15/0.968/0.065.
b
thefeatureforI isbasedontheirsimilarvolumerendering Thisresultisslightlyinferiortosupervisingonlythefused
b
generationmanners,whiletheselectionoff asthefeature view(29.36/0.969/0.064). Becauseeachofthetwointer-
r
forI isdrivenbytheirdirectprojectionrelationship. Here, mediateviewshasitsownadvantages,supervisingonlythe
r
wewilldiscussdifferentselectionstrategiesforthefeatures fused view allows the network to focus on the fusion pro-
ofintermediateviews. Analternativeapproachforthefea- cess, leveraging the strengths of both. However, simulta-
tures of I is to blend features from source views. Similar neouslysupervisingtheintermediateviewsburdensthenet-
b
toEq.(2)inthemaintext,thecalculationofthefeaturesf work,diminishingitsattentiontothefusionprocess.Inthe-
b
forI isasfollows: ory,ifbothintermediateviewsareentirelycorrect,thefinal
b
fusedviewwillbeaccurateregardlessofthefusionprocess.
f
=(cid:88)N exp(w i)f si
, (I)
Thenetworkprioritizespredictingtwoaccurateintermedi-
b (cid:80)N
exp(w )
ateviews,whichisamorechallengingtask.
i=1 j=1 j
where w = MLP(x,d,f ,fi), fi is the feature of the DepthSupervision. Acriticalfactorinthemodel’ssyn-
i p s s
sourceimageIi. xanddrepresentthecoordinateandview thesis of high-quality views is its perception of the scene
s
direction, respectively. f is the descriptor for 3D point. geometry. MVS-basedgeneralizableNeRFmethods[5,20,
pViews PSNR↑ SSIM↑ LPIPS↓ Abserr↓ Acc(2)↑ Acc(10)↑
2 26.98/25.48 0.955/0.942 0.081/0.107 3.86/5.53 0.835/0.756 0.942/0.107
3 29.36/27.61 0.969/0.957 0.064/0.089 2.83/4.60 0.879/0.792 0.961/0.917
4 29.77/27.73 0.971/0.959 0.062/0.089 2.73/4.26 0.880/0.804 0.961/0.929
5 29.91/27.54 0.971/0.958 0.062/0.091 2.69/4.29 0.882/0.800 0.961/0.928
Table III. The performance of our method and ENeRF with different numbers of input views on the DTU test set. Each item
represents(Ours/ENeRF’s). “Abserr”denotestheaverageabsoluteerrorand“Acc(X)”meansthepercentageofpixelswithanerrorless
thanXmm.
Views RealForward-facing[24] NeRFSynthetic[25] L SSIM and L Smooth are the structured similarity loss and
depthsmoothnessloss,respectively. β ,β ,andβ areset
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ 1 2 3
to12,6,and0.18inourimplementation,respectively.Refer
2 23.39 0.839 0.176 25.30 0.939 0.082
to[4,17]formoredetails. L isusedtosupervisethefinal
3 24.28 0.863 0.162 26.99 0.952 0.070 d
4 24.91 0.876 0.157 27.31 0.953 0.069 depth, i.e., d f (Sec. 4.3 in the main text). Since the DTU
datasetprovidesground-truthdepth,anotherapproachisto
TableIV.Theperformanceofourmethodwithvaryingnum- utilize the ground-truth depth for supervision. The depth
bersofinputviewsontheRealForward-facingandNeRFSyn- lossisasfollows:
theticdatasets.
L =ξ(d ,d ) (IV)
d f gt
PSNR↑ SSIM↑ LPIPS↓
whered andd representthefinalpredictedandground-
No.1 29.36 0.969 0.064 f gt
truth depth, respectively. ξ denotes a loss function. Fol-
No.2 29.24 0.969 0.065
No.3 29.08 0.968 0.066 lowing [14], ξ is instantiated as the Smooth L1 loss [13].
The quantitative results are presented in the Table VI. The
Table V. Different strategies for the features of intermediate performance of the three strategies in the table is compa-
views. No.1representsthestrategyinthemaintext. No.2repre- rable, indicating that supervising only the RGB images is
sentsthestrategyusingEq.(I).No.3representsthestrategyusing sufficient, and there is no need for additional introduction
Eq.(II). ofdepthsupervisionsignals.
Depth PSNR↑ SSIM↑ LPIPS↓ Abserr↓ Acc(2)↑ Acc(10)↑
More Comprehensive Depth Analysis. As shown in
Self-supervision 29.21 0.968 0.064 3.21 0.873 0.957
Fig. 3 in the main text, our pipeline first infers the geom-
Supervision 29.31 0.969 0.064 2.95 0.875 0.957
None 29.36 0.969 0.064 2.83 0.879 0.961 etryfromthecostvolume,re-samples3Dpointsaroundob-
jects’surfaces, andfinally encodes3D descriptorsfor ren-
Table VI. The comparison of different depth supervision sig- dering. We can obtain two depths: one inferred from the
nals. The self-supervision represents using unsupervised depth
costvolumeandtheotherobtainedthroughvolumerender-
loss and the supervision represents using ground-truth depth for
ing, which is the final depth. Here, we report the depth
supervision.Theterm“None”referstotrainingwithoutanydepth
obtainedfromthecostvolumeandthefinaldepthasshown
supervisionsignals.
in Table VII. Compared to the baseline, our method per-
forms better on both depth metrics. Thanks to Adaptive
23], including our method, aim to improve the quality of CostAggregation(ACA),thedepthobtainedfromthecost
view synthesis by enhancing the geometry prediction. By volumehasbeensignificantlyimproved. Basedonthis, as
only supervising RGB images, excellent geometry predic- the Spatial-View Aggregator (SVA) encodes 3D-aware de-
tions can be achieved (Sec. 5.4 in the main text). Here, scriptors,thefinaldepthhasalsobeenfurtherimproved. In
we will discuss the impact of incorporating depth supervi- addition, the well-designed decoding approach, i.e., CAF,
sionsignalsonthemodel.Weintroducesupervisionsignals greatlyfacilitatesthedepthpredictionofthemodel(Sec.5.5
fordepthintwoways: onethroughself-supervisedandthe inthemaintext).
otherthroughsupervisionusingground-truthdepth.
Following[4,17],theunsuperviseddepthlossis:
Visualization of ACA. Previous approaches using vari-
ancestruggletoencodeefficientcostinformationforchal-
L =β L +β L +β L , (III)
d 1 PC 2 SSIM 3 Smooth
lenging areas, such as the occluded areas marked in the
where L represents the photometric consistency loss. black box in Fig. II. Our proposed ACA module learns an
PCSource Image 1 Source Image 2 Source Image 3
Ground Truth
w/o SVA sm w/ SVA sm w/o SVA d w/ SVA d
Weight 1 Weight 2 Weight 3 (a) (b)
FigureII.VisualizationofACA. FigureIII.VisualizationofSVA.SVA smandSVA drepresentϕ sm
andϕ ofSVA,respectively.
d
adaptive weight for different views to encode accurate ge- Referenceview Novelview
Method
ometric information. As illustrated in Fig. II, the weights
Abserr↓Acc(2)↑Acc(10)↑Abserr↓Acc(2)↑Acc(10)↑
learnedbyACAamplifythecontributionofconsistentpixel
Baseline-mvs 3.71 0.815 0.942 4.49 0.778 0.928
pairs, such as the visible areas in source image 1 and 3,
Baseline-final 3.58 0.842 0.944 4.32 0.800 0.928
while suppressing inconsistent ones, as shown in the oc- Ours-mvs 2.79 0.836 0.965 3.15 0.816 0.958
cludedareasinthesourceimage2. Ours-final 2.47 0.900 0.971 2.83 0.879 0.961
Table VII. More comprehensive depth metrics.“-mvs” repre-
Different ACA networks. The primary challenge of ap-
sents the depth obtained from the cost volume and “-final” rep-
plying ACA to the NVS task is the unavailability of the resentsthefinaldepthobtainedthroughvolumerendering.
target view, which we addressed by adopting a coarse-
to-fine framework. In the main text, the weight learning
Approach PSNR↑ SSIM↑ LPIPS↓
network utilized in ACA is illustrated in Table I, follow-
Regression 27.32 0.945 0.119
ing the MVS method, i.e., AA-RMVSNet [35]. More-
Blending 28.40 0.962 0.091
over,othernetworkscanalsobeembeddedintoourcoarse-
Overall 29.36 0.969 0.064
to-fine framework to learn inter-view weights. Here, we
adopt another MVS method, i.e., MVSTER [33], to learn
TableVIII.Quantitativeresultsforintermediateresults. Over-
adaptive weights. The result on the DTU test set is
allrepresentsthefusedviews.
29.31/0.969/0.064(PSNR/SSIM/LPIPS),whichiscompa-
rable with theresult obtained using [35]. In summary, our
main contribution is to propose an approach for applying Thispracticeresultsincontinuitiesandsharpboundariesin
ACA to the NVS task, withoutspecifying a particular net- bothrenderedviewsanddepthmaps.
workforlearningweights.
III.MoreQualitativeResults
Qualitative Results under the Generalization Setting.
Analysis of SVA. Previous approaches directly uses a
As shown in Fig. IV, V, and VI, we present the qualita-
pooling network to aggregate multi-view 2D features for
tivecomparisonofrenderingqualityontheDTU[1],NeRF
encoding 3D descriptors, which are not spatially context-
Synthetic [25], and Real Forward-facing [24] datasets, re-
aware,leadingtodiscontinuitiesinthedecodeddepthmap
spectively. Our method can synthesize views with higher
and rendered view (see Fig. III (a)). To address this issue,
fidelity, especially in challenging areas. For example, in
convolutionalnetworkscanbeusedtoimposespatialcon-
occluded regions and geometrically complex scenes, our
straintsonadjacentdescriptors.However,duetothesmooth
methodcanreconstructmoredetailswhileexhibitingfewer
natureofconvolution,somehigh-frequencydetailsmaybe
artifactsatobjects’edgesandinreflectiveareas.
lost. Sincedetailedinformationcomesfromthemulti-view
features, we employ a divide-and-conquer approach to ag-
gregate descriptors. Firstly, we employ a 3D U-Net to ag- Qualitative Results under the Per-scene Optimiza-
gregatespatialcontextandobtainsmoothdescriptors. De- tion Setting. Benefiting from the strong initialization of
spiteresolvingtheissueofdiscontinuities,anunsharpened our generalizable model, excellent performance can be
objectedgeoccurs(Fig.III(b)). Secondly,weproposeus- achieved within just a short fine-tuning period, such as 15
ingsmoothedfeaturesasqueries, withmulti-viewfeatures minutes. As shown in Fig. VII, we present the results af-
servingaskeysandvalues. Applyingtheattentionmecha- ter fine-tuning. After per-scene optimization, the model
nismallowsustogatherhigh-frequencydetailsadaptively. demonstrates enhanced capabilities in handling scene de-tails,resultinginviewswithhigherfidelity.
Qualitative Comparison of Depth Maps. As shown in
Figs.VIII,IX,andX,wepresentthequalitativecomparison
Scan #1 #8 #21 #103 #114
of depth maps on the DTU [1], NeRF Synthetic [25], and
Metric PSNR↑
RealForward-facing[24]datasets,respectively. Thedepth
mapsgeneratedbyourmethodcanmaintainsharperobject PixelNeRF[44] 21.64 23.70 16.04 16.76 18.40
IBRNet[32] 25.97 27.45 20.94 27.91 27.91
edges and preserve more details of scenes, which verifies
MVSNeRF[5] 26.96 27.43 21.55 29.25 27.99
thestronggeometryreasoningcapabilityofourmethod.
NeuRay[23] 28.59 27.63 23.05 29.71 29.23
ENeRF[20] 28.85 29.05 22.53 30.51 28.86
Fusion Weight Visualization. As shown in Fig. XI, we GNT[30] 27.25 28.12 21.67 28.45 28.01
presentthefusionweightsoftheConsistency-awareFusion Ours 30.72 30.87 23.96 31.78 29.84
(CAF) module. The blending approach generally demon- NeRF [25] 26.62 28.33 23.24 30.40 26.47
10.2h
strateshigherconfidenceinmostareas,whiletheregression IBRNet ft−1.0h[32] 31.00 32.46 27.88 34.40 31.00
approach shows higher confidence in challenging regions MVSNeRF ft−15min[5] 28.05 28.88 24.87 32.23 28.47
NeuRay [23] 27.77 25.93 23.40 28.57 29.14
suchasobjectboundariesandreflections. ft−1.0h
ENeRF [20] 30.10 30.50 22.46 31.42 29.87
ft−1.0h
Ours 31.54 31.41 24.07 32.97 30.52
ft−15min
Error Map Visualization. As shown in Fig. XII, we Ours 31.58 31.61 24.07 33.09 30.53
ft−1.0h
present the error maps obtained by two decoding ap-
Metric SSIM↑
proaches,aswellastheerrormapsofthefusedviews. The
PixelNeRF[44] 0.827 0.829 0.691 0.836 0.763
blendingapproachtendstoexhibitlowererrorsinmostar-
IBRNet[32] 0.918 0.903 0.873 0.950 0.943
eas, while the regression approach may have lower errors
MVSNeRF[5] 0.937 0.922 0.890 0.962 0.949
insomeregionswithreflectionsandedges. Inaddition,we NeuRay[23] 0.872 0.826 0.830 0.920 0.901
alsopresentquantitativeresults,asshowninTableVIII.The ENeRF[20] 0.958 0.955 0.916 0.968 0.961
viewsfusedthroughConsistency-awareFusion(CAF)inte- GNT[30] 0.922 0.931 0.881 0.942 0.960
gratetheadvantagesofbothintermediateviews,achieving Ours 0.971 0.965 0.943 0.974 0.965
afurtherimprovementinquality. NeRF [25] 0.902 0.876 0.874 0.944 0.913
10.2h
IBRNet [32] 0.955 0.945 0.947 0.968 0.964
ft−1.0h
IV.Per-sceneBreakdown MVSNeRF [5] 0.934 0.900 0.922 0.964 0.945
ft−15min
NeuRay [23] 0.872 0.751 0.845 0.868 0.900
AsshowninTablesIX,X,XI,andXII,wepresenttheper- ft−1.0h
ENeRF [20] 0.966 0.959 0.924 0.971 0.965
ft−1.0h
scenebreakdownresultsofthreedatasets(DTU[1],NeRF Ours 0.973 0.967 0.945 0.976 0.969
ft−15min
Synthetic [25], and Real Forward-facing [24]). These re- Ours 0.973 0.967 0.945 0.976 0.969
ft−1.0h
sultsalignwiththeaveragedresultsinthemaintext.
Metric LPIPS↓
V.Limitations PixelNeRF[44] 0.373 0.384 0.407 0.376 0.372
IBRNet[32] 0.190 0.252 0.179 0.195 0.136
Although our approach can achieve high performance for MVSNeRF[5] 0.155 0.220 0.166 0.165 0.135
viewsynthesis,itstillhasthefollowinglimitations. 1)Like NeuRay[23] 0.157 0.201 0.156 0.140 0.128
manyotherbaselines[5,32],ourmethodistailoredspecifi- ENeRF[20] 0.086 0.119 0.107 0.107 0.076
callyforstaticscenesandmaynotperformoptimallywhen GNT[30] 0.143 0.210 0.171 0.149 0.139
Ours 0.061 0.088 0.068 0.085 0.065
applieddirectlytodynamicscenes.2)Duringper-sceneop-
timization,thetrainingspeedandrenderingspeedofNeRF- NeRF 10.2h[25] 0.265 0.321 0.246 0.256 0.225
basedmethods,includingourmethod,aretime-consuming. IBRNet ft−1.0h[32] 0.129 0.170 0.104 0.156 0.099
MVSNeRF [5] 0.171 0.261 0.142 0.170 0.153
WewillexplorethepotentialofGaussianSplatting[16]in ft−15min
NeuRay [23] 0.155 0.272 0.142 0.177 0.125
generalizableNVStoaddressthisissueinthefuture. ft−1.0h
ENeRF [20] 0.071 0.106 0.097 0.102 0.074
ft−1.0h
Ours 0.057 0.082 0.067 0.080 0.061
ft−15min
Ours 0.056 0.082 0.066 0.079 0.059
ft−1.0h
TableIX.QuantitativeresultsoffivesamplescenesontheDTU
testset.MVSNeRF MacthNeRF ENeRF Ours Ground Truth
FigureIV.Qualitativecomparisonofrenderingqualitywithstate-of-the-artmethods[5,7,20]ontheDTUdatasetundergeneral-
izationandthreeviewssettings.
Scene #30 #31 #34 #38 #40 #41 #45 #55 #63 #82 #110
Metric PSNR↑
NeuRay[23] 21.10 23.35 24.46 26.01 24.16 27.17 23.75 27.66 23.36 23.84 29.90
ENeRF[20] 29.20 25.13 26.77 28.61 25.67 29.51 24.83 30.26 27.22 26.83 27.97
GNT[30] 27.13 23.54 25.10 27.67 24.48 28.10 24.54 28.86 26.36 26.09 26.93
Ours 30.94 26.95 28.21 29.87 28.62 31.24 26.01 32.46 29.24 29.78 29.30
Metric SSIM↑
NeuRay[23] 0.916 0.851 0.767 0.800 0.812 0.872 0.878 0.870 0.927 0.919 0.927
ENeRF[20] 0.981 0.937 0.934 0.946 0.947 0.960 0.948 0.973 0.978 0.971 0.974
GNT[30] 0.954 0.907 0.880 0.921 0.893 0.908 0.918 0.934 0.938 0.949 0.930
Ours 0.986 0.956 0.954 0.961 0.966 0.972 0.963 0.983 0.984 0.980 0.980
Metric LPIPS↓
NeuRay[23] 0.141 0.161 0.234 0.225 0.209 0.172 0.121 0.163 0.104 0.119 0.116
ENeRF[20] 0.052 0.108 0.117 0.118 0.120 0.091 0.077 0.069 0.048 0.066 0.069
GNT[30] 0.110 0.172 0.201 0.231 0.116 0.168 0.134 0.155 0.127 0.138 0.127
Ours 0.039 0.075 0.085 0.082 0.082 0.065 0.051 0.045 0.032 0.044 0.052
TableX.QuantitativeresultsofotherelevenscenesontheDTUtestset.MVSNeRF MatchNeRF ENeRF Ours Ground Truth
FigureV.Qualitativecomparisonofrenderingqualitywithstate-of-the-artmethods[5,7,20]ontheNeRFSyntheticdatasetunder
generalizationandthreeviewssettings.
MVSNeRF MatchNeRF ENeRF Ours Ground Truth
FigureVI.Qualitativecomparisonofrenderingqualitywithstate-of-the-artmethods[5,7,20]ontheRealForward-facingdataset
undergeneralizationandthreeviewssettings.Generalization Fine-tuning Generalization Fine-tuning Generalization Fine-tuning
FigureVII.Qualitativecomparisonofresultsbeforeandafterfine-tuningontheDTU[1], RealForward-facing[24], andNeRF
Synthetic[25]datasets.
Image MVSNeRF MatchNeRF ENeRF Ours
FigureVIII.Qualitativecomparisonofdepthmapswithstate-of-the-artmethods[5,7,20]ontheDTUdataset.
UTD
gnicaf-drawroF
laeR
citehtnyS
FReNImage MVSNeRF MatchNeRF ENeRF Ours
FigureIX.Qualitativecomparisonofdepthmapswithstate-of-the-artmethods[5,7,20]ontheNeRFSyntheticdataset.
Image MVSNeRF MatchNeRF ENeRF Ours
FigureX.Qualitativecomparisonofdepthmapswithstate-of-the-artmethods[5,7,20]ontheRealForward-facingdataset.Image Image
FigureXI.VisualizationofFusion  W  eights. W andW   representtheweightmapsoftheblen  di  ngapproachandr  eg  ressionapproach,
b r
respectively.
Image
FigureXII.Visualizationoferrormaps. Err b  a  nd   Err r representtheerrorm  a  p  s  oftheviewsobtainedth  ro  u  g  htheblendingapproach
andtheregressionapproach,respectively.Err istheerrormapofthefinalfusedtargetview.Theyellowboxesindicatethattheblending
t
approachoutperformstheregressionapproach, whiletheorangeboxesindicateregionswheretheregressionapproachoutperformsthe
blendingapproach.
UTD
UTD
gnicaf-drawroF
laeR
citehtnyS
FReNScene Chair Drums Ficus Hotdog Lego Materials Mic Ship
Metric PSNR↑
PixelNeRF[44] 7.18 8.15 6.61 6.80 7.74 7.61 7.71 7.30
IBRNet[32] 24.20 18.63 21.59 27.70 22.01 20.91 22.10 22.36
MVSNeRF[5] 23.35 20.71 21.98 28.44 23.18 20.05 22.62 23.35
NeuRay[23] 27.27 21.09 24.09 30.50 24.38 21.90 26.08 21.30
ENeRF[20] 28.29 21.71 23.83 34.20 24.97 24.01 26.62 25.73
GNT[30] 27.98 20.27 26.86 29.34 23.17 30.75 23.19 24.86
Ours 28.87 22.33 24.55 34.96 24.90 26.08 27.98 26.22
NeRF[25] 31.07 25.46 29.73 34.63 32.66 30.22 31.81 29.49
IBRNet [32] 28.18 21.93 25.01 31.48 25.34 24.27 27.29 21.48
ft−1.0h
MVSNeRF [5] 26.80 22.48 26.24 32.65 26.62 25.28 29.78 26.73
ft−15min
NeuRay [23] 27.37 21.69 23.45 32.26 26.87 23.03 28.12 24.49
ft−1.0h
ENeRF [20] 28.94 25.33 24.71 35.63 25.39 24.98 29.25 26.36
ft−1.0h
Ours 30.93 23.29 25.46 36.28 26.96 26.91 31.20 27.51
ft−15min
Ours 31.07 23.38 25.62 36.73 27.24 27.05 31.49 27.87
ft−1.0h
Metric SSIM↑
PixelNeRF[44] 0.624 0.670 0.669 0.669 0.671 0.644 0.729 0.584
IBRNet[32] 0.888 0.836 0.881 0.923 0.874 0.872 0.927 0.794
MVSNeRF[5] 0.876 0.886 0.898 0.962 0.902 0.893 0.923 0.886
NeuRay[23] 0.912 0.856 0.901 0.953 0.899 0.881 0.952 0.779
ENeRF[20] 0.965 0.918 0.932 0.981 0.948 0.937 0.969 0.891
GNT[30] 0.935 0.891 0.941 0.940 0.897 0.974 0.791 0.874
Ours 0.971 0.931 0.939 0.983 0.956 0.953 0.980 0.899
NeRF[25] 0.971 0.943 0.969 0.980 0.975 0.968 0.981 0.908
IBRNet [32] 0.955 0.913 0.940 0.978 0.940 0.937 0.974 0.877
ft−1.0h
MVSNeRF [5] 0.934 0.898 0.944 0.971 0.924 0.927 0.970 0.879
ft−15min
NeuRay [23] 0.920 0.869 0.895 0.949 0.912 0.880 0.954 0.788
ft−1.0h
ENeRF [20] 0.971 0.960 0.939 0.985 0.949 0.947 0.985 0.893
ft−1.0h
Ours 0.978 0.936 0.946 0.987 0.959 0.958 0.987 0.909
ft−15min
Ours 0.979 0.938 0.947 0.988 0.963 0.960 0.989 0.912
ft−1.0h
Metric LPIPS↓
PixelNeRF[44] 0.386 0.421 0.335 0.433 0.427 0.432 0.329 0.526
IBRNet[32] 0.144 0.241 0.159 0.175 0.202 0.164 0.103 0.369
MVSNeRF[5] 0.282 0.187 0.211 0.173 0.204 0.216 0.177 0.244
NeuRay[23] 0.146 0.211 0.184 0.113 0.126 0.165 0.104 0.256
ENeRF[20] 0.055 0.110 0.076 0.059 0.075 0.084 0.039 0.183
GNT[30] 0.065 0.116 0.063 0.095 0.112 0.025 0.243 0.115
Ours 0.035 0.089 0.064 0.060 0.064 0.054 0.021 0.175
NeRF[25] 0.055 0.101 0.047 0.089 0.054 0.105 0.033 0.263
IBRNet [32] 0.079 0.133 0.082 0.093 0.105 0.093 0.040 0.257
ft−1.0h
MVSNeRF [5] 0.129 0.197 0.171 0.094 0.176 0.167 0.117 0.294
ft−15min
NeuRay [23] 0.074 0.136 0.105 0.072 0.091 0.137 0.072 0.230
ft−1.0h
ENeRF [20] 0.030 0.045 0.071 0.028 0.070 0.059 0.017 0.183
ft−1.0h
Ours 0.024 0.080 0.059 0.028 0.052 0.044 0.015 0.181
ft−15min
Ours 0.023 0.076 0.058 0.026 0.050 0.043 0.012 0.179
ft−1.0h
TableXI.QuantitativeresultsontheNeRFSyntheticdataset.Scene Fern Flower Fortress Horns Leaves Orchids Room Trex
Metric PSNR↑
PixelNeRF[44] 12.40 10.00 14.07 11.07 9.85 9.62 11.75 10.55
IBRNet[32] 20.83 22.38 27.67 22.06 18.75 15.29 27.26 20.06
MVSNeRF[5] 21.15 24.74 26.03 23.57 17.51 17.85 26.95 23.20
NeuRay[23] 21.17 26.29 27.98 23.91 19.51 18.81 28.92 20.55
ENeRF[20] 21.92 24.28 30.43 24.49 19.01 17.94 29.75 21.21
GNT[30] 22.21 23.56 29.16 22.80 19.18 17.43 29.35 20.15
Ours 22.53 25.73 30.54 25.41 19.46 18.76 29.79 22.05
NeRF [25] 23.87 26.84 31.37 25.96 21.21 19.81 33.54 25.19
ft−10.2h
IBRNet [32] 22.64 26.55 30.34 25.01 22.07 19.01 31.05 22.34
ft−1.0h
MVSNeRF [5] 23.10 27.23 30.43 26.35 21.54 20.51 30.12 24.32
ft−15min
NeuRay [23] 22.57 25.98 29.17 25.40 20.74 20.36 27.06 23.43
ft−1.0h
ENeRF [20] 22.08 27.74 29.58 25.50 21.26 19.50 30.07 23.39
ft−1.0h
Ours 23.67 27.89 31.63 27.47 22.41 20.63 33.69 25.53
ft−15min
Ours 23.82 28.09 31.63 27.66 22.59 20.80 33.97 25.54
ft−1.0h
Metric SSIM↑
PixelNeRF[44] 0.531 0.433 0.674 0.516 0.268 0.317 0.691 0.458
IBRNet[32] 0.710 0.854 0.894 0.840 0.705 0.571 0.950 0.768
MVSNeRF[5] 0.638 0.888 0.872 0.868 0.667 0.657 0.951 0.868
NeuRay[23] 0.632 0.823 0.829 0.779 0.668 0.590 0.916 0.718
ENeRF[20] 0.774 0.893 0.948 0.905 0.744 0.681 0.971 0.826
GNT[30] 0.736 0.791 0.867 0.820 0.650 0.538 0.945 0.744
Ours 0.798 0.912 0.947 0.924 0.773 0.725 0.975 0.848
NeRF [25] 0.828 0.897 0.945 0.900 0.792 0.721 0.978 0.899
ft−10.2h
IBRNet [32] 0.774 0.909 0.937 0.904 0.843 0.705 0.972 0.842
ft−1.0h
MVSNeRF [5] 0.795 0.912 0.943 0.917 0.826 0.732 0.966 0.895
ft−15min
NeuRay [23] 0.687 0.807 0.854 0.822 0.714 0.657 0.909 0.799
ft−1.0h
ENeRF [20] 0.770 0.923 0.940 0.904 0.827 0.725 0.965 0.869
ft−1.0h
Ours 0.825 0.930 0.963 0.948 0.869 0.785 0.986 0.915
ft−15min
Ours 0.829 0.932 0.963 0.949 0.873 0.791 0.987 0.915
ft−1.0h
Metric LPIPS↓
PixelNeRF[44] 0.650 0.708 0.608 0.705 0.695 0.721 0.611 0.667
IBRNet[32] 0.349 0.224 0.196 0.285 0.292 0.413 0.161 0.314
MVSNeRF[5] 0.238 0.196 0.208 0.237 0.313 0.274 0.172 0.184
NeuRay[23] 0.257 0.162 0.163 0.225 0.253 0.283 0.136 0.254
ENeRF[20] 0.224 0.164 0.092 0.161 0.216 0.289 0.120 0.192
GNT[30] 0.223 0.203 0.157 0.208 0.255 0.341 0.103 0.275
Ours 0.185 0.126 0.101 0.130 0.188 0.243 0.150 0.176
NeRF [25] 0.291 0.176 0.147 0.247 0.301 0.321 0.157 0.245
ft−10.2h
IBRNet [32] 0.266 0.146 0.133 0.190 0.180 0.286 0.089 0.222
ft−1.0h
MVSNeRF [5] 0.253 0.143 0.134 0.188 0.222 0.258 0.149 0.187
ft−15min
NeuRay [23] 0.229 0.173 0.162 0.209 0.243 0.257 0.160 0.208
ft−1.0h
ENeRF [20] 0.197 0.121 0.101 0.155 0.168 0.247 0.113 0.169
ft−1.0h
Ours 0.156 0.090 0.069 0.093 0.123 0.192 0.052 0.106
ft−15min
Ours 0.151 0.087 0.068 0.089 0.116 0.182 0.051 0.103
ft−1.0h
TableXII.QuantitativeresultsontheRealForward-facingdataset.