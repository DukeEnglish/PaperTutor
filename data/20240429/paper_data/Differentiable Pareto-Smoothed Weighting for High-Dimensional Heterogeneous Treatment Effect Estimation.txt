Differentiable Pareto-Smoothed Weighting
for High-Dimensional Heterogeneous Treatment Effect Estimation
YoichiChikahara1 KanseiUshiyama2*
1NTTCommunicationScienceLaboratories,Kyoto,Japan
2TheUniversityofTokyo,Tokyo,Japan
Abstract One of the fundamental difficulties in high-dimensional
heterogeneoustreatmenteffectestimationisthesamplese-
lectionbiasinducedbyconfounders,i.e.,thefeaturesofan
There is a growing interest in estimating hetero- individual that affect their treatment choice and outcome.
geneoustreatmenteffectsacrossindividualsusing
Forinstance, inthecaseof medicaltreatment,apossible
theirhigh-dimensionalfeatureattributes.Achiev- confounderisage:Olderpatientsavoidchoosingsurgery
ing high performance in such high-dimensional duetoitsriskwhiletheyhavehighermortalityingeneral
heterogeneoustreatmenteffectestimationischal-
[Zengetal.,2022].Sincetherearefewerrecordsofolder
lengingbecauseinthissetup,itisusualthatsome patients who received surgery treatment, it is not easy to
featuresinducesampleselectionbiaswhileothers accuratelypredicttheoutcomeofsurgeryonolderpatients,
donotbutarepredictiveofpotentialoutcomes.To thusmakingitdifficulttoestimatethetreatmenteffect.
avoidlosingsuchpredictivefeatureinformation,
Toaddresssuchsampleselectionbias,itiscrucialhowto
existingmethodslearnseparatefeaturerepresen-
breakthedependenceoftreatmentchoiceonconfounders. tationsusingtheinverseofprobabilityweighting
A promising approach for high-dimensional setup is rep-
(IPW).However,duetothenumericallyunstable
IPWweights,theysufferfromestimationbiasun- resentation learning [Johansson et al., 2016, Shalit et al.,
2017], which estimates the potential outcomes under dif-
derafinitesamplesetup.Todevelopanumerically
ferent treatment assignments by extracting the balanced
robustestimatorviaweightedrepresentationlearn-
ing,weproposeadifferentiablePareto-smoothed featurerepresentationthatislearnedsuchthatitsdistribu-
tionisidenticalbetweentreatedanduntreatedindividuals.
weightingframeworkthatreplacesextremeweight
valuesinanend-to-endfashion.Experimentalre- However,sincethisapproachconvertsallinputfeaturesto
sultsshowthatbyeffectivelycorrectingtheweight
a single balanced representation, if some features are ad-
values,ourmethodoutperformstheexistingones, justment variables (a.k.a., risk factors) [Brookhart et al.,
includingtraditionalweightingschemes. 2006],whicharenotrelatedtosampleselectionbiasbutare
usefulforoutcomeprediction,itmayinadvertentlyelimi-
natesuchusefulfeatureinformation,leadingtoinaccurate
1 INTRODUCTION treatmenteffectestimation[Saueretal.,2013].Thisissueis
serious,especiallyinhigh-dimensionalsetupwhereinput
In this paper, we tackle the problem of estimating het- featuresoftencontainadjustmentvariables.Moreover,due
erogeneoustreatmenteffectsacrossindividualsfromhigh- tothelackofpriorknowledgeaboutfeatures,itisdifficult
dimensional observational data. This problem, which we forpractitionerstocorrectlyseparateadjustmentvariables
callhigh-dimensionalheterogeneoustreatmenteffectesti-
fromconfounders.Suchfeatureseparationmightbeimpos-
mation,offerscrucialapplications.Applicationexamples
sibleifoneattemptstoinputthefeatureembeddingsofa
include the evaluation of medical treatment effects from complex object (e.g., texts, images, and graphs) that are
numerousattributes[Curthetal.,2024,Shalit,2020]and constructedfrompre-trainedgenerativemodels,including
theassessmentoftheadvertisingeffectsfromeachuser’s
largelanguagemodels(LLMs)[Keithetal.,2020].
manyattributes[Bottouetal.,2013,Wangetal.,2015].
Tofulfillsuchimportantbutoftenoverlookedneeds,several
data-drivenfeatureseparationmethodshavebeenproposed
*WorkduringsummerinternshipatNTTCommunicationSci- [Kuangetal.,2017,HassanpourandGreiner,2020,Kuang
enceLaboratories.
Acceptedforthe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).
4202
rpA
62
]LM.tats[
1v38471.4042:viXraet al., 2020, Wang et al., 2023]. Among these methods, 2 PRELIMINARIES
disentangledrepresentationsforcounterfactualregression
(DRCFR)[HassanpourandGreiner,2020]aimtoavoidlos- 2.1 PROBLEMSETUP
ingusefulfeatureinformationforheterogeneoustreatment
effect estimation by learning different representations for Suppose that we have a sample of n individuals, D =
confoundersandadjustmentvariables.Toachievethis,item- {(a,x,y)}n i.∼i.d. P(A,X,Y), where A ∈ {0,1} is a binary
ploysthetechniqueofinverseprobabilityweighting(IPW)
treai tmi enti (Ai=1
=1ifanindividualistreated;otherwiseA=0),
[RosenbaumandRubin,1983],whichperformsweighting X =[X ,...,X ]⊤isd-dimensionalfeatures(a.k.a.,covari-
1 d
basedontheinverseoftheprobabilitycalledapropensity ates),andY isanoutcome.LetY0andY1bepotentialout-
score. However, such weights often take extreme values comesi.e.,theoutcomeswhenuntreated(A=0)andwhen
(especially in high-dimensional setup [Li and Fu, 2017]), treated(A = 1),whicharegivenbyY = AY1+(1−A)Y0.
andevenaslightpropensityscoreestimationerrormaylead Thenatreatmenteffectforanindividualisdefinedastheir
toalargetreatmenteffectestimationerror.
difference,i.e.,Y1−Y0[Rubin,1974].
Toresolvethisissue,wedevelopaweightcorrectionframe- Weconsidertheheterogeneoustreatmenteffectestimation
workbyutilizingthetechniqueinextremevaluestatistics,
problem, where we take as input sample D and feature
called Pareto smoothing [Vehtari et al., 2024], which re-
valuesxandoutputtheestimateofaconditionalaverage
placestheextremeweightvalueswiththequantilesofgen- treatmenteffect(CATE)conditionedonX =x:
eralizedParetodistribution(GPD).Indeed,Zhuetal.[2020]
(cid:104) (cid:105)
have already adopted Pareto smoothing and empirically E Y1−Y0 |X =x . (1)
shownthatitcanconstructanumericallystableestimator
oftheaveragetreatmenteffect(ATE)overallindividuals. CATEisanaveragetreatmenteffectinasubgroupofindi-
vidualswhohaveidenticalfeatureattributesX =x.
Toestimateheterogeneoustreatmenteffectsacrossindivid-
uals,weproposeaPareto-smoothedweightingframework ToestimatetheCATEin(1),wemaketwostandardassump-
thatcanbecombinedwiththeweightedrepresentationlearn- tions.Oneisconditionalignorability,{Y0,Y1}⊥⊥A|X;this
ingapproach.Thisgoal,however,isdifficulttoachievebe- conditionalindependenceissatisfiediffeaturesXcontain
causetheweightcorrectionwithParetosmoothingrequires all confounders and include only pretreatment variables,
the computation of the rank (or position) of each weight
whicharenotaffectedbytreatmentA.Theotherispositiv-
value;thiscomputationisnon-differentiableandprevents ity,0<π(x)<1forallx,whereπ(x)(cid:66)P(A=1|X =x)is
gradientbackpropagation.Toovercomethisdifficulty,we theconditionaldistributionmodelcalledapropensityscore.
utilizethetechniqueofdifferentiableranking[Blondeletal.,
OurgoalistoachievehighCATEestimationperformance
2020]andestablishadifferentiableweightcorrectionframe-
inhigh-dimensionalsetup,wherethenumberoffeaturesd
workfoundedonParetosmoothing.Thisideaofcombining
isrelativelylarge.Underthissetup,removingthesample
Pareto smoothing and differentiable ranking, which have
selectionbiasbytransformingallfeaturesX intoasingle
beenstudiedintotallydifferentfields(i.e.,extremevalue
balancedrepresentationmightbeoverlysevere,whichleads
statistics and differentiable programming), allows for an toinaccuratetreatmenteffectestimation.Althoughseveral
effectiveend-to-endlearningforhigh-dimensionalhetero-
representationlearningmethodsaimtoavoidsuchanoverly
geneoustreatmenteffectestimation.
severe balancing [Kuang et al., 2017, Wang et al., 2023],
Ourcontributionsaresummarizedasfollows. mostofthemaredesignedATE,notCATE.Bycontrast,the
DRCFRmethod[HassanpourandGreiner,2020]focuseson
• WeproposeadifferentiablePareto-smoothedweight- CATEestimationandeffectivelybalanceshigh-dimensional
ingframeworkthatreplacestheextremeIPWweight featuresXviaweightedrepresentationlearning.
valuesinanend-to-endfashion.Tomakethisweightre-
placementproceduredifferentiable,weutilizethetech-
2.2 WEIGHTEDREPRESENTATIONLEARNING
niqueofdifferentiableranking[Blondeletal.,2020].
• Takingadvantageofthedifferentiability,webuildour DRCFR[HassanpourandGreiner,2020]isfoundedonthe
weight correction framework on the neural-network- graphical model in Figure 1, where treatment A is deter-
based weighted representation learning method (i.e., mined by functions Γ(X) and ∆(X), while outcome Y is
DR-CFR[HassanpourandGreiner,2020])toperform given by ∆(X) and Υ(X). Following this model, it formu-
data-driven feature separation for high-dimensional
latesΓ(X),∆(X),andΥ(X)asthreeneuralnetworkencoders,
heterogeneoustreatmenteffectestimation. eachofwhichextractstherepresentationofinstrumental
variables(i.e.,thefeaturesthatinfluenceAbutdonotaffect
• Weexperimentallyshowthatourmethodeffectively
Y),confounders,andadjustmentvariables,respectively.For
learnsthefeaturerepresentationsandoutperformsthe example,inmedicaltreatmentsetup,apossibleinstrumen-
existingones,includingtraditionalweightingschemes. talvariable,confounder,andadjustmentvariablewouldbe
2First,welearnpropensityscoreπ(whilefixingothermodel
Features X
parameters)byminimizingthecrossentropyloss:
1(cid:88)n (cid:16)
min− a log(π(Γ(x),∆(x)))
Γ(X) ∆(X) Υ(X) π n i i i
i=1
(cid:17)
+(1−a)log(1−π(Γ(x),∆(x))) +λ Ω(π), (2)
i i i π
where Ω(·) is some regularizer that penalizes the model
complexity,andλ >0isaregularizationparameter.
π
A Y
Second,welearntheothermodelparameters(withπ’spa-
rametersfixed)byminimizingtheweightedloss:
Figure1:GraphicalmodelillustrationofDRCFRmethod 1(cid:88)n
Γ,∆m ,Υi ,hn
0,h1 n
i=1
w il(y i,hai(∆(x i),Υ(x i)))
income,age,andsmokinghabits,respectively.
+λΥMMD(cid:16)(cid:8)Υ(x i)(cid:9) i:ai=0,(cid:8)Υ(x i)(cid:9) i:ai=1(cid:17)
(cid:16) (cid:17)
Tolearntherepresentationsofsuchfeatures,DRCFRmini- +λ −πΩ Γ,∆,Υ,h0,h1 , (3)
mizestheweightedlossbycomputingtheweightsbasedon
theinverseofthepropensityscore,givenbyπ(Γ(X),∆(X)). where w i is the weight that is given by propensity score
π(Γ(x i),∆(x i)),listhepredictionlossforoutcomey i,λΥ >0
AstrongadvantageofDRCFRisthatitcanavoidlosingthe andλ >0areregularizationparameters,1 andMMDde-
−π
informationofadjustmentvariables,representedbyΥ(X),
notesthekernelmaximummeandiscrepancy(MMD)[Gret-
which is useful for outcome prediction. However, since tonetal.,2012],whichmeasuresthediscrepancybetween
theinverseofconditionalprobability,π(Γ(X),∆(X)),often empirical conditional distributions Pˆ(Υ(X) | A = 0) and
yieldsextremevalues,underfinitesamplesettings,evena Pˆ(Υ(X)| A=1).RegularizingthisMMDtermenforcesΥ
slightestimationerrorofπleadstoalargeweightestimation tohavenoinformationabouttreatmentA,thusmakingΥ(X)
error.Thisnumericalinstabilityofweightestimationmakes agoodrepresentationofadjustmentvariables.
itdifficulttoachievehighCATEestimationperformance.
ToachievehighCATEestimationperformance,itisessen-
tial how to compute the weight value, i.e., w in (3). In
i
3 PROPOSEDMETHOD theDRCFRmethod[HassanpourandGreiner,2020],the
weightisformulatedusingimportancesampling,whichem-
ploysthedensity-ratio-basedweighttoconstructaweighted
Toimprovetheestimationstabilityofweightedrepresenta-
tionlearning,weproposeadifferentiableweightcorrection estimatorofexpectedvalue.Toestimatetheexpectedout-
comepredictionlossoverobservedindividuals(A=a)and
frameworkthatcanbeusedinanend-to-endfashion. i
theoneoverunobservedindividuals(A=1−a),DRCFR
i
formulatestheweightasthesumoftwodensityratios:
3.1 OVERVIEW
P(Γ(x),∆(x)| A=a) P(Γ(x),∆(x)| A=1−a)
w = i i i + i i i
i P(Γ(x),∆(x)| A=a) P(Γ(x),∆(x)| A=a)
To estimate the CATE in (1), following the DRCFR i i i i i i
P(A=1−a |Γ(x),∆(x)) P(A=a)
method [Hassanpour and Greiner, 2020], we perform =1+ i i i i
weighted representation learning. To achieve this, we P(A=1−a i) P(A=a i|Γ(x i),∆(x i))
learn the three model components: the feature represen- P(A=a) (cid:32) 1 (cid:33)
=1+ i −1 (4)
tations(i.e.,Γ(X),∆(X),andΥ(X)inFigure1),propensity P(A=1−a) P(A=a |Γ(x),∆(x))
i i i i
scoremodelπ(Γ(X),∆(X)),andoutcomepredictionmodels
1 1
h0(∆(X),Υ(X)) and h1(∆(X),Υ(X)), where h0 and h1 are ∝ P(A=a |Γ(x),∆(x)) (cid:66) π (Γ(x),∆(x)),
usedtopredictpotentialoutcomesY0andY1,respectively. i i i ai i i
where π (X) = aπ(X)+(1−a)(1−π(X)). Here weight
DRCFR jointly optimizes these three model components ai i i
w is proportional to the inverse of the propensity score
byminimizingtheweightedloss.However,weempirically i
π (Γ(x),∆(x)).SincesuchanIPWweightoftentakesan
observedthatsuchajointoptimizationisdifficult.Apos- ai i i
extremevalue,theweightestimationisnumericallyunstable,
siblereasonisthatthelossfunctiondramaticallychanges
leadingtoinaccurateCATEestimation.Thisissueisserious
withtheIPWweightsandhencesubstantiallyvarieswith
inhigh-dimensionalsetupduetothedifficultyofcorrectly
theparametervaluesofpropensityscoreπ.Forthisreason,
estimatingthepropensityscore[Assaadetal.,2021].
weseparatelylearnπandperformanalternateoptimization
thatrepeatedlytakesthefollowingtwosteps. 1Subscript−πdenotestheothermodelcomponentsthanπ.
3To resolve this issue, we improve the weight stability by where⌊n⌋denotesafloorfunction,whichreturnsthegreatest
replacinganextremevalueofw in(4).Todoso,weutilize integerthatislessthanorequalton.Lettingw ≤ ··· ≤
i [1]
theweightstabilizationtechnique,calledParetosmoothing. w betheweightssortedinanascendingorder,theM+1
[n]
largestonesaredenotedbyw ,...,w .
[n−M] [n]
3.2 WEIGHTCORRECTIONVIAPARETO FollowingVehtarietal.[2024],wesetlocationparameterµ
SMOOTHING tothe(M+1)-thlargestIPWweightvalue,i.e.,
µˆ =w . (9)
Paretosmoothing[Vehtarietal.,2024]isthetechniquefor [n−M]
improvingtheweightstabilityofimportancesampling. Bycontrast,weestimateσandξ usingw [n−M+1],...,w [n].
Among several estimators, we employ the standard one
AccordingtoVehtarietal.[2024],thistechniquehastwoad-
calledprobabilityweightedmoment(PWM)[Hoskingand
vantages.First,itcanyieldalessbiasedestimator,compared
Wallis,1987],2whichconstructstheestimatorsofσandξ
with weight truncation, which replaces extreme weights
usingthefollowingweightedmomentstatistic:
naivelywithconstants[Crumpetal.,2009,Ionides,2008]:
α
=E(cid:2) (1−F(W))s(W−µ)(cid:3) s∈(cid:8) 0,1(cid:9)
. (10)
 s
wT i runc.
(cid:66)
 w
UL
i
i
i
if
f
f
w ULi
≤
≤<
w
wL
i <U , (5) R ofo Wugh −ly µs wpe ita hki wn eg i, gs hta tt (i 1st −ic Fα (s Win ))( s1 a0 n) dis isa ew ste ii mgh at te ed da av serage
i
whereL>0andU >0arethetruncationthresholds.Sec- αˆ = 1
(cid:88)n
(cid:0) w −µˆ(cid:1) , (11)
ond, it can be combined with self-normalization, which 0 M [i]
i=n−M+1
p atr ie vv een tots et ah ce hw oe ti hg eh rts bf yro dm ivib de ii nn gg eto ao chsm wa el il go hr tt vo ao lula erg be yr ie tl s-
αˆ = 1
(cid:88)n
(n−i)(cid:0) w −µˆ(cid:1) . (12)
1 M [i]
empiricalmeanunderthesametreatmentassignment: i=n−M+1
wN
i
orm. (cid:66) ww A=i ai, wherewA=ai = (cid:80) (cid:80)n j=
n
j1 =1I( Ia (aj j= =a i a) iw )j . (6) Usingαˆ 0andαˆ 1,the σˆP =WM 2αˆm 0e αˆt 1ho ,destimatesσandξa (s
13)
αˆ −2αˆ
0 1
Here I(a = a) is the indicator function that takes 1 if αˆ
j i ξˆ =2− 0 . (14)
a j =a i;otherwise,0.Weexperimentallyconfirmedthatper- αˆ 0−2αˆ 1
formingself-normalizationoverPareto-smoothedweights
leadstobetterCATEestimationperformance(Section4.1). 3.2.2 WeightReplacementwithGPDQuantiles
Toconstructaweightedestimatorthatisnumericallyrobust
Second, we replace the M largest weight values, i.e.,
toweightestimationerror,Paretosmoothingreplacesthe
w [n−M+1],...,w [n],withthequantilesofthefittedGPDwith
extremelylargeweightvalueswiththeGPDquantiles.To parameters(µˆ,σˆ,ξˆ)in(9),(13),and(14).
achievethis,ittakestwosteps:theGPDparameterestima-
tionandtheweightreplacement. Since the quantile function is given by the inverse of the
cumulativedistributionfunction,wereplaceweightvalue
3.2.1 GPDParameterFitting
w [n−M+m](m=1,...,M)with m− M1/2-quantileas
(cid:32) (cid:33)
First,wefittheGPDparameterstolargeIPWweightvalues.
w [n−M+m]
=Fˆ−1 m− M1/2
, (15)
SupposethatarandomvariableW followstheGPD.Then where Fˆ denotes the fitted GPD cumulative distribution
itsGPDcumulativedistributionfunctionisdefinedas function.Bycontrast,wedonotchangetheotherweight
F(w)= 

1 1− −(cid:16) e1 −w+ σ−µξ(w σ−µ)(cid:17)−1 ξ (( ξξ =(cid:44)0 0)
)
(7) wva elu ce as wn, si. ue =m., m Iw (a[ i1 r ≥] i, z. e n.. t −h, ew M[ wn− +eM i 1g]. )hH t Fˆr −e e 1n p (cid:32)c la ie c, −ele m (ntt ei −nn tg Mpi r )o= −cen 1d− /u 2rM e (cid:33)a+ s m,
where µ ∈ R, σ > 0, and ξ ∈ R are location, scale, and [i] M
shapeparameters,respectively. +(1−I(i≥n−M+1))w . (16)
[i]
WefittheseGPDparameterstotheM+1largestIPWweight
2Although we empirically observed that using PWM leads
values.HereM(0< M <n)isgivenbyheuristic;following to good CATE estimation performance, GPD parameter fitting
Vehtarietal.[2024],wedetermineitby mightnotbeeasyingeneral.However,accordingtoVehtarietal.
[2024,Section6],onecaneffectivelyevaluatethereliabilityof
(cid:26)(cid:22)n(cid:23) √ (cid:27)
M =min ,⌊3 n⌋ , (8) GPDfittingbyemploying theestimatedvalueof GPD’sshape
5 parameter,ξˆ,whichdeterminestheheavinessofdistributiontail.
4IPWweightsby(16),andthenlearnsthefeaturerepresenta-
r
tions.Thisapproach,however,requirestofitapropensity
r ε(ε = 0.1)
scoremodeldirectlytofeaturesX,nottheirrepresentation.
r ε(ε = 0.2)
Sinceaccuratelyestimatingapropensityscoremodelfrom
r
1
high-dimensionalfeaturesXisconsiderablydifficult,dueto
themodelmisspecificationerror,suchaseparatelearning
approachwillleadtoCATEestimationbias.Weexperimen-
tallyshowitspoorperformanceinSection4.1.
For this reason, we develop a joint learning approach by
makingthenon-differentiablecomputationinParetosmooth-
w
1
ingdifferentiable.
Figure2:Illustrationofrankfunctionr=r(w)(black)and
differentiablerankfunctionsr=r (w)(orangeandgreen). 3.4 MAKINGPARETOSMOOTHING
ε
Herewetakeinputvectorw=[w ,1,2,3]⊤,varyw ’svalue, DIFFERENTIABLE
1 1
andlookathowitsrankr ∈rchanges.Whenregularization
1
parameterε→0,r convergestor[Blondeletal.,2020]. 3.4.1 DifferentiableApproximation
ε
Theweightreplacementformulain(16)requiresthecompu-
In this paper, we propose to utilize the weight replace- tationofthetwotroublesomepiecewiseconstantfunctions.
ment formula in (16) to improve the estimation stability Oneisrankfunctionr,whichisneededtoobtaintheposi-
ofweightedrepresentationlearning.Unfortunately,wecan- tionofweightw inthesortedvector[w ,...,w ]⊤,and
i [1] [n]
notdirectlyemploythisformulainanend-to-endmanner theotherisindicatorfunctionI(i≥n−M+1).
becauseitneedsnon-differentiablecomputations.
Tomakerankfunctionr differentiable,weutilizethedif-
ferentiablerankingtechnique[Cuturietal.,2019,Blondel
3.3 NON-DIFFERENTIABLEPROCEDURES etal.,2020],whichapproximatesrankfunctionr(w)with
a differentiable function. Among the recent methods, we
ThemaindifficultyofusingParetosmoothingforweighted selectacomputationallyefficientone[Blondeletal.,2020],
representationlearningisthatitrequiresthecomputationof which works with O(nlogn) time and O(n) memory com-
therankofeachIPWweight. plexity. With this method, we approximate rank function
Ranking is an operation that takes input vector w = r(w)asthesolutiontotheregularizedlinearprogramming
[w ,...,w ]⊤andoutputsthepositionofeachelementw in (LP) that contains the l2 regularization term with regular-
1 n i
thesortedvector[w ,...,w ]⊤,wherew ≤ ··· ≤ w . izationparameterε>0.Thesolution,r ε(w),isapiecewise
[1] [n] [1] [n]
To illustrate this operation, consider the case with n = 3. linearfunctionthatcanwellapproximaterankfunctionr
Forinstance,ifwsatisfiesw ≤ w ≤ w ,sincew = w ,
(asillustratedinFigure2)andisdifferentiablealmostevery-
3 1 2 1 [2]
w = w , and w = w hold, the rank of w is given as where,thusgreatlyfacilitatinggradientbackpropagation.
2 [3] 3 [1]
vectorr=[2,3,1]⊤.Formally,suchanoperationcanbeex-
RegardingindicatorfunctionIin(16),weapproximateitas
pressedasr=r(w),usingfunctionr,calledarankfunction
thesigmoidfunctionς:
(SeeAppendixAforthedefinitionoffunctionr).
1
Unfortunately,thisrankfunctionisnotdifferentiablewith I(i≥ j)≃ς(i, j)(cid:66) , (17)
1+e−κ(i−j)
respecttoinputw.Toseethis,considerw = [w ,1,2,3]⊤
1
andobservehowtherankofw varieswhenweincrease whereκ>0isahyperparameter.
1
w ’svalue.Inthiscase,itsrank,r ,isgivenasapiecewise
1 1
constantfunction,asillustratedastheblacklineinFigure2.
3.4.2 ReformulationofGPDParameterEstimators
Sincethederivativeofsuchapiecewiseconstantfunction
is always zero or undefined, we cannot perform gradient
Toemploydifferentiablerankr=r (w)forParetosmooth-
backpropagationandhencecannotemploytheweightcor- ε
ing,sinceittakescontinuousvalues,weneedtomodifythe
rectiontechniquein(16)inanend-to-endmanner.There-
GPDparameterestimators,i.e.,µˆ,σˆ,andξˆ.
fore,withsuchanon-differentiablerankfunction,wecannot
useParetosmoothingforweightedrepresentationlearning, As regards location parameter µˆ in (9), since this esti-
whichjointlylearnsthepropensityscoremodel,thefeature mator is given as w , i.e., the largest weight among
[n−M]
representations,andtheoutcomepredictionmodels. w ,...,w ,wereformulateitas
[1] [n−M]
Onemayconsideraseparatelearningapproachthattrains µ˜ =w, wherei=argmax(cid:8) r |r ≤n−M(cid:9) .
i i i
thepropensityscoremodel,computesthePareto-smoothed i
5Toreformulateσˆ andξˆin(13)and(14),werephraseestima- Algorithm 1 Differentiable Pareto-Smoothed Weighting
torsαˆ andαˆ in(11)and(12).Withnon-differentiablerank (DPSW)
0 1
r=r(w),theseestimatorsareequivalentlyreformulatedby
1: InitializetheparametersofΓ,∆,Υ,π,h0,andh1
rewritingthesummationoverw [n−M+1],...,w [n]in(11)and
2: whilenotconvergeddo
(12)withindicatorfunctionIas
3: whilenotconvergeddo
1 (cid:88)n 4: Samplemini-batchfromD={(a i,x i,y i)}n i=1
αˆ = I(r ≥n−M+1)(w −µˆ) 5: Updateπbyminimizingcrossentropylossin(2)
0 M i i
i=1 6: endwhile
1 (cid:88)n 7: whilenotconvergeddo
αˆ 1 = M I(r i ≥n−M+1)(n−r i)(w i−µˆ). 8: Samplemini-batchfromD={(a i,x i,y i)}n i=1
i=1 9: forinstanceiinmini-batchdo
Hence,whengivendifferentiablerankr=r (w),byreplac- 10: Computeweightw iby(4)
ε
11: endfor
ingindicatorfunctionIwithsigmoidfunctionςin(17),we
makeαˆ andαˆ differentiablewithrespecttor: 12: Computedifferentiablerankr=r ε(w)
0 1 13: EstimateGPDparametersasµ˜,σ˜,andξ˜
1
(cid:88)n 14: forinstanceiinmini-batchdo
α˜ 0 = M˜ ς(r i,n−M+1)(w i−µ˜), (18) 15: Replaceeachweightw iwithw˜ iin(20)
i=1 16: endfor
1 (cid:88)n 17: UpdateΓ,∆,Υ,h0,andh1byminimizingpredic-
α˜ = ς(r,n−M+1)(n−r)(w −µ˜), (19)
1 M˜ i i i tionlossin(3)withPareto-smoothedweights{w˜ i}
i=1
18: endwhile
whereM˜ =(cid:80)n ς(r,n−M+1).Bysubstitutingα˜ andα˜ 19: endwhile
i=1 i 0 1
forαˆ andαˆ in(13)and(14),wecomputescaleandshape
0 1
parametersasσ˜ andξ˜,respectively.
isabsolutelycontinuous,whichisemployedtoprovethe
asymptoticconsistencyinTheorem1ofVehtarietal.[2024].
3.4.3 OverallAlgorithm This assumption holds if each activation is differentiable
almost everywhere (i.e., differentiable except on a set of
Using the GPD cumulative distribution function, F˜, with measure zero). However, for instance, using the rectified
parameters(µ˜,σ˜,ξ˜),wereplaceeachweightw in(4)with linearunit(ReLU)inpropensityscoremodelπmakesthe
i
distributionofIPWweightdiscontinuous,thusviolatingthe
(cid:32) (cid:32) (cid:33)(cid:33)
w˜
=ς(r,n−M+1)F˜−1
ζ
r i−(n−M)−1/2 assumptionofParetosmoothing.
i i M
+(1−ς(r i,n−M+1))w i, (20) 4 EXPERIMENTS
where ζ(x) (cid:66) min{max{x,0},1}isafunction thatforces
4.1 SEMI-SYNTHETICDATA
input x to lie in [0,1]. Using Pareto-smoothed weight w˜
i
insteadofw,weminimizetheobjectivefunctionin(3).
i
First,weevaluatedtheCATEestimationperformanceusing
Algorithm1summarizesourmethod.Toalternatelymin- semi-syntheticbenchmarkdatasets,wherethetrueCATE
imize the objective functions in (2) and (3), we perform valuesareavailable,unlikereal-worlddata.
stochastic gradient descent [Kingma and Ba, 2015]. Af-
Data:Weselectedthetwohigh-dimensionaldatasets:the
ter the convergence, we estimate the CATE in (1) by
h1(∆(x),Υ(x))−h0(∆(x),Υ(x)). NewsandtheAtlanticCausalInferenceConference(ACIC)
datasets[Johanssonetal.,2016,Shimonietal.,2018].
ComparedwithDRCFR[HassanpourandGreiner,2020],
The News dataset is constructed from n = 5000 articles,
our method requires additional time to compute Pareto-
randomlysampledfromtheNewYorkTimescorpusinthe
smoothedweights(lines12-16inAlgorithm1).Inpartic-
ular, computing differentiable rank (line 12) requires the
UCIrepository.3Thetaskistoinfertheeffectoftheview-
ing device (desktop (A = 0) or mobile (A = 1)) on the
time complexity O(BlogB) for mini-batch size B, which
readers’experienceY.FeaturesXarethecountofd =3477
is needed to evaluate the objective function in (3) and its
wordsineacharticle.TreatmentAandoutcomeY aresim-
gradientforeachiterationinthetrainingphase.
ulatedusingthelatenttopicvariablesobtainedbyfittinga
Remark:Strictlyspeaking,thechoiceofactivationfunc- topic model on X. The ACIC dataset is derived from the
tionsinpropensityscoreπandfeaturerepresentationsΓand clinical measurements of d = 177 features in the Linked
∆isimportanttosatisfytheassumptionofParetosmooth-
ingthatthedistributionoftheimportancesamplingweight 3https://archive.ics.uci.edu/dataset/164/bag+of+words
6Birth and Infant Death Data (LBIDD) [MacDorman and Table1:MeanandstandarddeviationoftestPEHEonsemi-
Atkinson,1998]andisdevelopedforthedataanalysiscom- syntheticdatasets(Lowerisbetter)
petition called ACIC2018. Here, we randomly picked up
n=5000observationsandprepared20datasets.Withboth Method News(d =3477) ACIC(d =177)
semi-syntheticdatasets,werandomlyspliteachsampleinto
LR-1 3.35±1.28 0.72±0.07
training,validation,andtestdatawitharatioof60/20/20.
LR-2 5.36±1.75 3.82±0.15
Baselines: To evaluate our method (DPSW) and its vari-
SL 2.83±1.11 1.69±0.52
ant that performs self-normalization (DPSW Norm.),
TL 2.55±0.82 2.23±0.50
we consider 15 baselines. With DRCFR [Hassanpour
XL 2.77±1.01 1.05±0.72
and Greiner, 2020], we tested the four different weight-
DRL 23.9±5.96 3.77±8.96
ing schemes: no weight modification (DRCFR), self-
normalization(DRCFRNorm.;Eq.(6)),theweighttrun- CF 3.84±1.67 3.55±0.19
cationwiththethresholdsuggestedbyCrumpetal.[2009] CFDML 2.69±1.06 1.18±0.32
(DRCFRTrunc.;Eq.(5)),andtheschemethatignoresthe TARNet 4.92±1.80 3.31±0.51
prediction loss for the individuals with extreme weights GANITE 2.68±0.66 3.69±0.77
based on the same threshold (DRCFR Ignore). We also
DRCFR 2.38±0.66 0.98±0.07
tested a separate learning approach (PSW; Section 3.3),
DRCFRNorm. 2.37±0.94 0.73±0.12
whichtrainspropensityscoreπwith{(a,x)}n beforehand
andlearnsonly∆(X)andΥ(X)usingthi ePi ari= e1
to-smoothed
DRCFRTrunc. 2.42±0.79 1.06±0.06
DRCFRIgnore 2.35±0.75 0.84±0.06
IPWweights.Otherbaselinesinclude(i)linearregression
PSW 4.03±1.35 0.71±0.01
methods:asinglemodelwithtreatmentAaddedtoitsinput
(LR-1)andtwoseparatemodelsforeachtreatment(LR-2); DPSW 2.20±0.72 0.57±0.03
(ii)meta-learnermethods:theS-Learner(SL),theT-Learner DPSWNorm. 2.10±0.66 0.52±0.16
(TL),theX-Learner(XL),andtheDR-Learner(DRL);(iii)
tree-basedmethods:thecausalforest[Atheyetal.,2019]
(CF)andthevariantcombinedwithdouble/debiasedma-
theself-normalizationofPareto-smoothedweightscanfur-
chinelearning[Chernozhukovetal.,2018](CFDML);and therimprovethestabilityoftheweightestimation.
(iv)neuralnetworkmethods:thetreatment-agnosticregres-
Weighted representation learning methods (DR-CFR and
sionnetwork[Shalitetal.,2017](TARNet)andthegenera-
DPSW)performedbetterthanotherneuralnetworkmethods
tiveadversarialnetwork[Yoonetal.,2018](GANITE).We
(TARNet and GANITE), especially on the ACIC dataset.
detailthesettingsofthesebaselinesinAppendixB.1.
GiventhattreatmentAandoutcomeY ofthisdatasetwere
Settings:RegardingourmethodandDRCFR,weusedthree- simulated using different features in X, these results em-
layeredfeed-forwardneuralnetworks(FNNs)toformulate phasizetheimportanceofperformingdata-drivenfeature
feature representations Γ(X), ∆(X), and Υ(X), propensity separationviaweightedrepresentationinsuchasetup.
scoreπandoutcomepredictionmodelsh0andh1.
PSWworkedmuchworseontheNewsdatasetthanDPSW,
Wetunedthehyperparameters(e.g.,parameterεofdifferen-
indicating that fitting a propensity score directly to high-
tiablerankr ε(w)inourmethod)byminimizingtheobjective dimensionalfeaturesXleadstoaseveremodelmisspecifi-
functionvalueonvalidationdata;suchhyperparametertun- cationerror,makingthesubsequentweightedrepresentation
ingisstandardforCATEestimation[Shalitetal.,2017]. learningdifficult,evenwiththePareto-smoothedweights.
By contrast, our joint learning approach well performed,
Performancemetric:FollowingHill[2011],weusedthe
precisionintheestimationofheterogeneouseffect(PEHE),
thankstotheuseofdifferentiableParetosmoothing.
(cid:113)
PEHE(cid:66)
1(cid:16)
(y1−y0)−τˆ
(cid:17)2
,wherey0andy1arethetrue
n i i i i i
potential outcomes, and τˆ denotes the predicted CATE 4.2 SYNTHETICDATA
i
value.Wecomputedthemeanandthestandarddeviationof
testPEHEover50realizationsofpotentialoutcomes(the Next,weinvestigatedhowwellourmethodcanlearnthe
Newsdataset)and20realizations(theACICdataset). featurerepresentationsusingsyntheticdata,wherethedata-
generatingprocessesareentirelyknown.
Results:Table1presentsthetestPEHEsontheNewsand
ACICdatasets. Data:FollowingHassanpourandGreiner[2020],wesimu-
latedsyntheticdata.WerandomlygeneratedfeaturesX =
Ourproposedframework(DPSWandDPSWNorm.)out-
performedallbaselines,demonstratingtheireffectiveness
[XΓ,X∆,XΥ]⊤ ∈ Rd (d = 15,18,...,30).Then,byregard-
ingfeaturesubsetsXΓ ∈Rd/3,X∆ ∈Rd/3,andXΥ ∈Rd/3as
in CATE estimation from high-dimensional data. DPSW
instrumentalvariables,confounders,andadjustmentvari-
Norm.achievedlowerPEHEsthanDPSW,implyingthat
ables, respectively, we sampled binary treatment A using
7(a)Relative difference between |W𝚪1| and |W−1𝚪| (b)Relative difference between |W∆1| and |W−1 ∆| (c)Relative difference between |W𝚼1| and |W−1 𝚼| (d) Test PEHE
Number of features d Number of features d Number of features d Number of features d
Figure3:LearnedencoderparameterdifferencesandtestPEHEsonsyntheticdata.(a):ValuedifferenceofW1inencoder
Γ(X);(b):ValuedifferenceofW1 inencoder∆(X);(c):ValuedifferenceofW1 inencoderΥ(X);(d)TestPEHEs.With
TARNet,sinceitlearnssingleencoder,wecomputedallparametervaluedifferenceswithweightmatrixinsameencoder.
XΓ andX∆ andoutcomeY byemployingX∆ andXΥ (See andDRCFR,demonstratingtheimportanceofdata-driven
AppendixB.2forthedetail).Wespliteachof20datasets featureseparationviaweightedrepresentationlearning.By
(n=20000)witha50/25/25training/validation/testratio. contrast,ourmethodachievedthelowestPEHE,thusindicat-
ingthatourweightcorrectionframeworkcansuccessfully
Performance metric: As with Hassanpour and Greiner
improvetheCATEestimationperformanceofDRCFR.
[2020],weevaluatedthequalityofthelearnedfeaturerep-
resentationsΓ(X),∆(X),andΥ(X),eachofwhichisformu- Performance under high-dimensional setup: We con-
latedasathree-layeredFNNencoder: firmed that our method also worked well with d =
(cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17) 600,1200,...,3000(SeeAppendixCforthedetail).
FNN(X)(cid:66)ν W3ν W2ν W1X ,
whereνisexponentiallinearunits(ELUs)[Clevertetal.,
5 RELATEDWORK
2016],andW1,W2,andW3aretheweightparametermatri-
cesinthefirst,thesecond,andthethirdlayer,respectively.
Data-driven feature separation for CATE estimation:
TodeterminewhetherthelearnedFNNencoderscorrectly
CATEestimationhasgainedincreasingattentionbecause
lookatimportantfeatures,wemeasuredtheattributionof
ofitsgreatimportanceforcausalmechanismunderstanding
featuresXΓ,X∆,andXΥbyemployingW1,i.e.,thetrained
[Chikaharaetal.,2022,Zhaoetal.,2022]andfordecision
weightmatrixinthefirstlayerofeachencoder.Forinstance,
supportinvariousfields,suchasprecisionmedicine[Gao
wequantifiedXΓ’sattributiononΓ(X),bytakingtwosteps.
etal.,2021]andonlineadvertising[Sunetal.,2015].There
First, we partitioned its learned weight parameter matrix
hasbeenasurgeofinterestinleveragingflexiblemachine
asW1 = [W1,W1 ],whereW1 isthesubmatrixwiththe
Γ −Γ Γ learningmodels,includingtree-basedmodels[Atheyetal.,
firstd/3columnsofW1andW1 istheonewiththeother
−Γ 2019, Hill, 2011], Gaussian processes [Alaa and van der
columns.ThenwemeasuredhowgreatlyfeaturesXΓaffects
Schaar,2018,HoriiandChikahara,2024],andneuralnet-
thelearnedrepresentationΓ(X)bytakingtherelativedif-
works [Hassanpour and Greiner, 2019, Johansson et al.,
ferencebetweentheaverageabsolutevaluesoftheweight
2016,Shalitetal.,2017].However,mostmethodstreatall
parametersubmatrices,i.e., |W1 Γ|−|W1 −Γ| .Weevaluatedother inputfeaturesXasconfounders.AspointedoutbyWuetal.
learnedrepresentations,∆(X)an|W d1 −ΓΥ|
(X),inthesameway.
[2022],theempiricalperformanceofsuchmethodsvariesa
lotwiththepresenceofadjustmentvariablesinX,whichis
Results:Figure3showsthemeansandstandarddeviations usualinpractice,especiallyinhigh-dimensionalsettings.
of learned parameter differences and test PEHEs over 20
Thisissuemotivatesustodevelopdata-drivenfeaturesepara-
randomlygeneratedsyntheticdatasets.
tionmethodsfortreatmenteffectestimation.Apioneerwork
WithourDPSWmethodandDRCFR,theabsoluteparam- isthedata-drivenvariabledecomposition(D2VD)[Kuang
eter values |W1|, |W1|, and |W1| were sufficiently larger etal.,2017,2020],whichminimizestheweightedprediction
Γ ∆ Γ
than |W1 |, |W1 |, and |W1 |, respectively, showing that lossplustheregularizerforfeatureseparation.Therecent
−Γ −∆ −Γ
bothmethodscorrectlylearnΓ(X),∆(X),andΥ(X)thatare methodaddressesamorecomplicatedsetup,wherefeatures
highlydependentoninstrumentalvariablesXΓ,confounders X includepost-treatmentvariables,whichareaffectedby
X∆,andadjustmentvariablesXΥ,respectively.Theseresults treatment A[Wangetal.,2023].However,theestimation
made a clear contrast to TARNet, which learns a single targetofthesemethodsisATE,notCATE.
representationwithoutperformingfeatureseparation.
By contrast, DRCFR deals with CATE estimation and is
ThesameistruefortheCATEestimationperformance(Fig- founded on weighted representation learning, which is a
ure3(d)).ThetestPEHEofTARNetwaslargerthanDPSW promisingapproachforaddressinghigh-dimensionaldata.
8This is why we adopted it as the inference engine of our References
weight correction framework. Integrating the recent idea
ofenforcingindependencebetweenfeaturerepresentations Ahmed M Alaa and Mihaela van der Schaar. Bayesian
with mutual information [Cheng et al., 2022, Chu et al., nonparametric causal inference: Information rates and
2022,Liuetal.,2024]isleftasourfuturework. learningalgorithms. IEEEJournalofSelectedTopicsin
SignalProcessing,12(5):1031–1046,2018.
Weightingschemesfortreatmenteffectestimation:IPW
[RosenbaumandRubin,1983]isacommonweightingtech-
SergeAssaad,ShuxiZeng,ChenyangTao,ShounakDatta,
niquefortreatmenteffectestimation.However,theweighted
Nikhil Mehta, Ricardo Henao, Fan Li, and Lawrence
estimatorbasedonIPWisoftennumericallyunstabledue
Carin. Counterfactualrepresentationlearningwithbal-
to the computation of the inverse of the propensity score.
ancingweights. InAISTATS,pages1972–1980,2021.
Oneremedyforthisissueisweighttruncation[Crumpetal.,
2009],which,however,causestheestimationbias,leading Susan Athey, Julie Tibshirani, and Stefan Wager. Gener-
toinaccuratetreatmenteffectestimation.
alizedrandomforests. AnnalsofStatistics,47(2):1148–
1178,2019.
Toimprovetheestimationperformance,Zhuetal.[2020]
haveemployedParetosmoothing[Vehtarietal.,2024].Al-
KeithBattocchi,EleanorDillon,MaggieHei,GregLewis,
thoughtheyempiricallyshowthatusingthistechniqueleads
Paul Oka, Miruna Oprescu, and Vasilis Syrgkanis.
tobetterperformancethanweighttruncation,theirmethod
EconML:APythonpackageforML-basedheterogeneous
isdevelopedfortheestimationofATE,notCATE. treatmenteffectsestimation. Version0.14.1,2019.
ApplyingParetosmoothinginweightedrepresentationlearn-
ing for CATE estimation is difficult because it prevents MathieuBlondel,OlivierTeboul,QuentinBerthet,andJosip
gradient backpropagation due to the non-differentiability. Djolonga. Fast differentiable sorting and ranking. In
This difficulty is disappointing, given that previous work ICML,pages950–959,2020.
hastheoreticallyshownthattheweightcorrectionschemes,
Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela,
suchasweighttruncation,helpstoextractpredictivefeature
Denis X Charles, D Max Chickering, Elon Portugaly,
representationsforCATEestimation[Assaadetal.,2021].
DipankarRay,PatriceSimard,andEdSnelson. Counter-
To establish a Pareto-smoothed weighting framework for factualreasoningandlearningsystems:Theexampleof
CATE estimation from high-dimensional data, we have computationaladvertising. JMLR,14(11),2013.
shownthathowdifferentiablerankingtechnique[Blondel
MAlanBrookhart,SebastianSchneeweiss,KennethJRoth-
etal.,2020]canbeusedtosimultaneouslylearnthepropen-
man,RobertJGlynn,JerryAvorn,andTilStürmer. Vari-
sityscoremodelandthefeaturerepresentations.
able selection for propensity score models. American
JournalofEpidemiology,163(12):1149–1156,2006.
MingyuanCheng,XinruLiao,QuanLiu,BinMa,JianXu,
andBoZheng. Learningdisentangledrepresentationsfor
counterfactual regression via mutual information mini-
6 CONCLUSION mization. InProceedingsofthe45thInternationalACM
SIGIRConferenceonResearchandDevelopmentinIn-
formationRetrieval,pages1802–1806,2022.
In this paper, we have established a differentiable Pareto-
smoothedweightingframeworkforCATEestimationfrom Victor Chernozhukov, Denis Chetverikov, Mert Demirer,
high-dimensionaldata.ToconstructaCATEestimatorthat Esther Duflo, Christian Hansen, Whitney Newey, and
isnumericallyrobusttopropensityscoreestimationerror, James Robins. Double/debiased machine learning for
wemaketheweightcorrectionprocedureinParetosmooth-
treatmentandstructuralparameters:Double/debiasedma-
ingdifferentiableandincorporateitintotheweightedrep- chinelearning. EconometricsJournal,21(1),2018.
resentation learning approach for CATE estimation. Ex-
YoichiChikahara,MakotoYamada,andHisashiKashima.
perimental results show that our framework successfully
Featureselectionfordiscoveringdistributionaltreatment
outperformstraditionalweightingschemes,aswellasthe
effectmodifiers. InUAI,pages400–410,2022.
existingCATEestimationmethods.
Leveragingtheversatilityofweighting,ourfutureworkwill JiebinChu,YaoyunZhang,FeiHuang,LuoSi,Songfang
investigatehowtoextendourframeworktoestimatetheef- Huang,andZhengxingHuang. Disentangledrepresenta-
fectsofsuchcomplextreatmentashigh-dimensionalbinary tionforsequentialtreatmenteffectestimation. Computer
treatment[Zouetal.,2020],continuous-valuedtreatment Methods and Programs in Biomedicine, 226:107175,
[Wangetal.,2022],timeseriestreatment[Limetal.,2018]. 2022.
9Djork-ArnéClevert,ThomasUnterthiner,andSeppHochre- KatherineKeith,DavidJensen,andBrendanO’Connor.Text
iter. Fastandaccuratedeepnetworklearningbyexponen- andcausalinference:Areviewofusingtexttoremove
tiallinearunits(ELUs). InICLR,2016. confoundingfromcausalestimates. InACL,pages5332–
5344,2020.
RichardKCrump,VJosephHotz,GuidoWImbens,and
Oscar A Mitnik. Dealing with limited overlap in esti- Diederik Kingma and Jimmy Ba. Adam: A method for
mationofaveragetreatmenteffects. Biometrika,96(1): stochasticoptimization. InICLR,2015.
187–199,2009.
KunKuang,PengCui,BoLi,MengJiang,ShiqiangYang,
AliciaCurthandMihaelavanderSchaar. Nonparametrices- and Fei Wang. Treatment effect estimation with data-
timationofheterogeneoustreatmenteffects:Fromtheory driven variable decomposition. In AAAI, volume 31,
tolearningalgorithms. InAISTATS,pages1810–1818, 2017.
2021.
KunKuang,PengCui,HaoZou,BoLi,JianrongTao,Fei
Alicia Curth, Richard W Peck, Eoin McKinney, James Wu,andShiqiangYang. Data-drivenvariabledecomposi-
Weatherall, and Mihaela van Der Schaar. Using ma- tionfortreatmenteffectestimation. IEEETransactions
chinelearningtoindividualizetreatmenteffectestimation: onKnowledgeandDataEngineering,34(5):2120–2134,
Challengesandopportunities. ClinicalPharmacology& 2020.
Therapeutics,2024.
RavinKumar,ColinCarroll,AriHartikainen,andOsvaldo
MarcoCuturi,OlivierTeboul,andJean-PhilippeVert. Dif- Martin. ArviZaunifiedlibraryforexploratoryanalysis
ferentiablerankingandsortingusingoptimaltransport. ofBayesianmodelsinPython. JournalofOpenSource
InNeurIPS,pages6861–6871,2019. Software,4(33):1143,2019.
ZijunGao,TrevorHastie,andRobertTibshirani. Assess- ShengLiandYunFu. Matchingonbalancednonlinearrep-
mentofheterogeneoustreatmenteffectestimationaccu- resentationsfortreatmenteffectsestimation. InNeurIPS,
racyviamatching. StatisticsinMedicine,40(17):3990– pages930–940,2017.
4013,2021.
BryanLim,AhmedAlaa,andMihaelavanderSchaar. Fore-
ArthurGretton,KarstenM.Borgwardt,MalteJRasch,Bern- casting treatment responses over time using recurrent
hard Schölkopf, and Alexander Smola. A kernel two- marginalstructuralnetworks. InNeurIPS,pages7494–
sampletest. JMLR,13(1):723–773,2012. 7504,2018.
Negar Hassanpour and Russell Greiner. Counterfactual Yu Liu, Jian Wang, and Bing Li. EDVAE: Disentangled
regressionwithimportancesamplingweights. InIJCAI, latentfactorsmodelsincounterfactualreasoningforindi-
pages5880–5887,2019. vidualtreatmenteffectsestimation. InformationSciences,
652:119578,2024.
Negar Hassanpour and Russell Greiner. Learning disen-
MarianFMacDormanandJonnaeOAtkinson. Infantmor-
tangledrepresentationsforcounterfactualregression. In
tality statistics from the linked birth/infant death data
ICLR,2020.
set–1995perioddata. 1998.
Jennifer L. Hill. Bayesian nonparametric modeling for
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,
causalinference. JournalofComputationalandGraphi-
JamesBradbury,GregoryChanan,TrevorKilleen,Zem-
calStatistics,20(1):217–240,2011.
ingLin,NataliaGimelshein,LucaAntiga,etal. PyTorch:
ShunsukeHoriiandYoichiChikahara. Uncertaintyquantifi- Animperativestyle,high-performancedeeplearningli-
cationinheterogeneoustreatmenteffectestimationwith
brary. InNeurIPS,volume32,2019.
gaussian-process-basedpartiallylinearmodel. InAAAI,
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort,
volume38,pages20420–20429,2024.
VincentMichel,BertrandThirion,OlivierGrisel,Mathieu
JonathanRMHoskingandJamesRWallis. Parameterand Blondel,PeterPrettenhofer,RonWeiss,VincentDubourg,
quantileestimationforthegeneralizedparetodistribution. etal. Scikit-learn:MachinelearninginPython. JMLR,
Technometrics,29(3):339–349,1987. 12(Oct):2825–2830,2011.
EdwardLIonides. Truncatedimportancesampling. Journal PaulR.RosenbaumandDonaldB.Rubin. Thecentralrole
of Computational and Graphical Statistics, 17(2):295– ofthepropensityscoreinobservationalstudiesforcausal
311,2008. effects. Biometrika,70(1):41–55,1983.
FredrikJohansson,UriShalit,andDavidSontag. Learning DonaldB.Rubin. Estimatingcausaleffectsoftreatments
representations for counterfactual inference. In ICML, inrandomized andnonrandomizedstudies. Journal of
pages3020–3029,2016. EducationalPsychology,66(5):688,1974.
10Brian C Sauer, M Alan Brookhart, Jason Roy, and Tyler FujinZhu,JieLu,AdiLin,andGuangquanZhang.APareto-
VanderWeele. Areviewofcovariateselectionfornon- smoothingmethodforcausalinferenceusinggeneralized
experimentalcomparativeeffectivenessresearch. Phar- paretodistribution. Neurocomputing,378:142–152,2020.
macoepidemiologyandDrugSafety,22(11):1139–1145,
Hao Zou, Peng Cui, Bo Li, Zheyan Shen, Jianxin Ma,
2013.
HongxiaYang,andYueHe. Counterfactualprediction
UriShalit. Canwelearnindividual-leveltreatmentpolicies forbundletreatment. InNeurIPS,pages19705–19715,
fromclinicaldata? Biostatistics,21(2):359–362,2020. 2020.
UriShalit,FredrikD.Johansson,andDavidSontag. Esti-
matingindividualtreatmenteffect:Generalizationbounds
andalgorithms. InICML,pages3076–3085,2017.
Yishai Shimoni, Chen Yanover, Ehud Karavani, and
Yaara Goldschmnidt. Benchmarking framework for
performance-evaluation of causal inference analysis.
arXivpreprintarXiv:1802.05046,2018.
Wei Sun, Pengyuan Wang, Dawei Yin, Jian Yang, and
Yi Chang. Causal inference via sparse additive mod-
elswithapplicationtoonlineadvertising. InAAAI,vol-
ume29,2015.
AkiVehtari,DanielSimpson,AndrewGelman,YulingYao,
andJonahGabry. Paretosmoothedimportancesampling.
JMLR,2024.
Haotian Wang, Kun Kuang, Haoang Chi, Longqi Yang,
Mingyang Geng, Wanrong Huang, and Wenjing Yang.
Treatmenteffectestimationwithadjustmentfeaturese-
lection. InKDD,pages2290–2301,2023.
Pengyuan Wang, Wei Sun, Dawei Yin, Jian Yang, and
YiChang. Robusttree-basedcausalinferenceforcom-
plexadeffectivenessanalysis. InWSDM,pages67–76,
2015.
Xin Wang, Shengfei Lyu, Xingyu Wu, Tianhao Wu, and
HuanhuanChen. Generalizationboundsforestimating
causaleffectsofcontinuoustreatments.InNeurIPS,pages
8605–8617,2022.
AnpengWu,JunkunYuan,KunKuang,BoLi,RunzeWu,
QiangZhu,YuetingZhuang,andFeiWu. Learningde-
composedrepresentationsfortreatmenteffectestimation.
IEEETransactionsonKnowledgeandDataEngineering,
35(5):4989–5001,2022.
JinsungYoon,JamesJordon,andMihaelaVanDerSchaar.
GANITE:Estimationofindividualizedtreatmenteffects
usinggenerativeadversarialnets. InICLR,2018.
JiamingZeng,MichaelFGensheimer,DanielLRubin,Su-
sanAthey,andRossDShachter.Uncoveringinterpretable
potentialconfoundersinelectronicmedicalrecords. Na-
tureCommunications,13(1):1014,2022.
QingyuanZhao,DylanS.Small,andAshkanErtefaie. Se-
lective inference for effect modification via the lasso.
Journal of Royal Statistical Society: Series B (Statisti-
calMethodology),84(2):382–413,2022.
11Supplementary Materials for
"Differentiable Pareto-Smoothed Weighting
for High-Dimensional Heterogeneous Treatment Effect Estimation"
YoichiChikahara1 KanseiUshiyama2*
1NTTCommunicationScienceLaboratories,Kyoto,Japan
2TheUniversityofTokyo,Tokyo,Japan
A RANKFUNCTIONDEFINITION
InSection3.3,weconsiderrankfunctionr,whichtakesinputvectorw=[w ,...,w ]⊤andoutputstherankofeachelement
1 n
inw.Weformallydefinethisrankfunctionbasedonthesortingoperationandtheconceptofinversepermutation.
Considerasortingoperationoverw ,...,w inw∈Rnthatfindsapermutationρ=[ρ ,...,ρ ]⊤suchthatthevectorvalues
1 n 1 n
permutedaccordingtoρ,w =[w ,...,w ]⊤,areincreasingasw ≤···≤w .Letρ−1betheinverseofpermutationρ,
ρ ρ1 ρn ρ1 ρn
i.e.,apermutationwhoseρ-thelementisifori=1,...,n.
i
Thentherankingfunctionisdefinedastheinverseofthesortingpermutation:
r(w)=ρ−1(w). (21)
Throughoutthispaper,weconsiderrankfunctionr(w)thatevaluatesthepositionofeachw basedonthesortinginan
i
ascendingorder.Ifweneedtoconsidertherankingindescendingorder,wecanformulateitasr(−w).
B EXPERIMENTALDETAILS
B.1 SETTINGSOFBASELINES
Regarding linear regression methods (LR-1 and LR-2), to avoid overfitting due to the large number of input features,
weemployedtheridgeregressionmodelinscikit-learn[Pedregosaetal.,2011].Forthemeta-learnermethods(i.e.,the
S-Learner(SL),theT-Learner(TL),theX-Learner(XL),theDR-Learner(DRL))andthetree-basedmethods(CFand
CFDML)weusedtheEconMLPythonpackage[Battocchietal.,2019].AsthebaselearnersforSL,TL,XL,andDRL,
wechoserandomforestbecauseweempiricallyobservedthatitachievedthebestperformanceamongthethreemodel
candidates,randomforest,gradientboosting,andsupportvectormachine.
Weevaluatedtheperformanceofneuralnetworkmethods(TARNetandGANITE),usingtheexistingimplementations
[CurthandvanderSchaar,2021,Yoonetal.,2018].12AsregardstheDRCFRmethod,weimplementeditusingPyTorch
[Paszkeetal.,2019].
WithPSW,wetrainedpropensityscoremodelπ(X)beforehand,usingapairedsample{(a,x)}⊂D.Hereweformulated
i i
π(X)usingthethree-layeredFNN,aswithourmethodandDRCFR.AfterhavingcomputedtheIPWweightsbasedonthe
trainedpropensityscoremodel,weperformedParetosmoothingoverthem,usingpsislw()functioninthePythonpackage
namedArviZ[Kumaretal.,2019].UsingthePareto-smoothedweights,welearnedthetwoencoders∆(X)andΥ(X),as
wellasoutcomepredictionmodelsh0andh1.
1https://github.com/AliciaCurth/CATENets
2https://github.com/jsyoon0823/GANITE
Acceptedforthe40thConferenceonUncertaintyinArtificialIntelligence (UAI2024).B.2 SYNTHETICDATA
FollowingHassanpourandGreiner[2020],wepreparedthesyntheticdatasets.3
WefirstdrawnthevaluesoffeaturesX ∈Rd fromstandardmultivariateGaussiandistribution:
X ∼N(0,I), (22)
whereN denotestheGaussiandistribution.
Then,wesampledthevaluesoftreatment AandoutcomeY usingXΓ ∈ Rd/3,X∆ ∈ Rd/3,andXΥ ∈ Rd/3,whicharethe
featuresubsetsinX =[XΓ,X∆,XΥ]⊤.Inparticular,weemployedψ=[XΓ,X∆]⊤ ∈R2d/3,togenerateA’svaluesas
(cid:32) (cid:33)
1
A∼Ber , (23)
1+exp(−c ·(ψ+1))
A
wherec ∈R2d/3isthecoefficientvectordrawnfromN(0,1),and1=[1,...,1]⊤isavectorwithlength2d/3.Bycontrast,
A
weusedϕ=[X∆,XΥ]⊤ ∈R2d/3tosimulatepotentialoutcomesY0andY1as
3
Y0 = c ·ϕ+ϵ (24)
2d
Y0
3
Y1 = c ·(ϕ⊙ϕ)+ϵ, (25)
2d
Y1
(26)
wherec ∈R2d/3andc ∈R2d/3arethecoefficientvectorsdrawnfromN(0,1),symbol⊙denoteselement-wiseproduct
Y0 Y1
(a.k.a.,Hadamardproduct),andϵ ∼N(0,1)isascalarstandardGaussiannoise.UsingthevaluesofpotentialoutcomesY0
andY1,wecomputedthevaluesofoutcomeY byY = AY1+(1−A)Y0.
C ADDITIONALEXPERIMENTALRESULTS
In Section 4.2, as with Hassanpour and Greiner [2020], we present the performance on the relatively low-dimensional
syntheticdata.Tofurtherinvestigatetheperformanceofourmethod,weperformedadditionalexperimentsusinghigh-
dimensionalsyntheticdata.
Here,wegeneratedthesyntheticdatainthesameway(asdescribedinAppendixB.2)exceptthatthenumberoffeatured
wassettod =600,1200,...,3000.
Figure4showstheresults.Again,ourmethodachievedbetterCATEestimationperformancethanDRCFRandTARNet,
thusshowingthatitcansuccessfullyimprovetheestimationperformanceofDRCFRbyeffectivelycorrectingtheIPW
weightsviadifferentiableParetosmoothing.
Regardingthefeaturerepresentationlearningperformance(Figure4(a)-(c)),bothDRCFRandourmethodworkedbetter
thanTARNet.AsillustratedinFigure4(a),however,withDRCFRandourmethod,thedifferenceamongtheabsolute
parametervaluesinlearnedencoderΓ(X)decreased,asthenumberoffeaturesdincreased,indicatingthatdistinguishing
therepresentationofinstrumentalvariablesΓ(X)fromtheoneofconfounders∆(X)wasdifficultforbothmethods.This
difficultyispartlybecausebinarytreatmentAcanbeaccuratelypredictedsolelyfromtherepresentationofconfounders(i.e.,
∆(X))whenthereareasufficientnumberofconfoundersX∆.
Apossiblesolutionistoenforcetheindependencebetweenfeaturerepresentationsusinganadditionalmutualinformation
regularizer,aswithChengetal.[2022].OurfutureworkconstitutestheevaluationofsuchavariantofourmethodforCATE
estimationperformanceunderhigh-dimensionalsetups.
3Weusedtheirimplementationinhttps://www.dropbox.com/sh/vrux2exqwc9uh7k/AAAR4tlJLScPlkmPruvbrTJQa?dl=0.
13(a) Relative difference between |W1| and |W1 | (b) Relative difference between |W1| and |W1 |
𝚪 −𝚪 ∆ −∆
Number of features d Number of features d
(c)Relative difference between |W1| and |W1 | (d) Test PEHE
𝚼 −𝚼
Number of features d Number of features d
Figure4:LearnedencoderparameterdifferencesandtestPEHEsonsyntheticdata.(a):ValuedifferenceofW1inencoder
Γ(X);(b):ValuedifferenceofW1 inencoder∆(X);(c):ValuedifferenceofW1 inencoderΥ(X);(d)TestPEHEs.With
TARNet,sinceitlearnssingleencoder,wecomputedallparametervaluedifferenceswithweightmatrixinsameencoder.
14