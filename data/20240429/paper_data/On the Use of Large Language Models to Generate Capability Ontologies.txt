On the Use of Large Language Models
to Generate Capability Ontologies
Luis Miguel Vieira da Silva∗, Aljosha Ko¨cher∗, Felix Gehlhoff∗, Alexander Fay†
∗Institute of Automation
Helmut Schmidt University, Hamburg, Germany
Email: {miguel.vieira, aljosha.koecher, felix.gehlhoff}@hsu-hh.de
† Chair of Automation
Ruhr University, Bochum, Germany
Email: alexander.fay@rub.de
Abstract—Capabilityontologiesareincreasinglyusedtomodel abletoautomateavarietyofapplicationsrangingfromnatural
functionalitiesofsystemsormachines.Thecreationofsuchonto- language interactions, such as providing concise summaries
logical models with all properties and constraints of capabilities
and translation tasks, to solving complex problems like code-
is very complex and can only be done by ontology experts.
generation from natural language descriptions. With these
However, Large Language Models (LLMs) have shown that
they can generate machine-interpretable models from natural prerequisites, using LLMs to generate capability ontologies
languagetextinputandthussupportengineers/ontologyexperts. seems promising. Accordingly, in this article, we present an
Therefore, this paper investigates how LLMs can be used to investigation on how capability ontologies can effectively be
create capability ontologies. We present a study with a series
generatedusingLLMsinordertomitigatetheeffortofmanual
of experiments in which capabilities with varying complexities
or semi-automated capability modeling. More specifically, we are generated using different prompting techniques and with
differentLLMs.Errorsinthegeneratedontologiesarerecorded aim to answer the following questions:
andcompared.Toanalyzethequalityofthegeneratedontologies, RQ 1: How do different LLMs and prompting techniques
asemi-automatedapproachbasedonRDFsyntaxchecking,OWL
influence the quality of the generated ontologies?
reasoning, and SHACL constraints is used. The results of this
RQ 2: Is there a way to test the generated ontologies in a
study are very promising because even for complex capabilities,
the generated ontologies are almost free of errors. reliable and, ideally, automated way?
Index Terms—Large Language Models, LLMs, Capabilities, Toanswerthesequestion,astudyispresentedwhichexamines
Skills, Ontologies, Semantic Web, Model-Generation
two LLMs and different prompting techniques to generate
I. INTRODUCTION capability ontologies of increasing complexity. With a series
of experiments, we seek to understand the potential of LLMs
Machine-interpretable descriptions of functionalities are in-
in bridging the gap in the automated generation of machine-
dispensable for flexible systems as they facilitate algorithms
interpretable descriptions for capabilities.
for automated planning and adaptation. In recent years, the
In the following section, we first introduce relevant foun-
concepts of “capabilities” and “skills” have emerged as key
dations and then analyze existing contributions to capability
elements in structuring these functionalities and their de-
engineering methods and studies covering the use of LLMs
scriptions. Capabilities are defined as an implementation-
to generate machine-interpretable models. Afterwards in Sec-
independent specification of a function, whereas skills rep-
tion III, we present a concise overview of our ontological
resent executable implementations of a function specified by
capability model, which represents the metamodel of the
a capability [1]. The terminology and information models
ontologies generated in this study. In Section IV, the study
have been standardized in recent years by working groups
design is explained, before Section V discusses the results.
of Plattform Industrie 4.01 and IDTA2. Unfortunately, the
ThepapercloseswithaconclusionandoutlookinSectionVI.
creation of model instances, particularly for capability and
skill ontologies, remains a challenging and labor-intensive
II. STATEOFTHEART
process, demanding a high level of expertise. While there are
A. Background
approaches to generate parts of these ontologies from existing
information (e.g., [2]), the modeling of capabilities is still a Ontologies defined in Web Ontology Language (OWL)
time-consuming task. offeranumberofadvantagesforcapturingknowledge,sothey
In light of these challenges, recent advancements in Large are often used to define machine-interpretable models of ca-
LanguageModels(LLMs)offerpromisingavenues.LLMsare pabilities and skills. Ontologies are highly formal information
models that include a set of concepts with a specification of
1https://www.plattform-i40.de/IP/Redaktion/EN/Downloads/Publikation/
their meaning along with definitions of how concepts are re-
CapabilitiesSkillsServices.html
lated [3]. Ontologies can be divided into a terminological box
2https://github.com/admin-shell-io/submodel-templates/tree/main/
development/Capability/1/0 (TBox) and an assertional box (ABox). While a TBox entails
4202
rpA
62
]IA.sc[
1v42571.4042:viXraclass knowledge about a problem domain, an ABox contains artifacts, thus reducing the high effort required for ontology
factual knowledge of one specific problem [4]. An important creation. Using a provided framework, the skill aspect is
reason for using ontologies is reasoning, which automatically created automatically using source code of the skill behavior.
infersnewknowledgefrommodeledfacts,e.g.,fordiscovering For the capability aspect, a semi-automated approach using
contradictions. If contradictions (e.g. an instance assigned to graphicalmodelingisusedastherearenoengineeringartifacts
two disjoint classes) are detected by reasoning, the ontology describingfunctionsinastructuredmanner.Thus,thereisstill
is inconsistent. To ensure that certain information is or is not manual effort for creating the graphical model and some parts
presentinanontology,ShapesConstraintLanguage(SHACL) of the capability aspect are not covered (e.g., constraints).
can be used to formulate constraints that must be fulfilled [5]. TheChowlkframeworkpresentedin[11]offersthepossibil-
With SHACL, constraints are described as so-called shapes itytovisuallymodelaTBoxbasedonUnifiedModelingLan-
to express both that information must be available and that guage (UML) instead of using cumbersome ontology editors.
onlyspecifiedrelationsarepermittedbyusingso-calledclosed With Chowlk, a created diagram is automatically transformed
shapes. Especially when using LLMs to generate a capability into an OWL ontology. This framework is primarily intended
ontology, this additional validation is necessary to detect toreducetheeffortrequiredtodevelopaTBoxofanontology
possible hallucinations or missing information. foraspecificdomain,butalsoallowsthecreationofanABox.
The manual effort required to create an ABox remains high
Large Language Models (LLMs) are advanced compu-
and a user still needs to be an ontology expert to be able to
tational models that are commonly based on a transformer
use the framework and understand which elements to select.
architecture to generate texts, or more precisely, to predict
CapabilitiesarealsomodeledwiththeAssetAdministration
word sequences based on a given input. LLMs are trained
Shell (AAS) using an existing submodel template. In the
on large datasets of text data from books, websites and
contextoftheAAS,therearealsoinitialapproachestosupport
other media across multiple domains to identify and learn
modeling,suchasin[12],whichprovideagraphicalmodeling
patterns between words, making them suitable for a variety
framework for modeling systems in accordance with the AAS
of tasks [6]. To interact with LLMs, prompt engineering can
standard. The authors of [13] present an approach that uses
be used to optimize prompts to efficiently guide LLMs in
LLMs to generate instances of AAS from textual technical
performing complex tasks. Effectiveprompts can significantly
data.OneoftheconsideredLLMsisGPT-3.5withafew-shot
improve the quality and relevance of LLM responses. As a
prompting technique. The results are promising, showing an
result,avarietyofpromptingtechniqueshavebeendeveloped,
effective generation rate of 62–79% and thus a reduction in
such as zero-shot, one-shot, or few-shot prompting, which
the effort required to create AAS instances. Manual effort is
provide a corresponding number of examples as context in
only required to verify the results, for which no method is
a prompt. These examples provide demonstrations to help an
provided. Furthermore, ontologies are not considered by [13].
LLMingeneratingmorecoherentresponsesthroughaprocess
In [14], it was investigated to what extent LLMs in the
called in-context learning, where LLMs use a given context
form of ChatGPT and REBEL can be combined with seman-
to make predictions [7]. Besides using different prompting
tic technologies to enable reasoning. REBEL and ChatGPT
techniques, LLMs typically allow to configure certain param-
are used to extract the relations from unstructured text and
eters to influence the output. One such parameter is the so-
write them into a TBox. In addition, another experiment
called temperature, which controls the randomness of LLM
is performed with ChatGPT to create the entire ontology
results. A lower temperature makes the responses of an LLM
consisting of TBox and ABox directly with a single prompt.
more deterministic and repetitive, while a higher temperature
To obtain deterministic results, the temperature parameter is
encourages variety and creativity in the responses [8]. Even
setto0.Unfortunately,neitherthepromptsusednortheresults
at a low temperature, LLMs can generate information that is
of this study are publicly available. Furthermore, different
factuallyincorrect,invented,orirrelevanttothegiveninput—
prompting techniques are not investigated and due to the
knownashallucination[6].Hallucinationneedstobehandled
token limitation of ChatGPT only simple experiments are
in order not to store false information in an ontology.
conducted.Theevaluationwasonlycarriedoutmanuallyusing
B. Related Work different criteria (e.g., contradictions or redundant elements).
In [9] a systematic development process for an ontology In contrast, the aim of our paper is to test multiple prompting
to describe capabilities of manufacturing resources is pro- techniques for complex models and create a semi-automated
posed. This approach is based on the ontology engineering method of evaluating the generated ontologies.
methodology of [10] and consists of five phases: feasibility The authors of [8] present an approach to automatically
study, kickoff, refinement, evaluation as well as application create a TBox of an ontology to reduce the manual effort
and evolution. The focus of this method is on modeling the that requires domain experts by using GPT-3.5 Turbo. In this
TBox. An ABox is modeled in the evaluation phase, but no approach, only the subclass relation is considered. To verify
methodologicalapproachorautomatedprocessispresentedto the results, additional prompts are submitted to the LLM and
support this activity. a textual description of the generated ontology is created
Reference [2] presents a method for creating the various for manual verification. The results are considered promising,
aspects of a capability ontology from existing engineering despite some cases of hallucination and incompleteness. Dif-ferent LLMs or prompting techniques are not compared and ontology representing a transport capability is shown. This
only a very simple ontology is created using a manual and capability moves a product (Input_Product) from its
subjective method to verify the results. current position (CurrentProductPosition) to a target
In [15], a variety of experiments is conducted with Chat- position (TargetPosition). This capability is one of the
GPT to support ontology engineering in order to overcome capabilities used in our study (see Section IV).
challenges such as expert dependency and time consumption. Forpropertiesofinputs,requirementsofpermissiblevalues
One of the experiments is the creation of SPARQL queries: A canbeexpressed.Theserequirementscaneitherbeconstantor
SPARQLqueryiscreatedfromnaturallanguageforacustom, dependentonothermodelelements.Forexample,thetransport
smallontologybecauseofthetokenlimit.Inaddition,thecre- capability could only lift products up to a constant height. An
ationofontologiesfromproductfactsheetsusingChatGPTis example for a dependent requirement is given in Figure 1: the
investigated.Whiletheinformationextractionisreportedtobe current position of the input product is required to be equal
successful, modeling sometimes is incorrect. Simple prompts to the position of the transport resource (AGVPosition),
areusedthroughoutandnodifferentpromptingtechniquesare as otherwise the product cannot be picked up. In addition
compared to obtain a better solution. There is also a lack of a to requirements, unbound parameters can also be modeled.
method to integrate the LLM into a workflow and verify the In Figure 1, for example, TargetPosition is an unbound
results. parameter without expression goal.
In[16]theLLM4OLapproach,whichusesLLMsforontol- The properties of outputs are typically modeled as assur-
ogy learning, is presented. The aim is to automatically extract ances. Here, too, constant values as well as values depending
and structure knowledge from natural language text. A zero- onotherelementscanberepresented.InFigure1,thetransport
shot prompting technique is used for this purpose. Different capability guarantees that the assured position after transport
LLMs, such as GPT-3.5 and GPT-4, were tested for different is equal to the desired position to be selected (i.e., the input
ontologies from different domains. The ontology creation is parameterTargetPosition).Figure1onlyshowsequality
dividedintothreeparts:termtyping,creatingtypetaxonomies, relations for simplicity purposes. In our model, arbitrary
and relation extraction for further relations between types. mathematical relations can be expressed using the OpenMath
The authors of [16] conclude that zero-shot prompting is not ontology presented in [20].
sufficient for a fully automated ontology generation approach. Figure 1 shows that representing even a simplified example
Overall, LLMs are considered in [16] to be suitable assistants with an ontology can quickly become complicated and exten-
for creating ontologies as they significantly reduce the high sive. Therefore, methods for simplified ontology creation are
effort involved. required.
III. CAPABILITYONTOLOGY
IV. STUDYDESIGN
In order to evaluate the use of LLMs to create a capability
In [17], we present an initial version of an OWL ontology
ontology in a structured manner, the following methodology
called CaSk, which is based on industry standards and can be
is applied. With GPT-4 Turbo4, in the following GPT, and
used to describe machines, their capabilities, and executable
Claude 35, in the following Claude, we used two different
skills. CaSk is an extension of the reference model in [1] and
LLMs, each with three prompting techniques, to generate
available online3. In this contribution, we focus only on the
sevendifferentcapabilitieswithdifferentlevelsofcomplexity.
capability aspect and an overview of this aspect is presented
We had originally planned to use Gemini Pro6, but its token
in the following.
limit is too small to capture the ontology provided as context.
Capabilities are required by processes, which are modeled
These three LLMs are among the most popular LLMs and
according to [18]. A process consists of process operators
perform best in benchmarks [21]. An overview of the main
that can have products, information or energy as inputs and
features of the considered LLMs is shown in Tab I. However,
outputs. These inputs and outputs are further characterized
the number of tokens given for the context size of the LLMs
by data elements to describe properties according to [19]. A
is not directly comparable, as these LLMs calculate tokens
data element consists of a type description and one or more
differently. In this paper, an LLM is intended to provide
instance descriptions. While a type description contains type-
reliable solutions for the creation of capability ontologies, so
related information about a data element (e.g., ID, name, unit
that the parameter temperature is set to 0 in order to obtain
ofmeasure),aninstancedescriptioncapturesonedistinctvalue
deterministic solutions.
expression of that data element. Instance descriptions can be
The three prompting techniques used are zero-shot, one-
further subdivided by their expression goal into requirements,
shot and few-shot prompting, for which templates are created.
assurances, and actual (i.e., measured) values, as well as
In Listing 1 the one-shot prompt template is shown. The
unbound parameters.
three templates have the same structure and differ only in
Following this approach, processes and thus capabilities
the included examples. Every template starts with a general
can be modeled with their possible inputs and outputs in
a detailed way. In Figure 1, a simplified excerpt of an 4https://openai.com/gpt-4
5https://claude.ai
3https://github.com/caskade-automation/cask 6https://gemini.google.comInstanceDescription InstanceDescription
ProductPositionBefore AGVPosition
InstanceDescription
Exp.Goal: "Requirement" Exp.Goal: "ActualValue"
CurrentProductPosition
equals
Exp.Goal: "ActualValue"
Resource hasInstance
hasInstance AGV
hasInstance Product
hasDataElement
Input_Product
hasData DataElement
Element AGVPosition_DE
DataElement providesCapability
DataElement
IP_Position_DE hasInput Product
OP_Position_DE
Output_Product
DataElement Information
hasData
II_Position_DE Input_Information hasOutput
hasInput Element
hasDataElement Capability
Transport InstanceDescription
InstanceDescription ProductPositionAfter
TargetPosition Exp.Goal: "Assurance"
hasInstance equals hasInstance
TypeDescription
hasType Position_Type hasType hasType
hasType
Fig.1. SimplifiedrepresentationofatransportcapabilityanditspropertiesmodeledwiththeCaSkontology.
TABLEI
OVERVIEWOFTHELLMSCONSIDEREDINTHISSTUDY Listing1. One-shotprompttemplate
--- Instruction ---
Translate a natural language input describing a
Name Version TrainingData ContextSize capability into an OWL ontology ...
[Tokens]
GPT-4 gpt-4-turbo-2024-04-09 December2023 128.000 --- Context ---
Claude3 claude-3-opus-20240229 August2023 200.000 ${CaSk ontology}
Gemini gemini-1.0-pro February2024 30.720
--- Input Data ---
${Coffeemaking Task Description}
instruction to create a capability. The instruction is short --- Output ---
OWL-Ontology:
and concise and describes that the LLM should transform
${Coffeemaking Result}
a natural language description of a capability into an OWL
ontology, taking the following context into account. Each --- Input Data ---
${Task Description}
templatetakestheTBoxoftheCaSkontology(seeSectionIII)
inTurtlesyntaxascontext.Inaddition,eachtemplatecontains --- Output ---
a task description containing a natural language text for the OWL-Ontology:
specificcapability(placeholder${Task Description}in
Listing1)aswellasaplaceholderfortheoutputtocreate.The
one-shottemplateadditionallycontainsasimplecoffeemaking if a is even. There are no other inputs or outputs and no
capability as an example with task description and ontology constraints. Mixing on the other hand is a more complicated
as the solution. The few-shot prompts of the study contain capability. It has three input products (liquid 1 - 3), all related
three examples. In addition to the coffee-making example, toadataelementdefiningthevolumefractionofeachliquidin
a distillation capability and a simple mathematical operation themix.Inaddition,thetotalvolumetoproducev canalso
total
with multiplication are included in few-shot prompts. All be passed as an input to the capability. Mixing has one output
templates and prompts used are available online7. product and two constraints that ensure correct behavior. The
The capabilities used in this study range from a simple sumofthethreeinputvolumefractionsneedstoequate1.And
functionalitytochecktheparityofagivennumbertoamixing the total volume must not surpass 20. A list of all capabilities
operationfromprocessmanufacturing.Thesimplestcapability (C1-C7) and examples (E1-E3) with their inputs, outputs and
parity receives a single integer a as an input and returns true constraints is given in Table II.
For each of the capabilities given in Table II, a natural
7https://github.com/CaSkade-Automation/llm-capability-generation language task description was defined. Then, with the threeTABLEII
OVERVIEWOFTHECAPABILITIESUSEDINTHISSTUDY
Name Description Inputs Outputs Constraints
E1:Coffeemaking Brewacoffeecofspe- w:product,b:product c:product,grounds:product tempin>=0,tempin<=50
cific type t from water withtempw withtempout andtout tout=tin
w andbeansb tin:information,string tempout>=90
E2:Multiplication Multiplytwonumbers a:information,integer product:information,real -
b:information,integer
E3:Distillation Distillamixmindis- m:product d:product boil liq1̸=boil liq2
tillatedandresiduer withboil liq1,mass
liq1
withboil
d
andmass
d
boil d<=boilr
andboil liq2,mass
liq2
r:product mass liq1+mass liq2=
withboilr andmassr mass d+massr
C1:Parity Checkwhetheragiven a:information,integer isEven:information,boolean -
numberisevenorodd
C2:Addition Add two numbers and a:information,integer sum:information,integer -
returnstheirsum b:information,integer
C3:Division Divide a number a by a:information,integer quotient:information,real b̸=0
anumberb b:information,integer
C4:Drilling Drill a hole with a pin:product pout:product diamin<=20,depthin<=80
given depth and diam- diamin:information,real withdiamout anddepthout diamout=diamin
eter depthin:information,real depthout=depthin
C5:Transport aTr ga in vs ep no prt osa itip or nod wu ic tht at no p wi in th: ppr oo sd p iu nct,a ag nv d: pr oe sso au gr vce wpo iu tht: pp or so od uu tct p po os sp i on ut= =p po os sa ig nv
AGV posin:information,real
C6:Assembly Assembletwoproducts ain:product,bin:product pout:product weightout=
intoone withweighta
in
andweightb
in
withweightout weighta in+weightb
in
C7:Mixing Mix three liquids with liq1,liq2,liq3:product pout:product vf1+vf2+vf3=1
givenvolumefractions vf1,vf2,vf3:information,real v total<=20
v total:information,real
prompt templates described above for each of the 7 capabil- ontology. First of all, it is important to keep the number of
ities, all 21 individual prompts were generated by filling the hallucinated model elements as low as possible. We use the
placeholdersineachprompttemplatewiththeCaSkontology, term hallucinated model elements to refer to elements that
the examples as well as the individual task descriptions for cannotbetracedbacktothenaturallanguagetaskdescription.
each capability using a Python script. The 21 prompts were For example, the CaSk ontology contains terms that are not
thenenteredintoGPTandClaudeandtheresultingontologies used in any textual description, e.g., skill or skill interface.
were stored. For each capability, the expected result was These model elements should not be created by the LLMs.
manually modeled. To find hallucinated elements, constraints were defined using
One major challenge when using LLMs to automatically SHACL. Using closed SHACL shapes, one can restrict prop-
generatemachine-interpretableontologiesischeckingthegen- erties that are to be applied to individuals of a certain class.
eratedontologiesforcorrectnessandcompleteness.Toachieve Additional properties are reported as errors. All hallucinated
this, we follow a multi-step approach with automated and element scores are shown in column H in Tables III and
manual checks. Every generated ontology is first checked IV.Inadditiontocheckingforhallucinatedelements,SHACL
for syntactic errors by opening the ontology in the Protege8 constraints are also used to check for missing model content.
ontology editor, which throws warnings for syntax errors. Forexample,thecapabilitiesshowninTableIIallhaveatleast
Each syntax error is fixed and recorded (see column S in one input and output property. The existence of at least one
TablesIIIandIV).Afterthat,thesyntax-correctedversioncan input and output can be checked with a SHACL constraint.
beopenedinProtegeandtheOWLreasonerPellet9 isusedto However, the six SHACL shapes we defined are only a first
check for inconsistencies. An inconsistency indicates that an steptocheckingthecompletenessofeachindividualcapability
LLMdidnotfullyunderstandtherulesgoverningapplicability as there are many capability-specific elements to look out for.
of a certain class or property. Every incorrectly modeled Thus, every generated capability was also manually checked.
element leading to an inconsistency is counted (see column The incompleteness score of each prompt result can be seen
C in Tables III and IV). After analyzing inconsistencies, two in column I of Tables III and IV.
checks are carried out to verify completeness of the generated
V. RESULTS
8https://protege.stanford.edu/ AswritteninSectionIV,eachcapabilitywasgeneratedwith
9https://github.com/stardog-union/pellet all three prompting techniques and by both GPT and Claude.TABLEIII
RESULTSOBTAINEDWITHGPTBYGENERATINGEVERYCAPABILITYWITHTHREEPROMPTINGTECHNIQUES.
S:SyntaxErrors C:Contradictions(Inconsistencies) H:Hallucinations I:Incompleteness
#Triples zero-shot one-shot few-shot
S C H I (cid:80) S C H I (cid:80) S C H I (cid:80)
C1:Odd/Even 33 0 0.06 0.15 0.21 0.42 0 0 0.03 0.03 0.06 0 0 0 0 0
C2:Addition 42 0 0.21 0.19 0.38 0.79 0 0 0.07 0 0.07 0 0 0.02 0 0.02
C3:Division 52 0 0.04 0.10 0.40 0.54 0 0 0.08 0.21 0.29 0.02 0 0.04 0.10 0.15
C4:Drilling 95 0.01 0.03 0.01 0.38 0.43 0 0 0.02 0.16 0.18 0.01 0.02 0 0.14 0.17
C5:Transport 83 0 0.04 0.05 0.29 0.37 0 0 0 0.06 0.06 0.01 0 0.01 0.04 0.06
C6:Assembly 82 0 0.05 0.06 0.30 0.41 0 0 0 0.07 0.07 0 0 0 0 0
C7:Mixing 120 0 0.02 0.04 0.55 0.61 0 0 0 0.25 0.25 0.01 0.03 0.03 0.34 0.04
Meanerrorscore 0.51 0.14 0.12
TABLEIV
RESULTSOBTAINEDWITHCLAUDEBYGENERATINGEVERYCAPABILITYWITHTHREEPROMPTINGTECHNIQUES.
S:SyntaxErrors C:Contradictions(Inconsistencies) H:Hallucinations I:Incompleteness
#Triples zero-shot one-shot few-shot
S C H I (cid:80) S C H I (cid:80) S C H I (cid:80)
C1:Odd/Even 33 0 0 0.15 0.24 0.39 0 0 0 0 0 0 0 0 0 0
C2:Addition 42 0 0 0.19 0.33 0.52 0 0 0.26 0 0.26 0 0 0 0 0
C3:Division 52 0 0 0.04 0.52 0.56 0 0 0 0 0 0 0 0 0 0
C4:Drilling 95 0 0.05 0.07 0.53 0.65 0 0 0 0 0 0 0 0 0.06 0.06
C5:Transport 83 0 0 0 0.29 0.29 0 0.02 0.02 0.01 0.06 0 0.02 0.02 0.02 0.07
C6:Assembly 82 0 0 0.04 0.21 0.24 0 0 0.01 0 0.01 0 0 0 0 0
C7:Mixing 120 0 0 0.07 0.68 0.74 0 0 0 0.07 0.07 0 0 0 0.10 0.1
Meanerrorscore 0.49 0.06 0.03
Errors regarding syntax (S), contradictions (C), hallucinations evidentinthefew-shotprompts.However,itisalsonoticeable
(H) and incompleteness (I) were recorded. In order to relate inthezero-shotpromptsthatClaudecausessignificantlyfewer
these errors to the size of each capability ontology, a relative contradictions (C) than GPT. This suggests that Claude better
error measure is calculated by dividing the absolute errors by understands constraints such as domain and range or disjoint
the number of triples to be modeled. The results are shown classes, which are modeled in the CaSk ontology given as
in Table III (GPT) and Table IV (Claude). A more reduced context. GPT seems to pick up these constraints only with the
illustration of the results is given in Figure V. In this figure, examples(seeimprovingC forone-shotandfew-shotprompts
therelativecompleteness1−I iscomparedforeachgenerated in Table III).
capability. One triple that is missing in all zero-shot results is the
One of the first findings is that both LLMs make very owl:imports statement for importing the CaSk ontology.
few syntax errors, regardless of the complexity of the gen- NeitherGPTnorClaudeimportedtheCaSkontologyjustfrom
erated capability and even with simple zero-shot prompts. the task description and CaSk given as context. In order to be
The syntax errors found in GPT outputs are all due to able to test for inconsistencies, import statements were manu-
missing prefix declarations. Claude did not make a single allyadded.Withtheadditionofexamplesintheprompts(i.e.,
syntax error in the generated ontologies. However, with both using one-shot and few-shot prompts), the import statements
LLMs, additional text output occurred a few times behind the were reliably included.
actual ontology, regardless of the prompting technique. This Simple elements of the ontology (e.g., definition of ca-
extra output was often an interpretation or explanation of the pabilities, link to their inputs and outputs) can be handled
generatedontology.Itwasparticularlyinterestingthatinsome well by both LLMs even in the zero-shot case. However,
cases, Claude created additional task descriptions in natural the more complicated capabilities (C4 and higher) feature an
language together with corresponding solutions in its output. increasing amount of properties and also constraints, which
Weattributethistofactthatthebaseinstructionmightnothave arerathercomplicatedtomodel.Inordertocorrectlyrepresent
been formulated precise enough. In all cases with additional constraintsusingtheOpenMathontology,modelingguidelines
text output, we did not assign any syntax errors if the actual must be adhered to. These modeling guidelines cannot be
ontologies were syntactically correct. learned from the CaSk ontology alone without examples,
There is a clear trend towards improving results with bet- which is why both GPT and Claude have rather high values
ter prompting technique — an unsurprising finding. Overall, for the incompleteness measure I for C4 and higher. The
ClaudeperformsslightlybetterthanGPT,whichisparticularly improvement from zero-shot to one-shot prompts is clearlyClaude). A possible reason might be the compilation of
1 0.97 1.00 1.00 1.00 0.79 0.76 examples.Furtherstudiesareneededtocomparemultiplefew-
shotpromptsusingdifferentexamplesinvaryingcompilations.
0.5
The results include some particularly noteworthy individual
cases, which are worth discussing in a bit more detail. Claude
0
generatedtheadditioncapabilitywiththezero-shotpromptnot
1 1.00 1.00 1.00 1.00 as a general capability, but instead defined specific values and
0.62 0.67 solved the addition 42+23=65. However, this did not occur
0.5 in the division capability, even though the natural language
descriptions are very similar. Claude also sometimes uses a
0 more compact syntax with blank nodes (e.g., in the zero-shot
result for the transport capability) — something not included
1.00 1.00 1
0.79 intheexamples.Thisindicatesahighlevelofunderstandingof
0.60 0.60 Turtle syntax. Furthermore, Claude sometimes models simple
0.48
0.5
constraints, which we typically model as value expressions
(e.g., ̸= 0), as more complex OpenMath constraints (e.g, in
0
the one-shot result for the division capability). This more
1 0.84 1.00 0.86 0.94 complex representation for simple relations is not included in
the examples, but the expressions generated by Claude are all
0.62
0.5 0.47 valid. We also observe this different approach to expressing
simple relations for GPT (e.g., in the few-shot result for the
0 division capability). Even though we specifically asked to
only use the capability aspect of the CaSk ontology, GPT
1 0.94 0.99 0.96 0.98
created many hallucinated skill elements in the zero-shot
0.71 0.71
results. With Claude, that was not the case at all. None of
0.5
the experiments produced a complete result of the transport
capability. This can be attributed to the fact that there is
0
no example capability for the one-shot and few-shot prompts
1 0.93 1.00 1.00 1.00 that contains a resource as a capability provider. Accordingly, 0.79
0.70 this relation is missing in all experiments. All parts of this
0.5 study from the prompts used to the capabilities generated
are available at https://github.com/CaSkade-Automation/llm-
0 capability-generation.
1 0.93 0.90 Overall, we are impressed with the results. The big advan-
0.75 0.66 tageofhavingLLMsgenerateacapabilityontologyfromnat-
0.5 0.45 ural language descriptions is that — in contrast to approaches
0.33
like [2] — no models need to be manually created at all.
0 The zero-shot results are already a great simplification to our
zero-shot one-shot few-shot previous approach of capability modeling, although quite a
few corrections to the generated ontologies are still necessary.
GPT Claude
However, the few-shot results are of such high quality that
incomplete ones could be checked and corrected quickly.
Fig. 2. Completeness of the generated ontologies. A value of 1 means that Both the entire CaSk ontology and the example ontologies
alltriplestobecreatedhavebeengeneratedcorrectly.
(for one- and few-shot prompts) are passed in the prompts,
resulting in rather high amounts of tokens: While the sim-
plest prompt (one-shot prompt for the parity capability) uses
visible. This is reflected in the difference of the mean error
22,73010 input tokens, the most complex one uses 28,56111
scores (-0.37 for GPT and -0.43 for Claude) and is also
(few-shot prompt for the mixing capability). This results in
noticeable when correcting the generated ontologies. While
averagecostsperpromptof0.31USDforGPTand0.65USD
many manual corrections and additions are necessary for the
for Claude.
zero-shot results, the one-shot results (especially for Claude)
are practically error-free. For the transition from one-shot to VI. CONCLUSION
few-shotprompts,thedeclineoferrormeasurescoresismuch
In this contribution, we presented a study to examine the
smaller. And some one-shot prompts even performed better
suitability of LLMs for generating capability ontologies from
than their few-shot counterparts (see the mixing capability
generated by GPT or the drilling capability generated by 11CalculatedwithOpenAI’stokenizer.TokencountvariesforotherLLMs
ssenetelpmoC
ssenetelpmoC
ssenetelpmoC
ssenetelpmoC
ssenetelpmoC
ssenetelpmoC
ssenetelpmoC
)nevE/ddO(
)noitiddA(
)noisiviD(
)gnillirD(
)tropsnarT(
)ylbmessA(
)gnixiM(naturallanguagedescriptions.TheuseofLLMsisintendedto [3] M. Uschold, “Knowledge level modelling: concepts and terminology,”
reduce effort and expertise needed to create such an ontology. TheKnowledgeEngineeringReview,vol.13,no.1,pp.5–29,1998.
[4] F.Baader,D.Calvaneseetal.,TheDescriptionLogicHandbook:Theory,
ToanswerRQ1,weanalyzedtwoLLMswiththreeprompting
Implementation,andApplications. CambridgeUniversityPress,2007.
techniques to generate seven capability ontologies with differ- [5] A. Hogan, The Web of Data, 1st ed. Cham: Springer International
ent levels of complexity. Even zero-shot results are quite con- PublishingandImprint:Springer,2020.
[6] Y.Chang,X.Wangetal.,“ASurveyonEvaluationofLargeLanguage
vincing,butthefew-shotresults—especiallythosegenerated
Models,” ACM Transactions on Intelligent Systems and Technology,
by Claude — are close to perfect. Even complex capabilities vol.15,no.3,pp.1–45,2024.
with mathematical constraints are accurately modeled. One [7] T. B. Brown, B. Mann et al., “Language Models are Few-
Shot Learners,” 28.05.2020, 40+32 pages. [Online]. Available:
limitationofthestudyisthatthepromptsusedwerenotsubject
http://arxiv.org/pdf/2005.14165v4
to any further testing. Future work should evaluate different [8] “Towards Ontology Construction with Language Models.” [Online].
types of few-shot prompts, e.g., with different examples. Available:http://arxiv.org/pdf/2309.09898
[9] E. Ja¨rvenpa¨a¨, N. Siltala et al., “The development of an ontology
Furthermore, the natural language capability description was
for describing the capabilities of manufacturing resources,” Journal of
givenasplaintextwrittenbyontologyexperts.Comparingthis IntelligentManufacturing,vol.30,no.2,pp.959–978,2019.
with a more structured input that is clearly subdivided into [10] Y.Sure,S.Staab,andR.Studer,“OntologyEngineeringMethodology,”
in Handbook on Ontologies, S. Staab and R. Studer, Eds. Berlin,
capabilities, inputs, outputs etc. is also worth examining. In
Heidelberg:SpringerBerlinHeidelberg,2009,pp.135–152.
addition,thenaturallanguagecapabilitydescriptionsshouldbe [11] S. Cha´vez-Feria, R. Garc´ıa-Castro, and M. Poveda-Villalo´n, “Chowlk:
written by users without ontology expertise in another study. from uml-based ontology conceptualizations to owl,” in The Semantic
Web, P. Groth, M.-E. Vidal et al., Eds. Cham: Springer International
One persistent issue with LLMs is the fact that their
Publishing,2022,pp.338–352.
generated output is less dependable than rule-based mapping [12] Y. Huang, S. Dhouib, and J. Malenfant, “An AAS Modeling Tool for
approaches, which guarantee to generate a defined output for Capability-BasedEngineeringofFlexibleProductionLines,”inIECON
2021 - 47th Annual Conference of the IEEE Industrial Electronics
a given input. We therefore developed an approach to test the
Society. Piscataway,NJ:IEEE,2021,pp.1–6.
generated output in a robust and semi-automated way (RQ2). [13] Y. Xia, Z. Xiao et al., “Generation of asset administration shell with
This testing approach consists of an automated syntax check, large language model agents: Interoperability in digital twins with
semanticnode.”[Online].Available:http://arxiv.org/pdf/2403.17209
using OWL reasoning to test for inconsistencies as well as
[14] M. Trajanoska, R. Stojanov, and D. Trajanov, “Enhancing Knowledge
SHACL shapes to test for hallucinations and incompleteness. Graph Construction Using Large Language Models.” [Online].
Currently, these tests need to be conducted and interpreted Available:http://arxiv.org/pdf/2305.04676
[15] L.-P. Meyer, C. Stadler et al., “Llm-assisted knowledge graph
by ontology experts to properly record errors. In our future
engineering: Experiments with chatgpt.” [Online]. Available: http:
work, we want to use the findings of this study to create an //arxiv.org/pdf/2307.06917
engineeringmethodthatcanbeusedbyuserswithoutontology [16] H.BabaeiGiglou,J.D’Souza,andS.Auer,“Llms4ol:Largelanguage
modelsforontologylearning,”inThesemanticweb-ISWC2023,ser.
expertise. A first step in this direction is presented in [22].
LectureNotesinComputerScience,T.R.Payne,V.Presuttietal.,Eds.
Furthermore, we will further improve the CaSk ontology, in Cham:Springer,2023,vol.14265,pp.408–427.
whichannotationssuchasrdfs:labelorrdfs:comment [17] A.Ko¨cher,C.Hildebrandtetal.,“AFormalCapabilityandSkillModel
forUseinPlugandProduceScenarios,”in202025thIEEEInternational
are rarely used. These attributes, intended as purely human-
ConferenceonEmergingTechnologiesandFactoryAutomation(ETFA).
readableadditionalinformation,canbeprocessedbyLLMsto IEEE,9/8/2020-9/11/2020,pp.1663–1670.
better understand model elements. And finally, one drawback [18] “VDI 3682: Formalised Process Descriptions - Concept and Graphic
Representation,”2015.
of our prompts is that they include the CaSk TBox as well as
[19] “DINEN61360:StandardDataElementTypeswithAssociatedClassi-
the examples as plain text. This leads to high token consump- ficationScheme-Part1:Definitions-PrinciplesandMethods,”2018.
tion and in turn rather high cost per prompt. It also limits the [20] K. Wenzel, “OpenMath-RDF: RDF encodings for OpenMath objects
andContentDictionaries,”in31thOpenMathWorkshopatCICM2021,
choice of LLMs to be used to those with very large context
2021.
windows.Moreefficientwaystointegratecontextinformation [21] Anthropic, “The Claude 3 Model Family: Opus, Sonnet, Haiku,”
by using embedding techniques like the one presented in [23] Anthropic, Tech. Rep., 2024. [Online]. Available: https://www-cdn.
anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model
are thus worth investigating.
Card Claude 3.pdf
[22] L. M. Vieira da Silva, A. Ko¨cher et al., “Toward Automatically Gen-
ACKNOWLEDGMENT eratingCapabilityOntologiesfromNaturalLanguageDescriptionswith
LargeLanguageModels.”
This research in the RIVA project is funded by dtec.bw [23] J. Chen, P. Hu et al., “OWL2Vec*: embedding of OWL ontologies,”
– Digitalization and Technology Research Center of the MachineLearning,vol.110,no.7,pp.1813–1845,2021,pII:5997.
Bundeswehr. dtec.bw is funded by the European Union –
NextGenerationEU
REFERENCES
[1] A. Ko¨cher, A. Belyaev et al., “A Reference Model for Common
Understanding of Capabilities and Skills in Manufacturing,” at - Au-
tomatisierungstechnik,no.2,2023.
[2] A. Ko¨cher, C. Hildebrandt et al., “Automating the Development of
Machine Skills and their Semantic Description,” in 2020 25th IEEE
International Conference on Emerging Technologies and Factory Au-
tomation(ETFA). IEEE,08.09.2020-11.09.2020,pp.1013–1018.