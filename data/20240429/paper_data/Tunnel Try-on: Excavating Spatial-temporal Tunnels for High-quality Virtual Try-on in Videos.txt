Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual
Try-on in Videos
ZhengzeXu1∗,MengtingChen2,ZhaoWang2,LinyuXing2,ZhonghuaZhai2,NongSang1,JinsongLan2,ShuaiXiao2†,ChangxinGao1†
1NationalKeyLaboratoryofMultispectralInformationIntelligentProcessingTechnology,SchoolofArtificialIntelligence
andAutomation,HuazhongUniversityofScienceandTechnology
2Alibaba
{zhengzexu,cgao}@hust.edu.cn,{cmt271286,shuai.xsh}@alibaba-inc.com
https://mengtingchen.github.io/tunnel-try-on-page/
Input Video Clothing Synthetic Video
Figure1. GeneratedresultsofTunnelTry-on. Ourmodelachievesstate-of-the-artperformanceinthevideotry-ontask. Itcannotonly
handlecomplexclothingandbackgroundsbutalsoadapttodifferenttypesofhumanmovementsinthevideo(firstandsecondrows)and
cameraanglechanges(thirdrow).
Abstract
the clothing. To generate coherent motions, we first lever-
agetheKalmanfiltertoconstructsmoothcropsinthefocus
Videotry-onisachallengingtaskandhasnotbeenwell tunnelandinjectthepositionembeddingofthetunnelinto
tackled in previous works. The main obstacle lies in pre- attention layers to improve the continuity of the generated
serving the details of the clothing and modeling the co- videos. Inaddition,wedevelopanenvironmentencoderto
herent motions simultaneously. Faced with those difficul- extract the context information outside the tunnels as sup-
ties,weaddressvideotry-onbyproposingadiffusion-based plementary cues. Equipped with these techniques, Tunnel
frameworknamed”TunnelTry-on.”Thecoreideaisexca- Try-onkeepsthefinedetailsoftheclothingandsynthesizes
vatinga“focustunnel”intheinputvideothatgivesclose- stable and smooth videos. Demonstrating significant ad-
up shots around the clothing regions. We zoom in on the vancements, Tunnel Try-on could be regarded as the first
region in the tunnel to better preserve the fine details of attempttowardthecommercial-levelapplicationofvirtual
try-oninvideos.
*workdoneduringaninternshipatAlibaba
†correspondingauthors.
1
4202
rpA
62
]VC.sc[
1v17571.4042:viXra1.Introduction Net (noted as Main U-Net) as the main branch and uti-
lizesareferenceU-Net(notedasRefU-Net)toextractand
Video virtual try-on aims to dress the given clothing on
inject the fine details of the given clothing. By inserting
the target person in video sequences. It requires to pre-
Temporal-AttentionaftereachstageoftheMainU-Net,we
serve both the appearance of the clothing and the motions
extendthismodeltoconductvirtualtry-oninvideos. How-
of the person. It provides consumers with an interactive
ever,thisbasicsolutionisinsufficienttodealwiththechal-
experience,enablingthemtoexploreclothingoptionswith-
lengingcasesinreal-worldvideos.
out the necessity for physical try-on, which has garnered
We observe that the human often occupies a small area
widespread attention from both the fashion industry and
in videos and the area or location could change violently
consumersalike.
along with the camera movements. Thus, we propose to
Although there are not many studies on video try-
excavate a “tunnel” in the given video to provide a sta-
on, image-based try-on has already been extensively re-
ble close-up shot of the clothing region. Specifically, we
searched. Numerousclassicalimagevirtualtry-onmethods
conduct a region crop in each frame and zoom in on the
relyontheGenerative-Adversarial-Networks(GANs)[7,9,
cropped region to ensure that the individuals are appropri-
10, 20, 39]. These methods typically comprise two pri-
ately centered. This strategy maximizes the model’s capa-
mary components: a warping module that warps clothing
bilitiesforpreservingthefinedetailsofthereferencecloth-
tofitthehumanbodyinsemanticlevel,andatry-ongener-
ing. At the same time, we leverage Kalman filtering tech-
ator that blends the warped clothing with the human body
niques [43] to recalculate the coordinates of the cropping
image. Recently, with the development of diffusion mod-
boxesandinjectthepositionembeddingofthefocustunnel
els [33], the quality of image and video generation has
intotheTemporal-Attention. Inthisway,wecouldkeepthe
been significantly improved. Some diffusion-based meth-
smoothnessandcontinuityofthecroppedvideoregionand
ods [23, 52] for image virtual try-on have been proposed,
assistingeneratingmoreconsistentmotions. Additionally,
which do not explicitly incorporate a warp module but in-
although the regions inside the tunnel deserve more atten-
stead integrate the warp and blend process in a single uni-
tion,theoutsideregioncouldprovidetheglobalcontextfor
fiedprocess.Leveragingpre-trainedtext-to-imagediffusion
the background around the clothing. Thus, we develop an
models, these diffusion-based models achieve fidelity sur-
environmentencoder. Itextractsglobalhigh-levelfeatures
passingthatofGAN-basedmodels.
outsidethetunnelsandincorporatesthemintotheMainU-
It is evident that video try-on provides a more compre-
Nettoenhancethebackgroundgeneration.
hensive presentation of the try-on clothing under different
Extensive experiments demonstrate that equipped with
conditionscomparedtoimagetry-on. Adirecttransferap-
the aforementioned techniques, our proposed Tunnel Try-
proachistoapplyimagetry-onmethodstoprocessvideos
on significantly outperforms other video virtual try-on
frame by frame. However, this leads to significant inter-
methods.Insummary,ourcontributionscanbesummarized
frame inconsistency, resulting in unacceptable generation
inthefollowingthreeaspects:
outcomes. Several approaches have explored specialized
• We proposed Tunnel Try-on, the first diffusion-based
designsforvideovirtualtry-on[8,21,25,51]. Thesemeth-
videovirtualtry-onmodelthatdemonstratesstate-of-the-
odstypicallyutilizeopticalflowpredictionmodulestowarp
artperformanceincomplexscenarios.
framesgeneratedbythetry-ongenerator,aimingtoenhance
• We design a novel technique of constructing the focus
temporal consistency. ClothFormer [21] additionally pro-
tunneltoemphasizetheclothingregionandgenerateco-
poses temporal smoothing operations for the input to the
herentmotioninvideos.
warping module. While these explorations of video try-
• We further develop several enhancing strategies like in-
on make steady advancements, most of them only tackle
corporatingtheKalmanfiltertosmooththefocustunnel,
simple scenarios. For example, in VVT [8] dataset, sam-
leveraging the tunnel position embedding and environ-
ples mainly include simple textures, tight-fitting T-shirts,
ment context in the attentions to improve the generation
plain backgrounds, fixed camera angles, and repetitive hu-
quality.
man movements. This notably lags behind the standards
of image virtual try-on and falls short of meeting practi-
2.RelatedWork
cal application needs. We analyze that, different from the
image-basedsettings,themainchallengeinvideotry-onis
2.1.ImageVisualTry-on
preservingthefinedetailoftheclothingandgeneratingco-
herentmotionsatthesametime. Theimagevirtualtry-onmethodscangenerallybedivided
In this paper, to address the aforementioned challenges into two categories: GAN-based methods [7, 10, 13, 20,
in complex natural scenes, we propose a novel framework 26, 28, 36, 39] and diffusion-based methods [1, 4, 11, 23,
termed Tunnel Try-on. We start with a strong baseline of 29, 52]. The GAN-based methods typically utilize Condi-
image-based virtual try-on. It leverages an inpainting U- tionalGenerativeAdversarialNetwork(cGAN)[27]andof-
2ten have two decoupled modules: a warping module that motion, previous video try-on methods are limited to han-
adjusts the clothing to fit the human body at a semantic dlingsimplecases,suchasminormovements,simpleback-
levelandaGAN-basedtry-ongeneratorthatblendsthead- grounds, and clothing with simple textures. Additionally,
justed clothing with the human body image. To achieve previousvideotry-onmethodshaveonlyfocusedontight-
accurate clothing wrapping, existing techniques estimate a fitting tops. These limitations make them inadequate for
dense flow map or apply alignment strategies between the real-worldscenariosinvolvingdiverseclothingtypes,com-
warped clothing and the human body. VITON [13] pro- plex backgrounds, free-form movements, and variations in
poses a coarse-to-fine strategy to warp a desired clothing thesize,proportion,andpositionofindividuals. Therefore,
onto the corresponding region. CP-VTON [39] preserves weproposetoremoveexplicitwarpmodulesandutilizedif-
theclothingidentitywiththehelpofageometricmatching fusionmodelsforvideotry-on, whileemployingthefocus
module. Using knowledge distillation, PBAFN [10] pro- tunnelstrategytoadapttothevariedrelationshipsbetween
posed a parser-free method, which can reduce the require- individualsandbackgroundsinreal-worldapplications.
mentforaccuratemasks.VITON-HD[7]adoptsalignment-
2.3.ImageAnimation
awaresegmentnormalizationtoalleviatemisalignmentbe-
tweenthewarpedclothingandthehumanbody. However, Imageanimationaimstogenerateavideosequencefroma
these approaches face challenges in dealing with images static image. Recently, some diffusion-based models have
ofindividualsincomplexposesandintricatebackgrounds. shown unprecedented success [6, 18, 19, 22, 30, 40, 44,
Moreover,conditionalGANsstrugglewithsignificantspa- 46, 48, 50]. Among them, Magic Animate [44] and An-
tial transformations between the clothing and the person’s imate Anyone [18] have demonstrated the best generation
posture.Theexceptionalgenerativecapabilitiesofdiffusion results. Both models utilize an additional U-Net to extract
have inspired several diffusion-based image try-on meth- appearanceinformationfromimagesandanencodertoen-
ods. TryOnDiffusion[52]employsadualU-Netsarchitec- codeposesequences.Combiningthecurrentbestanimation
tureforimagetry-on,whichrequiresextensivedatasetsfor frameworks with advanced image try-on methods can also
training. Subsequent methods tend to leverage large-scale achievevideotry-on. However,adrawbackofthispipeline
pre-traineddiffusionmodelsaspriorsinthetry-onnetworks is the lack of guidance from human video information, re-
[17, 33, 45]. LADI-VTON [29] treats clothing as pseudo- sulting in the network only generating static backgrounds,
words. DCI-VTON[11]integratesclothingintopre-trained making it difficult for the characters to blend into the real
diffusionmodelsbyemployingwarpingnetworks. Stable- environment and achieve satisfactory try-on effects. Ad-
VITON[23]proposestoconditiontheintermediatefeature ditionally, relying solely on pose-driven actions can lead
mapsoftheMainU-Netusingazerocross-attentionblock. tostrangegenerationresultswhenconductingvisualtry-on
While these diffusion-based methods have achieved high- withsignificantperson’smovements.
fidelity single-image inference, when applied to video vir-
tual try-on, the absence of inter-frame relationship consid- 3.Method
erationleadstosignificantinter-frameinconsistency,result-
InSection3.1,weintroducethefoundationalknowledgeof
inginunacceptablegenerationresults.
latentdiffusionmodelsrequiredforsubsequentdiscussions.
Section3.2providesacomprehensiveexpositionofthenet-
2.2.VideoVisualTry-on
workarchitectureofourTunnelTry-on. InSection3.3,we
Comparedtoimage-basedtry-on,videovisualtry-onoffers present details of the focus tunnel extraction strategy. In
usersahigherdegreeoffreedomintryingonclothingand Section 3.4, we introduce the enhancing strategies for the
providesamorerealistictry-onexperience. However,there focus tunnel, including tunnel smoothing and tunnel em-
havebeenfewstudiesexploringvideovisualtry-ontodate. bedding. In Section 3.5, we elaborate on the environment
FW-GAN[8]utilizesanopticalflowpredictionmodule[41] encoder which aims at extracting the global context as the
to warp past frames in video virtual try-on to generate co- complementary.Atlast,wesummarizeourtrainingandval-
herent video. FashionMirror [3] also employs optical flow idationpipelineinSection3.6.
towarppastframes,butitwarpsatthefeaturelevelinstead
3.1.Preliminaries
of the pixel level. MV-TON [51] adopts a memory refine-
mentmoduletorememberthepreviouslygeneratedframes. Diffusion models [16] have demonstrated promising ca-
ClothFormer [21] proposes a dual-stream transformer ar- pabilities in both image and video generation. Built on
chitecturetoextractandfusetheclothingandtheperson’s the Latent Diffusion Model (LDM), Stable Diffusion [33]
features. It also suggests using a tracking strategy based conducts denoising in the latent space of an auto-encoder.
onopticalflowandridgeregressiontoobtainatemporally Trainedonthelarge-scaleLAIONdataset[35],StableDif-
consistent warp sequence. Due to the difficulties faced by fusiondemonstratesexcellentgenerationperformance. Our
warpmodulesinhandlingcomplextexturesandsignificant networkisbuiltuponStableDiffusion.
3Clothing
RefU-Net CLIP
Encoder
Pose
Encoder
InputVideo
Tunnel Blend
Tunnel
Extraction Zoom-In Main
U-Net
TunnelEmbedding
Ref-Attention Env-Attention C Concatenate
EnvEncoder
Temporal-Attention Self-Attention Add
C
TunnelEmbedding
Person C Embedding Projector
Denoising Clothing
Feature RC efl o Ft eh ain tug re Embedding EmbE en dv ding T Iu nn fn oel Sinusoidal Embedding
Ref-Attention Env-Attention Temporal-Attention
Figure2. TheoverviewofTunnelTry-on. Givenaninputvideoandaclothingimage,wefirstextractafocustunneltozoominonthe
regionaroundthegarmentstobetterpreservethedetails. Thezoomedregionisrepresentedbyasequenceoftensorsconsistingofthe
backgroundlatent,latentnoise,andthegarmentmask,whichareconcatenatedandfedintotheMainU-Net. Atthesametime,weuse
aRefU-NetandaCLIPEncodertoextracttherepresentationsoftheclothingimage. Theseclothingrepresentationsarethenaddedto
theMainU-Netusingref-attention. Moreover,humanposeinformationisaddedintothelatentfeaturetoassistingeneration. Thetunnel
embeddingisalsointegratedintotemporalattentiontogeneratingmoreconsistentmotions,andanenvironmentencoderisdevelopedto
extracttheglobalcontextasadditionalguidance.
Givenaninputimagex ,themodelfirstemploysalatent 3.2.OverallArchitecture
0
encoder[24]toprojectitintothelatentspace: z =E(x ).
0 0 This section provides a comprehensive illustration of the
Throughout the training, Stable Diffusion transforms the
pipeline presented in Figure 2. We start with introducing
latent representation into Gaussian noise by applying a
the strong baseline for image try-on. Then, we extend it
variance-preserving Markov process [37] to z , which can
0 to videos by adding Temporal-Attention. Afterwards, we
beformulatedas:
brieflydescribeournoveldesignswhichwillbeelaborated
√ √ oninthenextsections.
z = α¯ z + 1−α¯ ϵ, ϵ∼U([0,1]) (1)
t t 0 t
whereα¯ isthecumulativeproductofthenoisecoefficient Image try-on baseline. The baseline (modules in gray)
t
α ateachstep. Subsequently,thedenoisingprocesslearns of Tunnel Try-on consists of two U-Nets: the Main U-
t
the prediction of noise ϵ (z ,c,t), which can be summa- NetandtheRefU-Net. TheMainU-Netisinitializedwith
θ t
rizedas: aninpaintingmodel. TheRefU-Net[47]hasbeenproven
effective [4, 18, 44] in preserving detailed information of
L =E (|ϵ−ϵ (z ,c,t)|2). (2) reference images. Therefore, Tunnel Try-on utilizes the
LDM z,c,ϵ,t θ t 2
RefU-Nettoencodethefine-grainedfeaturesofreference
Here, t represents the diffusion timestep, c denotes the clothing. Additionally,TunnelTry-on employsaCLIPim-
conditioning text prompts from the CLIP [32], and ϵ age encoder to capture high-level semantic information of
θ
denotes the noise prediction neural networks like the U- target clothing images, such as overall color. Specifically,
Net[34]. Ininference,StableDiffusionreconstructsanim- theMainU-Nettakesa9-channeltensorwiththeshapeof
age from Gaussian noise step by step, predicting the noise B×9×H ×W as input, where B, H, and W denote the
addedateachstage. Thedenoisedresultsarethenfedinto batchsize,height,andwidth. The9channelsconsistofthe
alatentdecodertoregenerateimagesfromthelatentrepre- clothing-maskedvideoframe(4channels),thelatentnoise
sentations,denotedasxˆ =D(ˆz ). (4 channels), and the cloth-agnostic mask (1 channel). To
0 0
4
noitnettA-fleS noitnettA-ssorC noitnettA-fleSenhanceguidanceonthemovementsofthegeneratedvideo rulestoensurecoverageofallclothing. Sincetheexpanded
and further improve its fidelity, we incorporate pose maps boundingboxsequenceresemblesaninformationtunnelfo-
as an additional control adjustment. These pose maps, en- cusedontheperson,werefertoitasthe”focustunnel”of
coded by a pose encoder comprising several convolutions, the input video. Next, we zoom in on the tunnel. In other
areaddedtotheconcatenatedfeatureinthelatentspace. words,thevideoframeswithinthefocustunnelarecropped,
padded, and resized to the input resolution. Then they are
Adaption for videos. To adapt the image try-on model combined to form a new sequence input for the Main U-
for processing videos, we insert Temporal-Attention after Net. The generated video output from the Main U-Net is
each stage of the Main U-Net. Specifically, Temporal- thenblendedwiththeoriginalvideousingGaussianblurto
Attention conducts self-attention on features of the same achievenaturalintegration.
spatial position across different frames to ensure smooth
3.4.FocusTunnelEnhancement
transitions between frames. The feature maps of the
Main U-Net are extended with the temporal dimension Since the process of focus tunnel extraction is computed
of f, denoting the frames. Thus, the input shape be- only within individual frames without considering inter-
comesB×9×f ×H ×W. Therefore, asshowninRef- framerelationships,slightjittersorjumpsofboundingbox
Attention,thefeaturemapsfromtheRefU-Netarerepeated sequences may occur when applied to videos, due to the
f times and further concatenated along the spatial dimen- movement of people and the camera. These jitters and
sion. Subsequently, after flattening along the spatial di- jumpscanresultinfocustunnelsthatappearunnaturalcom-
mension, the concatenated features are input into the self- paredtovideoscapturednaturally,increasingthedifficulty
attention module, and the output features retain only the oftemporalattentionconvergenceandleadingtodecreased
originaldenoisingfeaturemappart. temporalconsistencyinthegeneratedvideos. Dealingwith
this challenge, we propose tunnel smoothing and injecting
tunnelembeddingintotheattentionlayers.
Novel designs of Tunnel Try-on. We excavate a Focus
Tunnelintheinputvideoandzoominontheregiontoem-
phasizetheclothing. Toenhancethevideoconsistency,we Tunnel smoothing. To smooth the focus tunnel and
leverage the Kalman filter to smooth the tunnel and inject achieve a variation effect similar to natural camera move-
thetunnelembeddingintothetemporalattentionlayers. Si- ments, we propose the focus tunnel smoothing strategy.
multaneously,wedesignanenvironmentencoder(EnvEn- Specifically,wefirstuseKalmanfilteringtocorrectthefo-
coderinFigure2)tocapturetheglobalcontextinformation custunnel,whichcanberepresentedasAlgorithm1.
in each video frame as complementary cues. In this way,
the Main U-Net primarily utilizes three types of attention Algorithm1:KalmanFilter.
modules to integrate control conditions at various levels,
Input: Rawtunnelcoordinatex,tunnellengthf.
enhancingthespatio-temporalconsistencyofthegenerated
Result: Smoothedtunnelcoordinatexˆ.
video. These modules are depicted in the bottom colored
1 Initialize
box in Figure 2. Each of the novel modules will be intro-
P =x ,xˆ =x ,Q=0.001,R=0.0015,t=1.
0 1 0 1
ducedindetailinthefollowingsections.
2 repeat
3.3.FocusTunnelExtraction 3 Projectthestateaheadxˆ− t =xˆ t−1.
4 Projecttheerrorcovarianceahead
Intypicalimagevirtualtry-ondatasets,thetargetpersonis P− =P +Q.
t t−1
typically centered and occupies a large portion of the im-
5 ComputetheKalmanGain
age. However, in video virtual try-on, due to the move- K =P−(P−+R)−1
t t t
mentofthepersonandcamerapanning,thepersoninvideo 6 Updatetheestimatexˆ t =xˆ− t +K t(x t−xˆ− t )
frames may appear at the edges or occupy a smaller por-
7 Updatetheerrorcovariance
tion. This can lead to a decrease in the quality of video
P =P−(1−K )−1
generationresultsandreducethemodel’sabilitytomaintain t t t
8 t←t+1.
clothingidentity.Toenhancethemodel’sabilitytopreserve
9 untilt>f;
details and better utilize the training weights learned from
Output: xˆ
image try-on data, we propose the ”focus tunnel” strategy,
asshownFigure2.
Specifically, depending on the type of try-on clothing, xˆ representsthesmoothedcoordinateofthefocustun-
t
weutilizetheposemaptoidentifytheminimumbounding nel at time t, calculated using the prediction equation of
boxfortheupperorlowerbody.Wethenexpandthecoordi- the Kalman filter. x represents the observed position of
t
natesoftheobtainedboundingboxaccordingtopredefined thetunnelattimet,i.e.,thecoordinateofthetunnelbefore
5smoothing. AftertheKalmanfilter,wefurtherfilteroutthe Inthesecondstage,allstrategiesandmodulesareincor-
high-frequency jitter caused by exceptional cases using a porated, and the model is trained on video try-on datasets.
low-passfilter. Only the parameters of the Temporal-Attention, Environ-
ment Encoder are updated in this stage. The goal of this
stageistoleveragetheimage-leveltry-oncapabilitylearned
Tunnel embedding. The input form of the focus tunnel
in the first stage while enabling the model to learn tempo-
has increased the magnitude of the camera movement. To
rally related information, resulting in high spatio-temporal
mitigatethechallengefacedbythetemporal-attentionmod-
consistencyintry-onvideos.
ule in smoothing out such significant camera movements,
we introduce the Tunnel Embedding. Tunnel Embedding
Test process. During the testing phase, the input video
accepts a three-tuple input, comprising the original image
undergoes Tunnel Extraction to obtain the Focus Tunnel.
size,tunnelcentercoordinates,andtunnelsize. Inspiredby
Theinputvideo, alongwiththeconditionalvideos, isthen
the design of resolution embedding in SDXL [31], Tunnel
zoomedinonthefocustunnelandfedintotheMainU-Net.
Embedding first encodes the three-tuple into 1D absolute
GuidedbytheoutputsoftheRefU-Net,CLIPEncoder,En-
positionencoding,andthenobtainsthecorrespondingem-
vironment Encoder, and Tunnel Embedding, the Main U-
bedding through linear mapping and activation functions.
Netprogressivelyrecoversthetry-onvideofromthenoise.
Subsequently, the focus tunnel embedding is added to the
Finally,thegeneratedtry-onvideoundergoesTunnel-Blend
temporal attention as position encoding. With Tunnel em-
post-processingtoobtainthedesiredcompletetry-onvideo.
bedding,temporalattentionintegratesdetailsaboutthesize
and position of the focus tunnel, aiding in preventing mis-
4.Experiments
alignment with focus tunnels affected by excessively large
camera movements. This enhancement contributes to im- 4.1.Datasets
provingthetemporalconsistencyofvideogenerationwithin
thefocustunnel. We evaluate our Tunnel Try-on on two video try-on
datasets:theVVT[8]datasetandourcollecteddataset.The
3.5.EnvironmentFeatureEncoding VVTdatasetisastandardvideovirtualtry-ondataset,com-
prising791pairedpersonvideosandclothingimages,with
Afterapplyingthefocustunnelstrategy,thecontexttendsto
a resolution of 192×256. The models in the videos have
belost,posingachallengeingeneratingareasonableback-
similar and simple poses and movements on a pure white
groundwithinthemaskedarea.Toaddressthis,wepropose
background, while the clothes are all fitted tops. Due to
theEnvironmentEncoder. ItconsistsofafrozenCLIPim-
these limitations, the VVT dataset fails to reflect the real-
ageencoderandalearnablelinearmappinglayer. Initially,
world application scenarios of visual video try-on. There-
themaskedoriginalimageisencodedbyafrozenCLIPim-
fore, we collected a dataset from real e-commerce appli-
ageencodertocapturetheoverallinformationabouttheen-
cation scenarios, featuring complex backgrounds, diverse
vironment.Subsequently,theoutputCLIPfeaturesarefine-
movementsandbodyposes, andvarioustypesofclothing.
tunedthroughalearnablelinearprojectionlayer. Asshown
The dataset consists of a total of 5,350 video-image pairs.
intheEnv-AttentionofFigure2,theoutputfeaturesofEn-
We divided it into 4,280 training videos and 1,070 testing
vironmentEncoder,servingaskeysandvalues,areinjected
videos, each containing 776,536 and 192,923 frames, re-
intothedenoisingprocessthroughcross-attention.
spectively.
3.6.TrainandTestPipeline 4.2.ImplementDetails
Trainingprocess. Thetrainingphasecanbedividedinto Model configurations. In our implementation, the Main
twostages. Inthefirststage, themodelexcludestemporal U-NetisinitializedwiththeinpaintingmodelweightofSta-
attention, the Environment Encoder, and the Tunnel Em- bleDiffusion-1.5[33]. TheRefU-Netisinitializedwitha
bedding. Additionally, we freeze the weights of the VAE standard text-to-image SD-1.5. The Temporal-Attention is
encoder and decoder (omitted in Fig 2 for simplicity), as initializedfromthemotionmoduleofAnimateDiff[12].
well as the CLIP image encoder, and only update the pa-
rameters of the Main U-Net, Ref U-Net, and pose guider. Training and testing protocols. The training phase is
In this stage, the model is trained on paired image try-on structuredintwostages. Inbothstages,weresizeandpad
data. The objective of this stage is to learn the extraction the inputs to a uniform resolution of 512x512 pixels, and
and preservation of clothing features using larger, higher- we adopt an initial learning rate of 1e-5. The models are
quality, and more diverse paired image data compared to trained on 8x A100 GPUs. In the first stage, we utilized
the video data, aiming to achieve high-fidelity image-level image try-on pairs extracted from video data, and merged
try-ongenerationresultsasasolidfoundation. them with existing image try-on datasets VITON-HD [7]
6Figure3.QualitativecomparisonwithexistingalternativesontheVVTdataset.Theclothingandtargetpersonisshownin(a).Theresults
of(b)FW-GAN,(c)PBAFN,(d)ClothFormer,(e)StableVITON,and(f)TunnelTry-onarerepresentedrespectively.
Figure4. QualitativeresultsofTunnelTry-on onourdataset. Wepresentthetry-onresultsofpantsandskirts,aswellascross-category
try-onresults.
fortraining.Then,wesampleaclipconsistingof24frames utilizedtheVITON-HD[7]datasetforthefirst-stagetrain-
inthevideosastheinputfortraininginstage2. Inthetest- ing and conducted second-stage training on the VVT [8]
ingphase, weusethetemporalaggregationtechnique[38] datasetwithoutusingourowndataset.
to combine different video clips, producing a longer video
Figure3displaysthequalitativeresultsofvariousmeth-
output.
ods on the VVT dataset. From Figure 3, it is evident that
GAN-basedmethodslikeFW-GANandPBAFN,whichuti-
4.3.ComparisonswithExistingAlternatives
lizewarpingmodules, struggletoadapteffectivelytovari-
We conducted a comprehensive comparison with other vi- ations in the sizes of individuals in the video. Satisfactory
sual try-on methods on the VVT dataset, including quali- results are achieved only in close-up shots, with the warp-
tative, quantitative comparisons and user studies. We col- ing of clothing producing acceptable outcomes. However,
lected several visual try-on methods, covering both GAN- whenthemodelmovesfartherawayandbecomessmaller,
basedmethodslikeFW-GAN[8], PBAFN[10]andCloth- the warping module produces inaccurately wrapped cloth-
Former[21],anddiffusion-basedmethodslikeAnydoor[5] ing, resulting in unsatisfactory single-frame try-on results.
and StableVITON [23]. To ensure a fair comparison, we ClothFormercanhandlesituationswheretheperson’spro-
7
tupnI
)a(
NFABP
)c(
NOTIVelbatS
)e(
NAG-WF
)b(
remroFhtolC
)d(
sruO
)f(Table 1. Comparison on the VVT dataset: ↑ denotes higher is For video-based evaluation, we employ the Video
better,while↓indicateslowerisbetter. Frechet Inception Distance (VFID) [8] to evaluate vi-
sual quality and temporal consistency. The FID [15]
Method SSIM↑ LPIPS↓ VFID ↓ VFID ↓
I3D ResNeXt
measures the diversity of generated samples. Further-
CP-VTON[39] 0.459 0.535 6.361 12.10
more, VFID employs 3D convolution to extract features
FW-GAN[8] 0.675 0.283 8.019 12.15
PBAFN[10] 0.870 0.157 4.516 8.690 in both temporal and spatial dimensions for better mea-
ClothFormer[21] 0.921 0.081 3.967 5.048 sures. Two CNN backbone models, namely I3D [2] and
AnyDoor[5] 0.800 0.127 4.535 5.990 3D-ResNeXt101[14],areadoptedasfeatureextractorsfor
StableVITON[23] 0.876 0.076 4.021 5.076
VFID.
TunnelTry-on 0.913 0.054 3.345 4.614
Table1demonstratesthatontheVVTdataset,ourTun-
nel Try-on outperforms others in terms of SSIM, LPIPS,
Table2.UserstudyforthepreferencerateontheVVTtestdataset.
* indicates testing was conducted only on examples shown in andVFIDmetrics,furtherconfirmingthesuperiorityofour
ClothFormerdemonstrations. modelinimagevisualquality(similarityanddiversity)and
temporalcontinuitycomparedtoothermethods. It’sworth
Method Quality% Fidelity% Smoothness% notingthatwehaveasubstantialadvantageinLPIPScom-
paredtoothermethods. ConsideringthatLPIPSismorein
FW-GAN[8] 0 0 5.62
linewithhumanvisualperceptioncomparedtoSSIM,this
PBAFN[10] 6.77 8.77 6.31
highlightsthesuperiorvisualqualityofourapproach.
AnyDoor[5] 7.85 7.08 0
Considering that the quantitative metrics could not per-
StableVITON[23] 15.46 16.54 0
fectlyalignwiththehumanpreferenceforgenerationtasks,
TunnelTry-on 69.92 67.62 88.07
weconductedauserstudytoprovidemorecomprehensive
ClothFormer*[21] 30.8 26.0 39.6 comparisons. We organized a group of 10 annotators to
TunnelTry-on* 69.2 74.0 60.4 makecomparisonsonthe130samplesofVVTtestset. We
let different methods generate videos for the same input,
andlettheannotatorspickthebestone. Theevaluationcri-
portion is relatively small, but its generated results are teria included three aspects: quality, fidelity, and smooth-
blurry,withsignificantcolordeviation. ness. Specifically, ”Quality” denotes the image quality,
encompassing aspects like artifacts, noise levels, and dis-
Wealsoextendsomediffusion-basedimagetry-onmeth-
ods (e.g., AnyDoor and StableVITON) to videos by per- tortion. ”Fidelity” measures the ability to preserve details
compared to the reference clothing image. ”Smoothness”
frame generation. We observe that they can generate rel-
evaluatesthetemporalconsistencyofthegeneratedvideos.
atively accurate single-frame results. However, due to the
Note that ClothFormer is not open-sourced but it provides
lackofconsiderationfortemporalcoherence,therearedis-
25generationresults.Weconductanindividualcomparison
crepancies between consecutive frames. As shown in Fig-
inthebottomblockofTable1forthe25providedresultsbe-
ure 3(e), the letters on the clothing change in different
tweenClothFormerandourmethod. Resultsshowthatour
frames. Additionally,therearelotsofjittersbetweenadja-
methoddemonstratessignificantsuperiorityovertheothers.
centframesinthesemethods,whichcanbeobservedmore
intuitivelyinvideos.
4.4.QualitativeAnalysis
Compared with those existing solutions, our Tunnel
Try-on seamlessly integrates diffusion-based models and Due to the limited diversity and the simplicity of samples
video generation models, enabling the generation of accu- in the VVT dataset, it fails to represent the scenarios en-
rate single-frame try-on videos with high inter-frame con- countered in actual video try-on applications. Therefore,
sistency. AsdepictedinFigure3(f),thelettersonthechest weprovideadditionalqualitativeresultsonourowndataset
oftheclothingremainconsistentandcorrectastheperson tohighlighttherobusttry-oncapabilitiesandpracticalityof
movescloser. Tunnel Try-on. Figure 1 illustrates various results gener-
In Table 1, we conduct quantitative experiments atedbyTunnelTry-on,includingscenariossuchaschanges
with both image-based and video-based metrics. For inthesize ofindividualsdueto person-to-camera distance
image-based evaluation, we utilize structural similarity variation,theparallelmotionrelativetothecamera,andal-
(SSIM)[42]andlearnedperceptualimagepatchsimilarity terations in background and perspective induced by cam-
(LPIPS) [49]. These two metrics are used to evaluate the eraanglechanges. Byintegratingthefocustunnelstrategy
qualityofsingle-imagegenerationunderthepairedsetting. and focus tunnel enhancement, our method demonstrates
The higher the SSIM and the lower the LPIPS, the greater the ability to effectively adapt to different types of human
thesimilaritybetweenthegeneratedimageandtheoriginal movements and camera variations, resulting in high-detail
image. preservationandtemporalconsistencyinthegeneratedtry-
8(a) input (b) w/o tunnel (c) w/ tunnel
Figure5. Qualitativeablationsforthefocustunnel. Thiszoom-
in strategy brings notable improvements for preserving the fine
detailsoftheclothing.
Figure6. Qualitativeablationsforthetunnelenhancement. Itas-
onvideos.
sistsingeneratingmorestableandcontinuoustextures.
Moreover,unlikepreviousvideotry-onmethodslimited
to fitting tight-fitting tops, our model can perform try-on (a) w/o Env (b) w/ Env (c) w/o Env (d) w/ Env
tasks for different types of tops and bottoms based on the
user’s choices. Figure 4 presents some try-on examples of
differenttypesofbottoms.
4.5.AblationStudy
We conducted ablation experiments for Tunnel Try-on to
exploretheimpactoffocustunnelextraction(Section3.3),
focus tunnel enhancement (Section 3.4), and environment
Figure 7. Qualitative ablations for the environment encoder.
encoding (Section 3.5). We conduct both qualitative and
Theglobalcontextcontributestotherecoveryofthebackground
quantitativeablationsonourcollecteddatasettoassesstheir
aroundtheclothingregions.
performance.
InTable3,weprovidequantitativemetricsrelatedtothe Table 3. Quantities ablations for the core components. “Tun-
ablation experiments. The Focus Tunnel strategy signifi- nel”, “Enhance”, and “Env” denote the focus tunnel, the tunnel
cantlyimprovesthemodel’sSSIMandLPIPSmetrics, but enhancement,andtheenvironmentencoderrespectively.
itleadstoacertaindegreeofdecreaseintheVFIDmetric.
ThisindicatesthattheFocusTunnelcaneffectivelyenhance
Tunnel Enhance Env SSIM↑ LPIPS↓ VFID I3D↓ VFID ResNeXt↓
0.801 0.061 6.103 8.751
the quality of frame generation but may introduce more
✓ 0.877 0.052 6.759 9.034
flickering, reducing the temporal consistency of the video. ✓ ✓ 0.914 0.049 5.997 8.356
However,withthetunnelenhancement,thenetwork’sVFID ✓ ✓ ✓ 0.909 0.042 5.901 8.348
shows a significant improvement, while the SSIM also in-
creases. Lastly, although the environment encoder does
not exert a significant impact on quantitative metrics, we todecreasedtemporalconsistencyinthegeneratedvideo.
observed that it contributes to the generation of the back- Figure 7 illustrates the impact of the environment en-
groundenvironmentsaroundtheclothing,asdemonstrated coderonthegenerationresults. Sincetheenvironmenten-
inFigure7. Weconductadetailedanalysisofeachcompo- codercanextractoverallcontextinformationoutsidethefo-
nentinthefollowingparagraphs. cus tunnel, it can enhance the quality of the background
As shown in Figure 5, the impact of the Focus Tunnel around the garment, making it more consistent with high-
Strategy is evident. Without the focus tunnel, there exists level semantic information about the environment. As
obviousdistortioninthedetailsofthelogos.However,after showninFigure7,whentheenvironmentencoderisadded,
zoominginonthetunnelregionswithaclose-upshotofthe the generation errors in the textures of the walls and zebra
clothing.Thedetailedinformationofthegarmentscouldbe crossingsnearthehumanarecorrected.
significantlybetterpreserved.
5.Conclusion
InFigure6,weinvestigatetheeffectivenessofthetunnel
enhancement. As depicted in the red box area, when the We propose the first diffusion-based video visual try-on
tunnelenhancementisnotemployed(firstrow),theclothing model, Tunnel Try-on. It outperforms all existing alter-
texturesexhibitvariationsandflickeringovertime,leading natives in both qualitative and quantitative comparisons.
9
tnemecnahnE
o/w
)a(
tnemecnahnE
/w
)b(Leveraging the focus tunnel, tunnel enhancement, and en- [11] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen
vironmentencoding,ourmodelcanadapttodiversecamera Qian, and Liqing Zhang. Taming the power of diffusion
movementsandhumanmotionsinvideos. Trainedonreal modelsforhigh-qualityvirtualtry-onwithappearanceflow.
datasets, our model could handle virtual try-on in videos arXivpreprintarXiv:2308.06101,2023. 2,3
withcomplexbackgroundsanddiverseclothingtypes,pro- [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang,
ducing high-fidelity try-on results. Serving as a practical YaohuiWang,YuQiao,ManeeshAgrawala,DahuaLin,and
Bo Dai. Animatediff: Animate your personalized text-to-
tool for the fashion industry, Tunnel Try-on provides new
image diffusion models without specific tuning. Interna-
insightsforfutureresearchinvirtualtry-onapplications.
tionalConferenceonLearningRepresentations,2024. 6
References [13] XintongHan,ZuxuanWu,ZheWu,RuichiYu,andLarryS
Davis. Viton: An image-based virtual try-on network. In
[1] Alberto Baldrati, Davide Morelli, Giuseppe Cartella, Mar- ProceedingsoftheIEEEconferenceoncomputervisionand
cellaCornia,MarcoBertini,andRitaCucchiara.Multimodal patternrecognition,pages7543–7552,2018. 2,3
garmentdesigner:Human-centriclatentdiffusionmodelsfor [14] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can
fashionimageediting. InProceedingsoftheIEEE/CVFIn- spatiotemporal3dcnnsretracethehistoryof2dcnnsandim-
ternational Conference on Computer Vision, pages 23393– agenet? InProceedingsoftheIEEEconferenceonComputer
23402,2023. 2 VisionandPatternRecognition,pages6546–6555,2018. 8
[2] Joao Carreira and Andrew Zisserman. Quo vadis, action
[15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
recognition? anewmodelandthekineticsdataset. Inpro-
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
ceedings of the IEEE Conference on Computer Vision and
twotime-scaleupdateruleconvergetoalocalnashequilib-
PatternRecognition,pages6299–6308,2017. 8
rium. Advances in neural information processing systems,
[3] Chieh-YunChen,LingLo,Pin-JuiHuang,Hong-HanShuai,
30,2017. 8
and Wen-Huang Cheng. Fashionmirror: Co-attention
[16] JonathanHo,AjayJain,andP.Abbeel. Denoisingdiffusion
feature-remapping virtual try-on with sequential template
probabilisticmodels. ArXiv,abs/2006.11239,2020. 3
poses.In2021IEEE/CVFInternationalConferenceonCom-
puterVision(ICCV),pages13789–13798,2021. 3 [17] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-
[4] MengtingChen,XiChen,ZhonghuaZhai,ChenJu,Xuewen fusionprobabilisticmodels. Advancesinneuralinformation
Hong,JinsongLan,andShuaiXiao. Wear-any-way:Manip- processingsystems,33:6840–6851,2020. 3
ulable virtual try-on via sparse correspondence alignment. [18] LiuchengHu,XinGao,PengZhang,KeSun,BangZhang,
arXivpreprintarXiv:2403.12965,2024. 2,4 and Liefeng Bo. Animate anyone: Consistent and con-
[5] XiChen,LianghuaHuang,YuLiu,YujunShen,DeliZhao, trollable image-to-video synthesis for character animation.
andHengshuangZhao. Anydoor:Zero-shotobject-levelim- ArXiv,abs/2311.17117,2023. 3,4
agecustomization. arXivpreprintarXiv:2307.09481,2023. [19] YaosiHu,ChongLuo,andZhenzhongChen. Makeitmove:
7,8 Controllable image-to-video generation with text descrip-
[6] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu tions. 2022IEEE/CVFConferenceonComputerVisionand
Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real PatternRecognition(CVPR),pages18198–18207,2021. 3
image animation with text-guided motion control. arXiv
[20] ThibautIssenhuth,Je´re´mieMary,andCle´mentCalauzenes.
preprintarXiv:2312.02928,2023. 3
Do not mask what you do not need to mask: a parser-free
[7] SeunghwanChoi, SunghyunPark, MinsooLee, andJaegul
virtualtry-on. InComputerVision–ECCV2020: 16thEuro-
Choo. Viton-hd: High-resolution virtual try-on via
pean Conference, Glasgow, UK, August 23–28, 2020, Pro-
misalignment-aware normalization. In Proceedings of the
ceedings,PartXX16,pages619–635.Springer,2020. 2
IEEE/CVF conference on computer vision and pattern
[21] Jianbin Jiang, Tan Wang, He Yan, and Junhui Liu. Cloth-
recognition,pages14131–14140,2021. 2,3,6,7
former: Tamingvideovirtualtry-oninallmodule. InPro-
[8] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu,
ceedingsoftheIEEE/CVFConferenceonComputerVision
Bing-Cheng Chen, and Jian Yin. Fw-gan: Flow-navigated
andPatternRecognition,pages10799–10808,2022. 2,3,7,
warping gan for video virtual try-on. In Proceedings of
8
theIEEE/CVFinternationalconferenceoncomputervision,
[22] Johanna Suvi Karras, Aleksander Holynski, Ting-Chun
pages1161–1170,2019. 2,3,6,7,8
Wang,andIraKemelmacher-Shlizerman.Dreampose:Fash-
[9] HaoyeDong, XiaodanLiang, YixuanZhang, XujieZhang,
ion image-to-video synthesis via stable diffusion. 2023
XiaohuiShen,ZhenyuXie,BowenWu,andJianYin. Fash-
IEEE/CVF International Conference on Computer Vision
ioneditingwithadversarialparsinglearning.InProceedings
(ICCV),pages22623–22633,2023. 3
oftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages8120–8128,2020. 2 [23] Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park,
[10] YuyingGe,YibingSong,RuimaoZhang,ChongjianGe,Wei andJaegulChoo.Stableviton:Learningsemanticcorrespon-
Liu, and Ping Luo. Parser-free virtual try-on via distilling dence with latent diffusion model for virtual try-on. arXiv
appearance flows. In Proceedings of the IEEE/CVF con- preprintarXiv:2312.01725,2023. 2,3,7,8
ference on computer vision and pattern recognition, pages [24] DiederikPKingmaandMaxWelling. Auto-encodingvaria-
8485–8493,2021. 2,3,7,8 tionalbayes. arXivpreprintarXiv:1312.6114,2013. 4
10[25] GauravKuppa,AndrewJong,XinLiu,ZiweiLiu,andTeng- [37] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
ShengMoh.Shineon:Illuminatingdesignchoicesforpracti- and Surya Ganguli. Deep unsupervised learning using
calvideo-basedvirtualclothingtry-on.InProceedingsofthe nonequilibrium thermodynamics. In International confer-
IEEE/CVFWinterConferenceonApplicationsofComputer enceonmachinelearning,pages2256–2265.PMLR,2015.
Vision,pages191–200,2021. 2 4
[26] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan [38] JonathanTseng,RodrigoCastellon,andCKarenLiu. Edge:
Choi,andJaegulChoo. High-resolutionvirtualtry-onwith Editable dance generation from music. arXiv preprint
misalignment and occlusion-handled conditions. In Eu- arXiv:2211.10658,2022. 7
ropean Conference on Computer Vision, pages 204–219. [39] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin
Springer,2022. 2 Chen, Liang Lin, and Meng Yang. Toward characteristic-
[27] Mehdi Mirza and Simon Osindero. Conditional generative preservingimage-basedvirtualtry-onnetwork. InProceed-
adversarialnets. arXivpreprintarXiv:1411.1784,2014. 2 ingsoftheEuropeanconferenceoncomputervision(ECCV),
[28] DavideMorelli,MatteoFincato,MarcellaCornia,Federico pages589–604,2018. 2,3,8
Landi,FabioCesari,andRitaCucchiara. Dresscode: high- [40] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,
resolution multi-category virtual try-on. In Proceedings of ZhengyuanYang,HanwangZhang,ZichengLiu,andLijuan
theIEEE/CVFConferenceonComputerVisionandPattern Wang.Disco:Disentangledcontrolforrealistichumandance
Recognition,pages2231–2235,2022. 2 generation. 2023. 3
[29] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Mar-
[41] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,
cellaCornia,MarcoBertini,andRitaCucchiara. Ladi-vton:
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
Latent diffusion textual-inversion enhanced virtual try-on. videosynthesis. arXivpreprintarXiv:1808.06601,2018. 3
arXivpreprintarXiv:2305.13501,2023. 2,3
[42] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
[30] Haomiao Ni, Changhao Shi, Kaican Li, Sharon X. Huang,
moncelli. Imagequalityassessment: fromerrorvisibilityto
andMartinRenqiangMin. Conditionalimage-to-videogen-
structuralsimilarity.IEEEtransactionsonimageprocessing,
erationwithlatentflowdiffusionmodels. 2023IEEE/CVF
13(4):600–612,2004. 8
Conference on Computer Vision and Pattern Recognition
[43] Greg Welch, Gary Bishop, et al. An introduction to the
(CVPR),pages18444–18455,2023. 3
kalmanfilter. 1995. 2
[31] Dustin Podell, Zion English, Kyle Lacey, Andreas
[44] ZhongcongXu,JianfengZhang,JunHaoLiew,HanshuYan,
Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng
Robin Rombach. Sdxl: Improving latent diffusion mod-
Shou. Magicanimate: Temporallyconsistenthumanimage
els for high-resolution image synthesis. arXiv preprint
animation using diffusion model. ArXiv, abs/2311.16498,
arXiv:2307.01952,2023. 6
2023. 3,4
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[45] BinxinYang, Shuyang Gu, Bo Zhang, TingZhang, Xuejin
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
example:Exemplar-basedimageeditingwithdiffusionmod-
transferable visual models from natural language supervi-
els. InProceedingsoftheIEEE/CVFConferenceonCom-
sion.InInternationalconferenceonmachinelearning,pages
puterVisionandPatternRecognition, pages18381–18391,
8748–8763.PMLR,2021. 4
2023. 3
[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
[46] Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng,
Patrick Esser, and Bjo¨rn Ommer. High-resolution image
andJunHaoLiew. Magicavatar: Multimodalavatargenera-
synthesis with latent diffusion models. In Proceedings of
tionandanimation. ArXiv,abs/2308.14748,2023. 3
the IEEE/CVF conference on computer vision and pattern
recognition,pages10684–10695,2022. 2,3,6 [47] Lvmin Zhang. Reference-only controlnet. https://
github.com/Mikubill/sd-webui-controlnet/
[34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutionalnetworksforbiomedicalimagesegmen- discussions/1236,2023.5. 4
tation. InMedicalimagecomputingandcomputer-assisted [48] PengzeZhang,LingxiaoYang,Jian-HuangLai,andXiaohua
intervention–MICCAI2015: 18thinternationalconference, Xie. Exploringdual-taskcorrelationforposeguidedperson
Munich,Germany,October5-9,2015,proceedings,partIII image generation. In Proceedings of the IEEE/CVF Con-
18,pages234–241.Springer,2015. 4 ferenceonComputerVisionandPatternRecognition,pages
[35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, 7713–7722,2022. 3
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo [49] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, and Oliver Wang. The unreasonable effectiveness of
man,etal.Laion-5b:Anopenlarge-scaledatasetfortraining deepfeaturesasaperceptualmetric. InProceedingsofthe
nextgenerationimage-textmodels. AdvancesinNeuralIn- IEEE conference on computer vision and pattern recogni-
formationProcessingSystems,35:25278–25294,2022. 3 tion,pages586–595,2018. 8
[36] Sang-HeonShim,JiwooChung,andJae-PilHeo. Towards [50] JianZhaoandHuiZhang. Thin-platesplinemotionmodel
squeezing-averse virtual try-on via sequential deformation. forimageanimation. InProceedingsoftheIEEE/CVFCon-
InProceedingsoftheAAAIConferenceonArtificialIntelli- ferenceonComputerVisionandPatternRecognition,pages
gence,pages4856–4863,2024. 2 3657–3666,2022. 3
11[51] XiaojingZhong,ZhonghuaWu,TaizheTan,GuoshengLin,
andQingyaoWu. Mv-ton:Memory-basedvideovirtualtry-
onnetwork. InProceedingsofthe29thACMInternational
ConferenceonMultimedia,pages908–916,2021. 2,3
[52] LuyangZhu,DaweiYang,TylerZhu,FitsumReda,William
Chan, Chitwan Saharia, Mohammad Norouzi, and Ira
Kemelmacher-Shlizerman. Tryondiffusion: A tale of two
unets.InProceedingsoftheIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition, pages 4606–4615,
2023. 2,3
12