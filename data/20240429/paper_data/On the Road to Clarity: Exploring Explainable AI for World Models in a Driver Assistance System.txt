On the Road to Clarity: Exploring Explainable AI
for World Models in a Driver Assistance System
Mohamed Roshdi∗, Julian Petzold∗, Mostafa Wahby∗, Hussein Ebrahim∗, Mladen Berekovic∗, Heiko Hamann†
∗Institute of Computer Engineering
University of Lu¨beck, Lu¨beck, Germany
Email: petzold@iti.uni-luebeck.de
†Department of Computer and Information Science
University of Konstanz, Konstanz, Germany
Abstract—In Autonomous Driving (AD) transparency and can comprehend the decisions and predictions of the ML
safety are paramount, as mistakes are costly. However, neural system [1]. This led to the emergence of the Explainable
networks used in AD systems are generally considered black
Artificial Intelligence (XAI) field, which aims to provide
boxes. As a countermeasure, we have methods of explainable AI
means to help interpreting and explaining ML systems.
(XAI), such as feature relevance estimation and dimensionality
reduction. Coarse graining techniques can also help reduce In the context of autonomous driving, which is the main
dimensionality and find interpretable global patterns. A spe- focus of this work, XAI is of particular interest due to the
cific coarse graining method is Renormalization Groups from continuous increase of automation levels and the necessity
statistical physics. It has previously been applied to Restricted
to explain, at least retrospectively, the decisions made by
BoltzmannMachines(RBMs)tointerpretunsupervisedlearning.
large ANNs in dangerous situations. Here, we present an XAI
We refine this technique by building a transparent backbone
model for convolutional variational autoencoders (VAE) that approach that we showcase in an AD-specific application. We
allows mapping latent values to input features and has perfor- useXAItoexplainpreviouslytrainedANNmodelsthatpredict
mance comparable to trained black box VAEs. Moreover, we thebehaviorofvulnerableroadusers(VRUs;e.g.,pedestrians,
proposeacustomfeaturemapvisualizationtechniquetoanalyze
bicycles) [2] in urban traffic, as their safety is of highest
the internal convolutional layers in the VAE to explain internal
concern. These prediction models can be used to develop
causes of poor reconstruction that may lead to dangerous traffic
scenarios in AD applications. In a second key contribution, we an advanced driver assistance system (DAS), for example,
propose explanation and evaluation techniques for the internal as an early warning system that tries to anticipate dangerous
dynamicsandfeaturerelevanceofpredictionnetworks.Wetesta situations on second-timescale. The line of sight between a
longshort-termmemory(LSTM)networkinthecomputervision
human driver and pedestrians on the sidewalks can provide
domain to evaluate the predictability and in future applications
information on whether a pedestrian is planning, for example,
potentiallysafetyofpredictionmodels.Weshowcaseourmethods
byanalyzingaVAE-LSTMworldmodelthatpredictspedestrian to enter the road or perform any dangerous behavior [3].
perception in an urban traffic situation. Somepredictionmodelsexploitthesamevisualinformationof
observing close-by VRUs to anticipate dangerous situations.
I. INTRODUCTION
We collected data from the pedestrian perspective at road
Methods based on artificial intelligence (AI) have been
crossing scenarios in simulations to train ANN [4]. This work
shown to have increasing successes when applied to a vast
is based on a synthetic environment using the CARLA traffic
variety of application fields (e.g., healthcare, farming, au-
simulator [5]. We trained variational autoencoders (VAE) and
tonomous vehicles, etc.). Many symbolic AI approaches (e.g.,
long short-term memory (LSTM) networks that can be used
rule-based methods) can represent problems in an easily
to predict the positions and trajectories of VRUs in the near
interpretable and human-readable format. However, machine
future(e.g.,onesecond).Withtoday’stechnology,asystemon
learning (ML) techniques that have shown more promising
avehicleequippedwitha360-degreecameracouldreconstruct
performance,suchasartificialneuralnetworks(ANNs),follow
relevant features of the perspective of surrounding VRUs to
the subsymbolic paradigm. These techniques are considered
use them as input for prediction models. The feasibility and
‘black boxes’ that are difficult to interpret and explain.
efficiencyofthisapproachareincreasedifmultiplecarsshare
When designing an ML system, its interpretability and
their perception (e.g., vehicle-to-X approaches [6]).
explainability are essential factors because they influence the
In this paper, we present four methods and tools to explain
user’strustandtheirabilitytoimproveorre-adaptthesystem.
both the VAE and LSTM networks of the prediction model
A system is interpretable when represented in a human-
from [4] as shown in Figure 1. To explain the internal
understandableformatandisconsideredexplainableifhumans
functionality of the convolutional VAE, we develop a tool
Partially funded by the Federal Ministry of Education and Re- based on activation visualization that visualizes the feature
search (BMBF), projects ‘NEUPA’, grant 01IS19078, and ‘KI-IoT’, grant maps and principal components of the convolutional filters.
16ME0091K.
This provides a visual understanding of the evolution of
We thank Robert, Ellen, and Anita de Mello Koch for their insights and
advice,andforprovidingtheirsourcecode. features through the convolutional layers and filters, enabling
© © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any
current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new
collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other
works.
4202
rpA
62
]GL.sc[
1v05371.4042:viXrathe user of the tool to understand the internal functionality ofmachinebehavior[18].Arelevantapproach[4]usesaction
of the ConvVAE layers and its generated features. Second, andcameradatafromtheperspectiveofapedestriantopredict
we develop a visualization of the learned data manifold. We what the pedestrian will see in future time steps.
visualizetherangeoffeaturesencodedbythelatentvectorina b) Model-Specific XAI Approaches: Numerous ap-
gridasdoneby[7]andutilizedinaninteractivetoolby[8]to proacheshavefocusedondevelopingexplainabilitytechniques
explain the mapping between the image features and encoded designed for particular Deep Learning (DL) architectures.
latentspacevalues.AsthechallengeoffullyexplainingVAEs Karpathy et al. [19] analyzed the functionality of LSTM
is vast, we present an alternative approach based on Renor- memory cells in language models, detecting functionalities,
malization Groups (RG) [9] from statistical physics as a third suchasmaintainingthestateoflong-termdependencies,while
method. It transforms the model to an inherently transparent Bach et al. [20] proposed Layer-wise Relevance Propagation
and interpretable model. In a fourth approach, we explain toassignrelevancescorestoinputfeaturesbypropagatingthe
LSTMs by correlating the memory cells’ hidden states with model’s outputs backwards using redistribution rules.
input features and implementing a custom feature relevance To interpret convolutional neural networks (CNNs), [21]
technique to assign relevance scores for input features. In visualized a network’s learned features by optimizing random
an application-focused effort, we test how the LSTM reacts images to maximize the activations of convolutional filters,
to input frames with domain-specific varying features (e.g., entire layers, or individual channels.
increasingnumberofpedestrians).Weempiricallyevaluatethe Selvarajuetal.[22]introducedGradCAM,agradient-based
interpretability by comparing LSTM relevance heat maps to technique that generates a localization map highlighting the
humanvisualattentionmaps.OuranalysisyieldsameanNor- relevant areas of an image that influence a classification.
malizedScanpathSaliency(NSS)[10]of0.53,comparingour Lie et al. [23] extend this gradient-based visualization from
heat maps to ground truth human visual attention, and reveals classification networks to generative models like the VAE.
abnormal prediction behavior with potential implications for Muhammad and Yeasin [24] augmented CAM methods by
traffic safety. Our explanation and evaluation methods offer visualizing the principal components of feature maps. Cetin
a basis for assessing the explainability and predictability of et al. [25] introduce the Attri-VAE, a VAE based on the
pedestrian perception prediction models that can be applied β-VAE [26]. By introducing an attribute regularization term
on different architectures. to their loss function, the authors disentangle latent space
variables, compelling them to align with predefined image
II. RELATEDWORK
attributes for enhanced interpretability.
To provide explainability for black box models, XAI tech- c) DL Interpretation through Renormalization Groups:
niques rely on interpretations, a mapping from an abstract In statistical physics, Renormalization Groups (RGs) are used
domain into a human-understandable domain [11]. [12] pro- totransformcomplexsystemswithahighorderofparameters
vide a taxonomy of XAI methodologies based on the insights into simpler ones with a smaller set of parameters that can
theyprovide.ModelSimplificationtechniquesaimtocompare describe the general behavior of the system [27]. Metha and
complexandsimplifiedmodelstogaininsights.DeepNetwork Schwab[28]experimentallydemonstratedamappingbetween
Representation techniques aim to interpret the representation RGandRestrictedBoltzmannMachines(RBMs).Theauthors
of data in the model. Deep Network Processing techniques used this mapping to coarse-grain an Ising model, a 2D
provide insights on why certain inputs lead to their observed grid representing the spins of a magnet. Koch et al. [29]
outputs. In this paper, we cover all three of these techniques. explored this connection further by comparing stacked RBMs
a) XAI in Autonomous Driving: Some advantages of to a flow of RG operations, where each layer performs a
XAI, such as accessibility, confidence, or fairness [12], are step of RG-like coarse graining. The authors also compared
beneficial for almost all uses of artificial intelligence, but downsamplingbyconvolutionalpoolinglayerstothereduction
certain application areas, such as autonomous driving (AD), indimensionalityperformedbyRG.Kochetal.[9]presentan
are under more scrutiny [13]–[16]. An error in an AD system RG-inspired interpretation of unsupervised learning of RBMs
may cause high costs. AD system failure modes may be and generative models. They compare between the weight
difficult to comprehend for humans, which reduces trust. For matrix of an RBM trained on Ising data and an RG using
example, in a fatal accident [17], an autonomous car did not Singular Value Decomposition (SVD) [30]. Koch et al. [9]
recognize a pedestrian pushing a bicycle. To increase safety notedthatthesingularvectorswiththehighestsingularvalues
and to alleviate trust issues, AD models require insights in of an SVD of image datasets have their support in low
causality to build confidence in their mechanisms. Hence, we frequencies, with little information encoded in high frequen-
focus on explaining a model used for AD. In the domain of cies. This phenomenon is similar to Momentum Space RG,
AD, vehicle-to-X (V2X) [6] allow cars to communicate with another type of RG that eliminates high momentum modes
traffic signs, other cars, or even pedestrians. Data obtained representedashighfrequencies.Kochetal.[9]introducedthe
this way enables new approaches, such as perceiving traffic RGMachine(RGM)thatusessingularvectorsofatrainingset
participants hidden from the car’s line of sight, but visible to estimate weights and biases of an RBM. The RGM relies
by the camera of a traffic light. V2X approaches might also on low frequency modes of singular vectors, produces images
pose different challenges for XAI, for example, in the domain similar to those of the RBM, and improves its performancewithin a few epochs. Hence, the authors interpret the RBM v in the same convolutional layer in VAE that minimized
1
learningprocessasaprocesssimilartomomentumspaceRG, r(u,v).Thisenablesustocomparefiltersperformingthesame
keepingrelevantfeaturesrepresentedbylowfrequencymodes functionality across both VAEs.
of singular vectors. b) Latent Space Interpretation: Besides our convolu-
tionalfeaturemapsvisualizationtoolthatexplainstheinternal
III. METHODS
functionality of the convolutional VAE, here, we present
In this section, we present the interpretability methods that a complementary approach to explain the mapping between
we applied to the Convolutional VAE and the LSTM of image features and latent vector values (Deep Network Pro-
the pedestrian perception prediction approach in traffic situ- cessing [12]). Inspired by [8] and [7], we observe how sys-
ations [4]. Our goal is to create an explainable AI framework tematic changes to latent values influence the visual features
that provides human-understandable explanations of the inter- of the decoded latent vectors. Such mapping between latent
nal processing within the models and the mapping between vector changes and decoded visual features will later help us
their inputs and outputs. to explain the LSTMs (see Section III-0d). To approach this,
a) Feature Maps Visualization Tool: We developed a we design an experiment where we systematically manipulate
tool that visually explains the internal functionality of the the 50D latent vector values of 30 encoded traffic scenario
ConvVAE model presented in [4]. The model encodes pedes- frames, extracted from the pedestrian scenario mentioned in
trianperceptionintoanabstract50Dlatentvectorz.Thisway, Section I. Our initial experiments indicate that manipulating
the perception is compressed and can be used to predict the thevaluesatoneorafewpositionsofthelatentvectorresults
next frame with an LSTM model, a type of recurrent neural inminutechangesatthedecodedframes.Therefore,wedivide
network for time series prediction [31]. The input frames are each latent vector into five equally sized regions of size ten.
represented in 24 channels corresponding to the 24 semantic Wearrivedatthisdivisionthroughqualitativelyexperimenting
classes of a custom CARLA environment. The VAE consists several division setting and choosing the most interpretable
of four convolutional layers that sequentially extract lower- one.Wetesttheinfluenceofiterativelyinterpolatingthevalues
dimensional features from the input image. We introduce a of a region by increments of {1,2,3}. After analyzing the
Deep Network Representation [12] tool that interprets the influence of one region, we restore the values of the original
functionality of the convolutional filters and their role in vector, then we switch to the next region. This results in three
the feature extraction process in two phases. Our feature different analyses per region and 15 frame analyses in total.
visualization tool feeds an image to the first convolutional Finally, the manipulated latent vectors are decoded back to
layer and saves the output feature maps at each layer. We 45×85 pixel images that are stored in a 3×5 grid that we
apply Min-Max normalization [32] to the feature maps and call the latent grid. We apply the latent grid visualization to
use them as weighted masks that act on the input image to VAE introduced in Section III-0a.
1
map the grey-scale features to the RGB features, producing c) RG-inspired Interpretable Autoencoder Architecture:
more legible feature maps. Next, the tool uses the feature MostNNsareinherentlynon-transparent,meaninginterpreting
maps at each layer to compute the SVD [30] of the images. their functionality requires external analysis [12]. So far, we
This allows us to visualize the top singular vectors v of the have presented two external analysis tools that visualize the
feature maps representing the principal components and gain internal functionality of convolutional layers and examine the
a visual description of the layer’s functionality, similar to mapping between latent encoding and image features. How-
EigenCAM [24] but in a regression rather than classification ever, those techniques provide a limited interpretation of the
task.Weconductedanexperimenttoevaluatetheeffectiveness mappingbetweenthelatentandfeaturespaceoftheConvVAE,
of our approach in producing interpretable visualizations and a common issue in interpreting black-box models [33].
generating insightful outputs at each layer. The experiment Here, we present a different approach, where instead of
utilized pedestrian perception frames from a traffic scenario explaining such a non-transparent network, we reconstruct
as input images for two different VAE networks. The first it into a model that is designed to be transparent and in-
network, VAE , was trained on a dataset of approximately terpretable by nature that we can use as a backbone for an
1
805k frames extracted from a fixed pedestrian route in the interpretable VAE architecture. First, we apply the SVD on a
traffic scenario. The second network, VAE , was an enhanced sample of the training data to extract interpretable singular
2
version trained on a larger dataset of approximately 4m vectors U that represent the most relevant features. Each
frames, extracted from different pedestrian routes covering a singular vector corresponds to a principal component with
wider range of pedestrian perspectives. Our objective was to the same dimensionality as the dataset frames. Consequently
determine if our tool could justify the improved performance theycanbevisualized.Weusethe2DFastFourierTransform
ofVAE .Toevaluatethefunctionalityofthetwonetworks,we (FFT) on the singular vectors U, filter out higher frequency
2
used the correlation distance metric r(u,v)=1−(u−u¯)·(v−v¯) modes with a low pass filter, which is equivalent to applying
||u||2 ||v||2
to group between filters u and v in the same layers similar an RG transformation [9] to remove irrelevant information,
filters of VAE and VAE and identify differences in extracted then apply the Inverse 2D Fourier transform to return to the
1 2
features between them. For each filter u in a convolutional singular vector space. The resulting filtered singular vectors
layer L ∈ {1,2,3,4} in VAE , we paired it with the filter matrix α is used to encode an input image into a latent
2Pedestrian Perception Prediction Explanation Evaluation
Feature Map Pipeline Feature Map
Comparison
Input x t Latent z t SVD VAE 1 VAE 2
Conv.
Encoder Actions a t Feature Maps Top Singular Vector
Latent Manipulation
Latent
Manipulation
States

h
c
LSTM
Weights
 Memory Cell Analysis Latent Grid
ω Driver
Attention

Heuristic Filters Comparison
Latent z
t+1 Layer-wise Relevance Propagation Cell
Output x
t+1 Interpretation
Deconv.
 Distance

LRP Perturbation
 Metric
Decoder Masking
Interpretability

Relevance Scores R Relevance Heatmap Score
t
Fig. 1: An overview of our XAI system and its components.
vector z by projecting it onto a lower dimensional vector pedestrian perception prediction model from [4], we present
space with the basis of singular vectors [30]. The values of z two approaches for calculating the relevance of input features
indicate the degree of similarity between the input image and and for interpreting the functionality of LSTM memory cells
the singular vectors, where higher values indicate a stronger in a street-crossing scenario, respectively. In the crossing
presence of features represented by the singular vectors in scenario, a pedestrian approaches a zebra crossing, uses it to
the input image. Similarly, z can be decoded back to an crossthestreet,andthencontinuesontheirway.Thescenario
image using the transpose of the singular vector matrix αT is executed in CARLA [5] and the pedestrian’s semantically
via z =αTx, xˆ=αz. segmented vision and action commands are recorded. At each
The hyper-parameters of the pipeline are the number of time step, the LSTM uses a pedestrian action a and latent
t
singularvectorsusedinthematrixαthatultimatelydetermine vector z to predict the next perception frame z , which
t t+1
the size of the latent vector z–since the input image is is passed back to the LSTM in a feedback loop to predict
projected onto every single vector–and the cutoff frequency again. The first approach is based on an input of 400 frames
of the low pass filter. Since each singular vector can be of the pedestrian’s vision and storing the hidden states of
visualized as a 45 × 85 frame, the features that z encodes the LSTM’s 512 memory cells at each frame prediction. We
can now be mapped to a human understandable domain. We use a sigmoid function to normalize the hidden state values
test the pipeline using a subset of the training dataset of between zero and one, where values closer to one indicate
VAE consisting of 16,500 pedestrian perception frames. We that the cell is triggered, that is, it passes its contents to the
1
use the Kullback-Leibler (KL) divergence [34] to evaluate the output. To visualize the activity of a hidden state with respect
pipeline’s performance at a range of hyperparameter settings to the predicted frame, we plot the hidden state on the y-axis
and to compare against the baseline VAE. We showcase the and the index of the generated frame on the x-axis in a 2D
pipeline’s interpretability by visualizing singular vectors of α plot. We aim to identify cells with interpretable behavior (i.e.,
demonstrating abstract features encoded by z. correlating the cells’ hidden values and the features of the
d) Interpreting LSTM Dynamics and Feature Relevance: pedestrian perception) using two heuristic filters.
The LSTM architecture’s memory cell is a crucial component
that stores state information from previous prediction steps. Wequalitativelyevaluatetheinterpretabilityofthetopcells
At each step, the memory cell decides whether to forget its identified by the two filters based on a human user’s ability
current state and replace it with new input or maintain the to correlate the cell’s hidden state behavior with the events
current state [31]. Hidden state values h of the memory cells in the traffic scene and the actions of the ego-pedestrian.
c
determine whether to affect the network’s output at each step. Our investigations indicate that cells with noisy hidden state
Thememorycell’sdecisionsareregulatedbygates,whichare behavior tend to be uninterpretable, while interpretable cells
simplemultiplicativeunitsthatpassthroughatanhorsigmoid tend to be active only at certain intervals.
activation function [31]. The LSTM’s design enables it to
store important information from prior states while protecting Wedefinethefirstfilterκthatreturnscellsthatminimizethe
them from irrelevant inputs or noise [31]. To explain both KL divergence between their hidden state values and a square
the internal functionality and the feature relevance of the pulse activated at a particular range when a certain eventoccurs, such as when the pedestrian passes the crosswalk: unable to effectively reconstruct the input, since the decoded
image is noisy with missing features, such as the car and the
κ=argmin(KL(h ||θ(t−r )−θ(t−r ))), (1)
c 1 2 crosswalk. In the bottom of Figure 2, each column represents
c
the two pairs of feature maps belonging to filters in the same
where KL(·||·) is the KL divergence, r and r are the start
1 2 layer in the two VAEs. In the first layer, VAE better captures
and end points of the interval I under investigation, c is 2
the skyline feature as indicated by its brighter color.
the index of a memory cell, h is the hidden state values
c In the visualization of the second layer and third convolu-
of cell c, and θ is the Heaviside step function. The LSTM
tional layers, we observe similar trends, with VAE poorly
takes in the ego-pedestrian actions a=[a ,a ,a ], where a 1
0 1 2 0 extracting the features of the crosswalk and skyline com-
indicates movement (0 for stopped, 1 for in motion), a is the
1 pared to VAE . These visualizations indicate that the filter of
pedestrian’s body angle parallel to the ground, and a is the 2
2 VAE have not learned to properly extract the skyline and
head’srotationparalleltotheground.Weintroducethesecond 1
crosswalk. We exclude the 2×2 fourth layer’s feature map
filter µ that identifies cells tracking action changes:
comparison and singular vectors, as their low dimensionality
µ=argmin(S (∇h ,∇a)), (2) lacks interpretability and hinders post-hoc interpretability in
c c
c networks with chained convolutional layers. The singular
whereS cisthecosinesimilarity,∇h cisthegradientofhidden vectorsprovideamethodforuserstoseetheeffectofinternal
values h c, and ∇a is the gradient of the action a. data transformations at each convolutional layer, especially in
In our second approach, we propose an augmented Layer- earlier layers with relatively larger filter dimensions. The first
wise Relevance Propagation [20] technique to visualize the layer singular vectors of VAE feature high activation values
2
relevance of the input latent features and map them to the (marked by brighter colors) in patches corresponding to the
RGB space. After the LSTM model predicts the future latent crosswalk and skyline. Overall, the two-phase feature visual-
vector,weassignarelevancevalueof1forallvaluesinz t+1. ization pipeline has qualitatively demonstrated its ability to
Then we redistribute the relevance for lower layer neurons visualize internal data processes of the ConvVAE, visualizing
throughtwopropagationrules.Thetworuleshandleweighted internalcausesofpoorreconstructionbytheConvVAE,which
connections and multiplicative connections such as in the cell could lead to dangerous traffic situations.
state(see[35]fortheformulasofourchosenrules).Similarto b) LatentSpaceInterpretation: Wepresentherethe5×3
the latent space interpretation (see Section III-0b) experiment, latent grids of VAE for one example of the 30 input latent
1
in order to map the relevance scores of the input latent space vectors we manipulate, shown in Figure 3. Each row in the
to the RGB space, we perturb each latent value and assign grid,correspondingtoadifferentlatentvectorsliceofsizeten,
its relevance score to the most affected pixel regions in the appears to control a set of semantic features of the decoded
decoded image. We use the image relevance scores as a mask images. For example, the fourth row z features the addition
4
to visualize the most relevant visual features of the input of a pedestrian, while the first row in features the addition
image. To evaluate the LRP explanation and the differences of a car. By observing the different rows of the latent grid
between the decision making process of the LSTM and other for a given image, a user can visualize a mapping between
human or machine models, we compare the resultant masks semantic features and their latent representation. The latent
to human driver attention maps generated by a pretrained grid tool will be crucial in analyzing responses of the LSTM,
model proposed by [10] predicting the visual attention of sincethelatentvectorisusedbytheLSTMtopredictthenext
humandrivers.AsmetricsweusethemeanNSSandPearson’s pedestrian perception frame.
Coeffecient[10]overthewholescenario.AsinputtotheLRP c) RG-inspired Interpretable Autoencoder Architecture:
experiment we use the pedestrian crossing scenario shown For the SVD-based pipeline, we sample the data from the
in Table I and the latent grids produced in Section III-0b. pedestrian vision dataset by [4] to provide an input space of
We test the LSTM’s response to varying input features and semanticallysegmentedimages.Wetesttwocutofffrequencies
unforeseen scenarios that may lead to traffic hazards. set at 150 and 175 frequency modes. We compute the SVD
using Tensorflow 2.9.1 on three Nvidia A100 GPUs. We rely
IV. RESULTS
on the implementation of [9] for the low pass filter, but
Wepresenttheresultsofouradaptationoftheconvolutional refactor it using Tensorflow. We compare the performance
feature map visualization [21] and latent space visualiza- of SVD architectures to the baseline VAE model using
1
tion [7], [8] to implement both Deep Network Representa- the KL Divergence reconstruction error of the test dataset
tion and Deep Network Processing [12]. We also show our of the VAE model trained by [4]. The VAE model, the
1 1
novel explanation by simplification [12] method to produce SVD autoencoder with cutoff of 175 and 150 have a mean
a transparent VAE, and our LSTM explainability framework reconstruction error of 0.024, 0.179, and 0.521, respectively.
combining memory cell analysis with latent space visualiza- In terms of interpretability, we can visualize and interpret
tion. features encoded by the latent vector as each latent value
a) ConvolutionalVAEFeatureVisualization: Wepresent represents the strength of a principal component (the singular
the visualizations of the feature map comparison discussed in vector).Forexample,thetopthreesingularvectorsofthe175
Section III-0a. Figure 2 shows that unlike VAE , VAE was frequency mode SVD in Figure 4 show different perspectives
2 1Original Input z
Input VAE VAE z+1 z+2 z+3
1 2 r r r
Reconstruction Reconstruction
z=z[0:9]
1
VAE Conv. Layer 1 VAE v Layer 1
1 1 0
z=z[10:19]
2
z=z[20:29]
VAE Conv. Layer 1 VAE v Layer 1 3
2 2 0
z=z[30:39]
4
z=z[40:49]
VAE Conv. Layer 2 VAE v Layer 2 5
1 1 0
Fig. 3: Latent grids for z (top). The rows represent regions of
size ten in the latent vector. Each entry is the decoded vector
VAE Conv. Layer 2 VAE v Layer 2 taken after increasing the values of the region by {1,2,3}.
2 2 0
S=6526 S=2680
VAE Conv. Layer 3 VAE v Layer 3
1 1 0
VAE Conv. Layer 3 VAE v Layer 3
2 2 0
Frequency Mode Frequency Mode
Fig.2:VisualizationoftheRGBfeaturemapsandtopsingular Fig. 4: Singular vectors of α with a cutoff frequency of
vector v for layers 1-3 for VAE and VAE . For each layer, 175 with the two highest singular values S visualized (top).
0 1 2
each column represents the VAEs’ most similar feature maps. We show the frequency domains of the first (bottom left)
and twelfth singular vector (bottom right). α[0] shows high
support in only the lowest frequency, while α[11] has support
oftheego-pedestrian.Thefirstthreevaluesofthelatentvector throughout the spectrum, in agreement with [9].
numerically correspond to each of these visualizable singular
vectors, forming a transparent model [33].
d) Interpreting LSTM Dynamics and Feature Relevance: sensitivetothethreeactionvalues,nointerpretablecellswere
We do two LSTM memory cell tests. First we analyze the found to react to differences in the first two action values,
LSTM memory cells as the VAE-LSTM model predicts the corresponding to the flag of the ego-pedestrian movement and
perception of an ego-pedestrian crossing the street. Table I theangleofthebodyoftheego-pedestrian.However,cell100
shows the three actions of the pedestrian and a textual (Figure 5-c) was found to react to changes in the third action
description of the scene. We present a sample of memory value that represents the angle of head of the ego-pedestrian.
cells with interpretable hidden state behavior, detected using Cell 100 has high hidden cell values coinciding with high
filters κ and µ. Of the 512 cells, only 82 cells were found normalized values of the head angle, when the pedestrians
to meet the qualitative interpretation standard described in looks sideways toward the street and passing cars.
Section III-0d. Cell 134 (Figure 5-a) is active at range 80 In our second LSTM approach, we visualize and evaluate
to159,correspondingtotheintervalwheretheego-pedestrian the input relevance heat maps using LRP augmented with
stops walking while changing the angle of the head towards perturbationmaskingforthepedestrianscenarioframesofTa-
the street. Cell 134 was detected through the filter κ using a ble I and the latent grids as the example shown in Figure 3.
Heavisidesquarepulsebetween80and159.Theinterpretation The empirical evaluation of the comparison between the LRP
of this behavior is that cell 134 keeps track of the periods heatmaps and driver attention maps resulted in a mean NSS
in the sequence when the pedestrian performs the ‘stop and score of 0.53 and a Pearson Coeffecient of 0.46 for the
scan’ behavior before crossing the street, effectively acting pedestrian scenario of Table I. Using the latent grid as input
as a movement detector. As for the filter µ that detects cells to the prediction model, we were able to detect unpredictable
edutilpmA
edutilpmAFrames a0 a1 a2 Scenedescription
0-80 1 180 0 Walk(Sidewalk),LookTo(Sidewalk)
81-118 0 [180-270] [0,-48.63] Turn(Right),LookTo(Street)
119-135 0 270 [-48.63,-0.66] Turn(Head,Right),LookTo(Crosswalk)
136-158 0 270 -0.66 LookTo(Crosswalk)
159-233 1 270 [-0.66-85.54] Walk(Crosswalk),LookTo(Street)
234-337 1 270 [85.54-0.24] Walk(Crosswalk),LookTo(Crosswalk)
338-377 0 [270-180] 0.24 Turn(Body,Left),LookTo(Sidewalk)
378-399 1 180 0.24 Walk(Sidewalk),LookTo(Sidewalk)
TABLE I: A pedestrian scenario description matching time frame ranges with pedestrian perception.
1
V. DISCUSSION
0.8 Our four methods apply several XAI techniques to
ConvVAEs and LSTMs. The use case we address is a vision
0.6
modelthatgivesdriversrecommendationsbasedonpedestrian
0.4
perception. We can use our feature map visualization tool
0.2
to investigate internal deficiencies that cause poor model
0
performance. The latent grid maps between latent space and
0 100 200 300 400 0 100 200 300 400
FrameNumber FrameNumber feature space help to visualize the effect of changing different
Fig. 5: Cell 134 (Left) and Cell 100 (Right) show the hidden input features on the LSTM prediction in Section IV-0d.
state on the y-axis in blue relative to the pedestrian vision A drawback of our latent grid approach is its high granu-
frames. The pink signal is movement action a , while the larity, as it does not interpret each latent vector value. Our
0
cream signal is head angle action a . prediction model explanation provides a baseline to examine
2
the internal functionality and safety of the LSTM predictions
priortodeploymentinaDAS.BoththememorycellandLRP
analysis can be applied to other prediction architectures such
behavior of the LSTM. In the next frame prediction of z +2
1
as Transformers [36] by analyzing the attention head activity
and z + 3 in Figure 6, the LSTM changes the car in the
1
and designing propagation rules for the different Transformer
input frame to a cyclist in the output frame, which may
layers. By creating scenarios with rare or dangerous features,
cause dangerous effectsin a DAS relying onfuture pedestrian
wecananalyzepredictionmodels’dynamicsandtherelevance
perception. The LRP heatmaps for this scenario indicate that
of critical features. However, a drawback of our evaluation
thelatentmanipulationofthatregiontriggeredtheresponseof
is the use of the driver attention model trained to simulate
theLSTMevenbeforethecarbecomesvisibleinframez +1.
1
drivers’ rather than pedestrians’ point of view. Finally, we
proposed a backbone for an interpretable VAE architecture.
zt
1 Inspired by the RGM [9], the ultimate goal is to fine-tune the
SVD pipeline as an interpretable backbone of a VAE. The
singular vectors of the reconstruction matrix α can be used
without post-hoc explanation to provide a mapping between
z 1t+1 feature and latent space. However, calculating the SVD for
large datasets seems computationally infeasible. Our use of
SVD to encode and decode images is a linear Principal
ComponentAnalysis(PCA),whichissensitivetooutliersand
LRP Relevance corrupted data [30].
VI. CONCLUSIONANDFUTUREWORK
Weintroducedanend-to-endXAIframeworkforexplaining
Driver Attention the internal functionality and feature-output mapping of DL
models in a perception prediction network for autonomous
driving [4]. We uncovered the internal feature extraction pro-
cessoftheConvolutionalVAEwithconvolutionalfeaturemap
visualization and adopted a Feature Relevance explanation
Fig.6:LSTMpredictionsinsecondforlatentgridz ,withthe
1 approach relating the feature space with the latent space
LRPheatmapandhumanattentionmapinthethirdandfourth
through the latent grid analysis. Using the latent grids as
rows, respectively. The LSTM generates a cyclist instead of
inputs helped us to detect significant prediction errors by the
car, with the LRP detecting the presence of the car’s region.
LSTM that may affect the safety of VRUs, with our LRP
explanation providing explanations of the relevant features
eulaVetatSneddiHimpactingpredictions.Wehavedetectedasignificantspaceof [15] E´.Zablocki,H.Ben-Younes,P.Pe´rez,andM.Cord,“Explainabilityof
interpretable LSTM memory cells and proposed methods to deepvision-basedautonomousdrivingsystems:Reviewandchallenges,”
InternationalJournalofComputerVision,pp.1–28,2022.
infer their functionality, such as keeping track of pedestrian
[16] J. Dong, S. Chen, S. Zong, T. Chen, and S. Labi, “Image transformer
movement, the presence of cars, and street crossing. Our forexplainableautonomousdrivingsystem,”in2021IEEEInternational
empirical comparison of the LRP and driver attention maps IntelligentTransportationSystemsConference(ITSC). IEEE,2021,pp.
2732–2737.
presents an interpretation baseline for pedestrian prediction
[17] N.T.S.Board,“Collisionbetweenvehiclecontrolledbydevelopmental
modelsandcanbeimprovedbytrainingapedestrianattention automated driving system and pedestrian in tempe, arizona, on march
model to compare to the prediction model feature relevance. 18,2018,”2019.
[18] I. Rahwan, M. Cebrian, N. Obradovich et al., “Machine behaviour,”
Our XAI methods may be beneficial in evaluating the trans-
Nature,vol.568,p.477–486,2019.
parency and trust-worthiness of state-of-the-art DL models in [19] A.Karpathy,J.Johnson,andL.Fei-Fei,“Visualizingandunderstanding
AD,enablingtheircertificationinthefuture.Ourautoencoder recurrent networks,” 2015. [Online]. Available: https://arxiv.org/abs/
1506.02078
architecture with an inherently interpretable latent space is
[20] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Mu¨ller, and
based on the relationship between autoencoders, PCA, and W. Samek, “On pixel-wise explanations for non-linear classifier deci-
RG[9].Forfuturework,weplantointegratetheSVDpipeline sionsbylayer-wiserelevancepropagation,”PLOSONE,vol.10,no.7,
pp.1–46,072015.
intoaVAEandcombineconvolutionallayerswiththeSVDto
[21] C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualization,”
create hybrid VAE models with a basis of interpretability. We Distill,2017,https://distill.pub/2017/feature-visualization.
plan to use incremental and randomized SVD [30] to increase [22] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
dataset size and Robust PCA [30] to enhance generalization
gradient-basedlocalization,”in2017IEEEInternationalConferenceon
and noise handling. ComputerVision(ICCV),2017,pp.618–626.
[23] W.Liu,R.Li,M.Zheng,S.Karanam,Z.Wu,B.Bhanu,R.J.Radke,
andO.Camps,“Towardsvisuallyexplainingvariationalautoencoders,”
REFERENCES
in Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,2020,pp.8642–8651.
[1] A.Erasmus,T.D.P.Brunet,andE.Fisher,“Whatisinterpretability?” [24] M. B. Muhammad and M. Yeasin, “Eigen-cam: Class activation map
Philosophy&Technology,vol.34,no.4,pp.833–862,Dec2021. usingprincipalcomponents,”in2020InternationalJointConferenceon
[2] G. Yannis, D. Nikolaou, A. Laiou, Y. A. Stu¨rmer, I. Buttler, and NeuralNetworks(IJCNN). IEEE,Jul.2020.
D. Jankowska-Karpa, “Vulnerable road users: Cross-cultural perspec- [25] I.Cetin,M.Stephens,O.Camara,andM.A.G.Ballester,“Attri-VAE:
tivesonperformanceandattitudes,”IATSSresearch,vol.44,no.3,pp. Attribute-based interpretable representations of medical images with
220–229,2020. variationalautoencoders,”ComputerizedMedicalImagingandGraphics,
[3] L.Le´veˆque,M.Ranchet,J.Deniel,J.-C.Bornard,andT.Bellet,“Where vol.104,p.102158,2023.
dopedestrianslookwhencrossing?astateoftheartoftheeye-tracking [26] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,
studies,”IEEEAccess,vol.8,pp.164833–164843,2020. S.Mohamed,andA.Lerchner,“beta-vae:Learningbasicvisualconcepts
[4] J. Petzold, M. Wahby, F. Stark, U. Behrje, and H. Hamann, ““if you with a constrained variational framework,” in International conference
could see me through my eyes”: Predicting pedestrian perception,” in onlearningrepresentations,2017.
8th Int. Conf. on Control, Automation & Robotics (ICCAR), 2022, pp. [27] S. Iso, S. Shiba, and S. Yokoo, “Scale-invariant feature extraction of
184–190. neuralnetworkandrenormalizationgroupflow,”Phys.Rev.E,vol.97,
[5] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, p.053304,May2018.
“CARLA:Anopenurbandrivingsimulator,”inProceedingsofthe1st [28] P.MehtaandD.J.Schwab,“Anexactmappingbetweenthevariational
AnnualConferenceonRobotLearning,2017,pp.1–16. renormalization group and deep learning,” ArXiv, vol. abs/1410.3831,
[6] Y.Li,D.Ma,Z.An,Z.Wang,Y.Zhong,S.Chen,andC.Feng,“V2X- 2014.
Sim: Multi-agent collaborative perception dataset and benchmark for [29] E.DeMelloKoch,R.DeMelloKoch,andL.Cheng,“Isdeeplearninga
autonomous driving,” IEEE Robotics and Automation Letters, vol. 7, renormalizationgroupflow?”IEEEAccess,vol.8,pp.106487–106505,
no.4,pp.10914–10921,2022. 2020.
[7] D. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in [30] S. L. Brunton and J. N. Kutz, Data-Driven Science and Engineering:
2ndInt.Conf.eonLearningRepresentations,ICLR,2014. Machine Learning, Dynamical Systems, and Control. Cambridge
[8] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy UniversityPress,2019.
evolution,” in Proceedings of the 32nd International Conference on [31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Neural Information Processing Systems (NIPS 2018). USA: Curran Comput.,vol.9,no.8,p.1735–1780,nov1997.
AssociatesInc.,2018,pp.2455–2467. [32] M. M. Ahsan, M. A. P. Mahmud, P. K. Saha, K. D. Gupta, and
[9] A.DeMelloKoch,E.DeMelloKoch,andR.DeMelloKoch,“Why Z. Siddique, “Effect of data scaling methods on machine learning
unsupervised deep networks generalize,” vol. abs/2012.03531, 2020. algorithmsandmodelperformance,”Technologies,2021.
[Online].Available:https://arxiv.org/abs/2012.03531 [33] C.Rudin,“Stopexplainingblackboxmachinelearningmodelsforhigh
[10] Y. Xia, D. Zhang, J. Kim, K. Nakayama, K. Zipser, and D. Whitney, stakesdecisionsanduseinterpretablemodelsinstead,”NatureMachine
PredictingDriverAttentioninCriticalSituations,052019,pp.658–674. Intelligence,vol.1,no.5,pp.206–215,May2019.
[11] G. Montavon, W. Samek, and K.-R. Mu¨ller, “Methods for interpreting [34] S. Kullback and R. A. Leibler, “On information and sufficiency,” The
and understanding deep neural networks,” Digital Signal Processing, annalsofmathematicalstatistics,vol.22,no.1,pp.79–86,1951.
vol.73,pp.1–15,2018. [35] L. Arras, G. Montavon, K.-R. Mu¨ller, and W. Samek, “Explaining
[12] A. Barredo Arrieta, N. D´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot, recurrentneuralnetworkpredictionsinsentimentanalysis,”inProceed-
S.Tabik,A.Barbado,S.Garcia,S.Gil-Lopez,D.Molina,R.Benjamins, ingsofthe8thWorkshoponComputationalApproachestoSubjectivity,
R. Chatila, and F. Herrera, “Explainable artificial intelligence (XAI): SentimentandSocialMediaAnalysis,2017,pp.159–168.
Concepts,taxonomies,opportunitiesandchallengestowardresponsible [36] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
ai,”InformationFusion,vol.58,pp.82–115,2020. L.u.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inAdvances
[13] S. Atakishiyev, M. Salameh, H. Yao, and R. Goebel, “Towards in Neural Information Processing Systems, I. Guyon, U. V. Luxburg,
safe, explainable, and regulated autonomous driving,” arXiv preprint S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
arXiv:2111.10518,2021. Eds.,vol.30. CurranAssociates,Inc.,2017.
[14] D. Omeiza, H. Webb, M. Jirotka, and L. Kunze, “Explanations in
autonomousdriving:Asurvey,”IEEETransactionsonIntelligentTrans-
portationSystems,2021.