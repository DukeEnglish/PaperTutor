Asymptotics of Stochastic Gradient Descent with
Dropout Regularization in Linear Models
Jiaqi Li∗1, Johannes Schmidt-Hieber2, and Wei Biao Wu1
1Department of Statistics, University of Chicago
2Department of Applied Mathematics, University of Twente
September 12, 2024
Abstract
This paper proposes an asymptotic theory for online inference of the stochastic gradient descent
(SGD) iterates with dropout regularization in linear regression. Specifically, we establish the geometric-
momentcontraction(GMC)forconstantstep-sizeSGDdropoutiteratestoshowtheexistenceofaunique
stationary distribution of the dropout recursive function. By the GMC property, we provide quenched
central limit theorems (CLT) for the difference between dropout and ℓ2-regularized iterates, regardless
of initialization. The CLT for the difference between the Ruppert-Polyak averaged SGD (ASGD) with
dropout and ℓ2-regularized iterates is also presented. Based on these asymptotic normality results, we
further introduce an online estimator for the long-run covariance matrix of ASGD dropout to facilitate
inference in a recursive manner with efficiency in computational time and memory. The numerical
experimentsdemonstratethatforsufficientlylargesamples,theproposedconfidenceintervalsforASGD
with dropout nearly achieve the nominal coverage probability.
Keywords: stochasticgradientdescent,dropoutregularization,ℓ2-regularization,onlineinference,quenched
central limit theorems
∗Correspondingauthor. Email: jqli@uchicago.edu
1
4202
peS
11
]LM.tats[
1v43470.9042:viXra1 Introduction
Dropout regularization is a popular method in deep learning (Hinton et al. 2012; Krizhevsky et al. 2012;
Srivastavaetal.2014). Duringeachtrainingiteration,eachhiddenunitisrandomlymaskedwithprobability
1−p. Thisensuresthatahiddenunitcannotrelyonthepresenceofanotherhiddenunit. Dropouttherefore
provides an incentive for different units to act more independently and avoids co-adaptation, which means
that different units do the same.
There is a rich literature contributing to the theoretical understanding of dropout regularization. As
pointed out in Srivastava et al. (2014), the core idea of dropout is to artificially introduce stochasticity to
the training process, preventing the model from learning statistical noise in the data. Starting with the
connection of dropout and ℓ2-regularization that appeared already in the original dropout article Srivastava
et al. (2014), numerous works investigated the statistical properties of dropout by marginalizing the loss
functions over dropout noises and linking them with explicit regularization (Arora et al. 2021; Baldi and
Sadowski 2013; Cavazza et al. 2018; McAllester 2013; Mianjy and Arora 2019; Mianjy et al. 2018; Senen-
CerdaandSanders2022; Srivastavaetal.2014; Wageretal.2013). TheempiricalstudyinWeietal.(2020)
concluded that adding dropout noise to gradient descent also introduces implicit effects, which cannot be
characterized by connections between the gradients of marginalized loss functions and explicit regularizers.
For the linear regression model and fixed learning rates, Clara et al. (2024) proved that the implicit effect of
dropout adds noise to the iterates and that for a large class of design matrices, this implicit noise does not
vanish in the limit.
Thoughtheconvergencetheoryofdropoutinfixeddesignandfullgradientshasbeenwidelyinvestigated,
an analysis of dropout with random design or sequential observations is still lacking, not to mention online
statisticalinference. Tobridgethisgap,weprovideatheoreticalframeworkfordropoutappliedtostochastic
gradient descent (SGD). In particular, we establish the geometric-moment contraction (GMC) for the SGD
dropoutiteratesforarangeofconstantlearningratesα. Weprovidetwousefulandsharpmomentinequalities
to prove the q-th moment convergence of SGD dropout for any q >1.
BesidestheconvergenceanderrorboundsofSGDdropout,statisticalinferenceofSGD-basedestimators
is also gaining attention (Fang 2019; Fang et al. 2019; Liang and Su 2019; Su and Zhu 2023; Zhong et al.
2024). Instead of focusing on point estimators using dropout regularization, we quantify the uncertainty of
the estimates through their confidence intervals or confidence regions (Chen et al. 2020; Zhu et al. 2023).
Nevertheless, it is challenging to derive asymptotic normality for SGD dropout or its variants, such as
averaged SGD (ASGD) (Ruppert 1988; Polyak and Juditsky 1992). The reason is that the initialization
makes the SGD iterates non-stationary. In this paper, we leverage the GMC property of SGD dropout and
2show quenched central limit theorems (CLT) for both SGD and ASGD dropout estimates. Additionally, we
propose an online estimator for the long-run covariance matrix of ASGD dropout to facilitate the online
inference.
Contributions. This study employs powerful techniques from time series analysis to derive a general
asymptotic theory for the SGD iterates with dropout regularization. Specifically, the key contributions can
be summarized as follows.
(1) We establish the geometric-moment contraction (GMC) of the non-stationary SGD dropout iterates,
whoserecursioncanbeviewedasavectorauto-regressiveprocess(VAR).Thepossiblerangeoflearning
rates that ensures GMC can be related to the condition number of the design matrix with dropout.
(2) TheGMCpropertyguaranteestheexistenceofauniquestationarydistributionoftheSGDiterateswith
dropout, and leads to the Lq-convergence, the asymptotic normality, and the Gaussian approximation
rate of the SGD dropout estimates and their Ruppert-Polyak averaged version.
(3) We derive a new moment inequality in Lemma 11, proving that for any two random vectors x,y of
the same length, the q-th moment E∥x+y∥q can have a sharp bound in terms of E∥x∥q, E∥y∥q and
2 2 2
E(x⊤y), without the condition E[y | x] a =.s. 0 required in previous results (Rio 2009). The derived
moment inequality is also applicable to many other Lq-convergence problems in machine learning.
(4) Anonlinestatisticalinferencemethodisintroducedtoconstructjointconfidenceintervalsforaveraged
SGD dropout iterates. The coverage probability is shown to be asymptotically accurate in theory and
simulation studies.
Therestofthepaperisorganizedasfollows. Weintroducethedropoutregularizationingradientdescent
in Section 2. Followed by Section 3, we establish the geometric-moment contraction for dropout in gradient
descent and provide the asymptotic normality. In Section 4, we generalize the theory to stochastic gradient
descent. In Section 5, we provide an online inference algorithm for the ASGD dropout with theoretical
guarantees. Finally, we present simulation studies in Section 6. All the technical proofs are postponed to
the Appendix.
1.1 Background
Dropout regularization. After its introduction by Hinton et al. (2012) and Srivastava et al. (2014),
dropoutregularizationwasfoundtobecloselyrelatedtoℓ2-regularizationinlinearregressionandgeneralized
linearmodels. SeealsoBaldiandSadowski(2013)andMcAllester(2013). Wageretal.(2013)extendedthis
connectiontomoregeneralinjectedformsofnoise,showingthatdropoutinducesanℓ2-penaltyafterrescaling
3thedatabytheestimatedinversediagonalFisherinformation. Inneuralnetworkswithasinglehiddenlayer,
dropout noise marginalization leads to a nuclear norm regularization, as studied in matrix factorization
(Cavazza et al. 2018), linear neural networks (Mianjy et al. 2018), deep linear neural networks (Mianjy
and Arora 2019) and shallow ReLU-activated networks (Arora et al. 2021). Moreover, Gal and Ghahramani
(2016a)showedthatdropoutcanbeinterpretedasavariationalapproximationtotheposteriorofaBayesian
neuralnetwork. GalandGhahramani(2016b)appliedthisnewvariationalinferencebaseddropouttechnique
in recurrent neural networks (RNN) and long-short term memory (LSTM) models. Additional research has
explored the impact of dropout on convolutional neural networks (Wu and Gu 2015) and generalization
propertiesviaRademachercomplexitybounds(Aroraetal.2021; GaoandZhou2016; Wanetal.2013; Zhai
and Wang 2018). Dropout has been successfully applied in various domains, including image classification
(Krizhevsky et al. 2012), handwriting recognition (Pham and Le 2021) and heart sound classification (Kay
and Agarwal 2016).
Stochastic gradient descent. To learn from huge datasets, stochastic gradient descent (SGD) (Robbins
and Monro 1951; Kiefer and Wolfowitz 1952) is a computationally attractive variant of the gradient descent
method. While dropout and SGD have been studied separately, only little theory has been developed so far
for SGD training with dropout regularization. Mianjy and Arora (2020) showed the necessary number of
SGD iterations to achieve suboptimality in ReLU shallow neural networks for classification tasks, which is
independentofthedropoutprobabilityduetoastrictconditionondatastructures. Senen-CerdaandSanders
(2023) extended this to more generic results without assuming any specific data structures, focusing instead
on reaching stationarity in non-convex functions using dropout-like SGD. Furthermore, Senen-Cerda and
Sanders (2022) analyzed the gradient flow of dropout in shallow linear networks and studied the asymptotic
convergencerateofdropoutbymarginalizingthedropoutnoiseinashallownetwork. However,atheoretical
convergence analysis or inference theory of SGD dropout iterates without marginalization has not been
explored yet in the literature.
1.2 Notation
We denote column vectors in Rd by lowercase bold letters, that is, x := (x ,...,x )⊤ and write ∥x∥ :=
1 d 2
(x⊤x)1/2fortheEuclideannorm. Theexpectationandcovarianceofrandomvectorsarerespectivelydenoted
by E[·] and Cov(·). For two positive number sequences (a ) and (b ), we say a = O(b ) (resp. a ≍ b )
n n n n n n
if there exists c > 0 such that a /b ≤ c (resp. 1/c ≤ a /b ≤ c) for all large n, and say a = o(b ) if
n n n n n n
a n/b
n
→ 0 as n → ∞. Let (x n) and (y n) be two sequences of random variables. Write x
n
= OP(y n) if for
∀ϵ>0, there exists c>0 such that P(|x n/y n|≤c)>1−ϵ for all large n, and say x
n
=oP(y n) if x n/y
n
→0
4in probability as n→∞.
We denote matrices by uppercase letters. The d×d identity matrix is symbolized by I . Given matrices
d
A and B of compatible dimension, their matrix product is denoted by juxtaposition. Write A⊤ for the
transpose of A and define A := A⊤A. When A and B are of the same dimension, the Hadamard product
A⊙B is given by element-wise multiplication (A⊙B) =A B . For any A∈Rd×d, let Diag(A):=I ⊙A
ij ij ij d
denote the diagonal matrix with the same main diagonal as A. Given p∈(0,1), define the matrices
A:=A−Diag(A),
A :=pA+(1−p)Diag(A).
p
In particular, A = pA+Diag(A), so A results from re-scaling the off-diagonal entries of A by p. For a
p p
matrix A, the operator norm induced by the Euclidean norm ∥·∥ is the spectral norm and will always be
2
written without sub-script, that is, ∥A∥:=∥A∥ .
op
2 Dropout Regularization
Thestochasticityofdropoutmakesitchallengingtoanalyzetheasymptoticpropertiesofdropoutinstochas-
tic gradient descent. To address the complex stochastic structure, we investigate in Section 2 the dropout
regularization in gradient descent, and then generalize it to stochastic gradient descent in Section 4.1.
Consider a linear regression model with fixed design matrix X ∈Rn×d and outcome y ∈Rn, that is,
y =Xβ∗+ϵ, (1)
with unknown regression vector β∗ ∈ Rd, and random noise ϵ ∈ Rn. The task is to recover β∗ from the
observed data (y,X). Moreover, we suppose that E[ϵ] = 0 and Cov(ϵ) = I . We highlight that the noise
n
distributionofϵisoftenexplicitlymodeledasmultivariatenormal,butthisisnotnecessaryforthisanalysis.
We also assume that the design matrix X has no zero columns. Because of that we also say that model
(1) is in reduced form. We can always bring the model into reduced form, since zero columns and the
corresponding regression coefficients have no effect on the outcome y and can thus be eliminated from the
model.
We consider the least-squares criterion 1∥y−Xβ∥2 for the estimation of β∗. For the minimization, we
2 2
adoptaconstantlearning-rategradientdescentalgorithmwithrandomdropoutsineachiteration. Following
the seminal work on dropout by Srivastava et al. (2014), we call a d×d random diagonal matrix D a p-
i.i.d.
dropoutmatrixifitsdiagonalentriessatisfyD ∼ Bernoulli(p),withsomeretainingprobabilityp∈(0,1).
ii
5Onaverage, D haspddiagonalentriesequalto1and(1−p)ddiagonalentriesequalto0. Forsimplicity, the
dependence of D on p will only be stated if unclear from the context. For a sequence of independent and
identicallydistributed(i.i.d.) dropoutmatricesD ,k =1,2,...,andsomeconstantlearningrateα>0, the
k
k-th step gradient descent iterate with dropout takes the form
β˜ k(α)=β˜ k−1(α)−α∇
β˜
k−11 2(cid:13) (cid:13)y−XD kβ˜ k−1(α)(cid:13) (cid:13)2
2
=β˜ k−1(α)+αD kX⊤(cid:0) y−XD kβ˜ k−1(α)(cid:1) . (2)
When there is no ambiguity, we omit the dependence on α, writing β˜ instead of β˜ (α).
k k
Marginalizingthenoise,whentheiterationnumberkgrowstoinfinity,therecursionin(2)shalleventually
minimize the ℓ2-regularized least-squares loss by solving
β˜:=arg βm ∈i Rn
d
E(cid:104) 21(cid:13) (cid:13)y−XDβ(cid:13) (cid:13)2
2
(cid:12) (cid:12) (cid:12)y,X(cid:105) , (3)
where the expectation is taken only over the stochasticity in the dropout matrix D. Thus, the randomness
in β˜ comes from the random noise ϵ in (1). In fact, the random vector β˜ has a closed form expression. To
see this, we denote the Gram matrix by X=X⊤X and recall
X=X−Diag(X), X =pX+(1−p)Diag(X). (4)
p
Note that D2 =D, Diag(X)=X −pX, and that diagonal matrices always commute. Since the fixed design
p
matrix X is assumed to be in reduced form with min X > 0, one can show that solving the gradient for
i ii
the minimizer β˜ in (3) (Srivastava et al. 2014; Clara et al. 2024) leads to the closed form expression
(cid:16) (cid:17)−1
β˜=p p2X+p(1−p)Diag(X) X⊤y =X−1X⊤y. (5)
p
If the columns of X are orthogonal, then X is a diagonal matrix, X =X and β˜ coincides with the classical
p
least-squares estimator X−1X⊤y. We refer to Section 4.1 for a counterpart of β˜ using stochastic gradient.
A crucial argument in the analysis of the dropout iterate β˜ is to rewrite the dropout update formula as
k
β˜ −β˜=(I −αD XD )(β˜ −β˜)+αD X(pI −D )β˜.
k d k k k−1 k d k (6)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:Ak(α) =:bk(α)
For a derivation of (6), see Section 4.1 in Clara et al. (2024). Throughout the rest of this paper, we will
exchangeably write A =A (α) and b =b (α) when no confusion should be caused.
k k k k
63 Asymptotic Properties of Dropout in GD
Tostudytheasymptoticpropertiesofgradientdescentwithdropout,wefirstestablishthegeometric-moment
contraction for the GD dropout sequence. Subsequently, we derive the quenched central limit theorems for
both iterative dropout estimates and their Ruppert-Polyak averaged variants. Furthermore, we provide
the quenched invariance principle for the Ruppert-Polyak averaged dropout with the optimal Gaussian
approximation rate.
3.1 Geometric-Moment Contraction (GMC)
First, we extend the geometric-moment contraction in Wu and Shao (2004) to the cases where the inputs of
iterated random functions are i.i.d. random matrices.
Definition 1 (Geometric-moment contraction). For i.i.d. d×d random matrices Ψ ,Ψ′, i,j ∈Z, consider
i j
a stationary causal process
θ =g(Ψ ,...,Ψ ,Ψ ,Ψ ,...), k ∈Z, (7)
k k 1 0 −1
for a measurable function g(·) such that the d-dimensional random vector θ has a finite q-th moment
k
E∥θ ∥q <∞, for some q ≥1. We say that θ is geometric-moment contracting if there exists some constant
k 2 k
r ∈(0,1) such that
q
(cid:0)E∥θ −θ′∥q(cid:1)1/q =O(rk), for all k =1,2,..., (8)
k k 2 q
where θ′ =g(Ψ ,...,Ψ ,Ψ′,Ψ′ ,...) is a coupled version of θ with Ψ , i≤0, replaced by i.i.d. copies Ψ′.
k k 1 0 −1 k i i
In general, an iterated random function satisfies the geometric-moment contraction property under reg-
ularity conditions on convexity and stochastic Lipschitz continuity, see Section B.1 in the Appendix for
details. Here, we focus on the contraction property with Ψ = D , the k-th dropout matrix. Setting
k k
f (u):=u+αDX⊤(y−XDu), we can rewrite the recursion of the dropout gradient descent iterate β˜ (α)
D k
in (2) as
β˜ (α)=β˜ (α)+αD X⊤(cid:0) y−XD β˜ (α)(cid:1) =:f (cid:0) β˜ (α)(cid:1) . (9)
k k−1 k k k−1 Dk k−1
Weshallshowthat, underquitegeneralconditionsontheconstantlearningrateα>0, thisprocesssatisfies
the geometric-moment contraction in Definition 1, and converges weakly to a unique stationary distribution
π on Rd, that is, for any continuous function h∈C(Rd) with ∥h∥ <∞, E(cid:2) h(cid:0) β˜ (α)(cid:1)(cid:3) →(cid:82) h(u)π (du) as
α ∞ k α
7k →∞. We then write β˜ (α)⇒π . Set
k α
r
:=(cid:18)
sup
E(cid:13)
(cid:13)(cid:0) I −αD XD (cid:1)
v(cid:13) (cid:13)q(cid:19)1/q
. (10)
α,q (cid:13) d 1 1 (cid:13)
v∈Rd:∥v∥2=1 2
In particular, for q = 2, we can rewrite the squared norm and obtain r2 = λ (E(I −αD XD )2) with
α,2 max d 1 1
λ (·) the largest eigenvalue.
max
Lemma 1 (Learning-rate range in GD dropout). If q >1 and α∥X∥<2, then, r <1.
α,q
We only assume that the design matrix X has no zero column. Still X = X⊤X can be singular. Inter-
estingly, dropout ensures that even for singular X, we have r < 1. Without dropout, v could be chosen
α,q
as an eigenvector of X with corresponding eigenvalue zero. Then ∥(I −αX)v∥ = ∥v∥ , implying that
d 2 2
sup E∥(cid:0) I −αX(cid:1) v∥q ≥1.
v∈Rd:∥v∥2=1 d 2
Theorem 1 (Geometric-moment contraction of GD dropout). Let q >1 and choose a positive learning rate
α satisfying α∥X∥<2. For two dropout sequences β˜ (α),β˜′(α), k =0,1,..., generated by the recursion (6)
k k
with the same dropout matrices but possibly different initial vectors β˜ , β˜′, we have
0 0
(cid:0)E∥β˜ (α)−β˜′(α)∥q(cid:1)1/q ≤rk ∥β˜ −β˜′∥ . (11)
k k 2 α,q 0 0 2
Moreover, there exists a unique stationary distribution π which does not depend on the initialization β˜ ,
α 0
such that β˜ (α)⇒π as k →∞.
k α
Asmentionedbefore,r2 =λ (E(I −αD XD )2)<1.AspecialcaseofTheorem1isthusE∥β˜ (α)−
α,2 max d 1 1 k
β˜′(α)∥2 ≤ (λ (E(I − αD XD )2))k∥β˜ − β˜′∥2. Theorem 1 indicates that although the GD dropout
k 2 max d 1 1 0 0 2
sequence {β˜ k} k∈N is non-stationary due to the initialization, it is asymptotically stationary and approaches
the unique stationary distribution π at an exponential rate. Such geometric-moment contraction result is
α
fundamental to establish a central limit theorem for the iterates.
Another consequence of Theorem 1 is that if β is drawn from the stationary distribution π and D is an
α
independently sampled dropout matrix, then also f (β)∼π . This also means that if the initialization β˜◦
D α 0
is sampled from the stationary distribution π , then, the marginal distribution of any of the GD dropout
α
iterates β˜◦ will follow this stationary distribution as well.
k
We can also define the GD dropout iterates β˜◦ for negative integers k by considering i.i.d. dropout
k
matrices D for all integers k ∈Z and observing that the limit
k
β˜◦ := lim f ◦f ◦···◦f (β)=:h (D ,D ,...), (12)
k
m→∞
Dk Dk−1 Dk−m α k k−1
8exists almost surely and does not depend on β. Then, β˜◦ =f (cid:0) β˜◦ (cid:1) also holds for negative integers and
k Dk k−1
the geometric-moment contraction in Definition 1 is satisfied for β˜◦, that is,
k
(cid:16) E(cid:13) (cid:13)h α(D k,D k−1,...,D 1,D 0,D −1,...)−h α(D k,D k−1,...,D 1,D 0′,D −′ 1,...)(cid:13) (cid:13)q 2(cid:17)1/q =O(r αk ,q), (13)
for some q ≥1, r ∈(0,1) defined in (10), and i.i.d. dropout matrices D ,D′, k,ℓ∈Z.
α,q k ℓ
3.2 Iterative Dropout Schemes
Equation (6) rewrites the GD dropout iterates into β˜ (α)−β˜=A (α)(β˜ (α)−β˜)+b (α). If the initial
k k k−1 k
vector β˜◦ is sampled from the stationary distribution π , we also have
0 α
β˜◦(α)−β˜=A (α)(β˜◦ (α)−β˜)+b (α), (14)
k k k−1 k
and for any k = 0,1,..., β˜ k◦(α) ∼ π α. We can see that {β˜ k◦(α)} k∈N is a stationary vector autoregressive
process (VAR) with random coefficients. While (A (α),b (α)) are i.i.d., A (α) and b (α) are dependent.
k k k k
This poses challenges to prove asymptotic normality of the dropout iterates. An intermediate recursion is
obtained by replacing A (α) = I −αD XD by its expectation E[A (α)] = I −αpX . This gives the
k d k k k d p
recursion
β†(α)−β˜=(I −αpX )(cid:0) β† (α)−β˜(cid:1) +b (α), (15)
k d p k−1 k
withinitialvectorβ† =β˜◦ ∼π . Theproofthenderivestheasymptoticnormalityforβ†(α),andshowsthat
0 0 α k
the difference between β˜ (α) and β†(α) is negligible, in the sense that for q ≥2, (E∥β˜ (α)−β†(α)∥q)1/q =
k k k k 2
O(cid:0) α+rk ∥β˜ −β˜◦∥ (cid:1) , where the first part is due to the affine approximation in Lemma 2 and the second
α,q 0 0 2
part results from the GMC property in Theorem 1.
Lemma 2 (Affine approximation). If α ∈ (0,2/∥X∥), then the difference sequence δ (α) = β˜◦(α)−β†(α)
k k k
satisfies E[δ (α)]=0 and for any q ≥2, max (cid:0)E∥δ (α)∥q(cid:1)1/q =O(α).
k k k 2
Lemma 3 (Moment convergence of iterative GD dropout). Let q ≥ 2. For the stationary GD dropout
sequence {β˜ k◦(α)} k∈N defined in (6), if α∈(0,2/∥X∥), we have
√
max(cid:0)E∥β˜◦(α)−β˜∥q(cid:1)1/q
=O( α). (16)
k 2
k
Theorem 2 (QuenchedCLTofiterativeGDdropout). Consider the iterative dropout sequence {β˜ k(α)} k∈N
in (6) and the ℓ2-regularized estimator β˜ in (5). Assume that the constant learning rate α satisfies α ∈
9(0,2/∥X∥), and suppose that for every l = 1,...,d, there exists m ̸= l such that X ̸= 0. Then, for any
lm
k ∈N, we have
β˜ (α)−β˜
k √ ⇒N(0,Ξ(0)), as α→0, (17)
α
where Ξ(0)=lim Ξ(α), and Ξ(α)∈Rd×d denotes the covariance matrix of the stationary affine sequence
α↓0
{β k†(α)−β˜} k∈N defined in (15), that is,
(cid:16)β†(α)−β˜(cid:17) E[(β†(α)−β˜)(β†(α)−β˜)⊤]
Ξ(α):=Cov 1 √ = 1 1 . (18)
α α
One can derive more explicit expressions of Ξ(0). Reshaping a d×s matrix U = (u ,...,u ) with d-
1 s
dimensional column vectors u ,...,u into a ds-dimensional column vector gives vec(U):=(u⊤,...,u⊤)⊤.
1 s 1 s
Moreover,foranytwomatricesA∈Rp×q andB ∈Rm×n,theKroneckerproductA⊗B isthepm×qnblock
matrix, with each block given by (A⊗B) = A B. Following Theorem 1 in Pflug (1986), and assuming
ij ij
that Ξ(α) is differentiable with respect to α, Ξ(α) becomes the solution of a classical Lyapunov equation
Ξ(α)(pX )+(pX )Ξ(α)=S,
p p
that is,
Ξ(α)=V +αB , (19)
0 p
where the d×d matrices S,V and B are respectively defined as
0 p
S = 1 Cov(cid:0) b (α)(cid:1) =Cov(cid:0) D X(pI −D )β˜(cid:1) , (20)
α2 1 1 d 1
vec(V )=(I ⊗pX +pX ⊗I )−1·vec(S), (21)
0 d p p d
vec(B )=(I ⊗pX +pX ⊗I )−1·vec(p2X V X ). (22)
p d p p d p 0 p
Bydefinition,thematrixS isindependentofαandX̸=0sincethereexistnon-zerodiagonalandoff-diagonal
elements by assumptions in Theorem 2. Let S =E[β˜β˜⊤]. By the proof of Theorem 2, we can express S in
0
terms of p, X and β˜ as follows,
(cid:16) (cid:17)
S =p3(XS X) −2p pX (S X) +p2(1−p)Diag(XS X)
0 p p 0 p 0
(cid:16) (cid:17)
+pX (S ) X +p2(1−p) Diag(X(S ) X)+2X Diag(S X)+(1−p)X⊙S ⊤ ⊙X . (23)
p 0 p p 0 p p 0 0
10One can see that, Ξ(0)=lim Ξ(α)=V , and in particular, for small p, vec(V ) can be approximated by
α↓0 0 0
(I ⊗X +X ⊗I )−1·vec(X E[β˜β˜⊤] X ).
d p p d p p p
3.3 Dropout with Ruppert-Polyak Averaging
Toreducethevarianceofthegradientdescentiteratesβ˜ (α)introducedbytherandomdropoutmatrixD ,
k k
we now consider the averaged GD dropout (AGD) iterate
n
β¯gd(α)= 1 (cid:88) β˜ (α), (24)
n n k
k=1
followingtheaveragingschemeinRuppert(1988)andPolyakandJuditsky(1992). Wederivetheasymptotic
normality of β¯gd(α) in the following theorem.
n
Theorem 3 (Quenched CLT of averaged GD dropout). For the constant learning rate α ∈ (0,2/∥X∥) and
any fixed initial vector β˜ , the averaged GD dropout sequence satisfies
0
√
n(cid:0) β¯gd(α)−β˜(cid:1) ⇒N(cid:0) 0,Σ(α)(cid:1)
, (25)
n
with Σ(α) = (cid:80)∞ Cov(β˜◦(α),β˜◦(α)) the long-run covariance matrix of the stationary process β˜◦(α) ∼
k=−∞ 0 k k
π .
α
One can choose a few learning rates, say α ,...,α , and run gradient descent for each of these learning
1 s
rates in parallel by computing β˜ (α ),...,β˜ (α ) for k = 1,2,.... An example is federated learning where
k 1 k s
dataaredistributedacrossdifferentclients(Deanetal.2012;Karimireddyetal.2020;Zinkevichetal.2010).
Additionally, if we consider the unknown parameter β∗ in a general model instead of the linear regression
in (1), then the (stochastic) gradient descent algorithm with a constant learning rate α may not converge to
√
the global minimum β∗, but oscillate around β∗ with the magnitude O( α) (Dieuleveut et al. 2020; Pflug
1986). In this case, one can adopt extrapolation techniques (Allmeier and Gast 2024; Huo et al. 2023; Yu
et al. 2021) to reduce the bias in β¯gd(α) by using the results from parallel runs for different learning rates.
k
Corollary 1 (Quenched CLT of parallel averaged GD dropout). Let s ≥ 1. Consider constant learning
rates α ,...,α ∈(0,2/∥X∥). Then, for any initial vectors β˜ (α ),...,β˜ (α ),
1 s 0 1 0 s
√
n·vec(cid:0) β¯gd(α )−β˜,...,β¯gd(α )−β˜(cid:1) ⇒N(0,Σvec), (26)
n 1 n s
with vec(u ,...,u )=(u⊤,...,u⊤)⊤ ∈Rds for d-dimensional vectors u ,...,u , and the long-run covari-
1 s 1 s 1 s
ance matrix Σvec =(cid:80)∞ Cov(cid:0) vec(cid:0) β˜◦(α ),...,β˜◦(α )(cid:1) ,vec(cid:0) β˜◦(α ),...,β˜◦(α )(cid:1)(cid:1) .
k=−∞ 0 1 0 s k 1 k s
11Assumption 1 (Finitemomentofgradientswithdropout). Let q >2. Assume that the q-th moment of the
gradient in (2) exists at the true parameter β∗ in model (31), that is,
(cid:16) E yE D(cid:13) (cid:13) (cid:13)∇ β∗1 2(cid:13) (cid:13)y−XDβ∗(cid:13) (cid:13)2 2(cid:13) (cid:13) (cid:13)q 2(cid:17)1/q =(cid:16) E(cid:13) (cid:13)DX⊤(cid:0) y−XDβ∗(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q <∞.
For the central limit theorems (cf. Theorem 3), Assumption 1 is only required to hold for q = 2.
Since we have already assumed that Cov(ϵ) = I , we did not additionally impose any moment condition
n
in Theorem 3. However, if one aims for a stronger Gaussian approximation result, such as the rate for the
Koml´os–Major–Tusn´ady(KMT)approximation(Koml´osetal.1975;Koml´osetal.1976;Berkesetal.2014),
q >2 moments are necessary. In the quenched invariance principle below, we show that one can achieve the
optimal Gaussian approximation rate oP(n1/q) for the averaged GD dropout process.
Theorem 4 (Quenched invariance principle of averaged GD dropout). Suppose that Assumption 1 holds
and the constant learning rate satisfies α∈(0,2/∥X∥). Define a partial sum process (S◦(α)) with
i 1≤i≤n
i
S◦(α)=(cid:88) (β˜◦(α)−β˜). (27)
i k
k=1
Then, there exists a (richer) probability space (Ω⋆,A⋆,P⋆) on which we can define d-dimensional random
vectors β˜⋆, the associated partial sum process S⋆(α) = (cid:80)i (β˜⋆(α)−β˜), and a Gaussian process G⋆ =
k i k=1 k i
(cid:80)i z⋆, with independent Gaussian random vectors z⋆ ∼N(0,I ), such that (S◦) =D (S⋆) and
k=1 k k d i 1≤i≤n i 1≤i≤n
1m ≤ia ≤x n(cid:13) (cid:13)S i⋆−Σ1/2(α)G⋆ i(cid:13) (cid:13)
2
=oP(n1/q), in (Ω⋆,A⋆,P⋆), (28)
where Σ(α) is the long-run covariance matrix defined in Theorem 3. In addition, this approximation holds
for all (Sβ˜ 0(α)) given any arbitrary initial vector β˜ ∈Rd, where
i 1≤i≤n 0
i
Sβ˜ 0(α)=(cid:88) (β˜ (α)−β˜). (29)
i k
k=1
Theorem 4 shows that one can approximate the averaged GD dropout sequence by Brownian motions.
Specifically, for any fixed initial vector β˜ ∈ Rd, the partial sum process converges in the Euclidean norm,
0
uniformly over u,
(cid:8) n−1/2Sβ˜ 0 (α), 0≤u≤1(cid:9) ⇒(cid:8) Σ1/2(α)B(u), 0≤u≤1(cid:9) , (30)
⌊nu⌋
where ⌊t⌋ = max{i ∈ Z : i ≤ t}, and B(u) is the standard d-dimensional Brownian motion, that is, it
can be represented as a d-dimensional vector of independent standard Brownian motions. According to the
12argumentsinKarmakarandWu(2020),theKMTapproximationrateoP(n1/q)isoptimalforfixed-dimension
time series. Since we can view the GD dropout sequence as a VAR(1) process, the approximation rate in
Theorem 4 is optimal for the partial sum process
(Sβ˜
0(α)) .
i 1≤i≤n
4 Generalization to Stochastic Gradient Descent
In the previous section, we considered a fixed design matrix and (full) gradient descent with dropout.
Computing the gradient over the entire dataset can be computationally expensive, especially with large
datasets. We now investigate stochastic gradient descent with dropout regularization.
4.1 Dropout Regularization in SGD
Consider i.i.d. covariate vectors x ∈Rd, k =1,2,..., from some distribution Π, and the realizations y |x
k k k
from a linear regression model
y =x⊤β∗+ϵ , (31)
k k k
with unknown regression vector β∗ ∈ Rd. We assume that the model is in reduced form, which here means
that min (E[x x⊤]) > 0. In addition, we assume that the i.i.d. random noises ϵ satisfy E[ϵ ] = 0 and
i 1 1 ii k k
Var(ϵ ) = 1. In this paper, we focus on the classical case where the SGD computes the gradient based on
k
an individual observation (y ,x ). For constant learning rate α and initialization β˘ (α), the k-th step SGD
k k 0
iterate with Bernoulli dropout is
β˘ (α)=β˘ (α)−α∇ 1(cid:0) y −x⊤D β˘ (α)(cid:1)2
k k−1 β˘ k−12 k k k k−1
=β˘ (α)+αD x (cid:0) y −x⊤D β˘ (α)(cid:1) . (32)
k−1 k k k k k k−1
This is a sequential estimation, or online learning scheme, as computing β˘ (α) from β˘ (α) only requires
k k−1
the k-th sample (y ,x ). To study the contraction property of the SGD dropout iterates β˘ (α), we express
k k k
the recursion (32) by an iterated random function f˘:Rd×d×R×Rd×Rd (cid:55)→Rd with
f (u)=u+αDx(y−xDu),
D,(y,x)
that is,
β˘ (α)=β˘ (α)+αD x (cid:0) y −x⊤D β˘ (α)(cid:1)
k k−1 k k k k k k−1
=:f˘ (β˘ (α)). (33)
Dk,(yk,xk) k−1
13Weshallshowthatthisiteratedrandomfunctionf˘isgeometricallycontracting,andtherefore,thereexistsa
unique stationary distribution π˘ such that β˘ (α)⇒π˘ , where ⇒ denotes the convergence in distribution.
α k α
From now on, let (y,x) be a sample with the same distribution as (y ,x ). By marginalizing over all
k k
randomness, we can view the SGD dropout in (32) as a minimizer of the ℓ2-regularized least-squares loss
(cid:104)1 (cid:105)
β˘:=arg min E E (y−x⊤Dβ)2 . (34)
β∈Rd (y,x) D 2
Here, the expectation is take over both the random sample (y,x) and the dropout matrix D. Throughout
the rest of the paper, we shall write E[·]=E E [·] when no confusion should be caused.
(y,x) D
Denote the d×d Gram matrix by X =x x⊤, and define
k k k
X =X −Diag(X ), X =pX +(1−p)Diag(X ). (35)
k k k k,p k k
By Lemma 12 in the Appendix, we have a closed form solution for β˘ as follows
(cid:16) (cid:17)−1
β˘=p p2E[x x⊤]+p(1−p)Diag(E[x x⊤]) E[y x ]=(E[X ])−1E[y x ],
1 1 1 1 1 1 1,p 1 1
and thus, we obtain the relationship E[X ]β˘=E[y x ]. To study the SGD with dropout, we now focus on
1,p 1 1
the difference process {β˘ k(α)−β˘} k∈N. As in the case of gradient descent, this process can be written in
autoregressive form,
β˘ (α)−β˘=(cid:0) I −αD X D (cid:1) (β˘ (α)−β˘)+αD x (y −x⊤D β˘). (36)
k d k k k k−1 k k k k k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:A˘ k(α) =:b˘ k(α)
4.2 GMC of Dropout in SGD
Establishing the geometric-moment contraction (GMC) property to the stochastic gradient descent iterates
withdropoutisnon-trivialastherandomnessofβ˘ (α)notonlycomesfromthedropoutmatrixD ,butalso
k k
therandomdesignvectorsx . RecallthatX isdefinedin(35)andbyLemma7(ii), E[DX D]=pE[X ].
k k,p k k,p
Lemma 4 (Learning-rate range in SGD dropout). Assume that µ (v)=(E∥D X D v∥q)1/q <∞ for some
q k k k 2
q ≥2 and some unit vector v ∈Rd. If the learning rate α>0 satisfies
α(q−1) (1+αµ (v))q−2µ (v)2
sup q q <1, (37)
2 pv⊤E[X ]v
v∈Rd,∥v∥2=1 k,p
14then, for a dropout matrix D,
sup E(cid:13) (cid:13)(I d−αDX kD)v(cid:13) (cid:13)q
2
<1.
v∈Rd,∥v∥2=1
This provides a sufficient condition for the learning rate α which ensures contraction of I −αDX D for
d k
momentsq ≥2.ThiswillleadtoLq-convergenceoftheSGDdropoutiteratesanddeterminestheconvergence
rate in the Gaussian approximation in Theorem 7.
For the special case q = 2, the identities µ (v)2 = E∥D X D v∥2 and E(D X D ) = pE[X ] imply
2 k k k 2 k k k k,p
that condition (37) can be rewritten into
α µ (v)2
sup 2 <1, (38)
2 pv⊤E[X ]v
v∈Rd,∥v∥2=1 k,p
and
2v⊤E(D X D )v
0<α< inf k k k . (39)
v∈Rd,∥v∥2=1 E∥D kX kD kv∥2
2
For q = 2, Lemma 13 in the Appendix states that the conclusion of the previous lemma is also implied by
the condition E[2X −αX2]>0.
k k
Remark 1 (Interpretation for the range of learning rate). Condition (37) can be viewed as an “L2-Lq
equivalence”, where the left hand side can be interpreted as a measure of the convexity and smoothness of
the loss functions. We show this for the case q =2, working with the equivalent condition (39).
Thelossfunctiong(β˘ ,(D ,y ,x )):=∥y −x⊤D β˘ ∥2/2in(32)isstronglyconvexandthegradient
k−1 k k k k k k k−1 2
is stochastic Lipschitz continuous. To see this, recall X =x x⊤. By taking the gradient with respect to the
k k k
first argument, we obtain ∇g(β˘ ,(D ,y ,x )) = D x (y −x⊤D β˘ ) = −(D X D )β˘ +D x y .
k−1 k k k k k k k k k−1 k k k k−1 k k k
For any two vectors β˘ ,β˘′ , we have the strong convexity
k−1 k−1
(cid:68) E∇g(cid:0) β˘ ,(D ,y ,x )(cid:1) −E∇g(cid:0) β˘′ ,(D ,y ,x )(cid:1) , β˘ −β˘′ (cid:69) ≥J∥β˘ −β˘′ ∥2,
k−1 k k k k−1 k k k k−1 k−1 k−1 k−1 2
with the constant J := inf E∥D X D v∥ = λ {E(D X D )}. Furthermore, given any two
v∈Rd,∥v∥2=1 k k k 2 min k k k
vectors β˘ ,β˘′ ∈ Rd, we also have E∥∇g(cid:0) β˘ ,(D ,y ,x )(cid:1) −∇g(cid:0) β˘′ ,(D ,y ,x )(cid:1) ∥2 ≤ K∥β˘ −
k−1 k−1 k−1 k k k k−1 k k k 2 k−1
β˘′ ∥2, with K := sup E∥D X D v∥2 ≤ λ2 {E(D X D )}. This implies the stochastic Lips-
k−1 2 v∈Rd,∥v∥2=1 k k k 2 max k k k
chitz continuity of the gradient ∇g. Therefore, learning rate α satisfying
0<α<2J/K, (40)
15ensures by Lemma 4 contraction of the second moment of I −αDX D. The constant K/J is also related to
d k
the condition number of the matrix E[D X D ]. When the dimension d grows, the condition number can be
k k k
larger and thus the learning rate α needs to be small.
For the geometric-moment contraction for the SGD dropout sequence, we impose the following moment
conditions.
Assumption 2 (Finitemoment). Assume that for some q ≥2, the random noises ϵ and the random sample
x in model (31) have finite 2q-th moment E[|ϵ|2q]+∥x∥2q]<∞.
2
Lemma 14 in the Appendix shows that this assumption ensures the finite q-th moment of the stochastic
gradient in (32) evaluated at the true parameter β∗ and the ℓ2-minimizer β˘ in model (31), that is,
(cid:16) E(cid:13) (cid:13) (cid:13)∇
β∗
21(cid:0) y−x⊤Dβ∗(cid:1)2(cid:13) (cid:13) (cid:13)q 2(cid:17)1/q =(cid:16) E(cid:13) (cid:13)Dx(cid:0) y−x⊤Dβ∗(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q <∞,
and (E∥∇ 1(cid:0) y−x⊤Dβ˘(cid:1)2 ∥q)1/q < ∞. Now, we are ready to show the GMC property of the SGD dropout
β˘2 2
sequence.
Theorem 5 (Geometric-momentcontractionofSGDdropout). Let q >1. Suppose that Assumption 2 holds
and the learning rate α satisfies (37). For two dropout sequences β˘ (α) and β˘′(α), k = 0,1,..., that are
k k
generated by the recursion (32) with the same sequence of dropout matrices {D k} k∈N but possibly different
initializations β˘ , β˘′, we have
0 0
(cid:16) E(cid:13) (cid:13)β˘ k(α)−β˘ k′(α)(cid:13) (cid:13)q 2(cid:17)1/q ≤r˘ αk ,q∥β˘ 0−β˘ 0′∥ 2, for all k =1,2,..., (41)
with
r˘
α,q
=(cid:16) sup E(cid:13) (cid:13)A˘ 1(α)v(cid:13) (cid:13)q 2(cid:17)1/q <1. (42)
v∈Rd:∥v∥2=1
Moreover, for any initial vector β˘ ∈ Rd, there exists a unique stationary distribution π˘ which does not
0 α
depend on β˘ , such that β˘ (α)⇒π˘ as k →∞.
0 k α
By Theorem 5, initializing β˘ 0◦ ∼ π˘ α leads to the stationary SGD dropout sequence {β˘ k◦(α)} k∈N by
following the recursion
β˘◦(α)−β˘=A˘ (α)(β˘◦ (α)−β˘)+b˘ (α), k =1,2,..., (43)
k k k−1 k
wheretheℓ2-regularizedminimizerβ˘isdefinedin(34),andtherandomcoefficientsA˘ (α)=I −αD X D
k d k k k
and b˘ (α) = αD x (y −x⊤D β˘) are defined in (36). Furthermore, recall the iterated random function
k k k k k k
16f˘ (β) defined in (33). As a direct consequence of Theorem 5, we have
D,(y,x)
β˘◦(α)=f˘ (β˘◦ (α)), (44)
k Dk,(yk,xk) k−1
which holds for all k ∈Z. To see the case with k ≤0, we only need to notice that, for any β ∈Rd, we have
the limit
β˘◦ := lim f˘ ◦···f˘ (β)=:h˘ (ξ ,ξ ,...), (45)
k
m→∞
ξk ξk−m α k k−1
where h˘ is a measurable function that depends on α, and we use ξ to denote all the new-coming random
α k
parts in the k-th iteration, that is,
ξ =(D ,(y ,x )), k ∈Z. (46)
k k k k
For k ≤0, ξ can be viewed as an i.i.d. copy of ξ for some j ≥1. The limit h˘ (ξ ,ξ ,...) exists almost
k j α k k−1
surely and does not depend on β. Therefore, the iteration β˘◦(α)=f˘ (β˘◦ (α)) in (44) holds for all
k Dk,(yk,xk) k−1
k ∈Z.
4.3 Asymptotics of Dropout in SGD
In this section, we provide the asymptotics for the k-th iterate of SGD dropout and the Ruppert-Polyak
averaged version.
Lemma 5(MomentconvergenceofiterativeSGDdropout). Letq ≥2andsupposethatAssumption2holds.
For the stationary SGD dropout sequence {β˘ k◦(α)} k∈N defined in (43) with learning rate α satisfying (37),
we have
√
max(cid:0)E∥β˘◦(α)−β˘∥q(cid:1)1/q
=O( α). (47)
k 2
k
BesidesthestochasticorderofthelastiterateofSGDdropoutβ˘ (α),wearealsointerestedinthelimiting
k
distribution of the Ruppert-Polyak averaged SGD dropout, which can effectively reduce the variance and
keep the online computing scheme. In particular, we define
n
β¯sgd(α)= 1 (cid:88) β˘ (α). (48)
n n k
k=1
Theorem 6 (Quenched CLT of averaged SGD dropout). If the learning rate α satisfies (37), then,
√
n(β¯sgd(α)−β˘)⇒N(0,Σ˘(α)), as n→∞, (49)
n
with Σ˘(α):=(cid:80)∞ E[(β˘◦(α)−β˘)(β˘◦(α)−β˘)⊤] the long-run covariance matrix of the stationary process
k=−∞ 0 k
17β˘◦(α)∼π˘ .
k α
As discussed above Corollary 1, one can also choose different learning rates α ,...,α and then run the
1 s
SGD dropout sequences β˘ (α ),...,β˘ (α ) in parallel. For d-dimensional vectors u ,...,u , recall that
k 1 k s 1 s
vec(u ,...,u ):=(u⊤,...,u⊤)⊤ is the ds-dimensional concatenation.
1 s 1 s
Corollary 2 (Quenched CLT of parallel averaged SGD dropout). Let s ≥ 1. Consider constant learning
rates α ,...,α satisfying the condition in (37). Then, for any initial vectors β˘ (α ),...,β˘ (α ),
1 s 0 1 0 s
√ n·vec(cid:16) β¯sgd(α )−β˘,...,β¯sgd(α )−β˘(cid:17) ⇒N(cid:0) 0,Σ˘vec(cid:1) , (50)
n 1 n s
with the long-run covariance matrix
∞
Σ˘vec = (cid:88) Cov(cid:0) vec(cid:0) β˘◦(α ),...,β˘◦(α )(cid:1) ,vec(cid:0) β˘◦(α ),...,β˘◦(α )(cid:1)(cid:1) .
0 1 0 s k 1 k s
k=−∞
Theorem 7 (Quenched invariance principle of averaged SGD dropout). Suppose that Assumption 2 holds
for some q >2 and the learning rate α satisfies (37). Define a partial sum process (S˘◦(α)) with
i 1≤i≤n
i
S˘◦(α)=(cid:88) (β˘◦(α)−β˘). (51)
i k
k=1
Then, there exists a (richer) probability space (Ω˘⋆,A˘⋆,P˘⋆) on which one can define d-dimensional random
vectors β˘⋆, the associated partial sum process S˘⋆(α) = (cid:80)i (β˘⋆(α)−β˘), and a Gaussian process G˘⋆ =
k i k=1 k i
(cid:80)i z˘⋆, with independent Gaussian random vectors z˘ ∼N(0,I ), such that (S˘◦) =D (S˘⋆) and
k=1 k k d i 1≤i≤n i 1≤i≤n
1m ≤ia ≤x n(cid:13) (cid:13)S˘ i⋆−Σ˘1/2(α)G˘⋆ i(cid:13) (cid:13)
2
=oP(n1/q), in (Ω˘⋆,A˘⋆,P˘⋆), (52)
where Σ˘(α) is the long-run covariance matrix defined in Theorem 6. In addition, this approximation holds
for all (S˘β˘ 0(α)) given any arbitrary initialization β˘ ∈Rd, where
i 1≤i≤n 0
i
S˘β˘ 0(α)=(cid:88) (β˘ (α)−β˘). (53)
i k
k=1
5 Online Inference for SGD with Dropout
The long-run covariance matrix Σ˘(α) of the averaged SGD dropouts is usually unknown and needs to be
estimated. We now propose an online estimation method for Σ˘(α), and establish theoretical guarantees.
18The key idea is to adopt the non-overlapping batched means (NBM) method (Lahiri 1999; Lahiri 2003;
XiaoandWu2011),whichresamplesblocksofobservationstoestimatethelong-runcovarianceofdependent
data. Essentially, a sequences of non-overlapping blocks are pre-specified. When the block sizes are large
enough, usually increasing as the the sample size grows, the block sums shall behave similar to independent
observations and therefore can be used to estimate the long-run covariance. In this paper, to facilitate
the online inference of the dependent SGD dropout iterates {β˘ k(α)} k∈N, we shall extend the offline NBM
estimatorstoonlineversionsbyonlyincludingthepastSGDdropoutiteratesineachbatch. Theoverlapped
batch-means (OBM) methods are also investigated in literature; see for example Xiao and Wu (2011) and
Zhu et al. (2023). We shall only focus on the NBM estimates in this study given its simpler structure.
Let η ,η ,... be a strictly increasing integer-valued sequence satisfying η = 1 and η −η → ∞ as
1 2 1 m+1 m
m→∞. For each m, we let B denote the block
m
B ={η ,η +1,...,η −1}. (54)
m m m m+1
For the n-th SGD dropout iteration, denote by ψ(n) the largest index m such that η ≤ n. For any d-
m
dimensionalvector v =(v ,...,v )⊤,theKroneckerproductisthe d×dmatrixv⊗2 =(v v )d .Basedon
1 d i j i,j=1
thenon-overlappingblocks{B m} m∈N,forthen-thiteration,wecanestimatethelong-runcovariancematrix
Σ˘(α) in Theorem 6 by
ψ(n)−1 n
Σˆ (α)= 1 (cid:88) (cid:16) (cid:88) (cid:2) β˘ (α)−β¯sgd(α)(cid:3)(cid:17)⊗2 + 1(cid:16) (cid:88) (cid:2) β˘ (α)−β¯sgd(α)(cid:3)(cid:17)⊗2 . (55)
n n k n n k n
m=1 k∈Bm k=ηψ(n)
The estimator Σˆ (α) is composed of two parts. The first part takes the sum within each block and then
n
estimates the sample covariances of these centered block sums. The second part accounts for the remaining
observations, which can be viewed as the estimated covariance of the tail block.
For the recursive computation of Σˆ (α), we need to rewrite (55) such that, in the n-th iteration, we can
n
update Σˆ (α) based on the information from the (n−1)-st step and the latest iterate β˘ (α). To this end,
n n
we denote the number of iterates included in the tail part (i.e., the second part) in (55) by
δ (n)=n−η +1, (56)
η ψ(n)
and define two partial sums
n
S (α)= (cid:88) β˘ (α) and R (α)= (cid:88) β˘ (α). (57)
m k n k
k∈Bm k=ηψ(n)
19Then, we notice that the estimator Σˆ (α) in (55) can be rewritten as follows,
n
(cid:34) ψ(n)−1 ψ(n)−1
Σˆ (α)= 1 (cid:16) (cid:88) S (α)⊗2+R (α)⊗2(cid:17) +(cid:16) (cid:88) |B |2+|δ (n)|2(cid:17) β¯sgd(α)⊗2
n n m n m η n
m=1 m=1
ψ(n)−1
−(cid:16) (cid:88) |B |S (α)+δ (n)R (α)(cid:17) β¯sgd(α)⊤
m m η n n
m=1
ψ(n)−1 (cid:35)
−β¯sgd(α)(cid:16) (cid:88)
|B |S (α)+δ (n)R
(α)(cid:17)⊤
n m m η n
m=1
1(cid:104) (cid:105)
=: V (α)+K β¯sgd(α)⊗2−H (α)β¯sgd(α)⊤−β¯sgd(α)H (α)⊤ . (58)
n n n n n n n n
Assuch,theestimationofΣ˘(α)reducestorecursivelycomputing{V (α),K ,H (α),β¯sgd(α)}withrespectto
n n n n
n.WeprovidedthepseudocodesoftherecursioninAlgorithm1. Weshallfurtherestablishtheconvergence
rate of the proposed online estimator Σˆ (α) in Theorem 8.
n
Algorithm 1: Online estimation of the long-run covariance matrix of ASGD dropout
Data: Sequential random samples (y ,x ),...,(y ,x ); sequential dropout matrices D ,...,D ;
1 1 n n 1 n
constant learning rate α; predefined sequences {η m} m∈N
Result: ASGD dropout β¯sgd (α); estimated long-run covariance matrix Σ˜ (α)
n+1 n+1
Initialize β˘ (α)=β¯sgd(α)=R (α)←0,
0 0 0
ψ(0)←1, δ (0)←1, K =H (α)←1, V (α)←0
η 0 0 0
for n=0,1,2,3,... do
β˘ (α)←β˘ (α)+αD x (cid:0) y −x⊤D β˘ (α)(cid:1) ; /* SGD dropout */
n+1 n n n n n n n−1
β¯sgd (α)←{nβ¯sgd(α)+β˘ (α)}/(n+1) ; /* ASGD dropout */
n+1 n n+1
if n+1<η then
ψ(n)+1
R (α)←R (α)+β˘ (α), δ (n+1)←δ (n)+1;
n+1 n n+1 η η
K ←K −δ2(n)+δ2(n+1), ψ(n+1)←ψ(n);
n+1 n η η
H (α)←H (α)−δ (n)R (α)+δ (n+1)R (α);
n+1 n η n η n+1
V (α)←V (α)−R (α)⊗2+R (α)⊗2;
n+1 n n n+1
else
R (α)←β˘ (α), δ (n+1)←1;
n+1 n+1 η
ψ(n+1)←ψ(n)+1;
K ←K +1, H (α)←H (α)+R (α);
n+1 n n+1 n n+1
V (α)←V (α)+R (α)⊗2;
n+1 n n+1
end
Σˆ (α)←(cid:2) V (α)+K β¯sgd (α)⊗2−H (α)β¯sgd (α)⊤−β¯sgd (α)H (α)⊤(cid:3) /(n+1);
n+1 n+1 n+1 n+1 n+1 n+1 n+1 n+1
/* Estimated long-run covariance matrix */
end
The rational behind Algorithm 1 is as follows: if n+1 < η , then the index n+1 still belongs to
ψ(n)+1
theblockB andψ(n+1)=ψ(n). AlsowehaveR (α)=R (α)+β˘ (α)andδ (n+1)=δ (n)+1.
ψ(n) n+1 n n+1 η η
20Consequently, {K ,V (α),H (α)} can be recursively updated via
n+1 n+1 n+1
K =K −|δ (n)|2+|δ (n+1)|2,
n+1 n η η
V (α)=V (α)−R (α)⊗2+R (α)⊗2,
n+1 n n n+1
H (α)=H (α)−δ (n)R (α)+δ (n+1)R (α).
n+1 n η n η n+1
Otherwise, if n+1=η , we have ψ(n+1)=ψ(n)+1. Hence R (α)=β˘ (α) and δ (n+1)=1. In
ψ(n) n+1 n+1 η
this case, {K ,V (α),H (α)} can be recursively updated as follows,
n+1 n+1 n+1
K =K +1,
n+1 n
V (α)=V (α)+R (α)⊗2,
n+1 n n+1
H (α)=H (α)+R (α).
n+1 n n+1
Assuch,givenβ˘ (α),...,β˘ (α),theestimatorΣˆ (α)forthelong-runcovariancematrixΣ˘(α)canbeupdated
1 n n
in an online manner, requiring only O(1) memory storage.
Theorem 8 (Precision of Σˆ (α)). Let η =⌊cmζ⌋ for some c>0 and ζ >1. Let conditions in Theorem 5
n m
hold with some q ≥4. Then, we have
E∥Σˆ (α)−Σ˘(α)∥≲n(1/ζ−1)∨(−1/(2ζ)),
n
where ∥·∥ denotes the operator norm, and the constant in ≲ depends on c,q and d.
In particular, for ζ =3/2,
E∥Σˆ (α)−Σ˘(α)∥≲n−1/3, (59)
n
and this rate is optimal among long-run covariance estimators, even when comparing to offline estimation.
See Xiao and Wu (2011) for details.
By the estimation procedure summarized in Algorithm 1, we can asymptotically estimate the long-run
covariance matrix of the SGD dropout iterates β¯sgd(α) for any arbitrarily fixed initial vector. For some
n
given confidence level ω ∈(0,1), in the n-th iteration the online confidence interval for each coordinate β∗,
j
j =1,...,d, of the unknown parameter β∗ in model (31) is
(cid:104) (cid:113) (cid:113) (cid:105)
CI := β¯sgd(α)−z σˆ (α)/n, β¯sgd(α)+z σˆ (α)/n , (60)
ω,n,j n,j 1−ω/2 n,jj n,j 1−ω/2 n,jj
21with z denoting the (1−ω/2)-percentile of the standard normal distribution. Here, σˆ (α) is the
1−ω/2 n,jj
j-th diagonal of the proposed online long-run covariance estimator Σˆ (α) in (55), and β¯sgd(α) is the j-th
n n,j
coordinate of the averaged SGD dropout estimate β¯sgd(α). Furthermore, the online joint confidence regions
n
for the vector β∗ is
CI :=(cid:110) β ∈Rd : n(cid:0) β¯sgd(α)−β(cid:1)⊤ Σˆ−1(α)(cid:0) β¯sgd(α)−β(cid:1) ≤χ2 (cid:111) , (61)
ω,n n n n d,1−ω/2
where χ2 is the (1−ω/2)-percentile of the χ2 distribution with d degrees of freedom.
d,1−ω/2 d
Corollary 3 (Asymptotic coverage probability). Suppose that Assumption 2 holds and the learning rate α
satisfies (37). Given ω ∈ (0,1) and η = ⌊cmζ⌋ for some c > 0 and ζ > 1, CI defined in (60), and
m ω,n,j
CI defined in (61) are asymptotic 100(1−ω)% confidence intervals, that is, P(cid:0) β∗ ∈CI (cid:1) →1−ω for
ω,n j ω,n,j
all j =1,...,d, and P(cid:0) β∗ ∈CI (cid:1) →1−ω, as n→∞. More generally, for any d-dimensional unit-length
ω,n
vector v with ∥v∥ =1, and z the (1−ω/2)-quantile of the standard normal distribution,
2 1−ω/2
(cid:104) (cid:113) (cid:113) (cid:105)
CIProj := v⊤β¯sgd(α)−z n−1(v⊤Σˆ (α)v), v⊤β¯sgd(α)+z n−1(v⊤Σˆ (α)v) (62)
ω,n n 1−ω/2 n n 1−ω/2 n
isanasymptotic100(1−ω)%confidenceintervalfortheone-dimensionalprojectionv⊤β∗,thatis, P(cid:0) v⊤β∗ ∈
CIProj(cid:1)
→1−ω, as n→∞.
ω,n
By the quenched CLT of the averaged SGD dropout sequence {β¯ nsgd(α)} n∈N in Theorem 7 and the
consistency of Σˆ (α) in Theorem 8, we can apply Slutsky’s theorem and obtain the results in Corollary 3.
n
In Section 6, we shall validate the proposed online inference method by examining the estimation accuracy
of the proposed online estimator Σˆ (α) and the coverage probability of CIProj under different settings.
n ω,n
6 Simulation Studies
Inthissection,wepresenttheresultsofthenumericalexperimentstodemonstratethevalidityoftheproposed
online inference methodology. The codes for reproducing all results and figures can be found online1.
6.1 Sharp Range of the Learning Rate
The GD dropout iterates can be defined via the recursion (6), β˜ (α)−β˜ = A (α)(β˜ (α)−β˜)+b (α),
k k k−1 k
and the derived theory requires the learning rate α to satisfy α∥X∥ < 2. Via a simulation study we show
1https://github.com/jiaqili97/Dropout_SGD
22that this range is close to sharp to guarantee that the contraction constant
r α2
,2
= sup E(cid:13) (cid:13)A 1(α)v(cid:13) (cid:13)2
2
=λ max(cid:0)E[A⊤ 1(α)A 1(α)](cid:1) <1. (63)
v∈Rd:∥v∥2=1
This then indicates that the condition α∥X∥<2 in Lemma 1 can likely not be improved.
Forthen×dfulldesignmatrixX, weindependentlygenerateeachentryofX fromthestandardnormal
distribution. Since X=X⊤X, the upper bound 2/λ (X) of the learning rate α can be computed. Then,
max
we independently generate N = 500 dropout matrices D , i = 1,...,N with retaining probability p. The
i
simulation study evaluates the empirical contraction constant
N
(cid:16) (cid:88) (cid:17)
rˆ2 :=λ N−1 A⊤(α)A (α) , (64)
α,2 max i i
i=1
for different sample size n, dimension d, retaining probability p, and learning rate α. Table 1 shows that
n,d 2/λ (X) p α rˆ2
max α,2
100, 5 0.0151 0.9 0.0150 0.97
0.0154 1.02
100, 50 0.0068 0.9 0.0067 0.93
0.0072 1.01
0.8 0.0068 0.90
0.0075 1.02
100, 100 0.0052 0.9 0.0050 0.93
0.0057 1.15
0.5 0.0050 0.94
0.0075 1.06
Table1: EffectsofthelearningrateαonthegeometricmomentcontractionoftheGDiterateswithdropout.
even if the learning rate α exceeds the upper bound 2/λ (X) by a small margin, the contraction will not
max
hold any more since rˆ2 >1. This indicates that the condition α∥X∥<2 is close to sharp.
α,2
6.2 Estimation of Long-Run Covariance Matrix
Inthissection,weprovidethesimulationresultsoftheproposedlong-runcovarianceestimatorΣˆ (α)defined
n
in (55), and its online version (58).
Figure 1 shows the convergence of the GD and SGD iterates with dropout. The coordinates of the true
regression vector β∗ are equidistantly spaced between 0 and 1. One can see that the initialization is quickly
forgotten in both GD and SGD algorithms.
Figure2evaluatestheperformanceoftheonlinelong-runcovarianceestimatorΣˆ (α)inthesamesetting
n
as considered before. As there is no closed-form expression for the true long-run covariance matrices Σ˘(α)
23(a) AGD. (b) ASGD.
Figure1: ConvergencetracesofAGDandASGDiterateswithdropoutregularizationbasedonasinglerun,
withdimensiond=10andinitializationatzero. Thecoordinatesofthetrueparameterβ∗ areequidistantly
spaced between 0 and 1, the learning rate α = 0.01, and the retaining probability p = 0.9. Each curve
represents the convergence trace of one coordinate.
definedinTheorem6,weshallonlyreporttheconvergencetraceoftheestimatedlong-runcovariancematrix.
For the non-overlapping blocks B ={η ,η ,...,η −1}, m=1,...,M, defined in (54), the number
m m m+1 m+1
√
of blocks is M =⌊ n⌋ and η =m2. In Figure 2, we can see that the long-run variances of each coordinate
m
oftheASGDdropoutiteratesβ¯sgd(α)convergesasthenumberofiterationsngrows. Thelengthofthejoint
n
confidence interval for the one-dimensional projection v⊤β∗ is also shown in Figure 3, where we set each
√
coordinate of the unit-length vector v to be 1/ d. In the next section, we shall show that by using these
estimated long-run variances, the online confidence intervals achieve asymptotically the nominal coverage
probability.
6.3 Online Confidence Intervals of ASGD Dropout Iterates
Recallthe100(1−ω)%onlineconfidenceintervalCI in(60)foreachcoordinateβ∗ ofthetrueparameter
ω,n,j j
β∗, for j = 1,...,d. Let dimension d = 10. We constructed the confidence interval CI for each
ω,n,j
j = 1,...,d, and averaged the coverage probabilities of CI over j. As shown in Figure 4, the averaged
ω,n,j
coverage probabilities converge to the nominal coverage rate 0.95 as the number of steps n increases.
Furthermore,recallthe100(1−ω)%jointonlineconfidenceintervalCIProj in(62)fortheone-dimensional
ω,n
projectionofthetrueparameterβ∗,i.e.,v⊤β∗. Letdimensiond=50. Asimilarperformanceinconvergence
ofthecoverageprobabilityisobservedinFigure5. InTables2–4,wereportthecoverageprobabilitiesofthe
joint confidence intervals CIProj under different settings. In particular, we consider the dimensions d=3,20
ω,n
and 50, the retaining probabilities of the dropout regularization p=0.9 and 0.5, and the constant learning
24Figure 2: Estimated long-run variances of ASGD dropout iterates, i.e., diagonals of the estimated long-run
covariance matrix Σˆ (α) for the same setting as in Figure 1.
n
Figure 3: Length of the joint CI for the one-dimensional projection v⊤β∗ of the ASGD dropout iterates for
the same setting as in Figure 1.
ratesαrangingfrom0.01to0.1. Alltheresultsdemonstratetheeffectivenessofourproposedonlineinference
method.
25Figure4: Coverageprobabilitiesof95%CIforASGDdropoutiteratesaveraged over d coordinates from200
independent runs. Red dashed line denotes the nominal coverage rate of 0.95. Dimension d = 10, p = 0.9,
α=0.01, and coordinates of β∗ are equidistantly spaced between 0 and 1 with initializations at zero.
Figure 5: Coverage probabilities of 95% joint confidence intervals for one-dimensional projection of ASGD
dropout iterates from 200 independent runs. Red dashed line denotes the nominal coverage rate of 0.95.
Dimension d=50, p=0.9, α =0.01, and coordinates of β∗ are equidistantly spaced between 0 and 1 with
initializations at zero.
26α n=100,000 n=150,000 n=180,000 n=190,000 n=200,000
0.01 0.815 (0.0275) 0.875 (0.0234) 0.920 (0.0192) 0.940 (0.0168) 0.945 (0.0161)
p=0.9 0.05 0.800 (0.0283) 0.895 (0.0217) 0.940 (0.0168) 0.925 (0.0186) 0.930 (0.0180)
0.1 0.830 (0.0266) 0.915 (0.0197) 0.930 (0.0180) 0.955 (0.0146) 0.960 (0.0138)
α n=200,000 n=250,000 n=280,000 n=290,000 n=300,000
0.01 0.850 (0.0253) 0.900 (0.0212) 0.910 (0.0202) 0.915 (0.0197) 0.925 (0.0186)
p=0.5 0.05 0.860 (0.0245) 0.890 (0.0221) 0.920 (0.0192) 0.925 (0.0186) 0.945 (0.0161)
0.1 0.910 (0.0202) 0.900 (0.0212) 0.940 (0.0168) 0.950 (0.0154) 0.950 (0.0154)
Table2: Empiricalcoverageprobabilityof95%confidenceintervalsfrom200independentruns(withstandard
errors in the brackets). Dimension d=3.
α n=100,000 n=150,000 n=180,000 n=190,000 n=200,000
0.01 0.855 (0.0249) 0.885 (0.0226) 0.935 (0.0174) 0.935 (0.0174) 0.945 (0.0161)
p=0.9 α=0.025 0.810 (0.0278) 0.895 (0.0217) 0.935 (0.0174) 0.950 (0.0154) 0.960 (0.0138)
0.05 0.875 (0.0234) 0.895 (0.0217) 0.905 (0.0207) 0.900 (0.0212) 0.930 (0.0180)
α n=200,000 n=250,000 n=280,000 n=290,000 n=300,000
0.01 0.845 (0.0256) 0.890 (0.0221) 0.925 (0.0186) 0.930 (0.0180) 0.935 (0.0174)
p=0.5 0.025 0.900 (0.0212) 0.950 (0.0154) 0.945 (0.0161) 0.955 (0.0146) 0.955 (0.0146)
0.05 0.880 (0.0230) 0.950 (0.0154) 0.955 (0.0146) 0.960 (0.0138) 0.960 (0.0138)
Table3: Empiricalcoverageprobabilityof95%confidenceintervalsfrom200independentruns(withstandard
errors in the brackets). Dimension d=20.
α n=100,000 n=150,000 n=180,000 n=190,000 n=200,000
0.01 0.850 (0.0253) 0.935 (0.0174) 0.945 (0.0161) 0.925 (0.0186) 0.950 (0.0154)
p=0.9 0.0125 0.840 (0.0259) 0.945 (0.0161) 0.940 (0.0168) 0.940 (0.0168) 0.945 (0.0161)
0.02 0.750 (0.0306) 0.910 (0.0202) 0.925 (0.0186) 0.925 (0.0186) 0.945 (0.0161)
α n=200,000 n=250,000 n=280,000 n=290,000 n=300,000
0.01 0.805 (0.0280) 0.875 (0.0234) 0.895 (0.0217) 0.920 (0.0192) 0.910 (0.0202)
p=0.5 0.0125 0.735 (0.0230) 0.930 (0.0180) 0.920 (0.0192) 0.930 (0.0180) 0.925 (0.0186)
0.02 0.870 (0.0238) 0.910 (0.0202) 0.920 (0.0192) 0.915 (0.0197) 0.930 (0.0180)
Table4: Empiricalcoverageprobabilityof95%confidenceintervalsfrom200independentruns(withstandard
errors in the brackets). Dimension d=50.
Acknowledgements
WeiBiaoWu’sresearchispartiallysupportedbytheNSF(GrantsNSF/DMS-2311249,NSF/DMS-2027723).
Johannes Schmidt-Hieber has received funding from the Dutch Research Council (NWO) via the Vidi grant
VI.Vidi.192.021.
27References
Allmeier, S. and N. Gast (2024). “Computing the Bias of Constant-step Stochastic Approximation with
Markovian Noise”. In: arXiv preprint. arXiv:2405.14285.
Arora, R., P. Bartlett, P. Mianjy, and N. Srebro (2021). “Dropout: Explicit Forms and Capacity Control”.
In: Proceedings of the 38th International Conference on Machine Learning. PMLR, pp. 351–361.
Baldi,P.andP.Sadowski(2013).“UnderstandingDropout”.In:Advances in Neural Information Processing
Systems. Vol. 26. Curran Associates, Inc.
Berkes, I., W. Liu, and W. B. Wu (2014). “Koml´os–Major–Tusn´ady approximation under dependence”. In:
The Annals of Probability 42.2, pp. 794–817.
Brandt,A.(1986).“TheStochasticEquationY =A Y +B withStationaryCoefficients”.In:Advances
n+1 n n n
in Applied Probability 18.1, pp. 211–220.
Burkholder, D. L. (1988). “Sharp inequalities for martingales and stochastic integrals”. In: Colloque Paul
L´evy sur les processus stochastiques. Ast´erisque 157-158. Soci´et´e math´ematique de France, pp. 75–94.
Cavazza, J., P. Morerio, B. Haeffele, C. Lane, V. Murino, and R. Vidal (2018). “Dropout as a Low-Rank
Regularizer for Matrix Factorization”. In: Proceedings of the 21st International Conference on Artificial
Intelligence and Statistics. PMLR, pp. 435–444.
Chen, L. and W. B. Wu (2016). “Stability and asymptotics for autoregressive processes”. In: Electronic
Journal of Statistics 10.2, pp. 3723–3751.
Chen, X., J. D. Lee, X. T. Tong, and Y. Zhang (2020). “Statistical inference for model parameters in
stochastic gradient descent”. In: The Annals of Statistics 48.1, pp. 251–273.
Clara, G., S. Langer, and J. Schmidt-Hieber (2024). “Dropout Regularization Versus l2-Penalization in the
Linear Model”. In: Journal of Machine Learning Research 25.204, pp. 1–48.
Dean,J.,G.Corrado,R.Monga,K.Chen,M.Devin,M.Mao,M.a.Ranzato,A.Senior,P.Tucker,K.Yang,
Q. Le, and A. Ng (2012). “Large Scale Distributed Deep Networks”. In: Advances in Neural Information
Processing Systems. Vol. 25. Curran Associates, Inc.
Diaconis, P. and D. Freedman (1999). “Iterated Random Functions”. In: SIAM Review 41.1, pp. 45–76.
Dieuleveut, A., A. Durmus, and F. Bach (2020). “Bridging the gap between constant step size stochastic
gradient descent and Markov chains”. In: The Annals of Statistics 48.3, pp. 1348–1382.
Fang, Y. (2019). “Scalable statistical inference for averaged implicit stochastic gradient descent”. In: Scan-
dinavian Journal of Statistics 46.4, pp. 987–1002.
Fang, Y., J. Xu, and L. Yang (2019). “Online Bootstrap Confidence Intervals for the Stochastic Gradient
Descent Estimator”. In: Journal of Machine Learning Research 19, pp. 1–21.
28Gal,Y.andZ.Ghahramani(2016a).“ATheoreticallyGroundedApplicationofDropoutinRecurrentNeural
Networks”. In: Advances in Neural Information Processing Systems. Vol. 29. Curran Associates, Inc.
Gal, Y. and Z. Ghahramani (2016b). “Dropout as a Bayesian Approximation: Representing Model Uncer-
tainty in Deep Learning”. In: Proceedings of the 33rd International Conference on Machine Learning.
PMLR, pp. 1050–1059.
Gao, W. and Z.-H. Zhou (2016). “Dropout Rademacher complexity of deep neural networks”. In: Science
China Information Sciences 59.7, p. 072104.
Hinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov (2012). “Improving neural
networks by preventing co-adaptation of feature detectors”. In: arXiv preprint. arXiv:1207.0580.
Huo,D.,Y.Chen,andQ.Xie(2023).“BiasandExtrapolationinMarkovianLinearStochasticApproximation
with Constant Stepsizes”. In: arXiv preprint. arXiv:2210.00953.
Karimireddy,S.P.,S.Kale,M.Mohri,S.Reddi,S.Stich,andA.T.Suresh(2020).“SCAFFOLD:Stochastic
Controlled Averaging for Federated Learning”. In: Proceedings of the 37th International Conference on
Machine Learning. PMLR, pp. 5132–5143.
Karmakar, S. and W. B. Wu (2020). “Optimal Gaussian Approximation for Multiple Time Series”. In:
Statistica Sinica 30.3, pp. 1399–1417.
Kay,E.andA.Agarwal(2016).“DropConnectedneuralnetworktrainedwithdiversefeaturesforclassifying
heart sounds”. In: 2016 Computing in Cardiology Conference (CinC), pp. 617–620.
Kiefer, J. and J. Wolfowitz (1952). “Stochastic Estimation of the Maximum of a Regression Function”. In:
The Annals of Mathematical Statistics 23.3, pp. 462–466.
Koml´os,J.,P.Major,andG.Tusn´ady(1975).“AnapproximationofpartialsumsofindependentRV’-s,and
the sample DF. I”. In: Zeitschrift fu¨r Wahrscheinlichkeitstheorie und Verwandte Gebiete 32.1, pp. 111–
131.
Koml´os, J., P. Major, and G. Tusn´ady (1976). “An approximation of partial sums of independent RV’s, and
thesampleDF.II”.In:Zeitschriftfu¨rWahrscheinlichkeitstheorieundVerwandteGebiete 34.1,pp.33–58.
Krizhevsky, A., I. Sutskever, and G. E. Hinton (2012). “ImageNet Classification with Deep Convolutional
Neural Networks”. In: Advances in Neural Information Processing Systems. Vol. 25. Curran Associates,
Inc.
Lahiri, S. N. (1999). “Theoretical comparisons of block bootstrap methods”. In: The Annals of Statistics
27.1, pp. 386–404.
Lahiri, S. N. (2003). Resampling Methods for Dependent Data. Springer Series in Statistics. New York, NY:
Springer.
29Liang, T. and W. J. Su (2019). “Statistical Inference for the Population Landscape via Moment-Adjusted
Stochastic Gradients”. In: Journal of the Royal Statistical Society Series B: Statistical Methodology 81.2,
pp. 431–456.
McAllester,D.(2013).“APAC-Bayesiantutorialwithadropoutbound”.In:arXivpreprint.arXiv:1307.2118.
Mianjy, P. and R. Arora (2019). “On Dropout and Nuclear Norm Regularization”. In: Proceedings of the
36th International Conference on Machine Learning. PMLR, pp. 4575–4584.
Mianjy, P. and R. Arora (2020). “On Convergence and Generalization of Dropout Training”. In: Advances
in Neural Information Processing Systems. Vol. 33. Curran Associates, Inc., pp. 21151–21161.
Mianjy, P., R. Arora, and R. Vidal (2018). “On the Implicit Bias of Dropout”. In: Proceedings of the 35th
International Conference on Machine Learning. PMLR, pp. 3540–3548.
Pflug,G.C.(1986).“StochasticMinimizationwithConstantStep-Size:AsymptoticLaws”.In:SIAMJournal
on Control and Optimization 24.4, pp. 655–666.
Pham, H. and Q. Le (2021). “AutoDropout: Learning Dropout Patterns to Regularize Deep Networks”. In:
Proceedings of the AAAI Conference on Artificial Intelligence 35.11, pp. 9351–9359.
Polyak, B. T. and A. B. Juditsky (1992). “Acceleration of Stochastic Approximation by Averaging”. In:
SIAM Journal on Control and Optimization 30.4, pp. 838–855.
Rio, E. (2009). “Moment inequalities for sums of dependent random variables under projective conditions”.
In: Journal of Theoretical Probability 22.1, pp. 146–163.
Robbins, H. and S. Monro (1951). “A Stochastic Approximation Method”. In: The Annals of Mathematical
Statistics 22.3, pp. 400–407.
Ruppert, D. (1988). Efficient Estimations from a Slowly Convergent Robbins-Monro Process. Technical re-
port, Cornell University Operations Research and Industrial Engineering.
Senen-Cerda,A.andJ.Sanders(2022).“AsymptoticConvergenceRateofDropoutonShallowLinearNeural
Networks”. In: SIGMETRICS Perform. Eval. Rev. 50.1, pp. 105–106.
Senen-Cerda, A. and J. Sanders (2023). “Almost Sure Convergence of Dropout Algorithms for Neural Net-
works”. In: arXiv preprint. arXiv:2002.02247.
Shao, X. and W. B. Wu (2007). “Asymptotic spectral theory for nonlinear time series”. In: The Annals of
Statistics 35.4, pp. 1773–1801.
Srivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov (2014). “Dropout: A Simple
Way to Prevent Neural Networks from Overfitting”. In: Journal of Machine Learning Research 15.56,
pp. 1929–1958.
Su, W. and Y. Zhu (2023). “HiGrad: Uncertainty Quantification for Online Learning and Stochastic Ap-
proximation”. In: Journal of Machine Learning Research 24, pp. 1–53.
30Wager, S., S. Wang, and P. S. Liang (2013). “Dropout Training as Adaptive Regularization”. In: Advances
in Neural Information Processing Systems. Vol. 26. Curran Associates, Inc.
Wan, L., M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus (2013). “Regularization of Neural Networks us-
ing DropConnect”. In: Proceedings of the 30th International Conference on Machine Learning. PMLR,
pp. 1058–1066.
Wei, C., S. Kakade, and T. Ma (2020). “The Implicit and Explicit Regularization Effects of Dropout”. In:
Proceedings of the 37th International Conference on Machine Learning. PMLR, pp. 10181–10192.
Wu,H.andX.Gu(2015).“Towardsdropouttrainingforconvolutionalneuralnetworks”.In:NeuralNetworks
71, pp. 1–10.
Wu, W. B. (2005). “Nonlinear system theory: Another look at dependence”. In: PNAS 102.40, pp. 14150–
14154.
Wu, W. B. and X. Shao (2004). “Limit Theorems for Iterated Random Functions”. In: Journal of Applied
Probability 41.2, pp. 425–436.
Xiao,H.andW.B.Wu(2011).“ASingle-PassAlgorithmforSpectrumEstimationWithFastConvergence”.
In: IEEE Transactions on Information Theory 57.7, pp. 4720–4731.
Yu, L., K. Balasubramanian, S. Volgushev, and M. A. Erdogdu (2021). “An Analysis of Constant Step Size
SGD in the Non-convex Regime: Asymptotic Normality and Bias”. In: Advances in Neural Information
Processing Systems. Curran Associates, Inc.
Zhai, K. and H. Wang (2018). “Adaptive Dropout with Rademacher Complexity Regularization”. In: Inter-
national Conference on Learning Representations.
Zhong, Y., J. Li, and S. Lahiri (2024). “Probabilistic Guarantees of Stochastic Recursive Gradient in Non-
convex Finite Sum Problems”. In: Advances in Knowledge Discovery and Data Mining. Springer Nature
Singapore, pp. 142–154.
Zhu, W., X. Chen, and W. B. Wu (2023). “Online Covariance Matrix Estimation in Stochastic Gradient
Descent”. In: Journal of the American Statistical Association 118.541, pp. 393–404.
Zinkevich, M., M. Weimer, L. Li, and A. Smola (2010). “Parallelized Stochastic Gradient Descent”. In:
Advances in Neural Information Processing Systems. Vol. 23. Curran Associates, Inc.
31A Some Useful Lemmas
Lemma 6 (Burkholder 1988; Rio 2009). Let q > 1,q′ = min{q,2}, and M = (cid:80)T ξ , where ξ are
T t=1 t t
martingale differences with a finite q-th moment. Then
T
(cid:0)E∥M ∥q(cid:1)q′/q ≤Kq′(cid:88)(cid:0)E∥ξ ∥q(cid:1)q′/q
, where K
=max{(q−1)−1,(cid:112)
q−1}.
T 2 q T 2 q
t=1
Lemma 7 (Clara et al. 2024). For any matrices A and B in Rd×d, p ∈ (0,1), and a diagonal matrix
D ∈Rd×d, the following results hold:
(i) AD =AD, DA=DA, and A =pA=A ;
p p
If in addition, the diagonal matrix D is random and independent of A and B, with the diagonal entries
i.i.d.
satisfying D ∼ Bernoulli(p), 1≤i≤d, then,
ii
(ii) E[DAD]=pA , where A =pA+(1−p)Diag(A);
p p
(iii) E[DADBD]=pA B +p2(1−p)Diag(AB), where A=A−Diag(A);
p p
(iv) E[DADBDCD] = pA B C +p2(1−p)(cid:2) Diag(AB C)+A Diag(BC)+Diag(AB)C +(1−p)A⊙
p p p p p p
⊤ (cid:3)
B ⊙C , where ⊙ denotes the Hadamard product.
Lemma 8 (Properties of operator norm). Let A = (a ) be a real d×d matrix. View A as a linear
ij 1≤i,j≤d
map Rd (cid:55)→Rd and denote its operator norm by ∥A∥.
(i) (Inequalities for variants of A). ∥Diag(A)∥ ≤ ∥A∥, ∥A ∥ ≤ ∥A∥, and if in addition, A is positive
p
semi-definite, then also ∥A∥≤∥A∥;
(ii) (Frobenius norm). ∥A∥ = sup ∥Av∥ ≤ ∥A∥ , where ∥A∥ denotes the Frobenius norm,
v∈Rd,∥v∥2=1 2 F F
i.e., ∥A∥
=(cid:0)(cid:80)d
|a
|2(cid:1)1/2
;
F i,j=1 ij
(iii) (Largest magnitude of eigenvalues). For a symmetric d×d matrix A, max |λ (A)| = ∥A∥,
1≤i≤d i
where λ (A) denotes the i-th largest eigenvalue of A. If in addition, A is positive semi-definite, then also
i
λ (A)=∥A∥.
max
Proof of Lemma 8. The inequalities in (i) follow directly from Lemma 19 in Clara et al. (2024). For (ii), we
notice that for any unit vector v ∈ Rd, one can find a basis {e ,...,e } and write v into v = (cid:80)d c e ,
1 d j=1 j j
with e ∈Rd, and the real coefficients c satisfying (cid:80)d c2 =1. Then, it follows from the orthogonality of
j j j=1 j
e and the Cauchy-Schwarz inequality that
j
(cid:13)(cid:88)d (cid:13)2 (cid:16)(cid:88)d (cid:17)2 (cid:16)(cid:88)d (cid:17)(cid:16)(cid:88)d (cid:17) (cid:88)d
∥Av∥2 =(cid:13) c Ae (cid:13) ≤ |c |∥Ae ∥ ≤ |c |2 ∥Ae ∥2 = ∥Ae ∥2 =∥A∥2. (65)
2 (cid:13) j j(cid:13) j j 2 j j 2 j 2 F
2
j=1 j=1 j=1 j=1 j=1
Since this result holds for any unit vector v ∈Rd, the desired result in (ii) is achieved.
32To see (iii), for any eigenvalue λ (A), we denote its associated unit eigenvector by v. Then, ∥Av∥ =
i 2
|λ (A)|∥v∥ =|λ (A)|, which further yields |λ (A)|≤sup ∥Av∥ =∥A∥, uniformly over i. Hence,
i 2 i i v∈Rd,∥v∥2=1 2
the inequality can be obtained. If in addition, A is symmetric, then A can be diagonalized by an or-
thogonal matrix Q and a diagonal matrix Λ such that A = Q⊤ΛQ. Therefore, sup ∥Av∥ =
v∈Rd,∥v∥2=1 2
max |λ (A)|, which completes the proof.
1≤i≤d i
B Proofs in Section 3.1
This section is devoted to the proofs of the geometric-moment contraction (GMC) for the dropout iterates
with gradient descent (GD), i.e., {β˜ k(α)−β} k∈N. We first extend the results in Wu and Shao (2004) to the
cases where the inputs of iterated random functions are i.i.d. random matrices. Then, we present the proof
for the sufficient condition of the GMC in terms of the constant learning rate α in Lemma 1, and showcase
the GMC of {β˜ k(α)−β} k∈N in Theorem 1.
B.1 GMC – Random Matrix Version
Let (Y,ρ) be a complete and separable metric space, endowed with its Borel sets Y. Consider an iterated
random function on the state space Y ⊂Rd, for some fixed d≥1, with the form
y =f(y ,X )=f (y ), k ∈N, (66)
k k−1 k Xk k−1
where f(y,·) is the y-section of a jointly measurable function f : Y ×X (cid:55)→ Y; the random matrices X ,
k
k ∈N,takevaluesinasecondmeasurablespaceX ⊂Rd×d,andareindependentlydistributedwithidentical
marginal distribution H. The initial point y ∈Y is independent of all X .
0 k
We are interested in the sufficient conditions on f (y) such that there is a unique stationary probability
X
π on Y with y ⇒π as k →∞. To this end, define a composite function
k
y (y)=f ◦f ◦···◦f (y), for y ∈Y. (67)
k Xk Xk−1 X1
We say that y is geometric-moment contracting if for any two independent random vectors y ∼ π and
k
y′ ∼π in Y, there exist some q >0, C >0 and r ∈(0,1), such that for all k ∈N,
q q
E(cid:2) ρq(cid:0) y (y),y (y′)(cid:1)(cid:3) ≤C rk. (68)
k k q q
Wu and Shao (2004) provided the sufficient conditions for (68) when X are random variables and y and
k k
33y are one-dimensional. Their results can be directly extended to the random matrix version and we state
them here for the completeness of this paper.
Assumption 3 (Finite moment). Assume that there exists a fixed vector y∗ ∈Y and some q >0 such that
(cid:90)
I(q,y∗):=E (cid:2) ρq(cid:0) y∗,f (y∗)(cid:1)(cid:3) = ρq(cid:0) y∗,f (y∗)(cid:1) H(dX)<∞.
X∼H X X
X
Assumption 4 (Stochastic Lipschitz continuity). Assume that there exists some q > 0 and some y ∈ Y
0
such that
E (cid:2) ρq(cid:0) f (y ),f (y′)(cid:1)(cid:3)
L := sup X∼H X 0 X 0 <1,
q ρq(y ,y′)
y 0′∈Y,y0̸=y 0′ 0 0
where L =L (y ) is a local Lipschitz constant.
q q 0
Corollary 4 (GMC – random matrix version). Suppose that Assumptions 3 and 4 hold. Define a backward
iteration process
z (y)=z (f (y))=f ◦f ◦···◦f (y), for y ∈Y. (69)
k k−1 Xk X1 X2 Xk
D
Then, z (y)=y (y), and there exists a random vector z ∈Y such that for any y ∈Y,
k k ∞
a.s.
z (y) → z .
k ∞
Thelimitz ismeasurablewithrespecttotheσ-algebraσ(X ,X ,...)anddoesnotdependony. Inaddition,
∞ 1 2
E(cid:2) ρq(cid:0) z (y),z (cid:1)(cid:3) ≤Crk, k ∈N, (70)
k ∞ q
where the constant C >0 only depends on q and y∗ in Assumption 3, L in Assumption 4 and y . Conse-
q 0
quently, (68) holds.
Remark 2 (Backward iteration). We shall comment on the intuition for defining the backward iteration z
k
in (69). Recall the i.i.d. random samples X ,...,X . Clearly, for any fixed initial point y ∈ Y, for all
1 n 0
k ∈N, we have the relations
(cid:0) (cid:1)
y (y )=f y (y ) ,
k+1 0 Xk+1 k 0
(cid:0) (cid:1)
z (y )=z f (y ) .
k+1 0 k Xk+1 0
To prove the existence of the limit for y =f ◦f ◦···◦f (y ), we need to make use of the contracting
k Xk Xk−1 X1 0
property of the function f (·) stated in Assumption 4. However, we cannot directly apply it to the forward
X
34iteration, becausebytheMarkovproperty, giventhepresentpositionofthechain, theconditionaldistribution
of the future does not depend on the past. This indicates
E(cid:2) ρq(cid:0) y (y∗),y (y∗)(cid:1)(cid:3) =E(cid:2)E(cid:2) ρq(cid:0) f (cid:0) y (y∗)(cid:1) ,y (y∗)(cid:1) |X (cid:3)(cid:3) , (71)
k+1 k Xk+1 k k k+1
where the two parts inside of ρ(·,·) are operated by two different functions, which are f (·) and f ◦
Xk+1 Xk
f ◦ ··· ◦ f (·) respectively. In fact, as pointed out by Diaconis and Freedman (1999), the forward
Xk−1 X1
iteration y moves ergodically through Y, which behaves quite differently from the backward iteration z (·)=
k k
f ◦f ◦···◦f (·)in(69),whichdoesconvergetoalimit. Toseethis,wenotethatbyAssumptions3and4,
X1 X2 Xk
there exists some y∗ ∈Y such that
E(cid:2) ρq(cid:0) z (y∗),z (y∗)(cid:1)(cid:3) =E(cid:2)E(cid:2) ρq(cid:0) z (cid:0) f (y∗)(cid:1) ,z (y∗)(cid:1) |X (cid:3)(cid:3)
k+1 k k Xk+1 k k+1
≤LkE(cid:2) ρq(cid:0)
f
(y∗),y∗(cid:1)(cid:3)
q Xk+1
=LkI(q,y∗), (72)
q
which is summable over k by Assumption 4. Since Y is a complete space, we can mimic the idea of a Cauchy
a.s.
sequence to prove the existence of the limit z and further show z → z , by applying the Borel-Cantelli
∞ k ∞
D
lemma. Since X ,...,X are i.i.d. and thus exchangeable, we have z (y ) = y (y ). Hence, we can show
1 n k 0 k 0
that y also converges to z in distribution.
k ∞
Proof of Corollary 4. Letq ∈(0,1]suchthatbothAssumptions3and4hold. Wewillonlyshowthedesired
results for this choice of q, since if Assumptions 3 and 4 are satisfied for some q > 1, then they are also
valid for all q ≤ 1 by H¨older’s inequality (Wu and Shao 2004). Recall the definition of integral I(q,y∗) in
Assumption 3. Let y ∈Y satisfy Assumption 4. Then,
0
I(q,y
)=E(cid:2) ρq(cid:0)
y ,f (y
)(cid:1)(cid:3)
0 0 X 0
≤E(cid:2) ρ(y ,y∗)+ρ(cid:0) y∗,f (y∗)(cid:1) +ρ(cid:0) f (y∗),f (y )(cid:1)(cid:3)q
0 X X X 0
≤ρq(y ,y∗)+I(q,y∗)+E(cid:2) ρq(cid:0) f (y∗),f (y )(cid:1)(cid:3)
0 X X 0
≤ρq(y ,y∗)+I(q,y∗)+L ρq(y∗,y )<∞,
0 q 0
wherethefirstinequalityfollowsfromthetriangleinequality,thesecondoneisbyAssumption4andJensen’s
inequality, and the last one is due to Assumption 3. A similar argument as in (72) yields
E(cid:2) ρq(cid:0) z (y ),z (y )(cid:1)(cid:3) ≤LkI(q,y )=:δ , (73)
k+1 0 k 0 q 0 k
35where δ =δ (q,y ) solely depends on k, q, L and y . By Markov’s inequality, we have
k k 0 q 0
P(cid:16) ρ(cid:0) z (y ),z (y )(cid:1) ≥δ1/(2q)(cid:17) ≤δ1/2. (74)
k+1 0 k 0 k k
Since (cid:80)∞ δ1/2 <∞, it follows from the first Borel-Cantelli lemma that
k=1 k
P(cid:16) ρ(cid:0) z (y ),z (y )(cid:1) ≥δ1/(2q)for infinitely many k(cid:17) =0. (75)
k+1 0 k 0 k
Again, since δ1/2 is summable, z is a Cauchy sequence in space Y, which together with the completeness
k k
of Y gives that almost surely, there exists a random vector z ∈Y such that
∞
a.s.
z (y ) → z , as k →∞,
k 0 ∞
where z is σ(X ,X ,...)-measurable. Let π be the probability distribution of z .
∞ 1 2 ∞
Furthermore, it follows from the triangle inequality and Jensen’s inequality that for any fixed y ∈Y,
0
∞
E(cid:2) ρq(cid:0)
z (y ),z
(cid:1)(cid:3) ≤E(cid:104)(cid:88) ρ(cid:0)
z (y ),z (y
)(cid:1)(cid:105)q
k 0 ∞ k+1+l 0 k+l 0
l=0
∞
≤(cid:88) E(cid:2) ρq(cid:0)
z (y ),z (y
)(cid:1)(cid:3)
k+1+l 0 k+l 0
l=0
≤δ /(1−L ), (76)
k q
For any y ∈Y, by Assumption 4 and triangle inequality,
E(cid:2) ρq(cid:0)
z (y),z
(cid:1)(cid:3) ≤E(cid:2) ρq(cid:0)
z (y),z (y
)(cid:1)(cid:3) +E(cid:2) ρq(cid:0)
z (y ),z
(cid:1)(cid:3)
k ∞ k k 0 k 0 ∞
≤Lkρq(y ,y)+δ /(1−L ). (77)
q 0 k q
Recall that δ = LkI(q,y ). Let C = I(q,y )/(1−L )+ρq(y ,y) and we have shown result (70) with
k q 0 0 q 0
r = L . Since Crk in (70) is summable over k, it again follows from Borel-Cantelli lemma that for any
q q q
y ∈Y,
a.s.
z (y) → z , as k →∞,
k ∞
and therefore, the limit
v (y)= lim f ◦f ◦···◦f (y) (78)
k
m→∞
Xk+1 Xk+2 Xk+m
36exists almost surely.
Finally, we notice that for any two independent random vectors y ∼π and y′ ∼π,
E(cid:2) ρq(cid:0)
y (y),y
(y′)(cid:1)(cid:3) ≤E(cid:2) ρq(cid:0)
y (y),y (y
)(cid:1)(cid:3) +E(cid:2) ρq(cid:0)
y (y ),y
(y′)(cid:1)(cid:3)
k k k k 0 k 0 k
=2E(cid:2) ρq(cid:0)
z (v ),z (y
)(cid:1)(cid:3)
k k k 0
=2E(cid:2) ρq(cid:0)
z ,z (y
)(cid:1)(cid:3)
≤2δ /(1−L ), (79)
∞ k 0 k q
where the first equation follows from the observation that v has the identical distribution as z =
k ∞
z (v (y)) ∼ π and is independent of i.i.d. random matrices X ,...,X because v as defined in (78) only
k k 1 k k
depends on X for large i≥k+1. The desired result in (68) has been achieved.
i
The recursion y =f(y ,X ) is only defined for positive integers k. Nevertheless, Corollary 4 guaran-
k k−1 k
tees that for k =0,−1,..., the relation y =f(y ,X ) also holds. See Remark 2 in Wu and Shao (2004)
k k−1 k
for a simple way to define y when k = 0,−1,... in the one-dimensional case. The vector versions can be
k
similarly constructed.
B.2 Proof of Lemma 1
Proof of Lemma 1. Let D be a dropout matrix with the same distribution as D . Since X = X⊤X is
1
positive semi-definite and by assumption α∥X∥ < 2, we have −I < I −αDXD ≤ I and consequently
d d d
∥I −αDXD∥ ≤ 1. Thus for a unit vector v, ∥(I −αD XD )v∥ ≤ 1. This means that for q ≥ 2, we can
d d k k 2
use ∥·∥q =∥·∥2∥·∥q−2 to bound
2 2 2
rq ≤ sup E(cid:13) (cid:13)(cid:0) I −αDXD(cid:1) v(cid:13) (cid:13)2 = sup v⊤E(cid:104)(cid:0) I −αDXD(cid:1)2(cid:105) v =(cid:13) (cid:13)E(cid:104)(cid:0) I −αDXD(cid:1)2(cid:105)(cid:13) (cid:13).
α,q (cid:13) d (cid:13) d (cid:13) d (cid:13)
v∈Rd:∥v∥2=1 2 v∈Rd:∥v∥2=1
(80)
Forad×dandpositivesemi-definitematrixA,wehaveA2 ≤∥A∥A.Toseethis,letv betheeigenvectorsof
j
Awithcorrespondingeigenvaluesλ .Anyvectorw canbewrittenasw =γ v +...+γ v withcoefficients
j 1 1 d d
γ ,...,γ . Now w⊤A2w = γ2λ2 +...+γ2λ2 ≤ (max λ )(γ2λ +...+γ2λ ) = w⊤∥A∥Aw. Since w was
1 d 1 1 d d j j 1 1 d d
arbitrary, this proves A2 ≤∥A∥A. Moreover, recall that D is a diagonal matrix with diagonal entries 0 and
k
1. Thus D2 =D ≤I . Because X is positive semi-definite and by assumption ∆:=2−α∥X∥>0, we have
k k d
α2D XD2XD ≤α2D X2D ≤α2D ∥X∥XD ≤(2−∆)αD XD . Thus,
1 1 1 1 1 1 1 1 1
(I −αD XD )2 =I −2αD XD +α2D XD2XD ≤I −∆αD XD .
d 1 1 d 1 1 1 1 1 d 1 1
37TakingexpectationandusingLemma7(ii)yieldsE(cid:2) (I −αD XD )2(cid:3) ≤I −∆αpX .ThefactthatE(cid:2) (I −
d 1 1 d p d
αD XD )2(cid:3) ispositivesemi-definiteimpliesthat∥E(cid:2) (I −αD XD )2(cid:3) ∥isboundedbythelargesteigenvalue
1 1 d 1 1
of I −∆αpX . By definition, X = pX+(1−p)Diag(X) ≥ (1−p)min X I . By assumption the design
d p p j jj d
is in reduced form which implies that min X > 0. This shows that X is positive definite and the largest
j jj p
eigenvalueofI −∆αpX mustbestrictlysmallerthan1.Thisimplies∥E[(I −αD XD )2]∥<1.Combined
d p d 1 1
with (80) this proves r <1.
α,q
If Lemma 1 holds for some q ≥2, then by H¨older’s inequality, it also holds for all 1<q <2. To see this,
consider a unit vector v ∈Rd and set r(q):=(cid:0)E∥(I −αD XD )v∥q(cid:1)1/q . Then for any 1<q′ <q, it follows
d 1 1 2
from H¨older’s inequality that
r(q′)q′ =E(cid:13) (cid:13)(cid:0) I −αD XD (cid:1) v(cid:13) (cid:13)q′ ≤(cid:16) E(cid:13) (cid:13)(cid:0) I −αD XD (cid:1) v(cid:13) (cid:13)q(cid:17)q′/q =r(q)q′ <1. (81)
(cid:13) d 1 1 (cid:13) (cid:13) d 1 1 (cid:13)
2 2
B.3 Proof of Theorem 1
Proof of Theorem 1. Recall the recursive estimator β˜ defined in (6). Write A = A (α) = I −αD XD .
k k k d k k
Weconsiderarbitraryd-dimensionalinitializationvectorsβ˜ , β˜′ ∈Rd andwriteβ˜ andβ˜′ fortherespective
0 0 k k
iterates(sharingthesamedropoutmatrices). Nowβ˜ −β˜′ =A (β˜ −β˜′ )=:A ∆ ,withindependent
k k k k−1 k−1 k k−1
A and ∆ . By Lemma 1, r := sup
(cid:0)E∥A v∥q(cid:1)1/q
< 1, and thus, for any fixed vector v,
k k−1 v∈Rd,∥v∥2=1 k 2
E∥A v∥q ≤rq∥v∥q. Due to the independence between A and ∆ , it follows from the tower rule and the
k 2 2 k k−1
condition above that
E∥β˜ −β˜′∥q =E∥A ∆ ∥q =E(cid:2)E[∥A ∆ ∥q |∆ ](cid:3) ≤E(cid:2) rq∥∆ ∥q(cid:3) =rqE∥∆ ∥q.
k k 2 k k−1 2 k k−1 2 k−1 k−1 2 k−1 2
Since A (α) are i.i.d. random matrices induction on k yields the claimed geometric-moment contraction
k
(cid:0)E∥β˜ (α)−β˜′(α)∥q(cid:1)1/q ≤rk ∥β˜ −β˜′∥ .
k k 2 α,q 0 0 2
Finally, by Corollary 4, this geometric-moment contraction implies the existence of a unique stationary
distribution π˜ of the GD dropout sequence β˜ (α). This completes the proof.
α k
38C Proofs in Section 3.2
C.1 Proof of Lemma 2
Proof of Lemma 2. Sinceβ˜◦(α)isstationaryandE[b ]=0by(100),itfollowsthatβ˜◦(α)−β˜andβ†(α)−β˜
k k k k
both have zero mean, and thus E[δ (α)]=0.
k
To prove the second claim, we first note that
δ (α)=(I −αD XD )(β˜◦ (α)−β˜)+b (α)−(cid:2) (I −αpX )(β† (α)−β˜)+b (α)(cid:3)
k d k k k−1 k d p k−1 k
=(I −αpX )δ +α(pX −D XD )(β˜◦ −β˜) (82)
d p k−1 p k k k−1
is a stationary sequence. By induction on k, we can write δ (α) into
k
(cid:104) (cid:105)
δ (α)=α (pX −D XD )(β˜◦ −β˜)+···+(pX −D XD )(I −αpX )k−1(β˜◦−β˜)+···
k p k k k−1 p 1 1 d p 0
∞
=α(cid:88) (pX −D XD )(I −αpX )i−1(β˜◦ −β˜)
p k−i+1 k−i+1 d p k−i
i=1
∞
(cid:88)
=:α M (α). (83)
k−i
i=1
For any k ∈ N, {M (α)} is a sequence of martingale differences with respect to the filtration F =
k−i i≥1 k−i
σ(...,D ,D ), since the dropout matrix D is independent of β˜◦ and β˜. Therefore, we can apply
k−i−1 k−i k k−1
Burkholder’s inequality in Lemma 6 to
(cid:80)∞
M (α), and obtain, for q ≥2,
i=1 k−i
(cid:18) E(cid:13) (cid:13)(cid:88)∞
M
(α)(cid:13) (cid:13)q(cid:19)1/q =(cid:16) E(cid:13) (cid:13)(cid:88)∞
(I −αpX )i−1(pX −D XD )(β˜◦
−β˜)(cid:13) (cid:13)q(cid:17)1/q
(cid:13) k−i (cid:13) (cid:13) d p p k−i+1 k−i+1 k−i (cid:13)
2 2
i=1 i=1
∞
≲(cid:104)(cid:88)(cid:0)E(cid:13) (cid:13)(I d−αpX p)i−1(pX p−D k−i+1XD k−i+1)(β˜ k◦ −i−β˜)(cid:13) (cid:13)q 2(cid:1)2/q(cid:105)1/2
i=1
∞
≤(cid:104)(cid:88) ∥I d−αpX p∥2(i−1)(cid:0)E(cid:13) (cid:13)(pX p−D k−i+1XD k−i+1)(β˜ k◦ −i−β˜)(cid:13) (cid:13)q 2(cid:1)2/q(cid:105)1/2 ,
i=1
where the constant in ≲ here and the rest of the proof only depends on q unless it is additionally specified.
Weshallproceedtheproofwithtwomainsteps. First,weshowthebound∥I −αpX ∥<1fortheoperator
d p
normandthus(cid:80)∞ i=1∥I d−αpX p∥2(i−1) <∞. Second,weprovideaboundforE(cid:13) (cid:13)(pX p−D kXD k)(β˜ k◦ −1−β˜)(cid:13) (cid:13)q
2
uniformly over k.
Step1. Sinceα∥X∥<2,itfollowsfromLemma8(i)thatα∥X ∥≤α∥X∥<2. Moreover,theassumption
p
thatthedesignmatrixX hasnozerocolumnsguaranteesthatalldiagonalentriesofXarepositiveandthus
Diag(X) > 0. Together with p < 1, this lead to X = pX+(1−p)Diag(X) ≥ (1−p)Diag(X) > 0. We thus
p
39have −I <I −αpX <I . Consequently, ∥I −αpX ∥<1 and
d d p d d p
∞
(cid:88) ∥I −αpX ∥2(i−1) = 1 =O(cid:0) α−1(cid:1) . (84)
d p 1−∥I −αpX ∥2
d p
i=1
Step 2. Next, weshallboundthetermE(cid:13) (cid:13)(pX p−D iXD i)(β˜ i◦ −1−β˜)(cid:13) (cid:13)q 2. Wefirstconsiderthecaseq =2.
Denote M =pX −D XD . Using that E[D XD ]=pX , we find E[M ]=0 and by the tower rule,
i p i i i i p i
E(cid:13) (cid:13)(pX p−D iXD i)(β˜ i◦ −1−β˜)(cid:13) (cid:13)2
2
=E[(β˜ i◦ −1−β˜)⊤M⊤
i
M i(β˜ i◦ −1−β˜)]
=E(cid:2)E(cid:2) (β˜◦ −β˜)⊤M⊤M (β˜◦ −β˜)|F (cid:3)(cid:3)
i−1 i i i−1 i−1
≤∥E[M⊤M ]∥·E∥β˜◦ −β˜∥2. (85)
i i i−1 2
By Lemma 3, we have E∥β˜◦ −β˜∥2 = O(α). We only need to bound the operator norm ∥E[M⊤M ]∥. To
i−1 2 i i
this end, we use again E[D XD ] = pX and moreover E[D XD XD ] = pX2 +p2(1−p)Diag(XX), which
i i p i i i p
yields,
E[M⊤M ]=E[(pX −D XD )⊤(pX −D XD )]
i i p i i p i i
=p2X2−2p2X2+pX2+p2(1−p)Diag(XX)
p p p
=p2(1−p)Diag(XX). (86)
Recall that X = X⊤X, where X is the fixed design matrix. Then, by Lemma 8 (i) and the sub-
multiplicativity of the operator norm, we have ∥Diag(XX)∥ ≤ ∥XX∥ ≤ ∥X∥∥X∥ ≤ ∥X∥2. As a direct
consequence, ∥E[M⊤M ]∥≤p2(1−p)∥X∥2 <∞, which together with Lemma 3 and (84) gives
i i
(cid:16) (cid:13) (cid:88)∞ (cid:13)2(cid:17)1/2 (cid:16)(cid:88)∞ (cid:17)1/2
E∥δ (α)∥ = E(cid:13)α M (α)(cid:13) ≲α ∥I −αpX ∥2(i−1)α =O(α),
k 2 (cid:13) k−i (cid:13) d p
2
i=1 i=1
uniformly over k. For the case with q >2, we can similarly apply the tower rule and obtain
E(cid:13) (cid:13)(pX p−D iXD i)(β˜ i◦ −1−β˜)(cid:13) (cid:13)q
2
=E(cid:104) E(cid:2) ∥(pX −D XD )(β˜◦ −β˜)∥q |F (cid:3)(cid:105)
p i i i−1 2 i−1
≤ sup E∥(pX −D XD )v∥q ·E∥β˜◦ −β˜∥q, (87)
p i i 2 i−1 2
v∈Rd,∥v∥2=1
where the last inequality can be achieved by writing β˜◦ −β˜ = ∥β˜◦ −β˜∥ v. Here v is the unit vector
i−1 i−1 2
(β˜◦ −β˜)/∥β˜◦ −β˜∥ with ∥v∥ =1. In addition, recall the Frobenius norm denoted by ∥·∥ . It follows
i−1 i−1 2 2 F
40from Lemma 8 (i) and (ii) that
sup E∥(pX −D XD )v∥q ≤E∥pX −D XD ∥q
p i i 2 p i i F
v∈Rd,∥v∥2=1
≲E(cid:0) ∥pX ∥q +∥D XD ∥q(cid:1)
p F i i F
≤(pq+1)∥X∥q <∞,
where the constant in ≲ only depends on q. Combining this with the inequality (84), we obtain
(E∥δ (α)∥q)1/q =O(α), completing the proof.
k 2
C.2 Proof of Lemma 3
Proof of Lemma 3. Recall that by applying induction on k to Equation (6), we can rewrite the GD dropout
iterates β˜ (α) into
k
β˜ (α)−β˜=A (α)(β˜ (α)−β˜)+b (α)
k k k−1 k
k−1 k k
=(cid:88)(cid:16) (cid:89) A (α)(cid:17) b (α)+(cid:16)(cid:89) A (α)(cid:17) (β˜ (α)−β˜),
j k−i j 0
i=0 j=k−i+1 j=1
where we set
(cid:81)k
A (α) = I . Following Brandt (1986), since both A and b are i.i.d. random coeffi-
j=k+1 j d k k
cients, the stationary solution {β˜ k◦(α)−β˜} k∈N of this recursion can be written into
β˜◦(α)−β˜=A (α)(β˜◦ (α)−β˜)+b (α)
k k k−1 k
∞ k
(cid:88)(cid:16) (cid:89) (cid:17)
= A (α) b (α)
j k−i
i=0 j=k−i+1
∞ k
=α(cid:88)(cid:104) (cid:89) (I −αD XD )(cid:105) D X(pI −D )β˜
d j j k−i d k−i
i=0 j=k−i+1
∞
=:α(cid:88) M˜ (α). (88)
i,k
i=0
We observe that, for any k ∈ N, {M˜ i,k(α)} i∈N is a sequence of martingale differences with respect to the
filtration F =σ(D ,D ,...). Hence, it follows from Burkholder’s inequality in Lemma 6 that, for
k−i k−i k−i−1
q ≥2,
(cid:0)E∥β˜◦(α)−β˜∥q(cid:1)1/q =α(cid:16) E(cid:13) (cid:13)(cid:88)∞ M˜ (α)(cid:13) (cid:13)q(cid:17)1/q
k 2 (cid:13) i,k (cid:13)
2
i=0
41∞
≲α(cid:16)(cid:88)(cid:0)E∥M˜ (α)∥q(cid:1)2/q(cid:17)1/2
, (89)
i,k 2
i=0
where the constant in ≲ only depends on q. Recall H defined in (96), and we define a d×d matrix B by
k i,k
k k
(cid:104) (cid:89) (cid:105) (cid:16) (cid:89) (cid:17)
B = (I −αD XD ) D X(pI −D )= A H . (90)
i,k d j j k−i d k−i j k−i
j=k−i+1 j=k−i+1
This random matrix is independent of β˜. For q =2, by the tower rule, we have
E∥M˜ (α)∥2 =E(cid:2)E(cid:2) β˜⊤B⊤ B β˜|F (cid:3)(cid:3)
i,k 2 i,k i,k k
=E(cid:2)E(cid:2) tr(β˜β˜⊤B⊤ B )|F (cid:3)(cid:3)
i,k i,k k
=E(cid:2) tr(cid:0)E(cid:2) β˜β˜⊤B⊤ B |F (cid:3)(cid:1)(cid:3)
i,k i,k k
=E(cid:2) tr(cid:0)E[β˜β˜⊤]B⊤
B
(cid:1)(cid:3)
i,k i,k
=tr(cid:0)E[β˜β˜⊤]E[B⊤
B
](cid:1)
i,k i,k
≤∥E[B⊤ B ]∥·E∥β˜∥2. (91)
i,k i,k 2
Following the similar arguments, we obtain for q ≥2,
E∥M˜ (α)∥q ≤ sup E∥B v∥q ·E∥β˜∥q. (92)
i,k 2 i,k 2 2
v∈Rd,∥v∥2=1
Moreover, we notice that by the tower rule
(cid:13) (cid:104) (cid:16) (cid:89)k (cid:17)⊤(cid:16) (cid:89)k (cid:17)(cid:105)(cid:13)
∥E[B⊤ B ]∥=(cid:13)E H⊤ A A H (cid:13)
i,k i,k (cid:13) k−i j j k−i (cid:13)
j=k−i+1 j=k−i+1
(cid:13) (cid:104)(cid:16) k (cid:89)−1 (cid:17)⊤(cid:16) k (cid:89)−1 (cid:17)(cid:105)(cid:13)
≤∥E[H⊤ A⊤A H ]∥·(cid:13)E A A (cid:13). (93)
k−i k k k−i (cid:13) j j (cid:13)
j=k−i+1 j=k−i+1
By a similar argument as Step 2 in the proof of Lemma 2, we obtain
∥E[H⊤ A⊤A H ]∥≲p2∥X∥2 <∞, (94)
k−i k k k−i
where the constant in ≲ is independent of α. Further, recall that A are i.i.d. random matrices and
j
∥E[A⊤A ]∥ ≤ 1−αpλ [X⊤(2I −αX)X] by the proof of Lemma 1. When α∥X∥ < 2, it follows from
1 1 min d
42the sub-multiplicativity of operator norm and the similar lines as the Step 1 in the proof of Lemma 2 that
(cid:88)∞ (cid:13) (cid:13) (cid:13)E(cid:104)(cid:16) k (cid:89)−1 A j(cid:17)⊤(cid:16) k (cid:89)−1 A j(cid:17)(cid:105)(cid:13) (cid:13) (cid:13)=(cid:88)∞ (cid:13) (cid:13)
(cid:13)
k (cid:89)−1 E[A⊤
j
A j](cid:13) (cid:13) (cid:13)≤(cid:88)∞ (cid:13) (cid:13)E[A⊤ 1A 1](cid:13) (cid:13)i−2 =O(1/α). (95)
i=0 j=k−i+1 j=k−i+1 i=0 j=k−i+1 i=2
√
Therefore, (cid:80)∞ E∥M˜ (α)∥2 = O(1/α), which yields (E∥β˜◦(α)−β˜∥2)1/2 = O( α). By leveraging the
i=0 i,k 2 k 2
inequality in (92) and the similar techniques adopted in the proof of Lemma 1 for the case with q > 2,
we obtain that for any q ≥ 2, (cid:80)∞ (E∥M˜ (α)∥q)2/q = O(1/α). As a direct consequence, we obtain
i=0 i,k 2
√
(E∥β˜◦(α)−β˜∥q)1/q =O( α), which completes the proof.
k 2
C.3 Proof of Theorem 2
Proof of Theorem 2. If we can establish the asymptotic normality for the affine sequence {β k†(α)−β˜} k∈N,
then by applying Lemma 2 and Markov’s inequality, we can prove the CLT for the stationary sequence
{β˜ k◦(α)−β˜} k∈N, which together with the geometric-moment contraction of the dropout iterates {β˜ k(α)−
β˜} k∈N in Theorem 1 can yield the desired result. Therefore, in this proof, we shall show the CLT for
{β k†(α)−β˜} k∈N, that is,
β†(α)−β˜
k √ ⇒N(0,Ξ(α)), as α→0.
α
First, we recall the random vectors b (α) in (15) and let
k
b (α)=:αH β˜, with H :=D X(pI −D ). (96)
k k k k d k
Then, since {β k†(α)−β˜} k∈N is a stationary sequence and using induction on k, we can rewrite β k†(α)−β˜
into
(cid:16) (cid:17)
β†(α)−β˜=α I H +(I −αpX )H +···+(I −αpX )k−1H +··· β˜
k d k d p k−1 d p 1
∞
=α(cid:88) (I −αpX )iH β˜. (97)
d p k−i
i=0
TheH arei.i.d.randommatrices,andindependentofβ˜andβ† . Therefore,weshallapplytheLindeberg-
k k−1
Feller central limit theorem to the partial sum in (97). To this end, we first take the expectation on both
sides of (97). Since the random matrices H are independent of β˜ for all i∈N, we obtain
i
∞
E[β†(α)−β˜]=α(cid:88) (I −αpX )iE[H β˜]
k d p k−i
i=0
43∞
=α(cid:88) (I −αpX )iE[H ]E[β˜]=0. (98)
d p k−i
i=0
To see the last equality, we apply Lemma 7 (i) and (ii) and obtain E[D XD ]=pX =p2X, which gives
k k p
E[H ]=E[D X(pI −D )]=p2X−p2X=0, (99)
k k d k
As a direct consequence, by (96) and the independence of D and β˜, we have
k
E[b (α)]=αE[H ]E[β˜]=0. (100)
k k
Next, we shall provide a closed form of the covariance matrix Cov(β†(α)−β˜). Notice that the random
k
vectors H β˜ are uncorrelated over different i, and E[H β˜β˜⊤H ] = E[H β˜β˜⊤H ] due to the stationarity of
i i i 1 1
the sequence {H iβ˜} i∈N. Hence, by (97), we have
(cid:16) (cid:17)
V :=Cov α−1/2(β†(α)−β˜)
α k
=α−1E(cid:2) (β†(α)−β˜)(β†(α)−β˜)⊤(cid:3)
k k
∞
=α(cid:88) (I −αpX )iE[H β˜β˜⊤H ](I −αpX )i
d p k−i k−i d p
i=0
∞
(cid:88)
=:α (I −αpX )iS(I −αpX )i, (101)
d p d p
i=0
with d×d matrix
S :=E[H β˜β˜⊤H ]. (102)
1 1
Since D is independent of β˜, and β˜β˜⊤ is a symmetric matrix, it follows from the tower rule that
k
S =E[E(H β˜β˜⊤H |y,X)]=E[H E(β˜β˜⊤)H ]=:E[H S H ]. (103)
1 1 1 1 1 0 1
By the closed form solution of β˜ in (5) and E[ϵ]=0, Cov(ϵ)=I , we obtain
n
S =E(β˜β˜⊤)=X−1X⊤E(yy⊤)XX−1
0 p p
=X−1X⊤E[(Xβ∗+ϵ)(Xβ∗+ϵ)⊤]XX−1
p p
=X−1X⊤(cid:0) Xβ∗β∗⊤X⊤+I (cid:1) XX−1. (104)
p n p
44Furthermore, by Lemma 7 (i), one can show that (X) = X and Diag(AX) = Diag(AX) for any matrix A.
Then, by the definition of H in (96) and Lemma 7 (ii)–(iv), we can simplify E[H S H ] as follows:
k 1 0 1
E[H S H ]=E[D X(pI −D )S (pI −D )XD ]
1 0 1 k d k 0 d k k
=p2E[D XS XD ]−pE[D XD S XD ]−pE[D XS D XD ]+E[D XD S D XD ]
1 0 1 1 1 0 1 1 0 1 1 1 k 0 k 1
(cid:16) (cid:17)
=p3(XS X) −2p pX (S X) +p2(1−p)Diag(XS X)
0 p p 0 p 0
(cid:16) (cid:17)
+pX (S ) X +p2(1−p) Diag(X(S ) X)+2X Diag(S X)+(1−p)X⊙S ⊤ ⊙X . (105)
p 0 p p 0 p p 0 0
Combining (104) and (105), we obtain a closed form solution of S which is independent of α.
Now we are ready to solve the covariance matrix V in (101). We multiply the matrix I −αpX to the
α d p
left and right sides of (101) and obtain
∞
(cid:88)
(I −αpX )V (I −αpX )=α (I −αpX )iS(I −αpX )i. (106)
d p α d p d p d p
i=0
Taking the difference between V and (I −αpX )V (I −αpX ) yields
α d p α d p
V −(I −αpX )V (I −αpX )=αS. (107)
α d p α d p
Denote the symmetric matrix A =pX . By simplifying the equation above, for α>0, we have
p p
V A −A V +αA V A =S. (108)
α p p α p α p
Let V = lim V . As α → 0, the quadratic term αA V A vanishes. Thus, we only need to solve the
0 α→0 α p α p
equation
S−V A −A V =0, (109)
0 p p 0
to get the solution for
V = lim V .
0 α
α→0
FollowingTheorem1inPflug(1986)andthesubsequentRemarktherein,wecangettheclosedformsolution
of V , that is,
0
vec(V )=(I ⊗A +A ⊗I )−1·vec(S), (110)
0 d p p d
45where the d2×d2 matrix I ⊗A +A ⊗I is invertible since the fixed design matrix X is assumed to be in
d p p d
a reduced form with no zero columns. For a small α>0, we shall provide a similar closed form solution for
V =V +αB . Specifically, we need to get the closed form of the matrix B by solving a similar equation:
α 0 p p
A V A −B A −A B =0, (111)
p 0 p p p p p
which gives
vec(B )=(I ⊗A +A ⊗I )−1×vec(A V A ). (112)
p d p p d p 0 p
The deterministic matrices V , A and B are all independent of α. By inserting the results of V and B
0 p p 0 p
into V =V +αB , we obtain
α 0 p
Ξ(α)=V =V +αB ,
α 0 p
which holds uniformly over k due to the stationarity of {β k†(α)−β˜} k∈N.
Finally, by applying the Lindeberg-Feller central limit theorem to the partial sum in (97), we establish
the asymptotic normality of {β k†(α)−β˜} k∈N and complete the proof.
D Proofs in Section 3.3
We first outline the main techniques for establishing the asymptotic normality of the averaged GD dropout
sequence {β¯ ngd(α)} n∈N defined in (24).
Recall the observation y in model (1) and the dropout matrix D. For the GD dropout {β˜ k(α)} k∈N in
(2), by Theorem 1, we can define a centering term as follows,
β˜ (α)= lim E [β˜ (α)]=E [β˜◦(α)], (113)
∞ D k D 1
k→∞
where β˜◦(α) follows the stationary distribution π as stated in (14). According to Lemma 1 in Clara et al.
1 α
(2024), we note that E [β˜ (α)−β˜] ̸= 0 but ∥E [β˜ (α)−β˜]∥ → 0 if αp∥X∥ < 1 with a geometric rate as
D k D k 2
k →∞. Therefore, weshallfirstshowthecentrallimittheoremsforthepartialsumof{β˜ k(α)−β˜ ∞(α)} k∈N
and then for the one of {β˜ k(α)−β˜} k∈N.
Next, we take a closer look at the partial sum of {β˜ k(α)−β˜ ∞(α)} k∈N. The iterative function f defined
in (9) allows us to write β˜ (α) = f (β˜ (α)) for all k ∈ N. Similarly, for the initialization β˜◦(α) that
k Dk k−1 0
followstheuniquestationarydistributionπ inTheorem1,wecanwritethestationaryGDdropoutsequence
α
46{β˜ k◦(α)} k∈N into
β˜◦(α)=f (β˜◦ (α)), k ∈N. (114)
k Dk k−1
Recall β˜ (α) defined in (113). Then, we can recursively rewrite β˜◦(α) using the iterative function f and
∞ k
obtain the partial sum
n
S˜◦(α):=(cid:88) [β˜◦(α)−β˜ (α)]
n k ∞
k=1
=(cid:8)
f
(β˜◦(α))−E(cid:2)
f
(β˜◦(α))(cid:3)(cid:9) +(cid:8)
f ◦f
(β˜◦(α))−E(cid:2)
f ◦f
(β˜◦(α))(cid:3)(cid:9)
+···
D1 0 D1 0 D2 D1 0 D2 D1 0
+(cid:8)
f ◦···◦f
(β˜◦(α))−E(cid:2)
f ◦···◦f
(β˜◦(α))(cid:3)(cid:9)
. (115)
Dn D1 0 Dn D1 0
Primarily,weaimto(i)provethecentrallimittheoremforthepartialsumn−1/2S˜◦(α),and(ii)provethe
n
invariance principle for the partial sum process (S˜◦(α)) . To this end, we borrow the idea of functional
i 1≤i≤n
dependence measure in Wu (2005), which was further investigated in Shao and Wu (2007) to establish the
asymptoticnormalityforsequenceswithshort-rangedependence (see(122)forthedefinition). Weshallshow
that the GD dropout sequence {β˜ k◦(α)} k∈N that satisfies the geometric-moment contraction (as proved in
Theorem 1) satisfies such short-range dependence condition.
Finally, we shall complete the proofs of the quenched central limit theorems by showing that, for any
given constant learning rate α > 0 satisfying the conditions in Theorem 3, and any initialization β˜ ∈ Rd,
0
the partial sum
n
S˜β˜ 0(α):=(cid:88) [β˜ (α)−β˜ (α)] (116)
n k ∞
k=1
convergestothestationarypartialsumprocessS˜◦(α),inthesensethatn−1/2(cid:0)E∥S˜β˜ 0(α)−S˜◦(α)∥q(cid:1)1/q
=o(1)
n n n 2
as n→∞.
D.1 Functional Dependence Measure
Before proceeding to the proofs of Theorems 3 and 4, we first provide the detailed form of the functional
dependence measure in Wu (2005) for the iterated random functions with i.i.d. random matrices as inputs.
ThiswillserveasthefoundationalpillartobuildtheasymptoticnormalityofaveragedGDdropoutiterates.
First, for any random vector ζ ∈Rd satisfying E∥ζ∥ <∞, define projection operators
2
P [ζ]=E[ζ |F ]−E[ζ |F ], k ∈Z, (117)
k k k−1
wherewerecallthefiltrationF =σ(D ,D ,...)withi.i.d.dropoutmatricesD ,i∈Z.ByTheorem1and
i i i−1 i
47(12), there exists a measurable function h α(·) such that the stationary GD dropout sequence {β˜ k◦(α)} k∈N
can be written as the following causal process
β˜◦(α)=h (D ,D ,...)=h (F ). (118)
k α k k−1 α k
Define a coupled version of filtration F as F =σ(D ,...,D ,D′,D ,...). In addition, F =F
i i,{j} i j+1 j j−1 i,{j} i
if j >i. For q >1, define the functional dependence measure of β˜◦(α) as
k
θ (α)=(cid:0)E∥β˜◦(α)−β˜◦ (α)∥q(cid:1)1/q , where β˜◦ (α)=h (F ). (119)
k,q k k,{0} 2 k,{0} α k,{0}
Theabovequantitycanbeinterpretedasthedependenceofβ˜◦(α)onD (seethediscussionbelowTheorem1
k 0
forthemeaningofβ˜◦ withk ≤0),andβ˜◦ (α)isacoupledversionofβ˜◦(α)withD inthelatterreplaced
k k,{0} k 0
by its i.i.d. copy D′. If β˜◦(α) does not functionally depend on D , then θ (α)=0.
0 k 0 k,q
Furthermore, if
(cid:80)∞
θ (α)<∞, we define the tail of the cumulative dependence measure as
k=0 k,q
∞
(cid:88)
Θ (α)= θ (α), m∈N. (120)
m,q k,q
k=m
Thiscanbeinterpretedasthecumulativedependenceof{β˜◦(α)} onD ,orequivalently,thecumulative
k k≥m 0
dependence of β˜◦(α) on D , j ≥m. The functional dependence measure in (119) and its cumulative variant
0 j
in (120) are easy to work with and they can directly reflect the underlying data-generating mechanism of
the iterative function β˜◦(α)=f (β˜◦ (α)).
k Dk k−1
Specifically, for all q ≥ 2, Theorem 1 in Wu (2005) pointed out a useful inequality for the functional
dependence measure as follows,
∞ ∞
(cid:88)(cid:0)E∥P [β˜◦(α)]∥q(cid:1)1/q ≤(cid:88)
θ (α)=Θ (α). (121)
0 k 2 k,q 0,q
k=0 k=0
In particular, for some given learning rate α > 0, we say the sequence {β˜ k◦(α)} k∈N satisfies the short-range
dependence condition if
Θ (α)<∞, for some q ≥2. (122)
0,q
This dependence assumption has been widely adopted in the literature; see for example the central limit
theorems in Shao and Wu (2007) and the invariance principle in Berkes et al. (2014) and Karmakar and Wu
(2020). If condition (122) fails, then β˜◦(α) can be long-range dependent, and the partial sum (resp. partial
k
sum processes) behave no longer like Gaussian random vectors (resp. Brownian motions).
48Here, we introduce Theorem 2.1 in Shao and Wu (2007) and Theorem 2 in Karmakar and Wu (2020),
which are the fundamental tools for the proofs of Theorems 3 and 4, respectively.
Lemma 9 (Asymptotic normality (Shao and Wu 2007)). Consider a sequence of stationary mean-zero
random variables x = g(ϵ ,ϵ ,...) ∈ R, for k = 1,...,n, where ϵ ’s are i.i.d. random variables, and
k k k−1 k
g(·) is a measurable function such that each x is a proper random variable. Assume that (E|x |2)1/2 <∞.
k k
Define the Fourier transform of x by
k
n
(cid:88)
S (ω)= x eikω,
n k
k=1
andletf(ω)=(2π)−1(cid:80) E[x x ]eikω, ω ∈R, bethespectraldensityofx . Denotetherealandimaginary
k∈Z 0 k k
(cid:112)
parts of S (ω )/ πnf(ω ) by
n j j
(cid:80)n
x cos(kω )
(cid:80)n
x sin(kω )
z = k=1 k j , z = k=1 k j , j =1,...,m,
j (cid:112) j+m (cid:112)
πnf(ω ) πnf(ω )
j j
where m = ⌊(n−1)/2⌋ with ⌊a⌋ denoting the integer part of a. Let Ω = {c ∈ Rd : |c| = 1} be the unit
d
sphere. For the set j ={j ,...,j } with 1≤j <···<j ≤2m, write the vector z =(z ,...,z )⊤. Let
1 d 1 d j j1 jd
the class Ξ
m,d
={j ⊂{1,...,2m}:#j =d}, where #j is the cardinality of j. If min ω∈Rf(ω)>0 and
∞
(cid:88)
sup
(cid:0)E|P
[x
]|2(cid:1)1/2
<∞, (123)
0 k 2
k=0c∈Ωd
where the projection operator P [·] is in (117), then
k
(cid:12) (cid:12)
sup sup sup(cid:12)P(cid:0) z⊤c≤u(cid:1) −Φ(u)(cid:12)=o(1), as n→∞. (124)
(cid:12) j (cid:12)
j∈Ξm,dc∈Ωdu∈R
Lemma 10 (Gaussian approximation (Karmakar and Wu 2020)). Consider a sequence of nonstationary
mean-zero random vectors x = g (ϵ ,ϵ ,...) ∈ Rd, for k = 1,...,n, where the ϵ ’s are i.i.d. random
k k k k−1 k
variables,andg (·)isameasurablefunctionsuchthateachx isaproperrandomvector. LetS
=(cid:80)j
x .
k k j k=1 k
Assume the following conditions hold for some q >2:
(i) The series (∥x ∥q) is uniformly integrable: sup E(cid:2) ∥x ∥q1 (cid:3) →0 as u→∞,
k 2 k≥1 k≥1 k 2 ∥xk∥2≥u
(ii) The eigenvalues of covariance matrices of increment processes are lower-bounded, that is, there exists
λ >0 and l ∈N, such that for all t≥1, l≥l ,
∗ ∗ ∗
(cid:0) (cid:1)
λ Cov(S −S ) ≥λ l;
min t+l t ∗
49(iii) There exist constants χ>χ and κ>0, where
0
(cid:112)
q2−4+(q−2) q2+20q+4
χ = ,
0 8q
such that the tail cumulative dependence measure
∞
Θ (α)=
(cid:88)
θ
(α)=O(cid:8) m−χ(cid:0) log(m)(cid:1)−κ(cid:9)
. (125)
m,q k,q
k=m
Then, for all q >2, there exists a probability space (Ω⋆,A⋆,P⋆) on which we can define random vectors x⋆,
k
with the partial sum process S⋆ =(cid:80)i x⋆ and a Gaussian process G⋆ =(cid:80)i z⋆. Here z⋆ is a mean-zero
i k=1 k i k=1 k k
independent Gaussian vector, such that (S⋆) =D (S ) and
i 1≤i≤n i 1≤i≤n
max|S i⋆−G⋆ i|=oP(n1/q) in (Ω⋆,A⋆,P⋆).
i≤n
As a special case of Lemma 9, by taking ω =0, one can establish the asymptotic normality of
(cid:80)n
x .
j k=1 k
WeshallleveragethisresultintheproofofTheorem3. Moreover,wenoticethatcondition(ii)inLemma10
on the non-singularity is required when the sequence {x k} k∈N is non-stationary. However, if the function
g k(·) ≡ g(·), that is, the sequence {x k} k∈N is stationary, then the covariance matrix of the increments is
allowed to be singular. To see this, consider a stationary partial sum S = (S ,...,S )⊤ with a singular
l l,1 l,d
covariance matrix Σ ∈ Rd×d and assume rank(Σ) = d−1. Then, there exists a unit vector v ∈ Rd such
that Σv = 0, which indicates that S can be written into a linear combination of S ,...,S , and the
l,1 l,2 l,d
covariance matrix of this linear combination is non-singular. Hence, condition (ii) in Lemma 10 is not
required for stationary processes.
Inaddition,theoriginalTheorem2.1inShaoandWu(2007)andTheorem2inKarmakarandWu(2020)
considered a simple case where the i.i.d. inputs ϵ are one-dimensional. These two theorems still hold even
k
if the inputs are i.i.d. random matrices such as the dropout matrices D in our case. In fact, as long as
k
the inputs are i.i.d. elements, the functional dependence measure can be similarly computed as the one in
one-dimensional case. The essence is that the short-range dependence condition (122) is satisfied using an
appropriatenorm(e.g.,L2-normforvectors,operatornormformatrices)bytheoutputx . Forexample,Wu
k
and Shao (2004) considered iterated random functions on a general metric space, and Chen and Wu (2016)
assumed the ϵ ’s to be i.i.d. random elements to derive asymptotics for x . We will verify this short-range
i k
dependence condition on the GD dropout vector estimates {β˜ k◦(α)} k∈N in the proof of Theorem 3.
50D.2 Proof of Theorem 3
Proof of Theorem 3. Weverifytheshort-rangedependenceconditionforthestationaryGDdropoutsequence
{β˜ k◦(α)} k∈N.
First, consider two different initial vectors β˜◦, β˜◦′ ∈ Rd following the unique stationary distribution π
0 0 α
in Theorem 1. Denote the two GD dropout sequences by {β˜ k◦(α)} k∈N and {β˜ k◦′(α)} k∈N accordingly. By the
geometric-moment contraction in Theorem 1, for all q ≥2, we have
(cid:0)E∥β˜◦(α)−β˜◦′(α)∥q(cid:1)1/q
sup k k 2 ≤rk , k ∈N, (126)
β˜◦,β˜◦′∈Rd,β˜◦̸=β˜◦′
∥β˜ 0◦−β˜ 0◦′∥
2
α,q
0 0 0 0
for some constant r ∈(0,1). Equivalently, it can be rewritten in terms of the iterative function f defined
α,q
in (9) and h (·) defined in (12). That is, for all β˜◦, β˜◦′ ∈Rd, such that β˜◦ ̸=β˜◦′, we have
α 0 0 0 0
(cid:0)E∥f ◦···◦f (β˜◦)−f ◦···◦f (β˜◦′ )∥q(cid:1)1/q
Dk D1 0 Dk D1 0 2
=(cid:0)E∥h (D ,...,D ,D ,D ,...)−h (D ,...,D ,D′,D′ ,...)∥q(cid:1)1/q
α k 1 0 −1 α k 1 0 −1 2
=(cid:0)E∥h
(F )−h (F
)∥q(cid:1)1/q
α k α k,{0,−1,...} 2
≤c rk , (127)
q α,q
where we recall the filtration F = σ(D ,...,D ,D′,D ,...), and c > 0 is some constant indepen-
i,{j} i j+1 j j−1 q
dent of k. Moreover, since h (F ) is stationary over k and D and D′ are i.i.d. random matrices, for all
α k i j
i,j ∈Z, it follows that
E∥h (F )−h (F )∥q
α k,{0} α k,{0,−1,...} 2
=E∥h (F )−h (F )∥q
α k α k,{−1,−2...} 2
=E∥h (F )−h (F )∥q ≤c′rk , (128)
α k+1 α k+1,{0,−1,...} 2 q α,q
where the constant c′ >0 is also independent of k. Hence, by (127) and (128), we can bound the functional
q
dependence measure defined in (119) as follows
θ
(α)=(cid:0)E∥h
(F )−h (F
)∥q(cid:1)1/q
k,q α k α k,{0} 2
≤(cid:0)E∥h
(F )−h (F
)∥q(cid:1)1/q +(cid:0)E∥h
(F )−h (F
)∥q(cid:1)1/q
α k α k,{0,−1,...} 2 α k,{0,−1,...} α k,{0} 2
≤(c +c′)rk . (129)
q q α,q
51As a direct result, we have finite cumulative dependence measure defined in (120), i.e.,
∞
(cid:88)
Θ (α)= θ (α)=O(rm )<∞. (130)
m,q k,q α,q
k=m
Therefore, for the constant learning rate α>0 satisfying the assumptions in Theorem 1, the stationary GD
dropout sequence {β˜ k◦(α)} k∈N meets the short-range dependence requirement in (122). Consequently, the
condition (123) in Lemma 9 is satisfied, which along with the Cram´er-Wold device yields the central limit
theorem for S˜◦(α) defined in (115), that is,
n
n−1/2S˜◦(α)⇒N(0,Σ(α)), (131)
n
where the long-run covariance matrix Σ(α) is defined in Theorem 3.
Next, we bound the difference between S˜ n◦(α) and S˜ nβ˜ 0(α) for any arbitrarily fixed β˜
0
∈ Rd in the q-th
moment, for all q ≥2. For the constant learning rate α>0 satisfying α∥X∥<2, applying Theorem 1 yields
(cid:16) E(cid:13) (cid:13)S˜◦(α)−S˜β˜ 0(α)(cid:13) (cid:13)q(cid:17)1/q
n n 2
=(cid:16) E(cid:13) (cid:13)(cid:2) f D1(β˜ 0◦(α))+f
D2
◦f D1(β˜ 0◦(α))+···+f
Dn
◦···◦f D1(β˜ 0◦(α))(cid:3)
−(cid:2) f D1(β˜ 0(α))+f
D2
◦f D1(β˜ 0(α))+···+f
Dn
◦···◦f D1(β˜ 0(α))(cid:3)(cid:13) (cid:13)q 2(cid:17)1/q
n
≤(cid:16)(cid:88)
rk
(cid:17)
∥β˜◦−β˜ ∥ . (132)
α,q 0 0 2
k=1
Since the contraction constant r ∈ (0,1), we can derive the limit for the sum of the geometric series
α,q
{rk }n as follows
α,q k=1
lim
(cid:88)n
rk = lim
r α,q(cid:0) 1−r αn ,q(cid:1)
=
r
α,q . (133)
n→∞ α,q n→∞ 1−r α,q 1−r α,q
k=1
This, together with (132) gives
√
(cid:0)E(cid:13) (cid:13)S˜◦(α)−S˜β˜ 0(α)(cid:13) (cid:13)q(cid:1)1/q
=O(1)=o( n), (134)
n n 2
which yields the quenched central limit theorem for the partial sum
S˜β˜
0(α) defined in (116), that is, for any
n
fixed initial point β˜ ∈Rd,
0
n−1/2S˜β˜
0(α)⇒N(0,Σ(α)). (135)
n
√
Finally, we shall show that ∥(cid:80)n E[β˜ (α) − β˜]∥ = o( n). To see this, we note that given two
k=1 k 2
52independently chosen initial vectors β˜ and β˜◦, where β˜◦ follows the stationary distribution π while β˜ is
0 0 0 α 0
an arbitrary initial point in Rd, it follows from the triangle inequality that
(cid:13) (cid:13)(cid:88)n
E[β˜
(α)−β˜](cid:13)
(cid:13)
=(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0) β˜
(α)−β˜◦(α)+β˜◦(α)−β˜(cid:1)(cid:105)(cid:13)
(cid:13)
(cid:13) k (cid:13) (cid:13) k k k (cid:13)
2 2
k=1 k=1
≤(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0) β˜
(α)−β˜◦(α)(cid:1)(cid:105)(cid:13)
(cid:13)
+(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0)
β˜◦(α)−β˜(cid:1)(cid:105)(cid:13)
(cid:13)
(cid:13) k k (cid:13) (cid:13) k (cid:13)
2 2
k=1 k=1
=:I +I . (136)
1 2
We first show I 2 = 0. Recall the representation of {β˜ k◦(α) − β˜} k∈N in (14). Since E[A k(α)] = E[I d −
αD XD ]=I −αpX and E[b (α)]=0 by (100), it follows that
k k d p k
E[β˜◦(α)−β˜]=(I −αpX )E[β˜◦ (α)−β˜]. (137)
k d p k−1
Thus, due to the stationarity of {β˜ k◦(α)} k∈N and the non-singularity of X p, we obtain that uniformly over
k ∈N,
E[β˜◦(α)−β˜]=0. (138)
k
As a direct consequence,
I
=(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0)
β˜◦(α)−β˜(cid:1)(cid:105)(cid:13)
(cid:13)
=(cid:13) (cid:13)(cid:88)n E[β˜◦(α)−β˜](cid:13)
(cid:13) =0. (139)
2 (cid:13) k (cid:13) (cid:13) k (cid:13)
2 2
k=1 k=1
In addition, for the part I , it follows from Jensen’s inequality and (132) that
1
I
=(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0) β˜
(α)−β˜◦(α)(cid:1)(cid:105)(cid:13)
(cid:13)
1 (cid:13) k k (cid:13)
2
k=1
≤(cid:16) E(cid:13) (cid:13)(cid:88)n (cid:0) β˜ (α)−β˜◦(α)(cid:1)(cid:13) (cid:13)2(cid:17)1/2
(cid:13) k k (cid:13)
2
k=1
n
≤(cid:16)(cid:88) r αk ,2(cid:17)(cid:13) (cid:13)β˜ 0−β˜ 0◦(cid:13) (cid:13) 2. (140)
k=1
By inserting the results of parts I and I back to (136), we obtain
1 2
(cid:13) (cid:13) (cid:13)(cid:88)n E[β˜ k(α)−β˜](cid:13) (cid:13) (cid:13)
2
≤(cid:16)(cid:88)n r αk ,2(cid:17)(cid:13) (cid:13)β˜ 0−β˜ 0◦(cid:13) (cid:13) 2. (141)
k=1 k=1
which remains bounded as n→∞ when α∥X∥<2 by Theorem 1. This completes the proof.
53D.3 Proof of Corollary 1
Proof of Corollary 1. Recall the stationary GD dropout sequence {β˜ k◦(α)} k∈N which follows the unique sta-
tionarydistributionπ . Sincethissequencesatisfiestheshort-rangedependenceconditionasstatedin(122),
α
it follows from Lemma 9 and the Cram´er-Wold device that any fixed linear combination of the coordinates
ofS˜◦(α)in(115)convergestothecorrespondinglinearcombinationofnormalvectorsindistribution. Then,
n
the CLT for the averaged GD dropout with multiple learning rates holds by applying the Cram´er-Wold
device again, that is,
n−1/2vec(cid:0) S˜◦(α ),...,S˜◦(α )(cid:1) ⇒N(0,Σvec). (142)
n 1 n s
Then, following the similar arguments in the proof of Theorem 3, we obtain the quenched CLT for
vec(cid:0) β¯gd(α )−β˜,...,β¯gd(α )−β˜(cid:1) . We omit the details here.
n 1 n s
D.4 Proof of Theorem 4
Proof of Theorem 4. Recall the stationary GD dropout sequence {β˜ k◦(α)} k∈N in (14), where β˜ k◦(α) follows
the stationary distribution π for all k ∈N. Also, recall the centering term β˜ (α)=E[β˜◦(α)] as defined in
α ∞ 1
(113). By (138), we have E[β˜◦(α)−β˜]=0 uniformly over k ∈N. Hence,
k
(cid:0)E∥β˜ (α)−β˜∥q(cid:1)1/q =(cid:0)E∥E[β˜◦(α)−β˜]∥q(cid:1)1/q
=0. (143)
∞ 2 1 2
This, along with Assumption 1 and Lemma 3 gives, for q >2,
√
(cid:0)E∥β˜◦(α)−β˜ (α)∥q(cid:1)1/q ≤(cid:0)E∥β˜◦(α)−β˜∥q(cid:1)1/q +(cid:0)E∥β˜ (α)−β˜∥q(cid:1)1/q
=O( α). (144)
k ∞ 2 k 2 ∞ 2
Moreover, we notice that by Markov’s inequality and (144), for any u∈R and δ >0, we have
supE(cid:2) ∥β˜◦(α)−β˜ (α)∥q1 (cid:3)
k≥1
k ∞ 2 ∥β˜ k◦(α)−β˜ ∞(α)∥2≥u
(cid:104) (cid:105)
≤supE ∥β˜◦(α)−β˜ (α)∥q ·∥β˜◦(α)−β˜ (α)∥δ/uδ
k ∞ 2 k ∞ 2
k≥1
=supE(cid:2) ∥β˜◦(α)−β˜ (α)∥q+δ(cid:3) /uδ
k ∞ 2
k≥1
=O(cid:8) α(q+δ)/2/uδ(cid:9)
, (145)
54which converges to 0 as u → ∞. Therefore, condition (i) in Lemma 10 is satisfied. Since {β˜ k◦(α)} k∈N is
stationary, following the arguments below Lemma 10, condition (ii) is not required. Regarding condition
(iii), for the constant learning rate α>0 satisfying α∥X∥<2, it follows from Assumption 1 and (129) that
the functional dependence measure θ (α) ≤ c·rk , for all q > 2 and k ∈ N, where the constant c > 0
k,q α,q
is independent of k. Consequently, there exists a constant κ > 0 such that the tail cumulative dependence
measure of {β˜ k◦(α)} k∈N can be bounded by
∞
Θ (α)=
(cid:88)
θ
(α)=O(cid:8) m−χ(cid:0) log(m)(cid:1)−κ(cid:9)
, (146)
m,q k,q
k=m
whereχ>0issomeconstantthatcanbetakentobearbitrarilylarge. Then,thecondition(iii)inLemma10
is satisfied.
Thus, we obtain the invariance principle for the stationary partial sum process (S˜◦(α)) defined in
i 1≤i≤n
(115). That is, there exists a (richer) probability space (Ω˜⋆,A˜⋆,P˜⋆) on which we can define random vectors
β˜⋆’s with the partial sum process S˜⋆ =(cid:80)i (β˜⋆−β˜ ), and a Gaussian process G˜⋆ =(cid:80)i z˜⋆, where z˜⋆’s
k i k=1 k ∞ i k=1 k k
are independent Gaussian random vectors in Rd following N(0,I ), such that
d
(S˜⋆) =D (S˜◦) , (147)
i 1≤i≤n i 1≤i≤n
and
1m ≤ia ≤x n(cid:13) (cid:13)S˜ i⋆−Σ1/2(α)G˜⋆ i(cid:13) (cid:13)
2
=oP(n1/q), in (Ω˜⋆,A˜⋆,P˜⋆), (148)
where the long-run covariance matrix Σ(α) is defined in Theorem 3.
Next, recall the partial sum S˜β˜ 0(α) = (cid:80)n [β˜ (α)−β˜ (α)] as defined in (116), given an arbitrarily
i k=1 k ∞
fixed initial point β˜ ∈Rd. It follows from the triangle inequality that
0
(cid:16) E(cid:104)
max
(cid:13) (cid:13)S˜β˜ 0(α)−Σ1/2(α)G˜⋆(cid:13)
(cid:13)
(cid:105)q(cid:17)1/q
1≤i≤n i i 2
=(cid:16) E(cid:104)
max
(cid:13) (cid:13)S˜β˜ 0(α)−S˜◦(α)+S˜◦(α)−Σ1/2(α)G˜⋆(cid:13)
(cid:13)
(cid:105)q(cid:17)1/q
1≤i≤n i i i i 2
≤(cid:16) E(cid:104)
max
(cid:13) (cid:13)S˜β˜ 0(α)−S˜◦(α)(cid:13)
(cid:13) + max
(cid:13) (cid:13)S˜◦(α)−Σ1/2(α)G˜⋆(cid:13)
(cid:13)
(cid:105)q(cid:17)1/q
1≤i≤n i i 2 1≤i≤n i i 2
≤(cid:16) E(cid:104)
max
(cid:13) (cid:13)S˜β˜ 0(α)−S˜◦(α)(cid:13)
(cid:13)
(cid:105)q(cid:17)1/q +(cid:16) E(cid:104)
max
(cid:13) (cid:13)S˜◦(α)−Σ1/2(α)G˜⋆(cid:13)
(cid:13)
(cid:105)q(cid:17)1/q
. (149)
1≤i≤n i i 2 1≤i≤n i i 2
Therefore, to show the invariance principle for
(cid:0) S˜β˜ 0(α)(cid:1)
, it suffices to bound the difference part
i 1≤i≤n
max ∥S˜β˜ 0(α) − S˜◦(α)∥ in terms of the q-th moment. To this end, recall the iterative function
1≤i≤n i i 2
55f (β)=β+αDX⊤(y−XDβ) in (9) that rewrites the GD dropout recursion (2). We note that
D
max
(cid:13) (cid:13)S˜β˜ 0(α)−S˜◦(α)(cid:13)
(cid:13)
1≤i≤n i i 2
(cid:13) (cid:13)
= max (cid:13)(cid:2) f (β˜ )−f (β˜◦)(cid:3) +···+(cid:2) f ◦···◦f (β˜ )−f ◦···◦f (β˜◦)(cid:3)(cid:13)
1≤i≤n(cid:13) D1 0 D1 0 Di D1 0 Di D1 0 (cid:13)
2
≤ 1m ≤ia ≤x n(cid:16)(cid:13) (cid:13)f D1(β˜ 0)−f D1(β˜ 0◦)(cid:13) (cid:13) 2+···+(cid:13) (cid:13)f Di ◦···◦f D1(β˜ 0)−f Di ◦···◦f D1(β˜ 0◦)(cid:13) (cid:13) 2(cid:17)
=(cid:13) (cid:13)f D1(β˜ 0)−f D1(β˜ 0◦)(cid:13) (cid:13) 2+···+(cid:13) (cid:13)f Dn ◦···◦f D1(β˜ 0)−f Dn ◦···◦f D1(β˜ 0◦)(cid:13) (cid:13) 2. (150)
This, along with the triangle inequality and Theorem 1 yields
(cid:16) E(cid:104)
max
(cid:13) (cid:13)S˜β˜ 0(α)−S˜◦(α)(cid:13)
(cid:13)
(cid:105)q(cid:17)1/q
1≤i≤n i i 2
≤(cid:0)E(cid:13) (cid:13)f D1(β˜ 0)−f D1(β˜ 0◦)(cid:13) (cid:13)q 2(cid:1)1/q +···+(cid:0)E(cid:13) (cid:13)f
Dn
◦···◦f D1(β˜ 0)−f
Dn
◦···◦f D1(β˜ 0◦)(cid:13) (cid:13)q 2(cid:1)1/q
r (1−rn )
≤ α,q α,q ∥β˜ −β˜◦∥ =o(n1/q). (151)
1−r 0 0 2
α,q
We insert this result back into (150), which together with (149) gives the invariance principle for the partial
sum S˜β˜ 0(α)=(cid:80)n [β˜ (α)−β˜ (α)].
i k=1 k ∞
Finally, let the partial sum Sβ˜ 0(α)=(cid:80)n [β˜ (α)−β˜] be as defined in Theorem 4. We shall bound the
i k=1 k
difference between Sβ˜ 0(α) and S˜β˜ 0(α). Since β˜ (α) = E [β˜◦(α)] as defined in (113) and β˜ (α)−β˜ =
i i ∞ D 1 ∞
E [β˜◦(α)−β˜]=0 by (138), it follows that
D 1
n
(cid:16) E(cid:104) 1m ≤ia ≤x n(cid:13) (cid:13)S˜ iβ˜ 0(α)−S iβ˜ 0(α)(cid:13) (cid:13) 2(cid:105)q(cid:17)1/q =(cid:16) E(cid:104) 1m ≤ia ≤x n(cid:13) (cid:13)(cid:88) β˜ ∞(α)−β˜(cid:13) (cid:13) 2(cid:105)q(cid:17)1/q =0. (152)
k=1
Combining this with the invariance principle for
(S˜β˜
0(α)) , we obtain the same approximation rate
i 1≤i≤n
oP(n1/q) for the partial sum process (S
iβ˜
0(α)) 1≤i≤n. This completes the proof.
E Proofs in Section 4.2
Recall the SGD dropout sequence {β˘ k(α)} k∈N and the random coefficient A˘ k(α) in (36). To prove that
sup E∥A˘ (α)v∥q <1isasufficientconditionforthegeometric-momentcontraction(GMC)ofthe
v∈Rd,∥v∥2=1 k 2
SGD dropout sequence, we first introduce two useful moment inequalities in Lemma 11.
56E.1 Proof of Lemma 4
Lemma 11 (Moment inequality). Let q ≥2. For any two random vectors x and y in Rd with fixed d≥1,
the following inequalities holds:
(cid:12) (cid:12)
(i) E(cid:12)∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:12)≤E(cid:0) ∥x∥ +∥y∥ (cid:1)q −E∥x∥q −qE(∥x∥q−1∥y∥ ).
(cid:12) 2 2 2 (cid:12) 2 2 2 2 2
(cid:12) (cid:12)
(ii) E(cid:12)∥x+y∥q−∥x∥q−q∥x∥q−2x⊤y(cid:12)≤(cid:2) (E∥x∥q)1/q+(E∥y∥q)1/q(cid:3)q −E∥x∥q−q(E∥x∥q)(q−1)/q(E∥y∥q)1/q.
(cid:12) 2 2 2 (cid:12) 2 2 2 2 2
Lemma 11(i) immediately follows if we can prove the inequality
(cid:12) (cid:12)
(cid:12)∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:12)≤(cid:0) ∥x∥ +∥y∥ (cid:1)q −∥x∥q −q(∥x∥q−1∥y∥ ), (153)
(cid:12) 2 2 2 (cid:12) 2 2 2 2 2
which is of independent interest. The right hand side of the inequality in Lemma 11(ii) only depends on
expectations of either one of the random vectors x or y. This makes the inequality particularly useful if x
and y are dependent. Lemma 11(i) is more favorable in cases where x and y are independent, or if one
vector is deterministic.
Proof of Lemma 11(i). We can assume that ∥x∥ > 0 and ∥y∥ > 0 as otherwise, the inequality holds
2 2
trivially. It is moreover sufficient to assume x = we , for a positive number w and e ∈ Rd a unit vector.
1 1
Then, we can find two numbers u,v such that
y =u·(we )+ve , (154)
1 2
wheree ∈Rd isaunitvectororthogonaltoe . Letr =∥y∥ =(cid:112) (uw)2+v2 >0. Wenotethatx⊤y =uw2
2 1 2
and ∥x+y∥2 =(1+u)2w2+v2, which gives
2
∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y
2 2 2
=(cid:2) (1+u)2w2+v2(cid:3)q/2 −wq−qwq−2uw2
=(w2+2uw2+r2)q/2−wq−quwq. (155)
(cid:112)
Since r = (uw)2+v2, we can rewrite uw = rδ for some scalar δ with δ ∈ [−δ∗,1], where δ∗ = (w2 +
r2)/(2wr), i.e., w2−2wrδ∗+r2 =0. Here, |δ| can be viewed as the projection length of y on the direction
of x, and the end point δ∗ falls in [0,1]. Then, (155) can be rewritten into
φ(δ):=(w2+2wrδ+r2)q/2−wq−qwq−1rδ. (156)
57Recall that w =∥x∥ >0 and r =∥y∥ >0. The first order derivative of φ(δ) is
2 2
q
φ′(δ)= 2wr(w2+2wrδ+r2)(q/2)−1−qwq−1r
2
=qwr(cid:2) (w2+2wrδ+r2)(q/2)−1−wq−2(cid:3)
(cid:104)(cid:16) 2rδ r2 (cid:17)(q/2)−1 (cid:105)
=qwq−1r 1+ + −1 . (157)
w w2
This indicates that, for q ≥ 2, φ′(δ) ≤ 0 when δ ∈ [−δ∗,r/(2w)], and φ′(δ) > 0 when δ ∈ (−r/(2w),1]. In
particular, by Bernoulli’s inequality, we can observe that
φ(−δ∗)=−wq+qwq−1rδ∗ =(cid:0) q/2−1(cid:1) wq+qwq−2r2 >0,
φ(−r/(2w))=−qwq−2r2/2<0,
φ(1)=(w+r)q−wq−qwq−1r >0. (158)
Moreover, regarding φ(−δ∗) on δ∗ ∈ [0,1], we consider a new function φ˜(s) = −wq −qwq−1rs, which is
decreasingons∈[−1,0]. Notethatφ˜(−1)=−wq+qwq−1r. Thus,bycomparison,wehave−φ(−r/(2w))<
φ(−δ∗)≤φ˜(−1)<φ(1). As a direct result, we obtain
sup |φ(δ)|=max{φ(−δ∗),−φ(−r/(2w)),φ(1)}=φ(1). (159)
|δ|≤1
By inserting w =∥x∥ and r =∥y∥ back to φ(δ), we obtain, for any x and y in Rd,
2 2
(cid:12) (cid:12)
(cid:12)∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:12)≤(∥x∥ +∥y∥ )q−∥x∥q −q∥x∥q−1∥y∥ . (160)
(cid:12) 2 2 2 (cid:12) 2 2 2 2 2
The desired result holds by taking the expectation on the both sides.
Proof of Lemma 11(ii). First, we define a function ϕ(t) = ∥x+ty∥q on t ∈ [0,∞). The first and second
2
order derivatives of ϕ(t) are as follows
ϕ′(t)= d ϕ(t)=q∥x+ty∥q−2(cid:0) t∥y∥2+x⊤y(cid:1) ,
dt 2 2
ϕ′′(t)=
d2
ϕ(t)=q(q−2)∥x+ty∥q−4(cid:0) t∥y∥2+x⊤y(cid:1)2 +q∥x+ty∥q−2∥y∥2
dt2 2 2 2 2
=q(q−2)∥x+ty∥q−4(cid:2) (x+ty)⊤y(cid:3)2 +q∥x+ty∥q−2∥y∥2
2 2 2
≤q(q−1)∥x+ty∥q−2∥y∥2, (161)
2 2
58where the last inequality follows from the Cauchy-Schwarz inequality. In the previous inequality, equality
holds when both random vectors x and y are scalars. Note that ϕ(1) = ∥x + y∥q, ϕ(0) = ∥x∥q, and
2 2
ϕ′(0)=q∥x∥q−2x⊤y. Since
2
(cid:90) 1
ϕ(1)−ϕ(0)−ϕ′(0)= ϕ′(t)dt−ϕ′(0)
t=0
(cid:90) 1 (cid:16)(cid:90) t (cid:17)
= ϕ′′(s)ds+ϕ′(0) dt−ϕ′(0)
t=0 s=0
(cid:90) 1 (cid:90) t
= ϕ′′(s)dsdt, (162)
t=0 s=0
it follows from the upper bound of ϕ′′(s) in (161) that
(cid:90) 1 (cid:90) t
∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y ≤q(q−1) ∥x+sy∥q−2∥y∥2dsdt. (163)
2 2 2 2 2
t=0 s=0
Taking the expectation yields
E(cid:16) ∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:17) ≤q(q−1)(cid:90) 1 (cid:90) t E(cid:0) ∥x+sy∥q−2∥y∥2(cid:1) dsdt. (164)
2 2 2 2 2
t=0 s=0
Furthermore, by H¨older’s inequality and the triangle inequality, we obtain
E(cid:0) ∥x+sy∥q−2∥y∥2(cid:1) ≤(cid:0)E∥x+sy∥q(cid:1)(q−2)/q(cid:0)E∥y∥q(cid:1)2/q
2 2 2 2
≤(cid:104)(cid:0)E∥x∥q(cid:1)1/q +s(cid:0)E∥y∥q(cid:1)1/q(cid:105)q−2(cid:0)E∥y∥q(cid:1)2/q
, (165)
2 2 2
which together with (164) gives
(cid:16) (cid:17)
E ∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y
2 2 2
≤q(q−1)(cid:90) 1 (cid:90) t (cid:104)(cid:0)E∥x∥q(cid:1)1/q +s(cid:0)E∥y∥q(cid:1)1/q(cid:105)q−2(cid:0)E∥y∥q(cid:1)2/q
dsdt
2 2 2
t=0 s=0
=q(cid:90) 1 (cid:110)(cid:104)(cid:0)E∥x∥q(cid:1)1/q +t(cid:0)E∥y∥q(cid:1)1/q(cid:105)q−1 −(cid:0)E∥x∥q(cid:1)(q−1)/q(cid:111)(cid:0)E∥y∥q(cid:1)1/q
dt
2 2 2 2
t=0
=(cid:104)(cid:0)E∥x∥q(cid:1)1/q +(cid:0)E∥y∥q(cid:1)1/q(cid:105)q −E∥x∥q −q(cid:0)E∥x∥q(cid:1)(q−1)/q(cid:0)E∥y∥q(cid:1)1/q . (166)
2 2 2 2 2
In addition, recall that by the proof of Lemma 11(i), we have
(cid:12) (cid:12)
(cid:12)∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:12)≤(∥x∥ +∥y∥ )q−∥x∥q −q∥x∥q−1∥y∥
(cid:12) 2 2 2 (cid:12) 2 2 2 2 2
(cid:90) 1 (cid:90) t
=q(q−1) (∥x∥ +s∥y∥ )q−2∥y∥2dsdt.
2 2 2
t=0 s=0
59Taking expectation on both sides, we obtain
(cid:12) (cid:12) (cid:90) 1(cid:90) t
E(cid:12)∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:12)≤q(q−1) E(cid:2) (∥x∥ +s∥y∥ )q−2∥y∥2(cid:3) dsdt. (167)
(cid:12) 2 2 2 (cid:12) 2 2 2
0 0
It follows from H¨older’s inequality and the triangle inequality that
E(cid:2)
(∥x∥ +s∥y∥
)q−2∥y∥2(cid:3) ≤(cid:2)E(cid:0)
∥x∥ +s∥y∥
(cid:1)q(cid:3)(q−2)/q(cid:0)E∥y∥q(cid:1)2/q
2 2 2 2 2 2
≤(cid:104)(cid:0)E∥x∥q(cid:1)1/q +s(cid:0)E∥y∥q(cid:1)1/q(cid:105)q−2(cid:0)E∥y∥q(cid:1)2/q
. (168)
2 2 2
Evaluating the double integral as in (166) yields
(cid:12) (cid:12)
E(cid:12)∥x+y∥q −∥x∥q −q∥x∥q−2x⊤y(cid:12)
(cid:12) 2 2 2 (cid:12)
≤(cid:104)(cid:0)E∥x∥q(cid:1)1/q +(cid:0)E∥y∥q(cid:1)1/q(cid:105)q −E∥x∥q −q(cid:0)E∥x∥q(cid:1)(q−1)/q(cid:0)E∥y∥q(cid:1)1/q .
2 2 2 2 2
This completes the proof.
Proof of Lemma 4. Since a dropout matrix D is a diagonal matrix with values 0 and 1 on the diagonal,
k
D2 =D and
k k
(cid:13) (cid:13)(I d−αD kX kD k)v(cid:13) (cid:13)2
2
=v⊤(cid:0) I d−2αD kX kD k+α2D kX kD kX kD k(cid:1) v
=1−2αv⊤D X D v+α2v⊤D X D2X D v
k k k k k k k k
=1−αv⊤(cid:2) 2D X D −αD X D X D (cid:3) v
k k k k k k k k
=:1−αv⊤M v, (169)
k
with
M (α)=2D X D −αD X D X D . (170)
k k k k k k k k k
Recall the condition on the learning rate α in (39), it follows that E[M ] is positive definite (p.d.), which
k
furtherimpliesthelowerboundE(αv⊤M v)≥αλ (E[M ])>0,thatholdsuniformlyoverallunitvectors
k min k
v. As a direct consequence,
sup E(cid:13) (cid:13)(I d−αD kX kD k)v(cid:13) (cid:13)2
2
≤ sup (cid:0) 1−E(αv⊤M kv)(cid:1) <1, (171)
v∈Rd,∥v∥2=1 v∈Rd,∥v∥2=1
proving the result in the case q =2.
60Next, we shall show that for all q >2, we also have
sup E∥(I −αD X D )v∥q <1.
d k k k 2
v∈Rd,∥v∥2=1
Inthiscase,thetechniquesintheproofofLemma1cannotbedirectlyappliedduetotherandomnessofX ,
k
andweneedtoleveragethemomentinequalitiesinLemma11instead. Specifically,letxandy inLemma11
be
x=x(v)=v, y =y(v)=D X D v, (172)
k k k
respectively, for v a deterministic d-dimensional unit vector. It remains to show that for any q >2, E∥x−
αy∥q <1 holds for any arbitrary unit vector v. By Lemma 11,
2
E∥x−αy∥q −∥x∥q −q∥x∥q−2E(−x⊤αy)≤E(cid:0) ∥x∥ +∥αy∥ (cid:1)q −∥x∥q −q∥x∥q−1E∥αy∥ , (173)
2 2 2 2 2 2 2 2
which along with ∥x∥ =∥v∥ =1 further yields,
2 2
E∥x−αy∥q −1+qαE(x⊤y)≤E(cid:0) 1+α∥y∥ (cid:1)q −1−qαE∥y∥ . (174)
2 2 2
Therefore, to prove sup E∥x−αy∥q <1, it suffices to show
v∈Rd,∥v∥2=1 2
E(cid:0) 1+α∥y∥ (cid:1)q −1−qαE∥y∥ <qαE(x⊤y). (175)
2 2
By applying Lemma 11 again, we have
E(cid:0) 1+α∥y∥ (cid:1)q −1−qαE∥y∥ ≤(cid:0) 1+α(E∥y∥q)1/q(cid:1)q −1−qα(E∥y∥q)1/q. (176)
2 2 2 2
Thus, we only need to show that for any d-dimensional vector v,
(cid:0) 1+α(E∥y∥q)1/q(cid:1)q −1−qα(E∥y∥q)1/q <qαE(x⊤y). (177)
2 2
Recall the definitions of x and y in (172). By Lemma 7 (i), it follows that E(x⊤y) = pv⊤E[X ]v, where
k,p
X =pX +(1−p)Diag(X ). With µ =µ (v)=(E∥y(v)∥q)1/q =(E∥D X D v∥q)1/q, it suffices to show
k,p k k q q 2 k k k 2
that
(cid:0) (cid:1)q
1+αµ −1−qαµ
q q
sup <qα. (178)
pv⊤E[X ]v
v∈Rd,∥v∥2=1 k,p
61Let q >2. Consider a function f : R (cid:55)→R with f(t)=(1+t)q−1−qt, which is strictly increasing on R .
+ +
Note that f′′(t) = q(q−1)(1+t)q−2. Since q > 2, it follows that f(t) = (cid:82)t (cid:82)s q(q−1)(1+u)q−2duds.
s=0 u=0
Therefore,
q(q−1)
f(t)=(1+t)q−1−qt≤ (1+t)q−2t2. (179)
2
Thus, the condition (178) is satisfied if
q(q−1)
α2 sup
(cid:0) 1+αµ q(cid:1)q−2 µ2
q <qα. (180)
2 pv⊤E[X ]v
v∈Rd,∥v∥2=1 k,p
As this is true by assumption the proof is complete.
E.2 Proof of Theorem 5
Proof of Theorem 5. Let the random coefficient matrix A˘ (α) = I −αD X D be as defined in (36). We
k d k k k
write A˘ (α)=A˘ exchangeably in this proof.
k k
First, we study the case with q =2. Consider two SGD dropout sequences {β˘ k(α)} k∈N and {β˘ k′(α)} k∈N,
given two arbitrarily fixed initial vectors β˘ , β˘′. Let δ˘=β˘ −β˘′. Then, it follows from the tower rule that
0 0 0 0
E∥β˘ (α)−β˘′(α)∥2 =E(cid:2) δ˘⊤A˘⊤···A˘⊤A˘ ···A˘ δ˘(cid:3)
k k 2 1 k k 1
=E(cid:2)E(cid:2) δ˘⊤A˘⊤···A˘⊤A˘ ···A˘ δ˘|A˘ ,...,A˘ (cid:3)(cid:3)
1 k k 1 1 k−1
=E(cid:2) δ˘⊤A˘⊤···A˘⊤ E(A˘⊤A˘ )A˘ ···A˘ δ˘(cid:3)
1 k−1 k k k−1 1
≤∥E(A˘⊤A˘ )∥·E(cid:2) δ˘⊤A˘⊤···A˘⊤ A˘ ···A˘ δ˘(cid:3)
k k 1 k−1 k−1 1
k
≤(cid:89) ∥E(A˘⊤A˘ )∥·∥δ˘∥2. (181)
i i 2
i=1
Recallthatfortheconstantlearningrateα>0satisfyingtheconditionsinLemma4,wehave∥E(A˘⊤A˘ )∥<1
i i
uniformly over i ∈ N. Thus, (cid:81)k ∥E(A˘⊤A˘ )∥ < 1. Since the dropout matrices D ’s are i.i.d. and are
i=1 i i k
independent of the i.i.d. observations x ’s, it follows that (cid:81)k ∥E(A˘⊤A˘ )∥ = ∥E(A˘⊤A˘ )∥k < 1. This gives
k i=1 i i 1 1
the desired result for the case with q =2.
For q >2, we note that
E∥β˘ (α)−β˘′(α)∥q =E∥A˘ ···A˘ δ˘∥q =E(cid:0) ∥A˘ ···A˘ δ˘∥2(cid:1)q/2 =E(cid:0) δ˘⊤A˘⊤···A˘⊤A˘ ···A˘ δ˘(cid:1)q/2 . (182)
k k 2 k 1 2 k 1 2 1 k k 1
62Similarly, it follows from the tower rule that
E(cid:0) δ˘⊤A˘⊤···A˘⊤A˘ ···A˘ δ˘(cid:1)q/2 =E(cid:2)E(cid:2)(cid:0) δ˘⊤A˘⊤···A˘⊤A˘ ···A˘ δ˘(cid:1)q/2 |A˘ ,...,A˘ (cid:3)(cid:3)
1 k k 1 1 k k 1 1 k−1
≤ sup E∥A˘ v∥q ·E(cid:0) δ˘⊤A˘⊤···A˘⊤ A˘ ···A˘ δ˘(cid:1)q/2
k 2 1 k−1 k−1 1
v∈Rd,∥v∥2=1
k
≤(cid:89) sup E∥A˘ v∥q ·∥δ˘∥q. (183)
i 2 2
i=1v∈Rd,∥v∥2=1
Since sup E∥A˘ v∥q <1 holds uniformly over i∈N, we obtain the geometric-moment contraction
v∈Rd,∥v∥2=1 i 2
in (41) for q >2.
Finally, by Corollary 4, the geometric-moment contraction in (41) implies the existence of a unique
stationary distribution π˘
α
of the SGD dropout {β˘ k(α)} k∈N. This completes the proof.
E.3 Proofs of Lemmas 12–14
Lemma 12 (Closed-form solution of the ℓ2 minimizer). Assume that model (31) is in a reduced form, i.e.,
min E[x x⊤] >0. Then, for the minimizer of the ℓ2-regularized least-squares loss β˘:=argmin E(cid:2) (y−
i 1 1 ii β∈Rd
x⊤Dβ)2/2(cid:3)
as defined in (34), we have the closed form solution
β˘=(E[X ])−1E[y x ].
1,p 1 1
Proof of Lemma 12. Recall the d×d Gram matrix X =x x⊤ and
k k k
X =X −Diag(X ), X =pX +(1−p)Diag(X ).
k k k k,p k k
The closed form solution can be obtained by first computing the gradient of the ℓ2-regularized least-squares
loss,
(y−x⊤β)2 =y2−2yx⊤Dβ+β⊤D(xx⊤)Dβ,
E [(y−x⊤β)2]=y2−2pyx⊤β+p2β⊤(xx⊤)β+p(1−p)β⊤Diag(xx⊤)β,
D
E E [(y−x⊤β)2]=E[y2]−2pE[yx⊤]β+p2β⊤E[xx⊤]β+p(1−p)β⊤Diag(E[xx⊤])β,
(y,x) D
(cid:16) (cid:17)
∇ E E [(y−x⊤β)2]=−2pE[yx]+2 p2E[xx⊤]+p(1−p)Diag(E[xx⊤]) β.
β (y,x) D
Recall that the i.i.d. random noise ϵ is independent of the i.i.d. random covariates x . Since model (31) is
k k
63assumed to be in a reduced form, i.e., min E[x x⊤] >0, the closed form solution of β˘ is
i 1 1 ii
(cid:16) (cid:17)−1
β˘=p p2E[x x⊤]+p(1−p)Diag(E[x x⊤]) E[y x ]=(E[X ])−1E[y x ].
1 1 1 1 1 1 1,p 1 1
Recall that for any d×d matrix A, A :=pA+(1−p)Diag(A).
p
Lemma 13. If the d×d matrix E[2X −αX2] is positive definite, then the condition on the learning rate
k k p
α in (37) holds for q =2.
Proof. By rewriting the condition (37) with q =2, for all the unit vector v ∈Rd, ∥v∥ =1, we aim to show
2
2v⊤E(D X D )v
0<α< k k k . (184)
E∥D X D v∥2
k k k 2
Since D2 =D ≤I , it follows from Lemma 7(ii) that
k k d
2v⊤E(D X D )v−αv⊤E[D X D X D ]v
k k k k k k k k
≥2v⊤E(D X D )v−αv⊤E[D X2D ]v
k k k k k k
=v⊤E[D (2X −αX2)D ]v
k k k k
=pv⊤E[2X −αX2] v >0.
k k p
As the unit vector v ∈Rd was arbitrary, condition (37) holds for q =2.
Lemma 14 (ℓ2-minimizer β˘ and true parameter β∗). Assume that E[|ϵ|2q]+∥x∥2q] < ∞. Then, the q-th
2
moment of the gradient in (32) exists at the true parameter β∗ in model (31), for some q ≥2, that is,
(cid:16) E(cid:13) (cid:13) (cid:13)∇ β∗1 2(cid:0) y−x⊤Dβ∗(cid:1)2(cid:13) (cid:13) (cid:13)q 2(cid:17)1/q =(cid:16) E(cid:13) (cid:13)Dx(cid:0) y−x⊤Dβ∗(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q <∞,
which further implies the finite q-th moment of the stochastic gradient at the ℓ2-minimizer β˘ defined in (34),
that is
(cid:16) E(cid:13) (cid:13) (cid:13)∇ β˘1 2(cid:0) y−x⊤Dβ˘(cid:1)2(cid:13) (cid:13) (cid:13)q 2(cid:17)1/q =(cid:16) E(cid:13) (cid:13)Dx(cid:0) y−x⊤Dβ˘(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q <∞,
Proof of Lemma 14. First, it follows from the triangle inequality that
(cid:16) E(cid:13) (cid:13)Dx(cid:0) y−x⊤Dβ∗(cid:1)(cid:13) (cid:13)q(cid:17)1/q =(cid:16) E(cid:13) (cid:13)Dx(cid:0) x⊤β∗+ϵ−x⊤Dβ∗(cid:1)(cid:13) (cid:13)q(cid:17)1/q
2 2
≤(cid:0)E(cid:13) (cid:13)Dxx⊤β∗(cid:13) (cid:13)q(cid:1)1/q +(cid:0)E(cid:13) (cid:13)Dxϵ(cid:13) (cid:13)q(cid:1)1/q +(cid:0)E(cid:13) (cid:13)Dxx⊤Dβ∗(cid:13) (cid:13)q(cid:1)1/q
. (185)
2 2 2
64By Assumption 2, since the dimension of β∗ is fixed, we have
(cid:0)E(cid:13) (cid:13)Dxx⊤β∗(cid:13) (cid:13)q 2(cid:1)1/q ≤(cid:0)E∥x∥2 2q(cid:1)1/q ∥β∗∥
2
<∞.
Due the independence between x and ϵ, Assumption 2 gives (cid:0)E(cid:13) (cid:13)Dxϵ(cid:13) (cid:13)q(cid:1)1/q ≤ (E∥x∥q)1/q(E∥ϵ∥q)1/q < ∞.
2 2 2
Moreover, we obtain
(cid:0)E(cid:13) (cid:13)Dxx⊤Dβ∗(cid:13) (cid:13)q 2(cid:1)1/q =(cid:0)E(cid:13) (cid:13)Dx(cid:13) (cid:13)2 2q(cid:1)1/q ∥β∗∥
2
≤(cid:0)E(cid:13) (cid:13)x(cid:13) (cid:13)2 2q(cid:1)1/q ∥β∗∥
2
<∞. (186)
Inserting the inequalities into (185), we obtain the finite q-th moment at the true parameter β∗.
Next, we show that the finite q-th moment of the stochastic gradient at β∗ can also imply the finite q-th
moment at β˘. Note that
(cid:16) E(cid:13) (cid:13)D 1x 1(cid:0) y 1−x⊤ 1D 1β˘(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q ≤(cid:16) E(cid:13) (cid:13)D 1x 1(cid:0) y 1−x⊤ 1D 1β∗(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q +(cid:16) E(cid:13) (cid:13)D 1X 1D 1(β˘−β∗(cid:1)(cid:13) (cid:13)q 2(cid:17)1/q . (187)
We only need to show that the second term is bounded. Since X = pX + (1 − p)Diag(X ), X =
1,p 1 1 1
X −Diag(X ), and β˘=(E[X ])−1E[y x ], it follows that
1 1 1,p 1 1
E(cid:13) (cid:13)D 1X 1D 1(β˘−β∗(cid:1)(cid:13) (cid:13)q
2
=E(cid:13) (cid:13)D 1X 1D 1(cid:0) (E[X 1,p])−1E[y 1x 1]−β∗(cid:1)(cid:13) (cid:13)q
2
=E(cid:13) (cid:13)D 1X 1D 1(cid:0) (E[X 1,p])−1E[(x⊤ 1β∗+ϵ 1)x 1]−β∗(cid:1)(cid:13) (cid:13)q
2
=E(cid:13) (cid:13)D 1X 1D 1(cid:0) (E[X 1,p])−1E[X 1]β∗−β∗(cid:1)(cid:13) (cid:13)q
2
=(1−p)qE(cid:13) (cid:13)D 1X 1D 1(E[X 1,p])−1E[X 1]β∗(cid:13) (cid:13)q
2
≤(1−p)q(cid:13) (cid:13)(E[X 1,p])−1E[X 1](cid:13) (cid:13)q sup E(cid:13) (cid:13)D 1X 1D 1v(cid:13) (cid:13)q 2·∥β∗∥q 2. (188)
v∈Rd,∥v∥2=1
The sub-multiplicativity of the operator norm yields
(cid:13) (cid:13)(E[X 1,p])−1E[X 1](cid:13) (cid:13)≤ λλ max (E(E [X[X 1] ])
)
<∞, (189)
min 1,p
sinceλ (E[X ])≥(1−p)λ (E[Diag(X )])=(1−p)min E[X ] >0,andλ (E[X ])≤λ (E[X ])<
min 1,p min 1 i 1 ii max 1 max 1
∞. Moreover, ∥β∗∥ <∞. As we assumed that sup E∥D X D v∥q <∞ in Lemma 4, also (188)
2 v∈Rd,∥v∥2=1 1 1 1 2
is bounded.
65F Proofs in Section 4.3
F.1 Proof of Lemma 5
Proof of Lemma 5. The recursion in (36) is β˘ (α)−β˘=A˘ (α)(β˘ (α)−β˘)+b˘ (α), with random matrix
k k k−1 k
A˘ (α)=I −αD X D , and random vector b˘ (α)=αD x (y −x⊤D β˘). Recall β˘=argmin E[(y−
k d k k k k k k k k k β∈Rd
x⊤Dβ)2/2] in (34), where the expectation is taken over both (y,x) and D. By Lemma 7 (ii),
E E [b˘ (α)]=E E [αD x (y −x⊤D β˘)]
(y,x) D k (y,x) D k k k k k
=E [αpI y x −αpX β˘]
(y,x) d k k k,p
=αpE[X ]β˘−αpE[X ]β˘
1,p 1,p
=0. (190)
Similar to (88), we can rewrite the stationary SGD dropout sequence β˘◦(α) into
k
∞ k
β˘◦(α)−β˘=(cid:88)(cid:16) (cid:89) A˘ (α)(cid:17) b˘ (α)
k j k−i
i=0 j=k−i+1
∞ k
=α(cid:88)(cid:16) (cid:89) (I −αD X D )(cid:17) D x (y −x⊤ D β˘)
d j j j k−i k−i k−i k−i k−i
i=0 j=k−i+1
∞
=:α(cid:88) M˘ (α). (191)
i,k
i=0
Recall the filtration F˘ =σ(ξ ,ξ ,...) in (202) for i∈Z, where ξ =(y ,x ,D ). Notice that E[b˘ (α)]=0
i i i−1 i i i i k
by (190), and therefore we have
E[M˘ (α)|F˘
]=E(cid:104) (cid:89)k
(I −αD X D
)(cid:12)
(cid:12)F˘
(cid:105) E(cid:104)
D x (y −x⊤ D
β˘)(cid:105)
i,k k−i+1 d j j j (cid:12) k−i+1 k−i k−i k−i k−i k−i
j=k−i+1
=E(cid:104) (cid:89)k
(I −αD X D
)(cid:12)
(cid:12)F˘
(cid:105)
·E[b˘ (α)]
d j j j (cid:12) k−i+1 k−i
j=k−i+1
=0. (192)
Hence, for any k ∈ N, {M˘ i,k(α)} i∈N is a sequence of martingale differences with respect to the filtration
F˘ . Let t=k−i. By applying Burkholder’s inequality in Lemma 6, we have,
k−i
(cid:0)E∥β˘◦(α)−β˘∥q(cid:1)1/q =α(cid:16) E(cid:13) (cid:13) (cid:88)k M˘ (α)(cid:13) (cid:13)q(cid:17)1/q
k 2 (cid:13) k−t,k (cid:13)
2
t=−∞
66k
≲α(cid:16) (cid:88) (cid:0)E∥M˘ (α)∥q(cid:1)2/q(cid:17)1/2
, (193)
k−t,k 2
t=−∞
where the constant in ≲ only depends on q. Denote the vector s˘ = D x (y −x⊤D β˘) and the matrix
t t t t t t
product A˘ =A˘ (α)=(cid:81)k A˘ (α) for simplicity. Then, we can write
(t+1):k (t+1):k j=t+1 j
b˘ (α)=αs˘ and M˘ (α)=A˘ (α)s˘ . (194)
t t k−t,k (t+1):k t
For the case with q =2, notice that A˘ is independent of s˘ , and by the tower rule, we have
(t+1):k t
E∥M˘ (α)∥2 =E∥A˘ s˘ ∥2
k−t,k 2 (t+1):k t 2
=E(cid:2)E(cid:2) s˘⊤A˘⊤ A˘ s˘ |F˘(cid:3)(cid:3)
t (t+1):k (t+1):k t t
=E(cid:2)E(cid:2) tr(s˘ s˘⊤A˘⊤ A˘ )|F˘(cid:3)(cid:3)
t t (t+1):k (t+1):k t
=E(cid:2) tr(cid:0)E[s˘ s˘⊤A˘⊤ A˘ |F˘](cid:1)(cid:3)
t t (t+1):k (t+1):k t
=E(cid:2) tr(cid:0)E[s˘ s˘⊤]A˘⊤ A˘ (cid:1)(cid:3)
t t (t+1):k (t+1):k
=tr(cid:0)E[s˘ s˘⊤]E[A˘⊤ A˘ ](cid:1)
t t (t+1):k (t+1):k
≤(cid:13) (cid:13)E[A˘⊤ (t+1):kA˘ (t+1):k](cid:13) (cid:13)·E∥s˘ t∥2 2. (195)
Next, we shall bound the parts (cid:13) (cid:13)E[A˘⊤ (t+1):kA˘ (t+1):k](cid:13) (cid:13) and E∥s˘ t∥2 2 separately. First, recall A˘ j(α) = I d −
αD X D in (36), which are i.i.d. over j. By the tower rule with the induction over j = t+1,t+2,...,k,
j j j
we have
(cid:13) (cid:13)E[A˘⊤ (t+1):kA˘ (t+1):k](cid:13) (cid:13)=(cid:13) (cid:13)E(cid:2)E[A˘⊤ (t+1):kA˘
(t+1):k
|A˘ t+1,A˘ t+2,...,A˘ k−1](cid:3)(cid:13)
(cid:13)
≤(cid:13) (cid:13)E[A˘⊤ k(α)A˘ k(α)](cid:13) (cid:13)·(cid:13) (cid:13)E[A˘⊤ (t+1):(k−1)A˘ (t+1):(k−1)](cid:13)
(cid:13)
k
≤ (cid:89) (cid:13) (cid:13)E[A˘⊤
j
(α)A˘ j(α)](cid:13) (cid:13)
j=t+1
=(cid:13) (cid:13)E[A˘⊤ 1(α)A˘ 1(α)](cid:13) (cid:13)k−t . (196)
Moreover, recall the random matrix M (α)=2D X D −αD X D X D as defined in (170). For any unit
k k k k k k k k k
vector v ∈Rd, by (171) in the proof of Lemma 4, we have
v⊤E[A˘⊤(α)A˘ (α)]v ≤1−αv⊤E[M (α)]v
1 1 1
≤1−αλ
(cid:0)E[M (α)](cid:1)
<1, (197)
min 1
67as the constant learning rate α satisfies condition (37). In fact, condition (37) also implies that E[M (α)] is
k
positive definite for each k ∈N, which can be seen by (39).
WeshallshowthatthetermE∥s˘ ∥2 =E∥D x (y −x⊤D β˘)∥2 remainsboundedask →∞. InAssump-
k 2 k k k k k 2
tion 2, we have assumed that the stochastic gradient in the SGD dropout recursion ∇ (y−x⊤Dβ)2/2 =
β
Dx(y−x⊤Dβ), has finite q-th moment when β = β∗ for some q ≥ 2. By Lemma 14, Assumption 2 also
implies the bounded q-th moment when β = β˘. As a direct consequence, E∥s˘ ∥2 is bounded as k → ∞.
k 2
This, along with (193), (195) and (197), yields
k
(cid:0)E∥β˘◦(α)−β˘∥2(cid:1)1/2 ≲α(cid:16) (cid:88) E∥M˘ (α)∥2(cid:17)1/2
k 2 k−t,k 2
t=−∞
k
≤α(cid:16) (cid:88) (cid:13) (cid:13)E[A˘⊤ (t+1):kA˘ (t+1):k](cid:13) (cid:13)·E∥s˘ t∥2 2(cid:17)1/2
t=−∞
k
(cid:16) (cid:88) (cid:17)1/2
≲α (1−αλ )k−t
∗
t=−∞
∞
(cid:16)(cid:88) (cid:17)1/2
=α (1−αλ )i
∗
i=0
√
=O( α), (198)
where the last equation holds since (cid:80)∞ (1 − αλ )i = 1 = O(1/α). Here, the constants in ≲
i=0 ∗ 1−(1−αλ∗)
are independent of k and α, and λ = λ (cid:0)E[M (α)](cid:1) is bounded away from zero since E[M (α)] =
∗ min 1 1
E[2D X D −αD X D X D ] is positive definite by condition (39).
1 1 1 1 1 1 1 1
For the case q >2, following similar arguments as in (195), we obtain
E∥M˘ (α)∥q =E(∥A˘ s˘ ∥2)q/2
k−t,k 2 (t+1):k t 2
=E(cid:0) s˘⊤A˘⊤ A˘ s˘ (cid:1)q/2
t (t+1):k (t+1):k t
≤ sup E∥A˘ v∥q ·E∥s˘ ∥q
(t+1):k 2 t 2
v∈Rd,∥v∥2=1
≤(cid:0) sup E∥A˘ v∥q(cid:1)k−t ·E∥s˘ ∥q. (199)
1 2 t 2
v∈Rd,∥v∥2=1
With µ (v) = (E∥D X D v∥q)1/q < ∞ as defined in Lemma 4 and the Equations (174) and (176) in the
q 1 1 1 2
proof of Lemma 4, we have
E∥A˘ v∥q ≤(1+αµ (v))q−qαµ (v)−qαpv⊤E[X ]v. (200)
1 2 q q 1,p
This, together with Taylor expansion around α = 0 and the inequalities (193) and (199) gives finally
68√
max
(cid:0)E∥β˘◦(α)−β˘∥q(cid:1)1/q
=O( α).
k k 2
F.2 Proof of Theorem 6
The proofs of the quenched CLT and the invariance principle for averaged SGD dropout follow similar
arguments as the ones for averaged GD dropout. The key differences lie in the functional dependence
measures, because for SGD settings, both the dropout matrix D and the sequential observation x are
k k
random. We shall first introduce some necessary definitions and then proceed with the rigorous proofs.
Recall the generic dropout matrix D ∈Rd×d and random sample (y,x)∈R×Rd. For the SGD dropout
sequence {β˘ k(α)} k∈N, we define the centering term
β˘ (α)= lim E[β˘ (α)]=E[β˘◦(α)], (201)
∞ k 1
k→∞
wheretheexpectationistakenoverboth(y,x)andD,andβ˘◦(α)definedin(45)followstheuniquestationary
1
distribution π˘ . We first use Lemma 9 to prove the CLT for the partial sum of the stationary sequence
α
{β˘ k◦(α)−β˘ ∞(α)} k∈N,andthenapplythegeometric-momentcontractioninTheorem5toshowthequenched
CLT for the partial sum of the non-stationary one {β˘ k(α)−β˘ ∞(α)} k∈N. Finally, we extend the quenched
CLT to the partial sum of {β˘ k(α)−β˘ ∞(α)} k∈N by providing the upper bound of β˘ ∞(α)−β˘ in terms of the
q-th moment for some q ≥2.
Similar to Section D.1, we introduce the functional dependence measure in Wu (2005) for the stationary
SGD dropout sequence {β˘ k◦(α)} k∈N. However, the randomness in β˘ k◦(α) is induced from both the dropout
matrix D and the random sample (y ,x ). Therefore, we define a new filtration F˘ by
k k k k
F˘ =σ(ξ ,ξ ,...), k ∈Z, (202)
k k k−1
where the i.i.d. random elements ξ = (D ,(y ,x )), k ∈ Z, are defined in (46). For any random vector
k k k k
ζ ∈Rd satisfying E∥ζ∥ <∞, define projection operators
2
P˘ [ζ]=E[ζ |F˘ ]−E[ζ |F˘ ], k ∈Z. (203)
k k k−1
By Theorem 5 and (45), there exists a measurable function h˘ (·) such that the stationary SGD dropout
α
sequence {β˘ k◦(α)} k∈N can be represented by a causal process
β˘◦(α)=h˘ (ξ ,ξ ,...)=h˘ (F˘ ). (204)
k α k k−1 α k
69We denote the coupled version of F˘ by
i
F˘ =σ(ξ ,...,ξ ,ξ′,ξ ...), (205)
i,{j} i j+1 j j−1
and let F˘ = F˘ if j > i, where ξ′ is an i.i.d. copy of ξ . For q > 1, define the functional dependence
i,{j} i j i
measure of β˘ (α) as
k
θ˘ (α)=(cid:0)E∥β˘◦(α)−β˘◦ (α)∥q(cid:1)1/q , where β˘◦ (α)=h˘ (F˘ ). (206)
k,q k k,{0} 2 k,{0} α k,{0}
In addition, if (cid:80)∞ θ˘ (α)<∞, we define the tail of cumulative dependence measure as
k=0 k,q
∞
Θ˘ (α)= (cid:88) θ˘ (α), m∈N. (207)
m,q k,q
k=m
Both θ˘ (α) and Θ˘ (α) are useful to study the dependence structure of the stationary SGD dropout
k,q k,q
iteration β˘◦(α)=f (β˘◦ (α)). To apply Lemma 9, we only need to show that the stationary SGD
k Dk,(yk,xk) k−1
dropout sequence {β˘ k◦(α)} k∈N is short-range dependent in the sense that Θ˘ 0,q(α)<∞, for some q ≥2.
Proof of Theorem 6. Consider two initial vectors β˘◦,β˘◦′ following the stationary distribution π˘ defined in
0 0 α
Theorem 5. By the recursion in (36), we obtain two stationary SGD dropout sequences {β˘ k◦(α)} k∈N and
{β˘ k◦′(α)} k∈N. It follows from Theorem 5 that for all q ≥2,
(cid:0)E∥β˘◦(α)−β˘◦′(α)∥q(cid:1)1/q
sup k k 2 ≤r˘k , k ∈N, (208)
β˘ 0◦,β˘ 0◦′∈Rd,β˘ 0◦̸=β˘ 0◦′
∥β˘ 0◦−β˘ 0◦′∥
2
α,q
withr˘ =(cid:0) sup E∥A˘ (α)v∥q(cid:1)1/q asdefinedin(42)andrandommatrixA˘ (α)=I −αD X D .
α,q v∈Rd:∥v∥2=1 1 2 1 d 1 1 1
When the constant learning rate α > 0 satisfies the condition in (37), we have r˘ ∈ (0,1) as shown in
α,q
Theorem 5. Recall the coupled filtration F˘ =σ(ξ ,...,ξ ,ξ′,ξ ...) as defined in (205). Then, (45)
i,{j} i j+1 j j−1
and (208) show that
(cid:0)E∥f˘ ◦···◦f˘ (β˘◦)−f˘ ◦···◦f˘ (β˘◦′ )∥q(cid:1)1/q
ξk ξ1 0 ξk ξ1 0 2
=(cid:0)E∥h˘ (ξ ,...,ξ ,ξ ,ξ ,...)−h˘ (ξ ,...,ξ ,ξ′,ξ′ ,...)∥q(cid:1)1/q
α k 1 0 −1 α k 1 0 −1 2
=(cid:0)E∥h˘ (F˘ )−h˘ (F˘ )∥q(cid:1)1/q
α k α k,{0,−1,...} 2
≤c˘ r˘k , (209)
q α,q
for some constant c˘ >0 that is independent of k. Following a similar argument in (128), for all q ≥2 and
q
70k ∈N, we can bound the functional dependence measure θ˘ (α) in (206) as follows
k,q
θ˘ (α)=(cid:0)E∥h˘ (F˘ )−h˘ (F˘ )∥q(cid:1)1/q
k,q α k α k,{0} 2
≤(cid:0)E∥h˘ (F˘ )−h˘ (F˘ )∥q(cid:1)1/q +(cid:0)E∥h˘ (F˘ )−h˘ (F˘ )∥q(cid:1)1/q
α k α k,{0,−1,...} 2 α k,{0,−1,...} α k,{0} 2
≤c˘′r˘k , (210)
q α,q
for some constant c˘′ > 0 that is independent of k. Consequently, the cumulative dependence measure
q
Θ˘ (α) in (207) is also bounded for all q ≥2 and m∈N, that is,
m,q
∞
Θ˘ (α)= (cid:88) θ˘ (α)=O(r˘m )<∞. (211)
m,q k,q α,q
k=m
Theinequalityin(121)derivedbyWu(2005)holdsforageneralclassoffunctionaldependencemeasures,as
longastheinputsofthefunctionalsystem(i.e., themeasurablefunctionh˘(ξ ,ξ ,...))arei.i.d. elements.
k k−1
Thus, we can apply (121) to the projection operator P˘ [·] in (203) and obtain
k
∞ ∞
(cid:88)(cid:0)E∥P˘ [β˘◦(α)]∥q(cid:1)1/q ≤(cid:88) θ˘ (α)=Θ˘ (α)<∞, (212)
0 k 2 k,q 0,q
k=0 k=0
implying the short-range dependence of the stationary SGD dropout sequence {β˘ k◦(α)} k∈N. Then, it follows
from Lemma 9 that
n
n−1/2(cid:88)(cid:0) β˘◦(α)−β˘ (α)(cid:1) ⇒N(0,Σ˘(α)), (213)
k ∞
k=1
where the long-run covariance matrix Σ˘(α) is defined in Theorem 6. Following similar arguments as in
(132)–(134),foranyinitialvectorβ˘ ∈Rd,wecanleveragethegeometric-momentcontractioninTheorem5
0
and achieve the quenched CLT for the corresponding SGD dropout sequence {β˘ k(α)} k∈N, that is,
n
n−1/2(cid:88)(cid:0) β˘ (α)−β˘ (α)(cid:1) ⇒N(0,Σ˘(α)). (214)
k ∞
k=1
Recall the ℓ2-minimizer β˘ in (34) and the centering term β˘ (α) = lim E(β˘ (α)) = E(β˘◦(α)) in
∞ k→∞ k 1
√
(201). We shall prove ∥(cid:80)n E[β˘ (α)−β˘]∥ = o( n). For any two initial vectors β˘ and β˘◦, where β˘◦
k=1 k 2 0 0 0
follows the stationary distribution π˘ in Theorem 5, while β˘ is an arbitrary initial vector in Rd, it follows
α 0
71from the triangle inequality that
(cid:13) (cid:13)(cid:88)n
E[β˘
(α)−β˘](cid:13)
(cid:13)
=(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0) β˘
(α)−β˘◦(α)+β˘◦(α)−β˘(cid:1)(cid:105)(cid:13)
(cid:13)
(cid:13) k (cid:13) (cid:13) k k k (cid:13)
2 2
k=1 k=1
≤(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0) β˘
(α)−β˘◦(α)(cid:1)(cid:105)(cid:13)
(cid:13)
+(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0)
β˘◦(α)−β˘(cid:1)(cid:105)(cid:13)
(cid:13)
(cid:13) k k (cid:13) (cid:13) k (cid:13)
2 2
k=1 k=1
=:˘I +˘I . (215)
1 2
For the part˘I , Jensen’s inequality and a similar argument as for (132) yield
1
˘I
=(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0) β˘
(α)−β˘◦(α)(cid:1)(cid:105)(cid:13)
(cid:13)
1 (cid:13) k k (cid:13)
2
k=1
≤(cid:16) E(cid:13) (cid:13)(cid:88)n (cid:0) β˘ (α)−β˘◦(α)(cid:1)(cid:13) (cid:13)2(cid:17)1/2
(cid:13) k k (cid:13)
2
k=1
n
≤(cid:16)(cid:88) r˘ αk ,2(cid:17)(cid:13) (cid:13)β˘ 0−β˘ 0◦(cid:13) (cid:13) 2. (216)
k=1
For the part˘I , we recall the random matrix A˘ (α) = I −αD X D in (36) with X = x x⊤. Recall the
2 1 d 1 1 1 1 1 1
notation X = pX +(1−p)Diag(X ) in (35). Notice that by Lemma 7 (ii), we have E E [A˘ (α)] =
1,p 1 1 (y,x) D 1
E E [I −αD X D ]=E [I −αpX ]=I −αpE[X ], which along with E[b˘ ]=0 in (190) gives
(y,x) D d 1 1 1 (y,x) d 1,p d 1,p k
E[β˘ k◦(α)−β˘]=(cid:0) I d−αpE[X 1,p](cid:1)E[β˘ k◦ −1(α)−β˘]. Since {β˘ k◦(α)} k∈N is stationary and E[X 1,p] is non-singular
by the reduced-form condition min (E[x x⊤]) >0 imposed on model (31), it follows that
i 1 1 ii
E[β˘◦(α)−β˘]=0, for all k ∈N. (217)
k
This further yields
˘I
=(cid:13) (cid:13)E(cid:104)(cid:88)n
(cid:0)
β˘◦(α)−β˘(cid:1)(cid:105)(cid:13)
(cid:13)
=(cid:13) (cid:13)(cid:88)n E[β˘◦(α)−β˘](cid:13)
(cid:13) =0. (218)
2 (cid:13) k (cid:13) (cid:13) k (cid:13)
2 2
k=1 k=1
By applying the results for˘I and˘I to (215), we obtain
1 2
(cid:13) (cid:13) (cid:13)(cid:88)n E[β˘ k(α)−β˘](cid:13) (cid:13) (cid:13)
2
≤(cid:16)(cid:88)n r˘ αk ,2(cid:17)(cid:13) (cid:13)β˘ 0−β˘ 0◦(cid:13) (cid:13) 2. (219)
k=1 k=1
When the constant learning rate α>0 satisfies the condition in (37), r˘ ∈(0,1) by Theorem 5, and hence
α,2
(219) remains bounded as n→∞. By this result and (214), the desired quenched CLT for the partial sum
(cid:80)n (β˘◦(α)−β˘) follows.
k=1 k
72Proof of Corollary 2. The proof of Corollary 2 applies the Cram´er-Wold device to Theorem 6 and can be
derived in the same way as the proof of Corollary 1. We omit the details here.
F.3 Proof of Theorem 7
Proof of Theorem 7. Consider a stationary SGD dropout sequence {β˘ k◦(α)} k∈N following the stationary dis-
tribution π˘ in Theorem 5. Define the mean-zero stationary partial sum
α
i
S˘◦(α)=(cid:88)(cid:0) β˘◦(α)−β˘ (cid:1) , i∈N. (220)
i k ∞
k=1
Recall that β˘ (α)=E[β˘◦(α)] as defined in (201). Due to the stationarity, we have E[β˘◦(α)−β˘]=0 for all
∞ 1 k
k ∈N by (217). This gives
(cid:0)E∥β˘ (α)−β˘∥q(cid:1)1/q =(cid:0)E∥E[β˘◦(α)−β˘]∥q(cid:1)1/q
=0. (221)
∞ 2 1 2
Since we supposed in Theorem 7 that Assumption 2 holds for some q >2, it follows from Lemma 5 that, for
all q >2,
√
(cid:0)E∥β˘◦(α)−β˘ (α)∥q(cid:1)1/q ≤(cid:0)E∥β˘◦(α)−β˘∥q(cid:1)1/q +(cid:0)E∥β˘ (α)−β˘∥q(cid:1)1/q
=O( α). (222)
k ∞ 2 k 2 ∞ 2
Following a similar argument as for (145), we can show that β˘◦(α)−β˘ (α) satisfies condition (i) on the
k ∞
uniform integrability in Lemma 10. Due to the stationarity of {β˘ k◦(α)} k∈N, condition (ii) in Lemma 10 is
not required (see the discussion below Lemma 10 for details). Condition (iii) is also satisfied, since when
the constant learning rate α>0 satisfies condition (37), the stationary SGD dropout sequence {β˘ k◦(α)} k∈N
is shown to be short-range dependent by (212), i.e., the tail of cumulative dependence measure Θ˘ (α) =
m,q
(cid:80)∞ θ˘ (α)<∞.
k=m k,q
Hence,byLemma10,thereexistsa(richer)probabilityspace(Ω˘⋆,A˘⋆,P˘⋆)onwhichwecandefinerandom
vectors β˘⋆’s with the partial sum process S˘⋆ = (cid:80)i (β˘⋆ −β˘ ), and a Gaussian process G˘⋆ = (cid:80)i z˘⋆,
k i k=1 k ∞ i k=1 k
where z˘⋆’s are independent Gaussian random vectors in Rd following N(0,I ), such that
k d
(S˘⋆) =D (S˘◦) , (223)
i 1≤i≤n i 1≤i≤n
and
1m ≤ia ≤x n(cid:13) (cid:13)S˘ i⋆−Σ˘1/2(α)G˘⋆ i(cid:13) (cid:13)
2
=oP(n1/q), in (Ω˘⋆,A˘⋆,P˘⋆), (224)
73where the long-run covariance matrix Σ˘(α) is defined in Theorem 6.
Following similar arguments as for (149)–(151), we can leverage the geometric-moment contraction in
Theorem 5 to show the same Gaussian approximation rate, i.e., oP(n1/q), for the partial sum sequence
(cid:0)(cid:80)i (β˘ (α)−β˘ (α))(cid:1) , for any arbitrarily fixed initial vector β˘ ∈ Rd. Finally, recall the partial
k=1 k ∞ 1≤i≤n 0
sumprocessS˘β˘ 0(α)=(cid:80)i (β˘ (α)−β˘). Byasimilarargumentin(152),thedesiredGaussianapproximation
i k=1 k
result for the partial sum process
(S˘β˘
0(α)) follows.
i 1≤i≤n
G Proofs in Section 5
G.1 Proof of Theorem 8
Lemma 15. For any d×d symmetric matrix S, we have E∥S∥≤(cid:112) trE(S2)≤(cid:112) d∥E(S2)∥.
Proof. IfλisaneigenvalueofasymmetricmatrixA, thenλ2 isaneigenvalueofA2. Denotethej-thlargest
(cid:113)
eigenvalue of A by λ j(A), j = 1,...,d. Then, E∥S∥ = Emax 1≤j≤d(cid:12) (cid:12)λ j(S)(cid:12) (cid:12) = E max 1≤j≤d(cid:2) λ j(S)(cid:3)2 =
E(cid:112)
max λ (S2). Since the quadratic matrix S2 is positive semi-definite, it follows from Jensen’s in-
1≤j≤d j
equality that E(cid:112) max λ (S2)≤E(cid:112) tr(S2)≤(cid:112) trE(S2)≤(cid:112) d∥E(S2)∥.
1≤j≤d j
Proof of Theorem 8. Recall the SGD dropout sequence {β˘ k(α)} k∈N in (36), the long-run covariance matrix
Σ˘(α) of the averaged SGD dropout iterates in Theorem 6, and the online estimator Σˆ (α) in (55). When
n
there is no ambiguity, we omit the dependence on α, e.g., Σˆ = Σˆ (α) and Σ˘ = Σ˘(α). We shall bound
n n
E∥Σˆ −Σ˘∥.
n
RecallthestationarySGDdropoutsequence{β˘ k◦(α)} k∈Nin(43),whichfollowsthestationarydistribution
π˘ in Theorem 5. For simplicity, we define V (α)=nΣˆ (α). By Equation (58), we can write V (α) into
α n n n
ψ(n)−1 ψ(n)−1
V (α)=(cid:16) (cid:88) S (α)⊗2+R (α)⊗2(cid:17) +(cid:16) (cid:88) |B |2+|δ (n)|2(cid:17) β¯sgd(α)⊗2
n m n m η n
m=1 m=1
ψ(n)−1
−(cid:16) (cid:88) |B |S (α)+δ (n)R (α)(cid:17) β¯sgd(α)⊤
m m η n n
m=1
ψ(n)−1
−β¯sgd(α)(cid:16) (cid:88)
|B |S (α)+δ (n)R
(α)(cid:17)⊤
. (225)
n m m η n
m=1
Recall the partial sums S (α)=(cid:80) β˘ (α) and R (α)=(cid:80)n β˘ (α) in (57). We similarly define
m k∈Bm k n k=ηψ(n) k
ψ(n)−1 n ψ(n)−1
V◦(α)=
(cid:88) (cid:16) (cid:88) β˘◦(α)(cid:17)⊗2 +(cid:16) (cid:88) β˘◦(α)(cid:17)⊗2
=:
(cid:88)
S◦(α)⊗2+R◦(α)⊗2. (226)
n k k m n
m=1 k∈Bm k=ηψ(n) m=1
74By the triangle inequality, we have
nE∥Σˆ −Σ˘∥=E∥V −nΣ˘∥≤E∥V −V◦∥+E∥V◦−nΣ˘∥. (227)
n n n n n
We shall bound these two terms separately.
First, for the term E∥V◦−nΣ˘∥, we shall use the results in Xiao and Wu (2011). To this end, we need to
n
verify the assumptions on the weak dependence of SGD dropout iterates {β˘ k} k∈N in (36) and the growing
sizes of blocks {B m} m∈N in (54). Denote the elements of d×d matrices V n◦ and Σ˘ respectively by
V◦ =:(v◦ ) and Σ˘ =:(σ˘ ) . (228)
n ij,n 1≤i,j≤d ij 1≤i,j≤d
Moreover,wewritethestationarySGDdropoutiterateβ˘◦ in(43)andtheℓ2-minimizerβ˘in(34)respectively
k
into
β˘◦ =(β˘◦ ,...,β˘◦ )⊤ and β˘=(β˘ ,...,β˘ )⊤. (229)
k k1 kd 1 d
Bytheshort-rangedependenceof{β˘ k◦} k∈Nin(212),itcanbeshownthatforany1≤i,j ≤d,Cov(β˘ k◦ i,β˘ 0◦ j)≤
c ijρk
ij
forsomeconstantsc
ij
>0and0≤ρ
ij
<1. Forthenon-overlappingblocks{B m} m∈N,sincethepositive
integers {η m} m∈N satisfy η m+1−η m →∞, it follows that η m+1/η m →1 as m→∞, and therefore,
M
(cid:88)
(η −η )2 ≍η (η −η ). (230)
m+1 m M+1 M+1 M
m=1
Then, by Theorems 1(i) and 2(iii) in Xiao and Wu (2011), we obtain, for each 1≤j ≤d,
E(v◦ −nσ˘ )2 ≤(cid:110)(cid:2)E(v◦ −Ev◦ )2(cid:3)1/2 +(cid:2) (Ev◦ −nσ˘ )2(cid:3)1/2(cid:111)2
jj,n jj jj,n jj,n jj,n jj
≲n(2/ζ)∨(2−1/ζ). (231)
For E(v◦ −nσ˘ )2 with i ̸= j, the same rate as in (231) holds. To see this, define two new sequences
ij,n ij
{β k+} k∈N and {β k−} k∈N with β k+ =(β˘ k◦ i+β˘ 0◦ j)−(β˘ i+β˘ j), and β k− =(β˘ k◦ i−β˘ 0◦ j)−(β˘ i−β˘ j). Notice that
∞
σ˘ = (cid:88) E(β˘◦ −β˘)(β˘◦ −β˘ )
ij ki i 0j j
k=−∞
=
(cid:88)∞ E[(β˘ k◦ i−β˘ i)+(β˘ 0◦
j
−β˘ j)]2−[(β˘ k◦ i−β˘ i)−(β˘ 0◦
j
−β˘ j)]2
4
k=−∞
75∞ ∞
1 (cid:88) 1 (cid:88)
= E(β+)2− E(β−)2, (232)
4 k 4 k
k=−∞ k=−∞
which can be viewed as the long-run variances of the sequences {β k+} k∈N and {β k+} k∈N as indicated by the
last line. A similar decomposition can be applied to v◦ . Since the results in Xiao and Wu (2011) hold for
ij,n
any linear combination of weak-dependent sequences, again by the short-range dependence of {β˘ k◦} k∈N in
(212), we have
E(v◦ −nσ˘ )2 ≲n(2/ζ)∨(2−1/ζ), for all 1≤i,j ≤d. (233)
ij,n ij
Since the dimension d is fixed, it follows from Lemma 15 that
(cid:113)
E∥V◦−nΣ˘∥≤ trE(V◦−nΣ˘)2
n n
(cid:115)
(cid:88)
= E(v◦ −nσ˘ )2
ij,n ij
1≤i,j≤d
(cid:114)
≤ d2 max E(v◦ −nσ˘ )2
ij,n ij
1≤i,j≤d
≲n(2/ζ)∨(2−1/ζ), (234)
where the constants in ≲ are independent of n.
Next, we bound E∥V −V◦∥. Similar arguments as for (234) show
n n
(cid:114)
E∥V −V◦∥≤ d2 max E(v −v◦ )2. (235)
n n ij,n ij,n
1≤i,j≤d
Thus, we only need to show the bound for the one-dimensional case. Now, consider V , V◦, V , H and
n n n n
β¯sgd as scalars. Note that E∥·∥=(E[·]2)1/2 for d=1. By the decomposition in (58) and applying Jensen’s
n
inequality, we have
(E[V −V◦]2)1/2 ≤(E[V −V◦]2)1/2+2(E[H β¯sgd]2)1/2+K (E[β¯sgd]4)1/2. (236)
n n n n n n n n
We shall bound the three term respectively. First, recall the contraction constant r˘ defined in (42). By
α,q
the GMC in Theorem 5 and H¨older’s inequality, it follows that
ψ(n)−1
(E[V −V◦]2)1/2 ≤ (cid:88) (cid:0)E[S −S◦]4(cid:1)1/4(cid:0)E[S +S◦]4(cid:1)1/4 +(cid:0)E[R −R◦]4(cid:1)1/4(cid:0)E[R +R◦]4(cid:1)1/4
n n m m m m n n n n
m=1
76ψ(n)−1
(cid:88)
≲ (r˘ α,q)ηm(η m+1−η m)1/2+(r˘ α,q)ηψ(n)(n−η ψ(n)+1)1/2
m=1
=O(1), (237)
where the constants in ≲ and O(·) are independent of n. Similarly, since the dimension d is fixed and
E[β˘◦]=β˘ by (217) with the ℓ2-minimizer β˘ defined in (34), we can show that
k
n n
(E[β¯sgd]4)1/2 ≲(cid:16) E(cid:104)1 (cid:88) β˘◦(cid:105)4(cid:17)1/2 +(cid:16) E(cid:104)1 (cid:88) (β˘ −β˘◦)(cid:105)4(cid:17)1/2 ≍ 1 , (238)
n n k n k k n
k=1 k=1
and (E[H ]4)1/2 ≲n(n−η )2. Combining these results with the fact that K ≍n(n−η ) yields
n ψ(n) n ψ(n)
K (E[β¯sgd]4)1/2 ≍n−η and (E[H β¯sgd]2)1/2 ≲n−η . (239)
n n ψ(n) n n ψ(n)
Inserting all these expressions back into (236), we obtain
(E[V −V◦]2)1/2 ≲n−η ≍n1−(1/ζ). (240)
n n ψ(n)
Consequently, by (235), we have E∥V − V◦∥ ≲ n1−(1/ζ) for the multi-dimensional case. Since ζ > 1,
n n
compared to the rate of E∥V◦−nΣ˘∥≲n(2/ζ)∨(2−1/ζ) in (234), the latter dominates. This, along with (227)
n
gives the desired result.
77