SUPER: Evaluating Agents on Setting Up and Executing Tasks
from Research Repositories
BenBogin1,2 KejuanYang2 ShashankGupta1 KyleRichardson1 ErinBransom1
PeterClark1 AshishSabharwal1 TusharKhot1
1AllenInstituteforAI 2UniversityofWashington
{benb, shashankg, kyler, erinbransom, peterc, ashishs, tushark}@allenai.org
Abstract ‚ÄúMEZE is a memory-efficient zeroth-order optimizer. Train with this
üßëüî¨ optimizer on my dataset (available at https://drive...), following the
paper‚Äôs code at: https://github.com/... and report accuracy.‚Äù
Given that Large Language Models (LLMs) ...
üí≠ I need to install the required dependencies listed in therequirements.txtfile
havemadesignificantprogressinwritingcode,
!pip install -r requirements.txt
cantheynowbeusedtoautonomouslyrepro- Successfully installed datasets-2.19.2 transformers-4.40.2 ...
duceresultsfromresearchrepositories? Such
üí≠ I will now download the provided data file.
a capability would be a boon to the research !gdown.download("https://drive.google.com../", "my_data.csv")
community, helping researchers validate, un- Downloading to: /content/dpt/my_data.csv
derstand, andextendpriorwork. Toadvance üí≠ Next, I will edit the `data_utils.py` file to configure the loaded dataset.
towards this goal, we introduce SUPER, the edit data_utils.py <BEFORE>
return datasets.load_dataset('glue', 'mrpc', split=split)
first benchmark designed to evaluate the ca- <AFTER>
return datasets.load_dataset('csv', data_files={...
pability of LLMs in setting up and execut- File edited successfully.
ing tasks from research repositories. SUPER üí≠ I will now execute the experiment with the. `.ru.n.py` script.
!python run.py output_dir result/output ...
aimstocapturetherealisticchallengesfaced
Traceback (most recent call last): ...
by researchers working with Machine Learn- ImportError: cannot import name 'default_hp_search_backend'
ing (ML) and Natural Language Processing üí≠ Error suggests a `transformers` version issue. I will downgrade it to resolve the error.
!pip install transformers==4.28.1 # downgrading dependency
(NLP) research repositories. Our benchmark Successfully installed tokenizers-0.13.3 transformers-4.28.1
...
comprisesthreedistinctproblemsets: 45end-
üí≠ I will now retry running the experiment.
to-end problems with annotated expert solu- !python run.py output_dir result/output ...
18:50:24,975 - INFO - ***** Running training *****
tions, 152 sub-problems derived from the ex- ...
18:51:12,059 - INFO - {'dev_accuracy': 0.27}
pertsetthatfocusonspecificchallenges(e.g.,
The training process has completed successfully. The
configuringatrainer), and604automatically
accuracy result on the development set is: 0.27
generated problems for larger-scale develop-
ment. We introduce various evaluation mea- Figure1: Anillustrationofaresearchtaskandsomeofthe
surestoassessbothtasksuccessandprogress, stepsanagentwouldneedtocompleteit,includingupdating
dataloadingconfiguration,resolvingdependencyissues(due
utilizinggoldsolutionswhenavailableorap-
tounlistedversiondependenciesintherepository),running
proximations otherwise. We show that state-
thetrainingscriptandreportingmetrics.
of-the-art approaches struggle to solve these
problemswiththebestmodel(GPT-4o)solving
open-sourcerepositories,whethertoverifyexisting
only16.3%oftheend-to-endset,and46.1%of
resultsortotestthemundernewconditions.
thescenarios. Thisillustratesthechallengeof
thistask,andsuggeststhatSUPERcanserveas In practice, even when research code is avail-
able, running code from arbitrary repositories is
avaluableresourceforthecommunitytomake
andmeasureprogress.1 oftennon-trivialandtime-consuming(Samueland
Mietchen,2022;Storksetal.,2023). Experimen-
1 Introduction tation frequently requires substantial effort to set
up and execute them: installing the environment,
Research and scientific discoveries often rely on
makingnon-trivialconfigurationchanges,resolv-
the reproducibility of experiments conducted by
ing outdated package dependencies, fixing bugs,
other researchers and the ease with which scien-
anddeterminingthecorrectexecutioncommands,
tistscanbuilduponeachother‚Äôswork. Inthecon-
amongothertasks. Allofthisrequiresaconsider-
text of empirical ML and NLP research, it is of-
ableunderstandingofthedocumentationandrepos-
ten crucial for researchers to be able to execute
itory code, knowledge about fixing issues (e.g.,
andreproducediverseresearchexperimentsfrom
CUDAerrors),aswellastheabilitytomodifythe
1https://github.com/allenai/super-benchmark codeappropriately. Thesestepsareespeciallytime-
1
4202
peS
11
]IA.sc[
1v04470.9042:viXra
Reporting
metrics
Setting
up
the
experiment
Resolving
Issues
Executing
&consumingforresearchrepositories‚Äúin-the-wild‚Äù, sets, for which we have gold solutions, we com-
assupportanddocumentationmaynotbeavailable. paretheiranswers(e.g.,metricstobereported)to
In this work, we ask: Can LLMs automate the thegoldsolutions. Toallowforpartialcredit,we
setupandexecutionoftasksinresearchreposito- alsomeasuretheprogressoftheagentsbycheck-
ries? Consider the research task in Fig. 1 where ingiftheyreachspecific‚Äòlandmark‚Äôstatesintheir
theagentisaskedtousearesearchcoderepository solutions,suchascompletingatrainingstage. For
totrainamodelwithanewoptimizer,andevaluate the automatically generated problems, for which
itsperformanceonacustomdataset. Asuccessful we have no gold solutions, we simply check if a
agent would need to set up the experiment by in- key script (e.g., the training or evaluation script)
stallingdependencies, downloadingtheprovided wasrunsuccessfullywithoutexceptions,whichwe
data,andmakingcodechangestoloadit(firstthree foundtobeaneffectiveapproximatemetric.
cellsinthefigure),thenexecutethetrainingscript We evaluate both proprietary and open-source
whilerespondingtounexpectedissuessuchasan LLMsonSUPERastheunderlyingmodelsofstrong
incompatible dependency (fourth and fifth cell), baselineagentswithaccesstofile-editingtools. We
andfinallyreporttheresultmetrics(lastcell). find that agents struggle to correctly solve many
While LLM-based agents have recently been oftheproblems, withthestrongestagentsolving
usedtoproduceexecutioncommandsfrompopular only 46.1% of the Masked sub-problems. These
research repositories (Liu et al., 2023b), execute agents are even further away from solving entire
popularMLrepositories(Huangetal.,2024),orre- researchtasks,completingcorrectlyonly16.3%of
solverepositoryissues(Jimenezetal.,2024),noex- theend-to-endExperttasks. Open-sourcemodels
istingbenchmarkevaluatesagentsonthecommon substantiallylagbehindonboththesub-problems
problemfacedbymanyresearchers: bothsettingup and end-to-end tasks. Moreover, we find that the
andexecutingexperimentsusingresearchreposito- rankingoftheagentsandmodelsontheAutoset
riesin-the-wild,i.e.,lesspopularrepositoriesthat ismostlythesameasitisonthecuratedsets,sug-
arenottypicallywell-documentedormaintained, gestingitspotentialusefulnessfordevelopment.
which make experiments harder to configure and Our analysis of model trajectories reveals that
execute. As a recent study shows (Storks et al., agents are better at resolving well-specified sub-
2023),bothnoviceandadvancedresearchersfind problems, such as solving exceptions, bugs, and
the challenge of ‚Äúsetting up the code base‚Äù to be otherissues,thantasksrequiringrepositoryandfile
themostdifficultpartofreproducingexperiments. explorationtounderstandcodestructure. Thesere-
To encourage research on this problem, we in- sultsunderscoremanyofthecorechallengesfacing
troduce SUPER (Setting UP and Executing tasks LLM-basedexperimentexecutionsystems,which
fromResearchrepositories),abenchmarkfocusing ourbenchmarkaimstohelpadvance.
onsuchlower-profileresearchrepositories. SUPER
consists of three distinct problem sets. The Ex- 2 RelatedWork
pert set contains 45 manually curated problems
solved by experts. The Masked set includes 152 Codingbenchmarks: Whileearlycodebench-
sub-problemsderivedfromtheexpertsetthrough marks(Chenetal.,2021;Austinetal.,2021;Cas-
ourproposed‚ÄúCodeMasking‚Äùmechanism,where sanoetal.,2022)mainlyfocusedonsynthesizing
weremovepartsoftheexpert-writtencodetogener- simplefunctionsfromdescriptions,recentbench-
ateadiversesetofsub-problemstargetingspecific marks have shifted to more complex competitive
challenges. Eachsub-problemaddressesaspecific programmingproblems(Lietal.,2022;Hendrycks
challenge,suchasinstallingdependenciesandre- et al., 2021; Jain et al., 2024) and evaluating pro-
solving conflicts, configuring experimental data, ficiency with popular data science libraries (Lai
settinguphyper-parameters,resolvingruntimeex- et al., 2023). Unlike these, we follow the recent
ceptions, correctly executing scripts, etc. Lastly, trendonevaluatingLLMsinmorenaturalprogram-
the Auto set contains an additional 604 automati- mingscenarios,suchasprogrammingwithexternal
callygeneratedtaskswithanevenmorediverseset toolsandAPIs(Lietal.,2023;Shenetal.,2023;
ofrepositoriesandchallenges. Itcanpotentiallybe Wangetal.,2023b;Patiletal.,2023),codeediting
usedinfutureworkfordevelopment,fine-tuning, and debugging (Cassano et al., 2023; Tian et al.,
ortrainingusingenvironmentfeedback. 2024;Haqueetal.,2023),resolvingGitHubissues
To evaluate agents on the Expert and Masked (Jimenezetal.,2024)andunderstandingandcod-
2SUPER DS-1000 ML-Bench(Agent) MLAgentBench SWE-bench
Resource
(thiswork) (Laietal.,2023) (Liuetal.,2023b) (Huangetal.,2024) (Jimenezetal.,2024)
Repo.understanding ‚úì ‚úó ‚úì ‚úó ‚úì
Requiresrepositorysetup ‚úì ‚úó ‚úì ‚úó ‚úó
Outcome-basedevaluation ‚úì ‚úì ‚úó ‚úì ‚úì
Low-profilerepositories ‚úì ‚úó ‚úó - ‚úó
(cid:44)‚ÜíMedianstars 14/14/23 35,309 9,632 - 12,557
#sourcerepositories 45/45/604 8 18 - 12
#problems 45/152/604 1000 9641 15 2300
Table1: ComparisonofSUPERagainstfourotherrelatedcodeexecutionbenchmarksintermsofthechallengesbeingtested
(rows1-5)andthenumberofsourcerepositoriesandproblemsinthedataset(row6-7).ForSUPER,numberofrepositoriesand
problemsrefertotheExpert/Masked/Autosetsrespectively.Repositoryunderstandingreferstotheagentbeingrequiredtogo
throughrepositoryfilestocompleteatask.Repositorysetupreferstotherequirementtoinstalldependenciesandenvironment.
Outcome-basedevaluationinvolvesassessingperformancebasedonunit-testsorcomparingoutcomeresults,suchasmetrics,to
goldones.Low-profilerepositoriesrefertorepositorieswithlownumberofGitHubstars.
ingwithinarepositorycontext(Liuetal.,2023a;
‚Üí
Dingetal.,2024;Zhangetal.,2023).
1. Write tasks from 2. Collect expert 3. Extract Masked
In contrast to these works, SUPER focuses on research repositories human solutions Code Scenarios
the end-to-end task of setting up and executing Figure2:AnoverviewoftheconstructionpipelinefortheEx-
research tasks in lower-profile repositories, pre- pertandMaskedsets.TheExpertsetcontainsmanuallywrit-
tentasks,alongwithexpertsolutions(Step2). TheMasked
sentingauniquesetofchallenges,withtasksthat
setcontainsproblemsextractedfromtheexpertsset(Step3).
require repository comprehension and reasoning,
editingmultiplefiles,settinguptherepositoryen- 3 BenchmarkConstruction
vironment for execution while interactively run-
ningcommandsintheenvironment. Table1com- In this section we describe the process of build-
paresthefourdatasetsmostrelevanttoSUPER.ML- ingtheSUPERbenchmark. TheSUPERbenchmark
Bench (Liu et al., 2023b), specifically, its ML- consists of 3 sets (see Table 2) serving different
Agent-Benchsetup,evaluatesLLMs‚Äôabilitytoexe- purposes. TheExpertset(¬ß3.1)containsmanually
cutetasksbutfocusesonpopularcoderepositories writtenproblems,solvedbyexperts. TheMasked
ratherthanlowprofile,anddoesnotevaluatebased set (¬ß3.2) contains sub-problems extracted from
oncorrectnessofoutcome,i.e.,whetherresulting theExpertsetusingthegoldsolution,whichpro-
metricsarecorrect. MLAgentBench(Huangetal., videeasierandmorefocusedsub-problems. Fig.2
2024) evaluates agents ability to run ML experi- providesahigh-leveloverviewoftheconstruction
mentsbutfocusesonoptimizingsingle-scriptML pipeline of these two sets. Finally, the Auto set
experimentsratherthancomprehendingandsetting (¬ß3.3)containsautomaticallygeneratedproblems
uparbitraryrepositoriesforexperimentation. whichcanbeusedfordevelopmentandimprove-
mentofagents.
EnvironmentSetup. Runningresearch-oriented
LLM Agents: Recent advancements in LLM- repositories often necessitates both being able to
based agents have shown significant progress runsystemshellcommands(e.g. toinstalldepen-
across various domains, including games (Wang denciesandrunscripts)andstatefulPythoncom-
et al., 2023a), web navigation (Yao et al., 2022; mands. Previousworkandenvironmentstypically
Zhou et al., 2023), human interaction simula- supportonlyoneofthese(e.g.,onlysystemshell
tion(Parketal.,2023),automatingcomplexcom- commands (Jimenez et al., 2024) or only Python
putertasks(Xieetal.,2024),datascienceandma- commands(Huangetal.,2024)). Instead,webuild
chinelearning(Guoetal.,2024;Hongetal.,2024; anenvironmentthatallowsrunningbothofthese
Liu et al., 2024; Yang et al., 2024b), open-ended commandswithaJupyternotebookasengine. In
discovery(Jansenetal.,2024),andcoding(Wang thissetup,eachexecutioncodeisequivalenttorun-
etal.,2024;Yangetal.,2024a;OpenDevinTeam, ninganotebookcell,whichcontainsPythoncode
2024). Our benchmark introduces an important and/orbashcommands,andwherestateisreserved
new domain that encourages the development of betweencellexecutions(e.g.,eachcellcanuseany
LLM-basedagentstoassistresearchersintheirend of the previously defined Python variables). The
toendresearchtaskswitharbitraryrepositories. executionofeachcellreturnsanobservationstring.
33.1 ExpertSet Set # Solutions Evaluation Purpose
Expert 45 ‚úì Solution-based Benchmark
We construct the Expert set by (1) identifying a (cid:44)‚ÜíMasked 152 ‚úì Solution-based Benchmark,analysis
setofrelevantcoderepositoriesfromresearchpa- Auto 604 ‚úó Proxy Development
persandmanuallywritingresearch-orientedtasks Table2: ThedifferentsetsofSUPER.
basedonthemand(2)askinghumanexpertstopro-
videend-to-endsolutionsforthesetasks(¬ß3.1.1). first,toallowfairevaluationofsubmittedanswers
We then use the expert solutions as the basis for bymakingsurethetaskisnotunder-specified,such
outcome-based evaluation,wherewecomparethe thattwoagentsthatcorrectlycompletethetaskget
agent‚Äôs answer to the gold answer, and a more the same results. Second, to minimize computa-
lenient landmark-based evaluation that indicates tionalrequirements,asdescribednext.
progresstowardcorrectlysolvingthetask,evenif
thesolutionisnotentirelycorrect(¬ß3.1.2). MinimizingComputationalRequirements. To
makeSUPERfasterandcheapertorun,weensure
3.1.1 Construction
tasksareexecutablewithoutrelianceonGPUma-
Tasks. Wecreatetasksmotivatedbythefollow- chines and that they do not require more than 10
ingtwocommonsettings: (1)reproducingnumbers minutesofcompute(e.g.,fortrainingmodelsorin-
from research papers by running specific experi- stallingpackages)onbasiccomputeinstances(see
ments,and(2)runningmodifiedexperimentswith ¬ß4forcomputedetails). Wethereforecreatetasks
differentdatasets,models,orconfigurations. that require minimal compute by only asking to
Westartbycollectingrepositoriesfromthe‚ÄúPa- trainandevaluatesmallmodels(e.g.,gpt2-small),
persWithCode‚Äù(github.com/paperswithcode/
andbyaddingimplementationinstructionstothe
paperswithcode-data)database,whichcontains
task,suchas‚Äúonlyloadthefirst10examplesofthe
researchpaperslinkedtotheirGitHubrepositories, dataset‚Äùor‚Äúrunasingleepoch‚Äù.
along with some additional metadata such as the Notethattheserestrictionsdonotmakethetask
modality of the datasets used. We only sample any easier for the agent. In fact, they often add
researchpaperswith‚ÄúText‚Äùmodalitiesandselect additionalchallengesthatagentsneedtosolve(e.g.,
repositoriesfrom2021orbeyond. configuringhyper-parameters,findingwheredata
Wethenmanuallyreviewthesampledreposito- isloadedtolimititsloadingtothefirst10samples,
ries and write tasks that involve running a single beingabletorunexperimentsthatweredesigned
experiment that is mentioned either in the repos- forGPUonaCPU,etc.).
itory‚Äôs ‚Äúreadme‚Äù file or under a script available
in the repository, if such can be found. When- Expert Annotation. We use Upwork (https:
everpossible,wemakethetaskmorechallenging
//www.upwork.com/)tofindandhireexpertsthat
by requiring the experiment to be run on a new have experience with running ML and NLP ex-
dataset or model, other than the one described periments. Wefiltertheinitiallistofapplications
in the available documentation. In these cases, by testing them on a pilot task which we solved
we select either datasets available on Hugging- ourselves,tomakesuretheyareabletocorrectly
FaceHub(https://huggingface.co/datasets) executeataskandeffectivelysolveissuesbycom-
orprovideaGoogleDrivelinkwherethedataset paringtheirsolutionandresultstoours. Weinstruct
can be found. The challenge of running on a workerstoexecutetheirsolutionsonGoogleColab,
specific dataset varies in difficulty: it could in- allowingustocollectthesolutionsinaconsistent
volve only a single configuration line change if notebook-likeenvironment.
thedatasetisalreadysupported,orcreatinganew We ask the experts to submit their (1) solution
datasetreader,adjustingcolumnnames,etc. notebook,(2)answers(i.e. metricstobereported),
For each task, we define (1) the target Github thespecificgitcommithashoftherepositorythat
repository, (2) the task definition (e.g., ‚Äútrain a they have used, and the final version list of all
model...‚Äù),(3)themetricsoroutputtobereported dependencies that were installed throughout the
(e.g., ‚ÄúF1 metric on the validation set‚Äù), along notebook.2 In addition, we instruct them to use
withaspecificstructureofhowtheanswershould defaultparameterswheneverpossible,andtoreport
beformatted,and(4)implementationinstructions
2Thegithashanddependenciesensurethatthesesolutions
(e.g. specific hyper-parameters). The implemen-
canbereproducedinthefutureevenasrepositoryandpackage
tation instructions are important for two reasons: versionschange.
4any decision that they had to make but was not
specified in the original task (e.g., the selection
of a delimiter token or specific hyper-parameters
whennodefaultvaluesareprovided). Weaddthese
decisionstothetaskdescriptionorimplementation
instructions to ensure that any agent solving the
sametaskwouldhaveallthenecessaryinformation
neededtogetthesameresults.
Finally,wemanuallyreviewtheirsolutions,mak-
Figure3: Anabstractdemonstrationofhowsub-problems
ingsurethat(1)thesolutioncorrectlyfollowsthe
areextracted: startingfromagoldend-to-endtasksolution
task,(2)itcanbeexecutedinourenvironment,(3) (left),weremovecells(middle)thatfocusoncertainaspects
(differentlycoloredcellsinthefigure),thencreateamasked
all unspecified decisions have been recorded and
problembydefiningagoalandprefixcells(right).Theprefix
(4)re-runningtheexperimentmultipletimesyields cellsareexecutedintheenvironment,andtheagentmustthen
thesameresults(uptoanerrorof10‚àí2). Ifneeded, writecodetosolvethesub-problem.
weasktheworkerstomakecorrections,ormanu-
indicate that some action was performed, but it
allyfixissues ourselves. Solutionsthatwe could
wasnotnecessarilycorrect(e.g.,atrainingscript
not execute on our Jupyter environment, such as
runsuccessfullybutwithwronghyper-parameters
solutions that had to modify the installed Python
couldbecountedassuccess). Similarly,albeitun-
version were discarded. We provide cost details
likelybydesign,amodelcouldcorrectlysolvethe
andguidelinesinAppendixD.
taskbutnothitallofthelandmarks(e.g.,ifituses
3.1.2 Evaluation an alternate approach or guesses a solution) and
have a lower landmark score. For each gold so-
Accuracy Evaluation. As described in ¬ß3.1.1,
lution we manually extract 2-6 landmark outputs
expertsprovideusadeterministicsolutionforeach
patterns. Thelandmarksmetricevaluatestheper-
task,whichwethenexecuteinourenvironmentto
centageofthesepatternsthatappearintheoutputs
getthegoldanswer,allowingustoevaluateagents
ofanyofthecellsexecutedbytheagent.
basedontheiroutcome. Answersconsistofseveral
values(e.g.,numbersformetrics,stringformodel
3.2 MaskedCodingsub-problemsExtraction
predictions). Wedefinetheaccuracymetricasthe
portion of correctly answered values: where the Solving an end-to-end execution task can often
predicted answer precisely matches the gold one belongandcomplex, consistingofmultiplenon-
(uptoa10‚àí2 error). Unlikereferencebasedevalu- trivial steps. As such, evaluating agents on their
ationusedinclozetestsandvariouspriorcoding abilitytorunanentiretaskprovidesasparsesignal
benchmarks(Liuetal.,2023a,b),outcome-based ofsuccess,whereagentshavetocompletenumer-
evaluationallowsforalternatevalidsolutions. ous steps correctly to succeed, making it harder
to ‚Äúhill-climb‚Äù results. Instead, we want to eval-
Landmark-BasedEvaluation. Sometimesanin- uate models in a more fine-grained way that will
dicationofwhetherthemodelwaspreciselycorrect allowustogetsuccesssignalsforanyincremental
may be too strict, ‚Äúpunishing‚Äù models that make progresstowardsthetask. Tothisend,wepropose
progress but don‚Äôt reach the end. E.g., an agent to focus on a specific sub-problem from the task
thatloadsthedatabutdoesn‚Äôttrainwouldhavethe solutionatatime,leveragingtheexpertsolutions
sameaccuracyasanagentthatfailsatthestart. fromtheexpertset.
Tomeasureprogresstowardsthefinalgoal,we Weturntoanapproachlooselyinspiredbycloze
usethegoldtasknotebookstoidentifylandmark tests(Taylor,1953)andmaskedlanguagemodels
outputs; outputs from the environments that act (MLM;Devlinetal.,2019): givenagoldsolution
as ‚Äúevidence‚Äù that a particular step was run suc- forataskweremove(mask)somepartofit(e.g.,
cessfully. E.g., the explicit output string ‚Äú*** code that solves a dependency installation issue),
training completed ***‚Äù in Figure 1 or the andmanuallydefinethesub-problemsuchthatan
string ‚ÄúLoading data... 100%‚Äù implying suc- agent only needs tosolve thisnarrower aspect of
cessfuldataloading. theoveralltask,ascapturedbytheremovedcells.
Importantly,aperfectlandmarkscoredoesnot While the ideal goal is for the agents to com-
entailaperfectaccuracyscore,aslandmarksonly plete tasks in an end-to-end manner, extracting
5maskedsub-problemsallowsustoevaluateagents withthesamemetricsdefinedin¬ß3.1.2.
onabroadrangeoftechnicalchallengesoftenen-
3.3 AutomaticallyGeneratedTasks
counteredwhenworkingwithresearchrepositories,
whileperformingafiner-grainedanalysisoftheir TheExpertandMaskedsetsprovidevalidatedprob-
performance. In addition, this setup aligns well lemsandreproduciblesolutions,allowingformore
withtheusageofinteractivecodeassistants(e.g., accurate evaluation of agents. However, creating
CoPilot and Colab‚Äôs AI-powered coding), where experttasksisbothtime-consumingandcostly,and
agentsassistusersthathavealreadywrittenoreven the limited number of tasks hinders their use for
executedsomecode,andcanspecifywhatissueor agent improvements, such as fine-tuning models
problemremainstobesolved. based on trajectories and environment feedback
(e.g.,Chenetal.,2023;Songetal.,2024;Yinetal.,
Masked Coding Sub-Problems. Each masked
2024). Toaddressthis,weautomaticallygenerate
sub-problem consists of (1) a textual description
tasksusinganLLM(namely,GPT-4o).
of the remaining tasks to execute or issues to be
resolved (e.g., ‚Äúfix the runtime error to complete 3.3.1 Construction
training‚Äùforasub-problemwherecellsthatfixa Generationinvolvestwosteps: (1)filteringsuitable
runtimeerrorwereremoved),and(2)codeprefix repositories,and(2)generatingtasksbasedonthe
(e.g.,theothercellsfromtheoriginalnotebookthat readmesoftheserepositories.
were notmasked). Givena maskedsub-problem,
Filtering. Westartwiththesamelistofreposito-
thecodeprefixispre-executedintheenvironment,
riesfrom¬ß3.1.1,selectingthoselistedin2021or
and agents must then execute their own cells to
later,resultinginatotalof5915repositories. Many
solvethesub-problem.
oftheserepositoriescannotbetriviallyusedtogen-
ExtractionProcedure. Weextractmaskedcod- eratetasksduetovariouslimitations: somedonot
ing sub-problems in a manual process where we supportrunninganyexperiment,somehavemiss-
firstidentifycertaincells(notnecessarilyconsec- ingdatasetlinks,somerequireGPUhardware,and
utive) in the gold solution that focus on a certain somedependonAPIsofotherLLMs(suchasOpe-
aspect. For example, the cells corresponding to nAIorAnthropic),whichcouldbeunavailableor
loading and modifying the dataset could be com- incurcostsforusers. Toalleviatetheseissues,we
binedintoadataconfigurationblockasshownin employacombinationofheuristicandLLM-based
pink in Fig. 3. We extract sub-problems by first filtering methods. Specifically, we keep reposito-
maskingablock(Issuesolvingorangeblockinthe riesthatmentionspecificdatasetsandmodels,do
figure). Wethenidentifycellsthatdonotdepend notuseAPIs,anddonotrequireGPUs. Detailed
on the masked block and define a goal that is re- informationonthefiltersandthepromptusedfor
mainingtobecompleted(e.g.,makingtrainingrun LLM-basedfilteringcanbefoundinAppendixB.
inourexample). Thesecellsarepre-executedand
CreatingTasks. Giventhefilteredrepositories,
theagent‚Äôstaskistocompletethegoal.
we prompt the LLM with the contents of each
Weusethemaskedblockandgoalcelltodefine
repository‚ÄôsREADMEfileandinstructittocreate
thesub-problem,e.g.,ifthecodetohandleexecu-
anexperiment-runningtask. Thisincludesdefining
tiononCPUhasbeenmasked,thesub-problemdef-
thegoal,dataset,model,andscripttoberun(‚ÄúRun
initionwouldbe‚ÄúIhavealreadyexecutedsomeof
probability-based prompt selection on the SST-2
therequiredsteps. Now,youshouldmaketheneces-
datasetusingopt-125masthebasemodelwiththe
sarychangestomakesurethecoderunsonaCPU.
script‚Äòrun_prompt_selection.py‚Äò‚Äù). Wealsospec-
Your goal is to successfully run ‚Äòtrain.py‚Äô.‚Äù. We
ifythattheLLMshouldchoosethesmallestmodel
choosecellswithclearlyidentifiablesuccessindi-
within model families (e.g., BERT-base if BERT
catorsasgoals,e.g.,successfullyrunning‚Äòtrain.py‚Äô
models are supported). See Appendix B for fur-
wouldproducemetricsoncompletion.
ther details on the generation process. To verify
Evaluation. Since each sub-problem has a the quality of the generated set, we sample 100
clearly defined goal, extracted from the original generatedtasksandfindthat81%ofthesamples
task,wecanusethesameoutputsandlandmarks are feasible; among the rest, the most prominent
asintheexpertset,andsimilarlyevaluateaccuracy issue was missing resources (dead or gated links
andlandmarks(¬ß3.1.2). Weevaluatesub-problems todatasets,missingcode)andconceptuallyflawed
6Category(%) Portion GoldLOC Example(s)descriptionofagoldsolution
Dependencies 19.7% 4.1 Downgrade‚Äòtransformers‚Äòversiontoallowexecutionofanolderrepository
CPU 7.2% 5.1 Remove‚Äò.cuda()‚Äòfromdifferentlocationsincode
Configuration 12.5% 8.2 EditPythonorshellscriptstosethyper-parametersandexperimentdetails
Data 23.7% 22.7 Downloadcustomdataset,updatedataloader,limittoloadingfirst10samples
Issue 9.2% 5.8 Pytorchincompatibletensorshapes;incorrectlyloadedPythonpackage
Goal 25.0% 6.5 Runtheevaluationscriptthenloadgeneratedfiletoreportmetrics
Other 2.6% 3.8 Savethemodelaftertrainingtoallowevaluationloading
Table3: Distributionofsubproblemscategoriesanddescriptionofrepresentativesolutionsfromexperts. LOC
standsforlinesofcode,countingthenumberoflines(excludingcomments)inthegoldsolution.
taskssuchasaskingtouseadiscriminativemodel (LOC)areneededtosolveatask,wecountthenum-
foragenerativetask. beroflines(excludingcomments)inthegoldsolu-
tion,andforeditingcells,thenumberofchanged
Difference from Expert Set. Importantly, the lines.3 Anaverageof44.3LOCand14.4cellsper
Auto tasks exhibit different properties than those
solution suggest that these tasks are particularly
of the Expert set: problems in the Expert set can
challengingduetopotentiallylongagenttrajecto-
requiretrainingorinferenceondatasetsnotspecif-
ries. Consequently,theabilitytosolvethesetasks
ically mentioned to be supported, in some cases
provides an important signal on the performance
withdifferentformatsandcolumns,whereasAuto
ofagentsinhandlinglongandcomplextasks.
tasks focus more on getting the dependencies in-
TheMaskedsetcontains152maskedcodingsub-
stalledandbeingabletostartanexperiment. More-
problemsderivedfromthe45experttasks. Likethe
over,theExpertsetonlyincludesproblemswhere
Expertset,eachsub-problemispairedwithagold
we were able to run the solutions in our Jupyter
outputandlandmarks. Table3showsthedistribu-
environment,whileproblemsintheAutosetcould
tionoftheextractedsub-problemsacrossvarious
potentiallyinvolveevenmorechallengingenviron-
categories,alongwitharepresentativesolutionfor
mentssetupsweavoided,suchaswhenchanging
eachcategoryandtheaveragelinesofcode(LOC)
a Python version is required. Finally, Auto tasks
that were executed in the gold solution. Finally,
sometimesrequirenavigatingthroughwebpages
ourautomatedsetincludes604problems,allfrom
that were mentioned in the repository‚Äôs Readme
uniquerepositories.
filetodownloaddatasets.
Toverifythattherepositoriesusedinourbench-
marksareindeed‚Äòlow-profile‚Äô,wecountthenum-
3.3.2 Evaluation
ber of GitHub stars in the source repositories as
Withoutexpertsolutions,wecannotevaluatebased
a proxy for popularity as shown in Table 1. Intu-
onoutcomesorlandmarks. Instead,weuseasim-
itively,popularitylooselycorrelateswiththequal-
pleheuristicmetric,termedScript-Executed,toen-
ityofdocumentationandreadiness,whichaffects
surethemodelsetsupandexecutestheexperiment
the difficulty of experiments execution. We see
without unresolved issues: we check if the script
thatthemediannumberofstarsforourrepositories
theagentwasaskedtorunexecuteswithoutexcep-
(14)isconsiderablylowerthanothercomparable
tionsforaminimumduration(seeAppendixB.2
datasets(seeAppendixAfordetails).
for details). The minimum duration ensures that
thescriptwassuccessfulanddidn‚Äôtjustfailsilently.
4 Experiments
Whilethismethoddoesnotguaranteeperfecteval-
uation,wefinditsurprisinglyeffective,asweshow Experimental Setup. We limit the execution
inouranalysisin¬ß4.3. time (i.e. time to run commands, not counting
theAPIreplytime)ofeachproblemto30minutes.
3.4 SUPERBenchmark Werunalltasksoncomputeinstancesusingsand-
TheExpertsetconsistsof45collectedtasks,where
boxedexecutioninModal(https://modal.com),
each problem is paired with a gold output for
3The gold LOC can be dramatically lower than those
outcome-basedevaluationandasmallsetofland- neededbytheagent; expertsdidnothavetowritecodeto
marksforoursofterevaluationmetric(anaverage readcontentsoffilesintherepositoryastheycanusetheir
IDEorbrowser, whileagentshavetobrowsefilesthrough
of3landmarksperproblem).
anexplicitcommand.Inaddition,expertsdidnotnecessarily
To roughly estimate how much lines of code keepcellswithfailedattempts.
7which allows us to evaluate problems safely and Agent Model Acc. Landm.
concurrently, speeding up evaluations. We limit
SWE-Agent GPT-4o 16.3¬±2.1 36.8¬±2.3
thetotalinputtokensnumber(summingallsteps) React GPT-4o 12.2¬±1.0 33.6¬±0.9
React-Super GPT-4o 14.4¬±2.2 42.6¬±2.9
of each sub-problem to 400k, and of expert and
SWE-Agent GPT-4omini 3.3 16.1
auto-generated tasks to 600k. The compute cost
React-Super GPT-4omini 5.6 20.6
forexecutingeachprobleminModal(notcounting SWE-Agent Llama3.170B 5.6 4.8
React-Super Llama3.170B 6.1 9.6
APIusage)is2-3cents,makingitnegligiblecom-
SWE-Agent Mixtral8x22B 1.1 0.0
pared to API costs. If time or token limit occurs
React-Super Mixtral8x22B 3.3 3.7
beforetheagentsubmitstheanswer,thetaskiscon-
cludedwithoutsubmission(insuchcases,agents Table4: ResultsonExpert,withGPT-4onumbersaver-
get 0 accuracy, but are still scored based on the agedacross3seeds.
landmarkevaluation). Wevalidatethatrunningthe
goldtrajectoriesyieldsperfectscoresonallofour long (e.g., the output of certain training scripts
metricsinthreeconsequentattempts. Toestimate anddependencyinstallationreach10k-40ktokens).
variance in our results, we average results over 3 ReAct agents ‚Äúaccumulate‚Äù history information
seeds for the expert set tasks with the strongest (thought,action,andobservationtriplets)ateach
underlyingLLM(tolimitcosts). step, which makes token usage grow rapidly. As
agentsaretypicallylimitedtoafixedbudget(either
Underlying LLMs. We experiment with
costortokens),thiscouldleadtofailures. Weapply
agents based on commerical LLMs GPT-
truncationstrategiesinallouragentsandbaselines
4o (gpt-4o-2024-08-06) and GPT-4o
tomitigatethisissue. SeeAppendixCfordetails.
mini (gpt-4o-mini-2024-07-18) (OpenAI,
2023), as well as the open-source models
Mixtral-8x22B-Instruct (Jiang et al., 2024) ReAct-SUPER. TheabilitytoexecutePythonand
and Llama 3.1 70B (Dubey et al., 2024) bash commands, in theory, allows agents to per-
(Meta-Llama-3.1-70B-Instruct-Turbo), both form any necessary task. However, these actions
servedbyhttps://www.together.ai/. arestilllimitedcomparedtohumanswhocanuse
IDEstobrowseandeditfiles. Inourearlyexperi-
4.1 Baselines
ments,weindeedfoundthatagentsstruggletoedit
In this section, we describe the three baseline files(e.g.,changeconfigs)usingjustbash.
agentsthatweevaluateonSUPER:ReAct(Yaoetal.,
To address this challenge, we supplement the
2023), our improved version ReAct-SUPER, and agent with an additional edit action, similar in
SWE-Agent(Yangetal.,2024a). Allofouragents spirittotheAgent-ComputerInterfacesfromSWE-
aregivenaccesstoaJupyternotebookenvironment Agent (Yang et al., 2024a). Specifically, the edit
wheretheycanexecutePythonorBashcommands. command accepts three parameters: the name of
Other than the execute action, they can also sub- thefile,theexactcontentofthelinestobereplaced,
mitananswerwhendone. Forsub-problems,we and the content to replace it with. We do not ask
executetheprovided‚Äòpre-execute‚Äôcells(¬ß3.2)and the agent to provide line numbers (as needed by
passthemasanexistinghistoryofactionstoeach gitpatchesorSWE-Agent),andprovidetheagent
agent. Withend-to-endtasks(ExpertandAuto),we withsuggestionsincasetheexactcontentoflines
simplyprompttheagentwiththetaskdescription. tobereplacedwerenotfound(e.g.,ifwhitespaces
aremissing). SeeApp.Eformoredetails.
ReAct (Yao et al., 2023) is a baseline agent
that iteratively prompts the underlying LLM to
output both an action and a natural language SWE-Agent (Yang et al., 2024a) is a ReAct-
‚Äúthought‚Äù,providingtheinteractionhistoryascon- basedagent, originallydesignedtosolveGitHub
text. Each step, the generated action is executed issues. LikeReAct-SUPER,thisapproachprovides
againsttheenvironmentanda<thought,action, agentswithtoolsthatalloweasiereditingoffiles,
observation>tupleisaddedtothehistory,until but also tools for reading files, scrolling through
theagentsubmitsananswer,orexceedstokenor theircontentsandmore(seeoriginalpaperforde-
computelimitations. tails). Weimplementthisagentinourenvironment
One challenge associated with running experi- andmodifytheprompttoaddresstheexecutionof
mentsisthatoutputobservationscangetextremely researchtasksinourenvironment.
8Agent Model Script-Executed Agent Model Acc. Landm.
SWE-Agent GPT-4o 18.0 SWE-Agent GPT-4o 46.1 74.9
React GPT-4o 14.0 React GPT-4o 37.0 65.7
React-Super GPT-4o 18.8 React-Super GPT-4o 41.6 72.5
SWE-Agent GPT-4omini 5.2 SWE-Agent GPT-4omini 27.0 51.8
React GPT-4omini 16.0 React-Super GPT-4omini 31.5 58.3
React-Super GPT-4omini 14.8 SWE-Agent Llama3.170B 17.4 35.0
React-Super Llama3.170B 22.8 38.3
Table5: Resultson250oftheAutotasks. SWE-Agent Mixtral8x22B 9.5 26.6
React-Super Mixtral8x22B 7.0 13.2
Reflecting Agents. To explore whether agents
Table 6: Results of our baselines on SUPER (Masked)
can improve their performance by reflecting on
withdifferentunderlyingLLMs.
their failures, we evaluate agents with a reflec-
tion mechanism (Shinn et al., 2023). Whenever
an agent‚Äôs first attempt to complete a task fails mostly consistent with the ranking of the models
to submit any answer, we prompt the underlying ontheMaskedset,suggestingpotentialusefulness
LLMtoreflectonthetrajectoryanddeviseaplan ofthissetforfuturedevelopment.
toavoidthesamemistakesinsubsequentattempts.
Ablations. ComparingReactwithReAct-SUPER
Thisreflectionisthenincorporatedintotheagent‚Äôs
showsthattheeditingfunctionenablestheagentto
promptandittriesagaintosolvetheproblem. The
hitmorelandmarks(72.5%vs65.7%)andproduce
agent is given k tries to solve the problem with
moreaccurateanswers(41.6%vs37.0%). Wefind
eachtrybeinggiven1/kth ofthetokenbudget.
that without the editing command, the agent usu-
allyresortstoeditingfileswiththesedcommand,
4.2 Results
whichisdesignedforsimplesingle-lineedits.
ExpertSet. Weshowresultsfortheexpertsset
inTable4,withtheresultsforthemostperformant Canagentsthatreflectdobetter? Wenexteval-
LLMaveragedacrossthreeseeds(decodingtem- uateifretryingafterreflectingonfailurescanim-
perature is 0.2). The low accuracies (12.2-16.3) provetheperformanceofourbaselineagents,with
suggestthatcurrentagentscannotyetperformthis k = 3retries. AsshowninTable7,theadditional
task well. However, in some cases, agents make retrieswithreflectionshaveapositivebutminimal
someprogresstowardsthegoal,asevidentbythe impact on the score. If models lack the inherent
landmarksmetric,suggestingthatagentscouldstill abilitytoresolvesomeoftheseissues,retrialwith
behelpfulinsettinguprepositories. reflectionsarenotlikelytohelp.
MaskedSet. WeshowresultsinTable6onthe
4.3 ErrorAnalysis
Masked set, demonstrating that SWE-agent cor-
TheMaskedsetcategorizeseachproblem,allowing
rectly solves a significant portion (46.1%) of the
ustobreakdownperformanceofagents(Table3).
challengesthatarerequiredtoset-upandexecute
We find that the hardest categories for the agent
experiments from research repositories, but that
aredata(27%),configuration(38%)andgoal(43%
mostsub-problemsarestillunsolved. Thehigher
accuracy),whereasCPU,issuesanddependencies
landmarksevaluationscore(74.9%)suggeststhat
areeasier(73%,61%and54%respectively). These
agents often make progress towards solving the
findings suggest that agents are better at solving
sub-problems,evenifsomeofthestepsmightnot
sub-problemswherethereisaspecificerrormes-
necessarilybecorrect.
sagetobesolved(suchasCPUsupporterrors,in-
We find that SWE-agent performs better than
ReAct-SUPERwithGPT-4oastheLLM,butslightly compatibledependencies,orexceptions)thanmore
open-endedproblemssuchasconfiguringdataload-
worse with all weaker models, suggesting that
ingforacustomdataset.
weakermodelswerelesseffectivelyinleveraging
Specifically, for the latter case, we find that
SWE-Agent tools. The open-source Mixtral and
agents commonly skip going through the repos-
Llamareachsignificantlylowerscoresonboththe
itory to understand relevant code. For example,
MaskedandExpertsets.
theyoftenhallucinateargumentsofscriptsorfunc-
Auto Set. We show in Table 5 results for the tions instead of looking up how they should be
Autotasks,whererankingofmodelsandagentsare called(e.g.,addingn_examples=10whennosuch
9Agent Acc. Landm. highlightsmanyofthecorechallengesinbuilding
ReAct-SUPER 41.6 72.5 autonomousLLM-basedexecutionagents,suchas
Reflexion(Shinnetal.,2023) 45.4 76.6 repository reasoning and code editing, which we
hope will help the community make measurable
Table7: ResultsoftheReAct-SUPERagent(usingGPT-4o)
progressonthisimportantproblem.
withandwithoutReflexionontheMaskedset.Whileretrying
with reflection does help improve the submission rate and
accuracy,SUPERbenchmarkstillremainschallenging. Limitations
DatasetSize. Thedatasetsizeofourbenchmark,
argumentisdefined),ortheopposite: theymissa
comprising 45 and 152 sub-problems, is smaller
scriptparameterandattempttochangetheminfiles
comparedtosomeotherbenchmarksavailablefor
unsuccessfully. Additionally,oncetheycommitto
agent evaluation, which could potentially affect
aparticularapproach,theyneverreconsidertheir
thestatisticalsignificanceofperformanceevalua-
decision until failure. These issues suggest that
tions. However, the use of smaller, high-quality
agentsshouldbebetterdesignedtoanalyzerepos-
benchmarksisnotuncommon. Forinstance,bench-
itoriesandconsidermultiplesolutionapproaches.
We provide all trajectories of ReAct-SUPER and marks such as HUMANEVAL (Chen et al., 2021),
SWE-Agentinourcoderepository.4
CLASSEVAL (Duetal.,2023),and BAMBOOGLE
(Pressetal.,2023)contain164,100,and125exam-
Effectivenessofproxymetric. Whileweevalu- plesrespectively,andarewidelyusedforassessing
atetheExpertandMaskedsetsbasedonsolutions modelperformance. Inaddition,recentworkhas
of experts, for the Auto set we have no such so- suggestedthatreducinglargedatasetstoasfewas
lutions, and therefore rely on the weaker Script- 100examplesdoesnotdiminishtheireffectiveness
Executedproxymetric(¬ß3.3.2). Toverifythatthis (MaiaPoloetal.,2024). Moreover,smaller-sized
proxymetricisreliable,weusethetrajectoriesof datasets offer the advantage of being less expen-
ReAct-SUPER on the Masked set to compare the sivetooperate,thusprovidingbetteraccessibility
Script-Executedmetricwiththeaccuracyandland- forresearcherswithlimitedresources,particularly
markmetrics. WefindthatScript-Executedagrees when running interactive agents in environments
withlandmark(assumingascoreof1whenland- that generate long outputs. Finally, our provided
mark>0.5)in90%ofthecasesandwiththeaccu- Auto set with 604 problems offers problems pur-
racymetricin69%ofthecases. Weidentifiedtwo posedfordevelopment,whichalleviatestheriskof
cases of disagreement with the landmark metric: overfittingtotheevaluationsets.
(1)thetargetscriptransufficientlytogetthecor-
Programming Languages and Domains. We
rectanswer, butstillencounteredanexceptionat
have only collected solutions written in Python,
theend(e.g.,anexceptioncreatingafigure)which
andourenvironmentonlysupportsthatprogram-
wouldbeconsideredincorrectbytheproxymetric
ming language. We focus mostly on text-based
(2) the script ran for the minimum time making
repositories. Whilethechallengesassociatedwith
itappearlikeasuccessbasedontheproxymetric
runningtheserepositorieslikelyoverlapwithother
onlytofailduetoamis-configurationorexceptions
domains,increasingthediversityoftherepository
muchlater(andnotreachingtheanswer).
domainscouldbebeneficial.
5 Conclusion
EvaluationBasedonExternalResources. Run-
OurworkintroducesSUPER,abenchmarkdesigned ning benchmarks in realistic environments often
toevaluateLLM-basedagentsonexecutingtasks dependonexternalresources. Inourcase,agents
from code repositories, focusing specifically on relyonavailabilityofresourcessuchasGitHub,pip
low-profile research repositories encountered in anddatasets,whichwecannotcontrolacrossruns.
thewild. Weshowempiricallythatourbenchmark While completely sand-boxed setups could have
is difficult, even for the current best commercial allowed for a more controlled evaluation, we opt
LLMs such as GPT4, both on landmark and end- forfidelity, similarlytoe.g. benchmarksforweb
to-endtaskevaluations(e.g.,GPT-4osolvingonly agentsthatrelyonaccesstorealwebsites(Mialon
46.1%ofthesub-problems). Ourbenchmarkalso etal.,2024;Heetal.,2024,interalia).5
4https://github.com/allenai/super-benchmark/ 5Notethatallourevaluationsarerunusingthesamebase
tree/main/trajectories Dockerimagewithsand-boxedcodeexecutionandshouldbe
10EthicalConsiderations YangruiboDing, ZijianWang, WasiAhmad, Hantian
Ding, Ming Tan, Nihal Jain, Murali Krishna Ra-
Whileautonomousresearchexecutionagentscould manathan,RameshNallapati,ParminderBhatia,Dan
significantlyenhanceresearchadvancements,there Roth, et al. 2024. CrossCodeEval: A diverse and
multilingualbenchmarkforcross-filecodecomple-
is a risk of over-reliance on these agents, which
tion. Advances in Neural Information Processing
couldleadtoconclusionsdrawnbasedonincorrect
Systems,36.
implementationsofagents,andcarelessactorsnot
checkingtheagent‚Äôsreproductionworkcarefully. XueyingDu,MingweiLiu,KaixinWang,HanlinWang,
Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng
Acknowledgments Sha, Xin Peng, and Yiling Lou. 2023. ClassE-
val: A manually-crafted benchmark for evaluating
llmsonclass-levelcodegeneration. arXivpreprint
WethankOriYoranforhisvaluablecommentsand
arXiv:2308.01861.
suggestions. We also thank the Upworker expert
programmersfortheirworkonthesolutionstothe
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,
Expertsetproblems. AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
AkhilMathur,AlanSchelten,etal.2024. Thellama
3herdofmodels. Preprint,arXiv:2407.21783.
References
SiyuanGuo,ChengDeng,YingWen,HechangChen,
JacobAustin,AugustusOdena,MaxwellNye,Maarten Yi Chang, and Jun Wang. 2024. DS-Agent: Au-
Bosma, Henryk Michalewski, David Dohan, Ellen tomated data science by empowering large lan-
Jiang, Carrie Cai, Michael Terry, Quoc Le, and guage models with case-based reasoning. ArXiv,
CharlesSutton.2021. Programsynthesiswithlarge abs/2402.17453.
languagemodels. arXivpreprintarXiv:2108.07732.
Md Mahim Anjum Haque, Wasi Uddin Ahmad, Is-
FedericoCassano,JohnGouwar,DanielNguyen,Syd- mini Lourentzou, and Chris Brown. 2023. FixE-
neyNguyen,LunaPhipps-Costin,DonaldPinckney, val: Execution-basedevaluationofprogramfixesfor
Ming-HoYee,YangtianZi,CarolynJaneAnderson, programmingproblems. In2023IEEE/ACMInter-
MollyQFeldman,etal.2022. MultiPL-E:Ascal- national Workshop on Automated Program Repair
ableandextensibleapproachtobenchmarkingneural (APR),pages11‚Äì18.IEEE.
codegeneration. arXivpreprintarXiv:2208.08227.
Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu,
FedericoCassano, LuisaLi, AkulSethi, NoahShinn, Yong Dai, Hongming Zhang, Zhenzhong Lan, and
AbbyBrennan-Jones,AntonLozhkov,CarolynAn- DongYu.2024. WebVoyager: Buildinganend-to-
derson, and Arjun Guha. 2023. Can it edit? eval- end web agent with large multimodal models. In
uating the ability of large language models to AnnualMeetingoftheAssociationforComputational
follow code editing instructions. arXiv preprint Linguistics.
arXiv:2312.12450.
DanHendrycks,StevenBasart,SauravKadavath,Man-
Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Col-
tasMazeika,AkulArora,EthanGuo,CollinBurns,
lier, Karthik Narasimhan, and Shunyu Yao. 2023.
SamirPuranik,HoraceHe,DawnSong,andJacob
FireAct: Towardlanguageagentfine-tuning. ArXiv,
Steinhardt.2021. Measuringcodingchallengecom-
abs/2310.05915.
petencewithapps. NeurIPS.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
SiruiHong,YizhangLin,BangbangLiu,BinhaoWu,
Yuan,HenriquePondedeOliveiraPinto,JaredKa-
DanyangLi, JiaqiChen, JiayiZhang, JinlinWang,
plan, HarriEdwards, YuriBurda, NicholasJoseph,
LingyaoZhang,MingchenZhuge,etal.2024. Data
Greg Brockman, et al. 2021. Evaluating large
interpreter: An llm agent for data science. arXiv
language models trained on code. arXiv preprint
preprintarXiv:2402.18679.
arXiv:2107.03374.
QianHuang,JianVora,PercyLiang,andJureLeskovec.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
2024. MLAgentBench: Evaluatinglanguageagents
Kristina Toutanova. 2019. BERT: Pre-training of
on machine learning experimentation. Preprint,
deepbidirectionaltransformersforlanguageunder-
arXiv:2310.03302.
standing. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech- NamanJain,KingHan,AlexGu,Wen-DingLi,Fanjia
nologies,Volume1(LongandShortPapers),pages Yan, Tianjun Zhang, Sida Wang, Armando Solar-
4171‚Äì4186,Minneapolis,Minnesota.Associationfor Lezama,KoushikSen,andIonStoica.2024. Live-
ComputationalLinguistics. CodeBench: Holistic and contamination free eval-
uation of large language models for code. arXiv
reproducible;barringanyexternalchanges. preprintarXiv:2403.07974.
11PeterJansen,Marc-AlexandreCot‚Äôe,TusharKhot,Erin Felipe Maia Polo, Lucas Weber, Leshem Choshen,
Bransom, BhavanaDalvi, BodhisattwaPrasadMa- Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin.
jumder,OyvindTafjord,andPeterClark.2024. DIS- 2024. tinyBenchmarks: evaluatingllmswithfewer
COVERYWORLD:Avirtualenvironmentfordevel- examples. arXivpreprintarXiv:2402.14992.
opingandevaluatingautomatedscientificdiscovery
agents. arXivpreprintarXiv:2406.06769. Gr√©goireMialon,Cl√©mentineFourrier,ThomasWolf,
YannLeCun,andThomasScialom.2024. GAIA:a
Albert Q. Jiang, Alexandre Sablayrolles, Antoine benchmarkforgeneralAIassistants. InTheTwelfth
Roux, Arthur Mensch, Blanche Savary, Chris International Conference on Learning Representa-
Bamford, Devendra Singh Chaplot, Diego de las tions.
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam- OpenAI. 2023. GPT-4 technical report. Preprint,
ple, L√©lio Renard Lavaud, Lucile Saulnier, Marie- arXiv:2303.08774.
AnneLachaux,PierreStock,SandeepSubramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao, OpenDevin Team. 2024. OpenDevin: An Open
Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Platform for AI Software Developers as Gener-
Timoth√©eLacroix,andWilliamElSayed.2024. Mix- alist Agents. https://github.com/OpenDevin/
tralofexperts. Preprint,arXiv:2401.04088. OpenDevin. Accessed: 06/11/2024.
JoonSungPark,JosephO‚ÄôBrien,CarrieJunCai,Mered-
Carlos E Jimenez, John Yang, Alexander Wettig,
ithRingelMorris,PercyLiang,andMichaelSBern-
Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R
stein.2023. Generativeagents: Interactivesimulacra
Narasimhan.2024. SWE-bench: Canlanguagemod-
ofhumanbehavior. InProceedingsofthe36thAn-
elsresolvereal-worldgithubissues? InTheTwelfth
nual ACM Symposium on User Interface Software
International Conference on Learning Representa-
andTechnology,pages1‚Äì22.
tions.
Shishir G. Patil, Tianjun Zhang, Xin Wang, and
Y.Lai,C.Li,Y.Wang,T.Zhang,R.Zhong,L.Zettle-
JosephE.Gonzalez.2023. Gorilla: Largelanguage
moyer,S.W.Yih,D.Fried,S.Wang,andT.Yu.2023.
modelconnectedwithmassiveAPIs. arXivpreprint
DS-1000: Anaturalandreliablebenchmarkfordata
arXiv:2305.15334.
sciencecodegeneration. InInternationalConference
onMachineLearning(ICML).
OfirPress,MuruZhang,SewonMin,LudwigSchmidt,
NoahSmith,andMikeLewis.2023. Measuringand
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song,
narrowingthecompositionalitygapinlanguagemod-
Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,
els. InFindingsoftheAssociationforComputational
andYongbinLi.2023. API-bank: Acomprehensive
Linguistics: EMNLP2023,pages5687‚Äì5711,Singa-
benchmark for tool-augmented llms. In The 2023
pore.AssociationforComputationalLinguistics.
Conference on Empirical Methods in Natural Lan-
guageProcessing.
Sheeba Samuel and Daniel Mietchen. 2022. Compu-
tational reproducibility of jupyter notebooks from
YujiaLi,DavidChoi,JunyoungChung,NateKushman,
biomedicalpublications. GigaScience,13.
Julian Schrittwieser, R√©mi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
YongliangShen,KaitaoSong,XuTan,WenqiZhang,
etal.2022. Competition-levelcodegenerationwith
Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li,
alphacode. Science,378(6624):1092‚Äì1097.
andYuetingZhuang.2023. TaskBench: Benchmark-
inglargelanguagemodelsfortaskautomation. arXiv
SiyiLiu,ChenGao,andYongLi.2024. Largelanguage preprintarXiv:2311.18760.
modelagentforhyper-parameteroptimization. arXiv
preprintarXiv:2402.01881. NoahShinn,FedericoCassano,EdwardBerman,Ash-
winGopinath,KarthikNarasimhan,andShunyuYao.
Tianyang Liu, Canwen Xu, and Julian McAuley. 2023. Reflexion: Languageagentswithverbalrein-
2023a. RepoBench: Benchmarking repository- forcementlearning. InNeurIPS.
levelcodeauto-completionsystems. arXivpreprint
arXiv:2306.03091. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian
Li, and Bill Yuchen Lin. 2024. Trial and error:
Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Exploration-based trajectory optimization of LLM
YichiZhang,YanjunShao,ZexuanDeng,HelanHu, agents. InProceedingsofthe62ndAnnualMeeting
ZengxianYang,KaikaiAn,RuijunHuang,Shuzheng oftheAssociationforComputationalLinguistics(Vol-
Si,ShengChen,HaozheZhao,ZhengliangLi,Liang ume1: LongPapers),pages7584‚Äì7600,Bangkok,
Chen, Yiming Zong, Yan Wang, Tianyu Liu, Zhi- Thailand.AssociationforComputationalLinguistics.
weiJiang,BaobaoChang,YujiaQin,Wangchunshu
Zhou,YilunZhao,ArmanCohan,andMarkGerstein. ShaneStorks,KeunwooYu,ZiqiaoMa,andJoyceChai.
2023b. ML-Bench: Evaluatinglargelanguagemod- 2023. NLP reproducibility for all: Understanding
elsforcodegenerationinrepository-levelmachine experiencesofbeginners. InProceedingsofthe61st
learningtasks. arXivpreprintarXiv:2311.09835. AnnualMeetingoftheAssociationforComputational
12Linguistics(Volume1: LongPapers),pages10199‚Äì In Proceedings of the 62nd Annual Meeting of the
10219,Toronto,Canada.AssociationforComputa- AssociationforComputationalLinguistics(Volume1:
tionalLinguistics. LongPapers),pages12380‚Äì12403,Bangkok,Thai-
land.AssociationforComputationalLinguistics.
Wilson L Taylor. 1953. ‚Äúcloze procedure‚Äù: A new
toolformeasuringreadability. Journalismquarterly, FengjiZhang,BeiChen,YueZhang,JackyKeung,Jin
30(4):415‚Äì433. Liu,DaoguangZan,YiMao,Jian-GuangLou,and
Weizhu Chen. 2023. Repocoder: Repository-level
RunchuTian,YiningYe,YujiaQin,XinCong,Yankai codecompletionthroughiterativeretrievalandgen-
Lin, Zhiyuan Liu, and Maosong Sun. 2024. De- eration. arXivpreprintarXiv:2303.12570.
bugBench: Evaluatingdebuggingcapabilityoflarge
languagemodels. arXivpreprintarXiv:2401.04621. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,
RobertLo,AbishekSridhar,XianyiCheng,Yonatan
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man- Bisk,DanielFried,UriAlon,etal.2023. WebArena:
dlekar,ChaoweiXiao,YukeZhu,LinxiFan,andAn- Arealisticwebenvironmentforbuildingautonomous
imaAnandkumar.2023a. Voyager: Anopen-ended agents. arXivpreprintarXiv:2307.13854.
embodiedagentwithlargelanguagemodels. arXiv
preprintarXiv:2305.16291.
XingyaoWang,YangyiChen,LifanYuan,YizheZhang,
YunzhuLi,HaoPeng,andHengJi.2024. Executable
codeactionselicitbetterllmagents. arXivpreprint
arXiv:2402.01030.
Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi
Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b.
Mint: Evaluating llms in multi-turn interaction
with tools and language feedback. arXiv preprint
arXiv:2309.10691.
TianbaoXie,DanyangZhang,JixuanChen,Xiaochuan
Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua,
ZhoujunCheng,DongchanShin,FangyuLei,etal.
2024. OSWorld: Benchmarkingmultimodalagents
foropen-endedtasksinrealcomputerenvironments.
arXivpreprintarXiv:2404.07972.
JohnYang,CarlosE.Jimenez,AlexanderWettig,Kil-
ian Lieret, Shunyu Yao, Karthik Narasimhan, and
Ofir Press. 2024a. SWE-agent: Agent-computer
interfaces enable automated software engineering.
Preprint,arXiv:2405.15793.
Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong,
Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan,
Pengyuan Liu, Dong Yu, et al. 2024b. MatPlotA-
gent: Method and evaluation for llm-based agen-
tic scientific data visualization. arXiv preprint
arXiv:2402.11453.
Shunyu Yao, Howard Chen, John Yang, and Karthik
Narasimhan.2022. Webshop: Towardsscalablereal-
worldwebinteractionwithgroundedlanguageagents.
AdvancesinNeuralInformationProcessingSystems,
35:20744‚Äì20757.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran,KarthikRNarasimhan,andYuanCao.2023.
React: Synergizingreasoningandactinginlanguage
models. InTheEleventhInternationalConference
onLearningRepresentations.
DaYin,FaezeBrahman,AbhilashaRavichander,Khy-
athi Chandu, Kai-Wei Chang, Yejin Choi, and
Bill Yuchen Lin. 2024. Agent lumos: Unified and
modular training for open-source language agents.
13A Repositorydetails
Table8showsinformationaboutthe45sourcerepositoriesusedtocreatetheExpertandMaskedsets,
includingtheirname,originalGitHublinkandthenumberofstarsonGitHub.
Task GitHub Stars
colbert https://github.com/stanford-futuredata/ColBERT 2826
textbox https://github.com/RUCAIBox/TextBox 1069
amrbart https://github.com/goodbai-nlp/AMRBART 94
g-transformer https://github.com/baoguangsheng/g-transformer 43
pie-perf https://github.com/madaan/pie-perf 80
safetybench https://github.com/thu-coai/SafetyBench 138
discodisco https://github.com/gucorpling/DisCoDisCo 6
acqsurvey https://github.com/rahmanidashti/acqsurvey 11
curriculum_learning https://github.com/adymaharana/curriculum_learning 9
spa https://github.com/OceannTwT/SPA 5
mezo https://github.com/princeton-nlp/MeZO 1016
mode-connectivity-plm https://github.com/thunlp/mode-connectivity-plm 7
mbib https://github.com/Media-Bias-Group/MBIB 22
quantifying-stereotypes-in-... https://github.com/nlply/quantifying-stereotypes-in-language 1
rah-kbqa https://github.com/yanmenxue/rah-kbqa 6
dir-gnn https://github.com/wuyxin/dir-gnn 115
unsupervisedhierarchicalsymbolic...https://github.com/SiyuLou/UnsupervisedHierarchicalSymbolicRegression 0
conv_graph https://github.com/huawei-noah/noah-research/tree/master/conv_graph 0
mera https://github.com/ai-forever/MERA 55
pira https://github.com/C4AI/Pira 5
pet https://github.com/timoschick/pet 1618
transnormerllm https://github.com/opennlplab/transnormerllm 221
bert-lnl https://github.com/uds-lsv/BERT-LNL 9
blockskim https://github.com/chandlerguan/blockskim 6
data_label_alignment https://github.com/gyauney/data-label-alignment 3
hype https://github.com/yuanhy1997/HyPe 13
paraphrase-nli https://github.com/matejklemen/paraphrase-nli 3
powerfulpromptft https://github.com/zhengxiangshi/powerfulpromptft 71
robust_prompt_classifier https://github.com/adianliusie/robust-prompt-classifier 5
align-to-distill https://github.com/ncsoft/Align-to-Distill 4
inbedder https://github.com/zhang-yu-wei/InBedder 20
transpolymer https://github.com/ChangwenXu98/TransPolymer 51
memorizing-transformers-... https://github.com/lucidrains/memorizing-transformers-pytorch 622
multi3woz https://github.com/cambridgeltl/multi3woz 14
galore https://github.com/jiaweizzhao/galore 1332
amos https://github.com/microsoft/amos 24
glee https://github.com/genezc/Glee 9
parallel-context-windows https://github.com/AI21Labs/Parallel-Context-Windows 98
logme-nlp https://github.com/mainlp/logme-nlp 5
mixup-amp https://github.com/pai-smallisallyourneed/mixup-amp 4
upet https://github.com/wjn1996/UPET 2
dpt https://github.com/xyaoooo/dpt 6
team https://github.com/declare-lab/team 22
cet https://github.com/zzz47zzz/CET 18
linkbert https://github.com/michiyasunaga/LinkBERT 411
Table8: Detailsofthe45repositoriesusedinSUPERalongwithGitHublinkandstarinformationasofSeptember
3rd,2024.
Adding to the information in Table 1, we show below the average and median star ratings for other
comparablebenchmarks(allstarratingsarecollectedasofSeptember 3rd, 2024). Thiswascomputed
automatically from the GitHub API based on the repositories listed in Jimenez et al. (2024), Lai et al.
(2023)andLiuetal.(2023b)(wegrouptogetherboththetrainandtestrepositoriesmentionedinthis
table).
dataset #repos stars(mean(median))
SWE-Bench 12 27,844(12,557)
DS1000 8 55,227(35,309)
MLBench 18 13,099(9,632)
SUPER(Expert) 45 224(14)
SUPER(Auto) 604 96(23)
B AutomaticGenerationofTasks
B.1 TasksGeneration
Theautomatictasksgenerationinvolvestwohigh-levelsteps: filteringrepositories,andcreatingtasksfor
repositories.
14Step1: FilteringRepositories. Westartfrom5915repositorieslistedby‚Äúpaperswithcode‚Äùtobeadded
onyear2021orlaterandhavingmodality‚ÄòText‚Äô. Wethenautomaticallycloneeachoftheserepositories,
filtering out those where: (1) cloning failed, (2) use LLM APIs (based on the occurence of importing
PythonpackagesofLLMprovidersuchasOpenAI,Anthropic,etc.) or(3)noreadmefilewasfound.
Ontheremainingrepositories,wethenusethefollowingpromptonGPT-4o(gpt-4o-2024-08-06)to
filterrepositories.
Repositories Filtering Prompt
Your task is to analyze a GitHub repository and answer the following questions:
1. Can this repository run on one of these datasets (they should be explicitly mentioned in readme
or grepped code)? SST-2, QNLI, QQP, MRPC, RTE, MMLU, wikitext, yelp, ai2_arc, hellaswag,
(cid:44)‚Üí
winogrande, piqa, humaneval, truthful_qa, cnn_dailymail, mnist, xsquad, squad, xnli, mnli,
(cid:44)‚Üí
multi_nli, ag_news, WikiText, gsm8k, cola, triviaqa, hotpotqa, humaneval, fever, boolq,
(cid:44)‚Üí
openbookqa, drop, coqa, GLUE.
(cid:44)‚Üí
2. Can the repository be run on a CPU? It is acceptable if it runs slowly, as we will use small
models and datasets. The repository should be rejected only if it relies on specific GPU
(cid:44)‚Üí
acceleration methods such as LoRA adapters, if it aims to improve GPU utilization or otherwise
(cid:44)‚Üí
relies on specific GPU features.
(cid:44)‚Üí
3. Can the repository be used with any of the following model families: BERT, T5, RoBERTa, GPT-2,
GPT-Neo, DeBERTa, DistilBERT, BART, Pythia, OPT?
(cid:44)‚Üí
4. Does the README provide an example or instructions on how to run an executable Python or Bash
file to start an experiment? If so, provide the name of the executable file. For example:
(cid:44)‚Üí
`python run_glue.py --model_name_or_path roberta-base --task_name mrpc --output_dir
(cid:44)‚Üí
results/ft/roberta_base/mrpc`
(cid:44)‚Üí
Return a json file in this format:
{
"q_supported_dataset_name": "hellaswag",
"q_supported_dataset_reason": "The README mentions the use of the hellaswag dataset, which is
one of the supported datasets.",
(cid:44)‚Üí
"q_cpu": true,
"q_cpu_reason": "The repository does not rely on specific GPU acceleration methods such as LORA
adapters or repositories that improve GPU utilization.",
(cid:44)‚Üí
"q_model_families": true,
"q_model_families_reason": "The repository supports the BERT model family, as indicated by the
presence of 'bert-large' in the model configuration.",
(cid:44)‚Üí
"q_execute_example": true,
"q_execute_example_reason": "The readme provides an example for running a training pipeline on
the hellaswag dataset."
(cid:44)‚Üí
}
We then keep repositories where q_supported_dataset_name, q_cpu, q_model_families and
q_execute_example are all predicted by the LLM to be true, resulting in 1006 repositories. Note
thatthisfilteringprocessisratherconservative;itislikelypossibletogetmorehigh-qualitytasksfrom
repositoriesthatwerefilteredout.
Step2: GeneratingTasks. WeiteratethefilteredrepositoriesandpromptGPT-4o(sameversion)with
thefollowingtexttogeneratethetasks.
Tasks Generation Prompt
Create an experiment-running task based on the provided README file of a research code repository.
Output a json dictionary with the following structure:
(cid:44)‚Üí
{"thought": "...", "output": {"task": "...", "entrypoint": "..."}}
Instructions:
1. Choose a specific script (either Bash or Python) for which the README provides an example. If no
example is found, skip the repository (return an empty dictionary). The "entrypoint" field
(cid:44)‚Üí
should specify the script file to be run without any arguments. Use a specific file path, even
(cid:44)‚Üí
if the script is typically executed using a module (e.g., "train/run.py", not "python -m
(cid:44)‚Üí
train.run").
(cid:44)‚Üí
Ensure that the script selected is for running experiments (e.g., evaluation or fine-tuning) and
not a utility script (such as a server or data processing script).
(cid:44)‚Üí
2. The task description should include the following:
1. A statement that reflects the goal of the repository. For instance, use "Fine-tune a model
with the question answering infused pre-training method" rather than just "fine-tune a
(cid:44)‚Üí
model," or "Pre-train a reduced-scale model" rather than just "pre-train a model." If the
(cid:44)‚Üí
repository lacks sufficient detail, you may keep this generic.
(cid:44)‚Üí
152. The specific dataset to be used. If the repository doesn‚Äôt specify supported datasets, skip
the repository. For repositories mentioning a group of datasets (e.g., GLUE), choose a
(cid:44)‚Üí
specific dataset (e.g., MRPC) instead of just mentioning the group. If the README indicates
(cid:44)‚Üí
that the data is unavailable (e.g., "data will be uploaded soon"), skip the repository.
(cid:44)‚Üí
3. The Python script/Bash file/entry point to be run, which should match the "entrypoint" field.
4. The model to be used. To ensure the task is feasible with minimal compute, select a small or
base model from one of the following families:
(cid:44)‚Üí
bert, roberta, t5, gpt, opt, deberta, distilbert, bart, pythia.
Then, select the smallest model in the family, based on this mapping: bert: bert-base, t5:
google-t5/t5-small, gpt: openai-community/gpt2, deberta: deberta-base, bart: bart-base,
(cid:44)‚Üí
pythia: EleutherAI/pythia-70m, OPT: facebook/opt-125m
(cid:44)‚Üí
If the README or repository content does not explicitly mention the model family or model
size, skip the repository.
(cid:44)‚Üí
Here are a few examples.
[...]
Wefilteroutrepositories/taskswhen(1)themodeldecidestoskip(e.g. ifnoindicationofmodelor
dataset), or(2)theprovidedscriptselectedbythemodelcannotbefoundintherepository, orisnota
Pythonorbashfile.
B.2 TasksEvaluation(Script-Executedmetric)
Weuseasimpleheuristictodetermineifascriptwasrunsuccessfully: wecheckifthescriptwasexecuted
withoutanyexceptionsbeingraised(accordingtoprintedoutput), andifitwasexecutedforatleasts
seconds. Weusetimelimittomakesureweavoidanyquickfailuresthatdidnotraiseanexception,such
asmessagesaboutmissingarguments. Basedonthegoldexpertsolutions,wefindthats = 10isagood
trade-offtodistinguishunsuccessfulshortrunsfromsuccessfulones. Importantly,thisevaluationmetric
isanapproximation,andcansurelybeincorrectorevenmanipulatedbyagentsthatareawareofit. Yetas
weshowin¬ß4.3,wefoundittomatchthelandmarksevaluationin90%ofthecases,hopefullymakingit
usefulfordevelopment,andasabasisforpotentiallycreatingevenlargersetsoftasksfordevelopment
andtrainingpurposes.
C PromptsandInteractionHistory
C.1 ReAct-SUPER
WeusethefollowingpromptforReAct-SUPER.
ReAct-SUPER Prompt
Interact in a jupyter notebook to solve the request.
Output a thought and an action.
Thought can reason about the current situation, and action allows you to interact with the Jupyter
notebook. There are three types of actions:
(cid:44)‚Üí
(1) execute, which executes a notebook cell with the command under the "content" field. This could
be a bash command (beginning with !), a python command, or magic command (beginning with %).
(cid:44)‚Üí
(2) edit, which allows you to replace existing lines in a file with new lines. To replace lines,
you will need to provide the lines as they appear before the replacement, and the lines to
(cid:44)‚Üí
replace them with.
(cid:44)‚Üí
In the "content" field, format should be
```
{filename}
<<<BEFORE_EDIT>>>
lines before edit
lines before edit
<<<AFTER_EDIT>>>
lines after edit
lines after edit
```
- Include the entire lines WITH indentation. After editing, make sure indentation is correct.
(3) submit, which submits the final answer (or null if there's no answer to submit), formatted as
JSON (make sure to use double quotes for keys and values).
(cid:44)‚Üí
Always output exactly one "thought" and one "action", and encapsulate the content with
<content>...</content> tags.
(cid:44)‚Üí
I will provide the cell output (observation) for you.
16Tips:
- When a github repository is given, you should start by cloning it. You can read the readme file
and browse the repo for installation and usage instructions.
(cid:44)‚Üí
- Avoid using conda, instead use pip for package management.
- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the
kernel or continue waiting.
(cid:44)‚Üí
You should interrupt if: (1) you are prompted with an interactive message (e.g., "Do you want to
continue? [Y/n]"). In that case, you can bypass it by executing echo "y" | command, or (2)
(cid:44)‚Üí
the execution is taking longer than expected because the wrong task is being executed, such
(cid:44)‚Üí
as a different model or incorrect number of epochs.
(cid:44)‚Üí
You should continue to wait if the current execution is correct and proceeding as expected.
Here are some examples.
Git repository: EleutherAI/lm-evaluation-harness
Query: Evaluate EleutherAI/gpt-j-6B on hellaswag. What's the score?
Thought: First, I need to clone the GitHub repository 'EleutherAI/lm-evaluation-harness'.
Action: execute
<content>
!git clone https://github.com/EleutherAI/lm-evaluation-harness.git
</content>
Observation:
```
Cloning into 'lm-evaluation-harness'...
```
Thought: The repository has been cloned. Next, I should check the README file for installation and
usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.
(cid:44)‚Üí
Action: execute
<content>
!cat lm-evaluation-harness/README.md
</content>
Observation:
```
# Language Model Evaluation Harness ... truncated ...
```
Thought: The README provides detailed instructions for installing the `lm-eval` package and
evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its
(cid:44)‚Üí
dependencies first.
(cid:44)‚Üí
Action: execute
<content>
!pip install -e lm-evaluation-harness/.
</content>
Observation:
```
Obtaining file:///home/nora/lm-evaluation-harness
Installing build dependencies ... truncated ...
```
Thought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can
proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the
(cid:44)‚Üí
instructions in the README.
(cid:44)‚Üí
Action: execute
<content>
!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0
--batch_size 8
(cid:44)‚Üí
</content>
Observation:
```
{
"hellaswag": {
"acc": 0.24603658536585363,
"acc_norm": 0.34436059706446426,
}, ... truncated ...
```
Thought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric
is 0.2460. The `acc_norm` metric is 0.3444
(cid:44)‚Üí
Action: submit
<content>
{"acc": 0.2460, "acc_norm": 0.3444}
</content>
17Query: use pandas to read the file 'data.csv' and display the first 2 rows.
Thought: First, I need to read the file 'data.csv' using the pandas library.
Action: execute
<content>
import pandas as pd
pd.read_csv('data.csv').head()
</content>
Observation:
```
name
0 Alice
1 Bob
2 Charlie
```
Thought: The file 'data.csv' has been read successfully and the first 2 rows have been displayed.
Action: submit
<content>
["Alice", "Bob"]
</content>
Query: Change the model name in the 'config.json' file from 'roberta-base' to 'roberta-large'.
Thought: First, I need to read the 'config.json' file to find the location where I can replace the
model name.
(cid:44)‚Üí
Action: execute
<content>
!cat config.json
</content>
Observation:
```
{
"model": "roberta-base",
"batch_size": 8,
"epochs": 10
}
```
Thought: The file has been read successfully. Now, I can replace the model name in the
'config.json' file.
(cid:44)‚Üí
Action: edit
<content>
config.json
<<<BEFORE_EDIT>>>
{
"model": "roberta-base",
"batch_size": 8,
<<<AFTER_EDIT>>>
{
"model": "roberta-large",
"batch_size": 8,
</content>
Observation: "editing file config.json"
Thought: The model name has been replaced in the 'config.json' file.
Action: submit
<content>
null
</content>
ThedemonstrationsusedinthispromptwerenottakenfromanyoftheSUPERsets,andweredesigned
specificallytoshowthebasicusageoftheactionsthatareavailabletotheagent.
HistoryofInteractions. Followingtheaboveprompt,ateachstepweprovidethehistoryofallpast
interactions by concatenating (thought, action, observation) tuples into a string, which we pass to the
LLMasasinglemessage:
Thought: {{thought}}
Action: {{action}}
Observation: {{observation}}
18WhenexecutingproblemsfromtheMaskedset,somestepsarepre-executed. Werunthepre-execute
commandsintheenviornment(withoutanyagentinteraction)tocollectactionandobservationpairs. We
thenusethesepairsinthehistoryofsucceedingagentsteps,usingthefollowingfixedthought:
[pre-executed by the user]
Sincetrajectoriescanoftengetlong(¬ß4.1),weusethefollowingtruncationstrategyforallagents: for
thelaststep,weprovidethe50klastcharacters,whichisusuallyenoughfortheentireobservation. For
earliersteps,weshortentheobservationstoshowthelast500characters.
C.2 ReflectionAgentPrompt
Weusethefollowingprompttogeneratereflections(Reflexionagent),withoutanydemonstrations.
Reflexion Agent Prompt
You will be given the history of a past experience in which you were placed in an environment and
given a task to complete.
(cid:44)‚Üí
You were unsuccessful in completing the task. Do not summarize your environment, but rather think
about the strategy and path you took to attempt to complete the task.
(cid:44)‚Üí
Devise a concise, new plan of action that accounts for your mistake with reference to specific
actions that you should have taken.
(cid:44)‚Üí
For example, if you tried A and B but forgot C, then devise a plan to achieve C with
environment-specific actions. If you wasted too much time on A, then devise a plan for more
(cid:44)‚Üí
easily and directly achieving A.
(cid:44)‚Üí
You will need this later when you are solving the same task. Give your plan after "Plan" and end it
with [END].
(cid:44)‚Üí
C.3 SWE-AgentPrompt
WetaketheoriginalSWE-Agentpromptandadjustittoinstructtheagenttocompletetheresearchtask
inourenvironmentratherthanfixingGitHubissues,whilemakingsurethetipsandinformationareas
similar as possible to the ReAct-SUPER prompt for fair comparison. We include the exact same three
demonstrationsprovidedintheotheragents, adjustedforSWE-Agenttools, inasimilarformattothe
originalimplementation.
SWE-Agent System Prompt
SETTING: You are an autonomous programmer, and you're working directly in the command line with a
special Jupyter notebook interface.
(cid:44)‚Üí
The special Jupyter notebook interface consists of a file editor that shows you {WINDOW} lines of
a file at a time.
(cid:44)‚Üí
You can execute commands in the notebook using:
1. Bash commands: Commands starting with !.
2. Python commands: Standard Python code.
3. Magic commands: Commands starting with %, e.g., %cd <path>.
Additionally, you can also use the following commands to help you navigate and edit files.
COMMANDS:
{command_docs}
Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION.
If you'd like to add the line ' print(x)' you must fully write that out, with all those
spaces before the code! Indentation is important and code that is not indented correctly will
(cid:44)‚Üí
fail and require fixing before it can be run.
(cid:44)‚Üí
RESPONSE FORMAT:
Your shell prompt is formatted as follows:
(Open file: <path>)
(Current directory: <cwd>)
In [ ]
You need to format your output using two fields: discussion and command.
Your output should always include _one_ discussion and _one_ command field EXACTLY as in the
following example:
(cid:44)‚Üí
19DISCUSSION
First I'll start by using ls to see what files are in the current directory. Then maybe we can
look at some relevant files to see what they look like.
(cid:44)‚Üí
```
!ls -a
```
You should only include a *SINGLE* command in the command section and then wait for a response
from the shell before continuing with more discussion and commands. Everything you include in
(cid:44)‚Üí
the DISCUSSION section will be saved for future reference.
(cid:44)‚Üí
If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit
just the first command, and then after receiving a response you'll be able to issue the
(cid:44)‚Üí
second command.
(cid:44)‚Üí
You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to
the special commands listed above.
(cid:44)‚Üí
SWE-Agent Instance Prompt
We're currently solving the following task in the repository.
TASK:
{query}
INSTRUCTIONS:
Now, you're going to execute this task on your own. You can use any bash commands or the special
interface commands to help you. Edit all the files you need to and run any checks or tests
(cid:44)‚Üí
that you want.
(cid:44)‚Üí
Remember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after
every command.
(cid:44)‚Üí
When you obtain the final answer for the requested TASK, you can submit it by running the
`submit` command.
(cid:44)‚Üí
NOTE ABOUT THE EDIT COMMAND: Indentation really matters! When editing a file, make sure to insert
appropriate indentation before each line!
(cid:44)‚Üí
IMPORTANT TIPS:
1. When a github repository is given, you should start by cloning it. You can read the readme
file and browse the installation and usage instructions. Then you need to set up the Python
(cid:44)‚Üí
environment and install necessary packages before you run any scripts in the repo. Avoid
(cid:44)‚Üí
using conda, instead use pip for package management.
(cid:44)‚Üí
2. If you run a command and it doesn't work, try running a different command. A command that did
not work once will not work the second time unless you modify it!
(cid:44)‚Üí
3. If you open a file and need to get to an area around a specific line that is not in the first
100 lines, say line 583, don't just use the scroll_down command multiple times. Instead, use
(cid:44)‚Üí
the goto 583 command. It's much quicker.
(cid:44)‚Üí
4. Always make sure to look at the currently open file and the current working directory (which
appears right after the currently open file). The currently open file might be in a different
(cid:44)‚Üí
directory than the working directory! Note that some commands, such as 'create', open files,
(cid:44)‚Üí
so they might change the current open file.
(cid:44)‚Üí
5. When editing files, it is easy to accidentally specify a wrong line number or to write code
with incorrect indentation. Always check the code after you issue an edit to make sure that
(cid:44)‚Üí
it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.
(cid:44)‚Üí
6. Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt
the kernel or continue waiting.
(cid:44)‚Üí
You should interrupt if: (1) you are prompted with an interactive message (e.g., "Do you want to
continue? [Y/n]"). In that case, you can bypass it by executing echo "y" | command, or (2)
(cid:44)‚Üí
the execution is taking longer than expected because the wrong task is being executed, such
(cid:44)‚Üí
as a different model or incorrect number of epochs. You should continue to wait if the
(cid:44)‚Üí
current execution is correct and proceeding as expected.
(cid:44)‚Üí
(Open file: {open_file})
(Current directory: {working_dir})
In [ ]
20D InstructionstoExperts
WorkerswerehiredthroughUpwork(upwork.com),werepaid$30-$40/hour,andwerelimitedtofour
hours per task, although in some cases they were approved for up to an additional 2 hours if the task
wasn‚Äôtcompletedintime. Inafewcases,theexpertswereunabletoruntheexperimentsduetoCoLabor
dependenciesissues;thesetaskswerediscarded. Intotal,taskcollectioncost$6,580for50solutions,of
whichwekeepthefinalsetof45tasks.
WeprovidetheinstructionsgiventotheUpworkexpertsinFigs.4to6.
Figure4: Guidelinesprovidedtoexperts.
E Editcommand
Theeditcommandisanactionthatagentscanselectateveryiteration,inadditiontoexecuteandcommand.
Theformatoftheinputtothiscommandisasfollows:
{filename}
<BEFORE_EDIT>
(lines before edit)
<AFTER_EDIT>
(lines after edit)
Where(lines before edit)aretheexactlinestobereplaced,and(lines after edit)arethe
linestoreplacethemwith.
Toprovidetheexactfilecontentsthatshouldbereplaced,agentstypicallyneedtoviewtheexisting
contentsofthefile,forexamplebyusingthecatcommand,andthencopyitverbatimasthelinestobe
21Figure5: Guidelinesprovidedtoexperts(continued).
Figure6: Guidelinesprovidedtoexperts(continued).
replaced.6 Ourenvironmentthenlooksforthecontentstobereplacedinthefileandreplacesitwiththe
6Solutionsthatacceptlinenumbersinsteadofexactcontentperformedworseinourearlyexperiments.
22newcontents.
Theeditcommandrequirestheprovidedreplacedlinestobe(1)preciselycopied,includingcorrect
whitespacesandindentationsand(2)uniqueinthecontentsfile,sothattheeditcommandisnotambiguous.
Tohelptheagentwiththeserequirements,weconfiguretheeditcommandtoprovidespecificfeedbackto
theagentincaseoneoftheseconditionsdoesnotapply.
Specifically,ifthethelinestobereplacedwerenotfoundas-is,buttheselinesdoappearintheedited
filewithoutsurroundingwhitespacesortabs,thentheenvironmentprovidesthefollowingfeedback:
Did you mean to replace the following lines (notice leading/trailing whitespaces
difference)?
(cid:44)‚Üí
followedbyanexactcopyoftheselines,includingthespaces.
Ifmorethanoneinstancesoftheselineswerefound,thefollowingfeedbackisprovided:
Found multiple ([k]) occurrences of the <BEFORE_EDIT> lines. Add 1-3 lines before
or after these lines to replace to disambiguate.
(cid:44)‚Üí
Here are the first two occurrences with additional context, did you mean one of
these?
(cid:44)‚Üí
Occurrence 1: ...
withthefirsttwooccurencesoftheselines.
23