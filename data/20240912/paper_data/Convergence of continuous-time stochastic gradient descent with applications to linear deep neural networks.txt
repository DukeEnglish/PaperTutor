Convergence of continuous-time stochastic gradient descent
Convergence of continuous-time stochastic gradient descent
with applications to linear deep neural networks
Gabor Lugosi gabor.lugosi@upf.edu
Universitat Pompeu Fabra and Barcelona School of Economics
Department of Economics and Business
Ram´on Trias Fargas 25-27, 08005, Barcelona, Spain
ICREA, Pg. Llu´ıs Companys 23, 08010 Barcelona, Spain
Eulalia Nualart eulalia.nualart@upf.edu
Universitat Pompeu Fabra and Barcelona School of Economics
Department of Economics and Business
Ram´on Trias Fargas 25-27, 08005, Barcelona, Spain
Editor:
Abstract
We study a continuous-time approximation of the stochastic gradient descent process for
minimizingtheexpectedlossinlearningproblems. Themainresultsestablishgeneralsuffi-
cientconditions forthe convergence,extending the resultsofChatterjee (2022)established
for (nonstochastic) gradient descent. We show how the main result can be applied to the
case of overparametrizedlinear neural network training.
Keywords: stochastic gradient descent, linear neural networks, Langevin stochastic
differential equation
1 Introduction
Stochastic gradient descent(sgd)is asimplebutremarkably powerfuloptimization method
that has been widely used in machine learning, notably in training large neural networks.
Indeed, sgd has a key role in the spectacular success of deep learning. In spite of its
importance, the method is far from being well understood and significant effort has been
devoted to understanding why large neural networks trained by stochastic gradient descent
learn so efficiently and generalize so well.
Consider the following general setup that encompasses a variety of problems in machine
learning. Let ℓ : RD Rd [0, ) be a loss function that assigns a nonnegative value to
any pair (w,z) where× w →RD is∞ a parameter to be learned and z Rd is an observation.
∈ ∈
We assume throughout that ℓ is twice continuously differentiable in the first variable. Let
Z be a random vector taking values in Rd. One aims at minimizing the expected loss (or
risk) f(w) = E(ℓ(w,Z)) over w RD. To this end, one has access to training data in the
∈
form of a sequence Z ,Z ,Z ,... of independent, identically distributed copies of Z.
0 1 2
Stochastic gradient descent (sgd) is a simple iterative optimization algorithm defined
by an arbitrary initial value w RD and a step size η > 0 that, for k = 0,1,2,.., updates
0
∈
w = w η ℓ(w ,Z ) , (1)
k+1 k k k
− ∇
1
4202
peS
11
]GL.sc[
1v10470.9042:viXraGabor Lugosi and Eulalia Nualart
where denotes the derivative with respect to w. Clearly,
∇
E( ℓ(w ,Z )w ) = f(w ) .
k k k k
∇ | ∇
In this paper we study a continuous-time approximation of the stochastic gradient descent
process. Various approximations have been proposed in the literature. We follow the model
proposed by Cheng et al. (2020), who approximate the sgd process (1) by the Langevin-
type continuous-time stochastic differential equation (sde)
dw = f(w )dt+√ησ(w )dB , (2)
t t t t
−∇
for t 0, where w RD, B is a D-dimensional Brownian motion, η > 0 is a fixed
0 t
param≥ eter that acts as∈ the variance of the noise term, and σ : RD RD RD is a D D
→ × ×
matrix defined as the unique square root of the covariance matrix Σ(w) = Cov( ℓ(w,Z))
∇
of the random vector ℓ(w,Z), that is,
∇
σ(w)σT(w) = Σ(w) .
For the heuristics behindthe approximation of the discrete-time process (1) by (2), we refer
the reader to Cheng et al. (2020). We study convergence properties of (2), as t , for
→ ∞
functions f : RD R and σ : RD SD (with SD denoting the set of positive definite
→ → + +
D D matrices), defined by a loss function as above.
×
Generalsufficientconditionsfortheconvergenceofthe“noiseless”process—corresponding
to η = 0 in(2)—to aglobal minimumof f were established by Chatterjee (2022). While the
behavior of gradient descent has been well understood when f is convex (Nesterov (2013)),
Chatterjee’s conditions go significantly beyond convexity. The main goal of this paper is
to extend Chatterjee’s results to the model of sgd given by (2). The presence of noise
introduces new challenges and addressing these is our main contribution.
A regularity condition for the function f that we require is a “local Lipschitz” condition
for f. This mild assumption guarantees that the equation (2) has a unique local solution,
∇
asexplainedbelow. Itisimportanttokeepinmindthatwedonotrequire f tobeglobally
∇
Lipschitz as it would exclude some important applications in machine learning.
Assumption 1 The functions f and σ are locally Lipschitz continuous, that is, for any
compact set K RD there exist∇ positive constants Lf and Lσ such that for all x,y RD,
⊂ K K ∈
f
f(x) f(y) L x y ,
k∇ −∇ k≤ Kk − k
and
d
σ (x) σ (y) Lσ x y ,
k i − i k ≤ Kk − k
i=1
X
where denotes the Euclidean norm (in RD) and σ ,...,σ denote the columns of the
1 d
k·k
matrix σ.
UnderAssumption1, itis well-known (see e.g., Mao (2007), Mao and Yuan (2006))that
for any initialization w RD, there exists a unique maximal local solution to equation (2)
0
∈
up to its (random) blow-up time
T := T(w ) = sup w < .
0 t
{k k ∞}
t>0
2Convergence of continuous-time stochastic gradient descent
Thismeansthatthereexistsauniquecontinuous adaptedMarkov process(w ) satisfying
t t≥0
the integral equation
t t
w = w f(w )ds+√η σ(w )dB ,
t 0 s s s
− ∇
Z0 Z0
for all t < T a.s., where the stochastic integral is understood in the Itoˆ sense. Moreover, if
T < , then
∞
limsup w = .
t
k k ∞
t→T
Ourmaingoalistoderivesufficientconditionsonthefunctionf underwhichthesolution
w converges to a point where f achieves its minimum. An obvious necessary condition for
t
convergence is that the norm of σ(w) needs to tend to zero as w approaches the set of
minima. In many machine learning applications this is not an unnatural assumption as
often it is reasonable to assume that min Eℓ(w,Z) = 0, which implies that when w is such
w
a minimizer, ℓ(w,Z) = 0 almost surely. We restrict our attention to such cases.
The rest of the paper is structured as follows. In Section 2 we present the main result
of the paper. In particular, Theorem 4 shows that, under Chatterjee’s conditions, together
with some additional conditions on the noise σ(), if the process is initialized sufficiently
·
close to a global minimum, then, with high probability, the trajectory w converges to the
t
set of global minima of f.
In Section 3 we review the related literature. In Section 4 we show how the main result
can be applied to the case of overparametrized linear neural network training. All proofs
are relegated to Section 5.
2 Convergence of the continuous-time sgd
In this section we establish sufficient conditions for the convergence of the process (2) as
t to a global minimizer of the function f. To this end, we state the key assumptions
→ ∞
which, together with Assumption 1, are needed for the main convergence result.
Assumption 2 The function f attains its minimum value which equals zero.
In other words, we assume that there exists w RD such that f(w) = Eℓ(w,Z) = 0,
∈
or equivalently, ℓ(w,Z) = 0 almost surely. We denote the (nonempty) set of minimizers by
S = w RD : f(w)= 0 .
{ ∈ }
An immediate simple consequence of Assumption 2 is that if f attains its minimum
value in a finite time, then the solution of the process stays at that point forever, almost
surely:
Lemma 3 Consider the sde (2) initialized at some w RD. If for some t [0,T),
0
∈ ∈
f(w ) = 0, then T = and for all s > t, w = w .
t s t
∞
Let B (w ) RD denote the closed Euclidean ball of radius r > 0 centered at w .
r 0 0
⊂
3Gabor Lugosi and Eulalia Nualart
The following real-valued functions, defined for w RD, play a key role in formulating
∈
our convergence conditions.
f(w) 2
a(w) := k∇ k ,
f(w)
Tr(σT(w)σ(w))
b(w) := ,
4f(w)
Tr(σT(w)Hf(w)σ(w))
g(w) := ,
2f(w)
where H denotes the Hessian matrix and Tr denotes the trace.
Let
A (r,w ):= inf a(w) ,
min 0
w∈Br(w0),f(w)6=0
B (r,w ):= sup b(w) ,
max 0
w∈Br(w0),f(w)6=0
G (r,w ):= sup g(w) .
max 0
w∈Br(w0),f(w)6=0
If f(w) = 0 for all w B (w ) then we let A (r,w ) = . For η 0, we also define
r 0 min 0
∈ ∞ ≥
θ(r,w ,η) := A (r,w ) ηG (r,w ).
0 min 0 max 0
−
The following theorem establishes sufficient conditions of convergence of the sde, with
positiveprobability,toaminimumoff andalsogivesenestimatefortherateofconvergence.
In order to state the result, we need to define the following stopping time:
τ := τ (w ) = inf w / B (w ) .
r r 0 t r 0
t>0{ ∈ }
Thus, τ is the first time when the process is more than distance r away from its initializa-
r
tion.
The theorem shows that if the process is initialized in a sufficiently small neighborhood
of a global minimum of f and the noise parameter η is sufficiently small, then the process
converges to a minimum of f with positive probability.
Theorem 4 Consider the sde (2) initialized at some w RD and suppose that Assump-
0
∈
tions 1 and 2 hold. Assume that there exist r > 0 and η 0 satisfying the following
≥
conditions:
A (r,w )
min 0
η < (3)
G (r,w )
max 0
(which is equivalent to θ(r,w ,η) > 0),
0
1
ηB (r,w ) , (4)
max 0
≤ 4
and
2 f(w ) G (r,w )
0 max 0
p := 1+√η + B (r,w ) < 1 . (5)
max 0
r pθ(r,w 0,η) p θ(r,w 0,η) !!
p
p p
4Convergence of continuous-time stochastic gradient descent
Then
P(τ τ = ) 1 p >0 . (6)
r
∧ ∞ ≥ −
Moreover, conditioned on the event τ τ = , w converges almost surely to some
r t
{ ∧ ∞}
x∗ B (w ) S, and for all ǫ > 0,
r 0
∈ ∩
r
P( w x∗ > ǫ τ τ = ) e−θ(r,w0,η)t/2. (7)
t r
k − k | ∧ ∞ ≤ ǫ
When η = 0, Theorem 4 reduces to (Chatterjee, 2022, Theorem 2.1) that establishes
convergence of (non-stochastic) gradient descent. In that case, assumption (5) coincides
with Chatterjee’s condition in Chatterjee (2022), that is,
4f(w )
0
< A (r,w ) (8)
r2 min 0
andtherateof convergence obtained inTheorem4isthesameas inChatterjee(2022)when
η = 0, that is, e−Amin(r,w0)t/2.
The additional conditions required to handle the stochastic case involve the functions
b(w) and g(w). In particular, it is required that B (r,w ) < and G (r,w ) <
max 0 max 0
∞ ∞
for a value of r for which B (w ) S = . In Section 4 we show that these assumptions
r 0
∩ 6 ∅
are plausible in overparametrized neural networks. We show that the assumptions hold
for linear neural networks but it is reasonable to expect that the conditions also hold in
more general cases. However, checking these conditions is nontrivial and further research is
needed to show how the theorem may be applied for nonlinear neural networks.
The theorem establishes convergence to a global minimum of f with probability 1 p
−
where p is bounded away from 1 for sufficiently small r and η. Note that as η 0,
→
the theorem does not imply that p converges to 0 under the condition (8). This lack of
continuity may be an artefact of the proof and it is natural to conjecture that p 0 as
→
η 0. On the positive side, as it is shown in Section 4, in the example of linear neural
→
networks, the probability of convergence can be made arbitrarily close to one by choosing
r and η sufficiently small.
The theorem shows that if the process is initialized sufficiently close to a minimum–and
f and σ satisfy certain properties–, then convergence occurs with positive probability. We
speculate that in many cases this result implies a much stronger property, namely that
from an arbitrary initialization, the process converges almost surely to a global minimum
of f. The reson is that, on the one hand, by the Markovian nature of the process, for
convergence to hold with positive probability, it suffices that there exists a time t 0 such
≥
that there exist r > 0, such that inf w w < r and η 0 satisfying the conditions
w∈S t
k − k ≥
(3), (4), and (5) with w replaced by w . In other words, it suffices that the process enters a
0 t
sufficiently small neighborhood of the minima at some point in time. For this it is sufficient
that f(w) as w (implying that the set S of global minima is compact)
k∇ k → ∞ k k → ∞
and, away from the set of minima, the process has a diffusive nature. For example, it is
required that for any closed ball B not intersecting S, almost surely the process does not
stay in B forever. This is reasonable to expect because of the structure of the “noise” σ():
·
when f is bounded away from zero, so is the norm of each column of σ. Establishing such
statements of almost sure convergence from arbitrary initialization goes beyond the scope
of the present paper and is left for future research.
5Gabor Lugosi and Eulalia Nualart
3 Related literature
A significant effort has been devoted to the theoretical understanding of the performance
of gradient descent and stochastic gradient descent algorithms in nonlinear optimization,
with special emphasis on training neural networks. It is both natural and useful to study
continuous-time approximations of these algorithms. For (non-stochastic) gradient descent
this leads to the study of gradient flows. The case when the objective function is convex
is well understood (Nesterov, 2013). While convexity is an important special case, the ob-
jective function in neural network training is nonconvex and this motivated a large body
of research. Our starting point is a result of Chatterjee (2022) who established a general
sufficient condition for convergence of gradient descent. Chatterjee’s criterion applies to
deep neural networks with smooth activation, implying that gradient descent with appro-
priate initialization and step size converges to a global minimum of the loss function. We
refer the reader to Chatterjee (2022) to comparison with earlier work on sufficient con-
ditions of convergence of gradient descent. Our main result extends Chatterjee’s result
to a continuous-time approximation of stochastic gradient descent under additional condi-
tions that are needed to accommodate the stochastic scenario. Sekhari et al. (2022) take a
different approach to establish general convergence properties of (discrete-time) stochastic
gradient descent by establishing general conditions under which stochastic gradient descent
and gradient descent converge to the same point. In our analysis there is no reason why
the two methods should converge to the same point as we analyze the process (2) directly.
WhileChatterjee(2022)showsthatthegeneralsufficientconditions forgradientdescent
toconvergetoanoptimumaresatisfiedforawideclassofdeepneuralnetworks,wehavenot
beenabletoproveananalogous resultfor(2). Thereason isthattheconditions(3)and(4),
that are present due to the stochastic component, are difficult to verify in such generality.
However, for the special case of linear neural networks we could verify these conditions
(under appropriate initialization), see Section 4. We mention here that Bah et al. (2022)
focus on linear multilayer neural networks. They show that the (nonstochastic) gradient
flow always converges to a critical point of the loss functional, see also Nguegnang et al.
(2021). An and Lu (2023) also derive general sufficient conditions for the convergence of
stochastic gradient descent. They write (1) as
w = w η f(w )+η( f(w ) ℓ(w ,z )) ,
k+1 k k k k k
− ∇ ∇ −∇
assuming
f(w ) ℓ(w ,z )= σf(w )Z ,
∇
k
−∇
k k k wk,zk
where σ > 0 is a constant and Z is a zero-mpean noise term with identity covariance.
wk,zk
In the continuous-time limit, this would be equivalent to assuming
σ(w)σT(w) = σ2f(w)E Z ZT .
t t
(cid:0) (cid:1)
This assumption is different from our setup and applies to a different range of problems. In
particular, the simple overparametrized linear regression setup described in Section 4 does
not satisfy this condition.
Li and Gazeau (2021) study stochastic gradient Langevin dynamics similar to (2) and
their discretization but with σ(w ) replaced by a constant.
t
6Convergence of continuous-time stochastic gradient descent
Schertzer and Pillaud-Vivien(2024)investigate theperformanceofasimilarcontinuous-
time model of stochastic gradient descent as (2), for the special case of linear regression,
but not for the overparametrized case considered here.
From thepointofviewof stochastic differential equations, ourresultisinteresting inthe
context of blow up because in the study of (2), we only assume locally Lipschitz coefficient
and therefore there may be a time of explosion and our results show that nevertheless the
process converges with positive probability, see Mao and Yuan (2006) and Mao (2007).
4 Application to linear deep neural networks
In this section we show how Theorem 4 can be applied to the case of training multilayer
linear neural networks by stochastic gradient descent. To this end, we need to check the
conditionsofthetheoremforthisparticularcase. Tosetupthemodel,consideramultilayer
feedforward neural network with linear activation function defined as follows. The weights
of the network are given by w = (W ,W ,...,W ), where W is a 1 q matrix, for
1 2 L L
×
i = 2...,L 1, the W ’s are q q matrices, and W is a q d matrix. W is called the
i 1 1
− × ×
output layer, while the others W ’s are called the hidden layers. The number of layers is
i
L 2, called the depth of the network.
≥
The parameter vector is defined by w = (W ,W ,...,W ) RD, where D = q+(L
1 2 L
∈ −
2)q2+qd. Then, given w RD we consider the function
∈
β(w) = W W W Rd
L 2 1
··· ∈
and the neural network is defined by the map β(w)x, where x Rd. Clearly, β(w)x is a
∈
linear function of x, defined in an overparametrized manner, depending on w in a highly
nonlinear way.
Consideringthe quadratic loss function, the learningproblem consists of minimizing the
function
f(w) = E (β(w)X Y)2 ,
−
(cid:0) (cid:1)
where the pair of random variables (X,Y) takes values in Rd R.
×
In order to apply Theorem 4, we need to assume f(w) = 0 for some w RD, which
∈
means that that Y is a (deterministic) linear function of X. Thus, we assume that there
exists a β∗ Rd, β∗ = 0 such that Y = β∗X, that is, the loss function is given by
∈ 6
ℓ(w,X) = (β(w)X β∗X)2.
−
LetΣ bethecovariancematrixoftherandomvectorX andletλ (Σ )andλ (Σ )
X min X max X
be its minimum and maximum eigenvalues, respectively.
Consider the closed subset of RD
= w RD :β(w) = β∗ .
S { ∈ }
The following theorem gives sufficient conditions for the convergence of the sde (2)
associated to this problem as an application of Theorem 4.
7Gabor Lugosi and Eulalia Nualart
Theorem 5 Assume that λmin(Σ X) > 0 and X K, a.s. for some constant K >
k k ≤
0. Let w be the solution to the sde (2) associated to this problem with initial condition
t
w = (W0,W0,...,W0) RD such that W0 = 0 and the entries of W0,...W0 are strictly
0 1 2 L ∈ 1 2 L
positive. Let γ > 0 be the minimum value of the entries of W0,...W0 and let M be the
2 L−1
maximum of the entries of W0,...W0. Then for all δ (0,1) there exist N > 0 and η > 0
2 L ∈ 0
depending only on λmin(Σ X), γ, M, kβ∗ k, L, D, K, such that if the entries of W L0 are all
N and η η then P(τ τ = ) 1 δ, and conditioned on that event, w converges
0 γ/2 t
≥ ≤ ∧ ∞ ≥ −
almost surely to some element in B (w ) S.
γ/2 0
∩
5 Proofs
Proof [Proof of Lemma 3] If f(w ) = 0, since f is a nonnegative C2 function, then
0
f(w ) = 0. Moreover, since f(w ) = E(ℓ(w ,x)) and ℓ is nonegative, we get that
0 0 0
∇
ℓ(w ,Z) = 0 and thus ℓ(w ,Z) = 0 as it is a C2 function. In particular, σ(w ) = 0.
0 0 0
∇
Then w = w for all t > 0 and the statement is true for t = 0. If w = x for some s > 0
t 0 s
suchthatf(x) = 0,sincetheprocessw istime-homogeneous, thedistributionofw starting
t t
at w = x is the same as the distribution of w starting at w = x. Then by the argument
s t−s 0
above we conclude that w = x for all t > s and this completes the proof.
t−s
Before embarking on the proof of Theorem 4, we start with a few simple lemmas.
Consider the continuous-time martingale (M ) defined as
t t≥0
t f(w )Tσ(w )
s s
M := √η ∇ dB . (9)
t s
f(w )
Z0 s
Observe that its quadratic variation is given by the process (see (Mao, 2007, Theorem 5.21
page 28))
t Tr σT(w ) f(w ) f(w )Tσ(w )
s s s s
M = η ∇ ∇ ds .
h it f2(w )
Z0 (cid:0) s (cid:1)
Applying the multi-dimensional Itˆo formula (see (Mao, 2007, Theorem 6.4 page 36))
to the process
t
= ecMt− 21 c2hMit with c R it is easy to see that
t
is a nonnegative
E ∈ E
martingale. Therefore,
E[ ]= 1 . (10)
t
E
Introduce the stopping time
τ := τ(w )= inf f(w )= 0 .
0 t
t>0{ }
Lemma 6 Consider the sde (2) initialized at some w RD and suppose that Assumption
0
∈
1 holds. Then, for all r > 0, t > 0, and η 0, almost surely,
≥
1
f(w t∧τr∧τ)
≤
f(w 0)e−(t∧τr∧τ)θ(r,w0,η)eMt∧τr∧τ− 2hMit∧τr∧τ.
8Convergence of continuous-time stochastic gradient descent
Proof We apply the multi-dimensional Itˆo formula (see (Mao, 2007, Theorem 6.4 page
36)) to the function logf(w ). We obtain
t∧τr∧τ
t∧τr∧τ 1
logf(w ) = logf(w ) (a(w ) ηg(w ))ds+M M
t∧τr∧τ 0
−
s
−
s t∧τr∧τ
− 2h
it∧τr∧τ
Z0
1
logf(w ) (t τ τ)θ(r,w ,η)+M M .
≤
0
− ∧
r
∧
0 t∧τr∧τ
− 2h
it∧τr∧τ
Then, taking exponentials, the result follows.
The next lemma shows that if f has not attained its minimum in a finite time then f
decays exponentially to zero at infinity.
Lemma 7 Consider the sde (2) initialized at some w RD and suppose that Assumption
0
∈
1 holds. Then, for all r > 0 and η 0, almost surely, conditioned on the event τ τ = ,
r
≥ { ∧ ∞}
logf(w )
t
limsup θ(r,w ,η) .
0
t ≤ −
t→∞
Proof Assume that τ τ = . Observe that, in particular, T = . Consider the
r
∧ ∞ ∞
martingale defined in (9). It follows from (Mao, 2007, Theorem 1.7.4) that for any fixed
n > 0 and for all x > 0,
1
P sup M M > x e−x .
t t
− 2h i ≤
(t∈[0,n](cid:18)
(cid:19)
)
Since
logf(w ) logf(w ) 1 1
t 0
θ(r,w ,η)+ M M ,
0 t t
t ≤ t − t − 2h i
(cid:18) (cid:19)
the lemma follows from a standard Borel-Cantelli argument (see (Mao, 2007, Theorem
2.5.1)).
Now we are prepared to prove the main result of the paper.
Proof [Proof of Theorem 4] Let 0 u < t and set t¯:= t τ τ and u¯ := u τ τ. Let
r r
≤ ∧ ∧ ∧ ∧
ǫ > 0. Then, by Markov’s inequality,
E( w w )
P( kw t¯ −w u¯
k
> ǫ)
≤
k
t¯
ǫ−
u¯
k
t¯ ¯t
E f(w ) ds E σ(w )dB
u¯k∇ s k k u¯ s s k
+√η
≤ (cid:16) R ǫ (cid:17) (cid:16) R ǫ (cid:17)
E
t¯
f(w ) ds E
¯t
Tr(σT(w )σ(w ))ds
1/2
u¯k∇ s k u¯ s s
+√η ,
≤ (cid:16) R ǫ (cid:17) n (cid:16) R ǫ (cid:17)o
wherethe lastinequality follows fromCauchy-Schwarz inequality and(Mao,2007, Theorem
5.21 page 28).
9Gabor Lugosi and Eulalia Nualart
By the Cauchy-Schwarz inequality,
t¯ t¯ f(w ) 2 1/2 t¯ 1/2
s
E f(w ) ds E k∇ k ds E 2 f(w )ds . (11)
s s
Zu¯ k∇ k ! ≤ ( Zu¯ 2 f(w s) !) ( Zu¯ !)
p
Observe that, on the event u τp τ , we have that u¯ = t¯and then all the integrals
r
{ ≥ ∧ }
between u¯ and t¯are zero. Thus, it suffices to consider the event A := u < τ τ , and so
r
{ ∧ }
u¯ = u.
Using Lemma 6, we get that
t¯ t¯
1 1
E 2 f(w s)ds1
A
E 2 f(w 0)e−θ(r,w0,η)s/2e2Ms− 4hMisds1
A
≤
Zu¯ ! Zu !
p p
∞
1 1
2 f(w 0)e−θ(r,w0,η)s/2E e2Ms− 8hMis ds
≤
Zu
p (cid:16) (cid:17)
4 f(w )
= u e−θ(r,w0,η)u/2,
θ(r,w ,η)
p 0
where in the second inequality we used that M 0 for all s 0 and the equality follows
s
h i ≥ ≥
from (10) with c= 1.
2
In order to bound the first term on the right-hand side of (11), we apply the multi-
dimensional Itˆo lemma (Mao, 2007, Theorem 6.4 page 36) to f(w ). That is,
t¯
t¯ f(w ) 2 t¯ f(wp )T
s s
f(w t¯) = f(w u¯) k∇ k ds+√η ∇ σ(w s)dB s+Z t¯,
− 2 f(w ) 2 f(w )
Zu¯ s Zu¯ s
p p
where p p
η ¯t Hf(w ) f(w ) f(w )T
Z t¯:=
2
Zu¯
Tr σT(w s)
2
f(ws
s) −
∇ 4fs (w∇ s)3/2s !σ(w s) !ds .
p
Taking expectations, and noting that the stochastic integral term has zero mean, we get
that,
t¯ f(w ) 2
s
E k∇ k ds = E f(w u¯) E f(w t¯) +E(Z t¯). (12)
Zu¯ 2 f(w s) !
(cid:16)p
(cid:17)−
(cid:16)p (cid:17)
p
Then, by the definition of G (r,w ) and using a similar argument as above with Lemma
max 0
6 and (10), we obtain
t¯ f(w ) 2 η t¯Tr σT(w )Hf(w )σ(w )
E k∇ s k ds1 E f(w )1 + E s s s ds1
A u A A
Zu¯ 2 f(w s) ! ≤
(cid:16)p (cid:17)
2 Zu (cid:0) 2 f(w s) (cid:1) !
p ηG (r,w )
pt¯
E f(w )1 + max 0 E f(w )ds1
u A s A
≤ 2
(cid:16)p (cid:17)
Zu
p
!
ηG (r,w )
f(w )e−θ(r,w0,η)u/2 + max 0 f(w )e−θ(r,w0,η)u/2.
0 0
≤ θ(r,w ,η)
0
p p
10Convergence of continuous-time stochastic gradient descent
Thus, we have proved that
t¯
2 f(w ) ηG (r,w )
E f(w ) ds 0 e−θ(r,w0,η)u/2 1+ max 0 .
s
Zu¯ k∇ k ! ≤ θ p(r,w 0,η) p θ(r,w 0,η) !
Moreover, by the definition of Bp (r,w ) and appealing to Lemmpa 6, we get that
max 0
t¯ t¯
E Tr(σT(w )σ(w ))ds1 B (r,w )E 4f(w )ds1
s s A max 0 s A
≤
Zu¯ ! Zu !
∞
1
B max(r,w 0)E 4f(w 0)e−θ(r,w0,η)seMs− 2hMisds
≤
(cid:18)Zu (cid:19)
4f(w )
= B (r,w ) 0 e−θ(r,w0,η)u,
max 0
θ(r,w ,η)
0
where the last equality follows again from (10).
All the above bounds together show that for all 0 u < t and ǫ > 0,
≤
P( w w > ǫ)
k
t∧τr∧τ
−
u∧τr∧τ
k
2 f(w ) G (r,w ) (13)
0 e−θ(r,w0,η)u/2 1+√η max 0 + B (r,w ) ,
max 0
≤ ǫ pθ(r,w 0,η) p θ(r,w 0,η) !!
p
where wepobserve that the right hand side is inpdependent of t, τ , and τ.
r
We are now ready to prove the two statements of the Theorem. We start proving (6).
Taking u= 0, t τ τ, and ǫ = r in (13), we get that
r
↑ ∧
2 f(w ) G (r,w )
0 max 0
P(τ τ < ) 1+√η + B (r,w ) := p < 1.
r max 0
∧ ∞ ≤ r pθ(r,w 0,η) p θ(r,w 0,η) !!
p
This implies that p p
P(τ τ = ) 1 p > 0 ,
r
∧ ∞ ≥ −
proving (6).
We next prove the second statement of the theorem. Using (13) and condition (5), we
obtain that for all 0 u < t and ǫ > 0,
≤
r
P( w w > ǫ) e−θ(r,w0,η)u/2. (14)
k
t∧τr∧τ
−
u∧τr∧τ
k ≤ ǫ
Assume that τ τ = . Then (14) shows that w is a Cauchy sequence in probability.
r t
∧ ∞
Therefore, by (Borovkov, 1999, Theorem 3, Chapter 6), the sequence w converges in prob-
t
ability to some x∗ B (w ) as t . Moreover, taking t in (14), we obtain (7).
r 0
∈ → ∞ ↑ ∞
Since the rate of convergence is exponential, we conclude that the sequence w converges
t
to x∗ almost surely conditioned on the event τ τ = . Finally, by Lemma 7, we have
r
{ ∧ ∞}
x∗ S. This concludes the proof.
∈
Next we turn to the proof of Theorem 5. We start with some preliminary lemmas in
order to find bounds for the function a(w), b(w), and g(w) defined in Section 2. Recall the
setup and notation introduced in Section 4.
11Gabor Lugosi and Eulalia Nualart
Lemma 8 For all w = (W ,W ,...,W ) RD,
1 2 L
∈
f(w) 2
4 W
L
W
2
2λmin(Σ X) a(w) = k∇ k
k ··· k ≤ f(w)
(15)
L
4 W W 2 W W 2λ (Σ ) ,
L i+1 i−1 1 max X
≤ k ··· k k ··· k
i=1
X
where we define W W = Id for i = 1 and W W = Id for i = L.
i−1 1 L i+1
···
Proof First observe that
f(w) = 2E((β(w)X β∗X) (β(w)X)), (16)
∇ − ∇
where β(w)X is given by
∇
β(w)X = (W W )T(W W X)T,
∇Wi L
···
i+1 i−1
···
1
for i 1,...,L .
∈ { }
On the other hand,
f(w) = α(w)TE(XXT)α(w) = α(w)TΣ α(w) ,
X
where α(w) = β(w)T (β∗)T Rd.
− ∈
We start by proving the lower bound in (15). Lower boundingall the partial derivatives
with respect to the hidden layers by 0, we get that
E α(w)TX(W W )TXT 2
L 2
a(w) 4k ··· k
≥ α(w)TΣ α(w)
(cid:0) X (cid:1)
(W W )Tα(w)TΣ 2
L 2 X
= 4k ··· k
α(w)TΣ α(w)
X
Σ α(w) 2
= 4 W W 2 k X k
k L ··· 2 k α(w)TΣ α(w)
X
4 W W 2 inf ξTΣ ξ ,
L 2 X
≥ k ··· k ξ∈Rd,kξk=1
proving the desired lower bound.
We next show the upper bound for a(w). Similarly as above, we have that
L E α(w)TX(W W )T(W W X)T 2
L i+1 i−1 1
a(w) = 4 k ··· ··· k
α(w)TΣ α(w)
i=1 (cid:0) X (cid:1)
X
L (W W )Tα(w)TΣ (W W )T 2
L i+1 X i−1 1
= 4 k ··· ··· k
α(w)TΣ α(w)
X
i=1
X
L Σ α(w) 2
4 W W 2 W W 2 k X k
≤ k L ··· i+1 k k i−1 ··· 1 k α(w)TΣ α(w)
X
i=1
X
L
4 W W 2 W W 2 sup ξTΣ ξ,
L i+1 i−1 1 X
≤ i=1k ··· k k ··· k ξ∈Rd,kξk=1
X
12Convergence of continuous-time stochastic gradient descent
which concludes the proof of the upper bound in (15).
Lemma 9 For all w = (W ,W ,...,W ) RD,
1 2 L
∈
Tr(σT(w)σ(w)) L
b(w) = K2 W W 2 (W W 2.
L i+1 i−1 1
4f(w) ≤ k ··· k k ··· k
i=1
X
where recall that W W = Id for i= 1 and W W = Id for i= L.
i−1 1 L i+1
···
Proof We observe that
Tr(σT(w)σ(w)) = Tr(σ(w)σT(w))
= E ℓ(w,X) 2 f(w) 2
k∇ k −k∇ k
E ℓ(w,X) 2.
≤ k∇ k
Since ℓ(w,X) = 2(β(w)X β∗X) (β(w)X), we get that
∇ − ∇
Tr(σT(w)σ(w)) 4E (β(w)X β∗X)2 (β(w)X) 2 .
≤ − k∇ k
(cid:0) (cid:1)
In particular,
Tr(σT(w)σ(w))
b(w) =
4f(w)
E (β(w)X β∗X)2 (β(w)X) 2
− k∇ k .
≤ E (β(w)X β∗X)2
(cid:0) (cid:1)
−
(cid:0) (cid:1)
Finally, since
L
(β(w)X) 2 = (W W )T(W W X)T 2
L i+1 i−1 1
k∇ k k ··· ··· k
i=1
X
L
X 2 W W 2 W W 2
L i+1 i−1 1
≤ k k k ··· k k ··· k
i=1
X
L
K2 W W 2 W W 2,
L i+1 i−1 1
≤ k ··· k k ··· k
i=1
X
we conclude the proof.
Lemma 10 For all w = (W ,W ,...,W ) RD,
1 2 L
∈
Tr(σT(w)Hf(w)σ(w))
g(w) = 16K4(G2 +DF G β(w) β∗ ),
2f(w) ≤ L L L k − k
13Gabor Lugosi and Eulalia Nualart
where
L
F := W W W W W W
L j−1 1 L i+1 i−1 j+1
k ··· kk ··· kk ··· k
i=1(cid:18) j<i
X X
+ W W W W W W ,
j−1 i+1 i−1 1 L j+1
k ··· kk ··· kk ··· k
j>i (cid:19)
X
L
G := W W 2 W W 2,
L L i+1 i−1 1
k ··· k k ··· k
i=1
X
and where recall that W W = Id for i = 1 and W W = Id for i = L.
i−1 1 L i+1
···
Proof Observe that
Hf(w) = E(Hℓ(w,X))
= 2E (β(w)X)( (β(w)X))T +(β(w)X β∗X)H(β(w)X) .
∇ ∇ −
(cid:0) (cid:1)
Therefore,
Tr(σT(w)Hf(w)σ(w)) = 2(I +I ),
1 2
where
I = E(Tr(σT(w) (β(w)X)( (β(w)X))Tσ(w)))
1
∇ ∇
and
I = E(Tr(σT(w)(β(w)X β∗X)H(β(w)X)σ(w))).
2
−
We next bound I and I separately. We have that
1 2
I = E(Tr(σT(w) (β(w)X)(σT(w) (β(w)X))T))
1
∇ ∇
= E((σ(w)σT(w) (β(w)X))T (β(w)X))
∇ ∇
λ (σ(w)σT(w))E( (β(w)X) 2)
max
≤ k∇ k
L
K2λ (σ(w)σT(w)) W W 2 W W 2.
max L i+1 i−1 1
≤ k ··· k k ··· k
i=1
X
Similarly,
I = E((β(w)X β∗X)Tr(σT(w)H(β(w)X)σ(w)))
2
−
= E((β(w)X β∗X)Tr(H(β(w)X)σ(w)σT(w)))
−
DE(β(w)X β∗X)λ (H(β(w)X)σ(w)σT(w)))
max
≤ −
DE((β(w)X β∗X)λ (H(β(w)X)))λ (σ(w)σT(w)).
max max
≤ −
We next bound λ (H(β(w)X)). Since
max
(β(w)X) = W W XW W ,
∇Wi i−1
···
1 L
···
i+1
14Convergence of continuous-time stochastic gradient descent
we have that for i,j 1,...,L ,
∈ { }
0, if i= j,
∂2 (β(w)X) = (W W )T(W W XW W )T, if j < i,
WjWi  i−1
···
j+1 j−1
···
1 L
···
i+1
 (W W XW W )T(W W )T, if j > i.
i−1 1 L j+1 j−1 i+1
··· ··· ···

Therefore, 
λ max(H(β(w)X))) sup ξ∈RD,kξk=1 H(β(w)X)ξ
≤ k k
KF .
L
≤
We next bound λ (σ(w)σT(w)). We have that
max
λ max(σ(w)σT(w)) = sup ξ∈Rd,kξk=1kσ(w)σT(w)ξ
k
= sup ξ∈Rd,kξk=1kE(( ∇ℓ(w,Z) −∇f(w))( ∇ℓ(w,Z) −∇f(w))Tξ)
k
E( ℓ(w,Z) f(w) 2)
≤ k∇ −∇ k
2(E( ℓ(w,Z) 2)+ f(w) 2).
≤ k∇ k k∇ k
The computations in the proof of Lemma 9 showed that
E( ℓ(w,Z) 2) L
k∇ k 4K2 W W 2 W W 2.
L i+1 i−1 1
f(w) ≤ k ··· k k ··· k
i=1
X
Then, using the upper bound of Lemma 8, we get that
λ (σ(w)σT(w))
max 16K2G .
L
f(w) ≤
On the other hand,
E(β(w)X β∗X ) K β(w) β∗ .
| − | ≤ k − k
Thus, we conclude that
I +I
g(w) = 1 2 16K4(G2 +DF G β(w) β∗ ),
f(w) ≤ L L L k − k
which concludes the proof.
Proof [Proof of Theorem 5] First observe that the function f(w)and the matrix σ(w)
∇
are, respectively, given by (16) and by the unique square root of the covariance matrix of
ℓ(w,X) = 2(β(w)X β∗X) (β(w)X).
∇ − ∇
Therefore, they are locally Lipschitz since they are continuous, differentiable, with locally
bounded derivatives. Hence, Assumption 1 holds. Moreover, for any w∗ , f(w∗) = 0,
∈ S
thus, by the assumption that β∗ exists, f attains its minimum value and is the set of
S
minima of f, thus Assumption 2 holds.
15Gabor Lugosi and Eulalia Nualart
Consider the initial condition w = (W0,W0,...,W0) RD as in the statement of the
0 1 2 L ∈
theorem, that is, such that W0 = 0 and the entries of W0,...W0 are strictly positive. In
1 2 L
particular, β(w ) = 0 and
0
f(w )= E (β∗X)2 K2 β∗ 2.
0
≤ k k
Let γ > 0 be the minimum of all entries(cid:0) of W0,..(cid:1) .,W0 and let M be the maximum of all
2 L−1
entries of W0,...,W0. Let w = (W ,W ,...,W ) RD such that w w γ/2. Then
2 L 1 2 L ∈ k − 0 k ≤
the entries of W ,...,W are all bounded below by γ/2 and the entries of W ,...,W
2 L−1 2 L
bounded above by M′ = M + γ/2. Moreover the absolute value of each entry of W is
1
bounded above by γ/2. Let N > γ/2 be a lower bound on the entries of W0. Then the
L
entries of W are bounded below by N γ/2.
L
−
We next check that there exists η > 0 satisfying assumption (3). The lower bound of
Lemma 8 gives
A (γ/2,w )= inf a(w) 4λ (Σ )(N γ/2)2(γ/2)2L−4.
min 0 min X
w∈B γ/2(w0),f(w)6=0 ≥ −
On the other hand, Lemma 10 shows that G (γ/2,w ) is bounded by a constant that
max 0
only depends on M′, K, γ, D, L, and β∗ . Thus, we conclude that there exists η > 0
k k
satisfying assumption (3).
Choose η <
Amin(γ/2,w0)
. Moreover, since Lemma 9 shows that B (γ/2,w ) is
2Gmax(γ/2,w0) max 0
bounded by a constant that only depends on M′, K, γ, D, and L, we can choose η suffi-
ciently small such that
1
ηB (γ/2,w ) ,
max 0
≤ 4
which is just condition (4).
Then, the left hand side term in the inequality of assumption (5) is bounded above by
2K β∗ 1 8K β∗
k k (1+1+ ) k k .
γ/2 A/2 2 ≤ γ(N γ/2)(γ/2)L−2 λ (Σ )
min X
−
Therefore, taking Npsufficiently large, this implies that conditpion (5) holds true, and since
the left hand side of (5) can be made sufficiently small, applying Theorem 5 with r = γ/2,
we conclude the proof of the theorem.
Acknowledgments and Disclosure of Funding
Both authors acknowledge support from the Spanish MINECO grant PID2022-138268NB-
100 and Ayudas Fundacion BBVA a Proyectos de Investigacio´n Cient´ıfica 2021.
References
Jing An and Jianfeng Lu. Convergence of stochastic gradient descent under a local La-
jasiewicz condition for deep neural networks. arXiv preprint arXiv:2304.09221, 2023.
16Convergence of continuous-time stochastic gradient descent
Bubacarr Bah, Holger Rauhut, Ulrich Terstiege, and Michael Westdickenberg. Learning
deep linear neural networks: Riemannian gradient flows and convergence to global mini-
mizers. Information and Inference: A Journal of the IMA, 11(1):307–353, 2022.
Aleksandr Alekseevich Borovkov. Probability theory. CRC Press, 1999.
SouravChatterjee. Convergenceofgradientdescentfordeepneuralnetworks. arXivpreprint
arXiv:2203.16462, 2022.
Xiang Cheng, Dong Yin, Peter Bartlett, and Michael Jordan. Stochastic gradient and
langevin processes. In International Conference on Machine Learning, pages 1810–1819.
PMLR, 2020.
Mufan Bill Li and Maxime Gazeau. Higher order generalization error for first order dis-
cretization of langevin diffusion. arXiv preprint arXiv:2102.06229, 2021.
Xuerong Mao. Stochastic differential equations and applications. Elsevier, 2007.
Xuerong Mao and Chenggui Yuan. Stochastic differential equations with Markovian switch-
ing. Imperial college press, 2006.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2013.
Gabin Maxime Nguegnang, Holger Rauhut, and Ulrich Terstiege. Convergence of gradient
descent for learning linear neural networks. arXiv preprint arXiv:2108.02040, 2021.
Adrien Schertzer and Loucas Pillaud-Vivien. Stochastic differential equations models for
least-squares stochastic gradient descent. arXiv preprint arXiv:2407.02322, 2024.
Ayush Sekhari, Satyen Kale, Jason D Lee, Chris De Sa, and Karthik Sridharan. From
gradient flow on population loss to learning with stochastic gradient descent. Advances
in Neural Information Processing Systems, 35:30963–30976, 2022.
17