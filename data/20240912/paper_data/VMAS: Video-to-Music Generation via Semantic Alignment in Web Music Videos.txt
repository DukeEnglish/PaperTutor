VMAS: Video-to-Music Generation via
Semantic Alignment in Web Music Videos
Yan-BoLin1* YuTian2 LinjieYang2 GedasBertasius1 HengWang2
1UNCChapelHill 2ByteDanceInc.
{yblin,gedas}@cs.unc.edu
{yutian.yt,linjie.yang, heng.wang}@bytedance.com
Abstract patterns of pre-produced music with the particular video
events. Nevertheless,awell-incorporatedmusicsoundtrack
We present a framework for learning to generate back- cansignificantlyenhancethevideoviewingexperienceand
ground music from video inputs. Unlike existing works make the video more engaging. Thus, to reduce the man-
that rely on symbolic musical annotations, which are lim- ual effort for content creators, the last several years have
ited in quantity and diversity, our method leverages large- witnessed an increased demand for automatic video back-
scalewebvideosaccompaniedbybackgroundmusic. This groundmusicgenerationmethods[14,24,43,65].
enables our model to learn to generate realistic and di- Most prior video-based music generation ap-
versemusic. Toaccomplishthisgoal,wedevelopagenera- proaches [14, 24, 65] use symbolic music annotations
tivevideo-musicTransformerwithanovelsemanticvideo- (e.g., MIDI), which store manually transcribed musical
music alignment scheme. Our model uses a joint autore- datainadigitalformat. However,relyingonsuchsymbolic
gressive and contrastive learning objective, which encour- annotations for video-to-music generation is typically
agesthegenerationofmusicalignedwithhigh-levelvideo suboptimal for several reasons. First, due to the limited
content. We also introduce a novel video-beat alignment expressivity of symbolic annotations, music generation
scheme to match the generated music beats with the low- methods trained on such data [14,24,65] cannot capture
level motions in the video. Lastly, to capture fine-grained subtle nuances in music soundtracks such as timbre
visualcuesinavideoneededforrealisticbackgroundmusic variations, articulation, and the expressiveness of live
generation,weintroduceanewtemporalvideoencoderar- performances. Moreover, the fidelity of the generated
chitecture,allowingustoefficientlyprocessvideosconsist- music is largely contingent upon the quality of a sound
ing of many densely sampled frames. We train our frame- synthesizer or a MIDI playback engine, which may not
workonournewlycuratedDISCO-MVdataset, consisting adequately reflect the full depth and complexity of the
of2.2Mvideo-musicsamples,whichisordersofmagnitude actual instruments. Lastly, the small scale and limited
larger than any prior datasets used for video music gen- genre diversity of MIDI annotations typically lead to poor
eration. Our method outperforms existing approaches on generalization.
theDISCO-MVandMusicCapsdatasetsaccordingtovar- To address these issues, we propose to use a large col-
iousmusicgenerationevaluationmetrics,includinghuman lection of freely accessible internet videos that can pro-
evaluation. Resultsareavailableathttps://genjib. vide rich supervision for learning an effective video-to-
github.io/project_page/VMAs/index.html musicgenerationmodelspanningmanymusicgenres. Sev-
eral recent methods [43,58,63] have explored using Web
1.Introduction
videos for music generation. However, these methods are
typically trained on small-scale datasets that span narrow
Enabled by modern technology, most people can create
domains (e.g., dancing videos). Instead, to leverage the
theirownvideosviaplatformslikeTikTok,Instagram,and
scale and diversity of Web music videos, we develop a
others. However, creating engaging videos often requires
video-music generation framework, dubbed VMAS. Our
matchingthevideowiththerightbackgroundmusic,which
approach leverages a generative video-music Transformer
is challenging and demands lots of time and expertise. In
augmented with a novel Video-Music Alignment Scheme
particular, adding background music to the video often in-
(VMAS). We train our model using joint autoregressive
volves a costly search for suitable music and aligning the
and contrastive learning objectives. The generative objec-
*WorkdoneduringaninternshipatByteDanceInc. tive focuses on realistic music generation, while the con-
4202
peS
11
]MM.sc[
1v05470.9042:viXraTable1. Comparisonofdifferentvideo-to-musicgenerationdatasets. OurnewlycuratedDISCO-MVdatasetisordersofmagnitude
largerthanthepreviousYouTube-8Msubset[1]andSymMV[65]datasets, commonlyusedformusic-to-videogeneration. Compared
to other music generation datasets, our DISCO-MV is filtered to exclude videos with little visual variance and remove the vocals for
improvedbackgroundmusicquality. SinceDISCO-MVisbuiltfromtheexistingpubliclyavailabledatasets,itwillbeavailableunderthe
sameCC-BY-4.0license.
Dataset SymMV YT-8MSubset DISCO-10M DISCO-MV
NumberofVideos 1.1K 110K 15.3M 2.2M
Avg. FrameSimilarityWithinVideo 0.64 0.66 0.91 0.61
VocalsRemoved? N/A1 ✘ ✘ ✔
License CCBY-NC-SA4.0 CC-BY-4.0 CC-BY-4.0 CC-BY-4.0
trastiveobjectiveencouragesalignmentbetweengenerated it difficult to align the generated music with fine-grained
music and high-level video content (e.g., the genre of a video content. Instead, our work addresses this issue by
video). We also introduce a novel video-beat alignment generatingmusicdirectlyfromthevideos.
scheme that guides the generated music to be in harmony
2.2.Video-to-MusicGeneration
withlow-levelvisualcues, suchasscenetransitionsordy-
namichumanmotions. Lastly,unlikeexistingvideo-music Many methods [8–10,16,23,34–36,45,61] have tack-
generationmethods[14,24,43,65]thatusevideoencoders led the general problem of video-to-audio generation in
optimized for coarse spatial recognition [7,15,37,57], we the recent years. Building on this work, recent video-
develop an efficient video encoder that can process videos to-music generation methods have been designed to pro-
withmanydenselysampledframestocapturefine-grained ducemusicalaccompanimentsforinstrumentperformance
temporalcuesneededforaccuratemusicgeneration. in the video [18,26,44], to generate music that matches
WetrainourmethodonournewlycuratedDISCO-MV human motion/dance [30,41,58,62–64], and also to syn-
dataset, which consists of 2.2M video-music videos span- thesize background music for silent videos [14, 24, 43,
ning 47,000 hours. Compared to prior video-based mu- 65]. Most of these prior video-to-music generation ap-
sic generation approaches, our method generates superior- proaches[14,24,65]aretrainedusingsymbolicmusicanno-
quality music on the MusicCaps [2] and DISCO-MV test tations(e.g.,MIDIandABC),whichisoftensuboptimalbe-
sets. We also conduct human evaluations and show that causeofthelimitedexpressivity,diversity,andscaleofsuch
human subjects consistently prefer our generated music to annotations. Another line of relevant work involves music
musicgeneratedbypreviousmethodsonbothmusicquality generationfromdancevideos[30,41,58,62–64]. However,
andmusic-videoalignment. aswithsymbolicmusicannotations,thesemethodsaretyp-
ically constrained by small and narrow training datasets.
2.RelatedWork The recent work [43] tries to address these issues by us-
ingInternetvideos[1]tolearntogeneratemusic. However,
2.1.Text-ConditionedAudioandMusicGeneration
thismethodalsousesasmallnumberofvideosfortraining,
Theemergenceofpowerfultextmodelshasledtoanum- leadingtopoorgeneralization.Instead,weproposeavideo-
ber of text-to-audio [5,17,22,31,32,51,55] and text-to- to-music generation framework, VMAS, that uses a novel
music generation models [2,11,28,40,47,48,66]. Sev- video-musicalignmentschemetomatchthegeneratedmu-
eral recent models [2, 5, 6, 11, 27] use a neural audio sicwiththehigh-levelandlow-levelvisualcues,leadingto
codec [13,59] first to convert a continuous audio signal vividandrealisticmusicgeneration.Furthermore,unlikeall
into discrete tokens, and then train their conditional music priorapproaches,ourproposedframeworkleveragesorders
generation model from textual prompts. Additionally, the ofmagnitudemorevideos,whichisshowntobebeneficial
recent diffusion-based audio and music generation meth- forbothgenerationqualityandvideo-musicalignment.
ods[19,22,31,32,55]convertrawaudiointoaspectrogram
via a Short-Time Fourier Transform (STFT), and then use 3.TheDISCO-MVDataset
diffusion networks to predict a spectrogram from textual
Priorvideo-to-musicgenerationmethods[14,24,43,65]
prompts. Despite the impressive progress of text-to-music
userelativelysmalltrainingdatasets,whichlimitstheirgen-
generationmodels,textpromptscanonlyprovidehigh-level
eralization. Instead, we believe that large-scale training
music characteristics such as genre and emotion, making
dataisthefirstrequirementofbuildingahighlyperformant
1SymMVisintheMIDIformatwithoutvocaltracks. video-to-musicgenerationframework. SincetheWebcon-Efficient Video Encoder
Temporal/Spatial Spatial Spatial Spatial Mean
Contrastive Loss
Down Down Down Down Pooling
Autoregressive Music Decoder Mean Pooling
Densely Sampled
Video Frames Codebook …
- 87 … 87 98 1212 1212 … 2012 Lookup
Weighted
Cross-Entropy Loss
Quantization - 87 … 520 98 77 1212 … 2012
Video-Beat
Ground Truth
Raw Music Alignment
Figure1. OurVideo-to-MusicGenerationFramework. Ourvideo-to-musicgenerationframeworkconsistsofthreemaincomponents:
1)anefficientvideoencoderforcapturingfine-grainedtemporalcuesfrommanydenselysampledvideoframes,2)anautoregressivemusic
decoderforgeneratingoutputaudiotokens,and3)anovelvideo-musicalignmentschemethatintegratesacontrastivetrainingobjective
andanovelvideo-beatalignmentscheme, ensuringthatthegeneratedmusicexhibitshigh-levelandlow-levelalignmentwiththevideo
content.
tains many freely available videos accompanied by music 81K video-music samples from YouTube-8M. Our newly
soundtracks,weaimtousesuchWeb-basedvideosasarich curated video-music generation dataset, DISCO-MV, con-
supervisorysignalfortrainingalarge-scalevideo-to-music tains 2.28M video-music samples spanning 47,000 hours.
generation model. Using large-scale Web-based video- Furthermore, we also manually selected 1,120 and 1,086
music data is also advantageous since the music sound- videoswithhigh-qualityvideo-musiccorrespondencetobe
tracks in Web videos are diverse and span various musical used for validation and testing. In Table 1, we compare
genres,enablinggeneralizationtovariousscenarios. variouscharacteristicsofpreviouslyusedvideo-musicgen-
To develop an expansive video-to-music generation eration datasets. Our DISCO-MV is 2000× larger than
dataset, we investigate using the recently introduced priorsymbolicmusicannotationdatasetssuchasSymMV.
DISCO-10M [29] dataset, which was created for music Furthermore, DISCO-MV has a much smaller average
research and contains 15M videos collected via Spotify’s frame similarity than YouTube-8M-Music and DISCO-
artist graph [42]. The videos in DISCO-10M were col- 10M, demonstrating that our dataset features videos with
lectedbymatchingthemetadataforartists’toptrackswith richvisualcontent.Lastly,wenotethatsinceDISCO-MVis
the corresponding YouTube videos. Upon our preliminary builtusingthepubliclyavailablevideo-musicdatasets(i.e.,
analysis, we found that many videos in DISCO-10M have YouTube-8M [1] and DISCO-10M [29]), both of which
poorvideo-musiccorrespondence, i.e., manyofthevideos arecoveredbytheCC-BY-4.0license,DISCO-MVwillbe
consistofasingleframereplicatedoverseveralminutesor availableunderthissamelicense.
hoursorhaveverylittlevisualvariation,whichmakesthem
unsuitable for learning a high-quality video-to-music gen- 4.TechnicalApproach
erationmodel. Toaddressthisissue,wedevelopedanauto-
maticframeworktoremovesuchunsuitablevideos. Specif- In this section, we present our generative video-music
ically,foreachvideo,wecomputedpairwiseframesimilar- framework. AsillustratedinFigure1, ourmethodiscom-
itieswithinavideoandonlykeptvideoswithanintra-video posed of three main components: 1) an efficient video en-
framesimilaritybelowacertainthreshold(i.e.,0.7). coder, 2) an autoregressive music decoder, and 3) a video-
musicalignmentschemethatintegratesacontrastivetrain-
We also found that most videos in DISCO-10M con-
ingobjectiveandanovelvideo-beatalignmentschemefor
tainmusicsoundtrackswithvocals,whichissuboptimalfor
high-level and low-level video-music alignment, respec-
training video-to-music generation models [2,21,28]. To
tively. Below,wedescribethedetailsofeachofthesecom-
obtaintrainingsoundtrackswithinstrumental soundsonly,
ponents.
we used Demucs [38] for vocal removal. Empirically, we
foundthatwecouldsuccessfullyremovevocalsoundsfrom
4.1.AudioandVideoInputs
approximately95%ofthemusicsoundtracks.
Ultimately,thisledto2.2Mfilteredvideosfromtheorig- Audio Inputs. Given a raw music waveform associ-
inal DISCO-10M dataset, which we also combined with ated with the video, we first compute audio representationTable2.OurEfficientVideoEncoderArchitecture.Wemodify chitecturetoallowittoefficientlyprocessvideosconsisting
the original Hiera architecture [39] to efficiently process videos of a much larger number of frames (i.e., 96) sampled at a
consisting of many frames sampled at a high FPS rate. Specifi-
highFPSrate(i.e.,9.6).
cally,weincreasethestridesofQ-Poolinglayerstoreducespatial
Specifically,asdepictedinTable2,wefirstusea3Dcon-
dimension in the early layers of the network while maintaining
volutionwitha[3×7×7]kernelanda[2×4×4]patchstride
thetemporaldimensionthroughoutthenetwork.Processingvideo
to obtain the visual tokens. Afterward, the original Hiera
frames at high temporal granularity allows our model to extract
modelperformsthreespatialdownsamplingoperations,de-
muchmorefine-grainedtemporalvideocuesthantheoriginalHi-
eramodelneededforeffectivemusicgeneration. notedasQ-Pooling,afterthefollowingtransformerblocks:
2nd,5th,and21st. Toimprovetheefficiencyofprocessing
ArchitectureDetails OutputSizesT×S2×D many frames sampled at high FPS, we increase the spatial
Stage Hiera[39] Ours Hiera[39] Ours
downsamplingrateofthefirstandsecondQ-Poolinglayers
VideoInput N/A 96×3×2242
3×72 from2to4and2to7,respectively.Thisleadstofeatureten-
3DConv stride2×42 48×562×96 48×562×96 sorswithasignificantlyreducedspatialresolutionof2×2
padding1×32 in the third stage and 1×1 in the final stage (assuming a
PoolingStrides
1×2×2 1×4×4 48×282×192 48×142×192 224 × 224 input size). We then directly obtain the final
at2nd
PoolingStrides 1×2×2 1×7×7 48×142×384 48×22×384 videorepresentationsX v ∈Rtv/2×d,wheret v anddrepre-
at5th sentsthenumberofvideoframesandachanneldimension,
PoolingStrides
1×2×2 1×2×2 48×72×768 48×12×768 respectively. These simple architecture modifications (see
at21st
PoolingStrides Table2)allowourvideoencodertomaintainhightemporal
1×1×1 1×1×1 48×72×768 48×12×768
at24th resolutiontocapturefine-grainedtemporalvideocues.
4.3.AutoregressiveMusicGeneration
X( a0) ∈ Rta×d,wherethesuperscript(0)indicatesthatthis Wenowdescribeourautoregressivemusicdecoderthat
featuretensorwillbeusedasaninputtothefirstlayerofour usesthevideofeaturesX andquantizedmusictokensX(0)
v a
music decoder, while t denotes the number of timesteps
a for music generation. Specifically, as our autoregressive
related to audio length. Following MusicGen [11], we musicdecoder,weadoptastandardtransformerarchitecture
useEnCodec[13],astate-of-the-artconvolutionalencoder- withLlayersofalternatingmulti-headandcausalattention
decoderframework,totransformcontinuousaudiostreams blocks. We feed the previously computed video features
into a series of discrete tokens. This procedure transforms X into every multi-head attention as contextual features
v
therawmusicwaveformintotheabove-describedquantized
forautoregressivemusicgeneration. Theoperationsinour
musictokensX( a0) associatedwithdiscretelabels,denoted decodercanbewrittenas:
by Y ∈ Rta×c with c discrete categories. Each category
representsaspecificrangeofaudiofrequencies. Thesedis- F(ℓ) =CA(X a(ℓ−1),X( aℓ−1),X( aℓ−1))+X a(ℓ−1),
(1)
creteaudiolabelsYwillbeusedasasupervisorysignalfor X(ℓ) =MHA(F(ℓ),X ,X )+F(ℓ),
a v v
ourvideo-to-musicgenerationmodel.
VideoInputs. WeconsidervideoclipsV ∈ Rtv×H×W×3, where CA(.) and MHA(.) are standard causal and multi-
each containing t
v
RGB frames with height H and width headattentionblocks,andF(ℓ) ∈ Rta×d isanintermediate
W.Wesampletheseframesuniformlyfromtheentireinput musicrepresentationcomputedfrommusictokensX(ℓ−1).
a
video. Thevideoinputsarealignedwiththecorresponding ThenewmusictokenrepresentationX(ℓ) atlayerℓisthen
a
audiosegments. computedusingamulti-headself-attentionthatusesthein-
termediate music representation F(ℓ) as query, and video
4.2.EfficientVideoEncoder
features X as keys and values. The final music output
v
GivenaninputvideoV,weaimtocomputeavideorep- Yˆ ∈ Rta×c is predicted via an MLP layer attached to the
resentation that could be used for video-to-music genera- lastdecoderlayer.
tion. Buildingonthedevelopmentsinthevideorecognition
4.4.SemanticVideo-MusicAlignment
domain, weuseHiera[39], whichdemonstratedstrongre-
sultsonvariousvideorecognitiontasks. Whiletheoriginal Toimprovethealignmentbetweengeneratedmusicand
Hiera model was designed to operate on sparsely sampled video,weincorporatethefollowingtwostrategiesintoour
video frames for coarse video recognition tasks, video-to- framework: (1)aglobalvideo-musiccontrastiveobjective,
music generation requires fine-grained temporal informa- whichguidesthemusictomatchthehigh-levelvideocues
tion for accurate video-music alignment in time, such as (e.g., genre, style, etc.), and (2) a fine-grained video-beat
matchingmusicbeatswiththeactivemotionsinthevideo. alignment scheme, designed to generate music beats that
To enable this capability, we modify the original Hiera ar- match the low-level video cues such as scene transitionsDynamic Scene Dynamic
Human Motion Transition Human Motion
Find Local
…
Max Flow Value
Time
Optical Flow
Video Beats
Merge
…
Time
Onset … Overlapping
Detection
Video-Music Beats
Densely Sampled Raw Music Time
Video Frames Music Beats
Figure2. Video-BeatAlignmentScheme. Ourproposedalignmentschemeallowsustodetectmomentsinthevideowheremusicbeats
alignwithlow-levelvisualcuessuchasdynamichumanmotionsorscenetransitions. WeuseOnsetDetection[3]andOpticalFlowto
identifysuchalignedvideo-beatmoments.Thisinformationisthenusedtosuperviseourvideo-musicgenerationmodelsuchthatitwould
producemusicalignedwithlow-leveldynamicvisualcontent.
or dynamic human motions. We describe each of these byexaminingthemagnitudeofopticalflow[49]computed
schemesbelow. forallconsecutivevideoframesinthevideo. Wethenspa-
Global Video-Music Contrastive Objective. To imple- tiallyaveragetheopticalflowmagnitudesacrossallpixels
ment our contrastive video-music matching objective, we tocomputetheopticalflowmagnitudeforeachframepair
usethevideorepresentationX vandcomputethemusicfea- and store these values in O ∈ Rta that is linearly interpo-
tures as M = YˆE, where Yˆ ∈ Rta×c is the output from latedtomatchthedimensionofthemusicbeatsP a. After-
our music decoder, and E ∈ Rc×d is the embedding ma- ward, to obtain “video beats” P
v
∈ Rta, we set P v[t] for
trixfromtheEnCodecmodel. Conceptually,thisoperation eachtimesteptto1whenO[t]isthemaximuminatempo-
allowsustomapthediscretepredictedaudiocategoriesYˆ ralwindowofO[t−δ :t+δ],otherwiseP [t]=0. Here,
v
intohigher-dimensionalmusicembeddingsformoreeffec- δ serves as a hyperparameter for controlling the size of a
tive contrastive matching. To obtain an aggregated video local temporal window, which determines the granularity
representationX¯ ∈ Rd,weapplytemporalmeanpooling ofactionsweaimtocapture. Conceptually,weaimtocap-
v
on the video features X . We also apply mean pooling on turethetimestepscorrespondingtolargeopticalflowvalues
v
themusicfeaturesMtogenerateanaggregatedmusicrep- since they likely depict the dynamic video moments (e.g.,
resentation M¯ ∈ Rd. Then, the video-music contrastive humanmotions,scenechanges,etc.). Finally,wecompute
objectiveiswrittenas: theoverlapbetweenthedetectedmusicandvideobeatsas:
L
c
=− B1 (cid:88)B log (cid:80)Bexp e( xg p( (X¯ gv (X( ¯i), (M i¯ ),( Mi ¯)) () j))), (2) P av[i]=(cid:40) α1 eif lsP ev .[i]=1and (cid:80)δ j=−δP a[i+j]>0
i=1 j=1 v
(3)
where g(x,y) is the standard cosine similarity function Intuitively,theformulationabovecomputestheoverlap-
andBisthebatchsize. pingvideo-musicbeatsbycheckingwhetherthevideobeats
Video-Beat Alignment Scheme. Compared to the high- P v[i]matchthemusicbeatsinawindowP a[i−δ :i+δ].
levelcontrastivevideo-musicmatchingobjectiveabove,our We also introduce a hyperparameter α to prevent dispro-
video-beat alignment scheme encourages low-level align- portionatelyde-emphasizingtimestepswithoutthedetected
ment between generated music and video dynamics (e.g., video-music beats during training. The detected video-
scenetransitions,rhythmichumanmotions,etc.). Ourpro- musicbeatsP av arethenincorporatedintoourautoregres-
posed video-beat alignment scheme (shown in Figure 2) sivegenerationobjective:
consistsofseveralsteps.First,wedetectthemusicbeatsvia
c
off-the-shelf onset detection algorithm [3]. Formally, we L =−(cid:88) P Y log(Yˆ ), (4)
denotethedetectedmusicbeatsasP
a
∈ Rta,whereP a[t] g av i i
i=1
is set to 1 if a music beat was detected at timestep t; oth-
erwise, itis setto0. Afterward, weidentify”video beats” where Y and Yˆ are the ground truth and predicted au-Table3.ComparisonwiththeState-of-the-Art.Weevaluateallmethods[11,14,24,43]onMusicCaps[2]andDISCO-MVusingFAD,
KL,andMusic-VideoAlignment(MVAlign)metrics. SinceV2Meow’s[43]codeandpretrainedmodelsareunavailable,weonlyreport
theirFADresultsonMusicCaps.OurVMASoutperformsallpriorapproachesinallevaluationmetricsonalldatasets.
MusicCapsTestSet DISCO-MV
Method FAD↓ KL↓ MVAlign↑ FAD↓ KL↓ MVAlign↑
SymbolicMusic
CMT[14] 16.2 1.42 0.18 3.70 1.82 0.34
Video2Music[24] 24.7 1.35 0.19 4.36 1.93 0.29
WaveformMusic
VidMusicGen[11] 6.91 1.26 0.17 2.93 1.60 0.25
Vid2MLDM[19] 8.99 1.15 0.20 3.21 1.41 0.32
V2Meow[43] 4.62 - - - - -
WaveformMusic
VMAS(Ours) 4.07 1.09 0.22 2.38 1.34 0.35
dio tokens, respectively. Intuitively, the detected video- • Music-VideoAlignment(MVAlign)[56]focuseson
music beats P are used as importance weights to guide assessingthesynchronizationbetweenmusicbeatsand
av
our model to generate music beats aligned with low-level visualdynamicsinthevideo.
visual content. Note that if all values in P are set to 1, • Human Evaluation asks the subjects to select the
av
theEq.4representsthestandard(i.e.,uniformlyweighted) video-musicsamplestheypreferbasedon1)theover-
autoregressive objective. Our overall training objective is all music generation quality and 2) the alignment
L = βL +L , where β is a balancing term between the between the generated music and its corresponding
c g
contrastiveandautoregressiveterms. video. Specifically, given a pair of video-music sam-
ples, where the video is the same but the music is
5.ExperimentalSetup
generatedbytwodifferentmethods, thehumanraters
5.1.DownstreamDatasets are asked to select their preferred video-music sam-
ple based on the following prompts: 1) Which music
We train our approach on the DISCO-MV training set videohashigheroverallqualitymusic? and2)Which
andevaluateontwovideo-musicdatasetstestsets: Music- musicvideohasbettersynchronizationbetweenmusic
Caps[2]andourDISCO-MV,whichwedescribebelow. and visual content? For each question, the subjects
• MusicCaps [2] evaluation set contains 2,858 ten- canchooseoneofthetwomethodsorthethirdoption
secondmusicvideosamplesfromYouTubepairedwith “Cannottell.”Eachsubjectconducts10evaluationsfor
captionswrittenbyprofessionalmusicians. agivenpairofmethods. Thevideosforeachtrialare
randomly selected. The methods are unknown to the
• DISCO-MV is our newly curated dataset designed for
raters. We compare our VMAS method to 4 compet-
video-to-musicgeneration.Wemanuallyselected1,120
ing approaches [11,14,19,24]. The performance is
videos for validation and 1,086 for testing, ensuring
reportedastheaveragehumanpreferencerateforeach
high-qualityvideo-musiccorrespondenceineachset.
method. We conduct our human study with 200 sub-
5.2.EvaluationMetrics jectsonAmazonMechanicalTurk.
Following[11,19,28,43],weuseavarietyofmetricsto 5.3.Baselines
evaluateourgeneratedmusicsamples:
• Fre´chetAudioDistance(FAD)[25]computesthedis- We evaluate our model against the following recent
tance between the distribution of the generated music video-musicgenerationbaselines.
andthereferencemusicinthepretrainedVGGish[20] • Controllable Music Transformer (CMT) [14].
featurespace. Givenavideo,thisapproachproducessymbolicmusic
• KL Divergence (KL) uses a music genre tagging outputsthatareusedformusicgeneration. Sincethis
model[53]pretrainedontheMillionSongdataset[4] methodreliesonsymbolicmusicannotationsfortrain-
tomeasurethedivergenceofoutputdistributionsw.r.t ing, it is not trainable on DISCO-MV. Thus, we use
thereferencemusic. the publicly available pretrained checkpoint to gener-100
Ours Ours Ours Ours
CMT Video2Music VidMusicGen Vid2MLDM
Cannot Tell Cannot Tell Cannot Tell Cannot Tell
75
50
0
Overall Align Overall Align Overall Align Overall Align
Figure3.HumanEvaluation.WeconducthumanevaluationtocompareourVMASmethodagainstseveralrecentvideo-musicgeneration
methods[11,14,24,43]. Wepresenttheresultsasaveragehumanpreferenceratingsfor1)theoverallmusicgenerationqualityand2)the
alignmentbetweengeneratedmusicandthecorrespondingvideocontent. Eachcomparisonisconductedbetweenapairofmethods. The
methodsareunknowntothehumanraters. Ourresultsindicatethathumansubjectsconsistentlyprefermusic-videosampleswithmusic
generatedbyourmethod.
atemusiconourtwoevaluationdatasets. Table 4. Comparison to V2Meow Using the Same Training
• Video2Music [24]. Like CMT, Video2Music lever- Data. ForafaircomparisonwithV2Meow,weincludethevari-
agessemantic,motion,andemotionalfeaturestopre-
antofourVMASmodeltrainedusingthesamedataasV2Meow.
BothmethodsareevaluatedonMusicCapsusingtheFADandKL
dictsymbolicmusicoutputs.Sinceitcannotbetrained
Div.computedviaLeafclassifer[60].
on raw music videos, we generated music by directly
applyingthepretrainedmodeltothetestingvideos.
• VidMusicGen[11]. Weextendthetext-to-musicgen- Method TrainingDataset #Videos FAD↓ KL.↓
erationmodelMusicGen[11]tovideobyusingapre- V2Meow[43] MV100K 100K 4.62 1.22
trainedmusicvideocaptioningmodel[52]toproduce VMAS(Ours) MV100K 100K 4.51 1.15
VMAS(Ours) DISCO-MV 2.2M 4.07 1.10
textual video descriptions, which are then fed to the
pretrainedMusicGenmodelformusicgeneration.
firms our earlier hypothesis that the approaches relying on
• Vid2MLDM [19]. We adopt the pretrained text-to-
symbolic annotations suffer from small amounts of train-
music diffusion model, Tango [19], for our video-to-
ing data and limited diversity in the annotations. Second,
music generation task by replacing its text encoder
we demonstrate that our autoregressive approach achieves
with our video encoder. We then train Vid2MLDM
better results than the diffusion-based Vid2MLDM [19] in
usingthesametrainingandtestingsplitsasourmodel.
all metrics on all datasets. Third, we show that our model
• V2Meow [43]. This method uses sparsely sampled
outperforms the state-of-the-art V2Meow on MusicCaps.
videoframesasinputsforvideo-to-musicgeneration.
Since V2Meow’s source code has not been publicly re-
ItwastrainedonasubsetofYouTube-8M[1]. Dueto
leased, we can only perform comparisons using the re-
the unavailability of source code or pretrained check-
sults reported in their paper. For a fair comparison with
points,wedirectlyreporttheresultsfromtheirpaper.
V2Meow, in Table 4, we also include the variant of our
VMASmodeltrainedusingthesamedataasV2Meow.Our
6.ResultsandAnalysis
results indicate that our VMAS surpasses V2Meow even
when both methods are trained on the same data. This
6.1.ComparisonwiththeState-of-the-Art
indicates the effectiveness of our architecture design and
InTable3,wepresenttheresultsofourVMASmethod also the importance of our video-music alignment training
on the MusicCaps and DISCO-MV datasets. Our method schemes. We also observe that by using our DISCO-MV
outperforms all prior approaches in all evaluation metrics dataset,wecanimproveourmodel’sperformancesubstan-
on all datasets, indicating the superiority of our proposed tially,thusdemonstratingtheimportanceofourlarge-scale
methodcomparedtotheexistingvideo-to-musicgeneration dataset.
approaches. We also make several other interesting obser- HumanEvaluation.InFigure3,wereporttheresultsof
vations. First, we note that the symbolic music generation ourhumanevaluation.Weusetwometrics:1)overallmusic
approaches,CMTandVideo2Music,obtaininferiorresults generationqualityand2)alignmentbetweengeneratedmu-
compared to other approaches (including ours). This con- sic and the corresponding video content. These results are
)%(
ecnereferPTable 5. Effectiveness of Video-Music Alignment Schemes. Table6.ComparisontoOtherVideoEncoders.Wecomparethe
Here, we study the impact of our two video-music alignment effectiveness of our proposed video encoder architecture to sev-
schemes: 1)video-musiccontrastiveobjectiveand2)video-beat eralexistingvideoencoders[37,39]forthevideo-to-musicgener-
alignment. Weobservethatusingbothalignmentschemesallows ationtask. WeevaluatetheresultsontheDISCO-MVdatasetus-
ustoobtainthebestperformanceinFAD,KL,andMVAlignment ingFAD,KL,andMVAlignmetrics. Basedontheseresults,we
metricsonDISCO-MV. observethatourproposedvideoencoderarchitectureisnotonly
more efficient but also leads to higher-quality music generation,
justifyingourmodeldesign.
Configuration FAD↓ KL↓ MVAlign↑
AutoregressiveBaseline 2.75 1.40 0.243
Video MV
+Video-MusicContrastive 2.40 1.34 0.251 #Frames FAD↓ KL↓ ↑ GFLOPS↓
Encoder Align
+Video-BeatAlignment 2.38 1.34 0.352
CLIP[37] 16 2.61 1.41 0.274 281.6
Hiera[39] 16 2.58 1.41 0.316 140.2
Ours 96 2.38 1.34 0.342 130.7
presented as human preference rates, quantifying the per-
centage that the subjects favor our method over a compet-
ing method. All comparisons are made between a pair of
sicCaps: 4.7, 4.4, 4.3, 4.1 and DISCO-MV: 3.2, 2.9, 2.7,
methods,e.g.,ourVMASvs. CMT,Video2Music,VidMu-
2.4), indicating the importance of large-scale training data
sicGen,andVid2MLDM.FromtheresultsinFigure3,we
formusicgeneration. MoreresultsareshowninAppendix.
observethathumansubjectsconsistentlypreferourmethod
Imapct of Data Cleaning. We next study the impor-
to all the other methods according to both evaluation met-
tanceofourdatacleaningprotocol, whichremovesvideos
rics. Specifically,onaverage,ourmethodispreferredover
from DISCO-10M [29] with high intra-frame similarity
70% of the time for overall music generation quality and
(i.e., videos with low visual variance). We train two vari-
67%forthemetricmeasuringthealignmentbetweengen-
ants of VMAS : 1) a variant trained on the full unfiltered
eratedmusicandthecorrespondingvideocontent.
DISCO-10Mdatasetvs. 2)avarianttrainedonourfiltered
6.2.AblationStudies DISCO-MVdataset. WereportthattrainingVMASonthe
full DISCO-10M only achieves an FAD of 4.52, a KL of
ImportanceofVideo-MusicAlignment.InTable5,we 2.11, and an MV-Align score of 0.18. In contrast, train-
evaluate theimportance ofour twovideo-music alignment ing VMAS on our filtered DISCO-MV leads to a substan-
schemes:1)video-musiccontrastiveobjectiveand2)video- tially improved performance across all metrics, achieving
beatalignmentscheme. Basedontheseresults,weobserve FADof2.38,aKLof1.34,andanMV-Alignscoreof0.35.
that the contrastive video-music matching objective pro- TheresultssuggestthatfilteringthenoisyvideosinDISCO-
ducesasignificantimprovementinFAD(from2.75to2.40) 10Misnecessaryforbettervideo-to-musicgenerationper-
and a significant improvement in KL (from 1.40 to 1.34) formance.
metrics. Furthermore, our proposed video-beat alignment
scheme significantly improves the music-video alignment
7.DiscussionandConclusions
metric (+0.109). By integrating these two complementary
video-music alignment schemes, our model achieves opti- In this paper, we presented a large-scale video-
malperformanceaccordingtoallthreemetrics. music generation framework that leverages our newly cu-
ComparisontoOtherVideoEncoders. InTable6,we rated DISCO-MV dataset, which is orders of magnitude
compareourefficientvideoencoderwithCLIP[37]andHi- larger than any existing video-music datasets. Our pro-
era[39]onDISCO-MV.Allcomparisonsaredonebyincor- posedframeworkleveragesanovelvideo-musicalignment
porating each encoder into our video-to-music generation scheme consisting of 1) contrastive video-music matching
pipeline. Despite using a larger number of video frames and 2) video-beat alignment objectives to align the gener-
(i.e.,96vs. 16),ourvideoencoderismorecomputationally ated music with the high-level (e.g., genre) and low-level
efficientthaneitherCLIPorHiera(i.e.,130.7vs. 140.2and (e.g., scene transitions) visual cues in the video. Further-
281.6 in GFLOPs) and it achieves better music generation more, to effectively learn fine-grained visual cues from
results. long, densely sampled video inputs, we designed a new
Impact of Training Data Size. We next study the per- video encoder, which we showed to be both more effec-
formanceofourmodelwhentrainingitwithdifferentfrac- tive and more efficient than the existing encoders for the
tionsofDISCO-MVdata(i.e.,10%,25%,50%,100%).We video-music generation task. In the future, we will ex-
reporttheFADresultsonMusicCapsandDISCO-MVtest tend our framework to other video-music modeling prob-
sets. On both datasets, we observe a consistent improve- lems such as video-music style transfer, video-music edit-
ment with an increasing amount of training data (i.e., Mu- ing, and video-music retrieval. We will also investigateMusic Video Evaluation
0:00/ 0:10 0:00/ 0:10
Music Video A Music Video B
Q1: Which music do you think has the better music quality?
A is better
Cannot tell
B is better
Q2: Which background music has better synchronization between music beats and visual dynamics?
A is better
Cannot tell
B is better
Figure4.HumanEvaluation.Humanratersareaskedtoselectthegeneratedmusicthatbestalignswithagivenvideoandthebestmusic
quality.Wereporttheaveragehumanpreferencerateforeachmethod.Notethatallsamplesarepresentinarandomorder.
ajointvideo-and-text-to-musicgenerationarchitecturethat a Residual Vector Quantization (RVQ) with four quantiz-
providesuserswithmorecontrolwhengeneratingmusic. ers,eachhavingacodebooksizeof2048. Asforthecode-
bookpattern,weadoptthedelayinterleavingpattern[11]to
Acknowledgments translate10secondsofaudiointo500autoregressivesteps
(audiotokens).
We thank Feng Cheng, Md Mohaiminul Islam, Ce
Efficient Video Encoder. Given an input video, we ex-
Zhang, and Yue Yang for their helpful discussions. This
0:00/ 0:10 trac0t:0f0r/a 0m:10es at a 9.6 FPS rate, resulting in a total of 96
workwassupportedbytheSonyFacultyInnovationAward,
video frames with a resolution of 224 × 224 for a 10-
Laboratory for Analytic Sciences via NC State University,
second clip. We utilize the pretrained Hiera-Base model
ONRAwardN00014-23-1-2356.
Music Video A Music Video B
[39], which consists of 24 layers and performs downsam-
A.AppendixOverview Q1: Which music do you think hasp thl ein bg etteth r mre ue sict i qm uae lits y?through pooling. These 96 video frames
areinitiallyprocessedbya3DCNNwitha[3×7×7]ker-
A is better
Ourappendixconsistsof: Cannot tnelell,a[2×4×4]stride,anda[1×3×3]paddingforvideo
1. ImplementationDetails. B is bettteorkenization. Subsequently, we adjust the original spatial
Q2: Which background music has better synchronization between music beats and visual dynamics?
downsamplingmethodinHiera, namelyQ-Pooling, which
2. HumanEvaluationDetails.
A is better
utilizesthesamesizeofpoolingkernelsandstrides.TheQ-
Cannot tell
3. MusicGenreAnalysisofDISCO-MV
B is bettPeroolingkernelsizesforthefirstandseconddownsampling
4. AdditionalQuantitativeResults. stagesareincreasedfrom2to4(atthe2nd layer)andfrom
2 to 7 (at the 5th layer), respectively. For the third down-
5. QualitativeResults.
sampling stage, the Q-Pooling kernel size is maintained at
6. ASupplementaryVideo.
2×2,implementedatthe21st layer. Atthe24th layer,the
finallayer,themodelonlyincreasesthechanneldimension
B.ImplementationDetails
throughthelinerprojection,yieldingvideorepresentations
ofdimension48×1×1×768.
Audio Tokenization Model and Patterns. To transform
a continuous 32 kHz audio into discrete audio tokens, we Autoregressive Audio Decoder. We adopt the au-
leverage the pretrained EnCodec [13] with a stride of 640, toregressive transformer models of pretrained MusicGen-
resulting in a frame rate of 50 Hz and an initial hidden medium[11]asourautoregressiveaudiodecoder. Thede-
feature size of 64. The embeddings are quantized using coderconsistsof48transformerlayersinafeaturedimen-
0:00/ 0:10 0:00/ 0:10
Music Video A Music Video B
Q1: Which music do you think has the better music quality?
A is better
Cannot tell
B is better
Q2: Which background music has better synchronization between music beats and visual dynamics?
A is better
Cannot tell
B is better
0:00/ 0:10 0:00/ 0:10
Music Video A Music Video B
Q1: Which music do you think has the better music quality?
A is better
Cannot tell
B is better
Q2: Which background music has better synchronization between music beats and visual dynamics?
A is better
Cannot tell
B is better
0:00/ 0:10 0:00/ 0:10
Music Video A Music Video B
Q1: Which music do you think has the better music quality?Music Genre Distribution
20.0
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0
Classical Country Pop Reggae Jazz Rock Hiphop Disco Metal Blues
Genre
Figure 5. Music Genre Distribution. We present the GTZAN genres [50] for the DISCO-MV dataset. Genres are assigned to each
soundtrackbasedonthemaximumcosinesimilaritybetweenitssoundembeddingandthecorrespondinggenre(text)embedding.
MusicCaps Test Set 0.35
4.5 DISCO-MV Test Set
0.30
4.0
0.25
3.5
0.20
3.0
MusicCaps Test Set
2.5 0.15 DISCO-MV Test Set
20 40 60 80 100
20 40 60 80 100
Percentage of DISCO-MV Training Set
Percentage of DISCO-MV Training Set
Figure6.ImpactoftheTrainingDataSize.Wetrainourmethod
Figure 7. Impact of the Training Data Size. We train VMAS
withincreasingsubsetsofDISCO-MVandthenreporttheMusic-
withincreasingsubsetsofDISCO-MVandthenevaluateonMus-
Video Alignment (MV Align) metric on MusicCaps (Red) and
icCaps(Red)andDISCO-MV(Blue)usingFADmetric(thelower
DISCO-MV(Blue).
thebetter).Ourresultsillustratethatthesizeofvideo-musictrain-
ingdatahasasignificantimpactonthegeneratedmusicquality.
TheseresultsjustifyourapproachofusingWebvideosforscaling
ourvideo-musictrainingdata.
mately four days using 32 NVIDIA GPUs across 4 nodes.
Eachnodeisequippedwith8GPUs,92CPUs,and1000G
sionof1536with24standardcausalandmulti-headatten-
ofmemory.WeutilizeD-Adaptation[12]toselecttheover-
tionblocks.
alllearningrateautomatically.Acosinelearningratesched-
Optimiazation. WetrainVMASon10-secondvideoclips ule with a warmup of 4000 steps is deployed alongside an
fromDISCO-MVusingtheAdamWoptimizer[33]witha exponentialmovingaveragewithadecayof0.99. Wesetα
batch size of 8 on each GPU. The training takes approxi- ineq(3)to0.05andβ inthemaindraftto0.25.
DAF
egatnecreP
ngilA
VMTable 7. Video-to-Music Retrieval Results. We conduct a comparison of our designed efficient video encoder with existing video
encoders[37,39]forvideo-to-musicretrieval.Ourefficientvideoencoderisgeneralizabletothevideo-to-musicretrievaltask.
Method VideoEncoder #Frames V2MR@1↑ V2MR@10↑
CLIP[37] 16 4.2 17.3
MVPt[46]
Hiera[39] 16 4.8 18.1
VMAS VMAS 96 6.3 26.7
C.HumanEvaluationDetails improvemusic-videobeatalignmentaswellasmusicqual-
itywithlargertrainingdatascales.
AsdepictedinFigure4,givenapairofvideo-musicsam-
ples with the same video but different music generated by Video-Music Retrieval Task. To evaluate our ap-
two methods, human raters are asked to choose their pre- proach’sgeneralizationcapability,inTable7,wealsocom-
ferredvideo-musicsamplebasedonthefollowingprompts: pare VMAS against MVPt [46] for the video-to-music re-
1)Whichmusicdoyouthinkhasthebettermusicquality? trievaltasksonDISCO-MV.Followingthepipelineandsize
and2) Whichbackgroundmusichasbettersynchronization evaluation set of video-to-music retrieval framework [46],
betweenmusicbeatsandvisualdynamics?”Foreachques- we randomly sample 2,000 videos in the DISCO-MV test
tion,thesubjectscanchooseoneofthetwomethodsorthe set and extracted music and video representations using
thirdoption”Cannottell.”Wecollectedapproximately200 our trained model on video-to-music generation task. We
subjectsinthehumanevaluationbypresentingresultsfrom usedthesefeaturerepresentationsforthevideo-to-musicre-
arandommethodagainstVMAS.Ineachsurvey,Eachcon- trieval task and measured performance using the standard
ducts 10 evaluations randomly selected among 50 evalua- Recall@1 and Recall@10 metrics. For a fair comparison,
tions. we implement MVPt [46] using CLIP [37] and Hiera [39]
asthevideoencodersandtrainitunderthesameconditions
D.MusicGenreAnalysisofDISCO-MV asVMAS. TheseresultsindicatethatVMASismoreaccu-
rateandgeneralizesbettercomparedtoMVPt(i.e.,26.7vs.
FollowingthesetupinDISCO-10M[29],weimplement 18.1and17.3R@10)onthevideo-to-musictask.
zero-shotmusicgenreclassificationforDISCO-MVbyuti-
lizing pretrained CLAP [54] embeddings extracted from
10-second music clips. Genre classification is conducted
throughgenre-specificprompts(”Thisaudioisa<genre> F.QualitativeResults
song”)andidentifyingthegenreviatop-1cosinesimilarity
inasharedlatentspaceforeachmusicclip. InFigure5,we In Figure 8, we visualize our generated music re-
report the GTZAN genre distribution [50] of our DISCO- sults as a 2D spectrogram. We also include the re-
MV dataset. Jazz and disco genres are predominant while sults of the following video-to-music generation meth-
ensuring a wide range of musical diversity. However, we ods: CMT[14],Video2Music[24],VidMusicGen[11]and
note that the blues genre has few samples in DISCO-MV Vid2MLDM [19]. All results are obtained using the same
due to the limited number of blues music in the original videoinputshownatthetopoftheFigure.
DISCO-10Mdataset.
Based on these results, we observe that symbolic mu-
E.AdditionalQuantitativeResults sicgenerationmethods(i.e.,CMTandVideo2Music)often
generatemusicwithuniformmusicbeatpatterns, whichis
Impact of Training Data Size. Similar to our analysis suboptimal as the music fails to match the temporal dy-
in the main draft, in Figure 6 and Figure 7, we visual- namics of the video content. Furthermore, the existing
ize our model’s performance on DISCO-MV as the train- waveform methods (i.e., VidMusicGen and Vid2MLDM)
ingdataincreasesintheFADandMusic-VideoAlignment struggle to generate music consistently synchronized with
(MV Align) metrics, respectively. We observe consistent dynamic low-level video events.In comparison, the music
improvement in both metrics when the DISCO-MV data beats generated by our model (highlighted in red boxes)
sizeincreases. DespitetheVid2MLDMmodelnotaccount- havehigherintensitywhenasurferinthevideoperformsa
ingforlow-levelmusic-videobeatsynchronization,itsper- dramaticturn. Thisdemonstratesthatourmodelgenerates
formance improves with more training data. These addi- music that reflects the pace and magnitude of the actions
tional results confirm that our large-scale DISCO-MV can occurringinthevideo.…
Video Input
VMAS
CMT
Video2Music
VidMusicGen
Vid2MLDM
Figure8.QualitativeVideo-to-MusicGenerationResults.Here,weillustratequalitativemusicgenerationresultsforagivensilentvideo
input.Thegeneratedmusicsampleisvisualizedasaspectrogram.WecomparetheresultsofourmodelwithCMT[14],Video2Music[24]),
VidMusicGen[11]andVid2MLDM[19]. Wenotethatmostpriorvideo-to-musicgenerationapproachesproducemusicbeatsofuniform
intensity. Incontrast, ourmodelgeneratesmusicbeatsthatalignwellwithdynamicvideocontent, i.e., significantmovementswhena
surferchangesdirectionduringasharpturninthisparticularexample.
References Tagliasacchi, et al. Audiolm: a language modeling
approachtoaudiogeneration. TASLP,2023. 2
[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee,
[6] Zala´n Borsos, Matt Sharifi, Damien Vincent, Eugene
PaulNatsev,GeorgeToderici,BalakrishnanVaradara-
Kharitonov,NeilZeghidour,andMarcoTagliasacchi.
jan,andSudheendraVijayanarasimhan. Youtube-8m:
Soundstorm:Efficientparallelaudiogeneration.arXiv
A large-scale video classification benchmark. arXiv
Preprint,2023. 2
Preprint,2016. 2,3,7
[7] Joao Carreira and Andrew Zisserman. Quo vadis,
[2] AndreaAgostinelli,TimoIDenk,Zala´nBorsos,Jesse
action recognition? a new model and the kinetics
Engel, Mauro Verzetti, Antoine Caillon, Qingqing
dataset. InCVPR,2017. 2
Huang,ArenJansen,AdamRoberts,MarcoTagliasac-
chi,etal.Musiclm:Generatingmusicfromtext.arXiv [8] Kan Chen, Chuanxi Zhang, Chen Fang, Zhaowen
Preprint,2023. 2,3,6 Wang, Trung Bui, and Ram Nevatia. Visually in-
dicated sound generation by perceptually optimized
[3] Juan Pablo Bello, Laurent Daudet, Samer Abdallah,
classification. InECCVW,2018. 2
ChrisDuxbury,MikeDavies,andMarkBSandler. A
tutorial on onset detection in music signals. TASLP, [9] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong
2005. 5 Xiao,DengHuang,andChuangGan. Generatingvi-
suallyalignedsoundfromvideos. TIP,2020. 2
[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian
Whitman,andPaulLamere. Themillionsongdataset. [10] ZiyangChen,DanielGeng,andAndrewOwens. Im-
InISMIR,2011. 6 agesthatsound: Composingimagesandsoundsona
singlecanvas. arXivPreprint,2024. 2
[5] Zala´nBorsos,Raphae¨lMarinier,DamienVincent,Eu-
gene Kharitonov, Olivier Pietquin, Matt Sharifi, Do- [11] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David
minikRoblek,OlivierTeboul,DavidGrangier,Marco Kant, Gabriel Synnaeve, Yossi Adi, and AlexandreDe´fossez. Simpleandcontrollablemusicgeneration. [24] Jaeyong Kang, Soujanya Poria, and Dorien Her-
InNeurIPS,2023. 2,4,6,7,9,11,12 remans. Video2music: Suitable music generation
[12] Aaron Defazio and Konstantin Mishchenko. fromvideosusinganaffectivemultimodaltransformer
Learning-rate-free learning by d-adaptation. In model. arXivPreprint,2023. 1,2,6,7,11,12
ICML,2023. 10 [25] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek,
[13] Alexandre De´fossez, Jade Copet, Gabriel Synnaeve, and Matthew Sharifi. Fr\’echet audio distance: A
and Yossi Adi. High fidelity neural audio compres- metric for evaluating music enhancement algorithms.
sion. TMLR,2023. 2,4,9 arXivPreprint,2018. 6
[26] ASophiaKoepke,OliviaWiles,YaelMoses,andAn-
[14] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang,
drew Zisserman. Sight to sound: An end-to-end ap-
LeyanZhu,ZexinHe,HongmingLiu,andShuicheng
proach for visual piano transcription. In ICASSP,
Yan. Video background music generation with con-
trollablemusictransformer. InACMMM,2021. 1,2, 2020. 2
6,7,11,12 [27] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel
Singer,AlexandreDe´fossez,JadeCopet,DeviParikh,
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander
Yaniv Taigman, and Yossi Adi. Audiogen: Textually
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
guidedaudiogeneration. InICLR,2023. 2
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. An [28] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin,
imageisworth16x16words: Transformersforimage Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo
recognitionatscale. InICLR,2021. 2 Ma,XuchenSong,etal. Efficientneuralmusicgener-
ation. InNeurIPS,2023. 2,3,6
[16] YuexiDu, ZiyangChen, Justin Salamon, Bryan Rus-
sell, and Andrew Owens. Conditional generation of [29] Luca Lanzendo¨rfer, Florian Gro¨tschla, Emil Funke,
audiofromvideoviafoleyanalogies. InCVPR,2023. and Roger Wattenhofer. Disco-10m: A large-scale
2 musicdataset. InNeurIPS,2023. 3,8,11
[17] Zach Evans, CJ Carr, Josiah Taylor, Scott H Hawley, [30] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-
and Jordi Pons. Fast timing-conditioned latent audio ChunWang,Yu-DingLu,Ming-HsuanYang,andJan
diffusion. arXivPreprint,2024. 2 Kautz. Dancingtomusic. InNeurIPS,2019. 2
[18] Chuang Gan, Deng Huang, Peihao Chen, Joshua B [31] HaoheLiu,ZehuaChen,YiYuan,XinhaoMei,Xubo
Tenenbaum, and Antonio Torralba. Foley music: Liu, Danilo Mandic, Wenwu Wang, and Mark D
Learning to generate music from videos. In ECCV, Plumbley. Audioldm: Text-to-audio generation with
2020. 2 latentdiffusionmodels. InICML,2023. 2
[19] Deepanway Ghosal, Navonil Majumder, Ambuj [32] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xin-
Mehrish, and Soujanya Poria. Text-to-audio genera- hao Mei, Qiuqiang Kong, Yuping Wang, Wenwu
tion using instruction guided latent diffusion model. Wang, Yuxuan Wang, and Mark D Plumbley. Audi-
InACMMM,2023. 2,6,7,11,12 oldm2: Learningholisticaudiogenerationwithself-
supervisedpretraining. arXivPreprint,2023. 2
[20] ShawnHershey,SourishChaudhuri,DanielPWEllis,
Jort F Gemmeke, Aren Jansen, R Channing Moore, [33] IlyaLoshchilovandFrankHutter. Decoupledweight
ManojPlakal,DevinPlatt,RifASaurous,BryanSey- decayregularization. arXivPreprint,2017. 10
bold, et al. Cnn architectures for large-scale audio [34] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang
classification. InICASSP,2017. 6 Zhao. Diff-foley: Synchronized video-to-audio syn-
[21] Qingqing Huang, Daniel S Park, Tao Wang, Timo I thesiswithlatentdiffusionmodels. InNeurIPS,2023.
Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, 2
Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. [35] XinhaoMei,VarunNagaraja,GaelLeLan,Zhaoheng
Noise2music:Text-conditionedmusicgenerationwith Ni, Ernie Chang, Yangyang Shi, and Vikas Chandra.
diffusionmodels. arXivPreprint,2023. 3 Foleygen: Visually-guided audio generation. arXiv
[22] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Preprint,2023. 2
Ren,LupingLiu,MingzeLi,ZhenhuiYe,JinglinLiu, [36] AndrewOwens,PhillipIsola,JoshMcDermott,Anto-
Xiang Yin, and Zhou Zhao. Make-an-audio: Text- nioTorralba,EdwardHAdelson,andWilliamTFree-
to-audio generation with prompt-enhanced diffusion man. Visuallyindicatedsounds. InCVPR,2016. 2
models. InICML,2023. 2 [37] AlecRadford,JongWookKim,ChrisHallacy,Aditya
[23] Vladimir Iashin and Esa Rahtu. Taming visually Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
guidedsoundgeneration. InBMVC,2021. 2 try, Amanda Askell, Pamela Mishkin, Jack Clark,et al. Learning transferable visual models from nat- [52] CaelenWang. Timesformer-gpt2videocaptioning. 7
urallanguagesupervision. InICML,2021. 2,8,11
[53] Minz Won, Keunwoo Choi, and Xavier Serra. Semi-
[38] Simon Rouard, Francisco Massa, and Alexandre supervised music tagging transformer. In ISMIR,
De´fossez. Hybridtransformersformusicsourcesep- 2021. 6
aration. InICASSP,2023. 3
[54] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui,
[39] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen TaylorBerg-Kirkpatrick,andShlomoDubnov. Large-
Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, scalecontrastivelanguage-audiopretrainingwithfea-
ArkabandhuChowdhury,OmidPoursaeed,JudyHoff- turefusionandkeyword-to-captionaugmentation. In
man, et al. Hiera: A hierarchical vision transformer ICASSP,2023. 11
withoutthebells-and-whistles. InICML,2023. 4, 8,
[55] DongchaoYang,JianweiYu,HelinWang,WenWang,
9,11
Chao Weng, Yuexian Zou, and Dong Yu. Diffsound:
[40] FlavioSchneider,OjasvKamal,ZhijingJin,andBern-
Discretediffusionmodelfortext-to-soundgeneration.
hard Scho¨lkopf. Mouˆsai: Text-to-music generation
TASLP,2023. 2
with long-context latent diffusion. arXiv Preprint,
[56] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan
2023. 2
Schwartz,andYossiAdi. Diverseandalignedaudio-
[41] Eli Shlizerman, Lucio Dery, Hayden Schoen, and Ira
to-video generation via text-to-video model adapta-
Kemelmacher-Shlizerman. Audio to body dynamics.
tion. InAAAI,2024. 6
InCVPR,2018. 2
[57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruom-
[42] Spotify. How”fansalsolike”works. 3
ing Pang, James Qin, Alexander Ku, Yuanzhong Xu,
[43] Kun Su, Judith Yue Li, Qingqing Huang, Dima Jason Baldridge, and Yonghui Wu. Vector-quantized
Kuzmin,JoonseokLee,ChrisDonahue,FeiSha,Aren imagemodelingwithimprovedvqgan.InICLR,2022.
Jansen, Yu Wang, Mauro Verzetti, et al. V2meow: 2
Meowing to the visual beat via music generation. In
[58] Jiashuo Yu, Yaohui Wang, Xinyuan Chen, Xiao Sun,
AAAI,2024. 1,2,6,7
andYuQiao.Long-termrhythmicvideosoundtracker.
[44] Kun Su, Xiulong Liu, and Eli Shlizerman. Audeo:
InICML,2023. 1,2
Audio generation for a silent performance video. In
[59] NeilZeghidour,AlejandroLuebs,AhmedOmran,Jan
NeurIPS,2020. 2
Skoglund,andMarcoTagliasacchi. Soundstream: An
[45] Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Tor-
end-to-endneuralaudiocodec. TASLP,2021. 2
ralba,andChuangGan.Physics-drivendiffusionmod-
elsforimpactsoundsynthesisfromvideos. InCVPR, [60] Neil Zeghidour, Olivier Teboul, Fe´lix de Chaumont
2023. 2 Quitry, and Marco Tagliasacchi. Leaf: A learnable
frontendforaudioclassification.arXivPreprint,2021.
[46] D´ıdacSur´ıs,CarlVondrick,BryanRussell,andJustin
7
Salamon.It’stimeforartisticcorrespondenceinmusic
andvideo. InCVPR,2022. 11 [61] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui,
andTamaraLBerg. Visualtosound: Generatingnat-
[47] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang
uralsoundforvideosinthewild. InCVPR,2018. 2
Liu, ChenguangZhu, andMohitBansal. Codi-2: In-
context,interleaved,andinteractiveany-to-anygener- [62] Ye Zhu, Kyle Olszewski, Yu Wu, Panos Achlioptas,
ation. arXivPreprint,2023. 2 MengleiChai,YanYan,andSergeyTulyakov. Quan-
tized gan for complex music generation from dance
[48] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael
videos. InECCV,2022. 2
Zeng, and Mohit Bansal. Any-to-any generation via
composablediffusion. InNeurIPS,2023. 2 [63] Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey
[49] ZacharyTeedandJiaDeng. Raft: Recurrentall-pairs Tulyakov,andYanYan. Discretecontrastivediffusion
fieldtransformsforopticalflow. InECCV,2020. 5 forcross-modalmusicandimagegeneration.InICLR,
2023. 1,2
[50] George Tzanetakis and Perry Cook. Musical genre
classificationofaudiosignals. TASLP,2002. 10,11 [64] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yan-
gangWang,MingShao,andSiyuXia. Music2dance:
[51] Apoorv Vyas, Bowen Shi, Matthew Le, Andros
Dancenetformusic-drivendancegeneration. TOMM,
Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang,
2022. 2
Xinyue Zhang, Robert Adkins, William Ngan, et al.
Audiobox: Unifiedaudiogenerationwithnaturallan- [65] Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao,
guageprompts. arXivPreprint,2023. 2 ChenxiBao,StanleyPeng,SonghaoHan,AixiZhang,FeiFang,andSiLiu. Videobackgroundmusicgener-
ation:Dataset,methodandevaluation.InICCV,2023.
1,2
[66] Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix
Kreuk,AlexandreDe´fossez,JadeCopet,GabrielSyn-
naeve, and Yossi Adi. Masked audio generation us-
ingasinglenon-autoregressivetransformer. InICLR,
2024. 2