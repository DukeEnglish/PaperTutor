Hi3D: Pursuing High-Resolution Image-to-3D Generation with
Video Diffusion Models
HaiboYang∗ YangChen YingweiPan
SchoolofComputerScience HiDream.aiInc. HiDream.aiInc.
FudanUniversity China China
China c1enyang@hidream.ai pandy@hidream.ai
yanghaibo.fdu@gmail.com
TingYao ZhinengChen† Chong-WahNgo
HiDream.aiInc. SchoolofComputerScience SingaporeManagementUniversity
China FudanUniversity Singapore
tiyao@hidream.ai China cwngo@smu.edu.sg
zhinchen@fudan.edu.cn
TaoMei
HiDream.aiInc.
China
tmei@hidream.ai
Input image Generated multi-view consistent and high-resolution sequential images Mesh
Figure1:WeproposeHi3D,thefirsthigh-resolution(1,024×1,024)image-to-3Dgenerationframework.Hi3Dfirstgenerates
multi-viewconsistentimagesfromtheinputimageandthenreconstructsahigh-fidelity3Dmeshfromthesegeneratedimages.
ABSTRACT diffusionbasedparadigmthatredefinesasingleimagetomulti-view
Despitehavingtremendousprogressinimage-to-3Dgeneration, imagesas3D-awaresequentialimagegeneration(i.e.,orbitalvideo
existingmethodsstillstruggletoproducemulti-viewconsistent generation).Thismethodologydelvesintotheunderlyingtemporal
imageswithhigh-resolutiontexturesindetail,especiallyinthe consistencyknowledgeinvideodiffusionmodelthatgeneralizes
paradigmof2Ddiffusionthatlacks3Dawareness.Inthiswork,we welltogeometryconsistencyacrossmultipleviewsin3Dgenera-
presentHigh-resolutionImage-to-3Dmodel(Hi3D),anewvideo tion.Technically,Hi3Dfirstempowersthepre-trainedvideodiffu-
sionmodelwith3D-awareprior(cameraposecondition),yielding
∗ThisworkwasperformedwhenHaiboYangwasvisitingHiDream.aiasaresearch
multi-viewimageswithlow-resolutiontexturedetails.A3D-aware
intern.
†CorrespondingAuthor. video-to-videorefinerislearnttofurtherscaleupthemulti-view
imageswithhigh-resolutiontexturedetails.Suchhigh-resolution
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor multi-viewimagesarefurtheraugmentedwithnovelviewsthrough
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
3DGaussianSplatting,whicharefinallyleveragedtoobtainhigh-
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe fidelitymeshesvia3Dreconstruction.Extensiveexperimentson
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or bothnovelviewsynthesisandsingleviewreconstructiondemon-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. stratethatourHi3Dmanagestoproducesuperiormulti-viewcon-
MM’24,October28-November1,2024,Melbourne,VIC,Australia sistencyimageswithhighly-detailedtextures.Sourcecodeanddata
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. areavailableathttps://github.com/yanghb22-fdu/Hi3D-Official.
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3681634
4202
peS
11
]VC.sc[
1v25470.9042:viXraMM’24,October28-November1,2024,Melbourne,VIC,Australia HaiboYangetal.
CCSCONCEPTS 2Ddiffusionmodelthatlacks3Dawareness,videodiffusionmodel
•Informationsystems→Multimediacontentcreation. istrainedwithalargevolumeofsequentialframeimages,andthe
learnttemporalconsistencyknowledgeamongframescanbenat-
KEYWORDS urallyinterpretedasonekindof3Dgeometryconsistencyacross
multi-viewimages,especiallyfororbitalvideos.Thismotivatesus
Image-to-3Dgeneration;Videodiffusionmodel;Highresolution
toexcavatesuch3Dpriorknowledgefromthepre-trainedvideodif-
ACMReferenceFormat: fusionmodeltoenhanceimage-to-3Dgeneration.Moreimportantly,
HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,Chong- suchvideodiffusionbasedparadigmenablesmorestablesequential
WahNgo,andTaoMei.2024.Hi3D:PursuingHigh-ResolutionImage-to-3D imagegenerationwithamplified3Dgeometryconsistency.Itin
GenerationwithVideoDiffusionModels.InProceedingsofthe32ndACM turnallowsflexiblescalingupofhigher-resolutionsequentialim-
InternationalConferenceonMultimedia(MM’24),October28-November agegeneration(e.g.,256×256→1,024×1,024),triggering3Dmesh
1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,11pages.
generationwithhigher-resolutiontexturedetails.
https://doi.org/10.1145/3664647.3681634
Byconsolidatingtheideaofframingimage-to-3Dinvideodiffu-
sionbasedparadigm,wenovellypresentHigh-resolutionImage-
1 INTRODUCTION
to-3Dmodel(Hi3D),tofacilitatethegenerationofmulti-viewcon-
Image-to-3Dgeneration,i.e.,thetaskofreconstructing3Dmesh sistentmesheswithhigh-resolutiondetailedtexturesintwo-stage
ofobjectwithcorrespondingtexturefromonlyasingle-viewim- manner.Specifically,inthefirststage,apre-trainedvideodiffusion
age, has been a fundamental problem in multimedia [7, 8, 37] modelisremouldedwithadditionalconditionofcamerapose,tar-
andcomputervision[41,68]fieldsfordecades.Intheearlystage, getingfortransformingsingle-viewimageintolow-resolution3D-
thetypicalsolutionistocapitalizeonregressionorretrievalap- awaresequentialimages(i.e.,orbitvideowith512×512resolution).
proaches[24,55]for3Dreconstruction,whichtendstobeconfined Inthesecondstage,thislow-resolutionorbitvideoisfurtherfedinto
toclose-worlddatawithcategory-specificpriors.Thisdirection 3D-awarevideo-to-videorefinerwithadditionaldepthcondition,
inevitablyfailstoscaleupinreal-worlddata.Recently,thesuccess leadingtohigh-resolutionorbitvideo(1,024×1,024)withhighlyde-
ofdiffusionmodels[18,19,69]hasledtowidespreaddominancefor tailedtexture.Consideringthattheobtainedhigh-resolutionorbit
open-worldimagecontentcreation[34,39,44,46,47,50].Inspired videocontainsafixednumberofmulti-viewimages,weaugment
bythis,modernimage-to-3Dstudiesturnthefocusonexploring themwithmorenovelviewsthrough3DGaussianSplatting.The
howtoexploit2Dpriorknowledgefromthepre-trained2Ddiffu- resultantdensehigh-resolutionsequentialimageseffectivelyease
sionmodelforimage-to-3Dgenerationinatwo-phasemanner,i.e., thefinal3Dreconstruction,yieldinghigh-quality3Dmeshes.
firstmulti-viewimagesgenerationandthen3Dreconstruction.One Themaincontributionofthisworkistheproposalofthetwo-
representativepracticeZero123[26]remouldsthetext-to-image stagevideodiffusionbasedparadigmthatfullyunleashesthepower
2Ddiffusionmodelforviewpoint-conditionedimagetranslation, ofinherent3Dpriorknowledgeinthepre-trainedvideodiffusion
whichexhibitspromisingzero-shotgeneralizationcapabilityfor modeltostrengthenimage-to-3Dgeneration.Thisalsoleadstothe
novelviewsynthesis.Nevertheless,suchindependentmodeling elegantviewsofhowvideodiffusionmodelshouldbedesigned
betweentheinputimageandeachnovel-viewimagemightresultin forfullyexploiting3Dgeometrypriors,andhowtoscaleupthe
severegeometryinconsistencyacrossmultipleviews.Toalleviate resolutionofmulti-viewimagesforhigh-resolutionimage-to-3D
thisissue,severalsubsequentworks[21,27,29,48,49,53]further generation.Extensiveexperimentsdemonstratethestate-of-the-art
upgradethe2Ddiffusionparadigmbysimultaneouslytriggering performancesofourHi3Donbothnovelviewsynthesisandsingle
imagetranslationbetweentheinputimageandmulti-viewimages. viewreconstructiontasks.
Despiteimprovingmulti-viewimagesgeneration,theseapproaches
in2Ddiffusionparadigmstillsufferfrommulti-viewinconsistency
2 RELATEDWORKS
issuesespeciallyforcomplexobjectgeometry.Theunderlyingratio-
naleisthatthepre-trained2Ddiffusionmodelisexclusivelytrained Image-to-3Dgeneration.Recently,withtheremarkableadvances
onindividual2Dimages,thereforelacking3Dawarenessandresult- intext-to-imagediffusionmodels[18,19],image-to-3Dgeneration
inginsub-optimalmulti-viewconsistency.Moreover,thegeometry hasalsogainedsignificantprogress.Theseworkscanbegenerally
inconsistencyamongtheoutputmulti-viewimageswillaffectthe categorizedintothreegroups.Thefirstgroupisoptimize-based
overallstabilityofsingle-to-multi-viewimagetranslationduring approaches[31,40,43,54,61].Motivatedbythepioneeringwork
training.Hence,existingImage-to-3Dtechniques[21,27,29]mostly DreamFusion[38]intext-to-3Dgeneration[4–6,62,63],thisdi-
reducetheimagesizetolowresolution(256×256).Suchwayprac- rectionfocusesonper-sceneoptimizationbyleveragingtheprior
ticallyincreasesbatchsizeandimprovestrainingstability,while knowledgeinthepre-trained2Ddiffusionmodelthroughscore
sacrificingthevisualqualityofoutputimages.Thisseverelyhin- distillationsampling.Whilethesemethodshaveshownpromising
derstheirapplicabilityinmanyreal-worldscenariosthatrequire results,theyoftenrequireextensiveoptimizationtime.Toovercome
high-fidelity3Dmeshwithhigher-resolutiontexturedetails,such thisissue,thesecondgroupexploresthedirecttrainingofimage
asVirtualRealityand3Dfilmproduction. conditional3Dgenerativemodels[3,10,22,28,35,65,66].Nonethe-
Inresponsetotheaboveissues,ourworkpavesanewwayto less,thelimitedavailabilityofdiverse3Ddatahashamperedthese
formulateimagetranslationacrossdifferentviewsas3D-aware models’abilitytogeneralize,withmanystudiesbeingvalidated
sequentialimagegeneration(i.e.,orbitalvideogeneration)bycapi- onlyonanarrowrangeofshapecategories.Thethirddirectionis
talizingonthepre-trainedvideodiffusionmodel.Differentfrom therecentlyemergingtwo-stageapproach[21,27,29,48,49,53],Hi3D:PursuingHigh-ResolutionImage-to-3DGenerationwithVideoDiffusionModels MM’24,October28-November1,2024,Melbourne,VIC,Australia
which first generates multi-view images, and then reconstructs VideoDiffusionispre-trainedonlarge-scalehigh-qualityvideo
thecorresponding3Dmodel.Thesemethodsachieveimpressive datasetsanddemonstratesimpressiveimage-to-videogeneration
resultsandhaveafastgenerationspeed.Ourworkalsofallsinto capacity.Inthiswork,weproposetoinherittheunderlyingtem-
thisgroup.However,unlikepreviousmethodsthatcapitalizeon poralconsistencyknowledgeinvideodiffusionmodeltoboostthe
the2Ddiffusionmodel,weremoldthevideodiffusionmodelfor multi-viewconsistencyforimage-to-3Dgeneration.
3D-awaremulti-viewimagegeneration.Thiscanfullyunleashthe 3D Gaussian Splatting. 3D Gaussian Splatting (3DGS) [23]
powerofinherent3Dpriorknowledgeinthepre-trainedvideo emergesasarecentgroundbreakingtechniquefornovelviewsyn-
diffusionmodeltostrengthenimage-to-3Dgeneration.Wenote thesis.Unlike3Dimplicitrepresentationmethods(e.g.,NeuralRa-
thatsomeconcurrentworks[9,17,56]alsousevideodiffusion dianceFields(NeRF)[32])thatrelyoncomputationallyintensive
modelsfor3Dgeneration.Thekeydifferenceisthatwecapitalize volumerenderingforimagegeneration,3DGSachievesreal-time
onvideodiffusionmodeltodeviseanovel3D-awarevideo-to-video renderingspeedsthroughasplattingapproach[64].Specifically,
refiner,whichnotonlyscalesuptheresolutionofthegenerated 3DGSrepresentsa3Dsceneasasetofscaled3DGaussianprim-
multi-viewimagesbutalsorefines3Ddetails&consistency. itives, and each scaled 3D Gaussian𝐺 𝑘 is parameterized by an
3DReconstruction.Therecentsuccessofneuralradiancefields opacity(scale)𝛼 𝑘 ∈ [0,1],view-dependent color𝑐 𝑘 ∈ R3,cen-
(NeRFs)[32]hasinspiredmanyfollow-upworks[15,33,57]to terposition 𝜇 𝑘 ∈ R3×1,covariancematrix(cid:205) 𝑘 ∈ R3×3.The3D
achieveimpressive3Dreconstruction.However,thesemethods Gaussianscanbequeriedasfollows:
t ay np dic ta hl el iy rn effiec ce as cs yita inte reo cv oe nr sa truh cu tn ind gre 3d Dim ma og de es lsf fo rr omtra si pn ain rsg ev mie uw lts i-, 𝐺 𝑘(𝒙)=𝑒−1 2(𝒙−𝜇𝑘)𝑇(cid:205) 𝑘−1(𝒙−𝜇𝑘). (2)
view images remains suboptimal. To address this issue, several 3DGScomputesthecolorofeachpixelviaalphablendingaccording
studieshaveendeavoredtominimizetherequisitenumberoftrain- totheprimitive’sdepthorder1,...,𝐾:
ingviews.Forinstance,DS-NeRF[13]introducedadditionaldepth
𝐾 𝑘−1
supervisiontoenhancerenderingquality,whileRegNeRF[36]de- 𝐶(𝒙)=∑︁ 𝑐 𝑘𝜎 𝑘(cid:214) (1−𝜎 𝑗),𝜎 𝑘 =𝛼 𝑘𝐺 𝑘(𝒙). (3)
velopedadepthsmoothnesslossforgeometricregularizationto 𝑘=1 𝑗=1
facilitatetrainingstability.Sparseneus[30]focusedonlearning
Sincetherenderingprocessin3DGSisfastanddifferentiable,the
geometryencodingpriorsfromimagefeaturesforadaptableneu-
parametersof3DGaussiancanbeefficientlyoptimizedthrougha
ralsurfacelearningfromsparseinputviews,thoughthedetailin
multi-viewloss(see[23]formoredetails).Inthispaper,weintegrate
reconstructionresultswasstilllacking.Inthiswork,wedevelopa
3DGSintoour3Dreconstructionpipelinetoextracthigh-fidelity
straightforwardyetefficientreconstructionpipelinethatleverages
meshes,tailoredforsynthesizedhigh-resolutionmulti-viewimages.
thestate-of-the-art3DGaussianSplattingalgorithm[23]toaug-
mentthegeneratedmulti-viewimages,whichenablesustostably
4 OURAPPROACH
andeffectivelyreconstructhigh-qualitymeshes.
Inthiswork,wedeviseanewHigh-resolutionimage-to-3Dgen-
3 PRELIMINARIES erationarchitecture,namelyHi3D,tonovellyintegratevideodif-
fusion models into 3D-aware 360◦ sequential image generation
Video Diffusion Models. Diffusion models [18, 51] are gener-
(i.e.,orbitalvideogeneration).Ourlaunchingpointistoexploitthe
ative models that can learn the target data distribution from a
intrinsictemporalconsistentknowledgeinvideodiffusionmodels
Gaussiandistributionthroughagradualdenoisingprocess.Video
toenhancecross-viewconsistencyin3Dgeneration.Webeginthis
diffusionmodels[2,20,60]areusuallybuiltuponpre-trainedim-
sectionbyelaboratingtheproblemformulationofimage-to-3Dgen-
agediffusionmodels[34,46],andenablethedenoisingprocess
eration(Sec.4.1).Wethenelaboratethedetailsoftwo-stagevideo
overmultipleframessimultaneously.Forsimplicity,weadoptSta-
diffusionbasedparadigminourHi3Dframework.Specifically,in
bleVideoDiffusion[1]asthebasicvideodiffusionmodel,which
thefirststage,weremouldthepre-trainedimage-to-videodiffusion
achievesstate-of-the-artperformanceinimage-to-videogenera-
modelwithadditionalconditionofcameraposeandthenfine-tune
tion.Formally,givenasingleframe𝑥0,videodiffusionmodelcan
iton3Ddatatoenableorbitalvideogeneration(Sec.4.2).Inthe
generateahigh-fidelityvideoconsistingof𝑁 sequentialframes
x =
{𝑥0,𝑥1,...,𝑥(𝑁−1)}
through an iterative denoising process.
secondstage,wefurtherscaleupthemulti-viewimageresolution
througha3D-awarevideo-to-videorefiner(Sec.4.3).Finally,anovel
Specifically,ateachdenoisingstep𝑡,videodiffusionmodelpredicts
3Dreconstructionpipelineisintroducedtoextracthigh-quality3D
theamountofnoiseaddedinthesequencethroughaconditional
meshfromthesehigh-resolutionmulti-viewimages(Sec.4.4).The
3D-UNet Φ, and then denoises the sequence by subtracting the
wholearchitectureofHi3DisillustratedinFigure2.
predictednoise:
x𝑡−1=Φ(x𝑡;𝑡,𝑐), (1) 4.1 ProblemFormulation
where𝑐istheconditionembeddingoftheinputframe.Inpractice, GivenasingleRGBimageI∈R3×𝐻×𝑊 (sourceview)ofanobject
StableVideoDiffusionisbuiltwithinalatentdiffusionframework 𝑋,ourtargetistogenerateitscorresponding3Dcontent(i.e.,tex-
[46]toreducecomputationalcomplexity,i.e.,operatingdiffusion turedtrianglemesh).Similartopreviousimage-to-3Dgeneration
processinanencodedlatentspace.Inthisway,theinputvideo methods,wealsodecomposethischallengingtaskintotwosteps:
sequenceisfirstencodedintoalatentcodebyapre-trainedVAE 1)generateasequenceofmulti-viewimagesaroundtheobject𝑋
encoderandthedenoisedlatentcodeisdecodedbacktopixelspace and2)reconstructthe3Dcontentfromthesegeneratedmulti-view
usingaVAEdecoderafterthedenoisingsteps.NotethatStable images.Technically,wefirstsynthesizeasequenceofmulti-viewMM’24,October28-November1,2024,Melbourne,VIC,Australia HaiboYangetal.
Stage-1: Basic Multi-view Generation Stage-2: 3D-aware Multi-view Refinement
E
docn PILC Lo Vw
id
R eoes . Hi Vg ih
d
eR oes.
e
r
... ... ... ...
UNet ϵɵ¹ UNet ϵɸ²
z
t
Camera Elevation e
...
zz
tt
Mesh Dense High Res. Images 3D Gaussian Splatting
: Sinusoidal Embedding : Concat
3D Mesh Extraction
Figure2:AnoverviewofourproposedHi3D.OurHi3Dfullyexploitsthecapabilitiesoflarge-scalepre-trainedvideodiffusion
modelstoeffectivelytriggerhigh-resolutionimage-to-3Dgeneration.Specifically,inthefirststageofbasicmulti-viewgenera-
tion,Hi3Dremouldsvideodiffusionmodelwithadditionalcameraposecondition,aimingtotransformsingle-viewimage
intolow-resolution3D-awaresequentialimages.Next,inthesecondstageof3D-awaremulti-viewrefinement,wefeedthis
low-resolutionorbitvideointo3D-awarevideo-to-videorefinerwithadditionaldepthcondition,leadingtohigh-resolution
orbitvideowithhighlydetailedtexture.Finally,weaugmenttheresultantmulti-viewimageswithmorenovelviewsthrough
3DGaussianSplattingandemploySDF-basedreconstructiontoextracthigh-quality3Dmeshes.
images F ∈ R𝑁×3×𝐻×𝑊 of the object from 𝑁 different camera theimageresolutioninthesemethodsisrestrictedto256×256
poses𝝅
∈R𝑁×3×4correspondingtotheinputconditionimageI
to ensure training stability. Maintaining the original resolution
inatwo-stagemanner.Herein,wegenerate𝑁 = 16multi-view (512×512)inpre-trainedimagediffusionmodelswillleadtoslower
imageswithahighresolutionof𝐻×𝑊 =1,024×1,024aroundthe convergenceandhighervariance,aspointedinZero123[26].Con-
objectinthiswork.Itisworthnotingthatpreviousstate-of-the-art sequently,duetosuchlow-resolutionlimitation,thesemethodsfail
image-to-3Dmodels[21,27,29]canonlygeneratelow-resolution tofullycapturetheprimaryrich3Dgeometryandtexturedetailsin
(i.e.,256×256)multi-viewimages.Incontrast,tothebestofour theinput2Dimage.Inaddition,weobservethattheseapproaches
knowledge, our work is the first to enable high-resolution (i.e., stillsufferfrommulti-viewinconsistencyissue,especiallyforcom-
1,024×1,024)image-to-3Dgeneration,whichcanpreservericher plexobjectgeometry.Thismaybeattributedtothefactthatthe
geometryandtexturedetailsoftheinputimage.Next,weextract underlyingpre-trained2Ddiffusionmodelisexclusivelytrained
3Dmeshfromthesesynthesizedhigh-resolutionmulti-viewim- onindividual2Dimagesandlacks3Dmodelingofmulti-viewcor-
agesthroughourcarefullydesigned3Dreconstructionpipeline. relation.Toalleviatetheaboveissues,weredefinesingleimageto
Sincethenumberofgeneratedviewsissomewhatlimited,itis multi-viewimagesas3D-awaresequenceimagegeneration(i.e.,
difficulttoextractahigh-qualitymeshfromthesesparseviews.To orbitalvideogeneration)andutilizepre-trainedvideodiffusion
alleviatethisissue,weleveragethenovelviewsynthesismethod modelstofulfillthisgoal.Inparticular,werepurposeStableVideo
(3DGaussianSplatting[23])toreconstructanimplicit3Dmodel Diffusion(SVD)[1]togeneratemulti-viewimagesfromtheinput
frommulti-viewimagesF.Thenwerenderadditionalinterpolation image.SVDisappealingbecauseitwastrainedonalargevariety
viewsF∗ ∈R𝑀×3×𝐻×𝑊 betweenthemulti-viewimagesandadd ofvideos,whichallowsthenetworktoencountermultipleviews
theserenderedviewsintoF,therebyobtainingdenseviewimages ofanobjectduringtraining.Thispotentiallyalleviatesthe3Ddata
K ∈ R(𝑁+𝑀)×3×𝐻×𝑊 =F+F∗oftheobject𝑋.Finally,weadopt scarcityproblem.Moreover,SVDhasalreadyexplicitlymodeledthe
anSDF-basedreconstructionmethod[57]toextractahigh-quality multi-framerelationviatemporalattentionlayers.Wecaninherit
meshfromthesedenseviewsK. theintrinsicmulti-frameconsistentknowledgeinthesetemporal
layerstopursuemulti-viewconsistencyin3Dgeneration.
TrainingData.Wefirstconstructahigh-resolutionmulti-view
4.2 Stage-1:BasicMulti-viewGeneration
imagedatasetfromtheLVISsubsetoftheObjaverse[12].Foreach
Previousimage-to-3Dgenerationmethods[21,27,29,48]usually 3D asset, we render 16 views with 1,024×1,024 resolution at
relyonpre-trainedimagediffusionmodelstoaccomplishmulti- random elevation 𝑒 ∈ [−10◦,40◦]. It is important to note that
viewgeneration.Thesemethodsgenerallyextendthe2DUNetin whiletheelevationisrandomlyselected,itremainsthesameacross
imagediffusionmodelsto3DUNetbyinjectingmulti-viewcross- allviewswithinasinglevideo.Foreachvideo,thecamerasare
attention layers. These added attention layers are trained from positioned equidistantly from the object with distance𝑟 = 1.5
scratchon3Ddatasetstolearnmulti-viewconsistency.However,
andspacedevenlyfrom0◦to360◦inazimuthangle.Intotal,ourHi3D:PursuingHigh-ResolutionImage-to-3DGenerationwithVideoDiffusionModels MM’24,October28-November1,2024,Melbourne,VIC,Australia
trainingdatasetcomprisesapproximately300,000videos,denoted refiner,leadingtohigher-resolution(i.e.,1,024×1,024)multi-view
asJ ={(J𝑖,I𝑖,𝑒 𝑖)},wheretheinputconditionimageI𝑖 = [J𝑖] 1is imageswithfiner3Ddetailsandconsistency.
thefirstframeinsequentialimagesJ𝑖. Inthisstage,wealsoremouldthepre-trainedvideodiffusion
VideoDiffusionFine-tuning.Inthefirststage,ourgoalis modelas3D-awarevideo-to-videorefiner.Formally,suchdenois-
to repurpose the pre-trained image-to-video diffusion model to ingneuralnetworkcanbeformulatedas𝜖 𝜙2(z𝑡;I,Jˆ,D,𝑡,𝑒),whereJˆ
generatemulti-viewconsistentsequentialimages.Theaforemen- denotesthegeneratedmulti-viewimagescorrespondingtheinput
tionedmulti-viewimagedatasetJ ={(J𝑖,I𝑖,𝑒 𝑖)}isthusleveraged imageIinStage-1,Distheestimateddepthsequenceofthegen-
tofine-tunethe3D-awarevideodiffusionmodelwithadditional eratedmulti-viewimagesJˆ.Tobeclear,theinputconditionsIand
cameraposecondition.Specifically,giventheinputsingle-view 𝑒areinjectedintopre-trainedvideodiffusionmodelbythesame
imageIi,wefirstprojectitintolatentspacebytheVAEencoderof wayasinStage-1.Besides,weadopttheVAEencodertoextract
videodiffusionmodel,andchannel-wiselyconcatenateitwiththe thelatentcodesequenceofthepre-generatedmulti-viewimages
noisylatentsequence,whichencouragessynthesizedmulti-view Jˆandchannel-wiselyconcatenatethemwiththenoiselatentz𝑡
imagestopreservetheidentityandintricatedetailsoftheinput asconditions.Moreover,tofullyexploittheunderlyinggeometry
image.Inaddition,weincorporatetheinputconditionimage’sCLIP informationofthegeneratedmulti-viewimages,weleveragean
embeddings[42]intothediffusionUNetthroughcross-attention off-the-shelfdepthestimationmodel[45]toestimatethedepthof
mechanism.Withineachtransformerblock,theCLIPembedding eachimageinJˆas3Dcues,yieldingadepthmapsequenceD.We
matrixactsasthekeyandvalueforthecross-attentionlayers,cou- thendirectlyresizethedepthmapsintothesameresolutionof
pledwiththelayer’sfeaturesservingasthequery.Inthisway,the thelatentcodez𝑡,andchannel-wiselyconcatenatethemwithz𝑡.
high-levelsemanticinformationoftheinputimageispropagated Finally,theremouldeddenoisingneuralnetworkistrainedthrough
intothevideodiffusionmodel.Sincethemulti-viewimagesequence standardMSElossindiffusionmodels:
isrenderedatrandomelevations,wesendtheelevationparameter
(cid:104) (cid:105)
intothevideodiffusionmodelasadditionalcondition.Mostspecifi- L𝑆𝑡𝑎𝑔𝑒−2=E I,J,Jˆ,D,𝑒,𝑡,𝜖 ||𝑤(𝑡)(𝜖 𝜙2(z𝑡;I,Jˆ,D,𝑒,𝑡)−𝜖)||2 2 , (5)
cally,thecameraelevationangle𝑒isfirstembeddedintosinusoidal
where𝑤(𝑡)isaweighingfactor.Notethattheresolutionoftraining
positionalembeddingsandthenfedintotheUNetalongwiththe
diffusionnoisetimestep𝑡.Asallmulti-viewsequencesfollowthe
imagesinEq.(5)isscaledupto1,024×1,024.
Duringtraining,weadoptsomeimagedegradationmethods[58]
sameazimuthtrajectory,wedonotsendtheazimuthparameter
tosynthesizeJˆfordataaugmentation,insteadofsolelyusingthe
intothediffusionmodel.Herein,weomittheoriginal“fpsid”and
generatedcoarsemulti-viewimagesfromStage-1.Inparticular,we
“motionbucketid”conditionsinvideodiffusionmodelasthese
utilizeahigh-orderdegradationmodeltosynthesizetrainingdata,
conditionsareirrelevanttomulti-viewimagegeneration.
includingaseriesofblur,resize,noise,andcompressionprocesses.
Ingeneral,thedenoisingneuralnetwork(3DUNet)inourre-
moldedvideodiffusionmodelcanberepresentedas𝜖 𝜃1(z𝑡;I,𝑡,𝑒). T sho ar re ppl ti rc aa nt se ito iv oe nr ssh inoo imta ar gti ef sa )c ,t ws( ee. ug t., ilr izin eg 𝑠i 𝑖n 𝑛g 𝑐o fir ltg eh r.os Ati dn dg ita ior nou aln lyd
,
Giventhemulti-viewimagesequenceJ,thepre-trainedVAEen-
coderE(·)firstextractsthelatentcodeofeachimagetoconstitute randommaskingtechniquesareusedtosimulatetheeffectofshape
alatentcodesequencez.Next,Gaussiannoise𝜖 ∼𝑁(0,𝐼)isadded deformation.Thiswaynotonlyacceleratesthetrainingprocess,
butalsoenhancestherobustnessofourvideo-to-videorefiner.
tozthroughatypicalforwarddiffusionprocedureateachtimestep
𝑡 togetthenoiselatentcodez𝑡.The3DUNet𝜖 𝜃1(z𝑡;I,𝑡,𝑒) with
4.4 3DMeshExtraction
parameter𝜃 istrainedtoestimatetheaddednoise𝜖basedonthe
noisylatentcodez𝑡,inputimageconditionIandelevationangle𝑒 Throughtheabovetwo-stagevideodiffusionbasedparadigm,we
throughthestandardmeansquareerror(MSE)loss:
canobtainahigh-resolutionimagesequenceF∈R𝑁×3×𝐻×𝑊(𝑁
=
16,𝐻 =𝑊 =1,024)conditionedontheinputimageI.Inthissection,
L𝑆𝑡𝑎𝑔𝑒−1=E I,J,𝑒,𝑡,𝜖 (cid:2) ||𝑤(𝑡)(𝜖 𝜃1(z𝑡;I,𝑒,𝑡)−𝜖)||2 2(cid:3), (4) weaimtoextracthigh-qualitymeshesfromthesegeneratedhigh-
resolutionmulti-viewimages.Previousimage-to-3Dmethods[21,
where𝑤(𝑡)isacorrespondingweighingfactor.
27,29]usuallyreconstructthetarget3Dmeshfromtheoutput
Insteadofdirectlytrainingdenoisingneuralnetworkinhighres-
imagesequencebyoptimizingtheneuralimplicitSignedDistance
olution(i.e.,1,024×1,024),wedecomposethisnon-trivialproblem
Field(SDF)[16,57].Nevertheless,theseSDF-basedreconstruction
intomorestablesub-problemsinacoarse-to-finemanner.Inthe
methodsareoriginallytailoredfordenseimagesequencescaptured
firststage,wetrainthedenoisingneuralnetworkbyusingEq.(4)
intherealworld,whichcommonlyfailtoreconstructhigh-quality
with512×512resolutionforlow-resolutionmulti-viewimagegen-
meshbasedononlysparseviews.
eration.Thesecondstagefurthertransforms512×512multi-view
Toalleviatethisissue,wedesignaunique3Dreconstruction
imagesintohigh-resolution(1,024×1,024)multi-viewimages.
pipelineforhigh-resolutionsparseviews.Insteadofdirectlyadopt-
ingSDF-basedreconstructionmethodstoextract3Dmesh,wefirst
4.3 Stage-2:3D-awareMulti-viewRefinement
usethe3DGaussianSplatting(3DGS)algorithm[23]tolearnanim-
Theoutput512×512multi-viewimagesofStage-1exhibitpromis- plicit3Dmodelfromthegeneratedhigh-resolutionimagesequence.
ingmulti-viewconsistency,whilestillfailingtofullycapturethe 3DGShasdemonstratedremarkablenovelviewsynthesiscapabili-
geometryandtexturedetailsofinputs.Toaddressthisissue,we tiesandimpressiverenderingspeed.Hereinweattempttoutilize
includeanadditionalstagetofurtherscaleupthelow-resolution 3DGS’simplicitreconstructionabilitytoaugmenttheoutputsparse
outputsofthefirststagethroughanew3D-awarevideo-to-video multi-viewimagesofStage-2withmorenovelviews.Specifically,MM’24,October28-November1,2024,Melbourne,VIC,Australia HaiboYangetal.
Input image
Stable-Zero123
SyncDreamer
EpiDiff
Hi3D (Ours)
（a） （b） （c） （d）
Figure3:QualitativecomparisonswithStable-Zero123[52],SyncDreamer[27]andEpiDiff[21]onnovelviewsynthesistask.
OurHi3Dgenerateshigh-resolutionmulti-viewimageswithremarkableconsistentdetails.
Table1:Quantitativecomparisonwithstate-of-the-artmeth- Forthesingleviewreconstructiontask,weuseChamferDistances
odsinnovelviewsynthesisonGSOdataset. andVolumeIoUtomeasurethequalityofthereconstructed3D
models.Inaddition,toassessthegeneralizationabilityofourHi3D,
Method PSNR↑ SSIM↑ LPIPS↓
weperformqualitativeevaluationoversingleimageswithvarious
Realfusion[31] 15.26 0.722 0.283 stylesderivedfromtheinternet.
Zero123[26] 18.93 0.779 0.166 ImplementationDetails.Duringthefirststageofbasicmulti-
Zero123-XL[11] 19.47 0.783 0.159 view generation, we downscale the video dataset as 512×512
Stable-Zero123[52] 19.79 0.788 0.153 videos.Forthesecondstageofmulti-viewrefinement,wenotonly
SyncDreamer[27] 20.05 0.798 0.146 feedtheoutputsofthefirststage,butalsoadoptsyntheticdata
EpiDiff[21] 20.49 0.855 0.128 generationstrategy(similartotraditionalimage/videorestoration
Hi3D(Ours) 24.26 0.864 0.119 methods[58])fordataaugmentation.Thisstrategyaimstoacceler-
atethetrainingprocessandenhancethemodel’srobustness.The
werender𝑀interpolationviewsF∗betweentheadjacentimagesin
overallexperimentsareconductedoneight80GA100GPUs.Specifi-
Ffromthereconstructed3DGS.Finally,weoptimizeanSDF-based cally,thefirststageundergoes80,000trainingsteps(approximately
reconstructionmethod[57]basedontheaugmenteddenseviews 3days),withalearningrateof1×10−5andatotalbatchsizeof16.
F+F∗toextractthehigh-quality3Dmeshoftheobject𝑋.
Thesecondstagecontains20,000trainingsteps(around3days),
withalearningrateof5×10−5andareducedbatchsizeof8.
5 EXPERIMENTS
ComparedMethods.WecompareourHi3Dwiththefollow-
5.1 ExperimentalSettings ingstate-of-the-artmethods:RealFusion[31]andMagic123[40]
exploit2Ddiffusionmodel(StableDiffusion[46])andSDSloss[38]
DatasetsandEvaluation.Weempiricallyvalidatethemeritofour
for reconstructing from single-view image. Zero123 [26] learns
Hi3Dmodelbyconductingexperimentsontwoprimarytasks,i.e.,
togeneratenovelviewimagesofthesameobjectfromdifferent
novelviewsynthesisandsingleviewreconstruction.Following[21,
viewpoints,andcanbeintegratedwithSDSlossfor3Dreconstruc-
27,29],weperformquantitativeevaluationonGoogleScanned
tion. Zero123-XL [11] and Stable-Zero123 [52] further upgrade
Object(GSO)dataset[14].Fornovelviewsynthesistask,weemploy
Zero123byenhancingthetrainingdataquality.One-2-3-45[25]
threecommonlyadoptedmetrics:PSNR,SSIM[59],andLPIPS[67].Hi3D:PursuingHigh-ResolutionImage-to-3DGenerationwithVideoDiffusionModels MM’24,October28-November1,2024,Melbourne,VIC,Australia
(a) View 1
(b) View 2
(a) View 1
(b) View 2
Input image One-2-3-45 Shape-E Stable-Zero123 SyncDreamer EpiDiff Wonder3D Hi3D (Ours)
Figure4:Qualitativecomparisonof3Dmeshesgeneratedbyvariousmethodsonsingleviewreconstructiontask.
directlylearnsexplicit3Drepresentationvia3DSignedDistance Table2:Quantitativecomparisonwithstate-of-the-artmeth-
Functions(SDFs)[30]frommulti-viewimages(i.e.,theoutputs odsinsingleviewreconstructiononGSOdataset.
ofZero123).Point-E[35]andShap-E[22]arepre-trainedoveran
Method ChamferDist.↓ VolumeIoU↑
extensiveinternalOpenAI3Ddataset,therebybeingcapableof
directlytransformingsingle-viewimagesinto3Dpointcloudsor Realfusion[31] 0.0819 0.2741
shapesencodedinMLPs.SyncDreamer[27]introducesa3Dglobal Magic123[40] 0.0516 0.4528
featurevolumetomaintainmulti-viewconsistency.Wonder3D[29] One-2-3-45[25] 0.0629 0.4086
andEpiDiff[21]leverage3Dattentionmechanismstoenableinter- Point-E[35] 0.0426 0.2875
actionamongmulti-viewimagesviacross-attentionlayers.Note Shap-E[22] 0.0436 0.3584
thatinnovelviewsynthesistask,weonlyincludepartialbaselines Stable-Zero123[52] 0.0321 0.5207
(i.e.,Zero123series,SyncDreamer,EpiDiff)thatcanproduceexactly SyncDreamer[27] 0.0261 0.5421
thesameviewpointsasourHi3Dforfaircomparison. EpiDiff[21] 0.0343 0.4927
Wonder3D[29] 0.0199 0.6244
5.2 NovelViewSynthesis Hi3D(Ours) 0.0172 0.6631
Table1summarizesperformancecomparisononnovelviewsynthe-
sistask,andFigure3showcasesqualitativeresultsintwodifferent
(256×256).Instead,bymining3Dpriorsandscalingupmulti-view
views.Overall,ourHi3Dconsistentlyexhibitsbetterperformances
image resolution via video diffusion model, our Hi3D manages
than existing 2D diffusion based approaches. Specifically, Hi3D
toproducemulti-viewconsistentandhigh-resolution1,024×1,024
achievesthePSNRof24.26%,whichoutperformsthebestcompeti-
images,leadingtohighestimagequality(e.g.,theclearlyvisible
torEpiDiffby3.77%.ThehighestimagequalityscoreofourHi3D
numbersinalarmclockinFigure3(a)).
generallyhighlightsthekeyadvantageofvideodiffusionbased
paradigmthatexploits3Dpriorknowledgetoboostnovelview
5.3 SingleViewReconstruction
synthesis.Inparticular,duetotheindependentimagetranslation,
Zero123series(e.g.,Stable-Zero123)failstoachievemulti-viewcon- Next,weevaluatethesingleviewreconstructionperformanceofour
sistencyresults(e.g.,one/tworingsontheheadofthealarmclock Hi3DinTable2.Inaddition,Figure4showsqualitativecomparison
indifferentviewsinFigure3(a)).SyncDreamerandEpiDifffurther betweenHi3Dandexistingmethods.Ingeneral,ourHi3Doutper-
strengthenmulti-viewconsistencybyexploiting3Dintermediatein- formsstate-of-the-artmethodsoverbothtwometrics.Specifically,
formationorusingmulti-viewattentionmechanisms.Nevertheless, One-2-3-45directlyleveragesmulti-viewoutputsofZero123with
theirnovel-viewresultsstillsufferfromblurryandunrealisticis- sub-optimal3Dconsistencyforreconstruction,whichcommonly
sueswithdegradedimagequality(e.g.,theblurrynumbersofalarm resultsinover-smoothmesheswithfewerdetails.Stable-Zero123
clockinFigure3(a))duetotherestrictedlowimageresolution furtherimproves3Dconsistencywithhigher-qualitytrainingdata,MM’24,October28-November1,2024,Melbourne,VIC,Australia HaiboYangetal.
Table3:Ablationstudyon3D-awaremulti-viewrefinement.
A colorful bird
perched on a grey
Setting PSNR↑ SSIM↑ LPIPS↓ stone in a proud
stance
Hi3D 24.26 0.864 0.119
A toy rocket
w/orefinement 22.09 0.842 0.136 launching with
w/odepth 23.12 0.848 0.128 smoke clouds
billowing beneath
Input text Text to image Generated multi-view images Mesh
Table4:Ablationstudyon3Dreconstructionpipeline. Figure5:ExamplesofusingHi3Dfortext-to-3Dgeneration.
Setting ChamferDist.↓ VolumeIoU↑
𝑀 =0 0.0186 0.6375 Input image
𝑀 =16 0.0172 0.6631
𝑀 =32 0.0174 0.6598
𝑀 =48 0.0175 0.6607
Seed A
whilestillsufferingfrommissingorover-smoothmeshes.Differ-
entfromindependentimagetranslationinZero123,SyncDreamer, Seed B
EpiDiff, and Wonder3D exploit simultaneous multi-view image
translationthrough2Ddiffusionmodel,therebyleadingtobetter
3Dconsistency.However,theystruggletoreconstructcomplex3D
Seed C
mesheswithrichdetailsduetothelimitationoflow-resolution
multi-viewimages.Incontrast,ourHi3Dfullyunleashesthepower
Figure6:DiverseandcreativeresultsofourHi3Dwithdif-
ofinherent3Dpriorknowledgeinpre-trainedvideodiffusionmodel
ferentseeds.
andscalesupthemulti-viewimagesintohigherresolution.Such
designenableshigher-quality3Dmeshreconstructionwithricher
asillustratedinFigure5.Ourapproachmanagestoproducehigher-
fine-graineddetails(e.g.,thefeetofbirdandpenguininFigure4).
fidelity3Dmodelswithhighly-detailedtexture,whichagainhigh-
lightsthemeritofhigh-resolutionmulti-viewimagegeneration
5.4 AblationStudies
with3Dconsistency.
Effect of 3D-aware Multi-view Refinement Stage. Here we DiversityandCreativityin3DModelGeneration.Herewe
examinetheeffectivenessofthesecondstage(i.e.,3D-awaremulti- examinethediversityandcreativityofourHi3Dbyusingdifferent
viewrefinement)onnovelviewsynthesis.Table3detailstheper- randomseeds.AsshowninFigure6,ourHi3Disabletogener-
formancesofablatedrunsofourHi3D.Specifically,thesecondrow atediverseandplausibleinstances,eachwithdistinctgeometric
removesthewholesecondstage,andtheperformancesdropby structuresortextures.Thiscapabilitynotonlyenhancestheflexi-
alargemargin.Thisvalidatesthemeritofscalingupmulti-view bilityof3Dmodelcreationbutalsosignificantlycontributestothe
imageresolutionvia3D-awarevideo-to-videorefiner.Inaddition, explorationofcreativepossibilitiesin3Ddesignandvisualization.
whenonlyremovingdepthconditioninsecondstage(row3),aclear
performancedropisattained,whichdemonstratestheeffectiveness 6 CONCLUSION
ofdepthconditionthatenhances3Dgeometryconsistencyamong
Thispaperexploresinherent3Dpriorknowledgeinpre-trained
multi-viewimages.
videodiffusionmodelforboostingimage-to-3Dgeneration.Par-
EffectofInterpolationviewnumber𝑀in3DReconstruc-
ticularly,westudytheproblemfromanovelviewpointofformu-
tion.Table4showsthesingleviewreconstructionperformancesof
latingsingleimagetomulti-viewimagesas3D-awaresequential
usingdifferentnumbersofinterpolationviews𝑀.Intheextreme
imagegeneration(i.e.,orbitalvideogeneration).Tomaterializeour
case of 𝑀 = 0, no interpolation view is employed, and the 3D
idea,wehaveintroducedHi3D,whichexecutestwo-stagevideo
reconstructionpipelinedegeneratestotypicalSDF-basedrecon-
diffusionbasedparadigmtotriggerhigh-resolutionimage-to-3D
struction.Byincreasing𝑀as16,thereconstructionperformances
generation.Technically,inthefirststageofbasicmulti-viewgen-
areclearlyimproved,whichbasicallyshowstheadvantageofinter-
eration,avideodiffusionmodelisremouldedwithadditional3D
polationviewsvia3DGS.However,whenfurtherenlarging𝑀,the
conditionofcamerapose,targetingfortransformingsingleimage
performancesslightlydecrease.Wespeculatethatthismaybethe
intolow-resolutionorbitalvideo.Inthesecondstageof3D-aware
resultofunnecessaryinformationacrossviewsrepeatanderror
multi-viewrefinement,avideo-to-videorefinerwithdepthcondi-
accumulating.Inpractice,𝑀isgenerallysetto16.
tionisdesignedtoscaleupthelow-resolutionorbitalvideointo
high-resolutionsequentialimageswithrichtexturedetails.The
5.5 MoreDiscussions
resultinghigh-resolutionoutputsarefurtheraugmentedwithin-
Text-to-image-to-3D.Byintegratingadvancedtext-to-imagemod- terpolationviewsthrough3DGaussianSplatting,andSDF-based
els(e.g.,StableDiffusion[46],Imagen[47])intoourHi3D,weare reconstructionisfinallyemployedtoachieve3Dmeshes.Exper-
capableofgenerating3Dmodelsdirectlyfromtextualdescriptions, imentsconductedonbothnovelviewsynthesisandsingleviewHi3D:PursuingHigh-ResolutionImage-to-3DGenerationwithVideoDiffusionModels MM’24,October28-November1,2024,Melbourne,VIC,Australia
reconstructiontasksvalidatethesuperiorityofourproposalover [25] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,ZexiangXu,andHaoSu.2023.
state-of-the-artapproaches. One-2-3-45:AnySingleImageto3DMeshin45SecondswithoutPer-Shape
Optimization.InNeurIPS.
[26] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,SergeyZakharov,
ACKNOWLEDGMENTS andCarlVondrick.2023.Zero-1-to-3:Zero-shotoneimageto3dobject.InICCV.
[27] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,and
ThisworkwassupportedbyNationalKeyR&DProgramofChina WenpingWang.2024.SyncDreamer:GeneratingMultiview-consistentImages
(No.2022YFB3104703)andinpartbytheNationalNaturalScience fromaSingle-viewImage.InICLR.
[28] ZhenLiu,YaoFeng,MichaelJBlack,DerekNowrouzezahrai,LiamPaull,and
FoundationofChina(No.62172103).
WeiyangLiu.2023.MeshDiffusion:Score-basedgenerative3dmeshmodeling.In
ICLR.
REFERENCES [29] XiaoxiaoLong,Yuan-ChenGuo,ChengLin,YuanLiu,ZhiyangDou,LingjieLiu,
YuexinMa,Song-HaiZhang,MarcHabermann,ChristianTheobalt,etal.2024.
[1] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,Maciej Wonder3D:SingleImageto3DusingCross-DomainDiffusion.InCVPR.
Kilian,DominikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts, [30] XiaoxiaoLong,ChengLin,PengWang,TakuKomura,andWenpingWang.2022.
VarunJampani,andRobinRombach.2023.StableVideoDiffusion:ScalingLatent Sparseneus:Fastgeneralizableneuralsurfacereconstructionfromsparseviews.
VideoDiffusionModelstoLargeDatasets.arXivpreprintarXiv:2311.15127(2023). InECCV.
[2] AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWook [31] LukeMelas-Kyriazi,IroLaina,ChristianRupprecht,andAndreaVedaldi.2023.
Kim,SanjaFidler,andKarstenKreis.2023.AlignyourLatents:High-Resolution Realfusion:360degreconstructionofanyobjectfromasingleimage.InCVPR.
VideoSynthesiswithLatentDiffusionModels.InCVPR. [32] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,Ravi
[3] HanshengChen,JiataoGu,AnpeiChen,WeiTian,ZhuowenTu,LingjieLiu,and Ramamoorthi,andRenNg.2020.NeRF:RepresentingScenesasNeuralRadiance
HaoSu.2023.Single-StageDiffusionNeRF:AUnifiedApproachto3DGeneration FieldsforViewSynthesis.InECCV.
andReconstruction.InICCV. [33] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller.2022.Instant
[4] YangChen,JingwenChen,YingweiPan,XinmeiTian,andTaoMei.2023.3D NeuralGraphicsPrimitiveswithaMultiresolutionHashEncoding.TOG(2022).
CreationatYourFingertips:FromTextorImageto3DAssets.InACMMM. [34] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,
[5] YangChen,YingweiPan,YehaoLi,TingYao,andTaoMei.2023. Control3d: BobMcGrew,IlyaSutskever,andMarkChen.2022. GLIDE:TowardsPhoto-
Towardscontrollabletext-to-3dgeneration.InACMMM. realisticImageGenerationandEditingwithText-GuidedDiffusionModels.In
[6] YangChen,YingweiPan,HaiboYang,TingYao,andTaoMei.2024. Vp3d: PMLR.
Unleashing2dvisualpromptfortext-to-3dgeneration.InCVPR. [35] AlexNichol,HeewooJun,PrafullaDhariwal,PamelaMishkin,andMarkChen.
[7] YangChen,YingweiPan,TingYao,XinmeiTian,andTaoMei.2019.Animating 2022.Point-e:Asystemforgenerating3dpointcloudsfromcomplexprompts.
YourLife:Real-TimeVideo-to-AnimationTranslation.InACMMM. arXivpreprintarXiv:2212.08751(2022).
[8] YangChen,YingweiPan,TingYao,XinmeiTian,andTaoMei.2019.Mocycle-gan: [36] MichaelNiemeyer,JonathanT.Barron,BenMildenhall,MehdiS.M.Sajjadi,
Unpairedvideo-to-videotranslation.InACMMM. AndreasGeiger,andNohaRadwan.2022.RegNeRF:RegularizingNeuralRadiance
[9] ZilongChen,YikaiWang,FengWang,ZhengyiWang,andHuapingLiu.2024.V3d: FieldsforViewSynthesisfromSparseInputs.InCVPR.
Videodiffusionmodelsareeffective3dgenerators.arXivpreprintarXiv:2403.06738 [37] YingweiPan,ZhaofanQiu,TingYao,HouqiangLi,andTaoMei.2017.Tocreate
(2024). whatyoutell:Generatingvideosfromcaptions.InACMMultimedia.
[10] Yen-ChiCheng,Hsin-YingLee,SergeyTulyakov,AlexanderGSchwing,and [38] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.2023.Dreamfusion:
Liang-YanGui.2023.SDFusion:Multimodal3dshapecompletion,reconstruction, Text-to-3dusing2ddiffusion.InICLR.
andgeneration.InCVPR. [39] TianhaoQi,ShanchengFang,YanzeWu,HongtaoXie,JiaweiLiu,LangChen,
[11] MattDeitke,RuoshiLiu,MatthewWallingford,HuongNgo,OscarMichel,Aditya QianHe,andYongdongZhang.2024.DEADiff:AnEfficientStylizationDiffusion
Kusupati,AlanFan,ChristianLaforte,VikramVoleti,SamirYitzhakGadre,Eli ModelwithDisentangledRepresentations.InCVPR.
VanderBilt,AniruddhaKembhavi,CarlVondrick,GeorgiaGkioxari,KianaEhsani, [40] GuochengQian,JinjieMai,AbdullahHamdi,JianRen,AliaksandrSiarohin,Bing
LudwigSchmidt,andAliFarhadi.2023.Objaverse-XL:AUniverseof10M+3D Li,Hsin-YingLee,IvanSkorokhodov,PeterWonka,SergeyTulyakov,etal.2024.
Objects.InNeurIPS. Magic123:OneImagetoHigh-Quality3DObjectGenerationUsingBoth2Dand
[12] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,Eli 3DDiffusionPriors.InICLR.
VanderBilt,LudwigSchmidt,KianaEhsani,AniruddhaKembhavi,andAliFarhadi. [41] YuruiQian,QiCai,YingweiPan,YehaoLi,TingYao,QibinSun,andTaoMei.
2023.Objaverse:Auniverseofannotated3dobjects.InCVPR. 2024.BoostingDiffusionModelswithMovingAverageSamplinginFrequency
[13] KangleDeng,AndrewLiu,Jun-YanZhu,andDevaRamanan.2022. Depth- Domain.InCVPR.
supervisedNeRF:FewerViewsandFasterTrainingforFree.InCVPR. [42] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
[14] LauraDowns,AnthonyFrancis,NateKoenig,BrandonKinman,RyanHickman, SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
KristaReymann,ThomasBMcHugh,andVincentVanhoucke.2022. Google etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
scannedobjects:Ahigh-qualitydatasetof3dscannedhouseholditems.InICRA. InICML.
[15] QianchengFu,QingshanXu,Yew-SoonOng,andWenbingTao.2022. Geo- [43] AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,NatanielRuiz,BenMilden-
Neus:Geometry-ConsistentNeuralImplicitSurfacesLearningforMulti-view hall,ShiranZada,KfirAberman,MichaelRubinstein,JonathanBarron,etal.2023.
Reconstruction.InNeurIPS. Dreambooth3d:Subject-driventext-to-3dgeneration.InICCV.
[16] Yuan-Chen Guo. 2022. Instant Neural Surface Reconstruction. [44] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.
https://github.com/bennyguo/instant-nsr-pl. 2022. Hierarchicaltext-conditionalimagegenerationwithcliplatents. arXiv
[17] JunlinHan,FilipposKokkinos,andPhilipTorr.2024.Vfusion3d:Learningscalable preprintarXiv:2204.06125(2022).
3dgenerativemodelsfromvideodiffusionmodels.arXivpreprintarXiv:2403.12034 [45] RenéRanftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun.
(2024). 2020.Towardsrobustmonoculardepthestimation:Mixingdatasetsforzero-shot
[18] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic cross-datasettransfer.TPAMI(2020).
models.InNeurIPS. [46] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn
[19] JonathanHoandTimSalimans.2022. Classifier-freediffusionguidance.In Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In
NeurIPSWorkshop. CVPR.
[20] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,Mohammad [47] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyL
Norouzi,andDavidJFleet.2022.Videodiffusionmodels.InNeurIPS. Denton,KamyarGhasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,Tim
[21] ZehuanHuang,HaoWen,JuntingDong,YaohuiWang,YangguangLi,Xinyuan Salimans,etal.2022.Photorealistictext-to-imagediffusionmodelswithdeep
Chen,Yan-PeiCao,DingLiang,YuQiao,BoDai,andLuSheng.2024.EpiDiff: languageunderstanding.InNeurIPS.
EnhancingMulti-ViewSynthesisviaLocalizedEpipolar-ConstrainedDiffusion. [48] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,Xinyue
InCVPR. Wei,LinghaoChen,ChongZeng,andHaoSu.2023.Zero123++:aSingleImage
[22] HeewooJunandAlexNichol.2023.Shap-e:Generatingconditional3dimplicit toConsistentMulti-viewDiffusionBaseModel.arXivpreprintarXiv:2310.15110
functions.arXivpreprintarXiv:2305.02463(2023). (2023).
[23] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. [49] YichunShi,PengWang,JianglongYe,LongMai,KejieLi,andXiaoYang.2024.
2023. 3DGaussianSplattingforReal-TimeRadianceFieldRendering. TOG MVDream:Multi-viewDiffusionfor3DGeneration.InICLR.
(2023). [50] YanShu,WeichaoZeng,ZhenhangLi,FangminZhao,andYuZhou.2024.Visual
[24] XuetingLi,SifeiLiu,KihwanKim,ShaliniDeMello,VarunJampani,Ming-Hsuan TextMeetsLow-levelVision:AComprehensiveSurveyonVisualTextProcessing.
Yang,andJanKautz.2020. Self-supervisedsingle-view3dreconstructionvia arXivpreprintarXiv:2402.03082(2024).
semanticconsistency.InECCV.MM’24,October28-November1,2024,Melbourne,VIC,Australia HaiboYangetal.
[51] JiamingSong,ChenlinMeng,andStefanoErmon.2021. Denoisingdiffusion inevitablyremainedintheSRoutputs.Incontrast,ourdevised3D-
implicitmodels.InICLR. awarevideo-to-videorefinernotonlyproducesclearresultswithno
[52] StabilityAI.2023.StableZero123. https://stability.ai/news/stable-zero123-3d-
blur,butalsogeneratescorrect“hand”and“face”thatareconsistent
generation.
[53] StanislawSzymanowicz,ChristianRupprecht,andAndreaVedaldi.2023.Viewset withtheinputimage.Thiscomparisonclearlydemonstratesthe
Diffusion:(0-)Image-Conditioned3DGenerativeModelsfrom2DData.InICCV. effectivenessofourproposed3D-awarevideo-to-videorefinerfor
[54] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi,LizhuangMa,and
DongChen.2023.Make-it-3d:High-fidelity3dcreationfromasingleimagewith
generatinghigh-resolution(1,024×1,024)multi-viewimageswith
diffusionprior.InICCV. finer3Ddetailsandconsistency.
[55] MaximTatarchenko,StephanRRichter,RenéRanftl,ZhuwenLi,VladlenKoltun,
andThomasBrox.2019.Whatdosingle-view3dreconstructionnetworkslearn?.
InCVPR.
[56] VikramVoleti,Chun-HanYao,MarkBoss,AdamLetts,DavidPankratz,Dmitry
Tochilkin,ChristianLaforte,RobinRombach,andVarunJampani.2024.Sv3d:
Novelmulti-viewsynthesisand3dgenerationfromasingleimageusinglatent
videodiffusion.arXivpreprintarXiv:2403.12008(2024).
[57] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,TakuKomura,andWenping
Wang.2021.NeuS:LearningNeuralImplicitSurfacesbyVolumeRenderingfor
Multi-viewReconstruction.InNeurIPS.
[58] XintaoWang,LiangbinXie,ChaoDong,andYingShan.2021. Real-esrgan:
Trainingreal-worldblindsuper-resolutionwithpuresyntheticdata.InICCVW.
[59] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli.2004.Image
qualityassessment:fromerrorvisibilitytostructuralsimilarity.TIP(2004).
[60] ZhenXing,QijunFeng,HaoranChen,QiDai,HanHu,HangXu,ZuxuanWu,
andYu-GangJiang.2023. Asurveyonvideodiffusionmodels. arXivpreprint
arXiv:2310.10647(2023).
[61] DejiaXu,YifanJiang,PeihaoWang,ZhiwenFan,YiWang,andZhangyangWang.
2023.Neurallift-360:Liftinganin-the-wild2dphototoa3dobjectwith360views.
InCVPR.
[62] HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,andTaoMei.
2023.3dstyle-diffusion:Pursuingfine-grainedtext-driven3dstylizationwith2d
diffusionmodels.InACMMM.
[63] HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,ZuxuanWu,Yu-
gangJiang,andTaoMei.2024.DreamMesh:Jointlymanipulatingandtexturing
trianglemeshesfortext-to-3dgeneration.InECCV.
[64] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga Sorkine-
Hornung.2019.DifferentiableSurfaceSplattingforPoint-basedGeometryPro-
cessing.TOG(2019).
[65] XiaohuiZeng,ArashVahdat,FrancisWilliams,ZanGojcic,OrLitany,Sanja
Fidler,andKarstenKreis.2022. LION:LatentPointDiffusionModelsfor3D
ShapeGeneration.InNeurIPS.
[66] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 2023.
3dshape2vecset:A3dshaperepresentationforneuralfieldsandgenerativediffu-
sionmodels.InSIGGRAPH.
[67] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.
2018.TheUnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric.
InCVPR.
[68] ZhongweiZhang,FuchenLong,YingweiPan,ZhaofanQiu,TingYao,YangCao,
andTaoMei.2024.TRIP:TemporalResidualLearningwithImageNoisePrior
forImage-to-VideoDiffusionModels.InCVPR.
[69] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and
ChangWenChen.2024.Sd-dit:Unleashingthepowerofself-superviseddiscrim-
inationindiffusiontransformer.InCVPR.
APPENDIX
RecallthatinStage-2(seeSec.4.3),wedeviseanew3D-awarevideo-
to-videorefinertofurtherscaleupthelow-resolution(512×512)
outputsofStage-1tohigherresolution(1,024×1,024).Analterna-
tivesolutionistouseasuper-resolution(SR)modeltodirectlyup-
scalethegeneratedmulti-viewimagesinStage-1into1,024×1,024
resolution.HereweadoptatypicalSRmethod(Real-ESRGAN[58])
forcomparison.
Figure7showcasesthecomparisonresults.TheSRmethodcan
onlyeliminatetheblurrinessandproducesharpoutputs,butfailsto
alleviatethegeometryandappearancedistortionsintheinputmulti-
viewimages.TakingFigure7(a)asanexample,comparedwiththe
inputimage,the“hands”and“face”inthegeneratedimagesofStage-
1aredistorted.TheSRmethodReal-ESRGANcannotcorrectthese
distortionsasitwasprimarilytrainedtoproducehigh-resolution
imagesstrictlyconsistentwiththeinput.ThusthesedistortionsHi3D:PursuingHigh-ResolutionImage-to-3DGenerationwithVideoDiffusionModels MM’24,October28-November1,2024,Melbourne,VIC,Australia
Input image
Stage-1 result
(512 × 512)
Stage-1 result +
Real-ESRGAN
(1024 × 1024)
Stage-1 result +
Our Stage-2 refiner
(1024 × 1024)
(a) (b)
Figure7:Comparingour3D-awarevideo-to-videorefinerwithtypicalsuper-resolutionmethod(Real-ESRGAN[58])inStage-2.