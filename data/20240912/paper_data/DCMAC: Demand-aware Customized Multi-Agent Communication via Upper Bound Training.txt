DCMAC: Demand-aware Customized Multi-Agent
Communication via Upper Bound Training
DongkunHuo HuatengZhang YixueHao
dongkunhuo@hust.edu.cn huatengzhang@hust.edu.cn yixuehao@hust.edu.cn
YuanlinYe LongHu RuiWang
yuanlinye@hust.edu.cn hulong@hust.edu.cn ruiwang2020@hust.edu.cn
MinChen
minchen@ieee.org
Abstract
Efficient communication can enhance the overall performance of collaborative
multi-agentreinforcementlearning. Acommonapproachistoshareobservations
throughfullcommunication,leadingtosignificantcommunicationoverhead. Ex-
istingworkattemptstoperceivetheglobalstatebyconductingteammatemodel
basedonlocalinformation. However,theyignorethattheuncertaintygenerated
bypredictionmayleadtodifficulttraining. Toaddressthisproblem,wepropose
aDemand-awareCustomizedMulti-AgentCommunication(DCMAC)protocol,
which use an upper bound training to obtain the ideal policy. By utilizing the
demandparsingmodule,agentcaninterpretthegainofsendinglocalmessageon
teammate,andgeneratecustomizedmessagesviacomputethecorrelationbetween
demandsandlocalobservationusingcross-attentionmechanism. Moreover,our
method can adapt to the communication resources of agents and accelerate the
training progress by appropriating the ideal policy which is trained with joint
observation. ExperimentalresultsrevealthatDCMACsignificantlyoutperforms
the baseline algorithms in both unconstrained and communication constrained
scenarios.
1 Introduction
CollaborativeMulti-AgentReinforcementLearning(MARL)[1,2,3,4]hasachievedsignificant
results in various fields, including traffic signal control, swarm robotics, and sensor networks.
Comparedtosingle-agentreinforcementlearning,MARLhasmorecomplexproblemstohandle
becausetheinteractionbetweenagentsleadstonon-stationarityintheenvironment. Toavoidnon-
stationarityandachievescalability,acentralizedtraininganddecentralizedexecution(CTDE)[5,
6]paradigmiscommonlyusedtotrainmulti-agentmodels. Acentralizedparadigmisusedduring
training,andattheendoftrainingtheagentsmakedecisionsusingthetrainedpoliciesbasedonly
ontheirownlocalobservations. Thisarchitecturecanovercometheproblemsofenvironmentnon-
stationarityandlarge-scaleagentstosomeextent. Manyapproacheshavebeenproposedbasedon
thisparadigm,suchasMADDPG [7]andQMIX [8],whichhaveshownexcellentperformancein
multipleenvironments. IntheCTDEframework,althoughtheproblemofnon-stationaritycanbe
mitigatedbycentralizedtraining,therearestilldifficultiesinthecollaborationbetweentheagentsas
eachagentonlyhasaccesstolocalobservationduringtheexecutionprocess. Inordertocoordinate
theagents,utilizingcommunicationtointeractinformationisagoodway.
4202
peS
11
]IA.sc[
1v72170.9042:viXraBBrrooaaddccaasstt
mmeessssaaggee
Msg: I am at
t th hee rs ee ia ss aid se h,
ip.
C mC muu eess sstt ssoo aamm ggeeiizzeedd
Msg: seaside
TTaarrggeett ppooiinntt TTaarrggeett ppooiinntt TTaarrggeett ppooiinntt
Target point Predict result: Parse result:
Obs：seaside teammate is at teammate is on
Action：ship the seaside. the land.
Seaside Seaside Seaside
Agent A Agent A AAggeenntt AA
You should go You should go
Msg: I am on there by ship. there by car.
the land, there You should go You should go
is a car. there by car. there by ship.
MMssgg:: llaanndd
Target point Predict result: Parse result:
Obs：land teammate is on teammate is at
Action：car the land. the seaside.
AAggeenntt BB LLaanndd AAggeenntt BB LLaanndd AAggeenntt BB LLaanndd
Broadcast Teammate model Demand parse
Figure1: Illustratetheneedforparsingdemandswithanexample. Twoagentsareseekingtoreach
a goal, and in the “Broadcast” mode, the agents need to broadcast all the observations and then
actaccordingTalsyk:. Inthe“Teammatemodel”,localobservationsareusedtopredictthestateofthe
Both agents
teammates,arwriveh tihce h may result in prediction errors and leading to the sending of error messages.
In the “Detmargaetn pdoinpt arse” model, the agent first parses the teammates’ demands and then gives the
successfully.
correspondingsuggestions,whichcaneffectivelyimprovethetaskcompletionefficiency.
Communicationcanhelpagentssharelocalinformation,whichenablestheagentstoobtainridof
perceptuallimitations,betterunderstandtheglobalstate,andthenmakemoreaccuratedecisions[9].
However,previousworksshowthatnotalltheagents’observationshavelearningvalue,andexcessive
redundant information may lead to counterproductive learning [10]. Moreover, communication
resourcesarelimitedinrealscenarionleadtothefully-connectedcommunicationapproachisnot
applicableinpractice. SchedNet[11]presentsamethodtoregulatethenumberofagentsallowedto
broadcast,takingintoaccounttheimportanceofeachagent’sobservationstoavoidlinkoverloading..
Toreducetheredundancyofmessages,G2ANet[12]proposestoextractfeaturesfromobservations
basedontheattentionmechanismandencodethefeaturesintomessagestofullyutilizetheshared
information. Theseapproachesaimtoinduceimplicitcoordinationbetweenagents,ignoringthe
pitfallsofenlargingthepolicyspace. MAIC[13]proposestogenerateincentivemessagesthatcan
biasanagent’sQ-valuebasedonteammatemodel,whicheffectivelyresolvedthisproblem. However,
theypredictteammatemodelsbasedonlocalinformation,whichwillcreateuncertaintyandincrease
trainingdifficulty,andfailtoutilizetheconvenienceprovidedbycommunication.
Inthiswork,wepresentanoveldemand-awarecustomizedmulti-agentcommunicationprotocol,
i.e.,DCMACwhichaimsatoptimizingtheutilizationoflimitedcommunicationresources,reducing
theuncertaintyduringthetrainingprocess,andimprovingagentcollaboration. Fig.1revealsthe
main idea of our method. Specifically, the agents initially broadcast tiny messages using scarce
communicationresources,thenparseteammatedemandsfromthereceivedtinymessages,andfinally
basedonlocalinformationandteammatedemandsgeneratecustomizedmessageswhichcanbias
teammateQ-values. Toacceleratethetrainingprogress,weproposeatrainingparadigmbasedon
theupperboundofmaximumreturn,includingTrainModeandTestMode. InTrainMode,we’ll
traintheidealpolicyasguidancemodelbyusingjointobservations,andassistthetargetpolicyin
convergingtowardstheguidancemodel. InTestMode,thelossiscalculatedutilizingthedemand
lossfunctionandtemporaldifference(TD)errorfunction,toupdatethedemandparsingmoduleand
thecustomizedmessagegenerationmodule. Theultimategoalistofacilitateefficientcommunication
withinlimitedcommunicationresources.
To validate the effectiveness of DCMAC, we employ Hallway, LBF and SMAC as experimental
settings. Weconductvariouscomparisonexperiments,includingunrestrictedcommunicationand
hierarchicalcommunicationresourcelimitations.CombiningDCMACwiththemulti-agentalgorithm
QMIX, and comparing it with common RL algorithms and communication algorithms, our goal
istodemonstratethatDCMAC’slearningefficacyisonparwithalgorithmsallowingunrestricted
2communicationincommunication-constrainedsettings,andsuperiorinscenarioswithcommunication
restrictions.
Themaincontributionsofthepaperareasfollows:
• Wepresentademand-awarecustomizedmulti-agentcommunicationprotocol. Insteadof
predictingteammatemodelsusinglocalinformation,weproposeteammatedemandmodule
thatcanparseteammatedemandsfromtinymessagestoreducetheuncertaintygenerated
bypredictionandenhancelearningefficiency.
• Webelievethattheidealpolicycanbetrainedusingthejointobservationsandproposea
trainingparadigmbasedonmaximum-returnupperbound, whichacceleratetrainingby
fittingidealpolicy.
• Underdefaultalgorithmparameters,comprehensiveexperimentswerecarriedoutinvari-
ouscommunicationenvironments,demonstratingthatourproposedmethodsignificantly
improvescommunicationperformance.
2 ProblemFormulation
Inthispaper,weconsiderafullycooperativeMARLcommunicationproblem,whichcanbemodeled
as Decentralised Partially Observable Markov Decision Process (Dec-POMDP). We formulate
Dec-POMDP with communication as a tuple < N,S,A,P,Ω,O,R,γ,C,D,G >, where N =
{1,··· ,n} is the set of agents, S is the set of joint states space, A is the set of actions, Ω is the
setofobservations,O istheobservationfunction,Ristherewardfunction,γ ∈ [0,1)standsfor
thediscountedfactor,C indicatesthecommunicationconstraintfunction,Disthedemandparsing
functionandG representsthecustomizedmessagegenerator. Ateachtimestep,eachagentican
acquiretheobservationo ∈Ω,whichisgeneratedbytheobservationfunctionO(s,i)withs∈S.
i
Then,d iscomputedbyD(i,j),whichdenotesthatagentiparsesthedemandofagentj. Agent
ij
i∈N useG toencodeitslocalobservationo andthedemandd parsedfromtinymessagemtiny.
i ij ji
Themessageintendedforotheragentiisdefinedasm =G(o ,d ),wherei̸=j. Wedenotem
ij i ij ·i
asthesetofmessagesreceivedbyagentifromotheragents,andm asthesetofmessagessent
i·
byagentitootheragents. C limitsthenumberofmessagessentbyagentiascount(m )≤C(i).
i·
Priortoexecutinganyactions,agentsengageincommunicationwithoneanother. Subsequently,each
agentifollowsitsindividualpolicyπ (a |τ ,m )toselectanactiona ∈ A,whereτ represents
i i i i i i
the history (o1,a1,··· ,ot−1,at−1,ot) of agent i up to the current time step t. The joint action
i i i i i
a = ⟨a ,··· ,a ⟩ is executed in the environment resulting in next state s′ = P (s′|s,a) and the
1 n
globalrewardR(s,a). Theformalobjectiveistofindajointpolicyπ(τ,a)tomaximizetheglobal
valuefunctionQπ (τ,a) = E [(cid:80)∞ γtR(s,a)|s = s,a = a,π], withτ = ⟨τ ,··· ,τ ⟩. To
tot s,a t=0 o 0 1 n
modeltheupperboundonthemaximumreturn,wealsodefinetheidealpolicywithfullobservability:
π∗ =[π∗(a |o ,··· ,o ),∀i].
i i 1 n
3 Method
Inthissection,weelaborateonthedesigndetailsofDCMAC.TheprimaryconceptofDCMACisto
parseteammatedemandsfromtinymessage. Agentwillgeneratetinymessagemtiny basedonlocal
historicalobservationandbroadcastitatregularintervals. Thereceivercanparseteammate’sdemand
frommtiny andgeneratecustomizedmessagesbycombiningitsownobservations. Toaccelerate
trainingefficiency,weproposethemaximumreturnupperboundtrainingparadigm,inspiredbythe
conceptofknowledgedistillation,toalignthetargetpolicywiththeidealpolicy.
3.1 Demand-awareCustomizedMulti-AgentCommunication
Webelievethatprovidingfeedbackbasedonteammatedemandcanenhancecollaborationamong
agentseffectively.Toachievethis,wedesignedthreemainmoduleswhicharetinymessagegeneration
module,teammatedemandparsingmodule,andcustomizedmessagegenerationmodule.
The existing work has pointed out that the dimension of the observation space is normally large
inmulti-agenttrainingscenarios,andthereexistsredundantinformationintherawmessages[12].
Thus,wedesignthefeatureextractionmodulewithself-attentionmechanism,whichcanhelpagent
3Qtot (h,a)
Mixing Network Qjloc (hj,aj)
Qiloc (hi,ai)
st
Test or Train mj.t
Msg generator
·
mi.t Agent j
Basic Network Qiloc (hi,ai) Qig,loc (hi,ai) ai.
Q Loss
Qi (hi,ai)
+ ∑ m.it
Qi (hi,ai)
+ ∑ m.ig,t Soft ·max vi MLP
qi ki
MLP hit genM es rg at or mi.t genM es rg at or mi.g,t FC FC MLP genM es rg at or
Di.t
hit
Di.g,t hit Di.t
hit T Dea em
m
m ana dte Mti In Ly o_ ssm.i DG el mo b aa nl
d
h-it Demand in Dfe ir
.infer,t
MLP
hit-1 RNN hit gt ein ny
e
m ras tog
r
tiny_mi. MI Loss MLP RNN
fit-1 Demand_infer LeakyReLU
Extractor
Extractor BatchNorm
Train
Test Mode
Mode MLP
Agent i
oi t,ait-1 hit ,h-it,a-it oj t,ajt-1
Figure2: TheDCMACNetwork. TheoverallarchitectureincludesBasicNetworkandComNet. The
BasicNetworkusesextractortoextractfeaturef asinputstocomputeQ andhistoricalobservation
i i
h whiletheComNetiscategorizedintotwomodes,i.e.testmodeandtrainingmode.Inthetestmode
i
tinymessagesaregeneratedandbroadcasted,theagentparsesteammates’demandsandgenerates
customizedmessagesbasedonthedemandsandlocalinformation. Inthetrainingmodeusejoint
observationtotraintheguidancemodel,useteammates’actionstoinferthedemandsdinfer and
ij
updatethedemandparsingmodule. bdenotesthecustomizedmessagegenerationmoduleandthe
demandinferencemodule.
i extract feature f from observation o and minimize the influence of redundant information on
i i
trainingoutcomes. Then,agenticaninputf intotheGRUmoduletoobtainhistoricalobservation
i
h .
i
Tominimizethecommunicationcostandassistagenttounderstandteammatedemand,wedesign
thetinymessagegenerationmodule. Agentiisenabletogeneratelowerdimensionaltinymessage
mtiny byprocessinghistoricalobservationsh ,whichwillbebroadcastperiodically. Additionally,
i· i
thedemandparsingmoduleisdesignedtohelpagentsinunderstandingteammatedemands. Agenti
isabletocomputethedemandofagentj,d basedonthereceivedtinymessagemtiny.
ij ji
Furthermore,consideringthecontinuityofmessagesandthescalabilityofthealgorithm,wedesign
the customized message generation module. Unlike the traditional methods that rely on input
messagestobroadentheagent’sobservationscope,ourapproachaimstopreventinaccuraciesin
parsingtheglobalstateduetomessageloss. Agenticangeneratecustomizedmessagem that
ij
willbiastheQvalueofagentj basedonteammatedemandd andh . Thismethodsuccessfully
ij i
mitigatestheissueofanomaliesinglobalstateinterpretationcausedbymessageloss.
Moreover,consideringthelimitedcommunicationresource,inordertoreducecommunicationburden,
weproposethelinkpruningfunctiontopk. Basedonthecommunicationresourceconstraint,agent
can only send m to agents which have higher correlation. The correlation α between agent i
i· ij
andagentj canbecalculatewiththeh andtheteammates’demandsd byusingcross-attention
i ij
mechanism.
α =softmax(λ(W h )T(W d )) (1)
ij q i k ij
4whereλisthetemperatureparametertoscalethemagnitudeofinput,W andW arethecoefficient
q k
matricesofh andd usedtocomputeQueryandKeyinattentionmechanism. Thenagentisends
i ij
messageswithhighercorrelationbasedonthecommunicationresourceconstraints,i.e.count(m )≤
i·
C(i). Therefore,wetakethetopkagentsofαtoestablishaconnectionvector:
l =topk(α ,C(i)) (2)
i i·
Thel istheconnectedvectorofagenticomposedof0and1,andwhenl =1meansthatagenti
i ij
sendamessagetoagentj. Thus,wecanobtainthelocalQloc(ht;θb)asfollows:
i i
N
(cid:88)
Qloc(h ;θb)=Q (h ;θb)+ m l (3)
i i i i ji ji
j̸=i
whereθbareparametersofbasicnetwork.
3.2 MaximumReturnUpperBoundTrain
Believing that the policy learned through global observation represents the ideal policy, i.e.,
π∗(a |o ,o ,··· ,o )[14]. Inthiswork,toacceleratetrainingefficiency,theidealpolicyisusedasa
i 1 2 n
guidancemodeltodirectthetargetpolicyπtoaligntheidealpolicyπ∗. Consideringtheexcessive
dimensionofglobalstateinformation,wedesigntheglobaldemandmoduletousetheh ofteam-
i
mateagentitoparseteammate’sdemandd andreplicatetheeffectsofglobalobservationduring
ij
training. Updatingtheglobaldemandmodulenecessitatesamorereliabledemandasareference
forcomputingthelossfunction. Wepositthatcomputingtheteammate’sdemandusingagentj’s
historicalobservationh andselectedactiona isamorecredibleapproach. Therefore,wedesign
j j
thedemandinfermoduletogetthemorecredibledemanddinfer withh andactiona ,whichis
ij j j
obtainedfromQ ofidealpolicyπ∗.
j
Then, we employ mutual information to design the demand loss function and update the global
demandmodule. ByusingtheconditionalentropiesH(d |h )andH(d |h ,a ),wecancompute
ij j ij j j
themutualinformationasfollow:
I(d ,a |h )=H(d |h )−H(d |h ,a ) (4)
ij j j ij j ij j j
Butitisdifficulttocomputetheconditionaldistributiondirectly,sincethevariableh,d,aallfollow
unknowndistribution. Basedonthedefinitionofmutualinformationandinspiredbythemethod
proposedin[15],wecanderivealowerboundformutualinformation:
N
(cid:88)
I(d ,a |h )≥− E [D (p(d |h )||q(d |h ,a )] (5)
ij j j B KL ij j ij j j
i̸=j
wherethevariablesofdistributionpandqaresampledfromthereplaybufferB,andD denotes
KL
theKullback-Leiblerdivergence. Sincea andd arenotindependentofeachother,I(d ,a |h )is
j ij ij j j
notlessthantherightsideofequalsign. Thenwecanwritethefollowinglossfunctiontoreducethe
differenceofmutualinformationbetweendg anddinfer.
ij ij
N
(cid:88)
Lg(θg,θinfer)= E [D (p(dg |h )||q(dinfer|h ,ag)] (6)
d d d B KL ij j ij j j
i̸=j
whereθg isallparametersoftheglobaldemandmodule,dg iscalculatedbyglobaldemandmodule,
d ij
dinferiscalculatedbydemandinfermodule,agisobtainedfromQ .Tomigratetheknowledgefrom
ij j j
theguidancemodeltotheteammatedemandmoduleandthecustomizedmessagegenerationmodule,
wecangetp andusethedg aslabeltocomputethemutualinformationwiththed obtainedusing
ϵ ij ij
mtiny.
ji
N
(cid:88)
L (θ ,θg)= E [D (p (d |mtiny)||p(dg |h )] (7)
d d d B KL ϵ ij ji i,j j
i̸=j
Sincethealgorithmwestudyisbasedonvaluefunctions,wedesigntheTDerrorlossfunctionusing
theQvalueofπ∗asthetargetQ.Wedefinetheformulaasfollow:
L (θt)=E[(Qg (h,ag;θg)−Q (h,a;θt))2] (8)
TD tot tot
whereθg aretheparametersoftheguidancenetworkcorrespondingtotheidealpolicyπ∗,andθt
definestheparametersoftheneuralnetworkassociatedwiththebehaviorpolicyπ.
53.3 OverallOptimizationObjective
AstheDCMACframeworkisimplementedwiththeCTDEparadigm,incentralizedtrainingphase,
theguidancenetworkisupdatedbythestandardTDlossinreinforcementlearningasfollows:
L (θg)=E [(y−Q (τ,a;θg))2] (9)
RL (τ,a,r,τ′)∼B tot
wherey =r+max Q (τ′,a′;θ−)isthetarget,θ−areparametersbelongtothetargetnetwork
a′ tot
thatisperiodicallyupdated,andQ isoutputofamixingnetworksuchasVDN[16],QMIX[8],
tot
andQPLEX[17].TogetherwiththementionedTDlossandtwodemandlosses,thelearningobjective
ofDCMACis:
L(θ)=L (θg)+λ L (θt)+λgLg(θg,θinfer)+λ L (θ ,θg) (10)
RL t TD d d d d d d d d
whereθisallparametersinDCMAC,andλ ,λg andλ denoteadjustablehyperparametersoftheTD
t d d
lossandtwodemandlosses,respectively. Inthedecentralizedexecutionphase,theguidancenetwork
andmixingnetworkwillnotparticipateinthecalculation. Topreventthelazy-agentproblem[18]
andfacilitatescalability,weensurethatthelocalnetwork—comprisingthebasicnetwork,teammate
demandmodule,tinymessagegeneratorandmessagegenerator—hassameparametersforallagents.
4 Experiment
𝑎1 … 𝑎4
𝑔 𝑏1 … 𝑏6
𝑐1 … 𝑐10
(a) Hallway (b) LBF (c) SMAC
Figure3: Multiplebenchmarksusedinourexperiments
Inthissection,weevaluatetheperformanceofDCMACinthreewell-knownmulti-agentcollaborative
environment, i.e., Hallway[19], LBF[20]andSMAC[21], asshowninFig.3, andcomparethe
experimentswiththebaselinealgorithms,i.e.,MAIC,NDQ,QMIX,andQPLEX.Specifically,first,
basedonthedemand-awarecustomizedmessage,ourDCMACalgorithmoutperformsthebaseline
models4.1. Thenwecomparethewinratesinthetrainingphasetodemonstratetheeffectivenessof
theupperboundtrainingparadigm4.2. Finally,wesetupmulti-levelcommunication-constrained
environmentsandconductcomparativeexperimentswithalgorithmsthatconsidercommunication
constraints4.3.
The evaluation environments are Hallway, Level-Based Foraging and StarCraft II Multi-Agent
Challenge(SMAC).TheHallwayisacommonlyusedcooperativeenvironmentinwhichagentscan
onlyobserveitsownpositionandchooseactionsfrommovingleft,movingright,orstayingstill. The
primaryobjectiveinHallwayisforagentstolearnoptimalpoliciesfornavigation,consideringthe
presenceandactionsofotheragents. Atthestartofthegame,nagentsarerandomlyinitializedat
differentpositions,andthentheywillreceivearewardonlyiftheyallreachthegoalgsimultaneously.
TheLBFisanotherMARLenvironmentdesignedtostudycollaborativebehaviorsamongagents. It
focusesonscenarioswhereagentsworktogethertoachievecommongoals,makingitanexcellent
testbedforcooperativepoliciesinMARL.Moreover,weappliedtheDCMACalgorithmtotheSMAC
benchmark. Ourevaluationincludedtwohardmaps: 2c_vs_64zgandMMM2,aswellasthesuper
hardmaps3c_vs_100zg. Inthesemaps,multipleagentsarerequiredtocooperateagainstenemy
forces. Eachagentcanonlyobservelocalinformationandneedstocooperatewithotheragentsto
formulatestrategiestodefeatopponents. Forevaluation,allresultsarereportedon3randomseeds.
Detailsaboutbenchmarks,hyper-parametersandpseudocodeofourmethodareallpresentedin
Appendices2,3and4respectively.
6         '  0  0  4 &  0 $  $ 0  ,  6  , &  ; , $  $ &           '  0  0  4 &  0 $  $ 0  ,  6  , &  ; , $  $ &          '  0  0  1  4 &  '  0 $  $ 0  4 ,  6  , &  ; , $  $ &          '  0  0  1  4 &  '  0 $  $ 0  4 ,  6  , &  ; , $  $ &
           4 3 / ( ;     4 3 / ( ;
            
            
         
    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
(a) Hallway (b) LBF (c) 2c_vs_64zg (d) MMM2
         '  0  4  4 &  0  3 $ 0  / ,  , &  ;  ( $  ; &          '  0  4  4 &  0  3 $ 0  / ,  , &  ;  ( $  ; &          '  0  0  1  4 &  '  0 $  $ 0  4 ,  6  , &  ; , $  $ &          '  0  4  4 &  0  3 $ 0  / ,  , &  ;  ( $  ; &
          4 3 / ( ;   
           
           
       
    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
(e) MMM (f) 5m_vs_6m (g) 3c_vs_100zg (h) MMM3
Figure4: Performancecomparisonwithbaselinesonmultiplebenchmarks
4.1 CommunicationPerformance
WefirstcompareDCMACwithdifferentbaselinealgorithmstoinvestigatecommunicationefficiency.
Since the Hallway and LBF are both sparse reward scenarios, agents need to perform multiple
explorations and frequent communication for better collaboration. As shown in Fig. 4(a), in the
Hallwayscenario,QMIXisunabletoobtainthestateofitsteammates,resultinginpoorsynergy
andaseverelossoflearningperformance.Fig.4(b)illustratesthatintheLBFscenario,basedon
thehelpofhybridnetwork,QMIXcanobtaintheteammateinformationandthecollaborationhas
ahighimprovement. MASIAalgorithmcansharetheobservationandhelptheagenttoextractthe
teammateinformation. InscenarioslikeHallwayandLBF,wheretheobservationspaceissmall,
thereislessredundantinformationandtheobservationscanbeshareddirectly. Therfore,inFig.4(a)
and Fig. 4(b), MASIA performs better than others. The uncertainty generated by predicting the
teammatemodelmayreducethebenefitfromcommunication. Thus,MAICperformsworsethan
MASIA.Duetothecomplexneuralnetwork,DCMACalgorithmperformanceisinbetween,butit
stillcaneffectivelyimprovethecollaborativeperformancebetweenagents.
We apply our method and baselines to SMAC. The results show that the algorithms eventually
reachtheidealstate. However,inthescenarioswithlargeobservationspaceslikenc_vs_mzg,the
observationsshouldbeencodedandcompressedtoextractfeatures. Otherwise,itnotonlyincrease
the communication overhead, but also much redundant information affects the training progress.
Fig.4(c),Fig.4(d),Fig.4(e)andFig.4(f)revealthatQMIX,QPLEX,andNDQalgorithmsperform
slightlypoorly. MAICpredictstheteammatemodel,andtheuncertaintygeneratedbytheprediction
contributes to the exploration process. DCMAC also has this type of effect, when parsing tiny
messages from the same agent, the demand parsed by different agents may differ, which is also
beneficialinexpandingexploration. Atthesametime,sincethedemandsareparsedbasedontiny
messages, theexplorationspaceislimited, reducingtheimpactofuncertaintyontraining. Thus,
DCMACworksbest. AsshowninFig.4(g)andFig.4(h),eveninthesuperhardmaps,DCMAC
showsbetterperformancethanothers.
4.2 GuidanceModelPerformance
Toverifytheperformanceoftheidealpolicy,wecomparethewinrateduringthetrainingprocess.
Fig. 5(a) and Fig. 5(b) illustrate that the guidance model of DCMAC performs slightly worse in
HallwayandLBFscenarios. Thisisbecauseitscomplexneuralnetworkstructureconvergesslower
thantheotherbaselinesinscenarioswithsmallerobservationspaces. However,asshowinFig.5(c),
Fig.5(d),Fig.5(e)andFig.5(f),theguidancemodelofDCMACoutperformstheothercompared
algorithmsnotonlyconvergingfasterbutalsoobtaininghigherwinratesinhardmaps. Moreover,in
thesuperhardmaps3c_vs_100zgandMMM3,DCMACshowsexcellentconvergence(seeFig.5(g)
andFig.5(h)). Thisindicatesthatitispossibletotrainanidealpolicyusingjointobservations,and
7
  H W D 5  Q R :  W V H 7
  H W D 5  Q R :  W V H 7
 Q D H 0  Q U X W H 5  W V H 7
  H W D 5  Q R :  W V H 7
  H W D 5  Q R :  W V H 7
  H W D 5  Q R :  W V H 7
  H W D 5  Q R :  W V H 7
  H W D 5  Q R :  W V H 7         '  0  0  4 &  0 $  $ 0  ,  6  , &  ; , $  $ &      '  0  0  4 &  0 $  $ 0  ,  6  , &  ; , $  $ &          '  0  0  1 &  ' $  $ 0  4 ,  6 &  , $  $ &          '  0  0  1 &  ' $  $ 0  4 ,  6 &  , $  $ &
           4  4 0  3 / , ;  ( ;     4  4 0  3 / , ;  ( ;
            
            
         
    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
(a) Hallway (b) LBF (c) 2c_vs_64zg (d) MMM2
         '  0  4  4 &  0  3 $ 0  / ,  , &  ;  ( $  ; &          '  0  4  4 &  0  3 $ 0  / ,  , &  ;  ( $  ; &          '  0  0  1 &  ' $  $ 0  4 ,  6 &  , $  $ &          '  0  4  4 &  0  3 $ 0  / ,  , &  ;  ( $  ; &
 4 0 , ;
          4 3 / ( ;   
           
           
       
    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
(e) MMM (f) 5m_vs_6m (g) 3c_vs_100zg (h) MMM3
Figure5: Performanceofguidancemodelcomparedwithbaselinesonmultiplebenchmarks
alsoprovesthatthedemandinfermoduleplaysanassistingrole. Inconnectionwiththeresultsin4.1,
thefactthatDCMACcanshowexcellentresultsinthetestscenarioindicatethattheguidancemodel
π∗playsagoodguidingrole.
4.3 CommunicationPerformancewithconstraint
           '  '  '  0  0  0 &  &  &  $  $  $ 0  0  0  ,  ,  , &  &  & $  $  $            &  &  &                                            '  '  '  0  0  0 &  &  &  $  $  $ 0  0  0  ,  ,  , &  &  & $  $  $            &  &  &                                            '  '  '  0  0  0 &  &  &  $  $  $ 0  0  0  ,  ,  , &  &  & $  $  $            &  &  &                                            '  '  '  0  0  0 &  &  &  $  $  $ 0  0  0  ,  ,  , &  &  & $  $  $            &  &  &                                
           
           
       
    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
(a) MMM2testmode (b) MMM2trainmode (c) 5m_vs_6mtestmode (d) 5m_vs_6mtrainmode
Figure6: Performanceofcomparisonwithcommunicationconstrained
ToverifytheperformanceofDCMACunderconstrainedcommunicationconditions,weconduct
experimentsintheMMM2and5m_vs_6mmapsofSMACandsetthreelevelsofcommunication
constraints(i.e.,95%,90%and85%)tocomparetheexperimentswithMAICundersameconstraints.
As shown in Fig. 6, under the 95% communication constraint, the performance of DCMAC has
almostnodegradationandstillmaintainsahighlearningperformance. TheperformanceofDCMAC
decreasesslightlyunderthe90%communicationconstraint,butstillmanagestoconvergefasterthan
MAICunderthe95%restriction,andtheguidancemodelgetshigherwinrateintheend. Atthe85%
communicationlimit,DCMACbeginstoshowasignificantdecline,butguidancemodelstillobtains
higherwinratethanMAICinTrainMode. Theoretically,ifthetrainingcontinues,DCMACwill
bebetterthanMAICinTestMode. TheexperimentalresultsrevealthatDCMACparseteammates’
demandsbetter,generatecustomizedmessages,andperformwellinlinkpruning. Benefitingfrom
theguidanceoftheidealpolicyandtheparsingofteammates’demands,thecollaborationbetween
theagentscanstillbemaintainedandtheoveralllearningperformancecanbekeptupeveninthe
environmentswithpoorcommunicationconditions. Insummary,DCMACcaneffectivelyimprove
theoverallperformanceofcollaborativemulti-agentreinforcementlearningundercommunication
constraints.
8
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  W V H 7
 Q D H 0  Q U X W H 5  Q L D U 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  W V H 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  Q L D U 7
  H W D 5  Q R :  Q L D U 75 RelatedWork
Multi-agentReinforcementLearning(MARL)hasmadesignificantprogressinrecentyears.Afterthe
publicannouncementoftheCTDEparadigm,manyapproacheshaveemergedandmadesignificant
progress. Theycanbebroadlycategorizedintopolicy-basedandvalue-basedmethods. Typicalpolicy
gradientmethodsincludeMADDPG[7],COMA[22],MAAC[23],MAPPO[24],FACMAC[25],
andHAPPO[26],whichaimatexploringtheoptimizationofmulti-agentpolicygradientmethods.
Value-basedmethodsfocusonfactorizationofglobalvaluefunctions. VDN[16]sumseachagent’s
Q toQ inthecentralizationnetwork. QMIXusesneuralnetworkstochangeindividualgainsand
i tot
teamgainsfromsimplesummationassumptionsofVDNtomonotonicityconstraintsthataremore
generalizable.
Inrecentyears,therehasbeenasignificantadvancementinresearchonmulti-agentcommunication
methods[27]. Previousworkscanbedividedintotwocategories. Onefocusesonlimitingtheamount
ofmessagestransmittedwithinthenetwork. TheATOC[28],IC3[29],andI2C[30],hasutilized
localgatingmechanismstodynamicallytrimcommunicationlinksbetweenagents,thusalleviating
communicationoverhead. Nevertheless,thereceivershoulddecodethemessage,andthelackofa
messagecanresultinerroneousinterpretation,leavingopenthequestionofwhetherthesemethods
areeffectiveinsystemswithseverelyrestrictedcommunicationbudgets. Conversely,approacheslike
DIALandthosebasedonvectorquantizationproducediscretemessagesdirectly,whileNDQ[19]and
TMC[31]alsocraftmessagesinaspace-efficientmanner. However,theexpressivepowerofdiscrete
messages may be curtailed by the communication budget imposed by broadcast communication
schemes. Furthermore, methods such as ETC [32], VBC [33], and MBC[34] introduced event-
triggeredcommunicationtodecreasethefrequencyofcommunicationandaddresscommunication
constraints. Thesestrategiesaimtooptimizetheutilizationofcommunicationresourcesforenhanced
performancebyfine-tuningthetimingoftransmissionsandallocatingcommunicationresourcesas
needed.
Onthecontrary,otherworksfocusonefficientlearningtocreatemeaningfulmessagesorextract
valuable information from messages. TarMAC [35], DICG[36], and DGN[37] have leveraged
attention mechanisms and graph neural networks (GNNs) to enable agents to learn from local
observationsandbroadcastmessagestoallaccessibleagents.Followingthese,subsequentapproaches
have proposed ways to enhance performance from both the sender’s and receiver’s perspectives.
Onthesenderside,advancementshavebeenmadeinmessageencodingmethods,withalgorithms
likeMAIC[13]andToM2C[38]conductingteammatemodelingtogeneratemotivationalmessages
tailored to the receiver’s identity. On the receiver side, more refined aggregation schemes have
been developed to make more efficient use of received messages, leading to algorithms such as
G2A [12] and MASIA[10]. PMAC [39] constructs peer-to-peer communication graphs, designs
personalizedmessagesendingandreceivingmethods,fullyunderstandsagents’state,andachieves
efficientcommunication. CACOM[40]designsacontext-awarecommunicationbasedapproachto
maintainmessagecontinuitybyusingtheLSQmethodtodifferentiatethegatingunits. TEM[41]
proposesaTransformer-basedemailmechanism(TEM)tosolvethescalabilityproblemofmulti-agent
communication.
Tothebestofourknowledge,thecurrentresearchoverlookstheimpactofuncertaintyresultingfrom
teammatemodelconductingandthesignificanceofteammatedemand. Ourapproachenablesagents
toparseteammatedemandandgeneratecustomizedmessages,therebyenhancingagentcollaboration
andoverallalgorithmperformance.
6 ConclusionandFutureWork
In this paper, we investigate enhancing the efficiency of collaborative multi-agent learning and
proposeademand-awarecustomizedmulti-agentcommunicationprotocol,DCMAC.Previouswork
concentratedonovercomingtheconstraintsofpartialobservationsbyextendingagentperception
rangewithsharedmessagesorutilizinglocalinformationtopredicttheteammatemodel. Theformer
approachleadstoanomaliesinparsingtheglobalstateasmessagelossoccurs. Thelatterapproach
maygenerateuncertaintyinthepredictionprocessandraisetrainingdifficulty. Ourapproachenables
theagenttoobtainthebasicinformationofteammatesbybroadcastingtinymessages. Thedemand
parsingmoduleinDCMACcanassistagenttoparsethedemandofteammateandthengenerate
customizedmessages,therebyimprovingcommunicationefficiency. Inaddition,wedrawontheidea
9ofknowledgedistillationandusejointobservationstotraintheidealpolicyasaguidancemodel,and
migratetheknowledgefromtheguidancemodeltothetargetpolicybydesigningthecorresponding
lossfunction. Wenotonlyconductmultiplesetsofexperimentsinvariousbenchmarks,butalso
designcommunication-constrainedscenariostoverifytheeffectivenessofDCMAC.Ourapproach
isstillatanearlystage, andfurtherrefinementofthecohortmodelisnecessaryinthefuture. It
willalsobeameaningfulworktoconsidertheimpactofmessagetransmissiondelayonlearningin
communicationenvironments.
References
[1] PabloHernandez-Leal,BilalKartal,andMatthewETaylor.“Asurveyandcritiqueofmulti-
agentdeepreinforcementlearning”.In:AutonomousAgentsandMulti-AgentSystems33.6
(2019),pp.750–797.
[2] Yiming Li, Shunli Ren, Pengxiang Wu, et al. “Learning distilled collaboration graph for
multi-agentperception”.In:AdvancesinNeuralInformationProcessingSystems34(2021),
pp.29541–29552.
[3] Muning Wen, Jakub Kuba, Runji Lin, et al. “Multi-agent reinforcement learning is a se-
quencemodelingproblem”.In:AdvancesinNeuralInformationProcessingSystems35(2022),
pp.16509–16521.
[4] TonghanWang,HengDong,VictorLesser,andChongjieZhang.“Roma:Multi-agentrein-
forcementlearningwithemergentroles”.In:arXivpreprintarXiv:2003.08039(2020).
[5] LandonKraemerandBikramjitBanerjee.“Multi-agentreinforcementlearningasarehearsal
fordecentralizedplanning”.In:Neurocomputing190(2016),pp.82–94.
[6] XueguangLyu,YuchenXiao,BrettDaley,andChristopherAmato.“Contrastingcentralized
anddecentralizedcriticsinmulti-agentreinforcementlearning”.In:(2021).
[7] Ryan Lowe, Yi I Wu, Aviv Tamar, et al. “Multi-agent actor-critic for mixed cooperative-
competitiveenvironments”.In:Advancesinneuralinformationprocessingsystems30(2017).
[8] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,etal.“Monotonicvalue
functionfactorisationfordeepmulti-agentreinforcementlearning”.In:JournalofMachine
LearningResearch21.178(2020),pp.1–51.
[9] JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShimonWhiteson.“Learn-
ingtocommunicatewithdeepmulti-agentreinforcementlearning”.In:Advancesinneural
informationprocessingsystems29(2016).
[10] Cong Guan, Feng Chen, Lei Yuan, et al. “Efficient multi-agent communication via self-
supervisedinformationaggregation”.In:AdvancesinNeuralInformationProcessingSystems.
2022.
[11] DaewooKim,SangwooMoon,DavidHostallero,etal.“Learningtoschedulecommunication
inmulti-agentreinforcementlearning”.In:InternationalConferenceonLearningRepresenta-
tions(2019).
[12] YongLiu,WeixunWang,YujingHu,etal.“Multi-agentgameabstractionviagraphattention
neuralnetwork”.In:ProceedingsoftheAAAIconferenceonartificialintelligence.2020.
[13] LeiYuan,JianhaoWang,FuxiangZhang,etal.“Multi-agentincentivecommunicationvia
decentralized teammate modeling”. In: Proceedings of the AAAI Conference on Artificial
Intelligence.2022.
[14] JingdiChen,TianLan,andCarleeJoe-Wong.“RGMComm:ReturnGapMinimizationvia
DiscreteCommunicationsinMulti-AgentReinforcementLearning”.In:Proceedingsofthe
AAAIConferenceonArtificialIntelligence.2024.
[15] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. “Deep Variational
InformationBottleneck”.In:InternationalConferenceonLearningRepresentations.2017.
[16] Peter Sunehag, Guy Lever, Audrunas Gruslys, et al. “Value-decomposition networks for
cooperativemulti-agentlearning”.In:arXivpreprintarXiv:1706.05296(2017).
[17] Jianhao Wang, Zhizhou Ren, Terry Liu, et al. “QPLEX: Duplex Dueling Multi-Agent Q-
Learning”.In:InternationalConferenceonLearningRepresentations.2021.
[18] RichardSSuttonandAndrewGBarto.Reinforcementlearning:Anintroduction.MITpress,
2018.
10[19] TonghanWang,JianhaoWang,ChongyiZheng,andChongjieZhang.“Learningnearlyde-
composablevaluefunctionsviacommunicationminimization”.In:InternationalConference
onLearningRepresentations.2020.
[20] GeorgiosPapoudakis,FilipposChristianos,LukasSchäfer,andStefanoVAlbrecht.“Bench-
marking multi-agent deep reinforcement learning algorithms in cooperative tasks”. In: In
ProceedingsoftheNeuralInformationProcessingSystemsTrackonDatasetsandBenchmarks.
2021.
[21] MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,etal.“Thestarcraftmulti-
agentchallenge”.In:arXivpreprintarXiv:1902.04043(2019).
[22] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,etal.“Counterfactualmulti-agent
policygradients”.In:ProceedingsoftheAAAIconferenceonartificialintelligence.Vol.32.1.
2018.
[23] ShariqIqbalandFeiSha.“Actor-attention-criticformulti-agentreinforcementlearning”.In:
Internationalconferenceonmachinelearning.PMLR.2019,pp.2961–2970.
[24] ChaoYu,AkashVelu,EugeneVinitsky,etal.“Thesurprisingeffectivenessofppoincoopera-
tivemulti-agentgames”.In:AdvancesinNeuralInformationProcessingSystems35(2022),
pp.24611–24624.
[25] BeiPeng,TabishRashid,ChristianSchroederdeWitt,etal.“Facmac:Factoredmulti-agent
centralised policy gradients”. In: Advances in Neural Information Processing Systems 34
(2021),pp.12208–12221.
[26] JakubGrudzienKuba,RuiqingChen,MuningWen,etal.“Trustregionpolicyoptimisationin
multi-agentreinforcementlearning”.In:arXivpreprintarXiv:2109.11251(2021).
[27] Changxi Zhu, Mehdi Dastani, and Shihan Wang. “A survey of multi-agent reinforcement
learningwithcommunication”.In:arXivpreprintarXiv:2203.08975(2022).
[28] Jiechuan Jiang and Zongqing Lu. “Learning Attentional Communication for Multi-Agent
Cooperation”.In:AdvancesinNeuralInformationProcessingSystems.2018.
[29] AmanpreetSingh,TusharJain,andSainbayarSukhbaatar.“Learningwhentocommunicate
atscaleinmultiagentcooperativeandcompetitivetasks”.In:InternationalConferenceon
LearningRepresentations.2019.
[30] ZiluoDing,TiejunHuang,andZongqingLu.“Learningindividuallyinferredcommunication
formulti-agentcooperation”.In:Advancesinneuralinformationprocessingsystems.2020.
[31] SaiQianZhang,QiZhang,andJieyuLin.“Succinctandrobustmulti-agentcommunication
withtemporalmessagecontrol”.In:Advancesinneuralinformationprocessingsystems.2020.
[32] GuangzhengHu,YuanhengZhu,DongbinZhao,etal.“Event-triggeredcommunicationnet-
workwithlimited-bandwidthconstraintformulti-agentreinforcementlearning”.In:IEEE
TransactionsonNeuralNetworksandLearningSystems34.8(2021),pp.3966–3978.
[33] SaiQianZhang,QiZhang,andJieyuLin.“Efficientcommunicationinmulti-agentreinforce-
ment learning via variance based control”. In: Advances in neural information processing
systems.2019.
[34] ShuaiHan,MehdiDastani,andShihanWang.“Model-basedSparseCommunicationinMulti-
agent Reinforcement Learning”. In: Proceedings of the 2023 International Conference on
AutonomousAgentsandMultiagentSystems.2023.
[35] AbhishekDas,ThéophileGervet,JoshuaRomoff,etal.“Tarmac:Targetedmulti-agentcom-
munication”.In:InternationalConferenceonmachinelearning.2019.
[36] Sheng Li, Jayesh K Gupta, Peter Morales, et al. “Deep Implicit Coordination Graphs for
Multi-agentReinforcementLearning”.In:InternationalConferenceonAutonomousAgents
andMultiagentSystems.2021.
[37] JiechuanJiang,ChenDun,TiejunHuang,andZongqingLu.“Graphconvolutionalreinforce-
mentlearning”.In:InternationalConferenceonLearningRepresentations.2018.
[38] YuanfeiWang,FangweiZhong,JingXu,andYizhouWang.“Tom2c:Target-orientedmulti-
agentcommunicationandcooperationwiththeoryofmind”.In:InternationalConferenceon
LearningRepresentations.2022.
[39] XiangruiMengandYingTan.“PMAC:PersonalizedMulti-AgentCommunication”.In:Pro-
ceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 16. 2024, pp. 17505–
17513.
11[40] XinranLiandJunZhang.“Context-awareCommunicationforMulti-agentReinforcement
Learning”.In:arXivpreprintarXiv:2312.15600(2023).
[41] Xudong Guo, Daming Shi, and Wenhui Fan. “Scalable communication for multi-agent
reinforcement learning via transformer-based email mechanism”. In: arXiv preprint
arXiv:2301.01919(2023).
12