A Suite for Acoustic Language Model Evaluation
Gallil Maimon† Amit Roth† Yossi Adi
School of Computer Science & Eng. School of Computer Science & Eng. School of Computer Science & Eng.
Hebrew University of Jerusalem Hebrew University of Jerusalem Hebrew University of Jerusalem
Jerusalem, Israel Jerusalem, Israel Jerusalem, Israel
Abstract—Speechlanguagemodelshaverecentlydemonstrated
great potential as universal speech processing systems. Such
models have the ability to model the rich acoustic information
existinginaudiosignals,beyondspokencontent,suchasemotion,
backgroundnoise,etc.Despitethis,evaluationbenchmarkswhich
evaluateawarenesstoawiderangeofacousticaspects,arelack-
ing.Tohelpbridgethisgap,weintroduceSALMON,anoveleval-
uation suite encompassing background noise, emotion, speaker
identity and room impulse response. The proposed benchmarks
both evaluate the consistency of the inspected element and how
much it matches the spoken text. We follow a modelling based
approach, measuring whether a model gives correct samples
higher scores than incorrect ones. This approach makes the
benchmark fast to compute even for large models. We evaluated
several speech language models on SALMON, thus highlighting
thestrengthsandweaknessesofeachevaluatedmethod.Codeand
data are publicly available - https://pages.cs.huji.ac.il/adiyoss-
lab/salmon/.
Index Terms—Speech Language Models, Acoustic Modelling
I. INTRODUCTION
Fig.1. AdemonstrationofSALMON.a)showsacousticconsistency,inthis
Speech Language Models (SLM) have recently gained casesentimentconsistency,andb)showssemantic-acousticalignment-here
large popularity as universal speech processing systems [1]. sentimentalignment.
Since early approaches [2], many improvements have been
presentedSTSPwhichusesapre-trainedsentimentclassifierto
achieved through scaling [3], [4], utilising text LMs [3], [5]–
evaluate whether the generated speech sentiment is consistent
[8], dialogue modeling [9], and aligning with human prefer-
withtheprompt.Althoughbeinghighlyvaluable,theseworks
ences [10], [11]. Recently, such models have demonstrated
evaluate a single aspect only.
impressive capabilities in real-time modeling of interactive
Parallelwork,SD-eval[19]alsoproposesamethodforeval-
conversations [12]–[15]. These improvements manifested in
uatingSLMs’abilitytoaddressacoustics.TheyusetextLLMs
higher semantic coherence of the generated speech as shown
to generate different text responses suitable for an audio input
by various text-based metrics which measure syntax and
in a given style (emotion, age, accent and background). They
grammar [16], or semantics [3], with not much emphasis on
then prompt the SLM with the recording, and compare the
other acoustic characteristics.
generated text response and the ground truth using automated
However,audiohasmanyaspectsotherthanspokencontent
orhumanmetrics.Thisworkstudiesimportantspeechaspects,
such as sentiment, speaker identity, background noise, rever-
butislimitedtoLMswhichoutputtextanswers.Furthermore,
berationetc.Forinstance,thephrase“Yeah,I’msure”couldbe
generation based metrics together with LLM based evaluation
sarcastic indicating lack of belief or sincere and sympathetic,
are more compute intensive to evaluate thus might become a
depending on the intonation. Likewise, a request for a song
bottleneck in a model development cycle.
recommendation to fit a mood might elicit different responses
ifthebackgroundnoiseandaccentindicateabeachinMexico
To address the above limitations we introduce SALMON,
a Suite for Acoustic Language Model evaluation. It follows
or a formal wedding in England.
modelling based metrics, i.e. checking that the model assigns
While some SLMs attempt to also model prosodic features
higher probability to “real” samples, considering a wide vari-
[5], [17], evaluation of their ability to model different aspects
ety of acoustic elements. Overall, we present two main tasks:
is still lacking. The authors in [18] introduced ProsAudit
(i) acoustic consistency and (ii) acoustic-semantic alignment,
which evaluates models’ ability to assign higher likelihood to
considering several acoustic aspects. The acoustic consistency
utterances with natural pauses. In addition, in [5] the authors
benchmark tests whether the model assigns higher likelihood
†Equalcontribution. totheoriginalrecording,comparedwithonewereanacoustic
4202
peS
11
]DS.sc[
1v73470.9042:viXraelement(e.g.,speaker)changesinthemiddleoftherecording. assigns higher probability to the correct sample as opposed to
The acoustic-semantic alignment metric checks if the model others which are unlikely in a specific element [3], [16].
assigns higher scores to samples where the acoustics match Modelling metrics are commonly used to evaluate spoken
the spoken content. For instance, the phrase “I wonder who is word related aspects like syntax or grammar [16] or semantic
atthedoor?”ismorelikelytobeheardnearadoorbellringin understanding [3]. They can also be used for prosody related
thebackgroundratherthanwithconstructionnoises.Thistask evaluation as in ProsAudit [18]. Modelling metrics have the
ismorechallengingforSLMsasitrequiresbothsemantictext benefits of being objective, fast and easy to compute as they
and acoustic understanding. Fig. 1 provides a visual example. do not require additional models, vocoders, or human studies.
We evaluate several SLMs using SALMON and discuss the Unlike the above mentioned SLM evaluation benchmarks,
insights in Sec. V. We show that while humans easily achieve SALMON measures acoustic elements - speaker identity,
over 90% on most tasks, SLMs struggle in modelling and sentiment, background noise and room acoustics, using the
identifyingbasicacousticinconsistencies.Weprovideaneasy modelling based approach. It consists of two levels of com-
touseevaluationscripttogetherwiththefullbenchmarks.We plexity - one measures the consistency of a given acoustic
hopeSALMONwillbebeneficialforthespeechcommunityin element, whereas the other measures alignment between the
guiding future SLMs development towards acoustic modeling acoustic details and the semantic, spoken content.
together with semantic modeling.
III. SALMON
II. RELATEDWORK The proposed benchmark is based on the modelling ap-
proachdescribedaboveandevaluatesvariousacousticaspects,
Speech Language Models. Much like text language models, namely speaker identity, sentiment, background and room im-
speech language models [2], [20], [21] use a next token pulse response. Formally, given an SLM - M :audio→[0,1]
prediction objective. As speech is continuous by nature, some that assigns a likelihood to a given audio input, for each task
SLMs operate over continuous speech representations [22], T containing samples s=(s ,s ) we define the score as:
p n
[23], yet, the mainstream approach is operating over discrete
1 (cid:88)
representations. These can be roughly divided into two main Score= 1 , (1)
|T| M(sp)>M(sn)
groups: (i) semantic tokens [2], based on applying k-means (sp,sn)∈T
over latent representations obtained from a pre-trained self-
i.e. how many times on average the model sets higher likeli-
supervisedmodel[2],[24],[25];(ii)acoustictokens[26],[27],
hood to positive samples over negative ones. On the rare case
basedonneuralcompressionmodels.Assemantictokenswere
ofidenticalscoreswegive0.5points,sothatanindiscriminate
shown to mostly discard prosody [28]–[30], expressive SLMs
model gets a random score. In practice, we implement the
augment these speech representations with prosodic features
likelihoodasM([w ,...,w ])=
1(cid:80)
p(w |w ,...,w ),
through separate streams of pitch or style tokens either in 1 t t i≤t i i−1 1
but we leave this as a design choice by the LM. Such scoring
parallel [17] or flattened into a single stream [5].
approach is similar to prior work [16]. We split SALMON
In order to use the abilities of text LMs, and to support
into two types - consistency of the inspected element and its
multi-modal tasks (e.g ASR) some LMs jointly operate over
alignment with the semantic content.
text and speech. Broadly speaking there are two main ap-
proaches: audio encoding into the text LM latent [7], [8], A. Acoustic Consistency
or vocabulary expansion, or hybrids of both [31], [32]. In WewishtoevaluatewhetherSLMsmodelacousticelements
audio encoding we optimise a mapping from the continuous of the input signal and can detect unnatural acoustic changes.
representationintothecontinuousLMlatent[33],forinstance As with other modelling metrics, we evaluate this by compar-
by fine-tuning Whisper [34]. This means we can not directly ingthelikelihoodthatanSLMassignsarecordingwithagiven
generateaudio,butonlypromptatextLMwithaudiosignals. acoustic properties, to the likelihood of the same recording
Conversely, vocabulary expansion adds speech tokens to the wherethesomeacousticpropertychangesmid-recording(e.g.,
text vocabulary and fine-tunes the LM on both modalities [6], backgroundnoise,reverberation).SeeFigure1(a)foravisual
whilesomeinterleavebetweenthemodalities[5].Thishasthe overview.
benefit of also supporting audio generation and modelling. We achieve this using a dataset with annotated attributes,
SLM Evaluation. Evaluating SLMs is non-trivial and can likespeaker,speakingstyle,andtranscriptions.Incaseswhere
broadly be categorised into three types: modelling, generative the attribute to be changed depends on the transcription (e.g.,
and task performance. Generative metrics, prompt the SLM speaker identity), we first apply a forced aligner [44] to get
with an input and evaluate the output on things such as senti- words alignment, then we split the recording by words. In
mentconsistency[5]andtextualdiversityandmeaningfulness other cases (e.g., background noise change) we simply split
[2]. Task metrics evaluate the ability of the SLM to perform the audio by time (and not words). We then take the original
tasks such as ASR or emotion recognition through prompting recording as the positive sample and mix the first part of
[35]. Another line of work, involves with evaluating SLMs as the real recording, with the following part from a different
auniversalspeechmodelsviainstructiontuninginazero-shot recording, with the same text content, as a negative. We give
fashion[36],[37].Finally,modellingmetricscheckiftheSLM task specific details below and in Tab. I.TABLEI
STATISTICSABOUTTHEDIFFERENTBENCHMARKSWITHINSALMON.FORTHESNRRANGESWEFIRSTRANDOMLYSAMPLEARANGEANDTHEN
SAMPLEUNIFORMLYWITHINTHATRANGETOGIVEVARIATION.THISMETA-DATAISRECORDEDPER-SAMPLE.
Datasets Classes SampleLength[Sec] Speakers SNRranges[dB]
SentimentConsistency Expresso[38] Happy,Sad,Whisper 5.63±1.53 4 -
SpeakerConsistency VCTK[39] 105speakers 5.97±1.70 105 -
GenderConsistency VCTK[39] male,female 5.93±1.70 104 -
Background(In-Domain)Consistency LJ[40],FSD50K[41] 20Backgrounds 5.71±1.42 LJ (.01,.02),(.1,.2),(1,2),(5,10)
Background(All)Consistency LJ[40],FSD50K[41] 20Backgrounds 5.61±1.39 LJ (.01,.02),(.1,.2),(1,2),(5,10)
RIRConsistency LJ,EchoThief[42] 5RIRs 5.48±1.33 LJ -
SentimentAlignment AzureTTS[43],GPT Happy,Sad 4.39±0.92 SaraNeural -
BackgroundAlignment FSD50K,AzureTTS,GPT 20classes 7.69±0.67 SaraNeural -
Background Consistency. For each sample, we randomly a recording with piano music to be more likely compared to
choose a speech recording from LJ speech [40], and two construction noises. Fig. 1 (b) shows a visual example.
backgroundnoisesfromFSD50K[41].Wefilteredsinglelabel We construct this benchmark by using GPT4 [47] to create
examples,manuallyselected20distinctclasses,andmanually texts that someone would likely say under a condition, i.e.
filtered recordings keeping those with clear noises and little hearing a sound or in a given sentiment. We manually filter
silences. For the positive recording, we merge between the these texts samples to leave only clear samples. We then use
speech and the first background noise. Conversely, for the Azure text-to-speech [43] to synthesise the text.
negative recording, we add the first background to the first Background Alignment. This task focuses on evaluating the
partofthespeech,andthesecondbackgroundtotherest.This alignment between background noise and the speech content.
creates a speech recording with varying background noise. For each background noise class, from the filtered FSD50K
We split this into two sub-tasks: random and in-domain, weuseinbackgroundconsistency,wegenerated20textsusing
which vary in background noise sampling. In random, we GPT4 [47] corresponding to speech that is likely to hear with
sample two random background noises from the dataset, and this class of background noise. As mentioned, we manually
in in-domain we limit the sampling of the background noise filter these texts to keep clear examples. We synthesize the
to be from the same class, e.g. the negative recording moves speech using Azure TTS [43]. For the positive recording, we
from one siren to another, making the task more challenging. sample random background noise from the positive class and
SentimentConsistency.WeusetheExpressodataset[38].For pre-pendittothesynthesizedspeech.Wegeneratethenegative
eachsample,werandomlychoose2recordingscorresponding recording by sampling a random background noise from a
to the same text with different sentiments from happy, sad or different background class and merging it with the speech
whisper. The positive recording is left as is, and the negative corresponding to the positive background class.
is generated by concatenating the first part of the sampled Sentiment Alignment. Lastly, we evaluate SLMs capability
recording, with the second part of another. to model the relation between spoken content, and speech
Speaker Consistency. Similar to sentiment consistency, we sentiment. We use GPT4 [47] to generate ∼ 200 sentences
use VCTK [39] and sample positive recording with the same which someone would say in a cheerful or sad sentiment. For
speaker and negative recording with alternating speakers. each sample, we randomly picked a sentence and generated
GenderConsistency.Wemadeaneasierversionoftheabove emotional speech for both sentiments with Azure expressive
task - called gender consistency, which forces the negative TTS. The positive sample is that where the text sentiment
speaker to be from a different gender. matches the TTS sentiment. We manually filter the result-
Room Impulse Response Consistency. Here we measure the ing samples rather aggressively as the quality of the TTS
ability of the models to grasp the change in acoustic scenes. is limited, especially in conflicting sentiment situations. We
We use LJ speech [40] and five diverse RIRs from EchoThief aim to remove any samples where the cheerful might come
[42]. We sample two impulse responses. The positive record- off as emphatic or stressed causing ambiguity. We also try
ing is the speech convolved with the first impulse response. to remove samples with noticeable synthesis artifacts. This
We construct the negative by convolving the first half of the manual process is inherently limited and subjective so we
speech with the first RIR, the second half with the second evaluate the effectiveness by human evaluation in section IV.
RIR and concatenating the two. This procedure results in two
IV. BASELINES
recordings of the same speech, one in a given room, and in
the other the room is switched mid-sample. We evaluate the performance of popular SLMs on the dif-
ferentpartsofSALMON.Throughthisweevaluatetheimpact
B. Acoustic-Semantic Alignment
of different model aspects, such as number of parameters and
This measures if an acoustic element such as background expressive modelling approaches.
noise is aligned with the text. For instance, given a recording We use TWIST [3], which uses a pre-trained text-LM as
of the phrase “I love the smooth sound of a piano” we expect an initialisation for SLM training over HuBERT [48] units.TABLEII
COMPARISONOFLEADINGSLMSONTHESALMONBENCHMARK.BG(DOMAIN)ANDBG(RAND.)STANDSFORBACKGROUNDNOISESAMPLEDFROM
THESAMEDOMAINORATRANDOM,RESPECTIVELY.
AcousticConsistency Semantic-AcousticAlignment SpokenContent
Sentiment↑ Speaker↑ Gender↑ Bg(domain)↑ Bg(rand.)↑ Room↑ Sentiment↑ Background↑ sWUGGY↑[16]
TWIST350M[3] 59.0 69.5 68.0 54.0 61.5 59.0 51.5 56.5 80.7
TWIST1.3B[3] 61.5 69.0 69.5 55.5 60.5 59.0 53.0 56.5 81.0
TWIST7B[3] 61.5 71.0 70.0 55.0 60.5 62.0 51.5 54.5 82.8
LAST350M[45] 64.0 63.0 70.5 55.5 60.5 61.0 51.5 54.5 73.5
LAST1.3B[45] 65.0 64.5 68.5 56.0 61.0 62.5 53.5 53.0 73.6
pGSLM[17] 40.5 83.0 88.5 57.0 66.0 53.5 55.5 53.5 74.1
ASR+LLM[34],[46] 53.5 52.2 52.0 50.0 50.5 53.2 50.5 51.5 77.0
Human 97.2 91.5 98.6 83.1 88.7 94.4 93.3 95.8 -
They show this noticeably improves semantic metrics such ofthis-speakerconsistency,withpGSLMreaching83.0.Itis
as sWUGGY, and we wish to evaluate how they perform on interesting to note that non-expressive methods which operate
acoustic metrics. We also utilise the existence of the three overHuBERTunitslikeTWISTmanagemuchlessevenwhen
model sizes (350M, 1.3B, 7B) to see whether this makes a almost two orders of magnitude larger. However, it is worth
difference, or if the use of only HuBERT units limits the noting that at the LM level, they can discern the speaker to
performance. an extent (as shown by the better than random performance),
We additionally explore LAST [45] as a different speech even if their vocoder might be single speaker.
tokeniser. It is a recently proposed tokeniser guided by a pre- For other consistency tasks, specifically - Sentiment, Im-
trained text LM. We note, that in this setup, LAST can be pulse Response (Room), and Background - the performance
considered a TWIST model with a different tokeniser. is better than random for some models. However, even for
We further wish to evaluate an explicitly expressive SLM the best performing ones the results are far from human
baseline, and choose pGSLM [17]. pGSLM trains an SLM performance. We note that increasing model size in TWIST
over three streams: de-duplicated HuBERT units, unit dura- and LAST has little to zero effect on model performance.
tions, and quantised pitch contour. This is meant to allow We believe these results could derive from lack of expressive
prosody modelling, which is a big part of acoustics evaluated modelling (i.e. HuBERT tokens only) or from lack of training
in SALMON. diversity.Forinstance,HuBERTunitsarelikelyill-equippedto
Human Evaluation. As the entirety of the SALMON bench- model background noise, but this could also be true for pitch
mark was created by semi-automatic measures we wished to trackers in pGSLM which are aimed at speech. Conversely,
assert that humans also find the true samples more likely we expect pitch to differ in emotional speech, making the
than the negative samples. While this task is trivial to people Sentiment Consistency task reasonable for pGSLM.
when “prompted” with the exact task question - e.g. “In We also note that a cascaded pipeline of ASR followed by
which sample does the text sentiment best match the speech a text-LM achieves practically random performance across all
sentiment?”, we wanted to assert that humans agree with the tasks. While this is intuitive because the text is identical in
labels even with the general question “Which sample is more both samples, we show that the ASR does not leak additional
likely?”. This allows to neutralise other elements which could information. Furthermore, for many semantic metrics like
impact likelihood such as TTS artifacts, speakers with strong sWUGGY this baseline is a leading option thus highlighting
accents etc, thus perfectly emulating the task the SLM faces. the importance of acoustic metric evaluation to steer research
We select 20 samples from each benchmark, and each is effortsatmethodswhichjointlyimproveacousticsandseman-
annotated by at least three annotators fluent in English. tics. We also note that models which have higher sWUGGY
scorese.g.TWISTdonotnecessarilyoutperformmethodslike
V. RESULTS
pGSLM in acoustic evaluation.
When considering the results on the semantic-acoustic
alignment tasks, in Table II, we see that no baseline achieves
VI. CONCLUSION
substantial improvements over the random baseline (56.5 We introduce SALMON, a suite for evaluating acoustic
maximum). This is despite the fact that this task is trivial to LMs on many acoustic aspects, namely: gender, speaker,
humans which achieve 93.3 and 95.8 on sentiment alignment background noise, sentiment, and room impulse response.
andbackgroundalignmentrespectively.Thesetasksremainan We achieve this through a modelling based metric which is
interesting challenging task for acoustic aware SLMs. objective and fast to compute. We evaluate several popular
In analysing the more simple acoustic consistency tasks in SLMs, and human raters, on the SALMON benchmark and
TableII,weseethatGenderconsistencyisthemosttrivial.The show that current models are far behind human performance
best model on this task, pGSLM, achieves high performance ontheevaluatedtasks.Wehopethatpublishingthisbenchmark
of 88.5 which is still shy of human performance (98.6). and sample generation pipeline will progress the development
Likewise,ontheslightlymorechallenging,generalisedversion of acoustic aware SLMs.Acknowledgements. This research work was supported by [23] EliyaNachmanietal.,“Spokenquestionansweringandspeechcontinua-
ISF grant 2049/22. tionusingspectrogram-poweredllm,”arXivpreprintarXiv:2305.15255,
2023.
REFERENCES [24] ShovalMessicaandYossiAdi, “Nast:Noiseawarespeechtokenization
forspeechlanguagemodels,” arXivpreprintarXiv:2406.11037,2024.
[1] Kai-Wei Chang et al., “Speechprompt: Prompting speech language [25] PoonehMousavi,JarodDuret,SalahZaiem,LucaDellaLibera,Artem
modelsforspeechprocessingtasks,”IEEE/ACMTransactionsonAudio, Ploujnikov,CemSubakan,andMircoRavanelli,“Howshouldweextract
Speech,andLanguageProcessing,2024. discrete audio tokens from self-supervised models?,” arXiv preprint
[2] Kushal Lakhotia et al., “On generative spoken language modeling arXiv:2406.10735,2024.
from raw audio,” Transactions of the Association for Computational [26] Alexandre De´fossez, Jade Copet, Gabriel Synnaeve, and Yossi
Linguistics,vol.9,pp.1336–1354,2021. Adi, “High fidelity neural audio compression,” arXiv preprint
[3] MichaelHassidetal., “Textuallypretrainedspeechlanguagemodels,” arXiv:2210.13438,2022.
AdvancesinNeuralInformationProcessingSystems,vol.36,2024. [27] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and
[4] Santiago Cuervo and Ricard Marxer, “Scaling properties of speech MarcoTagliasacchi, “Soundstream:Anend-to-endneuralaudiocodec,”
languagemodels,” arXivpreprintarXiv:2404.00685,2024. IEEE/ACM Transactions on Audio, Speech, and Language Processing,
[5] Tu Anh Nguyen et al., “Spirit-lm: Interleaved spoken and written vol.30,pp.495–507,2021.
languagemodel,” arXivpreprintarXiv:2402.05755,2024. [28] Gallil Maimon and Yossi Adi, “Speaking style conversion in the
[6] SoumiMaitietal., “Voxtlm:Unifieddecoder-onlymodelsforconsoli- waveform domain using discrete self-supervised units,” in Findings of
datingspeechrecognition,synthesisandspeech,textcontinuationtasks,” theAssociationforComputationalLinguistics:EMNLP2023.
in ICASSP 2024-2024 IEEE International Conference on Acoustics, [29] Amitay Sicherman and Yossi Adi, “Analysing discrete self supervised
SpeechandSignalProcessing(ICASSP).IEEE,2024,pp.13326–13330. speechrepresentationforspokenlanguagemodeling,” inICASSP2023-
[7] Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, He Huang, Boris Ginsburg, 2023 IEEE International Conference on Acoustics, Speech and Signal
Yu-Chiang Frank Wang, and Hung-yi Lee, “Desta: Enhancing speech Processing(ICASSP).IEEE,2023,pp.1–5.
language models through descriptive speech-text alignment,” arXiv [30] AdamPolyaketal.,“Speechresynthesisfromdiscretedisentangledself-
preprintarXiv:2406.18871,2024. supervisedrepresentations,” arXivpreprintarXiv:2104.00355,2021.
[8] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, [31] QianChen,YunfeiChu,ZhifuGao,ZeruiLi,KaiHu,XiaohuanZhou,
Wei Li, Lu Lu, Zejun Ma, and Chao Zhang, “Salmonn: Towards Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng, et al., “Lauragpt: Listen,
generic hearing abilities for large language models,” arXiv preprint attend, understand, and regenerate audio with gpt,” arXiv preprint
arXiv:2310.13289,2023. arXiv:2310.04673,2023.
[9] Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei- [32] PaulKRubensteinetal., “Audiopalm:Alargelanguagemodelthatcan
Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit speakandlisten,” arXivpreprintarXiv:2306.12925,2023.
Sagot, Abdelrahman Mohamed, et al., “Generative spoken dialogue [33] YunfeiChu,JinXu,XiaohuanZhou,QianYang,ShiliangZhang,Zhijie
languagemodeling,” TransactionsoftheAssociationforComputational Yan,ChangZhou,andJingrenZhou,“Qwen-audio:Advancinguniversal
Linguistics,vol.11,pp.250–266,2023. audio understanding via unified large-scale audio-language models,”
[10] DongZhang,ZhaoweiLi,ShiminLi,XinZhang,PengyuWang,Yaqian arXivpreprintarXiv:2311.07919,2023.
Zhou, and Xipeng Qiu, “Speechalign: Aligning speech generation to [34] Alec Radford et al., “Robust speech recognition via large-scale weak
humanpreferences,” arXivpreprintarXiv:2404.05600,2024. supervision,” inInternationalconferenceonmachinelearning.PMLR,
[11] Yassir Fathullah et al., “Audiochatllama: Towards general-purpose 2023,pp.28492–28518.
speech abilities for llms,” in Proceedings of the 2024 Conference [35] BinWangetal., “Audiobench:Auniversalbenchmarkforaudiolarge
of the North American Chapter of the Association for Computational languagemodels,” arXivpreprintarXiv:2406.16020,2024.
Linguistics: Human Language Technologies (Volume 1: Long Papers), [36] Chien-yuHuangetal., “Dynamic-superb:Towardsadynamic,collabo-
2024,pp.5522–5532. rative,andcomprehensiveinstruction-tuningbenchmarkforspeech,” in
[12] Xinrong Zhang, Yingfa Chen, Shengding Hu, Xu Han, Zihang Xu, ICASSP2024-2024IEEEInternationalConferenceonAcoustics,Speech
YuanweiXu,WeilinZhao,MaosongSun,andZhiyuanLiu,“Beyondthe andSignalProcessing(ICASSP).IEEE,2024,pp.12136–12140.
turn-basedgame:Enablingreal-timeconversationswithduplexmodels,” [37] QianYangetal.,“Air-bench:Benchmarkinglargeaudio-languagemod-
arXivpreprintarXiv:2406.15718,2024. els via generative comprehension,” arXiv preprint arXiv:2402.07729,
[13] Garrett Tanzer, Gustaf Ahdritz, and Luke Melas-Kyriazi, “Modeling 2024.
real-timeinteractiveconversationsastimeddiarizedtranscripts,” arXiv [38] TuAnhNguyenetal.,“Expresso:Abenchmarkandanalysisofdiscrete
preprintarXiv:2405.13203,2024. expressivespeechresynthesis,” arXivpreprintarXiv:2308.05725,2023.
[14] Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Yuanjun Xiong, [39] Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald, “Cstr
and Wei Xia, “A full-duplex speech dialogue scheme based on large vctkcorpus:Englishmulti-speakercorpusforcstrvoicecloningtoolkit
languagemodels,” arXivpreprintarXiv:2405.19487,2024. (version0.92),” 2019.
[15] ZiyangMa,YakunSong,ChenpengDu,JianCong,ZhuoChen,Yuping [40] Keith Ito and Linda Johnson, “The lj speech dataset,” https://keithito.
Wang,YuxuanWang,andXieChen, “Languagemodelcanlistenwhile com/LJ-Speech-Dataset/,2017.
speaking,” arXivpreprintarXiv:2408.02622,2024. [41] Eduardo Fonseca et al., “Fsd50k: an open dataset of human-labeled
[16] EwanDunbaretal., “Thezeroresourcespeechchallenge2021:Spoken sound events,” IEEE/ACM Transactions on Audio, Speech, and Lan-
languagemodelling,” arXivpreprintarXiv:2104.14700,2021. guageProcessing,vol.30,pp.829–852,2021.
[17] EugeneKharitonovetal., “Text-freeprosody-awaregenerativespoken [42] ChristopherWarren, “Echothief,”http://www.echothief.com, Accessed:
languagemodeling,” arXivpreprintarXiv:2109.03264,2021. Jul2024.
[18] Maureen de Seyssel et al., “Prosaudit, a prosodic benchmark for self- [43] Microsoft, “Azure tts,” https://learn.microsoft.com/en-us/azure/
supervisedspeechmodels,” arXivpreprintarXiv:2302.12057,2023. ai-services/speech-service/text-to-speech, Accessed:Jul2024.
[19] Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, [44] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner,
Lu Lu, Yuxuan Wang, Haizhou Li, and Zhizheng Wu, “Sd-eval: A and Morgan Sonderegger, “Montreal forced aligner: Trainable text-
benchmark dataset for spoken dialogue understanding beyond words,” speechalignmentusingkaldi.,” inInterspeech,2017.
arXivpreprintarXiv:2406.13340,2024. [45] Arnon Turetzky and Yossi Adi, “Last: Language model aware speech
[20] Zala´nBorsosetal., “Audiolm:alanguagemodelingapproachtoaudio tokenization,” arXivpreprintarXiv:2409.03701,2024.
generation,” IEEE/ACMTransactionsonAudio,Speech,andLanguage [46] Hugo Touvron et al., “Llama 2: Open foundation and fine-tuned chat
Processing,2023. models,” arXivpreprintarXiv:2307.09288,2023.
[21] KaranGoel,AlbertGu,ChrisDonahue,andChristopherRe´, “It’sraw! [47] Open-AI, “Gpt-4o,”https://openai.com/index/gpt-4o-system-card/, Ac-
audiogenerationwithstate-spacemodels,” inInternationalConference cessed:Jul2024.
onMachineLearning.PMLR,2022,pp.7616–7633. [48] Wei-Ning Hsu et al., “Hubert: Self-supervised speech representation
[22] Robin Algayres et al., “Generative spoken language model based on
learningbymaskedpredictionofhiddenunits,”IEEE/ACMTransactions
continuousword-sizedaudiotokens,” arXivpreprintarXiv:2310.05224, on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460,
2023. 2021.