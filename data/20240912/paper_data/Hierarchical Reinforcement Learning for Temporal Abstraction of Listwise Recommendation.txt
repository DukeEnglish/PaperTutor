Hierarchical Reinforcement Learning for Temporal
Abstraction of Listwise Recommendation
LuoJi,GaoLiu,MingyangYin,HongxiaYang,JingrenZhou
∗
DAMOAcademy,AlibabaGroup
Abstract
Modernlistwiserecommendationsystemsneedtoconsiderbothlong-termuser
perceptionsandshort-terminterestshifts. Reinforcementlearningcanbeapplied
onrecommendationtostudysuchaproblembutisalsosubjecttolargesearchspace,
sparseuserfeedbackandlonginteractivelatency. Motivatedbyrecentprogressin
hierarchicalreinforcementlearning,weproposeanovelframeworkcalledmccHRL
to provide different levels of temporal abstraction on listwise recommendation.
Withinthehierarchicalframework,thehigh-levelagentstudiestheevolutionof
userperception,whilethelow-levelagentproducestheitemselectionpolicyby
modeling the process as a sequential decision-making problem. We argue that
such framework has a well-defined decomposition of the outra-session context
andtheintra-sessioncontext,whichareencodedbythehigh-levelandlow-level
agents, respectively. To verify this argument, we implement both a simulator-
basedenvironmentandanindustrialdataset-basedexperiment. Resultsobserve
significantperformanceimprovementbyourmethod,comparedwithseveralwell-
knownbaselines. Dataandcodeshavebeenmadepublic.
1 Introduction
Inrecentyears,hybridRecommendersystems(RS)havebecomeincreasinglypopulartoovercome
information redundancy and have been widely utilized in a variety of domains (e.g. products,
articles,advertisements,music,andmovies) Anidorifónetal.[2015]. HybridRSselectsalistof
contents(usuallycalled‘items’)fromoverwhelmedcandidatesforexhibitiontomeetuserpreferences,
withexpectedclicks,dwelltimesorpurchasesincreased. Althoughwithsubstantialachievement,
more advanced recommendation techniques are always necessary because of the complexity of
business scenarios, including listwise, cold-start, heterogeneous contents, spatiotemporal-aware
(STA)recommendations,etc. Amongthese,thelistwiserecommendationisalwaysanimportant
issue,notonlyduetothewideapplicationofTop-Krecommendation Chenetal.[2019]orfeed
recommendation Wu et al. [2021], but also the computational complexity with consideration of
mutual-iteminfluence,andpositionbias.
ReinforcementLearning(RL)isanatural,unifiedframeworktolearninteractivelywiththeenviron-
mentandmaximizetheglobalandlong-termrewards,whichhighlightssomepromisingsolutionsof
listwiseranking. AccordingtotheformulationmanneroftheMarkovDecisionProcess(MDP),we
concludethatmostsuchworkscanbeclassifiedintotwomaincategories:
1. Definetheuserperception/preferenceasstateandtheentirerecommendationlistasaction,
andmodeltheuserinterestshiftasstatetransition Zhengetal.[2018],Chenetal.[2018,
2019],Ieetal.[2019].
∗Correspondingauthor:jiluo.lj@alibaba-inc.com
Preprint.Underreview.
4202
peS
11
]RI.sc[
1v61470.9042:viXras1 s2 s3
slow interval slow interval
t-1 t t+1
session 1 user session 2 user
service call feedback service call feedback
k=0 k=1 k=2 k=0 k=1 k=2
fast interval fast interval
Figure1:Twolevelsoftemporalabstractionsintypicallistwiserecommendations.Theuserrequestsa
sessionserviceattimet,thenarankingpolicyisexecutedkstepstoprovideatop-krecommendation.
2. Formulatethelistwiserankingproblemasasequentialdecision-makingprocess,withthe
staterepresentedbythecurrentlistrankeditem,whiletheactionistoselectthenextoptimal
itemtothelistGongetal.[2019],Zhaoetal.[2019,2018].
Nevertheless,bothmethodologieshavetheirobstacles,whichpreventthewideapplicationofRLon
industrialRS.Forthefirstcategory,athoroughrepresentationofuserstatescanbeexpectedespecially
forSTAorPOI-basedrecommendations,butthemodelingofstatetransitionisquestionablesinceits
modelingtimeintervaliscoarse-grained. UsersnaturallyinteractwithRSbasedontheitem-wise
experienceinsteadofthesession,andtheymightleavetheappforawhilebeforethenextsession
experience,bothofwhichmaketheMarkovassumptionquestionable. Thesecondcategory,onthe
otherhand,canconsiderthemutualinfluenceofitems,theintra-sessionuserinterestshift,andmay
eveninteractwithuserswithinthesession. However,itissubjecttothecurseofdimensionalitywhen
servingalluserssimultaneouslyatacentralizedserver. Italsohasthesparserewardissueforan
RLframeworksincetheuserresponsecannotbeobservedbytheagentuntiltheendofthesession.
Figure1indicatesthesetwotemporalpatternsoflistwiseranking.
Hereweaimtoovercometheaboveobstaclesthroughbettertemporalabstractions,bydesigning
afast-slowlearningparadigmMadanetal.[2021]ofhierarchicalreinforcementlearning(HRL).
AlthoughtherearesomepreviousHRL-basedRSeffortsTakanobuetal.[2019],Zhaoetal.[2020],
Xieetal.[2021],theymightfocusonheterogeneousormulti-goalrecommendationproblems.Instead,
wedesignourHRLsuchthatonelevelencodestheuserperceptionandouter-sessioncontext,while
anotherlevelencodesthespatiotemporalstatus,intra-sessioncontext,andmutual-iteminfluences.
Besidesthat,wefurtherimprovethesampleefficiencybyapplyingEdgeComputingorEdge-based
AI Zhou et al. [2019b] on our RS. With the rapid development of Edge Computing or Edge AI
Zhouetal.[2019b],itispossibletoutilizeuserfeaturesandfeedbackonmobileGongetal.[2020]
orcollaborativetrainingwithcloudYaoetal.[2021],Chenetal.[2021]inrecommendation. We
embracethisbenefittofurtherimprovethemodelingdepthofuserstatesandenhancetheMarkov
assumption. Wearguethatthelow-levelHRLcouldbedeployedonmobiledevices,thereforethe
on-device features can be involved, training is decoupled, and the communication frequency of
cloudserviceisreducedduringmodelinference. Althoughthisintuitionbelongstotheparadigmof
collaborativetrainingwithmobileandcloudYaoetal.[2021],Chenetal.[2021],however,itisthe
2firstattemptthattriestotrainpartofRLcomponentsontheedgeside,tothebestofourknowledge
Yaoetal.[2022].
In this paper, we propose a novel methodology, named mobile-cloud collaborative Hierarchical
ReinforcementLearning(mccHRL)forListwiseRecommendation,whichemploysHRLtosolvea
highlySTA,listwiseRSproblemonaworld-leadingmobileapplication. TheHigh-LevelofmccHRL
modelstheuserperceptionandSTAstatus,interactswithuserresponsesattheendofsession,and
takesthesuggestedlong-termuserpreferenceasaction;whiletheLow-LevelofmccHRLsolvesthe
listwiserankingproblem,byprovidinganitemselectionpolicybasedonusershort-terminterestand
on-devicefeatures. TheHigh-LevelactionisutilizedasthegoalofLow-Levelpolicytoachieve,
suchthattheLow-Levelagentdoesnotdirectlyinteractwithuserresponse. Embeddingstudiedon
theHigh-LevelaretransmittedtotheLow-Levelpolicyforknowledgetransferandgeneralization.
Detailedimplementationhasbeenmadepublic2. Toconclude,themajorcontributionsofthispaper
include:
• WeproposeanovelHRLframeworkonlistwiserecommendationwithanaturalwayoftem-
poralabstractions. TheHigh-Levelagentstudiestheoutra-sessioncontext(spatiotemporal
effects)anddirectlyinteractswithusers,whiletheLow-Levelagentstudiestheintra-session
context,intrinsicallymotivatedbythehigh-level.
• Weprovideanexplicitanddetailedimplementationofon-devicefeaturesandcloudtrans-
missionlatencyinourframework. TheHRLsolvesthemobile-cloudcollaborationwithits
hierarchicalarchitecture.
• Beforetheonlinedeployment,wedesigntwoofflinepipelines,includingsimulator-based
anddataset-basedexperiments. Wealsodeveloptheofflineversionoftrainingalgorithm
anditsperformanceisverifiedbyindustrial-scaleexperiments.
• Wedesignandimplementtherealisticmobile-cloudrecommendationenvironmentinour
experiments. On-devicefeatures,userone-devicefeedback,andmobile-cloudtransmission
delaysareexplicitlyconsideredinanonlinestudyingmanner,whichisseldompracticedby
previousedge-basedrecommendationstudies.
Therestofthepaperisorganizedasfollows. Theconnectionwithpreviousworksisfirstdiscussedin
Section2. PreliminariesandtheproblemformulationarethenintroducedinSection3. Ourmethod-
ologyandcorrespondingalgorithmsarestatedinSection4. Experimentresultsaresummarizedin
Section5. FinallySection7concludesthispaper.
2 RelatedWork
2.1 RL-basedRecommendation
RLhasbeenwidelyusedinrecommendationtasks,whiletheirdefinitionsofMDPcanbediversified
intotwocategories. Thefirstcategoriesmodelstheuserpreferenceasstateandrecommendation
theentireitemlistasaction,suchasZhengetal.[2018],Chenetal.[2018,2019],Ieetal.[2019],
inwhichthestatetransitdepictstheuserinterestshift. Ontheotherhand,thesecondcategoryis
to model the ranking steps of items as state, and the selection of the next favorite item is action.
SuchmethodsincludeMDPrankXiaetal.[2017],Weietal.[2017],Exact-KGongetal.[2019],
LIRDZhaoetal.[2019]andDeepPageZhaoetal.[2018],whichaimstomodelthemutualinfluence
betweenitemsandlistwisebias. TheintuitionofcombiningtheadvantagesoftwotypesofMDPs
motivatesourideaofmccHRL.
There are also RL works aiming to fix some special recommendation issues, such as location or
POI-based information Zhou et al. [2019a], cold start Wang et al. [2020], heterogeneous items
Takanobuetal.[2019],Xieetal.[2021],userlong-termengagementZouetal.[2019]orfairnessof
itemsGeetal.[2021]. OurworksharessimilarintereststothePOI-basedrecommendationwithLian
etal.[2020],Zhouetal.[2019a],Luoetal.[2021]. However,mostofthemworkonthenext-location
recommendationgivensparsespatialinformation,tolearnareasonableuser-locationrelationship
matrix, while we aim to provide an end-to-end solution of session recommendation with spatial
informationimplicitlyconsideredinourrecommendingpolicy.
2https://anonymous.4open.science/r/eccHRL-F99B/
32.2 HierarchicalReinforcementLearning
HierarchicalReinforcementLearning(HRL)hasahierarchicalstructureofRLlayersinordertosolve
morecomplextasks,reducethesearchingspace,providedifferentlevelsoftemporalabstractions,
ordealwithsparserewards. HRLmethodscanbeclassifiedintotwocategoriesbythecoordinated
way between different levels. The first is the goal-conditional framework Kulkarni et al. [2016],
Vezhnevetsetal.[2017],Nachumetal.[2018],Levyetal.[2019],inwhichahigh-levelagentlearns
thegoaltodrivethelow-levelagent; thesecondissub-taskdiscoveryframework, thehigh-level
agentmightprovidesomeoptions(orskill)toreducethesearchspaceoflow-levelagentBaconetal.
[2017],Florensaetal.[2017]. Ourmethodologybelongstothefirstcategoryinwhichthehigh-level
agentstudiestheuserperceptionembeddingasthelow-levelagentdecisionbasis.
HRLmethodsalsovaryinhierarchicalstructures. Choicesincludemulti-layerDQNKulkarnietal.
[2016],multi-layerpolicyNachumetal.[2018],ormulti-layeractor-criticLevyetal.[2019]. Our
model is similar to hierarchical actor-critic (HAC) but reduces its complexity for practical edge
deployment.
2.3 HRL-basedRecommendation
TherearealsoattemptstoapplyHierarchicalReinforcementLearning(HRL)onrecommendationor
searchTakanobuetal.[2019],Zhaoetal.[2020],Xieetal.[2021]. Takanobuetal.[2019]usesthe
High-LevelRLastheheterogeneoussearchsourceselector;similarly,High-LevelRLinHRL-rec
Xieetal.[2021]isthecontentchannelselector. MaHRLZhaoetal.[2020]usestheHigh-LevelRL
tomodeltheuserperceptionstate,tooptimizemultiplerecommendationgoals. Allofthesemethods
definetheLow-LevelRLastheitemselector,whichissimilartoourframework. Wehaveasimilar
definitionofHigh-LevelRLwithMaHRL,however,ourdifferenceincludesthat(1)ourtargetis
toimproveasession-based,STArecommendationCTRperformanceinsteadofmultiplelabels;(2)
weprovideanaturalformulationoftemporalabstractiontosolvethesparserewardissueoflistwise
recommendation;(3)wedeploytheLow-LevelpartofHRLontheedgesidetofurtherimprovethe
methodologythroughput.
2.4 Edge-basedRecommendation
EdgeComputing(mightalsobenamedEdgeIntelligence,EdgeAI,oron-deviceAI),incontrast
toCloudcomputing,hasbeenwidelystudiedrecentlyyearsZhouetal.[2019b]. Effortshavebeen
made on applications including IoT, 5G, and auto-driving. However, this field still is at its early
stagewithmosteffortsfocusingonthelightweightanddeploymentofedgemodelsYaoetal.[2022].
Forexample,EdgeRecGongetal.[2020]worksonthesplit-deploymentwhichplacesthememory-
consumingembeddingmoduleonthecloudwhilethelightweightrecommenderisinferencedonthe
device.
TherehasbeenincreasingattentiononEdge-Cloudcollaboration,eitherprivacy-primarysuchas
FederatedLearning,orefficient-primary. Weareinterestedinefficientprimarymethods,toimprove
therecommender’spersonalization. SucheffortsincludingCOLLALuetal.[2019],DCCLYao
etal.[2021]andMC2-SFChenetal.[2021]. Forexample,COLLAdesignedthecloudmodelas
aglobal aggregator distillingknowledgefrom manyedge models; MC2-SFproposes aslow-fast
learningmechanismonRS,withthebidirectionalcollaborationbetweenEdgeandCloud. However,
edge-basedRLhasnotbeenwidelystudiedsofar. Instead,RLisoftenutilizedasanasidesystemto
EdgeComputing,tohelpserviceoffloading,taskscheduling,orresourceallocationYaoetal.[2022].
Therefore,ourworkcanberegardedasthepioneeringstudyonedge-basedRL,withtheslow-fast
collaborativestudymechanismbetweencloudandedge,similartoMadanetal.[2021]andChenetal.
[2021]. TodifferentiatefromthetraditionaledgeAImethods,weusethenotation‘mobile’insteadof
‘edge’sincemostimpressionsofinternetRSareontheusers’mobiledevices(smartphonesorpads).
3 PreliminaryandProblemFormulation
Thissectionillustrateskeyconceptsofourapproach,includingthesystemconfiguration,formulation,
anddetailedstructureofmccHRL,aswellassomenecessarypreliminaries.
4III
Critic Critic
sh sh
t t+1
SRE … SRE
g 0 g 1 g K 1
−
rl rl rl Q
Q 0 1 K 1
−
…
Actor Actor Actor Actor
sl
al
0
al
1
al
K −1
al
K
II
0
sl
1 sl sl
K 1 K I
−
Environment
rh
t
Figure2: TheinteractionprocedureofmccHRL.I:Off-policytrainingofHRA,withexternalreward
collectedattheendofsession. II:On-policytrainingofLRA,withintrinsicmotivationfromthegoal.
III:GoalinitiatesfromtheactionofHRA,withatransitfunctionprovidedbySREimplicitlyalong
thesessionlength.
3.1 ListwiseRecommendingScenario
ThereisassumedtobealistwiserecommendationtaskthatexhibitsK itemsuponeachuserquery,
inwhichthesessionlengthK isapre-determined,fixedinteger. Userclicksresponsetoexposed
itemswithinasessionisthenc={c ,c ,··· ,c }inwhicheachc isabinaryvariable,k ∈[0,K].
0 1 K k
Therecommendationobjectiveisgenerallytomaximizetheglobalsession-wiseclick-throughrate
(CTR).
Wefirstencodereachitemintotheembeddingvectore ∈ RL. Thenasessionrecurrentencoder
(SRE)isemployedtoencodethesession’shistoricallyexposeditemsequence(Wesimplyusethe
notation‘historicalsequence’inthefollowingcontextsforbriefly)intothesameembeddingspace,
l:={e ,e ,··· ,e }∈RL. MoredetailsofSREwillbeintroducedinSection4.1.
0 1 K
For personalization purpose, the user general profile, the user on-device feature, and the STA
informationareencodedintou,mandco. respectively. Thesuperscriptodenotestheoutra-session
context,todifferentiatefromtheintra-sessioncontext(ci)whichisencodedinsidetheactorandwill
bementionedinthelatercontexts.
3.2 ReinforcementLearning
ReinforcementLearning(RL)isaninteractivelearningtechnologythatisgenerallybuiltonMarkov
DecisionProcess(MDP).MDPcanberepresentedwithafour-tupleofM :=(S,A,R,T),where
t
S isthesetofstates, Aisthesetofactiona, Risthesetofrewardr, andT(s |s ,a )isthe
t+1 t t
transitionfunctionofs afterexecutinga ons . ThesubscripttofMworksasthestepindicator.
t+1 t t
RLoptimizesalong-termobjectivewhichisdefinedasthediscountedaccumulatedrewardsJ =
(cid:80) ∞ t=0γtr t,whereγ ∈[0,1)isthediscountfactor. Thegoalofreinforcementlearningistolearna
policyπ(a|s)whichmaximizesJ. Inthiswork,weemploytheclassicalmodel-freeandoff-policy
algorithmcalledtheDeepDeterministicPolicyGradient(DDPG)methodLillicrapetal.[2016]to
solvethisproblem.
53.3 DeepDeterministicPolicyGradient
Inthispaper,weimplementRLwithaclassicalmodel-freeandoff-policyalgorithmcalledtheDeep
DeterministicPolicyGradient(DDPG)methodLillicrapetal.[2016]. Herewebrieflyreviewits
algorithmicdetailswhichisthebasisoflaterderivation.
TheDDPGisborrowedtogettherecommendedscoreoftheitem(i.e.,action)andthelong-term
estimatereturn(i.e.,Q-value).
DDPG has a typical actor-critic architecture in which the actor is the parametric policy network
π (a|s)whilethecriticisastate-actionvaluefunction
θ
(cid:88)∞
Q (s,a)= γi tE(r |s =s,a =a) (1)
w − i t t
i=t
inwhichθandwaretrainableparameters. DDPGalsokeepsatargetactorandatargetcricicnetwork
withtheirtrainableparametersθ′ andw′ asinstead.
Withtargetnetworksfixed,wcanbeupdatedbyminimizing
y
t
=r t+γQ w′(s t+1,π θ′(s t+1)) (2)
thenwisupdatedbyminimizing
L=E s∽dπ(r t+γQ w′(s t+1,π θ′(s t+1))−Q w(s t,π θ(s t)))2 (3)
whichisthefamousBellmanEquation. θisupdatedbythepolicygradient
▽ θJ =E s∽dπ▽ aQ w(s,a)| a=πθ(s)▽ θπ θ(s) (4)
wheredπ(s)isthediscounteddistributionofstatessamplingbypolicyπ. Targetnetworkparameters
aresoftlyupdatedby
′ ′
θµ ←τθµ +(1−τ)θµ
′ ′
w ←τw +(1−τ)w (5)
inapre-definedtimeintervalwithτ ∈(0,1).
Thenθcanbeupdatedas
θ ←θ+ηE s∽dπ[▽ θπ θ(s)▽ aQ w(s,a)| a=π(s)] (6)
withηasthelearningrate.
3.4 GatedRecurrentUnit
Gated Recurrent Unit (GRU) is a special type of RNN that can sequentially capture the item-
dependencyimpacts. ItisfoundthatGRUhasbetterstabilitywithfewerparametersthangeneral
GRN,thereforeisusuallyusedinrecommendationstudiesHidasietal.[2016],Zhaoetal.[2019],
Zhouetal.[2019b]. ThecorelogicofGRUisthefollowing:
z =σ(W ·[h ,x ])
t z t 1 t
−
r =σ(W ·[h ,x ])
t r t 1 t
− (7)
h˜ =tanh(W ·[r h ,x ])
t h t t 1 t
−
h =(1−z )h +z (h˜ )
t t t 1 t t
−
wherez ,r areupdateandresetgates.
t t
3.5 ThemccHRLFramework
RecallingthediscussioninSection2,thelistwiserankingproblemcanbedefinedastwoMDPsin
variedtimescales:
1. Theuserstateevolutionattherealphysicaltimet,withthenotationofMDPasM ;
t
62. DeterminationofthekthoptimalitemselectionamongremainedK−kcandidates. The
MDPisnotatedbyM ,withkasthevirtualdecisionstep.
k
In our work, we use HRL to address M and M simultaneously. We define our High-Level
t k
recommendationAgent(HRA)astheuserstateencoder,withthetuple(sh,ah,rh)inM ;whilethe
t
Low-LevelrecommendationAgent(LRA)istheitemselector,workingonM withtuple(sl,al,rl).
k
Belowarefurtherdetaileddefinitions:
• High-LevelStatesh:theuserpreferenceincludingtheuserprofileu,theuser-wisebrowsing
sessionhistoriesl:={l ,l ,··· ,l },andtheoutra-sessioncontextco.
1 2 Nl
• High-LevelActionah: embeddingofthefavorablerecommendedsessionl∈RL.
• High-LevelRewardrh: theuserCTRresponsew.r.tthecurrentsession,i.e.,(cid:80) c/K.
• Low-LevelStatesl: pastitem-selectiondecisionswithinasessioninference. Itincludesthe
useron-devicefeaturem,theintra-sessioncontextci,thecurrentselecteditemsequence
{e ,e ,··· ,e },andalsotheuserprofileu.
1 2 k
• Low-LevelActional: thestudieduserlocal,preferenceeˆ ∈RL,asanindicatorforitem
k
selection.
• Low-LevelRewardrl:theintrinsicmotivationreceivedbyLRA.Hereweembracethegoal-
conditionedframeworkofHRLVezhnevetsetal.[2017],Nachumetal.[2018],Nasiriany
etal.[2019],i.e.,thenearestactionofHRAstudiesthecurrentuserpreferenceandtherefore
canbeusedtocalculatethegoalofLRAaction. Forinstance,theLRArewardrl canbe
simplyformulatedastheEllucidiandistancebetweentheHRAactionandLRAaction.
ObjectivesofHRAandLRAarethenformulatedas
K
(cid:88)∞ (cid:88)
Jh(ah)= γtrh, Jl(al)= γkrl (8)
t k
t=0 k=0
inwhichthedefinitionoftheLRArewardconnectstheabovetwoequations,i.e.,rl(ah,al).
4 Method
ThissectionintroducesthetrainingmechanismsofmccHRL,withTable1summarizingimportant
symbolsfrequentlyusedinthepaper. Weinheritethefamousactor-criticRLarchitecture. However,
herewelettheHRAtobethecriticandtheLRAtobetheactorsolely,toreducethecomputational
complexity.
4.1 SessionRecurrentEncoder
The session recurrent encoder (SRE) is an incremental function that aims to provide a session
embeddingwitharbitrarysessionlengthandcanbeappliedbothinthelearningandplanningstages.
HereweutilizethefamousGRUstructure,withe astheinput:
k
l =SRE(l ,e ), k =0,··· ,K−1 (9)
k+1 k k
theencodedsessionembeddingl isthelatentstateofGRUwhichhasthesamedimensionofe .
k k
4.2 High-LevelAgent
The HRA performs as the critic in our mccHRL. It is modeled with a parametric value function
Q (sh,ah). TheleftpartofFigure3showsthecriticnetworkstructure. Historicalclicksequences
w
{e} are extracted from the cloud database, each encoded by SRE and then processed by a DIN
structureZhouetal.[2018]withthelastlasthetargettensor. TheDINoutputconcatenateswithu
andcoandgoesintofirstanFMlayerfirstthenanMLP.ThelastactivationfunctionofMLPistanh
whichgeneratestheQ-value. Atthebeginningofasessionservice,HRAalsotransmitsu,co,and
SREtoLRA,asindicatedbyFigure3. TheoutputofQ thenprovidesanimplicitfunctionofLRA
w
goalsg,asindicatedbyArrowIII.
7Table1: Notations.
Notation Description
t therealphysicaltime
k theposition&decisionstepwithintherecommendeditemlist
K themaximumlengthofrecommendeditemlist
S,s statespaceS,s∈S
A,a actionspaceA,a∈A
R,r rewardspaceR,r ∈R
γ discountfactortobalanceimmediateandfuturerewards
J discountedcumulativereturn
T thetransition(s ,a ,r ,s )
t t t t+1
M MDPformedby(S,A,R,γ)
π (a|s) policyfunction,parameterizedbyθ
θ
Q (s,a) state-actionvaluefunction,parameterizedbyw
w
c∈[0,1] binaryresponseofuserclick
L embeddingdimension
u∈RL embeddingofuserfeature
co ∈RL embeddingofoutra-sessioncontext
ci ∈RL embeddingofintra-sessioncontext
m∈RL embeddingofon-deviceuserprofile
e∈RL embeddingofitemfeature
l∈RL embeddingofsession(sequenceofitemswithinthesession)
SRE encodesanylengthofitemsequencetoRL
g thestudieduserlong-termpreferencebyHRA,thegoalofLRA
eˆ thestudiedusershort-termpreferencebyLRA
kˆ indexoftheoptimalitemtoexhibitatthenextposition
R thereplaymemorybuffer
N thebatchsizeofoff-policysampling
η thelearningrate
τ thelearningweightoftargetcriticandactor
α theweightingparameterofCQLcorrectionterm
4.3 Low-LevelAgent
Figure3showstheactor-networkstructureontheright,whichimplementsourLRApolicyfunc-
tion π (al)|sl)). Since the size of the edge model is significantly limited by the on-device hard-
θ
ware, we design a relatively simple network structure. The local intra-session exposed item
sequence {e ,e ,··· ,e } is again encoded by SRE, while the on-device user profile sequence
0 1 k
{m ,m ,··· ,m }isencodedbyanotherGRUunit. ConcatenationofthelastlatentstatesofSRE
0 1 k
andGRUformstheintra-sessioncontextci. Wefurtherconcatenateu,coandcitogetherandfeed
intoaMLPwhichgenerateseˆ ∈RL,asal. Duringthetrainingstage,gradientback-propagationof
k k
SREintheactorisblockedtoreducethecomputationalcomplexity.
Duringtheservicestage,eˆ isdotmultipliedwiththeembeddingsofunselecteditemsandgenerates
k
scalarscores,thengreedilyselectstheoptimalitem:
kˆ =argmax{eˆ eT}, k =k+1,··· ,K (10)
k k′ ′ ′
inwhichK istheremaineditemcandidatesetsize(andcanbeassumedtobelargerthanK). The
′
kˆthitemisappendedtotheendofrecommendation,thenthestepmovesforwardfromktok+1
untilallK itemsaredetermined.
4.4 Mechanism
Figure2reviewstheaforementionedinteractivepatternofHRAandLRA.HRAoperatesatthereal
worldtimet,observesthecurrentuserstatesh,andprovidestheuserfavorablepreferencebythe
t
High-Levelactionahonceasessionserviceisrequested. TheHRAtrainingisexecutedonthecloud
t
sidebasedonallusers’logs,andpolicyupdatesoncethesessionendswiththereal,non-delayed
8Critic Service
MLP
Unselected
items
Actor eˆ
FM k
III
concat MLP
Greedy
DIN Selection
concat ci
k
l concat GRU
I
uc SRE uc x SRE
o o
II
e
Click Seq Exposed Seq
m
slow state transit fast state transit
Figure3: TheNetworkStructureaswellasDataFlows. ArrowI:embeddingvectorsdownloaded
fromCritictoActoratthebeginningofthesession;ArrowII:userclickbehaviorsuploadedwith
latency;ArrowIII:HRAactioncalledforintrinsicmotivationofLRA.
session-wiseuserresponserh. Withineachsession’sservicing,LRAaimstodeterminetheranked
t
listofK items,thereforeoperatingatthevirtualtimestepsk =0,1,··· ,K onthedeviceside.
4.4.1 TrainingwithGenericEnvironment
Algorithm1showsthetrainingmechanismofourmccHRL.Consideringthelistwisemechanism,
WeassigntheLow-Levelactoranon-policystrategyandtheHigh-Levelcriticanoff-policystrategy.
SimilartoDDPG,wealsohaveatargetcriticandatargetactor.
4.4.2 TrainingwithOfflineData
Algorithm1isaninteractivelearningmethodwithbothcloudandedgeenvironments. Unfortunately,
itisoftennotpracticalfordirectonlinedeploymentandlearningonlarge-scaleindustrialsystems,
especiallyforonlinelearningonusers’mobiledevices. Therefore,itisofgreatimportancetodevelop
analgorithmthatcanlearnfromtheofflinedata,oratleastusetheofflinetrainingasawarmupstart.
Inthispaper,weassumeofflinedatacanberetrievedandreformulatedinthetemporalorder,with
eachlineintheformof[sh,ah,rh,{e },{m },sh ,ah ].
t t t k k t+1 t+1
RLstudyonsuchdatacallsforaspecialresearchdirectioncalledofflineRL,orbatch-constrained
RL,whichlearnsanoptimalpolicyfromafixeddatasetbyimplementingtheout-of-distribution
correctionsforoff-policyRLLevineetal.[2020].SinceourHRAhasanoff-policystrategy,wecould
employsimilarideastotraintheHRAcriticoffline. HerewerefertotheconservativeQ-learning
(CQL)Kumaretal.[2020],tohavearegularizerintheTDlossoftheQ-function:
expQ (s,a)
w
L (s,a,π )=E [Q (s,a) ]
off θ a ∼πθ w
Z
−E Q (s,a) (11)
a ∼πβ w
inwhichZ isthenormalizingfactorofQandπ isthesamplingpolicyindata. Thisformulation
β
indicatesthatwealsominimizethesoft-maximumofQatthecurrentsolvedpolicy(π (a|s))w.r.t
θ
thesamplingpolicyah,thereforethelearningbecomes’conservative’. AccordingtoAlgorithm1in
Kumaretal.[2020],suchatrickcanbealsoappliedintheactor-criticframework. Herewepresent
ourofflineversionofAlgorithm1whichisspecifiedinAlgorithm2.
9Algorithm1ThemccHRLAlgorithm.
1: Initializetheclockt←0
2: Initializeparametersw ←0,θ ←0,andw ←w,θ ←θ
′ ′
3: Initializeallusers’historiesl={}
4: Initializetheon-cloudreplaymemorybufferR ={}
5: foreachsessiondo
//DataTransmissionStage1,High-LeveltoLow-Level:
6: Observeu ,coandsh =[u ,co,l]
t t t t t
7: Initializek ←0,locatethemobiledevice
8: Synchronizeu ,co,andcandidatesefromcloud
t t
9: Synchronizeθ ←θ fromcloud
′
//TheLow-LevelTrainingStage:
10: whilek <K do
11: Initializetherankeditemliste={}
12: Collecttheinstantm andrecordsl =[u,co,m ,e]
k k k
13: Getal basedonthecurrentpolicyπ (al|sl)
k θ k k
14: SelectitemkˆfromEq. (10)andappendewithe
kˆ
15: UpdateθbasedonDDPG
16: endwhile
//DataTransmissionStage2,Low-LeveltoHigh-Level:
17: Withsomelatency:
18: Updatethetargetactorθ ←τθ+(1−τ)θ
′ ′
19: Collectc,calculaterh,andappendlwithe.
t
20: ObservethenextHRAstatesh
t+1
21: StoretransitionT =(sh,ah,rh,sh )inR
t t t t t+1
//TheHigh-LevelTrainingStage:
22: ifsize(R)>N then
23: SampleN transitionsT fromR
i
24: fori=1,··· ,N do
25: UpdatewbasedonDDPG
26: Updatethetargetcriticw ←τw+(1−τ)w
′ ′
27: endif
28: endfor
5 Experiments
Asanon-policyRL-basedmethodology,ourmccHRLshouldbeevaluatedbyonlineexperiments
andbusinessperformanceindicatorssuchasCTR.However,theindustrialrecommendationsystem
is complex and connected with enormous impressions and incomes, which makes thrugh online
experimentdifficult. Asaremedy,wedesigntwoofflineevaluationstrategies. Wedemonstratethe
effectivenessofmccHRLbybothofflineandliveexperiments,aswellasablationandsensitivity
studies.
5.1 ExperimentalEnvironments
InordertoevaluateourRL-basedmethod,ideallyweneedasimulatorwithbothcloudandedge
environments, which are interactively learnable by RL agents. Unfortunately, to the best of our
knowledge, there is no such test-purpose cloud-edge request simulator. Instead, we provide two
remedysolutionsforofflineexperiments,beforetheonlinedeployment:
1. AnRSsimulatorthatisgeneratedbyasmall-scalepublicdataset. Wearbitrarilydefine
somespatialandtemporalrelatedfeaturesas‘edgefeatures’whichisnotobservablebythe
cloud. Otherfeaturesaremanuallyimplementedwithsometransmissionlatencybetween
theedgeandthecloud. Theaveragerewardduringtheteststagecanbeusedasametricfor
suchasimulation-basedsystem.
10Algorithm2TheofflinemccHRLAlgorithm.
1: Initializeparametersw ←0,θ ←0
2: InitializethereplaymemorybufferR ={}
3: foreachlineofdatado
4: Retrievesh,ah,rh,{e },{m },sh ,ah fromdata
t t t k k t+1 t+1
5: StoretransitionT =(sh,ah,rh,sh ,ah )inR
t t t t t+1 t+1
//TheCloudTrainingStage:
6: ifsize(R)>N then
7: SampleN transitionsT fromR
i
8: fori=1,··· ,N do
9: Sety =rh+γQ (sh ,ah )
i i w i+1 i+1
10: CalculateLbasedoninEq. (3)
11: CalculateL basedoninEq. (11)
off
12: UpdatewbyminL+L
off
13: endif
//TheEdgeTrainingStage:
14: fork =0,··· ,K do
15: Initializetherankeditemliste={}
16: Observem andsl =[u ,co,m ,e]
k k t t k
17: Getal basedonthecurrentpolicyπ (al|sl)
k θ k k
18: Observethesamplediteme andappendittoe
k
19: Updateθby▽ Q (sh,al)▽ π (sl)
a w t k θ θ k
20: endfor
21: endfor
2. Experimentbasedonalarge-scaleindustrialdataset. TheofflineversionofAlgorithm1
canbeemployedonsuchanofflineRL(ordataset-basedRL)problem,withsomeoffline
accuracymetricstobeevaluated.
Weemploybothtwosolutionstoevaluateourmethodology. Implementationdetailsareintroducedin
thesubsequentsubsections. Table2exhibitsthestatisticsoftwoexperimentalenvironments.
5.1.1 Simulator-basedExperiment.
Movielens3 isauser-movieratingdatasetincludingthebehaviordata,theuserfeature,anditem
featuredata. Samplesarelabeledwithusers’ratingsonmoviesfrom1to5.
BasedontheMovieLens-100kdataset,Webuildasimulatorbasedonthemethodologyintroducedin
Zhaoetal.[2019],withtheaveragedratingsbehavingastheRLrewards. Theaverageratingis3.53
overallsamples. Amongtheuserfeatures,thespatialfeaturesincluding‘occupation’and‘zipcode’
aresetasedgefeatures,whichcannotbeaccessedbythecloud. Wefurtherassumetheusercloud
sequencehasafixedlatencywithedgesequence,i.e.,thecloudsequenceisdelayedby6itemsthe
edgesequence. Bytheseconfigurations,webuildanapproximatedmobile-cloudsimulator,withan
explicittransmissionlatency.
5.1.2 Dataset-basedExperiment.
Becauseoftheindustriallimitations,interactivetrainingofon-deviceRListemporarilyimpractical.
Instead, we conduct an industrial-scale experiment based on the offline dataset with realistic on-
devicefeatures. Hereweemployacontent-basedfeedrecommendationdataset4,samplingfrom
theTaobaofront-pagecontent-baserecommendationandhasbeendeclaredopen-sourcedinTianchi.
On-device features are aligned with the recommended item and corresponding click labels, and
the data is reorganized into a session-wise form. The recommendation objective is to maximize
theclick-throughrate(CTR).Wedenoteuserapp-relatedbehaviorssampledonthedeviceasm,
includingappIDs,stay-times,andLBSinformationinthegranularityofdistricts. Thespatiotemporal
informationincoincludingPOI,province,city,day,hour,andworkday/weekendlabels. Wefurther
3https://grouplens.org/datasets/movielens/
4https://tianchi.aliyun.com/dataset/dataDetail?dataId=109858
11assumetheusercloudsequencehasafixedlatencywithedgesequence,i.e.,thecloudsequenceis
delayedby10itemstheedgesequence.
Table2: Statisticsofdatasets.
Dataset set #impression #user #item CTR
All 100000
rate=1 6110
rate=2 11370 943 1602 -
Movielens
rate=3 27145
rate=4 34174
rate=5 21201
All 265003 4.92%
Alibaba day=3∼9 226508 34708 8 4.85%
day=10 38495 5.37%
5.2 EvaluationMetrics
Wedefinedifferentevaluationmetricsaccordingtothecharacteristicsoftwoexperimentalenviron-
ments:
• Simulator-basedExperiment.: theexpectedrewardindicatesthepolicyperformanceinan
interactive environment. The rating employed for evaluation is retrieved from the most
similar occurrence (with a cos similarity by user and item embeddings) in the dataset.
Such test stage is repeated for 50 rounds and the final averaged metric is reported. We
evaluateitbytheaverageratingduringtheteststageoftheexperiment,whichisdenotedby
S-<rating>.
• Dataset-basedExperiment.: thegeneralsolutionistoconverttheonlineCTRmaximization
intoanofflinebinaryclassificationproblem,i.e. predictiftheuserclicksornot. Weuse
AreaUndertheROCCurve(AUC)toevaluatethisclassificationperformance,inwhichwe
employtheactor’sservicingscoreasthepredictionandtheground-truthclickrecordasthe
label. WenameitasD-AUC.
5.3 ImplementationDetails
Table3showssomehyper-parametersoftwoexperiments. Furtherimplementationdetailsarelisted
below.
Table3: ParameterSettings.
Hyper-parameter Simulator-basedExperiment Dataset-basedExperiment
K 4 6
γ 0.99 0.9
L 128 16
N 64 512
η 0.001(HRA),0.0001(LRA) 0.0001
τ 0.001 0.001
Nl 12 50
α - 0.1
5.3.1 Simulator-basedExperiment.
MLPincritichashiddenunitsof[32,16,1]withthelastactivationastanh; whileMLPinactor
hashiddenunitsof[128,K].Thelatentstatedimensionis16forbothGRUandSREcomponents.
WeuseanAdamoptimizertointeractivelytrainthecriticandactor,withalearningrateof0.0001.
Performanceisevaluatedbyiteratingallusersamongthetrainingset,whileforeachgroundtruthitem,
12Table4: OfflineExperimentResults
Experiment S-<rating> D-AUC
random 3.583±0.034 -
DIN 3.767±0.045 0.781±0.006
DIN(ideal) - 0.832±0.008
GRU4rec 3.624±0.038 0.731±0.007
LIRD 3.786±0.051 0.631±0.007
MC2-SF 3.733±0.050 0.769±0.010
mccHRL 3.824±0.049 0.853±0.009
mccHRL(woedge) 3.746±0.046 0.787±0.009
mccHRL(woactor) 3.770±0.045 0.707±0.008
mccHRL(wocritic) 3.587±0.048 0.676±0.008
thepolicyselectstop-K itemsbasedonscores. BecausethesizeofMovieLens-100kisrelatively
small,weomittheDINstructureheretoavoidoverfitting
5.3.2 Dataset-basedExperiment.
TheDINlayerofcriticusesamulti-headtargetattentionstructure,withthelastsessionembedding
vectorasthetarget. Theattentionmodeiscosine. ThedeepFMlayerofcriticiscomposedoflinear
termsand2nd-ordercrossterms. MLPincritichashiddenunitsof[128,64, 32,1]withthelast
activationastanh; whileMLPinactorhashiddenunitsof[64, 32, K]withallactivationasrelu.
Thelatentstatedimensionis16forbothGRUandSREcomponents. WeusetheAdamoptimizer
withalearningrateof0.0001,1st-regularizationweightof0.0001and2nd-regularizationweightof
0.00001. Dataonthelastday(day=10)isclassifiedintothetestsetwhiletheotherconsistsofthe
trainingset. Tohelpthetrainingconverge,wefirstconduct3epochsofsupervisedlearning,withthe
binarycross-entropylossoftheclicklabel. RLthenconducts2moreepochs. Thetrainingendswith
convergence,costingabout17000steps. Trainingisruuningonanindustrial-scale,paralleltraining
platform,witheachexperimentallocatedwith8GPUsand20000MRAM.
5.4 Baselines
ThefollowingexperimentalbaselinesareemployedtocomparewithourmccHRL:
• Random: thepolicypickstherecommendeditemfromthecandidatessetrandomly. Only
implementedinthesimulatorexperiment.
• DIN:theDeepInterestNetworkZhouetal.[2018],asastandardizedsolutionofindustrial
pointwiseCTRmodel.
• DIN(ideal): theDINmodel(whichisdeployedonthecloud)canhavefullaccesstoedge
featureswithexactzerolatency. Therefore,itindicatesanunrealisticperformanceupper
boundforagenericsupervisedmodel. Onlyimplementedintheindustrialexperiment.
• GRU4rec: asession-basedmethodwithalatentstatecalculatedbyGRUHidasietal.[2016],
thereforeconsiderstheintra-sessioncontext.
• LIRD:aDDPG-basedRLlistwiserankingframeworkZhaoetal.[2019].
• MC2-SF:aslow-fasttrainingframeworkbasedonmoble-cloudcollaborationChenetal.
[2021],withmobilefeaturesincorporatedandconsidered.
5.5 OfflineResults
WecomparetheofflineexperimentalperformanceofmccHRLandbaselinesinTable4. mccHRL
outperformsotherbaselinesinbothexperiments. Specifically,theimprovementofmccHRLonthe
industrialexperimentismoreevident,sincethisexperimenthassubstantialon-edgefeatures,and
userbehaviorsaremoreresponsibleforsession-wiseexhibitionandindustrialcomputationallatency.
Comparisonwithbaselinescouldhighlightmoreinformation. Forexample,LIRDisthesecond-best
methodinthesimulatorexperimentsinceitsRLnaturefitstheinteractiveenvironment. However,it
13doesnotperformwellintheindustrialexperiment,probablybecausethatitdoesnothavetheoffline
trainingcorrection. GRU4recstudiestheintra-sessioncontextbyaGRUunitbutisnotenoughfor
ourindustrialexperimentinwhichtheoutra-sessioncontextisalsoimportant. MC2-SFcouldprovide
reasonableperformancewithconsiderationofmobilefeaturesbutlacktheRLlogiccomparedwith
ourmccHRL.Finally,DIN(ideal)hasasimilarperformancewithmccHRL,indicatingthataSOTA,
supervisedlearnedmodelcouldalsoperformwellwithimmediateaccesstoedgefeatures,aswellas
noedge-cloudtransmissionlatency. However,thisassumptionisobviouslyimpracticalinthereal
world.
5.6 AblationStudy
Comparedwithpreviouslypublishedworks,ourmccHRLhasseveralnovelcomponentstherefore
theirablationstudiesarenecessary. Table4alsoillustratestheresultsofthefollowingattempts:
• mccHRL(woedge): themccHRLapproachwithonlycloud-basedfeatures.
• mccHRL(wo actor): the mccHRL but without the actor. We simply use argmaxQ to
determinethefavorableitemselection.
• mccHRL(wocritic): themccHRLbutwithoutthecritic. Anitemselectionpolicyistrained
offlineandisdeployedonthedeviceforservice.
Notsurprisingly,mccHRLstillhasthebestperformance,suggestingthatthecritic,theactor,and
edge-basedfeaturesareallcrucial. NotethatmccHRL(wocritic)hastheworstperformancesinceit
suffersfromthesparserewardproblem.
5.7 SensitivityStudy
Thereisalwaysatrade-offbetweenperformanceandspeedfortheedge-basedmodel,duetolimited
resourcesontheedgeside. Tohaveaclearpicture,wechooseanimportantfactor,theuserbehavior
sequencelengthkeptbytheedgemodel,tohavesensitivityanalysis. Weinspectitsimpactfromthe
followingtwoaspects:
1. The latency between cloud and edge sequence update. Since the detailed value of time
latency is highly subject to the cloud and edge real-time conditions, here we pick an
approximatenumber,thenumberofitemssuchthatcloudisbehindtheedgesequence. With
thisnumberlarger,LRAcouldmaintainmoreinformationbutwithamorecomputational
cost.
2. ThelengthofbehaviorsequencestudiedbyslinLRA.Ifthisnumberislarger,LRAcould
providemorehistoricalinformationtopolicybutthemodelsizeisexpectedtoincrease.
Figure 4 shows the sensitivity results of the above two factors. Not surprisingly, the model per-
formance deteriorates as latency becomes larger according to Figure 4 (Left), indicating that the
edgemodelmemoryisapositivefactor. However,improvingthemodelmemorywillchallengethe
deploymentabilityonthedevice,asindicatedbyFigure4(Right). Basedonourpracticalexperience,
theedgemodelshouldgenerallybenobiggerthan3.5MBtoavoidunreasonablemobileoverhead,
whichindicatesourcurrentchoiceofNl =50isalmostoptimal.
5.8 LiveExperiment
Wedeployouralgorithmonaworld-leadingmobileapplicationandexecuteanA/Blivetest. The
experiment lasts a week. We randomly select a pool of users from different geological areas to
conductourmobile-baseddeployment. Comparedwiththelivebaseline,weobservetheglobalCTR
increasedby1.2%andthetotalviewsincreasedby1.5%.
6 DiscussionandLimitations
This work is motivated by decoupling the two time scales of the listwise recommendation: the
physicaltimestep(foruserpreferenceupdate)andthevirtualstep(fortherankeditemselection).
Ourmethodologyadoptsthefast-slowsystembylearningtheabovetwomechanismssimultaneously
14Figure4: SensitivityplotsoftheSimulatorexperiment(blue)andtheIndustryexperiment(red). Left:
Plotofperformancemetricswithdifferentcloudsequencelatency. Right: theedgemodelsizeasa
functionofsequencelength.
bytheHRLframework. Wealsotakeadvantageoftheedgecomputingtechniquebyimplementthe
low-levelagentonthemobiledeviceside,inwhichthelocalizedandSTAuserinformationcanbe
considerandfeedbackbeforeuploadingtothecloud. Therefore,ourmethodprovidesthetheoretical
optimalsolutionforsuchrecommendingscenarios.
However,onepotentialdrawbackistheextracomputationalburdenbroughtbythemccHRLframe-
work. Hereweperformasimpletimecomplexityanalysisforabriefdiscussion. Giventhephysical
timestepsasT andthevirtualtimestepsasK (thesessionlength),inferenceofagenericpointwise
recommendation model requires O(K ∗T), in which the model is called with the user and item
pairindependently. InourmccHRL,thetimecomplexitywouldbeO(K2∗T)sinceweneedto
calculatethesessioncontextualinformationrecurrently. WhenK istoolarge(e.g. largerthan100),
mccHRLmightbeinefficientandimpractical. Fortunately,intheindustrialimplementation,theuser
requestisusuallydividedintosequentialpagerequests. ThissolutionreducestheactualK therefore
couldalleviatetheefficiencyissue. Themobile-devicetransmissioncouldalsobeaproblemwhichis
alreadydiscussedinSection5.7.
Anotherimportantpotentialimprovementistoconsidertheuser’sinstantresponsewithinasessionand
makeimmediateadjustmentsforrankingresults. Suchconsiderationswilladdsomeuser-interactive
rewardsforLow-LevelRL,andourmethodthenbecomesaunifiedframeworkofcloud-basedranking
andmobile-basedreranking,whichisapromisingfuturedirection.
157 Conclusion
Inthispaper,webuildaunifiedframeworktodealwiththeuserpreferencemodeandthelistwise
item ranking simultaneously, by a novel hierarchical RL methodology. We reinforce the state
Markovnessbyedgefeaturesconsiderationanddesignamobile-cloudcollaborationmechanism. The
goal-conditionalmechanismisutilizedtosynchronizetheuserpreferencesolvedbyhigh-levelRL
tothelow-levelRL.Theoutra-andintra-sessioncontextsaredecoupledandstudiedintwotime
scales. Weimplementbothasimulatorandanofflineindustrialdatasettoverifyourmethodology,
beforetheformalliveexperiment. OurstudyshedsomelightsontheapplicationsofHRLonlistwise
recommendation,andfuturestudiescanbedevelopedtofurtherimplementRLinteractivelyinlarge
industrialenvironments.
References
Luis Anidorifón, Juan Santosgago, Manuel Caeirorodríguez, Manuel Fernándeziglesias, Rubén
Míguezpérez,AgustinCañasrodríguez,VictorAlonsororís,JavierGarcíaalonso,RobertoPérezro-
dríguez,andMiguelGómezcarballa. Recommendersystems. CommunicationsoftheAcm,40(3):
56–58,2015.
Pierre-LucBacon,JeanHarb,andDoinaPrecup. Theoption-criticarchitecture. InProceedingsof
thethirtiethAAAIConferenceonArtificialIntelligence,AAAI’17,2017.
Hi-Yong Chen, Yang Yu, Qing Da, J. Tan, Hai-Kuan Huang, and Hai-Hong Tang. Stabilizing
reinforcementlearningindynamicenvironmentwithapplicationtoonlinerecommendation. In
The24thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining(KDD’18),2018.
MinminChen,AlexBeutel,PaulCovington,SagarJain,FrancoisBelletti,andEdH.Chi. Top-k
off-policycorrectionforareinforcerecommendersystem. InProceedingsoftheTwelfthACM
InternationalConferenceonWebSearchandDataMining,WSDM’19,page456–464,2019.
ZeyuanChen,JiangchaoYao,FengWang,KunyangJia,BoHan,WeiZhang,andHongxiaYang.
Mc2-sf: Slow-fastlearningformobile-cloudcollaborativerecommendation,2021.
CarlosFlorensa,YanDuan,andPieterAbbeel. Stochasticneuralnetworksforhierarchicalreinforce-
mentlearning. InProceedingsofthe5th.InternationalConferenceonLearningRepresentation,
ICLR’17,2017.
YingqiangGe,ShuchangLiu,RuoyuanGao,YikunXian,YunqiLi,andXiangyuZhao. Towards
long-termfairnessinrecommendation. InProceedingsofthe14thACMInternationalConference
onWebSearchandDataMining,WSDM’21,2021.
YuGong, YuZhu, LuDuan, QingwenLiu, ZiyuGuan, FeiSun, WenwuOu, andKennyQ.Zhu.
Exact-krecommendationviamaximalcliqueoptimization. InThe25thACMSIGKDDConference
onKnowledgeDiscoveryandDataMining(KDD’19),2019.
YuGong,ZiwenJiang,YufeiFeng,BinbinHu,KaiqiZhao,QingwenLiu,andWenwuOu. Edgerec:
Recommendationsystemonedgeinmobiletaobao.InProceedingsofthe2020ACMonConference
onInformationandKnowledgeManagement,CIKM’20,2020.
B.Hidasi,A.Karatzoglou,L.Baltrunas,andD.Tikk. Session-basedrecommendationwithrecurrent
neuralnetworks. InProceedingsofthe4th.InternationalConferenceonLearningRepresentation,
ICLR’16,2016.
EugeneIe,VihanJain,JingWang,SanmitNarvekar,RiteshAgarwal,RuiWu,Heng-TzeCheng,
TusharChandra,andCraigBoutilier. Slateq: Atractabledecompositionforreinforcementlearning
withrecommendationsets. InProceedingsoftheTwenty-EighthInternationalJointConferenceon
ArtificialIntelligence,IJCAI’19,2019.
TejasDKulkarni,KarthikNarasimhan,ArdavanSaeedi,andJoshTenenbaum. Hierarchicaldeep
reinforcementlearning: Integratingtemporalabstractionandintrinsicmotivation. InProceedings
ofthe29thInternationalConferenceonNeuralInformationProcessingSystems,NIPS’16,2016.
16AviralKumar,AurickZhou,GeorgeTucker,andSergeyLevine. Conservativeq-learningforoffline
reinforcementlearning.InProceedingsofthe33thInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS’20,2020.
SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning: Tutorial,
review,andperspectivesonopenproblems,2020.
AndrewLevy,GeorgeKonidaris,RobertPlatt,andKateSaenko. Learningmulti-levelhierarchies
withhindsight. InProceedingsofthe7th.InternationalConferenceonLearningRepresentation,
ICLR’19,2019.
Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. Geography-aware sequential
locationrecommendation. InProceedingsofthe26thACMSIGKDDInternationalConferenceon
KnowledgeDiscovery&DataMining,2020.
TimothyP.Lillicrap,JonathanJ.Hunt,AlexanderPritzel,NicolasHeess,TomErez,YuvalTassa,
DavidSilver,andDaanWierstra. Continuouscontrolwithdeepreinforcementlearning. CoRR,
abs/1509.02971,2016.
YanLu,YuanchaoShu,XuTan,YunxinLiu,MengyuZhou,QiChen,andDanPei. Collaborative
learningbetweencloudandenddevices: anempiricalstudyonlocationprediction. InSEC,2019.
Yingtao Luo, Qiang Liu, and Zhaocheng Liu. Stan: Spatio-temporal attention network for next
locationrecommendation. InInProceedingsoftheInternationalWorldWideWebConference,
WWW’21,2021.
KanikaMadan,RosemaryNanKe,AnirudhGoyal,BernhardScholkopf,andYoshuaBengio. Fast
andslowlearningofrecurrentindependentmechanisms. InProceedingsofthe9th.International
ConferenceonLearningRepresentation,ICLR’21,2021.
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical rein-
forcementlearning. InProceedingsofthe31thInternationalConferenceonNeuralInformation
ProcessingSystems,NIPS’18,2018.
SoroushNasiriany,VitchyrH.Pong,StevenLin,andSergeyLevine. Planningwithgoal-conditioned
policies. InProceedingsofthe32thInternationalConferenceonNeuralInformationProcessing
Systems,NIPS’19,2019.
RyuichiTakanobu,TaoZhuang,MinlieHuang,JunFeng,HaihongTang,andBoZheng. Aggregating
e-commercesearchresultsfromheterogeneoussourcesviahierarchicalreinforcementlearning. In
InProceedingsoftheInternationalWorldWideWebConference,WWW’19,2019.
AlexanderSahsaVezhnevets,SimonOsindero,TomSchaul,NicolasHeess,MaxJaderberg,David
Silver,andKorayKavukcuogglu. Feualnetworksforhierarchicalreinforcementlearning. InPro-
ceedingsoftheWorkshoponNegativeDependenceinMachineLearningatthe34thInternational
ConferenceonMachineLearning,ICML’17,2017.
YananWang,YongGe,LiLi,RuiChen,andTongXu. M3rec: Anofflinemeta-levelmodel-based
reinforcement learning approach for cold-start recommendation. In Proceedings of the 33th
InternationalConferenceonNeuralInformationProcessingSystems,NIPS’20,2020.
ZengWei,JunXu,YanyanLan,JiafengGuo,andXueqiCheng. Reinforcementlearningtorankwith
markovdecisionprocess. InProceedingsofthe51thInternationalACMSIGIRConferenceon
ResearchandDevelopmentinInformationRetrieval,SIGIR’17,2017.
ChuhanWu,FangzhaoWu,TaoQi,andYongfengHuang. Feedrec: Newsfeedrecommendationwith
varioususerfeedbacks. InThe27thACMSIGKDDConferenceonKnowledgeDiscoveryandData
Mining,KDD’21,2021.
LongXia,JunXu,YanyanLan,JiafengGuo,WeiZeng,andXueqiCheng.Adaptingmarkovdecision
processforsearchresultdiversification. InProceedingsofthe51thInternationalACMSIGIR
ConferenceonResearchandDevelopmentinInformationRetrieval,SIGIR’17,2017.
17RuobingXie,ShaoliangZhang,RuiWang,FengXia,andLeyuLin. Hierarchicalreinforcement
learningforintegratedrecommendation. InProceedingsofthethirty-fifthAAAIConferenceon
ArtificialIntelligence,AAAI’21,2021.
JiangchaoYao,FengWang,KunyangJia,BoHan,JingrenZhou,andHongxiaYang. Device-cloud
collaborativelearningforrecommendation. InThe27thACMSIGKDDConferenceonKnowledge
DiscoveryandDataMining,KDD’21,2021.
JiangchaoYao,ShengyuZhang,YangYao,FengWang,JianxinMa,JianweiZhang,YunfeiChu,Luo
Ji,KunyangJia,TaoShen,AnpengWu,FengdaZhang,ZiqiTan,KunKuang,ChaoWu,FeiWu,
JingrenZhou,andHongxiaYang. Edge-cloudpolarizationandcollaboration: Acomprehensive
survey. IEEETransactionsonKnowledgeandDataEngineering,35(7):6866–6886,2022.
DongyangZhao,LiangZhang,BoZhang,LizhouZheng,andYongjunBaoanWeipengYan. Mahrl:
Multi-goalsabstractionbaseddeephierarchicalreinforcementlearningforrecommendations. In
Proceedingsofthe54thInternationalACMSIGIRConferenceonResearchandDevelopmentin
InformationRetrieval,SIGIR’20,2020.
XiangyuZhao,LongXia,L.Zhang,ZhuoyeDing,DaweiYin,andJiliangTang. Deepreinforce-
ment learning for page-wise recommendations. Proceedings of the 12th ACM Conference on
RecommenderSystems(RecSys’18),2018.
XiangyuZhao,LiangZhang,LongXia,ZhuoyeDing,DaweiYin,andJiliangTang. Deepreinforce-
mentlearningforlist-wiserecommendations. InDRL4KDD,2019.
Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and
Zhenhui Li. Drn: A deep reinforcement learning framework for news recommendation. In
Proceedingsofthe2018WorldWideWebConference,2018.
Fan Zhou, Ruiyang Yin, Kunpeng Zhang, Goce Trajcevski, Ting Zhong, and Jin Wu. Adversar-
ial point-of-interest recommendation. In In Proceedings of the International World Wide Web
Conference,WWW’19,2019a.
GuoruiZhou,ChengruSong,XiaoqiangZhu,YingFan,HanZhu,XiaoMa,YanghuiYan,JunqiJin,
HanLi,andKunGai. Deepinterestnetworkforclick-throughrateprediction. InThe24thACM
SIGKDDConferenceonKnowledgeDiscoveryandDataMining,KDD’18,2018.
ZhiZhou,XuChen,EnLi,LiekangZeng,KeLuo,andJunshanZhang. Edgeintelligence: Paving
thelastmileofartificialintelligencewithedgecomputing. ProceedingsoftheIEEE,2019b.
LixinZou,LongXia,ZhuoyeDing,JiaxingSong,WeidongLiu,andDaweiYin. Reinforcement
learning to optimize long-term user engagement in recommender systems. In The 25th ACM
SIGKDDConferenceonKnowledgeDiscoveryandDataMining(KDD’19),2019.
18