S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 1
Self-Evolving Depth-Supervised 3D
Gaussian Splatting from Rendered Stereo
Pairs
SadraSafadoust1* 1DepartmentofComputerEngineering
https://sadrasafa.github.io/ andKUISAICenter,KoçUniversity,
FabioTosi2 Istanbul,Turkey
https://fabiotosi92.github.io/ 2DepartmentofComputerScienceand
FatmaGüney1 Engineering(DISI),
UniversityofBologna,Italy
https://mysite.ku.edu.tr/fguney/
MatteoPoggi2
*projectstartedwhilevisitingUniversityofBologna
https://mattpoggi.github.io/
Abstract
3DGaussianSplatting(GS)significantlystrugglestoaccuratelyrepresenttheunder-
lying3Dscenegeometry,resultingininaccuraciesandfloatingartifactswhenrendering
depthmaps.Inthispaper,weaddressthislimitation,undertakingacomprehensiveanal-
ysisoftheintegrationofdepthpriorsthroughouttheoptimizationprocessofGaussian
primitives,andpresentanovelstrategyforthispurpose.Thislatterdynamicallyexploits
depthcuesfromareadilyavailablestereonetwork,processingvirtualstereopairsren-
deredbytheGSmodelitselfduringtrainingandachievingconsistentself-improvement
of the scene representation. Experimental results on three popular datasets, breaking
groundasthefirsttoassessdepthaccuracyforthesemodels,validateourfindings.
Projectpage:https://kuis-ai.github.io/StereoGS/
BlendedMVS ETH3D
Input Views Vanilla GS Ours Input Views Vanilla GS Ours
Figure 1: Self-Evolving Depth-Supervised 3D Gaussian Splatting (GS) in action. Our
strategyallowsGStoself-improveduringoptimization,andtorenderbetterdepthmaps.
1 Introduction
In recent years, NeRF [40] has deeply revolutionized several aspects of computer vision,
introducing innovative paradigms and redefining our understanding of the field. First and
©2024.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
4202
peS
11
]VC.sc[
1v65470.9042:viXra2 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
foremost,NeRFhasrepresentedaturningpointforimagerenderingandnovelviewsynthesis
[37,40,78],castingthesetasksastheoptimizationofacontinuous3Drepresentationofthe
sceneencodedinmulti-layerperceptrons(MLPs),queriedwith(x,y,z)coordinatesinspace
and(θ,φ)viewanglestopredictcolorandopacityforanygeneric3Dpoint. Renderingis
achievedbycastingraysandaccumulatingcolorsandopacitiesalongthemintopixels.
However,therecentadventof3DGaussianSplatting(GS)[29]israpidlyconqueringthe
mainstageattheexpenseofNeRF,duetothemuchlowertimerequiredforbothoptimization
andrendering.WhileGShascertainlyadvancedthestate-of-the-artintermsofphotorealism
andrenderingspeed,theunderlyinggeometrymodeledbytheGaussiansdoesnotreflectthe
same quality as the rendering. This is evident in the depth maps generated by GS itself –
byreplacingthecolorcomponentsoftheGaussianswiththeirpositionduringtherendering
process – as we can notice in Fig. 1, where examples from BlendedMVS and ETH3D are
affectedbyseveralfloatersandartifacts. Someworksfromtheliterature[2,13,51,63,68]
showevidencethatusingdepthpriorsasaformofadditionalsupervisionwhenoptimizing
a NeRF can improve the quality of the rendered images, especially when very few images
are available for training. Intuitively, this strategy also has the potential to improve the
underlyinggeometryencodedbytheNeRFitself,althoughnoattentionhasbeenpaidtothis
bypriorworks,nortothedifferent,possibleapproachesforretrievingdepthpriorsfromthe
verysameimagesusedtotrainNeRF–andtotheintrinsiclimitationeachchoicebrings.
In this paper, we first delve into a study of Depth-Supervised 3D Gaussian Splatting
(DS-GS) variants by examining and measuring the impact that different depth-from-image
solutions have on the optimization of both the appearance and geometry modeled by GS.
Specifically, we review four main strategies for extracting depth priors from the multiple
images involved in GS optimization: i) Structure-from-Motion (SfM) [55], ii) Monocular
Depth Estimation (MDE) [7], iii) Depth Completion (DC) [6], and iv) Multi-View Stereo
(MVS)[36]. Eachonehasitspeculiarstrengths,aswellasitsweaknesses. Tonameafew:
on the one hand, SfM and MVS suffer in the presence of sparse views where the overlap
between images is small; on the other hand, single-image approaches are free from this
constraint,yetassumeanetworkthatcangeneralizeproperlyacrossverydifferentscenarios.
Inadditiontothisexploration,weproposeanovelapproachtoimproveGSoptimization,
stillbyexploitingthesupervisionofdepthpriors,thistimeobtainedbyafifthstrategythat
is a cornerstone of computer vision, but not included in the previous list: stereo matching.
Indeed,wearguethatdespitetheinaccurateunderlyinggeometrymodeledbytheGaussians,
GScanstillrendergeometricallyconsistentimages–e.g.,rectifiedstereoimages,evenwhen
astereocameraisunavailable. Accordingly,GScanbeemployedtogenerateframes,which
canthenbeprocessedbyapre-traineddeepstereonetwork[34]toobtainthesupplementary
supervisionrequiredtoenhancetheunderlyinggeometryofGSitself.Thankstotheefficient
rasterizationprocessofGS,wecancarrythisoutdirectlyduringtheoptimization,deploying
anewGSframeworkcapableofself-evolving,supportedbythisexternalstereonetwork.
ExperimentsonETH3D[56],ScanNet++[73]andBlendedMVS[72]supportourclaims:
•Wecarryoutacomparisonbetweendifferentstrategiesforretrievingdepthpriorsfrom
images, by evaluating the impact of each on improving both the appearance and geometry
modeledbyDS-GSagainstvanillaGS.
•WeproposeanewSelf-EvolvingGSframework,capableofsupervisingitselfthrough
depthpriorsretrievedbyastereomatchingnetwork,processingtherectifiedimagesrendered
bytheGSitselfduringtraining.
• Compared to the use of depth priors from classical strategies, our approach renders
bothbetterimagesanddepthmapsinreal,sparseviewsettings.S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 3
2 Related Work
Inthissection,wepresentareviewoftheliteraturerelevanttoourstudy.
NovelViewSynthesis. Generatingrealisticnovelviewsfrom3Dscenerepresentations
has been an active area of research. Early approaches used geometries such as meshes
[26, 49, 50], planes[24], andpointclouds[69, 79]tomodelscenes. Morerecently, neural
radiancefields(NeRFs)[40],extensivelydiscussedin[11,17,45],haveemergedasahighly
effectiverepresentationforphotorealisticnovelviewsynthesis. NeRFmodelsthesceneas
an implicit, continuous radiance field, allowing fine-grained detail to be captured. Several
extensionstotheoriginalNeRFformulationhaveenhancedrenderingqualitybyimproving
anti-aliasing[3,4,5],modelingreflectancemoreaccurately[9,21,62],trainingwithsparse
views[31,43,68],andreducingcomputationaloverheadduringtraining[14,27,42,48,59]
andrenderingtime[18, 35, 75]. Inparallel, pointcloud-basedrepresentations[69, 74, 79]
have gained popularity due to their rendering efficiency. In addition, recent differentiable
pointsplattingtechniques,suchas3DGS[29],haveenabledstate-of-the-artreal-timescene
rendering. Amongothers,NeRFwasusedtogeneratestereoimagesfortrainingstereonet-
works[60]. Ourworkhasadifferentgoal–togeneratestereopairsandsuperviseGSitself.
Image-basedDepthEstimation. Traditionally,depthestimationfromimageshasrelied
on non-learning-based approaches. Among them, Structure-from-Motion [1, 20, 54] esti-
mates both the sparse 3D structure of a scene and the camera poses from a set of images,
with COLMAP [54] becoming a reference pipeline for the community in the last decade.
Whenposesareknownalready,Multi-ViewStereo[15,57]usedfeaturematchingandgeo-
metricconstraints–i.e., epipolargeometry–acrossmultipleviews, whilebinocularstereo
[23, 52] relied on correspondences between rectified stereo pairs. However, the field has
undergone a revolutionary transformation with the advent of deep learning. In the context
ofmulti-view[58,65,70]andbinocularstereo[22,44,61,81],learning-basedapproaches
have leveraged the capabilities of convolutional neural networks to extract robust feature
representationsformoreaccuratecorrespondenceestimation. Thisinfusionofdeeplearning
has not only increased accuracy, but has also facilitated the refinement and completion of
sparse/noisydepthmaps[25,25,30,67],effectivelyfillinginholesorrefininginaccuracies.
Withinthisparadigm,monoculardepthestimation[8,38,41,80]hasemergedasaspecial-
izedsubset,wheredeepneuralnetworksaretrainedtodirectlypredictdepthoutofasingle
image,typicallyonlargedatasetswherepseudo-groundtruthdepthisavailable[46,47].
RadianceFieldswithDepthPriors. Whilesuccessfulinimagerendering, challenges
arise in representing accurate scene geometries using advances in radiance fields. In re-
sponse,supervisedapproachesincorporatingdepthpriorshaverecentlyemerged. Asapre-
cursor,DS-NeRF[12]employsdepthsupervisionusingsparsepointcloudsfromCOLMAP
duringtraining.Inparallel,Rossleetal.[51]employdensedepthpriorsbydensifyingsparse
depthdataextractedfromCOLMAP.InNerfingMVS[66],instead,COLMAPextractssparse
depthpriors,subsequentlyutilizedtofine-tuneapretrainedmonoculardepthnetworkthatis
then employed to supervise volume sampling. PointNeRF [69] introduces an intermediate
stepusingfeaturepointcloudsanddemonstratesimprovedefficiencycomparedtothevanilla
NeRF. To the same end, CorresNeRF [32] uses adaptive correspondence generation, while
MonoSDF[76]improvesthereconstructionprocessbyincorporatingdepthandnormalcues
predictedbygeneral-purposemonocularestimators. Similarly, SparseNeRF[64]leverages
depth priors from real-world inaccurate observations, which can be from pre-trained depth
modelsorcoarsedepthmapsofconsumer-leveldepthsensors,whilesomeapproachesused
depthpriorswhendealingwithdynamicscenes[16,19]. Inaconcurrenteffort,Chungetal.4 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
[10]representstheonlyattempttoregularizeGSusingmonoculardepthnetworks.However,
ourpaperhighlightsthatbetterpriorscanbeexploitedtothisend.
3 Background Theory
3.1 3DGaussianSplatting(GS)
3DGaussianSplattingisagroundbreakingtechniqueinthedomainofexplicitradiancefields
andcomputergraphics. Thisuniqueapproachreliesontheincorporationofmillionsof3D
Gaussians,whichmarksashiftfromtheprevailingmethodsusedinneuralradiancefield.
Learning starts with multi-view images, by estimating camera poses and (optionally)
sparse3DpointstobootstrapGS,whichoptimizesasetG ={g ,g ,...,g }of3DGaus-
1 2 N
sians, where N is the number of Gaussians in the scene. Each Gaussian, denoted as g,
i
is characterized by a full 3D covariance matrix Σ ∈R3×3, center position µµµ ∈R3, opac-
i i
ity o ∈[0,1], and color c, which is represented by spherical harmonics (SH) for a view-
i i
dependentappearance. Backpropagationcanbeusedtolearnandoptimizealltheseproper-
ties. ThespatialinfluenceofasingleGaussianprimitivecanbeexpressedasfollows:
g i(x)=e− 21(x−µµµ i)⊤Σ− i1(x−µµµ i) (1)
Here,thespatialcovarianceΣdefinesanellipsoidasΣ=RSS⊤R⊤,whereS∈R3repre-
sentsthespatialscaleandR∈R3×3representstherotation,parameterizedbyaquaternion.
For rendering, GS operates similarly to NeRF but deviates significantly in the compu-
tation of blending coefficients. This involves the "splatting" of 3D Gaussian points onto a
2D image plane, as Σ′ =JWΣW⊤J⊤ and µµµ′ =JWµµµ. Then, pixel colorC is obtained by
merging3DGaussiansplatsthatoverlap,sortedbydepth:
i−1 (cid:18) 1 (cid:19)
C= ∑ ciαi∏(1−αj) with αi=o iexp − 2(x′−µµµ′ i)⊤Σ′ i−1(x′−µµµ′ i) (2)
i∈N j=1
The optimization process begins either from Structure-from-Motion (SfM) point clouds or
random3Dpoints. Then,StochasticGradientDescent(SGD)isemployed,withL1andD-
SSIM loss functions between real and rendered views. Analogously to color, the resulting
depthDcanbedeterminedbyreplacingc withd –i.e.,thedistanceofg fromthecamera.
i i i
3.2 DepthfromImages
Inthissection, wereviewestablishedmethodologiesforestimatingdepthfromimagestai-
loredforGSsetting–i.e.,multipleimagescapturedfromdifferentviewpointsusingasingle
camera–allowingtoimplementDepth-Supervised3DGaussianSplatting(DS-GS)variants.
Structure-from-Motion(SfM).Itaimsatreconstructing3Dstructureandcameraposi-
tionsfromasetofimages,startingwithtwo-viewtriangulation:
Xij∼τ(x˜i,x˜j,Pi,Pj) with i̸= j (3)
withX beingageneric3DpointvisiblefromimagesI,I ,x˜ ,x˜ itspixelcoordinatesonthe
ij i j i j
twoimages,P,P theircameraposes,andτ agenerictriangulationmethod. Usually,x˜ ,x˜
i j i j
pairsareidentifiedinadvancebyextractingfeaturesandmatchingthem. Eventually,global
optimizationiscarriedoutwithbundleadjustment,minimizingthereprojectionerrorES.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 5
RenderedImage RenderedDepth RenderedRight StereoDepth GTDepth
Figure 2: Depth priors retrieved from stereo. Vanilla GS produces noisy and inaccurate
depths,yetcanrenderstereopairsforgettingstrongdepthpriorsasadditionalsupervision.
E=∑||π(Pi,Xi)−xj||2
2
(4)
j
withπ beingtheprojectionfunctionfrom3Dtoimagespace. SfMalgorithms–COLMAP
[54]inparticular–arethefoundationforbootstrappingGSoptimization,providingboththe
cameraposesandthe3DpointsforinitializingtheGaussians.
DepthCompletion(DC).Thismethodaimstorecoveradensedepthmapfromasetof
sparsemeasurementsX,usuallyguidedbyacolorimageI,withanetworkΘ :
i i DC
D DC(Ii)=Θ DC(Ii,Xi) (5)
Inoursetting,aDCmodelcanprocessthesparsesetofpointsestimatedbyCOLMAPand
projected over the images, similarly to [51] in principle. From a practical perspective, the
availability of a DC model capable of generalizing across scenes and levels of sparsity is
crucialforthispurpose–althoughignoredin[51].
Multi-View Stereo (MVS). A dense depth map can be obtained by matching pixels
acrossmultiple,posedimagesalongepipolarlines. Thistaskisnowadaystackledwithdeep
networksaswell[71],withagenericmodelΘ processingareferenceimageI andaset
MVS i
ofN sourceviews,giventheirposes
D MVS(Ii)=Θ MVS(Ii,Pi,{Ij,Pj}N j) (6)
Despitetheoutstandingaccuracyreachedinthelastyears,MVSnetworksstillstrugglewhen
dealingwithsparseviewswithlimitedoverlap,andmaysufferfromgeneralizationissues.
Monocular Depth Estimation (MDE). With the rise of deep learning, estimating the
depthofasingleimagehasbecomeareality. Nowadays,state-of-the-artmodelsaretrained
overmillionsofimagestopredictaffine-invariantdepth[46,47]:
D MDE(Ii)=m·ΘMDE(Ii)+q (7)
wheremandqarerespectivelyscaleandshiftfactorsrequiredtorecovertheeffectivescale
withintherelativedepthmappredictedbythemodelΘ . Inourspecificsetting, mand
MDE
q can be directly derived by fitting the predicted depth map on the COLMAP depth points
through least squares. A concurrent work [10] also follows this strategy to regularize GS
optimization–yetwithoutmeasuringitsimpactontheunderlyinggeometry.6 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
Input View
Self-EvolvingDS-GS
RenderedView ℒ𝑑𝑒𝑝𝑡ℎ Stereo Network
SfM
Primitive RenderedStereo Pair
Initialization
Left Right
Uno Vb ies werved ℒ𝑐𝑜𝑙𝑜𝑟
Sparse Input Views
Input View Observed
View
Input View Input View Optimization
Figure 3: Self-Evolving DS-GS pipeline. As soon as GS can render stable images, we
renderstereopairs,estimatedepthwithapre-trainednetwork,anduseittocomputeL .
depth
4 Self-Evolving Depth-Supervised GS from Stereo
Inthissection,weintroduceanalternativestrategytoobtaindensedepthpriorsandimprove
the optimization process of GS. We begin with the empirical observation that, despite its
inaccurateunderlyinggeometry,GScanrenderimagesthatexhibitgeometricconsistency.
This means that a trained GS can render frames over which we can run conventional
depth-from-imagesalgorithmstoretrievequiteaccuratedepthpriorsforsupervisingtheGS
itself. Purposely,thesimpleststrategyconsistsofrenderingrectifiedstereopairs–i.e.,im-
agescapturedfromtwoviewpointsshiftedbyahorizontaloffset–andthenestimatingdepth
throughtriangulationfromdisparity. Fig.2showsqualitativeevidenceoftheeffectiveness
ofthisstrategy: whilevanillaGSrendersnoisydepthmaps,apre-trainedstereomodelcan
generatemuchbetterdepthmapsfromstereopairsrenderedbyvanillaGSitself.
Forthispurpose, givenanycameraposeP, wecanderiveacorrespondingrightview-
i
pointwith poseR ina fictitiousstereoconfiguration, accordingto anarbitrarybaseline b:
i
(cid:18)I t(cid:19)
(cid:0) (cid:1)⊤
R = ·P with t= b 0 0 (8)
i 0 1 i
Then, for each image I in the training set, we can render a corresponding right frame Ir,
i i
estimatedisparitywithastereonetworkΘandusethefocallength f totriangulatedepth:
f·b
D (I)= (9)
stereo i Θ(I,Ir)
i i
Fig.3providesanoverviewofourapproach. DuringGStraining,wecanstartexploiting
thisstrategyonlyafterthemodelcanrendergood-qualityimagesalready–i.e.,afterT steps.
Furthermore,asdisparityestimationrequiresanon-negligibleextracomputation,wecache
disparitymapsassoonastheyarecomputedthefirsttimeandreusetheminthesubsequent
steps; as long as the quality of rendered images increases with training, we set a refresh
intervalτ forrenderingagainthestereopairsandupdatingthedisparitypriors.
Why stereo? Our strategy would work to render views for MVS networks as well,
however: i)asweaimatminimizingtheoverheadduringGSoptimization,framepairsare
theminimumamountofinformationrequiredtoderivedepthfromimagesthroughgeometry;
ii) tuning a single parameter for rendering the novel, arbitrary views – i.e., the horizontal
baselineb–issimplerthantuning6-DoFposes;iii)state-of-the-artstereonetworksexcelat
domaingeneralization,whereasweobservedthisisoftennottrueforMVSnetworks.S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 7
5 Experiments
Inthissection,wepresenttheapproachesweselectedtoobtaindepthpriorsandthedatasets
used in our experiments. Next, we describe the implementation details of our framework.
Finally,wereportourfindings.
5.1 DepthPriorsSettings
Toevaluatetheeffectivenessofthedifferentstrategiesforextractingdepthpriorsfromim-
ages, we select one representative method for each. SfM: We use COLMAP [54], as it is
alreadyusedforcomputingcameraposesandtheinitial3DpointsfromwhichGSoptimiza-
tionisbootstrapped. DC:WeselectVPPDC[6]since,tothebestofourknowledge,itisthe
onlydepthcompletionapproachproposedforcross-domaingeneralization. MDE:Weuse
ZoeDepth[7],withtheweightsprovidedbytheauthors. Thesamemodelhasbeenusedin
a concurrent work [10], allowing us to asses its effectiveness against alternative depth pri-
orssources. MVS:WeuseCER-MVS[36], asitshowspromisinggeneralization. Weuse
BlendedMVSweightsfortestsonETH3D/ScanNet++,andDTUweightsonBlendedMVS,
toavoidoverlapwithtrainingdata. Stereo(Self-Evolving): WeusetheRAFT-Stereo[34]
varianttrainedfortheRobustVisionChallenge–iRAFT-Stereo_RVC[28].
5.2 Datasets
Weselectthreedatasetsprovidingground-truthdepth,instrumentalforourstudies. Wewill
appreciatehowthedifferencesinthethreewillimpacttheresultsbydifferentmethods.
ETH3D.Itisareal-worlddataset,providingimagesandgroundtruthdepthatabout24
Megapixels. Weuseallofthe13trainingscenesofthehigh-resolutionset,having14to76
images. Weusetheprovidedundistortedimages,cameraposes,andpointclouds. Following
vanilla GS settings, images are resized to have 1600 width before training. We manually
spliteachsceneintotrainingandtestsets(pleasecheckthesupplementaryfordetailsofthe
splits). Wealign(distorted)groundtruthdepthtoundistortedimagesforevaluation.
ScanNet++.Itisareal-worlddatasetwithhigh-fidelity3Dgeometryandhigh-resolution
RGBimagesofindoorscenes. Weperformourexperimentson2oftherandomlyselected
scenes, due to the large number of sequences. We undistort the fisheye images and depth
mapsusingtheprovidedofficialtoolkit. Thescenescontain291and399imagesataresolu-
tionof1752×1168,andweonlyuseevery10thimagefortrainingandtherestfortesting.
BlendedMVS.Itisasemi-syntheticdataset. Duetothelargenumberofsequences,we
randomlyselect4ofthemandperformexperimentsonthesesequences. Sincethisdataset
doesnotprovidethepointclouds,werunCOLMAPontheimagesusingthegivencamera
poses to obtain the point clouds for bootstrapping GS. We use the images at their original
resolutionof768×576,counting75to212framesperscene,withevery4thusedfortesting.
5.3 ImplementationDetails
Weimplementourself-evolvingGSstartingfrom[29]. ThelossforDS-GSisdefinedas:
L=(1−λ )||I−Iˆ|| +λ D-SSIM(I,Iˆ)+λ ||D (I)−Dˆ|| (10)
1 1 1 2 k 1
whereIistheoriginalimage,IˆandDˆ aretherenderedimageanddepthmaps,respectively,
andD isthedepthmap,obtainedthroughoneoftheproposedmethods. e.g.,forourself-
k8 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
Depth ViewSynthesis
Method Abs.Rel.↓ RMSE↓ δ<1.25↑ SSIM↑ PSNR↑ LPIPS↓
GS[29] 0.211 1.698 0.652 0.7425 20.4029 0.3385
+SfM[54] 0.109 0.870 0.844 0.7561 21.7224 0.3261
+DC[6] 0.148 1.272 0.828 0.7557 21.5206 0.3248
+MDE[7] 0.153 1.204 0.793 0.7475 21.3104 0.3377
+MVS[36] 0.094 1.031 0.914 0.7692 22.2806 0.3105
+Self-Evolving(ours) 0.057 0.599 0.942 0.7704 22.2825 0.3141
+Oracle(GTdepth) 0.020 0.317 0.980 0.7764 22.4669 0.3009
Table1: QuantitativeResultsonETH3D.
Depth ViewSynthesis
Method Abs.Rel.↓ RMSE↓ δ<1.25↑ SSIM↑ PSNR↑ LPIPS↓
GS[29] 0.154 0.416 0.735 0.9162 27.7907 0.1587
+SfM[54] 0.104 0.295 0.860 0.9131 27.9140 0.1663
+DC[6] 0.144 0.381 0.813 0.9145 27.5081 0.1659
+MDE[7] 0.083 0.242 0.926 0.9168 28.0568 0.1588
+MVS[36] 0.152 0.437 0.824 0.9138 27.3536 0.1699
+Self-Evolving(ours) 0.068 0.222 0.928 0.9165 28.1488 0.1601
+Oracle(GTdepth) 0.024 0.103 0.983 0.9199 28.6413 0.1539
Table2: QuantitativeResultsonScanNet++.
evolving method D =D . We set λ =0.2 and set λ =0.01 for the BlendedMVS
k Stereo 1 2
datasetandλ =0.1fortheETH3DandScanNet++datasets.Notethatforourself-evolving
2
method, we set λ =0 for all iterations before the starting step T. On the BlendedMVS
2
dataset,weuseT =17K andtrainthemodelsfor20K iterations,whilefortheETH3Dand
ScanNet++datasets,wesetT =7K andtrainfor11K iterations. Inalldatasets,wesetthe
refresh interval to τ =100 and randomly sample b from an interval (see supplementary
material). WeperformallofourexperimentsonasingleV100GPU.
5.4 Results
In this section, we report the outcome of our experiments. In each table, we highlight the
first, second and third -bestresults.
ETH3D.Table1showsquantitativeresultsonETH3D.WecanappreciatehowanyDS-
GS variant improves over vanilla GS, both in terms of depth and color rendering quality.
Using COLMAP yields the third-best result and unveils an interesting finding: vanilla GS
optimizationissub-optimalsincetheverysame3DpointsusedtobootstrapGaussianscan
provideadditionalsupervisionforfree. Nonetheless,DCfailsatimprovingoverCOLMAP,
because of the very few SfM points being insufficient for obtaining a good-quality dense
depthmap–seethesupplementarymaterialforqualitativeexamples. MVSrankssecond
bothintermsofdepthandcolorquality,whileourself-evolvingDS-GSlargelyoutperforms
itintermsofdepthestimation,slightlyimprovingcolorqualityinSSIMandPSNRaswell.
ScanNet++. Table2collectsresultsonScanNet++. Atfirstglance,wecanconfirmthe
superiority of our approach in terms of depth metrics, while resulting almost equivalent to
MDEoncolormetrics.Interestingly,MDEsignificantlyoutperformsSfMandothermethods
in this setting. This is caused by the lack of texture in these scenes, on which COLMAP
extractsfew3Dpoints(seesupplementarymaterial)andthusprovidespoorsupervision.
SG-SD
SG-SDS.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 9
Depth ViewSynthesis
Method Abs.Rel.↓ RMSE↓ δ<1.25↑ SSIM↑ PSNR↑ LPIPS↓
GS[29] 0.058 7.041 0.933 0.6301 21.6160 0.2729
+SfM[54] 0.021 3.910 0.990 0.6389 22.1409 0.2644
+DC[6] 0.021 3.719 0.991 0.6378 21.9899 0.2648
+MDE[7] 0.113 12.141 0.840 0.6142 21.1857 0.2867
+MVS[36] 0.065 10.316 0.914 0.6021 20.8971 0.2944
+Self-Evolving(ours) 0.020 3.714 0.992 0.6377 21.9734 0.2696
+Oracle(GTdepth) 0.013 2.645 0.993 0.6480 22.2282 0.2575
Table3: QuantitativeResultsonBlendedMVS.
Figure4: QualitativeResultsonETH3D.
BlendedMVS.Table3collectsresultsonBlendedMVS.Onthesesemi-syntheticimages,
SfM can extract very dense matches, resulting in much stronger supervision as well as a
much simpler completion task for the DC model (see supplementary material). Indeed,
DCrankssecondinbothdepthandcolormetrics,withSfMrankingfirstinrenderingquality.
Onthecontrary,oursolutionis,again,theabsolutewinnerintermsofdepthaccuracy,while
MDEandMVSfailatimprovingthebaseline: weascribethistogeneralizationissues.
Qualitative Results. We conclude this section by reporting some qualitative compar-
isons. Fig.4collectsthreesamplesfromETH3Ddataset. Attheverytop,weshowimages
and depth maps obtained by the vanilla GS, with several artifacts appearing in any of the
three examples, followed by results yielded by using SfM or our strategy. Conversely to
SfM,ourself-evolvingGScanconsistentlyimprovebothrenderedimagesanddepthmaps.
Finally, at the very bottom, we report ground-truth images and depth maps as a reference.
Wereportmorequalitativeresultsinthesupplementarymaterial.
5.5 AblationStudies
Finally, we conduct some ablation studies focused on our self-evolving framework. These
arecarriedoutontheBlendedMVSdatasetandaveragedover5runs.
StereoModels. Table4collectstheresultsachievedbyusingdifferentstereonetworks
toobtaindepthpriors. Foreachmodel,wereportthespecificweightsweusedamongthose
available in brackets. We can appreciate how using any of the state-of-the-art stereo back-
SG
MfS+
.vE-fleS+
TG
SG-SD10 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
Depth ViewSynthesis
Method Abs.Rel.↓ RMSE↓ δ<1.25↑ SSIM↑ PSNR↑ LPIPS↓
GS[29] 0.058 7.041 0.933 0.6301 21.6160 0.2729
IGEV-Stereo[23](Middlebury[53]) 0.025 4.331 0.984 0.6340 21.8255 0.2727
PCVNet[77](SceneFlow[39]) 0.023 4.082 0.988 0.6351 21.8605 0.2717
CREStereo[33](Mixed) 0.023 3.896 0.985 0.6361 21.9168 0.2714
iRaftStereo_RVC[28](Mixed) 0.020 3.714 0.992 0.6377 21.9734 0.2696
Table4: AblationStudiesonBlendedMVS–stereomodels.
Depth ViewSynthesis
Method Abs.Rel.↓ RMSE↓ δ<1.25↑ SSIM↑ PSNR↑ LPIPS↓
GS[29] 0.058 7.041 0.933 0.6301 21.6160 0.2729
RAFT-Stereo[34](NerfStereo[60]) 0.023 4.046 0.987 0.6360 21.8964 0.2715
RAFT-Stereo[34](Sceneflow[39]) 0.020 3.679 0.991 0.6375 21.9517 0.2696
RAFT-Stereo[34](Middlebury[53]) 0.021 3.929 0.989 0.6362 21.9027 0.2708
iRaftStereo_RVC[28](Mixed) 0.020 3.714 0.992 0.6377 21.9734 0.2696
Table5: AblationStudiesonBlendedMVS–RAFT-Stereovariants.
bonesallowsforlargelyimprovingtheresultsovervanillaGS.However,iRAFTStereo_RCV
andCREStereoshowhigherimprovementsagainstPCVNetandIGEV-Stereo,bothinterms
ofcoloranddepthrendering. Weascribethisbothtotheirspecificarchitecture,aswellasto
themixofseveraldatasetsusedtotrainthem.
RAFT-Stereo–TrainingDatasets.Tofigureouttherealimpactofboththetrainingdata
and the stereo backbone, in Table 5 we compare the results obtained with different RAFT-
Stereoweights. Thegapbetweentheseveralinstancesisverylow,withiRAFT-Stereoand
theoriginalRAFT-StereotrainedonSceneFlowbeingonparonthreeoutofsixmetrics.
6 Conclusion
Insummary,thisworkseekstoaddressacriticallimitationin3DGaussianSplattingbyfo-
cusingonimprovingitsunderlyingscenegeometry. Throughacomprehensiveanalysis,we
studyanoptimizationapproachthatintegratesexternaldepthpriors,simultaneouslyimprov-
ingtheinferred3Dstructureandthequalityofnovelviewsynthesis. Akeycontributionis
ournovelstrategyleveragingdepthpriorscomputedfromreadilyavailabledeepstereonet-
worksonvirtualstereopairsrenderedduringtrainingbyGSitself, demonstratingsuperior
performance compared to alternative depth-from-image solutions. Experimental results on
ETH3D,ScanNet++,andBlendedMVSdatasetssupporttheimportanceofourfindingsand
provideevidencefortheeffectivenessofourproposal.
Acknowledgements. WeacknowledgetheCINECAawardundertheISCRAinitiative,
fortheavailabilityofhigh-performancecomputingresourcesandsupport. SadraSafadoust
wassupportedbyKUISAIFellowshipandUNVESTR&DCenter.Thisprojectisco-funded
by the European Union (ERC, ENSURE, 101116486). Views and opinions expressed are
however those of the author(s) only and do not necessarily reflect those of the European
Union or the European Research Council. Neither the European Union nor the granting
authoritycanbeheldresponsibleforthem.
.vE-fleS+
.vE-fleS+S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 11
References
[1] Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz, and Richard Szeliski.
Building rome in a day. In 2009 IEEE 12th International Conference on Computer
Vision,pages72–79,2009.
[2] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt,
James Tompkin, and Matthew O’Toole. Törf: Time-of-flight radiance fields for dy-
namicsceneviewsynthesis. AdvancesinNeuralInformationProcessingSystems,34,
2021.
[3] JonathanTBarron,BenMildenhall,MatthewTancik,PeterHedman,RicardoMartin-
Brualla,andPratulPSrinivasan.Mip-nerf:Amultiscalerepresentationforanti-aliasing
neural radiance fields. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pages5855–5864,2021.
[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hed-
man. Mip-nerf360: Unboundedanti-aliasedneuralradiancefields. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages5470–
5479,2022.
[5] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter
Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. arXiv preprint
arXiv:2304.06706,2023.
[6] Luca Bartolomei, Matteo Poggi, Andrea Conti, Fabio Tosi, and Stefano Mattoccia.
Revisitingdepthcompletionfromastereomatchingperspectiveforcross-domaingen-
eralization. InInternationalConferenceon3DVision(3DV),March2024.
[7] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller.
Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint
arXiv:2302.12288,2023.
[8] Amlaan Bhoi. Monocular depth estimation: A survey. arXiv preprint
arXiv:1901.09402,2019.
[9] Xiaoxue Chen, Junchen Liu, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang. Nerrf:
3d reconstruction and viewsynthesis for transparent and specular objectswith neural
refractive-reflectivefields. arXivpreprintarXiv:2309.13039,2023.
[10] JaeyoungChung,JeongtaekOh,andKyoungMuLee. Depth-regularizedoptimization
for3dgaussiansplattinginfew-shotimages. arXivpreprintarXiv:2311.13398,2023.
[11] FrankDellaertandLinYen-Chen. Neuralvolumerendering: Nerfandbeyond. arXiv
preprintarXiv:2101.05204,2020.
[12] KangleDeng,AndrewLiu,Jun-YanZhu,andDevaRamanan. Depth-supervisednerf:
Fewerviewsandfastertrainingforfree. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages12882–12891,2022.
[13] KangleDeng,AndrewLiu,Jun-YanZhu,andDevaRamanan.Depth-supervisedNeRF:
Fewerviewsandfastertrainingforfree. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),June2022.12 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
[14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and
AngjooKanazawa. Plenoxels: Radiancefieldswithoutneuralnetworks. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
5501–5510,2022.
[15] YasutakaFurukawa, CarlosHernández, etal. Multi-viewstereo: Atutorial. Founda-
tionsandTrends®inComputerGraphicsandVision,9(1-2):1–148,2015.
[16] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthe-
sis from dynamic monocular video. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages5712–5721,2021.
[17] Kyle Gao, Yina Gao, Hongjie He, Denning Lu, Linlin Xu, and Jonathan Li.
Nerf: Neural radiance field in 3d vision, a comprehensive review. arXiv preprint
arXiv:2210.00379,2022.
[18] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien
Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages14346–14355,2021.
[19] Beerend GA Gerats, Jelmer M Wolterink, and Ivo AMJ Broeders. Dynamic depth-
supervisednerfformulti-viewrgb-doperatingroomvideos.InInternationalWorkshop
onPRedictiveIntelligenceInMEdicine,pages218–230.Springer,2023.
[20] RiccardoGherardi,MichelaFarenzena,andAndreaFusiello. Improvingtheefficiency
ofhierarchicalstructure-and-motion. In2010IEEEComputerSocietyConferenceon
ComputerVisionandPatternRecognition,pages1594–1600,2010.
[21] Yuan-ChenGuo,DiKang,LinchaoBao,YuHe,andSong-HaiZhang. Nerfren: Neu-
ral radiance fields with reflections. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages18409–18418,2022.
[22] Mohd Saad Hamid, NurulFajar Abd Manap, Rostam Affendi Hamzah, and Ah-
mad Fauzan Kadmin. Stereo matching algorithm based on deep learning: A survey.
Journal of King Saud University-Computer and Information Sciences, 34(5):1663–
1673,2022.
[23] RostamAffendiHamzah,HaidiIbrahim,etal. Literaturesurveyonstereovisiondis-
paritymapalgorithms. JournalofSensors,2016,2016.
[24] DerekHoiem,AlexeiAEfros,andMartialHebert. Automaticphotopop-up. InACM
SIGGRAPH2005Papers,pages577–584.2005.
[25] Junjie Hu, Chenyu Bao, Mete Ozay, Chenyou Fan, Qing Gao, Honghai Liu, and
Tin Lun Lam. Deep depth completion: a survey. arXiv preprint arXiv:2205.05335,
2022.
[26] Ronghang Hu, Nikhila Ravi, Alexander C Berg, and Deepak Pathak. Worldsheet:
Wrappingtheworldina3dsheetforviewsynthesisfromasingleimage. InProceed-
ings of the IEEE/CVF International Conference on Computer Vision, pages 12528–
12537,2021.S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 13
[27] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. Efficientnerf efficient
neuralradiancefields. InProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages12902–12911,2022.
[28] HualieJiang,RuiXu,andWenjieJiang. Animprovedraftstereotrainedwithamixed
datasetfortherobustvisionchallenge2022. arXivpreprintarXiv:2210.12785,2022.
[29] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d
gaussiansplattingforreal-timeradiancefieldrendering. ACMTransactionsonGraph-
ics,42(4),2023.
[30] MuhammadAhmedUllahKhan,DanishNazir,AlainPagani,HamamMokayed,Mar-
cusLiwicki,DidierStricker,andMuhammadZeshanAfzal. Acomprehensivesurvey
ofdepthcompletionapproaches. Sensors,22(18):6969,2022.
[31] MijeongKim, SeongukSeo, andBohyungHan. Infonerf: Rayentropyminimization
forfew-shotneuralvolumerendering. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages12912–12921,2022.
[32] Yixing Lao, Xiaogang Xu, Xihui Liu, Hengshuang Zhao, et al. Corresnerf: Image
correspondencepriorsforneuralradiancefields. AdvancesinNeuralInformationPro-
cessingSystems,36,2024.
[33] JiankunLi,PeisenWang,PengfeiXiong,TaoCai,ZiweiYan,LeiYang,JiangyuLiu,
HaoqiangFan, andShuaichengLiu. Practicalstereomatchingviacascadedrecurrent
network with adaptive correlation. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages16263–16272,2022.
[34] Lahav Lipson, Zachary Teed, and Jia Deng. Raft-stereo: Multilevel recurrent field
transformsforstereomatching.InInternationalConferenceon3DVision(3DV),2021.
[35] LingjieLiu,JiataoGu,KyawZawLin,Tat-SengChua,andChristianTheobalt. Neural
sparse voxel fields. Advances in Neural Information Processing Systems, 33:15651–
15663,2020.
[36] ZeyuMa,ZacharyTeed,andJiaDeng. Multiviewstereowithcascadedepipolarraft.
InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),2022.
[37] RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,JonathanTBarron,Alexey
Dosovitskiy,andDanielDuckworth.Nerfinthewild:Neuralradiancefieldsforuncon-
strainedphotocollections. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages7210–7219,2021.
[38] ArminMasoumian,HatemARashwan,JuliánCristiano,MSalmanAsif,andDomenec
Puig. Monocular depth estimation using deep learning: A review. Sensors, 22(14):
5353,2022.
[39] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey
Dosovitskiy,andThomasBrox. Alargedatasettotrainconvolutionalnetworksfordis-
parity,opticalflow,andsceneflowestimation. InTheIEEEConferenceonComputer
VisionandPatternRecognition(CVPR),June2016.14 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
[40] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-
mamoorthi,andRenNg. Nerf: Representingscenesasneuralradiancefieldsforview
synthesis. InECCV,2020.
[41] Yue Ming, Xuyang Meng, Chunxiao Fan, and Hui Yu. Deep learning for monocular
depthestimation: Areview. Neurocomputing,438:14–33,2021.
[42] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural
graphicsprimitiveswithamultiresolutionhashencoding. ACMTrans.Graph.,41(4):
102:1–102:15, July 2022. doi: 10.1145/3528223.3530127. URL https://doi.
org/10.1145/3528223.3530127.
[43] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas
Geiger,andNohaRadwan. Regnerf: Regularizingneuralradiancefieldsforviewsyn-
thesisfromsparseinputs. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages5480–5490,2022.
[44] MatteoPoggi,FabioTosi,KonstantinosBatsos,PhilipposMordohai,andStefanoMat-
toccia. Onthesynergiesbetweenmachinelearningandbinocularstereofordepthes-
timationfromimages: asurvey. IEEETransactionsonPatternAnalysisandMachine
Intelligence,44(9):5314–5334,2021.
[45] AKM Rabby and Chengcui Zhang. Beyondpixels: A comprehensive review of the
evolutionofneuralradiancefields. arXivpreprintarXiv:2306.03000,2023.
[46] RenéRanftl,AlexeyBochkovskiy,andVladlenKoltun. Visiontransformersfordense
prediction. ICCV,2021.
[47] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun.
Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-
dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence,
44(3),2022.
[48] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding
upneuralradiancefieldswiththousandsoftinymlps. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages14335–14345,2021.
[49] GernotRieglerandVladlenKoltun. Freeviewsynthesis. InComputerVision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
PartXIX16,pages623–640.Springer,2020.
[50] Gernot Riegler and Vladlen Koltun. Stable view synthesis. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12216–
12225,2021.
[51] BarbaraRoessle,JonathanTBarron,BenMildenhall,PratulPSrinivasan,andMatthias
Nießner. Densedepthpriorsforneuralradiancefieldsfromsparseinputviews. InPro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages12892–12901,2022.
[52] Daniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-
framestereocorrespondencealgorithms. Internationaljournalofcomputervision,47:
7–42,2002.S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 15
[53] Daniel Scharstein, Heiko Hirschmüller, York Kitajima, Greg Krathwohl, Nera Nesic,
XiWang,andPorterWestling. High-resolutionstereodatasetswithsubpixel-accurate
groundtruth. InXiaoyiJiang,JoachimHornegger,andReinhardKoch,editors,GCPR,
volume 8753 of Lecture Notes in Computer Science, pages 31–42. Springer, 2014.
ISBN978-3-319-11751-5.
[54] JohannesLutzSchönbergerandJan-MichaelFrahm. Structure-from-motionrevisited.
InConferenceonComputerVisionandPatternRecognition(CVPR),2016.
[55] JohannesLutzSchönberger,EnliangZheng,MarcPollefeys,andJan-MichaelFrahm.
Pixelwiseviewselectionforunstructuredmulti-viewstereo. InEuropeanConference
onComputerVision(ECCV),2016.
[56] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad
Schindler,MarcPollefeys,andAndreasGeiger. Amulti-viewstereobenchmarkwith
high-resolutionimagesandmulti-cameravideos. InProceedingsoftheIEEEconfer-
enceoncomputervisionandpatternrecognition,pages3260–3269,2017.
[57] StevenMSeitz,BrianCurless,JamesDiebel,DanielScharstein,andRichardSzeliski.
A comparison and evaluation of multi-view stereo reconstruction algorithms. In
2006 IEEE computer society conference on computer vision and pattern recognition
(CVPR’06),volume1,pages519–528.IEEE,2006.
[58] Elisavet Konstantina Stathopoulou and Fabio Remondino. A survey on conventional
and learning-based methods for multi-view stereo. The Photogrammetric Record, 38
(183):374–407,2023.
[59] ChengSun,MinSun,andHwann-TzongChen. Directvoxelgridoptimization: Super-
fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages5459–5469,2022.
[60] FabioTosi,AlessioTonioni,DanieleDeGregorio,andMatteoPoggi. Nerf-supervised
deepstereo.InConferenceonComputerVisionandPatternRecognition(CVPR),pages
855–866,June2023.
[61] FabioTosi,LucaBartolomei,andMatteoPoggi. Asurveyondeepstereomatchingin
thetwenties. arXivpreprintarXiv:2407.07816,2024.
[62] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and
PratulPSrinivasan. Ref-nerf: Structuredview-dependentappearanceforneuralradi-
ancefields. In2022IEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion(CVPR),pages5481–5490.IEEE,2022.
[63] Chen Wang, Jiadai Sun, Lina Liu, Chenming Wu, Zhelun Shen, Dayan Wu, Yuchao
Dai,andLiangjunZhang. Diggingintodepthpriorsforoutdoorneuralradiancefields.
Proceedingsofthe31thACMInternationalConferenceonMultimedia,2023.
[64] GuangcongWang,ZhaoxiChen,ChenChangeLoy,andZiweiLiu.Sparsenerf:Distill-
ingdepthrankingforfew-shotnovelviewsynthesis. arXivpreprintarXiv:2303.16196,
2023.16 S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING
[65] XiangWang,ChenWang,BingLiu,XiaoqingZhou,LiangZhang,JinZheng,andXiao
Bai. Multi-view stereo in the deep learning era: A comprehensive review. Displays,
70:102102,2021.
[66] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfin-
gmvs: Guidedoptimizationofneuralradiancefieldsforindoormulti-viewstereo. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
5610–5619,2021.
[67] ZexiaoXie,XiaoxuanYu,XiangGao,KunqianLi,andShuhanShen. Recentadvances
inconventionalanddeeplearning-baseddepthcompletion: Asurvey. IEEETransac-
tionsonNeuralNetworksandLearningSystems,2022.
[68] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang
Wang.Sinnerf:Trainingneuralradiancefieldsoncomplexscenesfromasingleimage.
InEuropeanConferenceonComputerVision,pages736–753.Springer,2022.
[69] QiangengXu, ZexiangXu, JulienPhilip, SaiBi, ZhixinShu, KalyanSunkavalli, and
Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages5438–
5448,2022.
[70] XiaoqiangYan,ShizheHu,YiqiaoMao,YangdongYe,andHuiYu. Deepmulti-view
learningmethods: Areview. Neurocomputing,448:106–129,2021.
[71] YaoYao,ZixinLuo,ShiweiLi,TianFang,andLongQuan. Mvsnet: Depthinference
for unstructured multi-view stereo. In Proceedings of the European Conference on
ComputerVision(ECCV),pages767–783,2018.
[72] YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,LeiZhou,TianFang,and
LongQuan. Blendedmvs: Alarge-scaledatasetforgeneralizedmulti-viewstereonet-
works. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages1790–1799,2020.
[73] ChandanYeshwanth,Yueh-ChengLiu,MatthiasNießner,andAngelaDai. Scannet++:
Ahigh-fidelitydatasetof3dindoorscenes. InProceedingsoftheInternationalCon-
ferenceonComputerVision(ICCV),2023.
[74] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Öztireli, and Olga Sorkine-Hornung.
Differentiable surface splatting for point-based geometry processing. ACM Transac-
tionsonGraphics(TOG),38(6):1–14,2019.
[75] AlexYu,RuilongLi,MatthewTancik,HaoLi,RenNg,andAngjooKanazawa.Plenoc-
treesforreal-timerenderingofneuralradiancefields. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages5752–5761,2021.
[76] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger.
Monosdf: Exploringmonoculargeometriccuesforneuralimplicitsurfacereconstruc-
tion. Advancesinneuralinformationprocessingsystems,35:25018–25032,2022.
[77] JiaxiZeng,ChengtangYao,LidongYu,YuweiWu,andYundeJia. Parameterizedcost
volumeforstereomatching.InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages18347–18357,2023.S.SAFADOUSTETAL.:SELF-EVOLVING3DGAUSSIANSPLATTING 17
[78] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing
andimprovingneuralradiancefields. arXivpreprintarXiv:2010.07492,2020.
[79] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differen-
tiablepoint-basedradiancefieldsforefficientviewsynthesis.InSIGGRAPHAsia2022
ConferencePapers,pages1–12,2022.
[80] ChaoqiangZhao,QiyuSun,ChongzhenZhang,YangTang,andFengQian.Monocular
depthestimationbasedondeeplearning: Anoverview. ScienceChinaTechnological
Sciences,63(9):1612–1627,2020.
[81] Kun Zhou, Xiangxi Meng, Bo Cheng, et al. Review of stereo matching algorithms
basedondeeplearning. Computationalintelligenceandneuroscience,2020,2020.