FreeEnhance: Tuning-Free Image Enhancement
via Content-Consistent Noising-and-Denoising Process
YangLuo∗ YihengZhang ZhaofanQiu TingYao
SchoolofComputerScience HiDream.aiInc. HiDream.aiInc. HiDream.aiInc.
FudanUniversity Beijing,China Beijing,China Beijing,China
Shanghai,China yihengzhang.chn@hidream.ai qiuzhaofan@hidream.ai tiyao@hidream.ai
yangluo.fdu@gmail.com
ZhinengChen Yu-GangJiang TaoMei
SchoolofComputerScience SchoolofComputerScience HiDream.aiInc.
FudanUniversity FudanUniversity Beijing,China
Shanghai,China Shanghai,China tmei@hidream.ai
zhinchen@fudan.edu.cn ygj@fudan.edu.cn
Figure1:ThelandscapeexamplesofFreeEnhanceversusSDXL.Ineachpairofimages,theleftoneisgeneratedbySDXLataresolutionof
1,024×1,024,whiletherightoneisproducedbyFreeEnhanceusingtheSDXL-synthesizedimageastheinput.FreeEnhancepreservesthe
resolutionoftheinputimageswhileintroducingadditionaldetailsinacontent-consistentmanner.
Abstract neverthelessisnottrivialandnecessitatestodelicatelyenrichplen-
Theemergenceoftext-to-imagegenerationmodelshasledtothe tifuldetailswhilepreservingthevisualappearanceofkeycontent
recognitionthatimageenhancement,performedaspost-processing, intheoriginalimage.Inthispaper,weproposeanovelframework,
wouldsignificantlyimprovethevisualqualityofthegeneratedim- namelyFreeEnhance,forcontent-consistentimageenhancement
ages.Exploringdiffusionmodelstoenhancethegeneratedimages usingtheoff-the-shelfimagediffusionmodels.Technically,FreeEn-
hanceisatwo-stageprocessthatfirstlyaddsrandomnoisetothe
∗ThisworkwasperformedatHiDream.ai. inputimageandthencapitalizesonapre-trainedimagediffusion
model(i.e.,LatentDiffusionModels)todenoiseandenhancethe
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor imagedetails.Inthenoisingstage,FreeEnhanceisdevisedtoadd
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation lighternoisetotheregionwithhigherfrequencytopreservethe
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe high-frequentpatterns(e.g.,edge,corner)intheoriginalimage.
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Inthedenoisingstage,wepresentthreetargetpropertiesascon-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. straintstoregularizethepredictednoise,enhancingimageswith
MM’24,October28-November1,2024,Melbourne,Australia. highacutanceandhighvisualquality.Extensiveexperimentscon-
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. ductedontheHPDv2datasetdemonstratethatourFreeEnhance
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3681506 outperforms the state-of-the-art image enhancement models in
4202
peS
11
]VC.sc[
1v15470.9042:viXraMM’24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
termsofquantitativemetricsandhumanpreference.Moreremark- Forthehigh-frequencyregion,weemployDDIMinversion[37]
ably,FreeEnhancealsoshowshigherhumanpreferencecompared toattachlightnoise,whichiseasiertobeeliminatedbyusinga
tothecommercialimageenhancementsolutionofMagnificAI. diffusionmodelthanrandomnoise.Forthelow-frequencyregion,
weintroducearandomnoisewithhigherintensitytoaccentuatethe
CCSConcepts changesinlow-frequencyarea,wherevisualdetailsaretypically
•Informationsystems→Multimediacontentcreation. absent.Then,inthedenoisingprocess,weutilizethepre-trained
SDXL[42],whichisoneofthemostpowerfulopen-sourceimage
Keywords diffusionmodels,asthedenoiser.Theobjectiveofdenoisingstageis
notmerelytoeliminatenoisebutalsotoaddhigh-qualitydetails.To
ImageGeneration,ImageEnhancement,DiffusionModel
achievethis,wedevelopthreegradient-basedregularizers:image
ACMReferenceFormat: actuation,noisedistribution,andadversarialdegradation.These
YangLuo,YihengZhang,ZhaofanQiu,TingYao,ZhinengChen,Yu-Gang regularizersaredesignedtoenhancethenoiseremovalprocessby
Jiang,andTaoMei.2024.FreeEnhance:Tuning-FreeImageEnhancement revisingpredictednoise,leadingtotheimprovementoftheoverall
viaContent-ConsistentNoising-and-DenoisingProcess.InProceedingsof imagequality.Figure1illustratestheexamplesoftheinputimages
the32ndACMInternationalConferenceonMultimedia(MM’24),October fromHPDv2datasetandtheenhancedimagesbyFreeEnhance.
28-November1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,
Insummary,wehavemadethefollowingcontributions:1)The
10pages.https://doi.org/10.1145/3664647.3681506
proposedFreeEnhanceisshowncapableoftuning-freestrategy
toimprovethequalityofthegeneratedimages;2)Thedesigns
1 Introduction
ofcontent-consistentnoisingandthreedenoisingcorrectionsare
Therecentdevelopmentofdiffusionmodelshassparkedaremark- unique;3)FreeEnhancehasbeenproperlyanalyzedandverified
ableincreaseinresearchareaofmultimediacontentgeneration. throughextensiveexperimentsoverHPDv2datasettovalidateits
Amongtheseendeavors,text-to-imagegenerationstandsoutasone efficacy.Withthegood,duetothecontent-consistentcapability,
ofthemostrepresentativetasks[18,50,65].DiffusionProbabilistic FreeEnhancecanbereadilyapplicabletoenhancerealimages.
Models(DPM)[24,40,52]regardimagegenerationasamulti-step
denoisingprocess,employingapowerfuldenoisernetworktopro- 2 Relatedworks
gressivelytransformaGaussionnoisemapintoanoutputimage.
2.1 DiffusionModels
Buildinguponthismethod,LatentDiffusionModels(LDM)[42,46]
Diffusionmodels[18,24,48,54,68]havegarneredattentionfortheir
proposetoexecutedenoisingprocessinthelatentfeaturespacethat
remarkablegenerativequalityanddiversityinlearningcomplex
isestablishedbyapre-trainedautoencoder,leadingtohighcompu-
datadistributions.Theyhavebeenappliedinvariousdownstream
tationefficiencyandimagequality.Toimprovethecontrollability
tasks,includingmultimediagenerationsliketext-to-image[10,44,
oftext-to-imagegeneration,ControlNet[65]andT2I-Adapter[38]
47,65],text-to-video[26,32,62],text-to-3D[11,12,43,63],text-to-
incorporatevariousspatialconditionsintothedenoisernetwork.
audio[21],andimage-to-video[9,67].Diffusionmodelssynthesizes
Despiteshowingimpressiveprogressincontentcontrolling,syn-
multimediacontentsfromaninitialrandomnoisebyiterativede-
thesizinghigh-qualityimageremainschallenging,duetothelack
noisingoperations.Existingpixel-baseddiffusionmodelsexhibit
ofvisualdetailsinthegeneratedimages,asshowninFigure1.
slowinferencespeedsandrequiredsubstantialcomputationalre-
Toenrichdetailsinthegeneratedimages,onegeneralsolution
sources.Manycreativeresearchesdevoteintoovercomethisissue
isthe“noising-and-denoising”process,whichfirstproperlyadds
fromapplyingdiscretediffusion[5],usingimagetokensfromVQ-
noise to the original image, and then uses a diffusion model to
VAE[22].Amongthem,theLatentDiffusionModels(LDM)[46]
denoisethenoisyimage.ThisideaisproposedinSDEdit[36]for
operatesthenoisinganddenoisinginacompressedlatentspace,
imageediting,andthenexploredinSDXL[42]toenhancethegener-
effectivelygetoutofthisdilemmabystrikingabettertrade-off
atedimage,asillustratedinFigure2(a).However,theeffectiveness
betweencostandgenerationquality.Subsequentimprovements
ofsuchprocesshighlyreliesonthestrengthoftheattachednoise.
includingattentionmechanism[8],enhancingthearchitectures
Specifically, when the noise magnitude is low, the input image
[17,41]andprompt-tuning[19]havevigorouslydriventhedevel-
cannotbeeffectivelyenhanced,whereaswhenitishigh,thekey
opmentofdiffusionmodelsinai-generatedcontent.Moreover,asa
content(e.g.,humanorobjects)undergoessignificantchangesthat
basicparadigm,image-to-imagetranslationtasksalsodemonstrate
deviatefromtheoriginalinputimage.Toalleviatethislimitation,
thepotentialofusingthediffusionmodelinstyle-transfer[58,66],
weproposetoremouldthisprocessbyselectivelyaddinglighter
inpainting[33,61]andimageediting[28,51].
noiseinhigh-frequencyregionstopreserveedgeandcornerdetails,
whileheaviernoiseisaddedinlow-frequencyregionstocarrymore
2.2 GuidanceinDiffusionModels
detailsinthesmootharea,asshowninFigure2(b).Moreover,wede-
visethreetypesofregularizationstocorrectthedenoisingprocess Guidanceisatechniqueemployedinthesamplingprocess.Itcan
andproduceimageswithsuperioracutanceandvisualquality. beregardedasanextraupdatetothesamplingdirectionandcan
Specifically,weproposeanewframeworkFreeEnhance,that modifytheoutputsaftertrainingbyguidingwithadditionalcon-
remouldthestandardnoising-and-denoisingprocesstoimprove ditions,suchaslabel[18],text[46].Classifierguidance(CG)[18]
thevisualqualityoftheinputimageandmeanwhilekeepthekey improvesqualityandgeneratesconditionalsamplesbyaddingthe
contentconsistent.Firstly,wedividetheinputimageintohigh- gradientofapre-trainedclassclassifier.Similarly,CLIPguidance
frequencyandlow-frequencyregionsbyutilizingahigh-passfilter. [39]utilizessimilarityscoresfromafine-tunedCLIPmodel[45].FreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MM’24,October28-November1,2024,Melbourne,Australia.
Low Magnitude High Magnitude
Noise Noise
Denoising Low Frequency
Filter Acutance
Regularization
High Magnitude Low Magnitude Blending Distribution
Noise Regularization
Noise
Adversarial
Denoising High Frequency Regularization
Input Input Filter Output
Noising Denoising
Output
(a)Conventional Schemes (b)FreeEnhance
Figure2:Theconventionalimageenhancementvia(a)noise-and-denoisingpipelinesuffersfromtradeoffsbetweencreativityandcontent-
consistency.Weintroduce(b)FreeEnhance,atuning-freeframeworkthatselectivelyaddslighternoiseinhigh-frequencyregionstopreserve
contentstructures,whileheaviernoiseisaddedinlow-frequencyregionstoenrichdetailsinsmoothareas.Moreover,threeregularizersare
employedtofurtherimprovevisualqualityduringdenoising.
Toavoidtrainingtheclassifier,classifier-freeguidance(CFG)[25] imageenablingcreativegenerationwhileperseveringattributesof
dropstheexplicitclassifierandmodelsanimplicitonebyomitting contents(Section3.2)inthenoisingstage.Andthenweintroduce
theconditionswithacertainprobabilityduringtraining.Andoth- thenoiseremovalusingadiffusionmodelincorporatedwiththree
ers[6,31,34,59]showthatthegradientofcanalsobeconsideredas gradient-basedterms.Theseterms,formulatedfromtheperspective
aguide.Forexample,ComposableDiffusion[31]adoptscomposed ofacutanceandvisualqualityofimages,respectively,regularizethe
guidancefrommultiapproximateenergy.ContrastiveGuidance predictednoiseandenhanceimagedetailsinthedenoisingstage
[59]utilizespositiveandnegativeprompttobuildacontrastivepair (Section3.3).Figure3depictstheframeworkofourFreeEnhance.
andregardsgradientofdifferenceasguidancetoguidesampling.
3.1 Preliminary
2.3 ImageEnhancementforHumanPreference
Diffusionmodelscreateimagesbyprogressivelyremovingnoise
Whilesharingsimilaritieswithtasksliketraditionimageenhance- throughaseriesofdenoisingsteps.Thisdenoisingprocessessen-
ment,imageenhanceondetailhasbeenstudiedmainlyonhow tiallyreversesanotherprocess(i.e.,noisingprocess)thataddsnoise
tostrikeabettertrade-offbetweendetailandcontentconsistency. toanimagesinapre-determinedtime-dependentmanner.Specifi-
Meanwhile,diffusionmodelshavebeenimplicitlyendowedwith cally,givenatimestep𝑡 ∈{𝑇,𝑇−1,...,1}andthenoise𝜖 𝑡,thenoisy
imagetheenrichingdetailcapability.Basedonhowthiscapability imageiscreatedas𝑥 𝑡 =𝛼 𝑡𝑥+𝜎 𝑡𝜖 𝑡,where𝑥 istheoriginalimage,
isbuilt,wecanbroadlycategorizeexistingstudiesintothreeclasses. 𝛼 𝑡 and𝜎 𝑡 areparametersdeterminedbythenoisescheduleandthe
Thefirstsolutionistherefinementmodel.SDXLrefiner[42]train timestep𝑡.Toperformthedenoisingprocessforimagesynthesis,a
aseparateLDMmodelinthesamelatentspace.Itcanimprove commonchoicefordiffusionmodelsislearninganeuralnetwork
qualityofdetailedbackgrounds.Thesecondistheupscale-then- 𝜖 𝜃 thatattemptstoestimatedthenoise𝜖 𝑡,where𝜃 isobtainedby:
t ci rl ee am tee dth eto ad il. sR oe nce lon ct as ltu red gie ios n[7 a, n2 d0 c, a2 n3] ks eh eo pw cot nh ta et ni tt .s Sc ta ap rta ib ni gli ft ry ot mo arg 𝜃minE 𝑡∼U(1,𝑇),𝜖𝑡∼N(0,I)||𝜖 𝑡 −𝜖 𝜃(𝑥 𝑡;𝑡,𝑦)||2, (1)
upscalinganimage,MultiDefusion[7]tiletheimageintoasetof and𝑦isanoptionalconditioningsignalliketextprompt.Oncethe
patches,thenproposesfusingmultiplediffusionpathsonthese model𝜖 𝜃 istrained,imagescanbegeneratedbystartingfromnoise
patches,resultinginhigh-resolutionimages.However,itsuffers 𝑥
𝑇
∼N(0,I)andthenalternatingbetweennoiseestimationand
fromtheobjectrepetitionissuesduetothepromptindependently
noisyimageupdating:
guidingthedenoisingofeachpatch.Thethird[3,27]involvesper-
formingasecondarypredictiononregionsthatarehardtogenerate 𝜖ˆ𝑡 =𝜖 𝜃(𝑥 𝑡;𝑡,𝑦), 𝑥 𝑡−1=𝑢𝑝𝑑𝑎𝑡𝑒(𝑥 𝑡,𝜖ˆ𝑡,𝑡), (2)
duringthedenoisingprocess.Self-attentionguidance(SAG)[27]
wheretheupdatingcanbeperformedbyDDPM[24],DDIM[53],
utilizesadversarialblurringontheregionsofdenoisingmodelfo-
DPM[52]orothersamplingalgorithms.Usingthereparameteriza-
cused,thenleveragesthesecondarypredictednoiseofblurredone
tiontrick[24],wecanfurtherobtainanintermediatereconstruction
toguidethesamplingdirectionoftheoriginalone.Itcaneffec- of𝑥 0atatimestep𝑡,denotedas𝑥ˆ𝑡→0.Toimprovetherealismand
tivelyimprovegenerationquality.PerturbedAttentionGuidance
faithfulnesstotheconditioningeneratedimages,SDEdit[36]and
(PAG)[3]introduceaperturbedattentionlayerwhichreplacesthe
SDXL[42]utilizeanoising-and-denoisingprocess,whichfirstadds
attentionmatrixwithanidentitymatrixtoimprovequality. randomnoisecorrespondingtothetimestep𝑡 0intotheinputimage
andthensubsequentlydenoisestheresultingimage.Thehyper-
3 Method parameter𝑡 0 canbetunedtotradeoffbetweenconsistencyand
Thissectionfirstreviewsthediffusionmodelsandthestandard creativity:withasmaller𝑡 0leadingtoamorecontent-consistent
schemesofnoising-and-denoisingprocessforimageenhancement butlessdetailedgeneratedimage.Thisapproachtreatseveryre-
withouttheconsiderationofcontentconsistency(Section3.1).Next, gionoftheinputimagethesame.Itaddsrandomnoisewiththe
we describe how FreeEnhance properly add noise on the input sameintensityattimestep𝑡 0acrosstheentireimage,disregardingMM’24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
Noising Stage Denoising Stage
High/Low
Frequency Filter
Acutance
Regularization
GGS
Creative Streamx t ×x (t− T1 −t 0) x tc 0 M l M h RD egis ut lr aib riu zt aio tin o n
Input
DDIM Inversion
x t0
Adversarial
x t−1 Output
x t Regularization
xs
Stable Stream t0 ×t
0
Figure3:AnoverviewofourTuning-FreeImageEnhancement(FreeEnhance)framework.TheprocessofFreeEnhancebeginswithaninput
image𝑥,whichundergoesatwo-streamnoisingschemetoadaptivelyaddnoiseinto𝑥.Thecreativesteamaddsstrongnoisewhichisthen
partiallyremovedbyadiffusionmodelwithgradient-guidedsampling(GGS),resulting𝑥 𝑡𝑐.Andinthestablestream,lightnoiseisattached
withtheinputimageusingDDIMinversionstrategy,obtaining𝑥 𝑡𝑠 .Then𝑥 𝑡𝑐 and𝑥 𝑡𝑠 areadap0 tivelyblendedaccordingtothehigh/lowfrequency
0 0 0
map𝑀 ℎ/𝑀 𝑙 producedbyfrequencyfilteringof𝑥,resultingthenoisyimage𝑥𝑡 0.Then,𝑥𝑡 0isfedintodiffusionmodelswhichisconstrained
bythreeregularizers,whicharedevisedfromtheperspectivesofimageacutance,noisedistribution,andadversarialdegeneration,inthe
denoisingstagetoproducetheenhancedversionoftheinputimage.
the varying needs of different areas. Some regions might bene-
fitfromcreativelyintroduceddetails,whileothersmightrequire
meticulouspreservationofexistingcontent.Astheresult,thenaive
noising-and-denoisingprocessstrugglestofindabalance,either
over-editingimagesorleavingthemlackingindetail.
3.2 NoisingStage
Toalleviatetheseissues,ourFreeEnhancetailorsthenoisingprocess Input Output w/o calibration Output with calibration
followinganintuitiveidea:High-frequencyareas,richinedgesand Figure4:Comparisonbetweenimagesgeneratedfromcomposited
corners,shouldreceivelighternoisetosafeguardtheiroriginal noisyimagewithandwithoutthedistributioncalibration.Thecolor
patterns. Conversely,low-frequency regions are expectedto be shift/fadingcanbeobservedontheoutputwithoutthecalibration.
exposedtostrongernoise,promotingcreativedetailgenerationand
refinement.Consideringtheassumptionofdiffusionmodelsthatall Forthestablestream,weemploytheDDIMinversion[37]to
regions/pixelsofnoisyimagessharethesamenoisedistribution(i.e., addnoiseinto𝑥 andobtainthenoisyimage𝑥 𝑡𝑠 .Itensuresthatthe
theintensityofnoise),weproposeatwo-streamnoisingschemeto contentsin𝑥canbereconstructedfrom𝑥 𝑡𝑠 wit0 hhighfidelitywhen
0
adaptivelyaddnoiseintotheoriginalimage.Thecreativestream weutilizedadeterministicsamplingalgorithmlikeDDIM.
involveshigherintensityofnoisetoenrichimagedetailsandthe Oncetwonoisyimages𝑥 𝑡𝑐 and𝑥 𝑡𝑠 areproducedbythecreative
0 0
stablestreamintroducesweakernoisetomaintaincontentfidelity. andstablenoisingstreams,weadaptivelyblendthetwonoisyimage
Forthecreativestreamwhichisdivisedforcreativedetailgen- accordingtothefrequencyofimageregions.Forthehigh-frequency
eration,arandomnoisecorrespondingtotimestep𝑇 isaddedinto imageregionslocalizedbythemap𝑀 ℎ,wedirectlyinvolve𝑥 𝑡𝑠 to
theinputimage𝑥,obtainingthenoisyimage𝑥 𝑇𝑐 = 𝛼 𝑇𝑥 +𝜎 𝑇𝜖 𝑇. maintain the the content structure, resulting𝑥 𝑡ℎ = 𝑀 ℎ𝑥 𝑡𝑠 . A0 nd
Thenadiffusionmodelisutilizedtoiterativelydenoising𝑥 𝑇𝑐 till forthelow-frequencyregionswhicharemarked0 by𝑀 𝑙 =10 −𝑀 ℎ,
thetimestep𝑡 0andobtain𝑥 𝑡𝑐 .Althoughthevariantoftheinput weconductanalpha-compositingfor𝑥 𝑡𝑐 and𝑥 𝑡𝑠 usingatradeoff
imageisencouragedduringth0 edenoising,westillneedtoalignthe parameter𝜏: 0 0
s intru hc igtu hr -a frl ee qle um ene cn yts r( eo gf it oen nsd )e ote fr 𝑥m 𝑡𝑐in we id thby the od sg ee os fa tn hd ec io nr pn ue trs imlo ac ga ete 𝑥d
.
𝑥 𝑡𝑙
0
=𝑀 𝑙(𝜏𝑥 𝑡𝑠 0+(1−𝜏)𝑥 𝑡𝑐 0). (5)
0 However,thedistributionofweightedaverageoftwonoisyim-
Thusweutilizethegradient-guidedsampling[13,14,16]tointro-
𝜎2
d pu roc ce ec so sn 𝑥d 𝑇𝑐it →ion 𝑥in 𝑡𝑐 0g io nn thau isx nil oia ir sy ini gnf so trr em aa mn .t Tio hn e[ g1 r3 a] df io er nt th -ge ud ide en doi ss ain mg
-
a dg ise ts rii bs uN tio( n𝛼 𝑡 N𝑥, (2 𝛼𝜏 𝑡2 𝑥−𝑡 ,2𝜏 𝜎+ 𝑡21 I)I) o,w
f
th hi ech dv iffio ul sa it oe nst ph re och ey sp s,ot rh ee susi lz tie nd gp sr uio br
-
plingutilizesguidancegeneratedfrompre-definedenergyfunctions optimalimagegeneration(e.g.,over-smoothsurface).Tomitigate
𝑔(𝑥 𝑡;𝑡,𝑦)toalteringtheupdatedirection𝜖ˆ𝑡:
this,werescalethecompositednoisyimageusingascalefactor
√
𝜖ˆ𝑡 =𝜖 𝜃(𝑥 𝑡;𝑡,𝑦)+𝜆𝜎 𝑡▽ 𝑥𝑡𝑔(𝑥 𝑡;𝑡,𝑦), (3) 1/ 2𝜏2−2𝜏+1tocalibratethedistribution.Figure4demonstrates
thecomparisonbetweenimagesgeneratedfromcompositednoisy
orrevisethesamplingresult𝑥 𝑡−1:
imagewithandwithoutthedistributioncalibration.
𝑥 𝑡∗ −1=𝑥 𝑡−1−𝜆▽ 𝑥𝑡𝑔(𝑥 𝑡;𝑡,𝑦), (4)
where𝜆istheweightoftheadditionalguidance.Herewedefinethe 3.3 DenoisingStage
energyfunction𝑔(𝑥 𝑡;𝑡,𝑦)=𝑀 ℎ||𝑥−𝑥ˆ𝑡→0||2forgradient-guided Withthenoisyimageproducedfromtwo-streamnoising,wethen
sampling,where𝑀 ℎisabinarymapobtainedbyhigh-passfiltering conductthedenoisingprocessandpresentthreetargetproperties
[57]ontheinputimagetoidentifyhigh-frequencyregions. as constraints to regularize the predict noise and/or revise theFreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MM’24,October28-November1,2024,Melbourne,Australia.
islarge.Buildinguponthisintuition,weregularizethedenoising
processviapunishingthegapofdistribution:
L𝑑𝑖𝑠𝑡 =||1−F𝑣𝑎𝑟(𝜖 𝜃(𝑥 𝑡;𝑡,𝑦))|| 2, (8)
whereF𝑣𝑎𝑟 caluatesthevarianceofthepredictednoise.
AdversarialRegularization.Motivatedbytheself-attentionguid-
ancefordiffusionmodels[27],weincorporateanadversarialregu-
larizationforthedenoisingstageofFreeEnhancetoavoidgenerat-
400 300 200 100 0 400 300 200 100 0
Timesteps Timesteps ingblurredimages.Specifically,wedefinetheF𝑏𝑙𝑢𝑟 asagaussian
Figure5:Thestatisticsofthenoise𝜖 𝜃(𝑥𝑡;𝑡,𝑦)predictedbyadiffu- blurfunctionanddevisetheobjectiveasfollws:
sionmodel.Giventhenoisyimagefromthenoisingstage,thered L𝑎𝑑𝑣 =||𝑥ˆ𝑡→0−F𝑏𝑙𝑢𝑟(𝑥ˆ𝑡→0)|| 2. (9)
scattersisestimatedduringthedenoisingprocessusingSDXLand
thegrayonesrepresenttheidealvaluesacrossdifferenttimesteps. RegularizatingtheDenoising.Withthehelpofthethreeregular-
izations,weadditionallyinsertarevisingstepattheendofupdating
updatednoisyimagesfromtheaspectsofimageacutanceandnoise ineachdenoisingiteration.Specifically,thesamplingresult𝑥
𝑡−1
distribution.Suchconstraintsareformulatedfromascore-based ineachdenoisingoperationisalteredby𝑥 𝑡∗ −1:
perspective[4]ofdiffusionmodelsandleveragethecapabilityof
themwhichcanadaptoutputsbyguidingthesamplingprocess.
𝑥 𝑡∗ −1=𝑥 𝑡−1−𝜌 𝑎𝑐𝑢▽ 𝑥𝑡L𝑎𝑐𝑢−𝜌 𝑑𝑖𝑠𝑡▽ 𝑥𝑡L𝑑𝑖𝑠𝑡−𝜌 𝑎𝑑𝑣▽ 𝑥𝑡L𝑎𝑑𝑣, (10)
Acutance Regularization. In photography, acutance refers to where𝜌 𝑎𝑐𝑢 =4,𝜌 𝑑𝑖𝑠𝑡 =20,and𝜌 𝑎𝑑𝑣 =0.3arethetradeoffparame-
theperceivedsharpnessassociatedwiththeedgecontrastofan tersdeterminedthroughexperimentalstudies.
image[35].Owingtocharacteristicsofthehumanvisualsystem,
imageswithhigheracutancetendtoappearsharper,despitethe 4 Experiments
factthatanincreaseinacutancedoesnothavetoenhanceactual
WeempiricallyverifythemeritofFreeEnhancefortuning-freeim-
resolutionofimages.Hereweutilizetheacutanceof𝑥ˆ𝑡→0,whichis
ageenhancementonthepublicdatasetHPDv2[60]followingthe
theintermediatereconstructionof𝑥 0atthetimestep𝑡,toregularize
evaluationprotocol[15,30]intermsofthequantitativemetricsand
thedenoising.Specifically,weutilizetheSobelkerneltoestimate
qualitativehumanpreference.Wefirstintroducethedataset,quan-
themagnitudeofthederivativeofbrightnessconcerningspatial
titativemetrics,baselineapproaches,andimplementationdetailsof
variationsof𝑥ˆ𝑡→0,denotedasF𝑎𝑐𝑢(𝑥ˆ𝑡→0).Toencourageahigher
ourFreeEnhance(Section4.1).Next,weelaboratethecomparisons
acutance,theobjectiveofacutanceregularizationis:
betweenFreeEnhanceandbaselinesonbothquantitativeandquali-
𝐻,𝑊 tativeresults(Section4.2),followedbythecomparationtoMagnific
1 ∑︁
L𝑎𝑐𝑢 =− 𝐻𝑊 F𝑎𝑐𝑢(𝑥ˆ𝑡→0) (𝑖,𝑗) , (6) AI(Section4.3).WefurtheranalyzethedesignsinourFreeEnhance
𝑖=0,𝑗=0 viaablationstudies(Section4.4)andassessthegeneralizationca-
where𝐻,𝑊 representthespatialsizeofthenoisyimageand(𝑖,𝑗) pabilityofFreeEnhanceundertwoscenarios(Section4.5).
aretheindicesofthespatialelement.Thisformulationassumes
4.1 ExperimentalSettings
thatallspatiallocationsinthegeneratedimagesareintendedto
be’sharp’.Butinpractice,emphasizingalltheedges/cornersofthe Dataset.HumanPreferenceDatasetv2(HPDv2)[60]isalarge-
inputimagemayintroduceunpleasantstructuresintheflatregions scaledatasetofhumanpreferencesforimagesgeneratedfromtext
(e.g.,skyandmetalsurfaces)andintricateregions(e.g.,treesand prompts.Itcomprises798,090humanpreferencechoiceson433,760
bushes), impacting human preferences. To tackle this issue, we pairsofimages.HPDv2providesasetofevaluationpromptsthat
extendtheformulationinEq.6withabinaryindicator𝑉(·): involvestestingamodelonatotalof3,200prompts,evenlydivided
𝐻,𝑊 into4styles:Animation,Concept-Art,Painting,andPhoto.For
1 ∑︁
L𝑎𝑐𝑢 =− 𝐻𝑊 𝑉(F𝑎𝑐𝑢(𝑥ˆ𝑡→0) (𝑖,𝑗))F𝑎𝑐𝑢(𝑥ˆ𝑡→0) (𝑖,𝑗) . (7) eachtypeofevaluationprompt,HPDv2providesthecorresponding
𝑖=0,𝑗=0 benchmarkimagesgeneratedbyvariousmainstreamtext-to-image
where𝑉(·)=1whentheinputvaluefallswithinthe35thand65th generativemodels.Hereweexploitthegroupofbenchmarkimages
percentilesofF𝑎𝑐𝑢(𝑥ˆ𝑡→0).Accordingly,ouracutanceregularization generatedbySDXL-Base-0.9astheinputsofimageenhancement
introduces additional details into the images while minimizing approachestovalidatethemeritofourproposal.
unpleasantstructures,enhancingtheoverallgenerationquality. Metrics.Non-referenceimagequalityassessment(NR-IQA)isa
DistributionRegularization.Consideringtheinevitabilityofgen- metricforevaluatingtheimagequalitywithoutneedingitspristine
eralizationerror,thenoisepredictedbydiffusionmodels𝜖 𝜃(𝑥 𝑡;𝑡,𝑦) versionforcomparison.WeemploythreekindsofNR-IQAmetrics
maynotfollowaGaussiandistributionN(0,I),particularlywhen forquantitativeevaluation,includingMANIQA[64],CLIPIQA+[56]
wedirectlyutilizeadiffusionmodeltogenerateimagesfromthe andMUSIQ[29].Sinceeachofthemhasmultiplepubliclyavail-
compositednoisyimageproducedinournoisingstage.Tovalidate ableversions,involvingfine-tuningfromdifferentdatasets(e.g.,
thisassumption,weanalyzemorethan3,000imagesandsummarize KADIDandKonIQ)oremployingdifferentmodels(e.g.,ResNetand
thedistributionofpredictednoiseduringthedenoisingprocessin ViT),hereweevaluateimagequalityusingmultiplemetricsfrom
Figure5.Weobservethatthemeanvaluesapproachzeroacross MANIQA(3versions),CLIPIQA(3versions),andMUSIQ(2ver-
differenttimestepsduringdenoising,whilethedifferencebetween sions).HumanPreferenceScorev2(HPSv2)[60]isascoringmodel
theactualvariancevaluesand1isnonnegligiblewhenthetimestep thattrainedontheHPDv2datasettopredicthumanpreferences
naeM
ecnairaVMM’24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
Table1:QuantitativecomparisonsonHPDv2benchmarkimagesgeneratedbySDXL-Base-0.9forimageenhance.Wemarkthebestresultsin
bold.†meansrunwithprompts.ForMANIQAandMUSIQ,wereporttheirsub-versionsfine-tunedondifferentdatasets.ForCLIPIQA+,we
reportitssub-versionswithdifferentbackbone.The*denotesthesub-versionfine-tunedwithbothpositiveandnegativeprompts.TheHPSv2
scoreadoptedv2.1version.
MANIQA↑ CLIPIQA+↑ MUSIQ↑
Method HPSv2↑
KonIQ KADID PIPAL ResNet50 ResNet50* ViT-L KonIQ SPAQ
SDXL-base[42] 0.3609 0.5821 0.5721 0.5688 0.4074 0.4267 63.7948 62.1716 27.39
SDXL-refiner[42] 0.3305 0.6006 0.5674 0.5671 0.3658 0.3636 61.7321 60.8410 27.27
SAG[27] 0.3933 0.6088 0.6154 0.6311 0.4450 0.4583 66.7950 65.1907 28.48
Fooocus[2] 0.4180 0.6359 0.6096 0.6130 0.4612 0.4667 68.0095 65.7813 28.31
DemoFusion†[20] 0.2747 0.5414 0.5129 0.5085 0.3424 0.3607 56.5470 58.5090 27.87
FreeU[51] 0.4194 0.6272 0.5938 0.6189 0.4649 0.4703 68.0083 66.2340 28.88
FreeEnhance 0.4122 0.6611 0.6332 0.6535 0.4929 0.4901 68.3928 66.8653 29.32
Input FreeEnhance Input SDXL refiner SAG DemoFusion FreeU FreeEnhance
Figure6:QuantitativecomparisonsofimagesenhancedbydifferentapproachesonHPDv2benchmark.Theregionsinredboxesarepresented
inzoom-inviewtoeasethecomparison.
onthegeneratedimages.WeutilizetheHPSv2toscoretheimages All the mentioned baselines are grouped into three directions:
before/afterenhancementtoverifythequalityimprovement. plainnoising-and-denoisingwithdiffusionmodel(SDXL-baseand
ImplementationDetails.WeusethebasemodelofStableDiffu- SDXL-refiner[42]),upscale-then-tileoperation(DemoFusion[20])
sionXL(SDXL-base)implementedinHuggingFaceTransformer andsamplingwithguidancescheme(SAG[27],Fooocus[2],and
andDiffuserlibraries[55]asthediffusionmodelforimageenhance- FreeU[51]).Notethatallmethods,exceptfor“SDXL-refiner”,use
ment,unlessotherwisestated.Hence,thenoising-and-denoising thepre-traineddiffusionmodelSDXL-base.“SDXL-refiner”utilizes
processisconductedinthelatentfeaturespace.Thehigh/lowfre- customweights.Ingeneral,ourFreeEnhanceapproachconsistently
quencyregionsoftheinputimagesarerecognizedbythefiltering achievesbetterimagequalitycomparedtothesebaselines.Notably,
proposedinDR2[57].Theresolutionbeforeandafterenhancement FreeEnhanceattainsascoreof29.32ontheHPSv2metricwithout
is1,024×1,024andtheoriginalpromptsofthebenchmarkimages anydiffusionmodelparametertuning.Comparedtothebaseline
fromHPDv2arenotinvolved.Duringthenoising-and-denoising of SDXL-base, the SDXL-refiner produces unsatisfactory image
processofFreeEnhance,theinferencestepsis100,withaguidance enhancementresultsduetotherelativelyhighintensityoftheat-
scaleof1.0.Thehyper-parameter𝑡 0 thatindicatesthestrength tachednoisewhichisconstrainedonthefirst200(discrete)noise
ofattachednoiseissetas500.Allexperimentsareconductedon scalesduringthetrainingofdiffusionmodel.Benefittingfromthe
NVIDIARTX3090GPUsandIntelXeonGold6226RCPU. self-attentionguidanceemployedduringnoiseremoval,SAGand
Fooocusexhibitbettergenerationqualityandhaveperformance
4.2 PerformanceComparison gainonNR-IQAmetricsandtheHPSv2scores(28.48/28.31vs.27.39).
DemoFusionhasdecentperformancesonbothNR-IQAmetricsand
QuantitativeResults.WecompareourFreeEnhancewithsev-
HPSv2.Wespeculatethatthismaybetheresultoftheemployed
eralopen-sourceoff-the-shelfapproachesintermsofthreegroups
ofNR-IQAmetricsandonehumanpreferencemetricinTable1.FreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MM’24,October28-November1,2024,Melbourne,Australia.
Input Magnific AI FreeEnhance Input Magnific AI FreeEnhance
Figure7:ComparisonsofimagesenhancedbyMagnificAIandourFreeEnhance.Regionsinredboxesarepresentedinzoom-inview.
100 benchmark.Thisspeedisgenerallyconsideredacceptableincom-
Magnific AI
80 FreeEnhance mercialimageenhancementproducts(around22secondsperimage
byMagnificAI).WecanalsodevelopafasterversionofFreeEn-
60 52 56
48 hancebyreducingtheoverallinferencestepsofthenosing-and-
44
40 denoisingprocessanddisablingtheregularizersatintervalsthrough-
out the denoising stage. This version of FreeEnhance takes 5.8
20
secondsperimageandachievesa29.23HPSv2score.
0
GPT-4 Human
4.3 ComparisonwithMagnificAI
Figure8:ComparisonsbetweenFreeEnhanceandMagnificAIwith
regardtoGPT-4andhumanpreferenceratios. Figure7presentsvisualizationsofimageenhancementresultsbe-
tweenFreeEnhanceandMagnificAI,renownedforitsadvanced
imageenhancementcapabilities.Inthefirstcase,MagnificAIfalls
shiftedcropsamplingwithdelatedsamplingwhichintroducesun-
shortinprovidingadditionaldetailedstructureforthebuilding
naturallocaltextures.FreeUconductsthedenoisinginafrequency
situatedontheleftsideoftheimage.Conversely,FreeEnhance
decoupledmanner,whichleadstobetterenhancementresultson
seamlesslyenhancesthevisualqualityandrealismoftheexternal
bothNR-IQAandHPSv2.FreeEnhance,whichsimultaneouslycon-
facadesofthebuilding,whileadeptlypreservingboththecontent
siders both ways to add and remove noise to the input images
andthedepthoffield.
forqualityimprovement,obtainsthehighestHPSv2score29.32,
We further conduct a human study to examine whether the
surpassingthebestcompetitorFreeUby0.44.Theresultsdemon-
enhancedimagesfromFreeEnhancebetterconformtohumanpref-
stratetheeffectivenessoffrequency-adaptivenoiseadditionand
erencesthanthatgivenbyMagnificAI.Specifically,werandomly
regularizeddenoisingforimageenhancementbydiffusionmodels.
sample100promptsfromHPDv2andgenerate1,024×1,024images
QualitativeResults.Wethenvisuallyexaminetheenhancement
usingSDXL-base.Werecruited50evaluators,including25males
qualityofourproposalbycomparingFreeEnhancewithfourbase-
and25females,withdiverseeducationalbackgroundsandages.
lines:SDXL-refiner,SAG,DemoFusion,andFreeUonthreeinput
Eachevaluatorwastaskedtoselectthepreferredimagefromtwo
images.Figure6showsthequalitativeresultsoftheenhancedim-
optionsgeneratedbydifferentparadigmsbutoriginatingfromthe
ages.Tobetterillustratetheimagedetails,weprovidezoom-in
sameimage.Evaluatorswereencouragedtochoosetheimagethat
viewsofimagepatches.Overall,alltheapproachessuccessfully
bestsatisfiedtheirpreferences.Wealsoconductthesameevalu-
modifytheinputimages,andourFreeEnhancecreatesthemost
ationusingtheGPT-4.Figure8illustratesthepreferenceratios.
plausiblelocaltexturesanddetailsintheimageswhilemaintain-
Overall,FreeEnhanceachievescompetitiveresultsbothonhuman
inggoodcontentconsistencybetweentheinputimagesandthe
andGPT-4study.
enhancedones.Takingtheimageinthefirstrowasanexample,
FreeEnhancenicelyprovidesmoredetailedstructures,clearbound-
4.4 ExperimentalAnalysis
aries,andrealisticmaterialfortheheadwear,whilepreservingits
shape and characteristics. In contrast, the SDXL-refiner fails to AblationStudy.WeinvestigatehoweachdesigninourFreeEn-
reconstructtheinputimage,resultinginacorruptedoutcome.SAG hanceinfluencesthevisualqualityoftheenhancedimages.Table2
andFreeUproducemoderatemodificationsandaddseveraldetail detailstheperformances(i.e.,HPSv2scores)acrossdifferentablated
structures,butstilllosethesparklingpointsattheleftsideofthe runsofFreeEnhance.Westartfromabasicnoising-and-denoisng
headwear.DemoFusiondramaticallychangestheheadweartoa schemeusingtheSDXL-basediffusionmodel,whichachieves27.39
humanface,whichisnotdesiredinimageenhancement. oftheHPSv2score.Next,bysolelyusingthenoisingstageofFreeEn-
InferenceSpeed.TheFreeEnhancetakes16.3secondsperimage hancewhichadaptivelyaddlightnoiseonhigh-frequencyregions
onanA100GPU,achievinga29.32HPSv2scoreontheHPDv2 and strong noise on low-frequency regions, we observe a clear
)%(
etarniWMM’24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
A bicycle with a basket next to a brick wall The motorcycle is tilting as he turns through a cave A man riding a bike down a dirt road
SD 1.5 SAG PAG FreeEnhance SD 1.5 SAG PAG FreeEnhance SD 1.5 SAG PAG FreeEnhance
Figure9:Comparisonsofimagessynthesizedbyvariousdenoisingapproachesinthetext-to-imagescenario,usingpromptsinHPDv2.
Table2:AblationstudyofeachdesigninFreeEnhanceontheHPDv2
benchmark.ThenotationsDist.,Acu.andAdv.denotesdistribution,
acutanceandadversarialregularizations,respectively.
NoisingStage DenoisingStage
HPSv2↑
Stable Creative Adaptive
Dist. Acu. Adv.
stream stream Blending
Ø Ø Ø Ø Ø Ø 27.39
¸ Ø Ø Ø Ø Ø 28.21
¸ ¸ Ø Ø Ø Ø 27.78
¸ ¸ ¸ Ø Ø Ø 28.63
¸ ¸ ¸ ¸ Ø Ø 28.71
Input FreeEnhance Input FreeEnhance
¸ ¸ ¸ ¸ ¸ Ø 28.92 Figure10:ExamplesofnaturalimagesenhancedbyFreeEnhance.
¸ ¸ ¸ ¸ ¸ ¸ 29.32
Table5:ComparisonofHPSv2scoresforimagessynthesizedusing
Table3:Studyofnoiseintensity(determinedby𝑡 0)onHPDv2. variousdenoisingapproachesinthetext-to-imagescenario,employ-
𝑡 0 300 400 500 600 700 ingSD1.5ontheHPDv2benchmark.
Method SD1.5 SAG[27] PAG[3] FreeEnhance
HPSv2↑ 20.30 21.63 29.32 28.80 28.20
HPSv2↑ 24.61 24.76 25.02 25.26
Table4:ComparisonsofHPSv2scoresofimagesproducedbydiffer-
(25.26),surpassingboththevanilladenoisingschemesofSD1.5
entdiffusionmodelswith/withoutFreeEnhance.
(24.61)andtheadvancedapproachesSAG(24.76)andPAG(25.02).
FreeEnhance SDXL-base SDXL-refiner DreamshaperXL WefurthershowcasethreeexamplesinFigure9.Overall,allfour
without 27.39 27.27 29.52 methods correctly align the prompt, and FreeEnhance presents
with 29.32 29.15 30.06 superiorvisualquality,withtheevidenceoftheclearerdepictionof
bricksonthewall(1strow),andthemorerealisticrepresentationof
performanceboost.Wethenleveragethethreeregularizersinthe dirtandgravelblocksontheroad(2ndand3rdrows).Theseresults
denoisingstageinturn.TheHPSv2scoreisconsistentlyboosted againhighlightthegeneralizationcapabilityofFreeEnhance.
upandfinallyreaches29.32.InTable3,theresultsofthenoise NaturalImageEnhancement.Hereweempiricallyevaluatethe
intensitydeterminedbythehyper-parameter𝑡 0,whereahigher capability of FreeEnhance on natural images. We select images
valuesignifiesahighernoiseintensity,showthatFreeEnhance fromtheLAION-5Bdataset[49]andenhancetheirqualityusing
performsoptimallyatamoderatenoiseintensity(𝑡 0=500=0.5𝑇). ourFreeEnhance.Figure10showcasesfourpairsofenhancement
Effect of the diffusion models. To investigate the impact of results.Forinstance,theleavesofthetreeinthefirstcasebecome
thediffusionmodelonimageenhancement,weutilizethreediffu- clearerafterenhancement.TheresultsindicatethatFreeEnhance
sionmodels:SDXL-base,SDXL-refiner,andDreamshaperXL[1],to iswell-suitedforrefiningnaturalimages.
executethenoising-and-denoisingprocesswithandwithoutour
proposedFreeEnhance.Table4summarizestheHPSv2scoresofthe 5 Conclusion
imagesproducedbyvariousdiffusionmodelswith/withoutFreeEn- WehavepresentedFreeEnhanceforimageenhancementbyexploit-
hance.TheresultsconstantlyverifythatFreeEnhancegenerates ingtheoff-the-shelftext-to-imagediffusionmodels.Particularly,
superiorimagesregardlessofthemodelused. FreeEnhanceformulatesimageenhancementasatwo-stageprocess,
whichfirstlyattachesrandomnoisetotheinputimage,followedby
4.5 Applications
noisereductionthroughthediffusionmodel.Inthenoisingstage,
ToassessthegeneralizationcapabilityofFreeEnhance,weconduct wedevidetheinputimageintohigh/lowfrequencyregions,adding
additionalexperimentsunderdifferenttwoscenarios. light/strongrandomnoisetopreserveexistingcontentstructures
Text-to-ImageGeneration.Tovalidatethatthedenoisingstagein whileenhancingvisualdetails.Inthedenoisingstage,weintro-
FreeEnhancecanbesimplyappliedtotheGaussianrandomnoise ducethreegradient-basedregularizationstorevisethepredicted
for image generation without the reference image, we perform noise,leadingtotheimprovementoftheoverallimagequality.The
text-to-imagegenerationusingourFreeEnhance.Specifically,we resultsontheimagegenerationbenchmarkdemonstratesuperior
synthesizeimagesforthepromptsinHPDv2benchmarkusingthe visualqualityandhumanpreferenceoverstate-of-the-artimage
stablediffusion1.5withdifferentdenoisingapproaches.Thescale enhancementapproaches.Furthermore,theFreeEnhancemodel
oftheclassifier-freeguidanceisfixedas7.5.Table5detailsthe isreadilyapplicabletoenhancenaturalimagestakenbytheend
comparisonresults.FreeEnhanceachievesthehighestHPSv2score users,enablingawiderangeofreal-lifeapplications.FreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MM’24,October28-November1,2024,Melbourne,Australia.
6 Acknowledgments
[26] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,Mohammad
Norouzi,andDavidJFleet.2022.Videodiffusionmodels.InNeurIPS.
ThisworkwaspartiallysupportedbyNationalNaturalScience
[27] SusungHong,GyuseongLee,WooseokJang,andSeungryongKim.2023. Im-
FoundationofChina(No.62032006,62172103).Thecomputations provingsamplequalityofdiffusionmodelsusingself-attentionguidance.In
inthisresearchwereperformedusingtheCFFFplatformofFudan ICCV.
[28] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,
University. InbarMosseri,andMichalIrani.2023.Imagic:Text-basedrealimageeditingwith
diffusionmodels.InCVPR.
[29] JunjieKe,QifeiWang,YilinWang,PeymanMilanfar,andFengYang.2021.Musiq:
References Multi-scaleimagequalitytransformer.InICCV.
[30] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig,
[1] 2024.Dreamshaperxl. https://civitai.com/models/112902/dreamshaper-xl PengchuanZhang,andDevaRamanan.2024.EvaluatingText-to-VisualGenera-
[2] 2024.Fooocus. https://github.com/lllyasviel/Fooocus tionwithImage-to-TextGeneration.arXivpreprintarXiv:2404.01291(2024).
[3] DonghoonAhn,HyoungwonCho,JaewonMin,WooseokJang,JungwooKim, [31] NanLiu,ShuangLi,YilunDu,AntonioTorralba,andJoshuaBTenenbaum.2022.
SeonHwaKim,HyunHeePark,KyongHwanJin,andSeungryongKim.2024. Compositionalvisualgenerationwithcomposablediffusionmodels.InEuropean
Self-RectifyingDiffusionSamplingwithPerturbed-AttentionGuidance.arXiv ConferenceonComputerVision.Springer,423–439.
preprintarXiv:2403.17377(2024). [32] FuchenLong,ZhaofanQiu,TingYao,andTaoMei.2024.Videodrafter:Content-
[4] ThiemoAlldieck,NikosKolotouros,andCristianSminchisescu.2024. Score consistentmulti-scenevideogenerationwithllm.arXivpreprintarXiv:2401.01256
Distillation Sampling with Learned Manifold Corrective. arXiv preprint (2024).
arXiv:2401.05293(2024). [33] AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,
[5] JacobAustin,DanielDJohnson,JonathanHo,DanielTarlow,andRianneVan andLucVanGool.2022. Repaint:Inpaintingusingdenoisingdiffusionproba-
DenBerg.2021.Structureddenoisingdiffusionmodelsindiscretestate-spaces. bilisticmodels.InCVPR.
InNeurIPS. [34] GraceLuo,TrevorDarrell,OliverWang,DanBGoldman,andAleksanderHolyn-
[6] ArpitBansal,Hong-MinChu,AviSchwarzschild,SoumyadipSengupta,Micah ski.2024.ReadoutGuidance:LearningControlfromDiffusionFeatures.InCVPR.
Goldblum,JonasGeiping,andTomGoldstein.2024. UniversalGuidancefor [35] HenriMaître.2015.ImageQuality.
DiffusionModels.InICLR. [36] ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
[7] OmerBar-Tal,LiorYariv,YaronLipman,andTaliDekel.2023.MultiDiffusion: Ermon.2022. Sdedit:Imagesynthesisandeditingwithstochasticdifferential
FusingDiffusionPathsforControlledImageGeneration.InICML. equations.InICLR.
[8] JamesBetker,GabrielGoh,LiJing,†TimBrooks,JianfengWang,LinjieLi,†Lon- [37] RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or.2023.
gOuyang,†JuntangZhuang,†JoyceLee,†YufeiGuo,†WesamManassra,†Praful- Null-textinversionforeditingrealimagesusingguideddiffusionmodels.In
laDhariwal,†CaseyChu,†YunxinJiao,andAdityaRamesh.[n.d.].ImprovingIm- CVPR.
ageGenerationwithBetterCaptions. https://api.semanticscholar.org/CorpusID: [38] ChongMou,XintaoWang,LiangbinXie,YanzeWu,JianZhang,Zhongang
264403242 Qi, Ying Shan, and Xiaohu Qie. 2023. T2I-Adapter: Learning Adapters
[9] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,Maciej to Dig out More Controllable Ability for Text-to-Image Diffusion Models.
Kilian,DominikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts, arXiv:2302.08453[cs.CV]
etal.2023.Stablevideodiffusion:Scalinglatentvideodiffusionmodelstolarge [39] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,
datasets.arXivpreprintarXiv:2311.15127(2023). BobMcGrew,IlyaSutskever,andMarkChen.2021.Glide:Towardsphotorealistic
[10] JingwenChen,YingweiPan,TingYao,andTaoMei.2023.Controlstyle:Text- imagegenerationandeditingwithtext-guideddiffusionmodels.(2021).
drivenstylizedimagegenerationusingdiffusionpriors.InACMMM.7540–7548. [40] AlexanderQuinnNicholandPrafullaDhariwal.2021.Improveddenoisingdiffu-
[11] YangChen,YingweiPan,YehaoLi,TingYao,andTaoMei.2023. Control3d: sionprobabilisticmodels.InICML.
Towardscontrollabletext-to-3dgeneration.InACMMM. [41] WilliamPeeblesandSainingXie.2023.Scalablediffusionmodelswithtransform-
[12] YangChen,YingweiPan,HaiboYang,TingYao,andTaoMei.2024. Vp3d: ers.InICCV.
Unleashing2dvisualpromptfortext-to-3dgeneration.InCVPR. [42] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,
[13] HyungjinChung,JeongsolKim,MichaelTMccann,MarcLKlasky,andJongChul JonasMüller,JoePenna,andRobinRombach.2024. SDXL:ImprovingLatent
Ye.2023. Diffusionposteriorsamplingforgeneralnoisyinverseproblems.In DiffusionModelsforHigh-ResolutionImageSynthesis.InICLR.
ICLR. [43] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.2023.Dreamfusion:
[14] HyungjinChung,ByeongsuSim,DohoonRyu,andJongChulYe.2022.Improving Text-to-3dusing2ddiffusion.InICLR.
diffusionmodelsforinverseproblemsusingmanifoldconstraints.InNeurIPS. [44] YuruiQian,QiCai,YingweiPan,YehaoLi,TingYao,QibinSun,andTaoMei.
[15] KevinClark,PaulVicol,KevinSwersky,andDavidJFleet.2024.Directlyfine- 2024.BoostingDiffusionModelswithMovingAverageSamplinginFrequency
tuningdiffusionmodelsondifferentiablerewards.InICLR. Domain.InCVPR.
[16] ChengliangDai,ShuoWang,YuanhanMo,KaichenZhou,ElsaAngelini,Yike [45] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
Guo,andWenjiaBai.2020.Suggestiveannotationofbraintumourimageswith SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
gradient-guidedsampling.InMICCAI. etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
[17] XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,Peizhao InICML.
Zhang,SimonVandenhende,XiaofangWang,AbhimanyuDubey,etal.2023.Emu: [46] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörn
Enhancingimagegenerationmodelsusingphotogenicneedlesinahaystack. Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In
arXivpreprintarXiv:2309.15807(2023). CVPR.
[18] PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatganson [47] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyL
imagesynthesis.InNeurIPS. Denton,KamyarGhasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,Tim
[19] WenkaiDong,SongXue,XiaoyueDuan,andShuminHan.2023.Prompttuning Salimans,etal.2022.Photorealistictext-to-imagediffusionmodelswithdeep
inversionfortext-drivenimageeditingusingdiffusionmodels.InICCV. languageunderstanding.InNeurIPS.
[20] RuoyiDu,DongliangChang,TimothyHospedales,Yi-ZheSong,andZhanyuMa. [48] CemSazara.2023.DiffusionModelsinGenerativeAI.InACMMM.
2024.DemoFusion:DemocratisingHigh-ResolutionImageGenerationWithNo [49] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,Ross
$$$.InCVPR. Wightman,MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,Mitchell
[21] DeepanwayGhosal,NavonilMajumder,AmbujMehrish,andSoujanyaPoria. Wortsman,PatrickSchramowski,SrivatsaKundurthy,KatherineCrowson,Lud-
2023.Text-to-AudioGenerationusingInstructionGuidedLatentDiffusionModel. wigSchmidt,RobertKaczmarczyk,andJeniaJitsev.2022.LAION-5B:Anopen
InACMMM. large-scaledatasetfortrainingnextgenerationimage-textmodels.InNeurIPS.
[22] ShuyangGu,DongChen,JianminBao,FangWen,BoZhang,DongdongChen,Lu [50] YanShu,WeichaoZeng,ZhenhangLi,FangminZhao,andYuZhou.2024.Visual
Yuan,andBainingGuo.2022.Vectorquantizeddiffusionmodelfortext-to-image TextMeetsLow-levelVision:AComprehensiveSurveyonVisualTextProcessing.
synthesis.InCVPR. arXiv:2402.03082[cs.CV] https://arxiv.org/abs/2402.03082
[23] YingqingHe,ShaoshuYang,HaoxinChen,XiaodongCun,MenghanXia,Yong [51] ChenyangSi,ZiqiHuang,YumingJiang,andZiweiLiu.2024.Freeu:Freelunch
Zhang,XintaoWang,RanHe,QifengChen,andYingShan.2024.ScaleCrafter: indiffusionu-net.InCVPR.
Tuning-freeHigher-ResolutionVisualGenerationwithDiffusionModels.In [52] JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli.
ICLR. 2015.DeepUnsupervisedLearningusingNonequilibriumThermodynamics.
[24] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic [53] JiamingSong,ChenlinMeng,andStefanoErmon.2021. DenoisingDiffusion
models.InNeurIPS. ImplicitModels.InICLR.
[25] JonathanHoandTimSalimans.2021. Classifier-freediffusionguidance.In [54] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,Stefano
NeurIPS. Ermon,andBenPoole.2020.Score-basedgenerativemodelingthroughstochasticMM’24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
differentialequations.InICLR. [62] ZhenXing,QijunFeng,HaoranChen,QiDai,Hang-RuiHu,HangXu,Zuxuan
[55] PatrickvonPlaten,SurajPatil,AntonLozhkov,PedroCuenca,NathanLambert, Wu,andYu-GangJiang.2023. ASurveyonVideoDiffusionModels. ArXiv
KashifRasul,MishigDavaadorj,DhruvNair,SayakPaul,WilliamBerman,Yiyi abs/2310.10647(2023).
Xu,StevenLiu,andThomasWolf.2022. Diffusers:State-of-the-artdiffusion [63] HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,andTaoMei.
models.https://github.com/huggingface/diffusers. 2023.3dstyle-diffusion:Pursuingfine-grainedtext-driven3dstylizationwith2d
[56] JianyiWang,KelvinCKChan,andChenChangeLoy.2023.Exploringclipfor diffusionmodels.InACMMM.6860–6868.
assessingthelookandfeelofimages.InAAAI. [64] SidiYang,TianheWu,ShuweiShi,ShanshanLao,YuanGong,MingdengCao,
[57] ZhixinWang,XiaoyunZhang,ZiyingZhang,HuangjieZheng,MingyuanZhou, JiahaoWang,andYujiuYang.2022.Maniqa:Multi-dimensionattentionnetwork
YaZhang,andYanfengWang.2023.DR2:Diffusion-basedRobustDegradation forno-referenceimagequalityassessment.InCVPR.
RemoverforBlindFaceRestoration.InCVPR. [65] LvminZhang,AnyiRao,andManeeshAgrawala.2023. Addingconditional
[58] ZhizhongWang,LeiZhao,andWeiXing.2023. Stylediffusion:Controllable controltotext-to-imagediffusionmodels.InICCV.
disentangledstyletransferviadiffusionmodels.InICCV. [66] YuxinZhang,NishaHuang,FanTang,HaibinHuang,ChongyangMa,Weiming
[59] ChenWuandFernandoDelaTorre.2024.ContrastivePromptsImproveDisen- Dong,andChangshengXu.2023.Inversion-basedstyletransferwithdiffusion
tanglementinText-to-ImageDiffusionModels.arXivpreprintarXiv:2402.13490 models.InCVPR.
(2024). [67] ZhongweiZhang,FuchenLong,YingweiPan,ZhaofanQiu,TingYao,YangCao,
[60] XiaoshiWu,YimingHao,KeqiangSun,YixiongChen,FengZhu,RuiZhao,and andTaoMei.2024.TRIP:TemporalResidualLearningwithImageNoisePrior
HongshengLi.2023.Humanpreferencescorev2:Asolidbenchmarkforevaluat- forImage-to-VideoDiffusionModels.InCVPR.
inghumanpreferencesoftext-to-imagesynthesis.arXivpreprintarXiv:2306.09341 [68] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and
(2023). ChangWenChen.2024.Sd-dit:Unleashingthepowerofself-superviseddiscrim-
[61] ShaoanXie,ZhifeiZhang,ZheLin,TobiasHinz,andKunZhang.2023.Smart- inationindiffusiontransformer.InCVPR.
brush:Textandshapeguidedobjectinpaintingwithdiffusionmodel.InCVPR.