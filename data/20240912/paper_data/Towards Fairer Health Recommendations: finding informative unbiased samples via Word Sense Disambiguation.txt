Towards Fairer Health Recommendations: finding informative
unbiased samples via Word Sense Disambiguation
GavinButts∗ PegahEmdad∗ JethroLee∗
LoyolaMarymountUniversity WorcesterPolytechnicInstitute NortheasternUniversity
gbutts@lion.lmu.edu pemdad@wpi.edu lee.jet@northeastern.edu
ShannonSong ChimanSalavati WillmarSosaDiaz
WorcesterPolytechnicInstitute UniversityofConnecticut UniversityofConnecticut
smsong@wpi.edu chiman.salavati@uconn.edu willmar.sosa_diaz@uconn.edu
ShiriDori-Hacohen FabricioMurai
UniversityofConnecticut WorcesterPolytechnicInstitute
shiridh@uconn.edu fmurai@wpi.edu
ABSTRACT 1 INTRODUCTION
Therehavebeengrowingconcernsaroundhigh-stakeapplications Fordecades,medicinehasbeenmarredbyimplicitandexplicit
thatrelyonmodelstrainedwithbiaseddata,whichconsequently biasesthatcontinuetonegativelyimpactpatientoutcomesbyper-
producebiasedpredictions,oftenharmingthemostvulnerable.In petuatingstereotypesandcontributingtohealthdisparitiesamong
particular,biasedmedicaldatacouldcausehealth-relatedapplica- socialgroupsthatfacesystemicoppression[8,9].Despiteeffortsto
tionsandrecommendersystemstocreateoutputsthatjeopardize remediateandaddressthesebiasesfromtheirsource,manymed-
patientcareandwidendisparitiesinhealthoutcomes.Arecent icalschoolsstillincorporatebiasedmedicalteachingsduringthe
frameworktitledFairnessviaAI positsthat,insteadofattempting preclinicalyears[12,28].Manyeducatorscontinuetomisuserace
tocorrectmodelbiases,researchersmustfocusontheirrootcauses asasubstituteforgeneticsorancestry,ortheyusegenderandsex
byusingAItodebiasdata.Inspiredbythisframework,wetackle termsincorrectlyreinforcingthenotionthatsexandgenderare
biasdetectioninmedicalcurriculausingNLPmodels,including binaryorfixedratherthanfluid,whichcanpotentiallyalienate
LLMs,andevaluatethemonagoldstandarddatasetcontaining gender-nonconformingstudentsandpatients[1,14,15].Thecur-
4,105excerptsannotatedbymedicalexpertsforbiasfromalarge rentfocusinAIresearchisprimarilyonidentifyingandexposing
corpus.Webuildonpreviousworkbycoauthorswhichaugments biaswithinAIsystems,oftenwithoutaddressingtherootcauses
thesetofnegativesampleswithnon-annotatedtextcontainingso- ofbiasinherentinthedatathesesystemsarebuiltupon.Aslong
cialidentifierterms.However,someoftheseterms,especiallythose asstructuralinequalitiesexistintherealworld,AIsystemswill
relatedtoraceandethnicity,cancarrydifferentmeanings(e.g., perpetuatethesebiases[10].Byharnessingmachinelearningto
“whitematterofspinalcord”).Toaddressthisissue,weproposethe analyzeanddetectthesebiases,wecanadvanceequityinmedical
useofWordSenseDisambiguationmodelstorefinedatasetqual- trainingandthefairnessofAImodels,leadingtoamoreaccurate
itybyremovingirrelevantsentences.Wethenevaluatefine-tuned andeffectivehealthcaresystem.
variationsofBERTmodelsaswellasGPTmodelswithzero-and Recently,Salavatietal.[25]introducedtheBRICC(BiasReduc-
few-shotprompting.WefoundLLMs,consideredSOTAonmany tioninCurricularContent)datasetandproposedasystematicand
NLPtasks,unsuitableforbiasdetection,whilefine-tunedBERT scalableAI-basedmethodforidentifyingpotentialbiasinmedical
modelsgenerallyperformwellacrossallevaluatedmetrics. curricula.Giventhesteepcostoffalsenegatives(i.e.,classifying
abiasedsentenceasunbiased),theyemphasizethatrecallmust
KEYWORDS beprioritizedoverprecision.Moreover,duetotheinherentdiffi-
cultyofthetask,oneofthebestapproachesinthisHigh-Recall
medicaltextdata,biasdetection,LLMs,wordsensedisambiguation
InformationRetrievalsettingistheTechnologyAssistedReview
ACMReferenceFormat: (TAR)[7,18],wherebyasetofexpertsreviewsthesamplesflagged
GavinButts,PegahEmdad,JethroLee,ShannonSong,ChimanSalavati, byamodel,asenvisionedbySalavatietal.Thepaperalsouseda
WillmarSosaDiaz,ShiriDori-Hacohen,andFabricioMurai.2024.Towards curatedlistofsocialidentifierstofindadditional,negative(i.e.,non-
FairerHealthRecommendations:findinginformativeunbiasedsamplesvia
biased)samplesinunlabeleddata.However,thislatterapproach
WordSenseDisambiguation.InProceedingsofMakesuretoenterthecorrect
sufferedfromsocialidentifiertermsthathadambiguousmeanings,
conferencetitlefromyourrightsconfirmationemai(FAccTRec’24).ACM,New
leadingtolowerqualityofthetrainingdata,andnegativesamples
York,NY,USA,7pages.https://doi.org/XXXXXXX.XXXXXXX
thatweretoo“easy”toclassifyasnon-biased.Forexample,one
socialidentifierusedtofilterforrace-relateddatawas“white”.In
∗Equalcontribution.
Table1,simplysearchingforthekeyword“white”willincludeboth
race-relatedandnon-race-relatedtextexcerpts.
FAccTRec’24,October14–18,2024,Bari,Italy Webelievethatusingtheseambiguousmeaningtermsas-isleads
2024.ACMISBN978-1-4503-XXXX-X/18/06...$15.00
tooverestimatingthetruediscernmentpowerofthebiasclassifier
https://doi.org/XXXXXXX.XXXXXXX
4202
peS
11
]LC.sc[
1v42470.9042:viXraFAccTRec’24,October14–18,2024,Bari,Italy Buttsetal.
Table1:Theterm“white”inaracialvs.non-racialcontext Khanetal.[16]manuallyexploredthesystemicbiasheldbymed-
icalprofessionalswhenwritingrecommendationletters.Onthe
Race-Related NotRace-Related otherhand,Razaetal.[22]andSalavatietal.[25]aimedtodetect
“5YearRelativeSurvival:over- “Whitematterwithinthespinal biasinmedicaltextusingtransformer-basedlanguagemodels.The
all84%forwhitewomen,62% cordcontainstheaxonsofneu- formerusedasemi-autonomouslylabeleddatasetcoveringdiverse
for black women, 95% for lo- ronsthatareascendingandde- medicaltopics,whereasthelatteremployedadatasetmanuallyla-
caldisease,69%regionaldisease scendingtotransmitsignalsto
beledbymedicalexpertsfocusingonbiasedinformationinmedical
(spreadtolymphnode),17%for andfromthebrain,respectively.”
curriculartexts.Althoughbothstudiesprovideacomprehensive
distantdisease.”
overviewofbiasdetection,ensuringhigh-qualitydataremainsan
bymakingtheproblemtooeasy.Itmaybethattheclassifieris issue.WhilewealsoexploreAImodelsfordebiasingmedicaltext
actuallydifferentiatingrace-relatedfromnon-race-relatedterms, data,weinvestigatebetterwaysofaugmentingthesetofunbiased
ratherthanbiasedfromnon-biasedsentences. samplesandconsiderawidergamutofmodels,includingLLMs.
Forthisreason,weproposeanewframeworktoaugmentthe
samplingprocessfornegativeexamples,usingWordSenseDisam- MachineLearningforBiasDetection. Priorworkshaveappliedvar-
biguation(WSD)methodsfordataenhancement.Wehypothesize iousBERTmodelsforbiasclassificationtasks.Tidermanetal.[26]
thatthiswouldimprovethedistinctionbetweenbiasedandnon- usedDistilBERT,atransformer-baseddistilledBERTmodel,toclas-
biasedsentencesinthebiasclassificationprocess. sifybiasedinformationinsocialmediacontent.Similarly,Razaet
Ourmaincontributionsareasfollows: al.[22]achievedthebestresultsforbiasclassificationinmedical
(1) Weenhanceaframeworkfordetectingbiasinmedicalcur- textthroughfine-tuningBERT,asimpleencoder-onlytransformer.
riculumcontent,withafocusonimprovingdataquality. Buildingontheexistingliteratureforbiasdetectioninmedical
(2) WeleverageWordSenseDisambiguation(WSD)modelsin contexts,weadditionallyapplyLargeLanguageModels(LLMs)for
trainingbiasdetectionclassifiersbyfilteringoutirrelevant thistask.Specifically,weuseTinyLlama[31],acomputationally
samplesfromthedata.Moreover,weuseChatGPT-4oto efficientvariantofLlama2,forbiasclassification.Inaddition,we
augmentasetofmanuallylabeledexampleswithsynthetic consideradditionalstrategiesforconstructingthesetofnegative
samplestofine-tuneand/orevaluateWSDmodels. samples–suchasthroughtheuseofWSDfordatarefinement.
(3) Wefine-tuneandevaluatevariousTransformer-basedmod-
elsincludingDistilBERT,RoBERTa,andBioBERTforthebias UseofLLMsforNLPtasksandpromptengineering. InNLPtasks,
detectiontask.Inaddition,weuseLargeLanguageModels promptingLLMshavebeenshowntoperformonparwithencoder-
(LLMs),suchasTinyLlama,forbiasclassification,evaluating onlyarchitectures,likeBERT,withouttheneedforfine-tuning[6].
zero-shotvs.few-shotpromptingwithGPT,toimprovethe Ithasbeenshownthatpromptingtechniques,suchaszero-shot,
performanceofbiasdetection. few-shot,orchainofthought(CoT),serveakeyroleinthequality
(4) Wepresentacomprehensiveevaluationofthevariousmod- andcorrectnessofamodel’soutput[19].Thesetechniqueshave
els,highlightingtheimprovementsachievedthroughtheuse beenusedinmanytasks,suchassentimentanalysis[3],textclassi-
ofWSDandChatGPT-generatedsentences. fication[5],aswellasforhealthcareapplications,suchasquestion-
answering,andasaclinicalrecommendersystem[20,29].Despite
2 RELATEDWORK theseinitiatives,wearethefirsttoevaluatezero-andfew-shot
HealthRecommenderSystems(HRS). Recommendersystemshave promptingfordetectingbiasinmedicalcurricularcontent.
becomeintegraltothehealthcareindustry,providingpersonalized
medicalrecommendationsthatenhancepatientunderstandingof
3 DATASET
theirmedicalconditionandimprovehealthoutcomes[27].These
OurworkbuildsontheBRICCdatasetintroducedbySalavatiet
systemsassisthealthcareprofessionalsinpredictingandtreating
al.[25],whichconsistsof509PDFfilesand12,647pagesofmedical
diseasesbyanalyzingpatientdatatorecommendpersonalizeddiets,
schoolinstructionalmaterialsannotatedbymedicalstudentsand
exerciseregimens,medications,diagnoses,andotherhealthser-
expertstrainedinidentifyingbias.Withinthedataset,thereare
vices[21,24].Despitenumerousstudiesexploringvariousaspects
threetiersofcoding.Thefirst-levelcodesidentifysocialidentifiers
ofHRS,theliteratureforaddressingvarioustypesofbiasesinsuch
within the excerpt. The second-level codes assess the presence
systemsrootedincurriculacontentsislimited[25].
or absence of bias in the excerpt, categorized into four distinct
DebiasingmedicalcorporamanuallyandviaAI. Concernsoverbi- groups:‘biased’,‘potentiallybiased’,‘non-biased’,and‘review’.
asedAImodelsand,particularly,recommendersystemsinhealth- Additionally,third-levelcodesestablishalinkbetweenamedical
careapplicationshavebeengainingmoreattentionduetotheir conditionandoneormorecategoriesofsocialidentifiers(e.g.,race),
increaseduseinhigh-stakedecisions[4].Inessence,theirbiases specifyingthetypeofidentityandwhetheritwasportrayedina
are rooted in implicit and explicit biases embedded in the data biasedorunbiasedmanner.Eachexcerptisthenassignedoneor
usedfortrainingthem[13],whichstemfromvarioussources,in- morecodesformattedas“TYPE-disease”,whereTYPErepresents
cludinginherentbiasesinmedicalliterature,thesubjectivityof oneof17categoriesofsocialidentifiers.Akintothepreviouswork,
humanannotators,andhistoricalandsystemicinequitiespresent wefocusonthemostfrequenttypesincludingsex,gender,race,
inhealthcaresystems[25].Numerousrecentstudieshaveaimed ethnicity,age,andgeography.Eachcategoryisassociatedwitha
toquantifyandaddressthisissuebothmanuallyandthroughAI. listofkeywordsthatcansignifysocialidentifiers.TowardsFairerHealthRecommendations:findinginformativeunbiasedsamplesviaWordSenseDisambiguation FAccTRec’24,October14–18,2024,Bari,Italy
Table2:BRICCDatasetCharacteristics template-basedpromptingusingchainofthoughtreasoning,and
highlighteditshighperformance.
Counts
NumberofPDFFiles 509
TotalNumberofPages 12,647
4.2 WordSenseDisambiguationTaskDefinition
AnnotatedExcerpts 4,105
Generallyspeaking,wordsensedisambiguation(WSD)isthetask
LabeledPositives 1,116
ofidentifyingthecorrectsenseofapolysemous𝑤 ∈W(aword
LabeledNegatives(LN) 2,989
withmultiplemeanings)inagivencontext𝑥.Formally,givenaset
ExtractedNegatives(XN) 4,391
ofwordsW,afinitesetofpossiblesensesS𝑤 ={𝑆 𝑤(1),...,𝑆 𝑤(𝑘) }
Positive and negative samples. Positive samples are defined as foreach𝑤 ∈ W andacontext(orderedsequenceofwords)𝑥 =
thoseexcerptsthatcontaineithera‘biased’,‘potentiallybiased’, (𝑥1,...,𝑥 𝑖−1,𝑤,𝑥 𝑖+1,...𝑥 𝑛) ∈X,findafunction𝑓 :W×X→S,
or‘review’andaselected“TYPE-disease”.Negativesamplesare suchthat𝑓(𝑤,𝑥)isthecorrectsenseof𝑤 incontext𝑥.
subdividedintovarioustypes,asdetailedinthepreviouswork[25]. For this paper, we are interested in determining if a
The negative types that we are most interested in are referred term 𝑤, listed as a possible social identifier for category
toasextractednegatives(XN).Inthiscase,thesearesentences 𝑡, is related to 𝑡 in an excerpt 𝑥. To do so, we need to
fromthecorpusthat,despitecontainingacategorykeyword,were learn a function IsRelated(𝑤,𝑥,𝑡) ∈ {true,false}. For in-
deliberatelyexcludedfromtheannotationprocess.Pleasereferto stance, the set of social identifiers for race is Srace =
supplementalmaterialsforadetailedexplanationofnegativetypes {‘white’,‘black’,...}.Ideally,inoneoftheexamplesseenearlier,we
foundintheBRICCdataset.OftheXNtypes,wefilteredforthose wantIsRelated(‘white’,‘whitematterwithin...’,‘race’)=false.
thatcontainedatleastonerelevantkeywordrelatingtoourselected
“TYPE-disease”.Thedistributionofpositivesandnegativesacross
thedatasetisdisplayedinTable2. 4.3 BiasDetectionTaskDefinition
WefocusonXNsamplesinourexperimentsbecausetheauthors Inthecontextofmedicaleducation,weconsiderbiasdetectionasa
previouslyreportedhigherrecall,0.925,butattheexpenseofpreci- HighRecallInformationRetrievaltask.ItisthefirststepinaTAR
sion,0.504,byusingthisnegativeset[25].Despitethisimproved system.Thistaskconsistsofclassifyingatextexcerpt𝑥asunbiased
performance,wenoticedthatmanyofthekeywordsmayleadto (𝑦ˆ=0)orpotentiallybiased(𝑦ˆ=1).Inthelattercase,thesample
theinclusionofnon-“TYPE-disease”relatedcontent.Anexample wouldbesubsequentlyreviewedbyamedicalexpert.
ofthisoccurrencemaybeseeninTable1.Hence,retainingonly Biasmayberelatedtooneormorecategoriesofsocialiden-
negativesamplesthatrelatetothesocialdemographicsofinterest tifiers,includingrace,ethnicity,sex,gender,age,andgeography.
isakeycomputationaltask,whichweaddressusingWSD. Forinstance,“Theypromotehairgrowthinthegroin,axilla,chest
andface,yettheyalsopromotehairlossinthescalpinmenwho
LabelingdataforWSD. TogathersamplessuitablefortrainingWSD aregeneticallysusceptibletoandrogeneticalopecia.”islabeledby
models,weselectedthoseXNexcerptsthatcontainedakeyword medicalexcerptsas‘biased’withrespecttogender (designated
relatedtotheselectedsocialdemographicscategories:sex,gen- bythesocialidentifiermen).Asexplained,inthecommentfrom
der,race,ethnicity,geography,andage.Afterobtainingarandom oneoftheannotators:“Usesextermswhenspeakingofpopulations,
sampleofXNexcerptsforeachcategory,wehadahumanexpert
shouldbemaleinsteadofmen.Also,includecitationtosupportthis
annotatewhetherthemeaningofthekeywordtermwasindeedre- assertion.”
latedtothatcategoryornot.Basedontheresultsofthisannotation Formally,Salavatietal.[25]definetype-specificbiasasabinary
process,wedecidedtoonlyfocusonracekeywordsbecausethey labelbias(𝑥,𝑡) ∈ {true,false} indicatingwhetherexcerpt𝑥 is
sufferedthemostfromambiguity.Theotherbiascategoriesdid biasedwithrespecttoasocialidentifiercategory𝑡.Inthepresent
nothaveasignificantdegreeofambiguitythatrequiredcorrection. work,weconsideronlythegeneraldefinitionofbias,regardlessof
TheselabeledexcerptswereusedtotrainourWSDmodels. whichcategory𝑡 inasetT itbelongsto:bias(x,T)=true ⇐⇒
∃𝑡 ∈T s.t.bias(x,t)=true.
4 PRELIMINARIES
4.1 LLMPrompting 5 METHODOLOGY
Reynoldsetal.[23]suggestthatzero-shotpromptscouldsignifi- In this section, we provide an overview of the proposed frame-
cantlyoutperformfew-shotprompts.Theiranalysishighlightsthe work.Figure1(left)showsthedataprocessingstepsperformed
needtoconsidertheroleofpromptsincontrollingandevaluating bySalavatietal.[25],whichweleverageinthepresentwork.As
theperformanceoflanguagemodels.Theirstudystatedthatsince explained, in addition to labeled data, BRICC includes negative
GPT-3isoftennotlearningfromfew-shotexamplesduringthe samplesextractedfromthenon-annotateddata(denotedasXN).
runtime,thismodelcaneffectivelybepromptedwithoutexam- Figure1(center)illustratestheapplicationofWSDtofilterout
ples[23].Additionally,Kojimaetal.[17]demonstratethatchain irrelevantsamples,whichresultsinthefilteredXNset(XN∗).This
ofthought(CoT)prompting,arecenttechniqueforelicitingcom- processisdescribedindetailinSection5.1.Last,Figure1(right)
plexmulti-stepreasoningthroughstep-by-stepanswerexamples, depictstheaugmentationoflabeleddatawithXN∗ fortraining
achieved state-of-the-art performances in arithmetics and sym- differentbiasclassifiers,whoseperformanceweevaluate.Details
bolicreasoningtasks.TheyproposedZero-shot-CoT,azero-shot arediscussedinSection5.2.FAccTRec’24,October14–18,2024,Bari,Italy Buttsetal.
andweightdecayof0.01over10epochs,keepingthemodelthat
Data Processing WSD Bias Classification yieldedthesmallestvalidationloss.
Giventhesubstantialempiricalandtheoreticalevidencesup-
Corpus Extracted Labeled Labeled
Negatives Data XN portingthebenefitsofchainofthought(CoT)promptinginvarious
LLMtasks[11,30],weincorporateCoTintoourzero-shotprompts.
WeoptedtofollowtheprompttemplatepresentedinKojimaet
Annotators WSD Bias Classifier al.[17],whichwasshowntoproducethehighestaccuracy(i.e.,
“Let’sthinkstepbystep”).Topromptthemodel,wefirstspecified
themodel’srole:“Youareahelpfulassistantthatdeterminesifthe
sentenceisraceorethnicityrelated”.Then,wedefinedthetaskas:
La Db ae tl aed NEx et gr aa tc it ve ed s Filtered XN Unb0 iased Bia1 sed “Giventhesentence‘text’,thinkstepbystep:Isthissentenceraceor
☺ ☹ ethnicityrelated?Onlyoutput1or0.Ifthissentencecontainsany
termsrelatingtoraceorethnicity,state1.Otherwise,state0.”
Figure1:Workflowstages.(Left)Dataprocessing:annotated
Metrics. Weevaluatethemodels’performanceonthetestsetwith
excerptsarelabeledas‘biased’(positive)or‘non-biased’(neg-
respecttoaccuracy,precision,recallandF1score.
ative);XN:additionalsentencesextractedasnegativeexam-
ples.(Center)WordSenseDisambiguation(WSD)usedfor
5.2 BiasClassificationExperiments
selectingfromXNrelevantnegatives(XN∗).(Right)Training
UsingtheBRICCdataset,wefine-tunebinaryclassificationneural
andevaluationofbiasclassifiers.
languagemodelsandpromptpre-trainedLLMsforbiasclassifica-
tion.Forfine-tuning,weconsiderencoder-onlyanddecoder-only
Data Collection Model Experimenting models(encoder-decodermodelsareoftenreservedformulti-modal
Annotators GPT-4o tasks and causal language inference [2]).The fine-tuned models
Training Dataset include RoBERTa, DistilBERT, BioBERT, and TinyLlama. Using
promptengineering,weadditionallypromptGPT-4omini.
Manual Synthetic TF- &ID F BERT GPT Differentsetsofnegatives. Thedatasetswewillconsidercontainall
Data Data Logistic positivesamples,plusoneofthefollowing:
Regression
• Labelednegatives(LN),
• Labelednegativesplusextractednegativesfilteredbykey-
Dataset Evaluation words(LN+XN),and
• Labeled negatives plus extracted negatives filtered using
wordsensedisambiguation(LN+XN∗).
Figure2:WSDtrainingandevaluation.Excerptsmanually AsdescribedinSection3,thesetofextractednegatives(XN)is
labeledasrace-relatedornotplusGPT-generatedsentences constructedbyfilteringdatabasedonkeywordsthatrelatetoeither
areusedtotrainandevaluatetheWSDmodels. gender,sex,race,ethnicity,age,and/orgeography.Then,weapply
the best-performing WSD model to ensure these samples truly
5.1 WordSenseDisambiguationExperiments relatetothesocialdemographicsofinterest,resultinginXN∗.To
WeevaluateseveralmodelsforWSD:asimplebaseline,twofine- assessperformancevariabilityasafunctionofthedatasplits,we
tunedvariantsofBERT,andtwoGPTmodels.Forfine-tuningand splitthedatasetinK-foldsforcross-validationandcalculateaverage
evaluation,wecombinedourmanualannotationswithsentences performanceandconfidenceintervals.
generatedbyChatGPT-4o,yielding352labeledexcerpts.
Fine-tuning. Toconstructthemodels,weaddaclassificationheadto
ExperimentalSetup. Wedivideeachofthetwodatasets(manually eachlanguagemodelandfullyfine-tuneeachmodelalongwiththe
annotatedexcerptsandsyntheticsamples)independentlywitha classificationhead.Themodelsweutilizeare:RoBERT,DistilBERT,
70-15-15stratifiedsplitintotraining,validation,andtestsets. andBioBERT,allencoder-only,andTinyLlama,a1.1Bdecoder-only
Weinvestigatedtwowaysofbuildingthetrainingset: modelderivedfromMeta’sLlama2.
For each dataset we outlined, we do initial fine-tuning on
• Onlymanually-annotatedexcerpts; RoBERTa,DistlBERT,BioBERT,andTinyLlamawithabatchsizeof
• Bothmanually-annotatedexcerptsandsyntheticsamples. 8andalearningrateof2×10−5.Weusethevalidationsettotune
thehyperparameterswithgridsearch,leadingtothefinalmodel.
WSDModels. Weevaluatethreemodelsshownintherecentlit-
eraturetoperformwellonWSD:ALBERT,GlossBERT,andGPT Prompting. WeevaluatetheperformanceofGPT-4ominibyusing
models.Thesemodelsarecomparedtoourbaseline,alogisticre- zero-andfew-shotprompting.Topromptthismodel,wefirstes-
gressionwithTF-IDF. tablishthemodel’srole:“Youareahelpfulassistantthatdetermines
iftextisbiased”.Then,weestablishthetask.Wefindthatthebest
Fine-tuningandPrompting. Wefine-tunealllayersofthepre-trained taskdescriptionis:“Giventext,determineifthetextcontainsbiasor
ALBERTandGlossBERTmodelswithalearningrateof2×10−5
nobias.Thebiasmaytargetgender,sex,race,ethnicity,age,and/orTowardsFairerHealthRecommendations:findinginformativeunbiasedsamplesviaWordSenseDisambiguation FAccTRec’24,October14–18,2024,Bari,Italy
Table3:Few-shotexampleinputs,outputs,andreasoning Table5:ExamplesofWSDtestcasesandGlossBERTpredicted
usedforpromptingGPT-4ominiforbiasclassification probabilitiesfor𝑦=1.Eachexcerpthasaterm(bolded)listed
amongrace/ethnicitykeywords.
Input𝑥 Label,Comment
52yearold,marriedfemalewithonedaugh- Label:1,Usegender Input𝑥(label𝑦) Prediction
ter,employedasaschooladministratorwith terms like woman Melanoma:increasinginincidenceinthewhitepopulation
0.9998
nopriorpsychhistoryreports2monthh/o forcasestudies (CDC).(𝑦=1)
[historyof]sadness,subjectiveanxietyand
intermittenttroublefallingasleep.
2015AmericanHeartAssociationguidelinessuggesttreat-
ingpatientspresentingwithsystolicBPabove150-220 0.9998
Oncepatientisonanothertreatmentforher Label:0,n/a(from mmHg,buttheydonotofferaspecificBPtarget.(𝑦=0)
disorder,shenolongerneedsthismedication. XNset)
Calcificplaquesarechalkywhiteandarisefromcardiac
0.0001
Recentmeta-analysissuggestednodifference Label:1,Term‘Late (aorticandmitral)valves.(𝑦=0)
inprevalenceamongcountries,rateis1-2% Adolescence’ is an
withincreaseduringlateadolescence. uncleartimeperiod Table6:PerformanceMetricsand95%-CIsforFine-Tuned
ModelstrainedonLN+XN*data.RoBERTayieldsthehighest
Table 4: Performance metrics for WSD on manually-
averages,butitisstatisticallytiedwithDistilBERT.
annotated+GPTexcerpts.Bestresultforeachmetricshown
inbold.GlossBERTandGPT-4oaretiedasthebestmodels. Metric RoBERTa DistilBERT BioBERT
TF-IDF+ Gloss GPT-3.5 GPT-4o
Precision 0.613±0.015 0.605±0.013 0.581±0.014
Metric LogisticReg. ALBERT BERT Turbo mini Recall 0.692±0.024 0.649±0.030 0.620±0.019
F1Score 0.650±0.014 0.626±0.018 0.599±0.010
Accuracy 0.839 0.926 0.944 0.925 0.944 F2Score 0.674±0.019 0.639±0.025 0.611±0.014
Precision 0.816 0.935 0.936 0.916 0.936 AUC 0.927±0.003 0.921±0.006 0.904±0.003
Recall 0.839 0.977 1.000 1.000 1.000
F1Score 0.817 0.956 0.967 0.956 0.967 Table 7: Performance Metrics and 95%-CIs for Prompting
GPT-4omini.Bestresultsforeachmetricshowninbold.AUC
geography,includeexclusivelanguage,ormakeunsupportedclaims.
wasommittedasitcannotbecomputedforbinaryoutputs.
Thetextmayalsocontainnobiasatall.Ifthetexthasbiasstate1,if
thetextdoesnothavebiasstate0.” Metric Zero-Shot Few-Shot
Bothzero-andfew-shotpromptingusedthepromptabove.For
Precision 0.367±0.071 0.259±0.019
few-shotprompting,wetestedvarioussetsofexamplesfromthe
Recall 0.260±0.029 0.610±0.026
dataset.Table3showsthesubsetthatperformedbest.
F1Score 0.303±0.040 0.363±0.023
Metrics. Forthebiasdetectiontask,wealsoevaluatethemodels’ F2Score 0.274±0.032 0.480±0.025
performanceonthetestsetwithrespecttoprecision,recallandF1
useof‘American’onlyindirectlyrelatestotheethnicityofthe
score.Inaddition,weconsidertheF2scoreandareaundertheROC
peoplethatanorganizationserves.
curve(AUC).F2issimilartoF1butprioritizesrecalloverprecision.
Wealsoinvestigatewhetherthesyntheticsamplesgenerated
Duetotheclassimbalance,AUCismorerelevantthanaccuracy
byChatGPT-4oweretrivial,whichwouldartificiallyinflateper-
becauseitaccountsforallpossiblethresholdchoices.
formance.Whenweevaluatethemodelresultsononlymanually
annotatedexcerpts,theperformanceofallmodelsstayssomewhat
6 RESULTS similar,exceptforGPTmodels,bothofwhichachieve100%accu-
racy.Therefore,thesyntheticexamplesareatleastashardasthe
6.1 EvaluationofWSDmodels
manuallyannotatedexcerptsforBERTmodels,justifyingtheiruse
Table4presentstheevaluationresultsfortheWSDmodelsexam-
inourevaluation.
ined.Thebaselineachievedworseresultsthantheothermodelsfor
Inaddition,weevaluatethemodels’performancewhentrained
everymetric.WefindthatGlossBERToutperformsALBERTand
onlyonthemanually-annotateddata.Inthiscase,thereisaperfor-
thatGPT-4ominiimprovesuponGPT-3.5Turbo.Furthermore,both
mancedropforthefine-tunedmodels(ALBERTdeclinesfrom0.926
GlossBERTandGPT-4oaretiedasthebestmodels,bothexhibiting
to0.852andGlossBERTdeclinesfrom0.944to0.852accuracy),
averyhighF1Score(0.967).Usingthecostasatie-breakerbetween
indicatingthatthesyntheticsampleshelpthemodelstogeneralize
thetwo,weoptedtouseGlossBERTfortheWSDtaskperformed
better.Furthermore,GlossBERTremainstiedasthebestmodel,
ontheextractednegatives.
whichsupportsourchoiceofusingitforbuildingthesetoffiltered
Table5illustratesexamplesfromthetestset,twoofwhichwere
extractednegativesinthebiasdetectiontask.
correctlyidentifiedandonethatwasnot.Whilethefirstandthird
werecorrectlypredictedwithhighconfidence,themiddlerowwas
incorrectlyclassifiedwitha“highconfidenceprediction”.Forthe 6.2 EvaluationofBiasDetectionModels
few instances that GlossBERT incurred false positives, a closer Firstly,wecomparetheperformanceofthefine-tunedBERTvari-
inspectionhasrevealedthatthoseexcerptsmaybelackingenough antsonthebiasdetectiontask.Table6displaysthemodels’perfor-
contextforthisspecifictask.Forexample,inthemiddlerow,the mancewithrespecttoprecision,recall,F1andF2score,andAUC.FAccTRec’24,October14–18,2024,Bari,Italy Buttsetal.
Table8:Performancemetricsand95%-CIsforRoBERTa,TinyLlamatrainedondatasetvariants(LN+XN*,LN+XN,LN).Best
resultsamongeachmodelvariants(resp.acrossallmodels)andstatisticaltiesshownarebolded(resp.underlined).
RoBERTa TinyLlama
Metric
LN+XN* LN+XN LN LN+XN* LN+XN LN
Precision 0.613±0.015 0.640±0.021 0.526±0.029 0.675±0.008 0.693±0.028 0.536±0.020
Recall 0.692±0.024 0.667±0.023 0.719±0.026 0.548±0.030 0.519±0.029 0.607±0.035
F1Score 0.650±0.013 0.652±0.017 0.606±0.017 0.604±0.021 0.593±0.017 0.568±0.016
F2Score 0.674±0.019 0.661±0.016 0.669±0.016 0.569±0.027 0.546±0.024 0.591±0.025
AUC 0.927±0.003 0.930±0.009 0.910±0.008 0.907±0.005 0.903±0.005 0.871±0.011
Table9:PerformanceMetricsand95%-CIsforFine-Tuned introducesaframeworkfordetectinganddiagnosingbiasinthe
ModelsagainstBaseline(∗Salavatietal.,2024).Bestresults medicalcurriculum, focusingon thedata guidingthese models
andstatisticaltiesshowninbold. ratherthanonthemodels’architecture.Weusemodelstrainedand
testedoninstructionalcontentannotatedbymedicalexpertsfor
Metric RoBERTa TinyLlama Baseline∗ bias.Wefocusonbiasrelatedtosex/gender,race/ethnicity,age,
Precision 0.613±0.015 0.675±0.008 0.504±0.054 and geography. Our method involves extracting non-annotated
Recall 0.692±0.024 0.548±0.030 0.812±0.069 samplesthatcontainasocialidentifierasnegativesamplesforthe
F1Score 0.650±0.014 0.604±0.021 0.615±0.022 biasclassifier.Forthoseextractednegatives,weemploywordsense
F2Score 0.674±0.019 0.569±0.027 0.717±0.027 disambiguationtocleanoutanythathaverace/ethnicity-related
AUC 0.927±0.003 0.907±0.005 0.923±0.004 termsbutarenotactuallyrelatedtothosecategories.
OurfindingsdemonstratethatwhileLLMscanhandlemany
While RoBERTa and DistilBERT are statistically tied, BioBERT
tasks,theyarenotwell-suitedforthisone.Ourzero-andfew-shot
clearly performs worst among all BERT models. We select the
promptingwithGPT-4ominiunderperformedcomparedtothebase-
RoBERTamodelforfurthercomparisonduetothemodel’shigher
linemodelfromourpreviousworkandscoredsignificantlylower
meanevaluationmetricswithlowerstandarddeviations.
thanthelanguagemodelswetested.Similarly,usingadomain-
Secondly,weevaluatetheperformanceofzero-andfew-shot
specificmodellikeBioBERTshowednosignificantimprovement.
prompting with GPT-4o mini on bias detection. Table 7 shows
RoBERTaandTinyLlamawerethebestperformersforbiasdetec-
theresultsobtainedusingthepromptingtechniquesoutlinedin
tion,withRoBERTamatchingthebaselineandshowingslightgains
Section 5.2. Despite the substantial increase in recall seen with
inprecisionandF1score.
few-shotprompting,thelowoverallperformanceofGPT-4omini
OurWSDmodelswerehighlyeffectiveatdistinguishingbiased
deemsitunsuitableforthebiasdetectiontask.
excerptsfromnon-biasedones.ALBERTandGlossBERTnearly
Next,wecomparethebestBERTmodelandabaselinefromour
perfectlydisambiguatedsentenceswithraceandethnicity-related
priorwork[25]withafine-tunedTinyLlama.Table9showsthe
keywords.AlthoughGPTmodelswerecomparabletoBERTmodels,
comparisonresults.AlthoughTinyLlamaachieveshighprecision,
BERTconsistentlyoutperformedGPTinallmetricsexceptrecall.
itslowerrecallcausesittobeoutperformedbyRoBERTaandby
Whilethistaskfocusedononebiascategory,thesemodelscould
the baseline with respect to both F1 and (especially) F2 scores.
beadaptedtoothertypeswithappropriateannotations.Applying
RoBERTaandthebaselinearestatisticallytiedwiththehighest
WSDtobiasdetectioninmedicalcurriculayieldedmixedresults.
AUCs(0.927±0.003and0.923±0.004),indicatingthatforeither
TheAUCforRoBERTawassimilartothebaseline,butWSDim-
modeltheclassificationthresholdcanbetunedtofindatrade-off
provedbothprecisionandF1score.
betweenprecisionandrecallsuitableforthetargetapplication.
This work could help identify potentially biased excerpts in
Last,weconductanablationtesttoassesstheimpactofWSD
medicalcurriculaforreviewbeforethey’reusedtotrainmodels
fordatarefinementbycomparingtheperformanceofRoBERTa
forfuturehealth-relatedapplicationsandrecommendersystems,
andTinyLlamaacrossvariousdatasetconfigurations.Theresultsin
contributingtomoreequitablehealthcareacrossalldemographics.
Table8showthattheLN+XN*settingledtohigherrecallaverages
thanLN+XN(despitenotstatisticallysignificant)atasmallcost
8 DISCUSSION
in precision. LN achieves the highest recall, but at a steep cost
DespitetheencouragingresultsprovidedbyourWSDandbias
inprecision.Therefore,LN+XN*resultsinthehighestF2scores,
classificationmodels,therearefuturedirectionswecantaketo
indicatingthatitisthemostadequatesettingforTAR(Technology
enhanceourproject’ssignificance.First,intheWSDexperiment,
AssistedReview)purposes.
usingChatGPT-4otogeneratemoresentencesnoticeablyincreased
theperformanceofourlanguagemodels.Hence,itislikelythat
7 CONCLUSION increasingthenumberofsyntheticsentencescanfurtherenhance
Despiterecentstridesinfairness,accountability,andtransparency, performanceifthesamplesarediverseenough.LLMsoftenhavea
health-related applications and recommender systems are still “temperature”parameterthatcancontroltheamountofrandomness
pronetobiasesamplifiedthroughdata,whichcanperpetuatehealth inthetextgeneration.However,excessivelyhightemperatures
disparitiesandaffectpatientcare.Tomitigatethisissue,thispaper couldalsoyieldlesscoherentsentences.TowardsFairerHealthRecommendations:findinginformativeunbiasedsamplesviaWordSenseDisambiguation FAccTRec’24,October14–18,2024,Bari,Italy
Wealsowanttoconsiderhowwordsensedisambiguationmight healthprofessionseducation:howeducators,facultydevelopers,andresearchers
beusefulinthecontextofothersocialidentifiers,suchasgeography canmakeadifference.AcademicMedicine,92(11S):S1–S6,2017.
[16] ShawnKhan,AbiramiKirubarajan,TahminaShamsheri,AdamClayton,and
(e.g.,“AmericanHeartAssociation”vs.“NativeAmericans”)and
GeetaMehta. Genderbiasinreferencelettersforresidencyandacademic
other domains where the tone of an excerpt is more important medicine:asystematicreview.Postgraduatemedicaljournal,99(1170):272–278,
whenevaluatingwordsense(e.g.,socialmedia). 2023.
[17] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusuke
Additionally,althoughLLMslikeGPTmodelshavesignificantly Iwasawa.Largelanguagemodelsarezero-shotreasoners.Advancesinneural
showntobeadvancedinnaturallanguageprocessing,theyalso informationprocessingsystems,35:22199–22213,2022.
presentaseriesofchallenges[19].Firstly,developingandtraining [18] WojciechKusa,GeorgiosPeikos,MoritzStaudinger,AldoLipani,andAllan
Hanbury.Normalisedprecisionatfixedrecallforevaluatingtar.InThe10thACM
LLMsrequirescomputationalcostandcanbetime-consuming.So, SIGIR/The14thInternationalConferenceontheTheoryofInformationRetrieval,
theymaybelessaccessibleforsmallergroupsofresearchers. 2024.
[19] HumzaNaveed,AsadUllahKhan,ShiQiu,MuhammadSaqib,SaeedAnwar,
MuhammadUsman,NickBarnes,andAjmalMian.Acomprehensiveoverview
oflargelanguagemodels.arXivpreprintarXiv:2307.06435,2023.
9 ACKNOWLEDGEMENTS [20] RajvardhanPatil,ThomasFHeston,andVijayBhuse. Promptengineeringin
ThismaterialisbaseduponworksupportedinpartbytheNational healthcare.Electronics,13(15):2961,2024.
[21] JhonnyPincay,LuisTerán,andEdyPortmann.Healthrecommendersystems:a
ScienceFoundationREUSiteGrant2349370andtheWPISTAR state-of-the-artreview.In2019SixthInternationalConferenceoneDemocracy&
Program.Anyopinions,findings,conclusions,orrecommendations eGovernment(ICEDEG),pages47–55.IEEE,2019.
[22] ShainaRaza,MuskanGarg,DeepakJohnReji,SyedRazaBashir,andChenDing.
expressedinthismaterialarethoseoftheauthor(s)anddonot
Nbias:Anaturallanguageprocessingframeworkforbiasidentificationintext.
necessarilyreflecttheviewsoftheNationalScienceFoundation. ExpertSystemswithApplications,237:121542,2024.
[23] LariaReynoldsandKyleMcDonell. Promptprogrammingforlargelanguage
models:Beyondthefew-shotparadigm. InExtendedabstractsofthe2021CHI
REFERENCES
conferenceonhumanfactorsincomputingsystems,pages1–7,2021.
[24] AbhayaKumarSahoo,ChittaranjanPradhan,RabindraKumarBarik,andHar-
[1] SarahEAli-Khan,TomaszKrakowski,RabiaTahir,andAbdallahSDaar.Theuse ishchandraDubey.Deepreco:deeplearningbasedhealthrecommendersystem
ofrace,ethnicityandancestryinhumangeneticresearch.TheHUGOjournal, usingcollaborativefiltering.Computation,7(2):25,2019.
5:47–63,2011. [25] ChimanSalavati,ShannonSong,WillmarSosaDiaz,ScottAHale,RobertoE
[2] AhmadAsadiandRezaSafabakhsh. Theencoder-decoderframeworkandits Montenegro,FabricioMurai,andShiriDori-Hacohen.Reducingbiasestowards
applications.Deeplearning:Conceptsandarchitectures,pages133–167,2020. minoritizedpopulationsinmedicalcurricularcontentviaartificialintelligence
[3] KunBu,YuanchaoLiu,andXiaolongJu. Efficientutilizationofpre-trained forfairerhealthoutcomes.arXivpreprintarXiv:2407.12680,2024.
models:Areviewofsentimentanalysisviapromptlearning.Knowledge-Based [26] LibbyTiderman,JuanSanchezMercedes,FionaRomanoschi,andFabricioMurai.
Systems,page111148,2023. Towardsdetectingcascadesofbiasedmedicalclaimsontwitter.In2023IEEEMIT
[4] RobertChallen,JoshuaDenny,MartinPitt,LukeGompels,TomEdwards,and UndergraduateResearchTechnologyConference(URTC),pages1–5,2023.
KrasimiraTsaneva-Atanasova. Artificialintelligence,biasandclinicalsafety. [27] ThiNgocTrangTran,AlexanderFelfernig,ChristophTrattner,andAndreas
BMJquality&safety,28(3):231–237,2019. Holzinger.Recommendersystemsinthehealthcaredomain:state-of-the-artand
[5] BenjaminClavié,AlexandruCiceu,FrederickNaylor,GuillaumeSoulié,and researchissues.JournalofIntelligentInformationSystems,57(1):171–201,2021.
ThomasBrightwell.Largelanguagemodelsintheworkplace:Acasestudyon [28] JenniferTsai,LauraUcik,NellBaldwin,ChristopherHasslinger,andPaulGeorge.
promptengineeringforjobtypeclassification. InInternationalConferenceon Racematters?examiningandrethinkingraceportrayalinpreclinicalmedical
ApplicationsofNaturalLanguagetoInformationSystems,pages3–17.Springer, education.AcademicMedicine,91(7):916–920,2016.
2023. [29] JiaqiWang,EnzeShi,SigangYu,ZihaoWu,ChongMa,HaixingDai,QiushiYang,
[6] GiuseppeColavito,FilippoLanubile,NicoleNovielli,andLuigiQuaranta.Leverag- YanqingKang,JinruWu,HuawenHu,etal.Promptengineeringforhealthcare:
inggpt-likellmstoautomateissuelabeling.In2024IEEE/ACM21stInternational Methodologiesandapplications.arXivpreprintarXiv:2304.14670,2023.
ConferenceonMiningSoftwareRepositories(MSR),pages469–480.IEEE,2024. [30] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,brianichter,Fei
[7] GordonVCormackandMauraRGrossman.Engineeringqualityandreliability Xia,EdChi,QuocVLe,andDennyZhou.Chain-of-thoughtpromptingelicits
intechnology-assistedreview.InProceedingsofthe39thInternationalACMSIGIR reasoninginlargelanguagemodels. InS.Koyejo,S.Mohamed,A.Agarwal,
conferenceonResearchandDevelopmentinInformationRetrieval,pages75–84, D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInformationProcessing
2016. Systems,volume35,pages24824–24837.CurranAssociates,Inc.,2022.
[8] LeonorCorsino,KenyonRailey,KatherineBrooks,DanielOstrovsky,SandroO [31] PeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama:An
Pinheiro,AlysonMcGhan-Johnson,andBlancaIrisPadilla.Theimpactofracial open-sourcesmalllanguagemodel.arXivpreprintarXiv:2401.02385,2024.
biasinpatientcareandmedicaleducation:let’sfocusontheeducator.MedEd-
PORTAL,17:11183,2021.
[9] ErinDehon,NicoleWeiss,JonathanJones,WhitneyFaulconer,ElizabethHinton,
andSarahSterling.Asystematicreviewoftheimpactofphysicianimplicitracial
biasonclinicaldecisionmaking.AcademicEmergencyMedicine,24(8):895–904,
2017.
[10] ShiriDori-Hacohen,RobertoMontenegro,FabricioMurai,ScottAHale,Keen
Sung,MichelaBlain,andJenniferEdwards-Johnson.Fairnessviaai:Biasreduc-
tioninmedicalinformation.arXivpreprintarXiv:2109.02202,2021.
[11] GuhaoFeng,BohangZhang,YuntianGu,HaotianYe,DiHe,andLiweiWang.
Towardsrevealingthemysterybehindchainofthought:Atheoreticalperspective.
InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,
AdvancesinNeuralInformationProcessingSystems,volume36,pages70757–70798.
CurranAssociates,Inc.,2023.
[12] MarkHalman,LindsayBaker,andStellaNg. Usingcriticalconsciousnessto
informhealthprofessionseducation:Aliteraturereview.Perspectivesonmedical
education,6:12–20,2017.
[13] DebraHowcroftandJillRubery.‘biasin,biasout’:genderequalityandthefuture
ofworkdebate.Labour&Industry:ajournalofthesocialandeconomicrelations
ofwork,29(2):213–227,2019.
[14] LindaMHunt,NicoleDTruesdell,andMetaJKreiner.Genes,race,andculture
inclinicalcare:racialprofilinginthemanagementofchronicillness.Medical
anthropologyquarterly,27(2):253–271,2013.
[15] Reena Karani, Lara Varpio, Win May, Tanya Horsley, John Chenault,
KarenHughesMiller,andBridgetO’Brien. Commentary:racismandbiasin