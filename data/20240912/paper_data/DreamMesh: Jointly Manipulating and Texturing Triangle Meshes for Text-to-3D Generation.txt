DreamMesh: Jointly Manipulating and Texturing
Triangle Meshes for Text-to-3D Generation
Haibo Yang1,2∗, Yang Chen3, Yingwei Pan3, Ting Yao3,
Zhineng Chen1,2†, Zuxuan Wu1,2, Yu-Gang Jiang1,2, and Tao Mei3
1 School of Computer Science, Fudan University
2 Shanghai Collaborative Innovation Center of Intelligent Visual Computing
3 HiDream.ai Inc.
yanghaibo.fdu@gmail.com, {c1enyang, pandy, tiyao}@hidream.ai,
{zhinchen, zxwu, ygj}@fudan.edu.cn, tmei@hidream.ai
Abstract. Learning radiance fields (NeRF) with powerful 2D diffusion
modelshasgarneredpopularityfortext-to-3Dgeneration.Nevertheless,
theimplicit3DrepresentationsofNeRFlackexplicitmodelingofmeshes
and textures over surfaces, and such surface-undefined way may suffer
from the issues, e.g., noisy surfaces with ambiguous texture details or
cross-view inconsistency. To alleviate this, we present DreamMesh, a
novel text-to-3D architecture that pivots on well-defined surfaces (tri-
angle meshes) to generate high-fidelity explicit 3D model. Technically,
DreamMesh capitalizes on a distinctive coarse-to-fine scheme. In the
coarse stage, the mesh is first deformed by text-guided Jacobians and
then DreamMesh textures the mesh with an interlaced use of 2D diffu-
sionmodelsinatuningfreemannerfrommultipleviewpoints.Inthefine
stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh signifi-
cantlyoutperformsstate-of-the-arttext-to-3Dmethodsinfaithfullygen-
erating 3D content with richer textual details and enhanced geometry.
Our project page is available at https://dreammesh.github.io.
Keywords: Text-to-3DGeneration·DiffusionModels·TriangleMeshes
1 Introduction
Diffusion models [16,17,55] have emerged as the basis of the powerful mod-
ern generative networks for producing realistic and diverse visual content (e.g.,
images and videos [8,9,31]). In between, a massive leap forward has been at-
tained in text-driven visual content generation tasks, e.g., text-to-image gen-
eration [29,34,36,38,39] and text-to-video generation [18,50,53]. The success
is attributed to several factors like billion-level multi-modal data and scalable
∗ ThisworkwasperformedwhenHaiboYangwasvisitingHiDream.aiasaresearch
intern.
† Corresponding author.
4202
peS
11
]VC.sc[
1v45470.9042:viXra2 H. Yang et al.
Prompt: “A human skull”
Prompt: “A high quality photo of a camel”
(a) Dreamfusion (b) LatentNeRF (c) SJC (d) Magic3D (e) Fantasia3D (f) ProlificDreamer (g) DreamMesh (Ours)
Fig.1: Existing methods [4,25,26,33,47,48] mostly hinge on implicit or hybrid 3D
representation and produce noisy surfaces. Instead, our DreamMesh pivots on com-
pletely explicit 3D representation, yielding high-quality 3D meshes that exhibit clean,
organized topology, devoid of any redundant vertices & faces.
denoisingdiffusion-basedgenerativemodeling.Nevertheless,itisnottrivialtodi-
rectly train a robust 3D-specific diffusion model for text-to-3D generation, since
the paired text-3D data is relatively scarce, and 3D scenes have more complex
geometric structures and multi-view visual appearances than 2D images.
The recent advance of Dreamfusion [33] nicely sidesteps the requirement of
massive paired text-3D data for text-to-3D generation task, and learns implicit
3D scene representation (NeRF [28]) with only 2D diffusion models pre-trained
over images. The core learning objective is to optimize implicit 3D scene with
2D observations of each sampled views derived from 2D diffusion models via
Score Distillation Sampling (SDS). Despite having impressive quantitative re-
sults through SDS, qualitative analysis shows that such text-to-3D generation
oftenresultsincross-viewinconsistencyorambiguoustexturedetailsduetothe
intrinsic bias of 2D diffusion priors. Later on, a series of efforts [4,25,26,47,48]
have been dedicated to upgrading the 2D diffusion priors in SDS with 3D-aware
knowledge,aimingtostrengthenthecapabilitiestoproducecross-viewconsistent
3D scene. Note that these text-to-3D works predominantly revolve around im-
plicit 3D scene representation of density-based geometry with undefined surface
boundaries. As shown in Figure 1, this surface-undefined framing easily leads
to noisy extracted surfaces and over-saturated/over-smoothed textures. More-
over, the learnt 3D assets with implicit 3D scene fail to be directly integrated
intographicspipeline,andnecessitateadditionalconversionfromimplicittoex-
plicit 3D scene. The conversion might inject more noise over surfaces, thereby
hindering the usage particularly in various high-quality 3D applications.
To address these challenges, our work shapes a new way to frame text-to-3D
generation on the basis of completely explicit 3D scene representation of the
ubiquitous and well-defined surface (triangle meshes). We propose a novel text-
to-3D framework, namely DreamMesh, that executes the learning of textured
trianglemeshesintotwostages.Specifically,inthefirstcoarsestage,DreamMesh
deformsthetrianglemeshesbytext-guidedJacobians,obtaininggloballysmoothDreamMesh 3
coarsemesh.Next,thecorrespondingcoarsetextureisattainedthroughatuning-
free process with an interlaced use of pre-trained 2D diffusion models. In the
second fine stage, DreamMesh jointly manipulates the coarse mesh and refines
the coarse texture map. This scheme learns the surface and material/texture of
explicit 3D representation in a coarse-to-fine fashion. Eventually the explicit 3D
model by DreamMesh faithfully reflects the high-quality geometry (clean and
organized topology) with rich texture details (see Figure 1).
Insummary,wehavemadethefollowingcontributions:(1)Wenovellyframe
text-to-3D generation based on a completely explicit 3D scene representation
of triangle meshes, which is shown capable of mitigating the issue associated
with implicit 3D scenes and learning more smooth surfaces. (2) The exquisitely
designed coarse-to-fine strategy pivoting on explicit 3D scene representation is
shown able to facilitate the manipulation and texturing of triangle meshes. (3)
The proposed DreamMesh has been analyzed and verified through extensive ex-
periments over a comprehensive text-to-3D benchmark (T3Bench [15]), demon-
strating superior results when compared to state-of-the-art approaches.
2 Related Works
Text-to-3D Generation. Recently, the text-to-3D generation has drawn in-
creasing research attention. Pioneering works [33,47] utilize pre-trained 2D dif-
fusion models to accomplish text-to-3D generation in a zero-shot fashion, mit-
igating the reliance on massive training data and becoming the mainstream.
The key technique underpinning these methods is score distillation sampling
(SDS), which enables distilling knowledge from the 2D diffusion model to opti-
mize an underlying 3D representation (e.g., NeRF [28]) and showcases remark-
able 3D generation capability. Subsequently, there has been a series of related
works [4–7,21,25,26,41,46,48,49,52,54] that continue to refine and strengthen
this methodology in different ways. For instance, Latent-NeRF [26] and Con-
trol3D [6] incorporate additional user-provided sketch mesh or image to guide
thetext-to-3Dgenerationprocess.Magic3D[25]andFantasia3D[4]upgradethe
implicitNeRFrepresentationusedinDreamFusiontoanimplicit-explicithybrid
3D representation (i.e., DMTet [40]) for SDS optimization on higher-resolution
renderings.VP3D[7]leverages2Dvisualprompttoboosttext-to-3Dgeneration.
Although the aforementioned works can generate high-quality renderings,
theyallpredominantlyadoptimplicit(NeRF)orimplicit-explicithybrid(DMTet)
3D representation. Integrating them into the mainstream graphics pipeline re-
quiresanadditionalconversionfromimplicit/hybrid3Drepresentationtowidely
used textured mesh. Unfortunately, this conversion may result in sub-optimal
results due to the absence of explicit modelingmeshes andtextures during opti-
mization, which prevents the usage of these methods in real-world deployments.
In contrast, we formulate the text-to-3D generation process from a new per-
spective based on the completely explicit 3D representation of triangle meshes,
leadingtomoreclean&well-organizedsurfacesandphoto-realistictexturesthat
can be seamlessly compatible with existing 3D engines (e.g., Blender).4 H. Yang et al.
Text-Driven Shape Manipulation/Texturing.Manipulatingandtexturing
3D shapes are key components in animation creation and computer-aided de-
sign pipelines, gaining a surge of interest in the literature. Classical approaches
[19,42,43,45]performshapemanipulationbypredictingmeshdeformationsfrom
user-provided handles and cast this problem as an optimization task, where the
sourcemeshisiterativelydeformedtominimizethefittingerrorfromthesource
to target shape. Instead of controlling the deformation through handle move-
ments, a recent work [13] guides the deformation process solely from a text
prompt by utilizing a pre-trained CLIP model [35] and differentiable rendering.
Anotherdirectionofresearchfocusesontext-driven3Dshapetexturingthatau-
tomatically generates textures for 3D bare meshes from the given text prompt.
State-of-the-art methods [3,37] utilize pre-trained diffusion models (e.g., depth-
to-image diffusion model and inpainting diffusion model) to “paint” the input
bare mesh with generated textures. Unlike the aforementioned approaches, our
work frames text-to-3D generation by jointly manipulating and texturing trian-
glemeshes.Insteadofformulatingeitherameshdeformationtaskorameshtex-
turingproblemin[3,12,13,37,49],weuniquelylookintothetext-to-3Dproblem
via generating explicit high-quality triangle meshes on the input text prompt.
3 Approach
3.1 Preliminaries
Wefirstbrieflyreviewthetypicalscoredistillationsampling(SDS)method,and
then discuss the relations and differences between our DreamMesh and related
methods based on implicit-explicit hybrid 3D representation.
Score Distillation Sampling (SDS). SDS is first introduced by Dreamfu-
sion[33]thatleveragespre-trainedtext-to-imagediffusionmodelstoenablezero-
shot text-to-3d generation. Specifically, DreamFusion employs Neural Radiance
Fields (NeRF) [28] to parameterize the implicit 3D scene as θ. Next, a differen-
tiable renderer is utilized to render an image x from the 3D scene. In an effort
to distill the knowledge of 2D diffusion model (e.g., Imagen [39]) into 3D scene,
random noise ϵ is initially added to the image x:
√ √
x = α¯x+ 1−α¯ϵ, (1)
t t t
where ϵ ∼ N(0,I), and α¯ is a time-variant constant. After that, DreamFusion
t
employs the denoiser of diffusion model (parameterized as ϵ ) to estimate the
ϕ
added noise ϵ from the noisy image x . The 3D scene parameters θ are thus
t
updatedaccordingtotheper-pixelgradientofdifferencebetweentheactualand
predicted noise:
(cid:20) (cid:21)
∂x
∇ L (ϕ,x)=E w(t)(ϵ (x ;y,t)−ϵ) , (2)
θ SDS t,ϵ ϕ t ∂θ
where w(t) is a weighting function and y is the input text prompt. In this way,
the pixel-level gradient is back-propagated to optimize the 3D scene, thereby
driving the learnt 3D scene to resemble the input text prompt.DreamMesh 5
Stage I: Generate Coarse Mesh and Texture Stage II: Jointly Refine Mesh and Texture
Input Base Mesh Coarse Mesh Coarse Mesh and Texture Coarse colored image
nnooiissee
ReD ndif efe rir ne gn t Pia ipb ele
lin e
rreeddooccnnEE
mmII
CJ oi
arse
TJ eL ae c xa o tr ubn i rae end s
CoaT ru sn ei n Tg e- xf tr ue re i ng
cP oa ar ra sm
e
e tete xtr uiz re
e
Ji
tL ee xoBa
ta nap
ur
n
c
tn
a rod
k
ee
r
p
a
d
mM prm
o
aJ eaa
rpe st
ac hateo
me
g
r
ab
ri
a
ea s
ni ta
l
tde
en rs
L
s
refiy nP e er llo om wp rt u: b“ bA e b r r dig uh ct k, ”
rreeddee ooD cD c
rreenniiffeeRR
eeggaa
eeggaa
oommI
-t
-I
-t
-
Refined colored image Image Refine Process
Fig.2: An overview of our DreamMesh that fully capitalizes on explicit 3D scene
representation (triangle meshes) for text-to-3D generation in a coarse-to-fine scheme.
In the first coarse stage, DreamMesh learns text-guided Jacobians matrices to deform
a base mesh into the coarse mesh, and then textures it through a tuning-free process.
In the second fine stage, both coarse mesh and texture are jointly optimized, yielding
high-quality mesh with high-fidelity texture.
Implicit-Explicit Hybrid 3D Representations. Recent advances in text-
to-3D generation predominantly employ implicit representations [28] for mod-
eling 3D scenes. A notable limitation of these techniques is the low-quality ex-
plicit mesh with noisy surfaces extracted from implicit fields. To alleviate this
issue, several methods [4,25,48] employ implicit-explicit hybrid 3D representa-
tion (DMTet [40]). Nevertheless, the meshes extracted from these DMTet-based
methods still suffer from the problems such as excessive faces and poor topo-
logical structures. Such downside severely hinders the seamless integration of
thesemeshesintotraditionalgraphicsrenderingpipelines,therebylimitingtheir
deployments in standard visualization and animation processes. As an alterna-
tive, our DreamMesh pivots on completely explicit 3D representation (triangle
meshes)formodeling3Dobjects,therebygettingridofthegapbetweenimplicit
and explicit representations. Formally, let M be a given base triangular mesh,
0
which comprises a set of vertices V ∈Rn×3 and triangular faces T ∈Rm×3. The
basemeshcanbeabasicsphere,user-provided,oralow-qualitymeshgenerated
by 3D generative methods [10,20,24,30]. Through joint manipulation and tex-
turing of triangle meshes, our DreamMesh learns and refines the 3D mesh and
the corresponding mesh textures T that adhere to the given text prompt y.
3.2 DreamMesh Optimization
In this section, we elaborate our DreamMesh, which frames text-to-3D genera-
tion based on completely explicit 3D representation in a coarse-to-fine fashion.
Figure 2 depicts the detailed framework, consisting of two stages: the coarse
stage to produce coarse mesh and texture, and the fine stage to jointly refine
mesh and texture with a diffusion-based 2D image-to-image refiner.6 H. Yang et al.
Stage I: Generate Coarse Mesh and Texture. In this stage, we aim to
generate a coarse triangle mesh and textured map that respects the input text
prompt. To achieve this, we first deform a base mesh into the coarse mesh and
then texture it through a tuning-free process. Practically, we use Shap-E [20]
outputsastheinputbasemesh.NotethatShap-Eistrainedonmillionsofpaired
text-3Ddataandcaneasilyproduce3Dobjectswithreasonablegeometry.That
makes Shpe-E an ideal choice to serve as the initial base mesh.
CoarseMeshDeformation.GivenabasetriangularmeshM ,DreamMesh
0
firstlearnstodeformitintoatargettriangularmeshthatfaithfullymatchesthe
input text prompt. Technically, we formulate this learning process as the op-
timization of a displacement map D : R3×3 → R3×3 over the vertices. Such
piecewise linear mapping D of a mesh can be defined by assigning a new posi-
tion to each one of the vertices: V → D . Nevertheless, direct optimization on
i i
the vertex positions of a triangular mesh can easily suffer from degeneracy and
localminima,andthusoverlydistortstheoriginalshape.Toalleviatethisissue,
we take the inspiration from [1,13] and parameterize the mesh deformation by
using a set of per-triangle Jacobians J = D∇T(J ∈ R3×3), where ∇T is the
i i i i
gradient operator of triangle t ∈ T. Given an arbitrary assignment of input
i
matrix M ∈ R3×3 for every triangle, we can achieve new vertex positions D∗
i
whose Jacobians J =D∗∇T are least-squares closest to M . And we can easily
i i i
obtain the deformed vertex positions D∗ by solving linear system:
D∗ =L−1A∇TM, (3)
where A is the mesh’s mass matrix, L is the mesh’s Laplacian, and M is the
stacking of the input matrices M . Accordingly, the optimization of the defor-
i
mation mapping (D) can be interpreted as the optimization of the learnable
Jacobians matrices M . In practice, we initialize these Jacobians matrices as
i
identity matrices, where D∗ is inherently established as the identity mapping.
Please refer to [1] for the full technical details.
Coarse Diffusion Guidance. To achieve text-driven deformation D∗ that
aligns with input text prompt, we exploit the powerful text-to-image diffusion
model (Stable Diffusion [38]) as coarse diffusion guidance to facilitate Jacobians
deformation. Specifically, given the base mesh M and deformation mapping
0
D∗, we utilize a differentiable renderer g [23] to render a normal map n:
n
n=g (D∗(M ),c), (4)
n 0
wherecrepresentsacameraposethatisarbitrarilysampledwithinthespherical
coordinate system. Such random sampling strategy ensures a uniform distribu-
tion of camera poses across the sphere, providing comprehensive coverage and
variability. Next, during t-th timestep of diffusion process, we encode the ren-
dered normal map n into the latent space to obtain the latent code zn, and add
Gaussian noise ϵ to get zn. The typical latent space SDS loss is thus utilized to
t
optimize the deformation D∗ by measuring the gradient w.r.t. D∗ as:
(cid:20)
∂n
∂zn(cid:21)
∇ L (ϕ,n)=E w(t)(ϵˆ (zn;y,t)−ϵ) , (5)
D∗ SDS ϕ t ∂D∗ ∂nDreamMesh 7
whereϵˆ denotesdenoiserinStableDiffusion.Itisworthytonotethatinsteadof
ϕ
using Stable Diffusion’s image encoder, here we exploit a downsampled version
ofthenormalmapnasthelatentcode[4,26],whichleadstoafasterconvergence
ofD∗.ByrandomlysamplingviewsandbackpropagatingthegradientinEq.(5)
to the learnable parameters in D∗, this way eventually achieves a target coarse
mesh M =D∗(M ) that resembles the input text prompt.
1 0
CoarseTextureGeneration.Next,wetargetforproducingrealisticcoarse
texturesforthelearntcoarsemeshM .Weapplyatuning-freeapproachtopro-
1
gressivelygeneratecoarsetexturesonthe3Dtrianglemeshwithaninterlaceduse
of pre-trained 2D diffusion models [37]. In particular, the texture is represented
as an atlas (T ) learnt through UV mapping process [51]. At the initialization
0
step, we use a differentiable renderer R [11] to render a depth map D from
0
an arbitrary initial viewpoint v , and use a pretrained depth-to-image diffusion
0
modelM [38]conditionedontherendereddepthmaptogenerateaninitial
depth
colored image I . The generated image I is then projected back to the texture
0 0
atlas T to color the shape’s visible parts from v . Following this initialization
0 0
step, we iteratively change the viewpoint and alternatively use a pretrained in-
paintingdiffusionmodelM [38]orM togenerationnewcoloredimage.
paint depth
These colored images are projected back onto the initial texture T . We repeat
0
this process until a complete coarse texture map T is formed.
1
Stage II: Jointly Refine Mesh and Texture. Recall that at the first coarse
stage,theoptimizationprocessofcoarsemeshdeformationsolelyfocusesonthe
primary mesh irrespective of any texture. Such process might inevitably simu-
late textured results and lead to excessive modifications of meshes. Meanwhile,
the coarse texture generation in first stage also encounters the inconsistency is-
sue across all viewpoints. We speculate that the sub-optimal texturing results
might be caused by the tuning-free texturing strategy that performs progres-
sive texture mapping starting from a singular viewpoint, and it is non-trivial to
maintain both local and global consistency. To alleviate these, we novelly devise
a fine stage to jointly optimize both the mesh and texture with fine diffusion
guidancederivedfromapretraineddiffusion-basedimagerefiner[32].Thisinno-
vative stage establishes a symbiotic relationship between the learnt meshes and
textures, harmonizing the enhancement that contributes to the overall realism
and consistency of synthetic 3D content.
Technically, in the fine stage, we adopt the same mesh deformation method-
ologyasinthecoarsestage,i.e.,theoptimizationofJacobianMatrices,torefine
the coarse mesh M . However, different from the tuning-free texturing strategy
1
in coarse stage, here we concurrently parameterize the coarse texture map T ,
1
and trigger a joint optimization of M and T in the fine stage. By doing so, we
1 1
employ a differentiable rendering pipeline g, which includes a sequence of mesh
operations,arasterizer,andadeferredshadingstage[14]torenderacoarsecol-
ored image x derived from the deformering mesh M and parameterized
coarse 1
texture map T , conditioned on a random camera pose c:
1
x =g(D∗ (M ),T ,c). (6)
coarse fine 1 18 H. Yang et al.
FineDiffusionGuidance.Ourfinestagenecessitatesintricateadjustments
tothecoarsemeshstructureanddedicatedeffortstoenhancetextureconsistency.
Onenaturalwaytooptimizesuchfineprocessistousethesamecoarsediffusion
guidance(SDSloss)asincoarsestageforsupervision.Nevertheless,suchwaywill
result in various artifacts like oversaturated color blocks. Instead, we excavate
thefinediffusionguidancebyadditionallyrefiningrenderedcoarsecoloredimage
x with diffusion-based image refiner E [32]. This refined colored image
coarse
x =E(x ,y)isfurtherutilizedtoguidethejointoptimizationofmesh
refine coarse
and texture through Mean Squared Error (MSE) loss:
L (D∗ ,T )=||x −x ||2
refine fine 1 refine coarse 2 (7)
=||E(g(D∗ (M ),T ,c),y)−g(D∗ (M ),T ,c)||2.
fine 1 1 fine 1 1 2
Byminimizingthisobjective,ourDreamMeshenforcestherenderedimagex
coarse
visually similar as the refined image x that faithfully matches with text
refine
prompt, thereby yielding high-quality mesh with high-fidelity texture map.
Discussion. Some related works [3,13,37] also explore text-driven mesh de-
formationortexturing,whileourDreamMeshtargetsforadifferenttaskoftext-
to-3D generation. Instead of a simple combination of existing mesh deformation
& texturing techniques(seedegenerated results inFig. 5),our DreamMeshnov-
elly upgrades mesh deformation with geometry-aware supervision and further
bridges both worlds by jointly optimizing mesh and texture with a fine-grained
image-to-image refiner in fine stage.
4 Experiments
4.1 Experimental Settings
ImplementionDetails.Atthecoarsestage,weutilizeanAdamoptimizerwith
a learning rate of 2×10−3 and render 12 normal maps per iteration. The coarse
textures are generated from 10 different viewpoints. In the fine stage, we set the
learningrateas2×10−3 formeshoptimizationand1×10−2 fortexturematerial
refinement.Thediffusion-basedimagerefinerperformsdenoisingoperationsover
15 steps to produce refined images. The whole experiment is conducted on a
single NVIDIA RTX 3090 GPU, and the learning process of each sample takes
approximately 30 minutes.
Dataset.Mostexistingtext-to-3Dgenerationmethodssolelyperformcasestud-
ies and user surveys for evaluation, but lack quantitative assessment due to the
absenceofstandardbenchmark.ThankstothenewlyreleasedT3Bench[15],we
perform quantitative comparisons over this first comprehensive benchmark for
text-to-3D generation. Specifically, T3Bench contains 100 test prompts for each
of three categories (single object, single object with surroundings, and multiple
objects). Two automatic metrics are designed to evaluate the subjective quality
andtextualalignment,basedontherenderedmulti-viewimagesgeneratedfrom
3D models. The quality metric combines multi-view text-image scoring with re-
gional convolution to assess both quality and view consistency. The alignmentDreamMesh 9
Table 1: Quantitative comparison between our DreamMesh and various text-to-3D
generation approaches on T3Bench [15] benchmark.
3DRepresentation SingleObject SingleObjectwithSurroundings MultipleObjects
QualityAlignmentAverageQualityAlignment Average QualityAlignmentAverage
Dreamfusion[33] Implict 24.9 24.0 24.4 19.3 29.8 24.6 17.3 14.8 16.1
LatentNeRF[26] Representation 34.2 32.0 33.1 23.7 37.5 30.6 21.7 19.5 20.6
SJC[47] (NeRF[28]) 26.3 23.0 24.7 17.3 22.3 19.8 17.7 5.8 11.7
Magic3D[25] Implicit-Explicit 38.7 35.3 37.0 29.8 41.0 35.4 26.6 24.8 25.7
Fantasia3D[4] HybridRepresentation 29.2 23.5 26.4 21.9 32.0 27.0 22.7 14.3 18.5
ProlificDreamer[48] (DMTet[40]) 51.1 47.8 49.4 42.5 47.0 44.8 45.7 25.8 35.8
ExplicitRepresentation
DreamMesh(Ours) (TriangleMesh) 55.6 53.8 54.7 43.1 54.3 48.7 47.6 30.8 39.2
metric first employs 3D-to-text caption model to achieve multi-view captions
and then leverages Large Language Model (GPT-4) to merge captions into 3D
caption for text-3D alignment assessment.
Compared Methods. To empirically verify the merit of our DreamMesh, we
include six state-of-the-art approaches for comparison. Specifically, DreamFu-
sion[33],LatentNeRF[26],andSJC[47]fullyhingeonimplicit3Drepresenta-
tion (NeRF [28]) for text-to-3D generation. Magic3D [25] upgrades DreamFu-
sion with additional stage that capitalizes on implicit-explicit hybrid 3D rep-
resentation (DMTet [40]) to enhance texture details. Fantasia3D [4] disen-
tangles geometry and appearance modeling in two stages, i.e., first generating
meshes based on DMTet and then leveraging Bidirectional Reflectance Distri-
bution Function (BRDF) to produce textures. ProlificDreamer [48] extends
Magic3D by generalizing SDS in the variational formulation, aiming to alleviate
the restricted diversity issue rooted in typical SDS.
4.2 Quantitative Results
Table 1 summarizes the quantitative performance comparisons over three cate-
gories of T3Bench benchmark between our DreamMesh and six state-of-the-art
approaches. Overall, for each category of text prompts, our DreamMesh con-
sistently achieves better performances against the existing methods across all
metrics,includingbothimplicit3Drepresentation-basedmethods(Dreamfusion,
LatentNeRF, SJC) and implicit-explicit hybrid 3D representation-based meth-
ods (Magic3D, Fantasia3D, ProlificDreamer). In particular, the average score of
quality and alignment of DreamMesh can reach 54.7%, 48.7%, and 39.2% for
eachcategory,whichleadstotheabsoluteimprovementof5.3%,3.9%,and3.4%
againstthebestcompetitorProlificDreamer.Theresultsclearlydemonstratethe
key advantage of joint manipulating and texturing based on completely explicit
3D representation to facilitate text-to-3D generation.
Morespecifically,Dreamfusionenablesazero-shotsolutionofoptimizingim-
plicit3Drepresentationwith2Ddiffusionpriors,yieldingpromisingresultseven
underchallengingpromptsinthecategoriesof“singleobjectwithsurroundings”
and “multiple objects”). SJC remoulds Dreamfusion by performing score jaco-
bian chaining within the voxel version of NeRF [2,44], which easily results in10 H. Yang et al.
degraded 3D models with a significant amount of floating density. In contrast,
LatentNeRF upgrades Dreamfusion with a coarse-to-fine paradigm by using the
implicit 3D representations of latent NeRF that is more tailored to the 2D la-
tentdiffusionmodel(StableDiffusion[38]),therebyleadingtoclearperformance
boosts.Furthermore,comparedtoaforementionedthreemethodsthatsolelyex-
ploitimplicit3Drepresentations,Magic3Dexhibitsbetterperformancesbycapi-
talizingonimplicit-explicithybrid3Drepresentation(DMTet)tolearntextured
3Dmesh.Fantasia3DalsoexploresDMTetinadecoupledmeshgenerationstage
andleveragesBRDFfortexturegenerationthatemphasizesrichobjecttextures,
while it fails to create complex and high-fidelity meshes (e.g., “multiple objects”
category).ProlificDreamerfurtherboostsuptheperformancesbyupgradingSDS
of Magic3D in variational formulation to address the restricted diversity issue.
Nevertheless,ProlificDreamerstillreliesonimplicit-explicithybrid3Drepresen-
tation (DMTet) and commonly suffers from noisy surfaces with over-smoothed
textures. In contrast, our DreamMesh completely eliminates the use of implicit
3D representation and achieves the best performances through coarse-to-fine
strategy pivoting on explicit 3D representation of triangle meshes.
4.3 Qualitative Results
AsindicatedbytheseexemplarresultsinFigure3,allthemethodscangenerate
somewhatreasonablemeshesandtextures,whileourDreamMeshcansynthesize
higherqualitymesheswithrichertexturesthatfaithfullyadheretotextprompts
by pivoting on completely explicit 3D representation. For instance, given the
first text prompt “A bright red fire hydrant”, the implicit 3D representation-
based methods (Dreamfusion, LatentNeRF, SJC) produce noisy surfaces and
simple textures with obvious deformation. By exploring implicit-explicit hybrid
3Drepresentationfortext-to-3Dgeneration,Magic3D,Fantasia3D,andProlific-
Dreamer further yield more complete and accurate meshes. Nevertheless, these
meshes generated via DMTet are inherently complex, necessitating excessive
faces and vertices, resulting in unsatisfactory triangle topologies. In contrast,
our DreamMesh novelly manipulates and textures well-defined surface (triangle
meshes), leading to high-quality textured meshes that reflect clean and well-
organized topology with neatly arranged vertices, edges, and faces.
4.4 Experimental Analysis
Ablation Studies. In an effort to study the effectiveness of each design in our
DreamMesh,wedepictthequalitativeresultsofseveralablatedrunsinFigure4.
DreamMesh− isanalternativeversionofourcoarsestagebysimultaneously
coarse
optimizing mesh and texture from scratch via typical SDS, which easily leads
to sub-optimal results. By decoupling mesh deformation and texturing into two
separate processes, DreamMesh further enhances the quality of textured
coarse
meshes. The results validate the effectiveness of decoupled mesh and texture
modeling in coarse stage. Nevertheless, DreamMesh still suffers from some
coarseDreamMesh 11
Prompt: “A bright red fire hydrant”
Prompt: “A corgi puppy”
Prompt: “Zombie bust, terror, 123dsculpt, bust, zombie”
(a) Dreamfusion (b) LatentNeRF (c) SJC (d) Magic3D (e) Fantasia3D (f) ProlificDreamer(g) DreamMesh (Ours)
Fig.3:Qualitativecomparisonoftextureandwireframeresults(renderinginBlender)
between our DreamMesh and other baseline methods.
overly distorted meshes. DreamMesh upgrades DreamMesh with an addi-
coarse
tional fine stage to jointly manipulate coarse meshes and refine coarse textures,
yielding higher-quality fine meshes and textures.
Tofurtherverifytheleverageofexplicit3Drepresentationfortext-to-3Dgen-
eration,wealsocompareourDreamMeshwiththerunofintegratingthestate-of-
the-arttextdrivenmeshdeformationtechnique[13]andtheadvancedtexturing
methods of TEXTure [37] or Text2Tex [3]. Figure 5 shows the comparisons.
It is not surprising that TextDeformer and TextDeformer+TEXTure/Text2Tex
produce unsatisfactory surfaces and textures since these methods are not par-
ticularly tailored for text-to-3D generation. Our DreamMesh, in comparison,
introduces more powerful diffusion model for mesh deformation and benefits
from the coarse-to-fine optimization scheme, making the mesh more clean and
the texture more realistic. Moreover, we conduct quantitative evaluations for
the aforementioned runs on a random subset of T3bench (50 prompts) and Ta-12 H. Yang et al.
Prompt: “a giraffe raises its front feet”
((aa)) DDrreeaammMMeesshhcc——ooaarrssee (b) DreamMeshcoarse (c) DreamMesh
Fig.4: Ablation study of our DreamMesh given the same text prompt.
DreamMesh− is a degraded version of coarse stage that jointly optimizes mesh
coarse
deformationandtexturesviaSDS.DreamMesh isthecompletecoarsestagethat
coarse
decouples the learning of coarse meshes and textures. DreamMesh is our full run with
both coarse and fine stages.
Table 2: Quantitative comparisons on the T3Bench subset.
TextDeformer+TEXTureTextDeformer+Text2TexDreamMesh− coarseDreamMeshcoarseDreamMesh
Quality 20.2 19.3 24.4 40.5 47.1
Alignment 19.0 16.5 22.0 38.5 45.5
Average 19.6 17.9 23.2 39.5 46.3
ble 2 lists the results. The runs of TextDeformer+TEXTure/Text2Tex indicate
poorest quality and alignment, again revealing the weakness of simple combi-
nation of mesh deformation and texture techniques for text-to-3D generation.
DreamMesh− is inferior to DreamMesh , showing that entangled opti-
coarse coarse
mizing mesh and texture from scratch is more challenging. Finally, DreamMesh
achieves the best performance, validating the effectiveness of our exquisitely de-
signed coarse-to-fine strategy.
Comparison against Typical Mesh-based Methods without Diffusion
Model.Itisworthytonotethatsomeearlyattempts(e.g.,Text2Mesh[27]and
CLIP-Mesh [22]) also explore explicit 3D representation for text-to-3D genera-
tion, while no powerful diffusion model is adopted. Figure 6 shows the qualita-
tivecomparisonbetweenourDreamMeshagainstText2MeshandCLIP-Mesh.In
general, our DreamMesh significantly outperforms the conventional mesh-based
methodswithregardtobothmesh/texturequalityandtext-3Dalignment.This
confirms the merit of exploiting explicit 3D representation for text-to-3D gener-
ation conditioned on powerful 2D diffusion priors.
Comparison against ProlificDreamer with Manual Post-processing.
Recall that both implicit and implicit-explicit hybrid representations of NeRF
and DMTet can be transformed into explicit meshes by using the Marching
Cubes and Marching Tetrahedral layer respectively. However, such automaticDreamMesh 13
Prompt: “a high quality photo of a stag”
Prompt: “a plush dragon toy”
(a) TextDeformer (b) +TEXTure (c) +Text2Tex (d) DreamMesh (Ours)
Fig.5: Comparisons between our DreamMesh and the integration of the state-of-the-
art text driven mesh deformation technique [13] and the advanced texturing methods
of TEXTure [37] or Text2Tex [3] for text-to-3D generation.
Prompt: “An intricate ceramic vase with peonies painted on it”
(a) Text2Mesh (b) CLIP-Mesh (c) DreamMesh (Ours)
Fig.6:Qualitativecomparisonoftextureandwireframeresults(renderinginBlender)
against Text2Mesh and CLIP-Mesh.
conversion commonly injects more noise over surfaces and leads to extremely
complex meshes containing a large number of vertices, edges, and faces (e.g.,
36,389 faces in Figure 7 (a)). To alleviate this issue, manual post-processing
(e.g., cleaning, smoothing and simplification) can be employed to improve the
mesh quality, while losing many geometric details (e.g., the missing of nose in
Figure7(b)).Incontrast,asshowninFigure7(c),ourDreamMeshmanagesto
achieve high-quality textured meshes that exhibit clean and organized topology.
Comparison with Different Base Meshes. Our DreamMesh is able to per-
form text-to-3D generation based on different kinds of input base meshes. For
example, given the input text prompt “A recliner chair”, we can generate it di-
rectly from a sphere. Alternatively, users can quickly create a rough 3D shape
that approximately aligns with text prompt in 3D engines (e.g., Blender). Such
user-provided rough mash can be fed into DreamMesh. Additionally, we can
take the low-quality mesh automatically generated by 3D generative models
(e.g.,Shap-E[20])astheinputs.AsshowninFigure8,alltext-to-3Dgeneration
results with different base meshes (i.e., basic sphere, user-provided shape, or
Shap-E outputs) can produce higher-quality 3D assets, which generally demon-
strate the generalization ability of our DreamMesh.14 H. Yang et al.
Prompt: “Jack Sparrow wearing sunglasses, head, photorealistic, 8K, HD”
(a) ProlificDreamer #Faces: 36389 (b) Post-Processing Results #Faces: 9400 (c) DreamMesh (Ours) #Faces: 9384
Fig.7:Qualitativecomparisonoftextureandwireframeresults(renderinginBlender)
against ProlificDreamer with additional manual post-processing.
Prompt: “A recliner chair”
Input mesh DreamMesh result Input mesh DreamMesh result Input mesh DreamMesh result
(a) Sphere base (b) User-provided base (c) Shap-E base
Fig.8: Text-to-3D generation results (rendering in Blender) of DreamMesh with dif-
ferent input base meshes.
5 Conclusions
In this paper, we propose DreamMesh, a novel framework for text-to-3D gener-
ation that fully relies on explicit 3D representations in a coarse-to-fine manner.
Specifically, in the coarse stage, we leverage neural jacobian fields to deform a
trianglemeshandthentexturethegeneratedcoarsemeshthroughatuning-free
process with an interlaced use of pre-trained 2D diffusion models. In the fine
stage, we jointly refine the coarse mesh and texture to produce high-quality 3D
model with rich texture details and enhanced 3D geometry. We evaluate our
proposal on T3Bench benchmark and demonstrate its superiority over state-of-
the-art techniques through both qualitative and quantitative comparisons.
Limitations and Broader Impact. DreamMesh may suffer from multi-
face Janus problems in some cases due to the limited 3D awareness of the prior
2D diffusion model. Finetuning the diffusion model on 3D data might alleviate
this problem. Since the generated meshes can be seamlessly compatible with
existing3Dengines,DreamMeshhasgreatpotentialtodisplacecreativeworkers
viaautomation,whichmayenablegrowthforthecreativeindustry.Nevertheless,
it could also be potentially applied to unexpected scenarios such as generating
fake and malicious content, which needs more caution.
Acknowledgement: ThisworkissupportedbytheNationalNaturalScience
Foundation of China (No. 32341012 and No. 62172103).DreamMesh 15
References
1. Aigerman,N.,Gupta,K.,Kim,V.G.,Chaudhuri,S.,Saito,J.,Groueix,T.:Neural
jacobian fields: Learning intrinsic mappings of arbitrary meshes. In: SIGGRAPH
(2022)
2. Chen,A., Xu,Z., Geiger, A.,Yu,J., Su,H.: Tensorf: Tensorial radiancefields. In:
ECCV (2022)
3. Chen, D.Z., Siddiqui, Y., Lee, H.Y., Tulyakov, S., Nießner, M.: Text2tex: Text-
driven texture synthesis via diffusion models. In: ICCV (2023)
4. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and
appearance for high-quality text-to-3d content creation. In: ICCV (2023)
5. Chen,Y.,Chen,J.,Pan,Y.,Tian,X.,Mei,T.:3dcreationatyourfingertips:From
text or image to 3d assets. In: ACM MM (2023)
6. Chen,Y.,Pan,Y.,Li,Y.,Yao,T.,Mei,T.:Control3d:Towardscontrollabletext-
to-3d generation. In: ACM MM (2023)
7. Chen,Y.,Pan,Y.,Yang,H.,Yao,T.,Mei,T.:Vp3d:Unleashing2dvisualprompt
for text-to-3d generation. In: CVPR (2024)
8. Chen, Y., Pan, Y., Yao, T., Tian, X., Mei, T.: Animating your life: Real-time
video-to-animation translation. In: ACM MM (2019)
9. Chen, Y., Pan, Y., Yao, T., Tian, X., Mei, T.: Mocycle-gan: Unpaired video-to-
video translation. In: ACM MM (2019)
10. Cheng,Y.C.,Lee,H.Y.,Tuyakov,S.,Schwing,A.,Gui,L.:SDFusion:Multimodal
3d shape completion, reconstruction, and generation. In: CVPR (2023)
11. Fuji Tsang, C., Shugrina, M., Lafleche, J.F., Takikawa, T., Wang, J., Loop, C.,
Chen, W., Jatavallabhula, K.M., Smith, E., Rozantsev, A., Perel, O., Shen, T.,
Gao, J., Fidler, S., State, G., Gorski, J., Xiang, T., Li, J., Li, M., Lebaredian,
R.: Kaolin: A pytorch library for accelerating 3d deep learning research. https:
//github.com/NVIDIAGameWorks/kaolin (2022)
12. Gao,C.,Jiang,B.,Li,X.,Zhang,Y.,Yu,Q.:Genesistex:Adaptingimagedenoising
diffusion to texture space. In: CVPR (2024)
13. Gao, W., Aigerman, N., Thibault, G., Kim, V., Hanocka, R.: Textdeformer: Ge-
ometry manipulation using text guidance. In: SIGGRAPH (2023)
14. Hasselgren, J., Munkberg, J., Lehtinen, J., Aittala, M., Laine, S.: Appearance-
driven automatic 3d model simplification. In: EGSR (2021)
15. He, Y., Bai, Y., Lin, M., Zhao, W., Hu, Y., Sheng, J., Yi, R., Li, J., Liu, Y.J.:
T3bench: Benchmarking current progress in text-to-3d generation. arXiv preprint
arXiv:2310.02977 (2023)
16. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020)
17. Ho, J., Salimans, T.: Classifier-free diffusion guidance. In: NeurIPS Workshop
(2022)
18. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
diffusion models. arXiv preprint arXiv:2204.03458 (2022)
19. Huang, Q., Huang, X., Sun, B., Zhang, Z., Jiang, J., Bajaj, C.: Arapreg: An as-
rigid-as possible regularization loss for learning deformable shape generators. In:
ICCV (2021)
20. Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv
preprint arXiv:2305.02463 (2023)
21. Katzir, O., Patashnik, O., Cohen-Or, D., Lischinski, D.: Noise-free score distilla-
tion. In: ICLR (2024)16 H. Yang et al.
22. Khalid,N.M.,Xie,T.,Belilovsky,E.,Tiberiu,P.:Clip-mesh:Generatingtextured
meshes from text using pretrained image-text models. In: SIGGRAPH (2022)
23. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., Aila, T.: Modular primi-
tives for high-performance differentiable rendering. ACM Trans. Graph. (2020)
24. Li, M., Duan, Y., Zhou, J., Lu, J.: Diffusion-sdf: Text-to-shape via voxelized dif-
fusion. In: CVPR (2023)
25. Lin,C.H.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,
S.,Liu,M.Y.,Lin,T.Y.:Magic3d:High-resolutiontext-to-3dcontentcreation.In:
CVPR (2023)
26. Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf
for shape-guided generation of 3d shapes and textures. In: CVPR (2023)
27. Michel,O.,Bar-On,R.,Liu,R.,Benaim,S.,Hanocka,R.:Text2mesh:Text-driven
neural stylization for meshes. In: CVPR (2022)
28. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.In:ECCV
(2020)
29. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)
30. Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for
generating3dpointcloudsfromcomplexprompts.arXivpreprintarXiv:2212.08751
(2022)
31. Pan, Y., Qiu, Z., Yao, T., Li, H., Mei, T.: To create what you tell: Generating
videos from captions. In: ACM Multimedia (2017)
32. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Müller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. In: ICLR (2024)
33. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. In: ICLR (2023)
34. Qian, Y., Cai, Q., Pan, Y., Li, Y., Yao, T., Sun, Q., Mei, T.: Boosting diffusion
models with moving average sampling in frequency domain. In: CVPR (2024)
35. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021)
36. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
(2022)
37. Richardson, E., Metzer, G., Alaluf, Y., Giryes, R., Cohen-Or, D.: Texture: Text-
guided texturing of 3d shapes. In: SIGGRAPH (2023)
38. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
39. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J.,
Norouzi,M.:Photorealistictext-to-imagediffusionmodelswithdeeplanguageun-
derstanding. In: NeurIPS (2022)
40. Shen, T., Gao, J., Yin, K., Liu, M.Y., Fidler, S.: Deep marching tetrahedra: a
hybrid representation for high-resolution 3d shape synthesis. In: NeurIPS (2021)
41. Shi,Y.,Wang,P.,Ye,J.,Long,M.,Li,K.,Yang,X.:Mvdream:Multi-viewdiffusion
for 3d generation. In: ICLR (2024)
42. Sorkine, O., Alexa, M.: As-rigid-as-possible surface modeling. In: SGP. Citeseer
(2007)DreamMesh 17
43. Sorkine,O.,Cohen-Or,D.,Lipman,Y.,Alexa,M.,Rössl,C.,Seidel,H.P.:Laplacian
surface editing. In: SGP (2004)
44. Sun,C.,Sun,M.,Chen,H.:Directvoxelgridoptimization:Super-fastconvergence
for radiance fields reconstruction. In: CVPR (2022)
45. Tang, J., Markhasin, L., Wang, B., Thies, J., Nießner, M.: Neural shape deforma-
tion priors. In: NeurIPS (2022)
46. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. In: ICLR (2024)
47. Wang, H., Du, X., Li, J., Yeh, R.A., Shakhnarovich, G.: Score jacobian chaining:
Lifting pretrained 2d diffusion models for 3d generation. In: CVPR (2023)
48. Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer:
High-fidelity and diverse text-to-3d generation with variational score distillation.
In: NeurIPS (2023)
49. Yang,H.,Chen,Y.,Pan,Y.,Yao,T.,Chen,Z.,Mei,T.:3dstyle-diffusion:Pursuing
fine-grained text-driven 3d stylization with 2d diffusion models. In: ACM MM
(2023)
50. Yang, R., Srivastava, P., Mandt, S.: Diffusion probabilistic modeling for video
generation. arXiv preprint arXiv:2203.09481 (2022)
51. Young., J.: Xatlas: Mesh parameterization / uv unwrapping library. https://
github.com/jpcy/xatlas (2022)
52. Yu,X.,Guo,Y.C.,Li,Y.,Liang,D.,Zhang,S.H.,Qi,X.:Text-to-3dwithclassifier
score distillation. In: ICLR (2024)
53. Zhang, Z., Long, F., Pan, Y., Qiu, Z., Yao, T., Cao, Y., Mei, T.: Trip: Temporal
residual learning with image noise prior for image-to-video diffusion models. In:
CVPR (2024)
54. Zhu, J., Zhuang, P., Koyejo, S.: Hifa: High-fidelity text-to-3d generation with ad-
vanced diffusion guidance. In: ICLR (2024)
55. Zhu,R.,Pan,Y.,Li,Y.,Yao,T.,Sun,Z.,Mei,T.,Chen,C.W.:Sd-dit:Unleashing
the power of self-supervised discrimination in diffusion transformer. In: CVPR
(2024)
Appendix
A Application of DreamMesh in 3D Rendering Pipeline
Unlike previous methods that predominantly revolve around implicit 3D scene
representationofdensity-basedgeometrywithundefinedsurfaceboundaries,our
work frames text-to-3D generation based on a completely explicit 3D scene rep-
resentation of triangle meshes. This enables us to generate high-quality meshes
with a clean and well-organized topology featuring neatly arranged vertices,
edges, and faces. Consequently, these generated meshes can be seamlessly inte-
grated into the traditional graphics rendering pipelines, enabling effortless de-
ployment in standard visualization and animation workflows. Herein, we show-
case two types of applications of our DreamMesh in the 3D rendering pipeline.
First, the textured mesh generated by DreamMesh can be easily rigged and
animated in Blender (see Figure 9 (a)). Second, we can use DreamMesh to gen-
erate multiple meshes and put them into a 3D scene to render an animation by
Blender (see Figure 9 (b)). The rigging and animating application videos are
also showcased on our project page (https://dreammesh.github.io).18 H. Yang et al.
(a) Rigging and animating a mesh in Blender
(b) Rendering animations of multiple meshes in Blender
Fig.9: We can directly rig and animate our generated mesh in Blender (a). We can
render an animation by using the textured meshes generated by our DreamMesh in
Blender, such as a giraffe and a camel walking on a grassy field (b).
B More Qualitative Comparisons with Baselines
Here, we provide more qualitative comparisons with baselines on the T3Bench
benchmark [15]. T3Bench has released the textured mesh generated by all base-
line methods [4,25,26,33,47,48] on their GitHub repository. We compare them
withthegeneratedtexturedmeshbyourDreamMesh.Forafaircomparison,we
use Blenderto render images andwireframes from thesemeshesunder the same
lighting environment. Figure 10 showcases some comparison results. It is easy
to see that baseline methods tend to generate distorted geometry and unrealis-
tic textures. In contrast, our DreamMesh can generate higher-quality textured
meshes characterized by clean mesh topology and realistic textures.DreamMesh 19
Prompt: “A gold glittery carnival mask”
Prompt: “A pair of worn in blue jeans”
Prompt: “A sleek stainless steel teapot”
Prompt: “A green cactus in a clay pot”
(a) Dreamfusion (b) LatentNeRF (c) SJC (d) Magic3D (e) Fantasia3D (f) ProlificDreamer(g) DreamMesh (Ours)
Fig.10:Additionalqualitativecomparisonoftextureandwireframeresults(rendering
in Blender) between our DreamMesh and other baseline methods.