A Scalable Algorithm for Active Learning
Youguang Chen Zheyu Wen George Biros
University of Texas at Austin University of Texas at Austin University of Texas at Austin
Austin, USA Austin, USA Austin, USA
youguang@utexas.edu zheyw@utexas.edu gbiros@acm.org
Abstract—FIRAL is a recently proposed deterministic active complexity due to dense computations. Here we propose an
learning algorithm for multiclass classification using logistic approximate algorithm that dramatically accelerates FIRAL.
regression. It was shown to outperform the state-of-the-art in
We dub the new algorithm Approx-FIRAL. A cornerstone
terms of accuracy and robustness and comes with theoretical
in FIRAL is the Fisher information matrix, which is the
performance guarantees. However, its scalability suffers when
dealing with datasets featuring a large number of points n, Hessian of a negative log-likelihood loss function. In Approx-
dimensions d, and classes c, due to its O(c2d2+nc2d) storage FIRAL we exploit the structure of the Hessian and we intro-
and O(c3(nd2+bd3+bn)) computational complexity where b is duce the following: a matrix-free matrix-vector multiplication
thenumberofpointstoselectinactivelearning.Toaddressthese
“matvec”, a preconditioner, randomized trace estimators, and
challenges, we propose an approximate algorithm with storage
requirements reduced to O(n(d+c)+cd2) and a computational amodifiedregretminimizationscheme.Overallthenewcom-
complexity of O(bncd2). Additionally, we present a parallel ponents dramatically improve the complexity of the scheme.
implementation on GPUs. We demonstrate the accuracy and Combined with GPU and distributed memory parallelism
scalabilityofourapproachusingMNIST,CIFAR-10,Caltech101, Approx-FIRAL enables active learning for datasets that were
and ImageNet. The accuracy tests reveal no deterioration in
intractable for FIRAL. Our contributions can be summarized
accuracycomparedtoFIRAL.Wereportstrongandweakscaling
as follows:
testsonupto12GPUs,forthreemillionpointsyntheticdataset.
• We exploit structure, randomized linear algebra, and
Index Terms—Active learning, contrastive learning, GPU ac-
iterative methods to accelerate FIRAL.
celeration, iterative solvers, randomized linear algebra, message
passing interface, performance analysis • Using Python and CuPy [6],and MPI [7], [8] we sup-
port multi-GPU acceleration. Our Python code is open-
I. INTRODUCTION sourced.
• We compare the accuracy of Approx-FIRAL with the
Let X be a set of labeled points and X a set of n
o u
exact FIRAL algorithm as well as several other popular
unlabeledpoints,bothsetssampledfromthesamedistribution.
We denote a labeled sample as a pair (x,y), where x Rd active learning methods; and we test its scalability on
∈ multi-GPU systems.
is a point and y 1,2, ,c is its label, where c is the
∈ { ··· }
number of classes. Our goal of active learning is to select b Wefurthertestthesensitivityofthemethodondifferentinput
points from X to label and use them along with pairs in X parameters like the dataset size and the number of classes.
u o
to train a multiclass logistic regression classifier. Overall Approx-FIRAL is orders of magnitude faster that
Labeling data can be costly, but recent advancements in FIRAL without any noticeable difference in accuracy. While
unsupervised and representation learning [1] enable us to FIRAL is limited to datasets with a few thousands of points
leverage pre-existing feature embeddings combined with shal- and up to 50 classes we demonstrate scalability to ImageNet
low learning techniques like logistic regression to develop 1.3 million points and 1000 classes, as well as synthetic
efficient classification methods [2], [3]. The question is how datasets with several million points.
to select training samples. Active learning addresses this Related work: There is a substantial body of work on
issue by focusing on sample selection [4]. Basic and popular active learning, including approaches such as uncertainty
sample selection methods include random sampling and k- estimation [9], sample diversity [10], [11], [12], Bayesian
meansclustering.Whilethesemethodsarescalableandeasyto inference [13], [14], and others. However, these methods lack
implement,theycanbesuboptimalandexhibithighvariability performance guarantees. FIRAL provides lower and upper
due to their inherent randomness, particularly when the label- bounds of the generalization error for a multinomial logistic
ingbudgetislimited.Weareseekingamethodthatisscalable, regression classifier assuming that the input points follow a
has low variability, and provides accuracy guarantees. sub-Gaussian distribution. It uses convex relaxation (RELAX
Weproposeamethodforsolvingthisproblembasedonthe step), similar to compressed sensing, to first compute weights
FIRAL algorithm (Fisher Information Ratio Active Learning) for each point in X and then uses regret minimization to
u
that appeared in 2023 [5]. FIRAL is an active learning al- select b points (ROUND step). Regarding parallel algorithms
gorithm with theoretical guarantees that outperforms the state andGPUimplementations,therearemanyimplementationsof
of the art in terms of accuracy. However, FIRAL has high random sampling and k-means and related combinations but
SC24,November17-22,2024,Atlanta,Georgia,USA
979-8-3503-5291-7/24/$31.00©2024IEEE
4202
peS
11
]GL.sc[
1v29370.9042:viXranothing related to FIRAL-like algorithms. Table I Summary of notation.
Notation Description
Outline of the paper: We start with the formulation of
d,c dimensionofpoint,numberofclasses
FIRAL in § II. We summarize the RELAX step in § II-B
d(cid:101) dc
and the ROUND step in § II-C. The storage and computa-
b budget:numberofpointstoselectforlabeling
tional complexity of FIRAL are summarized in § II-D. We
n numberofpointsinunlabeledpool
introduce Approx-FIRAL in § III: the Hessian structure and ⊗ matrixKroneckerproduct
the accelerated RELAX step are described in § III-A; and the ⊙ element-wisemultiplicationbetweentwovectors
ROUNDsolveisdescribedin§III-B.TheHPCimplementation vec(·) vectorizationofamatrixbystackingitscolumns
and complexity analysis are described in § III-C. We report Xo,Xu index sets for initial labeled points and unlabeled
results from numerical experiments in § IV: accuracy and points
comparisons with other active learning methods are reported
Hi Fisherinformationmatrixforpointi(Eq.(2))
in § IV-A; and single and multi-GPU performance results are
Ho,Hp,Hz (weighted)sumofHessians(Eq.(3))
reported in § IV-B and § IV-C respectively.
Σz sumofHessiansonselectedpoints(Eq.(7))
f(z) objectivefunction(Eq.(4))
gi gradientofrelaxedobjective(Eq.(6))
II. THEEXACTFIRALALGORITHM
B(·) blockdiagonaloperation(Definition1)
z⋄ solutionofrelaxedproblem(Eq.(5))
A. Formulation (cid:101)· matrixtransformation(Eq.(8))
η learningrateinroundsolver
A summary of the main notation used in the paper can At matrixinROUNDstep(Eq.(10))
be found in Table I. We consider the batch active learning Bt matrixusedinApprox-FIRAL(Eq.(17))
problem with given initial labeled points X and a pool of n
o
unlabeled points X . We denote a labeled sample as a pair B. FIRAL: RELAX step
u
(x,y), where x Rd is a data point, y 1,2, ,c
∈ ∈ { ··· }
is its label, and c is the number of classes. We use a
The first step is to solve a continuous convex optimization
multiclass logistic regression model as our classifier. Given
problem which is formed by relaxing the constraint for z in
x and classifier weights θ Rd×(c−1), the likelihood of a
∈ Eq. (4):
point x having label y is defined by
(cid:0) (cid:1)−1
 exp(θ⊤x) z ⋄ argmin H o+H z H p. (5)
p(y x,θ)= 1+(cid:80) l∈[c−1]y exp(θ l⊤x), y ∈[c −1] (1) ∈ z∈[0,1]n,∥z∥1=b ·
|  1+(cid:80) l∈[c−1 1]exp(θ l⊤x), y =c. The gradient of the ∂fob (zje )ctive w.r.t z i is
We denote the vector of all class probabilities for point x by g i =
∂z
= −H i ·Σ− z1H pΣ− z1, (6)
h(x) Rc−1, with h =p(y =ix). To simplify notation we i
∈ i | where we define
define d(cid:101)=d(c 1). The weights θ are found by minimizing
the negative log− -likelihood: ℓ (θ) ≜ logp(y x,θ). The Σ z =H o+H z. (7)
(x,y)
HessianorFisherinformationmatrixatx− isdefined| byH := FIRAL uses an entropic mirror descent algorithm to solve the
i
∂ ℓ Rd(cid:101)×d(cid:101)and for our classifier is given by relaxed problem.
θθ (x,y)
∈
H =[diag(h ) h h⊤] (x x⊤). (2)
i i − i i ⊗ i i
C. FIRAL: ROUND step
Let H be the summation of Hessians of the initial labeled
o
points, H of the unlabeled points, and H of weighted
p z
unlabeled points with weights z Rn, i.e. AftertheEq.(5)step,FIRALroundsz intoavalidsolution
(cid:88) (cid:88) ∈ (cid:88) ⋄
H o ≜ H i, H p ≜ H i, H z ≜ z iH i. (3) to Eq. (4) via regret minimization. Let us denote Σ ⋄ =H o+
i∈Xo i∈Xu i∈Xu H z⋄ and for any matrix H ∈Rd(cid:101)×d(cid:101), we define H(cid:101) by
Then given a budget of b points to sample (from X ), an H(cid:101) ≜Σ− ⋄1/2HΣ− ⋄1/2 . (8)
u
optimal way would be to minimize the Fisher Information The round solve has b iterations and at each iteration t [b],
∈
Ratio [5]: it selects the point i t s.t.
z∈{0a ,r 1g }nm ,∥i zn ∥1=b(cid:0) H o+H z(cid:1)−1 ·H p ≜f(z). (4) i t ∈ar ig ∈Xm uinTrace[(A t+ η bH(cid:101)o+ηH(cid:101)i)−1], (9)
where “” represents the matrix inner product. Unfortunately, where η > 0 is a hyperparameters (the learning rate), and
thisisan· NP-hardcombinatorialconvexoptimizationproblem. A Rd(cid:101)×d(cid:101)is a symmetric positive definite matrix defined by
t
∈
FIRALproposedanalgorithmtosolvethisproblemwithnear- the Follow-The-Regularized-Leader algorithm:
optimal performance guarantees. The algorithm is composed (cid:40)(cid:112)
d(cid:101)I t=1
of two parts: a RELAX step of continuous convex relaxation A t = d(cid:101) , (10)
optimization followed by a ROUND step to select b points.
ν tI+ηH(cid:101)t−1 t>1Algorithm 1 EXACT-FIRAL be approximated by
1 2:
:
z=(1/n,1/n,···,1/n)∈RnRELAXstep:
g
i
≈−1
s
(cid:88)
v j⊤H i(Σ− z1H pΣ− z1v j). (12)
3: {βt}T t=1:scheduleoflearningrateforrelaxsolve j∈[s]
4: fort=1toT do #T isiterationnumber To calculate the vector Σ−1H Σ−1v in Eq. (12), we can
5: Σz ←Ho+Hz z p z j
6: gi←−Trace(HiΣ− z1HpΣ− z1), ∀i∈[n] solve two linear systems using CG. Note that this term can
87 :: zz ii ←←z (cid:80)i je ∈x z [p i n( ]− zjβtgi) b Te hus sh ,a wre ed of no lr ya nl el ei d∈ toX cau lcuin latg era thd eie vn et ca top rpr oo nx ci em fa ot rio en acf hor mm iru rl oa r.
9: z⋄←bz descent iteration step.
10: ROUNDstep:
11: X←∅ (cid:112),Σ⋄←Ho+Hz⋄ Fast matrix-free matvec. The trace estimator and CG
12: A1← d(cid:101)I d(cid:101),H(cid:101) ←0 solvers require Hessian matvecs. The following Lemma gives
13: fort=1tobdo an exact closed form of the matvec without forming the
14: it←argmin i∈XuTrace[(At+ η bHo+ηHi)−1]
Hessian matrix explicitly.
15: H(cid:101) ←H(cid:101) + 1 bH(cid:101)o+H(cid:101)it
16: VΛV⊤←eigendecompositionofηH(cid:101) Lemma2(Matrix-freeHessianmatvec). Foranygivenvector
17: findνt+1 s.t.(cid:80) j∈[d(cid:101)](νt+1+λj)−2=1#bisection v Rdc, let V Rd×c be the reshaped matrix from v such
11 98 :: XAt ←+1 X← ∪V {( xν it t+ }1I d(cid:101)+Λ)V⊤ t kh -a∈ tht v coec m( pV o) ne= ntv∈ o. fD hen bo yte ht kh .e Hj-th isc go il vu em nn byof EV
q.
b (2y ).v
Tj
h∈
enRd,
i i i
 
where ν t ∈ R is the unique constant s.t. Trace(A− t 2) = 1, (x⊤ i v 1 −x⊤ i Vh i)h1 ix i ∈Rd
and (cid:88)t−1(cid:18) 1 (cid:19) H iv =   . . .   ∈Rd(cid:101).
H(cid:101)t−1 = bH(cid:101)o+H(cid:101)il . (11) (x⊤
i
v
c
−x⊤
i
Vh i)hc ix
i
∈Rd
l=1
Proof.
The FIRAL algorithm is near-optimal [5] in solving the
optimization problem of Eq. (4): H iv =[diag(h i) ⊗(x ix⊤ i )]v −[(h ih⊤ i ) ⊗(x ix⊤ i )]v
=vec(cid:0) x x⊤Vdiag(h )(cid:1) vec(cid:0) x x⊤Vh h⊤(cid:1)
Theorem 1. [Theorem 10 in [5]] Given ϵ (0,1), let η = i i i − i i i i
8(cid:112) d(cid:101)/ϵ, whenever b 32d(cid:101)/ϵ2 +16(cid:112) d(cid:101)/ϵ2,∈ denote z as the =vec(cid:0)(cid:2) (x⊤ i v 1)h1 ix i, ··· ,(x⊤ i v c)hc ix i(cid:3)(cid:1)
solution correspondin≥ g to the points selected by Algorithm 1, −(x⊤
i
Vh i)vec(x ih⊤
i
),
then the algorithm is near-optimal: f(z) (1+ϵ)f ∗, where where the second equality uses a property of the matrix
≤
f ∗ is the optimal value of the f in Eq. (4). Kronecker product.
D. Complexity and scalability of FIRAL
According to Lemma 2, we can compute H v in the
i
Algorithm 1 summarizes FIRAL. Its storage complexity is following steps: ❶ γ V⊤x , ❷ α γ⊤h , ❸ γ
O(c2d2 + nc2d) (Table II), which is prohibitively large for
(γ
i
α i) h i,
andi ❹←
H iv
i vec(γi i←
x
ii
).
i
It is
wi or←
th
large n, d or c. Furthermore, both relax and round solvers − ⊙ ← ⊗
notingthat thestoragerequiredfor thefirstthree steps isonly
involve calculating inverse matrix of size cd cd. Thus, a
× c+1 elements, while the last step requires dc elements for
scalable algorithm of FIRAL is needed.
storing the result of the matvec operation. A comparison of
III. THEAPPROX-FIRALALGORITHM the complexity between our fast matvec algorithm and direct
matvec is provided in Table III.
A. The Hessian structure and a fast RELAX step
With the help of the matrix-free matvec, we can calculate
The new RELAX solver has four components. First, we H pv by
replace the exact trace operator in line 6 of Algorithm 1
(cid:80)
γ1x

with a randomized trace estimator that only requires matvec  i∈X .u i i 
operations. Second, we replace the direct solvers with a H pv =  . .   (13)
matrix-free conjugate gradients iterative method (CG). Third, (cid:80) γcx ,
i∈Xu i i
wedeviseanexactfastmatvecapproximationfortheHessians. where γk = (x⊤v x⊤Vh )hk for k [c]. Based on the
Andfourth,weproposeaneffectivepreconditionerfortheCG i i 1 − i i i ∈
previous analysis, the additional storage required is solely for
scheme. Taken together these components result in a scalable
γ forallunlabeledpointsX ,amountingto4n(c+1)memory
i u
algorithm. We present the pseudo-code for our fast RELAX
cost.WecanusethesimilarcalculationforthematvecofΣ v
z
stepinAlgorithm2andsummarizeitscomplexityinTableII.
within the CG iterations.
We first develop an estimator for the gradient g in Eq. (6)
i Preconditioned CG. To further accelerate the calculation,
that avoids constructing dense d(cid:101)-by-d(cid:101)matrices such as Σ z,
we propose a simple but, as we will see, effective block
H , and Σ−1. The main idea is to use the Hutchinson trace
p z diagonal preconditioner for the CG solves. We first introduce
estimator [15] to approximate the gradient: suppose that we
the block diagonal operation as follows.
usesRademacherrandomvectors v Rd ,theng can
j j∈[s] i
{ ∈ }Table II ComparisonofalgorithmcomplexitybetweenFIRALandApprox-FIRAL.n relax isthenumberofmirrordescentiterationsinrelax
solver, n CG is the number of CG iterations in each mirror descent step of the Approx-FIRAL relax solver.
Exact-FIRAL Approx-FIRAL
Complexity
Relax Round Relax Round
Storage O(c2d2+nc2d) O(c2d2+nc2d) O(n(d+sc)+cd2) O(n(d+c)+cd2)
Computation
O(cid:0)
n
relaxnc3d2(cid:1) O(cid:0) bc3(d3+n)(cid:1) O(cid:0)
n
relaxncd(d+nCGs)(cid:1) O(cid:0) bncd2(cid:1)
Table III Comparison of storage and computational complexity Algorithm 2 FASTRELAXSOLVE
between matrix-free matvec and direct matvec. 1: z=(1/n,1/n,···,1/n)∈Rn
method storage computation
2: {βt}T t=1:scheduleoflearningrateforrelaxsolve
3: fort=1toT do #T isiterationnumber
directMatVec O(d2c2) O(d2c2) 4: V = [v1,v2,···,vs] ∈ Rdc×s: matrix of s Rademacher random
vectors.
fastMatVec O(dc) O(dc) 5: {B k(Σz)−1} k∈[c]←preconditionerforCGsolve
6: W←Σ− z1VbypreconditionedCG
100 CIFAR-10 w wC / /o pG p rer ce oco nn dd iti it oio nn erer 100 ImageNet-1k w w/ /o pC p rer ceG oco nn dd iti it oio nn erer 7 8 9: :
:
W W gi←← ← −H Σ
1
s− zp (cid:80)W 1W j∈[b sy ]vp j⊤re Hco in wd jit ,ioned ∀C iG
∈Xu
10: zi←ziexp(−βtgi)
10−3 10−3 11: zi← (cid:80) j∈z [i n]zj
12: z⋄←bz
0 2 C0 Gstep 40 0 200 CGstep400 600 Algorithm 3 APPROX-FIRAL
Figure 1 TheimpactofpreconditioneronCGiterations.Theexper- 1: z⋄←solutionofRELAXstepfromAlgorithm2
imentalsetupisdetailedin§IV-A.Weshowcasetheconvergenceof 2: DiagonalROUNDstep:
CGintheinitialmirrordescentiteration(i.e.,Line6ofAlgorithm2). 3: X←∅,form{(Σ⋄) k∈Rd×d} k∈[c]
4: (cid:8) (B1)− k1←[(cid:112) d(cid:101)(Σ⋄) k+ η b(Ho) k]−1(cid:9)
k∈[c]
Definition 1 (Block diagonal operation B( ·)). For any matrix 5: (cid:8) (H) k←0(cid:9)
k∈[c]
H Rd(cid:101)×d(cid:101), define (H) Rd(cid:101)×d(cid:101) as the matrix comprising 6: fort=1tobdo
d ma× t∈ rd ixb bl yoc Bk kd (i Hag )o ∈naB Rls d×o df .H∈ ; denote the k-th block diagonal 97 8 :: : (cid:8)i (cid:8)t [( λH← k) ,jkE ]q
d
j← =.( 11 (7 H ←) ) k ei+ ge1 nb v( aH luo e) sk o+ f(h H(cid:101)k it )( k1 (cid:9)− k∈h [ck i ]t)xitx⊤ it(cid:9) k∈[c]
bloT ch ken d, iaf go or ne av le ar sy Hessian matrix H i in Eq. (2), we have its 1 10 1: : fi (cid:8)n (Bd tν +t+ 1)1 − ks 1.t. ←(cid:80) [k ν∈ t+[c 1] ((cid:80) Σj ⋄∈ ) k[d] +(ν ηt+ (H1 )+ kη +λ k η b, (j H)− o2 ) k= ]−1 1(cid:9)
k∈[c]
(H )=[diag(h (1 h ))] (x x⊤), (14)
12: X←X∪{xit}
B i i ⊙ − i ⊗ i i
and its k-th matrix diagonal as that the ROUND step becomes much easier when considering
(H )=hk(1 hk) x x⊤. (15) only the block diagonals of all Hessian matrices. Specifically,
Bk i i − i · i i we assume that each Hessian matrix H retains only its block
We employ (Σ )−1 as the preconditioner for CG to solve i
B z diagonal parts, as expressed in Eq. (14). Consequently, all
the linear system required for gradient estimation in Eq. (12).
matrices with a size of d(cid:101) d(cid:101)in the ROUND step are block
We illustrate the effectiveness of the CG preconditioner for ×
diagonal. This assumption not only reduces storage require-
two datasets in Fig. 1. Using (Σ )−1 as preconditioner
B z ments but also simplifies calculations. Firstly, we introduce
accelerates CG convergence due to several factors. Firstly, it
a Sherman-Morrison-like formula for the low-rank updates
improves the conditioning of the matrix. For instance, in the
for the inverse of a block diagonal matrix in Lemma 3.
CIFAR-10 test, the condition number of Σ is 198, while
z Subsequently, we present a simple yet equivalent objective to
the condition number of (Σ )−1Σ is 72. Additionally,
B z z theoriginalexactROUNDstepinProposition4.Weoutlinethe
the majority of eigenvalues of the preconditioned matrix are
pseudo-codeinAlgorithm3andsummarizethecomplexityof
clustered into small intervals.
the new ROUND step in Table II.
B. The new ROUND step
Lemma3. LetA Rd(cid:101)×d(cid:101)beablockdiagonalpositivedefinite
The difficulty of the ROUND step lies in computing the matrix with c bloc∈ k diagonals of d d matrices, x Rd and
objectivevalueinEq.(9)foreachpointi ∈X u ateachround γ Rc bevectors.IfA+diag(γ) × (xx⊤)ispositiv∈ edefinite,
t
∈
[b]. Even when employing CG with the fast matrix-free the∈ n(cid:0)
A+diag(γ)
(xx⊤)(cid:1)−1 isa⊗
blockdiagonalmatrixwith
matvecintroducedintheprecedingsection,thecomputational ⊗
its k-th block having the following form:
complexity for estimating the objective is (bn n2cds),
which is prohibitively large for large-scale prO oblemC sG . (cid:0) A+diag(γ) (xx⊤)(cid:1)−1 =A−1 γ kA− k1xx⊤A− k1 ,
⊗ k k − 1+γ x⊤A−1x
Motivated by the effectiveness of the preconditioner in the k k
(16)
RELAX, it is natural to consider some approximation. Notice
laudiseRevitaleR laudiseRevitaleRwhereA− k1 istheinverseofk-thdiagonalofA,γ k isthek-th • cupy.linalg.eigvalsh: In the ROUND step, this
component of γ. function is employed to compute the eigenvalues of the
blockdiagonalsofH(cid:101) inabatch-wisemannerinLine9of
Proposition 4. If all Fisher information matrices H only
i Algorithm3.Inourimplementation,weevenlydistribute
preserve the block diagonals of d d matrices, then at each
× the computation of eigenvalues for c block diagonals
iteration of the ROUND step, the objective defined in Eq. (9)
among p GPUs.
is equivalent to the following:
• cupy.linalg.inv: This function is utilized to calcu-
i
argmax(cid:88)c
hk(1 hk)
x⊤
i
(B t)− k1(Σ ⋄)− k1(B t)− k1x
i , late the inverse of block diagonal matrices in Line 5 of
t ∈ i∈Xu k=1 i − i · 1+ηhk i(1 −hk i)x⊤ i (B t)− k1x i Algorithm 2 and Lines 4 and 11 of Algorithm 3.
(17) AsforcommunicationamongGPUs,weoutlinetheprimary
where B =Σ1/2A Σ1/2 + ηH . collective communication operations utilized as follows:
t ⋄ t ⋄ b o • MPI_Allreduce: For RELAX step in Algorithm 2, we
Proof. We denote the objective for point i X u in round needthisoperationforsummationoftheblockdiagonals
∈
problem Eq. (9) by r i, then inLine5.InLines6-8,itisnecessaryforthesummation
η
r
i
=Trace[(A t+ H(cid:101)o+ηH(cid:101)i)−1] oftheresultsfromthematvecoperation.ForROUNDstep
b in Algorithm 3, we use MPI_Allreduce in Line 7 to
=Trace(cid:104) Σ ⋄1/2(cid:0) Σ1 ⋄/2A tΣ1 ⋄/2 + η H o+ηH i(cid:1)−1 Σ1 ⋄/2(cid:105) find the point with the global maximum objective value
b
(cid:124) (cid:123)(cid:122) (cid:125) across all GPUs.
≜Bt • MPI_Allgather:Thisoperationisemployedtocollect
=Trace(cid:104)(cid:0) B +ηH (cid:1)−1 Σ (cid:105) . (18) all eigenvalues in the ROUND step (Line 9 of Algo-
t i ⋄
rithm 3).
Since B and H are both block diagonal, by Lemma 3, k-th
t i (cid:0) (cid:1)−1 • MPI_Bcast: In Lines 6-8 of Algorithm 2, we distribute
block diagonal of B +ηH has the following form:
t i W to each GPU. In Line 11 of Algorithm 3, we utilize
(cid:0) B t+ηH i(cid:1)− k1 =(cid:0) B t(cid:1)− k1
−
ηh 1k i +(1 η−
hk
ih (k i 1) −(B ht
k
i)− k )x1
⊤
ix i (x B⊤ i t( )B
−
k1t x)− k i1 . Inth Ti as bo lepe Ir Vat ,io wn eto st ura mn msm ari it zh eit tha end cox mit pt lo exa il tl yG oP fU ss t.
orage,
(19) computation and communication for our HPC implementation
Substitute Eq. (19) into Eq. (18), we have of Approx-FIRAL. The details are outlined as follows. To
r =Trace[B−1Σ ] estimate the cost of collective communications, we rely on
i t ⋄
the results presented in [17]. We assume that the time used to
η(cid:88)c
hk(1 hk)
x⊤
i
(B t)− k1(Σ ⋄)− k1(B t)− k1x
i , send a message between two processes is t s +mt w, where
− k=1 i − i · 1+ηhk i(1 −hk i)x⊤ i (B t)− k1x i t s is the latency, t w is the transfer time per byte, and m
(20) denotes the number of bytes transferred. Additionally, we
which leads to Eq. (17). denote the computation cost per byte by t c for performing
the reduction operation locally on any process. The costs
C. HPC implementation and complexity analysis associatedwiththethreeMPIoperationsweutilizedareasfol-
lows:❶MPI_Allreduce:employingtherecursivedoubling
Our HPC implementation of Approx-FIRAL, as outlined
algorithm, the time complexity is logp(t +m(t +t )). ❷
in Algorithms 2 and 3, is GPU-based. We employ cupy [6] s w c
MPI_Allgather:utilizingtherecursivedoublingalgorithm,
for computation and mpi4py [8] for communication within
the time complexity is logpt + p−1mt . ❸ MPI_Bcast:
GPUs. To utilize a GPU-aware Message Passing Interface s p w
using the binomial tree algorithm, the time complexity is
(MPI),weutilizeMVAPICH2-GDR[16].Ourimplementation
logp(t +mt ).
employs single-precision floating point for both storage and s w
computation. Let p be the number of GPUs, we start the RELAX step. In terms of storage, the parallel implemen-
parallel implementation by evenly distributing h i and x i of tation of Algorithm 2 requires storing Rademacher random
n points in X u across p GPUs. vectors V (Line 4), the intermediate matrix W (Lines 6-
Regarding computation, we utilize the built-in functions of 8), and the inverses of c block-diagonal matrices (Line
the linear algebra routines available in cupy. We provide a 5). Hence, the total storage for each GPU amounts to
(cid:16) (cid:17)
summary of some of the key functions as follows: n(d+c)+cds+cd2 includingthestorageofx andh
O p i i
• cupy.einsum: In the RELAX step outlined in Algo- for n points.
p
rithm 2, we utilize Einstein summation to construct the For building the preconditioner of CG (Line 5), each GPU
block diagonal matrix as a preconditioner in Line 5. In initially computes the block diagonal matrices (Σ )
z k k∈[c]
Lines 6-8, we employ Einstein summation for the fast with a complexity of (ncd2). The MPI_All{ rB educe} oper-
O p
matrix-free matvec developed in § III-A for matrices Σ ation for aggregation of these matrices across all GPUs incur
z
andH p.Forthe ROUND stepinAlgorithm3,weusethis a communication cost of (cid:0) logp(t s+cd2(t w+t c)(cid:1) . Then
O
function mainly for the objective calculation in Eq. (17) eachGPUcalculatestheinverseoftheblockdiagonalmatrices
(Line 7). as the preconditioner, which has a computational complexityTable IV Storage, computation and communication complexity of parallel implementation of Approx-FIRAL (Algorithm 3). The detailed
derivationsarepresentedin§III-C.n relax representsthenumberofmirrordescentiterationinAlgorithm2,n CG representsthenumberof
CG iterations.
Complexity Storage Computation Communication
RELAXstep O(cid:16) n p(d+c)+cds+cd2(cid:17) O(cid:16) n relaxcd(cid:0)n p(d+nCGs)+d2(cid:1)(cid:17) O(cid:16) n relaxlogp(cid:0) nCGts+cd(nCGs+d)(tw+tc)(cid:1)(cid:17)
ROUNDstep O(cid:16) n p(d+c)+cd2(cid:17) O(cid:16) bcd2(n
p
+d)(cid:17) O(cid:16) blogp(cid:0) ts+(d+c)tw+tc(cid:1)(cid:17)
of (cd3).Insummary,thecomputationalandcommunication IV. NUMERICALEXPERIMENTS
O
time required to construct the preconditioner are as follows:
We test the classification accuracy in § IV-A, single node
Tcomp = (cid:0) cd2(cid:0)n +d(cid:1)(cid:1) , (21) performancein§IV-Bandparallelcomputingperformancein
B(Σz) O p
Tcomm = (cid:0) logp(t +cd2(t +t )(cid:1) . (22) § IV-C on the Lonestar6 A100 nodes in the Texas Advanced
B(Σz) O s w c Computing Center (TACC). Lonestar6 A100 nodes are inter-
WithineachpreconditionedCGiteration(Lines6and8),the connected with IB HDR (200 Gbps) and have three A100
primarytimeconsumptionarisesfromthematveccalculations NVIDIA GPUs per node.
ofΣ Vand (Σ )V.Accordingtothecomplexityoutlinedin
z z
B A. Active learning performance
our fast matvec algorithm in Table III, computation of matvec
(cid:16) (cid:17)
has a complexity of ncds . Subsequently, the summation In our accuracy experiments, we attempt to answer the
O p
ofthesevectorsrequiresanMPI_Allreduceoperationwith following questions regarding Approx-FIRAL. How does the
a communication cost of (logp(t +cds(t +t )). The performance of Approx-FIRAL in active learning tests com-
s w c
O
computationof (Σ )V solelydemandsacomputationalcost pare to Exact-FIRAL? How does Approx-FIRAL compare
z
B
of (cd2s). Let n be the CG iteration number, we have to other active learning methods? Considering we utilize the
CG
O (cid:16) n (cid:17) Hutchinson trace estimator and CG for gradient estimation
Tcomp = n cds , (23)
CG O CG p in RELAX, what impact do variations in the number of
(cid:16) (cid:17) Rademacher random vectors and CG termination criteria have
Tcomm = n logp(t +cds(t +t )) . (24)
CG O CG s w c on the convergence of RELAX?
Regarding other components of the relax solver, Line 7 of
the matvec operation has a complexity similar to one step
Datasets.WedemonstratetheeffectivenessofApprox-FIRAL
of CG. The computation of the gradient g (Line 9) and using the following real-world datasets: MNIST [18], CIFAR-
i
the updating of z (Lines 10-11) necessitate a complexity of 10 [19], Caltech-101 [20] and ImageNet [21]. First we use
(cid:16) (cid:17)
ncds . unsupervisedlearningtoextractfeaturesandthenapplyactive
O p learning to the feature space, that is, we do not use any label
ROUND step. Regarding storage, all matrices utilized in information in our pre-processing. For MNIST, we calculate
Algorithm3areblockdiagonalmatrices,resultinginastorage the normalized Laplacian of the training data and use the
requirement of (cd2). Furthermore, to compute the objec- spectral subspace of the 20 smallest eigenvalues. For CIFAR-
O
tive for each point in Line 7, additional storage of (nc) 10, we use a contrastive learning SimCLR model [2] to
O
is necessary. As a result, the total storage requirement is extract feature; then we compute the normalized Laplacian
(n(c+d)+cd2). and select the subspace of the 20 smallest eigenvalues. For
O
During each iteration of the ROUND step, computing the Caltech-101 and ImageNet-1k, we use state-of-the-art self-
objective function for each point in Line 7 (Eq. (17)) needs supervised learning model DINOv2 [22] to extract features.
a computational complexity of (ncd2). Subsequently, to We additionally select 50 classes randomly from ImageNet-
O p
select the point with the maximum objective, we utilize 1k and construct dataset ImageNet-50.
MPI_Allreduce to gather and compare the maximum ob- We construct 7 datasets for the active learning tests. A
jective across local processes, resulting in a communication summary of the datasets is outlined in Table V. For the initial
cost of (logp(t s+t w+t c)). labeled set X o, we randomly pick two samples per class for
O
To update (H) k k∈[c] (Line 8), the process owning ImageNet-1k and one per class for all other datasets. To form
{ }
i t broadcasts x it and h it to other processes using an the unlabeled pool X u in MNIST, CIFAR-10, ImageNet-50,
MPI_Bcastoperationwithasizeof (c+d).InLine9,we and ImageNet-1k, we evenly select points from each class
firstcomputeeigenvaluesfor c matriceO sforeachprocess,fol- randomly. To simulate a non-i.i.d. scenario, we assemble X
p u
lowed by collecting all eigenvalues using MPI_Allgather. in an imbalanced manner for imb-CIFAR-10, imb-ImageNet-
The computational complexity of this step is (cd3), and 50, and Caltech-101. In imb-CIFAR-10 and Caltech-101, the
O p
the communication cost is (logpt + ct ). As for Line 11, maximum ratio of points between two classes is 10. In
O s p w
computingtheinversematricesrequiresacomputationalcom- imb-ImageNet-50, the maximum ratio of points between two
plexity of (cd3). The total computation and communication classes is eight. We use the points from the whole training
O
complexity for the ROUND step are summarized in Table IV. dataset for evaluation.Table V Summary of datasets for active learning experiments.
Name Type #classes dimension Xo Xu #rounds budget/round #evaluationpoints
MNIST balanced 10 20 10 3,000 3 10 60,000
CIFAR-10 balanced
10 20 10 3,000 3 10 50,000
imb-CIFAR-10 imbalanced
ImageNet-50 balanced
50 50 50 5,000 6 50 64,273
imb-ImageNet-50 imbalanced
Caltech-101 imbalanced 101 100 101 1,715 6 101 8,677
ImageNet-1k balanced 1,000 383 2,000 50,000 5 200 1,281,167
Random K-Means Entropy Exact-FIRAL Approx-FIRAL
(A) MNIST (B) CIFAR-10 (C) imb-CIFAR-10 (D) ImageNet-50 (E) imb-ImageNet-50
100 90 90 95 95
95
90 85 85 90 90
85 80 80 85 85
80 80 80
75 75
75 75 75
70 70 70
70 70
6510 20 30 40 6510 20 30 40 6510 20 30 40 50 100 150 200 250 300 350 50 100 150 200 250 300 350
NumberofLabeledSamples NumberofLabeledSamples NumberofLabeledSamples NumberofLabeledSamples NumberofLabeledSamples
(F) MNIST (G) CIFAR-10 (H) imb-CIFAR-10 (I) ImageNet-50 (J) imb-ImageNet-50
100 90 90 95 95
95
90 85 85 90 90
85 80 80 85 85
80 80 80
75 75
75 75 75
70 70 70
70 70
6510 20 30 40 6510 20 30 40 6510 20 30 40 50 100 150 200 250 300 350 50 100 150 200 250 300 350
NumberofLabeledSamples NumberofLabeledSamples NumberofLabeledSamples NumberofLabeledSamples NumberofLabeledSamples
Figure 2 Classification accuracy for active learning experiments conducted on MNIST, CIFAR-10, imb-CIFAR-10, ImageNet-50, and imb-
ImageNet-50onMNIST,CIFAR-10,imb-CIFAR-10,ImageNet-50andimb-ImageNet-50.Theupperrow((A)-(E))areplotsofpoolaccuracy
on the unlabeled pool X u, the lower row ((F)-(J)) are plots of evaluation accuracy on the evaluation data.
TableVITimecomparisonbetweenExact-FIRALandApprox-FIRAL we conclude the mirror descent iteration when the relative
onasingleA100GPU.Thetimereportedinthetableisinseconds.
changeoftheobjectiveislessthan1.0E 4.Inallofourtests
−
Exact-FIRAL Approx-FIRAL in Table V, this criterion is met within fewer than 100 mirror
descent iterations.
ImageNet-50
The ROUND step requires only one hyperparameter, η.
RELAX 33.6 1.3
We determine the value of η following the same approach
ROUND 34.8 1.1
Caltech-101
as Exact-FIRAL [5]: we execute the ROUND step with dif-
RELAX 172.3 1.9 ferent η values, and then select the one that maximizes
ROUND 945.3 4.4 min k∈[c]λ min(H) k, where H represents the summation of
Hessian of the selected b points (Algorithm 3).
Experimental setup. We compare our proposed Approx- We utilize the logistic regression implementation of
FIRALwithfourmethods:(1)Randomselection,(2)K-means scikit-learn [23] as our classifier, and we keep the
where k = b (b is the budget of the active learning selection parameters fixed during active learning.
per round), (3) Entropy: select top-b points that minimize
(cid:80) We present the classification accuracy results for both
p(y = cx)logp(y = cx), (4) Exact-FIRAL: the original
c | | pool accuracy and evaluation accuracy on MNIST, CIFAR-
implementation of Algorithm 1. For tests involving larger
10, imb-CIFAR-10, ImageNet-50 and imb-ImageNet-50 in
dimension and number of classes, such as Caltech-101 and
Fig. 2. Here, pool accuracy refers the accuracy of classifier
ImageNet-1k, we do not conduct tests on Exact-FIRAL due
on the unlabeled pool points X , while evaluation accuracy
u
to its demanding storage and computational requirements.
represents the accuracy on the evaluation data (the respective
For each of our active learning tests, we use a fixed budget quantities are detailed in Table V). In Fig. 3, we plot the
number for selecting points across 3 to 6 rounds. The details accuracy results obtained from active learning tests conducted
are outlined in Table V. We report the average and standard on Caltech-101 and ImageNet-1k.
deviation for Random and K-means based on 10 trials.
Approx-FIRAL vs. Exact-FIRAL. From the results depicted
Regarding the hyperparameters in RELAX, we fix the num- in Fig. 2, we can observe a very close resemblance in
ber of Rademacher vectors at 10, and terminate the CG iter- the performance of Approx-FIRAL and Exact-FIRAL. The
ation when the relative residual falls below 0.1. Additionally, discrepancies between these two methods are only visible
)%(ycaruccAnoitacfiissalC
)%(ycaruccAnoitacfiissalCin a few instances. For example, in the initial round of the Random K-Means Entropy Approx-FIRAL
CIFAR-10 test (where the number of labeled points is 20 (A) Caltech-101 (B) Caltech-101 in Fig. 2(B) and (G)), Exact-FIRAL exhibits slightly better 95 95
90 90
performance than Approx-FIRAL. However, Approx-FIRAL
85 85
surpasses Exact-FIRAL slightly in the imb-ImageNet-50 test
80 80
(Fig.2(E)and(J)),aswellasinthefinalroundoftheMNIST 75 75
test (Fig. 2(A) and (F)). 70 70
101 202 303 404 505 606 707 101 202 303 404 505 606 707
In Table VI, we illustrate the time comparison between NumberofLabeledSamples NumberofLabeledSamples
Exact-FIRAL and Approx-FIRAL for the initial round of (C) 49 ImageNet-1K (D) 47 ImageNet-1K
ImageNet-50 and Caltech-101 on a single A100 GPU. For
47 45
ImageNet-50, Approx-FIRAL demonstrates approximately 29
45 43
times faster performance than Exact-FIRAL. In the case of
Caltech-101, Approx-FIRAL is about 177 times faster com- 43 41
pared to Exact-FIRAL. 421000 2200 2400 2600 2800 3000 329000 2200 2400 2600 2800 3000
NumberofLabeledSamples NumberofLabeledSamples
Figure 3 Classification accuracy for active learning experiments
Approx-FIRALvs.othermethods.ItisevidentthatApprox-
on Caltech-101 and ImageNet-1k. Both (A) and (B) represent the
FIRAL outperforms other methods in the active learning accuracy on evaluation data for Caltech-101. In (A), the accuracy
test results presented in Figs. 2 and 3. Notably, methods is averaged with each point having the same weight, while in (B),
such as Random, K-means, and Entropy exhibit an obvious the accuracy is averaged with each class having the same weight.
(C) presents the pool accuracy for ImageNet-1k, and (D) presents
decrease in evaluation accuracy from the balanced CIFAR-10
the evaluation accuracy for ImageNet-1k.
test (Fig. 2(G)) to the imbalanced CIFAR-10 test (Fig. 2(H)).
However, FIRAL maintains a consistent performance level
CIFAR-10 ImageNet-50
across both CIFAR-10 and imb-CIFAR-10 tests. Further ob- 15 Exact 34 Exact
servations include: K-means outperforms Random in all the 13 Approx:cgtol=0.5 32 Approx:cgtol=0.5
active learning tests presented in Fig. 2, shows comparable 11 A Ap pp pr ro ox x: :c cg gt to ol l= =0 0. .1
01
23 80 A Ap pp pr ro ox x: :c cg gt to ol l= =0 0. .1
01
accuracy results to Random in Caltech-101 (Fig. 3(A) and 9 Approx:cgtol=0.001 26 Approx:cgtol=0.001
(B)), and exhibits inferior performance compared to Random 7 24
inImageNet-1k(Fig.3(C)and(D)).Additionally,inscenarios 5 0 10 20 30 40 22 0 10 20 30 40
iteration iteration
where the number of labeled samples is limited (such as in CIFAR-10 ImageNet-50
15
testsonMNIST,CIFAR-10,ortheinitialroundsofImageNet- Exact 34 Exact
13 Approx:s=10 32 Approx:s=10
50inFig.2),RandomandK-meansdisplayconsiderablevari- 11 Approx:s=20 30 Approx:s=20
ance,anduncertainty-basedmethodsuchasEntropyperforms Approx:s=100 28 Approx:s=100
9
the poorest. 26
7 24
Parameters in RELAX step. To explore the influence of the 5 0 10 itr20 # 30 40 22 0 10 itera2 t0 ion 30 40
numberofRademacherrandomvectors(s)andthetermination Figure 4 Effect of the number of Rademacher random vectors
tolerance of CG (cg tol) on RELAX, we analyze the initial (top) and CG termination criteria (bottom) on RELAX step (i.e.,
round of the active learning test on CIFAR-10 and ImageNet-
Algorithm 2). “Exact” refers to the precise RELAX solver utilized
inExact-FIRAL,while“Approx”denotesthefastRELAXsolverem-
50. We plot the objective function value Eq. (5) of RELAX
ployedinApprox-FIRAL.Here,sdenotesthenumberofRademacher
against the iteration number of mirror descent in Fig. 4, random vectors, and cg tol signifies the relative residual termination
varying the values of s or cg tol. Notably, we observe that tolerance used in the CG solves.
RELAX does not demonstrate sensitivity to either s or cg tol.
eigenvaluesthatisinvokedatline9ofAlgorithm3,evaluating
B. Single-GPU performance
the objective function, and other related tasks.
We now turn our attention to the HPC performance evalua-
tion.Westartwithdiscussingtheperformanceofouralgorithm Sensitivity to feature size d. As we saw, the
on a single GPU. We study the performance sensitivity to computational complexity of the RELAX solve is
feature size d and number of class c in ImageNet dataset for (cid:0) cd3+ncd2+n CGncsd(cid:1) , where n CG is the number
O
b
t
nho
ee
nth
tt
,hR
ae
soE srL ueA mtiX
c ia
nls gt pe aep naka idn
t
ed
im
alR
e
pO
o
eU
af
kN eD
a pc
ehs rt fe
m
op ra. mjW
o ar
ne
cc
eop mr oo fpv
u
1id
t
9ae
.t
5ie
o
Ts
n
Fti
a
Lm
l
Oa ct
o
Pe
m
Ss pf fo
o
or
-
r
o
p
Tf
r he
eC coG
n
codi nit te
sio
tr ra
n
ut ei co
r
tin o(cid:8)s nB. kT o(h
Σ
fe
− z
(cid:8)m
1
B)a k(cid:9)jo
(k
Σr ∈[c
−
zco
]
1s
a
)t (cid:9)nl
d
ki ∈e Cs
[cG
]in
ts
aot kh
l
eve sinc go cn dΣs 3t zru +Wcti 2o
=
cn
nV
do 2f
.
Float32computationontheGPUA100[24].Thecomputation time. The CG solve involves n evaluations of Σ V.
CG z
of RELAX solve is broken down into four major components: According to Lemma 2, the time complexity of CG is
setting up preconditioner (Σ z)−1, performing the conjugate dominated by 4n CGncsd. Time complexity of ROUND solve
B
gradient(CG),evaluatingthegradient andother relatedtasks. is (cd3+ncd2).Themajorcostliesinline9inAlgorithm3,
O
For ROUND solve, we focus on three components: computing and evaluation of objective function in Eq. (17). We use
f
f
)%(ycaruccAnoitacfiissalC
)%(ycaruccAnoitacfiissalC
f
f
)%(ycaruccAnoitacfiissalC
)%(ycaruccAnoitacfiissalCRELAXsolve,ImageNet-1K,single-node ROUNDsolve,ImageNet-1K,single-node
SetupB(Sz)°1(experiment) CG(experiment) gradient(experiment) other(experiment) computeeigenvalues(experiment) objectivefunction(experiment) other(experiment)
SetupB(Sz)°1(theoretical) CG(theoretical) gradient(theoretical) other(theoretical) computeeigenvalues(theoretical) objectivefunction(theoretical) other(theoretical)
(A) (B) (C) (D)
35 150 60 80
30 125 50
25 100 40 60
20
75 30 40
15
10 50 20 20
5 25 10
383 766 1022 100 200 400 800 1000 383 766 1022 100 200 400 800 1000
d c d c
Figure 5 Wall-clock time dependence of the RELAX and ROUND solves to the number of features d and the number of classes c using
ImageNet-1K. In the run for the d scaling, we fix the number of data points n = 1.0E5 and the number of classes c = 1000. We set the
number of random vectors to s=10. For each value of d, we run one gradient and fix the number of CG iterations to n CG =50; and the
left column represents theoretical time and the right column represents experimental time. In the run to test the algorithmics scalability in
c, we fix n = 1.3E6, d = 383 and vary c as [100,200,400,800,1000] . The remaining parameters of the algorithm are fixed. We report
the results as follows(A) RELAX run for d scaling. (B) RELAX run for c scaling. (C) ROUND solve for d scaling. (D) ROUND solve for c
scaling.
SetupB(Sz)°1(experiment) CG(experiment) gradient(experiment) MPIcommunication(experiment) other(experiment)
SetupB(Sz)°1(theoretical) CG(theoretical) gradient(theoretical) MPIcommunication(theoretical) other(theoretical)
(A) (B) (C) (D)
strongscaling(fullImageNet-1k) weakscaling(ImageNet-1k) strongscaling(extendedCIFAR-10) weakscaling(CIFAR-10)
150 12 10 0.30
125 10 8 0.25
100 8 0.20
6
75 6 0.15
4
50 4 0.10
25 2 2 0.05
1 2 3 6 12 1 2 3 6 12 1 2 3 6 12 1 2 3 6 12
GPUs GPUs GPUs GPUs
Figure 6 Strongandweakscalingofthe RELAX steponCIFAR-10andImageNet-1K.Thedashedlinesindicateidealscalingperformance.
(A) Strong scaling on the full ImageNet-1K dataset (1.3E6 points). (B) Weak scaling on ImageNet-1K (1.0E5 points per rank). (C) Strong
scaling on the extended CIFAR-10 dataset (3.0E6 points). (D) Weak scaling on CIFAR-10 (5.0E4 points per rank).
computeeigenvalues(experiment) objectivefunction(experiment) other(experiment)
computeeigenvalues(theoretical) objectivefunction(theoretical) other(theoretical)
(A) (B) (C) (D)
strongscaling(fullImageNet-1k) weakscaling(ImageNet-1k) strongscaling(extendedCIFAR-10) weakscaling(CIFAR-10)
80 8 3.0 0.10
60 6 2.5 0.08
2.0 0.06
40 4 1.5
0.04
1.0
20 2 0.5 0.02
0.0 0.00
1 2 3 6 12 1 2 3 6 12 1 2 3 6 12 1 2 3 6 12
GPUs GPUs GPUs GPUs
Figure7Strongandweakscalingofthe ROUND steponCIFAR-10andImageNet-1K.Thedashedlinesindicateidealscalingperformance.
(A) Strong scaling on the full ImageNet-1K dataset (1.3E6 points). (B) Weak scaling on ImageNet-1K (1.0E5 points per rank). (C) Strong
scaling on the extended CIFAR-10 dataset (3.0E6 points). (D) Weak scaling on CIFAR-10 (5.0E4 points per rank).
the cupy.linalg.eigvalsh to compute eigenvalues time. Specifically, Fig. 5 (A) presents the results for the
which takes (cd3). We fit the prefactor to 300 doing a RELAX step. We conduct the RELAX step for one mirror
O
few experiments isolated to this function. The evaluation descent iteration while keeping the number of CG iterations
of Eq. (17) has time complexity 3cd3 + 4ncd2. We utilize fixed at n = 50. Increasing d from 383 to 766 leads
CG
features from ImageNet-1K extracted using the pretrained to a 4.72 increase in the wall time of the preconditioner
×
self-supervised ViT models DINOv2 [22] with varying (Σ−1) , while the CG time increases by 1.7 .
{Bk z }k∈[c] ×
dimensions. Specifically, we explore feature dimensions d When d increases from 766 to 1022, the wall time of the
of 383, 766, and 1022. The number of classes is fixed at preconditioner increases by 1.66 , and the CG time increases
×
c = 1000, and we maintain a consistent number of points at by 1.26 .
×
500,000. We set the number of random vectors s=10 in V.
In the ROUND step, we conduct one iteration and showcase
Fig. 5(A)(C) show the sensitivity results for both RELAX and
theresultsinFig.5(C).Increasingdfrom383to766resultsin
ROUND steps. Each d value is represented by two adjacent
a 6.6 increase in eigenvalue computation time. Additionally,
columns. The left column displays the theoretical peak time ×
the evaluation time for the objective function increases by
for each d, while the right column shows the actual test
3.65 . Upon further increasing d from 766 to 1022, the time
×
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnoces
sdnocesrequired for eigenvalue decomposition increases by 2.08 , Fig.6(A)forImageNet-1Kand(C)forCIFAR-10,utilizing12
×
while the evaluation time for the objective function rises by GPUsleadstoaspeedupof10.9 forthepreconditionersetup
×
1.79 . and 11.3 for the CG solve in the case of ImageNet-1K. For
× ×
CIFAR-10, the speedup for the preconditioner is 6.7 , while
Sensitivitytoclassnumberc.Similarly,weexaminethealgo- ×
for CG, it reaches 8 when employing 12 GPUs. As for the
rithm’ssensitivitytothenumberofclasses,c.Asobserved,the
weakscaling, withthenumberof GPUsraisedto 12,thetime
complexityofthe RELAX stepscaleslinearlywiththenumber
increases by less than 10% for ImageNet-1K (Fig. 6(B)), and
of classes, as does the construction of the preconditioner.
within 20% for CIFAR-10 (Fig. 6(D)). The primary increases
Similarly, the two primary components of the ROUND step,
in time are attributed to MPI communications. We present
namely, computing eigenvalues (line 9 in Algorithm 3) and
the ideal speedup as dashed lines in Fig. 6, with negligible
evaluating the objective function (line 7 in Algorithm 3),
variance in performance.
also show linear scale to the number of classes. We conduct
tests on the ImageNet dataset with 1.3 million points and Scalability of ROUND step. In the ROUND step, commu-
a feature dimension of d = 383. The number of classes nication costs are negligible, so we include the time in the
c varies from 100,200,400,800,1000. Fig. 5 (B) illustrates ”other” category in the plots of Fig. 7. In the strong scaling
the results of the RELAX step. When c increases from 100 tests, employing 12 GPUs results in an 11.4
×
speedup for
to 200, the preconditioner cost increases by 2 , and the ImageNet-1K, as shown in Fig. 7(A), and achieves an 11.1
CG time increases by 1.79 . Conversely, for t× he scenario speedup for CIFAR-10, as seen in Fig. 7(C). Regarding wea× k
where c increases from 100× to 1000, the preconditioner time scaling, the time slightly decreases when we increase the
increases by 10.6 , and the CG time increases by 8.3 . numberofGPUs.Thisoccursbecauseweevenlydistributethe
In the ROUND ste× p, we execute one iteration and prese× nt eigenvalue calculations across all GPUs. This effect is more
the results in Fig. 5 (D). As c increases from 100 to 200, pronouncedinthecaseofImageNet-1K,asshowninFig.7(B),
the eigenvalue decomposition time increases by 2.08 , and compared to CIFAR-10 (Fig. 7(D)), since ImageNet-1K has
the time for evaluating the objective function increa× ses by 1000 classes while CIFAR-10 has only 10 classes. Similarly,
1.99 . Conversely, when c increases from 100 to 1000, the we present the ideal speedup as dashed lines in Fig. 7, with
eigen× valuedecompositiontimeincreasesby10 ,andthetime negligible variance in performance.
× Regarding the discrepancy between theoretical and exper-
for evaluating the objective function increases by 10.37 .
× imental performance shown in Figs. 6 and 7, one cause
Overall, the solver exhibits the expected scaling behavior.
is the performance of cupy.einsum, which is impacted
C. Parallel scalability
by memory management and suboptimal kernel performance
We perform strong and weak scaling tests on our paral- for certain input sizes. Additionally, the theoretical analysis
lel implementation of Approx-FIRAL using two datasets. ❶ includes certain constants related to specific kernels that have
ImageNet-1K: the dimension of points is d = 383, and the not been calibrated, such as the prefactors in the eigenvalue
number of classes is c=1000. For the strong scaling test, we solvers, contributing to the gap.
use the entire ImageNet-1K dataset with an unlabeled pool V. CONCLUSIONS
X containing n = 1.3 million points. In the weak scaling
u
test, we allocate 0.1 million points to each GPU. ❷ CIFAR- We presented Approx-FIRAL, a new algorithm that is
orders of magnitude faster than FIRAL. This improvement is
10: the dataset has points with dimension of d = 512 and
achieved by replacing FIRAL’s exact solutions with inexact
number of classes c = 10 . In the strong scaling test, we
iterative methods or block diagonal approximations, using
expand CIFAR-10 by introducing random noise from 50K
∼ randomizedapproximationsformatrixtraces,andapproximat-
to 3 million points. For the weak scaling test, we allocate
ing eigenvalue solves with block diagonal methods. Empirical
50,000 points to each GPU.
results show that these approximations have minimal impact
We present strong and weak scaling results for the RELAX
on sample selection effectiveness, as demonstrated by test
steps in Fig. 6 and the ROUND step in Fig. 7, employing up
accuracy across seven diverse datasets, including those with
to 12 GPUs for both tests. In the RELAX step, we present the
classimbalances.Furthermore,ourmuli-GPUimplementation
time for one mirror descent iteration. For the ROUND step,
allowsefficientscalingtolargedatasetssuchasImageNet.Our
we report the time for selecting one point. For estimating
open-source Python implementation allows interoperability
the theoretical collective communication time costs for MPI
with existing machine learning workflows.
operations,weassumealatencyoft =1.0E 4s,abandwidth
s − Our approach has several limitations. First, we still use
of 1/t =2.0E10 byte/s, and a computation cost per byte of
w
direct solvers in some parts of the code. Specifically, eigen-
t = 1.0E 10 s/byte. Additionally, for computation estima-
c
− value solves in the ROUND step and block factorization
tion, we maintain the use of 19.5TFLOPS peak performance
for our Hessian preconditioner are performed exactly. These
of GPU A100 as in the previous section.
methods are not scalable for certain parameters and could
Scalability of RELAX step. The main computational cost be replaced with sparsely preconditioned iterative solvers to
in the RELAX step stem from the preconditioner setup and enhance both performance and scalability of Approx-FIRAL.
the CG solve. Regarding strong scaling results presented in We aim to incorporate these improvements in future versionsofthealgorithm.Second,wehavenotextendedthetheoretical forNVIDIAGPUcalculations,”31stconferenceonneuralinformation
results of FIRAL to the approximate version. While most processingsystems,vol.151,no.7,2017.
[7] W. Gropp, E. Lusk, and A. Skjellum, Using MPI: portable parallel
of the matrices involved are symmetric positive definite and
programming with the message-passing interface. MIT press, 1999,
our approximations are stable perturbations, deriving precise vol.1.
error bounds requires detailed estimates of the approximation [8] L.DalcinandY.-L.L.Fang,“mpi4py:Statusupdateafter12yearsof
development,”ComputinginScience&Engineering,vol.23,no.4,pp.
error.Third, despite its efficiency, Approx-FIRAL is still more
47–54,2021.
resource-intensivecomparedtoothermethods.Itperformsbest [9] X. Li and Y. Guo, “Adaptive active learning for image classification,”
when the number of classes is relatively small, the feature in2013IEEEConferenceonComputerVisionandPatternRecognition,
2013,pp.859–866.
embeddingsareexcellent,andonlyafewexamplesareneeded
[10] O. Sener and S. Savarese, “Active learning for convolutional neural
for classification. As the number of classes grows, simpler networks:Acore-setapproach,”arXivpreprintarXiv:1708.00489,2017.
methodsmaybemoreappropriate.Fourth,ourtestinghasbeen [11] D.GissinandS.Shalev-Shwartz,“Discriminativeactivelearning,”arXiv
preprintarXiv:1907.06347,2019.
limited to NVIDIA GPUs. Although the code is theoretically
[12] G. Citovsky, G. DeSalvo, C. Gentile, L. Karydas, A. Rajagopalan,
portable—CuPy supports AMD GPUs and it can be adapted A. Rostamizadeh, and S. Kumar, “Batch active learning at scale,” Ad-
for CPUs using NumPy—these alternative implementations vancesinNeuralInformationProcessingSystems,vol.34,pp.11933–
11944,2021.
have not yet been carried out.
[13] Y. Gal, R. Islam, and Z. Ghahramani, “Deep bayesian active learning
A final, more fundamental limitation of the generic FIRAL with image data,” in International conference on machine learning.
PMLR,2017,pp.1183–1192.
approach is its inability to accommodate changes in the
[14] R. Pinsler, J. Gordon, E. Nalisnick, and J. M. Herna´ndez-Lobato,
featureembeddingasnewexamplesareintroduced.Typically, “Bayesian batch active learning as sparse subset approximation,” Ad-
empiricalmethodsaddressthisbyretrainingorfine-tuningthe vancesinneuralinformationprocessingsystems,vol.32,2019.
[15] M. Hutchinson, “A stochastic estimator of the trace of the influence
embedding whenever new labels are obtained. In such cases,
matrix for laplacian smoothing splines,” Communications in Statistics
since the embedding evolves with new data, the data points - Simulation and Computation, vol. 19, no. 2, pp. 433–450, 1990.
themselves change, rendering the FIRAL theory inapplicable. [Online].Available:https://doi.org/10.1080/03610919008812866
[16] D. K. Panda, H. Subramoni, C.-H. Chu, and M. Bayatpour, “The
Active learning with theoretical guarantees for such setups
mvapich project: Transforming research into high-performance mpi
remains an open problem that probably requires an entirely libraryforhpccommunity,”JournalofComputationalScience,vol.52,
different approach. p. 101208, 2021, case Studies in Translational Computer Science.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
ACKNOWLEDGEMENTS S1877750320305093
[17] R.Thakur,R.Rabenseifner,andW.Gropp,“Optimizationofcollective
This material is based upon work supported by NSF award communication operations in mpich,” Int. J. High Perform. Comput.
Appl., vol. 19, no. 1, p. 49–66, feb 2005. [Online]. Available:
OAC 2204226; by the U.S. Department of Energy, Office of
https://doi.org/10.1177/1094342005051521
Science, Office of Advanced Scientific Computing Research, [18] L.Deng,“Themnistdatabaseofhandwrittendigitimagesformachine
AppliedMathematicsprogram,MathematicalMultifacetedIn- learning research,” IEEE Signal Processing Magazine, vol. 29, no. 6,
pp.141–142,2012.
tegratedCapabilityCenters(MMICCS)program,underaward
[19] A. Krizhevsky, V. Nair, and G. Hinton, “Cifar-10 (canadian institute
number DE-SC0023171; by the U.S. Department of Energy, foradvancedresearch).”[Online].Available:http://www.cs.toronto.edu/
NationalNuclearSecurityAdministrationAwardNumberDE- ∼kriz/cifar.html
[20] F.-F. Li, M. Andreeto, M. Ranzato, and P. Perona, “Caltech 101,” Apr
NA0003969;andbytheU.S.NationalInstituteonAgingunder
2022.
awardnumberR21AG074276-01.Anyopinions,findings,and [21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
conclusions or recommendations expressed herein are those Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
of the authors and do not necessarily reflect the views of the
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
DOE,NIH,andNSF.ComputingtimeontheTexasAdvanced 211–252,2015.
Computing Centers Stampede system was provided by an [22] M.Oquab,T.Darcet,T.Moutakanni,H.V.Vo,M.Szafraniec,V.Khali-
dov,P.Fernandez,D.Haziza,F.Massa,A.El-Nouby,R.Howes,P.-Y.
allocation from TACC and the NSF.
Huang,H.Xu,V.Sharma,S.-W.Li,W.Galuba,M.Rabbat,M.Assran,
N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal, P. Labatut,
REFERENCES
A.Joulin,andP.Bojanowski,“Dinov2:Learningrobustvisualfeatures
withoutsupervision,”2023.
[1] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A [23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
review and new perspectives,” IEEE transactions on pattern analysis O.Grisel,M.Blondel,P.Prettenhofer,R.Weiss,V.Dubourg,J.Vander-
andmachineintelligence,vol.35,no.8,pp.1798–1828,2013. plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
[2] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A simple esnay,“Scikit-learn:MachinelearninginPython,”JournalofMachine
framework for contrastive learning of visual representations,” 2020. LearningResearch,vol.12,pp.2825–2830,2011.
[Online].Available:https://arxiv.org/abs/2002.05709 [24] Nvidia, “Nvidia a100 tensor core gpu architecture,”
[3] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/
Q. He, “A comprehensive survey on transfer learning,” Proceedings of nvidia-ampere-architecture-whitepaper.pdf, 2020, accessed: Apr. 2,
theIEEE,vol.109,no.1,pp.43–76,2020. 2024.
[4] P.Ren,Y.Xiao,X.Chang,P.-Y.Huang,Z.Li,B.B.Gupta,X.Chen,and
X.Wang,“Asurveyofdeepactivelearning,”ACMcomputingsurveys
(CSUR),vol.54,no.9,pp.1–40,2021.
[5] Y. Chen and G. Biros, “Firal: An active learning algorithm for multi-
nomiallogisticregression,”AdvancesinNeuralInformationProcessing
Systems,vol.36,2024.
[6] R. Nishino and S. H. C. Loomis, “CuPy: A numpy-compatible library