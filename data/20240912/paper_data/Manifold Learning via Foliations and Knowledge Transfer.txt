Manifold Learning via Foliations
and Knowledge Transfer
EliotTron RitaFioresi
OPTIM-IA,ENAC FaBiT,UniversitàdiBologna
7,av. EdouardBelin,31055Toulouse,France viaS.Donato15,I-40126Bologna,Italy
eliot.tron@enac.fr rita.fioresi@UniBo.it
Abstract
Understandinghowrealdataisdistributedinhighdimensionalspacesisthekeyto
manytasksinmachinelearning. Wewanttoprovideanaturalgeometricstructure
onthespaceofdataemployingadeepReLUneuralnetworktrainedasaclassifier.
Throughthedatainformationmatrix(DIM),avariationoftheFisherinformation
matrix,themodelwilldiscernasingularfoliationstructureonthespaceofdata.
Weshowthatthesingularpointsofsuchfoliationarecontainedinameasurezero
set,andthatalocalregularfoliationexistsalmosteverywhere. Experimentsshow
thatthedataiscorrelatedwithleavesofsuchfoliation. Moreoverweshowthe
potentialofourapproachforknowledgetransferbyanalyzingthespectrumofthe
DIMtomeasuredistancesbetweendatasets.
1 Introduction
Theconceptofmanifoldlearningliesattheveryheartofthedimensionalityreductionquestion,and
itisbasedontheassumptionthatwehaveanaturalRiemannianmanifoldstructureonthespaceof
data[40,19,41,14]. Indeed,withsuchassumption,manygeometricaltoolsbecomereadilyavailable
formachinelearningquestions,especiallyinconnectionwiththeproblemofknowledgetransfer
[9,45],asgeodesics,connections,Riccicurvatureandsimilar[1]. Inparticular,thefastdeveloping
fieldofInformationGeometry[4,27],isnowprovidingwiththetechniquestocorrectlyaddresssuch
questions[26].
However,thepracticalsituation,arisingforexampleinclassifyingbenchmarkdatasetsasMNIST[23],
Fashion-MNIST[49]andsimilar,withCNNs(ConvolutionalNeuralNetworks),isgenerallymore
complicatedanddoesnotallowforsuchsimpledescription,atleastinthecasesmostrelevantfor
concreteapplications,andcallsformoresophisticatedmathematicalmodeling.
Machinelearningmodelscanberoughlydividedintotwocategories:classifierandgenerativemodels.
Inparticular,DeepLearningvisionclassifiermodels,viatheirintrinsichierarchicalstructure,offer
anaturalrepresentationandimplicitorganizationoftheinputdata[31]. Forexample,theauthors
of[18]showhowamultilayerencodernetworkcanbesuccessfullyemployedtotransformhigh
dimensionaldataintolowdimensionalcode,thentoretrieveitviaa“decoder”.
Such organisation strikingly mirrors the one occurring in the human visual perception, as also
observedintheseminalwork[41],andlateraninspirationforthespectacularsuccessfulemployment
ofCNNsforsupervisedclassificationtasks[24].
Inthispaper,wewanttoprovidethedataspaceofagivendatasetwithanaturalgeometricalstructure,
andthenemploysuchstructuretoextractkeyinformation. WewillemployasuitablytrainedCNN
modeltodiscernafoliationstructureinthedataspaceandshowexperimentallyhowthedatasetour
modelhasbeentrainedwithisstronglycorrelatedwithitsleaves,whicharesubmanifoldsofthedata
space. Themathematicalideaoffoliationisquiteold(see[33,13]andrefstherein);however,its
Preprint.Underreview.
4202
peS
11
]GL.sc[
1v21470.9042:viXraapplicationsincontroltheory(see[3]andrefstherein)viasub-Riemanniangeometryandmachine
learninghaveonlyrecentlybecomeincreasinglyimportant[42,15].
The foliation structure on the data space, discerned by our model, however, is non-standard and
presentssingularpoints. Thesearepointsadmittinganeighbourhoodwheretherankofthedistribu-
tion,tangentateachpointtoaleafofthefoliation,changes. Moreover,inthepresenceoftypicalnon
linearitiesofthenetwork,asReLU,therearealsononsmoothpoints,thushinderinginsuchpoints
thesmoothmanifoldstructureitselfoftheleaf. However,weprovethatbothsingularandsmooth
pointsareameasurezerosetinthedataspace,sothatthefoliationisalmosteverywhereregular
anditsdistributionwelldefined. Asitturnsoutinourexperiments,thesamplesbelongingtothe
datasetwetrainourmodelwithareaveragelyclosetothesetofsingularpoints. Itforcesustomodel
thedataspacewithasingularfoliationthatwecalllearningfoliationinanalogywiththelearning
manifold. Applicationsofsingularfoliationswereintroducedinconnectionwithcontroltheory[38];
theirstudyiscurrentlyanactiveareaofinvestigation[22]. Asweshallshowinourexperiments,
togetherwiththeirdistributions,singularfoliationsprovidewithaneffectivetooltosingleoutthe
samplesbelongingtothedatasetsthemodelwastrainedwithandatthesametimediscernanotionof
distancebetweendifferentdatasetsbelongingtothesamedataspace.
Our paper is organized as follows. In Sec. 2, we recap the previous literature, closely related to
ourwork. InSec.3, westartbyrecallingsomeknownfactsaboutInformationGeometryin3.1.
Then,inSec.3.2weintroducethedatainformationmatrix(DIM),definedviaagivenmodel,and
twodistributionsDandD⊥ naturallyassociatedwithit,togetherwithsomeillustrativeexamples
elucidatingtheassociatedfoliationsandtheirleaves. InSec.3.3weintroducesingulardistributions
andfoliationsandthenweproveourmaintheoreticalresultsexpressedinLemma3.4andTheorem3.6.
Lemma3.4studiesthesingularitiesofthedistributionD. Theorem3.6establishesthatthesingular
pointsforDareameasurezerosetinthedataspace. Henceourfoliation,thoughsingular,acquires
geometricsignificance. FinallyinSec.4weelucidateourmainresultswithexperiments. Moreover,
weshowhowthefoliationstructureanditssingularitiescanbeexploitedtodeterminewhichdataset
themodelwastrainedon,anditsdistancefromsimilardatasetsinthesamedataspace. Wealsomake
some"proofofconcept"considerationsregardingknowledgetransfertoshowthepotentialofthe
mathematicalsingularlocalfoliationanddistributionstructuresindataspace.
2 RelatedWorks
TheuseofDeepNeuralNetworksastoolsforbothmanifoldlearningandknowledgetransferhas
beenextensivelyinvestigated. Thequestionoffindingalowdimensionalmanifoldstructure(latent
manifold)intohighdimensionaldatasetspacesstartswithPCAandsimilarmethodstoreachthe
moresophisticatedtechniquesasin[14,26], (seealso[29]foradescriptionofthemostpopular
techniquesindimensionalityreductionand[10]foracompletebibliographyortheoriginsofthe
subject). Suchunderstandingwasappliedtowardstheknowledgetransferquestionsin[11,7]and
morerecently[12]. Thoughthisisnotourapproach,itisimportanttomentionthatoneimportant
techniqueofknowledgetransferisviasharedparametersasin[28]. Theideaofusingtechniquesof
InformationGeometryformachinelearningstartedwith[4]. Morerecently,itwasusedformanifold
learningin[26](seealsorefstherein). Employingfoliationstoreducedimensionalityisnotperse
novel. In[15],theauthorsintroducesomefoliationstructureinthedataspaceofaneuralnetwork,
withtheaimofapproachingdimensionalityreduction. In[42],orthogonalfoliationsinthedataspace
areusedtocreateadversarialattacks,andprovidethedataspacewithcurvatureandotherRiemannian
notionstoanalysesuchattacks. In[39]invariantfoliationsareemployedtoproduceareducedorder
modelforautoencoders. Also,inconsideringsingularpointsforourfoliation,weareledtostudy
thesingularitiesofaneuralnetwork. Thiswasinvestigated, forthedifferentpurposeofnetwork
expressibilityin[16].
3 Methodology
3.1 InformationGeometry
The statistical simplex consists of all probability distributions in the form p(y|x,w) =
(p (y|x,w),...,p (y|x,w)), where x is a data point, belonging to a certain dataset D ⊂ Rd,
1 c
divided into c classes, while w ∈ RN are the learning parameters [4, 20, 27]. We define the
2informationlossas: I(x,w)=−log(p(y|x,w))andtheFisherinformationmatrix(FIM)[32]as
F(x,w)=E [∇ logp(y|x,w)·(∇ logp(y|x,w))T] (1)
y∼p w w
Inanalogyto(1)wedefinetheDatainformationmatrix(DIM):
D(x,w)=E [∇ logp(y|x,w)·(∇ logp(y|x,w))T] (2)
y∼p x x
Asnotedin[26],somedirectionsintheparameterordataspace,maybemorerelevantthanothersin
manifoldlearningtheory. Wehavethefollowingresult[15],obtainedwithadirectcalculation.
Proposition3.1. TheFisherinformationmatrixF(x,w)andthedatainformationmatrixD(x,w)
arepositivesemidefinitesymmetricmatrices. Moreover:
kerF(x,w)=(span {∇ logp (y|x,w)})⊥,
i=1,...,c w i
(3)
kerD(x,w)=(span {∇ logp (y|x,w)})⊥.
i=1,...,c x i
wheretheorthogonalistakenwithrespecttotheeuclideanproduct. InparticularrankF(x,w)<c
andrankD(x,w)<c.
NoticethatF(x,w)isaN×N matrix,whileD(x,w)isad×dmatrix,whereintypicalapplications
(e.g. classificationtasks)N,d >> c. Hence, bothF(x,w)andD(x,w)are, ingeneral, singular
matrices,withrankstypicallylowwithrespecttotheirsizes,henceneitherF(x,w)norD(x,w)can
providemeaningfulmetricsrespectivelyonparameteranddataspaces. Weshallnowfocusonthe
dataspace.
Theresult(3)suggeststoconsiderthedistributionD:
Rd ∋x(cid:55)→D :=span {∇ logp (y|x,w)}⊂T Rd (4)
x i=1,...,c x i x
whereweassumethelearningparametersw ∈RN tobefixedasxvaries. Ingeneraladistribution
onamanifoldM assignstoeachpointasubspaceofthetangentspacetoM atthatpoint(see[43,44]
formoredetails),inourcaseM =Rd.
3.2 DistributionsandFoliations
WheneverwehaveadistributiononamanifoldM,itisnaturaltoaskwhetheritisintegrable. A
distributionConM isintegrableifforeveryx∈M,thereexistsaconnectedimmersedsubmanifold
N ofM,withT N = C forally ∈ N. Inotherwords,thedistributiondefinesateachpointthe
y y
tangent space to a submanifold. Whenever a distribution is integrable, we have a foliation, that
is, M becomes a disjoint union of embedded submanifolds called the leaves of the foliation. If
dimC =dimC forallx,y ∈M wesaythatthefoliation,orthecorrespondingdistribution,has
x y
constantrank. Fig.1illustratesaconstantrankfoliationanditsorthogonalcomplementfoliation,
with respect to the euclidean metric in M = R3 (the ambient space). Under suitable regularity
hypothesis,Frobeniustheorem[43,44]provideswithacharacterizationofintegrabledistributions.
Theorem3.2. (FrobeniusTheorem). AsmoothconstantrankdistributionC onarealmanifoldM
isintegrableifandonlyifitisinvolutive,thatisforallvectorfieldsX,Y ∈C,[X,Y]∈C. 1
ToelucidatethisresultinFig.4welookattwoexamplesoffoliationsof1-dimensionaldistributions,
obtained from the distribution D in (4) of a neural network trained on the Xor function, with
non linearities GeLU and ReLU2. Notice that since in both cases D is 1-dimensional, then it is
automaticallyintegrable(bracketofvectorfieldsbeingzero). Wehavethefollowingresult;theproof
isasimplecalculationbasedontheFrobeniusTheorem[15].
Proposition3.3. Letthenotationbeasabove. Foranempiricalprobabilitypdefinedviaadeep
ReLUneuralnetworkthedistributionDinthedataspace:
x(cid:55)→D =span {∇ logp (y|x,w)}
x i=1,...,c x i
islocallyintegrableatsmoothpoints.
3L
D
Rd
L
D⊥
Figure1:Foliations:L andL denotethesetoftheleavesindistributionsDandD⊥respectively.
D D⊥
Figure2: MovingaccordingtoD.
Figure3: MovingaccordingtoD⊥.
WecallthefoliationresultingfromProp. 3.3thelearningfoliation. Itssignificanceisclarifiedbythe
followingexamples.
AsanillustrationofthedistributionsDandD⊥(orthogonalcomputedwithrespecttotheeuclidean
metricinM =Rd),inMNIST,wepresentFig.2andFig.3. Wenoticethat,whilemovinginDfrom
onedatapoint,themodelpredictsameaningfullabel,whilemovinginD⊥,themodelmaintains
highconfidenceonthelabel,thoughnotinlinewiththeimage. BothFig.2andFig.3wereobtained
byprojectingthesamedirectiononDandD⊥ateachstep. Noticethatthephenomenaweobserve
aboveareduetothefactthattheneuralnetworkoutputprobabilitiesareinvariantbymovinginthe
kerneloftheDIM,thatisthedistributionD⊥.
ForamodelwithGeLUnonlinearityhowever,onecanseeexperimentallythatwedonothavethe
involutivitypropertyanymoreforthedistributionD(seeTable2). Hence,thereisnofoliationwhose
leavesfillthedataspace,naturallyassociatedtoitviaFrobeniusTheorem. Toseethismoreindetail,
1IfX,Y arevectorfieldsonamanifold,wedefinetheirLiebracketas[X,Y]:=XY −YX.
2Welookonlyatsmoothpoints,whichformanopenset.
4wedefinethespaceVD generatedbyD andtheLiebracketsoftheirgenerators:
x x
VD =span{∇ logp (y|x,w),[∇ logp (y|x,w),∇ logp (y|x,w)],i,j,k =1,...,c}
x x i x j x k (5)
InTable2wereportaveragesofthedimensionsofthespacesD ,VD forasampleof100points
x x
Table1: InvolutivityofthedistributionD
Nonlinearity dimD dimVD
x x
ReLU 9 9
GeLU 9 44.84
Sigmoid 9 45
x∈Rd. Thenoninvolutivityofthedistributionisdeducedfromthefactthedimensionincreases
whenwetakethespacespannedbythedistributionandthebracketsofitsgenerators. Aswecan
see,whilefortheReLUnonlinearityDisinvolutiveandwecandefinethelearningfoliation,the
brackets of vector fields generating D do not lie in D for the GeLU and sigmoid non linearities.
Consequently, there is no foliation and the sub Riemannian formalism appears more suitable to
describethegeometryinthiscase. Weshallnotaddressthequestionhere.
3.3 SingularFoliations
AfoliationonamanifoldM isapartitionofM intoconnectedimmersedsubmanifolds,calledleaves.
Afoliationisregulariftheleaveshavethesamedimension,singularotherwise. Noticethatthemap
x(cid:55)→dim(L ),whichassociatestox∈M thedimensionofitsleafL ,islowersemi-continuous,
x x
thatis,thedimensionsoftheleavesinaneighbourhoodofxaregreaterthanorequaltodim(L )
x
[22]. Wheneverwehaveanequality,wesaythatxisregular,otherwisewecallxsingular. Itis
importanttoremarkthat,adheringtotheliterature[22],theterminologysingularpointhererefersto
apointthathasaneighbourhoodwheretheleaveshavenonconstantdimension. Wecanassociatea
distributiontoafoliationbyassociatingtoeachpoint,thetangentspacetotheleafatthatpoint. Such
distributionhasconstantrankifandonlyifthefoliationisregular. FrobeniusTheorem3.2applies
onlytothecaseofconstantrankdistributions,however,thereareresultsextendingpartofFrobenius
theoremtononconstantrankdistributions. In[17]and[30],theauthorsgivesufficientconditionsfor
integrabilityinthismoregeneralsetting(see[22]forcorrectattributionsonstatementsandproofs).
Inthepracticalapplicationsweareinterestedin,foliationsmayalsohavenonsmoothpoints,thatis
pointswheretheleafthroughthepoint,belongingtothefoliation,isnotsmooth. Forexample,Fig.4
showsaXornetworkwithReLUnonlinearitydisplayingbothsingularandnonsmoothpoints.
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(a)GeLU. (b)ReLU.
Figure4: ThelearningfoliationdefinedbythedistributionD(4)foraXornetwork.
The singular point of each picture is in the center, while non smooth points occur only for Xor
withReLUnonlinearity. Aspreviouslystated,thisisadegeneratecase,wheretheinvolutivityand
5integrabilityofDisgrantedautomaticallybylowdimensionality,thoughfor(b)inFig.4,wecannot
defineDatnonsmoothpoints.
Remark3.1. NoticethatFrobeniusTheorem3.2anditsnonconstantrankcounterparts,applyonly
inthesmoothnesshypothesis,whileforapplications,i.e. thecaseofReLUnetworks,itisnecessary
toexaminealsononsmoothfoliations. Weplantoexplorethenonsmoothsettingmoregenerallyin
aforthcomingpaper.
WenowwanttoinvestigatefurtherthesingularpointsofthedistributionD.
Let N(x) = p(y|x,w) = (p (y|x,w),...,p (y|x,w)) denote the output of the neural network
1 c
classifierandJ (x)=(∇ p (y|x,w))itsJacobianmatrix.So∇ p (y|x,w)isthei-throw(column)
N x i x i
ofJ (x). Onecansee,bytheverydefinitionofD(4),thatrankD =rankJ . Weassumewtobe
N N
constant,thatiswefixourmodel. LetP =diagonal(p (y|x,w),...,p (y|x,w)).
1 c
FromnowonweassumeN(x)=Softmax◦S(x)whereS(x)representsthescore. Thenwehave:
J (x)=(P −ptp)J (x) (6)
N S
wherep=(p (y|x,w),...,p (y|x,w))andJ (x)istheJacobianofthescore.
1 c S
TostudythedropofrankforDorequivalentlyforJ ,letusfirstlookatthekernelof(P −ptp).
N
Lemma3.4. LetE denotethevectorwiththei-thcoordinateequaltoone,andtheothersequalto
i
zero.
ker(P −ptp)=span{(1,...,1)}+span{E , ∀isuchthatp =0} (7)
i i
Proof. Letu∈Rc. Thenu∈ker(P−ptp)ifandonlyifp u −p (cid:80) p u =0. Thisisequivalent
i i i k k k
to:
(cid:88) (cid:88)
p =0 or u − p u =0 ⇐⇒ p =0 or p (u −u )=0
i i k k i k i k
k k
The inclusion ⊇ is thus straightforward. To get the other inclusion, let i denote the argmax of
0
(cid:80)
u. Then, p (u −u ) is a sum of non negative terms; hence to be equal to zero, must be
k k i0 k
p (u −u )=0forallk. Therefore,u =u forallksuchthatp ̸=0. Thisisenoughtoprove
k i0 k k i0 k
thedirectinclusion⊆.
Wehavethefollowingimportantobservation,thatweshallexploremoreindetailinourexperiments.
Observation3.2. Lemma3.4tellsusthattherankofthedistributionDorequivalentlyofJ (x)=
N
(P −ptp)J (x)isloweratpointsinthedataspacewheretheprobabilitydistributionhashigher
S
numberofp ̸=0. Clearlythepointsinthedataset,onwhichourmodelistrained,arepreciselythe
i
pointswheretheempiricalprobabilitypismostlyresemblingamassprobabilitydistribution. Hence
atsuchpoints,wewillobserveempiricallyanaveragedropofthevaluesoftheeigenvaluesofthe
DIM(whosecolumnsgenerateD),comparedtorandompointsinthedataspace,asourexperiments
confirminSec. 4. Asweshallsee,thispropertycharacterizesthepointsinthedatasetthemodelwas
trainedwith.
Inourhypotheses,sincetheprobabilityvectorpisgivenbyaSoftmaxfunction,itcannothavenull
coordinates. Therefore,Lemma3.4statesthatdimker(P −ptp)=1andthatthekernelofP −ptp
doesnotdependontheinputx. Thus,thedropsinrankofDdoesnotdependonker(P −ptp)and
canonlybecausedbyJ (x).
S
NowweassumethatthescoreS isacompositionoflinearlayersandactivationfunctionsasfollows:
S(x)=L ◦σ◦···◦σ◦L (8)
Wℓ W1
whereσistheReLUnonlinearity,L arelinearlayers(includingbias)andℓisthetotalnumberof
Wi
linearlayers. Wedenotetheoutputofthek-thlayer:
f (x)=L ◦σ◦···◦σ◦L (x)
k Wk W1
Letusdefine,forasubsetU inRd:
Z ={x∈U suchthat∃i, x =0} (9)
U i
6Lemma3.5. LetO bethesetofpointsinM = Rd,admittinganeighbourhoodwherethescore
functionS isnonconstant. Then,thesetofsingularpointsofJ ,theJacobianofS,isasubsetof:
S
ℓ (cid:91)−1 ℓ (cid:91)−1dim (cid:91)fk(M)
f−1(Z )∩O = {x|f (x) =0}∩O (10)
k fk(M) k i
k=1 k=1 i=1
Thissetisthefiniteunionofclosednullspaces,thusofzeroLebesguemeasure.
Proof. AshortcalculationbasedontheexpressionofS (8)gives:
J (x) = W J (f (x))W J (f (x))...J (f (x))W (11)
S ℓ σ ℓ−1 ℓ−1 σ ℓ−2 σ 1 1
Noticethat:
(cid:26)
δ ifx >0
(J (x)) = i,j i (12)
RELU i,j 0 ifx <0
i
Hence,thesetZ representsthesingularpointsofJ onthedomainU.
U ReLU
Ifx ∈ {x|f (x) =0}∩O,then,evenifitmeansmakinginfinitesimalchangestothenetwork
0 k i
weights,thereexistsaneighborhoodofx suchthat(f ) islinearonit. Besides,sincex ∈ O,
0 k i 0
again,evenifitmeansmakinginfinitesimalchangestothenetworkweights,then(f ) willnotbe
k i
trivial.Thus{x|f (x) =0}∩Ois containedinanhyperplanewithdimension<d=dimM.
k i
Nowweseethatsingularpointsoccurona(Lebesgue)measurezeroset.
Theorem3.6. Letthenotationbeasabove. ConsiderthedistributionD:
Rd ∋x(cid:55)→D =span{∇ p (y|x,w), i=1,...c} (13)
x x i
wherepisanempiricalprobabilitygivenbySofmaxandascorefunctionS consistingofasequence
oflinearlayersandReLUactivations. Then,itssingularpoints(i.e. pointswhereDchangesitsrank)
areaclosednullsubsetofRd containedintheunionofℓhypersurfaces,whereℓisthenumberof
layers.
Proof. ThesingularpointsofDcoincide,byLemma3.4withthepointswhereJ theJacobianofS
S
asin(8changesitsrank. ByLemma3.5thisoccursintheunionofℓhypersurfaces.
We conclude by observing that, as a consequence of the proofs of Lemma 3.5 and Thm 3.6, the
singular points of the distribution D are contained in the non smooth points and such points are
containedinameasurezerosubsetofthedataspace. Hence,ifwerestrictourselvestotheopenset
complementingsuchmeasurezeroset,wecanapplyFrobeniusThmtoDtogetthelearningfoliation.
4 Experiments
We perform our experiments on the following datasets: MNIST[23], Fashion-MNIST[49],
KMNIST[50]andEMNIST[51],lettersonly,thatwedenotewithLetters. Wealsocreateadataset
thatwecallCIFARMNIST:itistheCIFAR10dataset[52]croppedandtransformedtobe28×28
gray-scalepictures.
OurneuralnetworkissimilartoLeNet,withtwoconvolutionallayers,followedbyaMaxpooland
twolinearlayerswithReLUactivationfunctions,seeFig.5. Thisisslightlymoregeneralthanour
hypothesesinSec. 3.3. ThemodelisthentrainedonMNIST,reaching98%ofaccuracy.
InFig.6wecomputetheJacobianofthenetworkandwemeasureitsrankbylookingatitssingular
valuesfor100samplepointsinthedataspaceR784oftheMNISTdataset(wenormalizethesingular
values by dividing them by the largest one). The statistical significance of these experiments is
detailedinFig.7. WeseeclearlythatonpointsinthedatasetthesingularvaluesoftheJacobianJ ,
N
aresmaller,asweremarkedinObs. 3.2,asaconsequenceofourLemma3.4.
WereportDIM’seigenvalues(logarithmicscale)fordifferentdatasetsinFig. 7. Theverticalsegment
foreacheigenvalueandeachdatasetrepresentsthevaluesfor80%ofthesamples,whilethecolored
arearepresentsthevaluesfallinginbetweenfirstandthirdquartile. Thehorizontallinerepresentsthe
7Figure5: ThestructureofourCNN-picturecreatedwith[25].
Singular values of ( pi) Singular values of ( pi)
100
101
103 103
105
106
107
109 109
1011
1012
1013
1015 1015
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9
(a)SingularvaluesofJ at100datapointsinMNIST. (b)SingularvaluesofJ at100randompoints.
N N
Figure6: SingularvaluesoftheJacobianJ ofthenetwork,normalizedbythelargestone.
N
0
5
10
15
MNIST
KMNIST
Letters
FashionMNIST
20
CIFARMNIST
Noise
(n) (n 1) (n 2) (n 3) (n 4) (n 5) (n 6) (n 7) (n 8) (n 9)
DIM's eigenvalues in decreasing order
Figure7: DIMeigenvaluessortedbydecreasingorderevaluatedon250pointsforeachdataset.
medianandthetrianglerepresentsthemean. ThepointsinMNIST,thetrainingdataset,areclearly
identifiablebylookingatthecoloredarea.
Weperformaproofofconceptknowledgetransferbyretrainingthelastlinearlayerofourmodel
ondifferentdatasets. WereportinTable2themedianofhighestandlowestDIMeigenvaluesin
8
eulavnegie
eht
fo
01golKMNIST 0.8
Letters
6 FashionMNIST
CIFARMNIST 0.7
5
0.6
4
0.5
3
0.4
2
0.3 KMNIST
Letters
1 0.2 FashionMNIST
CIFARMNIST
0 5 10 15 20 25 30 0 5 10 15 20 25 30
Epochs Epochs
(a)Validationloss. (b)Validationaccuracy
Figure8: LossandaccuracyaftertransferlearningstartingfromtheweightsofaReLUnetwork
trainedonMNIST(98%ofaccuracy)andretrainingonlythelastlinearlayer.
logarithmicscale,theirdifference∆andvalidationaccuraciesafterretrainingofourCNN.Weseea
correspondencebetweenthemedianofthelowesteigenvalueandthevalidationaccuracy,suggesting
toexploreinfutureworkstherelationbetweenDIM,foliationsandknowledgetransfer.
Table2: ParametersforKnowledgeTransfer(logarithmicscale)
Dataset Highestevalue Lowestevalue ∆ DIMTrace Val. Acc.
MNIST -1.78 -8.58 6.70 -1.52 98%
KMNIST 0.49 -7.75 7.76 0.37 75%
Letters 0.11 -7.99 7.82 0.48 80%
Fashion-MNIST 0.14 -8.08 7.76 0.12 81%
CIFARMNIST 0.41 -6.90 6.75 0.27 33%
Noise 0.24 -5.36 5.49 0.27 NA
5 Conclusions
Weproposetocomplementthenotionoflearningmanifoldswiththemoregeneraloneoflearning
foliations. The(integrable)distributionD,definedviathedatainformationmatrix(DIM),agen-
eralizationoftheFisherinformationmatrixtothespaceofdata,indeedallowsforthepartitionof
thedataspaceaccordingtotheleavesofsuchafoliationviatheFrobeniustheorem. Examplesand
experimentsshowacorrelationofdatapointswiththeleavesofthefoliation:movingaccordingtothe
distributionD,i.e. alongaleaf,themodelgivesameaningfullabel,whilemovingintheorthogonal
directionsleadstogreaterandgreaterclassificationerrors. Thelearningfoliationishoweverboth
singular(dropinrank)andnonsmooth. Weprovethatsingularpointsarecontainedintoasetof
measurezero,hencemakingthelearningfoliationsignificantinthedataspace. Weshowthatpoints
inthedatasetthemodelwastrainedwithhavelowerDIMeigenvalues,sothatthedistributionD
allowssuccessfullytodeterminewhetherasampleofpointsbelongsornottothedatasetusedfor
training. Wemakesuchexplicitcomparisonwithsimilardatasets(i.e. MNISTversusFashionMNIST,
KMNISTetc). Then,weusethelowesteigenvalueoftheDIMtomeasurethedistancebetweendata
sets. Wetestourproposeddistancebyretrainingourmodelondatasetsbelongingtothesamedata
spacesandcheckingthevalidationaccuracy. Ourresultsarenotquantitativelyconclusiveinthis
regard,butshowagreatpromiseasafirststeptogobeyondthemanifoldhypothesisandexploiting
thetheoryofsingularfoliationstoperformdimensionalityreductionandknowledgetransfer.
References
[1] A. Ache. M. W. Warren Ricci curvature and the manifold learning problem. Advances in
MathematicsVolume342,21January2019,Pages14-66.
9
)noitadilav(
ssoL
)noitadilav(
ycaruccA[2] A.Achilleetal.“Task2Vec: TaskEmbeddingforMeta-Learning”.In: ProceedingsoftheIEEE
InternationalConferenceonComputerVision.2019,pp.6430–6439.
[3] A.A.Agrachev,Yu.L.Sachkov,Controltheoryfromthegeometricviewpoint.SpringerVerlag,
2004.
[4] S.-I.Amari.“NaturalGradientWorksEfficientlyinLearning”.In: NeuralComput.10.2(Feb.
1998),pp.251–276.
[5] MikhailBelkin,ParthaNiyogi,andVikasSindhwani.Manifoldregularization: Ageometric
framework for learning from labeled and unlabeled examples. Journal of machine learning
research,7(11),2006.
[6] Belkin,M.andNiyogi,P.Laplacianeigenmapsfordimensional-ityreductionanddatarepre-
sentation.NeuralComput.,15(6): 1373–1396,2003.
[7] YoshuaBengio.Deeplearningofrepresentationsforunsupervisedandtransferlearning.In
ProceedingsofICMLworkshoponunsupervisedandtransferlearning,pages17–36.JMLR
WorkshopandConferenceProceedings,2012.
[8] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review
and new perspectives. IEEE transactions on pattern analysis and machine intelligence,
35(8):1798–1828,2013.
[9] StevoBozinovski,ReminderoftheFirstPaperonTransferLearninginNeuralNetworks,1976,
Informatica44(2020)291–302.
[10] C.J.C.Burges.“DimensionReduction: AGuidedTour”.en.In: FoundationsandTrendsin
MachineLearning(2010).
[11] Cook,J.,Sutskever,I.,Mnih,A.,andHinton,G.E.Visualizingsimilaritydatawithamixtureof
maps.InAISTATS,JMLR:W&CP2,pp.67–74,2007.
[12] NishanthDikkala,GalKaplun,RinaPanigrahyForManifoldLearning,DeepNeuralNetworks
canbeLocalitySensitiveHashFunctions,https://arxiv.org/abs/2103.06875.
[13] C.Ehresmann.Structuresfeuilletees.Proc.FifthCanad.Math.Congress.-Montreal.(1961),
109-172.
[14] FeffermanC,MitterS,NarayananH.2016.Testingthemanifoldhypothesis.J.Amer.Math.
Soc.29(4):983–1049.
[15] LucaGrementieriandRitaFioresi.Model-centricdatamanifold: thedatathroughtheeyesof
themodel.SIAMJournalonImagingSciencesVol.15,Iss.3(2022)10.1137/21M1437056.
[16] Hanin,BorisRolnick,David.DeepReLUNetworksHaveSurprisinglyFewActivationPatterns
NeuriPS,(2019).
[17] R.Hermann.OntheAccessibilityProbleminControlTheory.InInternationalSymposiumon
NonlinearDifferentialEquationsandNonlinearMechanics,pages325–332,AcademicPress,
NewYork,1963.
[18] Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural
networks.Science,313(5786):504-507,2006.
[19] Hinton,G.E.andRoweis,S.T.StochasticNeighborEmbedding.InNIPS15,pp.833–840.
2003.
[20] Jost,J.RiemannianGeometryandGeometricAnalysis.Springer,5thedition,2008.
[21] Krizhevsky,A.,Sutskever,I.,andHinton,G.E.Imagenetclassificationwithdeepconvolutional
neuralnetworks.InAdvancesinneuralinformationprocessingsystems,pp.1097–1105,2012.
[22] Sylvain Lavau. A short guide through integration theorems of general- ized distributions.
DifferentialGeometryanditsApplications,2017.
[23] LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P.Gradient-basedlearningappliedtodocument
recognition.Pro-ceedingsoftheIEEE,86(11):2278–2324,1998.
[24] LeCunY.,BengioY.,HintonG.Deeplearning.Nature.2015;521(7553): 436-444.
[25] LeNail,(2019).NN-SVG:Publication-ReadyNeuralNetworkArchitectureSchematics.Journal
ofOpenSourceSoftware,4(33),747,https://doi.org/10.21105/joss.00747
10[26] KeSun,StéphaneMarchand-Maillet,AnInformationGeometryofStatisticalManifoldLearning,
Proceedingsofthe31stInternationalConferenceonMachineLearning,Beijing,China,2014.
JMLR.
[27] Martens,J.Newinsightsandperspectivesonthenatu-ralgradientmethod.JournalofMachine
LearningRe-search,21(146):1–76,2020.
[28] AndreasMaurer,MassimilianoPontil,andBernardinoRomera-Paredes.Thebenefitofmultitask
representationlearning.JournalofMachineLearningResearch,17(81):1–32,2016.
[29] Murphy,KevinP.ManifoldLearning".ProbabilisticMachineLearning.MITPress,2022.of
machinelearningresearch,4(Jun):119–155,2003.1
[30] T. Nagano. Lineardifferential systems withsingularities andan application totransitive lie
algebras.J.Math.Soc.Japan,18(4):398–404,1966.
[31] Olah, C., Mordvintsev, A., and Schubert, L. Feature vi- sualization. Distill, 2017. doi:
10.23915/distill.00007.https://distill.pub/2017/feature-visualization.
[32] Rao,C.R.Informationandaccuracyattainableintheestimationofstatisticalparameters.Bull.
Cal.Math.Soc.,37(3):81–91,1945.
[33] G.Reeb.Surlesstructuresfeuilletesdecodimension1etsuruntheremedeM.A.Denjoy.Ann.
Inst.Fourier,1961.P.185-200.
[34] RoweisST,SaulLK.Nonlineardimensionalityreductionbylocallylinearembedding.Science,
2000,290: 2323–2326
[35] HangShao,AbhishekKumar,ThomasFletcher,TheRiemannianGeometryofDeepGenerative
Models,2018IEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops
(CVPRW),SaltLakeCity,UT,USA,2018,pp.428-4288,doi: 10.1109/CVPRW.2018.00071.
[36] Sommer,S.andBronstein,A.M.Horizontalflowsandmanifoldstochasticsingeometricdeep
learning.IEEETransactionsonPatternAnalysisandMachineIntelli-gence,2020.
[37] PeterStefan.Accessibilityandfoliationswithsingularities.BulletinoftheAmericanMathemat-
icalSociety,80:1142–1145,1974.
[38] HectorJ.Sussmann.Orbitsoffamiliesofvectorfieldsandintegrabilityofdistributions.Trans-
actionsoftheAmericanMathematicalSociety,180:171–188,1973.
[39] Szalai, R. Data-Driven Reduced Order Models Using Invariant Foliations, Manifolds and
Autoencoders.JNonlinearSci33,75(2023).
[40] J. B. Tenenbaum. Mapping a manifold of perceptual observa- tions. In Advances in neural
informationprocessingsystems,pages682–688,1998.1
[41] JoshuaBTenenbaum,VinDeSilva,andJohnCLangford.Aglobalgeometricframeworkfor
nonlineardimensionalityreduction.Science,290(5500):2319–2323,2000.
[42] EliotTron,NicolasCouellan,StéphanePuechmorel,Canonicalfoliationsofneuralnetworks:
applicationtorobustness,abs/2203.00922.
[43] Tu,L.W.AnIntroductiontoManifolds.Universitext,Springer,2008.
[44] Tu,L.W.DifferentialGeometry.Universitext,Springer,2017.
[45] Weiss,K.,Khoshgoftaar,T.M.Wang,D.Asurveyoftransferlearning.JBigData3,9(2016).
https://doi.org/10.1186/s40537-016-0043-6
[46] Vapnik,V.N.StatisticalLearningTheory.Wiley-Interscience,1998.
JournalofBigData
[47] ZengW,DimitrisS,GuD.Ricciflowfor3Dshapeanalysis.IEEETransPatternAnalMach
Intell,2010,32: 662–677
[48] ZhangZY,ZhaHB.Principalmanifoldsandnonlineardimensionreductionvialocaltangent
spacealignment.SIAMJSciComput,2005,26: 313–338
[49] Xiao,H.,Rasul,K.,&Vollgraf,R.(2017).Fashion-mnist: anovelimagedatasetforbenchmark-
ingmachinelearningalgorithms.arXivpreprintarXiv:1708.07747.
[50] "KMNISTDataset"(createdbyCODH),adaptedfrom"KuzushijiDataset"(createdbyNIJL
andothers),doi:10.20676/00000341
11[51] Cohen,G.,Afshar,S.,Tapson,J.,&vanSchaik,A.(2017).EMNIST:anextensionofMNIST
tohandwrittenletters.Retrievedfromhttp://arxiv.org/abs/1702.05373
[52] Krizhevsky,Alex,andGeoffreyHinton."Learningmultiplelayersoffeaturesfromtinyimages."
(2009): 7.
12