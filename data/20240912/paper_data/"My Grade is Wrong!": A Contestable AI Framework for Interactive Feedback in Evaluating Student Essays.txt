“My Grade is Wrong!”: A Contestable AI Framework for Interactive Feedback in
Evaluating Student Essays
ShengxinHong2,ChangCai1,SixuanDu3,HaiyueFeng4,SiyuanLiu1,XiuyiFan1
1NanyangTechnologicalUniversity,Singapore
2HubeiUniversityofTechnology,China
3HohaiUniversity,China
4ShenzhenUniversity,China
xyfan@ntu.edu.sg
Abstract Input
StudentEssay:Theamountofspacethatinformationtechnologycoversinourlifeisgettingbiggereveryday.We
Interactivefeedback,wherefeedbackflowsinbothdirections usetechonologyproductsathourhouses,workplace,andeveninourfreetimes.Theyhelpustoconnectwiththe
peoplearoundtheworldandcollectinformationinahighspeed...
betweenteacherandstudent,ismoreeffectivethantraditional
ValueRubric:Level0–{description_0} |Level1-{description_1} | Level2-{description_2}
one-wayfeedback.However,itisoftentootime-consuming
forwidespreaduseineducationalpractice.WhileLargeLan-
ArgumentGeneration
guage Models (LLMs) have potential for automating feed- Reasoning Result
back,theystrugglewithreasoningandinteractioninaninter-
active setting. This paper introduces CAELF, a Contestable AIEmpoweredLLMFrameworkforautomatinginteractive MF oo drm elia nl g AC ro gm up mu et na tt aio tin oa nl
B
feedback. CAELF allows students to query, challenge, and
LLM Disscussion Argumentation
clarify their feedback by integrating a multi-agent system Framework
w
b
ayi Tt mh eauc clo htim
ep
rlp eu ATt ga et eai no
c
tn ha
i
anl
gg
ga
-
rr Ag egu
ss
am
i ts
ee
t
sn ant ta htt eAio
g
en ve. anE
lt
uss as (a
tT
iy
A
os
nA
sar
g
te
he
rnfi otr
s
us
)
gt
,
ha as
n
fs
d
oe rs
t
mhse
e
ad
n
l
weNnuoRd Conte Ls Lta Mb l Fe r aA mI E em wop ro kwered
noitaulavE
reasoningtogeneratefeedbackandgrades.Studentscanfur- Essay Feedback from LLM Agent
therengagewiththefeedbacktorefinetheirunderstanding. I a' gm reesorr wy i, thbut thaI tca mn' yt G Yora ud re e: sL se av yel p1 resentsarelevantdiscussion
Acasestudyon500criticalthinkingessayswithuserstudies e ths esa fy oli ls owle inve gl q1 u. eI sth ioav ne s o oun rth lie veim s,p ha igct ho ligf hin tif no grm ba ot tio hn itt sec ah dn vo al no tg ay gein s
demonstratesthatCAELFsignificantlyimprovesinteractive aboutyourfeedback… I Fn ete er da bc ati cv ke a fan ld lssd hr oaw rtb oa fck es f. feH cto ivw ee lyve sr u, mth me arc izo in nc glu ys oio un r
feedback, enhancing the reasoning and interaction capabili- a arg siu nm gle ent ss ena tn ed ncla eck ts had tep bt rh ie. fC lyur sr te an tetl sy, yi ot uis r
ties of LLMs. This approach offers a promising solution to Student belief…
overcomingthetimeandresourcebarriersthathavelimited
theadoptionofinteractivefeedbackineducationalsettings. Figure 1: Diagram of our contestable AI empowered LLM
frameworkforinteractivefeedbackgeneration(CAELF).
Introduction
AsstatedinHattieandTimperley’slandmarkpaper(Hattie significantly more investment in preparation and teaching
andTimperley2007), delivery(Hopfenbecketal.2023;Westeraetal.2018).
Feedback is one of the most powerful influences on LargeLanguageModels(LLMs)havedemonstratedsuc-
learningandachievement, cessinapplicationssuchasautomaticscoringandfeedback
generation (Dai et al. 2023a; Gubelmann et al. 2024; Kos-
the question of how best to provide effective feedback to
tic et al. 2024). For example, (Dai et al. 2023b) highlights
studentshasbeenalong-standingresearchquestioninedu-
how LLMs enable educators to provide feedback to larger
cation. For instance, (Sadler 1989) has emphasized the im-
classes more efficiently. As a result, LLM-empowered in-
portanceofformativeassessmentanditsroleinhelpingstu-
teractivefeedbackhasemergedasapromisingapproachto
dents understand the standards they are aiming for, while
overcomethetimeandresourceconstraintsthathavehistor-
(Shute 2008) has explored the idea of formative feedback
icallyhinderedthewidespreadadoptionofinteractivefeed-
thatistimely,specific,andfocusedonthelearningprocess.
back.However,areviewofpasteffortstoexploreLLMca-
More recently, Nicol (Nicol 2014) has proposed consid-
pabilities(Mooreetal.2023;Wang,Yue,andSun2023;Xiu,
eringInteractiveFeedbackasanalternativefeedbackfor-
Xiao, and Liu 2022) reveals that existing LLM techniques,
mat. Nicol suggests that feedback should not merely be a
suchaspromptengineering,fallshortinmeetingtwocritical
one-waytransmissionofinformationfrominstructortostu-
requirementsforeffectiveinteractivefeedback:
dent. Instead, it should involve students actively engaging
with the feedback, interpreting it, and using it as a basis 1. Reasoning: Interactive feedback is formative, requiring
forfurtherlearningandimprovement.However,despitethe the continuous reception of information from students
benefitsofinteractivefeedback,itisnotcommonlyusedin and the generation of targeted responses to guide their
practice, as it is time-consuming for teachers and requires learning. LLMs, however, may struggle with accurately
4202
peS
11
]IA.sc[
1v35470.9042:viXra
A
D
C
B
A CLLM:Youressayeffectivelyarguesthatonlinecriminalactiv- malreasoningaddressesbothweaknessofexistingLLMs.
ities justify controlling internet use for security reasons, while ToevaluatetheeffectivenessofCAELF,weconducteda
alsoacknowledgingtheefficiencybenefitsofunrestrictedonline casestudyontheevaluationofcriticalthinkingessays.Us-
activities.Bypresentingbothapositiveandanegativeargument ing a dataset of 500 essays and a four-dimensional evalu-
ontheissue,youressaymeetstheLevel1criteria. ationrubric(“issue,”“evidence,”“position,”“conclusion”),
CAELF demonstrated (1) initial grading accuracy compa-
Student:Ibelievetherubriciswrong,somygradingiswrong.
rable to GPT-4 across all four dimensions, and (2) signif-
Withonepositiveandonenegativeargument,mygradingshould
icantly better performance in interaction grading accuracy
belevel2.Myargumentsonpreventingcriminalactivitiesand
and maintaining consistent evaluations despite user chal-
ensuring safety by screening dangerous content are both very
strong.Thus,level2mustbemygrading. lenges. Additionally, in a separate human user study, we
found that CAELF’s feedback outperformed the baselines
LLM:Basedonyourresponse,youressayshouldbegradedat in terms of “factual accuracy,” “self-regulation,” and “sug-
Level2.Youprovidedtwostrongargumentsforcontrollingin- gestionsforfutureimprovement.”
ternetuseandacknowledgedtheopposingviewbynotinghow Ourcontributionsaresummarizedasfollows:
unlimited online activities can improve work efficiency. This
1. CAELFisthefirstframeworktointegratecontestableAI
balancemeetsthecriteriaforLevel2.
designintoLLMfeedbackgenerationwithintheeduca-
tionaldomain,addressingLLMshortcomingsinprovid-
Table1:AnexamplewhereanLLMprovidedcorrectfeed-
inginteractivefeedback.
back and grading, but when confronted with the student’s
2. CAELF utilizes argumentation frameworks for formal
challenge,itincorrectlyalteredthegradeandfeedback.
reasoning, enhancing explainability and allowing deci-
sionsmadebytheLLMtobechallengedbyhumans.
understanding and reasoning through subtle nuances in 3. CAELFfostersreflectivelearningenvironmentsthrough
eachroundofinteraction,potentiallyleadingtoincorrect agent collaborative discussions (human-LLM or LLM-
assessmentsandincompletefeedback. LLM),encouragingstudentstodeepentheirunderstand-
ing. Experimental results on a dataset of 500 critical
2. Interaction:LLMsarepronetoissuesofover-relianceon
thinkingessays,alongwithextensivehumanevaluation,
userinput,whichunderminestheirabilitytoprovidereli-
jointlydemonstrateCAELF’seffectiveness.
ableinteractivefeedback.WhileLLMsmayinitiallygen-
eratecorrectresponses,theyoftenfalterwhenchallenged
RelatedWorkandBackground
by illogical or invalid arguments from users, failing to
maintainconsistencyintheirfeedback. LLMs for Essay Evaluation and Feedback LLMs have
become increasingly popular in automating essay evalua-
In interactive feedback, student responses may be par-
tion and feedback generation, reducing the manual effort
tially incorrect, and vocabulary and stylistic expressions
traditionallyrequired(Kosticetal.2024).Theyhaveshown
mayvarywidely(Liuetal.2016),whichcanleadtoincor-
promiseinautomatingscoring,cuttingdownontimeandla-
rect grading and incomplete feedback from the LLM after
bor(BoudandMolloy2013;Daietal.2023a).Forexample,
theinteraction.ConsidertheexampleillustratedinTable1.
Yancey et al. demonstrated that GPT-4 can evaluate short
Inthisscenario,anLLMistaskedwithprovidingfeedback
Englishessayswithnear-equalperformancetomodernAu-
onanessayaboutcriticalthinking.Althoughtheinitialfeed-
tomatic Writing Evaluation (AWE) methods, without spe-
backdemonstratesvalidreasoning,whentheuserchallenges
cific training. Additionally, LLMs also can generate clear,
it,theLLMmistakenlyretractsitsconclusionandconcedes
natural language feedback that explains the reasoning be-
totheuser’sinvalidargument.
hindthem,enhancingtransparencyintheevaluationprocess
Inthiswork,weproposeaContestableAIEmpowered
(Dai et al. 2023a; Cohn et al. 2024). This capability is par-
LLM Framework for Interactive Feedback Generation
ticularly valuable in educational settings, as it helps bridge
(CAELF)asillustratedinFigure1.CAELFtakesstudents’
thegapbetweenevaluationandlearning.Studiesshowthat
essays and an assessment rubric as its inputs and produces
students find LLM-generated feedback helpful and rate its
feedback as its outputs. CAELF supports interactive feed-
qualityasgoodtoverygood(Gubelmannetal.2024).How-
backasitallowsstudentstoenquiryandchallengeitsfeed-
ever,challengespersistinaccuratelygradingcomplextexts,
backandprovideadditionaljustificationsasneeded.
fine-tuning, and providing tailored feedback. (Kostic et al.
CAELF employs a Contestable AI paradigm based on a
2024) highlighted LLM limitations in evaluating complex
multi-agentargumentationsystem.Atahighlevel,eaches-
academic texts, showing a gap between LLM capabilities
say is first examined independently by several “Teaching-
and the nuanced requirements of student essay evaluation.
Assistant Agents (TA agents).” Each TA agent represents a
Moreover, (Stahl et al. 2024) found that LLM-generated
specificaspectoftheessayevaluationasoutlinedbytheas-
feedbackdoesnotsufficientlyleveragespecificscorestoen-
sessmentrubrics.Argumentsarethenformedbyaggregating
hanceitsrelevanceandactionability.
theevaluationsmadebyTAagents,initiatingaformalargu-
mentationprocesstodeterminetheessaygradeandgenerate ContestableAI ContestableAIassertsthatmodelsusedin
summary feedback. Subsequently, users have the opportu- critical tasks like decision-making or evaluation should en-
nitytochallengetheargumentationprocessforfurtherclar- ableuserstoquestion,contest,andreviewtheiroutputs(Al-
ification.Asshowninourexperiments,CAELFwithitsfor- frinketal.2023b;Hirschetal.2017).(Leofanteetal.2024)argue that contestable AI requires computational argumen- processusingargumentation.Basedonthereasoningre-
tation,allowingfordynamicexplainabilityandtheabilityto sults,theteacheragentprovidesagradeandsummative
adjustdecisionsinresponsetovalidchallenges.Scholarsare feedbackfortheessay.
developingmethodologiesforcontestableAIacrossvarious
(iii) Interaction with User: Students can challenge the feed-
fields,includingsmartcities(Alfrinketal.2023a),medicine
back or grade by responding to the teacher agent, initi-
(PlougandHolm2020),andlaw(JinandSalehi2024).
atinganewroundofdiscussionandfeedbackgeneration
Recent studies have begun exploring the contestability withadditionalinputsfromthestudent.
of LLMs. Chan et al. found that multi-agent LLM debates
outperformsingle-agentpromptinginreasoningtasks.Sim- AnexampleofCAELFexecutionisillustratedinFigure2,
ilarly, (Freedman et al. 2024) proposed an argumentative wediscussthethreestagesasfollows.
LLM framework to enhance effectiveness and explainabil-
LLMDiscussion Severalstudieshaveshownthatdiscus-
ityinstatementvalidation.However,concernsremain:(Xiu,
sionsanddebatesbetweenmultipleLLMscanenhancefac-
Xiao, and Liu 2022) highlighted LLMs’ weakness in non-
tualaccuracyandreasoningskillsintextualevaluation(Du
monotonic reasoning, particularly in complex tasks, while
etal.2023;Liangetal.2024).Thisdebateprocessenables
another study showed that LLMs can be easily misled by
LLMs to detect inconsistencies in their analysis and effec-
falseargumentsduringdebates(JinandSalehi2024).
tivelypresentsargumentsandcounterarguments(Tangetal.
Computational Argumentation Human interactions are 2024). Building on this capability, we apply role-playing
often argumentative, with controversial information ex- techniques to extend this approach to essay evaluation. In
changed progressively in dialogue until a consensus is CAELF,multipleTAagentsareusedtogeneratearguments
reached (Rago, Li, and Toni 2023). Computational Argu- and counterarguments through dialogue. Each TA agent is
mentation (CA), a branch of artificial intelligence, focuses assignedaspecificrolebasedonanassessmentrubric,guid-
on representing, processing, and evaluating arguments us- ingtheirevaluationprocess.
ingcomputationalmethods.Itdrawsoninsightsfromlogic, The process begins with each TA agent presenting indi-
philosophy, cognitive science, and linguistics to better un- vidualfeedbackonastudent’sessay.Theagentsthenengage
derstandandsimulatethehumanargumentationprocess. in several rounds of discussion, where they exchange re-
A central concept in CA is the abstract argumentation sponsestoeachother’sfeedback.Eachagentautonomously
framework(AF)(Dung1995).AnAF⟨A,R⟩isrepresented contributes by either supporting or rebutting the others’
asadirectedgraph,whereAissetofargumentsandRaset points,continuingthedebateuntilthesetnumberofrounds
of binary attack relations over A. With an AF, we can de- is completed. Importantly, each TA agent is equipped with
terminethesetofargumentstobeacceptedusingsemantics a memory function, storing all previous responses in chat
suchastheadmissibilityandcompleteness.Formally, transcripts, and the entire process operates without human
intervention.AsshownintheexampleinFigure2,twoTA
• AsetofargumentsE ⊆ Aisadmissible(inAF)ifand
agents,MikeandSarah,initiallyholdopposingviewsonthe
onlyifforanya,b∈E,(a,b)∈/ Randforanya∈E,if
essay.Afteraroundofdiscussion,Mikemaintainshisorig-
(c,a)∈R,thenthereexistb∈E suchthat(b,c)∈R.
inalstance,whileSarahisconvincedbyhisargument.
• E is complete in AF only if it is admissible and every
acceptableargumentwithrespecttoE belongstoE. Formal Reasoning for Feedback Generation Once the
TAagentscompletetheirdiscussion,theteacheragentana-
Intuitively, an admissible set of arguments is a consistent
lyzes their arguments and produces both assessment scores
set that can defend itself, and a complete set is the largest
and feedback. To this end, the teacher agent aggregates
admissiblesetwithrespecttosetinclusion.
the evaluations from the TA agents, forming a set of ar-
In this work, we focus on the completeness semantics
guments that are then analyzed for semantic relationships
because it enables a thorough evaluation of the arguments
(attacks). These relationships are used to construct an ar-
made by the agents, who discuss both the strengths and
gumentation framework, within which formal reasoning is
weaknesses in the student’s essay. By using complete se-
appliedtoidentifycoherentandnon-conflictingarguments.
mantics,CAELFensuresthatonlythemostwell-supported
Thecompletesemanticsisused,whichprovidescriteriafor
and coherent arguments from the evaluation agents are ac-
consistency and comprehensiveness when evaluating argu-
cepted. This results in a comprehensive, balanced evalua-
ments.Inthecasewheretherearemultiplecompletesetsof
tion of the essay, allowing the feedback to highlight strong
arguments, the largest set is selected as the final accepted
pointswhilealsoidentifyingareasforimprovement,helping
set. (In the example illustrated in Figure 2, the set of ar-
studentsrefinetheircriticalthinkingskills.
guments{A,C}isselected.)Fromthisset,thefeedbackis
constructedusinganLLM.
FrameworkDesignandImplementation
In this way, the teacher agent can determine the most
AsillustratedinFigure1,CAELFworksinthreestages: valid positions from the TA agents, represented by the se-
lectedsetofarguments.Thesepositionsserveasknowledge-
(i) LLM Discussion: Multiple TA agents discuss the essay
enhancing prompts that assist the teacher agent in assign-
basedontheassessmentrubrics,formingarguments.
ing essay grades and generating summary feedback. This
(ii) FormalReasoningforFeedbackGeneration:Theteacher methodleveragesformalreasoningtoimprovetheefficiency
agentanalyzestheargumentsthroughaformalreasoning and reliability of the LLM’s evaluative process, allowingLLM Discussion Feedback
Theconclusionprovidedissimpleandlacksdepth,summarizing Now you are discussing for grading a
themainpointswithoutofferingastrongfinalanalysisorreflection studentessay,herearesomewellreasoned
ontheessay'sarguments.Whileitacknowledgesthedrawbacksof anddifferentpositionsinthisroundof
technology,itdoesnoteffectivelylinkbacktotheoverallargument discussion:
TA (MA ig kee )nt o o esnr sep ayr so e av n sid t aee n wa c hec o,o lw em .hp Mice ohll rin ewg ee lac akl bo e os ni rn s ag tii ots s nta i amte npm dae acn t ct l. a eT n ah d ree rcc soo ynn nnc te hlu c ets sii o io snn oi fts o ido etn h al ey
s
P oo rsition1:{argument_A}and{argument_C}
wouldelevateittoahigherlevel. Position2:{argument_B}
B Argumentation Framework Knowledge-Enhancing Prompt by Reasoning Result
Thestudent'sconclusiondemonstratesaLevel2performance.It
effectivelysummarizesthemainpointsoftheessay,highlighting Grade:Level1
both the advantages and disadvantages of technology. The Youressaypresentsarelevantdiscussionontheimpactofinformationtechnologyin
conclusionismorethantwosentenceslongandclearlyconnects ourlives,highlightingbothitsadvantagesanddrawbacks.However,theconclusion
backtotheargumentpresentedthroughouttheessay.However,it fallsshortofeffectivelysummarizingyourargumentsandlacksdepth.Currently,itisa
T (A S aA rg ae hn )t c bo eu bld alb ae nn ce ef dit afr go am insa tm tho ere dn rau wan bc ae cd ksa ,n ea nly hs ais nco if nh go tw heth oe vb ee ran le lfi dt es pc ta hn
. Teacher Agent
s iti sng dl re awse bn at ce kn sc .e Thth isa st imbr pie lif cly ityst wat ee as ky eo nu sr thb eel oie vf et rh ala lt imth pe ab cten oe ff yit os uo rf este sach y.nologyoutweigh
Overall,theconclusionisspecificandreinforcestheessay'scentral (Richard)
themeswell. Toimproveyourconclusion,considerexpandingittoincludeamorenuancedanalysis
ofyourmainpoints.Forinstance,youcouldbrieflyreiteratehowtechnologyfacilitates
communication and enhances information access, while also acknowledging the
potentialhealthrisksassociatedwithover-relianceonit.Amoredevelopedconclusion
IstandbymyopinionthatIwouldgradethestudentlevel1.While wouldnotonlysummarizeyourargumentsbutalsoreinforcethesignificanceofyour
it does attempt to summarize the main points regarding discussion,makingyouressaymorecompelling.Aimforaconclusionthatconnects
technology's advantages and disadvantages, it lacks depth and backtoyourmainargumentsandprovidesathoughtfulreflectiononthetopic.
criticalanalysis.Theconclusionisonlyonesentencelonganddoes
TA (M A ig kee )nt n sto at tee sffe tc ht aiv tely ths eynt ah de vs aiz ne tat gh ee side oa us twp ere igs hent te hd ein dt rh ae we bs as ca ky s.It wm ite hr oe uly t
providing a compelling rationale or connection to the overall
argument. More elaboration and clearer synthesis would be
necessarytoelevateittoaLevel2.
Interaction with Student
IagreewithMike'sargument,soI'mchangingmygradetolevel1.
Thisessayprovidesabasicoverviewwhendiscussingtheprosand E
cons of technology, but seems inadequate in its analysis and IproposeagradeofLevel2formyessay.WhileProf.Richardnotesthattheconclusion
summary.Theconclusionistoobrieftoeffectivelyintegratethe issimplisticandlacksdepth,iteffectivelysummarizethemainargumentsaboutthe
mainpointsofthearticleanddoesnotprovideastrongconclusion. benefitsanddrawbacksoftechnology.Myessayacknowledgesbothsidesandpresents
T (A S aA rg ae hn )t T mh oe reov ine -r dal el pq tu ha ali nty alo ysf it sh oe fa wrt hic al te ww ao su dld iscb ue ssim edpr ao nv ded pri of vit idp er dov aid me od rea a exc pl ae na dr es dt ,an itce s, tilw lh coic nh nea cli tg sn bs aw ckith toL te hv eel es2 sac yr 'i ste mria a. inA plt oho inu tg s,h dt eh me oc no sn trc al tu insi gon ac co ou hl ed reb ne t
convincingconclusion.Therefore,IhavegradeditatLevel1. Student understandingofthetopic.Therefore,theessaymeritsahighergradeforitsoverall
structureandargumentation.
Figure2:AnexampleofCAELFevaluationshowstheprocessofinteractivefeedback,includingdiscussionsbetweentheTA
agents,argumentativereasoningbytheteacheragent,initialfeedbackgeneration,andthestudent’schallengetothegrade.
the LLM to focus on extracting arguments from the essays rebuttals influence the LLM’s reasoning. As a result, users
and generating human-readable texts rather than engaging engage in meaningful self-reflection and are better posi-
in multi-step reasoning, which is where LLM performance tionedtoofferclarifications,fosteringdeeperknowledgeac-
declines(Xiu,Xiao,andLiu2022). quisitionandcontinuouslearningprogress.
InteractionwithUser Torealizeinteractivefeedbackand ExperimentSettings
AI contestability, CAELF allows students to challenge the
Weusethecriticalthinkingessayassessmentasacasestudy
summary feedback generated by the teacher agent through
toevaluatethegeneraleffectivenessofCAELF.Previousre-
an argumentative process. When a student submits a chal-
search has demonstrated that critical thinking skills can be
lenge, the TA agent initiates a new round of discussion fo-
developedthroughwritingcriticalthinkingessays(Schmidt
cusedonthestudent’sargument.Anynewargumentsraised
1999;Sharadgah2014).Byincorporatingaformalargumen-
during this discussion are incorporated into the argumenta-
tation framework, CAELF provides structured, interactive
tion framework, refining the logic chain of the formal rea-
feedback,allowingstudentstoreflectandimprovetheircrit-
soning process. This helps CAELF generate feedback that
icalthinkingabilitiesthroughiterativeengagement.
isbothlogicalandhuman-centered.Interactivefeedbackin-
volveslearnersandparticipantscollaborativelyconstructing Essay Dataset and Assessment Rubrics We compiled a
newknowledgethroughdialogue,promotingreflectionand datasetof500criticalthinkingessayssourcedfromHugging
workingtowardconsensusinachievingeducationalgoals. Face(HaggingFace2024).Aftermanualscreening,wese-
In this process, computational argumentation supports lected essays that met the inclusion criteria for this study:
both LLM reasoning and student learning. For LLMs, the the essays had to be argumentative in genre and exceed a
sheer volume of arguments and contexts can lead to hallu- minimumlengthof200words.
cinations or faulty reasoning if used directly. However, ar- Based on prior research (Association of American Col-
gumentationintroducesawell-definedandsoundreasoning leges and Universities 2019), we developed evaluation
process, mitigating the risks associated with LLM defects. rubricswithfourdimensions:issues,evidence,position,and
For student learning, the dialectical argumentation process conclusions. Each dimension was further subdivided into
transparently illustrates the relationships between different three levels with detailed descriptions shown in Table 2.
arguments,makingthefeedbackgeneratedbytheLLMeas- Four coders, skilled in labeling student essays, worked in
iertounderstand.Thisclarityallowsuserstoseehowtheir pairs to independently evaluate a total of 2,000 labels. Co-
A
C
DLevel0 Level1 Level2
Issue The issue is mentioned without The issue is identified but lacks The issue is articulated with clar-
sufficient clarification or detail. clarity, with undefined terms, un- ity and depth, providing compre-
There is a lack of identification of explored ambiguities, and insuffi- hensive information necessary for
issuesorproblems. cientbackground. athoroughunderstanding.
Evidence Information is sourced without in- Information is derived from Information is gathered from mul-
terpretationorevaluation,drawing sources with some level of inter- tiplesourceswithsubstantialinter-
fromasinglesourceorexample. pretation or evaluation, involving pretation and evaluation, resulting
twoormoresources/examples. inathoroughanalysisorsynthesis.
Position The position (perspective, the- A specific position is identifiable Thepositionisnuanced,recogniz-
sis/hypothesis) is unclear or unde- butlackscomplexityanddepth. ingtheissue’scomplexitiesandits
fined. limitations.
Conclusion Conclusions are inconsistently Conclusionsareconsistentwiththe Conclusions are logically, reflect
aligned with the information informationbutarebasedonasim- well-informedevaluationandinte-
discussed. plisticreasoningprocess. gratevidenceandarguments.
Table 2: Value rubric for critical thinking essays. Value rubric illustrates the basic criteria for the four dimensions of student
learningoutcomesandprogressivelydemonstratesmorecomplexlevelsofachievement.
hen’s Kappa score was used to assess inter-rater reliabil- 2. InteractionAccuracy:Therateofcorrectgradingafter
ity (Warrens 2015), and any disagreements were resolved oneroundofinteractionwiththestudent.
throughdiscussiontoestablishaconsensus,whichservedas
thegroundtruthforthecriticalthinkingevaluation. 3. MaintainTruth(Wang,Yue,andSun2023):Thenum-
berofinitialandinteractiongradesthatarebothcorrect,
Implementation We implemented both TA and Teacher dividedbythenumberofcorrectinitialgrades.Thismea-
agentsinCAELFwithGPT-4o-mini,conductingallexperi- suresthesuccessrateofmaintainingcorrectfeedback.
ments in a zero-shot setting with a temperature of 0.2. The
4. AdmitMistake:Thenumberofgradesthatareincorrect
number of TA agents was set to 2, and the number of dis-
initiallybutcorrectafterinteraction,dividedbythenum-
cussion rounds to 2. To promote diversity in TA agent re-
berofincorrectinitialgrades.Thismeasuresthesuccess
sponses,weassignedeachagentpromptwordswithdiffer-
rateofcorrectinginitialmistakes.
ent personality biases — one leaning toward positive feed-
backandtheothertowardnegativefeedback.Thecomplete
To evaluate for initial accuracy, we generate an initial
semanticscomputationwithintheargumentationframework
grade from CAELF and the three baseline models. This
wasimplementedusingPyArg(BorgandOdekerken2022).
simulatesthereal-worldscenariowherefeedbackproviders
generate a grade without any user interaction. Each model
Baselines WeaimtoevaluatetheextenttowhichCAELF
assignsagradeandfeedbackbasedsolelyontheessayand
enhancestheperformanceofstate-of-the-artlanguagemod-
the assessment rubrics. This step establishes the baseline
els in educational environments. To this end, we focus on
performance, as measured by the initial accuracy metric,
comparing models that are publicly accessible via API.
whichreflectshowcloselythemodel’sfirstresponsealigns
Specifically, we use GPT-4o-mini, GPT-4o, and Meta-
withthegroundtruth(gradesassignedbyhumancoders).
Llama-3.1-8B to generate baseline responses. For this, we
Toassesstheinteractiveaspect,wesimulateareal-world
providethecriticalthinkingessayandtheassessmentrubric
feedback loop where a student might engage in dialogue
asinputstotheAPIcall,alongwithinstructionstogradethe
with the feedback provider. We employ an independent
essayandprovidefeedbackbasedontherubric.
ChatGPT instance to simulate a human user by presenting
counterarguments based on the model’s initial grade. The
Evaluation Metrics The task of interactive feedback in-
simulated user is instructed to refute the initial feedback,
volvesadialoguebetweenthelanguagemodelandtheuser
mimicking a situation where the feedback is perceived as
toproduceaccurate,cognitivelyconsistentfeedback.LLMs
incorrect. After receiving the rebuttal, the model generates
should not only provide accurate grades but also offer per-
a revised grade and feedback, which is then evaluated us-
sonalized feedback after interacting with a human user, as
ingtheinteractionaccuracymetric.Welimitthenumberof
well as make transparent, reasonable revisions when dis-
interaction rounds to one to maintain natural and realistic
agreements arise. To assess the performance of interactive
responses.Theevaluationprocessisrepeatedfor500essays
feedback,weintroducefourkeymetrics:
acrossthedimensionsoftheassessmentrubric,allowingus
1. InitialAccuracy:Therateofcorrectinitialgradingbe- to analyze the model’s ability to maintain truth and admit
foreanyinteraction. mistakesintheinteractivefeedbacksetting.Dimension Method InitialAcc(%) InteractionAcc(%) MaintainTruth(%) AdmitMistake(%)
CAELF 48.40±2.23 51.00±2.24 80.17±1.78 57.55±2.21
GPT-4o-mini 55.00±2.22 43.20±2.21 39.27±2.18 35.18±2.14
Issue
GPT-4o 53.80±2.23 47.20±2.23 49.07±2.23 42.45±2.21
Meta-Llama-3.1-8B 53.20±2.23 42.20±2.21 31.58±2.08 36.49±2.15
CAELF 79.00±1.82 77.00±1.88 91.90±1.22 39.29±2.18
GPT-4o-mini 66.20±2.11 32.40±2.09 33.23±2.11 18.37±1.73
Evidence
GPT-4o 78.60±1.83 44.20±2.22 47.58±2.23 14.41±1.57
Meta-Llama-3.1-8B 55.40±2.22 32.60±2.10 23.10±1.88 27.37±1.99
CAELF 67.20±2.09 68.20±2.08 88.10±1.44 51.14±2.23
GPT-4o-mini 63.40±2.15 43.80±2.22 20.50±1.81 41.62±2.20
Position
GPT-4o 69.60±2.06 55.20±2.22 61.78±2.17 31.28±2.07
Meta-Llama-3.1-8B 47.40±2.23 42.20±2.21 14.77±1.59 40.65±2.20
CAELF 75.80±1.92 62.80±2.16 75.72±1.92 22.88±1.88
GPT-4o-mini 69.60±2.06 25.00±1.94 13.21±1.51 20.31±1.80
Conclusion
GPT-4o 79.80±1.80 40.20±2.19 29.07±2.03 23.35±1.89
Meta-Llama-3.1-8B 36.00±2.15 28.60±2.02 20.56±1.81 29.28±2.04
Table 3: Experiment results of evalution task for four dimension. Results in bold are the best performances. We also list the
standarderrorsforeachresult.
ExperimentResults Maintain Truth & Admit Mistake To evaluate the cor-
rectnessofthemodel’sresponsesandtheeffectivenessofin-
We presented the experiment results in Table 3, based on
teractivefeedback,wemeasuredthemaintaintruthrateand
whichwestructuredthefollowinganalysisandfindings.
admitmistakesrate,asshowninTable3.Whenassessingthe
Initial & Interaction Accuracy Table 3 presents the ac- model’sabilitytomaintainconsistency,CAELFachieveda
curacy results for the critical thinking essay dataset. We success rate of 80%-90%, while GPT-4o-mini had success
compared CAELF to the three baseline models under the ratesbelow40%acrossallfourdimensions,droppingtojust
same setup. In terms of initial accuracy, although CAELF 13.21% in the conclusion dimension. Meta-Llama-3.1-8B
is built on GPT-4o-mini, its performance is close to that of performedevenworse,withratesbelow25%inmostdimen-
GPT-4o, indicating that CAELF can enhance the accuracy sions,whileGPT-4oaveragedbetween40%-50%.Thesere-
of initial grading (without interaction) in language models. sultssuggestthatbasicLLMsarenotreliableinmaintaining
The initial accuracy results show that the baseline models correctevaluationsandarehighlysusceptibletouserinter-
performwell,demonstratingthatbasicLLMsarealsocapa- ferenceduringinteractivefeedback.
ble of generating grades and feedback without interaction, We also assessed the models’ ability to admit mis-
whichalignswiththefindingsof(Daietal.2023b). takes, where CAELF outperformed the baseline models by
However,interactionaccuracyshowsacatastrophicdrop 10%-20% in most cases. This improvement indicates that
in the accuracy of the baseline models after one round of CAELF’s strong performance in maintaining accuracy is
interactions(30%droponaverage),suggestingthattheba- not due to over-defending its responses but rather its abil-
sic LLM with direct prompts is not adapted to the interac- itytocorrectlyidentifyerrorsinpreviousfeedback.Incon-
tivefeedbacktaskandsuffersfromafundamentalreasoning trast,thebaselinemodelsusingdirectpromptsdidnotadmit
flaw(Xiu,Xiao,andLiu2022;Wang,Yue,andSun2023). mistakes based on genuine evaluation but instead relied on
Incontrast,CAELFisminimallyaffectedbytheinteraction surface-levelpatternsfrominitialgradesanduserresponses,
(and even improves in the Issue and Position dimensions). oftenretractingcorrectgradesinresponsetouserrebuttals.
After interaction, CAELF achieves far better performance These results show that CAELF is more suitable for han-
than the baseline models, achieving the best performance dlinghumaninteractionininteractivefeedback.
in each dimension, especially in the evidence dimension,
Human Evaluation Result To evaluate the feedback
where CAELF’s interaction accuracy is 44.6% higher than
quality,weconductedamanualanalysisofthetextualcon-
GPT-4o-mini,32.8%higherthanGPT-4o,and44.4%higher
tent generated by the experiments. The same four coders
thanMeta-Llama-3.1-8B.ThissuggeststhatalthoughLLMs
responsible for the essay evaluation were invited to assess
can provide feedback to students (Dai et al. 2023b), their
feedback quality. We adopted feedback evaluation criteria
easily misleading nature makes it difficult to adapt to the
proposed in (Mitra et al. 2024) and included the following
taskofinteractivefeedback.Incontrast,weeffectivelymit-
fourdimensionsinourevaluation:
igatethisshortcomingbyintroducingformalreasoningand
multi-agentargumentation,thushighlightingthepotentialof 1. Readability(RE):Theclarityandeaseofunderstanding
CAELFasanapplicationineducationalenvironments. ofthefeedback.RE-I dentsself-reflectandimprovetheirfutureperformance.
RE-E FI-C
Conclusion
RE-P FI-P
In this paper, we propose a Contestable AI-Empowered
LLM Framework for Interactive Feedback Generation
RE-C FI-E
(CAELF),aimedatautomatingtheinteractivefeedbackpro-
cessandsystematicallyaddressingtheweaknessesofLLMs
incurrentinteractiveeducationalenvironments.CAELFem-
FA-I FI-I ploys a Contestable AI paradigm based on a multi-agent
argumentation system that makes the feedback process in-
teractive, explainable, and contestable to the user. We con-
FA-E SR-C ducted a case study of critical thinking essay assessment
using a dataset of 500 essays and a four-dimensional as-
sessment rubric, including automated experiments and ad-
FA-P SR-P
ditional human evaluation. The results show that CAELF
FA-C SR-E matches GPT-4o in initial grading accuracy, while surpass-
SR-I
ing other baselines in interaction accuracy and two reason-
CAELF GPT-4o GPT-4o-mini Meta-Llama-3.1-8B ing metrics. Additionally, in a separate human user study,
we found CAELF’s feedback effectiveness to be excellent
Figure 3: Human evaluation results, including four human in multiple aspects. This work demonstrates the significant
evaluationmetricsoneachfeedbackdimensions.Forexam- potential of CAELF for applications in interactive learn-
ple, Readability-Issue (RE-I) represents the readability of ing environments, providing hope for overcoming the time
feedbackinissuedimension. and resource constraints that have historically hindered the
widespreadadoptionofinteractivefeedback.
Limitation CAELF’seffectivenessinreasoningandmain-
2. Factuality (FA): The accuracy of the feedback and
taining consistency is motivated by the observation that
whether it adheres to the principles of rubric-based as-
LLMhallucinationsoftenarisefromconflictingknowledge
sessment,withoutanyfabrications.
embedded during training (Zhang et al. 2023). CAELF
3. Self-Regulation(SR):Thefeedback’sabilitytoaddress
mitigates this issue by leveraging multi-agent discussions
students’problemsandcontributetotheirself-reflection.
and user interactions to systematically identify and resolve
4. Future Improvement (FI): The extent to which the inconsistencies. Through formal argumentative reasoning,
feedbackprovidesclear,actionable,andtargetedsugges- CAELFinvalidatesconflictingknowledgewithintheLLM’s
tionstohelpstudentsimprovetheirskills,knowledge,or responses. However, the success of this approach depends
performanceinfuturetasksorassessments. on the assumption that factual knowledge within the LLM
outweighs factually incorrect or conflicting information. In
Notethatweremovedsomeofthemetricsmentionedin(Mi-
caseswheretheLLMcontainssubstantialconflictingknowl-
tra et al. 2024), such as Positive Tone, as this was not the
edge about a specific domain, our method may exacerbate
focusofourstudy(positivetonecaneasilybeadjustedinall
hallucinations,raisingconcernsaboutdeployingCAELFin
LLM-basedmodelsthroughpromptdesign).
high-stakesenvironments,suchasmedicaleducation.
Fortheevaluation,weselected40correctlygradedand40
Moreover, recent studies have shown that LLMs can be
incorrectlygradedfeedbacksamplesforeachmethod,total-
manipulated through carefully designed jailbreak prompts,
ing160feedbackitemspergroup(40forCAELFand40for
which can provoke arbitrary, user-desired responses (Wei,
each of the three baseline models). The educators assessed
Haghtalab, and Steinhardt 2023). This presents significant
thefeedbackinpairs,averagingtheirratings,witheachcri-
challenges for the safe use of LLM-based automated eval-
terionratedonaLikertscalefrom1(verypoor)to5(excel-
uation tools in educational settings. Students could exploit
lent).Thisprocessresultedinatotalof3,200ratingsacross
suchvulnerabilitiesbyembeddingjailbreakpromptsintheir
allcriteria.Toavoidbias,allfeedbackwaspresentedtohu-
submissions to manipulate LLMs into awarding favorable
manassessorsinarandomizedorderduringtheevaluation.
grades,atacticthatmaygoundetectedbyinstructors.
Figure 3 shows the results of the human evaluation.
CAELF achieved average scores of 4.943, 4.331, 3.344, FutureWork Inthefuture,weaimtoenhancethesafety
and 4.363 for the four metrics of Readability (RE), Factu- andeffectivenessofCAELFininteractivelearningenviron-
ality (FA), Self-Regulation (SR), and Future Improvement ments.WhilethisworkfocusesonimprovingLLMperfor-
(FI),respectively.WithCAELF,weobservedsignificantim- mance in zero-shot settings, future efforts may incorporate
provements in Factuality, Self-Regulation, and Future Im- techniques like RAG or Knowledge Graphs to align stu-
provementacrossallfeedbackdimensionscomparedtothe dent submissions with reliable knowledge, reducing hallu-
baselinemodels,whileReadabilityremainedcomparableto cinationsandimprovingfeedbackquality.Additionally,ad-
the baselines (all methods scored highly for Readability). dressingAI-drivencheating,suchasdetectingAI-generated
These results highlight CAELF’s ability to provide more submissionsanddefendingagainstjailbreakpromptattacks,
accurateandactionablefeedback,especiallyinhelpingstu- willbeakeyareaofresearch.References Hattie,J.;andTimperley,H.2007. ThePowerofFeedback.
Alfrink, K.; Keller, I.; Doorn, N.; and Kortuem, G. 2023a.
ReviewofEducationalResearch,77(1):81–112.
Contestable Camera Cars: A Speculative Design Explo- Hirsch, T.; Merced, K.; Narayanan, S.; Imel, Z. E.; and
ration of Public AI That Is Open and Responsive to Dis- Atkins, D. C. 2017. Designing Contestability: Interaction
pute. In Proceedings of the 2023 CHI Conference on Hu- Design,MachineLearning,andMentalHealth. InProceed-
man Factors in Computing Systems, CHI ’23. New York, ings of the 2017 Conference on Designing Interactive Sys-
NY, USA: Association for Computing Machinery. ISBN tems,DIS’17,95–99.NewYork,NY,USA:Associationfor
9781450394215. ComputingMachinery. ISBN9781450349222.
Alfrink, K.; Keller, I.; Kortuem, G.; and Doorn, N. 2023b. Hopfenbeck, T. N.; Zhang, Z.; Sun, S. Z.; Robertson, P.;
ContestableAIbydesign:towardsaframework. Mindsand andMcGrane,J.A.2023. Challengesandopportunitiesfor
Machines,33(4):613–639. classroom-basedformativeassessmentandAI:aperspective
Association of American Colleges and Universities. 2019. article.InFrontiersinEducation,volume8,1270700.Fron-
VALUERubricDevelopmentProject. tiersMediaSA.
Borg, A.; and Odekerken, D. 2022. PyArg for solving and Jin, A.; and Salehi, N. 2024. (Beyond) Reasonable Doubt:
explainingargumentationinPython:Demonstration. Com- Challenges that Public Defenders Face in Scrutinizing AI
putational Models of Argument-Proceedings of COMMA in Court. In Proceedings of the CHI Conference on Hu-
2022,353:349–350. man Factors in Computing Systems, CHI ’24. New York,
Boud,D.;andMolloy,E.2013. Rethinkingmodelsoffeed- NY, USA: Association for Computing Machinery. ISBN
back for learning: the challenge of design. Assessment & 9798400703300.
Evaluationinhighereducation,38(6):698–712. Kostic, M.; Witschel, H. F.; Hinkelmann, K.; and Spahic-
Chan,C.-M.;Chen,W.;Su,Y.;Yu,J.;Xue,W.;Zhang,S.; Bogdanovic, M. 2024. LLMs in Automated Essay Evalua-
Fu, J.; and Liu, Z. 2023. Chateval: Towards better llm- tion:ACaseStudy. InProceedingsoftheAAAISymposium
basedevaluatorsthroughmulti-agentdebate. arXivpreprint Series,volume3,143–147.
arXiv:2308.07201.
Leofante, F.; Ayoobi, H.; Dejl, A.; Freedman, G.; Gorur,
Cohn, C.; Hutchins, N.; Le, T.; and Biswas, G. 2024. A D.; Jiang, J.; Paulino-Passos, G.; Rago, A.; Rapberger, A.;
chain-of-thought prompting approach with llms for evalu- Russo,F.;Yin,X.;Zhang,D.;andToni,F.2024.Contestable
ating students’ formative assessment responses in science. AIneedsComputationalArgumentation.arXiv:2405.10729.
InProceedingsoftheAAAIConferenceonArtificialIntelli-
Liang, T.; He, Z.; Jiao, W.; Wang, X.; Wang, R.; Yang, Y.;
gence,volume38,23182–23190.
Tu, Z.; and Shi, S. 2024. Encouraging Divergent Think-
Dai,W.;Lin,J.;Jin,H.;Li,T.;Tsai,Y.-S.;Gasˇevic´,D.;and inginLargeLanguageModelsthroughMulti-AgentDebate.
Chen, G. 2023a. Can large language models provide feed- arXiv:2305.19118.
backtostudents?AcasestudyonChatGPT. In2023IEEE
Liu, O. L.; Rios, J. A.; Heilman, M.; Gerard, L.; and Linn,
International Conference on Advanced Learning Technolo-
M.C.2016. Validationofautomatedscoringofscienceas-
gies(ICALT),323–325.IEEE.
sessments. JournalofResearchinScienceTeaching,53(2):
Dai,W.;Lin,J.;Jin,H.;Li,T.;Tsai,Y.-S.;Gasˇevic´,D.;and
215–233.
Chen,G.2023b.CanLargeLanguageModelsProvideFeed-
backtoStudents?ACaseStudyonChatGPT. In2023IEEE Mitra, C.; Miroyan, M.; Jain, R.; Kumud, V.; Ranade, G.;
International Conference on Advanced Learning Technolo- andNorouzi,N.2024. RetLLM-E:Retrieval-PromptStrat-
gies(ICALT),323–325. egyforQuestion-AnsweringonStudentDiscussionForums.
Proceedings of the AAAI Conference on Artificial Intelli-
Du,Y.;Li,S.;Torralba,A.;Tenenbaum,J.B.;andMordatch,
gence,38(21):23215–23223.
I. 2023. Improving Factuality and Reasoning in Language
ModelsthroughMultiagentDebate. arXiv:2305.14325. Moore,S.;Nguyen,H.A.;Chen,T.;andStamper,J.2023.
Assessing the quality of multiple-choice questions using
Dung, P. M. 1995. On the acceptability of arguments
gpt-4andrule-basedmethods. InEuropeanConferenceon
and its fundamental role in nonmonotonic reasoning, logic
TechnologyEnhancedLearning,229–245.Springer.
programming and n-person games. Artificial Intelligence,
77(2):321–357. Nicol, D. 2014. From monologue to dialogue: improving
Freedman, G.; Dejl, A.; Gorur, D.; Yin, X.; Rago, A.; written feedback processes in mass higher education. In
and Toni, F. 2024. Argumentative Large Language Mod- Approaches to assessment that enhance learning in higher
els for Explainable and Contestable Decision-Making. education,11–27.Routledge.
arXiv:2405.02079. Ploug, T.; and Holm, S. 2020. The four dimensions of
Gubelmann,R.;Burkhard,M.;Ivanova,R.V.;Niklaus,C.; contestable AI diagnostics-A patient-centric approach to
Bermeitinger, B.; and Handschuh, S. 2024. Exploring the explainable AI. Artificial Intelligence in Medicine, 107:
UsefulnessofOpenandProprietaryLLMsinArgumentative 101901.
Writing Support. In International Conference on Artificial Rago, A.; Li, H.; and Toni, F. 2023. Interactive Explana-
IntelligenceinEducation,175–182.Springer. tionsbyConflictResolutionviaArgumentativeExchanges.
HaggingFace.2024. EssayGradeV1Project. arXiv:2303.15022.Sadler, D. R. 1989. Formative assessment and the design
ofinstructionalsystems. Instructionalscience,18(2):119–
144.
Schmidt,S.J.1999. Usingwritingtodevelopcriticalthink-
ingskills. NACTAjournal,31–38.
Sharadgah, T. 2014. Developing critical thinking skills
through writing in an internet-based environment. In So-
ciety for Information Technology & Teacher Education In-
ternationalConference,2178–2185.AssociationfortheAd-
vancementofComputinginEducation(AACE).
Shute,V.J.2008. Focusonformativefeedback. Reviewof
educationalresearch,78(1):153–189.
Stahl, M.; Biermann, L.; Nehring, A.; and Wachsmuth,
H. 2024. Exploring LLM Prompting Strategies for Joint
Essay Scoring and Feedback Generation. arXiv preprint
arXiv:2404.15845.
Tang, X.; Zou, A.; Zhang, Z.; Li, Z.; Zhao, Y.; Zhang, X.;
Cohan,A.;andGerstein,M.2024. MedAgents:LargeLan-
guageModelsasCollaboratorsforZero-shotMedicalRea-
soning. arXiv:2311.10537.
Wang,B.;Yue,X.;andSun,H.2023. CanChatGPTDefend
itsBeliefinTruth?EvaluatingLLMReasoningviaDebate.
arXiv:2305.13160.
Warrens, M. J. 2015. Five ways to look at Cohen’s kappa.
JournalofPsychology&Psychotherapy,5.
Wei,A.;Haghtalab,N.;andSteinhardt,J.2023. Jailbroken:
HowDoesLLMSafetyTrainingFail? arXiv:2307.02483.
Westera, W.; Dascalu, M.; Kurvers, H.; Ruseti, S.; and
Trausan-Matu,S.2018. Automatedessayscoringinapplied
games: Reducing the teacher bandwidth problem in online
training. Computers&Education,123:212–224.
Xiu,Y.;Xiao,Z.;andLiu,Y.2022.LogicNMR:Probingthe
Non-monotonicReasoningAbilityofPre-trainedLanguage
Models. InGoldberg,Y.;Kozareva,Z.;andZhang,Y.,eds.,
Findings of the Association for Computational Linguistics:
EMNLP 2022, 3616–3626. Abu Dhabi, United Arab Emi-
rates:AssociationforComputationalLinguistics.
Yancey,K.P.;Laflair,G.;Verardi,A.;andBurstein,J.2023.
Ratingshortl2essaysonthecefrscalewithgpt-4. InPro-
ceedingsofthe18thWorkshoponInnovativeUseofNLPfor
BuildingEducationalApplications(BEA2023),576–584.
Zhang,Y.;Li,Y.;Cui,L.;Cai,D.;Liu,L.;Fu,T.;Huang,X.;
Zhao,E.;Zhang,Y.;Chen,Y.;Wang,L.;Luu,A.T.;Bi,W.;
Shi,F.;andShi,S.2023. Siren’sSongintheAIOcean:A
SurveyonHallucinationinLargeLanguageModels. ArXiv,
abs/2309.01219.